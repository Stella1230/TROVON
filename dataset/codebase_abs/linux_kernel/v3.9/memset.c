void * memset ( void * V_1 , int V_2 , unsigned int V_3 )\r\n{\r\nregister char * T_1 V_4 ( L_1 ) = V_1 ;\r\nregister int T_2 V_4 ( L_2 ) = V_3 ;\r\nregister int T_3 V_4 ( L_3 ) = V_2 ;\r\n__asm__("movu.b %0,r13 \n\\r\nlslq 8,r13 \n\\r\nmove.b %0,r13 \n\\r\nmove.d r13,%0 \n\\r\nlslq 16,r13 \n\\r\nor.d r13,%0"\r\n: "=r" (lc)\r\n: "0" (lc)\r\n: "r13");\r\n{\r\nregister char * T_4 V_4 ( L_4 ) = V_1 ;\r\nif ( ( ( unsigned long ) V_1 & 3 ) != 0\r\n&& T_2 >= 3 )\r\n{\r\nif ( ( unsigned long ) T_4 & 1 )\r\n{\r\n* T_4 = ( char ) T_3 ;\r\nT_2 -- ;\r\nT_4 ++ ;\r\n}\r\nif ( ( unsigned long ) T_4 & 2 )\r\n{\r\n* ( short * ) T_4 = T_3 ;\r\nT_2 -= 2 ;\r\nT_4 += 2 ;\r\n}\r\n}\r\nif ( T_2 >= V_5 )\r\n{\r\n__asm__ volatile\r\n("\\r\n;; GCC does promise correct register allocations, but let's \n\\r\n;; make sure it keeps its promises. \n\\r\n.ifnc %0-%1-%4,$r13-$r12-$r11 \n\\r\n.error \"GCC reg alloc bug: %0-%1-%4 != $r13-$r12-$r11\" \n\\r\n.endif \n\\r\n\n\\r\n;; Save the registers we'll clobber in the movem process \n\\r\n;; on the stack. Don't mention them to gcc, it will only be \n\\r\n;; upset. \n\\r\nsubq 11*4,sp \n\\r\nmovem r10,[sp] \n\\r\n\n\\r\nmove.d r11,r0 \n\\r\nmove.d r11,r1 \n\\r\nmove.d r11,r2 \n\\r\nmove.d r11,r3 \n\\r\nmove.d r11,r4 \n\\r\nmove.d r11,r5 \n\\r\nmove.d r11,r6 \n\\r\nmove.d r11,r7 \n\\r\nmove.d r11,r8 \n\\r\nmove.d r11,r9 \n\\r\nmove.d r11,r10 \n\\r\n\n\\r\n;; Now we've got this: \n\\r\n;; r13 - dst \n\\r\n;; r12 - n \n\\r\n\n\\r\n;; Update n for the first loop \n\\r\nsubq 12*4,r12 \n\\r\n0: \n\\r\n"\r\n#ifdef F_1\r\n" setf\n"\r\n#endif\r\n" subq 12*4,r12 \n\\r\nbge 0b \n\\r\nmovem r11,[r13+] \n\\r\n\n\\r\n;; Compensate for last loop underflowing n. \n\\r\naddq 12*4,r12 \n\\r\n\n\\r\n;; Restore registers from stack. \n\\r\nmovem [sp+],r10"\r\n: "=r" (dst), "=r" (n)\r\n: "0" (dst), "1" (n), "r" (lc));\r\n}\r\nwhile ( T_2 >= 16 )\r\n{\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\nT_2 -= 16 ;\r\n}\r\nswitch ( T_2 )\r\n{\r\ncase 0 :\r\nbreak;\r\ncase 1 :\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\ncase 2 :\r\n* ( short * ) T_4 = ( short ) T_3 ;\r\nbreak;\r\ncase 3 :\r\n* ( short * ) T_4 = ( short ) T_3 ; T_4 += 2 ;\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\ncase 4 :\r\n* ( long * ) T_4 = T_3 ;\r\nbreak;\r\ncase 5 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\ncase 6 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( short * ) T_4 = ( short ) T_3 ;\r\nbreak;\r\ncase 7 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( short * ) T_4 = ( short ) T_3 ; T_4 += 2 ;\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\ncase 8 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ;\r\nbreak;\r\ncase 9 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\ncase 10 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( short * ) T_4 = ( short ) T_3 ;\r\nbreak;\r\ncase 11 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( short * ) T_4 = ( short ) T_3 ; T_4 += 2 ;\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\ncase 12 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ;\r\nbreak;\r\ncase 13 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\ncase 14 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( short * ) T_4 = ( short ) T_3 ;\r\nbreak;\r\ncase 15 :\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( long * ) T_4 = T_3 ; T_4 += 4 ;\r\n* ( short * ) T_4 = ( short ) T_3 ; T_4 += 2 ;\r\n* T_4 = ( char ) T_3 ;\r\nbreak;\r\n}\r\n}\r\nreturn T_1 ;\r\n}
