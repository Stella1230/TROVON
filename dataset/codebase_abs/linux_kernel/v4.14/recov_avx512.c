static int F_1 ( void )\r\n{\r\nreturn F_2 ( V_1 ) &&\r\nF_2 ( V_2 ) &&\r\nF_2 ( V_3 ) &&\r\nF_2 ( V_4 ) &&\r\nF_2 ( V_5 ) &&\r\nF_2 ( V_6 ) ;\r\n}\r\nstatic void F_3 ( int V_7 , T_1 V_8 , int V_9 ,\r\nint V_10 , void * * V_11 )\r\n{\r\nT_2 * V_12 , * V_13 , * V_14 , * V_15 ;\r\nconst T_2 * V_16 ;\r\nconst T_2 * V_17 ;\r\nconst T_2 V_18 = 0x0f ;\r\nV_12 = ( T_2 * ) V_11 [ V_7 - 2 ] ;\r\nV_13 = ( T_2 * ) V_11 [ V_7 - 1 ] ;\r\nV_14 = ( T_2 * ) V_11 [ V_9 ] ;\r\nV_11 [ V_9 ] = ( void * ) V_19 ;\r\nV_11 [ V_7 - 2 ] = V_14 ;\r\nV_15 = ( T_2 * ) V_11 [ V_10 ] ;\r\nV_11 [ V_10 ] = ( void * ) V_19 ;\r\nV_11 [ V_7 - 1 ] = V_15 ;\r\nV_20 . V_21 ( V_7 , V_8 , V_11 ) ;\r\nV_11 [ V_9 ] = V_14 ;\r\nV_11 [ V_10 ] = V_15 ;\r\nV_11 [ V_7 - 2 ] = V_12 ;\r\nV_11 [ V_7 - 1 ] = V_13 ;\r\nV_16 = V_22 [ V_23 [ V_10 - V_9 ] ] ;\r\nV_17 = V_22 [ V_24 [ V_25 [ V_9 ] ^\r\nV_25 [ V_10 ] ] ] ;\r\nF_4 () ;\r\nasm volatile("vpbroadcastb %0, %%zmm7" : : "m" (x0f));\r\nwhile ( V_8 ) {\r\n#ifdef F_5\r\nasm volatile("vmovdqa64 %0, %%zmm1\n\t"\r\n"vmovdqa64 %1, %%zmm9\n\t"\r\n"vmovdqa64 %2, %%zmm0\n\t"\r\n"vmovdqa64 %3, %%zmm8\n\t"\r\n"vpxorq %4, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %5, %%zmm9, %%zmm9\n\t"\r\n"vpxorq %6, %%zmm0, %%zmm0\n\t"\r\n"vpxorq %7, %%zmm8, %%zmm8"\r\n:\r\n: "m" (q[0]), "m" (q[64]), "m" (p[0]),\r\n"m" (p[64]), "m" (dq[0]), "m" (dq[64]),\r\n"m" (dp[0]), "m" (dp[64]));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm5"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm1, %%zmm3\n\t"\r\n"vpsraw $4, %%zmm9, %%zmm12\n\t"\r\n"vpandq %%zmm7, %%zmm1, %%zmm1\n\t"\r\n"vpandq %%zmm7, %%zmm9, %%zmm9\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm12, %%zmm12\n\t"\r\n"vpshufb %%zmm9, %%zmm4, %%zmm14\n\t"\r\n"vpshufb %%zmm1, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm12, %%zmm5, %%zmm15\n\t"\r\n"vpshufb %%zmm3, %%zmm5, %%zmm5\n\t"\r\n"vpxorq %%zmm14, %%zmm15, %%zmm15\n\t"\r\n"vpxorq %%zmm4, %%zmm5, %%zmm5"\r\n:\r\n: );\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1\n\t"\r\n"vpsraw $4, %%zmm0, %%zmm2\n\t"\r\n"vpsraw $4, %%zmm8, %%zmm6\n\t"\r\n"vpandq %%zmm7, %%zmm0, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm8, %%zmm14\n\t"\r\n"vpandq %%zmm7, %%zmm2, %%zmm2\n\t"\r\n"vpandq %%zmm7, %%zmm6, %%zmm6\n\t"\r\n"vpshufb %%zmm14, %%zmm4, %%zmm12\n\t"\r\n"vpshufb %%zmm3, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm6, %%zmm1, %%zmm13\n\t"\r\n"vpshufb %%zmm2, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm4, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm12, %%zmm13, %%zmm13"\r\n:\r\n: "m" (pbmul[0]), "m" (pbmul[16]));\r\nasm volatile("vpxorq %%zmm5, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm15, %%zmm13, %%zmm13"\r\n:\r\n: );\r\nasm volatile("vmovdqa64 %%zmm1, %0\n\t"\r\n"vmovdqa64 %%zmm13,%1\n\t"\r\n"vpxorq %%zmm1, %%zmm0, %%zmm0\n\t"\r\n"vpxorq %%zmm13, %%zmm8, %%zmm8"\r\n:\r\n: "m" (dq[0]), "m" (dq[64]));\r\nasm volatile("vmovdqa64 %%zmm0, %0\n\t"\r\n"vmovdqa64 %%zmm8, %1"\r\n:\r\n: "m" (dp[0]), "m" (dp[64]));\r\nV_8 -= 128 ;\r\nV_12 += 128 ;\r\nV_13 += 128 ;\r\nV_14 += 128 ;\r\nV_15 += 128 ;\r\n#else\r\nasm volatile("vmovdqa64 %0, %%zmm1\n\t"\r\n"vmovdqa64 %1, %%zmm0\n\t"\r\n"vpxorq %2, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %3, %%zmm0, %%zmm0"\r\n:\r\n: "m" (*q), "m" (*p), "m"(*dq), "m" (*dp));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm5"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm1, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm1, %%zmm1\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpshufb %%zmm1, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm3, %%zmm5, %%zmm5\n\t"\r\n"vpxorq %%zmm4, %%zmm5, %%zmm5"\r\n:\r\n: );\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1"\r\n:\r\n: "m" (pbmul[0]), "m" (pbmul[16]));\r\nasm volatile("vpsraw $4, %%zmm0, %%zmm2\n\t"\r\n"vpandq %%zmm7, %%zmm0, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm2, %%zmm2\n\t"\r\n"vpshufb %%zmm3, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm2, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm4, %%zmm1, %%zmm1"\r\n:\r\n: );\r\nasm volatile("vpxorq %%zmm5, %%zmm1, %%zmm1\n\t"\r\n"vmovdqa64 %%zmm1, %0\n\t"\r\n:\r\n: "m" (dq[0]));\r\nasm volatile("vpxorq %%zmm1, %%zmm0, %%zmm0\n\t"\r\n"vmovdqa64 %%zmm0, %0"\r\n:\r\n: "m" (dp[0]));\r\nV_8 -= 64 ;\r\nV_12 += 64 ;\r\nV_13 += 64 ;\r\nV_14 += 64 ;\r\nV_15 += 64 ;\r\n#endif\r\n}\r\nF_6 () ;\r\n}\r\nstatic void F_7 ( int V_7 , T_1 V_8 , int V_9 ,\r\nvoid * * V_11 )\r\n{\r\nT_2 * V_12 , * V_13 , * V_15 ;\r\nconst T_2 * V_17 ;\r\nconst T_2 V_18 = 0x0f ;\r\nV_12 = ( T_2 * ) V_11 [ V_7 - 2 ] ;\r\nV_13 = ( T_2 * ) V_11 [ V_7 - 1 ] ;\r\nV_15 = ( T_2 * ) V_11 [ V_9 ] ;\r\nV_11 [ V_9 ] = ( void * ) V_19 ;\r\nV_11 [ V_7 - 1 ] = V_15 ;\r\nV_20 . V_21 ( V_7 , V_8 , V_11 ) ;\r\nV_11 [ V_9 ] = V_15 ;\r\nV_11 [ V_7 - 1 ] = V_13 ;\r\nV_17 = V_22 [ V_24 [ V_25 [ V_9 ] ] ] ;\r\nF_4 () ;\r\nasm volatile("vpbroadcastb %0, %%zmm7" : : "m" (x0f));\r\nwhile ( V_8 ) {\r\n#ifdef F_5\r\nasm volatile("vmovdqa64 %0, %%zmm3\n\t"\r\n"vmovdqa64 %1, %%zmm8\n\t"\r\n"vpxorq %2, %%zmm3, %%zmm3\n\t"\r\n"vpxorq %3, %%zmm8, %%zmm8"\r\n:\r\n: "m" (dq[0]), "m" (dq[64]), "m" (q[0]),\r\n"m" (q[64]));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm0\n\t"\r\n"vmovapd %%zmm0, %%zmm13\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1\n\t"\r\n"vmovapd %%zmm1, %%zmm14"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm3, %%zmm6\n\t"\r\n"vpsraw $4, %%zmm8, %%zmm12\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm8, %%zmm8\n\t"\r\n"vpandq %%zmm7, %%zmm6, %%zmm6\n\t"\r\n"vpandq %%zmm7, %%zmm12, %%zmm12\n\t"\r\n"vpshufb %%zmm3, %%zmm0, %%zmm0\n\t"\r\n"vpshufb %%zmm8, %%zmm13, %%zmm13\n\t"\r\n"vpshufb %%zmm6, %%zmm1, %%zmm1\n\t"\r\n"vpshufb %%zmm12, %%zmm14, %%zmm14\n\t"\r\n"vpxorq %%zmm0, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm13, %%zmm14, %%zmm14"\r\n:\r\n: );\r\nasm volatile("vmovdqa64 %0, %%zmm2\n\t"\r\n"vmovdqa64 %1, %%zmm12\n\t"\r\n"vpxorq %%zmm1, %%zmm2, %%zmm2\n\t"\r\n"vpxorq %%zmm14, %%zmm12, %%zmm12"\r\n:\r\n: "m" (p[0]), "m" (p[64]));\r\nasm volatile("vmovdqa64 %%zmm1, %0\n\t"\r\n"vmovdqa64 %%zmm14, %1\n\t"\r\n"vmovdqa64 %%zmm2, %2\n\t"\r\n"vmovdqa64 %%zmm12,%3"\r\n:\r\n: "m" (dq[0]), "m" (dq[64]), "m" (p[0]),\r\n"m" (p[64]));\r\nV_8 -= 128 ;\r\nV_12 += 128 ;\r\nV_13 += 128 ;\r\nV_15 += 128 ;\r\n#else\r\nasm volatile("vmovdqa64 %0, %%zmm3\n\t"\r\n"vpxorq %1, %%zmm3, %%zmm3"\r\n:\r\n: "m" (dq[0]), "m" (q[0]));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm0\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm3, %%zmm6\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm6, %%zmm6\n\t"\r\n"vpshufb %%zmm3, %%zmm0, %%zmm0\n\t"\r\n"vpshufb %%zmm6, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm0, %%zmm1, %%zmm1"\r\n:\r\n: );\r\nasm volatile("vmovdqa64 %0, %%zmm2\n\t"\r\n"vpxorq %%zmm1, %%zmm2, %%zmm2"\r\n:\r\n: "m" (p[0]));\r\nasm volatile("vmovdqa64 %%zmm1, %0\n\t"\r\n"vmovdqa64 %%zmm2, %1"\r\n:\r\n: "m" (dq[0]), "m" (p[0]));\r\nV_8 -= 64 ;\r\nV_12 += 64 ;\r\nV_13 += 64 ;\r\nV_15 += 64 ;\r\n#endif\r\n}\r\nF_6 () ;\r\n}
