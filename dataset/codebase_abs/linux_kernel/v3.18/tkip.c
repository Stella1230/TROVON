unsigned int F_1 ( unsigned int V_1 )\r\n{\r\nunsigned int V_2 ;\r\nunsigned int V_3 ;\r\nunsigned int V_4 , V_5 ;\r\nV_2 = ( V_1 % 256 ) ;\r\nV_3 = ( ( V_1 >> 8 ) % 256 ) ;\r\nV_4 = V_6 [ V_2 ] + ( V_7 [ V_2 ] * 256 ) ;\r\nV_5 = V_7 [ V_3 ] + ( V_6 [ V_3 ] * 256 ) ;\r\nreturn V_4 ^ V_5 ;\r\n}\r\nunsigned int F_2 ( unsigned int V_8 )\r\n{\r\nunsigned int V_9 ;\r\nif ( ( V_8 & 0x01 ) == 0x01 )\r\nV_9 = ( V_8 >> 1 ) | 0x8000 ;\r\nelse\r\nV_9 = ( V_8 >> 1 ) & 0x7fff ;\r\nV_9 = V_9 % 65536 ;\r\nreturn V_9 ;\r\n}\r\nvoid F_3 (\r\nunsigned char * V_10 ,\r\nunsigned char * V_11 ,\r\nunsigned short V_12 ,\r\nunsigned long V_13 ,\r\nunsigned char * V_14\r\n)\r\n{\r\nunsigned int V_15 [ 5 ] ;\r\nunsigned int V_16 , V_17 , V_18 ;\r\nunsigned int V_19 , V_20 , V_21 , V_22 , V_23 , V_24 ;\r\nunsigned long int V_25 , V_26 ;\r\nint V_27 , V_28 ;\r\nV_25 = V_12 ;\r\nV_26 = V_13 ;\r\nV_16 = ( unsigned int ) ( ( V_26 >> 16 ) % 65536 ) ;\r\nV_17 = ( unsigned int ) ( V_26 % 65536 ) ;\r\nV_18 = ( unsigned int ) ( V_25 % 65536 ) ;\r\nV_15 [ 0 ] = V_17 ;\r\nV_15 [ 1 ] = V_16 ;\r\nV_15 [ 2 ] = ( unsigned int ) ( V_11 [ 0 ] + ( V_11 [ 1 ] * 256 ) ) ;\r\nV_15 [ 3 ] = ( unsigned int ) ( V_11 [ 2 ] + ( V_11 [ 3 ] * 256 ) ) ;\r\nV_15 [ 4 ] = ( unsigned int ) ( V_11 [ 4 ] + ( V_11 [ 5 ] * 256 ) ) ;\r\nfor ( V_27 = 0 ; V_27 < 8 ; V_27 ++ ) {\r\nV_28 = 2 * ( V_27 & 1 ) ;\r\nV_15 [ 0 ] = ( V_15 [ 0 ] + F_1 ( ( V_15 [ 4 ] ^ ( ( 256 * V_10 [ 1 + V_28 ] ) + V_10 [ V_28 ] ) ) % 65536 ) ) % 65536 ;\r\nV_15 [ 1 ] = ( V_15 [ 1 ] + F_1 ( ( V_15 [ 0 ] ^ ( ( 256 * V_10 [ 5 + V_28 ] ) + V_10 [ 4 + V_28 ] ) ) % 65536 ) ) % 65536 ;\r\nV_15 [ 2 ] = ( V_15 [ 2 ] + F_1 ( ( V_15 [ 1 ] ^ ( ( 256 * V_10 [ 9 + V_28 ] ) + V_10 [ 8 + V_28 ] ) ) % 65536 ) ) % 65536 ;\r\nV_15 [ 3 ] = ( V_15 [ 3 ] + F_1 ( ( V_15 [ 2 ] ^ ( ( 256 * V_10 [ 13 + V_28 ] ) + V_10 [ 12 + V_28 ] ) ) % 65536 ) ) % 65536 ;\r\nV_15 [ 4 ] = ( V_15 [ 4 ] + F_1 ( ( V_15 [ 3 ] ^ ( ( ( 256 * V_10 [ 1 + V_28 ] ) + V_10 [ V_28 ] ) ) ) % 65536 ) ) % 65536 ;\r\nV_15 [ 4 ] = ( V_15 [ 4 ] + V_27 ) % 65536 ;\r\n}\r\nV_19 = V_15 [ 0 ] ;\r\nV_20 = V_15 [ 1 ] ;\r\nV_21 = V_15 [ 2 ] ;\r\nV_22 = V_15 [ 3 ] ;\r\nV_23 = V_15 [ 4 ] ;\r\nV_24 = ( V_15 [ 4 ] + V_18 ) % 65536 ;\r\nV_19 = V_19 + F_1 ( ( V_24 ^ ( ( 256 * V_10 [ 1 ] ) + V_10 [ 0 ] ) ) % 65536 ) ;\r\nV_20 = V_20 + F_1 ( ( V_19 ^ ( ( 256 * V_10 [ 3 ] ) + V_10 [ 2 ] ) ) % 65536 ) ;\r\nV_21 = V_21 + F_1 ( ( V_20 ^ ( ( 256 * V_10 [ 5 ] ) + V_10 [ 4 ] ) ) % 65536 ) ;\r\nV_22 = V_22 + F_1 ( ( V_21 ^ ( ( 256 * V_10 [ 7 ] ) + V_10 [ 6 ] ) ) % 65536 ) ;\r\nV_23 = V_23 + F_1 ( ( V_22 ^ ( ( 256 * V_10 [ 9 ] ) + V_10 [ 8 ] ) ) % 65536 ) ;\r\nV_24 = V_24 + F_1 ( ( V_23 ^ ( ( 256 * V_10 [ 11 ] ) + V_10 [ 10 ] ) ) % 65536 ) ;\r\nV_19 = V_19 + F_2 ( V_24 ^ ( ( 256 * V_10 [ 13 ] ) + V_10 [ 12 ] ) ) ;\r\nV_20 = V_20 + F_2 ( V_19 ^ ( ( 256 * V_10 [ 15 ] ) + V_10 [ 14 ] ) ) ;\r\nV_21 = V_21 + F_2 ( V_20 ) ;\r\nV_22 = V_22 + F_2 ( V_21 ) ;\r\nV_23 = V_23 + F_2 ( V_22 ) ;\r\nV_24 = V_24 + F_2 ( V_23 ) ;\r\nV_14 [ 0 ] = ( V_18 >> 8 ) % 256 ;\r\nV_14 [ 1 ] = ( ( ( V_18 >> 8 ) % 256 ) | 0x20 ) & 0x7f ;\r\nV_14 [ 2 ] = V_18 % 256 ;\r\nV_14 [ 3 ] = ( ( V_24 ^ ( ( 256 * V_10 [ 1 ] ) + V_10 [ 0 ] ) ) >> 1 ) % 256 ;\r\nV_14 [ 4 ] = V_19 % 256 ;\r\nV_14 [ 5 ] = ( V_19 >> 8 ) % 256 ;\r\nV_14 [ 6 ] = V_20 % 256 ;\r\nV_14 [ 7 ] = ( V_20 >> 8 ) % 256 ;\r\nV_14 [ 8 ] = V_21 % 256 ;\r\nV_14 [ 9 ] = ( V_21 >> 8 ) % 256 ;\r\nV_14 [ 10 ] = V_22 % 256 ;\r\nV_14 [ 11 ] = ( V_22 >> 8 ) % 256 ;\r\nV_14 [ 12 ] = V_23 % 256 ;\r\nV_14 [ 13 ] = ( V_23 >> 8 ) % 256 ;\r\nV_14 [ 14 ] = V_24 % 256 ;\r\nV_14 [ 15 ] = ( V_24 >> 8 ) % 256 ;\r\n}
