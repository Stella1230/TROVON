static inline int F_1 ( unsigned long V_1 , unsigned long V_2 , unsigned long V_3 )\r\n{\r\n#ifdef F_2\r\nif ( V_3 >= 64 && ( ( V_1 ^ V_2 ) & V_4 . V_5 ) )\r\nreturn 0 ;\r\n#endif\r\nreturn 1 ;\r\n}\r\nunsigned long\r\nF_3 ( void T_1 * V_6 , unsigned long V_3 )\r\n{\r\nF_4 () ;\r\nif ( F_5 ( V_7 , V_6 , V_3 ) )\r\nF_6 ( V_6 , V_3 ) ;\r\nreturn V_3 ;\r\n}\r\nunsigned long\r\nF_7 ( void T_1 * V_6 , unsigned long V_3 )\r\n{\r\nF_6 ( V_6 , V_3 ) ;\r\nreturn V_3 ;\r\n}\r\nstatic unsigned long\r\nF_8 ( void T_1 * V_6 , const void * V_8 , unsigned long V_9 )\r\n{\r\nint V_10 , V_11 ;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"1: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 3f\n"\r\n"2: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"3: movl 0(%4), %%eax\n"\r\n"4: movl 4(%4), %%edx\n"\r\n"5: movl %%eax, 0(%3)\n"\r\n"6: movl %%edx, 4(%3)\n"\r\n"7: movl 8(%4), %%eax\n"\r\n"8: movl 12(%4),%%edx\n"\r\n"9: movl %%eax, 8(%3)\n"\r\n"10: movl %%edx, 12(%3)\n"\r\n"11: movl 16(%4), %%eax\n"\r\n"12: movl 20(%4), %%edx\n"\r\n"13: movl %%eax, 16(%3)\n"\r\n"14: movl %%edx, 20(%3)\n"\r\n"15: movl 24(%4), %%eax\n"\r\n"16: movl 28(%4), %%edx\n"\r\n"17: movl %%eax, 24(%3)\n"\r\n"18: movl %%edx, 28(%3)\n"\r\n"19: movl 32(%4), %%eax\n"\r\n"20: movl 36(%4), %%edx\n"\r\n"21: movl %%eax, 32(%3)\n"\r\n"22: movl %%edx, 36(%3)\n"\r\n"23: movl 40(%4), %%eax\n"\r\n"24: movl 44(%4), %%edx\n"\r\n"25: movl %%eax, 40(%3)\n"\r\n"26: movl %%edx, 44(%3)\n"\r\n"27: movl 48(%4), %%eax\n"\r\n"28: movl 52(%4), %%edx\n"\r\n"29: movl %%eax, 48(%3)\n"\r\n"30: movl %%edx, 52(%3)\n"\r\n"31: movl 56(%4), %%eax\n"\r\n"32: movl 60(%4), %%edx\n"\r\n"33: movl %%eax, 56(%3)\n"\r\n"34: movl %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 1b\n"\r\n"35: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"99: rep; movsl\n"\r\n"36: movl %%eax, %0\n"\r\n"37: rep; movsb\n"\r\n"100:\n"\r\n".section .fixup,\"ax\"\n"\r\n"101: lea 0(%%eax,%0,4),%0\n"\r\n" jmp 100b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(1b,100b)\r\n_ASM_EXTABLE(2b,100b)\r\n_ASM_EXTABLE(3b,100b)\r\n_ASM_EXTABLE(4b,100b)\r\n_ASM_EXTABLE(5b,100b)\r\n_ASM_EXTABLE(6b,100b)\r\n_ASM_EXTABLE(7b,100b)\r\n_ASM_EXTABLE(8b,100b)\r\n_ASM_EXTABLE(9b,100b)\r\n_ASM_EXTABLE(10b,100b)\r\n_ASM_EXTABLE(11b,100b)\r\n_ASM_EXTABLE(12b,100b)\r\n_ASM_EXTABLE(13b,100b)\r\n_ASM_EXTABLE(14b,100b)\r\n_ASM_EXTABLE(15b,100b)\r\n_ASM_EXTABLE(16b,100b)\r\n_ASM_EXTABLE(17b,100b)\r\n_ASM_EXTABLE(18b,100b)\r\n_ASM_EXTABLE(19b,100b)\r\n_ASM_EXTABLE(20b,100b)\r\n_ASM_EXTABLE(21b,100b)\r\n_ASM_EXTABLE(22b,100b)\r\n_ASM_EXTABLE(23b,100b)\r\n_ASM_EXTABLE(24b,100b)\r\n_ASM_EXTABLE(25b,100b)\r\n_ASM_EXTABLE(26b,100b)\r\n_ASM_EXTABLE(27b,100b)\r\n_ASM_EXTABLE(28b,100b)\r\n_ASM_EXTABLE(29b,100b)\r\n_ASM_EXTABLE(30b,100b)\r\n_ASM_EXTABLE(31b,100b)\r\n_ASM_EXTABLE(32b,100b)\r\n_ASM_EXTABLE(33b,100b)\r\n_ASM_EXTABLE(34b,100b)\r\n_ASM_EXTABLE(35b,100b)\r\n_ASM_EXTABLE(36b,100b)\r\n_ASM_EXTABLE(37b,100b)\r\n_ASM_EXTABLE(99b,101b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn V_9 ;\r\n}\r\nstatic unsigned long\r\nF_9 ( void * V_6 , const void T_1 * V_8 , unsigned long V_9 )\r\n{\r\nint V_10 , V_11 ;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movl %%eax, 0(%3)\n"\r\n" movl %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movl %%eax, 8(%3)\n"\r\n" movl %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movl %%eax, 16(%3)\n"\r\n" movl %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movl %%eax, 24(%3)\n"\r\n" movl %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movl %%eax, 32(%3)\n"\r\n" movl %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movl %%eax, 40(%3)\n"\r\n" movl %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movl %%eax, 48(%3)\n"\r\n" movl %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movl %%eax, 56(%3)\n"\r\n" movl %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: pushl %0\n"\r\n" pushl %%eax\n"\r\n" xorl %%eax,%%eax\n"\r\n" rep; stosb\n"\r\n" popl %%eax\n"\r\n" popl %0\n"\r\n" jmp 8b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(0b,16b)\r\n_ASM_EXTABLE(1b,16b)\r\n_ASM_EXTABLE(2b,16b)\r\n_ASM_EXTABLE(21b,16b)\r\n_ASM_EXTABLE(3b,16b)\r\n_ASM_EXTABLE(31b,16b)\r\n_ASM_EXTABLE(4b,16b)\r\n_ASM_EXTABLE(41b,16b)\r\n_ASM_EXTABLE(10b,16b)\r\n_ASM_EXTABLE(51b,16b)\r\n_ASM_EXTABLE(11b,16b)\r\n_ASM_EXTABLE(61b,16b)\r\n_ASM_EXTABLE(12b,16b)\r\n_ASM_EXTABLE(71b,16b)\r\n_ASM_EXTABLE(13b,16b)\r\n_ASM_EXTABLE(81b,16b)\r\n_ASM_EXTABLE(14b,16b)\r\n_ASM_EXTABLE(91b,16b)\r\n_ASM_EXTABLE(6b,9b)\r\n_ASM_EXTABLE(7b,16b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn V_9 ;\r\n}\r\nstatic unsigned long F_10 ( void * V_6 ,\r\nconst void T_1 * V_8 , unsigned long V_9 )\r\n{\r\nint V_10 , V_11 ;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movnti %%eax, 0(%3)\n"\r\n" movnti %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movnti %%eax, 8(%3)\n"\r\n" movnti %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movnti %%eax, 16(%3)\n"\r\n" movnti %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movnti %%eax, 24(%3)\n"\r\n" movnti %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movnti %%eax, 32(%3)\n"\r\n" movnti %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movnti %%eax, 40(%3)\n"\r\n" movnti %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movnti %%eax, 48(%3)\n"\r\n" movnti %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movnti %%eax, 56(%3)\n"\r\n" movnti %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n" sfence \n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: pushl %0\n"\r\n" pushl %%eax\n"\r\n" xorl %%eax,%%eax\n"\r\n" rep; stosb\n"\r\n" popl %%eax\n"\r\n" popl %0\n"\r\n" jmp 8b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(0b,16b)\r\n_ASM_EXTABLE(1b,16b)\r\n_ASM_EXTABLE(2b,16b)\r\n_ASM_EXTABLE(21b,16b)\r\n_ASM_EXTABLE(3b,16b)\r\n_ASM_EXTABLE(31b,16b)\r\n_ASM_EXTABLE(4b,16b)\r\n_ASM_EXTABLE(41b,16b)\r\n_ASM_EXTABLE(10b,16b)\r\n_ASM_EXTABLE(51b,16b)\r\n_ASM_EXTABLE(11b,16b)\r\n_ASM_EXTABLE(61b,16b)\r\n_ASM_EXTABLE(12b,16b)\r\n_ASM_EXTABLE(71b,16b)\r\n_ASM_EXTABLE(13b,16b)\r\n_ASM_EXTABLE(81b,16b)\r\n_ASM_EXTABLE(14b,16b)\r\n_ASM_EXTABLE(91b,16b)\r\n_ASM_EXTABLE(6b,9b)\r\n_ASM_EXTABLE(7b,16b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn V_9 ;\r\n}\r\nstatic unsigned long F_11 ( void * V_6 ,\r\nconst void T_1 * V_8 , unsigned long V_9 )\r\n{\r\nint V_10 , V_11 ;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movnti %%eax, 0(%3)\n"\r\n" movnti %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movnti %%eax, 8(%3)\n"\r\n" movnti %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movnti %%eax, 16(%3)\n"\r\n" movnti %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movnti %%eax, 24(%3)\n"\r\n" movnti %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movnti %%eax, 32(%3)\n"\r\n" movnti %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movnti %%eax, 40(%3)\n"\r\n" movnti %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movnti %%eax, 48(%3)\n"\r\n" movnti %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movnti %%eax, 56(%3)\n"\r\n" movnti %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n" sfence \n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: jmp 8b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(0b,16b)\r\n_ASM_EXTABLE(1b,16b)\r\n_ASM_EXTABLE(2b,16b)\r\n_ASM_EXTABLE(21b,16b)\r\n_ASM_EXTABLE(3b,16b)\r\n_ASM_EXTABLE(31b,16b)\r\n_ASM_EXTABLE(4b,16b)\r\n_ASM_EXTABLE(41b,16b)\r\n_ASM_EXTABLE(10b,16b)\r\n_ASM_EXTABLE(51b,16b)\r\n_ASM_EXTABLE(11b,16b)\r\n_ASM_EXTABLE(61b,16b)\r\n_ASM_EXTABLE(12b,16b)\r\n_ASM_EXTABLE(71b,16b)\r\n_ASM_EXTABLE(13b,16b)\r\n_ASM_EXTABLE(81b,16b)\r\n_ASM_EXTABLE(14b,16b)\r\n_ASM_EXTABLE(91b,16b)\r\n_ASM_EXTABLE(6b,9b)\r\n_ASM_EXTABLE(7b,16b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn V_9 ;\r\n}\r\nunsigned long F_12 ( void T_1 * V_6 , const void * V_8 ,\r\nunsigned long V_3 )\r\n{\r\nF_13 () ;\r\nif ( F_14 ( V_6 , V_8 , V_3 ) )\r\nF_15 ( V_6 , V_8 , V_3 ) ;\r\nelse\r\nV_3 = F_8 ( V_6 , V_8 , V_3 ) ;\r\nF_16 () ;\r\nreturn V_3 ;\r\n}\r\nunsigned long F_17 ( void * V_6 , const void T_1 * V_8 ,\r\nunsigned long V_3 )\r\n{\r\nF_13 () ;\r\nif ( F_14 ( V_6 , V_8 , V_3 ) )\r\nF_18 ( V_6 , V_8 , V_3 ) ;\r\nelse\r\nV_3 = F_9 ( V_6 , V_8 , V_3 ) ;\r\nF_16 () ;\r\nreturn V_3 ;\r\n}\r\nunsigned long F_19 ( void * V_6 , const void T_1 * V_8 ,\r\nunsigned long V_3 )\r\n{\r\nF_13 () ;\r\nif ( F_14 ( V_6 , V_8 , V_3 ) )\r\nF_15 ( V_6 , V_8 , V_3 ) ;\r\nelse\r\nV_3 = F_8 ( ( void T_1 * ) V_6 ,\r\n( const void * ) V_8 , V_3 ) ;\r\nF_16 () ;\r\nreturn V_3 ;\r\n}\r\nunsigned long F_20 ( void * V_6 , const void T_1 * V_8 ,\r\nunsigned long V_3 )\r\n{\r\nF_13 () ;\r\n#ifdef F_2\r\nif ( V_3 > 64 && F_21 ( V_12 ) )\r\nV_3 = F_10 ( V_6 , V_8 , V_3 ) ;\r\nelse\r\nF_18 ( V_6 , V_8 , V_3 ) ;\r\n#else\r\nF_18 ( V_6 , V_8 , V_3 ) ;\r\n#endif\r\nF_16 () ;\r\nreturn V_3 ;\r\n}\r\nunsigned long F_22 ( void * V_6 , const void T_1 * V_8 ,\r\nunsigned long V_3 )\r\n{\r\nF_13 () ;\r\n#ifdef F_2\r\nif ( V_3 > 64 && F_21 ( V_12 ) )\r\nV_3 = F_11 ( V_6 , V_8 , V_3 ) ;\r\nelse\r\nF_15 ( V_6 , V_8 , V_3 ) ;\r\n#else\r\nF_15 ( V_6 , V_8 , V_3 ) ;\r\n#endif\r\nF_16 () ;\r\nreturn V_3 ;\r\n}
