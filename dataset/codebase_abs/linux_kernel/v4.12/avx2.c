static int F_1 ( void )\r\n{\r\nreturn F_2 ( V_1 ) && F_2 ( V_2 ) ;\r\n}\r\nstatic void F_3 ( int V_3 , T_1 V_4 , void * * V_5 )\r\n{\r\nT_2 * * V_6 = ( T_2 * * ) V_5 ;\r\nT_2 * V_7 , * V_8 ;\r\nint V_9 , V_10 , V_11 ;\r\nV_11 = V_3 - 3 ;\r\nV_7 = V_6 [ V_11 + 1 ] ;\r\nV_8 = V_6 [ V_11 + 2 ] ;\r\nF_4 () ;\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nasm volatile("vpxor %ymm3,%ymm3,%ymm3");\r\nfor ( V_9 = 0 ; V_9 < V_4 ; V_9 += 32 ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("vmovdqa %0,%%ymm2" : : "m" (dptr[z0][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0-1][d]));\r\nasm volatile("vmovdqa %ymm2,%ymm4");\r\nasm volatile("vmovdqa %0,%%ymm6" : : "m" (dptr[z0-1][d]));\r\nfor ( V_10 = V_11 - 2 ; V_10 >= 0 ; V_10 -- ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("vpcmpgtb %ymm4,%ymm3,%ymm5");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm6,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm6,%ymm4,%ymm4");\r\nasm volatile("vmovdqa %0,%%ymm6" : : "m" (dptr[z][d]));\r\n}\r\nasm volatile("vpcmpgtb %ymm4,%ymm3,%ymm5");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm6,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm6,%ymm4,%ymm4");\r\nasm volatile("vmovntdq %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vpxor %ymm2,%ymm2,%ymm2");\r\nasm volatile("vmovntdq %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vpxor %ymm4,%ymm4,%ymm4");\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nF_5 () ;\r\n}\r\nstatic void F_6 ( int V_3 , int V_12 , int V_13 ,\r\nT_1 V_4 , void * * V_5 )\r\n{\r\nT_2 * * V_6 = ( T_2 * * ) V_5 ;\r\nT_2 * V_7 , * V_8 ;\r\nint V_9 , V_10 , V_11 ;\r\nV_11 = V_13 ;\r\nV_7 = V_6 [ V_3 - 2 ] ;\r\nV_8 = V_6 [ V_3 - 1 ] ;\r\nF_4 () ;\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nfor ( V_9 = 0 ; V_9 < V_4 ; V_9 += 32 ) {\r\nasm volatile("vmovdqa %0,%%ymm4" :: "m" (dptr[z0][d]));\r\nasm volatile("vmovdqa %0,%%ymm2" : : "m" (p[d]));\r\nasm volatile("vpxor %ymm4,%ymm2,%ymm2");\r\nfor ( V_10 = V_11 - 1 ; V_10 >= V_12 ; V_10 -- ) {\r\nasm volatile("vpxor %ymm5,%ymm5,%ymm5");\r\nasm volatile("vpcmpgtb %ymm4,%ymm5,%ymm5");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vmovdqa %0,%%ymm5" :: "m" (dptr[z][d]));\r\nasm volatile("vpxor %ymm5,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\n}\r\nfor ( V_10 = V_12 - 1 ; V_10 >= 0 ; V_10 -- ) {\r\nasm volatile("vpxor %ymm5,%ymm5,%ymm5");\r\nasm volatile("vpcmpgtb %ymm4,%ymm5,%ymm5");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\n}\r\nasm volatile("vpxor %0,%%ymm4,%%ymm4" : : "m" (q[d]));\r\nasm volatile("vmovdqa %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vmovdqa %%ymm2,%0" : "=m" (p[d]));\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nF_5 () ;\r\n}\r\nstatic void F_7 ( int V_3 , T_1 V_4 , void * * V_5 )\r\n{\r\nT_2 * * V_6 = ( T_2 * * ) V_5 ;\r\nT_2 * V_7 , * V_8 ;\r\nint V_9 , V_10 , V_11 ;\r\nV_11 = V_3 - 3 ;\r\nV_7 = V_6 [ V_11 + 1 ] ;\r\nV_8 = V_6 [ V_11 + 2 ] ;\r\nF_4 () ;\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nasm volatile("vpxor %ymm1,%ymm1,%ymm1");\r\nfor ( V_9 = 0 ; V_9 < V_4 ; V_9 += 64 ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d+32]));\r\nasm volatile("vmovdqa %0,%%ymm2" : : "m" (dptr[z0][d]));\r\nasm volatile("vmovdqa %0,%%ymm3" : : "m" (dptr[z0][d+32]));\r\nasm volatile("vmovdqa %ymm2,%ymm4");\r\nasm volatile("vmovdqa %ymm3,%ymm6");\r\nfor ( V_10 = V_11 - 1 ; V_10 >= 0 ; V_10 -- ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+32]));\r\nasm volatile("vpcmpgtb %ymm4,%ymm1,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm1,%ymm7");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vmovdqa %0,%%ymm5" : : "m" (dptr[z][d]));\r\nasm volatile("vmovdqa %0,%%ymm7" : : "m" (dptr[z][d+32]));\r\nasm volatile("vpxor %ymm5,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm7,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\n}\r\nasm volatile("vmovntdq %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vmovntdq %%ymm3,%0" : "=m" (p[d+32]));\r\nasm volatile("vmovntdq %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vmovntdq %%ymm6,%0" : "=m" (q[d+32]));\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nF_5 () ;\r\n}\r\nstatic void F_8 ( int V_3 , int V_12 , int V_13 ,\r\nT_1 V_4 , void * * V_5 )\r\n{\r\nT_2 * * V_6 = ( T_2 * * ) V_5 ;\r\nT_2 * V_7 , * V_8 ;\r\nint V_9 , V_10 , V_11 ;\r\nV_11 = V_13 ;\r\nV_7 = V_6 [ V_3 - 2 ] ;\r\nV_8 = V_6 [ V_3 - 1 ] ;\r\nF_4 () ;\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nfor ( V_9 = 0 ; V_9 < V_4 ; V_9 += 64 ) {\r\nasm volatile("vmovdqa %0,%%ymm4" :: "m" (dptr[z0][d]));\r\nasm volatile("vmovdqa %0,%%ymm6" :: "m" (dptr[z0][d+32]));\r\nasm volatile("vmovdqa %0,%%ymm2" : : "m" (p[d]));\r\nasm volatile("vmovdqa %0,%%ymm3" : : "m" (p[d+32]));\r\nasm volatile("vpxor %ymm4,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm6,%ymm3,%ymm3");\r\nfor ( V_10 = V_11 - 1 ; V_10 >= V_12 ; V_10 -- ) {\r\nasm volatile("vpxor %ymm5,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm7,%ymm7,%ymm7");\r\nasm volatile("vpcmpgtb %ymm4,%ymm5,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm7,%ymm7");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vmovdqa %0,%%ymm5" :: "m" (dptr[z][d]));\r\nasm volatile("vmovdqa %0,%%ymm7"\r\n:: "m" (dptr[z][d+32]));\r\nasm volatile("vpxor %ymm5,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm7,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\n}\r\nfor ( V_10 = V_12 - 1 ; V_10 >= 0 ; V_10 -- ) {\r\nasm volatile("vpxor %ymm5,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm7,%ymm7,%ymm7");\r\nasm volatile("vpcmpgtb %ymm4,%ymm5,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm7,%ymm7");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\n}\r\nasm volatile("vpxor %0,%%ymm4,%%ymm4" : : "m" (q[d]));\r\nasm volatile("vpxor %0,%%ymm6,%%ymm6" : : "m" (q[d+32]));\r\nasm volatile("vmovdqa %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vmovdqa %%ymm6,%0" : "=m" (q[d+32]));\r\nasm volatile("vmovdqa %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vmovdqa %%ymm3,%0" : "=m" (p[d+32]));\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nF_5 () ;\r\n}\r\nstatic void F_9 ( int V_3 , T_1 V_4 , void * * V_5 )\r\n{\r\nT_2 * * V_6 = ( T_2 * * ) V_5 ;\r\nT_2 * V_7 , * V_8 ;\r\nint V_9 , V_10 , V_11 ;\r\nV_11 = V_3 - 3 ;\r\nV_7 = V_6 [ V_11 + 1 ] ;\r\nV_8 = V_6 [ V_11 + 2 ] ;\r\nF_4 () ;\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nasm volatile("vpxor %ymm1,%ymm1,%ymm1");\r\nasm volatile("vpxor %ymm2,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm3,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm10,%ymm10,%ymm10");\r\nasm volatile("vpxor %ymm11,%ymm11,%ymm11");\r\nasm volatile("vpxor %ymm12,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm14,%ymm14,%ymm14");\r\nfor ( V_9 = 0 ; V_9 < V_4 ; V_9 += 128 ) {\r\nfor ( V_10 = V_11 ; V_10 >= 0 ; V_10 -- ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+32]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+64]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+96]));\r\nasm volatile("vpcmpgtb %ymm4,%ymm1,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm1,%ymm7");\r\nasm volatile("vpcmpgtb %ymm12,%ymm1,%ymm13");\r\nasm volatile("vpcmpgtb %ymm14,%ymm1,%ymm15");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpaddb %ymm12,%ymm12,%ymm12");\r\nasm volatile("vpaddb %ymm14,%ymm14,%ymm14");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpand %ymm0,%ymm13,%ymm13");\r\nasm volatile("vpand %ymm0,%ymm15,%ymm15");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm13,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm15,%ymm14,%ymm14");\r\nasm volatile("vmovdqa %0,%%ymm5" : : "m" (dptr[z][d]));\r\nasm volatile("vmovdqa %0,%%ymm7" : : "m" (dptr[z][d+32]));\r\nasm volatile("vmovdqa %0,%%ymm13" : : "m" (dptr[z][d+64]));\r\nasm volatile("vmovdqa %0,%%ymm15" : : "m" (dptr[z][d+96]));\r\nasm volatile("vpxor %ymm5,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm7,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm13,%ymm10,%ymm10");\r\nasm volatile("vpxor %ymm15,%ymm11,%ymm11");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm13,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm15,%ymm14,%ymm14");\r\n}\r\nasm volatile("vmovntdq %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vpxor %ymm2,%ymm2,%ymm2");\r\nasm volatile("vmovntdq %%ymm3,%0" : "=m" (p[d+32]));\r\nasm volatile("vpxor %ymm3,%ymm3,%ymm3");\r\nasm volatile("vmovntdq %%ymm10,%0" : "=m" (p[d+64]));\r\nasm volatile("vpxor %ymm10,%ymm10,%ymm10");\r\nasm volatile("vmovntdq %%ymm11,%0" : "=m" (p[d+96]));\r\nasm volatile("vpxor %ymm11,%ymm11,%ymm11");\r\nasm volatile("vmovntdq %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vpxor %ymm4,%ymm4,%ymm4");\r\nasm volatile("vmovntdq %%ymm6,%0" : "=m" (q[d+32]));\r\nasm volatile("vpxor %ymm6,%ymm6,%ymm6");\r\nasm volatile("vmovntdq %%ymm12,%0" : "=m" (q[d+64]));\r\nasm volatile("vpxor %ymm12,%ymm12,%ymm12");\r\nasm volatile("vmovntdq %%ymm14,%0" : "=m" (q[d+96]));\r\nasm volatile("vpxor %ymm14,%ymm14,%ymm14");\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nF_5 () ;\r\n}\r\nstatic void F_10 ( int V_3 , int V_12 , int V_13 ,\r\nT_1 V_4 , void * * V_5 )\r\n{\r\nT_2 * * V_6 = ( T_2 * * ) V_5 ;\r\nT_2 * V_7 , * V_8 ;\r\nint V_9 , V_10 , V_11 ;\r\nV_11 = V_13 ;\r\nV_7 = V_6 [ V_3 - 2 ] ;\r\nV_8 = V_6 [ V_3 - 1 ] ;\r\nF_4 () ;\r\nasm volatile("vmovdqa %0,%%ymm0" :: "m" (raid6_avx2_constants.x1d[0]));\r\nfor ( V_9 = 0 ; V_9 < V_4 ; V_9 += 128 ) {\r\nasm volatile("vmovdqa %0,%%ymm4" :: "m" (dptr[z0][d]));\r\nasm volatile("vmovdqa %0,%%ymm6" :: "m" (dptr[z0][d+32]));\r\nasm volatile("vmovdqa %0,%%ymm12" :: "m" (dptr[z0][d+64]));\r\nasm volatile("vmovdqa %0,%%ymm14" :: "m" (dptr[z0][d+96]));\r\nasm volatile("vmovdqa %0,%%ymm2" : : "m" (p[d]));\r\nasm volatile("vmovdqa %0,%%ymm3" : : "m" (p[d+32]));\r\nasm volatile("vmovdqa %0,%%ymm10" : : "m" (p[d+64]));\r\nasm volatile("vmovdqa %0,%%ymm11" : : "m" (p[d+96]));\r\nasm volatile("vpxor %ymm4,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm6,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm12,%ymm10,%ymm10");\r\nasm volatile("vpxor %ymm14,%ymm11,%ymm11");\r\nfor ( V_10 = V_11 - 1 ; V_10 >= V_12 ; V_10 -- ) {\r\nasm volatile("prefetchnta %0" :: "m" (dptr[z][d]));\r\nasm volatile("prefetchnta %0" :: "m" (dptr[z][d+64]));\r\nasm volatile("vpxor %ymm5,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm7,%ymm7,%ymm7");\r\nasm volatile("vpxor %ymm13,%ymm13,%ymm13");\r\nasm volatile("vpxor %ymm15,%ymm15,%ymm15");\r\nasm volatile("vpcmpgtb %ymm4,%ymm5,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm7,%ymm7");\r\nasm volatile("vpcmpgtb %ymm12,%ymm13,%ymm13");\r\nasm volatile("vpcmpgtb %ymm14,%ymm15,%ymm15");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpaddb %ymm12,%ymm12,%ymm12");\r\nasm volatile("vpaddb %ymm14,%ymm14,%ymm14");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpand %ymm0,%ymm13,%ymm13");\r\nasm volatile("vpand %ymm0,%ymm15,%ymm15");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm13,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm15,%ymm14,%ymm14");\r\nasm volatile("vmovdqa %0,%%ymm5" :: "m" (dptr[z][d]));\r\nasm volatile("vmovdqa %0,%%ymm7"\r\n:: "m" (dptr[z][d+32]));\r\nasm volatile("vmovdqa %0,%%ymm13"\r\n:: "m" (dptr[z][d+64]));\r\nasm volatile("vmovdqa %0,%%ymm15"\r\n:: "m" (dptr[z][d+96]));\r\nasm volatile("vpxor %ymm5,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm7,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm13,%ymm10,%ymm10");\r\nasm volatile("vpxor %ymm15,%ymm11,%ymm11");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm13,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm15,%ymm14,%ymm14");\r\n}\r\nasm volatile("prefetchnta %0" :: "m" (q[d]));\r\nasm volatile("prefetchnta %0" :: "m" (q[d+64]));\r\nfor ( V_10 = V_12 - 1 ; V_10 >= 0 ; V_10 -- ) {\r\nasm volatile("vpxor %ymm5,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm7,%ymm7,%ymm7");\r\nasm volatile("vpxor %ymm13,%ymm13,%ymm13");\r\nasm volatile("vpxor %ymm15,%ymm15,%ymm15");\r\nasm volatile("vpcmpgtb %ymm4,%ymm5,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm7,%ymm7");\r\nasm volatile("vpcmpgtb %ymm12,%ymm13,%ymm13");\r\nasm volatile("vpcmpgtb %ymm14,%ymm15,%ymm15");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpaddb %ymm12,%ymm12,%ymm12");\r\nasm volatile("vpaddb %ymm14,%ymm14,%ymm14");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpand %ymm0,%ymm13,%ymm13");\r\nasm volatile("vpand %ymm0,%ymm15,%ymm15");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm13,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm15,%ymm14,%ymm14");\r\n}\r\nasm volatile("vmovntdq %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vmovntdq %%ymm3,%0" : "=m" (p[d+32]));\r\nasm volatile("vmovntdq %%ymm10,%0" : "=m" (p[d+64]));\r\nasm volatile("vmovntdq %%ymm11,%0" : "=m" (p[d+96]));\r\nasm volatile("vpxor %0,%%ymm4,%%ymm4" : : "m" (q[d]));\r\nasm volatile("vpxor %0,%%ymm6,%%ymm6" : : "m" (q[d+32]));\r\nasm volatile("vpxor %0,%%ymm12,%%ymm12" : : "m" (q[d+64]));\r\nasm volatile("vpxor %0,%%ymm14,%%ymm14" : : "m" (q[d+96]));\r\nasm volatile("vmovntdq %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vmovntdq %%ymm6,%0" : "=m" (q[d+32]));\r\nasm volatile("vmovntdq %%ymm12,%0" : "=m" (q[d+64]));\r\nasm volatile("vmovntdq %%ymm14,%0" : "=m" (q[d+96]));\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nF_5 () ;\r\n}
