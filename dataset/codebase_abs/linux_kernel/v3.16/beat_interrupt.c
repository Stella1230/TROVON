static inline void F_1 ( unsigned int V_1 )\r\n{\r\nint V_2 ;\r\nunsigned long V_3 [ 4 ] ;\r\nV_2 = ( V_1 / 256 ) * 4 ;\r\nV_3 [ 0 ] = V_4 [ V_2 + 0 ]\r\n& V_5 [ V_2 + 0 ] ;\r\nV_3 [ 1 ] = V_4 [ V_2 + 1 ]\r\n& V_5 [ V_2 + 1 ] ;\r\nV_3 [ 2 ] = V_4 [ V_2 + 2 ]\r\n& V_5 [ V_2 + 2 ] ;\r\nV_3 [ 3 ] = V_4 [ V_2 + 3 ]\r\n& V_5 [ V_2 + 3 ] ;\r\nif ( F_2 ( V_1 & ~ 255UL ,\r\nV_3 [ 0 ] , V_3 [ 1 ] , V_3 [ 2 ] , V_3 [ 3 ] ) != 0 )\r\nF_3 ( L_1 ) ;\r\n}\r\nstatic void F_4 ( struct V_6 * V_7 )\r\n{\r\nunsigned long V_8 ;\r\nF_5 ( & V_9 , V_8 ) ;\r\nV_4 [ V_7 -> V_10 / 64 ] &= ~ ( 1UL << ( 63 - ( V_7 -> V_10 % 64 ) ) ) ;\r\nF_1 ( V_7 -> V_10 ) ;\r\nF_6 ( & V_9 , V_8 ) ;\r\n}\r\nstatic void F_7 ( struct V_6 * V_7 )\r\n{\r\nunsigned long V_8 ;\r\nF_5 ( & V_9 , V_8 ) ;\r\nV_4 [ V_7 -> V_10 / 64 ] |= 1UL << ( 63 - ( V_7 -> V_10 % 64 ) ) ;\r\nF_1 ( V_7 -> V_10 ) ;\r\nF_6 ( & V_9 , V_8 ) ;\r\n}\r\nstatic void F_8 ( struct V_6 * V_7 )\r\n{\r\nunsigned long V_8 ;\r\nF_5 ( & V_9 , V_8 ) ;\r\nV_5 [ V_7 -> V_10 / 64 ] &= ~ ( 1UL << ( 63 - ( V_7 -> V_10 % 64 ) ) ) ;\r\nF_1 ( V_7 -> V_10 ) ;\r\nF_6 ( & V_9 , V_8 ) ;\r\n}\r\nstatic void F_9 ( struct V_6 * V_7 )\r\n{\r\nT_1 V_11 ;\r\nunsigned long V_8 ;\r\nV_11 = F_10 ( V_7 -> V_10 ) ;\r\nif ( V_11 != 0 ) {\r\nif ( ( V_11 & 0xFFFFFFFF ) != 0xFFFFFFF5 )\r\nF_3 ( L_2 , V_11 ) ;\r\nF_11 ( V_12 L_3 , V_7 -> V_10 ) ;\r\n}\r\nF_5 ( & V_9 , V_8 ) ;\r\nV_5 [ V_7 -> V_10 / 64 ] |= 1UL << ( 63 - ( V_7 -> V_10 % 64 ) ) ;\r\nF_1 ( V_7 -> V_10 ) ;\r\nF_6 ( & V_9 , V_8 ) ;\r\n}\r\nstatic void F_12 ( struct V_13 * V_14 , unsigned int V_15 )\r\n{\r\nF_13 ( V_15 ) ;\r\n}\r\nstatic int F_14 ( struct V_13 * V_14 , unsigned int V_15 ,\r\nT_2 V_16 )\r\n{\r\nT_3 V_11 ;\r\nV_11 = F_15 ( V_15 , V_16 ) ;\r\nif ( V_11 < 0 )\r\nreturn - V_17 ;\r\nF_16 ( V_15 , V_18 ) ;\r\nF_17 ( V_15 , & V_19 , V_20 ) ;\r\nreturn 0 ;\r\n}\r\nstatic int F_18 ( struct V_13 * V_14 , struct V_21 * V_22 ,\r\nconst T_4 * V_23 , unsigned int V_24 ,\r\nT_2 * V_25 ,\r\nunsigned int * V_26 )\r\n{\r\nconst T_5 * V_27 = ( const T_5 * ) V_23 ;\r\n* V_25 = * V_27 ;\r\n* V_26 |= V_28 ;\r\nreturn 0 ;\r\n}\r\nstatic int F_19 ( struct V_13 * V_14 , struct V_21 * V_29 )\r\n{\r\nreturn 1 ;\r\n}\r\nstatic inline unsigned int F_20 ( void )\r\n{\r\nint V_30 ;\r\nT_6 V_31 [ 4 ] , V_32 ;\r\nfor ( V_30 = 0 ; V_30 < V_33 ; V_30 += 256 ) {\r\nF_21 ( V_30 , V_31 ) ;\r\n__asm__ ("cntlzd %0,%1":"=r"(ub):\r\n"r"(pending[0] & beatic_irq_mask_enable[i/64+0]\r\n& beatic_irq_mask_ack[i/64+0]));\r\nif ( V_32 != 64 )\r\nreturn V_30 + V_32 + 0 ;\r\n__asm__ ("cntlzd %0,%1":"=r"(ub):\r\n"r"(pending[1] & beatic_irq_mask_enable[i/64+1]\r\n& beatic_irq_mask_ack[i/64+1]));\r\nif ( V_32 != 64 )\r\nreturn V_30 + V_32 + 64 ;\r\n__asm__ ("cntlzd %0,%1":"=r"(ub):\r\n"r"(pending[2] & beatic_irq_mask_enable[i/64+2]\r\n& beatic_irq_mask_ack[i/64+2]));\r\nif ( V_32 != 64 )\r\nreturn V_30 + V_32 + 128 ;\r\n__asm__ ("cntlzd %0,%1":"=r"(ub):\r\n"r"(pending[3] & beatic_irq_mask_enable[i/64+3]\r\n& beatic_irq_mask_ack[i/64+3]));\r\nif ( V_32 != 64 )\r\nreturn V_30 + V_32 + 192 ;\r\n}\r\nreturn V_34 ;\r\n}\r\nunsigned int F_22 ( void )\r\n{\r\nunsigned int V_35 ;\r\nV_35 = F_20 () ;\r\nif ( V_35 != V_34 )\r\nF_8 ( F_23 ( V_35 ) ) ;\r\nreturn V_35 ;\r\n}\r\nvoid T_7 F_24 ( void )\r\n{\r\nint V_30 ;\r\nmemset ( V_4 , 0 , sizeof( V_4 ) ) ;\r\nmemset ( V_5 , 255 , sizeof( V_5 ) ) ;\r\nfor ( V_30 = 0 ; V_30 < V_33 ; V_30 += 256 )\r\nF_2 ( V_30 , 0L , 0L , 0L , 0L ) ;\r\nV_36 . V_37 = F_22 ;\r\nV_38 = F_25 ( NULL , ~ 0 , & V_39 , NULL ) ;\r\nF_26 ( V_38 == NULL ) ;\r\nF_27 ( V_38 ) ;\r\n}\r\nvoid F_28 ( void )\r\n{\r\nint V_30 ;\r\nfor ( V_30 = 1 ; V_30 < V_40 ; V_30 ++ )\r\nF_13 ( V_30 ) ;\r\n}
