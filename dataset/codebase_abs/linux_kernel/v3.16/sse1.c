static int F_1 ( void )\r\n{\r\nreturn F_2 ( V_1 ) &&\r\n( F_2 ( V_2 ) ||\r\nF_2 ( V_3 ) ) ;\r\n}\r\nstatic void F_3 ( int V_4 , T_1 V_5 , void * * V_6 )\r\n{\r\nT_2 * * V_7 = ( T_2 * * ) V_6 ;\r\nT_2 * V_8 , * V_9 ;\r\nint V_10 , V_11 , V_12 ;\r\nV_12 = V_4 - 3 ;\r\nV_8 = V_7 [ V_12 + 1 ] ;\r\nV_9 = V_7 [ V_12 + 2 ] ;\r\nF_4 () ;\r\nasm volatile("movq %0,%%mm0" : : "m" (raid6_mmx_constants.x1d));\r\nasm volatile("pxor %mm5,%mm5");\r\nfor ( V_10 = 0 ; V_10 < V_5 ; V_10 += 8 ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("movq %0,%%mm2" : : "m" (dptr[z0][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0-1][d]));\r\nasm volatile("movq %mm2,%mm4");\r\nasm volatile("movq %0,%%mm6" : : "m" (dptr[z0-1][d]));\r\nfor ( V_11 = V_12 - 2 ; V_11 >= 0 ; V_11 -- ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("pcmpgtb %mm4,%mm5");\r\nasm volatile("paddb %mm4,%mm4");\r\nasm volatile("pand %mm0,%mm5");\r\nasm volatile("pxor %mm5,%mm4");\r\nasm volatile("pxor %mm5,%mm5");\r\nasm volatile("pxor %mm6,%mm2");\r\nasm volatile("pxor %mm6,%mm4");\r\nasm volatile("movq %0,%%mm6" : : "m" (dptr[z][d]));\r\n}\r\nasm volatile("pcmpgtb %mm4,%mm5");\r\nasm volatile("paddb %mm4,%mm4");\r\nasm volatile("pand %mm0,%mm5");\r\nasm volatile("pxor %mm5,%mm4");\r\nasm volatile("pxor %mm5,%mm5");\r\nasm volatile("pxor %mm6,%mm2");\r\nasm volatile("pxor %mm6,%mm4");\r\nasm volatile("movntq %%mm2,%0" : "=m" (p[d]));\r\nasm volatile("movntq %%mm4,%0" : "=m" (q[d]));\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nF_5 () ;\r\n}\r\nstatic void F_6 ( int V_4 , T_1 V_5 , void * * V_6 )\r\n{\r\nT_2 * * V_7 = ( T_2 * * ) V_6 ;\r\nT_2 * V_8 , * V_9 ;\r\nint V_10 , V_11 , V_12 ;\r\nV_12 = V_4 - 3 ;\r\nV_8 = V_7 [ V_12 + 1 ] ;\r\nV_9 = V_7 [ V_12 + 2 ] ;\r\nF_4 () ;\r\nasm volatile("movq %0,%%mm0" : : "m" (raid6_mmx_constants.x1d));\r\nasm volatile("pxor %mm5,%mm5");\r\nasm volatile("pxor %mm7,%mm7");\r\nfor ( V_10 = 0 ; V_10 < V_5 ; V_10 += 16 ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("movq %0,%%mm2" : : "m" (dptr[z0][d]));\r\nasm volatile("movq %0,%%mm3" : : "m" (dptr[z0][d+8]));\r\nasm volatile("movq %mm2,%mm4");\r\nasm volatile("movq %mm3,%mm6");\r\nfor ( V_11 = V_12 - 1 ; V_11 >= 0 ; V_11 -- ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("pcmpgtb %mm4,%mm5");\r\nasm volatile("pcmpgtb %mm6,%mm7");\r\nasm volatile("paddb %mm4,%mm4");\r\nasm volatile("paddb %mm6,%mm6");\r\nasm volatile("pand %mm0,%mm5");\r\nasm volatile("pand %mm0,%mm7");\r\nasm volatile("pxor %mm5,%mm4");\r\nasm volatile("pxor %mm7,%mm6");\r\nasm volatile("movq %0,%%mm5" : : "m" (dptr[z][d]));\r\nasm volatile("movq %0,%%mm7" : : "m" (dptr[z][d+8]));\r\nasm volatile("pxor %mm5,%mm2");\r\nasm volatile("pxor %mm7,%mm3");\r\nasm volatile("pxor %mm5,%mm4");\r\nasm volatile("pxor %mm7,%mm6");\r\nasm volatile("pxor %mm5,%mm5");\r\nasm volatile("pxor %mm7,%mm7");\r\n}\r\nasm volatile("movntq %%mm2,%0" : "=m" (p[d]));\r\nasm volatile("movntq %%mm3,%0" : "=m" (p[d+8]));\r\nasm volatile("movntq %%mm4,%0" : "=m" (q[d]));\r\nasm volatile("movntq %%mm6,%0" : "=m" (q[d+8]));\r\n}\r\nasm volatile("sfence" : :: "memory");\r\nF_5 () ;\r\n}
