static int F_1 ( void )\r\n{\r\nreturn F_2 ( V_1 ) &&\r\nF_2 ( V_2 ) &&\r\nF_2 ( V_3 ) ;\r\n}\r\nstatic void F_3 ( int V_4 , T_1 V_5 , int V_6 ,\r\nint V_7 , void * * V_8 )\r\n{\r\nT_2 * V_9 , * V_10 , * V_11 , * V_12 ;\r\nconst T_2 * V_13 ;\r\nconst T_2 * V_14 ;\r\nstatic const T_2 V_15 ( 16 ) V_16 [ 16 ] = {\r\n0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f ,\r\n0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f } ;\r\nV_9 = ( T_2 * ) V_8 [ V_4 - 2 ] ;\r\nV_10 = ( T_2 * ) V_8 [ V_4 - 1 ] ;\r\nV_11 = ( T_2 * ) V_8 [ V_6 ] ;\r\nV_8 [ V_6 ] = ( void * ) V_17 ;\r\nV_8 [ V_4 - 2 ] = V_11 ;\r\nV_12 = ( T_2 * ) V_8 [ V_7 ] ;\r\nV_8 [ V_7 ] = ( void * ) V_17 ;\r\nV_8 [ V_4 - 1 ] = V_12 ;\r\nV_18 . V_19 ( V_4 , V_5 , V_8 ) ;\r\nV_8 [ V_6 ] = V_11 ;\r\nV_8 [ V_7 ] = V_12 ;\r\nV_8 [ V_4 - 2 ] = V_9 ;\r\nV_8 [ V_4 - 1 ] = V_10 ;\r\nV_13 = V_20 [ V_21 [ V_7 - V_6 ] ] ;\r\nV_14 = V_20 [ V_22 [ V_23 [ V_6 ] ^\r\nV_23 [ V_7 ] ] ] ;\r\nF_4 () ;\r\nasm volatile("movdqa %0,%%xmm7" : : "m" (x0f[0]));\r\n#ifdef F_5\r\nasm volatile("movdqa %0,%%xmm6" : : "m" (qmul[0]));\r\nasm volatile("movdqa %0,%%xmm14" : : "m" (pbmul[0]));\r\nasm volatile("movdqa %0,%%xmm15" : : "m" (pbmul[16]));\r\n#endif\r\nwhile ( V_5 ) {\r\n#ifdef F_5\r\nasm volatile("movdqa %0,%%xmm1" : : "m" (q[0]));\r\nasm volatile("movdqa %0,%%xmm9" : : "m" (q[16]));\r\nasm volatile("movdqa %0,%%xmm0" : : "m" (p[0]));\r\nasm volatile("movdqa %0,%%xmm8" : : "m" (p[16]));\r\nasm volatile("pxor %0,%%xmm1" : : "m" (dq[0]));\r\nasm volatile("pxor %0,%%xmm9" : : "m" (dq[16]));\r\nasm volatile("pxor %0,%%xmm0" : : "m" (dp[0]));\r\nasm volatile("pxor %0,%%xmm8" : : "m" (dp[16]));\r\nasm volatile("movdqa %xmm6,%xmm4");\r\nasm volatile("movdqa %0,%%xmm5" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm6,%xmm12");\r\nasm volatile("movdqa %xmm5,%xmm13");\r\nasm volatile("movdqa %xmm1,%xmm3");\r\nasm volatile("movdqa %xmm9,%xmm11");\r\nasm volatile("movdqa %xmm0,%xmm2");\r\nasm volatile("movdqa %xmm8,%xmm10");\r\nasm volatile("psraw $4,%xmm1");\r\nasm volatile("psraw $4,%xmm9");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm11");\r\nasm volatile("pand %xmm7,%xmm1");\r\nasm volatile("pand %xmm7,%xmm9");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm11,%xmm12");\r\nasm volatile("pshufb %xmm1,%xmm5");\r\nasm volatile("pshufb %xmm9,%xmm13");\r\nasm volatile("pxor %xmm4,%xmm5");\r\nasm volatile("pxor %xmm12,%xmm13");\r\nasm volatile("movdqa %xmm14,%xmm4");\r\nasm volatile("movdqa %xmm15,%xmm1");\r\nasm volatile("movdqa %xmm14,%xmm12");\r\nasm volatile("movdqa %xmm15,%xmm9");\r\nasm volatile("movdqa %xmm2,%xmm3");\r\nasm volatile("movdqa %xmm10,%xmm11");\r\nasm volatile("psraw $4,%xmm2");\r\nasm volatile("psraw $4,%xmm10");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm11");\r\nasm volatile("pand %xmm7,%xmm2");\r\nasm volatile("pand %xmm7,%xmm10");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm11,%xmm12");\r\nasm volatile("pshufb %xmm2,%xmm1");\r\nasm volatile("pshufb %xmm10,%xmm9");\r\nasm volatile("pxor %xmm4,%xmm1");\r\nasm volatile("pxor %xmm12,%xmm9");\r\nasm volatile("pxor %xmm5,%xmm1");\r\nasm volatile("pxor %xmm13,%xmm9");\r\nasm volatile("movdqa %%xmm1,%0" : "=m" (dq[0]));\r\nasm volatile("movdqa %%xmm9,%0" : "=m" (dq[16]));\r\nasm volatile("pxor %xmm1,%xmm0");\r\nasm volatile("pxor %xmm9,%xmm8");\r\nasm volatile("movdqa %%xmm0,%0" : "=m" (dp[0]));\r\nasm volatile("movdqa %%xmm8,%0" : "=m" (dp[16]));\r\nV_5 -= 32 ;\r\nV_9 += 32 ;\r\nV_10 += 32 ;\r\nV_11 += 32 ;\r\nV_12 += 32 ;\r\n#else\r\nasm volatile("movdqa %0,%%xmm1" : : "m" (*q));\r\nasm volatile("movdqa %0,%%xmm0" : : "m" (*p));\r\nasm volatile("pxor %0,%%xmm1" : : "m" (*dq));\r\nasm volatile("pxor %0,%%xmm0" : : "m" (*dp));\r\nasm volatile("movdqa %0,%%xmm4" : : "m" (qmul[0]));\r\nasm volatile("movdqa %0,%%xmm5" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm1,%xmm3");\r\nasm volatile("psraw $4,%xmm1");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm1");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm1,%xmm5");\r\nasm volatile("pxor %xmm4,%xmm5");\r\nasm volatile("movdqa %xmm0,%xmm2");\r\nasm volatile("movdqa %0,%%xmm4" : : "m" (pbmul[0]));\r\nasm volatile("movdqa %0,%%xmm1" : : "m" (pbmul[16]));\r\nasm volatile("movdqa %xmm2,%xmm3");\r\nasm volatile("psraw $4,%xmm2");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm2");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm2,%xmm1");\r\nasm volatile("pxor %xmm4,%xmm1");\r\nasm volatile("pxor %xmm5,%xmm1");\r\nasm volatile("movdqa %%xmm1,%0" : "=m" (*dq));\r\nasm volatile("pxor %xmm1,%xmm0");\r\nasm volatile("movdqa %%xmm0,%0" : "=m" (*dp));\r\nV_5 -= 16 ;\r\nV_9 += 16 ;\r\nV_10 += 16 ;\r\nV_11 += 16 ;\r\nV_12 += 16 ;\r\n#endif\r\n}\r\nF_6 () ;\r\n}\r\nstatic void F_7 ( int V_4 , T_1 V_5 , int V_6 ,\r\nvoid * * V_8 )\r\n{\r\nT_2 * V_9 , * V_10 , * V_12 ;\r\nconst T_2 * V_14 ;\r\nstatic const T_2 V_15 ( 16 ) V_16 [ 16 ] = {\r\n0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f ,\r\n0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f , 0x0f } ;\r\nV_9 = ( T_2 * ) V_8 [ V_4 - 2 ] ;\r\nV_10 = ( T_2 * ) V_8 [ V_4 - 1 ] ;\r\nV_12 = ( T_2 * ) V_8 [ V_6 ] ;\r\nV_8 [ V_6 ] = ( void * ) V_17 ;\r\nV_8 [ V_4 - 1 ] = V_12 ;\r\nV_18 . V_19 ( V_4 , V_5 , V_8 ) ;\r\nV_8 [ V_6 ] = V_12 ;\r\nV_8 [ V_4 - 1 ] = V_10 ;\r\nV_14 = V_20 [ V_22 [ V_23 [ V_6 ] ] ] ;\r\nF_4 () ;\r\nasm volatile("movdqa %0, %%xmm7" : : "m" (x0f[0]));\r\nwhile ( V_5 ) {\r\n#ifdef F_5\r\nasm volatile("movdqa %0, %%xmm3" : : "m" (dq[0]));\r\nasm volatile("movdqa %0, %%xmm4" : : "m" (dq[16]));\r\nasm volatile("pxor %0, %%xmm3" : : "m" (q[0]));\r\nasm volatile("movdqa %0, %%xmm0" : : "m" (qmul[0]));\r\nasm volatile("pxor %0, %%xmm4" : : "m" (q[16]));\r\nasm volatile("movdqa %0, %%xmm1" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm3, %xmm6");\r\nasm volatile("movdqa %xmm4, %xmm8");\r\nasm volatile("psraw $4, %xmm3");\r\nasm volatile("pand %xmm7, %xmm6");\r\nasm volatile("pand %xmm7, %xmm3");\r\nasm volatile("pshufb %xmm6, %xmm0");\r\nasm volatile("pshufb %xmm3, %xmm1");\r\nasm volatile("movdqa %0, %%xmm10" : : "m" (qmul[0]));\r\nasm volatile("pxor %xmm0, %xmm1");\r\nasm volatile("movdqa %0, %%xmm11" : : "m" (qmul[16]));\r\nasm volatile("psraw $4, %xmm4");\r\nasm volatile("pand %xmm7, %xmm8");\r\nasm volatile("pand %xmm7, %xmm4");\r\nasm volatile("pshufb %xmm8, %xmm10");\r\nasm volatile("pshufb %xmm4, %xmm11");\r\nasm volatile("movdqa %0, %%xmm2" : : "m" (p[0]));\r\nasm volatile("pxor %xmm10, %xmm11");\r\nasm volatile("movdqa %0, %%xmm12" : : "m" (p[16]));\r\nasm volatile("pxor %xmm1, %xmm2");\r\nasm volatile("pxor %xmm11, %xmm12");\r\nasm volatile("movdqa %%xmm1, %0" : "=m" (dq[0]));\r\nasm volatile("movdqa %%xmm11, %0" : "=m" (dq[16]));\r\nasm volatile("movdqa %%xmm2, %0" : "=m" (p[0]));\r\nasm volatile("movdqa %%xmm12, %0" : "=m" (p[16]));\r\nV_5 -= 32 ;\r\nV_9 += 32 ;\r\nV_10 += 32 ;\r\nV_12 += 32 ;\r\n#else\r\nasm volatile("movdqa %0, %%xmm3" : : "m" (dq[0]));\r\nasm volatile("movdqa %0, %%xmm0" : : "m" (qmul[0]));\r\nasm volatile("pxor %0, %%xmm3" : : "m" (q[0]));\r\nasm volatile("movdqa %0, %%xmm1" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm3, %xmm6");\r\nasm volatile("movdqa %0, %%xmm2" : : "m" (p[0]));\r\nasm volatile("psraw $4, %xmm3");\r\nasm volatile("pand %xmm7, %xmm6");\r\nasm volatile("pand %xmm7, %xmm3");\r\nasm volatile("pshufb %xmm6, %xmm0");\r\nasm volatile("pshufb %xmm3, %xmm1");\r\nasm volatile("pxor %xmm0, %xmm1");\r\nasm volatile("pxor %xmm1, %xmm2");\r\nasm volatile("movdqa %%xmm1, %0" : "=m" (dq[0]));\r\nasm volatile("movdqa %%xmm2, %0" : "=m" (p[0]));\r\nV_5 -= 16 ;\r\nV_9 += 16 ;\r\nV_10 += 16 ;\r\nV_12 += 16 ;\r\n#endif\r\n}\r\nF_6 () ;\r\n}
