int\r\nksocknal_lib_get_conn_addrs (ksock_conn_t *conn)\r\n{\r\nint rc = libcfs_sock_getaddr(conn->ksnc_sock, 1,\r\n&conn->ksnc_ipaddr,\r\n&conn->ksnc_port);\r\nLASSERT (!conn->ksnc_closing);\r\nif (rc != 0) {\r\nCERROR ("Error %d getting sock peer IP\n", rc);\r\nreturn rc;\r\n}\r\nrc = libcfs_sock_getaddr(conn->ksnc_sock, 0,\r\n&conn->ksnc_myipaddr, NULL);\r\nif (rc != 0) {\r\nCERROR ("Error %d getting sock local IP\n", rc);\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nint\r\nksocknal_lib_zc_capable(ksock_conn_t *conn)\r\n{\r\nint caps = conn->ksnc_sock->sk->sk_route_caps;\r\nif (conn->ksnc_proto == &ksocknal_protocol_v1x)\r\nreturn 0;\r\nreturn ((caps & NETIF_F_SG) != 0 && (caps & NETIF_F_ALL_CSUM) != 0);\r\n}\r\nint\r\nksocknal_lib_send_iov (ksock_conn_t *conn, ksock_tx_t *tx)\r\n{\r\nstruct socket *sock = conn->ksnc_sock;\r\nint nob;\r\nint rc;\r\nif (*ksocknal_tunables.ksnd_enable_csum &&\r\nconn->ksnc_proto == &ksocknal_protocol_v2x &&\r\ntx->tx_nob == tx->tx_resid &&\r\ntx->tx_msg.ksm_csum == 0)\r\nksocknal_lib_csum_tx(tx);\r\n{\r\n#if SOCKNAL_SINGLE_FRAG_TX\r\nstruct iovec scratch;\r\nstruct iovec *scratchiov = &scratch;\r\nunsigned int niov = 1;\r\n#else\r\nstruct iovec *scratchiov = conn->ksnc_scheduler->kss_scratch_iov;\r\nunsigned int niov = tx->tx_niov;\r\n#endif\r\nstruct msghdr msg = {.msg_flags = MSG_DONTWAIT};\r\nint i;\r\nfor (nob = i = 0; i < niov; i++) {\r\nscratchiov[i] = tx->tx_iov[i];\r\nnob += scratchiov[i].iov_len;\r\n}\r\nif (!list_empty(&conn->ksnc_tx_queue) ||\r\nnob < tx->tx_resid)\r\nmsg.msg_flags |= MSG_MORE;\r\nrc = kernel_sendmsg(sock, &msg, (struct kvec *)scratchiov, niov, nob);\r\n}\r\nreturn rc;\r\n}\r\nint\r\nksocknal_lib_send_kiov (ksock_conn_t *conn, ksock_tx_t *tx)\r\n{\r\nstruct socket *sock = conn->ksnc_sock;\r\nlnet_kiov_t *kiov = tx->tx_kiov;\r\nint rc;\r\nint nob;\r\nLASSERT (tx->tx_lnetmsg != NULL);\r\nif (tx->tx_msg.ksm_zc_cookies[0] != 0) {\r\nstruct sock *sk = sock->sk;\r\nstruct page *page = kiov->kiov_page;\r\nint offset = kiov->kiov_offset;\r\nint fragsize = kiov->kiov_len;\r\nint msgflg = MSG_DONTWAIT;\r\nCDEBUG(D_NET, "page %p + offset %x for %d\n",\r\npage, offset, kiov->kiov_len);\r\nif (!list_empty(&conn->ksnc_tx_queue) ||\r\nfragsize < tx->tx_resid)\r\nmsgflg |= MSG_MORE;\r\nif (sk->sk_prot->sendpage != NULL) {\r\nrc = sk->sk_prot->sendpage(sk, page,\r\noffset, fragsize, msgflg);\r\n} else {\r\nrc = cfs_tcp_sendpage(sk, page, offset, fragsize,\r\nmsgflg);\r\n}\r\n} else {\r\n#if SOCKNAL_SINGLE_FRAG_TX || !SOCKNAL_RISK_KMAP_DEADLOCK\r\nstruct iovec scratch;\r\nstruct iovec *scratchiov = &scratch;\r\nunsigned int niov = 1;\r\n#else\r\n#ifdef CONFIG_HIGHMEM\r\n#warning "XXX risk of kmap deadlock on multiple frags..."\r\n#endif\r\nstruct iovec *scratchiov = conn->ksnc_scheduler->kss_scratch_iov;\r\nunsigned int niov = tx->tx_nkiov;\r\n#endif\r\nstruct msghdr msg = {.msg_flags = MSG_DONTWAIT};\r\nint i;\r\nfor (nob = i = 0; i < niov; i++) {\r\nscratchiov[i].iov_base = kmap(kiov[i].kiov_page) +\r\nkiov[i].kiov_offset;\r\nnob += scratchiov[i].iov_len = kiov[i].kiov_len;\r\n}\r\nif (!list_empty(&conn->ksnc_tx_queue) ||\r\nnob < tx->tx_resid)\r\nmsg.msg_flags |= MSG_MORE;\r\nrc = kernel_sendmsg(sock, &msg, (struct kvec *)scratchiov, niov, nob);\r\nfor (i = 0; i < niov; i++)\r\nkunmap(kiov[i].kiov_page);\r\n}\r\nreturn rc;\r\n}\r\nvoid\r\nksocknal_lib_eager_ack (ksock_conn_t *conn)\r\n{\r\nint opt = 1;\r\nstruct socket *sock = conn->ksnc_sock;\r\nkernel_setsockopt(sock, SOL_TCP, TCP_QUICKACK,\r\n(char *)&opt, sizeof (opt));\r\n}\r\nint\r\nksocknal_lib_recv_iov (ksock_conn_t *conn)\r\n{\r\n#if SOCKNAL_SINGLE_FRAG_RX\r\nstruct iovec scratch;\r\nstruct iovec *scratchiov = &scratch;\r\nunsigned int niov = 1;\r\n#else\r\nstruct iovec *scratchiov = conn->ksnc_scheduler->kss_scratch_iov;\r\nunsigned int niov = conn->ksnc_rx_niov;\r\n#endif\r\nstruct iovec *iov = conn->ksnc_rx_iov;\r\nstruct msghdr msg = {\r\n.msg_flags = 0\r\n};\r\nint nob;\r\nint i;\r\nint rc;\r\nint fragnob;\r\nint sum;\r\n__u32 saved_csum;\r\nLASSERT (niov > 0);\r\nfor (nob = i = 0; i < niov; i++) {\r\nscratchiov[i] = iov[i];\r\nnob += scratchiov[i].iov_len;\r\n}\r\nLASSERT (nob <= conn->ksnc_rx_nob_wanted);\r\nrc = kernel_recvmsg(conn->ksnc_sock, &msg,\r\n(struct kvec *)scratchiov, niov, nob, MSG_DONTWAIT);\r\nsaved_csum = 0;\r\nif (conn->ksnc_proto == &ksocknal_protocol_v2x) {\r\nsaved_csum = conn->ksnc_msg.ksm_csum;\r\nconn->ksnc_msg.ksm_csum = 0;\r\n}\r\nif (saved_csum != 0) {\r\nfor (i = 0, sum = rc; sum > 0; i++, sum -= fragnob) {\r\nLASSERT (i < niov);\r\nfragnob = iov[i].iov_len;\r\nif (fragnob > sum)\r\nfragnob = sum;\r\nconn->ksnc_rx_csum = ksocknal_csum(conn->ksnc_rx_csum,\r\niov[i].iov_base, fragnob);\r\n}\r\nconn->ksnc_msg.ksm_csum = saved_csum;\r\n}\r\nreturn rc;\r\n}\r\nstatic void\r\nksocknal_lib_kiov_vunmap(void *addr)\r\n{\r\nif (addr == NULL)\r\nreturn;\r\nvunmap(addr);\r\n}\r\nstatic void *\r\nksocknal_lib_kiov_vmap(lnet_kiov_t *kiov, int niov,\r\nstruct iovec *iov, struct page **pages)\r\n{\r\nvoid *addr;\r\nint nob;\r\nint i;\r\nif (!*ksocknal_tunables.ksnd_zc_recv || pages == NULL)\r\nreturn NULL;\r\nLASSERT (niov <= LNET_MAX_IOV);\r\nif (niov < 2 ||\r\nniov < *ksocknal_tunables.ksnd_zc_recv_min_nfrags)\r\nreturn NULL;\r\nfor (nob = i = 0; i < niov; i++) {\r\nif ((kiov[i].kiov_offset != 0 && i > 0) ||\r\n(kiov[i].kiov_offset + kiov[i].kiov_len != PAGE_CACHE_SIZE && i < niov - 1))\r\nreturn NULL;\r\npages[i] = kiov[i].kiov_page;\r\nnob += kiov[i].kiov_len;\r\n}\r\naddr = vmap(pages, niov, VM_MAP, PAGE_KERNEL);\r\nif (addr == NULL)\r\nreturn NULL;\r\niov->iov_base = addr + kiov[0].kiov_offset;\r\niov->iov_len = nob;\r\nreturn addr;\r\n}\r\nint\r\nksocknal_lib_recv_kiov (ksock_conn_t *conn)\r\n{\r\n#if SOCKNAL_SINGLE_FRAG_RX || !SOCKNAL_RISK_KMAP_DEADLOCK\r\nstruct iovec scratch;\r\nstruct iovec *scratchiov = &scratch;\r\nstruct page **pages = NULL;\r\nunsigned int niov = 1;\r\n#else\r\n#ifdef CONFIG_HIGHMEM\r\n#warning "XXX risk of kmap deadlock on multiple frags..."\r\n#endif\r\nstruct iovec *scratchiov = conn->ksnc_scheduler->kss_scratch_iov;\r\nstruct page **pages = conn->ksnc_scheduler->kss_rx_scratch_pgs;\r\nunsigned int niov = conn->ksnc_rx_nkiov;\r\n#endif\r\nlnet_kiov_t *kiov = conn->ksnc_rx_kiov;\r\nstruct msghdr msg = {\r\n.msg_flags = 0\r\n};\r\nint nob;\r\nint i;\r\nint rc;\r\nvoid *base;\r\nvoid *addr;\r\nint sum;\r\nint fragnob;\r\nint n;\r\naddr = ksocknal_lib_kiov_vmap(kiov, niov, scratchiov, pages);\r\nif (addr != NULL) {\r\nnob = scratchiov[0].iov_len;\r\nn = 1;\r\n} else {\r\nfor (nob = i = 0; i < niov; i++) {\r\nnob += scratchiov[i].iov_len = kiov[i].kiov_len;\r\nscratchiov[i].iov_base = kmap(kiov[i].kiov_page) +\r\nkiov[i].kiov_offset;\r\n}\r\nn = niov;\r\n}\r\nLASSERT (nob <= conn->ksnc_rx_nob_wanted);\r\nrc = kernel_recvmsg(conn->ksnc_sock, &msg,\r\n(struct kvec *)scratchiov, n, nob, MSG_DONTWAIT);\r\nif (conn->ksnc_msg.ksm_csum != 0) {\r\nfor (i = 0, sum = rc; sum > 0; i++, sum -= fragnob) {\r\nLASSERT (i < niov);\r\nbase = kmap(kiov[i].kiov_page) + kiov[i].kiov_offset;\r\nfragnob = kiov[i].kiov_len;\r\nif (fragnob > sum)\r\nfragnob = sum;\r\nconn->ksnc_rx_csum = ksocknal_csum(conn->ksnc_rx_csum,\r\nbase, fragnob);\r\nkunmap(kiov[i].kiov_page);\r\n}\r\n}\r\nif (addr != NULL) {\r\nksocknal_lib_kiov_vunmap(addr);\r\n} else {\r\nfor (i = 0; i < niov; i++)\r\nkunmap(kiov[i].kiov_page);\r\n}\r\nreturn rc;\r\n}\r\nvoid\r\nksocknal_lib_csum_tx(ksock_tx_t *tx)\r\n{\r\nint i;\r\n__u32 csum;\r\nvoid *base;\r\nLASSERT(tx->tx_iov[0].iov_base == (void *)&tx->tx_msg);\r\nLASSERT(tx->tx_conn != NULL);\r\nLASSERT(tx->tx_conn->ksnc_proto == &ksocknal_protocol_v2x);\r\ntx->tx_msg.ksm_csum = 0;\r\ncsum = ksocknal_csum(~0, (void *)tx->tx_iov[0].iov_base,\r\ntx->tx_iov[0].iov_len);\r\nif (tx->tx_kiov != NULL) {\r\nfor (i = 0; i < tx->tx_nkiov; i++) {\r\nbase = kmap(tx->tx_kiov[i].kiov_page) +\r\ntx->tx_kiov[i].kiov_offset;\r\ncsum = ksocknal_csum(csum, base, tx->tx_kiov[i].kiov_len);\r\nkunmap(tx->tx_kiov[i].kiov_page);\r\n}\r\n} else {\r\nfor (i = 1; i < tx->tx_niov; i++)\r\ncsum = ksocknal_csum(csum, tx->tx_iov[i].iov_base,\r\ntx->tx_iov[i].iov_len);\r\n}\r\nif (*ksocknal_tunables.ksnd_inject_csum_error) {\r\ncsum++;\r\n*ksocknal_tunables.ksnd_inject_csum_error = 0;\r\n}\r\ntx->tx_msg.ksm_csum = csum;\r\n}\r\nint\r\nksocknal_lib_get_conn_tunables (ksock_conn_t *conn, int *txmem, int *rxmem, int *nagle)\r\n{\r\nstruct socket *sock = conn->ksnc_sock;\r\nint len;\r\nint rc;\r\nrc = ksocknal_connsock_addref(conn);\r\nif (rc != 0) {\r\nLASSERT (conn->ksnc_closing);\r\n*txmem = *rxmem = *nagle = 0;\r\nreturn -ESHUTDOWN;\r\n}\r\nrc = libcfs_sock_getbuf(sock, txmem, rxmem);\r\nif (rc == 0) {\r\nlen = sizeof(*nagle);\r\nrc = kernel_getsockopt(sock, SOL_TCP, TCP_NODELAY,\r\n(char *)nagle, &len);\r\n}\r\nksocknal_connsock_decref(conn);\r\nif (rc == 0)\r\n*nagle = !*nagle;\r\nelse\r\n*txmem = *rxmem = *nagle = 0;\r\nreturn rc;\r\n}\r\nint\r\nksocknal_lib_setup_sock (struct socket *sock)\r\n{\r\nint rc;\r\nint option;\r\nint keep_idle;\r\nint keep_intvl;\r\nint keep_count;\r\nint do_keepalive;\r\nstruct linger linger;\r\nsock->sk->sk_allocation = GFP_NOFS;\r\nlinger.l_onoff = 0;\r\nlinger.l_linger = 0;\r\nrc = kernel_setsockopt(sock, SOL_SOCKET, SO_LINGER,\r\n(char *)&linger, sizeof (linger));\r\nif (rc != 0) {\r\nCERROR ("Can't set SO_LINGER: %d\n", rc);\r\nreturn rc;\r\n}\r\noption = -1;\r\nrc = kernel_setsockopt(sock, SOL_TCP, TCP_LINGER2,\r\n(char *)&option, sizeof (option));\r\nif (rc != 0) {\r\nCERROR ("Can't set SO_LINGER2: %d\n", rc);\r\nreturn rc;\r\n}\r\nif (!*ksocknal_tunables.ksnd_nagle) {\r\noption = 1;\r\nrc = kernel_setsockopt(sock, SOL_TCP, TCP_NODELAY,\r\n(char *)&option, sizeof (option));\r\nif (rc != 0) {\r\nCERROR ("Can't disable nagle: %d\n", rc);\r\nreturn rc;\r\n}\r\n}\r\nrc = libcfs_sock_setbuf(sock,\r\n*ksocknal_tunables.ksnd_tx_buffer_size,\r\n*ksocknal_tunables.ksnd_rx_buffer_size);\r\nif (rc != 0) {\r\nCERROR ("Can't set buffer tx %d, rx %d buffers: %d\n",\r\n*ksocknal_tunables.ksnd_tx_buffer_size,\r\n*ksocknal_tunables.ksnd_rx_buffer_size, rc);\r\nreturn rc;\r\n}\r\nkeep_idle = *ksocknal_tunables.ksnd_keepalive_idle;\r\nkeep_count = *ksocknal_tunables.ksnd_keepalive_count;\r\nkeep_intvl = *ksocknal_tunables.ksnd_keepalive_intvl;\r\ndo_keepalive = (keep_idle > 0 && keep_count > 0 && keep_intvl > 0);\r\noption = (do_keepalive ? 1 : 0);\r\nrc = kernel_setsockopt(sock, SOL_SOCKET, SO_KEEPALIVE,\r\n(char *)&option, sizeof (option));\r\nif (rc != 0) {\r\nCERROR ("Can't set SO_KEEPALIVE: %d\n", rc);\r\nreturn rc;\r\n}\r\nif (!do_keepalive)\r\nreturn 0;\r\nrc = kernel_setsockopt(sock, SOL_TCP, TCP_KEEPIDLE,\r\n(char *)&keep_idle, sizeof (keep_idle));\r\nif (rc != 0) {\r\nCERROR ("Can't set TCP_KEEPIDLE: %d\n", rc);\r\nreturn rc;\r\n}\r\nrc = kernel_setsockopt(sock, SOL_TCP, TCP_KEEPINTVL,\r\n(char *)&keep_intvl, sizeof (keep_intvl));\r\nif (rc != 0) {\r\nCERROR ("Can't set TCP_KEEPINTVL: %d\n", rc);\r\nreturn rc;\r\n}\r\nrc = kernel_setsockopt(sock, SOL_TCP, TCP_KEEPCNT,\r\n(char *)&keep_count, sizeof (keep_count));\r\nif (rc != 0) {\r\nCERROR ("Can't set TCP_KEEPCNT: %d\n", rc);\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nvoid\r\nksocknal_lib_push_conn (ksock_conn_t *conn)\r\n{\r\nstruct sock *sk;\r\nstruct tcp_sock *tp;\r\nint nonagle;\r\nint val = 1;\r\nint rc;\r\nrc = ksocknal_connsock_addref(conn);\r\nif (rc != 0)\r\nreturn;\r\nsk = conn->ksnc_sock->sk;\r\ntp = tcp_sk(sk);\r\nlock_sock (sk);\r\nnonagle = tp->nonagle;\r\ntp->nonagle = 1;\r\nrelease_sock (sk);\r\nrc = kernel_setsockopt(conn->ksnc_sock, SOL_TCP, TCP_NODELAY,\r\n(char *)&val, sizeof (val));\r\nLASSERT (rc == 0);\r\nlock_sock (sk);\r\ntp->nonagle = nonagle;\r\nrelease_sock (sk);\r\nksocknal_connsock_decref(conn);\r\n}\r\nstatic void\r\nksocknal_data_ready (struct sock *sk)\r\n{\r\nksock_conn_t *conn;\r\nLASSERT(!in_irq());\r\nread_lock(&ksocknal_data.ksnd_global_lock);\r\nconn = sk->sk_user_data;\r\nif (conn == NULL) {\r\nLASSERT (sk->sk_data_ready != &ksocknal_data_ready);\r\nsk->sk_data_ready (sk);\r\n} else\r\nksocknal_read_callback(conn);\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\n}\r\nstatic void\r\nksocknal_write_space (struct sock *sk)\r\n{\r\nksock_conn_t *conn;\r\nint wspace;\r\nint min_wpace;\r\nLASSERT(!in_irq());\r\nread_lock(&ksocknal_data.ksnd_global_lock);\r\nconn = sk->sk_user_data;\r\nwspace = SOCKNAL_WSPACE(sk);\r\nmin_wpace = SOCKNAL_MIN_WSPACE(sk);\r\nCDEBUG(D_NET, "sk %p wspace %d low water %d conn %p%s%s%s\n",\r\nsk, wspace, min_wpace, conn,\r\n(conn == NULL) ? "" : (conn->ksnc_tx_ready ?\r\n" ready" : " blocked"),\r\n(conn == NULL) ? "" : (conn->ksnc_tx_scheduled ?\r\n" scheduled" : " idle"),\r\n(conn == NULL) ? "" : (list_empty (&conn->ksnc_tx_queue) ?\r\n" empty" : " queued"));\r\nif (conn == NULL) {\r\nLASSERT (sk->sk_write_space != &ksocknal_write_space);\r\nsk->sk_write_space (sk);\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\nreturn;\r\n}\r\nif (wspace >= min_wpace) {\r\nksocknal_write_callback(conn);\r\nclear_bit (SOCK_NOSPACE, &sk->sk_socket->flags);\r\n}\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\n}\r\nvoid\r\nksocknal_lib_save_callback(struct socket *sock, ksock_conn_t *conn)\r\n{\r\nconn->ksnc_saved_data_ready = sock->sk->sk_data_ready;\r\nconn->ksnc_saved_write_space = sock->sk->sk_write_space;\r\n}\r\nvoid\r\nksocknal_lib_set_callback(struct socket *sock, ksock_conn_t *conn)\r\n{\r\nsock->sk->sk_user_data = conn;\r\nsock->sk->sk_data_ready = ksocknal_data_ready;\r\nsock->sk->sk_write_space = ksocknal_write_space;\r\nreturn;\r\n}\r\nvoid\r\nksocknal_lib_reset_callback(struct socket *sock, ksock_conn_t *conn)\r\n{\r\nsock->sk->sk_data_ready = conn->ksnc_saved_data_ready;\r\nsock->sk->sk_write_space = conn->ksnc_saved_write_space;\r\nsock->sk->sk_user_data = NULL;\r\nreturn ;\r\n}\r\nint\r\nksocknal_lib_memory_pressure(ksock_conn_t *conn)\r\n{\r\nint rc = 0;\r\nksock_sched_t *sched;\r\nsched = conn->ksnc_scheduler;\r\nspin_lock_bh(&sched->kss_lock);\r\nif (!test_bit(SOCK_NOSPACE, &conn->ksnc_sock->flags) &&\r\n!conn->ksnc_tx_ready) {\r\nrc = -ENOMEM;\r\n}\r\nspin_unlock_bh(&sched->kss_lock);\r\nreturn rc;\r\n}
