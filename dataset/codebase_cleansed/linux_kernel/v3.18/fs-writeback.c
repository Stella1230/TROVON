int writeback_in_progress(struct backing_dev_info *bdi)\r\n{\r\nreturn test_bit(BDI_writeback_running, &bdi->state);\r\n}\r\nstatic inline struct backing_dev_info *inode_to_bdi(struct inode *inode)\r\n{\r\nstruct super_block *sb = inode->i_sb;\r\nif (sb_is_blkdev_sb(sb))\r\nreturn inode->i_mapping->backing_dev_info;\r\nreturn sb->s_bdi;\r\n}\r\nstatic inline struct inode *wb_inode(struct list_head *head)\r\n{\r\nreturn list_entry(head, struct inode, i_wb_list);\r\n}\r\nstatic void bdi_wakeup_thread(struct backing_dev_info *bdi)\r\n{\r\nspin_lock_bh(&bdi->wb_lock);\r\nif (test_bit(BDI_registered, &bdi->state))\r\nmod_delayed_work(bdi_wq, &bdi->wb.dwork, 0);\r\nspin_unlock_bh(&bdi->wb_lock);\r\n}\r\nstatic void bdi_queue_work(struct backing_dev_info *bdi,\r\nstruct wb_writeback_work *work)\r\n{\r\ntrace_writeback_queue(bdi, work);\r\nspin_lock_bh(&bdi->wb_lock);\r\nif (!test_bit(BDI_registered, &bdi->state)) {\r\nif (work->done)\r\ncomplete(work->done);\r\ngoto out_unlock;\r\n}\r\nlist_add_tail(&work->list, &bdi->work_list);\r\nmod_delayed_work(bdi_wq, &bdi->wb.dwork, 0);\r\nout_unlock:\r\nspin_unlock_bh(&bdi->wb_lock);\r\n}\r\nstatic void\r\n__bdi_start_writeback(struct backing_dev_info *bdi, long nr_pages,\r\nbool range_cyclic, enum wb_reason reason)\r\n{\r\nstruct wb_writeback_work *work;\r\nwork = kzalloc(sizeof(*work), GFP_ATOMIC);\r\nif (!work) {\r\ntrace_writeback_nowork(bdi);\r\nbdi_wakeup_thread(bdi);\r\nreturn;\r\n}\r\nwork->sync_mode = WB_SYNC_NONE;\r\nwork->nr_pages = nr_pages;\r\nwork->range_cyclic = range_cyclic;\r\nwork->reason = reason;\r\nbdi_queue_work(bdi, work);\r\n}\r\nvoid bdi_start_writeback(struct backing_dev_info *bdi, long nr_pages,\r\nenum wb_reason reason)\r\n{\r\n__bdi_start_writeback(bdi, nr_pages, true, reason);\r\n}\r\nvoid bdi_start_background_writeback(struct backing_dev_info *bdi)\r\n{\r\ntrace_writeback_wake_background(bdi);\r\nbdi_wakeup_thread(bdi);\r\n}\r\nvoid inode_wb_list_del(struct inode *inode)\r\n{\r\nstruct backing_dev_info *bdi = inode_to_bdi(inode);\r\nspin_lock(&bdi->wb.list_lock);\r\nlist_del_init(&inode->i_wb_list);\r\nspin_unlock(&bdi->wb.list_lock);\r\n}\r\nstatic void redirty_tail(struct inode *inode, struct bdi_writeback *wb)\r\n{\r\nassert_spin_locked(&wb->list_lock);\r\nif (!list_empty(&wb->b_dirty)) {\r\nstruct inode *tail;\r\ntail = wb_inode(wb->b_dirty.next);\r\nif (time_before(inode->dirtied_when, tail->dirtied_when))\r\ninode->dirtied_when = jiffies;\r\n}\r\nlist_move(&inode->i_wb_list, &wb->b_dirty);\r\n}\r\nstatic void requeue_io(struct inode *inode, struct bdi_writeback *wb)\r\n{\r\nassert_spin_locked(&wb->list_lock);\r\nlist_move(&inode->i_wb_list, &wb->b_more_io);\r\n}\r\nstatic void inode_sync_complete(struct inode *inode)\r\n{\r\ninode->i_state &= ~I_SYNC;\r\ninode_add_lru(inode);\r\nsmp_mb();\r\nwake_up_bit(&inode->i_state, __I_SYNC);\r\n}\r\nstatic bool inode_dirtied_after(struct inode *inode, unsigned long t)\r\n{\r\nbool ret = time_after(inode->dirtied_when, t);\r\n#ifndef CONFIG_64BIT\r\nret = ret && time_before_eq(inode->dirtied_when, jiffies);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int move_expired_inodes(struct list_head *delaying_queue,\r\nstruct list_head *dispatch_queue,\r\nstruct wb_writeback_work *work)\r\n{\r\nLIST_HEAD(tmp);\r\nstruct list_head *pos, *node;\r\nstruct super_block *sb = NULL;\r\nstruct inode *inode;\r\nint do_sb_sort = 0;\r\nint moved = 0;\r\nwhile (!list_empty(delaying_queue)) {\r\ninode = wb_inode(delaying_queue->prev);\r\nif (work->older_than_this &&\r\ninode_dirtied_after(inode, *work->older_than_this))\r\nbreak;\r\nlist_move(&inode->i_wb_list, &tmp);\r\nmoved++;\r\nif (sb_is_blkdev_sb(inode->i_sb))\r\ncontinue;\r\nif (sb && sb != inode->i_sb)\r\ndo_sb_sort = 1;\r\nsb = inode->i_sb;\r\n}\r\nif (!do_sb_sort) {\r\nlist_splice(&tmp, dispatch_queue);\r\ngoto out;\r\n}\r\nwhile (!list_empty(&tmp)) {\r\nsb = wb_inode(tmp.prev)->i_sb;\r\nlist_for_each_prev_safe(pos, node, &tmp) {\r\ninode = wb_inode(pos);\r\nif (inode->i_sb == sb)\r\nlist_move(&inode->i_wb_list, dispatch_queue);\r\n}\r\n}\r\nout:\r\nreturn moved;\r\n}\r\nstatic void queue_io(struct bdi_writeback *wb, struct wb_writeback_work *work)\r\n{\r\nint moved;\r\nassert_spin_locked(&wb->list_lock);\r\nlist_splice_init(&wb->b_more_io, &wb->b_io);\r\nmoved = move_expired_inodes(&wb->b_dirty, &wb->b_io, work);\r\ntrace_writeback_queue_io(wb, work, moved);\r\n}\r\nstatic int write_inode(struct inode *inode, struct writeback_control *wbc)\r\n{\r\nint ret;\r\nif (inode->i_sb->s_op->write_inode && !is_bad_inode(inode)) {\r\ntrace_writeback_write_inode_start(inode, wbc);\r\nret = inode->i_sb->s_op->write_inode(inode, wbc);\r\ntrace_writeback_write_inode(inode, wbc);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __inode_wait_for_writeback(struct inode *inode)\r\n__releases(inode->i_lock)\r\n__acquires(inode->i_lock)\r\n{\r\nDEFINE_WAIT_BIT(wq, &inode->i_state, __I_SYNC);\r\nwait_queue_head_t *wqh;\r\nwqh = bit_waitqueue(&inode->i_state, __I_SYNC);\r\nwhile (inode->i_state & I_SYNC) {\r\nspin_unlock(&inode->i_lock);\r\n__wait_on_bit(wqh, &wq, bit_wait,\r\nTASK_UNINTERRUPTIBLE);\r\nspin_lock(&inode->i_lock);\r\n}\r\n}\r\nvoid inode_wait_for_writeback(struct inode *inode)\r\n{\r\nspin_lock(&inode->i_lock);\r\n__inode_wait_for_writeback(inode);\r\nspin_unlock(&inode->i_lock);\r\n}\r\nstatic void inode_sleep_on_writeback(struct inode *inode)\r\n__releases(inode->i_lock)\r\n{\r\nDEFINE_WAIT(wait);\r\nwait_queue_head_t *wqh = bit_waitqueue(&inode->i_state, __I_SYNC);\r\nint sleep;\r\nprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\r\nsleep = inode->i_state & I_SYNC;\r\nspin_unlock(&inode->i_lock);\r\nif (sleep)\r\nschedule();\r\nfinish_wait(wqh, &wait);\r\n}\r\nstatic void requeue_inode(struct inode *inode, struct bdi_writeback *wb,\r\nstruct writeback_control *wbc)\r\n{\r\nif (inode->i_state & I_FREEING)\r\nreturn;\r\nif ((inode->i_state & I_DIRTY) &&\r\n(wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages))\r\ninode->dirtied_when = jiffies;\r\nif (wbc->pages_skipped) {\r\nredirty_tail(inode, wb);\r\nreturn;\r\n}\r\nif (mapping_tagged(inode->i_mapping, PAGECACHE_TAG_DIRTY)) {\r\nif (wbc->nr_to_write <= 0) {\r\nrequeue_io(inode, wb);\r\n} else {\r\nredirty_tail(inode, wb);\r\n}\r\n} else if (inode->i_state & I_DIRTY) {\r\nredirty_tail(inode, wb);\r\n} else {\r\nlist_del_init(&inode->i_wb_list);\r\n}\r\n}\r\nstatic int\r\n__writeback_single_inode(struct inode *inode, struct writeback_control *wbc)\r\n{\r\nstruct address_space *mapping = inode->i_mapping;\r\nlong nr_to_write = wbc->nr_to_write;\r\nunsigned dirty;\r\nint ret;\r\nWARN_ON(!(inode->i_state & I_SYNC));\r\ntrace_writeback_single_inode_start(inode, wbc, nr_to_write);\r\nret = do_writepages(mapping, wbc);\r\nif (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync) {\r\nint err = filemap_fdatawait(mapping);\r\nif (ret == 0)\r\nret = err;\r\n}\r\nspin_lock(&inode->i_lock);\r\nif (!mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\r\ninode->i_state &= ~I_DIRTY_PAGES;\r\ndirty = inode->i_state & I_DIRTY;\r\ninode->i_state &= ~(I_DIRTY_SYNC | I_DIRTY_DATASYNC);\r\nspin_unlock(&inode->i_lock);\r\nif (dirty & (I_DIRTY_SYNC | I_DIRTY_DATASYNC)) {\r\nint err = write_inode(inode, wbc);\r\nif (ret == 0)\r\nret = err;\r\n}\r\ntrace_writeback_single_inode(inode, wbc, nr_to_write);\r\nreturn ret;\r\n}\r\nstatic int\r\nwriteback_single_inode(struct inode *inode, struct bdi_writeback *wb,\r\nstruct writeback_control *wbc)\r\n{\r\nint ret = 0;\r\nspin_lock(&inode->i_lock);\r\nif (!atomic_read(&inode->i_count))\r\nWARN_ON(!(inode->i_state & (I_WILL_FREE|I_FREEING)));\r\nelse\r\nWARN_ON(inode->i_state & I_WILL_FREE);\r\nif (inode->i_state & I_SYNC) {\r\nif (wbc->sync_mode != WB_SYNC_ALL)\r\ngoto out;\r\n__inode_wait_for_writeback(inode);\r\n}\r\nWARN_ON(inode->i_state & I_SYNC);\r\nif (!(inode->i_state & I_DIRTY) &&\r\n(wbc->sync_mode != WB_SYNC_ALL ||\r\n!mapping_tagged(inode->i_mapping, PAGECACHE_TAG_WRITEBACK)))\r\ngoto out;\r\ninode->i_state |= I_SYNC;\r\nspin_unlock(&inode->i_lock);\r\nret = __writeback_single_inode(inode, wbc);\r\nspin_lock(&wb->list_lock);\r\nspin_lock(&inode->i_lock);\r\nif (!(inode->i_state & I_DIRTY))\r\nlist_del_init(&inode->i_wb_list);\r\nspin_unlock(&wb->list_lock);\r\ninode_sync_complete(inode);\r\nout:\r\nspin_unlock(&inode->i_lock);\r\nreturn ret;\r\n}\r\nstatic long writeback_chunk_size(struct backing_dev_info *bdi,\r\nstruct wb_writeback_work *work)\r\n{\r\nlong pages;\r\nif (work->sync_mode == WB_SYNC_ALL || work->tagged_writepages)\r\npages = LONG_MAX;\r\nelse {\r\npages = min(bdi->avg_write_bandwidth / 2,\r\nglobal_dirty_limit / DIRTY_SCOPE);\r\npages = min(pages, work->nr_pages);\r\npages = round_down(pages + MIN_WRITEBACK_PAGES,\r\nMIN_WRITEBACK_PAGES);\r\n}\r\nreturn pages;\r\n}\r\nstatic long writeback_sb_inodes(struct super_block *sb,\r\nstruct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nstruct writeback_control wbc = {\r\n.sync_mode = work->sync_mode,\r\n.tagged_writepages = work->tagged_writepages,\r\n.for_kupdate = work->for_kupdate,\r\n.for_background = work->for_background,\r\n.for_sync = work->for_sync,\r\n.range_cyclic = work->range_cyclic,\r\n.range_start = 0,\r\n.range_end = LLONG_MAX,\r\n};\r\nunsigned long start_time = jiffies;\r\nlong write_chunk;\r\nlong wrote = 0;\r\nwhile (!list_empty(&wb->b_io)) {\r\nstruct inode *inode = wb_inode(wb->b_io.prev);\r\nif (inode->i_sb != sb) {\r\nif (work->sb) {\r\nredirty_tail(inode, wb);\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nspin_lock(&inode->i_lock);\r\nif (inode->i_state & (I_NEW | I_FREEING | I_WILL_FREE)) {\r\nspin_unlock(&inode->i_lock);\r\nredirty_tail(inode, wb);\r\ncontinue;\r\n}\r\nif ((inode->i_state & I_SYNC) && wbc.sync_mode != WB_SYNC_ALL) {\r\nspin_unlock(&inode->i_lock);\r\nrequeue_io(inode, wb);\r\ntrace_writeback_sb_inodes_requeue(inode);\r\ncontinue;\r\n}\r\nspin_unlock(&wb->list_lock);\r\nif (inode->i_state & I_SYNC) {\r\ninode_sleep_on_writeback(inode);\r\nspin_lock(&wb->list_lock);\r\ncontinue;\r\n}\r\ninode->i_state |= I_SYNC;\r\nspin_unlock(&inode->i_lock);\r\nwrite_chunk = writeback_chunk_size(wb->bdi, work);\r\nwbc.nr_to_write = write_chunk;\r\nwbc.pages_skipped = 0;\r\n__writeback_single_inode(inode, &wbc);\r\nwork->nr_pages -= write_chunk - wbc.nr_to_write;\r\nwrote += write_chunk - wbc.nr_to_write;\r\nspin_lock(&wb->list_lock);\r\nspin_lock(&inode->i_lock);\r\nif (!(inode->i_state & I_DIRTY))\r\nwrote++;\r\nrequeue_inode(inode, wb, &wbc);\r\ninode_sync_complete(inode);\r\nspin_unlock(&inode->i_lock);\r\ncond_resched_lock(&wb->list_lock);\r\nif (wrote) {\r\nif (time_is_before_jiffies(start_time + HZ / 10UL))\r\nbreak;\r\nif (work->nr_pages <= 0)\r\nbreak;\r\n}\r\n}\r\nreturn wrote;\r\n}\r\nstatic long __writeback_inodes_wb(struct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nunsigned long start_time = jiffies;\r\nlong wrote = 0;\r\nwhile (!list_empty(&wb->b_io)) {\r\nstruct inode *inode = wb_inode(wb->b_io.prev);\r\nstruct super_block *sb = inode->i_sb;\r\nif (!grab_super_passive(sb)) {\r\nredirty_tail(inode, wb);\r\ncontinue;\r\n}\r\nwrote += writeback_sb_inodes(sb, wb, work);\r\ndrop_super(sb);\r\nif (wrote) {\r\nif (time_is_before_jiffies(start_time + HZ / 10UL))\r\nbreak;\r\nif (work->nr_pages <= 0)\r\nbreak;\r\n}\r\n}\r\nreturn wrote;\r\n}\r\nstatic long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,\r\nenum wb_reason reason)\r\n{\r\nstruct wb_writeback_work work = {\r\n.nr_pages = nr_pages,\r\n.sync_mode = WB_SYNC_NONE,\r\n.range_cyclic = 1,\r\n.reason = reason,\r\n};\r\nspin_lock(&wb->list_lock);\r\nif (list_empty(&wb->b_io))\r\nqueue_io(wb, &work);\r\n__writeback_inodes_wb(wb, &work);\r\nspin_unlock(&wb->list_lock);\r\nreturn nr_pages - work.nr_pages;\r\n}\r\nstatic bool over_bground_thresh(struct backing_dev_info *bdi)\r\n{\r\nunsigned long background_thresh, dirty_thresh;\r\nglobal_dirty_limits(&background_thresh, &dirty_thresh);\r\nif (global_page_state(NR_FILE_DIRTY) +\r\nglobal_page_state(NR_UNSTABLE_NFS) > background_thresh)\r\nreturn true;\r\nif (bdi_stat(bdi, BDI_RECLAIMABLE) >\r\nbdi_dirty_limit(bdi, background_thresh))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void wb_update_bandwidth(struct bdi_writeback *wb,\r\nunsigned long start_time)\r\n{\r\n__bdi_update_bandwidth(wb->bdi, 0, 0, 0, 0, 0, start_time);\r\n}\r\nstatic long wb_writeback(struct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nunsigned long wb_start = jiffies;\r\nlong nr_pages = work->nr_pages;\r\nunsigned long oldest_jif;\r\nstruct inode *inode;\r\nlong progress;\r\noldest_jif = jiffies;\r\nwork->older_than_this = &oldest_jif;\r\nspin_lock(&wb->list_lock);\r\nfor (;;) {\r\nif (work->nr_pages <= 0)\r\nbreak;\r\nif ((work->for_background || work->for_kupdate) &&\r\n!list_empty(&wb->bdi->work_list))\r\nbreak;\r\nif (work->for_background && !over_bground_thresh(wb->bdi))\r\nbreak;\r\nif (work->for_kupdate) {\r\noldest_jif = jiffies -\r\nmsecs_to_jiffies(dirty_expire_interval * 10);\r\n} else if (work->for_background)\r\noldest_jif = jiffies;\r\ntrace_writeback_start(wb->bdi, work);\r\nif (list_empty(&wb->b_io))\r\nqueue_io(wb, work);\r\nif (work->sb)\r\nprogress = writeback_sb_inodes(work->sb, wb, work);\r\nelse\r\nprogress = __writeback_inodes_wb(wb, work);\r\ntrace_writeback_written(wb->bdi, work);\r\nwb_update_bandwidth(wb, wb_start);\r\nif (progress)\r\ncontinue;\r\nif (list_empty(&wb->b_more_io))\r\nbreak;\r\nif (!list_empty(&wb->b_more_io)) {\r\ntrace_writeback_wait(wb->bdi, work);\r\ninode = wb_inode(wb->b_more_io.prev);\r\nspin_lock(&inode->i_lock);\r\nspin_unlock(&wb->list_lock);\r\ninode_sleep_on_writeback(inode);\r\nspin_lock(&wb->list_lock);\r\n}\r\n}\r\nspin_unlock(&wb->list_lock);\r\nreturn nr_pages - work->nr_pages;\r\n}\r\nstatic struct wb_writeback_work *\r\nget_next_work_item(struct backing_dev_info *bdi)\r\n{\r\nstruct wb_writeback_work *work = NULL;\r\nspin_lock_bh(&bdi->wb_lock);\r\nif (!list_empty(&bdi->work_list)) {\r\nwork = list_entry(bdi->work_list.next,\r\nstruct wb_writeback_work, list);\r\nlist_del_init(&work->list);\r\n}\r\nspin_unlock_bh(&bdi->wb_lock);\r\nreturn work;\r\n}\r\nstatic unsigned long get_nr_dirty_pages(void)\r\n{\r\nreturn global_page_state(NR_FILE_DIRTY) +\r\nglobal_page_state(NR_UNSTABLE_NFS) +\r\nget_nr_dirty_inodes();\r\n}\r\nstatic long wb_check_background_flush(struct bdi_writeback *wb)\r\n{\r\nif (over_bground_thresh(wb->bdi)) {\r\nstruct wb_writeback_work work = {\r\n.nr_pages = LONG_MAX,\r\n.sync_mode = WB_SYNC_NONE,\r\n.for_background = 1,\r\n.range_cyclic = 1,\r\n.reason = WB_REASON_BACKGROUND,\r\n};\r\nreturn wb_writeback(wb, &work);\r\n}\r\nreturn 0;\r\n}\r\nstatic long wb_check_old_data_flush(struct bdi_writeback *wb)\r\n{\r\nunsigned long expired;\r\nlong nr_pages;\r\nif (!dirty_writeback_interval)\r\nreturn 0;\r\nexpired = wb->last_old_flush +\r\nmsecs_to_jiffies(dirty_writeback_interval * 10);\r\nif (time_before(jiffies, expired))\r\nreturn 0;\r\nwb->last_old_flush = jiffies;\r\nnr_pages = get_nr_dirty_pages();\r\nif (nr_pages) {\r\nstruct wb_writeback_work work = {\r\n.nr_pages = nr_pages,\r\n.sync_mode = WB_SYNC_NONE,\r\n.for_kupdate = 1,\r\n.range_cyclic = 1,\r\n.reason = WB_REASON_PERIODIC,\r\n};\r\nreturn wb_writeback(wb, &work);\r\n}\r\nreturn 0;\r\n}\r\nstatic long wb_do_writeback(struct bdi_writeback *wb)\r\n{\r\nstruct backing_dev_info *bdi = wb->bdi;\r\nstruct wb_writeback_work *work;\r\nlong wrote = 0;\r\nset_bit(BDI_writeback_running, &wb->bdi->state);\r\nwhile ((work = get_next_work_item(bdi)) != NULL) {\r\ntrace_writeback_exec(bdi, work);\r\nwrote += wb_writeback(wb, work);\r\nif (work->done)\r\ncomplete(work->done);\r\nelse\r\nkfree(work);\r\n}\r\nwrote += wb_check_old_data_flush(wb);\r\nwrote += wb_check_background_flush(wb);\r\nclear_bit(BDI_writeback_running, &wb->bdi->state);\r\nreturn wrote;\r\n}\r\nvoid bdi_writeback_workfn(struct work_struct *work)\r\n{\r\nstruct bdi_writeback *wb = container_of(to_delayed_work(work),\r\nstruct bdi_writeback, dwork);\r\nstruct backing_dev_info *bdi = wb->bdi;\r\nlong pages_written;\r\nset_worker_desc("flush-%s", dev_name(bdi->dev));\r\ncurrent->flags |= PF_SWAPWRITE;\r\nif (likely(!current_is_workqueue_rescuer() ||\r\n!test_bit(BDI_registered, &bdi->state))) {\r\ndo {\r\npages_written = wb_do_writeback(wb);\r\ntrace_writeback_pages_written(pages_written);\r\n} while (!list_empty(&bdi->work_list));\r\n} else {\r\npages_written = writeback_inodes_wb(&bdi->wb, 1024,\r\nWB_REASON_FORKER_THREAD);\r\ntrace_writeback_pages_written(pages_written);\r\n}\r\nif (!list_empty(&bdi->work_list))\r\nmod_delayed_work(bdi_wq, &wb->dwork, 0);\r\nelse if (wb_has_dirty_io(wb) && dirty_writeback_interval)\r\nbdi_wakeup_thread_delayed(bdi);\r\ncurrent->flags &= ~PF_SWAPWRITE;\r\n}\r\nvoid wakeup_flusher_threads(long nr_pages, enum wb_reason reason)\r\n{\r\nstruct backing_dev_info *bdi;\r\nif (!nr_pages)\r\nnr_pages = get_nr_dirty_pages();\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list) {\r\nif (!bdi_has_dirty_io(bdi))\r\ncontinue;\r\n__bdi_start_writeback(bdi, nr_pages, false, reason);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic noinline void block_dump___mark_inode_dirty(struct inode *inode)\r\n{\r\nif (inode->i_ino || strcmp(inode->i_sb->s_id, "bdev")) {\r\nstruct dentry *dentry;\r\nconst char *name = "?";\r\ndentry = d_find_alias(inode);\r\nif (dentry) {\r\nspin_lock(&dentry->d_lock);\r\nname = (const char *) dentry->d_name.name;\r\n}\r\nprintk(KERN_DEBUG\r\n"%s(%d): dirtied inode %lu (%s) on %s\n",\r\ncurrent->comm, task_pid_nr(current), inode->i_ino,\r\nname, inode->i_sb->s_id);\r\nif (dentry) {\r\nspin_unlock(&dentry->d_lock);\r\ndput(dentry);\r\n}\r\n}\r\n}\r\nvoid __mark_inode_dirty(struct inode *inode, int flags)\r\n{\r\nstruct super_block *sb = inode->i_sb;\r\nstruct backing_dev_info *bdi = NULL;\r\nif (flags & (I_DIRTY_SYNC | I_DIRTY_DATASYNC)) {\r\ntrace_writeback_dirty_inode_start(inode, flags);\r\nif (sb->s_op->dirty_inode)\r\nsb->s_op->dirty_inode(inode, flags);\r\ntrace_writeback_dirty_inode(inode, flags);\r\n}\r\nsmp_mb();\r\nif ((inode->i_state & flags) == flags)\r\nreturn;\r\nif (unlikely(block_dump))\r\nblock_dump___mark_inode_dirty(inode);\r\nspin_lock(&inode->i_lock);\r\nif ((inode->i_state & flags) != flags) {\r\nconst int was_dirty = inode->i_state & I_DIRTY;\r\ninode->i_state |= flags;\r\nif (inode->i_state & I_SYNC)\r\ngoto out_unlock_inode;\r\nif (!S_ISBLK(inode->i_mode)) {\r\nif (inode_unhashed(inode))\r\ngoto out_unlock_inode;\r\n}\r\nif (inode->i_state & I_FREEING)\r\ngoto out_unlock_inode;\r\nif (!was_dirty) {\r\nbool wakeup_bdi = false;\r\nbdi = inode_to_bdi(inode);\r\nspin_unlock(&inode->i_lock);\r\nspin_lock(&bdi->wb.list_lock);\r\nif (bdi_cap_writeback_dirty(bdi)) {\r\nWARN(!test_bit(BDI_registered, &bdi->state),\r\n"bdi-%s not registered\n", bdi->name);\r\nif (!wb_has_dirty_io(&bdi->wb))\r\nwakeup_bdi = true;\r\n}\r\ninode->dirtied_when = jiffies;\r\nlist_move(&inode->i_wb_list, &bdi->wb.b_dirty);\r\nspin_unlock(&bdi->wb.list_lock);\r\nif (wakeup_bdi)\r\nbdi_wakeup_thread_delayed(bdi);\r\nreturn;\r\n}\r\n}\r\nout_unlock_inode:\r\nspin_unlock(&inode->i_lock);\r\n}\r\nstatic void wait_sb_inodes(struct super_block *sb)\r\n{\r\nstruct inode *inode, *old_inode = NULL;\r\nWARN_ON(!rwsem_is_locked(&sb->s_umount));\r\nspin_lock(&inode_sb_list_lock);\r\nlist_for_each_entry(inode, &sb->s_inodes, i_sb_list) {\r\nstruct address_space *mapping = inode->i_mapping;\r\nspin_lock(&inode->i_lock);\r\nif ((inode->i_state & (I_FREEING|I_WILL_FREE|I_NEW)) ||\r\n(mapping->nrpages == 0)) {\r\nspin_unlock(&inode->i_lock);\r\ncontinue;\r\n}\r\n__iget(inode);\r\nspin_unlock(&inode->i_lock);\r\nspin_unlock(&inode_sb_list_lock);\r\niput(old_inode);\r\nold_inode = inode;\r\nfilemap_fdatawait(mapping);\r\ncond_resched();\r\nspin_lock(&inode_sb_list_lock);\r\n}\r\nspin_unlock(&inode_sb_list_lock);\r\niput(old_inode);\r\n}\r\nvoid writeback_inodes_sb_nr(struct super_block *sb,\r\nunsigned long nr,\r\nenum wb_reason reason)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(done);\r\nstruct wb_writeback_work work = {\r\n.sb = sb,\r\n.sync_mode = WB_SYNC_NONE,\r\n.tagged_writepages = 1,\r\n.done = &done,\r\n.nr_pages = nr,\r\n.reason = reason,\r\n};\r\nif (sb->s_bdi == &noop_backing_dev_info)\r\nreturn;\r\nWARN_ON(!rwsem_is_locked(&sb->s_umount));\r\nbdi_queue_work(sb->s_bdi, &work);\r\nwait_for_completion(&done);\r\n}\r\nvoid writeback_inodes_sb(struct super_block *sb, enum wb_reason reason)\r\n{\r\nreturn writeback_inodes_sb_nr(sb, get_nr_dirty_pages(), reason);\r\n}\r\nint try_to_writeback_inodes_sb_nr(struct super_block *sb,\r\nunsigned long nr,\r\nenum wb_reason reason)\r\n{\r\nif (writeback_in_progress(sb->s_bdi))\r\nreturn 1;\r\nif (!down_read_trylock(&sb->s_umount))\r\nreturn 0;\r\nwriteback_inodes_sb_nr(sb, nr, reason);\r\nup_read(&sb->s_umount);\r\nreturn 1;\r\n}\r\nint try_to_writeback_inodes_sb(struct super_block *sb, enum wb_reason reason)\r\n{\r\nreturn try_to_writeback_inodes_sb_nr(sb, get_nr_dirty_pages(), reason);\r\n}\r\nvoid sync_inodes_sb(struct super_block *sb)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(done);\r\nstruct wb_writeback_work work = {\r\n.sb = sb,\r\n.sync_mode = WB_SYNC_ALL,\r\n.nr_pages = LONG_MAX,\r\n.range_cyclic = 0,\r\n.done = &done,\r\n.reason = WB_REASON_SYNC,\r\n.for_sync = 1,\r\n};\r\nif (sb->s_bdi == &noop_backing_dev_info)\r\nreturn;\r\nWARN_ON(!rwsem_is_locked(&sb->s_umount));\r\nbdi_queue_work(sb->s_bdi, &work);\r\nwait_for_completion(&done);\r\nwait_sb_inodes(sb);\r\n}\r\nint write_inode_now(struct inode *inode, int sync)\r\n{\r\nstruct bdi_writeback *wb = &inode_to_bdi(inode)->wb;\r\nstruct writeback_control wbc = {\r\n.nr_to_write = LONG_MAX,\r\n.sync_mode = sync ? WB_SYNC_ALL : WB_SYNC_NONE,\r\n.range_start = 0,\r\n.range_end = LLONG_MAX,\r\n};\r\nif (!mapping_cap_writeback_dirty(inode->i_mapping))\r\nwbc.nr_to_write = 0;\r\nmight_sleep();\r\nreturn writeback_single_inode(inode, wb, &wbc);\r\n}\r\nint sync_inode(struct inode *inode, struct writeback_control *wbc)\r\n{\r\nreturn writeback_single_inode(inode, &inode_to_bdi(inode)->wb, wbc);\r\n}\r\nint sync_inode_metadata(struct inode *inode, int wait)\r\n{\r\nstruct writeback_control wbc = {\r\n.sync_mode = wait ? WB_SYNC_ALL : WB_SYNC_NONE,\r\n.nr_to_write = 0,\r\n};\r\nreturn sync_inode(inode, &wbc);\r\n}
