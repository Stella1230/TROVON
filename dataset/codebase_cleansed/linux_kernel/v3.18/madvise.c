static int madvise_need_mmap_write(int behavior)\r\n{\r\nswitch (behavior) {\r\ncase MADV_REMOVE:\r\ncase MADV_WILLNEED:\r\ncase MADV_DONTNEED:\r\nreturn 0;\r\ndefault:\r\nreturn 1;\r\n}\r\n}\r\nstatic long madvise_behavior(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end, int behavior)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nint error = 0;\r\npgoff_t pgoff;\r\nunsigned long new_flags = vma->vm_flags;\r\nswitch (behavior) {\r\ncase MADV_NORMAL:\r\nnew_flags = new_flags & ~VM_RAND_READ & ~VM_SEQ_READ;\r\nbreak;\r\ncase MADV_SEQUENTIAL:\r\nnew_flags = (new_flags & ~VM_RAND_READ) | VM_SEQ_READ;\r\nbreak;\r\ncase MADV_RANDOM:\r\nnew_flags = (new_flags & ~VM_SEQ_READ) | VM_RAND_READ;\r\nbreak;\r\ncase MADV_DONTFORK:\r\nnew_flags |= VM_DONTCOPY;\r\nbreak;\r\ncase MADV_DOFORK:\r\nif (vma->vm_flags & VM_IO) {\r\nerror = -EINVAL;\r\ngoto out;\r\n}\r\nnew_flags &= ~VM_DONTCOPY;\r\nbreak;\r\ncase MADV_DONTDUMP:\r\nnew_flags |= VM_DONTDUMP;\r\nbreak;\r\ncase MADV_DODUMP:\r\nif (new_flags & VM_SPECIAL) {\r\nerror = -EINVAL;\r\ngoto out;\r\n}\r\nnew_flags &= ~VM_DONTDUMP;\r\nbreak;\r\ncase MADV_MERGEABLE:\r\ncase MADV_UNMERGEABLE:\r\nerror = ksm_madvise(vma, start, end, behavior, &new_flags);\r\nif (error)\r\ngoto out;\r\nbreak;\r\ncase MADV_HUGEPAGE:\r\ncase MADV_NOHUGEPAGE:\r\nerror = hugepage_madvise(vma, &new_flags, behavior);\r\nif (error)\r\ngoto out;\r\nbreak;\r\n}\r\nif (new_flags == vma->vm_flags) {\r\n*prev = vma;\r\ngoto out;\r\n}\r\npgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\n*prev = vma_merge(mm, *prev, start, end, new_flags, vma->anon_vma,\r\nvma->vm_file, pgoff, vma_policy(vma));\r\nif (*prev) {\r\nvma = *prev;\r\ngoto success;\r\n}\r\n*prev = vma;\r\nif (start != vma->vm_start) {\r\nerror = split_vma(mm, vma, start, 1);\r\nif (error)\r\ngoto out;\r\n}\r\nif (end != vma->vm_end) {\r\nerror = split_vma(mm, vma, end, 0);\r\nif (error)\r\ngoto out;\r\n}\r\nsuccess:\r\nvma->vm_flags = new_flags;\r\nout:\r\nif (error == -ENOMEM)\r\nerror = -EAGAIN;\r\nreturn error;\r\n}\r\nstatic int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,\r\nunsigned long end, struct mm_walk *walk)\r\n{\r\npte_t *orig_pte;\r\nstruct vm_area_struct *vma = walk->private;\r\nunsigned long index;\r\nif (pmd_none_or_trans_huge_or_clear_bad(pmd))\r\nreturn 0;\r\nfor (index = start; index != end; index += PAGE_SIZE) {\r\npte_t pte;\r\nswp_entry_t entry;\r\nstruct page *page;\r\nspinlock_t *ptl;\r\norig_pte = pte_offset_map_lock(vma->vm_mm, pmd, start, &ptl);\r\npte = *(orig_pte + ((index - start) / PAGE_SIZE));\r\npte_unmap_unlock(orig_pte, ptl);\r\nif (pte_present(pte) || pte_none(pte) || pte_file(pte))\r\ncontinue;\r\nentry = pte_to_swp_entry(pte);\r\nif (unlikely(non_swap_entry(entry)))\r\ncontinue;\r\npage = read_swap_cache_async(entry, GFP_HIGHUSER_MOVABLE,\r\nvma, index);\r\nif (page)\r\npage_cache_release(page);\r\n}\r\nreturn 0;\r\n}\r\nstatic void force_swapin_readahead(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct mm_walk walk = {\r\n.mm = vma->vm_mm,\r\n.pmd_entry = swapin_walk_pmd_entry,\r\n.private = vma,\r\n};\r\nwalk_page_range(start, end, &walk);\r\nlru_add_drain();\r\n}\r\nstatic void force_shm_swapin_readahead(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end,\r\nstruct address_space *mapping)\r\n{\r\npgoff_t index;\r\nstruct page *page;\r\nswp_entry_t swap;\r\nfor (; start < end; start += PAGE_SIZE) {\r\nindex = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\npage = find_get_entry(mapping, index);\r\nif (!radix_tree_exceptional_entry(page)) {\r\nif (page)\r\npage_cache_release(page);\r\ncontinue;\r\n}\r\nswap = radix_to_swp_entry(page);\r\npage = read_swap_cache_async(swap, GFP_HIGHUSER_MOVABLE,\r\nNULL, 0);\r\nif (page)\r\npage_cache_release(page);\r\n}\r\nlru_add_drain();\r\n}\r\nstatic long madvise_willneed(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct file *file = vma->vm_file;\r\n#ifdef CONFIG_SWAP\r\nif (!file || mapping_cap_swap_backed(file->f_mapping)) {\r\n*prev = vma;\r\nif (!file)\r\nforce_swapin_readahead(vma, start, end);\r\nelse\r\nforce_shm_swapin_readahead(vma, start, end,\r\nfile->f_mapping);\r\nreturn 0;\r\n}\r\n#endif\r\nif (!file)\r\nreturn -EBADF;\r\nif (file->f_mapping->a_ops->get_xip_mem) {\r\nreturn 0;\r\n}\r\n*prev = vma;\r\nstart = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nif (end > vma->vm_end)\r\nend = vma->vm_end;\r\nend = ((end - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nforce_page_cache_readahead(file->f_mapping, file, start, end - start);\r\nreturn 0;\r\n}\r\nstatic long madvise_dontneed(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end)\r\n{\r\n*prev = vma;\r\nif (vma->vm_flags & (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))\r\nreturn -EINVAL;\r\nif (unlikely(vma->vm_flags & VM_NONLINEAR)) {\r\nstruct zap_details details = {\r\n.nonlinear_vma = vma,\r\n.last_index = ULONG_MAX,\r\n};\r\nzap_page_range(vma, start, end - start, &details);\r\n} else\r\nzap_page_range(vma, start, end - start, NULL);\r\nreturn 0;\r\n}\r\nstatic long madvise_remove(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end)\r\n{\r\nloff_t offset;\r\nint error;\r\nstruct file *f;\r\n*prev = NULL;\r\nif (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))\r\nreturn -EINVAL;\r\nf = vma->vm_file;\r\nif (!f || !f->f_mapping || !f->f_mapping->host) {\r\nreturn -EINVAL;\r\n}\r\nif ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))\r\nreturn -EACCES;\r\noffset = (loff_t)(start - vma->vm_start)\r\n+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\r\nget_file(f);\r\nup_read(&current->mm->mmap_sem);\r\nerror = do_fallocate(f,\r\nFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\r\noffset, end - start);\r\nfput(f);\r\ndown_read(&current->mm->mmap_sem);\r\nreturn error;\r\n}\r\nstatic int madvise_hwpoison(int bhv, unsigned long start, unsigned long end)\r\n{\r\nstruct page *p;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nfor (; start < end; start += PAGE_SIZE <<\r\ncompound_order(compound_head(p))) {\r\nint ret;\r\nret = get_user_pages_fast(start, 1, 0, &p);\r\nif (ret != 1)\r\nreturn ret;\r\nif (PageHWPoison(p)) {\r\nput_page(p);\r\ncontinue;\r\n}\r\nif (bhv == MADV_SOFT_OFFLINE) {\r\npr_info("Soft offlining page %#lx at %#lx\n",\r\npage_to_pfn(p), start);\r\nret = soft_offline_page(p, MF_COUNT_INCREASED);\r\nif (ret)\r\nreturn ret;\r\ncontinue;\r\n}\r\npr_info("Injecting memory failure for page %#lx at %#lx\n",\r\npage_to_pfn(p), start);\r\nmemory_failure(page_to_pfn(p), 0, MF_COUNT_INCREASED);\r\n}\r\nreturn 0;\r\n}\r\nstatic long\r\nmadvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,\r\nunsigned long start, unsigned long end, int behavior)\r\n{\r\nswitch (behavior) {\r\ncase MADV_REMOVE:\r\nreturn madvise_remove(vma, prev, start, end);\r\ncase MADV_WILLNEED:\r\nreturn madvise_willneed(vma, prev, start, end);\r\ncase MADV_DONTNEED:\r\nreturn madvise_dontneed(vma, prev, start, end);\r\ndefault:\r\nreturn madvise_behavior(vma, prev, start, end, behavior);\r\n}\r\n}\r\nstatic int\r\nmadvise_behavior_valid(int behavior)\r\n{\r\nswitch (behavior) {\r\ncase MADV_DOFORK:\r\ncase MADV_DONTFORK:\r\ncase MADV_NORMAL:\r\ncase MADV_SEQUENTIAL:\r\ncase MADV_RANDOM:\r\ncase MADV_REMOVE:\r\ncase MADV_WILLNEED:\r\ncase MADV_DONTNEED:\r\n#ifdef CONFIG_KSM\r\ncase MADV_MERGEABLE:\r\ncase MADV_UNMERGEABLE:\r\n#endif\r\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\r\ncase MADV_HUGEPAGE:\r\ncase MADV_NOHUGEPAGE:\r\n#endif\r\ncase MADV_DONTDUMP:\r\ncase MADV_DODUMP:\r\nreturn 1;\r\ndefault:\r\nreturn 0;\r\n}\r\n}
