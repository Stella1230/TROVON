static inline void rm_writeb(unsigned long paddr, u8 val)\r\n{\r\n__asm__ __volatile__("sync; stbcix %0,0,%1"\r\n: : "r" (val), "r" (paddr) : "memory");\r\n}\r\nstatic void icp_rm_set_vcpu_irq(struct kvm_vcpu *vcpu,\r\nstruct kvm_vcpu *this_vcpu)\r\n{\r\nstruct kvmppc_icp *this_icp = this_vcpu->arch.icp;\r\nunsigned long xics_phys;\r\nint cpu;\r\nvcpu->stat.queue_intr++;\r\nset_bit(BOOK3S_IRQPRIO_EXTERNAL_LEVEL, &vcpu->arch.pending_exceptions);\r\nif (vcpu == this_vcpu) {\r\nmtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_MER);\r\nreturn;\r\n}\r\ncpu = vcpu->cpu;\r\nif (cpu < 0 || cpu >= nr_cpu_ids) {\r\nthis_icp->rm_action |= XICS_RM_KICK_VCPU;\r\nthis_icp->rm_kick_target = vcpu;\r\nreturn;\r\n}\r\ncpu += vcpu->arch.ptid;\r\nxics_phys = paca[cpu].kvm_hstate.xics_phys;\r\nrm_writeb(xics_phys + XICS_MFRR, IPI_PRIORITY);\r\n}\r\nstatic void icp_rm_clr_vcpu_irq(struct kvm_vcpu *vcpu)\r\n{\r\nclear_bit(BOOK3S_IRQPRIO_EXTERNAL_LEVEL,\r\n&vcpu->arch.pending_exceptions);\r\nmtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~LPCR_MER);\r\n}\r\nstatic inline bool icp_rm_try_update(struct kvmppc_icp *icp,\r\nunion kvmppc_icp_state old,\r\nunion kvmppc_icp_state new)\r\n{\r\nstruct kvm_vcpu *this_vcpu = local_paca->kvm_hstate.kvm_vcpu;\r\nbool success;\r\nnew.out_ee = (new.xisr && (new.pending_pri < new.cppr));\r\nsuccess = cmpxchg64(&icp->state.raw, old.raw, new.raw) == old.raw;\r\nif (!success)\r\ngoto bail;\r\nif (new.out_ee)\r\nicp_rm_set_vcpu_irq(icp->vcpu, this_vcpu);\r\nthis_vcpu->arch.icp->rm_dbgstate = new;\r\nthis_vcpu->arch.icp->rm_dbgtgt = icp->vcpu;\r\nbail:\r\nreturn success;\r\n}\r\nstatic inline int check_too_hard(struct kvmppc_xics *xics,\r\nstruct kvmppc_icp *icp)\r\n{\r\nreturn (xics->real_mode_dbg || icp->rm_action) ? H_TOO_HARD : H_SUCCESS;\r\n}\r\nstatic void icp_rm_down_cppr(struct kvmppc_xics *xics, struct kvmppc_icp *icp,\r\nu8 new_cppr)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nbool resend;\r\ndo {\r\nold_state = new_state = ACCESS_ONCE(icp->state);\r\nnew_state.cppr = new_cppr;\r\nif (new_state.mfrr < new_cppr &&\r\nnew_state.mfrr <= new_state.pending_pri) {\r\nnew_state.pending_pri = new_state.mfrr;\r\nnew_state.xisr = XICS_IPI;\r\n}\r\nresend = new_state.need_resend;\r\nnew_state.need_resend = 0;\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nif (resend)\r\nicp->rm_action |= XICS_RM_CHECK_RESEND;\r\n}\r\nunsigned long kvmppc_rm_h_xirr(struct kvm_vcpu *vcpu)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp = vcpu->arch.icp;\r\nu32 xirr;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nicp_rm_clr_vcpu_irq(icp->vcpu);\r\ndo {\r\nold_state = new_state = ACCESS_ONCE(icp->state);\r\nxirr = old_state.xisr | (((u32)old_state.cppr) << 24);\r\nif (!old_state.xisr)\r\nbreak;\r\nnew_state.cppr = new_state.pending_pri;\r\nnew_state.pending_pri = 0xff;\r\nnew_state.xisr = 0;\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nvcpu->arch.gpr[4] = xirr;\r\nreturn check_too_hard(xics, icp);\r\n}\r\nint kvmppc_rm_h_ipi(struct kvm_vcpu *vcpu, unsigned long server,\r\nunsigned long mfrr)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp, *this_icp = vcpu->arch.icp;\r\nu32 reject;\r\nbool resend;\r\nbool local;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nlocal = this_icp->server_num == server;\r\nif (local)\r\nicp = this_icp;\r\nelse\r\nicp = kvmppc_xics_find_server(vcpu->kvm, server);\r\nif (!icp)\r\nreturn H_PARAMETER;\r\ndo {\r\nold_state = new_state = ACCESS_ONCE(icp->state);\r\nnew_state.mfrr = mfrr;\r\nreject = 0;\r\nresend = false;\r\nif (mfrr < new_state.cppr) {\r\nif (mfrr <= new_state.pending_pri)\r\nreject = new_state.xisr;\r\nnew_state.pending_pri = mfrr;\r\nnew_state.xisr = XICS_IPI;\r\n}\r\nif (mfrr > old_state.mfrr && mfrr > new_state.cppr) {\r\nresend = new_state.need_resend;\r\nnew_state.need_resend = 0;\r\n}\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nif (reject && reject != XICS_IPI) {\r\nthis_icp->rm_action |= XICS_RM_REJECT;\r\nthis_icp->rm_reject = reject;\r\n}\r\nif (resend)\r\nthis_icp->rm_action |= XICS_RM_CHECK_RESEND;\r\nreturn check_too_hard(xics, this_icp);\r\n}\r\nint kvmppc_rm_h_cppr(struct kvm_vcpu *vcpu, unsigned long cppr)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp = vcpu->arch.icp;\r\nu32 reject;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nif (cppr > icp->state.cppr) {\r\nicp_rm_down_cppr(xics, icp, cppr);\r\ngoto bail;\r\n} else if (cppr == icp->state.cppr)\r\nreturn H_SUCCESS;\r\nicp_rm_clr_vcpu_irq(icp->vcpu);\r\ndo {\r\nold_state = new_state = ACCESS_ONCE(icp->state);\r\nreject = 0;\r\nnew_state.cppr = cppr;\r\nif (cppr <= new_state.pending_pri) {\r\nreject = new_state.xisr;\r\nnew_state.xisr = 0;\r\nnew_state.pending_pri = 0xff;\r\n}\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nif (reject && reject != XICS_IPI) {\r\nicp->rm_action |= XICS_RM_REJECT;\r\nicp->rm_reject = reject;\r\n}\r\nbail:\r\nreturn check_too_hard(xics, icp);\r\n}\r\nint kvmppc_rm_h_eoi(struct kvm_vcpu *vcpu, unsigned long xirr)\r\n{\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp = vcpu->arch.icp;\r\nstruct kvmppc_ics *ics;\r\nstruct ics_irq_state *state;\r\nu32 irq = xirr & 0x00ffffff;\r\nu16 src;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nicp_rm_down_cppr(xics, icp, xirr >> 24);\r\nif (irq == XICS_IPI)\r\ngoto bail;\r\nics = kvmppc_xics_find_ics(xics, irq, &src);\r\nif (!ics)\r\ngoto bail;\r\nstate = &ics->irq_state[src];\r\nif (state->asserted) {\r\nicp->rm_action |= XICS_RM_REJECT;\r\nicp->rm_reject = irq;\r\n}\r\nif (!hlist_empty(&vcpu->kvm->irq_ack_notifier_list)) {\r\nicp->rm_action |= XICS_RM_NOTIFY_EOI;\r\nicp->rm_eoied_irq = irq;\r\n}\r\nbail:\r\nreturn check_too_hard(xics, icp);\r\n}
