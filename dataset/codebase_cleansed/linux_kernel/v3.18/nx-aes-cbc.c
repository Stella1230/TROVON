static int cbc_aes_nx_set_key(struct crypto_tfm *tfm,\r\nconst u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(tfm);\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nnx_ctx_init(nx_ctx, HCOP_FC_AES);\r\nswitch (key_len) {\r\ncase AES_KEYSIZE_128:\r\nNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_128);\r\nnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_128];\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_192);\r\nnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_192];\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_256);\r\nnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_256];\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ncsbcpb->cpb.hdr.mode = NX_MODE_AES_CBC;\r\nmemcpy(csbcpb->cpb.aes_cbc.key, in_key, key_len);\r\nreturn 0;\r\n}\r\nstatic int cbc_aes_nx_crypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src,\r\nunsigned int nbytes,\r\nint enc)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nunsigned long irq_flags;\r\nunsigned int processed = 0, to_process;\r\nu32 max_sg_len;\r\nint rc;\r\nspin_lock_irqsave(&nx_ctx->lock, irq_flags);\r\nmax_sg_len = min_t(u32, nx_driver.of.max_sg_len/sizeof(struct nx_sg),\r\nnx_ctx->ap->sglen);\r\nif (enc)\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_ENDE_ENCRYPT;\r\nelse\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_ENDE_ENCRYPT;\r\ndo {\r\nto_process = min_t(u64, nbytes - processed,\r\nnx_ctx->ap->databytelen);\r\nto_process = min_t(u64, to_process,\r\nNX_PAGE_SIZE * (max_sg_len - 1));\r\nto_process = to_process & ~(AES_BLOCK_SIZE - 1);\r\nrc = nx_build_sg_lists(nx_ctx, desc, dst, src, to_process,\r\nprocessed, csbcpb->cpb.aes_cbc.iv);\r\nif (rc)\r\ngoto out;\r\nif (!nx_ctx->op.inlen || !nx_ctx->op.outlen) {\r\nrc = -EINVAL;\r\ngoto out;\r\n}\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\r\ndesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\ngoto out;\r\nmemcpy(desc->info, csbcpb->cpb.aes_cbc.cv, AES_BLOCK_SIZE);\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(csbcpb->csb.processed_byte_count,\r\n&(nx_ctx->stats->aes_bytes));\r\nprocessed += to_process;\r\n} while (processed < nbytes);\r\nout:\r\nspin_unlock_irqrestore(&nx_ctx->lock, irq_flags);\r\nreturn rc;\r\n}\r\nstatic int cbc_aes_nx_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nreturn cbc_aes_nx_crypt(desc, dst, src, nbytes, 1);\r\n}\r\nstatic int cbc_aes_nx_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nreturn cbc_aes_nx_crypt(desc, dst, src, nbytes, 0);\r\n}
