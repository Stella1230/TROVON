static struct rxrpc_transport *rxrpc_alloc_transport(struct rxrpc_local *local,\r\nstruct rxrpc_peer *peer,\r\ngfp_t gfp)\r\n{\r\nstruct rxrpc_transport *trans;\r\n_enter("");\r\ntrans = kzalloc(sizeof(struct rxrpc_transport), gfp);\r\nif (trans) {\r\ntrans->local = local;\r\ntrans->peer = peer;\r\nINIT_LIST_HEAD(&trans->link);\r\ntrans->bundles = RB_ROOT;\r\ntrans->client_conns = RB_ROOT;\r\ntrans->server_conns = RB_ROOT;\r\nskb_queue_head_init(&trans->error_queue);\r\nspin_lock_init(&trans->client_lock);\r\nrwlock_init(&trans->conn_lock);\r\natomic_set(&trans->usage, 1);\r\ntrans->debug_id = atomic_inc_return(&rxrpc_debug_id);\r\nif (peer->srx.transport.family == AF_INET) {\r\nswitch (peer->srx.transport_type) {\r\ncase SOCK_DGRAM:\r\nINIT_WORK(&trans->error_handler,\r\nrxrpc_UDP_error_handler);\r\nbreak;\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\n} else {\r\nBUG();\r\n}\r\n}\r\n_leave(" = %p", trans);\r\nreturn trans;\r\n}\r\nstruct rxrpc_transport *rxrpc_get_transport(struct rxrpc_local *local,\r\nstruct rxrpc_peer *peer,\r\ngfp_t gfp)\r\n{\r\nstruct rxrpc_transport *trans, *candidate;\r\nconst char *new = "old";\r\nint usage;\r\n_enter("{%pI4+%hu},{%pI4+%hu},",\r\n&local->srx.transport.sin.sin_addr,\r\nntohs(local->srx.transport.sin.sin_port),\r\n&peer->srx.transport.sin.sin_addr,\r\nntohs(peer->srx.transport.sin.sin_port));\r\nread_lock_bh(&rxrpc_transport_lock);\r\nlist_for_each_entry(trans, &rxrpc_transports, link) {\r\nif (trans->local == local && trans->peer == peer)\r\ngoto found_extant_transport;\r\n}\r\nread_unlock_bh(&rxrpc_transport_lock);\r\ncandidate = rxrpc_alloc_transport(local, peer, gfp);\r\nif (!candidate) {\r\n_leave(" = -ENOMEM");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nwrite_lock_bh(&rxrpc_transport_lock);\r\nlist_for_each_entry(trans, &rxrpc_transports, link) {\r\nif (trans->local == local && trans->peer == peer)\r\ngoto found_extant_second;\r\n}\r\ntrans = candidate;\r\ncandidate = NULL;\r\nusage = atomic_read(&trans->usage);\r\nrxrpc_get_local(trans->local);\r\natomic_inc(&trans->peer->usage);\r\nlist_add_tail(&trans->link, &rxrpc_transports);\r\nwrite_unlock_bh(&rxrpc_transport_lock);\r\nnew = "new";\r\nsuccess:\r\n_net("TRANSPORT %s %d local %d -> peer %d",\r\nnew,\r\ntrans->debug_id,\r\ntrans->local->debug_id,\r\ntrans->peer->debug_id);\r\n_leave(" = %p {u=%d}", trans, usage);\r\nreturn trans;\r\nfound_extant_transport:\r\nusage = atomic_inc_return(&trans->usage);\r\nread_unlock_bh(&rxrpc_transport_lock);\r\ngoto success;\r\nfound_extant_second:\r\nusage = atomic_inc_return(&trans->usage);\r\nwrite_unlock_bh(&rxrpc_transport_lock);\r\nkfree(candidate);\r\ngoto success;\r\n}\r\nstruct rxrpc_transport *rxrpc_find_transport(struct rxrpc_local *local,\r\nstruct rxrpc_peer *peer)\r\n{\r\nstruct rxrpc_transport *trans;\r\n_enter("{%pI4+%hu},{%pI4+%hu},",\r\n&local->srx.transport.sin.sin_addr,\r\nntohs(local->srx.transport.sin.sin_port),\r\n&peer->srx.transport.sin.sin_addr,\r\nntohs(peer->srx.transport.sin.sin_port));\r\nread_lock_bh(&rxrpc_transport_lock);\r\nlist_for_each_entry(trans, &rxrpc_transports, link) {\r\nif (trans->local == local && trans->peer == peer)\r\ngoto found_extant_transport;\r\n}\r\nread_unlock_bh(&rxrpc_transport_lock);\r\n_leave(" = NULL");\r\nreturn NULL;\r\nfound_extant_transport:\r\natomic_inc(&trans->usage);\r\nread_unlock_bh(&rxrpc_transport_lock);\r\n_leave(" = %p", trans);\r\nreturn trans;\r\n}\r\nvoid rxrpc_put_transport(struct rxrpc_transport *trans)\r\n{\r\n_enter("%p{u=%d}", trans, atomic_read(&trans->usage));\r\nASSERTCMP(atomic_read(&trans->usage), >, 0);\r\ntrans->put_time = get_seconds();\r\nif (unlikely(atomic_dec_and_test(&trans->usage))) {\r\n_debug("zombie");\r\nrxrpc_queue_delayed_work(&rxrpc_transport_reap, 0);\r\n}\r\n_leave("");\r\n}\r\nstatic void rxrpc_cleanup_transport(struct rxrpc_transport *trans)\r\n{\r\n_net("DESTROY TRANS %d", trans->debug_id);\r\nrxrpc_purge_queue(&trans->error_queue);\r\nrxrpc_put_local(trans->local);\r\nrxrpc_put_peer(trans->peer);\r\nkfree(trans);\r\n}\r\nstatic void rxrpc_transport_reaper(struct work_struct *work)\r\n{\r\nstruct rxrpc_transport *trans, *_p;\r\nunsigned long now, earliest, reap_time;\r\nLIST_HEAD(graveyard);\r\n_enter("");\r\nnow = get_seconds();\r\nearliest = ULONG_MAX;\r\nwrite_lock_bh(&rxrpc_transport_lock);\r\nlist_for_each_entry_safe(trans, _p, &rxrpc_transports, link) {\r\n_debug("reap TRANS %d { u=%d t=%ld }",\r\ntrans->debug_id, atomic_read(&trans->usage),\r\n(long) now - (long) trans->put_time);\r\nif (likely(atomic_read(&trans->usage) > 0))\r\ncontinue;\r\nreap_time = trans->put_time + rxrpc_transport_expiry;\r\nif (reap_time <= now)\r\nlist_move_tail(&trans->link, &graveyard);\r\nelse if (reap_time < earliest)\r\nearliest = reap_time;\r\n}\r\nwrite_unlock_bh(&rxrpc_transport_lock);\r\nif (earliest != ULONG_MAX) {\r\n_debug("reschedule reaper %ld", (long) earliest - now);\r\nASSERTCMP(earliest, >, now);\r\nrxrpc_queue_delayed_work(&rxrpc_transport_reap,\r\n(earliest - now) * HZ);\r\n}\r\nwhile (!list_empty(&graveyard)) {\r\ntrans = list_entry(graveyard.next, struct rxrpc_transport,\r\nlink);\r\nlist_del_init(&trans->link);\r\nASSERTCMP(atomic_read(&trans->usage), ==, 0);\r\nrxrpc_cleanup_transport(trans);\r\n}\r\n_leave("");\r\n}\r\nvoid __exit rxrpc_destroy_all_transports(void)\r\n{\r\n_enter("");\r\nrxrpc_transport_expiry = 0;\r\ncancel_delayed_work(&rxrpc_transport_reap);\r\nrxrpc_queue_delayed_work(&rxrpc_transport_reap, 0);\r\n_leave("");\r\n}
