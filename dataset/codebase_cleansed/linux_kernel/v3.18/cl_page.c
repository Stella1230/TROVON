static struct cl_page *cl_page_top_trusted(struct cl_page *page)\r\n{\r\nwhile (page->cp_parent != NULL)\r\npage = page->cp_parent;\r\nreturn page;\r\n}\r\nstatic void cl_page_get_trust(struct cl_page *page)\r\n{\r\nLASSERT(atomic_read(&page->cp_ref) > 0);\r\natomic_inc(&page->cp_ref);\r\n}\r\nstatic const struct cl_page_slice *\r\ncl_page_at_trusted(const struct cl_page *page,\r\nconst struct lu_device_type *dtype)\r\n{\r\nconst struct cl_page_slice *slice;\r\npage = cl_page_top_trusted((struct cl_page *)page);\r\ndo {\r\nlist_for_each_entry(slice, &page->cp_layers, cpl_linkage) {\r\nif (slice->cpl_obj->co_lu.lo_dev->ld_type == dtype)\r\nreturn slice;\r\n}\r\npage = page->cp_child;\r\n} while (page != NULL);\r\nreturn NULL;\r\n}\r\nstruct cl_page *cl_page_lookup(struct cl_object_header *hdr, pgoff_t index)\r\n{\r\nstruct cl_page *page;\r\nassert_spin_locked(&hdr->coh_page_guard);\r\npage = radix_tree_lookup(&hdr->coh_tree, index);\r\nif (page != NULL)\r\ncl_page_get_trust(page);\r\nreturn page;\r\n}\r\nint cl_page_gang_lookup(const struct lu_env *env, struct cl_object *obj,\r\nstruct cl_io *io, pgoff_t start, pgoff_t end,\r\ncl_page_gang_cb_t cb, void *cbdata)\r\n{\r\nstruct cl_object_header *hdr;\r\nstruct cl_page *page;\r\nstruct cl_page **pvec;\r\nconst struct cl_page_slice *slice;\r\nconst struct lu_device_type *dtype;\r\npgoff_t idx;\r\nunsigned int nr;\r\nunsigned int i;\r\nunsigned int j;\r\nint res = CLP_GANG_OKAY;\r\nint tree_lock = 1;\r\nidx = start;\r\nhdr = cl_object_header(obj);\r\npvec = cl_env_info(env)->clt_pvec;\r\ndtype = cl_object_top(obj)->co_lu.lo_dev->ld_type;\r\nspin_lock(&hdr->coh_page_guard);\r\nwhile ((nr = radix_tree_gang_lookup(&hdr->coh_tree, (void **)pvec,\r\nidx, CLT_PVEC_SIZE)) > 0) {\r\nint end_of_region = 0;\r\nidx = pvec[nr - 1]->cp_index + 1;\r\nfor (i = 0, j = 0; i < nr; ++i) {\r\npage = pvec[i];\r\npvec[i] = NULL;\r\nLASSERT(page->cp_type == CPT_CACHEABLE);\r\nif (page->cp_index > end) {\r\nend_of_region = 1;\r\nbreak;\r\n}\r\nif (page->cp_state == CPS_FREEING)\r\ncontinue;\r\nslice = cl_page_at_trusted(page, dtype);\r\nPASSERT(env, page, slice != NULL);\r\npage = slice->cpl_page;\r\ncl_page_get_trust(page);\r\nlu_ref_add_atomic(&page->cp_reference,\r\n"gang_lookup", current);\r\npvec[j++] = page;\r\n}\r\nspin_unlock(&hdr->coh_page_guard);\r\ntree_lock = 0;\r\nfor (i = 0; i < j; ++i) {\r\npage = pvec[i];\r\nif (res == CLP_GANG_OKAY)\r\nres = (*cb)(env, io, page, cbdata);\r\nlu_ref_del(&page->cp_reference,\r\n"gang_lookup", current);\r\ncl_page_put(env, page);\r\n}\r\nif (nr < CLT_PVEC_SIZE || end_of_region)\r\nbreak;\r\nif (res == CLP_GANG_OKAY && need_resched())\r\nres = CLP_GANG_RESCHED;\r\nif (res != CLP_GANG_OKAY)\r\nbreak;\r\nspin_lock(&hdr->coh_page_guard);\r\ntree_lock = 1;\r\n}\r\nif (tree_lock)\r\nspin_unlock(&hdr->coh_page_guard);\r\nreturn res;\r\n}\r\nstatic void cl_page_free(const struct lu_env *env, struct cl_page *page)\r\n{\r\nstruct cl_object *obj = page->cp_obj;\r\nint pagesize = cl_object_header(obj)->coh_page_bufsize;\r\nPASSERT(env, page, list_empty(&page->cp_batch));\r\nPASSERT(env, page, page->cp_owner == NULL);\r\nPASSERT(env, page, page->cp_req == NULL);\r\nPASSERT(env, page, page->cp_parent == NULL);\r\nPASSERT(env, page, page->cp_state == CPS_FREEING);\r\nmight_sleep();\r\nwhile (!list_empty(&page->cp_layers)) {\r\nstruct cl_page_slice *slice;\r\nslice = list_entry(page->cp_layers.next,\r\nstruct cl_page_slice, cpl_linkage);\r\nlist_del_init(page->cp_layers.next);\r\nslice->cpl_ops->cpo_fini(env, slice);\r\n}\r\nCS_PAGE_DEC(obj, total);\r\nCS_PAGESTATE_DEC(obj, page->cp_state);\r\nlu_object_ref_del_at(&obj->co_lu, &page->cp_obj_ref, "cl_page", page);\r\ncl_object_put(env, obj);\r\nlu_ref_fini(&page->cp_reference);\r\nOBD_FREE(page, pagesize);\r\n}\r\nstatic inline void cl_page_state_set_trust(struct cl_page *page,\r\nenum cl_page_state state)\r\n{\r\n*(enum cl_page_state *)&page->cp_state = state;\r\n}\r\nstatic struct cl_page *cl_page_alloc(const struct lu_env *env,\r\nstruct cl_object *o, pgoff_t ind, struct page *vmpage,\r\nenum cl_page_type type)\r\n{\r\nstruct cl_page *page;\r\nstruct lu_object_header *head;\r\nOBD_ALLOC_GFP(page, cl_object_header(o)->coh_page_bufsize,\r\nGFP_NOFS);\r\nif (page != NULL) {\r\nint result = 0;\r\natomic_set(&page->cp_ref, 1);\r\nif (type == CPT_CACHEABLE)\r\natomic_inc(&page->cp_ref);\r\npage->cp_obj = o;\r\ncl_object_get(o);\r\nlu_object_ref_add_at(&o->co_lu, &page->cp_obj_ref, "cl_page",\r\npage);\r\npage->cp_index = ind;\r\ncl_page_state_set_trust(page, CPS_CACHED);\r\npage->cp_type = type;\r\nINIT_LIST_HEAD(&page->cp_layers);\r\nINIT_LIST_HEAD(&page->cp_batch);\r\nINIT_LIST_HEAD(&page->cp_flight);\r\nmutex_init(&page->cp_mutex);\r\nlu_ref_init(&page->cp_reference);\r\nhead = o->co_lu.lo_header;\r\nlist_for_each_entry(o, &head->loh_layers,\r\nco_lu.lo_linkage) {\r\nif (o->co_ops->coo_page_init != NULL) {\r\nresult = o->co_ops->coo_page_init(env, o,\r\npage, vmpage);\r\nif (result != 0) {\r\ncl_page_delete0(env, page, 0);\r\ncl_page_free(env, page);\r\npage = ERR_PTR(result);\r\nbreak;\r\n}\r\n}\r\n}\r\nif (result == 0) {\r\nCS_PAGE_INC(o, total);\r\nCS_PAGE_INC(o, create);\r\nCS_PAGESTATE_DEC(o, CPS_CACHED);\r\n}\r\n} else {\r\npage = ERR_PTR(-ENOMEM);\r\n}\r\nreturn page;\r\n}\r\nstatic struct cl_page *cl_page_find0(const struct lu_env *env,\r\nstruct cl_object *o,\r\npgoff_t idx, struct page *vmpage,\r\nenum cl_page_type type,\r\nstruct cl_page *parent)\r\n{\r\nstruct cl_page *page = NULL;\r\nstruct cl_page *ghost = NULL;\r\nstruct cl_object_header *hdr;\r\nint err;\r\nLASSERT(type == CPT_CACHEABLE || type == CPT_TRANSIENT);\r\nmight_sleep();\r\nhdr = cl_object_header(o);\r\nCS_PAGE_INC(o, lookup);\r\nCDEBUG(D_PAGE, "%lu@"DFID" %p %lx %d\n",\r\nidx, PFID(&hdr->coh_lu.loh_fid), vmpage, vmpage->private, type);\r\nif (type == CPT_CACHEABLE) {\r\nKLASSERT(PageLocked(vmpage));\r\npage = cl_vmpage_page(vmpage, o);\r\nPINVRNT(env, page,\r\nergo(page != NULL,\r\ncl_page_vmpage(env, page) == vmpage &&\r\n(void *)radix_tree_lookup(&hdr->coh_tree,\r\nidx) == page));\r\n}\r\nif (page != NULL) {\r\nCS_PAGE_INC(o, hit);\r\nreturn page;\r\n}\r\npage = cl_page_alloc(env, o, idx, vmpage, type);\r\nif (IS_ERR(page))\r\nreturn page;\r\nif (type == CPT_TRANSIENT) {\r\nif (parent) {\r\nLASSERT(page->cp_parent == NULL);\r\npage->cp_parent = parent;\r\nparent->cp_child = page;\r\n}\r\nreturn page;\r\n}\r\nspin_lock(&hdr->coh_page_guard);\r\nerr = radix_tree_insert(&hdr->coh_tree, idx, page);\r\nif (err != 0) {\r\nghost = page;\r\npage = ERR_PTR(err);\r\nCL_PAGE_DEBUG(D_ERROR, env, ghost,\r\n"fail to insert into radix tree: %d\n", err);\r\n} else {\r\nif (parent) {\r\nLASSERT(page->cp_parent == NULL);\r\npage->cp_parent = parent;\r\nparent->cp_child = page;\r\n}\r\nhdr->coh_pages++;\r\n}\r\nspin_unlock(&hdr->coh_page_guard);\r\nif (unlikely(ghost != NULL)) {\r\ncl_page_delete0(env, ghost, 0);\r\ncl_page_free(env, ghost);\r\n}\r\nreturn page;\r\n}\r\nstruct cl_page *cl_page_find(const struct lu_env *env, struct cl_object *o,\r\npgoff_t idx, struct page *vmpage,\r\nenum cl_page_type type)\r\n{\r\nreturn cl_page_find0(env, o, idx, vmpage, type, NULL);\r\n}\r\nstruct cl_page *cl_page_find_sub(const struct lu_env *env, struct cl_object *o,\r\npgoff_t idx, struct page *vmpage,\r\nstruct cl_page *parent)\r\n{\r\nreturn cl_page_find0(env, o, idx, vmpage, parent->cp_type, parent);\r\n}\r\nstatic inline int cl_page_invariant(const struct cl_page *pg)\r\n{\r\nstruct cl_object_header *header;\r\nstruct cl_page *parent;\r\nstruct cl_page *child;\r\nstruct cl_io *owner;\r\nLINVRNT(cl_page_is_vmlocked(NULL, pg));\r\nheader = cl_object_header(pg->cp_obj);\r\nparent = pg->cp_parent;\r\nchild = pg->cp_child;\r\nowner = pg->cp_owner;\r\nreturn cl_page_in_use(pg) &&\r\nergo(parent != NULL, parent->cp_child == pg) &&\r\nergo(child != NULL, child->cp_parent == pg) &&\r\nergo(child != NULL, pg->cp_obj != child->cp_obj) &&\r\nergo(parent != NULL, pg->cp_obj != parent->cp_obj) &&\r\nergo(owner != NULL && parent != NULL,\r\nparent->cp_owner == pg->cp_owner->ci_parent) &&\r\nergo(owner != NULL && child != NULL,\r\nchild->cp_owner->ci_parent == owner) &&\r\nergo(pg->cp_state < CPS_FREEING && pg->cp_type == CPT_CACHEABLE,\r\n(void *)radix_tree_lookup(&header->coh_tree,\r\npg->cp_index) == pg ||\r\n(child == NULL && parent == NULL));\r\n}\r\nstatic void cl_page_state_set0(const struct lu_env *env,\r\nstruct cl_page *page, enum cl_page_state state)\r\n{\r\nenum cl_page_state old;\r\nstatic const int allowed_transitions[CPS_NR][CPS_NR] = {\r\n[CPS_CACHED] = {\r\n[CPS_CACHED] = 0,\r\n[CPS_OWNED] = 1,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 1,\r\n[CPS_FREEING] = 1,\r\n},\r\n[CPS_OWNED] = {\r\n[CPS_CACHED] = 1,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 1,\r\n[CPS_PAGEOUT] = 1,\r\n[CPS_FREEING] = 1,\r\n},\r\n[CPS_PAGEIN] = {\r\n[CPS_CACHED] = 1,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 0,\r\n[CPS_FREEING] = 0,\r\n},\r\n[CPS_PAGEOUT] = {\r\n[CPS_CACHED] = 1,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 0,\r\n[CPS_FREEING] = 0,\r\n},\r\n[CPS_FREEING] = {\r\n[CPS_CACHED] = 0,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 0,\r\n[CPS_FREEING] = 0,\r\n}\r\n};\r\nold = page->cp_state;\r\nPASSERT(env, page, allowed_transitions[old][state]);\r\nCL_PAGE_HEADER(D_TRACE, env, page, "%d -> %d\n", old, state);\r\nfor (; page != NULL; page = page->cp_child) {\r\nPASSERT(env, page, page->cp_state == old);\r\nPASSERT(env, page,\r\nequi(state == CPS_OWNED, page->cp_owner != NULL));\r\nCS_PAGESTATE_DEC(page->cp_obj, page->cp_state);\r\nCS_PAGESTATE_INC(page->cp_obj, state);\r\ncl_page_state_set_trust(page, state);\r\n}\r\n}\r\nstatic void cl_page_state_set(const struct lu_env *env,\r\nstruct cl_page *page, enum cl_page_state state)\r\n{\r\ncl_page_state_set0(env, page, state);\r\n}\r\nvoid cl_page_get(struct cl_page *page)\r\n{\r\ncl_page_get_trust(page);\r\n}\r\nvoid cl_page_put(const struct lu_env *env, struct cl_page *page)\r\n{\r\nPASSERT(env, page, atomic_read(&page->cp_ref) > !!page->cp_parent);\r\nCL_PAGE_HEADER(D_TRACE, env, page, "%d\n",\r\natomic_read(&page->cp_ref));\r\nif (atomic_dec_and_test(&page->cp_ref)) {\r\nLASSERT(page->cp_state == CPS_FREEING);\r\nLASSERT(atomic_read(&page->cp_ref) == 0);\r\nPASSERT(env, page, page->cp_owner == NULL);\r\nPASSERT(env, page, list_empty(&page->cp_batch));\r\ncl_page_free(env, page);\r\n}\r\n}\r\nstruct page *cl_page_vmpage(const struct lu_env *env, struct cl_page *page)\r\n{\r\nconst struct cl_page_slice *slice;\r\npage = cl_page_top(page);\r\ndo {\r\nlist_for_each_entry(slice, &page->cp_layers, cpl_linkage) {\r\nif (slice->cpl_ops->cpo_vmpage != NULL)\r\nreturn slice->cpl_ops->cpo_vmpage(env, slice);\r\n}\r\npage = page->cp_child;\r\n} while (page != NULL);\r\nLBUG();\r\n}\r\nstruct cl_page *cl_vmpage_page(struct page *vmpage, struct cl_object *obj)\r\n{\r\nstruct cl_page *top;\r\nstruct cl_page *page;\r\nKLASSERT(PageLocked(vmpage));\r\ntop = (struct cl_page *)vmpage->private;\r\nif (top == NULL)\r\nreturn NULL;\r\nfor (page = top; page != NULL; page = page->cp_child) {\r\nif (cl_object_same(page->cp_obj, obj)) {\r\ncl_page_get_trust(page);\r\nbreak;\r\n}\r\n}\r\nLASSERT(ergo(page, page->cp_type == CPT_CACHEABLE));\r\nreturn page;\r\n}\r\nstruct cl_page *cl_page_top(struct cl_page *page)\r\n{\r\nreturn cl_page_top_trusted(page);\r\n}\r\nconst struct cl_page_slice *cl_page_at(const struct cl_page *page,\r\nconst struct lu_device_type *dtype)\r\n{\r\nreturn cl_page_at_trusted(page, dtype);\r\n}\r\nstatic int cl_page_invoke(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *page, ptrdiff_t op)\r\n{\r\nPINVRNT(env, page, cl_object_same(page->cp_obj, io->ci_obj));\r\nreturn CL_PAGE_INVOKE(env, page, op,\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *),\r\nio);\r\n}\r\nstatic void cl_page_invoid(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *page, ptrdiff_t op)\r\n{\r\nPINVRNT(env, page, cl_object_same(page->cp_obj, io->ci_obj));\r\nCL_PAGE_INVOID(env, page, op,\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *), io);\r\n}\r\nstatic void cl_page_owner_clear(struct cl_page *page)\r\n{\r\nfor (page = cl_page_top(page); page != NULL; page = page->cp_child) {\r\nif (page->cp_owner != NULL) {\r\nLASSERT(page->cp_owner->ci_owned_nr > 0);\r\npage->cp_owner->ci_owned_nr--;\r\npage->cp_owner = NULL;\r\npage->cp_task = NULL;\r\n}\r\n}\r\n}\r\nstatic void cl_page_owner_set(struct cl_page *page)\r\n{\r\nfor (page = cl_page_top(page); page != NULL; page = page->cp_child) {\r\nLASSERT(page->cp_owner != NULL);\r\npage->cp_owner->ci_owned_nr++;\r\n}\r\n}\r\nvoid cl_page_disown0(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nenum cl_page_state state;\r\nstate = pg->cp_state;\r\nPINVRNT(env, pg, state == CPS_OWNED || state == CPS_FREEING);\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\ncl_page_owner_clear(pg);\r\nif (state == CPS_OWNED)\r\ncl_page_state_set(env, pg, CPS_CACHED);\r\nCL_PAGE_INVOID_REVERSE(env, pg, CL_PAGE_OP(cpo_disown),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *),\r\nio);\r\n}\r\nint cl_page_is_owned(const struct cl_page *pg, const struct cl_io *io)\r\n{\r\nLINVRNT(cl_object_same(pg->cp_obj, io->ci_obj));\r\nreturn pg->cp_state == CPS_OWNED && pg->cp_owner == io;\r\n}\r\nstatic int cl_page_own0(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg, int nonblock)\r\n{\r\nint result;\r\nPINVRNT(env, pg, !cl_page_is_owned(pg, io));\r\npg = cl_page_top(pg);\r\nio = cl_io_top(io);\r\nif (pg->cp_state == CPS_FREEING) {\r\nresult = -ENOENT;\r\n} else {\r\nresult = CL_PAGE_INVOKE(env, pg, CL_PAGE_OP(cpo_own),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *,\r\nstruct cl_io *, int),\r\nio, nonblock);\r\nif (result == 0) {\r\nPASSERT(env, pg, pg->cp_owner == NULL);\r\nPASSERT(env, pg, pg->cp_req == NULL);\r\npg->cp_owner = io;\r\npg->cp_task = current;\r\ncl_page_owner_set(pg);\r\nif (pg->cp_state != CPS_FREEING) {\r\ncl_page_state_set(env, pg, CPS_OWNED);\r\n} else {\r\ncl_page_disown0(env, io, pg);\r\nresult = -ENOENT;\r\n}\r\n}\r\n}\r\nPINVRNT(env, pg, ergo(result == 0, cl_page_invariant(pg)));\r\nreturn result;\r\n}\r\nint cl_page_own(const struct lu_env *env, struct cl_io *io, struct cl_page *pg)\r\n{\r\nreturn cl_page_own0(env, io, pg, 0);\r\n}\r\nint cl_page_own_try(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg)\r\n{\r\nreturn cl_page_own0(env, io, pg, 1);\r\n}\r\nvoid cl_page_assume(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_object_same(pg->cp_obj, io->ci_obj));\r\npg = cl_page_top(pg);\r\nio = cl_io_top(io);\r\ncl_page_invoid(env, io, pg, CL_PAGE_OP(cpo_assume));\r\nPASSERT(env, pg, pg->cp_owner == NULL);\r\npg->cp_owner = io;\r\npg->cp_task = current;\r\ncl_page_owner_set(pg);\r\ncl_page_state_set(env, pg, CPS_OWNED);\r\n}\r\nvoid cl_page_unassume(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\npg = cl_page_top(pg);\r\nio = cl_io_top(io);\r\ncl_page_owner_clear(pg);\r\ncl_page_state_set(env, pg, CPS_CACHED);\r\nCL_PAGE_INVOID_REVERSE(env, pg, CL_PAGE_OP(cpo_unassume),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *),\r\nio);\r\n}\r\nvoid cl_page_disown(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\npg = cl_page_top(pg);\r\nio = cl_io_top(io);\r\ncl_page_disown0(env, io, pg);\r\n}\r\nvoid cl_page_discard(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\ncl_page_invoid(env, io, pg, CL_PAGE_OP(cpo_discard));\r\n}\r\nstatic void cl_page_delete0(const struct lu_env *env, struct cl_page *pg,\r\nint radix)\r\n{\r\nstruct cl_page *tmp = pg;\r\nPASSERT(env, pg, pg == cl_page_top(pg));\r\nPASSERT(env, pg, pg->cp_state != CPS_FREEING);\r\ncl_page_owner_clear(pg);\r\ncl_page_export(env, pg, 0);\r\ncl_page_state_set0(env, pg, CPS_FREEING);\r\nCL_PAGE_INVOID(env, pg, CL_PAGE_OP(cpo_delete),\r\n(const struct lu_env *, const struct cl_page_slice *));\r\nif (tmp->cp_type == CPT_CACHEABLE) {\r\nif (!radix)\r\ntmp = pg->cp_child;\r\nfor (; tmp != NULL; tmp = tmp->cp_child) {\r\nvoid *value;\r\nstruct cl_object_header *hdr;\r\nhdr = cl_object_header(tmp->cp_obj);\r\nspin_lock(&hdr->coh_page_guard);\r\nvalue = radix_tree_delete(&hdr->coh_tree,\r\ntmp->cp_index);\r\nPASSERT(env, tmp, value == tmp);\r\nPASSERT(env, tmp, hdr->coh_pages > 0);\r\nhdr->coh_pages--;\r\nspin_unlock(&hdr->coh_page_guard);\r\ncl_page_put(env, tmp);\r\n}\r\n}\r\n}\r\nvoid cl_page_delete(const struct lu_env *env, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\ncl_page_delete0(env, pg, 1);\r\n}\r\nint cl_page_unmap(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nreturn cl_page_invoke(env, io, pg, CL_PAGE_OP(cpo_unmap));\r\n}\r\nvoid cl_page_export(const struct lu_env *env, struct cl_page *pg, int uptodate)\r\n{\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nCL_PAGE_INVOID(env, pg, CL_PAGE_OP(cpo_export),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, int), uptodate);\r\n}\r\nint cl_page_is_vmlocked(const struct lu_env *env, const struct cl_page *pg)\r\n{\r\nint result;\r\nconst struct cl_page_slice *slice;\r\npg = cl_page_top_trusted((struct cl_page *)pg);\r\nslice = container_of(pg->cp_layers.next,\r\nconst struct cl_page_slice, cpl_linkage);\r\nPASSERT(env, pg, slice->cpl_ops->cpo_is_vmlocked != NULL);\r\nresult = slice->cpl_ops->cpo_is_vmlocked(env, slice);\r\nPASSERT(env, pg, result == -EBUSY || result == -ENODATA);\r\nreturn result == -EBUSY;\r\n}\r\nstatic enum cl_page_state cl_req_type_state(enum cl_req_type crt)\r\n{\r\nreturn crt == CRT_WRITE ? CPS_PAGEOUT : CPS_PAGEIN;\r\n}\r\nstatic void cl_page_io_start(const struct lu_env *env,\r\nstruct cl_page *pg, enum cl_req_type crt)\r\n{\r\ncl_page_owner_clear(pg);\r\ncl_page_state_set(env, pg, cl_req_type_state(crt));\r\n}\r\nint cl_page_prep(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg, enum cl_req_type crt)\r\n{\r\nint result;\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nPINVRNT(env, pg, crt < CRT_NR);\r\nif (crt >= CRT_NR)\r\nreturn -EINVAL;\r\nresult = cl_page_invoke(env, io, pg, CL_PAGE_OP(io[crt].cpo_prep));\r\nif (result == 0)\r\ncl_page_io_start(env, pg, crt);\r\nKLASSERT(ergo(crt == CRT_WRITE && pg->cp_type == CPT_CACHEABLE,\r\nequi(result == 0,\r\nPageWriteback(cl_page_vmpage(env, pg)))));\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", crt, result);\r\nreturn result;\r\n}\r\nvoid cl_page_completion(const struct lu_env *env,\r\nstruct cl_page *pg, enum cl_req_type crt, int ioret)\r\n{\r\nstruct cl_sync_io *anchor = pg->cp_sync_io;\r\nPASSERT(env, pg, crt < CRT_NR);\r\nPASSERT(env, pg, pg->cp_req == NULL);\r\nPASSERT(env, pg, pg->cp_state == cl_req_type_state(crt));\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", crt, ioret);\r\nif (crt == CRT_READ && ioret == 0) {\r\nPASSERT(env, pg, !(pg->cp_flags & CPF_READ_COMPLETED));\r\npg->cp_flags |= CPF_READ_COMPLETED;\r\n}\r\ncl_page_state_set(env, pg, CPS_CACHED);\r\nif (crt >= CRT_NR)\r\nreturn;\r\nCL_PAGE_INVOID_REVERSE(env, pg, CL_PAGE_OP(io[crt].cpo_completion),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, int), ioret);\r\nif (anchor) {\r\nLASSERT(cl_page_is_vmlocked(env, pg));\r\nLASSERT(pg->cp_sync_io == anchor);\r\npg->cp_sync_io = NULL;\r\n}\r\ncl_page_put(env, pg);\r\nif (anchor)\r\ncl_sync_io_note(anchor, ioret);\r\n}\r\nint cl_page_make_ready(const struct lu_env *env, struct cl_page *pg,\r\nenum cl_req_type crt)\r\n{\r\nint result;\r\nPINVRNT(env, pg, crt < CRT_NR);\r\nif (crt >= CRT_NR)\r\nreturn -EINVAL;\r\nresult = CL_PAGE_INVOKE(env, pg, CL_PAGE_OP(io[crt].cpo_make_ready),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *));\r\nif (result == 0) {\r\nPASSERT(env, pg, pg->cp_state == CPS_CACHED);\r\ncl_page_io_start(env, pg, crt);\r\n}\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", crt, result);\r\nreturn result;\r\n}\r\nint cl_page_cache_add(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg, enum cl_req_type crt)\r\n{\r\nconst struct cl_page_slice *scan;\r\nint result = 0;\r\nPINVRNT(env, pg, crt < CRT_NR);\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nif (crt >= CRT_NR)\r\nreturn -EINVAL;\r\nlist_for_each_entry(scan, &pg->cp_layers, cpl_linkage) {\r\nif (scan->cpl_ops->io[crt].cpo_cache_add == NULL)\r\ncontinue;\r\nresult = scan->cpl_ops->io[crt].cpo_cache_add(env, scan, io);\r\nif (result != 0)\r\nbreak;\r\n}\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", crt, result);\r\nreturn result;\r\n}\r\nint cl_page_flush(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg)\r\n{\r\nint result;\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nresult = cl_page_invoke(env, io, pg, CL_PAGE_OP(cpo_flush));\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d\n", result);\r\nreturn result;\r\n}\r\nint cl_page_is_under_lock(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *page)\r\n{\r\nint rc;\r\nPINVRNT(env, page, cl_page_invariant(page));\r\nrc = CL_PAGE_INVOKE(env, page, CL_PAGE_OP(cpo_is_under_lock),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *),\r\nio);\r\nPASSERT(env, page, rc != 0);\r\nreturn rc;\r\n}\r\nstatic int page_prune_cb(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *page, void *cbdata)\r\n{\r\ncl_page_own(env, io, page);\r\ncl_page_unmap(env, io, page);\r\ncl_page_discard(env, io, page);\r\ncl_page_disown(env, io, page);\r\nreturn CLP_GANG_OKAY;\r\n}\r\nint cl_pages_prune(const struct lu_env *env, struct cl_object *clobj)\r\n{\r\nstruct cl_thread_info *info;\r\nstruct cl_object *obj = cl_object_top(clobj);\r\nstruct cl_io *io;\r\nint result;\r\ninfo = cl_env_info(env);\r\nio = &info->clt_io;\r\nio->ci_obj = obj;\r\nio->ci_ignore_layout = 1;\r\nresult = cl_io_init(env, io, CIT_MISC, obj);\r\nif (result != 0) {\r\ncl_io_fini(env, io);\r\nreturn io->ci_result;\r\n}\r\ndo {\r\nresult = cl_page_gang_lookup(env, obj, io, 0, CL_PAGE_EOF,\r\npage_prune_cb, NULL);\r\nif (result == CLP_GANG_RESCHED)\r\ncond_resched();\r\n} while (result != CLP_GANG_OKAY);\r\ncl_io_fini(env, io);\r\nreturn result;\r\n}\r\nvoid cl_page_clip(const struct lu_env *env, struct cl_page *pg,\r\nint from, int to)\r\n{\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", from, to);\r\nCL_PAGE_INVOID(env, pg, CL_PAGE_OP(cpo_clip),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *,int, int),\r\nfrom, to);\r\n}\r\nvoid cl_page_header_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer, const struct cl_page *pg)\r\n{\r\n(*printer)(env, cookie,\r\n"page@%p[%d %p:%lu ^%p_%p %d %d %d %p %p %#x]\n",\r\npg, atomic_read(&pg->cp_ref), pg->cp_obj,\r\npg->cp_index, pg->cp_parent, pg->cp_child,\r\npg->cp_state, pg->cp_error, pg->cp_type,\r\npg->cp_owner, pg->cp_req, pg->cp_flags);\r\n}\r\nvoid cl_page_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer, const struct cl_page *pg)\r\n{\r\nstruct cl_page *scan;\r\nfor (scan = cl_page_top((struct cl_page *)pg);\r\nscan != NULL; scan = scan->cp_child)\r\ncl_page_header_print(env, cookie, printer, scan);\r\nCL_PAGE_INVOKE(env, (struct cl_page *)pg, CL_PAGE_OP(cpo_print),\r\n(const struct lu_env *env,\r\nconst struct cl_page_slice *slice,\r\nvoid *cookie, lu_printer_t p), cookie, printer);\r\n(*printer)(env, cookie, "end page@%p\n", pg);\r\n}\r\nint cl_page_cancel(const struct lu_env *env, struct cl_page *page)\r\n{\r\nreturn CL_PAGE_INVOKE(env, page, CL_PAGE_OP(cpo_cancel),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *));\r\n}\r\nloff_t cl_offset(const struct cl_object *obj, pgoff_t idx)\r\n{\r\nreturn (loff_t)idx << PAGE_CACHE_SHIFT;\r\n}\r\npgoff_t cl_index(const struct cl_object *obj, loff_t offset)\r\n{\r\nreturn offset >> PAGE_CACHE_SHIFT;\r\n}\r\nint cl_page_size(const struct cl_object *obj)\r\n{\r\nreturn 1 << PAGE_CACHE_SHIFT;\r\n}\r\nvoid cl_page_slice_add(struct cl_page *page, struct cl_page_slice *slice,\r\nstruct cl_object *obj,\r\nconst struct cl_page_operations *ops)\r\n{\r\nlist_add_tail(&slice->cpl_linkage, &page->cp_layers);\r\nslice->cpl_obj = obj;\r\nslice->cpl_ops = ops;\r\nslice->cpl_page = page;\r\n}\r\nint cl_page_init(void)\r\n{\r\nreturn 0;\r\n}\r\nvoid cl_page_fini(void)\r\n{\r\n}
