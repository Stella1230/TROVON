static int is_sqp(struct mthca_dev *dev, struct mthca_qp *qp)\r\n{\r\nreturn qp->qpn >= dev->qp_table.sqp_start &&\r\nqp->qpn <= dev->qp_table.sqp_start + 3;\r\n}\r\nstatic int is_qp0(struct mthca_dev *dev, struct mthca_qp *qp)\r\n{\r\nreturn qp->qpn >= dev->qp_table.sqp_start &&\r\nqp->qpn <= dev->qp_table.sqp_start + 1;\r\n}\r\nstatic void *get_recv_wqe(struct mthca_qp *qp, int n)\r\n{\r\nif (qp->is_direct)\r\nreturn qp->queue.direct.buf + (n << qp->rq.wqe_shift);\r\nelse\r\nreturn qp->queue.page_list[(n << qp->rq.wqe_shift) >> PAGE_SHIFT].buf +\r\n((n << qp->rq.wqe_shift) & (PAGE_SIZE - 1));\r\n}\r\nstatic void *get_send_wqe(struct mthca_qp *qp, int n)\r\n{\r\nif (qp->is_direct)\r\nreturn qp->queue.direct.buf + qp->send_wqe_offset +\r\n(n << qp->sq.wqe_shift);\r\nelse\r\nreturn qp->queue.page_list[(qp->send_wqe_offset +\r\n(n << qp->sq.wqe_shift)) >>\r\nPAGE_SHIFT].buf +\r\n((qp->send_wqe_offset + (n << qp->sq.wqe_shift)) &\r\n(PAGE_SIZE - 1));\r\n}\r\nstatic void mthca_wq_reset(struct mthca_wq *wq)\r\n{\r\nwq->next_ind = 0;\r\nwq->last_comp = wq->max - 1;\r\nwq->head = 0;\r\nwq->tail = 0;\r\n}\r\nvoid mthca_qp_event(struct mthca_dev *dev, u32 qpn,\r\nenum ib_event_type event_type)\r\n{\r\nstruct mthca_qp *qp;\r\nstruct ib_event event;\r\nspin_lock(&dev->qp_table.lock);\r\nqp = mthca_array_get(&dev->qp_table.qp, qpn & (dev->limits.num_qps - 1));\r\nif (qp)\r\n++qp->refcount;\r\nspin_unlock(&dev->qp_table.lock);\r\nif (!qp) {\r\nmthca_warn(dev, "Async event %d for bogus QP %08x\n",\r\nevent_type, qpn);\r\nreturn;\r\n}\r\nif (event_type == IB_EVENT_PATH_MIG)\r\nqp->port = qp->alt_port;\r\nevent.device = &dev->ib_dev;\r\nevent.event = event_type;\r\nevent.element.qp = &qp->ibqp;\r\nif (qp->ibqp.event_handler)\r\nqp->ibqp.event_handler(&event, qp->ibqp.qp_context);\r\nspin_lock(&dev->qp_table.lock);\r\nif (!--qp->refcount)\r\nwake_up(&qp->wait);\r\nspin_unlock(&dev->qp_table.lock);\r\n}\r\nstatic int to_mthca_state(enum ib_qp_state ib_state)\r\n{\r\nswitch (ib_state) {\r\ncase IB_QPS_RESET: return MTHCA_QP_STATE_RST;\r\ncase IB_QPS_INIT: return MTHCA_QP_STATE_INIT;\r\ncase IB_QPS_RTR: return MTHCA_QP_STATE_RTR;\r\ncase IB_QPS_RTS: return MTHCA_QP_STATE_RTS;\r\ncase IB_QPS_SQD: return MTHCA_QP_STATE_SQD;\r\ncase IB_QPS_SQE: return MTHCA_QP_STATE_SQE;\r\ncase IB_QPS_ERR: return MTHCA_QP_STATE_ERR;\r\ndefault: return -1;\r\n}\r\n}\r\nstatic int to_mthca_st(int transport)\r\n{\r\nswitch (transport) {\r\ncase RC: return MTHCA_QP_ST_RC;\r\ncase UC: return MTHCA_QP_ST_UC;\r\ncase UD: return MTHCA_QP_ST_UD;\r\ncase RD: return MTHCA_QP_ST_RD;\r\ncase MLX: return MTHCA_QP_ST_MLX;\r\ndefault: return -1;\r\n}\r\n}\r\nstatic void store_attrs(struct mthca_sqp *sqp, const struct ib_qp_attr *attr,\r\nint attr_mask)\r\n{\r\nif (attr_mask & IB_QP_PKEY_INDEX)\r\nsqp->pkey_index = attr->pkey_index;\r\nif (attr_mask & IB_QP_QKEY)\r\nsqp->qkey = attr->qkey;\r\nif (attr_mask & IB_QP_SQ_PSN)\r\nsqp->send_psn = attr->sq_psn;\r\n}\r\nstatic void init_port(struct mthca_dev *dev, int port)\r\n{\r\nint err;\r\nstruct mthca_init_ib_param param;\r\nmemset(&param, 0, sizeof param);\r\nparam.port_width = dev->limits.port_width_cap;\r\nparam.vl_cap = dev->limits.vl_cap;\r\nparam.mtu_cap = dev->limits.mtu_cap;\r\nparam.gid_cap = dev->limits.gid_table_len;\r\nparam.pkey_cap = dev->limits.pkey_table_len;\r\nerr = mthca_INIT_IB(dev, &param, port);\r\nif (err)\r\nmthca_warn(dev, "INIT_IB failed, return code %d.\n", err);\r\n}\r\nstatic __be32 get_hw_access_flags(struct mthca_qp *qp, const struct ib_qp_attr *attr,\r\nint attr_mask)\r\n{\r\nu8 dest_rd_atomic;\r\nu32 access_flags;\r\nu32 hw_access_flags = 0;\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\r\ndest_rd_atomic = attr->max_dest_rd_atomic;\r\nelse\r\ndest_rd_atomic = qp->resp_depth;\r\nif (attr_mask & IB_QP_ACCESS_FLAGS)\r\naccess_flags = attr->qp_access_flags;\r\nelse\r\naccess_flags = qp->atomic_rd_en;\r\nif (!dest_rd_atomic)\r\naccess_flags &= IB_ACCESS_REMOTE_WRITE;\r\nif (access_flags & IB_ACCESS_REMOTE_READ)\r\nhw_access_flags |= MTHCA_QP_BIT_RRE;\r\nif (access_flags & IB_ACCESS_REMOTE_ATOMIC)\r\nhw_access_flags |= MTHCA_QP_BIT_RAE;\r\nif (access_flags & IB_ACCESS_REMOTE_WRITE)\r\nhw_access_flags |= MTHCA_QP_BIT_RWE;\r\nreturn cpu_to_be32(hw_access_flags);\r\n}\r\nstatic inline enum ib_qp_state to_ib_qp_state(int mthca_state)\r\n{\r\nswitch (mthca_state) {\r\ncase MTHCA_QP_STATE_RST: return IB_QPS_RESET;\r\ncase MTHCA_QP_STATE_INIT: return IB_QPS_INIT;\r\ncase MTHCA_QP_STATE_RTR: return IB_QPS_RTR;\r\ncase MTHCA_QP_STATE_RTS: return IB_QPS_RTS;\r\ncase MTHCA_QP_STATE_DRAINING:\r\ncase MTHCA_QP_STATE_SQD: return IB_QPS_SQD;\r\ncase MTHCA_QP_STATE_SQE: return IB_QPS_SQE;\r\ncase MTHCA_QP_STATE_ERR: return IB_QPS_ERR;\r\ndefault: return -1;\r\n}\r\n}\r\nstatic inline enum ib_mig_state to_ib_mig_state(int mthca_mig_state)\r\n{\r\nswitch (mthca_mig_state) {\r\ncase 0: return IB_MIG_ARMED;\r\ncase 1: return IB_MIG_REARM;\r\ncase 3: return IB_MIG_MIGRATED;\r\ndefault: return -1;\r\n}\r\n}\r\nstatic int to_ib_qp_access_flags(int mthca_flags)\r\n{\r\nint ib_flags = 0;\r\nif (mthca_flags & MTHCA_QP_BIT_RRE)\r\nib_flags |= IB_ACCESS_REMOTE_READ;\r\nif (mthca_flags & MTHCA_QP_BIT_RWE)\r\nib_flags |= IB_ACCESS_REMOTE_WRITE;\r\nif (mthca_flags & MTHCA_QP_BIT_RAE)\r\nib_flags |= IB_ACCESS_REMOTE_ATOMIC;\r\nreturn ib_flags;\r\n}\r\nstatic void to_ib_ah_attr(struct mthca_dev *dev, struct ib_ah_attr *ib_ah_attr,\r\nstruct mthca_qp_path *path)\r\n{\r\nmemset(ib_ah_attr, 0, sizeof *ib_ah_attr);\r\nib_ah_attr->port_num = (be32_to_cpu(path->port_pkey) >> 24) & 0x3;\r\nif (ib_ah_attr->port_num == 0 || ib_ah_attr->port_num > dev->limits.num_ports)\r\nreturn;\r\nib_ah_attr->dlid = be16_to_cpu(path->rlid);\r\nib_ah_attr->sl = be32_to_cpu(path->sl_tclass_flowlabel) >> 28;\r\nib_ah_attr->src_path_bits = path->g_mylmc & 0x7f;\r\nib_ah_attr->static_rate = mthca_rate_to_ib(dev,\r\npath->static_rate & 0xf,\r\nib_ah_attr->port_num);\r\nib_ah_attr->ah_flags = (path->g_mylmc & (1 << 7)) ? IB_AH_GRH : 0;\r\nif (ib_ah_attr->ah_flags) {\r\nib_ah_attr->grh.sgid_index = path->mgid_index & (dev->limits.gid_table_len - 1);\r\nib_ah_attr->grh.hop_limit = path->hop_limit;\r\nib_ah_attr->grh.traffic_class =\r\n(be32_to_cpu(path->sl_tclass_flowlabel) >> 20) & 0xff;\r\nib_ah_attr->grh.flow_label =\r\nbe32_to_cpu(path->sl_tclass_flowlabel) & 0xfffff;\r\nmemcpy(ib_ah_attr->grh.dgid.raw,\r\npath->rgid, sizeof ib_ah_attr->grh.dgid.raw);\r\n}\r\n}\r\nint mthca_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr, int qp_attr_mask,\r\nstruct ib_qp_init_attr *qp_init_attr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibqp->device);\r\nstruct mthca_qp *qp = to_mqp(ibqp);\r\nint err = 0;\r\nstruct mthca_mailbox *mailbox = NULL;\r\nstruct mthca_qp_param *qp_param;\r\nstruct mthca_qp_context *context;\r\nint mthca_state;\r\nmutex_lock(&qp->mutex);\r\nif (qp->state == IB_QPS_RESET) {\r\nqp_attr->qp_state = IB_QPS_RESET;\r\ngoto done;\r\n}\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox)) {\r\nerr = PTR_ERR(mailbox);\r\ngoto out;\r\n}\r\nerr = mthca_QUERY_QP(dev, qp->qpn, 0, mailbox);\r\nif (err) {\r\nmthca_warn(dev, "QUERY_QP failed (%d)\n", err);\r\ngoto out_mailbox;\r\n}\r\nqp_param = mailbox->buf;\r\ncontext = &qp_param->context;\r\nmthca_state = be32_to_cpu(context->flags) >> 28;\r\nqp->state = to_ib_qp_state(mthca_state);\r\nqp_attr->qp_state = qp->state;\r\nqp_attr->path_mtu = context->mtu_msgmax >> 5;\r\nqp_attr->path_mig_state =\r\nto_ib_mig_state((be32_to_cpu(context->flags) >> 11) & 0x3);\r\nqp_attr->qkey = be32_to_cpu(context->qkey);\r\nqp_attr->rq_psn = be32_to_cpu(context->rnr_nextrecvpsn) & 0xffffff;\r\nqp_attr->sq_psn = be32_to_cpu(context->next_send_psn) & 0xffffff;\r\nqp_attr->dest_qp_num = be32_to_cpu(context->remote_qpn) & 0xffffff;\r\nqp_attr->qp_access_flags =\r\nto_ib_qp_access_flags(be32_to_cpu(context->params2));\r\nif (qp->transport == RC || qp->transport == UC) {\r\nto_ib_ah_attr(dev, &qp_attr->ah_attr, &context->pri_path);\r\nto_ib_ah_attr(dev, &qp_attr->alt_ah_attr, &context->alt_path);\r\nqp_attr->alt_pkey_index =\r\nbe32_to_cpu(context->alt_path.port_pkey) & 0x7f;\r\nqp_attr->alt_port_num = qp_attr->alt_ah_attr.port_num;\r\n}\r\nqp_attr->pkey_index = be32_to_cpu(context->pri_path.port_pkey) & 0x7f;\r\nqp_attr->port_num =\r\n(be32_to_cpu(context->pri_path.port_pkey) >> 24) & 0x3;\r\nqp_attr->sq_draining = mthca_state == MTHCA_QP_STATE_DRAINING;\r\nqp_attr->max_rd_atomic = 1 << ((be32_to_cpu(context->params1) >> 21) & 0x7);\r\nqp_attr->max_dest_rd_atomic =\r\n1 << ((be32_to_cpu(context->params2) >> 21) & 0x7);\r\nqp_attr->min_rnr_timer =\r\n(be32_to_cpu(context->rnr_nextrecvpsn) >> 24) & 0x1f;\r\nqp_attr->timeout = context->pri_path.ackto >> 3;\r\nqp_attr->retry_cnt = (be32_to_cpu(context->params1) >> 16) & 0x7;\r\nqp_attr->rnr_retry = context->pri_path.rnr_retry >> 5;\r\nqp_attr->alt_timeout = context->alt_path.ackto >> 3;\r\ndone:\r\nqp_attr->cur_qp_state = qp_attr->qp_state;\r\nqp_attr->cap.max_send_wr = qp->sq.max;\r\nqp_attr->cap.max_recv_wr = qp->rq.max;\r\nqp_attr->cap.max_send_sge = qp->sq.max_gs;\r\nqp_attr->cap.max_recv_sge = qp->rq.max_gs;\r\nqp_attr->cap.max_inline_data = qp->max_inline_data;\r\nqp_init_attr->cap = qp_attr->cap;\r\nqp_init_attr->sq_sig_type = qp->sq_policy;\r\nout_mailbox:\r\nmthca_free_mailbox(dev, mailbox);\r\nout:\r\nmutex_unlock(&qp->mutex);\r\nreturn err;\r\n}\r\nstatic int mthca_path_set(struct mthca_dev *dev, const struct ib_ah_attr *ah,\r\nstruct mthca_qp_path *path, u8 port)\r\n{\r\npath->g_mylmc = ah->src_path_bits & 0x7f;\r\npath->rlid = cpu_to_be16(ah->dlid);\r\npath->static_rate = mthca_get_rate(dev, ah->static_rate, port);\r\nif (ah->ah_flags & IB_AH_GRH) {\r\nif (ah->grh.sgid_index >= dev->limits.gid_table_len) {\r\nmthca_dbg(dev, "sgid_index (%u) too large. max is %d\n",\r\nah->grh.sgid_index, dev->limits.gid_table_len-1);\r\nreturn -1;\r\n}\r\npath->g_mylmc |= 1 << 7;\r\npath->mgid_index = ah->grh.sgid_index;\r\npath->hop_limit = ah->grh.hop_limit;\r\npath->sl_tclass_flowlabel =\r\ncpu_to_be32((ah->sl << 28) |\r\n(ah->grh.traffic_class << 20) |\r\n(ah->grh.flow_label));\r\nmemcpy(path->rgid, ah->grh.dgid.raw, 16);\r\n} else\r\npath->sl_tclass_flowlabel = cpu_to_be32(ah->sl << 28);\r\nreturn 0;\r\n}\r\nstatic int __mthca_modify_qp(struct ib_qp *ibqp,\r\nconst struct ib_qp_attr *attr, int attr_mask,\r\nenum ib_qp_state cur_state, enum ib_qp_state new_state)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibqp->device);\r\nstruct mthca_qp *qp = to_mqp(ibqp);\r\nstruct mthca_mailbox *mailbox;\r\nstruct mthca_qp_param *qp_param;\r\nstruct mthca_qp_context *qp_context;\r\nu32 sqd_event = 0;\r\nint err = -EINVAL;\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox)) {\r\nerr = PTR_ERR(mailbox);\r\ngoto out;\r\n}\r\nqp_param = mailbox->buf;\r\nqp_context = &qp_param->context;\r\nmemset(qp_param, 0, sizeof *qp_param);\r\nqp_context->flags = cpu_to_be32((to_mthca_state(new_state) << 28) |\r\n(to_mthca_st(qp->transport) << 16));\r\nqp_context->flags |= cpu_to_be32(MTHCA_QP_BIT_DE);\r\nif (!(attr_mask & IB_QP_PATH_MIG_STATE))\r\nqp_context->flags |= cpu_to_be32(MTHCA_QP_PM_MIGRATED << 11);\r\nelse {\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_PM_STATE);\r\nswitch (attr->path_mig_state) {\r\ncase IB_MIG_MIGRATED:\r\nqp_context->flags |= cpu_to_be32(MTHCA_QP_PM_MIGRATED << 11);\r\nbreak;\r\ncase IB_MIG_REARM:\r\nqp_context->flags |= cpu_to_be32(MTHCA_QP_PM_REARM << 11);\r\nbreak;\r\ncase IB_MIG_ARMED:\r\nqp_context->flags |= cpu_to_be32(MTHCA_QP_PM_ARMED << 11);\r\nbreak;\r\n}\r\n}\r\nif (qp->transport == MLX || qp->transport == UD)\r\nqp_context->mtu_msgmax = (IB_MTU_2048 << 5) | 11;\r\nelse if (attr_mask & IB_QP_PATH_MTU) {\r\nif (attr->path_mtu < IB_MTU_256 || attr->path_mtu > IB_MTU_2048) {\r\nmthca_dbg(dev, "path MTU (%u) is invalid\n",\r\nattr->path_mtu);\r\ngoto out_mailbox;\r\n}\r\nqp_context->mtu_msgmax = (attr->path_mtu << 5) | 31;\r\n}\r\nif (mthca_is_memfree(dev)) {\r\nif (qp->rq.max)\r\nqp_context->rq_size_stride = ilog2(qp->rq.max) << 3;\r\nqp_context->rq_size_stride |= qp->rq.wqe_shift - 4;\r\nif (qp->sq.max)\r\nqp_context->sq_size_stride = ilog2(qp->sq.max) << 3;\r\nqp_context->sq_size_stride |= qp->sq.wqe_shift - 4;\r\n}\r\nif (qp->ibqp.uobject)\r\nqp_context->usr_page =\r\ncpu_to_be32(to_mucontext(qp->ibqp.uobject->context)->uar.index);\r\nelse\r\nqp_context->usr_page = cpu_to_be32(dev->driver_uar.index);\r\nqp_context->local_qpn = cpu_to_be32(qp->qpn);\r\nif (attr_mask & IB_QP_DEST_QPN) {\r\nqp_context->remote_qpn = cpu_to_be32(attr->dest_qp_num);\r\n}\r\nif (qp->transport == MLX)\r\nqp_context->pri_path.port_pkey |=\r\ncpu_to_be32(qp->port << 24);\r\nelse {\r\nif (attr_mask & IB_QP_PORT) {\r\nqp_context->pri_path.port_pkey |=\r\ncpu_to_be32(attr->port_num << 24);\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_PORT_NUM);\r\n}\r\n}\r\nif (attr_mask & IB_QP_PKEY_INDEX) {\r\nqp_context->pri_path.port_pkey |=\r\ncpu_to_be32(attr->pkey_index);\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_PKEY_INDEX);\r\n}\r\nif (attr_mask & IB_QP_RNR_RETRY) {\r\nqp_context->alt_path.rnr_retry = qp_context->pri_path.rnr_retry =\r\nattr->rnr_retry << 5;\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_RNR_RETRY |\r\nMTHCA_QP_OPTPAR_ALT_RNR_RETRY);\r\n}\r\nif (attr_mask & IB_QP_AV) {\r\nif (mthca_path_set(dev, &attr->ah_attr, &qp_context->pri_path,\r\nattr_mask & IB_QP_PORT ? attr->port_num : qp->port))\r\ngoto out_mailbox;\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_PRIMARY_ADDR_PATH);\r\n}\r\nif (ibqp->qp_type == IB_QPT_RC &&\r\ncur_state == IB_QPS_INIT && new_state == IB_QPS_RTR) {\r\nu8 sched_queue = ibqp->uobject ? 0x2 : 0x1;\r\nif (mthca_is_memfree(dev))\r\nqp_context->rlkey_arbel_sched_queue |= sched_queue;\r\nelse\r\nqp_context->tavor_sched_queue |= cpu_to_be32(sched_queue);\r\nqp_param->opt_param_mask |=\r\ncpu_to_be32(MTHCA_QP_OPTPAR_SCHED_QUEUE);\r\n}\r\nif (attr_mask & IB_QP_TIMEOUT) {\r\nqp_context->pri_path.ackto = attr->timeout << 3;\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_ACK_TIMEOUT);\r\n}\r\nif (attr_mask & IB_QP_ALT_PATH) {\r\nif (attr->alt_pkey_index >= dev->limits.pkey_table_len) {\r\nmthca_dbg(dev, "Alternate P_Key index (%u) too large. max is %d\n",\r\nattr->alt_pkey_index, dev->limits.pkey_table_len-1);\r\ngoto out_mailbox;\r\n}\r\nif (attr->alt_port_num == 0 || attr->alt_port_num > dev->limits.num_ports) {\r\nmthca_dbg(dev, "Alternate port number (%u) is invalid\n",\r\nattr->alt_port_num);\r\ngoto out_mailbox;\r\n}\r\nif (mthca_path_set(dev, &attr->alt_ah_attr, &qp_context->alt_path,\r\nattr->alt_ah_attr.port_num))\r\ngoto out_mailbox;\r\nqp_context->alt_path.port_pkey |= cpu_to_be32(attr->alt_pkey_index |\r\nattr->alt_port_num << 24);\r\nqp_context->alt_path.ackto = attr->alt_timeout << 3;\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_ALT_ADDR_PATH);\r\n}\r\nqp_context->pd = cpu_to_be32(to_mpd(ibqp->pd)->pd_num);\r\nqp_context->wqe_lkey = cpu_to_be32(qp->mr.ibmr.lkey);\r\nqp_context->params1 = cpu_to_be32((MTHCA_ACK_REQ_FREQ << 28) |\r\n(MTHCA_FLIGHT_LIMIT << 24) |\r\nMTHCA_QP_BIT_SWE);\r\nif (qp->sq_policy == IB_SIGNAL_ALL_WR)\r\nqp_context->params1 |= cpu_to_be32(MTHCA_QP_BIT_SSC);\r\nif (attr_mask & IB_QP_RETRY_CNT) {\r\nqp_context->params1 |= cpu_to_be32(attr->retry_cnt << 16);\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_RETRY_COUNT);\r\n}\r\nif (attr_mask & IB_QP_MAX_QP_RD_ATOMIC) {\r\nif (attr->max_rd_atomic) {\r\nqp_context->params1 |=\r\ncpu_to_be32(MTHCA_QP_BIT_SRE |\r\nMTHCA_QP_BIT_SAE);\r\nqp_context->params1 |=\r\ncpu_to_be32(fls(attr->max_rd_atomic - 1) << 21);\r\n}\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_SRA_MAX);\r\n}\r\nif (attr_mask & IB_QP_SQ_PSN)\r\nqp_context->next_send_psn = cpu_to_be32(attr->sq_psn);\r\nqp_context->cqn_snd = cpu_to_be32(to_mcq(ibqp->send_cq)->cqn);\r\nif (mthca_is_memfree(dev)) {\r\nqp_context->snd_wqe_base_l = cpu_to_be32(qp->send_wqe_offset);\r\nqp_context->snd_db_index = cpu_to_be32(qp->sq.db_index);\r\n}\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) {\r\nif (attr->max_dest_rd_atomic)\r\nqp_context->params2 |=\r\ncpu_to_be32(fls(attr->max_dest_rd_atomic - 1) << 21);\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_RRA_MAX);\r\n}\r\nif (attr_mask & (IB_QP_ACCESS_FLAGS | IB_QP_MAX_DEST_RD_ATOMIC)) {\r\nqp_context->params2 |= get_hw_access_flags(qp, attr, attr_mask);\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_RWE |\r\nMTHCA_QP_OPTPAR_RRE |\r\nMTHCA_QP_OPTPAR_RAE);\r\n}\r\nqp_context->params2 |= cpu_to_be32(MTHCA_QP_BIT_RSC);\r\nif (ibqp->srq)\r\nqp_context->params2 |= cpu_to_be32(MTHCA_QP_BIT_RIC);\r\nif (attr_mask & IB_QP_MIN_RNR_TIMER) {\r\nqp_context->rnr_nextrecvpsn |= cpu_to_be32(attr->min_rnr_timer << 24);\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_RNR_TIMEOUT);\r\n}\r\nif (attr_mask & IB_QP_RQ_PSN)\r\nqp_context->rnr_nextrecvpsn |= cpu_to_be32(attr->rq_psn);\r\nqp_context->ra_buff_indx =\r\ncpu_to_be32(dev->qp_table.rdb_base +\r\n((qp->qpn & (dev->limits.num_qps - 1)) * MTHCA_RDB_ENTRY_SIZE <<\r\ndev->qp_table.rdb_shift));\r\nqp_context->cqn_rcv = cpu_to_be32(to_mcq(ibqp->recv_cq)->cqn);\r\nif (mthca_is_memfree(dev))\r\nqp_context->rcv_db_index = cpu_to_be32(qp->rq.db_index);\r\nif (attr_mask & IB_QP_QKEY) {\r\nqp_context->qkey = cpu_to_be32(attr->qkey);\r\nqp_param->opt_param_mask |= cpu_to_be32(MTHCA_QP_OPTPAR_Q_KEY);\r\n}\r\nif (ibqp->srq)\r\nqp_context->srqn = cpu_to_be32(1 << 24 |\r\nto_msrq(ibqp->srq)->srqn);\r\nif (cur_state == IB_QPS_RTS && new_state == IB_QPS_SQD &&\r\nattr_mask & IB_QP_EN_SQD_ASYNC_NOTIFY &&\r\nattr->en_sqd_async_notify)\r\nsqd_event = 1 << 31;\r\nerr = mthca_MODIFY_QP(dev, cur_state, new_state, qp->qpn, 0,\r\nmailbox, sqd_event);\r\nif (err) {\r\nmthca_warn(dev, "modify QP %d->%d returned %d.\n",\r\ncur_state, new_state, err);\r\ngoto out_mailbox;\r\n}\r\nqp->state = new_state;\r\nif (attr_mask & IB_QP_ACCESS_FLAGS)\r\nqp->atomic_rd_en = attr->qp_access_flags;\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\r\nqp->resp_depth = attr->max_dest_rd_atomic;\r\nif (attr_mask & IB_QP_PORT)\r\nqp->port = attr->port_num;\r\nif (attr_mask & IB_QP_ALT_PATH)\r\nqp->alt_port = attr->alt_port_num;\r\nif (is_sqp(dev, qp))\r\nstore_attrs(to_msqp(qp), attr, attr_mask);\r\nif (is_qp0(dev, qp)) {\r\nif (cur_state != IB_QPS_RTR &&\r\nnew_state == IB_QPS_RTR)\r\ninit_port(dev, qp->port);\r\nif (cur_state != IB_QPS_RESET &&\r\ncur_state != IB_QPS_ERR &&\r\n(new_state == IB_QPS_RESET ||\r\nnew_state == IB_QPS_ERR))\r\nmthca_CLOSE_IB(dev, qp->port);\r\n}\r\nif (new_state == IB_QPS_RESET && !qp->ibqp.uobject) {\r\nmthca_cq_clean(dev, to_mcq(qp->ibqp.recv_cq), qp->qpn,\r\nqp->ibqp.srq ? to_msrq(qp->ibqp.srq) : NULL);\r\nif (qp->ibqp.send_cq != qp->ibqp.recv_cq)\r\nmthca_cq_clean(dev, to_mcq(qp->ibqp.send_cq), qp->qpn, NULL);\r\nmthca_wq_reset(&qp->sq);\r\nqp->sq.last = get_send_wqe(qp, qp->sq.max - 1);\r\nmthca_wq_reset(&qp->rq);\r\nqp->rq.last = get_recv_wqe(qp, qp->rq.max - 1);\r\nif (mthca_is_memfree(dev)) {\r\n*qp->sq.db = 0;\r\n*qp->rq.db = 0;\r\n}\r\n}\r\nout_mailbox:\r\nmthca_free_mailbox(dev, mailbox);\r\nout:\r\nreturn err;\r\n}\r\nint mthca_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr, int attr_mask,\r\nstruct ib_udata *udata)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibqp->device);\r\nstruct mthca_qp *qp = to_mqp(ibqp);\r\nenum ib_qp_state cur_state, new_state;\r\nint err = -EINVAL;\r\nmutex_lock(&qp->mutex);\r\nif (attr_mask & IB_QP_CUR_STATE) {\r\ncur_state = attr->cur_qp_state;\r\n} else {\r\nspin_lock_irq(&qp->sq.lock);\r\nspin_lock(&qp->rq.lock);\r\ncur_state = qp->state;\r\nspin_unlock(&qp->rq.lock);\r\nspin_unlock_irq(&qp->sq.lock);\r\n}\r\nnew_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;\r\nif (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type, attr_mask,\r\nIB_LINK_LAYER_UNSPECIFIED)) {\r\nmthca_dbg(dev, "Bad QP transition (transport %d) "\r\n"%d->%d with attr 0x%08x\n",\r\nqp->transport, cur_state, new_state,\r\nattr_mask);\r\ngoto out;\r\n}\r\nif ((attr_mask & IB_QP_PKEY_INDEX) &&\r\nattr->pkey_index >= dev->limits.pkey_table_len) {\r\nmthca_dbg(dev, "P_Key index (%u) too large. max is %d\n",\r\nattr->pkey_index, dev->limits.pkey_table_len-1);\r\ngoto out;\r\n}\r\nif ((attr_mask & IB_QP_PORT) &&\r\n(attr->port_num == 0 || attr->port_num > dev->limits.num_ports)) {\r\nmthca_dbg(dev, "Port number (%u) is invalid\n", attr->port_num);\r\ngoto out;\r\n}\r\nif (attr_mask & IB_QP_MAX_QP_RD_ATOMIC &&\r\nattr->max_rd_atomic > dev->limits.max_qp_init_rdma) {\r\nmthca_dbg(dev, "Max rdma_atomic as initiator %u too large (max is %d)\n",\r\nattr->max_rd_atomic, dev->limits.max_qp_init_rdma);\r\ngoto out;\r\n}\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC &&\r\nattr->max_dest_rd_atomic > 1 << dev->qp_table.rdb_shift) {\r\nmthca_dbg(dev, "Max rdma_atomic as responder %u too large (max %d)\n",\r\nattr->max_dest_rd_atomic, 1 << dev->qp_table.rdb_shift);\r\ngoto out;\r\n}\r\nif (cur_state == new_state && cur_state == IB_QPS_RESET) {\r\nerr = 0;\r\ngoto out;\r\n}\r\nerr = __mthca_modify_qp(ibqp, attr, attr_mask, cur_state, new_state);\r\nout:\r\nmutex_unlock(&qp->mutex);\r\nreturn err;\r\n}\r\nstatic int mthca_max_data_size(struct mthca_dev *dev, struct mthca_qp *qp, int desc_sz)\r\n{\r\nint max_data_size = desc_sz - sizeof (struct mthca_next_seg);\r\nswitch (qp->transport) {\r\ncase MLX:\r\nmax_data_size -= 2 * sizeof (struct mthca_data_seg);\r\nbreak;\r\ncase UD:\r\nif (mthca_is_memfree(dev))\r\nmax_data_size -= sizeof (struct mthca_arbel_ud_seg);\r\nelse\r\nmax_data_size -= sizeof (struct mthca_tavor_ud_seg);\r\nbreak;\r\ndefault:\r\nmax_data_size -= sizeof (struct mthca_raddr_seg);\r\nbreak;\r\n}\r\nreturn max_data_size;\r\n}\r\nstatic inline int mthca_max_inline_data(struct mthca_pd *pd, int max_data_size)\r\n{\r\nreturn pd->ibpd.uobject ? max_data_size - MTHCA_INLINE_HEADER_SIZE : 0;\r\n}\r\nstatic void mthca_adjust_qp_caps(struct mthca_dev *dev,\r\nstruct mthca_pd *pd,\r\nstruct mthca_qp *qp)\r\n{\r\nint max_data_size = mthca_max_data_size(dev, qp,\r\nmin(dev->limits.max_desc_sz,\r\n1 << qp->sq.wqe_shift));\r\nqp->max_inline_data = mthca_max_inline_data(pd, max_data_size);\r\nqp->sq.max_gs = min_t(int, dev->limits.max_sg,\r\nmax_data_size / sizeof (struct mthca_data_seg));\r\nqp->rq.max_gs = min_t(int, dev->limits.max_sg,\r\n(min(dev->limits.max_desc_sz, 1 << qp->rq.wqe_shift) -\r\nsizeof (struct mthca_next_seg)) /\r\nsizeof (struct mthca_data_seg));\r\n}\r\nstatic int mthca_alloc_wqe_buf(struct mthca_dev *dev,\r\nstruct mthca_pd *pd,\r\nstruct mthca_qp *qp)\r\n{\r\nint size;\r\nint err = -ENOMEM;\r\nsize = sizeof (struct mthca_next_seg) +\r\nqp->rq.max_gs * sizeof (struct mthca_data_seg);\r\nif (size > dev->limits.max_desc_sz)\r\nreturn -EINVAL;\r\nfor (qp->rq.wqe_shift = 6; 1 << qp->rq.wqe_shift < size;\r\nqp->rq.wqe_shift++)\r\n;\r\nsize = qp->sq.max_gs * sizeof (struct mthca_data_seg);\r\nswitch (qp->transport) {\r\ncase MLX:\r\nsize += 2 * sizeof (struct mthca_data_seg);\r\nbreak;\r\ncase UD:\r\nsize += mthca_is_memfree(dev) ?\r\nsizeof (struct mthca_arbel_ud_seg) :\r\nsizeof (struct mthca_tavor_ud_seg);\r\nbreak;\r\ncase UC:\r\nsize += sizeof (struct mthca_raddr_seg);\r\nbreak;\r\ncase RC:\r\nsize += sizeof (struct mthca_raddr_seg);\r\nsize = max_t(int, size,\r\nsizeof (struct mthca_atomic_seg) +\r\nsizeof (struct mthca_raddr_seg) +\r\nsizeof (struct mthca_data_seg));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nsize = max_t(int, size, sizeof (struct mthca_bind_seg));\r\nsize += sizeof (struct mthca_next_seg);\r\nif (size > dev->limits.max_desc_sz)\r\nreturn -EINVAL;\r\nfor (qp->sq.wqe_shift = 6; 1 << qp->sq.wqe_shift < size;\r\nqp->sq.wqe_shift++)\r\n;\r\nqp->send_wqe_offset = ALIGN(qp->rq.max << qp->rq.wqe_shift,\r\n1 << qp->sq.wqe_shift);\r\nif (pd->ibpd.uobject)\r\nreturn 0;\r\nsize = PAGE_ALIGN(qp->send_wqe_offset +\r\n(qp->sq.max << qp->sq.wqe_shift));\r\nqp->wrid = kmalloc((qp->rq.max + qp->sq.max) * sizeof (u64),\r\nGFP_KERNEL);\r\nif (!qp->wrid)\r\ngoto err_out;\r\nerr = mthca_buf_alloc(dev, size, MTHCA_MAX_DIRECT_QP_SIZE,\r\n&qp->queue, &qp->is_direct, pd, 0, &qp->mr);\r\nif (err)\r\ngoto err_out;\r\nreturn 0;\r\nerr_out:\r\nkfree(qp->wrid);\r\nreturn err;\r\n}\r\nstatic void mthca_free_wqe_buf(struct mthca_dev *dev,\r\nstruct mthca_qp *qp)\r\n{\r\nmthca_buf_free(dev, PAGE_ALIGN(qp->send_wqe_offset +\r\n(qp->sq.max << qp->sq.wqe_shift)),\r\n&qp->queue, qp->is_direct, &qp->mr);\r\nkfree(qp->wrid);\r\n}\r\nstatic int mthca_map_memfree(struct mthca_dev *dev,\r\nstruct mthca_qp *qp)\r\n{\r\nint ret;\r\nif (mthca_is_memfree(dev)) {\r\nret = mthca_table_get(dev, dev->qp_table.qp_table, qp->qpn);\r\nif (ret)\r\nreturn ret;\r\nret = mthca_table_get(dev, dev->qp_table.eqp_table, qp->qpn);\r\nif (ret)\r\ngoto err_qpc;\r\nret = mthca_table_get(dev, dev->qp_table.rdb_table,\r\nqp->qpn << dev->qp_table.rdb_shift);\r\nif (ret)\r\ngoto err_eqpc;\r\n}\r\nreturn 0;\r\nerr_eqpc:\r\nmthca_table_put(dev, dev->qp_table.eqp_table, qp->qpn);\r\nerr_qpc:\r\nmthca_table_put(dev, dev->qp_table.qp_table, qp->qpn);\r\nreturn ret;\r\n}\r\nstatic void mthca_unmap_memfree(struct mthca_dev *dev,\r\nstruct mthca_qp *qp)\r\n{\r\nmthca_table_put(dev, dev->qp_table.rdb_table,\r\nqp->qpn << dev->qp_table.rdb_shift);\r\nmthca_table_put(dev, dev->qp_table.eqp_table, qp->qpn);\r\nmthca_table_put(dev, dev->qp_table.qp_table, qp->qpn);\r\n}\r\nstatic int mthca_alloc_memfree(struct mthca_dev *dev,\r\nstruct mthca_qp *qp)\r\n{\r\nif (mthca_is_memfree(dev)) {\r\nqp->rq.db_index = mthca_alloc_db(dev, MTHCA_DB_TYPE_RQ,\r\nqp->qpn, &qp->rq.db);\r\nif (qp->rq.db_index < 0)\r\nreturn -ENOMEM;\r\nqp->sq.db_index = mthca_alloc_db(dev, MTHCA_DB_TYPE_SQ,\r\nqp->qpn, &qp->sq.db);\r\nif (qp->sq.db_index < 0) {\r\nmthca_free_db(dev, MTHCA_DB_TYPE_RQ, qp->rq.db_index);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void mthca_free_memfree(struct mthca_dev *dev,\r\nstruct mthca_qp *qp)\r\n{\r\nif (mthca_is_memfree(dev)) {\r\nmthca_free_db(dev, MTHCA_DB_TYPE_SQ, qp->sq.db_index);\r\nmthca_free_db(dev, MTHCA_DB_TYPE_RQ, qp->rq.db_index);\r\n}\r\n}\r\nstatic int mthca_alloc_qp_common(struct mthca_dev *dev,\r\nstruct mthca_pd *pd,\r\nstruct mthca_cq *send_cq,\r\nstruct mthca_cq *recv_cq,\r\nenum ib_sig_type send_policy,\r\nstruct mthca_qp *qp)\r\n{\r\nint ret;\r\nint i;\r\nstruct mthca_next_seg *next;\r\nqp->refcount = 1;\r\ninit_waitqueue_head(&qp->wait);\r\nmutex_init(&qp->mutex);\r\nqp->state = IB_QPS_RESET;\r\nqp->atomic_rd_en = 0;\r\nqp->resp_depth = 0;\r\nqp->sq_policy = send_policy;\r\nmthca_wq_reset(&qp->sq);\r\nmthca_wq_reset(&qp->rq);\r\nspin_lock_init(&qp->sq.lock);\r\nspin_lock_init(&qp->rq.lock);\r\nret = mthca_map_memfree(dev, qp);\r\nif (ret)\r\nreturn ret;\r\nret = mthca_alloc_wqe_buf(dev, pd, qp);\r\nif (ret) {\r\nmthca_unmap_memfree(dev, qp);\r\nreturn ret;\r\n}\r\nmthca_adjust_qp_caps(dev, pd, qp);\r\nif (pd->ibpd.uobject)\r\nreturn 0;\r\nret = mthca_alloc_memfree(dev, qp);\r\nif (ret) {\r\nmthca_free_wqe_buf(dev, qp);\r\nmthca_unmap_memfree(dev, qp);\r\nreturn ret;\r\n}\r\nif (mthca_is_memfree(dev)) {\r\nstruct mthca_data_seg *scatter;\r\nint size = (sizeof (struct mthca_next_seg) +\r\nqp->rq.max_gs * sizeof (struct mthca_data_seg)) / 16;\r\nfor (i = 0; i < qp->rq.max; ++i) {\r\nnext = get_recv_wqe(qp, i);\r\nnext->nda_op = cpu_to_be32(((i + 1) & (qp->rq.max - 1)) <<\r\nqp->rq.wqe_shift);\r\nnext->ee_nds = cpu_to_be32(size);\r\nfor (scatter = (void *) (next + 1);\r\n(void *) scatter < (void *) next + (1 << qp->rq.wqe_shift);\r\n++scatter)\r\nscatter->lkey = cpu_to_be32(MTHCA_INVAL_LKEY);\r\n}\r\nfor (i = 0; i < qp->sq.max; ++i) {\r\nnext = get_send_wqe(qp, i);\r\nnext->nda_op = cpu_to_be32((((i + 1) & (qp->sq.max - 1)) <<\r\nqp->sq.wqe_shift) +\r\nqp->send_wqe_offset);\r\n}\r\n} else {\r\nfor (i = 0; i < qp->rq.max; ++i) {\r\nnext = get_recv_wqe(qp, i);\r\nnext->nda_op = htonl((((i + 1) % qp->rq.max) <<\r\nqp->rq.wqe_shift) | 1);\r\n}\r\n}\r\nqp->sq.last = get_send_wqe(qp, qp->sq.max - 1);\r\nqp->rq.last = get_recv_wqe(qp, qp->rq.max - 1);\r\nreturn 0;\r\n}\r\nstatic int mthca_set_qp_size(struct mthca_dev *dev, struct ib_qp_cap *cap,\r\nstruct mthca_pd *pd, struct mthca_qp *qp)\r\n{\r\nint max_data_size = mthca_max_data_size(dev, qp, dev->limits.max_desc_sz);\r\nif (cap->max_send_wr > dev->limits.max_wqes ||\r\ncap->max_recv_wr > dev->limits.max_wqes ||\r\ncap->max_send_sge > dev->limits.max_sg ||\r\ncap->max_recv_sge > dev->limits.max_sg ||\r\ncap->max_inline_data > mthca_max_inline_data(pd, max_data_size))\r\nreturn -EINVAL;\r\nif (qp->transport == MLX && cap->max_send_sge + 2 > dev->limits.max_sg)\r\nreturn -EINVAL;\r\nif (mthca_is_memfree(dev)) {\r\nqp->rq.max = cap->max_recv_wr ?\r\nroundup_pow_of_two(cap->max_recv_wr) : 0;\r\nqp->sq.max = cap->max_send_wr ?\r\nroundup_pow_of_two(cap->max_send_wr) : 0;\r\n} else {\r\nqp->rq.max = cap->max_recv_wr;\r\nqp->sq.max = cap->max_send_wr;\r\n}\r\nqp->rq.max_gs = cap->max_recv_sge;\r\nqp->sq.max_gs = max_t(int, cap->max_send_sge,\r\nALIGN(cap->max_inline_data + MTHCA_INLINE_HEADER_SIZE,\r\nMTHCA_INLINE_CHUNK_SIZE) /\r\nsizeof (struct mthca_data_seg));\r\nreturn 0;\r\n}\r\nint mthca_alloc_qp(struct mthca_dev *dev,\r\nstruct mthca_pd *pd,\r\nstruct mthca_cq *send_cq,\r\nstruct mthca_cq *recv_cq,\r\nenum ib_qp_type type,\r\nenum ib_sig_type send_policy,\r\nstruct ib_qp_cap *cap,\r\nstruct mthca_qp *qp)\r\n{\r\nint err;\r\nswitch (type) {\r\ncase IB_QPT_RC: qp->transport = RC; break;\r\ncase IB_QPT_UC: qp->transport = UC; break;\r\ncase IB_QPT_UD: qp->transport = UD; break;\r\ndefault: return -EINVAL;\r\n}\r\nerr = mthca_set_qp_size(dev, cap, pd, qp);\r\nif (err)\r\nreturn err;\r\nqp->qpn = mthca_alloc(&dev->qp_table.alloc);\r\nif (qp->qpn == -1)\r\nreturn -ENOMEM;\r\nqp->port = 0;\r\nerr = mthca_alloc_qp_common(dev, pd, send_cq, recv_cq,\r\nsend_policy, qp);\r\nif (err) {\r\nmthca_free(&dev->qp_table.alloc, qp->qpn);\r\nreturn err;\r\n}\r\nspin_lock_irq(&dev->qp_table.lock);\r\nmthca_array_set(&dev->qp_table.qp,\r\nqp->qpn & (dev->limits.num_qps - 1), qp);\r\nspin_unlock_irq(&dev->qp_table.lock);\r\nreturn 0;\r\n}\r\nstatic void mthca_lock_cqs(struct mthca_cq *send_cq, struct mthca_cq *recv_cq)\r\n__acquires(&send_cq->lock\r\nstatic void mthca_unlock_cqs(struct mthca_cq *send_cq, struct mthca_cq *recv_cq)\r\n__releases(&send_cq->lock\r\nint mthca_alloc_sqp(struct mthca_dev *dev,\r\nstruct mthca_pd *pd,\r\nstruct mthca_cq *send_cq,\r\nstruct mthca_cq *recv_cq,\r\nenum ib_sig_type send_policy,\r\nstruct ib_qp_cap *cap,\r\nint qpn,\r\nint port,\r\nstruct mthca_sqp *sqp)\r\n{\r\nu32 mqpn = qpn * 2 + dev->qp_table.sqp_start + port - 1;\r\nint err;\r\nsqp->qp.transport = MLX;\r\nerr = mthca_set_qp_size(dev, cap, pd, &sqp->qp);\r\nif (err)\r\nreturn err;\r\nsqp->header_buf_size = sqp->qp.sq.max * MTHCA_UD_HEADER_SIZE;\r\nsqp->header_buf = dma_alloc_coherent(&dev->pdev->dev, sqp->header_buf_size,\r\n&sqp->header_dma, GFP_KERNEL);\r\nif (!sqp->header_buf)\r\nreturn -ENOMEM;\r\nspin_lock_irq(&dev->qp_table.lock);\r\nif (mthca_array_get(&dev->qp_table.qp, mqpn))\r\nerr = -EBUSY;\r\nelse\r\nmthca_array_set(&dev->qp_table.qp, mqpn, sqp);\r\nspin_unlock_irq(&dev->qp_table.lock);\r\nif (err)\r\ngoto err_out;\r\nsqp->qp.port = port;\r\nsqp->qp.qpn = mqpn;\r\nsqp->qp.transport = MLX;\r\nerr = mthca_alloc_qp_common(dev, pd, send_cq, recv_cq,\r\nsend_policy, &sqp->qp);\r\nif (err)\r\ngoto err_out_free;\r\natomic_inc(&pd->sqp_count);\r\nreturn 0;\r\nerr_out_free:\r\nmthca_lock_cqs(send_cq, recv_cq);\r\nspin_lock(&dev->qp_table.lock);\r\nmthca_array_clear(&dev->qp_table.qp, mqpn);\r\nspin_unlock(&dev->qp_table.lock);\r\nmthca_unlock_cqs(send_cq, recv_cq);\r\nerr_out:\r\ndma_free_coherent(&dev->pdev->dev, sqp->header_buf_size,\r\nsqp->header_buf, sqp->header_dma);\r\nreturn err;\r\n}\r\nstatic inline int get_qp_refcount(struct mthca_dev *dev, struct mthca_qp *qp)\r\n{\r\nint c;\r\nspin_lock_irq(&dev->qp_table.lock);\r\nc = qp->refcount;\r\nspin_unlock_irq(&dev->qp_table.lock);\r\nreturn c;\r\n}\r\nvoid mthca_free_qp(struct mthca_dev *dev,\r\nstruct mthca_qp *qp)\r\n{\r\nstruct mthca_cq *send_cq;\r\nstruct mthca_cq *recv_cq;\r\nsend_cq = to_mcq(qp->ibqp.send_cq);\r\nrecv_cq = to_mcq(qp->ibqp.recv_cq);\r\nmthca_lock_cqs(send_cq, recv_cq);\r\nspin_lock(&dev->qp_table.lock);\r\nmthca_array_clear(&dev->qp_table.qp,\r\nqp->qpn & (dev->limits.num_qps - 1));\r\n--qp->refcount;\r\nspin_unlock(&dev->qp_table.lock);\r\nmthca_unlock_cqs(send_cq, recv_cq);\r\nwait_event(qp->wait, !get_qp_refcount(dev, qp));\r\nif (qp->state != IB_QPS_RESET)\r\nmthca_MODIFY_QP(dev, qp->state, IB_QPS_RESET, qp->qpn, 0,\r\nNULL, 0);\r\nif (!qp->ibqp.uobject) {\r\nmthca_cq_clean(dev, recv_cq, qp->qpn,\r\nqp->ibqp.srq ? to_msrq(qp->ibqp.srq) : NULL);\r\nif (send_cq != recv_cq)\r\nmthca_cq_clean(dev, send_cq, qp->qpn, NULL);\r\nmthca_free_memfree(dev, qp);\r\nmthca_free_wqe_buf(dev, qp);\r\n}\r\nmthca_unmap_memfree(dev, qp);\r\nif (is_sqp(dev, qp)) {\r\natomic_dec(&(to_mpd(qp->ibqp.pd)->sqp_count));\r\ndma_free_coherent(&dev->pdev->dev,\r\nto_msqp(qp)->header_buf_size,\r\nto_msqp(qp)->header_buf,\r\nto_msqp(qp)->header_dma);\r\n} else\r\nmthca_free(&dev->qp_table.alloc, qp->qpn);\r\n}\r\nstatic int build_mlx_header(struct mthca_dev *dev, struct mthca_sqp *sqp,\r\nint ind, struct ib_send_wr *wr,\r\nstruct mthca_mlx_seg *mlx,\r\nstruct mthca_data_seg *data)\r\n{\r\nint header_size;\r\nint err;\r\nu16 pkey;\r\nib_ud_header_init(256, 1, 0, 0,\r\nmthca_ah_grh_present(to_mah(wr->wr.ud.ah)), 0,\r\n&sqp->ud_header);\r\nerr = mthca_read_ah(dev, to_mah(wr->wr.ud.ah), &sqp->ud_header);\r\nif (err)\r\nreturn err;\r\nmlx->flags &= ~cpu_to_be32(MTHCA_NEXT_SOLICIT | 1);\r\nmlx->flags |= cpu_to_be32((!sqp->qp.ibqp.qp_num ? MTHCA_MLX_VL15 : 0) |\r\n(sqp->ud_header.lrh.destination_lid ==\r\nIB_LID_PERMISSIVE ? MTHCA_MLX_SLR : 0) |\r\n(sqp->ud_header.lrh.service_level << 8));\r\nmlx->rlid = sqp->ud_header.lrh.destination_lid;\r\nmlx->vcrc = 0;\r\nswitch (wr->opcode) {\r\ncase IB_WR_SEND:\r\nsqp->ud_header.bth.opcode = IB_OPCODE_UD_SEND_ONLY;\r\nsqp->ud_header.immediate_present = 0;\r\nbreak;\r\ncase IB_WR_SEND_WITH_IMM:\r\nsqp->ud_header.bth.opcode = IB_OPCODE_UD_SEND_ONLY_WITH_IMMEDIATE;\r\nsqp->ud_header.immediate_present = 1;\r\nsqp->ud_header.immediate_data = wr->ex.imm_data;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nsqp->ud_header.lrh.virtual_lane = !sqp->qp.ibqp.qp_num ? 15 : 0;\r\nif (sqp->ud_header.lrh.destination_lid == IB_LID_PERMISSIVE)\r\nsqp->ud_header.lrh.source_lid = IB_LID_PERMISSIVE;\r\nsqp->ud_header.bth.solicited_event = !!(wr->send_flags & IB_SEND_SOLICITED);\r\nif (!sqp->qp.ibqp.qp_num)\r\nib_get_cached_pkey(&dev->ib_dev, sqp->qp.port,\r\nsqp->pkey_index, &pkey);\r\nelse\r\nib_get_cached_pkey(&dev->ib_dev, sqp->qp.port,\r\nwr->wr.ud.pkey_index, &pkey);\r\nsqp->ud_header.bth.pkey = cpu_to_be16(pkey);\r\nsqp->ud_header.bth.destination_qpn = cpu_to_be32(wr->wr.ud.remote_qpn);\r\nsqp->ud_header.bth.psn = cpu_to_be32((sqp->send_psn++) & ((1 << 24) - 1));\r\nsqp->ud_header.deth.qkey = cpu_to_be32(wr->wr.ud.remote_qkey & 0x80000000 ?\r\nsqp->qkey : wr->wr.ud.remote_qkey);\r\nsqp->ud_header.deth.source_qpn = cpu_to_be32(sqp->qp.ibqp.qp_num);\r\nheader_size = ib_ud_header_pack(&sqp->ud_header,\r\nsqp->header_buf +\r\nind * MTHCA_UD_HEADER_SIZE);\r\ndata->byte_count = cpu_to_be32(header_size);\r\ndata->lkey = cpu_to_be32(to_mpd(sqp->qp.ibqp.pd)->ntmr.ibmr.lkey);\r\ndata->addr = cpu_to_be64(sqp->header_dma +\r\nind * MTHCA_UD_HEADER_SIZE);\r\nreturn 0;\r\n}\r\nstatic inline int mthca_wq_overflow(struct mthca_wq *wq, int nreq,\r\nstruct ib_cq *ib_cq)\r\n{\r\nunsigned cur;\r\nstruct mthca_cq *cq;\r\ncur = wq->head - wq->tail;\r\nif (likely(cur + nreq < wq->max))\r\nreturn 0;\r\ncq = to_mcq(ib_cq);\r\nspin_lock(&cq->lock);\r\ncur = wq->head - wq->tail;\r\nspin_unlock(&cq->lock);\r\nreturn cur + nreq >= wq->max;\r\n}\r\nstatic __always_inline void set_raddr_seg(struct mthca_raddr_seg *rseg,\r\nu64 remote_addr, u32 rkey)\r\n{\r\nrseg->raddr = cpu_to_be64(remote_addr);\r\nrseg->rkey = cpu_to_be32(rkey);\r\nrseg->reserved = 0;\r\n}\r\nstatic __always_inline void set_atomic_seg(struct mthca_atomic_seg *aseg,\r\nstruct ib_send_wr *wr)\r\n{\r\nif (wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP) {\r\naseg->swap_add = cpu_to_be64(wr->wr.atomic.swap);\r\naseg->compare = cpu_to_be64(wr->wr.atomic.compare_add);\r\n} else {\r\naseg->swap_add = cpu_to_be64(wr->wr.atomic.compare_add);\r\naseg->compare = 0;\r\n}\r\n}\r\nstatic void set_tavor_ud_seg(struct mthca_tavor_ud_seg *useg,\r\nstruct ib_send_wr *wr)\r\n{\r\nuseg->lkey = cpu_to_be32(to_mah(wr->wr.ud.ah)->key);\r\nuseg->av_addr = cpu_to_be64(to_mah(wr->wr.ud.ah)->avdma);\r\nuseg->dqpn = cpu_to_be32(wr->wr.ud.remote_qpn);\r\nuseg->qkey = cpu_to_be32(wr->wr.ud.remote_qkey);\r\n}\r\nstatic void set_arbel_ud_seg(struct mthca_arbel_ud_seg *useg,\r\nstruct ib_send_wr *wr)\r\n{\r\nmemcpy(useg->av, to_mah(wr->wr.ud.ah)->av, MTHCA_AV_SIZE);\r\nuseg->dqpn = cpu_to_be32(wr->wr.ud.remote_qpn);\r\nuseg->qkey = cpu_to_be32(wr->wr.ud.remote_qkey);\r\n}\r\nint mthca_tavor_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,\r\nstruct ib_send_wr **bad_wr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibqp->device);\r\nstruct mthca_qp *qp = to_mqp(ibqp);\r\nvoid *wqe;\r\nvoid *prev_wqe;\r\nunsigned long flags;\r\nint err = 0;\r\nint nreq;\r\nint i;\r\nint size;\r\nint uninitialized_var(size0);\r\nu32 uninitialized_var(f0);\r\nint ind;\r\nu8 op0 = 0;\r\nspin_lock_irqsave(&qp->sq.lock, flags);\r\nind = qp->sq.next_ind;\r\nfor (nreq = 0; wr; ++nreq, wr = wr->next) {\r\nif (mthca_wq_overflow(&qp->sq, nreq, qp->ibqp.send_cq)) {\r\nmthca_err(dev, "SQ %06x full (%u head, %u tail,"\r\n" %d max, %d nreq)\n", qp->qpn,\r\nqp->sq.head, qp->sq.tail,\r\nqp->sq.max, nreq);\r\nerr = -ENOMEM;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nwqe = get_send_wqe(qp, ind);\r\nprev_wqe = qp->sq.last;\r\nqp->sq.last = wqe;\r\n((struct mthca_next_seg *) wqe)->nda_op = 0;\r\n((struct mthca_next_seg *) wqe)->ee_nds = 0;\r\n((struct mthca_next_seg *) wqe)->flags =\r\n((wr->send_flags & IB_SEND_SIGNALED) ?\r\ncpu_to_be32(MTHCA_NEXT_CQ_UPDATE) : 0) |\r\n((wr->send_flags & IB_SEND_SOLICITED) ?\r\ncpu_to_be32(MTHCA_NEXT_SOLICIT) : 0) |\r\ncpu_to_be32(1);\r\nif (wr->opcode == IB_WR_SEND_WITH_IMM ||\r\nwr->opcode == IB_WR_RDMA_WRITE_WITH_IMM)\r\n((struct mthca_next_seg *) wqe)->imm = wr->ex.imm_data;\r\nwqe += sizeof (struct mthca_next_seg);\r\nsize = sizeof (struct mthca_next_seg) / 16;\r\nswitch (qp->transport) {\r\ncase RC:\r\nswitch (wr->opcode) {\r\ncase IB_WR_ATOMIC_CMP_AND_SWP:\r\ncase IB_WR_ATOMIC_FETCH_AND_ADD:\r\nset_raddr_seg(wqe, wr->wr.atomic.remote_addr,\r\nwr->wr.atomic.rkey);\r\nwqe += sizeof (struct mthca_raddr_seg);\r\nset_atomic_seg(wqe, wr);\r\nwqe += sizeof (struct mthca_atomic_seg);\r\nsize += (sizeof (struct mthca_raddr_seg) +\r\nsizeof (struct mthca_atomic_seg)) / 16;\r\nbreak;\r\ncase IB_WR_RDMA_WRITE:\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\ncase IB_WR_RDMA_READ:\r\nset_raddr_seg(wqe, wr->wr.rdma.remote_addr,\r\nwr->wr.rdma.rkey);\r\nwqe += sizeof (struct mthca_raddr_seg);\r\nsize += sizeof (struct mthca_raddr_seg) / 16;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase UC:\r\nswitch (wr->opcode) {\r\ncase IB_WR_RDMA_WRITE:\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nset_raddr_seg(wqe, wr->wr.rdma.remote_addr,\r\nwr->wr.rdma.rkey);\r\nwqe += sizeof (struct mthca_raddr_seg);\r\nsize += sizeof (struct mthca_raddr_seg) / 16;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase UD:\r\nset_tavor_ud_seg(wqe, wr);\r\nwqe += sizeof (struct mthca_tavor_ud_seg);\r\nsize += sizeof (struct mthca_tavor_ud_seg) / 16;\r\nbreak;\r\ncase MLX:\r\nerr = build_mlx_header(dev, to_msqp(qp), ind, wr,\r\nwqe - sizeof (struct mthca_next_seg),\r\nwqe);\r\nif (err) {\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nwqe += sizeof (struct mthca_data_seg);\r\nsize += sizeof (struct mthca_data_seg) / 16;\r\nbreak;\r\n}\r\nif (wr->num_sge > qp->sq.max_gs) {\r\nmthca_err(dev, "too many gathers\n");\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nfor (i = 0; i < wr->num_sge; ++i) {\r\nmthca_set_data_seg(wqe, wr->sg_list + i);\r\nwqe += sizeof (struct mthca_data_seg);\r\nsize += sizeof (struct mthca_data_seg) / 16;\r\n}\r\nif (qp->transport == MLX) {\r\n((struct mthca_data_seg *) wqe)->byte_count =\r\ncpu_to_be32((1 << 31) | 4);\r\n((u32 *) wqe)[1] = 0;\r\nwqe += sizeof (struct mthca_data_seg);\r\nsize += sizeof (struct mthca_data_seg) / 16;\r\n}\r\nqp->wrid[ind + qp->rq.max] = wr->wr_id;\r\nif (wr->opcode >= ARRAY_SIZE(mthca_opcode)) {\r\nmthca_err(dev, "opcode invalid\n");\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\n((struct mthca_next_seg *) prev_wqe)->nda_op =\r\ncpu_to_be32(((ind << qp->sq.wqe_shift) +\r\nqp->send_wqe_offset) |\r\nmthca_opcode[wr->opcode]);\r\nwmb();\r\n((struct mthca_next_seg *) prev_wqe)->ee_nds =\r\ncpu_to_be32((nreq ? 0 : MTHCA_NEXT_DBD) | size |\r\n((wr->send_flags & IB_SEND_FENCE) ?\r\nMTHCA_NEXT_FENCE : 0));\r\nif (!nreq) {\r\nsize0 = size;\r\nop0 = mthca_opcode[wr->opcode];\r\nf0 = wr->send_flags & IB_SEND_FENCE ?\r\nMTHCA_SEND_DOORBELL_FENCE : 0;\r\n}\r\n++ind;\r\nif (unlikely(ind >= qp->sq.max))\r\nind -= qp->sq.max;\r\n}\r\nout:\r\nif (likely(nreq)) {\r\nwmb();\r\nmthca_write64(((qp->sq.next_ind << qp->sq.wqe_shift) +\r\nqp->send_wqe_offset) | f0 | op0,\r\n(qp->qpn << 8) | size0,\r\ndev->kar + MTHCA_SEND_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\nmmiowb();\r\n}\r\nqp->sq.next_ind = ind;\r\nqp->sq.head += nreq;\r\nspin_unlock_irqrestore(&qp->sq.lock, flags);\r\nreturn err;\r\n}\r\nint mthca_tavor_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibqp->device);\r\nstruct mthca_qp *qp = to_mqp(ibqp);\r\nunsigned long flags;\r\nint err = 0;\r\nint nreq;\r\nint i;\r\nint size;\r\nint uninitialized_var(size0);\r\nint ind;\r\nvoid *wqe;\r\nvoid *prev_wqe;\r\nspin_lock_irqsave(&qp->rq.lock, flags);\r\nind = qp->rq.next_ind;\r\nfor (nreq = 0; wr; wr = wr->next) {\r\nif (mthca_wq_overflow(&qp->rq, nreq, qp->ibqp.recv_cq)) {\r\nmthca_err(dev, "RQ %06x full (%u head, %u tail,"\r\n" %d max, %d nreq)\n", qp->qpn,\r\nqp->rq.head, qp->rq.tail,\r\nqp->rq.max, nreq);\r\nerr = -ENOMEM;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nwqe = get_recv_wqe(qp, ind);\r\nprev_wqe = qp->rq.last;\r\nqp->rq.last = wqe;\r\n((struct mthca_next_seg *) wqe)->ee_nds =\r\ncpu_to_be32(MTHCA_NEXT_DBD);\r\n((struct mthca_next_seg *) wqe)->flags = 0;\r\nwqe += sizeof (struct mthca_next_seg);\r\nsize = sizeof (struct mthca_next_seg) / 16;\r\nif (unlikely(wr->num_sge > qp->rq.max_gs)) {\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nfor (i = 0; i < wr->num_sge; ++i) {\r\nmthca_set_data_seg(wqe, wr->sg_list + i);\r\nwqe += sizeof (struct mthca_data_seg);\r\nsize += sizeof (struct mthca_data_seg) / 16;\r\n}\r\nqp->wrid[ind] = wr->wr_id;\r\n((struct mthca_next_seg *) prev_wqe)->ee_nds =\r\ncpu_to_be32(MTHCA_NEXT_DBD | size);\r\nif (!nreq)\r\nsize0 = size;\r\n++ind;\r\nif (unlikely(ind >= qp->rq.max))\r\nind -= qp->rq.max;\r\n++nreq;\r\nif (unlikely(nreq == MTHCA_TAVOR_MAX_WQES_PER_RECV_DB)) {\r\nnreq = 0;\r\nwmb();\r\nmthca_write64((qp->rq.next_ind << qp->rq.wqe_shift) | size0,\r\nqp->qpn << 8, dev->kar + MTHCA_RECEIVE_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\nqp->rq.next_ind = ind;\r\nqp->rq.head += MTHCA_TAVOR_MAX_WQES_PER_RECV_DB;\r\n}\r\n}\r\nout:\r\nif (likely(nreq)) {\r\nwmb();\r\nmthca_write64((qp->rq.next_ind << qp->rq.wqe_shift) | size0,\r\nqp->qpn << 8 | nreq, dev->kar + MTHCA_RECEIVE_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\n}\r\nqp->rq.next_ind = ind;\r\nqp->rq.head += nreq;\r\nmmiowb();\r\nspin_unlock_irqrestore(&qp->rq.lock, flags);\r\nreturn err;\r\n}\r\nint mthca_arbel_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,\r\nstruct ib_send_wr **bad_wr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibqp->device);\r\nstruct mthca_qp *qp = to_mqp(ibqp);\r\nu32 dbhi;\r\nvoid *wqe;\r\nvoid *prev_wqe;\r\nunsigned long flags;\r\nint err = 0;\r\nint nreq;\r\nint i;\r\nint size;\r\nint uninitialized_var(size0);\r\nu32 uninitialized_var(f0);\r\nint ind;\r\nu8 op0 = 0;\r\nspin_lock_irqsave(&qp->sq.lock, flags);\r\nind = qp->sq.head & (qp->sq.max - 1);\r\nfor (nreq = 0; wr; ++nreq, wr = wr->next) {\r\nif (unlikely(nreq == MTHCA_ARBEL_MAX_WQES_PER_SEND_DB)) {\r\nnreq = 0;\r\ndbhi = (MTHCA_ARBEL_MAX_WQES_PER_SEND_DB << 24) |\r\n((qp->sq.head & 0xffff) << 8) | f0 | op0;\r\nqp->sq.head += MTHCA_ARBEL_MAX_WQES_PER_SEND_DB;\r\nwmb();\r\n*qp->sq.db = cpu_to_be32(qp->sq.head & 0xffff);\r\nwmb();\r\nmthca_write64(dbhi, (qp->qpn << 8) | size0,\r\ndev->kar + MTHCA_SEND_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\n}\r\nif (mthca_wq_overflow(&qp->sq, nreq, qp->ibqp.send_cq)) {\r\nmthca_err(dev, "SQ %06x full (%u head, %u tail,"\r\n" %d max, %d nreq)\n", qp->qpn,\r\nqp->sq.head, qp->sq.tail,\r\nqp->sq.max, nreq);\r\nerr = -ENOMEM;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nwqe = get_send_wqe(qp, ind);\r\nprev_wqe = qp->sq.last;\r\nqp->sq.last = wqe;\r\n((struct mthca_next_seg *) wqe)->flags =\r\n((wr->send_flags & IB_SEND_SIGNALED) ?\r\ncpu_to_be32(MTHCA_NEXT_CQ_UPDATE) : 0) |\r\n((wr->send_flags & IB_SEND_SOLICITED) ?\r\ncpu_to_be32(MTHCA_NEXT_SOLICIT) : 0) |\r\n((wr->send_flags & IB_SEND_IP_CSUM) ?\r\ncpu_to_be32(MTHCA_NEXT_IP_CSUM | MTHCA_NEXT_TCP_UDP_CSUM) : 0) |\r\ncpu_to_be32(1);\r\nif (wr->opcode == IB_WR_SEND_WITH_IMM ||\r\nwr->opcode == IB_WR_RDMA_WRITE_WITH_IMM)\r\n((struct mthca_next_seg *) wqe)->imm = wr->ex.imm_data;\r\nwqe += sizeof (struct mthca_next_seg);\r\nsize = sizeof (struct mthca_next_seg) / 16;\r\nswitch (qp->transport) {\r\ncase RC:\r\nswitch (wr->opcode) {\r\ncase IB_WR_ATOMIC_CMP_AND_SWP:\r\ncase IB_WR_ATOMIC_FETCH_AND_ADD:\r\nset_raddr_seg(wqe, wr->wr.atomic.remote_addr,\r\nwr->wr.atomic.rkey);\r\nwqe += sizeof (struct mthca_raddr_seg);\r\nset_atomic_seg(wqe, wr);\r\nwqe += sizeof (struct mthca_atomic_seg);\r\nsize += (sizeof (struct mthca_raddr_seg) +\r\nsizeof (struct mthca_atomic_seg)) / 16;\r\nbreak;\r\ncase IB_WR_RDMA_READ:\r\ncase IB_WR_RDMA_WRITE:\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nset_raddr_seg(wqe, wr->wr.rdma.remote_addr,\r\nwr->wr.rdma.rkey);\r\nwqe += sizeof (struct mthca_raddr_seg);\r\nsize += sizeof (struct mthca_raddr_seg) / 16;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase UC:\r\nswitch (wr->opcode) {\r\ncase IB_WR_RDMA_WRITE:\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nset_raddr_seg(wqe, wr->wr.rdma.remote_addr,\r\nwr->wr.rdma.rkey);\r\nwqe += sizeof (struct mthca_raddr_seg);\r\nsize += sizeof (struct mthca_raddr_seg) / 16;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase UD:\r\nset_arbel_ud_seg(wqe, wr);\r\nwqe += sizeof (struct mthca_arbel_ud_seg);\r\nsize += sizeof (struct mthca_arbel_ud_seg) / 16;\r\nbreak;\r\ncase MLX:\r\nerr = build_mlx_header(dev, to_msqp(qp), ind, wr,\r\nwqe - sizeof (struct mthca_next_seg),\r\nwqe);\r\nif (err) {\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nwqe += sizeof (struct mthca_data_seg);\r\nsize += sizeof (struct mthca_data_seg) / 16;\r\nbreak;\r\n}\r\nif (wr->num_sge > qp->sq.max_gs) {\r\nmthca_err(dev, "too many gathers\n");\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nfor (i = 0; i < wr->num_sge; ++i) {\r\nmthca_set_data_seg(wqe, wr->sg_list + i);\r\nwqe += sizeof (struct mthca_data_seg);\r\nsize += sizeof (struct mthca_data_seg) / 16;\r\n}\r\nif (qp->transport == MLX) {\r\n((struct mthca_data_seg *) wqe)->byte_count =\r\ncpu_to_be32((1 << 31) | 4);\r\n((u32 *) wqe)[1] = 0;\r\nwqe += sizeof (struct mthca_data_seg);\r\nsize += sizeof (struct mthca_data_seg) / 16;\r\n}\r\nqp->wrid[ind + qp->rq.max] = wr->wr_id;\r\nif (wr->opcode >= ARRAY_SIZE(mthca_opcode)) {\r\nmthca_err(dev, "opcode invalid\n");\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\n((struct mthca_next_seg *) prev_wqe)->nda_op =\r\ncpu_to_be32(((ind << qp->sq.wqe_shift) +\r\nqp->send_wqe_offset) |\r\nmthca_opcode[wr->opcode]);\r\nwmb();\r\n((struct mthca_next_seg *) prev_wqe)->ee_nds =\r\ncpu_to_be32(MTHCA_NEXT_DBD | size |\r\n((wr->send_flags & IB_SEND_FENCE) ?\r\nMTHCA_NEXT_FENCE : 0));\r\nif (!nreq) {\r\nsize0 = size;\r\nop0 = mthca_opcode[wr->opcode];\r\nf0 = wr->send_flags & IB_SEND_FENCE ?\r\nMTHCA_SEND_DOORBELL_FENCE : 0;\r\n}\r\n++ind;\r\nif (unlikely(ind >= qp->sq.max))\r\nind -= qp->sq.max;\r\n}\r\nout:\r\nif (likely(nreq)) {\r\ndbhi = (nreq << 24) | ((qp->sq.head & 0xffff) << 8) | f0 | op0;\r\nqp->sq.head += nreq;\r\nwmb();\r\n*qp->sq.db = cpu_to_be32(qp->sq.head & 0xffff);\r\nwmb();\r\nmthca_write64(dbhi, (qp->qpn << 8) | size0, dev->kar + MTHCA_SEND_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\n}\r\nmmiowb();\r\nspin_unlock_irqrestore(&qp->sq.lock, flags);\r\nreturn err;\r\n}\r\nint mthca_arbel_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibqp->device);\r\nstruct mthca_qp *qp = to_mqp(ibqp);\r\nunsigned long flags;\r\nint err = 0;\r\nint nreq;\r\nint ind;\r\nint i;\r\nvoid *wqe;\r\nspin_lock_irqsave(&qp->rq.lock, flags);\r\nind = qp->rq.head & (qp->rq.max - 1);\r\nfor (nreq = 0; wr; ++nreq, wr = wr->next) {\r\nif (mthca_wq_overflow(&qp->rq, nreq, qp->ibqp.recv_cq)) {\r\nmthca_err(dev, "RQ %06x full (%u head, %u tail,"\r\n" %d max, %d nreq)\n", qp->qpn,\r\nqp->rq.head, qp->rq.tail,\r\nqp->rq.max, nreq);\r\nerr = -ENOMEM;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nwqe = get_recv_wqe(qp, ind);\r\n((struct mthca_next_seg *) wqe)->flags = 0;\r\nwqe += sizeof (struct mthca_next_seg);\r\nif (unlikely(wr->num_sge > qp->rq.max_gs)) {\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nfor (i = 0; i < wr->num_sge; ++i) {\r\nmthca_set_data_seg(wqe, wr->sg_list + i);\r\nwqe += sizeof (struct mthca_data_seg);\r\n}\r\nif (i < qp->rq.max_gs)\r\nmthca_set_data_seg_inval(wqe);\r\nqp->wrid[ind] = wr->wr_id;\r\n++ind;\r\nif (unlikely(ind >= qp->rq.max))\r\nind -= qp->rq.max;\r\n}\r\nout:\r\nif (likely(nreq)) {\r\nqp->rq.head += nreq;\r\nwmb();\r\n*qp->rq.db = cpu_to_be32(qp->rq.head & 0xffff);\r\n}\r\nspin_unlock_irqrestore(&qp->rq.lock, flags);\r\nreturn err;\r\n}\r\nvoid mthca_free_err_wqe(struct mthca_dev *dev, struct mthca_qp *qp, int is_send,\r\nint index, int *dbd, __be32 *new_wqe)\r\n{\r\nstruct mthca_next_seg *next;\r\nif (qp->ibqp.srq && !is_send) {\r\n*new_wqe = 0;\r\nreturn;\r\n}\r\nif (is_send)\r\nnext = get_send_wqe(qp, index);\r\nelse\r\nnext = get_recv_wqe(qp, index);\r\n*dbd = !!(next->ee_nds & cpu_to_be32(MTHCA_NEXT_DBD));\r\nif (next->ee_nds & cpu_to_be32(0x3f))\r\n*new_wqe = (next->nda_op & cpu_to_be32(~0x3f)) |\r\n(next->ee_nds & cpu_to_be32(0x3f));\r\nelse\r\n*new_wqe = 0;\r\n}\r\nint mthca_init_qp_table(struct mthca_dev *dev)\r\n{\r\nint err;\r\nint i;\r\nspin_lock_init(&dev->qp_table.lock);\r\ndev->qp_table.sqp_start = (dev->limits.reserved_qps + 1) & ~1UL;\r\nerr = mthca_alloc_init(&dev->qp_table.alloc,\r\ndev->limits.num_qps,\r\n(1 << 24) - 1,\r\ndev->qp_table.sqp_start +\r\nMTHCA_MAX_PORTS * 2);\r\nif (err)\r\nreturn err;\r\nerr = mthca_array_init(&dev->qp_table.qp,\r\ndev->limits.num_qps);\r\nif (err) {\r\nmthca_alloc_cleanup(&dev->qp_table.alloc);\r\nreturn err;\r\n}\r\nfor (i = 0; i < 2; ++i) {\r\nerr = mthca_CONF_SPECIAL_QP(dev, i ? IB_QPT_GSI : IB_QPT_SMI,\r\ndev->qp_table.sqp_start + i * 2);\r\nif (err) {\r\nmthca_warn(dev, "CONF_SPECIAL_QP returned "\r\n"%d, aborting.\n", err);\r\ngoto err_out;\r\n}\r\n}\r\nreturn 0;\r\nerr_out:\r\nfor (i = 0; i < 2; ++i)\r\nmthca_CONF_SPECIAL_QP(dev, i, 0);\r\nmthca_array_cleanup(&dev->qp_table.qp, dev->limits.num_qps);\r\nmthca_alloc_cleanup(&dev->qp_table.alloc);\r\nreturn err;\r\n}\r\nvoid mthca_cleanup_qp_table(struct mthca_dev *dev)\r\n{\r\nint i;\r\nfor (i = 0; i < 2; ++i)\r\nmthca_CONF_SPECIAL_QP(dev, i, 0);\r\nmthca_array_cleanup(&dev->qp_table.qp, dev->limits.num_qps);\r\nmthca_alloc_cleanup(&dev->qp_table.alloc);\r\n}
