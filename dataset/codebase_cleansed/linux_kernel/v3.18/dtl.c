static void consume_dtle(struct dtl_entry *dtle, u64 index)\r\n{\r\nstruct dtl_ring *dtlr = &__get_cpu_var(dtl_rings);\r\nstruct dtl_entry *wp = dtlr->write_ptr;\r\nstruct lppaca *vpa = local_paca->lppaca_ptr;\r\nif (!wp)\r\nreturn;\r\n*wp = *dtle;\r\nbarrier();\r\nif (index + N_DISPATCH_LOG < be64_to_cpu(vpa->dtl_idx))\r\nreturn;\r\n++wp;\r\nif (wp == dtlr->buf_end)\r\nwp = dtlr->buf;\r\ndtlr->write_ptr = wp;\r\nsmp_wmb();\r\n++dtlr->write_index;\r\n}\r\nstatic int dtl_start(struct dtl *dtl)\r\n{\r\nstruct dtl_ring *dtlr = &per_cpu(dtl_rings, dtl->cpu);\r\ndtlr->buf = dtl->buf;\r\ndtlr->buf_end = dtl->buf + dtl->buf_entries;\r\ndtlr->write_index = 0;\r\nsmp_wmb();\r\ndtlr->write_ptr = dtl->buf;\r\ndtlr->saved_dtl_mask = lppaca_of(dtl->cpu).dtl_enable_mask;\r\nlppaca_of(dtl->cpu).dtl_enable_mask |= dtl_event_mask;\r\ndtl_consumer = consume_dtle;\r\natomic_inc(&dtl_count);\r\nreturn 0;\r\n}\r\nstatic void dtl_stop(struct dtl *dtl)\r\n{\r\nstruct dtl_ring *dtlr = &per_cpu(dtl_rings, dtl->cpu);\r\ndtlr->write_ptr = NULL;\r\nsmp_wmb();\r\ndtlr->buf = NULL;\r\nlppaca_of(dtl->cpu).dtl_enable_mask = dtlr->saved_dtl_mask;\r\nif (atomic_dec_and_test(&dtl_count))\r\ndtl_consumer = NULL;\r\n}\r\nstatic u64 dtl_current_index(struct dtl *dtl)\r\n{\r\nreturn per_cpu(dtl_rings, dtl->cpu).write_index;\r\n}\r\nstatic int dtl_start(struct dtl *dtl)\r\n{\r\nunsigned long addr;\r\nint ret, hwcpu;\r\n((u32 *)dtl->buf)[1] = DISPATCH_LOG_BYTES;\r\nhwcpu = get_hard_smp_processor_id(dtl->cpu);\r\naddr = __pa(dtl->buf);\r\nret = register_dtl(hwcpu, addr);\r\nif (ret) {\r\nprintk(KERN_WARNING "%s: DTL registration for cpu %d (hw %d) "\r\n"failed with %d\n", __func__, dtl->cpu, hwcpu, ret);\r\nreturn -EIO;\r\n}\r\nlppaca_of(dtl->cpu).dtl_idx = 0;\r\nsmp_wmb();\r\nlppaca_of(dtl->cpu).dtl_enable_mask = dtl_event_mask;\r\nreturn 0;\r\n}\r\nstatic void dtl_stop(struct dtl *dtl)\r\n{\r\nint hwcpu = get_hard_smp_processor_id(dtl->cpu);\r\nlppaca_of(dtl->cpu).dtl_enable_mask = 0x0;\r\nunregister_dtl(hwcpu);\r\n}\r\nstatic u64 dtl_current_index(struct dtl *dtl)\r\n{\r\nreturn lppaca_of(dtl->cpu).dtl_idx;\r\n}\r\nstatic int dtl_enable(struct dtl *dtl)\r\n{\r\nlong int n_entries;\r\nlong int rc;\r\nstruct dtl_entry *buf = NULL;\r\nif (!dtl_cache)\r\nreturn -ENOMEM;\r\nif (dtl->buf)\r\nreturn -EBUSY;\r\nn_entries = dtl_buf_entries;\r\nbuf = kmem_cache_alloc_node(dtl_cache, GFP_KERNEL, cpu_to_node(dtl->cpu));\r\nif (!buf) {\r\nprintk(KERN_WARNING "%s: buffer alloc failed for cpu %d\n",\r\n__func__, dtl->cpu);\r\nreturn -ENOMEM;\r\n}\r\nspin_lock(&dtl->lock);\r\nrc = -EBUSY;\r\nif (!dtl->buf) {\r\ndtl->buf_entries = n_entries;\r\ndtl->buf = buf;\r\ndtl->last_idx = 0;\r\nrc = dtl_start(dtl);\r\nif (rc)\r\ndtl->buf = NULL;\r\n}\r\nspin_unlock(&dtl->lock);\r\nif (rc)\r\nkmem_cache_free(dtl_cache, buf);\r\nreturn rc;\r\n}\r\nstatic void dtl_disable(struct dtl *dtl)\r\n{\r\nspin_lock(&dtl->lock);\r\ndtl_stop(dtl);\r\nkmem_cache_free(dtl_cache, dtl->buf);\r\ndtl->buf = NULL;\r\ndtl->buf_entries = 0;\r\nspin_unlock(&dtl->lock);\r\n}\r\nstatic int dtl_file_open(struct inode *inode, struct file *filp)\r\n{\r\nstruct dtl *dtl = inode->i_private;\r\nint rc;\r\nrc = dtl_enable(dtl);\r\nif (rc)\r\nreturn rc;\r\nfilp->private_data = dtl;\r\nreturn 0;\r\n}\r\nstatic int dtl_file_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct dtl *dtl = inode->i_private;\r\ndtl_disable(dtl);\r\nreturn 0;\r\n}\r\nstatic ssize_t dtl_file_read(struct file *filp, char __user *buf, size_t len,\r\nloff_t *pos)\r\n{\r\nlong int rc, n_read, n_req, read_size;\r\nstruct dtl *dtl;\r\nu64 cur_idx, last_idx, i;\r\nif ((len % sizeof(struct dtl_entry)) != 0)\r\nreturn -EINVAL;\r\ndtl = filp->private_data;\r\nn_req = len / sizeof(struct dtl_entry);\r\nn_read = 0;\r\nspin_lock(&dtl->lock);\r\ncur_idx = dtl_current_index(dtl);\r\nlast_idx = dtl->last_idx;\r\nif (last_idx + dtl->buf_entries <= cur_idx)\r\nlast_idx = cur_idx - dtl->buf_entries + 1;\r\nif (last_idx + n_req > cur_idx)\r\nn_req = cur_idx - last_idx;\r\nif (n_req > 0)\r\ndtl->last_idx = last_idx + n_req;\r\nspin_unlock(&dtl->lock);\r\nif (n_req <= 0)\r\nreturn 0;\r\ni = last_idx % dtl->buf_entries;\r\nif (i + n_req > dtl->buf_entries) {\r\nread_size = dtl->buf_entries - i;\r\nrc = copy_to_user(buf, &dtl->buf[i],\r\nread_size * sizeof(struct dtl_entry));\r\nif (rc)\r\nreturn -EFAULT;\r\ni = 0;\r\nn_req -= read_size;\r\nn_read += read_size;\r\nbuf += read_size * sizeof(struct dtl_entry);\r\n}\r\nrc = copy_to_user(buf, &dtl->buf[i], n_req * sizeof(struct dtl_entry));\r\nif (rc)\r\nreturn -EFAULT;\r\nn_read += n_req;\r\nreturn n_read * sizeof(struct dtl_entry);\r\n}\r\nstatic int dtl_setup_file(struct dtl *dtl)\r\n{\r\nchar name[10];\r\nsprintf(name, "cpu-%d", dtl->cpu);\r\ndtl->file = debugfs_create_file(name, 0400, dtl_dir, dtl, &dtl_fops);\r\nif (!dtl->file)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic int dtl_init(void)\r\n{\r\nstruct dentry *event_mask_file, *buf_entries_file;\r\nint rc, i;\r\nif (!firmware_has_feature(FW_FEATURE_SPLPAR))\r\nreturn -ENODEV;\r\nrc = -ENOMEM;\r\ndtl_dir = debugfs_create_dir("dtl", powerpc_debugfs_root);\r\nif (!dtl_dir) {\r\nprintk(KERN_WARNING "%s: can't create dtl root dir\n",\r\n__func__);\r\ngoto err;\r\n}\r\nevent_mask_file = debugfs_create_x8("dtl_event_mask", 0600,\r\ndtl_dir, &dtl_event_mask);\r\nbuf_entries_file = debugfs_create_u32("dtl_buf_entries", 0400,\r\ndtl_dir, &dtl_buf_entries);\r\nif (!event_mask_file || !buf_entries_file) {\r\nprintk(KERN_WARNING "%s: can't create dtl files\n", __func__);\r\ngoto err_remove_dir;\r\n}\r\nfor_each_possible_cpu(i) {\r\nstruct dtl *dtl = &per_cpu(cpu_dtl, i);\r\nspin_lock_init(&dtl->lock);\r\ndtl->cpu = i;\r\nrc = dtl_setup_file(dtl);\r\nif (rc)\r\ngoto err_remove_dir;\r\n}\r\nreturn 0;\r\nerr_remove_dir:\r\ndebugfs_remove_recursive(dtl_dir);\r\nerr:\r\nreturn rc;\r\n}
