static __ref void *early_alloc_pgtable(unsigned long size)\r\n{\r\nvoid *pt;\r\nif (init_bootmem_done)\r\npt = __alloc_bootmem(size, size, __pa(MAX_DMA_ADDRESS));\r\nelse\r\npt = __va(memblock_alloc_base(size, size,\r\n__pa(MAX_DMA_ADDRESS)));\r\nmemset(pt, 0, size);\r\nreturn pt;\r\n}\r\nint map_kernel_page(unsigned long ea, unsigned long pa, int flags)\r\n{\r\npgd_t *pgdp;\r\npud_t *pudp;\r\npmd_t *pmdp;\r\npte_t *ptep;\r\nif (slab_is_available()) {\r\npgdp = pgd_offset_k(ea);\r\npudp = pud_alloc(&init_mm, pgdp, ea);\r\nif (!pudp)\r\nreturn -ENOMEM;\r\npmdp = pmd_alloc(&init_mm, pudp, ea);\r\nif (!pmdp)\r\nreturn -ENOMEM;\r\nptep = pte_alloc_kernel(pmdp, ea);\r\nif (!ptep)\r\nreturn -ENOMEM;\r\nset_pte_at(&init_mm, ea, ptep, pfn_pte(pa >> PAGE_SHIFT,\r\n__pgprot(flags)));\r\n} else {\r\n#ifdef CONFIG_PPC_MMU_NOHASH\r\npgdp = pgd_offset_k(ea);\r\n#ifdef PUD_TABLE_SIZE\r\nif (pgd_none(*pgdp)) {\r\npudp = early_alloc_pgtable(PUD_TABLE_SIZE);\r\nBUG_ON(pudp == NULL);\r\npgd_populate(&init_mm, pgdp, pudp);\r\n}\r\n#endif\r\npudp = pud_offset(pgdp, ea);\r\nif (pud_none(*pudp)) {\r\npmdp = early_alloc_pgtable(PMD_TABLE_SIZE);\r\nBUG_ON(pmdp == NULL);\r\npud_populate(&init_mm, pudp, pmdp);\r\n}\r\npmdp = pmd_offset(pudp, ea);\r\nif (!pmd_present(*pmdp)) {\r\nptep = early_alloc_pgtable(PAGE_SIZE);\r\nBUG_ON(ptep == NULL);\r\npmd_populate_kernel(&init_mm, pmdp, ptep);\r\n}\r\nptep = pte_offset_kernel(pmdp, ea);\r\nset_pte_at(&init_mm, ea, ptep, pfn_pte(pa >> PAGE_SHIFT,\r\n__pgprot(flags)));\r\n#else\r\nif (htab_bolt_mapping(ea, ea + PAGE_SIZE, pa, flags,\r\nmmu_io_psize, mmu_kernel_ssize)) {\r\nprintk(KERN_ERR "Failed to do bolted mapping IO "\r\n"memory at %016lx !\n", pa);\r\nreturn -ENOMEM;\r\n}\r\n#endif\r\n}\r\n#ifdef CONFIG_PPC_BOOK3E_64\r\nmb();\r\n#else\r\nsmp_wmb();\r\n#endif\r\nreturn 0;\r\n}\r\nvoid __iomem * __ioremap_at(phys_addr_t pa, void *ea, unsigned long size,\r\nunsigned long flags)\r\n{\r\nunsigned long i;\r\nif ((flags & _PAGE_PRESENT) == 0)\r\nflags |= pgprot_val(PAGE_KERNEL);\r\nif (flags & _PAGE_NO_CACHE)\r\nflags &= ~_PAGE_COHERENT;\r\nif (flags & _PAGE_4K_PFN)\r\nreturn NULL;\r\nWARN_ON(pa & ~PAGE_MASK);\r\nWARN_ON(((unsigned long)ea) & ~PAGE_MASK);\r\nWARN_ON(size & ~PAGE_MASK);\r\nfor (i = 0; i < size; i += PAGE_SIZE)\r\nif (map_kernel_page((unsigned long)ea+i, pa+i, flags))\r\nreturn NULL;\r\nreturn (void __iomem *)ea;\r\n}\r\nvoid __iounmap_at(void *ea, unsigned long size)\r\n{\r\nWARN_ON(((unsigned long)ea) & ~PAGE_MASK);\r\nWARN_ON(size & ~PAGE_MASK);\r\nunmap_kernel_range((unsigned long)ea, size);\r\n}\r\nvoid __iomem * __ioremap_caller(phys_addr_t addr, unsigned long size,\r\nunsigned long flags, void *caller)\r\n{\r\nphys_addr_t paligned;\r\nvoid __iomem *ret;\r\npaligned = addr & PAGE_MASK;\r\nsize = PAGE_ALIGN(addr + size) - paligned;\r\nif ((size == 0) || (paligned == 0))\r\nreturn NULL;\r\nif (mem_init_done) {\r\nstruct vm_struct *area;\r\narea = __get_vm_area_caller(size, VM_IOREMAP,\r\nioremap_bot, IOREMAP_END,\r\ncaller);\r\nif (area == NULL)\r\nreturn NULL;\r\narea->phys_addr = paligned;\r\nret = __ioremap_at(paligned, area->addr, size, flags);\r\nif (!ret)\r\nvunmap(area->addr);\r\n} else {\r\nret = __ioremap_at(paligned, (void *)ioremap_bot, size, flags);\r\nif (ret)\r\nioremap_bot += size;\r\n}\r\nif (ret)\r\nret += addr & ~PAGE_MASK;\r\nreturn ret;\r\n}\r\nvoid __iomem * __ioremap(phys_addr_t addr, unsigned long size,\r\nunsigned long flags)\r\n{\r\nreturn __ioremap_caller(addr, size, flags, __builtin_return_address(0));\r\n}\r\nvoid __iomem * ioremap(phys_addr_t addr, unsigned long size)\r\n{\r\nunsigned long flags = _PAGE_NO_CACHE | _PAGE_GUARDED;\r\nvoid *caller = __builtin_return_address(0);\r\nif (ppc_md.ioremap)\r\nreturn ppc_md.ioremap(addr, size, flags, caller);\r\nreturn __ioremap_caller(addr, size, flags, caller);\r\n}\r\nvoid __iomem * ioremap_wc(phys_addr_t addr, unsigned long size)\r\n{\r\nunsigned long flags = _PAGE_NO_CACHE;\r\nvoid *caller = __builtin_return_address(0);\r\nif (ppc_md.ioremap)\r\nreturn ppc_md.ioremap(addr, size, flags, caller);\r\nreturn __ioremap_caller(addr, size, flags, caller);\r\n}\r\nvoid __iomem * ioremap_prot(phys_addr_t addr, unsigned long size,\r\nunsigned long flags)\r\n{\r\nvoid *caller = __builtin_return_address(0);\r\nif (flags & _PAGE_RW)\r\nflags |= _PAGE_DIRTY;\r\nflags &= ~(_PAGE_USER | _PAGE_EXEC);\r\n#ifdef _PAGE_BAP_SR\r\nflags |= _PAGE_BAP_SR;\r\n#endif\r\nif (ppc_md.ioremap)\r\nreturn ppc_md.ioremap(addr, size, flags, caller);\r\nreturn __ioremap_caller(addr, size, flags, caller);\r\n}\r\nvoid __iounmap(volatile void __iomem *token)\r\n{\r\nvoid *addr;\r\nif (!mem_init_done)\r\nreturn;\r\naddr = (void *) ((unsigned long __force)\r\nPCI_FIX_ADDR(token) & PAGE_MASK);\r\nif ((unsigned long)addr < ioremap_bot) {\r\nprintk(KERN_WARNING "Attempt to iounmap early bolted mapping"\r\n" at 0x%p\n", addr);\r\nreturn;\r\n}\r\nvunmap(addr);\r\n}\r\nvoid iounmap(volatile void __iomem *token)\r\n{\r\nif (ppc_md.iounmap)\r\nppc_md.iounmap(token);\r\nelse\r\n__iounmap(token);\r\n}\r\nstruct page *pmd_page(pmd_t pmd)\r\n{\r\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\r\nif (pmd_trans_huge(pmd))\r\nreturn pfn_to_page(pmd_pfn(pmd));\r\n#endif\r\nreturn virt_to_page(pmd_page_vaddr(pmd));\r\n}\r\nstatic pte_t *get_from_cache(struct mm_struct *mm)\r\n{\r\nvoid *pte_frag, *ret;\r\nspin_lock(&mm->page_table_lock);\r\nret = mm->context.pte_frag;\r\nif (ret) {\r\npte_frag = ret + PTE_FRAG_SIZE;\r\nif (((unsigned long)pte_frag & ~PAGE_MASK) == 0)\r\npte_frag = NULL;\r\nmm->context.pte_frag = pte_frag;\r\n}\r\nspin_unlock(&mm->page_table_lock);\r\nreturn (pte_t *)ret;\r\n}\r\nstatic pte_t *__alloc_for_cache(struct mm_struct *mm, int kernel)\r\n{\r\nvoid *ret = NULL;\r\nstruct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK |\r\n__GFP_REPEAT | __GFP_ZERO);\r\nif (!page)\r\nreturn NULL;\r\nif (!kernel && !pgtable_page_ctor(page)) {\r\n__free_page(page);\r\nreturn NULL;\r\n}\r\nret = page_address(page);\r\nspin_lock(&mm->page_table_lock);\r\nif (likely(!mm->context.pte_frag)) {\r\natomic_set(&page->_count, PTE_FRAG_NR);\r\nmm->context.pte_frag = ret + PTE_FRAG_SIZE;\r\n}\r\nspin_unlock(&mm->page_table_lock);\r\nreturn (pte_t *)ret;\r\n}\r\npte_t *page_table_alloc(struct mm_struct *mm, unsigned long vmaddr, int kernel)\r\n{\r\npte_t *pte;\r\npte = get_from_cache(mm);\r\nif (pte)\r\nreturn pte;\r\nreturn __alloc_for_cache(mm, kernel);\r\n}\r\nvoid page_table_free(struct mm_struct *mm, unsigned long *table, int kernel)\r\n{\r\nstruct page *page = virt_to_page(table);\r\nif (put_page_testzero(page)) {\r\nif (!kernel)\r\npgtable_page_dtor(page);\r\nfree_hot_cold_page(page, 0);\r\n}\r\n}\r\nstatic void page_table_free_rcu(void *table)\r\n{\r\nstruct page *page = virt_to_page(table);\r\nif (put_page_testzero(page)) {\r\npgtable_page_dtor(page);\r\nfree_hot_cold_page(page, 0);\r\n}\r\n}\r\nvoid pgtable_free_tlb(struct mmu_gather *tlb, void *table, int shift)\r\n{\r\nunsigned long pgf = (unsigned long)table;\r\nBUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);\r\npgf |= shift;\r\ntlb_remove_table(tlb, (void *)pgf);\r\n}\r\nvoid __tlb_remove_table(void *_table)\r\n{\r\nvoid *table = (void *)((unsigned long)_table & ~MAX_PGTABLE_INDEX_SIZE);\r\nunsigned shift = (unsigned long)_table & MAX_PGTABLE_INDEX_SIZE;\r\nif (!shift)\r\npage_table_free_rcu(table);\r\nelse {\r\nBUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);\r\nkmem_cache_free(PGT_CACHE(shift), table);\r\n}\r\n}\r\nvoid pgtable_free_tlb(struct mmu_gather *tlb, void *table, int shift)\r\n{\r\nif (!shift) {\r\nstruct page *page = virt_to_page(table);\r\nif (put_page_testzero(page)) {\r\npgtable_page_dtor(page);\r\nfree_hot_cold_page(page, 0);\r\n}\r\n} else {\r\nBUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);\r\nkmem_cache_free(PGT_CACHE(shift), table);\r\n}\r\n}\r\nint pmdp_set_access_flags(struct vm_area_struct *vma, unsigned long address,\r\npmd_t *pmdp, pmd_t entry, int dirty)\r\n{\r\nint changed;\r\n#ifdef CONFIG_DEBUG_VM\r\nWARN_ON(!pmd_trans_huge(*pmdp));\r\nassert_spin_locked(&vma->vm_mm->page_table_lock);\r\n#endif\r\nchanged = !pmd_same(*(pmdp), entry);\r\nif (changed) {\r\n__ptep_set_access_flags(pmdp_ptep(pmdp), pmd_pte(entry));\r\n}\r\nreturn changed;\r\n}\r\nunsigned long pmd_hugepage_update(struct mm_struct *mm, unsigned long addr,\r\npmd_t *pmdp, unsigned long clr,\r\nunsigned long set)\r\n{\r\nunsigned long old, tmp;\r\n#ifdef CONFIG_DEBUG_VM\r\nWARN_ON(!pmd_trans_huge(*pmdp));\r\nassert_spin_locked(&mm->page_table_lock);\r\n#endif\r\n#ifdef PTE_ATOMIC_UPDATES\r\n__asm__ __volatile__(\r\n"1: ldarx %0,0,%3\n\\r\nandi. %1,%0,%6\n\\r\nbne- 1b \n\\r\nandc %1,%0,%4 \n\\r\nor %1,%1,%7\n\\r\nstdcx. %1,0,%3 \n\\r\nbne- 1b"\r\n: "=&r" (old), "=&r" (tmp), "=m" (*pmdp)\r\n: "r" (pmdp), "r" (clr), "m" (*pmdp), "i" (_PAGE_BUSY), "r" (set)\r\n: "cc" );\r\n#else\r\nold = pmd_val(*pmdp);\r\n*pmdp = __pmd((old & ~clr) | set);\r\n#endif\r\ntrace_hugepage_update(addr, old, clr, set);\r\nif (old & _PAGE_HASHPTE)\r\nhpte_do_hugepage_flush(mm, addr, pmdp, old);\r\nreturn old;\r\n}\r\npmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,\r\npmd_t *pmdp)\r\n{\r\npmd_t pmd;\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\nif (pmd_trans_huge(*pmdp)) {\r\npmd = pmdp_get_and_clear(vma->vm_mm, address, pmdp);\r\n} else {\r\npmd = *pmdp;\r\npmd_clear(pmdp);\r\nkick_all_cpus_sync();\r\nflush_tlb_pmd_range(vma->vm_mm, &pmd, address);\r\n}\r\nreturn pmd;\r\n}\r\nint pmdp_test_and_clear_young(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp)\r\n{\r\nreturn __pmdp_test_and_clear_young(vma->vm_mm, address, pmdp);\r\n}\r\nint pmdp_clear_flush_young(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp)\r\n{\r\nreturn __pmdp_test_and_clear_young(vma->vm_mm, address, pmdp);\r\n}\r\nvoid pmdp_splitting_flush(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp)\r\n{\r\nunsigned long old, tmp;\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\n#ifdef CONFIG_DEBUG_VM\r\nWARN_ON(!pmd_trans_huge(*pmdp));\r\nassert_spin_locked(&vma->vm_mm->page_table_lock);\r\n#endif\r\n#ifdef PTE_ATOMIC_UPDATES\r\n__asm__ __volatile__(\r\n"1: ldarx %0,0,%3\n\\r\nandi. %1,%0,%6\n\\r\nbne- 1b \n\\r\nori %1,%0,%4 \n\\r\nstdcx. %1,0,%3 \n\\r\nbne- 1b"\r\n: "=&r" (old), "=&r" (tmp), "=m" (*pmdp)\r\n: "r" (pmdp), "i" (_PAGE_SPLITTING), "m" (*pmdp), "i" (_PAGE_BUSY)\r\n: "cc" );\r\n#else\r\nold = pmd_val(*pmdp);\r\n*pmdp = __pmd(old | _PAGE_SPLITTING);\r\n#endif\r\ntrace_hugepage_splitting(address, old);\r\nif (!(old & _PAGE_SPLITTING)) {\r\nif (old & _PAGE_HASHPTE)\r\nhpte_do_hugepage_flush(vma->vm_mm, address, pmdp, old);\r\n}\r\nkick_all_cpus_sync();\r\n}\r\nvoid pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,\r\npgtable_t pgtable)\r\n{\r\npgtable_t *pgtable_slot;\r\nassert_spin_locked(&mm->page_table_lock);\r\npgtable_slot = (pgtable_t *)pmdp + PTRS_PER_PMD;\r\n*pgtable_slot = pgtable;\r\nsmp_wmb();\r\n}\r\npgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)\r\n{\r\npgtable_t pgtable;\r\npgtable_t *pgtable_slot;\r\nassert_spin_locked(&mm->page_table_lock);\r\npgtable_slot = (pgtable_t *)pmdp + PTRS_PER_PMD;\r\npgtable = *pgtable_slot;\r\n*pgtable_slot = NULL;\r\nmemset(pgtable, 0, PTE_FRAG_SIZE);\r\nreturn pgtable;\r\n}\r\nvoid set_pmd_at(struct mm_struct *mm, unsigned long addr,\r\npmd_t *pmdp, pmd_t pmd)\r\n{\r\n#ifdef CONFIG_DEBUG_VM\r\nWARN_ON(pmd_val(*pmdp) & _PAGE_PRESENT);\r\nassert_spin_locked(&mm->page_table_lock);\r\nWARN_ON(!pmd_trans_huge(pmd));\r\n#endif\r\ntrace_hugepage_set_pmd(addr, pmd);\r\nreturn set_pte_at(mm, addr, pmdp_ptep(pmdp), pmd_pte(pmd));\r\n}\r\nvoid pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,\r\npmd_t *pmdp)\r\n{\r\npmd_hugepage_update(vma->vm_mm, address, pmdp, _PAGE_PRESENT, 0);\r\n}\r\nvoid hpte_do_hugepage_flush(struct mm_struct *mm, unsigned long addr,\r\npmd_t *pmdp, unsigned long old_pmd)\r\n{\r\nint ssize, i;\r\nunsigned long s_addr;\r\nint max_hpte_count;\r\nunsigned int psize, valid;\r\nunsigned char *hpte_slot_array;\r\nunsigned long hidx, vpn, vsid, hash, shift, slot;\r\ns_addr = addr & HPAGE_PMD_MASK;\r\nhpte_slot_array = get_hpte_slot_array(pmdp);\r\nif (!hpte_slot_array)\r\nreturn;\r\n#ifdef CONFIG_DEBUG_VM\r\npsize = get_slice_psize(mm, s_addr);\r\nBUG_ON(psize == MMU_PAGE_16M);\r\n#endif\r\nif (old_pmd & _PAGE_COMBO)\r\npsize = MMU_PAGE_4K;\r\nelse\r\npsize = MMU_PAGE_64K;\r\nif (!is_kernel_addr(s_addr)) {\r\nssize = user_segment_size(s_addr);\r\nvsid = get_vsid(mm->context.id, s_addr, ssize);\r\nWARN_ON(vsid == 0);\r\n} else {\r\nvsid = get_kernel_vsid(s_addr, mmu_kernel_ssize);\r\nssize = mmu_kernel_ssize;\r\n}\r\nif (ppc_md.hugepage_invalidate)\r\nreturn ppc_md.hugepage_invalidate(vsid, s_addr,\r\nhpte_slot_array,\r\npsize, ssize);\r\nshift = mmu_psize_defs[psize].shift;\r\nmax_hpte_count = HPAGE_PMD_SIZE >> shift;\r\nfor (i = 0; i < max_hpte_count; i++) {\r\nvalid = hpte_valid(hpte_slot_array, i);\r\nif (!valid)\r\ncontinue;\r\nhidx = hpte_hash_index(hpte_slot_array, i);\r\naddr = s_addr + (i * (1ul << shift));\r\nvpn = hpt_vpn(addr, vsid, ssize);\r\nhash = hpt_hash(vpn, shift, ssize);\r\nif (hidx & _PTEIDX_SECONDARY)\r\nhash = ~hash;\r\nslot = (hash & htab_hash_mask) * HPTES_PER_GROUP;\r\nslot += hidx & _PTEIDX_GROUP_IX;\r\nppc_md.hpte_invalidate(slot, vpn, psize,\r\nMMU_PAGE_16M, ssize, 0);\r\n}\r\n}\r\nstatic pmd_t pmd_set_protbits(pmd_t pmd, pgprot_t pgprot)\r\n{\r\npmd_val(pmd) |= pgprot_val(pgprot);\r\nreturn pmd;\r\n}\r\npmd_t pfn_pmd(unsigned long pfn, pgprot_t pgprot)\r\n{\r\npmd_t pmd;\r\npmd_val(pmd) = pfn << PTE_RPN_SHIFT;\r\npmd_val(pmd) |= _PAGE_THP_HUGE;\r\npmd = pmd_set_protbits(pmd, pgprot);\r\nreturn pmd;\r\n}\r\npmd_t mk_pmd(struct page *page, pgprot_t pgprot)\r\n{\r\nreturn pfn_pmd(page_to_pfn(page), pgprot);\r\n}\r\npmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)\r\n{\r\npmd_val(pmd) &= _HPAGE_CHG_MASK;\r\npmd = pmd_set_protbits(pmd, newprot);\r\nreturn pmd;\r\n}\r\nvoid update_mmu_cache_pmd(struct vm_area_struct *vma, unsigned long addr,\r\npmd_t *pmd)\r\n{\r\nreturn;\r\n}\r\npmd_t pmdp_get_and_clear(struct mm_struct *mm,\r\nunsigned long addr, pmd_t *pmdp)\r\n{\r\npmd_t old_pmd;\r\npgtable_t pgtable;\r\nunsigned long old;\r\npgtable_t *pgtable_slot;\r\nold = pmd_hugepage_update(mm, addr, pmdp, ~0UL, 0);\r\nold_pmd = __pmd(old);\r\npgtable_slot = (pgtable_t *)pmdp + PTRS_PER_PMD;\r\npgtable = *pgtable_slot;\r\nmemset(pgtable, 0, PTE_FRAG_SIZE);\r\nreturn old_pmd;\r\n}\r\nint has_transparent_hugepage(void)\r\n{\r\nif (!mmu_has_feature(MMU_FTR_16M_PAGE))\r\nreturn 0;\r\nif (mmu_psize_defs[MMU_PAGE_16M].shift != PMD_SHIFT)\r\nreturn 0;\r\nif (mmu_psize_defs[MMU_PAGE_64K].shift &&\r\n(mmu_psize_defs[MMU_PAGE_64K].penc[MMU_PAGE_16M] == -1))\r\nreturn 0;\r\nif (mmu_psize_defs[MMU_PAGE_4K].penc[MMU_PAGE_16M] == -1)\r\nreturn 0;\r\nreturn 1;\r\n}
