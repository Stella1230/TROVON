static void mvreg_write(struct mvneta_port *pp, u32 offset, u32 data)\r\n{\r\nwritel(data, pp->base + offset);\r\n}\r\nstatic u32 mvreg_read(struct mvneta_port *pp, u32 offset)\r\n{\r\nreturn readl(pp->base + offset);\r\n}\r\nstatic void mvneta_txq_inc_get(struct mvneta_tx_queue *txq)\r\n{\r\ntxq->txq_get_index++;\r\nif (txq->txq_get_index == txq->size)\r\ntxq->txq_get_index = 0;\r\n}\r\nstatic void mvneta_txq_inc_put(struct mvneta_tx_queue *txq)\r\n{\r\ntxq->txq_put_index++;\r\nif (txq->txq_put_index == txq->size)\r\ntxq->txq_put_index = 0;\r\n}\r\nstatic void mvneta_mib_counters_clear(struct mvneta_port *pp)\r\n{\r\nint i;\r\nu32 dummy;\r\nfor (i = 0; i < MVNETA_MIB_LATE_COLLISION; i += 4)\r\ndummy = mvreg_read(pp, (MVNETA_MIB_COUNTERS_BASE + i));\r\n}\r\nstruct rtnl_link_stats64 *mvneta_get_stats64(struct net_device *dev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nunsigned int start;\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct mvneta_pcpu_stats *cpu_stats;\r\nu64 rx_packets;\r\nu64 rx_bytes;\r\nu64 tx_packets;\r\nu64 tx_bytes;\r\ncpu_stats = per_cpu_ptr(pp->stats, cpu);\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&cpu_stats->syncp);\r\nrx_packets = cpu_stats->rx_packets;\r\nrx_bytes = cpu_stats->rx_bytes;\r\ntx_packets = cpu_stats->tx_packets;\r\ntx_bytes = cpu_stats->tx_bytes;\r\n} while (u64_stats_fetch_retry_irq(&cpu_stats->syncp, start));\r\nstats->rx_packets += rx_packets;\r\nstats->rx_bytes += rx_bytes;\r\nstats->tx_packets += tx_packets;\r\nstats->tx_bytes += tx_bytes;\r\n}\r\nstats->rx_errors = dev->stats.rx_errors;\r\nstats->rx_dropped = dev->stats.rx_dropped;\r\nstats->tx_dropped = dev->stats.tx_dropped;\r\nreturn stats;\r\n}\r\nstatic int mvneta_rxq_desc_is_first_last(u32 status)\r\n{\r\nreturn (status & MVNETA_RXD_FIRST_LAST_DESC) ==\r\nMVNETA_RXD_FIRST_LAST_DESC;\r\n}\r\nstatic void mvneta_rxq_non_occup_desc_add(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq,\r\nint ndescs)\r\n{\r\nwhile (ndescs > MVNETA_RXQ_ADD_NON_OCCUPIED_MAX) {\r\nmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id),\r\n(MVNETA_RXQ_ADD_NON_OCCUPIED_MAX <<\r\nMVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT));\r\nndescs -= MVNETA_RXQ_ADD_NON_OCCUPIED_MAX;\r\n}\r\nmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id),\r\n(ndescs << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT));\r\n}\r\nstatic int mvneta_rxq_busy_desc_num_get(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_RXQ_STATUS_REG(rxq->id));\r\nreturn val & MVNETA_RXQ_OCCUPIED_ALL_MASK;\r\n}\r\nstatic void mvneta_rxq_desc_num_update(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq,\r\nint rx_done, int rx_filled)\r\n{\r\nu32 val;\r\nif ((rx_done <= 0xff) && (rx_filled <= 0xff)) {\r\nval = rx_done |\r\n(rx_filled << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT);\r\nmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id), val);\r\nreturn;\r\n}\r\nwhile ((rx_done > 0) || (rx_filled > 0)) {\r\nif (rx_done <= 0xff) {\r\nval = rx_done;\r\nrx_done = 0;\r\n} else {\r\nval = 0xff;\r\nrx_done -= 0xff;\r\n}\r\nif (rx_filled <= 0xff) {\r\nval |= rx_filled << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT;\r\nrx_filled = 0;\r\n} else {\r\nval |= 0xff << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT;\r\nrx_filled -= 0xff;\r\n}\r\nmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id), val);\r\n}\r\n}\r\nstatic struct mvneta_rx_desc *\r\nmvneta_rxq_next_desc_get(struct mvneta_rx_queue *rxq)\r\n{\r\nint rx_desc = rxq->next_desc_to_proc;\r\nrxq->next_desc_to_proc = MVNETA_QUEUE_NEXT_DESC(rxq, rx_desc);\r\nprefetch(rxq->descs + rxq->next_desc_to_proc);\r\nreturn rxq->descs + rx_desc;\r\n}\r\nstatic void mvneta_max_rx_size_set(struct mvneta_port *pp, int max_rx_size)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_GMAC_CTRL_0);\r\nval &= ~MVNETA_GMAC_MAX_RX_SIZE_MASK;\r\nval |= ((max_rx_size - MVNETA_MH_SIZE) / 2) <<\r\nMVNETA_GMAC_MAX_RX_SIZE_SHIFT;\r\nmvreg_write(pp, MVNETA_GMAC_CTRL_0, val);\r\n}\r\nstatic void mvneta_rxq_offset_set(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq,\r\nint offset)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_RXQ_CONFIG_REG(rxq->id));\r\nval &= ~MVNETA_RXQ_PKT_OFFSET_ALL_MASK;\r\nval |= MVNETA_RXQ_PKT_OFFSET_MASK(offset >> 3);\r\nmvreg_write(pp, MVNETA_RXQ_CONFIG_REG(rxq->id), val);\r\n}\r\nstatic void mvneta_txq_pend_desc_add(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq,\r\nint pend_desc)\r\n{\r\nu32 val;\r\nval = pend_desc;\r\nmvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);\r\n}\r\nstatic struct mvneta_tx_desc *\r\nmvneta_txq_next_desc_get(struct mvneta_tx_queue *txq)\r\n{\r\nint tx_desc = txq->next_desc_to_proc;\r\ntxq->next_desc_to_proc = MVNETA_QUEUE_NEXT_DESC(txq, tx_desc);\r\nreturn txq->descs + tx_desc;\r\n}\r\nstatic void mvneta_txq_desc_put(struct mvneta_tx_queue *txq)\r\n{\r\nif (txq->next_desc_to_proc == 0)\r\ntxq->next_desc_to_proc = txq->last_desc - 1;\r\nelse\r\ntxq->next_desc_to_proc--;\r\n}\r\nstatic void mvneta_rxq_buf_size_set(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq,\r\nint buf_size)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_RXQ_SIZE_REG(rxq->id));\r\nval &= ~MVNETA_RXQ_BUF_SIZE_MASK;\r\nval |= ((buf_size >> 3) << MVNETA_RXQ_BUF_SIZE_SHIFT);\r\nmvreg_write(pp, MVNETA_RXQ_SIZE_REG(rxq->id), val);\r\n}\r\nstatic void mvneta_rxq_bm_disable(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_RXQ_CONFIG_REG(rxq->id));\r\nval &= ~MVNETA_RXQ_HW_BUF_ALLOC;\r\nmvreg_write(pp, MVNETA_RXQ_CONFIG_REG(rxq->id), val);\r\n}\r\nstatic void mvneta_port_up(struct mvneta_port *pp)\r\n{\r\nint queue;\r\nu32 q_map;\r\nmvneta_mib_counters_clear(pp);\r\nq_map = 0;\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nstruct mvneta_tx_queue *txq = &pp->txqs[queue];\r\nif (txq->descs != NULL)\r\nq_map |= (1 << queue);\r\n}\r\nmvreg_write(pp, MVNETA_TXQ_CMD, q_map);\r\nq_map = 0;\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\r\nif (rxq->descs != NULL)\r\nq_map |= (1 << queue);\r\n}\r\nmvreg_write(pp, MVNETA_RXQ_CMD, q_map);\r\n}\r\nstatic void mvneta_port_down(struct mvneta_port *pp)\r\n{\r\nu32 val;\r\nint count;\r\nval = mvreg_read(pp, MVNETA_RXQ_CMD) & MVNETA_RXQ_ENABLE_MASK;\r\nif (val != 0)\r\nmvreg_write(pp, MVNETA_RXQ_CMD,\r\nval << MVNETA_RXQ_DISABLE_SHIFT);\r\ncount = 0;\r\ndo {\r\nif (count++ >= MVNETA_RX_DISABLE_TIMEOUT_MSEC) {\r\nnetdev_warn(pp->dev,\r\n"TIMEOUT for RX stopped ! rx_queue_cmd: 0x08%x\n",\r\nval);\r\nbreak;\r\n}\r\nmdelay(1);\r\nval = mvreg_read(pp, MVNETA_RXQ_CMD);\r\n} while (val & 0xff);\r\nval = (mvreg_read(pp, MVNETA_TXQ_CMD)) & MVNETA_TXQ_ENABLE_MASK;\r\nif (val != 0)\r\nmvreg_write(pp, MVNETA_TXQ_CMD,\r\n(val << MVNETA_TXQ_DISABLE_SHIFT));\r\ncount = 0;\r\ndo {\r\nif (count++ >= MVNETA_TX_DISABLE_TIMEOUT_MSEC) {\r\nnetdev_warn(pp->dev,\r\n"TIMEOUT for TX stopped status=0x%08x\n",\r\nval);\r\nbreak;\r\n}\r\nmdelay(1);\r\nval = mvreg_read(pp, MVNETA_TXQ_CMD);\r\n} while (val & 0xff);\r\ncount = 0;\r\ndo {\r\nif (count++ >= MVNETA_TX_FIFO_EMPTY_TIMEOUT) {\r\nnetdev_warn(pp->dev,\r\n"TX FIFO empty timeout status=0x08%x\n",\r\nval);\r\nbreak;\r\n}\r\nmdelay(1);\r\nval = mvreg_read(pp, MVNETA_PORT_STATUS);\r\n} while (!(val & MVNETA_TX_FIFO_EMPTY) &&\r\n(val & MVNETA_TX_IN_PRGRS));\r\nudelay(200);\r\n}\r\nstatic void mvneta_port_enable(struct mvneta_port *pp)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_GMAC_CTRL_0);\r\nval |= MVNETA_GMAC0_PORT_ENABLE;\r\nmvreg_write(pp, MVNETA_GMAC_CTRL_0, val);\r\n}\r\nstatic void mvneta_port_disable(struct mvneta_port *pp)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_GMAC_CTRL_0);\r\nval &= ~MVNETA_GMAC0_PORT_ENABLE;\r\nmvreg_write(pp, MVNETA_GMAC_CTRL_0, val);\r\nudelay(200);\r\n}\r\nstatic void mvneta_set_ucast_table(struct mvneta_port *pp, int queue)\r\n{\r\nint offset;\r\nu32 val;\r\nif (queue == -1) {\r\nval = 0;\r\n} else {\r\nval = 0x1 | (queue << 1);\r\nval |= (val << 24) | (val << 16) | (val << 8);\r\n}\r\nfor (offset = 0; offset <= 0xc; offset += 4)\r\nmvreg_write(pp, MVNETA_DA_FILT_UCAST_BASE + offset, val);\r\n}\r\nstatic void mvneta_set_special_mcast_table(struct mvneta_port *pp, int queue)\r\n{\r\nint offset;\r\nu32 val;\r\nif (queue == -1) {\r\nval = 0;\r\n} else {\r\nval = 0x1 | (queue << 1);\r\nval |= (val << 24) | (val << 16) | (val << 8);\r\n}\r\nfor (offset = 0; offset <= 0xfc; offset += 4)\r\nmvreg_write(pp, MVNETA_DA_FILT_SPEC_MCAST + offset, val);\r\n}\r\nstatic void mvneta_set_other_mcast_table(struct mvneta_port *pp, int queue)\r\n{\r\nint offset;\r\nu32 val;\r\nif (queue == -1) {\r\nmemset(pp->mcast_count, 0, sizeof(pp->mcast_count));\r\nval = 0;\r\n} else {\r\nmemset(pp->mcast_count, 1, sizeof(pp->mcast_count));\r\nval = 0x1 | (queue << 1);\r\nval |= (val << 24) | (val << 16) | (val << 8);\r\n}\r\nfor (offset = 0; offset <= 0xfc; offset += 4)\r\nmvreg_write(pp, MVNETA_DA_FILT_OTH_MCAST + offset, val);\r\n}\r\nstatic void mvneta_defaults_set(struct mvneta_port *pp)\r\n{\r\nint cpu;\r\nint queue;\r\nu32 val;\r\nmvreg_write(pp, MVNETA_INTR_NEW_CAUSE, 0);\r\nmvreg_write(pp, MVNETA_INTR_OLD_CAUSE, 0);\r\nmvreg_write(pp, MVNETA_INTR_MISC_CAUSE, 0);\r\nmvreg_write(pp, MVNETA_INTR_NEW_MASK, 0);\r\nmvreg_write(pp, MVNETA_INTR_OLD_MASK, 0);\r\nmvreg_write(pp, MVNETA_INTR_MISC_MASK, 0);\r\nmvreg_write(pp, MVNETA_INTR_ENABLE, 0);\r\nmvreg_write(pp, MVNETA_MBUS_RETRY, 0x20);\r\nfor (cpu = 0; cpu < CONFIG_NR_CPUS; cpu++)\r\nmvreg_write(pp, MVNETA_CPU_MAP(cpu),\r\n(MVNETA_CPU_RXQ_ACCESS_ALL_MASK |\r\nMVNETA_CPU_TXQ_ACCESS_ALL_MASK));\r\nmvreg_write(pp, MVNETA_PORT_RX_RESET, MVNETA_PORT_RX_DMA_RESET);\r\nmvreg_write(pp, MVNETA_PORT_TX_RESET, MVNETA_PORT_TX_DMA_RESET);\r\nmvreg_write(pp, MVNETA_TXQ_CMD_1, 0);\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nmvreg_write(pp, MVETH_TXQ_TOKEN_COUNT_REG(queue), 0);\r\nmvreg_write(pp, MVETH_TXQ_TOKEN_CFG_REG(queue), 0);\r\n}\r\nmvreg_write(pp, MVNETA_PORT_TX_RESET, 0);\r\nmvreg_write(pp, MVNETA_PORT_RX_RESET, 0);\r\nval = MVNETA_ACC_MODE_EXT;\r\nmvreg_write(pp, MVNETA_ACC_MODE, val);\r\nval = MVNETA_PORT_CONFIG_DEFL_VALUE(rxq_def);\r\nmvreg_write(pp, MVNETA_PORT_CONFIG, val);\r\nval = 0;\r\nmvreg_write(pp, MVNETA_PORT_CONFIG_EXTEND, val);\r\nmvreg_write(pp, MVNETA_RX_MIN_FRAME_SIZE, 64);\r\nval = 0;\r\nval |= MVNETA_TX_BRST_SZ_MASK(MVNETA_SDMA_BRST_SIZE_16);\r\nval |= MVNETA_RX_BRST_SZ_MASK(MVNETA_SDMA_BRST_SIZE_16);\r\nval |= MVNETA_RX_NO_DATA_SWAP | MVNETA_TX_NO_DATA_SWAP;\r\n#if defined(__BIG_ENDIAN)\r\nval |= MVNETA_DESC_SWAP;\r\n#endif\r\nmvreg_write(pp, MVNETA_SDMA_CONFIG, val);\r\nval = mvreg_read(pp, MVNETA_UNIT_CONTROL);\r\nval &= ~MVNETA_PHY_POLLING_ENABLE;\r\nmvreg_write(pp, MVNETA_UNIT_CONTROL, val);\r\nmvneta_set_ucast_table(pp, -1);\r\nmvneta_set_special_mcast_table(pp, -1);\r\nmvneta_set_other_mcast_table(pp, -1);\r\nmvreg_write(pp, MVNETA_INTR_ENABLE,\r\n(MVNETA_RXQ_INTR_ENABLE_ALL_MASK\r\n| MVNETA_TXQ_INTR_ENABLE_ALL_MASK));\r\n}\r\nstatic void mvneta_txq_max_tx_size_set(struct mvneta_port *pp, int max_tx_size)\r\n{\r\nu32 val, size, mtu;\r\nint queue;\r\nmtu = max_tx_size * 8;\r\nif (mtu > MVNETA_TX_MTU_MAX)\r\nmtu = MVNETA_TX_MTU_MAX;\r\nval = mvreg_read(pp, MVNETA_TX_MTU);\r\nval &= ~MVNETA_TX_MTU_MAX;\r\nval |= mtu;\r\nmvreg_write(pp, MVNETA_TX_MTU, val);\r\nval = mvreg_read(pp, MVNETA_TX_TOKEN_SIZE);\r\nsize = val & MVNETA_TX_TOKEN_SIZE_MAX;\r\nif (size < mtu) {\r\nsize = mtu;\r\nval &= ~MVNETA_TX_TOKEN_SIZE_MAX;\r\nval |= size;\r\nmvreg_write(pp, MVNETA_TX_TOKEN_SIZE, val);\r\n}\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nval = mvreg_read(pp, MVNETA_TXQ_TOKEN_SIZE_REG(queue));\r\nsize = val & MVNETA_TXQ_TOKEN_SIZE_MAX;\r\nif (size < mtu) {\r\nsize = mtu;\r\nval &= ~MVNETA_TXQ_TOKEN_SIZE_MAX;\r\nval |= size;\r\nmvreg_write(pp, MVNETA_TXQ_TOKEN_SIZE_REG(queue), val);\r\n}\r\n}\r\n}\r\nstatic void mvneta_set_ucast_addr(struct mvneta_port *pp, u8 last_nibble,\r\nint queue)\r\n{\r\nunsigned int unicast_reg;\r\nunsigned int tbl_offset;\r\nunsigned int reg_offset;\r\nlast_nibble = (0xf & last_nibble);\r\ntbl_offset = (last_nibble / 4) * 4;\r\nreg_offset = last_nibble % 4;\r\nunicast_reg = mvreg_read(pp, (MVNETA_DA_FILT_UCAST_BASE + tbl_offset));\r\nif (queue == -1) {\r\nunicast_reg &= ~(0xff << (8 * reg_offset));\r\n} else {\r\nunicast_reg &= ~(0xff << (8 * reg_offset));\r\nunicast_reg |= ((0x01 | (queue << 1)) << (8 * reg_offset));\r\n}\r\nmvreg_write(pp, (MVNETA_DA_FILT_UCAST_BASE + tbl_offset), unicast_reg);\r\n}\r\nstatic void mvneta_mac_addr_set(struct mvneta_port *pp, unsigned char *addr,\r\nint queue)\r\n{\r\nunsigned int mac_h;\r\nunsigned int mac_l;\r\nif (queue != -1) {\r\nmac_l = (addr[4] << 8) | (addr[5]);\r\nmac_h = (addr[0] << 24) | (addr[1] << 16) |\r\n(addr[2] << 8) | (addr[3] << 0);\r\nmvreg_write(pp, MVNETA_MAC_ADDR_LOW, mac_l);\r\nmvreg_write(pp, MVNETA_MAC_ADDR_HIGH, mac_h);\r\n}\r\nmvneta_set_ucast_addr(pp, addr[5], queue);\r\n}\r\nstatic void mvneta_rx_pkts_coal_set(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq, u32 value)\r\n{\r\nmvreg_write(pp, MVNETA_RXQ_THRESHOLD_REG(rxq->id),\r\nvalue | MVNETA_RXQ_NON_OCCUPIED(0));\r\nrxq->pkts_coal = value;\r\n}\r\nstatic void mvneta_rx_time_coal_set(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq, u32 value)\r\n{\r\nu32 val;\r\nunsigned long clk_rate;\r\nclk_rate = clk_get_rate(pp->clk);\r\nval = (clk_rate / 1000000) * value;\r\nmvreg_write(pp, MVNETA_RXQ_TIME_COAL_REG(rxq->id), val);\r\nrxq->time_coal = value;\r\n}\r\nstatic void mvneta_tx_done_pkts_coal_set(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq, u32 value)\r\n{\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_TXQ_SIZE_REG(txq->id));\r\nval &= ~MVNETA_TXQ_SENT_THRESH_ALL_MASK;\r\nval |= MVNETA_TXQ_SENT_THRESH_MASK(value);\r\nmvreg_write(pp, MVNETA_TXQ_SIZE_REG(txq->id), val);\r\ntxq->done_pkts_coal = value;\r\n}\r\nstatic void mvneta_rx_desc_fill(struct mvneta_rx_desc *rx_desc,\r\nu32 phys_addr, u32 cookie)\r\n{\r\nrx_desc->buf_cookie = cookie;\r\nrx_desc->buf_phys_addr = phys_addr;\r\n}\r\nstatic void mvneta_txq_sent_desc_dec(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq,\r\nint sent_desc)\r\n{\r\nu32 val;\r\nwhile (sent_desc > 0xff) {\r\nval = 0xff << MVNETA_TXQ_DEC_SENT_SHIFT;\r\nmvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);\r\nsent_desc = sent_desc - 0xff;\r\n}\r\nval = sent_desc << MVNETA_TXQ_DEC_SENT_SHIFT;\r\nmvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);\r\n}\r\nstatic int mvneta_txq_sent_desc_num_get(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\nu32 val;\r\nint sent_desc;\r\nval = mvreg_read(pp, MVNETA_TXQ_STATUS_REG(txq->id));\r\nsent_desc = (val & MVNETA_TXQ_SENT_DESC_MASK) >>\r\nMVNETA_TXQ_SENT_DESC_SHIFT;\r\nreturn sent_desc;\r\n}\r\nstatic int mvneta_txq_sent_desc_proc(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\nint sent_desc;\r\nsent_desc = mvneta_txq_sent_desc_num_get(pp, txq);\r\nif (sent_desc)\r\nmvneta_txq_sent_desc_dec(pp, txq, sent_desc);\r\nreturn sent_desc;\r\n}\r\nstatic u32 mvneta_txq_desc_csum(int l3_offs, int l3_proto,\r\nint ip_hdr_len, int l4_proto)\r\n{\r\nu32 command;\r\ncommand = l3_offs << MVNETA_TX_L3_OFF_SHIFT;\r\ncommand |= ip_hdr_len << MVNETA_TX_IP_HLEN_SHIFT;\r\nif (l3_proto == htons(ETH_P_IP))\r\ncommand |= MVNETA_TXD_IP_CSUM;\r\nelse\r\ncommand |= MVNETA_TX_L3_IP6;\r\nif (l4_proto == IPPROTO_TCP)\r\ncommand |= MVNETA_TX_L4_CSUM_FULL;\r\nelse if (l4_proto == IPPROTO_UDP)\r\ncommand |= MVNETA_TX_L4_UDP | MVNETA_TX_L4_CSUM_FULL;\r\nelse\r\ncommand |= MVNETA_TX_L4_CSUM_NOT;\r\nreturn command;\r\n}\r\nstatic void mvneta_rx_error(struct mvneta_port *pp,\r\nstruct mvneta_rx_desc *rx_desc)\r\n{\r\nu32 status = rx_desc->status;\r\nif (!mvneta_rxq_desc_is_first_last(status)) {\r\nnetdev_err(pp->dev,\r\n"bad rx status %08x (buffer oversize), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nreturn;\r\n}\r\nswitch (status & MVNETA_RXD_ERR_CODE_MASK) {\r\ncase MVNETA_RXD_ERR_CRC:\r\nnetdev_err(pp->dev, "bad rx status %08x (crc error), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nbreak;\r\ncase MVNETA_RXD_ERR_OVERRUN:\r\nnetdev_err(pp->dev, "bad rx status %08x (overrun error), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nbreak;\r\ncase MVNETA_RXD_ERR_LEN:\r\nnetdev_err(pp->dev, "bad rx status %08x (max frame length error), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nbreak;\r\ncase MVNETA_RXD_ERR_RESOURCE:\r\nnetdev_err(pp->dev, "bad rx status %08x (resource error), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nbreak;\r\n}\r\n}\r\nstatic void mvneta_rx_csum(struct mvneta_port *pp, u32 status,\r\nstruct sk_buff *skb)\r\n{\r\nif ((status & MVNETA_RXD_L3_IP4) &&\r\n(status & MVNETA_RXD_L4_CSUM_OK)) {\r\nskb->csum = 0;\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nreturn;\r\n}\r\nskb->ip_summed = CHECKSUM_NONE;\r\n}\r\nstatic struct mvneta_tx_queue *mvneta_tx_done_policy(struct mvneta_port *pp,\r\nu32 cause)\r\n{\r\nint queue = fls(cause) - 1;\r\nreturn &pp->txqs[queue];\r\n}\r\nstatic void mvneta_txq_bufs_free(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq, int num)\r\n{\r\nint i;\r\nfor (i = 0; i < num; i++) {\r\nstruct mvneta_tx_desc *tx_desc = txq->descs +\r\ntxq->txq_get_index;\r\nstruct sk_buff *skb = txq->tx_skb[txq->txq_get_index];\r\nmvneta_txq_inc_get(txq);\r\nif (!IS_TSO_HEADER(txq, tx_desc->buf_phys_addr))\r\ndma_unmap_single(pp->dev->dev.parent,\r\ntx_desc->buf_phys_addr,\r\ntx_desc->data_size, DMA_TO_DEVICE);\r\nif (!skb)\r\ncontinue;\r\ndev_kfree_skb_any(skb);\r\n}\r\n}\r\nstatic void mvneta_txq_done(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\nstruct netdev_queue *nq = netdev_get_tx_queue(pp->dev, txq->id);\r\nint tx_done;\r\ntx_done = mvneta_txq_sent_desc_proc(pp, txq);\r\nif (!tx_done)\r\nreturn;\r\nmvneta_txq_bufs_free(pp, txq, tx_done);\r\ntxq->count -= tx_done;\r\nif (netif_tx_queue_stopped(nq)) {\r\nif (txq->count <= txq->tx_wake_threshold)\r\nnetif_tx_wake_queue(nq);\r\n}\r\n}\r\nstatic void *mvneta_frag_alloc(const struct mvneta_port *pp)\r\n{\r\nif (likely(pp->frag_size <= PAGE_SIZE))\r\nreturn netdev_alloc_frag(pp->frag_size);\r\nelse\r\nreturn kmalloc(pp->frag_size, GFP_ATOMIC);\r\n}\r\nstatic void mvneta_frag_free(const struct mvneta_port *pp, void *data)\r\n{\r\nif (likely(pp->frag_size <= PAGE_SIZE))\r\nput_page(virt_to_head_page(data));\r\nelse\r\nkfree(data);\r\n}\r\nstatic int mvneta_rx_refill(struct mvneta_port *pp,\r\nstruct mvneta_rx_desc *rx_desc)\r\n{\r\ndma_addr_t phys_addr;\r\nvoid *data;\r\ndata = mvneta_frag_alloc(pp);\r\nif (!data)\r\nreturn -ENOMEM;\r\nphys_addr = dma_map_single(pp->dev->dev.parent, data,\r\nMVNETA_RX_BUF_SIZE(pp->pkt_size),\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(pp->dev->dev.parent, phys_addr))) {\r\nmvneta_frag_free(pp, data);\r\nreturn -ENOMEM;\r\n}\r\nmvneta_rx_desc_fill(rx_desc, phys_addr, (u32)data);\r\nreturn 0;\r\n}\r\nstatic u32 mvneta_skb_tx_csum(struct mvneta_port *pp, struct sk_buff *skb)\r\n{\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nint ip_hdr_len = 0;\r\n__be16 l3_proto = vlan_get_protocol(skb);\r\nu8 l4_proto;\r\nif (l3_proto == htons(ETH_P_IP)) {\r\nstruct iphdr *ip4h = ip_hdr(skb);\r\nip_hdr_len = ip4h->ihl;\r\nl4_proto = ip4h->protocol;\r\n} else if (l3_proto == htons(ETH_P_IPV6)) {\r\nstruct ipv6hdr *ip6h = ipv6_hdr(skb);\r\nif (skb_network_header_len(skb) > 0)\r\nip_hdr_len = (skb_network_header_len(skb) >> 2);\r\nl4_proto = ip6h->nexthdr;\r\n} else\r\nreturn MVNETA_TX_L4_CSUM_NOT;\r\nreturn mvneta_txq_desc_csum(skb_network_offset(skb),\r\nl3_proto, ip_hdr_len, l4_proto);\r\n}\r\nreturn MVNETA_TX_L4_CSUM_NOT;\r\n}\r\nstatic struct mvneta_rx_queue *mvneta_rx_policy(struct mvneta_port *pp,\r\nu32 cause)\r\n{\r\nint queue = fls(cause >> 8) - 1;\r\nreturn (queue < 0 || queue >= rxq_number) ? NULL : &pp->rxqs[queue];\r\n}\r\nstatic void mvneta_rxq_drop_pkts(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq)\r\n{\r\nint rx_done, i;\r\nrx_done = mvneta_rxq_busy_desc_num_get(pp, rxq);\r\nfor (i = 0; i < rxq->size; i++) {\r\nstruct mvneta_rx_desc *rx_desc = rxq->descs + i;\r\nvoid *data = (void *)rx_desc->buf_cookie;\r\nmvneta_frag_free(pp, data);\r\ndma_unmap_single(pp->dev->dev.parent, rx_desc->buf_phys_addr,\r\nMVNETA_RX_BUF_SIZE(pp->pkt_size), DMA_FROM_DEVICE);\r\n}\r\nif (rx_done)\r\nmvneta_rxq_desc_num_update(pp, rxq, rx_done, rx_done);\r\n}\r\nstatic int mvneta_rx(struct mvneta_port *pp, int rx_todo,\r\nstruct mvneta_rx_queue *rxq)\r\n{\r\nstruct net_device *dev = pp->dev;\r\nint rx_done, rx_filled;\r\nu32 rcvd_pkts = 0;\r\nu32 rcvd_bytes = 0;\r\nrx_done = mvneta_rxq_busy_desc_num_get(pp, rxq);\r\nif (rx_todo > rx_done)\r\nrx_todo = rx_done;\r\nrx_done = 0;\r\nrx_filled = 0;\r\nwhile (rx_done < rx_todo) {\r\nstruct mvneta_rx_desc *rx_desc = mvneta_rxq_next_desc_get(rxq);\r\nstruct sk_buff *skb;\r\nunsigned char *data;\r\nu32 rx_status;\r\nint rx_bytes, err;\r\nrx_done++;\r\nrx_filled++;\r\nrx_status = rx_desc->status;\r\nrx_bytes = rx_desc->data_size - (ETH_FCS_LEN + MVNETA_MH_SIZE);\r\ndata = (unsigned char *)rx_desc->buf_cookie;\r\nif (!mvneta_rxq_desc_is_first_last(rx_status) ||\r\n(rx_status & MVNETA_RXD_ERR_SUMMARY)) {\r\nerr_drop_frame:\r\ndev->stats.rx_errors++;\r\nmvneta_rx_error(pp, rx_desc);\r\ncontinue;\r\n}\r\nif (rx_bytes <= rx_copybreak) {\r\nskb = netdev_alloc_skb_ip_align(dev, rx_bytes);\r\nif (unlikely(!skb))\r\ngoto err_drop_frame;\r\ndma_sync_single_range_for_cpu(dev->dev.parent,\r\nrx_desc->buf_phys_addr,\r\nMVNETA_MH_SIZE + NET_SKB_PAD,\r\nrx_bytes,\r\nDMA_FROM_DEVICE);\r\nmemcpy(skb_put(skb, rx_bytes),\r\ndata + MVNETA_MH_SIZE + NET_SKB_PAD,\r\nrx_bytes);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nmvneta_rx_csum(pp, rx_status, skb);\r\nnapi_gro_receive(&pp->napi, skb);\r\nrcvd_pkts++;\r\nrcvd_bytes += rx_bytes;\r\ncontinue;\r\n}\r\nskb = build_skb(data, pp->frag_size > PAGE_SIZE ? 0 : pp->frag_size);\r\nif (!skb)\r\ngoto err_drop_frame;\r\ndma_unmap_single(dev->dev.parent, rx_desc->buf_phys_addr,\r\nMVNETA_RX_BUF_SIZE(pp->pkt_size), DMA_FROM_DEVICE);\r\nrcvd_pkts++;\r\nrcvd_bytes += rx_bytes;\r\nskb_reserve(skb, MVNETA_MH_SIZE + NET_SKB_PAD);\r\nskb_put(skb, rx_bytes);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nmvneta_rx_csum(pp, rx_status, skb);\r\nnapi_gro_receive(&pp->napi, skb);\r\nerr = mvneta_rx_refill(pp, rx_desc);\r\nif (err) {\r\nnetdev_err(dev, "Linux processing - Can't refill\n");\r\nrxq->missed++;\r\nrx_filled--;\r\n}\r\n}\r\nif (rcvd_pkts) {\r\nstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->rx_packets += rcvd_pkts;\r\nstats->rx_bytes += rcvd_bytes;\r\nu64_stats_update_end(&stats->syncp);\r\n}\r\nmvneta_rxq_desc_num_update(pp, rxq, rx_done, rx_filled);\r\nreturn rx_done;\r\n}\r\nstatic inline void\r\nmvneta_tso_put_hdr(struct sk_buff *skb,\r\nstruct mvneta_port *pp, struct mvneta_tx_queue *txq)\r\n{\r\nstruct mvneta_tx_desc *tx_desc;\r\nint hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\ntxq->tx_skb[txq->txq_put_index] = NULL;\r\ntx_desc = mvneta_txq_next_desc_get(txq);\r\ntx_desc->data_size = hdr_len;\r\ntx_desc->command = mvneta_skb_tx_csum(pp, skb);\r\ntx_desc->command |= MVNETA_TXD_F_DESC;\r\ntx_desc->buf_phys_addr = txq->tso_hdrs_phys +\r\ntxq->txq_put_index * TSO_HEADER_SIZE;\r\nmvneta_txq_inc_put(txq);\r\n}\r\nstatic inline int\r\nmvneta_tso_put_data(struct net_device *dev, struct mvneta_tx_queue *txq,\r\nstruct sk_buff *skb, char *data, int size,\r\nbool last_tcp, bool is_last)\r\n{\r\nstruct mvneta_tx_desc *tx_desc;\r\ntx_desc = mvneta_txq_next_desc_get(txq);\r\ntx_desc->data_size = size;\r\ntx_desc->buf_phys_addr = dma_map_single(dev->dev.parent, data,\r\nsize, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev->dev.parent,\r\ntx_desc->buf_phys_addr))) {\r\nmvneta_txq_desc_put(txq);\r\nreturn -ENOMEM;\r\n}\r\ntx_desc->command = 0;\r\ntxq->tx_skb[txq->txq_put_index] = NULL;\r\nif (last_tcp) {\r\ntx_desc->command = MVNETA_TXD_L_DESC;\r\nif (is_last)\r\ntxq->tx_skb[txq->txq_put_index] = skb;\r\n}\r\nmvneta_txq_inc_put(txq);\r\nreturn 0;\r\n}\r\nstatic int mvneta_tx_tso(struct sk_buff *skb, struct net_device *dev,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\nint total_len, data_left;\r\nint desc_count = 0;\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nstruct tso_t tso;\r\nint hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nint i;\r\nif ((txq->count + tso_count_descs(skb)) >= txq->size)\r\nreturn 0;\r\nif (skb_headlen(skb) < (skb_transport_offset(skb) + tcp_hdrlen(skb))) {\r\npr_info("*** Is this even possible???!?!?\n");\r\nreturn 0;\r\n}\r\ntso_start(skb, &tso);\r\ntotal_len = skb->len - hdr_len;\r\nwhile (total_len > 0) {\r\nchar *hdr;\r\ndata_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);\r\ntotal_len -= data_left;\r\ndesc_count++;\r\nhdr = txq->tso_hdrs + txq->txq_put_index * TSO_HEADER_SIZE;\r\ntso_build_hdr(skb, hdr, &tso, data_left, total_len == 0);\r\nmvneta_tso_put_hdr(skb, pp, txq);\r\nwhile (data_left > 0) {\r\nint size;\r\ndesc_count++;\r\nsize = min_t(int, tso.size, data_left);\r\nif (mvneta_tso_put_data(dev, txq, skb,\r\ntso.data, size,\r\nsize == data_left,\r\ntotal_len == 0))\r\ngoto err_release;\r\ndata_left -= size;\r\ntso_build_data(skb, &tso, size);\r\n}\r\n}\r\nreturn desc_count;\r\nerr_release:\r\nfor (i = desc_count - 1; i >= 0; i--) {\r\nstruct mvneta_tx_desc *tx_desc = txq->descs + i;\r\nif (!IS_TSO_HEADER(txq, tx_desc->buf_phys_addr))\r\ndma_unmap_single(pp->dev->dev.parent,\r\ntx_desc->buf_phys_addr,\r\ntx_desc->data_size,\r\nDMA_TO_DEVICE);\r\nmvneta_txq_desc_put(txq);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mvneta_tx_frag_process(struct mvneta_port *pp, struct sk_buff *skb,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\nstruct mvneta_tx_desc *tx_desc;\r\nint i, nr_frags = skb_shinfo(skb)->nr_frags;\r\nfor (i = 0; i < nr_frags; i++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nvoid *addr = page_address(frag->page.p) + frag->page_offset;\r\ntx_desc = mvneta_txq_next_desc_get(txq);\r\ntx_desc->data_size = frag->size;\r\ntx_desc->buf_phys_addr =\r\ndma_map_single(pp->dev->dev.parent, addr,\r\ntx_desc->data_size, DMA_TO_DEVICE);\r\nif (dma_mapping_error(pp->dev->dev.parent,\r\ntx_desc->buf_phys_addr)) {\r\nmvneta_txq_desc_put(txq);\r\ngoto error;\r\n}\r\nif (i == nr_frags - 1) {\r\ntx_desc->command = MVNETA_TXD_L_DESC | MVNETA_TXD_Z_PAD;\r\ntxq->tx_skb[txq->txq_put_index] = skb;\r\n} else {\r\ntx_desc->command = 0;\r\ntxq->tx_skb[txq->txq_put_index] = NULL;\r\n}\r\nmvneta_txq_inc_put(txq);\r\n}\r\nreturn 0;\r\nerror:\r\nfor (i = i - 1; i >= 0; i--) {\r\ntx_desc = txq->descs + i;\r\ndma_unmap_single(pp->dev->dev.parent,\r\ntx_desc->buf_phys_addr,\r\ntx_desc->data_size,\r\nDMA_TO_DEVICE);\r\nmvneta_txq_desc_put(txq);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic int mvneta_tx(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nu16 txq_id = skb_get_queue_mapping(skb);\r\nstruct mvneta_tx_queue *txq = &pp->txqs[txq_id];\r\nstruct mvneta_tx_desc *tx_desc;\r\nint frags = 0;\r\nu32 tx_cmd;\r\nif (!netif_running(dev))\r\ngoto out;\r\nif (skb_is_gso(skb)) {\r\nfrags = mvneta_tx_tso(skb, dev, txq);\r\ngoto out;\r\n}\r\nfrags = skb_shinfo(skb)->nr_frags + 1;\r\ntx_desc = mvneta_txq_next_desc_get(txq);\r\ntx_cmd = mvneta_skb_tx_csum(pp, skb);\r\ntx_desc->data_size = skb_headlen(skb);\r\ntx_desc->buf_phys_addr = dma_map_single(dev->dev.parent, skb->data,\r\ntx_desc->data_size,\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev->dev.parent,\r\ntx_desc->buf_phys_addr))) {\r\nmvneta_txq_desc_put(txq);\r\nfrags = 0;\r\ngoto out;\r\n}\r\nif (frags == 1) {\r\ntx_cmd |= MVNETA_TXD_FLZ_DESC;\r\ntx_desc->command = tx_cmd;\r\ntxq->tx_skb[txq->txq_put_index] = skb;\r\nmvneta_txq_inc_put(txq);\r\n} else {\r\ntx_cmd |= MVNETA_TXD_F_DESC;\r\ntxq->tx_skb[txq->txq_put_index] = NULL;\r\nmvneta_txq_inc_put(txq);\r\ntx_desc->command = tx_cmd;\r\nif (mvneta_tx_frag_process(pp, skb, txq)) {\r\ndma_unmap_single(dev->dev.parent,\r\ntx_desc->buf_phys_addr,\r\ntx_desc->data_size,\r\nDMA_TO_DEVICE);\r\nmvneta_txq_desc_put(txq);\r\nfrags = 0;\r\ngoto out;\r\n}\r\n}\r\nout:\r\nif (frags > 0) {\r\nstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\r\nstruct netdev_queue *nq = netdev_get_tx_queue(dev, txq_id);\r\ntxq->count += frags;\r\nmvneta_txq_pend_desc_add(pp, txq, frags);\r\nif (txq->count >= txq->tx_stop_threshold)\r\nnetif_tx_stop_queue(nq);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->tx_packets++;\r\nstats->tx_bytes += skb->len;\r\nu64_stats_update_end(&stats->syncp);\r\n} else {\r\ndev->stats.tx_dropped++;\r\ndev_kfree_skb_any(skb);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void mvneta_txq_done_force(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\nint tx_done = txq->count;\r\nmvneta_txq_bufs_free(pp, txq, tx_done);\r\ntxq->count = 0;\r\ntxq->txq_put_index = 0;\r\ntxq->txq_get_index = 0;\r\n}\r\nstatic void mvneta_tx_done_gbe(struct mvneta_port *pp, u32 cause_tx_done)\r\n{\r\nstruct mvneta_tx_queue *txq;\r\nstruct netdev_queue *nq;\r\nwhile (cause_tx_done) {\r\ntxq = mvneta_tx_done_policy(pp, cause_tx_done);\r\nnq = netdev_get_tx_queue(pp->dev, txq->id);\r\n__netif_tx_lock(nq, smp_processor_id());\r\nif (txq->count)\r\nmvneta_txq_done(pp, txq);\r\n__netif_tx_unlock(nq);\r\ncause_tx_done &= ~((1 << txq->id));\r\n}\r\n}\r\nstatic int mvneta_addr_crc(unsigned char *addr)\r\n{\r\nint crc = 0;\r\nint i;\r\nfor (i = 0; i < ETH_ALEN; i++) {\r\nint j;\r\ncrc = (crc ^ addr[i]) << 8;\r\nfor (j = 7; j >= 0; j--) {\r\nif (crc & (0x100 << j))\r\ncrc ^= 0x107 << j;\r\n}\r\n}\r\nreturn crc;\r\n}\r\nstatic void mvneta_set_special_mcast_addr(struct mvneta_port *pp,\r\nunsigned char last_byte,\r\nint queue)\r\n{\r\nunsigned int smc_table_reg;\r\nunsigned int tbl_offset;\r\nunsigned int reg_offset;\r\ntbl_offset = (last_byte / 4);\r\nreg_offset = last_byte % 4;\r\nsmc_table_reg = mvreg_read(pp, (MVNETA_DA_FILT_SPEC_MCAST\r\n+ tbl_offset * 4));\r\nif (queue == -1)\r\nsmc_table_reg &= ~(0xff << (8 * reg_offset));\r\nelse {\r\nsmc_table_reg &= ~(0xff << (8 * reg_offset));\r\nsmc_table_reg |= ((0x01 | (queue << 1)) << (8 * reg_offset));\r\n}\r\nmvreg_write(pp, MVNETA_DA_FILT_SPEC_MCAST + tbl_offset * 4,\r\nsmc_table_reg);\r\n}\r\nstatic void mvneta_set_other_mcast_addr(struct mvneta_port *pp,\r\nunsigned char crc8,\r\nint queue)\r\n{\r\nunsigned int omc_table_reg;\r\nunsigned int tbl_offset;\r\nunsigned int reg_offset;\r\ntbl_offset = (crc8 / 4) * 4;\r\nreg_offset = crc8 % 4;\r\nomc_table_reg = mvreg_read(pp, MVNETA_DA_FILT_OTH_MCAST + tbl_offset);\r\nif (queue == -1) {\r\nomc_table_reg &= ~(0xff << (8 * reg_offset));\r\n} else {\r\nomc_table_reg &= ~(0xff << (8 * reg_offset));\r\nomc_table_reg |= ((0x01 | (queue << 1)) << (8 * reg_offset));\r\n}\r\nmvreg_write(pp, MVNETA_DA_FILT_OTH_MCAST + tbl_offset, omc_table_reg);\r\n}\r\nstatic int mvneta_mcast_addr_set(struct mvneta_port *pp, unsigned char *p_addr,\r\nint queue)\r\n{\r\nunsigned char crc_result = 0;\r\nif (memcmp(p_addr, "\x01\x00\x5e\x00\x00", 5) == 0) {\r\nmvneta_set_special_mcast_addr(pp, p_addr[5], queue);\r\nreturn 0;\r\n}\r\ncrc_result = mvneta_addr_crc(p_addr);\r\nif (queue == -1) {\r\nif (pp->mcast_count[crc_result] == 0) {\r\nnetdev_info(pp->dev, "No valid Mcast for crc8=0x%02x\n",\r\ncrc_result);\r\nreturn -EINVAL;\r\n}\r\npp->mcast_count[crc_result]--;\r\nif (pp->mcast_count[crc_result] != 0) {\r\nnetdev_info(pp->dev,\r\n"After delete there are %d valid Mcast for crc8=0x%02x\n",\r\npp->mcast_count[crc_result], crc_result);\r\nreturn -EINVAL;\r\n}\r\n} else\r\npp->mcast_count[crc_result]++;\r\nmvneta_set_other_mcast_addr(pp, crc_result, queue);\r\nreturn 0;\r\n}\r\nstatic void mvneta_rx_unicast_promisc_set(struct mvneta_port *pp,\r\nint is_promisc)\r\n{\r\nu32 port_cfg_reg, val;\r\nport_cfg_reg = mvreg_read(pp, MVNETA_PORT_CONFIG);\r\nval = mvreg_read(pp, MVNETA_TYPE_PRIO);\r\nif (is_promisc) {\r\nport_cfg_reg |= MVNETA_UNI_PROMISC_MODE;\r\nval |= MVNETA_FORCE_UNI;\r\nmvreg_write(pp, MVNETA_MAC_ADDR_LOW, 0xffff);\r\nmvreg_write(pp, MVNETA_MAC_ADDR_HIGH, 0xffffffff);\r\n} else {\r\nport_cfg_reg &= ~MVNETA_UNI_PROMISC_MODE;\r\nval &= ~MVNETA_FORCE_UNI;\r\n}\r\nmvreg_write(pp, MVNETA_PORT_CONFIG, port_cfg_reg);\r\nmvreg_write(pp, MVNETA_TYPE_PRIO, val);\r\n}\r\nstatic void mvneta_set_rx_mode(struct net_device *dev)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nstruct netdev_hw_addr *ha;\r\nif (dev->flags & IFF_PROMISC) {\r\nmvneta_rx_unicast_promisc_set(pp, 1);\r\nmvneta_set_ucast_table(pp, rxq_def);\r\nmvneta_set_special_mcast_table(pp, rxq_def);\r\nmvneta_set_other_mcast_table(pp, rxq_def);\r\n} else {\r\nmvneta_rx_unicast_promisc_set(pp, 0);\r\nmvneta_set_ucast_table(pp, -1);\r\nmvneta_mac_addr_set(pp, dev->dev_addr, rxq_def);\r\nif (dev->flags & IFF_ALLMULTI) {\r\nmvneta_set_special_mcast_table(pp, rxq_def);\r\nmvneta_set_other_mcast_table(pp, rxq_def);\r\n} else {\r\nmvneta_set_special_mcast_table(pp, -1);\r\nmvneta_set_other_mcast_table(pp, -1);\r\nif (!netdev_mc_empty(dev)) {\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nmvneta_mcast_addr_set(pp, ha->addr,\r\nrxq_def);\r\n}\r\n}\r\n}\r\n}\r\n}\r\nstatic irqreturn_t mvneta_isr(int irq, void *dev_id)\r\n{\r\nstruct mvneta_port *pp = (struct mvneta_port *)dev_id;\r\nmvreg_write(pp, MVNETA_INTR_NEW_MASK, 0);\r\nnapi_schedule(&pp->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int mvneta_poll(struct napi_struct *napi, int budget)\r\n{\r\nint rx_done = 0;\r\nu32 cause_rx_tx;\r\nunsigned long flags;\r\nstruct mvneta_port *pp = netdev_priv(napi->dev);\r\nif (!netif_running(pp->dev)) {\r\nnapi_complete(napi);\r\nreturn rx_done;\r\n}\r\ncause_rx_tx = mvreg_read(pp, MVNETA_INTR_NEW_CAUSE) &\r\n(MVNETA_RX_INTR_MASK(rxq_number) | MVNETA_TX_INTR_MASK(txq_number));\r\nif (cause_rx_tx & MVNETA_TX_INTR_MASK_ALL) {\r\nmvneta_tx_done_gbe(pp, (cause_rx_tx & MVNETA_TX_INTR_MASK_ALL));\r\ncause_rx_tx &= ~MVNETA_TX_INTR_MASK_ALL;\r\n}\r\ncause_rx_tx |= pp->cause_rx_tx;\r\nif (rxq_number > 1) {\r\nwhile ((cause_rx_tx & MVNETA_RX_INTR_MASK_ALL) && (budget > 0)) {\r\nint count;\r\nstruct mvneta_rx_queue *rxq;\r\nrxq = mvneta_rx_policy(pp, cause_rx_tx);\r\nif (!rxq)\r\nbreak;\r\ncount = mvneta_rx(pp, budget, rxq);\r\nrx_done += count;\r\nbudget -= count;\r\nif (budget > 0) {\r\ncause_rx_tx &= ~((1 << rxq->id) << 8);\r\n}\r\n}\r\n} else {\r\nrx_done = mvneta_rx(pp, budget, &pp->rxqs[rxq_def]);\r\nbudget -= rx_done;\r\n}\r\nif (budget > 0) {\r\ncause_rx_tx = 0;\r\nnapi_complete(napi);\r\nlocal_irq_save(flags);\r\nmvreg_write(pp, MVNETA_INTR_NEW_MASK,\r\nMVNETA_RX_INTR_MASK(rxq_number) | MVNETA_TX_INTR_MASK(txq_number));\r\nlocal_irq_restore(flags);\r\n}\r\npp->cause_rx_tx = cause_rx_tx;\r\nreturn rx_done;\r\n}\r\nstatic int mvneta_rxq_fill(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,\r\nint num)\r\n{\r\nint i;\r\nfor (i = 0; i < num; i++) {\r\nmemset(rxq->descs + i, 0, sizeof(struct mvneta_rx_desc));\r\nif (mvneta_rx_refill(pp, rxq->descs + i) != 0) {\r\nnetdev_err(pp->dev, "%s:rxq %d, %d of %d buffs filled\n",\r\n__func__, rxq->id, i, num);\r\nbreak;\r\n}\r\n}\r\nmvneta_rxq_non_occup_desc_add(pp, rxq, i);\r\nreturn i;\r\n}\r\nstatic void mvneta_tx_reset(struct mvneta_port *pp)\r\n{\r\nint queue;\r\nfor (queue = 0; queue < txq_number; queue++)\r\nmvneta_txq_done_force(pp, &pp->txqs[queue]);\r\nmvreg_write(pp, MVNETA_PORT_TX_RESET, MVNETA_PORT_TX_DMA_RESET);\r\nmvreg_write(pp, MVNETA_PORT_TX_RESET, 0);\r\n}\r\nstatic void mvneta_rx_reset(struct mvneta_port *pp)\r\n{\r\nmvreg_write(pp, MVNETA_PORT_RX_RESET, MVNETA_PORT_RX_DMA_RESET);\r\nmvreg_write(pp, MVNETA_PORT_RX_RESET, 0);\r\n}\r\nstatic int mvneta_rxq_init(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq)\r\n{\r\nrxq->size = pp->rx_ring_size;\r\nrxq->descs = dma_alloc_coherent(pp->dev->dev.parent,\r\nrxq->size * MVNETA_DESC_ALIGNED_SIZE,\r\n&rxq->descs_phys, GFP_KERNEL);\r\nif (rxq->descs == NULL)\r\nreturn -ENOMEM;\r\nBUG_ON(rxq->descs !=\r\nPTR_ALIGN(rxq->descs, MVNETA_CPU_D_CACHE_LINE_SIZE));\r\nrxq->last_desc = rxq->size - 1;\r\nmvreg_write(pp, MVNETA_RXQ_BASE_ADDR_REG(rxq->id), rxq->descs_phys);\r\nmvreg_write(pp, MVNETA_RXQ_SIZE_REG(rxq->id), rxq->size);\r\nmvneta_rxq_offset_set(pp, rxq, NET_SKB_PAD);\r\nmvneta_rx_pkts_coal_set(pp, rxq, rxq->pkts_coal);\r\nmvneta_rx_time_coal_set(pp, rxq, rxq->time_coal);\r\nmvneta_rxq_buf_size_set(pp, rxq, MVNETA_RX_BUF_SIZE(pp->pkt_size));\r\nmvneta_rxq_bm_disable(pp, rxq);\r\nmvneta_rxq_fill(pp, rxq, rxq->size);\r\nreturn 0;\r\n}\r\nstatic void mvneta_rxq_deinit(struct mvneta_port *pp,\r\nstruct mvneta_rx_queue *rxq)\r\n{\r\nmvneta_rxq_drop_pkts(pp, rxq);\r\nif (rxq->descs)\r\ndma_free_coherent(pp->dev->dev.parent,\r\nrxq->size * MVNETA_DESC_ALIGNED_SIZE,\r\nrxq->descs,\r\nrxq->descs_phys);\r\nrxq->descs = NULL;\r\nrxq->last_desc = 0;\r\nrxq->next_desc_to_proc = 0;\r\nrxq->descs_phys = 0;\r\n}\r\nstatic int mvneta_txq_init(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\ntxq->size = pp->tx_ring_size;\r\ntxq->tx_stop_threshold = txq->size - MVNETA_MAX_SKB_DESCS;\r\ntxq->tx_wake_threshold = txq->tx_stop_threshold / 2;\r\ntxq->descs = dma_alloc_coherent(pp->dev->dev.parent,\r\ntxq->size * MVNETA_DESC_ALIGNED_SIZE,\r\n&txq->descs_phys, GFP_KERNEL);\r\nif (txq->descs == NULL)\r\nreturn -ENOMEM;\r\nBUG_ON(txq->descs !=\r\nPTR_ALIGN(txq->descs, MVNETA_CPU_D_CACHE_LINE_SIZE));\r\ntxq->last_desc = txq->size - 1;\r\nmvreg_write(pp, MVETH_TXQ_TOKEN_CFG_REG(txq->id), 0x03ffffff);\r\nmvreg_write(pp, MVETH_TXQ_TOKEN_COUNT_REG(txq->id), 0x3fffffff);\r\nmvreg_write(pp, MVNETA_TXQ_BASE_ADDR_REG(txq->id), txq->descs_phys);\r\nmvreg_write(pp, MVNETA_TXQ_SIZE_REG(txq->id), txq->size);\r\ntxq->tx_skb = kmalloc(txq->size * sizeof(*txq->tx_skb), GFP_KERNEL);\r\nif (txq->tx_skb == NULL) {\r\ndma_free_coherent(pp->dev->dev.parent,\r\ntxq->size * MVNETA_DESC_ALIGNED_SIZE,\r\ntxq->descs, txq->descs_phys);\r\nreturn -ENOMEM;\r\n}\r\ntxq->tso_hdrs = dma_alloc_coherent(pp->dev->dev.parent,\r\ntxq->size * TSO_HEADER_SIZE,\r\n&txq->tso_hdrs_phys, GFP_KERNEL);\r\nif (txq->tso_hdrs == NULL) {\r\nkfree(txq->tx_skb);\r\ndma_free_coherent(pp->dev->dev.parent,\r\ntxq->size * MVNETA_DESC_ALIGNED_SIZE,\r\ntxq->descs, txq->descs_phys);\r\nreturn -ENOMEM;\r\n}\r\nmvneta_tx_done_pkts_coal_set(pp, txq, txq->done_pkts_coal);\r\nreturn 0;\r\n}\r\nstatic void mvneta_txq_deinit(struct mvneta_port *pp,\r\nstruct mvneta_tx_queue *txq)\r\n{\r\nkfree(txq->tx_skb);\r\nif (txq->tso_hdrs)\r\ndma_free_coherent(pp->dev->dev.parent,\r\ntxq->size * TSO_HEADER_SIZE,\r\ntxq->tso_hdrs, txq->tso_hdrs_phys);\r\nif (txq->descs)\r\ndma_free_coherent(pp->dev->dev.parent,\r\ntxq->size * MVNETA_DESC_ALIGNED_SIZE,\r\ntxq->descs, txq->descs_phys);\r\ntxq->descs = NULL;\r\ntxq->last_desc = 0;\r\ntxq->next_desc_to_proc = 0;\r\ntxq->descs_phys = 0;\r\nmvreg_write(pp, MVETH_TXQ_TOKEN_CFG_REG(txq->id), 0);\r\nmvreg_write(pp, MVETH_TXQ_TOKEN_COUNT_REG(txq->id), 0);\r\nmvreg_write(pp, MVNETA_TXQ_BASE_ADDR_REG(txq->id), 0);\r\nmvreg_write(pp, MVNETA_TXQ_SIZE_REG(txq->id), 0);\r\n}\r\nstatic void mvneta_cleanup_txqs(struct mvneta_port *pp)\r\n{\r\nint queue;\r\nfor (queue = 0; queue < txq_number; queue++)\r\nmvneta_txq_deinit(pp, &pp->txqs[queue]);\r\n}\r\nstatic void mvneta_cleanup_rxqs(struct mvneta_port *pp)\r\n{\r\nint queue;\r\nfor (queue = 0; queue < rxq_number; queue++)\r\nmvneta_rxq_deinit(pp, &pp->rxqs[queue]);\r\n}\r\nstatic int mvneta_setup_rxqs(struct mvneta_port *pp)\r\n{\r\nint queue;\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nint err = mvneta_rxq_init(pp, &pp->rxqs[queue]);\r\nif (err) {\r\nnetdev_err(pp->dev, "%s: can't create rxq=%d\n",\r\n__func__, queue);\r\nmvneta_cleanup_rxqs(pp);\r\nreturn err;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int mvneta_setup_txqs(struct mvneta_port *pp)\r\n{\r\nint queue;\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nint err = mvneta_txq_init(pp, &pp->txqs[queue]);\r\nif (err) {\r\nnetdev_err(pp->dev, "%s: can't create txq=%d\n",\r\n__func__, queue);\r\nmvneta_cleanup_txqs(pp);\r\nreturn err;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void mvneta_start_dev(struct mvneta_port *pp)\r\n{\r\nmvneta_max_rx_size_set(pp, pp->pkt_size);\r\nmvneta_txq_max_tx_size_set(pp, pp->pkt_size);\r\nmvneta_port_enable(pp);\r\nnapi_enable(&pp->napi);\r\nmvreg_write(pp, MVNETA_INTR_NEW_MASK,\r\nMVNETA_RX_INTR_MASK(rxq_number) | MVNETA_TX_INTR_MASK(txq_number));\r\nphy_start(pp->phy_dev);\r\nnetif_tx_start_all_queues(pp->dev);\r\n}\r\nstatic void mvneta_stop_dev(struct mvneta_port *pp)\r\n{\r\nphy_stop(pp->phy_dev);\r\nnapi_disable(&pp->napi);\r\nnetif_carrier_off(pp->dev);\r\nmvneta_port_down(pp);\r\nnetif_tx_stop_all_queues(pp->dev);\r\nmvneta_port_disable(pp);\r\nmvreg_write(pp, MVNETA_INTR_MISC_CAUSE, 0);\r\nmvreg_write(pp, MVNETA_INTR_OLD_CAUSE, 0);\r\nmvreg_write(pp, MVNETA_INTR_NEW_MASK, 0);\r\nmvreg_write(pp, MVNETA_INTR_OLD_MASK, 0);\r\nmvreg_write(pp, MVNETA_INTR_MISC_MASK, 0);\r\nmvneta_tx_reset(pp);\r\nmvneta_rx_reset(pp);\r\n}\r\nstatic int mvneta_check_mtu_valid(struct net_device *dev, int mtu)\r\n{\r\nif (mtu < 68) {\r\nnetdev_err(dev, "cannot change mtu to less than 68\n");\r\nreturn -EINVAL;\r\n}\r\nif (mtu > 9676) {\r\nnetdev_info(dev, "Illegal MTU value %d, round to 9676\n", mtu);\r\nmtu = 9676;\r\n}\r\nif (!IS_ALIGNED(MVNETA_RX_PKT_SIZE(mtu), 8)) {\r\nnetdev_info(dev, "Illegal MTU value %d, rounding to %d\n",\r\nmtu, ALIGN(MVNETA_RX_PKT_SIZE(mtu), 8));\r\nmtu = ALIGN(MVNETA_RX_PKT_SIZE(mtu), 8);\r\n}\r\nreturn mtu;\r\n}\r\nstatic int mvneta_change_mtu(struct net_device *dev, int mtu)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nint ret;\r\nmtu = mvneta_check_mtu_valid(dev, mtu);\r\nif (mtu < 0)\r\nreturn -EINVAL;\r\ndev->mtu = mtu;\r\nif (!netif_running(dev))\r\nreturn 0;\r\nmvneta_stop_dev(pp);\r\nmvneta_cleanup_txqs(pp);\r\nmvneta_cleanup_rxqs(pp);\r\npp->pkt_size = MVNETA_RX_PKT_SIZE(dev->mtu);\r\npp->frag_size = SKB_DATA_ALIGN(MVNETA_RX_BUF_SIZE(pp->pkt_size)) +\r\nSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\r\nret = mvneta_setup_rxqs(pp);\r\nif (ret) {\r\nnetdev_err(dev, "unable to setup rxqs after MTU change\n");\r\nreturn ret;\r\n}\r\nret = mvneta_setup_txqs(pp);\r\nif (ret) {\r\nnetdev_err(dev, "unable to setup txqs after MTU change\n");\r\nreturn ret;\r\n}\r\nmvneta_start_dev(pp);\r\nmvneta_port_up(pp);\r\nreturn 0;\r\n}\r\nstatic void mvneta_get_mac_addr(struct mvneta_port *pp, unsigned char *addr)\r\n{\r\nu32 mac_addr_l, mac_addr_h;\r\nmac_addr_l = mvreg_read(pp, MVNETA_MAC_ADDR_LOW);\r\nmac_addr_h = mvreg_read(pp, MVNETA_MAC_ADDR_HIGH);\r\naddr[0] = (mac_addr_h >> 24) & 0xFF;\r\naddr[1] = (mac_addr_h >> 16) & 0xFF;\r\naddr[2] = (mac_addr_h >> 8) & 0xFF;\r\naddr[3] = mac_addr_h & 0xFF;\r\naddr[4] = (mac_addr_l >> 8) & 0xFF;\r\naddr[5] = mac_addr_l & 0xFF;\r\n}\r\nstatic int mvneta_set_mac_addr(struct net_device *dev, void *addr)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nstruct sockaddr *sockaddr = addr;\r\nint ret;\r\nret = eth_prepare_mac_addr_change(dev, addr);\r\nif (ret < 0)\r\nreturn ret;\r\nmvneta_mac_addr_set(pp, dev->dev_addr, -1);\r\nmvneta_mac_addr_set(pp, sockaddr->sa_data, rxq_def);\r\neth_commit_mac_addr_change(dev, addr);\r\nreturn 0;\r\n}\r\nstatic void mvneta_adjust_link(struct net_device *ndev)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(ndev);\r\nstruct phy_device *phydev = pp->phy_dev;\r\nint status_change = 0;\r\nif (phydev->link) {\r\nif ((pp->speed != phydev->speed) ||\r\n(pp->duplex != phydev->duplex)) {\r\nu32 val;\r\nval = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\r\nval &= ~(MVNETA_GMAC_CONFIG_MII_SPEED |\r\nMVNETA_GMAC_CONFIG_GMII_SPEED |\r\nMVNETA_GMAC_CONFIG_FULL_DUPLEX |\r\nMVNETA_GMAC_AN_SPEED_EN |\r\nMVNETA_GMAC_AN_DUPLEX_EN);\r\nif (phydev->duplex)\r\nval |= MVNETA_GMAC_CONFIG_FULL_DUPLEX;\r\nif (phydev->speed == SPEED_1000)\r\nval |= MVNETA_GMAC_CONFIG_GMII_SPEED;\r\nelse if (phydev->speed == SPEED_100)\r\nval |= MVNETA_GMAC_CONFIG_MII_SPEED;\r\nmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, val);\r\npp->duplex = phydev->duplex;\r\npp->speed = phydev->speed;\r\n}\r\n}\r\nif (phydev->link != pp->link) {\r\nif (!phydev->link) {\r\npp->duplex = -1;\r\npp->speed = 0;\r\n}\r\npp->link = phydev->link;\r\nstatus_change = 1;\r\n}\r\nif (status_change) {\r\nif (phydev->link) {\r\nu32 val = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\r\nval |= (MVNETA_GMAC_FORCE_LINK_PASS |\r\nMVNETA_GMAC_FORCE_LINK_DOWN);\r\nmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, val);\r\nmvneta_port_up(pp);\r\nnetdev_info(pp->dev, "link up\n");\r\n} else {\r\nmvneta_port_down(pp);\r\nnetdev_info(pp->dev, "link down\n");\r\n}\r\n}\r\n}\r\nstatic int mvneta_mdio_probe(struct mvneta_port *pp)\r\n{\r\nstruct phy_device *phy_dev;\r\nphy_dev = of_phy_connect(pp->dev, pp->phy_node, mvneta_adjust_link, 0,\r\npp->phy_interface);\r\nif (!phy_dev) {\r\nnetdev_err(pp->dev, "could not find the PHY\n");\r\nreturn -ENODEV;\r\n}\r\nphy_dev->supported &= PHY_GBIT_FEATURES;\r\nphy_dev->advertising = phy_dev->supported;\r\npp->phy_dev = phy_dev;\r\npp->link = 0;\r\npp->duplex = 0;\r\npp->speed = 0;\r\nreturn 0;\r\n}\r\nstatic void mvneta_mdio_remove(struct mvneta_port *pp)\r\n{\r\nphy_disconnect(pp->phy_dev);\r\npp->phy_dev = NULL;\r\n}\r\nstatic int mvneta_open(struct net_device *dev)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nint ret;\r\npp->pkt_size = MVNETA_RX_PKT_SIZE(pp->dev->mtu);\r\npp->frag_size = SKB_DATA_ALIGN(MVNETA_RX_BUF_SIZE(pp->pkt_size)) +\r\nSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\r\nret = mvneta_setup_rxqs(pp);\r\nif (ret)\r\nreturn ret;\r\nret = mvneta_setup_txqs(pp);\r\nif (ret)\r\ngoto err_cleanup_rxqs;\r\nret = request_irq(pp->dev->irq, mvneta_isr, 0,\r\nMVNETA_DRIVER_NAME, pp);\r\nif (ret) {\r\nnetdev_err(pp->dev, "cannot request irq %d\n", pp->dev->irq);\r\ngoto err_cleanup_txqs;\r\n}\r\nnetif_carrier_off(pp->dev);\r\nret = mvneta_mdio_probe(pp);\r\nif (ret < 0) {\r\nnetdev_err(dev, "cannot probe MDIO bus\n");\r\ngoto err_free_irq;\r\n}\r\nmvneta_start_dev(pp);\r\nreturn 0;\r\nerr_free_irq:\r\nfree_irq(pp->dev->irq, pp);\r\nerr_cleanup_txqs:\r\nmvneta_cleanup_txqs(pp);\r\nerr_cleanup_rxqs:\r\nmvneta_cleanup_rxqs(pp);\r\nreturn ret;\r\n}\r\nstatic int mvneta_stop(struct net_device *dev)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nmvneta_stop_dev(pp);\r\nmvneta_mdio_remove(pp);\r\nfree_irq(dev->irq, pp);\r\nmvneta_cleanup_rxqs(pp);\r\nmvneta_cleanup_txqs(pp);\r\nreturn 0;\r\n}\r\nstatic int mvneta_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nint ret;\r\nif (!pp->phy_dev)\r\nreturn -ENOTSUPP;\r\nret = phy_mii_ioctl(pp->phy_dev, ifr, cmd);\r\nif (!ret)\r\nmvneta_adjust_link(dev);\r\nreturn ret;\r\n}\r\nint mvneta_ethtool_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nif (!pp->phy_dev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_gset(pp->phy_dev, cmd);\r\n}\r\nint mvneta_ethtool_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nif (!pp->phy_dev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_sset(pp->phy_dev, cmd);\r\n}\r\nstatic int mvneta_ethtool_set_coalesce(struct net_device *dev,\r\nstruct ethtool_coalesce *c)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nint queue;\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\r\nrxq->time_coal = c->rx_coalesce_usecs;\r\nrxq->pkts_coal = c->rx_max_coalesced_frames;\r\nmvneta_rx_pkts_coal_set(pp, rxq, rxq->pkts_coal);\r\nmvneta_rx_time_coal_set(pp, rxq, rxq->time_coal);\r\n}\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nstruct mvneta_tx_queue *txq = &pp->txqs[queue];\r\ntxq->done_pkts_coal = c->tx_max_coalesced_frames;\r\nmvneta_tx_done_pkts_coal_set(pp, txq, txq->done_pkts_coal);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mvneta_ethtool_get_coalesce(struct net_device *dev,\r\nstruct ethtool_coalesce *c)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nc->rx_coalesce_usecs = pp->rxqs[0].time_coal;\r\nc->rx_max_coalesced_frames = pp->rxqs[0].pkts_coal;\r\nc->tx_max_coalesced_frames = pp->txqs[0].done_pkts_coal;\r\nreturn 0;\r\n}\r\nstatic void mvneta_ethtool_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *drvinfo)\r\n{\r\nstrlcpy(drvinfo->driver, MVNETA_DRIVER_NAME,\r\nsizeof(drvinfo->driver));\r\nstrlcpy(drvinfo->version, MVNETA_DRIVER_VERSION,\r\nsizeof(drvinfo->version));\r\nstrlcpy(drvinfo->bus_info, dev_name(&dev->dev),\r\nsizeof(drvinfo->bus_info));\r\n}\r\nstatic void mvneta_ethtool_get_ringparam(struct net_device *netdev,\r\nstruct ethtool_ringparam *ring)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(netdev);\r\nring->rx_max_pending = MVNETA_MAX_RXD;\r\nring->tx_max_pending = MVNETA_MAX_TXD;\r\nring->rx_pending = pp->rx_ring_size;\r\nring->tx_pending = pp->tx_ring_size;\r\n}\r\nstatic int mvneta_ethtool_set_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ring)\r\n{\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nif ((ring->rx_pending == 0) || (ring->tx_pending == 0))\r\nreturn -EINVAL;\r\npp->rx_ring_size = ring->rx_pending < MVNETA_MAX_RXD ?\r\nring->rx_pending : MVNETA_MAX_RXD;\r\npp->tx_ring_size = clamp_t(u16, ring->tx_pending,\r\nMVNETA_MAX_SKB_DESCS * 2, MVNETA_MAX_TXD);\r\nif (pp->tx_ring_size != ring->tx_pending)\r\nnetdev_warn(dev, "TX queue size set to %u (requested %u)\n",\r\npp->tx_ring_size, ring->tx_pending);\r\nif (netif_running(dev)) {\r\nmvneta_stop(dev);\r\nif (mvneta_open(dev)) {\r\nnetdev_err(dev,\r\n"error on opening device after ring param change\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int mvneta_init(struct device *dev, struct mvneta_port *pp)\r\n{\r\nint queue;\r\nmvneta_port_disable(pp);\r\nmvneta_defaults_set(pp);\r\npp->txqs = devm_kcalloc(dev, txq_number, sizeof(struct mvneta_tx_queue),\r\nGFP_KERNEL);\r\nif (!pp->txqs)\r\nreturn -ENOMEM;\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nstruct mvneta_tx_queue *txq = &pp->txqs[queue];\r\ntxq->id = queue;\r\ntxq->size = pp->tx_ring_size;\r\ntxq->done_pkts_coal = MVNETA_TXDONE_COAL_PKTS;\r\n}\r\npp->rxqs = devm_kcalloc(dev, rxq_number, sizeof(struct mvneta_rx_queue),\r\nGFP_KERNEL);\r\nif (!pp->rxqs)\r\nreturn -ENOMEM;\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\r\nrxq->id = queue;\r\nrxq->size = pp->rx_ring_size;\r\nrxq->pkts_coal = MVNETA_RX_COAL_PKTS;\r\nrxq->time_coal = MVNETA_RX_COAL_USEC;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mvneta_conf_mbus_windows(struct mvneta_port *pp,\r\nconst struct mbus_dram_target_info *dram)\r\n{\r\nu32 win_enable;\r\nu32 win_protect;\r\nint i;\r\nfor (i = 0; i < 6; i++) {\r\nmvreg_write(pp, MVNETA_WIN_BASE(i), 0);\r\nmvreg_write(pp, MVNETA_WIN_SIZE(i), 0);\r\nif (i < 4)\r\nmvreg_write(pp, MVNETA_WIN_REMAP(i), 0);\r\n}\r\nwin_enable = 0x3f;\r\nwin_protect = 0;\r\nfor (i = 0; i < dram->num_cs; i++) {\r\nconst struct mbus_dram_window *cs = dram->cs + i;\r\nmvreg_write(pp, MVNETA_WIN_BASE(i), (cs->base & 0xffff0000) |\r\n(cs->mbus_attr << 8) | dram->mbus_dram_target_id);\r\nmvreg_write(pp, MVNETA_WIN_SIZE(i),\r\n(cs->size - 1) & 0xffff0000);\r\nwin_enable &= ~(1 << i);\r\nwin_protect |= 3 << (2 * i);\r\n}\r\nmvreg_write(pp, MVNETA_BASE_ADDR_ENABLE, win_enable);\r\n}\r\nstatic int mvneta_port_power_up(struct mvneta_port *pp, int phy_mode)\r\n{\r\nu32 ctrl;\r\nmvreg_write(pp, MVNETA_UNIT_INTR_CAUSE, 0);\r\nctrl = mvreg_read(pp, MVNETA_GMAC_CTRL_2);\r\nswitch(phy_mode) {\r\ncase PHY_INTERFACE_MODE_QSGMII:\r\nmvreg_write(pp, MVNETA_SERDES_CFG, MVNETA_QSGMII_SERDES_PROTO);\r\nctrl |= MVNETA_GMAC2_PCS_ENABLE | MVNETA_GMAC2_PORT_RGMII;\r\nbreak;\r\ncase PHY_INTERFACE_MODE_SGMII:\r\nmvreg_write(pp, MVNETA_SERDES_CFG, MVNETA_SGMII_SERDES_PROTO);\r\nctrl |= MVNETA_GMAC2_PCS_ENABLE | MVNETA_GMAC2_PORT_RGMII;\r\nbreak;\r\ncase PHY_INTERFACE_MODE_RGMII:\r\ncase PHY_INTERFACE_MODE_RGMII_ID:\r\nctrl |= MVNETA_GMAC2_PORT_RGMII;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nctrl &= ~MVNETA_GMAC2_PORT_RESET;\r\nmvreg_write(pp, MVNETA_GMAC_CTRL_2, ctrl);\r\nwhile ((mvreg_read(pp, MVNETA_GMAC_CTRL_2) &\r\nMVNETA_GMAC2_PORT_RESET) != 0)\r\ncontinue;\r\nreturn 0;\r\n}\r\nstatic int mvneta_probe(struct platform_device *pdev)\r\n{\r\nconst struct mbus_dram_target_info *dram_target_info;\r\nstruct resource *res;\r\nstruct device_node *dn = pdev->dev.of_node;\r\nstruct device_node *phy_node;\r\nstruct mvneta_port *pp;\r\nstruct net_device *dev;\r\nconst char *dt_mac_addr;\r\nchar hw_mac_addr[ETH_ALEN];\r\nconst char *mac_from;\r\nint phy_mode;\r\nint err;\r\nif (rxq_def != 0) {\r\ndev_err(&pdev->dev, "Invalid rxq_def argument: %d\n", rxq_def);\r\nreturn -EINVAL;\r\n}\r\ndev = alloc_etherdev_mqs(sizeof(struct mvneta_port), txq_number, rxq_number);\r\nif (!dev)\r\nreturn -ENOMEM;\r\ndev->irq = irq_of_parse_and_map(dn, 0);\r\nif (dev->irq == 0) {\r\nerr = -EINVAL;\r\ngoto err_free_netdev;\r\n}\r\nphy_node = of_parse_phandle(dn, "phy", 0);\r\nif (!phy_node) {\r\nif (!of_phy_is_fixed_link(dn)) {\r\ndev_err(&pdev->dev, "no PHY specified\n");\r\nerr = -ENODEV;\r\ngoto err_free_irq;\r\n}\r\nerr = of_phy_register_fixed_link(dn);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "cannot register fixed PHY\n");\r\ngoto err_free_irq;\r\n}\r\nphy_node = of_node_get(dn);\r\n}\r\nphy_mode = of_get_phy_mode(dn);\r\nif (phy_mode < 0) {\r\ndev_err(&pdev->dev, "incorrect phy-mode\n");\r\nerr = -EINVAL;\r\ngoto err_put_phy_node;\r\n}\r\ndev->tx_queue_len = MVNETA_MAX_TXD;\r\ndev->watchdog_timeo = 5 * HZ;\r\ndev->netdev_ops = &mvneta_netdev_ops;\r\ndev->ethtool_ops = &mvneta_eth_tool_ops;\r\npp = netdev_priv(dev);\r\npp->phy_node = phy_node;\r\npp->phy_interface = phy_mode;\r\npp->clk = devm_clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(pp->clk)) {\r\nerr = PTR_ERR(pp->clk);\r\ngoto err_put_phy_node;\r\n}\r\nclk_prepare_enable(pp->clk);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\npp->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(pp->base)) {\r\nerr = PTR_ERR(pp->base);\r\ngoto err_clk;\r\n}\r\npp->stats = netdev_alloc_pcpu_stats(struct mvneta_pcpu_stats);\r\nif (!pp->stats) {\r\nerr = -ENOMEM;\r\ngoto err_clk;\r\n}\r\ndt_mac_addr = of_get_mac_address(dn);\r\nif (dt_mac_addr) {\r\nmac_from = "device tree";\r\nmemcpy(dev->dev_addr, dt_mac_addr, ETH_ALEN);\r\n} else {\r\nmvneta_get_mac_addr(pp, hw_mac_addr);\r\nif (is_valid_ether_addr(hw_mac_addr)) {\r\nmac_from = "hardware";\r\nmemcpy(dev->dev_addr, hw_mac_addr, ETH_ALEN);\r\n} else {\r\nmac_from = "random";\r\neth_hw_addr_random(dev);\r\n}\r\n}\r\npp->tx_ring_size = MVNETA_MAX_TXD;\r\npp->rx_ring_size = MVNETA_MAX_RXD;\r\npp->dev = dev;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nerr = mvneta_init(&pdev->dev, pp);\r\nif (err < 0)\r\ngoto err_free_stats;\r\nerr = mvneta_port_power_up(pp, phy_mode);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "can't power up port\n");\r\ngoto err_free_stats;\r\n}\r\ndram_target_info = mv_mbus_dram_info();\r\nif (dram_target_info)\r\nmvneta_conf_mbus_windows(pp, dram_target_info);\r\nnetif_napi_add(dev, &pp->napi, mvneta_poll, NAPI_POLL_WEIGHT);\r\ndev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO;\r\ndev->hw_features |= dev->features;\r\ndev->vlan_features |= dev->features;\r\ndev->priv_flags |= IFF_UNICAST_FLT;\r\ndev->gso_max_segs = MVNETA_MAX_TSO_SEGS;\r\nerr = register_netdev(dev);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "failed to register\n");\r\ngoto err_free_stats;\r\n}\r\nnetdev_info(dev, "Using %s mac address %pM\n", mac_from,\r\ndev->dev_addr);\r\nplatform_set_drvdata(pdev, pp->dev);\r\nreturn 0;\r\nerr_free_stats:\r\nfree_percpu(pp->stats);\r\nerr_clk:\r\nclk_disable_unprepare(pp->clk);\r\nerr_put_phy_node:\r\nof_node_put(phy_node);\r\nerr_free_irq:\r\nirq_dispose_mapping(dev->irq);\r\nerr_free_netdev:\r\nfree_netdev(dev);\r\nreturn err;\r\n}\r\nstatic int mvneta_remove(struct platform_device *pdev)\r\n{\r\nstruct net_device *dev = platform_get_drvdata(pdev);\r\nstruct mvneta_port *pp = netdev_priv(dev);\r\nunregister_netdev(dev);\r\nclk_disable_unprepare(pp->clk);\r\nfree_percpu(pp->stats);\r\nirq_dispose_mapping(dev->irq);\r\nof_node_put(pp->phy_node);\r\nfree_netdev(dev);\r\nreturn 0;\r\n}
