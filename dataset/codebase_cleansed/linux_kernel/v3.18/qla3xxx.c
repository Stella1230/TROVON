static int ql_sem_spinlock(struct ql3_adapter *qdev,\r\nu32 sem_mask, u32 sem_bits)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nunsigned int seconds = 3;\r\ndo {\r\nwritel((sem_mask | sem_bits),\r\n&port_regs->CommonRegs.semaphoreReg);\r\nvalue = readl(&port_regs->CommonRegs.semaphoreReg);\r\nif ((value & (sem_mask >> 16)) == sem_bits)\r\nreturn 0;\r\nssleep(1);\r\n} while (--seconds);\r\nreturn -1;\r\n}\r\nstatic void ql_sem_unlock(struct ql3_adapter *qdev, u32 sem_mask)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nwritel(sem_mask, &port_regs->CommonRegs.semaphoreReg);\r\nreadl(&port_regs->CommonRegs.semaphoreReg);\r\n}\r\nstatic int ql_sem_lock(struct ql3_adapter *qdev, u32 sem_mask, u32 sem_bits)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nwritel((sem_mask | sem_bits), &port_regs->CommonRegs.semaphoreReg);\r\nvalue = readl(&port_regs->CommonRegs.semaphoreReg);\r\nreturn ((value & (sem_mask >> 16)) == sem_bits);\r\n}\r\nstatic int ql_wait_for_drvr_lock(struct ql3_adapter *qdev)\r\n{\r\nint i = 0;\r\nwhile (i < 10) {\r\nif (i)\r\nssleep(1);\r\nif (ql_sem_lock(qdev,\r\nQL_DRVR_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index)\r\n* 2) << 1)) {\r\nnetdev_printk(KERN_DEBUG, qdev->ndev,\r\n"driver lock acquired\n");\r\nreturn 1;\r\n}\r\n}\r\nnetdev_err(qdev->ndev, "Timed out waiting for driver lock...\n");\r\nreturn 0;\r\n}\r\nstatic void ql_set_register_page(struct ql3_adapter *qdev, u32 page)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nwritel(((ISP_CONTROL_NP_MASK << 16) | page),\r\n&port_regs->CommonRegs.ispControlStatus);\r\nreadl(&port_regs->CommonRegs.ispControlStatus);\r\nqdev->current_page = page;\r\n}\r\nstatic u32 ql_read_common_reg_l(struct ql3_adapter *qdev, u32 __iomem *reg)\r\n{\r\nu32 value;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nvalue = readl(reg);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn value;\r\n}\r\nstatic u32 ql_read_common_reg(struct ql3_adapter *qdev, u32 __iomem *reg)\r\n{\r\nreturn readl(reg);\r\n}\r\nstatic u32 ql_read_page0_reg_l(struct ql3_adapter *qdev, u32 __iomem *reg)\r\n{\r\nu32 value;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nif (qdev->current_page != 0)\r\nql_set_register_page(qdev, 0);\r\nvalue = readl(reg);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn value;\r\n}\r\nstatic u32 ql_read_page0_reg(struct ql3_adapter *qdev, u32 __iomem *reg)\r\n{\r\nif (qdev->current_page != 0)\r\nql_set_register_page(qdev, 0);\r\nreturn readl(reg);\r\n}\r\nstatic void ql_write_common_reg_l(struct ql3_adapter *qdev,\r\nu32 __iomem *reg, u32 value)\r\n{\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nwritel(value, reg);\r\nreadl(reg);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\n}\r\nstatic void ql_write_common_reg(struct ql3_adapter *qdev,\r\nu32 __iomem *reg, u32 value)\r\n{\r\nwritel(value, reg);\r\nreadl(reg);\r\n}\r\nstatic void ql_write_nvram_reg(struct ql3_adapter *qdev,\r\nu32 __iomem *reg, u32 value)\r\n{\r\nwritel(value, reg);\r\nreadl(reg);\r\nudelay(1);\r\n}\r\nstatic void ql_write_page0_reg(struct ql3_adapter *qdev,\r\nu32 __iomem *reg, u32 value)\r\n{\r\nif (qdev->current_page != 0)\r\nql_set_register_page(qdev, 0);\r\nwritel(value, reg);\r\nreadl(reg);\r\n}\r\nstatic void ql_write_page1_reg(struct ql3_adapter *qdev,\r\nu32 __iomem *reg, u32 value)\r\n{\r\nif (qdev->current_page != 1)\r\nql_set_register_page(qdev, 1);\r\nwritel(value, reg);\r\nreadl(reg);\r\n}\r\nstatic void ql_write_page2_reg(struct ql3_adapter *qdev,\r\nu32 __iomem *reg, u32 value)\r\n{\r\nif (qdev->current_page != 2)\r\nql_set_register_page(qdev, 2);\r\nwritel(value, reg);\r\nreadl(reg);\r\n}\r\nstatic void ql_disable_interrupts(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nql_write_common_reg_l(qdev, &port_regs->CommonRegs.ispInterruptMaskReg,\r\n(ISP_IMR_ENABLE_INT << 16));\r\n}\r\nstatic void ql_enable_interrupts(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nql_write_common_reg_l(qdev, &port_regs->CommonRegs.ispInterruptMaskReg,\r\n((0xff << 16) | ISP_IMR_ENABLE_INT));\r\n}\r\nstatic void ql_release_to_lrg_buf_free_list(struct ql3_adapter *qdev,\r\nstruct ql_rcv_buf_cb *lrg_buf_cb)\r\n{\r\ndma_addr_t map;\r\nint err;\r\nlrg_buf_cb->next = NULL;\r\nif (qdev->lrg_buf_free_tail == NULL) {\r\nqdev->lrg_buf_free_head = qdev->lrg_buf_free_tail = lrg_buf_cb;\r\n} else {\r\nqdev->lrg_buf_free_tail->next = lrg_buf_cb;\r\nqdev->lrg_buf_free_tail = lrg_buf_cb;\r\n}\r\nif (!lrg_buf_cb->skb) {\r\nlrg_buf_cb->skb = netdev_alloc_skb(qdev->ndev,\r\nqdev->lrg_buffer_len);\r\nif (unlikely(!lrg_buf_cb->skb)) {\r\nqdev->lrg_buf_skb_check++;\r\n} else {\r\nskb_reserve(lrg_buf_cb->skb, QL_HEADER_SPACE);\r\nmap = pci_map_single(qdev->pdev,\r\nlrg_buf_cb->skb->data,\r\nqdev->lrg_buffer_len -\r\nQL_HEADER_SPACE,\r\nPCI_DMA_FROMDEVICE);\r\nerr = pci_dma_mapping_error(qdev->pdev, map);\r\nif (err) {\r\nnetdev_err(qdev->ndev,\r\n"PCI mapping failed with error: %d\n",\r\nerr);\r\ndev_kfree_skb(lrg_buf_cb->skb);\r\nlrg_buf_cb->skb = NULL;\r\nqdev->lrg_buf_skb_check++;\r\nreturn;\r\n}\r\nlrg_buf_cb->buf_phy_addr_low =\r\ncpu_to_le32(LS_64BITS(map));\r\nlrg_buf_cb->buf_phy_addr_high =\r\ncpu_to_le32(MS_64BITS(map));\r\ndma_unmap_addr_set(lrg_buf_cb, mapaddr, map);\r\ndma_unmap_len_set(lrg_buf_cb, maplen,\r\nqdev->lrg_buffer_len -\r\nQL_HEADER_SPACE);\r\n}\r\n}\r\nqdev->lrg_buf_free_count++;\r\n}\r\nstatic struct ql_rcv_buf_cb *ql_get_from_lrg_buf_free_list(struct ql3_adapter\r\n*qdev)\r\n{\r\nstruct ql_rcv_buf_cb *lrg_buf_cb = qdev->lrg_buf_free_head;\r\nif (lrg_buf_cb != NULL) {\r\nqdev->lrg_buf_free_head = lrg_buf_cb->next;\r\nif (qdev->lrg_buf_free_head == NULL)\r\nqdev->lrg_buf_free_tail = NULL;\r\nqdev->lrg_buf_free_count--;\r\n}\r\nreturn lrg_buf_cb;\r\n}\r\nstatic void fm93c56a_select(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\n__iomem u32 *spir = &port_regs->CommonRegs.serialPortInterfaceReg;\r\nqdev->eeprom_cmd_data = AUBURN_EEPROM_CS_1;\r\nql_write_nvram_reg(qdev, spir, ISP_NVRAM_MASK | qdev->eeprom_cmd_data);\r\nql_write_nvram_reg(qdev, spir,\r\n((ISP_NVRAM_MASK << 16) | qdev->eeprom_cmd_data));\r\n}\r\nstatic void fm93c56a_cmd(struct ql3_adapter *qdev, u32 cmd, u32 eepromAddr)\r\n{\r\nint i;\r\nu32 mask;\r\nu32 dataBit;\r\nu32 previousBit;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\n__iomem u32 *spir = &port_regs->CommonRegs.serialPortInterfaceReg;\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\nAUBURN_EEPROM_DO_1));\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\nAUBURN_EEPROM_DO_1 | AUBURN_EEPROM_CLK_RISE));\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\nAUBURN_EEPROM_DO_1 | AUBURN_EEPROM_CLK_FALL));\r\nmask = 1 << (FM93C56A_CMD_BITS - 1);\r\npreviousBit = 0xffff;\r\nfor (i = 0; i < FM93C56A_CMD_BITS; i++) {\r\ndataBit = (cmd & mask)\r\n? AUBURN_EEPROM_DO_1\r\n: AUBURN_EEPROM_DO_0;\r\nif (previousBit != dataBit) {\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK |\r\nqdev->eeprom_cmd_data | dataBit));\r\npreviousBit = dataBit;\r\n}\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\ndataBit | AUBURN_EEPROM_CLK_RISE));\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\ndataBit | AUBURN_EEPROM_CLK_FALL));\r\ncmd = cmd << 1;\r\n}\r\nmask = 1 << (addrBits - 1);\r\npreviousBit = 0xffff;\r\nfor (i = 0; i < addrBits; i++) {\r\ndataBit = (eepromAddr & mask) ? AUBURN_EEPROM_DO_1\r\n: AUBURN_EEPROM_DO_0;\r\nif (previousBit != dataBit) {\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK |\r\nqdev->eeprom_cmd_data | dataBit));\r\npreviousBit = dataBit;\r\n}\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\ndataBit | AUBURN_EEPROM_CLK_RISE));\r\nql_write_nvram_reg(qdev, spir,\r\n(ISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\ndataBit | AUBURN_EEPROM_CLK_FALL));\r\neepromAddr = eepromAddr << 1;\r\n}\r\n}\r\nstatic void fm93c56a_deselect(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\n__iomem u32 *spir = &port_regs->CommonRegs.serialPortInterfaceReg;\r\nqdev->eeprom_cmd_data = AUBURN_EEPROM_CS_0;\r\nql_write_nvram_reg(qdev, spir, ISP_NVRAM_MASK | qdev->eeprom_cmd_data);\r\n}\r\nstatic void fm93c56a_datain(struct ql3_adapter *qdev, unsigned short *value)\r\n{\r\nint i;\r\nu32 data = 0;\r\nu32 dataBit;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\n__iomem u32 *spir = &port_regs->CommonRegs.serialPortInterfaceReg;\r\nfor (i = 0; i < dataBits; i++) {\r\nql_write_nvram_reg(qdev, spir,\r\nISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\nAUBURN_EEPROM_CLK_RISE);\r\nql_write_nvram_reg(qdev, spir,\r\nISP_NVRAM_MASK | qdev->eeprom_cmd_data |\r\nAUBURN_EEPROM_CLK_FALL);\r\ndataBit = (ql_read_common_reg(qdev, spir) &\r\nAUBURN_EEPROM_DI_1) ? 1 : 0;\r\ndata = (data << 1) | dataBit;\r\n}\r\n*value = (u16)data;\r\n}\r\nstatic void eeprom_readword(struct ql3_adapter *qdev,\r\nu32 eepromAddr, unsigned short *value)\r\n{\r\nfm93c56a_select(qdev);\r\nfm93c56a_cmd(qdev, (int)FM93C56A_READ, eepromAddr);\r\nfm93c56a_datain(qdev, value);\r\nfm93c56a_deselect(qdev);\r\n}\r\nstatic void ql_set_mac_addr(struct net_device *ndev, u16 *addr)\r\n{\r\n__le16 *p = (__le16 *)ndev->dev_addr;\r\np[0] = cpu_to_le16(addr[0]);\r\np[1] = cpu_to_le16(addr[1]);\r\np[2] = cpu_to_le16(addr[2]);\r\n}\r\nstatic int ql_get_nvram_params(struct ql3_adapter *qdev)\r\n{\r\nu16 *pEEPROMData;\r\nu16 checksum = 0;\r\nu32 index;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\npEEPROMData = (u16 *)&qdev->nvram_data;\r\nqdev->eeprom_cmd_data = 0;\r\nif (ql_sem_spinlock(qdev, QL_NVRAM_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index) *\r\n2) << 10)) {\r\npr_err("%s: Failed ql_sem_spinlock()\n", __func__);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn -1;\r\n}\r\nfor (index = 0; index < EEPROM_SIZE; index++) {\r\neeprom_readword(qdev, index, pEEPROMData);\r\nchecksum += *pEEPROMData;\r\npEEPROMData++;\r\n}\r\nql_sem_unlock(qdev, QL_NVRAM_SEM_MASK);\r\nif (checksum != 0) {\r\nnetdev_err(qdev->ndev, "checksum should be zero, is %x!!\n",\r\nchecksum);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn -1;\r\n}\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn checksum;\r\n}\r\nstatic int ql_wait_for_mii_ready(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 temp;\r\nint count = 1000;\r\nwhile (count) {\r\ntemp = ql_read_page0_reg(qdev, &port_regs->macMIIStatusReg);\r\nif (!(temp & MAC_MII_STATUS_BSY))\r\nreturn 0;\r\nudelay(10);\r\ncount--;\r\n}\r\nreturn -1;\r\n}\r\nstatic void ql_mii_enable_scan_mode(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 scanControl;\r\nif (qdev->numPorts > 1) {\r\nscanControl = MAC_MII_CONTROL_AS | MAC_MII_CONTROL_SC;\r\n} else {\r\nscanControl = MAC_MII_CONTROL_SC;\r\n}\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtAddrReg,\r\nPHYAddr[0] | MII_SCAN_REGISTER);\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtControlReg,\r\n(scanControl) |\r\n((MAC_MII_CONTROL_SC | MAC_MII_CONTROL_AS) << 16));\r\n}\r\nstatic u8 ql_mii_disable_scan_mode(struct ql3_adapter *qdev)\r\n{\r\nu8 ret;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nif (ql_read_page0_reg(qdev, &port_regs->macMIIMgmtControlReg) &\r\n(MAC_MII_CONTROL_AS | MAC_MII_CONTROL_SC)) {\r\nret = 1;\r\n} else {\r\nret = 0;\r\n}\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtAddrReg,\r\nPHYAddr[0] | MII_SCAN_REGISTER);\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtControlReg,\r\n((MAC_MII_CONTROL_SC | MAC_MII_CONTROL_AS |\r\nMAC_MII_CONTROL_RC) << 16));\r\nreturn ret;\r\n}\r\nstatic int ql_mii_write_reg_ex(struct ql3_adapter *qdev,\r\nu16 regAddr, u16 value, u32 phyAddr)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu8 scanWasEnabled;\r\nscanWasEnabled = ql_mii_disable_scan_mode(qdev);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtAddrReg,\r\nphyAddr | regAddr);\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtDataReg, value);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\nif (scanWasEnabled)\r\nql_mii_enable_scan_mode(qdev);\r\nreturn 0;\r\n}\r\nstatic int ql_mii_read_reg_ex(struct ql3_adapter *qdev, u16 regAddr,\r\nu16 *value, u32 phyAddr)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu8 scanWasEnabled;\r\nu32 temp;\r\nscanWasEnabled = ql_mii_disable_scan_mode(qdev);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtAddrReg,\r\nphyAddr | regAddr);\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtControlReg,\r\n(MAC_MII_CONTROL_RC << 16));\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtControlReg,\r\n(MAC_MII_CONTROL_RC << 16) | MAC_MII_CONTROL_RC);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\ntemp = ql_read_page0_reg(qdev, &port_regs->macMIIMgmtDataReg);\r\n*value = (u16) temp;\r\nif (scanWasEnabled)\r\nql_mii_enable_scan_mode(qdev);\r\nreturn 0;\r\n}\r\nstatic int ql_mii_write_reg(struct ql3_adapter *qdev, u16 regAddr, u16 value)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nql_mii_disable_scan_mode(qdev);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtAddrReg,\r\nqdev->PHYAddr | regAddr);\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtDataReg, value);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\nql_mii_enable_scan_mode(qdev);\r\nreturn 0;\r\n}\r\nstatic int ql_mii_read_reg(struct ql3_adapter *qdev, u16 regAddr, u16 *value)\r\n{\r\nu32 temp;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nql_mii_disable_scan_mode(qdev);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtAddrReg,\r\nqdev->PHYAddr | regAddr);\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtControlReg,\r\n(MAC_MII_CONTROL_RC << 16));\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtControlReg,\r\n(MAC_MII_CONTROL_RC << 16) | MAC_MII_CONTROL_RC);\r\nif (ql_wait_for_mii_ready(qdev)) {\r\nnetif_warn(qdev, link, qdev->ndev, TIMED_OUT_MSG);\r\nreturn -1;\r\n}\r\ntemp = ql_read_page0_reg(qdev, &port_regs->macMIIMgmtDataReg);\r\n*value = (u16) temp;\r\nql_mii_enable_scan_mode(qdev);\r\nreturn 0;\r\n}\r\nstatic void ql_petbi_reset(struct ql3_adapter *qdev)\r\n{\r\nql_mii_write_reg(qdev, PETBI_CONTROL_REG, PETBI_CTRL_SOFT_RESET);\r\n}\r\nstatic void ql_petbi_start_neg(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nql_mii_read_reg(qdev, PETBI_TBI_CTRL, &reg);\r\nreg |= PETBI_TBI_AUTO_SENSE;\r\nql_mii_write_reg(qdev, PETBI_TBI_CTRL, reg);\r\nql_mii_write_reg(qdev, PETBI_NEG_ADVER,\r\nPETBI_NEG_PAUSE | PETBI_NEG_DUPLEX);\r\nql_mii_write_reg(qdev, PETBI_CONTROL_REG,\r\nPETBI_CTRL_AUTO_NEG | PETBI_CTRL_RESTART_NEG |\r\nPETBI_CTRL_FULL_DUPLEX | PETBI_CTRL_SPEED_1000);\r\n}\r\nstatic void ql_petbi_reset_ex(struct ql3_adapter *qdev)\r\n{\r\nql_mii_write_reg_ex(qdev, PETBI_CONTROL_REG, PETBI_CTRL_SOFT_RESET,\r\nPHYAddr[qdev->mac_index]);\r\n}\r\nstatic void ql_petbi_start_neg_ex(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nql_mii_read_reg_ex(qdev, PETBI_TBI_CTRL, &reg,\r\nPHYAddr[qdev->mac_index]);\r\nreg |= PETBI_TBI_AUTO_SENSE;\r\nql_mii_write_reg_ex(qdev, PETBI_TBI_CTRL, reg,\r\nPHYAddr[qdev->mac_index]);\r\nql_mii_write_reg_ex(qdev, PETBI_NEG_ADVER,\r\nPETBI_NEG_PAUSE | PETBI_NEG_DUPLEX,\r\nPHYAddr[qdev->mac_index]);\r\nql_mii_write_reg_ex(qdev, PETBI_CONTROL_REG,\r\nPETBI_CTRL_AUTO_NEG | PETBI_CTRL_RESTART_NEG |\r\nPETBI_CTRL_FULL_DUPLEX | PETBI_CTRL_SPEED_1000,\r\nPHYAddr[qdev->mac_index]);\r\n}\r\nstatic void ql_petbi_init(struct ql3_adapter *qdev)\r\n{\r\nql_petbi_reset(qdev);\r\nql_petbi_start_neg(qdev);\r\n}\r\nstatic void ql_petbi_init_ex(struct ql3_adapter *qdev)\r\n{\r\nql_petbi_reset_ex(qdev);\r\nql_petbi_start_neg_ex(qdev);\r\n}\r\nstatic int ql_is_petbi_neg_pause(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nif (ql_mii_read_reg(qdev, PETBI_NEG_PARTNER, &reg) < 0)\r\nreturn 0;\r\nreturn (reg & PETBI_NEG_PAUSE_MASK) == PETBI_NEG_PAUSE;\r\n}\r\nstatic void phyAgereSpecificInit(struct ql3_adapter *qdev, u32 miiAddr)\r\n{\r\nnetdev_info(qdev->ndev, "enabling Agere specific PHY\n");\r\nql_mii_write_reg_ex(qdev, 0x00, 0x1940, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x12, 0x840e, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x10, 0x8805, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x11, 0xf03e, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x10, 0x8806, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x11, 0x003e, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x10, 0x8807, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x11, 0x1f00, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x10, 0x2806, miiAddr);\r\nql_mii_write_reg_ex(qdev, 0x11,\r\n0x0020 | (PHYAddr[qdev->mac_index] >> 8), miiAddr);\r\nql_mii_write_reg(qdev, 0x12, 0x840a);\r\nql_mii_write_reg(qdev, 0x00, 0x1140);\r\nql_mii_write_reg(qdev, 0x1c, 0xfaf0);\r\n}\r\nstatic enum PHY_DEVICE_TYPE getPhyType(struct ql3_adapter *qdev,\r\nu16 phyIdReg0, u16 phyIdReg1)\r\n{\r\nenum PHY_DEVICE_TYPE result = PHY_TYPE_UNKNOWN;\r\nu32 oui;\r\nu16 model;\r\nint i;\r\nif (phyIdReg0 == 0xffff)\r\nreturn result;\r\nif (phyIdReg1 == 0xffff)\r\nreturn result;\r\noui = (phyIdReg0 << 6) | ((phyIdReg1 & PHY_OUI_1_MASK) >> 10);\r\nmodel = (phyIdReg1 & PHY_MODEL_MASK) >> 4;\r\nfor (i = 0; i < MAX_PHY_DEV_TYPES; i++) {\r\nif ((oui == PHY_DEVICES[i].phyIdOUI) &&\r\n(model == PHY_DEVICES[i].phyIdModel)) {\r\nnetdev_info(qdev->ndev, "Phy: %s\n",\r\nPHY_DEVICES[i].name);\r\nresult = PHY_DEVICES[i].phyDevice;\r\nbreak;\r\n}\r\n}\r\nreturn result;\r\n}\r\nstatic int ql_phy_get_speed(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nswitch (qdev->phyType) {\r\ncase PHY_AGERE_ET1011C: {\r\nif (ql_mii_read_reg(qdev, 0x1A, &reg) < 0)\r\nreturn 0;\r\nreg = (reg >> 8) & 3;\r\nbreak;\r\n}\r\ndefault:\r\nif (ql_mii_read_reg(qdev, AUX_CONTROL_STATUS, &reg) < 0)\r\nreturn 0;\r\nreg = (((reg & 0x18) >> 3) & 3);\r\n}\r\nswitch (reg) {\r\ncase 2:\r\nreturn SPEED_1000;\r\ncase 1:\r\nreturn SPEED_100;\r\ncase 0:\r\nreturn SPEED_10;\r\ndefault:\r\nreturn -1;\r\n}\r\n}\r\nstatic int ql_is_full_dup(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nswitch (qdev->phyType) {\r\ncase PHY_AGERE_ET1011C: {\r\nif (ql_mii_read_reg(qdev, 0x1A, &reg))\r\nreturn 0;\r\nreturn ((reg & 0x0080) && (reg & 0x1000)) != 0;\r\n}\r\ncase PHY_VITESSE_VSC8211:\r\ndefault: {\r\nif (ql_mii_read_reg(qdev, AUX_CONTROL_STATUS, &reg) < 0)\r\nreturn 0;\r\nreturn (reg & PHY_AUX_DUPLEX_STAT) != 0;\r\n}\r\n}\r\n}\r\nstatic int ql_is_phy_neg_pause(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nif (ql_mii_read_reg(qdev, PHY_NEG_PARTNER, &reg) < 0)\r\nreturn 0;\r\nreturn (reg & PHY_NEG_PAUSE) != 0;\r\n}\r\nstatic int PHY_Setup(struct ql3_adapter *qdev)\r\n{\r\nu16 reg1;\r\nu16 reg2;\r\nbool agereAddrChangeNeeded = false;\r\nu32 miiAddr = 0;\r\nint err;\r\nerr = ql_mii_read_reg(qdev, PHY_ID_0_REG, &reg1);\r\nif (err != 0) {\r\nnetdev_err(qdev->ndev, "Could not read from reg PHY_ID_0_REG\n");\r\nreturn err;\r\n}\r\nerr = ql_mii_read_reg(qdev, PHY_ID_1_REG, &reg2);\r\nif (err != 0) {\r\nnetdev_err(qdev->ndev, "Could not read from reg PHY_ID_1_REG\n");\r\nreturn err;\r\n}\r\nif ((reg1 == 0xffff) || (reg2 == 0xffff)) {\r\nif (qdev->mac_index == 0)\r\nmiiAddr = MII_AGERE_ADDR_1;\r\nelse\r\nmiiAddr = MII_AGERE_ADDR_2;\r\nerr = ql_mii_read_reg_ex(qdev, PHY_ID_0_REG, &reg1, miiAddr);\r\nif (err != 0) {\r\nnetdev_err(qdev->ndev,\r\n"Could not read from reg PHY_ID_0_REG after Agere detected\n");\r\nreturn err;\r\n}\r\nerr = ql_mii_read_reg_ex(qdev, PHY_ID_1_REG, &reg2, miiAddr);\r\nif (err != 0) {\r\nnetdev_err(qdev->ndev, "Could not read from reg PHY_ID_1_REG after Agere detected\n");\r\nreturn err;\r\n}\r\nagereAddrChangeNeeded = true;\r\n}\r\nqdev->phyType = getPhyType(qdev, reg1, reg2);\r\nif ((qdev->phyType == PHY_AGERE_ET1011C) && agereAddrChangeNeeded) {\r\nphyAgereSpecificInit(qdev, miiAddr);\r\n} else if (qdev->phyType == PHY_TYPE_UNKNOWN) {\r\nnetdev_err(qdev->ndev, "PHY is unknown\n");\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ql_mac_enable(struct ql3_adapter *qdev, u32 enable)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nif (enable)\r\nvalue = (MAC_CONFIG_REG_PE | (MAC_CONFIG_REG_PE << 16));\r\nelse\r\nvalue = (MAC_CONFIG_REG_PE << 16);\r\nif (qdev->mac_index)\r\nql_write_page0_reg(qdev, &port_regs->mac1ConfigReg, value);\r\nelse\r\nql_write_page0_reg(qdev, &port_regs->mac0ConfigReg, value);\r\n}\r\nstatic void ql_mac_cfg_soft_reset(struct ql3_adapter *qdev, u32 enable)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nif (enable)\r\nvalue = (MAC_CONFIG_REG_SR | (MAC_CONFIG_REG_SR << 16));\r\nelse\r\nvalue = (MAC_CONFIG_REG_SR << 16);\r\nif (qdev->mac_index)\r\nql_write_page0_reg(qdev, &port_regs->mac1ConfigReg, value);\r\nelse\r\nql_write_page0_reg(qdev, &port_regs->mac0ConfigReg, value);\r\n}\r\nstatic void ql_mac_cfg_gig(struct ql3_adapter *qdev, u32 enable)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nif (enable)\r\nvalue = (MAC_CONFIG_REG_GM | (MAC_CONFIG_REG_GM << 16));\r\nelse\r\nvalue = (MAC_CONFIG_REG_GM << 16);\r\nif (qdev->mac_index)\r\nql_write_page0_reg(qdev, &port_regs->mac1ConfigReg, value);\r\nelse\r\nql_write_page0_reg(qdev, &port_regs->mac0ConfigReg, value);\r\n}\r\nstatic void ql_mac_cfg_full_dup(struct ql3_adapter *qdev, u32 enable)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nif (enable)\r\nvalue = (MAC_CONFIG_REG_FD | (MAC_CONFIG_REG_FD << 16));\r\nelse\r\nvalue = (MAC_CONFIG_REG_FD << 16);\r\nif (qdev->mac_index)\r\nql_write_page0_reg(qdev, &port_regs->mac1ConfigReg, value);\r\nelse\r\nql_write_page0_reg(qdev, &port_regs->mac0ConfigReg, value);\r\n}\r\nstatic void ql_mac_cfg_pause(struct ql3_adapter *qdev, u32 enable)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nif (enable)\r\nvalue =\r\n((MAC_CONFIG_REG_TF | MAC_CONFIG_REG_RF) |\r\n((MAC_CONFIG_REG_TF | MAC_CONFIG_REG_RF) << 16));\r\nelse\r\nvalue = ((MAC_CONFIG_REG_TF | MAC_CONFIG_REG_RF) << 16);\r\nif (qdev->mac_index)\r\nql_write_page0_reg(qdev, &port_regs->mac1ConfigReg, value);\r\nelse\r\nql_write_page0_reg(qdev, &port_regs->mac0ConfigReg, value);\r\n}\r\nstatic int ql_is_fiber(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 bitToCheck = 0;\r\nu32 temp;\r\nswitch (qdev->mac_index) {\r\ncase 0:\r\nbitToCheck = PORT_STATUS_SM0;\r\nbreak;\r\ncase 1:\r\nbitToCheck = PORT_STATUS_SM1;\r\nbreak;\r\n}\r\ntemp = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nreturn (temp & bitToCheck) != 0;\r\n}\r\nstatic int ql_is_auto_cfg(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nql_mii_read_reg(qdev, 0x00, &reg);\r\nreturn (reg & 0x1000) != 0;\r\n}\r\nstatic int ql_is_auto_neg_complete(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 bitToCheck = 0;\r\nu32 temp;\r\nswitch (qdev->mac_index) {\r\ncase 0:\r\nbitToCheck = PORT_STATUS_AC0;\r\nbreak;\r\ncase 1:\r\nbitToCheck = PORT_STATUS_AC1;\r\nbreak;\r\n}\r\ntemp = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nif (temp & bitToCheck) {\r\nnetif_info(qdev, link, qdev->ndev, "Auto-Negotiate complete\n");\r\nreturn 1;\r\n}\r\nnetif_info(qdev, link, qdev->ndev, "Auto-Negotiate incomplete\n");\r\nreturn 0;\r\n}\r\nstatic int ql_is_neg_pause(struct ql3_adapter *qdev)\r\n{\r\nif (ql_is_fiber(qdev))\r\nreturn ql_is_petbi_neg_pause(qdev);\r\nelse\r\nreturn ql_is_phy_neg_pause(qdev);\r\n}\r\nstatic int ql_auto_neg_error(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 bitToCheck = 0;\r\nu32 temp;\r\nswitch (qdev->mac_index) {\r\ncase 0:\r\nbitToCheck = PORT_STATUS_AE0;\r\nbreak;\r\ncase 1:\r\nbitToCheck = PORT_STATUS_AE1;\r\nbreak;\r\n}\r\ntemp = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nreturn (temp & bitToCheck) != 0;\r\n}\r\nstatic u32 ql_get_link_speed(struct ql3_adapter *qdev)\r\n{\r\nif (ql_is_fiber(qdev))\r\nreturn SPEED_1000;\r\nelse\r\nreturn ql_phy_get_speed(qdev);\r\n}\r\nstatic int ql_is_link_full_dup(struct ql3_adapter *qdev)\r\n{\r\nif (ql_is_fiber(qdev))\r\nreturn 1;\r\nelse\r\nreturn ql_is_full_dup(qdev);\r\n}\r\nstatic int ql_link_down_detect(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 bitToCheck = 0;\r\nu32 temp;\r\nswitch (qdev->mac_index) {\r\ncase 0:\r\nbitToCheck = ISP_CONTROL_LINK_DN_0;\r\nbreak;\r\ncase 1:\r\nbitToCheck = ISP_CONTROL_LINK_DN_1;\r\nbreak;\r\n}\r\ntemp =\r\nql_read_common_reg(qdev, &port_regs->CommonRegs.ispControlStatus);\r\nreturn (temp & bitToCheck) != 0;\r\n}\r\nstatic int ql_link_down_detect_clear(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nswitch (qdev->mac_index) {\r\ncase 0:\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.ispControlStatus,\r\n(ISP_CONTROL_LINK_DN_0) |\r\n(ISP_CONTROL_LINK_DN_0 << 16));\r\nbreak;\r\ncase 1:\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.ispControlStatus,\r\n(ISP_CONTROL_LINK_DN_1) |\r\n(ISP_CONTROL_LINK_DN_1 << 16));\r\nbreak;\r\ndefault:\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ql_this_adapter_controls_port(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 bitToCheck = 0;\r\nu32 temp;\r\nswitch (qdev->mac_index) {\r\ncase 0:\r\nbitToCheck = PORT_STATUS_F1_ENABLED;\r\nbreak;\r\ncase 1:\r\nbitToCheck = PORT_STATUS_F3_ENABLED;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\ntemp = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nif (temp & bitToCheck) {\r\nnetif_printk(qdev, link, KERN_DEBUG, qdev->ndev,\r\n"not link master\n");\r\nreturn 0;\r\n}\r\nnetif_printk(qdev, link, KERN_DEBUG, qdev->ndev, "link master\n");\r\nreturn 1;\r\n}\r\nstatic void ql_phy_reset_ex(struct ql3_adapter *qdev)\r\n{\r\nql_mii_write_reg_ex(qdev, CONTROL_REG, PHY_CTRL_SOFT_RESET,\r\nPHYAddr[qdev->mac_index]);\r\n}\r\nstatic void ql_phy_start_neg_ex(struct ql3_adapter *qdev)\r\n{\r\nu16 reg;\r\nu16 portConfiguration;\r\nif (qdev->phyType == PHY_AGERE_ET1011C)\r\nql_mii_write_reg(qdev, 0x13, 0x0000);\r\nif (qdev->mac_index == 0)\r\nportConfiguration =\r\nqdev->nvram_data.macCfg_port0.portConfiguration;\r\nelse\r\nportConfiguration =\r\nqdev->nvram_data.macCfg_port1.portConfiguration;\r\nif (portConfiguration == 0)\r\nportConfiguration = PORT_CONFIG_DEFAULT;\r\nql_mii_read_reg_ex(qdev, PHY_GIG_CONTROL, &reg,\r\nPHYAddr[qdev->mac_index]);\r\nreg &= ~PHY_GIG_ALL_PARAMS;\r\nif (portConfiguration & PORT_CONFIG_1000MB_SPEED) {\r\nif (portConfiguration & PORT_CONFIG_FULL_DUPLEX_ENABLED)\r\nreg |= PHY_GIG_ADV_1000F;\r\nelse\r\nreg |= PHY_GIG_ADV_1000H;\r\n}\r\nql_mii_write_reg_ex(qdev, PHY_GIG_CONTROL, reg,\r\nPHYAddr[qdev->mac_index]);\r\nql_mii_read_reg_ex(qdev, PHY_NEG_ADVER, &reg,\r\nPHYAddr[qdev->mac_index]);\r\nreg &= ~PHY_NEG_ALL_PARAMS;\r\nif (portConfiguration & PORT_CONFIG_SYM_PAUSE_ENABLED)\r\nreg |= PHY_NEG_ASY_PAUSE | PHY_NEG_SYM_PAUSE;\r\nif (portConfiguration & PORT_CONFIG_FULL_DUPLEX_ENABLED) {\r\nif (portConfiguration & PORT_CONFIG_100MB_SPEED)\r\nreg |= PHY_NEG_ADV_100F;\r\nif (portConfiguration & PORT_CONFIG_10MB_SPEED)\r\nreg |= PHY_NEG_ADV_10F;\r\n}\r\nif (portConfiguration & PORT_CONFIG_HALF_DUPLEX_ENABLED) {\r\nif (portConfiguration & PORT_CONFIG_100MB_SPEED)\r\nreg |= PHY_NEG_ADV_100H;\r\nif (portConfiguration & PORT_CONFIG_10MB_SPEED)\r\nreg |= PHY_NEG_ADV_10H;\r\n}\r\nif (portConfiguration & PORT_CONFIG_1000MB_SPEED)\r\nreg |= 1;\r\nql_mii_write_reg_ex(qdev, PHY_NEG_ADVER, reg,\r\nPHYAddr[qdev->mac_index]);\r\nql_mii_read_reg_ex(qdev, CONTROL_REG, &reg, PHYAddr[qdev->mac_index]);\r\nql_mii_write_reg_ex(qdev, CONTROL_REG,\r\nreg | PHY_CTRL_RESTART_NEG | PHY_CTRL_AUTO_NEG,\r\nPHYAddr[qdev->mac_index]);\r\n}\r\nstatic void ql_phy_init_ex(struct ql3_adapter *qdev)\r\n{\r\nql_phy_reset_ex(qdev);\r\nPHY_Setup(qdev);\r\nql_phy_start_neg_ex(qdev);\r\n}\r\nstatic u32 ql_get_link_state(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 bitToCheck = 0;\r\nu32 temp, linkState;\r\nswitch (qdev->mac_index) {\r\ncase 0:\r\nbitToCheck = PORT_STATUS_UP0;\r\nbreak;\r\ncase 1:\r\nbitToCheck = PORT_STATUS_UP1;\r\nbreak;\r\n}\r\ntemp = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nif (temp & bitToCheck)\r\nlinkState = LS_UP;\r\nelse\r\nlinkState = LS_DOWN;\r\nreturn linkState;\r\n}\r\nstatic int ql_port_start(struct ql3_adapter *qdev)\r\n{\r\nif (ql_sem_spinlock(qdev, QL_PHY_GIO_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index) *\r\n2) << 7)) {\r\nnetdev_err(qdev->ndev, "Could not get hw lock for GIO\n");\r\nreturn -1;\r\n}\r\nif (ql_is_fiber(qdev)) {\r\nql_petbi_init(qdev);\r\n} else {\r\nql_phy_init_ex(qdev);\r\n}\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nreturn 0;\r\n}\r\nstatic int ql_finish_auto_neg(struct ql3_adapter *qdev)\r\n{\r\nif (ql_sem_spinlock(qdev, QL_PHY_GIO_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index) *\r\n2) << 7))\r\nreturn -1;\r\nif (!ql_auto_neg_error(qdev)) {\r\nif (test_bit(QL_LINK_MASTER, &qdev->flags)) {\r\nnetif_printk(qdev, link, KERN_DEBUG, qdev->ndev,\r\n"Configuring link\n");\r\nql_mac_cfg_soft_reset(qdev, 1);\r\nql_mac_cfg_gig(qdev,\r\n(ql_get_link_speed\r\n(qdev) ==\r\nSPEED_1000));\r\nql_mac_cfg_full_dup(qdev,\r\nql_is_link_full_dup\r\n(qdev));\r\nql_mac_cfg_pause(qdev,\r\nql_is_neg_pause\r\n(qdev));\r\nql_mac_cfg_soft_reset(qdev, 0);\r\nnetif_printk(qdev, link, KERN_DEBUG, qdev->ndev,\r\n"Enabling mac\n");\r\nql_mac_enable(qdev, 1);\r\n}\r\nqdev->port_link_state = LS_UP;\r\nnetif_start_queue(qdev->ndev);\r\nnetif_carrier_on(qdev->ndev);\r\nnetif_info(qdev, link, qdev->ndev,\r\n"Link is up at %d Mbps, %s duplex\n",\r\nql_get_link_speed(qdev),\r\nql_is_link_full_dup(qdev) ? "full" : "half");\r\n} else {\r\nif (test_bit(QL_LINK_MASTER, &qdev->flags)) {\r\nnetif_printk(qdev, link, KERN_DEBUG, qdev->ndev,\r\n"Remote error detected. Calling ql_port_start()\n");\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nif (ql_port_start(qdev))\r\nreturn -1;\r\nreturn 0;\r\n}\r\n}\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nreturn 0;\r\n}\r\nstatic void ql_link_state_machine_work(struct work_struct *work)\r\n{\r\nstruct ql3_adapter *qdev =\r\ncontainer_of(work, struct ql3_adapter, link_state_work.work);\r\nu32 curr_link_state;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\ncurr_link_state = ql_get_link_state(qdev);\r\nif (test_bit(QL_RESET_ACTIVE, &qdev->flags)) {\r\nnetif_info(qdev, link, qdev->ndev,\r\n"Reset in progress, skip processing link state\n");\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nmod_timer(&qdev->adapter_timer, jiffies + HZ * 1);\r\nreturn;\r\n}\r\nswitch (qdev->port_link_state) {\r\ndefault:\r\nif (test_bit(QL_LINK_MASTER, &qdev->flags))\r\nql_port_start(qdev);\r\nqdev->port_link_state = LS_DOWN;\r\ncase LS_DOWN:\r\nif (curr_link_state == LS_UP) {\r\nnetif_info(qdev, link, qdev->ndev, "Link is up\n");\r\nif (ql_is_auto_neg_complete(qdev))\r\nql_finish_auto_neg(qdev);\r\nif (qdev->port_link_state == LS_UP)\r\nql_link_down_detect_clear(qdev);\r\nqdev->port_link_state = LS_UP;\r\n}\r\nbreak;\r\ncase LS_UP:\r\nif (curr_link_state == LS_DOWN) {\r\nnetif_info(qdev, link, qdev->ndev, "Link is down\n");\r\nqdev->port_link_state = LS_DOWN;\r\n}\r\nif (ql_link_down_detect(qdev))\r\nqdev->port_link_state = LS_DOWN;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nmod_timer(&qdev->adapter_timer, jiffies + HZ * 1);\r\n}\r\nstatic void ql_get_phy_owner(struct ql3_adapter *qdev)\r\n{\r\nif (ql_this_adapter_controls_port(qdev))\r\nset_bit(QL_LINK_MASTER, &qdev->flags);\r\nelse\r\nclear_bit(QL_LINK_MASTER, &qdev->flags);\r\n}\r\nstatic void ql_init_scan_mode(struct ql3_adapter *qdev)\r\n{\r\nql_mii_enable_scan_mode(qdev);\r\nif (test_bit(QL_LINK_OPTICAL, &qdev->flags)) {\r\nif (ql_this_adapter_controls_port(qdev))\r\nql_petbi_init_ex(qdev);\r\n} else {\r\nif (ql_this_adapter_controls_port(qdev))\r\nql_phy_init_ex(qdev);\r\n}\r\n}\r\nstatic int ql_mii_setup(struct ql3_adapter *qdev)\r\n{\r\nu32 reg;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nif (ql_sem_spinlock(qdev, QL_PHY_GIO_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index) *\r\n2) << 7))\r\nreturn -1;\r\nif (qdev->device_id == QL3032_DEVICE_ID)\r\nql_write_page0_reg(qdev,\r\n&port_regs->macMIIMgmtControlReg, 0x0f00000);\r\nreg = MAC_MII_CONTROL_CLK_SEL_DIV28;\r\nql_write_page0_reg(qdev, &port_regs->macMIIMgmtControlReg,\r\nreg | ((MAC_MII_CONTROL_CLK_SEL_MASK) << 16));\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nreturn 0;\r\n}\r\nstatic int ql_get_auto_cfg_status(struct ql3_adapter *qdev)\r\n{\r\nint status;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nif (ql_sem_spinlock(qdev, QL_PHY_GIO_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE |\r\n(qdev->mac_index) * 2) << 7)) {\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn 0;\r\n}\r\nstatus = ql_is_auto_cfg(qdev);\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn status;\r\n}\r\nstatic u32 ql_get_speed(struct ql3_adapter *qdev)\r\n{\r\nu32 status;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nif (ql_sem_spinlock(qdev, QL_PHY_GIO_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE |\r\n(qdev->mac_index) * 2) << 7)) {\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn 0;\r\n}\r\nstatus = ql_get_link_speed(qdev);\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn status;\r\n}\r\nstatic int ql_get_full_dup(struct ql3_adapter *qdev)\r\n{\r\nint status;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nif (ql_sem_spinlock(qdev, QL_PHY_GIO_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE |\r\n(qdev->mac_index) * 2) << 7)) {\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn 0;\r\n}\r\nstatus = ql_is_link_full_dup(qdev);\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn status;\r\n}\r\nstatic int ql_get_settings(struct net_device *ndev, struct ethtool_cmd *ecmd)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\necmd->transceiver = XCVR_INTERNAL;\r\necmd->supported = ql_supported_modes(qdev);\r\nif (test_bit(QL_LINK_OPTICAL, &qdev->flags)) {\r\necmd->port = PORT_FIBRE;\r\n} else {\r\necmd->port = PORT_TP;\r\necmd->phy_address = qdev->PHYAddr;\r\n}\r\necmd->advertising = ql_supported_modes(qdev);\r\necmd->autoneg = ql_get_auto_cfg_status(qdev);\r\nethtool_cmd_speed_set(ecmd, ql_get_speed(qdev));\r\necmd->duplex = ql_get_full_dup(qdev);\r\nreturn 0;\r\n}\r\nstatic void ql_get_drvinfo(struct net_device *ndev,\r\nstruct ethtool_drvinfo *drvinfo)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nstrlcpy(drvinfo->driver, ql3xxx_driver_name, sizeof(drvinfo->driver));\r\nstrlcpy(drvinfo->version, ql3xxx_driver_version,\r\nsizeof(drvinfo->version));\r\nstrlcpy(drvinfo->bus_info, pci_name(qdev->pdev),\r\nsizeof(drvinfo->bus_info));\r\ndrvinfo->regdump_len = 0;\r\ndrvinfo->eedump_len = 0;\r\n}\r\nstatic u32 ql_get_msglevel(struct net_device *ndev)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nreturn qdev->msg_enable;\r\n}\r\nstatic void ql_set_msglevel(struct net_device *ndev, u32 value)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nqdev->msg_enable = value;\r\n}\r\nstatic void ql_get_pauseparam(struct net_device *ndev,\r\nstruct ethtool_pauseparam *pause)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 reg;\r\nif (qdev->mac_index == 0)\r\nreg = ql_read_page0_reg(qdev, &port_regs->mac0ConfigReg);\r\nelse\r\nreg = ql_read_page0_reg(qdev, &port_regs->mac1ConfigReg);\r\npause->autoneg = ql_get_auto_cfg_status(qdev);\r\npause->rx_pause = (reg & MAC_CONFIG_REG_RF) >> 2;\r\npause->tx_pause = (reg & MAC_CONFIG_REG_TF) >> 1;\r\n}\r\nstatic int ql_populate_free_queue(struct ql3_adapter *qdev)\r\n{\r\nstruct ql_rcv_buf_cb *lrg_buf_cb = qdev->lrg_buf_free_head;\r\ndma_addr_t map;\r\nint err;\r\nwhile (lrg_buf_cb) {\r\nif (!lrg_buf_cb->skb) {\r\nlrg_buf_cb->skb =\r\nnetdev_alloc_skb(qdev->ndev,\r\nqdev->lrg_buffer_len);\r\nif (unlikely(!lrg_buf_cb->skb)) {\r\nnetdev_printk(KERN_DEBUG, qdev->ndev,\r\n"Failed netdev_alloc_skb()\n");\r\nbreak;\r\n} else {\r\nskb_reserve(lrg_buf_cb->skb, QL_HEADER_SPACE);\r\nmap = pci_map_single(qdev->pdev,\r\nlrg_buf_cb->skb->data,\r\nqdev->lrg_buffer_len -\r\nQL_HEADER_SPACE,\r\nPCI_DMA_FROMDEVICE);\r\nerr = pci_dma_mapping_error(qdev->pdev, map);\r\nif (err) {\r\nnetdev_err(qdev->ndev,\r\n"PCI mapping failed with error: %d\n",\r\nerr);\r\ndev_kfree_skb(lrg_buf_cb->skb);\r\nlrg_buf_cb->skb = NULL;\r\nbreak;\r\n}\r\nlrg_buf_cb->buf_phy_addr_low =\r\ncpu_to_le32(LS_64BITS(map));\r\nlrg_buf_cb->buf_phy_addr_high =\r\ncpu_to_le32(MS_64BITS(map));\r\ndma_unmap_addr_set(lrg_buf_cb, mapaddr, map);\r\ndma_unmap_len_set(lrg_buf_cb, maplen,\r\nqdev->lrg_buffer_len -\r\nQL_HEADER_SPACE);\r\n--qdev->lrg_buf_skb_check;\r\nif (!qdev->lrg_buf_skb_check)\r\nreturn 1;\r\n}\r\n}\r\nlrg_buf_cb = lrg_buf_cb->next;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ql_update_small_bufq_prod_index(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nif (qdev->small_buf_release_cnt >= 16) {\r\nwhile (qdev->small_buf_release_cnt >= 16) {\r\nqdev->small_buf_q_producer_index++;\r\nif (qdev->small_buf_q_producer_index ==\r\nNUM_SBUFQ_ENTRIES)\r\nqdev->small_buf_q_producer_index = 0;\r\nqdev->small_buf_release_cnt -= 8;\r\n}\r\nwmb();\r\nwritel(qdev->small_buf_q_producer_index,\r\n&port_regs->CommonRegs.rxSmallQProducerIndex);\r\n}\r\n}\r\nstatic void ql_update_lrg_bufq_prod_index(struct ql3_adapter *qdev)\r\n{\r\nstruct bufq_addr_element *lrg_buf_q_ele;\r\nint i;\r\nstruct ql_rcv_buf_cb *lrg_buf_cb;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nif ((qdev->lrg_buf_free_count >= 8) &&\r\n(qdev->lrg_buf_release_cnt >= 16)) {\r\nif (qdev->lrg_buf_skb_check)\r\nif (!ql_populate_free_queue(qdev))\r\nreturn;\r\nlrg_buf_q_ele = qdev->lrg_buf_next_free;\r\nwhile ((qdev->lrg_buf_release_cnt >= 16) &&\r\n(qdev->lrg_buf_free_count >= 8)) {\r\nfor (i = 0; i < 8; i++) {\r\nlrg_buf_cb =\r\nql_get_from_lrg_buf_free_list(qdev);\r\nlrg_buf_q_ele->addr_high =\r\nlrg_buf_cb->buf_phy_addr_high;\r\nlrg_buf_q_ele->addr_low =\r\nlrg_buf_cb->buf_phy_addr_low;\r\nlrg_buf_q_ele++;\r\nqdev->lrg_buf_release_cnt--;\r\n}\r\nqdev->lrg_buf_q_producer_index++;\r\nif (qdev->lrg_buf_q_producer_index ==\r\nqdev->num_lbufq_entries)\r\nqdev->lrg_buf_q_producer_index = 0;\r\nif (qdev->lrg_buf_q_producer_index ==\r\n(qdev->num_lbufq_entries - 1)) {\r\nlrg_buf_q_ele = qdev->lrg_buf_q_virt_addr;\r\n}\r\n}\r\nwmb();\r\nqdev->lrg_buf_next_free = lrg_buf_q_ele;\r\nwritel(qdev->lrg_buf_q_producer_index,\r\n&port_regs->CommonRegs.rxLargeQProducerIndex);\r\n}\r\n}\r\nstatic void ql_process_mac_tx_intr(struct ql3_adapter *qdev,\r\nstruct ob_mac_iocb_rsp *mac_rsp)\r\n{\r\nstruct ql_tx_buf_cb *tx_cb;\r\nint i;\r\nif (mac_rsp->flags & OB_MAC_IOCB_RSP_S) {\r\nnetdev_warn(qdev->ndev,\r\n"Frame too short but it was padded and sent\n");\r\n}\r\ntx_cb = &qdev->tx_buf[mac_rsp->transaction_id];\r\nif (mac_rsp->flags & OB_MAC_IOCB_RSP_S) {\r\nnetdev_err(qdev->ndev,\r\n"Frame too short to be legal, frame not sent\n");\r\nqdev->ndev->stats.tx_errors++;\r\ngoto frame_not_sent;\r\n}\r\nif (tx_cb->seg_count == 0) {\r\nnetdev_err(qdev->ndev, "tx_cb->seg_count == 0: %d\n",\r\nmac_rsp->transaction_id);\r\nqdev->ndev->stats.tx_errors++;\r\ngoto invalid_seg_count;\r\n}\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(&tx_cb->map[0], mapaddr),\r\ndma_unmap_len(&tx_cb->map[0], maplen),\r\nPCI_DMA_TODEVICE);\r\ntx_cb->seg_count--;\r\nif (tx_cb->seg_count) {\r\nfor (i = 1; i < tx_cb->seg_count; i++) {\r\npci_unmap_page(qdev->pdev,\r\ndma_unmap_addr(&tx_cb->map[i],\r\nmapaddr),\r\ndma_unmap_len(&tx_cb->map[i], maplen),\r\nPCI_DMA_TODEVICE);\r\n}\r\n}\r\nqdev->ndev->stats.tx_packets++;\r\nqdev->ndev->stats.tx_bytes += tx_cb->skb->len;\r\nframe_not_sent:\r\ndev_kfree_skb_irq(tx_cb->skb);\r\ntx_cb->skb = NULL;\r\ninvalid_seg_count:\r\natomic_inc(&qdev->tx_count);\r\n}\r\nstatic void ql_get_sbuf(struct ql3_adapter *qdev)\r\n{\r\nif (++qdev->small_buf_index == NUM_SMALL_BUFFERS)\r\nqdev->small_buf_index = 0;\r\nqdev->small_buf_release_cnt++;\r\n}\r\nstatic struct ql_rcv_buf_cb *ql_get_lbuf(struct ql3_adapter *qdev)\r\n{\r\nstruct ql_rcv_buf_cb *lrg_buf_cb = NULL;\r\nlrg_buf_cb = &qdev->lrg_buf[qdev->lrg_buf_index];\r\nqdev->lrg_buf_release_cnt++;\r\nif (++qdev->lrg_buf_index == qdev->num_large_buffers)\r\nqdev->lrg_buf_index = 0;\r\nreturn lrg_buf_cb;\r\n}\r\nstatic void ql_process_mac_rx_intr(struct ql3_adapter *qdev,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp_ptr)\r\n{\r\nstruct ql_rcv_buf_cb *lrg_buf_cb1 = NULL;\r\nstruct ql_rcv_buf_cb *lrg_buf_cb2 = NULL;\r\nstruct sk_buff *skb;\r\nu16 length = le16_to_cpu(ib_mac_rsp_ptr->length);\r\nql_get_sbuf(qdev);\r\nif (qdev->device_id == QL3022_DEVICE_ID)\r\nlrg_buf_cb1 = ql_get_lbuf(qdev);\r\nlrg_buf_cb2 = ql_get_lbuf(qdev);\r\nskb = lrg_buf_cb2->skb;\r\nqdev->ndev->stats.rx_packets++;\r\nqdev->ndev->stats.rx_bytes += length;\r\nskb_put(skb, length);\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(lrg_buf_cb2, mapaddr),\r\ndma_unmap_len(lrg_buf_cb2, maplen),\r\nPCI_DMA_FROMDEVICE);\r\nprefetch(skb->data);\r\nskb_checksum_none_assert(skb);\r\nskb->protocol = eth_type_trans(skb, qdev->ndev);\r\nnetif_receive_skb(skb);\r\nlrg_buf_cb2->skb = NULL;\r\nif (qdev->device_id == QL3022_DEVICE_ID)\r\nql_release_to_lrg_buf_free_list(qdev, lrg_buf_cb1);\r\nql_release_to_lrg_buf_free_list(qdev, lrg_buf_cb2);\r\n}\r\nstatic void ql_process_macip_rx_intr(struct ql3_adapter *qdev,\r\nstruct ib_ip_iocb_rsp *ib_ip_rsp_ptr)\r\n{\r\nstruct ql_rcv_buf_cb *lrg_buf_cb1 = NULL;\r\nstruct ql_rcv_buf_cb *lrg_buf_cb2 = NULL;\r\nstruct sk_buff *skb1 = NULL, *skb2;\r\nstruct net_device *ndev = qdev->ndev;\r\nu16 length = le16_to_cpu(ib_ip_rsp_ptr->length);\r\nu16 size = 0;\r\nql_get_sbuf(qdev);\r\nif (qdev->device_id == QL3022_DEVICE_ID) {\r\nlrg_buf_cb1 = ql_get_lbuf(qdev);\r\nskb1 = lrg_buf_cb1->skb;\r\nsize = ETH_HLEN;\r\nif (*((u16 *) skb1->data) != 0xFFFF)\r\nsize += VLAN_ETH_HLEN - ETH_HLEN;\r\n}\r\nlrg_buf_cb2 = ql_get_lbuf(qdev);\r\nskb2 = lrg_buf_cb2->skb;\r\nskb_put(skb2, length);\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(lrg_buf_cb2, mapaddr),\r\ndma_unmap_len(lrg_buf_cb2, maplen),\r\nPCI_DMA_FROMDEVICE);\r\nprefetch(skb2->data);\r\nskb_checksum_none_assert(skb2);\r\nif (qdev->device_id == QL3022_DEVICE_ID) {\r\nskb_copy_from_linear_data_offset(skb1, VLAN_ID_LEN,\r\nskb_push(skb2, size), size);\r\n} else {\r\nu16 checksum = le16_to_cpu(ib_ip_rsp_ptr->checksum);\r\nif (checksum &\r\n(IB_IP_IOCB_RSP_3032_ICE |\r\nIB_IP_IOCB_RSP_3032_CE)) {\r\nnetdev_err(ndev,\r\n"%s: Bad checksum for this %s packet, checksum = %x\n",\r\n__func__,\r\n((checksum & IB_IP_IOCB_RSP_3032_TCP) ?\r\n"TCP" : "UDP"), checksum);\r\n} else if ((checksum & IB_IP_IOCB_RSP_3032_TCP) ||\r\n(checksum & IB_IP_IOCB_RSP_3032_UDP &&\r\n!(checksum & IB_IP_IOCB_RSP_3032_NUC))) {\r\nskb2->ip_summed = CHECKSUM_UNNECESSARY;\r\n}\r\n}\r\nskb2->protocol = eth_type_trans(skb2, qdev->ndev);\r\nnetif_receive_skb(skb2);\r\nndev->stats.rx_packets++;\r\nndev->stats.rx_bytes += length;\r\nlrg_buf_cb2->skb = NULL;\r\nif (qdev->device_id == QL3022_DEVICE_ID)\r\nql_release_to_lrg_buf_free_list(qdev, lrg_buf_cb1);\r\nql_release_to_lrg_buf_free_list(qdev, lrg_buf_cb2);\r\n}\r\nstatic int ql_tx_rx_clean(struct ql3_adapter *qdev,\r\nint *tx_cleaned, int *rx_cleaned, int work_to_do)\r\n{\r\nstruct net_rsp_iocb *net_rsp;\r\nstruct net_device *ndev = qdev->ndev;\r\nint work_done = 0;\r\nwhile ((le32_to_cpu(*(qdev->prsp_producer_index)) !=\r\nqdev->rsp_consumer_index) && (work_done < work_to_do)) {\r\nnet_rsp = qdev->rsp_current;\r\nrmb();\r\nif (qdev->device_id == QL3032_DEVICE_ID)\r\nnet_rsp->opcode &= 0x7f;\r\nswitch (net_rsp->opcode) {\r\ncase OPCODE_OB_MAC_IOCB_FN0:\r\ncase OPCODE_OB_MAC_IOCB_FN2:\r\nql_process_mac_tx_intr(qdev, (struct ob_mac_iocb_rsp *)\r\nnet_rsp);\r\n(*tx_cleaned)++;\r\nbreak;\r\ncase OPCODE_IB_MAC_IOCB:\r\ncase OPCODE_IB_3032_MAC_IOCB:\r\nql_process_mac_rx_intr(qdev, (struct ib_mac_iocb_rsp *)\r\nnet_rsp);\r\n(*rx_cleaned)++;\r\nbreak;\r\ncase OPCODE_IB_IP_IOCB:\r\ncase OPCODE_IB_3032_IP_IOCB:\r\nql_process_macip_rx_intr(qdev, (struct ib_ip_iocb_rsp *)\r\nnet_rsp);\r\n(*rx_cleaned)++;\r\nbreak;\r\ndefault: {\r\nu32 *tmp = (u32 *)net_rsp;\r\nnetdev_err(ndev,\r\n"Hit default case, not handled!\n"\r\n" dropping the packet, opcode = %x\n"\r\n"0x%08lx 0x%08lx 0x%08lx 0x%08lx\n",\r\nnet_rsp->opcode,\r\n(unsigned long int)tmp[0],\r\n(unsigned long int)tmp[1],\r\n(unsigned long int)tmp[2],\r\n(unsigned long int)tmp[3]);\r\n}\r\n}\r\nqdev->rsp_consumer_index++;\r\nif (qdev->rsp_consumer_index == NUM_RSP_Q_ENTRIES) {\r\nqdev->rsp_consumer_index = 0;\r\nqdev->rsp_current = qdev->rsp_q_virt_addr;\r\n} else {\r\nqdev->rsp_current++;\r\n}\r\nwork_done = *tx_cleaned + *rx_cleaned;\r\n}\r\nreturn work_done;\r\n}\r\nstatic int ql_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct ql3_adapter *qdev = container_of(napi, struct ql3_adapter, napi);\r\nint rx_cleaned = 0, tx_cleaned = 0;\r\nunsigned long hw_flags;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nql_tx_rx_clean(qdev, &tx_cleaned, &rx_cleaned, budget);\r\nif (tx_cleaned + rx_cleaned != budget) {\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\n__napi_complete(napi);\r\nql_update_small_bufq_prod_index(qdev);\r\nql_update_lrg_bufq_prod_index(qdev);\r\nwritel(qdev->rsp_consumer_index,\r\n&port_regs->CommonRegs.rspQConsumerIndex);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nql_enable_interrupts(qdev);\r\n}\r\nreturn tx_cleaned + rx_cleaned;\r\n}\r\nstatic irqreturn_t ql3xxx_isr(int irq, void *dev_id)\r\n{\r\nstruct net_device *ndev = dev_id;\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nint handled = 1;\r\nu32 var;\r\nvalue = ql_read_common_reg_l(qdev,\r\n&port_regs->CommonRegs.ispControlStatus);\r\nif (value & (ISP_CONTROL_FE | ISP_CONTROL_RI)) {\r\nspin_lock(&qdev->adapter_lock);\r\nnetif_stop_queue(qdev->ndev);\r\nnetif_carrier_off(qdev->ndev);\r\nql_disable_interrupts(qdev);\r\nqdev->port_link_state = LS_DOWN;\r\nset_bit(QL_RESET_ACTIVE, &qdev->flags) ;\r\nif (value & ISP_CONTROL_FE) {\r\nvar =\r\nql_read_page0_reg_l(qdev,\r\n&port_regs->PortFatalErrStatus);\r\nnetdev_warn(ndev,\r\n"Resetting chip. PortFatalErrStatus register = 0x%x\n",\r\nvar);\r\nset_bit(QL_RESET_START, &qdev->flags) ;\r\n} else {\r\nset_bit(QL_RESET_PER_SCSI, &qdev->flags) ;\r\nnetdev_err(ndev,\r\n"Another function issued a reset to the chip. ISR value = %x\n",\r\nvalue);\r\n}\r\nqueue_delayed_work(qdev->workqueue, &qdev->reset_work, 0);\r\nspin_unlock(&qdev->adapter_lock);\r\n} else if (value & ISP_IMR_DISABLE_CMPL_INT) {\r\nql_disable_interrupts(qdev);\r\nif (likely(napi_schedule_prep(&qdev->napi)))\r\n__napi_schedule(&qdev->napi);\r\n} else\r\nreturn IRQ_NONE;\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic int ql_get_seg_count(struct ql3_adapter *qdev, unsigned short frags)\r\n{\r\nif (qdev->device_id == QL3022_DEVICE_ID)\r\nreturn 1;\r\nif (frags <= 2)\r\nreturn frags + 1;\r\nelse if (frags <= 6)\r\nreturn frags + 2;\r\nelse if (frags <= 10)\r\nreturn frags + 3;\r\nelse if (frags <= 14)\r\nreturn frags + 4;\r\nelse if (frags <= 18)\r\nreturn frags + 5;\r\nreturn -1;\r\n}\r\nstatic void ql_hw_csum_setup(const struct sk_buff *skb,\r\nstruct ob_mac_iocb_req *mac_iocb_ptr)\r\n{\r\nconst struct iphdr *ip = ip_hdr(skb);\r\nmac_iocb_ptr->ip_hdr_off = skb_network_offset(skb);\r\nmac_iocb_ptr->ip_hdr_len = ip->ihl;\r\nif (ip->protocol == IPPROTO_TCP) {\r\nmac_iocb_ptr->flags1 |= OB_3032MAC_IOCB_REQ_TC |\r\nOB_3032MAC_IOCB_REQ_IC;\r\n} else {\r\nmac_iocb_ptr->flags1 |= OB_3032MAC_IOCB_REQ_UC |\r\nOB_3032MAC_IOCB_REQ_IC;\r\n}\r\n}\r\nstatic int ql_send_map(struct ql3_adapter *qdev,\r\nstruct ob_mac_iocb_req *mac_iocb_ptr,\r\nstruct ql_tx_buf_cb *tx_cb,\r\nstruct sk_buff *skb)\r\n{\r\nstruct oal *oal;\r\nstruct oal_entry *oal_entry;\r\nint len = skb_headlen(skb);\r\ndma_addr_t map;\r\nint err;\r\nint completed_segs, i;\r\nint seg_cnt, seg = 0;\r\nint frag_cnt = (int)skb_shinfo(skb)->nr_frags;\r\nseg_cnt = tx_cb->seg_count;\r\nmap = pci_map_single(qdev->pdev, skb->data, len, PCI_DMA_TODEVICE);\r\nerr = pci_dma_mapping_error(qdev->pdev, map);\r\nif (err) {\r\nnetdev_err(qdev->ndev, "PCI mapping failed with error: %d\n",\r\nerr);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\noal_entry = (struct oal_entry *)&mac_iocb_ptr->buf_addr0_low;\r\noal_entry->dma_lo = cpu_to_le32(LS_64BITS(map));\r\noal_entry->dma_hi = cpu_to_le32(MS_64BITS(map));\r\noal_entry->len = cpu_to_le32(len);\r\ndma_unmap_addr_set(&tx_cb->map[seg], mapaddr, map);\r\ndma_unmap_len_set(&tx_cb->map[seg], maplen, len);\r\nseg++;\r\nif (seg_cnt == 1) {\r\noal_entry->len |= cpu_to_le32(OAL_LAST_ENTRY);\r\nreturn NETDEV_TX_OK;\r\n}\r\noal = tx_cb->oal;\r\nfor (completed_segs = 0;\r\ncompleted_segs < frag_cnt;\r\ncompleted_segs++, seg++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[completed_segs];\r\noal_entry++;\r\nif ((seg == 2 && seg_cnt > 3) ||\r\n(seg == 7 && seg_cnt > 8) ||\r\n(seg == 12 && seg_cnt > 13) ||\r\n(seg == 17 && seg_cnt > 18)) {\r\nmap = pci_map_single(qdev->pdev, oal,\r\nsizeof(struct oal),\r\nPCI_DMA_TODEVICE);\r\nerr = pci_dma_mapping_error(qdev->pdev, map);\r\nif (err) {\r\nnetdev_err(qdev->ndev,\r\n"PCI mapping outbound address list with error: %d\n",\r\nerr);\r\ngoto map_error;\r\n}\r\noal_entry->dma_lo = cpu_to_le32(LS_64BITS(map));\r\noal_entry->dma_hi = cpu_to_le32(MS_64BITS(map));\r\noal_entry->len = cpu_to_le32(sizeof(struct oal) |\r\nOAL_CONT_ENTRY);\r\ndma_unmap_addr_set(&tx_cb->map[seg], mapaddr, map);\r\ndma_unmap_len_set(&tx_cb->map[seg], maplen,\r\nsizeof(struct oal));\r\noal_entry = (struct oal_entry *)oal;\r\noal++;\r\nseg++;\r\n}\r\nmap = skb_frag_dma_map(&qdev->pdev->dev, frag, 0, skb_frag_size(frag),\r\nDMA_TO_DEVICE);\r\nerr = dma_mapping_error(&qdev->pdev->dev, map);\r\nif (err) {\r\nnetdev_err(qdev->ndev,\r\n"PCI mapping frags failed with error: %d\n",\r\nerr);\r\ngoto map_error;\r\n}\r\noal_entry->dma_lo = cpu_to_le32(LS_64BITS(map));\r\noal_entry->dma_hi = cpu_to_le32(MS_64BITS(map));\r\noal_entry->len = cpu_to_le32(skb_frag_size(frag));\r\ndma_unmap_addr_set(&tx_cb->map[seg], mapaddr, map);\r\ndma_unmap_len_set(&tx_cb->map[seg], maplen, skb_frag_size(frag));\r\n}\r\noal_entry->len |= cpu_to_le32(OAL_LAST_ENTRY);\r\nreturn NETDEV_TX_OK;\r\nmap_error:\r\nseg = 1;\r\noal_entry = (struct oal_entry *)&mac_iocb_ptr->buf_addr0_low;\r\noal = tx_cb->oal;\r\nfor (i = 0; i < completed_segs; i++, seg++) {\r\noal_entry++;\r\nif ((seg == 2 && seg_cnt > 3) ||\r\n(seg == 7 && seg_cnt > 8) ||\r\n(seg == 12 && seg_cnt > 13) ||\r\n(seg == 17 && seg_cnt > 18)) {\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(&tx_cb->map[seg], mapaddr),\r\ndma_unmap_len(&tx_cb->map[seg], maplen),\r\nPCI_DMA_TODEVICE);\r\noal++;\r\nseg++;\r\n}\r\npci_unmap_page(qdev->pdev,\r\ndma_unmap_addr(&tx_cb->map[seg], mapaddr),\r\ndma_unmap_len(&tx_cb->map[seg], maplen),\r\nPCI_DMA_TODEVICE);\r\n}\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(&tx_cb->map[0], mapaddr),\r\ndma_unmap_addr(&tx_cb->map[0], maplen),\r\nPCI_DMA_TODEVICE);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nstatic netdev_tx_t ql3xxx_send(struct sk_buff *skb,\r\nstruct net_device *ndev)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nstruct ql_tx_buf_cb *tx_cb;\r\nu32 tot_len = skb->len;\r\nstruct ob_mac_iocb_req *mac_iocb_ptr;\r\nif (unlikely(atomic_read(&qdev->tx_count) < 2))\r\nreturn NETDEV_TX_BUSY;\r\ntx_cb = &qdev->tx_buf[qdev->req_producer_index];\r\ntx_cb->seg_count = ql_get_seg_count(qdev,\r\nskb_shinfo(skb)->nr_frags);\r\nif (tx_cb->seg_count == -1) {\r\nnetdev_err(ndev, "%s: invalid segment count!\n", __func__);\r\nreturn NETDEV_TX_OK;\r\n}\r\nmac_iocb_ptr = tx_cb->queue_entry;\r\nmemset((void *)mac_iocb_ptr, 0, sizeof(struct ob_mac_iocb_req));\r\nmac_iocb_ptr->opcode = qdev->mac_ob_opcode;\r\nmac_iocb_ptr->flags = OB_MAC_IOCB_REQ_X;\r\nmac_iocb_ptr->flags |= qdev->mb_bit_mask;\r\nmac_iocb_ptr->transaction_id = qdev->req_producer_index;\r\nmac_iocb_ptr->data_len = cpu_to_le16((u16) tot_len);\r\ntx_cb->skb = skb;\r\nif (qdev->device_id == QL3032_DEVICE_ID &&\r\nskb->ip_summed == CHECKSUM_PARTIAL)\r\nql_hw_csum_setup(skb, mac_iocb_ptr);\r\nif (ql_send_map(qdev, mac_iocb_ptr, tx_cb, skb) != NETDEV_TX_OK) {\r\nnetdev_err(ndev, "%s: Could not map the segments!\n", __func__);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nwmb();\r\nqdev->req_producer_index++;\r\nif (qdev->req_producer_index == NUM_REQ_Q_ENTRIES)\r\nqdev->req_producer_index = 0;\r\nwmb();\r\nql_write_common_reg_l(qdev,\r\n&port_regs->CommonRegs.reqQProducerIndex,\r\nqdev->req_producer_index);\r\nnetif_printk(qdev, tx_queued, KERN_DEBUG, ndev,\r\n"tx queued, slot %d, len %d\n",\r\nqdev->req_producer_index, skb->len);\r\natomic_dec(&qdev->tx_count);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int ql_alloc_net_req_rsp_queues(struct ql3_adapter *qdev)\r\n{\r\nqdev->req_q_size =\r\n(u32) (NUM_REQ_Q_ENTRIES * sizeof(struct ob_mac_iocb_req));\r\nqdev->rsp_q_size = NUM_RSP_Q_ENTRIES * sizeof(struct net_rsp_iocb);\r\nwmb();\r\nqdev->req_q_virt_addr =\r\npci_alloc_consistent(qdev->pdev,\r\n(size_t) qdev->req_q_size,\r\n&qdev->req_q_phy_addr);\r\nif ((qdev->req_q_virt_addr == NULL) ||\r\nLS_64BITS(qdev->req_q_phy_addr) & (qdev->req_q_size - 1)) {\r\nnetdev_err(qdev->ndev, "reqQ failed\n");\r\nreturn -ENOMEM;\r\n}\r\nqdev->rsp_q_virt_addr =\r\npci_alloc_consistent(qdev->pdev,\r\n(size_t) qdev->rsp_q_size,\r\n&qdev->rsp_q_phy_addr);\r\nif ((qdev->rsp_q_virt_addr == NULL) ||\r\nLS_64BITS(qdev->rsp_q_phy_addr) & (qdev->rsp_q_size - 1)) {\r\nnetdev_err(qdev->ndev, "rspQ allocation failed\n");\r\npci_free_consistent(qdev->pdev, (size_t) qdev->req_q_size,\r\nqdev->req_q_virt_addr,\r\nqdev->req_q_phy_addr);\r\nreturn -ENOMEM;\r\n}\r\nset_bit(QL_ALLOC_REQ_RSP_Q_DONE, &qdev->flags);\r\nreturn 0;\r\n}\r\nstatic void ql_free_net_req_rsp_queues(struct ql3_adapter *qdev)\r\n{\r\nif (!test_bit(QL_ALLOC_REQ_RSP_Q_DONE, &qdev->flags)) {\r\nnetdev_info(qdev->ndev, "Already done\n");\r\nreturn;\r\n}\r\npci_free_consistent(qdev->pdev,\r\nqdev->req_q_size,\r\nqdev->req_q_virt_addr, qdev->req_q_phy_addr);\r\nqdev->req_q_virt_addr = NULL;\r\npci_free_consistent(qdev->pdev,\r\nqdev->rsp_q_size,\r\nqdev->rsp_q_virt_addr, qdev->rsp_q_phy_addr);\r\nqdev->rsp_q_virt_addr = NULL;\r\nclear_bit(QL_ALLOC_REQ_RSP_Q_DONE, &qdev->flags);\r\n}\r\nstatic int ql_alloc_buffer_queues(struct ql3_adapter *qdev)\r\n{\r\nqdev->lrg_buf_q_size =\r\nqdev->num_lbufq_entries * sizeof(struct lrg_buf_q_entry);\r\nif (qdev->lrg_buf_q_size < PAGE_SIZE)\r\nqdev->lrg_buf_q_alloc_size = PAGE_SIZE;\r\nelse\r\nqdev->lrg_buf_q_alloc_size = qdev->lrg_buf_q_size * 2;\r\nqdev->lrg_buf = kmalloc_array(qdev->num_large_buffers,\r\nsizeof(struct ql_rcv_buf_cb),\r\nGFP_KERNEL);\r\nif (qdev->lrg_buf == NULL)\r\nreturn -ENOMEM;\r\nqdev->lrg_buf_q_alloc_virt_addr =\r\npci_alloc_consistent(qdev->pdev,\r\nqdev->lrg_buf_q_alloc_size,\r\n&qdev->lrg_buf_q_alloc_phy_addr);\r\nif (qdev->lrg_buf_q_alloc_virt_addr == NULL) {\r\nnetdev_err(qdev->ndev, "lBufQ failed\n");\r\nreturn -ENOMEM;\r\n}\r\nqdev->lrg_buf_q_virt_addr = qdev->lrg_buf_q_alloc_virt_addr;\r\nqdev->lrg_buf_q_phy_addr = qdev->lrg_buf_q_alloc_phy_addr;\r\nqdev->small_buf_q_size =\r\nNUM_SBUFQ_ENTRIES * sizeof(struct lrg_buf_q_entry);\r\nif (qdev->small_buf_q_size < PAGE_SIZE)\r\nqdev->small_buf_q_alloc_size = PAGE_SIZE;\r\nelse\r\nqdev->small_buf_q_alloc_size = qdev->small_buf_q_size * 2;\r\nqdev->small_buf_q_alloc_virt_addr =\r\npci_alloc_consistent(qdev->pdev,\r\nqdev->small_buf_q_alloc_size,\r\n&qdev->small_buf_q_alloc_phy_addr);\r\nif (qdev->small_buf_q_alloc_virt_addr == NULL) {\r\nnetdev_err(qdev->ndev, "Small Buffer Queue allocation failed\n");\r\npci_free_consistent(qdev->pdev, qdev->lrg_buf_q_alloc_size,\r\nqdev->lrg_buf_q_alloc_virt_addr,\r\nqdev->lrg_buf_q_alloc_phy_addr);\r\nreturn -ENOMEM;\r\n}\r\nqdev->small_buf_q_virt_addr = qdev->small_buf_q_alloc_virt_addr;\r\nqdev->small_buf_q_phy_addr = qdev->small_buf_q_alloc_phy_addr;\r\nset_bit(QL_ALLOC_BUFQS_DONE, &qdev->flags);\r\nreturn 0;\r\n}\r\nstatic void ql_free_buffer_queues(struct ql3_adapter *qdev)\r\n{\r\nif (!test_bit(QL_ALLOC_BUFQS_DONE, &qdev->flags)) {\r\nnetdev_info(qdev->ndev, "Already done\n");\r\nreturn;\r\n}\r\nkfree(qdev->lrg_buf);\r\npci_free_consistent(qdev->pdev,\r\nqdev->lrg_buf_q_alloc_size,\r\nqdev->lrg_buf_q_alloc_virt_addr,\r\nqdev->lrg_buf_q_alloc_phy_addr);\r\nqdev->lrg_buf_q_virt_addr = NULL;\r\npci_free_consistent(qdev->pdev,\r\nqdev->small_buf_q_alloc_size,\r\nqdev->small_buf_q_alloc_virt_addr,\r\nqdev->small_buf_q_alloc_phy_addr);\r\nqdev->small_buf_q_virt_addr = NULL;\r\nclear_bit(QL_ALLOC_BUFQS_DONE, &qdev->flags);\r\n}\r\nstatic int ql_alloc_small_buffers(struct ql3_adapter *qdev)\r\n{\r\nint i;\r\nstruct bufq_addr_element *small_buf_q_entry;\r\nqdev->small_buf_total_size =\r\n(QL_ADDR_ELE_PER_BUFQ_ENTRY * NUM_SBUFQ_ENTRIES *\r\nQL_SMALL_BUFFER_SIZE);\r\nqdev->small_buf_virt_addr =\r\npci_alloc_consistent(qdev->pdev,\r\nqdev->small_buf_total_size,\r\n&qdev->small_buf_phy_addr);\r\nif (qdev->small_buf_virt_addr == NULL) {\r\nnetdev_err(qdev->ndev, "Failed to get small buffer memory\n");\r\nreturn -ENOMEM;\r\n}\r\nqdev->small_buf_phy_addr_low = LS_64BITS(qdev->small_buf_phy_addr);\r\nqdev->small_buf_phy_addr_high = MS_64BITS(qdev->small_buf_phy_addr);\r\nsmall_buf_q_entry = qdev->small_buf_q_virt_addr;\r\nfor (i = 0; i < (QL_ADDR_ELE_PER_BUFQ_ENTRY * NUM_SBUFQ_ENTRIES); i++) {\r\nsmall_buf_q_entry->addr_high =\r\ncpu_to_le32(qdev->small_buf_phy_addr_high);\r\nsmall_buf_q_entry->addr_low =\r\ncpu_to_le32(qdev->small_buf_phy_addr_low +\r\n(i * QL_SMALL_BUFFER_SIZE));\r\nsmall_buf_q_entry++;\r\n}\r\nqdev->small_buf_index = 0;\r\nset_bit(QL_ALLOC_SMALL_BUF_DONE, &qdev->flags);\r\nreturn 0;\r\n}\r\nstatic void ql_free_small_buffers(struct ql3_adapter *qdev)\r\n{\r\nif (!test_bit(QL_ALLOC_SMALL_BUF_DONE, &qdev->flags)) {\r\nnetdev_info(qdev->ndev, "Already done\n");\r\nreturn;\r\n}\r\nif (qdev->small_buf_virt_addr != NULL) {\r\npci_free_consistent(qdev->pdev,\r\nqdev->small_buf_total_size,\r\nqdev->small_buf_virt_addr,\r\nqdev->small_buf_phy_addr);\r\nqdev->small_buf_virt_addr = NULL;\r\n}\r\n}\r\nstatic void ql_free_large_buffers(struct ql3_adapter *qdev)\r\n{\r\nint i = 0;\r\nstruct ql_rcv_buf_cb *lrg_buf_cb;\r\nfor (i = 0; i < qdev->num_large_buffers; i++) {\r\nlrg_buf_cb = &qdev->lrg_buf[i];\r\nif (lrg_buf_cb->skb) {\r\ndev_kfree_skb(lrg_buf_cb->skb);\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(lrg_buf_cb, mapaddr),\r\ndma_unmap_len(lrg_buf_cb, maplen),\r\nPCI_DMA_FROMDEVICE);\r\nmemset(lrg_buf_cb, 0, sizeof(struct ql_rcv_buf_cb));\r\n} else {\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void ql_init_large_buffers(struct ql3_adapter *qdev)\r\n{\r\nint i;\r\nstruct ql_rcv_buf_cb *lrg_buf_cb;\r\nstruct bufq_addr_element *buf_addr_ele = qdev->lrg_buf_q_virt_addr;\r\nfor (i = 0; i < qdev->num_large_buffers; i++) {\r\nlrg_buf_cb = &qdev->lrg_buf[i];\r\nbuf_addr_ele->addr_high = lrg_buf_cb->buf_phy_addr_high;\r\nbuf_addr_ele->addr_low = lrg_buf_cb->buf_phy_addr_low;\r\nbuf_addr_ele++;\r\n}\r\nqdev->lrg_buf_index = 0;\r\nqdev->lrg_buf_skb_check = 0;\r\n}\r\nstatic int ql_alloc_large_buffers(struct ql3_adapter *qdev)\r\n{\r\nint i;\r\nstruct ql_rcv_buf_cb *lrg_buf_cb;\r\nstruct sk_buff *skb;\r\ndma_addr_t map;\r\nint err;\r\nfor (i = 0; i < qdev->num_large_buffers; i++) {\r\nskb = netdev_alloc_skb(qdev->ndev,\r\nqdev->lrg_buffer_len);\r\nif (unlikely(!skb)) {\r\nnetdev_err(qdev->ndev,\r\n"large buff alloc failed for %d bytes at index %d\n",\r\nqdev->lrg_buffer_len * 2, i);\r\nql_free_large_buffers(qdev);\r\nreturn -ENOMEM;\r\n} else {\r\nlrg_buf_cb = &qdev->lrg_buf[i];\r\nmemset(lrg_buf_cb, 0, sizeof(struct ql_rcv_buf_cb));\r\nlrg_buf_cb->index = i;\r\nlrg_buf_cb->skb = skb;\r\nskb_reserve(skb, QL_HEADER_SPACE);\r\nmap = pci_map_single(qdev->pdev,\r\nskb->data,\r\nqdev->lrg_buffer_len -\r\nQL_HEADER_SPACE,\r\nPCI_DMA_FROMDEVICE);\r\nerr = pci_dma_mapping_error(qdev->pdev, map);\r\nif (err) {\r\nnetdev_err(qdev->ndev,\r\n"PCI mapping failed with error: %d\n",\r\nerr);\r\nql_free_large_buffers(qdev);\r\nreturn -ENOMEM;\r\n}\r\ndma_unmap_addr_set(lrg_buf_cb, mapaddr, map);\r\ndma_unmap_len_set(lrg_buf_cb, maplen,\r\nqdev->lrg_buffer_len -\r\nQL_HEADER_SPACE);\r\nlrg_buf_cb->buf_phy_addr_low =\r\ncpu_to_le32(LS_64BITS(map));\r\nlrg_buf_cb->buf_phy_addr_high =\r\ncpu_to_le32(MS_64BITS(map));\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void ql_free_send_free_list(struct ql3_adapter *qdev)\r\n{\r\nstruct ql_tx_buf_cb *tx_cb;\r\nint i;\r\ntx_cb = &qdev->tx_buf[0];\r\nfor (i = 0; i < NUM_REQ_Q_ENTRIES; i++) {\r\nkfree(tx_cb->oal);\r\ntx_cb->oal = NULL;\r\ntx_cb++;\r\n}\r\n}\r\nstatic int ql_create_send_free_list(struct ql3_adapter *qdev)\r\n{\r\nstruct ql_tx_buf_cb *tx_cb;\r\nint i;\r\nstruct ob_mac_iocb_req *req_q_curr = qdev->req_q_virt_addr;\r\nfor (i = 0; i < NUM_REQ_Q_ENTRIES; i++) {\r\ntx_cb = &qdev->tx_buf[i];\r\ntx_cb->skb = NULL;\r\ntx_cb->queue_entry = req_q_curr;\r\nreq_q_curr++;\r\ntx_cb->oal = kmalloc(512, GFP_KERNEL);\r\nif (tx_cb->oal == NULL)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ql_alloc_mem_resources(struct ql3_adapter *qdev)\r\n{\r\nif (qdev->ndev->mtu == NORMAL_MTU_SIZE) {\r\nqdev->num_lbufq_entries = NUM_LBUFQ_ENTRIES;\r\nqdev->lrg_buffer_len = NORMAL_MTU_SIZE;\r\n} else if (qdev->ndev->mtu == JUMBO_MTU_SIZE) {\r\nqdev->num_lbufq_entries = JUMBO_NUM_LBUFQ_ENTRIES;\r\nqdev->lrg_buffer_len = JUMBO_MTU_SIZE;\r\n} else {\r\nnetdev_err(qdev->ndev, "Invalid mtu size: %d. Only %d and %d are accepted.\n",\r\nqdev->ndev->mtu, NORMAL_MTU_SIZE, JUMBO_MTU_SIZE);\r\nreturn -ENOMEM;\r\n}\r\nqdev->num_large_buffers =\r\nqdev->num_lbufq_entries * QL_ADDR_ELE_PER_BUFQ_ENTRY;\r\nqdev->lrg_buffer_len += VLAN_ETH_HLEN + VLAN_ID_LEN + QL_HEADER_SPACE;\r\nqdev->max_frame_size =\r\n(qdev->lrg_buffer_len - QL_HEADER_SPACE) + ETHERNET_CRC_SIZE;\r\nqdev->shadow_reg_virt_addr =\r\npci_alloc_consistent(qdev->pdev,\r\nPAGE_SIZE, &qdev->shadow_reg_phy_addr);\r\nif (qdev->shadow_reg_virt_addr != NULL) {\r\nqdev->preq_consumer_index = qdev->shadow_reg_virt_addr;\r\nqdev->req_consumer_index_phy_addr_high =\r\nMS_64BITS(qdev->shadow_reg_phy_addr);\r\nqdev->req_consumer_index_phy_addr_low =\r\nLS_64BITS(qdev->shadow_reg_phy_addr);\r\nqdev->prsp_producer_index =\r\n(__le32 *) (((u8 *) qdev->preq_consumer_index) + 8);\r\nqdev->rsp_producer_index_phy_addr_high =\r\nqdev->req_consumer_index_phy_addr_high;\r\nqdev->rsp_producer_index_phy_addr_low =\r\nqdev->req_consumer_index_phy_addr_low + 8;\r\n} else {\r\nnetdev_err(qdev->ndev, "shadowReg Alloc failed\n");\r\nreturn -ENOMEM;\r\n}\r\nif (ql_alloc_net_req_rsp_queues(qdev) != 0) {\r\nnetdev_err(qdev->ndev, "ql_alloc_net_req_rsp_queues failed\n");\r\ngoto err_req_rsp;\r\n}\r\nif (ql_alloc_buffer_queues(qdev) != 0) {\r\nnetdev_err(qdev->ndev, "ql_alloc_buffer_queues failed\n");\r\ngoto err_buffer_queues;\r\n}\r\nif (ql_alloc_small_buffers(qdev) != 0) {\r\nnetdev_err(qdev->ndev, "ql_alloc_small_buffers failed\n");\r\ngoto err_small_buffers;\r\n}\r\nif (ql_alloc_large_buffers(qdev) != 0) {\r\nnetdev_err(qdev->ndev, "ql_alloc_large_buffers failed\n");\r\ngoto err_small_buffers;\r\n}\r\nql_init_large_buffers(qdev);\r\nif (ql_create_send_free_list(qdev))\r\ngoto err_free_list;\r\nqdev->rsp_current = qdev->rsp_q_virt_addr;\r\nreturn 0;\r\nerr_free_list:\r\nql_free_send_free_list(qdev);\r\nerr_small_buffers:\r\nql_free_buffer_queues(qdev);\r\nerr_buffer_queues:\r\nql_free_net_req_rsp_queues(qdev);\r\nerr_req_rsp:\r\npci_free_consistent(qdev->pdev,\r\nPAGE_SIZE,\r\nqdev->shadow_reg_virt_addr,\r\nqdev->shadow_reg_phy_addr);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ql_free_mem_resources(struct ql3_adapter *qdev)\r\n{\r\nql_free_send_free_list(qdev);\r\nql_free_large_buffers(qdev);\r\nql_free_small_buffers(qdev);\r\nql_free_buffer_queues(qdev);\r\nql_free_net_req_rsp_queues(qdev);\r\nif (qdev->shadow_reg_virt_addr != NULL) {\r\npci_free_consistent(qdev->pdev,\r\nPAGE_SIZE,\r\nqdev->shadow_reg_virt_addr,\r\nqdev->shadow_reg_phy_addr);\r\nqdev->shadow_reg_virt_addr = NULL;\r\n}\r\n}\r\nstatic int ql_init_misc_registers(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_local_ram_registers __iomem *local_ram =\r\n(void __iomem *)qdev->mem_map_registers;\r\nif (ql_sem_spinlock(qdev, QL_DDR_RAM_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index) *\r\n2) << 4))\r\nreturn -1;\r\nql_write_page2_reg(qdev,\r\n&local_ram->bufletSize, qdev->nvram_data.bufletSize);\r\nql_write_page2_reg(qdev,\r\n&local_ram->maxBufletCount,\r\nqdev->nvram_data.bufletCount);\r\nql_write_page2_reg(qdev,\r\n&local_ram->freeBufletThresholdLow,\r\n(qdev->nvram_data.tcpWindowThreshold25 << 16) |\r\n(qdev->nvram_data.tcpWindowThreshold0));\r\nql_write_page2_reg(qdev,\r\n&local_ram->freeBufletThresholdHigh,\r\nqdev->nvram_data.tcpWindowThreshold50);\r\nql_write_page2_reg(qdev,\r\n&local_ram->ipHashTableBase,\r\n(qdev->nvram_data.ipHashTableBaseHi << 16) |\r\nqdev->nvram_data.ipHashTableBaseLo);\r\nql_write_page2_reg(qdev,\r\n&local_ram->ipHashTableCount,\r\nqdev->nvram_data.ipHashTableSize);\r\nql_write_page2_reg(qdev,\r\n&local_ram->tcpHashTableBase,\r\n(qdev->nvram_data.tcpHashTableBaseHi << 16) |\r\nqdev->nvram_data.tcpHashTableBaseLo);\r\nql_write_page2_reg(qdev,\r\n&local_ram->tcpHashTableCount,\r\nqdev->nvram_data.tcpHashTableSize);\r\nql_write_page2_reg(qdev,\r\n&local_ram->ncbBase,\r\n(qdev->nvram_data.ncbTableBaseHi << 16) |\r\nqdev->nvram_data.ncbTableBaseLo);\r\nql_write_page2_reg(qdev,\r\n&local_ram->maxNcbCount,\r\nqdev->nvram_data.ncbTableSize);\r\nql_write_page2_reg(qdev,\r\n&local_ram->drbBase,\r\n(qdev->nvram_data.drbTableBaseHi << 16) |\r\nqdev->nvram_data.drbTableBaseLo);\r\nql_write_page2_reg(qdev,\r\n&local_ram->maxDrbCount,\r\nqdev->nvram_data.drbTableSize);\r\nql_sem_unlock(qdev, QL_DDR_RAM_SEM_MASK);\r\nreturn 0;\r\n}\r\nstatic int ql_adapter_initialize(struct ql3_adapter *qdev)\r\n{\r\nu32 value;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\n__iomem u32 *spir = &port_regs->CommonRegs.serialPortInterfaceReg;\r\nstruct ql3xxx_host_memory_registers __iomem *hmem_regs =\r\n(void __iomem *)port_regs;\r\nu32 delay = 10;\r\nint status = 0;\r\nif (ql_mii_setup(qdev))\r\nreturn -1;\r\nql_write_common_reg(qdev, spir,\r\n(ISP_SERIAL_PORT_IF_WE |\r\n(ISP_SERIAL_PORT_IF_WE << 16)));\r\nmdelay(100);\r\nqdev->port_link_state = LS_DOWN;\r\nnetif_carrier_off(qdev->ndev);\r\nql_write_common_reg(qdev, spir,\r\n(ISP_SERIAL_PORT_IF_SDE |\r\n(ISP_SERIAL_PORT_IF_SDE << 16)));\r\n*((u32 *)(qdev->preq_consumer_index)) = 0;\r\natomic_set(&qdev->tx_count, NUM_REQ_Q_ENTRIES);\r\nqdev->req_producer_index = 0;\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->reqConsumerIndexAddrHigh,\r\nqdev->req_consumer_index_phy_addr_high);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->reqConsumerIndexAddrLow,\r\nqdev->req_consumer_index_phy_addr_low);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->reqBaseAddrHigh,\r\nMS_64BITS(qdev->req_q_phy_addr));\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->reqBaseAddrLow,\r\nLS_64BITS(qdev->req_q_phy_addr));\r\nql_write_page1_reg(qdev, &hmem_regs->reqLength, NUM_REQ_Q_ENTRIES);\r\n*((__le16 *) (qdev->prsp_producer_index)) = 0;\r\nqdev->rsp_consumer_index = 0;\r\nqdev->rsp_current = qdev->rsp_q_virt_addr;\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rspProducerIndexAddrHigh,\r\nqdev->rsp_producer_index_phy_addr_high);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rspProducerIndexAddrLow,\r\nqdev->rsp_producer_index_phy_addr_low);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rspBaseAddrHigh,\r\nMS_64BITS(qdev->rsp_q_phy_addr));\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rspBaseAddrLow,\r\nLS_64BITS(qdev->rsp_q_phy_addr));\r\nql_write_page1_reg(qdev, &hmem_regs->rspLength, NUM_RSP_Q_ENTRIES);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rxLargeQBaseAddrHigh,\r\nMS_64BITS(qdev->lrg_buf_q_phy_addr));\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rxLargeQBaseAddrLow,\r\nLS_64BITS(qdev->lrg_buf_q_phy_addr));\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rxLargeQLength,\r\nqdev->num_lbufq_entries);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rxLargeBufferLength,\r\nqdev->lrg_buffer_len);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rxSmallQBaseAddrHigh,\r\nMS_64BITS(qdev->small_buf_q_phy_addr));\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rxSmallQBaseAddrLow,\r\nLS_64BITS(qdev->small_buf_q_phy_addr));\r\nql_write_page1_reg(qdev, &hmem_regs->rxSmallQLength, NUM_SBUFQ_ENTRIES);\r\nql_write_page1_reg(qdev,\r\n&hmem_regs->rxSmallBufferLength,\r\nQL_SMALL_BUFFER_SIZE);\r\nqdev->small_buf_q_producer_index = NUM_SBUFQ_ENTRIES - 1;\r\nqdev->small_buf_release_cnt = 8;\r\nqdev->lrg_buf_q_producer_index = qdev->num_lbufq_entries - 1;\r\nqdev->lrg_buf_release_cnt = 8;\r\nqdev->lrg_buf_next_free = qdev->lrg_buf_q_virt_addr;\r\nqdev->small_buf_index = 0;\r\nqdev->lrg_buf_index = 0;\r\nqdev->lrg_buf_free_count = 0;\r\nqdev->lrg_buf_free_head = NULL;\r\nqdev->lrg_buf_free_tail = NULL;\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.\r\nrxSmallQProducerIndex,\r\nqdev->small_buf_q_producer_index);\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.\r\nrxLargeQProducerIndex,\r\nqdev->lrg_buf_q_producer_index);\r\nclear_bit(QL_LINK_MASTER, &qdev->flags);\r\nvalue = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nif ((value & PORT_STATUS_IC) == 0) {\r\nif (ql_init_misc_registers(qdev)) {\r\nstatus = -1;\r\ngoto out;\r\n}\r\nvalue = qdev->nvram_data.tcpMaxWindowSize;\r\nql_write_page0_reg(qdev, &port_regs->tcpMaxWindow, value);\r\nvalue = (0xFFFF << 16) | qdev->nvram_data.extHwConfig;\r\nif (ql_sem_spinlock(qdev, QL_FLASH_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index)\r\n* 2) << 13)) {\r\nstatus = -1;\r\ngoto out;\r\n}\r\nql_write_page0_reg(qdev, &port_regs->ExternalHWConfig, value);\r\nql_write_page0_reg(qdev, &port_regs->InternalChipConfig,\r\n(((INTERNAL_CHIP_SD | INTERNAL_CHIP_WE) <<\r\n16) | (INTERNAL_CHIP_SD |\r\nINTERNAL_CHIP_WE)));\r\nql_sem_unlock(qdev, QL_FLASH_SEM_MASK);\r\n}\r\nif (qdev->mac_index)\r\nql_write_page0_reg(qdev,\r\n&port_regs->mac1MaxFrameLengthReg,\r\nqdev->max_frame_size);\r\nelse\r\nql_write_page0_reg(qdev,\r\n&port_regs->mac0MaxFrameLengthReg,\r\nqdev->max_frame_size);\r\nif (ql_sem_spinlock(qdev, QL_PHY_GIO_SEM_MASK,\r\n(QL_RESOURCE_BITS_BASE_CODE | (qdev->mac_index) *\r\n2) << 7)) {\r\nstatus = -1;\r\ngoto out;\r\n}\r\nPHY_Setup(qdev);\r\nql_init_scan_mode(qdev);\r\nql_get_phy_owner(qdev);\r\nql_write_page0_reg(qdev, &port_regs->macAddrIndirectPtrReg,\r\n(MAC_ADDR_INDIRECT_PTR_REG_RP_MASK << 16));\r\nql_write_page0_reg(qdev, &port_regs->macAddrDataReg,\r\n((qdev->ndev->dev_addr[2] << 24)\r\n| (qdev->ndev->dev_addr[3] << 16)\r\n| (qdev->ndev->dev_addr[4] << 8)\r\n| qdev->ndev->dev_addr[5]));\r\nql_write_page0_reg(qdev, &port_regs->macAddrIndirectPtrReg,\r\n((MAC_ADDR_INDIRECT_PTR_REG_RP_MASK << 16) | 1));\r\nql_write_page0_reg(qdev, &port_regs->macAddrDataReg,\r\n((qdev->ndev->dev_addr[0] << 8)\r\n| qdev->ndev->dev_addr[1]));\r\nql_write_page0_reg(qdev, &port_regs->macAddrIndirectPtrReg,\r\n((MAC_ADDR_INDIRECT_PTR_REG_PE << 16) |\r\nMAC_ADDR_INDIRECT_PTR_REG_PE));\r\nql_write_page0_reg(qdev, &port_regs->ipAddrIndexReg,\r\n((IP_ADDR_INDEX_REG_MASK << 16) |\r\n(qdev->mac_index << 2)));\r\nql_write_page0_reg(qdev, &port_regs->ipAddrDataReg, 0);\r\nql_write_page0_reg(qdev, &port_regs->ipAddrIndexReg,\r\n((IP_ADDR_INDEX_REG_MASK << 16) |\r\n((qdev->mac_index << 2) + 1)));\r\nql_write_page0_reg(qdev, &port_regs->ipAddrDataReg, 0);\r\nql_sem_unlock(qdev, QL_PHY_GIO_SEM_MASK);\r\nql_write_page0_reg(qdev,\r\n&port_regs->portControl,\r\n((PORT_CONTROL_CC << 16) | PORT_CONTROL_CC));\r\ndo {\r\nvalue = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nif (value & PORT_STATUS_IC)\r\nbreak;\r\nspin_unlock_irq(&qdev->hw_lock);\r\nmsleep(500);\r\nspin_lock_irq(&qdev->hw_lock);\r\n} while (--delay);\r\nif (delay == 0) {\r\nnetdev_err(qdev->ndev, "Hw Initialization timeout\n");\r\nstatus = -1;\r\ngoto out;\r\n}\r\nif (qdev->device_id == QL3032_DEVICE_ID) {\r\nvalue =\r\n(QL3032_PORT_CONTROL_EF | QL3032_PORT_CONTROL_KIE |\r\nQL3032_PORT_CONTROL_EIv6 | QL3032_PORT_CONTROL_EIv4 |\r\nQL3032_PORT_CONTROL_ET);\r\nql_write_page0_reg(qdev, &port_regs->functionControl,\r\n((value << 16) | value));\r\n} else {\r\nvalue =\r\n(PORT_CONTROL_EF | PORT_CONTROL_ET | PORT_CONTROL_EI |\r\nPORT_CONTROL_HH);\r\nql_write_page0_reg(qdev, &port_regs->portControl,\r\n((value << 16) | value));\r\n}\r\nout:\r\nreturn status;\r\n}\r\nstatic int ql_adapter_reset(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nint status = 0;\r\nu16 value;\r\nint max_wait_time;\r\nset_bit(QL_RESET_ACTIVE, &qdev->flags);\r\nclear_bit(QL_RESET_DONE, &qdev->flags);\r\nnetdev_printk(KERN_DEBUG, qdev->ndev, "Issue soft reset to chip\n");\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.ispControlStatus,\r\n((ISP_CONTROL_SR << 16) | ISP_CONTROL_SR));\r\nnetdev_printk(KERN_DEBUG, qdev->ndev,\r\n"Wait 10 milliseconds for reset to complete\n");\r\nmax_wait_time = 5;\r\ndo {\r\nvalue =\r\nql_read_common_reg(qdev,\r\n&port_regs->CommonRegs.ispControlStatus);\r\nif ((value & ISP_CONTROL_SR) == 0)\r\nbreak;\r\nssleep(1);\r\n} while ((--max_wait_time));\r\nvalue =\r\nql_read_common_reg(qdev, &port_regs->CommonRegs.ispControlStatus);\r\nif (value & ISP_CONTROL_RI) {\r\nnetdev_printk(KERN_DEBUG, qdev->ndev,\r\n"clearing RI after reset\n");\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.\r\nispControlStatus,\r\n((ISP_CONTROL_RI << 16) | ISP_CONTROL_RI));\r\n}\r\nif (max_wait_time == 0) {\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.\r\nispControlStatus,\r\n((ISP_CONTROL_FSR << 16) |\r\nISP_CONTROL_FSR));\r\nmax_wait_time = 5;\r\ndo {\r\nvalue = ql_read_common_reg(qdev,\r\n&port_regs->CommonRegs.\r\nispControlStatus);\r\nif ((value & ISP_CONTROL_FSR) == 0)\r\nbreak;\r\nssleep(1);\r\n} while ((--max_wait_time));\r\n}\r\nif (max_wait_time == 0)\r\nstatus = 1;\r\nclear_bit(QL_RESET_ACTIVE, &qdev->flags);\r\nset_bit(QL_RESET_DONE, &qdev->flags);\r\nreturn status;\r\n}\r\nstatic void ql_set_mac_info(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value, port_status;\r\nu8 func_number;\r\nvalue =\r\nql_read_common_reg_l(qdev, &port_regs->CommonRegs.ispControlStatus);\r\nfunc_number = (u8) ((value >> 4) & OPCODE_FUNC_ID_MASK);\r\nport_status = ql_read_page0_reg(qdev, &port_regs->portStatus);\r\nswitch (value & ISP_CONTROL_FN_MASK) {\r\ncase ISP_CONTROL_FN0_NET:\r\nqdev->mac_index = 0;\r\nqdev->mac_ob_opcode = OUTBOUND_MAC_IOCB | func_number;\r\nqdev->mb_bit_mask = FN0_MA_BITS_MASK;\r\nqdev->PHYAddr = PORT0_PHY_ADDRESS;\r\nif (port_status & PORT_STATUS_SM0)\r\nset_bit(QL_LINK_OPTICAL, &qdev->flags);\r\nelse\r\nclear_bit(QL_LINK_OPTICAL, &qdev->flags);\r\nbreak;\r\ncase ISP_CONTROL_FN1_NET:\r\nqdev->mac_index = 1;\r\nqdev->mac_ob_opcode = OUTBOUND_MAC_IOCB | func_number;\r\nqdev->mb_bit_mask = FN1_MA_BITS_MASK;\r\nqdev->PHYAddr = PORT1_PHY_ADDRESS;\r\nif (port_status & PORT_STATUS_SM1)\r\nset_bit(QL_LINK_OPTICAL, &qdev->flags);\r\nelse\r\nclear_bit(QL_LINK_OPTICAL, &qdev->flags);\r\nbreak;\r\ncase ISP_CONTROL_FN0_SCSI:\r\ncase ISP_CONTROL_FN1_SCSI:\r\ndefault:\r\nnetdev_printk(KERN_DEBUG, qdev->ndev,\r\n"Invalid function number, ispControlStatus = 0x%x\n",\r\nvalue);\r\nbreak;\r\n}\r\nqdev->numPorts = qdev->nvram_data.version_and_numPorts >> 8;\r\n}\r\nstatic void ql_display_dev_info(struct net_device *ndev)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nstruct pci_dev *pdev = qdev->pdev;\r\nnetdev_info(ndev,\r\n"%s Adapter %d RevisionID %d found %s on PCI slot %d\n",\r\nDRV_NAME, qdev->index, qdev->chip_rev_id,\r\nqdev->device_id == QL3032_DEVICE_ID ? "QLA3032" : "QLA3022",\r\nqdev->pci_slot);\r\nnetdev_info(ndev, "%s Interface\n",\r\ntest_bit(QL_LINK_OPTICAL, &qdev->flags) ? "OPTICAL" : "COPPER");\r\nnetdev_info(ndev, "Bus interface is %s %s\n",\r\n((qdev->pci_width == 64) ? "64-bit" : "32-bit"),\r\n((qdev->pci_x) ? "PCI-X" : "PCI"));\r\nnetdev_info(ndev, "mem IO base address adjusted = 0x%p\n",\r\nqdev->mem_map_registers);\r\nnetdev_info(ndev, "Interrupt number = %d\n", pdev->irq);\r\nnetif_info(qdev, probe, ndev, "MAC address %pM\n", ndev->dev_addr);\r\n}\r\nstatic int ql_adapter_down(struct ql3_adapter *qdev, int do_reset)\r\n{\r\nstruct net_device *ndev = qdev->ndev;\r\nint retval = 0;\r\nnetif_stop_queue(ndev);\r\nnetif_carrier_off(ndev);\r\nclear_bit(QL_ADAPTER_UP, &qdev->flags);\r\nclear_bit(QL_LINK_MASTER, &qdev->flags);\r\nql_disable_interrupts(qdev);\r\nfree_irq(qdev->pdev->irq, ndev);\r\nif (qdev->msi && test_bit(QL_MSI_ENABLED, &qdev->flags)) {\r\nnetdev_info(qdev->ndev, "calling pci_disable_msi()\n");\r\nclear_bit(QL_MSI_ENABLED, &qdev->flags);\r\npci_disable_msi(qdev->pdev);\r\n}\r\ndel_timer_sync(&qdev->adapter_timer);\r\nnapi_disable(&qdev->napi);\r\nif (do_reset) {\r\nint soft_reset;\r\nunsigned long hw_flags;\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nif (ql_wait_for_drvr_lock(qdev)) {\r\nsoft_reset = ql_adapter_reset(qdev);\r\nif (soft_reset) {\r\nnetdev_err(ndev, "ql_adapter_reset(%d) FAILED!\n",\r\nqdev->index);\r\n}\r\nnetdev_err(ndev,\r\n"Releasing driver lock via chip reset\n");\r\n} else {\r\nnetdev_err(ndev,\r\n"Could not acquire driver lock to do reset!\n");\r\nretval = -1;\r\n}\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\n}\r\nql_free_mem_resources(qdev);\r\nreturn retval;\r\n}\r\nstatic int ql_adapter_up(struct ql3_adapter *qdev)\r\n{\r\nstruct net_device *ndev = qdev->ndev;\r\nint err;\r\nunsigned long irq_flags = IRQF_SHARED;\r\nunsigned long hw_flags;\r\nif (ql_alloc_mem_resources(qdev)) {\r\nnetdev_err(ndev, "Unable to allocate buffers\n");\r\nreturn -ENOMEM;\r\n}\r\nif (qdev->msi) {\r\nif (pci_enable_msi(qdev->pdev)) {\r\nnetdev_err(ndev,\r\n"User requested MSI, but MSI failed to initialize. Continuing without MSI.\n");\r\nqdev->msi = 0;\r\n} else {\r\nnetdev_info(ndev, "MSI Enabled...\n");\r\nset_bit(QL_MSI_ENABLED, &qdev->flags);\r\nirq_flags &= ~IRQF_SHARED;\r\n}\r\n}\r\nerr = request_irq(qdev->pdev->irq, ql3xxx_isr,\r\nirq_flags, ndev->name, ndev);\r\nif (err) {\r\nnetdev_err(ndev,\r\n"Failed to reserve interrupt %d - already in use\n",\r\nqdev->pdev->irq);\r\ngoto err_irq;\r\n}\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nerr = ql_wait_for_drvr_lock(qdev);\r\nif (err) {\r\nerr = ql_adapter_initialize(qdev);\r\nif (err) {\r\nnetdev_err(ndev, "Unable to initialize adapter\n");\r\ngoto err_init;\r\n}\r\nnetdev_err(ndev, "Releasing driver lock\n");\r\nql_sem_unlock(qdev, QL_DRVR_SEM_MASK);\r\n} else {\r\nnetdev_err(ndev, "Could not acquire driver lock\n");\r\ngoto err_lock;\r\n}\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nset_bit(QL_ADAPTER_UP, &qdev->flags);\r\nmod_timer(&qdev->adapter_timer, jiffies + HZ * 1);\r\nnapi_enable(&qdev->napi);\r\nql_enable_interrupts(qdev);\r\nreturn 0;\r\nerr_init:\r\nql_sem_unlock(qdev, QL_DRVR_SEM_MASK);\r\nerr_lock:\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nfree_irq(qdev->pdev->irq, ndev);\r\nerr_irq:\r\nif (qdev->msi && test_bit(QL_MSI_ENABLED, &qdev->flags)) {\r\nnetdev_info(ndev, "calling pci_disable_msi()\n");\r\nclear_bit(QL_MSI_ENABLED, &qdev->flags);\r\npci_disable_msi(qdev->pdev);\r\n}\r\nreturn err;\r\n}\r\nstatic int ql_cycle_adapter(struct ql3_adapter *qdev, int reset)\r\n{\r\nif (ql_adapter_down(qdev, reset) || ql_adapter_up(qdev)) {\r\nnetdev_err(qdev->ndev,\r\n"Driver up/down cycle failed, closing device\n");\r\nrtnl_lock();\r\ndev_close(qdev->ndev);\r\nrtnl_unlock();\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ql3xxx_close(struct net_device *ndev)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nwhile (!test_bit(QL_ADAPTER_UP, &qdev->flags))\r\nmsleep(50);\r\nql_adapter_down(qdev, QL_DO_RESET);\r\nreturn 0;\r\n}\r\nstatic int ql3xxx_open(struct net_device *ndev)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nreturn ql_adapter_up(qdev);\r\n}\r\nstatic int ql3xxx_set_mac_address(struct net_device *ndev, void *p)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nstruct sockaddr *addr = p;\r\nunsigned long hw_flags;\r\nif (netif_running(ndev))\r\nreturn -EBUSY;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(ndev->dev_addr, addr->sa_data, ndev->addr_len);\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nql_write_page0_reg(qdev, &port_regs->macAddrIndirectPtrReg,\r\n(MAC_ADDR_INDIRECT_PTR_REG_RP_MASK << 16));\r\nql_write_page0_reg(qdev, &port_regs->macAddrDataReg,\r\n((ndev->dev_addr[2] << 24) | (ndev->\r\ndev_addr[3] << 16) |\r\n(ndev->dev_addr[4] << 8) | ndev->dev_addr[5]));\r\nql_write_page0_reg(qdev, &port_regs->macAddrIndirectPtrReg,\r\n((MAC_ADDR_INDIRECT_PTR_REG_RP_MASK << 16) | 1));\r\nql_write_page0_reg(qdev, &port_regs->macAddrDataReg,\r\n((ndev->dev_addr[0] << 8) | ndev->dev_addr[1]));\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn 0;\r\n}\r\nstatic void ql3xxx_tx_timeout(struct net_device *ndev)\r\n{\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nnetdev_err(ndev, "Resetting...\n");\r\nnetif_stop_queue(ndev);\r\nqueue_delayed_work(qdev->workqueue, &qdev->tx_timeout_work, 0);\r\n}\r\nstatic void ql_reset_work(struct work_struct *work)\r\n{\r\nstruct ql3_adapter *qdev =\r\ncontainer_of(work, struct ql3_adapter, reset_work.work);\r\nstruct net_device *ndev = qdev->ndev;\r\nu32 value;\r\nstruct ql_tx_buf_cb *tx_cb;\r\nint max_wait_time, i;\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nunsigned long hw_flags;\r\nif (test_bit((QL_RESET_PER_SCSI | QL_RESET_START), &qdev->flags)) {\r\nclear_bit(QL_LINK_MASTER, &qdev->flags);\r\nfor (i = 0; i < NUM_REQ_Q_ENTRIES; i++) {\r\nint j;\r\ntx_cb = &qdev->tx_buf[i];\r\nif (tx_cb->skb) {\r\nnetdev_printk(KERN_DEBUG, ndev,\r\n"Freeing lost SKB\n");\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(&tx_cb->map[0],\r\nmapaddr),\r\ndma_unmap_len(&tx_cb->map[0], maplen),\r\nPCI_DMA_TODEVICE);\r\nfor (j = 1; j < tx_cb->seg_count; j++) {\r\npci_unmap_page(qdev->pdev,\r\ndma_unmap_addr(&tx_cb->map[j],\r\nmapaddr),\r\ndma_unmap_len(&tx_cb->map[j],\r\nmaplen),\r\nPCI_DMA_TODEVICE);\r\n}\r\ndev_kfree_skb(tx_cb->skb);\r\ntx_cb->skb = NULL;\r\n}\r\n}\r\nnetdev_err(ndev, "Clearing NRI after reset\n");\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nql_write_common_reg(qdev,\r\n&port_regs->CommonRegs.\r\nispControlStatus,\r\n((ISP_CONTROL_RI << 16) | ISP_CONTROL_RI));\r\nmax_wait_time = 10;\r\ndo {\r\nvalue = ql_read_common_reg(qdev,\r\n&port_regs->CommonRegs.\r\nispControlStatus);\r\nif ((value & ISP_CONTROL_SR) == 0) {\r\nnetdev_printk(KERN_DEBUG, ndev,\r\n"reset completed\n");\r\nbreak;\r\n}\r\nif (value & ISP_CONTROL_RI) {\r\nnetdev_printk(KERN_DEBUG, ndev,\r\n"clearing NRI after reset\n");\r\nql_write_common_reg(qdev,\r\n&port_regs->\r\nCommonRegs.\r\nispControlStatus,\r\n((ISP_CONTROL_RI <<\r\n16) | ISP_CONTROL_RI));\r\n}\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nssleep(1);\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\n} while (--max_wait_time);\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nif (value & ISP_CONTROL_SR) {\r\nnetdev_err(ndev,\r\n"Timed out waiting for reset to complete\n");\r\nnetdev_err(ndev, "Do a reset\n");\r\nclear_bit(QL_RESET_PER_SCSI, &qdev->flags);\r\nclear_bit(QL_RESET_START, &qdev->flags);\r\nql_cycle_adapter(qdev, QL_DO_RESET);\r\nreturn;\r\n}\r\nclear_bit(QL_RESET_ACTIVE, &qdev->flags);\r\nclear_bit(QL_RESET_PER_SCSI, &qdev->flags);\r\nclear_bit(QL_RESET_START, &qdev->flags);\r\nql_cycle_adapter(qdev, QL_NO_RESET);\r\n}\r\n}\r\nstatic void ql_tx_timeout_work(struct work_struct *work)\r\n{\r\nstruct ql3_adapter *qdev =\r\ncontainer_of(work, struct ql3_adapter, tx_timeout_work.work);\r\nql_cycle_adapter(qdev, QL_DO_RESET);\r\n}\r\nstatic void ql_get_board_info(struct ql3_adapter *qdev)\r\n{\r\nstruct ql3xxx_port_registers __iomem *port_regs =\r\nqdev->mem_map_registers;\r\nu32 value;\r\nvalue = ql_read_page0_reg_l(qdev, &port_regs->portStatus);\r\nqdev->chip_rev_id = ((value & PORT_STATUS_REV_ID_MASK) >> 12);\r\nif (value & PORT_STATUS_64)\r\nqdev->pci_width = 64;\r\nelse\r\nqdev->pci_width = 32;\r\nif (value & PORT_STATUS_X)\r\nqdev->pci_x = 1;\r\nelse\r\nqdev->pci_x = 0;\r\nqdev->pci_slot = (u8) PCI_SLOT(qdev->pdev->devfn);\r\n}\r\nstatic void ql3xxx_timer(unsigned long ptr)\r\n{\r\nstruct ql3_adapter *qdev = (struct ql3_adapter *)ptr;\r\nqueue_delayed_work(qdev->workqueue, &qdev->link_state_work, 0);\r\n}\r\nstatic int ql3xxx_probe(struct pci_dev *pdev,\r\nconst struct pci_device_id *pci_entry)\r\n{\r\nstruct net_device *ndev = NULL;\r\nstruct ql3_adapter *qdev = NULL;\r\nstatic int cards_found;\r\nint uninitialized_var(pci_using_dac), err;\r\nerr = pci_enable_device(pdev);\r\nif (err) {\r\npr_err("%s cannot enable PCI device\n", pci_name(pdev));\r\ngoto err_out;\r\n}\r\nerr = pci_request_regions(pdev, DRV_NAME);\r\nif (err) {\r\npr_err("%s cannot obtain PCI resources\n", pci_name(pdev));\r\ngoto err_out_disable_pdev;\r\n}\r\npci_set_master(pdev);\r\nif (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\npci_using_dac = 1;\r\nerr = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));\r\n} else if (!(err = pci_set_dma_mask(pdev, DMA_BIT_MASK(32)))) {\r\npci_using_dac = 0;\r\nerr = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\n}\r\nif (err) {\r\npr_err("%s no usable DMA configuration\n", pci_name(pdev));\r\ngoto err_out_free_regions;\r\n}\r\nndev = alloc_etherdev(sizeof(struct ql3_adapter));\r\nif (!ndev) {\r\nerr = -ENOMEM;\r\ngoto err_out_free_regions;\r\n}\r\nSET_NETDEV_DEV(ndev, &pdev->dev);\r\npci_set_drvdata(pdev, ndev);\r\nqdev = netdev_priv(ndev);\r\nqdev->index = cards_found;\r\nqdev->ndev = ndev;\r\nqdev->pdev = pdev;\r\nqdev->device_id = pci_entry->device;\r\nqdev->port_link_state = LS_DOWN;\r\nif (msi)\r\nqdev->msi = 1;\r\nqdev->msg_enable = netif_msg_init(debug, default_msg);\r\nif (pci_using_dac)\r\nndev->features |= NETIF_F_HIGHDMA;\r\nif (qdev->device_id == QL3032_DEVICE_ID)\r\nndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;\r\nqdev->mem_map_registers = pci_ioremap_bar(pdev, 1);\r\nif (!qdev->mem_map_registers) {\r\npr_err("%s: cannot map device registers\n", pci_name(pdev));\r\nerr = -EIO;\r\ngoto err_out_free_ndev;\r\n}\r\nspin_lock_init(&qdev->adapter_lock);\r\nspin_lock_init(&qdev->hw_lock);\r\nndev->netdev_ops = &ql3xxx_netdev_ops;\r\nndev->ethtool_ops = &ql3xxx_ethtool_ops;\r\nndev->watchdog_timeo = 5 * HZ;\r\nnetif_napi_add(ndev, &qdev->napi, ql_poll, 64);\r\nndev->irq = pdev->irq;\r\nif (ql_get_nvram_params(qdev)) {\r\npr_alert("%s: Adapter #%d, Invalid NVRAM parameters\n",\r\n__func__, qdev->index);\r\nerr = -EIO;\r\ngoto err_out_iounmap;\r\n}\r\nql_set_mac_info(qdev);\r\nif (qdev->mac_index) {\r\nndev->mtu = qdev->nvram_data.macCfg_port1.etherMtu_mac ;\r\nql_set_mac_addr(ndev, qdev->nvram_data.funcCfg_fn2.macAddress);\r\n} else {\r\nndev->mtu = qdev->nvram_data.macCfg_port0.etherMtu_mac ;\r\nql_set_mac_addr(ndev, qdev->nvram_data.funcCfg_fn0.macAddress);\r\n}\r\nndev->tx_queue_len = NUM_REQ_Q_ENTRIES;\r\nql_get_board_info(qdev);\r\nif (qdev->pci_x)\r\npci_write_config_word(pdev, (int)0x4e, (u16) 0x0036);\r\nerr = register_netdev(ndev);\r\nif (err) {\r\npr_err("%s: cannot register net device\n", pci_name(pdev));\r\ngoto err_out_iounmap;\r\n}\r\nnetif_carrier_off(ndev);\r\nnetif_stop_queue(ndev);\r\nqdev->workqueue = create_singlethread_workqueue(ndev->name);\r\nINIT_DELAYED_WORK(&qdev->reset_work, ql_reset_work);\r\nINIT_DELAYED_WORK(&qdev->tx_timeout_work, ql_tx_timeout_work);\r\nINIT_DELAYED_WORK(&qdev->link_state_work, ql_link_state_machine_work);\r\ninit_timer(&qdev->adapter_timer);\r\nqdev->adapter_timer.function = ql3xxx_timer;\r\nqdev->adapter_timer.expires = jiffies + HZ * 2;\r\nqdev->adapter_timer.data = (unsigned long)qdev;\r\nif (!cards_found) {\r\npr_alert("%s\n", DRV_STRING);\r\npr_alert("Driver name: %s, Version: %s\n",\r\nDRV_NAME, DRV_VERSION);\r\n}\r\nql_display_dev_info(ndev);\r\ncards_found++;\r\nreturn 0;\r\nerr_out_iounmap:\r\niounmap(qdev->mem_map_registers);\r\nerr_out_free_ndev:\r\nfree_netdev(ndev);\r\nerr_out_free_regions:\r\npci_release_regions(pdev);\r\nerr_out_disable_pdev:\r\npci_disable_device(pdev);\r\nerr_out:\r\nreturn err;\r\n}\r\nstatic void ql3xxx_remove(struct pci_dev *pdev)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql3_adapter *qdev = netdev_priv(ndev);\r\nunregister_netdev(ndev);\r\nql_disable_interrupts(qdev);\r\nif (qdev->workqueue) {\r\ncancel_delayed_work(&qdev->reset_work);\r\ncancel_delayed_work(&qdev->tx_timeout_work);\r\ndestroy_workqueue(qdev->workqueue);\r\nqdev->workqueue = NULL;\r\n}\r\niounmap(qdev->mem_map_registers);\r\npci_release_regions(pdev);\r\nfree_netdev(ndev);\r\n}
