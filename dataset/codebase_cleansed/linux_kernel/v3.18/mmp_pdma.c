static void set_desc(struct mmp_pdma_phy *phy, dma_addr_t addr)\r\n{\r\nu32 reg = (phy->idx << 4) + DDADR;\r\nwritel(addr, phy->base + reg);\r\n}\r\nstatic void enable_chan(struct mmp_pdma_phy *phy)\r\n{\r\nu32 reg, dalgn;\r\nif (!phy->vchan)\r\nreturn;\r\nreg = DRCMR(phy->vchan->drcmr);\r\nwritel(DRCMR_MAPVLD | phy->idx, phy->base + reg);\r\ndalgn = readl(phy->base + DALGN);\r\nif (phy->vchan->byte_align)\r\ndalgn |= 1 << phy->idx;\r\nelse\r\ndalgn &= ~(1 << phy->idx);\r\nwritel(dalgn, phy->base + DALGN);\r\nreg = (phy->idx << 2) + DCSR;\r\nwritel(readl(phy->base + reg) | DCSR_RUN, phy->base + reg);\r\n}\r\nstatic void disable_chan(struct mmp_pdma_phy *phy)\r\n{\r\nu32 reg;\r\nif (!phy)\r\nreturn;\r\nreg = (phy->idx << 2) + DCSR;\r\nwritel(readl(phy->base + reg) & ~DCSR_RUN, phy->base + reg);\r\n}\r\nstatic int clear_chan_irq(struct mmp_pdma_phy *phy)\r\n{\r\nu32 dcsr;\r\nu32 dint = readl(phy->base + DINT);\r\nu32 reg = (phy->idx << 2) + DCSR;\r\nif (!(dint & BIT(phy->idx)))\r\nreturn -EAGAIN;\r\ndcsr = readl(phy->base + reg);\r\nwritel(dcsr, phy->base + reg);\r\nif ((dcsr & DCSR_BUSERR) && (phy->vchan))\r\ndev_warn(phy->vchan->dev, "DCSR_BUSERR\n");\r\nreturn 0;\r\n}\r\nstatic irqreturn_t mmp_pdma_chan_handler(int irq, void *dev_id)\r\n{\r\nstruct mmp_pdma_phy *phy = dev_id;\r\nif (clear_chan_irq(phy) != 0)\r\nreturn IRQ_NONE;\r\ntasklet_schedule(&phy->vchan->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t mmp_pdma_int_handler(int irq, void *dev_id)\r\n{\r\nstruct mmp_pdma_device *pdev = dev_id;\r\nstruct mmp_pdma_phy *phy;\r\nu32 dint = readl(pdev->base + DINT);\r\nint i, ret;\r\nint irq_num = 0;\r\nwhile (dint) {\r\ni = __ffs(dint);\r\ndint &= (dint - 1);\r\nphy = &pdev->phy[i];\r\nret = mmp_pdma_chan_handler(irq, phy);\r\nif (ret == IRQ_HANDLED)\r\nirq_num++;\r\n}\r\nif (irq_num)\r\nreturn IRQ_HANDLED;\r\nreturn IRQ_NONE;\r\n}\r\nstatic struct mmp_pdma_phy *lookup_phy(struct mmp_pdma_chan *pchan)\r\n{\r\nint prio, i;\r\nstruct mmp_pdma_device *pdev = to_mmp_pdma_dev(pchan->chan.device);\r\nstruct mmp_pdma_phy *phy, *found = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pdev->phy_lock, flags);\r\nfor (prio = 0; prio <= ((pdev->dma_channels - 1) & 0xf) >> 2; prio++) {\r\nfor (i = 0; i < pdev->dma_channels; i++) {\r\nif (prio != (i & 0xf) >> 2)\r\ncontinue;\r\nphy = &pdev->phy[i];\r\nif (!phy->vchan) {\r\nphy->vchan = pchan;\r\nfound = phy;\r\ngoto out_unlock;\r\n}\r\n}\r\n}\r\nout_unlock:\r\nspin_unlock_irqrestore(&pdev->phy_lock, flags);\r\nreturn found;\r\n}\r\nstatic void mmp_pdma_free_phy(struct mmp_pdma_chan *pchan)\r\n{\r\nstruct mmp_pdma_device *pdev = to_mmp_pdma_dev(pchan->chan.device);\r\nunsigned long flags;\r\nu32 reg;\r\nif (!pchan->phy)\r\nreturn;\r\nreg = DRCMR(pchan->drcmr);\r\nwritel(0, pchan->phy->base + reg);\r\nspin_lock_irqsave(&pdev->phy_lock, flags);\r\npchan->phy->vchan = NULL;\r\npchan->phy = NULL;\r\nspin_unlock_irqrestore(&pdev->phy_lock, flags);\r\n}\r\nstatic void start_pending_queue(struct mmp_pdma_chan *chan)\r\n{\r\nstruct mmp_pdma_desc_sw *desc;\r\nif (!chan->idle) {\r\ndev_dbg(chan->dev, "DMA controller still busy\n");\r\nreturn;\r\n}\r\nif (list_empty(&chan->chain_pending)) {\r\nmmp_pdma_free_phy(chan);\r\ndev_dbg(chan->dev, "no pending list\n");\r\nreturn;\r\n}\r\nif (!chan->phy) {\r\nchan->phy = lookup_phy(chan);\r\nif (!chan->phy) {\r\ndev_dbg(chan->dev, "no free dma channel\n");\r\nreturn;\r\n}\r\n}\r\ndesc = list_first_entry(&chan->chain_pending,\r\nstruct mmp_pdma_desc_sw, node);\r\nlist_splice_tail_init(&chan->chain_pending, &chan->chain_running);\r\nset_desc(chan->phy, desc->async_tx.phys);\r\nenable_chan(chan->phy);\r\nchan->idle = false;\r\n}\r\nstatic dma_cookie_t mmp_pdma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(tx->chan);\r\nstruct mmp_pdma_desc_sw *desc = tx_to_mmp_pdma_desc(tx);\r\nstruct mmp_pdma_desc_sw *child;\r\nunsigned long flags;\r\ndma_cookie_t cookie = -EBUSY;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nlist_for_each_entry(child, &desc->tx_list, node) {\r\ncookie = dma_cookie_assign(&child->async_tx);\r\n}\r\nlist_splice_tail_init(&desc->tx_list, &chan->chain_pending);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nreturn cookie;\r\n}\r\nstatic struct mmp_pdma_desc_sw *\r\nmmp_pdma_alloc_descriptor(struct mmp_pdma_chan *chan)\r\n{\r\nstruct mmp_pdma_desc_sw *desc;\r\ndma_addr_t pdesc;\r\ndesc = dma_pool_alloc(chan->desc_pool, GFP_ATOMIC, &pdesc);\r\nif (!desc) {\r\ndev_err(chan->dev, "out of memory for link descriptor\n");\r\nreturn NULL;\r\n}\r\nmemset(desc, 0, sizeof(*desc));\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->chan);\r\ndesc->async_tx.tx_submit = mmp_pdma_tx_submit;\r\ndesc->async_tx.phys = pdesc;\r\nreturn desc;\r\n}\r\nstatic int mmp_pdma_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nif (chan->desc_pool)\r\nreturn 1;\r\nchan->desc_pool = dma_pool_create(dev_name(&dchan->dev->device),\r\nchan->dev,\r\nsizeof(struct mmp_pdma_desc_sw),\r\n__alignof__(struct mmp_pdma_desc_sw),\r\n0);\r\nif (!chan->desc_pool) {\r\ndev_err(chan->dev, "unable to allocate descriptor pool\n");\r\nreturn -ENOMEM;\r\n}\r\nmmp_pdma_free_phy(chan);\r\nchan->idle = true;\r\nchan->dev_addr = 0;\r\nreturn 1;\r\n}\r\nstatic void mmp_pdma_free_desc_list(struct mmp_pdma_chan *chan,\r\nstruct list_head *list)\r\n{\r\nstruct mmp_pdma_desc_sw *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, list, node) {\r\nlist_del(&desc->node);\r\ndma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);\r\n}\r\n}\r\nstatic void mmp_pdma_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_pending);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_running);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\ndma_pool_destroy(chan->desc_pool);\r\nchan->desc_pool = NULL;\r\nchan->idle = true;\r\nchan->dev_addr = 0;\r\nmmp_pdma_free_phy(chan);\r\nreturn;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmmp_pdma_prep_memcpy(struct dma_chan *dchan,\r\ndma_addr_t dma_dst, dma_addr_t dma_src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct mmp_pdma_chan *chan;\r\nstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new;\r\nsize_t copy = 0;\r\nif (!dchan)\r\nreturn NULL;\r\nif (!len)\r\nreturn NULL;\r\nchan = to_mmp_pdma_chan(dchan);\r\nchan->byte_align = false;\r\nif (!chan->dir) {\r\nchan->dir = DMA_MEM_TO_MEM;\r\nchan->dcmd = DCMD_INCTRGADDR | DCMD_INCSRCADDR;\r\nchan->dcmd |= DCMD_BURST32;\r\n}\r\ndo {\r\nnew = mmp_pdma_alloc_descriptor(chan);\r\nif (!new) {\r\ndev_err(chan->dev, "no memory for desc\n");\r\ngoto fail;\r\n}\r\ncopy = min_t(size_t, len, PDMA_MAX_DESC_BYTES);\r\nif (dma_src & 0x7 || dma_dst & 0x7)\r\nchan->byte_align = true;\r\nnew->desc.dcmd = chan->dcmd | (DCMD_LENGTH & copy);\r\nnew->desc.dsadr = dma_src;\r\nnew->desc.dtadr = dma_dst;\r\nif (!first)\r\nfirst = new;\r\nelse\r\nprev->desc.ddadr = new->async_tx.phys;\r\nnew->async_tx.cookie = 0;\r\nasync_tx_ack(&new->async_tx);\r\nprev = new;\r\nlen -= copy;\r\nif (chan->dir == DMA_MEM_TO_DEV) {\r\ndma_src += copy;\r\n} else if (chan->dir == DMA_DEV_TO_MEM) {\r\ndma_dst += copy;\r\n} else if (chan->dir == DMA_MEM_TO_MEM) {\r\ndma_src += copy;\r\ndma_dst += copy;\r\n}\r\nlist_add_tail(&new->node, &first->tx_list);\r\n} while (len);\r\nfirst->async_tx.flags = flags;\r\nfirst->async_tx.cookie = -EBUSY;\r\nnew->desc.ddadr = DDADR_STOP;\r\nnew->desc.dcmd |= DCMD_ENDIRQEN;\r\nchan->cyclic_first = NULL;\r\nreturn &first->async_tx;\r\nfail:\r\nif (first)\r\nmmp_pdma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmmp_pdma_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction dir,\r\nunsigned long flags, void *context)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new = NULL;\r\nsize_t len, avail;\r\nstruct scatterlist *sg;\r\ndma_addr_t addr;\r\nint i;\r\nif ((sgl == NULL) || (sg_len == 0))\r\nreturn NULL;\r\nchan->byte_align = false;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\naddr = sg_dma_address(sg);\r\navail = sg_dma_len(sgl);\r\ndo {\r\nlen = min_t(size_t, avail, PDMA_MAX_DESC_BYTES);\r\nif (addr & 0x7)\r\nchan->byte_align = true;\r\nnew = mmp_pdma_alloc_descriptor(chan);\r\nif (!new) {\r\ndev_err(chan->dev, "no memory for desc\n");\r\ngoto fail;\r\n}\r\nnew->desc.dcmd = chan->dcmd | (DCMD_LENGTH & len);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nnew->desc.dsadr = addr;\r\nnew->desc.dtadr = chan->dev_addr;\r\n} else {\r\nnew->desc.dsadr = chan->dev_addr;\r\nnew->desc.dtadr = addr;\r\n}\r\nif (!first)\r\nfirst = new;\r\nelse\r\nprev->desc.ddadr = new->async_tx.phys;\r\nnew->async_tx.cookie = 0;\r\nasync_tx_ack(&new->async_tx);\r\nprev = new;\r\nlist_add_tail(&new->node, &first->tx_list);\r\naddr += len;\r\navail -= len;\r\n} while (avail);\r\n}\r\nfirst->async_tx.cookie = -EBUSY;\r\nfirst->async_tx.flags = flags;\r\nnew->desc.ddadr = DDADR_STOP;\r\nnew->desc.dcmd |= DCMD_ENDIRQEN;\r\nchan->dir = dir;\r\nchan->cyclic_first = NULL;\r\nreturn &first->async_tx;\r\nfail:\r\nif (first)\r\nmmp_pdma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmmp_pdma_prep_dma_cyclic(struct dma_chan *dchan,\r\ndma_addr_t buf_addr, size_t len, size_t period_len,\r\nenum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct mmp_pdma_chan *chan;\r\nstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new;\r\ndma_addr_t dma_src, dma_dst;\r\nif (!dchan || !len || !period_len)\r\nreturn NULL;\r\nif (len % period_len != 0)\r\nreturn NULL;\r\nif (period_len > PDMA_MAX_DESC_BYTES)\r\nreturn NULL;\r\nchan = to_mmp_pdma_chan(dchan);\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\ndma_src = buf_addr;\r\ndma_dst = chan->dev_addr;\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\ndma_dst = buf_addr;\r\ndma_src = chan->dev_addr;\r\nbreak;\r\ndefault:\r\ndev_err(chan->dev, "Unsupported direction for cyclic DMA\n");\r\nreturn NULL;\r\n}\r\nchan->dir = direction;\r\ndo {\r\nnew = mmp_pdma_alloc_descriptor(chan);\r\nif (!new) {\r\ndev_err(chan->dev, "no memory for desc\n");\r\ngoto fail;\r\n}\r\nnew->desc.dcmd = (chan->dcmd | DCMD_ENDIRQEN |\r\n(DCMD_LENGTH & period_len));\r\nnew->desc.dsadr = dma_src;\r\nnew->desc.dtadr = dma_dst;\r\nif (!first)\r\nfirst = new;\r\nelse\r\nprev->desc.ddadr = new->async_tx.phys;\r\nnew->async_tx.cookie = 0;\r\nasync_tx_ack(&new->async_tx);\r\nprev = new;\r\nlen -= period_len;\r\nif (chan->dir == DMA_MEM_TO_DEV)\r\ndma_src += period_len;\r\nelse\r\ndma_dst += period_len;\r\nlist_add_tail(&new->node, &first->tx_list);\r\n} while (len);\r\nfirst->async_tx.flags = flags;\r\nfirst->async_tx.cookie = -EBUSY;\r\nnew->desc.ddadr = first->async_tx.phys;\r\nchan->cyclic_first = first;\r\nreturn &first->async_tx;\r\nfail:\r\nif (first)\r\nmmp_pdma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic int mmp_pdma_control(struct dma_chan *dchan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nstruct dma_slave_config *cfg = (void *)arg;\r\nunsigned long flags;\r\nu32 maxburst = 0, addr = 0;\r\nenum dma_slave_buswidth width = DMA_SLAVE_BUSWIDTH_UNDEFINED;\r\nif (!dchan)\r\nreturn -EINVAL;\r\nswitch (cmd) {\r\ncase DMA_TERMINATE_ALL:\r\ndisable_chan(chan->phy);\r\nmmp_pdma_free_phy(chan);\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_pending);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_running);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nchan->idle = true;\r\nbreak;\r\ncase DMA_SLAVE_CONFIG:\r\nif (cfg->direction == DMA_DEV_TO_MEM) {\r\nchan->dcmd = DCMD_INCTRGADDR | DCMD_FLOWSRC;\r\nmaxburst = cfg->src_maxburst;\r\nwidth = cfg->src_addr_width;\r\naddr = cfg->src_addr;\r\n} else if (cfg->direction == DMA_MEM_TO_DEV) {\r\nchan->dcmd = DCMD_INCSRCADDR | DCMD_FLOWTRG;\r\nmaxburst = cfg->dst_maxburst;\r\nwidth = cfg->dst_addr_width;\r\naddr = cfg->dst_addr;\r\n}\r\nif (width == DMA_SLAVE_BUSWIDTH_1_BYTE)\r\nchan->dcmd |= DCMD_WIDTH1;\r\nelse if (width == DMA_SLAVE_BUSWIDTH_2_BYTES)\r\nchan->dcmd |= DCMD_WIDTH2;\r\nelse if (width == DMA_SLAVE_BUSWIDTH_4_BYTES)\r\nchan->dcmd |= DCMD_WIDTH4;\r\nif (maxburst == 8)\r\nchan->dcmd |= DCMD_BURST8;\r\nelse if (maxburst == 16)\r\nchan->dcmd |= DCMD_BURST16;\r\nelse if (maxburst == 32)\r\nchan->dcmd |= DCMD_BURST32;\r\nchan->dir = cfg->direction;\r\nchan->dev_addr = addr;\r\nif (cfg->slave_id)\r\nchan->drcmr = cfg->slave_id;\r\nbreak;\r\ndefault:\r\nreturn -ENOSYS;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned int mmp_pdma_residue(struct mmp_pdma_chan *chan,\r\ndma_cookie_t cookie)\r\n{\r\nstruct mmp_pdma_desc_sw *sw;\r\nu32 curr, residue = 0;\r\nbool passed = false;\r\nbool cyclic = chan->cyclic_first != NULL;\r\nif (!chan->phy)\r\nreturn 0;\r\nif (chan->dir == DMA_DEV_TO_MEM)\r\ncurr = readl(chan->phy->base + DTADR(chan->phy->idx));\r\nelse\r\ncurr = readl(chan->phy->base + DSADR(chan->phy->idx));\r\nlist_for_each_entry(sw, &chan->chain_running, node) {\r\nu32 start, end, len;\r\nif (chan->dir == DMA_DEV_TO_MEM)\r\nstart = sw->desc.dtadr;\r\nelse\r\nstart = sw->desc.dsadr;\r\nlen = sw->desc.dcmd & DCMD_LENGTH;\r\nend = start + len;\r\nif (passed) {\r\nresidue += len;\r\n} else if (curr >= start && curr <= end) {\r\nresidue += end - curr;\r\npassed = true;\r\n}\r\nif (cyclic || !(sw->desc.dcmd & DCMD_ENDIRQEN))\r\ncontinue;\r\nif (sw->async_tx.cookie == cookie) {\r\nreturn residue;\r\n} else {\r\nresidue = 0;\r\npassed = false;\r\n}\r\n}\r\nreturn residue;\r\n}\r\nstatic enum dma_status mmp_pdma_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(dchan, cookie, txstate);\r\nif (likely(ret != DMA_ERROR))\r\ndma_set_residue(txstate, mmp_pdma_residue(chan, cookie));\r\nreturn ret;\r\n}\r\nstatic void mmp_pdma_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nstart_pending_queue(chan);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\n}\r\nstatic void dma_do_tasklet(unsigned long data)\r\n{\r\nstruct mmp_pdma_chan *chan = (struct mmp_pdma_chan *)data;\r\nstruct mmp_pdma_desc_sw *desc, *_desc;\r\nLIST_HEAD(chain_cleanup);\r\nunsigned long flags;\r\nif (chan->cyclic_first) {\r\ndma_async_tx_callback cb = NULL;\r\nvoid *cb_data = NULL;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\ndesc = chan->cyclic_first;\r\ncb = desc->async_tx.callback;\r\ncb_data = desc->async_tx.callback_param;\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nif (cb)\r\ncb(cb_data);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &chan->chain_running, node) {\r\nlist_move(&desc->node, &chain_cleanup);\r\nif (desc->desc.dcmd & DCMD_ENDIRQEN) {\r\ndma_cookie_t cookie = desc->async_tx.cookie;\r\ndma_cookie_complete(&desc->async_tx);\r\ndev_dbg(chan->dev, "completed_cookie=%d\n", cookie);\r\nbreak;\r\n}\r\n}\r\nchan->idle = list_empty(&chan->chain_running);\r\nstart_pending_queue(chan);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &chain_cleanup, node) {\r\nstruct dma_async_tx_descriptor *txd = &desc->async_tx;\r\nlist_del(&desc->node);\r\nif (txd->callback)\r\ntxd->callback(txd->callback_param);\r\ndma_pool_free(chan->desc_pool, desc, txd->phys);\r\n}\r\n}\r\nstatic int mmp_pdma_remove(struct platform_device *op)\r\n{\r\nstruct mmp_pdma_device *pdev = platform_get_drvdata(op);\r\ndma_async_device_unregister(&pdev->device);\r\nreturn 0;\r\n}\r\nstatic int mmp_pdma_chan_init(struct mmp_pdma_device *pdev, int idx, int irq)\r\n{\r\nstruct mmp_pdma_phy *phy = &pdev->phy[idx];\r\nstruct mmp_pdma_chan *chan;\r\nint ret;\r\nchan = devm_kzalloc(pdev->dev, sizeof(*chan), GFP_KERNEL);\r\nif (chan == NULL)\r\nreturn -ENOMEM;\r\nphy->idx = idx;\r\nphy->base = pdev->base;\r\nif (irq) {\r\nret = devm_request_irq(pdev->dev, irq, mmp_pdma_chan_handler,\r\nIRQF_SHARED, "pdma", phy);\r\nif (ret) {\r\ndev_err(pdev->dev, "channel request irq fail!\n");\r\nreturn ret;\r\n}\r\n}\r\nspin_lock_init(&chan->desc_lock);\r\nchan->dev = pdev->dev;\r\nchan->chan.device = &pdev->device;\r\ntasklet_init(&chan->tasklet, dma_do_tasklet, (unsigned long)chan);\r\nINIT_LIST_HEAD(&chan->chain_pending);\r\nINIT_LIST_HEAD(&chan->chain_running);\r\nlist_add_tail(&chan->chan.device_node, &pdev->device.channels);\r\nreturn 0;\r\n}\r\nstatic struct dma_chan *mmp_pdma_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct mmp_pdma_device *d = ofdma->of_dma_data;\r\nstruct dma_chan *chan;\r\nchan = dma_get_any_slave_channel(&d->device);\r\nif (!chan)\r\nreturn NULL;\r\nto_mmp_pdma_chan(chan)->drcmr = dma_spec->args[0];\r\nreturn chan;\r\n}\r\nstatic int mmp_pdma_probe(struct platform_device *op)\r\n{\r\nstruct mmp_pdma_device *pdev;\r\nconst struct of_device_id *of_id;\r\nstruct mmp_dma_platdata *pdata = dev_get_platdata(&op->dev);\r\nstruct resource *iores;\r\nint i, ret, irq = 0;\r\nint dma_channels = 0, irq_num = 0;\r\npdev = devm_kzalloc(&op->dev, sizeof(*pdev), GFP_KERNEL);\r\nif (!pdev)\r\nreturn -ENOMEM;\r\npdev->dev = &op->dev;\r\nspin_lock_init(&pdev->phy_lock);\r\niores = platform_get_resource(op, IORESOURCE_MEM, 0);\r\npdev->base = devm_ioremap_resource(pdev->dev, iores);\r\nif (IS_ERR(pdev->base))\r\nreturn PTR_ERR(pdev->base);\r\nof_id = of_match_device(mmp_pdma_dt_ids, pdev->dev);\r\nif (of_id)\r\nof_property_read_u32(pdev->dev->of_node, "#dma-channels",\r\n&dma_channels);\r\nelse if (pdata && pdata->dma_channels)\r\ndma_channels = pdata->dma_channels;\r\nelse\r\ndma_channels = 32;\r\npdev->dma_channels = dma_channels;\r\nfor (i = 0; i < dma_channels; i++) {\r\nif (platform_get_irq(op, i) > 0)\r\nirq_num++;\r\n}\r\npdev->phy = devm_kcalloc(pdev->dev, dma_channels, sizeof(*pdev->phy),\r\nGFP_KERNEL);\r\nif (pdev->phy == NULL)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&pdev->device.channels);\r\nif (irq_num != dma_channels) {\r\nirq = platform_get_irq(op, 0);\r\nret = devm_request_irq(pdev->dev, irq, mmp_pdma_int_handler,\r\nIRQF_SHARED, "pdma", pdev);\r\nif (ret)\r\nreturn ret;\r\n}\r\nfor (i = 0; i < dma_channels; i++) {\r\nirq = (irq_num != dma_channels) ? 0 : platform_get_irq(op, i);\r\nret = mmp_pdma_chan_init(pdev, i, irq);\r\nif (ret)\r\nreturn ret;\r\n}\r\ndma_cap_set(DMA_SLAVE, pdev->device.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, pdev->device.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, pdev->device.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, pdev->device.cap_mask);\r\npdev->device.dev = &op->dev;\r\npdev->device.device_alloc_chan_resources = mmp_pdma_alloc_chan_resources;\r\npdev->device.device_free_chan_resources = mmp_pdma_free_chan_resources;\r\npdev->device.device_tx_status = mmp_pdma_tx_status;\r\npdev->device.device_prep_dma_memcpy = mmp_pdma_prep_memcpy;\r\npdev->device.device_prep_slave_sg = mmp_pdma_prep_slave_sg;\r\npdev->device.device_prep_dma_cyclic = mmp_pdma_prep_dma_cyclic;\r\npdev->device.device_issue_pending = mmp_pdma_issue_pending;\r\npdev->device.device_control = mmp_pdma_control;\r\npdev->device.copy_align = PDMA_ALIGNMENT;\r\nif (pdev->dev->coherent_dma_mask)\r\ndma_set_mask(pdev->dev, pdev->dev->coherent_dma_mask);\r\nelse\r\ndma_set_mask(pdev->dev, DMA_BIT_MASK(64));\r\nret = dma_async_device_register(&pdev->device);\r\nif (ret) {\r\ndev_err(pdev->device.dev, "unable to register\n");\r\nreturn ret;\r\n}\r\nif (op->dev.of_node) {\r\nret = of_dma_controller_register(op->dev.of_node,\r\nmmp_pdma_dma_xlate, pdev);\r\nif (ret < 0) {\r\ndev_err(&op->dev, "of_dma_controller_register failed\n");\r\nreturn ret;\r\n}\r\n}\r\nplatform_set_drvdata(op, pdev);\r\ndev_info(pdev->device.dev, "initialized %d channels\n", dma_channels);\r\nreturn 0;\r\n}\r\nbool mmp_pdma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nstruct mmp_pdma_chan *c = to_mmp_pdma_chan(chan);\r\nif (chan->device->dev->driver != &mmp_pdma_driver.driver)\r\nreturn false;\r\nc->drcmr = *(unsigned int *)param;\r\nreturn true;\r\n}
