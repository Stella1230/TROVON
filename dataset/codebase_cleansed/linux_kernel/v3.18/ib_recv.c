void rds_ib_recv_init_ring(struct rds_ib_connection *ic)\r\n{\r\nstruct rds_ib_recv_work *recv;\r\nu32 i;\r\nfor (i = 0, recv = ic->i_recvs; i < ic->i_recv_ring.w_nr; i++, recv++) {\r\nstruct ib_sge *sge;\r\nrecv->r_ibinc = NULL;\r\nrecv->r_frag = NULL;\r\nrecv->r_wr.next = NULL;\r\nrecv->r_wr.wr_id = i;\r\nrecv->r_wr.sg_list = recv->r_sge;\r\nrecv->r_wr.num_sge = RDS_IB_RECV_SGE;\r\nsge = &recv->r_sge[0];\r\nsge->addr = ic->i_recv_hdrs_dma + (i * sizeof(struct rds_header));\r\nsge->length = sizeof(struct rds_header);\r\nsge->lkey = ic->i_mr->lkey;\r\nsge = &recv->r_sge[1];\r\nsge->addr = 0;\r\nsge->length = RDS_FRAG_SIZE;\r\nsge->lkey = ic->i_mr->lkey;\r\n}\r\n}\r\nstatic void list_splice_entire_tail(struct list_head *from,\r\nstruct list_head *to)\r\n{\r\nstruct list_head *from_last = from->prev;\r\nlist_splice_tail(from_last, to);\r\nlist_add_tail(from_last, to);\r\n}\r\nstatic void rds_ib_cache_xfer_to_ready(struct rds_ib_refill_cache *cache)\r\n{\r\nstruct list_head *tmp;\r\ntmp = xchg(&cache->xfer, NULL);\r\nif (tmp) {\r\nif (cache->ready)\r\nlist_splice_entire_tail(tmp, cache->ready);\r\nelse\r\ncache->ready = tmp;\r\n}\r\n}\r\nstatic int rds_ib_recv_alloc_cache(struct rds_ib_refill_cache *cache)\r\n{\r\nstruct rds_ib_cache_head *head;\r\nint cpu;\r\ncache->percpu = alloc_percpu(struct rds_ib_cache_head);\r\nif (!cache->percpu)\r\nreturn -ENOMEM;\r\nfor_each_possible_cpu(cpu) {\r\nhead = per_cpu_ptr(cache->percpu, cpu);\r\nhead->first = NULL;\r\nhead->count = 0;\r\n}\r\ncache->xfer = NULL;\r\ncache->ready = NULL;\r\nreturn 0;\r\n}\r\nint rds_ib_recv_alloc_caches(struct rds_ib_connection *ic)\r\n{\r\nint ret;\r\nret = rds_ib_recv_alloc_cache(&ic->i_cache_incs);\r\nif (!ret) {\r\nret = rds_ib_recv_alloc_cache(&ic->i_cache_frags);\r\nif (ret)\r\nfree_percpu(ic->i_cache_incs.percpu);\r\n}\r\nreturn ret;\r\n}\r\nstatic void rds_ib_cache_splice_all_lists(struct rds_ib_refill_cache *cache,\r\nstruct list_head *caller_list)\r\n{\r\nstruct rds_ib_cache_head *head;\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nhead = per_cpu_ptr(cache->percpu, cpu);\r\nif (head->first) {\r\nlist_splice_entire_tail(head->first, caller_list);\r\nhead->first = NULL;\r\n}\r\n}\r\nif (cache->ready) {\r\nlist_splice_entire_tail(cache->ready, caller_list);\r\ncache->ready = NULL;\r\n}\r\n}\r\nvoid rds_ib_recv_free_caches(struct rds_ib_connection *ic)\r\n{\r\nstruct rds_ib_incoming *inc;\r\nstruct rds_ib_incoming *inc_tmp;\r\nstruct rds_page_frag *frag;\r\nstruct rds_page_frag *frag_tmp;\r\nLIST_HEAD(list);\r\nrds_ib_cache_xfer_to_ready(&ic->i_cache_incs);\r\nrds_ib_cache_splice_all_lists(&ic->i_cache_incs, &list);\r\nfree_percpu(ic->i_cache_incs.percpu);\r\nlist_for_each_entry_safe(inc, inc_tmp, &list, ii_cache_entry) {\r\nlist_del(&inc->ii_cache_entry);\r\nWARN_ON(!list_empty(&inc->ii_frags));\r\nkmem_cache_free(rds_ib_incoming_slab, inc);\r\n}\r\nrds_ib_cache_xfer_to_ready(&ic->i_cache_frags);\r\nrds_ib_cache_splice_all_lists(&ic->i_cache_frags, &list);\r\nfree_percpu(ic->i_cache_frags.percpu);\r\nlist_for_each_entry_safe(frag, frag_tmp, &list, f_cache_entry) {\r\nlist_del(&frag->f_cache_entry);\r\nWARN_ON(!list_empty(&frag->f_item));\r\nkmem_cache_free(rds_ib_frag_slab, frag);\r\n}\r\n}\r\nstatic void rds_ib_frag_free(struct rds_ib_connection *ic,\r\nstruct rds_page_frag *frag)\r\n{\r\nrdsdebug("frag %p page %p\n", frag, sg_page(&frag->f_sg));\r\nrds_ib_recv_cache_put(&frag->f_cache_entry, &ic->i_cache_frags);\r\n}\r\nvoid rds_ib_inc_free(struct rds_incoming *inc)\r\n{\r\nstruct rds_ib_incoming *ibinc;\r\nstruct rds_page_frag *frag;\r\nstruct rds_page_frag *pos;\r\nstruct rds_ib_connection *ic = inc->i_conn->c_transport_data;\r\nibinc = container_of(inc, struct rds_ib_incoming, ii_inc);\r\nlist_for_each_entry_safe(frag, pos, &ibinc->ii_frags, f_item) {\r\nlist_del_init(&frag->f_item);\r\nrds_ib_frag_free(ic, frag);\r\n}\r\nBUG_ON(!list_empty(&ibinc->ii_frags));\r\nrdsdebug("freeing ibinc %p inc %p\n", ibinc, inc);\r\nrds_ib_recv_cache_put(&ibinc->ii_cache_entry, &ic->i_cache_incs);\r\n}\r\nstatic void rds_ib_recv_clear_one(struct rds_ib_connection *ic,\r\nstruct rds_ib_recv_work *recv)\r\n{\r\nif (recv->r_ibinc) {\r\nrds_inc_put(&recv->r_ibinc->ii_inc);\r\nrecv->r_ibinc = NULL;\r\n}\r\nif (recv->r_frag) {\r\nib_dma_unmap_sg(ic->i_cm_id->device, &recv->r_frag->f_sg, 1, DMA_FROM_DEVICE);\r\nrds_ib_frag_free(ic, recv->r_frag);\r\nrecv->r_frag = NULL;\r\n}\r\n}\r\nvoid rds_ib_recv_clear_ring(struct rds_ib_connection *ic)\r\n{\r\nu32 i;\r\nfor (i = 0; i < ic->i_recv_ring.w_nr; i++)\r\nrds_ib_recv_clear_one(ic, &ic->i_recvs[i]);\r\n}\r\nstatic struct rds_ib_incoming *rds_ib_refill_one_inc(struct rds_ib_connection *ic,\r\ngfp_t slab_mask)\r\n{\r\nstruct rds_ib_incoming *ibinc;\r\nstruct list_head *cache_item;\r\nint avail_allocs;\r\ncache_item = rds_ib_recv_cache_get(&ic->i_cache_incs);\r\nif (cache_item) {\r\nibinc = container_of(cache_item, struct rds_ib_incoming, ii_cache_entry);\r\n} else {\r\navail_allocs = atomic_add_unless(&rds_ib_allocation,\r\n1, rds_ib_sysctl_max_recv_allocation);\r\nif (!avail_allocs) {\r\nrds_ib_stats_inc(s_ib_rx_alloc_limit);\r\nreturn NULL;\r\n}\r\nibinc = kmem_cache_alloc(rds_ib_incoming_slab, slab_mask);\r\nif (!ibinc) {\r\natomic_dec(&rds_ib_allocation);\r\nreturn NULL;\r\n}\r\n}\r\nINIT_LIST_HEAD(&ibinc->ii_frags);\r\nrds_inc_init(&ibinc->ii_inc, ic->conn, ic->conn->c_faddr);\r\nreturn ibinc;\r\n}\r\nstatic struct rds_page_frag *rds_ib_refill_one_frag(struct rds_ib_connection *ic,\r\ngfp_t slab_mask, gfp_t page_mask)\r\n{\r\nstruct rds_page_frag *frag;\r\nstruct list_head *cache_item;\r\nint ret;\r\ncache_item = rds_ib_recv_cache_get(&ic->i_cache_frags);\r\nif (cache_item) {\r\nfrag = container_of(cache_item, struct rds_page_frag, f_cache_entry);\r\n} else {\r\nfrag = kmem_cache_alloc(rds_ib_frag_slab, slab_mask);\r\nif (!frag)\r\nreturn NULL;\r\nsg_init_table(&frag->f_sg, 1);\r\nret = rds_page_remainder_alloc(&frag->f_sg,\r\nRDS_FRAG_SIZE, page_mask);\r\nif (ret) {\r\nkmem_cache_free(rds_ib_frag_slab, frag);\r\nreturn NULL;\r\n}\r\n}\r\nINIT_LIST_HEAD(&frag->f_item);\r\nreturn frag;\r\n}\r\nstatic int rds_ib_recv_refill_one(struct rds_connection *conn,\r\nstruct rds_ib_recv_work *recv, int prefill)\r\n{\r\nstruct rds_ib_connection *ic = conn->c_transport_data;\r\nstruct ib_sge *sge;\r\nint ret = -ENOMEM;\r\ngfp_t slab_mask = GFP_NOWAIT;\r\ngfp_t page_mask = GFP_NOWAIT;\r\nif (prefill) {\r\nslab_mask = GFP_KERNEL;\r\npage_mask = GFP_HIGHUSER;\r\n}\r\nif (!ic->i_cache_incs.ready)\r\nrds_ib_cache_xfer_to_ready(&ic->i_cache_incs);\r\nif (!ic->i_cache_frags.ready)\r\nrds_ib_cache_xfer_to_ready(&ic->i_cache_frags);\r\nif (!recv->r_ibinc) {\r\nrecv->r_ibinc = rds_ib_refill_one_inc(ic, slab_mask);\r\nif (!recv->r_ibinc)\r\ngoto out;\r\n}\r\nWARN_ON(recv->r_frag);\r\nrecv->r_frag = rds_ib_refill_one_frag(ic, slab_mask, page_mask);\r\nif (!recv->r_frag)\r\ngoto out;\r\nret = ib_dma_map_sg(ic->i_cm_id->device, &recv->r_frag->f_sg,\r\n1, DMA_FROM_DEVICE);\r\nWARN_ON(ret != 1);\r\nsge = &recv->r_sge[0];\r\nsge->addr = ic->i_recv_hdrs_dma + (recv - ic->i_recvs) * sizeof(struct rds_header);\r\nsge->length = sizeof(struct rds_header);\r\nsge = &recv->r_sge[1];\r\nsge->addr = ib_sg_dma_address(ic->i_cm_id->device, &recv->r_frag->f_sg);\r\nsge->length = ib_sg_dma_len(ic->i_cm_id->device, &recv->r_frag->f_sg);\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nvoid rds_ib_recv_refill(struct rds_connection *conn, int prefill)\r\n{\r\nstruct rds_ib_connection *ic = conn->c_transport_data;\r\nstruct rds_ib_recv_work *recv;\r\nstruct ib_recv_wr *failed_wr;\r\nunsigned int posted = 0;\r\nint ret = 0;\r\nu32 pos;\r\nwhile ((prefill || rds_conn_up(conn)) &&\r\nrds_ib_ring_alloc(&ic->i_recv_ring, 1, &pos)) {\r\nif (pos >= ic->i_recv_ring.w_nr) {\r\nprintk(KERN_NOTICE "Argh - ring alloc returned pos=%u\n",\r\npos);\r\nbreak;\r\n}\r\nrecv = &ic->i_recvs[pos];\r\nret = rds_ib_recv_refill_one(conn, recv, prefill);\r\nif (ret) {\r\nbreak;\r\n}\r\nret = ib_post_recv(ic->i_cm_id->qp, &recv->r_wr, &failed_wr);\r\nrdsdebug("recv %p ibinc %p page %p addr %lu ret %d\n", recv,\r\nrecv->r_ibinc, sg_page(&recv->r_frag->f_sg),\r\n(long) ib_sg_dma_address(\r\nic->i_cm_id->device,\r\n&recv->r_frag->f_sg),\r\nret);\r\nif (ret) {\r\nrds_ib_conn_error(conn, "recv post on "\r\n"%pI4 returned %d, disconnecting and "\r\n"reconnecting\n", &conn->c_faddr,\r\nret);\r\nbreak;\r\n}\r\nposted++;\r\n}\r\nif (ic->i_flowctl && posted)\r\nrds_ib_advertise_credits(conn, posted);\r\nif (ret)\r\nrds_ib_ring_unalloc(&ic->i_recv_ring, 1);\r\n}\r\nstatic void rds_ib_recv_cache_put(struct list_head *new_item,\r\nstruct rds_ib_refill_cache *cache)\r\n{\r\nunsigned long flags;\r\nstruct list_head *old, *chpfirst;\r\nlocal_irq_save(flags);\r\nchpfirst = __this_cpu_read(cache->percpu->first);\r\nif (!chpfirst)\r\nINIT_LIST_HEAD(new_item);\r\nelse\r\nlist_add_tail(new_item, chpfirst);\r\n__this_cpu_write(cache->percpu->first, new_item);\r\n__this_cpu_inc(cache->percpu->count);\r\nif (__this_cpu_read(cache->percpu->count) < RDS_IB_RECYCLE_BATCH_COUNT)\r\ngoto end;\r\ndo {\r\nold = xchg(&cache->xfer, NULL);\r\nif (old)\r\nlist_splice_entire_tail(old, chpfirst);\r\nold = cmpxchg(&cache->xfer, NULL, chpfirst);\r\n} while (old);\r\n__this_cpu_write(cache->percpu->first, NULL);\r\n__this_cpu_write(cache->percpu->count, 0);\r\nend:\r\nlocal_irq_restore(flags);\r\n}\r\nstatic struct list_head *rds_ib_recv_cache_get(struct rds_ib_refill_cache *cache)\r\n{\r\nstruct list_head *head = cache->ready;\r\nif (head) {\r\nif (!list_empty(head)) {\r\ncache->ready = head->next;\r\nlist_del_init(head);\r\n} else\r\ncache->ready = NULL;\r\n}\r\nreturn head;\r\n}\r\nint rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *first_iov,\r\nsize_t size)\r\n{\r\nstruct rds_ib_incoming *ibinc;\r\nstruct rds_page_frag *frag;\r\nstruct iovec *iov = first_iov;\r\nunsigned long to_copy;\r\nunsigned long frag_off = 0;\r\nunsigned long iov_off = 0;\r\nint copied = 0;\r\nint ret;\r\nu32 len;\r\nibinc = container_of(inc, struct rds_ib_incoming, ii_inc);\r\nfrag = list_entry(ibinc->ii_frags.next, struct rds_page_frag, f_item);\r\nlen = be32_to_cpu(inc->i_hdr.h_len);\r\nwhile (copied < size && copied < len) {\r\nif (frag_off == RDS_FRAG_SIZE) {\r\nfrag = list_entry(frag->f_item.next,\r\nstruct rds_page_frag, f_item);\r\nfrag_off = 0;\r\n}\r\nwhile (iov_off == iov->iov_len) {\r\niov_off = 0;\r\niov++;\r\n}\r\nto_copy = min(iov->iov_len - iov_off, RDS_FRAG_SIZE - frag_off);\r\nto_copy = min_t(size_t, to_copy, size - copied);\r\nto_copy = min_t(unsigned long, to_copy, len - copied);\r\nrdsdebug("%lu bytes to user [%p, %zu] + %lu from frag "\r\n"[%p, %u] + %lu\n",\r\nto_copy, iov->iov_base, iov->iov_len, iov_off,\r\nsg_page(&frag->f_sg), frag->f_sg.offset, frag_off);\r\nret = rds_page_copy_to_user(sg_page(&frag->f_sg),\r\nfrag->f_sg.offset + frag_off,\r\niov->iov_base + iov_off,\r\nto_copy);\r\nif (ret) {\r\ncopied = ret;\r\nbreak;\r\n}\r\niov_off += to_copy;\r\nfrag_off += to_copy;\r\ncopied += to_copy;\r\n}\r\nreturn copied;\r\n}\r\nvoid rds_ib_recv_init_ack(struct rds_ib_connection *ic)\r\n{\r\nstruct ib_send_wr *wr = &ic->i_ack_wr;\r\nstruct ib_sge *sge = &ic->i_ack_sge;\r\nsge->addr = ic->i_ack_dma;\r\nsge->length = sizeof(struct rds_header);\r\nsge->lkey = ic->i_mr->lkey;\r\nwr->sg_list = sge;\r\nwr->num_sge = 1;\r\nwr->opcode = IB_WR_SEND;\r\nwr->wr_id = RDS_IB_ACK_WR_ID;\r\nwr->send_flags = IB_SEND_SIGNALED | IB_SEND_SOLICITED;\r\n}\r\nstatic void rds_ib_set_ack(struct rds_ib_connection *ic, u64 seq,\r\nint ack_required)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ic->i_ack_lock, flags);\r\nic->i_ack_next = seq;\r\nif (ack_required)\r\nset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nspin_unlock_irqrestore(&ic->i_ack_lock, flags);\r\n}\r\nstatic u64 rds_ib_get_ack(struct rds_ib_connection *ic)\r\n{\r\nunsigned long flags;\r\nu64 seq;\r\nclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nspin_lock_irqsave(&ic->i_ack_lock, flags);\r\nseq = ic->i_ack_next;\r\nspin_unlock_irqrestore(&ic->i_ack_lock, flags);\r\nreturn seq;\r\n}\r\nstatic void rds_ib_set_ack(struct rds_ib_connection *ic, u64 seq,\r\nint ack_required)\r\n{\r\natomic64_set(&ic->i_ack_next, seq);\r\nif (ack_required) {\r\nsmp_mb__before_atomic();\r\nset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\n}\r\n}\r\nstatic u64 rds_ib_get_ack(struct rds_ib_connection *ic)\r\n{\r\nclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nsmp_mb__after_atomic();\r\nreturn atomic64_read(&ic->i_ack_next);\r\n}\r\nstatic void rds_ib_send_ack(struct rds_ib_connection *ic, unsigned int adv_credits)\r\n{\r\nstruct rds_header *hdr = ic->i_ack;\r\nstruct ib_send_wr *failed_wr;\r\nu64 seq;\r\nint ret;\r\nseq = rds_ib_get_ack(ic);\r\nrdsdebug("send_ack: ic %p ack %llu\n", ic, (unsigned long long) seq);\r\nrds_message_populate_header(hdr, 0, 0, 0);\r\nhdr->h_ack = cpu_to_be64(seq);\r\nhdr->h_credit = adv_credits;\r\nrds_message_make_checksum(hdr);\r\nic->i_ack_queued = jiffies;\r\nret = ib_post_send(ic->i_cm_id->qp, &ic->i_ack_wr, &failed_wr);\r\nif (unlikely(ret)) {\r\nclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\r\nset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nrds_ib_stats_inc(s_ib_ack_send_failure);\r\nrds_ib_conn_error(ic->conn, "sending ack failed\n");\r\n} else\r\nrds_ib_stats_inc(s_ib_ack_sent);\r\n}\r\nvoid rds_ib_attempt_ack(struct rds_ib_connection *ic)\r\n{\r\nunsigned int adv_credits;\r\nif (!test_bit(IB_ACK_REQUESTED, &ic->i_ack_flags))\r\nreturn;\r\nif (test_and_set_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags)) {\r\nrds_ib_stats_inc(s_ib_ack_send_delayed);\r\nreturn;\r\n}\r\nif (!rds_ib_send_grab_credits(ic, 1, &adv_credits, 0, RDS_MAX_ADV_CREDIT)) {\r\nrds_ib_stats_inc(s_ib_tx_throttle);\r\nclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\r\nreturn;\r\n}\r\nclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nrds_ib_send_ack(ic, adv_credits);\r\n}\r\nvoid rds_ib_ack_send_complete(struct rds_ib_connection *ic)\r\n{\r\nclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\r\nrds_ib_attempt_ack(ic);\r\n}\r\nu64 rds_ib_piggyb_ack(struct rds_ib_connection *ic)\r\n{\r\nif (test_and_clear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags))\r\nrds_ib_stats_inc(s_ib_ack_send_piggybacked);\r\nreturn rds_ib_get_ack(ic);\r\n}\r\nstatic void rds_ib_cong_recv(struct rds_connection *conn,\r\nstruct rds_ib_incoming *ibinc)\r\n{\r\nstruct rds_cong_map *map;\r\nunsigned int map_off;\r\nunsigned int map_page;\r\nstruct rds_page_frag *frag;\r\nunsigned long frag_off;\r\nunsigned long to_copy;\r\nunsigned long copied;\r\nuint64_t uncongested = 0;\r\nvoid *addr;\r\nif (be32_to_cpu(ibinc->ii_inc.i_hdr.h_len) != RDS_CONG_MAP_BYTES)\r\nreturn;\r\nmap = conn->c_fcong;\r\nmap_page = 0;\r\nmap_off = 0;\r\nfrag = list_entry(ibinc->ii_frags.next, struct rds_page_frag, f_item);\r\nfrag_off = 0;\r\ncopied = 0;\r\nwhile (copied < RDS_CONG_MAP_BYTES) {\r\nuint64_t *src, *dst;\r\nunsigned int k;\r\nto_copy = min(RDS_FRAG_SIZE - frag_off, PAGE_SIZE - map_off);\r\nBUG_ON(to_copy & 7);\r\naddr = kmap_atomic(sg_page(&frag->f_sg));\r\nsrc = addr + frag_off;\r\ndst = (void *)map->m_page_addrs[map_page] + map_off;\r\nfor (k = 0; k < to_copy; k += 8) {\r\nuncongested |= ~(*src) & *dst;\r\n*dst++ = *src++;\r\n}\r\nkunmap_atomic(addr);\r\ncopied += to_copy;\r\nmap_off += to_copy;\r\nif (map_off == PAGE_SIZE) {\r\nmap_off = 0;\r\nmap_page++;\r\n}\r\nfrag_off += to_copy;\r\nif (frag_off == RDS_FRAG_SIZE) {\r\nfrag = list_entry(frag->f_item.next,\r\nstruct rds_page_frag, f_item);\r\nfrag_off = 0;\r\n}\r\n}\r\nuncongested = le64_to_cpu(uncongested);\r\nrds_cong_map_updated(map, uncongested);\r\n}\r\nstatic void rds_ib_process_recv(struct rds_connection *conn,\r\nstruct rds_ib_recv_work *recv, u32 data_len,\r\nstruct rds_ib_ack_state *state)\r\n{\r\nstruct rds_ib_connection *ic = conn->c_transport_data;\r\nstruct rds_ib_incoming *ibinc = ic->i_ibinc;\r\nstruct rds_header *ihdr, *hdr;\r\nrdsdebug("ic %p ibinc %p recv %p byte len %u\n", ic, ibinc, recv,\r\ndata_len);\r\nif (data_len < sizeof(struct rds_header)) {\r\nrds_ib_conn_error(conn, "incoming message "\r\n"from %pI4 didn't include a "\r\n"header, disconnecting and "\r\n"reconnecting\n",\r\n&conn->c_faddr);\r\nreturn;\r\n}\r\ndata_len -= sizeof(struct rds_header);\r\nihdr = &ic->i_recv_hdrs[recv - ic->i_recvs];\r\nif (!rds_message_verify_checksum(ihdr)) {\r\nrds_ib_conn_error(conn, "incoming message "\r\n"from %pI4 has corrupted header - "\r\n"forcing a reconnect\n",\r\n&conn->c_faddr);\r\nrds_stats_inc(s_recv_drop_bad_checksum);\r\nreturn;\r\n}\r\nstate->ack_recv = be64_to_cpu(ihdr->h_ack);\r\nstate->ack_recv_valid = 1;\r\nif (ihdr->h_credit)\r\nrds_ib_send_add_credits(conn, ihdr->h_credit);\r\nif (ihdr->h_sport == 0 && ihdr->h_dport == 0 && data_len == 0) {\r\nrds_ib_stats_inc(s_ib_ack_received);\r\nrds_ib_frag_free(ic, recv->r_frag);\r\nrecv->r_frag = NULL;\r\nreturn;\r\n}\r\nif (!ibinc) {\r\nibinc = recv->r_ibinc;\r\nrecv->r_ibinc = NULL;\r\nic->i_ibinc = ibinc;\r\nhdr = &ibinc->ii_inc.i_hdr;\r\nmemcpy(hdr, ihdr, sizeof(*hdr));\r\nic->i_recv_data_rem = be32_to_cpu(hdr->h_len);\r\nrdsdebug("ic %p ibinc %p rem %u flag 0x%x\n", ic, ibinc,\r\nic->i_recv_data_rem, hdr->h_flags);\r\n} else {\r\nhdr = &ibinc->ii_inc.i_hdr;\r\nif (hdr->h_sequence != ihdr->h_sequence ||\r\nhdr->h_len != ihdr->h_len ||\r\nhdr->h_sport != ihdr->h_sport ||\r\nhdr->h_dport != ihdr->h_dport) {\r\nrds_ib_conn_error(conn,\r\n"fragment header mismatch; forcing reconnect\n");\r\nreturn;\r\n}\r\n}\r\nlist_add_tail(&recv->r_frag->f_item, &ibinc->ii_frags);\r\nrecv->r_frag = NULL;\r\nif (ic->i_recv_data_rem > RDS_FRAG_SIZE)\r\nic->i_recv_data_rem -= RDS_FRAG_SIZE;\r\nelse {\r\nic->i_recv_data_rem = 0;\r\nic->i_ibinc = NULL;\r\nif (ibinc->ii_inc.i_hdr.h_flags == RDS_FLAG_CONG_BITMAP)\r\nrds_ib_cong_recv(conn, ibinc);\r\nelse {\r\nrds_recv_incoming(conn, conn->c_faddr, conn->c_laddr,\r\n&ibinc->ii_inc, GFP_ATOMIC);\r\nstate->ack_next = be64_to_cpu(hdr->h_sequence);\r\nstate->ack_next_valid = 1;\r\n}\r\nif (hdr->h_flags & RDS_FLAG_ACK_REQUIRED) {\r\nrds_stats_inc(s_recv_ack_required);\r\nstate->ack_required = 1;\r\n}\r\nrds_inc_put(&ibinc->ii_inc);\r\n}\r\n}\r\nvoid rds_ib_recv_cq_comp_handler(struct ib_cq *cq, void *context)\r\n{\r\nstruct rds_connection *conn = context;\r\nstruct rds_ib_connection *ic = conn->c_transport_data;\r\nrdsdebug("conn %p cq %p\n", conn, cq);\r\nrds_ib_stats_inc(s_ib_rx_cq_call);\r\ntasklet_schedule(&ic->i_recv_tasklet);\r\n}\r\nstatic inline void rds_poll_cq(struct rds_ib_connection *ic,\r\nstruct rds_ib_ack_state *state)\r\n{\r\nstruct rds_connection *conn = ic->conn;\r\nstruct ib_wc wc;\r\nstruct rds_ib_recv_work *recv;\r\nwhile (ib_poll_cq(ic->i_recv_cq, 1, &wc) > 0) {\r\nrdsdebug("wc wr_id 0x%llx status %u (%s) byte_len %u imm_data %u\n",\r\n(unsigned long long)wc.wr_id, wc.status,\r\nrds_ib_wc_status_str(wc.status), wc.byte_len,\r\nbe32_to_cpu(wc.ex.imm_data));\r\nrds_ib_stats_inc(s_ib_rx_cq_event);\r\nrecv = &ic->i_recvs[rds_ib_ring_oldest(&ic->i_recv_ring)];\r\nib_dma_unmap_sg(ic->i_cm_id->device, &recv->r_frag->f_sg, 1, DMA_FROM_DEVICE);\r\nif (wc.status == IB_WC_SUCCESS) {\r\nrds_ib_process_recv(conn, recv, wc.byte_len, state);\r\n} else {\r\nif (rds_conn_up(conn) || rds_conn_connecting(conn))\r\nrds_ib_conn_error(conn, "recv completion on %pI4 had "\r\n"status %u (%s), disconnecting and "\r\n"reconnecting\n", &conn->c_faddr,\r\nwc.status,\r\nrds_ib_wc_status_str(wc.status));\r\n}\r\nrds_ib_ring_free(&ic->i_recv_ring, 1);\r\n}\r\n}\r\nvoid rds_ib_recv_tasklet_fn(unsigned long data)\r\n{\r\nstruct rds_ib_connection *ic = (struct rds_ib_connection *) data;\r\nstruct rds_connection *conn = ic->conn;\r\nstruct rds_ib_ack_state state = { 0, };\r\nrds_poll_cq(ic, &state);\r\nib_req_notify_cq(ic->i_recv_cq, IB_CQ_SOLICITED);\r\nrds_poll_cq(ic, &state);\r\nif (state.ack_next_valid)\r\nrds_ib_set_ack(ic, state.ack_next, state.ack_required);\r\nif (state.ack_recv_valid && state.ack_recv > ic->i_ack_recv) {\r\nrds_send_drop_acked(conn, state.ack_recv, NULL);\r\nic->i_ack_recv = state.ack_recv;\r\n}\r\nif (rds_conn_up(conn))\r\nrds_ib_attempt_ack(ic);\r\nif (rds_ib_ring_empty(&ic->i_recv_ring))\r\nrds_ib_stats_inc(s_ib_rx_ring_empty);\r\nif (rds_ib_ring_low(&ic->i_recv_ring))\r\nrds_ib_recv_refill(conn, 0);\r\n}\r\nint rds_ib_recv(struct rds_connection *conn)\r\n{\r\nstruct rds_ib_connection *ic = conn->c_transport_data;\r\nint ret = 0;\r\nrdsdebug("conn %p\n", conn);\r\nif (rds_conn_up(conn))\r\nrds_ib_attempt_ack(ic);\r\nreturn ret;\r\n}\r\nint rds_ib_recv_init(void)\r\n{\r\nstruct sysinfo si;\r\nint ret = -ENOMEM;\r\nsi_meminfo(&si);\r\nrds_ib_sysctl_max_recv_allocation = si.totalram / 3 * PAGE_SIZE / RDS_FRAG_SIZE;\r\nrds_ib_incoming_slab = kmem_cache_create("rds_ib_incoming",\r\nsizeof(struct rds_ib_incoming),\r\n0, SLAB_HWCACHE_ALIGN, NULL);\r\nif (!rds_ib_incoming_slab)\r\ngoto out;\r\nrds_ib_frag_slab = kmem_cache_create("rds_ib_frag",\r\nsizeof(struct rds_page_frag),\r\n0, SLAB_HWCACHE_ALIGN, NULL);\r\nif (!rds_ib_frag_slab)\r\nkmem_cache_destroy(rds_ib_incoming_slab);\r\nelse\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nvoid rds_ib_recv_exit(void)\r\n{\r\nkmem_cache_destroy(rds_ib_incoming_slab);\r\nkmem_cache_destroy(rds_ib_frag_slab);\r\n}
