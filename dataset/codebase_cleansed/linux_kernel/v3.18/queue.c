static inline void __cw1200_queue_lock(struct cw1200_queue *queue)\r\n{\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nif (queue->tx_locked_cnt++ == 0) {\r\npr_debug("[TX] Queue %d is locked.\n",\r\nqueue->queue_id);\r\nieee80211_stop_queue(stats->priv->hw, queue->queue_id);\r\n}\r\n}\r\nstatic inline void __cw1200_queue_unlock(struct cw1200_queue *queue)\r\n{\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nBUG_ON(!queue->tx_locked_cnt);\r\nif (--queue->tx_locked_cnt == 0) {\r\npr_debug("[TX] Queue %d is unlocked.\n",\r\nqueue->queue_id);\r\nieee80211_wake_queue(stats->priv->hw, queue->queue_id);\r\n}\r\n}\r\nstatic inline void cw1200_queue_parse_id(u32 packet_id, u8 *queue_generation,\r\nu8 *queue_id, u8 *item_generation,\r\nu8 *item_id)\r\n{\r\n*item_id = (packet_id >> 0) & 0xFF;\r\n*item_generation = (packet_id >> 8) & 0xFF;\r\n*queue_id = (packet_id >> 16) & 0xFF;\r\n*queue_generation = (packet_id >> 24) & 0xFF;\r\n}\r\nstatic inline u32 cw1200_queue_mk_packet_id(u8 queue_generation, u8 queue_id,\r\nu8 item_generation, u8 item_id)\r\n{\r\nreturn ((u32)item_id << 0) |\r\n((u32)item_generation << 8) |\r\n((u32)queue_id << 16) |\r\n((u32)queue_generation << 24);\r\n}\r\nstatic void cw1200_queue_post_gc(struct cw1200_queue_stats *stats,\r\nstruct list_head *gc_list)\r\n{\r\nstruct cw1200_queue_item *item, *tmp;\r\nlist_for_each_entry_safe(item, tmp, gc_list, head) {\r\nlist_del(&item->head);\r\nstats->skb_dtor(stats->priv, item->skb, &item->txpriv);\r\nkfree(item);\r\n}\r\n}\r\nstatic void cw1200_queue_register_post_gc(struct list_head *gc_list,\r\nstruct cw1200_queue_item *item)\r\n{\r\nstruct cw1200_queue_item *gc_item;\r\ngc_item = kmalloc(sizeof(struct cw1200_queue_item),\r\nGFP_ATOMIC);\r\nBUG_ON(!gc_item);\r\nmemcpy(gc_item, item, sizeof(struct cw1200_queue_item));\r\nlist_add_tail(&gc_item->head, gc_list);\r\n}\r\nstatic void __cw1200_queue_gc(struct cw1200_queue *queue,\r\nstruct list_head *head,\r\nbool unlock)\r\n{\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nstruct cw1200_queue_item *item = NULL, *tmp;\r\nbool wakeup_stats = false;\r\nlist_for_each_entry_safe(item, tmp, &queue->queue, head) {\r\nif (jiffies - item->queue_timestamp < queue->ttl)\r\nbreak;\r\n--queue->num_queued;\r\n--queue->link_map_cache[item->txpriv.link_id];\r\nspin_lock_bh(&stats->lock);\r\n--stats->num_queued;\r\nif (!--stats->link_map_cache[item->txpriv.link_id])\r\nwakeup_stats = true;\r\nspin_unlock_bh(&stats->lock);\r\ncw1200_debug_tx_ttl(stats->priv);\r\ncw1200_queue_register_post_gc(head, item);\r\nitem->skb = NULL;\r\nlist_move_tail(&item->head, &queue->free_pool);\r\n}\r\nif (wakeup_stats)\r\nwake_up(&stats->wait_link_id_empty);\r\nif (queue->overfull) {\r\nif (queue->num_queued <= (queue->capacity >> 1)) {\r\nqueue->overfull = false;\r\nif (unlock)\r\n__cw1200_queue_unlock(queue);\r\n} else if (item) {\r\nunsigned long tmo = item->queue_timestamp + queue->ttl;\r\nmod_timer(&queue->gc, tmo);\r\ncw1200_pm_stay_awake(&stats->priv->pm_state,\r\ntmo - jiffies);\r\n}\r\n}\r\n}\r\nstatic void cw1200_queue_gc(unsigned long arg)\r\n{\r\nLIST_HEAD(list);\r\nstruct cw1200_queue *queue =\r\n(struct cw1200_queue *)arg;\r\nspin_lock_bh(&queue->lock);\r\n__cw1200_queue_gc(queue, &list, true);\r\nspin_unlock_bh(&queue->lock);\r\ncw1200_queue_post_gc(queue->stats, &list);\r\n}\r\nint cw1200_queue_stats_init(struct cw1200_queue_stats *stats,\r\nsize_t map_capacity,\r\ncw1200_queue_skb_dtor_t skb_dtor,\r\nstruct cw1200_common *priv)\r\n{\r\nmemset(stats, 0, sizeof(*stats));\r\nstats->map_capacity = map_capacity;\r\nstats->skb_dtor = skb_dtor;\r\nstats->priv = priv;\r\nspin_lock_init(&stats->lock);\r\ninit_waitqueue_head(&stats->wait_link_id_empty);\r\nstats->link_map_cache = kzalloc(sizeof(int) * map_capacity,\r\nGFP_KERNEL);\r\nif (!stats->link_map_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint cw1200_queue_init(struct cw1200_queue *queue,\r\nstruct cw1200_queue_stats *stats,\r\nu8 queue_id,\r\nsize_t capacity,\r\nunsigned long ttl)\r\n{\r\nsize_t i;\r\nmemset(queue, 0, sizeof(*queue));\r\nqueue->stats = stats;\r\nqueue->capacity = capacity;\r\nqueue->queue_id = queue_id;\r\nqueue->ttl = ttl;\r\nINIT_LIST_HEAD(&queue->queue);\r\nINIT_LIST_HEAD(&queue->pending);\r\nINIT_LIST_HEAD(&queue->free_pool);\r\nspin_lock_init(&queue->lock);\r\ninit_timer(&queue->gc);\r\nqueue->gc.data = (unsigned long)queue;\r\nqueue->gc.function = cw1200_queue_gc;\r\nqueue->pool = kzalloc(sizeof(struct cw1200_queue_item) * capacity,\r\nGFP_KERNEL);\r\nif (!queue->pool)\r\nreturn -ENOMEM;\r\nqueue->link_map_cache = kzalloc(sizeof(int) * stats->map_capacity,\r\nGFP_KERNEL);\r\nif (!queue->link_map_cache) {\r\nkfree(queue->pool);\r\nqueue->pool = NULL;\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < capacity; ++i)\r\nlist_add_tail(&queue->pool[i].head, &queue->free_pool);\r\nreturn 0;\r\n}\r\nint cw1200_queue_clear(struct cw1200_queue *queue)\r\n{\r\nint i;\r\nLIST_HEAD(gc_list);\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nstruct cw1200_queue_item *item, *tmp;\r\nspin_lock_bh(&queue->lock);\r\nqueue->generation++;\r\nlist_splice_tail_init(&queue->queue, &queue->pending);\r\nlist_for_each_entry_safe(item, tmp, &queue->pending, head) {\r\nWARN_ON(!item->skb);\r\ncw1200_queue_register_post_gc(&gc_list, item);\r\nitem->skb = NULL;\r\nlist_move_tail(&item->head, &queue->free_pool);\r\n}\r\nqueue->num_queued = 0;\r\nqueue->num_pending = 0;\r\nspin_lock_bh(&stats->lock);\r\nfor (i = 0; i < stats->map_capacity; ++i) {\r\nstats->num_queued -= queue->link_map_cache[i];\r\nstats->link_map_cache[i] -= queue->link_map_cache[i];\r\nqueue->link_map_cache[i] = 0;\r\n}\r\nspin_unlock_bh(&stats->lock);\r\nif (queue->overfull) {\r\nqueue->overfull = false;\r\n__cw1200_queue_unlock(queue);\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nwake_up(&stats->wait_link_id_empty);\r\ncw1200_queue_post_gc(stats, &gc_list);\r\nreturn 0;\r\n}\r\nvoid cw1200_queue_stats_deinit(struct cw1200_queue_stats *stats)\r\n{\r\nkfree(stats->link_map_cache);\r\nstats->link_map_cache = NULL;\r\n}\r\nvoid cw1200_queue_deinit(struct cw1200_queue *queue)\r\n{\r\ncw1200_queue_clear(queue);\r\ndel_timer_sync(&queue->gc);\r\nINIT_LIST_HEAD(&queue->free_pool);\r\nkfree(queue->pool);\r\nkfree(queue->link_map_cache);\r\nqueue->pool = NULL;\r\nqueue->link_map_cache = NULL;\r\nqueue->capacity = 0;\r\n}\r\nsize_t cw1200_queue_get_num_queued(struct cw1200_queue *queue,\r\nu32 link_id_map)\r\n{\r\nsize_t ret;\r\nint i, bit;\r\nsize_t map_capacity = queue->stats->map_capacity;\r\nif (!link_id_map)\r\nreturn 0;\r\nspin_lock_bh(&queue->lock);\r\nif (link_id_map == (u32)-1) {\r\nret = queue->num_queued - queue->num_pending;\r\n} else {\r\nret = 0;\r\nfor (i = 0, bit = 1; i < map_capacity; ++i, bit <<= 1) {\r\nif (link_id_map & bit)\r\nret += queue->link_map_cache[i];\r\n}\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nreturn ret;\r\n}\r\nint cw1200_queue_put(struct cw1200_queue *queue,\r\nstruct sk_buff *skb,\r\nstruct cw1200_txpriv *txpriv)\r\n{\r\nint ret = 0;\r\nLIST_HEAD(gc_list);\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nif (txpriv->link_id >= queue->stats->map_capacity)\r\nreturn -EINVAL;\r\nspin_lock_bh(&queue->lock);\r\nif (!WARN_ON(list_empty(&queue->free_pool))) {\r\nstruct cw1200_queue_item *item = list_first_entry(\r\n&queue->free_pool, struct cw1200_queue_item, head);\r\nBUG_ON(item->skb);\r\nlist_move_tail(&item->head, &queue->queue);\r\nitem->skb = skb;\r\nitem->txpriv = *txpriv;\r\nitem->generation = 0;\r\nitem->packet_id = cw1200_queue_mk_packet_id(queue->generation,\r\nqueue->queue_id,\r\nitem->generation,\r\nitem - queue->pool);\r\nitem->queue_timestamp = jiffies;\r\n++queue->num_queued;\r\n++queue->link_map_cache[txpriv->link_id];\r\nspin_lock_bh(&stats->lock);\r\n++stats->num_queued;\r\n++stats->link_map_cache[txpriv->link_id];\r\nspin_unlock_bh(&stats->lock);\r\nif (queue->overfull == false &&\r\nqueue->num_queued >=\r\n(queue->capacity - (num_present_cpus() - 1))) {\r\nqueue->overfull = true;\r\n__cw1200_queue_lock(queue);\r\nmod_timer(&queue->gc, jiffies);\r\n}\r\n} else {\r\nret = -ENOENT;\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nreturn ret;\r\n}\r\nint cw1200_queue_get(struct cw1200_queue *queue,\r\nu32 link_id_map,\r\nstruct wsm_tx **tx,\r\nstruct ieee80211_tx_info **tx_info,\r\nconst struct cw1200_txpriv **txpriv)\r\n{\r\nint ret = -ENOENT;\r\nstruct cw1200_queue_item *item;\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nbool wakeup_stats = false;\r\nspin_lock_bh(&queue->lock);\r\nlist_for_each_entry(item, &queue->queue, head) {\r\nif (link_id_map & BIT(item->txpriv.link_id)) {\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\nif (!WARN_ON(ret)) {\r\n*tx = (struct wsm_tx *)item->skb->data;\r\n*tx_info = IEEE80211_SKB_CB(item->skb);\r\n*txpriv = &item->txpriv;\r\n(*tx)->packet_id = item->packet_id;\r\nlist_move_tail(&item->head, &queue->pending);\r\n++queue->num_pending;\r\n--queue->link_map_cache[item->txpriv.link_id];\r\nitem->xmit_timestamp = jiffies;\r\nspin_lock_bh(&stats->lock);\r\n--stats->num_queued;\r\nif (!--stats->link_map_cache[item->txpriv.link_id])\r\nwakeup_stats = true;\r\nspin_unlock_bh(&stats->lock);\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nif (wakeup_stats)\r\nwake_up(&stats->wait_link_id_empty);\r\nreturn ret;\r\n}\r\nint cw1200_queue_requeue(struct cw1200_queue *queue, u32 packet_id)\r\n{\r\nint ret = 0;\r\nu8 queue_generation, queue_id, item_generation, item_id;\r\nstruct cw1200_queue_item *item;\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\ncw1200_queue_parse_id(packet_id, &queue_generation, &queue_id,\r\n&item_generation, &item_id);\r\nitem = &queue->pool[item_id];\r\nspin_lock_bh(&queue->lock);\r\nBUG_ON(queue_id != queue->queue_id);\r\nif (queue_generation != queue->generation) {\r\nret = -ENOENT;\r\n} else if (item_id >= (unsigned) queue->capacity) {\r\nWARN_ON(1);\r\nret = -EINVAL;\r\n} else if (item->generation != item_generation) {\r\nWARN_ON(1);\r\nret = -ENOENT;\r\n} else {\r\n--queue->num_pending;\r\n++queue->link_map_cache[item->txpriv.link_id];\r\nspin_lock_bh(&stats->lock);\r\n++stats->num_queued;\r\n++stats->link_map_cache[item->txpriv.link_id];\r\nspin_unlock_bh(&stats->lock);\r\nitem->generation = ++item_generation;\r\nitem->packet_id = cw1200_queue_mk_packet_id(queue_generation,\r\nqueue_id,\r\nitem_generation,\r\nitem_id);\r\nlist_move(&item->head, &queue->queue);\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nreturn ret;\r\n}\r\nint cw1200_queue_requeue_all(struct cw1200_queue *queue)\r\n{\r\nstruct cw1200_queue_item *item, *tmp;\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nspin_lock_bh(&queue->lock);\r\nlist_for_each_entry_safe_reverse(item, tmp, &queue->pending, head) {\r\n--queue->num_pending;\r\n++queue->link_map_cache[item->txpriv.link_id];\r\nspin_lock_bh(&stats->lock);\r\n++stats->num_queued;\r\n++stats->link_map_cache[item->txpriv.link_id];\r\nspin_unlock_bh(&stats->lock);\r\n++item->generation;\r\nitem->packet_id = cw1200_queue_mk_packet_id(queue->generation,\r\nqueue->queue_id,\r\nitem->generation,\r\nitem - queue->pool);\r\nlist_move(&item->head, &queue->queue);\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nreturn 0;\r\n}\r\nint cw1200_queue_remove(struct cw1200_queue *queue, u32 packet_id)\r\n{\r\nint ret = 0;\r\nu8 queue_generation, queue_id, item_generation, item_id;\r\nstruct cw1200_queue_item *item;\r\nstruct cw1200_queue_stats *stats = queue->stats;\r\nstruct sk_buff *gc_skb = NULL;\r\nstruct cw1200_txpriv gc_txpriv;\r\ncw1200_queue_parse_id(packet_id, &queue_generation, &queue_id,\r\n&item_generation, &item_id);\r\nitem = &queue->pool[item_id];\r\nspin_lock_bh(&queue->lock);\r\nBUG_ON(queue_id != queue->queue_id);\r\nif (queue_generation != queue->generation) {\r\nret = -ENOENT;\r\n} else if (item_id >= (unsigned) queue->capacity) {\r\nWARN_ON(1);\r\nret = -EINVAL;\r\n} else if (item->generation != item_generation) {\r\nWARN_ON(1);\r\nret = -ENOENT;\r\n} else {\r\ngc_txpriv = item->txpriv;\r\ngc_skb = item->skb;\r\nitem->skb = NULL;\r\n--queue->num_pending;\r\n--queue->num_queued;\r\n++queue->num_sent;\r\n++item->generation;\r\nlist_move(&item->head, &queue->free_pool);\r\nif (queue->overfull &&\r\n(queue->num_queued <= (queue->capacity >> 1))) {\r\nqueue->overfull = false;\r\n__cw1200_queue_unlock(queue);\r\n}\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nif (gc_skb)\r\nstats->skb_dtor(stats->priv, gc_skb, &gc_txpriv);\r\nreturn ret;\r\n}\r\nint cw1200_queue_get_skb(struct cw1200_queue *queue, u32 packet_id,\r\nstruct sk_buff **skb,\r\nconst struct cw1200_txpriv **txpriv)\r\n{\r\nint ret = 0;\r\nu8 queue_generation, queue_id, item_generation, item_id;\r\nstruct cw1200_queue_item *item;\r\ncw1200_queue_parse_id(packet_id, &queue_generation, &queue_id,\r\n&item_generation, &item_id);\r\nitem = &queue->pool[item_id];\r\nspin_lock_bh(&queue->lock);\r\nBUG_ON(queue_id != queue->queue_id);\r\nif (queue_generation != queue->generation) {\r\nret = -ENOENT;\r\n} else if (item_id >= (unsigned) queue->capacity) {\r\nWARN_ON(1);\r\nret = -EINVAL;\r\n} else if (item->generation != item_generation) {\r\nWARN_ON(1);\r\nret = -ENOENT;\r\n} else {\r\n*skb = item->skb;\r\n*txpriv = &item->txpriv;\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nreturn ret;\r\n}\r\nvoid cw1200_queue_lock(struct cw1200_queue *queue)\r\n{\r\nspin_lock_bh(&queue->lock);\r\n__cw1200_queue_lock(queue);\r\nspin_unlock_bh(&queue->lock);\r\n}\r\nvoid cw1200_queue_unlock(struct cw1200_queue *queue)\r\n{\r\nspin_lock_bh(&queue->lock);\r\n__cw1200_queue_unlock(queue);\r\nspin_unlock_bh(&queue->lock);\r\n}\r\nbool cw1200_queue_get_xmit_timestamp(struct cw1200_queue *queue,\r\nunsigned long *timestamp,\r\nu32 pending_frame_id)\r\n{\r\nstruct cw1200_queue_item *item;\r\nbool ret;\r\nspin_lock_bh(&queue->lock);\r\nret = !list_empty(&queue->pending);\r\nif (ret) {\r\nlist_for_each_entry(item, &queue->pending, head) {\r\nif (item->packet_id != pending_frame_id)\r\nif (time_before(item->xmit_timestamp,\r\n*timestamp))\r\n*timestamp = item->xmit_timestamp;\r\n}\r\n}\r\nspin_unlock_bh(&queue->lock);\r\nreturn ret;\r\n}\r\nbool cw1200_queue_stats_is_empty(struct cw1200_queue_stats *stats,\r\nu32 link_id_map)\r\n{\r\nbool empty = true;\r\nspin_lock_bh(&stats->lock);\r\nif (link_id_map == (u32)-1) {\r\nempty = stats->num_queued == 0;\r\n} else {\r\nint i;\r\nfor (i = 0; i < stats->map_capacity; ++i) {\r\nif (link_id_map & BIT(i)) {\r\nif (stats->link_map_cache[i]) {\r\nempty = false;\r\nbreak;\r\n}\r\n}\r\n}\r\n}\r\nspin_unlock_bh(&stats->lock);\r\nreturn empty;\r\n}
