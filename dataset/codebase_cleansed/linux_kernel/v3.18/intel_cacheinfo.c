static void\r\namd_cpuid4(int leaf, union _cpuid4_leaf_eax *eax,\r\nunion _cpuid4_leaf_ebx *ebx,\r\nunion _cpuid4_leaf_ecx *ecx)\r\n{\r\nunsigned dummy;\r\nunsigned line_size, lines_per_tag, assoc, size_in_kb;\r\nunion l1_cache l1i, l1d;\r\nunion l2_cache l2;\r\nunion l3_cache l3;\r\nunion l1_cache *l1 = &l1d;\r\neax->full = 0;\r\nebx->full = 0;\r\necx->full = 0;\r\ncpuid(0x80000005, &dummy, &dummy, &l1d.val, &l1i.val);\r\ncpuid(0x80000006, &dummy, &dummy, &l2.val, &l3.val);\r\nswitch (leaf) {\r\ncase 1:\r\nl1 = &l1i;\r\ncase 0:\r\nif (!l1->val)\r\nreturn;\r\nassoc = assocs[l1->assoc];\r\nline_size = l1->line_size;\r\nlines_per_tag = l1->lines_per_tag;\r\nsize_in_kb = l1->size_in_kb;\r\nbreak;\r\ncase 2:\r\nif (!l2.val)\r\nreturn;\r\nassoc = assocs[l2.assoc];\r\nline_size = l2.line_size;\r\nlines_per_tag = l2.lines_per_tag;\r\nsize_in_kb = __this_cpu_read(cpu_info.x86_cache_size);\r\nbreak;\r\ncase 3:\r\nif (!l3.val)\r\nreturn;\r\nassoc = assocs[l3.assoc];\r\nline_size = l3.line_size;\r\nlines_per_tag = l3.lines_per_tag;\r\nsize_in_kb = l3.size_encoded * 512;\r\nif (boot_cpu_has(X86_FEATURE_AMD_DCM)) {\r\nsize_in_kb = size_in_kb >> 1;\r\nassoc = assoc >> 1;\r\n}\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\neax->split.is_self_initializing = 1;\r\neax->split.type = types[leaf];\r\neax->split.level = levels[leaf];\r\neax->split.num_threads_sharing = 0;\r\neax->split.num_cores_on_die = __this_cpu_read(cpu_info.x86_max_cores) - 1;\r\nif (assoc == 0xffff)\r\neax->split.is_fully_associative = 1;\r\nebx->split.coherency_line_size = line_size - 1;\r\nebx->split.ways_of_associativity = assoc - 1;\r\nebx->split.physical_line_partition = lines_per_tag - 1;\r\necx->split.number_of_sets = (size_in_kb * 1024) / line_size /\r\n(ebx->split.ways_of_associativity + 1) - 1;\r\n}\r\nstatic void amd_calc_l3_indices(struct amd_northbridge *nb)\r\n{\r\nstruct amd_l3_cache *l3 = &nb->l3_cache;\r\nunsigned int sc0, sc1, sc2, sc3;\r\nu32 val = 0;\r\npci_read_config_dword(nb->misc, 0x1C4, &val);\r\nl3->subcaches[0] = sc0 = !(val & BIT(0));\r\nl3->subcaches[1] = sc1 = !(val & BIT(4));\r\nif (boot_cpu_data.x86 == 0x15) {\r\nl3->subcaches[0] = sc0 += !(val & BIT(1));\r\nl3->subcaches[1] = sc1 += !(val & BIT(5));\r\n}\r\nl3->subcaches[2] = sc2 = !(val & BIT(8)) + !(val & BIT(9));\r\nl3->subcaches[3] = sc3 = !(val & BIT(12)) + !(val & BIT(13));\r\nl3->indices = (max(max3(sc0, sc1, sc2), sc3) << 10) - 1;\r\n}\r\nstatic void amd_init_l3_cache(struct _cpuid4_info_regs *this_leaf, int index)\r\n{\r\nint node;\r\nif (index < 3)\r\nreturn;\r\nnode = amd_get_nb_id(smp_processor_id());\r\nthis_leaf->nb = node_to_amd_nb(node);\r\nif (this_leaf->nb && !this_leaf->nb->l3_cache.indices)\r\namd_calc_l3_indices(this_leaf->nb);\r\n}\r\nint amd_get_l3_disable_slot(struct amd_northbridge *nb, unsigned slot)\r\n{\r\nunsigned int reg = 0;\r\npci_read_config_dword(nb->misc, 0x1BC + slot * 4, &reg);\r\nif (reg & (3UL << 30))\r\nreturn reg & 0xfff;\r\nreturn -1;\r\n}\r\nstatic ssize_t show_cache_disable(struct _cpuid4_info *this_leaf, char *buf,\r\nunsigned int slot)\r\n{\r\nint index;\r\nif (!this_leaf->base.nb || !amd_nb_has_feature(AMD_NB_L3_INDEX_DISABLE))\r\nreturn -EINVAL;\r\nindex = amd_get_l3_disable_slot(this_leaf->base.nb, slot);\r\nif (index >= 0)\r\nreturn sprintf(buf, "%d\n", index);\r\nreturn sprintf(buf, "FREE\n");\r\n}\r\nstatic void amd_l3_disable_index(struct amd_northbridge *nb, int cpu,\r\nunsigned slot, unsigned long idx)\r\n{\r\nint i;\r\nidx |= BIT(30);\r\nfor (i = 0; i < 4; i++) {\r\nu32 reg = idx | (i << 20);\r\nif (!nb->l3_cache.subcaches[i])\r\ncontinue;\r\npci_write_config_dword(nb->misc, 0x1BC + slot * 4, reg);\r\nwbinvd_on_cpu(cpu);\r\nreg |= BIT(31);\r\npci_write_config_dword(nb->misc, 0x1BC + slot * 4, reg);\r\n}\r\n}\r\nint amd_set_l3_disable_slot(struct amd_northbridge *nb, int cpu, unsigned slot,\r\nunsigned long index)\r\n{\r\nint ret = 0;\r\nret = amd_get_l3_disable_slot(nb, slot);\r\nif (ret >= 0)\r\nreturn -EEXIST;\r\nif (index > nb->l3_cache.indices)\r\nreturn -EINVAL;\r\nif (index == amd_get_l3_disable_slot(nb, !slot))\r\nreturn -EEXIST;\r\namd_l3_disable_index(nb, cpu, slot, index);\r\nreturn 0;\r\n}\r\nstatic ssize_t store_cache_disable(struct _cpuid4_info *this_leaf,\r\nconst char *buf, size_t count,\r\nunsigned int slot)\r\n{\r\nunsigned long val = 0;\r\nint cpu, err = 0;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nif (!this_leaf->base.nb || !amd_nb_has_feature(AMD_NB_L3_INDEX_DISABLE))\r\nreturn -EINVAL;\r\ncpu = cpumask_first(to_cpumask(this_leaf->shared_cpu_map));\r\nif (kstrtoul(buf, 10, &val) < 0)\r\nreturn -EINVAL;\r\nerr = amd_set_l3_disable_slot(this_leaf->base.nb, cpu, slot, val);\r\nif (err) {\r\nif (err == -EEXIST)\r\npr_warning("L3 slot %d in use/index already disabled!\n",\r\nslot);\r\nreturn err;\r\n}\r\nreturn count;\r\n}\r\nstatic ssize_t\r\nshow_subcaches(struct _cpuid4_info *this_leaf, char *buf, unsigned int cpu)\r\n{\r\nif (!this_leaf->base.nb || !amd_nb_has_feature(AMD_NB_L3_PARTITIONING))\r\nreturn -EINVAL;\r\nreturn sprintf(buf, "%x\n", amd_get_subcaches(cpu));\r\n}\r\nstatic ssize_t\r\nstore_subcaches(struct _cpuid4_info *this_leaf, const char *buf, size_t count,\r\nunsigned int cpu)\r\n{\r\nunsigned long val;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nif (!this_leaf->base.nb || !amd_nb_has_feature(AMD_NB_L3_PARTITIONING))\r\nreturn -EINVAL;\r\nif (kstrtoul(buf, 16, &val) < 0)\r\nreturn -EINVAL;\r\nif (amd_set_subcaches(cpu, val))\r\nreturn -EINVAL;\r\nreturn count;\r\n}\r\nstatic int\r\ncpuid4_cache_lookup_regs(int index, struct _cpuid4_info_regs *this_leaf)\r\n{\r\nunion _cpuid4_leaf_eax eax;\r\nunion _cpuid4_leaf_ebx ebx;\r\nunion _cpuid4_leaf_ecx ecx;\r\nunsigned edx;\r\nif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {\r\nif (cpu_has_topoext)\r\ncpuid_count(0x8000001d, index, &eax.full,\r\n&ebx.full, &ecx.full, &edx);\r\nelse\r\namd_cpuid4(index, &eax, &ebx, &ecx);\r\namd_init_l3_cache(this_leaf, index);\r\n} else {\r\ncpuid_count(4, index, &eax.full, &ebx.full, &ecx.full, &edx);\r\n}\r\nif (eax.split.type == CACHE_TYPE_NULL)\r\nreturn -EIO;\r\nthis_leaf->eax = eax;\r\nthis_leaf->ebx = ebx;\r\nthis_leaf->ecx = ecx;\r\nthis_leaf->size = (ecx.split.number_of_sets + 1) *\r\n(ebx.split.coherency_line_size + 1) *\r\n(ebx.split.physical_line_partition + 1) *\r\n(ebx.split.ways_of_associativity + 1);\r\nreturn 0;\r\n}\r\nstatic int find_num_cache_leaves(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int eax, ebx, ecx, edx, op;\r\nunion _cpuid4_leaf_eax cache_eax;\r\nint i = -1;\r\nif (c->x86_vendor == X86_VENDOR_AMD)\r\nop = 0x8000001d;\r\nelse\r\nop = 4;\r\ndo {\r\n++i;\r\ncpuid_count(op, i, &eax, &ebx, &ecx, &edx);\r\ncache_eax.full = eax;\r\n} while (cache_eax.split.type != CACHE_TYPE_NULL);\r\nreturn i;\r\n}\r\nvoid init_amd_cacheinfo(struct cpuinfo_x86 *c)\r\n{\r\nif (cpu_has_topoext) {\r\nnum_cache_leaves = find_num_cache_leaves(c);\r\n} else if (c->extended_cpuid_level >= 0x80000006) {\r\nif (cpuid_edx(0x80000006) & 0xf000)\r\nnum_cache_leaves = 4;\r\nelse\r\nnum_cache_leaves = 3;\r\n}\r\n}\r\nunsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int trace = 0, l1i = 0, l1d = 0, l2 = 0, l3 = 0;\r\nunsigned int new_l1d = 0, new_l1i = 0;\r\nunsigned int new_l2 = 0, new_l3 = 0, i;\r\nunsigned int l2_id = 0, l3_id = 0, num_threads_sharing, index_msb;\r\n#ifdef CONFIG_X86_HT\r\nunsigned int cpu = c->cpu_index;\r\n#endif\r\nif (c->cpuid_level > 3) {\r\nstatic int is_initialized;\r\nif (is_initialized == 0) {\r\nnum_cache_leaves = find_num_cache_leaves(c);\r\nis_initialized++;\r\n}\r\nfor (i = 0; i < num_cache_leaves; i++) {\r\nstruct _cpuid4_info_regs this_leaf = {};\r\nint retval;\r\nretval = cpuid4_cache_lookup_regs(i, &this_leaf);\r\nif (retval < 0)\r\ncontinue;\r\nswitch (this_leaf.eax.split.level) {\r\ncase 1:\r\nif (this_leaf.eax.split.type == CACHE_TYPE_DATA)\r\nnew_l1d = this_leaf.size/1024;\r\nelse if (this_leaf.eax.split.type == CACHE_TYPE_INST)\r\nnew_l1i = this_leaf.size/1024;\r\nbreak;\r\ncase 2:\r\nnew_l2 = this_leaf.size/1024;\r\nnum_threads_sharing = 1 + this_leaf.eax.split.num_threads_sharing;\r\nindex_msb = get_count_order(num_threads_sharing);\r\nl2_id = c->apicid & ~((1 << index_msb) - 1);\r\nbreak;\r\ncase 3:\r\nnew_l3 = this_leaf.size/1024;\r\nnum_threads_sharing = 1 + this_leaf.eax.split.num_threads_sharing;\r\nindex_msb = get_count_order(num_threads_sharing);\r\nl3_id = c->apicid & ~((1 << index_msb) - 1);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\n}\r\nif ((num_cache_leaves == 0 || c->x86 == 15) && c->cpuid_level > 1) {\r\nint j, n;\r\nunsigned int regs[4];\r\nunsigned char *dp = (unsigned char *)regs;\r\nint only_trace = 0;\r\nif (num_cache_leaves != 0 && c->x86 == 15)\r\nonly_trace = 1;\r\nn = cpuid_eax(2) & 0xFF;\r\nfor (i = 0 ; i < n ; i++) {\r\ncpuid(2, &regs[0], &regs[1], &regs[2], &regs[3]);\r\nfor (j = 0 ; j < 3 ; j++)\r\nif (regs[j] & (1 << 31))\r\nregs[j] = 0;\r\nfor (j = 1 ; j < 16 ; j++) {\r\nunsigned char des = dp[j];\r\nunsigned char k = 0;\r\nwhile (cache_table[k].descriptor != 0) {\r\nif (cache_table[k].descriptor == des) {\r\nif (only_trace && cache_table[k].cache_type != LVL_TRACE)\r\nbreak;\r\nswitch (cache_table[k].cache_type) {\r\ncase LVL_1_INST:\r\nl1i += cache_table[k].size;\r\nbreak;\r\ncase LVL_1_DATA:\r\nl1d += cache_table[k].size;\r\nbreak;\r\ncase LVL_2:\r\nl2 += cache_table[k].size;\r\nbreak;\r\ncase LVL_3:\r\nl3 += cache_table[k].size;\r\nbreak;\r\ncase LVL_TRACE:\r\ntrace += cache_table[k].size;\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nk++;\r\n}\r\n}\r\n}\r\n}\r\nif (new_l1d)\r\nl1d = new_l1d;\r\nif (new_l1i)\r\nl1i = new_l1i;\r\nif (new_l2) {\r\nl2 = new_l2;\r\n#ifdef CONFIG_X86_HT\r\nper_cpu(cpu_llc_id, cpu) = l2_id;\r\n#endif\r\n}\r\nif (new_l3) {\r\nl3 = new_l3;\r\n#ifdef CONFIG_X86_HT\r\nper_cpu(cpu_llc_id, cpu) = l3_id;\r\n#endif\r\n}\r\n#ifdef CONFIG_X86_HT\r\nif (per_cpu(cpu_llc_id, cpu) == BAD_APICID)\r\nper_cpu(cpu_llc_id, cpu) = c->phys_proc_id;\r\n#endif\r\nc->x86_cache_size = l3 ? l3 : (l2 ? l2 : (l1i+l1d));\r\nreturn l2;\r\n}\r\nstatic int cache_shared_amd_cpu_map_setup(unsigned int cpu, int index)\r\n{\r\nstruct _cpuid4_info *this_leaf;\r\nint i, sibling;\r\nif (cpu_has_topoext) {\r\nunsigned int apicid, nshared, first, last;\r\nif (!per_cpu(ici_cpuid4_info, cpu))\r\nreturn 0;\r\nthis_leaf = CPUID4_INFO_IDX(cpu, index);\r\nnshared = this_leaf->base.eax.split.num_threads_sharing + 1;\r\napicid = cpu_data(cpu).apicid;\r\nfirst = apicid - (apicid % nshared);\r\nlast = first + nshared - 1;\r\nfor_each_online_cpu(i) {\r\napicid = cpu_data(i).apicid;\r\nif ((apicid < first) || (apicid > last))\r\ncontinue;\r\nif (!per_cpu(ici_cpuid4_info, i))\r\ncontinue;\r\nthis_leaf = CPUID4_INFO_IDX(i, index);\r\nfor_each_online_cpu(sibling) {\r\napicid = cpu_data(sibling).apicid;\r\nif ((apicid < first) || (apicid > last))\r\ncontinue;\r\nset_bit(sibling, this_leaf->shared_cpu_map);\r\n}\r\n}\r\n} else if (index == 3) {\r\nfor_each_cpu(i, cpu_llc_shared_mask(cpu)) {\r\nif (!per_cpu(ici_cpuid4_info, i))\r\ncontinue;\r\nthis_leaf = CPUID4_INFO_IDX(i, index);\r\nfor_each_cpu(sibling, cpu_llc_shared_mask(cpu)) {\r\nif (!cpu_online(sibling))\r\ncontinue;\r\nset_bit(sibling, this_leaf->shared_cpu_map);\r\n}\r\n}\r\n} else\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void cache_shared_cpu_map_setup(unsigned int cpu, int index)\r\n{\r\nstruct _cpuid4_info *this_leaf, *sibling_leaf;\r\nunsigned long num_threads_sharing;\r\nint index_msb, i;\r\nstruct cpuinfo_x86 *c = &cpu_data(cpu);\r\nif (c->x86_vendor == X86_VENDOR_AMD) {\r\nif (cache_shared_amd_cpu_map_setup(cpu, index))\r\nreturn;\r\n}\r\nthis_leaf = CPUID4_INFO_IDX(cpu, index);\r\nnum_threads_sharing = 1 + this_leaf->base.eax.split.num_threads_sharing;\r\nif (num_threads_sharing == 1)\r\ncpumask_set_cpu(cpu, to_cpumask(this_leaf->shared_cpu_map));\r\nelse {\r\nindex_msb = get_count_order(num_threads_sharing);\r\nfor_each_online_cpu(i) {\r\nif (cpu_data(i).apicid >> index_msb ==\r\nc->apicid >> index_msb) {\r\ncpumask_set_cpu(i,\r\nto_cpumask(this_leaf->shared_cpu_map));\r\nif (i != cpu && per_cpu(ici_cpuid4_info, i)) {\r\nsibling_leaf =\r\nCPUID4_INFO_IDX(i, index);\r\ncpumask_set_cpu(cpu, to_cpumask(\r\nsibling_leaf->shared_cpu_map));\r\n}\r\n}\r\n}\r\n}\r\n}\r\nstatic void cache_remove_shared_cpu_map(unsigned int cpu, int index)\r\n{\r\nstruct _cpuid4_info *this_leaf, *sibling_leaf;\r\nint sibling;\r\nthis_leaf = CPUID4_INFO_IDX(cpu, index);\r\nfor_each_cpu(sibling, to_cpumask(this_leaf->shared_cpu_map)) {\r\nsibling_leaf = CPUID4_INFO_IDX(sibling, index);\r\ncpumask_clear_cpu(cpu,\r\nto_cpumask(sibling_leaf->shared_cpu_map));\r\n}\r\n}\r\nstatic void cache_shared_cpu_map_setup(unsigned int cpu, int index)\r\n{\r\n}\r\nstatic void cache_remove_shared_cpu_map(unsigned int cpu, int index)\r\n{\r\n}\r\nstatic void free_cache_attributes(unsigned int cpu)\r\n{\r\nint i;\r\nfor (i = 0; i < num_cache_leaves; i++)\r\ncache_remove_shared_cpu_map(cpu, i);\r\nkfree(per_cpu(ici_cpuid4_info, cpu));\r\nper_cpu(ici_cpuid4_info, cpu) = NULL;\r\n}\r\nstatic void get_cpu_leaves(void *_retval)\r\n{\r\nint j, *retval = _retval, cpu = smp_processor_id();\r\nfor (j = 0; j < num_cache_leaves; j++) {\r\nstruct _cpuid4_info *this_leaf = CPUID4_INFO_IDX(cpu, j);\r\n*retval = cpuid4_cache_lookup_regs(j, &this_leaf->base);\r\nif (unlikely(*retval < 0)) {\r\nint i;\r\nfor (i = 0; i < j; i++)\r\ncache_remove_shared_cpu_map(cpu, i);\r\nbreak;\r\n}\r\ncache_shared_cpu_map_setup(cpu, j);\r\n}\r\n}\r\nstatic int detect_cache_attributes(unsigned int cpu)\r\n{\r\nint retval;\r\nif (num_cache_leaves == 0)\r\nreturn -ENOENT;\r\nper_cpu(ici_cpuid4_info, cpu) = kzalloc(\r\nsizeof(struct _cpuid4_info) * num_cache_leaves, GFP_KERNEL);\r\nif (per_cpu(ici_cpuid4_info, cpu) == NULL)\r\nreturn -ENOMEM;\r\nsmp_call_function_single(cpu, get_cpu_leaves, &retval, true);\r\nif (retval) {\r\nkfree(per_cpu(ici_cpuid4_info, cpu));\r\nper_cpu(ici_cpuid4_info, cpu) = NULL;\r\n}\r\nreturn retval;\r\n}\r\nstatic ssize_t show_size(struct _cpuid4_info *this_leaf, char *buf,\r\nunsigned int cpu)\r\n{\r\nreturn sprintf(buf, "%luK\n", this_leaf->base.size / 1024);\r\n}\r\nstatic ssize_t show_shared_cpu_map_func(struct _cpuid4_info *this_leaf,\r\nint type, char *buf)\r\n{\r\nptrdiff_t len = PTR_ALIGN(buf + PAGE_SIZE - 1, PAGE_SIZE) - buf;\r\nint n = 0;\r\nif (len > 1) {\r\nconst struct cpumask *mask;\r\nmask = to_cpumask(this_leaf->shared_cpu_map);\r\nn = type ?\r\ncpulist_scnprintf(buf, len-2, mask) :\r\ncpumask_scnprintf(buf, len-2, mask);\r\nbuf[n++] = '\n';\r\nbuf[n] = '\0';\r\n}\r\nreturn n;\r\n}\r\nstatic inline ssize_t show_shared_cpu_map(struct _cpuid4_info *leaf, char *buf,\r\nunsigned int cpu)\r\n{\r\nreturn show_shared_cpu_map_func(leaf, 0, buf);\r\n}\r\nstatic inline ssize_t show_shared_cpu_list(struct _cpuid4_info *leaf, char *buf,\r\nunsigned int cpu)\r\n{\r\nreturn show_shared_cpu_map_func(leaf, 1, buf);\r\n}\r\nstatic ssize_t show_type(struct _cpuid4_info *this_leaf, char *buf,\r\nunsigned int cpu)\r\n{\r\nswitch (this_leaf->base.eax.split.type) {\r\ncase CACHE_TYPE_DATA:\r\nreturn sprintf(buf, "Data\n");\r\ncase CACHE_TYPE_INST:\r\nreturn sprintf(buf, "Instruction\n");\r\ncase CACHE_TYPE_UNIFIED:\r\nreturn sprintf(buf, "Unified\n");\r\ndefault:\r\nreturn sprintf(buf, "Unknown\n");\r\n}\r\n}\r\nstatic struct attribute **amd_l3_attrs(void)\r\n{\r\nstatic struct attribute **attrs;\r\nint n;\r\nif (attrs)\r\nreturn attrs;\r\nn = ARRAY_SIZE(default_attrs);\r\nif (amd_nb_has_feature(AMD_NB_L3_INDEX_DISABLE))\r\nn += 2;\r\nif (amd_nb_has_feature(AMD_NB_L3_PARTITIONING))\r\nn += 1;\r\nattrs = kzalloc(n * sizeof (struct attribute *), GFP_KERNEL);\r\nif (attrs == NULL)\r\nreturn attrs = default_attrs;\r\nfor (n = 0; default_attrs[n]; n++)\r\nattrs[n] = default_attrs[n];\r\nif (amd_nb_has_feature(AMD_NB_L3_INDEX_DISABLE)) {\r\nattrs[n++] = &cache_disable_0.attr;\r\nattrs[n++] = &cache_disable_1.attr;\r\n}\r\nif (amd_nb_has_feature(AMD_NB_L3_PARTITIONING))\r\nattrs[n++] = &subcaches.attr;\r\nreturn attrs;\r\n}\r\nstatic ssize_t show(struct kobject *kobj, struct attribute *attr, char *buf)\r\n{\r\nstruct _cache_attr *fattr = to_attr(attr);\r\nstruct _index_kobject *this_leaf = to_object(kobj);\r\nssize_t ret;\r\nret = fattr->show ?\r\nfattr->show(CPUID4_INFO_IDX(this_leaf->cpu, this_leaf->index),\r\nbuf, this_leaf->cpu) :\r\n0;\r\nreturn ret;\r\n}\r\nstatic ssize_t store(struct kobject *kobj, struct attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct _cache_attr *fattr = to_attr(attr);\r\nstruct _index_kobject *this_leaf = to_object(kobj);\r\nssize_t ret;\r\nret = fattr->store ?\r\nfattr->store(CPUID4_INFO_IDX(this_leaf->cpu, this_leaf->index),\r\nbuf, count, this_leaf->cpu) :\r\n0;\r\nreturn ret;\r\n}\r\nstatic void cpuid4_cache_sysfs_exit(unsigned int cpu)\r\n{\r\nkfree(per_cpu(ici_cache_kobject, cpu));\r\nkfree(per_cpu(ici_index_kobject, cpu));\r\nper_cpu(ici_cache_kobject, cpu) = NULL;\r\nper_cpu(ici_index_kobject, cpu) = NULL;\r\nfree_cache_attributes(cpu);\r\n}\r\nstatic int cpuid4_cache_sysfs_init(unsigned int cpu)\r\n{\r\nint err;\r\nif (num_cache_leaves == 0)\r\nreturn -ENOENT;\r\nerr = detect_cache_attributes(cpu);\r\nif (err)\r\nreturn err;\r\nper_cpu(ici_cache_kobject, cpu) =\r\nkzalloc(sizeof(struct kobject), GFP_KERNEL);\r\nif (unlikely(per_cpu(ici_cache_kobject, cpu) == NULL))\r\ngoto err_out;\r\nper_cpu(ici_index_kobject, cpu) = kzalloc(\r\nsizeof(struct _index_kobject) * num_cache_leaves, GFP_KERNEL);\r\nif (unlikely(per_cpu(ici_index_kobject, cpu) == NULL))\r\ngoto err_out;\r\nreturn 0;\r\nerr_out:\r\ncpuid4_cache_sysfs_exit(cpu);\r\nreturn -ENOMEM;\r\n}\r\nstatic int cache_add_dev(struct device *dev)\r\n{\r\nunsigned int cpu = dev->id;\r\nunsigned long i, j;\r\nstruct _index_kobject *this_object;\r\nstruct _cpuid4_info *this_leaf;\r\nint retval;\r\nretval = cpuid4_cache_sysfs_init(cpu);\r\nif (unlikely(retval < 0))\r\nreturn retval;\r\nretval = kobject_init_and_add(per_cpu(ici_cache_kobject, cpu),\r\n&ktype_percpu_entry,\r\n&dev->kobj, "%s", "cache");\r\nif (retval < 0) {\r\ncpuid4_cache_sysfs_exit(cpu);\r\nreturn retval;\r\n}\r\nfor (i = 0; i < num_cache_leaves; i++) {\r\nthis_object = INDEX_KOBJECT_PTR(cpu, i);\r\nthis_object->cpu = cpu;\r\nthis_object->index = i;\r\nthis_leaf = CPUID4_INFO_IDX(cpu, i);\r\nktype_cache.default_attrs = default_attrs;\r\n#ifdef CONFIG_AMD_NB\r\nif (this_leaf->base.nb)\r\nktype_cache.default_attrs = amd_l3_attrs();\r\n#endif\r\nretval = kobject_init_and_add(&(this_object->kobj),\r\n&ktype_cache,\r\nper_cpu(ici_cache_kobject, cpu),\r\n"index%1lu", i);\r\nif (unlikely(retval)) {\r\nfor (j = 0; j < i; j++)\r\nkobject_put(&(INDEX_KOBJECT_PTR(cpu, j)->kobj));\r\nkobject_put(per_cpu(ici_cache_kobject, cpu));\r\ncpuid4_cache_sysfs_exit(cpu);\r\nreturn retval;\r\n}\r\nkobject_uevent(&(this_object->kobj), KOBJ_ADD);\r\n}\r\ncpumask_set_cpu(cpu, to_cpumask(cache_dev_map));\r\nkobject_uevent(per_cpu(ici_cache_kobject, cpu), KOBJ_ADD);\r\nreturn 0;\r\n}\r\nstatic void cache_remove_dev(struct device *dev)\r\n{\r\nunsigned int cpu = dev->id;\r\nunsigned long i;\r\nif (per_cpu(ici_cpuid4_info, cpu) == NULL)\r\nreturn;\r\nif (!cpumask_test_cpu(cpu, to_cpumask(cache_dev_map)))\r\nreturn;\r\ncpumask_clear_cpu(cpu, to_cpumask(cache_dev_map));\r\nfor (i = 0; i < num_cache_leaves; i++)\r\nkobject_put(&(INDEX_KOBJECT_PTR(cpu, i)->kobj));\r\nkobject_put(per_cpu(ici_cache_kobject, cpu));\r\ncpuid4_cache_sysfs_exit(cpu);\r\n}\r\nstatic int cacheinfo_cpu_callback(struct notifier_block *nfb,\r\nunsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (unsigned long)hcpu;\r\nstruct device *dev;\r\ndev = get_cpu_device(cpu);\r\nswitch (action) {\r\ncase CPU_ONLINE:\r\ncase CPU_ONLINE_FROZEN:\r\ncache_add_dev(dev);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\ncache_remove_dev(dev);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int __init cache_sysfs_init(void)\r\n{\r\nint i, err = 0;\r\nif (num_cache_leaves == 0)\r\nreturn 0;\r\ncpu_notifier_register_begin();\r\nfor_each_online_cpu(i) {\r\nstruct device *dev = get_cpu_device(i);\r\nerr = cache_add_dev(dev);\r\nif (err)\r\ngoto out;\r\n}\r\n__register_hotcpu_notifier(&cacheinfo_cpu_notifier);\r\nout:\r\ncpu_notifier_register_done();\r\nreturn err;\r\n}
