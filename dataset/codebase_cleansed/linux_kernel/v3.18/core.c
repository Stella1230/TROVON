static void qce_unregister_algs(struct qce_device *qce)\r\n{\r\nconst struct qce_algo_ops *ops;\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(qce_ops); i++) {\r\nops = qce_ops[i];\r\nops->unregister_algs(qce);\r\n}\r\n}\r\nstatic int qce_register_algs(struct qce_device *qce)\r\n{\r\nconst struct qce_algo_ops *ops;\r\nint i, ret = -ENODEV;\r\nfor (i = 0; i < ARRAY_SIZE(qce_ops); i++) {\r\nops = qce_ops[i];\r\nret = ops->register_algs(qce);\r\nif (ret)\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int qce_handle_request(struct crypto_async_request *async_req)\r\n{\r\nint ret = -EINVAL, i;\r\nconst struct qce_algo_ops *ops;\r\nu32 type = crypto_tfm_alg_type(async_req->tfm);\r\nfor (i = 0; i < ARRAY_SIZE(qce_ops); i++) {\r\nops = qce_ops[i];\r\nif (type != ops->type)\r\ncontinue;\r\nret = ops->async_req_handle(async_req);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int qce_handle_queue(struct qce_device *qce,\r\nstruct crypto_async_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nunsigned long flags;\r\nint ret = 0, err;\r\nspin_lock_irqsave(&qce->lock, flags);\r\nif (req)\r\nret = crypto_enqueue_request(&qce->queue, req);\r\nif (qce->req) {\r\nspin_unlock_irqrestore(&qce->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&qce->queue);\r\nasync_req = crypto_dequeue_request(&qce->queue);\r\nif (async_req)\r\nqce->req = async_req;\r\nspin_unlock_irqrestore(&qce->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog) {\r\nspin_lock_bh(&qce->lock);\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nspin_unlock_bh(&qce->lock);\r\n}\r\nerr = qce_handle_request(async_req);\r\nif (err) {\r\nqce->result = err;\r\ntasklet_schedule(&qce->done_tasklet);\r\n}\r\nreturn ret;\r\n}\r\nstatic void qce_tasklet_req_done(unsigned long data)\r\n{\r\nstruct qce_device *qce = (struct qce_device *)data;\r\nstruct crypto_async_request *req;\r\nunsigned long flags;\r\nspin_lock_irqsave(&qce->lock, flags);\r\nreq = qce->req;\r\nqce->req = NULL;\r\nspin_unlock_irqrestore(&qce->lock, flags);\r\nif (req)\r\nreq->complete(req, qce->result);\r\nqce_handle_queue(qce, NULL);\r\n}\r\nstatic int qce_async_request_enqueue(struct qce_device *qce,\r\nstruct crypto_async_request *req)\r\n{\r\nreturn qce_handle_queue(qce, req);\r\n}\r\nstatic void qce_async_request_done(struct qce_device *qce, int ret)\r\n{\r\nqce->result = ret;\r\ntasklet_schedule(&qce->done_tasklet);\r\n}\r\nstatic int qce_check_version(struct qce_device *qce)\r\n{\r\nu32 major, minor, step;\r\nqce_get_version(qce, &major, &minor, &step);\r\nif (major != QCE_MAJOR_VERSION5 || minor == 0)\r\nreturn -ENODEV;\r\nqce->burst_size = QCE_BAM_BURST_SIZE;\r\nqce->pipe_pair_id = 1;\r\ndev_dbg(qce->dev, "Crypto device found, version %d.%d.%d\n",\r\nmajor, minor, step);\r\nreturn 0;\r\n}\r\nstatic int qce_crypto_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct qce_device *qce;\r\nstruct resource *res;\r\nint ret;\r\nqce = devm_kzalloc(dev, sizeof(*qce), GFP_KERNEL);\r\nif (!qce)\r\nreturn -ENOMEM;\r\nqce->dev = dev;\r\nplatform_set_drvdata(pdev, qce);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nqce->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(qce->base))\r\nreturn PTR_ERR(qce->base);\r\nret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));\r\nif (ret < 0)\r\nreturn ret;\r\nqce->core = devm_clk_get(qce->dev, "core");\r\nif (IS_ERR(qce->core))\r\nreturn PTR_ERR(qce->core);\r\nqce->iface = devm_clk_get(qce->dev, "iface");\r\nif (IS_ERR(qce->iface))\r\nreturn PTR_ERR(qce->iface);\r\nqce->bus = devm_clk_get(qce->dev, "bus");\r\nif (IS_ERR(qce->bus))\r\nreturn PTR_ERR(qce->bus);\r\nret = clk_prepare_enable(qce->core);\r\nif (ret)\r\nreturn ret;\r\nret = clk_prepare_enable(qce->iface);\r\nif (ret)\r\ngoto err_clks_core;\r\nret = clk_prepare_enable(qce->bus);\r\nif (ret)\r\ngoto err_clks_iface;\r\nret = qce_dma_request(qce->dev, &qce->dma);\r\nif (ret)\r\ngoto err_clks;\r\nret = qce_check_version(qce);\r\nif (ret)\r\ngoto err_clks;\r\nspin_lock_init(&qce->lock);\r\ntasklet_init(&qce->done_tasklet, qce_tasklet_req_done,\r\n(unsigned long)qce);\r\ncrypto_init_queue(&qce->queue, QCE_QUEUE_LENGTH);\r\nqce->async_req_enqueue = qce_async_request_enqueue;\r\nqce->async_req_done = qce_async_request_done;\r\nret = qce_register_algs(qce);\r\nif (ret)\r\ngoto err_dma;\r\nreturn 0;\r\nerr_dma:\r\nqce_dma_release(&qce->dma);\r\nerr_clks:\r\nclk_disable_unprepare(qce->bus);\r\nerr_clks_iface:\r\nclk_disable_unprepare(qce->iface);\r\nerr_clks_core:\r\nclk_disable_unprepare(qce->core);\r\nreturn ret;\r\n}\r\nstatic int qce_crypto_remove(struct platform_device *pdev)\r\n{\r\nstruct qce_device *qce = platform_get_drvdata(pdev);\r\ntasklet_kill(&qce->done_tasklet);\r\nqce_unregister_algs(qce);\r\nqce_dma_release(&qce->dma);\r\nclk_disable_unprepare(qce->bus);\r\nclk_disable_unprepare(qce->iface);\r\nclk_disable_unprepare(qce->core);\r\nreturn 0;\r\n}
