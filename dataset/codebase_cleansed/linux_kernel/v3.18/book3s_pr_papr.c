static unsigned long get_pteg_addr(struct kvm_vcpu *vcpu, long pte_index)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s = to_book3s(vcpu);\r\nunsigned long pteg_addr;\r\npte_index <<= 4;\r\npte_index &= ((1 << ((vcpu_book3s->sdr1 & 0x1f) + 11)) - 1) << 7 | 0x70;\r\npteg_addr = vcpu_book3s->sdr1 & 0xfffffffffffc0000ULL;\r\npteg_addr |= pte_index;\r\nreturn pteg_addr;\r\n}\r\nstatic int kvmppc_h_pr_enter(struct kvm_vcpu *vcpu)\r\n{\r\nlong flags = kvmppc_get_gpr(vcpu, 4);\r\nlong pte_index = kvmppc_get_gpr(vcpu, 5);\r\n__be64 pteg[2 * 8];\r\n__be64 *hpte;\r\nunsigned long pteg_addr, i;\r\nlong int ret;\r\ni = pte_index & 7;\r\npte_index &= ~7UL;\r\npteg_addr = get_pteg_addr(vcpu, pte_index);\r\nmutex_lock(&vcpu->kvm->arch.hpt_mutex);\r\ncopy_from_user(pteg, (void __user *)pteg_addr, sizeof(pteg));\r\nhpte = pteg;\r\nret = H_PTEG_FULL;\r\nif (likely((flags & H_EXACT) == 0)) {\r\nfor (i = 0; ; ++i) {\r\nif (i == 8)\r\ngoto done;\r\nif ((be64_to_cpu(*hpte) & HPTE_V_VALID) == 0)\r\nbreak;\r\nhpte += 2;\r\n}\r\n} else {\r\nhpte += i * 2;\r\nif (*hpte & HPTE_V_VALID)\r\ngoto done;\r\n}\r\nhpte[0] = cpu_to_be64(kvmppc_get_gpr(vcpu, 6));\r\nhpte[1] = cpu_to_be64(kvmppc_get_gpr(vcpu, 7));\r\npteg_addr += i * HPTE_SIZE;\r\ncopy_to_user((void __user *)pteg_addr, hpte, HPTE_SIZE);\r\nkvmppc_set_gpr(vcpu, 4, pte_index | i);\r\nret = H_SUCCESS;\r\ndone:\r\nmutex_unlock(&vcpu->kvm->arch.hpt_mutex);\r\nkvmppc_set_gpr(vcpu, 3, ret);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_h_pr_remove(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long flags= kvmppc_get_gpr(vcpu, 4);\r\nunsigned long pte_index = kvmppc_get_gpr(vcpu, 5);\r\nunsigned long avpn = kvmppc_get_gpr(vcpu, 6);\r\nunsigned long v = 0, pteg, rb;\r\nunsigned long pte[2];\r\nlong int ret;\r\npteg = get_pteg_addr(vcpu, pte_index);\r\nmutex_lock(&vcpu->kvm->arch.hpt_mutex);\r\ncopy_from_user(pte, (void __user *)pteg, sizeof(pte));\r\npte[0] = be64_to_cpu((__force __be64)pte[0]);\r\npte[1] = be64_to_cpu((__force __be64)pte[1]);\r\nret = H_NOT_FOUND;\r\nif ((pte[0] & HPTE_V_VALID) == 0 ||\r\n((flags & H_AVPN) && (pte[0] & ~0x7fUL) != avpn) ||\r\n((flags & H_ANDCOND) && (pte[0] & avpn) != 0))\r\ngoto done;\r\ncopy_to_user((void __user *)pteg, &v, sizeof(v));\r\nrb = compute_tlbie_rb(pte[0], pte[1], pte_index);\r\nvcpu->arch.mmu.tlbie(vcpu, rb, rb & 1 ? true : false);\r\nret = H_SUCCESS;\r\nkvmppc_set_gpr(vcpu, 4, pte[0]);\r\nkvmppc_set_gpr(vcpu, 5, pte[1]);\r\ndone:\r\nmutex_unlock(&vcpu->kvm->arch.hpt_mutex);\r\nkvmppc_set_gpr(vcpu, 3, ret);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_h_pr_bulk_remove(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nint paramnr = 4;\r\nint ret = H_SUCCESS;\r\nmutex_lock(&vcpu->kvm->arch.hpt_mutex);\r\nfor (i = 0; i < H_BULK_REMOVE_MAX_BATCH; i++) {\r\nunsigned long tsh = kvmppc_get_gpr(vcpu, paramnr+(2*i));\r\nunsigned long tsl = kvmppc_get_gpr(vcpu, paramnr+(2*i)+1);\r\nunsigned long pteg, rb, flags;\r\nunsigned long pte[2];\r\nunsigned long v = 0;\r\nif ((tsh & H_BULK_REMOVE_TYPE) == H_BULK_REMOVE_END) {\r\nbreak;\r\n} else if ((tsh & H_BULK_REMOVE_TYPE) !=\r\nH_BULK_REMOVE_REQUEST) {\r\nret = H_PARAMETER;\r\nbreak;\r\n}\r\ntsh &= H_BULK_REMOVE_PTEX | H_BULK_REMOVE_FLAGS;\r\ntsh |= H_BULK_REMOVE_RESPONSE;\r\nif ((tsh & H_BULK_REMOVE_ANDCOND) &&\r\n(tsh & H_BULK_REMOVE_AVPN)) {\r\ntsh |= H_BULK_REMOVE_PARM;\r\nkvmppc_set_gpr(vcpu, paramnr+(2*i), tsh);\r\nret = H_PARAMETER;\r\nbreak;\r\n}\r\npteg = get_pteg_addr(vcpu, tsh & H_BULK_REMOVE_PTEX);\r\ncopy_from_user(pte, (void __user *)pteg, sizeof(pte));\r\npte[0] = be64_to_cpu((__force __be64)pte[0]);\r\npte[1] = be64_to_cpu((__force __be64)pte[1]);\r\nflags = (tsh & H_BULK_REMOVE_FLAGS) >> 26;\r\nif ((pte[0] & HPTE_V_VALID) == 0 ||\r\n((flags & H_AVPN) && (pte[0] & ~0x7fUL) != tsl) ||\r\n((flags & H_ANDCOND) && (pte[0] & tsl) != 0)) {\r\ntsh |= H_BULK_REMOVE_NOT_FOUND;\r\n} else {\r\ncopy_to_user((void __user *)pteg, &v, sizeof(v));\r\nrb = compute_tlbie_rb(pte[0], pte[1],\r\ntsh & H_BULK_REMOVE_PTEX);\r\nvcpu->arch.mmu.tlbie(vcpu, rb, rb & 1 ? true : false);\r\ntsh |= H_BULK_REMOVE_SUCCESS;\r\ntsh |= (pte[1] & (HPTE_R_C | HPTE_R_R)) << 43;\r\n}\r\nkvmppc_set_gpr(vcpu, paramnr+(2*i), tsh);\r\n}\r\nmutex_unlock(&vcpu->kvm->arch.hpt_mutex);\r\nkvmppc_set_gpr(vcpu, 3, ret);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_h_pr_protect(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long flags = kvmppc_get_gpr(vcpu, 4);\r\nunsigned long pte_index = kvmppc_get_gpr(vcpu, 5);\r\nunsigned long avpn = kvmppc_get_gpr(vcpu, 6);\r\nunsigned long rb, pteg, r, v;\r\nunsigned long pte[2];\r\nlong int ret;\r\npteg = get_pteg_addr(vcpu, pte_index);\r\nmutex_lock(&vcpu->kvm->arch.hpt_mutex);\r\ncopy_from_user(pte, (void __user *)pteg, sizeof(pte));\r\npte[0] = be64_to_cpu((__force __be64)pte[0]);\r\npte[1] = be64_to_cpu((__force __be64)pte[1]);\r\nret = H_NOT_FOUND;\r\nif ((pte[0] & HPTE_V_VALID) == 0 ||\r\n((flags & H_AVPN) && (pte[0] & ~0x7fUL) != avpn))\r\ngoto done;\r\nv = pte[0];\r\nr = pte[1];\r\nr &= ~(HPTE_R_PP0 | HPTE_R_PP | HPTE_R_N | HPTE_R_KEY_HI |\r\nHPTE_R_KEY_LO);\r\nr |= (flags << 55) & HPTE_R_PP0;\r\nr |= (flags << 48) & HPTE_R_KEY_HI;\r\nr |= flags & (HPTE_R_PP | HPTE_R_N | HPTE_R_KEY_LO);\r\npte[1] = r;\r\nrb = compute_tlbie_rb(v, r, pte_index);\r\nvcpu->arch.mmu.tlbie(vcpu, rb, rb & 1 ? true : false);\r\npte[0] = (__force u64)cpu_to_be64(pte[0]);\r\npte[1] = (__force u64)cpu_to_be64(pte[1]);\r\ncopy_to_user((void __user *)pteg, pte, sizeof(pte));\r\nret = H_SUCCESS;\r\ndone:\r\nmutex_unlock(&vcpu->kvm->arch.hpt_mutex);\r\nkvmppc_set_gpr(vcpu, 3, ret);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_h_pr_put_tce(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long liobn = kvmppc_get_gpr(vcpu, 4);\r\nunsigned long ioba = kvmppc_get_gpr(vcpu, 5);\r\nunsigned long tce = kvmppc_get_gpr(vcpu, 6);\r\nlong rc;\r\nrc = kvmppc_h_put_tce(vcpu, liobn, ioba, tce);\r\nif (rc == H_TOO_HARD)\r\nreturn EMULATE_FAIL;\r\nkvmppc_set_gpr(vcpu, 3, rc);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_h_pr_xics_hcall(struct kvm_vcpu *vcpu, u32 cmd)\r\n{\r\nlong rc = kvmppc_xics_hcall(vcpu, cmd);\r\nkvmppc_set_gpr(vcpu, 3, rc);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_h_pr(struct kvm_vcpu *vcpu, unsigned long cmd)\r\n{\r\nint rc, idx;\r\nif (cmd <= MAX_HCALL_OPCODE &&\r\n!test_bit(cmd/4, vcpu->kvm->arch.enabled_hcalls))\r\nreturn EMULATE_FAIL;\r\nswitch (cmd) {\r\ncase H_ENTER:\r\nreturn kvmppc_h_pr_enter(vcpu);\r\ncase H_REMOVE:\r\nreturn kvmppc_h_pr_remove(vcpu);\r\ncase H_PROTECT:\r\nreturn kvmppc_h_pr_protect(vcpu);\r\ncase H_BULK_REMOVE:\r\nreturn kvmppc_h_pr_bulk_remove(vcpu);\r\ncase H_PUT_TCE:\r\nreturn kvmppc_h_pr_put_tce(vcpu);\r\ncase H_CEDE:\r\nkvmppc_set_msr_fast(vcpu, kvmppc_get_msr(vcpu) | MSR_EE);\r\nkvm_vcpu_block(vcpu);\r\nclear_bit(KVM_REQ_UNHALT, &vcpu->requests);\r\nvcpu->stat.halt_wakeup++;\r\nreturn EMULATE_DONE;\r\ncase H_XIRR:\r\ncase H_CPPR:\r\ncase H_EOI:\r\ncase H_IPI:\r\ncase H_IPOLL:\r\ncase H_XIRR_X:\r\nif (kvmppc_xics_enabled(vcpu))\r\nreturn kvmppc_h_pr_xics_hcall(vcpu, cmd);\r\nbreak;\r\ncase H_RTAS:\r\nif (list_empty(&vcpu->kvm->arch.rtas_tokens))\r\nbreak;\r\nidx = srcu_read_lock(&vcpu->kvm->srcu);\r\nrc = kvmppc_rtas_hcall(vcpu);\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nif (rc)\r\nbreak;\r\nkvmppc_set_gpr(vcpu, 3, 0);\r\nreturn EMULATE_DONE;\r\n}\r\nreturn EMULATE_FAIL;\r\n}\r\nint kvmppc_hcall_impl_pr(unsigned long cmd)\r\n{\r\nswitch (cmd) {\r\ncase H_ENTER:\r\ncase H_REMOVE:\r\ncase H_PROTECT:\r\ncase H_BULK_REMOVE:\r\ncase H_PUT_TCE:\r\ncase H_CEDE:\r\n#ifdef CONFIG_KVM_XICS\r\ncase H_XIRR:\r\ncase H_CPPR:\r\ncase H_EOI:\r\ncase H_IPI:\r\ncase H_IPOLL:\r\ncase H_XIRR_X:\r\n#endif\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid kvmppc_pr_init_default_hcalls(struct kvm *kvm)\r\n{\r\nint i;\r\nunsigned int hcall;\r\nfor (i = 0; default_hcall_list[i]; ++i) {\r\nhcall = default_hcall_list[i];\r\nWARN_ON(!kvmppc_hcall_impl_pr(hcall));\r\n__set_bit(hcall / 4, kvm->arch.enabled_hcalls);\r\n}\r\n}
