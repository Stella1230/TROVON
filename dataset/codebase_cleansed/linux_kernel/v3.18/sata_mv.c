static inline void writelfl(unsigned long data, void __iomem *addr)\r\n{\r\nwritel(data, addr);\r\n(void) readl(addr);\r\n}\r\nstatic inline unsigned int mv_hc_from_port(unsigned int port)\r\n{\r\nreturn port >> MV_PORT_HC_SHIFT;\r\n}\r\nstatic inline unsigned int mv_hardport_from_port(unsigned int port)\r\n{\r\nreturn port & MV_PORT_MASK;\r\n}\r\nstatic inline void __iomem *mv_hc_base(void __iomem *base, unsigned int hc)\r\n{\r\nreturn (base + SATAHC0_REG_BASE + (hc * MV_SATAHC_REG_SZ));\r\n}\r\nstatic inline void __iomem *mv_hc_base_from_port(void __iomem *base,\r\nunsigned int port)\r\n{\r\nreturn mv_hc_base(base, mv_hc_from_port(port));\r\n}\r\nstatic inline void __iomem *mv_port_base(void __iomem *base, unsigned int port)\r\n{\r\nreturn mv_hc_base_from_port(base, port) +\r\nMV_SATAHC_ARBTR_REG_SZ +\r\n(mv_hardport_from_port(port) * MV_PORT_REG_SZ);\r\n}\r\nstatic void __iomem *mv5_phy_base(void __iomem *mmio, unsigned int port)\r\n{\r\nvoid __iomem *hc_mmio = mv_hc_base_from_port(mmio, port);\r\nunsigned long ofs = (mv_hardport_from_port(port) + 1) * 0x100UL;\r\nreturn hc_mmio + ofs;\r\n}\r\nstatic inline void __iomem *mv_host_base(struct ata_host *host)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nreturn hpriv->base;\r\n}\r\nstatic inline void __iomem *mv_ap_base(struct ata_port *ap)\r\n{\r\nreturn mv_port_base(mv_host_base(ap->host), ap->port_no);\r\n}\r\nstatic inline int mv_get_hc_count(unsigned long port_flags)\r\n{\r\nreturn ((port_flags & MV_FLAG_DUAL_HC) ? 2 : 1);\r\n}\r\nstatic void mv_save_cached_regs(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nstruct mv_port_priv *pp = ap->private_data;\r\npp->cached.fiscfg = readl(port_mmio + FISCFG);\r\npp->cached.ltmode = readl(port_mmio + LTMODE);\r\npp->cached.haltcond = readl(port_mmio + EDMA_HALTCOND);\r\npp->cached.unknown_rsvd = readl(port_mmio + EDMA_UNKNOWN_RSVD);\r\n}\r\nstatic inline void mv_write_cached_reg(void __iomem *addr, u32 *old, u32 new)\r\n{\r\nif (new != *old) {\r\nunsigned long laddr;\r\n*old = new;\r\nladdr = (long)addr & 0xffff;\r\nif (laddr >= 0x300 && laddr <= 0x33c) {\r\nladdr &= 0x000f;\r\nif (laddr == 0x4 || laddr == 0xc) {\r\nwritelfl(new, addr);\r\nreturn;\r\n}\r\n}\r\nwritel(new, addr);\r\n}\r\n}\r\nstatic void mv_set_edma_ptrs(void __iomem *port_mmio,\r\nstruct mv_host_priv *hpriv,\r\nstruct mv_port_priv *pp)\r\n{\r\nu32 index;\r\npp->req_idx &= MV_MAX_Q_DEPTH_MASK;\r\nindex = pp->req_idx << EDMA_REQ_Q_PTR_SHIFT;\r\nWARN_ON(pp->crqb_dma & 0x3ff);\r\nwritel((pp->crqb_dma >> 16) >> 16, port_mmio + EDMA_REQ_Q_BASE_HI);\r\nwritelfl((pp->crqb_dma & EDMA_REQ_Q_BASE_LO_MASK) | index,\r\nport_mmio + EDMA_REQ_Q_IN_PTR);\r\nwritelfl(index, port_mmio + EDMA_REQ_Q_OUT_PTR);\r\npp->resp_idx &= MV_MAX_Q_DEPTH_MASK;\r\nindex = pp->resp_idx << EDMA_RSP_Q_PTR_SHIFT;\r\nWARN_ON(pp->crpb_dma & 0xff);\r\nwritel((pp->crpb_dma >> 16) >> 16, port_mmio + EDMA_RSP_Q_BASE_HI);\r\nwritelfl(index, port_mmio + EDMA_RSP_Q_IN_PTR);\r\nwritelfl((pp->crpb_dma & EDMA_RSP_Q_BASE_LO_MASK) | index,\r\nport_mmio + EDMA_RSP_Q_OUT_PTR);\r\n}\r\nstatic void mv_write_main_irq_mask(u32 mask, struct mv_host_priv *hpriv)\r\n{\r\nif (mask & (ALL_PORTS_COAL_DONE | PORTS_0_3_COAL_DONE))\r\nmask &= ~DONE_IRQ_0_3;\r\nif (mask & (ALL_PORTS_COAL_DONE | PORTS_4_7_COAL_DONE))\r\nmask &= ~DONE_IRQ_4_7;\r\nwritelfl(mask, hpriv->main_irq_mask_addr);\r\n}\r\nstatic void mv_set_main_irq_mask(struct ata_host *host,\r\nu32 disable_bits, u32 enable_bits)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nu32 old_mask, new_mask;\r\nold_mask = hpriv->main_irq_mask;\r\nnew_mask = (old_mask & ~disable_bits) | enable_bits;\r\nif (new_mask != old_mask) {\r\nhpriv->main_irq_mask = new_mask;\r\nmv_write_main_irq_mask(new_mask, hpriv);\r\n}\r\n}\r\nstatic void mv_enable_port_irqs(struct ata_port *ap,\r\nunsigned int port_bits)\r\n{\r\nunsigned int shift, hardport, port = ap->port_no;\r\nu32 disable_bits, enable_bits;\r\nMV_PORT_TO_SHIFT_AND_HARDPORT(port, shift, hardport);\r\ndisable_bits = (DONE_IRQ | ERR_IRQ) << shift;\r\nenable_bits = port_bits << shift;\r\nmv_set_main_irq_mask(ap->host, disable_bits, enable_bits);\r\n}\r\nstatic void mv_clear_and_enable_port_irqs(struct ata_port *ap,\r\nvoid __iomem *port_mmio,\r\nunsigned int port_irqs)\r\n{\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nint hardport = mv_hardport_from_port(ap->port_no);\r\nvoid __iomem *hc_mmio = mv_hc_base_from_port(\r\nmv_host_base(ap->host), ap->port_no);\r\nu32 hc_irq_cause;\r\nwritelfl(0, port_mmio + EDMA_ERR_IRQ_CAUSE);\r\nhc_irq_cause = ~((DEV_IRQ | DMA_IRQ) << hardport);\r\nwritelfl(hc_irq_cause, hc_mmio + HC_IRQ_CAUSE);\r\nif (IS_GEN_IIE(hpriv))\r\nwritelfl(0, port_mmio + FIS_IRQ_CAUSE);\r\nmv_enable_port_irqs(ap, port_irqs);\r\n}\r\nstatic void mv_set_irq_coalescing(struct ata_host *host,\r\nunsigned int count, unsigned int usecs)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *mmio = hpriv->base, *hc_mmio;\r\nu32 coal_enable = 0;\r\nunsigned long flags;\r\nunsigned int clks, is_dual_hc = hpriv->n_ports > MV_PORTS_PER_HC;\r\nconst u32 coal_disable = PORTS_0_3_COAL_DONE | PORTS_4_7_COAL_DONE |\r\nALL_PORTS_COAL_DONE;\r\nif (!usecs || !count) {\r\nclks = count = 0;\r\n} else {\r\nclks = usecs * COAL_CLOCKS_PER_USEC;\r\nif (clks > MAX_COAL_TIME_THRESHOLD)\r\nclks = MAX_COAL_TIME_THRESHOLD;\r\nif (count > MAX_COAL_IO_COUNT)\r\ncount = MAX_COAL_IO_COUNT;\r\n}\r\nspin_lock_irqsave(&host->lock, flags);\r\nmv_set_main_irq_mask(host, coal_disable, 0);\r\nif (is_dual_hc && !IS_GEN_I(hpriv)) {\r\nwritel(clks, mmio + IRQ_COAL_TIME_THRESHOLD);\r\nwritel(count, mmio + IRQ_COAL_IO_THRESHOLD);\r\nwritel(~ALL_PORTS_COAL_IRQ, mmio + IRQ_COAL_CAUSE);\r\nif (count)\r\ncoal_enable = ALL_PORTS_COAL_DONE;\r\nclks = count = 0;\r\n}\r\nhc_mmio = mv_hc_base_from_port(mmio, 0);\r\nwritel(clks, hc_mmio + HC_IRQ_COAL_TIME_THRESHOLD);\r\nwritel(count, hc_mmio + HC_IRQ_COAL_IO_THRESHOLD);\r\nwritel(~HC_COAL_IRQ, hc_mmio + HC_IRQ_CAUSE);\r\nif (count)\r\ncoal_enable |= PORTS_0_3_COAL_DONE;\r\nif (is_dual_hc) {\r\nhc_mmio = mv_hc_base_from_port(mmio, MV_PORTS_PER_HC);\r\nwritel(clks, hc_mmio + HC_IRQ_COAL_TIME_THRESHOLD);\r\nwritel(count, hc_mmio + HC_IRQ_COAL_IO_THRESHOLD);\r\nwritel(~HC_COAL_IRQ, hc_mmio + HC_IRQ_CAUSE);\r\nif (count)\r\ncoal_enable |= PORTS_4_7_COAL_DONE;\r\n}\r\nmv_set_main_irq_mask(host, 0, coal_enable);\r\nspin_unlock_irqrestore(&host->lock, flags);\r\n}\r\nstatic void mv_start_edma(struct ata_port *ap, void __iomem *port_mmio,\r\nstruct mv_port_priv *pp, u8 protocol)\r\n{\r\nint want_ncq = (protocol == ATA_PROT_NCQ);\r\nif (pp->pp_flags & MV_PP_FLAG_EDMA_EN) {\r\nint using_ncq = ((pp->pp_flags & MV_PP_FLAG_NCQ_EN) != 0);\r\nif (want_ncq != using_ncq)\r\nmv_stop_edma(ap);\r\n}\r\nif (!(pp->pp_flags & MV_PP_FLAG_EDMA_EN)) {\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nmv_edma_cfg(ap, want_ncq, 1);\r\nmv_set_edma_ptrs(port_mmio, hpriv, pp);\r\nmv_clear_and_enable_port_irqs(ap, port_mmio, DONE_IRQ|ERR_IRQ);\r\nwritelfl(EDMA_EN, port_mmio + EDMA_CMD);\r\npp->pp_flags |= MV_PP_FLAG_EDMA_EN;\r\n}\r\n}\r\nstatic void mv_wait_for_edma_empty_idle(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nconst u32 empty_idle = (EDMA_STATUS_CACHE_EMPTY | EDMA_STATUS_IDLE);\r\nconst int per_loop = 5, timeout = (15 * 1000 / per_loop);\r\nint i;\r\nfor (i = 0; i < timeout; ++i) {\r\nu32 edma_stat = readl(port_mmio + EDMA_STATUS);\r\nif ((edma_stat & empty_idle) == empty_idle)\r\nbreak;\r\nudelay(per_loop);\r\n}\r\n}\r\nstatic int mv_stop_edma_engine(void __iomem *port_mmio)\r\n{\r\nint i;\r\nwritelfl(EDMA_DS, port_mmio + EDMA_CMD);\r\nfor (i = 10000; i > 0; i--) {\r\nu32 reg = readl(port_mmio + EDMA_CMD);\r\nif (!(reg & EDMA_EN))\r\nreturn 0;\r\nudelay(10);\r\n}\r\nreturn -EIO;\r\n}\r\nstatic int mv_stop_edma(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nstruct mv_port_priv *pp = ap->private_data;\r\nint err = 0;\r\nif (!(pp->pp_flags & MV_PP_FLAG_EDMA_EN))\r\nreturn 0;\r\npp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\r\nmv_wait_for_edma_empty_idle(ap);\r\nif (mv_stop_edma_engine(port_mmio)) {\r\nata_port_err(ap, "Unable to stop eDMA\n");\r\nerr = -EIO;\r\n}\r\nmv_edma_cfg(ap, 0, 0);\r\nreturn err;\r\n}\r\nstatic void mv_dump_mem(void __iomem *start, unsigned bytes)\r\n{\r\nint b, w;\r\nfor (b = 0; b < bytes; ) {\r\nDPRINTK("%p: ", start + b);\r\nfor (w = 0; b < bytes && w < 4; w++) {\r\nprintk("%08x ", readl(start + b));\r\nb += sizeof(u32);\r\n}\r\nprintk("\n");\r\n}\r\n}\r\nstatic void mv_dump_pci_cfg(struct pci_dev *pdev, unsigned bytes)\r\n{\r\n#ifdef ATA_DEBUG\r\nint b, w;\r\nu32 dw;\r\nfor (b = 0; b < bytes; ) {\r\nDPRINTK("%02x: ", b);\r\nfor (w = 0; b < bytes && w < 4; w++) {\r\n(void) pci_read_config_dword(pdev, b, &dw);\r\nprintk("%08x ", dw);\r\nb += sizeof(u32);\r\n}\r\nprintk("\n");\r\n}\r\n#endif\r\n}\r\nstatic void mv_dump_all_regs(void __iomem *mmio_base, int port,\r\nstruct pci_dev *pdev)\r\n{\r\n#ifdef ATA_DEBUG\r\nvoid __iomem *hc_base = mv_hc_base(mmio_base,\r\nport >> MV_PORT_HC_SHIFT);\r\nvoid __iomem *port_base;\r\nint start_port, num_ports, p, start_hc, num_hcs, hc;\r\nif (0 > port) {\r\nstart_hc = start_port = 0;\r\nnum_ports = 8;\r\nnum_hcs = 2;\r\n} else {\r\nstart_hc = port >> MV_PORT_HC_SHIFT;\r\nstart_port = port;\r\nnum_ports = num_hcs = 1;\r\n}\r\nDPRINTK("All registers for port(s) %u-%u:\n", start_port,\r\nnum_ports > 1 ? num_ports - 1 : start_port);\r\nif (NULL != pdev) {\r\nDPRINTK("PCI config space regs:\n");\r\nmv_dump_pci_cfg(pdev, 0x68);\r\n}\r\nDPRINTK("PCI regs:\n");\r\nmv_dump_mem(mmio_base+0xc00, 0x3c);\r\nmv_dump_mem(mmio_base+0xd00, 0x34);\r\nmv_dump_mem(mmio_base+0xf00, 0x4);\r\nmv_dump_mem(mmio_base+0x1d00, 0x6c);\r\nfor (hc = start_hc; hc < start_hc + num_hcs; hc++) {\r\nhc_base = mv_hc_base(mmio_base, hc);\r\nDPRINTK("HC regs (HC %i):\n", hc);\r\nmv_dump_mem(hc_base, 0x1c);\r\n}\r\nfor (p = start_port; p < start_port + num_ports; p++) {\r\nport_base = mv_port_base(mmio_base, p);\r\nDPRINTK("EDMA regs (port %i):\n", p);\r\nmv_dump_mem(port_base, 0x54);\r\nDPRINTK("SATA regs (port %i):\n", p);\r\nmv_dump_mem(port_base+0x300, 0x60);\r\n}\r\n#endif\r\n}\r\nstatic unsigned int mv_scr_offset(unsigned int sc_reg_in)\r\n{\r\nunsigned int ofs;\r\nswitch (sc_reg_in) {\r\ncase SCR_STATUS:\r\ncase SCR_CONTROL:\r\ncase SCR_ERROR:\r\nofs = SATA_STATUS + (sc_reg_in * sizeof(u32));\r\nbreak;\r\ncase SCR_ACTIVE:\r\nofs = SATA_ACTIVE;\r\nbreak;\r\ndefault:\r\nofs = 0xffffffffU;\r\nbreak;\r\n}\r\nreturn ofs;\r\n}\r\nstatic int mv_scr_read(struct ata_link *link, unsigned int sc_reg_in, u32 *val)\r\n{\r\nunsigned int ofs = mv_scr_offset(sc_reg_in);\r\nif (ofs != 0xffffffffU) {\r\n*val = readl(mv_ap_base(link->ap) + ofs);\r\nreturn 0;\r\n} else\r\nreturn -EINVAL;\r\n}\r\nstatic int mv_scr_write(struct ata_link *link, unsigned int sc_reg_in, u32 val)\r\n{\r\nunsigned int ofs = mv_scr_offset(sc_reg_in);\r\nif (ofs != 0xffffffffU) {\r\nvoid __iomem *addr = mv_ap_base(link->ap) + ofs;\r\nstruct mv_host_priv *hpriv = link->ap->host->private_data;\r\nif (sc_reg_in == SCR_CONTROL) {\r\nif ((val & 0xf) == 1 || (readl(addr) & 0xf) == 1)\r\nval |= 0xf000;\r\nif (hpriv->hp_flags & MV_HP_FIX_LP_PHY_CTL) {\r\nvoid __iomem *lp_phy_addr =\r\nmv_ap_base(link->ap) + LP_PHY_CTL;\r\nif ((val & 0xf0) == 0x10)\r\nwritelfl(0x7, lp_phy_addr);\r\nelse\r\nwritelfl(0x227, lp_phy_addr);\r\n}\r\n}\r\nwritelfl(val, addr);\r\nreturn 0;\r\n} else\r\nreturn -EINVAL;\r\n}\r\nstatic void mv6_dev_config(struct ata_device *adev)\r\n{\r\nif (adev->flags & ATA_DFLAG_NCQ) {\r\nif (sata_pmp_attached(adev->link->ap)) {\r\nadev->flags &= ~ATA_DFLAG_NCQ;\r\nata_dev_info(adev,\r\n"NCQ disabled for command-based switching\n");\r\n}\r\n}\r\n}\r\nstatic int mv_qc_defer(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_link *link = qc->dev->link;\r\nstruct ata_port *ap = link->ap;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nif (pp->pp_flags & MV_PP_FLAG_DELAYED_EH)\r\nreturn ATA_DEFER_PORT;\r\nif (unlikely(ap->excl_link)) {\r\nif (link == ap->excl_link) {\r\nif (ap->nr_active_links)\r\nreturn ATA_DEFER_PORT;\r\nqc->flags |= ATA_QCFLAG_CLEAR_EXCL;\r\nreturn 0;\r\n} else\r\nreturn ATA_DEFER_PORT;\r\n}\r\nif (ap->nr_active_links == 0)\r\nreturn 0;\r\nif ((pp->pp_flags & MV_PP_FLAG_EDMA_EN) &&\r\n(pp->pp_flags & MV_PP_FLAG_NCQ_EN)) {\r\nif (ata_is_ncq(qc->tf.protocol))\r\nreturn 0;\r\nelse {\r\nap->excl_link = link;\r\nreturn ATA_DEFER_PORT;\r\n}\r\n}\r\nreturn ATA_DEFER_PORT;\r\n}\r\nstatic void mv_config_fbs(struct ata_port *ap, int want_ncq, int want_fbs)\r\n{\r\nstruct mv_port_priv *pp = ap->private_data;\r\nvoid __iomem *port_mmio;\r\nu32 fiscfg, *old_fiscfg = &pp->cached.fiscfg;\r\nu32 ltmode, *old_ltmode = &pp->cached.ltmode;\r\nu32 haltcond, *old_haltcond = &pp->cached.haltcond;\r\nltmode = *old_ltmode & ~LTMODE_BIT8;\r\nhaltcond = *old_haltcond | EDMA_ERR_DEV;\r\nif (want_fbs) {\r\nfiscfg = *old_fiscfg | FISCFG_SINGLE_SYNC;\r\nltmode = *old_ltmode | LTMODE_BIT8;\r\nif (want_ncq)\r\nhaltcond &= ~EDMA_ERR_DEV;\r\nelse\r\nfiscfg |= FISCFG_WAIT_DEV_ERR;\r\n} else {\r\nfiscfg = *old_fiscfg & ~(FISCFG_SINGLE_SYNC | FISCFG_WAIT_DEV_ERR);\r\n}\r\nport_mmio = mv_ap_base(ap);\r\nmv_write_cached_reg(port_mmio + FISCFG, old_fiscfg, fiscfg);\r\nmv_write_cached_reg(port_mmio + LTMODE, old_ltmode, ltmode);\r\nmv_write_cached_reg(port_mmio + EDMA_HALTCOND, old_haltcond, haltcond);\r\n}\r\nstatic void mv_60x1_errata_sata25(struct ata_port *ap, int want_ncq)\r\n{\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nu32 old, new;\r\nold = readl(hpriv->base + GPIO_PORT_CTL);\r\nif (want_ncq)\r\nnew = old | (1 << 22);\r\nelse\r\nnew = old & ~(1 << 22);\r\nif (new != old)\r\nwritel(new, hpriv->base + GPIO_PORT_CTL);\r\n}\r\nstatic void mv_bmdma_enable_iie(struct ata_port *ap, int enable_bmdma)\r\n{\r\nstruct mv_port_priv *pp = ap->private_data;\r\nu32 new, *old = &pp->cached.unknown_rsvd;\r\nif (enable_bmdma)\r\nnew = *old | 1;\r\nelse\r\nnew = *old & ~1;\r\nmv_write_cached_reg(mv_ap_base(ap) + EDMA_UNKNOWN_RSVD, old, new);\r\n}\r\nstatic void mv_soc_led_blink_enable(struct ata_port *ap)\r\n{\r\nstruct ata_host *host = ap->host;\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *hc_mmio;\r\nu32 led_ctrl;\r\nif (hpriv->hp_flags & MV_HP_QUIRK_LED_BLINK_EN)\r\nreturn;\r\nhpriv->hp_flags |= MV_HP_QUIRK_LED_BLINK_EN;\r\nhc_mmio = mv_hc_base_from_port(mv_host_base(host), ap->port_no);\r\nled_ctrl = readl(hc_mmio + SOC_LED_CTRL);\r\nwritel(led_ctrl | SOC_LED_CTRL_BLINK, hc_mmio + SOC_LED_CTRL);\r\n}\r\nstatic void mv_soc_led_blink_disable(struct ata_port *ap)\r\n{\r\nstruct ata_host *host = ap->host;\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *hc_mmio;\r\nu32 led_ctrl;\r\nunsigned int port;\r\nif (!(hpriv->hp_flags & MV_HP_QUIRK_LED_BLINK_EN))\r\nreturn;\r\nfor (port = 0; port < hpriv->n_ports; port++) {\r\nstruct ata_port *this_ap = host->ports[port];\r\nstruct mv_port_priv *pp = this_ap->private_data;\r\nif (pp->pp_flags & MV_PP_FLAG_NCQ_EN)\r\nreturn;\r\n}\r\nhpriv->hp_flags &= ~MV_HP_QUIRK_LED_BLINK_EN;\r\nhc_mmio = mv_hc_base_from_port(mv_host_base(host), ap->port_no);\r\nled_ctrl = readl(hc_mmio + SOC_LED_CTRL);\r\nwritel(led_ctrl & ~SOC_LED_CTRL_BLINK, hc_mmio + SOC_LED_CTRL);\r\n}\r\nstatic void mv_edma_cfg(struct ata_port *ap, int want_ncq, int want_edma)\r\n{\r\nu32 cfg;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\ncfg = EDMA_CFG_Q_DEPTH;\r\npp->pp_flags &=\r\n~(MV_PP_FLAG_FBS_EN | MV_PP_FLAG_NCQ_EN | MV_PP_FLAG_FAKE_ATA_BUSY);\r\nif (IS_GEN_I(hpriv))\r\ncfg |= (1 << 8);\r\nelse if (IS_GEN_II(hpriv)) {\r\ncfg |= EDMA_CFG_RD_BRST_EXT | EDMA_CFG_WR_BUFF_LEN;\r\nmv_60x1_errata_sata25(ap, want_ncq);\r\n} else if (IS_GEN_IIE(hpriv)) {\r\nint want_fbs = sata_pmp_attached(ap);\r\nwant_fbs &= want_ncq;\r\nmv_config_fbs(ap, want_ncq, want_fbs);\r\nif (want_fbs) {\r\npp->pp_flags |= MV_PP_FLAG_FBS_EN;\r\ncfg |= EDMA_CFG_EDMA_FBS;\r\n}\r\ncfg |= (1 << 23);\r\nif (want_edma) {\r\ncfg |= (1 << 22);\r\nif (!IS_SOC(hpriv))\r\ncfg |= (1 << 18);\r\n}\r\nif (hpriv->hp_flags & MV_HP_CUT_THROUGH)\r\ncfg |= (1 << 17);\r\nmv_bmdma_enable_iie(ap, !want_edma);\r\nif (IS_SOC(hpriv)) {\r\nif (want_ncq)\r\nmv_soc_led_blink_enable(ap);\r\nelse\r\nmv_soc_led_blink_disable(ap);\r\n}\r\n}\r\nif (want_ncq) {\r\ncfg |= EDMA_CFG_NCQ;\r\npp->pp_flags |= MV_PP_FLAG_NCQ_EN;\r\n}\r\nwritelfl(cfg, port_mmio + EDMA_CFG);\r\n}\r\nstatic void mv_port_free_dma_mem(struct ata_port *ap)\r\n{\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nint tag;\r\nif (pp->crqb) {\r\ndma_pool_free(hpriv->crqb_pool, pp->crqb, pp->crqb_dma);\r\npp->crqb = NULL;\r\n}\r\nif (pp->crpb) {\r\ndma_pool_free(hpriv->crpb_pool, pp->crpb, pp->crpb_dma);\r\npp->crpb = NULL;\r\n}\r\nfor (tag = 0; tag < MV_MAX_Q_DEPTH; ++tag) {\r\nif (pp->sg_tbl[tag]) {\r\nif (tag == 0 || !IS_GEN_I(hpriv))\r\ndma_pool_free(hpriv->sg_tbl_pool,\r\npp->sg_tbl[tag],\r\npp->sg_tbl_dma[tag]);\r\npp->sg_tbl[tag] = NULL;\r\n}\r\n}\r\n}\r\nstatic int mv_port_start(struct ata_port *ap)\r\n{\r\nstruct device *dev = ap->host->dev;\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nstruct mv_port_priv *pp;\r\nunsigned long flags;\r\nint tag;\r\npp = devm_kzalloc(dev, sizeof(*pp), GFP_KERNEL);\r\nif (!pp)\r\nreturn -ENOMEM;\r\nap->private_data = pp;\r\npp->crqb = dma_pool_alloc(hpriv->crqb_pool, GFP_KERNEL, &pp->crqb_dma);\r\nif (!pp->crqb)\r\nreturn -ENOMEM;\r\nmemset(pp->crqb, 0, MV_CRQB_Q_SZ);\r\npp->crpb = dma_pool_alloc(hpriv->crpb_pool, GFP_KERNEL, &pp->crpb_dma);\r\nif (!pp->crpb)\r\ngoto out_port_free_dma_mem;\r\nmemset(pp->crpb, 0, MV_CRPB_Q_SZ);\r\nif (hpriv->hp_flags & MV_HP_ERRATA_60X1C0)\r\nap->flags |= ATA_FLAG_AN;\r\nfor (tag = 0; tag < MV_MAX_Q_DEPTH; ++tag) {\r\nif (tag == 0 || !IS_GEN_I(hpriv)) {\r\npp->sg_tbl[tag] = dma_pool_alloc(hpriv->sg_tbl_pool,\r\nGFP_KERNEL, &pp->sg_tbl_dma[tag]);\r\nif (!pp->sg_tbl[tag])\r\ngoto out_port_free_dma_mem;\r\n} else {\r\npp->sg_tbl[tag] = pp->sg_tbl[0];\r\npp->sg_tbl_dma[tag] = pp->sg_tbl_dma[0];\r\n}\r\n}\r\nspin_lock_irqsave(ap->lock, flags);\r\nmv_save_cached_regs(ap);\r\nmv_edma_cfg(ap, 0, 0);\r\nspin_unlock_irqrestore(ap->lock, flags);\r\nreturn 0;\r\nout_port_free_dma_mem:\r\nmv_port_free_dma_mem(ap);\r\nreturn -ENOMEM;\r\n}\r\nstatic void mv_port_stop(struct ata_port *ap)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(ap->lock, flags);\r\nmv_stop_edma(ap);\r\nmv_enable_port_irqs(ap, 0);\r\nspin_unlock_irqrestore(ap->lock, flags);\r\nmv_port_free_dma_mem(ap);\r\n}\r\nstatic void mv_fill_sg(struct ata_queued_cmd *qc)\r\n{\r\nstruct mv_port_priv *pp = qc->ap->private_data;\r\nstruct scatterlist *sg;\r\nstruct mv_sg *mv_sg, *last_sg = NULL;\r\nunsigned int si;\r\nmv_sg = pp->sg_tbl[qc->tag];\r\nfor_each_sg(qc->sg, sg, qc->n_elem, si) {\r\ndma_addr_t addr = sg_dma_address(sg);\r\nu32 sg_len = sg_dma_len(sg);\r\nwhile (sg_len) {\r\nu32 offset = addr & 0xffff;\r\nu32 len = sg_len;\r\nif (offset + len > 0x10000)\r\nlen = 0x10000 - offset;\r\nmv_sg->addr = cpu_to_le32(addr & 0xffffffff);\r\nmv_sg->addr_hi = cpu_to_le32((addr >> 16) >> 16);\r\nmv_sg->flags_size = cpu_to_le32(len & 0xffff);\r\nmv_sg->reserved = 0;\r\nsg_len -= len;\r\naddr += len;\r\nlast_sg = mv_sg;\r\nmv_sg++;\r\n}\r\n}\r\nif (likely(last_sg))\r\nlast_sg->flags_size |= cpu_to_le32(EPRD_FLAG_END_OF_TBL);\r\nmb();\r\n}\r\nstatic void mv_crqb_pack_cmd(__le16 *cmdw, u8 data, u8 addr, unsigned last)\r\n{\r\nu16 tmp = data | (addr << CRQB_CMD_ADDR_SHIFT) | CRQB_CMD_CS |\r\n(last ? CRQB_CMD_LAST : 0);\r\n*cmdw = cpu_to_le16(tmp);\r\n}\r\nstatic void mv_sff_irq_clear(struct ata_port *ap)\r\n{\r\nmv_clear_and_enable_port_irqs(ap, mv_ap_base(ap), ERR_IRQ);\r\n}\r\nstatic int mv_check_atapi_dma(struct ata_queued_cmd *qc)\r\n{\r\nstruct scsi_cmnd *scmd = qc->scsicmd;\r\nif (scmd) {\r\nswitch (scmd->cmnd[0]) {\r\ncase READ_6:\r\ncase READ_10:\r\ncase READ_12:\r\ncase WRITE_6:\r\ncase WRITE_10:\r\ncase WRITE_12:\r\ncase GPCMD_READ_CD:\r\ncase GPCMD_SEND_DVD_STRUCTURE:\r\ncase GPCMD_SEND_CUE_SHEET:\r\nreturn 0;\r\n}\r\n}\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic void mv_bmdma_setup(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nstruct mv_port_priv *pp = ap->private_data;\r\nmv_fill_sg(qc);\r\nwritel(0, port_mmio + BMDMA_CMD);\r\nwritel((pp->sg_tbl_dma[qc->tag] >> 16) >> 16,\r\nport_mmio + BMDMA_PRD_HIGH);\r\nwritelfl(pp->sg_tbl_dma[qc->tag],\r\nport_mmio + BMDMA_PRD_LOW);\r\nap->ops->sff_exec_command(ap, &qc->tf);\r\n}\r\nstatic void mv_bmdma_start(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nunsigned int rw = (qc->tf.flags & ATA_TFLAG_WRITE);\r\nu32 cmd = (rw ? 0 : ATA_DMA_WR) | ATA_DMA_START;\r\nwritelfl(cmd, port_mmio + BMDMA_CMD);\r\n}\r\nstatic void mv_bmdma_stop_ap(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nu32 cmd;\r\ncmd = readl(port_mmio + BMDMA_CMD);\r\nif (cmd & ATA_DMA_START) {\r\ncmd &= ~ATA_DMA_START;\r\nwritelfl(cmd, port_mmio + BMDMA_CMD);\r\nata_sff_dma_pause(ap);\r\n}\r\n}\r\nstatic void mv_bmdma_stop(struct ata_queued_cmd *qc)\r\n{\r\nmv_bmdma_stop_ap(qc->ap);\r\n}\r\nstatic u8 mv_bmdma_status(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nu32 reg, status;\r\nreg = readl(port_mmio + BMDMA_STATUS);\r\nif (reg & ATA_DMA_ACTIVE)\r\nstatus = ATA_DMA_ACTIVE;\r\nelse if (reg & ATA_DMA_ERR)\r\nstatus = (reg & ATA_DMA_ERR) | ATA_DMA_INTR;\r\nelse {\r\nmv_bmdma_stop_ap(ap);\r\nif (ioread8(ap->ioaddr.altstatus_addr) & ATA_BUSY)\r\nstatus = 0;\r\nelse\r\nstatus = ATA_DMA_INTR;\r\n}\r\nreturn status;\r\n}\r\nstatic void mv_rw_multi_errata_sata24(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_taskfile *tf = &qc->tf;\r\nif ((tf->flags & ATA_TFLAG_WRITE) && is_multi_taskfile(tf)) {\r\nif (qc->dev->multi_count > 7) {\r\nswitch (tf->command) {\r\ncase ATA_CMD_WRITE_MULTI:\r\ntf->command = ATA_CMD_PIO_WRITE;\r\nbreak;\r\ncase ATA_CMD_WRITE_MULTI_FUA_EXT:\r\ntf->flags &= ~ATA_TFLAG_FUA;\r\ncase ATA_CMD_WRITE_MULTI_EXT:\r\ntf->command = ATA_CMD_PIO_WRITE_EXT;\r\nbreak;\r\n}\r\n}\r\n}\r\n}\r\nstatic void mv_qc_prep(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct mv_port_priv *pp = ap->private_data;\r\n__le16 *cw;\r\nstruct ata_taskfile *tf = &qc->tf;\r\nu16 flags = 0;\r\nunsigned in_index;\r\nswitch (tf->protocol) {\r\ncase ATA_PROT_DMA:\r\nif (tf->command == ATA_CMD_DSM)\r\nreturn;\r\ncase ATA_PROT_NCQ:\r\nbreak;\r\ncase ATA_PROT_PIO:\r\nmv_rw_multi_errata_sata24(qc);\r\nreturn;\r\ndefault:\r\nreturn;\r\n}\r\nif (!(tf->flags & ATA_TFLAG_WRITE))\r\nflags |= CRQB_FLAG_READ;\r\nWARN_ON(MV_MAX_Q_DEPTH <= qc->tag);\r\nflags |= qc->tag << CRQB_TAG_SHIFT;\r\nflags |= (qc->dev->link->pmp & 0xf) << CRQB_PMP_SHIFT;\r\nin_index = pp->req_idx;\r\npp->crqb[in_index].sg_addr =\r\ncpu_to_le32(pp->sg_tbl_dma[qc->tag] & 0xffffffff);\r\npp->crqb[in_index].sg_addr_hi =\r\ncpu_to_le32((pp->sg_tbl_dma[qc->tag] >> 16) >> 16);\r\npp->crqb[in_index].ctrl_flags = cpu_to_le16(flags);\r\ncw = &pp->crqb[in_index].ata_cmd[0];\r\nswitch (tf->command) {\r\ncase ATA_CMD_READ:\r\ncase ATA_CMD_READ_EXT:\r\ncase ATA_CMD_WRITE:\r\ncase ATA_CMD_WRITE_EXT:\r\ncase ATA_CMD_WRITE_FUA_EXT:\r\nmv_crqb_pack_cmd(cw++, tf->hob_nsect, ATA_REG_NSECT, 0);\r\nbreak;\r\ncase ATA_CMD_FPDMA_READ:\r\ncase ATA_CMD_FPDMA_WRITE:\r\nmv_crqb_pack_cmd(cw++, tf->hob_feature, ATA_REG_FEATURE, 0);\r\nmv_crqb_pack_cmd(cw++, tf->feature, ATA_REG_FEATURE, 0);\r\nbreak;\r\ndefault:\r\nBUG_ON(tf->command);\r\nbreak;\r\n}\r\nmv_crqb_pack_cmd(cw++, tf->nsect, ATA_REG_NSECT, 0);\r\nmv_crqb_pack_cmd(cw++, tf->hob_lbal, ATA_REG_LBAL, 0);\r\nmv_crqb_pack_cmd(cw++, tf->lbal, ATA_REG_LBAL, 0);\r\nmv_crqb_pack_cmd(cw++, tf->hob_lbam, ATA_REG_LBAM, 0);\r\nmv_crqb_pack_cmd(cw++, tf->lbam, ATA_REG_LBAM, 0);\r\nmv_crqb_pack_cmd(cw++, tf->hob_lbah, ATA_REG_LBAH, 0);\r\nmv_crqb_pack_cmd(cw++, tf->lbah, ATA_REG_LBAH, 0);\r\nmv_crqb_pack_cmd(cw++, tf->device, ATA_REG_DEVICE, 0);\r\nmv_crqb_pack_cmd(cw++, tf->command, ATA_REG_CMD, 1);\r\nif (!(qc->flags & ATA_QCFLAG_DMAMAP))\r\nreturn;\r\nmv_fill_sg(qc);\r\n}\r\nstatic void mv_qc_prep_iie(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nstruct mv_crqb_iie *crqb;\r\nstruct ata_taskfile *tf = &qc->tf;\r\nunsigned in_index;\r\nu32 flags = 0;\r\nif ((tf->protocol != ATA_PROT_DMA) &&\r\n(tf->protocol != ATA_PROT_NCQ))\r\nreturn;\r\nif (tf->command == ATA_CMD_DSM)\r\nreturn;\r\nif (!(tf->flags & ATA_TFLAG_WRITE))\r\nflags |= CRQB_FLAG_READ;\r\nWARN_ON(MV_MAX_Q_DEPTH <= qc->tag);\r\nflags |= qc->tag << CRQB_TAG_SHIFT;\r\nflags |= qc->tag << CRQB_HOSTQ_SHIFT;\r\nflags |= (qc->dev->link->pmp & 0xf) << CRQB_PMP_SHIFT;\r\nin_index = pp->req_idx;\r\ncrqb = (struct mv_crqb_iie *) &pp->crqb[in_index];\r\ncrqb->addr = cpu_to_le32(pp->sg_tbl_dma[qc->tag] & 0xffffffff);\r\ncrqb->addr_hi = cpu_to_le32((pp->sg_tbl_dma[qc->tag] >> 16) >> 16);\r\ncrqb->flags = cpu_to_le32(flags);\r\ncrqb->ata_cmd[0] = cpu_to_le32(\r\n(tf->command << 16) |\r\n(tf->feature << 24)\r\n);\r\ncrqb->ata_cmd[1] = cpu_to_le32(\r\n(tf->lbal << 0) |\r\n(tf->lbam << 8) |\r\n(tf->lbah << 16) |\r\n(tf->device << 24)\r\n);\r\ncrqb->ata_cmd[2] = cpu_to_le32(\r\n(tf->hob_lbal << 0) |\r\n(tf->hob_lbam << 8) |\r\n(tf->hob_lbah << 16) |\r\n(tf->hob_feature << 24)\r\n);\r\ncrqb->ata_cmd[3] = cpu_to_le32(\r\n(tf->nsect << 0) |\r\n(tf->hob_nsect << 8)\r\n);\r\nif (!(qc->flags & ATA_QCFLAG_DMAMAP))\r\nreturn;\r\nmv_fill_sg(qc);\r\n}\r\nstatic u8 mv_sff_check_status(struct ata_port *ap)\r\n{\r\nu8 stat = ioread8(ap->ioaddr.status_addr);\r\nstruct mv_port_priv *pp = ap->private_data;\r\nif (pp->pp_flags & MV_PP_FLAG_FAKE_ATA_BUSY) {\r\nif (stat & (ATA_BUSY | ATA_DRQ | ATA_ERR))\r\npp->pp_flags &= ~MV_PP_FLAG_FAKE_ATA_BUSY;\r\nelse\r\nstat = ATA_BUSY;\r\n}\r\nreturn stat;\r\n}\r\nstatic unsigned int mv_send_fis(struct ata_port *ap, u32 *fis, int nwords)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nu32 ifctl, old_ifctl, ifstat;\r\nint i, timeout = 200, final_word = nwords - 1;\r\nold_ifctl = readl(port_mmio + SATA_IFCTL);\r\nifctl = 0x100 | (old_ifctl & 0xf);\r\nwritelfl(ifctl, port_mmio + SATA_IFCTL);\r\nfor (i = 0; i < final_word; ++i)\r\nwritel(fis[i], port_mmio + VENDOR_UNIQUE_FIS);\r\nwritelfl(ifctl | 0x200, port_mmio + SATA_IFCTL);\r\nwritelfl(fis[final_word], port_mmio + VENDOR_UNIQUE_FIS);\r\ndo {\r\nifstat = readl(port_mmio + SATA_IFSTAT);\r\n} while (!(ifstat & 0x1000) && --timeout);\r\nwritelfl(old_ifctl, port_mmio + SATA_IFCTL);\r\nif ((ifstat & 0x3000) != 0x1000) {\r\nata_port_warn(ap, "%s transmission error, ifstat=%08x\n",\r\n__func__, ifstat);\r\nreturn AC_ERR_OTHER;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned int mv_qc_issue_fis(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nstruct ata_link *link = qc->dev->link;\r\nu32 fis[5];\r\nint err = 0;\r\nata_tf_to_fis(&qc->tf, link->pmp, 1, (void *)fis);\r\nerr = mv_send_fis(ap, fis, ARRAY_SIZE(fis));\r\nif (err)\r\nreturn err;\r\nswitch (qc->tf.protocol) {\r\ncase ATAPI_PROT_PIO:\r\npp->pp_flags |= MV_PP_FLAG_FAKE_ATA_BUSY;\r\ncase ATAPI_PROT_NODATA:\r\nap->hsm_task_state = HSM_ST_FIRST;\r\nbreak;\r\ncase ATA_PROT_PIO:\r\npp->pp_flags |= MV_PP_FLAG_FAKE_ATA_BUSY;\r\nif (qc->tf.flags & ATA_TFLAG_WRITE)\r\nap->hsm_task_state = HSM_ST_FIRST;\r\nelse\r\nap->hsm_task_state = HSM_ST;\r\nbreak;\r\ndefault:\r\nap->hsm_task_state = HSM_ST_LAST;\r\nbreak;\r\n}\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nata_sff_queue_pio_task(link, 0);\r\nreturn 0;\r\n}\r\nstatic unsigned int mv_qc_issue(struct ata_queued_cmd *qc)\r\n{\r\nstatic int limit_warnings = 10;\r\nstruct ata_port *ap = qc->ap;\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nstruct mv_port_priv *pp = ap->private_data;\r\nu32 in_index;\r\nunsigned int port_irqs;\r\npp->pp_flags &= ~MV_PP_FLAG_FAKE_ATA_BUSY;\r\nswitch (qc->tf.protocol) {\r\ncase ATA_PROT_DMA:\r\nif (qc->tf.command == ATA_CMD_DSM) {\r\nif (!ap->ops->bmdma_setup)\r\nreturn AC_ERR_OTHER;\r\nbreak;\r\n}\r\ncase ATA_PROT_NCQ:\r\nmv_start_edma(ap, port_mmio, pp, qc->tf.protocol);\r\npp->req_idx = (pp->req_idx + 1) & MV_MAX_Q_DEPTH_MASK;\r\nin_index = pp->req_idx << EDMA_REQ_Q_PTR_SHIFT;\r\nwritelfl((pp->crqb_dma & EDMA_REQ_Q_BASE_LO_MASK) | in_index,\r\nport_mmio + EDMA_REQ_Q_IN_PTR);\r\nreturn 0;\r\ncase ATA_PROT_PIO:\r\nif (limit_warnings > 0 && (qc->nbytes / qc->sect_size) > 1) {\r\n--limit_warnings;\r\nata_link_warn(qc->dev->link, DRV_NAME\r\n": attempting PIO w/multiple DRQ: "\r\n"this may fail due to h/w errata\n");\r\n}\r\ncase ATA_PROT_NODATA:\r\ncase ATAPI_PROT_PIO:\r\ncase ATAPI_PROT_NODATA:\r\nif (ap->flags & ATA_FLAG_PIO_POLLING)\r\nqc->tf.flags |= ATA_TFLAG_POLLING;\r\nbreak;\r\n}\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nport_irqs = ERR_IRQ;\r\nelse\r\nport_irqs = ERR_IRQ | DONE_IRQ;\r\nmv_stop_edma(ap);\r\nmv_clear_and_enable_port_irqs(ap, mv_ap_base(ap), port_irqs);\r\nmv_pmp_select(ap, qc->dev->link->pmp);\r\nif (qc->tf.command == ATA_CMD_READ_LOG_EXT) {\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nif (IS_GEN_II(hpriv))\r\nreturn mv_qc_issue_fis(qc);\r\n}\r\nreturn ata_bmdma_qc_issue(qc);\r\n}\r\nstatic struct ata_queued_cmd *mv_get_active_qc(struct ata_port *ap)\r\n{\r\nstruct mv_port_priv *pp = ap->private_data;\r\nstruct ata_queued_cmd *qc;\r\nif (pp->pp_flags & MV_PP_FLAG_NCQ_EN)\r\nreturn NULL;\r\nqc = ata_qc_from_tag(ap, ap->link.active_tag);\r\nif (qc && !(qc->tf.flags & ATA_TFLAG_POLLING))\r\nreturn qc;\r\nreturn NULL;\r\n}\r\nstatic void mv_pmp_error_handler(struct ata_port *ap)\r\n{\r\nunsigned int pmp, pmp_map;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nif (pp->pp_flags & MV_PP_FLAG_DELAYED_EH) {\r\npmp_map = pp->delayed_eh_pmp_map;\r\npp->pp_flags &= ~MV_PP_FLAG_DELAYED_EH;\r\nfor (pmp = 0; pmp_map != 0; pmp++) {\r\nunsigned int this_pmp = (1 << pmp);\r\nif (pmp_map & this_pmp) {\r\nstruct ata_link *link = &ap->pmp_link[pmp];\r\npmp_map &= ~this_pmp;\r\nata_eh_analyze_ncq_error(link);\r\n}\r\n}\r\nata_port_freeze(ap);\r\n}\r\nsata_pmp_error_handler(ap);\r\n}\r\nstatic unsigned int mv_get_err_pmp_map(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nreturn readl(port_mmio + SATA_TESTCTL) >> 16;\r\n}\r\nstatic void mv_pmp_eh_prep(struct ata_port *ap, unsigned int pmp_map)\r\n{\r\nstruct ata_eh_info *ehi;\r\nunsigned int pmp;\r\nehi = &ap->link.eh_info;\r\nfor (pmp = 0; pmp_map != 0; pmp++) {\r\nunsigned int this_pmp = (1 << pmp);\r\nif (pmp_map & this_pmp) {\r\nstruct ata_link *link = &ap->pmp_link[pmp];\r\npmp_map &= ~this_pmp;\r\nehi = &link->eh_info;\r\nata_ehi_clear_desc(ehi);\r\nata_ehi_push_desc(ehi, "dev err");\r\nehi->err_mask |= AC_ERR_DEV;\r\nehi->action |= ATA_EH_RESET;\r\nata_link_abort(link);\r\n}\r\n}\r\n}\r\nstatic int mv_req_q_empty(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nu32 in_ptr, out_ptr;\r\nin_ptr = (readl(port_mmio + EDMA_REQ_Q_IN_PTR)\r\n>> EDMA_REQ_Q_PTR_SHIFT) & MV_MAX_Q_DEPTH_MASK;\r\nout_ptr = (readl(port_mmio + EDMA_REQ_Q_OUT_PTR)\r\n>> EDMA_REQ_Q_PTR_SHIFT) & MV_MAX_Q_DEPTH_MASK;\r\nreturn (in_ptr == out_ptr);\r\n}\r\nstatic int mv_handle_fbs_ncq_dev_err(struct ata_port *ap)\r\n{\r\nstruct mv_port_priv *pp = ap->private_data;\r\nint failed_links;\r\nunsigned int old_map, new_map;\r\nif (!(pp->pp_flags & MV_PP_FLAG_DELAYED_EH)) {\r\npp->pp_flags |= MV_PP_FLAG_DELAYED_EH;\r\npp->delayed_eh_pmp_map = 0;\r\n}\r\nold_map = pp->delayed_eh_pmp_map;\r\nnew_map = old_map | mv_get_err_pmp_map(ap);\r\nif (old_map != new_map) {\r\npp->delayed_eh_pmp_map = new_map;\r\nmv_pmp_eh_prep(ap, new_map & ~old_map);\r\n}\r\nfailed_links = hweight16(new_map);\r\nata_port_info(ap,\r\n"%s: pmp_map=%04x qc_map=%04x failed_links=%d nr_active_links=%d\n",\r\n__func__, pp->delayed_eh_pmp_map,\r\nap->qc_active, failed_links,\r\nap->nr_active_links);\r\nif (ap->nr_active_links <= failed_links && mv_req_q_empty(ap)) {\r\nmv_process_crpb_entries(ap, pp);\r\nmv_stop_edma(ap);\r\nmv_eh_freeze(ap);\r\nata_port_info(ap, "%s: done\n", __func__);\r\nreturn 1;\r\n}\r\nata_port_info(ap, "%s: waiting\n", __func__);\r\nreturn 1;\r\n}\r\nstatic int mv_handle_fbs_non_ncq_dev_err(struct ata_port *ap)\r\n{\r\nreturn 0;\r\n}\r\nstatic int mv_handle_dev_err(struct ata_port *ap, u32 edma_err_cause)\r\n{\r\nstruct mv_port_priv *pp = ap->private_data;\r\nif (!(pp->pp_flags & MV_PP_FLAG_EDMA_EN))\r\nreturn 0;\r\nif (!(pp->pp_flags & MV_PP_FLAG_FBS_EN))\r\nreturn 0;\r\nif (!(edma_err_cause & EDMA_ERR_DEV))\r\nreturn 0;\r\nedma_err_cause &= ~EDMA_ERR_IRQ_TRANSIENT;\r\nif (edma_err_cause & ~(EDMA_ERR_DEV | EDMA_ERR_SELF_DIS))\r\nreturn 0;\r\nif (pp->pp_flags & MV_PP_FLAG_NCQ_EN) {\r\nif (edma_err_cause & EDMA_ERR_SELF_DIS) {\r\nata_port_warn(ap, "%s: err_cause=0x%x pp_flags=0x%x\n",\r\n__func__, edma_err_cause, pp->pp_flags);\r\nreturn 0;\r\n}\r\nreturn mv_handle_fbs_ncq_dev_err(ap);\r\n} else {\r\nif (!(edma_err_cause & EDMA_ERR_SELF_DIS)) {\r\nata_port_warn(ap, "%s: err_cause=0x%x pp_flags=0x%x\n",\r\n__func__, edma_err_cause, pp->pp_flags);\r\nreturn 0;\r\n}\r\nreturn mv_handle_fbs_non_ncq_dev_err(ap);\r\n}\r\nreturn 0;\r\n}\r\nstatic void mv_unexpected_intr(struct ata_port *ap, int edma_was_enabled)\r\n{\r\nstruct ata_eh_info *ehi = &ap->link.eh_info;\r\nchar *when = "idle";\r\nata_ehi_clear_desc(ehi);\r\nif (edma_was_enabled) {\r\nwhen = "EDMA enabled";\r\n} else {\r\nstruct ata_queued_cmd *qc = ata_qc_from_tag(ap, ap->link.active_tag);\r\nif (qc && (qc->tf.flags & ATA_TFLAG_POLLING))\r\nwhen = "polling";\r\n}\r\nata_ehi_push_desc(ehi, "unexpected device interrupt while %s", when);\r\nehi->err_mask |= AC_ERR_OTHER;\r\nehi->action |= ATA_EH_RESET;\r\nata_port_freeze(ap);\r\n}\r\nstatic void mv_err_intr(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nu32 edma_err_cause, eh_freeze_mask, serr = 0;\r\nu32 fis_cause = 0;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nunsigned int action = 0, err_mask = 0;\r\nstruct ata_eh_info *ehi = &ap->link.eh_info;\r\nstruct ata_queued_cmd *qc;\r\nint abort = 0;\r\nsata_scr_read(&ap->link, SCR_ERROR, &serr);\r\nsata_scr_write_flush(&ap->link, SCR_ERROR, serr);\r\nedma_err_cause = readl(port_mmio + EDMA_ERR_IRQ_CAUSE);\r\nif (IS_GEN_IIE(hpriv) && (edma_err_cause & EDMA_ERR_TRANS_IRQ_7)) {\r\nfis_cause = readl(port_mmio + FIS_IRQ_CAUSE);\r\nwritelfl(~fis_cause, port_mmio + FIS_IRQ_CAUSE);\r\n}\r\nwritelfl(~edma_err_cause, port_mmio + EDMA_ERR_IRQ_CAUSE);\r\nif (edma_err_cause & EDMA_ERR_DEV) {\r\nif (mv_handle_dev_err(ap, edma_err_cause))\r\nreturn;\r\n}\r\nqc = mv_get_active_qc(ap);\r\nata_ehi_clear_desc(ehi);\r\nata_ehi_push_desc(ehi, "edma_err_cause=%08x pp_flags=%08x",\r\nedma_err_cause, pp->pp_flags);\r\nif (IS_GEN_IIE(hpriv) && (edma_err_cause & EDMA_ERR_TRANS_IRQ_7)) {\r\nata_ehi_push_desc(ehi, "fis_cause=%08x", fis_cause);\r\nif (fis_cause & FIS_IRQ_CAUSE_AN) {\r\nu32 ec = edma_err_cause &\r\n~(EDMA_ERR_TRANS_IRQ_7 | EDMA_ERR_IRQ_TRANSIENT);\r\nsata_async_notification(ap);\r\nif (!ec)\r\nreturn;\r\nata_ehi_push_desc(ehi, "SDB notify");\r\n}\r\n}\r\nif (edma_err_cause & EDMA_ERR_DEV) {\r\nerr_mask |= AC_ERR_DEV;\r\naction |= ATA_EH_RESET;\r\nata_ehi_push_desc(ehi, "dev error");\r\n}\r\nif (edma_err_cause & (EDMA_ERR_D_PAR | EDMA_ERR_PRD_PAR |\r\nEDMA_ERR_CRQB_PAR | EDMA_ERR_CRPB_PAR |\r\nEDMA_ERR_INTRL_PAR)) {\r\nerr_mask |= AC_ERR_ATA_BUS;\r\naction |= ATA_EH_RESET;\r\nata_ehi_push_desc(ehi, "parity error");\r\n}\r\nif (edma_err_cause & (EDMA_ERR_DEV_DCON | EDMA_ERR_DEV_CON)) {\r\nata_ehi_hotplugged(ehi);\r\nata_ehi_push_desc(ehi, edma_err_cause & EDMA_ERR_DEV_DCON ?\r\n"dev disconnect" : "dev connect");\r\naction |= ATA_EH_RESET;\r\n}\r\nif (IS_GEN_I(hpriv)) {\r\neh_freeze_mask = EDMA_EH_FREEZE_5;\r\nif (edma_err_cause & EDMA_ERR_SELF_DIS_5) {\r\npp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\r\nata_ehi_push_desc(ehi, "EDMA self-disable");\r\n}\r\n} else {\r\neh_freeze_mask = EDMA_EH_FREEZE;\r\nif (edma_err_cause & EDMA_ERR_SELF_DIS) {\r\npp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\r\nata_ehi_push_desc(ehi, "EDMA self-disable");\r\n}\r\nif (edma_err_cause & EDMA_ERR_SERR) {\r\nata_ehi_push_desc(ehi, "SError=%08x", serr);\r\nerr_mask |= AC_ERR_ATA_BUS;\r\naction |= ATA_EH_RESET;\r\n}\r\n}\r\nif (!err_mask) {\r\nerr_mask = AC_ERR_OTHER;\r\naction |= ATA_EH_RESET;\r\n}\r\nehi->serror |= serr;\r\nehi->action |= action;\r\nif (qc)\r\nqc->err_mask |= err_mask;\r\nelse\r\nehi->err_mask |= err_mask;\r\nif (err_mask == AC_ERR_DEV) {\r\nmv_eh_freeze(ap);\r\nabort = 1;\r\n} else if (edma_err_cause & eh_freeze_mask) {\r\nata_port_freeze(ap);\r\n} else {\r\nabort = 1;\r\n}\r\nif (abort) {\r\nif (qc)\r\nata_link_abort(qc->dev->link);\r\nelse\r\nata_port_abort(ap);\r\n}\r\n}\r\nstatic bool mv_process_crpb_response(struct ata_port *ap,\r\nstruct mv_crpb *response, unsigned int tag, int ncq_enabled)\r\n{\r\nu8 ata_status;\r\nu16 edma_status = le16_to_cpu(response->flags);\r\nif (!ncq_enabled) {\r\nu8 err_cause = edma_status & 0xff & ~EDMA_ERR_DEV;\r\nif (err_cause) {\r\nreturn false;\r\n}\r\n}\r\nata_status = edma_status >> CRPB_FLAG_STATUS_SHIFT;\r\nif (!ac_err_mask(ata_status))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void mv_process_crpb_entries(struct ata_port *ap, struct mv_port_priv *pp)\r\n{\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nu32 in_index;\r\nbool work_done = false;\r\nu32 done_mask = 0;\r\nint ncq_enabled = (pp->pp_flags & MV_PP_FLAG_NCQ_EN);\r\nin_index = (readl(port_mmio + EDMA_RSP_Q_IN_PTR)\r\n>> EDMA_RSP_Q_PTR_SHIFT) & MV_MAX_Q_DEPTH_MASK;\r\nwhile (in_index != pp->resp_idx) {\r\nunsigned int tag;\r\nstruct mv_crpb *response = &pp->crpb[pp->resp_idx];\r\npp->resp_idx = (pp->resp_idx + 1) & MV_MAX_Q_DEPTH_MASK;\r\nif (IS_GEN_I(hpriv)) {\r\ntag = ap->link.active_tag;\r\n} else {\r\ntag = le16_to_cpu(response->id) & 0x1f;\r\n}\r\nif (mv_process_crpb_response(ap, response, tag, ncq_enabled))\r\ndone_mask |= 1 << tag;\r\nwork_done = true;\r\n}\r\nif (work_done) {\r\nata_qc_complete_multiple(ap, ap->qc_active ^ done_mask);\r\nwritelfl((pp->crpb_dma & EDMA_RSP_Q_BASE_LO_MASK) |\r\n(pp->resp_idx << EDMA_RSP_Q_PTR_SHIFT),\r\nport_mmio + EDMA_RSP_Q_OUT_PTR);\r\n}\r\n}\r\nstatic void mv_port_intr(struct ata_port *ap, u32 port_cause)\r\n{\r\nstruct mv_port_priv *pp;\r\nint edma_was_enabled;\r\npp = ap->private_data;\r\nedma_was_enabled = (pp->pp_flags & MV_PP_FLAG_EDMA_EN);\r\nif (edma_was_enabled && (port_cause & DONE_IRQ)) {\r\nmv_process_crpb_entries(ap, pp);\r\nif (pp->pp_flags & MV_PP_FLAG_DELAYED_EH)\r\nmv_handle_fbs_ncq_dev_err(ap);\r\n}\r\nif (unlikely(port_cause & ERR_IRQ)) {\r\nmv_err_intr(ap);\r\n} else if (!edma_was_enabled) {\r\nstruct ata_queued_cmd *qc = mv_get_active_qc(ap);\r\nif (qc)\r\nata_bmdma_port_intr(ap, qc);\r\nelse\r\nmv_unexpected_intr(ap, edma_was_enabled);\r\n}\r\n}\r\nstatic int mv_host_intr(struct ata_host *host, u32 main_irq_cause)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *mmio = hpriv->base, *hc_mmio;\r\nunsigned int handled = 0, port;\r\nif (main_irq_cause & ALL_PORTS_COAL_DONE)\r\nwritel(~ALL_PORTS_COAL_IRQ, mmio + IRQ_COAL_CAUSE);\r\nfor (port = 0; port < hpriv->n_ports; port++) {\r\nstruct ata_port *ap = host->ports[port];\r\nunsigned int p, shift, hardport, port_cause;\r\nMV_PORT_TO_SHIFT_AND_HARDPORT(port, shift, hardport);\r\nif (hardport == 0) {\r\nu32 hc_cause = (main_irq_cause >> shift) & HC0_IRQ_PEND;\r\nu32 port_mask, ack_irqs;\r\nif (!hc_cause) {\r\nport += MV_PORTS_PER_HC - 1;\r\ncontinue;\r\n}\r\nack_irqs = 0;\r\nif (hc_cause & PORTS_0_3_COAL_DONE)\r\nack_irqs = HC_COAL_IRQ;\r\nfor (p = 0; p < MV_PORTS_PER_HC; ++p) {\r\nif ((port + p) >= hpriv->n_ports)\r\nbreak;\r\nport_mask = (DONE_IRQ | ERR_IRQ) << (p * 2);\r\nif (hc_cause & port_mask)\r\nack_irqs |= (DMA_IRQ | DEV_IRQ) << p;\r\n}\r\nhc_mmio = mv_hc_base_from_port(mmio, port);\r\nwritelfl(~ack_irqs, hc_mmio + HC_IRQ_CAUSE);\r\nhandled = 1;\r\n}\r\nport_cause = (main_irq_cause >> shift) & (DONE_IRQ | ERR_IRQ);\r\nif (port_cause)\r\nmv_port_intr(ap, port_cause);\r\n}\r\nreturn handled;\r\n}\r\nstatic int mv_pci_error(struct ata_host *host, void __iomem *mmio)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nstruct ata_port *ap;\r\nstruct ata_queued_cmd *qc;\r\nstruct ata_eh_info *ehi;\r\nunsigned int i, err_mask, printed = 0;\r\nu32 err_cause;\r\nerr_cause = readl(mmio + hpriv->irq_cause_offset);\r\ndev_err(host->dev, "PCI ERROR; PCI IRQ cause=0x%08x\n", err_cause);\r\nDPRINTK("All regs @ PCI error\n");\r\nmv_dump_all_regs(mmio, -1, to_pci_dev(host->dev));\r\nwritelfl(0, mmio + hpriv->irq_cause_offset);\r\nfor (i = 0; i < host->n_ports; i++) {\r\nap = host->ports[i];\r\nif (!ata_link_offline(&ap->link)) {\r\nehi = &ap->link.eh_info;\r\nata_ehi_clear_desc(ehi);\r\nif (!printed++)\r\nata_ehi_push_desc(ehi,\r\n"PCI err cause 0x%08x", err_cause);\r\nerr_mask = AC_ERR_HOST_BUS;\r\nehi->action = ATA_EH_RESET;\r\nqc = ata_qc_from_tag(ap, ap->link.active_tag);\r\nif (qc)\r\nqc->err_mask |= err_mask;\r\nelse\r\nehi->err_mask |= err_mask;\r\nata_port_freeze(ap);\r\n}\r\n}\r\nreturn 1;\r\n}\r\nstatic irqreturn_t mv_interrupt(int irq, void *dev_instance)\r\n{\r\nstruct ata_host *host = dev_instance;\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nunsigned int handled = 0;\r\nint using_msi = hpriv->hp_flags & MV_HP_FLAG_MSI;\r\nu32 main_irq_cause, pending_irqs;\r\nspin_lock(&host->lock);\r\nif (using_msi)\r\nmv_write_main_irq_mask(0, hpriv);\r\nmain_irq_cause = readl(hpriv->main_irq_cause_addr);\r\npending_irqs = main_irq_cause & hpriv->main_irq_mask;\r\nif (pending_irqs && main_irq_cause != 0xffffffffU) {\r\nif (unlikely((pending_irqs & PCI_ERR) && !IS_SOC(hpriv)))\r\nhandled = mv_pci_error(host, hpriv->base);\r\nelse\r\nhandled = mv_host_intr(host, pending_irqs);\r\n}\r\nif (using_msi)\r\nmv_write_main_irq_mask(hpriv->main_irq_mask, hpriv);\r\nspin_unlock(&host->lock);\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic unsigned int mv5_scr_offset(unsigned int sc_reg_in)\r\n{\r\nunsigned int ofs;\r\nswitch (sc_reg_in) {\r\ncase SCR_STATUS:\r\ncase SCR_ERROR:\r\ncase SCR_CONTROL:\r\nofs = sc_reg_in * sizeof(u32);\r\nbreak;\r\ndefault:\r\nofs = 0xffffffffU;\r\nbreak;\r\n}\r\nreturn ofs;\r\n}\r\nstatic int mv5_scr_read(struct ata_link *link, unsigned int sc_reg_in, u32 *val)\r\n{\r\nstruct mv_host_priv *hpriv = link->ap->host->private_data;\r\nvoid __iomem *mmio = hpriv->base;\r\nvoid __iomem *addr = mv5_phy_base(mmio, link->ap->port_no);\r\nunsigned int ofs = mv5_scr_offset(sc_reg_in);\r\nif (ofs != 0xffffffffU) {\r\n*val = readl(addr + ofs);\r\nreturn 0;\r\n} else\r\nreturn -EINVAL;\r\n}\r\nstatic int mv5_scr_write(struct ata_link *link, unsigned int sc_reg_in, u32 val)\r\n{\r\nstruct mv_host_priv *hpriv = link->ap->host->private_data;\r\nvoid __iomem *mmio = hpriv->base;\r\nvoid __iomem *addr = mv5_phy_base(mmio, link->ap->port_no);\r\nunsigned int ofs = mv5_scr_offset(sc_reg_in);\r\nif (ofs != 0xffffffffU) {\r\nwritelfl(val, addr + ofs);\r\nreturn 0;\r\n} else\r\nreturn -EINVAL;\r\n}\r\nstatic void mv5_reset_bus(struct ata_host *host, void __iomem *mmio)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(host->dev);\r\nint early_5080;\r\nearly_5080 = (pdev->device == 0x5080) && (pdev->revision == 0);\r\nif (!early_5080) {\r\nu32 tmp = readl(mmio + MV_PCI_EXP_ROM_BAR_CTL);\r\ntmp |= (1 << 0);\r\nwritel(tmp, mmio + MV_PCI_EXP_ROM_BAR_CTL);\r\n}\r\nmv_reset_pci_bus(host, mmio);\r\n}\r\nstatic void mv5_reset_flash(struct mv_host_priv *hpriv, void __iomem *mmio)\r\n{\r\nwritel(0x0fcfffff, mmio + FLASH_CTL);\r\n}\r\nstatic void mv5_read_preamp(struct mv_host_priv *hpriv, int idx,\r\nvoid __iomem *mmio)\r\n{\r\nvoid __iomem *phy_mmio = mv5_phy_base(mmio, idx);\r\nu32 tmp;\r\ntmp = readl(phy_mmio + MV5_PHY_MODE);\r\nhpriv->signal[idx].pre = tmp & 0x1800;\r\nhpriv->signal[idx].amps = tmp & 0xe0;\r\n}\r\nstatic void mv5_enable_leds(struct mv_host_priv *hpriv, void __iomem *mmio)\r\n{\r\nu32 tmp;\r\nwritel(0, mmio + GPIO_PORT_CTL);\r\ntmp = readl(mmio + MV_PCI_EXP_ROM_BAR_CTL);\r\ntmp |= ~(1 << 0);\r\nwritel(tmp, mmio + MV_PCI_EXP_ROM_BAR_CTL);\r\n}\r\nstatic void mv5_phy_errata(struct mv_host_priv *hpriv, void __iomem *mmio,\r\nunsigned int port)\r\n{\r\nvoid __iomem *phy_mmio = mv5_phy_base(mmio, port);\r\nconst u32 mask = (1<<12) | (1<<11) | (1<<7) | (1<<6) | (1<<5);\r\nu32 tmp;\r\nint fix_apm_sq = (hpriv->hp_flags & MV_HP_ERRATA_50XXB0);\r\nif (fix_apm_sq) {\r\ntmp = readl(phy_mmio + MV5_LTMODE);\r\ntmp |= (1 << 19);\r\nwritel(tmp, phy_mmio + MV5_LTMODE);\r\ntmp = readl(phy_mmio + MV5_PHY_CTL);\r\ntmp &= ~0x3;\r\ntmp |= 0x1;\r\nwritel(tmp, phy_mmio + MV5_PHY_CTL);\r\n}\r\ntmp = readl(phy_mmio + MV5_PHY_MODE);\r\ntmp &= ~mask;\r\ntmp |= hpriv->signal[port].pre;\r\ntmp |= hpriv->signal[port].amps;\r\nwritel(tmp, phy_mmio + MV5_PHY_MODE);\r\n}\r\nstatic void mv5_reset_hc_port(struct mv_host_priv *hpriv, void __iomem *mmio,\r\nunsigned int port)\r\n{\r\nvoid __iomem *port_mmio = mv_port_base(mmio, port);\r\nmv_reset_channel(hpriv, mmio, port);\r\nZERO(0x028);\r\nwritel(0x11f, port_mmio + EDMA_CFG);\r\nZERO(0x004);\r\nZERO(0x008);\r\nZERO(0x00c);\r\nZERO(0x010);\r\nZERO(0x014);\r\nZERO(0x018);\r\nZERO(0x01c);\r\nZERO(0x024);\r\nZERO(0x020);\r\nZERO(0x02c);\r\nwritel(0xbc, port_mmio + EDMA_IORDY_TMOUT);\r\n}\r\nstatic void mv5_reset_one_hc(struct mv_host_priv *hpriv, void __iomem *mmio,\r\nunsigned int hc)\r\n{\r\nvoid __iomem *hc_mmio = mv_hc_base(mmio, hc);\r\nu32 tmp;\r\nZERO(0x00c);\r\nZERO(0x010);\r\nZERO(0x014);\r\nZERO(0x018);\r\ntmp = readl(hc_mmio + 0x20);\r\ntmp &= 0x1c1c1c1c;\r\ntmp |= 0x03030303;\r\nwritel(tmp, hc_mmio + 0x20);\r\n}\r\nstatic int mv5_reset_hc(struct mv_host_priv *hpriv, void __iomem *mmio,\r\nunsigned int n_hc)\r\n{\r\nunsigned int hc, port;\r\nfor (hc = 0; hc < n_hc; hc++) {\r\nfor (port = 0; port < MV_PORTS_PER_HC; port++)\r\nmv5_reset_hc_port(hpriv, mmio,\r\n(hc * MV_PORTS_PER_HC) + port);\r\nmv5_reset_one_hc(hpriv, mmio, hc);\r\n}\r\nreturn 0;\r\n}\r\nstatic void mv_reset_pci_bus(struct ata_host *host, void __iomem *mmio)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nu32 tmp;\r\ntmp = readl(mmio + MV_PCI_MODE);\r\ntmp &= 0xff00ffff;\r\nwritel(tmp, mmio + MV_PCI_MODE);\r\nZERO(MV_PCI_DISC_TIMER);\r\nZERO(MV_PCI_MSI_TRIGGER);\r\nwritel(0x000100ff, mmio + MV_PCI_XBAR_TMOUT);\r\nZERO(MV_PCI_SERR_MASK);\r\nZERO(hpriv->irq_cause_offset);\r\nZERO(hpriv->irq_mask_offset);\r\nZERO(MV_PCI_ERR_LOW_ADDRESS);\r\nZERO(MV_PCI_ERR_HIGH_ADDRESS);\r\nZERO(MV_PCI_ERR_ATTRIBUTE);\r\nZERO(MV_PCI_ERR_COMMAND);\r\n}\r\nstatic void mv6_reset_flash(struct mv_host_priv *hpriv, void __iomem *mmio)\r\n{\r\nu32 tmp;\r\nmv5_reset_flash(hpriv, mmio);\r\ntmp = readl(mmio + GPIO_PORT_CTL);\r\ntmp &= 0x3;\r\ntmp |= (1 << 5) | (1 << 6);\r\nwritel(tmp, mmio + GPIO_PORT_CTL);\r\n}\r\nstatic int mv6_reset_hc(struct mv_host_priv *hpriv, void __iomem *mmio,\r\nunsigned int n_hc)\r\n{\r\nvoid __iomem *reg = mmio + PCI_MAIN_CMD_STS;\r\nint i, rc = 0;\r\nu32 t;\r\nt = readl(reg);\r\nwritel(t | STOP_PCI_MASTER, reg);\r\nfor (i = 0; i < 1000; i++) {\r\nudelay(1);\r\nt = readl(reg);\r\nif (PCI_MASTER_EMPTY & t)\r\nbreak;\r\n}\r\nif (!(PCI_MASTER_EMPTY & t)) {\r\nprintk(KERN_ERR DRV_NAME ": PCI master won't flush\n");\r\nrc = 1;\r\ngoto done;\r\n}\r\ni = 5;\r\ndo {\r\nwritel(t | GLOB_SFT_RST, reg);\r\nt = readl(reg);\r\nudelay(1);\r\n} while (!(GLOB_SFT_RST & t) && (i-- > 0));\r\nif (!(GLOB_SFT_RST & t)) {\r\nprintk(KERN_ERR DRV_NAME ": can't set global reset\n");\r\nrc = 1;\r\ngoto done;\r\n}\r\ni = 5;\r\ndo {\r\nwritel(t & ~(GLOB_SFT_RST | STOP_PCI_MASTER), reg);\r\nt = readl(reg);\r\nudelay(1);\r\n} while ((GLOB_SFT_RST & t) && (i-- > 0));\r\nif (GLOB_SFT_RST & t) {\r\nprintk(KERN_ERR DRV_NAME ": can't clear global reset\n");\r\nrc = 1;\r\n}\r\ndone:\r\nreturn rc;\r\n}\r\nstatic void mv6_read_preamp(struct mv_host_priv *hpriv, int idx,\r\nvoid __iomem *mmio)\r\n{\r\nvoid __iomem *port_mmio;\r\nu32 tmp;\r\ntmp = readl(mmio + RESET_CFG);\r\nif ((tmp & (1 << 0)) == 0) {\r\nhpriv->signal[idx].amps = 0x7 << 8;\r\nhpriv->signal[idx].pre = 0x1 << 5;\r\nreturn;\r\n}\r\nport_mmio = mv_port_base(mmio, idx);\r\ntmp = readl(port_mmio + PHY_MODE2);\r\nhpriv->signal[idx].amps = tmp & 0x700;\r\nhpriv->signal[idx].pre = tmp & 0xe0;\r\n}\r\nstatic void mv6_enable_leds(struct mv_host_priv *hpriv, void __iomem *mmio)\r\n{\r\nwritel(0x00000060, mmio + GPIO_PORT_CTL);\r\n}\r\nstatic void mv6_phy_errata(struct mv_host_priv *hpriv, void __iomem *mmio,\r\nunsigned int port)\r\n{\r\nvoid __iomem *port_mmio = mv_port_base(mmio, port);\r\nu32 hp_flags = hpriv->hp_flags;\r\nint fix_phy_mode2 =\r\nhp_flags & (MV_HP_ERRATA_60X1B2 | MV_HP_ERRATA_60X1C0);\r\nint fix_phy_mode4 =\r\nhp_flags & (MV_HP_ERRATA_60X1B2 | MV_HP_ERRATA_60X1C0);\r\nu32 m2, m3;\r\nif (fix_phy_mode2) {\r\nm2 = readl(port_mmio + PHY_MODE2);\r\nm2 &= ~(1 << 16);\r\nm2 |= (1 << 31);\r\nwritel(m2, port_mmio + PHY_MODE2);\r\nudelay(200);\r\nm2 = readl(port_mmio + PHY_MODE2);\r\nm2 &= ~((1 << 16) | (1 << 31));\r\nwritel(m2, port_mmio + PHY_MODE2);\r\nudelay(200);\r\n}\r\nm3 = readl(port_mmio + PHY_MODE3);\r\nm3 = (m3 & 0x1f) | (0x5555601 << 5);\r\nif (IS_SOC(hpriv))\r\nm3 &= ~0x1c;\r\nif (fix_phy_mode4) {\r\nu32 m4 = readl(port_mmio + PHY_MODE4);\r\nif (IS_GEN_IIE(hpriv))\r\nm4 = (m4 & ~PHY_MODE4_RSVD_ZEROS) | PHY_MODE4_RSVD_ONES;\r\nelse\r\nm4 = (m4 & ~PHY_MODE4_CFG_MASK) | PHY_MODE4_CFG_VALUE;\r\nwritel(m4, port_mmio + PHY_MODE4);\r\n}\r\nwritel(m3, port_mmio + PHY_MODE3);\r\nm2 = readl(port_mmio + PHY_MODE2);\r\nm2 &= ~MV_M2_PREAMP_MASK;\r\nm2 |= hpriv->signal[port].amps;\r\nm2 |= hpriv->signal[port].pre;\r\nm2 &= ~(1 << 16);\r\nif (IS_GEN_IIE(hpriv)) {\r\nm2 &= ~0xC30FF01F;\r\nm2 |= 0x0000900F;\r\n}\r\nwritel(m2, port_mmio + PHY_MODE2);\r\n}\r\nstatic void mv_soc_enable_leds(struct mv_host_priv *hpriv,\r\nvoid __iomem *mmio)\r\n{\r\nreturn;\r\n}\r\nstatic void mv_soc_read_preamp(struct mv_host_priv *hpriv, int idx,\r\nvoid __iomem *mmio)\r\n{\r\nvoid __iomem *port_mmio;\r\nu32 tmp;\r\nport_mmio = mv_port_base(mmio, idx);\r\ntmp = readl(port_mmio + PHY_MODE2);\r\nhpriv->signal[idx].amps = tmp & 0x700;\r\nhpriv->signal[idx].pre = tmp & 0xe0;\r\n}\r\nstatic void mv_soc_reset_hc_port(struct mv_host_priv *hpriv,\r\nvoid __iomem *mmio, unsigned int port)\r\n{\r\nvoid __iomem *port_mmio = mv_port_base(mmio, port);\r\nmv_reset_channel(hpriv, mmio, port);\r\nZERO(0x028);\r\nwritel(0x101f, port_mmio + EDMA_CFG);\r\nZERO(0x004);\r\nZERO(0x008);\r\nZERO(0x00c);\r\nZERO(0x010);\r\nZERO(0x014);\r\nZERO(0x018);\r\nZERO(0x01c);\r\nZERO(0x024);\r\nZERO(0x020);\r\nZERO(0x02c);\r\nwritel(0x800, port_mmio + EDMA_IORDY_TMOUT);\r\n}\r\nstatic void mv_soc_reset_one_hc(struct mv_host_priv *hpriv,\r\nvoid __iomem *mmio)\r\n{\r\nvoid __iomem *hc_mmio = mv_hc_base(mmio, 0);\r\nZERO(0x00c);\r\nZERO(0x010);\r\nZERO(0x014);\r\n}\r\nstatic int mv_soc_reset_hc(struct mv_host_priv *hpriv,\r\nvoid __iomem *mmio, unsigned int n_hc)\r\n{\r\nunsigned int port;\r\nfor (port = 0; port < hpriv->n_ports; port++)\r\nmv_soc_reset_hc_port(hpriv, mmio, port);\r\nmv_soc_reset_one_hc(hpriv, mmio);\r\nreturn 0;\r\n}\r\nstatic void mv_soc_reset_flash(struct mv_host_priv *hpriv,\r\nvoid __iomem *mmio)\r\n{\r\nreturn;\r\n}\r\nstatic void mv_soc_reset_bus(struct ata_host *host, void __iomem *mmio)\r\n{\r\nreturn;\r\n}\r\nstatic void mv_soc_65n_phy_errata(struct mv_host_priv *hpriv,\r\nvoid __iomem *mmio, unsigned int port)\r\n{\r\nvoid __iomem *port_mmio = mv_port_base(mmio, port);\r\nu32 reg;\r\nreg = readl(port_mmio + PHY_MODE3);\r\nreg &= ~(0x3 << 27);\r\nreg |= (0x1 << 27);\r\nreg &= ~(0x3 << 29);\r\nreg |= (0x1 << 29);\r\nwritel(reg, port_mmio + PHY_MODE3);\r\nreg = readl(port_mmio + PHY_MODE4);\r\nreg &= ~0x1;\r\nreg |= (0x1 << 16);\r\nwritel(reg, port_mmio + PHY_MODE4);\r\nreg = readl(port_mmio + PHY_MODE9_GEN2);\r\nreg &= ~0xf;\r\nreg |= 0x8;\r\nreg &= ~(0x1 << 14);\r\nwritel(reg, port_mmio + PHY_MODE9_GEN2);\r\nreg = readl(port_mmio + PHY_MODE9_GEN1);\r\nreg &= ~0xf;\r\nreg |= 0x8;\r\nreg &= ~(0x1 << 14);\r\nwritel(reg, port_mmio + PHY_MODE9_GEN1);\r\n}\r\nstatic bool soc_is_65n(struct mv_host_priv *hpriv)\r\n{\r\nvoid __iomem *port0_mmio = mv_port_base(hpriv->base, 0);\r\nif (readl(port0_mmio + PHYCFG_OFS))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void mv_setup_ifcfg(void __iomem *port_mmio, int want_gen2i)\r\n{\r\nu32 ifcfg = readl(port_mmio + SATA_IFCFG);\r\nifcfg = (ifcfg & 0xf7f) | 0x9b1000;\r\nif (want_gen2i)\r\nifcfg |= (1 << 7);\r\nwritelfl(ifcfg, port_mmio + SATA_IFCFG);\r\n}\r\nstatic void mv_reset_channel(struct mv_host_priv *hpriv, void __iomem *mmio,\r\nunsigned int port_no)\r\n{\r\nvoid __iomem *port_mmio = mv_port_base(mmio, port_no);\r\nmv_stop_edma_engine(port_mmio);\r\nwritelfl(EDMA_RESET, port_mmio + EDMA_CMD);\r\nif (!IS_GEN_I(hpriv)) {\r\nmv_setup_ifcfg(port_mmio, 1);\r\n}\r\nwritelfl(EDMA_RESET, port_mmio + EDMA_CMD);\r\nudelay(25);\r\nwritelfl(0, port_mmio + EDMA_CMD);\r\nhpriv->ops->phy_errata(hpriv, mmio, port_no);\r\nif (IS_GEN_I(hpriv))\r\nmdelay(1);\r\n}\r\nstatic void mv_pmp_select(struct ata_port *ap, int pmp)\r\n{\r\nif (sata_pmp_supported(ap)) {\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nu32 reg = readl(port_mmio + SATA_IFCTL);\r\nint old = reg & 0xf;\r\nif (old != pmp) {\r\nreg = (reg & ~0xf) | pmp;\r\nwritelfl(reg, port_mmio + SATA_IFCTL);\r\n}\r\n}\r\n}\r\nstatic int mv_pmp_hardreset(struct ata_link *link, unsigned int *class,\r\nunsigned long deadline)\r\n{\r\nmv_pmp_select(link->ap, sata_srst_pmp(link));\r\nreturn sata_std_hardreset(link, class, deadline);\r\n}\r\nstatic int mv_softreset(struct ata_link *link, unsigned int *class,\r\nunsigned long deadline)\r\n{\r\nmv_pmp_select(link->ap, sata_srst_pmp(link));\r\nreturn ata_sff_softreset(link, class, deadline);\r\n}\r\nstatic int mv_hardreset(struct ata_link *link, unsigned int *class,\r\nunsigned long deadline)\r\n{\r\nstruct ata_port *ap = link->ap;\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nstruct mv_port_priv *pp = ap->private_data;\r\nvoid __iomem *mmio = hpriv->base;\r\nint rc, attempts = 0, extra = 0;\r\nu32 sstatus;\r\nbool online;\r\nmv_reset_channel(hpriv, mmio, ap->port_no);\r\npp->pp_flags &= ~MV_PP_FLAG_EDMA_EN;\r\npp->pp_flags &=\r\n~(MV_PP_FLAG_FBS_EN | MV_PP_FLAG_NCQ_EN | MV_PP_FLAG_FAKE_ATA_BUSY);\r\ndo {\r\nconst unsigned long *timing =\r\nsata_ehc_deb_timing(&link->eh_context);\r\nrc = sata_link_hardreset(link, timing, deadline + extra,\r\n&online, NULL);\r\nrc = online ? -EAGAIN : rc;\r\nif (rc)\r\nreturn rc;\r\nsata_scr_read(link, SCR_STATUS, &sstatus);\r\nif (!IS_GEN_I(hpriv) && ++attempts >= 5 && sstatus == 0x121) {\r\nmv_setup_ifcfg(mv_ap_base(ap), 0);\r\nif (time_after(jiffies + HZ, deadline))\r\nextra = HZ;\r\n}\r\n} while (sstatus != 0x0 && sstatus != 0x113 && sstatus != 0x123);\r\nmv_save_cached_regs(ap);\r\nmv_edma_cfg(ap, 0, 0);\r\nreturn rc;\r\n}\r\nstatic void mv_eh_freeze(struct ata_port *ap)\r\n{\r\nmv_stop_edma(ap);\r\nmv_enable_port_irqs(ap, 0);\r\n}\r\nstatic void mv_eh_thaw(struct ata_port *ap)\r\n{\r\nstruct mv_host_priv *hpriv = ap->host->private_data;\r\nunsigned int port = ap->port_no;\r\nunsigned int hardport = mv_hardport_from_port(port);\r\nvoid __iomem *hc_mmio = mv_hc_base_from_port(hpriv->base, port);\r\nvoid __iomem *port_mmio = mv_ap_base(ap);\r\nu32 hc_irq_cause;\r\nwritel(0, port_mmio + EDMA_ERR_IRQ_CAUSE);\r\nhc_irq_cause = ~((DEV_IRQ | DMA_IRQ) << hardport);\r\nwritelfl(hc_irq_cause, hc_mmio + HC_IRQ_CAUSE);\r\nmv_enable_port_irqs(ap, ERR_IRQ);\r\n}\r\nstatic void mv_port_init(struct ata_ioports *port, void __iomem *port_mmio)\r\n{\r\nvoid __iomem *serr, *shd_base = port_mmio + SHD_BLK;\r\nport->data_addr = shd_base + (sizeof(u32) * ATA_REG_DATA);\r\nport->error_addr =\r\nport->feature_addr = shd_base + (sizeof(u32) * ATA_REG_ERR);\r\nport->nsect_addr = shd_base + (sizeof(u32) * ATA_REG_NSECT);\r\nport->lbal_addr = shd_base + (sizeof(u32) * ATA_REG_LBAL);\r\nport->lbam_addr = shd_base + (sizeof(u32) * ATA_REG_LBAM);\r\nport->lbah_addr = shd_base + (sizeof(u32) * ATA_REG_LBAH);\r\nport->device_addr = shd_base + (sizeof(u32) * ATA_REG_DEVICE);\r\nport->status_addr =\r\nport->command_addr = shd_base + (sizeof(u32) * ATA_REG_STATUS);\r\nport->altstatus_addr = port->ctl_addr = shd_base + SHD_CTL_AST;\r\nserr = port_mmio + mv_scr_offset(SCR_ERROR);\r\nwritelfl(readl(serr), serr);\r\nwritelfl(0, port_mmio + EDMA_ERR_IRQ_CAUSE);\r\nwritelfl(~EDMA_ERR_IRQ_TRANSIENT, port_mmio + EDMA_ERR_IRQ_MASK);\r\nVPRINTK("EDMA cfg=0x%08x EDMA IRQ err cause/mask=0x%08x/0x%08x\n",\r\nreadl(port_mmio + EDMA_CFG),\r\nreadl(port_mmio + EDMA_ERR_IRQ_CAUSE),\r\nreadl(port_mmio + EDMA_ERR_IRQ_MASK));\r\n}\r\nstatic unsigned int mv_in_pcix_mode(struct ata_host *host)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *mmio = hpriv->base;\r\nu32 reg;\r\nif (IS_SOC(hpriv) || !IS_PCIE(hpriv))\r\nreturn 0;\r\nreg = readl(mmio + MV_PCI_MODE);\r\nif ((reg & MV_PCI_MODE_MASK) == 0)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int mv_pci_cut_through_okay(struct ata_host *host)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *mmio = hpriv->base;\r\nu32 reg;\r\nif (!mv_in_pcix_mode(host)) {\r\nreg = readl(mmio + MV_PCI_COMMAND);\r\nif (reg & MV_PCI_COMMAND_MRDTRIG)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void mv_60x1b2_errata_pci7(struct ata_host *host)\r\n{\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *mmio = hpriv->base;\r\nif (mv_in_pcix_mode(host)) {\r\nu32 reg = readl(mmio + MV_PCI_COMMAND);\r\nwritelfl(reg & ~MV_PCI_COMMAND_MWRCOM, mmio + MV_PCI_COMMAND);\r\n}\r\n}\r\nstatic int mv_chip_id(struct ata_host *host, unsigned int board_idx)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(host->dev);\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nu32 hp_flags = hpriv->hp_flags;\r\nswitch (board_idx) {\r\ncase chip_5080:\r\nhpriv->ops = &mv5xxx_ops;\r\nhp_flags |= MV_HP_GEN_I;\r\nswitch (pdev->revision) {\r\ncase 0x1:\r\nhp_flags |= MV_HP_ERRATA_50XXB0;\r\nbreak;\r\ncase 0x3:\r\nhp_flags |= MV_HP_ERRATA_50XXB2;\r\nbreak;\r\ndefault:\r\ndev_warn(&pdev->dev,\r\n"Applying 50XXB2 workarounds to unknown rev\n");\r\nhp_flags |= MV_HP_ERRATA_50XXB2;\r\nbreak;\r\n}\r\nbreak;\r\ncase chip_504x:\r\ncase chip_508x:\r\nhpriv->ops = &mv5xxx_ops;\r\nhp_flags |= MV_HP_GEN_I;\r\nswitch (pdev->revision) {\r\ncase 0x0:\r\nhp_flags |= MV_HP_ERRATA_50XXB0;\r\nbreak;\r\ncase 0x3:\r\nhp_flags |= MV_HP_ERRATA_50XXB2;\r\nbreak;\r\ndefault:\r\ndev_warn(&pdev->dev,\r\n"Applying B2 workarounds to unknown rev\n");\r\nhp_flags |= MV_HP_ERRATA_50XXB2;\r\nbreak;\r\n}\r\nbreak;\r\ncase chip_604x:\r\ncase chip_608x:\r\nhpriv->ops = &mv6xxx_ops;\r\nhp_flags |= MV_HP_GEN_II;\r\nswitch (pdev->revision) {\r\ncase 0x7:\r\nmv_60x1b2_errata_pci7(host);\r\nhp_flags |= MV_HP_ERRATA_60X1B2;\r\nbreak;\r\ncase 0x9:\r\nhp_flags |= MV_HP_ERRATA_60X1C0;\r\nbreak;\r\ndefault:\r\ndev_warn(&pdev->dev,\r\n"Applying B2 workarounds to unknown rev\n");\r\nhp_flags |= MV_HP_ERRATA_60X1B2;\r\nbreak;\r\n}\r\nbreak;\r\ncase chip_7042:\r\nhp_flags |= MV_HP_PCIE | MV_HP_CUT_THROUGH;\r\nif (pdev->vendor == PCI_VENDOR_ID_TTI &&\r\n(pdev->device == 0x2300 || pdev->device == 0x2310))\r\n{\r\nprintk(KERN_WARNING DRV_NAME ": Highpoint RocketRAID"\r\n" BIOS CORRUPTS DATA on all attached drives,"\r\n" regardless of if/how they are configured."\r\n" BEWARE!\n");\r\nprintk(KERN_WARNING DRV_NAME ": For data safety, do not"\r\n" use sectors 8-9 on \"Legacy\" drives,"\r\n" and avoid the final two gigabytes on"\r\n" all RocketRAID BIOS initialized drives.\n");\r\n}\r\ncase chip_6042:\r\nhpriv->ops = &mv6xxx_ops;\r\nhp_flags |= MV_HP_GEN_IIE;\r\nif (board_idx == chip_6042 && mv_pci_cut_through_okay(host))\r\nhp_flags |= MV_HP_CUT_THROUGH;\r\nswitch (pdev->revision) {\r\ncase 0x2:\r\nhp_flags |= MV_HP_ERRATA_60X1C0;\r\nbreak;\r\ndefault:\r\ndev_warn(&pdev->dev,\r\n"Applying 60X1C0 workarounds to unknown rev\n");\r\nhp_flags |= MV_HP_ERRATA_60X1C0;\r\nbreak;\r\n}\r\nbreak;\r\ncase chip_soc:\r\nif (soc_is_65n(hpriv))\r\nhpriv->ops = &mv_soc_65n_ops;\r\nelse\r\nhpriv->ops = &mv_soc_ops;\r\nhp_flags |= MV_HP_FLAG_SOC | MV_HP_GEN_IIE |\r\nMV_HP_ERRATA_60X1C0;\r\nbreak;\r\ndefault:\r\ndev_err(host->dev, "BUG: invalid board index %u\n", board_idx);\r\nreturn 1;\r\n}\r\nhpriv->hp_flags = hp_flags;\r\nif (hp_flags & MV_HP_PCIE) {\r\nhpriv->irq_cause_offset = PCIE_IRQ_CAUSE;\r\nhpriv->irq_mask_offset = PCIE_IRQ_MASK;\r\nhpriv->unmask_all_irqs = PCIE_UNMASK_ALL_IRQS;\r\n} else {\r\nhpriv->irq_cause_offset = PCI_IRQ_CAUSE;\r\nhpriv->irq_mask_offset = PCI_IRQ_MASK;\r\nhpriv->unmask_all_irqs = PCI_UNMASK_ALL_IRQS;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mv_init_host(struct ata_host *host)\r\n{\r\nint rc = 0, n_hc, port, hc;\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nvoid __iomem *mmio = hpriv->base;\r\nrc = mv_chip_id(host, hpriv->board_idx);\r\nif (rc)\r\ngoto done;\r\nif (IS_SOC(hpriv)) {\r\nhpriv->main_irq_cause_addr = mmio + SOC_HC_MAIN_IRQ_CAUSE;\r\nhpriv->main_irq_mask_addr = mmio + SOC_HC_MAIN_IRQ_MASK;\r\n} else {\r\nhpriv->main_irq_cause_addr = mmio + PCI_HC_MAIN_IRQ_CAUSE;\r\nhpriv->main_irq_mask_addr = mmio + PCI_HC_MAIN_IRQ_MASK;\r\n}\r\nhpriv->main_irq_mask = readl(hpriv->main_irq_mask_addr);\r\nmv_set_main_irq_mask(host, ~0, 0);\r\nn_hc = mv_get_hc_count(host->ports[0]->flags);\r\nfor (port = 0; port < host->n_ports; port++)\r\nif (hpriv->ops->read_preamp)\r\nhpriv->ops->read_preamp(hpriv, port, mmio);\r\nrc = hpriv->ops->reset_hc(hpriv, mmio, n_hc);\r\nif (rc)\r\ngoto done;\r\nhpriv->ops->reset_flash(hpriv, mmio);\r\nhpriv->ops->reset_bus(host, mmio);\r\nhpriv->ops->enable_leds(hpriv, mmio);\r\nfor (port = 0; port < host->n_ports; port++) {\r\nstruct ata_port *ap = host->ports[port];\r\nvoid __iomem *port_mmio = mv_port_base(mmio, port);\r\nmv_port_init(&ap->ioaddr, port_mmio);\r\n}\r\nfor (hc = 0; hc < n_hc; hc++) {\r\nvoid __iomem *hc_mmio = mv_hc_base(mmio, hc);\r\nVPRINTK("HC%i: HC config=0x%08x HC IRQ cause "\r\n"(before clear)=0x%08x\n", hc,\r\nreadl(hc_mmio + HC_CFG),\r\nreadl(hc_mmio + HC_IRQ_CAUSE));\r\nwritelfl(0, hc_mmio + HC_IRQ_CAUSE);\r\n}\r\nif (!IS_SOC(hpriv)) {\r\nwritelfl(0, mmio + hpriv->irq_cause_offset);\r\nwritelfl(hpriv->unmask_all_irqs, mmio + hpriv->irq_mask_offset);\r\n}\r\nmv_set_main_irq_mask(host, 0, PCI_ERR);\r\nmv_set_irq_coalescing(host, irq_coalescing_io_count,\r\nirq_coalescing_usecs);\r\ndone:\r\nreturn rc;\r\n}\r\nstatic int mv_create_dma_pools(struct mv_host_priv *hpriv, struct device *dev)\r\n{\r\nhpriv->crqb_pool = dmam_pool_create("crqb_q", dev, MV_CRQB_Q_SZ,\r\nMV_CRQB_Q_SZ, 0);\r\nif (!hpriv->crqb_pool)\r\nreturn -ENOMEM;\r\nhpriv->crpb_pool = dmam_pool_create("crpb_q", dev, MV_CRPB_Q_SZ,\r\nMV_CRPB_Q_SZ, 0);\r\nif (!hpriv->crpb_pool)\r\nreturn -ENOMEM;\r\nhpriv->sg_tbl_pool = dmam_pool_create("sg_tbl", dev, MV_SG_TBL_SZ,\r\nMV_SG_TBL_SZ, 0);\r\nif (!hpriv->sg_tbl_pool)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void mv_conf_mbus_windows(struct mv_host_priv *hpriv,\r\nconst struct mbus_dram_target_info *dram)\r\n{\r\nint i;\r\nfor (i = 0; i < 4; i++) {\r\nwritel(0, hpriv->base + WINDOW_CTRL(i));\r\nwritel(0, hpriv->base + WINDOW_BASE(i));\r\n}\r\nfor (i = 0; i < dram->num_cs; i++) {\r\nconst struct mbus_dram_window *cs = dram->cs + i;\r\nwritel(((cs->size - 1) & 0xffff0000) |\r\n(cs->mbus_attr << 8) |\r\n(dram->mbus_dram_target_id << 4) | 1,\r\nhpriv->base + WINDOW_CTRL(i));\r\nwritel(cs->base, hpriv->base + WINDOW_BASE(i));\r\n}\r\n}\r\nstatic int mv_platform_probe(struct platform_device *pdev)\r\n{\r\nconst struct mv_sata_platform_data *mv_platform_data;\r\nconst struct mbus_dram_target_info *dram;\r\nconst struct ata_port_info *ppi[] =\r\n{ &mv_port_info[chip_soc], NULL };\r\nstruct ata_host *host;\r\nstruct mv_host_priv *hpriv;\r\nstruct resource *res;\r\nint n_ports = 0, irq = 0;\r\nint rc;\r\nint port;\r\nata_print_version_once(&pdev->dev, DRV_VERSION);\r\nif (unlikely(pdev->num_resources != 2)) {\r\ndev_err(&pdev->dev, "invalid number of resources\n");\r\nreturn -EINVAL;\r\n}\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (res == NULL)\r\nreturn -EINVAL;\r\nif (pdev->dev.of_node) {\r\nof_property_read_u32(pdev->dev.of_node, "nr-ports", &n_ports);\r\nirq = irq_of_parse_and_map(pdev->dev.of_node, 0);\r\n} else {\r\nmv_platform_data = dev_get_platdata(&pdev->dev);\r\nn_ports = mv_platform_data->n_ports;\r\nirq = platform_get_irq(pdev, 0);\r\n}\r\nhost = ata_host_alloc_pinfo(&pdev->dev, ppi, n_ports);\r\nhpriv = devm_kzalloc(&pdev->dev, sizeof(*hpriv), GFP_KERNEL);\r\nif (!host || !hpriv)\r\nreturn -ENOMEM;\r\nhpriv->port_clks = devm_kzalloc(&pdev->dev,\r\nsizeof(struct clk *) * n_ports,\r\nGFP_KERNEL);\r\nif (!hpriv->port_clks)\r\nreturn -ENOMEM;\r\nhpriv->port_phys = devm_kzalloc(&pdev->dev,\r\nsizeof(struct phy *) * n_ports,\r\nGFP_KERNEL);\r\nif (!hpriv->port_phys)\r\nreturn -ENOMEM;\r\nhost->private_data = hpriv;\r\nhpriv->board_idx = chip_soc;\r\nhost->iomap = NULL;\r\nhpriv->base = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nhpriv->base -= SATAHC0_REG_BASE;\r\nhpriv->clk = clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(hpriv->clk))\r\ndev_notice(&pdev->dev, "cannot get optional clkdev\n");\r\nelse\r\nclk_prepare_enable(hpriv->clk);\r\nfor (port = 0; port < n_ports; port++) {\r\nchar port_number[16];\r\nsprintf(port_number, "%d", port);\r\nhpriv->port_clks[port] = clk_get(&pdev->dev, port_number);\r\nif (!IS_ERR(hpriv->port_clks[port]))\r\nclk_prepare_enable(hpriv->port_clks[port]);\r\nsprintf(port_number, "port%d", port);\r\nhpriv->port_phys[port] = devm_phy_optional_get(&pdev->dev,\r\nport_number);\r\nif (IS_ERR(hpriv->port_phys[port])) {\r\nrc = PTR_ERR(hpriv->port_phys[port]);\r\nhpriv->port_phys[port] = NULL;\r\nif (rc != -EPROBE_DEFER)\r\ndev_warn(&pdev->dev, "error getting phy %d", rc);\r\nhpriv->n_ports = port;\r\ngoto err;\r\n} else\r\nphy_power_on(hpriv->port_phys[port]);\r\n}\r\nhpriv->n_ports = n_ports;\r\ndram = mv_mbus_dram_info();\r\nif (dram)\r\nmv_conf_mbus_windows(hpriv, dram);\r\nrc = mv_create_dma_pools(hpriv, &pdev->dev);\r\nif (rc)\r\ngoto err;\r\nif (pdev->dev.of_node &&\r\nof_device_is_compatible(pdev->dev.of_node,\r\n"marvell,armada-370-sata"))\r\nhpriv->hp_flags |= MV_HP_FIX_LP_PHY_CTL;\r\nrc = mv_init_host(host);\r\nif (rc)\r\ngoto err;\r\ndev_info(&pdev->dev, "slots %u ports %d\n",\r\n(unsigned)MV_MAX_Q_DEPTH, host->n_ports);\r\nrc = ata_host_activate(host, irq, mv_interrupt, IRQF_SHARED, &mv6_sht);\r\nif (!rc)\r\nreturn 0;\r\nerr:\r\nif (!IS_ERR(hpriv->clk)) {\r\nclk_disable_unprepare(hpriv->clk);\r\nclk_put(hpriv->clk);\r\n}\r\nfor (port = 0; port < hpriv->n_ports; port++) {\r\nif (!IS_ERR(hpriv->port_clks[port])) {\r\nclk_disable_unprepare(hpriv->port_clks[port]);\r\nclk_put(hpriv->port_clks[port]);\r\n}\r\nif (hpriv->port_phys[port])\r\nphy_power_off(hpriv->port_phys[port]);\r\n}\r\nreturn rc;\r\n}\r\nstatic int mv_platform_remove(struct platform_device *pdev)\r\n{\r\nstruct ata_host *host = platform_get_drvdata(pdev);\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nint port;\r\nata_host_detach(host);\r\nif (!IS_ERR(hpriv->clk)) {\r\nclk_disable_unprepare(hpriv->clk);\r\nclk_put(hpriv->clk);\r\n}\r\nfor (port = 0; port < host->n_ports; port++) {\r\nif (!IS_ERR(hpriv->port_clks[port])) {\r\nclk_disable_unprepare(hpriv->port_clks[port]);\r\nclk_put(hpriv->port_clks[port]);\r\n}\r\nif (hpriv->port_phys[port])\r\nphy_power_off(hpriv->port_phys[port]);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mv_platform_suspend(struct platform_device *pdev, pm_message_t state)\r\n{\r\nstruct ata_host *host = platform_get_drvdata(pdev);\r\nif (host)\r\nreturn ata_host_suspend(host, state);\r\nelse\r\nreturn 0;\r\n}\r\nstatic int mv_platform_resume(struct platform_device *pdev)\r\n{\r\nstruct ata_host *host = platform_get_drvdata(pdev);\r\nconst struct mbus_dram_target_info *dram;\r\nint ret;\r\nif (host) {\r\nstruct mv_host_priv *hpriv = host->private_data;\r\ndram = mv_mbus_dram_info();\r\nif (dram)\r\nmv_conf_mbus_windows(hpriv, dram);\r\nret = mv_init_host(host);\r\nif (ret) {\r\nprintk(KERN_ERR DRV_NAME ": Error during HW init\n");\r\nreturn ret;\r\n}\r\nata_host_resume(host);\r\n}\r\nreturn 0;\r\n}\r\nstatic int pci_go_64(struct pci_dev *pdev)\r\n{\r\nint rc;\r\nif (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (rc) {\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\ndev_err(&pdev->dev,\r\n"64-bit DMA enable failed\n");\r\nreturn rc;\r\n}\r\n}\r\n} else {\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\ndev_err(&pdev->dev, "32-bit DMA enable failed\n");\r\nreturn rc;\r\n}\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\ndev_err(&pdev->dev,\r\n"32-bit consistent DMA enable failed\n");\r\nreturn rc;\r\n}\r\n}\r\nreturn rc;\r\n}\r\nstatic void mv_print_info(struct ata_host *host)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(host->dev);\r\nstruct mv_host_priv *hpriv = host->private_data;\r\nu8 scc;\r\nconst char *scc_s, *gen;\r\npci_read_config_byte(pdev, PCI_CLASS_DEVICE, &scc);\r\nif (scc == 0)\r\nscc_s = "SCSI";\r\nelse if (scc == 0x01)\r\nscc_s = "RAID";\r\nelse\r\nscc_s = "?";\r\nif (IS_GEN_I(hpriv))\r\ngen = "I";\r\nelse if (IS_GEN_II(hpriv))\r\ngen = "II";\r\nelse if (IS_GEN_IIE(hpriv))\r\ngen = "IIE";\r\nelse\r\ngen = "?";\r\ndev_info(&pdev->dev, "Gen-%s %u slots %u ports %s mode IRQ via %s\n",\r\ngen, (unsigned)MV_MAX_Q_DEPTH, host->n_ports,\r\nscc_s, (MV_HP_FLAG_MSI & hpriv->hp_flags) ? "MSI" : "INTx");\r\n}\r\nstatic int mv_pci_init_one(struct pci_dev *pdev,\r\nconst struct pci_device_id *ent)\r\n{\r\nunsigned int board_idx = (unsigned int)ent->driver_data;\r\nconst struct ata_port_info *ppi[] = { &mv_port_info[board_idx], NULL };\r\nstruct ata_host *host;\r\nstruct mv_host_priv *hpriv;\r\nint n_ports, port, rc;\r\nata_print_version_once(&pdev->dev, DRV_VERSION);\r\nn_ports = mv_get_hc_count(ppi[0]->flags) * MV_PORTS_PER_HC;\r\nhost = ata_host_alloc_pinfo(&pdev->dev, ppi, n_ports);\r\nhpriv = devm_kzalloc(&pdev->dev, sizeof(*hpriv), GFP_KERNEL);\r\nif (!host || !hpriv)\r\nreturn -ENOMEM;\r\nhost->private_data = hpriv;\r\nhpriv->n_ports = n_ports;\r\nhpriv->board_idx = board_idx;\r\nrc = pcim_enable_device(pdev);\r\nif (rc)\r\nreturn rc;\r\nrc = pcim_iomap_regions(pdev, 1 << MV_PRIMARY_BAR, DRV_NAME);\r\nif (rc == -EBUSY)\r\npcim_pin_device(pdev);\r\nif (rc)\r\nreturn rc;\r\nhost->iomap = pcim_iomap_table(pdev);\r\nhpriv->base = host->iomap[MV_PRIMARY_BAR];\r\nrc = pci_go_64(pdev);\r\nif (rc)\r\nreturn rc;\r\nrc = mv_create_dma_pools(hpriv, &pdev->dev);\r\nif (rc)\r\nreturn rc;\r\nfor (port = 0; port < host->n_ports; port++) {\r\nstruct ata_port *ap = host->ports[port];\r\nvoid __iomem *port_mmio = mv_port_base(hpriv->base, port);\r\nunsigned int offset = port_mmio - hpriv->base;\r\nata_port_pbar_desc(ap, MV_PRIMARY_BAR, -1, "mmio");\r\nata_port_pbar_desc(ap, MV_PRIMARY_BAR, offset, "port");\r\n}\r\nrc = mv_init_host(host);\r\nif (rc)\r\nreturn rc;\r\nif (msi && pci_enable_msi(pdev) == 0)\r\nhpriv->hp_flags |= MV_HP_FLAG_MSI;\r\nmv_dump_pci_cfg(pdev, 0x68);\r\nmv_print_info(host);\r\npci_set_master(pdev);\r\npci_try_set_mwi(pdev);\r\nreturn ata_host_activate(host, pdev->irq, mv_interrupt, IRQF_SHARED,\r\nIS_GEN_I(hpriv) ? &mv5_sht : &mv6_sht);\r\n}\r\nstatic int mv_pci_device_resume(struct pci_dev *pdev)\r\n{\r\nstruct ata_host *host = pci_get_drvdata(pdev);\r\nint rc;\r\nrc = ata_pci_device_do_resume(pdev);\r\nif (rc)\r\nreturn rc;\r\nrc = mv_init_host(host);\r\nif (rc)\r\nreturn rc;\r\nata_host_resume(host);\r\nreturn 0;\r\n}\r\nstatic int __init mv_init(void)\r\n{\r\nint rc = -ENODEV;\r\n#ifdef CONFIG_PCI\r\nrc = pci_register_driver(&mv_pci_driver);\r\nif (rc < 0)\r\nreturn rc;\r\n#endif\r\nrc = platform_driver_register(&mv_platform_driver);\r\n#ifdef CONFIG_PCI\r\nif (rc < 0)\r\npci_unregister_driver(&mv_pci_driver);\r\n#endif\r\nreturn rc;\r\n}\r\nstatic void __exit mv_exit(void)\r\n{\r\n#ifdef CONFIG_PCI\r\npci_unregister_driver(&mv_pci_driver);\r\n#endif\r\nplatform_driver_unregister(&mv_platform_driver);\r\n}
