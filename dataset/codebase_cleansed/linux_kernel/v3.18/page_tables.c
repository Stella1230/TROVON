static pgd_t *spgd_addr(struct lg_cpu *cpu, u32 i, unsigned long vaddr)\r\n{\r\nunsigned int index = pgd_index(vaddr);\r\nreturn &cpu->lg->pgdirs[i].pgdir[index];\r\n}\r\nstatic pmd_t *spmd_addr(struct lg_cpu *cpu, pgd_t spgd, unsigned long vaddr)\r\n{\r\nunsigned int index = pmd_index(vaddr);\r\npmd_t *page;\r\nBUG_ON(!(pgd_flags(spgd) & _PAGE_PRESENT));\r\npage = __va(pgd_pfn(spgd) << PAGE_SHIFT);\r\nreturn &page[index];\r\n}\r\nstatic pte_t *spte_addr(struct lg_cpu *cpu, pgd_t spgd, unsigned long vaddr)\r\n{\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *pmd = spmd_addr(cpu, spgd, vaddr);\r\npte_t *page = __va(pmd_pfn(*pmd) << PAGE_SHIFT);\r\nBUG_ON(!(pmd_flags(*pmd) & _PAGE_PRESENT));\r\n#else\r\npte_t *page = __va(pgd_pfn(spgd) << PAGE_SHIFT);\r\nBUG_ON(!(pgd_flags(spgd) & _PAGE_PRESENT));\r\n#endif\r\nreturn &page[pte_index(vaddr)];\r\n}\r\nstatic unsigned long gpgd_addr(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\nunsigned int index = vaddr >> (PGDIR_SHIFT);\r\nreturn cpu->lg->pgdirs[cpu->cpu_pgd].gpgdir + index * sizeof(pgd_t);\r\n}\r\nstatic unsigned long gpmd_addr(pgd_t gpgd, unsigned long vaddr)\r\n{\r\nunsigned long gpage = pgd_pfn(gpgd) << PAGE_SHIFT;\r\nBUG_ON(!(pgd_flags(gpgd) & _PAGE_PRESENT));\r\nreturn gpage + pmd_index(vaddr) * sizeof(pmd_t);\r\n}\r\nstatic unsigned long gpte_addr(struct lg_cpu *cpu,\r\npmd_t gpmd, unsigned long vaddr)\r\n{\r\nunsigned long gpage = pmd_pfn(gpmd) << PAGE_SHIFT;\r\nBUG_ON(!(pmd_flags(gpmd) & _PAGE_PRESENT));\r\nreturn gpage + pte_index(vaddr) * sizeof(pte_t);\r\n}\r\nstatic unsigned long gpte_addr(struct lg_cpu *cpu,\r\npgd_t gpgd, unsigned long vaddr)\r\n{\r\nunsigned long gpage = pgd_pfn(gpgd) << PAGE_SHIFT;\r\nBUG_ON(!(pgd_flags(gpgd) & _PAGE_PRESENT));\r\nreturn gpage + pte_index(vaddr) * sizeof(pte_t);\r\n}\r\nstatic unsigned long get_pfn(unsigned long virtpfn, int write)\r\n{\r\nstruct page *page;\r\nif (get_user_pages_fast(virtpfn << PAGE_SHIFT, 1, write, &page) == 1)\r\nreturn page_to_pfn(page);\r\nreturn -1UL;\r\n}\r\nstatic pte_t gpte_to_spte(struct lg_cpu *cpu, pte_t gpte, int write)\r\n{\r\nunsigned long pfn, base, flags;\r\nflags = (pte_flags(gpte) & ~_PAGE_GLOBAL);\r\nbase = (unsigned long)cpu->lg->mem_base / PAGE_SIZE;\r\npfn = get_pfn(base + pte_pfn(gpte), write);\r\nif (pfn == -1UL) {\r\nkill_guest(cpu, "failed to get page %lu", pte_pfn(gpte));\r\nflags = 0;\r\n}\r\nreturn pfn_pte(pfn, __pgprot(flags));\r\n}\r\nstatic void release_pte(pte_t pte)\r\n{\r\nif (pte_flags(pte) & _PAGE_PRESENT)\r\nput_page(pte_page(pte));\r\n}\r\nstatic bool check_gpte(struct lg_cpu *cpu, pte_t gpte)\r\n{\r\nif ((pte_flags(gpte) & _PAGE_PSE) ||\r\npte_pfn(gpte) >= cpu->lg->pfn_limit) {\r\nkill_guest(cpu, "bad page table entry");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic bool check_gpgd(struct lg_cpu *cpu, pgd_t gpgd)\r\n{\r\nif ((pgd_flags(gpgd) & ~CHECK_GPGD_MASK) ||\r\n(pgd_pfn(gpgd) >= cpu->lg->pfn_limit)) {\r\nkill_guest(cpu, "bad page directory entry");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic bool check_gpmd(struct lg_cpu *cpu, pmd_t gpmd)\r\n{\r\nif ((pmd_flags(gpmd) & ~_PAGE_TABLE) ||\r\n(pmd_pfn(gpmd) >= cpu->lg->pfn_limit)) {\r\nkill_guest(cpu, "bad page middle directory entry");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic pte_t *find_spte(struct lg_cpu *cpu, unsigned long vaddr, bool allocate,\r\nint pgd_flags, int pmd_flags)\r\n{\r\npgd_t *spgd;\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *spmd;\r\n#endif\r\nspgd = spgd_addr(cpu, cpu->cpu_pgd, vaddr);\r\nif (!(pgd_flags(*spgd) & _PAGE_PRESENT)) {\r\nunsigned long ptepage;\r\nif (!allocate)\r\nreturn NULL;\r\nptepage = get_zeroed_page(GFP_KERNEL);\r\nif (!ptepage) {\r\nkill_guest(cpu, "out of memory allocating pte page");\r\nreturn NULL;\r\n}\r\nset_pgd(spgd, __pgd(__pa(ptepage) | pgd_flags));\r\n}\r\n#ifdef CONFIG_X86_PAE\r\nspmd = spmd_addr(cpu, *spgd, vaddr);\r\nif (!(pmd_flags(*spmd) & _PAGE_PRESENT)) {\r\nunsigned long ptepage;\r\nif (!allocate)\r\nreturn NULL;\r\nptepage = get_zeroed_page(GFP_KERNEL);\r\nif (!ptepage) {\r\nkill_guest(cpu, "out of memory allocating pmd page");\r\nreturn NULL;\r\n}\r\nset_pmd(spmd, __pmd(__pa(ptepage) | pmd_flags));\r\n}\r\n#endif\r\nreturn spte_addr(cpu, *spgd, vaddr);\r\n}\r\nbool demand_page(struct lg_cpu *cpu, unsigned long vaddr, int errcode)\r\n{\r\nunsigned long gpte_ptr;\r\npte_t gpte;\r\npte_t *spte;\r\npmd_t gpmd;\r\npgd_t gpgd;\r\nif (vaddr >= switcher_addr)\r\nreturn false;\r\nif (unlikely(cpu->linear_pages)) {\r\ngpgd = __pgd(CHECK_GPGD_MASK);\r\n} else {\r\ngpgd = lgread(cpu, gpgd_addr(cpu, vaddr), pgd_t);\r\nif (!(pgd_flags(gpgd) & _PAGE_PRESENT))\r\nreturn false;\r\nif (!check_gpgd(cpu, gpgd))\r\nreturn false;\r\n}\r\ngpmd = __pmd(_PAGE_TABLE);\r\n#ifdef CONFIG_X86_PAE\r\nif (likely(!cpu->linear_pages)) {\r\ngpmd = lgread(cpu, gpmd_addr(gpgd, vaddr), pmd_t);\r\nif (!(pmd_flags(gpmd) & _PAGE_PRESENT))\r\nreturn false;\r\nif (!check_gpmd(cpu, gpmd))\r\nreturn false;\r\n}\r\ngpte_ptr = gpte_addr(cpu, gpmd, vaddr);\r\n#else\r\ngpte_ptr = gpte_addr(cpu, gpgd, vaddr);\r\n#endif\r\nif (unlikely(cpu->linear_pages)) {\r\ngpte = __pte((vaddr & PAGE_MASK) | _PAGE_RW | _PAGE_PRESENT);\r\n} else {\r\ngpte = lgread(cpu, gpte_ptr, pte_t);\r\n}\r\nif (!(pte_flags(gpte) & _PAGE_PRESENT))\r\nreturn false;\r\nif ((errcode & 2) && !(pte_flags(gpte) & _PAGE_RW))\r\nreturn false;\r\nif ((errcode & 4) && !(pte_flags(gpte) & _PAGE_USER))\r\nreturn false;\r\nif (!check_gpte(cpu, gpte))\r\nreturn false;\r\ngpte = pte_mkyoung(gpte);\r\nif (errcode & 2)\r\ngpte = pte_mkdirty(gpte);\r\nspte = find_spte(cpu, vaddr, true, pgd_flags(gpgd), pmd_flags(gpmd));\r\nif (!spte)\r\nreturn false;\r\nrelease_pte(*spte);\r\nif (pte_dirty(gpte))\r\n*spte = gpte_to_spte(cpu, gpte, 1);\r\nelse\r\nset_pte(spte, gpte_to_spte(cpu, pte_wrprotect(gpte), 0));\r\nif (likely(!cpu->linear_pages))\r\nlgwrite(cpu, gpte_ptr, pte_t, gpte);\r\nreturn true;\r\n}\r\nstatic bool page_writable(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\npte_t *spte;\r\nunsigned long flags;\r\nif (vaddr >= switcher_addr)\r\nreturn false;\r\nspte = find_spte(cpu, vaddr, false, 0, 0);\r\nif (!spte)\r\nreturn false;\r\nflags = pte_flags(*spte);\r\nreturn (flags & (_PAGE_PRESENT|_PAGE_RW)) == (_PAGE_PRESENT|_PAGE_RW);\r\n}\r\nvoid pin_page(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\nif (!page_writable(cpu, vaddr) && !demand_page(cpu, vaddr, 2))\r\nkill_guest(cpu, "bad stack page %#lx", vaddr);\r\n}\r\nstatic void release_pmd(pmd_t *spmd)\r\n{\r\nif (pmd_flags(*spmd) & _PAGE_PRESENT) {\r\nunsigned int i;\r\npte_t *ptepage = __va(pmd_pfn(*spmd) << PAGE_SHIFT);\r\nfor (i = 0; i < PTRS_PER_PTE; i++)\r\nrelease_pte(ptepage[i]);\r\nfree_page((long)ptepage);\r\nset_pmd(spmd, __pmd(0));\r\n}\r\n}\r\nstatic void release_pgd(pgd_t *spgd)\r\n{\r\nif (pgd_flags(*spgd) & _PAGE_PRESENT) {\r\nunsigned int i;\r\npmd_t *pmdpage = __va(pgd_pfn(*spgd) << PAGE_SHIFT);\r\nfor (i = 0; i < PTRS_PER_PMD; i++)\r\nrelease_pmd(&pmdpage[i]);\r\nfree_page((long)pmdpage);\r\nset_pgd(spgd, __pgd(0));\r\n}\r\n}\r\nstatic void release_pgd(pgd_t *spgd)\r\n{\r\nif (pgd_flags(*spgd) & _PAGE_PRESENT) {\r\nunsigned int i;\r\npte_t *ptepage = __va(pgd_pfn(*spgd) << PAGE_SHIFT);\r\nfor (i = 0; i < PTRS_PER_PTE; i++)\r\nrelease_pte(ptepage[i]);\r\nfree_page((long)ptepage);\r\n*spgd = __pgd(0);\r\n}\r\n}\r\nstatic void flush_user_mappings(struct lguest *lg, int idx)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < pgd_index(lg->kernel_address); i++)\r\nrelease_pgd(lg->pgdirs[idx].pgdir + i);\r\n}\r\nvoid guest_pagetable_flush_user(struct lg_cpu *cpu)\r\n{\r\nflush_user_mappings(cpu->lg, cpu->cpu_pgd);\r\n}\r\nunsigned long guest_pa(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\npgd_t gpgd;\r\npte_t gpte;\r\n#ifdef CONFIG_X86_PAE\r\npmd_t gpmd;\r\n#endif\r\nif (unlikely(cpu->linear_pages))\r\nreturn vaddr;\r\ngpgd = lgread(cpu, gpgd_addr(cpu, vaddr), pgd_t);\r\nif (!(pgd_flags(gpgd) & _PAGE_PRESENT)) {\r\nkill_guest(cpu, "Bad address %#lx", vaddr);\r\nreturn -1UL;\r\n}\r\n#ifdef CONFIG_X86_PAE\r\ngpmd = lgread(cpu, gpmd_addr(gpgd, vaddr), pmd_t);\r\nif (!(pmd_flags(gpmd) & _PAGE_PRESENT)) {\r\nkill_guest(cpu, "Bad address %#lx", vaddr);\r\nreturn -1UL;\r\n}\r\ngpte = lgread(cpu, gpte_addr(cpu, gpmd, vaddr), pte_t);\r\n#else\r\ngpte = lgread(cpu, gpte_addr(cpu, gpgd, vaddr), pte_t);\r\n#endif\r\nif (!(pte_flags(gpte) & _PAGE_PRESENT))\r\nkill_guest(cpu, "Bad address %#lx", vaddr);\r\nreturn pte_pfn(gpte) * PAGE_SIZE | (vaddr & ~PAGE_MASK);\r\n}\r\nstatic unsigned int find_pgdir(struct lguest *lg, unsigned long pgtable)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(lg->pgdirs); i++)\r\nif (lg->pgdirs[i].pgdir && lg->pgdirs[i].gpgdir == pgtable)\r\nbreak;\r\nreturn i;\r\n}\r\nstatic unsigned int new_pgdir(struct lg_cpu *cpu,\r\nunsigned long gpgdir,\r\nint *blank_pgdir)\r\n{\r\nunsigned int next;\r\nnext = prandom_u32() % ARRAY_SIZE(cpu->lg->pgdirs);\r\nif (!cpu->lg->pgdirs[next].pgdir) {\r\ncpu->lg->pgdirs[next].pgdir =\r\n(pgd_t *)get_zeroed_page(GFP_KERNEL);\r\nif (!cpu->lg->pgdirs[next].pgdir)\r\nnext = cpu->cpu_pgd;\r\nelse {\r\n*blank_pgdir = 1;\r\n}\r\n}\r\ncpu->lg->pgdirs[next].gpgdir = gpgdir;\r\nflush_user_mappings(cpu->lg, next);\r\ncpu->lg->pgdirs[next].last_host_cpu = -1;\r\nreturn next;\r\n}\r\nstatic bool allocate_switcher_mapping(struct lg_cpu *cpu)\r\n{\r\nint i;\r\nfor (i = 0; i < TOTAL_SWITCHER_PAGES; i++) {\r\npte_t *pte = find_spte(cpu, switcher_addr + i * PAGE_SIZE, true,\r\nCHECK_GPGD_MASK, _PAGE_TABLE);\r\nif (!pte)\r\nreturn false;\r\nif (i == 0 && !(pte_flags(*pte) & _PAGE_PRESENT)) {\r\nget_page(lg_switcher_pages[0]);\r\nset_pte(pte,\r\nmk_pte(lg_switcher_pages[0], PAGE_KERNEL_RX));\r\n}\r\n}\r\ncpu->lg->pgdirs[cpu->cpu_pgd].switcher_mapped = true;\r\nreturn true;\r\n}\r\nstatic void release_all_pagetables(struct lguest *lg)\r\n{\r\nunsigned int i, j;\r\nfor (i = 0; i < ARRAY_SIZE(lg->pgdirs); i++) {\r\nif (!lg->pgdirs[i].pgdir)\r\ncontinue;\r\nfor (j = 0; j < PTRS_PER_PGD; j++)\r\nrelease_pgd(lg->pgdirs[i].pgdir + j);\r\nlg->pgdirs[i].switcher_mapped = false;\r\nlg->pgdirs[i].last_host_cpu = -1;\r\n}\r\n}\r\nvoid guest_pagetable_clear_all(struct lg_cpu *cpu)\r\n{\r\nrelease_all_pagetables(cpu->lg);\r\npin_stack_pages(cpu);\r\nif (!allocate_switcher_mapping(cpu))\r\nkill_guest(cpu, "Cannot populate switcher mapping");\r\n}\r\nvoid guest_new_pagetable(struct lg_cpu *cpu, unsigned long pgtable)\r\n{\r\nint newpgdir, repin = 0;\r\nif (unlikely(cpu->linear_pages)) {\r\nrelease_all_pagetables(cpu->lg);\r\ncpu->linear_pages = false;\r\nnewpgdir = ARRAY_SIZE(cpu->lg->pgdirs);\r\n} else {\r\nnewpgdir = find_pgdir(cpu->lg, pgtable);\r\n}\r\nif (newpgdir == ARRAY_SIZE(cpu->lg->pgdirs))\r\nnewpgdir = new_pgdir(cpu, pgtable, &repin);\r\ncpu->cpu_pgd = newpgdir;\r\nif (repin)\r\npin_stack_pages(cpu);\r\nif (!cpu->lg->pgdirs[cpu->cpu_pgd].switcher_mapped) {\r\nif (!allocate_switcher_mapping(cpu))\r\nkill_guest(cpu, "Cannot populate switcher mapping");\r\n}\r\n}\r\nstatic void __guest_set_pte(struct lg_cpu *cpu, int idx,\r\nunsigned long vaddr, pte_t gpte)\r\n{\r\npgd_t *spgd = spgd_addr(cpu, idx, vaddr);\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *spmd;\r\n#endif\r\nif (pgd_flags(*spgd) & _PAGE_PRESENT) {\r\n#ifdef CONFIG_X86_PAE\r\nspmd = spmd_addr(cpu, *spgd, vaddr);\r\nif (pmd_flags(*spmd) & _PAGE_PRESENT) {\r\n#endif\r\npte_t *spte = spte_addr(cpu, *spgd, vaddr);\r\nrelease_pte(*spte);\r\nif (pte_flags(gpte) & (_PAGE_DIRTY | _PAGE_ACCESSED)) {\r\nif (!check_gpte(cpu, gpte))\r\nreturn;\r\nset_pte(spte,\r\ngpte_to_spte(cpu, gpte,\r\npte_flags(gpte) & _PAGE_DIRTY));\r\n} else {\r\nset_pte(spte, __pte(0));\r\n}\r\n#ifdef CONFIG_X86_PAE\r\n}\r\n#endif\r\n}\r\n}\r\nvoid guest_set_pte(struct lg_cpu *cpu,\r\nunsigned long gpgdir, unsigned long vaddr, pte_t gpte)\r\n{\r\nif (vaddr >= switcher_addr) {\r\nkill_guest(cpu, "attempt to set pte into Switcher pages");\r\nreturn;\r\n}\r\nif (vaddr >= cpu->lg->kernel_address) {\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(cpu->lg->pgdirs); i++)\r\nif (cpu->lg->pgdirs[i].pgdir)\r\n__guest_set_pte(cpu, i, vaddr, gpte);\r\n} else {\r\nint pgdir = find_pgdir(cpu->lg, gpgdir);\r\nif (pgdir != ARRAY_SIZE(cpu->lg->pgdirs))\r\n__guest_set_pte(cpu, pgdir, vaddr, gpte);\r\n}\r\n}\r\nvoid guest_set_pgd(struct lguest *lg, unsigned long gpgdir, u32 idx)\r\n{\r\nint pgdir;\r\nif (idx > PTRS_PER_PGD) {\r\nkill_guest(&lg->cpus[0], "Attempt to set pgd %u/%u",\r\nidx, PTRS_PER_PGD);\r\nreturn;\r\n}\r\npgdir = find_pgdir(lg, gpgdir);\r\nif (pgdir < ARRAY_SIZE(lg->pgdirs)) {\r\nrelease_pgd(lg->pgdirs[pgdir].pgdir + idx);\r\nif (!allocate_switcher_mapping(&lg->cpus[0])) {\r\nkill_guest(&lg->cpus[0],\r\n"Cannot populate switcher mapping");\r\n}\r\nlg->pgdirs[pgdir].last_host_cpu = -1;\r\n}\r\n}\r\nvoid guest_set_pmd(struct lguest *lg, unsigned long pmdp, u32 idx)\r\n{\r\nguest_pagetable_clear_all(&lg->cpus[0]);\r\n}\r\nint init_guest_pagetable(struct lguest *lg)\r\n{\r\nstruct lg_cpu *cpu = &lg->cpus[0];\r\nint allocated = 0;\r\ncpu->cpu_pgd = new_pgdir(cpu, 0, &allocated);\r\nif (!allocated)\r\nreturn -ENOMEM;\r\ncpu->linear_pages = true;\r\nif (!allocate_switcher_mapping(cpu)) {\r\nrelease_all_pagetables(lg);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nvoid page_table_guest_data_init(struct lg_cpu *cpu)\r\n{\r\nu32 top = ~switcher_addr + 1;\r\nif (get_user(cpu->lg->kernel_address,\r\n&cpu->lg->lguest_data->kernel_address)\r\n|| put_user(top, &cpu->lg->lguest_data->reserve_mem)) {\r\nkill_guest(cpu, "bad guest page %p", cpu->lg->lguest_data);\r\nreturn;\r\n}\r\nif (cpu->lg->kernel_address >= switcher_addr)\r\nkill_guest(cpu, "bad kernel address %#lx",\r\ncpu->lg->kernel_address);\r\n}\r\nvoid free_guest_pagetable(struct lguest *lg)\r\n{\r\nunsigned int i;\r\nrelease_all_pagetables(lg);\r\nfor (i = 0; i < ARRAY_SIZE(lg->pgdirs); i++)\r\nfree_page((long)lg->pgdirs[i].pgdir);\r\n}\r\nstatic void remove_switcher_percpu_map(struct lg_cpu *cpu, unsigned int i)\r\n{\r\nunsigned long base = switcher_addr + PAGE_SIZE + i * PAGE_SIZE*2;\r\npte_t *pte;\r\npte = find_spte(cpu, base, false, 0, 0);\r\nrelease_pte(*pte);\r\nset_pte(pte, __pte(0));\r\npte = find_spte(cpu, base + PAGE_SIZE, false, 0, 0);\r\nrelease_pte(*pte);\r\nset_pte(pte, __pte(0));\r\n}\r\nvoid map_switcher_in_guest(struct lg_cpu *cpu, struct lguest_pages *pages)\r\n{\r\nunsigned long base;\r\nstruct page *percpu_switcher_page, *regs_page;\r\npte_t *pte;\r\nstruct pgdir *pgdir = &cpu->lg->pgdirs[cpu->cpu_pgd];\r\nBUG_ON(!pgdir->switcher_mapped);\r\nif (pgdir->last_host_cpu == raw_smp_processor_id())\r\nreturn;\r\nif (pgdir->last_host_cpu == -1) {\r\nunsigned int i;\r\nfor_each_possible_cpu(i)\r\nremove_switcher_percpu_map(cpu, i);\r\n} else {\r\nremove_switcher_percpu_map(cpu, pgdir->last_host_cpu);\r\n}\r\nbase = switcher_addr + PAGE_SIZE\r\n+ raw_smp_processor_id() * sizeof(struct lguest_pages);\r\npte = find_spte(cpu, base, false, 0, 0);\r\nregs_page = pfn_to_page(__pa(cpu->regs_page) >> PAGE_SHIFT);\r\nget_page(regs_page);\r\nset_pte(pte, mk_pte(regs_page, __pgprot(__PAGE_KERNEL & ~_PAGE_GLOBAL)));\r\npte = find_spte(cpu, base + PAGE_SIZE, false, 0, 0);\r\npercpu_switcher_page\r\n= lg_switcher_pages[1 + raw_smp_processor_id()*2 + 1];\r\nget_page(percpu_switcher_page);\r\nset_pte(pte, mk_pte(percpu_switcher_page,\r\n__pgprot(__PAGE_KERNEL_RO & ~_PAGE_GLOBAL)));\r\npgdir->last_host_cpu = raw_smp_processor_id();\r\n}
