static inline void tk_normalize_xtime(struct timekeeper *tk)\r\n{\r\nwhile (tk->tkr.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr.shift)) {\r\ntk->tkr.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr.shift;\r\ntk->xtime_sec++;\r\n}\r\n}\r\nstatic inline struct timespec64 tk_xtime(struct timekeeper *tk)\r\n{\r\nstruct timespec64 ts;\r\nts.tv_sec = tk->xtime_sec;\r\nts.tv_nsec = (long)(tk->tkr.xtime_nsec >> tk->tkr.shift);\r\nreturn ts;\r\n}\r\nstatic void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)\r\n{\r\ntk->xtime_sec = ts->tv_sec;\r\ntk->tkr.xtime_nsec = (u64)ts->tv_nsec << tk->tkr.shift;\r\n}\r\nstatic void tk_xtime_add(struct timekeeper *tk, const struct timespec64 *ts)\r\n{\r\ntk->xtime_sec += ts->tv_sec;\r\ntk->tkr.xtime_nsec += (u64)ts->tv_nsec << tk->tkr.shift;\r\ntk_normalize_xtime(tk);\r\n}\r\nstatic void tk_set_wall_to_mono(struct timekeeper *tk, struct timespec64 wtm)\r\n{\r\nstruct timespec64 tmp;\r\nset_normalized_timespec64(&tmp, -tk->wall_to_monotonic.tv_sec,\r\n-tk->wall_to_monotonic.tv_nsec);\r\nWARN_ON_ONCE(tk->offs_real.tv64 != timespec64_to_ktime(tmp).tv64);\r\ntk->wall_to_monotonic = wtm;\r\nset_normalized_timespec64(&tmp, -wtm.tv_sec, -wtm.tv_nsec);\r\ntk->offs_real = timespec64_to_ktime(tmp);\r\ntk->offs_tai = ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0));\r\n}\r\nstatic inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)\r\n{\r\ntk->offs_boot = ktime_add(tk->offs_boot, delta);\r\n}\r\nstatic void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)\r\n{\r\ncycle_t interval;\r\nu64 tmp, ntpinterval;\r\nstruct clocksource *old_clock;\r\nold_clock = tk->tkr.clock;\r\ntk->tkr.clock = clock;\r\ntk->tkr.read = clock->read;\r\ntk->tkr.mask = clock->mask;\r\ntk->tkr.cycle_last = tk->tkr.read(clock);\r\ntmp = NTP_INTERVAL_LENGTH;\r\ntmp <<= clock->shift;\r\nntpinterval = tmp;\r\ntmp += clock->mult/2;\r\ndo_div(tmp, clock->mult);\r\nif (tmp == 0)\r\ntmp = 1;\r\ninterval = (cycle_t) tmp;\r\ntk->cycle_interval = interval;\r\ntk->xtime_interval = (u64) interval * clock->mult;\r\ntk->xtime_remainder = ntpinterval - tk->xtime_interval;\r\ntk->raw_interval =\r\n((u64) interval * clock->mult) >> clock->shift;\r\nif (old_clock) {\r\nint shift_change = clock->shift - old_clock->shift;\r\nif (shift_change < 0)\r\ntk->tkr.xtime_nsec >>= -shift_change;\r\nelse\r\ntk->tkr.xtime_nsec <<= shift_change;\r\n}\r\ntk->tkr.shift = clock->shift;\r\ntk->ntp_error = 0;\r\ntk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;\r\ntk->ntp_tick = ntpinterval << tk->ntp_error_shift;\r\ntk->tkr.mult = clock->mult;\r\ntk->ntp_err_mult = 0;\r\n}\r\nstatic u32 default_arch_gettimeoffset(void) { return 0; }\r\nstatic inline u32 arch_gettimeoffset(void) { return 0; }\r\nstatic inline s64 timekeeping_get_ns(struct tk_read_base *tkr)\r\n{\r\ncycle_t cycle_now, delta;\r\ns64 nsec;\r\ncycle_now = tkr->read(tkr->clock);\r\ndelta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);\r\nnsec = delta * tkr->mult + tkr->xtime_nsec;\r\nnsec >>= tkr->shift;\r\nreturn nsec + arch_gettimeoffset();\r\n}\r\nstatic inline s64 timekeeping_get_ns_raw(struct timekeeper *tk)\r\n{\r\nstruct clocksource *clock = tk->tkr.clock;\r\ncycle_t cycle_now, delta;\r\ns64 nsec;\r\ncycle_now = tk->tkr.read(clock);\r\ndelta = clocksource_delta(cycle_now, tk->tkr.cycle_last, tk->tkr.mask);\r\nnsec = clocksource_cyc2ns(delta, clock->mult, clock->shift);\r\nreturn nsec + arch_gettimeoffset();\r\n}\r\nstatic void update_fast_timekeeper(struct timekeeper *tk)\r\n{\r\nstruct tk_read_base *base = tk_fast_mono.base;\r\nraw_write_seqcount_latch(&tk_fast_mono.seq);\r\nmemcpy(base, &tk->tkr, sizeof(*base));\r\nraw_write_seqcount_latch(&tk_fast_mono.seq);\r\nmemcpy(base + 1, base, sizeof(*base));\r\n}\r\nu64 notrace ktime_get_mono_fast_ns(void)\r\n{\r\nstruct tk_read_base *tkr;\r\nunsigned int seq;\r\nu64 now;\r\ndo {\r\nseq = raw_read_seqcount(&tk_fast_mono.seq);\r\ntkr = tk_fast_mono.base + (seq & 0x01);\r\nnow = ktime_to_ns(tkr->base_mono) + timekeeping_get_ns(tkr);\r\n} while (read_seqcount_retry(&tk_fast_mono.seq, seq));\r\nreturn now;\r\n}\r\nstatic inline void update_vsyscall(struct timekeeper *tk)\r\n{\r\nstruct timespec xt, wm;\r\nxt = timespec64_to_timespec(tk_xtime(tk));\r\nwm = timespec64_to_timespec(tk->wall_to_monotonic);\r\nupdate_vsyscall_old(&xt, &wm, tk->tkr.clock, tk->tkr.mult,\r\ntk->tkr.cycle_last);\r\n}\r\nstatic inline void old_vsyscall_fixup(struct timekeeper *tk)\r\n{\r\ns64 remainder;\r\nremainder = tk->tkr.xtime_nsec & ((1ULL << tk->tkr.shift) - 1);\r\ntk->tkr.xtime_nsec -= remainder;\r\ntk->tkr.xtime_nsec += 1ULL << tk->tkr.shift;\r\ntk->ntp_error += remainder << tk->ntp_error_shift;\r\ntk->ntp_error -= (1ULL << tk->tkr.shift) << tk->ntp_error_shift;\r\n}\r\nstatic void update_pvclock_gtod(struct timekeeper *tk, bool was_set)\r\n{\r\nraw_notifier_call_chain(&pvclock_gtod_chain, was_set, tk);\r\n}\r\nint pvclock_gtod_register_notifier(struct notifier_block *nb)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nint ret;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nret = raw_notifier_chain_register(&pvclock_gtod_chain, nb);\r\nupdate_pvclock_gtod(tk, true);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nreturn ret;\r\n}\r\nint pvclock_gtod_unregister_notifier(struct notifier_block *nb)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nret = raw_notifier_chain_unregister(&pvclock_gtod_chain, nb);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nreturn ret;\r\n}\r\nstatic inline void tk_update_ktime_data(struct timekeeper *tk)\r\n{\r\ns64 nsec;\r\nnsec = (s64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);\r\nnsec *= NSEC_PER_SEC;\r\nnsec += tk->wall_to_monotonic.tv_nsec;\r\ntk->tkr.base_mono = ns_to_ktime(nsec);\r\ntk->base_raw = timespec64_to_ktime(tk->raw_time);\r\n}\r\nstatic void timekeeping_update(struct timekeeper *tk, unsigned int action)\r\n{\r\nif (action & TK_CLEAR_NTP) {\r\ntk->ntp_error = 0;\r\nntp_clear();\r\n}\r\ntk_update_ktime_data(tk);\r\nupdate_vsyscall(tk);\r\nupdate_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);\r\nif (action & TK_MIRROR)\r\nmemcpy(&shadow_timekeeper, &tk_core.timekeeper,\r\nsizeof(tk_core.timekeeper));\r\nupdate_fast_timekeeper(tk);\r\n}\r\nstatic void timekeeping_forward_now(struct timekeeper *tk)\r\n{\r\nstruct clocksource *clock = tk->tkr.clock;\r\ncycle_t cycle_now, delta;\r\ns64 nsec;\r\ncycle_now = tk->tkr.read(clock);\r\ndelta = clocksource_delta(cycle_now, tk->tkr.cycle_last, tk->tkr.mask);\r\ntk->tkr.cycle_last = cycle_now;\r\ntk->tkr.xtime_nsec += delta * tk->tkr.mult;\r\ntk->tkr.xtime_nsec += (u64)arch_gettimeoffset() << tk->tkr.shift;\r\ntk_normalize_xtime(tk);\r\nnsec = clocksource_cyc2ns(delta, clock->mult, clock->shift);\r\ntimespec64_add_ns(&tk->raw_time, nsec);\r\n}\r\nint __getnstimeofday64(struct timespec64 *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\ns64 nsecs = 0;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nts->tv_sec = tk->xtime_sec;\r\nnsecs = timekeeping_get_ns(&tk->tkr);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nts->tv_nsec = 0;\r\ntimespec64_add_ns(ts, nsecs);\r\nif (unlikely(timekeeping_suspended))\r\nreturn -EAGAIN;\r\nreturn 0;\r\n}\r\nvoid getnstimeofday64(struct timespec64 *ts)\r\n{\r\nWARN_ON(__getnstimeofday64(ts));\r\n}\r\nktime_t ktime_get(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base;\r\ns64 nsecs;\r\nWARN_ON(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = tk->tkr.base_mono;\r\nnsecs = timekeeping_get_ns(&tk->tkr);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nktime_t ktime_get_with_offset(enum tk_offsets offs)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base, *offset = offsets[offs];\r\ns64 nsecs;\r\nWARN_ON(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = ktime_add(tk->tkr.base_mono, *offset);\r\nnsecs = timekeeping_get_ns(&tk->tkr);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)\r\n{\r\nktime_t *offset = offsets[offs];\r\nunsigned long seq;\r\nktime_t tconv;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\ntconv = ktime_add(tmono, *offset);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn tconv;\r\n}\r\nktime_t ktime_get_raw(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base;\r\ns64 nsecs;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = tk->base_raw;\r\nnsecs = timekeeping_get_ns_raw(tk);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nvoid ktime_get_ts64(struct timespec64 *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 tomono;\r\ns64 nsec;\r\nunsigned int seq;\r\nWARN_ON(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nts->tv_sec = tk->xtime_sec;\r\nnsec = timekeeping_get_ns(&tk->tkr);\r\ntomono = tk->wall_to_monotonic;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nts->tv_sec += tomono.tv_sec;\r\nts->tv_nsec = 0;\r\ntimespec64_add_ns(ts, nsec + tomono.tv_nsec);\r\n}\r\nvoid getnstime_raw_and_real(struct timespec *ts_raw, struct timespec *ts_real)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\ns64 nsecs_raw, nsecs_real;\r\nWARN_ON_ONCE(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\n*ts_raw = timespec64_to_timespec(tk->raw_time);\r\nts_real->tv_sec = tk->xtime_sec;\r\nts_real->tv_nsec = 0;\r\nnsecs_raw = timekeeping_get_ns_raw(tk);\r\nnsecs_real = timekeeping_get_ns(&tk->tkr);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\ntimespec_add_ns(ts_raw, nsecs_raw);\r\ntimespec_add_ns(ts_real, nsecs_real);\r\n}\r\nvoid do_gettimeofday(struct timeval *tv)\r\n{\r\nstruct timespec64 now;\r\ngetnstimeofday64(&now);\r\ntv->tv_sec = now.tv_sec;\r\ntv->tv_usec = now.tv_nsec/1000;\r\n}\r\nint do_settimeofday(const struct timespec *tv)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 ts_delta, xt, tmp;\r\nunsigned long flags;\r\nif (!timespec_valid_strict(tv))\r\nreturn -EINVAL;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\nxt = tk_xtime(tk);\r\nts_delta.tv_sec = tv->tv_sec - xt.tv_sec;\r\nts_delta.tv_nsec = tv->tv_nsec - xt.tv_nsec;\r\ntk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts_delta));\r\ntmp = timespec_to_timespec64(*tv);\r\ntk_set_xtime(tk, &tmp);\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\nreturn 0;\r\n}\r\nint timekeeping_inject_offset(struct timespec *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nstruct timespec64 ts64, tmp;\r\nint ret = 0;\r\nif ((unsigned long)ts->tv_nsec >= NSEC_PER_SEC)\r\nreturn -EINVAL;\r\nts64 = timespec_to_timespec64(*ts);\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\ntmp = timespec64_add(tk_xtime(tk), ts64);\r\nif (!timespec64_valid_strict(&tmp)) {\r\nret = -EINVAL;\r\ngoto error;\r\n}\r\ntk_xtime_add(tk, &ts64);\r\ntk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts64));\r\nerror:\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\nreturn ret;\r\n}\r\ns32 timekeeping_get_tai_offset(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\ns32 ret;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nret = tk->tai_offset;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ret;\r\n}\r\nstatic void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)\r\n{\r\ntk->tai_offset = tai_offset;\r\ntk->offs_tai = ktime_add(tk->offs_real, ktime_set(tai_offset, 0));\r\n}\r\nvoid timekeeping_set_tai_offset(s32 tai_offset)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\n__timekeeping_set_tai_offset(tk, tai_offset);\r\ntimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\n}\r\nstatic int change_clocksource(void *data)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct clocksource *new, *old;\r\nunsigned long flags;\r\nnew = (struct clocksource *) data;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\nif (try_module_get(new->owner)) {\r\nif (!new->enable || new->enable(new) == 0) {\r\nold = tk->tkr.clock;\r\ntk_setup_internals(tk, new);\r\nif (old->disable)\r\nold->disable(old);\r\nmodule_put(old->owner);\r\n} else {\r\nmodule_put(new->owner);\r\n}\r\n}\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nreturn 0;\r\n}\r\nint timekeeping_notify(struct clocksource *clock)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nif (tk->tkr.clock == clock)\r\nreturn 0;\r\nstop_machine(change_clocksource, clock, NULL);\r\ntick_clock_notify();\r\nreturn tk->tkr.clock == clock ? 0 : -1;\r\n}\r\nvoid getrawmonotonic(struct timespec *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 ts64;\r\nunsigned long seq;\r\ns64 nsecs;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnsecs = timekeeping_get_ns_raw(tk);\r\nts64 = tk->raw_time;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\ntimespec64_add_ns(&ts64, nsecs);\r\n*ts = timespec64_to_timespec(ts64);\r\n}\r\nint timekeeping_valid_for_hres(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\nint ret;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nret = tk->tkr.clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ret;\r\n}\r\nu64 timekeeping_max_deferment(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\nu64 ret;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nret = tk->tkr.clock->max_idle_ns;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ret;\r\n}\r\nvoid __weak read_persistent_clock(struct timespec *ts)\r\n{\r\nts->tv_sec = 0;\r\nts->tv_nsec = 0;\r\n}\r\nvoid __weak read_boot_clock(struct timespec *ts)\r\n{\r\nts->tv_sec = 0;\r\nts->tv_nsec = 0;\r\n}\r\nvoid __init timekeeping_init(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct clocksource *clock;\r\nunsigned long flags;\r\nstruct timespec64 now, boot, tmp;\r\nstruct timespec ts;\r\nread_persistent_clock(&ts);\r\nnow = timespec_to_timespec64(ts);\r\nif (!timespec64_valid_strict(&now)) {\r\npr_warn("WARNING: Persistent clock returned invalid value!\n"\r\n" Check your CMOS/BIOS settings.\n");\r\nnow.tv_sec = 0;\r\nnow.tv_nsec = 0;\r\n} else if (now.tv_sec || now.tv_nsec)\r\npersistent_clock_exist = true;\r\nread_boot_clock(&ts);\r\nboot = timespec_to_timespec64(ts);\r\nif (!timespec64_valid_strict(&boot)) {\r\npr_warn("WARNING: Boot clock returned invalid value!\n"\r\n" Check your CMOS/BIOS settings.\n");\r\nboot.tv_sec = 0;\r\nboot.tv_nsec = 0;\r\n}\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\nntp_init();\r\nclock = clocksource_default_clock();\r\nif (clock->enable)\r\nclock->enable(clock);\r\ntk_setup_internals(tk, clock);\r\ntk_set_xtime(tk, &now);\r\ntk->raw_time.tv_sec = 0;\r\ntk->raw_time.tv_nsec = 0;\r\ntk->base_raw.tv64 = 0;\r\nif (boot.tv_sec == 0 && boot.tv_nsec == 0)\r\nboot = tk_xtime(tk);\r\nset_normalized_timespec64(&tmp, -boot.tv_sec, -boot.tv_nsec);\r\ntk_set_wall_to_mono(tk, tmp);\r\ntimekeeping_update(tk, TK_MIRROR);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\n}\r\nstatic void __timekeeping_inject_sleeptime(struct timekeeper *tk,\r\nstruct timespec64 *delta)\r\n{\r\nif (!timespec64_valid_strict(delta)) {\r\nprintk_deferred(KERN_WARNING\r\n"__timekeeping_inject_sleeptime: Invalid "\r\n"sleep delta value!\n");\r\nreturn;\r\n}\r\ntk_xtime_add(tk, delta);\r\ntk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *delta));\r\ntk_update_sleep_time(tk, timespec64_to_ktime(*delta));\r\ntk_debug_account_sleep_time(delta);\r\n}\r\nvoid timekeeping_inject_sleeptime(struct timespec *delta)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 tmp;\r\nunsigned long flags;\r\nif (has_persistent_clock())\r\nreturn;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\ntmp = timespec_to_timespec64(*delta);\r\n__timekeeping_inject_sleeptime(tk, &tmp);\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\n}\r\nstatic void timekeeping_resume(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct clocksource *clock = tk->tkr.clock;\r\nunsigned long flags;\r\nstruct timespec64 ts_new, ts_delta;\r\nstruct timespec tmp;\r\ncycle_t cycle_now, cycle_delta;\r\nbool suspendtime_found = false;\r\nread_persistent_clock(&tmp);\r\nts_new = timespec_to_timespec64(tmp);\r\nclockevents_resume();\r\nclocksource_resume();\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ncycle_now = tk->tkr.read(clock);\r\nif ((clock->flags & CLOCK_SOURCE_SUSPEND_NONSTOP) &&\r\ncycle_now > tk->tkr.cycle_last) {\r\nu64 num, max = ULLONG_MAX;\r\nu32 mult = clock->mult;\r\nu32 shift = clock->shift;\r\ns64 nsec = 0;\r\ncycle_delta = clocksource_delta(cycle_now, tk->tkr.cycle_last,\r\ntk->tkr.mask);\r\ndo_div(max, mult);\r\nif (cycle_delta > max) {\r\nnum = div64_u64(cycle_delta, max);\r\nnsec = (((u64) max * mult) >> shift) * num;\r\ncycle_delta -= num * max;\r\n}\r\nnsec += ((u64) cycle_delta * mult) >> shift;\r\nts_delta = ns_to_timespec64(nsec);\r\nsuspendtime_found = true;\r\n} else if (timespec64_compare(&ts_new, &timekeeping_suspend_time) > 0) {\r\nts_delta = timespec64_sub(ts_new, timekeeping_suspend_time);\r\nsuspendtime_found = true;\r\n}\r\nif (suspendtime_found)\r\n__timekeeping_inject_sleeptime(tk, &ts_delta);\r\ntk->tkr.cycle_last = cycle_now;\r\ntk->ntp_error = 0;\r\ntimekeeping_suspended = 0;\r\ntimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\ntouch_softlockup_watchdog();\r\nclockevents_notify(CLOCK_EVT_NOTIFY_RESUME, NULL);\r\nhrtimers_resume();\r\n}\r\nstatic int timekeeping_suspend(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nstruct timespec64 delta, delta_delta;\r\nstatic struct timespec64 old_delta;\r\nstruct timespec tmp;\r\nread_persistent_clock(&tmp);\r\ntimekeeping_suspend_time = timespec_to_timespec64(tmp);\r\nif (timekeeping_suspend_time.tv_sec || timekeeping_suspend_time.tv_nsec)\r\npersistent_clock_exist = true;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\ntimekeeping_suspended = 1;\r\ndelta = timespec64_sub(tk_xtime(tk), timekeeping_suspend_time);\r\ndelta_delta = timespec64_sub(delta, old_delta);\r\nif (abs(delta_delta.tv_sec) >= 2) {\r\nold_delta = delta;\r\n} else {\r\ntimekeeping_suspend_time =\r\ntimespec64_add(timekeeping_suspend_time, delta_delta);\r\n}\r\ntimekeeping_update(tk, TK_MIRROR);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclockevents_notify(CLOCK_EVT_NOTIFY_SUSPEND, NULL);\r\nclocksource_suspend();\r\nclockevents_suspend();\r\nreturn 0;\r\n}\r\nstatic int __init timekeeping_init_ops(void)\r\n{\r\nregister_syscore_ops(&timekeeping_syscore_ops);\r\nreturn 0;\r\n}\r\nstatic __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,\r\ns64 offset,\r\nbool negative,\r\nint adj_scale)\r\n{\r\ns64 interval = tk->cycle_interval;\r\ns32 mult_adj = 1;\r\nif (negative) {\r\nmult_adj = -mult_adj;\r\ninterval = -interval;\r\noffset = -offset;\r\n}\r\nmult_adj <<= adj_scale;\r\ninterval <<= adj_scale;\r\noffset <<= adj_scale;\r\ntk->tkr.mult += mult_adj;\r\ntk->xtime_interval += interval;\r\ntk->tkr.xtime_nsec -= offset;\r\ntk->ntp_error -= (interval - offset) << tk->ntp_error_shift;\r\n}\r\nstatic __always_inline void timekeeping_freqadjust(struct timekeeper *tk,\r\ns64 offset)\r\n{\r\ns64 interval = tk->cycle_interval;\r\ns64 xinterval = tk->xtime_interval;\r\ns64 tick_error;\r\nbool negative;\r\nu32 adj;\r\nif (tk->ntp_err_mult)\r\nxinterval -= tk->cycle_interval;\r\ntk->ntp_tick = ntp_tick_length();\r\ntick_error = ntp_tick_length() >> tk->ntp_error_shift;\r\ntick_error -= (xinterval + tk->xtime_remainder);\r\nif (likely((tick_error >= 0) && (tick_error <= interval)))\r\nreturn;\r\nnegative = (tick_error < 0);\r\ntick_error = abs(tick_error);\r\nfor (adj = 0; tick_error > interval; adj++)\r\ntick_error >>= 1;\r\ntimekeeping_apply_adjustment(tk, offset, negative, adj);\r\n}\r\nstatic void timekeeping_adjust(struct timekeeper *tk, s64 offset)\r\n{\r\ntimekeeping_freqadjust(tk, offset);\r\nif (!tk->ntp_err_mult && (tk->ntp_error > 0)) {\r\ntk->ntp_err_mult = 1;\r\ntimekeeping_apply_adjustment(tk, offset, 0, 0);\r\n} else if (tk->ntp_err_mult && (tk->ntp_error <= 0)) {\r\ntimekeeping_apply_adjustment(tk, offset, 1, 0);\r\ntk->ntp_err_mult = 0;\r\n}\r\nif (unlikely(tk->tkr.clock->maxadj &&\r\n(tk->tkr.mult > tk->tkr.clock->mult + tk->tkr.clock->maxadj))) {\r\nprintk_once(KERN_WARNING\r\n"Adjusting %s more than 11%% (%ld vs %ld)\n",\r\ntk->tkr.clock->name, (long)tk->tkr.mult,\r\n(long)tk->tkr.clock->mult + tk->tkr.clock->maxadj);\r\n}\r\nif (unlikely((s64)tk->tkr.xtime_nsec < 0)) {\r\ns64 neg = -(s64)tk->tkr.xtime_nsec;\r\ntk->tkr.xtime_nsec = 0;\r\ntk->ntp_error += neg << tk->ntp_error_shift;\r\n}\r\n}\r\nstatic inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)\r\n{\r\nu64 nsecps = (u64)NSEC_PER_SEC << tk->tkr.shift;\r\nunsigned int clock_set = 0;\r\nwhile (tk->tkr.xtime_nsec >= nsecps) {\r\nint leap;\r\ntk->tkr.xtime_nsec -= nsecps;\r\ntk->xtime_sec++;\r\nleap = second_overflow(tk->xtime_sec);\r\nif (unlikely(leap)) {\r\nstruct timespec64 ts;\r\ntk->xtime_sec += leap;\r\nts.tv_sec = leap;\r\nts.tv_nsec = 0;\r\ntk_set_wall_to_mono(tk,\r\ntimespec64_sub(tk->wall_to_monotonic, ts));\r\n__timekeeping_set_tai_offset(tk, tk->tai_offset - leap);\r\nclock_set = TK_CLOCK_WAS_SET;\r\n}\r\n}\r\nreturn clock_set;\r\n}\r\nstatic cycle_t logarithmic_accumulation(struct timekeeper *tk, cycle_t offset,\r\nu32 shift,\r\nunsigned int *clock_set)\r\n{\r\ncycle_t interval = tk->cycle_interval << shift;\r\nu64 raw_nsecs;\r\nif (offset < interval)\r\nreturn offset;\r\noffset -= interval;\r\ntk->tkr.cycle_last += interval;\r\ntk->tkr.xtime_nsec += tk->xtime_interval << shift;\r\n*clock_set |= accumulate_nsecs_to_secs(tk);\r\nraw_nsecs = (u64)tk->raw_interval << shift;\r\nraw_nsecs += tk->raw_time.tv_nsec;\r\nif (raw_nsecs >= NSEC_PER_SEC) {\r\nu64 raw_secs = raw_nsecs;\r\nraw_nsecs = do_div(raw_secs, NSEC_PER_SEC);\r\ntk->raw_time.tv_sec += raw_secs;\r\n}\r\ntk->raw_time.tv_nsec = raw_nsecs;\r\ntk->ntp_error += tk->ntp_tick << shift;\r\ntk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<\r\n(tk->ntp_error_shift + shift);\r\nreturn offset;\r\n}\r\nvoid update_wall_time(void)\r\n{\r\nstruct timekeeper *real_tk = &tk_core.timekeeper;\r\nstruct timekeeper *tk = &shadow_timekeeper;\r\ncycle_t offset;\r\nint shift = 0, maxshift;\r\nunsigned int clock_set = 0;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nif (unlikely(timekeeping_suspended))\r\ngoto out;\r\n#ifdef CONFIG_ARCH_USES_GETTIMEOFFSET\r\noffset = real_tk->cycle_interval;\r\n#else\r\noffset = clocksource_delta(tk->tkr.read(tk->tkr.clock),\r\ntk->tkr.cycle_last, tk->tkr.mask);\r\n#endif\r\nif (offset < real_tk->cycle_interval)\r\ngoto out;\r\nshift = ilog2(offset) - ilog2(tk->cycle_interval);\r\nshift = max(0, shift);\r\nmaxshift = (64 - (ilog2(ntp_tick_length())+1)) - 1;\r\nshift = min(shift, maxshift);\r\nwhile (offset >= tk->cycle_interval) {\r\noffset = logarithmic_accumulation(tk, offset, shift,\r\n&clock_set);\r\nif (offset < tk->cycle_interval<<shift)\r\nshift--;\r\n}\r\ntimekeeping_adjust(tk, offset);\r\nold_vsyscall_fixup(tk);\r\nclock_set |= accumulate_nsecs_to_secs(tk);\r\nwrite_seqcount_begin(&tk_core.seq);\r\nmemcpy(real_tk, tk, sizeof(*tk));\r\ntimekeeping_update(real_tk, clock_set);\r\nwrite_seqcount_end(&tk_core.seq);\r\nout:\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nif (clock_set)\r\nclock_was_set_delayed();\r\n}\r\nvoid getboottime(struct timespec *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nktime_t t = ktime_sub(tk->offs_real, tk->offs_boot);\r\n*ts = ktime_to_timespec(t);\r\n}\r\nunsigned long get_seconds(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nreturn tk->xtime_sec;\r\n}\r\nstruct timespec __current_kernel_time(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nreturn timespec64_to_timespec(tk_xtime(tk));\r\n}\r\nstruct timespec current_kernel_time(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 now;\r\nunsigned long seq;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnow = tk_xtime(tk);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn timespec64_to_timespec(now);\r\n}\r\nstruct timespec get_monotonic_coarse(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 now, mono;\r\nunsigned long seq;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnow = tk_xtime(tk);\r\nmono = tk->wall_to_monotonic;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nset_normalized_timespec64(&now, now.tv_sec + mono.tv_sec,\r\nnow.tv_nsec + mono.tv_nsec);\r\nreturn timespec64_to_timespec(now);\r\n}\r\nvoid do_timer(unsigned long ticks)\r\n{\r\njiffies_64 += ticks;\r\ncalc_global_load(ticks);\r\n}\r\nktime_t ktime_get_update_offsets_tick(ktime_t *offs_real, ktime_t *offs_boot,\r\nktime_t *offs_tai)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base;\r\nu64 nsecs;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = tk->tkr.base_mono;\r\nnsecs = tk->tkr.xtime_nsec >> tk->tkr.shift;\r\n*offs_real = tk->offs_real;\r\n*offs_boot = tk->offs_boot;\r\n*offs_tai = tk->offs_tai;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nktime_t ktime_get_update_offsets_now(ktime_t *offs_real, ktime_t *offs_boot,\r\nktime_t *offs_tai)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base;\r\nu64 nsecs;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = tk->tkr.base_mono;\r\nnsecs = timekeeping_get_ns(&tk->tkr);\r\n*offs_real = tk->offs_real;\r\n*offs_boot = tk->offs_boot;\r\n*offs_tai = tk->offs_tai;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nint do_adjtimex(struct timex *txc)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nstruct timespec64 ts;\r\ns32 orig_tai, tai;\r\nint ret;\r\nret = ntp_validate_timex(txc);\r\nif (ret)\r\nreturn ret;\r\nif (txc->modes & ADJ_SETOFFSET) {\r\nstruct timespec delta;\r\ndelta.tv_sec = txc->time.tv_sec;\r\ndelta.tv_nsec = txc->time.tv_usec;\r\nif (!(txc->modes & ADJ_NANO))\r\ndelta.tv_nsec *= 1000;\r\nret = timekeeping_inject_offset(&delta);\r\nif (ret)\r\nreturn ret;\r\n}\r\ngetnstimeofday64(&ts);\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\norig_tai = tai = tk->tai_offset;\r\nret = __do_adjtimex(txc, &ts, &tai);\r\nif (tai != orig_tai) {\r\n__timekeeping_set_tai_offset(tk, tai);\r\ntimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\r\n}\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nif (tai != orig_tai)\r\nclock_was_set();\r\nntp_notify_cmos_timer();\r\nreturn ret;\r\n}\r\nvoid hardpps(const struct timespec *phase_ts, const struct timespec *raw_ts)\r\n{\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\n__hardpps(phase_ts, raw_ts);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\n}\r\nvoid xtime_update(unsigned long ticks)\r\n{\r\nwrite_seqlock(&jiffies_lock);\r\ndo_timer(ticks);\r\nwrite_sequnlock(&jiffies_lock);\r\nupdate_wall_time();\r\n}
