static inline u32 armv7_pmnc_read(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p15, 0, %0, c9, c12, 0" : "=r"(val));\r\nreturn val;\r\n}\r\nstatic inline void armv7_pmnc_write(u32 val)\r\n{\r\nval &= ARMV7_PMNC_MASK;\r\nisb();\r\nasm volatile("mcr p15, 0, %0, c9, c12, 0" : : "r"(val));\r\n}\r\nstatic inline int armv7_pmnc_has_overflowed(u32 pmnc)\r\n{\r\nreturn pmnc & ARMV7_OVERFLOWED_MASK;\r\n}\r\nstatic inline int armv7_pmnc_counter_valid(struct arm_pmu *cpu_pmu, int idx)\r\n{\r\nreturn idx >= ARMV7_IDX_CYCLE_COUNTER &&\r\nidx <= ARMV7_IDX_COUNTER_LAST(cpu_pmu);\r\n}\r\nstatic inline int armv7_pmnc_counter_has_overflowed(u32 pmnc, int idx)\r\n{\r\nreturn pmnc & BIT(ARMV7_IDX_TO_COUNTER(idx));\r\n}\r\nstatic inline int armv7_pmnc_select_counter(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c12, 5" : : "r" (counter));\r\nisb();\r\nreturn idx;\r\n}\r\nstatic inline u32 armv7pmu_read_counter(struct perf_event *event)\r\n{\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nu32 value = 0;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx))\r\npr_err("CPU%u reading wrong counter %d\n",\r\nsmp_processor_id(), idx);\r\nelse if (idx == ARMV7_IDX_CYCLE_COUNTER)\r\nasm volatile("mrc p15, 0, %0, c9, c13, 0" : "=r" (value));\r\nelse if (armv7_pmnc_select_counter(idx) == idx)\r\nasm volatile("mrc p15, 0, %0, c9, c13, 2" : "=r" (value));\r\nreturn value;\r\n}\r\nstatic inline void armv7pmu_write_counter(struct perf_event *event, u32 value)\r\n{\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx))\r\npr_err("CPU%u writing wrong counter %d\n",\r\nsmp_processor_id(), idx);\r\nelse if (idx == ARMV7_IDX_CYCLE_COUNTER)\r\nasm volatile("mcr p15, 0, %0, c9, c13, 0" : : "r" (value));\r\nelse if (armv7_pmnc_select_counter(idx) == idx)\r\nasm volatile("mcr p15, 0, %0, c9, c13, 2" : : "r" (value));\r\n}\r\nstatic inline void armv7_pmnc_write_evtsel(int idx, u32 val)\r\n{\r\nif (armv7_pmnc_select_counter(idx) == idx) {\r\nval &= ARMV7_EVTYPE_MASK;\r\nasm volatile("mcr p15, 0, %0, c9, c13, 1" : : "r" (val));\r\n}\r\n}\r\nstatic inline int armv7_pmnc_enable_counter(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c12, 1" : : "r" (BIT(counter)));\r\nreturn idx;\r\n}\r\nstatic inline int armv7_pmnc_disable_counter(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c12, 2" : : "r" (BIT(counter)));\r\nreturn idx;\r\n}\r\nstatic inline int armv7_pmnc_enable_intens(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c14, 1" : : "r" (BIT(counter)));\r\nreturn idx;\r\n}\r\nstatic inline int armv7_pmnc_disable_intens(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c14, 2" : : "r" (BIT(counter)));\r\nisb();\r\nasm volatile("mcr p15, 0, %0, c9, c12, 3" : : "r" (BIT(counter)));\r\nisb();\r\nreturn idx;\r\n}\r\nstatic inline u32 armv7_pmnc_getreset_flags(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p15, 0, %0, c9, c12, 3" : "=r" (val));\r\nval &= ARMV7_FLAG_MASK;\r\nasm volatile("mcr p15, 0, %0, c9, c12, 3" : : "r" (val));\r\nreturn val;\r\n}\r\nstatic void armv7_pmnc_dump_regs(struct arm_pmu *cpu_pmu)\r\n{\r\nu32 val;\r\nunsigned int cnt;\r\nprintk(KERN_INFO "PMNC registers dump:\n");\r\nasm volatile("mrc p15, 0, %0, c9, c12, 0" : "=r" (val));\r\nprintk(KERN_INFO "PMNC =0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c12, 1" : "=r" (val));\r\nprintk(KERN_INFO "CNTENS=0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c14, 1" : "=r" (val));\r\nprintk(KERN_INFO "INTENS=0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c12, 3" : "=r" (val));\r\nprintk(KERN_INFO "FLAGS =0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c12, 5" : "=r" (val));\r\nprintk(KERN_INFO "SELECT=0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c13, 0" : "=r" (val));\r\nprintk(KERN_INFO "CCNT =0x%08x\n", val);\r\nfor (cnt = ARMV7_IDX_COUNTER0;\r\ncnt <= ARMV7_IDX_COUNTER_LAST(cpu_pmu); cnt++) {\r\narmv7_pmnc_select_counter(cnt);\r\nasm volatile("mrc p15, 0, %0, c9, c13, 2" : "=r" (val));\r\nprintk(KERN_INFO "CNT[%d] count =0x%08x\n",\r\nARMV7_IDX_TO_COUNTER(cnt), val);\r\nasm volatile("mrc p15, 0, %0, c9, c13, 1" : "=r" (val));\r\nprintk(KERN_INFO "CNT[%d] evtsel=0x%08x\n",\r\nARMV7_IDX_TO_COUNTER(cnt), val);\r\n}\r\n}\r\nstatic void armv7pmu_enable_event(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx)) {\r\npr_err("CPU%u enabling wrong PMNC counter IRQ enable %d\n",\r\nsmp_processor_id(), idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_disable_counter(idx);\r\nif (cpu_pmu->set_event_filter || idx != ARMV7_IDX_CYCLE_COUNTER)\r\narmv7_pmnc_write_evtsel(idx, hwc->config_base);\r\narmv7_pmnc_enable_intens(idx);\r\narmv7_pmnc_enable_counter(idx);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void armv7pmu_disable_event(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx)) {\r\npr_err("CPU%u disabling wrong PMNC counter IRQ enable %d\n",\r\nsmp_processor_id(), idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_disable_counter(idx);\r\narmv7_pmnc_disable_intens(idx);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic irqreturn_t armv7pmu_handle_irq(int irq_num, void *dev)\r\n{\r\nu32 pmnc;\r\nstruct perf_sample_data data;\r\nstruct arm_pmu *cpu_pmu = (struct arm_pmu *)dev;\r\nstruct pmu_hw_events *cpuc = cpu_pmu->get_hw_events();\r\nstruct pt_regs *regs;\r\nint idx;\r\npmnc = armv7_pmnc_getreset_flags();\r\nif (!armv7_pmnc_has_overflowed(pmnc))\r\nreturn IRQ_NONE;\r\nregs = get_irq_regs();\r\nfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\r\nstruct perf_event *event = cpuc->events[idx];\r\nstruct hw_perf_event *hwc;\r\nif (!event)\r\ncontinue;\r\nif (!armv7_pmnc_counter_has_overflowed(pmnc, idx))\r\ncontinue;\r\nhwc = &event->hw;\r\narmpmu_event_update(event);\r\nperf_sample_data_init(&data, 0, hwc->last_period);\r\nif (!armpmu_event_set_period(event))\r\ncontinue;\r\nif (perf_event_overflow(event, &data, regs))\r\ncpu_pmu->disable(event);\r\n}\r\nirq_work_run();\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void armv7pmu_start(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_write(armv7_pmnc_read() | ARMV7_PMNC_E);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void armv7pmu_stop(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_write(armv7_pmnc_read() & ~ARMV7_PMNC_E);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic int armv7pmu_get_event_idx(struct pmu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nint idx;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned long evtype = hwc->config_base & ARMV7_EVTYPE_EVENT;\r\nif (evtype == ARMV7_PERFCTR_CPU_CYCLES) {\r\nif (test_and_set_bit(ARMV7_IDX_CYCLE_COUNTER, cpuc->used_mask))\r\nreturn -EAGAIN;\r\nreturn ARMV7_IDX_CYCLE_COUNTER;\r\n}\r\nfor (idx = ARMV7_IDX_COUNTER0; idx < cpu_pmu->num_events; ++idx) {\r\nif (!test_and_set_bit(idx, cpuc->used_mask))\r\nreturn idx;\r\n}\r\nreturn -EAGAIN;\r\n}\r\nstatic int armv7pmu_set_event_filter(struct hw_perf_event *event,\r\nstruct perf_event_attr *attr)\r\n{\r\nunsigned long config_base = 0;\r\nif (attr->exclude_idle)\r\nreturn -EPERM;\r\nif (attr->exclude_user)\r\nconfig_base |= ARMV7_EXCLUDE_USER;\r\nif (attr->exclude_kernel)\r\nconfig_base |= ARMV7_EXCLUDE_PL1;\r\nif (!attr->exclude_hv)\r\nconfig_base |= ARMV7_INCLUDE_HYP;\r\nevent->config_base = config_base;\r\nreturn 0;\r\n}\r\nstatic void armv7pmu_reset(void *info)\r\n{\r\nstruct arm_pmu *cpu_pmu = (struct arm_pmu *)info;\r\nu32 idx, nb_cnt = cpu_pmu->num_events;\r\nfor (idx = ARMV7_IDX_CYCLE_COUNTER; idx < nb_cnt; ++idx) {\r\narmv7_pmnc_disable_counter(idx);\r\narmv7_pmnc_disable_intens(idx);\r\n}\r\narmv7_pmnc_write(ARMV7_PMNC_P | ARMV7_PMNC_C);\r\n}\r\nstatic int armv7_a8_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a8_perf_map,\r\n&armv7_a8_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a9_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a9_perf_map,\r\n&armv7_a9_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a5_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a5_perf_map,\r\n&armv7_a5_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a15_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a15_perf_map,\r\n&armv7_a15_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a7_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a7_perf_map,\r\n&armv7_a7_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a12_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a12_perf_map,\r\n&armv7_a12_perf_cache_map, 0xFF);\r\n}\r\nstatic int krait_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &krait_perf_map,\r\n&krait_perf_cache_map, 0xFFFFF);\r\n}\r\nstatic int krait_map_event_no_branch(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &krait_perf_map_no_branch,\r\n&krait_perf_cache_map, 0xFFFFF);\r\n}\r\nstatic void armv7pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\ncpu_pmu->handle_irq = armv7pmu_handle_irq;\r\ncpu_pmu->enable = armv7pmu_enable_event;\r\ncpu_pmu->disable = armv7pmu_disable_event;\r\ncpu_pmu->read_counter = armv7pmu_read_counter;\r\ncpu_pmu->write_counter = armv7pmu_write_counter;\r\ncpu_pmu->get_event_idx = armv7pmu_get_event_idx;\r\ncpu_pmu->start = armv7pmu_start;\r\ncpu_pmu->stop = armv7pmu_stop;\r\ncpu_pmu->reset = armv7pmu_reset;\r\ncpu_pmu->max_period = (1LLU << 32) - 1;\r\n}\r\nstatic u32 armv7_read_num_pmnc_events(void)\r\n{\r\nu32 nb_cnt;\r\nnb_cnt = (armv7_pmnc_read() >> ARMV7_PMNC_N_SHIFT) & ARMV7_PMNC_N_MASK;\r\nreturn nb_cnt + 1;\r\n}\r\nstatic int armv7_a8_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_cortex_a8";\r\ncpu_pmu->map_event = armv7_a8_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\nreturn 0;\r\n}\r\nstatic int armv7_a9_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_cortex_a9";\r\ncpu_pmu->map_event = armv7_a9_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\nreturn 0;\r\n}\r\nstatic int armv7_a5_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_cortex_a5";\r\ncpu_pmu->map_event = armv7_a5_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\nreturn 0;\r\n}\r\nstatic int armv7_a15_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_cortex_a15";\r\ncpu_pmu->map_event = armv7_a15_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\ncpu_pmu->set_event_filter = armv7pmu_set_event_filter;\r\nreturn 0;\r\n}\r\nstatic int armv7_a7_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_cortex_a7";\r\ncpu_pmu->map_event = armv7_a7_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\ncpu_pmu->set_event_filter = armv7pmu_set_event_filter;\r\nreturn 0;\r\n}\r\nstatic int armv7_a12_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_cortex_a12";\r\ncpu_pmu->map_event = armv7_a12_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\ncpu_pmu->set_event_filter = armv7pmu_set_event_filter;\r\nreturn 0;\r\n}\r\nstatic int armv7_a17_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7_a12_pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_cortex_a17";\r\nreturn 0;\r\n}\r\nstatic u32 krait_read_pmresrn(int n)\r\n{\r\nu32 val;\r\nswitch (n) {\r\ncase 0:\r\nasm volatile("mrc p15, 1, %0, c9, c15, 0" : "=r" (val));\r\nbreak;\r\ncase 1:\r\nasm volatile("mrc p15, 1, %0, c9, c15, 1" : "=r" (val));\r\nbreak;\r\ncase 2:\r\nasm volatile("mrc p15, 1, %0, c9, c15, 2" : "=r" (val));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn val;\r\n}\r\nstatic void krait_write_pmresrn(int n, u32 val)\r\n{\r\nswitch (n) {\r\ncase 0:\r\nasm volatile("mcr p15, 1, %0, c9, c15, 0" : : "r" (val));\r\nbreak;\r\ncase 1:\r\nasm volatile("mcr p15, 1, %0, c9, c15, 1" : : "r" (val));\r\nbreak;\r\ncase 2:\r\nasm volatile("mcr p15, 1, %0, c9, c15, 2" : : "r" (val));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic u32 krait_read_vpmresr0(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p10, 7, %0, c11, c0, 0" : "=r" (val));\r\nreturn val;\r\n}\r\nstatic void krait_write_vpmresr0(u32 val)\r\n{\r\nasm volatile("mcr p10, 7, %0, c11, c0, 0" : : "r" (val));\r\n}\r\nstatic void krait_pre_vpmresr0(u32 *venum_orig_val, u32 *fp_orig_val)\r\n{\r\nu32 venum_new_val;\r\nu32 fp_new_val;\r\nBUG_ON(preemptible());\r\n*venum_orig_val = get_copro_access();\r\nvenum_new_val = *venum_orig_val | CPACC_SVC(10) | CPACC_SVC(11);\r\nset_copro_access(venum_new_val);\r\n*fp_orig_val = fmrx(FPEXC);\r\nfp_new_val = *fp_orig_val | FPEXC_EN;\r\nfmxr(FPEXC, fp_new_val);\r\n}\r\nstatic void krait_post_vpmresr0(u32 venum_orig_val, u32 fp_orig_val)\r\n{\r\nBUG_ON(preemptible());\r\nfmxr(FPEXC, fp_orig_val);\r\nisb();\r\nset_copro_access(venum_orig_val);\r\n}\r\nstatic u32 krait_get_pmresrn_event(unsigned int region)\r\n{\r\nstatic const u32 pmresrn_table[] = { KRAIT_PMRESR0_GROUP0,\r\nKRAIT_PMRESR1_GROUP0,\r\nKRAIT_PMRESR2_GROUP0 };\r\nreturn pmresrn_table[region];\r\n}\r\nstatic void krait_evt_setup(int idx, u32 config_base)\r\n{\r\nu32 val;\r\nu32 mask;\r\nu32 vval, fval;\r\nunsigned int region;\r\nunsigned int group;\r\nunsigned int code;\r\nunsigned int group_shift;\r\nbool venum_event;\r\nvenum_event = !!(config_base & VENUM_EVENT);\r\nregion = (config_base >> 12) & 0xf;\r\ncode = (config_base >> 4) & 0xff;\r\ngroup = (config_base >> 0) & 0xf;\r\ngroup_shift = group * 8;\r\nmask = 0xff << group_shift;\r\nif (venum_event)\r\nval = KRAIT_VPMRESR0_GROUP0;\r\nelse\r\nval = krait_get_pmresrn_event(region);\r\nval += group;\r\nval |= config_base & (ARMV7_EXCLUDE_USER | ARMV7_EXCLUDE_PL1);\r\narmv7_pmnc_write_evtsel(idx, val);\r\nasm volatile("mcr p15, 0, %0, c9, c15, 0" : : "r" (0));\r\nif (venum_event) {\r\nkrait_pre_vpmresr0(&vval, &fval);\r\nval = krait_read_vpmresr0();\r\nval &= ~mask;\r\nval |= code << group_shift;\r\nval |= PMRESRn_EN;\r\nkrait_write_vpmresr0(val);\r\nkrait_post_vpmresr0(vval, fval);\r\n} else {\r\nval = krait_read_pmresrn(region);\r\nval &= ~mask;\r\nval |= code << group_shift;\r\nval |= PMRESRn_EN;\r\nkrait_write_pmresrn(region, val);\r\n}\r\n}\r\nstatic u32 krait_clear_pmresrn_group(u32 val, int group)\r\n{\r\nu32 mask;\r\nint group_shift;\r\ngroup_shift = group * 8;\r\nmask = 0xff << group_shift;\r\nval &= ~mask;\r\nif (val & ~PMRESRn_EN)\r\nreturn val |= PMRESRn_EN;\r\nreturn 0;\r\n}\r\nstatic void krait_clearpmu(u32 config_base)\r\n{\r\nu32 val;\r\nu32 vval, fval;\r\nunsigned int region;\r\nunsigned int group;\r\nbool venum_event;\r\nvenum_event = !!(config_base & VENUM_EVENT);\r\nregion = (config_base >> 12) & 0xf;\r\ngroup = (config_base >> 0) & 0xf;\r\nif (venum_event) {\r\nkrait_pre_vpmresr0(&vval, &fval);\r\nval = krait_read_vpmresr0();\r\nval = krait_clear_pmresrn_group(val, group);\r\nkrait_write_vpmresr0(val);\r\nkrait_post_vpmresr0(vval, fval);\r\n} else {\r\nval = krait_read_pmresrn(region);\r\nval = krait_clear_pmresrn_group(val, group);\r\nkrait_write_pmresrn(region, val);\r\n}\r\n}\r\nstatic void krait_pmu_disable_event(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_disable_counter(idx);\r\nif (hwc->config_base & KRAIT_EVENT_MASK)\r\nkrait_clearpmu(hwc->config_base);\r\narmv7_pmnc_disable_intens(idx);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void krait_pmu_enable_event(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_disable_counter(idx);\r\nif (hwc->config_base & KRAIT_EVENT_MASK)\r\nkrait_evt_setup(idx, hwc->config_base);\r\nelse\r\narmv7_pmnc_write_evtsel(idx, hwc->config_base);\r\narmv7_pmnc_enable_intens(idx);\r\narmv7_pmnc_enable_counter(idx);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void krait_pmu_reset(void *info)\r\n{\r\nu32 vval, fval;\r\narmv7pmu_reset(info);\r\nkrait_write_pmresrn(0, 0);\r\nkrait_write_pmresrn(1, 0);\r\nkrait_write_pmresrn(2, 0);\r\nkrait_pre_vpmresr0(&vval, &fval);\r\nkrait_write_vpmresr0(0);\r\nkrait_post_vpmresr0(vval, fval);\r\n}\r\nstatic int krait_event_to_bit(struct perf_event *event, unsigned int region,\r\nunsigned int group)\r\n{\r\nint bit;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nif (hwc->config_base & VENUM_EVENT)\r\nbit = KRAIT_VPMRESR0_GROUP0;\r\nelse\r\nbit = krait_get_pmresrn_event(region);\r\nbit -= krait_get_pmresrn_event(0);\r\nbit += group;\r\nbit += ARMV7_IDX_COUNTER_LAST(cpu_pmu) + 1;\r\nreturn bit;\r\n}\r\nstatic int krait_pmu_get_event_idx(struct pmu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nint idx;\r\nint bit = -1;\r\nunsigned int prefix;\r\nunsigned int region;\r\nunsigned int code;\r\nunsigned int group;\r\nbool krait_event;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nregion = (hwc->config_base >> 12) & 0xf;\r\ncode = (hwc->config_base >> 4) & 0xff;\r\ngroup = (hwc->config_base >> 0) & 0xf;\r\nkrait_event = !!(hwc->config_base & KRAIT_EVENT_MASK);\r\nif (krait_event) {\r\nif (group > 3 || region > 2)\r\nreturn -EINVAL;\r\nprefix = hwc->config_base & KRAIT_EVENT_MASK;\r\nif (prefix != KRAIT_EVENT && prefix != VENUM_EVENT)\r\nreturn -EINVAL;\r\nif (prefix == VENUM_EVENT && (code & 0xe0))\r\nreturn -EINVAL;\r\nbit = krait_event_to_bit(event, region, group);\r\nif (test_and_set_bit(bit, cpuc->used_mask))\r\nreturn -EAGAIN;\r\n}\r\nidx = armv7pmu_get_event_idx(cpuc, event);\r\nif (idx < 0 && bit >= 0)\r\nclear_bit(bit, cpuc->used_mask);\r\nreturn idx;\r\n}\r\nstatic void krait_pmu_clear_event_idx(struct pmu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nint bit;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned int region;\r\nunsigned int group;\r\nbool krait_event;\r\nregion = (hwc->config_base >> 12) & 0xf;\r\ngroup = (hwc->config_base >> 0) & 0xf;\r\nkrait_event = !!(hwc->config_base & KRAIT_EVENT_MASK);\r\nif (krait_event) {\r\nbit = krait_event_to_bit(event, region, group);\r\nclear_bit(bit, cpuc->used_mask);\r\n}\r\n}\r\nstatic int krait_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "armv7_krait";\r\nif (of_property_read_bool(cpu_pmu->plat_device->dev.of_node,\r\n"qcom,no-pc-write"))\r\ncpu_pmu->map_event = krait_map_event_no_branch;\r\nelse\r\ncpu_pmu->map_event = krait_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\ncpu_pmu->set_event_filter = armv7pmu_set_event_filter;\r\ncpu_pmu->reset = krait_pmu_reset;\r\ncpu_pmu->enable = krait_pmu_enable_event;\r\ncpu_pmu->disable = krait_pmu_disable_event;\r\ncpu_pmu->get_event_idx = krait_pmu_get_event_idx;\r\ncpu_pmu->clear_event_idx = krait_pmu_clear_event_idx;\r\nreturn 0;\r\n}\r\nstatic inline int armv7_a8_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a9_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a5_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a15_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a7_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a12_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a17_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int krait_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}
