static inline struct ipath_fmr *to_ifmr(struct ib_fmr *ibfmr)\r\n{\r\nreturn container_of(ibfmr, struct ipath_fmr, ibfmr);\r\n}\r\nstruct ib_mr *ipath_get_dma_mr(struct ib_pd *pd, int acc)\r\n{\r\nstruct ipath_mr *mr;\r\nstruct ib_mr *ret;\r\nmr = kzalloc(sizeof *mr, GFP_KERNEL);\r\nif (!mr) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nmr->mr.access_flags = acc;\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic struct ipath_mr *alloc_mr(int count,\r\nstruct ipath_lkey_table *lk_table)\r\n{\r\nstruct ipath_mr *mr;\r\nint m, i = 0;\r\nm = (count + IPATH_SEGSZ - 1) / IPATH_SEGSZ;\r\nmr = kmalloc(sizeof *mr + m * sizeof mr->mr.map[0], GFP_KERNEL);\r\nif (!mr)\r\ngoto done;\r\nfor (; i < m; i++) {\r\nmr->mr.map[i] = kmalloc(sizeof *mr->mr.map[0], GFP_KERNEL);\r\nif (!mr->mr.map[i])\r\ngoto bail;\r\n}\r\nmr->mr.mapsz = m;\r\nif (!ipath_alloc_lkey(lk_table, &mr->mr))\r\ngoto bail;\r\nmr->ibmr.rkey = mr->ibmr.lkey = mr->mr.lkey;\r\ngoto done;\r\nbail:\r\nwhile (i) {\r\ni--;\r\nkfree(mr->mr.map[i]);\r\n}\r\nkfree(mr);\r\nmr = NULL;\r\ndone:\r\nreturn mr;\r\n}\r\nstruct ib_mr *ipath_reg_phys_mr(struct ib_pd *pd,\r\nstruct ib_phys_buf *buffer_list,\r\nint num_phys_buf, int acc, u64 *iova_start)\r\n{\r\nstruct ipath_mr *mr;\r\nint n, m, i;\r\nstruct ib_mr *ret;\r\nmr = alloc_mr(num_phys_buf, &to_idev(pd->device)->lk_table);\r\nif (mr == NULL) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nmr->mr.pd = pd;\r\nmr->mr.user_base = *iova_start;\r\nmr->mr.iova = *iova_start;\r\nmr->mr.length = 0;\r\nmr->mr.offset = 0;\r\nmr->mr.access_flags = acc;\r\nmr->mr.max_segs = num_phys_buf;\r\nmr->umem = NULL;\r\nm = 0;\r\nn = 0;\r\nfor (i = 0; i < num_phys_buf; i++) {\r\nmr->mr.map[m]->segs[n].vaddr = (void *) buffer_list[i].addr;\r\nmr->mr.map[m]->segs[n].length = buffer_list[i].size;\r\nmr->mr.length += buffer_list[i].size;\r\nn++;\r\nif (n == IPATH_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nstruct ib_mr *ipath_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\r\nu64 virt_addr, int mr_access_flags,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ipath_mr *mr;\r\nstruct ib_umem *umem;\r\nint n, m, entry;\r\nstruct scatterlist *sg;\r\nstruct ib_mr *ret;\r\nif (length == 0) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\numem = ib_umem_get(pd->uobject->context, start, length,\r\nmr_access_flags, 0);\r\nif (IS_ERR(umem))\r\nreturn (void *) umem;\r\nn = umem->nmap;\r\nmr = alloc_mr(n, &to_idev(pd->device)->lk_table);\r\nif (!mr) {\r\nret = ERR_PTR(-ENOMEM);\r\nib_umem_release(umem);\r\ngoto bail;\r\n}\r\nmr->mr.pd = pd;\r\nmr->mr.user_base = start;\r\nmr->mr.iova = virt_addr;\r\nmr->mr.length = length;\r\nmr->mr.offset = umem->offset;\r\nmr->mr.access_flags = mr_access_flags;\r\nmr->mr.max_segs = n;\r\nmr->umem = umem;\r\nm = 0;\r\nn = 0;\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {\r\nvoid *vaddr;\r\nvaddr = page_address(sg_page(sg));\r\nif (!vaddr) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nmr->mr.map[m]->segs[n].vaddr = vaddr;\r\nmr->mr.map[m]->segs[n].length = umem->page_size;\r\nn++;\r\nif (n == IPATH_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_dereg_mr(struct ib_mr *ibmr)\r\n{\r\nstruct ipath_mr *mr = to_imr(ibmr);\r\nint i;\r\nipath_free_lkey(&to_idev(ibmr->device)->lk_table, ibmr->lkey);\r\ni = mr->mr.mapsz;\r\nwhile (i) {\r\ni--;\r\nkfree(mr->mr.map[i]);\r\n}\r\nif (mr->umem)\r\nib_umem_release(mr->umem);\r\nkfree(mr);\r\nreturn 0;\r\n}\r\nstruct ib_fmr *ipath_alloc_fmr(struct ib_pd *pd, int mr_access_flags,\r\nstruct ib_fmr_attr *fmr_attr)\r\n{\r\nstruct ipath_fmr *fmr;\r\nint m, i = 0;\r\nstruct ib_fmr *ret;\r\nm = (fmr_attr->max_pages + IPATH_SEGSZ - 1) / IPATH_SEGSZ;\r\nfmr = kmalloc(sizeof *fmr + m * sizeof fmr->mr.map[0], GFP_KERNEL);\r\nif (!fmr)\r\ngoto bail;\r\nfor (; i < m; i++) {\r\nfmr->mr.map[i] = kmalloc(sizeof *fmr->mr.map[0],\r\nGFP_KERNEL);\r\nif (!fmr->mr.map[i])\r\ngoto bail;\r\n}\r\nfmr->mr.mapsz = m;\r\nif (!ipath_alloc_lkey(&to_idev(pd->device)->lk_table, &fmr->mr))\r\ngoto bail;\r\nfmr->ibfmr.rkey = fmr->ibfmr.lkey = fmr->mr.lkey;\r\nfmr->mr.pd = pd;\r\nfmr->mr.user_base = 0;\r\nfmr->mr.iova = 0;\r\nfmr->mr.length = 0;\r\nfmr->mr.offset = 0;\r\nfmr->mr.access_flags = mr_access_flags;\r\nfmr->mr.max_segs = fmr_attr->max_pages;\r\nfmr->page_shift = fmr_attr->page_shift;\r\nret = &fmr->ibfmr;\r\ngoto done;\r\nbail:\r\nwhile (i)\r\nkfree(fmr->mr.map[--i]);\r\nkfree(fmr);\r\nret = ERR_PTR(-ENOMEM);\r\ndone:\r\nreturn ret;\r\n}\r\nint ipath_map_phys_fmr(struct ib_fmr *ibfmr, u64 * page_list,\r\nint list_len, u64 iova)\r\n{\r\nstruct ipath_fmr *fmr = to_ifmr(ibfmr);\r\nstruct ipath_lkey_table *rkt;\r\nunsigned long flags;\r\nint m, n, i;\r\nu32 ps;\r\nint ret;\r\nif (list_len > fmr->mr.max_segs) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nrkt = &to_idev(ibfmr->device)->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = iova;\r\nfmr->mr.iova = iova;\r\nps = 1 << fmr->page_shift;\r\nfmr->mr.length = list_len * ps;\r\nm = 0;\r\nn = 0;\r\nps = 1 << fmr->page_shift;\r\nfor (i = 0; i < list_len; i++) {\r\nfmr->mr.map[m]->segs[n].vaddr = (void *) page_list[i];\r\nfmr->mr.map[m]->segs[n].length = ps;\r\nif (++n == IPATH_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_unmap_fmr(struct list_head *fmr_list)\r\n{\r\nstruct ipath_fmr *fmr;\r\nstruct ipath_lkey_table *rkt;\r\nunsigned long flags;\r\nlist_for_each_entry(fmr, fmr_list, ibfmr.list) {\r\nrkt = &to_idev(fmr->ibfmr.device)->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = 0;\r\nfmr->mr.iova = 0;\r\nfmr->mr.length = 0;\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nint ipath_dealloc_fmr(struct ib_fmr *ibfmr)\r\n{\r\nstruct ipath_fmr *fmr = to_ifmr(ibfmr);\r\nint i;\r\nipath_free_lkey(&to_idev(ibfmr->device)->lk_table, ibfmr->lkey);\r\ni = fmr->mr.mapsz;\r\nwhile (i)\r\nkfree(fmr->mr.map[--i]);\r\nkfree(fmr);\r\nreturn 0;\r\n}
