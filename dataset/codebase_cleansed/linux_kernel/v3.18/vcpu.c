void physical_mode_init(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->arch.mode_flags = GUEST_IN_PHY;\r\n}\r\nvoid switch_to_physical_rid(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long psr;\r\npsr = ia64_clear_ic();\r\nia64_set_rr(VRN0<<VRN_SHIFT, vcpu->arch.metaphysical_rr0);\r\nia64_srlz_d();\r\nia64_set_rr(VRN4<<VRN_SHIFT, vcpu->arch.metaphysical_rr4);\r\nia64_srlz_d();\r\nia64_set_psr(psr);\r\nreturn;\r\n}\r\nvoid switch_to_virtual_rid(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long psr;\r\npsr = ia64_clear_ic();\r\nia64_set_rr(VRN0 << VRN_SHIFT, vcpu->arch.metaphysical_saved_rr0);\r\nia64_srlz_d();\r\nia64_set_rr(VRN4 << VRN_SHIFT, vcpu->arch.metaphysical_saved_rr4);\r\nia64_srlz_d();\r\nia64_set_psr(psr);\r\nreturn;\r\n}\r\nstatic int mm_switch_action(struct ia64_psr opsr, struct ia64_psr npsr)\r\n{\r\nreturn mm_switch_table[MODE_IND(opsr)][MODE_IND(npsr)];\r\n}\r\nvoid switch_mm_mode(struct kvm_vcpu *vcpu, struct ia64_psr old_psr,\r\nstruct ia64_psr new_psr)\r\n{\r\nint act;\r\nact = mm_switch_action(old_psr, new_psr);\r\nswitch (act) {\r\ncase SW_V2P:\r\nswitch_to_physical_rid(vcpu);\r\nvcpu->arch.mode_flags |= GUEST_IN_PHY;\r\nbreak;\r\ncase SW_P2V:\r\nswitch_to_virtual_rid(vcpu);\r\nvcpu->arch.mode_flags &= ~GUEST_IN_PHY;\r\nbreak;\r\ncase SW_SELF:\r\nbreak;\r\ncase SW_NOP:\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn;\r\n}\r\nvoid check_mm_mode_switch(struct kvm_vcpu *vcpu, struct ia64_psr old_psr,\r\nstruct ia64_psr new_psr)\r\n{\r\nif ((old_psr.dt != new_psr.dt)\r\n|| (old_psr.it != new_psr.it)\r\n|| (old_psr.rt != new_psr.rt))\r\nswitch_mm_mode(vcpu, old_psr, new_psr);\r\nreturn;\r\n}\r\nvoid prepare_if_physical_mode(struct kvm_vcpu *vcpu)\r\n{\r\nif (is_physical_mode(vcpu)) {\r\nvcpu->arch.mode_flags |= GUEST_PHY_EMUL;\r\nswitch_to_virtual_rid(vcpu);\r\n}\r\nreturn;\r\n}\r\nvoid recover_if_physical_mode(struct kvm_vcpu *vcpu)\r\n{\r\nif (is_physical_mode(vcpu))\r\nswitch_to_physical_rid(vcpu);\r\nvcpu->arch.mode_flags &= ~GUEST_PHY_EMUL;\r\nreturn;\r\n}\r\nstatic inline unsigned long\r\nrotate_reg(unsigned long sor, unsigned long rrb, unsigned long reg)\r\n{\r\nreg += rrb;\r\nif (reg >= sor)\r\nreg -= sor;\r\nreturn reg;\r\n}\r\nstatic inline unsigned long fph_index(struct kvm_pt_regs *regs,\r\nlong regnum)\r\n{\r\nunsigned long rrb_fr = (regs->cr_ifs >> 25) & 0x7f;\r\nreturn rotate_reg(96, rrb_fr, (regnum - IA64_FIRST_ROTATING_FR));\r\n}\r\nstatic inline unsigned long *kvm_rse_skip_regs(unsigned long *addr,\r\nlong num_regs)\r\n{\r\nlong delta = ia64_rse_slot_num(addr) + num_regs;\r\nint i = 0;\r\nif (num_regs < 0)\r\ndelta -= 0x3e;\r\nif (delta < 0) {\r\nwhile (delta <= -0x3f) {\r\ni--;\r\ndelta += 0x3f;\r\n}\r\n} else {\r\nwhile (delta >= 0x3f) {\r\ni++;\r\ndelta -= 0x3f;\r\n}\r\n}\r\nreturn addr + num_regs + i;\r\n}\r\nstatic void get_rse_reg(struct kvm_pt_regs *regs, unsigned long r1,\r\nunsigned long *val, int *nat)\r\n{\r\nunsigned long *bsp, *addr, *rnat_addr, *bspstore;\r\nunsigned long *kbs = (void *) current_vcpu + VMM_RBS_OFFSET;\r\nunsigned long nat_mask;\r\nunsigned long old_rsc, new_rsc;\r\nlong sof = (regs->cr_ifs) & 0x7f;\r\nlong sor = (((regs->cr_ifs >> 14) & 0xf) << 3);\r\nlong rrb_gr = (regs->cr_ifs >> 18) & 0x7f;\r\nlong ridx = r1 - 32;\r\nif (ridx < sor)\r\nridx = rotate_reg(sor, rrb_gr, ridx);\r\nold_rsc = ia64_getreg(_IA64_REG_AR_RSC);\r\nnew_rsc = old_rsc&(~(0x3));\r\nia64_setreg(_IA64_REG_AR_RSC, new_rsc);\r\nbspstore = (unsigned long *)ia64_getreg(_IA64_REG_AR_BSPSTORE);\r\nbsp = kbs + (regs->loadrs >> 19);\r\naddr = kvm_rse_skip_regs(bsp, -sof + ridx);\r\nnat_mask = 1UL << ia64_rse_slot_num(addr);\r\nrnat_addr = ia64_rse_rnat_addr(addr);\r\nif (addr >= bspstore) {\r\nia64_flushrs();\r\nia64_mf();\r\nbspstore = (unsigned long *)ia64_getreg(_IA64_REG_AR_BSPSTORE);\r\n}\r\n*val = *addr;\r\nif (nat) {\r\nif (bspstore < rnat_addr)\r\n*nat = (int)!!(ia64_getreg(_IA64_REG_AR_RNAT)\r\n& nat_mask);\r\nelse\r\n*nat = (int)!!((*rnat_addr) & nat_mask);\r\nia64_setreg(_IA64_REG_AR_RSC, old_rsc);\r\n}\r\n}\r\nvoid set_rse_reg(struct kvm_pt_regs *regs, unsigned long r1,\r\nunsigned long val, unsigned long nat)\r\n{\r\nunsigned long *bsp, *bspstore, *addr, *rnat_addr;\r\nunsigned long *kbs = (void *) current_vcpu + VMM_RBS_OFFSET;\r\nunsigned long nat_mask;\r\nunsigned long old_rsc, new_rsc, psr;\r\nunsigned long rnat;\r\nlong sof = (regs->cr_ifs) & 0x7f;\r\nlong sor = (((regs->cr_ifs >> 14) & 0xf) << 3);\r\nlong rrb_gr = (regs->cr_ifs >> 18) & 0x7f;\r\nlong ridx = r1 - 32;\r\nif (ridx < sor)\r\nridx = rotate_reg(sor, rrb_gr, ridx);\r\nold_rsc = ia64_getreg(_IA64_REG_AR_RSC);\r\nnew_rsc = old_rsc & (~0x3fff0003);\r\nia64_setreg(_IA64_REG_AR_RSC, new_rsc);\r\nbsp = kbs + (regs->loadrs >> 19);\r\naddr = kvm_rse_skip_regs(bsp, -sof + ridx);\r\nnat_mask = 1UL << ia64_rse_slot_num(addr);\r\nrnat_addr = ia64_rse_rnat_addr(addr);\r\nlocal_irq_save(psr);\r\nbspstore = (unsigned long *)ia64_getreg(_IA64_REG_AR_BSPSTORE);\r\nif (addr >= bspstore) {\r\nia64_flushrs();\r\nia64_mf();\r\n*addr = val;\r\nbspstore = (unsigned long *)ia64_getreg(_IA64_REG_AR_BSPSTORE);\r\nrnat = ia64_getreg(_IA64_REG_AR_RNAT);\r\nif (bspstore < rnat_addr)\r\nrnat = rnat & (~nat_mask);\r\nelse\r\n*rnat_addr = (*rnat_addr)&(~nat_mask);\r\nia64_mf();\r\nia64_loadrs();\r\nia64_setreg(_IA64_REG_AR_RNAT, rnat);\r\n} else {\r\nrnat = ia64_getreg(_IA64_REG_AR_RNAT);\r\n*addr = val;\r\nif (bspstore < rnat_addr)\r\nrnat = rnat&(~nat_mask);\r\nelse\r\n*rnat_addr = (*rnat_addr) & (~nat_mask);\r\nia64_setreg(_IA64_REG_AR_BSPSTORE, (unsigned long)bspstore);\r\nia64_setreg(_IA64_REG_AR_RNAT, rnat);\r\n}\r\nlocal_irq_restore(psr);\r\nia64_setreg(_IA64_REG_AR_RSC, old_rsc);\r\n}\r\nvoid getreg(unsigned long regnum, unsigned long *val,\r\nint *nat, struct kvm_pt_regs *regs)\r\n{\r\nunsigned long addr, *unat;\r\nif (regnum >= IA64_FIRST_STACKED_GR) {\r\nget_rse_reg(regs, regnum, val, nat);\r\nreturn;\r\n}\r\naddr = (unsigned long)regs;\r\nunat = &regs->eml_unat;\r\naddr += gr_info[regnum];\r\n*val = *(unsigned long *)addr;\r\nif (nat)\r\n*nat = (*unat >> ((addr >> 3) & 0x3f)) & 0x1UL;\r\n}\r\nvoid setreg(unsigned long regnum, unsigned long val,\r\nint nat, struct kvm_pt_regs *regs)\r\n{\r\nunsigned long addr;\r\nunsigned long bitmask;\r\nunsigned long *unat;\r\nif (regnum >= IA64_FIRST_STACKED_GR) {\r\nset_rse_reg(regs, regnum, val, nat);\r\nreturn;\r\n}\r\naddr = (unsigned long)regs;\r\nunat = &regs->eml_unat;\r\naddr += gr_info[regnum];\r\n*(unsigned long *)addr = val;\r\nbitmask = 1UL << ((addr >> 3) & 0x3f);\r\nif (nat)\r\n*unat |= bitmask;\r\nelse\r\n*unat &= ~bitmask;\r\n}\r\nu64 vcpu_get_gr(struct kvm_vcpu *vcpu, unsigned long reg)\r\n{\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nunsigned long val;\r\nif (!reg)\r\nreturn 0;\r\ngetreg(reg, &val, 0, regs);\r\nreturn val;\r\n}\r\nvoid vcpu_set_gr(struct kvm_vcpu *vcpu, unsigned long reg, u64 value, int nat)\r\n{\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nlong sof = (regs->cr_ifs) & 0x7f;\r\nif (!reg)\r\nreturn;\r\nif (reg >= sof + 32)\r\nreturn;\r\nsetreg(reg, value, nat, regs);\r\n}\r\nvoid getfpreg(unsigned long regnum, struct ia64_fpreg *fpval,\r\nstruct kvm_pt_regs *regs)\r\n{\r\nif (regnum >= IA64_FIRST_ROTATING_FR)\r\nregnum = IA64_FIRST_ROTATING_FR + fph_index(regs, regnum);\r\n#define CASE_FIXED_FP(reg) \\r\ncase (reg) : \\r\nia64_stf_spill(fpval, reg); \\r\nbreak\r\nswitch (regnum) {\r\nCASE_FIXED_FP(0);\r\nCASE_FIXED_FP(1);\r\nCASE_FIXED_FP(2);\r\nCASE_FIXED_FP(3);\r\nCASE_FIXED_FP(4);\r\nCASE_FIXED_FP(5);\r\nCASE_FIXED_FP(6);\r\nCASE_FIXED_FP(7);\r\nCASE_FIXED_FP(8);\r\nCASE_FIXED_FP(9);\r\nCASE_FIXED_FP(10);\r\nCASE_FIXED_FP(11);\r\nCASE_FIXED_FP(12);\r\nCASE_FIXED_FP(13);\r\nCASE_FIXED_FP(14);\r\nCASE_FIXED_FP(15);\r\nCASE_FIXED_FP(16);\r\nCASE_FIXED_FP(17);\r\nCASE_FIXED_FP(18);\r\nCASE_FIXED_FP(19);\r\nCASE_FIXED_FP(20);\r\nCASE_FIXED_FP(21);\r\nCASE_FIXED_FP(22);\r\nCASE_FIXED_FP(23);\r\nCASE_FIXED_FP(24);\r\nCASE_FIXED_FP(25);\r\nCASE_FIXED_FP(26);\r\nCASE_FIXED_FP(27);\r\nCASE_FIXED_FP(28);\r\nCASE_FIXED_FP(29);\r\nCASE_FIXED_FP(30);\r\nCASE_FIXED_FP(31);\r\nCASE_FIXED_FP(32);\r\nCASE_FIXED_FP(33);\r\nCASE_FIXED_FP(34);\r\nCASE_FIXED_FP(35);\r\nCASE_FIXED_FP(36);\r\nCASE_FIXED_FP(37);\r\nCASE_FIXED_FP(38);\r\nCASE_FIXED_FP(39);\r\nCASE_FIXED_FP(40);\r\nCASE_FIXED_FP(41);\r\nCASE_FIXED_FP(42);\r\nCASE_FIXED_FP(43);\r\nCASE_FIXED_FP(44);\r\nCASE_FIXED_FP(45);\r\nCASE_FIXED_FP(46);\r\nCASE_FIXED_FP(47);\r\nCASE_FIXED_FP(48);\r\nCASE_FIXED_FP(49);\r\nCASE_FIXED_FP(50);\r\nCASE_FIXED_FP(51);\r\nCASE_FIXED_FP(52);\r\nCASE_FIXED_FP(53);\r\nCASE_FIXED_FP(54);\r\nCASE_FIXED_FP(55);\r\nCASE_FIXED_FP(56);\r\nCASE_FIXED_FP(57);\r\nCASE_FIXED_FP(58);\r\nCASE_FIXED_FP(59);\r\nCASE_FIXED_FP(60);\r\nCASE_FIXED_FP(61);\r\nCASE_FIXED_FP(62);\r\nCASE_FIXED_FP(63);\r\nCASE_FIXED_FP(64);\r\nCASE_FIXED_FP(65);\r\nCASE_FIXED_FP(66);\r\nCASE_FIXED_FP(67);\r\nCASE_FIXED_FP(68);\r\nCASE_FIXED_FP(69);\r\nCASE_FIXED_FP(70);\r\nCASE_FIXED_FP(71);\r\nCASE_FIXED_FP(72);\r\nCASE_FIXED_FP(73);\r\nCASE_FIXED_FP(74);\r\nCASE_FIXED_FP(75);\r\nCASE_FIXED_FP(76);\r\nCASE_FIXED_FP(77);\r\nCASE_FIXED_FP(78);\r\nCASE_FIXED_FP(79);\r\nCASE_FIXED_FP(80);\r\nCASE_FIXED_FP(81);\r\nCASE_FIXED_FP(82);\r\nCASE_FIXED_FP(83);\r\nCASE_FIXED_FP(84);\r\nCASE_FIXED_FP(85);\r\nCASE_FIXED_FP(86);\r\nCASE_FIXED_FP(87);\r\nCASE_FIXED_FP(88);\r\nCASE_FIXED_FP(89);\r\nCASE_FIXED_FP(90);\r\nCASE_FIXED_FP(91);\r\nCASE_FIXED_FP(92);\r\nCASE_FIXED_FP(93);\r\nCASE_FIXED_FP(94);\r\nCASE_FIXED_FP(95);\r\nCASE_FIXED_FP(96);\r\nCASE_FIXED_FP(97);\r\nCASE_FIXED_FP(98);\r\nCASE_FIXED_FP(99);\r\nCASE_FIXED_FP(100);\r\nCASE_FIXED_FP(101);\r\nCASE_FIXED_FP(102);\r\nCASE_FIXED_FP(103);\r\nCASE_FIXED_FP(104);\r\nCASE_FIXED_FP(105);\r\nCASE_FIXED_FP(106);\r\nCASE_FIXED_FP(107);\r\nCASE_FIXED_FP(108);\r\nCASE_FIXED_FP(109);\r\nCASE_FIXED_FP(110);\r\nCASE_FIXED_FP(111);\r\nCASE_FIXED_FP(112);\r\nCASE_FIXED_FP(113);\r\nCASE_FIXED_FP(114);\r\nCASE_FIXED_FP(115);\r\nCASE_FIXED_FP(116);\r\nCASE_FIXED_FP(117);\r\nCASE_FIXED_FP(118);\r\nCASE_FIXED_FP(119);\r\nCASE_FIXED_FP(120);\r\nCASE_FIXED_FP(121);\r\nCASE_FIXED_FP(122);\r\nCASE_FIXED_FP(123);\r\nCASE_FIXED_FP(124);\r\nCASE_FIXED_FP(125);\r\nCASE_FIXED_FP(126);\r\nCASE_FIXED_FP(127);\r\n}\r\n#undef CASE_FIXED_FP\r\n}\r\nvoid setfpreg(unsigned long regnum, struct ia64_fpreg *fpval,\r\nstruct kvm_pt_regs *regs)\r\n{\r\nif (regnum >= IA64_FIRST_ROTATING_FR)\r\nregnum = IA64_FIRST_ROTATING_FR + fph_index(regs, regnum);\r\n#define CASE_FIXED_FP(reg) \\r\ncase (reg) : \\r\nia64_ldf_fill(reg, fpval); \\r\nbreak\r\nswitch (regnum) {\r\nCASE_FIXED_FP(2);\r\nCASE_FIXED_FP(3);\r\nCASE_FIXED_FP(4);\r\nCASE_FIXED_FP(5);\r\nCASE_FIXED_FP(6);\r\nCASE_FIXED_FP(7);\r\nCASE_FIXED_FP(8);\r\nCASE_FIXED_FP(9);\r\nCASE_FIXED_FP(10);\r\nCASE_FIXED_FP(11);\r\nCASE_FIXED_FP(12);\r\nCASE_FIXED_FP(13);\r\nCASE_FIXED_FP(14);\r\nCASE_FIXED_FP(15);\r\nCASE_FIXED_FP(16);\r\nCASE_FIXED_FP(17);\r\nCASE_FIXED_FP(18);\r\nCASE_FIXED_FP(19);\r\nCASE_FIXED_FP(20);\r\nCASE_FIXED_FP(21);\r\nCASE_FIXED_FP(22);\r\nCASE_FIXED_FP(23);\r\nCASE_FIXED_FP(24);\r\nCASE_FIXED_FP(25);\r\nCASE_FIXED_FP(26);\r\nCASE_FIXED_FP(27);\r\nCASE_FIXED_FP(28);\r\nCASE_FIXED_FP(29);\r\nCASE_FIXED_FP(30);\r\nCASE_FIXED_FP(31);\r\nCASE_FIXED_FP(32);\r\nCASE_FIXED_FP(33);\r\nCASE_FIXED_FP(34);\r\nCASE_FIXED_FP(35);\r\nCASE_FIXED_FP(36);\r\nCASE_FIXED_FP(37);\r\nCASE_FIXED_FP(38);\r\nCASE_FIXED_FP(39);\r\nCASE_FIXED_FP(40);\r\nCASE_FIXED_FP(41);\r\nCASE_FIXED_FP(42);\r\nCASE_FIXED_FP(43);\r\nCASE_FIXED_FP(44);\r\nCASE_FIXED_FP(45);\r\nCASE_FIXED_FP(46);\r\nCASE_FIXED_FP(47);\r\nCASE_FIXED_FP(48);\r\nCASE_FIXED_FP(49);\r\nCASE_FIXED_FP(50);\r\nCASE_FIXED_FP(51);\r\nCASE_FIXED_FP(52);\r\nCASE_FIXED_FP(53);\r\nCASE_FIXED_FP(54);\r\nCASE_FIXED_FP(55);\r\nCASE_FIXED_FP(56);\r\nCASE_FIXED_FP(57);\r\nCASE_FIXED_FP(58);\r\nCASE_FIXED_FP(59);\r\nCASE_FIXED_FP(60);\r\nCASE_FIXED_FP(61);\r\nCASE_FIXED_FP(62);\r\nCASE_FIXED_FP(63);\r\nCASE_FIXED_FP(64);\r\nCASE_FIXED_FP(65);\r\nCASE_FIXED_FP(66);\r\nCASE_FIXED_FP(67);\r\nCASE_FIXED_FP(68);\r\nCASE_FIXED_FP(69);\r\nCASE_FIXED_FP(70);\r\nCASE_FIXED_FP(71);\r\nCASE_FIXED_FP(72);\r\nCASE_FIXED_FP(73);\r\nCASE_FIXED_FP(74);\r\nCASE_FIXED_FP(75);\r\nCASE_FIXED_FP(76);\r\nCASE_FIXED_FP(77);\r\nCASE_FIXED_FP(78);\r\nCASE_FIXED_FP(79);\r\nCASE_FIXED_FP(80);\r\nCASE_FIXED_FP(81);\r\nCASE_FIXED_FP(82);\r\nCASE_FIXED_FP(83);\r\nCASE_FIXED_FP(84);\r\nCASE_FIXED_FP(85);\r\nCASE_FIXED_FP(86);\r\nCASE_FIXED_FP(87);\r\nCASE_FIXED_FP(88);\r\nCASE_FIXED_FP(89);\r\nCASE_FIXED_FP(90);\r\nCASE_FIXED_FP(91);\r\nCASE_FIXED_FP(92);\r\nCASE_FIXED_FP(93);\r\nCASE_FIXED_FP(94);\r\nCASE_FIXED_FP(95);\r\nCASE_FIXED_FP(96);\r\nCASE_FIXED_FP(97);\r\nCASE_FIXED_FP(98);\r\nCASE_FIXED_FP(99);\r\nCASE_FIXED_FP(100);\r\nCASE_FIXED_FP(101);\r\nCASE_FIXED_FP(102);\r\nCASE_FIXED_FP(103);\r\nCASE_FIXED_FP(104);\r\nCASE_FIXED_FP(105);\r\nCASE_FIXED_FP(106);\r\nCASE_FIXED_FP(107);\r\nCASE_FIXED_FP(108);\r\nCASE_FIXED_FP(109);\r\nCASE_FIXED_FP(110);\r\nCASE_FIXED_FP(111);\r\nCASE_FIXED_FP(112);\r\nCASE_FIXED_FP(113);\r\nCASE_FIXED_FP(114);\r\nCASE_FIXED_FP(115);\r\nCASE_FIXED_FP(116);\r\nCASE_FIXED_FP(117);\r\nCASE_FIXED_FP(118);\r\nCASE_FIXED_FP(119);\r\nCASE_FIXED_FP(120);\r\nCASE_FIXED_FP(121);\r\nCASE_FIXED_FP(122);\r\nCASE_FIXED_FP(123);\r\nCASE_FIXED_FP(124);\r\nCASE_FIXED_FP(125);\r\nCASE_FIXED_FP(126);\r\nCASE_FIXED_FP(127);\r\n}\r\n}\r\nvoid vcpu_get_fpreg(struct kvm_vcpu *vcpu, unsigned long reg,\r\nstruct ia64_fpreg *val)\r\n{\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\ngetfpreg(reg, val, regs);\r\n}\r\nvoid vcpu_set_fpreg(struct kvm_vcpu *vcpu, unsigned long reg,\r\nstruct ia64_fpreg *val)\r\n{\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nif (reg > 1)\r\nsetfpreg(reg, val, regs);\r\n}\r\nstatic long kvm_get_itc(struct kvm_vcpu *vcpu)\r\n{\r\n#if defined(CONFIG_IA64_SGI_SN2) || defined(CONFIG_IA64_GENERIC)\r\nstruct kvm *kvm = (struct kvm *)KVM_VM_BASE;\r\nif (kvm->arch.is_sn2)\r\nreturn (*SN_RTC_BASE);\r\nelse\r\n#endif\r\nreturn ia64_getreg(_IA64_REG_AR_ITC);\r\n}\r\nu64 vcpu_get_itc(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long guest_itc;\r\nguest_itc = VMX(vcpu, itc_offset) + kvm_get_itc(vcpu);\r\nif (guest_itc >= VMX(vcpu, last_itc)) {\r\nVMX(vcpu, last_itc) = guest_itc;\r\nreturn guest_itc;\r\n} else\r\nreturn VMX(vcpu, last_itc);\r\n}\r\nstatic void vcpu_set_itc(struct kvm_vcpu *vcpu, u64 val)\r\n{\r\nstruct kvm_vcpu *v;\r\nstruct kvm *kvm;\r\nint i;\r\nlong itc_offset = val - kvm_get_itc(vcpu);\r\nunsigned long vitv = VCPU(vcpu, itv);\r\nkvm = (struct kvm *)KVM_VM_BASE;\r\nif (kvm_vcpu_is_bsp(vcpu)) {\r\nfor (i = 0; i < atomic_read(&kvm->online_vcpus); i++) {\r\nv = (struct kvm_vcpu *)((char *)vcpu +\r\nsizeof(struct kvm_vcpu_data) * i);\r\nVMX(v, itc_offset) = itc_offset;\r\nVMX(v, last_itc) = 0;\r\n}\r\n}\r\nVMX(vcpu, last_itc) = 0;\r\nif (VCPU(vcpu, itm) <= val) {\r\nVMX(vcpu, itc_check) = 0;\r\nvcpu_unpend_interrupt(vcpu, vitv);\r\n} else {\r\nVMX(vcpu, itc_check) = 1;\r\nvcpu_set_itm(vcpu, VCPU(vcpu, itm));\r\n}\r\n}\r\nstatic inline u64 vcpu_get_itm(struct kvm_vcpu *vcpu)\r\n{\r\nreturn ((u64)VCPU(vcpu, itm));\r\n}\r\nstatic inline void vcpu_set_itm(struct kvm_vcpu *vcpu, u64 val)\r\n{\r\nunsigned long vitv = VCPU(vcpu, itv);\r\nVCPU(vcpu, itm) = val;\r\nif (val > vcpu_get_itc(vcpu)) {\r\nVMX(vcpu, itc_check) = 1;\r\nvcpu_unpend_interrupt(vcpu, vitv);\r\nVMX(vcpu, timer_pending) = 0;\r\n} else\r\nVMX(vcpu, itc_check) = 0;\r\n}\r\nstatic inline void vcpu_set_itv(struct kvm_vcpu *vcpu, u64 val)\r\n{\r\nVCPU(vcpu, itv) = val;\r\nif (!ITV_IRQ_MASK(val) && vcpu->arch.timer_pending) {\r\nvcpu_pend_interrupt(vcpu, ITV_VECTOR(val));\r\nvcpu->arch.timer_pending = 0;\r\n}\r\n}\r\nstatic inline void vcpu_set_eoi(struct kvm_vcpu *vcpu, u64 val)\r\n{\r\nint vec;\r\nvec = highest_inservice_irq(vcpu);\r\nif (vec == NULL_VECTOR)\r\nreturn;\r\nVMX(vcpu, insvc[vec >> 6]) &= ~(1UL << (vec & 63));\r\nVCPU(vcpu, eoi) = 0;\r\nvcpu->arch.irq_new_pending = 1;\r\n}\r\nint irq_masked(struct kvm_vcpu *vcpu, int h_pending, int h_inservice)\r\n{\r\nunion ia64_tpr vtpr;\r\nvtpr.val = VCPU(vcpu, tpr);\r\nif (h_inservice == NMI_VECTOR)\r\nreturn IRQ_MASKED_BY_INSVC;\r\nif (h_pending == NMI_VECTOR) {\r\nreturn IRQ_NO_MASKED;\r\n}\r\nif (h_inservice == ExtINT_VECTOR)\r\nreturn IRQ_MASKED_BY_INSVC;\r\nif (h_pending == ExtINT_VECTOR) {\r\nif (vtpr.mmi) {\r\nreturn IRQ_MASKED_BY_VTPR;\r\n} else\r\nreturn IRQ_NO_MASKED;\r\n}\r\nif (is_higher_irq(h_pending, h_inservice)) {\r\nif (is_higher_class(h_pending, vtpr.mic + (vtpr.mmi << 4)))\r\nreturn IRQ_NO_MASKED;\r\nelse\r\nreturn IRQ_MASKED_BY_VTPR;\r\n} else {\r\nreturn IRQ_MASKED_BY_INSVC;\r\n}\r\n}\r\nvoid vcpu_pend_interrupt(struct kvm_vcpu *vcpu, u8 vec)\r\n{\r\nlong spsr;\r\nint ret;\r\nlocal_irq_save(spsr);\r\nret = test_and_set_bit(vec, &VCPU(vcpu, irr[0]));\r\nlocal_irq_restore(spsr);\r\nvcpu->arch.irq_new_pending = 1;\r\n}\r\nvoid vcpu_unpend_interrupt(struct kvm_vcpu *vcpu, u8 vec)\r\n{\r\nlong spsr;\r\nint ret;\r\nlocal_irq_save(spsr);\r\nret = test_and_clear_bit(vec, &VCPU(vcpu, irr[0]));\r\nlocal_irq_restore(spsr);\r\nif (ret) {\r\nvcpu->arch.irq_new_pending = 1;\r\nwmb();\r\n}\r\n}\r\nvoid update_vhpi(struct kvm_vcpu *vcpu, int vec)\r\n{\r\nu64 vhpi;\r\nif (vec == NULL_VECTOR)\r\nvhpi = 0;\r\nelse if (vec == NMI_VECTOR)\r\nvhpi = 32;\r\nelse if (vec == ExtINT_VECTOR)\r\nvhpi = 16;\r\nelse\r\nvhpi = vec >> 4;\r\nVCPU(vcpu, vhpi) = vhpi;\r\nif (VCPU(vcpu, vac).a_int)\r\nia64_call_vsa(PAL_VPS_SET_PENDING_INTERRUPT,\r\n(u64)vcpu->arch.vpd, 0, 0, 0, 0, 0, 0);\r\n}\r\nu64 vcpu_get_ivr(struct kvm_vcpu *vcpu)\r\n{\r\nint vec, h_inservice, mask;\r\nvec = highest_pending_irq(vcpu);\r\nh_inservice = highest_inservice_irq(vcpu);\r\nmask = irq_masked(vcpu, vec, h_inservice);\r\nif (vec == NULL_VECTOR || mask == IRQ_MASKED_BY_INSVC) {\r\nif (VCPU(vcpu, vhpi))\r\nupdate_vhpi(vcpu, NULL_VECTOR);\r\nreturn IA64_SPURIOUS_INT_VECTOR;\r\n}\r\nif (mask == IRQ_MASKED_BY_VTPR) {\r\nupdate_vhpi(vcpu, vec);\r\nreturn IA64_SPURIOUS_INT_VECTOR;\r\n}\r\nVMX(vcpu, insvc[vec >> 6]) |= (1UL << (vec & 63));\r\nvcpu_unpend_interrupt(vcpu, vec);\r\nreturn (u64)vec;\r\n}\r\nu64 vcpu_thash(struct kvm_vcpu *vcpu, u64 vadr)\r\n{\r\nunion ia64_pta vpta;\r\nunion ia64_rr vrr;\r\nu64 pval;\r\nu64 vhpt_offset;\r\nvpta.val = vcpu_get_pta(vcpu);\r\nvrr.val = vcpu_get_rr(vcpu, vadr);\r\nvhpt_offset = ((vadr >> vrr.ps) << 3) & ((1UL << (vpta.size)) - 1);\r\nif (vpta.vf) {\r\npval = ia64_call_vsa(PAL_VPS_THASH, vadr, vrr.val,\r\nvpta.val, 0, 0, 0, 0);\r\n} else {\r\npval = (vadr & VRN_MASK) | vhpt_offset |\r\n(vpta.val << 3 >> (vpta.size + 3) << (vpta.size));\r\n}\r\nreturn pval;\r\n}\r\nu64 vcpu_ttag(struct kvm_vcpu *vcpu, u64 vadr)\r\n{\r\nunion ia64_rr vrr;\r\nunion ia64_pta vpta;\r\nu64 pval;\r\nvpta.val = vcpu_get_pta(vcpu);\r\nvrr.val = vcpu_get_rr(vcpu, vadr);\r\nif (vpta.vf) {\r\npval = ia64_call_vsa(PAL_VPS_TTAG, vadr, vrr.val,\r\n0, 0, 0, 0, 0);\r\n} else\r\npval = 1;\r\nreturn pval;\r\n}\r\nu64 vcpu_tak(struct kvm_vcpu *vcpu, u64 vadr)\r\n{\r\nstruct thash_data *data;\r\nunion ia64_pta vpta;\r\nu64 key;\r\nvpta.val = vcpu_get_pta(vcpu);\r\nif (vpta.vf == 0) {\r\nkey = 1;\r\nreturn key;\r\n}\r\ndata = vtlb_lookup(vcpu, vadr, D_TLB);\r\nif (!data || !data->p)\r\nkey = 1;\r\nelse\r\nkey = data->key;\r\nreturn key;\r\n}\r\nvoid kvm_thash(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long thash, vadr;\r\nvadr = vcpu_get_gr(vcpu, inst.M46.r3);\r\nthash = vcpu_thash(vcpu, vadr);\r\nvcpu_set_gr(vcpu, inst.M46.r1, thash, 0);\r\n}\r\nvoid kvm_ttag(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long tag, vadr;\r\nvadr = vcpu_get_gr(vcpu, inst.M46.r3);\r\ntag = vcpu_ttag(vcpu, vadr);\r\nvcpu_set_gr(vcpu, inst.M46.r1, tag, 0);\r\n}\r\nint vcpu_tpa(struct kvm_vcpu *vcpu, u64 vadr, unsigned long *padr)\r\n{\r\nstruct thash_data *data;\r\nunion ia64_isr visr, pt_isr;\r\nstruct kvm_pt_regs *regs;\r\nstruct ia64_psr vpsr;\r\nregs = vcpu_regs(vcpu);\r\npt_isr.val = VMX(vcpu, cr_isr);\r\nvisr.val = 0;\r\nvisr.ei = pt_isr.ei;\r\nvisr.ir = pt_isr.ir;\r\nvpsr = *(struct ia64_psr *)&VCPU(vcpu, vpsr);\r\nvisr.na = 1;\r\ndata = vhpt_lookup(vadr);\r\nif (data) {\r\nif (data->p == 0) {\r\nvcpu_set_isr(vcpu, visr.val);\r\ndata_page_not_present(vcpu, vadr);\r\nreturn IA64_FAULT;\r\n} else if (data->ma == VA_MATTR_NATPAGE) {\r\nvcpu_set_isr(vcpu, visr.val);\r\ndnat_page_consumption(vcpu, vadr);\r\nreturn IA64_FAULT;\r\n} else {\r\n*padr = (data->gpaddr >> data->ps << data->ps) |\r\n(vadr & (PSIZE(data->ps) - 1));\r\nreturn IA64_NO_FAULT;\r\n}\r\n}\r\ndata = vtlb_lookup(vcpu, vadr, D_TLB);\r\nif (data) {\r\nif (data->p == 0) {\r\nvcpu_set_isr(vcpu, visr.val);\r\ndata_page_not_present(vcpu, vadr);\r\nreturn IA64_FAULT;\r\n} else if (data->ma == VA_MATTR_NATPAGE) {\r\nvcpu_set_isr(vcpu, visr.val);\r\ndnat_page_consumption(vcpu, vadr);\r\nreturn IA64_FAULT;\r\n} else{\r\n*padr = ((data->ppn >> (data->ps - 12)) << data->ps)\r\n| (vadr & (PSIZE(data->ps) - 1));\r\nreturn IA64_NO_FAULT;\r\n}\r\n}\r\nif (!vhpt_enabled(vcpu, vadr, NA_REF)) {\r\nif (vpsr.ic) {\r\nvcpu_set_isr(vcpu, visr.val);\r\nalt_dtlb(vcpu, vadr);\r\nreturn IA64_FAULT;\r\n} else {\r\nnested_dtlb(vcpu);\r\nreturn IA64_FAULT;\r\n}\r\n} else {\r\nif (vpsr.ic) {\r\nvcpu_set_isr(vcpu, visr.val);\r\ndvhpt_fault(vcpu, vadr);\r\nreturn IA64_FAULT;\r\n} else{\r\nnested_dtlb(vcpu);\r\nreturn IA64_FAULT;\r\n}\r\n}\r\nreturn IA64_NO_FAULT;\r\n}\r\nint kvm_tpa(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r1, r3;\r\nr3 = vcpu_get_gr(vcpu, inst.M46.r3);\r\nif (vcpu_tpa(vcpu, r3, &r1))\r\nreturn IA64_FAULT;\r\nvcpu_set_gr(vcpu, inst.M46.r1, r1, 0);\r\nreturn(IA64_NO_FAULT);\r\n}\r\nvoid kvm_tak(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r1, r3;\r\nr3 = vcpu_get_gr(vcpu, inst.M46.r3);\r\nr1 = vcpu_tak(vcpu, r3);\r\nvcpu_set_gr(vcpu, inst.M46.r1, r1, 0);\r\n}\r\nvoid vcpu_itc_i(struct kvm_vcpu *vcpu, u64 pte, u64 itir, u64 ifa)\r\n{\r\nthash_purge_and_insert(vcpu, pte, itir, ifa, I_TLB);\r\n}\r\nvoid vcpu_itc_d(struct kvm_vcpu *vcpu, u64 pte, u64 itir, u64 ifa)\r\n{\r\nthash_purge_and_insert(vcpu, pte, itir, ifa, D_TLB);\r\n}\r\nvoid vcpu_itr_i(struct kvm_vcpu *vcpu, u64 slot, u64 pte, u64 itir, u64 ifa)\r\n{\r\nu64 ps, va, rid;\r\nstruct thash_data *p_itr;\r\nps = itir_ps(itir);\r\nva = PAGEALIGN(ifa, ps);\r\npte &= ~PAGE_FLAGS_RV_MASK;\r\nrid = vcpu_get_rr(vcpu, ifa);\r\nrid = rid & RR_RID_MASK;\r\np_itr = (struct thash_data *)&vcpu->arch.itrs[slot];\r\nvcpu_set_tr(p_itr, pte, itir, va, rid);\r\nvcpu_quick_region_set(VMX(vcpu, itr_regions), va);\r\n}\r\nvoid vcpu_itr_d(struct kvm_vcpu *vcpu, u64 slot, u64 pte, u64 itir, u64 ifa)\r\n{\r\nu64 gpfn;\r\nu64 ps, va, rid;\r\nstruct thash_data *p_dtr;\r\nps = itir_ps(itir);\r\nva = PAGEALIGN(ifa, ps);\r\npte &= ~PAGE_FLAGS_RV_MASK;\r\nif (ps != _PAGE_SIZE_16M)\r\nthash_purge_entries(vcpu, va, ps);\r\ngpfn = (pte & _PAGE_PPN_MASK) >> PAGE_SHIFT;\r\nif (__gpfn_is_io(gpfn))\r\npte |= VTLB_PTE_IO;\r\nrid = vcpu_get_rr(vcpu, va);\r\nrid = rid & RR_RID_MASK;\r\np_dtr = (struct thash_data *)&vcpu->arch.dtrs[slot];\r\nvcpu_set_tr((struct thash_data *)&vcpu->arch.dtrs[slot],\r\npte, itir, va, rid);\r\nvcpu_quick_region_set(VMX(vcpu, dtr_regions), va);\r\n}\r\nvoid vcpu_ptr_d(struct kvm_vcpu *vcpu, u64 ifa, u64 ps)\r\n{\r\nint index;\r\nu64 va;\r\nva = PAGEALIGN(ifa, ps);\r\nwhile ((index = vtr_find_overlap(vcpu, va, ps, D_TLB)) >= 0)\r\nvcpu->arch.dtrs[index].page_flags = 0;\r\nthash_purge_entries(vcpu, va, ps);\r\n}\r\nvoid vcpu_ptr_i(struct kvm_vcpu *vcpu, u64 ifa, u64 ps)\r\n{\r\nint index;\r\nu64 va;\r\nva = PAGEALIGN(ifa, ps);\r\nwhile ((index = vtr_find_overlap(vcpu, va, ps, I_TLB)) >= 0)\r\nvcpu->arch.itrs[index].page_flags = 0;\r\nthash_purge_entries(vcpu, va, ps);\r\n}\r\nvoid vcpu_ptc_l(struct kvm_vcpu *vcpu, u64 va, u64 ps)\r\n{\r\nva = PAGEALIGN(va, ps);\r\nthash_purge_entries(vcpu, va, ps);\r\n}\r\nvoid vcpu_ptc_e(struct kvm_vcpu *vcpu, u64 va)\r\n{\r\nthash_purge_all(vcpu);\r\n}\r\nvoid vcpu_ptc_ga(struct kvm_vcpu *vcpu, u64 va, u64 ps)\r\n{\r\nstruct exit_ctl_data *p = &vcpu->arch.exit_data;\r\nlong psr;\r\nlocal_irq_save(psr);\r\np->exit_reason = EXIT_REASON_PTC_G;\r\np->u.ptc_g_data.rr = vcpu_get_rr(vcpu, va);\r\np->u.ptc_g_data.vaddr = va;\r\np->u.ptc_g_data.ps = ps;\r\nvmm_transition(vcpu);\r\nvcpu_ptc_l(vcpu, va, ps);\r\nlocal_irq_restore(psr);\r\n}\r\nvoid vcpu_ptc_g(struct kvm_vcpu *vcpu, u64 va, u64 ps)\r\n{\r\nvcpu_ptc_ga(vcpu, va, ps);\r\n}\r\nvoid kvm_ptc_e(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long ifa;\r\nifa = vcpu_get_gr(vcpu, inst.M45.r3);\r\nvcpu_ptc_e(vcpu, ifa);\r\n}\r\nvoid kvm_ptc_g(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long ifa, itir;\r\nifa = vcpu_get_gr(vcpu, inst.M45.r3);\r\nitir = vcpu_get_gr(vcpu, inst.M45.r2);\r\nvcpu_ptc_g(vcpu, ifa, itir_ps(itir));\r\n}\r\nvoid kvm_ptc_ga(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long ifa, itir;\r\nifa = vcpu_get_gr(vcpu, inst.M45.r3);\r\nitir = vcpu_get_gr(vcpu, inst.M45.r2);\r\nvcpu_ptc_ga(vcpu, ifa, itir_ps(itir));\r\n}\r\nvoid kvm_ptc_l(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long ifa, itir;\r\nifa = vcpu_get_gr(vcpu, inst.M45.r3);\r\nitir = vcpu_get_gr(vcpu, inst.M45.r2);\r\nvcpu_ptc_l(vcpu, ifa, itir_ps(itir));\r\n}\r\nvoid kvm_ptr_d(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long ifa, itir;\r\nifa = vcpu_get_gr(vcpu, inst.M45.r3);\r\nitir = vcpu_get_gr(vcpu, inst.M45.r2);\r\nvcpu_ptr_d(vcpu, ifa, itir_ps(itir));\r\n}\r\nvoid kvm_ptr_i(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long ifa, itir;\r\nifa = vcpu_get_gr(vcpu, inst.M45.r3);\r\nitir = vcpu_get_gr(vcpu, inst.M45.r2);\r\nvcpu_ptr_i(vcpu, ifa, itir_ps(itir));\r\n}\r\nvoid kvm_itr_d(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long itir, ifa, pte, slot;\r\nslot = vcpu_get_gr(vcpu, inst.M45.r3);\r\npte = vcpu_get_gr(vcpu, inst.M45.r2);\r\nitir = vcpu_get_itir(vcpu);\r\nifa = vcpu_get_ifa(vcpu);\r\nvcpu_itr_d(vcpu, slot, pte, itir, ifa);\r\n}\r\nvoid kvm_itr_i(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long itir, ifa, pte, slot;\r\nslot = vcpu_get_gr(vcpu, inst.M45.r3);\r\npte = vcpu_get_gr(vcpu, inst.M45.r2);\r\nitir = vcpu_get_itir(vcpu);\r\nifa = vcpu_get_ifa(vcpu);\r\nvcpu_itr_i(vcpu, slot, pte, itir, ifa);\r\n}\r\nvoid kvm_itc_d(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long itir, ifa, pte;\r\nitir = vcpu_get_itir(vcpu);\r\nifa = vcpu_get_ifa(vcpu);\r\npte = vcpu_get_gr(vcpu, inst.M45.r2);\r\nvcpu_itc_d(vcpu, pte, itir, ifa);\r\n}\r\nvoid kvm_itc_i(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long itir, ifa, pte;\r\nitir = vcpu_get_itir(vcpu);\r\nifa = vcpu_get_ifa(vcpu);\r\npte = vcpu_get_gr(vcpu, inst.M45.r2);\r\nvcpu_itc_i(vcpu, pte, itir, ifa);\r\n}\r\nvoid kvm_mov_to_ar_imm(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long imm;\r\nif (inst.M30.s)\r\nimm = -inst.M30.imm;\r\nelse\r\nimm = inst.M30.imm;\r\nvcpu_set_itc(vcpu, imm);\r\n}\r\nvoid kvm_mov_to_ar_reg(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r2;\r\nr2 = vcpu_get_gr(vcpu, inst.M29.r2);\r\nvcpu_set_itc(vcpu, r2);\r\n}\r\nvoid kvm_mov_from_ar_reg(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r1;\r\nr1 = vcpu_get_itc(vcpu);\r\nvcpu_set_gr(vcpu, inst.M31.r1, r1, 0);\r\n}\r\nunsigned long vcpu_get_pkr(struct kvm_vcpu *vcpu, unsigned long reg)\r\n{\r\nreturn ((unsigned long)ia64_get_pkr(reg));\r\n}\r\nvoid vcpu_set_pkr(struct kvm_vcpu *vcpu, unsigned long reg, unsigned long val)\r\n{\r\nia64_set_pkr(reg, val);\r\n}\r\nunsigned long vcpu_set_rr(struct kvm_vcpu *vcpu, unsigned long reg,\r\nunsigned long val)\r\n{\r\nunion ia64_rr oldrr, newrr;\r\nunsigned long rrval;\r\nstruct exit_ctl_data *p = &vcpu->arch.exit_data;\r\nunsigned long psr;\r\noldrr.val = vcpu_get_rr(vcpu, reg);\r\nnewrr.val = val;\r\nvcpu->arch.vrr[reg >> VRN_SHIFT] = val;\r\nswitch ((unsigned long)(reg >> VRN_SHIFT)) {\r\ncase VRN6:\r\nvcpu->arch.vmm_rr = vrrtomrr(val);\r\nlocal_irq_save(psr);\r\np->exit_reason = EXIT_REASON_SWITCH_RR6;\r\nvmm_transition(vcpu);\r\nlocal_irq_restore(psr);\r\nbreak;\r\ncase VRN4:\r\nrrval = vrrtomrr(val);\r\nvcpu->arch.metaphysical_saved_rr4 = rrval;\r\nif (!is_physical_mode(vcpu))\r\nia64_set_rr(reg, rrval);\r\nbreak;\r\ncase VRN0:\r\nrrval = vrrtomrr(val);\r\nvcpu->arch.metaphysical_saved_rr0 = rrval;\r\nif (!is_physical_mode(vcpu))\r\nia64_set_rr(reg, rrval);\r\nbreak;\r\ndefault:\r\nia64_set_rr(reg, vrrtomrr(val));\r\nbreak;\r\n}\r\nreturn (IA64_NO_FAULT);\r\n}\r\nvoid kvm_mov_to_rr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r2;\r\nr3 = vcpu_get_gr(vcpu, inst.M42.r3);\r\nr2 = vcpu_get_gr(vcpu, inst.M42.r2);\r\nvcpu_set_rr(vcpu, r3, r2);\r\n}\r\nvoid kvm_mov_to_dbr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\n}\r\nvoid kvm_mov_to_ibr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\n}\r\nvoid kvm_mov_to_pmc(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r2;\r\nr3 = vcpu_get_gr(vcpu, inst.M42.r3);\r\nr2 = vcpu_get_gr(vcpu, inst.M42.r2);\r\nvcpu_set_pmc(vcpu, r3, r2);\r\n}\r\nvoid kvm_mov_to_pmd(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r2;\r\nr3 = vcpu_get_gr(vcpu, inst.M42.r3);\r\nr2 = vcpu_get_gr(vcpu, inst.M42.r2);\r\nvcpu_set_pmd(vcpu, r3, r2);\r\n}\r\nvoid kvm_mov_to_pkr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nu64 r3, r2;\r\nr3 = vcpu_get_gr(vcpu, inst.M42.r3);\r\nr2 = vcpu_get_gr(vcpu, inst.M42.r2);\r\nvcpu_set_pkr(vcpu, r3, r2);\r\n}\r\nvoid kvm_mov_from_rr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r1;\r\nr3 = vcpu_get_gr(vcpu, inst.M43.r3);\r\nr1 = vcpu_get_rr(vcpu, r3);\r\nvcpu_set_gr(vcpu, inst.M43.r1, r1, 0);\r\n}\r\nvoid kvm_mov_from_pkr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r1;\r\nr3 = vcpu_get_gr(vcpu, inst.M43.r3);\r\nr1 = vcpu_get_pkr(vcpu, r3);\r\nvcpu_set_gr(vcpu, inst.M43.r1, r1, 0);\r\n}\r\nvoid kvm_mov_from_dbr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r1;\r\nr3 = vcpu_get_gr(vcpu, inst.M43.r3);\r\nr1 = vcpu_get_dbr(vcpu, r3);\r\nvcpu_set_gr(vcpu, inst.M43.r1, r1, 0);\r\n}\r\nvoid kvm_mov_from_ibr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r1;\r\nr3 = vcpu_get_gr(vcpu, inst.M43.r3);\r\nr1 = vcpu_get_ibr(vcpu, r3);\r\nvcpu_set_gr(vcpu, inst.M43.r1, r1, 0);\r\n}\r\nvoid kvm_mov_from_pmc(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r1;\r\nr3 = vcpu_get_gr(vcpu, inst.M43.r3);\r\nr1 = vcpu_get_pmc(vcpu, r3);\r\nvcpu_set_gr(vcpu, inst.M43.r1, r1, 0);\r\n}\r\nunsigned long vcpu_get_cpuid(struct kvm_vcpu *vcpu, unsigned long reg)\r\n{\r\nif (reg > (ia64_get_cpuid(3) & 0xff))\r\nreturn 0;\r\nelse\r\nreturn ia64_get_cpuid(reg);\r\n}\r\nvoid kvm_mov_from_cpuid(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r3, r1;\r\nr3 = vcpu_get_gr(vcpu, inst.M43.r3);\r\nr1 = vcpu_get_cpuid(vcpu, r3);\r\nvcpu_set_gr(vcpu, inst.M43.r1, r1, 0);\r\n}\r\nvoid vcpu_set_tpr(struct kvm_vcpu *vcpu, unsigned long val)\r\n{\r\nVCPU(vcpu, tpr) = val;\r\nvcpu->arch.irq_check = 1;\r\n}\r\nunsigned long kvm_mov_to_cr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long r2;\r\nr2 = vcpu_get_gr(vcpu, inst.M32.r2);\r\nVCPU(vcpu, vcr[inst.M32.cr3]) = r2;\r\nswitch (inst.M32.cr3) {\r\ncase 0:\r\nvcpu_set_dcr(vcpu, r2);\r\nbreak;\r\ncase 1:\r\nvcpu_set_itm(vcpu, r2);\r\nbreak;\r\ncase 66:\r\nvcpu_set_tpr(vcpu, r2);\r\nbreak;\r\ncase 67:\r\nvcpu_set_eoi(vcpu, r2);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nunsigned long kvm_mov_from_cr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long tgt = inst.M33.r1;\r\nunsigned long val;\r\nswitch (inst.M33.cr3) {\r\ncase 65:\r\nval = vcpu_get_ivr(vcpu);\r\nvcpu_set_gr(vcpu, tgt, val, 0);\r\nbreak;\r\ncase 67:\r\nvcpu_set_gr(vcpu, tgt, 0L, 0);\r\nbreak;\r\ndefault:\r\nval = VCPU(vcpu, vcr[inst.M33.cr3]);\r\nvcpu_set_gr(vcpu, tgt, val, 0);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nvoid vcpu_set_psr(struct kvm_vcpu *vcpu, unsigned long val)\r\n{\r\nunsigned long mask;\r\nstruct kvm_pt_regs *regs;\r\nstruct ia64_psr old_psr, new_psr;\r\nold_psr = *(struct ia64_psr *)&VCPU(vcpu, vpsr);\r\nregs = vcpu_regs(vcpu);\r\nif (val & (IA64_PSR_PK | IA64_PSR_IS | IA64_PSR_VM))\r\npanic_vm(vcpu, "Only support guests with vpsr.pk =0 "\r\n"& vpsr.is=0\n");\r\nVCPU(vcpu, vpsr) = val\r\n& (~(IA64_PSR_ID | IA64_PSR_DA | IA64_PSR_DD |\r\nIA64_PSR_SS | IA64_PSR_ED | IA64_PSR_IA));\r\nif (!old_psr.i && (val & IA64_PSR_I)) {\r\nvcpu->arch.irq_check = 1;\r\n}\r\nnew_psr = *(struct ia64_psr *)&VCPU(vcpu, vpsr);\r\nmask = IA64_PSR_IC + IA64_PSR_I + IA64_PSR_DT + IA64_PSR_SI +\r\nIA64_PSR_RT + IA64_PSR_MC + IA64_PSR_IT + IA64_PSR_BN +\r\nIA64_PSR_VM;\r\nregs->cr_ipsr = (regs->cr_ipsr & mask) | (val & (~mask));\r\ncheck_mm_mode_switch(vcpu, old_psr, new_psr);\r\nreturn ;\r\n}\r\nunsigned long vcpu_cover(struct kvm_vcpu *vcpu)\r\n{\r\nstruct ia64_psr vpsr;\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nvpsr = *(struct ia64_psr *)&VCPU(vcpu, vpsr);\r\nif (!vpsr.ic)\r\nVCPU(vcpu, ifs) = regs->cr_ifs;\r\nregs->cr_ifs = IA64_IFS_V;\r\nreturn (IA64_NO_FAULT);\r\n}\r\nvoid vcpu_bsw0(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long i;\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nunsigned long *r = &regs->r16;\r\nunsigned long *b0 = &VCPU(vcpu, vbgr[0]);\r\nunsigned long *b1 = &VCPU(vcpu, vgr[0]);\r\nunsigned long *runat = &regs->eml_unat;\r\nunsigned long *b0unat = &VCPU(vcpu, vbnat);\r\nunsigned long *b1unat = &VCPU(vcpu, vnat);\r\nif (VCPU(vcpu, vpsr) & IA64_PSR_BN) {\r\nfor (i = 0; i < 16; i++) {\r\n*b1++ = *r;\r\n*r++ = *b0++;\r\n}\r\nvcpu_bsw0_unat(i, b0unat, b1unat, runat,\r\nVMM_PT_REGS_R16_SLOT);\r\nVCPU(vcpu, vpsr) &= ~IA64_PSR_BN;\r\n}\r\n}\r\nvoid vcpu_bsw1(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long i;\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nunsigned long *r = &regs->r16;\r\nunsigned long *b0 = &VCPU(vcpu, vbgr[0]);\r\nunsigned long *b1 = &VCPU(vcpu, vgr[0]);\r\nunsigned long *runat = &regs->eml_unat;\r\nunsigned long *b0unat = &VCPU(vcpu, vbnat);\r\nunsigned long *b1unat = &VCPU(vcpu, vnat);\r\nif (!(VCPU(vcpu, vpsr) & IA64_PSR_BN)) {\r\nfor (i = 0; i < 16; i++) {\r\n*b0++ = *r;\r\n*r++ = *b1++;\r\n}\r\nvcpu_bsw1_unat(i, b0unat, b1unat, runat,\r\nVMM_PT_REGS_R16_SLOT);\r\nVCPU(vcpu, vpsr) |= IA64_PSR_BN;\r\n}\r\n}\r\nvoid vcpu_rfi(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long ifs, psr;\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\npsr = VCPU(vcpu, ipsr);\r\nif (psr & IA64_PSR_BN)\r\nvcpu_bsw1(vcpu);\r\nelse\r\nvcpu_bsw0(vcpu);\r\nvcpu_set_psr(vcpu, psr);\r\nifs = VCPU(vcpu, ifs);\r\nif (ifs >> 63)\r\nregs->cr_ifs = ifs;\r\nregs->cr_iip = VCPU(vcpu, iip);\r\n}\r\nunsigned long vcpu_get_psr(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long mask;\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nmask = IA64_PSR_BE | IA64_PSR_UP | IA64_PSR_AC | IA64_PSR_MFL |\r\nIA64_PSR_MFH | IA64_PSR_CPL | IA64_PSR_RI;\r\nreturn (VCPU(vcpu, vpsr) & ~mask) | (regs->cr_ipsr & mask);\r\n}\r\nvoid kvm_rsm(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long vpsr;\r\nunsigned long imm24 = (inst.M44.i<<23) | (inst.M44.i2<<21)\r\n| inst.M44.imm;\r\nvpsr = vcpu_get_psr(vcpu);\r\nvpsr &= (~imm24);\r\nvcpu_set_psr(vcpu, vpsr);\r\n}\r\nvoid kvm_ssm(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long vpsr;\r\nunsigned long imm24 = (inst.M44.i << 23) | (inst.M44.i2 << 21)\r\n| inst.M44.imm;\r\nvpsr = vcpu_get_psr(vcpu);\r\nvpsr |= imm24;\r\nvcpu_set_psr(vcpu, vpsr);\r\n}\r\nvoid vcpu_set_psr_l(struct kvm_vcpu *vcpu, unsigned long val)\r\n{\r\nval = (val & MASK(0, 32)) | (vcpu_get_psr(vcpu) & MASK(32, 32));\r\nvcpu_set_psr(vcpu, val);\r\n}\r\nvoid kvm_mov_to_psr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long val;\r\nval = vcpu_get_gr(vcpu, inst.M35.r2);\r\nvcpu_set_psr_l(vcpu, val);\r\n}\r\nvoid kvm_mov_from_psr(struct kvm_vcpu *vcpu, INST64 inst)\r\n{\r\nunsigned long val;\r\nval = vcpu_get_psr(vcpu);\r\nval = (val & MASK(0, 32)) | (val & MASK(35, 2));\r\nvcpu_set_gr(vcpu, inst.M33.r1, val, 0);\r\n}\r\nvoid vcpu_increment_iip(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nstruct ia64_psr *ipsr = (struct ia64_psr *)&regs->cr_ipsr;\r\nif (ipsr->ri == 2) {\r\nipsr->ri = 0;\r\nregs->cr_iip += 16;\r\n} else\r\nipsr->ri++;\r\n}\r\nvoid vcpu_decrement_iip(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_pt_regs *regs = vcpu_regs(vcpu);\r\nstruct ia64_psr *ipsr = (struct ia64_psr *)&regs->cr_ipsr;\r\nif (ipsr->ri == 0) {\r\nipsr->ri = 2;\r\nregs->cr_iip -= 16;\r\n} else\r\nipsr->ri--;\r\n}\r\nvoid kvm_emulate(struct kvm_vcpu *vcpu, struct kvm_pt_regs *regs)\r\n{\r\nunsigned long status, cause, opcode ;\r\nINST64 inst;\r\nstatus = IA64_NO_FAULT;\r\ncause = VMX(vcpu, cause);\r\nopcode = VMX(vcpu, opcode);\r\ninst.inst = opcode;\r\nprepare_if_physical_mode(vcpu);\r\nswitch (cause) {\r\ncase EVENT_RSM:\r\nkvm_rsm(vcpu, inst);\r\nbreak;\r\ncase EVENT_SSM:\r\nkvm_ssm(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_PSR:\r\nkvm_mov_to_psr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_PSR:\r\nkvm_mov_from_psr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_CR:\r\nkvm_mov_from_cr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_CR:\r\nkvm_mov_to_cr(vcpu, inst);\r\nbreak;\r\ncase EVENT_BSW_0:\r\nvcpu_bsw0(vcpu);\r\nbreak;\r\ncase EVENT_BSW_1:\r\nvcpu_bsw1(vcpu);\r\nbreak;\r\ncase EVENT_COVER:\r\nvcpu_cover(vcpu);\r\nbreak;\r\ncase EVENT_RFI:\r\nvcpu_rfi(vcpu);\r\nbreak;\r\ncase EVENT_ITR_D:\r\nkvm_itr_d(vcpu, inst);\r\nbreak;\r\ncase EVENT_ITR_I:\r\nkvm_itr_i(vcpu, inst);\r\nbreak;\r\ncase EVENT_PTR_D:\r\nkvm_ptr_d(vcpu, inst);\r\nbreak;\r\ncase EVENT_PTR_I:\r\nkvm_ptr_i(vcpu, inst);\r\nbreak;\r\ncase EVENT_ITC_D:\r\nkvm_itc_d(vcpu, inst);\r\nbreak;\r\ncase EVENT_ITC_I:\r\nkvm_itc_i(vcpu, inst);\r\nbreak;\r\ncase EVENT_PTC_L:\r\nkvm_ptc_l(vcpu, inst);\r\nbreak;\r\ncase EVENT_PTC_G:\r\nkvm_ptc_g(vcpu, inst);\r\nbreak;\r\ncase EVENT_PTC_GA:\r\nkvm_ptc_ga(vcpu, inst);\r\nbreak;\r\ncase EVENT_PTC_E:\r\nkvm_ptc_e(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_RR:\r\nkvm_mov_to_rr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_RR:\r\nkvm_mov_from_rr(vcpu, inst);\r\nbreak;\r\ncase EVENT_THASH:\r\nkvm_thash(vcpu, inst);\r\nbreak;\r\ncase EVENT_TTAG:\r\nkvm_ttag(vcpu, inst);\r\nbreak;\r\ncase EVENT_TPA:\r\nstatus = kvm_tpa(vcpu, inst);\r\nbreak;\r\ncase EVENT_TAK:\r\nkvm_tak(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_AR_IMM:\r\nkvm_mov_to_ar_imm(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_AR:\r\nkvm_mov_to_ar_reg(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_AR:\r\nkvm_mov_from_ar_reg(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_DBR:\r\nkvm_mov_to_dbr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_IBR:\r\nkvm_mov_to_ibr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_PMC:\r\nkvm_mov_to_pmc(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_PMD:\r\nkvm_mov_to_pmd(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_TO_PKR:\r\nkvm_mov_to_pkr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_DBR:\r\nkvm_mov_from_dbr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_IBR:\r\nkvm_mov_from_ibr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_PMC:\r\nkvm_mov_from_pmc(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_PKR:\r\nkvm_mov_from_pkr(vcpu, inst);\r\nbreak;\r\ncase EVENT_MOV_FROM_CPUID:\r\nkvm_mov_from_cpuid(vcpu, inst);\r\nbreak;\r\ncase EVENT_VMSW:\r\nstatus = IA64_FAULT;\r\nbreak;\r\ndefault:\r\nbreak;\r\n};\r\nif (status == IA64_NO_FAULT && cause != EVENT_RFI)\r\nvcpu_increment_iip(vcpu);\r\nrecover_if_physical_mode(vcpu);\r\n}\r\nvoid init_vcpu(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nvcpu->arch.mode_flags = GUEST_IN_PHY;\r\nVMX(vcpu, vrr[0]) = 0x38;\r\nVMX(vcpu, vrr[1]) = 0x38;\r\nVMX(vcpu, vrr[2]) = 0x38;\r\nVMX(vcpu, vrr[3]) = 0x38;\r\nVMX(vcpu, vrr[4]) = 0x38;\r\nVMX(vcpu, vrr[5]) = 0x38;\r\nVMX(vcpu, vrr[6]) = 0x38;\r\nVMX(vcpu, vrr[7]) = 0x38;\r\nVCPU(vcpu, vpsr) = IA64_PSR_BN;\r\nVCPU(vcpu, dcr) = 0;\r\nVCPU(vcpu, pta) = 15 << 2;\r\nVCPU(vcpu, itv) = 0x10000;\r\nVCPU(vcpu, itm) = 0;\r\nVMX(vcpu, last_itc) = 0;\r\nVCPU(vcpu, lid) = VCPU_LID(vcpu);\r\nVCPU(vcpu, ivr) = 0;\r\nVCPU(vcpu, tpr) = 0x10000;\r\nVCPU(vcpu, eoi) = 0;\r\nVCPU(vcpu, irr[0]) = 0;\r\nVCPU(vcpu, irr[1]) = 0;\r\nVCPU(vcpu, irr[2]) = 0;\r\nVCPU(vcpu, irr[3]) = 0;\r\nVCPU(vcpu, pmv) = 0x10000;\r\nVCPU(vcpu, cmcv) = 0x10000;\r\nVCPU(vcpu, lrr0) = 0x10000;\r\nVCPU(vcpu, lrr1) = 0x10000;\r\nupdate_vhpi(vcpu, NULL_VECTOR);\r\nVLSAPIC_XTP(vcpu) = 0x80;\r\nfor (i = 0; i < 4; i++)\r\nVLSAPIC_INSVC(vcpu, i) = 0;\r\n}\r\nvoid kvm_init_all_rr(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long psr;\r\nlocal_irq_save(psr);\r\nvcpu->arch.metaphysical_saved_rr0 = vrrtomrr(VMX(vcpu, vrr[VRN0]));\r\nvcpu->arch.metaphysical_saved_rr4 = vrrtomrr(VMX(vcpu, vrr[VRN4]));\r\nif (is_physical_mode(vcpu)) {\r\nif (vcpu->arch.mode_flags & GUEST_PHY_EMUL)\r\npanic_vm(vcpu, "Machine Status conflicts!\n");\r\nia64_set_rr((VRN0 << VRN_SHIFT), vcpu->arch.metaphysical_rr0);\r\nia64_dv_serialize_data();\r\nia64_set_rr((VRN4 << VRN_SHIFT), vcpu->arch.metaphysical_rr4);\r\nia64_dv_serialize_data();\r\n} else {\r\nia64_set_rr((VRN0 << VRN_SHIFT),\r\nvcpu->arch.metaphysical_saved_rr0);\r\nia64_dv_serialize_data();\r\nia64_set_rr((VRN4 << VRN_SHIFT),\r\nvcpu->arch.metaphysical_saved_rr4);\r\nia64_dv_serialize_data();\r\n}\r\nia64_set_rr((VRN1 << VRN_SHIFT),\r\nvrrtomrr(VMX(vcpu, vrr[VRN1])));\r\nia64_dv_serialize_data();\r\nia64_set_rr((VRN2 << VRN_SHIFT),\r\nvrrtomrr(VMX(vcpu, vrr[VRN2])));\r\nia64_dv_serialize_data();\r\nia64_set_rr((VRN3 << VRN_SHIFT),\r\nvrrtomrr(VMX(vcpu, vrr[VRN3])));\r\nia64_dv_serialize_data();\r\nia64_set_rr((VRN5 << VRN_SHIFT),\r\nvrrtomrr(VMX(vcpu, vrr[VRN5])));\r\nia64_dv_serialize_data();\r\nia64_set_rr((VRN7 << VRN_SHIFT),\r\nvrrtomrr(VMX(vcpu, vrr[VRN7])));\r\nia64_dv_serialize_data();\r\nia64_srlz_d();\r\nia64_set_psr(psr);\r\n}\r\nint vmm_entry(void)\r\n{\r\nstruct kvm_vcpu *v;\r\nv = current_vcpu;\r\nia64_call_vsa(PAL_VPS_RESTORE, (unsigned long)v->arch.vpd,\r\n0, 0, 0, 0, 0, 0);\r\nkvm_init_vtlb(v);\r\nkvm_init_vhpt(v);\r\ninit_vcpu(v);\r\nkvm_init_all_rr(v);\r\nvmm_reset_entry();\r\nreturn 0;\r\n}\r\nstatic void kvm_show_registers(struct kvm_pt_regs *regs)\r\n{\r\nunsigned long ip = regs->cr_iip + ia64_psr(regs)->ri;\r\nstruct kvm_vcpu *vcpu = current_vcpu;\r\nif (vcpu != NULL)\r\nprintk("vcpu 0x%p vcpu %d\n",\r\nvcpu, vcpu->vcpu_id);\r\nprintk("psr : %016lx ifs : %016lx ip : [<%016lx>]\n",\r\nregs->cr_ipsr, regs->cr_ifs, ip);\r\nprintk("unat: %016lx pfs : %016lx rsc : %016lx\n",\r\nregs->ar_unat, regs->ar_pfs, regs->ar_rsc);\r\nprintk("rnat: %016lx bspstore: %016lx pr : %016lx\n",\r\nregs->ar_rnat, regs->ar_bspstore, regs->pr);\r\nprintk("ldrs: %016lx ccv : %016lx fpsr: %016lx\n",\r\nregs->loadrs, regs->ar_ccv, regs->ar_fpsr);\r\nprintk("csd : %016lx ssd : %016lx\n", regs->ar_csd, regs->ar_ssd);\r\nprintk("b0 : %016lx b6 : %016lx b7 : %016lx\n", regs->b0,\r\nregs->b6, regs->b7);\r\nprintk("f6 : %05lx%016lx f7 : %05lx%016lx\n",\r\nregs->f6.u.bits[1], regs->f6.u.bits[0],\r\nregs->f7.u.bits[1], regs->f7.u.bits[0]);\r\nprintk("f8 : %05lx%016lx f9 : %05lx%016lx\n",\r\nregs->f8.u.bits[1], regs->f8.u.bits[0],\r\nregs->f9.u.bits[1], regs->f9.u.bits[0]);\r\nprintk("f10 : %05lx%016lx f11 : %05lx%016lx\n",\r\nregs->f10.u.bits[1], regs->f10.u.bits[0],\r\nregs->f11.u.bits[1], regs->f11.u.bits[0]);\r\nprintk("r1 : %016lx r2 : %016lx r3 : %016lx\n", regs->r1,\r\nregs->r2, regs->r3);\r\nprintk("r8 : %016lx r9 : %016lx r10 : %016lx\n", regs->r8,\r\nregs->r9, regs->r10);\r\nprintk("r11 : %016lx r12 : %016lx r13 : %016lx\n", regs->r11,\r\nregs->r12, regs->r13);\r\nprintk("r14 : %016lx r15 : %016lx r16 : %016lx\n", regs->r14,\r\nregs->r15, regs->r16);\r\nprintk("r17 : %016lx r18 : %016lx r19 : %016lx\n", regs->r17,\r\nregs->r18, regs->r19);\r\nprintk("r20 : %016lx r21 : %016lx r22 : %016lx\n", regs->r20,\r\nregs->r21, regs->r22);\r\nprintk("r23 : %016lx r24 : %016lx r25 : %016lx\n", regs->r23,\r\nregs->r24, regs->r25);\r\nprintk("r26 : %016lx r27 : %016lx r28 : %016lx\n", regs->r26,\r\nregs->r27, regs->r28);\r\nprintk("r29 : %016lx r30 : %016lx r31 : %016lx\n", regs->r29,\r\nregs->r30, regs->r31);\r\n}\r\nvoid panic_vm(struct kvm_vcpu *v, const char *fmt, ...)\r\n{\r\nva_list args;\r\nchar buf[256];\r\nstruct kvm_pt_regs *regs = vcpu_regs(v);\r\nstruct exit_ctl_data *p = &v->arch.exit_data;\r\nva_start(args, fmt);\r\nvsnprintf(buf, sizeof(buf), fmt, args);\r\nva_end(args);\r\nprintk(buf);\r\nkvm_show_registers(regs);\r\np->exit_reason = EXIT_REASON_VM_PANIC;\r\nvmm_transition(v);\r\nwhile (1);\r\n}
