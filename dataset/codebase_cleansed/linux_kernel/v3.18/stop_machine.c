static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)\r\n{\r\nmemset(done, 0, sizeof(*done));\r\natomic_set(&done->nr_todo, nr_todo);\r\ninit_completion(&done->completion);\r\n}\r\nstatic void cpu_stop_signal_done(struct cpu_stop_done *done, bool executed)\r\n{\r\nif (done) {\r\nif (executed)\r\ndone->executed = true;\r\nif (atomic_dec_and_test(&done->nr_todo))\r\ncomplete(&done->completion);\r\n}\r\n}\r\nstatic void cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstruct task_struct *p = per_cpu(cpu_stopper_task, cpu);\r\nunsigned long flags;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nif (stopper->enabled) {\r\nlist_add_tail(&work->list, &stopper->works);\r\nwake_up_process(p);\r\n} else\r\ncpu_stop_signal_done(work->done, false);\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\n}\r\nint stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\nstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done };\r\ncpu_stop_init_done(&done, 1);\r\ncpu_stop_queue_work(cpu, &work);\r\nwait_for_completion(&done.completion);\r\nreturn done.executed ? done.ret : -ENOENT;\r\n}\r\nstatic void set_state(struct multi_stop_data *msdata,\r\nenum multi_stop_state newstate)\r\n{\r\natomic_set(&msdata->thread_ack, msdata->num_threads);\r\nsmp_wmb();\r\nmsdata->state = newstate;\r\n}\r\nstatic void ack_state(struct multi_stop_data *msdata)\r\n{\r\nif (atomic_dec_and_test(&msdata->thread_ack))\r\nset_state(msdata, msdata->state + 1);\r\n}\r\nstatic int multi_cpu_stop(void *data)\r\n{\r\nstruct multi_stop_data *msdata = data;\r\nenum multi_stop_state curstate = MULTI_STOP_NONE;\r\nint cpu = smp_processor_id(), err = 0;\r\nunsigned long flags;\r\nbool is_active;\r\nlocal_save_flags(flags);\r\nif (!msdata->active_cpus)\r\nis_active = cpu == cpumask_first(cpu_online_mask);\r\nelse\r\nis_active = cpumask_test_cpu(cpu, msdata->active_cpus);\r\ndo {\r\ncpu_relax();\r\nif (msdata->state != curstate) {\r\ncurstate = msdata->state;\r\nswitch (curstate) {\r\ncase MULTI_STOP_DISABLE_IRQ:\r\nlocal_irq_disable();\r\nhard_irq_disable();\r\nbreak;\r\ncase MULTI_STOP_RUN:\r\nif (is_active)\r\nerr = msdata->fn(msdata->data);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nack_state(msdata);\r\n}\r\n} while (curstate != MULTI_STOP_EXIT);\r\nlocal_irq_restore(flags);\r\nreturn err;\r\n}\r\nstatic void irq_cpu_stop_queue_work(void *arg)\r\n{\r\nstruct irq_cpu_stop_queue_work_info *info = arg;\r\ncpu_stop_queue_work(info->cpu1, info->work1);\r\ncpu_stop_queue_work(info->cpu2, info->work2);\r\n}\r\nint stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\nstruct cpu_stop_work work1, work2;\r\nstruct irq_cpu_stop_queue_work_info call_args;\r\nstruct multi_stop_data msdata;\r\npreempt_disable();\r\nmsdata = (struct multi_stop_data){\r\n.fn = fn,\r\n.data = arg,\r\n.num_threads = 2,\r\n.active_cpus = cpumask_of(cpu1),\r\n};\r\nwork1 = work2 = (struct cpu_stop_work){\r\n.fn = multi_cpu_stop,\r\n.arg = &msdata,\r\n.done = &done\r\n};\r\ncall_args = (struct irq_cpu_stop_queue_work_info){\r\n.cpu1 = cpu1,\r\n.cpu2 = cpu2,\r\n.work1 = &work1,\r\n.work2 = &work2,\r\n};\r\ncpu_stop_init_done(&done, 2);\r\nset_state(&msdata, MULTI_STOP_PREPARE);\r\nif (!cpu_active(cpu1) || !cpu_active(cpu2)) {\r\npreempt_enable();\r\nreturn -ENOENT;\r\n}\r\nlg_local_lock(&stop_cpus_lock);\r\nsmp_call_function_single(min(cpu1, cpu2),\r\n&irq_cpu_stop_queue_work,\r\n&call_args, 1);\r\nlg_local_unlock(&stop_cpus_lock);\r\npreempt_enable();\r\nwait_for_completion(&done.completion);\r\nreturn done.executed ? done.ret : -ENOENT;\r\n}\r\nvoid stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\r\nstruct cpu_stop_work *work_buf)\r\n{\r\n*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };\r\ncpu_stop_queue_work(cpu, work_buf);\r\n}\r\nstatic void queue_stop_cpus_work(const struct cpumask *cpumask,\r\ncpu_stop_fn_t fn, void *arg,\r\nstruct cpu_stop_done *done)\r\n{\r\nstruct cpu_stop_work *work;\r\nunsigned int cpu;\r\nfor_each_cpu(cpu, cpumask) {\r\nwork = &per_cpu(stop_cpus_work, cpu);\r\nwork->fn = fn;\r\nwork->arg = arg;\r\nwork->done = done;\r\n}\r\nlg_global_lock(&stop_cpus_lock);\r\nfor_each_cpu(cpu, cpumask)\r\ncpu_stop_queue_work(cpu, &per_cpu(stop_cpus_work, cpu));\r\nlg_global_unlock(&stop_cpus_lock);\r\n}\r\nstatic int __stop_cpus(const struct cpumask *cpumask,\r\ncpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\ncpu_stop_init_done(&done, cpumask_weight(cpumask));\r\nqueue_stop_cpus_work(cpumask, fn, arg, &done);\r\nwait_for_completion(&done.completion);\r\nreturn done.executed ? done.ret : -ENOENT;\r\n}\r\nint stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\r\n{\r\nint ret;\r\nmutex_lock(&stop_cpus_mutex);\r\nret = __stop_cpus(cpumask, fn, arg);\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret;\r\n}\r\nint try_stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\r\n{\r\nint ret;\r\nif (!mutex_trylock(&stop_cpus_mutex))\r\nreturn -EAGAIN;\r\nret = __stop_cpus(cpumask, fn, arg);\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret;\r\n}\r\nstatic int cpu_stop_should_run(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nunsigned long flags;\r\nint run;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nrun = !list_empty(&stopper->works);\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\nreturn run;\r\n}\r\nstatic void cpu_stopper_thread(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstruct cpu_stop_work *work;\r\nint ret;\r\nrepeat:\r\nwork = NULL;\r\nspin_lock_irq(&stopper->lock);\r\nif (!list_empty(&stopper->works)) {\r\nwork = list_first_entry(&stopper->works,\r\nstruct cpu_stop_work, list);\r\nlist_del_init(&work->list);\r\n}\r\nspin_unlock_irq(&stopper->lock);\r\nif (work) {\r\ncpu_stop_fn_t fn = work->fn;\r\nvoid *arg = work->arg;\r\nstruct cpu_stop_done *done = work->done;\r\nchar ksym_buf[KSYM_NAME_LEN] __maybe_unused;\r\npreempt_disable();\r\nret = fn(arg);\r\nif (ret)\r\ndone->ret = ret;\r\npreempt_enable();\r\nWARN_ONCE(preempt_count(),\r\n"cpu_stop: %s(%p) leaked preempt count\n",\r\nkallsyms_lookup((unsigned long)fn, NULL, NULL, NULL,\r\nksym_buf), arg);\r\ncpu_stop_signal_done(done, true);\r\ngoto repeat;\r\n}\r\n}\r\nstatic void cpu_stop_create(unsigned int cpu)\r\n{\r\nsched_set_stop_task(cpu, per_cpu(cpu_stopper_task, cpu));\r\n}\r\nstatic void cpu_stop_park(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstruct cpu_stop_work *work;\r\nunsigned long flags;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nlist_for_each_entry(work, &stopper->works, list)\r\ncpu_stop_signal_done(work->done, false);\r\nstopper->enabled = false;\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\n}\r\nstatic void cpu_stop_unpark(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nspin_lock_irq(&stopper->lock);\r\nstopper->enabled = true;\r\nspin_unlock_irq(&stopper->lock);\r\n}\r\nstatic int __init cpu_stop_init(void)\r\n{\r\nunsigned int cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nspin_lock_init(&stopper->lock);\r\nINIT_LIST_HEAD(&stopper->works);\r\n}\r\nBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));\r\nstop_machine_initialized = true;\r\nreturn 0;\r\n}\r\nint __stop_machine(int (*fn)(void *), void *data, const struct cpumask *cpus)\r\n{\r\nstruct multi_stop_data msdata = {\r\n.fn = fn,\r\n.data = data,\r\n.num_threads = num_online_cpus(),\r\n.active_cpus = cpus,\r\n};\r\nif (!stop_machine_initialized) {\r\nunsigned long flags;\r\nint ret;\r\nWARN_ON_ONCE(msdata.num_threads != 1);\r\nlocal_irq_save(flags);\r\nhard_irq_disable();\r\nret = (*fn)(data);\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nset_state(&msdata, MULTI_STOP_PREPARE);\r\nreturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);\r\n}\r\nint stop_machine(int (*fn)(void *), void *data, const struct cpumask *cpus)\r\n{\r\nint ret;\r\nget_online_cpus();\r\nret = __stop_machine(fn, data, cpus);\r\nput_online_cpus();\r\nreturn ret;\r\n}\r\nint stop_machine_from_inactive_cpu(int (*fn)(void *), void *data,\r\nconst struct cpumask *cpus)\r\n{\r\nstruct multi_stop_data msdata = { .fn = fn, .data = data,\r\n.active_cpus = cpus };\r\nstruct cpu_stop_done done;\r\nint ret;\r\nBUG_ON(cpu_active(raw_smp_processor_id()));\r\nmsdata.num_threads = num_active_cpus() + 1;\r\nwhile (!mutex_trylock(&stop_cpus_mutex))\r\ncpu_relax();\r\nset_state(&msdata, MULTI_STOP_PREPARE);\r\ncpu_stop_init_done(&done, num_active_cpus());\r\nqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,\r\n&done);\r\nret = multi_cpu_stop(&msdata);\r\nwhile (!completion_done(&done.completion))\r\ncpu_relax();\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret ?: done.ret;\r\n}
