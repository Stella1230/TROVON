static int comp_tree_refs(struct btrfs_delayed_tree_ref *ref2,\r\nstruct btrfs_delayed_tree_ref *ref1, int type)\r\n{\r\nif (type == BTRFS_TREE_BLOCK_REF_KEY) {\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\n} else {\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int comp_data_refs(struct btrfs_delayed_data_ref *ref2,\r\nstruct btrfs_delayed_data_ref *ref1)\r\n{\r\nif (ref1->node.type == BTRFS_EXTENT_DATA_REF_KEY) {\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\nif (ref1->objectid < ref2->objectid)\r\nreturn -1;\r\nif (ref1->objectid > ref2->objectid)\r\nreturn 1;\r\nif (ref1->offset < ref2->offset)\r\nreturn -1;\r\nif (ref1->offset > ref2->offset)\r\nreturn 1;\r\n} else {\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int comp_entry(struct btrfs_delayed_ref_node *ref2,\r\nstruct btrfs_delayed_ref_node *ref1,\r\nbool compare_seq)\r\n{\r\nif (ref1->bytenr < ref2->bytenr)\r\nreturn -1;\r\nif (ref1->bytenr > ref2->bytenr)\r\nreturn 1;\r\nif (ref1->is_head && ref2->is_head)\r\nreturn 0;\r\nif (ref2->is_head)\r\nreturn -1;\r\nif (ref1->is_head)\r\nreturn 1;\r\nif (ref1->type < ref2->type)\r\nreturn -1;\r\nif (ref1->type > ref2->type)\r\nreturn 1;\r\nif (ref1->no_quota > ref2->no_quota)\r\nreturn 1;\r\nif (ref1->no_quota < ref2->no_quota)\r\nreturn -1;\r\nif (compare_seq) {\r\nif (ref1->seq < ref2->seq)\r\nreturn -1;\r\nif (ref1->seq > ref2->seq)\r\nreturn 1;\r\n}\r\nif (ref1->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nref1->type == BTRFS_SHARED_BLOCK_REF_KEY) {\r\nreturn comp_tree_refs(btrfs_delayed_node_to_tree_ref(ref2),\r\nbtrfs_delayed_node_to_tree_ref(ref1),\r\nref1->type);\r\n} else if (ref1->type == BTRFS_EXTENT_DATA_REF_KEY ||\r\nref1->type == BTRFS_SHARED_DATA_REF_KEY) {\r\nreturn comp_data_refs(btrfs_delayed_node_to_data_ref(ref2),\r\nbtrfs_delayed_node_to_data_ref(ref1));\r\n}\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic struct btrfs_delayed_ref_node *tree_insert(struct rb_root *root,\r\nstruct rb_node *node)\r\n{\r\nstruct rb_node **p = &root->rb_node;\r\nstruct rb_node *parent_node = NULL;\r\nstruct btrfs_delayed_ref_node *entry;\r\nstruct btrfs_delayed_ref_node *ins;\r\nint cmp;\r\nins = rb_entry(node, struct btrfs_delayed_ref_node, rb_node);\r\nwhile (*p) {\r\nparent_node = *p;\r\nentry = rb_entry(parent_node, struct btrfs_delayed_ref_node,\r\nrb_node);\r\ncmp = comp_entry(entry, ins, 1);\r\nif (cmp < 0)\r\np = &(*p)->rb_left;\r\nelse if (cmp > 0)\r\np = &(*p)->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nrb_link_node(node, parent_node, p);\r\nrb_insert_color(node, root);\r\nreturn NULL;\r\n}\r\nstatic struct btrfs_delayed_ref_head *htree_insert(struct rb_root *root,\r\nstruct rb_node *node)\r\n{\r\nstruct rb_node **p = &root->rb_node;\r\nstruct rb_node *parent_node = NULL;\r\nstruct btrfs_delayed_ref_head *entry;\r\nstruct btrfs_delayed_ref_head *ins;\r\nu64 bytenr;\r\nins = rb_entry(node, struct btrfs_delayed_ref_head, href_node);\r\nbytenr = ins->node.bytenr;\r\nwhile (*p) {\r\nparent_node = *p;\r\nentry = rb_entry(parent_node, struct btrfs_delayed_ref_head,\r\nhref_node);\r\nif (bytenr < entry->node.bytenr)\r\np = &(*p)->rb_left;\r\nelse if (bytenr > entry->node.bytenr)\r\np = &(*p)->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nrb_link_node(node, parent_node, p);\r\nrb_insert_color(node, root);\r\nreturn NULL;\r\n}\r\nstatic struct btrfs_delayed_ref_head *\r\nfind_ref_head(struct rb_root *root, u64 bytenr,\r\nint return_bigger)\r\n{\r\nstruct rb_node *n;\r\nstruct btrfs_delayed_ref_head *entry;\r\nn = root->rb_node;\r\nentry = NULL;\r\nwhile (n) {\r\nentry = rb_entry(n, struct btrfs_delayed_ref_head, href_node);\r\nif (bytenr < entry->node.bytenr)\r\nn = n->rb_left;\r\nelse if (bytenr > entry->node.bytenr)\r\nn = n->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nif (entry && return_bigger) {\r\nif (bytenr > entry->node.bytenr) {\r\nn = rb_next(&entry->href_node);\r\nif (!n)\r\nn = rb_first(root);\r\nentry = rb_entry(n, struct btrfs_delayed_ref_head,\r\nhref_node);\r\nreturn entry;\r\n}\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nint btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nassert_spin_locked(&delayed_refs->lock);\r\nif (mutex_trylock(&head->mutex))\r\nreturn 0;\r\natomic_inc(&head->node.refs);\r\nspin_unlock(&delayed_refs->lock);\r\nmutex_lock(&head->mutex);\r\nspin_lock(&delayed_refs->lock);\r\nif (!head->node.in_tree) {\r\nmutex_unlock(&head->mutex);\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn -EAGAIN;\r\n}\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn 0;\r\n}\r\nstatic inline void drop_delayed_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_head *head,\r\nstruct btrfs_delayed_ref_node *ref)\r\n{\r\nif (btrfs_delayed_ref_is_head(ref)) {\r\nhead = btrfs_delayed_node_to_head(ref);\r\nrb_erase(&head->href_node, &delayed_refs->href_root);\r\n} else {\r\nassert_spin_locked(&head->lock);\r\nrb_erase(&ref->rb_node, &head->ref_root);\r\n}\r\nref->in_tree = 0;\r\nbtrfs_put_delayed_ref(ref);\r\natomic_dec(&delayed_refs->num_entries);\r\nif (trans->delayed_ref_updates)\r\ntrans->delayed_ref_updates--;\r\n}\r\nstatic int merge_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_head *head,\r\nstruct btrfs_delayed_ref_node *ref, u64 seq)\r\n{\r\nstruct rb_node *node;\r\nint mod = 0;\r\nint done = 0;\r\nnode = rb_next(&ref->rb_node);\r\nwhile (!done && node) {\r\nstruct btrfs_delayed_ref_node *next;\r\nnext = rb_entry(node, struct btrfs_delayed_ref_node, rb_node);\r\nnode = rb_next(node);\r\nif (seq && next->seq >= seq)\r\nbreak;\r\nif (comp_entry(ref, next, 0))\r\ncontinue;\r\nif (ref->action == next->action) {\r\nmod = next->ref_mod;\r\n} else {\r\nif (ref->ref_mod < next->ref_mod) {\r\nstruct btrfs_delayed_ref_node *tmp;\r\ntmp = ref;\r\nref = next;\r\nnext = tmp;\r\ndone = 1;\r\n}\r\nmod = -next->ref_mod;\r\n}\r\ndrop_delayed_ref(trans, delayed_refs, head, next);\r\nref->ref_mod += mod;\r\nif (ref->ref_mod == 0) {\r\ndrop_delayed_ref(trans, delayed_refs, head, ref);\r\ndone = 1;\r\n} else {\r\nWARN_ON(ref->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nref->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\n}\r\n}\r\nreturn done;\r\n}\r\nvoid btrfs_merge_delayed_refs(struct btrfs_trans_handle *trans,\r\nstruct btrfs_fs_info *fs_info,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_head *head)\r\n{\r\nstruct rb_node *node;\r\nu64 seq = 0;\r\nassert_spin_locked(&head->lock);\r\nif (head->is_data)\r\nreturn;\r\nspin_lock(&fs_info->tree_mod_seq_lock);\r\nif (!list_empty(&fs_info->tree_mod_seq_list)) {\r\nstruct seq_list *elem;\r\nelem = list_first_entry(&fs_info->tree_mod_seq_list,\r\nstruct seq_list, list);\r\nseq = elem->seq;\r\n}\r\nspin_unlock(&fs_info->tree_mod_seq_lock);\r\nnode = rb_first(&head->ref_root);\r\nwhile (node) {\r\nstruct btrfs_delayed_ref_node *ref;\r\nref = rb_entry(node, struct btrfs_delayed_ref_node,\r\nrb_node);\r\nif (seq && ref->seq >= seq)\r\nbreak;\r\nif (merge_ref(trans, delayed_refs, head, ref, seq))\r\nnode = rb_first(&head->ref_root);\r\nelse\r\nnode = rb_next(&ref->rb_node);\r\n}\r\n}\r\nint btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nu64 seq)\r\n{\r\nstruct seq_list *elem;\r\nint ret = 0;\r\nspin_lock(&fs_info->tree_mod_seq_lock);\r\nif (!list_empty(&fs_info->tree_mod_seq_list)) {\r\nelem = list_first_entry(&fs_info->tree_mod_seq_list,\r\nstruct seq_list, list);\r\nif (seq >= elem->seq) {\r\npr_debug("holding back delayed_ref %#x.%x, lowest is %#x.%x (%p)\n",\r\n(u32)(seq >> 32), (u32)seq,\r\n(u32)(elem->seq >> 32), (u32)elem->seq,\r\ndelayed_refs);\r\nret = 1;\r\n}\r\n}\r\nspin_unlock(&fs_info->tree_mod_seq_lock);\r\nreturn ret;\r\n}\r\nstruct btrfs_delayed_ref_head *\r\nbtrfs_select_ref_head(struct btrfs_trans_handle *trans)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nstruct btrfs_delayed_ref_head *head;\r\nu64 start;\r\nbool loop = false;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nagain:\r\nstart = delayed_refs->run_delayed_start;\r\nhead = find_ref_head(&delayed_refs->href_root, start, 1);\r\nif (!head && !loop) {\r\ndelayed_refs->run_delayed_start = 0;\r\nstart = 0;\r\nloop = true;\r\nhead = find_ref_head(&delayed_refs->href_root, start, 1);\r\nif (!head)\r\nreturn NULL;\r\n} else if (!head && loop) {\r\nreturn NULL;\r\n}\r\nwhile (head->processing) {\r\nstruct rb_node *node;\r\nnode = rb_next(&head->href_node);\r\nif (!node) {\r\nif (loop)\r\nreturn NULL;\r\ndelayed_refs->run_delayed_start = 0;\r\nstart = 0;\r\nloop = true;\r\ngoto again;\r\n}\r\nhead = rb_entry(node, struct btrfs_delayed_ref_head,\r\nhref_node);\r\n}\r\nhead->processing = 1;\r\nWARN_ON(delayed_refs->num_heads_ready == 0);\r\ndelayed_refs->num_heads_ready--;\r\ndelayed_refs->run_delayed_start = head->node.bytenr +\r\nhead->node.num_bytes;\r\nreturn head;\r\n}\r\nstatic noinline void\r\nupdate_existing_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_head *head,\r\nstruct btrfs_delayed_ref_node *existing,\r\nstruct btrfs_delayed_ref_node *update)\r\n{\r\nif (update->action != existing->action) {\r\nexisting->ref_mod--;\r\nif (existing->ref_mod == 0)\r\ndrop_delayed_ref(trans, delayed_refs, head, existing);\r\nelse\r\nWARN_ON(existing->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nexisting->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\n} else {\r\nWARN_ON(existing->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nexisting->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\nexisting->ref_mod += update->ref_mod;\r\n}\r\n}\r\nstatic noinline void\r\nupdate_existing_head_ref(struct btrfs_delayed_ref_node *existing,\r\nstruct btrfs_delayed_ref_node *update)\r\n{\r\nstruct btrfs_delayed_ref_head *existing_ref;\r\nstruct btrfs_delayed_ref_head *ref;\r\nexisting_ref = btrfs_delayed_node_to_head(existing);\r\nref = btrfs_delayed_node_to_head(update);\r\nBUG_ON(existing_ref->is_data != ref->is_data);\r\nspin_lock(&existing_ref->lock);\r\nif (ref->must_insert_reserved) {\r\nexisting_ref->must_insert_reserved = ref->must_insert_reserved;\r\nexisting->num_bytes = update->num_bytes;\r\n}\r\nif (ref->extent_op) {\r\nif (!existing_ref->extent_op) {\r\nexisting_ref->extent_op = ref->extent_op;\r\n} else {\r\nif (ref->extent_op->update_key) {\r\nmemcpy(&existing_ref->extent_op->key,\r\n&ref->extent_op->key,\r\nsizeof(ref->extent_op->key));\r\nexisting_ref->extent_op->update_key = 1;\r\n}\r\nif (ref->extent_op->update_flags) {\r\nexisting_ref->extent_op->flags_to_set |=\r\nref->extent_op->flags_to_set;\r\nexisting_ref->extent_op->update_flags = 1;\r\n}\r\nbtrfs_free_delayed_extent_op(ref->extent_op);\r\n}\r\n}\r\nexisting->ref_mod += update->ref_mod;\r\nspin_unlock(&existing_ref->lock);\r\n}\r\nnoinline void\r\nadd_delayed_tree_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head_ref,\r\nstruct btrfs_delayed_ref_node *ref, u64 bytenr,\r\nu64 num_bytes, u64 parent, u64 ref_root, int level,\r\nint action, int no_quota)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_tree_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nu64 seq = 0;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\nif (is_fstree(ref_root))\r\nseq = atomic64_read(&fs_info->tree_mod_seq);\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nref->no_quota = no_quota;\r\nref->seq = seq;\r\nfull_ref = btrfs_delayed_node_to_tree_ref(ref);\r\nfull_ref->parent = parent;\r\nfull_ref->root = ref_root;\r\nif (parent)\r\nref->type = BTRFS_SHARED_BLOCK_REF_KEY;\r\nelse\r\nref->type = BTRFS_TREE_BLOCK_REF_KEY;\r\nfull_ref->level = level;\r\ntrace_add_delayed_tree_ref(ref, full_ref, action);\r\nspin_lock(&head_ref->lock);\r\nexisting = tree_insert(&head_ref->ref_root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_ref(trans, delayed_refs, head_ref, existing,\r\nref);\r\nkmem_cache_free(btrfs_delayed_tree_ref_cachep, full_ref);\r\n} else {\r\natomic_inc(&delayed_refs->num_entries);\r\ntrans->delayed_ref_updates++;\r\n}\r\nspin_unlock(&head_ref->lock);\r\n}\r\nstatic noinline void\r\nadd_delayed_data_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head_ref,\r\nstruct btrfs_delayed_ref_node *ref, u64 bytenr,\r\nu64 num_bytes, u64 parent, u64 ref_root, u64 owner,\r\nu64 offset, int action, int no_quota)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_data_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nu64 seq = 0;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nif (is_fstree(ref_root))\r\nseq = atomic64_read(&fs_info->tree_mod_seq);\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nref->no_quota = no_quota;\r\nref->seq = seq;\r\nfull_ref = btrfs_delayed_node_to_data_ref(ref);\r\nfull_ref->parent = parent;\r\nfull_ref->root = ref_root;\r\nif (parent)\r\nref->type = BTRFS_SHARED_DATA_REF_KEY;\r\nelse\r\nref->type = BTRFS_EXTENT_DATA_REF_KEY;\r\nfull_ref->objectid = owner;\r\nfull_ref->offset = offset;\r\ntrace_add_delayed_data_ref(ref, full_ref, action);\r\nspin_lock(&head_ref->lock);\r\nexisting = tree_insert(&head_ref->ref_root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_ref(trans, delayed_refs, head_ref, existing,\r\nref);\r\nkmem_cache_free(btrfs_delayed_data_ref_cachep, full_ref);\r\n} else {\r\natomic_inc(&delayed_refs->num_entries);\r\ntrans->delayed_ref_updates++;\r\n}\r\nspin_unlock(&head_ref->lock);\r\n}\r\nint btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, int level, int action,\r\nstruct btrfs_delayed_extent_op *extent_op,\r\nint no_quota)\r\n{\r\nstruct btrfs_delayed_tree_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nif (!is_fstree(ref_root) || !fs_info->quota_enabled)\r\nno_quota = 0;\r\nBUG_ON(extent_op && extent_op->is_data);\r\nref = kmem_cache_alloc(btrfs_delayed_tree_ref_cachep, GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\r\nif (!head_ref) {\r\nkmem_cache_free(btrfs_delayed_tree_ref_cachep, ref);\r\nreturn -ENOMEM;\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nhead_ref = add_delayed_ref_head(fs_info, trans, &head_ref->node,\r\nbytenr, num_bytes, action, 0);\r\nadd_delayed_tree_ref(fs_info, trans, head_ref, &ref->node, bytenr,\r\nnum_bytes, parent, ref_root, level, action,\r\nno_quota);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nu64 parent, u64 ref_root,\r\nu64 owner, u64 offset, int action,\r\nstruct btrfs_delayed_extent_op *extent_op,\r\nint no_quota)\r\n{\r\nstruct btrfs_delayed_data_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nif (!is_fstree(ref_root) || !fs_info->quota_enabled)\r\nno_quota = 0;\r\nBUG_ON(extent_op && !extent_op->is_data);\r\nref = kmem_cache_alloc(btrfs_delayed_data_ref_cachep, GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\r\nif (!head_ref) {\r\nkmem_cache_free(btrfs_delayed_data_ref_cachep, ref);\r\nreturn -ENOMEM;\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nhead_ref = add_delayed_ref_head(fs_info, trans, &head_ref->node,\r\nbytenr, num_bytes, action, 1);\r\nadd_delayed_data_ref(fs_info, trans, head_ref, &ref->node, bytenr,\r\nnum_bytes, parent, ref_root, owner, offset,\r\naction, no_quota);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nstruct btrfs_delayed_extent_op *extent_op)\r\n{\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nhead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\r\nif (!head_ref)\r\nreturn -ENOMEM;\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nadd_delayed_ref_head(fs_info, trans, &head_ref->node, bytenr,\r\nnum_bytes, BTRFS_UPDATE_DELAYED_HEAD,\r\nextent_op->is_data);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nstruct btrfs_delayed_ref_head *\r\nbtrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nreturn find_ref_head(&delayed_refs->href_root, bytenr, 0);\r\n}\r\nvoid btrfs_delayed_ref_exit(void)\r\n{\r\nif (btrfs_delayed_ref_head_cachep)\r\nkmem_cache_destroy(btrfs_delayed_ref_head_cachep);\r\nif (btrfs_delayed_tree_ref_cachep)\r\nkmem_cache_destroy(btrfs_delayed_tree_ref_cachep);\r\nif (btrfs_delayed_data_ref_cachep)\r\nkmem_cache_destroy(btrfs_delayed_data_ref_cachep);\r\nif (btrfs_delayed_extent_op_cachep)\r\nkmem_cache_destroy(btrfs_delayed_extent_op_cachep);\r\n}\r\nint btrfs_delayed_ref_init(void)\r\n{\r\nbtrfs_delayed_ref_head_cachep = kmem_cache_create(\r\n"btrfs_delayed_ref_head",\r\nsizeof(struct btrfs_delayed_ref_head), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_ref_head_cachep)\r\ngoto fail;\r\nbtrfs_delayed_tree_ref_cachep = kmem_cache_create(\r\n"btrfs_delayed_tree_ref",\r\nsizeof(struct btrfs_delayed_tree_ref), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_tree_ref_cachep)\r\ngoto fail;\r\nbtrfs_delayed_data_ref_cachep = kmem_cache_create(\r\n"btrfs_delayed_data_ref",\r\nsizeof(struct btrfs_delayed_data_ref), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_data_ref_cachep)\r\ngoto fail;\r\nbtrfs_delayed_extent_op_cachep = kmem_cache_create(\r\n"btrfs_delayed_extent_op",\r\nsizeof(struct btrfs_delayed_extent_op), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_extent_op_cachep)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nbtrfs_delayed_ref_exit();\r\nreturn -ENOMEM;\r\n}
