static struct k3_dma_chan *to_k3_chan(struct dma_chan *chan)\r\n{\r\nreturn container_of(chan, struct k3_dma_chan, vc.chan);\r\n}\r\nstatic void k3_dma_pause_dma(struct k3_dma_phy *phy, bool on)\r\n{\r\nu32 val = 0;\r\nif (on) {\r\nval = readl_relaxed(phy->base + CX_CFG);\r\nval |= CX_CFG_EN;\r\nwritel_relaxed(val, phy->base + CX_CFG);\r\n} else {\r\nval = readl_relaxed(phy->base + CX_CFG);\r\nval &= ~CX_CFG_EN;\r\nwritel_relaxed(val, phy->base + CX_CFG);\r\n}\r\n}\r\nstatic void k3_dma_terminate_chan(struct k3_dma_phy *phy, struct k3_dma_dev *d)\r\n{\r\nu32 val = 0;\r\nk3_dma_pause_dma(phy, false);\r\nval = 0x1 << phy->idx;\r\nwritel_relaxed(val, d->base + INT_TC1_RAW);\r\nwritel_relaxed(val, d->base + INT_ERR1_RAW);\r\nwritel_relaxed(val, d->base + INT_ERR2_RAW);\r\n}\r\nstatic void k3_dma_set_desc(struct k3_dma_phy *phy, struct k3_desc_hw *hw)\r\n{\r\nwritel_relaxed(hw->lli, phy->base + CX_LLI);\r\nwritel_relaxed(hw->count, phy->base + CX_CNT);\r\nwritel_relaxed(hw->saddr, phy->base + CX_SRC);\r\nwritel_relaxed(hw->daddr, phy->base + CX_DST);\r\nwritel_relaxed(AXI_CFG_DEFAULT, phy->base + AXI_CFG);\r\nwritel_relaxed(hw->config, phy->base + CX_CFG);\r\n}\r\nstatic u32 k3_dma_get_curr_cnt(struct k3_dma_dev *d, struct k3_dma_phy *phy)\r\n{\r\nu32 cnt = 0;\r\ncnt = readl_relaxed(d->base + CX_CUR_CNT + phy->idx * 0x10);\r\ncnt &= 0xffff;\r\nreturn cnt;\r\n}\r\nstatic u32 k3_dma_get_curr_lli(struct k3_dma_phy *phy)\r\n{\r\nreturn readl_relaxed(phy->base + CX_LLI);\r\n}\r\nstatic u32 k3_dma_get_chan_stat(struct k3_dma_dev *d)\r\n{\r\nreturn readl_relaxed(d->base + CH_STAT);\r\n}\r\nstatic void k3_dma_enable_dma(struct k3_dma_dev *d, bool on)\r\n{\r\nif (on) {\r\nwritel_relaxed(0x0, d->base + CH_PRI);\r\nwritel_relaxed(0xffff, d->base + INT_TC1_MASK);\r\nwritel_relaxed(0xffff, d->base + INT_ERR1_MASK);\r\nwritel_relaxed(0xffff, d->base + INT_ERR2_MASK);\r\n} else {\r\nwritel_relaxed(0x0, d->base + INT_TC1_MASK);\r\nwritel_relaxed(0x0, d->base + INT_ERR1_MASK);\r\nwritel_relaxed(0x0, d->base + INT_ERR2_MASK);\r\n}\r\n}\r\nstatic irqreturn_t k3_dma_int_handler(int irq, void *dev_id)\r\n{\r\nstruct k3_dma_dev *d = (struct k3_dma_dev *)dev_id;\r\nstruct k3_dma_phy *p;\r\nstruct k3_dma_chan *c;\r\nu32 stat = readl_relaxed(d->base + INT_STAT);\r\nu32 tc1 = readl_relaxed(d->base + INT_TC1);\r\nu32 err1 = readl_relaxed(d->base + INT_ERR1);\r\nu32 err2 = readl_relaxed(d->base + INT_ERR2);\r\nu32 i, irq_chan = 0;\r\nwhile (stat) {\r\ni = __ffs(stat);\r\nstat &= (stat - 1);\r\nif (likely(tc1 & BIT(i))) {\r\np = &d->phy[i];\r\nc = p->vchan;\r\nif (c) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nvchan_cookie_complete(&p->ds_run->vd);\r\np->ds_done = p->ds_run;\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nirq_chan |= BIT(i);\r\n}\r\nif (unlikely((err1 & BIT(i)) || (err2 & BIT(i))))\r\ndev_warn(d->slave.dev, "DMA ERR\n");\r\n}\r\nwritel_relaxed(irq_chan, d->base + INT_TC1_RAW);\r\nwritel_relaxed(err1, d->base + INT_ERR1_RAW);\r\nwritel_relaxed(err2, d->base + INT_ERR2_RAW);\r\nif (irq_chan) {\r\ntasklet_schedule(&d->task);\r\nreturn IRQ_HANDLED;\r\n} else\r\nreturn IRQ_NONE;\r\n}\r\nstatic int k3_dma_start_txd(struct k3_dma_chan *c)\r\n{\r\nstruct k3_dma_dev *d = to_k3_dma(c->vc.chan.device);\r\nstruct virt_dma_desc *vd = vchan_next_desc(&c->vc);\r\nif (!c->phy)\r\nreturn -EAGAIN;\r\nif (BIT(c->phy->idx) & k3_dma_get_chan_stat(d))\r\nreturn -EAGAIN;\r\nif (vd) {\r\nstruct k3_dma_desc_sw *ds =\r\ncontainer_of(vd, struct k3_dma_desc_sw, vd);\r\nlist_del(&ds->vd.node);\r\nc->phy->ds_run = ds;\r\nc->phy->ds_done = NULL;\r\nk3_dma_set_desc(c->phy, &ds->desc_hw[0]);\r\nreturn 0;\r\n}\r\nc->phy->ds_done = NULL;\r\nc->phy->ds_run = NULL;\r\nreturn -EAGAIN;\r\n}\r\nstatic void k3_dma_tasklet(unsigned long arg)\r\n{\r\nstruct k3_dma_dev *d = (struct k3_dma_dev *)arg;\r\nstruct k3_dma_phy *p;\r\nstruct k3_dma_chan *c, *cn;\r\nunsigned pch, pch_alloc = 0;\r\nlist_for_each_entry_safe(c, cn, &d->slave.channels, vc.chan.device_node) {\r\nspin_lock_irq(&c->vc.lock);\r\np = c->phy;\r\nif (p && p->ds_done) {\r\nif (k3_dma_start_txd(c)) {\r\ndev_dbg(d->slave.dev, "pchan %u: free\n", p->idx);\r\nc->phy = NULL;\r\np->vchan = NULL;\r\n}\r\n}\r\nspin_unlock_irq(&c->vc.lock);\r\n}\r\nspin_lock_irq(&d->lock);\r\nfor (pch = 0; pch < d->dma_channels; pch++) {\r\np = &d->phy[pch];\r\nif (p->vchan == NULL && !list_empty(&d->chan_pending)) {\r\nc = list_first_entry(&d->chan_pending,\r\nstruct k3_dma_chan, node);\r\nlist_del_init(&c->node);\r\npch_alloc |= 1 << pch;\r\np->vchan = c;\r\nc->phy = p;\r\ndev_dbg(d->slave.dev, "pchan %u: alloc vchan %p\n", pch, &c->vc);\r\n}\r\n}\r\nspin_unlock_irq(&d->lock);\r\nfor (pch = 0; pch < d->dma_channels; pch++) {\r\nif (pch_alloc & (1 << pch)) {\r\np = &d->phy[pch];\r\nc = p->vchan;\r\nif (c) {\r\nspin_lock_irq(&c->vc.lock);\r\nk3_dma_start_txd(c);\r\nspin_unlock_irq(&c->vc.lock);\r\n}\r\n}\r\n}\r\n}\r\nstatic int k3_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nreturn 0;\r\n}\r\nstatic void k3_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct k3_dma_chan *c = to_k3_chan(chan);\r\nstruct k3_dma_dev *d = to_k3_dma(chan->device);\r\nunsigned long flags;\r\nspin_lock_irqsave(&d->lock, flags);\r\nlist_del_init(&c->node);\r\nspin_unlock_irqrestore(&d->lock, flags);\r\nvchan_free_chan_resources(&c->vc);\r\nc->ccfg = 0;\r\n}\r\nstatic enum dma_status k3_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *state)\r\n{\r\nstruct k3_dma_chan *c = to_k3_chan(chan);\r\nstruct k3_dma_dev *d = to_k3_dma(chan->device);\r\nstruct k3_dma_phy *p;\r\nstruct virt_dma_desc *vd;\r\nunsigned long flags;\r\nenum dma_status ret;\r\nsize_t bytes = 0;\r\nret = dma_cookie_status(&c->vc.chan, cookie, state);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\np = c->phy;\r\nret = c->status;\r\nvd = vchan_find_desc(&c->vc, cookie);\r\nif (vd) {\r\nbytes = container_of(vd, struct k3_dma_desc_sw, vd)->size;\r\n} else if ((!p) || (!p->ds_run)) {\r\nbytes = 0;\r\n} else {\r\nstruct k3_dma_desc_sw *ds = p->ds_run;\r\nu32 clli = 0, index = 0;\r\nbytes = k3_dma_get_curr_cnt(d, p);\r\nclli = k3_dma_get_curr_lli(p);\r\nindex = (clli - ds->desc_hw_lli) / sizeof(struct k3_desc_hw);\r\nfor (; index < ds->desc_num; index++) {\r\nbytes += ds->desc_hw[index].count;\r\nif (!ds->desc_hw[index].lli)\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\ndma_set_residue(state, bytes);\r\nreturn ret;\r\n}\r\nstatic void k3_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct k3_dma_chan *c = to_k3_chan(chan);\r\nstruct k3_dma_dev *d = to_k3_dma(chan->device);\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (vchan_issue_pending(&c->vc)) {\r\nspin_lock(&d->lock);\r\nif (!c->phy) {\r\nif (list_empty(&c->node)) {\r\nlist_add_tail(&c->node, &d->chan_pending);\r\ntasklet_schedule(&d->task);\r\ndev_dbg(d->slave.dev, "vchan %p: issued\n", &c->vc);\r\n}\r\n}\r\nspin_unlock(&d->lock);\r\n} else\r\ndev_dbg(d->slave.dev, "vchan %p: nothing to issue\n", &c->vc);\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nstatic void k3_dma_fill_desc(struct k3_dma_desc_sw *ds, dma_addr_t dst,\r\ndma_addr_t src, size_t len, u32 num, u32 ccfg)\r\n{\r\nif ((num + 1) < ds->desc_num)\r\nds->desc_hw[num].lli = ds->desc_hw_lli + (num + 1) *\r\nsizeof(struct k3_desc_hw);\r\nds->desc_hw[num].lli |= CX_LLI_CHAIN_EN;\r\nds->desc_hw[num].count = len;\r\nds->desc_hw[num].saddr = src;\r\nds->desc_hw[num].daddr = dst;\r\nds->desc_hw[num].config = ccfg;\r\n}\r\nstatic struct dma_async_tx_descriptor *k3_dma_prep_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dst, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct k3_dma_chan *c = to_k3_chan(chan);\r\nstruct k3_dma_desc_sw *ds;\r\nsize_t copy = 0;\r\nint num = 0;\r\nif (!len)\r\nreturn NULL;\r\nnum = DIV_ROUND_UP(len, DMA_MAX_SIZE);\r\nds = kzalloc(sizeof(*ds) + num * sizeof(ds->desc_hw[0]), GFP_ATOMIC);\r\nif (!ds) {\r\ndev_dbg(chan->device->dev, "vchan %p: kzalloc fail\n", &c->vc);\r\nreturn NULL;\r\n}\r\nds->desc_hw_lli = __virt_to_phys((unsigned long)&ds->desc_hw[0]);\r\nds->size = len;\r\nds->desc_num = num;\r\nnum = 0;\r\nif (!c->ccfg) {\r\nc->ccfg = CX_CFG_SRCINCR | CX_CFG_DSTINCR | CX_CFG_EN;\r\nc->ccfg |= (0xf << 20) | (0xf << 24);\r\nc->ccfg |= (0x3 << 12) | (0x3 << 16);\r\n}\r\ndo {\r\ncopy = min_t(size_t, len, DMA_MAX_SIZE);\r\nk3_dma_fill_desc(ds, dst, src, copy, num++, c->ccfg);\r\nif (c->dir == DMA_MEM_TO_DEV) {\r\nsrc += copy;\r\n} else if (c->dir == DMA_DEV_TO_MEM) {\r\ndst += copy;\r\n} else {\r\nsrc += copy;\r\ndst += copy;\r\n}\r\nlen -= copy;\r\n} while (len);\r\nds->desc_hw[num-1].lli = 0;\r\nreturn vchan_tx_prep(&c->vc, &ds->vd, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *k3_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned int sglen,\r\nenum dma_transfer_direction dir, unsigned long flags, void *context)\r\n{\r\nstruct k3_dma_chan *c = to_k3_chan(chan);\r\nstruct k3_dma_desc_sw *ds;\r\nsize_t len, avail, total = 0;\r\nstruct scatterlist *sg;\r\ndma_addr_t addr, src = 0, dst = 0;\r\nint num = sglen, i;\r\nif (sgl == NULL)\r\nreturn NULL;\r\nfor_each_sg(sgl, sg, sglen, i) {\r\navail = sg_dma_len(sg);\r\nif (avail > DMA_MAX_SIZE)\r\nnum += DIV_ROUND_UP(avail, DMA_MAX_SIZE) - 1;\r\n}\r\nds = kzalloc(sizeof(*ds) + num * sizeof(ds->desc_hw[0]), GFP_ATOMIC);\r\nif (!ds) {\r\ndev_dbg(chan->device->dev, "vchan %p: kzalloc fail\n", &c->vc);\r\nreturn NULL;\r\n}\r\nds->desc_hw_lli = __virt_to_phys((unsigned long)&ds->desc_hw[0]);\r\nds->desc_num = num;\r\nnum = 0;\r\nfor_each_sg(sgl, sg, sglen, i) {\r\naddr = sg_dma_address(sg);\r\navail = sg_dma_len(sg);\r\ntotal += avail;\r\ndo {\r\nlen = min_t(size_t, avail, DMA_MAX_SIZE);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nsrc = addr;\r\ndst = c->dev_addr;\r\n} else if (dir == DMA_DEV_TO_MEM) {\r\nsrc = c->dev_addr;\r\ndst = addr;\r\n}\r\nk3_dma_fill_desc(ds, dst, src, len, num++, c->ccfg);\r\naddr += len;\r\navail -= len;\r\n} while (avail);\r\n}\r\nds->desc_hw[num-1].lli = 0;\r\nds->size = total;\r\nreturn vchan_tx_prep(&c->vc, &ds->vd, flags);\r\n}\r\nstatic int k3_dma_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct k3_dma_chan *c = to_k3_chan(chan);\r\nstruct k3_dma_dev *d = to_k3_dma(chan->device);\r\nstruct dma_slave_config *cfg = (void *)arg;\r\nstruct k3_dma_phy *p = c->phy;\r\nunsigned long flags;\r\nu32 maxburst = 0, val = 0;\r\nenum dma_slave_buswidth width = DMA_SLAVE_BUSWIDTH_UNDEFINED;\r\nLIST_HEAD(head);\r\nswitch (cmd) {\r\ncase DMA_SLAVE_CONFIG:\r\nif (cfg == NULL)\r\nreturn -EINVAL;\r\nc->dir = cfg->direction;\r\nif (c->dir == DMA_DEV_TO_MEM) {\r\nc->ccfg = CX_CFG_DSTINCR;\r\nc->dev_addr = cfg->src_addr;\r\nmaxburst = cfg->src_maxburst;\r\nwidth = cfg->src_addr_width;\r\n} else if (c->dir == DMA_MEM_TO_DEV) {\r\nc->ccfg = CX_CFG_SRCINCR;\r\nc->dev_addr = cfg->dst_addr;\r\nmaxburst = cfg->dst_maxburst;\r\nwidth = cfg->dst_addr_width;\r\n}\r\nswitch (width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\ncase DMA_SLAVE_BUSWIDTH_8_BYTES:\r\nval = __ffs(width);\r\nbreak;\r\ndefault:\r\nval = 3;\r\nbreak;\r\n}\r\nc->ccfg |= (val << 12) | (val << 16);\r\nif ((maxburst == 0) || (maxburst > 16))\r\nval = 16;\r\nelse\r\nval = maxburst - 1;\r\nc->ccfg |= (val << 20) | (val << 24);\r\nc->ccfg |= CX_CFG_MEM2PER | CX_CFG_EN;\r\nc->ccfg |= c->vc.chan.chan_id << 4;\r\nbreak;\r\ncase DMA_TERMINATE_ALL:\r\ndev_dbg(d->slave.dev, "vchan %p: terminate all\n", &c->vc);\r\nspin_lock(&d->lock);\r\nlist_del_init(&c->node);\r\nspin_unlock(&d->lock);\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nvchan_get_all_descriptors(&c->vc, &head);\r\nif (p) {\r\nk3_dma_terminate_chan(p, d);\r\nc->phy = NULL;\r\np->vchan = NULL;\r\np->ds_run = p->ds_done = NULL;\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nvchan_dma_desc_free_list(&c->vc, &head);\r\nbreak;\r\ncase DMA_PAUSE:\r\ndev_dbg(d->slave.dev, "vchan %p: pause\n", &c->vc);\r\nif (c->status == DMA_IN_PROGRESS) {\r\nc->status = DMA_PAUSED;\r\nif (p) {\r\nk3_dma_pause_dma(p, false);\r\n} else {\r\nspin_lock(&d->lock);\r\nlist_del_init(&c->node);\r\nspin_unlock(&d->lock);\r\n}\r\n}\r\nbreak;\r\ncase DMA_RESUME:\r\ndev_dbg(d->slave.dev, "vchan %p: resume\n", &c->vc);\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (c->status == DMA_PAUSED) {\r\nc->status = DMA_IN_PROGRESS;\r\nif (p) {\r\nk3_dma_pause_dma(p, true);\r\n} else if (!list_empty(&c->vc.desc_issued)) {\r\nspin_lock(&d->lock);\r\nlist_add_tail(&c->node, &d->chan_pending);\r\nspin_unlock(&d->lock);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nbreak;\r\ndefault:\r\nreturn -ENXIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic void k3_dma_free_desc(struct virt_dma_desc *vd)\r\n{\r\nstruct k3_dma_desc_sw *ds =\r\ncontainer_of(vd, struct k3_dma_desc_sw, vd);\r\nkfree(ds);\r\n}\r\nstatic struct dma_chan *k3_of_dma_simple_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct k3_dma_dev *d = ofdma->of_dma_data;\r\nunsigned int request = dma_spec->args[0];\r\nif (request > d->dma_requests)\r\nreturn NULL;\r\nreturn dma_get_slave_channel(&(d->chans[request].vc.chan));\r\n}\r\nstatic int k3_dma_probe(struct platform_device *op)\r\n{\r\nstruct k3_dma_dev *d;\r\nconst struct of_device_id *of_id;\r\nstruct resource *iores;\r\nint i, ret, irq = 0;\r\niores = platform_get_resource(op, IORESOURCE_MEM, 0);\r\nif (!iores)\r\nreturn -EINVAL;\r\nd = devm_kzalloc(&op->dev, sizeof(*d), GFP_KERNEL);\r\nif (!d)\r\nreturn -ENOMEM;\r\nd->base = devm_ioremap_resource(&op->dev, iores);\r\nif (IS_ERR(d->base))\r\nreturn PTR_ERR(d->base);\r\nof_id = of_match_device(k3_pdma_dt_ids, &op->dev);\r\nif (of_id) {\r\nof_property_read_u32((&op->dev)->of_node,\r\n"dma-channels", &d->dma_channels);\r\nof_property_read_u32((&op->dev)->of_node,\r\n"dma-requests", &d->dma_requests);\r\n}\r\nd->clk = devm_clk_get(&op->dev, NULL);\r\nif (IS_ERR(d->clk)) {\r\ndev_err(&op->dev, "no dma clk\n");\r\nreturn PTR_ERR(d->clk);\r\n}\r\nirq = platform_get_irq(op, 0);\r\nret = devm_request_irq(&op->dev, irq,\r\nk3_dma_int_handler, 0, DRIVER_NAME, d);\r\nif (ret)\r\nreturn ret;\r\nd->phy = devm_kzalloc(&op->dev,\r\nd->dma_channels * sizeof(struct k3_dma_phy), GFP_KERNEL);\r\nif (d->phy == NULL)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < d->dma_channels; i++) {\r\nstruct k3_dma_phy *p = &d->phy[i];\r\np->idx = i;\r\np->base = d->base + i * 0x40;\r\n}\r\nINIT_LIST_HEAD(&d->slave.channels);\r\ndma_cap_set(DMA_SLAVE, d->slave.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, d->slave.cap_mask);\r\nd->slave.dev = &op->dev;\r\nd->slave.device_alloc_chan_resources = k3_dma_alloc_chan_resources;\r\nd->slave.device_free_chan_resources = k3_dma_free_chan_resources;\r\nd->slave.device_tx_status = k3_dma_tx_status;\r\nd->slave.device_prep_dma_memcpy = k3_dma_prep_memcpy;\r\nd->slave.device_prep_slave_sg = k3_dma_prep_slave_sg;\r\nd->slave.device_issue_pending = k3_dma_issue_pending;\r\nd->slave.device_control = k3_dma_control;\r\nd->slave.copy_align = DMA_ALIGN;\r\nd->slave.chancnt = d->dma_requests;\r\nd->chans = devm_kzalloc(&op->dev,\r\nd->dma_requests * sizeof(struct k3_dma_chan), GFP_KERNEL);\r\nif (d->chans == NULL)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < d->dma_requests; i++) {\r\nstruct k3_dma_chan *c = &d->chans[i];\r\nc->status = DMA_IN_PROGRESS;\r\nINIT_LIST_HEAD(&c->node);\r\nc->vc.desc_free = k3_dma_free_desc;\r\nvchan_init(&c->vc, &d->slave);\r\n}\r\nret = clk_prepare_enable(d->clk);\r\nif (ret < 0) {\r\ndev_err(&op->dev, "clk_prepare_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nk3_dma_enable_dma(d, true);\r\nret = dma_async_device_register(&d->slave);\r\nif (ret)\r\nreturn ret;\r\nret = of_dma_controller_register((&op->dev)->of_node,\r\nk3_of_dma_simple_xlate, d);\r\nif (ret)\r\ngoto of_dma_register_fail;\r\nspin_lock_init(&d->lock);\r\nINIT_LIST_HEAD(&d->chan_pending);\r\ntasklet_init(&d->task, k3_dma_tasklet, (unsigned long)d);\r\nplatform_set_drvdata(op, d);\r\ndev_info(&op->dev, "initialized\n");\r\nreturn 0;\r\nof_dma_register_fail:\r\ndma_async_device_unregister(&d->slave);\r\nreturn ret;\r\n}\r\nstatic int k3_dma_remove(struct platform_device *op)\r\n{\r\nstruct k3_dma_chan *c, *cn;\r\nstruct k3_dma_dev *d = platform_get_drvdata(op);\r\ndma_async_device_unregister(&d->slave);\r\nof_dma_controller_free((&op->dev)->of_node);\r\nlist_for_each_entry_safe(c, cn, &d->slave.channels, vc.chan.device_node) {\r\nlist_del(&c->vc.chan.device_node);\r\ntasklet_kill(&c->vc.task);\r\n}\r\ntasklet_kill(&d->task);\r\nclk_disable_unprepare(d->clk);\r\nreturn 0;\r\n}\r\nstatic int k3_dma_suspend(struct device *dev)\r\n{\r\nstruct k3_dma_dev *d = dev_get_drvdata(dev);\r\nu32 stat = 0;\r\nstat = k3_dma_get_chan_stat(d);\r\nif (stat) {\r\ndev_warn(d->slave.dev,\r\n"chan %d is running fail to suspend\n", stat);\r\nreturn -1;\r\n}\r\nk3_dma_enable_dma(d, false);\r\nclk_disable_unprepare(d->clk);\r\nreturn 0;\r\n}\r\nstatic int k3_dma_resume(struct device *dev)\r\n{\r\nstruct k3_dma_dev *d = dev_get_drvdata(dev);\r\nint ret = 0;\r\nret = clk_prepare_enable(d->clk);\r\nif (ret < 0) {\r\ndev_err(d->slave.dev, "clk_prepare_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nk3_dma_enable_dma(d, true);\r\nreturn 0;\r\n}
