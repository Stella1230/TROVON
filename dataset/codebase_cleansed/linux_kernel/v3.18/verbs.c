static void\r\nrpcrdma_run_tasklet(unsigned long data)\r\n{\r\nstruct rpcrdma_rep *rep;\r\nvoid (*func)(struct rpcrdma_rep *);\r\nunsigned long flags;\r\ndata = data;\r\nspin_lock_irqsave(&rpcrdma_tk_lock_g, flags);\r\nwhile (!list_empty(&rpcrdma_tasklets_g)) {\r\nrep = list_entry(rpcrdma_tasklets_g.next,\r\nstruct rpcrdma_rep, rr_list);\r\nlist_del(&rep->rr_list);\r\nfunc = rep->rr_func;\r\nrep->rr_func = NULL;\r\nspin_unlock_irqrestore(&rpcrdma_tk_lock_g, flags);\r\nif (func)\r\nfunc(rep);\r\nelse\r\nrpcrdma_recv_buffer_put(rep);\r\nspin_lock_irqsave(&rpcrdma_tk_lock_g, flags);\r\n}\r\nspin_unlock_irqrestore(&rpcrdma_tk_lock_g, flags);\r\n}\r\nstatic void\r\nrpcrdma_qp_async_error_upcall(struct ib_event *event, void *context)\r\n{\r\nstruct rpcrdma_ep *ep = context;\r\ndprintk("RPC: %s: QP error %X on device %s ep %p\n",\r\n__func__, event->event, event->device->name, context);\r\nif (ep->rep_connected == 1) {\r\nep->rep_connected = -EIO;\r\nep->rep_func(ep);\r\nwake_up_all(&ep->rep_connect_wait);\r\n}\r\n}\r\nstatic void\r\nrpcrdma_cq_async_error_upcall(struct ib_event *event, void *context)\r\n{\r\nstruct rpcrdma_ep *ep = context;\r\ndprintk("RPC: %s: CQ error %X on device %s ep %p\n",\r\n__func__, event->event, event->device->name, context);\r\nif (ep->rep_connected == 1) {\r\nep->rep_connected = -EIO;\r\nep->rep_func(ep);\r\nwake_up_all(&ep->rep_connect_wait);\r\n}\r\n}\r\nstatic void\r\nrpcrdma_sendcq_process_wc(struct ib_wc *wc)\r\n{\r\nstruct rpcrdma_mw *frmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;\r\ndprintk("RPC: %s: frmr %p status %X opcode %d\n",\r\n__func__, frmr, wc->status, wc->opcode);\r\nif (wc->wr_id == 0ULL)\r\nreturn;\r\nif (wc->status != IB_WC_SUCCESS)\r\nfrmr->r.frmr.fr_state = FRMR_IS_STALE;\r\n}\r\nstatic int\r\nrpcrdma_sendcq_poll(struct ib_cq *cq, struct rpcrdma_ep *ep)\r\n{\r\nstruct ib_wc *wcs;\r\nint budget, count, rc;\r\nbudget = RPCRDMA_WC_BUDGET / RPCRDMA_POLLSIZE;\r\ndo {\r\nwcs = ep->rep_send_wcs;\r\nrc = ib_poll_cq(cq, RPCRDMA_POLLSIZE, wcs);\r\nif (rc <= 0)\r\nreturn rc;\r\ncount = rc;\r\nwhile (count-- > 0)\r\nrpcrdma_sendcq_process_wc(wcs++);\r\n} while (rc == RPCRDMA_POLLSIZE && --budget);\r\nreturn 0;\r\n}\r\nstatic void\r\nrpcrdma_sendcq_upcall(struct ib_cq *cq, void *cq_context)\r\n{\r\nstruct rpcrdma_ep *ep = (struct rpcrdma_ep *)cq_context;\r\nint rc;\r\nrc = rpcrdma_sendcq_poll(cq, ep);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_poll_cq failed: %i\n",\r\n__func__, rc);\r\nreturn;\r\n}\r\nrc = ib_req_notify_cq(cq,\r\nIB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS);\r\nif (rc == 0)\r\nreturn;\r\nif (rc < 0) {\r\ndprintk("RPC: %s: ib_req_notify_cq failed: %i\n",\r\n__func__, rc);\r\nreturn;\r\n}\r\nrpcrdma_sendcq_poll(cq, ep);\r\n}\r\nstatic void\r\nrpcrdma_recvcq_process_wc(struct ib_wc *wc, struct list_head *sched_list)\r\n{\r\nstruct rpcrdma_rep *rep =\r\n(struct rpcrdma_rep *)(unsigned long)wc->wr_id;\r\ndprintk("RPC: %s: rep %p status %X opcode %X length %u\n",\r\n__func__, rep, wc->status, wc->opcode, wc->byte_len);\r\nif (wc->status != IB_WC_SUCCESS) {\r\nrep->rr_len = ~0U;\r\ngoto out_schedule;\r\n}\r\nif (wc->opcode != IB_WC_RECV)\r\nreturn;\r\nrep->rr_len = wc->byte_len;\r\nib_dma_sync_single_for_cpu(rdmab_to_ia(rep->rr_buffer)->ri_id->device,\r\nrep->rr_iov.addr, rep->rr_len, DMA_FROM_DEVICE);\r\nif (rep->rr_len >= 16) {\r\nstruct rpcrdma_msg *p = (struct rpcrdma_msg *)rep->rr_base;\r\nunsigned int credits = ntohl(p->rm_credit);\r\nif (credits == 0)\r\ncredits = 1;\r\nelse if (credits > rep->rr_buffer->rb_max_requests)\r\ncredits = rep->rr_buffer->rb_max_requests;\r\natomic_set(&rep->rr_buffer->rb_credits, credits);\r\n}\r\nout_schedule:\r\nlist_add_tail(&rep->rr_list, sched_list);\r\n}\r\nstatic int\r\nrpcrdma_recvcq_poll(struct ib_cq *cq, struct rpcrdma_ep *ep)\r\n{\r\nstruct list_head sched_list;\r\nstruct ib_wc *wcs;\r\nint budget, count, rc;\r\nunsigned long flags;\r\nINIT_LIST_HEAD(&sched_list);\r\nbudget = RPCRDMA_WC_BUDGET / RPCRDMA_POLLSIZE;\r\ndo {\r\nwcs = ep->rep_recv_wcs;\r\nrc = ib_poll_cq(cq, RPCRDMA_POLLSIZE, wcs);\r\nif (rc <= 0)\r\ngoto out_schedule;\r\ncount = rc;\r\nwhile (count-- > 0)\r\nrpcrdma_recvcq_process_wc(wcs++, &sched_list);\r\n} while (rc == RPCRDMA_POLLSIZE && --budget);\r\nrc = 0;\r\nout_schedule:\r\nspin_lock_irqsave(&rpcrdma_tk_lock_g, flags);\r\nlist_splice_tail(&sched_list, &rpcrdma_tasklets_g);\r\nspin_unlock_irqrestore(&rpcrdma_tk_lock_g, flags);\r\ntasklet_schedule(&rpcrdma_tasklet_g);\r\nreturn rc;\r\n}\r\nstatic void\r\nrpcrdma_recvcq_upcall(struct ib_cq *cq, void *cq_context)\r\n{\r\nstruct rpcrdma_ep *ep = (struct rpcrdma_ep *)cq_context;\r\nint rc;\r\nrc = rpcrdma_recvcq_poll(cq, ep);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_poll_cq failed: %i\n",\r\n__func__, rc);\r\nreturn;\r\n}\r\nrc = ib_req_notify_cq(cq,\r\nIB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS);\r\nif (rc == 0)\r\nreturn;\r\nif (rc < 0) {\r\ndprintk("RPC: %s: ib_req_notify_cq failed: %i\n",\r\n__func__, rc);\r\nreturn;\r\n}\r\nrpcrdma_recvcq_poll(cq, ep);\r\n}\r\nstatic void\r\nrpcrdma_flush_cqs(struct rpcrdma_ep *ep)\r\n{\r\nrpcrdma_recvcq_upcall(ep->rep_attr.recv_cq, ep);\r\nrpcrdma_sendcq_upcall(ep->rep_attr.send_cq, ep);\r\n}\r\nstatic int\r\nrpcrdma_conn_upcall(struct rdma_cm_id *id, struct rdma_cm_event *event)\r\n{\r\nstruct rpcrdma_xprt *xprt = id->context;\r\nstruct rpcrdma_ia *ia = &xprt->rx_ia;\r\nstruct rpcrdma_ep *ep = &xprt->rx_ep;\r\n#ifdef RPC_DEBUG\r\nstruct sockaddr_in *addr = (struct sockaddr_in *) &ep->rep_remote_addr;\r\n#endif\r\nstruct ib_qp_attr attr;\r\nstruct ib_qp_init_attr iattr;\r\nint connstate = 0;\r\nswitch (event->event) {\r\ncase RDMA_CM_EVENT_ADDR_RESOLVED:\r\ncase RDMA_CM_EVENT_ROUTE_RESOLVED:\r\nia->ri_async_rc = 0;\r\ncomplete(&ia->ri_done);\r\nbreak;\r\ncase RDMA_CM_EVENT_ADDR_ERROR:\r\nia->ri_async_rc = -EHOSTUNREACH;\r\ndprintk("RPC: %s: CM address resolution error, ep 0x%p\n",\r\n__func__, ep);\r\ncomplete(&ia->ri_done);\r\nbreak;\r\ncase RDMA_CM_EVENT_ROUTE_ERROR:\r\nia->ri_async_rc = -ENETUNREACH;\r\ndprintk("RPC: %s: CM route resolution error, ep 0x%p\n",\r\n__func__, ep);\r\ncomplete(&ia->ri_done);\r\nbreak;\r\ncase RDMA_CM_EVENT_ESTABLISHED:\r\nconnstate = 1;\r\nib_query_qp(ia->ri_id->qp, &attr,\r\nIB_QP_MAX_QP_RD_ATOMIC | IB_QP_MAX_DEST_RD_ATOMIC,\r\n&iattr);\r\ndprintk("RPC: %s: %d responder resources"\r\n" (%d initiator)\n",\r\n__func__, attr.max_dest_rd_atomic, attr.max_rd_atomic);\r\ngoto connected;\r\ncase RDMA_CM_EVENT_CONNECT_ERROR:\r\nconnstate = -ENOTCONN;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_UNREACHABLE:\r\nconnstate = -ENETDOWN;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_REJECTED:\r\nconnstate = -ECONNREFUSED;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_DISCONNECTED:\r\nconnstate = -ECONNABORTED;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_DEVICE_REMOVAL:\r\nconnstate = -ENODEV;\r\nconnected:\r\natomic_set(&rpcx_to_rdmax(ep->rep_xprt)->rx_buf.rb_credits, 1);\r\ndprintk("RPC: %s: %sconnected\n",\r\n__func__, connstate > 0 ? "" : "dis");\r\nep->rep_connected = connstate;\r\nep->rep_func(ep);\r\nwake_up_all(&ep->rep_connect_wait);\r\ndefault:\r\ndprintk("RPC: %s: %pI4:%u (ep 0x%p): %s\n",\r\n__func__, &addr->sin_addr.s_addr,\r\nntohs(addr->sin_port), ep,\r\nCONNECTION_MSG(event->event));\r\nbreak;\r\n}\r\n#ifdef RPC_DEBUG\r\nif (connstate == 1) {\r\nint ird = attr.max_dest_rd_atomic;\r\nint tird = ep->rep_remote_cma.responder_resources;\r\nprintk(KERN_INFO "rpcrdma: connection to %pI4:%u "\r\n"on %s, memreg %d slots %d ird %d%s\n",\r\n&addr->sin_addr.s_addr,\r\nntohs(addr->sin_port),\r\nia->ri_id->device->name,\r\nia->ri_memreg_strategy,\r\nxprt->rx_buf.rb_max_requests,\r\nird, ird < 4 && ird < tird / 2 ? " (low!)" : "");\r\n} else if (connstate < 0) {\r\nprintk(KERN_INFO "rpcrdma: connection to %pI4:%u closed (%d)\n",\r\n&addr->sin_addr.s_addr,\r\nntohs(addr->sin_port),\r\nconnstate);\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic struct rdma_cm_id *\r\nrpcrdma_create_id(struct rpcrdma_xprt *xprt,\r\nstruct rpcrdma_ia *ia, struct sockaddr *addr)\r\n{\r\nstruct rdma_cm_id *id;\r\nint rc;\r\ninit_completion(&ia->ri_done);\r\nid = rdma_create_id(rpcrdma_conn_upcall, xprt, RDMA_PS_TCP, IB_QPT_RC);\r\nif (IS_ERR(id)) {\r\nrc = PTR_ERR(id);\r\ndprintk("RPC: %s: rdma_create_id() failed %i\n",\r\n__func__, rc);\r\nreturn id;\r\n}\r\nia->ri_async_rc = -ETIMEDOUT;\r\nrc = rdma_resolve_addr(id, NULL, addr, RDMA_RESOLVE_TIMEOUT);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_resolve_addr() failed %i\n",\r\n__func__, rc);\r\ngoto out;\r\n}\r\nwait_for_completion_interruptible_timeout(&ia->ri_done,\r\nmsecs_to_jiffies(RDMA_RESOLVE_TIMEOUT) + 1);\r\nrc = ia->ri_async_rc;\r\nif (rc)\r\ngoto out;\r\nia->ri_async_rc = -ETIMEDOUT;\r\nrc = rdma_resolve_route(id, RDMA_RESOLVE_TIMEOUT);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_resolve_route() failed %i\n",\r\n__func__, rc);\r\ngoto out;\r\n}\r\nwait_for_completion_interruptible_timeout(&ia->ri_done,\r\nmsecs_to_jiffies(RDMA_RESOLVE_TIMEOUT) + 1);\r\nrc = ia->ri_async_rc;\r\nif (rc)\r\ngoto out;\r\nreturn id;\r\nout:\r\nrdma_destroy_id(id);\r\nreturn ERR_PTR(rc);\r\n}\r\nstatic void\r\nrpcrdma_clean_cq(struct ib_cq *cq)\r\n{\r\nstruct ib_wc wc;\r\nint count = 0;\r\nwhile (1 == ib_poll_cq(cq, 1, &wc))\r\n++count;\r\nif (count)\r\ndprintk("RPC: %s: flushed %d events (last 0x%x)\n",\r\n__func__, count, wc.opcode);\r\n}\r\nint\r\nrpcrdma_ia_open(struct rpcrdma_xprt *xprt, struct sockaddr *addr, int memreg)\r\n{\r\nint rc, mem_priv;\r\nstruct ib_device_attr devattr;\r\nstruct rpcrdma_ia *ia = &xprt->rx_ia;\r\nia->ri_id = rpcrdma_create_id(xprt, ia, addr);\r\nif (IS_ERR(ia->ri_id)) {\r\nrc = PTR_ERR(ia->ri_id);\r\ngoto out1;\r\n}\r\nia->ri_pd = ib_alloc_pd(ia->ri_id->device);\r\nif (IS_ERR(ia->ri_pd)) {\r\nrc = PTR_ERR(ia->ri_pd);\r\ndprintk("RPC: %s: ib_alloc_pd() failed %i\n",\r\n__func__, rc);\r\ngoto out2;\r\n}\r\nrc = ib_query_device(ia->ri_id->device, &devattr);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_query_device failed %d\n",\r\n__func__, rc);\r\ngoto out2;\r\n}\r\nif (devattr.device_cap_flags & IB_DEVICE_LOCAL_DMA_LKEY) {\r\nia->ri_have_dma_lkey = 1;\r\nia->ri_dma_lkey = ia->ri_id->device->local_dma_lkey;\r\n}\r\nif (memreg == RPCRDMA_FRMR) {\r\nif ((devattr.device_cap_flags &\r\n(IB_DEVICE_MEM_MGT_EXTENSIONS|IB_DEVICE_LOCAL_DMA_LKEY)) !=\r\n(IB_DEVICE_MEM_MGT_EXTENSIONS|IB_DEVICE_LOCAL_DMA_LKEY)) {\r\ndprintk("RPC: %s: FRMR registration "\r\n"not supported by HCA\n", __func__);\r\nmemreg = RPCRDMA_MTHCAFMR;\r\n} else {\r\nia->ri_max_frmr_depth = min_t(unsigned int,\r\nRPCRDMA_MAX_DATA_SEGS,\r\ndevattr.max_fast_reg_page_list_len);\r\n}\r\n}\r\nif (memreg == RPCRDMA_MTHCAFMR) {\r\nif (!ia->ri_id->device->alloc_fmr) {\r\ndprintk("RPC: %s: MTHCAFMR registration "\r\n"not supported by HCA\n", __func__);\r\nmemreg = RPCRDMA_ALLPHYSICAL;\r\n}\r\n}\r\nswitch (memreg) {\r\ncase RPCRDMA_FRMR:\r\nbreak;\r\ncase RPCRDMA_ALLPHYSICAL:\r\nmem_priv = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_READ;\r\ngoto register_setup;\r\ncase RPCRDMA_MTHCAFMR:\r\nif (ia->ri_have_dma_lkey)\r\nbreak;\r\nmem_priv = IB_ACCESS_LOCAL_WRITE;\r\nregister_setup:\r\nia->ri_bind_mem = ib_get_dma_mr(ia->ri_pd, mem_priv);\r\nif (IS_ERR(ia->ri_bind_mem)) {\r\nprintk(KERN_ALERT "%s: ib_get_dma_mr for "\r\n"phys register failed with %lX\n",\r\n__func__, PTR_ERR(ia->ri_bind_mem));\r\nrc = -ENOMEM;\r\ngoto out2;\r\n}\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "RPC: Unsupported memory "\r\n"registration mode: %d\n", memreg);\r\nrc = -ENOMEM;\r\ngoto out2;\r\n}\r\ndprintk("RPC: %s: memory registration strategy is %d\n",\r\n__func__, memreg);\r\nia->ri_memreg_strategy = memreg;\r\nrwlock_init(&ia->ri_qplock);\r\nreturn 0;\r\nout2:\r\nrdma_destroy_id(ia->ri_id);\r\nia->ri_id = NULL;\r\nout1:\r\nreturn rc;\r\n}\r\nvoid\r\nrpcrdma_ia_close(struct rpcrdma_ia *ia)\r\n{\r\nint rc;\r\ndprintk("RPC: %s: entering\n", __func__);\r\nif (ia->ri_bind_mem != NULL) {\r\nrc = ib_dereg_mr(ia->ri_bind_mem);\r\ndprintk("RPC: %s: ib_dereg_mr returned %i\n",\r\n__func__, rc);\r\n}\r\nif (ia->ri_id != NULL && !IS_ERR(ia->ri_id)) {\r\nif (ia->ri_id->qp)\r\nrdma_destroy_qp(ia->ri_id);\r\nrdma_destroy_id(ia->ri_id);\r\nia->ri_id = NULL;\r\n}\r\nif (ia->ri_pd != NULL && !IS_ERR(ia->ri_pd)) {\r\nrc = ib_dealloc_pd(ia->ri_pd);\r\ndprintk("RPC: %s: ib_dealloc_pd returned %i\n",\r\n__func__, rc);\r\n}\r\n}\r\nint\r\nrpcrdma_ep_create(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia,\r\nstruct rpcrdma_create_data_internal *cdata)\r\n{\r\nstruct ib_device_attr devattr;\r\nstruct ib_cq *sendcq, *recvcq;\r\nint rc, err;\r\nrc = ib_query_device(ia->ri_id->device, &devattr);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_query_device failed %d\n",\r\n__func__, rc);\r\nreturn rc;\r\n}\r\nif (cdata->max_requests > devattr.max_qp_wr)\r\ncdata->max_requests = devattr.max_qp_wr;\r\nep->rep_attr.event_handler = rpcrdma_qp_async_error_upcall;\r\nep->rep_attr.qp_context = ep;\r\nep->rep_attr.srq = NULL;\r\nep->rep_attr.cap.max_send_wr = cdata->max_requests;\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR: {\r\nint depth = 7;\r\nif (ia->ri_max_frmr_depth < RPCRDMA_MAX_DATA_SEGS) {\r\nint delta = RPCRDMA_MAX_DATA_SEGS -\r\nia->ri_max_frmr_depth;\r\ndo {\r\ndepth += 2;\r\ndelta -= ia->ri_max_frmr_depth;\r\n} while (delta > 0);\r\n}\r\nep->rep_attr.cap.max_send_wr *= depth;\r\nif (ep->rep_attr.cap.max_send_wr > devattr.max_qp_wr) {\r\ncdata->max_requests = devattr.max_qp_wr / depth;\r\nif (!cdata->max_requests)\r\nreturn -EINVAL;\r\nep->rep_attr.cap.max_send_wr = cdata->max_requests *\r\ndepth;\r\n}\r\nbreak;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\nep->rep_attr.cap.max_recv_wr = cdata->max_requests;\r\nep->rep_attr.cap.max_send_sge = (cdata->padding ? 4 : 2);\r\nep->rep_attr.cap.max_recv_sge = 1;\r\nep->rep_attr.cap.max_inline_data = 0;\r\nep->rep_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\r\nep->rep_attr.qp_type = IB_QPT_RC;\r\nep->rep_attr.port_num = ~0;\r\ndprintk("RPC: %s: requested max: dtos: send %d recv %d; "\r\n"iovs: send %d recv %d\n",\r\n__func__,\r\nep->rep_attr.cap.max_send_wr,\r\nep->rep_attr.cap.max_recv_wr,\r\nep->rep_attr.cap.max_send_sge,\r\nep->rep_attr.cap.max_recv_sge);\r\nep->rep_cqinit = ep->rep_attr.cap.max_send_wr/2 - 1;\r\nif (ep->rep_cqinit <= 2)\r\nep->rep_cqinit = 0;\r\nINIT_CQCOUNT(ep);\r\nep->rep_ia = ia;\r\ninit_waitqueue_head(&ep->rep_connect_wait);\r\nINIT_DELAYED_WORK(&ep->rep_connect_worker, rpcrdma_connect_worker);\r\nsendcq = ib_create_cq(ia->ri_id->device, rpcrdma_sendcq_upcall,\r\nrpcrdma_cq_async_error_upcall, ep,\r\nep->rep_attr.cap.max_send_wr + 1, 0);\r\nif (IS_ERR(sendcq)) {\r\nrc = PTR_ERR(sendcq);\r\ndprintk("RPC: %s: failed to create send CQ: %i\n",\r\n__func__, rc);\r\ngoto out1;\r\n}\r\nrc = ib_req_notify_cq(sendcq, IB_CQ_NEXT_COMP);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_req_notify_cq failed: %i\n",\r\n__func__, rc);\r\ngoto out2;\r\n}\r\nrecvcq = ib_create_cq(ia->ri_id->device, rpcrdma_recvcq_upcall,\r\nrpcrdma_cq_async_error_upcall, ep,\r\nep->rep_attr.cap.max_recv_wr + 1, 0);\r\nif (IS_ERR(recvcq)) {\r\nrc = PTR_ERR(recvcq);\r\ndprintk("RPC: %s: failed to create recv CQ: %i\n",\r\n__func__, rc);\r\ngoto out2;\r\n}\r\nrc = ib_req_notify_cq(recvcq, IB_CQ_NEXT_COMP);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_req_notify_cq failed: %i\n",\r\n__func__, rc);\r\nib_destroy_cq(recvcq);\r\ngoto out2;\r\n}\r\nep->rep_attr.send_cq = sendcq;\r\nep->rep_attr.recv_cq = recvcq;\r\nep->rep_remote_cma.private_data = NULL;\r\nep->rep_remote_cma.private_data_len = 0;\r\nep->rep_remote_cma.initiator_depth = 0;\r\nif (devattr.max_qp_rd_atom > 32)\r\nep->rep_remote_cma.responder_resources = 32;\r\nelse\r\nep->rep_remote_cma.responder_resources = devattr.max_qp_rd_atom;\r\nep->rep_remote_cma.retry_count = 7;\r\nep->rep_remote_cma.flow_control = 0;\r\nep->rep_remote_cma.rnr_retry_count = 0;\r\nreturn 0;\r\nout2:\r\nerr = ib_destroy_cq(sendcq);\r\nif (err)\r\ndprintk("RPC: %s: ib_destroy_cq returned %i\n",\r\n__func__, err);\r\nout1:\r\nreturn rc;\r\n}\r\nvoid\r\nrpcrdma_ep_destroy(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia)\r\n{\r\nint rc;\r\ndprintk("RPC: %s: entering, connected is %d\n",\r\n__func__, ep->rep_connected);\r\ncancel_delayed_work_sync(&ep->rep_connect_worker);\r\nif (ia->ri_id->qp) {\r\nrpcrdma_ep_disconnect(ep, ia);\r\nrdma_destroy_qp(ia->ri_id);\r\nia->ri_id->qp = NULL;\r\n}\r\nif (ep->rep_pad_mr) {\r\nrpcrdma_deregister_internal(ia, ep->rep_pad_mr, &ep->rep_pad);\r\nep->rep_pad_mr = NULL;\r\n}\r\nrpcrdma_clean_cq(ep->rep_attr.recv_cq);\r\nrc = ib_destroy_cq(ep->rep_attr.recv_cq);\r\nif (rc)\r\ndprintk("RPC: %s: ib_destroy_cq returned %i\n",\r\n__func__, rc);\r\nrpcrdma_clean_cq(ep->rep_attr.send_cq);\r\nrc = ib_destroy_cq(ep->rep_attr.send_cq);\r\nif (rc)\r\ndprintk("RPC: %s: ib_destroy_cq returned %i\n",\r\n__func__, rc);\r\n}\r\nint\r\nrpcrdma_ep_connect(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia)\r\n{\r\nstruct rdma_cm_id *id, *old;\r\nint rc = 0;\r\nint retry_count = 0;\r\nif (ep->rep_connected != 0) {\r\nstruct rpcrdma_xprt *xprt;\r\nretry:\r\ndprintk("RPC: %s: reconnecting...\n", __func__);\r\nrpcrdma_ep_disconnect(ep, ia);\r\nrpcrdma_flush_cqs(ep);\r\nif (ia->ri_memreg_strategy == RPCRDMA_FRMR)\r\nrpcrdma_reset_frmrs(ia);\r\nxprt = container_of(ia, struct rpcrdma_xprt, rx_ia);\r\nid = rpcrdma_create_id(xprt, ia,\r\n(struct sockaddr *)&xprt->rx_data.addr);\r\nif (IS_ERR(id)) {\r\nrc = -EHOSTUNREACH;\r\ngoto out;\r\n}\r\nif (ia->ri_id->device != id->device) {\r\nprintk("RPC: %s: can't reconnect on "\r\n"different device!\n", __func__);\r\nrdma_destroy_id(id);\r\nrc = -ENETUNREACH;\r\ngoto out;\r\n}\r\nrc = rdma_create_qp(id, ia->ri_pd, &ep->rep_attr);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_create_qp failed %i\n",\r\n__func__, rc);\r\nrdma_destroy_id(id);\r\nrc = -ENETUNREACH;\r\ngoto out;\r\n}\r\nwrite_lock(&ia->ri_qplock);\r\nold = ia->ri_id;\r\nia->ri_id = id;\r\nwrite_unlock(&ia->ri_qplock);\r\nrdma_destroy_qp(old);\r\nrdma_destroy_id(old);\r\n} else {\r\ndprintk("RPC: %s: connecting...\n", __func__);\r\nrc = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_create_qp failed %i\n",\r\n__func__, rc);\r\nreturn -ENETUNREACH;\r\n}\r\n}\r\nep->rep_connected = 0;\r\nrc = rdma_connect(ia->ri_id, &ep->rep_remote_cma);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_connect() failed with %i\n",\r\n__func__, rc);\r\ngoto out;\r\n}\r\nwait_event_interruptible(ep->rep_connect_wait, ep->rep_connected != 0);\r\nif (ep->rep_connected == -ECONNREFUSED &&\r\n++retry_count <= RDMA_CONNECT_RETRY_MAX) {\r\ndprintk("RPC: %s: non-peer_reject, retry\n", __func__);\r\ngoto retry;\r\n}\r\nif (ep->rep_connected <= 0) {\r\nif (retry_count++ <= RDMA_CONNECT_RETRY_MAX + 1 &&\r\n(ep->rep_remote_cma.responder_resources == 0 ||\r\nep->rep_remote_cma.initiator_depth !=\r\nep->rep_remote_cma.responder_resources)) {\r\nif (ep->rep_remote_cma.responder_resources == 0)\r\nep->rep_remote_cma.responder_resources = 1;\r\nep->rep_remote_cma.initiator_depth =\r\nep->rep_remote_cma.responder_resources;\r\ngoto retry;\r\n}\r\nrc = ep->rep_connected;\r\n} else {\r\ndprintk("RPC: %s: connected\n", __func__);\r\n}\r\nout:\r\nif (rc)\r\nep->rep_connected = rc;\r\nreturn rc;\r\n}\r\nvoid\r\nrpcrdma_ep_disconnect(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia)\r\n{\r\nint rc;\r\nrpcrdma_flush_cqs(ep);\r\nrc = rdma_disconnect(ia->ri_id);\r\nif (!rc) {\r\nwait_event_interruptible(ep->rep_connect_wait,\r\nep->rep_connected != 1);\r\ndprintk("RPC: %s: after wait, %sconnected\n", __func__,\r\n(ep->rep_connected == 1) ? "still " : "dis");\r\n} else {\r\ndprintk("RPC: %s: rdma_disconnect %i\n", __func__, rc);\r\nep->rep_connected = rc;\r\n}\r\n}\r\nstatic int\r\nrpcrdma_init_fmrs(struct rpcrdma_ia *ia, struct rpcrdma_buffer *buf)\r\n{\r\nint mr_access_flags = IB_ACCESS_REMOTE_WRITE | IB_ACCESS_REMOTE_READ;\r\nstruct ib_fmr_attr fmr_attr = {\r\n.max_pages = RPCRDMA_MAX_DATA_SEGS,\r\n.max_maps = 1,\r\n.page_shift = PAGE_SHIFT\r\n};\r\nstruct rpcrdma_mw *r;\r\nint i, rc;\r\ni = (buf->rb_max_requests + 1) * RPCRDMA_MAX_SEGS;\r\ndprintk("RPC: %s: initalizing %d FMRs\n", __func__, i);\r\nwhile (i--) {\r\nr = kzalloc(sizeof(*r), GFP_KERNEL);\r\nif (r == NULL)\r\nreturn -ENOMEM;\r\nr->r.fmr = ib_alloc_fmr(ia->ri_pd, mr_access_flags, &fmr_attr);\r\nif (IS_ERR(r->r.fmr)) {\r\nrc = PTR_ERR(r->r.fmr);\r\ndprintk("RPC: %s: ib_alloc_fmr failed %i\n",\r\n__func__, rc);\r\ngoto out_free;\r\n}\r\nlist_add(&r->mw_list, &buf->rb_mws);\r\nlist_add(&r->mw_all, &buf->rb_all);\r\n}\r\nreturn 0;\r\nout_free:\r\nkfree(r);\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_init_frmrs(struct rpcrdma_ia *ia, struct rpcrdma_buffer *buf)\r\n{\r\nstruct rpcrdma_frmr *f;\r\nstruct rpcrdma_mw *r;\r\nint i, rc;\r\ni = (buf->rb_max_requests + 1) * RPCRDMA_MAX_SEGS;\r\ndprintk("RPC: %s: initalizing %d FRMRs\n", __func__, i);\r\nwhile (i--) {\r\nr = kzalloc(sizeof(*r), GFP_KERNEL);\r\nif (r == NULL)\r\nreturn -ENOMEM;\r\nf = &r->r.frmr;\r\nf->fr_mr = ib_alloc_fast_reg_mr(ia->ri_pd,\r\nia->ri_max_frmr_depth);\r\nif (IS_ERR(f->fr_mr)) {\r\nrc = PTR_ERR(f->fr_mr);\r\ndprintk("RPC: %s: ib_alloc_fast_reg_mr "\r\n"failed %i\n", __func__, rc);\r\ngoto out_free;\r\n}\r\nf->fr_pgl = ib_alloc_fast_reg_page_list(ia->ri_id->device,\r\nia->ri_max_frmr_depth);\r\nif (IS_ERR(f->fr_pgl)) {\r\nrc = PTR_ERR(f->fr_pgl);\r\ndprintk("RPC: %s: ib_alloc_fast_reg_page_list "\r\n"failed %i\n", __func__, rc);\r\nib_dereg_mr(f->fr_mr);\r\ngoto out_free;\r\n}\r\nlist_add(&r->mw_list, &buf->rb_mws);\r\nlist_add(&r->mw_all, &buf->rb_all);\r\n}\r\nreturn 0;\r\nout_free:\r\nkfree(r);\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_buffer_create(struct rpcrdma_buffer *buf, struct rpcrdma_ep *ep,\r\nstruct rpcrdma_ia *ia, struct rpcrdma_create_data_internal *cdata)\r\n{\r\nchar *p;\r\nsize_t len, rlen, wlen;\r\nint i, rc;\r\nbuf->rb_max_requests = cdata->max_requests;\r\nspin_lock_init(&buf->rb_lock);\r\natomic_set(&buf->rb_credits, 1);\r\nlen = buf->rb_max_requests *\r\n(sizeof(struct rpcrdma_req *) + sizeof(struct rpcrdma_rep *));\r\nlen += cdata->padding;\r\np = kzalloc(len, GFP_KERNEL);\r\nif (p == NULL) {\r\ndprintk("RPC: %s: req_t/rep_t/pad kzalloc(%zd) failed\n",\r\n__func__, len);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nbuf->rb_pool = p;\r\nbuf->rb_send_bufs = (struct rpcrdma_req **) p;\r\np = (char *) &buf->rb_send_bufs[buf->rb_max_requests];\r\nbuf->rb_recv_bufs = (struct rpcrdma_rep **) p;\r\np = (char *) &buf->rb_recv_bufs[buf->rb_max_requests];\r\nif (cdata->padding) {\r\nrc = rpcrdma_register_internal(ia, p, cdata->padding,\r\n&ep->rep_pad_mr, &ep->rep_pad);\r\nif (rc)\r\ngoto out;\r\n}\r\np += cdata->padding;\r\nINIT_LIST_HEAD(&buf->rb_mws);\r\nINIT_LIST_HEAD(&buf->rb_all);\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\nrc = rpcrdma_init_frmrs(ia, buf);\r\nif (rc)\r\ngoto out;\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nrc = rpcrdma_init_fmrs(ia, buf);\r\nif (rc)\r\ngoto out;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nwlen = 1 << fls(cdata->inline_wsize + sizeof(struct rpcrdma_req));\r\nrlen = 1 << fls(cdata->inline_rsize + sizeof(struct rpcrdma_rep));\r\ndprintk("RPC: %s: wlen = %zu, rlen = %zu\n",\r\n__func__, wlen, rlen);\r\nfor (i = 0; i < buf->rb_max_requests; i++) {\r\nstruct rpcrdma_req *req;\r\nstruct rpcrdma_rep *rep;\r\nreq = kmalloc(wlen, GFP_KERNEL);\r\nif (req == NULL) {\r\ndprintk("RPC: %s: request buffer %d alloc"\r\n" failed\n", __func__, i);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nmemset(req, 0, sizeof(struct rpcrdma_req));\r\nbuf->rb_send_bufs[i] = req;\r\nbuf->rb_send_bufs[i]->rl_buffer = buf;\r\nrc = rpcrdma_register_internal(ia, req->rl_base,\r\nwlen - offsetof(struct rpcrdma_req, rl_base),\r\n&buf->rb_send_bufs[i]->rl_handle,\r\n&buf->rb_send_bufs[i]->rl_iov);\r\nif (rc)\r\ngoto out;\r\nbuf->rb_send_bufs[i]->rl_size = wlen -\r\nsizeof(struct rpcrdma_req);\r\nrep = kmalloc(rlen, GFP_KERNEL);\r\nif (rep == NULL) {\r\ndprintk("RPC: %s: reply buffer %d alloc failed\n",\r\n__func__, i);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nmemset(rep, 0, sizeof(struct rpcrdma_rep));\r\nbuf->rb_recv_bufs[i] = rep;\r\nbuf->rb_recv_bufs[i]->rr_buffer = buf;\r\nrc = rpcrdma_register_internal(ia, rep->rr_base,\r\nrlen - offsetof(struct rpcrdma_rep, rr_base),\r\n&buf->rb_recv_bufs[i]->rr_handle,\r\n&buf->rb_recv_bufs[i]->rr_iov);\r\nif (rc)\r\ngoto out;\r\n}\r\ndprintk("RPC: %s: max_requests %d\n",\r\n__func__, buf->rb_max_requests);\r\nreturn 0;\r\nout:\r\nrpcrdma_buffer_destroy(buf);\r\nreturn rc;\r\n}\r\nstatic void\r\nrpcrdma_destroy_fmrs(struct rpcrdma_buffer *buf)\r\n{\r\nstruct rpcrdma_mw *r;\r\nint rc;\r\nwhile (!list_empty(&buf->rb_all)) {\r\nr = list_entry(buf->rb_all.next, struct rpcrdma_mw, mw_all);\r\nlist_del(&r->mw_all);\r\nlist_del(&r->mw_list);\r\nrc = ib_dealloc_fmr(r->r.fmr);\r\nif (rc)\r\ndprintk("RPC: %s: ib_dealloc_fmr failed %i\n",\r\n__func__, rc);\r\nkfree(r);\r\n}\r\n}\r\nstatic void\r\nrpcrdma_destroy_frmrs(struct rpcrdma_buffer *buf)\r\n{\r\nstruct rpcrdma_mw *r;\r\nint rc;\r\nwhile (!list_empty(&buf->rb_all)) {\r\nr = list_entry(buf->rb_all.next, struct rpcrdma_mw, mw_all);\r\nlist_del(&r->mw_all);\r\nlist_del(&r->mw_list);\r\nrc = ib_dereg_mr(r->r.frmr.fr_mr);\r\nif (rc)\r\ndprintk("RPC: %s: ib_dereg_mr failed %i\n",\r\n__func__, rc);\r\nib_free_fast_reg_page_list(r->r.frmr.fr_pgl);\r\nkfree(r);\r\n}\r\n}\r\nvoid\r\nrpcrdma_buffer_destroy(struct rpcrdma_buffer *buf)\r\n{\r\nstruct rpcrdma_ia *ia = rdmab_to_ia(buf);\r\nint i;\r\ndprintk("RPC: %s: entering\n", __func__);\r\nfor (i = 0; i < buf->rb_max_requests; i++) {\r\nif (buf->rb_recv_bufs && buf->rb_recv_bufs[i]) {\r\nrpcrdma_deregister_internal(ia,\r\nbuf->rb_recv_bufs[i]->rr_handle,\r\n&buf->rb_recv_bufs[i]->rr_iov);\r\nkfree(buf->rb_recv_bufs[i]);\r\n}\r\nif (buf->rb_send_bufs && buf->rb_send_bufs[i]) {\r\nrpcrdma_deregister_internal(ia,\r\nbuf->rb_send_bufs[i]->rl_handle,\r\n&buf->rb_send_bufs[i]->rl_iov);\r\nkfree(buf->rb_send_bufs[i]);\r\n}\r\n}\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\nrpcrdma_destroy_frmrs(buf);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nrpcrdma_destroy_fmrs(buf);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nkfree(buf->rb_pool);\r\n}\r\nstatic void\r\nrpcrdma_reset_frmrs(struct rpcrdma_ia *ia)\r\n{\r\nstruct rpcrdma_xprt *r_xprt =\r\ncontainer_of(ia, struct rpcrdma_xprt, rx_ia);\r\nstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\r\nstruct list_head *pos;\r\nstruct rpcrdma_mw *r;\r\nint rc;\r\nlist_for_each(pos, &buf->rb_all) {\r\nr = list_entry(pos, struct rpcrdma_mw, mw_all);\r\nif (r->r.frmr.fr_state == FRMR_IS_INVALID)\r\ncontinue;\r\nrc = ib_dereg_mr(r->r.frmr.fr_mr);\r\nif (rc)\r\ndprintk("RPC: %s: ib_dereg_mr failed %i\n",\r\n__func__, rc);\r\nib_free_fast_reg_page_list(r->r.frmr.fr_pgl);\r\nr->r.frmr.fr_mr = ib_alloc_fast_reg_mr(ia->ri_pd,\r\nia->ri_max_frmr_depth);\r\nif (IS_ERR(r->r.frmr.fr_mr)) {\r\nrc = PTR_ERR(r->r.frmr.fr_mr);\r\ndprintk("RPC: %s: ib_alloc_fast_reg_mr"\r\n" failed %i\n", __func__, rc);\r\ncontinue;\r\n}\r\nr->r.frmr.fr_pgl = ib_alloc_fast_reg_page_list(\r\nia->ri_id->device,\r\nia->ri_max_frmr_depth);\r\nif (IS_ERR(r->r.frmr.fr_pgl)) {\r\nrc = PTR_ERR(r->r.frmr.fr_pgl);\r\ndprintk("RPC: %s: "\r\n"ib_alloc_fast_reg_page_list "\r\n"failed %i\n", __func__, rc);\r\nib_dereg_mr(r->r.frmr.fr_mr);\r\ncontinue;\r\n}\r\nr->r.frmr.fr_state = FRMR_IS_INVALID;\r\n}\r\n}\r\nstatic void\r\nrpcrdma_buffer_put_mr(struct rpcrdma_mw **mw, struct rpcrdma_buffer *buf)\r\n{\r\nif (*mw) {\r\nlist_add_tail(&(*mw)->mw_list, &buf->rb_mws);\r\n*mw = NULL;\r\n}\r\n}\r\nstatic void\r\nrpcrdma_buffer_put_mrs(struct rpcrdma_req *req, struct rpcrdma_buffer *buf)\r\n{\r\nstruct rpcrdma_mr_seg *seg = req->rl_segments;\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nint i;\r\nfor (i = 1, seg++; i < RPCRDMA_MAX_SEGS; seg++, i++)\r\nrpcrdma_buffer_put_mr(&seg->mr_chunk.rl_mw, buf);\r\nrpcrdma_buffer_put_mr(&seg1->mr_chunk.rl_mw, buf);\r\n}\r\nstatic void\r\nrpcrdma_buffer_put_sendbuf(struct rpcrdma_req *req, struct rpcrdma_buffer *buf)\r\n{\r\nbuf->rb_send_bufs[--buf->rb_send_index] = req;\r\nreq->rl_niovs = 0;\r\nif (req->rl_reply) {\r\nbuf->rb_recv_bufs[--buf->rb_recv_index] = req->rl_reply;\r\nreq->rl_reply->rr_func = NULL;\r\nreq->rl_reply = NULL;\r\n}\r\n}\r\nstatic void\r\nrpcrdma_retry_local_inv(struct rpcrdma_mw *r, struct rpcrdma_ia *ia)\r\n{\r\nstruct rpcrdma_xprt *r_xprt =\r\ncontainer_of(ia, struct rpcrdma_xprt, rx_ia);\r\nstruct ib_send_wr invalidate_wr, *bad_wr;\r\nint rc;\r\ndprintk("RPC: %s: FRMR %p is stale\n", __func__, r);\r\nr->r.frmr.fr_state = FRMR_IS_INVALID;\r\nmemset(&invalidate_wr, 0, sizeof(invalidate_wr));\r\ninvalidate_wr.wr_id = (unsigned long)(void *)r;\r\ninvalidate_wr.opcode = IB_WR_LOCAL_INV;\r\ninvalidate_wr.ex.invalidate_rkey = r->r.frmr.fr_mr->rkey;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\ndprintk("RPC: %s: frmr %p invalidating rkey %08x\n",\r\n__func__, r, r->r.frmr.fr_mr->rkey);\r\nread_lock(&ia->ri_qplock);\r\nrc = ib_post_send(ia->ri_id->qp, &invalidate_wr, &bad_wr);\r\nread_unlock(&ia->ri_qplock);\r\nif (rc) {\r\nr->r.frmr.fr_state = FRMR_IS_STALE;\r\ndprintk("RPC: %s: ib_post_send failed, %i\n",\r\n__func__, rc);\r\n}\r\n}\r\nstatic void\r\nrpcrdma_retry_flushed_linv(struct list_head *stale,\r\nstruct rpcrdma_buffer *buf)\r\n{\r\nstruct rpcrdma_ia *ia = rdmab_to_ia(buf);\r\nstruct list_head *pos;\r\nstruct rpcrdma_mw *r;\r\nunsigned long flags;\r\nlist_for_each(pos, stale) {\r\nr = list_entry(pos, struct rpcrdma_mw, mw_list);\r\nrpcrdma_retry_local_inv(r, ia);\r\n}\r\nspin_lock_irqsave(&buf->rb_lock, flags);\r\nlist_splice_tail(stale, &buf->rb_mws);\r\nspin_unlock_irqrestore(&buf->rb_lock, flags);\r\n}\r\nstatic struct rpcrdma_req *\r\nrpcrdma_buffer_get_frmrs(struct rpcrdma_req *req, struct rpcrdma_buffer *buf,\r\nstruct list_head *stale)\r\n{\r\nstruct rpcrdma_mw *r;\r\nint i;\r\ni = RPCRDMA_MAX_SEGS - 1;\r\nwhile (!list_empty(&buf->rb_mws)) {\r\nr = list_entry(buf->rb_mws.next,\r\nstruct rpcrdma_mw, mw_list);\r\nlist_del(&r->mw_list);\r\nif (r->r.frmr.fr_state == FRMR_IS_STALE) {\r\nlist_add(&r->mw_list, stale);\r\ncontinue;\r\n}\r\nreq->rl_segments[i].mr_chunk.rl_mw = r;\r\nif (unlikely(i-- == 0))\r\nreturn req;\r\n}\r\nrpcrdma_buffer_put_sendbuf(req, buf);\r\nrpcrdma_buffer_put_mrs(req, buf);\r\nreturn NULL;\r\n}\r\nstatic struct rpcrdma_req *\r\nrpcrdma_buffer_get_fmrs(struct rpcrdma_req *req, struct rpcrdma_buffer *buf)\r\n{\r\nstruct rpcrdma_mw *r;\r\nint i;\r\ni = RPCRDMA_MAX_SEGS - 1;\r\nwhile (!list_empty(&buf->rb_mws)) {\r\nr = list_entry(buf->rb_mws.next,\r\nstruct rpcrdma_mw, mw_list);\r\nlist_del(&r->mw_list);\r\nreq->rl_segments[i].mr_chunk.rl_mw = r;\r\nif (unlikely(i-- == 0))\r\nreturn req;\r\n}\r\nrpcrdma_buffer_put_sendbuf(req, buf);\r\nrpcrdma_buffer_put_mrs(req, buf);\r\nreturn NULL;\r\n}\r\nstruct rpcrdma_req *\r\nrpcrdma_buffer_get(struct rpcrdma_buffer *buffers)\r\n{\r\nstruct rpcrdma_ia *ia = rdmab_to_ia(buffers);\r\nstruct list_head stale;\r\nstruct rpcrdma_req *req;\r\nunsigned long flags;\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nif (buffers->rb_send_index == buffers->rb_max_requests) {\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\ndprintk("RPC: %s: out of request buffers\n", __func__);\r\nreturn ((struct rpcrdma_req *)NULL);\r\n}\r\nreq = buffers->rb_send_bufs[buffers->rb_send_index];\r\nif (buffers->rb_send_index < buffers->rb_recv_index) {\r\ndprintk("RPC: %s: %d extra receives outstanding (ok)\n",\r\n__func__,\r\nbuffers->rb_recv_index - buffers->rb_send_index);\r\nreq->rl_reply = NULL;\r\n} else {\r\nreq->rl_reply = buffers->rb_recv_bufs[buffers->rb_recv_index];\r\nbuffers->rb_recv_bufs[buffers->rb_recv_index++] = NULL;\r\n}\r\nbuffers->rb_send_bufs[buffers->rb_send_index++] = NULL;\r\nINIT_LIST_HEAD(&stale);\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\nreq = rpcrdma_buffer_get_frmrs(req, buffers, &stale);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nreq = rpcrdma_buffer_get_fmrs(req, buffers);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\nif (!list_empty(&stale))\r\nrpcrdma_retry_flushed_linv(&stale, buffers);\r\nreturn req;\r\n}\r\nvoid\r\nrpcrdma_buffer_put(struct rpcrdma_req *req)\r\n{\r\nstruct rpcrdma_buffer *buffers = req->rl_buffer;\r\nstruct rpcrdma_ia *ia = rdmab_to_ia(buffers);\r\nunsigned long flags;\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nrpcrdma_buffer_put_sendbuf(req, buffers);\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\ncase RPCRDMA_MTHCAFMR:\r\nrpcrdma_buffer_put_mrs(req, buffers);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\n}\r\nvoid\r\nrpcrdma_recv_buffer_get(struct rpcrdma_req *req)\r\n{\r\nstruct rpcrdma_buffer *buffers = req->rl_buffer;\r\nunsigned long flags;\r\nif (req->rl_iov.length == 0)\r\nbuffers = ((struct rpcrdma_req *) buffers)->rl_buffer;\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nif (buffers->rb_recv_index < buffers->rb_max_requests) {\r\nreq->rl_reply = buffers->rb_recv_bufs[buffers->rb_recv_index];\r\nbuffers->rb_recv_bufs[buffers->rb_recv_index++] = NULL;\r\n}\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\n}\r\nvoid\r\nrpcrdma_recv_buffer_put(struct rpcrdma_rep *rep)\r\n{\r\nstruct rpcrdma_buffer *buffers = rep->rr_buffer;\r\nunsigned long flags;\r\nrep->rr_func = NULL;\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nbuffers->rb_recv_bufs[--buffers->rb_recv_index] = rep;\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\n}\r\nint\r\nrpcrdma_register_internal(struct rpcrdma_ia *ia, void *va, int len,\r\nstruct ib_mr **mrp, struct ib_sge *iov)\r\n{\r\nstruct ib_phys_buf ipb;\r\nstruct ib_mr *mr;\r\nint rc;\r\niov->addr = ib_dma_map_single(ia->ri_id->device,\r\nva, len, DMA_BIDIRECTIONAL);\r\nif (ib_dma_mapping_error(ia->ri_id->device, iov->addr))\r\nreturn -ENOMEM;\r\niov->length = len;\r\nif (ia->ri_have_dma_lkey) {\r\n*mrp = NULL;\r\niov->lkey = ia->ri_dma_lkey;\r\nreturn 0;\r\n} else if (ia->ri_bind_mem != NULL) {\r\n*mrp = NULL;\r\niov->lkey = ia->ri_bind_mem->lkey;\r\nreturn 0;\r\n}\r\nipb.addr = iov->addr;\r\nipb.size = iov->length;\r\nmr = ib_reg_phys_mr(ia->ri_pd, &ipb, 1,\r\nIB_ACCESS_LOCAL_WRITE, &iov->addr);\r\ndprintk("RPC: %s: phys convert: 0x%llx "\r\n"registered 0x%llx length %d\n",\r\n__func__, (unsigned long long)ipb.addr,\r\n(unsigned long long)iov->addr, len);\r\nif (IS_ERR(mr)) {\r\n*mrp = NULL;\r\nrc = PTR_ERR(mr);\r\ndprintk("RPC: %s: failed with %i\n", __func__, rc);\r\n} else {\r\n*mrp = mr;\r\niov->lkey = mr->lkey;\r\nrc = 0;\r\n}\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_deregister_internal(struct rpcrdma_ia *ia,\r\nstruct ib_mr *mr, struct ib_sge *iov)\r\n{\r\nint rc;\r\nib_dma_unmap_single(ia->ri_id->device,\r\niov->addr, iov->length, DMA_BIDIRECTIONAL);\r\nif (NULL == mr)\r\nreturn 0;\r\nrc = ib_dereg_mr(mr);\r\nif (rc)\r\ndprintk("RPC: %s: ib_dereg_mr failed %i\n", __func__, rc);\r\nreturn rc;\r\n}\r\nstatic void\r\nrpcrdma_map_one(struct rpcrdma_ia *ia, struct rpcrdma_mr_seg *seg, int writing)\r\n{\r\nseg->mr_dir = writing ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\r\nseg->mr_dmalen = seg->mr_len;\r\nif (seg->mr_page)\r\nseg->mr_dma = ib_dma_map_page(ia->ri_id->device,\r\nseg->mr_page, offset_in_page(seg->mr_offset),\r\nseg->mr_dmalen, seg->mr_dir);\r\nelse\r\nseg->mr_dma = ib_dma_map_single(ia->ri_id->device,\r\nseg->mr_offset,\r\nseg->mr_dmalen, seg->mr_dir);\r\nif (ib_dma_mapping_error(ia->ri_id->device, seg->mr_dma)) {\r\ndprintk("RPC: %s: mr_dma %llx mr_offset %p mr_dma_len %zu\n",\r\n__func__,\r\n(unsigned long long)seg->mr_dma,\r\nseg->mr_offset, seg->mr_dmalen);\r\n}\r\n}\r\nstatic void\r\nrpcrdma_unmap_one(struct rpcrdma_ia *ia, struct rpcrdma_mr_seg *seg)\r\n{\r\nif (seg->mr_page)\r\nib_dma_unmap_page(ia->ri_id->device,\r\nseg->mr_dma, seg->mr_dmalen, seg->mr_dir);\r\nelse\r\nib_dma_unmap_single(ia->ri_id->device,\r\nseg->mr_dma, seg->mr_dmalen, seg->mr_dir);\r\n}\r\nstatic int\r\nrpcrdma_register_frmr_external(struct rpcrdma_mr_seg *seg,\r\nint *nsegs, int writing, struct rpcrdma_ia *ia,\r\nstruct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nstruct rpcrdma_mw *mw = seg1->mr_chunk.rl_mw;\r\nstruct rpcrdma_frmr *frmr = &mw->r.frmr;\r\nstruct ib_mr *mr = frmr->fr_mr;\r\nstruct ib_send_wr fastreg_wr, *bad_wr;\r\nu8 key;\r\nint len, pageoff;\r\nint i, rc;\r\nint seg_len;\r\nu64 pa;\r\nint page_no;\r\npageoff = offset_in_page(seg1->mr_offset);\r\nseg1->mr_offset -= pageoff;\r\nseg1->mr_len += pageoff;\r\nlen = -pageoff;\r\nif (*nsegs > ia->ri_max_frmr_depth)\r\n*nsegs = ia->ri_max_frmr_depth;\r\nfor (page_no = i = 0; i < *nsegs;) {\r\nrpcrdma_map_one(ia, seg, writing);\r\npa = seg->mr_dma;\r\nfor (seg_len = seg->mr_len; seg_len > 0; seg_len -= PAGE_SIZE) {\r\nfrmr->fr_pgl->page_list[page_no++] = pa;\r\npa += PAGE_SIZE;\r\n}\r\nlen += seg->mr_len;\r\n++seg;\r\n++i;\r\nif ((i < *nsegs && offset_in_page(seg->mr_offset)) ||\r\noffset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))\r\nbreak;\r\n}\r\ndprintk("RPC: %s: Using frmr %p to map %d segments\n",\r\n__func__, mw, i);\r\nfrmr->fr_state = FRMR_IS_VALID;\r\nmemset(&fastreg_wr, 0, sizeof(fastreg_wr));\r\nfastreg_wr.wr_id = (unsigned long)(void *)mw;\r\nfastreg_wr.opcode = IB_WR_FAST_REG_MR;\r\nfastreg_wr.wr.fast_reg.iova_start = seg1->mr_dma;\r\nfastreg_wr.wr.fast_reg.page_list = frmr->fr_pgl;\r\nfastreg_wr.wr.fast_reg.page_list_len = page_no;\r\nfastreg_wr.wr.fast_reg.page_shift = PAGE_SHIFT;\r\nfastreg_wr.wr.fast_reg.length = page_no << PAGE_SHIFT;\r\nif (fastreg_wr.wr.fast_reg.length < len) {\r\nrc = -EIO;\r\ngoto out_err;\r\n}\r\nkey = (u8)(mr->rkey & 0x000000FF);\r\nib_update_fast_reg_key(mr, ++key);\r\nfastreg_wr.wr.fast_reg.access_flags = (writing ?\r\nIB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :\r\nIB_ACCESS_REMOTE_READ);\r\nfastreg_wr.wr.fast_reg.rkey = mr->rkey;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\nrc = ib_post_send(ia->ri_id->qp, &fastreg_wr, &bad_wr);\r\nif (rc) {\r\ndprintk("RPC: %s: failed ib_post_send for register,"\r\n" status %i\n", __func__, rc);\r\nib_update_fast_reg_key(mr, --key);\r\ngoto out_err;\r\n} else {\r\nseg1->mr_rkey = mr->rkey;\r\nseg1->mr_base = seg1->mr_dma + pageoff;\r\nseg1->mr_nsegs = i;\r\nseg1->mr_len = len;\r\n}\r\n*nsegs = i;\r\nreturn 0;\r\nout_err:\r\nfrmr->fr_state = FRMR_IS_INVALID;\r\nwhile (i--)\r\nrpcrdma_unmap_one(ia, --seg);\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_deregister_frmr_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_ia *ia, struct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nstruct ib_send_wr invalidate_wr, *bad_wr;\r\nint rc;\r\nseg1->mr_chunk.rl_mw->r.frmr.fr_state = FRMR_IS_INVALID;\r\nmemset(&invalidate_wr, 0, sizeof invalidate_wr);\r\ninvalidate_wr.wr_id = (unsigned long)(void *)seg1->mr_chunk.rl_mw;\r\ninvalidate_wr.opcode = IB_WR_LOCAL_INV;\r\ninvalidate_wr.ex.invalidate_rkey = seg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\nread_lock(&ia->ri_qplock);\r\nwhile (seg1->mr_nsegs--)\r\nrpcrdma_unmap_one(ia, seg++);\r\nrc = ib_post_send(ia->ri_id->qp, &invalidate_wr, &bad_wr);\r\nread_unlock(&ia->ri_qplock);\r\nif (rc) {\r\nseg1->mr_chunk.rl_mw->r.frmr.fr_state = FRMR_IS_STALE;\r\ndprintk("RPC: %s: failed ib_post_send for invalidate,"\r\n" status %i\n", __func__, rc);\r\n}\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_register_fmr_external(struct rpcrdma_mr_seg *seg,\r\nint *nsegs, int writing, struct rpcrdma_ia *ia)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nu64 physaddrs[RPCRDMA_MAX_DATA_SEGS];\r\nint len, pageoff, i, rc;\r\npageoff = offset_in_page(seg1->mr_offset);\r\nseg1->mr_offset -= pageoff;\r\nseg1->mr_len += pageoff;\r\nlen = -pageoff;\r\nif (*nsegs > RPCRDMA_MAX_DATA_SEGS)\r\n*nsegs = RPCRDMA_MAX_DATA_SEGS;\r\nfor (i = 0; i < *nsegs;) {\r\nrpcrdma_map_one(ia, seg, writing);\r\nphysaddrs[i] = seg->mr_dma;\r\nlen += seg->mr_len;\r\n++seg;\r\n++i;\r\nif ((i < *nsegs && offset_in_page(seg->mr_offset)) ||\r\noffset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))\r\nbreak;\r\n}\r\nrc = ib_map_phys_fmr(seg1->mr_chunk.rl_mw->r.fmr,\r\nphysaddrs, i, seg1->mr_dma);\r\nif (rc) {\r\ndprintk("RPC: %s: failed ib_map_phys_fmr "\r\n"%u@0x%llx+%i (%d)... status %i\n", __func__,\r\nlen, (unsigned long long)seg1->mr_dma,\r\npageoff, i, rc);\r\nwhile (i--)\r\nrpcrdma_unmap_one(ia, --seg);\r\n} else {\r\nseg1->mr_rkey = seg1->mr_chunk.rl_mw->r.fmr->rkey;\r\nseg1->mr_base = seg1->mr_dma + pageoff;\r\nseg1->mr_nsegs = i;\r\nseg1->mr_len = len;\r\n}\r\n*nsegs = i;\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_deregister_fmr_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_ia *ia)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nLIST_HEAD(l);\r\nint rc;\r\nlist_add(&seg1->mr_chunk.rl_mw->r.fmr->list, &l);\r\nrc = ib_unmap_fmr(&l);\r\nread_lock(&ia->ri_qplock);\r\nwhile (seg1->mr_nsegs--)\r\nrpcrdma_unmap_one(ia, seg++);\r\nread_unlock(&ia->ri_qplock);\r\nif (rc)\r\ndprintk("RPC: %s: failed ib_unmap_fmr,"\r\n" status %i\n", __func__, rc);\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_register_external(struct rpcrdma_mr_seg *seg,\r\nint nsegs, int writing, struct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_ia *ia = &r_xprt->rx_ia;\r\nint rc = 0;\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_ALLPHYSICAL:\r\nrpcrdma_map_one(ia, seg, writing);\r\nseg->mr_rkey = ia->ri_bind_mem->rkey;\r\nseg->mr_base = seg->mr_dma;\r\nseg->mr_nsegs = 1;\r\nnsegs = 1;\r\nbreak;\r\ncase RPCRDMA_FRMR:\r\nrc = rpcrdma_register_frmr_external(seg, &nsegs, writing, ia, r_xprt);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nrc = rpcrdma_register_fmr_external(seg, &nsegs, writing, ia);\r\nbreak;\r\ndefault:\r\nreturn -1;\r\n}\r\nif (rc)\r\nreturn -1;\r\nreturn nsegs;\r\n}\r\nint\r\nrpcrdma_deregister_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_ia *ia = &r_xprt->rx_ia;\r\nint nsegs = seg->mr_nsegs, rc;\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_ALLPHYSICAL:\r\nread_lock(&ia->ri_qplock);\r\nrpcrdma_unmap_one(ia, seg);\r\nread_unlock(&ia->ri_qplock);\r\nbreak;\r\ncase RPCRDMA_FRMR:\r\nrc = rpcrdma_deregister_frmr_external(seg, ia, r_xprt);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nrc = rpcrdma_deregister_fmr_external(seg, ia);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn nsegs;\r\n}\r\nint\r\nrpcrdma_ep_post(struct rpcrdma_ia *ia,\r\nstruct rpcrdma_ep *ep,\r\nstruct rpcrdma_req *req)\r\n{\r\nstruct ib_send_wr send_wr, *send_wr_fail;\r\nstruct rpcrdma_rep *rep = req->rl_reply;\r\nint rc;\r\nif (rep) {\r\nrc = rpcrdma_ep_post_recv(ia, ep, rep);\r\nif (rc)\r\ngoto out;\r\nreq->rl_reply = NULL;\r\n}\r\nsend_wr.next = NULL;\r\nsend_wr.wr_id = 0ULL;\r\nsend_wr.sg_list = req->rl_send_iov;\r\nsend_wr.num_sge = req->rl_niovs;\r\nsend_wr.opcode = IB_WR_SEND;\r\nif (send_wr.num_sge == 4)\r\nib_dma_sync_single_for_device(ia->ri_id->device,\r\nreq->rl_send_iov[3].addr, req->rl_send_iov[3].length,\r\nDMA_TO_DEVICE);\r\nib_dma_sync_single_for_device(ia->ri_id->device,\r\nreq->rl_send_iov[1].addr, req->rl_send_iov[1].length,\r\nDMA_TO_DEVICE);\r\nib_dma_sync_single_for_device(ia->ri_id->device,\r\nreq->rl_send_iov[0].addr, req->rl_send_iov[0].length,\r\nDMA_TO_DEVICE);\r\nif (DECR_CQCOUNT(ep) > 0)\r\nsend_wr.send_flags = 0;\r\nelse {\r\nINIT_CQCOUNT(ep);\r\nsend_wr.send_flags = IB_SEND_SIGNALED;\r\n}\r\nrc = ib_post_send(ia->ri_id->qp, &send_wr, &send_wr_fail);\r\nif (rc)\r\ndprintk("RPC: %s: ib_post_send returned %i\n", __func__,\r\nrc);\r\nout:\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_ep_post_recv(struct rpcrdma_ia *ia,\r\nstruct rpcrdma_ep *ep,\r\nstruct rpcrdma_rep *rep)\r\n{\r\nstruct ib_recv_wr recv_wr, *recv_wr_fail;\r\nint rc;\r\nrecv_wr.next = NULL;\r\nrecv_wr.wr_id = (u64) (unsigned long) rep;\r\nrecv_wr.sg_list = &rep->rr_iov;\r\nrecv_wr.num_sge = 1;\r\nib_dma_sync_single_for_cpu(ia->ri_id->device,\r\nrep->rr_iov.addr, rep->rr_iov.length, DMA_BIDIRECTIONAL);\r\nrc = ib_post_recv(ia->ri_id->qp, &recv_wr, &recv_wr_fail);\r\nif (rc)\r\ndprintk("RPC: %s: ib_post_recv returned %i\n", __func__,\r\nrc);\r\nreturn rc;\r\n}\r\nstatic size_t\r\nrpcrdma_physical_max_payload(struct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_create_data_internal *cdata = &r_xprt->rx_data;\r\nunsigned int inline_size, pages;\r\ninline_size = min_t(unsigned int,\r\ncdata->inline_wsize, cdata->inline_rsize);\r\ninline_size -= RPCRDMA_HDRLEN_MIN;\r\npages = inline_size / sizeof(struct rpcrdma_segment);\r\nreturn pages << PAGE_SHIFT;\r\n}\r\nstatic size_t\r\nrpcrdma_mr_max_payload(struct rpcrdma_xprt *r_xprt)\r\n{\r\nreturn RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT;\r\n}\r\nsize_t\r\nrpcrdma_max_payload(struct rpcrdma_xprt *r_xprt)\r\n{\r\nsize_t result;\r\nswitch (r_xprt->rx_ia.ri_memreg_strategy) {\r\ncase RPCRDMA_ALLPHYSICAL:\r\nresult = rpcrdma_physical_max_payload(r_xprt);\r\nbreak;\r\ndefault:\r\nresult = rpcrdma_mr_max_payload(r_xprt);\r\n}\r\nreturn result;\r\n}
