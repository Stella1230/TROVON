static int srp_tmo_get(char *buffer, const struct kernel_param *kp)\r\n{\r\nint tmo = *(int *)kp->arg;\r\nif (tmo >= 0)\r\nreturn sprintf(buffer, "%d", tmo);\r\nelse\r\nreturn sprintf(buffer, "off");\r\n}\r\nstatic int srp_tmo_set(const char *val, const struct kernel_param *kp)\r\n{\r\nint tmo, res;\r\nif (strncmp(val, "off", 3) != 0) {\r\nres = kstrtoint(val, 0, &tmo);\r\nif (res)\r\ngoto out;\r\n} else {\r\ntmo = -1;\r\n}\r\nif (kp->arg == &srp_reconnect_delay)\r\nres = srp_tmo_valid(tmo, srp_fast_io_fail_tmo,\r\nsrp_dev_loss_tmo);\r\nelse if (kp->arg == &srp_fast_io_fail_tmo)\r\nres = srp_tmo_valid(srp_reconnect_delay, tmo, srp_dev_loss_tmo);\r\nelse\r\nres = srp_tmo_valid(srp_reconnect_delay, srp_fast_io_fail_tmo,\r\ntmo);\r\nif (res)\r\ngoto out;\r\n*(int *)kp->arg = tmo;\r\nout:\r\nreturn res;\r\n}\r\nstatic inline struct srp_target_port *host_to_target(struct Scsi_Host *host)\r\n{\r\nreturn (struct srp_target_port *) host->hostdata;\r\n}\r\nstatic const char *srp_target_info(struct Scsi_Host *host)\r\n{\r\nreturn host_to_target(host)->target_name;\r\n}\r\nstatic int srp_target_is_topspin(struct srp_target_port *target)\r\n{\r\nstatic const u8 topspin_oui[3] = { 0x00, 0x05, 0xad };\r\nstatic const u8 cisco_oui[3] = { 0x00, 0x1b, 0x0d };\r\nreturn topspin_workarounds &&\r\n(!memcmp(&target->ioc_guid, topspin_oui, sizeof topspin_oui) ||\r\n!memcmp(&target->ioc_guid, cisco_oui, sizeof cisco_oui));\r\n}\r\nstatic struct srp_iu *srp_alloc_iu(struct srp_host *host, size_t size,\r\ngfp_t gfp_mask,\r\nenum dma_data_direction direction)\r\n{\r\nstruct srp_iu *iu;\r\niu = kmalloc(sizeof *iu, gfp_mask);\r\nif (!iu)\r\ngoto out;\r\niu->buf = kzalloc(size, gfp_mask);\r\nif (!iu->buf)\r\ngoto out_free_iu;\r\niu->dma = ib_dma_map_single(host->srp_dev->dev, iu->buf, size,\r\ndirection);\r\nif (ib_dma_mapping_error(host->srp_dev->dev, iu->dma))\r\ngoto out_free_buf;\r\niu->size = size;\r\niu->direction = direction;\r\nreturn iu;\r\nout_free_buf:\r\nkfree(iu->buf);\r\nout_free_iu:\r\nkfree(iu);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic void srp_free_iu(struct srp_host *host, struct srp_iu *iu)\r\n{\r\nif (!iu)\r\nreturn;\r\nib_dma_unmap_single(host->srp_dev->dev, iu->dma, iu->size,\r\niu->direction);\r\nkfree(iu->buf);\r\nkfree(iu);\r\n}\r\nstatic void srp_qp_event(struct ib_event *event, void *context)\r\n{\r\npr_debug("QP event %d\n", event->event);\r\n}\r\nstatic int srp_init_qp(struct srp_target_port *target,\r\nstruct ib_qp *qp)\r\n{\r\nstruct ib_qp_attr *attr;\r\nint ret;\r\nattr = kmalloc(sizeof *attr, GFP_KERNEL);\r\nif (!attr)\r\nreturn -ENOMEM;\r\nret = ib_find_pkey(target->srp_host->srp_dev->dev,\r\ntarget->srp_host->port,\r\nbe16_to_cpu(target->path.pkey),\r\n&attr->pkey_index);\r\nif (ret)\r\ngoto out;\r\nattr->qp_state = IB_QPS_INIT;\r\nattr->qp_access_flags = (IB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE);\r\nattr->port_num = target->srp_host->port;\r\nret = ib_modify_qp(qp, attr,\r\nIB_QP_STATE |\r\nIB_QP_PKEY_INDEX |\r\nIB_QP_ACCESS_FLAGS |\r\nIB_QP_PORT);\r\nout:\r\nkfree(attr);\r\nreturn ret;\r\n}\r\nstatic int srp_new_cm_id(struct srp_target_port *target)\r\n{\r\nstruct ib_cm_id *new_cm_id;\r\nnew_cm_id = ib_create_cm_id(target->srp_host->srp_dev->dev,\r\nsrp_cm_handler, target);\r\nif (IS_ERR(new_cm_id))\r\nreturn PTR_ERR(new_cm_id);\r\nif (target->cm_id)\r\nib_destroy_cm_id(target->cm_id);\r\ntarget->cm_id = new_cm_id;\r\nreturn 0;\r\n}\r\nstatic struct ib_fmr_pool *srp_alloc_fmr_pool(struct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_fmr_pool_param fmr_param;\r\nmemset(&fmr_param, 0, sizeof(fmr_param));\r\nfmr_param.pool_size = target->scsi_host->can_queue;\r\nfmr_param.dirty_watermark = fmr_param.pool_size / 4;\r\nfmr_param.cache = 1;\r\nfmr_param.max_pages_per_fmr = dev->max_pages_per_mr;\r\nfmr_param.page_shift = ilog2(dev->mr_page_size);\r\nfmr_param.access = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_READ);\r\nreturn ib_create_fmr_pool(dev->pd, &fmr_param);\r\n}\r\nstatic void srp_destroy_fr_pool(struct srp_fr_pool *pool)\r\n{\r\nint i;\r\nstruct srp_fr_desc *d;\r\nif (!pool)\r\nreturn;\r\nfor (i = 0, d = &pool->desc[0]; i < pool->size; i++, d++) {\r\nif (d->frpl)\r\nib_free_fast_reg_page_list(d->frpl);\r\nif (d->mr)\r\nib_dereg_mr(d->mr);\r\n}\r\nkfree(pool);\r\n}\r\nstatic struct srp_fr_pool *srp_create_fr_pool(struct ib_device *device,\r\nstruct ib_pd *pd, int pool_size,\r\nint max_page_list_len)\r\n{\r\nstruct srp_fr_pool *pool;\r\nstruct srp_fr_desc *d;\r\nstruct ib_mr *mr;\r\nstruct ib_fast_reg_page_list *frpl;\r\nint i, ret = -EINVAL;\r\nif (pool_size <= 0)\r\ngoto err;\r\nret = -ENOMEM;\r\npool = kzalloc(sizeof(struct srp_fr_pool) +\r\npool_size * sizeof(struct srp_fr_desc), GFP_KERNEL);\r\nif (!pool)\r\ngoto err;\r\npool->size = pool_size;\r\npool->max_page_list_len = max_page_list_len;\r\nspin_lock_init(&pool->lock);\r\nINIT_LIST_HEAD(&pool->free_list);\r\nfor (i = 0, d = &pool->desc[0]; i < pool->size; i++, d++) {\r\nmr = ib_alloc_fast_reg_mr(pd, max_page_list_len);\r\nif (IS_ERR(mr)) {\r\nret = PTR_ERR(mr);\r\ngoto destroy_pool;\r\n}\r\nd->mr = mr;\r\nfrpl = ib_alloc_fast_reg_page_list(device, max_page_list_len);\r\nif (IS_ERR(frpl)) {\r\nret = PTR_ERR(frpl);\r\ngoto destroy_pool;\r\n}\r\nd->frpl = frpl;\r\nlist_add_tail(&d->entry, &pool->free_list);\r\n}\r\nout:\r\nreturn pool;\r\ndestroy_pool:\r\nsrp_destroy_fr_pool(pool);\r\nerr:\r\npool = ERR_PTR(ret);\r\ngoto out;\r\n}\r\nstatic struct srp_fr_desc *srp_fr_pool_get(struct srp_fr_pool *pool)\r\n{\r\nstruct srp_fr_desc *d = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (!list_empty(&pool->free_list)) {\r\nd = list_first_entry(&pool->free_list, typeof(*d), entry);\r\nlist_del(&d->entry);\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn d;\r\n}\r\nstatic void srp_fr_pool_put(struct srp_fr_pool *pool, struct srp_fr_desc **desc,\r\nint n)\r\n{\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nfor (i = 0; i < n; i++)\r\nlist_add(&desc[i]->entry, &pool->free_list);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic struct srp_fr_pool *srp_alloc_fr_pool(struct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nreturn srp_create_fr_pool(dev->dev, dev->pd,\r\ntarget->scsi_host->can_queue,\r\ndev->max_pages_per_mr);\r\n}\r\nstatic int srp_create_target_ib(struct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_qp_init_attr *init_attr;\r\nstruct ib_cq *recv_cq, *send_cq;\r\nstruct ib_qp *qp;\r\nstruct ib_fmr_pool *fmr_pool = NULL;\r\nstruct srp_fr_pool *fr_pool = NULL;\r\nconst int m = 1 + dev->use_fast_reg;\r\nint ret;\r\ninit_attr = kzalloc(sizeof *init_attr, GFP_KERNEL);\r\nif (!init_attr)\r\nreturn -ENOMEM;\r\nrecv_cq = ib_create_cq(dev->dev, srp_recv_completion, NULL, target,\r\ntarget->queue_size, target->comp_vector);\r\nif (IS_ERR(recv_cq)) {\r\nret = PTR_ERR(recv_cq);\r\ngoto err;\r\n}\r\nsend_cq = ib_create_cq(dev->dev, srp_send_completion, NULL, target,\r\nm * target->queue_size, target->comp_vector);\r\nif (IS_ERR(send_cq)) {\r\nret = PTR_ERR(send_cq);\r\ngoto err_recv_cq;\r\n}\r\nib_req_notify_cq(recv_cq, IB_CQ_NEXT_COMP);\r\ninit_attr->event_handler = srp_qp_event;\r\ninit_attr->cap.max_send_wr = m * target->queue_size;\r\ninit_attr->cap.max_recv_wr = target->queue_size;\r\ninit_attr->cap.max_recv_sge = 1;\r\ninit_attr->cap.max_send_sge = 1;\r\ninit_attr->sq_sig_type = IB_SIGNAL_REQ_WR;\r\ninit_attr->qp_type = IB_QPT_RC;\r\ninit_attr->send_cq = send_cq;\r\ninit_attr->recv_cq = recv_cq;\r\nqp = ib_create_qp(dev->pd, init_attr);\r\nif (IS_ERR(qp)) {\r\nret = PTR_ERR(qp);\r\ngoto err_send_cq;\r\n}\r\nret = srp_init_qp(target, qp);\r\nif (ret)\r\ngoto err_qp;\r\nif (dev->use_fast_reg && dev->has_fr) {\r\nfr_pool = srp_alloc_fr_pool(target);\r\nif (IS_ERR(fr_pool)) {\r\nret = PTR_ERR(fr_pool);\r\nshost_printk(KERN_WARNING, target->scsi_host, PFX\r\n"FR pool allocation failed (%d)\n", ret);\r\ngoto err_qp;\r\n}\r\nif (target->fr_pool)\r\nsrp_destroy_fr_pool(target->fr_pool);\r\ntarget->fr_pool = fr_pool;\r\n} else if (!dev->use_fast_reg && dev->has_fmr) {\r\nfmr_pool = srp_alloc_fmr_pool(target);\r\nif (IS_ERR(fmr_pool)) {\r\nret = PTR_ERR(fmr_pool);\r\nshost_printk(KERN_WARNING, target->scsi_host, PFX\r\n"FMR pool allocation failed (%d)\n", ret);\r\ngoto err_qp;\r\n}\r\nif (target->fmr_pool)\r\nib_destroy_fmr_pool(target->fmr_pool);\r\ntarget->fmr_pool = fmr_pool;\r\n}\r\nif (target->qp)\r\nib_destroy_qp(target->qp);\r\nif (target->recv_cq)\r\nib_destroy_cq(target->recv_cq);\r\nif (target->send_cq)\r\nib_destroy_cq(target->send_cq);\r\ntarget->qp = qp;\r\ntarget->recv_cq = recv_cq;\r\ntarget->send_cq = send_cq;\r\nkfree(init_attr);\r\nreturn 0;\r\nerr_qp:\r\nib_destroy_qp(qp);\r\nerr_send_cq:\r\nib_destroy_cq(send_cq);\r\nerr_recv_cq:\r\nib_destroy_cq(recv_cq);\r\nerr:\r\nkfree(init_attr);\r\nreturn ret;\r\n}\r\nstatic void srp_free_target_ib(struct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nint i;\r\nif (dev->use_fast_reg) {\r\nif (target->fr_pool)\r\nsrp_destroy_fr_pool(target->fr_pool);\r\n} else {\r\nif (target->fmr_pool)\r\nib_destroy_fmr_pool(target->fmr_pool);\r\n}\r\nib_destroy_qp(target->qp);\r\nib_destroy_cq(target->send_cq);\r\nib_destroy_cq(target->recv_cq);\r\ntarget->qp = NULL;\r\ntarget->send_cq = target->recv_cq = NULL;\r\nif (target->rx_ring) {\r\nfor (i = 0; i < target->queue_size; ++i)\r\nsrp_free_iu(target->srp_host, target->rx_ring[i]);\r\nkfree(target->rx_ring);\r\ntarget->rx_ring = NULL;\r\n}\r\nif (target->tx_ring) {\r\nfor (i = 0; i < target->queue_size; ++i)\r\nsrp_free_iu(target->srp_host, target->tx_ring[i]);\r\nkfree(target->tx_ring);\r\ntarget->tx_ring = NULL;\r\n}\r\n}\r\nstatic void srp_path_rec_completion(int status,\r\nstruct ib_sa_path_rec *pathrec,\r\nvoid *target_ptr)\r\n{\r\nstruct srp_target_port *target = target_ptr;\r\ntarget->status = status;\r\nif (status)\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Got failed path rec status %d\n", status);\r\nelse\r\ntarget->path = *pathrec;\r\ncomplete(&target->done);\r\n}\r\nstatic int srp_lookup_path(struct srp_target_port *target)\r\n{\r\nint ret;\r\ntarget->path.numb_path = 1;\r\ninit_completion(&target->done);\r\ntarget->path_query_id = ib_sa_path_rec_get(&srp_sa_client,\r\ntarget->srp_host->srp_dev->dev,\r\ntarget->srp_host->port,\r\n&target->path,\r\nIB_SA_PATH_REC_SERVICE_ID |\r\nIB_SA_PATH_REC_DGID |\r\nIB_SA_PATH_REC_SGID |\r\nIB_SA_PATH_REC_NUMB_PATH |\r\nIB_SA_PATH_REC_PKEY,\r\nSRP_PATH_REC_TIMEOUT_MS,\r\nGFP_KERNEL,\r\nsrp_path_rec_completion,\r\ntarget, &target->path_query);\r\nif (target->path_query_id < 0)\r\nreturn target->path_query_id;\r\nret = wait_for_completion_interruptible(&target->done);\r\nif (ret < 0)\r\nreturn ret;\r\nif (target->status < 0)\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Path record query failed\n");\r\nreturn target->status;\r\n}\r\nstatic int srp_send_req(struct srp_target_port *target)\r\n{\r\nstruct {\r\nstruct ib_cm_req_param param;\r\nstruct srp_login_req priv;\r\n} *req = NULL;\r\nint status;\r\nreq = kzalloc(sizeof *req, GFP_KERNEL);\r\nif (!req)\r\nreturn -ENOMEM;\r\nreq->param.primary_path = &target->path;\r\nreq->param.alternate_path = NULL;\r\nreq->param.service_id = target->service_id;\r\nreq->param.qp_num = target->qp->qp_num;\r\nreq->param.qp_type = target->qp->qp_type;\r\nreq->param.private_data = &req->priv;\r\nreq->param.private_data_len = sizeof req->priv;\r\nreq->param.flow_control = 1;\r\nget_random_bytes(&req->param.starting_psn, 4);\r\nreq->param.starting_psn &= 0xffffff;\r\nreq->param.responder_resources = 4;\r\nreq->param.remote_cm_response_timeout = 20;\r\nreq->param.local_cm_response_timeout = 20;\r\nreq->param.retry_count = target->tl_retry_count;\r\nreq->param.rnr_retry_count = 7;\r\nreq->param.max_cm_retries = 15;\r\nreq->priv.opcode = SRP_LOGIN_REQ;\r\nreq->priv.tag = 0;\r\nreq->priv.req_it_iu_len = cpu_to_be32(target->max_iu_len);\r\nreq->priv.req_buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT |\r\nSRP_BUF_FORMAT_INDIRECT);\r\nif (target->io_class == SRP_REV10_IB_IO_CLASS) {\r\nmemcpy(req->priv.initiator_port_id,\r\n&target->path.sgid.global.interface_id, 8);\r\nmemcpy(req->priv.initiator_port_id + 8,\r\n&target->initiator_ext, 8);\r\nmemcpy(req->priv.target_port_id, &target->ioc_guid, 8);\r\nmemcpy(req->priv.target_port_id + 8, &target->id_ext, 8);\r\n} else {\r\nmemcpy(req->priv.initiator_port_id,\r\n&target->initiator_ext, 8);\r\nmemcpy(req->priv.initiator_port_id + 8,\r\n&target->path.sgid.global.interface_id, 8);\r\nmemcpy(req->priv.target_port_id, &target->id_ext, 8);\r\nmemcpy(req->priv.target_port_id + 8, &target->ioc_guid, 8);\r\n}\r\nif (srp_target_is_topspin(target)) {\r\nshost_printk(KERN_DEBUG, target->scsi_host,\r\nPFX "Topspin/Cisco initiator port ID workaround "\r\n"activated for target GUID %016llx\n",\r\n(unsigned long long) be64_to_cpu(target->ioc_guid));\r\nmemset(req->priv.initiator_port_id, 0, 8);\r\nmemcpy(req->priv.initiator_port_id + 8,\r\n&target->srp_host->srp_dev->dev->node_guid, 8);\r\n}\r\nstatus = ib_send_cm_req(target->cm_id, &req->param);\r\nkfree(req);\r\nreturn status;\r\n}\r\nstatic bool srp_queue_remove_work(struct srp_target_port *target)\r\n{\r\nbool changed = false;\r\nspin_lock_irq(&target->lock);\r\nif (target->state != SRP_TARGET_REMOVED) {\r\ntarget->state = SRP_TARGET_REMOVED;\r\nchanged = true;\r\n}\r\nspin_unlock_irq(&target->lock);\r\nif (changed)\r\nqueue_work(srp_remove_wq, &target->remove_work);\r\nreturn changed;\r\n}\r\nstatic bool srp_change_conn_state(struct srp_target_port *target,\r\nbool connected)\r\n{\r\nbool changed = false;\r\nspin_lock_irq(&target->lock);\r\nif (target->connected != connected) {\r\ntarget->connected = connected;\r\nchanged = true;\r\n}\r\nspin_unlock_irq(&target->lock);\r\nreturn changed;\r\n}\r\nstatic void srp_disconnect_target(struct srp_target_port *target)\r\n{\r\nif (srp_change_conn_state(target, false)) {\r\nif (ib_send_cm_dreq(target->cm_id, NULL, 0)) {\r\nshost_printk(KERN_DEBUG, target->scsi_host,\r\nPFX "Sending CM DREQ failed\n");\r\n}\r\n}\r\n}\r\nstatic void srp_free_req_data(struct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = dev->dev;\r\nstruct srp_request *req;\r\nint i;\r\nif (!target->req_ring)\r\nreturn;\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nreq = &target->req_ring[i];\r\nif (dev->use_fast_reg)\r\nkfree(req->fr_list);\r\nelse\r\nkfree(req->fmr_list);\r\nkfree(req->map_page);\r\nif (req->indirect_dma_addr) {\r\nib_dma_unmap_single(ibdev, req->indirect_dma_addr,\r\ntarget->indirect_size,\r\nDMA_TO_DEVICE);\r\n}\r\nkfree(req->indirect_desc);\r\n}\r\nkfree(target->req_ring);\r\ntarget->req_ring = NULL;\r\n}\r\nstatic int srp_alloc_req_data(struct srp_target_port *target)\r\n{\r\nstruct srp_device *srp_dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = srp_dev->dev;\r\nstruct srp_request *req;\r\nvoid *mr_list;\r\ndma_addr_t dma_addr;\r\nint i, ret = -ENOMEM;\r\nINIT_LIST_HEAD(&target->free_reqs);\r\ntarget->req_ring = kzalloc(target->req_ring_size *\r\nsizeof(*target->req_ring), GFP_KERNEL);\r\nif (!target->req_ring)\r\ngoto out;\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nreq = &target->req_ring[i];\r\nmr_list = kmalloc(target->cmd_sg_cnt * sizeof(void *),\r\nGFP_KERNEL);\r\nif (!mr_list)\r\ngoto out;\r\nif (srp_dev->use_fast_reg)\r\nreq->fr_list = mr_list;\r\nelse\r\nreq->fmr_list = mr_list;\r\nreq->map_page = kmalloc(srp_dev->max_pages_per_mr *\r\nsizeof(void *), GFP_KERNEL);\r\nif (!req->map_page)\r\ngoto out;\r\nreq->indirect_desc = kmalloc(target->indirect_size, GFP_KERNEL);\r\nif (!req->indirect_desc)\r\ngoto out;\r\ndma_addr = ib_dma_map_single(ibdev, req->indirect_desc,\r\ntarget->indirect_size,\r\nDMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(ibdev, dma_addr))\r\ngoto out;\r\nreq->indirect_dma_addr = dma_addr;\r\nreq->index = i;\r\nlist_add_tail(&req->list, &target->free_reqs);\r\n}\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic void srp_del_scsi_host_attr(struct Scsi_Host *shost)\r\n{\r\nstruct device_attribute **attr;\r\nfor (attr = shost->hostt->shost_attrs; attr && *attr; ++attr)\r\ndevice_remove_file(&shost->shost_dev, *attr);\r\n}\r\nstatic void srp_remove_target(struct srp_target_port *target)\r\n{\r\nWARN_ON_ONCE(target->state != SRP_TARGET_REMOVED);\r\nsrp_del_scsi_host_attr(target->scsi_host);\r\nsrp_rport_get(target->rport);\r\nsrp_remove_host(target->scsi_host);\r\nscsi_remove_host(target->scsi_host);\r\nsrp_stop_rport_timers(target->rport);\r\nsrp_disconnect_target(target);\r\nib_destroy_cm_id(target->cm_id);\r\nsrp_free_target_ib(target);\r\ncancel_work_sync(&target->tl_err_work);\r\nsrp_rport_put(target->rport);\r\nsrp_free_req_data(target);\r\nspin_lock(&target->srp_host->target_lock);\r\nlist_del(&target->list);\r\nspin_unlock(&target->srp_host->target_lock);\r\nscsi_host_put(target->scsi_host);\r\n}\r\nstatic void srp_remove_work(struct work_struct *work)\r\n{\r\nstruct srp_target_port *target =\r\ncontainer_of(work, struct srp_target_port, remove_work);\r\nWARN_ON_ONCE(target->state != SRP_TARGET_REMOVED);\r\nsrp_remove_target(target);\r\n}\r\nstatic void srp_rport_delete(struct srp_rport *rport)\r\n{\r\nstruct srp_target_port *target = rport->lld_data;\r\nsrp_queue_remove_work(target);\r\n}\r\nstatic int srp_connect_target(struct srp_target_port *target)\r\n{\r\nint retries = 3;\r\nint ret;\r\nWARN_ON_ONCE(target->connected);\r\ntarget->qp_in_error = false;\r\nret = srp_lookup_path(target);\r\nif (ret)\r\nreturn ret;\r\nwhile (1) {\r\ninit_completion(&target->done);\r\nret = srp_send_req(target);\r\nif (ret)\r\nreturn ret;\r\nret = wait_for_completion_interruptible(&target->done);\r\nif (ret < 0)\r\nreturn ret;\r\nswitch (target->status) {\r\ncase 0:\r\nsrp_change_conn_state(target, true);\r\nreturn 0;\r\ncase SRP_PORT_REDIRECT:\r\nret = srp_lookup_path(target);\r\nif (ret)\r\nreturn ret;\r\nbreak;\r\ncase SRP_DLID_REDIRECT:\r\nbreak;\r\ncase SRP_STALE_CONN:\r\nif (!retries-- || srp_new_cm_id(target)) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"giving up on stale connection\n");\r\ntarget->status = -ECONNRESET;\r\nreturn target->status;\r\n}\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"retrying stale connection\n");\r\nbreak;\r\ndefault:\r\nreturn target->status;\r\n}\r\n}\r\n}\r\nstatic int srp_inv_rkey(struct srp_target_port *target, u32 rkey)\r\n{\r\nstruct ib_send_wr *bad_wr;\r\nstruct ib_send_wr wr = {\r\n.opcode = IB_WR_LOCAL_INV,\r\n.wr_id = LOCAL_INV_WR_ID_MASK,\r\n.next = NULL,\r\n.num_sge = 0,\r\n.send_flags = 0,\r\n.ex.invalidate_rkey = rkey,\r\n};\r\nreturn ib_post_send(target->qp, &wr, &bad_wr);\r\n}\r\nstatic void srp_unmap_data(struct scsi_cmnd *scmnd,\r\nstruct srp_target_port *target,\r\nstruct srp_request *req)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = dev->dev;\r\nint i, res;\r\nif (!scsi_sglist(scmnd) ||\r\n(scmnd->sc_data_direction != DMA_TO_DEVICE &&\r\nscmnd->sc_data_direction != DMA_FROM_DEVICE))\r\nreturn;\r\nif (dev->use_fast_reg) {\r\nstruct srp_fr_desc **pfr;\r\nfor (i = req->nmdesc, pfr = req->fr_list; i > 0; i--, pfr++) {\r\nres = srp_inv_rkey(target, (*pfr)->mr->rkey);\r\nif (res < 0) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"Queueing INV WR for rkey %#x failed (%d)\n",\r\n(*pfr)->mr->rkey, res);\r\nqueue_work(system_long_wq,\r\n&target->tl_err_work);\r\n}\r\n}\r\nif (req->nmdesc)\r\nsrp_fr_pool_put(target->fr_pool, req->fr_list,\r\nreq->nmdesc);\r\n} else {\r\nstruct ib_pool_fmr **pfmr;\r\nfor (i = req->nmdesc, pfmr = req->fmr_list; i > 0; i--, pfmr++)\r\nib_fmr_pool_unmap(*pfmr);\r\n}\r\nib_dma_unmap_sg(ibdev, scsi_sglist(scmnd), scsi_sg_count(scmnd),\r\nscmnd->sc_data_direction);\r\n}\r\nstatic struct scsi_cmnd *srp_claim_req(struct srp_target_port *target,\r\nstruct srp_request *req,\r\nstruct scsi_device *sdev,\r\nstruct scsi_cmnd *scmnd)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&target->lock, flags);\r\nif (req->scmnd &&\r\n(!sdev || req->scmnd->device == sdev) &&\r\n(!scmnd || req->scmnd == scmnd)) {\r\nscmnd = req->scmnd;\r\nreq->scmnd = NULL;\r\n} else {\r\nscmnd = NULL;\r\n}\r\nspin_unlock_irqrestore(&target->lock, flags);\r\nreturn scmnd;\r\n}\r\nstatic void srp_free_req(struct srp_target_port *target,\r\nstruct srp_request *req, struct scsi_cmnd *scmnd,\r\ns32 req_lim_delta)\r\n{\r\nunsigned long flags;\r\nsrp_unmap_data(scmnd, target, req);\r\nspin_lock_irqsave(&target->lock, flags);\r\ntarget->req_lim += req_lim_delta;\r\nlist_add_tail(&req->list, &target->free_reqs);\r\nspin_unlock_irqrestore(&target->lock, flags);\r\n}\r\nstatic void srp_finish_req(struct srp_target_port *target,\r\nstruct srp_request *req, struct scsi_device *sdev,\r\nint result)\r\n{\r\nstruct scsi_cmnd *scmnd = srp_claim_req(target, req, sdev, NULL);\r\nif (scmnd) {\r\nsrp_free_req(target, req, scmnd, 0);\r\nscmnd->result = result;\r\nscmnd->scsi_done(scmnd);\r\n}\r\n}\r\nstatic void srp_terminate_io(struct srp_rport *rport)\r\n{\r\nstruct srp_target_port *target = rport->lld_data;\r\nstruct Scsi_Host *shost = target->scsi_host;\r\nstruct scsi_device *sdev;\r\nint i;\r\nshost_for_each_device(sdev, shost)\r\nWARN_ON_ONCE(sdev->request_queue->request_fn_active);\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nstruct srp_request *req = &target->req_ring[i];\r\nsrp_finish_req(target, req, NULL, DID_TRANSPORT_FAILFAST << 16);\r\n}\r\n}\r\nstatic int srp_rport_reconnect(struct srp_rport *rport)\r\n{\r\nstruct srp_target_port *target = rport->lld_data;\r\nint i, ret;\r\nsrp_disconnect_target(target);\r\nret = srp_new_cm_id(target);\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nstruct srp_request *req = &target->req_ring[i];\r\nsrp_finish_req(target, req, NULL, DID_RESET << 16);\r\n}\r\nret += srp_create_target_ib(target);\r\nINIT_LIST_HEAD(&target->free_tx);\r\nfor (i = 0; i < target->queue_size; ++i)\r\nlist_add(&target->tx_ring[i]->list, &target->free_tx);\r\nif (ret == 0)\r\nret = srp_connect_target(target);\r\nif (ret == 0)\r\nshost_printk(KERN_INFO, target->scsi_host,\r\nPFX "reconnect succeeded\n");\r\nreturn ret;\r\n}\r\nstatic void srp_map_desc(struct srp_map_state *state, dma_addr_t dma_addr,\r\nunsigned int dma_len, u32 rkey)\r\n{\r\nstruct srp_direct_buf *desc = state->desc;\r\ndesc->va = cpu_to_be64(dma_addr);\r\ndesc->key = cpu_to_be32(rkey);\r\ndesc->len = cpu_to_be32(dma_len);\r\nstate->total_len += dma_len;\r\nstate->desc++;\r\nstate->ndesc++;\r\n}\r\nstatic int srp_map_finish_fmr(struct srp_map_state *state,\r\nstruct srp_target_port *target)\r\n{\r\nstruct ib_pool_fmr *fmr;\r\nu64 io_addr = 0;\r\nfmr = ib_fmr_pool_map_phys(target->fmr_pool, state->pages,\r\nstate->npages, io_addr);\r\nif (IS_ERR(fmr))\r\nreturn PTR_ERR(fmr);\r\n*state->next_fmr++ = fmr;\r\nstate->nmdesc++;\r\nsrp_map_desc(state, 0, state->dma_len, fmr->fmr->rkey);\r\nreturn 0;\r\n}\r\nstatic int srp_map_finish_fr(struct srp_map_state *state,\r\nstruct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_send_wr *bad_wr;\r\nstruct ib_send_wr wr;\r\nstruct srp_fr_desc *desc;\r\nu32 rkey;\r\ndesc = srp_fr_pool_get(target->fr_pool);\r\nif (!desc)\r\nreturn -ENOMEM;\r\nrkey = ib_inc_rkey(desc->mr->rkey);\r\nib_update_fast_reg_key(desc->mr, rkey);\r\nmemcpy(desc->frpl->page_list, state->pages,\r\nsizeof(state->pages[0]) * state->npages);\r\nmemset(&wr, 0, sizeof(wr));\r\nwr.opcode = IB_WR_FAST_REG_MR;\r\nwr.wr_id = FAST_REG_WR_ID_MASK;\r\nwr.wr.fast_reg.iova_start = state->base_dma_addr;\r\nwr.wr.fast_reg.page_list = desc->frpl;\r\nwr.wr.fast_reg.page_list_len = state->npages;\r\nwr.wr.fast_reg.page_shift = ilog2(dev->mr_page_size);\r\nwr.wr.fast_reg.length = state->dma_len;\r\nwr.wr.fast_reg.access_flags = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE);\r\nwr.wr.fast_reg.rkey = desc->mr->lkey;\r\n*state->next_fr++ = desc;\r\nstate->nmdesc++;\r\nsrp_map_desc(state, state->base_dma_addr, state->dma_len,\r\ndesc->mr->rkey);\r\nreturn ib_post_send(target->qp, &wr, &bad_wr);\r\n}\r\nstatic int srp_finish_mapping(struct srp_map_state *state,\r\nstruct srp_target_port *target)\r\n{\r\nint ret = 0;\r\nif (state->npages == 0)\r\nreturn 0;\r\nif (state->npages == 1 && !register_always)\r\nsrp_map_desc(state, state->base_dma_addr, state->dma_len,\r\ntarget->rkey);\r\nelse\r\nret = target->srp_host->srp_dev->use_fast_reg ?\r\nsrp_map_finish_fr(state, target) :\r\nsrp_map_finish_fmr(state, target);\r\nif (ret == 0) {\r\nstate->npages = 0;\r\nstate->dma_len = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic void srp_map_update_start(struct srp_map_state *state,\r\nstruct scatterlist *sg, int sg_index,\r\ndma_addr_t dma_addr)\r\n{\r\nstate->unmapped_sg = sg;\r\nstate->unmapped_index = sg_index;\r\nstate->unmapped_addr = dma_addr;\r\n}\r\nstatic int srp_map_sg_entry(struct srp_map_state *state,\r\nstruct srp_target_port *target,\r\nstruct scatterlist *sg, int sg_index,\r\nbool use_mr)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = dev->dev;\r\ndma_addr_t dma_addr = ib_sg_dma_address(ibdev, sg);\r\nunsigned int dma_len = ib_sg_dma_len(ibdev, sg);\r\nunsigned int len;\r\nint ret;\r\nif (!dma_len)\r\nreturn 0;\r\nif (!use_mr) {\r\nsrp_map_desc(state, dma_addr, dma_len, target->rkey);\r\nreturn 0;\r\n}\r\nif ((!dev->use_fast_reg && dma_addr & ~dev->mr_page_mask) ||\r\ndma_len > dev->mr_max_size) {\r\nret = srp_finish_mapping(state, target);\r\nif (ret)\r\nreturn ret;\r\nsrp_map_desc(state, dma_addr, dma_len, target->rkey);\r\nsrp_map_update_start(state, NULL, 0, 0);\r\nreturn 0;\r\n}\r\nif (!state->unmapped_sg)\r\nsrp_map_update_start(state, sg, sg_index, dma_addr);\r\nwhile (dma_len) {\r\nunsigned offset = dma_addr & ~dev->mr_page_mask;\r\nif (state->npages == dev->max_pages_per_mr || offset != 0) {\r\nret = srp_finish_mapping(state, target);\r\nif (ret)\r\nreturn ret;\r\nsrp_map_update_start(state, sg, sg_index, dma_addr);\r\n}\r\nlen = min_t(unsigned int, dma_len, dev->mr_page_size - offset);\r\nif (!state->npages)\r\nstate->base_dma_addr = dma_addr;\r\nstate->pages[state->npages++] = dma_addr & dev->mr_page_mask;\r\nstate->dma_len += len;\r\ndma_addr += len;\r\ndma_len -= len;\r\n}\r\nret = 0;\r\nif (len != dev->mr_page_size) {\r\nret = srp_finish_mapping(state, target);\r\nif (!ret)\r\nsrp_map_update_start(state, NULL, 0, 0);\r\n}\r\nreturn ret;\r\n}\r\nstatic int srp_map_sg(struct srp_map_state *state,\r\nstruct srp_target_port *target, struct srp_request *req,\r\nstruct scatterlist *scat, int count)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = dev->dev;\r\nstruct scatterlist *sg;\r\nint i;\r\nbool use_mr;\r\nstate->desc = req->indirect_desc;\r\nstate->pages = req->map_page;\r\nif (dev->use_fast_reg) {\r\nstate->next_fr = req->fr_list;\r\nuse_mr = !!target->fr_pool;\r\n} else {\r\nstate->next_fmr = req->fmr_list;\r\nuse_mr = !!target->fmr_pool;\r\n}\r\nfor_each_sg(scat, sg, count, i) {\r\nif (srp_map_sg_entry(state, target, sg, i, use_mr)) {\r\ndma_addr_t dma_addr;\r\nunsigned int dma_len;\r\nbacktrack:\r\nsg = state->unmapped_sg;\r\ni = state->unmapped_index;\r\ndma_addr = ib_sg_dma_address(ibdev, sg);\r\ndma_len = ib_sg_dma_len(ibdev, sg);\r\ndma_len -= (state->unmapped_addr - dma_addr);\r\ndma_addr = state->unmapped_addr;\r\nuse_mr = false;\r\nsrp_map_desc(state, dma_addr, dma_len, target->rkey);\r\n}\r\n}\r\nif (use_mr && srp_finish_mapping(state, target))\r\ngoto backtrack;\r\nreq->nmdesc = state->nmdesc;\r\nreturn 0;\r\n}\r\nstatic int srp_map_data(struct scsi_cmnd *scmnd, struct srp_target_port *target,\r\nstruct srp_request *req)\r\n{\r\nstruct scatterlist *scat;\r\nstruct srp_cmd *cmd = req->cmd->buf;\r\nint len, nents, count;\r\nstruct srp_device *dev;\r\nstruct ib_device *ibdev;\r\nstruct srp_map_state state;\r\nstruct srp_indirect_buf *indirect_hdr;\r\nu32 table_len;\r\nu8 fmt;\r\nif (!scsi_sglist(scmnd) || scmnd->sc_data_direction == DMA_NONE)\r\nreturn sizeof (struct srp_cmd);\r\nif (scmnd->sc_data_direction != DMA_FROM_DEVICE &&\r\nscmnd->sc_data_direction != DMA_TO_DEVICE) {\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled data direction %d\n",\r\nscmnd->sc_data_direction);\r\nreturn -EINVAL;\r\n}\r\nnents = scsi_sg_count(scmnd);\r\nscat = scsi_sglist(scmnd);\r\ndev = target->srp_host->srp_dev;\r\nibdev = dev->dev;\r\ncount = ib_dma_map_sg(ibdev, scat, nents, scmnd->sc_data_direction);\r\nif (unlikely(count == 0))\r\nreturn -EIO;\r\nfmt = SRP_DATA_DESC_DIRECT;\r\nlen = sizeof (struct srp_cmd) + sizeof (struct srp_direct_buf);\r\nif (count == 1 && !register_always) {\r\nstruct srp_direct_buf *buf = (void *) cmd->add_data;\r\nbuf->va = cpu_to_be64(ib_sg_dma_address(ibdev, scat));\r\nbuf->key = cpu_to_be32(target->rkey);\r\nbuf->len = cpu_to_be32(ib_sg_dma_len(ibdev, scat));\r\nreq->nmdesc = 0;\r\ngoto map_complete;\r\n}\r\nindirect_hdr = (void *) cmd->add_data;\r\nib_dma_sync_single_for_cpu(ibdev, req->indirect_dma_addr,\r\ntarget->indirect_size, DMA_TO_DEVICE);\r\nmemset(&state, 0, sizeof(state));\r\nsrp_map_sg(&state, target, req, scat, count);\r\nif (state.ndesc == 1) {\r\nstruct srp_direct_buf *buf = (void *) cmd->add_data;\r\n*buf = req->indirect_desc[0];\r\ngoto map_complete;\r\n}\r\nif (unlikely(target->cmd_sg_cnt < state.ndesc &&\r\n!target->allow_ext_sg)) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\n"Could not fit S/G list into SRP_CMD\n");\r\nreturn -EIO;\r\n}\r\ncount = min(state.ndesc, target->cmd_sg_cnt);\r\ntable_len = state.ndesc * sizeof (struct srp_direct_buf);\r\nfmt = SRP_DATA_DESC_INDIRECT;\r\nlen = sizeof(struct srp_cmd) + sizeof (struct srp_indirect_buf);\r\nlen += count * sizeof (struct srp_direct_buf);\r\nmemcpy(indirect_hdr->desc_list, req->indirect_desc,\r\ncount * sizeof (struct srp_direct_buf));\r\nindirect_hdr->table_desc.va = cpu_to_be64(req->indirect_dma_addr);\r\nindirect_hdr->table_desc.key = cpu_to_be32(target->rkey);\r\nindirect_hdr->table_desc.len = cpu_to_be32(table_len);\r\nindirect_hdr->len = cpu_to_be32(state.total_len);\r\nif (scmnd->sc_data_direction == DMA_TO_DEVICE)\r\ncmd->data_out_desc_cnt = count;\r\nelse\r\ncmd->data_in_desc_cnt = count;\r\nib_dma_sync_single_for_device(ibdev, req->indirect_dma_addr, table_len,\r\nDMA_TO_DEVICE);\r\nmap_complete:\r\nif (scmnd->sc_data_direction == DMA_TO_DEVICE)\r\ncmd->buf_fmt = fmt << 4;\r\nelse\r\ncmd->buf_fmt = fmt;\r\nreturn len;\r\n}\r\nstatic void srp_put_tx_iu(struct srp_target_port *target, struct srp_iu *iu,\r\nenum srp_iu_type iu_type)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&target->lock, flags);\r\nlist_add(&iu->list, &target->free_tx);\r\nif (iu_type != SRP_IU_RSP)\r\n++target->req_lim;\r\nspin_unlock_irqrestore(&target->lock, flags);\r\n}\r\nstatic struct srp_iu *__srp_get_tx_iu(struct srp_target_port *target,\r\nenum srp_iu_type iu_type)\r\n{\r\ns32 rsv = (iu_type == SRP_IU_TSK_MGMT) ? 0 : SRP_TSK_MGMT_SQ_SIZE;\r\nstruct srp_iu *iu;\r\nsrp_send_completion(target->send_cq, target);\r\nif (list_empty(&target->free_tx))\r\nreturn NULL;\r\nif (iu_type != SRP_IU_RSP) {\r\nif (target->req_lim <= rsv) {\r\n++target->zero_req_lim;\r\nreturn NULL;\r\n}\r\n--target->req_lim;\r\n}\r\niu = list_first_entry(&target->free_tx, struct srp_iu, list);\r\nlist_del(&iu->list);\r\nreturn iu;\r\n}\r\nstatic int srp_post_send(struct srp_target_port *target,\r\nstruct srp_iu *iu, int len)\r\n{\r\nstruct ib_sge list;\r\nstruct ib_send_wr wr, *bad_wr;\r\nlist.addr = iu->dma;\r\nlist.length = len;\r\nlist.lkey = target->lkey;\r\nwr.next = NULL;\r\nwr.wr_id = (uintptr_t) iu;\r\nwr.sg_list = &list;\r\nwr.num_sge = 1;\r\nwr.opcode = IB_WR_SEND;\r\nwr.send_flags = IB_SEND_SIGNALED;\r\nreturn ib_post_send(target->qp, &wr, &bad_wr);\r\n}\r\nstatic int srp_post_recv(struct srp_target_port *target, struct srp_iu *iu)\r\n{\r\nstruct ib_recv_wr wr, *bad_wr;\r\nstruct ib_sge list;\r\nlist.addr = iu->dma;\r\nlist.length = iu->size;\r\nlist.lkey = target->lkey;\r\nwr.next = NULL;\r\nwr.wr_id = (uintptr_t) iu;\r\nwr.sg_list = &list;\r\nwr.num_sge = 1;\r\nreturn ib_post_recv(target->qp, &wr, &bad_wr);\r\n}\r\nstatic void srp_process_rsp(struct srp_target_port *target, struct srp_rsp *rsp)\r\n{\r\nstruct srp_request *req;\r\nstruct scsi_cmnd *scmnd;\r\nunsigned long flags;\r\nif (unlikely(rsp->tag & SRP_TAG_TSK_MGMT)) {\r\nspin_lock_irqsave(&target->lock, flags);\r\ntarget->req_lim += be32_to_cpu(rsp->req_lim_delta);\r\nspin_unlock_irqrestore(&target->lock, flags);\r\ntarget->tsk_mgmt_status = -1;\r\nif (be32_to_cpu(rsp->resp_data_len) >= 4)\r\ntarget->tsk_mgmt_status = rsp->data[3];\r\ncomplete(&target->tsk_mgmt_done);\r\n} else {\r\nreq = &target->req_ring[rsp->tag];\r\nscmnd = srp_claim_req(target, req, NULL, NULL);\r\nif (!scmnd) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\n"Null scmnd for RSP w/tag %016llx\n",\r\n(unsigned long long) rsp->tag);\r\nspin_lock_irqsave(&target->lock, flags);\r\ntarget->req_lim += be32_to_cpu(rsp->req_lim_delta);\r\nspin_unlock_irqrestore(&target->lock, flags);\r\nreturn;\r\n}\r\nscmnd->result = rsp->status;\r\nif (rsp->flags & SRP_RSP_FLAG_SNSVALID) {\r\nmemcpy(scmnd->sense_buffer, rsp->data +\r\nbe32_to_cpu(rsp->resp_data_len),\r\nmin_t(int, be32_to_cpu(rsp->sense_data_len),\r\nSCSI_SENSE_BUFFERSIZE));\r\n}\r\nif (unlikely(rsp->flags & SRP_RSP_FLAG_DIUNDER))\r\nscsi_set_resid(scmnd, be32_to_cpu(rsp->data_in_res_cnt));\r\nelse if (unlikely(rsp->flags & SRP_RSP_FLAG_DIOVER))\r\nscsi_set_resid(scmnd, -be32_to_cpu(rsp->data_in_res_cnt));\r\nelse if (unlikely(rsp->flags & SRP_RSP_FLAG_DOUNDER))\r\nscsi_set_resid(scmnd, be32_to_cpu(rsp->data_out_res_cnt));\r\nelse if (unlikely(rsp->flags & SRP_RSP_FLAG_DOOVER))\r\nscsi_set_resid(scmnd, -be32_to_cpu(rsp->data_out_res_cnt));\r\nsrp_free_req(target, req, scmnd,\r\nbe32_to_cpu(rsp->req_lim_delta));\r\nscmnd->host_scribble = NULL;\r\nscmnd->scsi_done(scmnd);\r\n}\r\n}\r\nstatic int srp_response_common(struct srp_target_port *target, s32 req_delta,\r\nvoid *rsp, int len)\r\n{\r\nstruct ib_device *dev = target->srp_host->srp_dev->dev;\r\nunsigned long flags;\r\nstruct srp_iu *iu;\r\nint err;\r\nspin_lock_irqsave(&target->lock, flags);\r\ntarget->req_lim += req_delta;\r\niu = __srp_get_tx_iu(target, SRP_IU_RSP);\r\nspin_unlock_irqrestore(&target->lock, flags);\r\nif (!iu) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"no IU available to send response\n");\r\nreturn 1;\r\n}\r\nib_dma_sync_single_for_cpu(dev, iu->dma, len, DMA_TO_DEVICE);\r\nmemcpy(iu->buf, rsp, len);\r\nib_dma_sync_single_for_device(dev, iu->dma, len, DMA_TO_DEVICE);\r\nerr = srp_post_send(target, iu, len);\r\nif (err) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"unable to post response: %d\n", err);\r\nsrp_put_tx_iu(target, iu, SRP_IU_RSP);\r\n}\r\nreturn err;\r\n}\r\nstatic void srp_process_cred_req(struct srp_target_port *target,\r\nstruct srp_cred_req *req)\r\n{\r\nstruct srp_cred_rsp rsp = {\r\n.opcode = SRP_CRED_RSP,\r\n.tag = req->tag,\r\n};\r\ns32 delta = be32_to_cpu(req->req_lim_delta);\r\nif (srp_response_common(target, delta, &rsp, sizeof rsp))\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"problems processing SRP_CRED_REQ\n");\r\n}\r\nstatic void srp_process_aer_req(struct srp_target_port *target,\r\nstruct srp_aer_req *req)\r\n{\r\nstruct srp_aer_rsp rsp = {\r\n.opcode = SRP_AER_RSP,\r\n.tag = req->tag,\r\n};\r\ns32 delta = be32_to_cpu(req->req_lim_delta);\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"ignoring AER for LUN %llu\n", be64_to_cpu(req->lun));\r\nif (srp_response_common(target, delta, &rsp, sizeof rsp))\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"problems processing SRP_AER_REQ\n");\r\n}\r\nstatic void srp_handle_recv(struct srp_target_port *target, struct ib_wc *wc)\r\n{\r\nstruct ib_device *dev = target->srp_host->srp_dev->dev;\r\nstruct srp_iu *iu = (struct srp_iu *) (uintptr_t) wc->wr_id;\r\nint res;\r\nu8 opcode;\r\nib_dma_sync_single_for_cpu(dev, iu->dma, target->max_ti_iu_len,\r\nDMA_FROM_DEVICE);\r\nopcode = *(u8 *) iu->buf;\r\nif (0) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "recv completion, opcode 0x%02x\n", opcode);\r\nprint_hex_dump(KERN_ERR, "", DUMP_PREFIX_OFFSET, 8, 1,\r\niu->buf, wc->byte_len, true);\r\n}\r\nswitch (opcode) {\r\ncase SRP_RSP:\r\nsrp_process_rsp(target, iu->buf);\r\nbreak;\r\ncase SRP_CRED_REQ:\r\nsrp_process_cred_req(target, iu->buf);\r\nbreak;\r\ncase SRP_AER_REQ:\r\nsrp_process_aer_req(target, iu->buf);\r\nbreak;\r\ncase SRP_T_LOGOUT:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Got target logout request\n");\r\nbreak;\r\ndefault:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled SRP opcode 0x%02x\n", opcode);\r\nbreak;\r\n}\r\nib_dma_sync_single_for_device(dev, iu->dma, target->max_ti_iu_len,\r\nDMA_FROM_DEVICE);\r\nres = srp_post_recv(target, iu);\r\nif (res != 0)\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Recv failed with error code %d\n", res);\r\n}\r\nstatic void srp_tl_err_work(struct work_struct *work)\r\n{\r\nstruct srp_target_port *target;\r\ntarget = container_of(work, struct srp_target_port, tl_err_work);\r\nif (target->rport)\r\nsrp_start_tl_fail_timers(target->rport);\r\n}\r\nstatic void srp_handle_qp_err(u64 wr_id, enum ib_wc_status wc_status,\r\nbool send_err, struct srp_target_port *target)\r\n{\r\nif (target->connected && !target->qp_in_error) {\r\nif (wr_id & LOCAL_INV_WR_ID_MASK) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"LOCAL_INV failed with status %d\n",\r\nwc_status);\r\n} else if (wr_id & FAST_REG_WR_ID_MASK) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"FAST_REG_MR failed status %d\n",\r\nwc_status);\r\n} else {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "failed %s status %d for iu %p\n",\r\nsend_err ? "send" : "receive",\r\nwc_status, (void *)(uintptr_t)wr_id);\r\n}\r\nqueue_work(system_long_wq, &target->tl_err_work);\r\n}\r\ntarget->qp_in_error = true;\r\n}\r\nstatic void srp_recv_completion(struct ib_cq *cq, void *target_ptr)\r\n{\r\nstruct srp_target_port *target = target_ptr;\r\nstruct ib_wc wc;\r\nib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\r\nwhile (ib_poll_cq(cq, 1, &wc) > 0) {\r\nif (likely(wc.status == IB_WC_SUCCESS)) {\r\nsrp_handle_recv(target, &wc);\r\n} else {\r\nsrp_handle_qp_err(wc.wr_id, wc.status, false, target);\r\n}\r\n}\r\n}\r\nstatic void srp_send_completion(struct ib_cq *cq, void *target_ptr)\r\n{\r\nstruct srp_target_port *target = target_ptr;\r\nstruct ib_wc wc;\r\nstruct srp_iu *iu;\r\nwhile (ib_poll_cq(cq, 1, &wc) > 0) {\r\nif (likely(wc.status == IB_WC_SUCCESS)) {\r\niu = (struct srp_iu *) (uintptr_t) wc.wr_id;\r\nlist_add(&iu->list, &target->free_tx);\r\n} else {\r\nsrp_handle_qp_err(wc.wr_id, wc.status, true, target);\r\n}\r\n}\r\n}\r\nstatic int srp_queuecommand(struct Scsi_Host *shost, struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(shost);\r\nstruct srp_rport *rport = target->rport;\r\nstruct srp_request *req;\r\nstruct srp_iu *iu;\r\nstruct srp_cmd *cmd;\r\nstruct ib_device *dev;\r\nunsigned long flags;\r\nint len, ret;\r\nconst bool in_scsi_eh = !in_interrupt() && current == shost->ehandler;\r\nif (in_scsi_eh)\r\nmutex_lock(&rport->mutex);\r\nscmnd->result = srp_chkready(target->rport);\r\nif (unlikely(scmnd->result))\r\ngoto err;\r\nspin_lock_irqsave(&target->lock, flags);\r\niu = __srp_get_tx_iu(target, SRP_IU_CMD);\r\nif (!iu)\r\ngoto err_unlock;\r\nreq = list_first_entry(&target->free_reqs, struct srp_request, list);\r\nlist_del(&req->list);\r\nspin_unlock_irqrestore(&target->lock, flags);\r\ndev = target->srp_host->srp_dev->dev;\r\nib_dma_sync_single_for_cpu(dev, iu->dma, target->max_iu_len,\r\nDMA_TO_DEVICE);\r\nscmnd->host_scribble = (void *) req;\r\ncmd = iu->buf;\r\nmemset(cmd, 0, sizeof *cmd);\r\ncmd->opcode = SRP_CMD;\r\ncmd->lun = cpu_to_be64((u64) scmnd->device->lun << 48);\r\ncmd->tag = req->index;\r\nmemcpy(cmd->cdb, scmnd->cmnd, scmnd->cmd_len);\r\nreq->scmnd = scmnd;\r\nreq->cmd = iu;\r\nlen = srp_map_data(scmnd, target, req);\r\nif (len < 0) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Failed to map data (%d)\n", len);\r\nscmnd->result = len == -ENOMEM ?\r\nDID_OK << 16 | QUEUE_FULL << 1 : DID_ERROR << 16;\r\ngoto err_iu;\r\n}\r\nib_dma_sync_single_for_device(dev, iu->dma, target->max_iu_len,\r\nDMA_TO_DEVICE);\r\nif (srp_post_send(target, iu, len)) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX "Send failed\n");\r\ngoto err_unmap;\r\n}\r\nret = 0;\r\nunlock_rport:\r\nif (in_scsi_eh)\r\nmutex_unlock(&rport->mutex);\r\nreturn ret;\r\nerr_unmap:\r\nsrp_unmap_data(scmnd, target, req);\r\nerr_iu:\r\nsrp_put_tx_iu(target, iu, SRP_IU_CMD);\r\nreq->scmnd = NULL;\r\nspin_lock_irqsave(&target->lock, flags);\r\nlist_add(&req->list, &target->free_reqs);\r\nerr_unlock:\r\nspin_unlock_irqrestore(&target->lock, flags);\r\nerr:\r\nif (scmnd->result) {\r\nscmnd->scsi_done(scmnd);\r\nret = 0;\r\n} else {\r\nret = SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\ngoto unlock_rport;\r\n}\r\nstatic int srp_alloc_iu_bufs(struct srp_target_port *target)\r\n{\r\nint i;\r\ntarget->rx_ring = kzalloc(target->queue_size * sizeof(*target->rx_ring),\r\nGFP_KERNEL);\r\nif (!target->rx_ring)\r\ngoto err_no_ring;\r\ntarget->tx_ring = kzalloc(target->queue_size * sizeof(*target->tx_ring),\r\nGFP_KERNEL);\r\nif (!target->tx_ring)\r\ngoto err_no_ring;\r\nfor (i = 0; i < target->queue_size; ++i) {\r\ntarget->rx_ring[i] = srp_alloc_iu(target->srp_host,\r\ntarget->max_ti_iu_len,\r\nGFP_KERNEL, DMA_FROM_DEVICE);\r\nif (!target->rx_ring[i])\r\ngoto err;\r\n}\r\nfor (i = 0; i < target->queue_size; ++i) {\r\ntarget->tx_ring[i] = srp_alloc_iu(target->srp_host,\r\ntarget->max_iu_len,\r\nGFP_KERNEL, DMA_TO_DEVICE);\r\nif (!target->tx_ring[i])\r\ngoto err;\r\nlist_add(&target->tx_ring[i]->list, &target->free_tx);\r\n}\r\nreturn 0;\r\nerr:\r\nfor (i = 0; i < target->queue_size; ++i) {\r\nsrp_free_iu(target->srp_host, target->rx_ring[i]);\r\nsrp_free_iu(target->srp_host, target->tx_ring[i]);\r\n}\r\nerr_no_ring:\r\nkfree(target->tx_ring);\r\ntarget->tx_ring = NULL;\r\nkfree(target->rx_ring);\r\ntarget->rx_ring = NULL;\r\nreturn -ENOMEM;\r\n}\r\nstatic uint32_t srp_compute_rq_tmo(struct ib_qp_attr *qp_attr, int attr_mask)\r\n{\r\nuint64_t T_tr_ns, max_compl_time_ms;\r\nuint32_t rq_tmo_jiffies;\r\nWARN_ON_ONCE((attr_mask & (IB_QP_TIMEOUT | IB_QP_RETRY_CNT)) !=\r\n(IB_QP_TIMEOUT | IB_QP_RETRY_CNT));\r\nT_tr_ns = 4096 * (1ULL << qp_attr->timeout);\r\nmax_compl_time_ms = qp_attr->retry_cnt * 4 * T_tr_ns;\r\ndo_div(max_compl_time_ms, NSEC_PER_MSEC);\r\nrq_tmo_jiffies = msecs_to_jiffies(max_compl_time_ms + 1000);\r\nreturn rq_tmo_jiffies;\r\n}\r\nstatic void srp_cm_rep_handler(struct ib_cm_id *cm_id,\r\nstruct srp_login_rsp *lrsp,\r\nstruct srp_target_port *target)\r\n{\r\nstruct ib_qp_attr *qp_attr = NULL;\r\nint attr_mask = 0;\r\nint ret;\r\nint i;\r\nif (lrsp->opcode == SRP_LOGIN_RSP) {\r\ntarget->max_ti_iu_len = be32_to_cpu(lrsp->max_ti_iu_len);\r\ntarget->req_lim = be32_to_cpu(lrsp->req_lim_delta);\r\ntarget->scsi_host->can_queue\r\n= min(target->req_lim - SRP_TSK_MGMT_SQ_SIZE,\r\ntarget->scsi_host->can_queue);\r\ntarget->scsi_host->cmd_per_lun\r\n= min_t(int, target->scsi_host->can_queue,\r\ntarget->scsi_host->cmd_per_lun);\r\n} else {\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled RSP opcode %#x\n", lrsp->opcode);\r\nret = -ECONNRESET;\r\ngoto error;\r\n}\r\nif (!target->rx_ring) {\r\nret = srp_alloc_iu_bufs(target);\r\nif (ret)\r\ngoto error;\r\n}\r\nret = -ENOMEM;\r\nqp_attr = kmalloc(sizeof *qp_attr, GFP_KERNEL);\r\nif (!qp_attr)\r\ngoto error;\r\nqp_attr->qp_state = IB_QPS_RTR;\r\nret = ib_cm_init_qp_attr(cm_id, qp_attr, &attr_mask);\r\nif (ret)\r\ngoto error_free;\r\nret = ib_modify_qp(target->qp, qp_attr, attr_mask);\r\nif (ret)\r\ngoto error_free;\r\nfor (i = 0; i < target->queue_size; i++) {\r\nstruct srp_iu *iu = target->rx_ring[i];\r\nret = srp_post_recv(target, iu);\r\nif (ret)\r\ngoto error_free;\r\n}\r\nqp_attr->qp_state = IB_QPS_RTS;\r\nret = ib_cm_init_qp_attr(cm_id, qp_attr, &attr_mask);\r\nif (ret)\r\ngoto error_free;\r\ntarget->rq_tmo_jiffies = srp_compute_rq_tmo(qp_attr, attr_mask);\r\nret = ib_modify_qp(target->qp, qp_attr, attr_mask);\r\nif (ret)\r\ngoto error_free;\r\nret = ib_send_cm_rtu(cm_id, NULL, 0);\r\nerror_free:\r\nkfree(qp_attr);\r\nerror:\r\ntarget->status = ret;\r\n}\r\nstatic void srp_cm_rej_handler(struct ib_cm_id *cm_id,\r\nstruct ib_cm_event *event,\r\nstruct srp_target_port *target)\r\n{\r\nstruct Scsi_Host *shost = target->scsi_host;\r\nstruct ib_class_port_info *cpi;\r\nint opcode;\r\nswitch (event->param.rej_rcvd.reason) {\r\ncase IB_CM_REJ_PORT_CM_REDIRECT:\r\ncpi = event->param.rej_rcvd.ari;\r\ntarget->path.dlid = cpi->redirect_lid;\r\ntarget->path.pkey = cpi->redirect_pkey;\r\ncm_id->remote_cm_qpn = be32_to_cpu(cpi->redirect_qp) & 0x00ffffff;\r\nmemcpy(target->path.dgid.raw, cpi->redirect_gid, 16);\r\ntarget->status = target->path.dlid ?\r\nSRP_DLID_REDIRECT : SRP_PORT_REDIRECT;\r\nbreak;\r\ncase IB_CM_REJ_PORT_REDIRECT:\r\nif (srp_target_is_topspin(target)) {\r\nmemcpy(target->path.dgid.raw,\r\nevent->param.rej_rcvd.ari, 16);\r\nshost_printk(KERN_DEBUG, shost,\r\nPFX "Topspin/Cisco redirect to target port GID %016llx%016llx\n",\r\n(unsigned long long) be64_to_cpu(target->path.dgid.global.subnet_prefix),\r\n(unsigned long long) be64_to_cpu(target->path.dgid.global.interface_id));\r\ntarget->status = SRP_PORT_REDIRECT;\r\n} else {\r\nshost_printk(KERN_WARNING, shost,\r\n" REJ reason: IB_CM_REJ_PORT_REDIRECT\n");\r\ntarget->status = -ECONNRESET;\r\n}\r\nbreak;\r\ncase IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID:\r\nshost_printk(KERN_WARNING, shost,\r\n" REJ reason: IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID\n");\r\ntarget->status = -ECONNRESET;\r\nbreak;\r\ncase IB_CM_REJ_CONSUMER_DEFINED:\r\nopcode = *(u8 *) event->private_data;\r\nif (opcode == SRP_LOGIN_REJ) {\r\nstruct srp_login_rej *rej = event->private_data;\r\nu32 reason = be32_to_cpu(rej->reason);\r\nif (reason == SRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE)\r\nshost_printk(KERN_WARNING, shost,\r\nPFX "SRP_LOGIN_REJ: requested max_it_iu_len too large\n");\r\nelse\r\nshost_printk(KERN_WARNING, shost, PFX\r\n"SRP LOGIN from %pI6 to %pI6 REJECTED, reason 0x%08x\n",\r\ntarget->path.sgid.raw,\r\ntarget->orig_dgid, reason);\r\n} else\r\nshost_printk(KERN_WARNING, shost,\r\n" REJ reason: IB_CM_REJ_CONSUMER_DEFINED,"\r\n" opcode 0x%02x\n", opcode);\r\ntarget->status = -ECONNRESET;\r\nbreak;\r\ncase IB_CM_REJ_STALE_CONN:\r\nshost_printk(KERN_WARNING, shost, " REJ reason: stale connection\n");\r\ntarget->status = SRP_STALE_CONN;\r\nbreak;\r\ndefault:\r\nshost_printk(KERN_WARNING, shost, " REJ reason 0x%x\n",\r\nevent->param.rej_rcvd.reason);\r\ntarget->status = -ECONNRESET;\r\n}\r\n}\r\nstatic int srp_cm_handler(struct ib_cm_id *cm_id, struct ib_cm_event *event)\r\n{\r\nstruct srp_target_port *target = cm_id->context;\r\nint comp = 0;\r\nswitch (event->event) {\r\ncase IB_CM_REQ_ERROR:\r\nshost_printk(KERN_DEBUG, target->scsi_host,\r\nPFX "Sending CM REQ failed\n");\r\ncomp = 1;\r\ntarget->status = -ECONNRESET;\r\nbreak;\r\ncase IB_CM_REP_RECEIVED:\r\ncomp = 1;\r\nsrp_cm_rep_handler(cm_id, event->private_data, target);\r\nbreak;\r\ncase IB_CM_REJ_RECEIVED:\r\nshost_printk(KERN_DEBUG, target->scsi_host, PFX "REJ received\n");\r\ncomp = 1;\r\nsrp_cm_rej_handler(cm_id, event, target);\r\nbreak;\r\ncase IB_CM_DREQ_RECEIVED:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "DREQ received - connection closed\n");\r\nsrp_change_conn_state(target, false);\r\nif (ib_send_cm_drep(cm_id, NULL, 0))\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Sending CM DREP failed\n");\r\nqueue_work(system_long_wq, &target->tl_err_work);\r\nbreak;\r\ncase IB_CM_TIMEWAIT_EXIT:\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "connection closed\n");\r\ncomp = 1;\r\ntarget->status = 0;\r\nbreak;\r\ncase IB_CM_MRA_RECEIVED:\r\ncase IB_CM_DREQ_ERROR:\r\ncase IB_CM_DREP_RECEIVED:\r\nbreak;\r\ndefault:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled CM event %d\n", event->event);\r\nbreak;\r\n}\r\nif (comp)\r\ncomplete(&target->done);\r\nreturn 0;\r\n}\r\nstatic int\r\nsrp_change_queue_type(struct scsi_device *sdev, int tag_type)\r\n{\r\nif (sdev->tagged_supported) {\r\nscsi_set_tag_type(sdev, tag_type);\r\nif (tag_type)\r\nscsi_activate_tcq(sdev, sdev->queue_depth);\r\nelse\r\nscsi_deactivate_tcq(sdev, sdev->queue_depth);\r\n} else\r\ntag_type = 0;\r\nreturn tag_type;\r\n}\r\nstatic int\r\nsrp_change_queue_depth(struct scsi_device *sdev, int qdepth, int reason)\r\n{\r\nstruct Scsi_Host *shost = sdev->host;\r\nint max_depth;\r\nif (reason == SCSI_QDEPTH_DEFAULT || reason == SCSI_QDEPTH_RAMP_UP) {\r\nmax_depth = shost->can_queue;\r\nif (!sdev->tagged_supported)\r\nmax_depth = 1;\r\nif (qdepth > max_depth)\r\nqdepth = max_depth;\r\nscsi_adjust_queue_depth(sdev, scsi_get_tag_type(sdev), qdepth);\r\n} else if (reason == SCSI_QDEPTH_QFULL)\r\nscsi_track_queue_full(sdev, qdepth);\r\nelse\r\nreturn -EOPNOTSUPP;\r\nreturn sdev->queue_depth;\r\n}\r\nstatic int srp_send_tsk_mgmt(struct srp_target_port *target,\r\nu64 req_tag, unsigned int lun, u8 func)\r\n{\r\nstruct srp_rport *rport = target->rport;\r\nstruct ib_device *dev = target->srp_host->srp_dev->dev;\r\nstruct srp_iu *iu;\r\nstruct srp_tsk_mgmt *tsk_mgmt;\r\nif (!target->connected || target->qp_in_error)\r\nreturn -1;\r\ninit_completion(&target->tsk_mgmt_done);\r\nmutex_lock(&rport->mutex);\r\nspin_lock_irq(&target->lock);\r\niu = __srp_get_tx_iu(target, SRP_IU_TSK_MGMT);\r\nspin_unlock_irq(&target->lock);\r\nif (!iu) {\r\nmutex_unlock(&rport->mutex);\r\nreturn -1;\r\n}\r\nib_dma_sync_single_for_cpu(dev, iu->dma, sizeof *tsk_mgmt,\r\nDMA_TO_DEVICE);\r\ntsk_mgmt = iu->buf;\r\nmemset(tsk_mgmt, 0, sizeof *tsk_mgmt);\r\ntsk_mgmt->opcode = SRP_TSK_MGMT;\r\ntsk_mgmt->lun = cpu_to_be64((u64) lun << 48);\r\ntsk_mgmt->tag = req_tag | SRP_TAG_TSK_MGMT;\r\ntsk_mgmt->tsk_mgmt_func = func;\r\ntsk_mgmt->task_tag = req_tag;\r\nib_dma_sync_single_for_device(dev, iu->dma, sizeof *tsk_mgmt,\r\nDMA_TO_DEVICE);\r\nif (srp_post_send(target, iu, sizeof *tsk_mgmt)) {\r\nsrp_put_tx_iu(target, iu, SRP_IU_TSK_MGMT);\r\nmutex_unlock(&rport->mutex);\r\nreturn -1;\r\n}\r\nmutex_unlock(&rport->mutex);\r\nif (!wait_for_completion_timeout(&target->tsk_mgmt_done,\r\nmsecs_to_jiffies(SRP_ABORT_TIMEOUT_MS)))\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic int srp_abort(struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(scmnd->device->host);\r\nstruct srp_request *req = (struct srp_request *) scmnd->host_scribble;\r\nint ret;\r\nshost_printk(KERN_ERR, target->scsi_host, "SRP abort called\n");\r\nif (!req || !srp_claim_req(target, req, NULL, scmnd))\r\nreturn SUCCESS;\r\nif (srp_send_tsk_mgmt(target, req->index, scmnd->device->lun,\r\nSRP_TSK_ABORT_TASK) == 0)\r\nret = SUCCESS;\r\nelse if (target->rport->state == SRP_RPORT_LOST)\r\nret = FAST_IO_FAIL;\r\nelse\r\nret = FAILED;\r\nsrp_free_req(target, req, scmnd, 0);\r\nscmnd->result = DID_ABORT << 16;\r\nscmnd->scsi_done(scmnd);\r\nreturn ret;\r\n}\r\nstatic int srp_reset_device(struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(scmnd->device->host);\r\nint i;\r\nshost_printk(KERN_ERR, target->scsi_host, "SRP reset_device called\n");\r\nif (srp_send_tsk_mgmt(target, SRP_TAG_NO_REQ, scmnd->device->lun,\r\nSRP_TSK_LUN_RESET))\r\nreturn FAILED;\r\nif (target->tsk_mgmt_status)\r\nreturn FAILED;\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nstruct srp_request *req = &target->req_ring[i];\r\nsrp_finish_req(target, req, scmnd->device, DID_RESET << 16);\r\n}\r\nreturn SUCCESS;\r\n}\r\nstatic int srp_reset_host(struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(scmnd->device->host);\r\nshost_printk(KERN_ERR, target->scsi_host, PFX "SRP reset_host called\n");\r\nreturn srp_reconnect_rport(target->rport) == 0 ? SUCCESS : FAILED;\r\n}\r\nstatic int srp_slave_configure(struct scsi_device *sdev)\r\n{\r\nstruct Scsi_Host *shost = sdev->host;\r\nstruct srp_target_port *target = host_to_target(shost);\r\nstruct request_queue *q = sdev->request_queue;\r\nunsigned long timeout;\r\nif (sdev->type == TYPE_DISK) {\r\ntimeout = max_t(unsigned, 30 * HZ, target->rq_tmo_jiffies);\r\nblk_queue_rq_timeout(q, timeout);\r\n}\r\nreturn 0;\r\n}\r\nstatic ssize_t show_id_ext(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%016llx\n",\r\n(unsigned long long) be64_to_cpu(target->id_ext));\r\n}\r\nstatic ssize_t show_ioc_guid(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%016llx\n",\r\n(unsigned long long) be64_to_cpu(target->ioc_guid));\r\n}\r\nstatic ssize_t show_service_id(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%016llx\n",\r\n(unsigned long long) be64_to_cpu(target->service_id));\r\n}\r\nstatic ssize_t show_pkey(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%04x\n", be16_to_cpu(target->path.pkey));\r\n}\r\nstatic ssize_t show_sgid(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%pI6\n", target->path.sgid.raw);\r\n}\r\nstatic ssize_t show_dgid(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%pI6\n", target->path.dgid.raw);\r\n}\r\nstatic ssize_t show_orig_dgid(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%pI6\n", target->orig_dgid);\r\n}\r\nstatic ssize_t show_req_lim(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->req_lim);\r\n}\r\nstatic ssize_t show_zero_req_lim(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->zero_req_lim);\r\n}\r\nstatic ssize_t show_local_ib_port(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->srp_host->port);\r\n}\r\nstatic ssize_t show_local_ib_device(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%s\n", target->srp_host->srp_dev->dev->name);\r\n}\r\nstatic ssize_t show_comp_vector(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->comp_vector);\r\n}\r\nstatic ssize_t show_tl_retry_count(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->tl_retry_count);\r\n}\r\nstatic ssize_t show_cmd_sg_entries(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%u\n", target->cmd_sg_cnt);\r\n}\r\nstatic ssize_t show_allow_ext_sg(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%s\n", target->allow_ext_sg ? "true" : "false");\r\n}\r\nstatic int srp_add_target(struct srp_host *host, struct srp_target_port *target)\r\n{\r\nstruct srp_rport_identifiers ids;\r\nstruct srp_rport *rport;\r\nsprintf(target->target_name, "SRP.T10:%016llX",\r\n(unsigned long long) be64_to_cpu(target->id_ext));\r\nif (scsi_add_host(target->scsi_host, host->srp_dev->dev->dma_device))\r\nreturn -ENODEV;\r\nmemcpy(ids.port_id, &target->id_ext, 8);\r\nmemcpy(ids.port_id + 8, &target->ioc_guid, 8);\r\nids.roles = SRP_RPORT_ROLE_TARGET;\r\nrport = srp_rport_add(target->scsi_host, &ids);\r\nif (IS_ERR(rport)) {\r\nscsi_remove_host(target->scsi_host);\r\nreturn PTR_ERR(rport);\r\n}\r\nrport->lld_data = target;\r\ntarget->rport = rport;\r\nspin_lock(&host->target_lock);\r\nlist_add_tail(&target->list, &host->target_list);\r\nspin_unlock(&host->target_lock);\r\ntarget->state = SRP_TARGET_LIVE;\r\nscsi_scan_target(&target->scsi_host->shost_gendev,\r\n0, target->scsi_id, SCAN_WILD_CARD, 0);\r\nreturn 0;\r\n}\r\nstatic void srp_release_dev(struct device *dev)\r\n{\r\nstruct srp_host *host =\r\ncontainer_of(dev, struct srp_host, dev);\r\ncomplete(&host->released);\r\n}\r\nstatic bool srp_conn_unique(struct srp_host *host,\r\nstruct srp_target_port *target)\r\n{\r\nstruct srp_target_port *t;\r\nbool ret = false;\r\nif (target->state == SRP_TARGET_REMOVED)\r\ngoto out;\r\nret = true;\r\nspin_lock(&host->target_lock);\r\nlist_for_each_entry(t, &host->target_list, list) {\r\nif (t != target &&\r\ntarget->id_ext == t->id_ext &&\r\ntarget->ioc_guid == t->ioc_guid &&\r\ntarget->initiator_ext == t->initiator_ext) {\r\nret = false;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&host->target_lock);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int srp_parse_options(const char *buf, struct srp_target_port *target)\r\n{\r\nchar *options, *sep_opt;\r\nchar *p;\r\nchar dgid[3];\r\nsubstring_t args[MAX_OPT_ARGS];\r\nint opt_mask = 0;\r\nint token;\r\nint ret = -EINVAL;\r\nint i;\r\noptions = kstrdup(buf, GFP_KERNEL);\r\nif (!options)\r\nreturn -ENOMEM;\r\nsep_opt = options;\r\nwhile ((p = strsep(&sep_opt, ",")) != NULL) {\r\nif (!*p)\r\ncontinue;\r\ntoken = match_token(p, srp_opt_tokens, args);\r\nopt_mask |= token;\r\nswitch (token) {\r\ncase SRP_OPT_ID_EXT:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->id_ext = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_IOC_GUID:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->ioc_guid = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_DGID:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (strlen(p) != 32) {\r\npr_warn("bad dest GID parameter '%s'\n", p);\r\nkfree(p);\r\ngoto out;\r\n}\r\nfor (i = 0; i < 16; ++i) {\r\nstrlcpy(dgid, p + i * 2, 3);\r\ntarget->path.dgid.raw[i] = simple_strtoul(dgid, NULL, 16);\r\n}\r\nkfree(p);\r\nmemcpy(target->orig_dgid, target->path.dgid.raw, 16);\r\nbreak;\r\ncase SRP_OPT_PKEY:\r\nif (match_hex(args, &token)) {\r\npr_warn("bad P_Key parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->path.pkey = cpu_to_be16(token);\r\nbreak;\r\ncase SRP_OPT_SERVICE_ID:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->service_id = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\ntarget->path.service_id = target->service_id;\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_MAX_SECT:\r\nif (match_int(args, &token)) {\r\npr_warn("bad max sect parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->scsi_host->max_sectors = token;\r\nbreak;\r\ncase SRP_OPT_QUEUE_SIZE:\r\nif (match_int(args, &token) || token < 1) {\r\npr_warn("bad queue_size parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->scsi_host->can_queue = token;\r\ntarget->queue_size = token + SRP_RSP_SQ_SIZE +\r\nSRP_TSK_MGMT_SQ_SIZE;\r\nif (!(opt_mask & SRP_OPT_MAX_CMD_PER_LUN))\r\ntarget->scsi_host->cmd_per_lun = token;\r\nbreak;\r\ncase SRP_OPT_MAX_CMD_PER_LUN:\r\nif (match_int(args, &token) || token < 1) {\r\npr_warn("bad max cmd_per_lun parameter '%s'\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->scsi_host->cmd_per_lun = token;\r\nbreak;\r\ncase SRP_OPT_IO_CLASS:\r\nif (match_hex(args, &token)) {\r\npr_warn("bad IO class parameter '%s'\n", p);\r\ngoto out;\r\n}\r\nif (token != SRP_REV10_IB_IO_CLASS &&\r\ntoken != SRP_REV16A_IB_IO_CLASS) {\r\npr_warn("unknown IO class parameter value %x specified (use %x or %x).\n",\r\ntoken, SRP_REV10_IB_IO_CLASS,\r\nSRP_REV16A_IB_IO_CLASS);\r\ngoto out;\r\n}\r\ntarget->io_class = token;\r\nbreak;\r\ncase SRP_OPT_INITIATOR_EXT:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->initiator_ext = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_CMD_SG_ENTRIES:\r\nif (match_int(args, &token) || token < 1 || token > 255) {\r\npr_warn("bad max cmd_sg_entries parameter '%s'\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->cmd_sg_cnt = token;\r\nbreak;\r\ncase SRP_OPT_ALLOW_EXT_SG:\r\nif (match_int(args, &token)) {\r\npr_warn("bad allow_ext_sg parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->allow_ext_sg = !!token;\r\nbreak;\r\ncase SRP_OPT_SG_TABLESIZE:\r\nif (match_int(args, &token) || token < 1 ||\r\ntoken > SCSI_MAX_SG_CHAIN_SEGMENTS) {\r\npr_warn("bad max sg_tablesize parameter '%s'\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->sg_tablesize = token;\r\nbreak;\r\ncase SRP_OPT_COMP_VECTOR:\r\nif (match_int(args, &token) || token < 0) {\r\npr_warn("bad comp_vector parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->comp_vector = token;\r\nbreak;\r\ncase SRP_OPT_TL_RETRY_COUNT:\r\nif (match_int(args, &token) || token < 2 || token > 7) {\r\npr_warn("bad tl_retry_count parameter '%s' (must be a number between 2 and 7)\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->tl_retry_count = token;\r\nbreak;\r\ndefault:\r\npr_warn("unknown parameter or missing value '%s' in target creation request\n",\r\np);\r\ngoto out;\r\n}\r\n}\r\nif ((opt_mask & SRP_OPT_ALL) == SRP_OPT_ALL)\r\nret = 0;\r\nelse\r\nfor (i = 0; i < ARRAY_SIZE(srp_opt_tokens); ++i)\r\nif ((srp_opt_tokens[i].token & SRP_OPT_ALL) &&\r\n!(srp_opt_tokens[i].token & opt_mask))\r\npr_warn("target creation request is missing parameter '%s'\n",\r\nsrp_opt_tokens[i].pattern);\r\nif (target->scsi_host->cmd_per_lun > target->scsi_host->can_queue\r\n&& (opt_mask & SRP_OPT_MAX_CMD_PER_LUN))\r\npr_warn("cmd_per_lun = %d > queue_size = %d\n",\r\ntarget->scsi_host->cmd_per_lun,\r\ntarget->scsi_host->can_queue);\r\nout:\r\nkfree(options);\r\nreturn ret;\r\n}\r\nstatic ssize_t srp_create_target(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct srp_host *host =\r\ncontainer_of(dev, struct srp_host, dev);\r\nstruct Scsi_Host *target_host;\r\nstruct srp_target_port *target;\r\nstruct srp_device *srp_dev = host->srp_dev;\r\nstruct ib_device *ibdev = srp_dev->dev;\r\nint ret;\r\ntarget_host = scsi_host_alloc(&srp_template,\r\nsizeof (struct srp_target_port));\r\nif (!target_host)\r\nreturn -ENOMEM;\r\ntarget_host->transportt = ib_srp_transport_template;\r\ntarget_host->max_channel = 0;\r\ntarget_host->max_id = 1;\r\ntarget_host->max_lun = SRP_MAX_LUN;\r\ntarget_host->max_cmd_len = sizeof ((struct srp_cmd *) (void *) 0L)->cdb;\r\ntarget = host_to_target(target_host);\r\ntarget->io_class = SRP_REV16A_IB_IO_CLASS;\r\ntarget->scsi_host = target_host;\r\ntarget->srp_host = host;\r\ntarget->lkey = host->srp_dev->mr->lkey;\r\ntarget->rkey = host->srp_dev->mr->rkey;\r\ntarget->cmd_sg_cnt = cmd_sg_entries;\r\ntarget->sg_tablesize = indirect_sg_entries ? : cmd_sg_entries;\r\ntarget->allow_ext_sg = allow_ext_sg;\r\ntarget->tl_retry_count = 7;\r\ntarget->queue_size = SRP_DEFAULT_QUEUE_SIZE;\r\nmutex_lock(&host->add_target_mutex);\r\nret = srp_parse_options(buf, target);\r\nif (ret)\r\ngoto err;\r\ntarget->req_ring_size = target->queue_size - SRP_TSK_MGMT_SQ_SIZE;\r\nif (!srp_conn_unique(target->srp_host, target)) {\r\nshost_printk(KERN_INFO, target->scsi_host,\r\nPFX "Already connected to target port with id_ext=%016llx;ioc_guid=%016llx;initiator_ext=%016llx\n",\r\nbe64_to_cpu(target->id_ext),\r\nbe64_to_cpu(target->ioc_guid),\r\nbe64_to_cpu(target->initiator_ext));\r\nret = -EEXIST;\r\ngoto err;\r\n}\r\nif (!srp_dev->has_fmr && !srp_dev->has_fr && !target->allow_ext_sg &&\r\ntarget->cmd_sg_cnt < target->sg_tablesize) {\r\npr_warn("No MR pool and no external indirect descriptors, limiting sg_tablesize to cmd_sg_cnt\n");\r\ntarget->sg_tablesize = target->cmd_sg_cnt;\r\n}\r\ntarget_host->sg_tablesize = target->sg_tablesize;\r\ntarget->indirect_size = target->sg_tablesize *\r\nsizeof (struct srp_direct_buf);\r\ntarget->max_iu_len = sizeof (struct srp_cmd) +\r\nsizeof (struct srp_indirect_buf) +\r\ntarget->cmd_sg_cnt * sizeof (struct srp_direct_buf);\r\nINIT_WORK(&target->tl_err_work, srp_tl_err_work);\r\nINIT_WORK(&target->remove_work, srp_remove_work);\r\nspin_lock_init(&target->lock);\r\nINIT_LIST_HEAD(&target->free_tx);\r\nret = srp_alloc_req_data(target);\r\nif (ret)\r\ngoto err_free_mem;\r\nret = ib_query_gid(ibdev, host->port, 0, &target->path.sgid);\r\nif (ret)\r\ngoto err_free_mem;\r\nret = srp_create_target_ib(target);\r\nif (ret)\r\ngoto err_free_mem;\r\nret = srp_new_cm_id(target);\r\nif (ret)\r\ngoto err_free_ib;\r\nret = srp_connect_target(target);\r\nif (ret) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Connection failed\n");\r\ngoto err_cm_id;\r\n}\r\nret = srp_add_target(host, target);\r\nif (ret)\r\ngoto err_disconnect;\r\nshost_printk(KERN_DEBUG, target->scsi_host, PFX\r\n"new target: id_ext %016llx ioc_guid %016llx pkey %04x service_id %016llx sgid %pI6 dgid %pI6\n",\r\nbe64_to_cpu(target->id_ext),\r\nbe64_to_cpu(target->ioc_guid),\r\nbe16_to_cpu(target->path.pkey),\r\nbe64_to_cpu(target->service_id),\r\ntarget->path.sgid.raw, target->path.dgid.raw);\r\nret = count;\r\nout:\r\nmutex_unlock(&host->add_target_mutex);\r\nreturn ret;\r\nerr_disconnect:\r\nsrp_disconnect_target(target);\r\nerr_cm_id:\r\nib_destroy_cm_id(target->cm_id);\r\nerr_free_ib:\r\nsrp_free_target_ib(target);\r\nerr_free_mem:\r\nsrp_free_req_data(target);\r\nerr:\r\nscsi_host_put(target_host);\r\ngoto out;\r\n}\r\nstatic ssize_t show_ibdev(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_host *host = container_of(dev, struct srp_host, dev);\r\nreturn sprintf(buf, "%s\n", host->srp_dev->dev->name);\r\n}\r\nstatic ssize_t show_port(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_host *host = container_of(dev, struct srp_host, dev);\r\nreturn sprintf(buf, "%d\n", host->port);\r\n}\r\nstatic struct srp_host *srp_add_port(struct srp_device *device, u8 port)\r\n{\r\nstruct srp_host *host;\r\nhost = kzalloc(sizeof *host, GFP_KERNEL);\r\nif (!host)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&host->target_list);\r\nspin_lock_init(&host->target_lock);\r\ninit_completion(&host->released);\r\nmutex_init(&host->add_target_mutex);\r\nhost->srp_dev = device;\r\nhost->port = port;\r\nhost->dev.class = &srp_class;\r\nhost->dev.parent = device->dev->dma_device;\r\ndev_set_name(&host->dev, "srp-%s-%d", device->dev->name, port);\r\nif (device_register(&host->dev))\r\ngoto free_host;\r\nif (device_create_file(&host->dev, &dev_attr_add_target))\r\ngoto err_class;\r\nif (device_create_file(&host->dev, &dev_attr_ibdev))\r\ngoto err_class;\r\nif (device_create_file(&host->dev, &dev_attr_port))\r\ngoto err_class;\r\nreturn host;\r\nerr_class:\r\ndevice_unregister(&host->dev);\r\nfree_host:\r\nkfree(host);\r\nreturn NULL;\r\n}\r\nstatic void srp_add_one(struct ib_device *device)\r\n{\r\nstruct srp_device *srp_dev;\r\nstruct ib_device_attr *dev_attr;\r\nstruct srp_host *host;\r\nint mr_page_shift, s, e, p;\r\nu64 max_pages_per_mr;\r\ndev_attr = kmalloc(sizeof *dev_attr, GFP_KERNEL);\r\nif (!dev_attr)\r\nreturn;\r\nif (ib_query_device(device, dev_attr)) {\r\npr_warn("Query device failed for %s\n", device->name);\r\ngoto free_attr;\r\n}\r\nsrp_dev = kmalloc(sizeof *srp_dev, GFP_KERNEL);\r\nif (!srp_dev)\r\ngoto free_attr;\r\nsrp_dev->has_fmr = (device->alloc_fmr && device->dealloc_fmr &&\r\ndevice->map_phys_fmr && device->unmap_fmr);\r\nsrp_dev->has_fr = (dev_attr->device_cap_flags &\r\nIB_DEVICE_MEM_MGT_EXTENSIONS);\r\nif (!srp_dev->has_fmr && !srp_dev->has_fr)\r\ndev_warn(&device->dev, "neither FMR nor FR is supported\n");\r\nsrp_dev->use_fast_reg = (srp_dev->has_fr &&\r\n(!srp_dev->has_fmr || prefer_fr));\r\nmr_page_shift = max(12, ffs(dev_attr->page_size_cap) - 1);\r\nsrp_dev->mr_page_size = 1 << mr_page_shift;\r\nsrp_dev->mr_page_mask = ~((u64) srp_dev->mr_page_size - 1);\r\nmax_pages_per_mr = dev_attr->max_mr_size;\r\ndo_div(max_pages_per_mr, srp_dev->mr_page_size);\r\nsrp_dev->max_pages_per_mr = min_t(u64, SRP_MAX_PAGES_PER_MR,\r\nmax_pages_per_mr);\r\nif (srp_dev->use_fast_reg) {\r\nsrp_dev->max_pages_per_mr =\r\nmin_t(u32, srp_dev->max_pages_per_mr,\r\ndev_attr->max_fast_reg_page_list_len);\r\n}\r\nsrp_dev->mr_max_size = srp_dev->mr_page_size *\r\nsrp_dev->max_pages_per_mr;\r\npr_debug("%s: mr_page_shift = %d, dev_attr->max_mr_size = %#llx, dev_attr->max_fast_reg_page_list_len = %u, max_pages_per_mr = %d, mr_max_size = %#x\n",\r\ndevice->name, mr_page_shift, dev_attr->max_mr_size,\r\ndev_attr->max_fast_reg_page_list_len,\r\nsrp_dev->max_pages_per_mr, srp_dev->mr_max_size);\r\nINIT_LIST_HEAD(&srp_dev->dev_list);\r\nsrp_dev->dev = device;\r\nsrp_dev->pd = ib_alloc_pd(device);\r\nif (IS_ERR(srp_dev->pd))\r\ngoto free_dev;\r\nsrp_dev->mr = ib_get_dma_mr(srp_dev->pd,\r\nIB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE);\r\nif (IS_ERR(srp_dev->mr))\r\ngoto err_pd;\r\nif (device->node_type == RDMA_NODE_IB_SWITCH) {\r\ns = 0;\r\ne = 0;\r\n} else {\r\ns = 1;\r\ne = device->phys_port_cnt;\r\n}\r\nfor (p = s; p <= e; ++p) {\r\nhost = srp_add_port(srp_dev, p);\r\nif (host)\r\nlist_add_tail(&host->list, &srp_dev->dev_list);\r\n}\r\nib_set_client_data(device, &srp_client, srp_dev);\r\ngoto free_attr;\r\nerr_pd:\r\nib_dealloc_pd(srp_dev->pd);\r\nfree_dev:\r\nkfree(srp_dev);\r\nfree_attr:\r\nkfree(dev_attr);\r\n}\r\nstatic void srp_remove_one(struct ib_device *device)\r\n{\r\nstruct srp_device *srp_dev;\r\nstruct srp_host *host, *tmp_host;\r\nstruct srp_target_port *target;\r\nsrp_dev = ib_get_client_data(device, &srp_client);\r\nif (!srp_dev)\r\nreturn;\r\nlist_for_each_entry_safe(host, tmp_host, &srp_dev->dev_list, list) {\r\ndevice_unregister(&host->dev);\r\nwait_for_completion(&host->released);\r\nspin_lock(&host->target_lock);\r\nlist_for_each_entry(target, &host->target_list, list)\r\nsrp_queue_remove_work(target);\r\nspin_unlock(&host->target_lock);\r\nflush_workqueue(system_long_wq);\r\nflush_workqueue(srp_remove_wq);\r\nkfree(host);\r\n}\r\nib_dereg_mr(srp_dev->mr);\r\nib_dealloc_pd(srp_dev->pd);\r\nkfree(srp_dev);\r\n}\r\nstatic int __init srp_init_module(void)\r\n{\r\nint ret;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct ib_wc, wr_id) < sizeof(void *));\r\nif (srp_sg_tablesize) {\r\npr_warn("srp_sg_tablesize is deprecated, please use cmd_sg_entries\n");\r\nif (!cmd_sg_entries)\r\ncmd_sg_entries = srp_sg_tablesize;\r\n}\r\nif (!cmd_sg_entries)\r\ncmd_sg_entries = SRP_DEF_SG_TABLESIZE;\r\nif (cmd_sg_entries > 255) {\r\npr_warn("Clamping cmd_sg_entries to 255\n");\r\ncmd_sg_entries = 255;\r\n}\r\nif (!indirect_sg_entries)\r\nindirect_sg_entries = cmd_sg_entries;\r\nelse if (indirect_sg_entries < cmd_sg_entries) {\r\npr_warn("Bumping up indirect_sg_entries to match cmd_sg_entries (%u)\n",\r\ncmd_sg_entries);\r\nindirect_sg_entries = cmd_sg_entries;\r\n}\r\nsrp_remove_wq = create_workqueue("srp_remove");\r\nif (!srp_remove_wq) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = -ENOMEM;\r\nib_srp_transport_template =\r\nsrp_attach_transport(&ib_srp_transport_functions);\r\nif (!ib_srp_transport_template)\r\ngoto destroy_wq;\r\nret = class_register(&srp_class);\r\nif (ret) {\r\npr_err("couldn't register class infiniband_srp\n");\r\ngoto release_tr;\r\n}\r\nib_sa_register_client(&srp_sa_client);\r\nret = ib_register_client(&srp_client);\r\nif (ret) {\r\npr_err("couldn't register IB client\n");\r\ngoto unreg_sa;\r\n}\r\nout:\r\nreturn ret;\r\nunreg_sa:\r\nib_sa_unregister_client(&srp_sa_client);\r\nclass_unregister(&srp_class);\r\nrelease_tr:\r\nsrp_release_transport(ib_srp_transport_template);\r\ndestroy_wq:\r\ndestroy_workqueue(srp_remove_wq);\r\ngoto out;\r\n}\r\nstatic void __exit srp_cleanup_module(void)\r\n{\r\nib_unregister_client(&srp_client);\r\nib_sa_unregister_client(&srp_sa_client);\r\nclass_unregister(&srp_class);\r\nsrp_release_transport(ib_srp_transport_template);\r\ndestroy_workqueue(srp_remove_wq);\r\n}
