static u32 ipmmu_read(struct ipmmu_vmsa_device *mmu, unsigned int offset)\r\n{\r\nreturn ioread32(mmu->base + offset);\r\n}\r\nstatic void ipmmu_write(struct ipmmu_vmsa_device *mmu, unsigned int offset,\r\nu32 data)\r\n{\r\niowrite32(data, mmu->base + offset);\r\n}\r\nstatic u32 ipmmu_ctx_read(struct ipmmu_vmsa_domain *domain, unsigned int reg)\r\n{\r\nreturn ipmmu_read(domain->mmu, domain->context_id * IM_CTX_SIZE + reg);\r\n}\r\nstatic void ipmmu_ctx_write(struct ipmmu_vmsa_domain *domain, unsigned int reg,\r\nu32 data)\r\n{\r\nipmmu_write(domain->mmu, domain->context_id * IM_CTX_SIZE + reg, data);\r\n}\r\nstatic void ipmmu_tlb_sync(struct ipmmu_vmsa_domain *domain)\r\n{\r\nunsigned int count = 0;\r\nwhile (ipmmu_ctx_read(domain, IMCTR) & IMCTR_FLUSH) {\r\ncpu_relax();\r\nif (++count == TLB_LOOP_TIMEOUT) {\r\ndev_err_ratelimited(domain->mmu->dev,\r\n"TLB sync timed out -- MMU may be deadlocked\n");\r\nreturn;\r\n}\r\nudelay(1);\r\n}\r\n}\r\nstatic void ipmmu_tlb_invalidate(struct ipmmu_vmsa_domain *domain)\r\n{\r\nu32 reg;\r\nreg = ipmmu_ctx_read(domain, IMCTR);\r\nreg |= IMCTR_FLUSH;\r\nipmmu_ctx_write(domain, IMCTR, reg);\r\nipmmu_tlb_sync(domain);\r\n}\r\nstatic void ipmmu_utlb_enable(struct ipmmu_vmsa_domain *domain,\r\nunsigned int utlb)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\nipmmu_write(mmu, IMUASID(utlb), 0);\r\nipmmu_write(mmu, IMUCTR(utlb),\r\nIMUCTR_TTSEL_MMU(domain->context_id) | IMUCTR_FLUSH |\r\nIMUCTR_MMUEN);\r\n}\r\nstatic void ipmmu_utlb_disable(struct ipmmu_vmsa_domain *domain,\r\nunsigned int utlb)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\nipmmu_write(mmu, IMUCTR(utlb), 0);\r\n}\r\nstatic void ipmmu_flush_pgtable(struct ipmmu_vmsa_device *mmu, void *addr,\r\nsize_t size)\r\n{\r\nunsigned long offset = (unsigned long)addr & ~PAGE_MASK;\r\ndma_map_page(mmu->dev, virt_to_page(addr), offset, size, DMA_TO_DEVICE);\r\n}\r\nstatic int ipmmu_domain_init_context(struct ipmmu_vmsa_domain *domain)\r\n{\r\nphys_addr_t ttbr;\r\nu32 reg;\r\ndomain->context_id = 0;\r\nipmmu_flush_pgtable(domain->mmu, domain->pgd,\r\nIPMMU_PTRS_PER_PGD * sizeof(*domain->pgd));\r\nttbr = __pa(domain->pgd);\r\nipmmu_ctx_write(domain, IMTTLBR0, ttbr);\r\nipmmu_ctx_write(domain, IMTTUBR0, ttbr >> 32);\r\nipmmu_ctx_write(domain, IMTTBCR, IMTTBCR_EAE |\r\nIMTTBCR_SH0_INNER_SHAREABLE | IMTTBCR_ORGN0_WB_WA |\r\nIMTTBCR_IRGN0_WB_WA | IMTTBCR_SL0_LVL_1);\r\nreg = (IMMAIR_ATTR_NC << IMMAIR_ATTR_SHIFT(IMMAIR_ATTR_IDX_NC))\r\n| (IMMAIR_ATTR_WBRWA << IMMAIR_ATTR_SHIFT(IMMAIR_ATTR_IDX_WBRWA))\r\n| (IMMAIR_ATTR_DEVICE << IMMAIR_ATTR_SHIFT(IMMAIR_ATTR_IDX_DEV));\r\nipmmu_ctx_write(domain, IMMAIR0, reg);\r\nipmmu_ctx_write(domain, IMBUSCR,\r\nipmmu_ctx_read(domain, IMBUSCR) &\r\n~(IMBUSCR_DVM | IMBUSCR_BUSSEL_MASK));\r\nipmmu_ctx_write(domain, IMSTR, ipmmu_ctx_read(domain, IMSTR));\r\nipmmu_ctx_write(domain, IMCTR, IMCTR_INTEN | IMCTR_FLUSH | IMCTR_MMUEN);\r\nreturn 0;\r\n}\r\nstatic void ipmmu_domain_destroy_context(struct ipmmu_vmsa_domain *domain)\r\n{\r\nipmmu_ctx_write(domain, IMCTR, IMCTR_FLUSH);\r\nipmmu_tlb_sync(domain);\r\n}\r\nstatic irqreturn_t ipmmu_domain_irq(struct ipmmu_vmsa_domain *domain)\r\n{\r\nconst u32 err_mask = IMSTR_MHIT | IMSTR_ABORT | IMSTR_PF | IMSTR_TF;\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\nu32 status;\r\nu32 iova;\r\nstatus = ipmmu_ctx_read(domain, IMSTR);\r\nif (!(status & err_mask))\r\nreturn IRQ_NONE;\r\niova = ipmmu_ctx_read(domain, IMEAR);\r\nipmmu_ctx_write(domain, IMSTR, 0);\r\nif (status & IMSTR_MHIT)\r\ndev_err_ratelimited(mmu->dev, "Multiple TLB hits @0x%08x\n",\r\niova);\r\nif (status & IMSTR_ABORT)\r\ndev_err_ratelimited(mmu->dev, "Page Table Walk Abort @0x%08x\n",\r\niova);\r\nif (!(status & (IMSTR_PF | IMSTR_TF)))\r\nreturn IRQ_NONE;\r\nif (!report_iommu_fault(domain->io_domain, mmu->dev, iova, 0))\r\nreturn IRQ_HANDLED;\r\ndev_err_ratelimited(mmu->dev,\r\n"Unhandled fault: status 0x%08x iova 0x%08x\n",\r\nstatus, iova);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ipmmu_irq(int irq, void *dev)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = dev;\r\nstruct iommu_domain *io_domain;\r\nstruct ipmmu_vmsa_domain *domain;\r\nif (!mmu->mapping)\r\nreturn IRQ_NONE;\r\nio_domain = mmu->mapping->domain;\r\ndomain = io_domain->priv;\r\nreturn ipmmu_domain_irq(domain);\r\n}\r\nstatic void ipmmu_free_ptes(pmd_t *pmd)\r\n{\r\npgtable_t table = pmd_pgtable(*pmd);\r\n__free_page(table);\r\n}\r\nstatic void ipmmu_free_pmds(pud_t *pud)\r\n{\r\npmd_t *pmd = pmd_offset(pud, 0);\r\npgtable_t table;\r\nunsigned int i;\r\nfor (i = 0; i < IPMMU_PTRS_PER_PMD; ++i) {\r\nif (!pmd_table(*pmd))\r\ncontinue;\r\nipmmu_free_ptes(pmd);\r\npmd++;\r\n}\r\ntable = pud_pgtable(*pud);\r\n__free_page(table);\r\n}\r\nstatic void ipmmu_free_pgtables(struct ipmmu_vmsa_domain *domain)\r\n{\r\npgd_t *pgd, *pgd_base = domain->pgd;\r\nunsigned int i;\r\npgd = pgd_base;\r\nfor (i = 0; i < IPMMU_PTRS_PER_PGD; ++i) {\r\nif (pgd_none(*pgd))\r\ncontinue;\r\nipmmu_free_pmds((pud_t *)pgd);\r\npgd++;\r\n}\r\nkfree(pgd_base);\r\n}\r\nstatic pte_t *ipmmu_alloc_pte(struct ipmmu_vmsa_device *mmu, pmd_t *pmd,\r\nunsigned long iova)\r\n{\r\npte_t *pte;\r\nif (!pmd_none(*pmd))\r\nreturn pte_offset_kernel(pmd, iova);\r\npte = (pte_t *)get_zeroed_page(GFP_ATOMIC);\r\nif (!pte)\r\nreturn NULL;\r\nipmmu_flush_pgtable(mmu, pte, PAGE_SIZE);\r\n*pmd = __pmd(__pa(pte) | PMD_NSTABLE | PMD_TYPE_TABLE);\r\nipmmu_flush_pgtable(mmu, pmd, sizeof(*pmd));\r\nreturn pte + pte_index(iova);\r\n}\r\nstatic pmd_t *ipmmu_alloc_pmd(struct ipmmu_vmsa_device *mmu, pgd_t *pgd,\r\nunsigned long iova)\r\n{\r\npud_t *pud = (pud_t *)pgd;\r\npmd_t *pmd;\r\nif (!pud_none(*pud))\r\nreturn pmd_offset(pud, iova);\r\npmd = (pmd_t *)get_zeroed_page(GFP_ATOMIC);\r\nif (!pmd)\r\nreturn NULL;\r\nipmmu_flush_pgtable(mmu, pmd, PAGE_SIZE);\r\n*pud = __pud(__pa(pmd) | PMD_NSTABLE | PMD_TYPE_TABLE);\r\nipmmu_flush_pgtable(mmu, pud, sizeof(*pud));\r\nreturn pmd + pmd_index(iova);\r\n}\r\nstatic u64 ipmmu_page_prot(unsigned int prot, u64 type)\r\n{\r\nu64 pgprot = ARM_VMSA_PTE_XN | ARM_VMSA_PTE_nG | ARM_VMSA_PTE_AF\r\n| ARM_VMSA_PTE_SH_IS | ARM_VMSA_PTE_AP_UNPRIV\r\n| ARM_VMSA_PTE_NS | type;\r\nif (!(prot & IOMMU_WRITE) && (prot & IOMMU_READ))\r\npgprot |= ARM_VMSA_PTE_AP_RDONLY;\r\nif (prot & IOMMU_CACHE)\r\npgprot |= IMMAIR_ATTR_IDX_WBRWA << ARM_VMSA_PTE_ATTRINDX_SHIFT;\r\nif (prot & IOMMU_EXEC)\r\npgprot &= ~ARM_VMSA_PTE_XN;\r\nelse if (!(prot & (IOMMU_READ | IOMMU_WRITE)))\r\npgprot &= ~ARM_VMSA_PTE_PAGE;\r\nreturn pgprot;\r\n}\r\nstatic int ipmmu_alloc_init_pte(struct ipmmu_vmsa_device *mmu, pmd_t *pmd,\r\nunsigned long iova, unsigned long pfn,\r\nsize_t size, int prot)\r\n{\r\npteval_t pteval = ipmmu_page_prot(prot, ARM_VMSA_PTE_PAGE);\r\nunsigned int num_ptes = 1;\r\npte_t *pte, *start;\r\nunsigned int i;\r\npte = ipmmu_alloc_pte(mmu, pmd, iova);\r\nif (!pte)\r\nreturn -ENOMEM;\r\nstart = pte;\r\nif (size == SZ_64K) {\r\npteval |= ARM_VMSA_PTE_CONT;\r\nnum_ptes = ARM_VMSA_PTE_CONT_ENTRIES;\r\n}\r\nfor (i = num_ptes; i; --i)\r\n*pte++ = pfn_pte(pfn++, __pgprot(pteval));\r\nipmmu_flush_pgtable(mmu, start, sizeof(*pte) * num_ptes);\r\nreturn 0;\r\n}\r\nstatic int ipmmu_alloc_init_pmd(struct ipmmu_vmsa_device *mmu, pmd_t *pmd,\r\nunsigned long iova, unsigned long pfn,\r\nint prot)\r\n{\r\npmdval_t pmdval = ipmmu_page_prot(prot, PMD_TYPE_SECT);\r\n*pmd = pfn_pmd(pfn, __pgprot(pmdval));\r\nipmmu_flush_pgtable(mmu, pmd, sizeof(*pmd));\r\nreturn 0;\r\n}\r\nstatic int ipmmu_create_mapping(struct ipmmu_vmsa_domain *domain,\r\nunsigned long iova, phys_addr_t paddr,\r\nsize_t size, int prot)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\npgd_t *pgd = domain->pgd;\r\nunsigned long flags;\r\nunsigned long pfn;\r\npmd_t *pmd;\r\nint ret;\r\nif (!pgd)\r\nreturn -EINVAL;\r\nif (size & ~PAGE_MASK)\r\nreturn -EINVAL;\r\nif (paddr & ~((1ULL << 40) - 1))\r\nreturn -ERANGE;\r\npfn = __phys_to_pfn(paddr);\r\npgd += pgd_index(iova);\r\nspin_lock_irqsave(&domain->lock, flags);\r\npmd = ipmmu_alloc_pmd(mmu, pgd, iova);\r\nif (!pmd) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\nswitch (size) {\r\ncase SZ_2M:\r\nret = ipmmu_alloc_init_pmd(mmu, pmd, iova, pfn, prot);\r\nbreak;\r\ncase SZ_64K:\r\ncase SZ_4K:\r\nret = ipmmu_alloc_init_pte(mmu, pmd, iova, pfn, size, prot);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\ndone:\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nif (!ret)\r\nipmmu_tlb_invalidate(domain);\r\nreturn ret;\r\n}\r\nstatic void ipmmu_clear_pud(struct ipmmu_vmsa_device *mmu, pud_t *pud)\r\n{\r\npgtable_t table = pud_pgtable(*pud);\r\n__free_page(table);\r\n*pud = __pud(0);\r\nipmmu_flush_pgtable(mmu, pud, sizeof(*pud));\r\n}\r\nstatic void ipmmu_clear_pmd(struct ipmmu_vmsa_device *mmu, pud_t *pud,\r\npmd_t *pmd)\r\n{\r\nunsigned int i;\r\nif (pmd_table(*pmd)) {\r\npgtable_t table = pmd_pgtable(*pmd);\r\n__free_page(table);\r\n}\r\n*pmd = __pmd(0);\r\nipmmu_flush_pgtable(mmu, pmd, sizeof(*pmd));\r\npmd = pmd_offset(pud, 0);\r\nfor (i = 0; i < IPMMU_PTRS_PER_PMD; ++i) {\r\nif (!pmd_none(pmd[i]))\r\nreturn;\r\n}\r\nipmmu_clear_pud(mmu, pud);\r\n}\r\nstatic void ipmmu_clear_pte(struct ipmmu_vmsa_device *mmu, pud_t *pud,\r\npmd_t *pmd, pte_t *pte, unsigned int num_ptes)\r\n{\r\nunsigned int i;\r\nfor (i = num_ptes; i; --i)\r\npte[i-1] = __pte(0);\r\nipmmu_flush_pgtable(mmu, pte, sizeof(*pte) * num_ptes);\r\npte = pte_offset_kernel(pmd, 0);\r\nfor (i = 0; i < IPMMU_PTRS_PER_PTE; ++i) {\r\nif (!pte_none(pte[i]))\r\nreturn;\r\n}\r\nipmmu_clear_pmd(mmu, pud, pmd);\r\n}\r\nstatic int ipmmu_split_pmd(struct ipmmu_vmsa_device *mmu, pmd_t *pmd)\r\n{\r\npte_t *pte, *start;\r\npteval_t pteval;\r\nunsigned long pfn;\r\nunsigned int i;\r\npte = (pte_t *)get_zeroed_page(GFP_ATOMIC);\r\nif (!pte)\r\nreturn -ENOMEM;\r\npteval = (pmd_val(*pmd) & ARM_VMSA_PTE_ATTRS_MASK)\r\n| ARM_VMSA_PTE_CONT | ARM_VMSA_PTE_PAGE;\r\npfn = pmd_pfn(*pmd);\r\nstart = pte;\r\nfor (i = IPMMU_PTRS_PER_PTE; i; --i)\r\n*pte++ = pfn_pte(pfn++, __pgprot(pteval));\r\nipmmu_flush_pgtable(mmu, start, PAGE_SIZE);\r\n*pmd = __pmd(__pa(start) | PMD_NSTABLE | PMD_TYPE_TABLE);\r\nipmmu_flush_pgtable(mmu, pmd, sizeof(*pmd));\r\nreturn 0;\r\n}\r\nstatic void ipmmu_split_pte(struct ipmmu_vmsa_device *mmu, pte_t *pte)\r\n{\r\nunsigned int i;\r\nfor (i = ARM_VMSA_PTE_CONT_ENTRIES; i; --i)\r\npte[i-1] = __pte(pte_val(*pte) & ~ARM_VMSA_PTE_CONT);\r\nipmmu_flush_pgtable(mmu, pte, sizeof(*pte) * ARM_VMSA_PTE_CONT_ENTRIES);\r\n}\r\nstatic int ipmmu_clear_mapping(struct ipmmu_vmsa_domain *domain,\r\nunsigned long iova, size_t size)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\nunsigned long flags;\r\npgd_t *pgd = domain->pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nint ret = 0;\r\nif (!pgd)\r\nreturn -EINVAL;\r\nif (size & ~PAGE_MASK)\r\nreturn -EINVAL;\r\npgd += pgd_index(iova);\r\npud = (pud_t *)pgd;\r\nspin_lock_irqsave(&domain->lock, flags);\r\nif (pud_none(*pud))\r\ngoto done;\r\npmd = pmd_offset(pud, iova);\r\nif (pmd_none(*pmd))\r\ngoto done;\r\nif (size == SZ_2M) {\r\nipmmu_clear_pmd(mmu, pud, pmd);\r\ngoto done;\r\n}\r\nif (pmd_sect(*pmd))\r\nipmmu_split_pmd(mmu, pmd);\r\npte = pte_offset_kernel(pmd, iova);\r\nif (size == SZ_64K) {\r\nipmmu_clear_pte(mmu, pud, pmd, pte, ARM_VMSA_PTE_CONT_ENTRIES);\r\ngoto done;\r\n}\r\nif (pte_val(*pte) & ARM_VMSA_PTE_CONT)\r\nipmmu_split_pte(mmu, pte);\r\nipmmu_clear_pte(mmu, pud, pmd, pte, 1);\r\ndone:\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nif (ret)\r\nipmmu_tlb_invalidate(domain);\r\nreturn 0;\r\n}\r\nstatic int ipmmu_domain_init(struct iommu_domain *io_domain)\r\n{\r\nstruct ipmmu_vmsa_domain *domain;\r\ndomain = kzalloc(sizeof(*domain), GFP_KERNEL);\r\nif (!domain)\r\nreturn -ENOMEM;\r\nspin_lock_init(&domain->lock);\r\ndomain->pgd = kzalloc(IPMMU_PTRS_PER_PGD * sizeof(pgd_t), GFP_KERNEL);\r\nif (!domain->pgd) {\r\nkfree(domain);\r\nreturn -ENOMEM;\r\n}\r\nio_domain->priv = domain;\r\ndomain->io_domain = io_domain;\r\nreturn 0;\r\n}\r\nstatic void ipmmu_domain_destroy(struct iommu_domain *io_domain)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = io_domain->priv;\r\nipmmu_domain_destroy_context(domain);\r\nipmmu_free_pgtables(domain);\r\nkfree(domain);\r\n}\r\nstatic int ipmmu_attach_device(struct iommu_domain *io_domain,\r\nstruct device *dev)\r\n{\r\nstruct ipmmu_vmsa_archdata *archdata = dev->archdata.iommu;\r\nstruct ipmmu_vmsa_device *mmu = archdata->mmu;\r\nstruct ipmmu_vmsa_domain *domain = io_domain->priv;\r\nunsigned long flags;\r\nint ret = 0;\r\nif (!mmu) {\r\ndev_err(dev, "Cannot attach to IPMMU\n");\r\nreturn -ENXIO;\r\n}\r\nspin_lock_irqsave(&domain->lock, flags);\r\nif (!domain->mmu) {\r\ndomain->mmu = mmu;\r\nret = ipmmu_domain_init_context(domain);\r\n} else if (domain->mmu != mmu) {\r\ndev_err(dev, "Can't attach IPMMU %s to domain on IPMMU %s\n",\r\ndev_name(mmu->dev), dev_name(domain->mmu->dev));\r\nret = -EINVAL;\r\n}\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nif (ret < 0)\r\nreturn ret;\r\nipmmu_utlb_enable(domain, archdata->utlb);\r\nreturn 0;\r\n}\r\nstatic void ipmmu_detach_device(struct iommu_domain *io_domain,\r\nstruct device *dev)\r\n{\r\nstruct ipmmu_vmsa_archdata *archdata = dev->archdata.iommu;\r\nstruct ipmmu_vmsa_domain *domain = io_domain->priv;\r\nipmmu_utlb_disable(domain, archdata->utlb);\r\n}\r\nstatic int ipmmu_map(struct iommu_domain *io_domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = io_domain->priv;\r\nif (!domain)\r\nreturn -ENODEV;\r\nreturn ipmmu_create_mapping(domain, iova, paddr, size, prot);\r\n}\r\nstatic size_t ipmmu_unmap(struct iommu_domain *io_domain, unsigned long iova,\r\nsize_t size)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = io_domain->priv;\r\nint ret;\r\nret = ipmmu_clear_mapping(domain, iova, size);\r\nreturn ret ? 0 : size;\r\n}\r\nstatic phys_addr_t ipmmu_iova_to_phys(struct iommu_domain *io_domain,\r\ndma_addr_t iova)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = io_domain->priv;\r\npgd_t pgd;\r\npud_t pud;\r\npmd_t pmd;\r\npte_t pte;\r\nif (!domain->pgd)\r\nreturn 0;\r\npgd = *(domain->pgd + pgd_index(iova));\r\nif (pgd_none(pgd))\r\nreturn 0;\r\npud = *pud_offset(&pgd, iova);\r\nif (pud_none(pud))\r\nreturn 0;\r\npmd = *pmd_offset(&pud, iova);\r\nif (pmd_none(pmd))\r\nreturn 0;\r\nif (pmd_sect(pmd))\r\nreturn __pfn_to_phys(pmd_pfn(pmd)) | (iova & ~PMD_MASK);\r\npte = *(pmd_page_vaddr(pmd) + pte_index(iova));\r\nif (pte_none(pte))\r\nreturn 0;\r\nreturn __pfn_to_phys(pte_pfn(pte)) | (iova & ~PAGE_MASK);\r\n}\r\nstatic int ipmmu_find_utlb(struct ipmmu_vmsa_device *mmu, struct device *dev)\r\n{\r\nconst struct ipmmu_vmsa_master *master = mmu->pdata->masters;\r\nconst char *devname = dev_name(dev);\r\nunsigned int i;\r\nfor (i = 0; i < mmu->pdata->num_masters; ++i, ++master) {\r\nif (strcmp(master->name, devname) == 0)\r\nreturn master->utlb;\r\n}\r\nreturn -1;\r\n}\r\nstatic int ipmmu_add_device(struct device *dev)\r\n{\r\nstruct ipmmu_vmsa_archdata *archdata;\r\nstruct ipmmu_vmsa_device *mmu;\r\nstruct iommu_group *group;\r\nint utlb = -1;\r\nint ret;\r\nif (dev->archdata.iommu) {\r\ndev_warn(dev, "IOMMU driver already assigned to device %s\n",\r\ndev_name(dev));\r\nreturn -EINVAL;\r\n}\r\nspin_lock(&ipmmu_devices_lock);\r\nlist_for_each_entry(mmu, &ipmmu_devices, list) {\r\nutlb = ipmmu_find_utlb(mmu, dev);\r\nif (utlb >= 0) {\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&ipmmu_devices_lock);\r\nif (utlb < 0)\r\nreturn -ENODEV;\r\nif (utlb >= mmu->num_utlbs)\r\nreturn -EINVAL;\r\ngroup = iommu_group_alloc();\r\nif (IS_ERR(group)) {\r\ndev_err(dev, "Failed to allocate IOMMU group\n");\r\nreturn PTR_ERR(group);\r\n}\r\nret = iommu_group_add_device(group, dev);\r\niommu_group_put(group);\r\nif (ret < 0) {\r\ndev_err(dev, "Failed to add device to IPMMU group\n");\r\nreturn ret;\r\n}\r\narchdata = kzalloc(sizeof(*archdata), GFP_KERNEL);\r\nif (!archdata) {\r\nret = -ENOMEM;\r\ngoto error;\r\n}\r\narchdata->mmu = mmu;\r\narchdata->utlb = utlb;\r\ndev->archdata.iommu = archdata;\r\nif (!mmu->mapping) {\r\nstruct dma_iommu_mapping *mapping;\r\nmapping = arm_iommu_create_mapping(&platform_bus_type,\r\nSZ_1G, SZ_2G);\r\nif (IS_ERR(mapping)) {\r\ndev_err(mmu->dev, "failed to create ARM IOMMU mapping\n");\r\nreturn PTR_ERR(mapping);\r\n}\r\nmmu->mapping = mapping;\r\n}\r\nret = arm_iommu_attach_device(dev, mmu->mapping);\r\nif (ret < 0) {\r\ndev_err(dev, "Failed to attach device to VA mapping\n");\r\ngoto error;\r\n}\r\nreturn 0;\r\nerror:\r\nkfree(dev->archdata.iommu);\r\ndev->archdata.iommu = NULL;\r\niommu_group_remove_device(dev);\r\nreturn ret;\r\n}\r\nstatic void ipmmu_remove_device(struct device *dev)\r\n{\r\narm_iommu_detach_device(dev);\r\niommu_group_remove_device(dev);\r\nkfree(dev->archdata.iommu);\r\ndev->archdata.iommu = NULL;\r\n}\r\nstatic void ipmmu_device_reset(struct ipmmu_vmsa_device *mmu)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < 4; ++i)\r\nipmmu_write(mmu, i * IM_CTX_SIZE + IMCTR, 0);\r\n}\r\nstatic int ipmmu_probe(struct platform_device *pdev)\r\n{\r\nstruct ipmmu_vmsa_device *mmu;\r\nstruct resource *res;\r\nint irq;\r\nint ret;\r\nif (!pdev->dev.platform_data) {\r\ndev_err(&pdev->dev, "missing platform data\n");\r\nreturn -EINVAL;\r\n}\r\nmmu = devm_kzalloc(&pdev->dev, sizeof(*mmu), GFP_KERNEL);\r\nif (!mmu) {\r\ndev_err(&pdev->dev, "cannot allocate device data\n");\r\nreturn -ENOMEM;\r\n}\r\nmmu->dev = &pdev->dev;\r\nmmu->pdata = pdev->dev.platform_data;\r\nmmu->num_utlbs = 32;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nmmu->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(mmu->base))\r\nreturn PTR_ERR(mmu->base);\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0) {\r\ndev_err(&pdev->dev, "no IRQ found\n");\r\nreturn irq;\r\n}\r\nret = devm_request_irq(&pdev->dev, irq, ipmmu_irq, 0,\r\ndev_name(&pdev->dev), mmu);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "failed to request IRQ %d\n", irq);\r\nreturn irq;\r\n}\r\nipmmu_device_reset(mmu);\r\nspin_lock(&ipmmu_devices_lock);\r\nlist_add(&mmu->list, &ipmmu_devices);\r\nspin_unlock(&ipmmu_devices_lock);\r\nplatform_set_drvdata(pdev, mmu);\r\nreturn 0;\r\n}\r\nstatic int ipmmu_remove(struct platform_device *pdev)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = platform_get_drvdata(pdev);\r\nspin_lock(&ipmmu_devices_lock);\r\nlist_del(&mmu->list);\r\nspin_unlock(&ipmmu_devices_lock);\r\narm_iommu_release_mapping(mmu->mapping);\r\nipmmu_device_reset(mmu);\r\nreturn 0;\r\n}\r\nstatic int __init ipmmu_init(void)\r\n{\r\nint ret;\r\nret = platform_driver_register(&ipmmu_driver);\r\nif (ret < 0)\r\nreturn ret;\r\nif (!iommu_present(&platform_bus_type))\r\nbus_set_iommu(&platform_bus_type, &ipmmu_ops);\r\nreturn 0;\r\n}\r\nstatic void __exit ipmmu_exit(void)\r\n{\r\nreturn platform_driver_unregister(&ipmmu_driver);\r\n}
