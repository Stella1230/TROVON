static inline int is_gru_paddr(unsigned long paddr)\r\n{\r\nreturn paddr >= gru_start_paddr && paddr < gru_end_paddr;\r\n}\r\nstruct vm_area_struct *gru_find_vma(unsigned long vaddr)\r\n{\r\nstruct vm_area_struct *vma;\r\nvma = find_vma(current->mm, vaddr);\r\nif (vma && vma->vm_start <= vaddr && vma->vm_ops == &gru_vm_ops)\r\nreturn vma;\r\nreturn NULL;\r\n}\r\nstatic struct gru_thread_state *gru_find_lock_gts(unsigned long vaddr)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nstruct gru_thread_state *gts = NULL;\r\ndown_read(&mm->mmap_sem);\r\nvma = gru_find_vma(vaddr);\r\nif (vma)\r\ngts = gru_find_thread_state(vma, TSID(vaddr, vma));\r\nif (gts)\r\nmutex_lock(&gts->ts_ctxlock);\r\nelse\r\nup_read(&mm->mmap_sem);\r\nreturn gts;\r\n}\r\nstatic struct gru_thread_state *gru_alloc_locked_gts(unsigned long vaddr)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nstruct gru_thread_state *gts = ERR_PTR(-EINVAL);\r\ndown_write(&mm->mmap_sem);\r\nvma = gru_find_vma(vaddr);\r\nif (!vma)\r\ngoto err;\r\ngts = gru_alloc_thread_state(vma, TSID(vaddr, vma));\r\nif (IS_ERR(gts))\r\ngoto err;\r\nmutex_lock(&gts->ts_ctxlock);\r\ndowngrade_write(&mm->mmap_sem);\r\nreturn gts;\r\nerr:\r\nup_write(&mm->mmap_sem);\r\nreturn gts;\r\n}\r\nstatic void gru_unlock_gts(struct gru_thread_state *gts)\r\n{\r\nmutex_unlock(&gts->ts_ctxlock);\r\nup_read(&current->mm->mmap_sem);\r\n}\r\nstatic void gru_cb_set_istatus_active(struct gru_instruction_bits *cbk)\r\n{\r\nif (cbk) {\r\ncbk->istatus = CBS_ACTIVE;\r\n}\r\n}\r\nstatic void get_clear_fault_map(struct gru_state *gru,\r\nstruct gru_tlb_fault_map *imap,\r\nstruct gru_tlb_fault_map *dmap)\r\n{\r\nunsigned long i, k;\r\nstruct gru_tlb_fault_map *tfm;\r\ntfm = get_tfm_for_cpu(gru, gru_cpu_fault_map_id());\r\nprefetchw(tfm);\r\nfor (i = 0; i < BITS_TO_LONGS(GRU_NUM_CBE); i++) {\r\nk = tfm->fault_bits[i];\r\nif (k)\r\nk = xchg(&tfm->fault_bits[i], 0UL);\r\nimap->fault_bits[i] = k;\r\nk = tfm->done_bits[i];\r\nif (k)\r\nk = xchg(&tfm->done_bits[i], 0UL);\r\ndmap->fault_bits[i] = k;\r\n}\r\ngru_flush_cache(tfm);\r\n}\r\nstatic int non_atomic_pte_lookup(struct vm_area_struct *vma,\r\nunsigned long vaddr, int write,\r\nunsigned long *paddr, int *pageshift)\r\n{\r\nstruct page *page;\r\n#ifdef CONFIG_HUGETLB_PAGE\r\n*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\r\n#else\r\n*pageshift = PAGE_SHIFT;\r\n#endif\r\nif (get_user_pages\r\n(current, current->mm, vaddr, 1, write, 0, &page, NULL) <= 0)\r\nreturn -EFAULT;\r\n*paddr = page_to_phys(page);\r\nput_page(page);\r\nreturn 0;\r\n}\r\nstatic int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,\r\nint write, unsigned long *paddr, int *pageshift)\r\n{\r\npgd_t *pgdp;\r\npmd_t *pmdp;\r\npud_t *pudp;\r\npte_t pte;\r\npgdp = pgd_offset(vma->vm_mm, vaddr);\r\nif (unlikely(pgd_none(*pgdp)))\r\ngoto err;\r\npudp = pud_offset(pgdp, vaddr);\r\nif (unlikely(pud_none(*pudp)))\r\ngoto err;\r\npmdp = pmd_offset(pudp, vaddr);\r\nif (unlikely(pmd_none(*pmdp)))\r\ngoto err;\r\n#ifdef CONFIG_X86_64\r\nif (unlikely(pmd_large(*pmdp)))\r\npte = *(pte_t *) pmdp;\r\nelse\r\n#endif\r\npte = *pte_offset_kernel(pmdp, vaddr);\r\nif (unlikely(!pte_present(pte) ||\r\n(write && (!pte_write(pte) || !pte_dirty(pte)))))\r\nreturn 1;\r\n*paddr = pte_pfn(pte) << PAGE_SHIFT;\r\n#ifdef CONFIG_HUGETLB_PAGE\r\n*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\r\n#else\r\n*pageshift = PAGE_SHIFT;\r\n#endif\r\nreturn 0;\r\nerr:\r\nreturn 1;\r\n}\r\nstatic int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,\r\nint write, int atomic, unsigned long *gpa, int *pageshift)\r\n{\r\nstruct mm_struct *mm = gts->ts_mm;\r\nstruct vm_area_struct *vma;\r\nunsigned long paddr;\r\nint ret, ps;\r\nvma = find_vma(mm, vaddr);\r\nif (!vma)\r\ngoto inval;\r\nrmb();\r\nret = atomic_pte_lookup(vma, vaddr, write, &paddr, &ps);\r\nif (ret) {\r\nif (atomic)\r\ngoto upm;\r\nif (non_atomic_pte_lookup(vma, vaddr, write, &paddr, &ps))\r\ngoto inval;\r\n}\r\nif (is_gru_paddr(paddr))\r\ngoto inval;\r\npaddr = paddr & ~((1UL << ps) - 1);\r\n*gpa = uv_soc_phys_ram_to_gpa(paddr);\r\n*pageshift = ps;\r\nreturn VTOP_SUCCESS;\r\ninval:\r\nreturn VTOP_INVALID;\r\nupm:\r\nreturn VTOP_RETRY;\r\n}\r\nstatic void gru_flush_cache_cbe(struct gru_control_block_extended *cbe)\r\n{\r\nif (unlikely(cbe)) {\r\ncbe->cbrexecstatus = 0;\r\ngru_flush_cache(cbe);\r\n}\r\n}\r\nstatic void gru_preload_tlb(struct gru_state *gru,\r\nstruct gru_thread_state *gts, int atomic,\r\nunsigned long fault_vaddr, int asid, int write,\r\nunsigned char tlb_preload_count,\r\nstruct gru_tlb_fault_handle *tfh,\r\nstruct gru_control_block_extended *cbe)\r\n{\r\nunsigned long vaddr = 0, gpa;\r\nint ret, pageshift;\r\nif (cbe->opccpy != OP_BCOPY)\r\nreturn;\r\nif (fault_vaddr == cbe->cbe_baddr0)\r\nvaddr = fault_vaddr + GRU_CACHE_LINE_BYTES * cbe->cbe_src_cl - 1;\r\nelse if (fault_vaddr == cbe->cbe_baddr1)\r\nvaddr = fault_vaddr + (1 << cbe->xtypecpy) * cbe->cbe_nelemcur - 1;\r\nfault_vaddr &= PAGE_MASK;\r\nvaddr &= PAGE_MASK;\r\nvaddr = min(vaddr, fault_vaddr + tlb_preload_count * PAGE_SIZE);\r\nwhile (vaddr > fault_vaddr) {\r\nret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\r\nif (ret || tfh_write_only(tfh, gpa, GAA_RAM, vaddr, asid, write,\r\nGRU_PAGESIZE(pageshift)))\r\nreturn;\r\ngru_dbg(grudev,\r\n"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, rw %d, ps %d, gpa 0x%lx\n",\r\natomic ? "atomic" : "non-atomic", gru->gs_gid, gts, tfh,\r\nvaddr, asid, write, pageshift, gpa);\r\nvaddr -= PAGE_SIZE;\r\nSTAT(tlb_preload_page);\r\n}\r\n}\r\nstatic int gru_try_dropin(struct gru_state *gru,\r\nstruct gru_thread_state *gts,\r\nstruct gru_tlb_fault_handle *tfh,\r\nstruct gru_instruction_bits *cbk)\r\n{\r\nstruct gru_control_block_extended *cbe = NULL;\r\nunsigned char tlb_preload_count = gts->ts_tlb_preload_count;\r\nint pageshift = 0, asid, write, ret, atomic = !cbk, indexway;\r\nunsigned long gpa = 0, vaddr = 0;\r\nif (unlikely(tlb_preload_count)) {\r\ncbe = gru_tfh_to_cbe(tfh);\r\nprefetchw(cbe);\r\n}\r\nif (tfh->status != TFHSTATUS_EXCEPTION) {\r\ngru_flush_cache(tfh);\r\nsync_core();\r\nif (tfh->status != TFHSTATUS_EXCEPTION)\r\ngoto failnoexception;\r\nSTAT(tfh_stale_on_fault);\r\n}\r\nif (tfh->state == TFHSTATE_IDLE)\r\ngoto failidle;\r\nif (tfh->state == TFHSTATE_MISS_FMM && cbk)\r\ngoto failfmm;\r\nwrite = (tfh->cause & TFHCAUSE_TLB_MOD) != 0;\r\nvaddr = tfh->missvaddr;\r\nasid = tfh->missasid;\r\nindexway = tfh->indexway;\r\nif (asid == 0)\r\ngoto failnoasid;\r\nrmb();\r\nif (atomic_read(&gts->ts_gms->ms_range_active))\r\ngoto failactive;\r\nret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\r\nif (ret == VTOP_INVALID)\r\ngoto failinval;\r\nif (ret == VTOP_RETRY)\r\ngoto failupm;\r\nif (!(gts->ts_sizeavail & GRU_SIZEAVAIL(pageshift))) {\r\ngts->ts_sizeavail |= GRU_SIZEAVAIL(pageshift);\r\nif (atomic || !gru_update_cch(gts)) {\r\ngts->ts_force_cch_reload = 1;\r\ngoto failupm;\r\n}\r\n}\r\nif (unlikely(cbe) && pageshift == PAGE_SHIFT) {\r\ngru_preload_tlb(gru, gts, atomic, vaddr, asid, write, tlb_preload_count, tfh, cbe);\r\ngru_flush_cache_cbe(cbe);\r\n}\r\ngru_cb_set_istatus_active(cbk);\r\ngts->ustats.tlbdropin++;\r\ntfh_write_restart(tfh, gpa, GAA_RAM, vaddr, asid, write,\r\nGRU_PAGESIZE(pageshift));\r\ngru_dbg(grudev,\r\n"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, indexway 0x%x,"\r\n" rw %d, ps %d, gpa 0x%lx\n",\r\natomic ? "atomic" : "non-atomic", gru->gs_gid, gts, tfh, vaddr, asid,\r\nindexway, write, pageshift, gpa);\r\nSTAT(tlb_dropin);\r\nreturn 0;\r\nfailnoasid:\r\nSTAT(tlb_dropin_fail_no_asid);\r\ngru_dbg(grudev, "FAILED no_asid tfh: 0x%p, vaddr 0x%lx\n", tfh, vaddr);\r\nif (!cbk)\r\ntfh_user_polling_mode(tfh);\r\nelse\r\ngru_flush_cache(tfh);\r\ngru_flush_cache_cbe(cbe);\r\nreturn -EAGAIN;\r\nfailupm:\r\ntfh_user_polling_mode(tfh);\r\ngru_flush_cache_cbe(cbe);\r\nSTAT(tlb_dropin_fail_upm);\r\ngru_dbg(grudev, "FAILED upm tfh: 0x%p, vaddr 0x%lx\n", tfh, vaddr);\r\nreturn 1;\r\nfailfmm:\r\ngru_flush_cache(tfh);\r\ngru_flush_cache_cbe(cbe);\r\nSTAT(tlb_dropin_fail_fmm);\r\ngru_dbg(grudev, "FAILED fmm tfh: 0x%p, state %d\n", tfh, tfh->state);\r\nreturn 0;\r\nfailnoexception:\r\ngru_flush_cache(tfh);\r\ngru_flush_cache_cbe(cbe);\r\nif (cbk)\r\ngru_flush_cache(cbk);\r\nSTAT(tlb_dropin_fail_no_exception);\r\ngru_dbg(grudev, "FAILED non-exception tfh: 0x%p, status %d, state %d\n",\r\ntfh, tfh->status, tfh->state);\r\nreturn 0;\r\nfailidle:\r\ngru_flush_cache(tfh);\r\ngru_flush_cache_cbe(cbe);\r\nif (cbk)\r\ngru_flush_cache(cbk);\r\nSTAT(tlb_dropin_fail_idle);\r\ngru_dbg(grudev, "FAILED idle tfh: 0x%p, state %d\n", tfh, tfh->state);\r\nreturn 0;\r\nfailinval:\r\ntfh_exception(tfh);\r\ngru_flush_cache_cbe(cbe);\r\nSTAT(tlb_dropin_fail_invalid);\r\ngru_dbg(grudev, "FAILED inval tfh: 0x%p, vaddr 0x%lx\n", tfh, vaddr);\r\nreturn -EFAULT;\r\nfailactive:\r\nif (!cbk)\r\ntfh_user_polling_mode(tfh);\r\nelse\r\ngru_flush_cache(tfh);\r\ngru_flush_cache_cbe(cbe);\r\nSTAT(tlb_dropin_fail_range_active);\r\ngru_dbg(grudev, "FAILED range active: tfh 0x%p, vaddr 0x%lx\n",\r\ntfh, vaddr);\r\nreturn 1;\r\n}\r\nstatic irqreturn_t gru_intr(int chiplet, int blade)\r\n{\r\nstruct gru_state *gru;\r\nstruct gru_tlb_fault_map imap, dmap;\r\nstruct gru_thread_state *gts;\r\nstruct gru_tlb_fault_handle *tfh = NULL;\r\nstruct completion *cmp;\r\nint cbrnum, ctxnum;\r\nSTAT(intr);\r\ngru = &gru_base[blade]->bs_grus[chiplet];\r\nif (!gru) {\r\ndev_err(grudev, "GRU: invalid interrupt: cpu %d, chiplet %d\n",\r\nraw_smp_processor_id(), chiplet);\r\nreturn IRQ_NONE;\r\n}\r\nget_clear_fault_map(gru, &imap, &dmap);\r\ngru_dbg(grudev,\r\n"cpu %d, chiplet %d, gid %d, imap %016lx %016lx, dmap %016lx %016lx\n",\r\nsmp_processor_id(), chiplet, gru->gs_gid,\r\nimap.fault_bits[0], imap.fault_bits[1],\r\ndmap.fault_bits[0], dmap.fault_bits[1]);\r\nfor_each_cbr_in_tfm(cbrnum, dmap.fault_bits) {\r\nSTAT(intr_cbr);\r\ncmp = gru->gs_blade->bs_async_wq;\r\nif (cmp)\r\ncomplete(cmp);\r\ngru_dbg(grudev, "gid %d, cbr_done %d, done %d\n",\r\ngru->gs_gid, cbrnum, cmp ? cmp->done : -1);\r\n}\r\nfor_each_cbr_in_tfm(cbrnum, imap.fault_bits) {\r\nSTAT(intr_tfh);\r\ntfh = get_tfh_by_index(gru, cbrnum);\r\nprefetchw(tfh);\r\nctxnum = tfh->ctxnum;\r\ngts = gru->gs_gts[ctxnum];\r\nif (!gts) {\r\nSTAT(intr_spurious);\r\ncontinue;\r\n}\r\ngts->ustats.fmm_tlbmiss++;\r\nif (!gts->ts_force_cch_reload &&\r\ndown_read_trylock(&gts->ts_mm->mmap_sem)) {\r\ngru_try_dropin(gru, gts, tfh, NULL);\r\nup_read(&gts->ts_mm->mmap_sem);\r\n} else {\r\ntfh_user_polling_mode(tfh);\r\nSTAT(intr_mm_lock_failed);\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nirqreturn_t gru0_intr(int irq, void *dev_id)\r\n{\r\nreturn gru_intr(0, uv_numa_blade_id());\r\n}\r\nirqreturn_t gru1_intr(int irq, void *dev_id)\r\n{\r\nreturn gru_intr(1, uv_numa_blade_id());\r\n}\r\nirqreturn_t gru_intr_mblade(int irq, void *dev_id)\r\n{\r\nint blade;\r\nfor_each_possible_blade(blade) {\r\nif (uv_blade_nr_possible_cpus(blade))\r\ncontinue;\r\ngru_intr(0, blade);\r\ngru_intr(1, blade);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int gru_user_dropin(struct gru_thread_state *gts,\r\nstruct gru_tlb_fault_handle *tfh,\r\nvoid *cb)\r\n{\r\nstruct gru_mm_struct *gms = gts->ts_gms;\r\nint ret;\r\ngts->ustats.upm_tlbmiss++;\r\nwhile (1) {\r\nwait_event(gms->ms_wait_queue,\r\natomic_read(&gms->ms_range_active) == 0);\r\nprefetchw(tfh);\r\nret = gru_try_dropin(gts->ts_gru, gts, tfh, cb);\r\nif (ret <= 0)\r\nreturn ret;\r\nSTAT(call_os_wait_queue);\r\n}\r\n}\r\nint gru_handle_user_call_os(unsigned long cb)\r\n{\r\nstruct gru_tlb_fault_handle *tfh;\r\nstruct gru_thread_state *gts;\r\nvoid *cbk;\r\nint ucbnum, cbrnum, ret = -EINVAL;\r\nSTAT(call_os);\r\nucbnum = get_cb_number((void *)cb);\r\nif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\r\nreturn -EINVAL;\r\ngts = gru_find_lock_gts(cb);\r\nif (!gts)\r\nreturn -EINVAL;\r\ngru_dbg(grudev, "address 0x%lx, gid %d, gts 0x%p\n", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\r\nif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\r\ngoto exit;\r\ngru_check_context_placement(gts);\r\nif (gts->ts_gru && gts->ts_force_cch_reload) {\r\ngts->ts_force_cch_reload = 0;\r\ngru_update_cch(gts);\r\n}\r\nret = -EAGAIN;\r\ncbrnum = thread_cbr_number(gts, ucbnum);\r\nif (gts->ts_gru) {\r\ntfh = get_tfh_by_index(gts->ts_gru, cbrnum);\r\ncbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\r\ngts->ts_ctxnum, ucbnum);\r\nret = gru_user_dropin(gts, tfh, cbk);\r\n}\r\nexit:\r\ngru_unlock_gts(gts);\r\nreturn ret;\r\n}\r\nint gru_get_exception_detail(unsigned long arg)\r\n{\r\nstruct control_block_extended_exc_detail excdet;\r\nstruct gru_control_block_extended *cbe;\r\nstruct gru_thread_state *gts;\r\nint ucbnum, cbrnum, ret;\r\nSTAT(user_exception);\r\nif (copy_from_user(&excdet, (void __user *)arg, sizeof(excdet)))\r\nreturn -EFAULT;\r\ngts = gru_find_lock_gts(excdet.cb);\r\nif (!gts)\r\nreturn -EINVAL;\r\ngru_dbg(grudev, "address 0x%lx, gid %d, gts 0x%p\n", excdet.cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\r\nucbnum = get_cb_number((void *)excdet.cb);\r\nif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE) {\r\nret = -EINVAL;\r\n} else if (gts->ts_gru) {\r\ncbrnum = thread_cbr_number(gts, ucbnum);\r\ncbe = get_cbe_by_index(gts->ts_gru, cbrnum);\r\ngru_flush_cache(cbe);\r\nsync_core();\r\nexcdet.opc = cbe->opccpy;\r\nexcdet.exopc = cbe->exopccpy;\r\nexcdet.ecause = cbe->ecause;\r\nexcdet.exceptdet0 = cbe->idef1upd;\r\nexcdet.exceptdet1 = cbe->idef3upd;\r\nexcdet.cbrstate = cbe->cbrstate;\r\nexcdet.cbrexecstatus = cbe->cbrexecstatus;\r\ngru_flush_cache_cbe(cbe);\r\nret = 0;\r\n} else {\r\nret = -EAGAIN;\r\n}\r\ngru_unlock_gts(gts);\r\ngru_dbg(grudev,\r\n"cb 0x%lx, op %d, exopc %d, cbrstate %d, cbrexecstatus 0x%x, ecause 0x%x, "\r\n"exdet0 0x%lx, exdet1 0x%x\n",\r\nexcdet.cb, excdet.opc, excdet.exopc, excdet.cbrstate, excdet.cbrexecstatus,\r\nexcdet.ecause, excdet.exceptdet0, excdet.exceptdet1);\r\nif (!ret && copy_to_user((void __user *)arg, &excdet, sizeof(excdet)))\r\nret = -EFAULT;\r\nreturn ret;\r\n}\r\nstatic int gru_unload_all_contexts(void)\r\n{\r\nstruct gru_thread_state *gts;\r\nstruct gru_state *gru;\r\nint gid, ctxnum;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nforeach_gid(gid) {\r\ngru = GID_TO_GRU(gid);\r\nspin_lock(&gru->gs_lock);\r\nfor (ctxnum = 0; ctxnum < GRU_NUM_CCH; ctxnum++) {\r\ngts = gru->gs_gts[ctxnum];\r\nif (gts && mutex_trylock(&gts->ts_ctxlock)) {\r\nspin_unlock(&gru->gs_lock);\r\ngru_unload_context(gts, 1);\r\nmutex_unlock(&gts->ts_ctxlock);\r\nspin_lock(&gru->gs_lock);\r\n}\r\n}\r\nspin_unlock(&gru->gs_lock);\r\n}\r\nreturn 0;\r\n}\r\nint gru_user_unload_context(unsigned long arg)\r\n{\r\nstruct gru_thread_state *gts;\r\nstruct gru_unload_context_req req;\r\nSTAT(user_unload_context);\r\nif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\r\nreturn -EFAULT;\r\ngru_dbg(grudev, "gseg 0x%lx\n", req.gseg);\r\nif (!req.gseg)\r\nreturn gru_unload_all_contexts();\r\ngts = gru_find_lock_gts(req.gseg);\r\nif (!gts)\r\nreturn -EINVAL;\r\nif (gts->ts_gru)\r\ngru_unload_context(gts, 1);\r\ngru_unlock_gts(gts);\r\nreturn 0;\r\n}\r\nint gru_user_flush_tlb(unsigned long arg)\r\n{\r\nstruct gru_thread_state *gts;\r\nstruct gru_flush_tlb_req req;\r\nstruct gru_mm_struct *gms;\r\nSTAT(user_flush_tlb);\r\nif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\r\nreturn -EFAULT;\r\ngru_dbg(grudev, "gseg 0x%lx, vaddr 0x%lx, len 0x%lx\n", req.gseg,\r\nreq.vaddr, req.len);\r\ngts = gru_find_lock_gts(req.gseg);\r\nif (!gts)\r\nreturn -EINVAL;\r\ngms = gts->ts_gms;\r\ngru_unlock_gts(gts);\r\ngru_flush_tlb_range(gms, req.vaddr, req.len);\r\nreturn 0;\r\n}\r\nlong gru_get_gseg_statistics(unsigned long arg)\r\n{\r\nstruct gru_thread_state *gts;\r\nstruct gru_get_gseg_statistics_req req;\r\nif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\r\nreturn -EFAULT;\r\ngts = gru_find_lock_gts(req.gseg);\r\nif (gts) {\r\nmemcpy(&req.stats, &gts->ustats, sizeof(gts->ustats));\r\ngru_unlock_gts(gts);\r\n} else {\r\nmemset(&req.stats, 0, sizeof(gts->ustats));\r\n}\r\nif (copy_to_user((void __user *)arg, &req, sizeof(req)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nint gru_set_context_option(unsigned long arg)\r\n{\r\nstruct gru_thread_state *gts;\r\nstruct gru_set_context_option_req req;\r\nint ret = 0;\r\nSTAT(set_context_option);\r\nif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\r\nreturn -EFAULT;\r\ngru_dbg(grudev, "op %d, gseg 0x%lx, value1 0x%lx\n", req.op, req.gseg, req.val1);\r\ngts = gru_find_lock_gts(req.gseg);\r\nif (!gts) {\r\ngts = gru_alloc_locked_gts(req.gseg);\r\nif (IS_ERR(gts))\r\nreturn PTR_ERR(gts);\r\n}\r\nswitch (req.op) {\r\ncase sco_blade_chiplet:\r\nif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\r\nreq.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\r\n(req.val1 >= 0 && !gru_base[req.val1])) {\r\nret = -EINVAL;\r\n} else {\r\ngts->ts_user_blade_id = req.val1;\r\ngts->ts_user_chiplet_id = req.val0;\r\ngru_check_context_placement(gts);\r\n}\r\nbreak;\r\ncase sco_gseg_owner:\r\ngts->ts_tgid_owner = current->tgid;\r\nbreak;\r\ncase sco_cch_req_slice:\r\ngts->ts_cch_req_slice = req.val1 & 3;\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\n}\r\ngru_unlock_gts(gts);\r\nreturn ret;\r\n}
