static inline unsigned char swap_count(unsigned char ent)\r\n{\r\nreturn ent & ~SWAP_HAS_CACHE;\r\n}\r\nstatic int\r\n__try_to_reclaim_swap(struct swap_info_struct *si, unsigned long offset)\r\n{\r\nswp_entry_t entry = swp_entry(si->type, offset);\r\nstruct page *page;\r\nint ret = 0;\r\npage = find_get_page(swap_address_space(entry), entry.val);\r\nif (!page)\r\nreturn 0;\r\nif (trylock_page(page)) {\r\nret = try_to_free_swap(page);\r\nunlock_page(page);\r\n}\r\npage_cache_release(page);\r\nreturn ret;\r\n}\r\nstatic int discard_swap(struct swap_info_struct *si)\r\n{\r\nstruct swap_extent *se;\r\nsector_t start_block;\r\nsector_t nr_blocks;\r\nint err = 0;\r\nse = &si->first_swap_extent;\r\nstart_block = (se->start_block + 1) << (PAGE_SHIFT - 9);\r\nnr_blocks = ((sector_t)se->nr_pages - 1) << (PAGE_SHIFT - 9);\r\nif (nr_blocks) {\r\nerr = blkdev_issue_discard(si->bdev, start_block,\r\nnr_blocks, GFP_KERNEL, 0);\r\nif (err)\r\nreturn err;\r\ncond_resched();\r\n}\r\nlist_for_each_entry(se, &si->first_swap_extent.list, list) {\r\nstart_block = se->start_block << (PAGE_SHIFT - 9);\r\nnr_blocks = (sector_t)se->nr_pages << (PAGE_SHIFT - 9);\r\nerr = blkdev_issue_discard(si->bdev, start_block,\r\nnr_blocks, GFP_KERNEL, 0);\r\nif (err)\r\nbreak;\r\ncond_resched();\r\n}\r\nreturn err;\r\n}\r\nstatic void discard_swap_cluster(struct swap_info_struct *si,\r\npgoff_t start_page, pgoff_t nr_pages)\r\n{\r\nstruct swap_extent *se = si->curr_swap_extent;\r\nint found_extent = 0;\r\nwhile (nr_pages) {\r\nstruct list_head *lh;\r\nif (se->start_page <= start_page &&\r\nstart_page < se->start_page + se->nr_pages) {\r\npgoff_t offset = start_page - se->start_page;\r\nsector_t start_block = se->start_block + offset;\r\nsector_t nr_blocks = se->nr_pages - offset;\r\nif (nr_blocks > nr_pages)\r\nnr_blocks = nr_pages;\r\nstart_page += nr_blocks;\r\nnr_pages -= nr_blocks;\r\nif (!found_extent++)\r\nsi->curr_swap_extent = se;\r\nstart_block <<= PAGE_SHIFT - 9;\r\nnr_blocks <<= PAGE_SHIFT - 9;\r\nif (blkdev_issue_discard(si->bdev, start_block,\r\nnr_blocks, GFP_NOIO, 0))\r\nbreak;\r\n}\r\nlh = se->list.next;\r\nse = list_entry(lh, struct swap_extent, list);\r\n}\r\n}\r\nstatic inline void cluster_set_flag(struct swap_cluster_info *info,\r\nunsigned int flag)\r\n{\r\ninfo->flags = flag;\r\n}\r\nstatic inline unsigned int cluster_count(struct swap_cluster_info *info)\r\n{\r\nreturn info->data;\r\n}\r\nstatic inline void cluster_set_count(struct swap_cluster_info *info,\r\nunsigned int c)\r\n{\r\ninfo->data = c;\r\n}\r\nstatic inline void cluster_set_count_flag(struct swap_cluster_info *info,\r\nunsigned int c, unsigned int f)\r\n{\r\ninfo->flags = f;\r\ninfo->data = c;\r\n}\r\nstatic inline unsigned int cluster_next(struct swap_cluster_info *info)\r\n{\r\nreturn info->data;\r\n}\r\nstatic inline void cluster_set_next(struct swap_cluster_info *info,\r\nunsigned int n)\r\n{\r\ninfo->data = n;\r\n}\r\nstatic inline void cluster_set_next_flag(struct swap_cluster_info *info,\r\nunsigned int n, unsigned int f)\r\n{\r\ninfo->flags = f;\r\ninfo->data = n;\r\n}\r\nstatic inline bool cluster_is_free(struct swap_cluster_info *info)\r\n{\r\nreturn info->flags & CLUSTER_FLAG_FREE;\r\n}\r\nstatic inline bool cluster_is_null(struct swap_cluster_info *info)\r\n{\r\nreturn info->flags & CLUSTER_FLAG_NEXT_NULL;\r\n}\r\nstatic inline void cluster_set_null(struct swap_cluster_info *info)\r\n{\r\ninfo->flags = CLUSTER_FLAG_NEXT_NULL;\r\ninfo->data = 0;\r\n}\r\nstatic void swap_cluster_schedule_discard(struct swap_info_struct *si,\r\nunsigned int idx)\r\n{\r\nmemset(si->swap_map + idx * SWAPFILE_CLUSTER,\r\nSWAP_MAP_BAD, SWAPFILE_CLUSTER);\r\nif (cluster_is_null(&si->discard_cluster_head)) {\r\ncluster_set_next_flag(&si->discard_cluster_head,\r\nidx, 0);\r\ncluster_set_next_flag(&si->discard_cluster_tail,\r\nidx, 0);\r\n} else {\r\nunsigned int tail = cluster_next(&si->discard_cluster_tail);\r\ncluster_set_next(&si->cluster_info[tail], idx);\r\ncluster_set_next_flag(&si->discard_cluster_tail,\r\nidx, 0);\r\n}\r\nschedule_work(&si->discard_work);\r\n}\r\nstatic void swap_do_scheduled_discard(struct swap_info_struct *si)\r\n{\r\nstruct swap_cluster_info *info;\r\nunsigned int idx;\r\ninfo = si->cluster_info;\r\nwhile (!cluster_is_null(&si->discard_cluster_head)) {\r\nidx = cluster_next(&si->discard_cluster_head);\r\ncluster_set_next_flag(&si->discard_cluster_head,\r\ncluster_next(&info[idx]), 0);\r\nif (cluster_next(&si->discard_cluster_tail) == idx) {\r\ncluster_set_null(&si->discard_cluster_head);\r\ncluster_set_null(&si->discard_cluster_tail);\r\n}\r\nspin_unlock(&si->lock);\r\ndiscard_swap_cluster(si, idx * SWAPFILE_CLUSTER,\r\nSWAPFILE_CLUSTER);\r\nspin_lock(&si->lock);\r\ncluster_set_flag(&info[idx], CLUSTER_FLAG_FREE);\r\nif (cluster_is_null(&si->free_cluster_head)) {\r\ncluster_set_next_flag(&si->free_cluster_head,\r\nidx, 0);\r\ncluster_set_next_flag(&si->free_cluster_tail,\r\nidx, 0);\r\n} else {\r\nunsigned int tail;\r\ntail = cluster_next(&si->free_cluster_tail);\r\ncluster_set_next(&info[tail], idx);\r\ncluster_set_next_flag(&si->free_cluster_tail,\r\nidx, 0);\r\n}\r\nmemset(si->swap_map + idx * SWAPFILE_CLUSTER,\r\n0, SWAPFILE_CLUSTER);\r\n}\r\n}\r\nstatic void swap_discard_work(struct work_struct *work)\r\n{\r\nstruct swap_info_struct *si;\r\nsi = container_of(work, struct swap_info_struct, discard_work);\r\nspin_lock(&si->lock);\r\nswap_do_scheduled_discard(si);\r\nspin_unlock(&si->lock);\r\n}\r\nstatic void inc_cluster_info_page(struct swap_info_struct *p,\r\nstruct swap_cluster_info *cluster_info, unsigned long page_nr)\r\n{\r\nunsigned long idx = page_nr / SWAPFILE_CLUSTER;\r\nif (!cluster_info)\r\nreturn;\r\nif (cluster_is_free(&cluster_info[idx])) {\r\nVM_BUG_ON(cluster_next(&p->free_cluster_head) != idx);\r\ncluster_set_next_flag(&p->free_cluster_head,\r\ncluster_next(&cluster_info[idx]), 0);\r\nif (cluster_next(&p->free_cluster_tail) == idx) {\r\ncluster_set_null(&p->free_cluster_tail);\r\ncluster_set_null(&p->free_cluster_head);\r\n}\r\ncluster_set_count_flag(&cluster_info[idx], 0, 0);\r\n}\r\nVM_BUG_ON(cluster_count(&cluster_info[idx]) >= SWAPFILE_CLUSTER);\r\ncluster_set_count(&cluster_info[idx],\r\ncluster_count(&cluster_info[idx]) + 1);\r\n}\r\nstatic void dec_cluster_info_page(struct swap_info_struct *p,\r\nstruct swap_cluster_info *cluster_info, unsigned long page_nr)\r\n{\r\nunsigned long idx = page_nr / SWAPFILE_CLUSTER;\r\nif (!cluster_info)\r\nreturn;\r\nVM_BUG_ON(cluster_count(&cluster_info[idx]) == 0);\r\ncluster_set_count(&cluster_info[idx],\r\ncluster_count(&cluster_info[idx]) - 1);\r\nif (cluster_count(&cluster_info[idx]) == 0) {\r\nif ((p->flags & (SWP_WRITEOK | SWP_PAGE_DISCARD)) ==\r\n(SWP_WRITEOK | SWP_PAGE_DISCARD)) {\r\nswap_cluster_schedule_discard(p, idx);\r\nreturn;\r\n}\r\ncluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);\r\nif (cluster_is_null(&p->free_cluster_head)) {\r\ncluster_set_next_flag(&p->free_cluster_head, idx, 0);\r\ncluster_set_next_flag(&p->free_cluster_tail, idx, 0);\r\n} else {\r\nunsigned int tail = cluster_next(&p->free_cluster_tail);\r\ncluster_set_next(&cluster_info[tail], idx);\r\ncluster_set_next_flag(&p->free_cluster_tail, idx, 0);\r\n}\r\n}\r\n}\r\nstatic bool\r\nscan_swap_map_ssd_cluster_conflict(struct swap_info_struct *si,\r\nunsigned long offset)\r\n{\r\nstruct percpu_cluster *percpu_cluster;\r\nbool conflict;\r\noffset /= SWAPFILE_CLUSTER;\r\nconflict = !cluster_is_null(&si->free_cluster_head) &&\r\noffset != cluster_next(&si->free_cluster_head) &&\r\ncluster_is_free(&si->cluster_info[offset]);\r\nif (!conflict)\r\nreturn false;\r\npercpu_cluster = this_cpu_ptr(si->percpu_cluster);\r\ncluster_set_null(&percpu_cluster->index);\r\nreturn true;\r\n}\r\nstatic void scan_swap_map_try_ssd_cluster(struct swap_info_struct *si,\r\nunsigned long *offset, unsigned long *scan_base)\r\n{\r\nstruct percpu_cluster *cluster;\r\nbool found_free;\r\nunsigned long tmp;\r\nnew_cluster:\r\ncluster = this_cpu_ptr(si->percpu_cluster);\r\nif (cluster_is_null(&cluster->index)) {\r\nif (!cluster_is_null(&si->free_cluster_head)) {\r\ncluster->index = si->free_cluster_head;\r\ncluster->next = cluster_next(&cluster->index) *\r\nSWAPFILE_CLUSTER;\r\n} else if (!cluster_is_null(&si->discard_cluster_head)) {\r\nswap_do_scheduled_discard(si);\r\n*scan_base = *offset = si->cluster_next;\r\ngoto new_cluster;\r\n} else\r\nreturn;\r\n}\r\nfound_free = false;\r\ntmp = cluster->next;\r\nwhile (tmp < si->max && tmp < (cluster_next(&cluster->index) + 1) *\r\nSWAPFILE_CLUSTER) {\r\nif (!si->swap_map[tmp]) {\r\nfound_free = true;\r\nbreak;\r\n}\r\ntmp++;\r\n}\r\nif (!found_free) {\r\ncluster_set_null(&cluster->index);\r\ngoto new_cluster;\r\n}\r\ncluster->next = tmp + 1;\r\n*offset = tmp;\r\n*scan_base = tmp;\r\n}\r\nstatic unsigned long scan_swap_map(struct swap_info_struct *si,\r\nunsigned char usage)\r\n{\r\nunsigned long offset;\r\nunsigned long scan_base;\r\nunsigned long last_in_cluster = 0;\r\nint latency_ration = LATENCY_LIMIT;\r\nsi->flags += SWP_SCANNING;\r\nscan_base = offset = si->cluster_next;\r\nif (si->cluster_info) {\r\nscan_swap_map_try_ssd_cluster(si, &offset, &scan_base);\r\ngoto checks;\r\n}\r\nif (unlikely(!si->cluster_nr--)) {\r\nif (si->pages - si->inuse_pages < SWAPFILE_CLUSTER) {\r\nsi->cluster_nr = SWAPFILE_CLUSTER - 1;\r\ngoto checks;\r\n}\r\nspin_unlock(&si->lock);\r\nscan_base = offset = si->lowest_bit;\r\nlast_in_cluster = offset + SWAPFILE_CLUSTER - 1;\r\nfor (; last_in_cluster <= si->highest_bit; offset++) {\r\nif (si->swap_map[offset])\r\nlast_in_cluster = offset + SWAPFILE_CLUSTER;\r\nelse if (offset == last_in_cluster) {\r\nspin_lock(&si->lock);\r\noffset -= SWAPFILE_CLUSTER - 1;\r\nsi->cluster_next = offset;\r\nsi->cluster_nr = SWAPFILE_CLUSTER - 1;\r\ngoto checks;\r\n}\r\nif (unlikely(--latency_ration < 0)) {\r\ncond_resched();\r\nlatency_ration = LATENCY_LIMIT;\r\n}\r\n}\r\noffset = scan_base;\r\nspin_lock(&si->lock);\r\nsi->cluster_nr = SWAPFILE_CLUSTER - 1;\r\n}\r\nchecks:\r\nif (si->cluster_info) {\r\nwhile (scan_swap_map_ssd_cluster_conflict(si, offset))\r\nscan_swap_map_try_ssd_cluster(si, &offset, &scan_base);\r\n}\r\nif (!(si->flags & SWP_WRITEOK))\r\ngoto no_page;\r\nif (!si->highest_bit)\r\ngoto no_page;\r\nif (offset > si->highest_bit)\r\nscan_base = offset = si->lowest_bit;\r\nif (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {\r\nint swap_was_freed;\r\nspin_unlock(&si->lock);\r\nswap_was_freed = __try_to_reclaim_swap(si, offset);\r\nspin_lock(&si->lock);\r\nif (swap_was_freed)\r\ngoto checks;\r\ngoto scan;\r\n}\r\nif (si->swap_map[offset])\r\ngoto scan;\r\nif (offset == si->lowest_bit)\r\nsi->lowest_bit++;\r\nif (offset == si->highest_bit)\r\nsi->highest_bit--;\r\nsi->inuse_pages++;\r\nif (si->inuse_pages == si->pages) {\r\nsi->lowest_bit = si->max;\r\nsi->highest_bit = 0;\r\nspin_lock(&swap_avail_lock);\r\nplist_del(&si->avail_list, &swap_avail_head);\r\nspin_unlock(&swap_avail_lock);\r\n}\r\nsi->swap_map[offset] = usage;\r\ninc_cluster_info_page(si, si->cluster_info, offset);\r\nsi->cluster_next = offset + 1;\r\nsi->flags -= SWP_SCANNING;\r\nreturn offset;\r\nscan:\r\nspin_unlock(&si->lock);\r\nwhile (++offset <= si->highest_bit) {\r\nif (!si->swap_map[offset]) {\r\nspin_lock(&si->lock);\r\ngoto checks;\r\n}\r\nif (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {\r\nspin_lock(&si->lock);\r\ngoto checks;\r\n}\r\nif (unlikely(--latency_ration < 0)) {\r\ncond_resched();\r\nlatency_ration = LATENCY_LIMIT;\r\n}\r\n}\r\noffset = si->lowest_bit;\r\nwhile (offset < scan_base) {\r\nif (!si->swap_map[offset]) {\r\nspin_lock(&si->lock);\r\ngoto checks;\r\n}\r\nif (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {\r\nspin_lock(&si->lock);\r\ngoto checks;\r\n}\r\nif (unlikely(--latency_ration < 0)) {\r\ncond_resched();\r\nlatency_ration = LATENCY_LIMIT;\r\n}\r\noffset++;\r\n}\r\nspin_lock(&si->lock);\r\nno_page:\r\nsi->flags -= SWP_SCANNING;\r\nreturn 0;\r\n}\r\nswp_entry_t get_swap_page(void)\r\n{\r\nstruct swap_info_struct *si, *next;\r\npgoff_t offset;\r\nif (atomic_long_read(&nr_swap_pages) <= 0)\r\ngoto noswap;\r\natomic_long_dec(&nr_swap_pages);\r\nspin_lock(&swap_avail_lock);\r\nstart_over:\r\nplist_for_each_entry_safe(si, next, &swap_avail_head, avail_list) {\r\nplist_requeue(&si->avail_list, &swap_avail_head);\r\nspin_unlock(&swap_avail_lock);\r\nspin_lock(&si->lock);\r\nif (!si->highest_bit || !(si->flags & SWP_WRITEOK)) {\r\nspin_lock(&swap_avail_lock);\r\nif (plist_node_empty(&si->avail_list)) {\r\nspin_unlock(&si->lock);\r\ngoto nextsi;\r\n}\r\nWARN(!si->highest_bit,\r\n"swap_info %d in list but !highest_bit\n",\r\nsi->type);\r\nWARN(!(si->flags & SWP_WRITEOK),\r\n"swap_info %d in list but !SWP_WRITEOK\n",\r\nsi->type);\r\nplist_del(&si->avail_list, &swap_avail_head);\r\nspin_unlock(&si->lock);\r\ngoto nextsi;\r\n}\r\noffset = scan_swap_map(si, SWAP_HAS_CACHE);\r\nspin_unlock(&si->lock);\r\nif (offset)\r\nreturn swp_entry(si->type, offset);\r\npr_debug("scan_swap_map of si %d failed to find offset\n",\r\nsi->type);\r\nspin_lock(&swap_avail_lock);\r\nnextsi:\r\nif (plist_node_empty(&next->avail_list))\r\ngoto start_over;\r\n}\r\nspin_unlock(&swap_avail_lock);\r\natomic_long_inc(&nr_swap_pages);\r\nnoswap:\r\nreturn (swp_entry_t) {0};\r\n}\r\nswp_entry_t get_swap_page_of_type(int type)\r\n{\r\nstruct swap_info_struct *si;\r\npgoff_t offset;\r\nsi = swap_info[type];\r\nspin_lock(&si->lock);\r\nif (si && (si->flags & SWP_WRITEOK)) {\r\natomic_long_dec(&nr_swap_pages);\r\noffset = scan_swap_map(si, 1);\r\nif (offset) {\r\nspin_unlock(&si->lock);\r\nreturn swp_entry(type, offset);\r\n}\r\natomic_long_inc(&nr_swap_pages);\r\n}\r\nspin_unlock(&si->lock);\r\nreturn (swp_entry_t) {0};\r\n}\r\nstatic struct swap_info_struct *swap_info_get(swp_entry_t entry)\r\n{\r\nstruct swap_info_struct *p;\r\nunsigned long offset, type;\r\nif (!entry.val)\r\ngoto out;\r\ntype = swp_type(entry);\r\nif (type >= nr_swapfiles)\r\ngoto bad_nofile;\r\np = swap_info[type];\r\nif (!(p->flags & SWP_USED))\r\ngoto bad_device;\r\noffset = swp_offset(entry);\r\nif (offset >= p->max)\r\ngoto bad_offset;\r\nif (!p->swap_map[offset])\r\ngoto bad_free;\r\nspin_lock(&p->lock);\r\nreturn p;\r\nbad_free:\r\npr_err("swap_free: %s%08lx\n", Unused_offset, entry.val);\r\ngoto out;\r\nbad_offset:\r\npr_err("swap_free: %s%08lx\n", Bad_offset, entry.val);\r\ngoto out;\r\nbad_device:\r\npr_err("swap_free: %s%08lx\n", Unused_file, entry.val);\r\ngoto out;\r\nbad_nofile:\r\npr_err("swap_free: %s%08lx\n", Bad_file, entry.val);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic unsigned char swap_entry_free(struct swap_info_struct *p,\r\nswp_entry_t entry, unsigned char usage)\r\n{\r\nunsigned long offset = swp_offset(entry);\r\nunsigned char count;\r\nunsigned char has_cache;\r\ncount = p->swap_map[offset];\r\nhas_cache = count & SWAP_HAS_CACHE;\r\ncount &= ~SWAP_HAS_CACHE;\r\nif (usage == SWAP_HAS_CACHE) {\r\nVM_BUG_ON(!has_cache);\r\nhas_cache = 0;\r\n} else if (count == SWAP_MAP_SHMEM) {\r\ncount = 0;\r\n} else if ((count & ~COUNT_CONTINUED) <= SWAP_MAP_MAX) {\r\nif (count == COUNT_CONTINUED) {\r\nif (swap_count_continued(p, offset, count))\r\ncount = SWAP_MAP_MAX | COUNT_CONTINUED;\r\nelse\r\ncount = SWAP_MAP_MAX;\r\n} else\r\ncount--;\r\n}\r\nif (!count)\r\nmem_cgroup_uncharge_swap(entry);\r\nusage = count | has_cache;\r\np->swap_map[offset] = usage;\r\nif (!usage) {\r\ndec_cluster_info_page(p, p->cluster_info, offset);\r\nif (offset < p->lowest_bit)\r\np->lowest_bit = offset;\r\nif (offset > p->highest_bit) {\r\nbool was_full = !p->highest_bit;\r\np->highest_bit = offset;\r\nif (was_full && (p->flags & SWP_WRITEOK)) {\r\nspin_lock(&swap_avail_lock);\r\nWARN_ON(!plist_node_empty(&p->avail_list));\r\nif (plist_node_empty(&p->avail_list))\r\nplist_add(&p->avail_list,\r\n&swap_avail_head);\r\nspin_unlock(&swap_avail_lock);\r\n}\r\n}\r\natomic_long_inc(&nr_swap_pages);\r\np->inuse_pages--;\r\nfrontswap_invalidate_page(p->type, offset);\r\nif (p->flags & SWP_BLKDEV) {\r\nstruct gendisk *disk = p->bdev->bd_disk;\r\nif (disk->fops->swap_slot_free_notify)\r\ndisk->fops->swap_slot_free_notify(p->bdev,\r\noffset);\r\n}\r\n}\r\nreturn usage;\r\n}\r\nvoid swap_free(swp_entry_t entry)\r\n{\r\nstruct swap_info_struct *p;\r\np = swap_info_get(entry);\r\nif (p) {\r\nswap_entry_free(p, entry, 1);\r\nspin_unlock(&p->lock);\r\n}\r\n}\r\nvoid swapcache_free(swp_entry_t entry)\r\n{\r\nstruct swap_info_struct *p;\r\np = swap_info_get(entry);\r\nif (p) {\r\nswap_entry_free(p, entry, SWAP_HAS_CACHE);\r\nspin_unlock(&p->lock);\r\n}\r\n}\r\nint page_swapcount(struct page *page)\r\n{\r\nint count = 0;\r\nstruct swap_info_struct *p;\r\nswp_entry_t entry;\r\nentry.val = page_private(page);\r\np = swap_info_get(entry);\r\nif (p) {\r\ncount = swap_count(p->swap_map[swp_offset(entry)]);\r\nspin_unlock(&p->lock);\r\n}\r\nreturn count;\r\n}\r\nint swp_swapcount(swp_entry_t entry)\r\n{\r\nint count, tmp_count, n;\r\nstruct swap_info_struct *p;\r\nstruct page *page;\r\npgoff_t offset;\r\nunsigned char *map;\r\np = swap_info_get(entry);\r\nif (!p)\r\nreturn 0;\r\ncount = swap_count(p->swap_map[swp_offset(entry)]);\r\nif (!(count & COUNT_CONTINUED))\r\ngoto out;\r\ncount &= ~COUNT_CONTINUED;\r\nn = SWAP_MAP_MAX + 1;\r\noffset = swp_offset(entry);\r\npage = vmalloc_to_page(p->swap_map + offset);\r\noffset &= ~PAGE_MASK;\r\nVM_BUG_ON(page_private(page) != SWP_CONTINUED);\r\ndo {\r\npage = list_entry(page->lru.next, struct page, lru);\r\nmap = kmap_atomic(page);\r\ntmp_count = map[offset];\r\nkunmap_atomic(map);\r\ncount += (tmp_count & ~COUNT_CONTINUED) * n;\r\nn *= (SWAP_CONT_MAX + 1);\r\n} while (tmp_count & COUNT_CONTINUED);\r\nout:\r\nspin_unlock(&p->lock);\r\nreturn count;\r\n}\r\nint reuse_swap_page(struct page *page)\r\n{\r\nint count;\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nif (unlikely(PageKsm(page)))\r\nreturn 0;\r\ncount = page_mapcount(page);\r\nif (count <= 1 && PageSwapCache(page)) {\r\ncount += page_swapcount(page);\r\nif (count == 1 && !PageWriteback(page)) {\r\ndelete_from_swap_cache(page);\r\nSetPageDirty(page);\r\n}\r\n}\r\nreturn count <= 1;\r\n}\r\nint try_to_free_swap(struct page *page)\r\n{\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nif (!PageSwapCache(page))\r\nreturn 0;\r\nif (PageWriteback(page))\r\nreturn 0;\r\nif (page_swapcount(page))\r\nreturn 0;\r\nif (pm_suspended_storage())\r\nreturn 0;\r\ndelete_from_swap_cache(page);\r\nSetPageDirty(page);\r\nreturn 1;\r\n}\r\nint free_swap_and_cache(swp_entry_t entry)\r\n{\r\nstruct swap_info_struct *p;\r\nstruct page *page = NULL;\r\nif (non_swap_entry(entry))\r\nreturn 1;\r\np = swap_info_get(entry);\r\nif (p) {\r\nif (swap_entry_free(p, entry, 1) == SWAP_HAS_CACHE) {\r\npage = find_get_page(swap_address_space(entry),\r\nentry.val);\r\nif (page && !trylock_page(page)) {\r\npage_cache_release(page);\r\npage = NULL;\r\n}\r\n}\r\nspin_unlock(&p->lock);\r\n}\r\nif (page) {\r\nif (PageSwapCache(page) && !PageWriteback(page) &&\r\n(!page_mapped(page) || vm_swap_full())) {\r\ndelete_from_swap_cache(page);\r\nSetPageDirty(page);\r\n}\r\nunlock_page(page);\r\npage_cache_release(page);\r\n}\r\nreturn p != NULL;\r\n}\r\nint swap_type_of(dev_t device, sector_t offset, struct block_device **bdev_p)\r\n{\r\nstruct block_device *bdev = NULL;\r\nint type;\r\nif (device)\r\nbdev = bdget(device);\r\nspin_lock(&swap_lock);\r\nfor (type = 0; type < nr_swapfiles; type++) {\r\nstruct swap_info_struct *sis = swap_info[type];\r\nif (!(sis->flags & SWP_WRITEOK))\r\ncontinue;\r\nif (!bdev) {\r\nif (bdev_p)\r\n*bdev_p = bdgrab(sis->bdev);\r\nspin_unlock(&swap_lock);\r\nreturn type;\r\n}\r\nif (bdev == sis->bdev) {\r\nstruct swap_extent *se = &sis->first_swap_extent;\r\nif (se->start_block == offset) {\r\nif (bdev_p)\r\n*bdev_p = bdgrab(sis->bdev);\r\nspin_unlock(&swap_lock);\r\nbdput(bdev);\r\nreturn type;\r\n}\r\n}\r\n}\r\nspin_unlock(&swap_lock);\r\nif (bdev)\r\nbdput(bdev);\r\nreturn -ENODEV;\r\n}\r\nsector_t swapdev_block(int type, pgoff_t offset)\r\n{\r\nstruct block_device *bdev;\r\nif ((unsigned int)type >= nr_swapfiles)\r\nreturn 0;\r\nif (!(swap_info[type]->flags & SWP_WRITEOK))\r\nreturn 0;\r\nreturn map_swap_entry(swp_entry(type, offset), &bdev);\r\n}\r\nunsigned int count_swap_pages(int type, int free)\r\n{\r\nunsigned int n = 0;\r\nspin_lock(&swap_lock);\r\nif ((unsigned int)type < nr_swapfiles) {\r\nstruct swap_info_struct *sis = swap_info[type];\r\nspin_lock(&sis->lock);\r\nif (sis->flags & SWP_WRITEOK) {\r\nn = sis->pages;\r\nif (free)\r\nn -= sis->inuse_pages;\r\n}\r\nspin_unlock(&sis->lock);\r\n}\r\nspin_unlock(&swap_lock);\r\nreturn n;\r\n}\r\nstatic inline int maybe_same_pte(pte_t pte, pte_t swp_pte)\r\n{\r\n#ifdef CONFIG_MEM_SOFT_DIRTY\r\npte_t swp_pte_dirty = pte_swp_mksoft_dirty(swp_pte);\r\nreturn pte_same(pte, swp_pte) || pte_same(pte, swp_pte_dirty);\r\n#else\r\nreturn pte_same(pte, swp_pte);\r\n#endif\r\n}\r\nstatic int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,\r\nunsigned long addr, swp_entry_t entry, struct page *page)\r\n{\r\nstruct page *swapcache;\r\nstruct mem_cgroup *memcg;\r\nspinlock_t *ptl;\r\npte_t *pte;\r\nint ret = 1;\r\nswapcache = page;\r\npage = ksm_might_need_to_copy(page, vma, addr);\r\nif (unlikely(!page))\r\nreturn -ENOMEM;\r\nif (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL, &memcg)) {\r\nret = -ENOMEM;\r\ngoto out_nolock;\r\n}\r\npte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\r\nif (unlikely(!maybe_same_pte(*pte, swp_entry_to_pte(entry)))) {\r\nmem_cgroup_cancel_charge(page, memcg);\r\nret = 0;\r\ngoto out;\r\n}\r\ndec_mm_counter(vma->vm_mm, MM_SWAPENTS);\r\ninc_mm_counter(vma->vm_mm, MM_ANONPAGES);\r\nget_page(page);\r\nset_pte_at(vma->vm_mm, addr, pte,\r\npte_mkold(mk_pte(page, vma->vm_page_prot)));\r\nif (page == swapcache) {\r\npage_add_anon_rmap(page, vma, addr);\r\nmem_cgroup_commit_charge(page, memcg, true);\r\n} else {\r\npage_add_new_anon_rmap(page, vma, addr);\r\nmem_cgroup_commit_charge(page, memcg, false);\r\nlru_cache_add_active_or_unevictable(page, vma);\r\n}\r\nswap_free(entry);\r\nactivate_page(page);\r\nout:\r\npte_unmap_unlock(pte, ptl);\r\nout_nolock:\r\nif (page != swapcache) {\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\nreturn ret;\r\n}\r\nstatic int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\r\nunsigned long addr, unsigned long end,\r\nswp_entry_t entry, struct page *page)\r\n{\r\npte_t swp_pte = swp_entry_to_pte(entry);\r\npte_t *pte;\r\nint ret = 0;\r\npte = pte_offset_map(pmd, addr);\r\ndo {\r\nif (unlikely(maybe_same_pte(*pte, swp_pte))) {\r\npte_unmap(pte);\r\nret = unuse_pte(vma, pmd, addr, entry, page);\r\nif (ret)\r\ngoto out;\r\npte = pte_offset_map(pmd, addr);\r\n}\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\npte_unmap(pte - 1);\r\nout:\r\nreturn ret;\r\n}\r\nstatic inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,\r\nunsigned long addr, unsigned long end,\r\nswp_entry_t entry, struct page *page)\r\n{\r\npmd_t *pmd;\r\nunsigned long next;\r\nint ret;\r\npmd = pmd_offset(pud, addr);\r\ndo {\r\nnext = pmd_addr_end(addr, end);\r\nif (pmd_none_or_trans_huge_or_clear_bad(pmd))\r\ncontinue;\r\nret = unuse_pte_range(vma, pmd, addr, next, entry, page);\r\nif (ret)\r\nreturn ret;\r\n} while (pmd++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nstatic inline int unuse_pud_range(struct vm_area_struct *vma, pgd_t *pgd,\r\nunsigned long addr, unsigned long end,\r\nswp_entry_t entry, struct page *page)\r\n{\r\npud_t *pud;\r\nunsigned long next;\r\nint ret;\r\npud = pud_offset(pgd, addr);\r\ndo {\r\nnext = pud_addr_end(addr, end);\r\nif (pud_none_or_clear_bad(pud))\r\ncontinue;\r\nret = unuse_pmd_range(vma, pud, addr, next, entry, page);\r\nif (ret)\r\nreturn ret;\r\n} while (pud++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nstatic int unuse_vma(struct vm_area_struct *vma,\r\nswp_entry_t entry, struct page *page)\r\n{\r\npgd_t *pgd;\r\nunsigned long addr, end, next;\r\nint ret;\r\nif (page_anon_vma(page)) {\r\naddr = page_address_in_vma(page, vma);\r\nif (addr == -EFAULT)\r\nreturn 0;\r\nelse\r\nend = addr + PAGE_SIZE;\r\n} else {\r\naddr = vma->vm_start;\r\nend = vma->vm_end;\r\n}\r\npgd = pgd_offset(vma->vm_mm, addr);\r\ndo {\r\nnext = pgd_addr_end(addr, end);\r\nif (pgd_none_or_clear_bad(pgd))\r\ncontinue;\r\nret = unuse_pud_range(vma, pgd, addr, next, entry, page);\r\nif (ret)\r\nreturn ret;\r\n} while (pgd++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nstatic int unuse_mm(struct mm_struct *mm,\r\nswp_entry_t entry, struct page *page)\r\n{\r\nstruct vm_area_struct *vma;\r\nint ret = 0;\r\nif (!down_read_trylock(&mm->mmap_sem)) {\r\nactivate_page(page);\r\nunlock_page(page);\r\ndown_read(&mm->mmap_sem);\r\nlock_page(page);\r\n}\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next) {\r\nif (vma->anon_vma && (ret = unuse_vma(vma, entry, page)))\r\nbreak;\r\n}\r\nup_read(&mm->mmap_sem);\r\nreturn (ret < 0)? ret: 0;\r\n}\r\nstatic unsigned int find_next_to_unuse(struct swap_info_struct *si,\r\nunsigned int prev, bool frontswap)\r\n{\r\nunsigned int max = si->max;\r\nunsigned int i = prev;\r\nunsigned char count;\r\nfor (;;) {\r\nif (++i >= max) {\r\nif (!prev) {\r\ni = 0;\r\nbreak;\r\n}\r\nmax = prev + 1;\r\nprev = 0;\r\ni = 1;\r\n}\r\nif (frontswap) {\r\nif (frontswap_test(si, i))\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\ncount = READ_ONCE(si->swap_map[i]);\r\nif (count && swap_count(count) != SWAP_MAP_BAD)\r\nbreak;\r\n}\r\nreturn i;\r\n}\r\nint try_to_unuse(unsigned int type, bool frontswap,\r\nunsigned long pages_to_unuse)\r\n{\r\nstruct swap_info_struct *si = swap_info[type];\r\nstruct mm_struct *start_mm;\r\nvolatile unsigned char *swap_map;\r\nunsigned char swcount;\r\nstruct page *page;\r\nswp_entry_t entry;\r\nunsigned int i = 0;\r\nint retval = 0;\r\nstart_mm = &init_mm;\r\natomic_inc(&init_mm.mm_users);\r\nwhile ((i = find_next_to_unuse(si, i, frontswap)) != 0) {\r\nif (signal_pending(current)) {\r\nretval = -EINTR;\r\nbreak;\r\n}\r\nswap_map = &si->swap_map[i];\r\nentry = swp_entry(type, i);\r\npage = read_swap_cache_async(entry,\r\nGFP_HIGHUSER_MOVABLE, NULL, 0);\r\nif (!page) {\r\nswcount = *swap_map;\r\nif (!swcount || swcount == SWAP_MAP_BAD)\r\ncontinue;\r\nretval = -ENOMEM;\r\nbreak;\r\n}\r\nif (atomic_read(&start_mm->mm_users) == 1) {\r\nmmput(start_mm);\r\nstart_mm = &init_mm;\r\natomic_inc(&init_mm.mm_users);\r\n}\r\nwait_on_page_locked(page);\r\nwait_on_page_writeback(page);\r\nlock_page(page);\r\nwait_on_page_writeback(page);\r\nswcount = *swap_map;\r\nif (swap_count(swcount) == SWAP_MAP_SHMEM) {\r\nretval = shmem_unuse(entry, page);\r\nif (retval < 0)\r\nbreak;\r\ncontinue;\r\n}\r\nif (swap_count(swcount) && start_mm != &init_mm)\r\nretval = unuse_mm(start_mm, entry, page);\r\nif (swap_count(*swap_map)) {\r\nint set_start_mm = (*swap_map >= swcount);\r\nstruct list_head *p = &start_mm->mmlist;\r\nstruct mm_struct *new_start_mm = start_mm;\r\nstruct mm_struct *prev_mm = start_mm;\r\nstruct mm_struct *mm;\r\natomic_inc(&new_start_mm->mm_users);\r\natomic_inc(&prev_mm->mm_users);\r\nspin_lock(&mmlist_lock);\r\nwhile (swap_count(*swap_map) && !retval &&\r\n(p = p->next) != &start_mm->mmlist) {\r\nmm = list_entry(p, struct mm_struct, mmlist);\r\nif (!atomic_inc_not_zero(&mm->mm_users))\r\ncontinue;\r\nspin_unlock(&mmlist_lock);\r\nmmput(prev_mm);\r\nprev_mm = mm;\r\ncond_resched();\r\nswcount = *swap_map;\r\nif (!swap_count(swcount))\r\n;\r\nelse if (mm == &init_mm)\r\nset_start_mm = 1;\r\nelse\r\nretval = unuse_mm(mm, entry, page);\r\nif (set_start_mm && *swap_map < swcount) {\r\nmmput(new_start_mm);\r\natomic_inc(&mm->mm_users);\r\nnew_start_mm = mm;\r\nset_start_mm = 0;\r\n}\r\nspin_lock(&mmlist_lock);\r\n}\r\nspin_unlock(&mmlist_lock);\r\nmmput(prev_mm);\r\nmmput(start_mm);\r\nstart_mm = new_start_mm;\r\n}\r\nif (retval) {\r\nunlock_page(page);\r\npage_cache_release(page);\r\nbreak;\r\n}\r\nif (swap_count(*swap_map) &&\r\nPageDirty(page) && PageSwapCache(page)) {\r\nstruct writeback_control wbc = {\r\n.sync_mode = WB_SYNC_NONE,\r\n};\r\nswap_writepage(page, &wbc);\r\nlock_page(page);\r\nwait_on_page_writeback(page);\r\n}\r\nif (PageSwapCache(page) &&\r\nlikely(page_private(page) == entry.val))\r\ndelete_from_swap_cache(page);\r\nSetPageDirty(page);\r\nunlock_page(page);\r\npage_cache_release(page);\r\ncond_resched();\r\nif (frontswap && pages_to_unuse > 0) {\r\nif (!--pages_to_unuse)\r\nbreak;\r\n}\r\n}\r\nmmput(start_mm);\r\nreturn retval;\r\n}\r\nstatic void drain_mmlist(void)\r\n{\r\nstruct list_head *p, *next;\r\nunsigned int type;\r\nfor (type = 0; type < nr_swapfiles; type++)\r\nif (swap_info[type]->inuse_pages)\r\nreturn;\r\nspin_lock(&mmlist_lock);\r\nlist_for_each_safe(p, next, &init_mm.mmlist)\r\nlist_del_init(p);\r\nspin_unlock(&mmlist_lock);\r\n}\r\nstatic sector_t map_swap_entry(swp_entry_t entry, struct block_device **bdev)\r\n{\r\nstruct swap_info_struct *sis;\r\nstruct swap_extent *start_se;\r\nstruct swap_extent *se;\r\npgoff_t offset;\r\nsis = swap_info[swp_type(entry)];\r\n*bdev = sis->bdev;\r\noffset = swp_offset(entry);\r\nstart_se = sis->curr_swap_extent;\r\nse = start_se;\r\nfor ( ; ; ) {\r\nstruct list_head *lh;\r\nif (se->start_page <= offset &&\r\noffset < (se->start_page + se->nr_pages)) {\r\nreturn se->start_block + (offset - se->start_page);\r\n}\r\nlh = se->list.next;\r\nse = list_entry(lh, struct swap_extent, list);\r\nsis->curr_swap_extent = se;\r\nBUG_ON(se == start_se);\r\n}\r\n}\r\nsector_t map_swap_page(struct page *page, struct block_device **bdev)\r\n{\r\nswp_entry_t entry;\r\nentry.val = page_private(page);\r\nreturn map_swap_entry(entry, bdev);\r\n}\r\nstatic void destroy_swap_extents(struct swap_info_struct *sis)\r\n{\r\nwhile (!list_empty(&sis->first_swap_extent.list)) {\r\nstruct swap_extent *se;\r\nse = list_entry(sis->first_swap_extent.list.next,\r\nstruct swap_extent, list);\r\nlist_del(&se->list);\r\nkfree(se);\r\n}\r\nif (sis->flags & SWP_FILE) {\r\nstruct file *swap_file = sis->swap_file;\r\nstruct address_space *mapping = swap_file->f_mapping;\r\nsis->flags &= ~SWP_FILE;\r\nmapping->a_ops->swap_deactivate(swap_file);\r\n}\r\n}\r\nint\r\nadd_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\r\nunsigned long nr_pages, sector_t start_block)\r\n{\r\nstruct swap_extent *se;\r\nstruct swap_extent *new_se;\r\nstruct list_head *lh;\r\nif (start_page == 0) {\r\nse = &sis->first_swap_extent;\r\nsis->curr_swap_extent = se;\r\nse->start_page = 0;\r\nse->nr_pages = nr_pages;\r\nse->start_block = start_block;\r\nreturn 1;\r\n} else {\r\nlh = sis->first_swap_extent.list.prev;\r\nse = list_entry(lh, struct swap_extent, list);\r\nBUG_ON(se->start_page + se->nr_pages != start_page);\r\nif (se->start_block + se->nr_pages == start_block) {\r\nse->nr_pages += nr_pages;\r\nreturn 0;\r\n}\r\n}\r\nnew_se = kmalloc(sizeof(*se), GFP_KERNEL);\r\nif (new_se == NULL)\r\nreturn -ENOMEM;\r\nnew_se->start_page = start_page;\r\nnew_se->nr_pages = nr_pages;\r\nnew_se->start_block = start_block;\r\nlist_add_tail(&new_se->list, &sis->first_swap_extent.list);\r\nreturn 1;\r\n}\r\nstatic int setup_swap_extents(struct swap_info_struct *sis, sector_t *span)\r\n{\r\nstruct file *swap_file = sis->swap_file;\r\nstruct address_space *mapping = swap_file->f_mapping;\r\nstruct inode *inode = mapping->host;\r\nint ret;\r\nif (S_ISBLK(inode->i_mode)) {\r\nret = add_swap_extent(sis, 0, sis->max, 0);\r\n*span = sis->pages;\r\nreturn ret;\r\n}\r\nif (mapping->a_ops->swap_activate) {\r\nret = mapping->a_ops->swap_activate(sis, swap_file, span);\r\nif (!ret) {\r\nsis->flags |= SWP_FILE;\r\nret = add_swap_extent(sis, 0, sis->max, 0);\r\n*span = sis->pages;\r\n}\r\nreturn ret;\r\n}\r\nreturn generic_swapfile_activate(sis, swap_file, span);\r\n}\r\nstatic void _enable_swap_info(struct swap_info_struct *p, int prio,\r\nunsigned char *swap_map,\r\nstruct swap_cluster_info *cluster_info)\r\n{\r\nif (prio >= 0)\r\np->prio = prio;\r\nelse\r\np->prio = --least_priority;\r\np->list.prio = -p->prio;\r\np->avail_list.prio = -p->prio;\r\np->swap_map = swap_map;\r\np->cluster_info = cluster_info;\r\np->flags |= SWP_WRITEOK;\r\natomic_long_add(p->pages, &nr_swap_pages);\r\ntotal_swap_pages += p->pages;\r\nassert_spin_locked(&swap_lock);\r\nplist_add(&p->list, &swap_active_head);\r\nspin_lock(&swap_avail_lock);\r\nplist_add(&p->avail_list, &swap_avail_head);\r\nspin_unlock(&swap_avail_lock);\r\n}\r\nstatic void enable_swap_info(struct swap_info_struct *p, int prio,\r\nunsigned char *swap_map,\r\nstruct swap_cluster_info *cluster_info,\r\nunsigned long *frontswap_map)\r\n{\r\nfrontswap_init(p->type, frontswap_map);\r\nspin_lock(&swap_lock);\r\nspin_lock(&p->lock);\r\n_enable_swap_info(p, prio, swap_map, cluster_info);\r\nspin_unlock(&p->lock);\r\nspin_unlock(&swap_lock);\r\n}\r\nstatic void reinsert_swap_info(struct swap_info_struct *p)\r\n{\r\nspin_lock(&swap_lock);\r\nspin_lock(&p->lock);\r\n_enable_swap_info(p, p->prio, p->swap_map, p->cluster_info);\r\nspin_unlock(&p->lock);\r\nspin_unlock(&swap_lock);\r\n}\r\nstatic unsigned swaps_poll(struct file *file, poll_table *wait)\r\n{\r\nstruct seq_file *seq = file->private_data;\r\npoll_wait(file, &proc_poll_wait, wait);\r\nif (seq->poll_event != atomic_read(&proc_poll_event)) {\r\nseq->poll_event = atomic_read(&proc_poll_event);\r\nreturn POLLIN | POLLRDNORM | POLLERR | POLLPRI;\r\n}\r\nreturn POLLIN | POLLRDNORM;\r\n}\r\nstatic void *swap_start(struct seq_file *swap, loff_t *pos)\r\n{\r\nstruct swap_info_struct *si;\r\nint type;\r\nloff_t l = *pos;\r\nmutex_lock(&swapon_mutex);\r\nif (!l)\r\nreturn SEQ_START_TOKEN;\r\nfor (type = 0; type < nr_swapfiles; type++) {\r\nsmp_rmb();\r\nsi = swap_info[type];\r\nif (!(si->flags & SWP_USED) || !si->swap_map)\r\ncontinue;\r\nif (!--l)\r\nreturn si;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void *swap_next(struct seq_file *swap, void *v, loff_t *pos)\r\n{\r\nstruct swap_info_struct *si = v;\r\nint type;\r\nif (v == SEQ_START_TOKEN)\r\ntype = 0;\r\nelse\r\ntype = si->type + 1;\r\nfor (; type < nr_swapfiles; type++) {\r\nsmp_rmb();\r\nsi = swap_info[type];\r\nif (!(si->flags & SWP_USED) || !si->swap_map)\r\ncontinue;\r\n++*pos;\r\nreturn si;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void swap_stop(struct seq_file *swap, void *v)\r\n{\r\nmutex_unlock(&swapon_mutex);\r\n}\r\nstatic int swap_show(struct seq_file *swap, void *v)\r\n{\r\nstruct swap_info_struct *si = v;\r\nstruct file *file;\r\nint len;\r\nif (si == SEQ_START_TOKEN) {\r\nseq_puts(swap,"Filename\t\t\t\tType\t\tSize\tUsed\tPriority\n");\r\nreturn 0;\r\n}\r\nfile = si->swap_file;\r\nlen = seq_file_path(swap, file, " \t\n\\");\r\nseq_printf(swap, "%*s%s\t%u\t%u\t%d\n",\r\nlen < 40 ? 40 - len : 1, " ",\r\nS_ISBLK(file_inode(file)->i_mode) ?\r\n"partition" : "file\t",\r\nsi->pages << (PAGE_SHIFT - 10),\r\nsi->inuse_pages << (PAGE_SHIFT - 10),\r\nsi->prio);\r\nreturn 0;\r\n}\r\nstatic int swaps_open(struct inode *inode, struct file *file)\r\n{\r\nstruct seq_file *seq;\r\nint ret;\r\nret = seq_open(file, &swaps_op);\r\nif (ret)\r\nreturn ret;\r\nseq = file->private_data;\r\nseq->poll_event = atomic_read(&proc_poll_event);\r\nreturn 0;\r\n}\r\nstatic int __init procswaps_init(void)\r\n{\r\nproc_create("swaps", 0, NULL, &proc_swaps_operations);\r\nreturn 0;\r\n}\r\nstatic int __init max_swapfiles_check(void)\r\n{\r\nMAX_SWAPFILES_CHECK();\r\nreturn 0;\r\n}\r\nstatic struct swap_info_struct *alloc_swap_info(void)\r\n{\r\nstruct swap_info_struct *p;\r\nunsigned int type;\r\np = kzalloc(sizeof(*p), GFP_KERNEL);\r\nif (!p)\r\nreturn ERR_PTR(-ENOMEM);\r\nspin_lock(&swap_lock);\r\nfor (type = 0; type < nr_swapfiles; type++) {\r\nif (!(swap_info[type]->flags & SWP_USED))\r\nbreak;\r\n}\r\nif (type >= MAX_SWAPFILES) {\r\nspin_unlock(&swap_lock);\r\nkfree(p);\r\nreturn ERR_PTR(-EPERM);\r\n}\r\nif (type >= nr_swapfiles) {\r\np->type = type;\r\nswap_info[type] = p;\r\nsmp_wmb();\r\nnr_swapfiles++;\r\n} else {\r\nkfree(p);\r\np = swap_info[type];\r\n}\r\nINIT_LIST_HEAD(&p->first_swap_extent.list);\r\nplist_node_init(&p->list, 0);\r\nplist_node_init(&p->avail_list, 0);\r\np->flags = SWP_USED;\r\nspin_unlock(&swap_lock);\r\nspin_lock_init(&p->lock);\r\nreturn p;\r\n}\r\nstatic int claim_swapfile(struct swap_info_struct *p, struct inode *inode)\r\n{\r\nint error;\r\nif (S_ISBLK(inode->i_mode)) {\r\np->bdev = bdgrab(I_BDEV(inode));\r\nerror = blkdev_get(p->bdev,\r\nFMODE_READ | FMODE_WRITE | FMODE_EXCL, p);\r\nif (error < 0) {\r\np->bdev = NULL;\r\nreturn error;\r\n}\r\np->old_block_size = block_size(p->bdev);\r\nerror = set_blocksize(p->bdev, PAGE_SIZE);\r\nif (error < 0)\r\nreturn error;\r\np->flags |= SWP_BLKDEV;\r\n} else if (S_ISREG(inode->i_mode)) {\r\np->bdev = inode->i_sb->s_bdev;\r\nmutex_lock(&inode->i_mutex);\r\nif (IS_SWAPFILE(inode))\r\nreturn -EBUSY;\r\n} else\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic unsigned long read_swap_header(struct swap_info_struct *p,\r\nunion swap_header *swap_header,\r\nstruct inode *inode)\r\n{\r\nint i;\r\nunsigned long maxpages;\r\nunsigned long swapfilepages;\r\nunsigned long last_page;\r\nif (memcmp("SWAPSPACE2", swap_header->magic.magic, 10)) {\r\npr_err("Unable to find swap-space signature\n");\r\nreturn 0;\r\n}\r\nif (swab32(swap_header->info.version) == 1) {\r\nswab32s(&swap_header->info.version);\r\nswab32s(&swap_header->info.last_page);\r\nswab32s(&swap_header->info.nr_badpages);\r\nfor (i = 0; i < swap_header->info.nr_badpages; i++)\r\nswab32s(&swap_header->info.badpages[i]);\r\n}\r\nif (swap_header->info.version != 1) {\r\npr_warn("Unable to handle swap header version %d\n",\r\nswap_header->info.version);\r\nreturn 0;\r\n}\r\np->lowest_bit = 1;\r\np->cluster_next = 1;\r\np->cluster_nr = 0;\r\nmaxpages = swp_offset(pte_to_swp_entry(\r\nswp_entry_to_pte(swp_entry(0, ~0UL)))) + 1;\r\nlast_page = swap_header->info.last_page;\r\nif (last_page > maxpages) {\r\npr_warn("Truncating oversized swap area, only using %luk out of %luk\n",\r\nmaxpages << (PAGE_SHIFT - 10),\r\nlast_page << (PAGE_SHIFT - 10));\r\n}\r\nif (maxpages > last_page) {\r\nmaxpages = last_page + 1;\r\nif ((unsigned int)maxpages == 0)\r\nmaxpages = UINT_MAX;\r\n}\r\np->highest_bit = maxpages - 1;\r\nif (!maxpages)\r\nreturn 0;\r\nswapfilepages = i_size_read(inode) >> PAGE_SHIFT;\r\nif (swapfilepages && maxpages > swapfilepages) {\r\npr_warn("Swap area shorter than signature indicates\n");\r\nreturn 0;\r\n}\r\nif (swap_header->info.nr_badpages && S_ISREG(inode->i_mode))\r\nreturn 0;\r\nif (swap_header->info.nr_badpages > MAX_SWAP_BADPAGES)\r\nreturn 0;\r\nreturn maxpages;\r\n}\r\nstatic int setup_swap_map_and_extents(struct swap_info_struct *p,\r\nunion swap_header *swap_header,\r\nunsigned char *swap_map,\r\nstruct swap_cluster_info *cluster_info,\r\nunsigned long maxpages,\r\nsector_t *span)\r\n{\r\nint i;\r\nunsigned int nr_good_pages;\r\nint nr_extents;\r\nunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\r\nunsigned long idx = p->cluster_next / SWAPFILE_CLUSTER;\r\nnr_good_pages = maxpages - 1;\r\ncluster_set_null(&p->free_cluster_head);\r\ncluster_set_null(&p->free_cluster_tail);\r\ncluster_set_null(&p->discard_cluster_head);\r\ncluster_set_null(&p->discard_cluster_tail);\r\nfor (i = 0; i < swap_header->info.nr_badpages; i++) {\r\nunsigned int page_nr = swap_header->info.badpages[i];\r\nif (page_nr == 0 || page_nr > swap_header->info.last_page)\r\nreturn -EINVAL;\r\nif (page_nr < maxpages) {\r\nswap_map[page_nr] = SWAP_MAP_BAD;\r\nnr_good_pages--;\r\ninc_cluster_info_page(p, cluster_info, page_nr);\r\n}\r\n}\r\nfor (i = maxpages; i < round_up(maxpages, SWAPFILE_CLUSTER); i++)\r\ninc_cluster_info_page(p, cluster_info, i);\r\nif (nr_good_pages) {\r\nswap_map[0] = SWAP_MAP_BAD;\r\ninc_cluster_info_page(p, cluster_info, 0);\r\np->max = maxpages;\r\np->pages = nr_good_pages;\r\nnr_extents = setup_swap_extents(p, span);\r\nif (nr_extents < 0)\r\nreturn nr_extents;\r\nnr_good_pages = p->pages;\r\n}\r\nif (!nr_good_pages) {\r\npr_warn("Empty swap-file\n");\r\nreturn -EINVAL;\r\n}\r\nif (!cluster_info)\r\nreturn nr_extents;\r\nfor (i = 0; i < nr_clusters; i++) {\r\nif (!cluster_count(&cluster_info[idx])) {\r\ncluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);\r\nif (cluster_is_null(&p->free_cluster_head)) {\r\ncluster_set_next_flag(&p->free_cluster_head,\r\nidx, 0);\r\ncluster_set_next_flag(&p->free_cluster_tail,\r\nidx, 0);\r\n} else {\r\nunsigned int tail;\r\ntail = cluster_next(&p->free_cluster_tail);\r\ncluster_set_next(&cluster_info[tail], idx);\r\ncluster_set_next_flag(&p->free_cluster_tail,\r\nidx, 0);\r\n}\r\n}\r\nidx++;\r\nif (idx == nr_clusters)\r\nidx = 0;\r\n}\r\nreturn nr_extents;\r\n}\r\nstatic bool swap_discardable(struct swap_info_struct *si)\r\n{\r\nstruct request_queue *q = bdev_get_queue(si->bdev);\r\nif (!q || !blk_queue_discard(q))\r\nreturn false;\r\nreturn true;\r\n}\r\nvoid si_swapinfo(struct sysinfo *val)\r\n{\r\nunsigned int type;\r\nunsigned long nr_to_be_unused = 0;\r\nspin_lock(&swap_lock);\r\nfor (type = 0; type < nr_swapfiles; type++) {\r\nstruct swap_info_struct *si = swap_info[type];\r\nif ((si->flags & SWP_USED) && !(si->flags & SWP_WRITEOK))\r\nnr_to_be_unused += si->inuse_pages;\r\n}\r\nval->freeswap = atomic_long_read(&nr_swap_pages) + nr_to_be_unused;\r\nval->totalswap = total_swap_pages + nr_to_be_unused;\r\nspin_unlock(&swap_lock);\r\n}\r\nstatic int __swap_duplicate(swp_entry_t entry, unsigned char usage)\r\n{\r\nstruct swap_info_struct *p;\r\nunsigned long offset, type;\r\nunsigned char count;\r\nunsigned char has_cache;\r\nint err = -EINVAL;\r\nif (non_swap_entry(entry))\r\ngoto out;\r\ntype = swp_type(entry);\r\nif (type >= nr_swapfiles)\r\ngoto bad_file;\r\np = swap_info[type];\r\noffset = swp_offset(entry);\r\nspin_lock(&p->lock);\r\nif (unlikely(offset >= p->max))\r\ngoto unlock_out;\r\ncount = p->swap_map[offset];\r\nif (unlikely(swap_count(count) == SWAP_MAP_BAD)) {\r\nerr = -ENOENT;\r\ngoto unlock_out;\r\n}\r\nhas_cache = count & SWAP_HAS_CACHE;\r\ncount &= ~SWAP_HAS_CACHE;\r\nerr = 0;\r\nif (usage == SWAP_HAS_CACHE) {\r\nif (!has_cache && count)\r\nhas_cache = SWAP_HAS_CACHE;\r\nelse if (has_cache)\r\nerr = -EEXIST;\r\nelse\r\nerr = -ENOENT;\r\n} else if (count || has_cache) {\r\nif ((count & ~COUNT_CONTINUED) < SWAP_MAP_MAX)\r\ncount += usage;\r\nelse if ((count & ~COUNT_CONTINUED) > SWAP_MAP_MAX)\r\nerr = -EINVAL;\r\nelse if (swap_count_continued(p, offset, count))\r\ncount = COUNT_CONTINUED;\r\nelse\r\nerr = -ENOMEM;\r\n} else\r\nerr = -ENOENT;\r\np->swap_map[offset] = count | has_cache;\r\nunlock_out:\r\nspin_unlock(&p->lock);\r\nout:\r\nreturn err;\r\nbad_file:\r\npr_err("swap_dup: %s%08lx\n", Bad_file, entry.val);\r\ngoto out;\r\n}\r\nvoid swap_shmem_alloc(swp_entry_t entry)\r\n{\r\n__swap_duplicate(entry, SWAP_MAP_SHMEM);\r\n}\r\nint swap_duplicate(swp_entry_t entry)\r\n{\r\nint err = 0;\r\nwhile (!err && __swap_duplicate(entry, 1) == -ENOMEM)\r\nerr = add_swap_count_continuation(entry, GFP_ATOMIC);\r\nreturn err;\r\n}\r\nint swapcache_prepare(swp_entry_t entry)\r\n{\r\nreturn __swap_duplicate(entry, SWAP_HAS_CACHE);\r\n}\r\nstruct swap_info_struct *page_swap_info(struct page *page)\r\n{\r\nswp_entry_t swap = { .val = page_private(page) };\r\nBUG_ON(!PageSwapCache(page));\r\nreturn swap_info[swp_type(swap)];\r\n}\r\nstruct address_space *__page_file_mapping(struct page *page)\r\n{\r\nVM_BUG_ON_PAGE(!PageSwapCache(page), page);\r\nreturn page_swap_info(page)->swap_file->f_mapping;\r\n}\r\npgoff_t __page_file_index(struct page *page)\r\n{\r\nswp_entry_t swap = { .val = page_private(page) };\r\nVM_BUG_ON_PAGE(!PageSwapCache(page), page);\r\nreturn swp_offset(swap);\r\n}\r\nint add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\r\n{\r\nstruct swap_info_struct *si;\r\nstruct page *head;\r\nstruct page *page;\r\nstruct page *list_page;\r\npgoff_t offset;\r\nunsigned char count;\r\npage = alloc_page(gfp_mask | __GFP_HIGHMEM);\r\nsi = swap_info_get(entry);\r\nif (!si) {\r\ngoto outer;\r\n}\r\noffset = swp_offset(entry);\r\ncount = si->swap_map[offset] & ~SWAP_HAS_CACHE;\r\nif ((count & ~COUNT_CONTINUED) != SWAP_MAP_MAX) {\r\ngoto out;\r\n}\r\nif (!page) {\r\nspin_unlock(&si->lock);\r\nreturn -ENOMEM;\r\n}\r\nhead = vmalloc_to_page(si->swap_map + offset);\r\noffset &= ~PAGE_MASK;\r\nif (!page_private(head)) {\r\nBUG_ON(count & COUNT_CONTINUED);\r\nINIT_LIST_HEAD(&head->lru);\r\nset_page_private(head, SWP_CONTINUED);\r\nsi->flags |= SWP_CONTINUED;\r\n}\r\nlist_for_each_entry(list_page, &head->lru, lru) {\r\nunsigned char *map;\r\nif (!(count & COUNT_CONTINUED))\r\ngoto out;\r\nmap = kmap_atomic(list_page) + offset;\r\ncount = *map;\r\nkunmap_atomic(map);\r\nif ((count & ~COUNT_CONTINUED) != SWAP_CONT_MAX)\r\ngoto out;\r\n}\r\nlist_add_tail(&page->lru, &head->lru);\r\npage = NULL;\r\nout:\r\nspin_unlock(&si->lock);\r\nouter:\r\nif (page)\r\n__free_page(page);\r\nreturn 0;\r\n}\r\nstatic bool swap_count_continued(struct swap_info_struct *si,\r\npgoff_t offset, unsigned char count)\r\n{\r\nstruct page *head;\r\nstruct page *page;\r\nunsigned char *map;\r\nhead = vmalloc_to_page(si->swap_map + offset);\r\nif (page_private(head) != SWP_CONTINUED) {\r\nBUG_ON(count & COUNT_CONTINUED);\r\nreturn false;\r\n}\r\noffset &= ~PAGE_MASK;\r\npage = list_entry(head->lru.next, struct page, lru);\r\nmap = kmap_atomic(page) + offset;\r\nif (count == SWAP_MAP_MAX)\r\ngoto init_map;\r\nif (count == (SWAP_MAP_MAX | COUNT_CONTINUED)) {\r\nwhile (*map == (SWAP_CONT_MAX | COUNT_CONTINUED)) {\r\nkunmap_atomic(map);\r\npage = list_entry(page->lru.next, struct page, lru);\r\nBUG_ON(page == head);\r\nmap = kmap_atomic(page) + offset;\r\n}\r\nif (*map == SWAP_CONT_MAX) {\r\nkunmap_atomic(map);\r\npage = list_entry(page->lru.next, struct page, lru);\r\nif (page == head)\r\nreturn false;\r\nmap = kmap_atomic(page) + offset;\r\ninit_map: *map = 0;\r\n}\r\n*map += 1;\r\nkunmap_atomic(map);\r\npage = list_entry(page->lru.prev, struct page, lru);\r\nwhile (page != head) {\r\nmap = kmap_atomic(page) + offset;\r\n*map = COUNT_CONTINUED;\r\nkunmap_atomic(map);\r\npage = list_entry(page->lru.prev, struct page, lru);\r\n}\r\nreturn true;\r\n} else {\r\nBUG_ON(count != COUNT_CONTINUED);\r\nwhile (*map == COUNT_CONTINUED) {\r\nkunmap_atomic(map);\r\npage = list_entry(page->lru.next, struct page, lru);\r\nBUG_ON(page == head);\r\nmap = kmap_atomic(page) + offset;\r\n}\r\nBUG_ON(*map == 0);\r\n*map -= 1;\r\nif (*map == 0)\r\ncount = 0;\r\nkunmap_atomic(map);\r\npage = list_entry(page->lru.prev, struct page, lru);\r\nwhile (page != head) {\r\nmap = kmap_atomic(page) + offset;\r\n*map = SWAP_CONT_MAX | count;\r\ncount = COUNT_CONTINUED;\r\nkunmap_atomic(map);\r\npage = list_entry(page->lru.prev, struct page, lru);\r\n}\r\nreturn count == COUNT_CONTINUED;\r\n}\r\n}\r\nstatic void free_swap_count_continuations(struct swap_info_struct *si)\r\n{\r\npgoff_t offset;\r\nfor (offset = 0; offset < si->max; offset += PAGE_SIZE) {\r\nstruct page *head;\r\nhead = vmalloc_to_page(si->swap_map + offset);\r\nif (page_private(head)) {\r\nstruct list_head *this, *next;\r\nlist_for_each_safe(this, next, &head->lru) {\r\nstruct page *page;\r\npage = list_entry(this, struct page, lru);\r\nlist_del(this);\r\n__free_page(page);\r\n}\r\n}\r\n}\r\n}
