static void do_set_multicast(struct work_struct *w)\r\n{\r\nstruct net_device_context *ndevctx =\r\ncontainer_of(w, struct net_device_context, work);\r\nstruct netvsc_device *nvdev;\r\nstruct rndis_device *rdev;\r\nnvdev = hv_get_drvdata(ndevctx->device_ctx);\r\nif (nvdev == NULL || nvdev->ndev == NULL)\r\nreturn;\r\nrdev = nvdev->extension;\r\nif (rdev == NULL)\r\nreturn;\r\nif (nvdev->ndev->flags & IFF_PROMISC)\r\nrndis_filter_set_packet_filter(rdev,\r\nNDIS_PACKET_TYPE_PROMISCUOUS);\r\nelse\r\nrndis_filter_set_packet_filter(rdev,\r\nNDIS_PACKET_TYPE_BROADCAST |\r\nNDIS_PACKET_TYPE_ALL_MULTICAST |\r\nNDIS_PACKET_TYPE_DIRECTED);\r\n}\r\nstatic void netvsc_set_multicast_list(struct net_device *net)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(net);\r\nschedule_work(&net_device_ctx->work);\r\n}\r\nstatic int netvsc_open(struct net_device *net)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(net);\r\nstruct hv_device *device_obj = net_device_ctx->device_ctx;\r\nstruct netvsc_device *nvdev;\r\nstruct rndis_device *rdev;\r\nint ret = 0;\r\nnetif_carrier_off(net);\r\nret = rndis_filter_open(device_obj);\r\nif (ret != 0) {\r\nnetdev_err(net, "unable to open device (ret %d).\n", ret);\r\nreturn ret;\r\n}\r\nnetif_tx_wake_all_queues(net);\r\nnvdev = hv_get_drvdata(device_obj);\r\nrdev = nvdev->extension;\r\nif (!rdev->link_state)\r\nnetif_carrier_on(net);\r\nreturn ret;\r\n}\r\nstatic int netvsc_close(struct net_device *net)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(net);\r\nstruct hv_device *device_obj = net_device_ctx->device_ctx;\r\nstruct netvsc_device *nvdev = hv_get_drvdata(device_obj);\r\nint ret;\r\nu32 aread, awrite, i, msec = 10, retry = 0, retry_max = 20;\r\nstruct vmbus_channel *chn;\r\nnetif_tx_disable(net);\r\ncancel_work_sync(&net_device_ctx->work);\r\nret = rndis_filter_close(device_obj);\r\nif (ret != 0) {\r\nnetdev_err(net, "unable to close device (ret %d).\n", ret);\r\nreturn ret;\r\n}\r\nwhile (true) {\r\naread = 0;\r\nfor (i = 0; i < nvdev->num_chn; i++) {\r\nchn = nvdev->chn_table[i];\r\nif (!chn)\r\ncontinue;\r\nhv_get_ringbuffer_availbytes(&chn->inbound, &aread,\r\n&awrite);\r\nif (aread)\r\nbreak;\r\nhv_get_ringbuffer_availbytes(&chn->outbound, &aread,\r\n&awrite);\r\nif (aread)\r\nbreak;\r\n}\r\nretry++;\r\nif (retry > retry_max || aread == 0)\r\nbreak;\r\nmsleep(msec);\r\nif (msec < 1000)\r\nmsec *= 2;\r\n}\r\nif (aread) {\r\nnetdev_err(net, "Ring buffer not empty after closing rndis\n");\r\nret = -ETIMEDOUT;\r\n}\r\nreturn ret;\r\n}\r\nstatic void *init_ppi_data(struct rndis_message *msg, u32 ppi_size,\r\nint pkt_type)\r\n{\r\nstruct rndis_packet *rndis_pkt;\r\nstruct rndis_per_packet_info *ppi;\r\nrndis_pkt = &msg->msg.pkt;\r\nrndis_pkt->data_offset += ppi_size;\r\nppi = (struct rndis_per_packet_info *)((void *)rndis_pkt +\r\nrndis_pkt->per_pkt_info_offset + rndis_pkt->per_pkt_info_len);\r\nppi->size = ppi_size;\r\nppi->type = pkt_type;\r\nppi->ppi_offset = sizeof(struct rndis_per_packet_info);\r\nrndis_pkt->per_pkt_info_len += ppi_size;\r\nreturn ppi;\r\n}\r\nstatic u32 comp_hash(u8 *key, int klen, void *data, int dlen)\r\n{\r\nunion sub_key subk;\r\nint k_next = 4;\r\nu8 dt;\r\nint i, j;\r\nu32 ret = 0;\r\nsubk.k = 0;\r\nsubk.ka = ntohl(*(u32 *)key);\r\nfor (i = 0; i < dlen; i++) {\r\nsubk.kb = key[k_next];\r\nk_next = (k_next + 1) % klen;\r\ndt = ((u8 *)data)[i];\r\nfor (j = 0; j < 8; j++) {\r\nif (dt & 0x80)\r\nret ^= subk.ka;\r\ndt <<= 1;\r\nsubk.k <<= 1;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic bool netvsc_set_hash(u32 *hash, struct sk_buff *skb)\r\n{\r\nstruct flow_keys flow;\r\nint data_len;\r\nif (!skb_flow_dissect_flow_keys(skb, &flow, 0) ||\r\n!(flow.basic.n_proto == htons(ETH_P_IP) ||\r\nflow.basic.n_proto == htons(ETH_P_IPV6)))\r\nreturn false;\r\nif (flow.basic.ip_proto == IPPROTO_TCP)\r\ndata_len = 12;\r\nelse\r\ndata_len = 8;\r\n*hash = comp_hash(netvsc_hash_key, HASH_KEYLEN, &flow, data_len);\r\nreturn true;\r\n}\r\nstatic u16 netvsc_select_queue(struct net_device *ndev, struct sk_buff *skb,\r\nvoid *accel_priv, select_queue_fallback_t fallback)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(ndev);\r\nstruct hv_device *hdev = net_device_ctx->device_ctx;\r\nstruct netvsc_device *nvsc_dev = hv_get_drvdata(hdev);\r\nu32 hash;\r\nu16 q_idx = 0;\r\nif (nvsc_dev == NULL || ndev->real_num_tx_queues <= 1)\r\nreturn 0;\r\nif (netvsc_set_hash(&hash, skb)) {\r\nq_idx = nvsc_dev->send_table[hash % VRSS_SEND_TAB_SIZE] %\r\nndev->real_num_tx_queues;\r\nskb_set_hash(skb, hash, PKT_HASH_TYPE_L3);\r\n}\r\nreturn q_idx;\r\n}\r\nvoid netvsc_xmit_completion(void *context)\r\n{\r\nstruct hv_netvsc_packet *packet = (struct hv_netvsc_packet *)context;\r\nstruct sk_buff *skb = (struct sk_buff *)\r\n(unsigned long)packet->send_completion_tid;\r\nif (skb)\r\ndev_kfree_skb_any(skb);\r\n}\r\nstatic u32 fill_pg_buf(struct page *page, u32 offset, u32 len,\r\nstruct hv_page_buffer *pb)\r\n{\r\nint j = 0;\r\npage += (offset >> PAGE_SHIFT);\r\noffset &= ~PAGE_MASK;\r\nwhile (len > 0) {\r\nunsigned long bytes;\r\nbytes = PAGE_SIZE - offset;\r\nif (bytes > len)\r\nbytes = len;\r\npb[j].pfn = page_to_pfn(page);\r\npb[j].offset = offset;\r\npb[j].len = bytes;\r\noffset += bytes;\r\nlen -= bytes;\r\nif (offset == PAGE_SIZE && len) {\r\npage++;\r\noffset = 0;\r\nj++;\r\n}\r\n}\r\nreturn j + 1;\r\n}\r\nstatic u32 init_page_array(void *hdr, u32 len, struct sk_buff *skb,\r\nstruct hv_netvsc_packet *packet)\r\n{\r\nstruct hv_page_buffer *pb = packet->page_buf;\r\nu32 slots_used = 0;\r\nchar *data = skb->data;\r\nint frags = skb_shinfo(skb)->nr_frags;\r\nint i;\r\nif (hdr != NULL)\r\nslots_used += fill_pg_buf(virt_to_page(hdr),\r\noffset_in_page(hdr),\r\nlen, &pb[slots_used]);\r\npacket->rmsg_size = len;\r\npacket->rmsg_pgcnt = slots_used;\r\nslots_used += fill_pg_buf(virt_to_page(data),\r\noffset_in_page(data),\r\nskb_headlen(skb), &pb[slots_used]);\r\nfor (i = 0; i < frags; i++) {\r\nskb_frag_t *frag = skb_shinfo(skb)->frags + i;\r\nslots_used += fill_pg_buf(skb_frag_page(frag),\r\nfrag->page_offset,\r\nskb_frag_size(frag), &pb[slots_used]);\r\n}\r\nreturn slots_used;\r\n}\r\nstatic int count_skb_frag_slots(struct sk_buff *skb)\r\n{\r\nint i, frags = skb_shinfo(skb)->nr_frags;\r\nint pages = 0;\r\nfor (i = 0; i < frags; i++) {\r\nskb_frag_t *frag = skb_shinfo(skb)->frags + i;\r\nunsigned long size = skb_frag_size(frag);\r\nunsigned long offset = frag->page_offset;\r\noffset &= ~PAGE_MASK;\r\npages += PFN_UP(offset + size);\r\n}\r\nreturn pages;\r\n}\r\nstatic int netvsc_get_slots(struct sk_buff *skb)\r\n{\r\nchar *data = skb->data;\r\nunsigned int offset = offset_in_page(data);\r\nunsigned int len = skb_headlen(skb);\r\nint slots;\r\nint frag_slots;\r\nslots = DIV_ROUND_UP(offset + len, PAGE_SIZE);\r\nfrag_slots = count_skb_frag_slots(skb);\r\nreturn slots + frag_slots;\r\n}\r\nstatic u32 get_net_transport_info(struct sk_buff *skb, u32 *trans_off)\r\n{\r\nu32 ret_val = TRANSPORT_INFO_NOT_IP;\r\nif ((eth_hdr(skb)->h_proto != htons(ETH_P_IP)) &&\r\n(eth_hdr(skb)->h_proto != htons(ETH_P_IPV6))) {\r\ngoto not_ip;\r\n}\r\n*trans_off = skb_transport_offset(skb);\r\nif ((eth_hdr(skb)->h_proto == htons(ETH_P_IP))) {\r\nstruct iphdr *iphdr = ip_hdr(skb);\r\nif (iphdr->protocol == IPPROTO_TCP)\r\nret_val = TRANSPORT_INFO_IPV4_TCP;\r\nelse if (iphdr->protocol == IPPROTO_UDP)\r\nret_val = TRANSPORT_INFO_IPV4_UDP;\r\n} else {\r\nif (ipv6_hdr(skb)->nexthdr == IPPROTO_TCP)\r\nret_val = TRANSPORT_INFO_IPV6_TCP;\r\nelse if (ipv6_hdr(skb)->nexthdr == IPPROTO_UDP)\r\nret_val = TRANSPORT_INFO_IPV6_UDP;\r\n}\r\nnot_ip:\r\nreturn ret_val;\r\n}\r\nstatic int netvsc_start_xmit(struct sk_buff *skb, struct net_device *net)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(net);\r\nstruct hv_netvsc_packet *packet = NULL;\r\nint ret;\r\nunsigned int num_data_pgs;\r\nstruct rndis_message *rndis_msg;\r\nstruct rndis_packet *rndis_pkt;\r\nu32 rndis_msg_size;\r\nbool isvlan;\r\nbool linear = false;\r\nstruct rndis_per_packet_info *ppi;\r\nstruct ndis_tcp_ip_checksum_info *csum_info;\r\nstruct ndis_tcp_lso_info *lso_info;\r\nint hdr_offset;\r\nu32 net_trans_info;\r\nu32 hash;\r\nu32 skb_length;\r\nu32 pkt_sz;\r\nstruct hv_page_buffer page_buf[MAX_PAGE_BUFFER_COUNT];\r\nstruct netvsc_stats *tx_stats = this_cpu_ptr(net_device_ctx->tx_stats);\r\ncheck_size:\r\nskb_length = skb->len;\r\nnum_data_pgs = netvsc_get_slots(skb) + 2;\r\nif (num_data_pgs > MAX_PAGE_BUFFER_COUNT && linear) {\r\nnet_alert_ratelimited("packet too big: %u pages (%u bytes)\n",\r\nnum_data_pgs, skb->len);\r\nret = -EFAULT;\r\ngoto drop;\r\n} else if (num_data_pgs > MAX_PAGE_BUFFER_COUNT) {\r\nif (skb_linearize(skb)) {\r\nnet_alert_ratelimited("failed to linearize skb\n");\r\nret = -ENOMEM;\r\ngoto drop;\r\n}\r\nlinear = true;\r\ngoto check_size;\r\n}\r\npkt_sz = sizeof(struct hv_netvsc_packet) + RNDIS_AND_PPI_SIZE;\r\nret = skb_cow_head(skb, pkt_sz);\r\nif (ret) {\r\nnetdev_err(net, "unable to alloc hv_netvsc_packet\n");\r\nret = -ENOMEM;\r\ngoto drop;\r\n}\r\npacket = (struct hv_netvsc_packet *)skb->head;\r\npacket->status = 0;\r\npacket->xmit_more = skb->xmit_more;\r\npacket->vlan_tci = skb->vlan_tci;\r\npacket->page_buf = page_buf;\r\npacket->q_idx = skb_get_queue_mapping(skb);\r\npacket->is_data_pkt = true;\r\npacket->total_data_buflen = skb->len;\r\npacket->rndis_msg = (struct rndis_message *)((unsigned long)packet +\r\nsizeof(struct hv_netvsc_packet));\r\nmemset(packet->rndis_msg, 0, RNDIS_AND_PPI_SIZE);\r\npacket->send_completion = netvsc_xmit_completion;\r\npacket->send_completion_ctx = packet;\r\npacket->send_completion_tid = (unsigned long)skb;\r\nisvlan = packet->vlan_tci & VLAN_TAG_PRESENT;\r\nrndis_msg = packet->rndis_msg;\r\nrndis_msg->ndis_msg_type = RNDIS_MSG_PACKET;\r\nrndis_msg->msg_len = packet->total_data_buflen;\r\nrndis_pkt = &rndis_msg->msg.pkt;\r\nrndis_pkt->data_offset = sizeof(struct rndis_packet);\r\nrndis_pkt->data_len = packet->total_data_buflen;\r\nrndis_pkt->per_pkt_info_offset = sizeof(struct rndis_packet);\r\nrndis_msg_size = RNDIS_MESSAGE_SIZE(struct rndis_packet);\r\nhash = skb_get_hash_raw(skb);\r\nif (hash != 0 && net->real_num_tx_queues > 1) {\r\nrndis_msg_size += NDIS_HASH_PPI_SIZE;\r\nppi = init_ppi_data(rndis_msg, NDIS_HASH_PPI_SIZE,\r\nNBL_HASH_VALUE);\r\n*(u32 *)((void *)ppi + ppi->ppi_offset) = hash;\r\n}\r\nif (isvlan) {\r\nstruct ndis_pkt_8021q_info *vlan;\r\nrndis_msg_size += NDIS_VLAN_PPI_SIZE;\r\nppi = init_ppi_data(rndis_msg, NDIS_VLAN_PPI_SIZE,\r\nIEEE_8021Q_INFO);\r\nvlan = (struct ndis_pkt_8021q_info *)((void *)ppi +\r\nppi->ppi_offset);\r\nvlan->vlanid = packet->vlan_tci & VLAN_VID_MASK;\r\nvlan->pri = (packet->vlan_tci & VLAN_PRIO_MASK) >>\r\nVLAN_PRIO_SHIFT;\r\n}\r\nnet_trans_info = get_net_transport_info(skb, &hdr_offset);\r\nif (net_trans_info == TRANSPORT_INFO_NOT_IP)\r\ngoto do_send;\r\nif (skb_is_gso(skb))\r\ngoto do_lso;\r\nif ((skb->ip_summed == CHECKSUM_NONE) ||\r\n(skb->ip_summed == CHECKSUM_UNNECESSARY))\r\ngoto do_send;\r\nrndis_msg_size += NDIS_CSUM_PPI_SIZE;\r\nppi = init_ppi_data(rndis_msg, NDIS_CSUM_PPI_SIZE,\r\nTCPIP_CHKSUM_PKTINFO);\r\ncsum_info = (struct ndis_tcp_ip_checksum_info *)((void *)ppi +\r\nppi->ppi_offset);\r\nif (net_trans_info & (INFO_IPV4 << 16))\r\ncsum_info->transmit.is_ipv4 = 1;\r\nelse\r\ncsum_info->transmit.is_ipv6 = 1;\r\nif (net_trans_info & INFO_TCP) {\r\ncsum_info->transmit.tcp_checksum = 1;\r\ncsum_info->transmit.tcp_header_offset = hdr_offset;\r\n} else if (net_trans_info & INFO_UDP) {\r\nstruct udphdr *uh;\r\nu16 udp_len;\r\nret = skb_cow_head(skb, 0);\r\nif (ret)\r\ngoto drop;\r\nuh = udp_hdr(skb);\r\nudp_len = ntohs(uh->len);\r\nuh->check = 0;\r\nuh->check = csum_tcpudp_magic(ip_hdr(skb)->saddr,\r\nip_hdr(skb)->daddr,\r\nudp_len, IPPROTO_UDP,\r\ncsum_partial(uh, udp_len, 0));\r\nif (uh->check == 0)\r\nuh->check = CSUM_MANGLED_0;\r\ncsum_info->transmit.udp_checksum = 0;\r\n}\r\ngoto do_send;\r\ndo_lso:\r\nrndis_msg_size += NDIS_LSO_PPI_SIZE;\r\nppi = init_ppi_data(rndis_msg, NDIS_LSO_PPI_SIZE,\r\nTCP_LARGESEND_PKTINFO);\r\nlso_info = (struct ndis_tcp_lso_info *)((void *)ppi +\r\nppi->ppi_offset);\r\nlso_info->lso_v2_transmit.type = NDIS_TCP_LARGE_SEND_OFFLOAD_V2_TYPE;\r\nif (net_trans_info & (INFO_IPV4 << 16)) {\r\nlso_info->lso_v2_transmit.ip_version =\r\nNDIS_TCP_LARGE_SEND_OFFLOAD_IPV4;\r\nip_hdr(skb)->tot_len = 0;\r\nip_hdr(skb)->check = 0;\r\ntcp_hdr(skb)->check =\r\n~csum_tcpudp_magic(ip_hdr(skb)->saddr,\r\nip_hdr(skb)->daddr, 0, IPPROTO_TCP, 0);\r\n} else {\r\nlso_info->lso_v2_transmit.ip_version =\r\nNDIS_TCP_LARGE_SEND_OFFLOAD_IPV6;\r\nipv6_hdr(skb)->payload_len = 0;\r\ntcp_hdr(skb)->check =\r\n~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\r\n&ipv6_hdr(skb)->daddr, 0, IPPROTO_TCP, 0);\r\n}\r\nlso_info->lso_v2_transmit.tcp_header_offset = hdr_offset;\r\nlso_info->lso_v2_transmit.mss = skb_shinfo(skb)->gso_size;\r\ndo_send:\r\nrndis_msg->msg_len += rndis_msg_size;\r\npacket->total_data_buflen = rndis_msg->msg_len;\r\npacket->page_buf_cnt = init_page_array(rndis_msg, rndis_msg_size,\r\nskb, packet);\r\nret = netvsc_send(net_device_ctx->device_ctx, packet);\r\ndrop:\r\nif (ret == 0) {\r\nu64_stats_update_begin(&tx_stats->syncp);\r\ntx_stats->packets++;\r\ntx_stats->bytes += skb_length;\r\nu64_stats_update_end(&tx_stats->syncp);\r\n} else {\r\nif (ret != -EAGAIN) {\r\ndev_kfree_skb_any(skb);\r\nnet->stats.tx_dropped++;\r\n}\r\n}\r\nreturn (ret == -EAGAIN) ? NETDEV_TX_BUSY : NETDEV_TX_OK;\r\n}\r\nvoid netvsc_linkstatus_callback(struct hv_device *device_obj,\r\nstruct rndis_message *resp)\r\n{\r\nstruct rndis_indicate_status *indicate = &resp->msg.indicate_status;\r\nstruct net_device *net;\r\nstruct net_device_context *ndev_ctx;\r\nstruct netvsc_device *net_device;\r\nstruct rndis_device *rdev;\r\nnet_device = hv_get_drvdata(device_obj);\r\nrdev = net_device->extension;\r\nswitch (indicate->status) {\r\ncase RNDIS_STATUS_MEDIA_CONNECT:\r\nrdev->link_state = false;\r\nbreak;\r\ncase RNDIS_STATUS_MEDIA_DISCONNECT:\r\nrdev->link_state = true;\r\nbreak;\r\ncase RNDIS_STATUS_NETWORK_CHANGE:\r\nrdev->link_change = true;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nnet = net_device->ndev;\r\nif (!net || net->reg_state != NETREG_REGISTERED)\r\nreturn;\r\nndev_ctx = netdev_priv(net);\r\nif (!rdev->link_state) {\r\nschedule_delayed_work(&ndev_ctx->dwork, 0);\r\nschedule_delayed_work(&ndev_ctx->dwork, msecs_to_jiffies(20));\r\n} else {\r\nschedule_delayed_work(&ndev_ctx->dwork, 0);\r\n}\r\n}\r\nint netvsc_recv_callback(struct hv_device *device_obj,\r\nstruct hv_netvsc_packet *packet,\r\nstruct ndis_tcp_ip_checksum_info *csum_info)\r\n{\r\nstruct net_device *net;\r\nstruct net_device_context *net_device_ctx;\r\nstruct sk_buff *skb;\r\nstruct netvsc_stats *rx_stats;\r\nnet = ((struct netvsc_device *)hv_get_drvdata(device_obj))->ndev;\r\nif (!net || net->reg_state != NETREG_REGISTERED) {\r\npacket->status = NVSP_STAT_FAIL;\r\nreturn 0;\r\n}\r\nnet_device_ctx = netdev_priv(net);\r\nrx_stats = this_cpu_ptr(net_device_ctx->rx_stats);\r\nskb = netdev_alloc_skb_ip_align(net, packet->total_data_buflen);\r\nif (unlikely(!skb)) {\r\n++net->stats.rx_dropped;\r\npacket->status = NVSP_STAT_FAIL;\r\nreturn 0;\r\n}\r\nmemcpy(skb_put(skb, packet->total_data_buflen), packet->data,\r\npacket->total_data_buflen);\r\nskb->protocol = eth_type_trans(skb, net);\r\nif (csum_info) {\r\nif (csum_info->receive.ip_checksum_succeeded)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nelse\r\nskb->ip_summed = CHECKSUM_NONE;\r\n}\r\nif (packet->vlan_tci & VLAN_TAG_PRESENT)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\r\npacket->vlan_tci);\r\nskb_record_rx_queue(skb, packet->channel->\r\noffermsg.offer.sub_channel_index);\r\nu64_stats_update_begin(&rx_stats->syncp);\r\nrx_stats->packets++;\r\nrx_stats->bytes += packet->total_data_buflen;\r\nu64_stats_update_end(&rx_stats->syncp);\r\nnetif_rx(skb);\r\nreturn 0;\r\n}\r\nstatic void netvsc_get_drvinfo(struct net_device *net,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstrlcpy(info->driver, KBUILD_MODNAME, sizeof(info->driver));\r\nstrlcpy(info->fw_version, "N/A", sizeof(info->fw_version));\r\n}\r\nstatic void netvsc_get_channels(struct net_device *net,\r\nstruct ethtool_channels *channel)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(net);\r\nstruct hv_device *dev = net_device_ctx->device_ctx;\r\nstruct netvsc_device *nvdev = hv_get_drvdata(dev);\r\nif (nvdev) {\r\nchannel->max_combined = nvdev->max_chn;\r\nchannel->combined_count = nvdev->num_chn;\r\n}\r\n}\r\nstatic int netvsc_set_channels(struct net_device *net,\r\nstruct ethtool_channels *channels)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(net);\r\nstruct hv_device *dev = net_device_ctx->device_ctx;\r\nstruct netvsc_device *nvdev = hv_get_drvdata(dev);\r\nstruct netvsc_device_info device_info;\r\nu32 num_chn;\r\nu32 max_chn;\r\nint ret = 0;\r\nbool recovering = false;\r\nif (!nvdev || nvdev->destroy)\r\nreturn -ENODEV;\r\nnum_chn = nvdev->num_chn;\r\nmax_chn = min_t(u32, nvdev->max_chn, num_online_cpus());\r\nif (nvdev->nvsp_version < NVSP_PROTOCOL_VERSION_5) {\r\npr_info("vRSS unsupported before NVSP Version 5\n");\r\nreturn -EINVAL;\r\n}\r\nif (!channels ||\r\nchannels->rx_count ||\r\nchannels->tx_count ||\r\nchannels->other_count ||\r\n(channels->combined_count < 1))\r\nreturn -EINVAL;\r\nif (channels->combined_count > max_chn) {\r\npr_info("combined channels too high, using %d\n", max_chn);\r\nchannels->combined_count = max_chn;\r\n}\r\nret = netvsc_close(net);\r\nif (ret)\r\ngoto out;\r\ndo_set:\r\nnvdev->start_remove = true;\r\nrndis_filter_device_remove(dev);\r\nnvdev->num_chn = channels->combined_count;\r\nnet_device_ctx->device_ctx = dev;\r\nhv_set_drvdata(dev, net);\r\nmemset(&device_info, 0, sizeof(device_info));\r\ndevice_info.num_chn = nvdev->num_chn;\r\ndevice_info.ring_size = ring_size;\r\ndevice_info.max_num_vrss_chns = max_num_vrss_chns;\r\nret = rndis_filter_device_add(dev, &device_info);\r\nif (ret) {\r\nif (recovering) {\r\nnetdev_err(net, "unable to add netvsc device (ret %d)\n", ret);\r\nreturn ret;\r\n}\r\ngoto recover;\r\n}\r\nnvdev = hv_get_drvdata(dev);\r\nret = netif_set_real_num_tx_queues(net, nvdev->num_chn);\r\nif (ret) {\r\nif (recovering) {\r\nnetdev_err(net, "could not set tx queue count (ret %d)\n", ret);\r\nreturn ret;\r\n}\r\ngoto recover;\r\n}\r\nret = netif_set_real_num_rx_queues(net, nvdev->num_chn);\r\nif (ret) {\r\nif (recovering) {\r\nnetdev_err(net, "could not set rx queue count (ret %d)\n", ret);\r\nreturn ret;\r\n}\r\ngoto recover;\r\n}\r\nout:\r\nnetvsc_open(net);\r\nreturn ret;\r\nrecover:\r\nnetdev_err(net, "could not set channels, recovering\n");\r\nrecovering = true;\r\nchannels->combined_count = num_chn;\r\ngoto do_set;\r\n}\r\nstatic int netvsc_change_mtu(struct net_device *ndev, int mtu)\r\n{\r\nstruct net_device_context *ndevctx = netdev_priv(ndev);\r\nstruct hv_device *hdev = ndevctx->device_ctx;\r\nstruct netvsc_device *nvdev = hv_get_drvdata(hdev);\r\nstruct netvsc_device_info device_info;\r\nint limit = ETH_DATA_LEN;\r\nint ret = 0;\r\nif (nvdev == NULL || nvdev->destroy)\r\nreturn -ENODEV;\r\nif (nvdev->nvsp_version >= NVSP_PROTOCOL_VERSION_2)\r\nlimit = NETVSC_MTU - ETH_HLEN;\r\nif (mtu < NETVSC_MTU_MIN || mtu > limit)\r\nreturn -EINVAL;\r\nret = netvsc_close(ndev);\r\nif (ret)\r\ngoto out;\r\nnvdev->start_remove = true;\r\nrndis_filter_device_remove(hdev);\r\nndev->mtu = mtu;\r\nndevctx->device_ctx = hdev;\r\nhv_set_drvdata(hdev, ndev);\r\nmemset(&device_info, 0, sizeof(device_info));\r\ndevice_info.ring_size = ring_size;\r\ndevice_info.num_chn = nvdev->num_chn;\r\ndevice_info.max_num_vrss_chns = max_num_vrss_chns;\r\nrndis_filter_device_add(hdev, &device_info);\r\nout:\r\nnetvsc_open(ndev);\r\nreturn ret;\r\n}\r\nstatic struct rtnl_link_stats64 *netvsc_get_stats64(struct net_device *net,\r\nstruct rtnl_link_stats64 *t)\r\n{\r\nstruct net_device_context *ndev_ctx = netdev_priv(net);\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct netvsc_stats *tx_stats = per_cpu_ptr(ndev_ctx->tx_stats,\r\ncpu);\r\nstruct netvsc_stats *rx_stats = per_cpu_ptr(ndev_ctx->rx_stats,\r\ncpu);\r\nu64 tx_packets, tx_bytes, rx_packets, rx_bytes;\r\nunsigned int start;\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&tx_stats->syncp);\r\ntx_packets = tx_stats->packets;\r\ntx_bytes = tx_stats->bytes;\r\n} while (u64_stats_fetch_retry_irq(&tx_stats->syncp, start));\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&rx_stats->syncp);\r\nrx_packets = rx_stats->packets;\r\nrx_bytes = rx_stats->bytes;\r\n} while (u64_stats_fetch_retry_irq(&rx_stats->syncp, start));\r\nt->tx_bytes += tx_bytes;\r\nt->tx_packets += tx_packets;\r\nt->rx_bytes += rx_bytes;\r\nt->rx_packets += rx_packets;\r\n}\r\nt->tx_dropped = net->stats.tx_dropped;\r\nt->tx_errors = net->stats.tx_dropped;\r\nt->rx_dropped = net->stats.rx_dropped;\r\nt->rx_errors = net->stats.rx_errors;\r\nreturn t;\r\n}\r\nstatic int netvsc_set_mac_addr(struct net_device *ndev, void *p)\r\n{\r\nstruct net_device_context *ndevctx = netdev_priv(ndev);\r\nstruct hv_device *hdev = ndevctx->device_ctx;\r\nstruct sockaddr *addr = p;\r\nchar save_adr[ETH_ALEN];\r\nunsigned char save_aatype;\r\nint err;\r\nmemcpy(save_adr, ndev->dev_addr, ETH_ALEN);\r\nsave_aatype = ndev->addr_assign_type;\r\nerr = eth_mac_addr(ndev, p);\r\nif (err != 0)\r\nreturn err;\r\nerr = rndis_filter_set_device_mac(hdev, addr->sa_data);\r\nif (err != 0) {\r\nmemcpy(ndev->dev_addr, save_adr, ETH_ALEN);\r\nndev->addr_assign_type = save_aatype;\r\n}\r\nreturn err;\r\n}\r\nstatic void netvsc_poll_controller(struct net_device *net)\r\n{\r\n}\r\nstatic void netvsc_link_change(struct work_struct *w)\r\n{\r\nstruct net_device_context *ndev_ctx;\r\nstruct net_device *net;\r\nstruct netvsc_device *net_device;\r\nstruct rndis_device *rdev;\r\nbool notify, refresh = false;\r\nchar *argv[] = { "/etc/init.d/network", "restart", NULL };\r\nchar *envp[] = { "HOME=/", "PATH=/sbin:/usr/sbin:/bin:/usr/bin", NULL };\r\nrtnl_lock();\r\nndev_ctx = container_of(w, struct net_device_context, dwork.work);\r\nnet_device = hv_get_drvdata(ndev_ctx->device_ctx);\r\nrdev = net_device->extension;\r\nnet = net_device->ndev;\r\nif (rdev->link_state) {\r\nnetif_carrier_off(net);\r\nnotify = false;\r\n} else {\r\nnetif_carrier_on(net);\r\nnotify = true;\r\nif (rdev->link_change) {\r\nrdev->link_change = false;\r\nrefresh = true;\r\n}\r\n}\r\nrtnl_unlock();\r\nif (refresh)\r\ncall_usermodehelper(argv[0], argv, envp, UMH_WAIT_EXEC);\r\nif (notify)\r\nnetdev_notify_peers(net);\r\n}\r\nstatic void netvsc_free_netdev(struct net_device *netdev)\r\n{\r\nstruct net_device_context *net_device_ctx = netdev_priv(netdev);\r\nfree_percpu(net_device_ctx->tx_stats);\r\nfree_percpu(net_device_ctx->rx_stats);\r\nfree_netdev(netdev);\r\n}\r\nstatic int netvsc_probe(struct hv_device *dev,\r\nconst struct hv_vmbus_device_id *dev_id)\r\n{\r\nstruct net_device *net = NULL;\r\nstruct net_device_context *net_device_ctx;\r\nstruct netvsc_device_info device_info;\r\nstruct netvsc_device *nvdev;\r\nint ret;\r\nu32 max_needed_headroom;\r\nnet = alloc_etherdev_mq(sizeof(struct net_device_context),\r\nnum_online_cpus());\r\nif (!net)\r\nreturn -ENOMEM;\r\nmax_needed_headroom = sizeof(struct hv_netvsc_packet) +\r\nRNDIS_AND_PPI_SIZE;\r\nnetif_carrier_off(net);\r\nnet_device_ctx = netdev_priv(net);\r\nnet_device_ctx->device_ctx = dev;\r\nnet_device_ctx->msg_enable = netif_msg_init(debug, default_msg);\r\nif (netif_msg_probe(net_device_ctx))\r\nnetdev_dbg(net, "netvsc msg_enable: %d\n",\r\nnet_device_ctx->msg_enable);\r\nnet_device_ctx->tx_stats = netdev_alloc_pcpu_stats(struct netvsc_stats);\r\nif (!net_device_ctx->tx_stats) {\r\nfree_netdev(net);\r\nreturn -ENOMEM;\r\n}\r\nnet_device_ctx->rx_stats = netdev_alloc_pcpu_stats(struct netvsc_stats);\r\nif (!net_device_ctx->rx_stats) {\r\nfree_percpu(net_device_ctx->tx_stats);\r\nfree_netdev(net);\r\nreturn -ENOMEM;\r\n}\r\nhv_set_drvdata(dev, net);\r\nINIT_DELAYED_WORK(&net_device_ctx->dwork, netvsc_link_change);\r\nINIT_WORK(&net_device_ctx->work, do_set_multicast);\r\nnet->netdev_ops = &device_ops;\r\nnet->hw_features = NETIF_F_RXCSUM | NETIF_F_SG | NETIF_F_IP_CSUM |\r\nNETIF_F_TSO;\r\nnet->features = NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_SG | NETIF_F_RXCSUM |\r\nNETIF_F_IP_CSUM | NETIF_F_TSO;\r\nnet->ethtool_ops = &ethtool_ops;\r\nSET_NETDEV_DEV(net, &dev->device);\r\nnet->needed_headroom = max_needed_headroom;\r\nmemset(&device_info, 0, sizeof(device_info));\r\ndevice_info.ring_size = ring_size;\r\ndevice_info.max_num_vrss_chns = max_num_vrss_chns;\r\nret = rndis_filter_device_add(dev, &device_info);\r\nif (ret != 0) {\r\nnetdev_err(net, "unable to add netvsc device (ret %d)\n", ret);\r\nnetvsc_free_netdev(net);\r\nhv_set_drvdata(dev, NULL);\r\nreturn ret;\r\n}\r\nmemcpy(net->dev_addr, device_info.mac_adr, ETH_ALEN);\r\nnvdev = hv_get_drvdata(dev);\r\nnetif_set_real_num_tx_queues(net, nvdev->num_chn);\r\nnetif_set_real_num_rx_queues(net, nvdev->num_chn);\r\nret = register_netdev(net);\r\nif (ret != 0) {\r\npr_err("Unable to register netdev.\n");\r\nrndis_filter_device_remove(dev);\r\nnetvsc_free_netdev(net);\r\n} else {\r\nschedule_delayed_work(&net_device_ctx->dwork, 0);\r\n}\r\nreturn ret;\r\n}\r\nstatic int netvsc_remove(struct hv_device *dev)\r\n{\r\nstruct net_device *net;\r\nstruct net_device_context *ndev_ctx;\r\nstruct netvsc_device *net_device;\r\nnet_device = hv_get_drvdata(dev);\r\nnet = net_device->ndev;\r\nif (net == NULL) {\r\ndev_err(&dev->device, "No net device to remove\n");\r\nreturn 0;\r\n}\r\nnet_device->start_remove = true;\r\nndev_ctx = netdev_priv(net);\r\ncancel_delayed_work_sync(&ndev_ctx->dwork);\r\ncancel_work_sync(&ndev_ctx->work);\r\nnetif_tx_disable(net);\r\nunregister_netdev(net);\r\nrndis_filter_device_remove(dev);\r\nnetvsc_free_netdev(net);\r\nreturn 0;\r\n}\r\nstatic void __exit netvsc_drv_exit(void)\r\n{\r\nvmbus_driver_unregister(&netvsc_drv);\r\n}\r\nstatic int __init netvsc_drv_init(void)\r\n{\r\nif (ring_size < RING_SIZE_MIN) {\r\nring_size = RING_SIZE_MIN;\r\npr_info("Increased ring_size to %d (min allowed)\n",\r\nring_size);\r\n}\r\nreturn vmbus_driver_register(&netvsc_drv);\r\n}
