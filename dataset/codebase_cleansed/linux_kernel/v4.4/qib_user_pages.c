static void __qib_release_user_pages(struct page **p, size_t num_pages,\r\nint dirty)\r\n{\r\nsize_t i;\r\nfor (i = 0; i < num_pages; i++) {\r\nif (dirty)\r\nset_page_dirty_lock(p[i]);\r\nput_page(p[i]);\r\n}\r\n}\r\nstatic int __qib_get_user_pages(unsigned long start_page, size_t num_pages,\r\nstruct page **p)\r\n{\r\nunsigned long lock_limit;\r\nsize_t got;\r\nint ret;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif (num_pages > lock_limit && !capable(CAP_IPC_LOCK)) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nfor (got = 0; got < num_pages; got += ret) {\r\nret = get_user_pages(current, current->mm,\r\nstart_page + got * PAGE_SIZE,\r\nnum_pages - got, 1, 1,\r\np + got, NULL);\r\nif (ret < 0)\r\ngoto bail_release;\r\n}\r\ncurrent->mm->pinned_vm += num_pages;\r\nret = 0;\r\ngoto bail;\r\nbail_release:\r\n__qib_release_user_pages(p, got, 0);\r\nbail:\r\nreturn ret;\r\n}\r\ndma_addr_t qib_map_page(struct pci_dev *hwdev, struct page *page,\r\nunsigned long offset, size_t size, int direction)\r\n{\r\ndma_addr_t phys;\r\nphys = pci_map_page(hwdev, page, offset, size, direction);\r\nif (phys == 0) {\r\npci_unmap_page(hwdev, phys, size, direction);\r\nphys = pci_map_page(hwdev, page, offset, size, direction);\r\n}\r\nreturn phys;\r\n}\r\nint qib_get_user_pages(unsigned long start_page, size_t num_pages,\r\nstruct page **p)\r\n{\r\nint ret;\r\ndown_write(&current->mm->mmap_sem);\r\nret = __qib_get_user_pages(start_page, num_pages, p);\r\nup_write(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nvoid qib_release_user_pages(struct page **p, size_t num_pages)\r\n{\r\nif (current->mm)\r\ndown_write(&current->mm->mmap_sem);\r\n__qib_release_user_pages(p, num_pages, 1);\r\nif (current->mm) {\r\ncurrent->mm->pinned_vm -= num_pages;\r\nup_write(&current->mm->mmap_sem);\r\n}\r\n}
