static u32 kv_convert_vid2_to_vid7(struct amdgpu_device *adev,\r\nstruct sumo_vid_mapping_table *vid_mapping_table,\r\nu32 vid_2bit)\r\n{\r\nstruct amdgpu_clock_voltage_dependency_table *vddc_sclk_table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nu32 i;\r\nif (vddc_sclk_table && vddc_sclk_table->count) {\r\nif (vid_2bit < vddc_sclk_table->count)\r\nreturn vddc_sclk_table->entries[vid_2bit].v;\r\nelse\r\nreturn vddc_sclk_table->entries[vddc_sclk_table->count - 1].v;\r\n} else {\r\nfor (i = 0; i < vid_mapping_table->num_entries; i++) {\r\nif (vid_mapping_table->entries[i].vid_2bit == vid_2bit)\r\nreturn vid_mapping_table->entries[i].vid_7bit;\r\n}\r\nreturn vid_mapping_table->entries[vid_mapping_table->num_entries - 1].vid_7bit;\r\n}\r\n}\r\nstatic u32 kv_convert_vid7_to_vid2(struct amdgpu_device *adev,\r\nstruct sumo_vid_mapping_table *vid_mapping_table,\r\nu32 vid_7bit)\r\n{\r\nstruct amdgpu_clock_voltage_dependency_table *vddc_sclk_table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nu32 i;\r\nif (vddc_sclk_table && vddc_sclk_table->count) {\r\nfor (i = 0; i < vddc_sclk_table->count; i++) {\r\nif (vddc_sclk_table->entries[i].v == vid_7bit)\r\nreturn i;\r\n}\r\nreturn vddc_sclk_table->count - 1;\r\n} else {\r\nfor (i = 0; i < vid_mapping_table->num_entries; i++) {\r\nif (vid_mapping_table->entries[i].vid_7bit == vid_7bit)\r\nreturn vid_mapping_table->entries[i].vid_2bit;\r\n}\r\nreturn vid_mapping_table->entries[vid_mapping_table->num_entries - 1].vid_2bit;\r\n}\r\n}\r\nstatic void sumo_take_smu_control(struct amdgpu_device *adev, bool enable)\r\n{\r\n#if 0\r\nu32 v = RREG32(mmDOUT_SCRATCH3);\r\nif (enable)\r\nv |= 0x4;\r\nelse\r\nv &= 0xFFFFFFFB;\r\nWREG32(mmDOUT_SCRATCH3, v);\r\n#endif\r\n}\r\nstatic u32 sumo_get_sleep_divider_from_id(u32 id)\r\n{\r\nreturn 1 << id;\r\n}\r\nstatic void sumo_construct_sclk_voltage_mapping_table(struct amdgpu_device *adev,\r\nstruct sumo_sclk_voltage_mapping_table *sclk_voltage_mapping_table,\r\nATOM_AVAILABLE_SCLK_LIST *table)\r\n{\r\nu32 i;\r\nu32 n = 0;\r\nu32 prev_sclk = 0;\r\nfor (i = 0; i < SUMO_MAX_HARDWARE_POWERLEVELS; i++) {\r\nif (table[i].ulSupportedSCLK > prev_sclk) {\r\nsclk_voltage_mapping_table->entries[n].sclk_frequency =\r\ntable[i].ulSupportedSCLK;\r\nsclk_voltage_mapping_table->entries[n].vid_2bit =\r\ntable[i].usVoltageIndex;\r\nprev_sclk = table[i].ulSupportedSCLK;\r\nn++;\r\n}\r\n}\r\nsclk_voltage_mapping_table->num_max_dpm_entries = n;\r\n}\r\nstatic void sumo_construct_vid_mapping_table(struct amdgpu_device *adev,\r\nstruct sumo_vid_mapping_table *vid_mapping_table,\r\nATOM_AVAILABLE_SCLK_LIST *table)\r\n{\r\nu32 i, j;\r\nfor (i = 0; i < SUMO_MAX_HARDWARE_POWERLEVELS; i++) {\r\nif (table[i].ulSupportedSCLK != 0) {\r\nvid_mapping_table->entries[table[i].usVoltageIndex].vid_7bit =\r\ntable[i].usVoltageID;\r\nvid_mapping_table->entries[table[i].usVoltageIndex].vid_2bit =\r\ntable[i].usVoltageIndex;\r\n}\r\n}\r\nfor (i = 0; i < SUMO_MAX_NUMBER_VOLTAGES; i++) {\r\nif (vid_mapping_table->entries[i].vid_7bit == 0) {\r\nfor (j = i + 1; j < SUMO_MAX_NUMBER_VOLTAGES; j++) {\r\nif (vid_mapping_table->entries[j].vid_7bit != 0) {\r\nvid_mapping_table->entries[i] =\r\nvid_mapping_table->entries[j];\r\nvid_mapping_table->entries[j].vid_7bit = 0;\r\nbreak;\r\n}\r\n}\r\nif (j == SUMO_MAX_NUMBER_VOLTAGES)\r\nbreak;\r\n}\r\n}\r\nvid_mapping_table->num_entries = i;\r\n}\r\nstatic struct kv_ps *kv_get_ps(struct amdgpu_ps *rps)\r\n{\r\nstruct kv_ps *ps = rps->ps_priv;\r\nreturn ps;\r\n}\r\nstatic struct kv_power_info *kv_get_pi(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = adev->pm.dpm.priv;\r\nreturn pi;\r\n}\r\nstatic int kv_program_pt_config_registers(struct amdgpu_device *adev,\r\nconst struct kv_pt_config_reg *cac_config_regs)\r\n{\r\nconst struct kv_pt_config_reg *config_regs = cac_config_regs;\r\nu32 data;\r\nu32 cache = 0;\r\nif (config_regs == NULL)\r\nreturn -EINVAL;\r\nwhile (config_regs->offset != 0xFFFFFFFF) {\r\nif (config_regs->type == KV_CONFIGREG_CACHE) {\r\ncache |= ((config_regs->value << config_regs->shift) & config_regs->mask);\r\n} else {\r\nswitch (config_regs->type) {\r\ncase KV_CONFIGREG_SMC_IND:\r\ndata = RREG32_SMC(config_regs->offset);\r\nbreak;\r\ncase KV_CONFIGREG_DIDT_IND:\r\ndata = RREG32_DIDT(config_regs->offset);\r\nbreak;\r\ndefault:\r\ndata = RREG32(config_regs->offset);\r\nbreak;\r\n}\r\ndata &= ~config_regs->mask;\r\ndata |= ((config_regs->value << config_regs->shift) & config_regs->mask);\r\ndata |= cache;\r\ncache = 0;\r\nswitch (config_regs->type) {\r\ncase KV_CONFIGREG_SMC_IND:\r\nWREG32_SMC(config_regs->offset, data);\r\nbreak;\r\ncase KV_CONFIGREG_DIDT_IND:\r\nWREG32_DIDT(config_regs->offset, data);\r\nbreak;\r\ndefault:\r\nWREG32(config_regs->offset, data);\r\nbreak;\r\n}\r\n}\r\nconfig_regs++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void kv_do_enable_didt(struct amdgpu_device *adev, bool enable)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 data;\r\nif (pi->caps_sq_ramping) {\r\ndata = RREG32_DIDT(ixDIDT_SQ_CTRL0);\r\nif (enable)\r\ndata |= DIDT_SQ_CTRL0__DIDT_CTRL_EN_MASK;\r\nelse\r\ndata &= ~DIDT_SQ_CTRL0__DIDT_CTRL_EN_MASK;\r\nWREG32_DIDT(ixDIDT_SQ_CTRL0, data);\r\n}\r\nif (pi->caps_db_ramping) {\r\ndata = RREG32_DIDT(ixDIDT_DB_CTRL0);\r\nif (enable)\r\ndata |= DIDT_DB_CTRL0__DIDT_CTRL_EN_MASK;\r\nelse\r\ndata &= ~DIDT_DB_CTRL0__DIDT_CTRL_EN_MASK;\r\nWREG32_DIDT(ixDIDT_DB_CTRL0, data);\r\n}\r\nif (pi->caps_td_ramping) {\r\ndata = RREG32_DIDT(ixDIDT_TD_CTRL0);\r\nif (enable)\r\ndata |= DIDT_TD_CTRL0__DIDT_CTRL_EN_MASK;\r\nelse\r\ndata &= ~DIDT_TD_CTRL0__DIDT_CTRL_EN_MASK;\r\nWREG32_DIDT(ixDIDT_TD_CTRL0, data);\r\n}\r\nif (pi->caps_tcp_ramping) {\r\ndata = RREG32_DIDT(ixDIDT_TCP_CTRL0);\r\nif (enable)\r\ndata |= DIDT_TCP_CTRL0__DIDT_CTRL_EN_MASK;\r\nelse\r\ndata &= ~DIDT_TCP_CTRL0__DIDT_CTRL_EN_MASK;\r\nWREG32_DIDT(ixDIDT_TCP_CTRL0, data);\r\n}\r\n}\r\nstatic int kv_enable_didt(struct amdgpu_device *adev, bool enable)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nif (pi->caps_sq_ramping ||\r\npi->caps_db_ramping ||\r\npi->caps_td_ramping ||\r\npi->caps_tcp_ramping) {\r\ngfx_v7_0_enter_rlc_safe_mode(adev);\r\nif (enable) {\r\nret = kv_program_pt_config_registers(adev, didt_config_kv);\r\nif (ret) {\r\ngfx_v7_0_exit_rlc_safe_mode(adev);\r\nreturn ret;\r\n}\r\n}\r\nkv_do_enable_didt(adev, enable);\r\ngfx_v7_0_exit_rlc_safe_mode(adev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_enable_smc_cac(struct amdgpu_device *adev, bool enable)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret = 0;\r\nif (pi->caps_cac) {\r\nif (enable) {\r\nret = amdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_EnableCac);\r\nif (ret)\r\npi->cac_enabled = false;\r\nelse\r\npi->cac_enabled = true;\r\n} else if (pi->cac_enabled) {\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_DisableCac);\r\npi->cac_enabled = false;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int kv_process_firmware_header(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 tmp;\r\nint ret;\r\nret = amdgpu_kv_read_smc_sram_dword(adev, SMU7_FIRMWARE_HEADER_LOCATION +\r\noffsetof(SMU7_Firmware_Header, DpmTable),\r\n&tmp, pi->sram_end);\r\nif (ret == 0)\r\npi->dpm_table_start = tmp;\r\nret = amdgpu_kv_read_smc_sram_dword(adev, SMU7_FIRMWARE_HEADER_LOCATION +\r\noffsetof(SMU7_Firmware_Header, SoftRegisters),\r\n&tmp, pi->sram_end);\r\nif (ret == 0)\r\npi->soft_regs_start = tmp;\r\nreturn ret;\r\n}\r\nstatic int kv_enable_dpm_voltage_scaling(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\npi->graphics_voltage_change_enable = 1;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, GraphicsVoltageChangeEnable),\r\n&pi->graphics_voltage_change_enable,\r\nsizeof(u8), pi->sram_end);\r\nreturn ret;\r\n}\r\nstatic int kv_set_dpm_interval(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\npi->graphics_interval = 1;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, GraphicsInterval),\r\n&pi->graphics_interval,\r\nsizeof(u8), pi->sram_end);\r\nreturn ret;\r\n}\r\nstatic int kv_set_dpm_boot_state(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, GraphicsBootLevel),\r\n&pi->graphics_boot_level,\r\nsizeof(u8), pi->sram_end);\r\nreturn ret;\r\n}\r\nstatic void kv_program_vc(struct amdgpu_device *adev)\r\n{\r\nWREG32_SMC(ixCG_FREQ_TRAN_VOTING_0, 0x3FFFC100);\r\n}\r\nstatic void kv_clear_vc(struct amdgpu_device *adev)\r\n{\r\nWREG32_SMC(ixCG_FREQ_TRAN_VOTING_0, 0);\r\n}\r\nstatic int kv_set_divider_value(struct amdgpu_device *adev,\r\nu32 index, u32 sclk)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct atom_clock_dividers dividers;\r\nint ret;\r\nret = amdgpu_atombios_get_clock_dividers(adev, COMPUTE_ENGINE_PLL_PARAM,\r\nsclk, false, &dividers);\r\nif (ret)\r\nreturn ret;\r\npi->graphics_level[index].SclkDid = (u8)dividers.post_div;\r\npi->graphics_level[index].SclkFrequency = cpu_to_be32(sclk);\r\nreturn 0;\r\n}\r\nstatic u16 kv_convert_8bit_index_to_voltage(struct amdgpu_device *adev,\r\nu16 voltage)\r\n{\r\nreturn 6200 - (voltage * 25);\r\n}\r\nstatic u16 kv_convert_2bit_index_to_voltage(struct amdgpu_device *adev,\r\nu32 vid_2bit)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 vid_8bit = kv_convert_vid2_to_vid7(adev,\r\n&pi->sys_info.vid_mapping_table,\r\nvid_2bit);\r\nreturn kv_convert_8bit_index_to_voltage(adev, (u16)vid_8bit);\r\n}\r\nstatic int kv_set_vid(struct amdgpu_device *adev, u32 index, u32 vid)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->graphics_level[index].VoltageDownH = (u8)pi->voltage_drop_t;\r\npi->graphics_level[index].MinVddNb =\r\ncpu_to_be32(kv_convert_2bit_index_to_voltage(adev, vid));\r\nreturn 0;\r\n}\r\nstatic int kv_set_at(struct amdgpu_device *adev, u32 index, u32 at)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->graphics_level[index].AT = cpu_to_be16((u16)at);\r\nreturn 0;\r\n}\r\nstatic void kv_dpm_power_level_enable(struct amdgpu_device *adev,\r\nu32 index, bool enable)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->graphics_level[index].EnabledForActivity = enable ? 1 : 0;\r\n}\r\nstatic void kv_start_dpm(struct amdgpu_device *adev)\r\n{\r\nu32 tmp = RREG32_SMC(ixGENERAL_PWRMGT);\r\ntmp |= GENERAL_PWRMGT__GLOBAL_PWRMGT_EN_MASK;\r\nWREG32_SMC(ixGENERAL_PWRMGT, tmp);\r\namdgpu_kv_smc_dpm_enable(adev, true);\r\n}\r\nstatic void kv_stop_dpm(struct amdgpu_device *adev)\r\n{\r\namdgpu_kv_smc_dpm_enable(adev, false);\r\n}\r\nstatic void kv_start_am(struct amdgpu_device *adev)\r\n{\r\nu32 sclk_pwrmgt_cntl = RREG32_SMC(ixSCLK_PWRMGT_CNTL);\r\nsclk_pwrmgt_cntl &= ~(SCLK_PWRMGT_CNTL__RESET_SCLK_CNT_MASK |\r\nSCLK_PWRMGT_CNTL__RESET_BUSY_CNT_MASK);\r\nsclk_pwrmgt_cntl |= SCLK_PWRMGT_CNTL__DYNAMIC_PM_EN_MASK;\r\nWREG32_SMC(ixSCLK_PWRMGT_CNTL, sclk_pwrmgt_cntl);\r\n}\r\nstatic void kv_reset_am(struct amdgpu_device *adev)\r\n{\r\nu32 sclk_pwrmgt_cntl = RREG32_SMC(ixSCLK_PWRMGT_CNTL);\r\nsclk_pwrmgt_cntl |= (SCLK_PWRMGT_CNTL__RESET_SCLK_CNT_MASK |\r\nSCLK_PWRMGT_CNTL__RESET_BUSY_CNT_MASK);\r\nWREG32_SMC(ixSCLK_PWRMGT_CNTL, sclk_pwrmgt_cntl);\r\n}\r\nstatic int kv_freeze_sclk_dpm(struct amdgpu_device *adev, bool freeze)\r\n{\r\nreturn amdgpu_kv_notify_message_to_smu(adev, freeze ?\r\nPPSMC_MSG_SCLKDPM_FreezeLevel : PPSMC_MSG_SCLKDPM_UnfreezeLevel);\r\n}\r\nstatic int kv_force_lowest_valid(struct amdgpu_device *adev)\r\n{\r\nreturn kv_force_dpm_lowest(adev);\r\n}\r\nstatic int kv_unforce_levels(struct amdgpu_device *adev)\r\n{\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS)\r\nreturn amdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_NoForcedLevel);\r\nelse\r\nreturn kv_set_enabled_levels(adev);\r\n}\r\nstatic int kv_update_sclk_t(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 low_sclk_interrupt_t = 0;\r\nint ret = 0;\r\nif (pi->caps_sclk_throttle_low_notification) {\r\nlow_sclk_interrupt_t = cpu_to_be32(pi->low_sclk_interrupt_t);\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, LowSclkInterruptT),\r\n(u8 *)&low_sclk_interrupt_t,\r\nsizeof(u32), pi->sram_end);\r\n}\r\nreturn ret;\r\n}\r\nstatic int kv_program_bootup_state(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nif (table && table->count) {\r\nfor (i = pi->graphics_dpm_level_count - 1; i > 0; i--) {\r\nif (table->entries[i].clk == pi->boot_pl.sclk)\r\nbreak;\r\n}\r\npi->graphics_boot_level = (u8)i;\r\nkv_dpm_power_level_enable(adev, i, true);\r\n} else {\r\nstruct sumo_sclk_voltage_mapping_table *table =\r\n&pi->sys_info.sclk_voltage_mapping_table;\r\nif (table->num_max_dpm_entries == 0)\r\nreturn -EINVAL;\r\nfor (i = pi->graphics_dpm_level_count - 1; i > 0; i--) {\r\nif (table->entries[i].sclk_frequency == pi->boot_pl.sclk)\r\nbreak;\r\n}\r\npi->graphics_boot_level = (u8)i;\r\nkv_dpm_power_level_enable(adev, i, true);\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_enable_auto_thermal_throttling(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\npi->graphics_therm_throttle_enable = 1;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, GraphicsThermThrottleEnable),\r\n&pi->graphics_therm_throttle_enable,\r\nsizeof(u8), pi->sram_end);\r\nreturn ret;\r\n}\r\nstatic int kv_upload_dpm_settings(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, GraphicsLevel),\r\n(u8 *)&pi->graphics_level,\r\nsizeof(SMU7_Fusion_GraphicsLevel) * SMU7_MAX_LEVELS_GRAPHICS,\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, GraphicsDpmLevelCount),\r\n&pi->graphics_dpm_level_count,\r\nsizeof(u8), pi->sram_end);\r\nreturn ret;\r\n}\r\nstatic u32 kv_get_clock_difference(u32 a, u32 b)\r\n{\r\nreturn (a >= b) ? a - b : b - a;\r\n}\r\nstatic u32 kv_get_clk_bypass(struct amdgpu_device *adev, u32 clk)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 value;\r\nif (pi->caps_enable_dfs_bypass) {\r\nif (kv_get_clock_difference(clk, 40000) < 200)\r\nvalue = 3;\r\nelse if (kv_get_clock_difference(clk, 30000) < 200)\r\nvalue = 2;\r\nelse if (kv_get_clock_difference(clk, 20000) < 200)\r\nvalue = 7;\r\nelse if (kv_get_clock_difference(clk, 15000) < 200)\r\nvalue = 6;\r\nelse if (kv_get_clock_difference(clk, 10000) < 200)\r\nvalue = 8;\r\nelse\r\nvalue = 0;\r\n} else {\r\nvalue = 0;\r\n}\r\nreturn value;\r\n}\r\nstatic int kv_populate_uvd_table(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_uvd_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;\r\nstruct atom_clock_dividers dividers;\r\nint ret;\r\nu32 i;\r\nif (table == NULL || table->count == 0)\r\nreturn 0;\r\npi->uvd_level_count = 0;\r\nfor (i = 0; i < table->count; i++) {\r\nif (pi->high_voltage_t &&\r\n(pi->high_voltage_t < table->entries[i].v))\r\nbreak;\r\npi->uvd_level[i].VclkFrequency = cpu_to_be32(table->entries[i].vclk);\r\npi->uvd_level[i].DclkFrequency = cpu_to_be32(table->entries[i].dclk);\r\npi->uvd_level[i].MinVddNb = cpu_to_be16(table->entries[i].v);\r\npi->uvd_level[i].VClkBypassCntl =\r\n(u8)kv_get_clk_bypass(adev, table->entries[i].vclk);\r\npi->uvd_level[i].DClkBypassCntl =\r\n(u8)kv_get_clk_bypass(adev, table->entries[i].dclk);\r\nret = amdgpu_atombios_get_clock_dividers(adev, COMPUTE_ENGINE_PLL_PARAM,\r\ntable->entries[i].vclk, false, &dividers);\r\nif (ret)\r\nreturn ret;\r\npi->uvd_level[i].VclkDivider = (u8)dividers.post_div;\r\nret = amdgpu_atombios_get_clock_dividers(adev, COMPUTE_ENGINE_PLL_PARAM,\r\ntable->entries[i].dclk, false, &dividers);\r\nif (ret)\r\nreturn ret;\r\npi->uvd_level[i].DclkDivider = (u8)dividers.post_div;\r\npi->uvd_level_count++;\r\n}\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, UvdLevelCount),\r\n(u8 *)&pi->uvd_level_count,\r\nsizeof(u8), pi->sram_end);\r\nif (ret)\r\nreturn ret;\r\npi->uvd_interval = 1;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, UVDInterval),\r\n&pi->uvd_interval,\r\nsizeof(u8), pi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, UvdLevel),\r\n(u8 *)&pi->uvd_level,\r\nsizeof(SMU7_Fusion_UvdLevel) * SMU7_MAX_LEVELS_UVD,\r\npi->sram_end);\r\nreturn ret;\r\n}\r\nstatic int kv_populate_vce_table(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nu32 i;\r\nstruct amdgpu_vce_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;\r\nstruct atom_clock_dividers dividers;\r\nif (table == NULL || table->count == 0)\r\nreturn 0;\r\npi->vce_level_count = 0;\r\nfor (i = 0; i < table->count; i++) {\r\nif (pi->high_voltage_t &&\r\npi->high_voltage_t < table->entries[i].v)\r\nbreak;\r\npi->vce_level[i].Frequency = cpu_to_be32(table->entries[i].evclk);\r\npi->vce_level[i].MinVoltage = cpu_to_be16(table->entries[i].v);\r\npi->vce_level[i].ClkBypassCntl =\r\n(u8)kv_get_clk_bypass(adev, table->entries[i].evclk);\r\nret = amdgpu_atombios_get_clock_dividers(adev, COMPUTE_ENGINE_PLL_PARAM,\r\ntable->entries[i].evclk, false, &dividers);\r\nif (ret)\r\nreturn ret;\r\npi->vce_level[i].Divider = (u8)dividers.post_div;\r\npi->vce_level_count++;\r\n}\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, VceLevelCount),\r\n(u8 *)&pi->vce_level_count,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\npi->vce_interval = 1;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, VCEInterval),\r\n(u8 *)&pi->vce_interval,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, VceLevel),\r\n(u8 *)&pi->vce_level,\r\nsizeof(SMU7_Fusion_ExtClkLevel) * SMU7_MAX_LEVELS_VCE,\r\npi->sram_end);\r\nreturn ret;\r\n}\r\nstatic int kv_populate_samu_table(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table;\r\nstruct atom_clock_dividers dividers;\r\nint ret;\r\nu32 i;\r\nif (table == NULL || table->count == 0)\r\nreturn 0;\r\npi->samu_level_count = 0;\r\nfor (i = 0; i < table->count; i++) {\r\nif (pi->high_voltage_t &&\r\npi->high_voltage_t < table->entries[i].v)\r\nbreak;\r\npi->samu_level[i].Frequency = cpu_to_be32(table->entries[i].clk);\r\npi->samu_level[i].MinVoltage = cpu_to_be16(table->entries[i].v);\r\npi->samu_level[i].ClkBypassCntl =\r\n(u8)kv_get_clk_bypass(adev, table->entries[i].clk);\r\nret = amdgpu_atombios_get_clock_dividers(adev, COMPUTE_ENGINE_PLL_PARAM,\r\ntable->entries[i].clk, false, &dividers);\r\nif (ret)\r\nreturn ret;\r\npi->samu_level[i].Divider = (u8)dividers.post_div;\r\npi->samu_level_count++;\r\n}\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, SamuLevelCount),\r\n(u8 *)&pi->samu_level_count,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\npi->samu_interval = 1;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, SAMUInterval),\r\n(u8 *)&pi->samu_interval,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, SamuLevel),\r\n(u8 *)&pi->samu_level,\r\nsizeof(SMU7_Fusion_ExtClkLevel) * SMU7_MAX_LEVELS_SAMU,\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nreturn ret;\r\n}\r\nstatic int kv_populate_acp_table(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;\r\nstruct atom_clock_dividers dividers;\r\nint ret;\r\nu32 i;\r\nif (table == NULL || table->count == 0)\r\nreturn 0;\r\npi->acp_level_count = 0;\r\nfor (i = 0; i < table->count; i++) {\r\npi->acp_level[i].Frequency = cpu_to_be32(table->entries[i].clk);\r\npi->acp_level[i].MinVoltage = cpu_to_be16(table->entries[i].v);\r\nret = amdgpu_atombios_get_clock_dividers(adev, COMPUTE_ENGINE_PLL_PARAM,\r\ntable->entries[i].clk, false, &dividers);\r\nif (ret)\r\nreturn ret;\r\npi->acp_level[i].Divider = (u8)dividers.post_div;\r\npi->acp_level_count++;\r\n}\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, AcpLevelCount),\r\n(u8 *)&pi->acp_level_count,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\npi->acp_interval = 1;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, ACPInterval),\r\n(u8 *)&pi->acp_interval,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, AcpLevel),\r\n(u8 *)&pi->acp_level,\r\nsizeof(SMU7_Fusion_ExtClkLevel) * SMU7_MAX_LEVELS_ACP,\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nreturn ret;\r\n}\r\nstatic void kv_calculate_dfs_bypass_settings(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nif (table && table->count) {\r\nfor (i = 0; i < pi->graphics_dpm_level_count; i++) {\r\nif (pi->caps_enable_dfs_bypass) {\r\nif (kv_get_clock_difference(table->entries[i].clk, 40000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 3;\r\nelse if (kv_get_clock_difference(table->entries[i].clk, 30000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 2;\r\nelse if (kv_get_clock_difference(table->entries[i].clk, 26600) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 7;\r\nelse if (kv_get_clock_difference(table->entries[i].clk , 20000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 6;\r\nelse if (kv_get_clock_difference(table->entries[i].clk , 10000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 8;\r\nelse\r\npi->graphics_level[i].ClkBypassCntl = 0;\r\n} else {\r\npi->graphics_level[i].ClkBypassCntl = 0;\r\n}\r\n}\r\n} else {\r\nstruct sumo_sclk_voltage_mapping_table *table =\r\n&pi->sys_info.sclk_voltage_mapping_table;\r\nfor (i = 0; i < pi->graphics_dpm_level_count; i++) {\r\nif (pi->caps_enable_dfs_bypass) {\r\nif (kv_get_clock_difference(table->entries[i].sclk_frequency, 40000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 3;\r\nelse if (kv_get_clock_difference(table->entries[i].sclk_frequency, 30000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 2;\r\nelse if (kv_get_clock_difference(table->entries[i].sclk_frequency, 26600) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 7;\r\nelse if (kv_get_clock_difference(table->entries[i].sclk_frequency, 20000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 6;\r\nelse if (kv_get_clock_difference(table->entries[i].sclk_frequency, 10000) < 200)\r\npi->graphics_level[i].ClkBypassCntl = 8;\r\nelse\r\npi->graphics_level[i].ClkBypassCntl = 0;\r\n} else {\r\npi->graphics_level[i].ClkBypassCntl = 0;\r\n}\r\n}\r\n}\r\n}\r\nstatic int kv_enable_ulv(struct amdgpu_device *adev, bool enable)\r\n{\r\nreturn amdgpu_kv_notify_message_to_smu(adev, enable ?\r\nPPSMC_MSG_EnableULV : PPSMC_MSG_DisableULV);\r\n}\r\nstatic void kv_reset_acp_boot_level(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->acp_boot_level = 0xff;\r\n}\r\nstatic void kv_update_current_ps(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *rps)\r\n{\r\nstruct kv_ps *new_ps = kv_get_ps(rps);\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->current_rps = *rps;\r\npi->current_ps = *new_ps;\r\npi->current_rps.ps_priv = &pi->current_ps;\r\n}\r\nstatic void kv_update_requested_ps(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *rps)\r\n{\r\nstruct kv_ps *new_ps = kv_get_ps(rps);\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->requested_rps = *rps;\r\npi->requested_ps = *new_ps;\r\npi->requested_rps.ps_priv = &pi->requested_ps;\r\n}\r\nstatic void kv_dpm_enable_bapm(struct amdgpu_device *adev, bool enable)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nif (pi->bapm_enable) {\r\nret = amdgpu_kv_smc_bapm_enable(adev, enable);\r\nif (ret)\r\nDRM_ERROR("amdgpu_kv_smc_bapm_enable failed\n");\r\n}\r\n}\r\nstatic int kv_dpm_enable(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nret = kv_process_firmware_header(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_process_firmware_header failed\n");\r\nreturn ret;\r\n}\r\nkv_init_fps_limits(adev);\r\nkv_init_graphics_levels(adev);\r\nret = kv_program_bootup_state(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_program_bootup_state failed\n");\r\nreturn ret;\r\n}\r\nkv_calculate_dfs_bypass_settings(adev);\r\nret = kv_upload_dpm_settings(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_upload_dpm_settings failed\n");\r\nreturn ret;\r\n}\r\nret = kv_populate_uvd_table(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_populate_uvd_table failed\n");\r\nreturn ret;\r\n}\r\nret = kv_populate_vce_table(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_populate_vce_table failed\n");\r\nreturn ret;\r\n}\r\nret = kv_populate_samu_table(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_populate_samu_table failed\n");\r\nreturn ret;\r\n}\r\nret = kv_populate_acp_table(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_populate_acp_table failed\n");\r\nreturn ret;\r\n}\r\nkv_program_vc(adev);\r\n#if 0\r\nkv_initialize_hardware_cac_manager(adev);\r\n#endif\r\nkv_start_am(adev);\r\nif (pi->enable_auto_thermal_throttling) {\r\nret = kv_enable_auto_thermal_throttling(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_enable_auto_thermal_throttling failed\n");\r\nreturn ret;\r\n}\r\n}\r\nret = kv_enable_dpm_voltage_scaling(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_enable_dpm_voltage_scaling failed\n");\r\nreturn ret;\r\n}\r\nret = kv_set_dpm_interval(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_set_dpm_interval failed\n");\r\nreturn ret;\r\n}\r\nret = kv_set_dpm_boot_state(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_set_dpm_boot_state failed\n");\r\nreturn ret;\r\n}\r\nret = kv_enable_ulv(adev, true);\r\nif (ret) {\r\nDRM_ERROR("kv_enable_ulv failed\n");\r\nreturn ret;\r\n}\r\nkv_start_dpm(adev);\r\nret = kv_enable_didt(adev, true);\r\nif (ret) {\r\nDRM_ERROR("kv_enable_didt failed\n");\r\nreturn ret;\r\n}\r\nret = kv_enable_smc_cac(adev, true);\r\nif (ret) {\r\nDRM_ERROR("kv_enable_smc_cac failed\n");\r\nreturn ret;\r\n}\r\nkv_reset_acp_boot_level(adev);\r\nret = amdgpu_kv_smc_bapm_enable(adev, false);\r\nif (ret) {\r\nDRM_ERROR("amdgpu_kv_smc_bapm_enable failed\n");\r\nreturn ret;\r\n}\r\nkv_update_current_ps(adev, adev->pm.dpm.boot_ps);\r\nif (adev->irq.installed &&\r\namdgpu_is_internal_thermal_sensor(adev->pm.int_thermal_type)) {\r\nret = kv_set_thermal_temperature_range(adev, KV_TEMP_RANGE_MIN, KV_TEMP_RANGE_MAX);\r\nif (ret) {\r\nDRM_ERROR("kv_set_thermal_temperature_range failed\n");\r\nreturn ret;\r\n}\r\namdgpu_irq_get(adev, &adev->pm.dpm.thermal.irq,\r\nAMDGPU_THERMAL_IRQ_LOW_TO_HIGH);\r\namdgpu_irq_get(adev, &adev->pm.dpm.thermal.irq,\r\nAMDGPU_THERMAL_IRQ_HIGH_TO_LOW);\r\n}\r\nreturn ret;\r\n}\r\nstatic void kv_dpm_disable(struct amdgpu_device *adev)\r\n{\r\namdgpu_irq_put(adev, &adev->pm.dpm.thermal.irq,\r\nAMDGPU_THERMAL_IRQ_LOW_TO_HIGH);\r\namdgpu_irq_put(adev, &adev->pm.dpm.thermal.irq,\r\nAMDGPU_THERMAL_IRQ_HIGH_TO_LOW);\r\namdgpu_kv_smc_bapm_enable(adev, false);\r\nif (adev->asic_type == CHIP_MULLINS)\r\nkv_enable_nb_dpm(adev, false);\r\nkv_dpm_powergate_acp(adev, false);\r\nkv_dpm_powergate_samu(adev, false);\r\nkv_dpm_powergate_vce(adev, false);\r\nkv_dpm_powergate_uvd(adev, false);\r\nkv_enable_smc_cac(adev, false);\r\nkv_enable_didt(adev, false);\r\nkv_clear_vc(adev);\r\nkv_stop_dpm(adev);\r\nkv_enable_ulv(adev, false);\r\nkv_reset_am(adev);\r\nkv_update_current_ps(adev, adev->pm.dpm.boot_ps);\r\n}\r\nstatic void kv_init_sclk_t(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->low_sclk_interrupt_t = 0;\r\n}\r\nstatic int kv_init_fps_limits(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret = 0;\r\nif (pi->caps_fps) {\r\nu16 tmp;\r\ntmp = 45;\r\npi->fps_high_t = cpu_to_be16(tmp);\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, FpsHighT),\r\n(u8 *)&pi->fps_high_t,\r\nsizeof(u16), pi->sram_end);\r\ntmp = 30;\r\npi->fps_low_t = cpu_to_be16(tmp);\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, FpsLowT),\r\n(u8 *)&pi->fps_low_t,\r\nsizeof(u16), pi->sram_end);\r\n}\r\nreturn ret;\r\n}\r\nstatic void kv_init_powergate_state(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->uvd_power_gated = false;\r\npi->vce_power_gated = false;\r\npi->samu_power_gated = false;\r\npi->acp_power_gated = false;\r\n}\r\nstatic int kv_enable_uvd_dpm(struct amdgpu_device *adev, bool enable)\r\n{\r\nreturn amdgpu_kv_notify_message_to_smu(adev, enable ?\r\nPPSMC_MSG_UVDDPM_Enable : PPSMC_MSG_UVDDPM_Disable);\r\n}\r\nstatic int kv_enable_vce_dpm(struct amdgpu_device *adev, bool enable)\r\n{\r\nreturn amdgpu_kv_notify_message_to_smu(adev, enable ?\r\nPPSMC_MSG_VCEDPM_Enable : PPSMC_MSG_VCEDPM_Disable);\r\n}\r\nstatic int kv_enable_samu_dpm(struct amdgpu_device *adev, bool enable)\r\n{\r\nreturn amdgpu_kv_notify_message_to_smu(adev, enable ?\r\nPPSMC_MSG_SAMUDPM_Enable : PPSMC_MSG_SAMUDPM_Disable);\r\n}\r\nstatic int kv_enable_acp_dpm(struct amdgpu_device *adev, bool enable)\r\n{\r\nreturn amdgpu_kv_notify_message_to_smu(adev, enable ?\r\nPPSMC_MSG_ACPDPM_Enable : PPSMC_MSG_ACPDPM_Disable);\r\n}\r\nstatic int kv_update_uvd_dpm(struct amdgpu_device *adev, bool gate)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_uvd_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;\r\nint ret;\r\nu32 mask;\r\nif (!gate) {\r\nif (table->count)\r\npi->uvd_boot_level = table->count - 1;\r\nelse\r\npi->uvd_boot_level = 0;\r\nif (!pi->caps_uvd_dpm || pi->caps_stable_p_state) {\r\nmask = 1 << pi->uvd_boot_level;\r\n} else {\r\nmask = 0x1f;\r\n}\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, UvdBootLevel),\r\n(uint8_t *)&pi->uvd_boot_level,\r\nsizeof(u8), pi->sram_end);\r\nif (ret)\r\nreturn ret;\r\namdgpu_kv_send_msg_to_smc_with_parameter(adev,\r\nPPSMC_MSG_UVDDPM_SetEnabledMask,\r\nmask);\r\n}\r\nreturn kv_enable_uvd_dpm(adev, !gate);\r\n}\r\nstatic u8 kv_get_vce_boot_level(struct amdgpu_device *adev, u32 evclk)\r\n{\r\nu8 i;\r\nstruct amdgpu_vce_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;\r\nfor (i = 0; i < table->count; i++) {\r\nif (table->entries[i].evclk >= evclk)\r\nbreak;\r\n}\r\nreturn i;\r\n}\r\nstatic int kv_update_vce_dpm(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *amdgpu_new_state,\r\nstruct amdgpu_ps *amdgpu_current_state)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_vce_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;\r\nint ret;\r\nif (amdgpu_new_state->evclk > 0 && amdgpu_current_state->evclk == 0) {\r\nkv_dpm_powergate_vce(adev, false);\r\nret = amdgpu_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\r\nAMD_CG_STATE_UNGATE);\r\nif (ret)\r\nreturn ret;\r\nif (pi->caps_stable_p_state)\r\npi->vce_boot_level = table->count - 1;\r\nelse\r\npi->vce_boot_level = kv_get_vce_boot_level(adev, amdgpu_new_state->evclk);\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, VceBootLevel),\r\n(u8 *)&pi->vce_boot_level,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nif (pi->caps_stable_p_state)\r\namdgpu_kv_send_msg_to_smc_with_parameter(adev,\r\nPPSMC_MSG_VCEDPM_SetEnabledMask,\r\n(1 << pi->vce_boot_level));\r\nkv_enable_vce_dpm(adev, true);\r\n} else if (amdgpu_new_state->evclk == 0 && amdgpu_current_state->evclk > 0) {\r\nkv_enable_vce_dpm(adev, false);\r\nret = amdgpu_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\r\nAMD_CG_STATE_GATE);\r\nif (ret)\r\nreturn ret;\r\nkv_dpm_powergate_vce(adev, true);\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_update_samu_dpm(struct amdgpu_device *adev, bool gate)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table;\r\nint ret;\r\nif (!gate) {\r\nif (pi->caps_stable_p_state)\r\npi->samu_boot_level = table->count - 1;\r\nelse\r\npi->samu_boot_level = 0;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, SamuBootLevel),\r\n(u8 *)&pi->samu_boot_level,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nif (pi->caps_stable_p_state)\r\namdgpu_kv_send_msg_to_smc_with_parameter(adev,\r\nPPSMC_MSG_SAMUDPM_SetEnabledMask,\r\n(1 << pi->samu_boot_level));\r\n}\r\nreturn kv_enable_samu_dpm(adev, !gate);\r\n}\r\nstatic u8 kv_get_acp_boot_level(struct amdgpu_device *adev)\r\n{\r\nu8 i;\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;\r\nfor (i = 0; i < table->count; i++) {\r\nif (table->entries[i].clk >= 0)\r\nbreak;\r\n}\r\nif (i >= table->count)\r\ni = table->count - 1;\r\nreturn i;\r\n}\r\nstatic void kv_update_acp_boot_level(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu8 acp_boot_level;\r\nif (!pi->caps_stable_p_state) {\r\nacp_boot_level = kv_get_acp_boot_level(adev);\r\nif (acp_boot_level != pi->acp_boot_level) {\r\npi->acp_boot_level = acp_boot_level;\r\namdgpu_kv_send_msg_to_smc_with_parameter(adev,\r\nPPSMC_MSG_ACPDPM_SetEnabledMask,\r\n(1 << pi->acp_boot_level));\r\n}\r\n}\r\n}\r\nstatic int kv_update_acp_dpm(struct amdgpu_device *adev, bool gate)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;\r\nint ret;\r\nif (!gate) {\r\nif (pi->caps_stable_p_state)\r\npi->acp_boot_level = table->count - 1;\r\nelse\r\npi->acp_boot_level = kv_get_acp_boot_level(adev);\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\npi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, AcpBootLevel),\r\n(u8 *)&pi->acp_boot_level,\r\nsizeof(u8),\r\npi->sram_end);\r\nif (ret)\r\nreturn ret;\r\nif (pi->caps_stable_p_state)\r\namdgpu_kv_send_msg_to_smc_with_parameter(adev,\r\nPPSMC_MSG_ACPDPM_SetEnabledMask,\r\n(1 << pi->acp_boot_level));\r\n}\r\nreturn kv_enable_acp_dpm(adev, !gate);\r\n}\r\nstatic void kv_dpm_powergate_uvd(struct amdgpu_device *adev, bool gate)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nif (pi->uvd_power_gated == gate)\r\nreturn;\r\npi->uvd_power_gated = gate;\r\nif (gate) {\r\nif (pi->caps_uvd_pg) {\r\nret = amdgpu_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\r\nAMD_CG_STATE_UNGATE);\r\nret = amdgpu_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\r\nAMD_PG_STATE_GATE);\r\n}\r\nkv_update_uvd_dpm(adev, gate);\r\nif (pi->caps_uvd_pg)\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_UVDPowerOFF);\r\n} else {\r\nif (pi->caps_uvd_pg) {\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_UVDPowerON);\r\nret = amdgpu_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\r\nAMD_PG_STATE_UNGATE);\r\nret = amdgpu_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\r\nAMD_CG_STATE_GATE);\r\n}\r\nkv_update_uvd_dpm(adev, gate);\r\n}\r\n}\r\nstatic void kv_dpm_powergate_vce(struct amdgpu_device *adev, bool gate)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret;\r\nif (pi->vce_power_gated == gate)\r\nreturn;\r\npi->vce_power_gated = gate;\r\nif (gate) {\r\nif (pi->caps_vce_pg) {\r\nret = amdgpu_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\r\nAMD_PG_STATE_GATE);\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_VCEPowerOFF);\r\n}\r\n} else {\r\nif (pi->caps_vce_pg) {\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_VCEPowerON);\r\nret = amdgpu_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\r\nAMD_PG_STATE_UNGATE);\r\n}\r\n}\r\n}\r\nstatic void kv_dpm_powergate_samu(struct amdgpu_device *adev, bool gate)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nif (pi->samu_power_gated == gate)\r\nreturn;\r\npi->samu_power_gated = gate;\r\nif (gate) {\r\nkv_update_samu_dpm(adev, true);\r\nif (pi->caps_samu_pg)\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_SAMPowerOFF);\r\n} else {\r\nif (pi->caps_samu_pg)\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_SAMPowerON);\r\nkv_update_samu_dpm(adev, false);\r\n}\r\n}\r\nstatic void kv_dpm_powergate_acp(struct amdgpu_device *adev, bool gate)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nif (pi->acp_power_gated == gate)\r\nreturn;\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS)\r\nreturn;\r\npi->acp_power_gated = gate;\r\nif (gate) {\r\nkv_update_acp_dpm(adev, true);\r\nif (pi->caps_acp_pg)\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_ACPPowerOFF);\r\n} else {\r\nif (pi->caps_acp_pg)\r\namdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_ACPPowerON);\r\nkv_update_acp_dpm(adev, false);\r\n}\r\n}\r\nstatic void kv_set_valid_clock_range(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *new_rps)\r\n{\r\nstruct kv_ps *new_ps = kv_get_ps(new_rps);\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nif (table && table->count) {\r\nfor (i = 0; i < pi->graphics_dpm_level_count; i++) {\r\nif ((table->entries[i].clk >= new_ps->levels[0].sclk) ||\r\n(i == (pi->graphics_dpm_level_count - 1))) {\r\npi->lowest_valid = i;\r\nbreak;\r\n}\r\n}\r\nfor (i = pi->graphics_dpm_level_count - 1; i > 0; i--) {\r\nif (table->entries[i].clk <= new_ps->levels[new_ps->num_levels - 1].sclk)\r\nbreak;\r\n}\r\npi->highest_valid = i;\r\nif (pi->lowest_valid > pi->highest_valid) {\r\nif ((new_ps->levels[0].sclk - table->entries[pi->highest_valid].clk) >\r\n(table->entries[pi->lowest_valid].clk - new_ps->levels[new_ps->num_levels - 1].sclk))\r\npi->highest_valid = pi->lowest_valid;\r\nelse\r\npi->lowest_valid = pi->highest_valid;\r\n}\r\n} else {\r\nstruct sumo_sclk_voltage_mapping_table *table =\r\n&pi->sys_info.sclk_voltage_mapping_table;\r\nfor (i = 0; i < (int)pi->graphics_dpm_level_count; i++) {\r\nif (table->entries[i].sclk_frequency >= new_ps->levels[0].sclk ||\r\ni == (int)(pi->graphics_dpm_level_count - 1)) {\r\npi->lowest_valid = i;\r\nbreak;\r\n}\r\n}\r\nfor (i = pi->graphics_dpm_level_count - 1; i > 0; i--) {\r\nif (table->entries[i].sclk_frequency <=\r\nnew_ps->levels[new_ps->num_levels - 1].sclk)\r\nbreak;\r\n}\r\npi->highest_valid = i;\r\nif (pi->lowest_valid > pi->highest_valid) {\r\nif ((new_ps->levels[0].sclk -\r\ntable->entries[pi->highest_valid].sclk_frequency) >\r\n(table->entries[pi->lowest_valid].sclk_frequency -\r\nnew_ps->levels[new_ps->num_levels -1].sclk))\r\npi->highest_valid = pi->lowest_valid;\r\nelse\r\npi->lowest_valid = pi->highest_valid;\r\n}\r\n}\r\n}\r\nstatic int kv_update_dfs_bypass_settings(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *new_rps)\r\n{\r\nstruct kv_ps *new_ps = kv_get_ps(new_rps);\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret = 0;\r\nu8 clk_bypass_cntl;\r\nif (pi->caps_enable_dfs_bypass) {\r\nclk_bypass_cntl = new_ps->need_dfs_bypass ?\r\npi->graphics_level[pi->graphics_boot_level].ClkBypassCntl : 0;\r\nret = amdgpu_kv_copy_bytes_to_smc(adev,\r\n(pi->dpm_table_start +\r\noffsetof(SMU7_Fusion_DpmTable, GraphicsLevel) +\r\n(pi->graphics_boot_level * sizeof(SMU7_Fusion_GraphicsLevel)) +\r\noffsetof(SMU7_Fusion_GraphicsLevel, ClkBypassCntl)),\r\n&clk_bypass_cntl,\r\nsizeof(u8), pi->sram_end);\r\n}\r\nreturn ret;\r\n}\r\nstatic int kv_enable_nb_dpm(struct amdgpu_device *adev,\r\nbool enable)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nint ret = 0;\r\nif (enable) {\r\nif (pi->enable_nb_dpm && !pi->nb_dpm_enabled) {\r\nret = amdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_NBDPM_Enable);\r\nif (ret == 0)\r\npi->nb_dpm_enabled = true;\r\n}\r\n} else {\r\nif (pi->enable_nb_dpm && pi->nb_dpm_enabled) {\r\nret = amdgpu_kv_notify_message_to_smu(adev, PPSMC_MSG_NBDPM_Disable);\r\nif (ret == 0)\r\npi->nb_dpm_enabled = false;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int kv_dpm_force_performance_level(struct amdgpu_device *adev,\r\nenum amdgpu_dpm_forced_level level)\r\n{\r\nint ret;\r\nif (level == AMDGPU_DPM_FORCED_LEVEL_HIGH) {\r\nret = kv_force_dpm_highest(adev);\r\nif (ret)\r\nreturn ret;\r\n} else if (level == AMDGPU_DPM_FORCED_LEVEL_LOW) {\r\nret = kv_force_dpm_lowest(adev);\r\nif (ret)\r\nreturn ret;\r\n} else if (level == AMDGPU_DPM_FORCED_LEVEL_AUTO) {\r\nret = kv_unforce_levels(adev);\r\nif (ret)\r\nreturn ret;\r\n}\r\nadev->pm.dpm.forced_level = level;\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_pre_set_power_state(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_ps requested_ps = *adev->pm.dpm.requested_ps;\r\nstruct amdgpu_ps *new_ps = &requested_ps;\r\nkv_update_requested_ps(adev, new_ps);\r\nkv_apply_state_adjust_rules(adev,\r\n&pi->requested_rps,\r\n&pi->current_rps);\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_set_power_state(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_ps *new_ps = &pi->requested_rps;\r\nstruct amdgpu_ps *old_ps = &pi->current_rps;\r\nint ret;\r\nif (pi->bapm_enable) {\r\nret = amdgpu_kv_smc_bapm_enable(adev, adev->pm.dpm.ac_power);\r\nif (ret) {\r\nDRM_ERROR("amdgpu_kv_smc_bapm_enable failed\n");\r\nreturn ret;\r\n}\r\n}\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS) {\r\nif (pi->enable_dpm) {\r\nkv_set_valid_clock_range(adev, new_ps);\r\nkv_update_dfs_bypass_settings(adev, new_ps);\r\nret = kv_calculate_ds_divider(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_calculate_ds_divider failed\n");\r\nreturn ret;\r\n}\r\nkv_calculate_nbps_level_settings(adev);\r\nkv_calculate_dpm_settings(adev);\r\nkv_force_lowest_valid(adev);\r\nkv_enable_new_levels(adev);\r\nkv_upload_dpm_settings(adev);\r\nkv_program_nbps_index_settings(adev, new_ps);\r\nkv_unforce_levels(adev);\r\nkv_set_enabled_levels(adev);\r\nkv_force_lowest_valid(adev);\r\nkv_unforce_levels(adev);\r\nret = kv_update_vce_dpm(adev, new_ps, old_ps);\r\nif (ret) {\r\nDRM_ERROR("kv_update_vce_dpm failed\n");\r\nreturn ret;\r\n}\r\nkv_update_sclk_t(adev);\r\nif (adev->asic_type == CHIP_MULLINS)\r\nkv_enable_nb_dpm(adev, true);\r\n}\r\n} else {\r\nif (pi->enable_dpm) {\r\nkv_set_valid_clock_range(adev, new_ps);\r\nkv_update_dfs_bypass_settings(adev, new_ps);\r\nret = kv_calculate_ds_divider(adev);\r\nif (ret) {\r\nDRM_ERROR("kv_calculate_ds_divider failed\n");\r\nreturn ret;\r\n}\r\nkv_calculate_nbps_level_settings(adev);\r\nkv_calculate_dpm_settings(adev);\r\nkv_freeze_sclk_dpm(adev, true);\r\nkv_upload_dpm_settings(adev);\r\nkv_program_nbps_index_settings(adev, new_ps);\r\nkv_freeze_sclk_dpm(adev, false);\r\nkv_set_enabled_levels(adev);\r\nret = kv_update_vce_dpm(adev, new_ps, old_ps);\r\nif (ret) {\r\nDRM_ERROR("kv_update_vce_dpm failed\n");\r\nreturn ret;\r\n}\r\nkv_update_acp_boot_level(adev);\r\nkv_update_sclk_t(adev);\r\nkv_enable_nb_dpm(adev, true);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void kv_dpm_post_set_power_state(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_ps *new_ps = &pi->requested_rps;\r\nkv_update_current_ps(adev, new_ps);\r\n}\r\nstatic void kv_dpm_setup_asic(struct amdgpu_device *adev)\r\n{\r\nsumo_take_smu_control(adev, true);\r\nkv_init_powergate_state(adev);\r\nkv_init_sclk_t(adev);\r\n}\r\nstatic void kv_construct_max_power_limits_table(struct amdgpu_device *adev,\r\nstruct amdgpu_clock_and_voltage_limits *table)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nif (pi->sys_info.sclk_voltage_mapping_table.num_max_dpm_entries > 0) {\r\nint idx = pi->sys_info.sclk_voltage_mapping_table.num_max_dpm_entries - 1;\r\ntable->sclk =\r\npi->sys_info.sclk_voltage_mapping_table.entries[idx].sclk_frequency;\r\ntable->vddc =\r\nkv_convert_2bit_index_to_voltage(adev,\r\npi->sys_info.sclk_voltage_mapping_table.entries[idx].vid_2bit);\r\n}\r\ntable->mclk = pi->sys_info.nbp_memory_clock[0];\r\n}\r\nstatic void kv_patch_voltage_values(struct amdgpu_device *adev)\r\n{\r\nint i;\r\nstruct amdgpu_uvd_clock_voltage_dependency_table *uvd_table =\r\n&adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;\r\nstruct amdgpu_vce_clock_voltage_dependency_table *vce_table =\r\n&adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;\r\nstruct amdgpu_clock_voltage_dependency_table *samu_table =\r\n&adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table;\r\nstruct amdgpu_clock_voltage_dependency_table *acp_table =\r\n&adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;\r\nif (uvd_table->count) {\r\nfor (i = 0; i < uvd_table->count; i++)\r\nuvd_table->entries[i].v =\r\nkv_convert_8bit_index_to_voltage(adev,\r\nuvd_table->entries[i].v);\r\n}\r\nif (vce_table->count) {\r\nfor (i = 0; i < vce_table->count; i++)\r\nvce_table->entries[i].v =\r\nkv_convert_8bit_index_to_voltage(adev,\r\nvce_table->entries[i].v);\r\n}\r\nif (samu_table->count) {\r\nfor (i = 0; i < samu_table->count; i++)\r\nsamu_table->entries[i].v =\r\nkv_convert_8bit_index_to_voltage(adev,\r\nsamu_table->entries[i].v);\r\n}\r\nif (acp_table->count) {\r\nfor (i = 0; i < acp_table->count; i++)\r\nacp_table->entries[i].v =\r\nkv_convert_8bit_index_to_voltage(adev,\r\nacp_table->entries[i].v);\r\n}\r\n}\r\nstatic void kv_construct_boot_state(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->boot_pl.sclk = pi->sys_info.bootup_sclk;\r\npi->boot_pl.vddc_index = pi->sys_info.bootup_nb_voltage_index;\r\npi->boot_pl.ds_divider_index = 0;\r\npi->boot_pl.ss_divider_index = 0;\r\npi->boot_pl.allow_gnb_slow = 1;\r\npi->boot_pl.force_nbp_state = 0;\r\npi->boot_pl.display_wm = 0;\r\npi->boot_pl.vce_wm = 0;\r\n}\r\nstatic int kv_force_dpm_highest(struct amdgpu_device *adev)\r\n{\r\nint ret;\r\nu32 enable_mask, i;\r\nret = amdgpu_kv_dpm_get_enable_mask(adev, &enable_mask);\r\nif (ret)\r\nreturn ret;\r\nfor (i = SMU7_MAX_LEVELS_GRAPHICS - 1; i > 0; i--) {\r\nif (enable_mask & (1 << i))\r\nbreak;\r\n}\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS)\r\nreturn amdgpu_kv_send_msg_to_smc_with_parameter(adev, PPSMC_MSG_DPM_ForceState, i);\r\nelse\r\nreturn kv_set_enabled_level(adev, i);\r\n}\r\nstatic int kv_force_dpm_lowest(struct amdgpu_device *adev)\r\n{\r\nint ret;\r\nu32 enable_mask, i;\r\nret = amdgpu_kv_dpm_get_enable_mask(adev, &enable_mask);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < SMU7_MAX_LEVELS_GRAPHICS; i++) {\r\nif (enable_mask & (1 << i))\r\nbreak;\r\n}\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS)\r\nreturn amdgpu_kv_send_msg_to_smc_with_parameter(adev, PPSMC_MSG_DPM_ForceState, i);\r\nelse\r\nreturn kv_set_enabled_level(adev, i);\r\n}\r\nstatic u8 kv_get_sleep_divider_id_from_clock(struct amdgpu_device *adev,\r\nu32 sclk, u32 min_sclk_in_sr)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nu32 temp;\r\nu32 min = (min_sclk_in_sr > KV_MINIMUM_ENGINE_CLOCK) ?\r\nmin_sclk_in_sr : KV_MINIMUM_ENGINE_CLOCK;\r\nif (sclk < min)\r\nreturn 0;\r\nif (!pi->caps_sclk_ds)\r\nreturn 0;\r\nfor (i = KV_MAX_DEEPSLEEP_DIVIDER_ID; i > 0; i--) {\r\ntemp = sclk / sumo_get_sleep_divider_from_id(i);\r\nif (temp >= min)\r\nbreak;\r\n}\r\nreturn (u8)i;\r\n}\r\nstatic int kv_get_high_voltage_limit(struct amdgpu_device *adev, int *limit)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nint i;\r\nif (table && table->count) {\r\nfor (i = table->count - 1; i >= 0; i--) {\r\nif (pi->high_voltage_t &&\r\n(kv_convert_8bit_index_to_voltage(adev, table->entries[i].v) <=\r\npi->high_voltage_t)) {\r\n*limit = i;\r\nreturn 0;\r\n}\r\n}\r\n} else {\r\nstruct sumo_sclk_voltage_mapping_table *table =\r\n&pi->sys_info.sclk_voltage_mapping_table;\r\nfor (i = table->num_max_dpm_entries - 1; i >= 0; i--) {\r\nif (pi->high_voltage_t &&\r\n(kv_convert_2bit_index_to_voltage(adev, table->entries[i].vid_2bit) <=\r\npi->high_voltage_t)) {\r\n*limit = i;\r\nreturn 0;\r\n}\r\n}\r\n}\r\n*limit = 0;\r\nreturn 0;\r\n}\r\nstatic void kv_apply_state_adjust_rules(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *new_rps,\r\nstruct amdgpu_ps *old_rps)\r\n{\r\nstruct kv_ps *ps = kv_get_ps(new_rps);\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 min_sclk = 10000;\r\nu32 sclk, mclk = 0;\r\nint i, limit;\r\nbool force_high;\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nu32 stable_p_state_sclk = 0;\r\nstruct amdgpu_clock_and_voltage_limits *max_limits =\r\n&adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;\r\nif (new_rps->vce_active) {\r\nnew_rps->evclk = adev->pm.dpm.vce_states[adev->pm.dpm.vce_level].evclk;\r\nnew_rps->ecclk = adev->pm.dpm.vce_states[adev->pm.dpm.vce_level].ecclk;\r\n} else {\r\nnew_rps->evclk = 0;\r\nnew_rps->ecclk = 0;\r\n}\r\nmclk = max_limits->mclk;\r\nsclk = min_sclk;\r\nif (pi->caps_stable_p_state) {\r\nstable_p_state_sclk = (max_limits->sclk * 75) / 100;\r\nfor (i = table->count - 1; i >= 0; i++) {\r\nif (stable_p_state_sclk >= table->entries[i].clk) {\r\nstable_p_state_sclk = table->entries[i].clk;\r\nbreak;\r\n}\r\n}\r\nif (i > 0)\r\nstable_p_state_sclk = table->entries[0].clk;\r\nsclk = stable_p_state_sclk;\r\n}\r\nif (new_rps->vce_active) {\r\nif (sclk < adev->pm.dpm.vce_states[adev->pm.dpm.vce_level].sclk)\r\nsclk = adev->pm.dpm.vce_states[adev->pm.dpm.vce_level].sclk;\r\n}\r\nps->need_dfs_bypass = true;\r\nfor (i = 0; i < ps->num_levels; i++) {\r\nif (ps->levels[i].sclk < sclk)\r\nps->levels[i].sclk = sclk;\r\n}\r\nif (table && table->count) {\r\nfor (i = 0; i < ps->num_levels; i++) {\r\nif (pi->high_voltage_t &&\r\n(pi->high_voltage_t <\r\nkv_convert_8bit_index_to_voltage(adev, ps->levels[i].vddc_index))) {\r\nkv_get_high_voltage_limit(adev, &limit);\r\nps->levels[i].sclk = table->entries[limit].clk;\r\n}\r\n}\r\n} else {\r\nstruct sumo_sclk_voltage_mapping_table *table =\r\n&pi->sys_info.sclk_voltage_mapping_table;\r\nfor (i = 0; i < ps->num_levels; i++) {\r\nif (pi->high_voltage_t &&\r\n(pi->high_voltage_t <\r\nkv_convert_8bit_index_to_voltage(adev, ps->levels[i].vddc_index))) {\r\nkv_get_high_voltage_limit(adev, &limit);\r\nps->levels[i].sclk = table->entries[limit].sclk_frequency;\r\n}\r\n}\r\n}\r\nif (pi->caps_stable_p_state) {\r\nfor (i = 0; i < ps->num_levels; i++) {\r\nps->levels[i].sclk = stable_p_state_sclk;\r\n}\r\n}\r\npi->video_start = new_rps->dclk || new_rps->vclk ||\r\nnew_rps->evclk || new_rps->ecclk;\r\nif ((new_rps->class & ATOM_PPLIB_CLASSIFICATION_UI_MASK) ==\r\nATOM_PPLIB_CLASSIFICATION_UI_BATTERY)\r\npi->battery_state = true;\r\nelse\r\npi->battery_state = false;\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS) {\r\nps->dpm0_pg_nb_ps_lo = 0x1;\r\nps->dpm0_pg_nb_ps_hi = 0x0;\r\nps->dpmx_nb_ps_lo = 0x1;\r\nps->dpmx_nb_ps_hi = 0x0;\r\n} else {\r\nps->dpm0_pg_nb_ps_lo = 0x3;\r\nps->dpm0_pg_nb_ps_hi = 0x0;\r\nps->dpmx_nb_ps_lo = 0x3;\r\nps->dpmx_nb_ps_hi = 0x0;\r\nif (pi->sys_info.nb_dpm_enable) {\r\nforce_high = (mclk >= pi->sys_info.nbp_memory_clock[3]) ||\r\npi->video_start || (adev->pm.dpm.new_active_crtc_count >= 3) ||\r\npi->disable_nb_ps3_in_battery;\r\nps->dpm0_pg_nb_ps_lo = force_high ? 0x2 : 0x3;\r\nps->dpm0_pg_nb_ps_hi = 0x2;\r\nps->dpmx_nb_ps_lo = force_high ? 0x2 : 0x3;\r\nps->dpmx_nb_ps_hi = 0x2;\r\n}\r\n}\r\n}\r\nstatic void kv_dpm_power_level_enabled_for_throttle(struct amdgpu_device *adev,\r\nu32 index, bool enable)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\npi->graphics_level[index].EnabledForThrottle = enable ? 1 : 0;\r\n}\r\nstatic int kv_calculate_ds_divider(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 sclk_in_sr = 10000;\r\nu32 i;\r\nif (pi->lowest_valid > pi->highest_valid)\r\nreturn -EINVAL;\r\nfor (i = pi->lowest_valid; i <= pi->highest_valid; i++) {\r\npi->graphics_level[i].DeepSleepDivId =\r\nkv_get_sleep_divider_id_from_clock(adev,\r\nbe32_to_cpu(pi->graphics_level[i].SclkFrequency),\r\nsclk_in_sr);\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_calculate_nbps_level_settings(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nbool force_high;\r\nstruct amdgpu_clock_and_voltage_limits *max_limits =\r\n&adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;\r\nu32 mclk = max_limits->mclk;\r\nif (pi->lowest_valid > pi->highest_valid)\r\nreturn -EINVAL;\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS) {\r\nfor (i = pi->lowest_valid; i <= pi->highest_valid; i++) {\r\npi->graphics_level[i].GnbSlow = 1;\r\npi->graphics_level[i].ForceNbPs1 = 0;\r\npi->graphics_level[i].UpH = 0;\r\n}\r\nif (!pi->sys_info.nb_dpm_enable)\r\nreturn 0;\r\nforce_high = ((mclk >= pi->sys_info.nbp_memory_clock[3]) ||\r\n(adev->pm.dpm.new_active_crtc_count >= 3) || pi->video_start);\r\nif (force_high) {\r\nfor (i = pi->lowest_valid; i <= pi->highest_valid; i++)\r\npi->graphics_level[i].GnbSlow = 0;\r\n} else {\r\nif (pi->battery_state)\r\npi->graphics_level[0].ForceNbPs1 = 1;\r\npi->graphics_level[1].GnbSlow = 0;\r\npi->graphics_level[2].GnbSlow = 0;\r\npi->graphics_level[3].GnbSlow = 0;\r\npi->graphics_level[4].GnbSlow = 0;\r\n}\r\n} else {\r\nfor (i = pi->lowest_valid; i <= pi->highest_valid; i++) {\r\npi->graphics_level[i].GnbSlow = 1;\r\npi->graphics_level[i].ForceNbPs1 = 0;\r\npi->graphics_level[i].UpH = 0;\r\n}\r\nif (pi->sys_info.nb_dpm_enable && pi->battery_state) {\r\npi->graphics_level[pi->lowest_valid].UpH = 0x28;\r\npi->graphics_level[pi->lowest_valid].GnbSlow = 0;\r\nif (pi->lowest_valid != pi->highest_valid)\r\npi->graphics_level[pi->lowest_valid].ForceNbPs1 = 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_calculate_dpm_settings(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nif (pi->lowest_valid > pi->highest_valid)\r\nreturn -EINVAL;\r\nfor (i = pi->lowest_valid; i <= pi->highest_valid; i++)\r\npi->graphics_level[i].DisplayWatermark = (i == pi->highest_valid) ? 1 : 0;\r\nreturn 0;\r\n}\r\nstatic void kv_init_graphics_levels(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nstruct amdgpu_clock_voltage_dependency_table *table =\r\n&adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;\r\nif (table && table->count) {\r\nu32 vid_2bit;\r\npi->graphics_dpm_level_count = 0;\r\nfor (i = 0; i < table->count; i++) {\r\nif (pi->high_voltage_t &&\r\n(pi->high_voltage_t <\r\nkv_convert_8bit_index_to_voltage(adev, table->entries[i].v)))\r\nbreak;\r\nkv_set_divider_value(adev, i, table->entries[i].clk);\r\nvid_2bit = kv_convert_vid7_to_vid2(adev,\r\n&pi->sys_info.vid_mapping_table,\r\ntable->entries[i].v);\r\nkv_set_vid(adev, i, vid_2bit);\r\nkv_set_at(adev, i, pi->at[i]);\r\nkv_dpm_power_level_enabled_for_throttle(adev, i, true);\r\npi->graphics_dpm_level_count++;\r\n}\r\n} else {\r\nstruct sumo_sclk_voltage_mapping_table *table =\r\n&pi->sys_info.sclk_voltage_mapping_table;\r\npi->graphics_dpm_level_count = 0;\r\nfor (i = 0; i < table->num_max_dpm_entries; i++) {\r\nif (pi->high_voltage_t &&\r\npi->high_voltage_t <\r\nkv_convert_2bit_index_to_voltage(adev, table->entries[i].vid_2bit))\r\nbreak;\r\nkv_set_divider_value(adev, i, table->entries[i].sclk_frequency);\r\nkv_set_vid(adev, i, table->entries[i].vid_2bit);\r\nkv_set_at(adev, i, pi->at[i]);\r\nkv_dpm_power_level_enabled_for_throttle(adev, i, true);\r\npi->graphics_dpm_level_count++;\r\n}\r\n}\r\nfor (i = 0; i < SMU7_MAX_LEVELS_GRAPHICS; i++)\r\nkv_dpm_power_level_enable(adev, i, false);\r\n}\r\nstatic void kv_enable_new_levels(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i;\r\nfor (i = 0; i < SMU7_MAX_LEVELS_GRAPHICS; i++) {\r\nif (i >= pi->lowest_valid && i <= pi->highest_valid)\r\nkv_dpm_power_level_enable(adev, i, true);\r\n}\r\n}\r\nstatic int kv_set_enabled_level(struct amdgpu_device *adev, u32 level)\r\n{\r\nu32 new_mask = (1 << level);\r\nreturn amdgpu_kv_send_msg_to_smc_with_parameter(adev,\r\nPPSMC_MSG_SCLKDPM_SetEnabledMask,\r\nnew_mask);\r\n}\r\nstatic int kv_set_enabled_levels(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 i, new_mask = 0;\r\nfor (i = pi->lowest_valid; i <= pi->highest_valid; i++)\r\nnew_mask |= (1 << i);\r\nreturn amdgpu_kv_send_msg_to_smc_with_parameter(adev,\r\nPPSMC_MSG_SCLKDPM_SetEnabledMask,\r\nnew_mask);\r\n}\r\nstatic void kv_program_nbps_index_settings(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *new_rps)\r\n{\r\nstruct kv_ps *new_ps = kv_get_ps(new_rps);\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 nbdpmconfig1;\r\nif (adev->asic_type == CHIP_KABINI || adev->asic_type == CHIP_MULLINS)\r\nreturn;\r\nif (pi->sys_info.nb_dpm_enable) {\r\nnbdpmconfig1 = RREG32_SMC(ixNB_DPM_CONFIG_1);\r\nnbdpmconfig1 &= ~(NB_DPM_CONFIG_1__Dpm0PgNbPsLo_MASK |\r\nNB_DPM_CONFIG_1__Dpm0PgNbPsHi_MASK |\r\nNB_DPM_CONFIG_1__DpmXNbPsLo_MASK |\r\nNB_DPM_CONFIG_1__DpmXNbPsHi_MASK);\r\nnbdpmconfig1 |= (new_ps->dpm0_pg_nb_ps_lo << NB_DPM_CONFIG_1__Dpm0PgNbPsLo__SHIFT) |\r\n(new_ps->dpm0_pg_nb_ps_hi << NB_DPM_CONFIG_1__Dpm0PgNbPsHi__SHIFT) |\r\n(new_ps->dpmx_nb_ps_lo << NB_DPM_CONFIG_1__DpmXNbPsLo__SHIFT) |\r\n(new_ps->dpmx_nb_ps_hi << NB_DPM_CONFIG_1__DpmXNbPsHi__SHIFT);\r\nWREG32_SMC(ixNB_DPM_CONFIG_1, nbdpmconfig1);\r\n}\r\n}\r\nstatic int kv_set_thermal_temperature_range(struct amdgpu_device *adev,\r\nint min_temp, int max_temp)\r\n{\r\nint low_temp = 0 * 1000;\r\nint high_temp = 255 * 1000;\r\nu32 tmp;\r\nif (low_temp < min_temp)\r\nlow_temp = min_temp;\r\nif (high_temp > max_temp)\r\nhigh_temp = max_temp;\r\nif (high_temp < low_temp) {\r\nDRM_ERROR("invalid thermal range: %d - %d\n", low_temp, high_temp);\r\nreturn -EINVAL;\r\n}\r\ntmp = RREG32_SMC(ixCG_THERMAL_INT_CTRL);\r\ntmp &= ~(CG_THERMAL_INT_CTRL__DIG_THERM_INTH_MASK |\r\nCG_THERMAL_INT_CTRL__DIG_THERM_INTL_MASK);\r\ntmp |= ((49 + (high_temp / 1000)) << CG_THERMAL_INT_CTRL__DIG_THERM_INTH__SHIFT) |\r\n((49 + (low_temp / 1000)) << CG_THERMAL_INT_CTRL__DIG_THERM_INTL__SHIFT);\r\nWREG32_SMC(ixCG_THERMAL_INT_CTRL, tmp);\r\nadev->pm.dpm.thermal.min_temp = low_temp;\r\nadev->pm.dpm.thermal.max_temp = high_temp;\r\nreturn 0;\r\n}\r\nstatic int kv_parse_sys_info_table(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct amdgpu_mode_info *mode_info = &adev->mode_info;\r\nint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\r\nunion igp_info *igp_info;\r\nu8 frev, crev;\r\nu16 data_offset;\r\nint i;\r\nif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\r\n&frev, &crev, &data_offset)) {\r\nigp_info = (union igp_info *)(mode_info->atom_context->bios +\r\ndata_offset);\r\nif (crev != 8) {\r\nDRM_ERROR("Unsupported IGP table: %d %d\n", frev, crev);\r\nreturn -EINVAL;\r\n}\r\npi->sys_info.bootup_sclk = le32_to_cpu(igp_info->info_8.ulBootUpEngineClock);\r\npi->sys_info.bootup_uma_clk = le32_to_cpu(igp_info->info_8.ulBootUpUMAClock);\r\npi->sys_info.bootup_nb_voltage_index =\r\nle16_to_cpu(igp_info->info_8.usBootUpNBVoltage);\r\nif (igp_info->info_8.ucHtcTmpLmt == 0)\r\npi->sys_info.htc_tmp_lmt = 203;\r\nelse\r\npi->sys_info.htc_tmp_lmt = igp_info->info_8.ucHtcTmpLmt;\r\nif (igp_info->info_8.ucHtcHystLmt == 0)\r\npi->sys_info.htc_hyst_lmt = 5;\r\nelse\r\npi->sys_info.htc_hyst_lmt = igp_info->info_8.ucHtcHystLmt;\r\nif (pi->sys_info.htc_tmp_lmt <= pi->sys_info.htc_hyst_lmt) {\r\nDRM_ERROR("The htcTmpLmt should be larger than htcHystLmt.\n");\r\n}\r\nif (le32_to_cpu(igp_info->info_8.ulSystemConfig) & (1 << 3))\r\npi->sys_info.nb_dpm_enable = true;\r\nelse\r\npi->sys_info.nb_dpm_enable = false;\r\nfor (i = 0; i < KV_NUM_NBPSTATES; i++) {\r\npi->sys_info.nbp_memory_clock[i] =\r\nle32_to_cpu(igp_info->info_8.ulNbpStateMemclkFreq[i]);\r\npi->sys_info.nbp_n_clock[i] =\r\nle32_to_cpu(igp_info->info_8.ulNbpStateNClkFreq[i]);\r\n}\r\nif (le32_to_cpu(igp_info->info_8.ulGPUCapInfo) &\r\nSYS_INFO_GPUCAPS__ENABEL_DFS_BYPASS)\r\npi->caps_enable_dfs_bypass = true;\r\nsumo_construct_sclk_voltage_mapping_table(adev,\r\n&pi->sys_info.sclk_voltage_mapping_table,\r\nigp_info->info_8.sAvail_SCLK);\r\nsumo_construct_vid_mapping_table(adev,\r\n&pi->sys_info.vid_mapping_table,\r\nigp_info->info_8.sAvail_SCLK);\r\nkv_construct_max_power_limits_table(adev,\r\n&adev->pm.dpm.dyn_state.max_clock_voltage_on_ac);\r\n}\r\nreturn 0;\r\n}\r\nstatic void kv_patch_boot_state(struct amdgpu_device *adev,\r\nstruct kv_ps *ps)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nps->num_levels = 1;\r\nps->levels[0] = pi->boot_pl;\r\n}\r\nstatic void kv_parse_pplib_non_clock_info(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *rps,\r\nstruct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info,\r\nu8 table_rev)\r\n{\r\nstruct kv_ps *ps = kv_get_ps(rps);\r\nrps->caps = le32_to_cpu(non_clock_info->ulCapsAndSettings);\r\nrps->class = le16_to_cpu(non_clock_info->usClassification);\r\nrps->class2 = le16_to_cpu(non_clock_info->usClassification2);\r\nif (ATOM_PPLIB_NONCLOCKINFO_VER1 < table_rev) {\r\nrps->vclk = le32_to_cpu(non_clock_info->ulVCLK);\r\nrps->dclk = le32_to_cpu(non_clock_info->ulDCLK);\r\n} else {\r\nrps->vclk = 0;\r\nrps->dclk = 0;\r\n}\r\nif (rps->class & ATOM_PPLIB_CLASSIFICATION_BOOT) {\r\nadev->pm.dpm.boot_ps = rps;\r\nkv_patch_boot_state(adev, ps);\r\n}\r\nif (rps->class & ATOM_PPLIB_CLASSIFICATION_UVDSTATE)\r\nadev->pm.dpm.uvd_ps = rps;\r\n}\r\nstatic void kv_parse_pplib_clock_info(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *rps, int index,\r\nunion pplib_clock_info *clock_info)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct kv_ps *ps = kv_get_ps(rps);\r\nstruct kv_pl *pl = &ps->levels[index];\r\nu32 sclk;\r\nsclk = le16_to_cpu(clock_info->sumo.usEngineClockLow);\r\nsclk |= clock_info->sumo.ucEngineClockHigh << 16;\r\npl->sclk = sclk;\r\npl->vddc_index = clock_info->sumo.vddcIndex;\r\nps->num_levels = index + 1;\r\nif (pi->caps_sclk_ds) {\r\npl->ds_divider_index = 5;\r\npl->ss_divider_index = 5;\r\n}\r\n}\r\nstatic int kv_parse_power_table(struct amdgpu_device *adev)\r\n{\r\nstruct amdgpu_mode_info *mode_info = &adev->mode_info;\r\nstruct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info;\r\nunion pplib_power_state *power_state;\r\nint i, j, k, non_clock_array_index, clock_array_index;\r\nunion pplib_clock_info *clock_info;\r\nstruct _StateArray *state_array;\r\nstruct _ClockInfoArray *clock_info_array;\r\nstruct _NonClockInfoArray *non_clock_info_array;\r\nunion power_info *power_info;\r\nint index = GetIndexIntoMasterTable(DATA, PowerPlayInfo);\r\nu16 data_offset;\r\nu8 frev, crev;\r\nu8 *power_state_offset;\r\nstruct kv_ps *ps;\r\nif (!amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\r\n&frev, &crev, &data_offset))\r\nreturn -EINVAL;\r\npower_info = (union power_info *)(mode_info->atom_context->bios + data_offset);\r\namdgpu_add_thermal_controller(adev);\r\nstate_array = (struct _StateArray *)\r\n(mode_info->atom_context->bios + data_offset +\r\nle16_to_cpu(power_info->pplib.usStateArrayOffset));\r\nclock_info_array = (struct _ClockInfoArray *)\r\n(mode_info->atom_context->bios + data_offset +\r\nle16_to_cpu(power_info->pplib.usClockInfoArrayOffset));\r\nnon_clock_info_array = (struct _NonClockInfoArray *)\r\n(mode_info->atom_context->bios + data_offset +\r\nle16_to_cpu(power_info->pplib.usNonClockInfoArrayOffset));\r\nadev->pm.dpm.ps = kzalloc(sizeof(struct amdgpu_ps) *\r\nstate_array->ucNumEntries, GFP_KERNEL);\r\nif (!adev->pm.dpm.ps)\r\nreturn -ENOMEM;\r\npower_state_offset = (u8 *)state_array->states;\r\nfor (i = 0; i < state_array->ucNumEntries; i++) {\r\nu8 *idx;\r\npower_state = (union pplib_power_state *)power_state_offset;\r\nnon_clock_array_index = power_state->v2.nonClockInfoIndex;\r\nnon_clock_info = (struct _ATOM_PPLIB_NONCLOCK_INFO *)\r\n&non_clock_info_array->nonClockInfo[non_clock_array_index];\r\nps = kzalloc(sizeof(struct kv_ps), GFP_KERNEL);\r\nif (ps == NULL) {\r\nkfree(adev->pm.dpm.ps);\r\nreturn -ENOMEM;\r\n}\r\nadev->pm.dpm.ps[i].ps_priv = ps;\r\nk = 0;\r\nidx = (u8 *)&power_state->v2.clockInfoIndex[0];\r\nfor (j = 0; j < power_state->v2.ucNumDPMLevels; j++) {\r\nclock_array_index = idx[j];\r\nif (clock_array_index >= clock_info_array->ucNumEntries)\r\ncontinue;\r\nif (k >= SUMO_MAX_HARDWARE_POWERLEVELS)\r\nbreak;\r\nclock_info = (union pplib_clock_info *)\r\n((u8 *)&clock_info_array->clockInfo[0] +\r\n(clock_array_index * clock_info_array->ucEntrySize));\r\nkv_parse_pplib_clock_info(adev,\r\n&adev->pm.dpm.ps[i], k,\r\nclock_info);\r\nk++;\r\n}\r\nkv_parse_pplib_non_clock_info(adev, &adev->pm.dpm.ps[i],\r\nnon_clock_info,\r\nnon_clock_info_array->ucEntrySize);\r\npower_state_offset += 2 + power_state->v2.ucNumDPMLevels;\r\n}\r\nadev->pm.dpm.num_ps = state_array->ucNumEntries;\r\nfor (i = 0; i < AMDGPU_MAX_VCE_LEVELS; i++) {\r\nu32 sclk;\r\nclock_array_index = adev->pm.dpm.vce_states[i].clk_idx;\r\nclock_info = (union pplib_clock_info *)\r\n&clock_info_array->clockInfo[clock_array_index * clock_info_array->ucEntrySize];\r\nsclk = le16_to_cpu(clock_info->sumo.usEngineClockLow);\r\nsclk |= clock_info->sumo.ucEngineClockHigh << 16;\r\nadev->pm.dpm.vce_states[i].sclk = sclk;\r\nadev->pm.dpm.vce_states[i].mclk = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_init(struct amdgpu_device *adev)\r\n{\r\nstruct kv_power_info *pi;\r\nint ret, i;\r\npi = kzalloc(sizeof(struct kv_power_info), GFP_KERNEL);\r\nif (pi == NULL)\r\nreturn -ENOMEM;\r\nadev->pm.dpm.priv = pi;\r\nret = amdgpu_get_platform_caps(adev);\r\nif (ret)\r\nreturn ret;\r\nret = amdgpu_parse_extended_power_table(adev);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < SUMO_MAX_HARDWARE_POWERLEVELS; i++)\r\npi->at[i] = TRINITY_AT_DFLT;\r\npi->sram_end = SMC_RAM_END;\r\npi->enable_nb_dpm = true;\r\npi->caps_power_containment = true;\r\npi->caps_cac = true;\r\npi->enable_didt = false;\r\nif (pi->enable_didt) {\r\npi->caps_sq_ramping = true;\r\npi->caps_db_ramping = true;\r\npi->caps_td_ramping = true;\r\npi->caps_tcp_ramping = true;\r\n}\r\npi->caps_sclk_ds = true;\r\npi->enable_auto_thermal_throttling = true;\r\npi->disable_nb_ps3_in_battery = false;\r\nif (amdgpu_bapm == 0)\r\npi->bapm_enable = false;\r\nelse\r\npi->bapm_enable = true;\r\npi->voltage_drop_t = 0;\r\npi->caps_sclk_throttle_low_notification = false;\r\npi->caps_fps = false;\r\npi->caps_uvd_pg = (adev->pg_flags & AMDGPU_PG_SUPPORT_UVD) ? true : false;\r\npi->caps_uvd_dpm = true;\r\npi->caps_vce_pg = (adev->pg_flags & AMDGPU_PG_SUPPORT_VCE) ? true : false;\r\npi->caps_samu_pg = (adev->pg_flags & AMDGPU_PG_SUPPORT_SAMU) ? true : false;\r\npi->caps_acp_pg = (adev->pg_flags & AMDGPU_PG_SUPPORT_ACP) ? true : false;\r\npi->caps_stable_p_state = false;\r\nret = kv_parse_sys_info_table(adev);\r\nif (ret)\r\nreturn ret;\r\nkv_patch_voltage_values(adev);\r\nkv_construct_boot_state(adev);\r\nret = kv_parse_power_table(adev);\r\nif (ret)\r\nreturn ret;\r\npi->enable_dpm = true;\r\nreturn 0;\r\n}\r\nstatic void\r\nkv_dpm_debugfs_print_current_performance_level(struct amdgpu_device *adev,\r\nstruct seq_file *m)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nu32 current_index =\r\n(RREG32_SMC(ixTARGET_AND_CURRENT_PROFILE_INDEX) &\r\nTARGET_AND_CURRENT_PROFILE_INDEX__CURR_SCLK_INDEX_MASK) >>\r\nTARGET_AND_CURRENT_PROFILE_INDEX__CURR_SCLK_INDEX__SHIFT;\r\nu32 sclk, tmp;\r\nu16 vddc;\r\nif (current_index >= SMU__NUM_SCLK_DPM_STATE) {\r\nseq_printf(m, "invalid dpm profile %d\n", current_index);\r\n} else {\r\nsclk = be32_to_cpu(pi->graphics_level[current_index].SclkFrequency);\r\ntmp = (RREG32_SMC(ixSMU_VOLTAGE_STATUS) &\r\nSMU_VOLTAGE_STATUS__SMU_VOLTAGE_CURRENT_LEVEL_MASK) >>\r\nSMU_VOLTAGE_STATUS__SMU_VOLTAGE_CURRENT_LEVEL__SHIFT;\r\nvddc = kv_convert_8bit_index_to_voltage(adev, (u16)tmp);\r\nseq_printf(m, "uvd %sabled\n", pi->uvd_power_gated ? "dis" : "en");\r\nseq_printf(m, "vce %sabled\n", pi->vce_power_gated ? "dis" : "en");\r\nseq_printf(m, "power level %d sclk: %u vddc: %u\n",\r\ncurrent_index, sclk, vddc);\r\n}\r\n}\r\nstatic void\r\nkv_dpm_print_power_state(struct amdgpu_device *adev,\r\nstruct amdgpu_ps *rps)\r\n{\r\nint i;\r\nstruct kv_ps *ps = kv_get_ps(rps);\r\namdgpu_dpm_print_class_info(rps->class, rps->class2);\r\namdgpu_dpm_print_cap_info(rps->caps);\r\nprintk("\tuvd vclk: %d dclk: %d\n", rps->vclk, rps->dclk);\r\nfor (i = 0; i < ps->num_levels; i++) {\r\nstruct kv_pl *pl = &ps->levels[i];\r\nprintk("\t\tpower level %d sclk: %u vddc: %u\n",\r\ni, pl->sclk,\r\nkv_convert_8bit_index_to_voltage(adev, pl->vddc_index));\r\n}\r\namdgpu_dpm_print_ps_status(adev, rps);\r\n}\r\nstatic void kv_dpm_fini(struct amdgpu_device *adev)\r\n{\r\nint i;\r\nfor (i = 0; i < adev->pm.dpm.num_ps; i++) {\r\nkfree(adev->pm.dpm.ps[i].ps_priv);\r\n}\r\nkfree(adev->pm.dpm.ps);\r\nkfree(adev->pm.dpm.priv);\r\namdgpu_free_extended_power_table(adev);\r\n}\r\nstatic void kv_dpm_display_configuration_changed(struct amdgpu_device *adev)\r\n{\r\n}\r\nstatic u32 kv_dpm_get_sclk(struct amdgpu_device *adev, bool low)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nstruct kv_ps *requested_state = kv_get_ps(&pi->requested_rps);\r\nif (low)\r\nreturn requested_state->levels[0].sclk;\r\nelse\r\nreturn requested_state->levels[requested_state->num_levels - 1].sclk;\r\n}\r\nstatic u32 kv_dpm_get_mclk(struct amdgpu_device *adev, bool low)\r\n{\r\nstruct kv_power_info *pi = kv_get_pi(adev);\r\nreturn pi->sys_info.bootup_uma_clk;\r\n}\r\nstatic int kv_dpm_get_temp(struct amdgpu_device *adev)\r\n{\r\nu32 temp;\r\nint actual_temp = 0;\r\ntemp = RREG32_SMC(0xC0300E0C);\r\nif (temp)\r\nactual_temp = (temp / 8) - 49;\r\nelse\r\nactual_temp = 0;\r\nactual_temp = actual_temp * 1000;\r\nreturn actual_temp;\r\n}\r\nstatic int kv_dpm_early_init(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nkv_dpm_set_dpm_funcs(adev);\r\nkv_dpm_set_irq_funcs(adev);\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_late_init(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nint ret;\r\nif (!amdgpu_dpm)\r\nreturn 0;\r\nret = amdgpu_pm_sysfs_init(adev);\r\nif (ret)\r\nreturn ret;\r\nkv_dpm_powergate_acp(adev, true);\r\nkv_dpm_powergate_samu(adev, true);\r\nkv_dpm_powergate_vce(adev, true);\r\nkv_dpm_powergate_uvd(adev, true);\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_sw_init(void *handle)\r\n{\r\nint ret;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nret = amdgpu_irq_add_id(adev, 230, &adev->pm.dpm.thermal.irq);\r\nif (ret)\r\nreturn ret;\r\nret = amdgpu_irq_add_id(adev, 231, &adev->pm.dpm.thermal.irq);\r\nif (ret)\r\nreturn ret;\r\nadev->pm.dpm.state = POWER_STATE_TYPE_BALANCED;\r\nadev->pm.dpm.user_state = POWER_STATE_TYPE_BALANCED;\r\nadev->pm.dpm.forced_level = AMDGPU_DPM_FORCED_LEVEL_AUTO;\r\nadev->pm.default_sclk = adev->clock.default_sclk;\r\nadev->pm.default_mclk = adev->clock.default_mclk;\r\nadev->pm.current_sclk = adev->clock.default_sclk;\r\nadev->pm.current_mclk = adev->clock.default_mclk;\r\nadev->pm.int_thermal_type = THERMAL_TYPE_NONE;\r\nif (amdgpu_dpm == 0)\r\nreturn 0;\r\nINIT_WORK(&adev->pm.dpm.thermal.work, amdgpu_dpm_thermal_work_handler);\r\nmutex_lock(&adev->pm.mutex);\r\nret = kv_dpm_init(adev);\r\nif (ret)\r\ngoto dpm_failed;\r\nadev->pm.dpm.current_ps = adev->pm.dpm.requested_ps = adev->pm.dpm.boot_ps;\r\nif (amdgpu_dpm == 1)\r\namdgpu_pm_print_power_states(adev);\r\nmutex_unlock(&adev->pm.mutex);\r\nDRM_INFO("amdgpu: dpm initialized\n");\r\nreturn 0;\r\ndpm_failed:\r\nkv_dpm_fini(adev);\r\nmutex_unlock(&adev->pm.mutex);\r\nDRM_ERROR("amdgpu: dpm initialization failed\n");\r\nreturn ret;\r\n}\r\nstatic int kv_dpm_sw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nmutex_lock(&adev->pm.mutex);\r\namdgpu_pm_sysfs_fini(adev);\r\nkv_dpm_fini(adev);\r\nmutex_unlock(&adev->pm.mutex);\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_hw_init(void *handle)\r\n{\r\nint ret;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nmutex_lock(&adev->pm.mutex);\r\nkv_dpm_setup_asic(adev);\r\nret = kv_dpm_enable(adev);\r\nif (ret)\r\nadev->pm.dpm_enabled = false;\r\nelse\r\nadev->pm.dpm_enabled = true;\r\nmutex_unlock(&adev->pm.mutex);\r\nreturn ret;\r\n}\r\nstatic int kv_dpm_hw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nif (adev->pm.dpm_enabled) {\r\nmutex_lock(&adev->pm.mutex);\r\nkv_dpm_disable(adev);\r\nmutex_unlock(&adev->pm.mutex);\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_suspend(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nif (adev->pm.dpm_enabled) {\r\nmutex_lock(&adev->pm.mutex);\r\nkv_dpm_disable(adev);\r\nadev->pm.dpm.current_ps = adev->pm.dpm.requested_ps = adev->pm.dpm.boot_ps;\r\nmutex_unlock(&adev->pm.mutex);\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_resume(void *handle)\r\n{\r\nint ret;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nif (adev->pm.dpm_enabled) {\r\nmutex_lock(&adev->pm.mutex);\r\nkv_dpm_setup_asic(adev);\r\nret = kv_dpm_enable(adev);\r\nif (ret)\r\nadev->pm.dpm_enabled = false;\r\nelse\r\nadev->pm.dpm_enabled = true;\r\nmutex_unlock(&adev->pm.mutex);\r\nif (adev->pm.dpm_enabled)\r\namdgpu_pm_compute_clocks(adev);\r\n}\r\nreturn 0;\r\n}\r\nstatic bool kv_dpm_is_idle(void *handle)\r\n{\r\nreturn true;\r\n}\r\nstatic int kv_dpm_wait_for_idle(void *handle)\r\n{\r\nreturn 0;\r\n}\r\nstatic void kv_dpm_print_status(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\ndev_info(adev->dev, "KV/KB DPM registers\n");\r\ndev_info(adev->dev, " DIDT_SQ_CTRL0=0x%08X\n",\r\nRREG32_DIDT(ixDIDT_SQ_CTRL0));\r\ndev_info(adev->dev, " DIDT_DB_CTRL0=0x%08X\n",\r\nRREG32_DIDT(ixDIDT_DB_CTRL0));\r\ndev_info(adev->dev, " DIDT_TD_CTRL0=0x%08X\n",\r\nRREG32_DIDT(ixDIDT_TD_CTRL0));\r\ndev_info(adev->dev, " DIDT_TCP_CTRL0=0x%08X\n",\r\nRREG32_DIDT(ixDIDT_TCP_CTRL0));\r\ndev_info(adev->dev, " LCAC_SX0_OVR_SEL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_SX0_OVR_SEL));\r\ndev_info(adev->dev, " LCAC_SX0_OVR_VAL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_SX0_OVR_VAL));\r\ndev_info(adev->dev, " LCAC_MC0_OVR_SEL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC0_OVR_SEL));\r\ndev_info(adev->dev, " LCAC_MC0_OVR_VAL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC0_OVR_VAL));\r\ndev_info(adev->dev, " LCAC_MC1_OVR_SEL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC1_OVR_SEL));\r\ndev_info(adev->dev, " LCAC_MC1_OVR_VAL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC1_OVR_VAL));\r\ndev_info(adev->dev, " LCAC_MC2_OVR_SEL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC2_OVR_SEL));\r\ndev_info(adev->dev, " LCAC_MC2_OVR_VAL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC2_OVR_VAL));\r\ndev_info(adev->dev, " LCAC_MC3_OVR_SEL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC3_OVR_SEL));\r\ndev_info(adev->dev, " LCAC_MC3_OVR_VAL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_MC3_OVR_VAL));\r\ndev_info(adev->dev, " LCAC_CPL_OVR_SEL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_CPL_OVR_SEL));\r\ndev_info(adev->dev, " LCAC_CPL_OVR_VAL=0x%08X\n",\r\nRREG32_SMC(ixLCAC_CPL_OVR_VAL));\r\ndev_info(adev->dev, " CG_FREQ_TRAN_VOTING_0=0x%08X\n",\r\nRREG32_SMC(ixCG_FREQ_TRAN_VOTING_0));\r\ndev_info(adev->dev, " GENERAL_PWRMGT=0x%08X\n",\r\nRREG32_SMC(ixGENERAL_PWRMGT));\r\ndev_info(adev->dev, " SCLK_PWRMGT_CNTL=0x%08X\n",\r\nRREG32_SMC(ixSCLK_PWRMGT_CNTL));\r\ndev_info(adev->dev, " SMC_MESSAGE_0=0x%08X\n",\r\nRREG32(mmSMC_MESSAGE_0));\r\ndev_info(adev->dev, " SMC_RESP_0=0x%08X\n",\r\nRREG32(mmSMC_RESP_0));\r\ndev_info(adev->dev, " SMC_MSG_ARG_0=0x%08X\n",\r\nRREG32(mmSMC_MSG_ARG_0));\r\ndev_info(adev->dev, " SMC_IND_INDEX_0=0x%08X\n",\r\nRREG32(mmSMC_IND_INDEX_0));\r\ndev_info(adev->dev, " SMC_IND_DATA_0=0x%08X\n",\r\nRREG32(mmSMC_IND_DATA_0));\r\ndev_info(adev->dev, " SMC_IND_ACCESS_CNTL=0x%08X\n",\r\nRREG32(mmSMC_IND_ACCESS_CNTL));\r\n}\r\nstatic int kv_dpm_soft_reset(void *handle)\r\n{\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_set_interrupt_state(struct amdgpu_device *adev,\r\nstruct amdgpu_irq_src *src,\r\nunsigned type,\r\nenum amdgpu_interrupt_state state)\r\n{\r\nu32 cg_thermal_int;\r\nswitch (type) {\r\ncase AMDGPU_THERMAL_IRQ_LOW_TO_HIGH:\r\nswitch (state) {\r\ncase AMDGPU_IRQ_STATE_DISABLE:\r\ncg_thermal_int = RREG32_SMC(ixCG_THERMAL_INT_CTRL);\r\ncg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;\r\nWREG32_SMC(ixCG_THERMAL_INT_CTRL, cg_thermal_int);\r\nbreak;\r\ncase AMDGPU_IRQ_STATE_ENABLE:\r\ncg_thermal_int = RREG32_SMC(ixCG_THERMAL_INT_CTRL);\r\ncg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTH_MASK_MASK;\r\nWREG32_SMC(ixCG_THERMAL_INT_CTRL, cg_thermal_int);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase AMDGPU_THERMAL_IRQ_HIGH_TO_LOW:\r\nswitch (state) {\r\ncase AMDGPU_IRQ_STATE_DISABLE:\r\ncg_thermal_int = RREG32_SMC(ixCG_THERMAL_INT_CTRL);\r\ncg_thermal_int &= ~CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;\r\nWREG32_SMC(ixCG_THERMAL_INT_CTRL, cg_thermal_int);\r\nbreak;\r\ncase AMDGPU_IRQ_STATE_ENABLE:\r\ncg_thermal_int = RREG32_SMC(ixCG_THERMAL_INT_CTRL);\r\ncg_thermal_int |= CG_THERMAL_INT_CTRL__THERM_INTL_MASK_MASK;\r\nWREG32_SMC(ixCG_THERMAL_INT_CTRL, cg_thermal_int);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_process_interrupt(struct amdgpu_device *adev,\r\nstruct amdgpu_irq_src *source,\r\nstruct amdgpu_iv_entry *entry)\r\n{\r\nbool queue_thermal = false;\r\nif (entry == NULL)\r\nreturn -EINVAL;\r\nswitch (entry->src_id) {\r\ncase 230:\r\nDRM_DEBUG("IH: thermal low to high\n");\r\nadev->pm.dpm.thermal.high_to_low = false;\r\nqueue_thermal = true;\r\nbreak;\r\ncase 231:\r\nDRM_DEBUG("IH: thermal high to low\n");\r\nadev->pm.dpm.thermal.high_to_low = true;\r\nqueue_thermal = true;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (queue_thermal)\r\nschedule_work(&adev->pm.dpm.thermal.work);\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_set_clockgating_state(void *handle,\r\nenum amd_clockgating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic int kv_dpm_set_powergating_state(void *handle,\r\nenum amd_powergating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic void kv_dpm_set_dpm_funcs(struct amdgpu_device *adev)\r\n{\r\nif (adev->pm.funcs == NULL)\r\nadev->pm.funcs = &kv_dpm_funcs;\r\n}\r\nstatic void kv_dpm_set_irq_funcs(struct amdgpu_device *adev)\r\n{\r\nadev->pm.dpm.thermal.irq.num_types = AMDGPU_THERMAL_IRQ_LAST;\r\nadev->pm.dpm.thermal.irq.funcs = &kv_dpm_irq_funcs;\r\n}
