void fsnotify_get_mark(struct fsnotify_mark *mark)\r\n{\r\natomic_inc(&mark->refcnt);\r\n}\r\nvoid fsnotify_put_mark(struct fsnotify_mark *mark)\r\n{\r\nif (atomic_dec_and_test(&mark->refcnt)) {\r\nif (mark->group)\r\nfsnotify_put_group(mark->group);\r\nmark->free_mark(mark);\r\n}\r\n}\r\nu32 fsnotify_recalc_mask(struct hlist_head *head)\r\n{\r\nu32 new_mask = 0;\r\nstruct fsnotify_mark *mark;\r\nhlist_for_each_entry(mark, head, obj_list)\r\nnew_mask |= mark->mask;\r\nreturn new_mask;\r\n}\r\nvoid fsnotify_detach_mark(struct fsnotify_mark *mark)\r\n{\r\nstruct inode *inode = NULL;\r\nstruct fsnotify_group *group = mark->group;\r\nBUG_ON(!mutex_is_locked(&group->mark_mutex));\r\nspin_lock(&mark->lock);\r\nif (!(mark->flags & FSNOTIFY_MARK_FLAG_ATTACHED)) {\r\nspin_unlock(&mark->lock);\r\nreturn;\r\n}\r\nmark->flags &= ~FSNOTIFY_MARK_FLAG_ATTACHED;\r\nif (mark->flags & FSNOTIFY_MARK_FLAG_INODE) {\r\ninode = mark->inode;\r\nfsnotify_destroy_inode_mark(mark);\r\n} else if (mark->flags & FSNOTIFY_MARK_FLAG_VFSMOUNT)\r\nfsnotify_destroy_vfsmount_mark(mark);\r\nelse\r\nBUG();\r\nlist_del_init(&mark->g_list);\r\nspin_unlock(&mark->lock);\r\nif (inode && (mark->flags & FSNOTIFY_MARK_FLAG_OBJECT_PINNED))\r\niput(inode);\r\natomic_dec(&group->num_marks);\r\n}\r\nvoid fsnotify_free_mark(struct fsnotify_mark *mark)\r\n{\r\nstruct fsnotify_group *group = mark->group;\r\nspin_lock(&mark->lock);\r\nif (!(mark->flags & FSNOTIFY_MARK_FLAG_ALIVE)) {\r\nspin_unlock(&mark->lock);\r\nreturn;\r\n}\r\nmark->flags &= ~FSNOTIFY_MARK_FLAG_ALIVE;\r\nspin_unlock(&mark->lock);\r\nspin_lock(&destroy_lock);\r\nlist_add(&mark->g_list, &destroy_list);\r\nspin_unlock(&destroy_lock);\r\nwake_up(&destroy_waitq);\r\nif (group->ops->freeing_mark)\r\ngroup->ops->freeing_mark(mark, group);\r\n}\r\nvoid fsnotify_destroy_mark(struct fsnotify_mark *mark,\r\nstruct fsnotify_group *group)\r\n{\r\nmutex_lock_nested(&group->mark_mutex, SINGLE_DEPTH_NESTING);\r\nfsnotify_detach_mark(mark);\r\nmutex_unlock(&group->mark_mutex);\r\nfsnotify_free_mark(mark);\r\n}\r\nvoid fsnotify_destroy_marks(struct hlist_head *head, spinlock_t *lock)\r\n{\r\nstruct fsnotify_mark *mark;\r\nwhile (1) {\r\nspin_lock(lock);\r\nif (hlist_empty(head)) {\r\nspin_unlock(lock);\r\nbreak;\r\n}\r\nmark = hlist_entry(head->first, struct fsnotify_mark, obj_list);\r\nhlist_del_init_rcu(&mark->obj_list);\r\nfsnotify_get_mark(mark);\r\nspin_unlock(lock);\r\nfsnotify_destroy_mark(mark, mark->group);\r\nfsnotify_put_mark(mark);\r\n}\r\n}\r\nvoid fsnotify_set_mark_mask_locked(struct fsnotify_mark *mark, __u32 mask)\r\n{\r\nassert_spin_locked(&mark->lock);\r\nmark->mask = mask;\r\nif (mark->flags & FSNOTIFY_MARK_FLAG_INODE)\r\nfsnotify_set_inode_mark_mask_locked(mark, mask);\r\n}\r\nvoid fsnotify_set_mark_ignored_mask_locked(struct fsnotify_mark *mark, __u32 mask)\r\n{\r\nassert_spin_locked(&mark->lock);\r\nmark->ignored_mask = mask;\r\n}\r\nint fsnotify_compare_groups(struct fsnotify_group *a, struct fsnotify_group *b)\r\n{\r\nif (a == b)\r\nreturn 0;\r\nif (!a)\r\nreturn 1;\r\nif (!b)\r\nreturn -1;\r\nif (a->priority < b->priority)\r\nreturn 1;\r\nif (a->priority > b->priority)\r\nreturn -1;\r\nif (a < b)\r\nreturn 1;\r\nreturn -1;\r\n}\r\nint fsnotify_add_mark_list(struct hlist_head *head, struct fsnotify_mark *mark,\r\nint allow_dups)\r\n{\r\nstruct fsnotify_mark *lmark, *last = NULL;\r\nint cmp;\r\nif (hlist_empty(head)) {\r\nhlist_add_head_rcu(&mark->obj_list, head);\r\nreturn 0;\r\n}\r\nhlist_for_each_entry(lmark, head, obj_list) {\r\nlast = lmark;\r\nif ((lmark->group == mark->group) && !allow_dups)\r\nreturn -EEXIST;\r\ncmp = fsnotify_compare_groups(lmark->group, mark->group);\r\nif (cmp >= 0) {\r\nhlist_add_before_rcu(&mark->obj_list, &lmark->obj_list);\r\nreturn 0;\r\n}\r\n}\r\nBUG_ON(last == NULL);\r\nhlist_add_behind_rcu(&mark->obj_list, &last->obj_list);\r\nreturn 0;\r\n}\r\nint fsnotify_add_mark_locked(struct fsnotify_mark *mark,\r\nstruct fsnotify_group *group, struct inode *inode,\r\nstruct vfsmount *mnt, int allow_dups)\r\n{\r\nint ret = 0;\r\nBUG_ON(inode && mnt);\r\nBUG_ON(!inode && !mnt);\r\nBUG_ON(!mutex_is_locked(&group->mark_mutex));\r\nspin_lock(&mark->lock);\r\nmark->flags |= FSNOTIFY_MARK_FLAG_ALIVE | FSNOTIFY_MARK_FLAG_ATTACHED;\r\nfsnotify_get_group(group);\r\nmark->group = group;\r\nlist_add(&mark->g_list, &group->marks_list);\r\natomic_inc(&group->num_marks);\r\nfsnotify_get_mark(mark);\r\nif (inode) {\r\nret = fsnotify_add_inode_mark(mark, group, inode, allow_dups);\r\nif (ret)\r\ngoto err;\r\n} else if (mnt) {\r\nret = fsnotify_add_vfsmount_mark(mark, group, mnt, allow_dups);\r\nif (ret)\r\ngoto err;\r\n} else {\r\nBUG();\r\n}\r\nfsnotify_set_mark_mask_locked(mark, mark->mask);\r\nspin_unlock(&mark->lock);\r\nif (inode)\r\n__fsnotify_update_child_dentry_flags(inode);\r\nreturn ret;\r\nerr:\r\nmark->flags &= ~FSNOTIFY_MARK_FLAG_ALIVE;\r\nlist_del_init(&mark->g_list);\r\nfsnotify_put_group(group);\r\nmark->group = NULL;\r\natomic_dec(&group->num_marks);\r\nspin_unlock(&mark->lock);\r\nspin_lock(&destroy_lock);\r\nlist_add(&mark->g_list, &destroy_list);\r\nspin_unlock(&destroy_lock);\r\nwake_up(&destroy_waitq);\r\nreturn ret;\r\n}\r\nint fsnotify_add_mark(struct fsnotify_mark *mark, struct fsnotify_group *group,\r\nstruct inode *inode, struct vfsmount *mnt, int allow_dups)\r\n{\r\nint ret;\r\nmutex_lock(&group->mark_mutex);\r\nret = fsnotify_add_mark_locked(mark, group, inode, mnt, allow_dups);\r\nmutex_unlock(&group->mark_mutex);\r\nreturn ret;\r\n}\r\nstruct fsnotify_mark *fsnotify_find_mark(struct hlist_head *head,\r\nstruct fsnotify_group *group)\r\n{\r\nstruct fsnotify_mark *mark;\r\nhlist_for_each_entry(mark, head, obj_list) {\r\nif (mark->group == group) {\r\nfsnotify_get_mark(mark);\r\nreturn mark;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nvoid fsnotify_clear_marks_by_group_flags(struct fsnotify_group *group,\r\nunsigned int flags)\r\n{\r\nstruct fsnotify_mark *lmark, *mark;\r\nLIST_HEAD(to_free);\r\nmutex_lock_nested(&group->mark_mutex, SINGLE_DEPTH_NESTING);\r\nlist_for_each_entry_safe(mark, lmark, &group->marks_list, g_list) {\r\nif (mark->flags & flags)\r\nlist_move(&mark->g_list, &to_free);\r\n}\r\nmutex_unlock(&group->mark_mutex);\r\nwhile (1) {\r\nmutex_lock_nested(&group->mark_mutex, SINGLE_DEPTH_NESTING);\r\nif (list_empty(&to_free)) {\r\nmutex_unlock(&group->mark_mutex);\r\nbreak;\r\n}\r\nmark = list_first_entry(&to_free, struct fsnotify_mark, g_list);\r\nfsnotify_get_mark(mark);\r\nfsnotify_detach_mark(mark);\r\nmutex_unlock(&group->mark_mutex);\r\nfsnotify_free_mark(mark);\r\nfsnotify_put_mark(mark);\r\n}\r\n}\r\nvoid fsnotify_clear_marks_by_group(struct fsnotify_group *group)\r\n{\r\nfsnotify_clear_marks_by_group_flags(group, (unsigned int)-1);\r\n}\r\nvoid fsnotify_duplicate_mark(struct fsnotify_mark *new, struct fsnotify_mark *old)\r\n{\r\nassert_spin_locked(&old->lock);\r\nnew->inode = old->inode;\r\nnew->mnt = old->mnt;\r\nif (old->group)\r\nfsnotify_get_group(old->group);\r\nnew->group = old->group;\r\nnew->mask = old->mask;\r\nnew->free_mark = old->free_mark;\r\n}\r\nvoid fsnotify_init_mark(struct fsnotify_mark *mark,\r\nvoid (*free_mark)(struct fsnotify_mark *mark))\r\n{\r\nmemset(mark, 0, sizeof(*mark));\r\nspin_lock_init(&mark->lock);\r\natomic_set(&mark->refcnt, 1);\r\nmark->free_mark = free_mark;\r\n}\r\nstatic int fsnotify_mark_destroy(void *ignored)\r\n{\r\nstruct fsnotify_mark *mark, *next;\r\nstruct list_head private_destroy_list;\r\nfor (;;) {\r\nspin_lock(&destroy_lock);\r\nlist_replace_init(&destroy_list, &private_destroy_list);\r\nspin_unlock(&destroy_lock);\r\nsynchronize_srcu(&fsnotify_mark_srcu);\r\nlist_for_each_entry_safe(mark, next, &private_destroy_list, g_list) {\r\nlist_del_init(&mark->g_list);\r\nfsnotify_put_mark(mark);\r\n}\r\nwait_event_interruptible(destroy_waitq, !list_empty(&destroy_list));\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init fsnotify_mark_init(void)\r\n{\r\nstruct task_struct *thread;\r\nthread = kthread_run(fsnotify_mark_destroy, NULL,\r\n"fsnotify_mark");\r\nif (IS_ERR(thread))\r\npanic("unable to start fsnotify mark destruction thread.");\r\nreturn 0;\r\n}
