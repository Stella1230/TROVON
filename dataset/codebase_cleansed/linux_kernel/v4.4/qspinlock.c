static inline u32 encode_tail(int cpu, int idx)\r\n{\r\nu32 tail;\r\n#ifdef CONFIG_DEBUG_SPINLOCK\r\nBUG_ON(idx > 3);\r\n#endif\r\ntail = (cpu + 1) << _Q_TAIL_CPU_OFFSET;\r\ntail |= idx << _Q_TAIL_IDX_OFFSET;\r\nreturn tail;\r\n}\r\nstatic inline struct mcs_spinlock *decode_tail(u32 tail)\r\n{\r\nint cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;\r\nint idx = (tail & _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;\r\nreturn per_cpu_ptr(&mcs_nodes[idx], cpu);\r\n}\r\nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\r\n{\r\nstruct __qspinlock *l = (void *)lock;\r\nWRITE_ONCE(l->locked_pending, _Q_LOCKED_VAL);\r\n}\r\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\r\n{\r\nstruct __qspinlock *l = (void *)lock;\r\nreturn (u32)xchg(&l->tail, tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;\r\n}\r\nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\r\n{\r\natomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);\r\n}\r\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\r\n{\r\nu32 old, new, val = atomic_read(&lock->val);\r\nfor (;;) {\r\nnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\r\nold = atomic_cmpxchg(&lock->val, val, new);\r\nif (old == val)\r\nbreak;\r\nval = old;\r\n}\r\nreturn old;\r\n}\r\nstatic __always_inline void set_locked(struct qspinlock *lock)\r\n{\r\nstruct __qspinlock *l = (void *)lock;\r\nWRITE_ONCE(l->locked, _Q_LOCKED_VAL);\r\n}\r\nstatic __always_inline void __pv_init_node(struct mcs_spinlock *node) { }\r\nstatic __always_inline void __pv_wait_node(struct mcs_spinlock *node) { }\r\nstatic __always_inline void __pv_kick_node(struct qspinlock *lock,\r\nstruct mcs_spinlock *node) { }\r\nstatic __always_inline void __pv_wait_head(struct qspinlock *lock,\r\nstruct mcs_spinlock *node) { }\r\nvoid queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)\r\n{\r\nstruct mcs_spinlock *prev, *next, *node;\r\nu32 new, old, tail;\r\nint idx;\r\nBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));\r\nif (pv_enabled())\r\ngoto queue;\r\nif (virt_spin_lock(lock))\r\nreturn;\r\nif (val == _Q_PENDING_VAL) {\r\nwhile ((val = atomic_read(&lock->val)) == _Q_PENDING_VAL)\r\ncpu_relax();\r\n}\r\nfor (;;) {\r\nif (val & ~_Q_LOCKED_MASK)\r\ngoto queue;\r\nnew = _Q_LOCKED_VAL;\r\nif (val == new)\r\nnew |= _Q_PENDING_VAL;\r\nold = atomic_cmpxchg(&lock->val, val, new);\r\nif (old == val)\r\nbreak;\r\nval = old;\r\n}\r\nif (new == _Q_LOCKED_VAL)\r\nreturn;\r\nwhile ((val = smp_load_acquire(&lock->val.counter)) & _Q_LOCKED_MASK)\r\ncpu_relax();\r\nclear_pending_set_locked(lock);\r\nreturn;\r\nqueue:\r\nnode = this_cpu_ptr(&mcs_nodes[0]);\r\nidx = node->count++;\r\ntail = encode_tail(smp_processor_id(), idx);\r\nnode += idx;\r\nnode->locked = 0;\r\nnode->next = NULL;\r\npv_init_node(node);\r\nif (queued_spin_trylock(lock))\r\ngoto release;\r\nold = xchg_tail(lock, tail);\r\nif (old & _Q_TAIL_MASK) {\r\nprev = decode_tail(old);\r\nWRITE_ONCE(prev->next, node);\r\npv_wait_node(node);\r\narch_mcs_spin_lock_contended(&node->locked);\r\n}\r\npv_wait_head(lock, node);\r\nwhile ((val = smp_load_acquire(&lock->val.counter)) & _Q_LOCKED_PENDING_MASK)\r\ncpu_relax();\r\nfor (;;) {\r\nif (val != tail) {\r\nset_locked(lock);\r\nbreak;\r\n}\r\nold = atomic_cmpxchg(&lock->val, val, _Q_LOCKED_VAL);\r\nif (old == val)\r\ngoto release;\r\nval = old;\r\n}\r\nwhile (!(next = READ_ONCE(node->next)))\r\ncpu_relax();\r\narch_mcs_spin_unlock_contended(&next->locked);\r\npv_kick_node(lock, next);\r\nrelease:\r\nthis_cpu_dec(mcs_nodes[0].count);\r\n}
