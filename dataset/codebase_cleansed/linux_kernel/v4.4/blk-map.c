static bool iovec_gap_to_prv(struct request_queue *q,\r\nstruct iovec *prv, struct iovec *cur)\r\n{\r\nunsigned long prev_end;\r\nif (!queue_virt_boundary(q))\r\nreturn false;\r\nif (prv->iov_base == NULL && prv->iov_len == 0)\r\nreturn false;\r\nprev_end = (unsigned long)(prv->iov_base + prv->iov_len);\r\nreturn (((unsigned long)cur->iov_base & queue_virt_boundary(q)) ||\r\nprev_end & queue_virt_boundary(q));\r\n}\r\nint blk_rq_append_bio(struct request_queue *q, struct request *rq,\r\nstruct bio *bio)\r\n{\r\nif (!rq->bio)\r\nblk_rq_bio_prep(q, rq, bio);\r\nelse if (!ll_back_merge_fn(q, rq, bio))\r\nreturn -EINVAL;\r\nelse {\r\nrq->biotail->bi_next = bio;\r\nrq->biotail = bio;\r\nrq->__data_len += bio->bi_iter.bi_size;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __blk_rq_unmap_user(struct bio *bio)\r\n{\r\nint ret = 0;\r\nif (bio) {\r\nif (bio_flagged(bio, BIO_USER_MAPPED))\r\nbio_unmap_user(bio);\r\nelse\r\nret = bio_uncopy_user(bio);\r\n}\r\nreturn ret;\r\n}\r\nint blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\r\nstruct rq_map_data *map_data,\r\nconst struct iov_iter *iter, gfp_t gfp_mask)\r\n{\r\nstruct bio *bio;\r\nint unaligned = 0;\r\nstruct iov_iter i;\r\nstruct iovec iov, prv = {.iov_base = NULL, .iov_len = 0};\r\nif (!iter || !iter->count)\r\nreturn -EINVAL;\r\niov_for_each(iov, i, *iter) {\r\nunsigned long uaddr = (unsigned long) iov.iov_base;\r\nif (!iov.iov_len)\r\nreturn -EINVAL;\r\nif ((uaddr & queue_dma_alignment(q)) ||\r\niovec_gap_to_prv(q, &prv, &iov))\r\nunaligned = 1;\r\nprv.iov_base = iov.iov_base;\r\nprv.iov_len = iov.iov_len;\r\n}\r\nif (unaligned || (q->dma_pad_mask & iter->count) || map_data)\r\nbio = bio_copy_user_iov(q, map_data, iter, gfp_mask);\r\nelse\r\nbio = bio_map_user_iov(q, iter, gfp_mask);\r\nif (IS_ERR(bio))\r\nreturn PTR_ERR(bio);\r\nif (map_data && map_data->null_mapped)\r\nbio_set_flag(bio, BIO_NULL_MAPPED);\r\nif (bio->bi_iter.bi_size != iter->count) {\r\nbio_get(bio);\r\nbio_endio(bio);\r\n__blk_rq_unmap_user(bio);\r\nreturn -EINVAL;\r\n}\r\nif (!bio_flagged(bio, BIO_USER_MAPPED))\r\nrq->cmd_flags |= REQ_COPY_USER;\r\nblk_queue_bounce(q, &bio);\r\nbio_get(bio);\r\nblk_rq_bio_prep(q, rq, bio);\r\nreturn 0;\r\n}\r\nint blk_rq_map_user(struct request_queue *q, struct request *rq,\r\nstruct rq_map_data *map_data, void __user *ubuf,\r\nunsigned long len, gfp_t gfp_mask)\r\n{\r\nstruct iovec iov;\r\nstruct iov_iter i;\r\nint ret = import_single_range(rq_data_dir(rq), ubuf, len, &iov, &i);\r\nif (unlikely(ret < 0))\r\nreturn ret;\r\nreturn blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);\r\n}\r\nint blk_rq_unmap_user(struct bio *bio)\r\n{\r\nstruct bio *mapped_bio;\r\nint ret = 0, ret2;\r\nwhile (bio) {\r\nmapped_bio = bio;\r\nif (unlikely(bio_flagged(bio, BIO_BOUNCED)))\r\nmapped_bio = bio->bi_private;\r\nret2 = __blk_rq_unmap_user(mapped_bio);\r\nif (ret2 && !ret)\r\nret = ret2;\r\nmapped_bio = bio;\r\nbio = bio->bi_next;\r\nbio_put(mapped_bio);\r\n}\r\nreturn ret;\r\n}\r\nint blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,\r\nunsigned int len, gfp_t gfp_mask)\r\n{\r\nint reading = rq_data_dir(rq) == READ;\r\nunsigned long addr = (unsigned long) kbuf;\r\nint do_copy = 0;\r\nstruct bio *bio;\r\nint ret;\r\nif (len > (queue_max_hw_sectors(q) << 9))\r\nreturn -EINVAL;\r\nif (!len || !kbuf)\r\nreturn -EINVAL;\r\ndo_copy = !blk_rq_aligned(q, addr, len) || object_is_on_stack(kbuf);\r\nif (do_copy)\r\nbio = bio_copy_kern(q, kbuf, len, gfp_mask, reading);\r\nelse\r\nbio = bio_map_kern(q, kbuf, len, gfp_mask);\r\nif (IS_ERR(bio))\r\nreturn PTR_ERR(bio);\r\nif (!reading)\r\nbio->bi_rw |= REQ_WRITE;\r\nif (do_copy)\r\nrq->cmd_flags |= REQ_COPY_USER;\r\nret = blk_rq_append_bio(q, rq, bio);\r\nif (unlikely(ret)) {\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nblk_queue_bounce(q, &rq->bio);\r\nreturn 0;\r\n}
