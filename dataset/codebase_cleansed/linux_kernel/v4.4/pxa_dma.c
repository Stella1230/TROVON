static unsigned int pxad_drcmr(unsigned int line)\r\n{\r\nif (line < 64)\r\nreturn 0x100 + line * 4;\r\nreturn 0x1000 + line * 4;\r\n}\r\nstatic int dbg_show_requester_chan(struct seq_file *s, void *p)\r\n{\r\nstruct pxad_phy *phy = s->private;\r\nint i;\r\nu32 drcmr;\r\nseq_printf(s, "DMA channel %d requester :\n", phy->idx);\r\nfor (i = 0; i < 70; i++) {\r\ndrcmr = readl_relaxed(phy->base + pxad_drcmr(i));\r\nif ((drcmr & DRCMR_CHLNUM) == phy->idx)\r\nseq_printf(s, "\tRequester %d (MAPVLD=%d)\n", i,\r\n!!(drcmr & DRCMR_MAPVLD));\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int dbg_burst_from_dcmd(u32 dcmd)\r\n{\r\nint burst = (dcmd >> 16) & 0x3;\r\nreturn burst ? 4 << burst : 0;\r\n}\r\nstatic int is_phys_valid(unsigned long addr)\r\n{\r\nreturn pfn_valid(__phys_to_pfn(addr));\r\n}\r\nstatic int dbg_show_descriptors(struct seq_file *s, void *p)\r\n{\r\nstruct pxad_phy *phy = s->private;\r\nint i, max_show = 20, burst, width;\r\nu32 dcmd;\r\nunsigned long phys_desc, ddadr;\r\nstruct pxad_desc_hw *desc;\r\nphys_desc = ddadr = _phy_readl_relaxed(phy, DDADR);\r\nseq_printf(s, "DMA channel %d descriptors :\n", phy->idx);\r\nseq_printf(s, "[%03d] First descriptor unknown\n", 0);\r\nfor (i = 1; i < max_show && is_phys_valid(phys_desc); i++) {\r\ndesc = phys_to_virt(phys_desc);\r\ndcmd = desc->dcmd;\r\nburst = dbg_burst_from_dcmd(dcmd);\r\nwidth = (1 << ((dcmd >> 14) & 0x3)) >> 1;\r\nseq_printf(s, "[%03d] Desc at %08lx(virt %p)\n",\r\ni, phys_desc, desc);\r\nseq_printf(s, "\tDDADR = %08x\n", desc->ddadr);\r\nseq_printf(s, "\tDSADR = %08x\n", desc->dsadr);\r\nseq_printf(s, "\tDTADR = %08x\n", desc->dtadr);\r\nseq_printf(s, "\tDCMD = %08x (%s%s%s%s%s%s%sburst=%d width=%d len=%d)\n",\r\ndcmd,\r\nPXA_DCMD_STR(INCSRCADDR), PXA_DCMD_STR(INCTRGADDR),\r\nPXA_DCMD_STR(FLOWSRC), PXA_DCMD_STR(FLOWTRG),\r\nPXA_DCMD_STR(STARTIRQEN), PXA_DCMD_STR(ENDIRQEN),\r\nPXA_DCMD_STR(ENDIAN), burst, width,\r\ndcmd & PXA_DCMD_LENGTH);\r\nphys_desc = desc->ddadr;\r\n}\r\nif (i == max_show)\r\nseq_printf(s, "[%03d] Desc at %08lx ... max display reached\n",\r\ni, phys_desc);\r\nelse\r\nseq_printf(s, "[%03d] Desc at %08lx is %s\n",\r\ni, phys_desc, phys_desc == DDADR_STOP ?\r\n"DDADR_STOP" : "invalid");\r\nreturn 0;\r\n}\r\nstatic int dbg_show_chan_state(struct seq_file *s, void *p)\r\n{\r\nstruct pxad_phy *phy = s->private;\r\nu32 dcsr, dcmd;\r\nint burst, width;\r\nstatic const char * const str_prio[] = {\r\n"high", "normal", "low", "invalid"\r\n};\r\ndcsr = _phy_readl_relaxed(phy, DCSR);\r\ndcmd = _phy_readl_relaxed(phy, DCMD);\r\nburst = dbg_burst_from_dcmd(dcmd);\r\nwidth = (1 << ((dcmd >> 14) & 0x3)) >> 1;\r\nseq_printf(s, "DMA channel %d\n", phy->idx);\r\nseq_printf(s, "\tPriority : %s\n",\r\nstr_prio[(phy->idx & 0xf) / 4]);\r\nseq_printf(s, "\tUnaligned transfer bit: %s\n",\r\n_phy_readl_relaxed(phy, DALGN) & BIT(phy->idx) ?\r\n"yes" : "no");\r\nseq_printf(s, "\tDCSR = %08x (%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s)\n",\r\ndcsr, PXA_DCSR_STR(RUN), PXA_DCSR_STR(NODESC),\r\nPXA_DCSR_STR(STOPIRQEN), PXA_DCSR_STR(EORIRQEN),\r\nPXA_DCSR_STR(EORJMPEN), PXA_DCSR_STR(EORSTOPEN),\r\nPXA_DCSR_STR(SETCMPST), PXA_DCSR_STR(CLRCMPST),\r\nPXA_DCSR_STR(CMPST), PXA_DCSR_STR(EORINTR),\r\nPXA_DCSR_STR(REQPEND), PXA_DCSR_STR(STOPSTATE),\r\nPXA_DCSR_STR(ENDINTR), PXA_DCSR_STR(STARTINTR),\r\nPXA_DCSR_STR(BUSERR));\r\nseq_printf(s, "\tDCMD = %08x (%s%s%s%s%s%s%sburst=%d width=%d len=%d)\n",\r\ndcmd,\r\nPXA_DCMD_STR(INCSRCADDR), PXA_DCMD_STR(INCTRGADDR),\r\nPXA_DCMD_STR(FLOWSRC), PXA_DCMD_STR(FLOWTRG),\r\nPXA_DCMD_STR(STARTIRQEN), PXA_DCMD_STR(ENDIRQEN),\r\nPXA_DCMD_STR(ENDIAN), burst, width, dcmd & PXA_DCMD_LENGTH);\r\nseq_printf(s, "\tDSADR = %08x\n", _phy_readl_relaxed(phy, DSADR));\r\nseq_printf(s, "\tDTADR = %08x\n", _phy_readl_relaxed(phy, DTADR));\r\nseq_printf(s, "\tDDADR = %08x\n", _phy_readl_relaxed(phy, DDADR));\r\nreturn 0;\r\n}\r\nstatic int dbg_show_state(struct seq_file *s, void *p)\r\n{\r\nstruct pxad_device *pdev = s->private;\r\nseq_puts(s, "DMA engine status\n");\r\nseq_printf(s, "\tChannel number: %d\n", pdev->nr_chans);\r\nreturn 0;\r\n}\r\nstatic struct dentry *pxad_dbg_alloc_chan(struct pxad_device *pdev,\r\nint ch, struct dentry *chandir)\r\n{\r\nchar chan_name[11];\r\nstruct dentry *chan, *chan_state = NULL, *chan_descr = NULL;\r\nstruct dentry *chan_reqs = NULL;\r\nvoid *dt;\r\nscnprintf(chan_name, sizeof(chan_name), "%d", ch);\r\nchan = debugfs_create_dir(chan_name, chandir);\r\ndt = (void *)&pdev->phys[ch];\r\nif (chan)\r\nchan_state = debugfs_create_file("state", 0400, chan, dt,\r\n&dbg_fops_chan_state);\r\nif (chan_state)\r\nchan_descr = debugfs_create_file("descriptors", 0400, chan, dt,\r\n&dbg_fops_descriptors);\r\nif (chan_descr)\r\nchan_reqs = debugfs_create_file("requesters", 0400, chan, dt,\r\n&dbg_fops_requester_chan);\r\nif (!chan_reqs)\r\ngoto err_state;\r\nreturn chan;\r\nerr_state:\r\ndebugfs_remove_recursive(chan);\r\nreturn NULL;\r\n}\r\nstatic void pxad_init_debugfs(struct pxad_device *pdev)\r\n{\r\nint i;\r\nstruct dentry *chandir;\r\npdev->dbgfs_root = debugfs_create_dir(dev_name(pdev->slave.dev), NULL);\r\nif (IS_ERR(pdev->dbgfs_root) || !pdev->dbgfs_root)\r\ngoto err_root;\r\npdev->dbgfs_state = debugfs_create_file("state", 0400, pdev->dbgfs_root,\r\npdev, &dbg_fops_state);\r\nif (!pdev->dbgfs_state)\r\ngoto err_state;\r\npdev->dbgfs_chan =\r\nkmalloc_array(pdev->nr_chans, sizeof(*pdev->dbgfs_state),\r\nGFP_KERNEL);\r\nif (!pdev->dbgfs_chan)\r\ngoto err_alloc;\r\nchandir = debugfs_create_dir("channels", pdev->dbgfs_root);\r\nif (!chandir)\r\ngoto err_chandir;\r\nfor (i = 0; i < pdev->nr_chans; i++) {\r\npdev->dbgfs_chan[i] = pxad_dbg_alloc_chan(pdev, i, chandir);\r\nif (!pdev->dbgfs_chan[i])\r\ngoto err_chans;\r\n}\r\nreturn;\r\nerr_chans:\r\nerr_chandir:\r\nkfree(pdev->dbgfs_chan);\r\nerr_alloc:\r\nerr_state:\r\ndebugfs_remove_recursive(pdev->dbgfs_root);\r\nerr_root:\r\npr_err("pxad: debugfs is not available\n");\r\n}\r\nstatic void pxad_cleanup_debugfs(struct pxad_device *pdev)\r\n{\r\ndebugfs_remove_recursive(pdev->dbgfs_root);\r\n}\r\nstatic inline void pxad_init_debugfs(struct pxad_device *pdev) {}\r\nstatic inline void pxad_cleanup_debugfs(struct pxad_device *pdev) {}\r\nstatic struct pxad_phy *lookup_phy(struct pxad_chan *pchan)\r\n{\r\nint prio, i;\r\nstruct pxad_device *pdev = to_pxad_dev(pchan->vc.chan.device);\r\nstruct pxad_phy *phy, *found = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pdev->phy_lock, flags);\r\nfor (prio = pchan->prio; prio >= PXAD_PRIO_HIGHEST; prio--) {\r\nfor (i = 0; i < pdev->nr_chans; i++) {\r\nif (prio != (i & 0xf) >> 2)\r\ncontinue;\r\nif ((i < 32) && (legacy_reserved & BIT(i)))\r\ncontinue;\r\nphy = &pdev->phys[i];\r\nif (!phy->vchan) {\r\nphy->vchan = pchan;\r\nfound = phy;\r\nif (i < 32)\r\nlegacy_unavailable |= BIT(i);\r\ngoto out_unlock;\r\n}\r\n}\r\n}\r\nout_unlock:\r\nspin_unlock_irqrestore(&pdev->phy_lock, flags);\r\ndev_dbg(&pchan->vc.chan.dev->device,\r\n"%s(): phy=%p(%d)\n", __func__, found,\r\nfound ? found->idx : -1);\r\nreturn found;\r\n}\r\nstatic void pxad_free_phy(struct pxad_chan *chan)\r\n{\r\nstruct pxad_device *pdev = to_pxad_dev(chan->vc.chan.device);\r\nunsigned long flags;\r\nu32 reg;\r\nint i;\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): freeing\n", __func__);\r\nif (!chan->phy)\r\nreturn;\r\nif (chan->drcmr <= DRCMR_CHLNUM) {\r\nreg = pxad_drcmr(chan->drcmr);\r\nwritel_relaxed(0, chan->phy->base + reg);\r\n}\r\nspin_lock_irqsave(&pdev->phy_lock, flags);\r\nfor (i = 0; i < 32; i++)\r\nif (chan->phy == &pdev->phys[i])\r\nlegacy_unavailable &= ~BIT(i);\r\nchan->phy->vchan = NULL;\r\nchan->phy = NULL;\r\nspin_unlock_irqrestore(&pdev->phy_lock, flags);\r\n}\r\nstatic bool is_chan_running(struct pxad_chan *chan)\r\n{\r\nu32 dcsr;\r\nstruct pxad_phy *phy = chan->phy;\r\nif (!phy)\r\nreturn false;\r\ndcsr = phy_readl_relaxed(phy, DCSR);\r\nreturn dcsr & PXA_DCSR_RUN;\r\n}\r\nstatic bool is_running_chan_misaligned(struct pxad_chan *chan)\r\n{\r\nu32 dalgn;\r\nBUG_ON(!chan->phy);\r\ndalgn = phy_readl_relaxed(chan->phy, DALGN);\r\nreturn dalgn & (BIT(chan->phy->idx));\r\n}\r\nstatic void phy_enable(struct pxad_phy *phy, bool misaligned)\r\n{\r\nu32 reg, dalgn;\r\nif (!phy->vchan)\r\nreturn;\r\ndev_dbg(&phy->vchan->vc.chan.dev->device,\r\n"%s(); phy=%p(%d) misaligned=%d\n", __func__,\r\nphy, phy->idx, misaligned);\r\nif (phy->vchan->drcmr <= DRCMR_CHLNUM) {\r\nreg = pxad_drcmr(phy->vchan->drcmr);\r\nwritel_relaxed(DRCMR_MAPVLD | phy->idx, phy->base + reg);\r\n}\r\ndalgn = phy_readl_relaxed(phy, DALGN);\r\nif (misaligned)\r\ndalgn |= BIT(phy->idx);\r\nelse\r\ndalgn &= ~BIT(phy->idx);\r\nphy_writel_relaxed(phy, dalgn, DALGN);\r\nphy_writel(phy, PXA_DCSR_STOPIRQEN | PXA_DCSR_ENDINTR |\r\nPXA_DCSR_BUSERR | PXA_DCSR_RUN, DCSR);\r\n}\r\nstatic void phy_disable(struct pxad_phy *phy)\r\n{\r\nu32 dcsr;\r\nif (!phy)\r\nreturn;\r\ndcsr = phy_readl_relaxed(phy, DCSR);\r\ndev_dbg(&phy->vchan->vc.chan.dev->device,\r\n"%s(): phy=%p(%d)\n", __func__, phy, phy->idx);\r\nphy_writel(phy, dcsr & ~PXA_DCSR_RUN & ~PXA_DCSR_STOPIRQEN, DCSR);\r\n}\r\nstatic void pxad_launch_chan(struct pxad_chan *chan,\r\nstruct pxad_desc_sw *desc)\r\n{\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): desc=%p\n", __func__, desc);\r\nif (!chan->phy) {\r\nchan->phy = lookup_phy(chan);\r\nif (!chan->phy) {\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): no free dma channel\n", __func__);\r\nreturn;\r\n}\r\n}\r\nphy_writel(chan->phy, desc->first, DDADR);\r\nphy_enable(chan->phy, chan->misaligned);\r\n}\r\nstatic void set_updater_desc(struct pxad_desc_sw *sw_desc,\r\nunsigned long flags)\r\n{\r\nstruct pxad_desc_hw *updater =\r\nsw_desc->hw_desc[sw_desc->nb_desc - 1];\r\ndma_addr_t dma = sw_desc->hw_desc[sw_desc->nb_desc - 2]->ddadr;\r\nupdater->ddadr = DDADR_STOP;\r\nupdater->dsadr = dma;\r\nupdater->dtadr = dma + 8;\r\nupdater->dcmd = PXA_DCMD_WIDTH4 | PXA_DCMD_BURST32 |\r\n(PXA_DCMD_LENGTH & sizeof(u32));\r\nif (flags & DMA_PREP_INTERRUPT)\r\nupdater->dcmd |= PXA_DCMD_ENDIRQEN;\r\n}\r\nstatic bool is_desc_completed(struct virt_dma_desc *vd)\r\n{\r\nstruct pxad_desc_sw *sw_desc = to_pxad_sw_desc(vd);\r\nstruct pxad_desc_hw *updater =\r\nsw_desc->hw_desc[sw_desc->nb_desc - 1];\r\nreturn updater->dtadr != (updater->dsadr + 8);\r\n}\r\nstatic void pxad_desc_chain(struct virt_dma_desc *vd1,\r\nstruct virt_dma_desc *vd2)\r\n{\r\nstruct pxad_desc_sw *desc1 = to_pxad_sw_desc(vd1);\r\nstruct pxad_desc_sw *desc2 = to_pxad_sw_desc(vd2);\r\ndma_addr_t dma_to_chain;\r\ndma_to_chain = desc2->first;\r\ndesc1->hw_desc[desc1->nb_desc - 1]->ddadr = dma_to_chain;\r\n}\r\nstatic bool pxad_try_hotchain(struct virt_dma_chan *vc,\r\nstruct virt_dma_desc *vd)\r\n{\r\nstruct virt_dma_desc *vd_last_issued = NULL;\r\nstruct pxad_chan *chan = to_pxad_chan(&vc->chan);\r\nif (is_chan_running(chan)) {\r\nBUG_ON(list_empty(&vc->desc_issued));\r\nif (!is_running_chan_misaligned(chan) &&\r\nto_pxad_sw_desc(vd)->misaligned)\r\nreturn false;\r\nvd_last_issued = list_entry(vc->desc_issued.prev,\r\nstruct virt_dma_desc, node);\r\npxad_desc_chain(vd_last_issued, vd);\r\nif (is_chan_running(chan) || is_desc_completed(vd_last_issued))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic unsigned int clear_chan_irq(struct pxad_phy *phy)\r\n{\r\nu32 dcsr;\r\nu32 dint = readl(phy->base + DINT);\r\nif (!(dint & BIT(phy->idx)))\r\nreturn PXA_DCSR_RUN;\r\ndcsr = phy_readl_relaxed(phy, DCSR);\r\nphy_writel(phy, dcsr, DCSR);\r\nif ((dcsr & PXA_DCSR_BUSERR) && (phy->vchan))\r\ndev_warn(&phy->vchan->vc.chan.dev->device,\r\n"%s(chan=%p): PXA_DCSR_BUSERR\n",\r\n__func__, &phy->vchan);\r\nreturn dcsr & ~PXA_DCSR_RUN;\r\n}\r\nstatic irqreturn_t pxad_chan_handler(int irq, void *dev_id)\r\n{\r\nstruct pxad_phy *phy = dev_id;\r\nstruct pxad_chan *chan = phy->vchan;\r\nstruct virt_dma_desc *vd, *tmp;\r\nunsigned int dcsr;\r\nunsigned long flags;\r\nBUG_ON(!chan);\r\ndcsr = clear_chan_irq(phy);\r\nif (dcsr & PXA_DCSR_RUN)\r\nreturn IRQ_NONE;\r\nspin_lock_irqsave(&chan->vc.lock, flags);\r\nlist_for_each_entry_safe(vd, tmp, &chan->vc.desc_issued, node) {\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): checking txd %p[%x]: completed=%d\n",\r\n__func__, vd, vd->tx.cookie, is_desc_completed(vd));\r\nif (is_desc_completed(vd)) {\r\nlist_del(&vd->node);\r\nvchan_cookie_complete(vd);\r\n} else {\r\nbreak;\r\n}\r\n}\r\nif (dcsr & PXA_DCSR_STOPSTATE) {\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): channel stopped, submitted_empty=%d issued_empty=%d",\r\n__func__,\r\nlist_empty(&chan->vc.desc_submitted),\r\nlist_empty(&chan->vc.desc_issued));\r\nphy_writel_relaxed(phy, dcsr & ~PXA_DCSR_STOPIRQEN, DCSR);\r\nif (list_empty(&chan->vc.desc_issued)) {\r\nchan->misaligned =\r\n!list_empty(&chan->vc.desc_submitted);\r\n} else {\r\nvd = list_first_entry(&chan->vc.desc_issued,\r\nstruct virt_dma_desc, node);\r\npxad_launch_chan(chan, to_pxad_sw_desc(vd));\r\n}\r\n}\r\nspin_unlock_irqrestore(&chan->vc.lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t pxad_int_handler(int irq, void *dev_id)\r\n{\r\nstruct pxad_device *pdev = dev_id;\r\nstruct pxad_phy *phy;\r\nu32 dint = readl(pdev->base + DINT);\r\nint i, ret = IRQ_NONE;\r\nwhile (dint) {\r\ni = __ffs(dint);\r\ndint &= (dint - 1);\r\nphy = &pdev->phys[i];\r\nif ((i < 32) && (legacy_reserved & BIT(i)))\r\ncontinue;\r\nif (pxad_chan_handler(irq, phy) == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nreturn ret;\r\n}\r\nstatic int pxad_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nstruct pxad_device *pdev = to_pxad_dev(chan->vc.chan.device);\r\nif (chan->desc_pool)\r\nreturn 1;\r\nchan->desc_pool = dma_pool_create(dma_chan_name(dchan),\r\npdev->slave.dev,\r\nsizeof(struct pxad_desc_hw),\r\n__alignof__(struct pxad_desc_hw),\r\n0);\r\nif (!chan->desc_pool) {\r\ndev_err(&chan->vc.chan.dev->device,\r\n"%s(): unable to allocate descriptor pool\n",\r\n__func__);\r\nreturn -ENOMEM;\r\n}\r\nreturn 1;\r\n}\r\nstatic void pxad_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nvchan_free_chan_resources(&chan->vc);\r\ndma_pool_destroy(chan->desc_pool);\r\nchan->desc_pool = NULL;\r\n}\r\nstatic void pxad_free_desc(struct virt_dma_desc *vd)\r\n{\r\nint i;\r\ndma_addr_t dma;\r\nstruct pxad_desc_sw *sw_desc = to_pxad_sw_desc(vd);\r\nBUG_ON(sw_desc->nb_desc == 0);\r\nfor (i = sw_desc->nb_desc - 1; i >= 0; i--) {\r\nif (i > 0)\r\ndma = sw_desc->hw_desc[i - 1]->ddadr;\r\nelse\r\ndma = sw_desc->first;\r\ndma_pool_free(sw_desc->desc_pool,\r\nsw_desc->hw_desc[i], dma);\r\n}\r\nsw_desc->nb_desc = 0;\r\nkfree(sw_desc);\r\n}\r\nstatic struct pxad_desc_sw *\r\npxad_alloc_desc(struct pxad_chan *chan, unsigned int nb_hw_desc)\r\n{\r\nstruct pxad_desc_sw *sw_desc;\r\ndma_addr_t dma;\r\nint i;\r\nsw_desc = kzalloc(sizeof(*sw_desc) +\r\nnb_hw_desc * sizeof(struct pxad_desc_hw *),\r\nGFP_NOWAIT);\r\nif (!sw_desc)\r\nreturn NULL;\r\nsw_desc->desc_pool = chan->desc_pool;\r\nfor (i = 0; i < nb_hw_desc; i++) {\r\nsw_desc->hw_desc[i] = dma_pool_alloc(sw_desc->desc_pool,\r\nGFP_NOWAIT, &dma);\r\nif (!sw_desc->hw_desc[i]) {\r\ndev_err(&chan->vc.chan.dev->device,\r\n"%s(): Couldn't allocate the %dth hw_desc from dma_pool %p\n",\r\n__func__, i, sw_desc->desc_pool);\r\ngoto err;\r\n}\r\nif (i == 0)\r\nsw_desc->first = dma;\r\nelse\r\nsw_desc->hw_desc[i - 1]->ddadr = dma;\r\nsw_desc->nb_desc++;\r\n}\r\nreturn sw_desc;\r\nerr:\r\npxad_free_desc(&sw_desc->vd);\r\nreturn NULL;\r\n}\r\nstatic dma_cookie_t pxad_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct virt_dma_chan *vc = to_virt_chan(tx->chan);\r\nstruct pxad_chan *chan = to_pxad_chan(&vc->chan);\r\nstruct virt_dma_desc *vd_chained = NULL,\r\n*vd = container_of(tx, struct virt_dma_desc, tx);\r\ndma_cookie_t cookie;\r\nunsigned long flags;\r\nset_updater_desc(to_pxad_sw_desc(vd), tx->flags);\r\nspin_lock_irqsave(&vc->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\nif (list_empty(&vc->desc_submitted) && pxad_try_hotchain(vc, vd)) {\r\nlist_move_tail(&vd->node, &vc->desc_issued);\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): txd %p[%x]: submitted (hot linked)\n",\r\n__func__, vd, cookie);\r\ngoto out;\r\n}\r\nif (!list_empty(&vc->desc_submitted)) {\r\nvd_chained = list_entry(vc->desc_submitted.prev,\r\nstruct virt_dma_desc, node);\r\nif (chan->misaligned || !to_pxad_sw_desc(vd)->misaligned)\r\npxad_desc_chain(vd_chained, vd);\r\nelse\r\nvd_chained = NULL;\r\n}\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): txd %p[%x]: submitted (%s linked)\n",\r\n__func__, vd, cookie, vd_chained ? "cold" : "not");\r\nlist_move_tail(&vd->node, &vc->desc_submitted);\r\nchan->misaligned |= to_pxad_sw_desc(vd)->misaligned;\r\nout:\r\nspin_unlock_irqrestore(&vc->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic void pxad_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nstruct virt_dma_desc *vd_first;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->vc.lock, flags);\r\nif (list_empty(&chan->vc.desc_submitted))\r\ngoto out;\r\nvd_first = list_first_entry(&chan->vc.desc_submitted,\r\nstruct virt_dma_desc, node);\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): txd %p[%x]", __func__, vd_first, vd_first->tx.cookie);\r\nvchan_issue_pending(&chan->vc);\r\nif (!pxad_try_hotchain(&chan->vc, vd_first))\r\npxad_launch_chan(chan, to_pxad_sw_desc(vd_first));\r\nout:\r\nspin_unlock_irqrestore(&chan->vc.lock, flags);\r\n}\r\nstatic inline struct dma_async_tx_descriptor *\r\npxad_tx_prep(struct virt_dma_chan *vc, struct virt_dma_desc *vd,\r\nunsigned long tx_flags)\r\n{\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct pxad_chan *chan = container_of(vc, struct pxad_chan, vc);\r\nINIT_LIST_HEAD(&vd->node);\r\ntx = vchan_tx_prep(vc, vd, tx_flags);\r\ntx->tx_submit = pxad_tx_submit;\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): vc=%p txd=%p[%x] flags=0x%lx\n", __func__,\r\nvc, vd, vd->tx.cookie,\r\ntx_flags);\r\nreturn tx;\r\n}\r\nstatic void pxad_get_config(struct pxad_chan *chan,\r\nenum dma_transfer_direction dir,\r\nu32 *dcmd, u32 *dev_src, u32 *dev_dst)\r\n{\r\nu32 maxburst = 0, dev_addr = 0;\r\nenum dma_slave_buswidth width = DMA_SLAVE_BUSWIDTH_UNDEFINED;\r\n*dcmd = 0;\r\nif (dir == DMA_DEV_TO_MEM) {\r\nmaxburst = chan->cfg.src_maxburst;\r\nwidth = chan->cfg.src_addr_width;\r\ndev_addr = chan->cfg.src_addr;\r\n*dev_src = dev_addr;\r\n*dcmd |= PXA_DCMD_INCTRGADDR;\r\nif (chan->drcmr <= DRCMR_CHLNUM)\r\n*dcmd |= PXA_DCMD_FLOWSRC;\r\n}\r\nif (dir == DMA_MEM_TO_DEV) {\r\nmaxburst = chan->cfg.dst_maxburst;\r\nwidth = chan->cfg.dst_addr_width;\r\ndev_addr = chan->cfg.dst_addr;\r\n*dev_dst = dev_addr;\r\n*dcmd |= PXA_DCMD_INCSRCADDR;\r\nif (chan->drcmr <= DRCMR_CHLNUM)\r\n*dcmd |= PXA_DCMD_FLOWTRG;\r\n}\r\nif (dir == DMA_MEM_TO_MEM)\r\n*dcmd |= PXA_DCMD_BURST32 | PXA_DCMD_INCTRGADDR |\r\nPXA_DCMD_INCSRCADDR;\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): dev_addr=0x%x maxburst=%d width=%d dir=%d\n",\r\n__func__, dev_addr, maxburst, width, dir);\r\nif (width == DMA_SLAVE_BUSWIDTH_1_BYTE)\r\n*dcmd |= PXA_DCMD_WIDTH1;\r\nelse if (width == DMA_SLAVE_BUSWIDTH_2_BYTES)\r\n*dcmd |= PXA_DCMD_WIDTH2;\r\nelse if (width == DMA_SLAVE_BUSWIDTH_4_BYTES)\r\n*dcmd |= PXA_DCMD_WIDTH4;\r\nif (maxburst == 8)\r\n*dcmd |= PXA_DCMD_BURST8;\r\nelse if (maxburst == 16)\r\n*dcmd |= PXA_DCMD_BURST16;\r\nelse if (maxburst == 32)\r\n*dcmd |= PXA_DCMD_BURST32;\r\nif (chan->cfg.slave_id)\r\nchan->drcmr = chan->cfg.slave_id;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\npxad_prep_memcpy(struct dma_chan *dchan,\r\ndma_addr_t dma_dst, dma_addr_t dma_src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nstruct pxad_desc_sw *sw_desc;\r\nstruct pxad_desc_hw *hw_desc;\r\nu32 dcmd;\r\nunsigned int i, nb_desc = 0;\r\nsize_t copy;\r\nif (!dchan || !len)\r\nreturn NULL;\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): dma_dst=0x%lx dma_src=0x%lx len=%zu flags=%lx\n",\r\n__func__, (unsigned long)dma_dst, (unsigned long)dma_src,\r\nlen, flags);\r\npxad_get_config(chan, DMA_MEM_TO_MEM, &dcmd, NULL, NULL);\r\nnb_desc = DIV_ROUND_UP(len, PDMA_MAX_DESC_BYTES);\r\nsw_desc = pxad_alloc_desc(chan, nb_desc + 1);\r\nif (!sw_desc)\r\nreturn NULL;\r\nsw_desc->len = len;\r\nif (!IS_ALIGNED(dma_src, 1 << PDMA_ALIGNMENT) ||\r\n!IS_ALIGNED(dma_dst, 1 << PDMA_ALIGNMENT))\r\nsw_desc->misaligned = true;\r\ni = 0;\r\ndo {\r\nhw_desc = sw_desc->hw_desc[i++];\r\ncopy = min_t(size_t, len, PDMA_MAX_DESC_BYTES);\r\nhw_desc->dcmd = dcmd | (PXA_DCMD_LENGTH & copy);\r\nhw_desc->dsadr = dma_src;\r\nhw_desc->dtadr = dma_dst;\r\nlen -= copy;\r\ndma_src += copy;\r\ndma_dst += copy;\r\n} while (len);\r\nset_updater_desc(sw_desc, flags);\r\nreturn pxad_tx_prep(&chan->vc, &sw_desc->vd, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\npxad_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction dir,\r\nunsigned long flags, void *context)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nstruct pxad_desc_sw *sw_desc;\r\nsize_t len, avail;\r\nstruct scatterlist *sg;\r\ndma_addr_t dma;\r\nu32 dcmd, dsadr = 0, dtadr = 0;\r\nunsigned int nb_desc = 0, i, j = 0;\r\nif ((sgl == NULL) || (sg_len == 0))\r\nreturn NULL;\r\npxad_get_config(chan, dir, &dcmd, &dsadr, &dtadr);\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): dir=%d flags=%lx\n", __func__, dir, flags);\r\nfor_each_sg(sgl, sg, sg_len, i)\r\nnb_desc += DIV_ROUND_UP(sg_dma_len(sg), PDMA_MAX_DESC_BYTES);\r\nsw_desc = pxad_alloc_desc(chan, nb_desc + 1);\r\nif (!sw_desc)\r\nreturn NULL;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndma = sg_dma_address(sg);\r\navail = sg_dma_len(sg);\r\nsw_desc->len += avail;\r\ndo {\r\nlen = min_t(size_t, avail, PDMA_MAX_DESC_BYTES);\r\nif (dma & 0x7)\r\nsw_desc->misaligned = true;\r\nsw_desc->hw_desc[j]->dcmd =\r\ndcmd | (PXA_DCMD_LENGTH & len);\r\nsw_desc->hw_desc[j]->dsadr = dsadr ? dsadr : dma;\r\nsw_desc->hw_desc[j++]->dtadr = dtadr ? dtadr : dma;\r\ndma += len;\r\navail -= len;\r\n} while (avail);\r\n}\r\nset_updater_desc(sw_desc, flags);\r\nreturn pxad_tx_prep(&chan->vc, &sw_desc->vd, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\npxad_prep_dma_cyclic(struct dma_chan *dchan,\r\ndma_addr_t buf_addr, size_t len, size_t period_len,\r\nenum dma_transfer_direction dir, unsigned long flags)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nstruct pxad_desc_sw *sw_desc;\r\nstruct pxad_desc_hw **phw_desc;\r\ndma_addr_t dma;\r\nu32 dcmd, dsadr = 0, dtadr = 0;\r\nunsigned int nb_desc = 0;\r\nif (!dchan || !len || !period_len)\r\nreturn NULL;\r\nif ((dir != DMA_DEV_TO_MEM) && (dir != DMA_MEM_TO_DEV)) {\r\ndev_err(&chan->vc.chan.dev->device,\r\n"Unsupported direction for cyclic DMA\n");\r\nreturn NULL;\r\n}\r\nif (len % period_len != 0 || period_len > PDMA_MAX_DESC_BYTES ||\r\n!IS_ALIGNED(period_len, 1 << PDMA_ALIGNMENT))\r\nreturn NULL;\r\npxad_get_config(chan, dir, &dcmd, &dsadr, &dtadr);\r\ndcmd |= PXA_DCMD_ENDIRQEN | (PXA_DCMD_LENGTH | period_len);\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): buf_addr=0x%lx len=%zu period=%zu dir=%d flags=%lx\n",\r\n__func__, (unsigned long)buf_addr, len, period_len, dir, flags);\r\nnb_desc = DIV_ROUND_UP(period_len, PDMA_MAX_DESC_BYTES);\r\nnb_desc *= DIV_ROUND_UP(len, period_len);\r\nsw_desc = pxad_alloc_desc(chan, nb_desc + 1);\r\nif (!sw_desc)\r\nreturn NULL;\r\nsw_desc->cyclic = true;\r\nsw_desc->len = len;\r\nphw_desc = sw_desc->hw_desc;\r\ndma = buf_addr;\r\ndo {\r\nphw_desc[0]->dsadr = dsadr ? dsadr : dma;\r\nphw_desc[0]->dtadr = dtadr ? dtadr : dma;\r\nphw_desc[0]->dcmd = dcmd;\r\nphw_desc++;\r\ndma += period_len;\r\nlen -= period_len;\r\n} while (len);\r\nset_updater_desc(sw_desc, flags);\r\nreturn pxad_tx_prep(&chan->vc, &sw_desc->vd, flags);\r\n}\r\nstatic int pxad_config(struct dma_chan *dchan,\r\nstruct dma_slave_config *cfg)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nif (!dchan)\r\nreturn -EINVAL;\r\nchan->cfg = *cfg;\r\nreturn 0;\r\n}\r\nstatic int pxad_terminate_all(struct dma_chan *dchan)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nstruct pxad_device *pdev = to_pxad_dev(chan->vc.chan.device);\r\nstruct virt_dma_desc *vd = NULL;\r\nunsigned long flags;\r\nstruct pxad_phy *phy;\r\nLIST_HEAD(head);\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): vchan %p: terminate all\n", __func__, &chan->vc);\r\nspin_lock_irqsave(&chan->vc.lock, flags);\r\nvchan_get_all_descriptors(&chan->vc, &head);\r\nlist_for_each_entry(vd, &head, node) {\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): cancelling txd %p[%x] (completed=%d)", __func__,\r\nvd, vd->tx.cookie, is_desc_completed(vd));\r\n}\r\nphy = chan->phy;\r\nif (phy) {\r\nphy_disable(chan->phy);\r\npxad_free_phy(chan);\r\nchan->phy = NULL;\r\nspin_lock(&pdev->phy_lock);\r\nphy->vchan = NULL;\r\nspin_unlock(&pdev->phy_lock);\r\n}\r\nspin_unlock_irqrestore(&chan->vc.lock, flags);\r\nvchan_dma_desc_free_list(&chan->vc, &head);\r\nreturn 0;\r\n}\r\nstatic unsigned int pxad_residue(struct pxad_chan *chan,\r\ndma_cookie_t cookie)\r\n{\r\nstruct virt_dma_desc *vd = NULL;\r\nstruct pxad_desc_sw *sw_desc = NULL;\r\nstruct pxad_desc_hw *hw_desc = NULL;\r\nu32 curr, start, len, end, residue = 0;\r\nunsigned long flags;\r\nbool passed = false;\r\nint i;\r\nif (!chan->phy)\r\nreturn 0;\r\nspin_lock_irqsave(&chan->vc.lock, flags);\r\nvd = vchan_find_desc(&chan->vc, cookie);\r\nif (!vd)\r\ngoto out;\r\nsw_desc = to_pxad_sw_desc(vd);\r\nif (sw_desc->hw_desc[0]->dcmd & PXA_DCMD_INCSRCADDR)\r\ncurr = phy_readl_relaxed(chan->phy, DSADR);\r\nelse\r\ncurr = phy_readl_relaxed(chan->phy, DTADR);\r\nrmb();\r\nif (is_desc_completed(vd))\r\ngoto out;\r\nfor (i = 0; i < sw_desc->nb_desc - 1; i++) {\r\nhw_desc = sw_desc->hw_desc[i];\r\nif (sw_desc->hw_desc[0]->dcmd & PXA_DCMD_INCSRCADDR)\r\nstart = hw_desc->dsadr;\r\nelse\r\nstart = hw_desc->dtadr;\r\nlen = hw_desc->dcmd & PXA_DCMD_LENGTH;\r\nend = start + len;\r\nif (passed) {\r\nresidue += len;\r\n} else if (curr >= start && curr <= end) {\r\nresidue += end - curr;\r\npassed = true;\r\n}\r\n}\r\nif (!passed)\r\nresidue = sw_desc->len;\r\nout:\r\nspin_unlock_irqrestore(&chan->vc.lock, flags);\r\ndev_dbg(&chan->vc.chan.dev->device,\r\n"%s(): txd %p[%x] sw_desc=%p: %d\n",\r\n__func__, vd, cookie, sw_desc, residue);\r\nreturn residue;\r\n}\r\nstatic enum dma_status pxad_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct pxad_chan *chan = to_pxad_chan(dchan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(dchan, cookie, txstate);\r\nif (likely(txstate && (ret != DMA_ERROR)))\r\ndma_set_residue(txstate, pxad_residue(chan, cookie));\r\nreturn ret;\r\n}\r\nstatic void pxad_free_channels(struct dma_device *dmadev)\r\n{\r\nstruct pxad_chan *c, *cn;\r\nlist_for_each_entry_safe(c, cn, &dmadev->channels,\r\nvc.chan.device_node) {\r\nlist_del(&c->vc.chan.device_node);\r\ntasklet_kill(&c->vc.task);\r\n}\r\n}\r\nstatic int pxad_remove(struct platform_device *op)\r\n{\r\nstruct pxad_device *pdev = platform_get_drvdata(op);\r\npxad_cleanup_debugfs(pdev);\r\npxad_free_channels(&pdev->slave);\r\ndma_async_device_unregister(&pdev->slave);\r\nreturn 0;\r\n}\r\nstatic int pxad_init_phys(struct platform_device *op,\r\nstruct pxad_device *pdev,\r\nunsigned int nb_phy_chans)\r\n{\r\nint irq0, irq, nr_irq = 0, i, ret;\r\nstruct pxad_phy *phy;\r\nirq0 = platform_get_irq(op, 0);\r\nif (irq0 < 0)\r\nreturn irq0;\r\npdev->phys = devm_kcalloc(&op->dev, nb_phy_chans,\r\nsizeof(pdev->phys[0]), GFP_KERNEL);\r\nif (!pdev->phys)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < nb_phy_chans; i++)\r\nif (platform_get_irq(op, i) > 0)\r\nnr_irq++;\r\nfor (i = 0; i < nb_phy_chans; i++) {\r\nphy = &pdev->phys[i];\r\nphy->base = pdev->base;\r\nphy->idx = i;\r\nirq = platform_get_irq(op, i);\r\nif ((nr_irq > 1) && (irq > 0))\r\nret = devm_request_irq(&op->dev, irq,\r\npxad_chan_handler,\r\nIRQF_SHARED, "pxa-dma", phy);\r\nif ((nr_irq == 1) && (i == 0))\r\nret = devm_request_irq(&op->dev, irq0,\r\npxad_int_handler,\r\nIRQF_SHARED, "pxa-dma", pdev);\r\nif (ret) {\r\ndev_err(pdev->slave.dev,\r\n"%s(): can't request irq %d:%d\n", __func__,\r\nirq, ret);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic struct dma_chan *pxad_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct pxad_device *d = ofdma->of_dma_data;\r\nstruct dma_chan *chan;\r\nchan = dma_get_any_slave_channel(&d->slave);\r\nif (!chan)\r\nreturn NULL;\r\nto_pxad_chan(chan)->drcmr = dma_spec->args[0];\r\nto_pxad_chan(chan)->prio = dma_spec->args[1];\r\nreturn chan;\r\n}\r\nstatic int pxad_init_dmadev(struct platform_device *op,\r\nstruct pxad_device *pdev,\r\nunsigned int nr_phy_chans)\r\n{\r\nint ret;\r\nunsigned int i;\r\nstruct pxad_chan *c;\r\npdev->nr_chans = nr_phy_chans;\r\nINIT_LIST_HEAD(&pdev->slave.channels);\r\npdev->slave.device_alloc_chan_resources = pxad_alloc_chan_resources;\r\npdev->slave.device_free_chan_resources = pxad_free_chan_resources;\r\npdev->slave.device_tx_status = pxad_tx_status;\r\npdev->slave.device_issue_pending = pxad_issue_pending;\r\npdev->slave.device_config = pxad_config;\r\npdev->slave.device_terminate_all = pxad_terminate_all;\r\nif (op->dev.coherent_dma_mask)\r\ndma_set_mask(&op->dev, op->dev.coherent_dma_mask);\r\nelse\r\ndma_set_mask(&op->dev, DMA_BIT_MASK(32));\r\nret = pxad_init_phys(op, pdev, nr_phy_chans);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < nr_phy_chans; i++) {\r\nc = devm_kzalloc(&op->dev, sizeof(*c), GFP_KERNEL);\r\nif (!c)\r\nreturn -ENOMEM;\r\nc->vc.desc_free = pxad_free_desc;\r\nvchan_init(&c->vc, &pdev->slave);\r\n}\r\nreturn dma_async_device_register(&pdev->slave);\r\n}\r\nstatic int pxad_probe(struct platform_device *op)\r\n{\r\nstruct pxad_device *pdev;\r\nconst struct of_device_id *of_id;\r\nstruct mmp_dma_platdata *pdata = dev_get_platdata(&op->dev);\r\nstruct resource *iores;\r\nint ret, dma_channels = 0;\r\nconst enum dma_slave_buswidth widths =\r\nDMA_SLAVE_BUSWIDTH_1_BYTE | DMA_SLAVE_BUSWIDTH_2_BYTES |\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\npdev = devm_kzalloc(&op->dev, sizeof(*pdev), GFP_KERNEL);\r\nif (!pdev)\r\nreturn -ENOMEM;\r\nspin_lock_init(&pdev->phy_lock);\r\niores = platform_get_resource(op, IORESOURCE_MEM, 0);\r\npdev->base = devm_ioremap_resource(&op->dev, iores);\r\nif (IS_ERR(pdev->base))\r\nreturn PTR_ERR(pdev->base);\r\nof_id = of_match_device(pxad_dt_ids, &op->dev);\r\nif (of_id)\r\nof_property_read_u32(op->dev.of_node, "#dma-channels",\r\n&dma_channels);\r\nelse if (pdata && pdata->dma_channels)\r\ndma_channels = pdata->dma_channels;\r\nelse\r\ndma_channels = 32;\r\ndma_cap_set(DMA_SLAVE, pdev->slave.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, pdev->slave.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, pdev->slave.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, pdev->slave.cap_mask);\r\npdev->slave.device_prep_dma_memcpy = pxad_prep_memcpy;\r\npdev->slave.device_prep_slave_sg = pxad_prep_slave_sg;\r\npdev->slave.device_prep_dma_cyclic = pxad_prep_dma_cyclic;\r\npdev->slave.copy_align = PDMA_ALIGNMENT;\r\npdev->slave.src_addr_widths = widths;\r\npdev->slave.dst_addr_widths = widths;\r\npdev->slave.directions = BIT(DMA_MEM_TO_DEV) | BIT(DMA_DEV_TO_MEM);\r\npdev->slave.residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\r\npdev->slave.dev = &op->dev;\r\nret = pxad_init_dmadev(op, pdev, dma_channels);\r\nif (ret) {\r\ndev_err(pdev->slave.dev, "unable to register\n");\r\nreturn ret;\r\n}\r\nif (op->dev.of_node) {\r\nret = of_dma_controller_register(op->dev.of_node,\r\npxad_dma_xlate, pdev);\r\nif (ret < 0) {\r\ndev_err(pdev->slave.dev,\r\n"of_dma_controller_register failed\n");\r\nreturn ret;\r\n}\r\n}\r\nplatform_set_drvdata(op, pdev);\r\npxad_init_debugfs(pdev);\r\ndev_info(pdev->slave.dev, "initialized %d channels\n", dma_channels);\r\nreturn 0;\r\n}\r\nbool pxad_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nstruct pxad_chan *c = to_pxad_chan(chan);\r\nstruct pxad_param *p = param;\r\nif (chan->device->dev->driver != &pxad_driver.driver)\r\nreturn false;\r\nc->drcmr = p->drcmr;\r\nc->prio = p->prio;\r\nreturn true;\r\n}\r\nint pxad_toggle_reserved_channel(int legacy_channel)\r\n{\r\nif (legacy_unavailable & (BIT(legacy_channel)))\r\nreturn -EBUSY;\r\nlegacy_reserved ^= BIT(legacy_channel);\r\nreturn 0;\r\n}
