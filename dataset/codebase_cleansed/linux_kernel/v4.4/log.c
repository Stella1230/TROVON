struct ubifs_bud *ubifs_search_bud(struct ubifs_info *c, int lnum)\r\n{\r\nstruct rb_node *p;\r\nstruct ubifs_bud *bud;\r\nspin_lock(&c->buds_lock);\r\np = c->buds.rb_node;\r\nwhile (p) {\r\nbud = rb_entry(p, struct ubifs_bud, rb);\r\nif (lnum < bud->lnum)\r\np = p->rb_left;\r\nelse if (lnum > bud->lnum)\r\np = p->rb_right;\r\nelse {\r\nspin_unlock(&c->buds_lock);\r\nreturn bud;\r\n}\r\n}\r\nspin_unlock(&c->buds_lock);\r\nreturn NULL;\r\n}\r\nstruct ubifs_wbuf *ubifs_get_wbuf(struct ubifs_info *c, int lnum)\r\n{\r\nstruct rb_node *p;\r\nstruct ubifs_bud *bud;\r\nint jhead;\r\nif (!c->jheads)\r\nreturn NULL;\r\nspin_lock(&c->buds_lock);\r\np = c->buds.rb_node;\r\nwhile (p) {\r\nbud = rb_entry(p, struct ubifs_bud, rb);\r\nif (lnum < bud->lnum)\r\np = p->rb_left;\r\nelse if (lnum > bud->lnum)\r\np = p->rb_right;\r\nelse {\r\njhead = bud->jhead;\r\nspin_unlock(&c->buds_lock);\r\nreturn &c->jheads[jhead].wbuf;\r\n}\r\n}\r\nspin_unlock(&c->buds_lock);\r\nreturn NULL;\r\n}\r\nstatic inline long long empty_log_bytes(const struct ubifs_info *c)\r\n{\r\nlong long h, t;\r\nh = (long long)c->lhead_lnum * c->leb_size + c->lhead_offs;\r\nt = (long long)c->ltail_lnum * c->leb_size;\r\nif (h > t)\r\nreturn c->log_bytes - h + t;\r\nelse if (h != t)\r\nreturn t - h;\r\nelse if (c->lhead_lnum != c->ltail_lnum)\r\nreturn 0;\r\nelse\r\nreturn c->log_bytes;\r\n}\r\nvoid ubifs_add_bud(struct ubifs_info *c, struct ubifs_bud *bud)\r\n{\r\nstruct rb_node **p, *parent = NULL;\r\nstruct ubifs_bud *b;\r\nstruct ubifs_jhead *jhead;\r\nspin_lock(&c->buds_lock);\r\np = &c->buds.rb_node;\r\nwhile (*p) {\r\nparent = *p;\r\nb = rb_entry(parent, struct ubifs_bud, rb);\r\nubifs_assert(bud->lnum != b->lnum);\r\nif (bud->lnum < b->lnum)\r\np = &(*p)->rb_left;\r\nelse\r\np = &(*p)->rb_right;\r\n}\r\nrb_link_node(&bud->rb, parent, p);\r\nrb_insert_color(&bud->rb, &c->buds);\r\nif (c->jheads) {\r\njhead = &c->jheads[bud->jhead];\r\nlist_add_tail(&bud->list, &jhead->buds_list);\r\n} else\r\nubifs_assert(c->replaying && c->ro_mount);\r\nc->bud_bytes += c->leb_size - bud->start;\r\ndbg_log("LEB %d:%d, jhead %s, bud_bytes %lld", bud->lnum,\r\nbud->start, dbg_jhead(bud->jhead), c->bud_bytes);\r\nspin_unlock(&c->buds_lock);\r\n}\r\nint ubifs_add_bud_to_log(struct ubifs_info *c, int jhead, int lnum, int offs)\r\n{\r\nint err;\r\nstruct ubifs_bud *bud;\r\nstruct ubifs_ref_node *ref;\r\nbud = kmalloc(sizeof(struct ubifs_bud), GFP_NOFS);\r\nif (!bud)\r\nreturn -ENOMEM;\r\nref = kzalloc(c->ref_node_alsz, GFP_NOFS);\r\nif (!ref) {\r\nkfree(bud);\r\nreturn -ENOMEM;\r\n}\r\nmutex_lock(&c->log_mutex);\r\nubifs_assert(!c->ro_media && !c->ro_mount);\r\nif (c->ro_error) {\r\nerr = -EROFS;\r\ngoto out_unlock;\r\n}\r\nif (empty_log_bytes(c) - c->ref_node_alsz < c->min_log_bytes) {\r\ndbg_log("not enough log space - %lld, required %d",\r\nempty_log_bytes(c), c->min_log_bytes);\r\nubifs_commit_required(c);\r\nerr = -EAGAIN;\r\ngoto out_unlock;\r\n}\r\nif (c->bud_bytes + c->leb_size - offs > c->max_bud_bytes) {\r\ndbg_log("bud bytes %lld (%lld max), require commit",\r\nc->bud_bytes, c->max_bud_bytes);\r\nubifs_commit_required(c);\r\nerr = -EAGAIN;\r\ngoto out_unlock;\r\n}\r\nif (c->bud_bytes >= c->bg_bud_bytes &&\r\nc->cmt_state == COMMIT_RESTING) {\r\ndbg_log("bud bytes %lld (%lld max), initiate BG commit",\r\nc->bud_bytes, c->max_bud_bytes);\r\nubifs_request_bg_commit(c);\r\n}\r\nbud->lnum = lnum;\r\nbud->start = offs;\r\nbud->jhead = jhead;\r\nref->ch.node_type = UBIFS_REF_NODE;\r\nref->lnum = cpu_to_le32(bud->lnum);\r\nref->offs = cpu_to_le32(bud->start);\r\nref->jhead = cpu_to_le32(jhead);\r\nif (c->lhead_offs > c->leb_size - c->ref_node_alsz) {\r\nc->lhead_lnum = ubifs_next_log_lnum(c, c->lhead_lnum);\r\nubifs_assert(c->lhead_lnum != c->ltail_lnum);\r\nc->lhead_offs = 0;\r\n}\r\nif (c->lhead_offs == 0) {\r\nerr = ubifs_leb_unmap(c, c->lhead_lnum);\r\nif (err)\r\ngoto out_unlock;\r\n}\r\nif (bud->start == 0) {\r\nerr = ubifs_leb_map(c, bud->lnum);\r\nif (err)\r\ngoto out_unlock;\r\n}\r\ndbg_log("write ref LEB %d:%d",\r\nc->lhead_lnum, c->lhead_offs);\r\nerr = ubifs_write_node(c, ref, UBIFS_REF_NODE_SZ, c->lhead_lnum,\r\nc->lhead_offs);\r\nif (err)\r\ngoto out_unlock;\r\nc->lhead_offs += c->ref_node_alsz;\r\nubifs_add_bud(c, bud);\r\nmutex_unlock(&c->log_mutex);\r\nkfree(ref);\r\nreturn 0;\r\nout_unlock:\r\nmutex_unlock(&c->log_mutex);\r\nkfree(ref);\r\nkfree(bud);\r\nreturn err;\r\n}\r\nstatic void remove_buds(struct ubifs_info *c)\r\n{\r\nstruct rb_node *p;\r\nubifs_assert(list_empty(&c->old_buds));\r\nc->cmt_bud_bytes = 0;\r\nspin_lock(&c->buds_lock);\r\np = rb_first(&c->buds);\r\nwhile (p) {\r\nstruct rb_node *p1 = p;\r\nstruct ubifs_bud *bud;\r\nstruct ubifs_wbuf *wbuf;\r\np = rb_next(p);\r\nbud = rb_entry(p1, struct ubifs_bud, rb);\r\nwbuf = &c->jheads[bud->jhead].wbuf;\r\nif (wbuf->lnum == bud->lnum) {\r\nc->cmt_bud_bytes += wbuf->offs - bud->start;\r\ndbg_log("preserve %d:%d, jhead %s, bud bytes %d, cmt_bud_bytes %lld",\r\nbud->lnum, bud->start, dbg_jhead(bud->jhead),\r\nwbuf->offs - bud->start, c->cmt_bud_bytes);\r\nbud->start = wbuf->offs;\r\n} else {\r\nc->cmt_bud_bytes += c->leb_size - bud->start;\r\ndbg_log("remove %d:%d, jhead %s, bud bytes %d, cmt_bud_bytes %lld",\r\nbud->lnum, bud->start, dbg_jhead(bud->jhead),\r\nc->leb_size - bud->start, c->cmt_bud_bytes);\r\nrb_erase(p1, &c->buds);\r\nlist_move(&bud->list, &c->old_buds);\r\n}\r\n}\r\nspin_unlock(&c->buds_lock);\r\n}\r\nint ubifs_log_start_commit(struct ubifs_info *c, int *ltail_lnum)\r\n{\r\nvoid *buf;\r\nstruct ubifs_cs_node *cs;\r\nstruct ubifs_ref_node *ref;\r\nint err, i, max_len, len;\r\nerr = dbg_check_bud_bytes(c);\r\nif (err)\r\nreturn err;\r\nmax_len = UBIFS_CS_NODE_SZ + c->jhead_cnt * UBIFS_REF_NODE_SZ;\r\nmax_len = ALIGN(max_len, c->min_io_size);\r\nbuf = cs = kmalloc(max_len, GFP_NOFS);\r\nif (!buf)\r\nreturn -ENOMEM;\r\ncs->ch.node_type = UBIFS_CS_NODE;\r\ncs->cmt_no = cpu_to_le64(c->cmt_no);\r\nubifs_prepare_node(c, cs, UBIFS_CS_NODE_SZ, 0);\r\nlen = UBIFS_CS_NODE_SZ;\r\nfor (i = 0; i < c->jhead_cnt; i++) {\r\nint lnum = c->jheads[i].wbuf.lnum;\r\nint offs = c->jheads[i].wbuf.offs;\r\nif (lnum == -1 || offs == c->leb_size)\r\ncontinue;\r\ndbg_log("add ref to LEB %d:%d for jhead %s",\r\nlnum, offs, dbg_jhead(i));\r\nref = buf + len;\r\nref->ch.node_type = UBIFS_REF_NODE;\r\nref->lnum = cpu_to_le32(lnum);\r\nref->offs = cpu_to_le32(offs);\r\nref->jhead = cpu_to_le32(i);\r\nubifs_prepare_node(c, ref, UBIFS_REF_NODE_SZ, 0);\r\nlen += UBIFS_REF_NODE_SZ;\r\n}\r\nubifs_pad(c, buf + len, ALIGN(len, c->min_io_size) - len);\r\nif (c->lhead_offs) {\r\nc->lhead_lnum = ubifs_next_log_lnum(c, c->lhead_lnum);\r\nubifs_assert(c->lhead_lnum != c->ltail_lnum);\r\nc->lhead_offs = 0;\r\n}\r\nerr = ubifs_leb_unmap(c, c->lhead_lnum);\r\nif (err)\r\ngoto out;\r\nlen = ALIGN(len, c->min_io_size);\r\ndbg_log("writing commit start at LEB %d:0, len %d", c->lhead_lnum, len);\r\nerr = ubifs_leb_write(c, c->lhead_lnum, cs, 0, len);\r\nif (err)\r\ngoto out;\r\n*ltail_lnum = c->lhead_lnum;\r\nc->lhead_offs += len;\r\nif (c->lhead_offs == c->leb_size) {\r\nc->lhead_lnum = ubifs_next_log_lnum(c, c->lhead_lnum);\r\nc->lhead_offs = 0;\r\n}\r\nremove_buds(c);\r\nc->min_log_bytes = 0;\r\nout:\r\nkfree(buf);\r\nreturn err;\r\n}\r\nint ubifs_log_end_commit(struct ubifs_info *c, int ltail_lnum)\r\n{\r\nint err;\r\nmutex_lock(&c->log_mutex);\r\ndbg_log("old tail was LEB %d:0, new tail is LEB %d:0",\r\nc->ltail_lnum, ltail_lnum);\r\nc->ltail_lnum = ltail_lnum;\r\nc->min_log_bytes = c->leb_size;\r\nspin_lock(&c->buds_lock);\r\nc->bud_bytes -= c->cmt_bud_bytes;\r\nspin_unlock(&c->buds_lock);\r\nerr = dbg_check_bud_bytes(c);\r\nif (err)\r\ngoto out;\r\nerr = ubifs_write_master(c);\r\nout:\r\nmutex_unlock(&c->log_mutex);\r\nreturn err;\r\n}\r\nint ubifs_log_post_commit(struct ubifs_info *c, int old_ltail_lnum)\r\n{\r\nint lnum, err = 0;\r\nwhile (!list_empty(&c->old_buds)) {\r\nstruct ubifs_bud *bud;\r\nbud = list_entry(c->old_buds.next, struct ubifs_bud, list);\r\nerr = ubifs_return_leb(c, bud->lnum);\r\nif (err)\r\nreturn err;\r\nlist_del(&bud->list);\r\nkfree(bud);\r\n}\r\nmutex_lock(&c->log_mutex);\r\nfor (lnum = old_ltail_lnum; lnum != c->ltail_lnum;\r\nlnum = ubifs_next_log_lnum(c, lnum)) {\r\ndbg_log("unmap log LEB %d", lnum);\r\nerr = ubifs_leb_unmap(c, lnum);\r\nif (err)\r\ngoto out;\r\n}\r\nout:\r\nmutex_unlock(&c->log_mutex);\r\nreturn err;\r\n}\r\nstatic int done_already(struct rb_root *done_tree, int lnum)\r\n{\r\nstruct rb_node **p = &done_tree->rb_node, *parent = NULL;\r\nstruct done_ref *dr;\r\nwhile (*p) {\r\nparent = *p;\r\ndr = rb_entry(parent, struct done_ref, rb);\r\nif (lnum < dr->lnum)\r\np = &(*p)->rb_left;\r\nelse if (lnum > dr->lnum)\r\np = &(*p)->rb_right;\r\nelse\r\nreturn 1;\r\n}\r\ndr = kzalloc(sizeof(struct done_ref), GFP_NOFS);\r\nif (!dr)\r\nreturn -ENOMEM;\r\ndr->lnum = lnum;\r\nrb_link_node(&dr->rb, parent, p);\r\nrb_insert_color(&dr->rb, done_tree);\r\nreturn 0;\r\n}\r\nstatic void destroy_done_tree(struct rb_root *done_tree)\r\n{\r\nstruct done_ref *dr, *n;\r\nrbtree_postorder_for_each_entry_safe(dr, n, done_tree, rb)\r\nkfree(dr);\r\n}\r\nstatic int add_node(struct ubifs_info *c, void *buf, int *lnum, int *offs,\r\nvoid *node)\r\n{\r\nstruct ubifs_ch *ch = node;\r\nint len = le32_to_cpu(ch->len), remains = c->leb_size - *offs;\r\nif (len > remains) {\r\nint sz = ALIGN(*offs, c->min_io_size), err;\r\nubifs_pad(c, buf + *offs, sz - *offs);\r\nerr = ubifs_leb_change(c, *lnum, buf, sz);\r\nif (err)\r\nreturn err;\r\n*lnum = ubifs_next_log_lnum(c, *lnum);\r\n*offs = 0;\r\n}\r\nmemcpy(buf + *offs, node, len);\r\n*offs += ALIGN(len, 8);\r\nreturn 0;\r\n}\r\nint ubifs_consolidate_log(struct ubifs_info *c)\r\n{\r\nstruct ubifs_scan_leb *sleb;\r\nstruct ubifs_scan_node *snod;\r\nstruct rb_root done_tree = RB_ROOT;\r\nint lnum, err, first = 1, write_lnum, offs = 0;\r\nvoid *buf;\r\ndbg_rcvry("log tail LEB %d, log head LEB %d", c->ltail_lnum,\r\nc->lhead_lnum);\r\nbuf = vmalloc(c->leb_size);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nlnum = c->ltail_lnum;\r\nwrite_lnum = lnum;\r\nwhile (1) {\r\nsleb = ubifs_scan(c, lnum, 0, c->sbuf, 0);\r\nif (IS_ERR(sleb)) {\r\nerr = PTR_ERR(sleb);\r\ngoto out_free;\r\n}\r\nlist_for_each_entry(snod, &sleb->nodes, list) {\r\nswitch (snod->type) {\r\ncase UBIFS_REF_NODE: {\r\nstruct ubifs_ref_node *ref = snod->node;\r\nint ref_lnum = le32_to_cpu(ref->lnum);\r\nerr = done_already(&done_tree, ref_lnum);\r\nif (err < 0)\r\ngoto out_scan;\r\nif (err != 1) {\r\nerr = add_node(c, buf, &write_lnum,\r\n&offs, snod->node);\r\nif (err)\r\ngoto out_scan;\r\n}\r\nbreak;\r\n}\r\ncase UBIFS_CS_NODE:\r\nif (!first)\r\nbreak;\r\nerr = add_node(c, buf, &write_lnum, &offs,\r\nsnod->node);\r\nif (err)\r\ngoto out_scan;\r\nfirst = 0;\r\nbreak;\r\n}\r\n}\r\nubifs_scan_destroy(sleb);\r\nif (lnum == c->lhead_lnum)\r\nbreak;\r\nlnum = ubifs_next_log_lnum(c, lnum);\r\n}\r\nif (offs) {\r\nint sz = ALIGN(offs, c->min_io_size);\r\nubifs_pad(c, buf + offs, sz - offs);\r\nerr = ubifs_leb_change(c, write_lnum, buf, sz);\r\nif (err)\r\ngoto out_free;\r\noffs = ALIGN(offs, c->min_io_size);\r\n}\r\ndestroy_done_tree(&done_tree);\r\nvfree(buf);\r\nif (write_lnum == c->lhead_lnum) {\r\nubifs_err(c, "log is too full");\r\nreturn -EINVAL;\r\n}\r\nlnum = write_lnum;\r\ndo {\r\nlnum = ubifs_next_log_lnum(c, lnum);\r\nerr = ubifs_leb_unmap(c, lnum);\r\nif (err)\r\nreturn err;\r\n} while (lnum != c->lhead_lnum);\r\nc->lhead_lnum = write_lnum;\r\nc->lhead_offs = offs;\r\ndbg_rcvry("new log head at %d:%d", c->lhead_lnum, c->lhead_offs);\r\nreturn 0;\r\nout_scan:\r\nubifs_scan_destroy(sleb);\r\nout_free:\r\ndestroy_done_tree(&done_tree);\r\nvfree(buf);\r\nreturn err;\r\n}\r\nstatic int dbg_check_bud_bytes(struct ubifs_info *c)\r\n{\r\nint i, err = 0;\r\nstruct ubifs_bud *bud;\r\nlong long bud_bytes = 0;\r\nif (!dbg_is_chk_gen(c))\r\nreturn 0;\r\nspin_lock(&c->buds_lock);\r\nfor (i = 0; i < c->jhead_cnt; i++)\r\nlist_for_each_entry(bud, &c->jheads[i].buds_list, list)\r\nbud_bytes += c->leb_size - bud->start;\r\nif (c->bud_bytes != bud_bytes) {\r\nubifs_err(c, "bad bud_bytes %lld, calculated %lld",\r\nc->bud_bytes, bud_bytes);\r\nerr = -EINVAL;\r\n}\r\nspin_unlock(&c->buds_lock);\r\nreturn err;\r\n}
