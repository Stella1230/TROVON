static size_t copy_page_to_iter_iovec(struct page *page, size_t offset, size_t bytes,\r\nstruct iov_iter *i)\r\n{\r\nsize_t skip, copy, left, wanted;\r\nconst struct iovec *iov;\r\nchar __user *buf;\r\nvoid *kaddr, *from;\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\nwanted = bytes;\r\niov = i->iov;\r\nskip = i->iov_offset;\r\nbuf = iov->iov_base + skip;\r\ncopy = min(bytes, iov->iov_len - skip);\r\nif (!fault_in_pages_writeable(buf, copy)) {\r\nkaddr = kmap_atomic(page);\r\nfrom = kaddr + offset;\r\nleft = __copy_to_user_inatomic(buf, from, copy);\r\ncopy -= left;\r\nskip += copy;\r\nfrom += copy;\r\nbytes -= copy;\r\nwhile (unlikely(!left && bytes)) {\r\niov++;\r\nbuf = iov->iov_base;\r\ncopy = min(bytes, iov->iov_len);\r\nleft = __copy_to_user_inatomic(buf, from, copy);\r\ncopy -= left;\r\nskip = copy;\r\nfrom += copy;\r\nbytes -= copy;\r\n}\r\nif (likely(!bytes)) {\r\nkunmap_atomic(kaddr);\r\ngoto done;\r\n}\r\noffset = from - kaddr;\r\nbuf += copy;\r\nkunmap_atomic(kaddr);\r\ncopy = min(bytes, iov->iov_len - skip);\r\n}\r\nkaddr = kmap(page);\r\nfrom = kaddr + offset;\r\nleft = __copy_to_user(buf, from, copy);\r\ncopy -= left;\r\nskip += copy;\r\nfrom += copy;\r\nbytes -= copy;\r\nwhile (unlikely(!left && bytes)) {\r\niov++;\r\nbuf = iov->iov_base;\r\ncopy = min(bytes, iov->iov_len);\r\nleft = __copy_to_user(buf, from, copy);\r\ncopy -= left;\r\nskip = copy;\r\nfrom += copy;\r\nbytes -= copy;\r\n}\r\nkunmap(page);\r\ndone:\r\nif (skip == iov->iov_len) {\r\niov++;\r\nskip = 0;\r\n}\r\ni->count -= wanted - bytes;\r\ni->nr_segs -= iov - i->iov;\r\ni->iov = iov;\r\ni->iov_offset = skip;\r\nreturn wanted - bytes;\r\n}\r\nstatic size_t copy_page_from_iter_iovec(struct page *page, size_t offset, size_t bytes,\r\nstruct iov_iter *i)\r\n{\r\nsize_t skip, copy, left, wanted;\r\nconst struct iovec *iov;\r\nchar __user *buf;\r\nvoid *kaddr, *to;\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\nwanted = bytes;\r\niov = i->iov;\r\nskip = i->iov_offset;\r\nbuf = iov->iov_base + skip;\r\ncopy = min(bytes, iov->iov_len - skip);\r\nif (!fault_in_pages_readable(buf, copy)) {\r\nkaddr = kmap_atomic(page);\r\nto = kaddr + offset;\r\nleft = __copy_from_user_inatomic(to, buf, copy);\r\ncopy -= left;\r\nskip += copy;\r\nto += copy;\r\nbytes -= copy;\r\nwhile (unlikely(!left && bytes)) {\r\niov++;\r\nbuf = iov->iov_base;\r\ncopy = min(bytes, iov->iov_len);\r\nleft = __copy_from_user_inatomic(to, buf, copy);\r\ncopy -= left;\r\nskip = copy;\r\nto += copy;\r\nbytes -= copy;\r\n}\r\nif (likely(!bytes)) {\r\nkunmap_atomic(kaddr);\r\ngoto done;\r\n}\r\noffset = to - kaddr;\r\nbuf += copy;\r\nkunmap_atomic(kaddr);\r\ncopy = min(bytes, iov->iov_len - skip);\r\n}\r\nkaddr = kmap(page);\r\nto = kaddr + offset;\r\nleft = __copy_from_user(to, buf, copy);\r\ncopy -= left;\r\nskip += copy;\r\nto += copy;\r\nbytes -= copy;\r\nwhile (unlikely(!left && bytes)) {\r\niov++;\r\nbuf = iov->iov_base;\r\ncopy = min(bytes, iov->iov_len);\r\nleft = __copy_from_user(to, buf, copy);\r\ncopy -= left;\r\nskip = copy;\r\nto += copy;\r\nbytes -= copy;\r\n}\r\nkunmap(page);\r\ndone:\r\nif (skip == iov->iov_len) {\r\niov++;\r\nskip = 0;\r\n}\r\ni->count -= wanted - bytes;\r\ni->nr_segs -= iov - i->iov;\r\ni->iov = iov;\r\ni->iov_offset = skip;\r\nreturn wanted - bytes;\r\n}\r\nint iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)\r\n{\r\nif (!(i->type & (ITER_BVEC|ITER_KVEC))) {\r\nchar __user *buf = i->iov->iov_base + i->iov_offset;\r\nbytes = min(bytes, i->iov->iov_len - i->iov_offset);\r\nreturn fault_in_pages_readable(buf, bytes);\r\n}\r\nreturn 0;\r\n}\r\nint iov_iter_fault_in_multipages_readable(struct iov_iter *i, size_t bytes)\r\n{\r\nsize_t skip = i->iov_offset;\r\nconst struct iovec *iov;\r\nint err;\r\nstruct iovec v;\r\nif (!(i->type & (ITER_BVEC|ITER_KVEC))) {\r\niterate_iovec(i, bytes, v, iov, skip, ({\r\nerr = fault_in_multipages_readable(v.iov_base,\r\nv.iov_len);\r\nif (unlikely(err))\r\nreturn err;\r\n0;}))\r\n}\r\nreturn 0;\r\n}\r\nvoid iov_iter_init(struct iov_iter *i, int direction,\r\nconst struct iovec *iov, unsigned long nr_segs,\r\nsize_t count)\r\n{\r\nif (segment_eq(get_fs(), KERNEL_DS)) {\r\ndirection |= ITER_KVEC;\r\ni->type = direction;\r\ni->kvec = (struct kvec *)iov;\r\n} else {\r\ni->type = direction;\r\ni->iov = iov;\r\n}\r\ni->nr_segs = nr_segs;\r\ni->iov_offset = 0;\r\ni->count = count;\r\n}\r\nstatic void memcpy_from_page(char *to, struct page *page, size_t offset, size_t len)\r\n{\r\nchar *from = kmap_atomic(page);\r\nmemcpy(to, from + offset, len);\r\nkunmap_atomic(from);\r\n}\r\nstatic void memcpy_to_page(struct page *page, size_t offset, char *from, size_t len)\r\n{\r\nchar *to = kmap_atomic(page);\r\nmemcpy(to + offset, from, len);\r\nkunmap_atomic(to);\r\n}\r\nstatic void memzero_page(struct page *page, size_t offset, size_t len)\r\n{\r\nchar *addr = kmap_atomic(page);\r\nmemset(addr + offset, 0, len);\r\nkunmap_atomic(addr);\r\n}\r\nsize_t copy_to_iter(void *addr, size_t bytes, struct iov_iter *i)\r\n{\r\nchar *from = addr;\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\niterate_and_advance(i, bytes, v,\r\n__copy_to_user(v.iov_base, (from += v.iov_len) - v.iov_len,\r\nv.iov_len),\r\nmemcpy_to_page(v.bv_page, v.bv_offset,\r\n(from += v.bv_len) - v.bv_len, v.bv_len),\r\nmemcpy(v.iov_base, (from += v.iov_len) - v.iov_len, v.iov_len)\r\n)\r\nreturn bytes;\r\n}\r\nsize_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)\r\n{\r\nchar *to = addr;\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\niterate_and_advance(i, bytes, v,\r\n__copy_from_user((to += v.iov_len) - v.iov_len, v.iov_base,\r\nv.iov_len),\r\nmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\r\nv.bv_offset, v.bv_len),\r\nmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\r\n)\r\nreturn bytes;\r\n}\r\nsize_t copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i)\r\n{\r\nchar *to = addr;\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\niterate_and_advance(i, bytes, v,\r\n__copy_from_user_nocache((to += v.iov_len) - v.iov_len,\r\nv.iov_base, v.iov_len),\r\nmemcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,\r\nv.bv_offset, v.bv_len),\r\nmemcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\r\n)\r\nreturn bytes;\r\n}\r\nsize_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,\r\nstruct iov_iter *i)\r\n{\r\nif (i->type & (ITER_BVEC|ITER_KVEC)) {\r\nvoid *kaddr = kmap_atomic(page);\r\nsize_t wanted = copy_to_iter(kaddr + offset, bytes, i);\r\nkunmap_atomic(kaddr);\r\nreturn wanted;\r\n} else\r\nreturn copy_page_to_iter_iovec(page, offset, bytes, i);\r\n}\r\nsize_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,\r\nstruct iov_iter *i)\r\n{\r\nif (i->type & (ITER_BVEC|ITER_KVEC)) {\r\nvoid *kaddr = kmap_atomic(page);\r\nsize_t wanted = copy_from_iter(kaddr + offset, bytes, i);\r\nkunmap_atomic(kaddr);\r\nreturn wanted;\r\n} else\r\nreturn copy_page_from_iter_iovec(page, offset, bytes, i);\r\n}\r\nsize_t iov_iter_zero(size_t bytes, struct iov_iter *i)\r\n{\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\niterate_and_advance(i, bytes, v,\r\n__clear_user(v.iov_base, v.iov_len),\r\nmemzero_page(v.bv_page, v.bv_offset, v.bv_len),\r\nmemset(v.iov_base, 0, v.iov_len)\r\n)\r\nreturn bytes;\r\n}\r\nsize_t iov_iter_copy_from_user_atomic(struct page *page,\r\nstruct iov_iter *i, unsigned long offset, size_t bytes)\r\n{\r\nchar *kaddr = kmap_atomic(page), *p = kaddr + offset;\r\niterate_all_kinds(i, bytes, v,\r\n__copy_from_user_inatomic((p += v.iov_len) - v.iov_len,\r\nv.iov_base, v.iov_len),\r\nmemcpy_from_page((p += v.bv_len) - v.bv_len, v.bv_page,\r\nv.bv_offset, v.bv_len),\r\nmemcpy((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)\r\n)\r\nkunmap_atomic(kaddr);\r\nreturn bytes;\r\n}\r\nvoid iov_iter_advance(struct iov_iter *i, size_t size)\r\n{\r\niterate_and_advance(i, size, v, 0, 0, 0)\r\n}\r\nsize_t iov_iter_single_seg_count(const struct iov_iter *i)\r\n{\r\nif (i->nr_segs == 1)\r\nreturn i->count;\r\nelse if (i->type & ITER_BVEC)\r\nreturn min(i->count, i->bvec->bv_len - i->iov_offset);\r\nelse\r\nreturn min(i->count, i->iov->iov_len - i->iov_offset);\r\n}\r\nvoid iov_iter_kvec(struct iov_iter *i, int direction,\r\nconst struct kvec *kvec, unsigned long nr_segs,\r\nsize_t count)\r\n{\r\nBUG_ON(!(direction & ITER_KVEC));\r\ni->type = direction;\r\ni->kvec = kvec;\r\ni->nr_segs = nr_segs;\r\ni->iov_offset = 0;\r\ni->count = count;\r\n}\r\nvoid iov_iter_bvec(struct iov_iter *i, int direction,\r\nconst struct bio_vec *bvec, unsigned long nr_segs,\r\nsize_t count)\r\n{\r\nBUG_ON(!(direction & ITER_BVEC));\r\ni->type = direction;\r\ni->bvec = bvec;\r\ni->nr_segs = nr_segs;\r\ni->iov_offset = 0;\r\ni->count = count;\r\n}\r\nunsigned long iov_iter_alignment(const struct iov_iter *i)\r\n{\r\nunsigned long res = 0;\r\nsize_t size = i->count;\r\nif (!size)\r\nreturn 0;\r\niterate_all_kinds(i, size, v,\r\n(res |= (unsigned long)v.iov_base | v.iov_len, 0),\r\nres |= v.bv_offset | v.bv_len,\r\nres |= (unsigned long)v.iov_base | v.iov_len\r\n)\r\nreturn res;\r\n}\r\nssize_t iov_iter_get_pages(struct iov_iter *i,\r\nstruct page **pages, size_t maxsize, unsigned maxpages,\r\nsize_t *start)\r\n{\r\nif (maxsize > i->count)\r\nmaxsize = i->count;\r\nif (!maxsize)\r\nreturn 0;\r\niterate_all_kinds(i, maxsize, v, ({\r\nunsigned long addr = (unsigned long)v.iov_base;\r\nsize_t len = v.iov_len + (*start = addr & (PAGE_SIZE - 1));\r\nint n;\r\nint res;\r\nif (len > maxpages * PAGE_SIZE)\r\nlen = maxpages * PAGE_SIZE;\r\naddr &= ~(PAGE_SIZE - 1);\r\nn = DIV_ROUND_UP(len, PAGE_SIZE);\r\nres = get_user_pages_fast(addr, n, (i->type & WRITE) != WRITE, pages);\r\nif (unlikely(res < 0))\r\nreturn res;\r\nreturn (res == n ? len : res * PAGE_SIZE) - *start;\r\n0;}),({\r\n*start = v.bv_offset;\r\nget_page(*pages = v.bv_page);\r\nreturn v.bv_len;\r\n}),({\r\nreturn -EFAULT;\r\n})\r\n)\r\nreturn 0;\r\n}\r\nstatic struct page **get_pages_array(size_t n)\r\n{\r\nstruct page **p = kmalloc(n * sizeof(struct page *), GFP_KERNEL);\r\nif (!p)\r\np = vmalloc(n * sizeof(struct page *));\r\nreturn p;\r\n}\r\nssize_t iov_iter_get_pages_alloc(struct iov_iter *i,\r\nstruct page ***pages, size_t maxsize,\r\nsize_t *start)\r\n{\r\nstruct page **p;\r\nif (maxsize > i->count)\r\nmaxsize = i->count;\r\nif (!maxsize)\r\nreturn 0;\r\niterate_all_kinds(i, maxsize, v, ({\r\nunsigned long addr = (unsigned long)v.iov_base;\r\nsize_t len = v.iov_len + (*start = addr & (PAGE_SIZE - 1));\r\nint n;\r\nint res;\r\naddr &= ~(PAGE_SIZE - 1);\r\nn = DIV_ROUND_UP(len, PAGE_SIZE);\r\np = get_pages_array(n);\r\nif (!p)\r\nreturn -ENOMEM;\r\nres = get_user_pages_fast(addr, n, (i->type & WRITE) != WRITE, p);\r\nif (unlikely(res < 0)) {\r\nkvfree(p);\r\nreturn res;\r\n}\r\n*pages = p;\r\nreturn (res == n ? len : res * PAGE_SIZE) - *start;\r\n0;}),({\r\n*start = v.bv_offset;\r\n*pages = p = get_pages_array(1);\r\nif (!p)\r\nreturn -ENOMEM;\r\nget_page(*p = v.bv_page);\r\nreturn v.bv_len;\r\n}),({\r\nreturn -EFAULT;\r\n})\r\n)\r\nreturn 0;\r\n}\r\nsize_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,\r\nstruct iov_iter *i)\r\n{\r\nchar *to = addr;\r\n__wsum sum, next;\r\nsize_t off = 0;\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\nsum = *csum;\r\niterate_and_advance(i, bytes, v, ({\r\nint err = 0;\r\nnext = csum_and_copy_from_user(v.iov_base,\r\n(to += v.iov_len) - v.iov_len,\r\nv.iov_len, 0, &err);\r\nif (!err) {\r\nsum = csum_block_add(sum, next, off);\r\noff += v.iov_len;\r\n}\r\nerr ? v.iov_len : 0;\r\n}), ({\r\nchar *p = kmap_atomic(v.bv_page);\r\nnext = csum_partial_copy_nocheck(p + v.bv_offset,\r\n(to += v.bv_len) - v.bv_len,\r\nv.bv_len, 0);\r\nkunmap_atomic(p);\r\nsum = csum_block_add(sum, next, off);\r\noff += v.bv_len;\r\n}),({\r\nnext = csum_partial_copy_nocheck(v.iov_base,\r\n(to += v.iov_len) - v.iov_len,\r\nv.iov_len, 0);\r\nsum = csum_block_add(sum, next, off);\r\noff += v.iov_len;\r\n})\r\n)\r\n*csum = sum;\r\nreturn bytes;\r\n}\r\nsize_t csum_and_copy_to_iter(void *addr, size_t bytes, __wsum *csum,\r\nstruct iov_iter *i)\r\n{\r\nchar *from = addr;\r\n__wsum sum, next;\r\nsize_t off = 0;\r\nif (unlikely(bytes > i->count))\r\nbytes = i->count;\r\nif (unlikely(!bytes))\r\nreturn 0;\r\nsum = *csum;\r\niterate_and_advance(i, bytes, v, ({\r\nint err = 0;\r\nnext = csum_and_copy_to_user((from += v.iov_len) - v.iov_len,\r\nv.iov_base,\r\nv.iov_len, 0, &err);\r\nif (!err) {\r\nsum = csum_block_add(sum, next, off);\r\noff += v.iov_len;\r\n}\r\nerr ? v.iov_len : 0;\r\n}), ({\r\nchar *p = kmap_atomic(v.bv_page);\r\nnext = csum_partial_copy_nocheck((from += v.bv_len) - v.bv_len,\r\np + v.bv_offset,\r\nv.bv_len, 0);\r\nkunmap_atomic(p);\r\nsum = csum_block_add(sum, next, off);\r\noff += v.bv_len;\r\n}),({\r\nnext = csum_partial_copy_nocheck((from += v.iov_len) - v.iov_len,\r\nv.iov_base,\r\nv.iov_len, 0);\r\nsum = csum_block_add(sum, next, off);\r\noff += v.iov_len;\r\n})\r\n)\r\n*csum = sum;\r\nreturn bytes;\r\n}\r\nint iov_iter_npages(const struct iov_iter *i, int maxpages)\r\n{\r\nsize_t size = i->count;\r\nint npages = 0;\r\nif (!size)\r\nreturn 0;\r\niterate_all_kinds(i, size, v, ({\r\nunsigned long p = (unsigned long)v.iov_base;\r\nnpages += DIV_ROUND_UP(p + v.iov_len, PAGE_SIZE)\r\n- p / PAGE_SIZE;\r\nif (npages >= maxpages)\r\nreturn maxpages;\r\n0;}),({\r\nnpages++;\r\nif (npages >= maxpages)\r\nreturn maxpages;\r\n}),({\r\nunsigned long p = (unsigned long)v.iov_base;\r\nnpages += DIV_ROUND_UP(p + v.iov_len, PAGE_SIZE)\r\n- p / PAGE_SIZE;\r\nif (npages >= maxpages)\r\nreturn maxpages;\r\n})\r\n)\r\nreturn npages;\r\n}\r\nconst void *dup_iter(struct iov_iter *new, struct iov_iter *old, gfp_t flags)\r\n{\r\n*new = *old;\r\nif (new->type & ITER_BVEC)\r\nreturn new->bvec = kmemdup(new->bvec,\r\nnew->nr_segs * sizeof(struct bio_vec),\r\nflags);\r\nelse\r\nreturn new->iov = kmemdup(new->iov,\r\nnew->nr_segs * sizeof(struct iovec),\r\nflags);\r\n}\r\nint import_iovec(int type, const struct iovec __user * uvector,\r\nunsigned nr_segs, unsigned fast_segs,\r\nstruct iovec **iov, struct iov_iter *i)\r\n{\r\nssize_t n;\r\nstruct iovec *p;\r\nn = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,\r\n*iov, &p);\r\nif (n < 0) {\r\nif (p != *iov)\r\nkfree(p);\r\n*iov = NULL;\r\nreturn n;\r\n}\r\niov_iter_init(i, type, p, nr_segs, n);\r\n*iov = p == *iov ? NULL : p;\r\nreturn 0;\r\n}\r\nint compat_import_iovec(int type, const struct compat_iovec __user * uvector,\r\nunsigned nr_segs, unsigned fast_segs,\r\nstruct iovec **iov, struct iov_iter *i)\r\n{\r\nssize_t n;\r\nstruct iovec *p;\r\nn = compat_rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,\r\n*iov, &p);\r\nif (n < 0) {\r\nif (p != *iov)\r\nkfree(p);\r\n*iov = NULL;\r\nreturn n;\r\n}\r\niov_iter_init(i, type, p, nr_segs, n);\r\n*iov = p == *iov ? NULL : p;\r\nreturn 0;\r\n}\r\nint import_single_range(int rw, void __user *buf, size_t len,\r\nstruct iovec *iov, struct iov_iter *i)\r\n{\r\nif (len > MAX_RW_COUNT)\r\nlen = MAX_RW_COUNT;\r\nif (unlikely(!access_ok(!rw, buf, len)))\r\nreturn -EFAULT;\r\niov->iov_base = buf;\r\niov->iov_len = len;\r\niov_iter_init(i, rw, iov, 1, len);\r\nreturn 0;\r\n}
