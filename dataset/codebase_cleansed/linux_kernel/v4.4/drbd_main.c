struct bio *bio_alloc_drbd(gfp_t gfp_mask)\r\n{\r\nstruct bio *bio;\r\nif (!drbd_md_io_bio_set)\r\nreturn bio_alloc(gfp_mask, 1);\r\nbio = bio_alloc_bioset(gfp_mask, 1, drbd_md_io_bio_set);\r\nif (!bio)\r\nreturn NULL;\r\nreturn bio;\r\n}\r\nint _get_ldev_if_state(struct drbd_device *device, enum drbd_disk_state mins)\r\n{\r\nint io_allowed;\r\natomic_inc(&device->local_cnt);\r\nio_allowed = (device->state.disk >= mins);\r\nif (!io_allowed) {\r\nif (atomic_dec_and_test(&device->local_cnt))\r\nwake_up(&device->misc_wait);\r\n}\r\nreturn io_allowed;\r\n}\r\nvoid tl_release(struct drbd_connection *connection, unsigned int barrier_nr,\r\nunsigned int set_size)\r\n{\r\nstruct drbd_request *r;\r\nstruct drbd_request *req = NULL;\r\nint expect_epoch = 0;\r\nint expect_size = 0;\r\nspin_lock_irq(&connection->resource->req_lock);\r\nlist_for_each_entry(r, &connection->transfer_log, tl_requests) {\r\nconst unsigned s = r->rq_state;\r\nif (!req) {\r\nif (!(s & RQ_WRITE))\r\ncontinue;\r\nif (!(s & RQ_NET_MASK))\r\ncontinue;\r\nif (s & RQ_NET_DONE)\r\ncontinue;\r\nreq = r;\r\nexpect_epoch = req->epoch;\r\nexpect_size ++;\r\n} else {\r\nif (r->epoch != expect_epoch)\r\nbreak;\r\nif (!(s & RQ_WRITE))\r\ncontinue;\r\nexpect_size++;\r\n}\r\n}\r\nif (req == NULL) {\r\ndrbd_err(connection, "BAD! BarrierAck #%u received, but no epoch in tl!?\n",\r\nbarrier_nr);\r\ngoto bail;\r\n}\r\nif (expect_epoch != barrier_nr) {\r\ndrbd_err(connection, "BAD! BarrierAck #%u received, expected #%u!\n",\r\nbarrier_nr, expect_epoch);\r\ngoto bail;\r\n}\r\nif (expect_size != set_size) {\r\ndrbd_err(connection, "BAD! BarrierAck #%u received with n_writes=%u, expected n_writes=%u!\n",\r\nbarrier_nr, set_size, expect_size);\r\ngoto bail;\r\n}\r\nlist_for_each_entry(req, &connection->transfer_log, tl_requests)\r\nif (req->epoch == expect_epoch)\r\nbreak;\r\nlist_for_each_entry_safe_from(req, r, &connection->transfer_log, tl_requests) {\r\nif (req->epoch != expect_epoch)\r\nbreak;\r\n_req_mod(req, BARRIER_ACKED);\r\n}\r\nspin_unlock_irq(&connection->resource->req_lock);\r\nreturn;\r\nbail:\r\nspin_unlock_irq(&connection->resource->req_lock);\r\nconn_request_state(connection, NS(conn, C_PROTOCOL_ERROR), CS_HARD);\r\n}\r\nvoid _tl_restart(struct drbd_connection *connection, enum drbd_req_event what)\r\n{\r\nstruct drbd_request *req, *r;\r\nlist_for_each_entry_safe(req, r, &connection->transfer_log, tl_requests)\r\n_req_mod(req, what);\r\n}\r\nvoid tl_restart(struct drbd_connection *connection, enum drbd_req_event what)\r\n{\r\nspin_lock_irq(&connection->resource->req_lock);\r\n_tl_restart(connection, what);\r\nspin_unlock_irq(&connection->resource->req_lock);\r\n}\r\nvoid tl_clear(struct drbd_connection *connection)\r\n{\r\ntl_restart(connection, CONNECTION_LOST_WHILE_PENDING);\r\n}\r\nvoid tl_abort_disk_io(struct drbd_device *device)\r\n{\r\nstruct drbd_connection *connection = first_peer_device(device)->connection;\r\nstruct drbd_request *req, *r;\r\nspin_lock_irq(&connection->resource->req_lock);\r\nlist_for_each_entry_safe(req, r, &connection->transfer_log, tl_requests) {\r\nif (!(req->rq_state & RQ_LOCAL_PENDING))\r\ncontinue;\r\nif (req->device != device)\r\ncontinue;\r\n_req_mod(req, ABORT_DISK_IO);\r\n}\r\nspin_unlock_irq(&connection->resource->req_lock);\r\n}\r\nstatic int drbd_thread_setup(void *arg)\r\n{\r\nstruct drbd_thread *thi = (struct drbd_thread *) arg;\r\nstruct drbd_resource *resource = thi->resource;\r\nunsigned long flags;\r\nint retval;\r\nsnprintf(current->comm, sizeof(current->comm), "drbd_%c_%s",\r\nthi->name[0],\r\nresource->name);\r\nrestart:\r\nretval = thi->function(thi);\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nif (thi->t_state == RESTARTING) {\r\ndrbd_info(resource, "Restarting %s thread\n", thi->name);\r\nthi->t_state = RUNNING;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\ngoto restart;\r\n}\r\nthi->task = NULL;\r\nthi->t_state = NONE;\r\nsmp_mb();\r\ncomplete_all(&thi->stop);\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\ndrbd_info(resource, "Terminating %s\n", current->comm);\r\nif (thi->connection)\r\nkref_put(&thi->connection->kref, drbd_destroy_connection);\r\nkref_put(&resource->kref, drbd_destroy_resource);\r\nmodule_put(THIS_MODULE);\r\nreturn retval;\r\n}\r\nstatic void drbd_thread_init(struct drbd_resource *resource, struct drbd_thread *thi,\r\nint (*func) (struct drbd_thread *), const char *name)\r\n{\r\nspin_lock_init(&thi->t_lock);\r\nthi->task = NULL;\r\nthi->t_state = NONE;\r\nthi->function = func;\r\nthi->resource = resource;\r\nthi->connection = NULL;\r\nthi->name = name;\r\n}\r\nint drbd_thread_start(struct drbd_thread *thi)\r\n{\r\nstruct drbd_resource *resource = thi->resource;\r\nstruct task_struct *nt;\r\nunsigned long flags;\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nswitch (thi->t_state) {\r\ncase NONE:\r\ndrbd_info(resource, "Starting %s thread (from %s [%d])\n",\r\nthi->name, current->comm, current->pid);\r\nif (!try_module_get(THIS_MODULE)) {\r\ndrbd_err(resource, "Failed to get module reference in drbd_thread_start\n");\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nreturn false;\r\n}\r\nkref_get(&resource->kref);\r\nif (thi->connection)\r\nkref_get(&thi->connection->kref);\r\ninit_completion(&thi->stop);\r\nthi->reset_cpu_mask = 1;\r\nthi->t_state = RUNNING;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nflush_signals(current);\r\nnt = kthread_create(drbd_thread_setup, (void *) thi,\r\n"drbd_%c_%s", thi->name[0], thi->resource->name);\r\nif (IS_ERR(nt)) {\r\ndrbd_err(resource, "Couldn't start thread\n");\r\nif (thi->connection)\r\nkref_put(&thi->connection->kref, drbd_destroy_connection);\r\nkref_put(&resource->kref, drbd_destroy_resource);\r\nmodule_put(THIS_MODULE);\r\nreturn false;\r\n}\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nthi->task = nt;\r\nthi->t_state = RUNNING;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nwake_up_process(nt);\r\nbreak;\r\ncase EXITING:\r\nthi->t_state = RESTARTING;\r\ndrbd_info(resource, "Restarting %s thread (from %s [%d])\n",\r\nthi->name, current->comm, current->pid);\r\ncase RUNNING:\r\ncase RESTARTING:\r\ndefault:\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nbreak;\r\n}\r\nreturn true;\r\n}\r\nvoid _drbd_thread_stop(struct drbd_thread *thi, int restart, int wait)\r\n{\r\nunsigned long flags;\r\nenum drbd_thread_state ns = restart ? RESTARTING : EXITING;\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nif (thi->t_state == NONE) {\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nif (restart)\r\ndrbd_thread_start(thi);\r\nreturn;\r\n}\r\nif (thi->t_state != ns) {\r\nif (thi->task == NULL) {\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nreturn;\r\n}\r\nthi->t_state = ns;\r\nsmp_mb();\r\ninit_completion(&thi->stop);\r\nif (thi->task != current)\r\nforce_sig(DRBD_SIGKILL, thi->task);\r\n}\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nif (wait)\r\nwait_for_completion(&thi->stop);\r\n}\r\nint conn_lowest_minor(struct drbd_connection *connection)\r\n{\r\nstruct drbd_peer_device *peer_device;\r\nint vnr = 0, minor = -1;\r\nrcu_read_lock();\r\npeer_device = idr_get_next(&connection->peer_devices, &vnr);\r\nif (peer_device)\r\nminor = device_to_minor(peer_device->device);\r\nrcu_read_unlock();\r\nreturn minor;\r\n}\r\nstatic void drbd_calc_cpu_mask(cpumask_var_t *cpu_mask)\r\n{\r\nunsigned int *resources_per_cpu, min_index = ~0;\r\nresources_per_cpu = kzalloc(nr_cpu_ids * sizeof(*resources_per_cpu), GFP_KERNEL);\r\nif (resources_per_cpu) {\r\nstruct drbd_resource *resource;\r\nunsigned int cpu, min = ~0;\r\nrcu_read_lock();\r\nfor_each_resource_rcu(resource, &drbd_resources) {\r\nfor_each_cpu(cpu, resource->cpu_mask)\r\nresources_per_cpu[cpu]++;\r\n}\r\nrcu_read_unlock();\r\nfor_each_online_cpu(cpu) {\r\nif (resources_per_cpu[cpu] < min) {\r\nmin = resources_per_cpu[cpu];\r\nmin_index = cpu;\r\n}\r\n}\r\nkfree(resources_per_cpu);\r\n}\r\nif (min_index == ~0) {\r\ncpumask_setall(*cpu_mask);\r\nreturn;\r\n}\r\ncpumask_set_cpu(min_index, *cpu_mask);\r\n}\r\nvoid drbd_thread_current_set_cpu(struct drbd_thread *thi)\r\n{\r\nstruct drbd_resource *resource = thi->resource;\r\nstruct task_struct *p = current;\r\nif (!thi->reset_cpu_mask)\r\nreturn;\r\nthi->reset_cpu_mask = 0;\r\nset_cpus_allowed_ptr(p, resource->cpu_mask);\r\n}\r\nunsigned int drbd_header_size(struct drbd_connection *connection)\r\n{\r\nif (connection->agreed_pro_version >= 100) {\r\nBUILD_BUG_ON(!IS_ALIGNED(sizeof(struct p_header100), 8));\r\nreturn sizeof(struct p_header100);\r\n} else {\r\nBUILD_BUG_ON(sizeof(struct p_header80) !=\r\nsizeof(struct p_header95));\r\nBUILD_BUG_ON(!IS_ALIGNED(sizeof(struct p_header80), 8));\r\nreturn sizeof(struct p_header80);\r\n}\r\n}\r\nstatic unsigned int prepare_header80(struct p_header80 *h, enum drbd_packet cmd, int size)\r\n{\r\nh->magic = cpu_to_be32(DRBD_MAGIC);\r\nh->command = cpu_to_be16(cmd);\r\nh->length = cpu_to_be16(size);\r\nreturn sizeof(struct p_header80);\r\n}\r\nstatic unsigned int prepare_header95(struct p_header95 *h, enum drbd_packet cmd, int size)\r\n{\r\nh->magic = cpu_to_be16(DRBD_MAGIC_BIG);\r\nh->command = cpu_to_be16(cmd);\r\nh->length = cpu_to_be32(size);\r\nreturn sizeof(struct p_header95);\r\n}\r\nstatic unsigned int prepare_header100(struct p_header100 *h, enum drbd_packet cmd,\r\nint size, int vnr)\r\n{\r\nh->magic = cpu_to_be32(DRBD_MAGIC_100);\r\nh->volume = cpu_to_be16(vnr);\r\nh->command = cpu_to_be16(cmd);\r\nh->length = cpu_to_be32(size);\r\nh->pad = 0;\r\nreturn sizeof(struct p_header100);\r\n}\r\nstatic unsigned int prepare_header(struct drbd_connection *connection, int vnr,\r\nvoid *buffer, enum drbd_packet cmd, int size)\r\n{\r\nif (connection->agreed_pro_version >= 100)\r\nreturn prepare_header100(buffer, cmd, size, vnr);\r\nelse if (connection->agreed_pro_version >= 95 &&\r\nsize > DRBD_MAX_SIZE_H80_PACKET)\r\nreturn prepare_header95(buffer, cmd, size);\r\nelse\r\nreturn prepare_header80(buffer, cmd, size);\r\n}\r\nstatic void *__conn_prepare_command(struct drbd_connection *connection,\r\nstruct drbd_socket *sock)\r\n{\r\nif (!sock->socket)\r\nreturn NULL;\r\nreturn sock->sbuf + drbd_header_size(connection);\r\n}\r\nvoid *conn_prepare_command(struct drbd_connection *connection, struct drbd_socket *sock)\r\n{\r\nvoid *p;\r\nmutex_lock(&sock->mutex);\r\np = __conn_prepare_command(connection, sock);\r\nif (!p)\r\nmutex_unlock(&sock->mutex);\r\nreturn p;\r\n}\r\nvoid *drbd_prepare_command(struct drbd_peer_device *peer_device, struct drbd_socket *sock)\r\n{\r\nreturn conn_prepare_command(peer_device->connection, sock);\r\n}\r\nstatic int __send_command(struct drbd_connection *connection, int vnr,\r\nstruct drbd_socket *sock, enum drbd_packet cmd,\r\nunsigned int header_size, void *data,\r\nunsigned int size)\r\n{\r\nint msg_flags;\r\nint err;\r\nmsg_flags = data ? MSG_MORE : 0;\r\nheader_size += prepare_header(connection, vnr, sock->sbuf, cmd,\r\nheader_size + size);\r\nerr = drbd_send_all(connection, sock->socket, sock->sbuf, header_size,\r\nmsg_flags);\r\nif (data && !err)\r\nerr = drbd_send_all(connection, sock->socket, data, size, 0);\r\nif (!err && (cmd == P_PING || cmd == P_PING_ACK))\r\ndrbd_tcp_nodelay(sock->socket);\r\nreturn err;\r\n}\r\nstatic int __conn_send_command(struct drbd_connection *connection, struct drbd_socket *sock,\r\nenum drbd_packet cmd, unsigned int header_size,\r\nvoid *data, unsigned int size)\r\n{\r\nreturn __send_command(connection, 0, sock, cmd, header_size, data, size);\r\n}\r\nint conn_send_command(struct drbd_connection *connection, struct drbd_socket *sock,\r\nenum drbd_packet cmd, unsigned int header_size,\r\nvoid *data, unsigned int size)\r\n{\r\nint err;\r\nerr = __conn_send_command(connection, sock, cmd, header_size, data, size);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_command(struct drbd_peer_device *peer_device, struct drbd_socket *sock,\r\nenum drbd_packet cmd, unsigned int header_size,\r\nvoid *data, unsigned int size)\r\n{\r\nint err;\r\nerr = __send_command(peer_device->connection, peer_device->device->vnr,\r\nsock, cmd, header_size, data, size);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_ping(struct drbd_connection *connection)\r\n{\r\nstruct drbd_socket *sock;\r\nsock = &connection->meta;\r\nif (!conn_prepare_command(connection, sock))\r\nreturn -EIO;\r\nreturn conn_send_command(connection, sock, P_PING, 0, NULL, 0);\r\n}\r\nint drbd_send_ping_ack(struct drbd_connection *connection)\r\n{\r\nstruct drbd_socket *sock;\r\nsock = &connection->meta;\r\nif (!conn_prepare_command(connection, sock))\r\nreturn -EIO;\r\nreturn conn_send_command(connection, sock, P_PING_ACK, 0, NULL, 0);\r\n}\r\nint drbd_send_sync_param(struct drbd_peer_device *peer_device)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_rs_param_95 *p;\r\nint size;\r\nconst int apv = peer_device->connection->agreed_pro_version;\r\nenum drbd_packet cmd;\r\nstruct net_conf *nc;\r\nstruct disk_conf *dc;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\nrcu_read_lock();\r\nnc = rcu_dereference(peer_device->connection->net_conf);\r\nsize = apv <= 87 ? sizeof(struct p_rs_param)\r\n: apv == 88 ? sizeof(struct p_rs_param)\r\n+ strlen(nc->verify_alg) + 1\r\n: apv <= 94 ? sizeof(struct p_rs_param_89)\r\n: sizeof(struct p_rs_param_95);\r\ncmd = apv >= 89 ? P_SYNC_PARAM89 : P_SYNC_PARAM;\r\nmemset(p->verify_alg, 0, 2 * SHARED_SECRET_MAX);\r\nif (get_ldev(peer_device->device)) {\r\ndc = rcu_dereference(peer_device->device->ldev->disk_conf);\r\np->resync_rate = cpu_to_be32(dc->resync_rate);\r\np->c_plan_ahead = cpu_to_be32(dc->c_plan_ahead);\r\np->c_delay_target = cpu_to_be32(dc->c_delay_target);\r\np->c_fill_target = cpu_to_be32(dc->c_fill_target);\r\np->c_max_rate = cpu_to_be32(dc->c_max_rate);\r\nput_ldev(peer_device->device);\r\n} else {\r\np->resync_rate = cpu_to_be32(DRBD_RESYNC_RATE_DEF);\r\np->c_plan_ahead = cpu_to_be32(DRBD_C_PLAN_AHEAD_DEF);\r\np->c_delay_target = cpu_to_be32(DRBD_C_DELAY_TARGET_DEF);\r\np->c_fill_target = cpu_to_be32(DRBD_C_FILL_TARGET_DEF);\r\np->c_max_rate = cpu_to_be32(DRBD_C_MAX_RATE_DEF);\r\n}\r\nif (apv >= 88)\r\nstrcpy(p->verify_alg, nc->verify_alg);\r\nif (apv >= 89)\r\nstrcpy(p->csums_alg, nc->csums_alg);\r\nrcu_read_unlock();\r\nreturn drbd_send_command(peer_device, sock, cmd, size, NULL, 0);\r\n}\r\nint __drbd_send_protocol(struct drbd_connection *connection, enum drbd_packet cmd)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_protocol *p;\r\nstruct net_conf *nc;\r\nint size, cf;\r\nsock = &connection->data;\r\np = __conn_prepare_command(connection, sock);\r\nif (!p)\r\nreturn -EIO;\r\nrcu_read_lock();\r\nnc = rcu_dereference(connection->net_conf);\r\nif (nc->tentative && connection->agreed_pro_version < 92) {\r\nrcu_read_unlock();\r\nmutex_unlock(&sock->mutex);\r\ndrbd_err(connection, "--dry-run is not supported by peer");\r\nreturn -EOPNOTSUPP;\r\n}\r\nsize = sizeof(*p);\r\nif (connection->agreed_pro_version >= 87)\r\nsize += strlen(nc->integrity_alg) + 1;\r\np->protocol = cpu_to_be32(nc->wire_protocol);\r\np->after_sb_0p = cpu_to_be32(nc->after_sb_0p);\r\np->after_sb_1p = cpu_to_be32(nc->after_sb_1p);\r\np->after_sb_2p = cpu_to_be32(nc->after_sb_2p);\r\np->two_primaries = cpu_to_be32(nc->two_primaries);\r\ncf = 0;\r\nif (nc->discard_my_data)\r\ncf |= CF_DISCARD_MY_DATA;\r\nif (nc->tentative)\r\ncf |= CF_DRY_RUN;\r\np->conn_flags = cpu_to_be32(cf);\r\nif (connection->agreed_pro_version >= 87)\r\nstrcpy(p->integrity_alg, nc->integrity_alg);\r\nrcu_read_unlock();\r\nreturn __conn_send_command(connection, sock, cmd, size, NULL, 0);\r\n}\r\nint drbd_send_protocol(struct drbd_connection *connection)\r\n{\r\nint err;\r\nmutex_lock(&connection->data.mutex);\r\nerr = __drbd_send_protocol(connection, P_PROTOCOL);\r\nmutex_unlock(&connection->data.mutex);\r\nreturn err;\r\n}\r\nstatic int _drbd_send_uuids(struct drbd_peer_device *peer_device, u64 uuid_flags)\r\n{\r\nstruct drbd_device *device = peer_device->device;\r\nstruct drbd_socket *sock;\r\nstruct p_uuids *p;\r\nint i;\r\nif (!get_ldev_if_state(device, D_NEGOTIATING))\r\nreturn 0;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p) {\r\nput_ldev(device);\r\nreturn -EIO;\r\n}\r\nspin_lock_irq(&device->ldev->md.uuid_lock);\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\np->uuid[i] = cpu_to_be64(device->ldev->md.uuid[i]);\r\nspin_unlock_irq(&device->ldev->md.uuid_lock);\r\ndevice->comm_bm_set = drbd_bm_total_weight(device);\r\np->uuid[UI_SIZE] = cpu_to_be64(device->comm_bm_set);\r\nrcu_read_lock();\r\nuuid_flags |= rcu_dereference(peer_device->connection->net_conf)->discard_my_data ? 1 : 0;\r\nrcu_read_unlock();\r\nuuid_flags |= test_bit(CRASHED_PRIMARY, &device->flags) ? 2 : 0;\r\nuuid_flags |= device->new_state_tmp.disk == D_INCONSISTENT ? 4 : 0;\r\np->uuid[UI_FLAGS] = cpu_to_be64(uuid_flags);\r\nput_ldev(device);\r\nreturn drbd_send_command(peer_device, sock, P_UUIDS, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_uuids(struct drbd_peer_device *peer_device)\r\n{\r\nreturn _drbd_send_uuids(peer_device, 0);\r\n}\r\nint drbd_send_uuids_skip_initial_sync(struct drbd_peer_device *peer_device)\r\n{\r\nreturn _drbd_send_uuids(peer_device, 8);\r\n}\r\nvoid drbd_print_uuids(struct drbd_device *device, const char *text)\r\n{\r\nif (get_ldev_if_state(device, D_NEGOTIATING)) {\r\nu64 *uuid = device->ldev->md.uuid;\r\ndrbd_info(device, "%s %016llX:%016llX:%016llX:%016llX\n",\r\ntext,\r\n(unsigned long long)uuid[UI_CURRENT],\r\n(unsigned long long)uuid[UI_BITMAP],\r\n(unsigned long long)uuid[UI_HISTORY_START],\r\n(unsigned long long)uuid[UI_HISTORY_END]);\r\nput_ldev(device);\r\n} else {\r\ndrbd_info(device, "%s effective data uuid: %016llX\n",\r\ntext,\r\n(unsigned long long)device->ed_uuid);\r\n}\r\n}\r\nvoid drbd_gen_and_send_sync_uuid(struct drbd_peer_device *peer_device)\r\n{\r\nstruct drbd_device *device = peer_device->device;\r\nstruct drbd_socket *sock;\r\nstruct p_rs_uuid *p;\r\nu64 uuid;\r\nD_ASSERT(device, device->state.disk == D_UP_TO_DATE);\r\nuuid = device->ldev->md.uuid[UI_BITMAP];\r\nif (uuid && uuid != UUID_JUST_CREATED)\r\nuuid = uuid + UUID_NEW_BM_OFFSET;\r\nelse\r\nget_random_bytes(&uuid, sizeof(u64));\r\ndrbd_uuid_set(device, UI_BITMAP, uuid);\r\ndrbd_print_uuids(device, "updated sync UUID");\r\ndrbd_md_sync(device);\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (p) {\r\np->uuid = cpu_to_be64(uuid);\r\ndrbd_send_command(peer_device, sock, P_SYNC_UUID, sizeof(*p), NULL, 0);\r\n}\r\n}\r\nint drbd_send_sizes(struct drbd_peer_device *peer_device, int trigger_reply, enum dds_flags flags)\r\n{\r\nstruct drbd_device *device = peer_device->device;\r\nstruct drbd_socket *sock;\r\nstruct p_sizes *p;\r\nsector_t d_size, u_size;\r\nint q_order_type;\r\nunsigned int max_bio_size;\r\nif (get_ldev_if_state(device, D_NEGOTIATING)) {\r\nD_ASSERT(device, device->ldev->backing_bdev);\r\nd_size = drbd_get_max_capacity(device->ldev);\r\nrcu_read_lock();\r\nu_size = rcu_dereference(device->ldev->disk_conf)->disk_size;\r\nrcu_read_unlock();\r\nq_order_type = drbd_queue_order_type(device);\r\nmax_bio_size = queue_max_hw_sectors(device->ldev->backing_bdev->bd_disk->queue) << 9;\r\nmax_bio_size = min(max_bio_size, DRBD_MAX_BIO_SIZE);\r\nput_ldev(device);\r\n} else {\r\nd_size = 0;\r\nu_size = 0;\r\nq_order_type = QUEUE_ORDERED_NONE;\r\nmax_bio_size = DRBD_MAX_BIO_SIZE;\r\n}\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\nif (peer_device->connection->agreed_pro_version <= 94)\r\nmax_bio_size = min(max_bio_size, DRBD_MAX_SIZE_H80_PACKET);\r\nelse if (peer_device->connection->agreed_pro_version < 100)\r\nmax_bio_size = min(max_bio_size, DRBD_MAX_BIO_SIZE_P95);\r\np->d_size = cpu_to_be64(d_size);\r\np->u_size = cpu_to_be64(u_size);\r\np->c_size = cpu_to_be64(trigger_reply ? 0 : drbd_get_capacity(device->this_bdev));\r\np->max_bio_size = cpu_to_be32(max_bio_size);\r\np->queue_order_type = cpu_to_be16(q_order_type);\r\np->dds_flags = cpu_to_be16(flags);\r\nreturn drbd_send_command(peer_device, sock, P_SIZES, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_current_state(struct drbd_peer_device *peer_device)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_state *p;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->state = cpu_to_be32(peer_device->device->state.i);\r\nreturn drbd_send_command(peer_device, sock, P_STATE, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_state(struct drbd_peer_device *peer_device, union drbd_state state)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_state *p;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->state = cpu_to_be32(state.i);\r\nreturn drbd_send_command(peer_device, sock, P_STATE, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_state_req(struct drbd_peer_device *peer_device, union drbd_state mask, union drbd_state val)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_req_state *p;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->mask = cpu_to_be32(mask.i);\r\np->val = cpu_to_be32(val.i);\r\nreturn drbd_send_command(peer_device, sock, P_STATE_CHG_REQ, sizeof(*p), NULL, 0);\r\n}\r\nint conn_send_state_req(struct drbd_connection *connection, union drbd_state mask, union drbd_state val)\r\n{\r\nenum drbd_packet cmd;\r\nstruct drbd_socket *sock;\r\nstruct p_req_state *p;\r\ncmd = connection->agreed_pro_version < 100 ? P_STATE_CHG_REQ : P_CONN_ST_CHG_REQ;\r\nsock = &connection->data;\r\np = conn_prepare_command(connection, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->mask = cpu_to_be32(mask.i);\r\np->val = cpu_to_be32(val.i);\r\nreturn conn_send_command(connection, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\nvoid drbd_send_sr_reply(struct drbd_peer_device *peer_device, enum drbd_state_rv retcode)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_req_state_reply *p;\r\nsock = &peer_device->connection->meta;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (p) {\r\np->retcode = cpu_to_be32(retcode);\r\ndrbd_send_command(peer_device, sock, P_STATE_CHG_REPLY, sizeof(*p), NULL, 0);\r\n}\r\n}\r\nvoid conn_send_sr_reply(struct drbd_connection *connection, enum drbd_state_rv retcode)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_req_state_reply *p;\r\nenum drbd_packet cmd = connection->agreed_pro_version < 100 ? P_STATE_CHG_REPLY : P_CONN_ST_CHG_REPLY;\r\nsock = &connection->meta;\r\np = conn_prepare_command(connection, sock);\r\nif (p) {\r\np->retcode = cpu_to_be32(retcode);\r\nconn_send_command(connection, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\n}\r\nstatic void dcbp_set_code(struct p_compressed_bm *p, enum drbd_bitmap_code code)\r\n{\r\nBUG_ON(code & ~0xf);\r\np->encoding = (p->encoding & ~0xf) | code;\r\n}\r\nstatic void dcbp_set_start(struct p_compressed_bm *p, int set)\r\n{\r\np->encoding = (p->encoding & ~0x80) | (set ? 0x80 : 0);\r\n}\r\nstatic void dcbp_set_pad_bits(struct p_compressed_bm *p, int n)\r\n{\r\nBUG_ON(n & ~0x7);\r\np->encoding = (p->encoding & (~0x7 << 4)) | (n << 4);\r\n}\r\nstatic int fill_bitmap_rle_bits(struct drbd_device *device,\r\nstruct p_compressed_bm *p,\r\nunsigned int size,\r\nstruct bm_xfer_ctx *c)\r\n{\r\nstruct bitstream bs;\r\nunsigned long plain_bits;\r\nunsigned long tmp;\r\nunsigned long rl;\r\nunsigned len;\r\nunsigned toggle;\r\nint bits, use_rle;\r\nrcu_read_lock();\r\nuse_rle = rcu_dereference(first_peer_device(device)->connection->net_conf)->use_rle;\r\nrcu_read_unlock();\r\nif (!use_rle || first_peer_device(device)->connection->agreed_pro_version < 90)\r\nreturn 0;\r\nif (c->bit_offset >= c->bm_bits)\r\nreturn 0;\r\nbitstream_init(&bs, p->code, size, 0);\r\nmemset(p->code, 0, size);\r\nplain_bits = 0;\r\ntoggle = 2;\r\ndo {\r\ntmp = (toggle == 0) ? _drbd_bm_find_next_zero(device, c->bit_offset)\r\n: _drbd_bm_find_next(device, c->bit_offset);\r\nif (tmp == -1UL)\r\ntmp = c->bm_bits;\r\nrl = tmp - c->bit_offset;\r\nif (toggle == 2) {\r\nif (rl == 0) {\r\ndcbp_set_start(p, 1);\r\ntoggle = !toggle;\r\ncontinue;\r\n}\r\ndcbp_set_start(p, 0);\r\n}\r\nif (rl == 0) {\r\ndrbd_err(device, "unexpected zero runlength while encoding bitmap "\r\n"t:%u bo:%lu\n", toggle, c->bit_offset);\r\nreturn -1;\r\n}\r\nbits = vli_encode_bits(&bs, rl);\r\nif (bits == -ENOBUFS)\r\nbreak;\r\nif (bits <= 0) {\r\ndrbd_err(device, "error while encoding bitmap: %d\n", bits);\r\nreturn 0;\r\n}\r\ntoggle = !toggle;\r\nplain_bits += rl;\r\nc->bit_offset = tmp;\r\n} while (c->bit_offset < c->bm_bits);\r\nlen = bs.cur.b - p->code + !!bs.cur.bit;\r\nif (plain_bits < (len << 3)) {\r\nc->bit_offset -= plain_bits;\r\nbm_xfer_ctx_bit_to_word_offset(c);\r\nc->bit_offset = c->word_offset * BITS_PER_LONG;\r\nreturn 0;\r\n}\r\nbm_xfer_ctx_bit_to_word_offset(c);\r\ndcbp_set_pad_bits(p, (8 - bs.cur.bit) & 0x7);\r\nreturn len;\r\n}\r\nstatic int\r\nsend_bitmap_rle_or_plain(struct drbd_device *device, struct bm_xfer_ctx *c)\r\n{\r\nstruct drbd_socket *sock = &first_peer_device(device)->connection->data;\r\nunsigned int header_size = drbd_header_size(first_peer_device(device)->connection);\r\nstruct p_compressed_bm *p = sock->sbuf + header_size;\r\nint len, err;\r\nlen = fill_bitmap_rle_bits(device, p,\r\nDRBD_SOCKET_BUFFER_SIZE - header_size - sizeof(*p), c);\r\nif (len < 0)\r\nreturn -EIO;\r\nif (len) {\r\ndcbp_set_code(p, RLE_VLI_Bits);\r\nerr = __send_command(first_peer_device(device)->connection, device->vnr, sock,\r\nP_COMPRESSED_BITMAP, sizeof(*p) + len,\r\nNULL, 0);\r\nc->packets[0]++;\r\nc->bytes[0] += header_size + sizeof(*p) + len;\r\nif (c->bit_offset >= c->bm_bits)\r\nlen = 0;\r\n} else {\r\nunsigned int data_size;\r\nunsigned long num_words;\r\nunsigned long *p = sock->sbuf + header_size;\r\ndata_size = DRBD_SOCKET_BUFFER_SIZE - header_size;\r\nnum_words = min_t(size_t, data_size / sizeof(*p),\r\nc->bm_words - c->word_offset);\r\nlen = num_words * sizeof(*p);\r\nif (len)\r\ndrbd_bm_get_lel(device, c->word_offset, num_words, p);\r\nerr = __send_command(first_peer_device(device)->connection, device->vnr, sock, P_BITMAP, len, NULL, 0);\r\nc->word_offset += num_words;\r\nc->bit_offset = c->word_offset * BITS_PER_LONG;\r\nc->packets[1]++;\r\nc->bytes[1] += header_size + len;\r\nif (c->bit_offset > c->bm_bits)\r\nc->bit_offset = c->bm_bits;\r\n}\r\nif (!err) {\r\nif (len == 0) {\r\nINFO_bm_xfer_stats(device, "send", c);\r\nreturn 0;\r\n} else\r\nreturn 1;\r\n}\r\nreturn -EIO;\r\n}\r\nstatic int _drbd_send_bitmap(struct drbd_device *device)\r\n{\r\nstruct bm_xfer_ctx c;\r\nint err;\r\nif (!expect(device->bitmap))\r\nreturn false;\r\nif (get_ldev(device)) {\r\nif (drbd_md_test_flag(device->ldev, MDF_FULL_SYNC)) {\r\ndrbd_info(device, "Writing the whole bitmap, MDF_FullSync was set.\n");\r\ndrbd_bm_set_all(device);\r\nif (drbd_bm_write(device)) {\r\ndrbd_err(device, "Failed to write bitmap to disk!\n");\r\n} else {\r\ndrbd_md_clear_flag(device, MDF_FULL_SYNC);\r\ndrbd_md_sync(device);\r\n}\r\n}\r\nput_ldev(device);\r\n}\r\nc = (struct bm_xfer_ctx) {\r\n.bm_bits = drbd_bm_bits(device),\r\n.bm_words = drbd_bm_words(device),\r\n};\r\ndo {\r\nerr = send_bitmap_rle_or_plain(device, &c);\r\n} while (err > 0);\r\nreturn err == 0;\r\n}\r\nint drbd_send_bitmap(struct drbd_device *device)\r\n{\r\nstruct drbd_socket *sock = &first_peer_device(device)->connection->data;\r\nint err = -1;\r\nmutex_lock(&sock->mutex);\r\nif (sock->socket)\r\nerr = !_drbd_send_bitmap(device);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nvoid drbd_send_b_ack(struct drbd_connection *connection, u32 barrier_nr, u32 set_size)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_barrier_ack *p;\r\nif (connection->cstate < C_WF_REPORT_PARAMS)\r\nreturn;\r\nsock = &connection->meta;\r\np = conn_prepare_command(connection, sock);\r\nif (!p)\r\nreturn;\r\np->barrier = barrier_nr;\r\np->set_size = cpu_to_be32(set_size);\r\nconn_send_command(connection, sock, P_BARRIER_ACK, sizeof(*p), NULL, 0);\r\n}\r\nstatic int _drbd_send_ack(struct drbd_peer_device *peer_device, enum drbd_packet cmd,\r\nu64 sector, u32 blksize, u64 block_id)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_ack *p;\r\nif (peer_device->device->state.conn < C_CONNECTED)\r\nreturn -EIO;\r\nsock = &peer_device->connection->meta;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = sector;\r\np->block_id = block_id;\r\np->blksize = blksize;\r\np->seq_num = cpu_to_be32(atomic_inc_return(&peer_device->device->packet_seq));\r\nreturn drbd_send_command(peer_device, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\nvoid drbd_send_ack_dp(struct drbd_peer_device *peer_device, enum drbd_packet cmd,\r\nstruct p_data *dp, int data_size)\r\n{\r\nif (peer_device->connection->peer_integrity_tfm)\r\ndata_size -= crypto_hash_digestsize(peer_device->connection->peer_integrity_tfm);\r\n_drbd_send_ack(peer_device, cmd, dp->sector, cpu_to_be32(data_size),\r\ndp->block_id);\r\n}\r\nvoid drbd_send_ack_rp(struct drbd_peer_device *peer_device, enum drbd_packet cmd,\r\nstruct p_block_req *rp)\r\n{\r\n_drbd_send_ack(peer_device, cmd, rp->sector, rp->blksize, rp->block_id);\r\n}\r\nint drbd_send_ack(struct drbd_peer_device *peer_device, enum drbd_packet cmd,\r\nstruct drbd_peer_request *peer_req)\r\n{\r\nreturn _drbd_send_ack(peer_device, cmd,\r\ncpu_to_be64(peer_req->i.sector),\r\ncpu_to_be32(peer_req->i.size),\r\npeer_req->block_id);\r\n}\r\nint drbd_send_ack_ex(struct drbd_peer_device *peer_device, enum drbd_packet cmd,\r\nsector_t sector, int blksize, u64 block_id)\r\n{\r\nreturn _drbd_send_ack(peer_device, cmd,\r\ncpu_to_be64(sector),\r\ncpu_to_be32(blksize),\r\ncpu_to_be64(block_id));\r\n}\r\nint drbd_send_drequest(struct drbd_peer_device *peer_device, int cmd,\r\nsector_t sector, int size, u64 block_id)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_req *p;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(sector);\r\np->block_id = block_id;\r\np->blksize = cpu_to_be32(size);\r\nreturn drbd_send_command(peer_device, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_drequest_csum(struct drbd_peer_device *peer_device, sector_t sector, int size,\r\nvoid *digest, int digest_size, enum drbd_packet cmd)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_req *p;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(sector);\r\np->block_id = ID_SYNCER ;\r\np->blksize = cpu_to_be32(size);\r\nreturn drbd_send_command(peer_device, sock, cmd, sizeof(*p), digest, digest_size);\r\n}\r\nint drbd_send_ov_request(struct drbd_peer_device *peer_device, sector_t sector, int size)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_req *p;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(sector);\r\np->block_id = ID_SYNCER ;\r\np->blksize = cpu_to_be32(size);\r\nreturn drbd_send_command(peer_device, sock, P_OV_REQUEST, sizeof(*p), NULL, 0);\r\n}\r\nstatic int we_should_drop_the_connection(struct drbd_connection *connection, struct socket *sock)\r\n{\r\nint drop_it;\r\ndrop_it = connection->meta.socket == sock\r\n|| !connection->asender.task\r\n|| get_t_state(&connection->asender) != RUNNING\r\n|| connection->cstate < C_WF_REPORT_PARAMS;\r\nif (drop_it)\r\nreturn true;\r\ndrop_it = !--connection->ko_count;\r\nif (!drop_it) {\r\ndrbd_err(connection, "[%s/%d] sock_sendmsg time expired, ko = %u\n",\r\ncurrent->comm, current->pid, connection->ko_count);\r\nrequest_ping(connection);\r\n}\r\nreturn drop_it; ;\r\n}\r\nstatic void drbd_update_congested(struct drbd_connection *connection)\r\n{\r\nstruct sock *sk = connection->data.socket->sk;\r\nif (sk->sk_wmem_queued > sk->sk_sndbuf * 4 / 5)\r\nset_bit(NET_CONGESTED, &connection->flags);\r\n}\r\nstatic int _drbd_no_send_page(struct drbd_peer_device *peer_device, struct page *page,\r\nint offset, size_t size, unsigned msg_flags)\r\n{\r\nstruct socket *socket;\r\nvoid *addr;\r\nint err;\r\nsocket = peer_device->connection->data.socket;\r\naddr = kmap(page) + offset;\r\nerr = drbd_send_all(peer_device->connection, socket, addr, size, msg_flags);\r\nkunmap(page);\r\nif (!err)\r\npeer_device->device->send_cnt += size >> 9;\r\nreturn err;\r\n}\r\nstatic int _drbd_send_page(struct drbd_peer_device *peer_device, struct page *page,\r\nint offset, size_t size, unsigned msg_flags)\r\n{\r\nstruct socket *socket = peer_device->connection->data.socket;\r\nmm_segment_t oldfs = get_fs();\r\nint len = size;\r\nint err = -EIO;\r\nif (disable_sendpage || (page_count(page) < 1) || PageSlab(page))\r\nreturn _drbd_no_send_page(peer_device, page, offset, size, msg_flags);\r\nmsg_flags |= MSG_NOSIGNAL;\r\ndrbd_update_congested(peer_device->connection);\r\nset_fs(KERNEL_DS);\r\ndo {\r\nint sent;\r\nsent = socket->ops->sendpage(socket, page, offset, len, msg_flags);\r\nif (sent <= 0) {\r\nif (sent == -EAGAIN) {\r\nif (we_should_drop_the_connection(peer_device->connection, socket))\r\nbreak;\r\ncontinue;\r\n}\r\ndrbd_warn(peer_device->device, "%s: size=%d len=%d sent=%d\n",\r\n__func__, (int)size, len, sent);\r\nif (sent < 0)\r\nerr = sent;\r\nbreak;\r\n}\r\nlen -= sent;\r\noffset += sent;\r\n} while (len > 0 );\r\nset_fs(oldfs);\r\nclear_bit(NET_CONGESTED, &peer_device->connection->flags);\r\nif (len == 0) {\r\nerr = 0;\r\npeer_device->device->send_cnt += size >> 9;\r\n}\r\nreturn err;\r\n}\r\nstatic int _drbd_send_bio(struct drbd_peer_device *peer_device, struct bio *bio)\r\n{\r\nstruct bio_vec bvec;\r\nstruct bvec_iter iter;\r\nbio_for_each_segment(bvec, bio, iter) {\r\nint err;\r\nerr = _drbd_no_send_page(peer_device, bvec.bv_page,\r\nbvec.bv_offset, bvec.bv_len,\r\nbio_iter_last(bvec, iter)\r\n? 0 : MSG_MORE);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int _drbd_send_zc_bio(struct drbd_peer_device *peer_device, struct bio *bio)\r\n{\r\nstruct bio_vec bvec;\r\nstruct bvec_iter iter;\r\nbio_for_each_segment(bvec, bio, iter) {\r\nint err;\r\nerr = _drbd_send_page(peer_device, bvec.bv_page,\r\nbvec.bv_offset, bvec.bv_len,\r\nbio_iter_last(bvec, iter) ? 0 : MSG_MORE);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int _drbd_send_zc_ee(struct drbd_peer_device *peer_device,\r\nstruct drbd_peer_request *peer_req)\r\n{\r\nstruct page *page = peer_req->pages;\r\nunsigned len = peer_req->i.size;\r\nint err;\r\npage_chain_for_each(page) {\r\nunsigned l = min_t(unsigned, len, PAGE_SIZE);\r\nerr = _drbd_send_page(peer_device, page, 0, l,\r\npage_chain_next(page) ? MSG_MORE : 0);\r\nif (err)\r\nreturn err;\r\nlen -= l;\r\n}\r\nreturn 0;\r\n}\r\nstatic u32 bio_flags_to_wire(struct drbd_connection *connection, unsigned long bi_rw)\r\n{\r\nif (connection->agreed_pro_version >= 95)\r\nreturn (bi_rw & REQ_SYNC ? DP_RW_SYNC : 0) |\r\n(bi_rw & REQ_FUA ? DP_FUA : 0) |\r\n(bi_rw & REQ_FLUSH ? DP_FLUSH : 0) |\r\n(bi_rw & REQ_DISCARD ? DP_DISCARD : 0);\r\nelse\r\nreturn bi_rw & REQ_SYNC ? DP_RW_SYNC : 0;\r\n}\r\nint drbd_send_dblock(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_device *device = peer_device->device;\r\nstruct drbd_socket *sock;\r\nstruct p_data *p;\r\nunsigned int dp_flags = 0;\r\nint digest_size;\r\nint err;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\ndigest_size = peer_device->connection->integrity_tfm ?\r\ncrypto_hash_digestsize(peer_device->connection->integrity_tfm) : 0;\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(req->i.sector);\r\np->block_id = (unsigned long)req;\r\np->seq_num = cpu_to_be32(atomic_inc_return(&device->packet_seq));\r\ndp_flags = bio_flags_to_wire(peer_device->connection, req->master_bio->bi_rw);\r\nif (device->state.conn >= C_SYNC_SOURCE &&\r\ndevice->state.conn <= C_PAUSED_SYNC_T)\r\ndp_flags |= DP_MAY_SET_IN_SYNC;\r\nif (peer_device->connection->agreed_pro_version >= 100) {\r\nif (req->rq_state & RQ_EXP_RECEIVE_ACK)\r\ndp_flags |= DP_SEND_RECEIVE_ACK;\r\nif (req->rq_state & RQ_EXP_WRITE_ACK\r\n|| (dp_flags & DP_MAY_SET_IN_SYNC))\r\ndp_flags |= DP_SEND_WRITE_ACK;\r\n}\r\np->dp_flags = cpu_to_be32(dp_flags);\r\nif (dp_flags & DP_DISCARD) {\r\nstruct p_trim *t = (struct p_trim*)p;\r\nt->size = cpu_to_be32(req->i.size);\r\nerr = __send_command(peer_device->connection, device->vnr, sock, P_TRIM, sizeof(*t), NULL, 0);\r\ngoto out;\r\n}\r\nif (digest_size)\r\ndrbd_csum_bio(peer_device->connection->integrity_tfm, req->master_bio, p + 1);\r\nerr = __send_command(peer_device->connection, device->vnr, sock, P_DATA, sizeof(*p) + digest_size, NULL, req->i.size);\r\nif (!err) {\r\nif (!(req->rq_state & (RQ_EXP_RECEIVE_ACK | RQ_EXP_WRITE_ACK)) || digest_size)\r\nerr = _drbd_send_bio(peer_device, req->master_bio);\r\nelse\r\nerr = _drbd_send_zc_bio(peer_device, req->master_bio);\r\nif (digest_size > 0 && digest_size <= 64) {\r\nunsigned char digest[64];\r\ndrbd_csum_bio(peer_device->connection->integrity_tfm, req->master_bio, digest);\r\nif (memcmp(p + 1, digest, digest_size)) {\r\ndrbd_warn(device,\r\n"Digest mismatch, buffer modified by upper layers during write: %llus +%u\n",\r\n(unsigned long long)req->i.sector, req->i.size);\r\n}\r\n}\r\n}\r\nout:\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_block(struct drbd_peer_device *peer_device, enum drbd_packet cmd,\r\nstruct drbd_peer_request *peer_req)\r\n{\r\nstruct drbd_device *device = peer_device->device;\r\nstruct drbd_socket *sock;\r\nstruct p_data *p;\r\nint err;\r\nint digest_size;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\ndigest_size = peer_device->connection->integrity_tfm ?\r\ncrypto_hash_digestsize(peer_device->connection->integrity_tfm) : 0;\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(peer_req->i.sector);\r\np->block_id = peer_req->block_id;\r\np->seq_num = 0;\r\np->dp_flags = 0;\r\nif (digest_size)\r\ndrbd_csum_ee(peer_device->connection->integrity_tfm, peer_req, p + 1);\r\nerr = __send_command(peer_device->connection, device->vnr, sock, cmd, sizeof(*p) + digest_size, NULL, peer_req->i.size);\r\nif (!err)\r\nerr = _drbd_send_zc_ee(peer_device, peer_req);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_out_of_sync(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_desc *p;\r\nsock = &peer_device->connection->data;\r\np = drbd_prepare_command(peer_device, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(req->i.sector);\r\np->blksize = cpu_to_be32(req->i.size);\r\nreturn drbd_send_command(peer_device, sock, P_OUT_OF_SYNC, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send(struct drbd_connection *connection, struct socket *sock,\r\nvoid *buf, size_t size, unsigned msg_flags)\r\n{\r\nstruct kvec iov;\r\nstruct msghdr msg;\r\nint rv, sent = 0;\r\nif (!sock)\r\nreturn -EBADR;\r\niov.iov_base = buf;\r\niov.iov_len = size;\r\nmsg.msg_name = NULL;\r\nmsg.msg_namelen = 0;\r\nmsg.msg_control = NULL;\r\nmsg.msg_controllen = 0;\r\nmsg.msg_flags = msg_flags | MSG_NOSIGNAL;\r\nif (sock == connection->data.socket) {\r\nrcu_read_lock();\r\nconnection->ko_count = rcu_dereference(connection->net_conf)->ko_count;\r\nrcu_read_unlock();\r\ndrbd_update_congested(connection);\r\n}\r\ndo {\r\nrv = kernel_sendmsg(sock, &msg, &iov, 1, size);\r\nif (rv == -EAGAIN) {\r\nif (we_should_drop_the_connection(connection, sock))\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\nif (rv == -EINTR) {\r\nflush_signals(current);\r\nrv = 0;\r\n}\r\nif (rv < 0)\r\nbreak;\r\nsent += rv;\r\niov.iov_base += rv;\r\niov.iov_len -= rv;\r\n} while (sent < size);\r\nif (sock == connection->data.socket)\r\nclear_bit(NET_CONGESTED, &connection->flags);\r\nif (rv <= 0) {\r\nif (rv != -EAGAIN) {\r\ndrbd_err(connection, "%s_sendmsg returned %d\n",\r\nsock == connection->meta.socket ? "msock" : "sock",\r\nrv);\r\nconn_request_state(connection, NS(conn, C_BROKEN_PIPE), CS_HARD);\r\n} else\r\nconn_request_state(connection, NS(conn, C_TIMEOUT), CS_HARD);\r\n}\r\nreturn sent;\r\n}\r\nint drbd_send_all(struct drbd_connection *connection, struct socket *sock, void *buffer,\r\nsize_t size, unsigned msg_flags)\r\n{\r\nint err;\r\nerr = drbd_send(connection, sock, buffer, size, msg_flags);\r\nif (err < 0)\r\nreturn err;\r\nif (err != size)\r\nreturn -EIO;\r\nreturn 0;\r\n}\r\nstatic int drbd_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nstruct drbd_device *device = bdev->bd_disk->private_data;\r\nunsigned long flags;\r\nint rv = 0;\r\nmutex_lock(&drbd_main_mutex);\r\nspin_lock_irqsave(&device->resource->req_lock, flags);\r\nif (device->state.role != R_PRIMARY) {\r\nif (mode & FMODE_WRITE)\r\nrv = -EROFS;\r\nelse if (!allow_oos)\r\nrv = -EMEDIUMTYPE;\r\n}\r\nif (!rv)\r\ndevice->open_cnt++;\r\nspin_unlock_irqrestore(&device->resource->req_lock, flags);\r\nmutex_unlock(&drbd_main_mutex);\r\nreturn rv;\r\n}\r\nstatic void drbd_release(struct gendisk *gd, fmode_t mode)\r\n{\r\nstruct drbd_device *device = gd->private_data;\r\nmutex_lock(&drbd_main_mutex);\r\ndevice->open_cnt--;\r\nmutex_unlock(&drbd_main_mutex);\r\n}\r\nstatic void drbd_set_defaults(struct drbd_device *device)\r\n{\r\ndevice->state = (union drbd_dev_state) {\r\n{ .role = R_SECONDARY,\r\n.peer = R_UNKNOWN,\r\n.conn = C_STANDALONE,\r\n.disk = D_DISKLESS,\r\n.pdsk = D_UNKNOWN,\r\n} };\r\n}\r\nvoid drbd_init_set_defaults(struct drbd_device *device)\r\n{\r\ndrbd_set_defaults(device);\r\natomic_set(&device->ap_bio_cnt, 0);\r\natomic_set(&device->ap_actlog_cnt, 0);\r\natomic_set(&device->ap_pending_cnt, 0);\r\natomic_set(&device->rs_pending_cnt, 0);\r\natomic_set(&device->unacked_cnt, 0);\r\natomic_set(&device->local_cnt, 0);\r\natomic_set(&device->pp_in_use_by_net, 0);\r\natomic_set(&device->rs_sect_in, 0);\r\natomic_set(&device->rs_sect_ev, 0);\r\natomic_set(&device->ap_in_flight, 0);\r\natomic_set(&device->md_io.in_use, 0);\r\nmutex_init(&device->own_state_mutex);\r\ndevice->state_mutex = &device->own_state_mutex;\r\nspin_lock_init(&device->al_lock);\r\nspin_lock_init(&device->peer_seq_lock);\r\nINIT_LIST_HEAD(&device->active_ee);\r\nINIT_LIST_HEAD(&device->sync_ee);\r\nINIT_LIST_HEAD(&device->done_ee);\r\nINIT_LIST_HEAD(&device->read_ee);\r\nINIT_LIST_HEAD(&device->net_ee);\r\nINIT_LIST_HEAD(&device->resync_reads);\r\nINIT_LIST_HEAD(&device->resync_work.list);\r\nINIT_LIST_HEAD(&device->unplug_work.list);\r\nINIT_LIST_HEAD(&device->bm_io_work.w.list);\r\nINIT_LIST_HEAD(&device->pending_master_completion[0]);\r\nINIT_LIST_HEAD(&device->pending_master_completion[1]);\r\nINIT_LIST_HEAD(&device->pending_completion[0]);\r\nINIT_LIST_HEAD(&device->pending_completion[1]);\r\ndevice->resync_work.cb = w_resync_timer;\r\ndevice->unplug_work.cb = w_send_write_hint;\r\ndevice->bm_io_work.w.cb = w_bitmap_io;\r\ninit_timer(&device->resync_timer);\r\ninit_timer(&device->md_sync_timer);\r\ninit_timer(&device->start_resync_timer);\r\ninit_timer(&device->request_timer);\r\ndevice->resync_timer.function = resync_timer_fn;\r\ndevice->resync_timer.data = (unsigned long) device;\r\ndevice->md_sync_timer.function = md_sync_timer_fn;\r\ndevice->md_sync_timer.data = (unsigned long) device;\r\ndevice->start_resync_timer.function = start_resync_timer_fn;\r\ndevice->start_resync_timer.data = (unsigned long) device;\r\ndevice->request_timer.function = request_timer_fn;\r\ndevice->request_timer.data = (unsigned long) device;\r\ninit_waitqueue_head(&device->misc_wait);\r\ninit_waitqueue_head(&device->state_wait);\r\ninit_waitqueue_head(&device->ee_wait);\r\ninit_waitqueue_head(&device->al_wait);\r\ninit_waitqueue_head(&device->seq_wait);\r\ndevice->resync_wenr = LC_FREE;\r\ndevice->peer_max_bio_size = DRBD_MAX_BIO_SIZE_SAFE;\r\ndevice->local_max_bio_size = DRBD_MAX_BIO_SIZE_SAFE;\r\n}\r\nvoid drbd_device_cleanup(struct drbd_device *device)\r\n{\r\nint i;\r\nif (first_peer_device(device)->connection->receiver.t_state != NONE)\r\ndrbd_err(device, "ASSERT FAILED: receiver t_state == %d expected 0.\n",\r\nfirst_peer_device(device)->connection->receiver.t_state);\r\ndevice->al_writ_cnt =\r\ndevice->bm_writ_cnt =\r\ndevice->read_cnt =\r\ndevice->recv_cnt =\r\ndevice->send_cnt =\r\ndevice->writ_cnt =\r\ndevice->p_size =\r\ndevice->rs_start =\r\ndevice->rs_total =\r\ndevice->rs_failed = 0;\r\ndevice->rs_last_events = 0;\r\ndevice->rs_last_sect_ev = 0;\r\nfor (i = 0; i < DRBD_SYNC_MARKS; i++) {\r\ndevice->rs_mark_left[i] = 0;\r\ndevice->rs_mark_time[i] = 0;\r\n}\r\nD_ASSERT(device, first_peer_device(device)->connection->net_conf == NULL);\r\ndrbd_set_my_capacity(device, 0);\r\nif (device->bitmap) {\r\ndrbd_bm_resize(device, 0, 1);\r\ndrbd_bm_cleanup(device);\r\n}\r\ndrbd_free_ldev(device->ldev);\r\ndevice->ldev = NULL;\r\nclear_bit(AL_SUSPENDED, &device->flags);\r\nD_ASSERT(device, list_empty(&device->active_ee));\r\nD_ASSERT(device, list_empty(&device->sync_ee));\r\nD_ASSERT(device, list_empty(&device->done_ee));\r\nD_ASSERT(device, list_empty(&device->read_ee));\r\nD_ASSERT(device, list_empty(&device->net_ee));\r\nD_ASSERT(device, list_empty(&device->resync_reads));\r\nD_ASSERT(device, list_empty(&first_peer_device(device)->connection->sender_work.q));\r\nD_ASSERT(device, list_empty(&device->resync_work.list));\r\nD_ASSERT(device, list_empty(&device->unplug_work.list));\r\ndrbd_set_defaults(device);\r\n}\r\nstatic void drbd_destroy_mempools(void)\r\n{\r\nstruct page *page;\r\nwhile (drbd_pp_pool) {\r\npage = drbd_pp_pool;\r\ndrbd_pp_pool = (struct page *)page_private(page);\r\n__free_page(page);\r\ndrbd_pp_vacant--;\r\n}\r\nif (drbd_md_io_bio_set)\r\nbioset_free(drbd_md_io_bio_set);\r\nif (drbd_md_io_page_pool)\r\nmempool_destroy(drbd_md_io_page_pool);\r\nif (drbd_ee_mempool)\r\nmempool_destroy(drbd_ee_mempool);\r\nif (drbd_request_mempool)\r\nmempool_destroy(drbd_request_mempool);\r\nif (drbd_ee_cache)\r\nkmem_cache_destroy(drbd_ee_cache);\r\nif (drbd_request_cache)\r\nkmem_cache_destroy(drbd_request_cache);\r\nif (drbd_bm_ext_cache)\r\nkmem_cache_destroy(drbd_bm_ext_cache);\r\nif (drbd_al_ext_cache)\r\nkmem_cache_destroy(drbd_al_ext_cache);\r\ndrbd_md_io_bio_set = NULL;\r\ndrbd_md_io_page_pool = NULL;\r\ndrbd_ee_mempool = NULL;\r\ndrbd_request_mempool = NULL;\r\ndrbd_ee_cache = NULL;\r\ndrbd_request_cache = NULL;\r\ndrbd_bm_ext_cache = NULL;\r\ndrbd_al_ext_cache = NULL;\r\nreturn;\r\n}\r\nstatic int drbd_create_mempools(void)\r\n{\r\nstruct page *page;\r\nconst int number = (DRBD_MAX_BIO_SIZE/PAGE_SIZE) * minor_count;\r\nint i;\r\ndrbd_request_mempool = NULL;\r\ndrbd_ee_cache = NULL;\r\ndrbd_request_cache = NULL;\r\ndrbd_bm_ext_cache = NULL;\r\ndrbd_al_ext_cache = NULL;\r\ndrbd_pp_pool = NULL;\r\ndrbd_md_io_page_pool = NULL;\r\ndrbd_md_io_bio_set = NULL;\r\ndrbd_request_cache = kmem_cache_create(\r\n"drbd_req", sizeof(struct drbd_request), 0, 0, NULL);\r\nif (drbd_request_cache == NULL)\r\ngoto Enomem;\r\ndrbd_ee_cache = kmem_cache_create(\r\n"drbd_ee", sizeof(struct drbd_peer_request), 0, 0, NULL);\r\nif (drbd_ee_cache == NULL)\r\ngoto Enomem;\r\ndrbd_bm_ext_cache = kmem_cache_create(\r\n"drbd_bm", sizeof(struct bm_extent), 0, 0, NULL);\r\nif (drbd_bm_ext_cache == NULL)\r\ngoto Enomem;\r\ndrbd_al_ext_cache = kmem_cache_create(\r\n"drbd_al", sizeof(struct lc_element), 0, 0, NULL);\r\nif (drbd_al_ext_cache == NULL)\r\ngoto Enomem;\r\ndrbd_md_io_bio_set = bioset_create(DRBD_MIN_POOL_PAGES, 0);\r\nif (drbd_md_io_bio_set == NULL)\r\ngoto Enomem;\r\ndrbd_md_io_page_pool = mempool_create_page_pool(DRBD_MIN_POOL_PAGES, 0);\r\nif (drbd_md_io_page_pool == NULL)\r\ngoto Enomem;\r\ndrbd_request_mempool = mempool_create_slab_pool(number,\r\ndrbd_request_cache);\r\nif (drbd_request_mempool == NULL)\r\ngoto Enomem;\r\ndrbd_ee_mempool = mempool_create_slab_pool(number, drbd_ee_cache);\r\nif (drbd_ee_mempool == NULL)\r\ngoto Enomem;\r\nspin_lock_init(&drbd_pp_lock);\r\nfor (i = 0; i < number; i++) {\r\npage = alloc_page(GFP_HIGHUSER);\r\nif (!page)\r\ngoto Enomem;\r\nset_page_private(page, (unsigned long)drbd_pp_pool);\r\ndrbd_pp_pool = page;\r\n}\r\ndrbd_pp_vacant = number;\r\nreturn 0;\r\nEnomem:\r\ndrbd_destroy_mempools();\r\nreturn -ENOMEM;\r\n}\r\nstatic void drbd_release_all_peer_reqs(struct drbd_device *device)\r\n{\r\nint rr;\r\nrr = drbd_free_peer_reqs(device, &device->active_ee);\r\nif (rr)\r\ndrbd_err(device, "%d EEs in active list found!\n", rr);\r\nrr = drbd_free_peer_reqs(device, &device->sync_ee);\r\nif (rr)\r\ndrbd_err(device, "%d EEs in sync list found!\n", rr);\r\nrr = drbd_free_peer_reqs(device, &device->read_ee);\r\nif (rr)\r\ndrbd_err(device, "%d EEs in read list found!\n", rr);\r\nrr = drbd_free_peer_reqs(device, &device->done_ee);\r\nif (rr)\r\ndrbd_err(device, "%d EEs in done list found!\n", rr);\r\nrr = drbd_free_peer_reqs(device, &device->net_ee);\r\nif (rr)\r\ndrbd_err(device, "%d EEs in net list found!\n", rr);\r\n}\r\nvoid drbd_destroy_device(struct kref *kref)\r\n{\r\nstruct drbd_device *device = container_of(kref, struct drbd_device, kref);\r\nstruct drbd_resource *resource = device->resource;\r\nstruct drbd_peer_device *peer_device, *tmp_peer_device;\r\ndel_timer_sync(&device->request_timer);\r\nD_ASSERT(device, device->open_cnt == 0);\r\nif (device->this_bdev)\r\nbdput(device->this_bdev);\r\ndrbd_free_ldev(device->ldev);\r\ndevice->ldev = NULL;\r\ndrbd_release_all_peer_reqs(device);\r\nlc_destroy(device->act_log);\r\nlc_destroy(device->resync);\r\nkfree(device->p_uuid);\r\nif (device->bitmap)\r\ndrbd_bm_cleanup(device);\r\n__free_page(device->md_io.page);\r\nput_disk(device->vdisk);\r\nblk_cleanup_queue(device->rq_queue);\r\nkfree(device->rs_plan_s);\r\nfor_each_peer_device_safe(peer_device, tmp_peer_device, device) {\r\nkref_put(&peer_device->connection->kref, drbd_destroy_connection);\r\nkfree(peer_device);\r\n}\r\nmemset(device, 0xfd, sizeof(*device));\r\nkfree(device);\r\nkref_put(&resource->kref, drbd_destroy_resource);\r\n}\r\nstatic void do_retry(struct work_struct *ws)\r\n{\r\nstruct retry_worker *retry = container_of(ws, struct retry_worker, worker);\r\nLIST_HEAD(writes);\r\nstruct drbd_request *req, *tmp;\r\nspin_lock_irq(&retry->lock);\r\nlist_splice_init(&retry->writes, &writes);\r\nspin_unlock_irq(&retry->lock);\r\nlist_for_each_entry_safe(req, tmp, &writes, tl_requests) {\r\nstruct drbd_device *device = req->device;\r\nstruct bio *bio = req->master_bio;\r\nunsigned long start_jif = req->start_jif;\r\nbool expected;\r\nexpected =\r\nexpect(atomic_read(&req->completion_ref) == 0) &&\r\nexpect(req->rq_state & RQ_POSTPONED) &&\r\nexpect((req->rq_state & RQ_LOCAL_PENDING) == 0 ||\r\n(req->rq_state & RQ_LOCAL_ABORTED) != 0);\r\nif (!expected)\r\ndrbd_err(device, "req=%p completion_ref=%d rq_state=%x\n",\r\nreq, atomic_read(&req->completion_ref),\r\nreq->rq_state);\r\nkref_put(&req->kref, drbd_req_destroy);\r\ninc_ap_bio(device);\r\n__drbd_make_request(device, bio, start_jif);\r\n}\r\n}\r\nvoid drbd_restart_request(struct drbd_request *req)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&retry.lock, flags);\r\nlist_move_tail(&req->tl_requests, &retry.writes);\r\nspin_unlock_irqrestore(&retry.lock, flags);\r\ndec_ap_bio(req->device);\r\nqueue_work(retry.wq, &retry.worker);\r\n}\r\nvoid drbd_destroy_resource(struct kref *kref)\r\n{\r\nstruct drbd_resource *resource =\r\ncontainer_of(kref, struct drbd_resource, kref);\r\nidr_destroy(&resource->devices);\r\nfree_cpumask_var(resource->cpu_mask);\r\nkfree(resource->name);\r\nmemset(resource, 0xf2, sizeof(*resource));\r\nkfree(resource);\r\n}\r\nvoid drbd_free_resource(struct drbd_resource *resource)\r\n{\r\nstruct drbd_connection *connection, *tmp;\r\nfor_each_connection_safe(connection, tmp, resource) {\r\nlist_del(&connection->connections);\r\ndrbd_debugfs_connection_cleanup(connection);\r\nkref_put(&connection->kref, drbd_destroy_connection);\r\n}\r\ndrbd_debugfs_resource_cleanup(resource);\r\nkref_put(&resource->kref, drbd_destroy_resource);\r\n}\r\nstatic void drbd_cleanup(void)\r\n{\r\nunsigned int i;\r\nstruct drbd_device *device;\r\nstruct drbd_resource *resource, *tmp;\r\nif (drbd_proc)\r\nremove_proc_entry("drbd", NULL);\r\nif (retry.wq)\r\ndestroy_workqueue(retry.wq);\r\ndrbd_genl_unregister();\r\ndrbd_debugfs_cleanup();\r\nidr_for_each_entry(&drbd_devices, device, i)\r\ndrbd_delete_device(device);\r\nfor_each_resource_safe(resource, tmp, &drbd_resources) {\r\nlist_del(&resource->resources);\r\ndrbd_free_resource(resource);\r\n}\r\ndrbd_destroy_mempools();\r\nunregister_blkdev(DRBD_MAJOR, "drbd");\r\nidr_destroy(&drbd_devices);\r\npr_info("module cleanup done.\n");\r\n}\r\nstatic int drbd_congested(void *congested_data, int bdi_bits)\r\n{\r\nstruct drbd_device *device = congested_data;\r\nstruct request_queue *q;\r\nchar reason = '-';\r\nint r = 0;\r\nif (!may_inc_ap_bio(device)) {\r\nr = bdi_bits;\r\nreason = 'd';\r\ngoto out;\r\n}\r\nif (test_bit(CALLBACK_PENDING, &first_peer_device(device)->connection->flags)) {\r\nr |= (1 << WB_async_congested);\r\nif (!get_ldev_if_state(device, D_UP_TO_DATE))\r\nr |= (1 << WB_sync_congested);\r\nelse\r\nput_ldev(device);\r\nr &= bdi_bits;\r\nreason = 'c';\r\ngoto out;\r\n}\r\nif (get_ldev(device)) {\r\nq = bdev_get_queue(device->ldev->backing_bdev);\r\nr = bdi_congested(&q->backing_dev_info, bdi_bits);\r\nput_ldev(device);\r\nif (r)\r\nreason = 'b';\r\n}\r\nif (bdi_bits & (1 << WB_async_congested) &&\r\ntest_bit(NET_CONGESTED, &first_peer_device(device)->connection->flags)) {\r\nr |= (1 << WB_async_congested);\r\nreason = reason == 'b' ? 'a' : 'n';\r\n}\r\nout:\r\ndevice->congestion_reason = reason;\r\nreturn r;\r\n}\r\nstatic void drbd_init_workqueue(struct drbd_work_queue* wq)\r\n{\r\nspin_lock_init(&wq->q_lock);\r\nINIT_LIST_HEAD(&wq->q);\r\ninit_waitqueue_head(&wq->q_wait);\r\n}\r\nstatic int w_complete(struct drbd_work *w, int cancel)\r\n{\r\nstruct completion_work *completion_work =\r\ncontainer_of(w, struct completion_work, w);\r\ncomplete(&completion_work->done);\r\nreturn 0;\r\n}\r\nvoid drbd_flush_workqueue(struct drbd_work_queue *work_queue)\r\n{\r\nstruct completion_work completion_work;\r\ncompletion_work.w.cb = w_complete;\r\ninit_completion(&completion_work.done);\r\ndrbd_queue_work(work_queue, &completion_work.w);\r\nwait_for_completion(&completion_work.done);\r\n}\r\nstruct drbd_resource *drbd_find_resource(const char *name)\r\n{\r\nstruct drbd_resource *resource;\r\nif (!name || !name[0])\r\nreturn NULL;\r\nrcu_read_lock();\r\nfor_each_resource_rcu(resource, &drbd_resources) {\r\nif (!strcmp(resource->name, name)) {\r\nkref_get(&resource->kref);\r\ngoto found;\r\n}\r\n}\r\nresource = NULL;\r\nfound:\r\nrcu_read_unlock();\r\nreturn resource;\r\n}\r\nstruct drbd_connection *conn_get_by_addrs(void *my_addr, int my_addr_len,\r\nvoid *peer_addr, int peer_addr_len)\r\n{\r\nstruct drbd_resource *resource;\r\nstruct drbd_connection *connection;\r\nrcu_read_lock();\r\nfor_each_resource_rcu(resource, &drbd_resources) {\r\nfor_each_connection_rcu(connection, resource) {\r\nif (connection->my_addr_len == my_addr_len &&\r\nconnection->peer_addr_len == peer_addr_len &&\r\n!memcmp(&connection->my_addr, my_addr, my_addr_len) &&\r\n!memcmp(&connection->peer_addr, peer_addr, peer_addr_len)) {\r\nkref_get(&connection->kref);\r\ngoto found;\r\n}\r\n}\r\n}\r\nconnection = NULL;\r\nfound:\r\nrcu_read_unlock();\r\nreturn connection;\r\n}\r\nstatic int drbd_alloc_socket(struct drbd_socket *socket)\r\n{\r\nsocket->rbuf = (void *) __get_free_page(GFP_KERNEL);\r\nif (!socket->rbuf)\r\nreturn -ENOMEM;\r\nsocket->sbuf = (void *) __get_free_page(GFP_KERNEL);\r\nif (!socket->sbuf)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void drbd_free_socket(struct drbd_socket *socket)\r\n{\r\nfree_page((unsigned long) socket->sbuf);\r\nfree_page((unsigned long) socket->rbuf);\r\n}\r\nvoid conn_free_crypto(struct drbd_connection *connection)\r\n{\r\ndrbd_free_sock(connection);\r\ncrypto_free_hash(connection->csums_tfm);\r\ncrypto_free_hash(connection->verify_tfm);\r\ncrypto_free_hash(connection->cram_hmac_tfm);\r\ncrypto_free_hash(connection->integrity_tfm);\r\ncrypto_free_hash(connection->peer_integrity_tfm);\r\nkfree(connection->int_dig_in);\r\nkfree(connection->int_dig_vv);\r\nconnection->csums_tfm = NULL;\r\nconnection->verify_tfm = NULL;\r\nconnection->cram_hmac_tfm = NULL;\r\nconnection->integrity_tfm = NULL;\r\nconnection->peer_integrity_tfm = NULL;\r\nconnection->int_dig_in = NULL;\r\nconnection->int_dig_vv = NULL;\r\n}\r\nint set_resource_options(struct drbd_resource *resource, struct res_opts *res_opts)\r\n{\r\nstruct drbd_connection *connection;\r\ncpumask_var_t new_cpu_mask;\r\nint err;\r\nif (!zalloc_cpumask_var(&new_cpu_mask, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nif (nr_cpu_ids > 1 && res_opts->cpu_mask[0] != 0) {\r\nerr = bitmap_parse(res_opts->cpu_mask, DRBD_CPU_MASK_SIZE,\r\ncpumask_bits(new_cpu_mask), nr_cpu_ids);\r\nif (err == -EOVERFLOW) {\r\ncpumask_var_t tmp_cpu_mask;\r\nif (zalloc_cpumask_var(&tmp_cpu_mask, GFP_KERNEL)) {\r\ncpumask_setall(tmp_cpu_mask);\r\ncpumask_and(new_cpu_mask, new_cpu_mask, tmp_cpu_mask);\r\ndrbd_warn(resource, "Overflow in bitmap_parse(%.12s%s), truncating to %u bits\n",\r\nres_opts->cpu_mask,\r\nstrlen(res_opts->cpu_mask) > 12 ? "..." : "",\r\nnr_cpu_ids);\r\nfree_cpumask_var(tmp_cpu_mask);\r\nerr = 0;\r\n}\r\n}\r\nif (err) {\r\ndrbd_warn(resource, "bitmap_parse() failed with %d\n", err);\r\ngoto fail;\r\n}\r\n}\r\nresource->res_opts = *res_opts;\r\nif (cpumask_empty(new_cpu_mask))\r\ndrbd_calc_cpu_mask(&new_cpu_mask);\r\nif (!cpumask_equal(resource->cpu_mask, new_cpu_mask)) {\r\ncpumask_copy(resource->cpu_mask, new_cpu_mask);\r\nfor_each_connection_rcu(connection, resource) {\r\nconnection->receiver.reset_cpu_mask = 1;\r\nconnection->asender.reset_cpu_mask = 1;\r\nconnection->worker.reset_cpu_mask = 1;\r\n}\r\n}\r\nerr = 0;\r\nfail:\r\nfree_cpumask_var(new_cpu_mask);\r\nreturn err;\r\n}\r\nstruct drbd_resource *drbd_create_resource(const char *name)\r\n{\r\nstruct drbd_resource *resource;\r\nresource = kzalloc(sizeof(struct drbd_resource), GFP_KERNEL);\r\nif (!resource)\r\ngoto fail;\r\nresource->name = kstrdup(name, GFP_KERNEL);\r\nif (!resource->name)\r\ngoto fail_free_resource;\r\nif (!zalloc_cpumask_var(&resource->cpu_mask, GFP_KERNEL))\r\ngoto fail_free_name;\r\nkref_init(&resource->kref);\r\nidr_init(&resource->devices);\r\nINIT_LIST_HEAD(&resource->connections);\r\nresource->write_ordering = WO_bdev_flush;\r\nlist_add_tail_rcu(&resource->resources, &drbd_resources);\r\nmutex_init(&resource->conf_update);\r\nmutex_init(&resource->adm_mutex);\r\nspin_lock_init(&resource->req_lock);\r\ndrbd_debugfs_resource_add(resource);\r\nreturn resource;\r\nfail_free_name:\r\nkfree(resource->name);\r\nfail_free_resource:\r\nkfree(resource);\r\nfail:\r\nreturn NULL;\r\n}\r\nstruct drbd_connection *conn_create(const char *name, struct res_opts *res_opts)\r\n{\r\nstruct drbd_resource *resource;\r\nstruct drbd_connection *connection;\r\nconnection = kzalloc(sizeof(struct drbd_connection), GFP_KERNEL);\r\nif (!connection)\r\nreturn NULL;\r\nif (drbd_alloc_socket(&connection->data))\r\ngoto fail;\r\nif (drbd_alloc_socket(&connection->meta))\r\ngoto fail;\r\nconnection->current_epoch = kzalloc(sizeof(struct drbd_epoch), GFP_KERNEL);\r\nif (!connection->current_epoch)\r\ngoto fail;\r\nINIT_LIST_HEAD(&connection->transfer_log);\r\nINIT_LIST_HEAD(&connection->current_epoch->list);\r\nconnection->epochs = 1;\r\nspin_lock_init(&connection->epoch_lock);\r\nconnection->send.seen_any_write_yet = false;\r\nconnection->send.current_epoch_nr = 0;\r\nconnection->send.current_epoch_writes = 0;\r\nresource = drbd_create_resource(name);\r\nif (!resource)\r\ngoto fail;\r\nconnection->cstate = C_STANDALONE;\r\nmutex_init(&connection->cstate_mutex);\r\ninit_waitqueue_head(&connection->ping_wait);\r\nidr_init(&connection->peer_devices);\r\ndrbd_init_workqueue(&connection->sender_work);\r\nmutex_init(&connection->data.mutex);\r\nmutex_init(&connection->meta.mutex);\r\ndrbd_thread_init(resource, &connection->receiver, drbd_receiver, "receiver");\r\nconnection->receiver.connection = connection;\r\ndrbd_thread_init(resource, &connection->worker, drbd_worker, "worker");\r\nconnection->worker.connection = connection;\r\ndrbd_thread_init(resource, &connection->asender, drbd_asender, "asender");\r\nconnection->asender.connection = connection;\r\nkref_init(&connection->kref);\r\nconnection->resource = resource;\r\nif (set_resource_options(resource, res_opts))\r\ngoto fail_resource;\r\nkref_get(&resource->kref);\r\nlist_add_tail_rcu(&connection->connections, &resource->connections);\r\ndrbd_debugfs_connection_add(connection);\r\nreturn connection;\r\nfail_resource:\r\nlist_del(&resource->resources);\r\ndrbd_free_resource(resource);\r\nfail:\r\nkfree(connection->current_epoch);\r\ndrbd_free_socket(&connection->meta);\r\ndrbd_free_socket(&connection->data);\r\nkfree(connection);\r\nreturn NULL;\r\n}\r\nvoid drbd_destroy_connection(struct kref *kref)\r\n{\r\nstruct drbd_connection *connection = container_of(kref, struct drbd_connection, kref);\r\nstruct drbd_resource *resource = connection->resource;\r\nif (atomic_read(&connection->current_epoch->epoch_size) != 0)\r\ndrbd_err(connection, "epoch_size:%d\n", atomic_read(&connection->current_epoch->epoch_size));\r\nkfree(connection->current_epoch);\r\nidr_destroy(&connection->peer_devices);\r\ndrbd_free_socket(&connection->meta);\r\ndrbd_free_socket(&connection->data);\r\nkfree(connection->int_dig_in);\r\nkfree(connection->int_dig_vv);\r\nmemset(connection, 0xfc, sizeof(*connection));\r\nkfree(connection);\r\nkref_put(&resource->kref, drbd_destroy_resource);\r\n}\r\nstatic int init_submitter(struct drbd_device *device)\r\n{\r\ndevice->submit.wq = alloc_workqueue("drbd%u_submit",\r\nWQ_UNBOUND | WQ_MEM_RECLAIM, 1, device->minor);\r\nif (!device->submit.wq)\r\nreturn -ENOMEM;\r\nINIT_WORK(&device->submit.worker, do_submit);\r\nINIT_LIST_HEAD(&device->submit.writes);\r\nreturn 0;\r\n}\r\nenum drbd_ret_code drbd_create_device(struct drbd_config_context *adm_ctx, unsigned int minor)\r\n{\r\nstruct drbd_resource *resource = adm_ctx->resource;\r\nstruct drbd_connection *connection;\r\nstruct drbd_device *device;\r\nstruct drbd_peer_device *peer_device, *tmp_peer_device;\r\nstruct gendisk *disk;\r\nstruct request_queue *q;\r\nint id;\r\nint vnr = adm_ctx->volume;\r\nenum drbd_ret_code err = ERR_NOMEM;\r\ndevice = minor_to_device(minor);\r\nif (device)\r\nreturn ERR_MINOR_OR_VOLUME_EXISTS;\r\ndevice = kzalloc(sizeof(struct drbd_device), GFP_KERNEL);\r\nif (!device)\r\nreturn ERR_NOMEM;\r\nkref_init(&device->kref);\r\nkref_get(&resource->kref);\r\ndevice->resource = resource;\r\ndevice->minor = minor;\r\ndevice->vnr = vnr;\r\ndrbd_init_set_defaults(device);\r\nq = blk_alloc_queue(GFP_KERNEL);\r\nif (!q)\r\ngoto out_no_q;\r\ndevice->rq_queue = q;\r\nq->queuedata = device;\r\ndisk = alloc_disk(1);\r\nif (!disk)\r\ngoto out_no_disk;\r\ndevice->vdisk = disk;\r\nset_disk_ro(disk, true);\r\ndisk->queue = q;\r\ndisk->major = DRBD_MAJOR;\r\ndisk->first_minor = minor;\r\ndisk->fops = &drbd_ops;\r\nsprintf(disk->disk_name, "drbd%d", minor);\r\ndisk->private_data = device;\r\ndevice->this_bdev = bdget(MKDEV(DRBD_MAJOR, minor));\r\ndevice->this_bdev->bd_contains = device->this_bdev;\r\nq->backing_dev_info.congested_fn = drbd_congested;\r\nq->backing_dev_info.congested_data = device;\r\nblk_queue_make_request(q, drbd_make_request);\r\nblk_queue_flush(q, REQ_FLUSH | REQ_FUA);\r\nblk_queue_max_hw_sectors(q, DRBD_MAX_BIO_SIZE_SAFE >> 8);\r\nblk_queue_bounce_limit(q, BLK_BOUNCE_ANY);\r\nq->queue_lock = &resource->req_lock;\r\ndevice->md_io.page = alloc_page(GFP_KERNEL);\r\nif (!device->md_io.page)\r\ngoto out_no_io_page;\r\nif (drbd_bm_init(device))\r\ngoto out_no_bitmap;\r\ndevice->read_requests = RB_ROOT;\r\ndevice->write_requests = RB_ROOT;\r\nid = idr_alloc(&drbd_devices, device, minor, minor + 1, GFP_KERNEL);\r\nif (id < 0) {\r\nif (id == -ENOSPC)\r\nerr = ERR_MINOR_OR_VOLUME_EXISTS;\r\ngoto out_no_minor_idr;\r\n}\r\nkref_get(&device->kref);\r\nid = idr_alloc(&resource->devices, device, vnr, vnr + 1, GFP_KERNEL);\r\nif (id < 0) {\r\nif (id == -ENOSPC)\r\nerr = ERR_MINOR_OR_VOLUME_EXISTS;\r\ngoto out_idr_remove_minor;\r\n}\r\nkref_get(&device->kref);\r\nINIT_LIST_HEAD(&device->peer_devices);\r\nINIT_LIST_HEAD(&device->pending_bitmap_io);\r\nfor_each_connection(connection, resource) {\r\npeer_device = kzalloc(sizeof(struct drbd_peer_device), GFP_KERNEL);\r\nif (!peer_device)\r\ngoto out_idr_remove_from_resource;\r\npeer_device->connection = connection;\r\npeer_device->device = device;\r\nlist_add(&peer_device->peer_devices, &device->peer_devices);\r\nkref_get(&device->kref);\r\nid = idr_alloc(&connection->peer_devices, peer_device, vnr, vnr + 1, GFP_KERNEL);\r\nif (id < 0) {\r\nif (id == -ENOSPC)\r\nerr = ERR_INVALID_REQUEST;\r\ngoto out_idr_remove_from_resource;\r\n}\r\nkref_get(&connection->kref);\r\n}\r\nif (init_submitter(device)) {\r\nerr = ERR_NOMEM;\r\ngoto out_idr_remove_vol;\r\n}\r\nadd_disk(disk);\r\ndevice->state.conn = first_connection(resource)->cstate;\r\nif (device->state.conn == C_WF_REPORT_PARAMS) {\r\nfor_each_peer_device(peer_device, device)\r\ndrbd_connected(peer_device);\r\n}\r\nfor_each_peer_device(peer_device, device)\r\ndrbd_debugfs_peer_device_add(peer_device);\r\ndrbd_debugfs_device_add(device);\r\nreturn NO_ERROR;\r\nout_idr_remove_vol:\r\nidr_remove(&connection->peer_devices, vnr);\r\nout_idr_remove_from_resource:\r\nfor_each_connection(connection, resource) {\r\npeer_device = idr_find(&connection->peer_devices, vnr);\r\nif (peer_device) {\r\nidr_remove(&connection->peer_devices, vnr);\r\nkref_put(&connection->kref, drbd_destroy_connection);\r\n}\r\n}\r\nfor_each_peer_device_safe(peer_device, tmp_peer_device, device) {\r\nlist_del(&peer_device->peer_devices);\r\nkfree(peer_device);\r\n}\r\nidr_remove(&resource->devices, vnr);\r\nout_idr_remove_minor:\r\nidr_remove(&drbd_devices, minor);\r\nsynchronize_rcu();\r\nout_no_minor_idr:\r\ndrbd_bm_cleanup(device);\r\nout_no_bitmap:\r\n__free_page(device->md_io.page);\r\nout_no_io_page:\r\nput_disk(disk);\r\nout_no_disk:\r\nblk_cleanup_queue(q);\r\nout_no_q:\r\nkref_put(&resource->kref, drbd_destroy_resource);\r\nkfree(device);\r\nreturn err;\r\n}\r\nvoid drbd_delete_device(struct drbd_device *device)\r\n{\r\nstruct drbd_resource *resource = device->resource;\r\nstruct drbd_connection *connection;\r\nstruct drbd_peer_device *peer_device;\r\nint refs = 3;\r\nfor_each_peer_device(peer_device, device)\r\ndrbd_debugfs_peer_device_cleanup(peer_device);\r\ndrbd_debugfs_device_cleanup(device);\r\nfor_each_connection(connection, resource) {\r\nidr_remove(&connection->peer_devices, device->vnr);\r\nrefs++;\r\n}\r\nidr_remove(&resource->devices, device->vnr);\r\nidr_remove(&drbd_devices, device_to_minor(device));\r\ndel_gendisk(device->vdisk);\r\nsynchronize_rcu();\r\nkref_sub(&device->kref, refs, drbd_destroy_device);\r\n}\r\nstatic int __init drbd_init(void)\r\n{\r\nint err;\r\nif (minor_count < DRBD_MINOR_COUNT_MIN || minor_count > DRBD_MINOR_COUNT_MAX) {\r\npr_err("invalid minor_count (%d)\n", minor_count);\r\n#ifdef MODULE\r\nreturn -EINVAL;\r\n#else\r\nminor_count = DRBD_MINOR_COUNT_DEF;\r\n#endif\r\n}\r\nerr = register_blkdev(DRBD_MAJOR, "drbd");\r\nif (err) {\r\npr_err("unable to register block device major %d\n",\r\nDRBD_MAJOR);\r\nreturn err;\r\n}\r\ninit_waitqueue_head(&drbd_pp_wait);\r\ndrbd_proc = NULL;\r\nidr_init(&drbd_devices);\r\nrwlock_init(&global_state_lock);\r\nINIT_LIST_HEAD(&drbd_resources);\r\nerr = drbd_genl_register();\r\nif (err) {\r\npr_err("unable to register generic netlink family\n");\r\ngoto fail;\r\n}\r\nerr = drbd_create_mempools();\r\nif (err)\r\ngoto fail;\r\nerr = -ENOMEM;\r\ndrbd_proc = proc_create_data("drbd", S_IFREG | S_IRUGO , NULL, &drbd_proc_fops, NULL);\r\nif (!drbd_proc) {\r\npr_err("unable to register proc file\n");\r\ngoto fail;\r\n}\r\nretry.wq = create_singlethread_workqueue("drbd-reissue");\r\nif (!retry.wq) {\r\npr_err("unable to create retry workqueue\n");\r\ngoto fail;\r\n}\r\nINIT_WORK(&retry.worker, do_retry);\r\nspin_lock_init(&retry.lock);\r\nINIT_LIST_HEAD(&retry.writes);\r\nif (drbd_debugfs_init())\r\npr_notice("failed to initialize debugfs -- will not be available\n");\r\npr_info("initialized. "\r\n"Version: " REL_VERSION " (api:%d/proto:%d-%d)\n",\r\nAPI_VERSION, PRO_VERSION_MIN, PRO_VERSION_MAX);\r\npr_info("%s\n", drbd_buildtag());\r\npr_info("registered as block device major %d\n", DRBD_MAJOR);\r\nreturn 0;\r\nfail:\r\ndrbd_cleanup();\r\nif (err == -ENOMEM)\r\npr_err("ran out of memory\n");\r\nelse\r\npr_err("initialization failure\n");\r\nreturn err;\r\n}\r\nvoid drbd_free_ldev(struct drbd_backing_dev *ldev)\r\n{\r\nif (ldev == NULL)\r\nreturn;\r\nblkdev_put(ldev->backing_bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nblkdev_put(ldev->md_bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nkfree(ldev->disk_conf);\r\nkfree(ldev);\r\n}\r\nstatic void drbd_free_one_sock(struct drbd_socket *ds)\r\n{\r\nstruct socket *s;\r\nmutex_lock(&ds->mutex);\r\ns = ds->socket;\r\nds->socket = NULL;\r\nmutex_unlock(&ds->mutex);\r\nif (s) {\r\nsynchronize_rcu();\r\nkernel_sock_shutdown(s, SHUT_RDWR);\r\nsock_release(s);\r\n}\r\n}\r\nvoid drbd_free_sock(struct drbd_connection *connection)\r\n{\r\nif (connection->data.socket)\r\ndrbd_free_one_sock(&connection->data);\r\nif (connection->meta.socket)\r\ndrbd_free_one_sock(&connection->meta);\r\n}\r\nvoid conn_md_sync(struct drbd_connection *connection)\r\n{\r\nstruct drbd_peer_device *peer_device;\r\nint vnr;\r\nrcu_read_lock();\r\nidr_for_each_entry(&connection->peer_devices, peer_device, vnr) {\r\nstruct drbd_device *device = peer_device->device;\r\nkref_get(&device->kref);\r\nrcu_read_unlock();\r\ndrbd_md_sync(device);\r\nkref_put(&device->kref, drbd_destroy_device);\r\nrcu_read_lock();\r\n}\r\nrcu_read_unlock();\r\n}\r\nvoid drbd_md_write(struct drbd_device *device, void *b)\r\n{\r\nstruct meta_data_on_disk *buffer = b;\r\nsector_t sector;\r\nint i;\r\nmemset(buffer, 0, sizeof(*buffer));\r\nbuffer->la_size_sect = cpu_to_be64(drbd_get_capacity(device->this_bdev));\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\nbuffer->uuid[i] = cpu_to_be64(device->ldev->md.uuid[i]);\r\nbuffer->flags = cpu_to_be32(device->ldev->md.flags);\r\nbuffer->magic = cpu_to_be32(DRBD_MD_MAGIC_84_UNCLEAN);\r\nbuffer->md_size_sect = cpu_to_be32(device->ldev->md.md_size_sect);\r\nbuffer->al_offset = cpu_to_be32(device->ldev->md.al_offset);\r\nbuffer->al_nr_extents = cpu_to_be32(device->act_log->nr_elements);\r\nbuffer->bm_bytes_per_bit = cpu_to_be32(BM_BLOCK_SIZE);\r\nbuffer->device_uuid = cpu_to_be64(device->ldev->md.device_uuid);\r\nbuffer->bm_offset = cpu_to_be32(device->ldev->md.bm_offset);\r\nbuffer->la_peer_max_bio_size = cpu_to_be32(device->peer_max_bio_size);\r\nbuffer->al_stripes = cpu_to_be32(device->ldev->md.al_stripes);\r\nbuffer->al_stripe_size_4k = cpu_to_be32(device->ldev->md.al_stripe_size_4k);\r\nD_ASSERT(device, drbd_md_ss(device->ldev) == device->ldev->md.md_offset);\r\nsector = device->ldev->md.md_offset;\r\nif (drbd_md_sync_page_io(device, device->ldev, sector, WRITE)) {\r\ndrbd_err(device, "meta data update failed!\n");\r\ndrbd_chk_io_error(device, 1, DRBD_META_IO_ERROR);\r\n}\r\n}\r\nvoid drbd_md_sync(struct drbd_device *device)\r\n{\r\nstruct meta_data_on_disk *buffer;\r\nBUILD_BUG_ON(UI_SIZE != 4);\r\nBUILD_BUG_ON(sizeof(struct meta_data_on_disk) != 4096);\r\ndel_timer(&device->md_sync_timer);\r\nif (!test_and_clear_bit(MD_DIRTY, &device->flags))\r\nreturn;\r\nif (!get_ldev_if_state(device, D_FAILED))\r\nreturn;\r\nbuffer = drbd_md_get_buffer(device, __func__);\r\nif (!buffer)\r\ngoto out;\r\ndrbd_md_write(device, buffer);\r\ndevice->ldev->md.la_size_sect = drbd_get_capacity(device->this_bdev);\r\ndrbd_md_put_buffer(device);\r\nout:\r\nput_ldev(device);\r\n}\r\nstatic int check_activity_log_stripe_size(struct drbd_device *device,\r\nstruct meta_data_on_disk *on_disk,\r\nstruct drbd_md *in_core)\r\n{\r\nu32 al_stripes = be32_to_cpu(on_disk->al_stripes);\r\nu32 al_stripe_size_4k = be32_to_cpu(on_disk->al_stripe_size_4k);\r\nu64 al_size_4k;\r\nif (al_stripes == 0 && al_stripe_size_4k == 0) {\r\nal_stripes = 1;\r\nal_stripe_size_4k = MD_32kB_SECT/8;\r\n}\r\nif (al_stripes == 0 || al_stripe_size_4k == 0)\r\ngoto err;\r\nal_size_4k = (u64)al_stripes * al_stripe_size_4k;\r\nif (al_size_4k > (16 * 1024 * 1024/4))\r\ngoto err;\r\nif (al_size_4k < MD_32kB_SECT/8)\r\ngoto err;\r\nin_core->al_stripe_size_4k = al_stripe_size_4k;\r\nin_core->al_stripes = al_stripes;\r\nin_core->al_size_4k = al_size_4k;\r\nreturn 0;\r\nerr:\r\ndrbd_err(device, "invalid activity log striping: al_stripes=%u, al_stripe_size_4k=%u\n",\r\nal_stripes, al_stripe_size_4k);\r\nreturn -EINVAL;\r\n}\r\nstatic int check_offsets_and_sizes(struct drbd_device *device, struct drbd_backing_dev *bdev)\r\n{\r\nsector_t capacity = drbd_get_capacity(bdev->md_bdev);\r\nstruct drbd_md *in_core = &bdev->md;\r\ns32 on_disk_al_sect;\r\ns32 on_disk_bm_sect;\r\nif (in_core->al_offset < 0) {\r\nif (in_core->bm_offset > in_core->al_offset)\r\ngoto err;\r\non_disk_al_sect = -in_core->al_offset;\r\non_disk_bm_sect = in_core->al_offset - in_core->bm_offset;\r\n} else {\r\nif (in_core->al_offset != MD_4kB_SECT)\r\ngoto err;\r\nif (in_core->bm_offset < in_core->al_offset + in_core->al_size_4k * MD_4kB_SECT)\r\ngoto err;\r\non_disk_al_sect = in_core->bm_offset - MD_4kB_SECT;\r\non_disk_bm_sect = in_core->md_size_sect - in_core->bm_offset;\r\n}\r\nif (in_core->meta_dev_idx >= 0) {\r\nif (in_core->md_size_sect != MD_128MB_SECT\r\n|| in_core->al_offset != MD_4kB_SECT\r\n|| in_core->bm_offset != MD_4kB_SECT + MD_32kB_SECT\r\n|| in_core->al_stripes != 1\r\n|| in_core->al_stripe_size_4k != MD_32kB_SECT/8)\r\ngoto err;\r\n}\r\nif (capacity < in_core->md_size_sect)\r\ngoto err;\r\nif (capacity - in_core->md_size_sect < drbd_md_first_sector(bdev))\r\ngoto err;\r\nif ((on_disk_al_sect & 7) || (on_disk_al_sect < MD_32kB_SECT))\r\ngoto err;\r\nif (on_disk_al_sect != in_core->al_size_4k * MD_4kB_SECT)\r\ngoto err;\r\nif (in_core->bm_offset & 7)\r\ngoto err;\r\nif (on_disk_bm_sect < (in_core->la_size_sect+7)/MD_4kB_SECT/8/512)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\ndrbd_err(device, "meta data offsets don't make sense: idx=%d "\r\n"al_s=%u, al_sz4k=%u, al_offset=%d, bm_offset=%d, "\r\n"md_size_sect=%u, la_size=%llu, md_capacity=%llu\n",\r\nin_core->meta_dev_idx,\r\nin_core->al_stripes, in_core->al_stripe_size_4k,\r\nin_core->al_offset, in_core->bm_offset, in_core->md_size_sect,\r\n(unsigned long long)in_core->la_size_sect,\r\n(unsigned long long)capacity);\r\nreturn -EINVAL;\r\n}\r\nint drbd_md_read(struct drbd_device *device, struct drbd_backing_dev *bdev)\r\n{\r\nstruct meta_data_on_disk *buffer;\r\nu32 magic, flags;\r\nint i, rv = NO_ERROR;\r\nif (device->state.disk != D_DISKLESS)\r\nreturn ERR_DISK_CONFIGURED;\r\nbuffer = drbd_md_get_buffer(device, __func__);\r\nif (!buffer)\r\nreturn ERR_NOMEM;\r\nbdev->md.meta_dev_idx = bdev->disk_conf->meta_dev_idx;\r\nbdev->md.md_offset = drbd_md_ss(bdev);\r\nif (drbd_md_sync_page_io(device, bdev, bdev->md.md_offset, READ)) {\r\ndrbd_err(device, "Error while reading metadata.\n");\r\nrv = ERR_IO_MD_DISK;\r\ngoto err;\r\n}\r\nmagic = be32_to_cpu(buffer->magic);\r\nflags = be32_to_cpu(buffer->flags);\r\nif (magic == DRBD_MD_MAGIC_84_UNCLEAN ||\r\n(magic == DRBD_MD_MAGIC_08 && !(flags & MDF_AL_CLEAN))) {\r\ndrbd_err(device, "Found unclean meta data. Did you \"drbdadm apply-al\"?\n");\r\nrv = ERR_MD_UNCLEAN;\r\ngoto err;\r\n}\r\nrv = ERR_MD_INVALID;\r\nif (magic != DRBD_MD_MAGIC_08) {\r\nif (magic == DRBD_MD_MAGIC_07)\r\ndrbd_err(device, "Found old (0.7) meta data magic. Did you \"drbdadm create-md\"?\n");\r\nelse\r\ndrbd_err(device, "Meta data magic not found. Did you \"drbdadm create-md\"?\n");\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->bm_bytes_per_bit) != BM_BLOCK_SIZE) {\r\ndrbd_err(device, "unexpected bm_bytes_per_bit: %u (expected %u)\n",\r\nbe32_to_cpu(buffer->bm_bytes_per_bit), BM_BLOCK_SIZE);\r\ngoto err;\r\n}\r\nbdev->md.la_size_sect = be64_to_cpu(buffer->la_size_sect);\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\nbdev->md.uuid[i] = be64_to_cpu(buffer->uuid[i]);\r\nbdev->md.flags = be32_to_cpu(buffer->flags);\r\nbdev->md.device_uuid = be64_to_cpu(buffer->device_uuid);\r\nbdev->md.md_size_sect = be32_to_cpu(buffer->md_size_sect);\r\nbdev->md.al_offset = be32_to_cpu(buffer->al_offset);\r\nbdev->md.bm_offset = be32_to_cpu(buffer->bm_offset);\r\nif (check_activity_log_stripe_size(device, buffer, &bdev->md))\r\ngoto err;\r\nif (check_offsets_and_sizes(device, bdev))\r\ngoto err;\r\nif (be32_to_cpu(buffer->bm_offset) != bdev->md.bm_offset) {\r\ndrbd_err(device, "unexpected bm_offset: %d (expected %d)\n",\r\nbe32_to_cpu(buffer->bm_offset), bdev->md.bm_offset);\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->md_size_sect) != bdev->md.md_size_sect) {\r\ndrbd_err(device, "unexpected md_size: %u (expected %u)\n",\r\nbe32_to_cpu(buffer->md_size_sect), bdev->md.md_size_sect);\r\ngoto err;\r\n}\r\nrv = NO_ERROR;\r\nspin_lock_irq(&device->resource->req_lock);\r\nif (device->state.conn < C_CONNECTED) {\r\nunsigned int peer;\r\npeer = be32_to_cpu(buffer->la_peer_max_bio_size);\r\npeer = max(peer, DRBD_MAX_BIO_SIZE_SAFE);\r\ndevice->peer_max_bio_size = peer;\r\n}\r\nspin_unlock_irq(&device->resource->req_lock);\r\nerr:\r\ndrbd_md_put_buffer(device);\r\nreturn rv;\r\n}\r\nvoid drbd_md_mark_dirty_(struct drbd_device *device, unsigned int line, const char *func)\r\n{\r\nif (!test_and_set_bit(MD_DIRTY, &device->flags)) {\r\nmod_timer(&device->md_sync_timer, jiffies + HZ);\r\ndevice->last_md_mark_dirty.line = line;\r\ndevice->last_md_mark_dirty.func = func;\r\n}\r\n}\r\nvoid drbd_md_mark_dirty(struct drbd_device *device)\r\n{\r\nif (!test_and_set_bit(MD_DIRTY, &device->flags))\r\nmod_timer(&device->md_sync_timer, jiffies + 5*HZ);\r\n}\r\nvoid drbd_uuid_move_history(struct drbd_device *device) __must_hold(local)\r\n{\r\nint i;\r\nfor (i = UI_HISTORY_START; i < UI_HISTORY_END; i++)\r\ndevice->ldev->md.uuid[i+1] = device->ldev->md.uuid[i];\r\n}\r\nvoid __drbd_uuid_set(struct drbd_device *device, int idx, u64 val) __must_hold(local)\r\n{\r\nif (idx == UI_CURRENT) {\r\nif (device->state.role == R_PRIMARY)\r\nval |= 1;\r\nelse\r\nval &= ~((u64)1);\r\ndrbd_set_ed_uuid(device, val);\r\n}\r\ndevice->ldev->md.uuid[idx] = val;\r\ndrbd_md_mark_dirty(device);\r\n}\r\nvoid _drbd_uuid_set(struct drbd_device *device, int idx, u64 val) __must_hold(local)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&device->ldev->md.uuid_lock, flags);\r\n__drbd_uuid_set(device, idx, val);\r\nspin_unlock_irqrestore(&device->ldev->md.uuid_lock, flags);\r\n}\r\nvoid drbd_uuid_set(struct drbd_device *device, int idx, u64 val) __must_hold(local)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&device->ldev->md.uuid_lock, flags);\r\nif (device->ldev->md.uuid[idx]) {\r\ndrbd_uuid_move_history(device);\r\ndevice->ldev->md.uuid[UI_HISTORY_START] = device->ldev->md.uuid[idx];\r\n}\r\n__drbd_uuid_set(device, idx, val);\r\nspin_unlock_irqrestore(&device->ldev->md.uuid_lock, flags);\r\n}\r\nvoid drbd_uuid_new_current(struct drbd_device *device) __must_hold(local)\r\n{\r\nu64 val;\r\nunsigned long long bm_uuid;\r\nget_random_bytes(&val, sizeof(u64));\r\nspin_lock_irq(&device->ldev->md.uuid_lock);\r\nbm_uuid = device->ldev->md.uuid[UI_BITMAP];\r\nif (bm_uuid)\r\ndrbd_warn(device, "bm UUID was already set: %llX\n", bm_uuid);\r\ndevice->ldev->md.uuid[UI_BITMAP] = device->ldev->md.uuid[UI_CURRENT];\r\n__drbd_uuid_set(device, UI_CURRENT, val);\r\nspin_unlock_irq(&device->ldev->md.uuid_lock);\r\ndrbd_print_uuids(device, "new current UUID");\r\ndrbd_md_sync(device);\r\n}\r\nvoid drbd_uuid_set_bm(struct drbd_device *device, u64 val) __must_hold(local)\r\n{\r\nunsigned long flags;\r\nif (device->ldev->md.uuid[UI_BITMAP] == 0 && val == 0)\r\nreturn;\r\nspin_lock_irqsave(&device->ldev->md.uuid_lock, flags);\r\nif (val == 0) {\r\ndrbd_uuid_move_history(device);\r\ndevice->ldev->md.uuid[UI_HISTORY_START] = device->ldev->md.uuid[UI_BITMAP];\r\ndevice->ldev->md.uuid[UI_BITMAP] = 0;\r\n} else {\r\nunsigned long long bm_uuid = device->ldev->md.uuid[UI_BITMAP];\r\nif (bm_uuid)\r\ndrbd_warn(device, "bm UUID was already set: %llX\n", bm_uuid);\r\ndevice->ldev->md.uuid[UI_BITMAP] = val & ~((u64)1);\r\n}\r\nspin_unlock_irqrestore(&device->ldev->md.uuid_lock, flags);\r\ndrbd_md_mark_dirty(device);\r\n}\r\nint drbd_bmio_set_n_write(struct drbd_device *device) __must_hold(local)\r\n{\r\nint rv = -EIO;\r\ndrbd_md_set_flag(device, MDF_FULL_SYNC);\r\ndrbd_md_sync(device);\r\ndrbd_bm_set_all(device);\r\nrv = drbd_bm_write(device);\r\nif (!rv) {\r\ndrbd_md_clear_flag(device, MDF_FULL_SYNC);\r\ndrbd_md_sync(device);\r\n}\r\nreturn rv;\r\n}\r\nint drbd_bmio_clear_n_write(struct drbd_device *device) __must_hold(local)\r\n{\r\ndrbd_resume_al(device);\r\ndrbd_bm_clear_all(device);\r\nreturn drbd_bm_write(device);\r\n}\r\nstatic int w_bitmap_io(struct drbd_work *w, int unused)\r\n{\r\nstruct drbd_device *device =\r\ncontainer_of(w, struct drbd_device, bm_io_work.w);\r\nstruct bm_io_work *work = &device->bm_io_work;\r\nint rv = -EIO;\r\nD_ASSERT(device, atomic_read(&device->ap_bio_cnt) == 0);\r\nif (get_ldev(device)) {\r\ndrbd_bm_lock(device, work->why, work->flags);\r\nrv = work->io_fn(device);\r\ndrbd_bm_unlock(device);\r\nput_ldev(device);\r\n}\r\nclear_bit_unlock(BITMAP_IO, &device->flags);\r\nwake_up(&device->misc_wait);\r\nif (work->done)\r\nwork->done(device, rv);\r\nclear_bit(BITMAP_IO_QUEUED, &device->flags);\r\nwork->why = NULL;\r\nwork->flags = 0;\r\nreturn 0;\r\n}\r\nvoid drbd_queue_bitmap_io(struct drbd_device *device,\r\nint (*io_fn)(struct drbd_device *),\r\nvoid (*done)(struct drbd_device *, int),\r\nchar *why, enum bm_flag flags)\r\n{\r\nD_ASSERT(device, current == first_peer_device(device)->connection->worker.task);\r\nD_ASSERT(device, !test_bit(BITMAP_IO_QUEUED, &device->flags));\r\nD_ASSERT(device, !test_bit(BITMAP_IO, &device->flags));\r\nD_ASSERT(device, list_empty(&device->bm_io_work.w.list));\r\nif (device->bm_io_work.why)\r\ndrbd_err(device, "FIXME going to queue '%s' but '%s' still pending?\n",\r\nwhy, device->bm_io_work.why);\r\ndevice->bm_io_work.io_fn = io_fn;\r\ndevice->bm_io_work.done = done;\r\ndevice->bm_io_work.why = why;\r\ndevice->bm_io_work.flags = flags;\r\nspin_lock_irq(&device->resource->req_lock);\r\nset_bit(BITMAP_IO, &device->flags);\r\nif (atomic_read(&device->ap_bio_cnt) == 0) {\r\nif (!test_and_set_bit(BITMAP_IO_QUEUED, &device->flags))\r\ndrbd_queue_work(&first_peer_device(device)->connection->sender_work,\r\n&device->bm_io_work.w);\r\n}\r\nspin_unlock_irq(&device->resource->req_lock);\r\n}\r\nint drbd_bitmap_io(struct drbd_device *device, int (*io_fn)(struct drbd_device *),\r\nchar *why, enum bm_flag flags)\r\n{\r\nint rv;\r\nD_ASSERT(device, current != first_peer_device(device)->connection->worker.task);\r\nif ((flags & BM_LOCKED_SET_ALLOWED) == 0)\r\ndrbd_suspend_io(device);\r\ndrbd_bm_lock(device, why, flags);\r\nrv = io_fn(device);\r\ndrbd_bm_unlock(device);\r\nif ((flags & BM_LOCKED_SET_ALLOWED) == 0)\r\ndrbd_resume_io(device);\r\nreturn rv;\r\n}\r\nvoid drbd_md_set_flag(struct drbd_device *device, int flag) __must_hold(local)\r\n{\r\nif ((device->ldev->md.flags & flag) != flag) {\r\ndrbd_md_mark_dirty(device);\r\ndevice->ldev->md.flags |= flag;\r\n}\r\n}\r\nvoid drbd_md_clear_flag(struct drbd_device *device, int flag) __must_hold(local)\r\n{\r\nif ((device->ldev->md.flags & flag) != 0) {\r\ndrbd_md_mark_dirty(device);\r\ndevice->ldev->md.flags &= ~flag;\r\n}\r\n}\r\nint drbd_md_test_flag(struct drbd_backing_dev *bdev, int flag)\r\n{\r\nreturn (bdev->md.flags & flag) != 0;\r\n}\r\nstatic void md_sync_timer_fn(unsigned long data)\r\n{\r\nstruct drbd_device *device = (struct drbd_device *) data;\r\ndrbd_device_post_work(device, MD_SYNC);\r\n}\r\nconst char *cmdname(enum drbd_packet cmd)\r\n{\r\nstatic const char *cmdnames[] = {\r\n[P_DATA] = "Data",\r\n[P_DATA_REPLY] = "DataReply",\r\n[P_RS_DATA_REPLY] = "RSDataReply",\r\n[P_BARRIER] = "Barrier",\r\n[P_BITMAP] = "ReportBitMap",\r\n[P_BECOME_SYNC_TARGET] = "BecomeSyncTarget",\r\n[P_BECOME_SYNC_SOURCE] = "BecomeSyncSource",\r\n[P_UNPLUG_REMOTE] = "UnplugRemote",\r\n[P_DATA_REQUEST] = "DataRequest",\r\n[P_RS_DATA_REQUEST] = "RSDataRequest",\r\n[P_SYNC_PARAM] = "SyncParam",\r\n[P_SYNC_PARAM89] = "SyncParam89",\r\n[P_PROTOCOL] = "ReportProtocol",\r\n[P_UUIDS] = "ReportUUIDs",\r\n[P_SIZES] = "ReportSizes",\r\n[P_STATE] = "ReportState",\r\n[P_SYNC_UUID] = "ReportSyncUUID",\r\n[P_AUTH_CHALLENGE] = "AuthChallenge",\r\n[P_AUTH_RESPONSE] = "AuthResponse",\r\n[P_PING] = "Ping",\r\n[P_PING_ACK] = "PingAck",\r\n[P_RECV_ACK] = "RecvAck",\r\n[P_WRITE_ACK] = "WriteAck",\r\n[P_RS_WRITE_ACK] = "RSWriteAck",\r\n[P_SUPERSEDED] = "Superseded",\r\n[P_NEG_ACK] = "NegAck",\r\n[P_NEG_DREPLY] = "NegDReply",\r\n[P_NEG_RS_DREPLY] = "NegRSDReply",\r\n[P_BARRIER_ACK] = "BarrierAck",\r\n[P_STATE_CHG_REQ] = "StateChgRequest",\r\n[P_STATE_CHG_REPLY] = "StateChgReply",\r\n[P_OV_REQUEST] = "OVRequest",\r\n[P_OV_REPLY] = "OVReply",\r\n[P_OV_RESULT] = "OVResult",\r\n[P_CSUM_RS_REQUEST] = "CsumRSRequest",\r\n[P_RS_IS_IN_SYNC] = "CsumRSIsInSync",\r\n[P_COMPRESSED_BITMAP] = "CBitmap",\r\n[P_DELAY_PROBE] = "DelayProbe",\r\n[P_OUT_OF_SYNC] = "OutOfSync",\r\n[P_RETRY_WRITE] = "RetryWrite",\r\n[P_RS_CANCEL] = "RSCancel",\r\n[P_CONN_ST_CHG_REQ] = "conn_st_chg_req",\r\n[P_CONN_ST_CHG_REPLY] = "conn_st_chg_reply",\r\n[P_RETRY_WRITE] = "retry_write",\r\n[P_PROTOCOL_UPDATE] = "protocol_update",\r\n};\r\nif (cmd == P_INITIAL_META)\r\nreturn "InitialMeta";\r\nif (cmd == P_INITIAL_DATA)\r\nreturn "InitialData";\r\nif (cmd == P_CONNECTION_FEATURES)\r\nreturn "ConnectionFeatures";\r\nif (cmd >= ARRAY_SIZE(cmdnames))\r\nreturn "Unknown";\r\nreturn cmdnames[cmd];\r\n}\r\nint drbd_wait_misc(struct drbd_device *device, struct drbd_interval *i)\r\n{\r\nstruct net_conf *nc;\r\nDEFINE_WAIT(wait);\r\nlong timeout;\r\nrcu_read_lock();\r\nnc = rcu_dereference(first_peer_device(device)->connection->net_conf);\r\nif (!nc) {\r\nrcu_read_unlock();\r\nreturn -ETIMEDOUT;\r\n}\r\ntimeout = nc->ko_count ? nc->timeout * HZ / 10 * nc->ko_count : MAX_SCHEDULE_TIMEOUT;\r\nrcu_read_unlock();\r\ni->waiting = true;\r\nprepare_to_wait(&device->misc_wait, &wait, TASK_INTERRUPTIBLE);\r\nspin_unlock_irq(&device->resource->req_lock);\r\ntimeout = schedule_timeout(timeout);\r\nfinish_wait(&device->misc_wait, &wait);\r\nspin_lock_irq(&device->resource->req_lock);\r\nif (!timeout || device->state.conn < C_CONNECTED)\r\nreturn -ETIMEDOUT;\r\nif (signal_pending(current))\r\nreturn -ERESTARTSYS;\r\nreturn 0;\r\n}\r\nstatic unsigned long\r\n_drbd_fault_random(struct fault_random_state *rsp)\r\n{\r\nlong refresh;\r\nif (!rsp->count--) {\r\nget_random_bytes(&refresh, sizeof(refresh));\r\nrsp->state += refresh;\r\nrsp->count = FAULT_RANDOM_REFRESH;\r\n}\r\nrsp->state = rsp->state * FAULT_RANDOM_MULT + FAULT_RANDOM_ADD;\r\nreturn swahw32(rsp->state);\r\n}\r\nstatic char *\r\n_drbd_fault_str(unsigned int type) {\r\nstatic char *_faults[] = {\r\n[DRBD_FAULT_MD_WR] = "Meta-data write",\r\n[DRBD_FAULT_MD_RD] = "Meta-data read",\r\n[DRBD_FAULT_RS_WR] = "Resync write",\r\n[DRBD_FAULT_RS_RD] = "Resync read",\r\n[DRBD_FAULT_DT_WR] = "Data write",\r\n[DRBD_FAULT_DT_RD] = "Data read",\r\n[DRBD_FAULT_DT_RA] = "Data read ahead",\r\n[DRBD_FAULT_BM_ALLOC] = "BM allocation",\r\n[DRBD_FAULT_AL_EE] = "EE allocation",\r\n[DRBD_FAULT_RECEIVE] = "receive data corruption",\r\n};\r\nreturn (type < DRBD_FAULT_MAX) ? _faults[type] : "**Unknown**";\r\n}\r\nunsigned int\r\n_drbd_insert_fault(struct drbd_device *device, unsigned int type)\r\n{\r\nstatic struct fault_random_state rrs = {0, 0};\r\nunsigned int ret = (\r\n(fault_devs == 0 ||\r\n((1 << device_to_minor(device)) & fault_devs) != 0) &&\r\n(((_drbd_fault_random(&rrs) % 100) + 1) <= fault_rate));\r\nif (ret) {\r\nfault_count++;\r\nif (__ratelimit(&drbd_ratelimit_state))\r\ndrbd_warn(device, "***Simulating %s failure\n",\r\n_drbd_fault_str(type));\r\n}\r\nreturn ret;\r\n}\r\nconst char *drbd_buildtag(void)\r\n{\r\nstatic char buildtag[38] = "\0uilt-in";\r\nif (buildtag[0] == 0) {\r\n#ifdef MODULE\r\nsprintf(buildtag, "srcversion: %-24s", THIS_MODULE->srcversion);\r\n#else\r\nbuildtag[0] = 'b';\r\n#endif\r\n}\r\nreturn buildtag;\r\n}
