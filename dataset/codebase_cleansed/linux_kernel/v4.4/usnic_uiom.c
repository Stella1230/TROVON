static void usnic_uiom_reg_account(struct work_struct *work)\r\n{\r\nstruct usnic_uiom_reg *umem = container_of(work,\r\nstruct usnic_uiom_reg, work);\r\ndown_write(&umem->mm->mmap_sem);\r\numem->mm->locked_vm -= umem->diff;\r\nup_write(&umem->mm->mmap_sem);\r\nmmput(umem->mm);\r\nkfree(umem);\r\n}\r\nstatic int usnic_uiom_dma_fault(struct iommu_domain *domain,\r\nstruct device *dev,\r\nunsigned long iova, int flags,\r\nvoid *token)\r\n{\r\nusnic_err("Device %s iommu fault domain 0x%pK va 0x%lx flags 0x%x\n",\r\ndev_name(dev),\r\ndomain, iova, flags);\r\nreturn -ENOSYS;\r\n}\r\nstatic void usnic_uiom_put_pages(struct list_head *chunk_list, int dirty)\r\n{\r\nstruct usnic_uiom_chunk *chunk, *tmp;\r\nstruct page *page;\r\nstruct scatterlist *sg;\r\nint i;\r\ndma_addr_t pa;\r\nlist_for_each_entry_safe(chunk, tmp, chunk_list, list) {\r\nfor_each_sg(chunk->page_list, sg, chunk->nents, i) {\r\npage = sg_page(sg);\r\npa = sg_phys(sg);\r\nif (dirty)\r\nset_page_dirty_lock(page);\r\nput_page(page);\r\nusnic_dbg("pa: %pa\n", &pa);\r\n}\r\nkfree(chunk);\r\n}\r\n}\r\nstatic int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,\r\nint dmasync, struct list_head *chunk_list)\r\n{\r\nstruct page **page_list;\r\nstruct scatterlist *sg;\r\nstruct usnic_uiom_chunk *chunk;\r\nunsigned long locked;\r\nunsigned long lock_limit;\r\nunsigned long cur_base;\r\nunsigned long npages;\r\nint ret;\r\nint off;\r\nint i;\r\nint flags;\r\ndma_addr_t pa;\r\nDEFINE_DMA_ATTRS(attrs);\r\nif (dmasync)\r\ndma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);\r\nif (!can_do_mlock())\r\nreturn -EPERM;\r\nINIT_LIST_HEAD(chunk_list);\r\npage_list = (struct page **) __get_free_page(GFP_KERNEL);\r\nif (!page_list)\r\nreturn -ENOMEM;\r\nnpages = PAGE_ALIGN(size + (addr & ~PAGE_MASK)) >> PAGE_SHIFT;\r\ndown_write(&current->mm->mmap_sem);\r\nlocked = npages + current->mm->locked_vm;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nflags = IOMMU_READ | IOMMU_CACHE;\r\nflags |= (writable) ? IOMMU_WRITE : 0;\r\ncur_base = addr & PAGE_MASK;\r\nret = 0;\r\nwhile (npages) {\r\nret = get_user_pages(current, current->mm, cur_base,\r\nmin_t(unsigned long, npages,\r\nPAGE_SIZE / sizeof(struct page *)),\r\n1, !writable, page_list, NULL);\r\nif (ret < 0)\r\ngoto out;\r\nnpages -= ret;\r\noff = 0;\r\nwhile (ret) {\r\nchunk = kmalloc(sizeof(*chunk) +\r\nsizeof(struct scatterlist) *\r\nmin_t(int, ret, USNIC_UIOM_PAGE_CHUNK),\r\nGFP_KERNEL);\r\nif (!chunk) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nchunk->nents = min_t(int, ret, USNIC_UIOM_PAGE_CHUNK);\r\nsg_init_table(chunk->page_list, chunk->nents);\r\nfor_each_sg(chunk->page_list, sg, chunk->nents, i) {\r\nsg_set_page(sg, page_list[i + off],\r\nPAGE_SIZE, 0);\r\npa = sg_phys(sg);\r\nusnic_dbg("va: 0x%lx pa: %pa\n",\r\ncur_base + i*PAGE_SIZE, &pa);\r\n}\r\ncur_base += chunk->nents * PAGE_SIZE;\r\nret -= chunk->nents;\r\noff += chunk->nents;\r\nlist_add_tail(&chunk->list, chunk_list);\r\n}\r\nret = 0;\r\n}\r\nout:\r\nif (ret < 0)\r\nusnic_uiom_put_pages(chunk_list, 0);\r\nelse\r\ncurrent->mm->locked_vm = locked;\r\nup_write(&current->mm->mmap_sem);\r\nfree_page((unsigned long) page_list);\r\nreturn ret;\r\n}\r\nstatic void usnic_uiom_unmap_sorted_intervals(struct list_head *intervals,\r\nstruct usnic_uiom_pd *pd)\r\n{\r\nstruct usnic_uiom_interval_node *interval, *tmp;\r\nlong unsigned va, size;\r\nlist_for_each_entry_safe(interval, tmp, intervals, link) {\r\nva = interval->start << PAGE_SHIFT;\r\nsize = ((interval->last - interval->start) + 1) << PAGE_SHIFT;\r\nwhile (size > 0) {\r\nusnic_dbg("va 0x%lx size 0x%lx", va, PAGE_SIZE);\r\niommu_unmap(pd->domain, va, PAGE_SIZE);\r\nva += PAGE_SIZE;\r\nsize -= PAGE_SIZE;\r\n}\r\n}\r\n}\r\nstatic void __usnic_uiom_reg_release(struct usnic_uiom_pd *pd,\r\nstruct usnic_uiom_reg *uiomr,\r\nint dirty)\r\n{\r\nint npages;\r\nunsigned long vpn_start, vpn_last;\r\nstruct usnic_uiom_interval_node *interval, *tmp;\r\nint writable = 0;\r\nLIST_HEAD(rm_intervals);\r\nnpages = PAGE_ALIGN(uiomr->length + uiomr->offset) >> PAGE_SHIFT;\r\nvpn_start = (uiomr->va & PAGE_MASK) >> PAGE_SHIFT;\r\nvpn_last = vpn_start + npages - 1;\r\nspin_lock(&pd->lock);\r\nusnic_uiom_remove_interval(&pd->rb_root, vpn_start,\r\nvpn_last, &rm_intervals);\r\nusnic_uiom_unmap_sorted_intervals(&rm_intervals, pd);\r\nlist_for_each_entry_safe(interval, tmp, &rm_intervals, link) {\r\nif (interval->flags & IOMMU_WRITE)\r\nwritable = 1;\r\nlist_del(&interval->link);\r\nkfree(interval);\r\n}\r\nusnic_uiom_put_pages(&uiomr->chunk_list, dirty & writable);\r\nspin_unlock(&pd->lock);\r\n}\r\nstatic int usnic_uiom_map_sorted_intervals(struct list_head *intervals,\r\nstruct usnic_uiom_reg *uiomr)\r\n{\r\nint i, err;\r\nsize_t size;\r\nstruct usnic_uiom_chunk *chunk;\r\nstruct usnic_uiom_interval_node *interval_node;\r\ndma_addr_t pa;\r\ndma_addr_t pa_start = 0;\r\ndma_addr_t pa_end = 0;\r\nlong int va_start = -EINVAL;\r\nstruct usnic_uiom_pd *pd = uiomr->pd;\r\nlong int va = uiomr->va & PAGE_MASK;\r\nint flags = IOMMU_READ | IOMMU_CACHE;\r\nflags |= (uiomr->writable) ? IOMMU_WRITE : 0;\r\nchunk = list_first_entry(&uiomr->chunk_list, struct usnic_uiom_chunk,\r\nlist);\r\nlist_for_each_entry(interval_node, intervals, link) {\r\niter_chunk:\r\nfor (i = 0; i < chunk->nents; i++, va += PAGE_SIZE) {\r\npa = sg_phys(&chunk->page_list[i]);\r\nif ((va >> PAGE_SHIFT) < interval_node->start)\r\ncontinue;\r\nif ((va >> PAGE_SHIFT) == interval_node->start) {\r\nva_start = va;\r\npa_start = pa;\r\npa_end = pa;\r\n}\r\nWARN_ON(va_start == -EINVAL);\r\nif ((pa_end + PAGE_SIZE != pa) &&\r\n(pa != pa_start)) {\r\nsize = pa_end - pa_start + PAGE_SIZE;\r\nusnic_dbg("va 0x%lx pa %pa size 0x%zx flags 0x%x",\r\nva_start, &pa_start, size, flags);\r\nerr = iommu_map(pd->domain, va_start, pa_start,\r\nsize, flags);\r\nif (err) {\r\nusnic_err("Failed to map va 0x%lx pa %pa size 0x%zx with err %d\n",\r\nva_start, &pa_start, size, err);\r\ngoto err_out;\r\n}\r\nva_start = va;\r\npa_start = pa;\r\npa_end = pa;\r\n}\r\nif ((va >> PAGE_SHIFT) == interval_node->last) {\r\nsize = pa - pa_start + PAGE_SIZE;\r\nusnic_dbg("va 0x%lx pa %pa size 0x%zx flags 0x%x\n",\r\nva_start, &pa_start, size, flags);\r\nerr = iommu_map(pd->domain, va_start, pa_start,\r\nsize, flags);\r\nif (err) {\r\nusnic_err("Failed to map va 0x%lx pa %pa size 0x%zx with err %d\n",\r\nva_start, &pa_start, size, err);\r\ngoto err_out;\r\n}\r\nbreak;\r\n}\r\nif (pa != pa_start)\r\npa_end += PAGE_SIZE;\r\n}\r\nif (i == chunk->nents) {\r\nchunk = list_first_entry(&chunk->list,\r\nstruct usnic_uiom_chunk,\r\nlist);\r\ngoto iter_chunk;\r\n}\r\n}\r\nreturn 0;\r\nerr_out:\r\nusnic_uiom_unmap_sorted_intervals(intervals, pd);\r\nreturn err;\r\n}\r\nstruct usnic_uiom_reg *usnic_uiom_reg_get(struct usnic_uiom_pd *pd,\r\nunsigned long addr, size_t size,\r\nint writable, int dmasync)\r\n{\r\nstruct usnic_uiom_reg *uiomr;\r\nunsigned long va_base, vpn_start, vpn_last;\r\nunsigned long npages;\r\nint offset, err;\r\nLIST_HEAD(sorted_diff_intervals);\r\nwritable = 1;\r\nva_base = addr & PAGE_MASK;\r\noffset = addr & ~PAGE_MASK;\r\nnpages = PAGE_ALIGN(size + offset) >> PAGE_SHIFT;\r\nvpn_start = (addr & PAGE_MASK) >> PAGE_SHIFT;\r\nvpn_last = vpn_start + npages - 1;\r\nuiomr = kmalloc(sizeof(*uiomr), GFP_KERNEL);\r\nif (!uiomr)\r\nreturn ERR_PTR(-ENOMEM);\r\nuiomr->va = va_base;\r\nuiomr->offset = offset;\r\nuiomr->length = size;\r\nuiomr->writable = writable;\r\nuiomr->pd = pd;\r\nerr = usnic_uiom_get_pages(addr, size, writable, dmasync,\r\n&uiomr->chunk_list);\r\nif (err) {\r\nusnic_err("Failed get_pages vpn [0x%lx,0x%lx] err %d\n",\r\nvpn_start, vpn_last, err);\r\ngoto out_free_uiomr;\r\n}\r\nspin_lock(&pd->lock);\r\nerr = usnic_uiom_get_intervals_diff(vpn_start, vpn_last,\r\n(writable) ? IOMMU_WRITE : 0,\r\nIOMMU_WRITE,\r\n&pd->rb_root,\r\n&sorted_diff_intervals);\r\nif (err) {\r\nusnic_err("Failed disjoint interval vpn [0x%lx,0x%lx] err %d\n",\r\nvpn_start, vpn_last, err);\r\ngoto out_put_pages;\r\n}\r\nerr = usnic_uiom_map_sorted_intervals(&sorted_diff_intervals, uiomr);\r\nif (err) {\r\nusnic_err("Failed map interval vpn [0x%lx,0x%lx] err %d\n",\r\nvpn_start, vpn_last, err);\r\ngoto out_put_intervals;\r\n}\r\nerr = usnic_uiom_insert_interval(&pd->rb_root, vpn_start, vpn_last,\r\n(writable) ? IOMMU_WRITE : 0);\r\nif (err) {\r\nusnic_err("Failed insert interval vpn [0x%lx,0x%lx] err %d\n",\r\nvpn_start, vpn_last, err);\r\ngoto out_unmap_intervals;\r\n}\r\nusnic_uiom_put_interval_set(&sorted_diff_intervals);\r\nspin_unlock(&pd->lock);\r\nreturn uiomr;\r\nout_unmap_intervals:\r\nusnic_uiom_unmap_sorted_intervals(&sorted_diff_intervals, pd);\r\nout_put_intervals:\r\nusnic_uiom_put_interval_set(&sorted_diff_intervals);\r\nout_put_pages:\r\nusnic_uiom_put_pages(&uiomr->chunk_list, 0);\r\nspin_unlock(&pd->lock);\r\nout_free_uiomr:\r\nkfree(uiomr);\r\nreturn ERR_PTR(err);\r\n}\r\nvoid usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr, int closing)\r\n{\r\nstruct mm_struct *mm;\r\nunsigned long diff;\r\n__usnic_uiom_reg_release(uiomr->pd, uiomr, 1);\r\nmm = get_task_mm(current);\r\nif (!mm) {\r\nkfree(uiomr);\r\nreturn;\r\n}\r\ndiff = PAGE_ALIGN(uiomr->length + uiomr->offset) >> PAGE_SHIFT;\r\nif (closing) {\r\nif (!down_write_trylock(&mm->mmap_sem)) {\r\nINIT_WORK(&uiomr->work, usnic_uiom_reg_account);\r\nuiomr->mm = mm;\r\nuiomr->diff = diff;\r\nqueue_work(usnic_uiom_wq, &uiomr->work);\r\nreturn;\r\n}\r\n} else\r\ndown_write(&mm->mmap_sem);\r\ncurrent->mm->locked_vm -= diff;\r\nup_write(&mm->mmap_sem);\r\nmmput(mm);\r\nkfree(uiomr);\r\n}\r\nstruct usnic_uiom_pd *usnic_uiom_alloc_pd(void)\r\n{\r\nstruct usnic_uiom_pd *pd;\r\nvoid *domain;\r\npd = kzalloc(sizeof(*pd), GFP_KERNEL);\r\nif (!pd)\r\nreturn ERR_PTR(-ENOMEM);\r\npd->domain = domain = iommu_domain_alloc(&pci_bus_type);\r\nif (!domain) {\r\nusnic_err("Failed to allocate IOMMU domain");\r\nkfree(pd);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\niommu_set_fault_handler(pd->domain, usnic_uiom_dma_fault, NULL);\r\nspin_lock_init(&pd->lock);\r\nINIT_LIST_HEAD(&pd->devs);\r\nreturn pd;\r\n}\r\nvoid usnic_uiom_dealloc_pd(struct usnic_uiom_pd *pd)\r\n{\r\niommu_domain_free(pd->domain);\r\nkfree(pd);\r\n}\r\nint usnic_uiom_attach_dev_to_pd(struct usnic_uiom_pd *pd, struct device *dev)\r\n{\r\nstruct usnic_uiom_dev *uiom_dev;\r\nint err;\r\nuiom_dev = kzalloc(sizeof(*uiom_dev), GFP_ATOMIC);\r\nif (!uiom_dev)\r\nreturn -ENOMEM;\r\nuiom_dev->dev = dev;\r\nerr = iommu_attach_device(pd->domain, dev);\r\nif (err)\r\ngoto out_free_dev;\r\nif (!iommu_capable(dev->bus, IOMMU_CAP_CACHE_COHERENCY)) {\r\nusnic_err("IOMMU of %s does not support cache coherency\n",\r\ndev_name(dev));\r\nerr = -EINVAL;\r\ngoto out_detach_device;\r\n}\r\nspin_lock(&pd->lock);\r\nlist_add_tail(&uiom_dev->link, &pd->devs);\r\npd->dev_cnt++;\r\nspin_unlock(&pd->lock);\r\nreturn 0;\r\nout_detach_device:\r\niommu_detach_device(pd->domain, dev);\r\nout_free_dev:\r\nkfree(uiom_dev);\r\nreturn err;\r\n}\r\nvoid usnic_uiom_detach_dev_from_pd(struct usnic_uiom_pd *pd, struct device *dev)\r\n{\r\nstruct usnic_uiom_dev *uiom_dev;\r\nint found = 0;\r\nspin_lock(&pd->lock);\r\nlist_for_each_entry(uiom_dev, &pd->devs, link) {\r\nif (uiom_dev->dev == dev) {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nusnic_err("Unable to free dev %s - not found\n",\r\ndev_name(dev));\r\nspin_unlock(&pd->lock);\r\nreturn;\r\n}\r\nlist_del(&uiom_dev->link);\r\npd->dev_cnt--;\r\nspin_unlock(&pd->lock);\r\nreturn iommu_detach_device(pd->domain, dev);\r\n}\r\nstruct device **usnic_uiom_get_dev_list(struct usnic_uiom_pd *pd)\r\n{\r\nstruct usnic_uiom_dev *uiom_dev;\r\nstruct device **devs;\r\nint i = 0;\r\nspin_lock(&pd->lock);\r\ndevs = kcalloc(pd->dev_cnt + 1, sizeof(*devs), GFP_ATOMIC);\r\nif (!devs) {\r\ndevs = ERR_PTR(-ENOMEM);\r\ngoto out;\r\n}\r\nlist_for_each_entry(uiom_dev, &pd->devs, link) {\r\ndevs[i++] = uiom_dev->dev;\r\n}\r\nout:\r\nspin_unlock(&pd->lock);\r\nreturn devs;\r\n}\r\nvoid usnic_uiom_free_dev_list(struct device **devs)\r\n{\r\nkfree(devs);\r\n}\r\nint usnic_uiom_init(char *drv_name)\r\n{\r\nif (!iommu_present(&pci_bus_type)) {\r\nusnic_err("IOMMU required but not present or enabled. USNIC QPs will not function w/o enabling IOMMU\n");\r\nreturn -EPERM;\r\n}\r\nusnic_uiom_wq = create_workqueue(drv_name);\r\nif (!usnic_uiom_wq) {\r\nusnic_err("Unable to alloc wq for drv %s\n", drv_name);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nvoid usnic_uiom_fini(void)\r\n{\r\nflush_workqueue(usnic_uiom_wq);\r\ndestroy_workqueue(usnic_uiom_wq);\r\n}
