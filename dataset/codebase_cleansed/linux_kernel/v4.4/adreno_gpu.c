int adreno_get_param(struct msm_gpu *gpu, uint32_t param, uint64_t *value)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nswitch (param) {\r\ncase MSM_PARAM_GPU_ID:\r\n*value = adreno_gpu->info->revn;\r\nreturn 0;\r\ncase MSM_PARAM_GMEM_SIZE:\r\n*value = adreno_gpu->gmem;\r\nreturn 0;\r\ncase MSM_PARAM_CHIP_ID:\r\n*value = adreno_gpu->rev.patchid |\r\n(adreno_gpu->rev.minor << 8) |\r\n(adreno_gpu->rev.major << 16) |\r\n(adreno_gpu->rev.core << 24);\r\nreturn 0;\r\ndefault:\r\nDBG("%s: invalid param: %u", gpu->name, param);\r\nreturn -EINVAL;\r\n}\r\n}\r\nint adreno_hw_init(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nint ret;\r\nDBG("%s", gpu->name);\r\nret = msm_gem_get_iova(gpu->rb->bo, gpu->id, &gpu->rb_iova);\r\nif (ret) {\r\ngpu->rb_iova = 0;\r\ndev_err(gpu->dev->dev, "could not map ringbuffer: %d\n", ret);\r\nreturn ret;\r\n}\r\nadreno_gpu_write(adreno_gpu, REG_ADRENO_CP_RB_CNTL,\r\nAXXX_CP_RB_CNTL_BUFSZ(ilog2(gpu->rb->size / 8)) |\r\nAXXX_CP_RB_CNTL_BLKSZ(ilog2(RB_BLKSIZE / 8)));\r\nadreno_gpu_write(adreno_gpu, REG_ADRENO_CP_RB_BASE, gpu->rb_iova);\r\nadreno_gpu_write(adreno_gpu, REG_ADRENO_CP_RB_RPTR_ADDR,\r\nrbmemptr(adreno_gpu, rptr));\r\nadreno_gpu_write(adreno_gpu, REG_ADRENO_SCRATCH_ADDR,\r\nrbmemptr(adreno_gpu, fence));\r\nadreno_gpu_write(adreno_gpu, REG_ADRENO_SCRATCH_UMSK, 0x1);\r\nreturn 0;\r\n}\r\nstatic uint32_t get_wptr(struct msm_ringbuffer *ring)\r\n{\r\nreturn ring->cur - ring->start;\r\n}\r\nuint32_t adreno_last_fence(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nreturn adreno_gpu->memptrs->fence;\r\n}\r\nvoid adreno_recover(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nstruct drm_device *dev = gpu->dev;\r\nint ret;\r\ngpu->funcs->pm_suspend(gpu);\r\ngpu->rb->cur = gpu->rb->start;\r\nadreno_gpu->memptrs->fence = gpu->submitted_fence;\r\nadreno_gpu->memptrs->rptr = 0;\r\nadreno_gpu->memptrs->wptr = 0;\r\ngpu->funcs->pm_resume(gpu);\r\nret = gpu->funcs->hw_init(gpu);\r\nif (ret) {\r\ndev_err(dev->dev, "gpu hw init failed: %d\n", ret);\r\n}\r\n}\r\nint adreno_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit,\r\nstruct msm_file_private *ctx)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nstruct msm_drm_private *priv = gpu->dev->dev_private;\r\nstruct msm_ringbuffer *ring = gpu->rb;\r\nunsigned i, ibs = 0;\r\nfor (i = 0; i < submit->nr_cmds; i++) {\r\nswitch (submit->cmd[i].type) {\r\ncase MSM_SUBMIT_CMD_IB_TARGET_BUF:\r\nbreak;\r\ncase MSM_SUBMIT_CMD_CTX_RESTORE_BUF:\r\nif (priv->lastctx == ctx)\r\nbreak;\r\ncase MSM_SUBMIT_CMD_BUF:\r\nOUT_PKT3(ring, CP_INDIRECT_BUFFER_PFD, 2);\r\nOUT_RING(ring, submit->cmd[i].iova);\r\nOUT_RING(ring, submit->cmd[i].size);\r\nibs++;\r\nbreak;\r\n}\r\n}\r\nif (ibs % 2)\r\nOUT_PKT2(ring);\r\nOUT_PKT0(ring, REG_AXXX_CP_SCRATCH_REG2, 1);\r\nOUT_RING(ring, submit->fence);\r\nif (adreno_is_a3xx(adreno_gpu) || adreno_is_a4xx(adreno_gpu)) {\r\nOUT_PKT3(ring, CP_EVENT_WRITE, 1);\r\nOUT_RING(ring, HLSQ_FLUSH);\r\nOUT_PKT3(ring, CP_WAIT_FOR_IDLE, 1);\r\nOUT_RING(ring, 0x00000000);\r\n}\r\nOUT_PKT3(ring, CP_EVENT_WRITE, 3);\r\nOUT_RING(ring, CACHE_FLUSH_TS);\r\nOUT_RING(ring, rbmemptr(adreno_gpu, fence));\r\nOUT_RING(ring, submit->fence);\r\nOUT_PKT3(ring, CP_INTERRUPT, 1);\r\nOUT_RING(ring, 0x80000000);\r\nif (adreno_is_a306(adreno_gpu)) {\r\nOUT_PKT3(ring, CP_WAIT_FOR_IDLE, 1);\r\nOUT_RING(ring, 0x00000000);\r\nOUT_PKT3(ring, CP_INTERRUPT, 1);\r\nOUT_RING(ring, 0x80000000);\r\n}\r\n#if 0\r\nif (adreno_is_a3xx(adreno_gpu)) {\r\nOUT_PKT3(ring, CP_SET_CONSTANT, 2);\r\nOUT_RING(ring, CP_REG(REG_A3XX_HLSQ_CL_KERNEL_GROUP_X_REG));\r\nOUT_RING(ring, 0x00000000);\r\n}\r\n#endif\r\ngpu->funcs->flush(gpu);\r\nreturn 0;\r\n}\r\nvoid adreno_flush(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nuint32_t wptr = get_wptr(gpu->rb);\r\nmb();\r\nadreno_gpu_write(adreno_gpu, REG_ADRENO_CP_RB_WPTR, wptr);\r\n}\r\nvoid adreno_idle(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nuint32_t wptr = get_wptr(gpu->rb);\r\nif (spin_until(adreno_gpu->memptrs->rptr == wptr))\r\nDRM_ERROR("%s: timeout waiting to drain ringbuffer!\n", gpu->name);\r\n}\r\nvoid adreno_show(struct msm_gpu *gpu, struct seq_file *m)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nint i;\r\nseq_printf(m, "revision: %d (%d.%d.%d.%d)\n",\r\nadreno_gpu->info->revn, adreno_gpu->rev.core,\r\nadreno_gpu->rev.major, adreno_gpu->rev.minor,\r\nadreno_gpu->rev.patchid);\r\nseq_printf(m, "fence: %d/%d\n", adreno_gpu->memptrs->fence,\r\ngpu->submitted_fence);\r\nseq_printf(m, "rptr: %d\n", adreno_gpu->memptrs->rptr);\r\nseq_printf(m, "wptr: %d\n", adreno_gpu->memptrs->wptr);\r\nseq_printf(m, "rb wptr: %d\n", get_wptr(gpu->rb));\r\ngpu->funcs->pm_resume(gpu);\r\nseq_printf(m, "IO:region %s 00000000 00020000\n", gpu->name);\r\nfor (i = 0; adreno_gpu->registers[i] != ~0; i += 2) {\r\nuint32_t start = adreno_gpu->registers[i];\r\nuint32_t end = adreno_gpu->registers[i+1];\r\nuint32_t addr;\r\nfor (addr = start; addr <= end; addr++) {\r\nuint32_t val = gpu_read(gpu, addr);\r\nseq_printf(m, "IO:R %08x %08x\n", addr<<2, val);\r\n}\r\n}\r\ngpu->funcs->pm_suspend(gpu);\r\n}\r\nvoid adreno_dump_info(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nint i;\r\nprintk("revision: %d (%d.%d.%d.%d)\n",\r\nadreno_gpu->info->revn, adreno_gpu->rev.core,\r\nadreno_gpu->rev.major, adreno_gpu->rev.minor,\r\nadreno_gpu->rev.patchid);\r\nprintk("fence: %d/%d\n", adreno_gpu->memptrs->fence,\r\ngpu->submitted_fence);\r\nprintk("rptr: %d\n", adreno_gpu->memptrs->rptr);\r\nprintk("wptr: %d\n", adreno_gpu->memptrs->wptr);\r\nprintk("rb wptr: %d\n", get_wptr(gpu->rb));\r\nfor (i = 0; i < 8; i++) {\r\nprintk("CP_SCRATCH_REG%d: %u\n", i,\r\ngpu_read(gpu, REG_AXXX_CP_SCRATCH_REG0 + i));\r\n}\r\n}\r\nvoid adreno_dump(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nint i;\r\nprintk("IO:region %s 00000000 00020000\n", gpu->name);\r\nfor (i = 0; adreno_gpu->registers[i] != ~0; i += 2) {\r\nuint32_t start = adreno_gpu->registers[i];\r\nuint32_t end = adreno_gpu->registers[i+1];\r\nuint32_t addr;\r\nfor (addr = start; addr <= end; addr++) {\r\nuint32_t val = gpu_read(gpu, addr);\r\nprintk("IO:R %08x %08x\n", addr<<2, val);\r\n}\r\n}\r\n}\r\nstatic uint32_t ring_freewords(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nuint32_t size = gpu->rb->size / 4;\r\nuint32_t wptr = get_wptr(gpu->rb);\r\nuint32_t rptr = adreno_gpu->memptrs->rptr;\r\nreturn (rptr + (size - 1) - wptr) % size;\r\n}\r\nvoid adreno_wait_ring(struct msm_gpu *gpu, uint32_t ndwords)\r\n{\r\nif (spin_until(ring_freewords(gpu) >= ndwords))\r\nDRM_ERROR("%s: timeout waiting for ringbuffer space\n", gpu->name);\r\n}\r\nint adreno_gpu_init(struct drm_device *drm, struct platform_device *pdev,\r\nstruct adreno_gpu *adreno_gpu, const struct adreno_gpu_funcs *funcs)\r\n{\r\nstruct adreno_platform_config *config = pdev->dev.platform_data;\r\nstruct msm_gpu *gpu = &adreno_gpu->base;\r\nstruct msm_mmu *mmu;\r\nint ret;\r\nadreno_gpu->funcs = funcs;\r\nadreno_gpu->info = adreno_info(config->rev);\r\nadreno_gpu->gmem = adreno_gpu->info->gmem;\r\nadreno_gpu->revn = adreno_gpu->info->revn;\r\nadreno_gpu->rev = config->rev;\r\ngpu->fast_rate = config->fast_rate;\r\ngpu->slow_rate = config->slow_rate;\r\ngpu->bus_freq = config->bus_freq;\r\n#ifdef DOWNSTREAM_CONFIG_MSM_BUS_SCALING\r\ngpu->bus_scale_table = config->bus_scale_table;\r\n#endif\r\nDBG("fast_rate=%u, slow_rate=%u, bus_freq=%u",\r\ngpu->fast_rate, gpu->slow_rate, gpu->bus_freq);\r\nret = msm_gpu_init(drm, pdev, &adreno_gpu->base, &funcs->base,\r\nadreno_gpu->info->name, "kgsl_3d0_reg_memory", "kgsl_3d0_irq",\r\nRB_SIZE);\r\nif (ret)\r\nreturn ret;\r\nret = request_firmware(&adreno_gpu->pm4, adreno_gpu->info->pm4fw, drm->dev);\r\nif (ret) {\r\ndev_err(drm->dev, "failed to load %s PM4 firmware: %d\n",\r\nadreno_gpu->info->pm4fw, ret);\r\nreturn ret;\r\n}\r\nret = request_firmware(&adreno_gpu->pfp, adreno_gpu->info->pfpfw, drm->dev);\r\nif (ret) {\r\ndev_err(drm->dev, "failed to load %s PFP firmware: %d\n",\r\nadreno_gpu->info->pfpfw, ret);\r\nreturn ret;\r\n}\r\nmmu = gpu->mmu;\r\nif (mmu) {\r\nret = mmu->funcs->attach(mmu, iommu_ports,\r\nARRAY_SIZE(iommu_ports));\r\nif (ret)\r\nreturn ret;\r\n}\r\nmutex_lock(&drm->struct_mutex);\r\nadreno_gpu->memptrs_bo = msm_gem_new(drm, sizeof(*adreno_gpu->memptrs),\r\nMSM_BO_UNCACHED);\r\nmutex_unlock(&drm->struct_mutex);\r\nif (IS_ERR(adreno_gpu->memptrs_bo)) {\r\nret = PTR_ERR(adreno_gpu->memptrs_bo);\r\nadreno_gpu->memptrs_bo = NULL;\r\ndev_err(drm->dev, "could not allocate memptrs: %d\n", ret);\r\nreturn ret;\r\n}\r\nadreno_gpu->memptrs = msm_gem_vaddr(adreno_gpu->memptrs_bo);\r\nif (!adreno_gpu->memptrs) {\r\ndev_err(drm->dev, "could not vmap memptrs\n");\r\nreturn -ENOMEM;\r\n}\r\nret = msm_gem_get_iova(adreno_gpu->memptrs_bo, gpu->id,\r\n&adreno_gpu->memptrs_iova);\r\nif (ret) {\r\ndev_err(drm->dev, "could not map memptrs: %d\n", ret);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nvoid adreno_gpu_cleanup(struct adreno_gpu *gpu)\r\n{\r\nif (gpu->memptrs_bo) {\r\nif (gpu->memptrs_iova)\r\nmsm_gem_put_iova(gpu->memptrs_bo, gpu->base.id);\r\ndrm_gem_object_unreference_unlocked(gpu->memptrs_bo);\r\n}\r\nrelease_firmware(gpu->pm4);\r\nrelease_firmware(gpu->pfp);\r\nmsm_gpu_cleanup(&gpu->base);\r\n}
