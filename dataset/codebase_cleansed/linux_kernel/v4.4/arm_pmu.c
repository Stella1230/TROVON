static int\r\narmpmu_map_raw_event(u32 raw_event_mask, u64 config)\r\n{\r\nreturn (int)(config & raw_event_mask);\r\n}\r\nint armpmu_event_set_period(struct perf_event *event)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\ns64 left = local64_read(&hwc->period_left);\r\ns64 period = hwc->sample_period;\r\nint ret = 0;\r\nif (unlikely(left <= -period)) {\r\nleft = period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n}\r\nif (unlikely(left <= 0)) {\r\nleft += period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n}\r\nif (left > (armpmu->max_period >> 1))\r\nleft = armpmu->max_period >> 1;\r\nlocal64_set(&hwc->prev_count, (u64)-left);\r\narmpmu->write_counter(event, (u64)(-left) & 0xffffffff);\r\nperf_event_update_userpage(event);\r\nreturn ret;\r\n}\r\nu64 armpmu_event_update(struct perf_event *event)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 delta, prev_raw_count, new_raw_count;\r\nagain:\r\nprev_raw_count = local64_read(&hwc->prev_count);\r\nnew_raw_count = armpmu->read_counter(event);\r\nif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\r\nnew_raw_count) != prev_raw_count)\r\ngoto again;\r\ndelta = (new_raw_count - prev_raw_count) & armpmu->max_period;\r\nlocal64_add(delta, &event->count);\r\nlocal64_sub(delta, &hwc->period_left);\r\nreturn new_raw_count;\r\n}\r\nstatic void\r\narmpmu_read(struct perf_event *event)\r\n{\r\narmpmu_event_update(event);\r\n}\r\nstatic void\r\narmpmu_stop(struct perf_event *event, int flags)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (!(hwc->state & PERF_HES_STOPPED)) {\r\narmpmu->disable(event);\r\narmpmu_event_update(event);\r\nhwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\n}\r\n}\r\nstatic void armpmu_start(struct perf_event *event, int flags)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (flags & PERF_EF_RELOAD)\r\nWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\r\nhwc->state = 0;\r\narmpmu_event_set_period(event);\r\narmpmu->enable(event);\r\n}\r\nstatic void\r\narmpmu_del(struct perf_event *event, int flags)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\narmpmu_stop(event, PERF_EF_UPDATE);\r\nhw_events->events[idx] = NULL;\r\nclear_bit(idx, hw_events->used_mask);\r\nif (armpmu->clear_event_idx)\r\narmpmu->clear_event_idx(hw_events, event);\r\nperf_event_update_userpage(event);\r\n}\r\nstatic int\r\narmpmu_add(struct perf_event *event, int flags)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx;\r\nint err = 0;\r\nif (!cpumask_test_cpu(smp_processor_id(), &armpmu->supported_cpus))\r\nreturn -ENOENT;\r\nperf_pmu_disable(event->pmu);\r\nidx = armpmu->get_event_idx(hw_events, event);\r\nif (idx < 0) {\r\nerr = idx;\r\ngoto out;\r\n}\r\nevent->hw.idx = idx;\r\narmpmu->disable(event);\r\nhw_events->events[idx] = event;\r\nhwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nif (flags & PERF_EF_START)\r\narmpmu_start(event, PERF_EF_RELOAD);\r\nperf_event_update_userpage(event);\r\nout:\r\nperf_pmu_enable(event->pmu);\r\nreturn err;\r\n}\r\nstatic int\r\nvalidate_event(struct pmu *pmu, struct pmu_hw_events *hw_events,\r\nstruct perf_event *event)\r\n{\r\nstruct arm_pmu *armpmu;\r\nif (is_software_event(event))\r\nreturn 1;\r\nif (event->pmu != pmu)\r\nreturn 0;\r\nif (event->state < PERF_EVENT_STATE_OFF)\r\nreturn 1;\r\nif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\r\nreturn 1;\r\narmpmu = to_arm_pmu(event->pmu);\r\nreturn armpmu->get_event_idx(hw_events, event) >= 0;\r\n}\r\nstatic int\r\nvalidate_group(struct perf_event *event)\r\n{\r\nstruct perf_event *sibling, *leader = event->group_leader;\r\nstruct pmu_hw_events fake_pmu;\r\nmemset(&fake_pmu.used_mask, 0, sizeof(fake_pmu.used_mask));\r\nif (!validate_event(event->pmu, &fake_pmu, leader))\r\nreturn -EINVAL;\r\nlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\r\nif (!validate_event(event->pmu, &fake_pmu, sibling))\r\nreturn -EINVAL;\r\n}\r\nif (!validate_event(event->pmu, &fake_pmu, event))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic irqreturn_t armpmu_dispatch_irq(int irq, void *dev)\r\n{\r\nstruct arm_pmu *armpmu;\r\nstruct platform_device *plat_device;\r\nstruct arm_pmu_platdata *plat;\r\nint ret;\r\nu64 start_clock, finish_clock;\r\narmpmu = *(void **)dev;\r\nplat_device = armpmu->plat_device;\r\nplat = dev_get_platdata(&plat_device->dev);\r\nstart_clock = sched_clock();\r\nif (plat && plat->handle_irq)\r\nret = plat->handle_irq(irq, armpmu, armpmu->handle_irq);\r\nelse\r\nret = armpmu->handle_irq(irq, armpmu);\r\nfinish_clock = sched_clock();\r\nperf_sample_event_took(finish_clock - start_clock);\r\nreturn ret;\r\n}\r\nstatic void\r\narmpmu_release_hardware(struct arm_pmu *armpmu)\r\n{\r\narmpmu->free_irq(armpmu);\r\n}\r\nstatic int\r\narmpmu_reserve_hardware(struct arm_pmu *armpmu)\r\n{\r\nint err = armpmu->request_irq(armpmu, armpmu_dispatch_irq);\r\nif (err) {\r\narmpmu_release_hardware(armpmu);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nhw_perf_event_destroy(struct perf_event *event)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\natomic_t *active_events = &armpmu->active_events;\r\nstruct mutex *pmu_reserve_mutex = &armpmu->reserve_mutex;\r\nif (atomic_dec_and_mutex_lock(active_events, pmu_reserve_mutex)) {\r\narmpmu_release_hardware(armpmu);\r\nmutex_unlock(pmu_reserve_mutex);\r\n}\r\n}\r\nstatic int\r\nevent_requires_mode_exclusion(struct perf_event_attr *attr)\r\n{\r\nreturn attr->exclude_idle || attr->exclude_user ||\r\nattr->exclude_kernel || attr->exclude_hv;\r\n}\r\nstatic int\r\n__hw_perf_event_init(struct perf_event *event)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint mapping;\r\nmapping = armpmu->map_event(event);\r\nif (mapping < 0) {\r\npr_debug("event %x:%llx not supported\n", event->attr.type,\r\nevent->attr.config);\r\nreturn mapping;\r\n}\r\nhwc->idx = -1;\r\nhwc->config_base = 0;\r\nhwc->config = 0;\r\nhwc->event_base = 0;\r\nif ((!armpmu->set_event_filter ||\r\narmpmu->set_event_filter(hwc, &event->attr)) &&\r\nevent_requires_mode_exclusion(&event->attr)) {\r\npr_debug("ARM performance counters do not support "\r\n"mode exclusion\n");\r\nreturn -EOPNOTSUPP;\r\n}\r\nhwc->config_base |= (unsigned long)mapping;\r\nif (!is_sampling_event(event)) {\r\nhwc->sample_period = armpmu->max_period >> 1;\r\nhwc->last_period = hwc->sample_period;\r\nlocal64_set(&hwc->period_left, hwc->sample_period);\r\n}\r\nif (event->group_leader != event) {\r\nif (validate_group(event) != 0)\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int armpmu_event_init(struct perf_event *event)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nint err = 0;\r\natomic_t *active_events = &armpmu->active_events;\r\nif (event->cpu != -1 &&\r\n!cpumask_test_cpu(event->cpu, &armpmu->supported_cpus))\r\nreturn -ENOENT;\r\nif (has_branch_stack(event))\r\nreturn -EOPNOTSUPP;\r\nif (armpmu->map_event(event) == -ENOENT)\r\nreturn -ENOENT;\r\nevent->destroy = hw_perf_event_destroy;\r\nif (!atomic_inc_not_zero(active_events)) {\r\nmutex_lock(&armpmu->reserve_mutex);\r\nif (atomic_read(active_events) == 0)\r\nerr = armpmu_reserve_hardware(armpmu);\r\nif (!err)\r\natomic_inc(active_events);\r\nmutex_unlock(&armpmu->reserve_mutex);\r\n}\r\nif (err)\r\nreturn err;\r\nerr = __hw_perf_event_init(event);\r\nif (err)\r\nhw_perf_event_destroy(event);\r\nreturn err;\r\n}\r\nstatic void armpmu_enable(struct pmu *pmu)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(pmu);\r\nstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\r\nint enabled = bitmap_weight(hw_events->used_mask, armpmu->num_events);\r\nif (!cpumask_test_cpu(smp_processor_id(), &armpmu->supported_cpus))\r\nreturn;\r\nif (enabled)\r\narmpmu->start(armpmu);\r\n}\r\nstatic void armpmu_disable(struct pmu *pmu)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(pmu);\r\nif (!cpumask_test_cpu(smp_processor_id(), &armpmu->supported_cpus))\r\nreturn;\r\narmpmu->stop(armpmu);\r\n}\r\nstatic int armpmu_filter_match(struct perf_event *event)\r\n{\r\nstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\r\nunsigned int cpu = smp_processor_id();\r\nreturn cpumask_test_cpu(cpu, &armpmu->supported_cpus);\r\n}\r\nstatic void armpmu_init(struct arm_pmu *armpmu)\r\n{\r\natomic_set(&armpmu->active_events, 0);\r\nmutex_init(&armpmu->reserve_mutex);\r\narmpmu->pmu = (struct pmu) {\r\n.pmu_enable = armpmu_enable,\r\n.pmu_disable = armpmu_disable,\r\n.event_init = armpmu_event_init,\r\n.add = armpmu_add,\r\n.del = armpmu_del,\r\n.start = armpmu_start,\r\n.stop = armpmu_stop,\r\n.read = armpmu_read,\r\n.filter_match = armpmu_filter_match,\r\n};\r\n}\r\nint armpmu_register(struct arm_pmu *armpmu, int type)\r\n{\r\narmpmu_init(armpmu);\r\npr_info("enabled with %s PMU driver, %d counters available\n",\r\narmpmu->name, armpmu->num_events);\r\nreturn perf_pmu_register(&armpmu->pmu, armpmu->name, type);\r\n}\r\nconst char *perf_pmu_name(void)\r\n{\r\nif (!__oprofile_cpu_pmu)\r\nreturn NULL;\r\nreturn __oprofile_cpu_pmu->name;\r\n}\r\nint perf_num_counters(void)\r\n{\r\nint max_events = 0;\r\nif (__oprofile_cpu_pmu != NULL)\r\nmax_events = __oprofile_cpu_pmu->num_events;\r\nreturn max_events;\r\n}\r\nstatic void cpu_pmu_enable_percpu_irq(void *data)\r\n{\r\nint irq = *(int *)data;\r\nenable_percpu_irq(irq, IRQ_TYPE_NONE);\r\n}\r\nstatic void cpu_pmu_disable_percpu_irq(void *data)\r\n{\r\nint irq = *(int *)data;\r\ndisable_percpu_irq(irq);\r\n}\r\nstatic void cpu_pmu_free_irq(struct arm_pmu *cpu_pmu)\r\n{\r\nint i, irq, irqs;\r\nstruct platform_device *pmu_device = cpu_pmu->plat_device;\r\nstruct pmu_hw_events __percpu *hw_events = cpu_pmu->hw_events;\r\nirqs = min(pmu_device->num_resources, num_possible_cpus());\r\nirq = platform_get_irq(pmu_device, 0);\r\nif (irq >= 0 && irq_is_percpu(irq)) {\r\non_each_cpu(cpu_pmu_disable_percpu_irq, &irq, 1);\r\nfree_percpu_irq(irq, &hw_events->percpu_pmu);\r\n} else {\r\nfor (i = 0; i < irqs; ++i) {\r\nint cpu = i;\r\nif (cpu_pmu->irq_affinity)\r\ncpu = cpu_pmu->irq_affinity[i];\r\nif (!cpumask_test_and_clear_cpu(cpu, &cpu_pmu->active_irqs))\r\ncontinue;\r\nirq = platform_get_irq(pmu_device, i);\r\nif (irq >= 0)\r\nfree_irq(irq, per_cpu_ptr(&hw_events->percpu_pmu, cpu));\r\n}\r\n}\r\n}\r\nstatic int cpu_pmu_request_irq(struct arm_pmu *cpu_pmu, irq_handler_t handler)\r\n{\r\nint i, err, irq, irqs;\r\nstruct platform_device *pmu_device = cpu_pmu->plat_device;\r\nstruct pmu_hw_events __percpu *hw_events = cpu_pmu->hw_events;\r\nif (!pmu_device)\r\nreturn -ENODEV;\r\nirqs = min(pmu_device->num_resources, num_possible_cpus());\r\nif (irqs < 1) {\r\npr_warn_once("perf/ARM: No irqs for PMU defined, sampling events not supported\n");\r\nreturn 0;\r\n}\r\nirq = platform_get_irq(pmu_device, 0);\r\nif (irq >= 0 && irq_is_percpu(irq)) {\r\nerr = request_percpu_irq(irq, handler, "arm-pmu",\r\n&hw_events->percpu_pmu);\r\nif (err) {\r\npr_err("unable to request IRQ%d for ARM PMU counters\n",\r\nirq);\r\nreturn err;\r\n}\r\non_each_cpu(cpu_pmu_enable_percpu_irq, &irq, 1);\r\n} else {\r\nfor (i = 0; i < irqs; ++i) {\r\nint cpu = i;\r\nerr = 0;\r\nirq = platform_get_irq(pmu_device, i);\r\nif (irq < 0)\r\ncontinue;\r\nif (cpu_pmu->irq_affinity)\r\ncpu = cpu_pmu->irq_affinity[i];\r\nif (irq_set_affinity(irq, cpumask_of(cpu)) && irqs > 1) {\r\npr_warn("unable to set irq affinity (irq=%d, cpu=%u)\n",\r\nirq, cpu);\r\ncontinue;\r\n}\r\nerr = request_irq(irq, handler,\r\nIRQF_NOBALANCING | IRQF_NO_THREAD, "arm-pmu",\r\nper_cpu_ptr(&hw_events->percpu_pmu, cpu));\r\nif (err) {\r\npr_err("unable to request IRQ%d for ARM PMU counters\n",\r\nirq);\r\nreturn err;\r\n}\r\ncpumask_set_cpu(cpu, &cpu_pmu->active_irqs);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cpu_pmu_notify(struct notifier_block *b, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nint cpu = (unsigned long)hcpu;\r\nstruct arm_pmu *pmu = container_of(b, struct arm_pmu, hotplug_nb);\r\nif ((action & ~CPU_TASKS_FROZEN) != CPU_STARTING)\r\nreturn NOTIFY_DONE;\r\nif (!cpumask_test_cpu(cpu, &pmu->supported_cpus))\r\nreturn NOTIFY_DONE;\r\nif (pmu->reset)\r\npmu->reset(pmu);\r\nelse\r\nreturn NOTIFY_DONE;\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int cpu_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nint err;\r\nint cpu;\r\nstruct pmu_hw_events __percpu *cpu_hw_events;\r\ncpu_hw_events = alloc_percpu(struct pmu_hw_events);\r\nif (!cpu_hw_events)\r\nreturn -ENOMEM;\r\ncpu_pmu->hotplug_nb.notifier_call = cpu_pmu_notify;\r\nerr = register_cpu_notifier(&cpu_pmu->hotplug_nb);\r\nif (err)\r\ngoto out_hw_events;\r\nfor_each_possible_cpu(cpu) {\r\nstruct pmu_hw_events *events = per_cpu_ptr(cpu_hw_events, cpu);\r\nraw_spin_lock_init(&events->pmu_lock);\r\nevents->percpu_pmu = cpu_pmu;\r\n}\r\ncpu_pmu->hw_events = cpu_hw_events;\r\ncpu_pmu->request_irq = cpu_pmu_request_irq;\r\ncpu_pmu->free_irq = cpu_pmu_free_irq;\r\nif (cpu_pmu->reset)\r\non_each_cpu_mask(&cpu_pmu->supported_cpus, cpu_pmu->reset,\r\ncpu_pmu, 1);\r\nif (!platform_get_irq(cpu_pmu->plat_device, 0))\r\ncpu_pmu->pmu.capabilities |= PERF_PMU_CAP_NO_INTERRUPT;\r\nreturn 0;\r\nout_hw_events:\r\nfree_percpu(cpu_hw_events);\r\nreturn err;\r\n}\r\nstatic void cpu_pmu_destroy(struct arm_pmu *cpu_pmu)\r\n{\r\nunregister_cpu_notifier(&cpu_pmu->hotplug_nb);\r\nfree_percpu(cpu_pmu->hw_events);\r\n}\r\nstatic int probe_current_pmu(struct arm_pmu *pmu,\r\nconst struct pmu_probe_info *info)\r\n{\r\nint cpu = get_cpu();\r\nunsigned int cpuid = read_cpuid_id();\r\nint ret = -ENODEV;\r\npr_info("probing PMU on CPU %d\n", cpu);\r\nfor (; info->init != NULL; info++) {\r\nif ((cpuid & info->mask) != info->cpuid)\r\ncontinue;\r\nret = info->init(pmu);\r\nbreak;\r\n}\r\nput_cpu();\r\nreturn ret;\r\n}\r\nstatic int of_pmu_irq_cfg(struct arm_pmu *pmu)\r\n{\r\nint *irqs, i = 0;\r\nbool using_spi = false;\r\nstruct platform_device *pdev = pmu->plat_device;\r\nirqs = kcalloc(pdev->num_resources, sizeof(*irqs), GFP_KERNEL);\r\nif (!irqs)\r\nreturn -ENOMEM;\r\ndo {\r\nstruct device_node *dn;\r\nint cpu, irq;\r\ndn = of_parse_phandle(pdev->dev.of_node, "interrupt-affinity", i);\r\nif (!dn)\r\nbreak;\r\nirq = platform_get_irq(pdev, i);\r\nif (irq >= 0) {\r\nbool spi = !irq_is_percpu(irq);\r\nif (i > 0 && spi != using_spi) {\r\npr_err("PPI/SPI IRQ type mismatch for %s!\n",\r\ndn->name);\r\nkfree(irqs);\r\nreturn -EINVAL;\r\n}\r\nusing_spi = spi;\r\n}\r\nfor_each_possible_cpu(cpu) {\r\nstruct device_node *cpu_dn;\r\ncpu_dn = of_cpu_device_node_get(cpu);\r\nof_node_put(cpu_dn);\r\nif (dn == cpu_dn)\r\nbreak;\r\n}\r\nif (cpu >= nr_cpu_ids) {\r\npr_warn("Failed to find logical CPU for %s\n",\r\ndn->name);\r\nof_node_put(dn);\r\ncpumask_setall(&pmu->supported_cpus);\r\nbreak;\r\n}\r\nof_node_put(dn);\r\nif (using_spi) {\r\nif (i >= pdev->num_resources) {\r\nof_node_put(dn);\r\nbreak;\r\n}\r\nirqs[i] = cpu;\r\n}\r\ncpumask_set_cpu(cpu, &pmu->supported_cpus);\r\nof_node_put(dn);\r\ni++;\r\n} while (1);\r\nif (cpumask_weight(&pmu->supported_cpus) == 0)\r\ncpumask_setall(&pmu->supported_cpus);\r\nif (using_spi && i == pdev->num_resources)\r\npmu->irq_affinity = irqs;\r\nelse\r\nkfree(irqs);\r\nreturn 0;\r\n}\r\nint arm_pmu_device_probe(struct platform_device *pdev,\r\nconst struct of_device_id *of_table,\r\nconst struct pmu_probe_info *probe_table)\r\n{\r\nconst struct of_device_id *of_id;\r\nconst int (*init_fn)(struct arm_pmu *);\r\nstruct device_node *node = pdev->dev.of_node;\r\nstruct arm_pmu *pmu;\r\nint ret = -ENODEV;\r\npmu = kzalloc(sizeof(struct arm_pmu), GFP_KERNEL);\r\nif (!pmu) {\r\npr_info("failed to allocate PMU device!\n");\r\nreturn -ENOMEM;\r\n}\r\nif (!__oprofile_cpu_pmu)\r\n__oprofile_cpu_pmu = pmu;\r\npmu->plat_device = pdev;\r\nif (node && (of_id = of_match_node(of_table, pdev->dev.of_node))) {\r\ninit_fn = of_id->data;\r\nret = of_pmu_irq_cfg(pmu);\r\nif (!ret)\r\nret = init_fn(pmu);\r\n} else {\r\nret = probe_current_pmu(pmu, probe_table);\r\ncpumask_setall(&pmu->supported_cpus);\r\n}\r\nif (ret) {\r\npr_info("failed to probe PMU!\n");\r\ngoto out_free;\r\n}\r\nret = cpu_pmu_init(pmu);\r\nif (ret)\r\ngoto out_free;\r\nret = armpmu_register(pmu, -1);\r\nif (ret)\r\ngoto out_destroy;\r\nreturn 0;\r\nout_destroy:\r\ncpu_pmu_destroy(pmu);\r\nout_free:\r\npr_info("failed to register PMU devices!\n");\r\nkfree(pmu);\r\nreturn ret;\r\n}
