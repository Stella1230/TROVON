static inline void\r\n__spin_lock_mb_cache_entry(struct mb_cache_entry *ce)\r\n{\r\nspin_lock(bgl_lock_ptr(mb_cache_bg_lock,\r\nMB_CACHE_ENTRY_LOCK_INDEX(ce)));\r\n}\r\nstatic inline void\r\n__spin_unlock_mb_cache_entry(struct mb_cache_entry *ce)\r\n{\r\nspin_unlock(bgl_lock_ptr(mb_cache_bg_lock,\r\nMB_CACHE_ENTRY_LOCK_INDEX(ce)));\r\n}\r\nstatic inline int\r\n__mb_cache_entry_is_block_hashed(struct mb_cache_entry *ce)\r\n{\r\nreturn !hlist_bl_unhashed(&ce->e_block_list);\r\n}\r\nstatic inline void\r\n__mb_cache_entry_unhash_block(struct mb_cache_entry *ce)\r\n{\r\nif (__mb_cache_entry_is_block_hashed(ce))\r\nhlist_bl_del_init(&ce->e_block_list);\r\n}\r\nstatic inline int\r\n__mb_cache_entry_is_index_hashed(struct mb_cache_entry *ce)\r\n{\r\nreturn !hlist_bl_unhashed(&ce->e_index.o_list);\r\n}\r\nstatic inline void\r\n__mb_cache_entry_unhash_index(struct mb_cache_entry *ce)\r\n{\r\nif (__mb_cache_entry_is_index_hashed(ce))\r\nhlist_bl_del_init(&ce->e_index.o_list);\r\n}\r\nstatic inline void\r\n__mb_cache_entry_unhash_unlock(struct mb_cache_entry *ce)\r\n{\r\n__mb_cache_entry_unhash_index(ce);\r\nhlist_bl_unlock(ce->e_index_hash_p);\r\n__mb_cache_entry_unhash_block(ce);\r\nhlist_bl_unlock(ce->e_block_hash_p);\r\n}\r\nstatic void\r\n__mb_cache_entry_forget(struct mb_cache_entry *ce, gfp_t gfp_mask)\r\n{\r\nstruct mb_cache *cache = ce->e_cache;\r\nmb_assert(!(ce->e_used || ce->e_queued || atomic_read(&ce->e_refcnt)));\r\nkmem_cache_free(cache->c_entry_cache, ce);\r\natomic_dec(&cache->c_entry_count);\r\n}\r\nstatic void\r\n__mb_cache_entry_release(struct mb_cache_entry *ce)\r\n{\r\n__spin_lock_mb_cache_entry(ce);\r\nif (ce->e_queued)\r\nwake_up_all(&mb_cache_queue);\r\nif (ce->e_used >= MB_CACHE_WRITER)\r\nce->e_used -= MB_CACHE_WRITER;\r\nce->e_used--;\r\nif (!(ce->e_used || ce->e_queued || atomic_read(&ce->e_refcnt))) {\r\nif (!__mb_cache_entry_is_block_hashed(ce)) {\r\n__spin_unlock_mb_cache_entry(ce);\r\ngoto forget;\r\n}\r\nspin_lock(&mb_cache_spinlock);\r\nif (list_empty(&ce->e_lru_list))\r\nlist_add_tail(&ce->e_lru_list, &mb_cache_lru_list);\r\nspin_unlock(&mb_cache_spinlock);\r\n}\r\n__spin_unlock_mb_cache_entry(ce);\r\nreturn;\r\nforget:\r\nmb_assert(list_empty(&ce->e_lru_list));\r\n__mb_cache_entry_forget(ce, GFP_KERNEL);\r\n}\r\nstatic unsigned long\r\nmb_cache_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nLIST_HEAD(free_list);\r\nstruct mb_cache_entry *entry, *tmp;\r\nint nr_to_scan = sc->nr_to_scan;\r\ngfp_t gfp_mask = sc->gfp_mask;\r\nunsigned long freed = 0;\r\nmb_debug("trying to free %d entries", nr_to_scan);\r\nspin_lock(&mb_cache_spinlock);\r\nwhile ((nr_to_scan-- > 0) && !list_empty(&mb_cache_lru_list)) {\r\nstruct mb_cache_entry *ce =\r\nlist_entry(mb_cache_lru_list.next,\r\nstruct mb_cache_entry, e_lru_list);\r\nlist_del_init(&ce->e_lru_list);\r\nif (ce->e_used || ce->e_queued || atomic_read(&ce->e_refcnt))\r\ncontinue;\r\nspin_unlock(&mb_cache_spinlock);\r\nhlist_bl_lock(ce->e_block_hash_p);\r\nhlist_bl_lock(ce->e_index_hash_p);\r\nif (ce->e_used || ce->e_queued || atomic_read(&ce->e_refcnt) ||\r\n!list_empty(&ce->e_lru_list)) {\r\nhlist_bl_unlock(ce->e_index_hash_p);\r\nhlist_bl_unlock(ce->e_block_hash_p);\r\nspin_lock(&mb_cache_spinlock);\r\ncontinue;\r\n}\r\n__mb_cache_entry_unhash_unlock(ce);\r\nlist_add_tail(&ce->e_lru_list, &free_list);\r\nspin_lock(&mb_cache_spinlock);\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\nlist_for_each_entry_safe(entry, tmp, &free_list, e_lru_list) {\r\n__mb_cache_entry_forget(entry, gfp_mask);\r\nfreed++;\r\n}\r\nreturn freed;\r\n}\r\nstatic unsigned long\r\nmb_cache_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nstruct mb_cache *cache;\r\nunsigned long count = 0;\r\nspin_lock(&mb_cache_spinlock);\r\nlist_for_each_entry(cache, &mb_cache_list, c_cache_list) {\r\nmb_debug("cache %s (%d)", cache->c_name,\r\natomic_read(&cache->c_entry_count));\r\ncount += atomic_read(&cache->c_entry_count);\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\nreturn vfs_pressure_ratio(count);\r\n}\r\nstruct mb_cache *\r\nmb_cache_create(const char *name, int bucket_bits)\r\n{\r\nint n, bucket_count = 1 << bucket_bits;\r\nstruct mb_cache *cache = NULL;\r\nif (!mb_cache_bg_lock) {\r\nmb_cache_bg_lock = kmalloc(sizeof(struct blockgroup_lock),\r\nGFP_KERNEL);\r\nif (!mb_cache_bg_lock)\r\nreturn NULL;\r\nbgl_lock_init(mb_cache_bg_lock);\r\n}\r\ncache = kmalloc(sizeof(struct mb_cache), GFP_KERNEL);\r\nif (!cache)\r\nreturn NULL;\r\ncache->c_name = name;\r\natomic_set(&cache->c_entry_count, 0);\r\ncache->c_bucket_bits = bucket_bits;\r\ncache->c_block_hash = kmalloc(bucket_count *\r\nsizeof(struct hlist_bl_head), GFP_KERNEL);\r\nif (!cache->c_block_hash)\r\ngoto fail;\r\nfor (n=0; n<bucket_count; n++)\r\nINIT_HLIST_BL_HEAD(&cache->c_block_hash[n]);\r\ncache->c_index_hash = kmalloc(bucket_count *\r\nsizeof(struct hlist_bl_head), GFP_KERNEL);\r\nif (!cache->c_index_hash)\r\ngoto fail;\r\nfor (n=0; n<bucket_count; n++)\r\nINIT_HLIST_BL_HEAD(&cache->c_index_hash[n]);\r\nif (!mb_cache_kmem_cache) {\r\nmb_cache_kmem_cache = kmem_cache_create(name,\r\nsizeof(struct mb_cache_entry), 0,\r\nSLAB_RECLAIM_ACCOUNT|SLAB_MEM_SPREAD, NULL);\r\nif (!mb_cache_kmem_cache)\r\ngoto fail2;\r\n}\r\ncache->c_entry_cache = mb_cache_kmem_cache;\r\ncache->c_max_entries = bucket_count << 4;\r\nspin_lock(&mb_cache_spinlock);\r\nlist_add(&cache->c_cache_list, &mb_cache_list);\r\nspin_unlock(&mb_cache_spinlock);\r\nreturn cache;\r\nfail2:\r\nkfree(cache->c_index_hash);\r\nfail:\r\nkfree(cache->c_block_hash);\r\nkfree(cache);\r\nreturn NULL;\r\n}\r\nvoid\r\nmb_cache_shrink(struct block_device *bdev)\r\n{\r\nLIST_HEAD(free_list);\r\nstruct list_head *l;\r\nstruct mb_cache_entry *ce, *tmp;\r\nl = &mb_cache_lru_list;\r\nspin_lock(&mb_cache_spinlock);\r\nwhile (!list_is_last(l, &mb_cache_lru_list)) {\r\nl = l->next;\r\nce = list_entry(l, struct mb_cache_entry, e_lru_list);\r\nif (ce->e_bdev == bdev) {\r\nlist_del_init(&ce->e_lru_list);\r\nif (ce->e_used || ce->e_queued ||\r\natomic_read(&ce->e_refcnt))\r\ncontinue;\r\nspin_unlock(&mb_cache_spinlock);\r\nhlist_bl_lock(ce->e_block_hash_p);\r\nhlist_bl_lock(ce->e_index_hash_p);\r\nif (ce->e_used || ce->e_queued ||\r\natomic_read(&ce->e_refcnt) ||\r\n!list_empty(&ce->e_lru_list)) {\r\nhlist_bl_unlock(ce->e_index_hash_p);\r\nhlist_bl_unlock(ce->e_block_hash_p);\r\nl = &mb_cache_lru_list;\r\nspin_lock(&mb_cache_spinlock);\r\ncontinue;\r\n}\r\n__mb_cache_entry_unhash_unlock(ce);\r\nmb_assert(!(ce->e_used || ce->e_queued ||\r\natomic_read(&ce->e_refcnt)));\r\nlist_add_tail(&ce->e_lru_list, &free_list);\r\nl = &mb_cache_lru_list;\r\nspin_lock(&mb_cache_spinlock);\r\n}\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\nlist_for_each_entry_safe(ce, tmp, &free_list, e_lru_list) {\r\n__mb_cache_entry_forget(ce, GFP_KERNEL);\r\n}\r\n}\r\nvoid\r\nmb_cache_destroy(struct mb_cache *cache)\r\n{\r\nLIST_HEAD(free_list);\r\nstruct mb_cache_entry *ce, *tmp;\r\nspin_lock(&mb_cache_spinlock);\r\nlist_for_each_entry_safe(ce, tmp, &mb_cache_lru_list, e_lru_list) {\r\nif (ce->e_cache == cache)\r\nlist_move_tail(&ce->e_lru_list, &free_list);\r\n}\r\nlist_del(&cache->c_cache_list);\r\nspin_unlock(&mb_cache_spinlock);\r\nlist_for_each_entry_safe(ce, tmp, &free_list, e_lru_list) {\r\nlist_del_init(&ce->e_lru_list);\r\nhlist_bl_lock(ce->e_block_hash_p);\r\nhlist_bl_lock(ce->e_index_hash_p);\r\nmb_assert(!(ce->e_used || ce->e_queued ||\r\natomic_read(&ce->e_refcnt)));\r\n__mb_cache_entry_unhash_unlock(ce);\r\n__mb_cache_entry_forget(ce, GFP_KERNEL);\r\n}\r\nif (atomic_read(&cache->c_entry_count) > 0) {\r\nmb_error("cache %s: %d orphaned entries",\r\ncache->c_name,\r\natomic_read(&cache->c_entry_count));\r\n}\r\nif (list_empty(&mb_cache_list)) {\r\nkmem_cache_destroy(mb_cache_kmem_cache);\r\nmb_cache_kmem_cache = NULL;\r\n}\r\nkfree(cache->c_index_hash);\r\nkfree(cache->c_block_hash);\r\nkfree(cache);\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_alloc(struct mb_cache *cache, gfp_t gfp_flags)\r\n{\r\nstruct mb_cache_entry *ce;\r\nif (atomic_read(&cache->c_entry_count) >= cache->c_max_entries) {\r\nstruct list_head *l;\r\nl = &mb_cache_lru_list;\r\nspin_lock(&mb_cache_spinlock);\r\nwhile (!list_is_last(l, &mb_cache_lru_list)) {\r\nl = l->next;\r\nce = list_entry(l, struct mb_cache_entry, e_lru_list);\r\nif (ce->e_cache == cache) {\r\nlist_del_init(&ce->e_lru_list);\r\nif (ce->e_used || ce->e_queued ||\r\natomic_read(&ce->e_refcnt))\r\ncontinue;\r\nspin_unlock(&mb_cache_spinlock);\r\nhlist_bl_lock(ce->e_block_hash_p);\r\nhlist_bl_lock(ce->e_index_hash_p);\r\nif (ce->e_used || ce->e_queued ||\r\natomic_read(&ce->e_refcnt) ||\r\n!list_empty(&ce->e_lru_list)) {\r\nhlist_bl_unlock(ce->e_index_hash_p);\r\nhlist_bl_unlock(ce->e_block_hash_p);\r\nl = &mb_cache_lru_list;\r\nspin_lock(&mb_cache_spinlock);\r\ncontinue;\r\n}\r\nmb_assert(list_empty(&ce->e_lru_list));\r\nmb_assert(!(ce->e_used || ce->e_queued ||\r\natomic_read(&ce->e_refcnt)));\r\n__mb_cache_entry_unhash_unlock(ce);\r\ngoto found;\r\n}\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\n}\r\nce = kmem_cache_alloc(cache->c_entry_cache, gfp_flags);\r\nif (!ce)\r\nreturn NULL;\r\natomic_inc(&cache->c_entry_count);\r\nINIT_LIST_HEAD(&ce->e_lru_list);\r\nINIT_HLIST_BL_NODE(&ce->e_block_list);\r\nINIT_HLIST_BL_NODE(&ce->e_index.o_list);\r\nce->e_cache = cache;\r\nce->e_queued = 0;\r\natomic_set(&ce->e_refcnt, 0);\r\nfound:\r\nce->e_block_hash_p = &cache->c_block_hash[0];\r\nce->e_index_hash_p = &cache->c_index_hash[0];\r\nce->e_used = 1 + MB_CACHE_WRITER;\r\nreturn ce;\r\n}\r\nint\r\nmb_cache_entry_insert(struct mb_cache_entry *ce, struct block_device *bdev,\r\nsector_t block, unsigned int key)\r\n{\r\nstruct mb_cache *cache = ce->e_cache;\r\nunsigned int bucket;\r\nstruct hlist_bl_node *l;\r\nstruct hlist_bl_head *block_hash_p;\r\nstruct hlist_bl_head *index_hash_p;\r\nstruct mb_cache_entry *lce;\r\nmb_assert(ce);\r\nbucket = hash_long((unsigned long)bdev + (block & 0xffffffff),\r\ncache->c_bucket_bits);\r\nblock_hash_p = &cache->c_block_hash[bucket];\r\nhlist_bl_lock(block_hash_p);\r\nhlist_bl_for_each_entry(lce, l, block_hash_p, e_block_list) {\r\nif (lce->e_bdev == bdev && lce->e_block == block) {\r\nhlist_bl_unlock(block_hash_p);\r\nreturn -EBUSY;\r\n}\r\n}\r\nmb_assert(!__mb_cache_entry_is_block_hashed(ce));\r\n__mb_cache_entry_unhash_block(ce);\r\n__mb_cache_entry_unhash_index(ce);\r\nce->e_bdev = bdev;\r\nce->e_block = block;\r\nce->e_block_hash_p = block_hash_p;\r\nce->e_index.o_key = key;\r\nhlist_bl_add_head(&ce->e_block_list, block_hash_p);\r\nhlist_bl_unlock(block_hash_p);\r\nbucket = hash_long(key, cache->c_bucket_bits);\r\nindex_hash_p = &cache->c_index_hash[bucket];\r\nhlist_bl_lock(index_hash_p);\r\nce->e_index_hash_p = index_hash_p;\r\nhlist_bl_add_head(&ce->e_index.o_list, index_hash_p);\r\nhlist_bl_unlock(index_hash_p);\r\nreturn 0;\r\n}\r\nvoid\r\nmb_cache_entry_release(struct mb_cache_entry *ce)\r\n{\r\n__mb_cache_entry_release(ce);\r\n}\r\nvoid\r\nmb_cache_entry_free(struct mb_cache_entry *ce)\r\n{\r\nmb_assert(ce);\r\nmb_assert(list_empty(&ce->e_lru_list));\r\nhlist_bl_lock(ce->e_index_hash_p);\r\n__mb_cache_entry_unhash_index(ce);\r\nhlist_bl_unlock(ce->e_index_hash_p);\r\nhlist_bl_lock(ce->e_block_hash_p);\r\n__mb_cache_entry_unhash_block(ce);\r\nhlist_bl_unlock(ce->e_block_hash_p);\r\n__mb_cache_entry_release(ce);\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_get(struct mb_cache *cache, struct block_device *bdev,\r\nsector_t block)\r\n{\r\nunsigned int bucket;\r\nstruct hlist_bl_node *l;\r\nstruct mb_cache_entry *ce;\r\nstruct hlist_bl_head *block_hash_p;\r\nbucket = hash_long((unsigned long)bdev + (block & 0xffffffff),\r\ncache->c_bucket_bits);\r\nblock_hash_p = &cache->c_block_hash[bucket];\r\nhlist_bl_lock(block_hash_p);\r\nhlist_bl_for_each_entry(ce, l, block_hash_p, e_block_list) {\r\nmb_assert(ce->e_block_hash_p == block_hash_p);\r\nif (ce->e_bdev == bdev && ce->e_block == block) {\r\natomic_inc(&ce->e_refcnt);\r\nhlist_bl_unlock(block_hash_p);\r\n__spin_lock_mb_cache_entry(ce);\r\natomic_dec(&ce->e_refcnt);\r\nif (ce->e_used > 0) {\r\nDEFINE_WAIT(wait);\r\nwhile (ce->e_used > 0) {\r\nce->e_queued++;\r\nprepare_to_wait(&mb_cache_queue, &wait,\r\nTASK_UNINTERRUPTIBLE);\r\n__spin_unlock_mb_cache_entry(ce);\r\nschedule();\r\n__spin_lock_mb_cache_entry(ce);\r\nce->e_queued--;\r\n}\r\nfinish_wait(&mb_cache_queue, &wait);\r\n}\r\nce->e_used += 1 + MB_CACHE_WRITER;\r\n__spin_unlock_mb_cache_entry(ce);\r\nif (!list_empty(&ce->e_lru_list)) {\r\nspin_lock(&mb_cache_spinlock);\r\nlist_del_init(&ce->e_lru_list);\r\nspin_unlock(&mb_cache_spinlock);\r\n}\r\nif (!__mb_cache_entry_is_block_hashed(ce)) {\r\n__mb_cache_entry_release(ce);\r\nreturn NULL;\r\n}\r\nreturn ce;\r\n}\r\n}\r\nhlist_bl_unlock(block_hash_p);\r\nreturn NULL;\r\n}\r\nstatic struct mb_cache_entry *\r\n__mb_cache_entry_find(struct hlist_bl_node *l, struct hlist_bl_head *head,\r\nstruct block_device *bdev, unsigned int key)\r\n{\r\nwhile (l != NULL) {\r\nstruct mb_cache_entry *ce =\r\nhlist_bl_entry(l, struct mb_cache_entry,\r\ne_index.o_list);\r\nmb_assert(ce->e_index_hash_p == head);\r\nif (ce->e_bdev == bdev && ce->e_index.o_key == key) {\r\natomic_inc(&ce->e_refcnt);\r\nhlist_bl_unlock(head);\r\n__spin_lock_mb_cache_entry(ce);\r\natomic_dec(&ce->e_refcnt);\r\nce->e_used++;\r\nif (ce->e_used >= MB_CACHE_WRITER) {\r\nDEFINE_WAIT(wait);\r\nwhile (ce->e_used >= MB_CACHE_WRITER) {\r\nce->e_queued++;\r\nprepare_to_wait(&mb_cache_queue, &wait,\r\nTASK_UNINTERRUPTIBLE);\r\n__spin_unlock_mb_cache_entry(ce);\r\nschedule();\r\n__spin_lock_mb_cache_entry(ce);\r\nce->e_queued--;\r\n}\r\nfinish_wait(&mb_cache_queue, &wait);\r\n}\r\n__spin_unlock_mb_cache_entry(ce);\r\nif (!list_empty(&ce->e_lru_list)) {\r\nspin_lock(&mb_cache_spinlock);\r\nlist_del_init(&ce->e_lru_list);\r\nspin_unlock(&mb_cache_spinlock);\r\n}\r\nif (!__mb_cache_entry_is_block_hashed(ce)) {\r\n__mb_cache_entry_release(ce);\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nreturn ce;\r\n}\r\nl = l->next;\r\n}\r\nhlist_bl_unlock(head);\r\nreturn NULL;\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_find_first(struct mb_cache *cache, struct block_device *bdev,\r\nunsigned int key)\r\n{\r\nunsigned int bucket = hash_long(key, cache->c_bucket_bits);\r\nstruct hlist_bl_node *l;\r\nstruct mb_cache_entry *ce = NULL;\r\nstruct hlist_bl_head *index_hash_p;\r\nindex_hash_p = &cache->c_index_hash[bucket];\r\nhlist_bl_lock(index_hash_p);\r\nif (!hlist_bl_empty(index_hash_p)) {\r\nl = hlist_bl_first(index_hash_p);\r\nce = __mb_cache_entry_find(l, index_hash_p, bdev, key);\r\n} else\r\nhlist_bl_unlock(index_hash_p);\r\nreturn ce;\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_find_next(struct mb_cache_entry *prev,\r\nstruct block_device *bdev, unsigned int key)\r\n{\r\nstruct mb_cache *cache = prev->e_cache;\r\nunsigned int bucket = hash_long(key, cache->c_bucket_bits);\r\nstruct hlist_bl_node *l;\r\nstruct mb_cache_entry *ce;\r\nstruct hlist_bl_head *index_hash_p;\r\nindex_hash_p = &cache->c_index_hash[bucket];\r\nmb_assert(prev->e_index_hash_p == index_hash_p);\r\nhlist_bl_lock(index_hash_p);\r\nmb_assert(!hlist_bl_empty(index_hash_p));\r\nl = prev->e_index.o_list.next;\r\nce = __mb_cache_entry_find(l, index_hash_p, bdev, key);\r\n__mb_cache_entry_release(prev);\r\nreturn ce;\r\n}\r\nstatic int __init init_mbcache(void)\r\n{\r\nregister_shrinker(&mb_cache_shrinker);\r\nreturn 0;\r\n}\r\nstatic void __exit exit_mbcache(void)\r\n{\r\nunregister_shrinker(&mb_cache_shrinker);\r\n}
