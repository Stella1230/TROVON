static void send_act_open_req(struct cxgbi_sock *csk, struct sk_buff *skb,\r\nconst struct l2t_entry *e)\r\n{\r\nunsigned int wscale = cxgbi_sock_compute_wscale(csk->rcv_win);\r\nstruct cpl_act_open_req *req = (struct cpl_act_open_req *)skb->head;\r\nskb->priority = CPL_PRIORITY_SETUP;\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ACT_OPEN_REQ, csk->atid));\r\nreq->local_port = csk->saddr.sin_port;\r\nreq->peer_port = csk->daddr.sin_port;\r\nreq->local_ip = csk->saddr.sin_addr.s_addr;\r\nreq->peer_ip = csk->daddr.sin_addr.s_addr;\r\nreq->opt0h = htonl(V_KEEP_ALIVE(1) | F_TCAM_BYPASS |\r\nV_WND_SCALE(wscale) | V_MSS_IDX(csk->mss_idx) |\r\nV_L2T_IDX(e->idx) | V_TX_CHANNEL(e->smt_idx));\r\nreq->opt0l = htonl(V_ULP_MODE(ULP2_MODE_ISCSI) |\r\nV_RCV_BUFSIZ(csk->rcv_win >> 10));\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, %pI4:%u-%pI4:%u, %u,%u,%u.\n",\r\ncsk, csk->state, csk->flags, csk->atid,\r\n&req->local_ip, ntohs(req->local_port),\r\n&req->peer_ip, ntohs(req->peer_port),\r\ncsk->mss_idx, e->idx, e->smt_idx);\r\nl2t_send(csk->cdev->lldev, skb, csk->l2t);\r\n}\r\nstatic inline void act_open_arp_failure(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\ncxgbi_sock_act_open_req_arp_failure(NULL, skb);\r\n}\r\nstatic void send_close_req(struct cxgbi_sock *csk)\r\n{\r\nstruct sk_buff *skb = csk->cpl_close;\r\nstruct cpl_close_con_req *req = (struct cpl_close_con_req *)skb->head;\r\nunsigned int tid = csk->tid;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncsk->cpl_close = NULL;\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_CLOSE_CON));\r\nreq->wr.wr_lo = htonl(V_WR_TID(tid));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_CLOSE_CON_REQ, tid));\r\nreq->rsvd = htonl(csk->write_seq);\r\ncxgbi_sock_skb_entail(csk, skb);\r\nif (csk->state >= CTP_ESTABLISHED)\r\npush_tx_frames(csk, 1);\r\n}\r\nstatic void abort_arp_failure(struct t3cdev *tdev, struct sk_buff *skb)\r\n{\r\nstruct cpl_abort_req *req = cplhdr(skb);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"t3dev 0x%p, tid %u, skb 0x%p.\n",\r\ntdev, GET_TID(req), skb);\r\nreq->cmd = CPL_ABORT_NO_RST;\r\ncxgb3_ofld_send(tdev, skb);\r\n}\r\nstatic void send_abort_req(struct cxgbi_sock *csk)\r\n{\r\nstruct sk_buff *skb = csk->cpl_abort_req;\r\nstruct cpl_abort_req *req;\r\nif (unlikely(csk->state == CTP_ABORTING || !skb))\r\nreturn;\r\ncxgbi_sock_set_state(csk, CTP_ABORTING);\r\ncxgbi_sock_set_flag(csk, CTPF_ABORT_RPL_PENDING);\r\ncxgbi_sock_purge_write_queue(csk);\r\ncsk->cpl_abort_req = NULL;\r\nreq = (struct cpl_abort_req *)skb->head;\r\nskb->priority = CPL_PRIORITY_DATA;\r\nset_arp_failure_handler(skb, abort_arp_failure);\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_REQ));\r\nreq->wr.wr_lo = htonl(V_WR_TID(csk->tid));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_ABORT_REQ, csk->tid));\r\nreq->rsvd0 = htonl(csk->snd_nxt);\r\nreq->rsvd1 = !cxgbi_sock_flag(csk, CTPF_TX_DATA_SENT);\r\nreq->cmd = CPL_ABORT_SEND_RST;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, snd_nxt %u, 0x%x.\n",\r\ncsk, csk->state, csk->flags, csk->tid, csk->snd_nxt,\r\nreq->rsvd1);\r\nl2t_send(csk->cdev->lldev, skb, csk->l2t);\r\n}\r\nstatic void send_abort_rpl(struct cxgbi_sock *csk, int rst_status)\r\n{\r\nstruct sk_buff *skb = csk->cpl_abort_rpl;\r\nstruct cpl_abort_rpl *rpl = (struct cpl_abort_rpl *)skb->head;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, status %d.\n",\r\ncsk, csk->state, csk->flags, csk->tid, rst_status);\r\ncsk->cpl_abort_rpl = NULL;\r\nskb->priority = CPL_PRIORITY_DATA;\r\nrpl->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_RPL));\r\nrpl->wr.wr_lo = htonl(V_WR_TID(csk->tid));\r\nOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_ABORT_RPL, csk->tid));\r\nrpl->cmd = rst_status;\r\ncxgb3_ofld_send(csk->cdev->lldev, skb);\r\n}\r\nstatic u32 send_rx_credits(struct cxgbi_sock *csk, u32 credits)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_rx_data_ack *req;\r\nu32 dack = F_RX_DACK_CHANGE | V_RX_DACK_MODE(1);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx,%u, credit %u, dack %u.\n",\r\ncsk, csk->state, csk->flags, csk->tid, credits, dack);\r\nskb = alloc_wr(sizeof(*req), 0, GFP_ATOMIC);\r\nif (!skb) {\r\npr_info("csk 0x%p, credit %u, OOM.\n", csk, credits);\r\nreturn 0;\r\n}\r\nreq = (struct cpl_rx_data_ack *)skb->head;\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_RX_DATA_ACK, csk->tid));\r\nreq->credit_dack = htonl(F_RX_DACK_CHANGE | V_RX_DACK_MODE(1) |\r\nV_RX_CREDITS(credits));\r\nskb->priority = CPL_PRIORITY_ACK;\r\ncxgb3_ofld_send(csk->cdev->lldev, skb);\r\nreturn credits;\r\n}\r\nstatic void init_wr_tab(unsigned int wr_len)\r\n{\r\nint i;\r\nif (skb_wrs[1])\r\nreturn;\r\nfor (i = 1; i < SKB_WR_LIST_SIZE; i++) {\r\nint sgl_len = (3 * i) / 2 + (i & 1);\r\nsgl_len += 3;\r\nskb_wrs[i] = (sgl_len <= wr_len\r\n? 1 : 1 + (sgl_len - 2) / (wr_len - 1));\r\n}\r\nwrlen = wr_len * 8;\r\n}\r\nstatic inline void make_tx_data_wr(struct cxgbi_sock *csk, struct sk_buff *skb,\r\nint len, int req_completion)\r\n{\r\nstruct tx_data_wr *req;\r\nstruct l2t_entry *l2t = csk->l2t;\r\nskb_reset_transport_header(skb);\r\nreq = (struct tx_data_wr *)__skb_push(skb, sizeof(*req));\r\nreq->wr_hi = htonl(V_WR_OP(FW_WROPCODE_OFLD_TX_DATA) |\r\n(req_completion ? F_WR_COMPL : 0));\r\nreq->wr_lo = htonl(V_WR_TID(csk->tid));\r\nreq->len = htonl(len);\r\nreq->flags = htonl(V_TX_ULP_SUBMODE(cxgbi_skcb_ulp_mode(skb)) |\r\nV_TX_SHOVE((skb_peek(&csk->write_queue) ? 0 : 1)));\r\nreq->sndseq = htonl(csk->snd_nxt);\r\nreq->param = htonl(V_TX_PORT(l2t->smt_idx));\r\nif (!cxgbi_sock_flag(csk, CTPF_TX_DATA_SENT)) {\r\nreq->flags |= htonl(V_TX_ACK_PAGES(2) | F_TX_INIT |\r\nV_TX_CPU_IDX(csk->rss_qid));\r\nreq->param |= htonl(V_TX_SNDBUF(csk->snd_win >> 15));\r\ncxgbi_sock_set_flag(csk, CTPF_TX_DATA_SENT);\r\n}\r\n}\r\nstatic void arp_failure_skb_discard(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nkfree_skb(skb);\r\n}\r\nstatic int push_tx_frames(struct cxgbi_sock *csk, int req_completion)\r\n{\r\nint total_size = 0;\r\nstruct sk_buff *skb;\r\nif (unlikely(csk->state < CTP_ESTABLISHED ||\r\ncsk->state == CTP_CLOSE_WAIT_1 || csk->state >= CTP_ABORTING)) {\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p,%u,0x%lx,%u, in closing state.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nreturn 0;\r\n}\r\nwhile (csk->wr_cred && (skb = skb_peek(&csk->write_queue)) != NULL) {\r\nint len = skb->len;\r\nint frags = skb_shinfo(skb)->nr_frags + (len != skb->data_len);\r\nint wrs_needed = skb_wrs[frags];\r\nif (wrs_needed > 1 && len + sizeof(struct tx_data_wr) <= wrlen)\r\nwrs_needed = 1;\r\nWARN_ON(frags >= SKB_WR_LIST_SIZE || wrs_needed < 1);\r\nif (csk->wr_cred < wrs_needed) {\r\nlog_debug(1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p, skb len %u/%u, frag %u, wr %d<%u.\n",\r\ncsk, skb->len, skb->data_len, frags,\r\nwrs_needed, csk->wr_cred);\r\nbreak;\r\n}\r\n__skb_unlink(skb, &csk->write_queue);\r\nskb->priority = CPL_PRIORITY_DATA;\r\nskb->csum = wrs_needed;\r\ncsk->wr_cred -= wrs_needed;\r\ncsk->wr_una_cred += wrs_needed;\r\ncxgbi_sock_enqueue_wr(csk, skb);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p, enqueue, skb len %u/%u, frag %u, wr %d, "\r\n"left %u, unack %u.\n",\r\ncsk, skb->len, skb->data_len, frags, skb->csum,\r\ncsk->wr_cred, csk->wr_una_cred);\r\nif (likely(cxgbi_skcb_test_flag(skb, SKCBF_TX_NEED_HDR))) {\r\nif ((req_completion &&\r\ncsk->wr_una_cred == wrs_needed) ||\r\ncsk->wr_una_cred >= csk->wr_max_cred / 2) {\r\nreq_completion = 1;\r\ncsk->wr_una_cred = 0;\r\n}\r\nlen += cxgbi_ulp_extra_len(cxgbi_skcb_ulp_mode(skb));\r\nmake_tx_data_wr(csk, skb, len, req_completion);\r\ncsk->snd_nxt += len;\r\ncxgbi_skcb_clear_flag(skb, SKCBF_TX_NEED_HDR);\r\n}\r\ntotal_size += skb->truesize;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p, tid 0x%x, send skb 0x%p.\n",\r\ncsk, csk->tid, skb);\r\nset_arp_failure_handler(skb, arp_failure_skb_discard);\r\nl2t_send(csk->cdev->lldev, skb, csk->l2t);\r\n}\r\nreturn total_size;\r\n}\r\nstatic inline void free_atid(struct cxgbi_sock *csk)\r\n{\r\nif (cxgbi_sock_flag(csk, CTPF_HAS_ATID)) {\r\ncxgb3_free_atid(csk->cdev->lldev, csk->atid);\r\ncxgbi_sock_clear_flag(csk, CTPF_HAS_ATID);\r\ncxgbi_sock_put(csk);\r\n}\r\n}\r\nstatic int do_act_establish(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\r\n{\r\nstruct cxgbi_sock *csk = ctx;\r\nstruct cpl_act_establish *req = cplhdr(skb);\r\nunsigned int tid = GET_TID(req);\r\nunsigned int atid = G_PASS_OPEN_TID(ntohl(req->tos_tid));\r\nu32 rcv_isn = ntohl(req->rcv_isn);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"atid 0x%x,tid 0x%x, csk 0x%p,%u,0x%lx, isn %u.\n",\r\natid, atid, csk, csk->state, csk->flags, rcv_isn);\r\ncxgbi_sock_get(csk);\r\ncxgbi_sock_set_flag(csk, CTPF_HAS_TID);\r\ncsk->tid = tid;\r\ncxgb3_insert_tid(csk->cdev->lldev, &t3_client, csk, tid);\r\nfree_atid(csk);\r\ncsk->rss_qid = G_QNUM(ntohs(skb->csum));\r\nspin_lock_bh(&csk->lock);\r\nif (csk->retry_timer.function) {\r\ndel_timer(&csk->retry_timer);\r\ncsk->retry_timer.function = NULL;\r\n}\r\nif (unlikely(csk->state != CTP_ACTIVE_OPEN))\r\npr_info("csk 0x%p,%u,0x%lx,%u, got EST.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncsk->copied_seq = csk->rcv_wup = csk->rcv_nxt = rcv_isn;\r\nif (csk->rcv_win > (M_RCV_BUFSIZ << 10))\r\ncsk->rcv_wup -= csk->rcv_win - (M_RCV_BUFSIZ << 10);\r\ncxgbi_sock_established(csk, ntohl(req->snd_isn), ntohs(req->tcp_opt));\r\nif (unlikely(cxgbi_sock_flag(csk, CTPF_ACTIVE_CLOSE_NEEDED)))\r\nsend_abort_req(csk);\r\nelse {\r\nif (skb_queue_len(&csk->write_queue))\r\npush_tx_frames(csk, 1);\r\ncxgbi_conn_tx_open(csk);\r\n}\r\nspin_unlock_bh(&csk->lock);\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int act_open_rpl_status_to_errno(int status)\r\n{\r\nswitch (status) {\r\ncase CPL_ERR_CONN_RESET:\r\nreturn -ECONNREFUSED;\r\ncase CPL_ERR_ARP_MISS:\r\nreturn -EHOSTUNREACH;\r\ncase CPL_ERR_CONN_TIMEDOUT:\r\nreturn -ETIMEDOUT;\r\ncase CPL_ERR_TCAM_FULL:\r\nreturn -ENOMEM;\r\ncase CPL_ERR_CONN_EXIST:\r\nreturn -EADDRINUSE;\r\ndefault:\r\nreturn -EIO;\r\n}\r\n}\r\nstatic void act_open_retry_timer(unsigned long data)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cxgbi_sock *csk = (struct cxgbi_sock *)data;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncxgbi_sock_get(csk);\r\nspin_lock_bh(&csk->lock);\r\nskb = alloc_wr(sizeof(struct cpl_act_open_req), 0, GFP_ATOMIC);\r\nif (!skb)\r\ncxgbi_sock_fail_act_open(csk, -ENOMEM);\r\nelse {\r\nskb->sk = (struct sock *)csk;\r\nset_arp_failure_handler(skb, act_open_arp_failure);\r\nsend_act_open_req(csk, skb, csk->l2t);\r\n}\r\nspin_unlock_bh(&csk->lock);\r\ncxgbi_sock_put(csk);\r\n}\r\nstatic int do_act_open_rpl(struct t3cdev *tdev, struct sk_buff *skb, void *ctx)\r\n{\r\nstruct cxgbi_sock *csk = ctx;\r\nstruct cpl_act_open_rpl *rpl = cplhdr(skb);\r\npr_info("csk 0x%p,%u,0x%lx,%u, status %u, %pI4:%u-%pI4:%u.\n",\r\ncsk, csk->state, csk->flags, csk->atid, rpl->status,\r\n&csk->saddr.sin_addr.s_addr, ntohs(csk->saddr.sin_port),\r\n&csk->daddr.sin_addr.s_addr, ntohs(csk->daddr.sin_port));\r\nif (rpl->status != CPL_ERR_TCAM_FULL &&\r\nrpl->status != CPL_ERR_CONN_EXIST &&\r\nrpl->status != CPL_ERR_ARP_MISS)\r\ncxgb3_queue_tid_release(tdev, GET_TID(rpl));\r\ncxgbi_sock_get(csk);\r\nspin_lock_bh(&csk->lock);\r\nif (rpl->status == CPL_ERR_CONN_EXIST &&\r\ncsk->retry_timer.function != act_open_retry_timer) {\r\ncsk->retry_timer.function = act_open_retry_timer;\r\nmod_timer(&csk->retry_timer, jiffies + HZ / 2);\r\n} else\r\ncxgbi_sock_fail_act_open(csk,\r\nact_open_rpl_status_to_errno(rpl->status));\r\nspin_unlock_bh(&csk->lock);\r\ncxgbi_sock_put(csk);\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int do_peer_close(struct t3cdev *cdev, struct sk_buff *skb, void *ctx)\r\n{\r\nstruct cxgbi_sock *csk = ctx;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncxgbi_sock_rcv_peer_close(csk);\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int do_close_con_rpl(struct t3cdev *cdev, struct sk_buff *skb,\r\nvoid *ctx)\r\n{\r\nstruct cxgbi_sock *csk = ctx;\r\nstruct cpl_close_con_rpl *rpl = cplhdr(skb);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, snxt %u.\n",\r\ncsk, csk->state, csk->flags, csk->tid, ntohl(rpl->snd_nxt));\r\ncxgbi_sock_rcv_close_conn_rpl(csk, ntohl(rpl->snd_nxt));\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int abort_status_to_errno(struct cxgbi_sock *csk, int abort_reason,\r\nint *need_rst)\r\n{\r\nswitch (abort_reason) {\r\ncase CPL_ERR_BAD_SYN:\r\ncase CPL_ERR_CONN_RESET:\r\nreturn csk->state > CTP_ESTABLISHED ? -EPIPE : -ECONNRESET;\r\ncase CPL_ERR_XMIT_TIMEDOUT:\r\ncase CPL_ERR_PERSIST_TIMEDOUT:\r\ncase CPL_ERR_FINWAIT2_TIMEDOUT:\r\ncase CPL_ERR_KEEPALIVE_TIMEDOUT:\r\nreturn -ETIMEDOUT;\r\ndefault:\r\nreturn -EIO;\r\n}\r\n}\r\nstatic int do_abort_req(struct t3cdev *cdev, struct sk_buff *skb, void *ctx)\r\n{\r\nconst struct cpl_abort_req_rss *req = cplhdr(skb);\r\nstruct cxgbi_sock *csk = ctx;\r\nint rst_status = CPL_ABORT_NO_RST;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nif (req->status == CPL_ERR_RTX_NEG_ADVICE ||\r\nreq->status == CPL_ERR_PERSIST_NEG_ADVICE) {\r\ngoto done;\r\n}\r\ncxgbi_sock_get(csk);\r\nspin_lock_bh(&csk->lock);\r\nif (!cxgbi_sock_flag(csk, CTPF_ABORT_REQ_RCVD)) {\r\ncxgbi_sock_set_flag(csk, CTPF_ABORT_REQ_RCVD);\r\ncxgbi_sock_set_state(csk, CTP_ABORTING);\r\ngoto out;\r\n}\r\ncxgbi_sock_clear_flag(csk, CTPF_ABORT_REQ_RCVD);\r\nsend_abort_rpl(csk, rst_status);\r\nif (!cxgbi_sock_flag(csk, CTPF_ABORT_RPL_PENDING)) {\r\ncsk->err = abort_status_to_errno(csk, req->status, &rst_status);\r\ncxgbi_sock_closed(csk);\r\n}\r\nout:\r\nspin_unlock_bh(&csk->lock);\r\ncxgbi_sock_put(csk);\r\ndone:\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int do_abort_rpl(struct t3cdev *cdev, struct sk_buff *skb, void *ctx)\r\n{\r\nstruct cpl_abort_rpl_rss *rpl = cplhdr(skb);\r\nstruct cxgbi_sock *csk = ctx;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"status 0x%x, csk 0x%p, s %u, 0x%lx.\n",\r\nrpl->status, csk, csk ? csk->state : 0,\r\ncsk ? csk->flags : 0UL);\r\nif (rpl->status == CPL_ERR_ABORT_FAILED)\r\ngoto rel_skb;\r\nif (csk)\r\ncxgbi_sock_rcv_abort_rpl(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int do_iscsi_hdr(struct t3cdev *t3dev, struct sk_buff *skb, void *ctx)\r\n{\r\nstruct cxgbi_sock *csk = ctx;\r\nstruct cpl_iscsi_hdr *hdr_cpl = cplhdr(skb);\r\nstruct cpl_iscsi_hdr_norss data_cpl;\r\nstruct cpl_rx_data_ddp_norss ddp_cpl;\r\nunsigned int hdr_len, data_len, status;\r\nunsigned int len;\r\nint err;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx,%u, skb 0x%p,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid, skb, skb->len);\r\nspin_lock_bh(&csk->lock);\r\nif (unlikely(csk->state >= CTP_PASSIVE_CLOSE)) {\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, bad state.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nif (csk->state != CTP_ABORTING)\r\ngoto abort_conn;\r\nelse\r\ngoto discard;\r\n}\r\ncxgbi_skcb_tcp_seq(skb) = ntohl(hdr_cpl->seq);\r\ncxgbi_skcb_flags(skb) = 0;\r\nskb_reset_transport_header(skb);\r\n__skb_pull(skb, sizeof(struct cpl_iscsi_hdr));\r\nlen = hdr_len = ntohs(hdr_cpl->len);\r\nif (skb->len <= hdr_len) {\r\npr_err("%s: tid %u, CPL_ISCSI_HDR, skb len %u < %u.\n",\r\ncsk->cdev->ports[csk->port_id]->name, csk->tid,\r\nskb->len, hdr_len);\r\ngoto abort_conn;\r\n}\r\ncxgbi_skcb_set_flag(skb, SKCBF_RX_COALESCED);\r\nerr = skb_copy_bits(skb, skb->len - sizeof(ddp_cpl), &ddp_cpl,\r\nsizeof(ddp_cpl));\r\nif (err < 0) {\r\npr_err("%s: tid %u, copy cpl_ddp %u-%zu failed %d.\n",\r\ncsk->cdev->ports[csk->port_id]->name, csk->tid,\r\nskb->len, sizeof(ddp_cpl), err);\r\ngoto abort_conn;\r\n}\r\ncxgbi_skcb_set_flag(skb, SKCBF_RX_STATUS);\r\ncxgbi_skcb_rx_pdulen(skb) = ntohs(ddp_cpl.len);\r\ncxgbi_skcb_rx_ddigest(skb) = ntohl(ddp_cpl.ulp_crc);\r\nstatus = ntohl(ddp_cpl.ddp_status);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p, skb 0x%p,%u, pdulen %u, status 0x%x.\n",\r\ncsk, skb, skb->len, cxgbi_skcb_rx_pdulen(skb), status);\r\nif (status & (1 << CPL_RX_DDP_STATUS_HCRC_SHIFT))\r\ncxgbi_skcb_set_flag(skb, SKCBF_RX_HCRC_ERR);\r\nif (status & (1 << CPL_RX_DDP_STATUS_DCRC_SHIFT))\r\ncxgbi_skcb_set_flag(skb, SKCBF_RX_DCRC_ERR);\r\nif (status & (1 << CPL_RX_DDP_STATUS_PAD_SHIFT))\r\ncxgbi_skcb_set_flag(skb, SKCBF_RX_PAD_ERR);\r\nif (skb->len > (hdr_len + sizeof(ddp_cpl))) {\r\nerr = skb_copy_bits(skb, hdr_len, &data_cpl, sizeof(data_cpl));\r\nif (err < 0) {\r\npr_err("%s: tid %u, cp %zu/%u failed %d.\n",\r\ncsk->cdev->ports[csk->port_id]->name,\r\ncsk->tid, sizeof(data_cpl), skb->len, err);\r\ngoto abort_conn;\r\n}\r\ndata_len = ntohs(data_cpl.len);\r\nlog_debug(1 << CXGBI_DBG_DDP | 1 << CXGBI_DBG_PDU_RX,\r\n"skb 0x%p, pdu not ddp'ed %u/%u, status 0x%x.\n",\r\nskb, data_len, cxgbi_skcb_rx_pdulen(skb), status);\r\nlen += sizeof(data_cpl) + data_len;\r\n} else if (status & (1 << CPL_RX_DDP_STATUS_DDP_SHIFT))\r\ncxgbi_skcb_set_flag(skb, SKCBF_RX_DATA_DDPD);\r\ncsk->rcv_nxt = ntohl(ddp_cpl.seq) + cxgbi_skcb_rx_pdulen(skb);\r\n__pskb_trim(skb, len);\r\n__skb_queue_tail(&csk->receive_queue, skb);\r\ncxgbi_conn_pdu_ready(csk);\r\nspin_unlock_bh(&csk->lock);\r\nreturn 0;\r\nabort_conn:\r\nsend_abort_req(csk);\r\ndiscard:\r\nspin_unlock_bh(&csk->lock);\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int do_wr_ack(struct t3cdev *cdev, struct sk_buff *skb, void *ctx)\r\n{\r\nstruct cxgbi_sock *csk = ctx;\r\nstruct cpl_wr_ack *hdr = cplhdr(skb);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx,%u, cr %u.\n",\r\ncsk, csk->state, csk->flags, csk->tid, ntohs(hdr->credits));\r\ncxgbi_sock_rcv_wr_ack(csk, ntohs(hdr->credits), ntohl(hdr->snd_una), 1);\r\n__kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int alloc_cpls(struct cxgbi_sock *csk)\r\n{\r\ncsk->cpl_close = alloc_wr(sizeof(struct cpl_close_con_req), 0,\r\nGFP_KERNEL);\r\nif (!csk->cpl_close)\r\nreturn -ENOMEM;\r\ncsk->cpl_abort_req = alloc_wr(sizeof(struct cpl_abort_req), 0,\r\nGFP_KERNEL);\r\nif (!csk->cpl_abort_req)\r\ngoto free_cpl_skbs;\r\ncsk->cpl_abort_rpl = alloc_wr(sizeof(struct cpl_abort_rpl), 0,\r\nGFP_KERNEL);\r\nif (!csk->cpl_abort_rpl)\r\ngoto free_cpl_skbs;\r\nreturn 0;\r\nfree_cpl_skbs:\r\ncxgbi_sock_free_cpl_skbs(csk);\r\nreturn -ENOMEM;\r\n}\r\nstatic void l2t_put(struct cxgbi_sock *csk)\r\n{\r\nstruct t3cdev *t3dev = (struct t3cdev *)csk->cdev->lldev;\r\nif (csk->l2t) {\r\nl2t_release(t3dev, csk->l2t);\r\ncsk->l2t = NULL;\r\ncxgbi_sock_put(csk);\r\n}\r\n}\r\nstatic void release_offload_resources(struct cxgbi_sock *csk)\r\n{\r\nstruct t3cdev *t3dev = (struct t3cdev *)csk->cdev->lldev;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncsk->rss_qid = 0;\r\ncxgbi_sock_free_cpl_skbs(csk);\r\nif (csk->wr_cred != csk->wr_max_cred) {\r\ncxgbi_sock_purge_wr_queue(csk);\r\ncxgbi_sock_reset_wr_list(csk);\r\n}\r\nl2t_put(csk);\r\nif (cxgbi_sock_flag(csk, CTPF_HAS_ATID))\r\nfree_atid(csk);\r\nelse if (cxgbi_sock_flag(csk, CTPF_HAS_TID)) {\r\ncxgb3_remove_tid(t3dev, (void *)csk, csk->tid);\r\ncxgbi_sock_clear_flag(csk, CTPF_HAS_TID);\r\ncxgbi_sock_put(csk);\r\n}\r\ncsk->dst = NULL;\r\ncsk->cdev = NULL;\r\n}\r\nstatic void update_address(struct cxgbi_hba *chba)\r\n{\r\nif (chba->ipv4addr) {\r\nif (chba->vdev &&\r\nchba->ipv4addr != cxgb3i_get_private_ipv4addr(chba->vdev)) {\r\ncxgb3i_set_private_ipv4addr(chba->vdev, chba->ipv4addr);\r\ncxgb3i_set_private_ipv4addr(chba->ndev, 0);\r\npr_info("%s set %pI4.\n",\r\nchba->vdev->name, &chba->ipv4addr);\r\n} else if (chba->ipv4addr !=\r\ncxgb3i_get_private_ipv4addr(chba->ndev)) {\r\ncxgb3i_set_private_ipv4addr(chba->ndev, chba->ipv4addr);\r\npr_info("%s set %pI4.\n",\r\nchba->ndev->name, &chba->ipv4addr);\r\n}\r\n} else if (cxgb3i_get_private_ipv4addr(chba->ndev)) {\r\nif (chba->vdev)\r\ncxgb3i_set_private_ipv4addr(chba->vdev, 0);\r\ncxgb3i_set_private_ipv4addr(chba->ndev, 0);\r\n}\r\n}\r\nstatic int init_act_open(struct cxgbi_sock *csk)\r\n{\r\nstruct dst_entry *dst = csk->dst;\r\nstruct cxgbi_device *cdev = csk->cdev;\r\nstruct t3cdev *t3dev = (struct t3cdev *)cdev->lldev;\r\nstruct net_device *ndev = cdev->ports[csk->port_id];\r\nstruct cxgbi_hba *chba = cdev->hbas[csk->port_id];\r\nstruct sk_buff *skb = NULL;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx.\n", csk, csk->state, csk->flags);\r\nupdate_address(chba);\r\nif (chba->ipv4addr)\r\ncsk->saddr.sin_addr.s_addr = chba->ipv4addr;\r\ncsk->rss_qid = 0;\r\ncsk->l2t = t3_l2t_get(t3dev, dst, ndev,\r\n&csk->daddr.sin_addr.s_addr);\r\nif (!csk->l2t) {\r\npr_err("NO l2t available.\n");\r\nreturn -EINVAL;\r\n}\r\ncxgbi_sock_get(csk);\r\ncsk->atid = cxgb3_alloc_atid(t3dev, &t3_client, csk);\r\nif (csk->atid < 0) {\r\npr_err("NO atid available.\n");\r\ngoto rel_resource;\r\n}\r\ncxgbi_sock_set_flag(csk, CTPF_HAS_ATID);\r\ncxgbi_sock_get(csk);\r\nskb = alloc_wr(sizeof(struct cpl_act_open_req), 0, GFP_KERNEL);\r\nif (!skb)\r\ngoto rel_resource;\r\nskb->sk = (struct sock *)csk;\r\nset_arp_failure_handler(skb, act_open_arp_failure);\r\ncsk->snd_win = cxgb3i_snd_win;\r\ncsk->rcv_win = cxgb3i_rcv_win;\r\ncsk->wr_max_cred = csk->wr_cred = T3C_DATA(t3dev)->max_wrs - 1;\r\ncsk->wr_una_cred = 0;\r\ncsk->mss_idx = cxgbi_sock_select_mss(csk, dst_mtu(dst));\r\ncxgbi_sock_reset_wr_list(csk);\r\ncsk->err = 0;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx, %pI4:%u-%pI4:%u.\n",\r\ncsk, csk->state, csk->flags,\r\n&csk->saddr.sin_addr.s_addr, ntohs(csk->saddr.sin_port),\r\n&csk->daddr.sin_addr.s_addr, ntohs(csk->daddr.sin_port));\r\ncxgbi_sock_set_state(csk, CTP_ACTIVE_OPEN);\r\nsend_act_open_req(csk, skb, csk->l2t);\r\nreturn 0;\r\nrel_resource:\r\nif (skb)\r\n__kfree_skb(skb);\r\nreturn -EINVAL;\r\n}\r\nint cxgb3i_ofld_init(struct cxgbi_device *cdev)\r\n{\r\nstruct t3cdev *t3dev = (struct t3cdev *)cdev->lldev;\r\nstruct adap_ports port;\r\nstruct ofld_page_info rx_page_info;\r\nunsigned int wr_len;\r\nint rc;\r\nif (t3dev->ctl(t3dev, GET_WR_LEN, &wr_len) < 0 ||\r\nt3dev->ctl(t3dev, GET_PORTS, &port) < 0 ||\r\nt3dev->ctl(t3dev, GET_RX_PAGE_INFO, &rx_page_info) < 0) {\r\npr_warn("t3 0x%p, offload up, ioctl failed.\n", t3dev);\r\nreturn -EINVAL;\r\n}\r\nif (cxgb3i_max_connect > CXGBI_MAX_CONN)\r\ncxgb3i_max_connect = CXGBI_MAX_CONN;\r\nrc = cxgbi_device_portmap_create(cdev, cxgb3i_sport_base,\r\ncxgb3i_max_connect);\r\nif (rc < 0)\r\nreturn rc;\r\ninit_wr_tab(wr_len);\r\ncdev->csk_release_offload_resources = release_offload_resources;\r\ncdev->csk_push_tx_frames = push_tx_frames;\r\ncdev->csk_send_abort_req = send_abort_req;\r\ncdev->csk_send_close_req = send_close_req;\r\ncdev->csk_send_rx_credits = send_rx_credits;\r\ncdev->csk_alloc_cpls = alloc_cpls;\r\ncdev->csk_init_act_open = init_act_open;\r\npr_info("cdev 0x%p, offload up, added.\n", cdev);\r\nreturn 0;\r\n}\r\nstatic inline void ulp_mem_io_set_hdr(struct sk_buff *skb, unsigned int addr)\r\n{\r\nstruct ulp_mem_io *req = (struct ulp_mem_io *)skb->head;\r\nmemset(req, 0, sizeof(*req));\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_BYPASS));\r\nreq->cmd_lock_addr = htonl(V_ULP_MEMIO_ADDR(addr >> 5) |\r\nV_ULPTX_CMD(ULP_MEM_WRITE));\r\nreq->len = htonl(V_ULP_MEMIO_DATA_LEN(PPOD_SIZE >> 5) |\r\nV_ULPTX_NFLITS((PPOD_SIZE >> 3) + 1));\r\n}\r\nstatic int ddp_set_map(struct cxgbi_sock *csk, struct cxgbi_pagepod_hdr *hdr,\r\nunsigned int idx, unsigned int npods,\r\nstruct cxgbi_gather_list *gl)\r\n{\r\nstruct cxgbi_device *cdev = csk->cdev;\r\nstruct cxgbi_ddp_info *ddp = cdev->ddp;\r\nunsigned int pm_addr = (idx << PPOD_SIZE_SHIFT) + ddp->llimit;\r\nint i;\r\nlog_debug(1 << CXGBI_DBG_DDP,\r\n"csk 0x%p, idx %u, npods %u, gl 0x%p.\n",\r\ncsk, idx, npods, gl);\r\nfor (i = 0; i < npods; i++, idx++, pm_addr += PPOD_SIZE) {\r\nstruct sk_buff *skb = alloc_wr(sizeof(struct ulp_mem_io) +\r\nPPOD_SIZE, 0, GFP_ATOMIC);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nulp_mem_io_set_hdr(skb, pm_addr);\r\ncxgbi_ddp_ppod_set((struct cxgbi_pagepod *)(skb->head +\r\nsizeof(struct ulp_mem_io)),\r\nhdr, gl, i * PPOD_PAGES_MAX);\r\nskb->priority = CPL_PRIORITY_CONTROL;\r\ncxgb3_ofld_send(cdev->lldev, skb);\r\n}\r\nreturn 0;\r\n}\r\nstatic void ddp_clear_map(struct cxgbi_hba *chba, unsigned int tag,\r\nunsigned int idx, unsigned int npods)\r\n{\r\nstruct cxgbi_device *cdev = chba->cdev;\r\nstruct cxgbi_ddp_info *ddp = cdev->ddp;\r\nunsigned int pm_addr = (idx << PPOD_SIZE_SHIFT) + ddp->llimit;\r\nint i;\r\nlog_debug(1 << CXGBI_DBG_DDP,\r\n"cdev 0x%p, idx %u, npods %u, tag 0x%x.\n",\r\ncdev, idx, npods, tag);\r\nfor (i = 0; i < npods; i++, idx++, pm_addr += PPOD_SIZE) {\r\nstruct sk_buff *skb = alloc_wr(sizeof(struct ulp_mem_io) +\r\nPPOD_SIZE, 0, GFP_ATOMIC);\r\nif (!skb) {\r\npr_err("tag 0x%x, 0x%x, %d/%u, skb OOM.\n",\r\ntag, idx, i, npods);\r\ncontinue;\r\n}\r\nulp_mem_io_set_hdr(skb, pm_addr);\r\nskb->priority = CPL_PRIORITY_CONTROL;\r\ncxgb3_ofld_send(cdev->lldev, skb);\r\n}\r\n}\r\nstatic int ddp_setup_conn_pgidx(struct cxgbi_sock *csk,\r\nunsigned int tid, int pg_idx, bool reply)\r\n{\r\nstruct sk_buff *skb = alloc_wr(sizeof(struct cpl_set_tcb_field), 0,\r\nGFP_KERNEL);\r\nstruct cpl_set_tcb_field *req;\r\nu64 val = pg_idx < DDP_PGIDX_MAX ? pg_idx : 0;\r\nlog_debug(1 << CXGBI_DBG_DDP,\r\n"csk 0x%p, tid %u, pg_idx %d.\n", csk, tid, pg_idx);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nreq = (struct cpl_set_tcb_field *)skb->head;\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, tid));\r\nreq->reply = V_NO_REPLY(reply ? 0 : 1);\r\nreq->cpu_idx = 0;\r\nreq->word = htons(31);\r\nreq->mask = cpu_to_be64(0xF0000000);\r\nreq->val = cpu_to_be64(val << 28);\r\nskb->priority = CPL_PRIORITY_CONTROL;\r\ncxgb3_ofld_send(csk->cdev->lldev, skb);\r\nreturn 0;\r\n}\r\nstatic int ddp_setup_conn_digest(struct cxgbi_sock *csk, unsigned int tid,\r\nint hcrc, int dcrc, int reply)\r\n{\r\nstruct sk_buff *skb = alloc_wr(sizeof(struct cpl_set_tcb_field), 0,\r\nGFP_KERNEL);\r\nstruct cpl_set_tcb_field *req;\r\nu64 val = (hcrc ? 1 : 0) | (dcrc ? 2 : 0);\r\nlog_debug(1 << CXGBI_DBG_DDP,\r\n"csk 0x%p, tid %u, crc %d,%d.\n", csk, tid, hcrc, dcrc);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nreq = (struct cpl_set_tcb_field *)skb->head;\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, tid));\r\nreq->reply = V_NO_REPLY(reply ? 0 : 1);\r\nreq->cpu_idx = 0;\r\nreq->word = htons(31);\r\nreq->mask = cpu_to_be64(0x0F000000);\r\nreq->val = cpu_to_be64(val << 24);\r\nskb->priority = CPL_PRIORITY_CONTROL;\r\ncxgb3_ofld_send(csk->cdev->lldev, skb);\r\nreturn 0;\r\n}\r\nstatic void t3_ddp_cleanup(struct cxgbi_device *cdev)\r\n{\r\nstruct t3cdev *tdev = (struct t3cdev *)cdev->lldev;\r\nif (cxgbi_ddp_cleanup(cdev)) {\r\npr_info("t3dev 0x%p, ulp_iscsi no more user.\n", tdev);\r\ntdev->ulp_iscsi = NULL;\r\n}\r\n}\r\nstatic int cxgb3i_ddp_init(struct cxgbi_device *cdev)\r\n{\r\nstruct t3cdev *tdev = (struct t3cdev *)cdev->lldev;\r\nstruct cxgbi_ddp_info *ddp = tdev->ulp_iscsi;\r\nstruct ulp_iscsi_info uinfo;\r\nunsigned int pgsz_factor[4];\r\nint i, err;\r\nif (ddp) {\r\nkref_get(&ddp->refcnt);\r\npr_warn("t3dev 0x%p, ddp 0x%p already set up.\n",\r\ntdev, tdev->ulp_iscsi);\r\ncdev->ddp = ddp;\r\nreturn -EALREADY;\r\n}\r\nerr = tdev->ctl(tdev, ULP_ISCSI_GET_PARAMS, &uinfo);\r\nif (err < 0) {\r\npr_err("%s, failed to get iscsi param err=%d.\n",\r\ntdev->name, err);\r\nreturn err;\r\n}\r\nerr = cxgbi_ddp_init(cdev, uinfo.llimit, uinfo.ulimit,\r\nuinfo.max_txsz, uinfo.max_rxsz);\r\nif (err < 0)\r\nreturn err;\r\nddp = cdev->ddp;\r\nuinfo.tagmask = ddp->idx_mask << PPOD_IDX_SHIFT;\r\ncxgbi_ddp_page_size_factor(pgsz_factor);\r\nfor (i = 0; i < 4; i++)\r\nuinfo.pgsz_factor[i] = pgsz_factor[i];\r\nuinfo.ulimit = uinfo.llimit + (ddp->nppods << PPOD_SIZE_SHIFT);\r\nerr = tdev->ctl(tdev, ULP_ISCSI_SET_PARAMS, &uinfo);\r\nif (err < 0) {\r\npr_warn("%s unable to set iscsi param err=%d, ddp disabled.\n",\r\ntdev->name, err);\r\ncxgbi_ddp_cleanup(cdev);\r\nreturn err;\r\n}\r\ntdev->ulp_iscsi = ddp;\r\ncdev->csk_ddp_setup_digest = ddp_setup_conn_digest;\r\ncdev->csk_ddp_setup_pgidx = ddp_setup_conn_pgidx;\r\ncdev->csk_ddp_set = ddp_set_map;\r\ncdev->csk_ddp_clear = ddp_clear_map;\r\npr_info("tdev 0x%p, nppods %u, bits %u, mask 0x%x,0x%x pkt %u/%u, "\r\n"%u/%u.\n",\r\ntdev, ddp->nppods, ddp->idx_bits, ddp->idx_mask,\r\nddp->rsvd_tag_mask, ddp->max_txsz, uinfo.max_txsz,\r\nddp->max_rxsz, uinfo.max_rxsz);\r\nreturn 0;\r\n}\r\nstatic void cxgb3i_dev_close(struct t3cdev *t3dev)\r\n{\r\nstruct cxgbi_device *cdev = cxgbi_device_find_by_lldev(t3dev);\r\nif (!cdev || cdev->flags & CXGBI_FLAG_ADAPTER_RESET) {\r\npr_info("0x%p close, f 0x%x.\n", cdev, cdev ? cdev->flags : 0);\r\nreturn;\r\n}\r\ncxgbi_device_unregister(cdev);\r\n}\r\nstatic void cxgb3i_dev_open(struct t3cdev *t3dev)\r\n{\r\nstruct cxgbi_device *cdev = cxgbi_device_find_by_lldev(t3dev);\r\nstruct adapter *adapter = tdev2adap(t3dev);\r\nint i, err;\r\nif (cdev) {\r\npr_info("0x%p, updating.\n", cdev);\r\nreturn;\r\n}\r\ncdev = cxgbi_device_register(0, adapter->params.nports);\r\nif (!cdev) {\r\npr_warn("device 0x%p register failed.\n", t3dev);\r\nreturn;\r\n}\r\ncdev->flags = CXGBI_FLAG_DEV_T3 | CXGBI_FLAG_IPV4_SET;\r\ncdev->lldev = t3dev;\r\ncdev->pdev = adapter->pdev;\r\ncdev->ports = adapter->port;\r\ncdev->nports = adapter->params.nports;\r\ncdev->mtus = adapter->params.mtus;\r\ncdev->nmtus = NMTUS;\r\ncdev->rx_credit_thres = cxgb3i_rx_credit_thres;\r\ncdev->skb_tx_rsvd = CXGB3I_TX_HEADER_LEN;\r\ncdev->skb_rx_extra = sizeof(struct cpl_iscsi_hdr_norss);\r\ncdev->dev_ddp_cleanup = t3_ddp_cleanup;\r\ncdev->itp = &cxgb3i_iscsi_transport;\r\nerr = cxgb3i_ddp_init(cdev);\r\nif (err) {\r\npr_info("0x%p ddp init failed\n", cdev);\r\ngoto err_out;\r\n}\r\nerr = cxgb3i_ofld_init(cdev);\r\nif (err) {\r\npr_info("0x%p offload init failed\n", cdev);\r\ngoto err_out;\r\n}\r\nerr = cxgbi_hbas_add(cdev, CXGB3I_MAX_LUN, CXGBI_MAX_CONN,\r\n&cxgb3i_host_template, cxgb3i_stt);\r\nif (err)\r\ngoto err_out;\r\nfor (i = 0; i < cdev->nports; i++)\r\ncdev->hbas[i]->ipv4addr =\r\ncxgb3i_get_private_ipv4addr(cdev->ports[i]);\r\npr_info("cdev 0x%p, f 0x%x, t3dev 0x%p open, err %d.\n",\r\ncdev, cdev ? cdev->flags : 0, t3dev, err);\r\nreturn;\r\nerr_out:\r\ncxgbi_device_unregister(cdev);\r\n}\r\nstatic void cxgb3i_dev_event_handler(struct t3cdev *t3dev, u32 event, u32 port)\r\n{\r\nstruct cxgbi_device *cdev = cxgbi_device_find_by_lldev(t3dev);\r\nlog_debug(1 << CXGBI_DBG_TOE,\r\n"0x%p, cdev 0x%p, event 0x%x, port 0x%x.\n",\r\nt3dev, cdev, event, port);\r\nif (!cdev)\r\nreturn;\r\nswitch (event) {\r\ncase OFFLOAD_STATUS_DOWN:\r\ncdev->flags |= CXGBI_FLAG_ADAPTER_RESET;\r\nbreak;\r\ncase OFFLOAD_STATUS_UP:\r\ncdev->flags &= ~CXGBI_FLAG_ADAPTER_RESET;\r\nbreak;\r\n}\r\n}\r\nstatic int __init cxgb3i_init_module(void)\r\n{\r\nint rc;\r\nprintk(KERN_INFO "%s", version);\r\nrc = cxgbi_iscsi_init(&cxgb3i_iscsi_transport, &cxgb3i_stt);\r\nif (rc < 0)\r\nreturn rc;\r\ncxgb3_register_client(&t3_client);\r\nreturn 0;\r\n}\r\nstatic void __exit cxgb3i_exit_module(void)\r\n{\r\ncxgb3_unregister_client(&t3_client);\r\ncxgbi_device_unregister_all(CXGBI_FLAG_DEV_T3);\r\ncxgbi_iscsi_cleanup(&cxgb3i_iscsi_transport, &cxgb3i_stt);\r\n}
