int xprt_register_transport(struct xprt_class *transport)\r\n{\r\nstruct xprt_class *t;\r\nint result;\r\nresult = -EEXIST;\r\nspin_lock(&xprt_list_lock);\r\nlist_for_each_entry(t, &xprt_list, list) {\r\nif (t->ident == transport->ident)\r\ngoto out;\r\n}\r\nlist_add_tail(&transport->list, &xprt_list);\r\nprintk(KERN_INFO "RPC: Registered %s transport module.\n",\r\ntransport->name);\r\nresult = 0;\r\nout:\r\nspin_unlock(&xprt_list_lock);\r\nreturn result;\r\n}\r\nint xprt_unregister_transport(struct xprt_class *transport)\r\n{\r\nstruct xprt_class *t;\r\nint result;\r\nresult = 0;\r\nspin_lock(&xprt_list_lock);\r\nlist_for_each_entry(t, &xprt_list, list) {\r\nif (t == transport) {\r\nprintk(KERN_INFO\r\n"RPC: Unregistered %s transport module.\n",\r\ntransport->name);\r\nlist_del_init(&transport->list);\r\ngoto out;\r\n}\r\n}\r\nresult = -ENOENT;\r\nout:\r\nspin_unlock(&xprt_list_lock);\r\nreturn result;\r\n}\r\nint xprt_load_transport(const char *transport_name)\r\n{\r\nstruct xprt_class *t;\r\nint result;\r\nresult = 0;\r\nspin_lock(&xprt_list_lock);\r\nlist_for_each_entry(t, &xprt_list, list) {\r\nif (strcmp(t->name, transport_name) == 0) {\r\nspin_unlock(&xprt_list_lock);\r\ngoto out;\r\n}\r\n}\r\nspin_unlock(&xprt_list_lock);\r\nresult = request_module("xprt%s", transport_name);\r\nout:\r\nreturn result;\r\n}\r\nint xprt_reserve_xprt(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nint priority;\r\nif (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {\r\nif (task == xprt->snd_task)\r\nreturn 1;\r\ngoto out_sleep;\r\n}\r\nxprt->snd_task = task;\r\nif (req != NULL)\r\nreq->rq_ntrans++;\r\nreturn 1;\r\nout_sleep:\r\ndprintk("RPC: %5u failed to lock transport %p\n",\r\ntask->tk_pid, xprt);\r\ntask->tk_timeout = 0;\r\ntask->tk_status = -EAGAIN;\r\nif (req == NULL)\r\npriority = RPC_PRIORITY_LOW;\r\nelse if (!req->rq_ntrans)\r\npriority = RPC_PRIORITY_NORMAL;\r\nelse\r\npriority = RPC_PRIORITY_HIGH;\r\nrpc_sleep_on_priority(&xprt->sending, task, NULL, priority);\r\nreturn 0;\r\n}\r\nstatic void xprt_clear_locked(struct rpc_xprt *xprt)\r\n{\r\nxprt->snd_task = NULL;\r\nif (!test_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\r\nsmp_mb__before_atomic();\r\nclear_bit(XPRT_LOCKED, &xprt->state);\r\nsmp_mb__after_atomic();\r\n} else\r\nqueue_work(rpciod_workqueue, &xprt->task_cleanup);\r\n}\r\nint xprt_reserve_xprt_cong(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nint priority;\r\nif (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {\r\nif (task == xprt->snd_task)\r\nreturn 1;\r\ngoto out_sleep;\r\n}\r\nif (req == NULL) {\r\nxprt->snd_task = task;\r\nreturn 1;\r\n}\r\nif (__xprt_get_cong(xprt, task)) {\r\nxprt->snd_task = task;\r\nreq->rq_ntrans++;\r\nreturn 1;\r\n}\r\nxprt_clear_locked(xprt);\r\nout_sleep:\r\nif (req)\r\n__xprt_put_cong(xprt, req);\r\ndprintk("RPC: %5u failed to lock transport %p\n", task->tk_pid, xprt);\r\ntask->tk_timeout = 0;\r\ntask->tk_status = -EAGAIN;\r\nif (req == NULL)\r\npriority = RPC_PRIORITY_LOW;\r\nelse if (!req->rq_ntrans)\r\npriority = RPC_PRIORITY_NORMAL;\r\nelse\r\npriority = RPC_PRIORITY_HIGH;\r\nrpc_sleep_on_priority(&xprt->sending, task, NULL, priority);\r\nreturn 0;\r\n}\r\nstatic inline int xprt_lock_write(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nint retval;\r\nspin_lock_bh(&xprt->transport_lock);\r\nretval = xprt->ops->reserve_xprt(xprt, task);\r\nspin_unlock_bh(&xprt->transport_lock);\r\nreturn retval;\r\n}\r\nstatic bool __xprt_lock_write_func(struct rpc_task *task, void *data)\r\n{\r\nstruct rpc_xprt *xprt = data;\r\nstruct rpc_rqst *req;\r\nreq = task->tk_rqstp;\r\nxprt->snd_task = task;\r\nif (req)\r\nreq->rq_ntrans++;\r\nreturn true;\r\n}\r\nstatic void __xprt_lock_write_next(struct rpc_xprt *xprt)\r\n{\r\nif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\r\nreturn;\r\nif (rpc_wake_up_first(&xprt->sending, __xprt_lock_write_func, xprt))\r\nreturn;\r\nxprt_clear_locked(xprt);\r\n}\r\nstatic bool __xprt_lock_write_cong_func(struct rpc_task *task, void *data)\r\n{\r\nstruct rpc_xprt *xprt = data;\r\nstruct rpc_rqst *req;\r\nreq = task->tk_rqstp;\r\nif (req == NULL) {\r\nxprt->snd_task = task;\r\nreturn true;\r\n}\r\nif (__xprt_get_cong(xprt, task)) {\r\nxprt->snd_task = task;\r\nreq->rq_ntrans++;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void __xprt_lock_write_next_cong(struct rpc_xprt *xprt)\r\n{\r\nif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\r\nreturn;\r\nif (RPCXPRT_CONGESTED(xprt))\r\ngoto out_unlock;\r\nif (rpc_wake_up_first(&xprt->sending, __xprt_lock_write_cong_func, xprt))\r\nreturn;\r\nout_unlock:\r\nxprt_clear_locked(xprt);\r\n}\r\nstatic void xprt_task_clear_bytes_sent(struct rpc_task *task)\r\n{\r\nif (task != NULL) {\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nif (req != NULL)\r\nreq->rq_bytes_sent = 0;\r\n}\r\n}\r\nvoid xprt_release_xprt(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nif (xprt->snd_task == task) {\r\nxprt_task_clear_bytes_sent(task);\r\nxprt_clear_locked(xprt);\r\n__xprt_lock_write_next(xprt);\r\n}\r\n}\r\nvoid xprt_release_xprt_cong(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nif (xprt->snd_task == task) {\r\nxprt_task_clear_bytes_sent(task);\r\nxprt_clear_locked(xprt);\r\n__xprt_lock_write_next_cong(xprt);\r\n}\r\n}\r\nstatic inline void xprt_release_write(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nspin_lock_bh(&xprt->transport_lock);\r\nxprt->ops->release_xprt(xprt, task);\r\nspin_unlock_bh(&xprt->transport_lock);\r\n}\r\nstatic int\r\n__xprt_get_cong(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nif (req->rq_cong)\r\nreturn 1;\r\ndprintk("RPC: %5u xprt_cwnd_limited cong = %lu cwnd = %lu\n",\r\ntask->tk_pid, xprt->cong, xprt->cwnd);\r\nif (RPCXPRT_CONGESTED(xprt))\r\nreturn 0;\r\nreq->rq_cong = 1;\r\nxprt->cong += RPC_CWNDSCALE;\r\nreturn 1;\r\n}\r\nstatic void\r\n__xprt_put_cong(struct rpc_xprt *xprt, struct rpc_rqst *req)\r\n{\r\nif (!req->rq_cong)\r\nreturn;\r\nreq->rq_cong = 0;\r\nxprt->cong -= RPC_CWNDSCALE;\r\n__xprt_lock_write_next_cong(xprt);\r\n}\r\nvoid xprt_release_rqst_cong(struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\n__xprt_put_cong(req->rq_xprt, req);\r\n}\r\nvoid xprt_adjust_cwnd(struct rpc_xprt *xprt, struct rpc_task *task, int result)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nunsigned long cwnd = xprt->cwnd;\r\nif (result >= 0 && cwnd <= xprt->cong) {\r\ncwnd += (RPC_CWNDSCALE * RPC_CWNDSCALE + (cwnd >> 1)) / cwnd;\r\nif (cwnd > RPC_MAXCWND(xprt))\r\ncwnd = RPC_MAXCWND(xprt);\r\n__xprt_lock_write_next_cong(xprt);\r\n} else if (result == -ETIMEDOUT) {\r\ncwnd >>= 1;\r\nif (cwnd < RPC_CWNDSCALE)\r\ncwnd = RPC_CWNDSCALE;\r\n}\r\ndprintk("RPC: cong %ld, cwnd was %ld, now %ld\n",\r\nxprt->cong, xprt->cwnd, cwnd);\r\nxprt->cwnd = cwnd;\r\n__xprt_put_cong(xprt, req);\r\n}\r\nvoid xprt_wake_pending_tasks(struct rpc_xprt *xprt, int status)\r\n{\r\nif (status < 0)\r\nrpc_wake_up_status(&xprt->pending, status);\r\nelse\r\nrpc_wake_up(&xprt->pending);\r\n}\r\nvoid xprt_wait_for_buffer_space(struct rpc_task *task, rpc_action action)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\ntask->tk_timeout = RPC_IS_SOFT(task) ? req->rq_timeout : 0;\r\nrpc_sleep_on(&xprt->pending, task, action);\r\n}\r\nvoid xprt_write_space(struct rpc_xprt *xprt)\r\n{\r\nspin_lock_bh(&xprt->transport_lock);\r\nif (xprt->snd_task) {\r\ndprintk("RPC: write space: waking waiting task on "\r\n"xprt %p\n", xprt);\r\nrpc_wake_up_queued_task(&xprt->pending, xprt->snd_task);\r\n}\r\nspin_unlock_bh(&xprt->transport_lock);\r\n}\r\nvoid xprt_set_retrans_timeout_def(struct rpc_task *task)\r\n{\r\ntask->tk_timeout = task->tk_rqstp->rq_timeout;\r\n}\r\nvoid xprt_set_retrans_timeout_rtt(struct rpc_task *task)\r\n{\r\nint timer = task->tk_msg.rpc_proc->p_timer;\r\nstruct rpc_clnt *clnt = task->tk_client;\r\nstruct rpc_rtt *rtt = clnt->cl_rtt;\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nunsigned long max_timeout = clnt->cl_timeout->to_maxval;\r\ntask->tk_timeout = rpc_calc_rto(rtt, timer);\r\ntask->tk_timeout <<= rpc_ntimeo(rtt, timer) + req->rq_retries;\r\nif (task->tk_timeout > max_timeout || task->tk_timeout == 0)\r\ntask->tk_timeout = max_timeout;\r\n}\r\nstatic void xprt_reset_majortimeo(struct rpc_rqst *req)\r\n{\r\nconst struct rpc_timeout *to = req->rq_task->tk_client->cl_timeout;\r\nreq->rq_majortimeo = req->rq_timeout;\r\nif (to->to_exponential)\r\nreq->rq_majortimeo <<= to->to_retries;\r\nelse\r\nreq->rq_majortimeo += to->to_increment * to->to_retries;\r\nif (req->rq_majortimeo > to->to_maxval || req->rq_majortimeo == 0)\r\nreq->rq_majortimeo = to->to_maxval;\r\nreq->rq_majortimeo += jiffies;\r\n}\r\nint xprt_adjust_timeout(struct rpc_rqst *req)\r\n{\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\nconst struct rpc_timeout *to = req->rq_task->tk_client->cl_timeout;\r\nint status = 0;\r\nif (time_before(jiffies, req->rq_majortimeo)) {\r\nif (to->to_exponential)\r\nreq->rq_timeout <<= 1;\r\nelse\r\nreq->rq_timeout += to->to_increment;\r\nif (to->to_maxval && req->rq_timeout >= to->to_maxval)\r\nreq->rq_timeout = to->to_maxval;\r\nreq->rq_retries++;\r\n} else {\r\nreq->rq_timeout = to->to_initval;\r\nreq->rq_retries = 0;\r\nxprt_reset_majortimeo(req);\r\nspin_lock_bh(&xprt->transport_lock);\r\nrpc_init_rtt(req->rq_task->tk_client->cl_rtt, to->to_initval);\r\nspin_unlock_bh(&xprt->transport_lock);\r\nstatus = -ETIMEDOUT;\r\n}\r\nif (req->rq_timeout == 0) {\r\nprintk(KERN_WARNING "xprt_adjust_timeout: rq_timeout = 0!\n");\r\nreq->rq_timeout = 5 * HZ;\r\n}\r\nreturn status;\r\n}\r\nstatic void xprt_autoclose(struct work_struct *work)\r\n{\r\nstruct rpc_xprt *xprt =\r\ncontainer_of(work, struct rpc_xprt, task_cleanup);\r\nclear_bit(XPRT_CLOSE_WAIT, &xprt->state);\r\nxprt->ops->close(xprt);\r\nxprt_release_write(xprt, NULL);\r\nwake_up_bit(&xprt->state, XPRT_LOCKED);\r\n}\r\nvoid xprt_disconnect_done(struct rpc_xprt *xprt)\r\n{\r\ndprintk("RPC: disconnected transport %p\n", xprt);\r\nspin_lock_bh(&xprt->transport_lock);\r\nxprt_clear_connected(xprt);\r\nxprt_wake_pending_tasks(xprt, -EAGAIN);\r\nspin_unlock_bh(&xprt->transport_lock);\r\n}\r\nvoid xprt_force_disconnect(struct rpc_xprt *xprt)\r\n{\r\nspin_lock_bh(&xprt->transport_lock);\r\nset_bit(XPRT_CLOSE_WAIT, &xprt->state);\r\nif (test_and_set_bit(XPRT_LOCKED, &xprt->state) == 0)\r\nqueue_work(rpciod_workqueue, &xprt->task_cleanup);\r\nxprt_wake_pending_tasks(xprt, -EAGAIN);\r\nspin_unlock_bh(&xprt->transport_lock);\r\n}\r\nvoid xprt_conditional_disconnect(struct rpc_xprt *xprt, unsigned int cookie)\r\n{\r\nspin_lock_bh(&xprt->transport_lock);\r\nif (cookie != xprt->connect_cookie)\r\ngoto out;\r\nif (test_bit(XPRT_CLOSING, &xprt->state) || !xprt_connected(xprt))\r\ngoto out;\r\nset_bit(XPRT_CLOSE_WAIT, &xprt->state);\r\nif (test_and_set_bit(XPRT_LOCKED, &xprt->state) == 0)\r\nqueue_work(rpciod_workqueue, &xprt->task_cleanup);\r\nxprt_wake_pending_tasks(xprt, -EAGAIN);\r\nout:\r\nspin_unlock_bh(&xprt->transport_lock);\r\n}\r\nstatic void\r\nxprt_init_autodisconnect(unsigned long data)\r\n{\r\nstruct rpc_xprt *xprt = (struct rpc_xprt *)data;\r\nspin_lock(&xprt->transport_lock);\r\nif (!list_empty(&xprt->recv))\r\ngoto out_abort;\r\nif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\r\ngoto out_abort;\r\nspin_unlock(&xprt->transport_lock);\r\nqueue_work(rpciod_workqueue, &xprt->task_cleanup);\r\nreturn;\r\nout_abort:\r\nspin_unlock(&xprt->transport_lock);\r\n}\r\nbool xprt_lock_connect(struct rpc_xprt *xprt,\r\nstruct rpc_task *task,\r\nvoid *cookie)\r\n{\r\nbool ret = false;\r\nspin_lock_bh(&xprt->transport_lock);\r\nif (!test_bit(XPRT_LOCKED, &xprt->state))\r\ngoto out;\r\nif (xprt->snd_task != task)\r\ngoto out;\r\nxprt_task_clear_bytes_sent(task);\r\nxprt->snd_task = cookie;\r\nret = true;\r\nout:\r\nspin_unlock_bh(&xprt->transport_lock);\r\nreturn ret;\r\n}\r\nvoid xprt_unlock_connect(struct rpc_xprt *xprt, void *cookie)\r\n{\r\nspin_lock_bh(&xprt->transport_lock);\r\nif (xprt->snd_task != cookie)\r\ngoto out;\r\nif (!test_bit(XPRT_LOCKED, &xprt->state))\r\ngoto out;\r\nxprt->snd_task =NULL;\r\nxprt->ops->release_xprt(xprt, NULL);\r\nout:\r\nspin_unlock_bh(&xprt->transport_lock);\r\nwake_up_bit(&xprt->state, XPRT_LOCKED);\r\n}\r\nvoid xprt_connect(struct rpc_task *task)\r\n{\r\nstruct rpc_xprt *xprt = task->tk_rqstp->rq_xprt;\r\ndprintk("RPC: %5u xprt_connect xprt %p %s connected\n", task->tk_pid,\r\nxprt, (xprt_connected(xprt) ? "is" : "is not"));\r\nif (!xprt_bound(xprt)) {\r\ntask->tk_status = -EAGAIN;\r\nreturn;\r\n}\r\nif (!xprt_lock_write(xprt, task))\r\nreturn;\r\nif (test_and_clear_bit(XPRT_CLOSE_WAIT, &xprt->state))\r\nxprt->ops->close(xprt);\r\nif (!xprt_connected(xprt)) {\r\ntask->tk_rqstp->rq_bytes_sent = 0;\r\ntask->tk_timeout = task->tk_rqstp->rq_timeout;\r\nrpc_sleep_on(&xprt->pending, task, xprt_connect_status);\r\nif (test_bit(XPRT_CLOSING, &xprt->state))\r\nreturn;\r\nif (xprt_test_and_set_connecting(xprt))\r\nreturn;\r\nxprt->stat.connect_start = jiffies;\r\nxprt->ops->connect(xprt, task);\r\n}\r\nxprt_release_write(xprt, task);\r\n}\r\nstatic void xprt_connect_status(struct rpc_task *task)\r\n{\r\nstruct rpc_xprt *xprt = task->tk_rqstp->rq_xprt;\r\nif (task->tk_status == 0) {\r\nxprt->stat.connect_count++;\r\nxprt->stat.connect_time += (long)jiffies - xprt->stat.connect_start;\r\ndprintk("RPC: %5u xprt_connect_status: connection established\n",\r\ntask->tk_pid);\r\nreturn;\r\n}\r\nswitch (task->tk_status) {\r\ncase -ECONNREFUSED:\r\ncase -ECONNRESET:\r\ncase -ECONNABORTED:\r\ncase -ENETUNREACH:\r\ncase -EHOSTUNREACH:\r\ncase -EPIPE:\r\ncase -EAGAIN:\r\ndprintk("RPC: %5u xprt_connect_status: retrying\n", task->tk_pid);\r\nbreak;\r\ncase -ETIMEDOUT:\r\ndprintk("RPC: %5u xprt_connect_status: connect attempt timed "\r\n"out\n", task->tk_pid);\r\nbreak;\r\ndefault:\r\ndprintk("RPC: %5u xprt_connect_status: error %d connecting to "\r\n"server %s\n", task->tk_pid, -task->tk_status,\r\nxprt->servername);\r\ntask->tk_status = -EIO;\r\n}\r\n}\r\nstruct rpc_rqst *xprt_lookup_rqst(struct rpc_xprt *xprt, __be32 xid)\r\n{\r\nstruct rpc_rqst *entry;\r\nlist_for_each_entry(entry, &xprt->recv, rq_list)\r\nif (entry->rq_xid == xid) {\r\ntrace_xprt_lookup_rqst(xprt, xid, 0);\r\nreturn entry;\r\n}\r\ndprintk("RPC: xprt_lookup_rqst did not find xid %08x\n",\r\nntohl(xid));\r\ntrace_xprt_lookup_rqst(xprt, xid, -ENOENT);\r\nxprt->stat.bad_xids++;\r\nreturn NULL;\r\n}\r\nstatic void xprt_update_rtt(struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nstruct rpc_rtt *rtt = task->tk_client->cl_rtt;\r\nunsigned int timer = task->tk_msg.rpc_proc->p_timer;\r\nlong m = usecs_to_jiffies(ktime_to_us(req->rq_rtt));\r\nif (timer) {\r\nif (req->rq_ntrans == 1)\r\nrpc_update_rtt(rtt, timer, m);\r\nrpc_set_timeo(rtt, timer, req->rq_ntrans - 1);\r\n}\r\n}\r\nvoid xprt_complete_rqst(struct rpc_task *task, int copied)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\ndprintk("RPC: %5u xid %08x complete (%d bytes received)\n",\r\ntask->tk_pid, ntohl(req->rq_xid), copied);\r\ntrace_xprt_complete_rqst(xprt, req->rq_xid, copied);\r\nxprt->stat.recvs++;\r\nreq->rq_rtt = ktime_sub(ktime_get(), req->rq_xtime);\r\nif (xprt->ops->timer != NULL)\r\nxprt_update_rtt(task);\r\nlist_del_init(&req->rq_list);\r\nreq->rq_private_buf.len = copied;\r\nsmp_wmb();\r\nreq->rq_reply_bytes_recvd = copied;\r\nrpc_wake_up_queued_task(&xprt->pending, task);\r\n}\r\nstatic void xprt_timer(struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\nif (task->tk_status != -ETIMEDOUT)\r\nreturn;\r\ndprintk("RPC: %5u xprt_timer\n", task->tk_pid);\r\nspin_lock_bh(&xprt->transport_lock);\r\nif (!req->rq_reply_bytes_recvd) {\r\nif (xprt->ops->timer)\r\nxprt->ops->timer(xprt, task);\r\n} else\r\ntask->tk_status = 0;\r\nspin_unlock_bh(&xprt->transport_lock);\r\n}\r\nstatic inline int xprt_has_timer(struct rpc_xprt *xprt)\r\n{\r\nreturn xprt->idle_timeout != 0;\r\n}\r\nbool xprt_prepare_transmit(struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\nbool ret = false;\r\ndprintk("RPC: %5u xprt_prepare_transmit\n", task->tk_pid);\r\nspin_lock_bh(&xprt->transport_lock);\r\nif (!req->rq_bytes_sent) {\r\nif (req->rq_reply_bytes_recvd) {\r\ntask->tk_status = req->rq_reply_bytes_recvd;\r\ngoto out_unlock;\r\n}\r\nif ((task->tk_flags & RPC_TASK_NO_RETRANS_TIMEOUT)\r\n&& xprt_connected(xprt)\r\n&& req->rq_connect_cookie == xprt->connect_cookie) {\r\nxprt->ops->set_retrans_timeout(task);\r\nrpc_sleep_on(&xprt->pending, task, xprt_timer);\r\ngoto out_unlock;\r\n}\r\n}\r\nif (!xprt->ops->reserve_xprt(xprt, task)) {\r\ntask->tk_status = -EAGAIN;\r\ngoto out_unlock;\r\n}\r\nret = true;\r\nout_unlock:\r\nspin_unlock_bh(&xprt->transport_lock);\r\nreturn ret;\r\n}\r\nvoid xprt_end_transmit(struct rpc_task *task)\r\n{\r\nxprt_release_write(task->tk_rqstp->rq_xprt, task);\r\n}\r\nvoid xprt_transmit(struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\nint status, numreqs;\r\ndprintk("RPC: %5u xprt_transmit(%u)\n", task->tk_pid, req->rq_slen);\r\nif (!req->rq_reply_bytes_recvd) {\r\nif (list_empty(&req->rq_list) && rpc_reply_expected(task)) {\r\nspin_lock_bh(&xprt->transport_lock);\r\nmemcpy(&req->rq_private_buf, &req->rq_rcv_buf,\r\nsizeof(req->rq_private_buf));\r\nlist_add_tail(&req->rq_list, &xprt->recv);\r\nspin_unlock_bh(&xprt->transport_lock);\r\nxprt_reset_majortimeo(req);\r\ndel_singleshot_timer_sync(&xprt->timer);\r\n}\r\n} else if (!req->rq_bytes_sent)\r\nreturn;\r\nreq->rq_xtime = ktime_get();\r\nstatus = xprt->ops->send_request(task);\r\ntrace_xprt_transmit(xprt, req->rq_xid, status);\r\nif (status != 0) {\r\ntask->tk_status = status;\r\nreturn;\r\n}\r\nxprt_inject_disconnect(xprt);\r\ndprintk("RPC: %5u xmit complete\n", task->tk_pid);\r\ntask->tk_flags |= RPC_TASK_SENT;\r\nspin_lock_bh(&xprt->transport_lock);\r\nxprt->ops->set_retrans_timeout(task);\r\nnumreqs = atomic_read(&xprt->num_reqs);\r\nif (numreqs > xprt->stat.max_slots)\r\nxprt->stat.max_slots = numreqs;\r\nxprt->stat.sends++;\r\nxprt->stat.req_u += xprt->stat.sends - xprt->stat.recvs;\r\nxprt->stat.bklog_u += xprt->backlog.qlen;\r\nxprt->stat.sending_u += xprt->sending.qlen;\r\nxprt->stat.pending_u += xprt->pending.qlen;\r\nif (!xprt_connected(xprt))\r\ntask->tk_status = -ENOTCONN;\r\nelse {\r\nif (!req->rq_reply_bytes_recvd && rpc_reply_expected(task))\r\nrpc_sleep_on(&xprt->pending, task, xprt_timer);\r\nreq->rq_connect_cookie = xprt->connect_cookie;\r\n}\r\nspin_unlock_bh(&xprt->transport_lock);\r\n}\r\nstatic void xprt_add_backlog(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nset_bit(XPRT_CONGESTED, &xprt->state);\r\nrpc_sleep_on(&xprt->backlog, task, NULL);\r\n}\r\nstatic void xprt_wake_up_backlog(struct rpc_xprt *xprt)\r\n{\r\nif (rpc_wake_up_next(&xprt->backlog) == NULL)\r\nclear_bit(XPRT_CONGESTED, &xprt->state);\r\n}\r\nstatic bool xprt_throttle_congested(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nbool ret = false;\r\nif (!test_bit(XPRT_CONGESTED, &xprt->state))\r\ngoto out;\r\nspin_lock(&xprt->reserve_lock);\r\nif (test_bit(XPRT_CONGESTED, &xprt->state)) {\r\nrpc_sleep_on(&xprt->backlog, task, NULL);\r\nret = true;\r\n}\r\nspin_unlock(&xprt->reserve_lock);\r\nout:\r\nreturn ret;\r\n}\r\nstatic struct rpc_rqst *xprt_dynamic_alloc_slot(struct rpc_xprt *xprt, gfp_t gfp_flags)\r\n{\r\nstruct rpc_rqst *req = ERR_PTR(-EAGAIN);\r\nif (!atomic_add_unless(&xprt->num_reqs, 1, xprt->max_reqs))\r\ngoto out;\r\nreq = kzalloc(sizeof(struct rpc_rqst), gfp_flags);\r\nif (req != NULL)\r\ngoto out;\r\natomic_dec(&xprt->num_reqs);\r\nreq = ERR_PTR(-ENOMEM);\r\nout:\r\nreturn req;\r\n}\r\nstatic bool xprt_dynamic_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *req)\r\n{\r\nif (atomic_add_unless(&xprt->num_reqs, -1, xprt->min_reqs)) {\r\nkfree(req);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nvoid xprt_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nstruct rpc_rqst *req;\r\nspin_lock(&xprt->reserve_lock);\r\nif (!list_empty(&xprt->free)) {\r\nreq = list_entry(xprt->free.next, struct rpc_rqst, rq_list);\r\nlist_del(&req->rq_list);\r\ngoto out_init_req;\r\n}\r\nreq = xprt_dynamic_alloc_slot(xprt, GFP_NOWAIT|__GFP_NOWARN);\r\nif (!IS_ERR(req))\r\ngoto out_init_req;\r\nswitch (PTR_ERR(req)) {\r\ncase -ENOMEM:\r\ndprintk("RPC: dynamic allocation of request slot "\r\n"failed! Retrying\n");\r\ntask->tk_status = -ENOMEM;\r\nbreak;\r\ncase -EAGAIN:\r\nxprt_add_backlog(xprt, task);\r\ndprintk("RPC: waiting for request slot\n");\r\ndefault:\r\ntask->tk_status = -EAGAIN;\r\n}\r\nspin_unlock(&xprt->reserve_lock);\r\nreturn;\r\nout_init_req:\r\ntask->tk_status = 0;\r\ntask->tk_rqstp = req;\r\nxprt_request_init(task, xprt);\r\nspin_unlock(&xprt->reserve_lock);\r\n}\r\nvoid xprt_lock_and_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)\r\n{\r\nif (xprt_lock_write(xprt, task)) {\r\nxprt_alloc_slot(xprt, task);\r\nxprt_release_write(xprt, task);\r\n}\r\n}\r\nstatic void xprt_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *req)\r\n{\r\nspin_lock(&xprt->reserve_lock);\r\nif (!xprt_dynamic_free_slot(xprt, req)) {\r\nmemset(req, 0, sizeof(*req));\r\nlist_add(&req->rq_list, &xprt->free);\r\n}\r\nxprt_wake_up_backlog(xprt);\r\nspin_unlock(&xprt->reserve_lock);\r\n}\r\nstatic void xprt_free_all_slots(struct rpc_xprt *xprt)\r\n{\r\nstruct rpc_rqst *req;\r\nwhile (!list_empty(&xprt->free)) {\r\nreq = list_first_entry(&xprt->free, struct rpc_rqst, rq_list);\r\nlist_del(&req->rq_list);\r\nkfree(req);\r\n}\r\n}\r\nstruct rpc_xprt *xprt_alloc(struct net *net, size_t size,\r\nunsigned int num_prealloc,\r\nunsigned int max_alloc)\r\n{\r\nstruct rpc_xprt *xprt;\r\nstruct rpc_rqst *req;\r\nint i;\r\nxprt = kzalloc(size, GFP_KERNEL);\r\nif (xprt == NULL)\r\ngoto out;\r\nxprt_init(xprt, net);\r\nfor (i = 0; i < num_prealloc; i++) {\r\nreq = kzalloc(sizeof(struct rpc_rqst), GFP_KERNEL);\r\nif (!req)\r\ngoto out_free;\r\nlist_add(&req->rq_list, &xprt->free);\r\n}\r\nif (max_alloc > num_prealloc)\r\nxprt->max_reqs = max_alloc;\r\nelse\r\nxprt->max_reqs = num_prealloc;\r\nxprt->min_reqs = num_prealloc;\r\natomic_set(&xprt->num_reqs, num_prealloc);\r\nreturn xprt;\r\nout_free:\r\nxprt_free(xprt);\r\nout:\r\nreturn NULL;\r\n}\r\nvoid xprt_free(struct rpc_xprt *xprt)\r\n{\r\nput_net(xprt->xprt_net);\r\nxprt_free_all_slots(xprt);\r\nkfree(xprt);\r\n}\r\nvoid xprt_reserve(struct rpc_task *task)\r\n{\r\nstruct rpc_xprt *xprt;\r\ntask->tk_status = 0;\r\nif (task->tk_rqstp != NULL)\r\nreturn;\r\ntask->tk_timeout = 0;\r\ntask->tk_status = -EAGAIN;\r\nrcu_read_lock();\r\nxprt = rcu_dereference(task->tk_client->cl_xprt);\r\nif (!xprt_throttle_congested(xprt, task))\r\nxprt->ops->alloc_slot(xprt, task);\r\nrcu_read_unlock();\r\n}\r\nvoid xprt_retry_reserve(struct rpc_task *task)\r\n{\r\nstruct rpc_xprt *xprt;\r\ntask->tk_status = 0;\r\nif (task->tk_rqstp != NULL)\r\nreturn;\r\ntask->tk_timeout = 0;\r\ntask->tk_status = -EAGAIN;\r\nrcu_read_lock();\r\nxprt = rcu_dereference(task->tk_client->cl_xprt);\r\nxprt->ops->alloc_slot(xprt, task);\r\nrcu_read_unlock();\r\n}\r\nstatic inline __be32 xprt_alloc_xid(struct rpc_xprt *xprt)\r\n{\r\nreturn (__force __be32)xprt->xid++;\r\n}\r\nstatic inline void xprt_init_xid(struct rpc_xprt *xprt)\r\n{\r\nxprt->xid = prandom_u32();\r\n}\r\nstatic void xprt_request_init(struct rpc_task *task, struct rpc_xprt *xprt)\r\n{\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nINIT_LIST_HEAD(&req->rq_list);\r\nreq->rq_timeout = task->tk_client->cl_timeout->to_initval;\r\nreq->rq_task = task;\r\nreq->rq_xprt = xprt;\r\nreq->rq_buffer = NULL;\r\nreq->rq_xid = xprt_alloc_xid(xprt);\r\nreq->rq_connect_cookie = xprt->connect_cookie - 1;\r\nreq->rq_bytes_sent = 0;\r\nreq->rq_snd_buf.len = 0;\r\nreq->rq_snd_buf.buflen = 0;\r\nreq->rq_rcv_buf.len = 0;\r\nreq->rq_rcv_buf.buflen = 0;\r\nreq->rq_release_snd_buf = NULL;\r\nxprt_reset_majortimeo(req);\r\ndprintk("RPC: %5u reserved req %p xid %08x\n", task->tk_pid,\r\nreq, ntohl(req->rq_xid));\r\n}\r\nvoid xprt_release(struct rpc_task *task)\r\n{\r\nstruct rpc_xprt *xprt;\r\nstruct rpc_rqst *req = task->tk_rqstp;\r\nif (req == NULL) {\r\nif (task->tk_client) {\r\nrcu_read_lock();\r\nxprt = rcu_dereference(task->tk_client->cl_xprt);\r\nif (xprt->snd_task == task)\r\nxprt_release_write(xprt, task);\r\nrcu_read_unlock();\r\n}\r\nreturn;\r\n}\r\nxprt = req->rq_xprt;\r\nif (task->tk_ops->rpc_count_stats != NULL)\r\ntask->tk_ops->rpc_count_stats(task, task->tk_calldata);\r\nelse if (task->tk_client)\r\nrpc_count_iostats(task, task->tk_client->cl_metrics);\r\nspin_lock_bh(&xprt->transport_lock);\r\nxprt->ops->release_xprt(xprt, task);\r\nif (xprt->ops->release_request)\r\nxprt->ops->release_request(task);\r\nif (!list_empty(&req->rq_list))\r\nlist_del(&req->rq_list);\r\nxprt->last_used = jiffies;\r\nif (list_empty(&xprt->recv) && xprt_has_timer(xprt))\r\nmod_timer(&xprt->timer,\r\nxprt->last_used + xprt->idle_timeout);\r\nspin_unlock_bh(&xprt->transport_lock);\r\nif (req->rq_buffer)\r\nxprt->ops->buf_free(req->rq_buffer);\r\nxprt_inject_disconnect(xprt);\r\nif (req->rq_cred != NULL)\r\nput_rpccred(req->rq_cred);\r\ntask->tk_rqstp = NULL;\r\nif (req->rq_release_snd_buf)\r\nreq->rq_release_snd_buf(req);\r\ndprintk("RPC: %5u release request %p\n", task->tk_pid, req);\r\nif (likely(!bc_prealloc(req)))\r\nxprt_free_slot(xprt, req);\r\nelse\r\nxprt_free_bc_request(req);\r\n}\r\nstatic void xprt_init(struct rpc_xprt *xprt, struct net *net)\r\n{\r\natomic_set(&xprt->count, 1);\r\nspin_lock_init(&xprt->transport_lock);\r\nspin_lock_init(&xprt->reserve_lock);\r\nINIT_LIST_HEAD(&xprt->free);\r\nINIT_LIST_HEAD(&xprt->recv);\r\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\r\nspin_lock_init(&xprt->bc_pa_lock);\r\nINIT_LIST_HEAD(&xprt->bc_pa_list);\r\n#endif\r\nxprt->last_used = jiffies;\r\nxprt->cwnd = RPC_INITCWND;\r\nxprt->bind_index = 0;\r\nrpc_init_wait_queue(&xprt->binding, "xprt_binding");\r\nrpc_init_wait_queue(&xprt->pending, "xprt_pending");\r\nrpc_init_priority_wait_queue(&xprt->sending, "xprt_sending");\r\nrpc_init_priority_wait_queue(&xprt->backlog, "xprt_backlog");\r\nxprt_init_xid(xprt);\r\nxprt->xprt_net = get_net(net);\r\n}\r\nstruct rpc_xprt *xprt_create_transport(struct xprt_create *args)\r\n{\r\nstruct rpc_xprt *xprt;\r\nstruct xprt_class *t;\r\nspin_lock(&xprt_list_lock);\r\nlist_for_each_entry(t, &xprt_list, list) {\r\nif (t->ident == args->ident) {\r\nspin_unlock(&xprt_list_lock);\r\ngoto found;\r\n}\r\n}\r\nspin_unlock(&xprt_list_lock);\r\ndprintk("RPC: transport (%d) not supported\n", args->ident);\r\nreturn ERR_PTR(-EIO);\r\nfound:\r\nxprt = t->setup(args);\r\nif (IS_ERR(xprt)) {\r\ndprintk("RPC: xprt_create_transport: failed, %ld\n",\r\n-PTR_ERR(xprt));\r\ngoto out;\r\n}\r\nif (args->flags & XPRT_CREATE_NO_IDLE_TIMEOUT)\r\nxprt->idle_timeout = 0;\r\nINIT_WORK(&xprt->task_cleanup, xprt_autoclose);\r\nif (xprt_has_timer(xprt))\r\nsetup_timer(&xprt->timer, xprt_init_autodisconnect,\r\n(unsigned long)xprt);\r\nelse\r\ninit_timer(&xprt->timer);\r\nif (strlen(args->servername) > RPC_MAXNETNAMELEN) {\r\nxprt_destroy(xprt);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nxprt->servername = kstrdup(args->servername, GFP_KERNEL);\r\nif (xprt->servername == NULL) {\r\nxprt_destroy(xprt);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nrpc_xprt_debugfs_register(xprt);\r\ndprintk("RPC: created transport %p with %u slots\n", xprt,\r\nxprt->max_reqs);\r\nout:\r\nreturn xprt;\r\n}\r\nstatic void xprt_destroy(struct rpc_xprt *xprt)\r\n{\r\ndprintk("RPC: destroying transport %p\n", xprt);\r\nwait_on_bit_lock(&xprt->state, XPRT_LOCKED, TASK_UNINTERRUPTIBLE);\r\ndel_timer_sync(&xprt->timer);\r\nrpc_xprt_debugfs_unregister(xprt);\r\nrpc_destroy_wait_queue(&xprt->binding);\r\nrpc_destroy_wait_queue(&xprt->pending);\r\nrpc_destroy_wait_queue(&xprt->sending);\r\nrpc_destroy_wait_queue(&xprt->backlog);\r\ncancel_work_sync(&xprt->task_cleanup);\r\nkfree(xprt->servername);\r\nxprt->ops->destroy(xprt);\r\n}\r\nvoid xprt_put(struct rpc_xprt *xprt)\r\n{\r\nif (atomic_dec_and_test(&xprt->count))\r\nxprt_destroy(xprt);\r\n}
