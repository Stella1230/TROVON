static inline struct ahash_alg *crypto_ahash_alg(struct crypto_ahash *hash)\r\n{\r\nreturn container_of(crypto_hash_alg_common(hash), struct ahash_alg,\r\nhalg);\r\n}\r\nstatic int hash_walk_next(struct crypto_hash_walk *walk)\r\n{\r\nunsigned int alignmask = walk->alignmask;\r\nunsigned int offset = walk->offset;\r\nunsigned int nbytes = min(walk->entrylen,\r\n((unsigned int)(PAGE_SIZE)) - offset);\r\nif (walk->flags & CRYPTO_ALG_ASYNC)\r\nwalk->data = kmap(walk->pg);\r\nelse\r\nwalk->data = kmap_atomic(walk->pg);\r\nwalk->data += offset;\r\nif (offset & alignmask) {\r\nunsigned int unaligned = alignmask + 1 - (offset & alignmask);\r\nif (nbytes > unaligned)\r\nnbytes = unaligned;\r\n}\r\nwalk->entrylen -= nbytes;\r\nreturn nbytes;\r\n}\r\nstatic int hash_walk_new_entry(struct crypto_hash_walk *walk)\r\n{\r\nstruct scatterlist *sg;\r\nsg = walk->sg;\r\nwalk->pg = sg_page(sg);\r\nwalk->offset = sg->offset;\r\nwalk->entrylen = sg->length;\r\nif (walk->entrylen > walk->total)\r\nwalk->entrylen = walk->total;\r\nwalk->total -= walk->entrylen;\r\nreturn hash_walk_next(walk);\r\n}\r\nint crypto_hash_walk_done(struct crypto_hash_walk *walk, int err)\r\n{\r\nunsigned int alignmask = walk->alignmask;\r\nunsigned int nbytes = walk->entrylen;\r\nwalk->data -= walk->offset;\r\nif (nbytes && walk->offset & alignmask && !err) {\r\nwalk->offset = ALIGN(walk->offset, alignmask + 1);\r\nwalk->data += walk->offset;\r\nnbytes = min(nbytes,\r\n((unsigned int)(PAGE_SIZE)) - walk->offset);\r\nwalk->entrylen -= nbytes;\r\nreturn nbytes;\r\n}\r\nif (walk->flags & CRYPTO_ALG_ASYNC)\r\nkunmap(walk->pg);\r\nelse {\r\nkunmap_atomic(walk->data);\r\ncrypto_yield(walk->flags);\r\n}\r\nif (err)\r\nreturn err;\r\nif (nbytes) {\r\nwalk->offset = 0;\r\nwalk->pg++;\r\nreturn hash_walk_next(walk);\r\n}\r\nif (!walk->total)\r\nreturn 0;\r\nwalk->sg = sg_next(walk->sg);\r\nreturn hash_walk_new_entry(walk);\r\n}\r\nint crypto_hash_walk_first(struct ahash_request *req,\r\nstruct crypto_hash_walk *walk)\r\n{\r\nwalk->total = req->nbytes;\r\nif (!walk->total) {\r\nwalk->entrylen = 0;\r\nreturn 0;\r\n}\r\nwalk->alignmask = crypto_ahash_alignmask(crypto_ahash_reqtfm(req));\r\nwalk->sg = req->src;\r\nwalk->flags = req->base.flags & CRYPTO_TFM_REQ_MASK;\r\nreturn hash_walk_new_entry(walk);\r\n}\r\nint crypto_ahash_walk_first(struct ahash_request *req,\r\nstruct crypto_hash_walk *walk)\r\n{\r\nwalk->total = req->nbytes;\r\nif (!walk->total) {\r\nwalk->entrylen = 0;\r\nreturn 0;\r\n}\r\nwalk->alignmask = crypto_ahash_alignmask(crypto_ahash_reqtfm(req));\r\nwalk->sg = req->src;\r\nwalk->flags = req->base.flags & CRYPTO_TFM_REQ_MASK;\r\nwalk->flags |= CRYPTO_ALG_ASYNC;\r\nBUILD_BUG_ON(CRYPTO_TFM_REQ_MASK & CRYPTO_ALG_ASYNC);\r\nreturn hash_walk_new_entry(walk);\r\n}\r\nint crypto_hash_walk_first_compat(struct hash_desc *hdesc,\r\nstruct crypto_hash_walk *walk,\r\nstruct scatterlist *sg, unsigned int len)\r\n{\r\nwalk->total = len;\r\nif (!walk->total) {\r\nwalk->entrylen = 0;\r\nreturn 0;\r\n}\r\nwalk->alignmask = crypto_hash_alignmask(hdesc->tfm);\r\nwalk->sg = sg;\r\nwalk->flags = hdesc->flags & CRYPTO_TFM_REQ_MASK;\r\nreturn hash_walk_new_entry(walk);\r\n}\r\nstatic int ahash_setkey_unaligned(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nunsigned long alignmask = crypto_ahash_alignmask(tfm);\r\nint ret;\r\nu8 *buffer, *alignbuffer;\r\nunsigned long absize;\r\nabsize = keylen + alignmask;\r\nbuffer = kmalloc(absize, GFP_KERNEL);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nalignbuffer = (u8 *)ALIGN((unsigned long)buffer, alignmask + 1);\r\nmemcpy(alignbuffer, key, keylen);\r\nret = tfm->setkey(tfm, alignbuffer, keylen);\r\nkzfree(buffer);\r\nreturn ret;\r\n}\r\nint crypto_ahash_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nunsigned long alignmask = crypto_ahash_alignmask(tfm);\r\nif ((unsigned long)key & alignmask)\r\nreturn ahash_setkey_unaligned(tfm, key, keylen);\r\nreturn tfm->setkey(tfm, key, keylen);\r\n}\r\nstatic int ahash_nosetkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic inline unsigned int ahash_align_buffer_size(unsigned len,\r\nunsigned long mask)\r\n{\r\nreturn len + (mask & ~(crypto_tfm_ctx_alignment() - 1));\r\n}\r\nstatic int ahash_save_req(struct ahash_request *req, crypto_completion_t cplt)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nunsigned long alignmask = crypto_ahash_alignmask(tfm);\r\nunsigned int ds = crypto_ahash_digestsize(tfm);\r\nstruct ahash_request_priv *priv;\r\npriv = kmalloc(sizeof(*priv) + ahash_align_buffer_size(ds, alignmask),\r\n(req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?\r\nGFP_KERNEL : GFP_ATOMIC);\r\nif (!priv)\r\nreturn -ENOMEM;\r\npriv->result = req->result;\r\npriv->complete = req->base.complete;\r\npriv->data = req->base.data;\r\nreq->result = PTR_ALIGN((u8 *)priv->ubuf, alignmask + 1);\r\nreq->base.complete = cplt;\r\nreq->base.data = req;\r\nreq->priv = priv;\r\nreturn 0;\r\n}\r\nstatic void ahash_restore_req(struct ahash_request *req)\r\n{\r\nstruct ahash_request_priv *priv = req->priv;\r\nreq->result = priv->result;\r\nreq->base.complete = priv->complete;\r\nreq->base.data = priv->data;\r\nreq->priv = NULL;\r\nkzfree(priv);\r\n}\r\nstatic void ahash_op_unaligned_finish(struct ahash_request *req, int err)\r\n{\r\nstruct ahash_request_priv *priv = req->priv;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nif (!err)\r\nmemcpy(priv->result, req->result,\r\ncrypto_ahash_digestsize(crypto_ahash_reqtfm(req)));\r\nahash_restore_req(req);\r\n}\r\nstatic void ahash_op_unaligned_done(struct crypto_async_request *req, int err)\r\n{\r\nstruct ahash_request *areq = req->data;\r\nahash_op_unaligned_finish(areq, err);\r\nareq->base.complete(&areq->base, err);\r\n}\r\nstatic int ahash_op_unaligned(struct ahash_request *req,\r\nint (*op)(struct ahash_request *))\r\n{\r\nint err;\r\nerr = ahash_save_req(req, ahash_op_unaligned_done);\r\nif (err)\r\nreturn err;\r\nerr = op(req);\r\nahash_op_unaligned_finish(req, err);\r\nreturn err;\r\n}\r\nstatic int crypto_ahash_op(struct ahash_request *req,\r\nint (*op)(struct ahash_request *))\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nunsigned long alignmask = crypto_ahash_alignmask(tfm);\r\nif ((unsigned long)req->result & alignmask)\r\nreturn ahash_op_unaligned(req, op);\r\nreturn op(req);\r\n}\r\nint crypto_ahash_final(struct ahash_request *req)\r\n{\r\nreturn crypto_ahash_op(req, crypto_ahash_reqtfm(req)->final);\r\n}\r\nint crypto_ahash_finup(struct ahash_request *req)\r\n{\r\nreturn crypto_ahash_op(req, crypto_ahash_reqtfm(req)->finup);\r\n}\r\nint crypto_ahash_digest(struct ahash_request *req)\r\n{\r\nreturn crypto_ahash_op(req, crypto_ahash_reqtfm(req)->digest);\r\n}\r\nstatic void ahash_def_finup_finish2(struct ahash_request *req, int err)\r\n{\r\nstruct ahash_request_priv *priv = req->priv;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nif (!err)\r\nmemcpy(priv->result, req->result,\r\ncrypto_ahash_digestsize(crypto_ahash_reqtfm(req)));\r\nahash_restore_req(req);\r\n}\r\nstatic void ahash_def_finup_done2(struct crypto_async_request *req, int err)\r\n{\r\nstruct ahash_request *areq = req->data;\r\nahash_def_finup_finish2(areq, err);\r\nareq->base.complete(&areq->base, err);\r\n}\r\nstatic int ahash_def_finup_finish1(struct ahash_request *req, int err)\r\n{\r\nif (err)\r\ngoto out;\r\nreq->base.complete = ahash_def_finup_done2;\r\nreq->base.flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nerr = crypto_ahash_reqtfm(req)->final(req);\r\nout:\r\nahash_def_finup_finish2(req, err);\r\nreturn err;\r\n}\r\nstatic void ahash_def_finup_done1(struct crypto_async_request *req, int err)\r\n{\r\nstruct ahash_request *areq = req->data;\r\nerr = ahash_def_finup_finish1(areq, err);\r\nareq->base.complete(&areq->base, err);\r\n}\r\nstatic int ahash_def_finup(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nint err;\r\nerr = ahash_save_req(req, ahash_def_finup_done1);\r\nif (err)\r\nreturn err;\r\nerr = tfm->update(req);\r\nreturn ahash_def_finup_finish1(req, err);\r\n}\r\nstatic int ahash_no_export(struct ahash_request *req, void *out)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic int ahash_no_import(struct ahash_request *req, const void *in)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic int crypto_ahash_init_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_ahash *hash = __crypto_ahash_cast(tfm);\r\nstruct ahash_alg *alg = crypto_ahash_alg(hash);\r\nhash->setkey = ahash_nosetkey;\r\nhash->export = ahash_no_export;\r\nhash->import = ahash_no_import;\r\nif (tfm->__crt_alg->cra_type != &crypto_ahash_type)\r\nreturn crypto_init_shash_ops_async(tfm);\r\nhash->init = alg->init;\r\nhash->update = alg->update;\r\nhash->final = alg->final;\r\nhash->finup = alg->finup ?: ahash_def_finup;\r\nhash->digest = alg->digest;\r\nif (alg->setkey)\r\nhash->setkey = alg->setkey;\r\nif (alg->export)\r\nhash->export = alg->export;\r\nif (alg->import)\r\nhash->import = alg->import;\r\nreturn 0;\r\n}\r\nstatic unsigned int crypto_ahash_extsize(struct crypto_alg *alg)\r\n{\r\nif (alg->cra_type == &crypto_ahash_type)\r\nreturn alg->cra_ctxsize;\r\nreturn sizeof(struct crypto_shash *);\r\n}\r\nstatic int crypto_ahash_report(struct sk_buff *skb, struct crypto_alg *alg)\r\n{\r\nstruct crypto_report_hash rhash;\r\nstrncpy(rhash.type, "ahash", sizeof(rhash.type));\r\nrhash.blocksize = alg->cra_blocksize;\r\nrhash.digestsize = __crypto_hash_alg_common(alg)->digestsize;\r\nif (nla_put(skb, CRYPTOCFGA_REPORT_HASH,\r\nsizeof(struct crypto_report_hash), &rhash))\r\ngoto nla_put_failure;\r\nreturn 0;\r\nnla_put_failure:\r\nreturn -EMSGSIZE;\r\n}\r\nstatic int crypto_ahash_report(struct sk_buff *skb, struct crypto_alg *alg)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic void crypto_ahash_show(struct seq_file *m, struct crypto_alg *alg)\r\n{\r\nseq_printf(m, "type : ahash\n");\r\nseq_printf(m, "async : %s\n", alg->cra_flags & CRYPTO_ALG_ASYNC ?\r\n"yes" : "no");\r\nseq_printf(m, "blocksize : %u\n", alg->cra_blocksize);\r\nseq_printf(m, "digestsize : %u\n",\r\n__crypto_hash_alg_common(alg)->digestsize);\r\n}\r\nstruct crypto_ahash *crypto_alloc_ahash(const char *alg_name, u32 type,\r\nu32 mask)\r\n{\r\nreturn crypto_alloc_tfm(alg_name, &crypto_ahash_type, type, mask);\r\n}\r\nstatic int ahash_prepare_alg(struct ahash_alg *alg)\r\n{\r\nstruct crypto_alg *base = &alg->halg.base;\r\nif (alg->halg.digestsize > PAGE_SIZE / 8 ||\r\nalg->halg.statesize > PAGE_SIZE / 8 ||\r\nalg->halg.statesize == 0)\r\nreturn -EINVAL;\r\nbase->cra_type = &crypto_ahash_type;\r\nbase->cra_flags &= ~CRYPTO_ALG_TYPE_MASK;\r\nbase->cra_flags |= CRYPTO_ALG_TYPE_AHASH;\r\nreturn 0;\r\n}\r\nint crypto_register_ahash(struct ahash_alg *alg)\r\n{\r\nstruct crypto_alg *base = &alg->halg.base;\r\nint err;\r\nerr = ahash_prepare_alg(alg);\r\nif (err)\r\nreturn err;\r\nreturn crypto_register_alg(base);\r\n}\r\nint crypto_unregister_ahash(struct ahash_alg *alg)\r\n{\r\nreturn crypto_unregister_alg(&alg->halg.base);\r\n}\r\nint ahash_register_instance(struct crypto_template *tmpl,\r\nstruct ahash_instance *inst)\r\n{\r\nint err;\r\nerr = ahash_prepare_alg(&inst->alg);\r\nif (err)\r\nreturn err;\r\nreturn crypto_register_instance(tmpl, ahash_crypto_instance(inst));\r\n}\r\nvoid ahash_free_instance(struct crypto_instance *inst)\r\n{\r\ncrypto_drop_spawn(crypto_instance_ctx(inst));\r\nkfree(ahash_instance(inst));\r\n}\r\nint crypto_init_ahash_spawn(struct crypto_ahash_spawn *spawn,\r\nstruct hash_alg_common *alg,\r\nstruct crypto_instance *inst)\r\n{\r\nreturn crypto_init_spawn2(&spawn->base, &alg->base, inst,\r\n&crypto_ahash_type);\r\n}\r\nstruct hash_alg_common *ahash_attr_alg(struct rtattr *rta, u32 type, u32 mask)\r\n{\r\nstruct crypto_alg *alg;\r\nalg = crypto_attr_alg2(rta, &crypto_ahash_type, type, mask);\r\nreturn IS_ERR(alg) ? ERR_CAST(alg) : __crypto_hash_alg_common(alg);\r\n}
