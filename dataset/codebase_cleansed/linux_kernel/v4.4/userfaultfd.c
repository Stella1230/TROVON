static int mcopy_atomic_pte(struct mm_struct *dst_mm,\r\npmd_t *dst_pmd,\r\nstruct vm_area_struct *dst_vma,\r\nunsigned long dst_addr,\r\nunsigned long src_addr,\r\nstruct page **pagep)\r\n{\r\nstruct mem_cgroup *memcg;\r\npte_t _dst_pte, *dst_pte;\r\nspinlock_t *ptl;\r\nvoid *page_kaddr;\r\nint ret;\r\nstruct page *page;\r\nif (!*pagep) {\r\nret = -ENOMEM;\r\npage = alloc_page_vma(GFP_HIGHUSER_MOVABLE, dst_vma, dst_addr);\r\nif (!page)\r\ngoto out;\r\npage_kaddr = kmap_atomic(page);\r\nret = copy_from_user(page_kaddr,\r\n(const void __user *) src_addr,\r\nPAGE_SIZE);\r\nkunmap_atomic(page_kaddr);\r\nif (unlikely(ret)) {\r\nret = -EFAULT;\r\n*pagep = page;\r\ngoto out;\r\n}\r\n} else {\r\npage = *pagep;\r\n*pagep = NULL;\r\n}\r\n__SetPageUptodate(page);\r\nret = -ENOMEM;\r\nif (mem_cgroup_try_charge(page, dst_mm, GFP_KERNEL, &memcg))\r\ngoto out_release;\r\n_dst_pte = mk_pte(page, dst_vma->vm_page_prot);\r\nif (dst_vma->vm_flags & VM_WRITE)\r\n_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));\r\nret = -EEXIST;\r\ndst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\r\nif (!pte_none(*dst_pte))\r\ngoto out_release_uncharge_unlock;\r\ninc_mm_counter(dst_mm, MM_ANONPAGES);\r\npage_add_new_anon_rmap(page, dst_vma, dst_addr);\r\nmem_cgroup_commit_charge(page, memcg, false);\r\nlru_cache_add_active_or_unevictable(page, dst_vma);\r\nset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\r\nupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\r\npte_unmap_unlock(dst_pte, ptl);\r\nret = 0;\r\nout:\r\nreturn ret;\r\nout_release_uncharge_unlock:\r\npte_unmap_unlock(dst_pte, ptl);\r\nmem_cgroup_cancel_charge(page, memcg);\r\nout_release:\r\npage_cache_release(page);\r\ngoto out;\r\n}\r\nstatic int mfill_zeropage_pte(struct mm_struct *dst_mm,\r\npmd_t *dst_pmd,\r\nstruct vm_area_struct *dst_vma,\r\nunsigned long dst_addr)\r\n{\r\npte_t _dst_pte, *dst_pte;\r\nspinlock_t *ptl;\r\nint ret;\r\n_dst_pte = pte_mkspecial(pfn_pte(my_zero_pfn(dst_addr),\r\ndst_vma->vm_page_prot));\r\nret = -EEXIST;\r\ndst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\r\nif (!pte_none(*dst_pte))\r\ngoto out_unlock;\r\nset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\r\nupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\r\nret = 0;\r\nout_unlock:\r\npte_unmap_unlock(dst_pte, ptl);\r\nreturn ret;\r\n}\r\nstatic pmd_t *mm_alloc_pmd(struct mm_struct *mm, unsigned long address)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd = NULL;\r\npgd = pgd_offset(mm, address);\r\npud = pud_alloc(mm, pgd, address);\r\nif (pud)\r\npmd = pmd_alloc(mm, pud, address);\r\nreturn pmd;\r\n}\r\nstatic __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,\r\nunsigned long dst_start,\r\nunsigned long src_start,\r\nunsigned long len,\r\nbool zeropage)\r\n{\r\nstruct vm_area_struct *dst_vma;\r\nssize_t err;\r\npmd_t *dst_pmd;\r\nunsigned long src_addr, dst_addr;\r\nlong copied;\r\nstruct page *page;\r\nBUG_ON(dst_start & ~PAGE_MASK);\r\nBUG_ON(len & ~PAGE_MASK);\r\nBUG_ON(src_start + len <= src_start);\r\nBUG_ON(dst_start + len <= dst_start);\r\nsrc_addr = src_start;\r\ndst_addr = dst_start;\r\ncopied = 0;\r\npage = NULL;\r\nretry:\r\ndown_read(&dst_mm->mmap_sem);\r\nerr = -EINVAL;\r\ndst_vma = find_vma(dst_mm, dst_start);\r\nif (!dst_vma || (dst_vma->vm_flags & VM_SHARED))\r\ngoto out_unlock;\r\nif (dst_start < dst_vma->vm_start ||\r\ndst_start + len > dst_vma->vm_end)\r\ngoto out_unlock;\r\nif (!dst_vma->vm_userfaultfd_ctx.ctx)\r\ngoto out_unlock;\r\nif (dst_vma->vm_ops)\r\ngoto out_unlock;\r\nerr = -ENOMEM;\r\nif (unlikely(anon_vma_prepare(dst_vma)))\r\ngoto out_unlock;\r\nwhile (src_addr < src_start + len) {\r\npmd_t dst_pmdval;\r\nBUG_ON(dst_addr >= dst_start + len);\r\ndst_pmd = mm_alloc_pmd(dst_mm, dst_addr);\r\nif (unlikely(!dst_pmd)) {\r\nerr = -ENOMEM;\r\nbreak;\r\n}\r\ndst_pmdval = pmd_read_atomic(dst_pmd);\r\nif (unlikely(pmd_trans_huge(dst_pmdval))) {\r\nerr = -EEXIST;\r\nbreak;\r\n}\r\nif (unlikely(pmd_none(dst_pmdval)) &&\r\nunlikely(__pte_alloc(dst_mm, dst_vma, dst_pmd,\r\ndst_addr))) {\r\nerr = -ENOMEM;\r\nbreak;\r\n}\r\nif (unlikely(pmd_trans_huge(*dst_pmd))) {\r\nerr = -EFAULT;\r\nbreak;\r\n}\r\nBUG_ON(pmd_none(*dst_pmd));\r\nBUG_ON(pmd_trans_huge(*dst_pmd));\r\nif (!zeropage)\r\nerr = mcopy_atomic_pte(dst_mm, dst_pmd, dst_vma,\r\ndst_addr, src_addr, &page);\r\nelse\r\nerr = mfill_zeropage_pte(dst_mm, dst_pmd, dst_vma,\r\ndst_addr);\r\ncond_resched();\r\nif (unlikely(err == -EFAULT)) {\r\nvoid *page_kaddr;\r\nup_read(&dst_mm->mmap_sem);\r\nBUG_ON(!page);\r\npage_kaddr = kmap(page);\r\nerr = copy_from_user(page_kaddr,\r\n(const void __user *) src_addr,\r\nPAGE_SIZE);\r\nkunmap(page);\r\nif (unlikely(err)) {\r\nerr = -EFAULT;\r\ngoto out;\r\n}\r\ngoto retry;\r\n} else\r\nBUG_ON(page);\r\nif (!err) {\r\ndst_addr += PAGE_SIZE;\r\nsrc_addr += PAGE_SIZE;\r\ncopied += PAGE_SIZE;\r\nif (fatal_signal_pending(current))\r\nerr = -EINTR;\r\n}\r\nif (err)\r\nbreak;\r\n}\r\nout_unlock:\r\nup_read(&dst_mm->mmap_sem);\r\nout:\r\nif (page)\r\npage_cache_release(page);\r\nBUG_ON(copied < 0);\r\nBUG_ON(err > 0);\r\nBUG_ON(!copied && !err);\r\nreturn copied ? copied : err;\r\n}\r\nssize_t mcopy_atomic(struct mm_struct *dst_mm, unsigned long dst_start,\r\nunsigned long src_start, unsigned long len)\r\n{\r\nreturn __mcopy_atomic(dst_mm, dst_start, src_start, len, false);\r\n}\r\nssize_t mfill_zeropage(struct mm_struct *dst_mm, unsigned long start,\r\nunsigned long len)\r\n{\r\nreturn __mcopy_atomic(dst_mm, start, 0, len, true);\r\n}
