static void ch_reg_write(struct hpb_dmae_chan *hpb_dc, u32 data, u32 reg)\r\n{\r\niowrite32(data, hpb_dc->base + reg);\r\n}\r\nstatic u32 ch_reg_read(struct hpb_dmae_chan *hpb_dc, u32 reg)\r\n{\r\nreturn ioread32(hpb_dc->base + reg);\r\n}\r\nstatic void dcmdr_write(struct hpb_dmae_device *hpbdev, u32 data)\r\n{\r\niowrite32(data, hpbdev->chan_reg + HPB_DMAE_DCMDR);\r\n}\r\nstatic void hsrstr_write(struct hpb_dmae_device *hpbdev, u32 ch)\r\n{\r\niowrite32(0x1, hpbdev->comm_reg + HPB_DMAE_HSRSTR(ch));\r\n}\r\nstatic u32 dintsr_read(struct hpb_dmae_device *hpbdev, u32 ch)\r\n{\r\nu32 v;\r\nif (ch < 32)\r\nv = ioread32(hpbdev->comm_reg + HPB_DMAE_DINTSR0) >> ch;\r\nelse\r\nv = ioread32(hpbdev->comm_reg + HPB_DMAE_DINTSR1) >> (ch - 32);\r\nreturn v & 0x1;\r\n}\r\nstatic void dintcr_write(struct hpb_dmae_device *hpbdev, u32 ch)\r\n{\r\nif (ch < 32)\r\niowrite32((0x1 << ch), hpbdev->comm_reg + HPB_DMAE_DINTCR0);\r\nelse\r\niowrite32((0x1 << (ch - 32)),\r\nhpbdev->comm_reg + HPB_DMAE_DINTCR1);\r\n}\r\nstatic void asyncmdr_write(struct hpb_dmae_device *hpbdev, u32 data)\r\n{\r\niowrite32(data, hpbdev->mode_reg);\r\n}\r\nstatic u32 asyncmdr_read(struct hpb_dmae_device *hpbdev)\r\n{\r\nreturn ioread32(hpbdev->mode_reg);\r\n}\r\nstatic void hpb_dmae_enable_int(struct hpb_dmae_device *hpbdev, u32 ch)\r\n{\r\nu32 intreg;\r\nspin_lock_irq(&hpbdev->reg_lock);\r\nif (ch < 32) {\r\nintreg = ioread32(hpbdev->comm_reg + HPB_DMAE_DINTMR0);\r\niowrite32(BIT(ch) | intreg,\r\nhpbdev->comm_reg + HPB_DMAE_DINTMR0);\r\n} else {\r\nintreg = ioread32(hpbdev->comm_reg + HPB_DMAE_DINTMR1);\r\niowrite32(BIT(ch - 32) | intreg,\r\nhpbdev->comm_reg + HPB_DMAE_DINTMR1);\r\n}\r\nspin_unlock_irq(&hpbdev->reg_lock);\r\n}\r\nstatic void hpb_dmae_async_reset(struct hpb_dmae_device *hpbdev, u32 data)\r\n{\r\nu32 rstr;\r\nint timeout = 10000;\r\nspin_lock(&hpbdev->reg_lock);\r\nrstr = ioread32(hpbdev->reset_reg);\r\nrstr |= data;\r\niowrite32(rstr, hpbdev->reset_reg);\r\ndo {\r\nrstr = ioread32(hpbdev->reset_reg);\r\nif ((rstr & data) == data)\r\nbreak;\r\nudelay(10);\r\n} while (timeout--);\r\nif (timeout < 0)\r\ndev_err(hpbdev->shdma_dev.dma_dev.dev,\r\n"%s timeout\n", __func__);\r\nrstr &= ~data;\r\niowrite32(rstr, hpbdev->reset_reg);\r\nspin_unlock(&hpbdev->reg_lock);\r\n}\r\nstatic void hpb_dmae_set_async_mode(struct hpb_dmae_device *hpbdev,\r\nu32 mask, u32 data)\r\n{\r\nu32 mode;\r\nspin_lock_irq(&hpbdev->reg_lock);\r\nmode = asyncmdr_read(hpbdev);\r\nmode &= ~mask;\r\nmode |= data;\r\nasyncmdr_write(hpbdev, mode);\r\nspin_unlock_irq(&hpbdev->reg_lock);\r\n}\r\nstatic void hpb_dmae_ctl_stop(struct hpb_dmae_device *hpbdev)\r\n{\r\ndcmdr_write(hpbdev, HPB_DMAE_DCMDR_DQSPD);\r\n}\r\nstatic void hpb_dmae_reset(struct hpb_dmae_device *hpbdev)\r\n{\r\nu32 ch;\r\nfor (ch = 0; ch < hpbdev->pdata->num_hw_channels; ch++)\r\nhsrstr_write(hpbdev, ch);\r\n}\r\nstatic unsigned int calc_xmit_shift(struct hpb_dmae_chan *hpb_chan)\r\n{\r\nstruct hpb_dmae_device *hpbdev = to_dev(hpb_chan);\r\nstruct hpb_dmae_pdata *pdata = hpbdev->pdata;\r\nint width = ch_reg_read(hpb_chan, HPB_DMAE_DCR);\r\nint i;\r\nswitch (width & (HPB_DMAE_DCR_SPDS_MASK | HPB_DMAE_DCR_DPDS_MASK)) {\r\ncase HPB_DMAE_DCR_SPDS_8BIT | HPB_DMAE_DCR_DPDS_8BIT:\r\ndefault:\r\ni = XMIT_SZ_8BIT;\r\nbreak;\r\ncase HPB_DMAE_DCR_SPDS_16BIT | HPB_DMAE_DCR_DPDS_16BIT:\r\ni = XMIT_SZ_16BIT;\r\nbreak;\r\ncase HPB_DMAE_DCR_SPDS_32BIT | HPB_DMAE_DCR_DPDS_32BIT:\r\ni = XMIT_SZ_32BIT;\r\nbreak;\r\n}\r\nreturn pdata->ts_shift[i];\r\n}\r\nstatic void hpb_dmae_set_reg(struct hpb_dmae_chan *hpb_chan,\r\nstruct hpb_dmae_regs *hw, unsigned plane)\r\n{\r\nch_reg_write(hpb_chan, hw->sar,\r\nplane ? HPB_DMAE_DSAR1 : HPB_DMAE_DSAR0);\r\nch_reg_write(hpb_chan, hw->dar,\r\nplane ? HPB_DMAE_DDAR1 : HPB_DMAE_DDAR0);\r\nch_reg_write(hpb_chan, hw->tcr >> hpb_chan->xmit_shift,\r\nplane ? HPB_DMAE_DTCR1 : HPB_DMAE_DTCR0);\r\n}\r\nstatic void hpb_dmae_start(struct hpb_dmae_chan *hpb_chan, bool next)\r\n{\r\nch_reg_write(hpb_chan, (next ? HPB_DMAE_DCMDR_DNXT : 0) |\r\nHPB_DMAE_DCMDR_DMEN, HPB_DMAE_DCMDR);\r\n}\r\nstatic void hpb_dmae_halt(struct shdma_chan *schan)\r\n{\r\nstruct hpb_dmae_chan *chan = to_chan(schan);\r\nch_reg_write(chan, HPB_DMAE_DCMDR_DQEND, HPB_DMAE_DCMDR);\r\nch_reg_write(chan, HPB_DMAE_DSTPR_DMSTP, HPB_DMAE_DSTPR);\r\nchan->plane_idx = 0;\r\nchan->first_desc = true;\r\n}\r\nstatic const struct hpb_dmae_slave_config *\r\nhpb_dmae_find_slave(struct hpb_dmae_chan *hpb_chan, int slave_id)\r\n{\r\nstruct hpb_dmae_device *hpbdev = to_dev(hpb_chan);\r\nstruct hpb_dmae_pdata *pdata = hpbdev->pdata;\r\nint i;\r\nif (slave_id >= HPB_DMA_SLAVE_NUMBER)\r\nreturn NULL;\r\nfor (i = 0; i < pdata->num_slaves; i++)\r\nif (pdata->slaves[i].id == slave_id)\r\nreturn pdata->slaves + i;\r\nreturn NULL;\r\n}\r\nstatic void hpb_dmae_start_xfer(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc)\r\n{\r\nstruct hpb_dmae_chan *chan = to_chan(schan);\r\nstruct hpb_dmae_device *hpbdev = to_dev(chan);\r\nstruct hpb_desc *desc = to_desc(sdesc);\r\nif (chan->cfg->flags & HPB_DMAE_SET_ASYNC_RESET)\r\nhpb_dmae_async_reset(hpbdev, chan->cfg->rstr);\r\ndesc->plane_idx = chan->plane_idx;\r\nhpb_dmae_set_reg(chan, &desc->hw, chan->plane_idx);\r\nhpb_dmae_start(chan, !chan->first_desc);\r\nif (chan->xfer_mode == XFER_DOUBLE) {\r\nchan->plane_idx ^= 1;\r\nchan->first_desc = false;\r\n}\r\n}\r\nstatic bool hpb_dmae_desc_completed(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc)\r\n{\r\nreturn true;\r\n}\r\nstatic bool hpb_dmae_chan_irq(struct shdma_chan *schan, int irq)\r\n{\r\nstruct hpb_dmae_chan *chan = to_chan(schan);\r\nstruct hpb_dmae_device *hpbdev = to_dev(chan);\r\nint ch = chan->cfg->dma_ch;\r\nif (dintsr_read(hpbdev, ch)) {\r\ndintcr_write(hpbdev, ch);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int hpb_dmae_desc_setup(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc,\r\ndma_addr_t src, dma_addr_t dst, size_t *len)\r\n{\r\nstruct hpb_desc *desc = to_desc(sdesc);\r\nif (*len > (size_t)HPB_DMA_TCR_MAX)\r\n*len = (size_t)HPB_DMA_TCR_MAX;\r\ndesc->hw.sar = src;\r\ndesc->hw.dar = dst;\r\ndesc->hw.tcr = *len;\r\nreturn 0;\r\n}\r\nstatic size_t hpb_dmae_get_partial(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc)\r\n{\r\nstruct hpb_desc *desc = to_desc(sdesc);\r\nstruct hpb_dmae_chan *chan = to_chan(schan);\r\nu32 tcr = ch_reg_read(chan, desc->plane_idx ?\r\nHPB_DMAE_DTCR1 : HPB_DMAE_DTCR0);\r\nreturn (desc->hw.tcr - tcr) << chan->xmit_shift;\r\n}\r\nstatic bool hpb_dmae_channel_busy(struct shdma_chan *schan)\r\n{\r\nstruct hpb_dmae_chan *chan = to_chan(schan);\r\nu32 dstsr = ch_reg_read(chan, HPB_DMAE_DSTSR);\r\nif (chan->xfer_mode == XFER_DOUBLE)\r\nreturn dstsr & HPB_DMAE_DSTSR_DQSTS;\r\nelse\r\nreturn dstsr & HPB_DMAE_DSTSR_DMSTS;\r\n}\r\nstatic int\r\nhpb_dmae_alloc_chan_resources(struct hpb_dmae_chan *hpb_chan,\r\nconst struct hpb_dmae_slave_config *cfg)\r\n{\r\nstruct hpb_dmae_device *hpbdev = to_dev(hpb_chan);\r\nstruct hpb_dmae_pdata *pdata = hpbdev->pdata;\r\nconst struct hpb_dmae_channel *channel = pdata->channels;\r\nint slave_id = cfg->id;\r\nint i, err;\r\nfor (i = 0; i < pdata->num_channels; i++, channel++) {\r\nif (channel->s_id == slave_id) {\r\nstruct device *dev = hpb_chan->shdma_chan.dev;\r\nhpb_chan->base = hpbdev->chan_reg +\r\nHPB_DMAE_CHAN(cfg->dma_ch);\r\ndev_dbg(dev, "Detected Slave device\n");\r\ndev_dbg(dev, " -- slave_id : 0x%x\n", slave_id);\r\ndev_dbg(dev, " -- cfg->dma_ch : %d\n", cfg->dma_ch);\r\ndev_dbg(dev, " -- channel->ch_irq: %d\n",\r\nchannel->ch_irq);\r\nbreak;\r\n}\r\n}\r\nerr = shdma_request_irq(&hpb_chan->shdma_chan, channel->ch_irq,\r\nIRQF_SHARED, hpb_chan->dev_id);\r\nif (err) {\r\ndev_err(hpb_chan->shdma_chan.dev,\r\n"DMA channel request_irq %d failed with error %d\n",\r\nchannel->ch_irq, err);\r\nreturn err;\r\n}\r\nhpb_chan->plane_idx = 0;\r\nhpb_chan->first_desc = true;\r\nif ((cfg->dcr & (HPB_DMAE_DCR_CT | HPB_DMAE_DCR_DIP)) == 0) {\r\nhpb_chan->xfer_mode = XFER_SINGLE;\r\n} else if ((cfg->dcr & (HPB_DMAE_DCR_CT | HPB_DMAE_DCR_DIP)) ==\r\n(HPB_DMAE_DCR_CT | HPB_DMAE_DCR_DIP)) {\r\nhpb_chan->xfer_mode = XFER_DOUBLE;\r\n} else {\r\ndev_err(hpb_chan->shdma_chan.dev, "DCR setting error");\r\nreturn -EINVAL;\r\n}\r\nif (cfg->flags & HPB_DMAE_SET_ASYNC_MODE)\r\nhpb_dmae_set_async_mode(hpbdev, cfg->mdm, cfg->mdr);\r\nch_reg_write(hpb_chan, cfg->dcr, HPB_DMAE_DCR);\r\nch_reg_write(hpb_chan, cfg->port, HPB_DMAE_DPTR);\r\nhpb_chan->xmit_shift = calc_xmit_shift(hpb_chan);\r\nhpb_dmae_enable_int(hpbdev, cfg->dma_ch);\r\nreturn 0;\r\n}\r\nstatic int hpb_dmae_set_slave(struct shdma_chan *schan, int slave_id,\r\ndma_addr_t slave_addr, bool try)\r\n{\r\nstruct hpb_dmae_chan *chan = to_chan(schan);\r\nconst struct hpb_dmae_slave_config *sc =\r\nhpb_dmae_find_slave(chan, slave_id);\r\nif (!sc)\r\nreturn -ENODEV;\r\nif (try)\r\nreturn 0;\r\nchan->cfg = sc;\r\nchan->slave_addr = slave_addr ? : sc->addr;\r\nreturn hpb_dmae_alloc_chan_resources(chan, sc);\r\n}\r\nstatic void hpb_dmae_setup_xfer(struct shdma_chan *schan, int slave_id)\r\n{\r\n}\r\nstatic dma_addr_t hpb_dmae_slave_addr(struct shdma_chan *schan)\r\n{\r\nstruct hpb_dmae_chan *chan = to_chan(schan);\r\nreturn chan->slave_addr;\r\n}\r\nstatic struct shdma_desc *hpb_dmae_embedded_desc(void *buf, int i)\r\n{\r\nreturn &((struct hpb_desc *)buf)[i].shdma_desc;\r\n}\r\nstatic int hpb_dmae_chan_probe(struct hpb_dmae_device *hpbdev, int id)\r\n{\r\nstruct shdma_dev *sdev = &hpbdev->shdma_dev;\r\nstruct platform_device *pdev =\r\nto_platform_device(hpbdev->shdma_dev.dma_dev.dev);\r\nstruct hpb_dmae_chan *new_hpb_chan;\r\nstruct shdma_chan *schan;\r\nnew_hpb_chan = devm_kzalloc(&pdev->dev,\r\nsizeof(struct hpb_dmae_chan), GFP_KERNEL);\r\nif (!new_hpb_chan) {\r\ndev_err(hpbdev->shdma_dev.dma_dev.dev,\r\n"No free memory for allocating DMA channels!\n");\r\nreturn -ENOMEM;\r\n}\r\nschan = &new_hpb_chan->shdma_chan;\r\nschan->max_xfer_len = HPB_DMA_TCR_MAX;\r\nshdma_chan_probe(sdev, schan, id);\r\nif (pdev->id >= 0)\r\nsnprintf(new_hpb_chan->dev_id, sizeof(new_hpb_chan->dev_id),\r\n"hpb-dmae%d.%d", pdev->id, id);\r\nelse\r\nsnprintf(new_hpb_chan->dev_id, sizeof(new_hpb_chan->dev_id),\r\n"hpb-dma.%d", id);\r\nreturn 0;\r\n}\r\nstatic int hpb_dmae_probe(struct platform_device *pdev)\r\n{\r\nconst enum dma_slave_buswidth widths = DMA_SLAVE_BUSWIDTH_1_BYTE |\r\nDMA_SLAVE_BUSWIDTH_2_BYTES | DMA_SLAVE_BUSWIDTH_4_BYTES;\r\nstruct hpb_dmae_pdata *pdata = pdev->dev.platform_data;\r\nstruct hpb_dmae_device *hpbdev;\r\nstruct dma_device *dma_dev;\r\nstruct resource *chan, *comm, *rest, *mode, *irq_res;\r\nint err, i;\r\nif (!pdata || !pdata->num_channels)\r\nreturn -ENODEV;\r\nchan = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ncomm = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nrest = platform_get_resource(pdev, IORESOURCE_MEM, 2);\r\nmode = platform_get_resource(pdev, IORESOURCE_MEM, 3);\r\nirq_res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!irq_res)\r\nreturn -ENODEV;\r\nhpbdev = devm_kzalloc(&pdev->dev, sizeof(struct hpb_dmae_device),\r\nGFP_KERNEL);\r\nif (!hpbdev) {\r\ndev_err(&pdev->dev, "Not enough memory\n");\r\nreturn -ENOMEM;\r\n}\r\nhpbdev->chan_reg = devm_ioremap_resource(&pdev->dev, chan);\r\nif (IS_ERR(hpbdev->chan_reg))\r\nreturn PTR_ERR(hpbdev->chan_reg);\r\nhpbdev->comm_reg = devm_ioremap_resource(&pdev->dev, comm);\r\nif (IS_ERR(hpbdev->comm_reg))\r\nreturn PTR_ERR(hpbdev->comm_reg);\r\nhpbdev->reset_reg = devm_ioremap_resource(&pdev->dev, rest);\r\nif (IS_ERR(hpbdev->reset_reg))\r\nreturn PTR_ERR(hpbdev->reset_reg);\r\nhpbdev->mode_reg = devm_ioremap_resource(&pdev->dev, mode);\r\nif (IS_ERR(hpbdev->mode_reg))\r\nreturn PTR_ERR(hpbdev->mode_reg);\r\ndma_dev = &hpbdev->shdma_dev.dma_dev;\r\nspin_lock_init(&hpbdev->reg_lock);\r\nhpbdev->pdata = pdata;\r\npm_runtime_enable(&pdev->dev);\r\nerr = pm_runtime_get_sync(&pdev->dev);\r\nif (err < 0)\r\ndev_err(&pdev->dev, "%s(): GET = %d\n", __func__, err);\r\nhpb_dmae_reset(hpbdev);\r\npm_runtime_put(&pdev->dev);\r\ndma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\r\ndma_cap_set(DMA_SLAVE, dma_dev->cap_mask);\r\ndma_dev->src_addr_widths = widths;\r\ndma_dev->dst_addr_widths = widths;\r\ndma_dev->directions = BIT(DMA_MEM_TO_DEV) | BIT(DMA_DEV_TO_MEM);\r\ndma_dev->residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\r\nhpbdev->shdma_dev.ops = &hpb_dmae_ops;\r\nhpbdev->shdma_dev.desc_size = sizeof(struct hpb_desc);\r\nerr = shdma_init(&pdev->dev, &hpbdev->shdma_dev, pdata->num_channels);\r\nif (err < 0)\r\ngoto error;\r\nfor (i = 0; i < pdata->num_channels; i++)\r\nhpb_dmae_chan_probe(hpbdev, i);\r\nplatform_set_drvdata(pdev, hpbdev);\r\nerr = dma_async_device_register(dma_dev);\r\nif (!err)\r\nreturn 0;\r\nshdma_cleanup(&hpbdev->shdma_dev);\r\nerror:\r\npm_runtime_disable(&pdev->dev);\r\nreturn err;\r\n}\r\nstatic void hpb_dmae_chan_remove(struct hpb_dmae_device *hpbdev)\r\n{\r\nstruct shdma_chan *schan;\r\nint i;\r\nshdma_for_each_chan(schan, &hpbdev->shdma_dev, i) {\r\nBUG_ON(!schan);\r\nshdma_chan_remove(schan);\r\n}\r\n}\r\nstatic int hpb_dmae_remove(struct platform_device *pdev)\r\n{\r\nstruct hpb_dmae_device *hpbdev = platform_get_drvdata(pdev);\r\ndma_async_device_unregister(&hpbdev->shdma_dev.dma_dev);\r\npm_runtime_disable(&pdev->dev);\r\nhpb_dmae_chan_remove(hpbdev);\r\nreturn 0;\r\n}\r\nstatic void hpb_dmae_shutdown(struct platform_device *pdev)\r\n{\r\nstruct hpb_dmae_device *hpbdev = platform_get_drvdata(pdev);\r\nhpb_dmae_ctl_stop(hpbdev);\r\n}
