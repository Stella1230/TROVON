void smp_info(struct seq_file *m)\r\n{\r\nint i;\r\nseq_printf(m, "State:\n");\r\nfor_each_online_cpu(i)\r\nseq_printf(m, "CPU%d:\t\tonline\n", i);\r\n}\r\nvoid smp_bogo(struct seq_file *m)\r\n{\r\nint i;\r\nfor_each_online_cpu(i)\r\nseq_printf(m,\r\n"Cpu%dClkTck\t: %016lx\n",\r\ni, cpu_data(i).clock_tick);\r\n}\r\nvoid smp_callin(void)\r\n{\r\nint cpuid = hard_smp_processor_id();\r\n__local_per_cpu_offset = __per_cpu_offset(cpuid);\r\nif (tlb_type == hypervisor)\r\nsun4v_ktsb_register();\r\n__flush_tlb_all();\r\nsetup_sparc64_timer();\r\nif (cheetah_pcache_forced_on)\r\ncheetah_enable_pcache();\r\ncallin_flag = 1;\r\n__asm__ __volatile__("membar #Sync\n\t"\r\n"flush %%g6" : : : "memory");\r\ncurrent_thread_info()->new_child = 0;\r\natomic_inc(&init_mm.mm_count);\r\ncurrent->active_mm = &init_mm;\r\nnotify_cpu_starting(cpuid);\r\nwhile (!cpumask_test_cpu(cpuid, &smp_commenced_mask))\r\nrmb();\r\nset_cpu_online(cpuid, true);\r\npreempt_disable();\r\nlocal_irq_enable();\r\ncpu_startup_entry(CPUHP_ONLINE);\r\n}\r\nvoid cpu_panic(void)\r\n{\r\nprintk("CPU[%d]: Returns from cpu_idle!\n", smp_processor_id());\r\npanic("SMP bolixed\n");\r\n}\r\nstatic inline long get_delta (long *rt, long *master)\r\n{\r\nunsigned long best_t0 = 0, best_t1 = ~0UL, best_tm = 0;\r\nunsigned long tcenter, t0, t1, tm;\r\nunsigned long i;\r\nfor (i = 0; i < NUM_ITERS; i++) {\r\nt0 = tick_ops->get_tick();\r\ngo[MASTER] = 1;\r\nmembar_safe("#StoreLoad");\r\nwhile (!(tm = go[SLAVE]))\r\nrmb();\r\ngo[SLAVE] = 0;\r\nwmb();\r\nt1 = tick_ops->get_tick();\r\nif (t1 - t0 < best_t1 - best_t0)\r\nbest_t0 = t0, best_t1 = t1, best_tm = tm;\r\n}\r\n*rt = best_t1 - best_t0;\r\n*master = best_tm - best_t0;\r\ntcenter = (best_t0/2 + best_t1/2);\r\nif (best_t0 % 2 + best_t1 % 2 == 2)\r\ntcenter++;\r\nreturn tcenter - best_tm;\r\n}\r\nvoid smp_synchronize_tick_client(void)\r\n{\r\nlong i, delta, adj, adjust_latency = 0, done = 0;\r\nunsigned long flags, rt, master_time_stamp;\r\n#if DEBUG_TICK_SYNC\r\nstruct {\r\nlong rt;\r\nlong master;\r\nlong diff;\r\nlong lat;\r\n} t[NUM_ROUNDS];\r\n#endif\r\ngo[MASTER] = 1;\r\nwhile (go[MASTER])\r\nrmb();\r\nlocal_irq_save(flags);\r\n{\r\nfor (i = 0; i < NUM_ROUNDS; i++) {\r\ndelta = get_delta(&rt, &master_time_stamp);\r\nif (delta == 0)\r\ndone = 1;\r\nif (!done) {\r\nif (i > 0) {\r\nadjust_latency += -delta;\r\nadj = -delta + adjust_latency/4;\r\n} else\r\nadj = -delta;\r\ntick_ops->add_tick(adj);\r\n}\r\n#if DEBUG_TICK_SYNC\r\nt[i].rt = rt;\r\nt[i].master = master_time_stamp;\r\nt[i].diff = delta;\r\nt[i].lat = adjust_latency/4;\r\n#endif\r\n}\r\n}\r\nlocal_irq_restore(flags);\r\n#if DEBUG_TICK_SYNC\r\nfor (i = 0; i < NUM_ROUNDS; i++)\r\nprintk("rt=%5ld master=%5ld diff=%5ld adjlat=%5ld\n",\r\nt[i].rt, t[i].master, t[i].diff, t[i].lat);\r\n#endif\r\nprintk(KERN_INFO "CPU %d: synchronized TICK with master CPU "\r\n"(last diff %ld cycles, maxerr %lu cycles)\n",\r\nsmp_processor_id(), delta, rt);\r\n}\r\nstatic void smp_synchronize_one_tick(int cpu)\r\n{\r\nunsigned long flags, i;\r\ngo[MASTER] = 0;\r\nsmp_start_sync_tick_client(cpu);\r\nwhile (!go[MASTER])\r\nrmb();\r\ngo[MASTER] = 0;\r\nmembar_safe("#StoreLoad");\r\nraw_spin_lock_irqsave(&itc_sync_lock, flags);\r\n{\r\nfor (i = 0; i < NUM_ROUNDS*NUM_ITERS; i++) {\r\nwhile (!go[MASTER])\r\nrmb();\r\ngo[MASTER] = 0;\r\nwmb();\r\ngo[SLAVE] = tick_ops->get_tick();\r\nmembar_safe("#StoreLoad");\r\n}\r\n}\r\nraw_spin_unlock_irqrestore(&itc_sync_lock, flags);\r\n}\r\nstatic void ldom_startcpu_cpuid(unsigned int cpu, unsigned long thread_reg,\r\nvoid **descrp)\r\n{\r\nextern unsigned long sparc64_ttable_tl0;\r\nextern unsigned long kern_locked_tte_data;\r\nstruct hvtramp_descr *hdesc;\r\nunsigned long trampoline_ra;\r\nstruct trap_per_cpu *tb;\r\nu64 tte_vaddr, tte_data;\r\nunsigned long hv_err;\r\nint i;\r\nhdesc = kzalloc(sizeof(*hdesc) +\r\n(sizeof(struct hvtramp_mapping) *\r\nnum_kernel_image_mappings - 1),\r\nGFP_KERNEL);\r\nif (!hdesc) {\r\nprintk(KERN_ERR "ldom_startcpu_cpuid: Cannot allocate "\r\n"hvtramp_descr.\n");\r\nreturn;\r\n}\r\n*descrp = hdesc;\r\nhdesc->cpu = cpu;\r\nhdesc->num_mappings = num_kernel_image_mappings;\r\ntb = &trap_block[cpu];\r\nhdesc->fault_info_va = (unsigned long) &tb->fault_info;\r\nhdesc->fault_info_pa = kimage_addr_to_ra(&tb->fault_info);\r\nhdesc->thread_reg = thread_reg;\r\ntte_vaddr = (unsigned long) KERNBASE;\r\ntte_data = kern_locked_tte_data;\r\nfor (i = 0; i < hdesc->num_mappings; i++) {\r\nhdesc->maps[i].vaddr = tte_vaddr;\r\nhdesc->maps[i].tte = tte_data;\r\ntte_vaddr += 0x400000;\r\ntte_data += 0x400000;\r\n}\r\ntrampoline_ra = kimage_addr_to_ra(hv_cpu_startup);\r\nhv_err = sun4v_cpu_start(cpu, trampoline_ra,\r\nkimage_addr_to_ra(&sparc64_ttable_tl0),\r\n__pa(hdesc));\r\nif (hv_err)\r\nprintk(KERN_ERR "ldom_startcpu_cpuid: sun4v_cpu_start() "\r\n"gives error %lu\n", hv_err);\r\n}\r\nstatic int smp_boot_one_cpu(unsigned int cpu, struct task_struct *idle)\r\n{\r\nunsigned long entry =\r\n(unsigned long)(&sparc64_cpu_startup);\r\nunsigned long cookie =\r\n(unsigned long)(&cpu_new_thread);\r\nvoid *descr = NULL;\r\nint timeout, ret;\r\ncallin_flag = 0;\r\ncpu_new_thread = task_thread_info(idle);\r\nif (tlb_type == hypervisor) {\r\n#if defined(CONFIG_SUN_LDOMS) && defined(CONFIG_HOTPLUG_CPU)\r\nif (ldom_domaining_enabled)\r\nldom_startcpu_cpuid(cpu,\r\n(unsigned long) cpu_new_thread,\r\n&descr);\r\nelse\r\n#endif\r\nprom_startcpu_cpuid(cpu, entry, cookie);\r\n} else {\r\nstruct device_node *dp = of_find_node_by_cpuid(cpu);\r\nprom_startcpu(dp->phandle, entry, cookie);\r\n}\r\nfor (timeout = 0; timeout < 50000; timeout++) {\r\nif (callin_flag)\r\nbreak;\r\nudelay(100);\r\n}\r\nif (callin_flag) {\r\nret = 0;\r\n} else {\r\nprintk("Processor %d is stuck.\n", cpu);\r\nret = -ENODEV;\r\n}\r\ncpu_new_thread = NULL;\r\nkfree(descr);\r\nreturn ret;\r\n}\r\nstatic void spitfire_xcall_helper(u64 data0, u64 data1, u64 data2, u64 pstate, unsigned long cpu)\r\n{\r\nu64 result, target;\r\nint stuck, tmp;\r\nif (this_is_starfire) {\r\ncpu = (((cpu & 0x3c) << 1) |\r\n((cpu & 0x40) >> 4) |\r\n(cpu & 0x3));\r\n}\r\ntarget = (cpu << 14) | 0x70;\r\nagain:\r\ntmp = 0x40;\r\n__asm__ __volatile__(\r\n"wrpr %1, %2, %%pstate\n\t"\r\n"stxa %4, [%0] %3\n\t"\r\n"stxa %5, [%0+%8] %3\n\t"\r\n"add %0, %8, %0\n\t"\r\n"stxa %6, [%0+%8] %3\n\t"\r\n"membar #Sync\n\t"\r\n"stxa %%g0, [%7] %3\n\t"\r\n"membar #Sync\n\t"\r\n"mov 0x20, %%g1\n\t"\r\n"ldxa [%%g1] 0x7f, %%g0\n\t"\r\n"membar #Sync"\r\n: "=r" (tmp)\r\n: "r" (pstate), "i" (PSTATE_IE), "i" (ASI_INTR_W),\r\n"r" (data0), "r" (data1), "r" (data2), "r" (target),\r\n"r" (0x10), "0" (tmp)\r\n: "g1");\r\nstuck = 100000;\r\ndo {\r\n__asm__ __volatile__("ldxa [%%g0] %1, %0"\r\n: "=r" (result)\r\n: "i" (ASI_INTR_DISPATCH_STAT));\r\nif (result == 0) {\r\n__asm__ __volatile__("wrpr %0, 0x0, %%pstate"\r\n: : "r" (pstate));\r\nreturn;\r\n}\r\nstuck -= 1;\r\nif (stuck == 0)\r\nbreak;\r\n} while (result & 0x1);\r\n__asm__ __volatile__("wrpr %0, 0x0, %%pstate"\r\n: : "r" (pstate));\r\nif (stuck == 0) {\r\nprintk("CPU[%d]: mondo stuckage result[%016llx]\n",\r\nsmp_processor_id(), result);\r\n} else {\r\nudelay(2);\r\ngoto again;\r\n}\r\n}\r\nstatic void spitfire_xcall_deliver(struct trap_per_cpu *tb, int cnt)\r\n{\r\nu64 *mondo, data0, data1, data2;\r\nu16 *cpu_list;\r\nu64 pstate;\r\nint i;\r\n__asm__ __volatile__("rdpr %%pstate, %0" : "=r" (pstate));\r\ncpu_list = __va(tb->cpu_list_pa);\r\nmondo = __va(tb->cpu_mondo_block_pa);\r\ndata0 = mondo[0];\r\ndata1 = mondo[1];\r\ndata2 = mondo[2];\r\nfor (i = 0; i < cnt; i++)\r\nspitfire_xcall_helper(data0, data1, data2, pstate, cpu_list[i]);\r\n}\r\nstatic void cheetah_xcall_deliver(struct trap_per_cpu *tb, int cnt)\r\n{\r\nint nack_busy_id, is_jbus, need_more;\r\nu64 *mondo, pstate, ver, busy_mask;\r\nu16 *cpu_list;\r\ncpu_list = __va(tb->cpu_list_pa);\r\nmondo = __va(tb->cpu_mondo_block_pa);\r\n__asm__ ("rdpr %%ver, %0" : "=r" (ver));\r\nis_jbus = ((ver >> 32) == __JALAPENO_ID ||\r\n(ver >> 32) == __SERRANO_ID);\r\n__asm__ __volatile__("rdpr %%pstate, %0" : "=r" (pstate));\r\nretry:\r\nneed_more = 0;\r\n__asm__ __volatile__("wrpr %0, %1, %%pstate\n\t"\r\n: : "r" (pstate), "i" (PSTATE_IE));\r\n__asm__ __volatile__("stxa %0, [%3] %6\n\t"\r\n"stxa %1, [%4] %6\n\t"\r\n"stxa %2, [%5] %6\n\t"\r\n"membar #Sync\n\t"\r\n:\r\n: "r" (mondo[0]), "r" (mondo[1]), "r" (mondo[2]),\r\n"r" (0x40), "r" (0x50), "r" (0x60),\r\n"i" (ASI_INTR_W));\r\nnack_busy_id = 0;\r\nbusy_mask = 0;\r\n{\r\nint i;\r\nfor (i = 0; i < cnt; i++) {\r\nu64 target, nr;\r\nnr = cpu_list[i];\r\nif (nr == 0xffff)\r\ncontinue;\r\ntarget = (nr << 14) | 0x70;\r\nif (is_jbus) {\r\nbusy_mask |= (0x1UL << (nr * 2));\r\n} else {\r\ntarget |= (nack_busy_id << 24);\r\nbusy_mask |= (0x1UL <<\r\n(nack_busy_id * 2));\r\n}\r\n__asm__ __volatile__(\r\n"stxa %%g0, [%0] %1\n\t"\r\n"membar #Sync\n\t"\r\n:\r\n: "r" (target), "i" (ASI_INTR_W));\r\nnack_busy_id++;\r\nif (nack_busy_id == 32) {\r\nneed_more = 1;\r\nbreak;\r\n}\r\n}\r\n}\r\n{\r\nu64 dispatch_stat, nack_mask;\r\nlong stuck;\r\nstuck = 100000 * nack_busy_id;\r\nnack_mask = busy_mask << 1;\r\ndo {\r\n__asm__ __volatile__("ldxa [%%g0] %1, %0"\r\n: "=r" (dispatch_stat)\r\n: "i" (ASI_INTR_DISPATCH_STAT));\r\nif (!(dispatch_stat & (busy_mask | nack_mask))) {\r\n__asm__ __volatile__("wrpr %0, 0x0, %%pstate"\r\n: : "r" (pstate));\r\nif (unlikely(need_more)) {\r\nint i, this_cnt = 0;\r\nfor (i = 0; i < cnt; i++) {\r\nif (cpu_list[i] == 0xffff)\r\ncontinue;\r\ncpu_list[i] = 0xffff;\r\nthis_cnt++;\r\nif (this_cnt == 32)\r\nbreak;\r\n}\r\ngoto retry;\r\n}\r\nreturn;\r\n}\r\nif (!--stuck)\r\nbreak;\r\n} while (dispatch_stat & busy_mask);\r\n__asm__ __volatile__("wrpr %0, 0x0, %%pstate"\r\n: : "r" (pstate));\r\nif (dispatch_stat & busy_mask) {\r\nprintk("CPU[%d]: mondo stuckage result[%016llx]\n",\r\nsmp_processor_id(), dispatch_stat);\r\n} else {\r\nint i, this_busy_nack = 0;\r\nudelay(2 * nack_busy_id);\r\nfor (i = 0; i < cnt; i++) {\r\nu64 check_mask, nr;\r\nnr = cpu_list[i];\r\nif (nr == 0xffff)\r\ncontinue;\r\nif (is_jbus)\r\ncheck_mask = (0x2UL << (2*nr));\r\nelse\r\ncheck_mask = (0x2UL <<\r\nthis_busy_nack);\r\nif ((dispatch_stat & check_mask) == 0)\r\ncpu_list[i] = 0xffff;\r\nthis_busy_nack += 2;\r\nif (this_busy_nack == 64)\r\nbreak;\r\n}\r\ngoto retry;\r\n}\r\n}\r\n}\r\nstatic void hypervisor_xcall_deliver(struct trap_per_cpu *tb, int cnt)\r\n{\r\nint retries, this_cpu, prev_sent, i, saw_cpu_error;\r\nunsigned long status;\r\nu16 *cpu_list;\r\nthis_cpu = smp_processor_id();\r\ncpu_list = __va(tb->cpu_list_pa);\r\nsaw_cpu_error = 0;\r\nretries = 0;\r\nprev_sent = 0;\r\ndo {\r\nint forward_progress, n_sent;\r\nstatus = sun4v_cpu_mondo_send(cnt,\r\ntb->cpu_list_pa,\r\ntb->cpu_mondo_block_pa);\r\nif (likely(status == HV_EOK))\r\nbreak;\r\nn_sent = 0;\r\nfor (i = 0; i < cnt; i++) {\r\nif (likely(cpu_list[i] == 0xffff))\r\nn_sent++;\r\n}\r\nforward_progress = 0;\r\nif (n_sent > prev_sent)\r\nforward_progress = 1;\r\nprev_sent = n_sent;\r\nif (unlikely(status == HV_ECPUERROR)) {\r\nfor (i = 0; i < cnt; i++) {\r\nlong err;\r\nu16 cpu;\r\ncpu = cpu_list[i];\r\nif (cpu == 0xffff)\r\ncontinue;\r\nerr = sun4v_cpu_state(cpu);\r\nif (err == HV_CPU_STATE_ERROR) {\r\nsaw_cpu_error = (cpu + 1);\r\ncpu_list[i] = 0xffff;\r\n}\r\n}\r\n} else if (unlikely(status != HV_EWOULDBLOCK))\r\ngoto fatal_mondo_error;\r\nif (unlikely(!forward_progress)) {\r\nif (unlikely(++retries > 10000))\r\ngoto fatal_mondo_timeout;\r\nudelay(2 * cnt);\r\n}\r\n} while (1);\r\nif (unlikely(saw_cpu_error))\r\ngoto fatal_mondo_cpu_error;\r\nreturn;\r\nfatal_mondo_cpu_error:\r\nprintk(KERN_CRIT "CPU[%d]: SUN4V mondo cpu error, some target cpus "\r\n"(including %d) were in error state\n",\r\nthis_cpu, saw_cpu_error - 1);\r\nreturn;\r\nfatal_mondo_timeout:\r\nprintk(KERN_CRIT "CPU[%d]: SUN4V mondo timeout, no forward "\r\n" progress after %d retries.\n",\r\nthis_cpu, retries);\r\ngoto dump_cpu_list_and_out;\r\nfatal_mondo_error:\r\nprintk(KERN_CRIT "CPU[%d]: Unexpected SUN4V mondo error %lu\n",\r\nthis_cpu, status);\r\nprintk(KERN_CRIT "CPU[%d]: Args were cnt(%d) cpulist_pa(%lx) "\r\n"mondo_block_pa(%lx)\n",\r\nthis_cpu, cnt, tb->cpu_list_pa, tb->cpu_mondo_block_pa);\r\ndump_cpu_list_and_out:\r\nprintk(KERN_CRIT "CPU[%d]: CPU list [ ", this_cpu);\r\nfor (i = 0; i < cnt; i++)\r\nprintk("%u ", cpu_list[i]);\r\nprintk("]\n");\r\n}\r\nstatic void xcall_deliver(u64 data0, u64 data1, u64 data2, const cpumask_t *mask)\r\n{\r\nstruct trap_per_cpu *tb;\r\nint this_cpu, i, cnt;\r\nunsigned long flags;\r\nu16 *cpu_list;\r\nu64 *mondo;\r\nlocal_irq_save(flags);\r\nthis_cpu = smp_processor_id();\r\ntb = &trap_block[this_cpu];\r\nmondo = __va(tb->cpu_mondo_block_pa);\r\nmondo[0] = data0;\r\nmondo[1] = data1;\r\nmondo[2] = data2;\r\nwmb();\r\ncpu_list = __va(tb->cpu_list_pa);\r\ncnt = 0;\r\nfor_each_cpu(i, mask) {\r\nif (i == this_cpu || !cpu_online(i))\r\ncontinue;\r\ncpu_list[cnt++] = i;\r\n}\r\nif (cnt)\r\nxcall_deliver_impl(tb, cnt);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void smp_cross_call_masked(unsigned long *func, u32 ctx, u64 data1, u64 data2, const cpumask_t *mask)\r\n{\r\nu64 data0 = (((u64)ctx)<<32 | (((u64)func) & 0xffffffff));\r\nxcall_deliver(data0, data1, data2, mask);\r\n}\r\nstatic void smp_cross_call(unsigned long *func, u32 ctx, u64 data1, u64 data2)\r\n{\r\nsmp_cross_call_masked(func, ctx, data1, data2, cpu_online_mask);\r\n}\r\nstatic void smp_start_sync_tick_client(int cpu)\r\n{\r\nxcall_deliver((u64) &xcall_sync_tick, 0, 0,\r\ncpumask_of(cpu));\r\n}\r\nvoid arch_send_call_function_ipi_mask(const struct cpumask *mask)\r\n{\r\nxcall_deliver((u64) &xcall_call_function, 0, 0, mask);\r\n}\r\nvoid arch_send_call_function_single_ipi(int cpu)\r\n{\r\nxcall_deliver((u64) &xcall_call_function_single, 0, 0,\r\ncpumask_of(cpu));\r\n}\r\nvoid __irq_entry smp_call_function_client(int irq, struct pt_regs *regs)\r\n{\r\nclear_softint(1 << irq);\r\nirq_enter();\r\ngeneric_smp_call_function_interrupt();\r\nirq_exit();\r\n}\r\nvoid __irq_entry smp_call_function_single_client(int irq, struct pt_regs *regs)\r\n{\r\nclear_softint(1 << irq);\r\nirq_enter();\r\ngeneric_smp_call_function_single_interrupt();\r\nirq_exit();\r\n}\r\nstatic void tsb_sync(void *info)\r\n{\r\nstruct trap_per_cpu *tp = &trap_block[raw_smp_processor_id()];\r\nstruct mm_struct *mm = info;\r\nif (tp->pgd_paddr == __pa(mm->pgd))\r\ntsb_context_switch(mm);\r\n}\r\nvoid smp_tsb_sync(struct mm_struct *mm)\r\n{\r\nsmp_call_function_many(mm_cpumask(mm), tsb_sync, mm, 1);\r\n}\r\nstatic inline void __local_flush_dcache_page(struct page *page)\r\n{\r\n#ifdef DCACHE_ALIASING_POSSIBLE\r\n__flush_dcache_page(page_address(page),\r\n((tlb_type == spitfire) &&\r\npage_mapping(page) != NULL));\r\n#else\r\nif (page_mapping(page) != NULL &&\r\ntlb_type == spitfire)\r\n__flush_icache_page(__pa(page_address(page)));\r\n#endif\r\n}\r\nvoid smp_flush_dcache_page_impl(struct page *page, int cpu)\r\n{\r\nint this_cpu;\r\nif (tlb_type == hypervisor)\r\nreturn;\r\n#ifdef CONFIG_DEBUG_DCFLUSH\r\natomic_inc(&dcpage_flushes);\r\n#endif\r\nthis_cpu = get_cpu();\r\nif (cpu == this_cpu) {\r\n__local_flush_dcache_page(page);\r\n} else if (cpu_online(cpu)) {\r\nvoid *pg_addr = page_address(page);\r\nu64 data0 = 0;\r\nif (tlb_type == spitfire) {\r\ndata0 = ((u64)&xcall_flush_dcache_page_spitfire);\r\nif (page_mapping(page) != NULL)\r\ndata0 |= ((u64)1 << 32);\r\n} else if (tlb_type == cheetah || tlb_type == cheetah_plus) {\r\n#ifdef DCACHE_ALIASING_POSSIBLE\r\ndata0 = ((u64)&xcall_flush_dcache_page_cheetah);\r\n#endif\r\n}\r\nif (data0) {\r\nxcall_deliver(data0, __pa(pg_addr),\r\n(u64) pg_addr, cpumask_of(cpu));\r\n#ifdef CONFIG_DEBUG_DCFLUSH\r\natomic_inc(&dcpage_flushes_xcall);\r\n#endif\r\n}\r\n}\r\nput_cpu();\r\n}\r\nvoid flush_dcache_page_all(struct mm_struct *mm, struct page *page)\r\n{\r\nvoid *pg_addr;\r\nu64 data0;\r\nif (tlb_type == hypervisor)\r\nreturn;\r\npreempt_disable();\r\n#ifdef CONFIG_DEBUG_DCFLUSH\r\natomic_inc(&dcpage_flushes);\r\n#endif\r\ndata0 = 0;\r\npg_addr = page_address(page);\r\nif (tlb_type == spitfire) {\r\ndata0 = ((u64)&xcall_flush_dcache_page_spitfire);\r\nif (page_mapping(page) != NULL)\r\ndata0 |= ((u64)1 << 32);\r\n} else if (tlb_type == cheetah || tlb_type == cheetah_plus) {\r\n#ifdef DCACHE_ALIASING_POSSIBLE\r\ndata0 = ((u64)&xcall_flush_dcache_page_cheetah);\r\n#endif\r\n}\r\nif (data0) {\r\nxcall_deliver(data0, __pa(pg_addr),\r\n(u64) pg_addr, cpu_online_mask);\r\n#ifdef CONFIG_DEBUG_DCFLUSH\r\natomic_inc(&dcpage_flushes_xcall);\r\n#endif\r\n}\r\n__local_flush_dcache_page(page);\r\npreempt_enable();\r\n}\r\nvoid __irq_entry smp_new_mmu_context_version_client(int irq, struct pt_regs *regs)\r\n{\r\nstruct mm_struct *mm;\r\nunsigned long flags;\r\nclear_softint(1 << irq);\r\nmm = current->active_mm;\r\nif (unlikely(!mm || (mm == &init_mm)))\r\nreturn;\r\nspin_lock_irqsave(&mm->context.lock, flags);\r\nif (unlikely(!CTX_VALID(mm->context)))\r\nget_new_mmu_context(mm);\r\nspin_unlock_irqrestore(&mm->context.lock, flags);\r\nload_secondary_context(mm);\r\n__flush_tlb_mm(CTX_HWBITS(mm->context),\r\nSECONDARY_CONTEXT);\r\n}\r\nvoid smp_new_mmu_context_version(void)\r\n{\r\nsmp_cross_call(&xcall_new_mmu_context_version, 0, 0, 0);\r\n}\r\nvoid kgdb_roundup_cpus(unsigned long flags)\r\n{\r\nsmp_cross_call(&xcall_kgdb_capture, 0, 0, 0);\r\n}\r\nvoid smp_fetch_global_regs(void)\r\n{\r\nsmp_cross_call(&xcall_fetch_glob_regs, 0, 0, 0);\r\n}\r\nvoid smp_fetch_global_pmu(void)\r\n{\r\nif (tlb_type == hypervisor &&\r\nsun4v_chip_type >= SUN4V_CHIP_NIAGARA4)\r\nsmp_cross_call(&xcall_fetch_glob_pmu_n4, 0, 0, 0);\r\nelse\r\nsmp_cross_call(&xcall_fetch_glob_pmu, 0, 0, 0);\r\n}\r\nvoid smp_flush_tlb_mm(struct mm_struct *mm)\r\n{\r\nu32 ctx = CTX_HWBITS(mm->context);\r\nint cpu = get_cpu();\r\nif (atomic_read(&mm->mm_users) == 1) {\r\ncpumask_copy(mm_cpumask(mm), cpumask_of(cpu));\r\ngoto local_flush_and_out;\r\n}\r\nsmp_cross_call_masked(&xcall_flush_tlb_mm,\r\nctx, 0, 0,\r\nmm_cpumask(mm));\r\nlocal_flush_and_out:\r\n__flush_tlb_mm(ctx, SECONDARY_CONTEXT);\r\nput_cpu();\r\n}\r\nstatic void tlb_pending_func(void *info)\r\n{\r\nstruct tlb_pending_info *t = info;\r\n__flush_tlb_pending(t->ctx, t->nr, t->vaddrs);\r\n}\r\nvoid smp_flush_tlb_pending(struct mm_struct *mm, unsigned long nr, unsigned long *vaddrs)\r\n{\r\nu32 ctx = CTX_HWBITS(mm->context);\r\nstruct tlb_pending_info info;\r\nint cpu = get_cpu();\r\ninfo.ctx = ctx;\r\ninfo.nr = nr;\r\ninfo.vaddrs = vaddrs;\r\nif (mm == current->mm && atomic_read(&mm->mm_users) == 1)\r\ncpumask_copy(mm_cpumask(mm), cpumask_of(cpu));\r\nelse\r\nsmp_call_function_many(mm_cpumask(mm), tlb_pending_func,\r\n&info, 1);\r\n__flush_tlb_pending(ctx, nr, vaddrs);\r\nput_cpu();\r\n}\r\nvoid smp_flush_tlb_page(struct mm_struct *mm, unsigned long vaddr)\r\n{\r\nunsigned long context = CTX_HWBITS(mm->context);\r\nint cpu = get_cpu();\r\nif (mm == current->mm && atomic_read(&mm->mm_users) == 1)\r\ncpumask_copy(mm_cpumask(mm), cpumask_of(cpu));\r\nelse\r\nsmp_cross_call_masked(&xcall_flush_tlb_page,\r\ncontext, vaddr, 0,\r\nmm_cpumask(mm));\r\n__flush_tlb_page(context, vaddr);\r\nput_cpu();\r\n}\r\nvoid smp_flush_tlb_kernel_range(unsigned long start, unsigned long end)\r\n{\r\nstart &= PAGE_MASK;\r\nend = PAGE_ALIGN(end);\r\nif (start != end) {\r\nsmp_cross_call(&xcall_flush_tlb_kernel_range,\r\n0, start, end);\r\n__flush_tlb_kernel_range(start, end);\r\n}\r\n}\r\nvoid smp_capture(void)\r\n{\r\nint result = atomic_add_return(1, &smp_capture_depth);\r\nif (result == 1) {\r\nint ncpus = num_online_cpus();\r\n#ifdef CAPTURE_DEBUG\r\nprintk("CPU[%d]: Sending penguins to jail...",\r\nsmp_processor_id());\r\n#endif\r\npenguins_are_doing_time = 1;\r\natomic_inc(&smp_capture_registry);\r\nsmp_cross_call(&xcall_capture, 0, 0, 0);\r\nwhile (atomic_read(&smp_capture_registry) != ncpus)\r\nrmb();\r\n#ifdef CAPTURE_DEBUG\r\nprintk("done\n");\r\n#endif\r\n}\r\n}\r\nvoid smp_release(void)\r\n{\r\nif (atomic_dec_and_test(&smp_capture_depth)) {\r\n#ifdef CAPTURE_DEBUG\r\nprintk("CPU[%d]: Giving pardon to "\r\n"imprisoned penguins\n",\r\nsmp_processor_id());\r\n#endif\r\npenguins_are_doing_time = 0;\r\nmembar_safe("#StoreLoad");\r\natomic_dec(&smp_capture_registry);\r\n}\r\n}\r\nvoid __irq_entry smp_penguin_jailcell(int irq, struct pt_regs *regs)\r\n{\r\nclear_softint(1 << irq);\r\npreempt_disable();\r\n__asm__ __volatile__("flushw");\r\nprom_world(1);\r\natomic_inc(&smp_capture_registry);\r\nmembar_safe("#StoreLoad");\r\nwhile (penguins_are_doing_time)\r\nrmb();\r\natomic_dec(&smp_capture_registry);\r\nprom_world(0);\r\npreempt_enable();\r\n}\r\nint setup_profiling_timer(unsigned int multiplier)\r\n{\r\nreturn -EINVAL;\r\n}\r\nvoid __init smp_prepare_cpus(unsigned int max_cpus)\r\n{\r\n}\r\nvoid smp_prepare_boot_cpu(void)\r\n{\r\n}\r\nvoid __init smp_setup_processor_id(void)\r\n{\r\nif (tlb_type == spitfire)\r\nxcall_deliver_impl = spitfire_xcall_deliver;\r\nelse if (tlb_type == cheetah || tlb_type == cheetah_plus)\r\nxcall_deliver_impl = cheetah_xcall_deliver;\r\nelse\r\nxcall_deliver_impl = hypervisor_xcall_deliver;\r\n}\r\nvoid smp_fill_in_sib_core_maps(void)\r\n{\r\nunsigned int i;\r\nfor_each_present_cpu(i) {\r\nunsigned int j;\r\ncpumask_clear(&cpu_core_map[i]);\r\nif (cpu_data(i).core_id == 0) {\r\ncpumask_set_cpu(i, &cpu_core_map[i]);\r\ncontinue;\r\n}\r\nfor_each_present_cpu(j) {\r\nif (cpu_data(i).core_id ==\r\ncpu_data(j).core_id)\r\ncpumask_set_cpu(j, &cpu_core_map[i]);\r\n}\r\n}\r\nfor_each_present_cpu(i) {\r\nunsigned int j;\r\nfor_each_present_cpu(j) {\r\nif (cpu_data(i).sock_id == cpu_data(j).sock_id)\r\ncpumask_set_cpu(j, &cpu_core_sib_map[i]);\r\n}\r\n}\r\nfor_each_present_cpu(i) {\r\nunsigned int j;\r\ncpumask_clear(&per_cpu(cpu_sibling_map, i));\r\nif (cpu_data(i).proc_id == -1) {\r\ncpumask_set_cpu(i, &per_cpu(cpu_sibling_map, i));\r\ncontinue;\r\n}\r\nfor_each_present_cpu(j) {\r\nif (cpu_data(i).proc_id ==\r\ncpu_data(j).proc_id)\r\ncpumask_set_cpu(j, &per_cpu(cpu_sibling_map, i));\r\n}\r\n}\r\n}\r\nint __cpu_up(unsigned int cpu, struct task_struct *tidle)\r\n{\r\nint ret = smp_boot_one_cpu(cpu, tidle);\r\nif (!ret) {\r\ncpumask_set_cpu(cpu, &smp_commenced_mask);\r\nwhile (!cpu_online(cpu))\r\nmb();\r\nif (!cpu_online(cpu)) {\r\nret = -ENODEV;\r\n} else {\r\nif (tlb_type != hypervisor)\r\nsmp_synchronize_one_tick(cpu);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nvoid cpu_play_dead(void)\r\n{\r\nint cpu = smp_processor_id();\r\nunsigned long pstate;\r\nidle_task_exit();\r\nif (tlb_type == hypervisor) {\r\nstruct trap_per_cpu *tb = &trap_block[cpu];\r\nsun4v_cpu_qconf(HV_CPU_QUEUE_CPU_MONDO,\r\ntb->cpu_mondo_pa, 0);\r\nsun4v_cpu_qconf(HV_CPU_QUEUE_DEVICE_MONDO,\r\ntb->dev_mondo_pa, 0);\r\nsun4v_cpu_qconf(HV_CPU_QUEUE_RES_ERROR,\r\ntb->resum_mondo_pa, 0);\r\nsun4v_cpu_qconf(HV_CPU_QUEUE_NONRES_ERROR,\r\ntb->nonresum_mondo_pa, 0);\r\n}\r\ncpumask_clear_cpu(cpu, &smp_commenced_mask);\r\nmembar_safe("#Sync");\r\nlocal_irq_disable();\r\n__asm__ __volatile__(\r\n"rdpr %%pstate, %0\n\t"\r\n"wrpr %0, %1, %%pstate"\r\n: "=r" (pstate)\r\n: "i" (PSTATE_IE));\r\nwhile (1)\r\nbarrier();\r\n}\r\nint __cpu_disable(void)\r\n{\r\nint cpu = smp_processor_id();\r\ncpuinfo_sparc *c;\r\nint i;\r\nfor_each_cpu(i, &cpu_core_map[cpu])\r\ncpumask_clear_cpu(cpu, &cpu_core_map[i]);\r\ncpumask_clear(&cpu_core_map[cpu]);\r\nfor_each_cpu(i, &per_cpu(cpu_sibling_map, cpu))\r\ncpumask_clear_cpu(cpu, &per_cpu(cpu_sibling_map, i));\r\ncpumask_clear(&per_cpu(cpu_sibling_map, cpu));\r\nc = &cpu_data(cpu);\r\nc->core_id = 0;\r\nc->proc_id = -1;\r\nsmp_wmb();\r\nfixup_irqs();\r\nlocal_irq_enable();\r\nmdelay(1);\r\nlocal_irq_disable();\r\nset_cpu_online(cpu, false);\r\ncpu_map_rebuild();\r\nreturn 0;\r\n}\r\nvoid __cpu_die(unsigned int cpu)\r\n{\r\nint i;\r\nfor (i = 0; i < 100; i++) {\r\nsmp_rmb();\r\nif (!cpumask_test_cpu(cpu, &smp_commenced_mask))\r\nbreak;\r\nmsleep(100);\r\n}\r\nif (cpumask_test_cpu(cpu, &smp_commenced_mask)) {\r\nprintk(KERN_ERR "CPU %u didn't die...\n", cpu);\r\n} else {\r\n#if defined(CONFIG_SUN_LDOMS)\r\nunsigned long hv_err;\r\nint limit = 100;\r\ndo {\r\nhv_err = sun4v_cpu_stop(cpu);\r\nif (hv_err == HV_EOK) {\r\nset_cpu_present(cpu, false);\r\nbreak;\r\n}\r\n} while (--limit > 0);\r\nif (limit <= 0) {\r\nprintk(KERN_ERR "sun4v_cpu_stop() fails err=%lu\n",\r\nhv_err);\r\n}\r\n#endif\r\n}\r\n}\r\nvoid __init smp_cpus_done(unsigned int max_cpus)\r\n{\r\n}\r\nvoid smp_send_reschedule(int cpu)\r\n{\r\nif (cpu == smp_processor_id()) {\r\nWARN_ON_ONCE(preemptible());\r\nset_softint(1 << PIL_SMP_RECEIVE_SIGNAL);\r\n} else {\r\nxcall_deliver((u64) &xcall_receive_signal,\r\n0, 0, cpumask_of(cpu));\r\n}\r\n}\r\nvoid __irq_entry smp_receive_signal_client(int irq, struct pt_regs *regs)\r\n{\r\nclear_softint(1 << irq);\r\nscheduler_ipi();\r\n}\r\nstatic void stop_this_cpu(void *dummy)\r\n{\r\nprom_stopself();\r\n}\r\nvoid smp_send_stop(void)\r\n{\r\nint cpu;\r\nif (tlb_type == hypervisor) {\r\nfor_each_online_cpu(cpu) {\r\nif (cpu == smp_processor_id())\r\ncontinue;\r\n#ifdef CONFIG_SUN_LDOMS\r\nif (ldom_domaining_enabled) {\r\nunsigned long hv_err;\r\nhv_err = sun4v_cpu_stop(cpu);\r\nif (hv_err)\r\nprintk(KERN_ERR "sun4v_cpu_stop() "\r\n"failed err=%lu\n", hv_err);\r\n} else\r\n#endif\r\nprom_stopcpu_cpuid(cpu);\r\n}\r\n} else\r\nsmp_call_function(stop_this_cpu, NULL, 0);\r\n}\r\nstatic void * __init pcpu_alloc_bootmem(unsigned int cpu, size_t size,\r\nsize_t align)\r\n{\r\nconst unsigned long goal = __pa(MAX_DMA_ADDRESS);\r\n#ifdef CONFIG_NEED_MULTIPLE_NODES\r\nint node = cpu_to_node(cpu);\r\nvoid *ptr;\r\nif (!node_online(node) || !NODE_DATA(node)) {\r\nptr = __alloc_bootmem(size, align, goal);\r\npr_info("cpu %d has no node %d or node-local memory\n",\r\ncpu, node);\r\npr_debug("per cpu data for cpu%d %lu bytes at %016lx\n",\r\ncpu, size, __pa(ptr));\r\n} else {\r\nptr = __alloc_bootmem_node(NODE_DATA(node),\r\nsize, align, goal);\r\npr_debug("per cpu data for cpu%d %lu bytes on node%d at "\r\n"%016lx\n", cpu, size, node, __pa(ptr));\r\n}\r\nreturn ptr;\r\n#else\r\nreturn __alloc_bootmem(size, align, goal);\r\n#endif\r\n}\r\nstatic void __init pcpu_free_bootmem(void *ptr, size_t size)\r\n{\r\nfree_bootmem(__pa(ptr), size);\r\n}\r\nstatic int __init pcpu_cpu_distance(unsigned int from, unsigned int to)\r\n{\r\nif (cpu_to_node(from) == cpu_to_node(to))\r\nreturn LOCAL_DISTANCE;\r\nelse\r\nreturn REMOTE_DISTANCE;\r\n}\r\nstatic void __init pcpu_populate_pte(unsigned long addr)\r\n{\r\npgd_t *pgd = pgd_offset_k(addr);\r\npud_t *pud;\r\npmd_t *pmd;\r\nif (pgd_none(*pgd)) {\r\npud_t *new;\r\nnew = __alloc_bootmem(PAGE_SIZE, PAGE_SIZE, PAGE_SIZE);\r\npgd_populate(&init_mm, pgd, new);\r\n}\r\npud = pud_offset(pgd, addr);\r\nif (pud_none(*pud)) {\r\npmd_t *new;\r\nnew = __alloc_bootmem(PAGE_SIZE, PAGE_SIZE, PAGE_SIZE);\r\npud_populate(&init_mm, pud, new);\r\n}\r\npmd = pmd_offset(pud, addr);\r\nif (!pmd_present(*pmd)) {\r\npte_t *new;\r\nnew = __alloc_bootmem(PAGE_SIZE, PAGE_SIZE, PAGE_SIZE);\r\npmd_populate_kernel(&init_mm, pmd, new);\r\n}\r\n}\r\nvoid __init setup_per_cpu_areas(void)\r\n{\r\nunsigned long delta;\r\nunsigned int cpu;\r\nint rc = -EINVAL;\r\nif (pcpu_chosen_fc != PCPU_FC_PAGE) {\r\nrc = pcpu_embed_first_chunk(PERCPU_MODULE_RESERVE,\r\nPERCPU_DYNAMIC_RESERVE, 4 << 20,\r\npcpu_cpu_distance,\r\npcpu_alloc_bootmem,\r\npcpu_free_bootmem);\r\nif (rc)\r\npr_warning("PERCPU: %s allocator failed (%d), "\r\n"falling back to page size\n",\r\npcpu_fc_names[pcpu_chosen_fc], rc);\r\n}\r\nif (rc < 0)\r\nrc = pcpu_page_first_chunk(PERCPU_MODULE_RESERVE,\r\npcpu_alloc_bootmem,\r\npcpu_free_bootmem,\r\npcpu_populate_pte);\r\nif (rc < 0)\r\npanic("cannot initialize percpu area (err=%d)", rc);\r\ndelta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;\r\nfor_each_possible_cpu(cpu)\r\n__per_cpu_offset(cpu) = delta + pcpu_unit_offsets[cpu];\r\n__local_per_cpu_offset = __per_cpu_offset(smp_processor_id());\r\nof_fill_in_cpu_data();\r\nif (tlb_type == hypervisor)\r\nmdesc_fill_in_cpu_data(cpu_all_mask);\r\n}
