static u8 *vgic_get_sgi_sources(struct vgic_dist *dist, int vcpu_id, int sgi)\r\n{\r\nreturn dist->irq_sgi_sources + vcpu_id * VGIC_NR_SGIS + sgi;\r\n}\r\nstatic bool handle_mmio_misc(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg;\r\nu32 word_offset = offset & 3;\r\nswitch (offset & ~3) {\r\ncase 0:\r\nreg = vcpu->kvm->arch.vgic.enabled;\r\nvgic_reg_access(mmio, &reg, word_offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nvcpu->kvm->arch.vgic.enabled = reg & 1;\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nbreak;\r\ncase 4:\r\nreg = (atomic_read(&vcpu->kvm->online_vcpus) - 1) << 5;\r\nreg |= (vcpu->kvm->arch.vgic.nr_irqs >> 5) - 1;\r\nvgic_reg_access(mmio, &reg, word_offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nbreak;\r\ncase 8:\r\nreg = (PRODUCT_ID_KVM << 24) | (IMPLEMENTER_ARM << 0);\r\nvgic_reg_access(mmio, &reg, word_offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nbreak;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_set_enable_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nreturn vgic_handle_enable_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id, ACCESS_WRITE_SETBIT);\r\n}\r\nstatic bool handle_mmio_clear_enable_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nreturn vgic_handle_enable_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id, ACCESS_WRITE_CLEARBIT);\r\n}\r\nstatic bool handle_mmio_set_pending_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nreturn vgic_handle_set_pending_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_clear_pending_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nreturn vgic_handle_clear_pending_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_set_active_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nreturn vgic_handle_set_active_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_clear_active_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nreturn vgic_handle_clear_active_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_priority_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg = vgic_bytemap_get_reg(&vcpu->kvm->arch.vgic.irq_priority,\r\nvcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nreturn false;\r\n}\r\nstatic u32 vgic_get_target_reg(struct kvm *kvm, int irq)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nint i;\r\nu32 val = 0;\r\nirq -= VGIC_NR_PRIVATE_IRQS;\r\nfor (i = 0; i < GICD_IRQS_PER_ITARGETSR; i++)\r\nval |= 1 << (dist->irq_spi_cpu[irq + i] + i * 8);\r\nreturn val;\r\n}\r\nstatic void vgic_set_target_reg(struct kvm *kvm, u32 val, int irq)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint i, c;\r\nunsigned long *bmap;\r\nu32 target;\r\nirq -= VGIC_NR_PRIVATE_IRQS;\r\nfor (i = 0; i < GICD_IRQS_PER_ITARGETSR; i++) {\r\nint shift = i * GICD_CPUTARGETS_BITS;\r\ntarget = ffs((val >> shift) & 0xffU);\r\ntarget = target ? (target - 1) : 0;\r\ndist->irq_spi_cpu[irq + i] = target;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nbmap = vgic_bitmap_get_shared_map(&dist->irq_spi_target[c]);\r\nif (c == target)\r\nset_bit(irq + i, bmap);\r\nelse\r\nclear_bit(irq + i, bmap);\r\n}\r\n}\r\n}\r\nstatic bool handle_mmio_target_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 reg;\r\nif (offset < 32) {\r\nu32 roreg;\r\nroreg = 1 << vcpu->vcpu_id;\r\nroreg |= roreg << 8;\r\nroreg |= roreg << 16;\r\nvgic_reg_access(mmio, &roreg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nreg = vgic_get_target_reg(vcpu->kvm, offset & ~3U);\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nvgic_set_target_reg(vcpu->kvm, reg, offset & ~3U);\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_cfg_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 *reg;\r\nreg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_cfg,\r\nvcpu->vcpu_id, offset >> 1);\r\nreturn vgic_handle_cfg_reg(reg, mmio, offset);\r\n}\r\nstatic bool handle_mmio_sgi_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg;\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nvgic_dispatch_sgi(vcpu, reg);\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool read_set_clear_sgi_pend_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nint sgi;\r\nint min_sgi = (offset & ~0x3);\r\nint max_sgi = min_sgi + 3;\r\nint vcpu_id = vcpu->vcpu_id;\r\nu32 reg = 0;\r\nfor (sgi = min_sgi; sgi <= max_sgi; sgi++) {\r\nu8 sources = *vgic_get_sgi_sources(dist, vcpu_id, sgi);\r\nreg |= ((u32)sources) << (8 * (sgi - min_sgi));\r\n}\r\nmmio_data_write(mmio, ~0, reg);\r\nreturn false;\r\n}\r\nstatic bool write_set_clear_sgi_pend_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset, bool set)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nint sgi;\r\nint min_sgi = (offset & ~0x3);\r\nint max_sgi = min_sgi + 3;\r\nint vcpu_id = vcpu->vcpu_id;\r\nu32 reg;\r\nbool updated = false;\r\nreg = mmio_data_read(mmio, ~0);\r\nfor (sgi = min_sgi; sgi <= max_sgi; sgi++) {\r\nu8 mask = reg >> (8 * (sgi - min_sgi));\r\nu8 *src = vgic_get_sgi_sources(dist, vcpu_id, sgi);\r\nif (set) {\r\nif ((*src & mask) != mask)\r\nupdated = true;\r\n*src |= mask;\r\n} else {\r\nif (*src & mask)\r\nupdated = true;\r\n*src &= ~mask;\r\n}\r\n}\r\nif (updated)\r\nvgic_update_state(vcpu->kvm);\r\nreturn updated;\r\n}\r\nstatic bool handle_mmio_sgi_set(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (!mmio->is_write)\r\nreturn read_set_clear_sgi_pend_reg(vcpu, mmio, offset);\r\nelse\r\nreturn write_set_clear_sgi_pend_reg(vcpu, mmio, offset, true);\r\n}\r\nstatic bool handle_mmio_sgi_clear(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (!mmio->is_write)\r\nreturn read_set_clear_sgi_pend_reg(vcpu, mmio, offset);\r\nelse\r\nreturn write_set_clear_sgi_pend_reg(vcpu, mmio, offset, false);\r\n}\r\nstatic void vgic_dispatch_sgi(struct kvm_vcpu *vcpu, u32 reg)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nint nrcpus = atomic_read(&kvm->online_vcpus);\r\nu8 target_cpus;\r\nint sgi, mode, c, vcpu_id;\r\nvcpu_id = vcpu->vcpu_id;\r\nsgi = reg & 0xf;\r\ntarget_cpus = (reg >> 16) & 0xff;\r\nmode = (reg >> 24) & 3;\r\nswitch (mode) {\r\ncase 0:\r\nif (!target_cpus)\r\nreturn;\r\nbreak;\r\ncase 1:\r\ntarget_cpus = ((1 << nrcpus) - 1) & ~(1 << vcpu_id) & 0xff;\r\nbreak;\r\ncase 2:\r\ntarget_cpus = 1 << vcpu_id;\r\nbreak;\r\n}\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nif (target_cpus & 1) {\r\nvgic_dist_irq_set_pending(vcpu, sgi);\r\n*vgic_get_sgi_sources(dist, c, sgi) |= 1 << vcpu_id;\r\nkvm_debug("SGI%d from CPU%d to CPU%d\n",\r\nsgi, vcpu_id, c);\r\n}\r\ntarget_cpus >>= 1;\r\n}\r\n}\r\nstatic bool vgic_v2_queue_sgi(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nunsigned long sources;\r\nint vcpu_id = vcpu->vcpu_id;\r\nint c;\r\nsources = *vgic_get_sgi_sources(dist, vcpu_id, irq);\r\nfor_each_set_bit(c, &sources, dist->nr_cpus) {\r\nif (vgic_queue_irq(vcpu, c, irq))\r\nclear_bit(c, &sources);\r\n}\r\n*vgic_get_sgi_sources(dist, vcpu_id, irq) = sources;\r\nif (!sources) {\r\nvgic_dist_irq_clear_pending(vcpu, irq);\r\nvgic_cpu_irq_clear(vcpu, irq);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int vgic_v2_map_resources(struct kvm *kvm,\r\nconst struct vgic_params *params)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nint ret = 0;\r\nif (!irqchip_in_kernel(kvm))\r\nreturn 0;\r\nmutex_lock(&kvm->lock);\r\nif (vgic_ready(kvm))\r\ngoto out;\r\nif (IS_VGIC_ADDR_UNDEF(dist->vgic_dist_base) ||\r\nIS_VGIC_ADDR_UNDEF(dist->vgic_cpu_base)) {\r\nkvm_err("Need to set vgic cpu and dist addresses first\n");\r\nret = -ENXIO;\r\ngoto out;\r\n}\r\nvgic_register_kvm_io_dev(kvm, dist->vgic_dist_base,\r\nKVM_VGIC_V2_DIST_SIZE,\r\nvgic_dist_ranges, -1, &dist->dist_iodev);\r\nret = vgic_init(kvm);\r\nif (ret) {\r\nkvm_err("Unable to allocate maps\n");\r\ngoto out_unregister;\r\n}\r\nret = kvm_phys_addr_ioremap(kvm, dist->vgic_cpu_base,\r\nparams->vcpu_base, KVM_VGIC_V2_CPU_SIZE,\r\ntrue);\r\nif (ret) {\r\nkvm_err("Unable to remap VGIC CPU to VCPU\n");\r\ngoto out_unregister;\r\n}\r\ndist->ready = true;\r\ngoto out;\r\nout_unregister:\r\nkvm_io_bus_unregister_dev(kvm, KVM_MMIO_BUS, &dist->dist_iodev.dev);\r\nout:\r\nif (ret)\r\nkvm_vgic_destroy(kvm);\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nstatic void vgic_v2_add_sgi_source(struct kvm_vcpu *vcpu, int irq, int source)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\n*vgic_get_sgi_sources(dist, vcpu->vcpu_id, irq) |= 1 << source;\r\n}\r\nstatic int vgic_v2_init_model(struct kvm *kvm)\r\n{\r\nint i;\r\nfor (i = VGIC_NR_PRIVATE_IRQS; i < kvm->arch.vgic.nr_irqs; i += 4)\r\nvgic_set_target_reg(kvm, 0, i);\r\nreturn 0;\r\n}\r\nvoid vgic_v2_init_emulation(struct kvm *kvm)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\ndist->vm_ops.queue_sgi = vgic_v2_queue_sgi;\r\ndist->vm_ops.add_sgi_source = vgic_v2_add_sgi_source;\r\ndist->vm_ops.init_model = vgic_v2_init_model;\r\ndist->vm_ops.map_resources = vgic_v2_map_resources;\r\nkvm->arch.max_vcpus = VGIC_V2_MAX_CPUS;\r\n}\r\nstatic bool handle_cpu_mmio_misc(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nbool updated = false;\r\nstruct vgic_vmcr vmcr;\r\nu32 *vmcr_field;\r\nu32 reg;\r\nvgic_get_vmcr(vcpu, &vmcr);\r\nswitch (offset & ~0x3) {\r\ncase GIC_CPU_CTRL:\r\nvmcr_field = &vmcr.ctlr;\r\nbreak;\r\ncase GIC_CPU_PRIMASK:\r\nvmcr_field = &vmcr.pmr;\r\nbreak;\r\ncase GIC_CPU_BINPOINT:\r\nvmcr_field = &vmcr.bpr;\r\nbreak;\r\ncase GIC_CPU_ALIAS_BINPOINT:\r\nvmcr_field = &vmcr.abpr;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nif (!mmio->is_write) {\r\nreg = *vmcr_field;\r\nmmio_data_write(mmio, ~0, reg);\r\n} else {\r\nreg = mmio_data_read(mmio, ~0);\r\nif (reg != *vmcr_field) {\r\n*vmcr_field = reg;\r\nvgic_set_vmcr(vcpu, &vmcr);\r\nupdated = true;\r\n}\r\n}\r\nreturn updated;\r\n}\r\nstatic bool handle_mmio_abpr(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nreturn handle_cpu_mmio_misc(vcpu, mmio, GIC_CPU_ALIAS_BINPOINT);\r\n}\r\nstatic bool handle_cpu_mmio_ident(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 reg;\r\nif (mmio->is_write)\r\nreturn false;\r\nreg = (PRODUCT_ID_KVM << 20) |\r\n(GICC_ARCH_VERSION_V2 << 16) |\r\n(IMPLEMENTER_ARM << 0);\r\nmmio_data_write(mmio, ~0, reg);\r\nreturn false;\r\n}\r\nstatic int vgic_attr_regs_access(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr,\r\nu32 *reg, bool is_write)\r\n{\r\nconst struct vgic_io_range *r = NULL, *ranges;\r\nphys_addr_t offset;\r\nint ret, cpuid, c;\r\nstruct kvm_vcpu *vcpu, *tmp_vcpu;\r\nstruct vgic_dist *vgic;\r\nstruct kvm_exit_mmio mmio;\r\nu32 data;\r\noffset = attr->attr & KVM_DEV_ARM_VGIC_OFFSET_MASK;\r\ncpuid = (attr->attr & KVM_DEV_ARM_VGIC_CPUID_MASK) >>\r\nKVM_DEV_ARM_VGIC_CPUID_SHIFT;\r\nmutex_lock(&dev->kvm->lock);\r\nret = vgic_init(dev->kvm);\r\nif (ret)\r\ngoto out;\r\nif (cpuid >= atomic_read(&dev->kvm->online_vcpus)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nvcpu = kvm_get_vcpu(dev->kvm, cpuid);\r\nvgic = &dev->kvm->arch.vgic;\r\nmmio.len = 4;\r\nmmio.is_write = is_write;\r\nmmio.data = &data;\r\nif (is_write)\r\nmmio_data_write(&mmio, ~0, *reg);\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\nmmio.phys_addr = vgic->vgic_dist_base + offset;\r\nranges = vgic_dist_ranges;\r\nbreak;\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS:\r\nmmio.phys_addr = vgic->vgic_cpu_base + offset;\r\nranges = vgic_cpu_ranges;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nr = vgic_find_range(ranges, 4, offset);\r\nif (unlikely(!r || !r->handle_mmio)) {\r\nret = -ENXIO;\r\ngoto out;\r\n}\r\nspin_lock(&vgic->lock);\r\nkvm_for_each_vcpu(c, tmp_vcpu, dev->kvm) {\r\nif (unlikely(tmp_vcpu->cpu != -1)) {\r\nret = -EBUSY;\r\ngoto out_vgic_unlock;\r\n}\r\n}\r\nkvm_for_each_vcpu(c, tmp_vcpu, dev->kvm)\r\nvgic_unqueue_irqs(tmp_vcpu);\r\noffset -= r->base;\r\nr->handle_mmio(vcpu, &mmio, offset);\r\nif (!is_write)\r\n*reg = mmio_data_read(&mmio, ~0);\r\nret = 0;\r\nout_vgic_unlock:\r\nspin_unlock(&vgic->lock);\r\nout:\r\nmutex_unlock(&dev->kvm->lock);\r\nreturn ret;\r\n}\r\nstatic int vgic_v2_create(struct kvm_device *dev, u32 type)\r\n{\r\nreturn kvm_vgic_create(dev->kvm, type);\r\n}\r\nstatic void vgic_v2_destroy(struct kvm_device *dev)\r\n{\r\nkfree(dev);\r\n}\r\nstatic int vgic_v2_set_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nint ret;\r\nret = vgic_set_common_attr(dev, attr);\r\nif (ret != -ENXIO)\r\nreturn ret;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS: {\r\nu32 __user *uaddr = (u32 __user *)(long)attr->addr;\r\nu32 reg;\r\nif (get_user(reg, uaddr))\r\nreturn -EFAULT;\r\nreturn vgic_attr_regs_access(dev, attr, &reg, true);\r\n}\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic int vgic_v2_get_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nint ret;\r\nret = vgic_get_common_attr(dev, attr);\r\nif (ret != -ENXIO)\r\nreturn ret;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS: {\r\nu32 __user *uaddr = (u32 __user *)(long)attr->addr;\r\nu32 reg = 0;\r\nret = vgic_attr_regs_access(dev, attr, &reg, false);\r\nif (ret)\r\nreturn ret;\r\nreturn put_user(reg, uaddr);\r\n}\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic int vgic_v2_has_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nphys_addr_t offset;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_ADDR:\r\nswitch (attr->attr) {\r\ncase KVM_VGIC_V2_ADDR_TYPE_DIST:\r\ncase KVM_VGIC_V2_ADDR_TYPE_CPU:\r\nreturn 0;\r\n}\r\nbreak;\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\noffset = attr->attr & KVM_DEV_ARM_VGIC_OFFSET_MASK;\r\nreturn vgic_has_attr_regs(vgic_dist_ranges, offset);\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS:\r\noffset = attr->attr & KVM_DEV_ARM_VGIC_OFFSET_MASK;\r\nreturn vgic_has_attr_regs(vgic_cpu_ranges, offset);\r\ncase KVM_DEV_ARM_VGIC_GRP_NR_IRQS:\r\nreturn 0;\r\ncase KVM_DEV_ARM_VGIC_GRP_CTRL:\r\nswitch (attr->attr) {\r\ncase KVM_DEV_ARM_VGIC_CTRL_INIT:\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENXIO;\r\n}
