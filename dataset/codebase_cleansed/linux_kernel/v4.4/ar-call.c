static unsigned long rxrpc_call_hashfunc(\r\nu8 clientflag,\r\n__be32 cid,\r\n__be32 call_id,\r\n__be32 epoch,\r\n__be16 service_id,\r\nsa_family_t proto,\r\nvoid *localptr,\r\nunsigned int addr_size,\r\nconst u8 *peer_addr)\r\n{\r\nconst u16 *p;\r\nunsigned int i;\r\nunsigned long key;\r\nu32 hcid = ntohl(cid);\r\n_enter("");\r\nkey = (unsigned long)localptr;\r\nkey += (__force u32)epoch;\r\nkey += (__force u16)service_id;\r\nkey += (__force u32)call_id;\r\nkey += (hcid & RXRPC_CIDMASK) >> RXRPC_CIDSHIFT;\r\nkey += hcid & RXRPC_CHANNELMASK;\r\nkey += clientflag;\r\nkey += proto;\r\nfor (i = 0, p = (const u16 *)peer_addr; i < addr_size >> 1; i++, p++)\r\nkey += *p;\r\n_leave(" key = 0x%lx", key);\r\nreturn key;\r\n}\r\nstatic void rxrpc_call_hash_add(struct rxrpc_call *call)\r\n{\r\nunsigned long key;\r\nunsigned int addr_size = 0;\r\n_enter("");\r\nswitch (call->proto) {\r\ncase AF_INET:\r\naddr_size = sizeof(call->peer_ip.ipv4_addr);\r\nbreak;\r\ncase AF_INET6:\r\naddr_size = sizeof(call->peer_ip.ipv6_addr);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nkey = rxrpc_call_hashfunc(call->in_clientflag, call->cid,\r\ncall->call_id, call->epoch,\r\ncall->service_id, call->proto,\r\ncall->conn->trans->local, addr_size,\r\ncall->peer_ip.ipv6_addr);\r\ncall->hash_key = key;\r\nspin_lock(&rxrpc_call_hash_lock);\r\nhash_add_rcu(rxrpc_call_hash, &call->hash_node, key);\r\nspin_unlock(&rxrpc_call_hash_lock);\r\n_leave("");\r\n}\r\nstatic void rxrpc_call_hash_del(struct rxrpc_call *call)\r\n{\r\n_enter("");\r\nspin_lock(&rxrpc_call_hash_lock);\r\nhash_del_rcu(&call->hash_node);\r\nspin_unlock(&rxrpc_call_hash_lock);\r\n_leave("");\r\n}\r\nstruct rxrpc_call *rxrpc_find_call_hash(\r\nu8 clientflag,\r\n__be32 cid,\r\n__be32 call_id,\r\n__be32 epoch,\r\n__be16 service_id,\r\nvoid *localptr,\r\nsa_family_t proto,\r\nconst u8 *peer_addr)\r\n{\r\nunsigned long key;\r\nunsigned int addr_size = 0;\r\nstruct rxrpc_call *call = NULL;\r\nstruct rxrpc_call *ret = NULL;\r\n_enter("");\r\nswitch (proto) {\r\ncase AF_INET:\r\naddr_size = sizeof(call->peer_ip.ipv4_addr);\r\nbreak;\r\ncase AF_INET6:\r\naddr_size = sizeof(call->peer_ip.ipv6_addr);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nkey = rxrpc_call_hashfunc(clientflag, cid, call_id, epoch,\r\nservice_id, proto, localptr, addr_size,\r\npeer_addr);\r\nhash_for_each_possible_rcu(rxrpc_call_hash, call, hash_node, key) {\r\nif (call->hash_key == key &&\r\ncall->call_id == call_id &&\r\ncall->cid == cid &&\r\ncall->in_clientflag == clientflag &&\r\ncall->service_id == service_id &&\r\ncall->proto == proto &&\r\ncall->local == localptr &&\r\nmemcmp(call->peer_ip.ipv6_addr, peer_addr,\r\naddr_size) == 0 &&\r\ncall->epoch == epoch) {\r\nret = call;\r\nbreak;\r\n}\r\n}\r\n_leave(" = %p", ret);\r\nreturn ret;\r\n}\r\nstatic struct rxrpc_call *rxrpc_alloc_call(gfp_t gfp)\r\n{\r\nstruct rxrpc_call *call;\r\ncall = kmem_cache_zalloc(rxrpc_call_jar, gfp);\r\nif (!call)\r\nreturn NULL;\r\ncall->acks_winsz = 16;\r\ncall->acks_window = kmalloc(call->acks_winsz * sizeof(unsigned long),\r\ngfp);\r\nif (!call->acks_window) {\r\nkmem_cache_free(rxrpc_call_jar, call);\r\nreturn NULL;\r\n}\r\nsetup_timer(&call->lifetimer, &rxrpc_call_life_expired,\r\n(unsigned long) call);\r\nsetup_timer(&call->deadspan, &rxrpc_dead_call_expired,\r\n(unsigned long) call);\r\nsetup_timer(&call->ack_timer, &rxrpc_ack_time_expired,\r\n(unsigned long) call);\r\nsetup_timer(&call->resend_timer, &rxrpc_resend_time_expired,\r\n(unsigned long) call);\r\nINIT_WORK(&call->destroyer, &rxrpc_destroy_call);\r\nINIT_WORK(&call->processor, &rxrpc_process_call);\r\nINIT_LIST_HEAD(&call->accept_link);\r\nskb_queue_head_init(&call->rx_queue);\r\nskb_queue_head_init(&call->rx_oos_queue);\r\ninit_waitqueue_head(&call->tx_waitq);\r\nspin_lock_init(&call->lock);\r\nrwlock_init(&call->state_lock);\r\natomic_set(&call->usage, 1);\r\ncall->debug_id = atomic_inc_return(&rxrpc_debug_id);\r\ncall->state = RXRPC_CALL_CLIENT_SEND_REQUEST;\r\nmemset(&call->sock_node, 0xed, sizeof(call->sock_node));\r\ncall->rx_data_expect = 1;\r\ncall->rx_data_eaten = 0;\r\ncall->rx_first_oos = 0;\r\ncall->ackr_win_top = call->rx_data_eaten + 1 + rxrpc_rx_window_size;\r\ncall->creation_jif = jiffies;\r\nreturn call;\r\n}\r\nstatic struct rxrpc_call *rxrpc_alloc_client_call(\r\nstruct rxrpc_sock *rx,\r\nstruct rxrpc_transport *trans,\r\nstruct rxrpc_conn_bundle *bundle,\r\ngfp_t gfp)\r\n{\r\nstruct rxrpc_call *call;\r\nint ret;\r\n_enter("");\r\nASSERT(rx != NULL);\r\nASSERT(trans != NULL);\r\nASSERT(bundle != NULL);\r\ncall = rxrpc_alloc_call(gfp);\r\nif (!call)\r\nreturn ERR_PTR(-ENOMEM);\r\nsock_hold(&rx->sk);\r\ncall->socket = rx;\r\ncall->rx_data_post = 1;\r\nret = rxrpc_connect_call(rx, trans, bundle, call, gfp);\r\nif (ret < 0) {\r\nkmem_cache_free(rxrpc_call_jar, call);\r\nreturn ERR_PTR(ret);\r\n}\r\ncall->proto = rx->proto;\r\ncall->local = trans->local;\r\nswitch (call->proto) {\r\ncase AF_INET:\r\ncall->peer_ip.ipv4_addr =\r\ntrans->peer->srx.transport.sin.sin_addr.s_addr;\r\nbreak;\r\ncase AF_INET6:\r\nmemcpy(call->peer_ip.ipv6_addr,\r\ntrans->peer->srx.transport.sin6.sin6_addr.in6_u.u6_addr8,\r\nsizeof(call->peer_ip.ipv6_addr));\r\nbreak;\r\n}\r\ncall->epoch = call->conn->epoch;\r\ncall->service_id = call->conn->service_id;\r\ncall->in_clientflag = call->conn->in_clientflag;\r\nrxrpc_call_hash_add(call);\r\nspin_lock(&call->conn->trans->peer->lock);\r\nlist_add(&call->error_link, &call->conn->trans->peer->error_targets);\r\nspin_unlock(&call->conn->trans->peer->lock);\r\ncall->lifetimer.expires = jiffies + rxrpc_max_call_lifetime;\r\nadd_timer(&call->lifetimer);\r\n_leave(" = %p", call);\r\nreturn call;\r\n}\r\nstruct rxrpc_call *rxrpc_get_client_call(struct rxrpc_sock *rx,\r\nstruct rxrpc_transport *trans,\r\nstruct rxrpc_conn_bundle *bundle,\r\nunsigned long user_call_ID,\r\nint create,\r\ngfp_t gfp)\r\n{\r\nstruct rxrpc_call *call, *candidate;\r\nstruct rb_node *p, *parent, **pp;\r\n_enter("%p,%d,%d,%lx,%d",\r\nrx, trans ? trans->debug_id : -1, bundle ? bundle->debug_id : -1,\r\nuser_call_ID, create);\r\nread_lock(&rx->call_lock);\r\np = rx->calls.rb_node;\r\nwhile (p) {\r\ncall = rb_entry(p, struct rxrpc_call, sock_node);\r\nif (user_call_ID < call->user_call_ID)\r\np = p->rb_left;\r\nelse if (user_call_ID > call->user_call_ID)\r\np = p->rb_right;\r\nelse\r\ngoto found_extant_call;\r\n}\r\nread_unlock(&rx->call_lock);\r\nif (!create || !trans)\r\nreturn ERR_PTR(-EBADSLT);\r\ncandidate = rxrpc_alloc_client_call(rx, trans, bundle, gfp);\r\nif (IS_ERR(candidate)) {\r\n_leave(" = %ld", PTR_ERR(candidate));\r\nreturn candidate;\r\n}\r\ncandidate->user_call_ID = user_call_ID;\r\n__set_bit(RXRPC_CALL_HAS_USERID, &candidate->flags);\r\nwrite_lock(&rx->call_lock);\r\npp = &rx->calls.rb_node;\r\nparent = NULL;\r\nwhile (*pp) {\r\nparent = *pp;\r\ncall = rb_entry(parent, struct rxrpc_call, sock_node);\r\nif (user_call_ID < call->user_call_ID)\r\npp = &(*pp)->rb_left;\r\nelse if (user_call_ID > call->user_call_ID)\r\npp = &(*pp)->rb_right;\r\nelse\r\ngoto found_extant_second;\r\n}\r\ncall = candidate;\r\ncandidate = NULL;\r\nrxrpc_get_call(call);\r\nrb_link_node(&call->sock_node, parent, pp);\r\nrb_insert_color(&call->sock_node, &rx->calls);\r\nwrite_unlock(&rx->call_lock);\r\nwrite_lock_bh(&rxrpc_call_lock);\r\nlist_add_tail(&call->link, &rxrpc_calls);\r\nwrite_unlock_bh(&rxrpc_call_lock);\r\n_net("CALL new %d on CONN %d", call->debug_id, call->conn->debug_id);\r\n_leave(" = %p [new]", call);\r\nreturn call;\r\nfound_extant_call:\r\nrxrpc_get_call(call);\r\nread_unlock(&rx->call_lock);\r\n_leave(" = %p [extant %d]", call, atomic_read(&call->usage));\r\nreturn call;\r\nfound_extant_second:\r\nrxrpc_get_call(call);\r\nwrite_unlock(&rx->call_lock);\r\nrxrpc_put_call(candidate);\r\n_leave(" = %p [second %d]", call, atomic_read(&call->usage));\r\nreturn call;\r\n}\r\nstruct rxrpc_call *rxrpc_incoming_call(struct rxrpc_sock *rx,\r\nstruct rxrpc_connection *conn,\r\nstruct rxrpc_header *hdr,\r\ngfp_t gfp)\r\n{\r\nstruct rxrpc_call *call, *candidate;\r\nstruct rb_node **p, *parent;\r\n__be32 call_id;\r\n_enter(",%d,,%x", conn->debug_id, gfp);\r\nASSERT(rx != NULL);\r\ncandidate = rxrpc_alloc_call(gfp);\r\nif (!candidate)\r\nreturn ERR_PTR(-EBUSY);\r\ncandidate->socket = rx;\r\ncandidate->conn = conn;\r\ncandidate->cid = hdr->cid;\r\ncandidate->call_id = hdr->callNumber;\r\ncandidate->channel = ntohl(hdr->cid) & RXRPC_CHANNELMASK;\r\ncandidate->rx_data_post = 0;\r\ncandidate->state = RXRPC_CALL_SERVER_ACCEPTING;\r\nif (conn->security_ix > 0)\r\ncandidate->state = RXRPC_CALL_SERVER_SECURING;\r\nwrite_lock_bh(&conn->lock);\r\ncall = conn->channels[candidate->channel];\r\n_debug("channel[%u] is %p", candidate->channel, call);\r\nif (call && call->call_id == hdr->callNumber) {\r\n_debug("extant call [%d]", call->state);\r\nASSERTCMP(call->conn, ==, conn);\r\nread_lock(&call->state_lock);\r\nswitch (call->state) {\r\ncase RXRPC_CALL_LOCALLY_ABORTED:\r\nif (!test_and_set_bit(RXRPC_CALL_ABORT, &call->events))\r\nrxrpc_queue_call(call);\r\ncase RXRPC_CALL_REMOTELY_ABORTED:\r\nread_unlock(&call->state_lock);\r\ngoto aborted_call;\r\ndefault:\r\nrxrpc_get_call(call);\r\nread_unlock(&call->state_lock);\r\ngoto extant_call;\r\n}\r\n}\r\nif (call) {\r\n_debug("CALL: %u { %s }",\r\ncall->debug_id, rxrpc_call_states[call->state]);\r\nif (call->state >= RXRPC_CALL_COMPLETE) {\r\nconn->channels[call->channel] = NULL;\r\n} else {\r\nwrite_unlock_bh(&conn->lock);\r\nkmem_cache_free(rxrpc_call_jar, candidate);\r\n_leave(" = -EBUSY");\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\n}\r\n_debug("check dup");\r\ncall_id = hdr->callNumber;\r\np = &conn->calls.rb_node;\r\nparent = NULL;\r\nwhile (*p) {\r\nparent = *p;\r\ncall = rb_entry(parent, struct rxrpc_call, conn_node);\r\nif ((__force u32)call_id < (__force u32)call->call_id)\r\np = &(*p)->rb_left;\r\nelse if ((__force u32)call_id > (__force u32)call->call_id)\r\np = &(*p)->rb_right;\r\nelse\r\ngoto old_call;\r\n}\r\n_debug("new call");\r\ncall = candidate;\r\ncandidate = NULL;\r\nrb_link_node(&call->conn_node, parent, p);\r\nrb_insert_color(&call->conn_node, &conn->calls);\r\nconn->channels[call->channel] = call;\r\nsock_hold(&rx->sk);\r\natomic_inc(&conn->usage);\r\nwrite_unlock_bh(&conn->lock);\r\nspin_lock(&conn->trans->peer->lock);\r\nlist_add(&call->error_link, &conn->trans->peer->error_targets);\r\nspin_unlock(&conn->trans->peer->lock);\r\nwrite_lock_bh(&rxrpc_call_lock);\r\nlist_add_tail(&call->link, &rxrpc_calls);\r\nwrite_unlock_bh(&rxrpc_call_lock);\r\ncall->proto = rx->proto;\r\ncall->local = conn->trans->local;\r\nswitch (call->proto) {\r\ncase AF_INET:\r\ncall->peer_ip.ipv4_addr =\r\nconn->trans->peer->srx.transport.sin.sin_addr.s_addr;\r\nbreak;\r\ncase AF_INET6:\r\nmemcpy(call->peer_ip.ipv6_addr,\r\nconn->trans->peer->srx.transport.sin6.sin6_addr.in6_u.u6_addr8,\r\nsizeof(call->peer_ip.ipv6_addr));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\ncall->epoch = conn->epoch;\r\ncall->service_id = conn->service_id;\r\ncall->in_clientflag = conn->in_clientflag;\r\nrxrpc_call_hash_add(call);\r\n_net("CALL incoming %d on CONN %d", call->debug_id, call->conn->debug_id);\r\ncall->lifetimer.expires = jiffies + rxrpc_max_call_lifetime;\r\nadd_timer(&call->lifetimer);\r\n_leave(" = %p {%d} [new]", call, call->debug_id);\r\nreturn call;\r\nextant_call:\r\nwrite_unlock_bh(&conn->lock);\r\nkmem_cache_free(rxrpc_call_jar, candidate);\r\n_leave(" = %p {%d} [extant]", call, call ? call->debug_id : -1);\r\nreturn call;\r\naborted_call:\r\nwrite_unlock_bh(&conn->lock);\r\nkmem_cache_free(rxrpc_call_jar, candidate);\r\n_leave(" = -ECONNABORTED");\r\nreturn ERR_PTR(-ECONNABORTED);\r\nold_call:\r\nwrite_unlock_bh(&conn->lock);\r\nkmem_cache_free(rxrpc_call_jar, candidate);\r\n_leave(" = -ECONNRESET [old]");\r\nreturn ERR_PTR(-ECONNRESET);\r\n}\r\nstruct rxrpc_call *rxrpc_find_server_call(struct rxrpc_sock *rx,\r\nunsigned long user_call_ID)\r\n{\r\nstruct rxrpc_call *call;\r\nstruct rb_node *p;\r\n_enter("%p,%lx", rx, user_call_ID);\r\nread_lock(&rx->call_lock);\r\np = rx->calls.rb_node;\r\nwhile (p) {\r\ncall = rb_entry(p, struct rxrpc_call, sock_node);\r\nif (user_call_ID < call->user_call_ID)\r\np = p->rb_left;\r\nelse if (user_call_ID > call->user_call_ID)\r\np = p->rb_right;\r\nelse\r\ngoto found_extant_call;\r\n}\r\nread_unlock(&rx->call_lock);\r\n_leave(" = NULL");\r\nreturn NULL;\r\nfound_extant_call:\r\nrxrpc_get_call(call);\r\nread_unlock(&rx->call_lock);\r\n_leave(" = %p [%d]", call, atomic_read(&call->usage));\r\nreturn call;\r\n}\r\nvoid rxrpc_release_call(struct rxrpc_call *call)\r\n{\r\nstruct rxrpc_connection *conn = call->conn;\r\nstruct rxrpc_sock *rx = call->socket;\r\n_enter("{%d,%d,%d,%d}",\r\ncall->debug_id, atomic_read(&call->usage),\r\natomic_read(&call->ackr_not_idle),\r\ncall->rx_first_oos);\r\nspin_lock_bh(&call->lock);\r\nif (test_and_set_bit(RXRPC_CALL_RELEASED, &call->flags))\r\nBUG();\r\nspin_unlock_bh(&call->lock);\r\n_debug("RELEASE CALL %p (%d CONN %p)", call, call->debug_id, conn);\r\nwrite_lock_bh(&rx->call_lock);\r\nif (!list_empty(&call->accept_link)) {\r\n_debug("unlinking once-pending call %p { e=%lx f=%lx }",\r\ncall, call->events, call->flags);\r\nASSERT(!test_bit(RXRPC_CALL_HAS_USERID, &call->flags));\r\nlist_del_init(&call->accept_link);\r\nsk_acceptq_removed(&rx->sk);\r\n} else if (test_bit(RXRPC_CALL_HAS_USERID, &call->flags)) {\r\nrb_erase(&call->sock_node, &rx->calls);\r\nmemset(&call->sock_node, 0xdd, sizeof(call->sock_node));\r\nclear_bit(RXRPC_CALL_HAS_USERID, &call->flags);\r\n}\r\nwrite_unlock_bh(&rx->call_lock);\r\nspin_lock(&conn->trans->client_lock);\r\nwrite_lock_bh(&conn->lock);\r\nwrite_lock(&call->state_lock);\r\nif (conn->channels[call->channel] == call)\r\nconn->channels[call->channel] = NULL;\r\nif (conn->out_clientflag && conn->bundle) {\r\nconn->avail_calls++;\r\nswitch (conn->avail_calls) {\r\ncase 1:\r\nlist_move_tail(&conn->bundle_link,\r\n&conn->bundle->avail_conns);\r\ncase 2 ... RXRPC_MAXCALLS - 1:\r\nASSERT(conn->channels[0] == NULL ||\r\nconn->channels[1] == NULL ||\r\nconn->channels[2] == NULL ||\r\nconn->channels[3] == NULL);\r\nbreak;\r\ncase RXRPC_MAXCALLS:\r\nlist_move_tail(&conn->bundle_link,\r\n&conn->bundle->unused_conns);\r\nASSERT(conn->channels[0] == NULL &&\r\nconn->channels[1] == NULL &&\r\nconn->channels[2] == NULL &&\r\nconn->channels[3] == NULL);\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "RxRPC: conn->avail_calls=%d\n",\r\nconn->avail_calls);\r\nBUG();\r\n}\r\n}\r\nspin_unlock(&conn->trans->client_lock);\r\nif (call->state < RXRPC_CALL_COMPLETE &&\r\ncall->state != RXRPC_CALL_CLIENT_FINAL_ACK) {\r\n_debug("+++ ABORTING STATE %d +++\n", call->state);\r\ncall->state = RXRPC_CALL_LOCALLY_ABORTED;\r\ncall->abort_code = RX_CALL_DEAD;\r\nset_bit(RXRPC_CALL_ABORT, &call->events);\r\nrxrpc_queue_call(call);\r\n}\r\nwrite_unlock(&call->state_lock);\r\nwrite_unlock_bh(&conn->lock);\r\nif (!skb_queue_empty(&call->rx_queue) ||\r\n!skb_queue_empty(&call->rx_oos_queue)) {\r\nstruct rxrpc_skb_priv *sp;\r\nstruct sk_buff *skb;\r\n_debug("purge Rx queues");\r\nspin_lock_bh(&call->lock);\r\nwhile ((skb = skb_dequeue(&call->rx_queue)) ||\r\n(skb = skb_dequeue(&call->rx_oos_queue))) {\r\nsp = rxrpc_skb(skb);\r\nif (sp->call) {\r\nASSERTCMP(sp->call, ==, call);\r\nrxrpc_put_call(call);\r\nsp->call = NULL;\r\n}\r\nskb->destructor = NULL;\r\nspin_unlock_bh(&call->lock);\r\n_debug("- zap %s %%%u #%u",\r\nrxrpc_pkts[sp->hdr.type],\r\nntohl(sp->hdr.serial),\r\nntohl(sp->hdr.seq));\r\nrxrpc_free_skb(skb);\r\nspin_lock_bh(&call->lock);\r\n}\r\nspin_unlock_bh(&call->lock);\r\nASSERTCMP(call->state, !=, RXRPC_CALL_COMPLETE);\r\n}\r\ndel_timer_sync(&call->resend_timer);\r\ndel_timer_sync(&call->ack_timer);\r\ndel_timer_sync(&call->lifetimer);\r\ncall->deadspan.expires = jiffies + rxrpc_dead_call_expiry;\r\nadd_timer(&call->deadspan);\r\n_leave("");\r\n}\r\nstatic void rxrpc_dead_call_expired(unsigned long _call)\r\n{\r\nstruct rxrpc_call *call = (struct rxrpc_call *) _call;\r\n_enter("{%d}", call->debug_id);\r\nwrite_lock_bh(&call->state_lock);\r\ncall->state = RXRPC_CALL_DEAD;\r\nwrite_unlock_bh(&call->state_lock);\r\nrxrpc_put_call(call);\r\n}\r\nstatic void rxrpc_mark_call_released(struct rxrpc_call *call)\r\n{\r\nbool sched;\r\nwrite_lock(&call->state_lock);\r\nif (call->state < RXRPC_CALL_DEAD) {\r\nsched = false;\r\nif (call->state < RXRPC_CALL_COMPLETE) {\r\n_debug("abort call %p", call);\r\ncall->state = RXRPC_CALL_LOCALLY_ABORTED;\r\ncall->abort_code = RX_CALL_DEAD;\r\nif (!test_and_set_bit(RXRPC_CALL_ABORT, &call->events))\r\nsched = true;\r\n}\r\nif (!test_and_set_bit(RXRPC_CALL_RELEASE, &call->events))\r\nsched = true;\r\nif (sched)\r\nrxrpc_queue_call(call);\r\n}\r\nwrite_unlock(&call->state_lock);\r\n}\r\nvoid rxrpc_release_calls_on_socket(struct rxrpc_sock *rx)\r\n{\r\nstruct rxrpc_call *call;\r\nstruct rb_node *p;\r\n_enter("%p", rx);\r\nread_lock_bh(&rx->call_lock);\r\nfor (p = rb_first(&rx->calls); p; p = rb_next(p)) {\r\ncall = rb_entry(p, struct rxrpc_call, sock_node);\r\nrxrpc_mark_call_released(call);\r\n}\r\nlist_for_each_entry(call, &rx->secureq, accept_link) {\r\nrxrpc_mark_call_released(call);\r\n}\r\nlist_for_each_entry(call, &rx->acceptq, accept_link) {\r\nrxrpc_mark_call_released(call);\r\n}\r\nread_unlock_bh(&rx->call_lock);\r\n_leave("");\r\n}\r\nvoid __rxrpc_put_call(struct rxrpc_call *call)\r\n{\r\nASSERT(call != NULL);\r\n_enter("%p{u=%d}", call, atomic_read(&call->usage));\r\nASSERTCMP(atomic_read(&call->usage), >, 0);\r\nif (atomic_dec_and_test(&call->usage)) {\r\n_debug("call %d dead", call->debug_id);\r\nASSERTCMP(call->state, ==, RXRPC_CALL_DEAD);\r\nrxrpc_queue_work(&call->destroyer);\r\n}\r\n_leave("");\r\n}\r\nstatic void rxrpc_cleanup_call(struct rxrpc_call *call)\r\n{\r\n_net("DESTROY CALL %d", call->debug_id);\r\nASSERT(call->socket);\r\nmemset(&call->sock_node, 0xcd, sizeof(call->sock_node));\r\ndel_timer_sync(&call->lifetimer);\r\ndel_timer_sync(&call->deadspan);\r\ndel_timer_sync(&call->ack_timer);\r\ndel_timer_sync(&call->resend_timer);\r\nASSERT(test_bit(RXRPC_CALL_RELEASED, &call->flags));\r\nASSERTCMP(call->events, ==, 0);\r\nif (work_pending(&call->processor)) {\r\n_debug("defer destroy");\r\nrxrpc_queue_work(&call->destroyer);\r\nreturn;\r\n}\r\nif (call->conn) {\r\nspin_lock(&call->conn->trans->peer->lock);\r\nlist_del(&call->error_link);\r\nspin_unlock(&call->conn->trans->peer->lock);\r\nwrite_lock_bh(&call->conn->lock);\r\nrb_erase(&call->conn_node, &call->conn->calls);\r\nwrite_unlock_bh(&call->conn->lock);\r\nrxrpc_put_connection(call->conn);\r\n}\r\nrxrpc_call_hash_del(call);\r\nif (call->acks_window) {\r\n_debug("kill Tx window %d",\r\nCIRC_CNT(call->acks_head, call->acks_tail,\r\ncall->acks_winsz));\r\nsmp_mb();\r\nwhile (CIRC_CNT(call->acks_head, call->acks_tail,\r\ncall->acks_winsz) > 0) {\r\nstruct rxrpc_skb_priv *sp;\r\nunsigned long _skb;\r\n_skb = call->acks_window[call->acks_tail] & ~1;\r\nsp = rxrpc_skb((struct sk_buff *) _skb);\r\n_debug("+++ clear Tx %u", ntohl(sp->hdr.seq));\r\nrxrpc_free_skb((struct sk_buff *) _skb);\r\ncall->acks_tail =\r\n(call->acks_tail + 1) & (call->acks_winsz - 1);\r\n}\r\nkfree(call->acks_window);\r\n}\r\nrxrpc_free_skb(call->tx_pending);\r\nrxrpc_purge_queue(&call->rx_queue);\r\nASSERT(skb_queue_empty(&call->rx_oos_queue));\r\nsock_put(&call->socket->sk);\r\nkmem_cache_free(rxrpc_call_jar, call);\r\n}\r\nstatic void rxrpc_destroy_call(struct work_struct *work)\r\n{\r\nstruct rxrpc_call *call =\r\ncontainer_of(work, struct rxrpc_call, destroyer);\r\n_enter("%p{%d,%d,%p}",\r\ncall, atomic_read(&call->usage), call->channel, call->conn);\r\nASSERTCMP(call->state, ==, RXRPC_CALL_DEAD);\r\nwrite_lock_bh(&rxrpc_call_lock);\r\nlist_del_init(&call->link);\r\nwrite_unlock_bh(&rxrpc_call_lock);\r\nrxrpc_cleanup_call(call);\r\n_leave("");\r\n}\r\nvoid __exit rxrpc_destroy_all_calls(void)\r\n{\r\nstruct rxrpc_call *call;\r\n_enter("");\r\nwrite_lock_bh(&rxrpc_call_lock);\r\nwhile (!list_empty(&rxrpc_calls)) {\r\ncall = list_entry(rxrpc_calls.next, struct rxrpc_call, link);\r\n_debug("Zapping call %p", call);\r\nlist_del_init(&call->link);\r\nswitch (atomic_read(&call->usage)) {\r\ncase 0:\r\nASSERTCMP(call->state, ==, RXRPC_CALL_DEAD);\r\nbreak;\r\ncase 1:\r\nif (del_timer_sync(&call->deadspan) != 0 &&\r\ncall->state != RXRPC_CALL_DEAD)\r\nrxrpc_dead_call_expired((unsigned long) call);\r\nif (call->state != RXRPC_CALL_DEAD)\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "RXRPC:"\r\n" Call %p still in use (%d,%d,%s,%lx,%lx)!\n",\r\ncall, atomic_read(&call->usage),\r\natomic_read(&call->ackr_not_idle),\r\nrxrpc_call_states[call->state],\r\ncall->flags, call->events);\r\nif (!skb_queue_empty(&call->rx_queue))\r\nprintk(KERN_ERR"RXRPC: Rx queue occupied\n");\r\nif (!skb_queue_empty(&call->rx_oos_queue))\r\nprintk(KERN_ERR"RXRPC: OOS queue occupied\n");\r\nbreak;\r\n}\r\nwrite_unlock_bh(&rxrpc_call_lock);\r\ncond_resched();\r\nwrite_lock_bh(&rxrpc_call_lock);\r\n}\r\nwrite_unlock_bh(&rxrpc_call_lock);\r\n_leave("");\r\n}\r\nstatic void rxrpc_call_life_expired(unsigned long _call)\r\n{\r\nstruct rxrpc_call *call = (struct rxrpc_call *) _call;\r\nif (call->state >= RXRPC_CALL_COMPLETE)\r\nreturn;\r\n_enter("{%d}", call->debug_id);\r\nread_lock_bh(&call->state_lock);\r\nif (call->state < RXRPC_CALL_COMPLETE) {\r\nset_bit(RXRPC_CALL_LIFE_TIMER, &call->events);\r\nrxrpc_queue_call(call);\r\n}\r\nread_unlock_bh(&call->state_lock);\r\n}\r\nstatic void rxrpc_resend_time_expired(unsigned long _call)\r\n{\r\nstruct rxrpc_call *call = (struct rxrpc_call *) _call;\r\n_enter("{%d}", call->debug_id);\r\nif (call->state >= RXRPC_CALL_COMPLETE)\r\nreturn;\r\nclear_bit(RXRPC_CALL_RUN_RTIMER, &call->flags);\r\nif (!test_and_set_bit(RXRPC_CALL_RESEND_TIMER, &call->events))\r\nrxrpc_queue_call(call);\r\n}\r\nstatic void rxrpc_ack_time_expired(unsigned long _call)\r\n{\r\nstruct rxrpc_call *call = (struct rxrpc_call *) _call;\r\n_enter("{%d}", call->debug_id);\r\nif (call->state >= RXRPC_CALL_COMPLETE)\r\nreturn;\r\nread_lock_bh(&call->state_lock);\r\nif (call->state < RXRPC_CALL_COMPLETE &&\r\n!test_and_set_bit(RXRPC_CALL_ACK, &call->events))\r\nrxrpc_queue_call(call);\r\nread_unlock_bh(&call->state_lock);\r\n}
