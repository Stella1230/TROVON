static void aesni_gcm_enc_avx(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long plaintext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nstruct crypto_aes_ctx *aes_ctx = (struct crypto_aes_ctx*)ctx;\r\nif ((plaintext_len < AVX_GEN2_OPTSIZE) || (aes_ctx-> key_length != AES_KEYSIZE_128)){\r\naesni_gcm_enc(ctx, out, in, plaintext_len, iv, hash_subkey, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_enc_avx_gen2(ctx, out, in, plaintext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic void aesni_gcm_dec_avx(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long ciphertext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nstruct crypto_aes_ctx *aes_ctx = (struct crypto_aes_ctx*)ctx;\r\nif ((ciphertext_len < AVX_GEN2_OPTSIZE) || (aes_ctx-> key_length != AES_KEYSIZE_128)) {\r\naesni_gcm_dec(ctx, out, in, ciphertext_len, iv, hash_subkey, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_dec_avx_gen2(ctx, out, in, ciphertext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic void aesni_gcm_enc_avx2(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long plaintext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nstruct crypto_aes_ctx *aes_ctx = (struct crypto_aes_ctx*)ctx;\r\nif ((plaintext_len < AVX_GEN2_OPTSIZE) || (aes_ctx-> key_length != AES_KEYSIZE_128)) {\r\naesni_gcm_enc(ctx, out, in, plaintext_len, iv, hash_subkey, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else if (plaintext_len < AVX_GEN4_OPTSIZE) {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_enc_avx_gen2(ctx, out, in, plaintext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen4(ctx, hash_subkey);\r\naesni_gcm_enc_avx_gen4(ctx, out, in, plaintext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic void aesni_gcm_dec_avx2(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long ciphertext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nstruct crypto_aes_ctx *aes_ctx = (struct crypto_aes_ctx*)ctx;\r\nif ((ciphertext_len < AVX_GEN2_OPTSIZE) || (aes_ctx-> key_length != AES_KEYSIZE_128)) {\r\naesni_gcm_dec(ctx, out, in, ciphertext_len, iv, hash_subkey,\r\naad, aad_len, auth_tag, auth_tag_len);\r\n} else if (ciphertext_len < AVX_GEN4_OPTSIZE) {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_dec_avx_gen2(ctx, out, in, ciphertext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen4(ctx, hash_subkey);\r\naesni_gcm_dec_avx_gen4(ctx, out, in, ciphertext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic inline struct\r\naesni_rfc4106_gcm_ctx *aesni_rfc4106_gcm_ctx_get(struct crypto_aead *tfm)\r\n{\r\nunsigned long align = AESNI_ALIGN;\r\nif (align <= crypto_tfm_ctx_alignment())\r\nalign = 1;\r\nreturn PTR_ALIGN(crypto_aead_ctx(tfm), align);\r\n}\r\nstatic inline struct crypto_aes_ctx *aes_ctx(void *raw_ctx)\r\n{\r\nunsigned long addr = (unsigned long)raw_ctx;\r\nunsigned long align = AESNI_ALIGN;\r\nif (align <= crypto_tfm_ctx_alignment())\r\nalign = 1;\r\nreturn (struct crypto_aes_ctx *)ALIGN(addr, align);\r\n}\r\nstatic int aes_set_key_common(struct crypto_tfm *tfm, void *raw_ctx,\r\nconst u8 *in_key, unsigned int key_len)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(raw_ctx);\r\nu32 *flags = &tfm->crt_flags;\r\nint err;\r\nif (key_len != AES_KEYSIZE_128 && key_len != AES_KEYSIZE_192 &&\r\nkey_len != AES_KEYSIZE_256) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nif (!irq_fpu_usable())\r\nerr = crypto_aes_expand_key(ctx, in_key, key_len);\r\nelse {\r\nkernel_fpu_begin();\r\nerr = aesni_set_key(ctx, in_key, key_len);\r\nkernel_fpu_end();\r\n}\r\nreturn err;\r\n}\r\nstatic int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nreturn aes_set_key_common(tfm, crypto_tfm_ctx(tfm), in_key, key_len);\r\n}\r\nstatic void aes_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\nif (!irq_fpu_usable())\r\ncrypto_aes_encrypt_x86(ctx, dst, src);\r\nelse {\r\nkernel_fpu_begin();\r\naesni_enc(ctx, dst, src);\r\nkernel_fpu_end();\r\n}\r\n}\r\nstatic void aes_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\nif (!irq_fpu_usable())\r\ncrypto_aes_decrypt_x86(ctx, dst, src);\r\nelse {\r\nkernel_fpu_begin();\r\naesni_dec(ctx, dst, src);\r\nkernel_fpu_end();\r\n}\r\n}\r\nstatic void __aes_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\naesni_enc(ctx, dst, src);\r\n}\r\nstatic void __aes_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\naesni_dec(ctx, dst, src);\r\n}\r\nstatic int ecb_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK, walk.iv);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK, walk.iv);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic void ctr_crypt_final(struct crypto_aes_ctx *ctx,\r\nstruct blkcipher_walk *walk)\r\n{\r\nu8 *ctrblk = walk->iv;\r\nu8 keystream[AES_BLOCK_SIZE];\r\nu8 *src = walk->src.virt.addr;\r\nu8 *dst = walk->dst.virt.addr;\r\nunsigned int nbytes = walk->nbytes;\r\naesni_enc(ctx, keystream, ctrblk);\r\ncrypto_xor(keystream, src, nbytes);\r\nmemcpy(dst, keystream, nbytes);\r\ncrypto_inc(ctrblk, AES_BLOCK_SIZE);\r\n}\r\nstatic void aesni_ctr_enc_avx_tfm(struct crypto_aes_ctx *ctx, u8 *out,\r\nconst u8 *in, unsigned int len, u8 *iv)\r\n{\r\nif (ctx->key_length == AES_KEYSIZE_128)\r\naes_ctr_enc_128_avx_by8(in, iv, (void *)ctx, out, len);\r\nelse if (ctx->key_length == AES_KEYSIZE_192)\r\naes_ctr_enc_192_avx_by8(in, iv, (void *)ctx, out, len);\r\nelse\r\naes_ctr_enc_256_avx_by8(in, iv, (void *)ctx, out, len);\r\n}\r\nstatic int ctr_crypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {\r\naesni_ctr_enc_tfm(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK, walk.iv);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nif (walk.nbytes) {\r\nctr_crypt_final(ctx, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, 0);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int ablk_ecb_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "__driver-ecb-aes-aesni");\r\n}\r\nstatic int ablk_cbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "__driver-cbc-aes-aesni");\r\n}\r\nstatic int ablk_ctr_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "__driver-ctr-aes-aesni");\r\n}\r\nstatic int ablk_pcbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "fpu(pcbc(__driver-aes-aesni))");\r\n}\r\nstatic void lrw_xts_encrypt_callback(void *ctx, u8 *blks, unsigned int nbytes)\r\n{\r\naesni_ecb_enc(ctx, blks, blks, nbytes);\r\n}\r\nstatic void lrw_xts_decrypt_callback(void *ctx, u8 *blks, unsigned int nbytes)\r\n{\r\naesni_ecb_dec(ctx, blks, blks, nbytes);\r\n}\r\nstatic int lrw_aesni_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nerr = aes_set_key_common(tfm, ctx->raw_aes_ctx, key,\r\nkeylen - AES_BLOCK_SIZE);\r\nif (err)\r\nreturn err;\r\nreturn lrw_init_table(&ctx->lrw_table, key + keylen - AES_BLOCK_SIZE);\r\n}\r\nstatic void lrw_aesni_exit_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nlrw_free_table(&ctx->lrw_table);\r\n}\r\nstatic int lrw_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = aes_ctx(ctx->raw_aes_ctx),\r\n.crypt_fn = lrw_xts_encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int lrw_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = aes_ctx(ctx->raw_aes_ctx),\r\n.crypt_fn = lrw_xts_decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int xts_aesni_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_tfm_ctx(tfm);\r\nu32 *flags = &tfm->crt_flags;\r\nint err;\r\nif (keylen % 2) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nerr = aes_set_key_common(tfm, ctx->raw_crypt_ctx, key, keylen / 2);\r\nif (err)\r\nreturn err;\r\nreturn aes_set_key_common(tfm, ctx->raw_tweak_ctx, key + keylen / 2,\r\nkeylen / 2);\r\n}\r\nstatic void aesni_xts_tweak(void *ctx, u8 *out, const u8 *in)\r\n{\r\naesni_enc(ctx, out, in);\r\n}\r\nstatic void aesni_xts_enc(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\nglue_xts_crypt_128bit_one(ctx, dst, src, iv, GLUE_FUNC_CAST(aesni_enc));\r\n}\r\nstatic void aesni_xts_dec(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\nglue_xts_crypt_128bit_one(ctx, dst, src, iv, GLUE_FUNC_CAST(aesni_dec));\r\n}\r\nstatic void aesni_xts_enc8(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\naesni_xts_crypt8(ctx, (u8 *)dst, (const u8 *)src, true, (u8 *)iv);\r\n}\r\nstatic void aesni_xts_dec8(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\naesni_xts_crypt8(ctx, (u8 *)dst, (const u8 *)src, false, (u8 *)iv);\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nreturn glue_xts_crypt_128bit(&aesni_enc_xts, desc, dst, src, nbytes,\r\nXTS_TWEAK_CAST(aesni_xts_tweak),\r\naes_ctx(ctx->raw_tweak_ctx),\r\naes_ctx(ctx->raw_crypt_ctx));\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nreturn glue_xts_crypt_128bit(&aesni_dec_xts, desc, dst, src, nbytes,\r\nXTS_TWEAK_CAST(aesni_xts_tweak),\r\naes_ctx(ctx->raw_tweak_ctx),\r\naes_ctx(ctx->raw_crypt_ctx));\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = aes_ctx(ctx->raw_tweak_ctx),\r\n.tweak_fn = aesni_xts_tweak,\r\n.crypt_ctx = aes_ctx(ctx->raw_crypt_ctx),\r\n.crypt_fn = lrw_xts_encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = aes_ctx(ctx->raw_tweak_ctx),\r\n.tweak_fn = aesni_xts_tweak,\r\n.crypt_ctx = aes_ctx(ctx->raw_crypt_ctx),\r\n.crypt_fn = lrw_xts_decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int rfc4106_init(struct crypto_aead *aead)\r\n{\r\nstruct cryptd_aead *cryptd_tfm;\r\nstruct cryptd_aead **ctx = crypto_aead_ctx(aead);\r\ncryptd_tfm = cryptd_alloc_aead("__driver-gcm-aes-aesni",\r\nCRYPTO_ALG_INTERNAL,\r\nCRYPTO_ALG_INTERNAL);\r\nif (IS_ERR(cryptd_tfm))\r\nreturn PTR_ERR(cryptd_tfm);\r\n*ctx = cryptd_tfm;\r\ncrypto_aead_set_reqsize(aead, crypto_aead_reqsize(&cryptd_tfm->base));\r\nreturn 0;\r\n}\r\nstatic void rfc4106_exit(struct crypto_aead *aead)\r\n{\r\nstruct cryptd_aead **ctx = crypto_aead_ctx(aead);\r\ncryptd_free_aead(*ctx);\r\n}\r\nstatic void\r\nrfc4106_set_hash_subkey_done(struct crypto_async_request *req, int err)\r\n{\r\nstruct aesni_gcm_set_hash_subkey_result *result = req->data;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nresult->err = err;\r\ncomplete(&result->completion);\r\n}\r\nstatic int\r\nrfc4106_set_hash_subkey(u8 *hash_subkey, const u8 *key, unsigned int key_len)\r\n{\r\nstruct crypto_ablkcipher *ctr_tfm;\r\nstruct ablkcipher_request *req;\r\nint ret = -EINVAL;\r\nstruct aesni_hash_subkey_req_data *req_data;\r\nctr_tfm = crypto_alloc_ablkcipher("ctr(aes)", 0, 0);\r\nif (IS_ERR(ctr_tfm))\r\nreturn PTR_ERR(ctr_tfm);\r\nret = crypto_ablkcipher_setkey(ctr_tfm, key, key_len);\r\nif (ret)\r\ngoto out_free_ablkcipher;\r\nret = -ENOMEM;\r\nreq = ablkcipher_request_alloc(ctr_tfm, GFP_KERNEL);\r\nif (!req)\r\ngoto out_free_ablkcipher;\r\nreq_data = kmalloc(sizeof(*req_data), GFP_KERNEL);\r\nif (!req_data)\r\ngoto out_free_request;\r\nmemset(req_data->iv, 0, sizeof(req_data->iv));\r\nmemset(hash_subkey, 0, RFC4106_HASH_SUBKEY_SIZE);\r\ninit_completion(&req_data->result.completion);\r\nsg_init_one(&req_data->sg, hash_subkey, RFC4106_HASH_SUBKEY_SIZE);\r\nablkcipher_request_set_tfm(req, ctr_tfm);\r\nablkcipher_request_set_callback(req, CRYPTO_TFM_REQ_MAY_SLEEP |\r\nCRYPTO_TFM_REQ_MAY_BACKLOG,\r\nrfc4106_set_hash_subkey_done,\r\n&req_data->result);\r\nablkcipher_request_set_crypt(req, &req_data->sg,\r\n&req_data->sg, RFC4106_HASH_SUBKEY_SIZE, req_data->iv);\r\nret = crypto_ablkcipher_encrypt(req);\r\nif (ret == -EINPROGRESS || ret == -EBUSY) {\r\nret = wait_for_completion_interruptible\r\n(&req_data->result.completion);\r\nif (!ret)\r\nret = req_data->result.err;\r\n}\r\nkfree(req_data);\r\nout_free_request:\r\nablkcipher_request_free(req);\r\nout_free_ablkcipher:\r\ncrypto_free_ablkcipher(ctr_tfm);\r\nreturn ret;\r\n}\r\nstatic int common_rfc4106_set_key(struct crypto_aead *aead, const u8 *key,\r\nunsigned int key_len)\r\n{\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(aead);\r\nif (key_len < 4) {\r\ncrypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nkey_len -= 4;\r\nmemcpy(ctx->nonce, key + key_len, sizeof(ctx->nonce));\r\nreturn aes_set_key_common(crypto_aead_tfm(aead),\r\n&ctx->aes_key_expanded, key, key_len) ?:\r\nrfc4106_set_hash_subkey(ctx->hash_subkey, key, key_len);\r\n}\r\nstatic int rfc4106_set_key(struct crypto_aead *parent, const u8 *key,\r\nunsigned int key_len)\r\n{\r\nstruct cryptd_aead **ctx = crypto_aead_ctx(parent);\r\nstruct cryptd_aead *cryptd_tfm = *ctx;\r\nreturn crypto_aead_setkey(&cryptd_tfm->base, key, key_len);\r\n}\r\nstatic int common_rfc4106_set_authsize(struct crypto_aead *aead,\r\nunsigned int authsize)\r\n{\r\nswitch (authsize) {\r\ncase 8:\r\ncase 12:\r\ncase 16:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rfc4106_set_authsize(struct crypto_aead *parent,\r\nunsigned int authsize)\r\n{\r\nstruct cryptd_aead **ctx = crypto_aead_ctx(parent);\r\nstruct cryptd_aead *cryptd_tfm = *ctx;\r\nreturn crypto_aead_setauthsize(&cryptd_tfm->base, authsize);\r\n}\r\nstatic int helper_rfc4106_encrypt(struct aead_request *req)\r\n{\r\nu8 one_entry_in_sg = 0;\r\nu8 *src, *dst, *assoc;\r\n__be32 counter = cpu_to_be32(1);\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\r\nvoid *aes_ctx = &(ctx->aes_key_expanded);\r\nunsigned long auth_tag_len = crypto_aead_authsize(tfm);\r\nu8 iv[16] __attribute__ ((__aligned__(AESNI_ALIGN)));\r\nstruct scatter_walk src_sg_walk;\r\nstruct scatter_walk dst_sg_walk;\r\nunsigned int i;\r\nif (unlikely(req->assoclen != 16 && req->assoclen != 20))\r\nreturn -EINVAL;\r\nfor (i = 0; i < 4; i++)\r\n*(iv+i) = ctx->nonce[i];\r\nfor (i = 0; i < 8; i++)\r\n*(iv+4+i) = req->iv[i];\r\n*((__be32 *)(iv+12)) = counter;\r\nif (sg_is_last(req->src) &&\r\nreq->src->offset + req->src->length <= PAGE_SIZE &&\r\nsg_is_last(req->dst) &&\r\nreq->dst->offset + req->dst->length <= PAGE_SIZE) {\r\none_entry_in_sg = 1;\r\nscatterwalk_start(&src_sg_walk, req->src);\r\nassoc = scatterwalk_map(&src_sg_walk);\r\nsrc = assoc + req->assoclen;\r\ndst = src;\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_start(&dst_sg_walk, req->dst);\r\ndst = scatterwalk_map(&dst_sg_walk) + req->assoclen;\r\n}\r\n} else {\r\nassoc = kmalloc(req->cryptlen + auth_tag_len + req->assoclen,\r\nGFP_ATOMIC);\r\nif (unlikely(!assoc))\r\nreturn -ENOMEM;\r\nscatterwalk_map_and_copy(assoc, req->src, 0,\r\nreq->assoclen + req->cryptlen, 0);\r\nsrc = assoc + req->assoclen;\r\ndst = src;\r\n}\r\nkernel_fpu_begin();\r\naesni_gcm_enc_tfm(aes_ctx, dst, src, req->cryptlen, iv,\r\nctx->hash_subkey, assoc, req->assoclen - 8,\r\ndst + req->cryptlen, auth_tag_len);\r\nkernel_fpu_end();\r\nif (one_entry_in_sg) {\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_unmap(dst - req->assoclen);\r\nscatterwalk_advance(&dst_sg_walk, req->dst->length);\r\nscatterwalk_done(&dst_sg_walk, 1, 0);\r\n}\r\nscatterwalk_unmap(assoc);\r\nscatterwalk_advance(&src_sg_walk, req->src->length);\r\nscatterwalk_done(&src_sg_walk, req->src == req->dst, 0);\r\n} else {\r\nscatterwalk_map_and_copy(dst, req->dst, req->assoclen,\r\nreq->cryptlen + auth_tag_len, 1);\r\nkfree(assoc);\r\n}\r\nreturn 0;\r\n}\r\nstatic int helper_rfc4106_decrypt(struct aead_request *req)\r\n{\r\nu8 one_entry_in_sg = 0;\r\nu8 *src, *dst, *assoc;\r\nunsigned long tempCipherLen = 0;\r\n__be32 counter = cpu_to_be32(1);\r\nint retval = 0;\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\r\nvoid *aes_ctx = &(ctx->aes_key_expanded);\r\nunsigned long auth_tag_len = crypto_aead_authsize(tfm);\r\nu8 iv[16] __attribute__ ((__aligned__(AESNI_ALIGN)));\r\nu8 authTag[16];\r\nstruct scatter_walk src_sg_walk;\r\nstruct scatter_walk dst_sg_walk;\r\nunsigned int i;\r\nif (unlikely(req->assoclen != 16 && req->assoclen != 20))\r\nreturn -EINVAL;\r\ntempCipherLen = (unsigned long)(req->cryptlen - auth_tag_len);\r\nfor (i = 0; i < 4; i++)\r\n*(iv+i) = ctx->nonce[i];\r\nfor (i = 0; i < 8; i++)\r\n*(iv+4+i) = req->iv[i];\r\n*((__be32 *)(iv+12)) = counter;\r\nif (sg_is_last(req->src) &&\r\nreq->src->offset + req->src->length <= PAGE_SIZE &&\r\nsg_is_last(req->dst) &&\r\nreq->dst->offset + req->dst->length <= PAGE_SIZE) {\r\none_entry_in_sg = 1;\r\nscatterwalk_start(&src_sg_walk, req->src);\r\nassoc = scatterwalk_map(&src_sg_walk);\r\nsrc = assoc + req->assoclen;\r\ndst = src;\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_start(&dst_sg_walk, req->dst);\r\ndst = scatterwalk_map(&dst_sg_walk) + req->assoclen;\r\n}\r\n} else {\r\nassoc = kmalloc(req->cryptlen + req->assoclen, GFP_ATOMIC);\r\nif (!assoc)\r\nreturn -ENOMEM;\r\nscatterwalk_map_and_copy(assoc, req->src, 0,\r\nreq->assoclen + req->cryptlen, 0);\r\nsrc = assoc + req->assoclen;\r\ndst = src;\r\n}\r\nkernel_fpu_begin();\r\naesni_gcm_dec_tfm(aes_ctx, dst, src, tempCipherLen, iv,\r\nctx->hash_subkey, assoc, req->assoclen - 8,\r\nauthTag, auth_tag_len);\r\nkernel_fpu_end();\r\nretval = crypto_memneq(src + tempCipherLen, authTag, auth_tag_len) ?\r\n-EBADMSG : 0;\r\nif (one_entry_in_sg) {\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_unmap(dst - req->assoclen);\r\nscatterwalk_advance(&dst_sg_walk, req->dst->length);\r\nscatterwalk_done(&dst_sg_walk, 1, 0);\r\n}\r\nscatterwalk_unmap(assoc);\r\nscatterwalk_advance(&src_sg_walk, req->src->length);\r\nscatterwalk_done(&src_sg_walk, req->src == req->dst, 0);\r\n} else {\r\nscatterwalk_map_and_copy(dst, req->dst, req->assoclen,\r\ntempCipherLen, 1);\r\nkfree(assoc);\r\n}\r\nreturn retval;\r\n}\r\nstatic int rfc4106_encrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct cryptd_aead **ctx = crypto_aead_ctx(tfm);\r\nstruct cryptd_aead *cryptd_tfm = *ctx;\r\naead_request_set_tfm(req, irq_fpu_usable() ?\r\ncryptd_aead_child(cryptd_tfm) :\r\n&cryptd_tfm->base);\r\nreturn crypto_aead_encrypt(req);\r\n}\r\nstatic int rfc4106_decrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct cryptd_aead **ctx = crypto_aead_ctx(tfm);\r\nstruct cryptd_aead *cryptd_tfm = *ctx;\r\naead_request_set_tfm(req, irq_fpu_usable() ?\r\ncryptd_aead_child(cryptd_tfm) :\r\n&cryptd_tfm->base);\r\nreturn crypto_aead_decrypt(req);\r\n}\r\nstatic int __init aesni_init(void)\r\n{\r\nint err;\r\nif (!x86_match_cpu(aesni_cpu_id))\r\nreturn -ENODEV;\r\n#ifdef CONFIG_X86_64\r\n#ifdef CONFIG_AS_AVX2\r\nif (boot_cpu_has(X86_FEATURE_AVX2)) {\r\npr_info("AVX2 version of gcm_enc/dec engaged.\n");\r\naesni_gcm_enc_tfm = aesni_gcm_enc_avx2;\r\naesni_gcm_dec_tfm = aesni_gcm_dec_avx2;\r\n} else\r\n#endif\r\n#ifdef CONFIG_AS_AVX\r\nif (boot_cpu_has(X86_FEATURE_AVX)) {\r\npr_info("AVX version of gcm_enc/dec engaged.\n");\r\naesni_gcm_enc_tfm = aesni_gcm_enc_avx;\r\naesni_gcm_dec_tfm = aesni_gcm_dec_avx;\r\n} else\r\n#endif\r\n{\r\npr_info("SSE version of gcm_enc/dec engaged.\n");\r\naesni_gcm_enc_tfm = aesni_gcm_enc;\r\naesni_gcm_dec_tfm = aesni_gcm_dec;\r\n}\r\naesni_ctr_enc_tfm = aesni_ctr_enc;\r\n#ifdef CONFIG_AS_AVX\r\nif (cpu_has_avx) {\r\naesni_ctr_enc_tfm = aesni_ctr_enc_avx_tfm;\r\npr_info("AES CTR mode by8 optimization enabled\n");\r\n}\r\n#endif\r\n#endif\r\nerr = crypto_fpu_init();\r\nif (err)\r\nreturn err;\r\nerr = crypto_register_algs(aesni_algs, ARRAY_SIZE(aesni_algs));\r\nif (err)\r\ngoto fpu_exit;\r\nerr = crypto_register_aeads(aesni_aead_algs,\r\nARRAY_SIZE(aesni_aead_algs));\r\nif (err)\r\ngoto unregister_algs;\r\nreturn err;\r\nunregister_algs:\r\ncrypto_unregister_algs(aesni_algs, ARRAY_SIZE(aesni_algs));\r\nfpu_exit:\r\ncrypto_fpu_exit();\r\nreturn err;\r\n}\r\nstatic void __exit aesni_exit(void)\r\n{\r\ncrypto_unregister_aeads(aesni_aead_algs, ARRAY_SIZE(aesni_aead_algs));\r\ncrypto_unregister_algs(aesni_algs, ARRAY_SIZE(aesni_algs));\r\ncrypto_fpu_exit();\r\n}
