static void xen_qlock_kick(int cpu)\r\n{\r\nxen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);\r\n}\r\nstatic void xen_qlock_wait(u8 *byte, u8 val)\r\n{\r\nint irq = __this_cpu_read(lock_kicker_irq);\r\nif (irq == -1)\r\nreturn;\r\nxen_clear_irq_pending(irq);\r\nbarrier();\r\nif (READ_ONCE(*byte) != val)\r\nreturn;\r\nxen_poll_irq(irq);\r\n}\r\nstatic inline void check_zero(void)\r\n{\r\nu8 ret;\r\nu8 old = READ_ONCE(zero_stats);\r\nif (unlikely(old)) {\r\nret = cmpxchg(&zero_stats, old, 0);\r\nif (ret == old)\r\nmemset(&spinlock_stats, 0, sizeof(spinlock_stats));\r\n}\r\n}\r\nstatic inline void add_stats(enum xen_contention_stat var, u32 val)\r\n{\r\ncheck_zero();\r\nspinlock_stats.contention_stats[var] += val;\r\n}\r\nstatic inline u64 spin_time_start(void)\r\n{\r\nreturn xen_clocksource_read();\r\n}\r\nstatic void __spin_time_accum(u64 delta, u32 *array)\r\n{\r\nunsigned index = ilog2(delta);\r\ncheck_zero();\r\nif (index < HISTO_BUCKETS)\r\narray[index]++;\r\nelse\r\narray[HISTO_BUCKETS]++;\r\n}\r\nstatic inline void spin_time_accum_blocked(u64 start)\r\n{\r\nu32 delta = xen_clocksource_read() - start;\r\n__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);\r\nspinlock_stats.time_blocked += delta;\r\n}\r\nstatic inline void add_stats(enum xen_contention_stat var, u32 val)\r\n{\r\n}\r\nstatic inline u64 spin_time_start(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void spin_time_accum_blocked(u64 start)\r\n{\r\n}\r\n__visible void xen_lock_spinning(struct arch_spinlock *lock, __ticket_t want)\r\n{\r\nint irq = __this_cpu_read(lock_kicker_irq);\r\nstruct xen_lock_waiting *w = this_cpu_ptr(&lock_waiting);\r\nint cpu = smp_processor_id();\r\nu64 start;\r\n__ticket_t head;\r\nunsigned long flags;\r\nif (irq == -1)\r\nreturn;\r\nstart = spin_time_start();\r\nlocal_irq_save(flags);\r\nw->lock = NULL;\r\nsmp_wmb();\r\nw->want = want;\r\nsmp_wmb();\r\nw->lock = lock;\r\ncpumask_set_cpu(cpu, &waiting_cpus);\r\nadd_stats(TAKEN_SLOW, 1);\r\nxen_clear_irq_pending(irq);\r\nbarrier();\r\n__ticket_enter_slowpath(lock);\r\nsmp_mb__after_atomic();\r\nhead = READ_ONCE(lock->tickets.head);\r\nif (__tickets_equal(head, want)) {\r\nadd_stats(TAKEN_SLOW_PICKUP, 1);\r\ngoto out;\r\n}\r\nlocal_irq_restore(flags);\r\nxen_poll_irq(irq);\r\nadd_stats(TAKEN_SLOW_SPURIOUS, !xen_test_irq_pending(irq));\r\nlocal_irq_save(flags);\r\nkstat_incr_irq_this_cpu(irq);\r\nout:\r\ncpumask_clear_cpu(cpu, &waiting_cpus);\r\nw->lock = NULL;\r\nlocal_irq_restore(flags);\r\nspin_time_accum_blocked(start);\r\n}\r\nstatic void xen_unlock_kick(struct arch_spinlock *lock, __ticket_t next)\r\n{\r\nint cpu;\r\nadd_stats(RELEASED_SLOW, 1);\r\nfor_each_cpu(cpu, &waiting_cpus) {\r\nconst struct xen_lock_waiting *w = &per_cpu(lock_waiting, cpu);\r\nif (READ_ONCE(w->lock) == lock &&\r\nREAD_ONCE(w->want) == next) {\r\nadd_stats(RELEASED_SLOW_KICKED, 1);\r\nxen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic irqreturn_t dummy_handler(int irq, void *dev_id)\r\n{\r\nBUG();\r\nreturn IRQ_HANDLED;\r\n}\r\nvoid xen_init_lock_cpu(int cpu)\r\n{\r\nint irq;\r\nchar *name;\r\nif (!xen_pvspin)\r\nreturn;\r\nWARN(per_cpu(lock_kicker_irq, cpu) >= 0, "spinlock on CPU%d exists on IRQ%d!\n",\r\ncpu, per_cpu(lock_kicker_irq, cpu));\r\nname = kasprintf(GFP_KERNEL, "spinlock%d", cpu);\r\nirq = bind_ipi_to_irqhandler(XEN_SPIN_UNLOCK_VECTOR,\r\ncpu,\r\ndummy_handler,\r\nIRQF_PERCPU|IRQF_NOBALANCING,\r\nname,\r\nNULL);\r\nif (irq >= 0) {\r\ndisable_irq(irq);\r\nper_cpu(lock_kicker_irq, cpu) = irq;\r\nper_cpu(irq_name, cpu) = name;\r\n}\r\nprintk("cpu %d spinlock event irq %d\n", cpu, irq);\r\n}\r\nvoid xen_uninit_lock_cpu(int cpu)\r\n{\r\nif (!xen_pvspin)\r\nreturn;\r\nunbind_from_irqhandler(per_cpu(lock_kicker_irq, cpu), NULL);\r\nper_cpu(lock_kicker_irq, cpu) = -1;\r\nkfree(per_cpu(irq_name, cpu));\r\nper_cpu(irq_name, cpu) = NULL;\r\n}\r\nvoid __init xen_init_spinlocks(void)\r\n{\r\nif (!xen_pvspin) {\r\nprintk(KERN_DEBUG "xen: PV spinlocks disabled\n");\r\nreturn;\r\n}\r\nprintk(KERN_DEBUG "xen: PV spinlocks enabled\n");\r\n#ifdef CONFIG_QUEUED_SPINLOCKS\r\n__pv_init_lock_hash();\r\npv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;\r\npv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);\r\npv_lock_ops.wait = xen_qlock_wait;\r\npv_lock_ops.kick = xen_qlock_kick;\r\n#else\r\npv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);\r\npv_lock_ops.unlock_kick = xen_unlock_kick;\r\n#endif\r\n}\r\nstatic __init int xen_init_spinlocks_jump(void)\r\n{\r\nif (!xen_pvspin)\r\nreturn 0;\r\nif (!xen_domain())\r\nreturn 0;\r\nstatic_key_slow_inc(&paravirt_ticketlocks_enabled);\r\nreturn 0;\r\n}\r\nstatic __init int xen_parse_nopvspin(char *arg)\r\n{\r\nxen_pvspin = false;\r\nreturn 0;\r\n}\r\nstatic int __init xen_spinlock_debugfs(void)\r\n{\r\nstruct dentry *d_xen = xen_init_debugfs();\r\nif (d_xen == NULL)\r\nreturn -ENOMEM;\r\nif (!xen_pvspin)\r\nreturn 0;\r\nd_spin_debug = debugfs_create_dir("spinlocks", d_xen);\r\ndebugfs_create_u8("zero_stats", 0644, d_spin_debug, &zero_stats);\r\ndebugfs_create_u32("taken_slow", 0444, d_spin_debug,\r\n&spinlock_stats.contention_stats[TAKEN_SLOW]);\r\ndebugfs_create_u32("taken_slow_pickup", 0444, d_spin_debug,\r\n&spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);\r\ndebugfs_create_u32("taken_slow_spurious", 0444, d_spin_debug,\r\n&spinlock_stats.contention_stats[TAKEN_SLOW_SPURIOUS]);\r\ndebugfs_create_u32("released_slow", 0444, d_spin_debug,\r\n&spinlock_stats.contention_stats[RELEASED_SLOW]);\r\ndebugfs_create_u32("released_slow_kicked", 0444, d_spin_debug,\r\n&spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);\r\ndebugfs_create_u64("time_blocked", 0444, d_spin_debug,\r\n&spinlock_stats.time_blocked);\r\ndebugfs_create_u32_array("histo_blocked", 0444, d_spin_debug,\r\nspinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);\r\nreturn 0;\r\n}
