int fw_iso_buffer_alloc(struct fw_iso_buffer *buffer, int page_count)\r\n{\r\nint i;\r\nbuffer->page_count = 0;\r\nbuffer->page_count_mapped = 0;\r\nbuffer->pages = kmalloc(page_count * sizeof(buffer->pages[0]),\r\nGFP_KERNEL);\r\nif (buffer->pages == NULL)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < page_count; i++) {\r\nbuffer->pages[i] = alloc_page(GFP_KERNEL | GFP_DMA32 | __GFP_ZERO);\r\nif (buffer->pages[i] == NULL)\r\nbreak;\r\n}\r\nbuffer->page_count = i;\r\nif (i < page_count) {\r\nfw_iso_buffer_destroy(buffer, NULL);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nint fw_iso_buffer_map_dma(struct fw_iso_buffer *buffer, struct fw_card *card,\r\nenum dma_data_direction direction)\r\n{\r\ndma_addr_t address;\r\nint i;\r\nbuffer->direction = direction;\r\nfor (i = 0; i < buffer->page_count; i++) {\r\naddress = dma_map_page(card->device, buffer->pages[i],\r\n0, PAGE_SIZE, direction);\r\nif (dma_mapping_error(card->device, address))\r\nbreak;\r\nset_page_private(buffer->pages[i], address);\r\n}\r\nbuffer->page_count_mapped = i;\r\nif (i < buffer->page_count)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint fw_iso_buffer_init(struct fw_iso_buffer *buffer, struct fw_card *card,\r\nint page_count, enum dma_data_direction direction)\r\n{\r\nint ret;\r\nret = fw_iso_buffer_alloc(buffer, page_count);\r\nif (ret < 0)\r\nreturn ret;\r\nret = fw_iso_buffer_map_dma(buffer, card, direction);\r\nif (ret < 0)\r\nfw_iso_buffer_destroy(buffer, card);\r\nreturn ret;\r\n}\r\nint fw_iso_buffer_map_vma(struct fw_iso_buffer *buffer,\r\nstruct vm_area_struct *vma)\r\n{\r\nunsigned long uaddr;\r\nint i, err;\r\nuaddr = vma->vm_start;\r\nfor (i = 0; i < buffer->page_count; i++) {\r\nerr = vm_insert_page(vma, uaddr, buffer->pages[i]);\r\nif (err)\r\nreturn err;\r\nuaddr += PAGE_SIZE;\r\n}\r\nreturn 0;\r\n}\r\nvoid fw_iso_buffer_destroy(struct fw_iso_buffer *buffer,\r\nstruct fw_card *card)\r\n{\r\nint i;\r\ndma_addr_t address;\r\nfor (i = 0; i < buffer->page_count_mapped; i++) {\r\naddress = page_private(buffer->pages[i]);\r\ndma_unmap_page(card->device, address,\r\nPAGE_SIZE, buffer->direction);\r\n}\r\nfor (i = 0; i < buffer->page_count; i++)\r\n__free_page(buffer->pages[i]);\r\nkfree(buffer->pages);\r\nbuffer->pages = NULL;\r\nbuffer->page_count = 0;\r\nbuffer->page_count_mapped = 0;\r\n}\r\nsize_t fw_iso_buffer_lookup(struct fw_iso_buffer *buffer, dma_addr_t completed)\r\n{\r\nsize_t i;\r\ndma_addr_t address;\r\nssize_t offset;\r\nfor (i = 0; i < buffer->page_count; i++) {\r\naddress = page_private(buffer->pages[i]);\r\noffset = (ssize_t)completed - (ssize_t)address;\r\nif (offset > 0 && offset <= PAGE_SIZE)\r\nreturn (i << PAGE_SHIFT) + offset;\r\n}\r\nreturn 0;\r\n}\r\nstruct fw_iso_context *fw_iso_context_create(struct fw_card *card,\r\nint type, int channel, int speed, size_t header_size,\r\nfw_iso_callback_t callback, void *callback_data)\r\n{\r\nstruct fw_iso_context *ctx;\r\nctx = card->driver->allocate_iso_context(card,\r\ntype, channel, header_size);\r\nif (IS_ERR(ctx))\r\nreturn ctx;\r\nctx->card = card;\r\nctx->type = type;\r\nctx->channel = channel;\r\nctx->speed = speed;\r\nctx->header_size = header_size;\r\nctx->callback.sc = callback;\r\nctx->callback_data = callback_data;\r\nreturn ctx;\r\n}\r\nvoid fw_iso_context_destroy(struct fw_iso_context *ctx)\r\n{\r\nctx->card->driver->free_iso_context(ctx);\r\n}\r\nint fw_iso_context_start(struct fw_iso_context *ctx,\r\nint cycle, int sync, int tags)\r\n{\r\nreturn ctx->card->driver->start_iso(ctx, cycle, sync, tags);\r\n}\r\nint fw_iso_context_set_channels(struct fw_iso_context *ctx, u64 *channels)\r\n{\r\nreturn ctx->card->driver->set_iso_channels(ctx, channels);\r\n}\r\nint fw_iso_context_queue(struct fw_iso_context *ctx,\r\nstruct fw_iso_packet *packet,\r\nstruct fw_iso_buffer *buffer,\r\nunsigned long payload)\r\n{\r\nreturn ctx->card->driver->queue_iso(ctx, packet, buffer, payload);\r\n}\r\nvoid fw_iso_context_queue_flush(struct fw_iso_context *ctx)\r\n{\r\nctx->card->driver->flush_queue_iso(ctx);\r\n}\r\nint fw_iso_context_flush_completions(struct fw_iso_context *ctx)\r\n{\r\nreturn ctx->card->driver->flush_iso_completions(ctx);\r\n}\r\nint fw_iso_context_stop(struct fw_iso_context *ctx)\r\n{\r\nreturn ctx->card->driver->stop_iso(ctx);\r\n}\r\nstatic int manage_bandwidth(struct fw_card *card, int irm_id, int generation,\r\nint bandwidth, bool allocate)\r\n{\r\nint try, new, old = allocate ? BANDWIDTH_AVAILABLE_INITIAL : 0;\r\n__be32 data[2];\r\nfor (try = 0; try < 5; try++) {\r\nnew = allocate ? old - bandwidth : old + bandwidth;\r\nif (new < 0 || new > BANDWIDTH_AVAILABLE_INITIAL)\r\nreturn -EBUSY;\r\ndata[0] = cpu_to_be32(old);\r\ndata[1] = cpu_to_be32(new);\r\nswitch (fw_run_transaction(card, TCODE_LOCK_COMPARE_SWAP,\r\nirm_id, generation, SCODE_100,\r\nCSR_REGISTER_BASE + CSR_BANDWIDTH_AVAILABLE,\r\ndata, 8)) {\r\ncase RCODE_GENERATION:\r\nreturn allocate ? -EAGAIN : bandwidth;\r\ncase RCODE_COMPLETE:\r\nif (be32_to_cpup(data) == old)\r\nreturn bandwidth;\r\nold = be32_to_cpup(data);\r\n}\r\n}\r\nreturn -EIO;\r\n}\r\nstatic int manage_channel(struct fw_card *card, int irm_id, int generation,\r\nu32 channels_mask, u64 offset, bool allocate)\r\n{\r\n__be32 bit, all, old;\r\n__be32 data[2];\r\nint channel, ret = -EIO, retry = 5;\r\nold = all = allocate ? cpu_to_be32(~0) : 0;\r\nfor (channel = 0; channel < 32; channel++) {\r\nif (!(channels_mask & 1 << channel))\r\ncontinue;\r\nret = -EBUSY;\r\nbit = cpu_to_be32(1 << (31 - channel));\r\nif ((old & bit) != (all & bit))\r\ncontinue;\r\ndata[0] = old;\r\ndata[1] = old ^ bit;\r\nswitch (fw_run_transaction(card, TCODE_LOCK_COMPARE_SWAP,\r\nirm_id, generation, SCODE_100,\r\noffset, data, 8)) {\r\ncase RCODE_GENERATION:\r\nreturn allocate ? -EAGAIN : channel;\r\ncase RCODE_COMPLETE:\r\nif (data[0] == old)\r\nreturn channel;\r\nold = data[0];\r\nif ((data[0] & bit) == (data[1] & bit))\r\ncontinue;\r\ndefault:\r\nif (retry) {\r\nretry--;\r\nchannel--;\r\n} else {\r\nret = -EIO;\r\n}\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void deallocate_channel(struct fw_card *card, int irm_id,\r\nint generation, int channel)\r\n{\r\nu32 mask;\r\nu64 offset;\r\nmask = channel < 32 ? 1 << channel : 1 << (channel - 32);\r\noffset = channel < 32 ? CSR_REGISTER_BASE + CSR_CHANNELS_AVAILABLE_HI :\r\nCSR_REGISTER_BASE + CSR_CHANNELS_AVAILABLE_LO;\r\nmanage_channel(card, irm_id, generation, mask, offset, false);\r\n}\r\nvoid fw_iso_resource_manage(struct fw_card *card, int generation,\r\nu64 channels_mask, int *channel, int *bandwidth,\r\nbool allocate)\r\n{\r\nu32 channels_hi = channels_mask;\r\nu32 channels_lo = channels_mask >> 32;\r\nint irm_id, ret, c = -EINVAL;\r\nspin_lock_irq(&card->lock);\r\nirm_id = card->irm_node->node_id;\r\nspin_unlock_irq(&card->lock);\r\nif (channels_hi)\r\nc = manage_channel(card, irm_id, generation, channels_hi,\r\nCSR_REGISTER_BASE + CSR_CHANNELS_AVAILABLE_HI,\r\nallocate);\r\nif (channels_lo && c < 0) {\r\nc = manage_channel(card, irm_id, generation, channels_lo,\r\nCSR_REGISTER_BASE + CSR_CHANNELS_AVAILABLE_LO,\r\nallocate);\r\nif (c >= 0)\r\nc += 32;\r\n}\r\n*channel = c;\r\nif (allocate && channels_mask != 0 && c < 0)\r\n*bandwidth = 0;\r\nif (*bandwidth == 0)\r\nreturn;\r\nret = manage_bandwidth(card, irm_id, generation, *bandwidth, allocate);\r\nif (ret < 0)\r\n*bandwidth = 0;\r\nif (allocate && ret < 0) {\r\nif (c >= 0)\r\ndeallocate_channel(card, irm_id, generation, c);\r\n*channel = ret;\r\n}\r\n}
