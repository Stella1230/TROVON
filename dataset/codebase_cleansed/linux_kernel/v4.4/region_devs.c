static void nd_region_release(struct device *dev)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nu16 i;\r\nfor (i = 0; i < nd_region->ndr_mappings; i++) {\r\nstruct nd_mapping *nd_mapping = &nd_region->mapping[i];\r\nstruct nvdimm *nvdimm = nd_mapping->nvdimm;\r\nput_device(&nvdimm->dev);\r\n}\r\nfree_percpu(nd_region->lane);\r\nida_simple_remove(&region_ida, nd_region->id);\r\nif (is_nd_blk(dev))\r\nkfree(to_nd_blk_region(dev));\r\nelse\r\nkfree(nd_region);\r\n}\r\nbool is_nd_pmem(struct device *dev)\r\n{\r\nreturn dev ? dev->type == &nd_pmem_device_type : false;\r\n}\r\nbool is_nd_blk(struct device *dev)\r\n{\r\nreturn dev ? dev->type == &nd_blk_device_type : false;\r\n}\r\nstruct nd_region *to_nd_region(struct device *dev)\r\n{\r\nstruct nd_region *nd_region = container_of(dev, struct nd_region, dev);\r\nWARN_ON(dev->type->release != nd_region_release);\r\nreturn nd_region;\r\n}\r\nstruct nd_blk_region *to_nd_blk_region(struct device *dev)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nWARN_ON(!is_nd_blk(dev));\r\nreturn container_of(nd_region, struct nd_blk_region, nd_region);\r\n}\r\nvoid *nd_region_provider_data(struct nd_region *nd_region)\r\n{\r\nreturn nd_region->provider_data;\r\n}\r\nvoid *nd_blk_region_provider_data(struct nd_blk_region *ndbr)\r\n{\r\nreturn ndbr->blk_provider_data;\r\n}\r\nvoid nd_blk_region_set_provider_data(struct nd_blk_region *ndbr, void *data)\r\n{\r\nndbr->blk_provider_data = data;\r\n}\r\nint nd_region_to_nstype(struct nd_region *nd_region)\r\n{\r\nif (is_nd_pmem(&nd_region->dev)) {\r\nu16 i, alias;\r\nfor (i = 0, alias = 0; i < nd_region->ndr_mappings; i++) {\r\nstruct nd_mapping *nd_mapping = &nd_region->mapping[i];\r\nstruct nvdimm *nvdimm = nd_mapping->nvdimm;\r\nif (nvdimm->flags & NDD_ALIASING)\r\nalias++;\r\n}\r\nif (alias)\r\nreturn ND_DEVICE_NAMESPACE_PMEM;\r\nelse\r\nreturn ND_DEVICE_NAMESPACE_IO;\r\n} else if (is_nd_blk(&nd_region->dev)) {\r\nreturn ND_DEVICE_NAMESPACE_BLK;\r\n}\r\nreturn 0;\r\n}\r\nstatic int is_uuid_busy(struct device *dev, void *data)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev->parent);\r\nu8 *uuid = data;\r\nswitch (nd_region_to_nstype(nd_region)) {\r\ncase ND_DEVICE_NAMESPACE_PMEM: {\r\nstruct nd_namespace_pmem *nspm = to_nd_namespace_pmem(dev);\r\nif (!nspm->uuid)\r\nbreak;\r\nif (memcmp(uuid, nspm->uuid, NSLABEL_UUID_LEN) == 0)\r\nreturn -EBUSY;\r\nbreak;\r\n}\r\ncase ND_DEVICE_NAMESPACE_BLK: {\r\nstruct nd_namespace_blk *nsblk = to_nd_namespace_blk(dev);\r\nif (!nsblk->uuid)\r\nbreak;\r\nif (memcmp(uuid, nsblk->uuid, NSLABEL_UUID_LEN) == 0)\r\nreturn -EBUSY;\r\nbreak;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int is_namespace_uuid_busy(struct device *dev, void *data)\r\n{\r\nif (is_nd_pmem(dev) || is_nd_blk(dev))\r\nreturn device_for_each_child(dev, data, is_uuid_busy);\r\nreturn 0;\r\n}\r\nbool nd_is_uuid_unique(struct device *dev, u8 *uuid)\r\n{\r\nstruct nvdimm_bus *nvdimm_bus = walk_to_nvdimm_bus(dev);\r\nif (!nvdimm_bus)\r\nreturn false;\r\nWARN_ON_ONCE(!is_nvdimm_bus_locked(&nvdimm_bus->dev));\r\nif (device_for_each_child(&nvdimm_bus->dev, uuid,\r\nis_namespace_uuid_busy) != 0)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic ssize_t size_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nunsigned long long size = 0;\r\nif (is_nd_pmem(dev)) {\r\nsize = nd_region->ndr_size;\r\n} else if (nd_region->ndr_mappings == 1) {\r\nstruct nd_mapping *nd_mapping = &nd_region->mapping[0];\r\nsize = nd_mapping->size;\r\n}\r\nreturn sprintf(buf, "%llu\n", size);\r\n}\r\nstatic ssize_t mappings_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nreturn sprintf(buf, "%d\n", nd_region->ndr_mappings);\r\n}\r\nstatic ssize_t nstype_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nreturn sprintf(buf, "%d\n", nd_region_to_nstype(nd_region));\r\n}\r\nstatic ssize_t set_cookie_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nstruct nd_interleave_set *nd_set = nd_region->nd_set;\r\nif (is_nd_pmem(dev) && nd_set)\r\n;\r\nelse\r\nreturn -ENXIO;\r\nreturn sprintf(buf, "%#llx\n", nd_set->cookie);\r\n}\r\nresource_size_t nd_region_available_dpa(struct nd_region *nd_region)\r\n{\r\nresource_size_t blk_max_overlap = 0, available, overlap;\r\nint i;\r\nWARN_ON(!is_nvdimm_bus_locked(&nd_region->dev));\r\nretry:\r\navailable = 0;\r\noverlap = blk_max_overlap;\r\nfor (i = 0; i < nd_region->ndr_mappings; i++) {\r\nstruct nd_mapping *nd_mapping = &nd_region->mapping[i];\r\nstruct nvdimm_drvdata *ndd = to_ndd(nd_mapping);\r\nif (!ndd)\r\nreturn 0;\r\nif (is_nd_pmem(&nd_region->dev)) {\r\navailable += nd_pmem_available_dpa(nd_region,\r\nnd_mapping, &overlap);\r\nif (overlap > blk_max_overlap) {\r\nblk_max_overlap = overlap;\r\ngoto retry;\r\n}\r\n} else if (is_nd_blk(&nd_region->dev)) {\r\navailable += nd_blk_available_dpa(nd_mapping);\r\n}\r\n}\r\nreturn available;\r\n}\r\nstatic ssize_t available_size_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nunsigned long long available = 0;\r\nnvdimm_bus_lock(dev);\r\nwait_nvdimm_bus_probe_idle(dev);\r\navailable = nd_region_available_dpa(nd_region);\r\nnvdimm_bus_unlock(dev);\r\nreturn sprintf(buf, "%llu\n", available);\r\n}\r\nstatic ssize_t init_namespaces_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region_namespaces *num_ns = dev_get_drvdata(dev);\r\nssize_t rc;\r\nnvdimm_bus_lock(dev);\r\nif (num_ns)\r\nrc = sprintf(buf, "%d/%d\n", num_ns->active, num_ns->count);\r\nelse\r\nrc = -ENXIO;\r\nnvdimm_bus_unlock(dev);\r\nreturn rc;\r\n}\r\nstatic ssize_t namespace_seed_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nssize_t rc;\r\nnvdimm_bus_lock(dev);\r\nif (nd_region->ns_seed)\r\nrc = sprintf(buf, "%s\n", dev_name(nd_region->ns_seed));\r\nelse\r\nrc = sprintf(buf, "\n");\r\nnvdimm_bus_unlock(dev);\r\nreturn rc;\r\n}\r\nstatic ssize_t btt_seed_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nssize_t rc;\r\nnvdimm_bus_lock(dev);\r\nif (nd_region->btt_seed)\r\nrc = sprintf(buf, "%s\n", dev_name(nd_region->btt_seed));\r\nelse\r\nrc = sprintf(buf, "\n");\r\nnvdimm_bus_unlock(dev);\r\nreturn rc;\r\n}\r\nstatic ssize_t pfn_seed_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nssize_t rc;\r\nnvdimm_bus_lock(dev);\r\nif (nd_region->pfn_seed)\r\nrc = sprintf(buf, "%s\n", dev_name(nd_region->pfn_seed));\r\nelse\r\nrc = sprintf(buf, "\n");\r\nnvdimm_bus_unlock(dev);\r\nreturn rc;\r\n}\r\nstatic ssize_t read_only_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nreturn sprintf(buf, "%d\n", nd_region->ro);\r\n}\r\nstatic ssize_t read_only_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nbool ro;\r\nint rc = strtobool(buf, &ro);\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nif (rc)\r\nreturn rc;\r\nnd_region->ro = ro;\r\nreturn len;\r\n}\r\nstatic umode_t region_visible(struct kobject *kobj, struct attribute *a, int n)\r\n{\r\nstruct device *dev = container_of(kobj, typeof(*dev), kobj);\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nstruct nd_interleave_set *nd_set = nd_region->nd_set;\r\nint type = nd_region_to_nstype(nd_region);\r\nif (a != &dev_attr_set_cookie.attr\r\n&& a != &dev_attr_available_size.attr)\r\nreturn a->mode;\r\nif ((type == ND_DEVICE_NAMESPACE_PMEM\r\n|| type == ND_DEVICE_NAMESPACE_BLK)\r\n&& a == &dev_attr_available_size.attr)\r\nreturn a->mode;\r\nelse if (is_nd_pmem(dev) && nd_set)\r\nreturn a->mode;\r\nreturn 0;\r\n}\r\nu64 nd_region_interleave_set_cookie(struct nd_region *nd_region)\r\n{\r\nstruct nd_interleave_set *nd_set = nd_region->nd_set;\r\nif (nd_set)\r\nreturn nd_set->cookie;\r\nreturn 0;\r\n}\r\nstatic void nd_region_notify_driver_action(struct nvdimm_bus *nvdimm_bus,\r\nstruct device *dev, bool probe)\r\n{\r\nstruct nd_region *nd_region;\r\nif (!probe && (is_nd_pmem(dev) || is_nd_blk(dev))) {\r\nint i;\r\nnd_region = to_nd_region(dev);\r\nfor (i = 0; i < nd_region->ndr_mappings; i++) {\r\nstruct nd_mapping *nd_mapping = &nd_region->mapping[i];\r\nstruct nvdimm_drvdata *ndd = nd_mapping->ndd;\r\nstruct nvdimm *nvdimm = nd_mapping->nvdimm;\r\nkfree(nd_mapping->labels);\r\nnd_mapping->labels = NULL;\r\nput_ndd(ndd);\r\nnd_mapping->ndd = NULL;\r\nif (ndd)\r\natomic_dec(&nvdimm->busy);\r\n}\r\nif (is_nd_pmem(dev))\r\nreturn;\r\nto_nd_blk_region(dev)->disable(nvdimm_bus, dev);\r\n}\r\nif (dev->parent && is_nd_blk(dev->parent) && probe) {\r\nnd_region = to_nd_region(dev->parent);\r\nnvdimm_bus_lock(dev);\r\nif (nd_region->ns_seed == dev)\r\nnd_region_create_blk_seed(nd_region);\r\nnvdimm_bus_unlock(dev);\r\n}\r\nif (is_nd_btt(dev) && probe) {\r\nstruct nd_btt *nd_btt = to_nd_btt(dev);\r\nnd_region = to_nd_region(dev->parent);\r\nnvdimm_bus_lock(dev);\r\nif (nd_region->btt_seed == dev)\r\nnd_region_create_btt_seed(nd_region);\r\nif (nd_region->ns_seed == &nd_btt->ndns->dev &&\r\nis_nd_blk(dev->parent))\r\nnd_region_create_blk_seed(nd_region);\r\nnvdimm_bus_unlock(dev);\r\n}\r\n}\r\nvoid nd_region_probe_success(struct nvdimm_bus *nvdimm_bus, struct device *dev)\r\n{\r\nnd_region_notify_driver_action(nvdimm_bus, dev, true);\r\n}\r\nvoid nd_region_disable(struct nvdimm_bus *nvdimm_bus, struct device *dev)\r\n{\r\nnd_region_notify_driver_action(nvdimm_bus, dev, false);\r\n}\r\nstatic ssize_t mappingN(struct device *dev, char *buf, int n)\r\n{\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nstruct nd_mapping *nd_mapping;\r\nstruct nvdimm *nvdimm;\r\nif (n >= nd_region->ndr_mappings)\r\nreturn -ENXIO;\r\nnd_mapping = &nd_region->mapping[n];\r\nnvdimm = nd_mapping->nvdimm;\r\nreturn sprintf(buf, "%s,%llu,%llu\n", dev_name(&nvdimm->dev),\r\nnd_mapping->start, nd_mapping->size);\r\n}\r\nstatic umode_t mapping_visible(struct kobject *kobj, struct attribute *a, int n)\r\n{\r\nstruct device *dev = container_of(kobj, struct device, kobj);\r\nstruct nd_region *nd_region = to_nd_region(dev);\r\nif (n < nd_region->ndr_mappings)\r\nreturn a->mode;\r\nreturn 0;\r\n}\r\nint nd_blk_region_init(struct nd_region *nd_region)\r\n{\r\nstruct device *dev = &nd_region->dev;\r\nstruct nvdimm_bus *nvdimm_bus = walk_to_nvdimm_bus(dev);\r\nif (!is_nd_blk(dev))\r\nreturn 0;\r\nif (nd_region->ndr_mappings < 1) {\r\ndev_err(dev, "invalid BLK region\n");\r\nreturn -ENXIO;\r\n}\r\nreturn to_nd_blk_region(dev)->enable(nvdimm_bus, dev);\r\n}\r\nunsigned int nd_region_acquire_lane(struct nd_region *nd_region)\r\n{\r\nunsigned int cpu, lane;\r\ncpu = get_cpu();\r\nif (nd_region->num_lanes < nr_cpu_ids) {\r\nstruct nd_percpu_lane *ndl_lock, *ndl_count;\r\nlane = cpu % nd_region->num_lanes;\r\nndl_count = per_cpu_ptr(nd_region->lane, cpu);\r\nndl_lock = per_cpu_ptr(nd_region->lane, lane);\r\nif (ndl_count->count++ == 0)\r\nspin_lock(&ndl_lock->lock);\r\n} else\r\nlane = cpu;\r\nreturn lane;\r\n}\r\nvoid nd_region_release_lane(struct nd_region *nd_region, unsigned int lane)\r\n{\r\nif (nd_region->num_lanes < nr_cpu_ids) {\r\nunsigned int cpu = get_cpu();\r\nstruct nd_percpu_lane *ndl_lock, *ndl_count;\r\nndl_count = per_cpu_ptr(nd_region->lane, cpu);\r\nndl_lock = per_cpu_ptr(nd_region->lane, lane);\r\nif (--ndl_count->count == 0)\r\nspin_unlock(&ndl_lock->lock);\r\nput_cpu();\r\n}\r\nput_cpu();\r\n}\r\nstatic struct nd_region *nd_region_create(struct nvdimm_bus *nvdimm_bus,\r\nstruct nd_region_desc *ndr_desc, struct device_type *dev_type,\r\nconst char *caller)\r\n{\r\nstruct nd_region *nd_region;\r\nstruct device *dev;\r\nvoid *region_buf;\r\nunsigned int i;\r\nint ro = 0;\r\nfor (i = 0; i < ndr_desc->num_mappings; i++) {\r\nstruct nd_mapping *nd_mapping = &ndr_desc->nd_mapping[i];\r\nstruct nvdimm *nvdimm = nd_mapping->nvdimm;\r\nif ((nd_mapping->start | nd_mapping->size) % SZ_4K) {\r\ndev_err(&nvdimm_bus->dev, "%s: %s mapping%d is not 4K aligned\n",\r\ncaller, dev_name(&nvdimm->dev), i);\r\nreturn NULL;\r\n}\r\nif (nvdimm->flags & NDD_UNARMED)\r\nro = 1;\r\n}\r\nif (dev_type == &nd_blk_device_type) {\r\nstruct nd_blk_region_desc *ndbr_desc;\r\nstruct nd_blk_region *ndbr;\r\nndbr_desc = to_blk_region_desc(ndr_desc);\r\nndbr = kzalloc(sizeof(*ndbr) + sizeof(struct nd_mapping)\r\n* ndr_desc->num_mappings,\r\nGFP_KERNEL);\r\nif (ndbr) {\r\nnd_region = &ndbr->nd_region;\r\nndbr->enable = ndbr_desc->enable;\r\nndbr->disable = ndbr_desc->disable;\r\nndbr->do_io = ndbr_desc->do_io;\r\n}\r\nregion_buf = ndbr;\r\n} else {\r\nnd_region = kzalloc(sizeof(struct nd_region)\r\n+ sizeof(struct nd_mapping)\r\n* ndr_desc->num_mappings,\r\nGFP_KERNEL);\r\nregion_buf = nd_region;\r\n}\r\nif (!region_buf)\r\nreturn NULL;\r\nnd_region->id = ida_simple_get(&region_ida, 0, 0, GFP_KERNEL);\r\nif (nd_region->id < 0)\r\ngoto err_id;\r\nnd_region->lane = alloc_percpu(struct nd_percpu_lane);\r\nif (!nd_region->lane)\r\ngoto err_percpu;\r\nfor (i = 0; i < nr_cpu_ids; i++) {\r\nstruct nd_percpu_lane *ndl;\r\nndl = per_cpu_ptr(nd_region->lane, i);\r\nspin_lock_init(&ndl->lock);\r\nndl->count = 0;\r\n}\r\nmemcpy(nd_region->mapping, ndr_desc->nd_mapping,\r\nsizeof(struct nd_mapping) * ndr_desc->num_mappings);\r\nfor (i = 0; i < ndr_desc->num_mappings; i++) {\r\nstruct nd_mapping *nd_mapping = &ndr_desc->nd_mapping[i];\r\nstruct nvdimm *nvdimm = nd_mapping->nvdimm;\r\nget_device(&nvdimm->dev);\r\n}\r\nnd_region->ndr_mappings = ndr_desc->num_mappings;\r\nnd_region->provider_data = ndr_desc->provider_data;\r\nnd_region->nd_set = ndr_desc->nd_set;\r\nnd_region->num_lanes = ndr_desc->num_lanes;\r\nnd_region->flags = ndr_desc->flags;\r\nnd_region->ro = ro;\r\nnd_region->numa_node = ndr_desc->numa_node;\r\nida_init(&nd_region->ns_ida);\r\nida_init(&nd_region->btt_ida);\r\nida_init(&nd_region->pfn_ida);\r\ndev = &nd_region->dev;\r\ndev_set_name(dev, "region%d", nd_region->id);\r\ndev->parent = &nvdimm_bus->dev;\r\ndev->type = dev_type;\r\ndev->groups = ndr_desc->attr_groups;\r\nnd_region->ndr_size = resource_size(ndr_desc->res);\r\nnd_region->ndr_start = ndr_desc->res->start;\r\nnd_device_register(dev);\r\nreturn nd_region;\r\nerr_percpu:\r\nida_simple_remove(&region_ida, nd_region->id);\r\nerr_id:\r\nkfree(region_buf);\r\nreturn NULL;\r\n}\r\nstruct nd_region *nvdimm_pmem_region_create(struct nvdimm_bus *nvdimm_bus,\r\nstruct nd_region_desc *ndr_desc)\r\n{\r\nndr_desc->num_lanes = ND_MAX_LANES;\r\nreturn nd_region_create(nvdimm_bus, ndr_desc, &nd_pmem_device_type,\r\n__func__);\r\n}\r\nstruct nd_region *nvdimm_blk_region_create(struct nvdimm_bus *nvdimm_bus,\r\nstruct nd_region_desc *ndr_desc)\r\n{\r\nif (ndr_desc->num_mappings > 1)\r\nreturn NULL;\r\nndr_desc->num_lanes = min(ndr_desc->num_lanes, ND_MAX_LANES);\r\nreturn nd_region_create(nvdimm_bus, ndr_desc, &nd_blk_device_type,\r\n__func__);\r\n}\r\nstruct nd_region *nvdimm_volatile_region_create(struct nvdimm_bus *nvdimm_bus,\r\nstruct nd_region_desc *ndr_desc)\r\n{\r\nndr_desc->num_lanes = ND_MAX_LANES;\r\nreturn nd_region_create(nvdimm_bus, ndr_desc, &nd_volatile_device_type,\r\n__func__);\r\n}
