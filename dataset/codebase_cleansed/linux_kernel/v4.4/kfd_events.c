static uint64_t *page_slots(struct signal_page *page)\r\n{\r\nreturn page->kernel_address;\r\n}\r\nstatic bool allocate_free_slot(struct kfd_process *process,\r\nstruct signal_page **out_page,\r\nunsigned int *out_slot_index)\r\n{\r\nstruct signal_page *page;\r\nlist_for_each_entry(page, &process->signal_event_pages, event_pages) {\r\nif (page->free_slots > 0) {\r\nunsigned int slot =\r\nfind_first_zero_bit(page->used_slot_bitmap,\r\nSLOTS_PER_PAGE);\r\n__set_bit(slot, page->used_slot_bitmap);\r\npage->free_slots--;\r\npage_slots(page)[slot] = UNSIGNALED_EVENT_SLOT;\r\n*out_page = page;\r\n*out_slot_index = slot;\r\npr_debug("allocated event signal slot in page %p, slot %d\n",\r\npage, slot);\r\nreturn true;\r\n}\r\n}\r\npr_debug("No free event signal slots were found for process %p\n",\r\nprocess);\r\nreturn false;\r\n}\r\nstatic bool allocate_signal_page(struct file *devkfd, struct kfd_process *p)\r\n{\r\nvoid *backing_store;\r\nstruct signal_page *page;\r\npage = kzalloc(SIGNAL_PAGE_SIZE, GFP_KERNEL);\r\nif (!page)\r\ngoto fail_alloc_signal_page;\r\npage->free_slots = SLOTS_PER_PAGE;\r\nbacking_store = (void *) __get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(KFD_SIGNAL_EVENT_LIMIT * 8));\r\nif (!backing_store)\r\ngoto fail_alloc_signal_store;\r\nmemset(backing_store, (uint8_t) UNSIGNALED_EVENT_SLOT,\r\nKFD_SIGNAL_EVENT_LIMIT * 8);\r\npage->kernel_address = backing_store;\r\nif (list_empty(&p->signal_event_pages))\r\npage->page_index = 0;\r\nelse\r\npage->page_index = list_tail_entry(&p->signal_event_pages,\r\nstruct signal_page,\r\nevent_pages)->page_index + 1;\r\npr_debug("allocated new event signal page at %p, for process %p\n",\r\npage, p);\r\npr_debug("page index is %d\n", page->page_index);\r\nlist_add(&page->event_pages, &p->signal_event_pages);\r\nreturn true;\r\nfail_alloc_signal_store:\r\nkfree(page);\r\nfail_alloc_signal_page:\r\nreturn false;\r\n}\r\nstatic bool allocate_event_notification_slot(struct file *devkfd,\r\nstruct kfd_process *p,\r\nstruct signal_page **page,\r\nunsigned int *signal_slot_index)\r\n{\r\nbool ret;\r\nret = allocate_free_slot(p, page, signal_slot_index);\r\nif (ret == false) {\r\nret = allocate_signal_page(devkfd, p);\r\nif (ret == true)\r\nret = allocate_free_slot(p, page, signal_slot_index);\r\n}\r\nreturn ret;\r\n}\r\nstatic void release_event_notification_slot(struct signal_page *page,\r\nsize_t slot_index)\r\n{\r\n__clear_bit(slot_index, page->used_slot_bitmap);\r\npage->free_slots++;\r\n}\r\nstatic struct signal_page *lookup_signal_page_by_index(struct kfd_process *p,\r\nunsigned int page_index)\r\n{\r\nstruct signal_page *page;\r\nlist_for_each_entry(page, &p->signal_event_pages, event_pages)\r\nif (page->page_index == page_index)\r\nreturn page;\r\nreturn NULL;\r\n}\r\nstatic struct kfd_event *lookup_event_by_id(struct kfd_process *p, uint32_t id)\r\n{\r\nstruct kfd_event *ev;\r\nhash_for_each_possible(p->events, ev, events, id)\r\nif (ev->event_id == id)\r\nreturn ev;\r\nreturn NULL;\r\n}\r\nstatic u32 make_signal_event_id(struct signal_page *page,\r\nunsigned int signal_slot_index)\r\n{\r\nreturn page->page_index |\r\n(signal_slot_index << SIGNAL_EVENT_ID_SLOT_SHIFT);\r\n}\r\nstatic u32 make_nonsignal_event_id(struct kfd_process *p)\r\n{\r\nu32 id;\r\nfor (id = p->next_nonsignal_event_id;\r\nid < KFD_LAST_NONSIGNAL_EVENT_ID &&\r\nlookup_event_by_id(p, id) != NULL;\r\nid++)\r\n;\r\nif (id < KFD_LAST_NONSIGNAL_EVENT_ID) {\r\np->next_nonsignal_event_id = id + 1;\r\nreturn id;\r\n}\r\nfor (id = KFD_FIRST_NONSIGNAL_EVENT_ID;\r\nid < KFD_LAST_NONSIGNAL_EVENT_ID &&\r\nlookup_event_by_id(p, id) != NULL;\r\nid++)\r\n;\r\nif (id < KFD_LAST_NONSIGNAL_EVENT_ID) {\r\np->next_nonsignal_event_id = id + 1;\r\nreturn id;\r\n}\r\np->next_nonsignal_event_id = KFD_FIRST_NONSIGNAL_EVENT_ID;\r\nreturn 0;\r\n}\r\nstatic struct kfd_event *lookup_event_by_page_slot(struct kfd_process *p,\r\nstruct signal_page *page,\r\nunsigned int signal_slot)\r\n{\r\nreturn lookup_event_by_id(p, make_signal_event_id(page, signal_slot));\r\n}\r\nstatic int create_signal_event(struct file *devkfd,\r\nstruct kfd_process *p,\r\nstruct kfd_event *ev)\r\n{\r\nif (p->signal_event_count == KFD_SIGNAL_EVENT_LIMIT) {\r\npr_warn("amdkfd: Signal event wasn't created because limit was reached\n");\r\nreturn -ENOMEM;\r\n}\r\nif (!allocate_event_notification_slot(devkfd, p, &ev->signal_page,\r\n&ev->signal_slot_index)) {\r\npr_warn("amdkfd: Signal event wasn't created because out of kernel memory\n");\r\nreturn -ENOMEM;\r\n}\r\np->signal_event_count++;\r\nev->user_signal_address =\r\n&ev->signal_page->user_address[ev->signal_slot_index];\r\nev->event_id = make_signal_event_id(ev->signal_page,\r\nev->signal_slot_index);\r\npr_debug("signal event number %zu created with id %d, address %p\n",\r\np->signal_event_count, ev->event_id,\r\nev->user_signal_address);\r\npr_debug("signal event number %zu created with id %d, address %p\n",\r\np->signal_event_count, ev->event_id,\r\nev->user_signal_address);\r\nreturn 0;\r\n}\r\nstatic int create_other_event(struct kfd_process *p, struct kfd_event *ev)\r\n{\r\nev->event_id = make_nonsignal_event_id(p);\r\nif (ev->event_id == 0)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid kfd_event_init_process(struct kfd_process *p)\r\n{\r\nmutex_init(&p->event_mutex);\r\nhash_init(p->events);\r\nINIT_LIST_HEAD(&p->signal_event_pages);\r\np->next_nonsignal_event_id = KFD_FIRST_NONSIGNAL_EVENT_ID;\r\np->signal_event_count = 0;\r\n}\r\nstatic void destroy_event(struct kfd_process *p, struct kfd_event *ev)\r\n{\r\nif (ev->signal_page != NULL) {\r\nrelease_event_notification_slot(ev->signal_page,\r\nev->signal_slot_index);\r\np->signal_event_count--;\r\n}\r\nlist_del(&ev->waiters);\r\nhash_del(&ev->events);\r\nkfree(ev);\r\n}\r\nstatic void destroy_events(struct kfd_process *p)\r\n{\r\nstruct kfd_event *ev;\r\nstruct hlist_node *tmp;\r\nunsigned int hash_bkt;\r\nhash_for_each_safe(p->events, hash_bkt, tmp, ev, events)\r\ndestroy_event(p, ev);\r\n}\r\nstatic void shutdown_signal_pages(struct kfd_process *p)\r\n{\r\nstruct signal_page *page, *tmp;\r\nlist_for_each_entry_safe(page, tmp, &p->signal_event_pages,\r\nevent_pages) {\r\nfree_pages((unsigned long)page->kernel_address,\r\nget_order(KFD_SIGNAL_EVENT_LIMIT * 8));\r\nkfree(page);\r\n}\r\n}\r\nvoid kfd_event_free_process(struct kfd_process *p)\r\n{\r\ndestroy_events(p);\r\nshutdown_signal_pages(p);\r\n}\r\nstatic bool event_can_be_gpu_signaled(const struct kfd_event *ev)\r\n{\r\nreturn ev->type == KFD_EVENT_TYPE_SIGNAL ||\r\nev->type == KFD_EVENT_TYPE_DEBUG;\r\n}\r\nstatic bool event_can_be_cpu_signaled(const struct kfd_event *ev)\r\n{\r\nreturn ev->type == KFD_EVENT_TYPE_SIGNAL;\r\n}\r\nint kfd_event_create(struct file *devkfd, struct kfd_process *p,\r\nuint32_t event_type, bool auto_reset, uint32_t node_id,\r\nuint32_t *event_id, uint32_t *event_trigger_data,\r\nuint64_t *event_page_offset, uint32_t *event_slot_index)\r\n{\r\nint ret = 0;\r\nstruct kfd_event *ev = kzalloc(sizeof(*ev), GFP_KERNEL);\r\nif (!ev)\r\nreturn -ENOMEM;\r\nev->type = event_type;\r\nev->auto_reset = auto_reset;\r\nev->signaled = false;\r\nINIT_LIST_HEAD(&ev->waiters);\r\n*event_page_offset = 0;\r\nmutex_lock(&p->event_mutex);\r\nswitch (event_type) {\r\ncase KFD_EVENT_TYPE_SIGNAL:\r\ncase KFD_EVENT_TYPE_DEBUG:\r\nret = create_signal_event(devkfd, p, ev);\r\nif (!ret) {\r\n*event_page_offset = (ev->signal_page->page_index |\r\nKFD_MMAP_EVENTS_MASK);\r\n*event_page_offset <<= PAGE_SHIFT;\r\n*event_slot_index = ev->signal_slot_index;\r\n}\r\nbreak;\r\ndefault:\r\nret = create_other_event(p, ev);\r\nbreak;\r\n}\r\nif (!ret) {\r\nhash_add(p->events, &ev->events, ev->event_id);\r\n*event_id = ev->event_id;\r\n*event_trigger_data = ev->event_id;\r\n} else {\r\nkfree(ev);\r\n}\r\nmutex_unlock(&p->event_mutex);\r\nreturn ret;\r\n}\r\nint kfd_event_destroy(struct kfd_process *p, uint32_t event_id)\r\n{\r\nstruct kfd_event *ev;\r\nint ret = 0;\r\nmutex_lock(&p->event_mutex);\r\nev = lookup_event_by_id(p, event_id);\r\nif (ev)\r\ndestroy_event(p, ev);\r\nelse\r\nret = -EINVAL;\r\nmutex_unlock(&p->event_mutex);\r\nreturn ret;\r\n}\r\nstatic void set_event(struct kfd_event *ev)\r\n{\r\nstruct kfd_event_waiter *waiter;\r\nstruct kfd_event_waiter *next;\r\nev->signaled = !ev->auto_reset || list_empty(&ev->waiters);\r\nlist_for_each_entry_safe(waiter, next, &ev->waiters, waiters) {\r\nwaiter->activated = true;\r\nlist_del_init(&waiter->waiters);\r\nwake_up_process(waiter->sleeping_task);\r\n}\r\n}\r\nint kfd_set_event(struct kfd_process *p, uint32_t event_id)\r\n{\r\nint ret = 0;\r\nstruct kfd_event *ev;\r\nmutex_lock(&p->event_mutex);\r\nev = lookup_event_by_id(p, event_id);\r\nif (ev && event_can_be_cpu_signaled(ev))\r\nset_event(ev);\r\nelse\r\nret = -EINVAL;\r\nmutex_unlock(&p->event_mutex);\r\nreturn ret;\r\n}\r\nstatic void reset_event(struct kfd_event *ev)\r\n{\r\nev->signaled = false;\r\n}\r\nint kfd_reset_event(struct kfd_process *p, uint32_t event_id)\r\n{\r\nint ret = 0;\r\nstruct kfd_event *ev;\r\nmutex_lock(&p->event_mutex);\r\nev = lookup_event_by_id(p, event_id);\r\nif (ev && event_can_be_cpu_signaled(ev))\r\nreset_event(ev);\r\nelse\r\nret = -EINVAL;\r\nmutex_unlock(&p->event_mutex);\r\nreturn ret;\r\n}\r\nstatic void acknowledge_signal(struct kfd_process *p, struct kfd_event *ev)\r\n{\r\npage_slots(ev->signal_page)[ev->signal_slot_index] =\r\nUNSIGNALED_EVENT_SLOT;\r\n}\r\nstatic bool is_slot_signaled(struct signal_page *page, unsigned int index)\r\n{\r\nreturn page_slots(page)[index] != UNSIGNALED_EVENT_SLOT;\r\n}\r\nstatic void set_event_from_interrupt(struct kfd_process *p,\r\nstruct kfd_event *ev)\r\n{\r\nif (ev && event_can_be_gpu_signaled(ev)) {\r\nacknowledge_signal(p, ev);\r\nset_event(ev);\r\n}\r\n}\r\nvoid kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id,\r\nuint32_t valid_id_bits)\r\n{\r\nstruct kfd_event *ev;\r\nstruct kfd_process *p = kfd_lookup_process_by_pasid(pasid);\r\nif (!p)\r\nreturn;\r\nmutex_lock(&p->event_mutex);\r\nif (valid_id_bits >= INTERRUPT_DATA_BITS) {\r\nev = lookup_event_by_id(p, partial_id);\r\nset_event_from_interrupt(p, ev);\r\n} else {\r\nstruct signal_page *page;\r\nunsigned i;\r\nlist_for_each_entry(page, &p->signal_event_pages, event_pages)\r\nfor (i = 0; i < SLOTS_PER_PAGE; i++)\r\nif (is_slot_signaled(page, i)) {\r\nev = lookup_event_by_page_slot(p,\r\npage, i);\r\nset_event_from_interrupt(p, ev);\r\n}\r\n}\r\nmutex_unlock(&p->event_mutex);\r\nmutex_unlock(&p->mutex);\r\n}\r\nstatic struct kfd_event_waiter *alloc_event_waiters(uint32_t num_events)\r\n{\r\nstruct kfd_event_waiter *event_waiters;\r\nuint32_t i;\r\nevent_waiters = kmalloc_array(num_events,\r\nsizeof(struct kfd_event_waiter),\r\nGFP_KERNEL);\r\nfor (i = 0; (event_waiters) && (i < num_events) ; i++) {\r\nINIT_LIST_HEAD(&event_waiters[i].waiters);\r\nevent_waiters[i].sleeping_task = current;\r\nevent_waiters[i].activated = false;\r\n}\r\nreturn event_waiters;\r\n}\r\nstatic int init_event_waiter(struct kfd_process *p,\r\nstruct kfd_event_waiter *waiter,\r\nuint32_t event_id,\r\nuint32_t input_index)\r\n{\r\nstruct kfd_event *ev = lookup_event_by_id(p, event_id);\r\nif (!ev)\r\nreturn -EINVAL;\r\nwaiter->event = ev;\r\nwaiter->input_index = input_index;\r\nwaiter->activated = ev->signaled;\r\nev->signaled = ev->signaled && !ev->auto_reset;\r\nlist_add(&waiter->waiters, &ev->waiters);\r\nreturn 0;\r\n}\r\nstatic bool test_event_condition(bool all, uint32_t num_events,\r\nstruct kfd_event_waiter *event_waiters)\r\n{\r\nuint32_t i;\r\nuint32_t activated_count = 0;\r\nfor (i = 0; i < num_events; i++) {\r\nif (event_waiters[i].activated) {\r\nif (!all)\r\nreturn true;\r\nactivated_count++;\r\n}\r\n}\r\nreturn activated_count == num_events;\r\n}\r\nstatic bool copy_signaled_event_data(uint32_t num_events,\r\nstruct kfd_event_waiter *event_waiters,\r\nstruct kfd_event_data __user *data)\r\n{\r\nstruct kfd_hsa_memory_exception_data *src;\r\nstruct kfd_hsa_memory_exception_data __user *dst;\r\nstruct kfd_event_waiter *waiter;\r\nstruct kfd_event *event;\r\nuint32_t i;\r\nfor (i = 0; i < num_events; i++) {\r\nwaiter = &event_waiters[i];\r\nevent = waiter->event;\r\nif (waiter->activated && event->type == KFD_EVENT_TYPE_MEMORY) {\r\ndst = &data[waiter->input_index].memory_exception_data;\r\nsrc = &event->memory_exception_data;\r\nif (copy_to_user(dst, src,\r\nsizeof(struct kfd_hsa_memory_exception_data)))\r\nreturn false;\r\n}\r\n}\r\nreturn true;\r\n}\r\nstatic long user_timeout_to_jiffies(uint32_t user_timeout_ms)\r\n{\r\nif (user_timeout_ms == KFD_EVENT_TIMEOUT_IMMEDIATE)\r\nreturn 0;\r\nif (user_timeout_ms == KFD_EVENT_TIMEOUT_INFINITE)\r\nreturn MAX_SCHEDULE_TIMEOUT;\r\nuser_timeout_ms = min_t(uint32_t, user_timeout_ms, 0x7FFFFFFF);\r\nreturn msecs_to_jiffies(user_timeout_ms) + 1;\r\n}\r\nstatic void free_waiters(uint32_t num_events, struct kfd_event_waiter *waiters)\r\n{\r\nuint32_t i;\r\nfor (i = 0; i < num_events; i++)\r\nlist_del(&waiters[i].waiters);\r\nkfree(waiters);\r\n}\r\nint kfd_wait_on_events(struct kfd_process *p,\r\nuint32_t num_events, void __user *data,\r\nbool all, uint32_t user_timeout_ms,\r\nenum kfd_event_wait_result *wait_result)\r\n{\r\nstruct kfd_event_data __user *events =\r\n(struct kfd_event_data __user *) data;\r\nuint32_t i;\r\nint ret = 0;\r\nstruct kfd_event_waiter *event_waiters = NULL;\r\nlong timeout = user_timeout_to_jiffies(user_timeout_ms);\r\nmutex_lock(&p->event_mutex);\r\nevent_waiters = alloc_event_waiters(num_events);\r\nif (!event_waiters) {\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nfor (i = 0; i < num_events; i++) {\r\nstruct kfd_event_data event_data;\r\nif (copy_from_user(&event_data, &events[i],\r\nsizeof(struct kfd_event_data)))\r\ngoto fail;\r\nret = init_event_waiter(p, &event_waiters[i],\r\nevent_data.event_id, i);\r\nif (ret)\r\ngoto fail;\r\n}\r\nmutex_unlock(&p->event_mutex);\r\nwhile (true) {\r\nif (fatal_signal_pending(current)) {\r\nret = -EINTR;\r\nbreak;\r\n}\r\nif (signal_pending(current)) {\r\nret = -ERESTARTSYS;\r\nbreak;\r\n}\r\nif (test_event_condition(all, num_events, event_waiters)) {\r\nif (copy_signaled_event_data(num_events,\r\nevent_waiters, events))\r\n*wait_result = KFD_WAIT_COMPLETE;\r\nelse\r\n*wait_result = KFD_WAIT_ERROR;\r\nbreak;\r\n}\r\nif (timeout <= 0) {\r\n*wait_result = KFD_WAIT_TIMEOUT;\r\nbreak;\r\n}\r\ntimeout = schedule_timeout_interruptible(timeout);\r\n}\r\n__set_current_state(TASK_RUNNING);\r\nmutex_lock(&p->event_mutex);\r\nfree_waiters(num_events, event_waiters);\r\nmutex_unlock(&p->event_mutex);\r\nreturn ret;\r\nfail:\r\nif (event_waiters)\r\nfree_waiters(num_events, event_waiters);\r\nmutex_unlock(&p->event_mutex);\r\n*wait_result = KFD_WAIT_ERROR;\r\nreturn ret;\r\n}\r\nint kfd_event_mmap(struct kfd_process *p, struct vm_area_struct *vma)\r\n{\r\nunsigned int page_index;\r\nunsigned long pfn;\r\nstruct signal_page *page;\r\nif (get_order(KFD_SIGNAL_EVENT_LIMIT * 8) !=\r\nget_order(vma->vm_end - vma->vm_start)) {\r\npr_err("amdkfd: event page mmap requested illegal size\n");\r\nreturn -EINVAL;\r\n}\r\npage_index = vma->vm_pgoff;\r\npage = lookup_signal_page_by_index(p, page_index);\r\nif (!page) {\r\npr_debug("signal page could not be found for page_index %u\n",\r\npage_index);\r\nreturn -EINVAL;\r\n}\r\npfn = __pa(page->kernel_address);\r\npfn >>= PAGE_SHIFT;\r\nvma->vm_flags |= VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE\r\n| VM_DONTDUMP | VM_PFNMAP;\r\npr_debug("mapping signal page\n");\r\npr_debug(" start user address == 0x%08lx\n", vma->vm_start);\r\npr_debug(" end user address == 0x%08lx\n", vma->vm_end);\r\npr_debug(" pfn == 0x%016lX\n", pfn);\r\npr_debug(" vm_flags == 0x%08lX\n", vma->vm_flags);\r\npr_debug(" size == 0x%08lX\n",\r\nvma->vm_end - vma->vm_start);\r\npage->user_address = (uint64_t __user *)vma->vm_start;\r\nreturn remap_pfn_range(vma, vma->vm_start, pfn,\r\nvma->vm_end - vma->vm_start, vma->vm_page_prot);\r\n}\r\nstatic void lookup_events_by_type_and_signal(struct kfd_process *p,\r\nint type, void *event_data)\r\n{\r\nstruct kfd_hsa_memory_exception_data *ev_data;\r\nstruct kfd_event *ev;\r\nint bkt;\r\nbool send_signal = true;\r\nev_data = (struct kfd_hsa_memory_exception_data *) event_data;\r\nhash_for_each(p->events, bkt, ev, events)\r\nif (ev->type == type) {\r\nsend_signal = false;\r\ndev_dbg(kfd_device,\r\n"Event found: id %X type %d",\r\nev->event_id, ev->type);\r\nset_event(ev);\r\nif (ev->type == KFD_EVENT_TYPE_MEMORY && ev_data)\r\nev->memory_exception_data = *ev_data;\r\n}\r\nif (send_signal) {\r\nif (send_sigterm) {\r\ndev_warn(kfd_device,\r\n"Sending SIGTERM to HSA Process with PID %d ",\r\np->lead_thread->pid);\r\nsend_sig(SIGTERM, p->lead_thread, 0);\r\n} else {\r\ndev_err(kfd_device,\r\n"HSA Process (PID %d) got unhandled exception",\r\np->lead_thread->pid);\r\n}\r\n}\r\n}\r\nvoid kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,\r\nunsigned long address, bool is_write_requested,\r\nbool is_execute_requested)\r\n{\r\nstruct kfd_hsa_memory_exception_data memory_exception_data;\r\nstruct vm_area_struct *vma;\r\nstruct kfd_process *p = kfd_lookup_process_by_pasid(pasid);\r\nif (!p)\r\nreturn;\r\nmemset(&memory_exception_data, 0, sizeof(memory_exception_data));\r\ndown_read(&p->mm->mmap_sem);\r\nvma = find_vma(p->mm, address);\r\nmemory_exception_data.gpu_id = dev->id;\r\nmemory_exception_data.va = address;\r\nmemory_exception_data.failure.NotPresent = 1;\r\nmemory_exception_data.failure.NoExecute = 0;\r\nmemory_exception_data.failure.ReadOnly = 0;\r\nif (vma) {\r\nif (vma->vm_start > address) {\r\nmemory_exception_data.failure.NotPresent = 1;\r\nmemory_exception_data.failure.NoExecute = 0;\r\nmemory_exception_data.failure.ReadOnly = 0;\r\n} else {\r\nmemory_exception_data.failure.NotPresent = 0;\r\nif (is_write_requested && !(vma->vm_flags & VM_WRITE))\r\nmemory_exception_data.failure.ReadOnly = 1;\r\nelse\r\nmemory_exception_data.failure.ReadOnly = 0;\r\nif (is_execute_requested && !(vma->vm_flags & VM_EXEC))\r\nmemory_exception_data.failure.NoExecute = 1;\r\nelse\r\nmemory_exception_data.failure.NoExecute = 0;\r\n}\r\n}\r\nup_read(&p->mm->mmap_sem);\r\nmutex_lock(&p->event_mutex);\r\nlookup_events_by_type_and_signal(p, KFD_EVENT_TYPE_MEMORY,\r\n&memory_exception_data);\r\nmutex_unlock(&p->event_mutex);\r\nmutex_unlock(&p->mutex);\r\n}\r\nvoid kfd_signal_hw_exception_event(unsigned int pasid)\r\n{\r\nstruct kfd_process *p = kfd_lookup_process_by_pasid(pasid);\r\nif (!p)\r\nreturn;\r\nmutex_lock(&p->event_mutex);\r\nlookup_events_by_type_and_signal(p, KFD_EVENT_TYPE_HW_EXCEPTION, NULL);\r\nmutex_unlock(&p->event_mutex);\r\nmutex_unlock(&p->mutex);\r\n}
