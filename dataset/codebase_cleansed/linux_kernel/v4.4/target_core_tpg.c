struct se_node_acl *__core_tpg_get_initiator_node_acl(\r\nstruct se_portal_group *tpg,\r\nconst char *initiatorname)\r\n{\r\nstruct se_node_acl *acl;\r\nlist_for_each_entry(acl, &tpg->acl_node_list, acl_list) {\r\nif (!strcmp(acl->initiatorname, initiatorname))\r\nreturn acl;\r\n}\r\nreturn NULL;\r\n}\r\nstruct se_node_acl *core_tpg_get_initiator_node_acl(\r\nstruct se_portal_group *tpg,\r\nunsigned char *initiatorname)\r\n{\r\nstruct se_node_acl *acl;\r\nmutex_lock(&tpg->acl_node_mutex);\r\nacl = __core_tpg_get_initiator_node_acl(tpg, initiatorname);\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nreturn acl;\r\n}\r\nvoid core_allocate_nexus_loss_ua(\r\nstruct se_node_acl *nacl)\r\n{\r\nstruct se_dev_entry *deve;\r\nif (!nacl)\r\nreturn;\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link)\r\ncore_scsi3_ua_allocate(deve, 0x29,\r\nASCQ_29H_NEXUS_LOSS_OCCURRED);\r\nrcu_read_unlock();\r\n}\r\nvoid core_tpg_add_node_to_devs(\r\nstruct se_node_acl *acl,\r\nstruct se_portal_group *tpg,\r\nstruct se_lun *lun_orig)\r\n{\r\nu32 lun_access = 0;\r\nstruct se_lun *lun;\r\nstruct se_device *dev;\r\nmutex_lock(&tpg->tpg_lun_mutex);\r\nhlist_for_each_entry_rcu(lun, &tpg->tpg_lun_hlist, link) {\r\nif (lun_orig && lun != lun_orig)\r\ncontinue;\r\ndev = rcu_dereference_check(lun->lun_se_dev,\r\nlockdep_is_held(&tpg->tpg_lun_mutex));\r\nif (!tpg->se_tpg_tfo->tpg_check_demo_mode_write_protect(tpg)) {\r\nlun_access = TRANSPORT_LUNFLAGS_READ_WRITE;\r\n} else {\r\nif (dev->transport->get_device_type(dev) == TYPE_DISK)\r\nlun_access = TRANSPORT_LUNFLAGS_READ_ONLY;\r\nelse\r\nlun_access = TRANSPORT_LUNFLAGS_READ_WRITE;\r\n}\r\npr_debug("TARGET_CORE[%s]->TPG[%u]_LUN[%llu] - Adding %s"\r\n" access for LUN in Demo Mode\n",\r\ntpg->se_tpg_tfo->get_fabric_name(),\r\ntpg->se_tpg_tfo->tpg_get_tag(tpg), lun->unpacked_lun,\r\n(lun_access == TRANSPORT_LUNFLAGS_READ_WRITE) ?\r\n"READ-WRITE" : "READ-ONLY");\r\ncore_enable_device_list_for_node(lun, NULL, lun->unpacked_lun,\r\nlun_access, acl, tpg);\r\ncore_scsi3_check_aptpl_registration(dev, tpg, lun, acl,\r\nlun->unpacked_lun);\r\n}\r\nmutex_unlock(&tpg->tpg_lun_mutex);\r\n}\r\nstatic int core_set_queue_depth_for_node(\r\nstruct se_portal_group *tpg,\r\nstruct se_node_acl *acl)\r\n{\r\nif (!acl->queue_depth) {\r\npr_err("Queue depth for %s Initiator Node: %s is 0,"\r\n"defaulting to 1.\n", tpg->se_tpg_tfo->get_fabric_name(),\r\nacl->initiatorname);\r\nacl->queue_depth = 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct se_node_acl *target_alloc_node_acl(struct se_portal_group *tpg,\r\nconst unsigned char *initiatorname)\r\n{\r\nstruct se_node_acl *acl;\r\nacl = kzalloc(max(sizeof(*acl), tpg->se_tpg_tfo->node_acl_size),\r\nGFP_KERNEL);\r\nif (!acl)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&acl->acl_list);\r\nINIT_LIST_HEAD(&acl->acl_sess_list);\r\nINIT_HLIST_HEAD(&acl->lun_entry_hlist);\r\nkref_init(&acl->acl_kref);\r\ninit_completion(&acl->acl_free_comp);\r\nspin_lock_init(&acl->nacl_sess_lock);\r\nmutex_init(&acl->lun_entry_mutex);\r\natomic_set(&acl->acl_pr_ref_count, 0);\r\nif (tpg->se_tpg_tfo->tpg_get_default_depth)\r\nacl->queue_depth = tpg->se_tpg_tfo->tpg_get_default_depth(tpg);\r\nelse\r\nacl->queue_depth = 1;\r\nsnprintf(acl->initiatorname, TRANSPORT_IQN_LEN, "%s", initiatorname);\r\nacl->se_tpg = tpg;\r\nacl->acl_index = scsi_get_new_index(SCSI_AUTH_INTR_INDEX);\r\ntpg->se_tpg_tfo->set_default_node_attributes(acl);\r\nif (core_set_queue_depth_for_node(tpg, acl) < 0)\r\ngoto out_free_acl;\r\nreturn acl;\r\nout_free_acl:\r\nkfree(acl);\r\nreturn NULL;\r\n}\r\nstatic void target_add_node_acl(struct se_node_acl *acl)\r\n{\r\nstruct se_portal_group *tpg = acl->se_tpg;\r\nmutex_lock(&tpg->acl_node_mutex);\r\nlist_add_tail(&acl->acl_list, &tpg->acl_node_list);\r\ntpg->num_node_acls++;\r\nmutex_unlock(&tpg->acl_node_mutex);\r\npr_debug("%s_TPG[%hu] - Added %s ACL with TCQ Depth: %d for %s"\r\n" Initiator Node: %s\n",\r\ntpg->se_tpg_tfo->get_fabric_name(),\r\ntpg->se_tpg_tfo->tpg_get_tag(tpg),\r\nacl->dynamic_node_acl ? "DYNAMIC" : "",\r\nacl->queue_depth,\r\ntpg->se_tpg_tfo->get_fabric_name(),\r\nacl->initiatorname);\r\n}\r\nstruct se_node_acl *core_tpg_check_initiator_node_acl(\r\nstruct se_portal_group *tpg,\r\nunsigned char *initiatorname)\r\n{\r\nstruct se_node_acl *acl;\r\nacl = core_tpg_get_initiator_node_acl(tpg, initiatorname);\r\nif (acl)\r\nreturn acl;\r\nif (!tpg->se_tpg_tfo->tpg_check_demo_mode(tpg))\r\nreturn NULL;\r\nacl = target_alloc_node_acl(tpg, initiatorname);\r\nif (!acl)\r\nreturn NULL;\r\nacl->dynamic_node_acl = 1;\r\nif ((tpg->se_tpg_tfo->tpg_check_demo_mode_login_only == NULL) ||\r\n(tpg->se_tpg_tfo->tpg_check_demo_mode_login_only(tpg) != 1))\r\ncore_tpg_add_node_to_devs(acl, tpg, NULL);\r\ntarget_add_node_acl(acl);\r\nreturn acl;\r\n}\r\nvoid core_tpg_wait_for_nacl_pr_ref(struct se_node_acl *nacl)\r\n{\r\nwhile (atomic_read(&nacl->acl_pr_ref_count) != 0)\r\ncpu_relax();\r\n}\r\nstruct se_node_acl *core_tpg_add_initiator_node_acl(\r\nstruct se_portal_group *tpg,\r\nconst char *initiatorname)\r\n{\r\nstruct se_node_acl *acl;\r\nmutex_lock(&tpg->acl_node_mutex);\r\nacl = __core_tpg_get_initiator_node_acl(tpg, initiatorname);\r\nif (acl) {\r\nif (acl->dynamic_node_acl) {\r\nacl->dynamic_node_acl = 0;\r\npr_debug("%s_TPG[%u] - Replacing dynamic ACL"\r\n" for %s\n", tpg->se_tpg_tfo->get_fabric_name(),\r\ntpg->se_tpg_tfo->tpg_get_tag(tpg), initiatorname);\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nreturn acl;\r\n}\r\npr_err("ACL entry for %s Initiator"\r\n" Node %s already exists for TPG %u, ignoring"\r\n" request.\n", tpg->se_tpg_tfo->get_fabric_name(),\r\ninitiatorname, tpg->se_tpg_tfo->tpg_get_tag(tpg));\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nreturn ERR_PTR(-EEXIST);\r\n}\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nacl = target_alloc_node_acl(tpg, initiatorname);\r\nif (!acl)\r\nreturn ERR_PTR(-ENOMEM);\r\ntarget_add_node_acl(acl);\r\nreturn acl;\r\n}\r\nvoid core_tpg_del_initiator_node_acl(struct se_node_acl *acl)\r\n{\r\nstruct se_portal_group *tpg = acl->se_tpg;\r\nLIST_HEAD(sess_list);\r\nstruct se_session *sess, *sess_tmp;\r\nunsigned long flags;\r\nint rc;\r\nmutex_lock(&tpg->acl_node_mutex);\r\nif (acl->dynamic_node_acl) {\r\nacl->dynamic_node_acl = 0;\r\n}\r\nlist_del(&acl->acl_list);\r\ntpg->num_node_acls--;\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nspin_lock_irqsave(&acl->nacl_sess_lock, flags);\r\nacl->acl_stop = 1;\r\nlist_for_each_entry_safe(sess, sess_tmp, &acl->acl_sess_list,\r\nsess_acl_list) {\r\nif (sess->sess_tearing_down != 0)\r\ncontinue;\r\ntarget_get_session(sess);\r\nlist_move(&sess->sess_acl_list, &sess_list);\r\n}\r\nspin_unlock_irqrestore(&acl->nacl_sess_lock, flags);\r\nlist_for_each_entry_safe(sess, sess_tmp, &sess_list, sess_acl_list) {\r\nlist_del(&sess->sess_acl_list);\r\nrc = tpg->se_tpg_tfo->shutdown_session(sess);\r\ntarget_put_session(sess);\r\nif (!rc)\r\ncontinue;\r\ntarget_put_session(sess);\r\n}\r\ntarget_put_nacl(acl);\r\nwait_for_completion(&acl->acl_free_comp);\r\ncore_tpg_wait_for_nacl_pr_ref(acl);\r\ncore_free_device_list_for_node(acl, tpg);\r\npr_debug("%s_TPG[%hu] - Deleted ACL with TCQ Depth: %d for %s"\r\n" Initiator Node: %s\n", tpg->se_tpg_tfo->get_fabric_name(),\r\ntpg->se_tpg_tfo->tpg_get_tag(tpg), acl->queue_depth,\r\ntpg->se_tpg_tfo->get_fabric_name(), acl->initiatorname);\r\nkfree(acl);\r\n}\r\nint core_tpg_set_initiator_node_queue_depth(\r\nstruct se_portal_group *tpg,\r\nunsigned char *initiatorname,\r\nu32 queue_depth,\r\nint force)\r\n{\r\nstruct se_session *sess, *init_sess = NULL;\r\nstruct se_node_acl *acl;\r\nunsigned long flags;\r\nint dynamic_acl = 0;\r\nmutex_lock(&tpg->acl_node_mutex);\r\nacl = __core_tpg_get_initiator_node_acl(tpg, initiatorname);\r\nif (!acl) {\r\npr_err("Access Control List entry for %s Initiator"\r\n" Node %s does not exists for TPG %hu, ignoring"\r\n" request.\n", tpg->se_tpg_tfo->get_fabric_name(),\r\ninitiatorname, tpg->se_tpg_tfo->tpg_get_tag(tpg));\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nreturn -ENODEV;\r\n}\r\nif (acl->dynamic_node_acl) {\r\nacl->dynamic_node_acl = 0;\r\ndynamic_acl = 1;\r\n}\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nspin_lock_irqsave(&tpg->session_lock, flags);\r\nlist_for_each_entry(sess, &tpg->tpg_sess_list, sess_list) {\r\nif (sess->se_node_acl != acl)\r\ncontinue;\r\nif (!force) {\r\npr_err("Unable to change queue depth for %s"\r\n" Initiator Node: %s while session is"\r\n" operational. To forcefully change the queue"\r\n" depth and force session reinstatement"\r\n" use the \"force=1\" parameter.\n",\r\ntpg->se_tpg_tfo->get_fabric_name(), initiatorname);\r\nspin_unlock_irqrestore(&tpg->session_lock, flags);\r\nmutex_lock(&tpg->acl_node_mutex);\r\nif (dynamic_acl)\r\nacl->dynamic_node_acl = 1;\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nreturn -EEXIST;\r\n}\r\nif (!tpg->se_tpg_tfo->shutdown_session(sess))\r\ncontinue;\r\ninit_sess = sess;\r\nbreak;\r\n}\r\nacl->queue_depth = queue_depth;\r\nif (core_set_queue_depth_for_node(tpg, acl) < 0) {\r\nspin_unlock_irqrestore(&tpg->session_lock, flags);\r\nif (init_sess)\r\ntpg->se_tpg_tfo->close_session(init_sess);\r\nmutex_lock(&tpg->acl_node_mutex);\r\nif (dynamic_acl)\r\nacl->dynamic_node_acl = 1;\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nreturn -EINVAL;\r\n}\r\nspin_unlock_irqrestore(&tpg->session_lock, flags);\r\nif (init_sess)\r\ntpg->se_tpg_tfo->close_session(init_sess);\r\npr_debug("Successfully changed queue depth to: %d for Initiator"\r\n" Node: %s on %s Target Portal Group: %u\n", queue_depth,\r\ninitiatorname, tpg->se_tpg_tfo->get_fabric_name(),\r\ntpg->se_tpg_tfo->tpg_get_tag(tpg));\r\nmutex_lock(&tpg->acl_node_mutex);\r\nif (dynamic_acl)\r\nacl->dynamic_node_acl = 1;\r\nmutex_unlock(&tpg->acl_node_mutex);\r\nreturn 0;\r\n}\r\nint core_tpg_set_initiator_node_tag(\r\nstruct se_portal_group *tpg,\r\nstruct se_node_acl *acl,\r\nconst char *new_tag)\r\n{\r\nif (strlen(new_tag) >= MAX_ACL_TAG_SIZE)\r\nreturn -EINVAL;\r\nif (!strncmp("NULL", new_tag, 4)) {\r\nacl->acl_tag[0] = '\0';\r\nreturn 0;\r\n}\r\nreturn snprintf(acl->acl_tag, MAX_ACL_TAG_SIZE, "%s", new_tag);\r\n}\r\nstatic void core_tpg_lun_ref_release(struct percpu_ref *ref)\r\n{\r\nstruct se_lun *lun = container_of(ref, struct se_lun, lun_ref);\r\ncomplete(&lun->lun_ref_comp);\r\n}\r\nint core_tpg_register(\r\nstruct se_wwn *se_wwn,\r\nstruct se_portal_group *se_tpg,\r\nint proto_id)\r\n{\r\nint ret;\r\nif (!se_tpg)\r\nreturn -EINVAL;\r\nif (se_wwn)\r\nse_tpg->se_tpg_tfo = se_wwn->wwn_tf->tf_ops;\r\nif (!se_tpg->se_tpg_tfo) {\r\npr_err("Unable to locate se_tpg->se_tpg_tfo pointer\n");\r\nreturn -EINVAL;\r\n}\r\nINIT_HLIST_HEAD(&se_tpg->tpg_lun_hlist);\r\nse_tpg->proto_id = proto_id;\r\nse_tpg->se_tpg_wwn = se_wwn;\r\natomic_set(&se_tpg->tpg_pr_ref_count, 0);\r\nINIT_LIST_HEAD(&se_tpg->acl_node_list);\r\nINIT_LIST_HEAD(&se_tpg->se_tpg_node);\r\nINIT_LIST_HEAD(&se_tpg->tpg_sess_list);\r\nspin_lock_init(&se_tpg->session_lock);\r\nmutex_init(&se_tpg->tpg_lun_mutex);\r\nmutex_init(&se_tpg->acl_node_mutex);\r\nif (se_tpg->proto_id >= 0) {\r\nse_tpg->tpg_virt_lun0 = core_tpg_alloc_lun(se_tpg, 0);\r\nif (IS_ERR(se_tpg->tpg_virt_lun0))\r\nreturn PTR_ERR(se_tpg->tpg_virt_lun0);\r\nret = core_tpg_add_lun(se_tpg, se_tpg->tpg_virt_lun0,\r\nTRANSPORT_LUNFLAGS_READ_ONLY, g_lun0_dev);\r\nif (ret < 0) {\r\nkfree(se_tpg->tpg_virt_lun0);\r\nreturn ret;\r\n}\r\n}\r\nspin_lock_bh(&tpg_lock);\r\nlist_add_tail(&se_tpg->se_tpg_node, &tpg_list);\r\nspin_unlock_bh(&tpg_lock);\r\npr_debug("TARGET_CORE[%s]: Allocated portal_group for endpoint: %s, "\r\n"Proto: %d, Portal Tag: %u\n", se_tpg->se_tpg_tfo->get_fabric_name(),\r\nse_tpg->se_tpg_tfo->tpg_get_wwn(se_tpg) ?\r\nse_tpg->se_tpg_tfo->tpg_get_wwn(se_tpg) : NULL,\r\nse_tpg->proto_id, se_tpg->se_tpg_tfo->tpg_get_tag(se_tpg));\r\nreturn 0;\r\n}\r\nint core_tpg_deregister(struct se_portal_group *se_tpg)\r\n{\r\nconst struct target_core_fabric_ops *tfo = se_tpg->se_tpg_tfo;\r\nstruct se_node_acl *nacl, *nacl_tmp;\r\nLIST_HEAD(node_list);\r\npr_debug("TARGET_CORE[%s]: Deallocating portal_group for endpoint: %s, "\r\n"Proto: %d, Portal Tag: %u\n", tfo->get_fabric_name(),\r\ntfo->tpg_get_wwn(se_tpg) ? tfo->tpg_get_wwn(se_tpg) : NULL,\r\nse_tpg->proto_id, tfo->tpg_get_tag(se_tpg));\r\nspin_lock_bh(&tpg_lock);\r\nlist_del(&se_tpg->se_tpg_node);\r\nspin_unlock_bh(&tpg_lock);\r\nwhile (atomic_read(&se_tpg->tpg_pr_ref_count) != 0)\r\ncpu_relax();\r\nmutex_lock(&se_tpg->acl_node_mutex);\r\nlist_splice_init(&se_tpg->acl_node_list, &node_list);\r\nmutex_unlock(&se_tpg->acl_node_mutex);\r\nlist_for_each_entry_safe(nacl, nacl_tmp, &node_list, acl_list) {\r\nlist_del(&nacl->acl_list);\r\nse_tpg->num_node_acls--;\r\ncore_tpg_wait_for_nacl_pr_ref(nacl);\r\ncore_free_device_list_for_node(nacl, se_tpg);\r\nkfree(nacl);\r\n}\r\nif (se_tpg->proto_id >= 0) {\r\ncore_tpg_remove_lun(se_tpg, se_tpg->tpg_virt_lun0);\r\nkfree_rcu(se_tpg->tpg_virt_lun0, rcu_head);\r\n}\r\nreturn 0;\r\n}\r\nstruct se_lun *core_tpg_alloc_lun(\r\nstruct se_portal_group *tpg,\r\nu64 unpacked_lun)\r\n{\r\nstruct se_lun *lun;\r\nlun = kzalloc(sizeof(*lun), GFP_KERNEL);\r\nif (!lun) {\r\npr_err("Unable to allocate se_lun memory\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nlun->unpacked_lun = unpacked_lun;\r\nlun->lun_link_magic = SE_LUN_LINK_MAGIC;\r\natomic_set(&lun->lun_acl_count, 0);\r\ninit_completion(&lun->lun_ref_comp);\r\nINIT_LIST_HEAD(&lun->lun_deve_list);\r\nINIT_LIST_HEAD(&lun->lun_dev_link);\r\natomic_set(&lun->lun_tg_pt_secondary_offline, 0);\r\nspin_lock_init(&lun->lun_deve_lock);\r\nmutex_init(&lun->lun_tg_pt_md_mutex);\r\nINIT_LIST_HEAD(&lun->lun_tg_pt_gp_link);\r\nspin_lock_init(&lun->lun_tg_pt_gp_lock);\r\nlun->lun_tpg = tpg;\r\nreturn lun;\r\n}\r\nint core_tpg_add_lun(\r\nstruct se_portal_group *tpg,\r\nstruct se_lun *lun,\r\nu32 lun_access,\r\nstruct se_device *dev)\r\n{\r\nint ret;\r\nret = percpu_ref_init(&lun->lun_ref, core_tpg_lun_ref_release, 0,\r\nGFP_KERNEL);\r\nif (ret < 0)\r\ngoto out;\r\nret = core_alloc_rtpi(lun, dev);\r\nif (ret)\r\ngoto out_kill_ref;\r\nif (!(dev->transport->transport_flags & TRANSPORT_FLAG_PASSTHROUGH) &&\r\n!(dev->se_hba->hba_flags & HBA_FLAGS_INTERNAL_USE))\r\ntarget_attach_tg_pt_gp(lun, dev->t10_alua.default_tg_pt_gp);\r\nmutex_lock(&tpg->tpg_lun_mutex);\r\nspin_lock(&dev->se_port_lock);\r\nlun->lun_index = dev->dev_index;\r\nrcu_assign_pointer(lun->lun_se_dev, dev);\r\ndev->export_count++;\r\nlist_add_tail(&lun->lun_dev_link, &dev->dev_sep_list);\r\nspin_unlock(&dev->se_port_lock);\r\nif (dev->dev_flags & DF_READ_ONLY)\r\nlun->lun_access = TRANSPORT_LUNFLAGS_READ_ONLY;\r\nelse\r\nlun->lun_access = lun_access;\r\nif (!(dev->se_hba->hba_flags & HBA_FLAGS_INTERNAL_USE))\r\nhlist_add_head_rcu(&lun->link, &tpg->tpg_lun_hlist);\r\nmutex_unlock(&tpg->tpg_lun_mutex);\r\nreturn 0;\r\nout_kill_ref:\r\npercpu_ref_exit(&lun->lun_ref);\r\nout:\r\nreturn ret;\r\n}\r\nvoid core_tpg_remove_lun(\r\nstruct se_portal_group *tpg,\r\nstruct se_lun *lun)\r\n{\r\nstruct se_device *dev = rcu_dereference_raw(lun->lun_se_dev);\r\ncore_clear_lun_from_tpg(lun, tpg);\r\ntransport_clear_lun_ref(lun);\r\nmutex_lock(&tpg->tpg_lun_mutex);\r\nif (lun->lun_se_dev) {\r\ntarget_detach_tg_pt_gp(lun);\r\nspin_lock(&dev->se_port_lock);\r\nlist_del(&lun->lun_dev_link);\r\ndev->export_count--;\r\nrcu_assign_pointer(lun->lun_se_dev, NULL);\r\nspin_unlock(&dev->se_port_lock);\r\n}\r\nif (!(dev->se_hba->hba_flags & HBA_FLAGS_INTERNAL_USE))\r\nhlist_del_rcu(&lun->link);\r\nmutex_unlock(&tpg->tpg_lun_mutex);\r\npercpu_ref_exit(&lun->lun_ref);\r\n}
