unsigned long total_swapcache_pages(void)\r\n{\r\nint i;\r\nunsigned long ret = 0;\r\nfor (i = 0; i < MAX_SWAPFILES; i++)\r\nret += swapper_spaces[i].nrpages;\r\nreturn ret;\r\n}\r\nvoid show_swap_cache_info(void)\r\n{\r\nprintk("%lu pages in swap cache\n", total_swapcache_pages());\r\nprintk("Swap cache stats: add %lu, delete %lu, find %lu/%lu\n",\r\nswap_cache_info.add_total, swap_cache_info.del_total,\r\nswap_cache_info.find_success, swap_cache_info.find_total);\r\nprintk("Free swap = %ldkB\n",\r\nget_nr_swap_pages() << (PAGE_SHIFT - 10));\r\nprintk("Total swap = %lukB\n", total_swap_pages << (PAGE_SHIFT - 10));\r\n}\r\nint __add_to_swap_cache(struct page *page, swp_entry_t entry)\r\n{\r\nint error;\r\nstruct address_space *address_space;\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nVM_BUG_ON_PAGE(PageSwapCache(page), page);\r\nVM_BUG_ON_PAGE(!PageSwapBacked(page), page);\r\npage_cache_get(page);\r\nSetPageSwapCache(page);\r\nset_page_private(page, entry.val);\r\naddress_space = swap_address_space(entry);\r\nspin_lock_irq(&address_space->tree_lock);\r\nerror = radix_tree_insert(&address_space->page_tree,\r\nentry.val, page);\r\nif (likely(!error)) {\r\naddress_space->nrpages++;\r\n__inc_zone_page_state(page, NR_FILE_PAGES);\r\nINC_CACHE_INFO(add_total);\r\n}\r\nspin_unlock_irq(&address_space->tree_lock);\r\nif (unlikely(error)) {\r\nVM_BUG_ON(error == -EEXIST);\r\nset_page_private(page, 0UL);\r\nClearPageSwapCache(page);\r\npage_cache_release(page);\r\n}\r\nreturn error;\r\n}\r\nint add_to_swap_cache(struct page *page, swp_entry_t entry, gfp_t gfp_mask)\r\n{\r\nint error;\r\nerror = radix_tree_maybe_preload(gfp_mask);\r\nif (!error) {\r\nerror = __add_to_swap_cache(page, entry);\r\nradix_tree_preload_end();\r\n}\r\nreturn error;\r\n}\r\nvoid __delete_from_swap_cache(struct page *page)\r\n{\r\nswp_entry_t entry;\r\nstruct address_space *address_space;\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nVM_BUG_ON_PAGE(!PageSwapCache(page), page);\r\nVM_BUG_ON_PAGE(PageWriteback(page), page);\r\nentry.val = page_private(page);\r\naddress_space = swap_address_space(entry);\r\nradix_tree_delete(&address_space->page_tree, page_private(page));\r\nset_page_private(page, 0);\r\nClearPageSwapCache(page);\r\naddress_space->nrpages--;\r\n__dec_zone_page_state(page, NR_FILE_PAGES);\r\nINC_CACHE_INFO(del_total);\r\n}\r\nint add_to_swap(struct page *page, struct list_head *list)\r\n{\r\nswp_entry_t entry;\r\nint err;\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nVM_BUG_ON_PAGE(!PageUptodate(page), page);\r\nentry = get_swap_page();\r\nif (!entry.val)\r\nreturn 0;\r\nif (unlikely(PageTransHuge(page)))\r\nif (unlikely(split_huge_page_to_list(page, list))) {\r\nswapcache_free(entry);\r\nreturn 0;\r\n}\r\nerr = add_to_swap_cache(page, entry,\r\n__GFP_HIGH|__GFP_NOMEMALLOC|__GFP_NOWARN);\r\nif (!err) {\r\nSetPageDirty(page);\r\nreturn 1;\r\n} else {\r\nswapcache_free(entry);\r\nreturn 0;\r\n}\r\n}\r\nvoid delete_from_swap_cache(struct page *page)\r\n{\r\nswp_entry_t entry;\r\nstruct address_space *address_space;\r\nentry.val = page_private(page);\r\naddress_space = swap_address_space(entry);\r\nspin_lock_irq(&address_space->tree_lock);\r\n__delete_from_swap_cache(page);\r\nspin_unlock_irq(&address_space->tree_lock);\r\nswapcache_free(entry);\r\npage_cache_release(page);\r\n}\r\nstatic inline void free_swap_cache(struct page *page)\r\n{\r\nif (PageSwapCache(page) && !page_mapped(page) && trylock_page(page)) {\r\ntry_to_free_swap(page);\r\nunlock_page(page);\r\n}\r\n}\r\nvoid free_page_and_swap_cache(struct page *page)\r\n{\r\nfree_swap_cache(page);\r\npage_cache_release(page);\r\n}\r\nvoid free_pages_and_swap_cache(struct page **pages, int nr)\r\n{\r\nstruct page **pagep = pages;\r\nint i;\r\nlru_add_drain();\r\nfor (i = 0; i < nr; i++)\r\nfree_swap_cache(pagep[i]);\r\nrelease_pages(pagep, nr, false);\r\n}\r\nstruct page * lookup_swap_cache(swp_entry_t entry)\r\n{\r\nstruct page *page;\r\npage = find_get_page(swap_address_space(entry), entry.val);\r\nif (page) {\r\nINC_CACHE_INFO(find_success);\r\nif (TestClearPageReadahead(page))\r\natomic_inc(&swapin_readahead_hits);\r\n}\r\nINC_CACHE_INFO(find_total);\r\nreturn page;\r\n}\r\nstruct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\r\nstruct vm_area_struct *vma, unsigned long addr,\r\nbool *new_page_allocated)\r\n{\r\nstruct page *found_page, *new_page = NULL;\r\nstruct address_space *swapper_space = swap_address_space(entry);\r\nint err;\r\n*new_page_allocated = false;\r\ndo {\r\nfound_page = find_get_page(swapper_space, entry.val);\r\nif (found_page)\r\nbreak;\r\nif (!new_page) {\r\nnew_page = alloc_page_vma(gfp_mask, vma, addr);\r\nif (!new_page)\r\nbreak;\r\n}\r\nerr = radix_tree_maybe_preload(gfp_mask & GFP_KERNEL);\r\nif (err)\r\nbreak;\r\nerr = swapcache_prepare(entry);\r\nif (err == -EEXIST) {\r\nradix_tree_preload_end();\r\ncond_resched();\r\ncontinue;\r\n}\r\nif (err) {\r\nradix_tree_preload_end();\r\nbreak;\r\n}\r\n__set_page_locked(new_page);\r\nSetPageSwapBacked(new_page);\r\nerr = __add_to_swap_cache(new_page, entry);\r\nif (likely(!err)) {\r\nradix_tree_preload_end();\r\nlru_cache_add_anon(new_page);\r\n*new_page_allocated = true;\r\nreturn new_page;\r\n}\r\nradix_tree_preload_end();\r\nClearPageSwapBacked(new_page);\r\n__clear_page_locked(new_page);\r\nswapcache_free(entry);\r\n} while (err != -ENOMEM);\r\nif (new_page)\r\npage_cache_release(new_page);\r\nreturn found_page;\r\n}\r\nstruct page *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\r\nstruct vm_area_struct *vma, unsigned long addr)\r\n{\r\nbool page_was_allocated;\r\nstruct page *retpage = __read_swap_cache_async(entry, gfp_mask,\r\nvma, addr, &page_was_allocated);\r\nif (page_was_allocated)\r\nswap_readpage(retpage);\r\nreturn retpage;\r\n}\r\nstatic unsigned long swapin_nr_pages(unsigned long offset)\r\n{\r\nstatic unsigned long prev_offset;\r\nunsigned int pages, max_pages, last_ra;\r\nstatic atomic_t last_readahead_pages;\r\nmax_pages = 1 << READ_ONCE(page_cluster);\r\nif (max_pages <= 1)\r\nreturn 1;\r\npages = atomic_xchg(&swapin_readahead_hits, 0) + 2;\r\nif (pages == 2) {\r\nif (offset != prev_offset + 1 && offset != prev_offset - 1)\r\npages = 1;\r\nprev_offset = offset;\r\n} else {\r\nunsigned int roundup = 4;\r\nwhile (roundup < pages)\r\nroundup <<= 1;\r\npages = roundup;\r\n}\r\nif (pages > max_pages)\r\npages = max_pages;\r\nlast_ra = atomic_read(&last_readahead_pages) / 2;\r\nif (pages < last_ra)\r\npages = last_ra;\r\natomic_set(&last_readahead_pages, pages);\r\nreturn pages;\r\n}\r\nstruct page *swapin_readahead(swp_entry_t entry, gfp_t gfp_mask,\r\nstruct vm_area_struct *vma, unsigned long addr)\r\n{\r\nstruct page *page;\r\nunsigned long entry_offset = swp_offset(entry);\r\nunsigned long offset = entry_offset;\r\nunsigned long start_offset, end_offset;\r\nunsigned long mask;\r\nstruct blk_plug plug;\r\nmask = swapin_nr_pages(offset) - 1;\r\nif (!mask)\r\ngoto skip;\r\nstart_offset = offset & ~mask;\r\nend_offset = offset | mask;\r\nif (!start_offset)\r\nstart_offset++;\r\nblk_start_plug(&plug);\r\nfor (offset = start_offset; offset <= end_offset ; offset++) {\r\npage = read_swap_cache_async(swp_entry(swp_type(entry), offset),\r\ngfp_mask, vma, addr);\r\nif (!page)\r\ncontinue;\r\nif (offset != entry_offset)\r\nSetPageReadahead(page);\r\npage_cache_release(page);\r\n}\r\nblk_finish_plug(&plug);\r\nlru_add_drain();\r\nskip:\r\nreturn read_swap_cache_async(entry, gfp_mask, vma, addr);\r\n}
