static int amdgpu_bo_list_create(struct amdgpu_fpriv *fpriv,\r\nstruct amdgpu_bo_list **result,\r\nint *id)\r\n{\r\nint r;\r\n*result = kzalloc(sizeof(struct amdgpu_bo_list), GFP_KERNEL);\r\nif (!*result)\r\nreturn -ENOMEM;\r\nmutex_lock(&fpriv->bo_list_lock);\r\nr = idr_alloc(&fpriv->bo_list_handles, *result,\r\n1, 0, GFP_KERNEL);\r\nif (r < 0) {\r\nmutex_unlock(&fpriv->bo_list_lock);\r\nkfree(*result);\r\nreturn r;\r\n}\r\n*id = r;\r\nmutex_init(&(*result)->lock);\r\n(*result)->num_entries = 0;\r\n(*result)->array = NULL;\r\nmutex_lock(&(*result)->lock);\r\nmutex_unlock(&fpriv->bo_list_lock);\r\nreturn 0;\r\n}\r\nstatic void amdgpu_bo_list_destroy(struct amdgpu_fpriv *fpriv, int id)\r\n{\r\nstruct amdgpu_bo_list *list;\r\nmutex_lock(&fpriv->bo_list_lock);\r\nlist = idr_find(&fpriv->bo_list_handles, id);\r\nif (list) {\r\nmutex_lock(&list->lock);\r\nidr_remove(&fpriv->bo_list_handles, id);\r\nmutex_unlock(&list->lock);\r\namdgpu_bo_list_free(list);\r\n}\r\nmutex_unlock(&fpriv->bo_list_lock);\r\n}\r\nstatic int amdgpu_bo_list_set(struct amdgpu_device *adev,\r\nstruct drm_file *filp,\r\nstruct amdgpu_bo_list *list,\r\nstruct drm_amdgpu_bo_list_entry *info,\r\nunsigned num_entries)\r\n{\r\nstruct amdgpu_bo_list_entry *array;\r\nstruct amdgpu_bo *gds_obj = adev->gds.gds_gfx_bo;\r\nstruct amdgpu_bo *gws_obj = adev->gds.gws_gfx_bo;\r\nstruct amdgpu_bo *oa_obj = adev->gds.oa_gfx_bo;\r\nbool has_userptr = false;\r\nunsigned i;\r\narray = drm_malloc_ab(num_entries, sizeof(struct amdgpu_bo_list_entry));\r\nif (!array)\r\nreturn -ENOMEM;\r\nmemset(array, 0, num_entries * sizeof(struct amdgpu_bo_list_entry));\r\nfor (i = 0; i < num_entries; ++i) {\r\nstruct amdgpu_bo_list_entry *entry = &array[i];\r\nstruct drm_gem_object *gobj;\r\ngobj = drm_gem_object_lookup(adev->ddev, filp, info[i].bo_handle);\r\nif (!gobj)\r\ngoto error_free;\r\nentry->robj = amdgpu_bo_ref(gem_to_amdgpu_bo(gobj));\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nentry->priority = info[i].bo_priority;\r\nentry->prefered_domains = entry->robj->initial_domain;\r\nentry->allowed_domains = entry->prefered_domains;\r\nif (entry->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\r\nentry->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\r\nif (amdgpu_ttm_tt_has_userptr(entry->robj->tbo.ttm)) {\r\nhas_userptr = true;\r\nentry->prefered_domains = AMDGPU_GEM_DOMAIN_GTT;\r\nentry->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\r\n}\r\nentry->tv.bo = &entry->robj->tbo;\r\nentry->tv.shared = true;\r\nif (entry->prefered_domains == AMDGPU_GEM_DOMAIN_GDS)\r\ngds_obj = entry->robj;\r\nif (entry->prefered_domains == AMDGPU_GEM_DOMAIN_GWS)\r\ngws_obj = entry->robj;\r\nif (entry->prefered_domains == AMDGPU_GEM_DOMAIN_OA)\r\noa_obj = entry->robj;\r\ntrace_amdgpu_bo_list_set(list, entry->robj);\r\n}\r\nfor (i = 0; i < list->num_entries; ++i)\r\namdgpu_bo_unref(&list->array[i].robj);\r\ndrm_free_large(list->array);\r\nlist->gds_obj = gds_obj;\r\nlist->gws_obj = gws_obj;\r\nlist->oa_obj = oa_obj;\r\nlist->has_userptr = has_userptr;\r\nlist->array = array;\r\nlist->num_entries = num_entries;\r\nreturn 0;\r\nerror_free:\r\ndrm_free_large(array);\r\nreturn -ENOENT;\r\n}\r\nstruct amdgpu_bo_list *\r\namdgpu_bo_list_get(struct amdgpu_fpriv *fpriv, int id)\r\n{\r\nstruct amdgpu_bo_list *result;\r\nmutex_lock(&fpriv->bo_list_lock);\r\nresult = idr_find(&fpriv->bo_list_handles, id);\r\nif (result)\r\nmutex_lock(&result->lock);\r\nmutex_unlock(&fpriv->bo_list_lock);\r\nreturn result;\r\n}\r\nvoid amdgpu_bo_list_put(struct amdgpu_bo_list *list)\r\n{\r\nmutex_unlock(&list->lock);\r\n}\r\nvoid amdgpu_bo_list_free(struct amdgpu_bo_list *list)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < list->num_entries; ++i)\r\namdgpu_bo_unref(&list->array[i].robj);\r\nmutex_destroy(&list->lock);\r\ndrm_free_large(list->array);\r\nkfree(list);\r\n}\r\nint amdgpu_bo_list_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nconst uint32_t info_size = sizeof(struct drm_amdgpu_bo_list_entry);\r\nstruct amdgpu_device *adev = dev->dev_private;\r\nstruct amdgpu_fpriv *fpriv = filp->driver_priv;\r\nunion drm_amdgpu_bo_list *args = data;\r\nuint32_t handle = args->in.list_handle;\r\nconst void __user *uptr = (const void*)(long)args->in.bo_info_ptr;\r\nstruct drm_amdgpu_bo_list_entry *info;\r\nstruct amdgpu_bo_list *list;\r\nint r;\r\ninfo = drm_malloc_ab(args->in.bo_number,\r\nsizeof(struct drm_amdgpu_bo_list_entry));\r\nif (!info)\r\nreturn -ENOMEM;\r\nr = -EFAULT;\r\nif (likely(info_size == args->in.bo_info_size)) {\r\nunsigned long bytes = args->in.bo_number *\r\nargs->in.bo_info_size;\r\nif (copy_from_user(info, uptr, bytes))\r\ngoto error_free;\r\n} else {\r\nunsigned long bytes = min(args->in.bo_info_size, info_size);\r\nunsigned i;\r\nmemset(info, 0, args->in.bo_number * info_size);\r\nfor (i = 0; i < args->in.bo_number; ++i) {\r\nif (copy_from_user(&info[i], uptr, bytes))\r\ngoto error_free;\r\nuptr += args->in.bo_info_size;\r\n}\r\n}\r\nswitch (args->in.operation) {\r\ncase AMDGPU_BO_LIST_OP_CREATE:\r\nr = amdgpu_bo_list_create(fpriv, &list, &handle);\r\nif (r)\r\ngoto error_free;\r\nr = amdgpu_bo_list_set(adev, filp, list, info,\r\nargs->in.bo_number);\r\namdgpu_bo_list_put(list);\r\nif (r)\r\ngoto error_free;\r\nbreak;\r\ncase AMDGPU_BO_LIST_OP_DESTROY:\r\namdgpu_bo_list_destroy(fpriv, handle);\r\nhandle = 0;\r\nbreak;\r\ncase AMDGPU_BO_LIST_OP_UPDATE:\r\nr = -ENOENT;\r\nlist = amdgpu_bo_list_get(fpriv, handle);\r\nif (!list)\r\ngoto error_free;\r\nr = amdgpu_bo_list_set(adev, filp, list, info,\r\nargs->in.bo_number);\r\namdgpu_bo_list_put(list);\r\nif (r)\r\ngoto error_free;\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\ngoto error_free;\r\n}\r\nmemset(args, 0, sizeof(*args));\r\nargs->out.list_handle = handle;\r\ndrm_free_large(info);\r\nreturn 0;\r\nerror_free:\r\ndrm_free_large(info);\r\nreturn r;\r\n}
