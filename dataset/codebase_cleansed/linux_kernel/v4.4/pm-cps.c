bool cps_pm_support_state(enum cps_pm_state state)\r\n{\r\nreturn test_bit(state, state_support);\r\n}\r\nstatic void coupled_barrier(atomic_t *a, unsigned online)\r\n{\r\nif (!coupled_coherence)\r\nreturn;\r\nsmp_mb__before_atomic();\r\natomic_inc(a);\r\nwhile (atomic_read(a) < online)\r\ncpu_relax();\r\nif (atomic_inc_return(a) == online * 2) {\r\natomic_set(a, 0);\r\nreturn;\r\n}\r\nwhile (atomic_read(a) > online)\r\ncpu_relax();\r\n}\r\nint cps_pm_enter_state(enum cps_pm_state state)\r\n{\r\nunsigned cpu = smp_processor_id();\r\nunsigned core = current_cpu_data.core;\r\nunsigned online, left;\r\ncpumask_t *coupled_mask = this_cpu_ptr(&online_coupled);\r\nu32 *core_ready_count, *nc_core_ready_count;\r\nvoid *nc_addr;\r\ncps_nc_entry_fn entry;\r\nstruct core_boot_config *core_cfg;\r\nstruct vpe_boot_config *vpe_cfg;\r\nentry = per_cpu(nc_asm_enter, core)[state];\r\nif (!entry)\r\nreturn -EINVAL;\r\n#ifdef CONFIG_MIPS_MT\r\nif (cpu_online(cpu)) {\r\ncpumask_and(coupled_mask, cpu_online_mask,\r\n&cpu_sibling_map[cpu]);\r\nonline = cpumask_weight(coupled_mask);\r\ncpumask_clear_cpu(cpu, coupled_mask);\r\n} else\r\n#endif\r\n{\r\ncpumask_clear(coupled_mask);\r\nonline = 1;\r\n}\r\nif (config_enabled(CONFIG_CPU_PM) && state == CPS_PM_POWER_GATED) {\r\nif (!mips_cps_smp_in_use())\r\nreturn -EINVAL;\r\ncore_cfg = &mips_cps_core_bootcfg[core];\r\nvpe_cfg = &core_cfg->vpe_config[cpu_vpe_id(&current_cpu_data)];\r\nvpe_cfg->pc = (unsigned long)mips_cps_pm_restore;\r\nvpe_cfg->gp = (unsigned long)current_thread_info();\r\nvpe_cfg->sp = 0;\r\n}\r\ncpumask_clear_cpu(cpu, &cpu_coherent_mask);\r\nsmp_mb__after_atomic();\r\ncore_ready_count = per_cpu(ready_count, core);\r\nnc_addr = kmap_noncoherent(virt_to_page(core_ready_count),\r\n(unsigned long)core_ready_count);\r\nnc_addr += ((unsigned long)core_ready_count & ~PAGE_MASK);\r\nnc_core_ready_count = nc_addr;\r\nACCESS_ONCE(*nc_core_ready_count) = 0;\r\ncoupled_barrier(&per_cpu(pm_barrier, core), online);\r\nleft = entry(online, nc_core_ready_count);\r\nkunmap_noncoherent();\r\ncpumask_set_cpu(cpu, &cpu_coherent_mask);\r\nif (coupled_coherence && (state == CPS_PM_NC_WAIT) && (left == online))\r\narch_send_call_function_ipi_mask(coupled_mask);\r\nreturn 0;\r\n}\r\nstatic void __init cps_gen_cache_routine(u32 **pp, struct uasm_label **pl,\r\nstruct uasm_reloc **pr,\r\nconst struct cache_desc *cache,\r\nunsigned op, int lbl)\r\n{\r\nunsigned cache_size = cache->ways << cache->waybit;\r\nunsigned i;\r\nconst unsigned unroll_lines = 32;\r\nif (cache->flags & MIPS_CACHE_NOT_PRESENT)\r\nreturn;\r\nUASM_i_LA(pp, t0, (long)CKSEG0);\r\nif (cache_size < 0x8000)\r\nuasm_i_addiu(pp, t1, t0, cache_size);\r\nelse\r\nUASM_i_LA(pp, t1, (long)(CKSEG0 + cache_size));\r\nuasm_build_label(pl, *pp, lbl);\r\nfor (i = 0; i < unroll_lines; i++)\r\nuasm_i_cache(pp, op, i * cache->linesz, t0);\r\nuasm_i_addiu(pp, t0, t0, unroll_lines * cache->linesz);\r\nuasm_il_bne(pp, pr, t0, t1, lbl);\r\nuasm_i_nop(pp);\r\n}\r\nstatic int __init cps_gen_flush_fsb(u32 **pp, struct uasm_label **pl,\r\nstruct uasm_reloc **pr,\r\nconst struct cpuinfo_mips *cpu_info,\r\nint lbl)\r\n{\r\nunsigned i, fsb_size = 8;\r\nunsigned num_loads = (fsb_size * 3) / 2;\r\nunsigned line_stride = 2;\r\nunsigned line_size = cpu_info->dcache.linesz;\r\nunsigned perf_counter, perf_event;\r\nunsigned revision = cpu_info->processor_id & PRID_REV_MASK;\r\nswitch (__get_cpu_type(cpu_info->cputype)) {\r\ncase CPU_INTERAPTIV:\r\nperf_counter = 1;\r\nperf_event = 51;\r\nbreak;\r\ncase CPU_PROAPTIV:\r\nif (revision >= PRID_REV_ENCODE_332(1, 1, 0))\r\nreturn 0;\r\nreturn -1;\r\ncase CPU_P5600:\r\ncase CPU_I6400:\r\nreturn 0;\r\ndefault:\r\nWARN_ONCE(1, "pm-cps: FSB flush unsupported for this CPU\n");\r\nreturn -1;\r\n}\r\nuasm_i_mfc0(pp, t2, 25, (perf_counter * 2) + 0);\r\nuasm_i_mfc0(pp, t3, 25, (perf_counter * 2) + 1);\r\nuasm_i_addiu(pp, t0, zero, (perf_event << 5) | 0xf);\r\nuasm_i_mtc0(pp, t0, 25, (perf_counter * 2) + 0);\r\nuasm_i_ehb(pp);\r\nuasm_i_mtc0(pp, zero, 25, (perf_counter * 2) + 1);\r\nuasm_i_ehb(pp);\r\nUASM_i_LA(pp, t0, (long)CKSEG0);\r\nuasm_build_label(pl, *pp, lbl);\r\nfor (i = 0; i < num_loads; i++)\r\nuasm_i_lw(pp, zero, i * line_size * line_stride, t0);\r\nfor (i = 0; i < num_loads; i++) {\r\nuasm_i_cache(pp, Hit_Invalidate_D,\r\ni * line_size * line_stride, t0);\r\nuasm_i_cache(pp, Hit_Writeback_Inv_SD,\r\ni * line_size * line_stride, t0);\r\n}\r\nuasm_i_sync(pp, stype_memory);\r\nuasm_i_ehb(pp);\r\nuasm_i_mfc0(pp, t1, 25, (perf_counter * 2) + 1);\r\nuasm_il_beqz(pp, pr, t1, lbl);\r\nuasm_i_nop(pp);\r\nuasm_i_mtc0(pp, t2, 25, (perf_counter * 2) + 0);\r\nuasm_i_ehb(pp);\r\nuasm_i_mtc0(pp, t3, 25, (perf_counter * 2) + 1);\r\nuasm_i_ehb(pp);\r\nreturn 0;\r\n}\r\nstatic void __init cps_gen_set_top_bit(u32 **pp, struct uasm_label **pl,\r\nstruct uasm_reloc **pr,\r\nunsigned r_addr, int lbl)\r\n{\r\nuasm_i_lui(pp, t0, uasm_rel_hi(0x80000000));\r\nuasm_build_label(pl, *pp, lbl);\r\nuasm_i_ll(pp, t1, 0, r_addr);\r\nuasm_i_or(pp, t1, t1, t0);\r\nuasm_i_sc(pp, t1, 0, r_addr);\r\nuasm_il_beqz(pp, pr, t1, lbl);\r\nuasm_i_nop(pp);\r\n}\r\nstatic void * __init cps_gen_entry_code(unsigned cpu, enum cps_pm_state state)\r\n{\r\nstruct uasm_label *l = labels;\r\nstruct uasm_reloc *r = relocs;\r\nu32 *buf, *p;\r\nconst unsigned r_online = a0;\r\nconst unsigned r_nc_count = a1;\r\nconst unsigned r_pcohctl = t7;\r\nconst unsigned max_instrs = 256;\r\nunsigned cpc_cmd;\r\nint err;\r\nenum {\r\nlbl_incready = 1,\r\nlbl_poll_cont,\r\nlbl_secondary_hang,\r\nlbl_disable_coherence,\r\nlbl_flush_fsb,\r\nlbl_invicache,\r\nlbl_flushdcache,\r\nlbl_hang,\r\nlbl_set_cont,\r\nlbl_secondary_cont,\r\nlbl_decready,\r\n};\r\np = buf = kcalloc(max_instrs, sizeof(u32), GFP_KERNEL);\r\nif (!buf)\r\nreturn NULL;\r\nmemset(labels, 0, sizeof(labels));\r\nmemset(relocs, 0, sizeof(relocs));\r\nif (config_enabled(CONFIG_CPU_PM) && state == CPS_PM_POWER_GATED) {\r\nif (!mips_cps_smp_in_use())\r\ngoto out_err;\r\nUASM_i_LA(&p, t0, (long)mips_cps_pm_save);\r\nuasm_i_jalr(&p, v0, t0);\r\nuasm_i_nop(&p);\r\n}\r\nUASM_i_LA(&p, r_pcohctl, (long)addr_gcr_cl_coherence());\r\nif (coupled_coherence) {\r\nuasm_i_sync(&p, stype_ordering);\r\nuasm_build_label(&l, p, lbl_incready);\r\nuasm_i_ll(&p, t1, 0, r_nc_count);\r\nuasm_i_addiu(&p, t2, t1, 1);\r\nuasm_i_sc(&p, t2, 0, r_nc_count);\r\nuasm_il_beqz(&p, &r, t2, lbl_incready);\r\nuasm_i_addiu(&p, t1, t1, 1);\r\nuasm_i_sync(&p, stype_ordering);\r\nuasm_il_beq(&p, &r, t1, r_online, lbl_disable_coherence);\r\nuasm_i_nop(&p);\r\nif (state < CPS_PM_POWER_GATED) {\r\nuasm_i_addiu(&p, t1, zero, -1);\r\nuasm_build_label(&l, p, lbl_poll_cont);\r\nuasm_i_lw(&p, t0, 0, r_nc_count);\r\nuasm_il_bltz(&p, &r, t0, lbl_secondary_cont);\r\nuasm_i_ehb(&p);\r\nuasm_i_yield(&p, zero, t1);\r\nuasm_il_b(&p, &r, lbl_poll_cont);\r\nuasm_i_nop(&p);\r\n} else {\r\nuasm_i_addiu(&p, t0, zero, TCHALT_H);\r\nuasm_i_mtc0(&p, t0, 2, 4);\r\nuasm_build_label(&l, p, lbl_secondary_hang);\r\nuasm_il_b(&p, &r, lbl_secondary_hang);\r\nuasm_i_nop(&p);\r\n}\r\n}\r\nuasm_build_label(&l, p, lbl_disable_coherence);\r\ncps_gen_cache_routine(&p, &l, &r, &cpu_data[cpu].icache,\r\nIndex_Invalidate_I, lbl_invicache);\r\ncps_gen_cache_routine(&p, &l, &r, &cpu_data[cpu].dcache,\r\nIndex_Writeback_Inv_D, lbl_flushdcache);\r\nuasm_i_sync(&p, stype_memory);\r\nuasm_i_ehb(&p);\r\nuasm_i_addiu(&p, t0, zero, 1 << cpu_data[cpu].core);\r\nuasm_i_sw(&p, t0, 0, r_pcohctl);\r\nuasm_i_lw(&p, t0, 0, r_pcohctl);\r\nuasm_i_sync(&p, stype_intervention);\r\nuasm_i_ehb(&p);\r\nuasm_i_sw(&p, zero, 0, r_pcohctl);\r\nuasm_i_lw(&p, t0, 0, r_pcohctl);\r\nif (state >= CPS_PM_CLOCK_GATED) {\r\nerr = cps_gen_flush_fsb(&p, &l, &r, &cpu_data[cpu],\r\nlbl_flush_fsb);\r\nif (err)\r\ngoto out_err;\r\nswitch (state) {\r\ncase CPS_PM_CLOCK_GATED:\r\ncpc_cmd = CPC_Cx_CMD_CLOCKOFF;\r\nbreak;\r\ncase CPS_PM_POWER_GATED:\r\ncpc_cmd = CPC_Cx_CMD_PWRDOWN;\r\nbreak;\r\ndefault:\r\nBUG();\r\ngoto out_err;\r\n}\r\nUASM_i_LA(&p, t0, (long)addr_cpc_cl_cmd());\r\nuasm_i_addiu(&p, t1, zero, cpc_cmd);\r\nuasm_i_sw(&p, t1, 0, t0);\r\nif (state == CPS_PM_POWER_GATED) {\r\nuasm_build_label(&l, p, lbl_hang);\r\nuasm_il_b(&p, &r, lbl_hang);\r\nuasm_i_nop(&p);\r\ngoto gen_done;\r\n}\r\nuasm_i_sync(&p, stype_memory);\r\nuasm_i_ehb(&p);\r\n}\r\nif (state == CPS_PM_NC_WAIT) {\r\nif (coupled_coherence)\r\ncps_gen_set_top_bit(&p, &l, &r, r_nc_count,\r\nlbl_set_cont);\r\nuasm_build_label(&l, p, lbl_secondary_cont);\r\nuasm_i_wait(&p, 0);\r\n}\r\nuasm_i_addiu(&p, t0, zero, CM_GCR_Cx_COHERENCE_COHDOMAINEN_MSK);\r\nuasm_i_sw(&p, t0, 0, r_pcohctl);\r\nuasm_i_lw(&p, t0, 0, r_pcohctl);\r\nuasm_i_sync(&p, stype_memory);\r\nuasm_i_ehb(&p);\r\nif (coupled_coherence && (state == CPS_PM_NC_WAIT)) {\r\nuasm_build_label(&l, p, lbl_decready);\r\nuasm_i_sync(&p, stype_ordering);\r\nuasm_i_ll(&p, t1, 0, r_nc_count);\r\nuasm_i_addiu(&p, t2, t1, -1);\r\nuasm_i_sc(&p, t2, 0, r_nc_count);\r\nuasm_il_beqz(&p, &r, t2, lbl_decready);\r\nuasm_i_andi(&p, v0, t1, (1 << fls(smp_num_siblings)) - 1);\r\nuasm_i_sync(&p, stype_ordering);\r\n}\r\nif (coupled_coherence && (state == CPS_PM_CLOCK_GATED)) {\r\ncps_gen_set_top_bit(&p, &l, &r, r_nc_count, lbl_set_cont);\r\nuasm_build_label(&l, p, lbl_secondary_cont);\r\nuasm_i_sync(&p, stype_ordering);\r\n}\r\nuasm_i_jr(&p, ra);\r\nuasm_i_nop(&p);\r\ngen_done:\r\nBUG_ON((p - buf) > max_instrs);\r\nBUG_ON((l - labels) > ARRAY_SIZE(labels));\r\nBUG_ON((r - relocs) > ARRAY_SIZE(relocs));\r\nuasm_resolve_relocs(relocs, labels);\r\nlocal_flush_icache_range((unsigned long)buf, (unsigned long)p);\r\nreturn buf;\r\nout_err:\r\nkfree(buf);\r\nreturn NULL;\r\n}\r\nstatic int __init cps_gen_core_entries(unsigned cpu)\r\n{\r\nenum cps_pm_state state;\r\nunsigned core = cpu_data[cpu].core;\r\nunsigned dlinesz = cpu_data[cpu].dcache.linesz;\r\nvoid *entry_fn, *core_rc;\r\nfor (state = CPS_PM_NC_WAIT; state < CPS_PM_STATE_COUNT; state++) {\r\nif (per_cpu(nc_asm_enter, core)[state])\r\ncontinue;\r\nif (!test_bit(state, state_support))\r\ncontinue;\r\nentry_fn = cps_gen_entry_code(cpu, state);\r\nif (!entry_fn) {\r\npr_err("Failed to generate core %u state %u entry\n",\r\ncore, state);\r\nclear_bit(state, state_support);\r\n}\r\nper_cpu(nc_asm_enter, core)[state] = entry_fn;\r\n}\r\nif (!per_cpu(ready_count, core)) {\r\ncore_rc = kmalloc(dlinesz * 2, GFP_KERNEL);\r\nif (!core_rc) {\r\npr_err("Failed allocate core %u ready_count\n", core);\r\nreturn -ENOMEM;\r\n}\r\nper_cpu(ready_count_alloc, core) = core_rc;\r\ncore_rc += dlinesz - 1;\r\ncore_rc = (void *)((unsigned long)core_rc & ~(dlinesz - 1));\r\nper_cpu(ready_count, core) = core_rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init cps_pm_init(void)\r\n{\r\nunsigned cpu;\r\nint err;\r\nswitch (current_cpu_data.cputype) {\r\ncase CPU_INTERAPTIV:\r\ncase CPU_PROAPTIV:\r\ncase CPU_M5150:\r\ncase CPU_P5600:\r\ncase CPU_I6400:\r\nstype_intervention = 0x2;\r\nstype_memory = 0x3;\r\nstype_ordering = 0x10;\r\nbreak;\r\ndefault:\r\npr_warn("Power management is using heavyweight sync 0\n");\r\n}\r\nif (!mips_cm_present()) {\r\npr_warn("pm-cps: no CM, non-coherent states unavailable\n");\r\ngoto out;\r\n}\r\nif (cpu_wait == r4k_wait_irqoff)\r\nset_bit(CPS_PM_NC_WAIT, state_support);\r\nelse\r\npr_warn("pm-cps: non-coherent wait unavailable\n");\r\nif (mips_cpc_present()) {\r\nif (read_cpc_cl_stat_conf() & CPC_Cx_STAT_CONF_CLKGAT_IMPL_MSK)\r\nset_bit(CPS_PM_CLOCK_GATED, state_support);\r\nelse\r\npr_warn("pm-cps: CPC does not support clock gating\n");\r\nif (mips_cps_smp_in_use())\r\nset_bit(CPS_PM_POWER_GATED, state_support);\r\nelse\r\npr_warn("pm-cps: CPS SMP not in use, power gating unavailable\n");\r\n} else {\r\npr_warn("pm-cps: no CPC, clock & power gating unavailable\n");\r\n}\r\nfor_each_present_cpu(cpu) {\r\nerr = cps_gen_core_entries(cpu);\r\nif (err)\r\nreturn err;\r\n}\r\nout:\r\nreturn 0;\r\n}
