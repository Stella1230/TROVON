static inline unsigned long espfix_base_addr(unsigned int cpu)\r\n{\r\nunsigned long page, slot;\r\nunsigned long addr;\r\npage = (cpu / ESPFIX_STACKS_PER_PAGE) ^ page_random;\r\nslot = (cpu + slot_random) % ESPFIX_STACKS_PER_PAGE;\r\naddr = (page << PAGE_SHIFT) + (slot * ESPFIX_STACK_SIZE);\r\naddr = (addr & 0xffffUL) | ((addr & ~0xffffUL) << 16);\r\naddr += ESPFIX_BASE_ADDR;\r\nreturn addr;\r\n}\r\nstatic void init_espfix_random(void)\r\n{\r\nunsigned long rand;\r\nif (!arch_get_random_long(&rand)) {\r\nrand = rdtsc();\r\nrand *= 0xc345c6b72fd16123UL;\r\n}\r\nslot_random = rand % ESPFIX_STACKS_PER_PAGE;\r\npage_random = (rand / ESPFIX_STACKS_PER_PAGE)\r\n& (ESPFIX_PAGE_SPACE - 1);\r\n}\r\nvoid __init init_espfix_bsp(void)\r\n{\r\npgd_t *pgd_p;\r\npgd_p = &init_level4_pgt[pgd_index(ESPFIX_BASE_ADDR)];\r\npgd_populate(&init_mm, pgd_p, (pud_t *)espfix_pud_page);\r\ninit_espfix_random();\r\ninit_espfix_ap(0);\r\n}\r\nvoid init_espfix_ap(int cpu)\r\n{\r\nunsigned int page;\r\nunsigned long addr;\r\npud_t pud, *pud_p;\r\npmd_t pmd, *pmd_p;\r\npte_t pte, *pte_p;\r\nint n, node;\r\nvoid *stack_page;\r\npteval_t ptemask;\r\nif (likely(per_cpu(espfix_stack, cpu)))\r\nreturn;\r\naddr = espfix_base_addr(cpu);\r\npage = cpu/ESPFIX_STACKS_PER_PAGE;\r\nstack_page = ACCESS_ONCE(espfix_pages[page]);\r\nif (likely(stack_page))\r\ngoto done;\r\nmutex_lock(&espfix_init_mutex);\r\nstack_page = ACCESS_ONCE(espfix_pages[page]);\r\nif (stack_page)\r\ngoto unlock_done;\r\nnode = cpu_to_node(cpu);\r\nptemask = __supported_pte_mask;\r\npud_p = &espfix_pud_page[pud_index(addr)];\r\npud = *pud_p;\r\nif (!pud_present(pud)) {\r\nstruct page *page = alloc_pages_node(node, PGALLOC_GFP, 0);\r\npmd_p = (pmd_t *)page_address(page);\r\npud = __pud(__pa(pmd_p) | (PGTABLE_PROT & ptemask));\r\nparavirt_alloc_pmd(&init_mm, __pa(pmd_p) >> PAGE_SHIFT);\r\nfor (n = 0; n < ESPFIX_PUD_CLONES; n++)\r\nset_pud(&pud_p[n], pud);\r\n}\r\npmd_p = pmd_offset(&pud, addr);\r\npmd = *pmd_p;\r\nif (!pmd_present(pmd)) {\r\nstruct page *page = alloc_pages_node(node, PGALLOC_GFP, 0);\r\npte_p = (pte_t *)page_address(page);\r\npmd = __pmd(__pa(pte_p) | (PGTABLE_PROT & ptemask));\r\nparavirt_alloc_pte(&init_mm, __pa(pte_p) >> PAGE_SHIFT);\r\nfor (n = 0; n < ESPFIX_PMD_CLONES; n++)\r\nset_pmd(&pmd_p[n], pmd);\r\n}\r\npte_p = pte_offset_kernel(&pmd, addr);\r\nstack_page = page_address(alloc_pages_node(node, GFP_KERNEL, 0));\r\npte = __pte(__pa(stack_page) | (__PAGE_KERNEL_RO & ptemask));\r\nfor (n = 0; n < ESPFIX_PTE_CLONES; n++)\r\nset_pte(&pte_p[n*PTE_STRIDE], pte);\r\nACCESS_ONCE(espfix_pages[page]) = stack_page;\r\nunlock_done:\r\nmutex_unlock(&espfix_init_mutex);\r\ndone:\r\nper_cpu(espfix_stack, cpu) = addr;\r\nper_cpu(espfix_waddr, cpu) = (unsigned long)stack_page\r\n+ (addr & ~PAGE_MASK);\r\n}
