static void echainiv_read_iv(u8 *dst, unsigned size)\r\n{\r\nu32 *a = (u32 *)dst;\r\nu32 __percpu *b = echainiv_iv;\r\nfor (; size >= 4; size -= 4) {\r\n*a++ = this_cpu_read(*b);\r\nb++;\r\n}\r\n}\r\nstatic void echainiv_write_iv(const u8 *src, unsigned size)\r\n{\r\nconst u32 *a = (const u32 *)src;\r\nu32 __percpu *b = echainiv_iv;\r\nfor (; size >= 4; size -= 4) {\r\nthis_cpu_write(*b, *a);\r\na++;\r\nb++;\r\n}\r\n}\r\nstatic void echainiv_encrypt_complete2(struct aead_request *req, int err)\r\n{\r\nstruct aead_request *subreq = aead_request_ctx(req);\r\nstruct crypto_aead *geniv;\r\nunsigned int ivsize;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nif (err)\r\ngoto out;\r\ngeniv = crypto_aead_reqtfm(req);\r\nivsize = crypto_aead_ivsize(geniv);\r\nechainiv_write_iv(subreq->iv, ivsize);\r\nif (req->iv != subreq->iv)\r\nmemcpy(req->iv, subreq->iv, ivsize);\r\nout:\r\nif (req->iv != subreq->iv)\r\nkzfree(subreq->iv);\r\n}\r\nstatic void echainiv_encrypt_complete(struct crypto_async_request *base,\r\nint err)\r\n{\r\nstruct aead_request *req = base->data;\r\nechainiv_encrypt_complete2(req, err);\r\naead_request_complete(req, err);\r\n}\r\nstatic int echainiv_encrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *geniv = crypto_aead_reqtfm(req);\r\nstruct aead_geniv_ctx *ctx = crypto_aead_ctx(geniv);\r\nstruct aead_request *subreq = aead_request_ctx(req);\r\ncrypto_completion_t compl;\r\nvoid *data;\r\nu8 *info;\r\nunsigned int ivsize = crypto_aead_ivsize(geniv);\r\nint err;\r\nif (req->cryptlen < ivsize)\r\nreturn -EINVAL;\r\naead_request_set_tfm(subreq, ctx->child);\r\ncompl = echainiv_encrypt_complete;\r\ndata = req;\r\ninfo = req->iv;\r\nif (req->src != req->dst) {\r\nstruct blkcipher_desc desc = {\r\n.tfm = ctx->null,\r\n};\r\nerr = crypto_blkcipher_encrypt(\r\n&desc, req->dst, req->src,\r\nreq->assoclen + req->cryptlen);\r\nif (err)\r\nreturn err;\r\n}\r\nif (unlikely(!IS_ALIGNED((unsigned long)info,\r\ncrypto_aead_alignmask(geniv) + 1))) {\r\ninfo = kmalloc(ivsize, req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:\r\nGFP_ATOMIC);\r\nif (!info)\r\nreturn -ENOMEM;\r\nmemcpy(info, req->iv, ivsize);\r\n}\r\naead_request_set_callback(subreq, req->base.flags, compl, data);\r\naead_request_set_crypt(subreq, req->dst, req->dst,\r\nreq->cryptlen, info);\r\naead_request_set_ad(subreq, req->assoclen);\r\ncrypto_xor(info, ctx->salt, ivsize);\r\nscatterwalk_map_and_copy(info, req->dst, req->assoclen, ivsize, 1);\r\nechainiv_read_iv(info, ivsize);\r\nerr = crypto_aead_encrypt(subreq);\r\nechainiv_encrypt_complete2(req, err);\r\nreturn err;\r\n}\r\nstatic int echainiv_decrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *geniv = crypto_aead_reqtfm(req);\r\nstruct aead_geniv_ctx *ctx = crypto_aead_ctx(geniv);\r\nstruct aead_request *subreq = aead_request_ctx(req);\r\ncrypto_completion_t compl;\r\nvoid *data;\r\nunsigned int ivsize = crypto_aead_ivsize(geniv);\r\nif (req->cryptlen < ivsize)\r\nreturn -EINVAL;\r\naead_request_set_tfm(subreq, ctx->child);\r\ncompl = req->base.complete;\r\ndata = req->base.data;\r\naead_request_set_callback(subreq, req->base.flags, compl, data);\r\naead_request_set_crypt(subreq, req->src, req->dst,\r\nreq->cryptlen - ivsize, req->iv);\r\naead_request_set_ad(subreq, req->assoclen + ivsize);\r\nscatterwalk_map_and_copy(req->iv, req->src, req->assoclen, ivsize, 0);\r\nreturn crypto_aead_decrypt(subreq);\r\n}\r\nstatic int echainiv_aead_create(struct crypto_template *tmpl,\r\nstruct rtattr **tb)\r\n{\r\nstruct aead_instance *inst;\r\nstruct crypto_aead_spawn *spawn;\r\nstruct aead_alg *alg;\r\nint err;\r\ninst = aead_geniv_alloc(tmpl, tb, 0, 0);\r\nif (IS_ERR(inst))\r\nreturn PTR_ERR(inst);\r\nspawn = aead_instance_ctx(inst);\r\nalg = crypto_spawn_aead_alg(spawn);\r\nerr = -EINVAL;\r\nif (inst->alg.ivsize & (sizeof(u32) - 1) ||\r\ninst->alg.ivsize > MAX_IV_SIZE)\r\ngoto free_inst;\r\ninst->alg.encrypt = echainiv_encrypt;\r\ninst->alg.decrypt = echainiv_decrypt;\r\ninst->alg.init = aead_init_geniv;\r\ninst->alg.exit = aead_exit_geniv;\r\ninst->alg.base.cra_alignmask |= __alignof__(u32) - 1;\r\ninst->alg.base.cra_ctxsize = sizeof(struct aead_geniv_ctx);\r\ninst->alg.base.cra_ctxsize += inst->alg.ivsize;\r\ninst->free = aead_geniv_free;\r\nerr = aead_register_instance(tmpl, inst);\r\nif (err)\r\ngoto free_inst;\r\nout:\r\nreturn err;\r\nfree_inst:\r\naead_geniv_free(inst);\r\ngoto out;\r\n}\r\nstatic void echainiv_free(struct crypto_instance *inst)\r\n{\r\naead_geniv_free(aead_instance(inst));\r\n}\r\nstatic int __init echainiv_module_init(void)\r\n{\r\nreturn crypto_register_template(&echainiv_tmpl);\r\n}\r\nstatic void __exit echainiv_module_exit(void)\r\n{\r\ncrypto_unregister_template(&echainiv_tmpl);\r\n}
