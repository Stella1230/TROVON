static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)\r\n{\r\nint i;\r\nfor (i = 0; i < pmu->nr_arch_fixed_counters; i++) {\r\nu8 new_ctrl = fixed_ctrl_field(data, i);\r\nu8 old_ctrl = fixed_ctrl_field(pmu->fixed_ctr_ctrl, i);\r\nstruct kvm_pmc *pmc;\r\npmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);\r\nif (old_ctrl == new_ctrl)\r\ncontinue;\r\nreprogram_fixed_counter(pmc, new_ctrl, i);\r\n}\r\npmu->fixed_ctr_ctrl = data;\r\n}\r\nstatic void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)\r\n{\r\nint bit;\r\nu64 diff = pmu->global_ctrl ^ data;\r\npmu->global_ctrl = data;\r\nfor_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)\r\nreprogram_counter(pmu, bit);\r\n}\r\nstatic unsigned intel_find_arch_event(struct kvm_pmu *pmu,\r\nu8 event_select,\r\nu8 unit_mask)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(intel_arch_events); i++)\r\nif (intel_arch_events[i].eventsel == event_select\r\n&& intel_arch_events[i].unit_mask == unit_mask\r\n&& (pmu->available_event_types & (1 << i)))\r\nbreak;\r\nif (i == ARRAY_SIZE(intel_arch_events))\r\nreturn PERF_COUNT_HW_MAX;\r\nreturn intel_arch_events[i].event_type;\r\n}\r\nstatic unsigned intel_find_fixed_event(int idx)\r\n{\r\nif (idx >= ARRAY_SIZE(fixed_pmc_events))\r\nreturn PERF_COUNT_HW_MAX;\r\nreturn intel_arch_events[fixed_pmc_events[idx]].event_type;\r\n}\r\nstatic bool intel_pmc_is_enabled(struct kvm_pmc *pmc)\r\n{\r\nstruct kvm_pmu *pmu = pmc_to_pmu(pmc);\r\nreturn test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);\r\n}\r\nstatic struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)\r\n{\r\nif (pmc_idx < INTEL_PMC_IDX_FIXED)\r\nreturn get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,\r\nMSR_P6_EVNTSEL0);\r\nelse {\r\nu32 idx = pmc_idx - INTEL_PMC_IDX_FIXED;\r\nreturn get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);\r\n}\r\n}\r\nstatic int intel_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned idx)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nbool fixed = idx & (1u << 30);\r\nidx &= ~(3u << 30);\r\nreturn (!fixed && idx >= pmu->nr_arch_gp_counters) ||\r\n(fixed && idx >= pmu->nr_arch_fixed_counters);\r\n}\r\nstatic struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu,\r\nunsigned idx)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nbool fixed = idx & (1u << 30);\r\nstruct kvm_pmc *counters;\r\nidx &= ~(3u << 30);\r\nif (!fixed && idx >= pmu->nr_arch_gp_counters)\r\nreturn NULL;\r\nif (fixed && idx >= pmu->nr_arch_fixed_counters)\r\nreturn NULL;\r\ncounters = fixed ? pmu->fixed_counters : pmu->gp_counters;\r\nreturn &counters[idx];\r\n}\r\nstatic bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nint ret;\r\nswitch (msr) {\r\ncase MSR_CORE_PERF_FIXED_CTR_CTRL:\r\ncase MSR_CORE_PERF_GLOBAL_STATUS:\r\ncase MSR_CORE_PERF_GLOBAL_CTRL:\r\ncase MSR_CORE_PERF_GLOBAL_OVF_CTRL:\r\nret = pmu->version > 1;\r\nbreak;\r\ndefault:\r\nret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||\r\nget_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||\r\nget_fixed_pmc(pmu, msr);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int intel_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nstruct kvm_pmc *pmc;\r\nswitch (msr) {\r\ncase MSR_CORE_PERF_FIXED_CTR_CTRL:\r\n*data = pmu->fixed_ctr_ctrl;\r\nreturn 0;\r\ncase MSR_CORE_PERF_GLOBAL_STATUS:\r\n*data = pmu->global_status;\r\nreturn 0;\r\ncase MSR_CORE_PERF_GLOBAL_CTRL:\r\n*data = pmu->global_ctrl;\r\nreturn 0;\r\ncase MSR_CORE_PERF_GLOBAL_OVF_CTRL:\r\n*data = pmu->global_ovf_ctrl;\r\nreturn 0;\r\ndefault:\r\nif ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||\r\n(pmc = get_fixed_pmc(pmu, msr))) {\r\n*data = pmc_read_counter(pmc);\r\nreturn 0;\r\n} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {\r\n*data = pmc->eventsel;\r\nreturn 0;\r\n}\r\n}\r\nreturn 1;\r\n}\r\nstatic int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nstruct kvm_pmc *pmc;\r\nu32 msr = msr_info->index;\r\nu64 data = msr_info->data;\r\nswitch (msr) {\r\ncase MSR_CORE_PERF_FIXED_CTR_CTRL:\r\nif (pmu->fixed_ctr_ctrl == data)\r\nreturn 0;\r\nif (!(data & 0xfffffffffffff444ull)) {\r\nreprogram_fixed_counters(pmu, data);\r\nreturn 0;\r\n}\r\nbreak;\r\ncase MSR_CORE_PERF_GLOBAL_STATUS:\r\nif (msr_info->host_initiated) {\r\npmu->global_status = data;\r\nreturn 0;\r\n}\r\nbreak;\r\ncase MSR_CORE_PERF_GLOBAL_CTRL:\r\nif (pmu->global_ctrl == data)\r\nreturn 0;\r\nif (!(data & pmu->global_ctrl_mask)) {\r\nglobal_ctrl_changed(pmu, data);\r\nreturn 0;\r\n}\r\nbreak;\r\ncase MSR_CORE_PERF_GLOBAL_OVF_CTRL:\r\nif (!(data & (pmu->global_ctrl_mask & ~(3ull<<62)))) {\r\nif (!msr_info->host_initiated)\r\npmu->global_status &= ~data;\r\npmu->global_ovf_ctrl = data;\r\nreturn 0;\r\n}\r\nbreak;\r\ndefault:\r\nif ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||\r\n(pmc = get_fixed_pmc(pmu, msr))) {\r\nif (!msr_info->host_initiated)\r\ndata = (s64)(s32)data;\r\npmc->counter += data - pmc_read_counter(pmc);\r\nreturn 0;\r\n} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {\r\nif (data == pmc->eventsel)\r\nreturn 0;\r\nif (!(data & pmu->reserved_bits)) {\r\nreprogram_gp_counter(pmc, data);\r\nreturn 0;\r\n}\r\n}\r\n}\r\nreturn 1;\r\n}\r\nstatic void intel_pmu_refresh(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nstruct kvm_cpuid_entry2 *entry;\r\nunion cpuid10_eax eax;\r\nunion cpuid10_edx edx;\r\npmu->nr_arch_gp_counters = 0;\r\npmu->nr_arch_fixed_counters = 0;\r\npmu->counter_bitmask[KVM_PMC_GP] = 0;\r\npmu->counter_bitmask[KVM_PMC_FIXED] = 0;\r\npmu->version = 0;\r\npmu->reserved_bits = 0xffffffff00200000ull;\r\nentry = kvm_find_cpuid_entry(vcpu, 0xa, 0);\r\nif (!entry)\r\nreturn;\r\neax.full = entry->eax;\r\nedx.full = entry->edx;\r\npmu->version = eax.split.version_id;\r\nif (!pmu->version)\r\nreturn;\r\npmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters,\r\nINTEL_PMC_MAX_GENERIC);\r\npmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << eax.split.bit_width) - 1;\r\npmu->available_event_types = ~entry->ebx &\r\n((1ull << eax.split.mask_length) - 1);\r\nif (pmu->version == 1) {\r\npmu->nr_arch_fixed_counters = 0;\r\n} else {\r\npmu->nr_arch_fixed_counters =\r\nmin_t(int, edx.split.num_counters_fixed,\r\nINTEL_PMC_MAX_FIXED);\r\npmu->counter_bitmask[KVM_PMC_FIXED] =\r\n((u64)1 << edx.split.bit_width_fixed) - 1;\r\n}\r\npmu->global_ctrl = ((1 << pmu->nr_arch_gp_counters) - 1) |\r\n(((1ull << pmu->nr_arch_fixed_counters) - 1) << INTEL_PMC_IDX_FIXED);\r\npmu->global_ctrl_mask = ~pmu->global_ctrl;\r\nentry = kvm_find_cpuid_entry(vcpu, 7, 0);\r\nif (entry &&\r\n(boot_cpu_has(X86_FEATURE_HLE) || boot_cpu_has(X86_FEATURE_RTM)) &&\r\n(entry->ebx & (X86_FEATURE_HLE|X86_FEATURE_RTM)))\r\npmu->reserved_bits ^= HSW_IN_TX|HSW_IN_TX_CHECKPOINTED;\r\n}\r\nstatic void intel_pmu_init(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nfor (i = 0; i < INTEL_PMC_MAX_GENERIC; i++) {\r\npmu->gp_counters[i].type = KVM_PMC_GP;\r\npmu->gp_counters[i].vcpu = vcpu;\r\npmu->gp_counters[i].idx = i;\r\n}\r\nfor (i = 0; i < INTEL_PMC_MAX_FIXED; i++) {\r\npmu->fixed_counters[i].type = KVM_PMC_FIXED;\r\npmu->fixed_counters[i].vcpu = vcpu;\r\npmu->fixed_counters[i].idx = i + INTEL_PMC_IDX_FIXED;\r\n}\r\n}\r\nstatic void intel_pmu_reset(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nint i;\r\nfor (i = 0; i < INTEL_PMC_MAX_GENERIC; i++) {\r\nstruct kvm_pmc *pmc = &pmu->gp_counters[i];\r\npmc_stop_counter(pmc);\r\npmc->counter = pmc->eventsel = 0;\r\n}\r\nfor (i = 0; i < INTEL_PMC_MAX_FIXED; i++)\r\npmc_stop_counter(&pmu->fixed_counters[i]);\r\npmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status =\r\npmu->global_ovf_ctrl = 0;\r\n}
