static inline int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq,\r\nstruct mlx5e_rx_wqe *wqe, u16 ix)\r\n{\r\nstruct sk_buff *skb;\r\ndma_addr_t dma_addr;\r\nskb = netdev_alloc_skb(rq->netdev, rq->wqe_sz);\r\nif (unlikely(!skb))\r\nreturn -ENOMEM;\r\ndma_addr = dma_map_single(rq->pdev,\r\nskb->data,\r\nrq->wqe_sz,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(rq->pdev, dma_addr)))\r\ngoto err_free_skb;\r\nskb_reserve(skb, MLX5E_NET_IP_ALIGN);\r\n*((dma_addr_t *)skb->cb) = dma_addr;\r\nwqe->data.addr = cpu_to_be64(dma_addr + MLX5E_NET_IP_ALIGN);\r\nrq->skb[ix] = skb;\r\nreturn 0;\r\nerr_free_skb:\r\ndev_kfree_skb(skb);\r\nreturn -ENOMEM;\r\n}\r\nbool mlx5e_post_rx_wqes(struct mlx5e_rq *rq)\r\n{\r\nstruct mlx5_wq_ll *wq = &rq->wq;\r\nif (unlikely(!test_bit(MLX5E_RQ_STATE_POST_WQES_ENABLE, &rq->state)))\r\nreturn false;\r\nwhile (!mlx5_wq_ll_is_full(wq)) {\r\nstruct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);\r\nif (unlikely(mlx5e_alloc_rx_wqe(rq, wqe, wq->head)))\r\nbreak;\r\nmlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));\r\n}\r\ndma_wmb();\r\nmlx5_wq_ll_update_db_record(wq);\r\nreturn !mlx5_wq_ll_is_full(wq);\r\n}\r\nstatic void mlx5e_lro_update_hdr(struct sk_buff *skb, struct mlx5_cqe64 *cqe)\r\n{\r\nstruct ethhdr *eth = (struct ethhdr *)(skb->data);\r\nstruct iphdr *ipv4 = (struct iphdr *)(skb->data + ETH_HLEN);\r\nstruct ipv6hdr *ipv6 = (struct ipv6hdr *)(skb->data + ETH_HLEN);\r\nstruct tcphdr *tcp;\r\nu8 l4_hdr_type = get_cqe_l4_hdr_type(cqe);\r\nint tcp_ack = ((CQE_L4_HDR_TYPE_TCP_ACK_NO_DATA == l4_hdr_type) ||\r\n(CQE_L4_HDR_TYPE_TCP_ACK_AND_DATA == l4_hdr_type));\r\nu16 tot_len = be32_to_cpu(cqe->byte_cnt) - ETH_HLEN;\r\nif (eth->h_proto == htons(ETH_P_IP)) {\r\ntcp = (struct tcphdr *)(skb->data + ETH_HLEN +\r\nsizeof(struct iphdr));\r\nipv6 = NULL;\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\r\n} else {\r\ntcp = (struct tcphdr *)(skb->data + ETH_HLEN +\r\nsizeof(struct ipv6hdr));\r\nipv4 = NULL;\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\r\n}\r\nif (get_cqe_lro_tcppsh(cqe))\r\ntcp->psh = 1;\r\nif (tcp_ack) {\r\ntcp->ack = 1;\r\ntcp->ack_seq = cqe->lro_ack_seq_num;\r\ntcp->window = cqe->lro_tcp_win;\r\n}\r\nif (ipv4) {\r\nipv4->ttl = cqe->lro_min_ttl;\r\nipv4->tot_len = cpu_to_be16(tot_len);\r\nipv4->check = 0;\r\nipv4->check = ip_fast_csum((unsigned char *)ipv4,\r\nipv4->ihl);\r\n} else {\r\nipv6->hop_limit = cqe->lro_min_ttl;\r\nipv6->payload_len = cpu_to_be16(tot_len -\r\nsizeof(struct ipv6hdr));\r\n}\r\n}\r\nstatic inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,\r\nstruct sk_buff *skb)\r\n{\r\nu8 cht = cqe->rss_hash_type;\r\nint ht = (cht & CQE_RSS_HTYPE_L4) ? PKT_HASH_TYPE_L4 :\r\n(cht & CQE_RSS_HTYPE_IP) ? PKT_HASH_TYPE_L3 :\r\nPKT_HASH_TYPE_NONE;\r\nskb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);\r\n}\r\nstatic inline bool is_first_ethertype_ip(struct sk_buff *skb)\r\n{\r\n__be16 ethertype = ((struct ethhdr *)skb->data)->h_proto;\r\nreturn (ethertype == htons(ETH_P_IP) || ethertype == htons(ETH_P_IPV6));\r\n}\r\nstatic inline void mlx5e_handle_csum(struct net_device *netdev,\r\nstruct mlx5_cqe64 *cqe,\r\nstruct mlx5e_rq *rq,\r\nstruct sk_buff *skb)\r\n{\r\nif (unlikely(!(netdev->features & NETIF_F_RXCSUM)))\r\ngoto csum_none;\r\nif (likely(cqe->hds_ip_ext & CQE_L4_OK)) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n} else if (is_first_ethertype_ip(skb)) {\r\nskb->ip_summed = CHECKSUM_COMPLETE;\r\nskb->csum = csum_unfold((__force __sum16)cqe->check_sum);\r\nrq->stats.csum_sw++;\r\n} else {\r\ngoto csum_none;\r\n}\r\nreturn;\r\ncsum_none:\r\nskb->ip_summed = CHECKSUM_NONE;\r\nrq->stats.csum_none++;\r\n}\r\nstatic inline void mlx5e_build_rx_skb(struct mlx5_cqe64 *cqe,\r\nstruct mlx5e_rq *rq,\r\nstruct sk_buff *skb)\r\n{\r\nstruct net_device *netdev = rq->netdev;\r\nu32 cqe_bcnt = be32_to_cpu(cqe->byte_cnt);\r\nint lro_num_seg;\r\nskb_put(skb, cqe_bcnt);\r\nlro_num_seg = be32_to_cpu(cqe->srqn) >> 24;\r\nif (lro_num_seg > 1) {\r\nmlx5e_lro_update_hdr(skb, cqe);\r\nskb_shinfo(skb)->gso_size = DIV_ROUND_UP(cqe_bcnt, lro_num_seg);\r\nrq->stats.lro_packets++;\r\nrq->stats.lro_bytes += cqe_bcnt;\r\n}\r\nmlx5e_handle_csum(netdev, cqe, rq, skb);\r\nskb->protocol = eth_type_trans(skb, netdev);\r\nskb_record_rx_queue(skb, rq->ix);\r\nif (likely(netdev->features & NETIF_F_RXHASH))\r\nmlx5e_skb_set_hash(cqe, skb);\r\nif (cqe_has_vlan(cqe))\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\r\nbe16_to_cpu(cqe->vlan_info));\r\n}\r\nbool mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)\r\n{\r\nstruct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);\r\nint i;\r\nif (!test_and_clear_bit(MLX5E_CQ_HAS_CQES, &cq->flags))\r\nreturn false;\r\nfor (i = 0; i < budget; i++) {\r\nstruct mlx5e_rx_wqe *wqe;\r\nstruct mlx5_cqe64 *cqe;\r\nstruct sk_buff *skb;\r\n__be16 wqe_counter_be;\r\nu16 wqe_counter;\r\ncqe = mlx5e_get_cqe(cq);\r\nif (!cqe)\r\nbreak;\r\nmlx5_cqwq_pop(&cq->wq);\r\nwqe_counter_be = cqe->wqe_counter;\r\nwqe_counter = be16_to_cpu(wqe_counter_be);\r\nwqe = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);\r\nskb = rq->skb[wqe_counter];\r\nprefetch(skb->data);\r\nrq->skb[wqe_counter] = NULL;\r\ndma_unmap_single(rq->pdev,\r\n*((dma_addr_t *)skb->cb),\r\nrq->wqe_sz,\r\nDMA_FROM_DEVICE);\r\nif (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {\r\nrq->stats.wqe_err++;\r\ndev_kfree_skb(skb);\r\ngoto wq_ll_pop;\r\n}\r\nmlx5e_build_rx_skb(cqe, rq, skb);\r\nrq->stats.packets++;\r\nnapi_gro_receive(cq->napi, skb);\r\nwq_ll_pop:\r\nmlx5_wq_ll_pop(&rq->wq, wqe_counter_be,\r\n&wqe->next.next_wqe_index);\r\n}\r\nmlx5_cqwq_update_db_record(&cq->wq);\r\nwmb();\r\nif (i == budget) {\r\nset_bit(MLX5E_CQ_HAS_CQES, &cq->flags);\r\nreturn true;\r\n}\r\nreturn false;\r\n}
