int *__atomic_hashed_lock(volatile void *v)\r\n{\r\nunsigned long ptr = __insn_mm((unsigned long)v >> 1,\r\n(unsigned long)atomic_locks,\r\n2, (ATOMIC_HASH_SHIFT + 2) - 1);\r\nreturn (int *)ptr;\r\n}\r\nstatic int is_atomic_lock(int *p)\r\n{\r\nreturn p >= &atomic_locks[0] && p < &atomic_locks[ATOMIC_HASH_SIZE];\r\n}\r\nvoid __atomic_fault_unlock(int *irqlock_word)\r\n{\r\nBUG_ON(!is_atomic_lock(irqlock_word));\r\nBUG_ON(*irqlock_word != 1);\r\n*irqlock_word = 0;\r\n}\r\nstatic inline int *__atomic_setup(volatile void *v)\r\n{\r\n*(volatile int *)v;\r\nreturn __atomic_hashed_lock(v);\r\n}\r\nint _atomic_xchg(int *v, int n)\r\n{\r\nreturn __atomic_xchg(v, __atomic_setup(v), n).val;\r\n}\r\nint _atomic_xchg_add(int *v, int i)\r\n{\r\nreturn __atomic_xchg_add(v, __atomic_setup(v), i).val;\r\n}\r\nint _atomic_xchg_add_unless(int *v, int a, int u)\r\n{\r\nreturn __atomic_xchg_add_unless(v, __atomic_setup(v), u, a).val;\r\n}\r\nint _atomic_cmpxchg(int *v, int o, int n)\r\n{\r\nreturn __atomic_cmpxchg(v, __atomic_setup(v), o, n).val;\r\n}\r\nunsigned long _atomic_or(volatile unsigned long *p, unsigned long mask)\r\n{\r\nreturn __atomic_or((int *)p, __atomic_setup(p), mask).val;\r\n}\r\nunsigned long _atomic_and(volatile unsigned long *p, unsigned long mask)\r\n{\r\nreturn __atomic_and((int *)p, __atomic_setup(p), mask).val;\r\n}\r\nunsigned long _atomic_andn(volatile unsigned long *p, unsigned long mask)\r\n{\r\nreturn __atomic_andn((int *)p, __atomic_setup(p), mask).val;\r\n}\r\nunsigned long _atomic_xor(volatile unsigned long *p, unsigned long mask)\r\n{\r\nreturn __atomic_xor((int *)p, __atomic_setup(p), mask).val;\r\n}\r\nlong long _atomic64_xchg(long long *v, long long n)\r\n{\r\nreturn __atomic64_xchg(v, __atomic_setup(v), n);\r\n}\r\nlong long _atomic64_xchg_add(long long *v, long long i)\r\n{\r\nreturn __atomic64_xchg_add(v, __atomic_setup(v), i);\r\n}\r\nlong long _atomic64_xchg_add_unless(long long *v, long long a, long long u)\r\n{\r\nreturn __atomic64_xchg_add_unless(v, __atomic_setup(v), u, a);\r\n}\r\nlong long _atomic64_cmpxchg(long long *v, long long o, long long n)\r\n{\r\nreturn __atomic64_cmpxchg(v, __atomic_setup(v), o, n);\r\n}\r\nlong long _atomic64_and(long long *v, long long n)\r\n{\r\nreturn __atomic64_and(v, __atomic_setup(v), n);\r\n}\r\nlong long _atomic64_or(long long *v, long long n)\r\n{\r\nreturn __atomic64_or(v, __atomic_setup(v), n);\r\n}\r\nlong long _atomic64_xor(long long *v, long long n)\r\n{\r\nreturn __atomic64_xor(v, __atomic_setup(v), n);\r\n}\r\nstruct __get_user __atomic_bad_address(int __user *addr)\r\n{\r\nif (unlikely(!access_ok(VERIFY_WRITE, addr, sizeof(int))))\r\npanic("Bad address used for kernel atomic op: %p\n", addr);\r\nreturn (struct __get_user) { .err = -EFAULT };\r\n}\r\nvoid __init __init_atomic_per_cpu(void)\r\n{\r\nBUILD_BUG_ON(ATOMIC_HASH_SIZE & (ATOMIC_HASH_SIZE-1));\r\nBUG_ON(ATOMIC_HASH_SIZE < nr_cpu_ids);\r\nBUG_ON((unsigned long)atomic_locks % PAGE_SIZE != 0);\r\nBUILD_BUG_ON(ATOMIC_HASH_SIZE * sizeof(int) > PAGE_SIZE);\r\nBUILD_BUG_ON((PAGE_SIZE >> 3) > ATOMIC_HASH_SIZE);\r\n}
