static bool check_config(struct knav_dma_chan *chan, struct knav_dma_cfg *cfg)\r\n{\r\nif (!memcmp(&chan->cfg, cfg, sizeof(*cfg)))\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic int chan_start(struct knav_dma_chan *chan,\r\nstruct knav_dma_cfg *cfg)\r\n{\r\nu32 v = 0;\r\nspin_lock(&chan->lock);\r\nif ((chan->direction == DMA_MEM_TO_DEV) && chan->reg_chan) {\r\nif (cfg->u.tx.filt_pswords)\r\nv |= DMA_TX_FILT_PSWORDS;\r\nif (cfg->u.tx.filt_einfo)\r\nv |= DMA_TX_FILT_EINFO;\r\nwritel_relaxed(v, &chan->reg_chan->mode);\r\nwritel_relaxed(DMA_ENABLE, &chan->reg_chan->control);\r\n}\r\nif (chan->reg_tx_sched)\r\nwritel_relaxed(cfg->u.tx.priority, &chan->reg_tx_sched->prio);\r\nif (chan->reg_rx_flow) {\r\nv = 0;\r\nif (cfg->u.rx.einfo_present)\r\nv |= CHAN_HAS_EPIB;\r\nif (cfg->u.rx.psinfo_present)\r\nv |= CHAN_HAS_PSINFO;\r\nif (cfg->u.rx.err_mode == DMA_RETRY)\r\nv |= CHAN_ERR_RETRY;\r\nv |= (cfg->u.rx.desc_type & DESC_TYPE_MASK) << DESC_TYPE_SHIFT;\r\nif (cfg->u.rx.psinfo_at_sop)\r\nv |= CHAN_PSINFO_AT_SOP;\r\nv |= (cfg->u.rx.sop_offset & CHAN_SOP_OFF_MASK)\r\n<< CHAN_SOP_OFF_SHIFT;\r\nv |= cfg->u.rx.dst_q & CHAN_QNUM_MASK;\r\nwritel_relaxed(v, &chan->reg_rx_flow->control);\r\nwritel_relaxed(0, &chan->reg_rx_flow->tags);\r\nwritel_relaxed(0, &chan->reg_rx_flow->tag_sel);\r\nv = cfg->u.rx.fdq[0] << 16;\r\nv |= cfg->u.rx.fdq[1] & CHAN_QNUM_MASK;\r\nwritel_relaxed(v, &chan->reg_rx_flow->fdq_sel[0]);\r\nv = cfg->u.rx.fdq[2] << 16;\r\nv |= cfg->u.rx.fdq[3] & CHAN_QNUM_MASK;\r\nwritel_relaxed(v, &chan->reg_rx_flow->fdq_sel[1]);\r\nwritel_relaxed(0, &chan->reg_rx_flow->thresh[0]);\r\nwritel_relaxed(0, &chan->reg_rx_flow->thresh[1]);\r\nwritel_relaxed(0, &chan->reg_rx_flow->thresh[2]);\r\n}\r\nmemcpy(&chan->cfg, cfg, sizeof(*cfg));\r\nspin_unlock(&chan->lock);\r\nreturn 0;\r\n}\r\nstatic int chan_teardown(struct knav_dma_chan *chan)\r\n{\r\nunsigned long end, value;\r\nif (!chan->reg_chan)\r\nreturn 0;\r\nwritel_relaxed(DMA_TEARDOWN, &chan->reg_chan->control);\r\nend = jiffies + msecs_to_jiffies(DMA_TIMEOUT);\r\ndo {\r\nvalue = readl_relaxed(&chan->reg_chan->control);\r\nif ((value & DMA_ENABLE) == 0)\r\nbreak;\r\n} while (time_after(end, jiffies));\r\nif (readl_relaxed(&chan->reg_chan->control) & DMA_ENABLE) {\r\ndev_err(kdev->dev, "timeout waiting for teardown\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nreturn 0;\r\n}\r\nstatic void chan_stop(struct knav_dma_chan *chan)\r\n{\r\nspin_lock(&chan->lock);\r\nif (chan->reg_rx_flow) {\r\nwritel_relaxed(0, &chan->reg_rx_flow->fdq_sel[0]);\r\nwritel_relaxed(0, &chan->reg_rx_flow->fdq_sel[1]);\r\nwritel_relaxed(0, &chan->reg_rx_flow->thresh[0]);\r\nwritel_relaxed(0, &chan->reg_rx_flow->thresh[1]);\r\nwritel_relaxed(0, &chan->reg_rx_flow->thresh[2]);\r\n}\r\nchan_teardown(chan);\r\nif (chan->reg_rx_flow) {\r\nwritel_relaxed(0, &chan->reg_rx_flow->control);\r\nwritel_relaxed(0, &chan->reg_rx_flow->tags);\r\nwritel_relaxed(0, &chan->reg_rx_flow->tag_sel);\r\n}\r\nmemset(&chan->cfg, 0, sizeof(struct knav_dma_cfg));\r\nspin_unlock(&chan->lock);\r\ndev_dbg(kdev->dev, "channel stopped\n");\r\n}\r\nstatic void dma_hw_enable_all(struct knav_dma_device *dma)\r\n{\r\nint i;\r\nfor (i = 0; i < dma->max_tx_chan; i++) {\r\nwritel_relaxed(0, &dma->reg_tx_chan[i].mode);\r\nwritel_relaxed(DMA_ENABLE, &dma->reg_tx_chan[i].control);\r\n}\r\n}\r\nstatic void knav_dma_hw_init(struct knav_dma_device *dma)\r\n{\r\nunsigned v;\r\nint i;\r\nspin_lock(&dma->lock);\r\nv = dma->loopback ? DMA_LOOPBACK : 0;\r\nwritel_relaxed(v, &dma->reg_global->emulation_control);\r\nv = readl_relaxed(&dma->reg_global->perf_control);\r\nv |= ((dma->rx_timeout & DMA_RX_TIMEOUT_MASK) << DMA_RX_TIMEOUT_SHIFT);\r\nwritel_relaxed(v, &dma->reg_global->perf_control);\r\nv = ((dma->tx_priority << DMA_TX_PRIO_SHIFT) |\r\n(dma->rx_priority << DMA_RX_PRIO_SHIFT));\r\nwritel_relaxed(v, &dma->reg_global->priority_control);\r\nfor (i = 0; i < dma->max_rx_chan; i++)\r\nwritel_relaxed(DMA_ENABLE, &dma->reg_rx_chan[i].control);\r\nfor (i = 0; i < dma->logical_queue_managers; i++)\r\nwritel_relaxed(dma->qm_base_address[i],\r\n&dma->reg_global->qm_base_address[i]);\r\nspin_unlock(&dma->lock);\r\n}\r\nstatic void knav_dma_hw_destroy(struct knav_dma_device *dma)\r\n{\r\nint i;\r\nunsigned v;\r\nspin_lock(&dma->lock);\r\nv = ~DMA_ENABLE & REG_MASK;\r\nfor (i = 0; i < dma->max_rx_chan; i++)\r\nwritel_relaxed(v, &dma->reg_rx_chan[i].control);\r\nfor (i = 0; i < dma->max_tx_chan; i++)\r\nwritel_relaxed(v, &dma->reg_tx_chan[i].control);\r\nspin_unlock(&dma->lock);\r\n}\r\nstatic void dma_debug_show_channels(struct seq_file *s,\r\nstruct knav_dma_chan *chan)\r\n{\r\nint i;\r\nseq_printf(s, "\t%s %d:\t",\r\n((chan->direction == DMA_MEM_TO_DEV) ? "tx chan" : "rx flow"),\r\nchan_number(chan));\r\nif (chan->direction == DMA_MEM_TO_DEV) {\r\nseq_printf(s, "einfo - %d, pswords - %d, priority - %d\n",\r\nchan->cfg.u.tx.filt_einfo,\r\nchan->cfg.u.tx.filt_pswords,\r\nchan->cfg.u.tx.priority);\r\n} else {\r\nseq_printf(s, "einfo - %d, psinfo - %d, desc_type - %d\n",\r\nchan->cfg.u.rx.einfo_present,\r\nchan->cfg.u.rx.psinfo_present,\r\nchan->cfg.u.rx.desc_type);\r\nseq_printf(s, "\t\t\tdst_q: [%d], thresh: %d fdq: ",\r\nchan->cfg.u.rx.dst_q,\r\nchan->cfg.u.rx.thresh);\r\nfor (i = 0; i < KNAV_DMA_FDQ_PER_CHAN; i++)\r\nseq_printf(s, "[%d]", chan->cfg.u.rx.fdq[i]);\r\nseq_printf(s, "\n");\r\n}\r\n}\r\nstatic void dma_debug_show_devices(struct seq_file *s,\r\nstruct knav_dma_device *dma)\r\n{\r\nstruct knav_dma_chan *chan;\r\nlist_for_each_entry(chan, &dma->chan_list, list) {\r\nif (atomic_read(&chan->ref_count))\r\ndma_debug_show_channels(s, chan);\r\n}\r\n}\r\nstatic int dma_debug_show(struct seq_file *s, void *v)\r\n{\r\nstruct knav_dma_device *dma;\r\nlist_for_each_entry(dma, &kdev->list, list) {\r\nif (atomic_read(&dma->ref_count)) {\r\nseq_printf(s, "%s : max_tx_chan: (%d), max_rx_flows: (%d)\n",\r\ndma->name, dma->max_tx_chan, dma->max_rx_flow);\r\ndma_debug_show_devices(s, dma);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_dma_debug_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, dma_debug_show, NULL);\r\n}\r\nstatic int of_channel_match_helper(struct device_node *np, const char *name,\r\nconst char **dma_instance)\r\n{\r\nstruct of_phandle_args args;\r\nstruct device_node *dma_node;\r\nint index;\r\ndma_node = of_parse_phandle(np, "ti,navigator-dmas", 0);\r\nif (!dma_node)\r\nreturn -ENODEV;\r\n*dma_instance = dma_node->name;\r\nindex = of_property_match_string(np, "ti,navigator-dma-names", name);\r\nif (index < 0) {\r\ndev_err(kdev->dev, "No 'ti,navigator-dma-names' property\n");\r\nreturn -ENODEV;\r\n}\r\nif (of_parse_phandle_with_fixed_args(np, "ti,navigator-dmas",\r\n1, index, &args)) {\r\ndev_err(kdev->dev, "Missing the pahndle args name %s\n", name);\r\nreturn -ENODEV;\r\n}\r\nif (args.args[0] < 0) {\r\ndev_err(kdev->dev, "Missing args for %s\n", name);\r\nreturn -ENODEV;\r\n}\r\nreturn args.args[0];\r\n}\r\nvoid *knav_dma_open_channel(struct device *dev, const char *name,\r\nstruct knav_dma_cfg *config)\r\n{\r\nstruct knav_dma_chan *chan;\r\nstruct knav_dma_device *dma;\r\nbool found = false;\r\nint chan_num = -1;\r\nconst char *instance;\r\nif (!kdev) {\r\npr_err("keystone-navigator-dma driver not registered\n");\r\nreturn (void *)-EINVAL;\r\n}\r\nchan_num = of_channel_match_helper(dev->of_node, name, &instance);\r\nif (chan_num < 0) {\r\ndev_err(kdev->dev, "No DMA instace with name %s\n", name);\r\nreturn (void *)-EINVAL;\r\n}\r\ndev_dbg(kdev->dev, "initializing %s channel %d from DMA %s\n",\r\nconfig->direction == DMA_MEM_TO_DEV ? "transmit" :\r\nconfig->direction == DMA_DEV_TO_MEM ? "receive" :\r\n"unknown", chan_num, instance);\r\nif (config->direction != DMA_MEM_TO_DEV &&\r\nconfig->direction != DMA_DEV_TO_MEM) {\r\ndev_err(kdev->dev, "bad direction\n");\r\nreturn (void *)-EINVAL;\r\n}\r\nlist_for_each_entry(dma, &kdev->list, list) {\r\nif (!strcmp(dma->name, instance)) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\ndev_err(kdev->dev, "No DMA instace with name %s\n", instance);\r\nreturn (void *)-EINVAL;\r\n}\r\nfound = false;\r\nlist_for_each_entry(chan, &dma->chan_list, list) {\r\nif (config->direction == DMA_MEM_TO_DEV) {\r\nif (chan->channel == chan_num) {\r\nfound = true;\r\nbreak;\r\n}\r\n} else {\r\nif (chan->flow == chan_num) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (!found) {\r\ndev_err(kdev->dev, "channel %d is not in DMA %s\n",\r\nchan_num, instance);\r\nreturn (void *)-EINVAL;\r\n}\r\nif (atomic_read(&chan->ref_count) >= 1) {\r\nif (!check_config(chan, config)) {\r\ndev_err(kdev->dev, "channel %d config miss-match\n",\r\nchan_num);\r\nreturn (void *)-EINVAL;\r\n}\r\n}\r\nif (atomic_inc_return(&chan->dma->ref_count) <= 1)\r\nknav_dma_hw_init(chan->dma);\r\nif (atomic_inc_return(&chan->ref_count) <= 1)\r\nchan_start(chan, config);\r\ndev_dbg(kdev->dev, "channel %d opened from DMA %s\n",\r\nchan_num, instance);\r\nreturn chan;\r\n}\r\nvoid knav_dma_close_channel(void *channel)\r\n{\r\nstruct knav_dma_chan *chan = channel;\r\nif (!kdev) {\r\npr_err("keystone-navigator-dma driver not registered\n");\r\nreturn;\r\n}\r\nif (atomic_dec_return(&chan->ref_count) <= 0)\r\nchan_stop(chan);\r\nif (atomic_dec_return(&chan->dma->ref_count) <= 0)\r\nknav_dma_hw_destroy(chan->dma);\r\ndev_dbg(kdev->dev, "channel %d or flow %d closed from DMA %s\n",\r\nchan->channel, chan->flow, chan->dma->name);\r\n}\r\nstatic void __iomem *pktdma_get_regs(struct knav_dma_device *dma,\r\nstruct device_node *node,\r\nunsigned index, resource_size_t *_size)\r\n{\r\nstruct device *dev = kdev->dev;\r\nstruct resource res;\r\nvoid __iomem *regs;\r\nint ret;\r\nret = of_address_to_resource(node, index, &res);\r\nif (ret) {\r\ndev_err(dev, "Can't translate of node(%s) address for index(%d)\n",\r\nnode->name, index);\r\nreturn ERR_PTR(ret);\r\n}\r\nregs = devm_ioremap_resource(kdev->dev, &res);\r\nif (IS_ERR(regs))\r\ndev_err(dev, "Failed to map register base for index(%d) node(%s)\n",\r\nindex, node->name);\r\nif (_size)\r\n*_size = resource_size(&res);\r\nreturn regs;\r\n}\r\nstatic int pktdma_init_rx_chan(struct knav_dma_chan *chan, u32 flow)\r\n{\r\nstruct knav_dma_device *dma = chan->dma;\r\nchan->flow = flow;\r\nchan->reg_rx_flow = dma->reg_rx_flow + flow;\r\nchan->channel = DMA_INVALID_ID;\r\ndev_dbg(kdev->dev, "rx flow(%d) (%p)\n", chan->flow, chan->reg_rx_flow);\r\nreturn 0;\r\n}\r\nstatic int pktdma_init_tx_chan(struct knav_dma_chan *chan, u32 channel)\r\n{\r\nstruct knav_dma_device *dma = chan->dma;\r\nchan->channel = channel;\r\nchan->reg_chan = dma->reg_tx_chan + channel;\r\nchan->reg_tx_sched = dma->reg_tx_sched + channel;\r\nchan->flow = DMA_INVALID_ID;\r\ndev_dbg(kdev->dev, "tx channel(%d) (%p)\n", chan->channel, chan->reg_chan);\r\nreturn 0;\r\n}\r\nstatic int pktdma_init_chan(struct knav_dma_device *dma,\r\nenum dma_transfer_direction dir,\r\nunsigned chan_num)\r\n{\r\nstruct device *dev = kdev->dev;\r\nstruct knav_dma_chan *chan;\r\nint ret = -EINVAL;\r\nchan = devm_kzalloc(dev, sizeof(*chan), GFP_KERNEL);\r\nif (!chan)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&chan->list);\r\nchan->dma = dma;\r\nchan->direction = DMA_NONE;\r\natomic_set(&chan->ref_count, 0);\r\nspin_lock_init(&chan->lock);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nchan->direction = dir;\r\nret = pktdma_init_tx_chan(chan, chan_num);\r\n} else if (dir == DMA_DEV_TO_MEM) {\r\nchan->direction = dir;\r\nret = pktdma_init_rx_chan(chan, chan_num);\r\n} else {\r\ndev_err(dev, "channel(%d) direction unknown\n", chan_num);\r\n}\r\nlist_add_tail(&chan->list, &dma->chan_list);\r\nreturn ret;\r\n}\r\nstatic int dma_init(struct device_node *cloud, struct device_node *dma_node)\r\n{\r\nunsigned max_tx_chan, max_rx_chan, max_rx_flow, max_tx_sched;\r\nstruct device_node *node = dma_node;\r\nstruct knav_dma_device *dma;\r\nint ret, len, num_chan = 0;\r\nresource_size_t size;\r\nu32 timeout;\r\nu32 i;\r\ndma = devm_kzalloc(kdev->dev, sizeof(*dma), GFP_KERNEL);\r\nif (!dma) {\r\ndev_err(kdev->dev, "could not allocate driver mem\n");\r\nreturn -ENOMEM;\r\n}\r\nINIT_LIST_HEAD(&dma->list);\r\nINIT_LIST_HEAD(&dma->chan_list);\r\nif (!of_find_property(cloud, "ti,navigator-cloud-address", &len)) {\r\ndev_err(kdev->dev, "unspecified navigator cloud addresses\n");\r\nreturn -ENODEV;\r\n}\r\ndma->logical_queue_managers = len / sizeof(u32);\r\nif (dma->logical_queue_managers > DMA_MAX_QMS) {\r\ndev_warn(kdev->dev, "too many queue mgrs(>%d) rest ignored\n",\r\ndma->logical_queue_managers);\r\ndma->logical_queue_managers = DMA_MAX_QMS;\r\n}\r\nret = of_property_read_u32_array(cloud, "ti,navigator-cloud-address",\r\ndma->qm_base_address,\r\ndma->logical_queue_managers);\r\nif (ret) {\r\ndev_err(kdev->dev, "invalid navigator cloud addresses\n");\r\nreturn -ENODEV;\r\n}\r\ndma->reg_global = pktdma_get_regs(dma, node, 0, &size);\r\nif (!dma->reg_global)\r\nreturn -ENODEV;\r\nif (size < sizeof(struct reg_global)) {\r\ndev_err(kdev->dev, "bad size %pa for global regs\n", &size);\r\nreturn -ENODEV;\r\n}\r\ndma->reg_tx_chan = pktdma_get_regs(dma, node, 1, &size);\r\nif (!dma->reg_tx_chan)\r\nreturn -ENODEV;\r\nmax_tx_chan = size / sizeof(struct reg_chan);\r\ndma->reg_rx_chan = pktdma_get_regs(dma, node, 2, &size);\r\nif (!dma->reg_rx_chan)\r\nreturn -ENODEV;\r\nmax_rx_chan = size / sizeof(struct reg_chan);\r\ndma->reg_tx_sched = pktdma_get_regs(dma, node, 3, &size);\r\nif (!dma->reg_tx_sched)\r\nreturn -ENODEV;\r\nmax_tx_sched = size / sizeof(struct reg_tx_sched);\r\ndma->reg_rx_flow = pktdma_get_regs(dma, node, 4, &size);\r\nif (!dma->reg_rx_flow)\r\nreturn -ENODEV;\r\nmax_rx_flow = size / sizeof(struct reg_rx_flow);\r\ndma->rx_priority = DMA_PRIO_DEFAULT;\r\ndma->tx_priority = DMA_PRIO_DEFAULT;\r\ndma->enable_all = (of_get_property(node, "ti,enable-all", NULL) != NULL);\r\ndma->loopback = (of_get_property(node, "ti,loop-back", NULL) != NULL);\r\nret = of_property_read_u32(node, "ti,rx-retry-timeout", &timeout);\r\nif (ret < 0) {\r\ndev_dbg(kdev->dev, "unspecified rx timeout using value %d\n",\r\nDMA_RX_TIMEOUT_DEFAULT);\r\ntimeout = DMA_RX_TIMEOUT_DEFAULT;\r\n}\r\ndma->rx_timeout = timeout;\r\ndma->max_rx_chan = max_rx_chan;\r\ndma->max_rx_flow = max_rx_flow;\r\ndma->max_tx_chan = min(max_tx_chan, max_tx_sched);\r\natomic_set(&dma->ref_count, 0);\r\nstrcpy(dma->name, node->name);\r\nspin_lock_init(&dma->lock);\r\nfor (i = 0; i < dma->max_tx_chan; i++) {\r\nif (pktdma_init_chan(dma, DMA_MEM_TO_DEV, i) >= 0)\r\nnum_chan++;\r\n}\r\nfor (i = 0; i < dma->max_rx_flow; i++) {\r\nif (pktdma_init_chan(dma, DMA_DEV_TO_MEM, i) >= 0)\r\nnum_chan++;\r\n}\r\nlist_add_tail(&dma->list, &kdev->list);\r\nif (dma->enable_all) {\r\natomic_inc(&dma->ref_count);\r\nknav_dma_hw_init(dma);\r\ndma_hw_enable_all(dma);\r\n}\r\ndev_info(kdev->dev, "DMA %s registered %d logical channels, flows %d, tx chans: %d, rx chans: %d%s\n",\r\ndma->name, num_chan, dma->max_rx_flow,\r\ndma->max_tx_chan, dma->max_rx_chan,\r\ndma->loopback ? ", loopback" : "");\r\nreturn 0;\r\n}\r\nstatic int knav_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct device_node *node = pdev->dev.of_node;\r\nstruct device_node *child;\r\nint ret = 0;\r\nif (!node) {\r\ndev_err(&pdev->dev, "could not find device info\n");\r\nreturn -EINVAL;\r\n}\r\nkdev = devm_kzalloc(dev,\r\nsizeof(struct knav_dma_pool_device), GFP_KERNEL);\r\nif (!kdev) {\r\ndev_err(dev, "could not allocate driver mem\n");\r\nreturn -ENOMEM;\r\n}\r\nkdev->dev = dev;\r\nINIT_LIST_HEAD(&kdev->list);\r\npm_runtime_enable(kdev->dev);\r\nret = pm_runtime_get_sync(kdev->dev);\r\nif (ret < 0) {\r\ndev_err(kdev->dev, "unable to enable pktdma, err %d\n", ret);\r\nreturn ret;\r\n}\r\nfor_each_child_of_node(node, child) {\r\nret = dma_init(node, child);\r\nif (ret) {\r\ndev_err(&pdev->dev, "init failed with %d\n", ret);\r\nbreak;\r\n}\r\n}\r\nif (list_empty(&kdev->list)) {\r\ndev_err(dev, "no valid dma instance\n");\r\nreturn -ENODEV;\r\n}\r\ndebugfs_create_file("knav_dma", S_IFREG | S_IRUGO, NULL, NULL,\r\n&knav_dma_debug_ops);\r\nreturn ret;\r\n}\r\nstatic int knav_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct knav_dma_device *dma;\r\nlist_for_each_entry(dma, &kdev->list, list) {\r\nif (atomic_dec_return(&dma->ref_count) == 0)\r\nknav_dma_hw_destroy(dma);\r\n}\r\npm_runtime_put_sync(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nreturn 0;\r\n}
