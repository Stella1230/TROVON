int blk_rq_append_bio(struct request_queue *q, struct request *rq,\r\nstruct bio *bio)\r\n{\r\nif (!rq->bio)\r\nblk_rq_bio_prep(q, rq, bio);\r\nelse if (!ll_back_merge_fn(q, rq, bio))\r\nreturn -EINVAL;\r\nelse {\r\nrq->biotail->bi_next = bio;\r\nrq->biotail = bio;\r\nrq->__data_len += bio->bi_iter.bi_size;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __blk_rq_unmap_user(struct bio *bio)\r\n{\r\nint ret = 0;\r\nif (bio) {\r\nif (bio_flagged(bio, BIO_USER_MAPPED))\r\nbio_unmap_user(bio);\r\nelse\r\nret = bio_uncopy_user(bio);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __blk_rq_map_user_iov(struct request *rq,\r\nstruct rq_map_data *map_data, struct iov_iter *iter,\r\ngfp_t gfp_mask, bool copy)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct bio *bio, *orig_bio;\r\nint ret;\r\nif (copy)\r\nbio = bio_copy_user_iov(q, map_data, iter, gfp_mask);\r\nelse\r\nbio = bio_map_user_iov(q, iter, gfp_mask);\r\nif (IS_ERR(bio))\r\nreturn PTR_ERR(bio);\r\nif (map_data && map_data->null_mapped)\r\nbio_set_flag(bio, BIO_NULL_MAPPED);\r\niov_iter_advance(iter, bio->bi_iter.bi_size);\r\nif (map_data)\r\nmap_data->offset += bio->bi_iter.bi_size;\r\norig_bio = bio;\r\nblk_queue_bounce(q, &bio);\r\nbio_get(bio);\r\nret = blk_rq_append_bio(q, rq, bio);\r\nif (ret) {\r\nbio_endio(bio);\r\n__blk_rq_unmap_user(orig_bio);\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\r\nstruct rq_map_data *map_data,\r\nconst struct iov_iter *iter, gfp_t gfp_mask)\r\n{\r\nbool copy = false;\r\nunsigned long align = q->dma_pad_mask | queue_dma_alignment(q);\r\nstruct bio *bio = NULL;\r\nstruct iov_iter i;\r\nint ret;\r\nif (map_data)\r\ncopy = true;\r\nelse if (iov_iter_alignment(iter) & align)\r\ncopy = true;\r\nelse if (queue_virt_boundary(q))\r\ncopy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);\r\ni = *iter;\r\ndo {\r\nret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);\r\nif (ret)\r\ngoto unmap_rq;\r\nif (!bio)\r\nbio = rq->bio;\r\n} while (iov_iter_count(&i));\r\nif (!bio_flagged(bio, BIO_USER_MAPPED))\r\nrq->cmd_flags |= REQ_COPY_USER;\r\nreturn 0;\r\nunmap_rq:\r\n__blk_rq_unmap_user(bio);\r\nrq->bio = NULL;\r\nreturn -EINVAL;\r\n}\r\nint blk_rq_map_user(struct request_queue *q, struct request *rq,\r\nstruct rq_map_data *map_data, void __user *ubuf,\r\nunsigned long len, gfp_t gfp_mask)\r\n{\r\nstruct iovec iov;\r\nstruct iov_iter i;\r\nint ret = import_single_range(rq_data_dir(rq), ubuf, len, &iov, &i);\r\nif (unlikely(ret < 0))\r\nreturn ret;\r\nreturn blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);\r\n}\r\nint blk_rq_unmap_user(struct bio *bio)\r\n{\r\nstruct bio *mapped_bio;\r\nint ret = 0, ret2;\r\nwhile (bio) {\r\nmapped_bio = bio;\r\nif (unlikely(bio_flagged(bio, BIO_BOUNCED)))\r\nmapped_bio = bio->bi_private;\r\nret2 = __blk_rq_unmap_user(mapped_bio);\r\nif (ret2 && !ret)\r\nret = ret2;\r\nmapped_bio = bio;\r\nbio = bio->bi_next;\r\nbio_put(mapped_bio);\r\n}\r\nreturn ret;\r\n}\r\nint blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,\r\nunsigned int len, gfp_t gfp_mask)\r\n{\r\nint reading = rq_data_dir(rq) == READ;\r\nunsigned long addr = (unsigned long) kbuf;\r\nint do_copy = 0;\r\nstruct bio *bio;\r\nint ret;\r\nif (len > (queue_max_hw_sectors(q) << 9))\r\nreturn -EINVAL;\r\nif (!len || !kbuf)\r\nreturn -EINVAL;\r\ndo_copy = !blk_rq_aligned(q, addr, len) || object_is_on_stack(kbuf);\r\nif (do_copy)\r\nbio = bio_copy_kern(q, kbuf, len, gfp_mask, reading);\r\nelse\r\nbio = bio_map_kern(q, kbuf, len, gfp_mask);\r\nif (IS_ERR(bio))\r\nreturn PTR_ERR(bio);\r\nif (!reading)\r\nbio->bi_rw |= REQ_WRITE;\r\nif (do_copy)\r\nrq->cmd_flags |= REQ_COPY_USER;\r\nret = blk_rq_append_bio(q, rq, bio);\r\nif (unlikely(ret)) {\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nblk_queue_bounce(q, &rq->bio);\r\nreturn 0;\r\n}
