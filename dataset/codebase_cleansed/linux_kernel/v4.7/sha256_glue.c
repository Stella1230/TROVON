int crypto_sha256_arm_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nBUILD_BUG_ON(offsetof(struct sha256_state, state) != 0);\r\nreturn sha256_base_do_update(desc, data, len,\r\n(sha256_block_fn *)sha256_block_data_order);\r\n}\r\nstatic int sha256_final(struct shash_desc *desc, u8 *out)\r\n{\r\nsha256_base_do_finalize(desc,\r\n(sha256_block_fn *)sha256_block_data_order);\r\nreturn sha256_base_finish(desc, out);\r\n}\r\nint crypto_sha256_arm_finup(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, u8 *out)\r\n{\r\nsha256_base_do_update(desc, data, len,\r\n(sha256_block_fn *)sha256_block_data_order);\r\nreturn sha256_final(desc, out);\r\n}\r\nstatic int __init sha256_mod_init(void)\r\n{\r\nint res = crypto_register_shashes(algs, ARRAY_SIZE(algs));\r\nif (res < 0)\r\nreturn res;\r\nif (IS_ENABLED(CONFIG_KERNEL_MODE_NEON) && cpu_has_neon()) {\r\nres = crypto_register_shashes(sha256_neon_algs,\r\nARRAY_SIZE(sha256_neon_algs));\r\nif (res < 0)\r\ncrypto_unregister_shashes(algs, ARRAY_SIZE(algs));\r\n}\r\nreturn res;\r\n}\r\nstatic void __exit sha256_mod_fini(void)\r\n{\r\ncrypto_unregister_shashes(algs, ARRAY_SIZE(algs));\r\nif (IS_ENABLED(CONFIG_KERNEL_MODE_NEON) && cpu_has_neon())\r\ncrypto_unregister_shashes(sha256_neon_algs,\r\nARRAY_SIZE(sha256_neon_algs));\r\n}
