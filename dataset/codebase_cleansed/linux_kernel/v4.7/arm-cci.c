static ssize_t cci400_pmu_cycle_event_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct dev_ext_attribute *eattr = container_of(attr,\r\nstruct dev_ext_attribute, attr);\r\nreturn snprintf(buf, PAGE_SIZE, "config=0x%lx\n", (unsigned long)eattr->var);\r\n}\r\nstatic int cci400_get_event_idx(struct cci_pmu *cci_pmu,\r\nstruct cci_pmu_hw_events *hw,\r\nunsigned long cci_event)\r\n{\r\nint idx;\r\nif (cci_event == CCI400_PMU_CYCLES) {\r\nif (test_and_set_bit(CCI400_PMU_CYCLE_CNTR_IDX, hw->used_mask))\r\nreturn -EAGAIN;\r\nreturn CCI400_PMU_CYCLE_CNTR_IDX;\r\n}\r\nfor (idx = CCI400_PMU_CNTR0_IDX; idx <= CCI_PMU_CNTR_LAST(cci_pmu); ++idx)\r\nif (!test_and_set_bit(idx, hw->used_mask))\r\nreturn idx;\r\nreturn -EAGAIN;\r\n}\r\nstatic int cci400_validate_hw_event(struct cci_pmu *cci_pmu, unsigned long hw_event)\r\n{\r\nu8 ev_source = CCI400_PMU_EVENT_SOURCE(hw_event);\r\nu8 ev_code = CCI400_PMU_EVENT_CODE(hw_event);\r\nint if_type;\r\nif (hw_event & ~CCI400_PMU_EVENT_MASK)\r\nreturn -ENOENT;\r\nif (hw_event == CCI400_PMU_CYCLES)\r\nreturn hw_event;\r\nswitch (ev_source) {\r\ncase CCI400_PORT_S0:\r\ncase CCI400_PORT_S1:\r\ncase CCI400_PORT_S2:\r\ncase CCI400_PORT_S3:\r\ncase CCI400_PORT_S4:\r\nif_type = CCI_IF_SLAVE;\r\nbreak;\r\ncase CCI400_PORT_M0:\r\ncase CCI400_PORT_M1:\r\ncase CCI400_PORT_M2:\r\nif_type = CCI_IF_MASTER;\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nif (ev_code >= cci_pmu->model->event_ranges[if_type].min &&\r\nev_code <= cci_pmu->model->event_ranges[if_type].max)\r\nreturn hw_event;\r\nreturn -ENOENT;\r\n}\r\nstatic int probe_cci400_revision(void)\r\n{\r\nint rev;\r\nrev = readl_relaxed(cci_ctrl_base + CCI_PID2) & CCI_PID2_REV_MASK;\r\nrev >>= CCI_PID2_REV_SHIFT;\r\nif (rev < CCI400_R1_PX)\r\nreturn CCI400_R0;\r\nelse\r\nreturn CCI400_R1;\r\n}\r\nstatic const struct cci_pmu_model *probe_cci_model(struct platform_device *pdev)\r\n{\r\nif (platform_has_secure_cci_access())\r\nreturn &cci_pmu_models[probe_cci400_revision()];\r\nreturn NULL;\r\n}\r\nstatic inline struct cci_pmu_model *probe_cci_model(struct platform_device *pdev)\r\n{\r\nreturn NULL;\r\n}\r\nstatic ssize_t cci5xx_pmu_global_event_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct dev_ext_attribute *eattr = container_of(attr,\r\nstruct dev_ext_attribute, attr);\r\nreturn snprintf(buf, PAGE_SIZE, "event=0x%lx,source=0x%x\n",\r\n(unsigned long)eattr->var, CCI5xx_PORT_GLOBAL);\r\n}\r\nstatic int cci500_validate_hw_event(struct cci_pmu *cci_pmu,\r\nunsigned long hw_event)\r\n{\r\nu32 ev_source = CCI5xx_PMU_EVENT_SOURCE(hw_event);\r\nu32 ev_code = CCI5xx_PMU_EVENT_CODE(hw_event);\r\nint if_type;\r\nif (hw_event & ~CCI5xx_PMU_EVENT_MASK)\r\nreturn -ENOENT;\r\nswitch (ev_source) {\r\ncase CCI5xx_PORT_S0:\r\ncase CCI5xx_PORT_S1:\r\ncase CCI5xx_PORT_S2:\r\ncase CCI5xx_PORT_S3:\r\ncase CCI5xx_PORT_S4:\r\ncase CCI5xx_PORT_S5:\r\ncase CCI5xx_PORT_S6:\r\nif_type = CCI_IF_SLAVE;\r\nbreak;\r\ncase CCI5xx_PORT_M0:\r\ncase CCI5xx_PORT_M1:\r\ncase CCI5xx_PORT_M2:\r\ncase CCI5xx_PORT_M3:\r\ncase CCI5xx_PORT_M4:\r\ncase CCI5xx_PORT_M5:\r\nif_type = CCI_IF_MASTER;\r\nbreak;\r\ncase CCI5xx_PORT_GLOBAL:\r\nif_type = CCI_IF_GLOBAL;\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nif (ev_code >= cci_pmu->model->event_ranges[if_type].min &&\r\nev_code <= cci_pmu->model->event_ranges[if_type].max)\r\nreturn hw_event;\r\nreturn -ENOENT;\r\n}\r\nstatic int cci550_validate_hw_event(struct cci_pmu *cci_pmu,\r\nunsigned long hw_event)\r\n{\r\nu32 ev_source = CCI5xx_PMU_EVENT_SOURCE(hw_event);\r\nu32 ev_code = CCI5xx_PMU_EVENT_CODE(hw_event);\r\nint if_type;\r\nif (hw_event & ~CCI5xx_PMU_EVENT_MASK)\r\nreturn -ENOENT;\r\nswitch (ev_source) {\r\ncase CCI5xx_PORT_S0:\r\ncase CCI5xx_PORT_S1:\r\ncase CCI5xx_PORT_S2:\r\ncase CCI5xx_PORT_S3:\r\ncase CCI5xx_PORT_S4:\r\ncase CCI5xx_PORT_S5:\r\ncase CCI5xx_PORT_S6:\r\nif_type = CCI_IF_SLAVE;\r\nbreak;\r\ncase CCI5xx_PORT_M0:\r\ncase CCI5xx_PORT_M1:\r\ncase CCI5xx_PORT_M2:\r\ncase CCI5xx_PORT_M3:\r\ncase CCI5xx_PORT_M4:\r\ncase CCI5xx_PORT_M5:\r\ncase CCI5xx_PORT_M6:\r\nif_type = CCI_IF_MASTER;\r\nbreak;\r\ncase CCI5xx_PORT_GLOBAL:\r\nif_type = CCI_IF_GLOBAL;\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nif (ev_code >= cci_pmu->model->event_ranges[if_type].min &&\r\nev_code <= cci_pmu->model->event_ranges[if_type].max)\r\nreturn hw_event;\r\nreturn -ENOENT;\r\n}\r\nstatic void cci_pmu_sync_counters(struct cci_pmu *cci_pmu)\r\n{\r\nint i;\r\nstruct cci_pmu_hw_events *cci_hw = &cci_pmu->hw_events;\r\nDECLARE_BITMAP(mask, cci_pmu->num_cntrs);\r\nbitmap_zero(mask, cci_pmu->num_cntrs);\r\nfor_each_set_bit(i, cci_pmu->hw_events.used_mask, cci_pmu->num_cntrs) {\r\nstruct perf_event *event = cci_hw->events[i];\r\nif (WARN_ON(!event))\r\ncontinue;\r\nif (event->hw.state & PERF_HES_STOPPED)\r\ncontinue;\r\nif (event->hw.state & PERF_HES_ARCH) {\r\nset_bit(i, mask);\r\nevent->hw.state &= ~PERF_HES_ARCH;\r\n}\r\n}\r\npmu_write_counters(cci_pmu, mask);\r\n}\r\nstatic void __cci_pmu_enable_nosync(struct cci_pmu *cci_pmu)\r\n{\r\nu32 val;\r\nval = readl_relaxed(cci_ctrl_base + CCI_PMCR) | CCI_PMCR_CEN;\r\nwritel(val, cci_ctrl_base + CCI_PMCR);\r\n}\r\nstatic void __cci_pmu_enable_sync(struct cci_pmu *cci_pmu)\r\n{\r\ncci_pmu_sync_counters(cci_pmu);\r\n__cci_pmu_enable_nosync(cci_pmu);\r\n}\r\nstatic void __cci_pmu_disable(void)\r\n{\r\nu32 val;\r\nval = readl_relaxed(cci_ctrl_base + CCI_PMCR) & ~CCI_PMCR_CEN;\r\nwritel(val, cci_ctrl_base + CCI_PMCR);\r\n}\r\nstatic ssize_t cci_pmu_format_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct dev_ext_attribute *eattr = container_of(attr,\r\nstruct dev_ext_attribute, attr);\r\nreturn snprintf(buf, PAGE_SIZE, "%s\n", (char *)eattr->var);\r\n}\r\nstatic ssize_t cci_pmu_event_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct dev_ext_attribute *eattr = container_of(attr,\r\nstruct dev_ext_attribute, attr);\r\nreturn snprintf(buf, PAGE_SIZE, "source=?,event=0x%lx\n",\r\n(unsigned long)eattr->var);\r\n}\r\nstatic int pmu_is_valid_counter(struct cci_pmu *cci_pmu, int idx)\r\n{\r\nreturn 0 <= idx && idx <= CCI_PMU_CNTR_LAST(cci_pmu);\r\n}\r\nstatic u32 pmu_read_register(struct cci_pmu *cci_pmu, int idx, unsigned int offset)\r\n{\r\nreturn readl_relaxed(cci_pmu->base +\r\nCCI_PMU_CNTR_BASE(cci_pmu->model, idx) + offset);\r\n}\r\nstatic void pmu_write_register(struct cci_pmu *cci_pmu, u32 value,\r\nint idx, unsigned int offset)\r\n{\r\nwritel_relaxed(value, cci_pmu->base +\r\nCCI_PMU_CNTR_BASE(cci_pmu->model, idx) + offset);\r\n}\r\nstatic void pmu_disable_counter(struct cci_pmu *cci_pmu, int idx)\r\n{\r\npmu_write_register(cci_pmu, 0, idx, CCI_PMU_CNTR_CTRL);\r\n}\r\nstatic void pmu_enable_counter(struct cci_pmu *cci_pmu, int idx)\r\n{\r\npmu_write_register(cci_pmu, 1, idx, CCI_PMU_CNTR_CTRL);\r\n}\r\nstatic bool __maybe_unused\r\npmu_counter_is_enabled(struct cci_pmu *cci_pmu, int idx)\r\n{\r\nreturn (pmu_read_register(cci_pmu, idx, CCI_PMU_CNTR_CTRL) & 0x1) != 0;\r\n}\r\nstatic void pmu_set_event(struct cci_pmu *cci_pmu, int idx, unsigned long event)\r\n{\r\npmu_write_register(cci_pmu, event, idx, CCI_PMU_EVT_SEL);\r\n}\r\nstatic void __maybe_unused\r\npmu_save_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\r\n{\r\nint i;\r\nfor (i = 0; i < cci_pmu->num_cntrs; i++) {\r\nif (pmu_counter_is_enabled(cci_pmu, i)) {\r\nset_bit(i, mask);\r\npmu_disable_counter(cci_pmu, i);\r\n}\r\n}\r\n}\r\nstatic void __maybe_unused\r\npmu_restore_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\r\n{\r\nint i;\r\nfor_each_set_bit(i, mask, cci_pmu->num_cntrs)\r\npmu_enable_counter(cci_pmu, i);\r\n}\r\nstatic u32 pmu_get_max_counters(void)\r\n{\r\nreturn (readl_relaxed(cci_ctrl_base + CCI_PMCR) &\r\nCCI_PMCR_NCNT_MASK) >> CCI_PMCR_NCNT_SHIFT;\r\n}\r\nstatic int pmu_get_event_idx(struct cci_pmu_hw_events *hw, struct perf_event *event)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nunsigned long cci_event = event->hw.config_base;\r\nint idx;\r\nif (cci_pmu->model->get_event_idx)\r\nreturn cci_pmu->model->get_event_idx(cci_pmu, hw, cci_event);\r\nfor(idx = 0; idx <= CCI_PMU_CNTR_LAST(cci_pmu); idx++)\r\nif (!test_and_set_bit(idx, hw->used_mask))\r\nreturn idx;\r\nreturn -EAGAIN;\r\n}\r\nstatic int pmu_map_event(struct perf_event *event)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nif (event->attr.type < PERF_TYPE_MAX ||\r\n!cci_pmu->model->validate_hw_event)\r\nreturn -ENOENT;\r\nreturn cci_pmu->model->validate_hw_event(cci_pmu, event->attr.config);\r\n}\r\nstatic int pmu_request_irq(struct cci_pmu *cci_pmu, irq_handler_t handler)\r\n{\r\nint i;\r\nstruct platform_device *pmu_device = cci_pmu->plat_device;\r\nif (unlikely(!pmu_device))\r\nreturn -ENODEV;\r\nif (cci_pmu->nr_irqs < 1) {\r\ndev_err(&pmu_device->dev, "no irqs for CCI PMUs defined\n");\r\nreturn -ENODEV;\r\n}\r\nfor (i = 0; i < cci_pmu->nr_irqs; i++) {\r\nint err = request_irq(cci_pmu->irqs[i], handler, IRQF_SHARED,\r\n"arm-cci-pmu", cci_pmu);\r\nif (err) {\r\ndev_err(&pmu_device->dev, "unable to request IRQ%d for ARM CCI PMU counters\n",\r\ncci_pmu->irqs[i]);\r\nreturn err;\r\n}\r\nset_bit(i, &cci_pmu->active_irqs);\r\n}\r\nreturn 0;\r\n}\r\nstatic void pmu_free_irq(struct cci_pmu *cci_pmu)\r\n{\r\nint i;\r\nfor (i = 0; i < cci_pmu->nr_irqs; i++) {\r\nif (!test_and_clear_bit(i, &cci_pmu->active_irqs))\r\ncontinue;\r\nfree_irq(cci_pmu->irqs[i], cci_pmu);\r\n}\r\n}\r\nstatic u32 pmu_read_counter(struct perf_event *event)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nstruct hw_perf_event *hw_counter = &event->hw;\r\nint idx = hw_counter->idx;\r\nu32 value;\r\nif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\r\ndev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);\r\nreturn 0;\r\n}\r\nvalue = pmu_read_register(cci_pmu, idx, CCI_PMU_CNTR);\r\nreturn value;\r\n}\r\nstatic void pmu_write_counter(struct cci_pmu *cci_pmu, u32 value, int idx)\r\n{\r\npmu_write_register(cci_pmu, value, idx, CCI_PMU_CNTR);\r\n}\r\nstatic void __pmu_write_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\r\n{\r\nint i;\r\nstruct cci_pmu_hw_events *cci_hw = &cci_pmu->hw_events;\r\nfor_each_set_bit(i, mask, cci_pmu->num_cntrs) {\r\nstruct perf_event *event = cci_hw->events[i];\r\nif (WARN_ON(!event))\r\ncontinue;\r\npmu_write_counter(cci_pmu, local64_read(&event->hw.prev_count), i);\r\n}\r\n}\r\nstatic void pmu_write_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\r\n{\r\nif (cci_pmu->model->write_counters)\r\ncci_pmu->model->write_counters(cci_pmu, mask);\r\nelse\r\n__pmu_write_counters(cci_pmu, mask);\r\n}\r\nstatic void cci5xx_pmu_write_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\r\n{\r\nint i;\r\nDECLARE_BITMAP(saved_mask, cci_pmu->num_cntrs);\r\nbitmap_zero(saved_mask, cci_pmu->num_cntrs);\r\npmu_save_counters(cci_pmu, saved_mask);\r\n__cci_pmu_enable_nosync(cci_pmu);\r\nfor_each_set_bit(i, mask, cci_pmu->num_cntrs) {\r\nstruct perf_event *event = cci_pmu->hw_events.events[i];\r\nif (WARN_ON(!event))\r\ncontinue;\r\npmu_set_event(cci_pmu, i, CCI5xx_INVALID_EVENT);\r\npmu_enable_counter(cci_pmu, i);\r\npmu_write_counter(cci_pmu, local64_read(&event->hw.prev_count), i);\r\npmu_disable_counter(cci_pmu, i);\r\npmu_set_event(cci_pmu, i, event->hw.config_base);\r\n}\r\n__cci_pmu_disable();\r\npmu_restore_counters(cci_pmu, saved_mask);\r\n}\r\nstatic u64 pmu_event_update(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 delta, prev_raw_count, new_raw_count;\r\ndo {\r\nprev_raw_count = local64_read(&hwc->prev_count);\r\nnew_raw_count = pmu_read_counter(event);\r\n} while (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\r\nnew_raw_count) != prev_raw_count);\r\ndelta = (new_raw_count - prev_raw_count) & CCI_PMU_CNTR_MASK;\r\nlocal64_add(delta, &event->count);\r\nreturn new_raw_count;\r\n}\r\nstatic void pmu_read(struct perf_event *event)\r\n{\r\npmu_event_update(event);\r\n}\r\nstatic void pmu_event_set_period(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 val = 1ULL << 31;\r\nlocal64_set(&hwc->prev_count, val);\r\nhwc->state |= PERF_HES_ARCH;\r\n}\r\nstatic irqreturn_t pmu_handle_irq(int irq_num, void *dev)\r\n{\r\nunsigned long flags;\r\nstruct cci_pmu *cci_pmu = dev;\r\nstruct cci_pmu_hw_events *events = &cci_pmu->hw_events;\r\nint idx, handled = IRQ_NONE;\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\n__cci_pmu_disable();\r\nfor (idx = 0; idx <= CCI_PMU_CNTR_LAST(cci_pmu); idx++) {\r\nstruct perf_event *event = events->events[idx];\r\nif (!event)\r\ncontinue;\r\nif (!(pmu_read_register(cci_pmu, idx, CCI_PMU_OVRFLW) &\r\nCCI_PMU_OVRFLW_FLAG))\r\ncontinue;\r\npmu_write_register(cci_pmu, CCI_PMU_OVRFLW_FLAG, idx,\r\nCCI_PMU_OVRFLW);\r\npmu_event_update(event);\r\npmu_event_set_period(event);\r\nhandled = IRQ_HANDLED;\r\n}\r\n__cci_pmu_enable_sync(cci_pmu);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic int cci_pmu_get_hw(struct cci_pmu *cci_pmu)\r\n{\r\nint ret = pmu_request_irq(cci_pmu, pmu_handle_irq);\r\nif (ret) {\r\npmu_free_irq(cci_pmu);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cci_pmu_put_hw(struct cci_pmu *cci_pmu)\r\n{\r\npmu_free_irq(cci_pmu);\r\n}\r\nstatic void hw_perf_event_destroy(struct perf_event *event)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\natomic_t *active_events = &cci_pmu->active_events;\r\nstruct mutex *reserve_mutex = &cci_pmu->reserve_mutex;\r\nif (atomic_dec_and_mutex_lock(active_events, reserve_mutex)) {\r\ncci_pmu_put_hw(cci_pmu);\r\nmutex_unlock(reserve_mutex);\r\n}\r\n}\r\nstatic void cci_pmu_enable(struct pmu *pmu)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(pmu);\r\nstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\r\nint enabled = bitmap_weight(hw_events->used_mask, cci_pmu->num_cntrs);\r\nunsigned long flags;\r\nif (!enabled)\r\nreturn;\r\nraw_spin_lock_irqsave(&hw_events->pmu_lock, flags);\r\n__cci_pmu_enable_sync(cci_pmu);\r\nraw_spin_unlock_irqrestore(&hw_events->pmu_lock, flags);\r\n}\r\nstatic void cci_pmu_disable(struct pmu *pmu)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(pmu);\r\nstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&hw_events->pmu_lock, flags);\r\n__cci_pmu_disable();\r\nraw_spin_unlock_irqrestore(&hw_events->pmu_lock, flags);\r\n}\r\nstatic bool pmu_fixed_hw_idx(struct cci_pmu *cci_pmu, int idx)\r\n{\r\nreturn (idx >= 0) && (idx < cci_pmu->model->fixed_hw_cntrs);\r\n}\r\nstatic void cci_pmu_start(struct perf_event *event, int pmu_flags)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nunsigned long flags;\r\nif (pmu_flags & PERF_EF_RELOAD)\r\nWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\r\nhwc->state = 0;\r\nif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\r\ndev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&hw_events->pmu_lock, flags);\r\nif (!pmu_fixed_hw_idx(cci_pmu, idx))\r\npmu_set_event(cci_pmu, idx, hwc->config_base);\r\npmu_event_set_period(event);\r\npmu_enable_counter(cci_pmu, idx);\r\nraw_spin_unlock_irqrestore(&hw_events->pmu_lock, flags);\r\n}\r\nstatic void cci_pmu_stop(struct perf_event *event, int pmu_flags)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nif (hwc->state & PERF_HES_STOPPED)\r\nreturn;\r\nif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\r\ndev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);\r\nreturn;\r\n}\r\npmu_disable_counter(cci_pmu, idx);\r\npmu_event_update(event);\r\nhwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\n}\r\nstatic int cci_pmu_add(struct perf_event *event, int flags)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx;\r\nint err = 0;\r\nperf_pmu_disable(event->pmu);\r\nidx = pmu_get_event_idx(hw_events, event);\r\nif (idx < 0) {\r\nerr = idx;\r\ngoto out;\r\n}\r\nevent->hw.idx = idx;\r\nhw_events->events[idx] = event;\r\nhwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nif (flags & PERF_EF_START)\r\ncci_pmu_start(event, PERF_EF_RELOAD);\r\nperf_event_update_userpage(event);\r\nout:\r\nperf_pmu_enable(event->pmu);\r\nreturn err;\r\n}\r\nstatic void cci_pmu_del(struct perf_event *event, int flags)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\ncci_pmu_stop(event, PERF_EF_UPDATE);\r\nhw_events->events[idx] = NULL;\r\nclear_bit(idx, hw_events->used_mask);\r\nperf_event_update_userpage(event);\r\n}\r\nstatic int\r\nvalidate_event(struct pmu *cci_pmu,\r\nstruct cci_pmu_hw_events *hw_events,\r\nstruct perf_event *event)\r\n{\r\nif (is_software_event(event))\r\nreturn 1;\r\nif (event->pmu != cci_pmu)\r\nreturn 0;\r\nif (event->state < PERF_EVENT_STATE_OFF)\r\nreturn 1;\r\nif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\r\nreturn 1;\r\nreturn pmu_get_event_idx(hw_events, event) >= 0;\r\n}\r\nstatic int\r\nvalidate_group(struct perf_event *event)\r\n{\r\nstruct perf_event *sibling, *leader = event->group_leader;\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\nunsigned long mask[BITS_TO_LONGS(cci_pmu->num_cntrs)];\r\nstruct cci_pmu_hw_events fake_pmu = {\r\n.used_mask = mask,\r\n};\r\nmemset(mask, 0, BITS_TO_LONGS(cci_pmu->num_cntrs) * sizeof(unsigned long));\r\nif (!validate_event(event->pmu, &fake_pmu, leader))\r\nreturn -EINVAL;\r\nlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\r\nif (!validate_event(event->pmu, &fake_pmu, sibling))\r\nreturn -EINVAL;\r\n}\r\nif (!validate_event(event->pmu, &fake_pmu, event))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int\r\n__hw_perf_event_init(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint mapping;\r\nmapping = pmu_map_event(event);\r\nif (mapping < 0) {\r\npr_debug("event %x:%llx not supported\n", event->attr.type,\r\nevent->attr.config);\r\nreturn mapping;\r\n}\r\nhwc->idx = -1;\r\nhwc->config_base = 0;\r\nhwc->config = 0;\r\nhwc->event_base = 0;\r\nhwc->config_base |= (unsigned long)mapping;\r\nhwc->sample_period = CCI_PMU_CNTR_MASK >> 1;\r\nhwc->last_period = hwc->sample_period;\r\nlocal64_set(&hwc->period_left, hwc->sample_period);\r\nif (event->group_leader != event) {\r\nif (validate_group(event) != 0)\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int cci_pmu_event_init(struct perf_event *event)\r\n{\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\r\natomic_t *active_events = &cci_pmu->active_events;\r\nint err = 0;\r\nint cpu;\r\nif (event->attr.type != event->pmu->type)\r\nreturn -ENOENT;\r\nif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\r\nreturn -EOPNOTSUPP;\r\nif (event->attr.exclude_user ||\r\nevent->attr.exclude_kernel ||\r\nevent->attr.exclude_hv ||\r\nevent->attr.exclude_idle ||\r\nevent->attr.exclude_host ||\r\nevent->attr.exclude_guest)\r\nreturn -EINVAL;\r\ncpu = cpumask_first(&cci_pmu->cpus);\r\nif (event->cpu < 0 || cpu < 0)\r\nreturn -EINVAL;\r\nevent->cpu = cpu;\r\nevent->destroy = hw_perf_event_destroy;\r\nif (!atomic_inc_not_zero(active_events)) {\r\nmutex_lock(&cci_pmu->reserve_mutex);\r\nif (atomic_read(active_events) == 0)\r\nerr = cci_pmu_get_hw(cci_pmu);\r\nif (!err)\r\natomic_inc(active_events);\r\nmutex_unlock(&cci_pmu->reserve_mutex);\r\n}\r\nif (err)\r\nreturn err;\r\nerr = __hw_perf_event_init(event);\r\nif (err)\r\nhw_perf_event_destroy(event);\r\nreturn err;\r\n}\r\nstatic ssize_t pmu_cpumask_attr_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct pmu *pmu = dev_get_drvdata(dev);\r\nstruct cci_pmu *cci_pmu = to_cci_pmu(pmu);\r\nint n = scnprintf(buf, PAGE_SIZE - 1, "%*pbl",\r\ncpumask_pr_args(&cci_pmu->cpus));\r\nbuf[n++] = '\n';\r\nbuf[n] = '\0';\r\nreturn n;\r\n}\r\nstatic int cci_pmu_init(struct cci_pmu *cci_pmu, struct platform_device *pdev)\r\n{\r\nconst struct cci_pmu_model *model = cci_pmu->model;\r\nchar *name = model->name;\r\nu32 num_cntrs;\r\npmu_event_attr_group.attrs = model->event_attrs;\r\npmu_format_attr_group.attrs = model->format_attrs;\r\ncci_pmu->pmu = (struct pmu) {\r\n.name = cci_pmu->model->name,\r\n.task_ctx_nr = perf_invalid_context,\r\n.pmu_enable = cci_pmu_enable,\r\n.pmu_disable = cci_pmu_disable,\r\n.event_init = cci_pmu_event_init,\r\n.add = cci_pmu_add,\r\n.del = cci_pmu_del,\r\n.start = cci_pmu_start,\r\n.stop = cci_pmu_stop,\r\n.read = pmu_read,\r\n.attr_groups = pmu_attr_groups,\r\n};\r\ncci_pmu->plat_device = pdev;\r\nnum_cntrs = pmu_get_max_counters();\r\nif (num_cntrs > cci_pmu->model->num_hw_cntrs) {\r\ndev_warn(&pdev->dev,\r\n"PMU implements more counters(%d) than supported by"\r\n" the model(%d), truncated.",\r\nnum_cntrs, cci_pmu->model->num_hw_cntrs);\r\nnum_cntrs = cci_pmu->model->num_hw_cntrs;\r\n}\r\ncci_pmu->num_cntrs = num_cntrs + cci_pmu->model->fixed_hw_cntrs;\r\nreturn perf_pmu_register(&cci_pmu->pmu, name, -1);\r\n}\r\nstatic int cci_pmu_cpu_notifier(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nstruct cci_pmu *cci_pmu = container_of(self,\r\nstruct cci_pmu, cpu_nb);\r\nunsigned int cpu = (long)hcpu;\r\nunsigned int target;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_DOWN_PREPARE:\r\nif (!cpumask_test_and_clear_cpu(cpu, &cci_pmu->cpus))\r\nbreak;\r\ntarget = cpumask_any_but(cpu_online_mask, cpu);\r\nif (target >= nr_cpu_ids)\r\nbreak;\r\ncpumask_set_cpu(target, &cci_pmu->cpus);\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic inline const struct cci_pmu_model *get_cci_model(struct platform_device *pdev)\r\n{\r\nconst struct of_device_id *match = of_match_node(arm_cci_pmu_matches,\r\npdev->dev.of_node);\r\nif (!match)\r\nreturn NULL;\r\nif (match->data)\r\nreturn match->data;\r\ndev_warn(&pdev->dev, "DEPRECATED compatible property,"\r\n"requires secure access to CCI registers");\r\nreturn probe_cci_model(pdev);\r\n}\r\nstatic bool is_duplicate_irq(int irq, int *irqs, int nr_irqs)\r\n{\r\nint i;\r\nfor (i = 0; i < nr_irqs; i++)\r\nif (irq == irqs[i])\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic struct cci_pmu *cci_pmu_alloc(struct platform_device *pdev)\r\n{\r\nstruct cci_pmu *cci_pmu;\r\nconst struct cci_pmu_model *model;\r\nmodel = get_cci_model(pdev);\r\nif (!model) {\r\ndev_warn(&pdev->dev, "CCI PMU version not supported\n");\r\nreturn ERR_PTR(-ENODEV);\r\n}\r\ncci_pmu = devm_kzalloc(&pdev->dev, sizeof(*cci_pmu), GFP_KERNEL);\r\nif (!cci_pmu)\r\nreturn ERR_PTR(-ENOMEM);\r\ncci_pmu->model = model;\r\ncci_pmu->irqs = devm_kcalloc(&pdev->dev, CCI_PMU_MAX_HW_CNTRS(model),\r\nsizeof(*cci_pmu->irqs), GFP_KERNEL);\r\nif (!cci_pmu->irqs)\r\nreturn ERR_PTR(-ENOMEM);\r\ncci_pmu->hw_events.events = devm_kcalloc(&pdev->dev,\r\nCCI_PMU_MAX_HW_CNTRS(model),\r\nsizeof(*cci_pmu->hw_events.events),\r\nGFP_KERNEL);\r\nif (!cci_pmu->hw_events.events)\r\nreturn ERR_PTR(-ENOMEM);\r\ncci_pmu->hw_events.used_mask = devm_kcalloc(&pdev->dev,\r\nBITS_TO_LONGS(CCI_PMU_MAX_HW_CNTRS(model)),\r\nsizeof(*cci_pmu->hw_events.used_mask),\r\nGFP_KERNEL);\r\nif (!cci_pmu->hw_events.used_mask)\r\nreturn ERR_PTR(-ENOMEM);\r\nreturn cci_pmu;\r\n}\r\nstatic int cci_pmu_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *res;\r\nstruct cci_pmu *cci_pmu;\r\nint i, ret, irq;\r\ncci_pmu = cci_pmu_alloc(pdev);\r\nif (IS_ERR(cci_pmu))\r\nreturn PTR_ERR(cci_pmu);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ncci_pmu->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(cci_pmu->base))\r\nreturn -ENOMEM;\r\ncci_pmu->nr_irqs = 0;\r\nfor (i = 0; i < CCI_PMU_MAX_HW_CNTRS(cci_pmu->model); i++) {\r\nirq = platform_get_irq(pdev, i);\r\nif (irq < 0)\r\nbreak;\r\nif (is_duplicate_irq(irq, cci_pmu->irqs, cci_pmu->nr_irqs))\r\ncontinue;\r\ncci_pmu->irqs[cci_pmu->nr_irqs++] = irq;\r\n}\r\nif (i < CCI_PMU_MAX_HW_CNTRS(cci_pmu->model)) {\r\ndev_warn(&pdev->dev, "In-correct number of interrupts: %d, should be %d\n",\r\ni, CCI_PMU_MAX_HW_CNTRS(cci_pmu->model));\r\nreturn -EINVAL;\r\n}\r\nraw_spin_lock_init(&cci_pmu->hw_events.pmu_lock);\r\nmutex_init(&cci_pmu->reserve_mutex);\r\natomic_set(&cci_pmu->active_events, 0);\r\ncpumask_set_cpu(smp_processor_id(), &cci_pmu->cpus);\r\ncci_pmu->cpu_nb = (struct notifier_block) {\r\n.notifier_call = cci_pmu_cpu_notifier,\r\n.priority = CPU_PRI_PERF + 1,\r\n};\r\nret = register_cpu_notifier(&cci_pmu->cpu_nb);\r\nif (ret)\r\nreturn ret;\r\nret = cci_pmu_init(cci_pmu, pdev);\r\nif (ret) {\r\nunregister_cpu_notifier(&cci_pmu->cpu_nb);\r\nreturn ret;\r\n}\r\npr_info("ARM %s PMU driver probed", cci_pmu->model->name);\r\nreturn 0;\r\n}\r\nstatic int cci_platform_probe(struct platform_device *pdev)\r\n{\r\nif (!cci_probed())\r\nreturn -ENODEV;\r\nreturn of_platform_populate(pdev->dev.of_node, NULL, NULL, &pdev->dev);\r\n}\r\nstatic int __init cci_platform_init(void)\r\n{\r\nint ret;\r\nret = platform_driver_register(&cci_pmu_driver);\r\nif (ret)\r\nreturn ret;\r\nreturn platform_driver_register(&cci_platform_driver);\r\n}\r\nstatic int __init cci_platform_init(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void init_cpu_port(struct cpu_port *port, u32 index, u64 mpidr)\r\n{\r\nport->port = PORT_VALID | index;\r\nport->mpidr = mpidr;\r\n}\r\nstatic inline bool cpu_port_is_valid(struct cpu_port *port)\r\n{\r\nreturn !!(port->port & PORT_VALID);\r\n}\r\nstatic inline bool cpu_port_match(struct cpu_port *port, u64 mpidr)\r\n{\r\nreturn port->mpidr == (mpidr & MPIDR_HWID_BITMASK);\r\n}\r\nstatic int __cci_ace_get_port(struct device_node *dn, int type)\r\n{\r\nint i;\r\nbool ace_match;\r\nstruct device_node *cci_portn;\r\ncci_portn = of_parse_phandle(dn, "cci-control-port", 0);\r\nfor (i = 0; i < nb_cci_ports; i++) {\r\nace_match = ports[i].type == type;\r\nif (ace_match && cci_portn == ports[i].dn)\r\nreturn i;\r\n}\r\nreturn -ENODEV;\r\n}\r\nint cci_ace_get_port(struct device_node *dn)\r\n{\r\nreturn __cci_ace_get_port(dn, ACE_LITE_PORT);\r\n}\r\nstatic void cci_ace_init_ports(void)\r\n{\r\nint port, cpu;\r\nstruct device_node *cpun;\r\nfor_each_possible_cpu(cpu) {\r\ncpun = of_get_cpu_node(cpu, NULL);\r\nif (WARN(!cpun, "Missing cpu device node\n"))\r\ncontinue;\r\nport = __cci_ace_get_port(cpun, ACE_PORT);\r\nif (port < 0)\r\ncontinue;\r\ninit_cpu_port(&cpu_port[cpu], port, cpu_logical_map(cpu));\r\n}\r\nfor_each_possible_cpu(cpu) {\r\nWARN(!cpu_port_is_valid(&cpu_port[cpu]),\r\n"CPU %u does not have an associated CCI port\n",\r\ncpu);\r\n}\r\n}\r\nstatic void notrace cci_port_control(unsigned int port, bool enable)\r\n{\r\nvoid __iomem *base = ports[port].base;\r\nwritel_relaxed(enable ? CCI_ENABLE_REQ : 0, base + CCI_PORT_CTRL);\r\nwhile (readl_relaxed(cci_ctrl_base + CCI_CTRL_STATUS) & 0x1)\r\n;\r\n}\r\nint notrace cci_disable_port_by_cpu(u64 mpidr)\r\n{\r\nint cpu;\r\nbool is_valid;\r\nfor (cpu = 0; cpu < nr_cpu_ids; cpu++) {\r\nis_valid = cpu_port_is_valid(&cpu_port[cpu]);\r\nif (is_valid && cpu_port_match(&cpu_port[cpu], mpidr)) {\r\ncci_port_control(cpu_port[cpu].port, false);\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENODEV;\r\n}\r\nasmlinkage void __naked cci_enable_port_for_self(void)\r\n{\r\nasm volatile ("\n"\r\n" .arch armv7-a\n"\r\n" mrc p15, 0, r0, c0, c0, 5 @ get MPIDR value \n"\r\n" and r0, r0, #"__stringify(MPIDR_HWID_BITMASK)" \n"\r\n" adr r1, 5f \n"\r\n" ldr r2, [r1] \n"\r\n" add r1, r1, r2 @ &cpu_port \n"\r\n" add ip, r1, %[sizeof_cpu_port] \n"\r\n"1: ldr r2, [r1, %[offsetof_cpu_port_mpidr_lsb]] \n"\r\n" cmp r2, r0 @ compare MPIDR \n"\r\n" bne 2f \n"\r\n" ldr r3, [r1, %[offsetof_cpu_port_port]] \n"\r\n" tst r3, #"__stringify(PORT_VALID)" \n"\r\n" bne 3f \n"\r\n"2: add r1, r1, %[sizeof_struct_cpu_port] \n"\r\n" cmp r1, ip @ done? \n"\r\n" blo 1b \n"\r\n"cci_port_not_found: \n"\r\n" wfi \n"\r\n" wfe \n"\r\n" b cci_port_not_found \n"\r\n"3: bic r3, r3, #"__stringify(PORT_VALID)" \n"\r\n" adr r0, 6f \n"\r\n" ldmia r0, {r1, r2} \n"\r\n" sub r1, r1, r0 @ virt - phys \n"\r\n" ldr r0, [r0, r2] @ *(&ports) \n"\r\n" mov r2, %[sizeof_struct_ace_port] \n"\r\n" mla r0, r2, r3, r0 @ &ports[index] \n"\r\n" sub r0, r0, r1 @ virt_to_phys() \n"\r\n" ldr r0, [r0, %[offsetof_port_phys]] \n"\r\n" mov r3, %[cci_enable_req]\n"\r\n" str r3, [r0, #"__stringify(CCI_PORT_CTRL)"] \n"\r\n" adr r1, 7f \n"\r\n" ldr r0, [r1] \n"\r\n" ldr r0, [r0, r1] @ cci_ctrl_base \n"\r\n"4: ldr r1, [r0, #"__stringify(CCI_CTRL_STATUS)"] \n"\r\n" tst r1, %[cci_control_status_bits] \n"\r\n" bne 4b \n"\r\n" mov r0, #0 \n"\r\n" bx lr \n"\r\n" .align 2 \n"\r\n"5: .word cpu_port - . \n"\r\n"6: .word . \n"\r\n" .word ports - 6b \n"\r\n"7: .word cci_ctrl_phys - . \n"\r\n: :\r\n[sizeof_cpu_port] "i" (sizeof(cpu_port)),\r\n[cci_enable_req] "i" cpu_to_le32(CCI_ENABLE_REQ),\r\n[cci_control_status_bits] "i" cpu_to_le32(1),\r\n#ifndef __ARMEB__\r\n[offsetof_cpu_port_mpidr_lsb] "i" (offsetof(struct cpu_port, mpidr)),\r\n#else\r\n[offsetof_cpu_port_mpidr_lsb] "i" (offsetof(struct cpu_port, mpidr)+4),\r\n#endif\r\n[offsetof_cpu_port_port] "i" (offsetof(struct cpu_port, port)),\r\n[sizeof_struct_cpu_port] "i" (sizeof(struct cpu_port)),\r\n[sizeof_struct_ace_port] "i" (sizeof(struct cci_ace_port)),\r\n[offsetof_port_phys] "i" (offsetof(struct cci_ace_port, phys)) );\r\nunreachable();\r\n}\r\nint notrace __cci_control_port_by_device(struct device_node *dn, bool enable)\r\n{\r\nint port;\r\nif (!dn)\r\nreturn -ENODEV;\r\nport = __cci_ace_get_port(dn, ACE_LITE_PORT);\r\nif (WARN_ONCE(port < 0, "node %s ACE lite port look-up failure\n",\r\ndn->full_name))\r\nreturn -ENODEV;\r\ncci_port_control(port, enable);\r\nreturn 0;\r\n}\r\nint notrace __cci_control_port_by_index(u32 port, bool enable)\r\n{\r\nif (port >= nb_cci_ports || ports[port].type == ACE_INVALID_PORT)\r\nreturn -ENODEV;\r\nif (ports[port].type == ACE_PORT)\r\nreturn -EPERM;\r\ncci_port_control(port, enable);\r\nreturn 0;\r\n}\r\nstatic int cci_probe_ports(struct device_node *np)\r\n{\r\nstruct cci_nb_ports const *cci_config;\r\nint ret, i, nb_ace = 0, nb_ace_lite = 0;\r\nstruct device_node *cp;\r\nstruct resource res;\r\nconst char *match_str;\r\nbool is_ace;\r\ncci_config = of_match_node(arm_cci_matches, np)->data;\r\nif (!cci_config)\r\nreturn -ENODEV;\r\nnb_cci_ports = cci_config->nb_ace + cci_config->nb_ace_lite;\r\nports = kcalloc(nb_cci_ports, sizeof(*ports), GFP_KERNEL);\r\nif (!ports)\r\nreturn -ENOMEM;\r\nfor_each_child_of_node(np, cp) {\r\nif (!of_match_node(arm_cci_ctrl_if_matches, cp))\r\ncontinue;\r\ni = nb_ace + nb_ace_lite;\r\nif (i >= nb_cci_ports)\r\nbreak;\r\nif (of_property_read_string(cp, "interface-type",\r\n&match_str)) {\r\nWARN(1, "node %s missing interface-type property\n",\r\ncp->full_name);\r\ncontinue;\r\n}\r\nis_ace = strcmp(match_str, "ace") == 0;\r\nif (!is_ace && strcmp(match_str, "ace-lite")) {\r\nWARN(1, "node %s containing invalid interface-type property, skipping it\n",\r\ncp->full_name);\r\ncontinue;\r\n}\r\nret = of_address_to_resource(cp, 0, &res);\r\nif (!ret) {\r\nports[i].base = ioremap(res.start, resource_size(&res));\r\nports[i].phys = res.start;\r\n}\r\nif (ret || !ports[i].base) {\r\nWARN(1, "unable to ioremap CCI port %d\n", i);\r\ncontinue;\r\n}\r\nif (is_ace) {\r\nif (WARN_ON(nb_ace >= cci_config->nb_ace))\r\ncontinue;\r\nports[i].type = ACE_PORT;\r\n++nb_ace;\r\n} else {\r\nif (WARN_ON(nb_ace_lite >= cci_config->nb_ace_lite))\r\ncontinue;\r\nports[i].type = ACE_LITE_PORT;\r\n++nb_ace_lite;\r\n}\r\nports[i].dn = cp;\r\n}\r\ncci_ace_init_ports();\r\nsync_cache_w(&cci_ctrl_base);\r\nsync_cache_w(&cci_ctrl_phys);\r\nsync_cache_w(&ports);\r\nsync_cache_w(&cpu_port);\r\n__sync_cache_range_w(ports, sizeof(*ports) * nb_cci_ports);\r\npr_info("ARM CCI driver probed\n");\r\nreturn 0;\r\n}\r\nstatic inline int cci_probe_ports(struct device_node *np)\r\n{\r\nreturn 0;\r\n}\r\nstatic int cci_probe(void)\r\n{\r\nint ret;\r\nstruct device_node *np;\r\nstruct resource res;\r\nnp = of_find_matching_node(NULL, arm_cci_matches);\r\nif(!np || !of_device_is_available(np))\r\nreturn -ENODEV;\r\nret = of_address_to_resource(np, 0, &res);\r\nif (!ret) {\r\ncci_ctrl_base = ioremap(res.start, resource_size(&res));\r\ncci_ctrl_phys = res.start;\r\n}\r\nif (ret || !cci_ctrl_base) {\r\nWARN(1, "unable to ioremap CCI ctrl\n");\r\nreturn -ENXIO;\r\n}\r\nreturn cci_probe_ports(np);\r\n}\r\nstatic int cci_init(void)\r\n{\r\nif (cci_init_status != -EAGAIN)\r\nreturn cci_init_status;\r\nmutex_lock(&cci_probing);\r\nif (cci_init_status == -EAGAIN)\r\ncci_init_status = cci_probe();\r\nmutex_unlock(&cci_probing);\r\nreturn cci_init_status;\r\n}\r\nbool cci_probed(void)\r\n{\r\nreturn cci_init() == 0;\r\n}
