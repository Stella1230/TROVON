static struct tipc_bc_base *tipc_bc_base(struct net *net)\r\n{\r\nreturn tipc_net(net)->bcbase;\r\n}\r\nint tipc_bcast_get_mtu(struct net *net)\r\n{\r\nreturn tipc_link_mtu(tipc_bc_sndlink(net));\r\n}\r\nstatic void tipc_bcbase_select_primary(struct net *net)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\nint all_dests = tipc_link_bc_peers(bb->link);\r\nint i, mtu;\r\nbb->primary_bearer = INVALID_BEARER_ID;\r\nif (!all_dests)\r\nreturn;\r\nfor (i = 0; i < MAX_BEARERS; i++) {\r\nif (!bb->dests[i])\r\ncontinue;\r\nmtu = tipc_bearer_mtu(net, i);\r\nif (mtu < tipc_link_mtu(bb->link))\r\ntipc_link_set_mtu(bb->link, mtu);\r\nif (bb->dests[i] < all_dests)\r\ncontinue;\r\nbb->primary_bearer = i;\r\nif ((i ^ tipc_own_addr(net)) & 1)\r\nbreak;\r\n}\r\n}\r\nvoid tipc_bcast_inc_bearer_dst_cnt(struct net *net, int bearer_id)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\ntipc_bcast_lock(net);\r\nbb->dests[bearer_id]++;\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcast_unlock(net);\r\n}\r\nvoid tipc_bcast_dec_bearer_dst_cnt(struct net *net, int bearer_id)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\ntipc_bcast_lock(net);\r\nbb->dests[bearer_id]--;\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcast_unlock(net);\r\n}\r\nstatic void tipc_bcbase_xmit(struct net *net, struct sk_buff_head *xmitq)\r\n{\r\nint bearer_id;\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\nstruct sk_buff *skb, *_skb;\r\nstruct sk_buff_head _xmitq;\r\nif (skb_queue_empty(xmitq))\r\nreturn;\r\nbearer_id = bb->primary_bearer;\r\nif (bearer_id >= 0) {\r\ntipc_bearer_bc_xmit(net, bearer_id, xmitq);\r\nreturn;\r\n}\r\nskb_queue_head_init(&_xmitq);\r\nfor (bearer_id = 0; bearer_id < MAX_BEARERS; bearer_id++) {\r\nif (!bb->dests[bearer_id])\r\ncontinue;\r\nskb_queue_walk(xmitq, skb) {\r\n_skb = pskb_copy_for_clone(skb, GFP_ATOMIC);\r\nif (!_skb)\r\nbreak;\r\n__skb_queue_tail(&_xmitq, _skb);\r\n}\r\ntipc_bearer_bc_xmit(net, bearer_id, &_xmitq);\r\n}\r\n__skb_queue_purge(xmitq);\r\n__skb_queue_purge(&_xmitq);\r\n}\r\nint tipc_bcast_xmit(struct net *net, struct sk_buff_head *list)\r\n{\r\nstruct tipc_link *l = tipc_bc_sndlink(net);\r\nstruct sk_buff_head xmitq, inputq, rcvq;\r\nint rc = 0;\r\n__skb_queue_head_init(&rcvq);\r\n__skb_queue_head_init(&xmitq);\r\nskb_queue_head_init(&inputq);\r\nif (unlikely(!tipc_msg_reassemble(list, &rcvq)))\r\nreturn -EHOSTUNREACH;\r\ntipc_bcast_lock(net);\r\nif (tipc_link_bc_peers(l))\r\nrc = tipc_link_xmit(l, list, &xmitq);\r\ntipc_bcast_unlock(net);\r\nif (unlikely(rc)) {\r\n__skb_queue_purge(&rcvq);\r\nreturn rc;\r\n}\r\ntipc_bcbase_xmit(net, &xmitq);\r\ntipc_sk_mcast_rcv(net, &rcvq, &inputq);\r\n__skb_queue_purge(list);\r\nreturn 0;\r\n}\r\nint tipc_bcast_rcv(struct net *net, struct tipc_link *l, struct sk_buff *skb)\r\n{\r\nstruct tipc_msg *hdr = buf_msg(skb);\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nstruct sk_buff_head xmitq;\r\nint rc;\r\n__skb_queue_head_init(&xmitq);\r\nif (msg_mc_netid(hdr) != tipc_netid(net) || !tipc_link_is_up(l)) {\r\nkfree_skb(skb);\r\nreturn 0;\r\n}\r\ntipc_bcast_lock(net);\r\nif (msg_user(hdr) == BCAST_PROTOCOL)\r\nrc = tipc_link_bc_nack_rcv(l, skb, &xmitq);\r\nelse\r\nrc = tipc_link_rcv(l, skb, NULL);\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\nreturn rc;\r\n}\r\nvoid tipc_bcast_ack_rcv(struct net *net, struct tipc_link *l, u32 acked)\r\n{\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nstruct sk_buff_head xmitq;\r\n__skb_queue_head_init(&xmitq);\r\ntipc_bcast_lock(net);\r\ntipc_link_bc_ack_rcv(l, acked, &xmitq);\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\n}\r\nvoid tipc_bcast_sync_rcv(struct net *net, struct tipc_link *l,\r\nstruct tipc_msg *hdr)\r\n{\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nstruct sk_buff_head xmitq;\r\n__skb_queue_head_init(&xmitq);\r\ntipc_bcast_lock(net);\r\nif (msg_type(hdr) == STATE_MSG) {\r\ntipc_link_bc_ack_rcv(l, msg_bcast_ack(hdr), &xmitq);\r\ntipc_link_bc_sync_rcv(l, hdr, &xmitq);\r\n} else {\r\ntipc_link_bc_init_rcv(l, hdr);\r\n}\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\n}\r\nvoid tipc_bcast_add_peer(struct net *net, struct tipc_link *uc_l,\r\nstruct sk_buff_head *xmitq)\r\n{\r\nstruct tipc_link *snd_l = tipc_bc_sndlink(net);\r\ntipc_bcast_lock(net);\r\ntipc_link_add_bc_peer(snd_l, uc_l, xmitq);\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcast_unlock(net);\r\n}\r\nvoid tipc_bcast_remove_peer(struct net *net, struct tipc_link *rcv_l)\r\n{\r\nstruct tipc_link *snd_l = tipc_bc_sndlink(net);\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nstruct sk_buff_head xmitq;\r\n__skb_queue_head_init(&xmitq);\r\ntipc_bcast_lock(net);\r\ntipc_link_remove_bc_peer(snd_l, rcv_l, &xmitq);\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\n}\r\nint tipc_bclink_reset_stats(struct net *net)\r\n{\r\nstruct tipc_link *l = tipc_bc_sndlink(net);\r\nif (!l)\r\nreturn -ENOPROTOOPT;\r\ntipc_bcast_lock(net);\r\ntipc_link_reset_stats(l);\r\ntipc_bcast_unlock(net);\r\nreturn 0;\r\n}\r\nstatic int tipc_bc_link_set_queue_limits(struct net *net, u32 limit)\r\n{\r\nstruct tipc_link *l = tipc_bc_sndlink(net);\r\nif (!l)\r\nreturn -ENOPROTOOPT;\r\nif (limit < BCLINK_WIN_MIN)\r\nlimit = BCLINK_WIN_MIN;\r\nif (limit > TIPC_MAX_LINK_WIN)\r\nreturn -EINVAL;\r\ntipc_bcast_lock(net);\r\ntipc_link_set_queue_limits(l, limit);\r\ntipc_bcast_unlock(net);\r\nreturn 0;\r\n}\r\nint tipc_nl_bc_link_set(struct net *net, struct nlattr *attrs[])\r\n{\r\nint err;\r\nu32 win;\r\nstruct nlattr *props[TIPC_NLA_PROP_MAX + 1];\r\nif (!attrs[TIPC_NLA_LINK_PROP])\r\nreturn -EINVAL;\r\nerr = tipc_nl_parse_link_prop(attrs[TIPC_NLA_LINK_PROP], props);\r\nif (err)\r\nreturn err;\r\nif (!props[TIPC_NLA_PROP_WIN])\r\nreturn -EOPNOTSUPP;\r\nwin = nla_get_u32(props[TIPC_NLA_PROP_WIN]);\r\nreturn tipc_bc_link_set_queue_limits(net, win);\r\n}\r\nint tipc_bcast_init(struct net *net)\r\n{\r\nstruct tipc_net *tn = tipc_net(net);\r\nstruct tipc_bc_base *bb = NULL;\r\nstruct tipc_link *l = NULL;\r\nbb = kzalloc(sizeof(*bb), GFP_ATOMIC);\r\nif (!bb)\r\ngoto enomem;\r\ntn->bcbase = bb;\r\nspin_lock_init(&tipc_net(net)->bclock);\r\nif (!tipc_link_bc_create(net, 0, 0,\r\nU16_MAX,\r\nBCLINK_WIN_DEFAULT,\r\n0,\r\n&bb->inputq,\r\nNULL,\r\nNULL,\r\n&l))\r\ngoto enomem;\r\nbb->link = l;\r\ntn->bcl = l;\r\nreturn 0;\r\nenomem:\r\nkfree(bb);\r\nkfree(l);\r\nreturn -ENOMEM;\r\n}\r\nvoid tipc_bcast_stop(struct net *net)\r\n{\r\nstruct tipc_net *tn = net_generic(net, tipc_net_id);\r\nsynchronize_net();\r\nkfree(tn->bcbase);\r\nkfree(tn->bcl);\r\n}
