static u64 amdgpu_get_vis_part_size(struct amdgpu_device *adev,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nu64 ret = 0;\r\nif (mem->start << PAGE_SHIFT < adev->mc.visible_vram_size) {\r\nret = (u64)((mem->start << PAGE_SHIFT) + mem->size) >\r\nadev->mc.visible_vram_size ?\r\nadev->mc.visible_vram_size - (mem->start << PAGE_SHIFT) :\r\nmem->size;\r\n}\r\nreturn ret;\r\n}\r\nstatic void amdgpu_update_memory_usage(struct amdgpu_device *adev,\r\nstruct ttm_mem_reg *old_mem,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nu64 vis_size;\r\nif (!adev)\r\nreturn;\r\nif (new_mem) {\r\nswitch (new_mem->mem_type) {\r\ncase TTM_PL_TT:\r\natomic64_add(new_mem->size, &adev->gtt_usage);\r\nbreak;\r\ncase TTM_PL_VRAM:\r\natomic64_add(new_mem->size, &adev->vram_usage);\r\nvis_size = amdgpu_get_vis_part_size(adev, new_mem);\r\natomic64_add(vis_size, &adev->vram_vis_usage);\r\nbreak;\r\n}\r\n}\r\nif (old_mem) {\r\nswitch (old_mem->mem_type) {\r\ncase TTM_PL_TT:\r\natomic64_sub(old_mem->size, &adev->gtt_usage);\r\nbreak;\r\ncase TTM_PL_VRAM:\r\natomic64_sub(old_mem->size, &adev->vram_usage);\r\nvis_size = amdgpu_get_vis_part_size(adev, old_mem);\r\natomic64_sub(vis_size, &adev->vram_vis_usage);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void amdgpu_ttm_bo_destroy(struct ttm_buffer_object *tbo)\r\n{\r\nstruct amdgpu_bo *bo;\r\nbo = container_of(tbo, struct amdgpu_bo, tbo);\r\namdgpu_update_memory_usage(bo->adev, &bo->tbo.mem, NULL);\r\ndrm_gem_object_release(&bo->gem_base);\r\namdgpu_bo_unref(&bo->parent);\r\nkfree(bo->metadata);\r\nkfree(bo);\r\n}\r\nbool amdgpu_ttm_bo_is_amdgpu_bo(struct ttm_buffer_object *bo)\r\n{\r\nif (bo->destroy == &amdgpu_ttm_bo_destroy)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void amdgpu_ttm_placement_init(struct amdgpu_device *adev,\r\nstruct ttm_placement *placement,\r\nstruct ttm_place *placements,\r\nu32 domain, u64 flags)\r\n{\r\nu32 c = 0, i;\r\nplacement->placement = placements;\r\nplacement->busy_placement = placements;\r\nif (domain & AMDGPU_GEM_DOMAIN_VRAM) {\r\nif (flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS &&\r\nadev->mc.visible_vram_size < adev->mc.real_vram_size) {\r\nplacements[c].fpfn =\r\nadev->mc.visible_vram_size >> PAGE_SHIFT;\r\nplacements[c++].flags = TTM_PL_FLAG_WC | TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_VRAM | TTM_PL_FLAG_TOPDOWN;\r\n}\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_WC | TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_VRAM;\r\nif (!(flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED))\r\nplacements[c - 1].flags |= TTM_PL_FLAG_TOPDOWN;\r\n}\r\nif (domain & AMDGPU_GEM_DOMAIN_GTT) {\r\nif (flags & AMDGPU_GEM_CREATE_CPU_GTT_USWC) {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_WC | TTM_PL_FLAG_TT |\r\nTTM_PL_FLAG_UNCACHED;\r\n} else {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_CACHED | TTM_PL_FLAG_TT;\r\n}\r\n}\r\nif (domain & AMDGPU_GEM_DOMAIN_CPU) {\r\nif (flags & AMDGPU_GEM_CREATE_CPU_GTT_USWC) {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_WC | TTM_PL_FLAG_SYSTEM |\r\nTTM_PL_FLAG_UNCACHED;\r\n} else {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_CACHED | TTM_PL_FLAG_SYSTEM;\r\n}\r\n}\r\nif (domain & AMDGPU_GEM_DOMAIN_GDS) {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_UNCACHED |\r\nAMDGPU_PL_FLAG_GDS;\r\n}\r\nif (domain & AMDGPU_GEM_DOMAIN_GWS) {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_UNCACHED |\r\nAMDGPU_PL_FLAG_GWS;\r\n}\r\nif (domain & AMDGPU_GEM_DOMAIN_OA) {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_FLAG_UNCACHED |\r\nAMDGPU_PL_FLAG_OA;\r\n}\r\nif (!c) {\r\nplacements[c].fpfn = 0;\r\nplacements[c++].flags = TTM_PL_MASK_CACHING |\r\nTTM_PL_FLAG_SYSTEM;\r\n}\r\nplacement->num_placement = c;\r\nplacement->num_busy_placement = c;\r\nfor (i = 0; i < c; i++) {\r\nif ((flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) &&\r\n(placements[i].flags & TTM_PL_FLAG_VRAM) &&\r\n!placements[i].fpfn)\r\nplacements[i].lpfn =\r\nadev->mc.visible_vram_size >> PAGE_SHIFT;\r\nelse\r\nplacements[i].lpfn = 0;\r\n}\r\n}\r\nvoid amdgpu_ttm_placement_from_domain(struct amdgpu_bo *rbo, u32 domain)\r\n{\r\namdgpu_ttm_placement_init(rbo->adev, &rbo->placement,\r\nrbo->placements, domain, rbo->flags);\r\n}\r\nstatic void amdgpu_fill_placement_to_bo(struct amdgpu_bo *bo,\r\nstruct ttm_placement *placement)\r\n{\r\nBUG_ON(placement->num_placement > (AMDGPU_GEM_DOMAIN_MAX + 1));\r\nmemcpy(bo->placements, placement->placement,\r\nplacement->num_placement * sizeof(struct ttm_place));\r\nbo->placement.num_placement = placement->num_placement;\r\nbo->placement.num_busy_placement = placement->num_busy_placement;\r\nbo->placement.placement = bo->placements;\r\nbo->placement.busy_placement = bo->placements;\r\n}\r\nint amdgpu_bo_create_restricted(struct amdgpu_device *adev,\r\nunsigned long size, int byte_align,\r\nbool kernel, u32 domain, u64 flags,\r\nstruct sg_table *sg,\r\nstruct ttm_placement *placement,\r\nstruct reservation_object *resv,\r\nstruct amdgpu_bo **bo_ptr)\r\n{\r\nstruct amdgpu_bo *bo;\r\nenum ttm_bo_type type;\r\nunsigned long page_align;\r\nsize_t acc_size;\r\nint r;\r\npage_align = roundup(byte_align, PAGE_SIZE) >> PAGE_SHIFT;\r\nsize = ALIGN(size, PAGE_SIZE);\r\nif (kernel) {\r\ntype = ttm_bo_type_kernel;\r\n} else if (sg) {\r\ntype = ttm_bo_type_sg;\r\n} else {\r\ntype = ttm_bo_type_device;\r\n}\r\n*bo_ptr = NULL;\r\nacc_size = ttm_bo_dma_acc_size(&adev->mman.bdev, size,\r\nsizeof(struct amdgpu_bo));\r\nbo = kzalloc(sizeof(struct amdgpu_bo), GFP_KERNEL);\r\nif (bo == NULL)\r\nreturn -ENOMEM;\r\nr = drm_gem_object_init(adev->ddev, &bo->gem_base, size);\r\nif (unlikely(r)) {\r\nkfree(bo);\r\nreturn r;\r\n}\r\nbo->adev = adev;\r\nINIT_LIST_HEAD(&bo->list);\r\nINIT_LIST_HEAD(&bo->va);\r\nbo->prefered_domains = domain & (AMDGPU_GEM_DOMAIN_VRAM |\r\nAMDGPU_GEM_DOMAIN_GTT |\r\nAMDGPU_GEM_DOMAIN_CPU |\r\nAMDGPU_GEM_DOMAIN_GDS |\r\nAMDGPU_GEM_DOMAIN_GWS |\r\nAMDGPU_GEM_DOMAIN_OA);\r\nbo->allowed_domains = bo->prefered_domains;\r\nif (!kernel && bo->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\r\nbo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\r\nbo->flags = flags;\r\nif (!drm_arch_can_wc_memory())\r\nbo->flags &= ~AMDGPU_GEM_CREATE_CPU_GTT_USWC;\r\namdgpu_fill_placement_to_bo(bo, placement);\r\nr = ttm_bo_init(&adev->mman.bdev, &bo->tbo, size, type,\r\n&bo->placement, page_align, !kernel, NULL,\r\nacc_size, sg, resv, &amdgpu_ttm_bo_destroy);\r\nif (unlikely(r != 0)) {\r\nreturn r;\r\n}\r\n*bo_ptr = bo;\r\ntrace_amdgpu_bo_create(bo);\r\nreturn 0;\r\n}\r\nint amdgpu_bo_create(struct amdgpu_device *adev,\r\nunsigned long size, int byte_align,\r\nbool kernel, u32 domain, u64 flags,\r\nstruct sg_table *sg,\r\nstruct reservation_object *resv,\r\nstruct amdgpu_bo **bo_ptr)\r\n{\r\nstruct ttm_placement placement = {0};\r\nstruct ttm_place placements[AMDGPU_GEM_DOMAIN_MAX + 1];\r\nmemset(&placements, 0,\r\n(AMDGPU_GEM_DOMAIN_MAX + 1) * sizeof(struct ttm_place));\r\namdgpu_ttm_placement_init(adev, &placement,\r\nplacements, domain, flags);\r\nreturn amdgpu_bo_create_restricted(adev, size, byte_align, kernel,\r\ndomain, flags, sg, &placement,\r\nresv, bo_ptr);\r\n}\r\nint amdgpu_bo_kmap(struct amdgpu_bo *bo, void **ptr)\r\n{\r\nbool is_iomem;\r\nlong r;\r\nif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\r\nreturn -EPERM;\r\nif (bo->kptr) {\r\nif (ptr) {\r\n*ptr = bo->kptr;\r\n}\r\nreturn 0;\r\n}\r\nr = reservation_object_wait_timeout_rcu(bo->tbo.resv, false, false,\r\nMAX_SCHEDULE_TIMEOUT);\r\nif (r < 0)\r\nreturn r;\r\nr = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages, &bo->kmap);\r\nif (r)\r\nreturn r;\r\nbo->kptr = ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);\r\nif (ptr)\r\n*ptr = bo->kptr;\r\nreturn 0;\r\n}\r\nvoid amdgpu_bo_kunmap(struct amdgpu_bo *bo)\r\n{\r\nif (bo->kptr == NULL)\r\nreturn;\r\nbo->kptr = NULL;\r\nttm_bo_kunmap(&bo->kmap);\r\n}\r\nstruct amdgpu_bo *amdgpu_bo_ref(struct amdgpu_bo *bo)\r\n{\r\nif (bo == NULL)\r\nreturn NULL;\r\nttm_bo_reference(&bo->tbo);\r\nreturn bo;\r\n}\r\nvoid amdgpu_bo_unref(struct amdgpu_bo **bo)\r\n{\r\nstruct ttm_buffer_object *tbo;\r\nif ((*bo) == NULL)\r\nreturn;\r\ntbo = &((*bo)->tbo);\r\nttm_bo_unref(&tbo);\r\nif (tbo == NULL)\r\n*bo = NULL;\r\n}\r\nint amdgpu_bo_pin_restricted(struct amdgpu_bo *bo, u32 domain,\r\nu64 min_offset, u64 max_offset,\r\nu64 *gpu_addr)\r\n{\r\nint r, i;\r\nunsigned fpfn, lpfn;\r\nif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\r\nreturn -EPERM;\r\nif (WARN_ON_ONCE(min_offset > max_offset))\r\nreturn -EINVAL;\r\nif (bo->pin_count) {\r\nbo->pin_count++;\r\nif (gpu_addr)\r\n*gpu_addr = amdgpu_bo_gpu_offset(bo);\r\nif (max_offset != 0) {\r\nu64 domain_start;\r\nif (domain == AMDGPU_GEM_DOMAIN_VRAM)\r\ndomain_start = bo->adev->mc.vram_start;\r\nelse\r\ndomain_start = bo->adev->mc.gtt_start;\r\nWARN_ON_ONCE(max_offset <\r\n(amdgpu_bo_gpu_offset(bo) - domain_start));\r\n}\r\nreturn 0;\r\n}\r\namdgpu_ttm_placement_from_domain(bo, domain);\r\nfor (i = 0; i < bo->placement.num_placement; i++) {\r\nif ((bo->placements[i].flags & TTM_PL_FLAG_VRAM) &&\r\n!(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS) &&\r\n(!max_offset || max_offset > bo->adev->mc.visible_vram_size)) {\r\nif (WARN_ON_ONCE(min_offset >\r\nbo->adev->mc.visible_vram_size))\r\nreturn -EINVAL;\r\nfpfn = min_offset >> PAGE_SHIFT;\r\nlpfn = bo->adev->mc.visible_vram_size >> PAGE_SHIFT;\r\n} else {\r\nfpfn = min_offset >> PAGE_SHIFT;\r\nlpfn = max_offset >> PAGE_SHIFT;\r\n}\r\nif (fpfn > bo->placements[i].fpfn)\r\nbo->placements[i].fpfn = fpfn;\r\nif (!bo->placements[i].lpfn ||\r\n(lpfn && lpfn < bo->placements[i].lpfn))\r\nbo->placements[i].lpfn = lpfn;\r\nbo->placements[i].flags |= TTM_PL_FLAG_NO_EVICT;\r\n}\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (likely(r == 0)) {\r\nbo->pin_count = 1;\r\nif (gpu_addr != NULL)\r\n*gpu_addr = amdgpu_bo_gpu_offset(bo);\r\nif (domain == AMDGPU_GEM_DOMAIN_VRAM) {\r\nbo->adev->vram_pin_size += amdgpu_bo_size(bo);\r\nif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\r\nbo->adev->invisible_pin_size += amdgpu_bo_size(bo);\r\n} else\r\nbo->adev->gart_pin_size += amdgpu_bo_size(bo);\r\n} else {\r\ndev_err(bo->adev->dev, "%p pin failed\n", bo);\r\n}\r\nreturn r;\r\n}\r\nint amdgpu_bo_pin(struct amdgpu_bo *bo, u32 domain, u64 *gpu_addr)\r\n{\r\nreturn amdgpu_bo_pin_restricted(bo, domain, 0, 0, gpu_addr);\r\n}\r\nint amdgpu_bo_unpin(struct amdgpu_bo *bo)\r\n{\r\nint r, i;\r\nif (!bo->pin_count) {\r\ndev_warn(bo->adev->dev, "%p unpin not necessary\n", bo);\r\nreturn 0;\r\n}\r\nbo->pin_count--;\r\nif (bo->pin_count)\r\nreturn 0;\r\nfor (i = 0; i < bo->placement.num_placement; i++) {\r\nbo->placements[i].lpfn = 0;\r\nbo->placements[i].flags &= ~TTM_PL_FLAG_NO_EVICT;\r\n}\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (likely(r == 0)) {\r\nif (bo->tbo.mem.mem_type == TTM_PL_VRAM) {\r\nbo->adev->vram_pin_size -= amdgpu_bo_size(bo);\r\nif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\r\nbo->adev->invisible_pin_size -= amdgpu_bo_size(bo);\r\n} else\r\nbo->adev->gart_pin_size -= amdgpu_bo_size(bo);\r\n} else {\r\ndev_err(bo->adev->dev, "%p validate failed for unpin\n", bo);\r\n}\r\nreturn r;\r\n}\r\nint amdgpu_bo_evict_vram(struct amdgpu_device *adev)\r\n{\r\nif (0 && (adev->flags & AMD_IS_APU)) {\r\nreturn 0;\r\n}\r\nreturn ttm_bo_evict_mm(&adev->mman.bdev, TTM_PL_VRAM);\r\n}\r\nint amdgpu_bo_init(struct amdgpu_device *adev)\r\n{\r\nadev->mc.vram_mtrr = arch_phys_wc_add(adev->mc.aper_base,\r\nadev->mc.aper_size);\r\nDRM_INFO("Detected VRAM RAM=%lluM, BAR=%lluM\n",\r\nadev->mc.mc_vram_size >> 20,\r\n(unsigned long long)adev->mc.aper_size >> 20);\r\nDRM_INFO("RAM width %dbits %s\n",\r\nadev->mc.vram_width, amdgpu_vram_names[adev->mc.vram_type]);\r\nreturn amdgpu_ttm_init(adev);\r\n}\r\nvoid amdgpu_bo_fini(struct amdgpu_device *adev)\r\n{\r\namdgpu_ttm_fini(adev);\r\narch_phys_wc_del(adev->mc.vram_mtrr);\r\n}\r\nint amdgpu_bo_fbdev_mmap(struct amdgpu_bo *bo,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn ttm_fbdev_mmap(vma, &bo->tbo);\r\n}\r\nint amdgpu_bo_set_tiling_flags(struct amdgpu_bo *bo, u64 tiling_flags)\r\n{\r\nif (AMDGPU_TILING_GET(tiling_flags, TILE_SPLIT) > 6)\r\nreturn -EINVAL;\r\nbo->tiling_flags = tiling_flags;\r\nreturn 0;\r\n}\r\nvoid amdgpu_bo_get_tiling_flags(struct amdgpu_bo *bo, u64 *tiling_flags)\r\n{\r\nlockdep_assert_held(&bo->tbo.resv->lock.base);\r\nif (tiling_flags)\r\n*tiling_flags = bo->tiling_flags;\r\n}\r\nint amdgpu_bo_set_metadata (struct amdgpu_bo *bo, void *metadata,\r\nuint32_t metadata_size, uint64_t flags)\r\n{\r\nvoid *buffer;\r\nif (!metadata_size) {\r\nif (bo->metadata_size) {\r\nkfree(bo->metadata);\r\nbo->metadata = NULL;\r\nbo->metadata_size = 0;\r\n}\r\nreturn 0;\r\n}\r\nif (metadata == NULL)\r\nreturn -EINVAL;\r\nbuffer = kmemdup(metadata, metadata_size, GFP_KERNEL);\r\nif (buffer == NULL)\r\nreturn -ENOMEM;\r\nkfree(bo->metadata);\r\nbo->metadata_flags = flags;\r\nbo->metadata = buffer;\r\nbo->metadata_size = metadata_size;\r\nreturn 0;\r\n}\r\nint amdgpu_bo_get_metadata(struct amdgpu_bo *bo, void *buffer,\r\nsize_t buffer_size, uint32_t *metadata_size,\r\nuint64_t *flags)\r\n{\r\nif (!buffer && !metadata_size)\r\nreturn -EINVAL;\r\nif (buffer) {\r\nif (buffer_size < bo->metadata_size)\r\nreturn -EINVAL;\r\nif (bo->metadata_size)\r\nmemcpy(buffer, bo->metadata, bo->metadata_size);\r\n}\r\nif (metadata_size)\r\n*metadata_size = bo->metadata_size;\r\nif (flags)\r\n*flags = bo->metadata_flags;\r\nreturn 0;\r\n}\r\nvoid amdgpu_bo_move_notify(struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nstruct amdgpu_bo *rbo;\r\nif (!amdgpu_ttm_bo_is_amdgpu_bo(bo))\r\nreturn;\r\nrbo = container_of(bo, struct amdgpu_bo, tbo);\r\namdgpu_vm_bo_invalidate(rbo->adev, rbo);\r\nif (!new_mem)\r\nreturn;\r\namdgpu_update_memory_usage(rbo->adev, &bo->mem, new_mem);\r\n}\r\nint amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nstruct amdgpu_device *adev;\r\nstruct amdgpu_bo *abo;\r\nunsigned long offset, size, lpfn;\r\nint i, r;\r\nif (!amdgpu_ttm_bo_is_amdgpu_bo(bo))\r\nreturn 0;\r\nabo = container_of(bo, struct amdgpu_bo, tbo);\r\nadev = abo->adev;\r\nif (bo->mem.mem_type != TTM_PL_VRAM)\r\nreturn 0;\r\nsize = bo->mem.num_pages << PAGE_SHIFT;\r\noffset = bo->mem.start << PAGE_SHIFT;\r\nif ((offset + size) <= adev->mc.visible_vram_size)\r\nreturn 0;\r\nif (abo->pin_count > 0)\r\nreturn -EINVAL;\r\namdgpu_ttm_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_VRAM);\r\nlpfn = adev->mc.visible_vram_size >> PAGE_SHIFT;\r\nfor (i = 0; i < abo->placement.num_placement; i++) {\r\nif ((abo->placements[i].flags & TTM_PL_FLAG_VRAM) &&\r\n(!abo->placements[i].lpfn || abo->placements[i].lpfn > lpfn))\r\nabo->placements[i].lpfn = lpfn;\r\n}\r\nr = ttm_bo_validate(bo, &abo->placement, false, false);\r\nif (unlikely(r == -ENOMEM)) {\r\namdgpu_ttm_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_GTT);\r\nreturn ttm_bo_validate(bo, &abo->placement, false, false);\r\n} else if (unlikely(r != 0)) {\r\nreturn r;\r\n}\r\noffset = bo->mem.start << PAGE_SHIFT;\r\nif ((offset + size) > adev->mc.visible_vram_size)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nvoid amdgpu_bo_fence(struct amdgpu_bo *bo, struct fence *fence,\r\nbool shared)\r\n{\r\nstruct reservation_object *resv = bo->tbo.resv;\r\nif (shared)\r\nreservation_object_add_shared_fence(resv, fence);\r\nelse\r\nreservation_object_add_excl_fence(resv, fence);\r\n}
