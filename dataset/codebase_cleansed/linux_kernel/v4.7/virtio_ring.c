static bool vring_use_dma_api(struct virtio_device *vdev)\r\n{\r\nif (xen_domain())\r\nreturn true;\r\nreturn false;\r\n}\r\nstruct device *vring_dma_dev(const struct vring_virtqueue *vq)\r\n{\r\nreturn vq->vq.vdev->dev.parent;\r\n}\r\nstatic dma_addr_t vring_map_one_sg(const struct vring_virtqueue *vq,\r\nstruct scatterlist *sg,\r\nenum dma_data_direction direction)\r\n{\r\nif (!vring_use_dma_api(vq->vq.vdev))\r\nreturn (dma_addr_t)sg_phys(sg);\r\nreturn dma_map_page(vring_dma_dev(vq),\r\nsg_page(sg), sg->offset, sg->length,\r\ndirection);\r\n}\r\nstatic dma_addr_t vring_map_single(const struct vring_virtqueue *vq,\r\nvoid *cpu_addr, size_t size,\r\nenum dma_data_direction direction)\r\n{\r\nif (!vring_use_dma_api(vq->vq.vdev))\r\nreturn (dma_addr_t)virt_to_phys(cpu_addr);\r\nreturn dma_map_single(vring_dma_dev(vq),\r\ncpu_addr, size, direction);\r\n}\r\nstatic void vring_unmap_one(const struct vring_virtqueue *vq,\r\nstruct vring_desc *desc)\r\n{\r\nu16 flags;\r\nif (!vring_use_dma_api(vq->vq.vdev))\r\nreturn;\r\nflags = virtio16_to_cpu(vq->vq.vdev, desc->flags);\r\nif (flags & VRING_DESC_F_INDIRECT) {\r\ndma_unmap_single(vring_dma_dev(vq),\r\nvirtio64_to_cpu(vq->vq.vdev, desc->addr),\r\nvirtio32_to_cpu(vq->vq.vdev, desc->len),\r\n(flags & VRING_DESC_F_WRITE) ?\r\nDMA_FROM_DEVICE : DMA_TO_DEVICE);\r\n} else {\r\ndma_unmap_page(vring_dma_dev(vq),\r\nvirtio64_to_cpu(vq->vq.vdev, desc->addr),\r\nvirtio32_to_cpu(vq->vq.vdev, desc->len),\r\n(flags & VRING_DESC_F_WRITE) ?\r\nDMA_FROM_DEVICE : DMA_TO_DEVICE);\r\n}\r\n}\r\nstatic int vring_mapping_error(const struct vring_virtqueue *vq,\r\ndma_addr_t addr)\r\n{\r\nif (!vring_use_dma_api(vq->vq.vdev))\r\nreturn 0;\r\nreturn dma_mapping_error(vring_dma_dev(vq), addr);\r\n}\r\nstatic struct vring_desc *alloc_indirect(struct virtqueue *_vq,\r\nunsigned int total_sg, gfp_t gfp)\r\n{\r\nstruct vring_desc *desc;\r\nunsigned int i;\r\ngfp &= ~__GFP_HIGHMEM;\r\ndesc = kmalloc(total_sg * sizeof(struct vring_desc), gfp);\r\nif (!desc)\r\nreturn NULL;\r\nfor (i = 0; i < total_sg; i++)\r\ndesc[i].next = cpu_to_virtio16(_vq->vdev, i + 1);\r\nreturn desc;\r\n}\r\nstatic inline int virtqueue_add(struct virtqueue *_vq,\r\nstruct scatterlist *sgs[],\r\nunsigned int total_sg,\r\nunsigned int out_sgs,\r\nunsigned int in_sgs,\r\nvoid *data,\r\ngfp_t gfp)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nstruct scatterlist *sg;\r\nstruct vring_desc *desc;\r\nunsigned int i, n, avail, descs_used, uninitialized_var(prev), err_idx;\r\nint head;\r\nbool indirect;\r\nSTART_USE(vq);\r\nBUG_ON(data == NULL);\r\nif (unlikely(vq->broken)) {\r\nEND_USE(vq);\r\nreturn -EIO;\r\n}\r\n#ifdef DEBUG\r\n{\r\nktime_t now = ktime_get();\r\nif (vq->last_add_time_valid)\r\nWARN_ON(ktime_to_ms(ktime_sub(now, vq->last_add_time))\r\n> 100);\r\nvq->last_add_time = now;\r\nvq->last_add_time_valid = true;\r\n}\r\n#endif\r\nBUG_ON(total_sg > vq->vring.num);\r\nBUG_ON(total_sg == 0);\r\nhead = vq->free_head;\r\nif (vq->indirect && total_sg > 1 && vq->vq.num_free)\r\ndesc = alloc_indirect(_vq, total_sg, gfp);\r\nelse\r\ndesc = NULL;\r\nif (desc) {\r\nindirect = true;\r\ni = 0;\r\ndescs_used = 1;\r\n} else {\r\nindirect = false;\r\ndesc = vq->vring.desc;\r\ni = head;\r\ndescs_used = total_sg;\r\n}\r\nif (vq->vq.num_free < descs_used) {\r\npr_debug("Can't add buf len %i - avail = %i\n",\r\ndescs_used, vq->vq.num_free);\r\nif (out_sgs)\r\nvq->notify(&vq->vq);\r\nEND_USE(vq);\r\nreturn -ENOSPC;\r\n}\r\nfor (n = 0; n < out_sgs; n++) {\r\nfor (sg = sgs[n]; sg; sg = sg_next(sg)) {\r\ndma_addr_t addr = vring_map_one_sg(vq, sg, DMA_TO_DEVICE);\r\nif (vring_mapping_error(vq, addr))\r\ngoto unmap_release;\r\ndesc[i].flags = cpu_to_virtio16(_vq->vdev, VRING_DESC_F_NEXT);\r\ndesc[i].addr = cpu_to_virtio64(_vq->vdev, addr);\r\ndesc[i].len = cpu_to_virtio32(_vq->vdev, sg->length);\r\nprev = i;\r\ni = virtio16_to_cpu(_vq->vdev, desc[i].next);\r\n}\r\n}\r\nfor (; n < (out_sgs + in_sgs); n++) {\r\nfor (sg = sgs[n]; sg; sg = sg_next(sg)) {\r\ndma_addr_t addr = vring_map_one_sg(vq, sg, DMA_FROM_DEVICE);\r\nif (vring_mapping_error(vq, addr))\r\ngoto unmap_release;\r\ndesc[i].flags = cpu_to_virtio16(_vq->vdev, VRING_DESC_F_NEXT | VRING_DESC_F_WRITE);\r\ndesc[i].addr = cpu_to_virtio64(_vq->vdev, addr);\r\ndesc[i].len = cpu_to_virtio32(_vq->vdev, sg->length);\r\nprev = i;\r\ni = virtio16_to_cpu(_vq->vdev, desc[i].next);\r\n}\r\n}\r\ndesc[prev].flags &= cpu_to_virtio16(_vq->vdev, ~VRING_DESC_F_NEXT);\r\nif (indirect) {\r\ndma_addr_t addr = vring_map_single(\r\nvq, desc, total_sg * sizeof(struct vring_desc),\r\nDMA_TO_DEVICE);\r\nif (vring_mapping_error(vq, addr))\r\ngoto unmap_release;\r\nvq->vring.desc[head].flags = cpu_to_virtio16(_vq->vdev, VRING_DESC_F_INDIRECT);\r\nvq->vring.desc[head].addr = cpu_to_virtio64(_vq->vdev, addr);\r\nvq->vring.desc[head].len = cpu_to_virtio32(_vq->vdev, total_sg * sizeof(struct vring_desc));\r\n}\r\nvq->vq.num_free -= descs_used;\r\nif (indirect)\r\nvq->free_head = virtio16_to_cpu(_vq->vdev, vq->vring.desc[head].next);\r\nelse\r\nvq->free_head = i;\r\nvq->desc_state[head].data = data;\r\nif (indirect)\r\nvq->desc_state[head].indir_desc = desc;\r\navail = vq->avail_idx_shadow & (vq->vring.num - 1);\r\nvq->vring.avail->ring[avail] = cpu_to_virtio16(_vq->vdev, head);\r\nvirtio_wmb(vq->weak_barriers);\r\nvq->avail_idx_shadow++;\r\nvq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);\r\nvq->num_added++;\r\npr_debug("Added buffer head %i to %p\n", head, vq);\r\nEND_USE(vq);\r\nif (unlikely(vq->num_added == (1 << 16) - 1))\r\nvirtqueue_kick(_vq);\r\nreturn 0;\r\nunmap_release:\r\nerr_idx = i;\r\ni = head;\r\nfor (n = 0; n < total_sg; n++) {\r\nif (i == err_idx)\r\nbreak;\r\nvring_unmap_one(vq, &desc[i]);\r\ni = vq->vring.desc[i].next;\r\n}\r\nvq->vq.num_free += total_sg;\r\nif (indirect)\r\nkfree(desc);\r\nreturn -EIO;\r\n}\r\nint virtqueue_add_sgs(struct virtqueue *_vq,\r\nstruct scatterlist *sgs[],\r\nunsigned int out_sgs,\r\nunsigned int in_sgs,\r\nvoid *data,\r\ngfp_t gfp)\r\n{\r\nunsigned int i, total_sg = 0;\r\nfor (i = 0; i < out_sgs + in_sgs; i++) {\r\nstruct scatterlist *sg;\r\nfor (sg = sgs[i]; sg; sg = sg_next(sg))\r\ntotal_sg++;\r\n}\r\nreturn virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs, data, gfp);\r\n}\r\nint virtqueue_add_outbuf(struct virtqueue *vq,\r\nstruct scatterlist *sg, unsigned int num,\r\nvoid *data,\r\ngfp_t gfp)\r\n{\r\nreturn virtqueue_add(vq, &sg, num, 1, 0, data, gfp);\r\n}\r\nint virtqueue_add_inbuf(struct virtqueue *vq,\r\nstruct scatterlist *sg, unsigned int num,\r\nvoid *data,\r\ngfp_t gfp)\r\n{\r\nreturn virtqueue_add(vq, &sg, num, 0, 1, data, gfp);\r\n}\r\nbool virtqueue_kick_prepare(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nu16 new, old;\r\nbool needs_kick;\r\nSTART_USE(vq);\r\nvirtio_mb(vq->weak_barriers);\r\nold = vq->avail_idx_shadow - vq->num_added;\r\nnew = vq->avail_idx_shadow;\r\nvq->num_added = 0;\r\n#ifdef DEBUG\r\nif (vq->last_add_time_valid) {\r\nWARN_ON(ktime_to_ms(ktime_sub(ktime_get(),\r\nvq->last_add_time)) > 100);\r\n}\r\nvq->last_add_time_valid = false;\r\n#endif\r\nif (vq->event) {\r\nneeds_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)),\r\nnew, old);\r\n} else {\r\nneeds_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));\r\n}\r\nEND_USE(vq);\r\nreturn needs_kick;\r\n}\r\nbool virtqueue_notify(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nif (unlikely(vq->broken))\r\nreturn false;\r\nif (!vq->notify(_vq)) {\r\nvq->broken = true;\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nbool virtqueue_kick(struct virtqueue *vq)\r\n{\r\nif (virtqueue_kick_prepare(vq))\r\nreturn virtqueue_notify(vq);\r\nreturn true;\r\n}\r\nstatic void detach_buf(struct vring_virtqueue *vq, unsigned int head)\r\n{\r\nunsigned int i, j;\r\nu16 nextflag = cpu_to_virtio16(vq->vq.vdev, VRING_DESC_F_NEXT);\r\nvq->desc_state[head].data = NULL;\r\ni = head;\r\nwhile (vq->vring.desc[i].flags & nextflag) {\r\nvring_unmap_one(vq, &vq->vring.desc[i]);\r\ni = virtio16_to_cpu(vq->vq.vdev, vq->vring.desc[i].next);\r\nvq->vq.num_free++;\r\n}\r\nvring_unmap_one(vq, &vq->vring.desc[i]);\r\nvq->vring.desc[i].next = cpu_to_virtio16(vq->vq.vdev, vq->free_head);\r\nvq->free_head = head;\r\nvq->vq.num_free++;\r\nif (vq->desc_state[head].indir_desc) {\r\nstruct vring_desc *indir_desc = vq->desc_state[head].indir_desc;\r\nu32 len = virtio32_to_cpu(vq->vq.vdev, vq->vring.desc[head].len);\r\nBUG_ON(!(vq->vring.desc[head].flags &\r\ncpu_to_virtio16(vq->vq.vdev, VRING_DESC_F_INDIRECT)));\r\nBUG_ON(len == 0 || len % sizeof(struct vring_desc));\r\nfor (j = 0; j < len / sizeof(struct vring_desc); j++)\r\nvring_unmap_one(vq, &indir_desc[j]);\r\nkfree(vq->desc_state[head].indir_desc);\r\nvq->desc_state[head].indir_desc = NULL;\r\n}\r\n}\r\nstatic inline bool more_used(const struct vring_virtqueue *vq)\r\n{\r\nreturn vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);\r\n}\r\nvoid *virtqueue_get_buf(struct virtqueue *_vq, unsigned int *len)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nvoid *ret;\r\nunsigned int i;\r\nu16 last_used;\r\nSTART_USE(vq);\r\nif (unlikely(vq->broken)) {\r\nEND_USE(vq);\r\nreturn NULL;\r\n}\r\nif (!more_used(vq)) {\r\npr_debug("No more buffers in queue\n");\r\nEND_USE(vq);\r\nreturn NULL;\r\n}\r\nvirtio_rmb(vq->weak_barriers);\r\nlast_used = (vq->last_used_idx & (vq->vring.num - 1));\r\ni = virtio32_to_cpu(_vq->vdev, vq->vring.used->ring[last_used].id);\r\n*len = virtio32_to_cpu(_vq->vdev, vq->vring.used->ring[last_used].len);\r\nif (unlikely(i >= vq->vring.num)) {\r\nBAD_RING(vq, "id %u out of range\n", i);\r\nreturn NULL;\r\n}\r\nif (unlikely(!vq->desc_state[i].data)) {\r\nBAD_RING(vq, "id %u is not a head!\n", i);\r\nreturn NULL;\r\n}\r\nret = vq->desc_state[i].data;\r\ndetach_buf(vq, i);\r\nvq->last_used_idx++;\r\nif (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))\r\nvirtio_store_mb(vq->weak_barriers,\r\n&vring_used_event(&vq->vring),\r\ncpu_to_virtio16(_vq->vdev, vq->last_used_idx));\r\n#ifdef DEBUG\r\nvq->last_add_time_valid = false;\r\n#endif\r\nEND_USE(vq);\r\nreturn ret;\r\n}\r\nvoid virtqueue_disable_cb(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nif (!(vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {\r\nvq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;\r\nvq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);\r\n}\r\n}\r\nunsigned virtqueue_enable_cb_prepare(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nu16 last_used_idx;\r\nSTART_USE(vq);\r\nif (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {\r\nvq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;\r\nvq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);\r\n}\r\nvring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);\r\nEND_USE(vq);\r\nreturn last_used_idx;\r\n}\r\nbool virtqueue_poll(struct virtqueue *_vq, unsigned last_used_idx)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nvirtio_mb(vq->weak_barriers);\r\nreturn (u16)last_used_idx != virtio16_to_cpu(_vq->vdev, vq->vring.used->idx);\r\n}\r\nbool virtqueue_enable_cb(struct virtqueue *_vq)\r\n{\r\nunsigned last_used_idx = virtqueue_enable_cb_prepare(_vq);\r\nreturn !virtqueue_poll(_vq, last_used_idx);\r\n}\r\nbool virtqueue_enable_cb_delayed(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nu16 bufs;\r\nSTART_USE(vq);\r\nif (vq->avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {\r\nvq->avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;\r\nvq->vring.avail->flags = cpu_to_virtio16(_vq->vdev, vq->avail_flags_shadow);\r\n}\r\nbufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;\r\nvirtio_store_mb(vq->weak_barriers,\r\n&vring_used_event(&vq->vring),\r\ncpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));\r\nif (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {\r\nEND_USE(vq);\r\nreturn false;\r\n}\r\nEND_USE(vq);\r\nreturn true;\r\n}\r\nvoid *virtqueue_detach_unused_buf(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nunsigned int i;\r\nvoid *buf;\r\nSTART_USE(vq);\r\nfor (i = 0; i < vq->vring.num; i++) {\r\nif (!vq->desc_state[i].data)\r\ncontinue;\r\nbuf = vq->desc_state[i].data;\r\ndetach_buf(vq, i);\r\nvq->avail_idx_shadow--;\r\nvq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);\r\nEND_USE(vq);\r\nreturn buf;\r\n}\r\nBUG_ON(vq->vq.num_free != vq->vring.num);\r\nEND_USE(vq);\r\nreturn NULL;\r\n}\r\nirqreturn_t vring_interrupt(int irq, void *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nif (!more_used(vq)) {\r\npr_debug("virtqueue interrupt with no work for %p\n", vq);\r\nreturn IRQ_NONE;\r\n}\r\nif (unlikely(vq->broken))\r\nreturn IRQ_HANDLED;\r\npr_debug("virtqueue callback for %p (%p)\n", vq, vq->vq.callback);\r\nif (vq->vq.callback)\r\nvq->vq.callback(&vq->vq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstruct virtqueue *__vring_new_virtqueue(unsigned int index,\r\nstruct vring vring,\r\nstruct virtio_device *vdev,\r\nbool weak_barriers,\r\nbool (*notify)(struct virtqueue *),\r\nvoid (*callback)(struct virtqueue *),\r\nconst char *name)\r\n{\r\nunsigned int i;\r\nstruct vring_virtqueue *vq;\r\nvq = kmalloc(sizeof(*vq) + vring.num * sizeof(struct vring_desc_state),\r\nGFP_KERNEL);\r\nif (!vq)\r\nreturn NULL;\r\nvq->vring = vring;\r\nvq->vq.callback = callback;\r\nvq->vq.vdev = vdev;\r\nvq->vq.name = name;\r\nvq->vq.num_free = vring.num;\r\nvq->vq.index = index;\r\nvq->we_own_ring = false;\r\nvq->queue_dma_addr = 0;\r\nvq->queue_size_in_bytes = 0;\r\nvq->notify = notify;\r\nvq->weak_barriers = weak_barriers;\r\nvq->broken = false;\r\nvq->last_used_idx = 0;\r\nvq->avail_flags_shadow = 0;\r\nvq->avail_idx_shadow = 0;\r\nvq->num_added = 0;\r\nlist_add_tail(&vq->vq.list, &vdev->vqs);\r\n#ifdef DEBUG\r\nvq->in_use = false;\r\nvq->last_add_time_valid = false;\r\n#endif\r\nvq->indirect = virtio_has_feature(vdev, VIRTIO_RING_F_INDIRECT_DESC);\r\nvq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);\r\nif (!callback) {\r\nvq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;\r\nvq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);\r\n}\r\nvq->free_head = 0;\r\nfor (i = 0; i < vring.num-1; i++)\r\nvq->vring.desc[i].next = cpu_to_virtio16(vdev, i + 1);\r\nmemset(vq->desc_state, 0, vring.num * sizeof(struct vring_desc_state));\r\nreturn &vq->vq;\r\n}\r\nstatic void *vring_alloc_queue(struct virtio_device *vdev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t flag)\r\n{\r\nif (vring_use_dma_api(vdev)) {\r\nreturn dma_alloc_coherent(vdev->dev.parent, size,\r\ndma_handle, flag);\r\n} else {\r\nvoid *queue = alloc_pages_exact(PAGE_ALIGN(size), flag);\r\nif (queue) {\r\nphys_addr_t phys_addr = virt_to_phys(queue);\r\n*dma_handle = (dma_addr_t)phys_addr;\r\nif (WARN_ON_ONCE(*dma_handle != phys_addr)) {\r\nfree_pages_exact(queue, PAGE_ALIGN(size));\r\nreturn NULL;\r\n}\r\n}\r\nreturn queue;\r\n}\r\n}\r\nstatic void vring_free_queue(struct virtio_device *vdev, size_t size,\r\nvoid *queue, dma_addr_t dma_handle)\r\n{\r\nif (vring_use_dma_api(vdev)) {\r\ndma_free_coherent(vdev->dev.parent, size, queue, dma_handle);\r\n} else {\r\nfree_pages_exact(queue, PAGE_ALIGN(size));\r\n}\r\n}\r\nstruct virtqueue *vring_create_virtqueue(\r\nunsigned int index,\r\nunsigned int num,\r\nunsigned int vring_align,\r\nstruct virtio_device *vdev,\r\nbool weak_barriers,\r\nbool may_reduce_num,\r\nbool (*notify)(struct virtqueue *),\r\nvoid (*callback)(struct virtqueue *),\r\nconst char *name)\r\n{\r\nstruct virtqueue *vq;\r\nvoid *queue = NULL;\r\ndma_addr_t dma_addr;\r\nsize_t queue_size_in_bytes;\r\nstruct vring vring;\r\nif (num & (num - 1)) {\r\ndev_warn(&vdev->dev, "Bad virtqueue length %u\n", num);\r\nreturn NULL;\r\n}\r\nfor (; num && vring_size(num, vring_align) > PAGE_SIZE; num /= 2) {\r\nqueue = vring_alloc_queue(vdev, vring_size(num, vring_align),\r\n&dma_addr,\r\nGFP_KERNEL|__GFP_NOWARN|__GFP_ZERO);\r\nif (queue)\r\nbreak;\r\n}\r\nif (!num)\r\nreturn NULL;\r\nif (!queue) {\r\nqueue = vring_alloc_queue(vdev, vring_size(num, vring_align),\r\n&dma_addr, GFP_KERNEL|__GFP_ZERO);\r\n}\r\nif (!queue)\r\nreturn NULL;\r\nqueue_size_in_bytes = vring_size(num, vring_align);\r\nvring_init(&vring, num, queue, vring_align);\r\nvq = __vring_new_virtqueue(index, vring, vdev, weak_barriers,\r\nnotify, callback, name);\r\nif (!vq) {\r\nvring_free_queue(vdev, queue_size_in_bytes, queue,\r\ndma_addr);\r\nreturn NULL;\r\n}\r\nto_vvq(vq)->queue_dma_addr = dma_addr;\r\nto_vvq(vq)->queue_size_in_bytes = queue_size_in_bytes;\r\nto_vvq(vq)->we_own_ring = true;\r\nreturn vq;\r\n}\r\nstruct virtqueue *vring_new_virtqueue(unsigned int index,\r\nunsigned int num,\r\nunsigned int vring_align,\r\nstruct virtio_device *vdev,\r\nbool weak_barriers,\r\nvoid *pages,\r\nbool (*notify)(struct virtqueue *vq),\r\nvoid (*callback)(struct virtqueue *vq),\r\nconst char *name)\r\n{\r\nstruct vring vring;\r\nvring_init(&vring, num, pages, vring_align);\r\nreturn __vring_new_virtqueue(index, vring, vdev, weak_barriers,\r\nnotify, callback, name);\r\n}\r\nvoid vring_del_virtqueue(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nif (vq->we_own_ring) {\r\nvring_free_queue(vq->vq.vdev, vq->queue_size_in_bytes,\r\nvq->vring.desc, vq->queue_dma_addr);\r\n}\r\nlist_del(&_vq->list);\r\nkfree(vq);\r\n}\r\nvoid vring_transport_features(struct virtio_device *vdev)\r\n{\r\nunsigned int i;\r\nfor (i = VIRTIO_TRANSPORT_F_START; i < VIRTIO_TRANSPORT_F_END; i++) {\r\nswitch (i) {\r\ncase VIRTIO_RING_F_INDIRECT_DESC:\r\nbreak;\r\ncase VIRTIO_RING_F_EVENT_IDX:\r\nbreak;\r\ncase VIRTIO_F_VERSION_1:\r\nbreak;\r\ndefault:\r\n__virtio_clear_bit(vdev, i);\r\n}\r\n}\r\n}\r\nunsigned int virtqueue_get_vring_size(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nreturn vq->vring.num;\r\n}\r\nbool virtqueue_is_broken(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nreturn vq->broken;\r\n}\r\nvoid virtio_break_device(struct virtio_device *dev)\r\n{\r\nstruct virtqueue *_vq;\r\nlist_for_each_entry(_vq, &dev->vqs, list) {\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nvq->broken = true;\r\n}\r\n}\r\ndma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nBUG_ON(!vq->we_own_ring);\r\nreturn vq->queue_dma_addr;\r\n}\r\ndma_addr_t virtqueue_get_avail_addr(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nBUG_ON(!vq->we_own_ring);\r\nreturn vq->queue_dma_addr +\r\n((char *)vq->vring.avail - (char *)vq->vring.desc);\r\n}\r\ndma_addr_t virtqueue_get_used_addr(struct virtqueue *_vq)\r\n{\r\nstruct vring_virtqueue *vq = to_vvq(_vq);\r\nBUG_ON(!vq->we_own_ring);\r\nreturn vq->queue_dma_addr +\r\n((char *)vq->vring.used - (char *)vq->vring.desc);\r\n}\r\nconst struct vring *virtqueue_get_vring(struct virtqueue *vq)\r\n{\r\nreturn &to_vvq(vq)->vring;\r\n}
