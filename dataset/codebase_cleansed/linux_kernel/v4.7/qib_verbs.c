void qib_copy_sge(struct rvt_sge_state *ss, void *data, u32 length, int release)\r\n{\r\nstruct rvt_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nmemcpy(sge->vaddr, data, len);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (release)\r\nrvt_put_mr(sge->mr);\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= RVT_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndata += len;\r\nlength -= len;\r\n}\r\n}\r\nvoid qib_skip_sge(struct rvt_sge_state *ss, u32 length, int release)\r\n{\r\nstruct rvt_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (release)\r\nrvt_put_mr(sge->mr);\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= RVT_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\nlength -= len;\r\n}\r\n}\r\nstatic u32 qib_count_sge(struct rvt_sge_state *ss, u32 length)\r\n{\r\nstruct rvt_sge *sg_list = ss->sg_list;\r\nstruct rvt_sge sge = ss->sge;\r\nu8 num_sge = ss->num_sge;\r\nu32 ndesc = 1;\r\nwhile (length) {\r\nu32 len = sge.length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge.sge_length)\r\nlen = sge.sge_length;\r\nBUG_ON(len == 0);\r\nif (((long) sge.vaddr & (sizeof(u32) - 1)) ||\r\n(len != length && (len & (sizeof(u32) - 1)))) {\r\nndesc = 0;\r\nbreak;\r\n}\r\nndesc++;\r\nsge.vaddr += len;\r\nsge.length -= len;\r\nsge.sge_length -= len;\r\nif (sge.sge_length == 0) {\r\nif (--num_sge)\r\nsge = *sg_list++;\r\n} else if (sge.length == 0 && sge.mr->lkey) {\r\nif (++sge.n >= RVT_SEGSZ) {\r\nif (++sge.m >= sge.mr->mapsz)\r\nbreak;\r\nsge.n = 0;\r\n}\r\nsge.vaddr =\r\nsge.mr->map[sge.m]->segs[sge.n].vaddr;\r\nsge.length =\r\nsge.mr->map[sge.m]->segs[sge.n].length;\r\n}\r\nlength -= len;\r\n}\r\nreturn ndesc;\r\n}\r\nstatic void qib_copy_from_sge(void *data, struct rvt_sge_state *ss, u32 length)\r\n{\r\nstruct rvt_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nmemcpy(data, sge->vaddr, len);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= RVT_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndata += len;\r\nlength -= len;\r\n}\r\n}\r\nstatic void qib_qp_rcv(struct qib_ctxtdata *rcd, struct qib_ib_header *hdr,\r\nint has_grh, void *data, u32 tlen, struct rvt_qp *qp)\r\n{\r\nstruct qib_ibport *ibp = &rcd->ppd->ibport_data;\r\nspin_lock(&qp->r_lock);\r\nif (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)) {\r\nibp->rvp.n_pkt_drops++;\r\ngoto unlock;\r\n}\r\nswitch (qp->ibqp.qp_type) {\r\ncase IB_QPT_SMI:\r\ncase IB_QPT_GSI:\r\nif (ib_qib_disable_sma)\r\nbreak;\r\ncase IB_QPT_UD:\r\nqib_ud_rcv(ibp, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ncase IB_QPT_RC:\r\nqib_rc_rcv(rcd, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ncase IB_QPT_UC:\r\nqib_uc_rcv(ibp, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nunlock:\r\nspin_unlock(&qp->r_lock);\r\n}\r\nvoid qib_ib_rcv(struct qib_ctxtdata *rcd, void *rhdr, void *data, u32 tlen)\r\n{\r\nstruct qib_pportdata *ppd = rcd->ppd;\r\nstruct qib_ibport *ibp = &ppd->ibport_data;\r\nstruct qib_ib_header *hdr = rhdr;\r\nstruct qib_devdata *dd = ppd->dd;\r\nstruct rvt_dev_info *rdi = &dd->verbs_dev.rdi;\r\nstruct qib_other_headers *ohdr;\r\nstruct rvt_qp *qp;\r\nu32 qp_num;\r\nint lnh;\r\nu8 opcode;\r\nu16 lid;\r\nif (unlikely(tlen < 24))\r\ngoto drop;\r\nlid = be16_to_cpu(hdr->lrh[1]);\r\nif (lid < be16_to_cpu(IB_MULTICAST_LID_BASE)) {\r\nlid &= ~((1 << ppd->lmc) - 1);\r\nif (unlikely(lid != ppd->lid))\r\ngoto drop;\r\n}\r\nlnh = be16_to_cpu(hdr->lrh[0]) & 3;\r\nif (lnh == QIB_LRH_BTH)\r\nohdr = &hdr->u.oth;\r\nelse if (lnh == QIB_LRH_GRH) {\r\nu32 vtf;\r\nohdr = &hdr->u.l.oth;\r\nif (hdr->u.l.grh.next_hdr != IB_GRH_NEXT_HDR)\r\ngoto drop;\r\nvtf = be32_to_cpu(hdr->u.l.grh.version_tclass_flow);\r\nif ((vtf >> IB_GRH_VERSION_SHIFT) != IB_GRH_VERSION)\r\ngoto drop;\r\n} else\r\ngoto drop;\r\nopcode = (be32_to_cpu(ohdr->bth[0]) >> 24) & 0x7f;\r\n#ifdef CONFIG_DEBUG_FS\r\nrcd->opstats->stats[opcode].n_bytes += tlen;\r\nrcd->opstats->stats[opcode].n_packets++;\r\n#endif\r\nqp_num = be32_to_cpu(ohdr->bth[1]) & RVT_QPN_MASK;\r\nif (qp_num == QIB_MULTICAST_QPN) {\r\nstruct rvt_mcast *mcast;\r\nstruct rvt_mcast_qp *p;\r\nif (lnh != QIB_LRH_GRH)\r\ngoto drop;\r\nmcast = rvt_mcast_find(&ibp->rvp, &hdr->u.l.grh.dgid);\r\nif (mcast == NULL)\r\ngoto drop;\r\nthis_cpu_inc(ibp->pmastats->n_multicast_rcv);\r\nlist_for_each_entry_rcu(p, &mcast->qp_list, list)\r\nqib_qp_rcv(rcd, hdr, 1, data, tlen, p->qp);\r\nif (atomic_dec_return(&mcast->refcount) <= 1)\r\nwake_up(&mcast->wait);\r\n} else {\r\nrcu_read_lock();\r\nqp = rvt_lookup_qpn(rdi, &ibp->rvp, qp_num);\r\nif (!qp) {\r\nrcu_read_unlock();\r\ngoto drop;\r\n}\r\nthis_cpu_inc(ibp->pmastats->n_unicast_rcv);\r\nqib_qp_rcv(rcd, hdr, lnh == QIB_LRH_GRH, data, tlen, qp);\r\nrcu_read_unlock();\r\n}\r\nreturn;\r\ndrop:\r\nibp->rvp.n_pkt_drops++;\r\n}\r\nstatic void mem_timer(unsigned long data)\r\n{\r\nstruct qib_ibdev *dev = (struct qib_ibdev *) data;\r\nstruct list_head *list = &dev->memwait;\r\nstruct rvt_qp *qp = NULL;\r\nstruct qib_qp_priv *priv = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->rdi.pending_lock, flags);\r\nif (!list_empty(list)) {\r\npriv = list_entry(list->next, struct qib_qp_priv, iowait);\r\nqp = priv->owner;\r\nlist_del_init(&priv->iowait);\r\natomic_inc(&qp->refcount);\r\nif (!list_empty(list))\r\nmod_timer(&dev->mem_timer, jiffies + 1);\r\n}\r\nspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\r\nif (qp) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->s_flags & RVT_S_WAIT_KMEM) {\r\nqp->s_flags &= ~RVT_S_WAIT_KMEM;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic void update_sge(struct rvt_sge_state *ss, u32 length)\r\n{\r\nstruct rvt_sge *sge = &ss->sge;\r\nsge->vaddr += length;\r\nsge->length -= length;\r\nsge->sge_length -= length;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= RVT_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nreturn;\r\nsge->n = 0;\r\n}\r\nsge->vaddr = sge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length = sge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\n}\r\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data >> shift;\r\n}\r\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data << shift;\r\n}\r\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\r\n{\r\ndata <<= ((sizeof(u32) - n) * BITS_PER_BYTE);\r\ndata >>= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\r\nreturn data;\r\n}\r\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data << shift;\r\n}\r\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data >> shift;\r\n}\r\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\r\n{\r\ndata >>= ((sizeof(u32) - n) * BITS_PER_BYTE);\r\ndata <<= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\r\nreturn data;\r\n}\r\nstatic void copy_io(u32 __iomem *piobuf, struct rvt_sge_state *ss,\r\nu32 length, unsigned flush_wc)\r\n{\r\nu32 extra = 0;\r\nu32 data = 0;\r\nu32 last;\r\nwhile (1) {\r\nu32 len = ss->sge.length;\r\nu32 off;\r\nif (len > length)\r\nlen = length;\r\nif (len > ss->sge.sge_length)\r\nlen = ss->sge.sge_length;\r\nBUG_ON(len == 0);\r\noff = (unsigned long)ss->sge.vaddr & (sizeof(u32) - 1);\r\nif (off) {\r\nu32 *addr = (u32 *)((unsigned long)ss->sge.vaddr &\r\n~(sizeof(u32) - 1));\r\nu32 v = get_upper_bits(*addr, off * BITS_PER_BYTE);\r\nu32 y;\r\ny = sizeof(u32) - off;\r\nif (len > y)\r\nlen = y;\r\nif (len + extra >= sizeof(u32)) {\r\ndata |= set_upper_bits(v, extra *\r\nBITS_PER_BYTE);\r\nlen = sizeof(u32) - extra;\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n__raw_writel(data, piobuf);\r\npiobuf++;\r\nextra = 0;\r\ndata = 0;\r\n} else {\r\ndata |= clear_upper_bytes(v, len, extra);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\nextra += len;\r\n}\r\n} else if (extra) {\r\nu32 *addr = (u32 *) ss->sge.vaddr;\r\nint shift = extra * BITS_PER_BYTE;\r\nint ushift = 32 - shift;\r\nu32 l = len;\r\nwhile (l >= sizeof(u32)) {\r\nu32 v = *addr;\r\ndata |= set_upper_bits(v, shift);\r\n__raw_writel(data, piobuf);\r\ndata = get_upper_bits(v, ushift);\r\npiobuf++;\r\naddr++;\r\nl -= sizeof(u32);\r\n}\r\nif (l) {\r\nu32 v = *addr;\r\nif (l + extra >= sizeof(u32)) {\r\ndata |= set_upper_bits(v, shift);\r\nlen -= l + extra - sizeof(u32);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n__raw_writel(data, piobuf);\r\npiobuf++;\r\nextra = 0;\r\ndata = 0;\r\n} else {\r\ndata |= clear_upper_bytes(v, l, extra);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\nextra += l;\r\n}\r\n} else if (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n} else if (len == length) {\r\nu32 w;\r\nw = (len + 3) >> 2;\r\nqib_pio_copy(piobuf, ss->sge.vaddr, w - 1);\r\npiobuf += w - 1;\r\nlast = ((u32 *) ss->sge.vaddr)[w - 1];\r\nbreak;\r\n} else {\r\nu32 w = len >> 2;\r\nqib_pio_copy(piobuf, ss->sge.vaddr, w);\r\npiobuf += w;\r\nextra = len & (sizeof(u32) - 1);\r\nif (extra) {\r\nu32 v = ((u32 *) ss->sge.vaddr)[w];\r\ndata = clear_upper_bytes(v, extra, 0);\r\n}\r\n}\r\nupdate_sge(ss, len);\r\nlength -= len;\r\n}\r\nupdate_sge(ss, length);\r\nif (flush_wc) {\r\nqib_flush_wc();\r\n__raw_writel(last, piobuf);\r\nqib_flush_wc();\r\n} else\r\n__raw_writel(last, piobuf);\r\n}\r\ninline struct qib_verbs_txreq *get_txreq(struct qib_ibdev *dev,\r\nstruct rvt_qp *qp)\r\n{\r\nstruct qib_verbs_txreq *tx;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->rdi.pending_lock, flags);\r\nif (likely(!list_empty(&dev->txreq_free))) {\r\nstruct list_head *l = dev->txreq_free.next;\r\nlist_del(l);\r\nspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\r\ntx = list_entry(l, struct qib_verbs_txreq, txreq.list);\r\n} else {\r\nspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\r\ntx = __get_txreq(dev, qp);\r\n}\r\nreturn tx;\r\n}\r\nvoid qib_put_txreq(struct qib_verbs_txreq *tx)\r\n{\r\nstruct qib_ibdev *dev;\r\nstruct rvt_qp *qp;\r\nstruct qib_qp_priv *priv;\r\nunsigned long flags;\r\nqp = tx->qp;\r\ndev = to_idev(qp->ibqp.device);\r\nif (tx->mr) {\r\nrvt_put_mr(tx->mr);\r\ntx->mr = NULL;\r\n}\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_FREEBUF) {\r\ntx->txreq.flags &= ~QIB_SDMA_TXREQ_F_FREEBUF;\r\ndma_unmap_single(&dd_from_dev(dev)->pcidev->dev,\r\ntx->txreq.addr, tx->hdr_dwords << 2,\r\nDMA_TO_DEVICE);\r\nkfree(tx->align_buf);\r\n}\r\nspin_lock_irqsave(&dev->rdi.pending_lock, flags);\r\nlist_add(&tx->txreq.list, &dev->txreq_free);\r\nif (!list_empty(&dev->txwait)) {\r\npriv = list_entry(dev->txwait.next, struct qib_qp_priv,\r\niowait);\r\nqp = priv->owner;\r\nlist_del_init(&priv->iowait);\r\natomic_inc(&qp->refcount);\r\nspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->s_flags & RVT_S_WAIT_TX) {\r\nqp->s_flags &= ~RVT_S_WAIT_TX;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n} else\r\nspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\r\n}\r\nvoid qib_verbs_sdma_desc_avail(struct qib_pportdata *ppd, unsigned avail)\r\n{\r\nstruct rvt_qp *qp, *nqp;\r\nstruct qib_qp_priv *qpp, *nqpp;\r\nstruct rvt_qp *qps[20];\r\nstruct qib_ibdev *dev;\r\nunsigned i, n;\r\nn = 0;\r\ndev = &ppd->dd->verbs_dev;\r\nspin_lock(&dev->rdi.pending_lock);\r\nlist_for_each_entry_safe(qpp, nqpp, &dev->dmawait, iowait) {\r\nqp = qpp->owner;\r\nnqp = nqpp->owner;\r\nif (qp->port_num != ppd->port)\r\ncontinue;\r\nif (n == ARRAY_SIZE(qps))\r\nbreak;\r\nif (qpp->s_tx->txreq.sg_count > avail)\r\nbreak;\r\navail -= qpp->s_tx->txreq.sg_count;\r\nlist_del_init(&qpp->iowait);\r\natomic_inc(&qp->refcount);\r\nqps[n++] = qp;\r\n}\r\nspin_unlock(&dev->rdi.pending_lock);\r\nfor (i = 0; i < n; i++) {\r\nqp = qps[i];\r\nspin_lock(&qp->s_lock);\r\nif (qp->s_flags & RVT_S_WAIT_DMA_DESC) {\r\nqp->s_flags &= ~RVT_S_WAIT_DMA_DESC;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock(&qp->s_lock);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic void sdma_complete(struct qib_sdma_txreq *cookie, int status)\r\n{\r\nstruct qib_verbs_txreq *tx =\r\ncontainer_of(cookie, struct qib_verbs_txreq, txreq);\r\nstruct rvt_qp *qp = tx->qp;\r\nstruct qib_qp_priv *priv = qp->priv;\r\nspin_lock(&qp->s_lock);\r\nif (tx->wqe)\r\nqib_send_complete(qp, tx->wqe, IB_WC_SUCCESS);\r\nelse if (qp->ibqp.qp_type == IB_QPT_RC) {\r\nstruct qib_ib_header *hdr;\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_FREEBUF)\r\nhdr = &tx->align_buf->hdr;\r\nelse {\r\nstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\r\nhdr = &dev->pio_hdrs[tx->hdr_inx].hdr;\r\n}\r\nqib_rc_send_complete(qp, hdr);\r\n}\r\nif (atomic_dec_and_test(&priv->s_dma_busy)) {\r\nif (qp->state == IB_QPS_RESET)\r\nwake_up(&priv->wait_dma);\r\nelse if (qp->s_flags & RVT_S_WAIT_DMA) {\r\nqp->s_flags &= ~RVT_S_WAIT_DMA;\r\nqib_schedule_send(qp);\r\n}\r\n}\r\nspin_unlock(&qp->s_lock);\r\nqib_put_txreq(tx);\r\n}\r\nstatic int wait_kmem(struct qib_ibdev *dev, struct rvt_qp *qp)\r\n{\r\nstruct qib_qp_priv *priv = qp->priv;\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {\r\nspin_lock(&dev->rdi.pending_lock);\r\nif (list_empty(&priv->iowait)) {\r\nif (list_empty(&dev->memwait))\r\nmod_timer(&dev->mem_timer, jiffies + 1);\r\nqp->s_flags |= RVT_S_WAIT_KMEM;\r\nlist_add_tail(&priv->iowait, &dev->memwait);\r\n}\r\nspin_unlock(&dev->rdi.pending_lock);\r\nqp->s_flags &= ~RVT_S_BUSY;\r\nret = -EBUSY;\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int qib_verbs_send_dma(struct rvt_qp *qp, struct qib_ib_header *hdr,\r\nu32 hdrwords, struct rvt_sge_state *ss, u32 len,\r\nu32 plen, u32 dwords)\r\n{\r\nstruct qib_qp_priv *priv = qp->priv;\r\nstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct qib_devdata *dd = dd_from_dev(dev);\r\nstruct qib_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nstruct qib_verbs_txreq *tx;\r\nstruct qib_pio_header *phdr;\r\nu32 control;\r\nu32 ndesc;\r\nint ret;\r\ntx = priv->s_tx;\r\nif (tx) {\r\npriv->s_tx = NULL;\r\nret = qib_sdma_verbs_send(ppd, tx->ss, tx->dwords, tx);\r\ngoto bail;\r\n}\r\ntx = get_txreq(dev, qp);\r\nif (IS_ERR(tx))\r\ngoto bail_tx;\r\ncontrol = dd->f_setpbc_control(ppd, plen, qp->s_srate,\r\nbe16_to_cpu(hdr->lrh[0]) >> 12);\r\ntx->qp = qp;\r\ntx->wqe = qp->s_wqe;\r\ntx->mr = qp->s_rdma_mr;\r\nif (qp->s_rdma_mr)\r\nqp->s_rdma_mr = NULL;\r\ntx->txreq.callback = sdma_complete;\r\nif (dd->flags & QIB_HAS_SDMA_TIMEOUT)\r\ntx->txreq.flags = QIB_SDMA_TXREQ_F_HEADTOHOST;\r\nelse\r\ntx->txreq.flags = QIB_SDMA_TXREQ_F_INTREQ;\r\nif (plen + 1 > dd->piosize2kmax_dwords)\r\ntx->txreq.flags |= QIB_SDMA_TXREQ_F_USELARGEBUF;\r\nif (len) {\r\nndesc = qib_count_sge(ss, len);\r\nif (ndesc >= ppd->sdma_descq_cnt)\r\nndesc = 0;\r\n} else\r\nndesc = 1;\r\nif (ndesc) {\r\nphdr = &dev->pio_hdrs[tx->hdr_inx];\r\nphdr->pbc[0] = cpu_to_le32(plen);\r\nphdr->pbc[1] = cpu_to_le32(control);\r\nmemcpy(&phdr->hdr, hdr, hdrwords << 2);\r\ntx->txreq.flags |= QIB_SDMA_TXREQ_F_FREEDESC;\r\ntx->txreq.sg_count = ndesc;\r\ntx->txreq.addr = dev->pio_hdrs_phys +\r\ntx->hdr_inx * sizeof(struct qib_pio_header);\r\ntx->hdr_dwords = hdrwords + 2;\r\nret = qib_sdma_verbs_send(ppd, ss, dwords, tx);\r\ngoto bail;\r\n}\r\ntx->hdr_dwords = plen + 1;\r\nphdr = kmalloc(tx->hdr_dwords << 2, GFP_ATOMIC);\r\nif (!phdr)\r\ngoto err_tx;\r\nphdr->pbc[0] = cpu_to_le32(plen);\r\nphdr->pbc[1] = cpu_to_le32(control);\r\nmemcpy(&phdr->hdr, hdr, hdrwords << 2);\r\nqib_copy_from_sge((u32 *) &phdr->hdr + hdrwords, ss, len);\r\ntx->txreq.addr = dma_map_single(&dd->pcidev->dev, phdr,\r\ntx->hdr_dwords << 2, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev, tx->txreq.addr))\r\ngoto map_err;\r\ntx->align_buf = phdr;\r\ntx->txreq.flags |= QIB_SDMA_TXREQ_F_FREEBUF;\r\ntx->txreq.sg_count = 1;\r\nret = qib_sdma_verbs_send(ppd, NULL, 0, tx);\r\ngoto unaligned;\r\nmap_err:\r\nkfree(phdr);\r\nerr_tx:\r\nqib_put_txreq(tx);\r\nret = wait_kmem(dev, qp);\r\nunaligned:\r\nibp->rvp.n_unaligned++;\r\nbail:\r\nreturn ret;\r\nbail_tx:\r\nret = PTR_ERR(tx);\r\ngoto bail;\r\n}\r\nstatic int no_bufs_available(struct rvt_qp *qp)\r\n{\r\nstruct qib_qp_priv *priv = qp->priv;\r\nstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct qib_devdata *dd;\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {\r\nspin_lock(&dev->rdi.pending_lock);\r\nif (list_empty(&priv->iowait)) {\r\ndev->n_piowait++;\r\nqp->s_flags |= RVT_S_WAIT_PIO;\r\nlist_add_tail(&priv->iowait, &dev->piowait);\r\ndd = dd_from_dev(dev);\r\ndd->f_wantpiobuf_intr(dd, 1);\r\n}\r\nspin_unlock(&dev->rdi.pending_lock);\r\nqp->s_flags &= ~RVT_S_BUSY;\r\nret = -EBUSY;\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int qib_verbs_send_pio(struct rvt_qp *qp, struct qib_ib_header *ibhdr,\r\nu32 hdrwords, struct rvt_sge_state *ss, u32 len,\r\nu32 plen, u32 dwords)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(qp->ibqp.device);\r\nstruct qib_pportdata *ppd = dd->pport + qp->port_num - 1;\r\nu32 *hdr = (u32 *) ibhdr;\r\nu32 __iomem *piobuf_orig;\r\nu32 __iomem *piobuf;\r\nu64 pbc;\r\nunsigned long flags;\r\nunsigned flush_wc;\r\nu32 control;\r\nu32 pbufn;\r\ncontrol = dd->f_setpbc_control(ppd, plen, qp->s_srate,\r\nbe16_to_cpu(ibhdr->lrh[0]) >> 12);\r\npbc = ((u64) control << 32) | plen;\r\npiobuf = dd->f_getsendbuf(ppd, pbc, &pbufn);\r\nif (unlikely(piobuf == NULL))\r\nreturn no_bufs_available(qp);\r\nwriteq(pbc, piobuf);\r\npiobuf_orig = piobuf;\r\npiobuf += 2;\r\nflush_wc = dd->flags & QIB_PIO_FLUSH_WC;\r\nif (len == 0) {\r\nif (flush_wc) {\r\nqib_flush_wc();\r\nqib_pio_copy(piobuf, hdr, hdrwords - 1);\r\nqib_flush_wc();\r\n__raw_writel(hdr[hdrwords - 1], piobuf + hdrwords - 1);\r\nqib_flush_wc();\r\n} else\r\nqib_pio_copy(piobuf, hdr, hdrwords);\r\ngoto done;\r\n}\r\nif (flush_wc)\r\nqib_flush_wc();\r\nqib_pio_copy(piobuf, hdr, hdrwords);\r\npiobuf += hdrwords;\r\nif (likely(ss->num_sge == 1 && len <= ss->sge.length &&\r\n!((unsigned long)ss->sge.vaddr & (sizeof(u32) - 1)))) {\r\nu32 *addr = (u32 *) ss->sge.vaddr;\r\nupdate_sge(ss, len);\r\nif (flush_wc) {\r\nqib_pio_copy(piobuf, addr, dwords - 1);\r\nqib_flush_wc();\r\n__raw_writel(addr[dwords - 1], piobuf + dwords - 1);\r\nqib_flush_wc();\r\n} else\r\nqib_pio_copy(piobuf, addr, dwords);\r\ngoto done;\r\n}\r\ncopy_io(piobuf, ss, len, flush_wc);\r\ndone:\r\nif (dd->flags & QIB_USE_SPCL_TRIG) {\r\nu32 spcl_off = (pbufn >= dd->piobcnt2k) ? 2047 : 1023;\r\nqib_flush_wc();\r\n__raw_writel(0xaebecede, piobuf_orig + spcl_off);\r\n}\r\nqib_sendbuf_done(dd, pbufn);\r\nif (qp->s_rdma_mr) {\r\nrvt_put_mr(qp->s_rdma_mr);\r\nqp->s_rdma_mr = NULL;\r\n}\r\nif (qp->s_wqe) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nqib_send_complete(qp, qp->s_wqe, IB_WC_SUCCESS);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\n} else if (qp->ibqp.qp_type == IB_QPT_RC) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nqib_rc_send_complete(qp, ibhdr);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nint qib_verbs_send(struct rvt_qp *qp, struct qib_ib_header *hdr,\r\nu32 hdrwords, struct rvt_sge_state *ss, u32 len)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(qp->ibqp.device);\r\nu32 plen;\r\nint ret;\r\nu32 dwords = (len + 3) >> 2;\r\nplen = hdrwords + dwords + 1;\r\nif (qp->ibqp.qp_type == IB_QPT_SMI ||\r\n!(dd->flags & QIB_HAS_SEND_DMA))\r\nret = qib_verbs_send_pio(qp, hdr, hdrwords, ss, len,\r\nplen, dwords);\r\nelse\r\nret = qib_verbs_send_dma(qp, hdr, hdrwords, ss, len,\r\nplen, dwords);\r\nreturn ret;\r\n}\r\nint qib_snapshot_counters(struct qib_pportdata *ppd, u64 *swords,\r\nu64 *rwords, u64 *spkts, u64 *rpkts,\r\nu64 *xmit_wait)\r\n{\r\nint ret;\r\nstruct qib_devdata *dd = ppd->dd;\r\nif (!(dd->flags & QIB_PRESENT)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\n*swords = dd->f_portcntr(ppd, QIBPORTCNTR_WORDSEND);\r\n*rwords = dd->f_portcntr(ppd, QIBPORTCNTR_WORDRCV);\r\n*spkts = dd->f_portcntr(ppd, QIBPORTCNTR_PKTSEND);\r\n*rpkts = dd->f_portcntr(ppd, QIBPORTCNTR_PKTRCV);\r\n*xmit_wait = dd->f_portcntr(ppd, QIBPORTCNTR_SENDSTALL);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_get_counters(struct qib_pportdata *ppd,\r\nstruct qib_verbs_counters *cntrs)\r\n{\r\nint ret;\r\nif (!(ppd->dd->flags & QIB_PRESENT)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\ncntrs->symbol_error_counter =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBSYMBOLERR);\r\ncntrs->link_error_recovery_counter =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBLINKERRRECOV);\r\ncntrs->link_downed_counter =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBLINKDOWN);\r\ncntrs->port_rcv_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXDROPPKT) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RCVOVFL) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERR_RLEN) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_INVALIDRLEN) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRLINK) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRICRC) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRVCRC) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRLPCRC) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_BADFORMAT);\r\ncntrs->port_rcv_errors +=\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXLOCALPHYERR);\r\ncntrs->port_rcv_errors +=\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXVLERR);\r\ncntrs->port_rcv_remphys_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RCVEBP);\r\ncntrs->port_xmit_discards =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_UNSUPVL);\r\ncntrs->port_xmit_data = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_WORDSEND);\r\ncntrs->port_rcv_data = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_WORDRCV);\r\ncntrs->port_xmit_packets = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_PKTSEND);\r\ncntrs->port_rcv_packets = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_PKTRCV);\r\ncntrs->local_link_integrity_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_LLI);\r\ncntrs->excessive_buffer_overrun_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_EXCESSBUFOVFL);\r\ncntrs->vl15_dropped =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_VL15PKTDROP);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nvoid qib_ib_piobufavail(struct qib_devdata *dd)\r\n{\r\nstruct qib_ibdev *dev = &dd->verbs_dev;\r\nstruct list_head *list;\r\nstruct rvt_qp *qps[5];\r\nstruct rvt_qp *qp;\r\nunsigned long flags;\r\nunsigned i, n;\r\nstruct qib_qp_priv *priv;\r\nlist = &dev->piowait;\r\nn = 0;\r\nspin_lock_irqsave(&dev->rdi.pending_lock, flags);\r\nwhile (!list_empty(list)) {\r\nif (n == ARRAY_SIZE(qps))\r\ngoto full;\r\npriv = list_entry(list->next, struct qib_qp_priv, iowait);\r\nqp = priv->owner;\r\nlist_del_init(&priv->iowait);\r\natomic_inc(&qp->refcount);\r\nqps[n++] = qp;\r\n}\r\ndd->f_wantpiobuf_intr(dd, 0);\r\nfull:\r\nspin_unlock_irqrestore(&dev->rdi.pending_lock, flags);\r\nfor (i = 0; i < n; i++) {\r\nqp = qps[i];\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->s_flags & RVT_S_WAIT_PIO) {\r\nqp->s_flags &= ~RVT_S_WAIT_PIO;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic int qib_query_port(struct rvt_dev_info *rdi, u8 port_num,\r\nstruct ib_port_attr *props)\r\n{\r\nstruct qib_ibdev *ibdev = container_of(rdi, struct qib_ibdev, rdi);\r\nstruct qib_devdata *dd = dd_from_dev(ibdev);\r\nstruct qib_pportdata *ppd = &dd->pport[port_num - 1];\r\nenum ib_mtu mtu;\r\nu16 lid = ppd->lid;\r\nprops->lid = lid ? lid : be16_to_cpu(IB_LID_PERMISSIVE);\r\nprops->lmc = ppd->lmc;\r\nprops->state = dd->f_iblink_state(ppd->lastibcstat);\r\nprops->phys_state = dd->f_ibphys_portstate(ppd->lastibcstat);\r\nprops->gid_tbl_len = QIB_GUIDS_PER_PORT;\r\nprops->active_width = ppd->link_width_active;\r\nprops->active_speed = ppd->link_speed_active;\r\nprops->max_vl_num = qib_num_vls(ppd->vls_supported);\r\nprops->max_mtu = qib_ibmtu ? qib_ibmtu : IB_MTU_4096;\r\nswitch (ppd->ibmtu) {\r\ncase 4096:\r\nmtu = IB_MTU_4096;\r\nbreak;\r\ncase 2048:\r\nmtu = IB_MTU_2048;\r\nbreak;\r\ncase 1024:\r\nmtu = IB_MTU_1024;\r\nbreak;\r\ncase 512:\r\nmtu = IB_MTU_512;\r\nbreak;\r\ncase 256:\r\nmtu = IB_MTU_256;\r\nbreak;\r\ndefault:\r\nmtu = IB_MTU_2048;\r\n}\r\nprops->active_mtu = mtu;\r\nreturn 0;\r\n}\r\nstatic int qib_modify_device(struct ib_device *device,\r\nint device_modify_mask,\r\nstruct ib_device_modify *device_modify)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(device);\r\nunsigned i;\r\nint ret;\r\nif (device_modify_mask & ~(IB_DEVICE_MODIFY_SYS_IMAGE_GUID |\r\nIB_DEVICE_MODIFY_NODE_DESC)) {\r\nret = -EOPNOTSUPP;\r\ngoto bail;\r\n}\r\nif (device_modify_mask & IB_DEVICE_MODIFY_NODE_DESC) {\r\nmemcpy(device->node_desc, device_modify->node_desc, 64);\r\nfor (i = 0; i < dd->num_pports; i++) {\r\nstruct qib_ibport *ibp = &dd->pport[i].ibport_data;\r\nqib_node_desc_chg(ibp);\r\n}\r\n}\r\nif (device_modify_mask & IB_DEVICE_MODIFY_SYS_IMAGE_GUID) {\r\nib_qib_sys_image_guid =\r\ncpu_to_be64(device_modify->sys_image_guid);\r\nfor (i = 0; i < dd->num_pports; i++) {\r\nstruct qib_ibport *ibp = &dd->pport[i].ibport_data;\r\nqib_sys_guid_chg(ibp);\r\n}\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_shut_down_port(struct rvt_dev_info *rdi, u8 port_num)\r\n{\r\nstruct qib_ibdev *ibdev = container_of(rdi, struct qib_ibdev, rdi);\r\nstruct qib_devdata *dd = dd_from_dev(ibdev);\r\nstruct qib_pportdata *ppd = &dd->pport[port_num - 1];\r\nqib_set_linkstate(ppd, QIB_IB_LINKDOWN);\r\nreturn 0;\r\n}\r\nstatic int qib_get_guid_be(struct rvt_dev_info *rdi, struct rvt_ibport *rvp,\r\nint guid_index, __be64 *guid)\r\n{\r\nstruct qib_ibport *ibp = container_of(rvp, struct qib_ibport, rvp);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nif (guid_index == 0)\r\n*guid = ppd->guid;\r\nelse if (guid_index < QIB_GUIDS_PER_PORT)\r\n*guid = ibp->guids[guid_index - 1];\r\nelse\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nint qib_check_ah(struct ib_device *ibdev, struct ib_ah_attr *ah_attr)\r\n{\r\nif (ah_attr->sl > 15)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic void qib_notify_new_ah(struct ib_device *ibdev,\r\nstruct ib_ah_attr *ah_attr,\r\nstruct rvt_ah *ah)\r\n{\r\nstruct qib_ibport *ibp;\r\nstruct qib_pportdata *ppd;\r\nibp = to_iport(ibdev, ah_attr->port_num);\r\nppd = ppd_from_ibp(ibp);\r\nah->vl = ibp->sl_to_vl[ah->attr.sl];\r\nah->log_pmtu = ilog2(ppd->ibmtu);\r\n}\r\nstruct ib_ah *qib_create_qp0_ah(struct qib_ibport *ibp, u16 dlid)\r\n{\r\nstruct ib_ah_attr attr;\r\nstruct ib_ah *ah = ERR_PTR(-EINVAL);\r\nstruct rvt_qp *qp0;\r\nmemset(&attr, 0, sizeof(attr));\r\nattr.dlid = dlid;\r\nattr.port_num = ppd_from_ibp(ibp)->port;\r\nrcu_read_lock();\r\nqp0 = rcu_dereference(ibp->rvp.qp[0]);\r\nif (qp0)\r\nah = ib_create_ah(qp0->ibqp.pd, &attr);\r\nrcu_read_unlock();\r\nreturn ah;\r\n}\r\nunsigned qib_get_npkeys(struct qib_devdata *dd)\r\n{\r\nreturn ARRAY_SIZE(dd->rcd[0]->pkeys);\r\n}\r\nunsigned qib_get_pkey(struct qib_ibport *ibp, unsigned index)\r\n{\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nstruct qib_devdata *dd = ppd->dd;\r\nunsigned ctxt = ppd->hw_pidx;\r\nunsigned ret;\r\nif (!dd->rcd || index >= ARRAY_SIZE(dd->rcd[ctxt]->pkeys))\r\nret = 0;\r\nelse\r\nret = dd->rcd[ctxt]->pkeys[index];\r\nreturn ret;\r\n}\r\nstatic void init_ibport(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_verbs_counters cntrs;\r\nstruct qib_ibport *ibp = &ppd->ibport_data;\r\nspin_lock_init(&ibp->rvp.lock);\r\nibp->rvp.gid_prefix = IB_DEFAULT_GID_PREFIX;\r\nibp->rvp.sm_lid = be16_to_cpu(IB_LID_PERMISSIVE);\r\nibp->rvp.port_cap_flags = IB_PORT_SYS_IMAGE_GUID_SUP |\r\nIB_PORT_CLIENT_REG_SUP | IB_PORT_SL_MAP_SUP |\r\nIB_PORT_TRAP_SUP | IB_PORT_AUTO_MIGR_SUP |\r\nIB_PORT_DR_NOTICE_SUP | IB_PORT_CAP_MASK_NOTICE_SUP |\r\nIB_PORT_OTHER_LOCAL_CHANGES_SUP;\r\nif (ppd->dd->flags & QIB_HAS_LINK_LATENCY)\r\nibp->rvp.port_cap_flags |= IB_PORT_LINK_LATENCY_SUP;\r\nibp->rvp.pma_counter_select[0] = IB_PMA_PORT_XMIT_DATA;\r\nibp->rvp.pma_counter_select[1] = IB_PMA_PORT_RCV_DATA;\r\nibp->rvp.pma_counter_select[2] = IB_PMA_PORT_XMIT_PKTS;\r\nibp->rvp.pma_counter_select[3] = IB_PMA_PORT_RCV_PKTS;\r\nibp->rvp.pma_counter_select[4] = IB_PMA_PORT_XMIT_WAIT;\r\nqib_get_counters(ppd, &cntrs);\r\nibp->z_symbol_error_counter = cntrs.symbol_error_counter;\r\nibp->z_link_error_recovery_counter =\r\ncntrs.link_error_recovery_counter;\r\nibp->z_link_downed_counter = cntrs.link_downed_counter;\r\nibp->z_port_rcv_errors = cntrs.port_rcv_errors;\r\nibp->z_port_rcv_remphys_errors = cntrs.port_rcv_remphys_errors;\r\nibp->z_port_xmit_discards = cntrs.port_xmit_discards;\r\nibp->z_port_xmit_data = cntrs.port_xmit_data;\r\nibp->z_port_rcv_data = cntrs.port_rcv_data;\r\nibp->z_port_xmit_packets = cntrs.port_xmit_packets;\r\nibp->z_port_rcv_packets = cntrs.port_rcv_packets;\r\nibp->z_local_link_integrity_errors =\r\ncntrs.local_link_integrity_errors;\r\nibp->z_excessive_buffer_overrun_errors =\r\ncntrs.excessive_buffer_overrun_errors;\r\nibp->z_vl15_dropped = cntrs.vl15_dropped;\r\nRCU_INIT_POINTER(ibp->rvp.qp[0], NULL);\r\nRCU_INIT_POINTER(ibp->rvp.qp[1], NULL);\r\n}\r\nstatic void qib_fill_device_attr(struct qib_devdata *dd)\r\n{\r\nstruct rvt_dev_info *rdi = &dd->verbs_dev.rdi;\r\nmemset(&rdi->dparms.props, 0, sizeof(rdi->dparms.props));\r\nrdi->dparms.props.max_pd = ib_qib_max_pds;\r\nrdi->dparms.props.max_ah = ib_qib_max_ahs;\r\nrdi->dparms.props.device_cap_flags = IB_DEVICE_BAD_PKEY_CNTR |\r\nIB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT |\r\nIB_DEVICE_SYS_IMAGE_GUID | IB_DEVICE_RC_RNR_NAK_GEN |\r\nIB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE;\r\nrdi->dparms.props.page_size_cap = PAGE_SIZE;\r\nrdi->dparms.props.vendor_id =\r\nQIB_SRC_OUI_1 << 16 | QIB_SRC_OUI_2 << 8 | QIB_SRC_OUI_3;\r\nrdi->dparms.props.vendor_part_id = dd->deviceid;\r\nrdi->dparms.props.hw_ver = dd->minrev;\r\nrdi->dparms.props.sys_image_guid = ib_qib_sys_image_guid;\r\nrdi->dparms.props.max_mr_size = ~0ULL;\r\nrdi->dparms.props.max_qp = ib_qib_max_qps;\r\nrdi->dparms.props.max_qp_wr = ib_qib_max_qp_wrs;\r\nrdi->dparms.props.max_sge = ib_qib_max_sges;\r\nrdi->dparms.props.max_sge_rd = ib_qib_max_sges;\r\nrdi->dparms.props.max_cq = ib_qib_max_cqs;\r\nrdi->dparms.props.max_cqe = ib_qib_max_cqes;\r\nrdi->dparms.props.max_ah = ib_qib_max_ahs;\r\nrdi->dparms.props.max_mr = rdi->lkey_table.max;\r\nrdi->dparms.props.max_fmr = rdi->lkey_table.max;\r\nrdi->dparms.props.max_map_per_fmr = 32767;\r\nrdi->dparms.props.max_qp_rd_atom = QIB_MAX_RDMA_ATOMIC;\r\nrdi->dparms.props.max_qp_init_rd_atom = 255;\r\nrdi->dparms.props.max_srq = ib_qib_max_srqs;\r\nrdi->dparms.props.max_srq_wr = ib_qib_max_srq_wrs;\r\nrdi->dparms.props.max_srq_sge = ib_qib_max_srq_sges;\r\nrdi->dparms.props.atomic_cap = IB_ATOMIC_GLOB;\r\nrdi->dparms.props.max_pkeys = qib_get_npkeys(dd);\r\nrdi->dparms.props.max_mcast_grp = ib_qib_max_mcast_grps;\r\nrdi->dparms.props.max_mcast_qp_attach = ib_qib_max_mcast_qp_attached;\r\nrdi->dparms.props.max_total_mcast_qp_attach =\r\nrdi->dparms.props.max_mcast_qp_attach *\r\nrdi->dparms.props.max_mcast_grp;\r\n}\r\nint qib_register_ib_device(struct qib_devdata *dd)\r\n{\r\nstruct qib_ibdev *dev = &dd->verbs_dev;\r\nstruct ib_device *ibdev = &dev->rdi.ibdev;\r\nstruct qib_pportdata *ppd = dd->pport;\r\nunsigned i, ctxt;\r\nint ret;\r\nget_random_bytes(&dev->qp_rnd, sizeof(dev->qp_rnd));\r\nfor (i = 0; i < dd->num_pports; i++)\r\ninit_ibport(ppd + i);\r\nsetup_timer(&dev->mem_timer, mem_timer, (unsigned long)dev);\r\nqpt_mask = dd->qpn_mask;\r\nINIT_LIST_HEAD(&dev->piowait);\r\nINIT_LIST_HEAD(&dev->dmawait);\r\nINIT_LIST_HEAD(&dev->txwait);\r\nINIT_LIST_HEAD(&dev->memwait);\r\nINIT_LIST_HEAD(&dev->txreq_free);\r\nif (ppd->sdma_descq_cnt) {\r\ndev->pio_hdrs = dma_alloc_coherent(&dd->pcidev->dev,\r\nppd->sdma_descq_cnt *\r\nsizeof(struct qib_pio_header),\r\n&dev->pio_hdrs_phys,\r\nGFP_KERNEL);\r\nif (!dev->pio_hdrs) {\r\nret = -ENOMEM;\r\ngoto err_hdrs;\r\n}\r\n}\r\nfor (i = 0; i < ppd->sdma_descq_cnt; i++) {\r\nstruct qib_verbs_txreq *tx;\r\ntx = kzalloc(sizeof(*tx), GFP_KERNEL);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err_tx;\r\n}\r\ntx->hdr_inx = i;\r\nlist_add(&tx->txreq.list, &dev->txreq_free);\r\n}\r\nif (!ib_qib_sys_image_guid)\r\nib_qib_sys_image_guid = ppd->guid;\r\nstrlcpy(ibdev->name, "qib%d", IB_DEVICE_NAME_MAX);\r\nibdev->owner = THIS_MODULE;\r\nibdev->node_guid = ppd->guid;\r\nibdev->phys_port_cnt = dd->num_pports;\r\nibdev->dma_device = &dd->pcidev->dev;\r\nibdev->modify_device = qib_modify_device;\r\nibdev->process_mad = qib_process_mad;\r\nsnprintf(ibdev->node_desc, sizeof(ibdev->node_desc),\r\n"Intel Infiniband HCA %s", init_utsname()->nodename);\r\ndd->verbs_dev.rdi.driver_f.port_callback = qib_create_port_files;\r\ndd->verbs_dev.rdi.driver_f.get_card_name = qib_get_card_name;\r\ndd->verbs_dev.rdi.driver_f.get_pci_dev = qib_get_pci_dev;\r\ndd->verbs_dev.rdi.driver_f.check_ah = qib_check_ah;\r\ndd->verbs_dev.rdi.driver_f.check_send_wqe = qib_check_send_wqe;\r\ndd->verbs_dev.rdi.driver_f.notify_new_ah = qib_notify_new_ah;\r\ndd->verbs_dev.rdi.driver_f.alloc_qpn = qib_alloc_qpn;\r\ndd->verbs_dev.rdi.driver_f.qp_priv_alloc = qib_qp_priv_alloc;\r\ndd->verbs_dev.rdi.driver_f.qp_priv_free = qib_qp_priv_free;\r\ndd->verbs_dev.rdi.driver_f.free_all_qps = qib_free_all_qps;\r\ndd->verbs_dev.rdi.driver_f.notify_qp_reset = qib_notify_qp_reset;\r\ndd->verbs_dev.rdi.driver_f.do_send = qib_do_send;\r\ndd->verbs_dev.rdi.driver_f.schedule_send = qib_schedule_send;\r\ndd->verbs_dev.rdi.driver_f.quiesce_qp = qib_quiesce_qp;\r\ndd->verbs_dev.rdi.driver_f.stop_send_queue = qib_stop_send_queue;\r\ndd->verbs_dev.rdi.driver_f.flush_qp_waiters = qib_flush_qp_waiters;\r\ndd->verbs_dev.rdi.driver_f.notify_error_qp = qib_notify_error_qp;\r\ndd->verbs_dev.rdi.driver_f.mtu_to_path_mtu = qib_mtu_to_path_mtu;\r\ndd->verbs_dev.rdi.driver_f.mtu_from_qp = qib_mtu_from_qp;\r\ndd->verbs_dev.rdi.driver_f.get_pmtu_from_attr = qib_get_pmtu_from_attr;\r\ndd->verbs_dev.rdi.driver_f.schedule_send_no_lock = _qib_schedule_send;\r\ndd->verbs_dev.rdi.driver_f.query_port_state = qib_query_port;\r\ndd->verbs_dev.rdi.driver_f.shut_down_port = qib_shut_down_port;\r\ndd->verbs_dev.rdi.driver_f.cap_mask_chg = qib_cap_mask_chg;\r\ndd->verbs_dev.rdi.driver_f.notify_create_mad_agent =\r\nqib_notify_create_mad_agent;\r\ndd->verbs_dev.rdi.driver_f.notify_free_mad_agent =\r\nqib_notify_free_mad_agent;\r\ndd->verbs_dev.rdi.dparms.max_rdma_atomic = QIB_MAX_RDMA_ATOMIC;\r\ndd->verbs_dev.rdi.driver_f.get_guid_be = qib_get_guid_be;\r\ndd->verbs_dev.rdi.dparms.lkey_table_size = qib_lkey_table_size;\r\ndd->verbs_dev.rdi.dparms.qp_table_size = ib_qib_qp_table_size;\r\ndd->verbs_dev.rdi.dparms.qpn_start = 1;\r\ndd->verbs_dev.rdi.dparms.qpn_res_start = QIB_KD_QP;\r\ndd->verbs_dev.rdi.dparms.qpn_res_end = QIB_KD_QP;\r\ndd->verbs_dev.rdi.dparms.qpn_inc = 1;\r\ndd->verbs_dev.rdi.dparms.qos_shift = 1;\r\ndd->verbs_dev.rdi.dparms.psn_mask = QIB_PSN_MASK;\r\ndd->verbs_dev.rdi.dparms.psn_shift = QIB_PSN_SHIFT;\r\ndd->verbs_dev.rdi.dparms.psn_modify_mask = QIB_PSN_MASK;\r\ndd->verbs_dev.rdi.dparms.nports = dd->num_pports;\r\ndd->verbs_dev.rdi.dparms.npkeys = qib_get_npkeys(dd);\r\ndd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;\r\ndd->verbs_dev.rdi.dparms.core_cap_flags = RDMA_CORE_PORT_IBA_IB;\r\ndd->verbs_dev.rdi.dparms.max_mad_size = IB_MGMT_MAD_SIZE;\r\nsnprintf(dd->verbs_dev.rdi.dparms.cq_name,\r\nsizeof(dd->verbs_dev.rdi.dparms.cq_name),\r\n"qib_cq%d", dd->unit);\r\nqib_fill_device_attr(dd);\r\nppd = dd->pport;\r\nfor (i = 0; i < dd->num_pports; i++, ppd++) {\r\nctxt = ppd->hw_pidx;\r\nrvt_init_port(&dd->verbs_dev.rdi,\r\n&ppd->ibport_data.rvp,\r\ni,\r\ndd->rcd[ctxt]->pkeys);\r\n}\r\nret = rvt_register_device(&dd->verbs_dev.rdi);\r\nif (ret)\r\ngoto err_tx;\r\nret = qib_verbs_register_sysfs(dd);\r\nif (ret)\r\ngoto err_class;\r\nreturn ret;\r\nerr_class:\r\nrvt_unregister_device(&dd->verbs_dev.rdi);\r\nerr_tx:\r\nwhile (!list_empty(&dev->txreq_free)) {\r\nstruct list_head *l = dev->txreq_free.next;\r\nstruct qib_verbs_txreq *tx;\r\nlist_del(l);\r\ntx = list_entry(l, struct qib_verbs_txreq, txreq.list);\r\nkfree(tx);\r\n}\r\nif (ppd->sdma_descq_cnt)\r\ndma_free_coherent(&dd->pcidev->dev,\r\nppd->sdma_descq_cnt *\r\nsizeof(struct qib_pio_header),\r\ndev->pio_hdrs, dev->pio_hdrs_phys);\r\nerr_hdrs:\r\nqib_dev_err(dd, "cannot register verbs: %d!\n", -ret);\r\nreturn ret;\r\n}\r\nvoid qib_unregister_ib_device(struct qib_devdata *dd)\r\n{\r\nstruct qib_ibdev *dev = &dd->verbs_dev;\r\nqib_verbs_unregister_sysfs(dd);\r\nrvt_unregister_device(&dd->verbs_dev.rdi);\r\nif (!list_empty(&dev->piowait))\r\nqib_dev_err(dd, "piowait list not empty!\n");\r\nif (!list_empty(&dev->dmawait))\r\nqib_dev_err(dd, "dmawait list not empty!\n");\r\nif (!list_empty(&dev->txwait))\r\nqib_dev_err(dd, "txwait list not empty!\n");\r\nif (!list_empty(&dev->memwait))\r\nqib_dev_err(dd, "memwait list not empty!\n");\r\ndel_timer_sync(&dev->mem_timer);\r\nwhile (!list_empty(&dev->txreq_free)) {\r\nstruct list_head *l = dev->txreq_free.next;\r\nstruct qib_verbs_txreq *tx;\r\nlist_del(l);\r\ntx = list_entry(l, struct qib_verbs_txreq, txreq.list);\r\nkfree(tx);\r\n}\r\nif (dd->pport->sdma_descq_cnt)\r\ndma_free_coherent(&dd->pcidev->dev,\r\ndd->pport->sdma_descq_cnt *\r\nsizeof(struct qib_pio_header),\r\ndev->pio_hdrs, dev->pio_hdrs_phys);\r\n}\r\nvoid _qib_schedule_send(struct rvt_qp *qp)\r\n{\r\nstruct qib_ibport *ibp =\r\nto_iport(qp->ibqp.device, qp->port_num);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nstruct qib_qp_priv *priv = qp->priv;\r\nqueue_work(ppd->qib_wq, &priv->s_work);\r\n}\r\nvoid qib_schedule_send(struct rvt_qp *qp)\r\n{\r\nif (qib_send_ok(qp))\r\n_qib_schedule_send(qp);\r\n}
