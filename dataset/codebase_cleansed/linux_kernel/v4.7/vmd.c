static inline struct vmd_dev *vmd_from_bus(struct pci_bus *bus)\r\n{\r\nreturn container_of(bus->sysdata, struct vmd_dev, sysdata);\r\n}\r\nstatic void vmd_compose_msi_msg(struct irq_data *data, struct msi_msg *msg)\r\n{\r\nstruct vmd_irq *vmdirq = data->chip_data;\r\nstruct vmd_irq_list *irq = vmdirq->irq;\r\nmsg->address_hi = MSI_ADDR_BASE_HI;\r\nmsg->address_lo = MSI_ADDR_BASE_LO | MSI_ADDR_DEST_ID(irq->index);\r\nmsg->data = 0;\r\n}\r\nstatic void vmd_irq_enable(struct irq_data *data)\r\n{\r\nstruct vmd_irq *vmdirq = data->chip_data;\r\nraw_spin_lock(&list_lock);\r\nlist_add_tail_rcu(&vmdirq->node, &vmdirq->irq->irq_list);\r\nraw_spin_unlock(&list_lock);\r\ndata->chip->irq_unmask(data);\r\n}\r\nstatic void vmd_irq_disable(struct irq_data *data)\r\n{\r\nstruct vmd_irq *vmdirq = data->chip_data;\r\ndata->chip->irq_mask(data);\r\nraw_spin_lock(&list_lock);\r\nlist_del_rcu(&vmdirq->node);\r\nraw_spin_unlock(&list_lock);\r\n}\r\nstatic int vmd_irq_set_affinity(struct irq_data *data,\r\nconst struct cpumask *dest, bool force)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic irq_hw_number_t vmd_get_hwirq(struct msi_domain_info *info,\r\nmsi_alloc_info_t *arg)\r\n{\r\nreturn 0;\r\n}\r\nstatic struct vmd_irq_list *vmd_next_irq(struct vmd_dev *vmd)\r\n{\r\nint i, best = 0;\r\nraw_spin_lock(&list_lock);\r\nfor (i = 1; i < vmd->msix_count; i++)\r\nif (vmd->irqs[i].count < vmd->irqs[best].count)\r\nbest = i;\r\nvmd->irqs[best].count++;\r\nraw_spin_unlock(&list_lock);\r\nreturn &vmd->irqs[best];\r\n}\r\nstatic int vmd_msi_init(struct irq_domain *domain, struct msi_domain_info *info,\r\nunsigned int virq, irq_hw_number_t hwirq,\r\nmsi_alloc_info_t *arg)\r\n{\r\nstruct vmd_dev *vmd = vmd_from_bus(msi_desc_to_pci_dev(arg->desc)->bus);\r\nstruct vmd_irq *vmdirq = kzalloc(sizeof(*vmdirq), GFP_KERNEL);\r\nif (!vmdirq)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&vmdirq->node);\r\nvmdirq->irq = vmd_next_irq(vmd);\r\nvmdirq->virq = virq;\r\nirq_domain_set_info(domain, virq, vmdirq->irq->vmd_vector, info->chip,\r\nvmdirq, handle_simple_irq, vmd, NULL);\r\nreturn 0;\r\n}\r\nstatic void vmd_msi_free(struct irq_domain *domain,\r\nstruct msi_domain_info *info, unsigned int virq)\r\n{\r\nstruct vmd_irq *vmdirq = irq_get_chip_data(virq);\r\nraw_spin_lock(&list_lock);\r\nvmdirq->irq->count--;\r\nraw_spin_unlock(&list_lock);\r\nkfree_rcu(vmdirq, rcu);\r\n}\r\nstatic int vmd_msi_prepare(struct irq_domain *domain, struct device *dev,\r\nint nvec, msi_alloc_info_t *arg)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct vmd_dev *vmd = vmd_from_bus(pdev->bus);\r\nif (nvec > vmd->msix_count)\r\nreturn vmd->msix_count;\r\nmemset(arg, 0, sizeof(*arg));\r\nreturn 0;\r\n}\r\nstatic void vmd_set_desc(msi_alloc_info_t *arg, struct msi_desc *desc)\r\n{\r\narg->desc = desc;\r\n}\r\nstatic struct device *to_vmd_dev(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct vmd_dev *vmd = vmd_from_bus(pdev->bus);\r\nreturn &vmd->dev->dev;\r\n}\r\nstatic struct dma_map_ops *vmd_dma_ops(struct device *dev)\r\n{\r\nreturn to_vmd_dev(dev)->archdata.dma_ops;\r\n}\r\nstatic void *vmd_alloc(struct device *dev, size_t size, dma_addr_t *addr,\r\ngfp_t flag, struct dma_attrs *attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->alloc(to_vmd_dev(dev), size, addr, flag,\r\nattrs);\r\n}\r\nstatic void vmd_free(struct device *dev, size_t size, void *vaddr,\r\ndma_addr_t addr, struct dma_attrs *attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->free(to_vmd_dev(dev), size, vaddr, addr,\r\nattrs);\r\n}\r\nstatic int vmd_mmap(struct device *dev, struct vm_area_struct *vma,\r\nvoid *cpu_addr, dma_addr_t addr, size_t size,\r\nstruct dma_attrs *attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->mmap(to_vmd_dev(dev), vma, cpu_addr, addr,\r\nsize, attrs);\r\n}\r\nstatic int vmd_get_sgtable(struct device *dev, struct sg_table *sgt,\r\nvoid *cpu_addr, dma_addr_t addr, size_t size,\r\nstruct dma_attrs *attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->get_sgtable(to_vmd_dev(dev), sgt, cpu_addr,\r\naddr, size, attrs);\r\n}\r\nstatic dma_addr_t vmd_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->map_page(to_vmd_dev(dev), page, offset, size,\r\ndir, attrs);\r\n}\r\nstatic void vmd_unmap_page(struct device *dev, dma_addr_t addr, size_t size,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\nvmd_dma_ops(dev)->unmap_page(to_vmd_dev(dev), addr, size, dir, attrs);\r\n}\r\nstatic int vmd_map_sg(struct device *dev, struct scatterlist *sg, int nents,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->map_sg(to_vmd_dev(dev), sg, nents, dir, attrs);\r\n}\r\nstatic void vmd_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\nvmd_dma_ops(dev)->unmap_sg(to_vmd_dev(dev), sg, nents, dir, attrs);\r\n}\r\nstatic void vmd_sync_single_for_cpu(struct device *dev, dma_addr_t addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_single_for_cpu(to_vmd_dev(dev), addr, size, dir);\r\n}\r\nstatic void vmd_sync_single_for_device(struct device *dev, dma_addr_t addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_single_for_device(to_vmd_dev(dev), addr, size,\r\ndir);\r\n}\r\nstatic void vmd_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_sg_for_cpu(to_vmd_dev(dev), sg, nents, dir);\r\n}\r\nstatic void vmd_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_sg_for_device(to_vmd_dev(dev), sg, nents, dir);\r\n}\r\nstatic int vmd_mapping_error(struct device *dev, dma_addr_t addr)\r\n{\r\nreturn vmd_dma_ops(dev)->mapping_error(to_vmd_dev(dev), addr);\r\n}\r\nstatic int vmd_dma_supported(struct device *dev, u64 mask)\r\n{\r\nreturn vmd_dma_ops(dev)->dma_supported(to_vmd_dev(dev), mask);\r\n}\r\nstatic u64 vmd_get_required_mask(struct device *dev)\r\n{\r\nreturn vmd_dma_ops(dev)->get_required_mask(to_vmd_dev(dev));\r\n}\r\nstatic void vmd_teardown_dma_ops(struct vmd_dev *vmd)\r\n{\r\nstruct dma_domain *domain = &vmd->dma_domain;\r\nif (vmd->dev->dev.archdata.dma_ops)\r\ndel_dma_domain(domain);\r\n}\r\nstatic void vmd_setup_dma_ops(struct vmd_dev *vmd)\r\n{\r\nconst struct dma_map_ops *source = vmd->dev->dev.archdata.dma_ops;\r\nstruct dma_map_ops *dest = &vmd->dma_ops;\r\nstruct dma_domain *domain = &vmd->dma_domain;\r\ndomain->domain_nr = vmd->sysdata.domain;\r\ndomain->dma_ops = dest;\r\nif (!source)\r\nreturn;\r\nASSIGN_VMD_DMA_OPS(source, dest, alloc);\r\nASSIGN_VMD_DMA_OPS(source, dest, free);\r\nASSIGN_VMD_DMA_OPS(source, dest, mmap);\r\nASSIGN_VMD_DMA_OPS(source, dest, get_sgtable);\r\nASSIGN_VMD_DMA_OPS(source, dest, map_page);\r\nASSIGN_VMD_DMA_OPS(source, dest, unmap_page);\r\nASSIGN_VMD_DMA_OPS(source, dest, map_sg);\r\nASSIGN_VMD_DMA_OPS(source, dest, unmap_sg);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_single_for_cpu);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_single_for_device);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_sg_for_cpu);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_sg_for_device);\r\nASSIGN_VMD_DMA_OPS(source, dest, mapping_error);\r\nASSIGN_VMD_DMA_OPS(source, dest, dma_supported);\r\n#ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK\r\nASSIGN_VMD_DMA_OPS(source, dest, get_required_mask);\r\n#endif\r\nadd_dma_domain(domain);\r\n}\r\nstatic void vmd_teardown_dma_ops(struct vmd_dev *vmd) {}\r\nstatic void vmd_setup_dma_ops(struct vmd_dev *vmd) {}\r\nstatic char __iomem *vmd_cfg_addr(struct vmd_dev *vmd, struct pci_bus *bus,\r\nunsigned int devfn, int reg, int len)\r\n{\r\nchar __iomem *addr = vmd->cfgbar +\r\n(bus->number << 20) + (devfn << 12) + reg;\r\nif ((addr - vmd->cfgbar) + len >=\r\nresource_size(&vmd->dev->resource[VMD_CFGBAR]))\r\nreturn NULL;\r\nreturn addr;\r\n}\r\nstatic int vmd_pci_read(struct pci_bus *bus, unsigned int devfn, int reg,\r\nint len, u32 *value)\r\n{\r\nstruct vmd_dev *vmd = vmd_from_bus(bus);\r\nchar __iomem *addr = vmd_cfg_addr(vmd, bus, devfn, reg, len);\r\nunsigned long flags;\r\nint ret = 0;\r\nif (!addr)\r\nreturn -EFAULT;\r\nspin_lock_irqsave(&vmd->cfg_lock, flags);\r\nswitch (len) {\r\ncase 1:\r\n*value = readb(addr);\r\nbreak;\r\ncase 2:\r\n*value = readw(addr);\r\nbreak;\r\ncase 4:\r\n*value = readl(addr);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&vmd->cfg_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int vmd_pci_write(struct pci_bus *bus, unsigned int devfn, int reg,\r\nint len, u32 value)\r\n{\r\nstruct vmd_dev *vmd = vmd_from_bus(bus);\r\nchar __iomem *addr = vmd_cfg_addr(vmd, bus, devfn, reg, len);\r\nunsigned long flags;\r\nint ret = 0;\r\nif (!addr)\r\nreturn -EFAULT;\r\nspin_lock_irqsave(&vmd->cfg_lock, flags);\r\nswitch (len) {\r\ncase 1:\r\nwriteb(value, addr);\r\nreadb(addr);\r\nbreak;\r\ncase 2:\r\nwritew(value, addr);\r\nreadw(addr);\r\nbreak;\r\ncase 4:\r\nwritel(value, addr);\r\nreadl(addr);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&vmd->cfg_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void vmd_attach_resources(struct vmd_dev *vmd)\r\n{\r\nvmd->dev->resource[VMD_MEMBAR1].child = &vmd->resources[1];\r\nvmd->dev->resource[VMD_MEMBAR2].child = &vmd->resources[2];\r\n}\r\nstatic void vmd_detach_resources(struct vmd_dev *vmd)\r\n{\r\nvmd->dev->resource[VMD_MEMBAR1].child = NULL;\r\nvmd->dev->resource[VMD_MEMBAR2].child = NULL;\r\n}\r\nstatic int vmd_find_free_domain(void)\r\n{\r\nint domain = 0xffff;\r\nstruct pci_bus *bus = NULL;\r\nwhile ((bus = pci_find_next_bus(bus)) != NULL)\r\ndomain = max_t(int, domain, pci_domain_nr(bus));\r\nreturn domain + 1;\r\n}\r\nstatic int vmd_enable_domain(struct vmd_dev *vmd)\r\n{\r\nstruct pci_sysdata *sd = &vmd->sysdata;\r\nstruct resource *res;\r\nu32 upper_bits;\r\nunsigned long flags;\r\nLIST_HEAD(resources);\r\nres = &vmd->dev->resource[VMD_CFGBAR];\r\nvmd->resources[0] = (struct resource) {\r\n.name = "VMD CFGBAR",\r\n.start = 0,\r\n.end = (resource_size(res) >> 20) - 1,\r\n.flags = IORESOURCE_BUS | IORESOURCE_PCI_FIXED,\r\n};\r\nres = &vmd->dev->resource[VMD_MEMBAR1];\r\nupper_bits = upper_32_bits(res->end);\r\nflags = res->flags & ~IORESOURCE_SIZEALIGN;\r\nif (!upper_bits)\r\nflags &= ~IORESOURCE_MEM_64;\r\nvmd->resources[1] = (struct resource) {\r\n.name = "VMD MEMBAR1",\r\n.start = res->start,\r\n.end = res->end,\r\n.flags = flags,\r\n.parent = res,\r\n};\r\nres = &vmd->dev->resource[VMD_MEMBAR2];\r\nupper_bits = upper_32_bits(res->end);\r\nflags = res->flags & ~IORESOURCE_SIZEALIGN;\r\nif (!upper_bits)\r\nflags &= ~IORESOURCE_MEM_64;\r\nvmd->resources[2] = (struct resource) {\r\n.name = "VMD MEMBAR2",\r\n.start = res->start + 0x2000,\r\n.end = res->end,\r\n.flags = flags,\r\n.parent = res,\r\n};\r\nsd->domain = vmd_find_free_domain();\r\nif (sd->domain < 0)\r\nreturn sd->domain;\r\nsd->node = pcibus_to_node(vmd->dev->bus);\r\nvmd->irq_domain = pci_msi_create_irq_domain(NULL, &vmd_msi_domain_info,\r\nNULL);\r\nif (!vmd->irq_domain)\r\nreturn -ENODEV;\r\npci_add_resource(&resources, &vmd->resources[0]);\r\npci_add_resource(&resources, &vmd->resources[1]);\r\npci_add_resource(&resources, &vmd->resources[2]);\r\nvmd->bus = pci_create_root_bus(&vmd->dev->dev, 0, &vmd_ops, sd,\r\n&resources);\r\nif (!vmd->bus) {\r\npci_free_resource_list(&resources);\r\nirq_domain_remove(vmd->irq_domain);\r\nreturn -ENODEV;\r\n}\r\nvmd_attach_resources(vmd);\r\nvmd_setup_dma_ops(vmd);\r\ndev_set_msi_domain(&vmd->bus->dev, vmd->irq_domain);\r\npci_rescan_bus(vmd->bus);\r\nWARN(sysfs_create_link(&vmd->dev->dev.kobj, &vmd->bus->dev.kobj,\r\n"domain"), "Can't create symlink to domain\n");\r\nreturn 0;\r\n}\r\nstatic irqreturn_t vmd_irq(int irq, void *data)\r\n{\r\nstruct vmd_irq_list *irqs = data;\r\nstruct vmd_irq *vmdirq;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(vmdirq, &irqs->irq_list, node)\r\ngeneric_handle_irq(vmdirq->virq);\r\nrcu_read_unlock();\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int vmd_probe(struct pci_dev *dev, const struct pci_device_id *id)\r\n{\r\nstruct vmd_dev *vmd;\r\nint i, err;\r\nif (resource_size(&dev->resource[VMD_CFGBAR]) < (1 << 20))\r\nreturn -ENOMEM;\r\nvmd = devm_kzalloc(&dev->dev, sizeof(*vmd), GFP_KERNEL);\r\nif (!vmd)\r\nreturn -ENOMEM;\r\nvmd->dev = dev;\r\nerr = pcim_enable_device(dev);\r\nif (err < 0)\r\nreturn err;\r\nvmd->cfgbar = pcim_iomap(dev, VMD_CFGBAR, 0);\r\nif (!vmd->cfgbar)\r\nreturn -ENOMEM;\r\npci_set_master(dev);\r\nif (dma_set_mask_and_coherent(&dev->dev, DMA_BIT_MASK(64)) &&\r\ndma_set_mask_and_coherent(&dev->dev, DMA_BIT_MASK(32)))\r\nreturn -ENODEV;\r\nvmd->msix_count = pci_msix_vec_count(dev);\r\nif (vmd->msix_count < 0)\r\nreturn -ENODEV;\r\nvmd->irqs = devm_kcalloc(&dev->dev, vmd->msix_count, sizeof(*vmd->irqs),\r\nGFP_KERNEL);\r\nif (!vmd->irqs)\r\nreturn -ENOMEM;\r\nvmd->msix_entries = devm_kcalloc(&dev->dev, vmd->msix_count,\r\nsizeof(*vmd->msix_entries),\r\nGFP_KERNEL);\r\nif (!vmd->msix_entries)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < vmd->msix_count; i++)\r\nvmd->msix_entries[i].entry = i;\r\nvmd->msix_count = pci_enable_msix_range(vmd->dev, vmd->msix_entries, 1,\r\nvmd->msix_count);\r\nif (vmd->msix_count < 0)\r\nreturn vmd->msix_count;\r\nfor (i = 0; i < vmd->msix_count; i++) {\r\nINIT_LIST_HEAD(&vmd->irqs[i].irq_list);\r\nvmd->irqs[i].vmd_vector = vmd->msix_entries[i].vector;\r\nvmd->irqs[i].index = i;\r\nerr = devm_request_irq(&dev->dev, vmd->irqs[i].vmd_vector,\r\nvmd_irq, 0, "vmd", &vmd->irqs[i]);\r\nif (err)\r\nreturn err;\r\n}\r\nspin_lock_init(&vmd->cfg_lock);\r\npci_set_drvdata(dev, vmd);\r\nerr = vmd_enable_domain(vmd);\r\nif (err)\r\nreturn err;\r\ndev_info(&vmd->dev->dev, "Bound to PCI domain %04x\n",\r\nvmd->sysdata.domain);\r\nreturn 0;\r\n}\r\nstatic void vmd_remove(struct pci_dev *dev)\r\n{\r\nstruct vmd_dev *vmd = pci_get_drvdata(dev);\r\nvmd_detach_resources(vmd);\r\npci_set_drvdata(dev, NULL);\r\nsysfs_remove_link(&vmd->dev->dev.kobj, "domain");\r\npci_stop_root_bus(vmd->bus);\r\npci_remove_root_bus(vmd->bus);\r\nvmd_teardown_dma_ops(vmd);\r\nirq_domain_remove(vmd->irq_domain);\r\n}\r\nstatic int vmd_suspend(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\npci_save_state(pdev);\r\nreturn 0;\r\n}\r\nstatic int vmd_resume(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\npci_restore_state(pdev);\r\nreturn 0;\r\n}
