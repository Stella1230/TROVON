static struct cfq_rb_root *st_for(struct cfq_group *cfqg,\r\nenum wl_class_t class,\r\nenum wl_type_t type)\r\n{\r\nif (!cfqg)\r\nreturn NULL;\r\nif (class == IDLE_WORKLOAD)\r\nreturn &cfqg->service_tree_idle;\r\nreturn &cfqg->service_trees[class][type];\r\n}\r\nstatic void cfqg_stats_update_group_wait_time(struct cfqg_stats *stats)\r\n{\r\nunsigned long long now;\r\nif (!cfqg_stats_waiting(stats))\r\nreturn;\r\nnow = sched_clock();\r\nif (time_after64(now, stats->start_group_wait_time))\r\nblkg_stat_add(&stats->group_wait_time,\r\nnow - stats->start_group_wait_time);\r\ncfqg_stats_clear_waiting(stats);\r\n}\r\nstatic void cfqg_stats_set_start_group_wait_time(struct cfq_group *cfqg,\r\nstruct cfq_group *curr_cfqg)\r\n{\r\nstruct cfqg_stats *stats = &cfqg->stats;\r\nif (cfqg_stats_waiting(stats))\r\nreturn;\r\nif (cfqg == curr_cfqg)\r\nreturn;\r\nstats->start_group_wait_time = sched_clock();\r\ncfqg_stats_mark_waiting(stats);\r\n}\r\nstatic void cfqg_stats_end_empty_time(struct cfqg_stats *stats)\r\n{\r\nunsigned long long now;\r\nif (!cfqg_stats_empty(stats))\r\nreturn;\r\nnow = sched_clock();\r\nif (time_after64(now, stats->start_empty_time))\r\nblkg_stat_add(&stats->empty_time,\r\nnow - stats->start_empty_time);\r\ncfqg_stats_clear_empty(stats);\r\n}\r\nstatic void cfqg_stats_update_dequeue(struct cfq_group *cfqg)\r\n{\r\nblkg_stat_add(&cfqg->stats.dequeue, 1);\r\n}\r\nstatic void cfqg_stats_set_start_empty_time(struct cfq_group *cfqg)\r\n{\r\nstruct cfqg_stats *stats = &cfqg->stats;\r\nif (blkg_rwstat_total(&stats->queued))\r\nreturn;\r\nif (cfqg_stats_empty(stats))\r\nreturn;\r\nstats->start_empty_time = sched_clock();\r\ncfqg_stats_mark_empty(stats);\r\n}\r\nstatic void cfqg_stats_update_idle_time(struct cfq_group *cfqg)\r\n{\r\nstruct cfqg_stats *stats = &cfqg->stats;\r\nif (cfqg_stats_idling(stats)) {\r\nunsigned long long now = sched_clock();\r\nif (time_after64(now, stats->start_idle_time))\r\nblkg_stat_add(&stats->idle_time,\r\nnow - stats->start_idle_time);\r\ncfqg_stats_clear_idling(stats);\r\n}\r\n}\r\nstatic void cfqg_stats_set_start_idle_time(struct cfq_group *cfqg)\r\n{\r\nstruct cfqg_stats *stats = &cfqg->stats;\r\nBUG_ON(cfqg_stats_idling(stats));\r\nstats->start_idle_time = sched_clock();\r\ncfqg_stats_mark_idling(stats);\r\n}\r\nstatic void cfqg_stats_update_avg_queue_size(struct cfq_group *cfqg)\r\n{\r\nstruct cfqg_stats *stats = &cfqg->stats;\r\nblkg_stat_add(&stats->avg_queue_size_sum,\r\nblkg_rwstat_total(&stats->queued));\r\nblkg_stat_add(&stats->avg_queue_size_samples, 1);\r\ncfqg_stats_update_group_wait_time(stats);\r\n}\r\nstatic inline void cfqg_stats_set_start_group_wait_time(struct cfq_group *cfqg, struct cfq_group *curr_cfqg) { }\r\nstatic inline void cfqg_stats_end_empty_time(struct cfqg_stats *stats) { }\r\nstatic inline void cfqg_stats_update_dequeue(struct cfq_group *cfqg) { }\r\nstatic inline void cfqg_stats_set_start_empty_time(struct cfq_group *cfqg) { }\r\nstatic inline void cfqg_stats_update_idle_time(struct cfq_group *cfqg) { }\r\nstatic inline void cfqg_stats_set_start_idle_time(struct cfq_group *cfqg) { }\r\nstatic inline void cfqg_stats_update_avg_queue_size(struct cfq_group *cfqg) { }\r\nstatic inline struct cfq_group *pd_to_cfqg(struct blkg_policy_data *pd)\r\n{\r\nreturn pd ? container_of(pd, struct cfq_group, pd) : NULL;\r\n}\r\nstatic struct cfq_group_data\r\n*cpd_to_cfqgd(struct blkcg_policy_data *cpd)\r\n{\r\nreturn cpd ? container_of(cpd, struct cfq_group_data, cpd) : NULL;\r\n}\r\nstatic inline struct blkcg_gq *cfqg_to_blkg(struct cfq_group *cfqg)\r\n{\r\nreturn pd_to_blkg(&cfqg->pd);\r\n}\r\nstatic inline struct cfq_group *blkg_to_cfqg(struct blkcg_gq *blkg)\r\n{\r\nreturn pd_to_cfqg(blkg_to_pd(blkg, &blkcg_policy_cfq));\r\n}\r\nstatic struct cfq_group_data *blkcg_to_cfqgd(struct blkcg *blkcg)\r\n{\r\nreturn cpd_to_cfqgd(blkcg_to_cpd(blkcg, &blkcg_policy_cfq));\r\n}\r\nstatic inline struct cfq_group *cfqg_parent(struct cfq_group *cfqg)\r\n{\r\nstruct blkcg_gq *pblkg = cfqg_to_blkg(cfqg)->parent;\r\nreturn pblkg ? blkg_to_cfqg(pblkg) : NULL;\r\n}\r\nstatic inline bool cfqg_is_descendant(struct cfq_group *cfqg,\r\nstruct cfq_group *ancestor)\r\n{\r\nreturn cgroup_is_descendant(cfqg_to_blkg(cfqg)->blkcg->css.cgroup,\r\ncfqg_to_blkg(ancestor)->blkcg->css.cgroup);\r\n}\r\nstatic inline void cfqg_get(struct cfq_group *cfqg)\r\n{\r\nreturn blkg_get(cfqg_to_blkg(cfqg));\r\n}\r\nstatic inline void cfqg_put(struct cfq_group *cfqg)\r\n{\r\nreturn blkg_put(cfqg_to_blkg(cfqg));\r\n}\r\nstatic inline void cfqg_stats_update_io_add(struct cfq_group *cfqg,\r\nstruct cfq_group *curr_cfqg, int rw)\r\n{\r\nblkg_rwstat_add(&cfqg->stats.queued, rw, 1);\r\ncfqg_stats_end_empty_time(&cfqg->stats);\r\ncfqg_stats_set_start_group_wait_time(cfqg, curr_cfqg);\r\n}\r\nstatic inline void cfqg_stats_update_timeslice_used(struct cfq_group *cfqg,\r\nunsigned long time, unsigned long unaccounted_time)\r\n{\r\nblkg_stat_add(&cfqg->stats.time, time);\r\n#ifdef CONFIG_DEBUG_BLK_CGROUP\r\nblkg_stat_add(&cfqg->stats.unaccounted_time, unaccounted_time);\r\n#endif\r\n}\r\nstatic inline void cfqg_stats_update_io_remove(struct cfq_group *cfqg, int rw)\r\n{\r\nblkg_rwstat_add(&cfqg->stats.queued, rw, -1);\r\n}\r\nstatic inline void cfqg_stats_update_io_merged(struct cfq_group *cfqg, int rw)\r\n{\r\nblkg_rwstat_add(&cfqg->stats.merged, rw, 1);\r\n}\r\nstatic inline void cfqg_stats_update_completion(struct cfq_group *cfqg,\r\nuint64_t start_time, uint64_t io_start_time, int rw)\r\n{\r\nstruct cfqg_stats *stats = &cfqg->stats;\r\nunsigned long long now = sched_clock();\r\nif (time_after64(now, io_start_time))\r\nblkg_rwstat_add(&stats->service_time, rw, now - io_start_time);\r\nif (time_after64(io_start_time, start_time))\r\nblkg_rwstat_add(&stats->wait_time, rw,\r\nio_start_time - start_time);\r\n}\r\nstatic void cfqg_stats_reset(struct cfqg_stats *stats)\r\n{\r\nblkg_rwstat_reset(&stats->merged);\r\nblkg_rwstat_reset(&stats->service_time);\r\nblkg_rwstat_reset(&stats->wait_time);\r\nblkg_stat_reset(&stats->time);\r\n#ifdef CONFIG_DEBUG_BLK_CGROUP\r\nblkg_stat_reset(&stats->unaccounted_time);\r\nblkg_stat_reset(&stats->avg_queue_size_sum);\r\nblkg_stat_reset(&stats->avg_queue_size_samples);\r\nblkg_stat_reset(&stats->dequeue);\r\nblkg_stat_reset(&stats->group_wait_time);\r\nblkg_stat_reset(&stats->idle_time);\r\nblkg_stat_reset(&stats->empty_time);\r\n#endif\r\n}\r\nstatic void cfqg_stats_add_aux(struct cfqg_stats *to, struct cfqg_stats *from)\r\n{\r\nblkg_rwstat_add_aux(&to->merged, &from->merged);\r\nblkg_rwstat_add_aux(&to->service_time, &from->service_time);\r\nblkg_rwstat_add_aux(&to->wait_time, &from->wait_time);\r\nblkg_stat_add_aux(&from->time, &from->time);\r\n#ifdef CONFIG_DEBUG_BLK_CGROUP\r\nblkg_stat_add_aux(&to->unaccounted_time, &from->unaccounted_time);\r\nblkg_stat_add_aux(&to->avg_queue_size_sum, &from->avg_queue_size_sum);\r\nblkg_stat_add_aux(&to->avg_queue_size_samples, &from->avg_queue_size_samples);\r\nblkg_stat_add_aux(&to->dequeue, &from->dequeue);\r\nblkg_stat_add_aux(&to->group_wait_time, &from->group_wait_time);\r\nblkg_stat_add_aux(&to->idle_time, &from->idle_time);\r\nblkg_stat_add_aux(&to->empty_time, &from->empty_time);\r\n#endif\r\n}\r\nstatic void cfqg_stats_xfer_dead(struct cfq_group *cfqg)\r\n{\r\nstruct cfq_group *parent = cfqg_parent(cfqg);\r\nlockdep_assert_held(cfqg_to_blkg(cfqg)->q->queue_lock);\r\nif (unlikely(!parent))\r\nreturn;\r\ncfqg_stats_add_aux(&parent->stats, &cfqg->stats);\r\ncfqg_stats_reset(&cfqg->stats);\r\n}\r\nstatic inline struct cfq_group *cfqg_parent(struct cfq_group *cfqg) { return NULL; }\r\nstatic inline bool cfqg_is_descendant(struct cfq_group *cfqg,\r\nstruct cfq_group *ancestor)\r\n{\r\nreturn true;\r\n}\r\nstatic inline void cfqg_get(struct cfq_group *cfqg) { }\r\nstatic inline void cfqg_put(struct cfq_group *cfqg) { }\r\nstatic inline void cfqg_stats_update_io_add(struct cfq_group *cfqg,\r\nstruct cfq_group *curr_cfqg, int rw) { }\r\nstatic inline void cfqg_stats_update_timeslice_used(struct cfq_group *cfqg,\r\nunsigned long time, unsigned long unaccounted_time) { }\r\nstatic inline void cfqg_stats_update_io_remove(struct cfq_group *cfqg, int rw) { }\r\nstatic inline void cfqg_stats_update_io_merged(struct cfq_group *cfqg, int rw) { }\r\nstatic inline void cfqg_stats_update_completion(struct cfq_group *cfqg,\r\nuint64_t start_time, uint64_t io_start_time, int rw) { }\r\nstatic inline bool iops_mode(struct cfq_data *cfqd)\r\n{\r\nif (!cfqd->cfq_slice_idle && cfqd->hw_tag)\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic inline enum wl_class_t cfqq_class(struct cfq_queue *cfqq)\r\n{\r\nif (cfq_class_idle(cfqq))\r\nreturn IDLE_WORKLOAD;\r\nif (cfq_class_rt(cfqq))\r\nreturn RT_WORKLOAD;\r\nreturn BE_WORKLOAD;\r\n}\r\nstatic enum wl_type_t cfqq_type(struct cfq_queue *cfqq)\r\n{\r\nif (!cfq_cfqq_sync(cfqq))\r\nreturn ASYNC_WORKLOAD;\r\nif (!cfq_cfqq_idle_window(cfqq))\r\nreturn SYNC_NOIDLE_WORKLOAD;\r\nreturn SYNC_WORKLOAD;\r\n}\r\nstatic inline int cfq_group_busy_queues_wl(enum wl_class_t wl_class,\r\nstruct cfq_data *cfqd,\r\nstruct cfq_group *cfqg)\r\n{\r\nif (wl_class == IDLE_WORKLOAD)\r\nreturn cfqg->service_tree_idle.count;\r\nreturn cfqg->service_trees[wl_class][ASYNC_WORKLOAD].count +\r\ncfqg->service_trees[wl_class][SYNC_NOIDLE_WORKLOAD].count +\r\ncfqg->service_trees[wl_class][SYNC_WORKLOAD].count;\r\n}\r\nstatic inline int cfqg_busy_async_queues(struct cfq_data *cfqd,\r\nstruct cfq_group *cfqg)\r\n{\r\nreturn cfqg->service_trees[RT_WORKLOAD][ASYNC_WORKLOAD].count +\r\ncfqg->service_trees[BE_WORKLOAD][ASYNC_WORKLOAD].count;\r\n}\r\nstatic inline struct cfq_io_cq *icq_to_cic(struct io_cq *icq)\r\n{\r\nreturn container_of(icq, struct cfq_io_cq, icq);\r\n}\r\nstatic inline struct cfq_io_cq *cfq_cic_lookup(struct cfq_data *cfqd,\r\nstruct io_context *ioc)\r\n{\r\nif (ioc)\r\nreturn icq_to_cic(ioc_lookup_icq(ioc, cfqd->queue));\r\nreturn NULL;\r\n}\r\nstatic inline struct cfq_queue *cic_to_cfqq(struct cfq_io_cq *cic, bool is_sync)\r\n{\r\nreturn cic->cfqq[is_sync];\r\n}\r\nstatic inline void cic_set_cfqq(struct cfq_io_cq *cic, struct cfq_queue *cfqq,\r\nbool is_sync)\r\n{\r\ncic->cfqq[is_sync] = cfqq;\r\n}\r\nstatic inline struct cfq_data *cic_to_cfqd(struct cfq_io_cq *cic)\r\n{\r\nreturn cic->icq.q->elevator->elevator_data;\r\n}\r\nstatic inline bool cfq_bio_sync(struct bio *bio)\r\n{\r\nreturn bio_data_dir(bio) == READ || (bio->bi_rw & REQ_SYNC);\r\n}\r\nstatic inline void cfq_schedule_dispatch(struct cfq_data *cfqd)\r\n{\r\nif (cfqd->busy_queues) {\r\ncfq_log(cfqd, "schedule dispatch");\r\nkblockd_schedule_work(&cfqd->unplug_work);\r\n}\r\n}\r\nstatic inline int cfq_prio_slice(struct cfq_data *cfqd, bool sync,\r\nunsigned short prio)\r\n{\r\nconst int base_slice = cfqd->cfq_slice[sync];\r\nWARN_ON(prio >= IOPRIO_BE_NR);\r\nreturn base_slice + (base_slice/CFQ_SLICE_SCALE * (4 - prio));\r\n}\r\nstatic inline int\r\ncfq_prio_to_slice(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nreturn cfq_prio_slice(cfqd, cfq_cfqq_sync(cfqq), cfqq->ioprio);\r\n}\r\nstatic inline u64 cfqg_scale_charge(unsigned long charge,\r\nunsigned int vfraction)\r\n{\r\nu64 c = charge << CFQ_SERVICE_SHIFT;\r\nc <<= CFQ_SERVICE_SHIFT;\r\ndo_div(c, vfraction);\r\nreturn c;\r\n}\r\nstatic inline u64 max_vdisktime(u64 min_vdisktime, u64 vdisktime)\r\n{\r\ns64 delta = (s64)(vdisktime - min_vdisktime);\r\nif (delta > 0)\r\nmin_vdisktime = vdisktime;\r\nreturn min_vdisktime;\r\n}\r\nstatic inline u64 min_vdisktime(u64 min_vdisktime, u64 vdisktime)\r\n{\r\ns64 delta = (s64)(vdisktime - min_vdisktime);\r\nif (delta < 0)\r\nmin_vdisktime = vdisktime;\r\nreturn min_vdisktime;\r\n}\r\nstatic void update_min_vdisktime(struct cfq_rb_root *st)\r\n{\r\nstruct cfq_group *cfqg;\r\nif (st->left) {\r\ncfqg = rb_entry_cfqg(st->left);\r\nst->min_vdisktime = max_vdisktime(st->min_vdisktime,\r\ncfqg->vdisktime);\r\n}\r\n}\r\nstatic inline unsigned cfq_group_get_avg_queues(struct cfq_data *cfqd,\r\nstruct cfq_group *cfqg, bool rt)\r\n{\r\nunsigned min_q, max_q;\r\nunsigned mult = cfq_hist_divisor - 1;\r\nunsigned round = cfq_hist_divisor / 2;\r\nunsigned busy = cfq_group_busy_queues_wl(rt, cfqd, cfqg);\r\nmin_q = min(cfqg->busy_queues_avg[rt], busy);\r\nmax_q = max(cfqg->busy_queues_avg[rt], busy);\r\ncfqg->busy_queues_avg[rt] = (mult * max_q + min_q + round) /\r\ncfq_hist_divisor;\r\nreturn cfqg->busy_queues_avg[rt];\r\n}\r\nstatic inline unsigned\r\ncfq_group_slice(struct cfq_data *cfqd, struct cfq_group *cfqg)\r\n{\r\nreturn cfqd->cfq_target_latency * cfqg->vfraction >> CFQ_SERVICE_SHIFT;\r\n}\r\nstatic inline unsigned\r\ncfq_scaled_cfqq_slice(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nunsigned slice = cfq_prio_to_slice(cfqd, cfqq);\r\nif (cfqd->cfq_latency) {\r\nunsigned iq = cfq_group_get_avg_queues(cfqd, cfqq->cfqg,\r\ncfq_class_rt(cfqq));\r\nunsigned sync_slice = cfqd->cfq_slice[1];\r\nunsigned expect_latency = sync_slice * iq;\r\nunsigned group_slice = cfq_group_slice(cfqd, cfqq->cfqg);\r\nif (expect_latency > group_slice) {\r\nunsigned base_low_slice = 2 * cfqd->cfq_slice_idle;\r\nunsigned low_slice =\r\nmin(slice, base_low_slice * slice / sync_slice);\r\nslice = max(slice * group_slice / expect_latency,\r\nlow_slice);\r\n}\r\n}\r\nreturn slice;\r\n}\r\nstatic inline void\r\ncfq_set_prio_slice(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nunsigned slice = cfq_scaled_cfqq_slice(cfqd, cfqq);\r\ncfqq->slice_start = jiffies;\r\ncfqq->slice_end = jiffies + slice;\r\ncfqq->allocated_slice = slice;\r\ncfq_log_cfqq(cfqd, cfqq, "set_slice=%lu", cfqq->slice_end - jiffies);\r\n}\r\nstatic inline bool cfq_slice_used(struct cfq_queue *cfqq)\r\n{\r\nif (cfq_cfqq_slice_new(cfqq))\r\nreturn false;\r\nif (time_before(jiffies, cfqq->slice_end))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic struct request *\r\ncfq_choose_req(struct cfq_data *cfqd, struct request *rq1, struct request *rq2, sector_t last)\r\n{\r\nsector_t s1, s2, d1 = 0, d2 = 0;\r\nunsigned long back_max;\r\n#define CFQ_RQ1_WRAP 0x01\r\n#define CFQ_RQ2_WRAP 0x02\r\nunsigned wrap = 0;\r\nif (rq1 == NULL || rq1 == rq2)\r\nreturn rq2;\r\nif (rq2 == NULL)\r\nreturn rq1;\r\nif (rq_is_sync(rq1) != rq_is_sync(rq2))\r\nreturn rq_is_sync(rq1) ? rq1 : rq2;\r\nif ((rq1->cmd_flags ^ rq2->cmd_flags) & REQ_PRIO)\r\nreturn rq1->cmd_flags & REQ_PRIO ? rq1 : rq2;\r\ns1 = blk_rq_pos(rq1);\r\ns2 = blk_rq_pos(rq2);\r\nback_max = cfqd->cfq_back_max * 2;\r\nif (s1 >= last)\r\nd1 = s1 - last;\r\nelse if (s1 + back_max >= last)\r\nd1 = (last - s1) * cfqd->cfq_back_penalty;\r\nelse\r\nwrap |= CFQ_RQ1_WRAP;\r\nif (s2 >= last)\r\nd2 = s2 - last;\r\nelse if (s2 + back_max >= last)\r\nd2 = (last - s2) * cfqd->cfq_back_penalty;\r\nelse\r\nwrap |= CFQ_RQ2_WRAP;\r\nswitch (wrap) {\r\ncase 0:\r\nif (d1 < d2)\r\nreturn rq1;\r\nelse if (d2 < d1)\r\nreturn rq2;\r\nelse {\r\nif (s1 >= s2)\r\nreturn rq1;\r\nelse\r\nreturn rq2;\r\n}\r\ncase CFQ_RQ2_WRAP:\r\nreturn rq1;\r\ncase CFQ_RQ1_WRAP:\r\nreturn rq2;\r\ncase (CFQ_RQ1_WRAP|CFQ_RQ2_WRAP):\r\ndefault:\r\nif (s1 <= s2)\r\nreturn rq1;\r\nelse\r\nreturn rq2;\r\n}\r\n}\r\nstatic struct cfq_queue *cfq_rb_first(struct cfq_rb_root *root)\r\n{\r\nif (!root->count)\r\nreturn NULL;\r\nif (!root->left)\r\nroot->left = rb_first(&root->rb);\r\nif (root->left)\r\nreturn rb_entry(root->left, struct cfq_queue, rb_node);\r\nreturn NULL;\r\n}\r\nstatic struct cfq_group *cfq_rb_first_group(struct cfq_rb_root *root)\r\n{\r\nif (!root->left)\r\nroot->left = rb_first(&root->rb);\r\nif (root->left)\r\nreturn rb_entry_cfqg(root->left);\r\nreturn NULL;\r\n}\r\nstatic void rb_erase_init(struct rb_node *n, struct rb_root *root)\r\n{\r\nrb_erase(n, root);\r\nRB_CLEAR_NODE(n);\r\n}\r\nstatic void cfq_rb_erase(struct rb_node *n, struct cfq_rb_root *root)\r\n{\r\nif (root->left == n)\r\nroot->left = NULL;\r\nrb_erase_init(n, &root->rb);\r\n--root->count;\r\n}\r\nstatic struct request *\r\ncfq_find_next_rq(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nstruct request *last)\r\n{\r\nstruct rb_node *rbnext = rb_next(&last->rb_node);\r\nstruct rb_node *rbprev = rb_prev(&last->rb_node);\r\nstruct request *next = NULL, *prev = NULL;\r\nBUG_ON(RB_EMPTY_NODE(&last->rb_node));\r\nif (rbprev)\r\nprev = rb_entry_rq(rbprev);\r\nif (rbnext)\r\nnext = rb_entry_rq(rbnext);\r\nelse {\r\nrbnext = rb_first(&cfqq->sort_list);\r\nif (rbnext && rbnext != &last->rb_node)\r\nnext = rb_entry_rq(rbnext);\r\n}\r\nreturn cfq_choose_req(cfqd, next, prev, blk_rq_pos(last));\r\n}\r\nstatic unsigned long cfq_slice_offset(struct cfq_data *cfqd,\r\nstruct cfq_queue *cfqq)\r\n{\r\nreturn (cfqq->cfqg->nr_cfqq - 1) * (cfq_prio_slice(cfqd, 1, 0) -\r\ncfq_prio_slice(cfqd, cfq_cfqq_sync(cfqq), cfqq->ioprio));\r\n}\r\nstatic inline s64\r\ncfqg_key(struct cfq_rb_root *st, struct cfq_group *cfqg)\r\n{\r\nreturn cfqg->vdisktime - st->min_vdisktime;\r\n}\r\nstatic void\r\n__cfq_group_service_tree_add(struct cfq_rb_root *st, struct cfq_group *cfqg)\r\n{\r\nstruct rb_node **node = &st->rb.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct cfq_group *__cfqg;\r\ns64 key = cfqg_key(st, cfqg);\r\nint left = 1;\r\nwhile (*node != NULL) {\r\nparent = *node;\r\n__cfqg = rb_entry_cfqg(parent);\r\nif (key < cfqg_key(st, __cfqg))\r\nnode = &parent->rb_left;\r\nelse {\r\nnode = &parent->rb_right;\r\nleft = 0;\r\n}\r\n}\r\nif (left)\r\nst->left = &cfqg->rb_node;\r\nrb_link_node(&cfqg->rb_node, parent, node);\r\nrb_insert_color(&cfqg->rb_node, &st->rb);\r\n}\r\nstatic void\r\ncfq_update_group_weight(struct cfq_group *cfqg)\r\n{\r\nif (cfqg->new_weight) {\r\ncfqg->weight = cfqg->new_weight;\r\ncfqg->new_weight = 0;\r\n}\r\n}\r\nstatic void\r\ncfq_update_group_leaf_weight(struct cfq_group *cfqg)\r\n{\r\nBUG_ON(!RB_EMPTY_NODE(&cfqg->rb_node));\r\nif (cfqg->new_leaf_weight) {\r\ncfqg->leaf_weight = cfqg->new_leaf_weight;\r\ncfqg->new_leaf_weight = 0;\r\n}\r\n}\r\nstatic void\r\ncfq_group_service_tree_add(struct cfq_rb_root *st, struct cfq_group *cfqg)\r\n{\r\nunsigned int vfr = 1 << CFQ_SERVICE_SHIFT;\r\nstruct cfq_group *pos = cfqg;\r\nstruct cfq_group *parent;\r\nbool propagate;\r\nBUG_ON(!RB_EMPTY_NODE(&cfqg->rb_node));\r\ncfq_update_group_leaf_weight(cfqg);\r\n__cfq_group_service_tree_add(st, cfqg);\r\npropagate = !pos->nr_active++;\r\npos->children_weight += pos->leaf_weight;\r\nvfr = vfr * pos->leaf_weight / pos->children_weight;\r\nwhile ((parent = cfqg_parent(pos))) {\r\nif (propagate) {\r\ncfq_update_group_weight(pos);\r\npropagate = !parent->nr_active++;\r\nparent->children_weight += pos->weight;\r\n}\r\nvfr = vfr * pos->weight / parent->children_weight;\r\npos = parent;\r\n}\r\ncfqg->vfraction = max_t(unsigned, vfr, 1);\r\n}\r\nstatic void\r\ncfq_group_notify_queue_add(struct cfq_data *cfqd, struct cfq_group *cfqg)\r\n{\r\nstruct cfq_rb_root *st = &cfqd->grp_service_tree;\r\nstruct cfq_group *__cfqg;\r\nstruct rb_node *n;\r\ncfqg->nr_cfqq++;\r\nif (!RB_EMPTY_NODE(&cfqg->rb_node))\r\nreturn;\r\nn = rb_last(&st->rb);\r\nif (n) {\r\n__cfqg = rb_entry_cfqg(n);\r\ncfqg->vdisktime = __cfqg->vdisktime + CFQ_IDLE_DELAY;\r\n} else\r\ncfqg->vdisktime = st->min_vdisktime;\r\ncfq_group_service_tree_add(st, cfqg);\r\n}\r\nstatic void\r\ncfq_group_service_tree_del(struct cfq_rb_root *st, struct cfq_group *cfqg)\r\n{\r\nstruct cfq_group *pos = cfqg;\r\nbool propagate;\r\npropagate = !--pos->nr_active;\r\npos->children_weight -= pos->leaf_weight;\r\nwhile (propagate) {\r\nstruct cfq_group *parent = cfqg_parent(pos);\r\nWARN_ON_ONCE(pos->children_weight);\r\npos->vfraction = 0;\r\nif (!parent)\r\nbreak;\r\npropagate = !--parent->nr_active;\r\nparent->children_weight -= pos->weight;\r\npos = parent;\r\n}\r\nif (!RB_EMPTY_NODE(&cfqg->rb_node))\r\ncfq_rb_erase(&cfqg->rb_node, st);\r\n}\r\nstatic void\r\ncfq_group_notify_queue_del(struct cfq_data *cfqd, struct cfq_group *cfqg)\r\n{\r\nstruct cfq_rb_root *st = &cfqd->grp_service_tree;\r\nBUG_ON(cfqg->nr_cfqq < 1);\r\ncfqg->nr_cfqq--;\r\nif (cfqg->nr_cfqq)\r\nreturn;\r\ncfq_log_cfqg(cfqd, cfqg, "del_from_rr group");\r\ncfq_group_service_tree_del(st, cfqg);\r\ncfqg->saved_wl_slice = 0;\r\ncfqg_stats_update_dequeue(cfqg);\r\n}\r\nstatic inline unsigned int cfq_cfqq_slice_usage(struct cfq_queue *cfqq,\r\nunsigned int *unaccounted_time)\r\n{\r\nunsigned int slice_used;\r\nif (!cfqq->slice_start || cfqq->slice_start == jiffies) {\r\nslice_used = max_t(unsigned, (jiffies - cfqq->dispatch_start),\r\n1);\r\n} else {\r\nslice_used = jiffies - cfqq->slice_start;\r\nif (slice_used > cfqq->allocated_slice) {\r\n*unaccounted_time = slice_used - cfqq->allocated_slice;\r\nslice_used = cfqq->allocated_slice;\r\n}\r\nif (time_after(cfqq->slice_start, cfqq->dispatch_start))\r\n*unaccounted_time += cfqq->slice_start -\r\ncfqq->dispatch_start;\r\n}\r\nreturn slice_used;\r\n}\r\nstatic void cfq_group_served(struct cfq_data *cfqd, struct cfq_group *cfqg,\r\nstruct cfq_queue *cfqq)\r\n{\r\nstruct cfq_rb_root *st = &cfqd->grp_service_tree;\r\nunsigned int used_sl, charge, unaccounted_sl = 0;\r\nint nr_sync = cfqg->nr_cfqq - cfqg_busy_async_queues(cfqd, cfqg)\r\n- cfqg->service_tree_idle.count;\r\nunsigned int vfr;\r\nBUG_ON(nr_sync < 0);\r\nused_sl = charge = cfq_cfqq_slice_usage(cfqq, &unaccounted_sl);\r\nif (iops_mode(cfqd))\r\ncharge = cfqq->slice_dispatch;\r\nelse if (!cfq_cfqq_sync(cfqq) && !nr_sync)\r\ncharge = cfqq->allocated_slice;\r\nvfr = cfqg->vfraction;\r\ncfq_group_service_tree_del(st, cfqg);\r\ncfqg->vdisktime += cfqg_scale_charge(charge, vfr);\r\ncfq_group_service_tree_add(st, cfqg);\r\nif (time_after(cfqd->workload_expires, jiffies)) {\r\ncfqg->saved_wl_slice = cfqd->workload_expires\r\n- jiffies;\r\ncfqg->saved_wl_type = cfqd->serving_wl_type;\r\ncfqg->saved_wl_class = cfqd->serving_wl_class;\r\n} else\r\ncfqg->saved_wl_slice = 0;\r\ncfq_log_cfqg(cfqd, cfqg, "served: vt=%llu min_vt=%llu", cfqg->vdisktime,\r\nst->min_vdisktime);\r\ncfq_log_cfqq(cfqq->cfqd, cfqq,\r\n"sl_used=%u disp=%u charge=%u iops=%u sect=%lu",\r\nused_sl, cfqq->slice_dispatch, charge,\r\niops_mode(cfqd), cfqq->nr_sectors);\r\ncfqg_stats_update_timeslice_used(cfqg, used_sl, unaccounted_sl);\r\ncfqg_stats_set_start_empty_time(cfqg);\r\n}\r\nstatic void cfq_init_cfqg_base(struct cfq_group *cfqg)\r\n{\r\nstruct cfq_rb_root *st;\r\nint i, j;\r\nfor_each_cfqg_st(cfqg, i, j, st)\r\n*st = CFQ_RB_ROOT;\r\nRB_CLEAR_NODE(&cfqg->rb_node);\r\ncfqg->ttime.last_end_request = jiffies;\r\n}\r\nstatic void cfqg_stats_exit(struct cfqg_stats *stats)\r\n{\r\nblkg_rwstat_exit(&stats->merged);\r\nblkg_rwstat_exit(&stats->service_time);\r\nblkg_rwstat_exit(&stats->wait_time);\r\nblkg_rwstat_exit(&stats->queued);\r\nblkg_stat_exit(&stats->time);\r\n#ifdef CONFIG_DEBUG_BLK_CGROUP\r\nblkg_stat_exit(&stats->unaccounted_time);\r\nblkg_stat_exit(&stats->avg_queue_size_sum);\r\nblkg_stat_exit(&stats->avg_queue_size_samples);\r\nblkg_stat_exit(&stats->dequeue);\r\nblkg_stat_exit(&stats->group_wait_time);\r\nblkg_stat_exit(&stats->idle_time);\r\nblkg_stat_exit(&stats->empty_time);\r\n#endif\r\n}\r\nstatic int cfqg_stats_init(struct cfqg_stats *stats, gfp_t gfp)\r\n{\r\nif (blkg_rwstat_init(&stats->merged, gfp) ||\r\nblkg_rwstat_init(&stats->service_time, gfp) ||\r\nblkg_rwstat_init(&stats->wait_time, gfp) ||\r\nblkg_rwstat_init(&stats->queued, gfp) ||\r\nblkg_stat_init(&stats->time, gfp))\r\ngoto err;\r\n#ifdef CONFIG_DEBUG_BLK_CGROUP\r\nif (blkg_stat_init(&stats->unaccounted_time, gfp) ||\r\nblkg_stat_init(&stats->avg_queue_size_sum, gfp) ||\r\nblkg_stat_init(&stats->avg_queue_size_samples, gfp) ||\r\nblkg_stat_init(&stats->dequeue, gfp) ||\r\nblkg_stat_init(&stats->group_wait_time, gfp) ||\r\nblkg_stat_init(&stats->idle_time, gfp) ||\r\nblkg_stat_init(&stats->empty_time, gfp))\r\ngoto err;\r\n#endif\r\nreturn 0;\r\nerr:\r\ncfqg_stats_exit(stats);\r\nreturn -ENOMEM;\r\n}\r\nstatic struct blkcg_policy_data *cfq_cpd_alloc(gfp_t gfp)\r\n{\r\nstruct cfq_group_data *cgd;\r\ncgd = kzalloc(sizeof(*cgd), GFP_KERNEL);\r\nif (!cgd)\r\nreturn NULL;\r\nreturn &cgd->cpd;\r\n}\r\nstatic void cfq_cpd_init(struct blkcg_policy_data *cpd)\r\n{\r\nstruct cfq_group_data *cgd = cpd_to_cfqgd(cpd);\r\nunsigned int weight = cgroup_subsys_on_dfl(io_cgrp_subsys) ?\r\nCGROUP_WEIGHT_DFL : CFQ_WEIGHT_LEGACY_DFL;\r\nif (cpd_to_blkcg(cpd) == &blkcg_root)\r\nweight *= 2;\r\ncgd->weight = weight;\r\ncgd->leaf_weight = weight;\r\n}\r\nstatic void cfq_cpd_free(struct blkcg_policy_data *cpd)\r\n{\r\nkfree(cpd_to_cfqgd(cpd));\r\n}\r\nstatic void cfq_cpd_bind(struct blkcg_policy_data *cpd)\r\n{\r\nstruct blkcg *blkcg = cpd_to_blkcg(cpd);\r\nbool on_dfl = cgroup_subsys_on_dfl(io_cgrp_subsys);\r\nunsigned int weight = on_dfl ? CGROUP_WEIGHT_DFL : CFQ_WEIGHT_LEGACY_DFL;\r\nif (blkcg == &blkcg_root)\r\nweight *= 2;\r\nWARN_ON_ONCE(__cfq_set_weight(&blkcg->css, weight, on_dfl, true, false));\r\nWARN_ON_ONCE(__cfq_set_weight(&blkcg->css, weight, on_dfl, true, true));\r\n}\r\nstatic struct blkg_policy_data *cfq_pd_alloc(gfp_t gfp, int node)\r\n{\r\nstruct cfq_group *cfqg;\r\ncfqg = kzalloc_node(sizeof(*cfqg), gfp, node);\r\nif (!cfqg)\r\nreturn NULL;\r\ncfq_init_cfqg_base(cfqg);\r\nif (cfqg_stats_init(&cfqg->stats, gfp)) {\r\nkfree(cfqg);\r\nreturn NULL;\r\n}\r\nreturn &cfqg->pd;\r\n}\r\nstatic void cfq_pd_init(struct blkg_policy_data *pd)\r\n{\r\nstruct cfq_group *cfqg = pd_to_cfqg(pd);\r\nstruct cfq_group_data *cgd = blkcg_to_cfqgd(pd->blkg->blkcg);\r\ncfqg->weight = cgd->weight;\r\ncfqg->leaf_weight = cgd->leaf_weight;\r\n}\r\nstatic void cfq_pd_offline(struct blkg_policy_data *pd)\r\n{\r\nstruct cfq_group *cfqg = pd_to_cfqg(pd);\r\nint i;\r\nfor (i = 0; i < IOPRIO_BE_NR; i++) {\r\nif (cfqg->async_cfqq[0][i])\r\ncfq_put_queue(cfqg->async_cfqq[0][i]);\r\nif (cfqg->async_cfqq[1][i])\r\ncfq_put_queue(cfqg->async_cfqq[1][i]);\r\n}\r\nif (cfqg->async_idle_cfqq)\r\ncfq_put_queue(cfqg->async_idle_cfqq);\r\ncfqg_stats_xfer_dead(cfqg);\r\n}\r\nstatic void cfq_pd_free(struct blkg_policy_data *pd)\r\n{\r\nstruct cfq_group *cfqg = pd_to_cfqg(pd);\r\ncfqg_stats_exit(&cfqg->stats);\r\nreturn kfree(cfqg);\r\n}\r\nstatic void cfq_pd_reset_stats(struct blkg_policy_data *pd)\r\n{\r\nstruct cfq_group *cfqg = pd_to_cfqg(pd);\r\ncfqg_stats_reset(&cfqg->stats);\r\n}\r\nstatic struct cfq_group *cfq_lookup_cfqg(struct cfq_data *cfqd,\r\nstruct blkcg *blkcg)\r\n{\r\nstruct blkcg_gq *blkg;\r\nblkg = blkg_lookup(blkcg, cfqd->queue);\r\nif (likely(blkg))\r\nreturn blkg_to_cfqg(blkg);\r\nreturn NULL;\r\n}\r\nstatic void cfq_link_cfqq_cfqg(struct cfq_queue *cfqq, struct cfq_group *cfqg)\r\n{\r\ncfqq->cfqg = cfqg;\r\ncfqg_get(cfqg);\r\n}\r\nstatic u64 cfqg_prfill_weight_device(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct cfq_group *cfqg = pd_to_cfqg(pd);\r\nif (!cfqg->dev_weight)\r\nreturn 0;\r\nreturn __blkg_prfill_u64(sf, pd, cfqg->dev_weight);\r\n}\r\nstatic int cfqg_print_weight_device(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\ncfqg_prfill_weight_device, &blkcg_policy_cfq,\r\n0, false);\r\nreturn 0;\r\n}\r\nstatic u64 cfqg_prfill_leaf_weight_device(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct cfq_group *cfqg = pd_to_cfqg(pd);\r\nif (!cfqg->dev_leaf_weight)\r\nreturn 0;\r\nreturn __blkg_prfill_u64(sf, pd, cfqg->dev_leaf_weight);\r\n}\r\nstatic int cfqg_print_leaf_weight_device(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\ncfqg_prfill_leaf_weight_device, &blkcg_policy_cfq,\r\n0, false);\r\nreturn 0;\r\n}\r\nstatic int cfq_print_weight(struct seq_file *sf, void *v)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\r\nstruct cfq_group_data *cgd = blkcg_to_cfqgd(blkcg);\r\nunsigned int val = 0;\r\nif (cgd)\r\nval = cgd->weight;\r\nseq_printf(sf, "%u\n", val);\r\nreturn 0;\r\n}\r\nstatic int cfq_print_leaf_weight(struct seq_file *sf, void *v)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\r\nstruct cfq_group_data *cgd = blkcg_to_cfqgd(blkcg);\r\nunsigned int val = 0;\r\nif (cgd)\r\nval = cgd->leaf_weight;\r\nseq_printf(sf, "%u\n", val);\r\nreturn 0;\r\n}\r\nstatic ssize_t __cfqg_set_weight_device(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off,\r\nbool on_dfl, bool is_leaf_weight)\r\n{\r\nunsigned int min = on_dfl ? CGROUP_WEIGHT_MIN : CFQ_WEIGHT_LEGACY_MIN;\r\nunsigned int max = on_dfl ? CGROUP_WEIGHT_MAX : CFQ_WEIGHT_LEGACY_MAX;\r\nstruct blkcg *blkcg = css_to_blkcg(of_css(of));\r\nstruct blkg_conf_ctx ctx;\r\nstruct cfq_group *cfqg;\r\nstruct cfq_group_data *cfqgd;\r\nint ret;\r\nu64 v;\r\nret = blkg_conf_prep(blkcg, &blkcg_policy_cfq, buf, &ctx);\r\nif (ret)\r\nreturn ret;\r\nif (sscanf(ctx.body, "%llu", &v) == 1) {\r\nret = -ERANGE;\r\nif (!v && on_dfl)\r\ngoto out_finish;\r\n} else if (!strcmp(strim(ctx.body), "default")) {\r\nv = 0;\r\n} else {\r\nret = -EINVAL;\r\ngoto out_finish;\r\n}\r\ncfqg = blkg_to_cfqg(ctx.blkg);\r\ncfqgd = blkcg_to_cfqgd(blkcg);\r\nret = -ERANGE;\r\nif (!v || (v >= min && v <= max)) {\r\nif (!is_leaf_weight) {\r\ncfqg->dev_weight = v;\r\ncfqg->new_weight = v ?: cfqgd->weight;\r\n} else {\r\ncfqg->dev_leaf_weight = v;\r\ncfqg->new_leaf_weight = v ?: cfqgd->leaf_weight;\r\n}\r\nret = 0;\r\n}\r\nout_finish:\r\nblkg_conf_finish(&ctx);\r\nreturn ret ?: nbytes;\r\n}\r\nstatic ssize_t cfqg_set_weight_device(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off)\r\n{\r\nreturn __cfqg_set_weight_device(of, buf, nbytes, off, false, false);\r\n}\r\nstatic ssize_t cfqg_set_leaf_weight_device(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off)\r\n{\r\nreturn __cfqg_set_weight_device(of, buf, nbytes, off, false, true);\r\n}\r\nstatic int __cfq_set_weight(struct cgroup_subsys_state *css, u64 val,\r\nbool on_dfl, bool reset_dev, bool is_leaf_weight)\r\n{\r\nunsigned int min = on_dfl ? CGROUP_WEIGHT_MIN : CFQ_WEIGHT_LEGACY_MIN;\r\nunsigned int max = on_dfl ? CGROUP_WEIGHT_MAX : CFQ_WEIGHT_LEGACY_MAX;\r\nstruct blkcg *blkcg = css_to_blkcg(css);\r\nstruct blkcg_gq *blkg;\r\nstruct cfq_group_data *cfqgd;\r\nint ret = 0;\r\nif (val < min || val > max)\r\nreturn -ERANGE;\r\nspin_lock_irq(&blkcg->lock);\r\ncfqgd = blkcg_to_cfqgd(blkcg);\r\nif (!cfqgd) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (!is_leaf_weight)\r\ncfqgd->weight = val;\r\nelse\r\ncfqgd->leaf_weight = val;\r\nhlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {\r\nstruct cfq_group *cfqg = blkg_to_cfqg(blkg);\r\nif (!cfqg)\r\ncontinue;\r\nif (!is_leaf_weight) {\r\nif (reset_dev)\r\ncfqg->dev_weight = 0;\r\nif (!cfqg->dev_weight)\r\ncfqg->new_weight = cfqgd->weight;\r\n} else {\r\nif (reset_dev)\r\ncfqg->dev_leaf_weight = 0;\r\nif (!cfqg->dev_leaf_weight)\r\ncfqg->new_leaf_weight = cfqgd->leaf_weight;\r\n}\r\n}\r\nout:\r\nspin_unlock_irq(&blkcg->lock);\r\nreturn ret;\r\n}\r\nstatic int cfq_set_weight(struct cgroup_subsys_state *css, struct cftype *cft,\r\nu64 val)\r\n{\r\nreturn __cfq_set_weight(css, val, false, false, false);\r\n}\r\nstatic int cfq_set_leaf_weight(struct cgroup_subsys_state *css,\r\nstruct cftype *cft, u64 val)\r\n{\r\nreturn __cfq_set_weight(css, val, false, false, true);\r\n}\r\nstatic int cfqg_print_stat(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), blkg_prfill_stat,\r\n&blkcg_policy_cfq, seq_cft(sf)->private, false);\r\nreturn 0;\r\n}\r\nstatic int cfqg_print_rwstat(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), blkg_prfill_rwstat,\r\n&blkcg_policy_cfq, seq_cft(sf)->private, true);\r\nreturn 0;\r\n}\r\nstatic u64 cfqg_prfill_stat_recursive(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nu64 sum = blkg_stat_recursive_sum(pd_to_blkg(pd),\r\n&blkcg_policy_cfq, off);\r\nreturn __blkg_prfill_u64(sf, pd, sum);\r\n}\r\nstatic u64 cfqg_prfill_rwstat_recursive(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct blkg_rwstat sum = blkg_rwstat_recursive_sum(pd_to_blkg(pd),\r\n&blkcg_policy_cfq, off);\r\nreturn __blkg_prfill_rwstat(sf, pd, &sum);\r\n}\r\nstatic int cfqg_print_stat_recursive(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\ncfqg_prfill_stat_recursive, &blkcg_policy_cfq,\r\nseq_cft(sf)->private, false);\r\nreturn 0;\r\n}\r\nstatic int cfqg_print_rwstat_recursive(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\ncfqg_prfill_rwstat_recursive, &blkcg_policy_cfq,\r\nseq_cft(sf)->private, true);\r\nreturn 0;\r\n}\r\nstatic u64 cfqg_prfill_sectors(struct seq_file *sf, struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nu64 sum = blkg_rwstat_total(&pd->blkg->stat_bytes);\r\nreturn __blkg_prfill_u64(sf, pd, sum >> 9);\r\n}\r\nstatic int cfqg_print_stat_sectors(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\ncfqg_prfill_sectors, &blkcg_policy_cfq, 0, false);\r\nreturn 0;\r\n}\r\nstatic u64 cfqg_prfill_sectors_recursive(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct blkg_rwstat tmp = blkg_rwstat_recursive_sum(pd->blkg, NULL,\r\noffsetof(struct blkcg_gq, stat_bytes));\r\nu64 sum = atomic64_read(&tmp.aux_cnt[BLKG_RWSTAT_READ]) +\r\natomic64_read(&tmp.aux_cnt[BLKG_RWSTAT_WRITE]);\r\nreturn __blkg_prfill_u64(sf, pd, sum >> 9);\r\n}\r\nstatic int cfqg_print_stat_sectors_recursive(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\ncfqg_prfill_sectors_recursive, &blkcg_policy_cfq, 0,\r\nfalse);\r\nreturn 0;\r\n}\r\nstatic u64 cfqg_prfill_avg_queue_size(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct cfq_group *cfqg = pd_to_cfqg(pd);\r\nu64 samples = blkg_stat_read(&cfqg->stats.avg_queue_size_samples);\r\nu64 v = 0;\r\nif (samples) {\r\nv = blkg_stat_read(&cfqg->stats.avg_queue_size_sum);\r\nv = div64_u64(v, samples);\r\n}\r\n__blkg_prfill_u64(sf, pd, v);\r\nreturn 0;\r\n}\r\nstatic int cfqg_print_avg_queue_size(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\ncfqg_prfill_avg_queue_size, &blkcg_policy_cfq,\r\n0, false);\r\nreturn 0;\r\n}\r\nstatic int cfq_print_weight_on_dfl(struct seq_file *sf, void *v)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\r\nstruct cfq_group_data *cgd = blkcg_to_cfqgd(blkcg);\r\nseq_printf(sf, "default %u\n", cgd->weight);\r\nblkcg_print_blkgs(sf, blkcg, cfqg_prfill_weight_device,\r\n&blkcg_policy_cfq, 0, false);\r\nreturn 0;\r\n}\r\nstatic ssize_t cfq_set_weight_on_dfl(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off)\r\n{\r\nchar *endp;\r\nint ret;\r\nu64 v;\r\nbuf = strim(buf);\r\nv = simple_strtoull(buf, &endp, 0);\r\nif (*endp == '\0' || sscanf(buf, "default %llu", &v) == 1) {\r\nret = __cfq_set_weight(of_css(of), v, true, false, false);\r\nreturn ret ?: nbytes;\r\n}\r\nreturn __cfqg_set_weight_device(of, buf, nbytes, off, true, false);\r\n}\r\nstatic struct cfq_group *cfq_lookup_cfqg(struct cfq_data *cfqd,\r\nstruct blkcg *blkcg)\r\n{\r\nreturn cfqd->root_group;\r\n}\r\nstatic inline void\r\ncfq_link_cfqq_cfqg(struct cfq_queue *cfqq, struct cfq_group *cfqg) {\r\ncfqq->cfqg = cfqg;\r\n}\r\nstatic void cfq_service_tree_add(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nbool add_front)\r\n{\r\nstruct rb_node **p, *parent;\r\nstruct cfq_queue *__cfqq;\r\nunsigned long rb_key;\r\nstruct cfq_rb_root *st;\r\nint left;\r\nint new_cfqq = 1;\r\nst = st_for(cfqq->cfqg, cfqq_class(cfqq), cfqq_type(cfqq));\r\nif (cfq_class_idle(cfqq)) {\r\nrb_key = CFQ_IDLE_DELAY;\r\nparent = rb_last(&st->rb);\r\nif (parent && parent != &cfqq->rb_node) {\r\n__cfqq = rb_entry(parent, struct cfq_queue, rb_node);\r\nrb_key += __cfqq->rb_key;\r\n} else\r\nrb_key += jiffies;\r\n} else if (!add_front) {\r\nrb_key = cfq_slice_offset(cfqd, cfqq) + jiffies;\r\nrb_key -= cfqq->slice_resid;\r\ncfqq->slice_resid = 0;\r\n} else {\r\nrb_key = -HZ;\r\n__cfqq = cfq_rb_first(st);\r\nrb_key += __cfqq ? __cfqq->rb_key : jiffies;\r\n}\r\nif (!RB_EMPTY_NODE(&cfqq->rb_node)) {\r\nnew_cfqq = 0;\r\nif (rb_key == cfqq->rb_key && cfqq->service_tree == st)\r\nreturn;\r\ncfq_rb_erase(&cfqq->rb_node, cfqq->service_tree);\r\ncfqq->service_tree = NULL;\r\n}\r\nleft = 1;\r\nparent = NULL;\r\ncfqq->service_tree = st;\r\np = &st->rb.rb_node;\r\nwhile (*p) {\r\nparent = *p;\r\n__cfqq = rb_entry(parent, struct cfq_queue, rb_node);\r\nif (time_before(rb_key, __cfqq->rb_key))\r\np = &parent->rb_left;\r\nelse {\r\np = &parent->rb_right;\r\nleft = 0;\r\n}\r\n}\r\nif (left)\r\nst->left = &cfqq->rb_node;\r\ncfqq->rb_key = rb_key;\r\nrb_link_node(&cfqq->rb_node, parent, p);\r\nrb_insert_color(&cfqq->rb_node, &st->rb);\r\nst->count++;\r\nif (add_front || !new_cfqq)\r\nreturn;\r\ncfq_group_notify_queue_add(cfqd, cfqq->cfqg);\r\n}\r\nstatic struct cfq_queue *\r\ncfq_prio_tree_lookup(struct cfq_data *cfqd, struct rb_root *root,\r\nsector_t sector, struct rb_node **ret_parent,\r\nstruct rb_node ***rb_link)\r\n{\r\nstruct rb_node **p, *parent;\r\nstruct cfq_queue *cfqq = NULL;\r\nparent = NULL;\r\np = &root->rb_node;\r\nwhile (*p) {\r\nstruct rb_node **n;\r\nparent = *p;\r\ncfqq = rb_entry(parent, struct cfq_queue, p_node);\r\nif (sector > blk_rq_pos(cfqq->next_rq))\r\nn = &(*p)->rb_right;\r\nelse if (sector < blk_rq_pos(cfqq->next_rq))\r\nn = &(*p)->rb_left;\r\nelse\r\nbreak;\r\np = n;\r\ncfqq = NULL;\r\n}\r\n*ret_parent = parent;\r\nif (rb_link)\r\n*rb_link = p;\r\nreturn cfqq;\r\n}\r\nstatic void cfq_prio_tree_add(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nstruct rb_node **p, *parent;\r\nstruct cfq_queue *__cfqq;\r\nif (cfqq->p_root) {\r\nrb_erase(&cfqq->p_node, cfqq->p_root);\r\ncfqq->p_root = NULL;\r\n}\r\nif (cfq_class_idle(cfqq))\r\nreturn;\r\nif (!cfqq->next_rq)\r\nreturn;\r\ncfqq->p_root = &cfqd->prio_trees[cfqq->org_ioprio];\r\n__cfqq = cfq_prio_tree_lookup(cfqd, cfqq->p_root,\r\nblk_rq_pos(cfqq->next_rq), &parent, &p);\r\nif (!__cfqq) {\r\nrb_link_node(&cfqq->p_node, parent, p);\r\nrb_insert_color(&cfqq->p_node, cfqq->p_root);\r\n} else\r\ncfqq->p_root = NULL;\r\n}\r\nstatic void cfq_resort_rr_list(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nif (cfq_cfqq_on_rr(cfqq)) {\r\ncfq_service_tree_add(cfqd, cfqq, 0);\r\ncfq_prio_tree_add(cfqd, cfqq);\r\n}\r\n}\r\nstatic void cfq_add_cfqq_rr(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\ncfq_log_cfqq(cfqd, cfqq, "add_to_rr");\r\nBUG_ON(cfq_cfqq_on_rr(cfqq));\r\ncfq_mark_cfqq_on_rr(cfqq);\r\ncfqd->busy_queues++;\r\nif (cfq_cfqq_sync(cfqq))\r\ncfqd->busy_sync_queues++;\r\ncfq_resort_rr_list(cfqd, cfqq);\r\n}\r\nstatic void cfq_del_cfqq_rr(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\ncfq_log_cfqq(cfqd, cfqq, "del_from_rr");\r\nBUG_ON(!cfq_cfqq_on_rr(cfqq));\r\ncfq_clear_cfqq_on_rr(cfqq);\r\nif (!RB_EMPTY_NODE(&cfqq->rb_node)) {\r\ncfq_rb_erase(&cfqq->rb_node, cfqq->service_tree);\r\ncfqq->service_tree = NULL;\r\n}\r\nif (cfqq->p_root) {\r\nrb_erase(&cfqq->p_node, cfqq->p_root);\r\ncfqq->p_root = NULL;\r\n}\r\ncfq_group_notify_queue_del(cfqd, cfqq->cfqg);\r\nBUG_ON(!cfqd->busy_queues);\r\ncfqd->busy_queues--;\r\nif (cfq_cfqq_sync(cfqq))\r\ncfqd->busy_sync_queues--;\r\n}\r\nstatic void cfq_del_rq_rb(struct request *rq)\r\n{\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\nconst int sync = rq_is_sync(rq);\r\nBUG_ON(!cfqq->queued[sync]);\r\ncfqq->queued[sync]--;\r\nelv_rb_del(&cfqq->sort_list, rq);\r\nif (cfq_cfqq_on_rr(cfqq) && RB_EMPTY_ROOT(&cfqq->sort_list)) {\r\nif (cfqq->p_root) {\r\nrb_erase(&cfqq->p_node, cfqq->p_root);\r\ncfqq->p_root = NULL;\r\n}\r\n}\r\n}\r\nstatic void cfq_add_rq_rb(struct request *rq)\r\n{\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\nstruct cfq_data *cfqd = cfqq->cfqd;\r\nstruct request *prev;\r\ncfqq->queued[rq_is_sync(rq)]++;\r\nelv_rb_add(&cfqq->sort_list, rq);\r\nif (!cfq_cfqq_on_rr(cfqq))\r\ncfq_add_cfqq_rr(cfqd, cfqq);\r\nprev = cfqq->next_rq;\r\ncfqq->next_rq = cfq_choose_req(cfqd, cfqq->next_rq, rq, cfqd->last_position);\r\nif (prev != cfqq->next_rq)\r\ncfq_prio_tree_add(cfqd, cfqq);\r\nBUG_ON(!cfqq->next_rq);\r\n}\r\nstatic void cfq_reposition_rq_rb(struct cfq_queue *cfqq, struct request *rq)\r\n{\r\nelv_rb_del(&cfqq->sort_list, rq);\r\ncfqq->queued[rq_is_sync(rq)]--;\r\ncfqg_stats_update_io_remove(RQ_CFQG(rq), rq->cmd_flags);\r\ncfq_add_rq_rb(rq);\r\ncfqg_stats_update_io_add(RQ_CFQG(rq), cfqq->cfqd->serving_group,\r\nrq->cmd_flags);\r\n}\r\nstatic struct request *\r\ncfq_find_rq_fmerge(struct cfq_data *cfqd, struct bio *bio)\r\n{\r\nstruct task_struct *tsk = current;\r\nstruct cfq_io_cq *cic;\r\nstruct cfq_queue *cfqq;\r\ncic = cfq_cic_lookup(cfqd, tsk->io_context);\r\nif (!cic)\r\nreturn NULL;\r\ncfqq = cic_to_cfqq(cic, cfq_bio_sync(bio));\r\nif (cfqq)\r\nreturn elv_rb_find(&cfqq->sort_list, bio_end_sector(bio));\r\nreturn NULL;\r\n}\r\nstatic void cfq_activate_request(struct request_queue *q, struct request *rq)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\ncfqd->rq_in_driver++;\r\ncfq_log_cfqq(cfqd, RQ_CFQQ(rq), "activate rq, drv=%d",\r\ncfqd->rq_in_driver);\r\ncfqd->last_position = blk_rq_pos(rq) + blk_rq_sectors(rq);\r\n}\r\nstatic void cfq_deactivate_request(struct request_queue *q, struct request *rq)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nWARN_ON(!cfqd->rq_in_driver);\r\ncfqd->rq_in_driver--;\r\ncfq_log_cfqq(cfqd, RQ_CFQQ(rq), "deactivate rq, drv=%d",\r\ncfqd->rq_in_driver);\r\n}\r\nstatic void cfq_remove_request(struct request *rq)\r\n{\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\nif (cfqq->next_rq == rq)\r\ncfqq->next_rq = cfq_find_next_rq(cfqq->cfqd, cfqq, rq);\r\nlist_del_init(&rq->queuelist);\r\ncfq_del_rq_rb(rq);\r\ncfqq->cfqd->rq_queued--;\r\ncfqg_stats_update_io_remove(RQ_CFQG(rq), rq->cmd_flags);\r\nif (rq->cmd_flags & REQ_PRIO) {\r\nWARN_ON(!cfqq->prio_pending);\r\ncfqq->prio_pending--;\r\n}\r\n}\r\nstatic int cfq_merge(struct request_queue *q, struct request **req,\r\nstruct bio *bio)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nstruct request *__rq;\r\n__rq = cfq_find_rq_fmerge(cfqd, bio);\r\nif (__rq && elv_rq_merge_ok(__rq, bio)) {\r\n*req = __rq;\r\nreturn ELEVATOR_FRONT_MERGE;\r\n}\r\nreturn ELEVATOR_NO_MERGE;\r\n}\r\nstatic void cfq_merged_request(struct request_queue *q, struct request *req,\r\nint type)\r\n{\r\nif (type == ELEVATOR_FRONT_MERGE) {\r\nstruct cfq_queue *cfqq = RQ_CFQQ(req);\r\ncfq_reposition_rq_rb(cfqq, req);\r\n}\r\n}\r\nstatic void cfq_bio_merged(struct request_queue *q, struct request *req,\r\nstruct bio *bio)\r\n{\r\ncfqg_stats_update_io_merged(RQ_CFQG(req), bio->bi_rw);\r\n}\r\nstatic void\r\ncfq_merged_requests(struct request_queue *q, struct request *rq,\r\nstruct request *next)\r\n{\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nif (!list_empty(&rq->queuelist) && !list_empty(&next->queuelist) &&\r\ntime_before(next->fifo_time, rq->fifo_time) &&\r\ncfqq == RQ_CFQQ(next)) {\r\nlist_move(&rq->queuelist, &next->queuelist);\r\nrq->fifo_time = next->fifo_time;\r\n}\r\nif (cfqq->next_rq == next)\r\ncfqq->next_rq = rq;\r\ncfq_remove_request(next);\r\ncfqg_stats_update_io_merged(RQ_CFQG(rq), next->cmd_flags);\r\ncfqq = RQ_CFQQ(next);\r\nif (cfq_cfqq_on_rr(cfqq) && RB_EMPTY_ROOT(&cfqq->sort_list) &&\r\ncfqq != cfqd->active_queue)\r\ncfq_del_cfqq_rr(cfqd, cfqq);\r\n}\r\nstatic int cfq_allow_merge(struct request_queue *q, struct request *rq,\r\nstruct bio *bio)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nstruct cfq_io_cq *cic;\r\nstruct cfq_queue *cfqq;\r\nif (cfq_bio_sync(bio) && !rq_is_sync(rq))\r\nreturn false;\r\ncic = cfq_cic_lookup(cfqd, current->io_context);\r\nif (!cic)\r\nreturn false;\r\ncfqq = cic_to_cfqq(cic, cfq_bio_sync(bio));\r\nreturn cfqq == RQ_CFQQ(rq);\r\n}\r\nstatic inline void cfq_del_timer(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\ndel_timer(&cfqd->idle_slice_timer);\r\ncfqg_stats_update_idle_time(cfqq->cfqg);\r\n}\r\nstatic void __cfq_set_active_queue(struct cfq_data *cfqd,\r\nstruct cfq_queue *cfqq)\r\n{\r\nif (cfqq) {\r\ncfq_log_cfqq(cfqd, cfqq, "set_active wl_class:%d wl_type:%d",\r\ncfqd->serving_wl_class, cfqd->serving_wl_type);\r\ncfqg_stats_update_avg_queue_size(cfqq->cfqg);\r\ncfqq->slice_start = 0;\r\ncfqq->dispatch_start = jiffies;\r\ncfqq->allocated_slice = 0;\r\ncfqq->slice_end = 0;\r\ncfqq->slice_dispatch = 0;\r\ncfqq->nr_sectors = 0;\r\ncfq_clear_cfqq_wait_request(cfqq);\r\ncfq_clear_cfqq_must_dispatch(cfqq);\r\ncfq_clear_cfqq_must_alloc_slice(cfqq);\r\ncfq_clear_cfqq_fifo_expire(cfqq);\r\ncfq_mark_cfqq_slice_new(cfqq);\r\ncfq_del_timer(cfqd, cfqq);\r\n}\r\ncfqd->active_queue = cfqq;\r\n}\r\nstatic void\r\n__cfq_slice_expired(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nbool timed_out)\r\n{\r\ncfq_log_cfqq(cfqd, cfqq, "slice expired t=%d", timed_out);\r\nif (cfq_cfqq_wait_request(cfqq))\r\ncfq_del_timer(cfqd, cfqq);\r\ncfq_clear_cfqq_wait_request(cfqq);\r\ncfq_clear_cfqq_wait_busy(cfqq);\r\nif (cfq_cfqq_coop(cfqq) && CFQQ_SEEKY(cfqq))\r\ncfq_mark_cfqq_split_coop(cfqq);\r\nif (timed_out) {\r\nif (cfq_cfqq_slice_new(cfqq))\r\ncfqq->slice_resid = cfq_scaled_cfqq_slice(cfqd, cfqq);\r\nelse\r\ncfqq->slice_resid = cfqq->slice_end - jiffies;\r\ncfq_log_cfqq(cfqd, cfqq, "resid=%ld", cfqq->slice_resid);\r\n}\r\ncfq_group_served(cfqd, cfqq->cfqg, cfqq);\r\nif (cfq_cfqq_on_rr(cfqq) && RB_EMPTY_ROOT(&cfqq->sort_list))\r\ncfq_del_cfqq_rr(cfqd, cfqq);\r\ncfq_resort_rr_list(cfqd, cfqq);\r\nif (cfqq == cfqd->active_queue)\r\ncfqd->active_queue = NULL;\r\nif (cfqd->active_cic) {\r\nput_io_context(cfqd->active_cic->icq.ioc);\r\ncfqd->active_cic = NULL;\r\n}\r\n}\r\nstatic inline void cfq_slice_expired(struct cfq_data *cfqd, bool timed_out)\r\n{\r\nstruct cfq_queue *cfqq = cfqd->active_queue;\r\nif (cfqq)\r\n__cfq_slice_expired(cfqd, cfqq, timed_out);\r\n}\r\nstatic struct cfq_queue *cfq_get_next_queue(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_rb_root *st = st_for(cfqd->serving_group,\r\ncfqd->serving_wl_class, cfqd->serving_wl_type);\r\nif (!cfqd->rq_queued)\r\nreturn NULL;\r\nif (!st)\r\nreturn NULL;\r\nif (RB_EMPTY_ROOT(&st->rb))\r\nreturn NULL;\r\nreturn cfq_rb_first(st);\r\n}\r\nstatic struct cfq_queue *cfq_get_next_queue_forced(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_group *cfqg;\r\nstruct cfq_queue *cfqq;\r\nint i, j;\r\nstruct cfq_rb_root *st;\r\nif (!cfqd->rq_queued)\r\nreturn NULL;\r\ncfqg = cfq_get_next_cfqg(cfqd);\r\nif (!cfqg)\r\nreturn NULL;\r\nfor_each_cfqg_st(cfqg, i, j, st)\r\nif ((cfqq = cfq_rb_first(st)) != NULL)\r\nreturn cfqq;\r\nreturn NULL;\r\n}\r\nstatic struct cfq_queue *cfq_set_active_queue(struct cfq_data *cfqd,\r\nstruct cfq_queue *cfqq)\r\n{\r\nif (!cfqq)\r\ncfqq = cfq_get_next_queue(cfqd);\r\n__cfq_set_active_queue(cfqd, cfqq);\r\nreturn cfqq;\r\n}\r\nstatic inline sector_t cfq_dist_from_last(struct cfq_data *cfqd,\r\nstruct request *rq)\r\n{\r\nif (blk_rq_pos(rq) >= cfqd->last_position)\r\nreturn blk_rq_pos(rq) - cfqd->last_position;\r\nelse\r\nreturn cfqd->last_position - blk_rq_pos(rq);\r\n}\r\nstatic inline int cfq_rq_close(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nstruct request *rq)\r\n{\r\nreturn cfq_dist_from_last(cfqd, rq) <= CFQQ_CLOSE_THR;\r\n}\r\nstatic struct cfq_queue *cfqq_close(struct cfq_data *cfqd,\r\nstruct cfq_queue *cur_cfqq)\r\n{\r\nstruct rb_root *root = &cfqd->prio_trees[cur_cfqq->org_ioprio];\r\nstruct rb_node *parent, *node;\r\nstruct cfq_queue *__cfqq;\r\nsector_t sector = cfqd->last_position;\r\nif (RB_EMPTY_ROOT(root))\r\nreturn NULL;\r\n__cfqq = cfq_prio_tree_lookup(cfqd, root, sector, &parent, NULL);\r\nif (__cfqq)\r\nreturn __cfqq;\r\n__cfqq = rb_entry(parent, struct cfq_queue, p_node);\r\nif (cfq_rq_close(cfqd, cur_cfqq, __cfqq->next_rq))\r\nreturn __cfqq;\r\nif (blk_rq_pos(__cfqq->next_rq) < sector)\r\nnode = rb_next(&__cfqq->p_node);\r\nelse\r\nnode = rb_prev(&__cfqq->p_node);\r\nif (!node)\r\nreturn NULL;\r\n__cfqq = rb_entry(node, struct cfq_queue, p_node);\r\nif (cfq_rq_close(cfqd, cur_cfqq, __cfqq->next_rq))\r\nreturn __cfqq;\r\nreturn NULL;\r\n}\r\nstatic struct cfq_queue *cfq_close_cooperator(struct cfq_data *cfqd,\r\nstruct cfq_queue *cur_cfqq)\r\n{\r\nstruct cfq_queue *cfqq;\r\nif (cfq_class_idle(cur_cfqq))\r\nreturn NULL;\r\nif (!cfq_cfqq_sync(cur_cfqq))\r\nreturn NULL;\r\nif (CFQQ_SEEKY(cur_cfqq))\r\nreturn NULL;\r\nif (cur_cfqq->cfqg->nr_cfqq == 1)\r\nreturn NULL;\r\ncfqq = cfqq_close(cfqd, cur_cfqq);\r\nif (!cfqq)\r\nreturn NULL;\r\nif (cur_cfqq->cfqg != cfqq->cfqg)\r\nreturn NULL;\r\nif (!cfq_cfqq_sync(cfqq))\r\nreturn NULL;\r\nif (CFQQ_SEEKY(cfqq))\r\nreturn NULL;\r\nif (cfq_class_rt(cfqq) != cfq_class_rt(cur_cfqq))\r\nreturn NULL;\r\nreturn cfqq;\r\n}\r\nstatic bool cfq_should_idle(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nenum wl_class_t wl_class = cfqq_class(cfqq);\r\nstruct cfq_rb_root *st = cfqq->service_tree;\r\nBUG_ON(!st);\r\nBUG_ON(!st->count);\r\nif (!cfqd->cfq_slice_idle)\r\nreturn false;\r\nif (wl_class == IDLE_WORKLOAD)\r\nreturn false;\r\nif (cfq_cfqq_idle_window(cfqq) &&\r\n!(blk_queue_nonrot(cfqd->queue) && cfqd->hw_tag))\r\nreturn true;\r\nif (st->count == 1 && cfq_cfqq_sync(cfqq) &&\r\n!cfq_io_thinktime_big(cfqd, &st->ttime, false))\r\nreturn true;\r\ncfq_log_cfqq(cfqd, cfqq, "Not idling. st->count:%d", st->count);\r\nreturn false;\r\n}\r\nstatic void cfq_arm_slice_timer(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_queue *cfqq = cfqd->active_queue;\r\nstruct cfq_rb_root *st = cfqq->service_tree;\r\nstruct cfq_io_cq *cic;\r\nunsigned long sl, group_idle = 0;\r\nif (blk_queue_nonrot(cfqd->queue) && cfqd->hw_tag)\r\nreturn;\r\nWARN_ON(!RB_EMPTY_ROOT(&cfqq->sort_list));\r\nWARN_ON(cfq_cfqq_slice_new(cfqq));\r\nif (!cfq_should_idle(cfqd, cfqq)) {\r\nif (cfqd->cfq_group_idle)\r\ngroup_idle = cfqd->cfq_group_idle;\r\nelse\r\nreturn;\r\n}\r\nif (cfqq->dispatched)\r\nreturn;\r\ncic = cfqd->active_cic;\r\nif (!cic || !atomic_read(&cic->icq.ioc->active_ref))\r\nreturn;\r\nif (sample_valid(cic->ttime.ttime_samples) &&\r\n(cfqq->slice_end - jiffies < cic->ttime.ttime_mean)) {\r\ncfq_log_cfqq(cfqd, cfqq, "Not idling. think_time:%lu",\r\ncic->ttime.ttime_mean);\r\nreturn;\r\n}\r\nif (group_idle &&\r\n(cfqq->cfqg->nr_cfqq > 1 ||\r\ncfq_io_thinktime_big(cfqd, &st->ttime, true)))\r\nreturn;\r\ncfq_mark_cfqq_wait_request(cfqq);\r\nif (group_idle)\r\nsl = cfqd->cfq_group_idle;\r\nelse\r\nsl = cfqd->cfq_slice_idle;\r\nmod_timer(&cfqd->idle_slice_timer, jiffies + sl);\r\ncfqg_stats_set_start_idle_time(cfqq->cfqg);\r\ncfq_log_cfqq(cfqd, cfqq, "arm_idle: %lu group_idle: %d", sl,\r\ngroup_idle ? 1 : 0);\r\n}\r\nstatic void cfq_dispatch_insert(struct request_queue *q, struct request *rq)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\ncfq_log_cfqq(cfqd, cfqq, "dispatch_insert");\r\ncfqq->next_rq = cfq_find_next_rq(cfqd, cfqq, rq);\r\ncfq_remove_request(rq);\r\ncfqq->dispatched++;\r\n(RQ_CFQG(rq))->dispatched++;\r\nelv_dispatch_sort(q, rq);\r\ncfqd->rq_in_flight[cfq_cfqq_sync(cfqq)]++;\r\ncfqq->nr_sectors += blk_rq_sectors(rq);\r\n}\r\nstatic struct request *cfq_check_fifo(struct cfq_queue *cfqq)\r\n{\r\nstruct request *rq = NULL;\r\nif (cfq_cfqq_fifo_expire(cfqq))\r\nreturn NULL;\r\ncfq_mark_cfqq_fifo_expire(cfqq);\r\nif (list_empty(&cfqq->fifo))\r\nreturn NULL;\r\nrq = rq_entry_fifo(cfqq->fifo.next);\r\nif (time_before(jiffies, rq->fifo_time))\r\nrq = NULL;\r\ncfq_log_cfqq(cfqq->cfqd, cfqq, "fifo=%p", rq);\r\nreturn rq;\r\n}\r\nstatic inline int\r\ncfq_prio_to_maxrq(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nconst int base_rq = cfqd->cfq_slice_async_rq;\r\nWARN_ON(cfqq->ioprio >= IOPRIO_BE_NR);\r\nreturn 2 * base_rq * (IOPRIO_BE_NR - cfqq->ioprio);\r\n}\r\nstatic int cfqq_process_refs(struct cfq_queue *cfqq)\r\n{\r\nint process_refs, io_refs;\r\nio_refs = cfqq->allocated[READ] + cfqq->allocated[WRITE];\r\nprocess_refs = cfqq->ref - io_refs;\r\nBUG_ON(process_refs < 0);\r\nreturn process_refs;\r\n}\r\nstatic void cfq_setup_merge(struct cfq_queue *cfqq, struct cfq_queue *new_cfqq)\r\n{\r\nint process_refs, new_process_refs;\r\nstruct cfq_queue *__cfqq;\r\nif (!cfqq_process_refs(new_cfqq))\r\nreturn;\r\nwhile ((__cfqq = new_cfqq->new_cfqq)) {\r\nif (__cfqq == cfqq)\r\nreturn;\r\nnew_cfqq = __cfqq;\r\n}\r\nprocess_refs = cfqq_process_refs(cfqq);\r\nnew_process_refs = cfqq_process_refs(new_cfqq);\r\nif (process_refs == 0 || new_process_refs == 0)\r\nreturn;\r\nif (new_process_refs >= process_refs) {\r\ncfqq->new_cfqq = new_cfqq;\r\nnew_cfqq->ref += process_refs;\r\n} else {\r\nnew_cfqq->new_cfqq = cfqq;\r\ncfqq->ref += new_process_refs;\r\n}\r\n}\r\nstatic enum wl_type_t cfq_choose_wl_type(struct cfq_data *cfqd,\r\nstruct cfq_group *cfqg, enum wl_class_t wl_class)\r\n{\r\nstruct cfq_queue *queue;\r\nint i;\r\nbool key_valid = false;\r\nunsigned long lowest_key = 0;\r\nenum wl_type_t cur_best = SYNC_NOIDLE_WORKLOAD;\r\nfor (i = 0; i <= SYNC_WORKLOAD; ++i) {\r\nqueue = cfq_rb_first(st_for(cfqg, wl_class, i));\r\nif (queue &&\r\n(!key_valid || time_before(queue->rb_key, lowest_key))) {\r\nlowest_key = queue->rb_key;\r\ncur_best = i;\r\nkey_valid = true;\r\n}\r\n}\r\nreturn cur_best;\r\n}\r\nstatic void\r\nchoose_wl_class_and_type(struct cfq_data *cfqd, struct cfq_group *cfqg)\r\n{\r\nunsigned slice;\r\nunsigned count;\r\nstruct cfq_rb_root *st;\r\nunsigned group_slice;\r\nenum wl_class_t original_class = cfqd->serving_wl_class;\r\nif (cfq_group_busy_queues_wl(RT_WORKLOAD, cfqd, cfqg))\r\ncfqd->serving_wl_class = RT_WORKLOAD;\r\nelse if (cfq_group_busy_queues_wl(BE_WORKLOAD, cfqd, cfqg))\r\ncfqd->serving_wl_class = BE_WORKLOAD;\r\nelse {\r\ncfqd->serving_wl_class = IDLE_WORKLOAD;\r\ncfqd->workload_expires = jiffies + 1;\r\nreturn;\r\n}\r\nif (original_class != cfqd->serving_wl_class)\r\ngoto new_workload;\r\nst = st_for(cfqg, cfqd->serving_wl_class, cfqd->serving_wl_type);\r\ncount = st->count;\r\nif (count && !time_after(jiffies, cfqd->workload_expires))\r\nreturn;\r\nnew_workload:\r\ncfqd->serving_wl_type = cfq_choose_wl_type(cfqd, cfqg,\r\ncfqd->serving_wl_class);\r\nst = st_for(cfqg, cfqd->serving_wl_class, cfqd->serving_wl_type);\r\ncount = st->count;\r\ngroup_slice = cfq_group_slice(cfqd, cfqg);\r\nslice = group_slice * count /\r\nmax_t(unsigned, cfqg->busy_queues_avg[cfqd->serving_wl_class],\r\ncfq_group_busy_queues_wl(cfqd->serving_wl_class, cfqd,\r\ncfqg));\r\nif (cfqd->serving_wl_type == ASYNC_WORKLOAD) {\r\nunsigned int tmp;\r\ntmp = cfqd->cfq_target_latency *\r\ncfqg_busy_async_queues(cfqd, cfqg);\r\ntmp = tmp/cfqd->busy_queues;\r\nslice = min_t(unsigned, slice, tmp);\r\nslice = slice * cfqd->cfq_slice[0] / cfqd->cfq_slice[1];\r\n} else\r\nslice = max(slice, 2 * cfqd->cfq_slice_idle);\r\nslice = max_t(unsigned, slice, CFQ_MIN_TT);\r\ncfq_log(cfqd, "workload slice:%d", slice);\r\ncfqd->workload_expires = jiffies + slice;\r\n}\r\nstatic struct cfq_group *cfq_get_next_cfqg(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_rb_root *st = &cfqd->grp_service_tree;\r\nstruct cfq_group *cfqg;\r\nif (RB_EMPTY_ROOT(&st->rb))\r\nreturn NULL;\r\ncfqg = cfq_rb_first_group(st);\r\nupdate_min_vdisktime(st);\r\nreturn cfqg;\r\n}\r\nstatic void cfq_choose_cfqg(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_group *cfqg = cfq_get_next_cfqg(cfqd);\r\ncfqd->serving_group = cfqg;\r\nif (cfqg->saved_wl_slice) {\r\ncfqd->workload_expires = jiffies + cfqg->saved_wl_slice;\r\ncfqd->serving_wl_type = cfqg->saved_wl_type;\r\ncfqd->serving_wl_class = cfqg->saved_wl_class;\r\n} else\r\ncfqd->workload_expires = jiffies - 1;\r\nchoose_wl_class_and_type(cfqd, cfqg);\r\n}\r\nstatic struct cfq_queue *cfq_select_queue(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_queue *cfqq, *new_cfqq = NULL;\r\ncfqq = cfqd->active_queue;\r\nif (!cfqq)\r\ngoto new_queue;\r\nif (!cfqd->rq_queued)\r\nreturn NULL;\r\nif (cfq_cfqq_wait_busy(cfqq) && !RB_EMPTY_ROOT(&cfqq->sort_list))\r\ngoto expire;\r\nif (cfq_slice_used(cfqq) && !cfq_cfqq_must_dispatch(cfqq)) {\r\nif (cfqq->cfqg->nr_cfqq == 1 && RB_EMPTY_ROOT(&cfqq->sort_list)\r\n&& cfqq->dispatched && cfq_should_idle(cfqd, cfqq)) {\r\ncfqq = NULL;\r\ngoto keep_queue;\r\n} else\r\ngoto check_group_idle;\r\n}\r\nif (!RB_EMPTY_ROOT(&cfqq->sort_list))\r\ngoto keep_queue;\r\nnew_cfqq = cfq_close_cooperator(cfqd, cfqq);\r\nif (new_cfqq) {\r\nif (!cfqq->new_cfqq)\r\ncfq_setup_merge(cfqq, new_cfqq);\r\ngoto expire;\r\n}\r\nif (timer_pending(&cfqd->idle_slice_timer)) {\r\ncfqq = NULL;\r\ngoto keep_queue;\r\n}\r\nif (CFQQ_SEEKY(cfqq) && cfq_cfqq_idle_window(cfqq) &&\r\n(cfq_cfqq_slice_new(cfqq) ||\r\n(cfqq->slice_end - jiffies > jiffies - cfqq->slice_start))) {\r\ncfq_clear_cfqq_deep(cfqq);\r\ncfq_clear_cfqq_idle_window(cfqq);\r\n}\r\nif (cfqq->dispatched && cfq_should_idle(cfqd, cfqq)) {\r\ncfqq = NULL;\r\ngoto keep_queue;\r\n}\r\ncheck_group_idle:\r\nif (cfqd->cfq_group_idle && cfqq->cfqg->nr_cfqq == 1 &&\r\ncfqq->cfqg->dispatched &&\r\n!cfq_io_thinktime_big(cfqd, &cfqq->cfqg->ttime, true)) {\r\ncfqq = NULL;\r\ngoto keep_queue;\r\n}\r\nexpire:\r\ncfq_slice_expired(cfqd, 0);\r\nnew_queue:\r\nif (!new_cfqq)\r\ncfq_choose_cfqg(cfqd);\r\ncfqq = cfq_set_active_queue(cfqd, new_cfqq);\r\nkeep_queue:\r\nreturn cfqq;\r\n}\r\nstatic int __cfq_forced_dispatch_cfqq(struct cfq_queue *cfqq)\r\n{\r\nint dispatched = 0;\r\nwhile (cfqq->next_rq) {\r\ncfq_dispatch_insert(cfqq->cfqd->queue, cfqq->next_rq);\r\ndispatched++;\r\n}\r\nBUG_ON(!list_empty(&cfqq->fifo));\r\n__cfq_slice_expired(cfqq->cfqd, cfqq, 0);\r\nreturn dispatched;\r\n}\r\nstatic int cfq_forced_dispatch(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_queue *cfqq;\r\nint dispatched = 0;\r\ncfq_slice_expired(cfqd, 0);\r\nwhile ((cfqq = cfq_get_next_queue_forced(cfqd)) != NULL) {\r\n__cfq_set_active_queue(cfqd, cfqq);\r\ndispatched += __cfq_forced_dispatch_cfqq(cfqq);\r\n}\r\nBUG_ON(cfqd->busy_queues);\r\ncfq_log(cfqd, "forced_dispatch=%d", dispatched);\r\nreturn dispatched;\r\n}\r\nstatic inline bool cfq_slice_used_soon(struct cfq_data *cfqd,\r\nstruct cfq_queue *cfqq)\r\n{\r\nif (cfq_cfqq_slice_new(cfqq))\r\nreturn true;\r\nif (time_after(jiffies + cfqd->cfq_slice_idle * cfqq->dispatched,\r\ncfqq->slice_end))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool cfq_may_dispatch(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nunsigned int max_dispatch;\r\nif (cfq_should_idle(cfqd, cfqq) && cfqd->rq_in_flight[BLK_RW_ASYNC])\r\nreturn false;\r\nif (cfqd->rq_in_flight[BLK_RW_SYNC] && !cfq_cfqq_sync(cfqq))\r\nreturn false;\r\nmax_dispatch = max_t(unsigned int, cfqd->cfq_quantum / 2, 1);\r\nif (cfq_class_idle(cfqq))\r\nmax_dispatch = 1;\r\nif (cfqq->dispatched >= max_dispatch) {\r\nbool promote_sync = false;\r\nif (cfq_class_idle(cfqq))\r\nreturn false;\r\nif (cfq_cfqq_sync(cfqq) && cfqd->busy_sync_queues == 1)\r\npromote_sync = true;\r\nif (cfqd->busy_queues > 1 && cfq_slice_used_soon(cfqd, cfqq) &&\r\n!promote_sync)\r\nreturn false;\r\nif (cfqd->busy_queues == 1 || promote_sync)\r\nmax_dispatch = -1;\r\nelse\r\nmax_dispatch = cfqd->cfq_quantum;\r\n}\r\nif (!cfq_cfqq_sync(cfqq) && cfqd->cfq_latency) {\r\nunsigned long last_sync = jiffies - cfqd->last_delayed_sync;\r\nunsigned int depth;\r\ndepth = last_sync / cfqd->cfq_slice[1];\r\nif (!depth && !cfqq->dispatched)\r\ndepth = 1;\r\nif (depth < max_dispatch)\r\nmax_dispatch = depth;\r\n}\r\nreturn cfqq->dispatched < max_dispatch;\r\n}\r\nstatic bool cfq_dispatch_request(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nstruct request *rq;\r\nBUG_ON(RB_EMPTY_ROOT(&cfqq->sort_list));\r\nif (!cfq_may_dispatch(cfqd, cfqq))\r\nreturn false;\r\nrq = cfq_check_fifo(cfqq);\r\nif (!rq)\r\nrq = cfqq->next_rq;\r\ncfq_dispatch_insert(cfqd->queue, rq);\r\nif (!cfqd->active_cic) {\r\nstruct cfq_io_cq *cic = RQ_CIC(rq);\r\natomic_long_inc(&cic->icq.ioc->refcount);\r\ncfqd->active_cic = cic;\r\n}\r\nreturn true;\r\n}\r\nstatic int cfq_dispatch_requests(struct request_queue *q, int force)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nstruct cfq_queue *cfqq;\r\nif (!cfqd->busy_queues)\r\nreturn 0;\r\nif (unlikely(force))\r\nreturn cfq_forced_dispatch(cfqd);\r\ncfqq = cfq_select_queue(cfqd);\r\nif (!cfqq)\r\nreturn 0;\r\nif (!cfq_dispatch_request(cfqd, cfqq))\r\nreturn 0;\r\ncfqq->slice_dispatch++;\r\ncfq_clear_cfqq_must_dispatch(cfqq);\r\nif (cfqd->busy_queues > 1 && ((!cfq_cfqq_sync(cfqq) &&\r\ncfqq->slice_dispatch >= cfq_prio_to_maxrq(cfqd, cfqq)) ||\r\ncfq_class_idle(cfqq))) {\r\ncfqq->slice_end = jiffies + 1;\r\ncfq_slice_expired(cfqd, 0);\r\n}\r\ncfq_log_cfqq(cfqd, cfqq, "dispatched a request");\r\nreturn 1;\r\n}\r\nstatic void cfq_put_queue(struct cfq_queue *cfqq)\r\n{\r\nstruct cfq_data *cfqd = cfqq->cfqd;\r\nstruct cfq_group *cfqg;\r\nBUG_ON(cfqq->ref <= 0);\r\ncfqq->ref--;\r\nif (cfqq->ref)\r\nreturn;\r\ncfq_log_cfqq(cfqd, cfqq, "put_queue");\r\nBUG_ON(rb_first(&cfqq->sort_list));\r\nBUG_ON(cfqq->allocated[READ] + cfqq->allocated[WRITE]);\r\ncfqg = cfqq->cfqg;\r\nif (unlikely(cfqd->active_queue == cfqq)) {\r\n__cfq_slice_expired(cfqd, cfqq, 0);\r\ncfq_schedule_dispatch(cfqd);\r\n}\r\nBUG_ON(cfq_cfqq_on_rr(cfqq));\r\nkmem_cache_free(cfq_pool, cfqq);\r\ncfqg_put(cfqg);\r\n}\r\nstatic void cfq_put_cooperator(struct cfq_queue *cfqq)\r\n{\r\nstruct cfq_queue *__cfqq, *next;\r\n__cfqq = cfqq->new_cfqq;\r\nwhile (__cfqq) {\r\nif (__cfqq == cfqq) {\r\nWARN(1, "cfqq->new_cfqq loop detected\n");\r\nbreak;\r\n}\r\nnext = __cfqq->new_cfqq;\r\ncfq_put_queue(__cfqq);\r\n__cfqq = next;\r\n}\r\n}\r\nstatic void cfq_exit_cfqq(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nif (unlikely(cfqq == cfqd->active_queue)) {\r\n__cfq_slice_expired(cfqd, cfqq, 0);\r\ncfq_schedule_dispatch(cfqd);\r\n}\r\ncfq_put_cooperator(cfqq);\r\ncfq_put_queue(cfqq);\r\n}\r\nstatic void cfq_init_icq(struct io_cq *icq)\r\n{\r\nstruct cfq_io_cq *cic = icq_to_cic(icq);\r\ncic->ttime.last_end_request = jiffies;\r\n}\r\nstatic void cfq_exit_icq(struct io_cq *icq)\r\n{\r\nstruct cfq_io_cq *cic = icq_to_cic(icq);\r\nstruct cfq_data *cfqd = cic_to_cfqd(cic);\r\nif (cic_to_cfqq(cic, false)) {\r\ncfq_exit_cfqq(cfqd, cic_to_cfqq(cic, false));\r\ncic_set_cfqq(cic, NULL, false);\r\n}\r\nif (cic_to_cfqq(cic, true)) {\r\ncfq_exit_cfqq(cfqd, cic_to_cfqq(cic, true));\r\ncic_set_cfqq(cic, NULL, true);\r\n}\r\n}\r\nstatic void cfq_init_prio_data(struct cfq_queue *cfqq, struct cfq_io_cq *cic)\r\n{\r\nstruct task_struct *tsk = current;\r\nint ioprio_class;\r\nif (!cfq_cfqq_prio_changed(cfqq))\r\nreturn;\r\nioprio_class = IOPRIO_PRIO_CLASS(cic->ioprio);\r\nswitch (ioprio_class) {\r\ndefault:\r\nprintk(KERN_ERR "cfq: bad prio %x\n", ioprio_class);\r\ncase IOPRIO_CLASS_NONE:\r\ncfqq->ioprio = task_nice_ioprio(tsk);\r\ncfqq->ioprio_class = task_nice_ioclass(tsk);\r\nbreak;\r\ncase IOPRIO_CLASS_RT:\r\ncfqq->ioprio = IOPRIO_PRIO_DATA(cic->ioprio);\r\ncfqq->ioprio_class = IOPRIO_CLASS_RT;\r\nbreak;\r\ncase IOPRIO_CLASS_BE:\r\ncfqq->ioprio = IOPRIO_PRIO_DATA(cic->ioprio);\r\ncfqq->ioprio_class = IOPRIO_CLASS_BE;\r\nbreak;\r\ncase IOPRIO_CLASS_IDLE:\r\ncfqq->ioprio_class = IOPRIO_CLASS_IDLE;\r\ncfqq->ioprio = 7;\r\ncfq_clear_cfqq_idle_window(cfqq);\r\nbreak;\r\n}\r\ncfqq->org_ioprio = cfqq->ioprio;\r\ncfq_clear_cfqq_prio_changed(cfqq);\r\n}\r\nstatic void check_ioprio_changed(struct cfq_io_cq *cic, struct bio *bio)\r\n{\r\nint ioprio = cic->icq.ioc->ioprio;\r\nstruct cfq_data *cfqd = cic_to_cfqd(cic);\r\nstruct cfq_queue *cfqq;\r\nif (unlikely(!cfqd) || likely(cic->ioprio == ioprio))\r\nreturn;\r\ncfqq = cic_to_cfqq(cic, false);\r\nif (cfqq) {\r\ncfq_put_queue(cfqq);\r\ncfqq = cfq_get_queue(cfqd, BLK_RW_ASYNC, cic, bio);\r\ncic_set_cfqq(cic, cfqq, false);\r\n}\r\ncfqq = cic_to_cfqq(cic, true);\r\nif (cfqq)\r\ncfq_mark_cfqq_prio_changed(cfqq);\r\ncic->ioprio = ioprio;\r\n}\r\nstatic void cfq_init_cfqq(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\npid_t pid, bool is_sync)\r\n{\r\nRB_CLEAR_NODE(&cfqq->rb_node);\r\nRB_CLEAR_NODE(&cfqq->p_node);\r\nINIT_LIST_HEAD(&cfqq->fifo);\r\ncfqq->ref = 0;\r\ncfqq->cfqd = cfqd;\r\ncfq_mark_cfqq_prio_changed(cfqq);\r\nif (is_sync) {\r\nif (!cfq_class_idle(cfqq))\r\ncfq_mark_cfqq_idle_window(cfqq);\r\ncfq_mark_cfqq_sync(cfqq);\r\n}\r\ncfqq->pid = pid;\r\n}\r\nstatic void check_blkcg_changed(struct cfq_io_cq *cic, struct bio *bio)\r\n{\r\nstruct cfq_data *cfqd = cic_to_cfqd(cic);\r\nstruct cfq_queue *cfqq;\r\nuint64_t serial_nr;\r\nrcu_read_lock();\r\nserial_nr = bio_blkcg(bio)->css.serial_nr;\r\nrcu_read_unlock();\r\nif (unlikely(!cfqd) || likely(cic->blkcg_serial_nr == serial_nr))\r\nreturn;\r\ncfqq = cic_to_cfqq(cic, false);\r\nif (cfqq) {\r\ncfq_log_cfqq(cfqd, cfqq, "changed cgroup");\r\ncic_set_cfqq(cic, NULL, false);\r\ncfq_put_queue(cfqq);\r\n}\r\ncfqq = cic_to_cfqq(cic, true);\r\nif (cfqq) {\r\ncfq_log_cfqq(cfqd, cfqq, "changed cgroup");\r\ncic_set_cfqq(cic, NULL, true);\r\ncfq_put_queue(cfqq);\r\n}\r\ncic->blkcg_serial_nr = serial_nr;\r\n}\r\nstatic inline void check_blkcg_changed(struct cfq_io_cq *cic, struct bio *bio) { }\r\nstatic struct cfq_queue **\r\ncfq_async_queue_prio(struct cfq_group *cfqg, int ioprio_class, int ioprio)\r\n{\r\nswitch (ioprio_class) {\r\ncase IOPRIO_CLASS_RT:\r\nreturn &cfqg->async_cfqq[0][ioprio];\r\ncase IOPRIO_CLASS_NONE:\r\nioprio = IOPRIO_NORM;\r\ncase IOPRIO_CLASS_BE:\r\nreturn &cfqg->async_cfqq[1][ioprio];\r\ncase IOPRIO_CLASS_IDLE:\r\nreturn &cfqg->async_idle_cfqq;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic struct cfq_queue *\r\ncfq_get_queue(struct cfq_data *cfqd, bool is_sync, struct cfq_io_cq *cic,\r\nstruct bio *bio)\r\n{\r\nint ioprio_class = IOPRIO_PRIO_CLASS(cic->ioprio);\r\nint ioprio = IOPRIO_PRIO_DATA(cic->ioprio);\r\nstruct cfq_queue **async_cfqq = NULL;\r\nstruct cfq_queue *cfqq;\r\nstruct cfq_group *cfqg;\r\nrcu_read_lock();\r\ncfqg = cfq_lookup_cfqg(cfqd, bio_blkcg(bio));\r\nif (!cfqg) {\r\ncfqq = &cfqd->oom_cfqq;\r\ngoto out;\r\n}\r\nif (!is_sync) {\r\nif (!ioprio_valid(cic->ioprio)) {\r\nstruct task_struct *tsk = current;\r\nioprio = task_nice_ioprio(tsk);\r\nioprio_class = task_nice_ioclass(tsk);\r\n}\r\nasync_cfqq = cfq_async_queue_prio(cfqg, ioprio_class, ioprio);\r\ncfqq = *async_cfqq;\r\nif (cfqq)\r\ngoto out;\r\n}\r\ncfqq = kmem_cache_alloc_node(cfq_pool, GFP_NOWAIT | __GFP_ZERO,\r\ncfqd->queue->node);\r\nif (!cfqq) {\r\ncfqq = &cfqd->oom_cfqq;\r\ngoto out;\r\n}\r\ncfq_init_cfqq(cfqd, cfqq, current->pid, is_sync);\r\ncfq_init_prio_data(cfqq, cic);\r\ncfq_link_cfqq_cfqg(cfqq, cfqg);\r\ncfq_log_cfqq(cfqd, cfqq, "alloced");\r\nif (async_cfqq) {\r\ncfqq->ref++;\r\n*async_cfqq = cfqq;\r\n}\r\nout:\r\ncfqq->ref++;\r\nrcu_read_unlock();\r\nreturn cfqq;\r\n}\r\nstatic void\r\n__cfq_update_io_thinktime(struct cfq_ttime *ttime, unsigned long slice_idle)\r\n{\r\nunsigned long elapsed = jiffies - ttime->last_end_request;\r\nelapsed = min(elapsed, 2UL * slice_idle);\r\nttime->ttime_samples = (7*ttime->ttime_samples + 256) / 8;\r\nttime->ttime_total = (7*ttime->ttime_total + 256*elapsed) / 8;\r\nttime->ttime_mean = (ttime->ttime_total + 128) / ttime->ttime_samples;\r\n}\r\nstatic void\r\ncfq_update_io_thinktime(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nstruct cfq_io_cq *cic)\r\n{\r\nif (cfq_cfqq_sync(cfqq)) {\r\n__cfq_update_io_thinktime(&cic->ttime, cfqd->cfq_slice_idle);\r\n__cfq_update_io_thinktime(&cfqq->service_tree->ttime,\r\ncfqd->cfq_slice_idle);\r\n}\r\n#ifdef CONFIG_CFQ_GROUP_IOSCHED\r\n__cfq_update_io_thinktime(&cfqq->cfqg->ttime, cfqd->cfq_group_idle);\r\n#endif\r\n}\r\nstatic void\r\ncfq_update_io_seektime(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nstruct request *rq)\r\n{\r\nsector_t sdist = 0;\r\nsector_t n_sec = blk_rq_sectors(rq);\r\nif (cfqq->last_request_pos) {\r\nif (cfqq->last_request_pos < blk_rq_pos(rq))\r\nsdist = blk_rq_pos(rq) - cfqq->last_request_pos;\r\nelse\r\nsdist = cfqq->last_request_pos - blk_rq_pos(rq);\r\n}\r\ncfqq->seek_history <<= 1;\r\nif (blk_queue_nonrot(cfqd->queue))\r\ncfqq->seek_history |= (n_sec < CFQQ_SECT_THR_NONROT);\r\nelse\r\ncfqq->seek_history |= (sdist > CFQQ_SEEK_THR);\r\n}\r\nstatic void\r\ncfq_update_idle_window(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nstruct cfq_io_cq *cic)\r\n{\r\nint old_idle, enable_idle;\r\nif (!cfq_cfqq_sync(cfqq) || cfq_class_idle(cfqq))\r\nreturn;\r\nenable_idle = old_idle = cfq_cfqq_idle_window(cfqq);\r\nif (cfqq->queued[0] + cfqq->queued[1] >= 4)\r\ncfq_mark_cfqq_deep(cfqq);\r\nif (cfqq->next_rq && (cfqq->next_rq->cmd_flags & REQ_NOIDLE))\r\nenable_idle = 0;\r\nelse if (!atomic_read(&cic->icq.ioc->active_ref) ||\r\n!cfqd->cfq_slice_idle ||\r\n(!cfq_cfqq_deep(cfqq) && CFQQ_SEEKY(cfqq)))\r\nenable_idle = 0;\r\nelse if (sample_valid(cic->ttime.ttime_samples)) {\r\nif (cic->ttime.ttime_mean > cfqd->cfq_slice_idle)\r\nenable_idle = 0;\r\nelse\r\nenable_idle = 1;\r\n}\r\nif (old_idle != enable_idle) {\r\ncfq_log_cfqq(cfqd, cfqq, "idle=%d", enable_idle);\r\nif (enable_idle)\r\ncfq_mark_cfqq_idle_window(cfqq);\r\nelse\r\ncfq_clear_cfqq_idle_window(cfqq);\r\n}\r\n}\r\nstatic bool\r\ncfq_should_preempt(struct cfq_data *cfqd, struct cfq_queue *new_cfqq,\r\nstruct request *rq)\r\n{\r\nstruct cfq_queue *cfqq;\r\ncfqq = cfqd->active_queue;\r\nif (!cfqq)\r\nreturn false;\r\nif (cfq_class_idle(new_cfqq))\r\nreturn false;\r\nif (cfq_class_idle(cfqq))\r\nreturn true;\r\nif (cfq_class_rt(cfqq) && !cfq_class_rt(new_cfqq))\r\nreturn false;\r\nif (rq_is_sync(rq) && !cfq_cfqq_sync(cfqq))\r\nreturn true;\r\nif (!cfqg_is_descendant(cfqq->cfqg, new_cfqq->cfqg))\r\nreturn false;\r\nif (cfq_slice_used(cfqq))\r\nreturn true;\r\nif (cfq_class_rt(new_cfqq) && !cfq_class_rt(cfqq))\r\nreturn true;\r\nWARN_ON_ONCE(cfqq->ioprio_class != new_cfqq->ioprio_class);\r\nif (cfqd->serving_wl_type == SYNC_NOIDLE_WORKLOAD &&\r\ncfqq_type(new_cfqq) == SYNC_NOIDLE_WORKLOAD &&\r\nRB_EMPTY_ROOT(&cfqq->sort_list))\r\nreturn true;\r\nif ((rq->cmd_flags & REQ_PRIO) && !cfqq->prio_pending)\r\nreturn true;\r\nif (RB_EMPTY_ROOT(&cfqq->sort_list) && !cfq_should_idle(cfqd, cfqq))\r\nreturn true;\r\nif (!cfqd->active_cic || !cfq_cfqq_wait_request(cfqq))\r\nreturn false;\r\nif (cfq_rq_close(cfqd, cfqq, rq))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void cfq_preempt_queue(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nenum wl_type_t old_type = cfqq_type(cfqd->active_queue);\r\ncfq_log_cfqq(cfqd, cfqq, "preempt");\r\ncfq_slice_expired(cfqd, 1);\r\nif (old_type != cfqq_type(cfqq))\r\ncfqq->cfqg->saved_wl_slice = 0;\r\nBUG_ON(!cfq_cfqq_on_rr(cfqq));\r\ncfq_service_tree_add(cfqd, cfqq, 1);\r\ncfqq->slice_end = 0;\r\ncfq_mark_cfqq_slice_new(cfqq);\r\n}\r\nstatic void\r\ncfq_rq_enqueued(struct cfq_data *cfqd, struct cfq_queue *cfqq,\r\nstruct request *rq)\r\n{\r\nstruct cfq_io_cq *cic = RQ_CIC(rq);\r\ncfqd->rq_queued++;\r\nif (rq->cmd_flags & REQ_PRIO)\r\ncfqq->prio_pending++;\r\ncfq_update_io_thinktime(cfqd, cfqq, cic);\r\ncfq_update_io_seektime(cfqd, cfqq, rq);\r\ncfq_update_idle_window(cfqd, cfqq, cic);\r\ncfqq->last_request_pos = blk_rq_pos(rq) + blk_rq_sectors(rq);\r\nif (cfqq == cfqd->active_queue) {\r\nif (cfq_cfqq_wait_request(cfqq)) {\r\nif (blk_rq_bytes(rq) > PAGE_SIZE ||\r\ncfqd->busy_queues > 1) {\r\ncfq_del_timer(cfqd, cfqq);\r\ncfq_clear_cfqq_wait_request(cfqq);\r\n__blk_run_queue(cfqd->queue);\r\n} else {\r\ncfqg_stats_update_idle_time(cfqq->cfqg);\r\ncfq_mark_cfqq_must_dispatch(cfqq);\r\n}\r\n}\r\n} else if (cfq_should_preempt(cfqd, cfqq, rq)) {\r\ncfq_preempt_queue(cfqd, cfqq);\r\n__blk_run_queue(cfqd->queue);\r\n}\r\n}\r\nstatic void cfq_insert_request(struct request_queue *q, struct request *rq)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\ncfq_log_cfqq(cfqd, cfqq, "insert_request");\r\ncfq_init_prio_data(cfqq, RQ_CIC(rq));\r\nrq->fifo_time = jiffies + cfqd->cfq_fifo_expire[rq_is_sync(rq)];\r\nlist_add_tail(&rq->queuelist, &cfqq->fifo);\r\ncfq_add_rq_rb(rq);\r\ncfqg_stats_update_io_add(RQ_CFQG(rq), cfqd->serving_group,\r\nrq->cmd_flags);\r\ncfq_rq_enqueued(cfqd, cfqq, rq);\r\n}\r\nstatic void cfq_update_hw_tag(struct cfq_data *cfqd)\r\n{\r\nstruct cfq_queue *cfqq = cfqd->active_queue;\r\nif (cfqd->rq_in_driver > cfqd->hw_tag_est_depth)\r\ncfqd->hw_tag_est_depth = cfqd->rq_in_driver;\r\nif (cfqd->hw_tag == 1)\r\nreturn;\r\nif (cfqd->rq_queued <= CFQ_HW_QUEUE_MIN &&\r\ncfqd->rq_in_driver <= CFQ_HW_QUEUE_MIN)\r\nreturn;\r\nif (cfqq && cfq_cfqq_idle_window(cfqq) &&\r\ncfqq->dispatched + cfqq->queued[0] + cfqq->queued[1] <\r\nCFQ_HW_QUEUE_MIN && cfqd->rq_in_driver < CFQ_HW_QUEUE_MIN)\r\nreturn;\r\nif (cfqd->hw_tag_samples++ < 50)\r\nreturn;\r\nif (cfqd->hw_tag_est_depth >= CFQ_HW_QUEUE_MIN)\r\ncfqd->hw_tag = 1;\r\nelse\r\ncfqd->hw_tag = 0;\r\n}\r\nstatic bool cfq_should_wait_busy(struct cfq_data *cfqd, struct cfq_queue *cfqq)\r\n{\r\nstruct cfq_io_cq *cic = cfqd->active_cic;\r\nif (!RB_EMPTY_ROOT(&cfqq->sort_list))\r\nreturn false;\r\nif (cfqq->cfqg->nr_cfqq > 1)\r\nreturn false;\r\nif (cfq_io_thinktime_big(cfqd, &cfqq->cfqg->ttime, true))\r\nreturn false;\r\nif (cfq_slice_used(cfqq))\r\nreturn true;\r\nif (cic && sample_valid(cic->ttime.ttime_samples)\r\n&& (cfqq->slice_end - jiffies < cic->ttime.ttime_mean))\r\nreturn true;\r\nif (cfqq->slice_end - jiffies == 1)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void cfq_completed_request(struct request_queue *q, struct request *rq)\r\n{\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\nstruct cfq_data *cfqd = cfqq->cfqd;\r\nconst int sync = rq_is_sync(rq);\r\nunsigned long now;\r\nnow = jiffies;\r\ncfq_log_cfqq(cfqd, cfqq, "complete rqnoidle %d",\r\n!!(rq->cmd_flags & REQ_NOIDLE));\r\ncfq_update_hw_tag(cfqd);\r\nWARN_ON(!cfqd->rq_in_driver);\r\nWARN_ON(!cfqq->dispatched);\r\ncfqd->rq_in_driver--;\r\ncfqq->dispatched--;\r\n(RQ_CFQG(rq))->dispatched--;\r\ncfqg_stats_update_completion(cfqq->cfqg, rq_start_time_ns(rq),\r\nrq_io_start_time_ns(rq), rq->cmd_flags);\r\ncfqd->rq_in_flight[cfq_cfqq_sync(cfqq)]--;\r\nif (sync) {\r\nstruct cfq_rb_root *st;\r\nRQ_CIC(rq)->ttime.last_end_request = now;\r\nif (cfq_cfqq_on_rr(cfqq))\r\nst = cfqq->service_tree;\r\nelse\r\nst = st_for(cfqq->cfqg, cfqq_class(cfqq),\r\ncfqq_type(cfqq));\r\nst->ttime.last_end_request = now;\r\nif (!time_after(rq->start_time + cfqd->cfq_fifo_expire[1], now))\r\ncfqd->last_delayed_sync = now;\r\n}\r\n#ifdef CONFIG_CFQ_GROUP_IOSCHED\r\ncfqq->cfqg->ttime.last_end_request = now;\r\n#endif\r\nif (cfqd->active_queue == cfqq) {\r\nconst bool cfqq_empty = RB_EMPTY_ROOT(&cfqq->sort_list);\r\nif (cfq_cfqq_slice_new(cfqq)) {\r\ncfq_set_prio_slice(cfqd, cfqq);\r\ncfq_clear_cfqq_slice_new(cfqq);\r\n}\r\nif (cfq_should_wait_busy(cfqd, cfqq)) {\r\nunsigned long extend_sl = cfqd->cfq_slice_idle;\r\nif (!cfqd->cfq_slice_idle)\r\nextend_sl = cfqd->cfq_group_idle;\r\ncfqq->slice_end = jiffies + extend_sl;\r\ncfq_mark_cfqq_wait_busy(cfqq);\r\ncfq_log_cfqq(cfqd, cfqq, "will busy wait");\r\n}\r\nif (cfq_slice_used(cfqq) || cfq_class_idle(cfqq))\r\ncfq_slice_expired(cfqd, 1);\r\nelse if (sync && cfqq_empty &&\r\n!cfq_close_cooperator(cfqd, cfqq)) {\r\ncfq_arm_slice_timer(cfqd);\r\n}\r\n}\r\nif (!cfqd->rq_in_driver)\r\ncfq_schedule_dispatch(cfqd);\r\n}\r\nstatic inline int __cfq_may_queue(struct cfq_queue *cfqq)\r\n{\r\nif (cfq_cfqq_wait_request(cfqq) && !cfq_cfqq_must_alloc_slice(cfqq)) {\r\ncfq_mark_cfqq_must_alloc_slice(cfqq);\r\nreturn ELV_MQUEUE_MUST;\r\n}\r\nreturn ELV_MQUEUE_MAY;\r\n}\r\nstatic int cfq_may_queue(struct request_queue *q, int rw)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nstruct task_struct *tsk = current;\r\nstruct cfq_io_cq *cic;\r\nstruct cfq_queue *cfqq;\r\ncic = cfq_cic_lookup(cfqd, tsk->io_context);\r\nif (!cic)\r\nreturn ELV_MQUEUE_MAY;\r\ncfqq = cic_to_cfqq(cic, rw_is_sync(rw));\r\nif (cfqq) {\r\ncfq_init_prio_data(cfqq, cic);\r\nreturn __cfq_may_queue(cfqq);\r\n}\r\nreturn ELV_MQUEUE_MAY;\r\n}\r\nstatic void cfq_put_request(struct request *rq)\r\n{\r\nstruct cfq_queue *cfqq = RQ_CFQQ(rq);\r\nif (cfqq) {\r\nconst int rw = rq_data_dir(rq);\r\nBUG_ON(!cfqq->allocated[rw]);\r\ncfqq->allocated[rw]--;\r\ncfqg_put(RQ_CFQG(rq));\r\nrq->elv.priv[0] = NULL;\r\nrq->elv.priv[1] = NULL;\r\ncfq_put_queue(cfqq);\r\n}\r\n}\r\nstatic struct cfq_queue *\r\ncfq_merge_cfqqs(struct cfq_data *cfqd, struct cfq_io_cq *cic,\r\nstruct cfq_queue *cfqq)\r\n{\r\ncfq_log_cfqq(cfqd, cfqq, "merging with queue %p", cfqq->new_cfqq);\r\ncic_set_cfqq(cic, cfqq->new_cfqq, 1);\r\ncfq_mark_cfqq_coop(cfqq->new_cfqq);\r\ncfq_put_queue(cfqq);\r\nreturn cic_to_cfqq(cic, 1);\r\n}\r\nstatic struct cfq_queue *\r\nsplit_cfqq(struct cfq_io_cq *cic, struct cfq_queue *cfqq)\r\n{\r\nif (cfqq_process_refs(cfqq) == 1) {\r\ncfqq->pid = current->pid;\r\ncfq_clear_cfqq_coop(cfqq);\r\ncfq_clear_cfqq_split_coop(cfqq);\r\nreturn cfqq;\r\n}\r\ncic_set_cfqq(cic, NULL, 1);\r\ncfq_put_cooperator(cfqq);\r\ncfq_put_queue(cfqq);\r\nreturn NULL;\r\n}\r\nstatic int\r\ncfq_set_request(struct request_queue *q, struct request *rq, struct bio *bio,\r\ngfp_t gfp_mask)\r\n{\r\nstruct cfq_data *cfqd = q->elevator->elevator_data;\r\nstruct cfq_io_cq *cic = icq_to_cic(rq->elv.icq);\r\nconst int rw = rq_data_dir(rq);\r\nconst bool is_sync = rq_is_sync(rq);\r\nstruct cfq_queue *cfqq;\r\nspin_lock_irq(q->queue_lock);\r\ncheck_ioprio_changed(cic, bio);\r\ncheck_blkcg_changed(cic, bio);\r\nnew_queue:\r\ncfqq = cic_to_cfqq(cic, is_sync);\r\nif (!cfqq || cfqq == &cfqd->oom_cfqq) {\r\nif (cfqq)\r\ncfq_put_queue(cfqq);\r\ncfqq = cfq_get_queue(cfqd, is_sync, cic, bio);\r\ncic_set_cfqq(cic, cfqq, is_sync);\r\n} else {\r\nif (cfq_cfqq_coop(cfqq) && cfq_cfqq_split_coop(cfqq)) {\r\ncfq_log_cfqq(cfqd, cfqq, "breaking apart cfqq");\r\ncfqq = split_cfqq(cic, cfqq);\r\nif (!cfqq)\r\ngoto new_queue;\r\n}\r\nif (cfqq->new_cfqq)\r\ncfqq = cfq_merge_cfqqs(cfqd, cic, cfqq);\r\n}\r\ncfqq->allocated[rw]++;\r\ncfqq->ref++;\r\ncfqg_get(cfqq->cfqg);\r\nrq->elv.priv[0] = cfqq;\r\nrq->elv.priv[1] = cfqq->cfqg;\r\nspin_unlock_irq(q->queue_lock);\r\nreturn 0;\r\n}\r\nstatic void cfq_kick_queue(struct work_struct *work)\r\n{\r\nstruct cfq_data *cfqd =\r\ncontainer_of(work, struct cfq_data, unplug_work);\r\nstruct request_queue *q = cfqd->queue;\r\nspin_lock_irq(q->queue_lock);\r\n__blk_run_queue(cfqd->queue);\r\nspin_unlock_irq(q->queue_lock);\r\n}\r\nstatic void cfq_idle_slice_timer(unsigned long data)\r\n{\r\nstruct cfq_data *cfqd = (struct cfq_data *) data;\r\nstruct cfq_queue *cfqq;\r\nunsigned long flags;\r\nint timed_out = 1;\r\ncfq_log(cfqd, "idle timer fired");\r\nspin_lock_irqsave(cfqd->queue->queue_lock, flags);\r\ncfqq = cfqd->active_queue;\r\nif (cfqq) {\r\ntimed_out = 0;\r\nif (cfq_cfqq_must_dispatch(cfqq))\r\ngoto out_kick;\r\nif (cfq_slice_used(cfqq))\r\ngoto expire;\r\nif (!cfqd->busy_queues)\r\ngoto out_cont;\r\nif (!RB_EMPTY_ROOT(&cfqq->sort_list))\r\ngoto out_kick;\r\ncfq_clear_cfqq_deep(cfqq);\r\n}\r\nexpire:\r\ncfq_slice_expired(cfqd, timed_out);\r\nout_kick:\r\ncfq_schedule_dispatch(cfqd);\r\nout_cont:\r\nspin_unlock_irqrestore(cfqd->queue->queue_lock, flags);\r\n}\r\nstatic void cfq_shutdown_timer_wq(struct cfq_data *cfqd)\r\n{\r\ndel_timer_sync(&cfqd->idle_slice_timer);\r\ncancel_work_sync(&cfqd->unplug_work);\r\n}\r\nstatic void cfq_exit_queue(struct elevator_queue *e)\r\n{\r\nstruct cfq_data *cfqd = e->elevator_data;\r\nstruct request_queue *q = cfqd->queue;\r\ncfq_shutdown_timer_wq(cfqd);\r\nspin_lock_irq(q->queue_lock);\r\nif (cfqd->active_queue)\r\n__cfq_slice_expired(cfqd, cfqd->active_queue, 0);\r\nspin_unlock_irq(q->queue_lock);\r\ncfq_shutdown_timer_wq(cfqd);\r\n#ifdef CONFIG_CFQ_GROUP_IOSCHED\r\nblkcg_deactivate_policy(q, &blkcg_policy_cfq);\r\n#else\r\nkfree(cfqd->root_group);\r\n#endif\r\nkfree(cfqd);\r\n}\r\nstatic int cfq_init_queue(struct request_queue *q, struct elevator_type *e)\r\n{\r\nstruct cfq_data *cfqd;\r\nstruct blkcg_gq *blkg __maybe_unused;\r\nint i, ret;\r\nstruct elevator_queue *eq;\r\neq = elevator_alloc(q, e);\r\nif (!eq)\r\nreturn -ENOMEM;\r\ncfqd = kzalloc_node(sizeof(*cfqd), GFP_KERNEL, q->node);\r\nif (!cfqd) {\r\nkobject_put(&eq->kobj);\r\nreturn -ENOMEM;\r\n}\r\neq->elevator_data = cfqd;\r\ncfqd->queue = q;\r\nspin_lock_irq(q->queue_lock);\r\nq->elevator = eq;\r\nspin_unlock_irq(q->queue_lock);\r\ncfqd->grp_service_tree = CFQ_RB_ROOT;\r\n#ifdef CONFIG_CFQ_GROUP_IOSCHED\r\nret = blkcg_activate_policy(q, &blkcg_policy_cfq);\r\nif (ret)\r\ngoto out_free;\r\ncfqd->root_group = blkg_to_cfqg(q->root_blkg);\r\n#else\r\nret = -ENOMEM;\r\ncfqd->root_group = kzalloc_node(sizeof(*cfqd->root_group),\r\nGFP_KERNEL, cfqd->queue->node);\r\nif (!cfqd->root_group)\r\ngoto out_free;\r\ncfq_init_cfqg_base(cfqd->root_group);\r\ncfqd->root_group->weight = 2 * CFQ_WEIGHT_LEGACY_DFL;\r\ncfqd->root_group->leaf_weight = 2 * CFQ_WEIGHT_LEGACY_DFL;\r\n#endif\r\nfor (i = 0; i < CFQ_PRIO_LISTS; i++)\r\ncfqd->prio_trees[i] = RB_ROOT;\r\ncfq_init_cfqq(cfqd, &cfqd->oom_cfqq, 1, 0);\r\ncfqd->oom_cfqq.ref++;\r\nspin_lock_irq(q->queue_lock);\r\ncfq_link_cfqq_cfqg(&cfqd->oom_cfqq, cfqd->root_group);\r\ncfqg_put(cfqd->root_group);\r\nspin_unlock_irq(q->queue_lock);\r\ninit_timer(&cfqd->idle_slice_timer);\r\ncfqd->idle_slice_timer.function = cfq_idle_slice_timer;\r\ncfqd->idle_slice_timer.data = (unsigned long) cfqd;\r\nINIT_WORK(&cfqd->unplug_work, cfq_kick_queue);\r\ncfqd->cfq_quantum = cfq_quantum;\r\ncfqd->cfq_fifo_expire[0] = cfq_fifo_expire[0];\r\ncfqd->cfq_fifo_expire[1] = cfq_fifo_expire[1];\r\ncfqd->cfq_back_max = cfq_back_max;\r\ncfqd->cfq_back_penalty = cfq_back_penalty;\r\ncfqd->cfq_slice[0] = cfq_slice_async;\r\ncfqd->cfq_slice[1] = cfq_slice_sync;\r\ncfqd->cfq_target_latency = cfq_target_latency;\r\ncfqd->cfq_slice_async_rq = cfq_slice_async_rq;\r\ncfqd->cfq_slice_idle = cfq_slice_idle;\r\ncfqd->cfq_group_idle = cfq_group_idle;\r\ncfqd->cfq_latency = 1;\r\ncfqd->hw_tag = -1;\r\ncfqd->last_delayed_sync = jiffies - HZ;\r\nreturn 0;\r\nout_free:\r\nkfree(cfqd);\r\nkobject_put(&eq->kobj);\r\nreturn ret;\r\n}\r\nstatic void cfq_registered_queue(struct request_queue *q)\r\n{\r\nstruct elevator_queue *e = q->elevator;\r\nstruct cfq_data *cfqd = e->elevator_data;\r\nif (blk_queue_nonrot(q))\r\ncfqd->cfq_slice_idle = 0;\r\n}\r\nstatic ssize_t\r\ncfq_var_show(unsigned int var, char *page)\r\n{\r\nreturn sprintf(page, "%u\n", var);\r\n}\r\nstatic ssize_t\r\ncfq_var_store(unsigned int *var, const char *page, size_t count)\r\n{\r\nchar *p = (char *) page;\r\n*var = simple_strtoul(p, &p, 10);\r\nreturn count;\r\n}\r\nstatic int __init cfq_init(void)\r\n{\r\nint ret;\r\nif (!cfq_slice_async)\r\ncfq_slice_async = 1;\r\nif (!cfq_slice_idle)\r\ncfq_slice_idle = 1;\r\n#ifdef CONFIG_CFQ_GROUP_IOSCHED\r\nif (!cfq_group_idle)\r\ncfq_group_idle = 1;\r\nret = blkcg_policy_register(&blkcg_policy_cfq);\r\nif (ret)\r\nreturn ret;\r\n#else\r\ncfq_group_idle = 0;\r\n#endif\r\nret = -ENOMEM;\r\ncfq_pool = KMEM_CACHE(cfq_queue, 0);\r\nif (!cfq_pool)\r\ngoto err_pol_unreg;\r\nret = elv_register(&iosched_cfq);\r\nif (ret)\r\ngoto err_free_pool;\r\nreturn 0;\r\nerr_free_pool:\r\nkmem_cache_destroy(cfq_pool);\r\nerr_pol_unreg:\r\n#ifdef CONFIG_CFQ_GROUP_IOSCHED\r\nblkcg_policy_unregister(&blkcg_policy_cfq);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic void __exit cfq_exit(void)\r\n{\r\n#ifdef CONFIG_CFQ_GROUP_IOSCHED\r\nblkcg_policy_unregister(&blkcg_policy_cfq);\r\n#endif\r\nelv_unregister(&iosched_cfq);\r\nkmem_cache_destroy(cfq_pool);\r\n}
