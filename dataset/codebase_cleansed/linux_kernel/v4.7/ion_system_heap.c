static int order_to_index(unsigned int order)\r\n{\r\nint i;\r\nfor (i = 0; i < num_orders; i++)\r\nif (order == orders[i])\r\nreturn i;\r\nBUG();\r\nreturn -1;\r\n}\r\nstatic inline unsigned int order_to_size(int order)\r\n{\r\nreturn PAGE_SIZE << order;\r\n}\r\nstatic struct page *alloc_buffer_page(struct ion_system_heap *heap,\r\nstruct ion_buffer *buffer,\r\nunsigned long order)\r\n{\r\nbool cached = ion_buffer_cached(buffer);\r\nstruct ion_page_pool *pool = heap->pools[order_to_index(order)];\r\nstruct page *page;\r\nif (!cached) {\r\npage = ion_page_pool_alloc(pool);\r\n} else {\r\ngfp_t gfp_flags = low_order_gfp_flags;\r\nif (order > 4)\r\ngfp_flags = high_order_gfp_flags;\r\npage = alloc_pages(gfp_flags | __GFP_COMP, order);\r\nif (!page)\r\nreturn NULL;\r\nion_pages_sync_for_device(NULL, page, PAGE_SIZE << order,\r\nDMA_BIDIRECTIONAL);\r\n}\r\nreturn page;\r\n}\r\nstatic void free_buffer_page(struct ion_system_heap *heap,\r\nstruct ion_buffer *buffer, struct page *page)\r\n{\r\nunsigned int order = compound_order(page);\r\nbool cached = ion_buffer_cached(buffer);\r\nif (!cached && !(buffer->private_flags & ION_PRIV_FLAG_SHRINKER_FREE)) {\r\nstruct ion_page_pool *pool = heap->pools[order_to_index(order)];\r\nion_page_pool_free(pool, page);\r\n} else {\r\n__free_pages(page, order);\r\n}\r\n}\r\nstatic struct page *alloc_largest_available(struct ion_system_heap *heap,\r\nstruct ion_buffer *buffer,\r\nunsigned long size,\r\nunsigned int max_order)\r\n{\r\nstruct page *page;\r\nint i;\r\nfor (i = 0; i < num_orders; i++) {\r\nif (size < order_to_size(orders[i]))\r\ncontinue;\r\nif (max_order < orders[i])\r\ncontinue;\r\npage = alloc_buffer_page(heap, buffer, orders[i]);\r\nif (!page)\r\ncontinue;\r\nreturn page;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int ion_system_heap_allocate(struct ion_heap *heap,\r\nstruct ion_buffer *buffer,\r\nunsigned long size, unsigned long align,\r\nunsigned long flags)\r\n{\r\nstruct ion_system_heap *sys_heap = container_of(heap,\r\nstruct ion_system_heap,\r\nheap);\r\nstruct sg_table *table;\r\nstruct scatterlist *sg;\r\nstruct list_head pages;\r\nstruct page *page, *tmp_page;\r\nint i = 0;\r\nunsigned long size_remaining = PAGE_ALIGN(size);\r\nunsigned int max_order = orders[0];\r\nif (align > PAGE_SIZE)\r\nreturn -EINVAL;\r\nif (size / PAGE_SIZE > totalram_pages / 2)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&pages);\r\nwhile (size_remaining > 0) {\r\npage = alloc_largest_available(sys_heap, buffer, size_remaining,\r\nmax_order);\r\nif (!page)\r\ngoto free_pages;\r\nlist_add_tail(&page->lru, &pages);\r\nsize_remaining -= PAGE_SIZE << compound_order(page);\r\nmax_order = compound_order(page);\r\ni++;\r\n}\r\ntable = kmalloc(sizeof(struct sg_table), GFP_KERNEL);\r\nif (!table)\r\ngoto free_pages;\r\nif (sg_alloc_table(table, i, GFP_KERNEL))\r\ngoto free_table;\r\nsg = table->sgl;\r\nlist_for_each_entry_safe(page, tmp_page, &pages, lru) {\r\nsg_set_page(sg, page, PAGE_SIZE << compound_order(page), 0);\r\nsg = sg_next(sg);\r\nlist_del(&page->lru);\r\n}\r\nbuffer->priv_virt = table;\r\nreturn 0;\r\nfree_table:\r\nkfree(table);\r\nfree_pages:\r\nlist_for_each_entry_safe(page, tmp_page, &pages, lru)\r\nfree_buffer_page(sys_heap, buffer, page);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ion_system_heap_free(struct ion_buffer *buffer)\r\n{\r\nstruct ion_system_heap *sys_heap = container_of(buffer->heap,\r\nstruct ion_system_heap,\r\nheap);\r\nstruct sg_table *table = buffer->sg_table;\r\nbool cached = ion_buffer_cached(buffer);\r\nstruct scatterlist *sg;\r\nint i;\r\nif (!cached && !(buffer->private_flags & ION_PRIV_FLAG_SHRINKER_FREE))\r\nion_heap_buffer_zero(buffer);\r\nfor_each_sg(table->sgl, sg, table->nents, i)\r\nfree_buffer_page(sys_heap, buffer, sg_page(sg));\r\nsg_free_table(table);\r\nkfree(table);\r\n}\r\nstatic struct sg_table *ion_system_heap_map_dma(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\nreturn buffer->priv_virt;\r\n}\r\nstatic void ion_system_heap_unmap_dma(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\n}\r\nstatic int ion_system_heap_shrink(struct ion_heap *heap, gfp_t gfp_mask,\r\nint nr_to_scan)\r\n{\r\nstruct ion_system_heap *sys_heap;\r\nint nr_total = 0;\r\nint i, nr_freed;\r\nint only_scan = 0;\r\nsys_heap = container_of(heap, struct ion_system_heap, heap);\r\nif (!nr_to_scan)\r\nonly_scan = 1;\r\nfor (i = 0; i < num_orders; i++) {\r\nstruct ion_page_pool *pool = sys_heap->pools[i];\r\nnr_freed = ion_page_pool_shrink(pool, gfp_mask, nr_to_scan);\r\nnr_total += nr_freed;\r\nif (!only_scan) {\r\nnr_to_scan -= nr_freed;\r\nif (nr_to_scan <= 0)\r\nbreak;\r\n}\r\n}\r\nreturn nr_total;\r\n}\r\nstatic int ion_system_heap_debug_show(struct ion_heap *heap, struct seq_file *s,\r\nvoid *unused)\r\n{\r\nstruct ion_system_heap *sys_heap = container_of(heap,\r\nstruct ion_system_heap,\r\nheap);\r\nint i;\r\nfor (i = 0; i < num_orders; i++) {\r\nstruct ion_page_pool *pool = sys_heap->pools[i];\r\nseq_printf(s, "%d order %u highmem pages in pool = %lu total\n",\r\npool->high_count, pool->order,\r\n(PAGE_SIZE << pool->order) * pool->high_count);\r\nseq_printf(s, "%d order %u lowmem pages in pool = %lu total\n",\r\npool->low_count, pool->order,\r\n(PAGE_SIZE << pool->order) * pool->low_count);\r\n}\r\nreturn 0;\r\n}\r\nstruct ion_heap *ion_system_heap_create(struct ion_platform_heap *unused)\r\n{\r\nstruct ion_system_heap *heap;\r\nint i;\r\nheap = kzalloc(sizeof(struct ion_system_heap) +\r\nsizeof(struct ion_page_pool *) * num_orders,\r\nGFP_KERNEL);\r\nif (!heap)\r\nreturn ERR_PTR(-ENOMEM);\r\nheap->heap.ops = &system_heap_ops;\r\nheap->heap.type = ION_HEAP_TYPE_SYSTEM;\r\nheap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;\r\nfor (i = 0; i < num_orders; i++) {\r\nstruct ion_page_pool *pool;\r\ngfp_t gfp_flags = low_order_gfp_flags;\r\nif (orders[i] > 4)\r\ngfp_flags = high_order_gfp_flags;\r\npool = ion_page_pool_create(gfp_flags, orders[i]);\r\nif (!pool)\r\ngoto destroy_pools;\r\nheap->pools[i] = pool;\r\n}\r\nheap->heap.debug_show = ion_system_heap_debug_show;\r\nreturn &heap->heap;\r\ndestroy_pools:\r\nwhile (i--)\r\nion_page_pool_destroy(heap->pools[i]);\r\nkfree(heap);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvoid ion_system_heap_destroy(struct ion_heap *heap)\r\n{\r\nstruct ion_system_heap *sys_heap = container_of(heap,\r\nstruct ion_system_heap,\r\nheap);\r\nint i;\r\nfor (i = 0; i < num_orders; i++)\r\nion_page_pool_destroy(sys_heap->pools[i]);\r\nkfree(sys_heap);\r\n}\r\nstatic int ion_system_contig_heap_allocate(struct ion_heap *heap,\r\nstruct ion_buffer *buffer,\r\nunsigned long len,\r\nunsigned long align,\r\nunsigned long flags)\r\n{\r\nint order = get_order(len);\r\nstruct page *page;\r\nstruct sg_table *table;\r\nunsigned long i;\r\nint ret;\r\nif (align > (PAGE_SIZE << order))\r\nreturn -EINVAL;\r\npage = alloc_pages(low_order_gfp_flags, order);\r\nif (!page)\r\nreturn -ENOMEM;\r\nsplit_page(page, order);\r\nlen = PAGE_ALIGN(len);\r\nfor (i = len >> PAGE_SHIFT; i < (1 << order); i++)\r\n__free_page(page + i);\r\ntable = kmalloc(sizeof(struct sg_table), GFP_KERNEL);\r\nif (!table) {\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\nret = sg_alloc_table(table, 1, GFP_KERNEL);\r\nif (ret)\r\ngoto free_table;\r\nsg_set_page(table->sgl, page, len, 0);\r\nbuffer->priv_virt = table;\r\nion_pages_sync_for_device(NULL, page, len, DMA_BIDIRECTIONAL);\r\nreturn 0;\r\nfree_table:\r\nkfree(table);\r\nfree_pages:\r\nfor (i = 0; i < len >> PAGE_SHIFT; i++)\r\n__free_page(page + i);\r\nreturn ret;\r\n}\r\nstatic void ion_system_contig_heap_free(struct ion_buffer *buffer)\r\n{\r\nstruct sg_table *table = buffer->priv_virt;\r\nstruct page *page = sg_page(table->sgl);\r\nunsigned long pages = PAGE_ALIGN(buffer->size) >> PAGE_SHIFT;\r\nunsigned long i;\r\nfor (i = 0; i < pages; i++)\r\n__free_page(page + i);\r\nsg_free_table(table);\r\nkfree(table);\r\n}\r\nstatic int ion_system_contig_heap_phys(struct ion_heap *heap,\r\nstruct ion_buffer *buffer,\r\nion_phys_addr_t *addr, size_t *len)\r\n{\r\nstruct sg_table *table = buffer->priv_virt;\r\nstruct page *page = sg_page(table->sgl);\r\n*addr = page_to_phys(page);\r\n*len = buffer->size;\r\nreturn 0;\r\n}\r\nstatic struct sg_table *ion_system_contig_heap_map_dma(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\nreturn buffer->priv_virt;\r\n}\r\nstatic void ion_system_contig_heap_unmap_dma(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\n}\r\nstruct ion_heap *ion_system_contig_heap_create(struct ion_platform_heap *unused)\r\n{\r\nstruct ion_heap *heap;\r\nheap = kzalloc(sizeof(struct ion_heap), GFP_KERNEL);\r\nif (!heap)\r\nreturn ERR_PTR(-ENOMEM);\r\nheap->ops = &kmalloc_ops;\r\nheap->type = ION_HEAP_TYPE_SYSTEM_CONTIG;\r\nreturn heap;\r\n}\r\nvoid ion_system_contig_heap_destroy(struct ion_heap *heap)\r\n{\r\nkfree(heap);\r\n}
