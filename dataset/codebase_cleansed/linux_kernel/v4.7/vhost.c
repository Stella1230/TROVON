static void vhost_disable_cross_endian(struct vhost_virtqueue *vq)\r\n{\r\nvq->user_be = !virtio_legacy_is_little_endian();\r\n}\r\nstatic void vhost_enable_cross_endian_big(struct vhost_virtqueue *vq)\r\n{\r\nvq->user_be = true;\r\n}\r\nstatic void vhost_enable_cross_endian_little(struct vhost_virtqueue *vq)\r\n{\r\nvq->user_be = false;\r\n}\r\nstatic long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)\r\n{\r\nstruct vhost_vring_state s;\r\nif (vq->private_data)\r\nreturn -EBUSY;\r\nif (copy_from_user(&s, argp, sizeof(s)))\r\nreturn -EFAULT;\r\nif (s.num != VHOST_VRING_LITTLE_ENDIAN &&\r\ns.num != VHOST_VRING_BIG_ENDIAN)\r\nreturn -EINVAL;\r\nif (s.num == VHOST_VRING_BIG_ENDIAN)\r\nvhost_enable_cross_endian_big(vq);\r\nelse\r\nvhost_enable_cross_endian_little(vq);\r\nreturn 0;\r\n}\r\nstatic long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,\r\nint __user *argp)\r\n{\r\nstruct vhost_vring_state s = {\r\n.index = idx,\r\n.num = vq->user_be\r\n};\r\nif (copy_to_user(argp, &s, sizeof(s)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic void vhost_init_is_le(struct vhost_virtqueue *vq)\r\n{\r\nvq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1) || !vq->user_be;\r\n}\r\nstatic void vhost_disable_cross_endian(struct vhost_virtqueue *vq)\r\n{\r\n}\r\nstatic long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nstatic long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,\r\nint __user *argp)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nstatic void vhost_init_is_le(struct vhost_virtqueue *vq)\r\n{\r\nif (vhost_has_feature(vq, VIRTIO_F_VERSION_1))\r\nvq->is_le = true;\r\n}\r\nstatic void vhost_reset_is_le(struct vhost_virtqueue *vq)\r\n{\r\nvq->is_le = virtio_legacy_is_little_endian();\r\n}\r\nstatic void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,\r\npoll_table *pt)\r\n{\r\nstruct vhost_poll *poll;\r\npoll = container_of(pt, struct vhost_poll, table);\r\npoll->wqh = wqh;\r\nadd_wait_queue(wqh, &poll->wait);\r\n}\r\nstatic int vhost_poll_wakeup(wait_queue_t *wait, unsigned mode, int sync,\r\nvoid *key)\r\n{\r\nstruct vhost_poll *poll = container_of(wait, struct vhost_poll, wait);\r\nif (!((unsigned long)key & poll->mask))\r\nreturn 0;\r\nvhost_poll_queue(poll);\r\nreturn 0;\r\n}\r\nvoid vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)\r\n{\r\nINIT_LIST_HEAD(&work->node);\r\nwork->fn = fn;\r\ninit_waitqueue_head(&work->done);\r\nwork->flushing = 0;\r\nwork->queue_seq = work->done_seq = 0;\r\n}\r\nvoid vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,\r\nunsigned long mask, struct vhost_dev *dev)\r\n{\r\ninit_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);\r\ninit_poll_funcptr(&poll->table, vhost_poll_func);\r\npoll->mask = mask;\r\npoll->dev = dev;\r\npoll->wqh = NULL;\r\nvhost_work_init(&poll->work, fn);\r\n}\r\nint vhost_poll_start(struct vhost_poll *poll, struct file *file)\r\n{\r\nunsigned long mask;\r\nint ret = 0;\r\nif (poll->wqh)\r\nreturn 0;\r\nmask = file->f_op->poll(file, &poll->table);\r\nif (mask)\r\nvhost_poll_wakeup(&poll->wait, 0, 0, (void *)mask);\r\nif (mask & POLLERR) {\r\nif (poll->wqh)\r\nremove_wait_queue(poll->wqh, &poll->wait);\r\nret = -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nvoid vhost_poll_stop(struct vhost_poll *poll)\r\n{\r\nif (poll->wqh) {\r\nremove_wait_queue(poll->wqh, &poll->wait);\r\npoll->wqh = NULL;\r\n}\r\n}\r\nstatic bool vhost_work_seq_done(struct vhost_dev *dev, struct vhost_work *work,\r\nunsigned seq)\r\n{\r\nint left;\r\nspin_lock_irq(&dev->work_lock);\r\nleft = seq - work->done_seq;\r\nspin_unlock_irq(&dev->work_lock);\r\nreturn left <= 0;\r\n}\r\nvoid vhost_work_flush(struct vhost_dev *dev, struct vhost_work *work)\r\n{\r\nunsigned seq;\r\nint flushing;\r\nspin_lock_irq(&dev->work_lock);\r\nseq = work->queue_seq;\r\nwork->flushing++;\r\nspin_unlock_irq(&dev->work_lock);\r\nwait_event(work->done, vhost_work_seq_done(dev, work, seq));\r\nspin_lock_irq(&dev->work_lock);\r\nflushing = --work->flushing;\r\nspin_unlock_irq(&dev->work_lock);\r\nBUG_ON(flushing < 0);\r\n}\r\nvoid vhost_poll_flush(struct vhost_poll *poll)\r\n{\r\nvhost_work_flush(poll->dev, &poll->work);\r\n}\r\nvoid vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->work_lock, flags);\r\nif (list_empty(&work->node)) {\r\nlist_add_tail(&work->node, &dev->work_list);\r\nwork->queue_seq++;\r\nspin_unlock_irqrestore(&dev->work_lock, flags);\r\nwake_up_process(dev->worker);\r\n} else {\r\nspin_unlock_irqrestore(&dev->work_lock, flags);\r\n}\r\n}\r\nbool vhost_has_work(struct vhost_dev *dev)\r\n{\r\nreturn !list_empty(&dev->work_list);\r\n}\r\nvoid vhost_poll_queue(struct vhost_poll *poll)\r\n{\r\nvhost_work_queue(poll->dev, &poll->work);\r\n}\r\nstatic void vhost_vq_reset(struct vhost_dev *dev,\r\nstruct vhost_virtqueue *vq)\r\n{\r\nvq->num = 1;\r\nvq->desc = NULL;\r\nvq->avail = NULL;\r\nvq->used = NULL;\r\nvq->last_avail_idx = 0;\r\nvq->avail_idx = 0;\r\nvq->last_used_idx = 0;\r\nvq->signalled_used = 0;\r\nvq->signalled_used_valid = false;\r\nvq->used_flags = 0;\r\nvq->log_used = false;\r\nvq->log_addr = -1ull;\r\nvq->private_data = NULL;\r\nvq->acked_features = 0;\r\nvq->log_base = NULL;\r\nvq->error_ctx = NULL;\r\nvq->error = NULL;\r\nvq->kick = NULL;\r\nvq->call_ctx = NULL;\r\nvq->call = NULL;\r\nvq->log_ctx = NULL;\r\nvq->memory = NULL;\r\nvhost_reset_is_le(vq);\r\nvhost_disable_cross_endian(vq);\r\nvq->busyloop_timeout = 0;\r\n}\r\nstatic int vhost_worker(void *data)\r\n{\r\nstruct vhost_dev *dev = data;\r\nstruct vhost_work *work = NULL;\r\nunsigned uninitialized_var(seq);\r\nmm_segment_t oldfs = get_fs();\r\nset_fs(USER_DS);\r\nuse_mm(dev->mm);\r\nfor (;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nspin_lock_irq(&dev->work_lock);\r\nif (work) {\r\nwork->done_seq = seq;\r\nif (work->flushing)\r\nwake_up_all(&work->done);\r\n}\r\nif (kthread_should_stop()) {\r\nspin_unlock_irq(&dev->work_lock);\r\n__set_current_state(TASK_RUNNING);\r\nbreak;\r\n}\r\nif (!list_empty(&dev->work_list)) {\r\nwork = list_first_entry(&dev->work_list,\r\nstruct vhost_work, node);\r\nlist_del_init(&work->node);\r\nseq = work->queue_seq;\r\n} else\r\nwork = NULL;\r\nspin_unlock_irq(&dev->work_lock);\r\nif (work) {\r\n__set_current_state(TASK_RUNNING);\r\nwork->fn(work);\r\nif (need_resched())\r\nschedule();\r\n} else\r\nschedule();\r\n}\r\nunuse_mm(dev->mm);\r\nset_fs(oldfs);\r\nreturn 0;\r\n}\r\nstatic void vhost_vq_free_iovecs(struct vhost_virtqueue *vq)\r\n{\r\nkfree(vq->indirect);\r\nvq->indirect = NULL;\r\nkfree(vq->log);\r\nvq->log = NULL;\r\nkfree(vq->heads);\r\nvq->heads = NULL;\r\n}\r\nstatic long vhost_dev_alloc_iovecs(struct vhost_dev *dev)\r\n{\r\nstruct vhost_virtqueue *vq;\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nvq = dev->vqs[i];\r\nvq->indirect = kmalloc(sizeof *vq->indirect * UIO_MAXIOV,\r\nGFP_KERNEL);\r\nvq->log = kmalloc(sizeof *vq->log * UIO_MAXIOV, GFP_KERNEL);\r\nvq->heads = kmalloc(sizeof *vq->heads * UIO_MAXIOV, GFP_KERNEL);\r\nif (!vq->indirect || !vq->log || !vq->heads)\r\ngoto err_nomem;\r\n}\r\nreturn 0;\r\nerr_nomem:\r\nfor (; i >= 0; --i)\r\nvhost_vq_free_iovecs(dev->vqs[i]);\r\nreturn -ENOMEM;\r\n}\r\nstatic void vhost_dev_free_iovecs(struct vhost_dev *dev)\r\n{\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i)\r\nvhost_vq_free_iovecs(dev->vqs[i]);\r\n}\r\nvoid vhost_dev_init(struct vhost_dev *dev,\r\nstruct vhost_virtqueue **vqs, int nvqs)\r\n{\r\nstruct vhost_virtqueue *vq;\r\nint i;\r\ndev->vqs = vqs;\r\ndev->nvqs = nvqs;\r\nmutex_init(&dev->mutex);\r\ndev->log_ctx = NULL;\r\ndev->log_file = NULL;\r\ndev->memory = NULL;\r\ndev->mm = NULL;\r\nspin_lock_init(&dev->work_lock);\r\nINIT_LIST_HEAD(&dev->work_list);\r\ndev->worker = NULL;\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nvq = dev->vqs[i];\r\nvq->log = NULL;\r\nvq->indirect = NULL;\r\nvq->heads = NULL;\r\nvq->dev = dev;\r\nmutex_init(&vq->mutex);\r\nvhost_vq_reset(dev, vq);\r\nif (vq->handle_kick)\r\nvhost_poll_init(&vq->poll, vq->handle_kick,\r\nPOLLIN, dev);\r\n}\r\n}\r\nlong vhost_dev_check_owner(struct vhost_dev *dev)\r\n{\r\nreturn dev->mm == current->mm ? 0 : -EPERM;\r\n}\r\nstatic void vhost_attach_cgroups_work(struct vhost_work *work)\r\n{\r\nstruct vhost_attach_cgroups_struct *s;\r\ns = container_of(work, struct vhost_attach_cgroups_struct, work);\r\ns->ret = cgroup_attach_task_all(s->owner, current);\r\n}\r\nstatic int vhost_attach_cgroups(struct vhost_dev *dev)\r\n{\r\nstruct vhost_attach_cgroups_struct attach;\r\nattach.owner = current;\r\nvhost_work_init(&attach.work, vhost_attach_cgroups_work);\r\nvhost_work_queue(dev, &attach.work);\r\nvhost_work_flush(dev, &attach.work);\r\nreturn attach.ret;\r\n}\r\nbool vhost_dev_has_owner(struct vhost_dev *dev)\r\n{\r\nreturn dev->mm;\r\n}\r\nlong vhost_dev_set_owner(struct vhost_dev *dev)\r\n{\r\nstruct task_struct *worker;\r\nint err;\r\nif (vhost_dev_has_owner(dev)) {\r\nerr = -EBUSY;\r\ngoto err_mm;\r\n}\r\ndev->mm = get_task_mm(current);\r\nworker = kthread_create(vhost_worker, dev, "vhost-%d", current->pid);\r\nif (IS_ERR(worker)) {\r\nerr = PTR_ERR(worker);\r\ngoto err_worker;\r\n}\r\ndev->worker = worker;\r\nwake_up_process(worker);\r\nerr = vhost_attach_cgroups(dev);\r\nif (err)\r\ngoto err_cgroup;\r\nerr = vhost_dev_alloc_iovecs(dev);\r\nif (err)\r\ngoto err_cgroup;\r\nreturn 0;\r\nerr_cgroup:\r\nkthread_stop(worker);\r\ndev->worker = NULL;\r\nerr_worker:\r\nif (dev->mm)\r\nmmput(dev->mm);\r\ndev->mm = NULL;\r\nerr_mm:\r\nreturn err;\r\n}\r\nstruct vhost_memory *vhost_dev_reset_owner_prepare(void)\r\n{\r\nreturn kmalloc(offsetof(struct vhost_memory, regions), GFP_KERNEL);\r\n}\r\nvoid vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_memory *memory)\r\n{\r\nint i;\r\nvhost_dev_cleanup(dev, true);\r\nmemory->nregions = 0;\r\ndev->memory = memory;\r\nfor (i = 0; i < dev->nvqs; ++i)\r\ndev->vqs[i]->memory = memory;\r\n}\r\nvoid vhost_dev_stop(struct vhost_dev *dev)\r\n{\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nif (dev->vqs[i]->kick && dev->vqs[i]->handle_kick) {\r\nvhost_poll_stop(&dev->vqs[i]->poll);\r\nvhost_poll_flush(&dev->vqs[i]->poll);\r\n}\r\n}\r\n}\r\nvoid vhost_dev_cleanup(struct vhost_dev *dev, bool locked)\r\n{\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nif (dev->vqs[i]->error_ctx)\r\neventfd_ctx_put(dev->vqs[i]->error_ctx);\r\nif (dev->vqs[i]->error)\r\nfput(dev->vqs[i]->error);\r\nif (dev->vqs[i]->kick)\r\nfput(dev->vqs[i]->kick);\r\nif (dev->vqs[i]->call_ctx)\r\neventfd_ctx_put(dev->vqs[i]->call_ctx);\r\nif (dev->vqs[i]->call)\r\nfput(dev->vqs[i]->call);\r\nvhost_vq_reset(dev, dev->vqs[i]);\r\n}\r\nvhost_dev_free_iovecs(dev);\r\nif (dev->log_ctx)\r\neventfd_ctx_put(dev->log_ctx);\r\ndev->log_ctx = NULL;\r\nif (dev->log_file)\r\nfput(dev->log_file);\r\ndev->log_file = NULL;\r\nkvfree(dev->memory);\r\ndev->memory = NULL;\r\nWARN_ON(!list_empty(&dev->work_list));\r\nif (dev->worker) {\r\nkthread_stop(dev->worker);\r\ndev->worker = NULL;\r\n}\r\nif (dev->mm)\r\nmmput(dev->mm);\r\ndev->mm = NULL;\r\n}\r\nstatic int log_access_ok(void __user *log_base, u64 addr, unsigned long sz)\r\n{\r\nu64 a = addr / VHOST_PAGE_SIZE / 8;\r\nif (a > ULONG_MAX - (unsigned long)log_base ||\r\na + (unsigned long)log_base > ULONG_MAX)\r\nreturn 0;\r\nreturn access_ok(VERIFY_WRITE, log_base + a,\r\n(sz + VHOST_PAGE_SIZE * 8 - 1) / VHOST_PAGE_SIZE / 8);\r\n}\r\nstatic int vq_memory_access_ok(void __user *log_base, struct vhost_memory *mem,\r\nint log_all)\r\n{\r\nint i;\r\nif (!mem)\r\nreturn 0;\r\nfor (i = 0; i < mem->nregions; ++i) {\r\nstruct vhost_memory_region *m = mem->regions + i;\r\nunsigned long a = m->userspace_addr;\r\nif (m->memory_size > ULONG_MAX)\r\nreturn 0;\r\nelse if (!access_ok(VERIFY_WRITE, (void __user *)a,\r\nm->memory_size))\r\nreturn 0;\r\nelse if (log_all && !log_access_ok(log_base,\r\nm->guest_phys_addr,\r\nm->memory_size))\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int memory_access_ok(struct vhost_dev *d, struct vhost_memory *mem,\r\nint log_all)\r\n{\r\nint i;\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nint ok;\r\nbool log;\r\nmutex_lock(&d->vqs[i]->mutex);\r\nlog = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);\r\nif (d->vqs[i]->private_data)\r\nok = vq_memory_access_ok(d->vqs[i]->log_base, mem, log);\r\nelse\r\nok = 1;\r\nmutex_unlock(&d->vqs[i]->mutex);\r\nif (!ok)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,\r\nstruct vring_desc __user *desc,\r\nstruct vring_avail __user *avail,\r\nstruct vring_used __user *used)\r\n{\r\nsize_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;\r\nreturn access_ok(VERIFY_READ, desc, num * sizeof *desc) &&\r\naccess_ok(VERIFY_READ, avail,\r\nsizeof *avail + num * sizeof *avail->ring + s) &&\r\naccess_ok(VERIFY_WRITE, used,\r\nsizeof *used + num * sizeof *used->ring + s);\r\n}\r\nint vhost_log_access_ok(struct vhost_dev *dev)\r\n{\r\nreturn memory_access_ok(dev, dev->memory, 1);\r\n}\r\nstatic int vq_log_access_ok(struct vhost_virtqueue *vq,\r\nvoid __user *log_base)\r\n{\r\nsize_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;\r\nreturn vq_memory_access_ok(log_base, vq->memory,\r\nvhost_has_feature(vq, VHOST_F_LOG_ALL)) &&\r\n(!vq->log_used || log_access_ok(log_base, vq->log_addr,\r\nsizeof *vq->used +\r\nvq->num * sizeof *vq->used->ring + s));\r\n}\r\nint vhost_vq_access_ok(struct vhost_virtqueue *vq)\r\n{\r\nreturn vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used) &&\r\nvq_log_access_ok(vq, vq->log_base);\r\n}\r\nstatic int vhost_memory_reg_sort_cmp(const void *p1, const void *p2)\r\n{\r\nconst struct vhost_memory_region *r1 = p1, *r2 = p2;\r\nif (r1->guest_phys_addr < r2->guest_phys_addr)\r\nreturn 1;\r\nif (r1->guest_phys_addr > r2->guest_phys_addr)\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic void *vhost_kvzalloc(unsigned long size)\r\n{\r\nvoid *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\r\nif (!n)\r\nn = vzalloc(size);\r\nreturn n;\r\n}\r\nstatic long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)\r\n{\r\nstruct vhost_memory mem, *newmem, *oldmem;\r\nunsigned long size = offsetof(struct vhost_memory, regions);\r\nint i;\r\nif (copy_from_user(&mem, m, size))\r\nreturn -EFAULT;\r\nif (mem.padding)\r\nreturn -EOPNOTSUPP;\r\nif (mem.nregions > max_mem_regions)\r\nreturn -E2BIG;\r\nnewmem = vhost_kvzalloc(size + mem.nregions * sizeof(*m->regions));\r\nif (!newmem)\r\nreturn -ENOMEM;\r\nmemcpy(newmem, &mem, size);\r\nif (copy_from_user(newmem->regions, m->regions,\r\nmem.nregions * sizeof *m->regions)) {\r\nkvfree(newmem);\r\nreturn -EFAULT;\r\n}\r\nsort(newmem->regions, newmem->nregions, sizeof(*newmem->regions),\r\nvhost_memory_reg_sort_cmp, NULL);\r\nif (!memory_access_ok(d, newmem, 0)) {\r\nkvfree(newmem);\r\nreturn -EFAULT;\r\n}\r\noldmem = d->memory;\r\nd->memory = newmem;\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nmutex_lock(&d->vqs[i]->mutex);\r\nd->vqs[i]->memory = newmem;\r\nmutex_unlock(&d->vqs[i]->mutex);\r\n}\r\nkvfree(oldmem);\r\nreturn 0;\r\n}\r\nlong vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)\r\n{\r\nstruct file *eventfp, *filep = NULL;\r\nbool pollstart = false, pollstop = false;\r\nstruct eventfd_ctx *ctx = NULL;\r\nu32 __user *idxp = argp;\r\nstruct vhost_virtqueue *vq;\r\nstruct vhost_vring_state s;\r\nstruct vhost_vring_file f;\r\nstruct vhost_vring_addr a;\r\nu32 idx;\r\nlong r;\r\nr = get_user(idx, idxp);\r\nif (r < 0)\r\nreturn r;\r\nif (idx >= d->nvqs)\r\nreturn -ENOBUFS;\r\nvq = d->vqs[idx];\r\nmutex_lock(&vq->mutex);\r\nswitch (ioctl) {\r\ncase VHOST_SET_VRING_NUM:\r\nif (vq->private_data) {\r\nr = -EBUSY;\r\nbreak;\r\n}\r\nif (copy_from_user(&s, argp, sizeof s)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif (!s.num || s.num > 0xffff || (s.num & (s.num - 1))) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nvq->num = s.num;\r\nbreak;\r\ncase VHOST_SET_VRING_BASE:\r\nif (vq->private_data) {\r\nr = -EBUSY;\r\nbreak;\r\n}\r\nif (copy_from_user(&s, argp, sizeof s)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif (s.num > 0xffff) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nvq->last_avail_idx = s.num;\r\nvq->avail_idx = vq->last_avail_idx;\r\nbreak;\r\ncase VHOST_GET_VRING_BASE:\r\ns.index = idx;\r\ns.num = vq->last_avail_idx;\r\nif (copy_to_user(argp, &s, sizeof s))\r\nr = -EFAULT;\r\nbreak;\r\ncase VHOST_SET_VRING_ADDR:\r\nif (copy_from_user(&a, argp, sizeof a)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif (a.flags & ~(0x1 << VHOST_VRING_F_LOG)) {\r\nr = -EOPNOTSUPP;\r\nbreak;\r\n}\r\nif ((u64)(unsigned long)a.desc_user_addr != a.desc_user_addr ||\r\n(u64)(unsigned long)a.used_user_addr != a.used_user_addr ||\r\n(u64)(unsigned long)a.avail_user_addr != a.avail_user_addr) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nBUILD_BUG_ON(__alignof__ *vq->avail > VRING_AVAIL_ALIGN_SIZE);\r\nBUILD_BUG_ON(__alignof__ *vq->used > VRING_USED_ALIGN_SIZE);\r\nif ((a.avail_user_addr & (VRING_AVAIL_ALIGN_SIZE - 1)) ||\r\n(a.used_user_addr & (VRING_USED_ALIGN_SIZE - 1)) ||\r\n(a.log_guest_addr & (VRING_USED_ALIGN_SIZE - 1))) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nif (vq->private_data) {\r\nif (!vq_access_ok(vq, vq->num,\r\n(void __user *)(unsigned long)a.desc_user_addr,\r\n(void __user *)(unsigned long)a.avail_user_addr,\r\n(void __user *)(unsigned long)a.used_user_addr)) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nif ((a.flags & (0x1 << VHOST_VRING_F_LOG)) &&\r\n!log_access_ok(vq->log_base, a.log_guest_addr,\r\nsizeof *vq->used +\r\nvq->num * sizeof *vq->used->ring)) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\n}\r\nvq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));\r\nvq->desc = (void __user *)(unsigned long)a.desc_user_addr;\r\nvq->avail = (void __user *)(unsigned long)a.avail_user_addr;\r\nvq->log_addr = a.log_guest_addr;\r\nvq->used = (void __user *)(unsigned long)a.used_user_addr;\r\nbreak;\r\ncase VHOST_SET_VRING_KICK:\r\nif (copy_from_user(&f, argp, sizeof f)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\neventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != vq->kick) {\r\npollstop = (filep = vq->kick) != NULL;\r\npollstart = (vq->kick = eventfp) != NULL;\r\n} else\r\nfilep = eventfp;\r\nbreak;\r\ncase VHOST_SET_VRING_CALL:\r\nif (copy_from_user(&f, argp, sizeof f)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\neventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != vq->call) {\r\nfilep = vq->call;\r\nctx = vq->call_ctx;\r\nvq->call = eventfp;\r\nvq->call_ctx = eventfp ?\r\neventfd_ctx_fileget(eventfp) : NULL;\r\n} else\r\nfilep = eventfp;\r\nbreak;\r\ncase VHOST_SET_VRING_ERR:\r\nif (copy_from_user(&f, argp, sizeof f)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\neventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != vq->error) {\r\nfilep = vq->error;\r\nvq->error = eventfp;\r\nctx = vq->error_ctx;\r\nvq->error_ctx = eventfp ?\r\neventfd_ctx_fileget(eventfp) : NULL;\r\n} else\r\nfilep = eventfp;\r\nbreak;\r\ncase VHOST_SET_VRING_ENDIAN:\r\nr = vhost_set_vring_endian(vq, argp);\r\nbreak;\r\ncase VHOST_GET_VRING_ENDIAN:\r\nr = vhost_get_vring_endian(vq, idx, argp);\r\nbreak;\r\ncase VHOST_SET_VRING_BUSYLOOP_TIMEOUT:\r\nif (copy_from_user(&s, argp, sizeof(s))) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nvq->busyloop_timeout = s.num;\r\nbreak;\r\ncase VHOST_GET_VRING_BUSYLOOP_TIMEOUT:\r\ns.index = idx;\r\ns.num = vq->busyloop_timeout;\r\nif (copy_to_user(argp, &s, sizeof(s)))\r\nr = -EFAULT;\r\nbreak;\r\ndefault:\r\nr = -ENOIOCTLCMD;\r\n}\r\nif (pollstop && vq->handle_kick)\r\nvhost_poll_stop(&vq->poll);\r\nif (ctx)\r\neventfd_ctx_put(ctx);\r\nif (filep)\r\nfput(filep);\r\nif (pollstart && vq->handle_kick)\r\nr = vhost_poll_start(&vq->poll, vq->kick);\r\nmutex_unlock(&vq->mutex);\r\nif (pollstop && vq->handle_kick)\r\nvhost_poll_flush(&vq->poll);\r\nreturn r;\r\n}\r\nlong vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)\r\n{\r\nstruct file *eventfp, *filep = NULL;\r\nstruct eventfd_ctx *ctx = NULL;\r\nu64 p;\r\nlong r;\r\nint i, fd;\r\nif (ioctl == VHOST_SET_OWNER) {\r\nr = vhost_dev_set_owner(d);\r\ngoto done;\r\n}\r\nr = vhost_dev_check_owner(d);\r\nif (r)\r\ngoto done;\r\nswitch (ioctl) {\r\ncase VHOST_SET_MEM_TABLE:\r\nr = vhost_set_memory(d, argp);\r\nbreak;\r\ncase VHOST_SET_LOG_BASE:\r\nif (copy_from_user(&p, argp, sizeof p)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif ((u64)(unsigned long)p != p) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nstruct vhost_virtqueue *vq;\r\nvoid __user *base = (void __user *)(unsigned long)p;\r\nvq = d->vqs[i];\r\nmutex_lock(&vq->mutex);\r\nif (vq->private_data && !vq_log_access_ok(vq, base))\r\nr = -EFAULT;\r\nelse\r\nvq->log_base = base;\r\nmutex_unlock(&vq->mutex);\r\n}\r\nbreak;\r\ncase VHOST_SET_LOG_FD:\r\nr = get_user(fd, (int __user *)argp);\r\nif (r < 0)\r\nbreak;\r\neventfp = fd == -1 ? NULL : eventfd_fget(fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != d->log_file) {\r\nfilep = d->log_file;\r\nd->log_file = eventfp;\r\nctx = d->log_ctx;\r\nd->log_ctx = eventfp ?\r\neventfd_ctx_fileget(eventfp) : NULL;\r\n} else\r\nfilep = eventfp;\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nmutex_lock(&d->vqs[i]->mutex);\r\nd->vqs[i]->log_ctx = d->log_ctx;\r\nmutex_unlock(&d->vqs[i]->mutex);\r\n}\r\nif (ctx)\r\neventfd_ctx_put(ctx);\r\nif (filep)\r\nfput(filep);\r\nbreak;\r\ndefault:\r\nr = -ENOIOCTLCMD;\r\nbreak;\r\n}\r\ndone:\r\nreturn r;\r\n}\r\nstatic const struct vhost_memory_region *find_region(struct vhost_memory *mem,\r\n__u64 addr, __u32 len)\r\n{\r\nconst struct vhost_memory_region *reg;\r\nint start = 0, end = mem->nregions;\r\nwhile (start < end) {\r\nint slot = start + (end - start) / 2;\r\nreg = mem->regions + slot;\r\nif (addr >= reg->guest_phys_addr)\r\nend = slot;\r\nelse\r\nstart = slot + 1;\r\n}\r\nreg = mem->regions + start;\r\nif (addr >= reg->guest_phys_addr &&\r\nreg->guest_phys_addr + reg->memory_size > addr)\r\nreturn reg;\r\nreturn NULL;\r\n}\r\nstatic int set_bit_to_user(int nr, void __user *addr)\r\n{\r\nunsigned long log = (unsigned long)addr;\r\nstruct page *page;\r\nvoid *base;\r\nint bit = nr + (log % PAGE_SIZE) * 8;\r\nint r;\r\nr = get_user_pages_fast(log, 1, 1, &page);\r\nif (r < 0)\r\nreturn r;\r\nBUG_ON(r != 1);\r\nbase = kmap_atomic(page);\r\nset_bit(bit, base);\r\nkunmap_atomic(base);\r\nset_page_dirty_lock(page);\r\nput_page(page);\r\nreturn 0;\r\n}\r\nstatic int log_write(void __user *log_base,\r\nu64 write_address, u64 write_length)\r\n{\r\nu64 write_page = write_address / VHOST_PAGE_SIZE;\r\nint r;\r\nif (!write_length)\r\nreturn 0;\r\nwrite_length += write_address % VHOST_PAGE_SIZE;\r\nfor (;;) {\r\nu64 base = (u64)(unsigned long)log_base;\r\nu64 log = base + write_page / 8;\r\nint bit = write_page % 8;\r\nif ((u64)(unsigned long)log != log)\r\nreturn -EFAULT;\r\nr = set_bit_to_user(bit, (void __user *)(unsigned long)log);\r\nif (r < 0)\r\nreturn r;\r\nif (write_length <= VHOST_PAGE_SIZE)\r\nbreak;\r\nwrite_length -= VHOST_PAGE_SIZE;\r\nwrite_page += 1;\r\n}\r\nreturn r;\r\n}\r\nint vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,\r\nunsigned int log_num, u64 len)\r\n{\r\nint i, r;\r\nsmp_wmb();\r\nfor (i = 0; i < log_num; ++i) {\r\nu64 l = min(log[i].len, len);\r\nr = log_write(vq->log_base, log[i].addr, l);\r\nif (r < 0)\r\nreturn r;\r\nlen -= l;\r\nif (!len) {\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\nreturn 0;\r\n}\r\n}\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic int vhost_update_used_flags(struct vhost_virtqueue *vq)\r\n{\r\nvoid __user *used;\r\nif (__put_user(cpu_to_vhost16(vq, vq->used_flags), &vq->used->flags) < 0)\r\nreturn -EFAULT;\r\nif (unlikely(vq->log_used)) {\r\nsmp_wmb();\r\nused = &vq->used->flags;\r\nlog_write(vq->log_base, vq->log_addr +\r\n(used - (void __user *)vq->used),\r\nsizeof vq->used->flags);\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\n}\r\nreturn 0;\r\n}\r\nstatic int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)\r\n{\r\nif (__put_user(cpu_to_vhost16(vq, vq->avail_idx), vhost_avail_event(vq)))\r\nreturn -EFAULT;\r\nif (unlikely(vq->log_used)) {\r\nvoid __user *used;\r\nsmp_wmb();\r\nused = vhost_avail_event(vq);\r\nlog_write(vq->log_base, vq->log_addr +\r\n(used - (void __user *)vq->used),\r\nsizeof *vhost_avail_event(vq));\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\n}\r\nreturn 0;\r\n}\r\nint vhost_vq_init_access(struct vhost_virtqueue *vq)\r\n{\r\n__virtio16 last_used_idx;\r\nint r;\r\nbool is_le = vq->is_le;\r\nif (!vq->private_data) {\r\nvhost_reset_is_le(vq);\r\nreturn 0;\r\n}\r\nvhost_init_is_le(vq);\r\nr = vhost_update_used_flags(vq);\r\nif (r)\r\ngoto err;\r\nvq->signalled_used_valid = false;\r\nif (!access_ok(VERIFY_READ, &vq->used->idx, sizeof vq->used->idx)) {\r\nr = -EFAULT;\r\ngoto err;\r\n}\r\nr = __get_user(last_used_idx, &vq->used->idx);\r\nif (r)\r\ngoto err;\r\nvq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);\r\nreturn 0;\r\nerr:\r\nvq->is_le = is_le;\r\nreturn r;\r\n}\r\nstatic int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,\r\nstruct iovec iov[], int iov_size)\r\n{\r\nconst struct vhost_memory_region *reg;\r\nstruct vhost_memory *mem;\r\nstruct iovec *_iov;\r\nu64 s = 0;\r\nint ret = 0;\r\nmem = vq->memory;\r\nwhile ((u64)len > s) {\r\nu64 size;\r\nif (unlikely(ret >= iov_size)) {\r\nret = -ENOBUFS;\r\nbreak;\r\n}\r\nreg = find_region(mem, addr, len);\r\nif (unlikely(!reg)) {\r\nret = -EFAULT;\r\nbreak;\r\n}\r\n_iov = iov + ret;\r\nsize = reg->memory_size - addr + reg->guest_phys_addr;\r\n_iov->iov_len = min((u64)len - s, size);\r\n_iov->iov_base = (void __user *)(unsigned long)\r\n(reg->userspace_addr + addr - reg->guest_phys_addr);\r\ns += size;\r\naddr += size;\r\n++ret;\r\n}\r\nreturn ret;\r\n}\r\nstatic unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)\r\n{\r\nunsigned int next;\r\nif (!(desc->flags & cpu_to_vhost16(vq, VRING_DESC_F_NEXT)))\r\nreturn -1U;\r\nnext = vhost16_to_cpu(vq, desc->next);\r\nread_barrier_depends();\r\nreturn next;\r\n}\r\nstatic int get_indirect(struct vhost_virtqueue *vq,\r\nstruct iovec iov[], unsigned int iov_size,\r\nunsigned int *out_num, unsigned int *in_num,\r\nstruct vhost_log *log, unsigned int *log_num,\r\nstruct vring_desc *indirect)\r\n{\r\nstruct vring_desc desc;\r\nunsigned int i = 0, count, found = 0;\r\nu32 len = vhost32_to_cpu(vq, indirect->len);\r\nstruct iov_iter from;\r\nint ret;\r\nif (unlikely(len % sizeof desc)) {\r\nvq_err(vq, "Invalid length in indirect descriptor: "\r\n"len 0x%llx not multiple of 0x%zx\n",\r\n(unsigned long long)len,\r\nsizeof desc);\r\nreturn -EINVAL;\r\n}\r\nret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,\r\nUIO_MAXIOV);\r\nif (unlikely(ret < 0)) {\r\nvq_err(vq, "Translation failure %d in indirect.\n", ret);\r\nreturn ret;\r\n}\r\niov_iter_init(&from, READ, vq->indirect, ret, len);\r\nread_barrier_depends();\r\ncount = len / sizeof desc;\r\nif (unlikely(count > USHRT_MAX + 1)) {\r\nvq_err(vq, "Indirect buffer length too big: %d\n",\r\nindirect->len);\r\nreturn -E2BIG;\r\n}\r\ndo {\r\nunsigned iov_count = *in_num + *out_num;\r\nif (unlikely(++found > count)) {\r\nvq_err(vq, "Loop detected: last one at %u "\r\n"indirect size %u\n",\r\ni, count);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(copy_from_iter(&desc, sizeof(desc), &from) !=\r\nsizeof(desc))) {\r\nvq_err(vq, "Failed indirect descriptor: idx %d, %zx\n",\r\ni, (size_t)vhost64_to_cpu(vq, indirect->addr) + i * sizeof desc);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT))) {\r\nvq_err(vq, "Nested indirect descriptor: idx %d, %zx\n",\r\ni, (size_t)vhost64_to_cpu(vq, indirect->addr) + i * sizeof desc);\r\nreturn -EINVAL;\r\n}\r\nret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),\r\nvhost32_to_cpu(vq, desc.len), iov + iov_count,\r\niov_size - iov_count);\r\nif (unlikely(ret < 0)) {\r\nvq_err(vq, "Translation failure %d indirect idx %d\n",\r\nret, i);\r\nreturn ret;\r\n}\r\nif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE)) {\r\n*in_num += ret;\r\nif (unlikely(log)) {\r\nlog[*log_num].addr = vhost64_to_cpu(vq, desc.addr);\r\nlog[*log_num].len = vhost32_to_cpu(vq, desc.len);\r\n++*log_num;\r\n}\r\n} else {\r\nif (unlikely(*in_num)) {\r\nvq_err(vq, "Indirect descriptor "\r\n"has out after in: idx %d\n", i);\r\nreturn -EINVAL;\r\n}\r\n*out_num += ret;\r\n}\r\n} while ((i = next_desc(vq, &desc)) != -1);\r\nreturn 0;\r\n}\r\nint vhost_get_vq_desc(struct vhost_virtqueue *vq,\r\nstruct iovec iov[], unsigned int iov_size,\r\nunsigned int *out_num, unsigned int *in_num,\r\nstruct vhost_log *log, unsigned int *log_num)\r\n{\r\nstruct vring_desc desc;\r\nunsigned int i, head, found = 0;\r\nu16 last_avail_idx;\r\n__virtio16 avail_idx;\r\n__virtio16 ring_head;\r\nint ret;\r\nlast_avail_idx = vq->last_avail_idx;\r\nif (unlikely(__get_user(avail_idx, &vq->avail->idx))) {\r\nvq_err(vq, "Failed to access avail idx at %p\n",\r\n&vq->avail->idx);\r\nreturn -EFAULT;\r\n}\r\nvq->avail_idx = vhost16_to_cpu(vq, avail_idx);\r\nif (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {\r\nvq_err(vq, "Guest moved used index from %u to %u",\r\nlast_avail_idx, vq->avail_idx);\r\nreturn -EFAULT;\r\n}\r\nif (vq->avail_idx == last_avail_idx)\r\nreturn vq->num;\r\nsmp_rmb();\r\nif (unlikely(__get_user(ring_head,\r\n&vq->avail->ring[last_avail_idx & (vq->num - 1)]))) {\r\nvq_err(vq, "Failed to read head: idx %d address %p\n",\r\nlast_avail_idx,\r\n&vq->avail->ring[last_avail_idx % vq->num]);\r\nreturn -EFAULT;\r\n}\r\nhead = vhost16_to_cpu(vq, ring_head);\r\nif (unlikely(head >= vq->num)) {\r\nvq_err(vq, "Guest says index %u > %u is available",\r\nhead, vq->num);\r\nreturn -EINVAL;\r\n}\r\n*out_num = *in_num = 0;\r\nif (unlikely(log))\r\n*log_num = 0;\r\ni = head;\r\ndo {\r\nunsigned iov_count = *in_num + *out_num;\r\nif (unlikely(i >= vq->num)) {\r\nvq_err(vq, "Desc index is %u > %u, head = %u",\r\ni, vq->num, head);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(++found > vq->num)) {\r\nvq_err(vq, "Loop detected: last one at %u "\r\n"vq size %u head %u\n",\r\ni, vq->num, head);\r\nreturn -EINVAL;\r\n}\r\nret = __copy_from_user(&desc, vq->desc + i, sizeof desc);\r\nif (unlikely(ret)) {\r\nvq_err(vq, "Failed to get descriptor: idx %d addr %p\n",\r\ni, vq->desc + i);\r\nreturn -EFAULT;\r\n}\r\nif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {\r\nret = get_indirect(vq, iov, iov_size,\r\nout_num, in_num,\r\nlog, log_num, &desc);\r\nif (unlikely(ret < 0)) {\r\nvq_err(vq, "Failure detected "\r\n"in indirect descriptor at idx %d\n", i);\r\nreturn ret;\r\n}\r\ncontinue;\r\n}\r\nret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),\r\nvhost32_to_cpu(vq, desc.len), iov + iov_count,\r\niov_size - iov_count);\r\nif (unlikely(ret < 0)) {\r\nvq_err(vq, "Translation failure %d descriptor idx %d\n",\r\nret, i);\r\nreturn ret;\r\n}\r\nif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE)) {\r\n*in_num += ret;\r\nif (unlikely(log)) {\r\nlog[*log_num].addr = vhost64_to_cpu(vq, desc.addr);\r\nlog[*log_num].len = vhost32_to_cpu(vq, desc.len);\r\n++*log_num;\r\n}\r\n} else {\r\nif (unlikely(*in_num)) {\r\nvq_err(vq, "Descriptor has out after in: "\r\n"idx %d\n", i);\r\nreturn -EINVAL;\r\n}\r\n*out_num += ret;\r\n}\r\n} while ((i = next_desc(vq, &desc)) != -1);\r\nvq->last_avail_idx++;\r\nBUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));\r\nreturn head;\r\n}\r\nvoid vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)\r\n{\r\nvq->last_avail_idx -= n;\r\n}\r\nint vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)\r\n{\r\nstruct vring_used_elem heads = {\r\ncpu_to_vhost32(vq, head),\r\ncpu_to_vhost32(vq, len)\r\n};\r\nreturn vhost_add_used_n(vq, &heads, 1);\r\n}\r\nstatic int __vhost_add_used_n(struct vhost_virtqueue *vq,\r\nstruct vring_used_elem *heads,\r\nunsigned count)\r\n{\r\nstruct vring_used_elem __user *used;\r\nu16 old, new;\r\nint start;\r\nstart = vq->last_used_idx & (vq->num - 1);\r\nused = vq->used->ring + start;\r\nif (count == 1) {\r\nif (__put_user(heads[0].id, &used->id)) {\r\nvq_err(vq, "Failed to write used id");\r\nreturn -EFAULT;\r\n}\r\nif (__put_user(heads[0].len, &used->len)) {\r\nvq_err(vq, "Failed to write used len");\r\nreturn -EFAULT;\r\n}\r\n} else if (__copy_to_user(used, heads, count * sizeof *used)) {\r\nvq_err(vq, "Failed to write used");\r\nreturn -EFAULT;\r\n}\r\nif (unlikely(vq->log_used)) {\r\nsmp_wmb();\r\nlog_write(vq->log_base,\r\nvq->log_addr +\r\n((void __user *)used - (void __user *)vq->used),\r\ncount * sizeof *used);\r\n}\r\nold = vq->last_used_idx;\r\nnew = (vq->last_used_idx += count);\r\nif (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))\r\nvq->signalled_used_valid = false;\r\nreturn 0;\r\n}\r\nint vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,\r\nunsigned count)\r\n{\r\nint start, n, r;\r\nstart = vq->last_used_idx & (vq->num - 1);\r\nn = vq->num - start;\r\nif (n < count) {\r\nr = __vhost_add_used_n(vq, heads, n);\r\nif (r < 0)\r\nreturn r;\r\nheads += n;\r\ncount -= n;\r\n}\r\nr = __vhost_add_used_n(vq, heads, count);\r\nsmp_wmb();\r\nif (__put_user(cpu_to_vhost16(vq, vq->last_used_idx), &vq->used->idx)) {\r\nvq_err(vq, "Failed to increment used idx");\r\nreturn -EFAULT;\r\n}\r\nif (unlikely(vq->log_used)) {\r\nlog_write(vq->log_base,\r\nvq->log_addr + offsetof(struct vring_used, idx),\r\nsizeof vq->used->idx);\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\n}\r\nreturn r;\r\n}\r\nstatic bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\n__u16 old, new;\r\n__virtio16 event;\r\nbool v;\r\nsmp_mb();\r\nif (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&\r\nunlikely(vq->avail_idx == vq->last_avail_idx))\r\nreturn true;\r\nif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\r\n__virtio16 flags;\r\nif (__get_user(flags, &vq->avail->flags)) {\r\nvq_err(vq, "Failed to get flags");\r\nreturn true;\r\n}\r\nreturn !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));\r\n}\r\nold = vq->signalled_used;\r\nv = vq->signalled_used_valid;\r\nnew = vq->signalled_used = vq->last_used_idx;\r\nvq->signalled_used_valid = true;\r\nif (unlikely(!v))\r\nreturn true;\r\nif (__get_user(event, vhost_used_event(vq))) {\r\nvq_err(vq, "Failed to get used event idx");\r\nreturn true;\r\n}\r\nreturn vring_need_event(vhost16_to_cpu(vq, event), new, old);\r\n}\r\nvoid vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\nif (vq->call_ctx && vhost_notify(dev, vq))\r\neventfd_signal(vq->call_ctx, 1);\r\n}\r\nvoid vhost_add_used_and_signal(struct vhost_dev *dev,\r\nstruct vhost_virtqueue *vq,\r\nunsigned int head, int len)\r\n{\r\nvhost_add_used(vq, head, len);\r\nvhost_signal(dev, vq);\r\n}\r\nvoid vhost_add_used_and_signal_n(struct vhost_dev *dev,\r\nstruct vhost_virtqueue *vq,\r\nstruct vring_used_elem *heads, unsigned count)\r\n{\r\nvhost_add_used_n(vq, heads, count);\r\nvhost_signal(dev, vq);\r\n}\r\nbool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\n__virtio16 avail_idx;\r\nint r;\r\nr = __get_user(avail_idx, &vq->avail->idx);\r\nif (r)\r\nreturn false;\r\nreturn vhost16_to_cpu(vq, avail_idx) == vq->avail_idx;\r\n}\r\nbool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\n__virtio16 avail_idx;\r\nint r;\r\nif (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))\r\nreturn false;\r\nvq->used_flags &= ~VRING_USED_F_NO_NOTIFY;\r\nif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\r\nr = vhost_update_used_flags(vq);\r\nif (r) {\r\nvq_err(vq, "Failed to enable notification at %p: %d\n",\r\n&vq->used->flags, r);\r\nreturn false;\r\n}\r\n} else {\r\nr = vhost_update_avail_event(vq, vq->avail_idx);\r\nif (r) {\r\nvq_err(vq, "Failed to update avail event index at %p: %d\n",\r\nvhost_avail_event(vq), r);\r\nreturn false;\r\n}\r\n}\r\nsmp_mb();\r\nr = __get_user(avail_idx, &vq->avail->idx);\r\nif (r) {\r\nvq_err(vq, "Failed to check avail idx at %p: %d\n",\r\n&vq->avail->idx, r);\r\nreturn false;\r\n}\r\nreturn vhost16_to_cpu(vq, avail_idx) != vq->avail_idx;\r\n}\r\nvoid vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\nint r;\r\nif (vq->used_flags & VRING_USED_F_NO_NOTIFY)\r\nreturn;\r\nvq->used_flags |= VRING_USED_F_NO_NOTIFY;\r\nif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\r\nr = vhost_update_used_flags(vq);\r\nif (r)\r\nvq_err(vq, "Failed to enable notification at %p: %d\n",\r\n&vq->used->flags, r);\r\n}\r\n}\r\nstatic int __init vhost_init(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic void __exit vhost_exit(void)\r\n{\r\n}
