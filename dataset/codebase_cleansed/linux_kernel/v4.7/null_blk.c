static int null_param_store_val(const char *str, int *val, int min, int max)\r\n{\r\nint ret, new_val;\r\nret = kstrtoint(str, 10, &new_val);\r\nif (ret)\r\nreturn -EINVAL;\r\nif (new_val < min || new_val > max)\r\nreturn -EINVAL;\r\n*val = new_val;\r\nreturn 0;\r\n}\r\nstatic int null_set_queue_mode(const char *str, const struct kernel_param *kp)\r\n{\r\nreturn null_param_store_val(str, &queue_mode, NULL_Q_BIO, NULL_Q_MQ);\r\n}\r\nstatic int null_set_irqmode(const char *str, const struct kernel_param *kp)\r\n{\r\nreturn null_param_store_val(str, &irqmode, NULL_IRQ_NONE,\r\nNULL_IRQ_TIMER);\r\n}\r\nstatic void put_tag(struct nullb_queue *nq, unsigned int tag)\r\n{\r\nclear_bit_unlock(tag, nq->tag_map);\r\nif (waitqueue_active(&nq->wait))\r\nwake_up(&nq->wait);\r\n}\r\nstatic unsigned int get_tag(struct nullb_queue *nq)\r\n{\r\nunsigned int tag;\r\ndo {\r\ntag = find_first_zero_bit(nq->tag_map, nq->queue_depth);\r\nif (tag >= nq->queue_depth)\r\nreturn -1U;\r\n} while (test_and_set_bit_lock(tag, nq->tag_map));\r\nreturn tag;\r\n}\r\nstatic void free_cmd(struct nullb_cmd *cmd)\r\n{\r\nput_tag(cmd->nq, cmd->tag);\r\n}\r\nstatic struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)\r\n{\r\nstruct nullb_cmd *cmd;\r\nunsigned int tag;\r\ntag = get_tag(nq);\r\nif (tag != -1U) {\r\ncmd = &nq->cmds[tag];\r\ncmd->tag = tag;\r\ncmd->nq = nq;\r\nif (irqmode == NULL_IRQ_TIMER) {\r\nhrtimer_init(&cmd->timer, CLOCK_MONOTONIC,\r\nHRTIMER_MODE_REL);\r\ncmd->timer.function = null_cmd_timer_expired;\r\n}\r\nreturn cmd;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)\r\n{\r\nstruct nullb_cmd *cmd;\r\nDEFINE_WAIT(wait);\r\ncmd = __alloc_cmd(nq);\r\nif (cmd || !can_wait)\r\nreturn cmd;\r\ndo {\r\nprepare_to_wait(&nq->wait, &wait, TASK_UNINTERRUPTIBLE);\r\ncmd = __alloc_cmd(nq);\r\nif (cmd)\r\nbreak;\r\nio_schedule();\r\n} while (1);\r\nfinish_wait(&nq->wait, &wait);\r\nreturn cmd;\r\n}\r\nstatic void end_cmd(struct nullb_cmd *cmd)\r\n{\r\nstruct request_queue *q = NULL;\r\nif (cmd->rq)\r\nq = cmd->rq->q;\r\nswitch (queue_mode) {\r\ncase NULL_Q_MQ:\r\nblk_mq_end_request(cmd->rq, 0);\r\nreturn;\r\ncase NULL_Q_RQ:\r\nINIT_LIST_HEAD(&cmd->rq->queuelist);\r\nblk_end_request_all(cmd->rq, 0);\r\nbreak;\r\ncase NULL_Q_BIO:\r\nbio_endio(cmd->bio);\r\nbreak;\r\n}\r\nfree_cmd(cmd);\r\nif (queue_mode == NULL_Q_RQ && blk_queue_stopped(q)) {\r\nunsigned long flags;\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_start_queue_async(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\n}\r\n}\r\nstatic enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer)\r\n{\r\nend_cmd(container_of(timer, struct nullb_cmd, timer));\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic void null_cmd_end_timer(struct nullb_cmd *cmd)\r\n{\r\nktime_t kt = ktime_set(0, completion_nsec);\r\nhrtimer_start(&cmd->timer, kt, HRTIMER_MODE_REL);\r\n}\r\nstatic void null_softirq_done_fn(struct request *rq)\r\n{\r\nif (queue_mode == NULL_Q_MQ)\r\nend_cmd(blk_mq_rq_to_pdu(rq));\r\nelse\r\nend_cmd(rq->special);\r\n}\r\nstatic inline void null_handle_cmd(struct nullb_cmd *cmd)\r\n{\r\nswitch (irqmode) {\r\ncase NULL_IRQ_SOFTIRQ:\r\nswitch (queue_mode) {\r\ncase NULL_Q_MQ:\r\nblk_mq_complete_request(cmd->rq, cmd->rq->errors);\r\nbreak;\r\ncase NULL_Q_RQ:\r\nblk_complete_request(cmd->rq);\r\nbreak;\r\ncase NULL_Q_BIO:\r\nend_cmd(cmd);\r\nbreak;\r\n}\r\nbreak;\r\ncase NULL_IRQ_NONE:\r\nend_cmd(cmd);\r\nbreak;\r\ncase NULL_IRQ_TIMER:\r\nnull_cmd_end_timer(cmd);\r\nbreak;\r\n}\r\n}\r\nstatic struct nullb_queue *nullb_to_queue(struct nullb *nullb)\r\n{\r\nint index = 0;\r\nif (nullb->nr_queues != 1)\r\nindex = raw_smp_processor_id() / ((nr_cpu_ids + nullb->nr_queues - 1) / nullb->nr_queues);\r\nreturn &nullb->queues[index];\r\n}\r\nstatic blk_qc_t null_queue_bio(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct nullb *nullb = q->queuedata;\r\nstruct nullb_queue *nq = nullb_to_queue(nullb);\r\nstruct nullb_cmd *cmd;\r\ncmd = alloc_cmd(nq, 1);\r\ncmd->bio = bio;\r\nnull_handle_cmd(cmd);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic int null_rq_prep_fn(struct request_queue *q, struct request *req)\r\n{\r\nstruct nullb *nullb = q->queuedata;\r\nstruct nullb_queue *nq = nullb_to_queue(nullb);\r\nstruct nullb_cmd *cmd;\r\ncmd = alloc_cmd(nq, 0);\r\nif (cmd) {\r\ncmd->rq = req;\r\nreq->special = cmd;\r\nreturn BLKPREP_OK;\r\n}\r\nblk_stop_queue(q);\r\nreturn BLKPREP_DEFER;\r\n}\r\nstatic void null_request_fn(struct request_queue *q)\r\n{\r\nstruct request *rq;\r\nwhile ((rq = blk_fetch_request(q)) != NULL) {\r\nstruct nullb_cmd *cmd = rq->special;\r\nspin_unlock_irq(q->queue_lock);\r\nnull_handle_cmd(cmd);\r\nspin_lock_irq(q->queue_lock);\r\n}\r\n}\r\nstatic int null_queue_rq(struct blk_mq_hw_ctx *hctx,\r\nconst struct blk_mq_queue_data *bd)\r\n{\r\nstruct nullb_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);\r\nif (irqmode == NULL_IRQ_TIMER) {\r\nhrtimer_init(&cmd->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\ncmd->timer.function = null_cmd_timer_expired;\r\n}\r\ncmd->rq = bd->rq;\r\ncmd->nq = hctx->driver_data;\r\nblk_mq_start_request(bd->rq);\r\nnull_handle_cmd(cmd);\r\nreturn BLK_MQ_RQ_QUEUE_OK;\r\n}\r\nstatic void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)\r\n{\r\nBUG_ON(!nullb);\r\nBUG_ON(!nq);\r\ninit_waitqueue_head(&nq->wait);\r\nnq->queue_depth = nullb->queue_depth;\r\n}\r\nstatic int null_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\r\nunsigned int index)\r\n{\r\nstruct nullb *nullb = data;\r\nstruct nullb_queue *nq = &nullb->queues[index];\r\nhctx->driver_data = nq;\r\nnull_init_queue(nullb, nq);\r\nnullb->nr_queues++;\r\nreturn 0;\r\n}\r\nstatic void cleanup_queue(struct nullb_queue *nq)\r\n{\r\nkfree(nq->tag_map);\r\nkfree(nq->cmds);\r\n}\r\nstatic void cleanup_queues(struct nullb *nullb)\r\n{\r\nint i;\r\nfor (i = 0; i < nullb->nr_queues; i++)\r\ncleanup_queue(&nullb->queues[i]);\r\nkfree(nullb->queues);\r\n}\r\nstatic void null_del_dev(struct nullb *nullb)\r\n{\r\nlist_del_init(&nullb->list);\r\nif (use_lightnvm)\r\nnvm_unregister(nullb->disk_name);\r\nelse\r\ndel_gendisk(nullb->disk);\r\nblk_cleanup_queue(nullb->q);\r\nif (queue_mode == NULL_Q_MQ)\r\nblk_mq_free_tag_set(&nullb->tag_set);\r\nif (!use_lightnvm)\r\nput_disk(nullb->disk);\r\ncleanup_queues(nullb);\r\nkfree(nullb);\r\n}\r\nstatic void null_lnvm_end_io(struct request *rq, int error)\r\n{\r\nstruct nvm_rq *rqd = rq->end_io_data;\r\nnvm_end_io(rqd, error);\r\nblk_put_request(rq);\r\n}\r\nstatic int null_lnvm_submit_io(struct nvm_dev *dev, struct nvm_rq *rqd)\r\n{\r\nstruct request_queue *q = dev->q;\r\nstruct request *rq;\r\nstruct bio *bio = rqd->bio;\r\nrq = blk_mq_alloc_request(q, bio_rw(bio), 0);\r\nif (IS_ERR(rq))\r\nreturn -ENOMEM;\r\nrq->cmd_type = REQ_TYPE_DRV_PRIV;\r\nrq->__sector = bio->bi_iter.bi_sector;\r\nrq->ioprio = bio_prio(bio);\r\nif (bio_has_data(bio))\r\nrq->nr_phys_segments = bio_phys_segments(q, bio);\r\nrq->__data_len = bio->bi_iter.bi_size;\r\nrq->bio = rq->biotail = bio;\r\nrq->end_io_data = rqd;\r\nblk_execute_rq_nowait(q, NULL, rq, 0, null_lnvm_end_io);\r\nreturn 0;\r\n}\r\nstatic int null_lnvm_id(struct nvm_dev *dev, struct nvm_id *id)\r\n{\r\nsector_t size = gb * 1024 * 1024 * 1024ULL;\r\nsector_t blksize;\r\nstruct nvm_id_group *grp;\r\nid->ver_id = 0x1;\r\nid->vmnt = 0;\r\nid->cgrps = 1;\r\nid->cap = 0x2;\r\nid->dom = 0x1;\r\nid->ppaf.blk_offset = 0;\r\nid->ppaf.blk_len = 16;\r\nid->ppaf.pg_offset = 16;\r\nid->ppaf.pg_len = 16;\r\nid->ppaf.sect_offset = 32;\r\nid->ppaf.sect_len = 8;\r\nid->ppaf.pln_offset = 40;\r\nid->ppaf.pln_len = 8;\r\nid->ppaf.lun_offset = 48;\r\nid->ppaf.lun_len = 8;\r\nid->ppaf.ch_offset = 56;\r\nid->ppaf.ch_len = 8;\r\nsector_div(size, bs);\r\nsize >>= 8;\r\ngrp = &id->groups[0];\r\ngrp->mtype = 0;\r\ngrp->fmtype = 0;\r\ngrp->num_ch = 1;\r\ngrp->num_pg = 256;\r\nblksize = size;\r\nsize >>= 16;\r\ngrp->num_lun = size + 1;\r\nsector_div(blksize, grp->num_lun);\r\ngrp->num_blk = blksize;\r\ngrp->num_pln = 1;\r\ngrp->fpg_sz = bs;\r\ngrp->csecs = bs;\r\ngrp->trdt = 25000;\r\ngrp->trdm = 25000;\r\ngrp->tprt = 500000;\r\ngrp->tprm = 500000;\r\ngrp->tbet = 1500000;\r\ngrp->tbem = 1500000;\r\ngrp->mpos = 0x010101;\r\ngrp->cpar = hw_queue_depth;\r\nreturn 0;\r\n}\r\nstatic void *null_lnvm_create_dma_pool(struct nvm_dev *dev, char *name)\r\n{\r\nmempool_t *virtmem_pool;\r\nvirtmem_pool = mempool_create_slab_pool(64, ppa_cache);\r\nif (!virtmem_pool) {\r\npr_err("null_blk: Unable to create virtual memory pool\n");\r\nreturn NULL;\r\n}\r\nreturn virtmem_pool;\r\n}\r\nstatic void null_lnvm_destroy_dma_pool(void *pool)\r\n{\r\nmempool_destroy(pool);\r\n}\r\nstatic void *null_lnvm_dev_dma_alloc(struct nvm_dev *dev, void *pool,\r\ngfp_t mem_flags, dma_addr_t *dma_handler)\r\n{\r\nreturn mempool_alloc(pool, mem_flags);\r\n}\r\nstatic void null_lnvm_dev_dma_free(void *pool, void *entry,\r\ndma_addr_t dma_handler)\r\n{\r\nmempool_free(entry, pool);\r\n}\r\nstatic int null_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nreturn 0;\r\n}\r\nstatic void null_release(struct gendisk *disk, fmode_t mode)\r\n{\r\n}\r\nstatic int setup_commands(struct nullb_queue *nq)\r\n{\r\nstruct nullb_cmd *cmd;\r\nint i, tag_size;\r\nnq->cmds = kzalloc(nq->queue_depth * sizeof(*cmd), GFP_KERNEL);\r\nif (!nq->cmds)\r\nreturn -ENOMEM;\r\ntag_size = ALIGN(nq->queue_depth, BITS_PER_LONG) / BITS_PER_LONG;\r\nnq->tag_map = kzalloc(tag_size * sizeof(unsigned long), GFP_KERNEL);\r\nif (!nq->tag_map) {\r\nkfree(nq->cmds);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < nq->queue_depth; i++) {\r\ncmd = &nq->cmds[i];\r\nINIT_LIST_HEAD(&cmd->list);\r\ncmd->ll_list.next = NULL;\r\ncmd->tag = -1U;\r\n}\r\nreturn 0;\r\n}\r\nstatic int setup_queues(struct nullb *nullb)\r\n{\r\nnullb->queues = kzalloc(submit_queues * sizeof(struct nullb_queue),\r\nGFP_KERNEL);\r\nif (!nullb->queues)\r\nreturn -ENOMEM;\r\nnullb->nr_queues = 0;\r\nnullb->queue_depth = hw_queue_depth;\r\nreturn 0;\r\n}\r\nstatic int init_driver_queues(struct nullb *nullb)\r\n{\r\nstruct nullb_queue *nq;\r\nint i, ret = 0;\r\nfor (i = 0; i < submit_queues; i++) {\r\nnq = &nullb->queues[i];\r\nnull_init_queue(nullb, nq);\r\nret = setup_commands(nq);\r\nif (ret)\r\nreturn ret;\r\nnullb->nr_queues++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int null_add_dev(void)\r\n{\r\nstruct gendisk *disk;\r\nstruct nullb *nullb;\r\nsector_t size;\r\nint rv;\r\nnullb = kzalloc_node(sizeof(*nullb), GFP_KERNEL, home_node);\r\nif (!nullb) {\r\nrv = -ENOMEM;\r\ngoto out;\r\n}\r\nspin_lock_init(&nullb->lock);\r\nif (queue_mode == NULL_Q_MQ && use_per_node_hctx)\r\nsubmit_queues = nr_online_nodes;\r\nrv = setup_queues(nullb);\r\nif (rv)\r\ngoto out_free_nullb;\r\nif (queue_mode == NULL_Q_MQ) {\r\nnullb->tag_set.ops = &null_mq_ops;\r\nnullb->tag_set.nr_hw_queues = submit_queues;\r\nnullb->tag_set.queue_depth = hw_queue_depth;\r\nnullb->tag_set.numa_node = home_node;\r\nnullb->tag_set.cmd_size = sizeof(struct nullb_cmd);\r\nnullb->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;\r\nnullb->tag_set.driver_data = nullb;\r\nrv = blk_mq_alloc_tag_set(&nullb->tag_set);\r\nif (rv)\r\ngoto out_cleanup_queues;\r\nnullb->q = blk_mq_init_queue(&nullb->tag_set);\r\nif (IS_ERR(nullb->q)) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_tags;\r\n}\r\n} else if (queue_mode == NULL_Q_BIO) {\r\nnullb->q = blk_alloc_queue_node(GFP_KERNEL, home_node);\r\nif (!nullb->q) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_queues;\r\n}\r\nblk_queue_make_request(nullb->q, null_queue_bio);\r\nrv = init_driver_queues(nullb);\r\nif (rv)\r\ngoto out_cleanup_blk_queue;\r\n} else {\r\nnullb->q = blk_init_queue_node(null_request_fn, &nullb->lock, home_node);\r\nif (!nullb->q) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_queues;\r\n}\r\nblk_queue_prep_rq(nullb->q, null_rq_prep_fn);\r\nblk_queue_softirq_done(nullb->q, null_softirq_done_fn);\r\nrv = init_driver_queues(nullb);\r\nif (rv)\r\ngoto out_cleanup_blk_queue;\r\n}\r\nnullb->q->queuedata = nullb;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, nullb->q);\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, nullb->q);\r\nmutex_lock(&lock);\r\nnullb->index = nullb_indexes++;\r\nmutex_unlock(&lock);\r\nblk_queue_logical_block_size(nullb->q, bs);\r\nblk_queue_physical_block_size(nullb->q, bs);\r\nsprintf(nullb->disk_name, "nullb%d", nullb->index);\r\nif (use_lightnvm) {\r\nrv = nvm_register(nullb->q, nullb->disk_name,\r\n&null_lnvm_dev_ops);\r\nif (rv)\r\ngoto out_cleanup_blk_queue;\r\ngoto done;\r\n}\r\ndisk = nullb->disk = alloc_disk_node(1, home_node);\r\nif (!disk) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_lightnvm;\r\n}\r\nsize = gb * 1024 * 1024 * 1024ULL;\r\nset_capacity(disk, size >> 9);\r\ndisk->flags |= GENHD_FL_EXT_DEVT | GENHD_FL_SUPPRESS_PARTITION_INFO;\r\ndisk->major = null_major;\r\ndisk->first_minor = nullb->index;\r\ndisk->fops = &null_fops;\r\ndisk->private_data = nullb;\r\ndisk->queue = nullb->q;\r\nstrncpy(disk->disk_name, nullb->disk_name, DISK_NAME_LEN);\r\nadd_disk(disk);\r\ndone:\r\nmutex_lock(&lock);\r\nlist_add_tail(&nullb->list, &nullb_list);\r\nmutex_unlock(&lock);\r\nreturn 0;\r\nout_cleanup_lightnvm:\r\nif (use_lightnvm)\r\nnvm_unregister(nullb->disk_name);\r\nout_cleanup_blk_queue:\r\nblk_cleanup_queue(nullb->q);\r\nout_cleanup_tags:\r\nif (queue_mode == NULL_Q_MQ)\r\nblk_mq_free_tag_set(&nullb->tag_set);\r\nout_cleanup_queues:\r\ncleanup_queues(nullb);\r\nout_free_nullb:\r\nkfree(nullb);\r\nout:\r\nreturn rv;\r\n}\r\nstatic int __init null_init(void)\r\n{\r\nint ret = 0;\r\nunsigned int i;\r\nstruct nullb *nullb;\r\nif (bs > PAGE_SIZE) {\r\npr_warn("null_blk: invalid block size\n");\r\npr_warn("null_blk: defaults block size to %lu\n", PAGE_SIZE);\r\nbs = PAGE_SIZE;\r\n}\r\nif (use_lightnvm && bs != 4096) {\r\npr_warn("null_blk: LightNVM only supports 4k block size\n");\r\npr_warn("null_blk: defaults block size to 4k\n");\r\nbs = 4096;\r\n}\r\nif (use_lightnvm && queue_mode != NULL_Q_MQ) {\r\npr_warn("null_blk: LightNVM only supported for blk-mq\n");\r\npr_warn("null_blk: defaults queue mode to blk-mq\n");\r\nqueue_mode = NULL_Q_MQ;\r\n}\r\nif (queue_mode == NULL_Q_MQ && use_per_node_hctx) {\r\nif (submit_queues < nr_online_nodes) {\r\npr_warn("null_blk: submit_queues param is set to %u.",\r\nnr_online_nodes);\r\nsubmit_queues = nr_online_nodes;\r\n}\r\n} else if (submit_queues > nr_cpu_ids)\r\nsubmit_queues = nr_cpu_ids;\r\nelse if (!submit_queues)\r\nsubmit_queues = 1;\r\nmutex_init(&lock);\r\nnull_major = register_blkdev(0, "nullb");\r\nif (null_major < 0)\r\nreturn null_major;\r\nif (use_lightnvm) {\r\nppa_cache = kmem_cache_create("ppa_cache", 64 * sizeof(u64),\r\n0, 0, NULL);\r\nif (!ppa_cache) {\r\npr_err("null_blk: unable to create ppa cache\n");\r\nret = -ENOMEM;\r\ngoto err_ppa;\r\n}\r\n}\r\nfor (i = 0; i < nr_devices; i++) {\r\nret = null_add_dev();\r\nif (ret)\r\ngoto err_dev;\r\n}\r\npr_info("null: module loaded\n");\r\nreturn 0;\r\nerr_dev:\r\nwhile (!list_empty(&nullb_list)) {\r\nnullb = list_entry(nullb_list.next, struct nullb, list);\r\nnull_del_dev(nullb);\r\n}\r\nkmem_cache_destroy(ppa_cache);\r\nerr_ppa:\r\nunregister_blkdev(null_major, "nullb");\r\nreturn ret;\r\n}\r\nstatic void __exit null_exit(void)\r\n{\r\nstruct nullb *nullb;\r\nunregister_blkdev(null_major, "nullb");\r\nmutex_lock(&lock);\r\nwhile (!list_empty(&nullb_list)) {\r\nnullb = list_entry(nullb_list.next, struct nullb, list);\r\nnull_del_dev(nullb);\r\n}\r\nmutex_unlock(&lock);\r\nkmem_cache_destroy(ppa_cache);\r\n}
