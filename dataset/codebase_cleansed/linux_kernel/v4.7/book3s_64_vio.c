static unsigned long kvmppc_tce_pages(unsigned long iommu_pages)\r\n{\r\nreturn ALIGN(iommu_pages * sizeof(u64), PAGE_SIZE) / PAGE_SIZE;\r\n}\r\nstatic unsigned long kvmppc_stt_pages(unsigned long tce_pages)\r\n{\r\nunsigned long stt_bytes = sizeof(struct kvmppc_spapr_tce_table) +\r\n(tce_pages * sizeof(struct page *));\r\nreturn tce_pages + ALIGN(stt_bytes, PAGE_SIZE) / PAGE_SIZE;\r\n}\r\nstatic long kvmppc_account_memlimit(unsigned long stt_pages, bool inc)\r\n{\r\nlong ret = 0;\r\nif (!current || !current->mm)\r\nreturn ret;\r\ndown_write(&current->mm->mmap_sem);\r\nif (inc) {\r\nunsigned long locked, lock_limit;\r\nlocked = current->mm->locked_vm + stt_pages;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif (locked > lock_limit && !capable(CAP_IPC_LOCK))\r\nret = -ENOMEM;\r\nelse\r\ncurrent->mm->locked_vm += stt_pages;\r\n} else {\r\nif (WARN_ON_ONCE(stt_pages > current->mm->locked_vm))\r\nstt_pages = current->mm->locked_vm;\r\ncurrent->mm->locked_vm -= stt_pages;\r\n}\r\npr_debug("[%d] RLIMIT_MEMLOCK KVM %c%ld %ld/%ld%s\n", current->pid,\r\ninc ? '+' : '-',\r\nstt_pages << PAGE_SHIFT,\r\ncurrent->mm->locked_vm << PAGE_SHIFT,\r\nrlimit(RLIMIT_MEMLOCK),\r\nret ? " - exceeded" : "");\r\nup_write(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic void release_spapr_tce_table(struct rcu_head *head)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = container_of(head,\r\nstruct kvmppc_spapr_tce_table, rcu);\r\nunsigned long i, npages = kvmppc_tce_pages(stt->size);\r\nfor (i = 0; i < npages; i++)\r\n__free_page(stt->pages[i]);\r\nkfree(stt);\r\n}\r\nstatic int kvm_spapr_tce_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = vma->vm_file->private_data;\r\nstruct page *page;\r\nif (vmf->pgoff >= kvmppc_tce_pages(stt->size))\r\nreturn VM_FAULT_SIGBUS;\r\npage = stt->pages[vmf->pgoff];\r\nget_page(page);\r\nvmf->page = page;\r\nreturn 0;\r\n}\r\nstatic int kvm_spapr_tce_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nvma->vm_ops = &kvm_spapr_tce_vm_ops;\r\nreturn 0;\r\n}\r\nstatic int kvm_spapr_tce_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = filp->private_data;\r\nlist_del_rcu(&stt->list);\r\nkvm_put_kvm(stt->kvm);\r\nkvmppc_account_memlimit(\r\nkvmppc_stt_pages(kvmppc_tce_pages(stt->size)), false);\r\ncall_rcu(&stt->rcu, release_spapr_tce_table);\r\nreturn 0;\r\n}\r\nlong kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,\r\nstruct kvm_create_spapr_tce_64 *args)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = NULL;\r\nunsigned long npages, size;\r\nint ret = -ENOMEM;\r\nint i;\r\nif (!args->size)\r\nreturn -EINVAL;\r\nlist_for_each_entry(stt, &kvm->arch.spapr_tce_tables, list) {\r\nif (stt->liobn == args->liobn)\r\nreturn -EBUSY;\r\n}\r\nsize = args->size;\r\nnpages = kvmppc_tce_pages(size);\r\nret = kvmppc_account_memlimit(kvmppc_stt_pages(npages), true);\r\nif (ret) {\r\nstt = NULL;\r\ngoto fail;\r\n}\r\nstt = kzalloc(sizeof(*stt) + npages * sizeof(struct page *),\r\nGFP_KERNEL);\r\nif (!stt)\r\ngoto fail;\r\nstt->liobn = args->liobn;\r\nstt->page_shift = args->page_shift;\r\nstt->offset = args->offset;\r\nstt->size = size;\r\nstt->kvm = kvm;\r\nfor (i = 0; i < npages; i++) {\r\nstt->pages[i] = alloc_page(GFP_KERNEL | __GFP_ZERO);\r\nif (!stt->pages[i])\r\ngoto fail;\r\n}\r\nkvm_get_kvm(kvm);\r\nmutex_lock(&kvm->lock);\r\nlist_add_rcu(&stt->list, &kvm->arch.spapr_tce_tables);\r\nmutex_unlock(&kvm->lock);\r\nreturn anon_inode_getfd("kvm-spapr-tce", &kvm_spapr_tce_fops,\r\nstt, O_RDWR | O_CLOEXEC);\r\nfail:\r\nif (stt) {\r\nfor (i = 0; i < npages; i++)\r\nif (stt->pages[i])\r\n__free_page(stt->pages[i]);\r\nkfree(stt);\r\n}\r\nreturn ret;\r\n}\r\nlong kvmppc_h_put_tce(struct kvm_vcpu *vcpu, unsigned long liobn,\r\nunsigned long ioba, unsigned long tce)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = kvmppc_find_table(vcpu, liobn);\r\nlong ret;\r\nif (!stt)\r\nreturn H_TOO_HARD;\r\nret = kvmppc_ioba_validate(stt, ioba, 1);\r\nif (ret != H_SUCCESS)\r\nreturn ret;\r\nret = kvmppc_tce_validate(stt, tce);\r\nif (ret != H_SUCCESS)\r\nreturn ret;\r\nkvmppc_tce_put(stt, ioba >> stt->page_shift, tce);\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_put_tce_indirect(struct kvm_vcpu *vcpu,\r\nunsigned long liobn, unsigned long ioba,\r\nunsigned long tce_list, unsigned long npages)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt;\r\nlong i, ret = H_SUCCESS, idx;\r\nunsigned long entry, ua = 0;\r\nu64 __user *tces, tce;\r\nstt = kvmppc_find_table(vcpu, liobn);\r\nif (!stt)\r\nreturn H_TOO_HARD;\r\nentry = ioba >> stt->page_shift;\r\nif (npages > 512)\r\nreturn H_PARAMETER;\r\nif (tce_list & (SZ_4K - 1))\r\nreturn H_PARAMETER;\r\nret = kvmppc_ioba_validate(stt, ioba, npages);\r\nif (ret != H_SUCCESS)\r\nreturn ret;\r\nidx = srcu_read_lock(&vcpu->kvm->srcu);\r\nif (kvmppc_gpa_to_ua(vcpu->kvm, tce_list, &ua, NULL)) {\r\nret = H_TOO_HARD;\r\ngoto unlock_exit;\r\n}\r\ntces = (u64 __user *) ua;\r\nfor (i = 0; i < npages; ++i) {\r\nif (get_user(tce, tces + i)) {\r\nret = H_TOO_HARD;\r\ngoto unlock_exit;\r\n}\r\ntce = be64_to_cpu(tce);\r\nret = kvmppc_tce_validate(stt, tce);\r\nif (ret != H_SUCCESS)\r\ngoto unlock_exit;\r\nkvmppc_tce_put(stt, entry + i, tce);\r\n}\r\nunlock_exit:\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nreturn ret;\r\n}\r\nlong kvmppc_h_stuff_tce(struct kvm_vcpu *vcpu,\r\nunsigned long liobn, unsigned long ioba,\r\nunsigned long tce_value, unsigned long npages)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt;\r\nlong i, ret;\r\nstt = kvmppc_find_table(vcpu, liobn);\r\nif (!stt)\r\nreturn H_TOO_HARD;\r\nret = kvmppc_ioba_validate(stt, ioba, npages);\r\nif (ret != H_SUCCESS)\r\nreturn ret;\r\nif (tce_value & (TCE_PCI_WRITE | TCE_PCI_READ))\r\nreturn H_PARAMETER;\r\nfor (i = 0; i < npages; ++i, ioba += (1ULL << stt->page_shift))\r\nkvmppc_tce_put(stt, ioba >> stt->page_shift, tce_value);\r\nreturn H_SUCCESS;\r\n}
