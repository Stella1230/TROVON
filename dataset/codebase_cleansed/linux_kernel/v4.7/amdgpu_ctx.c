static int amdgpu_ctx_init(struct amdgpu_device *adev, struct amdgpu_ctx *ctx)\r\n{\r\nunsigned i, j;\r\nint r;\r\nmemset(ctx, 0, sizeof(*ctx));\r\nctx->adev = adev;\r\nkref_init(&ctx->refcount);\r\nspin_lock_init(&ctx->ring_lock);\r\nctx->fences = kcalloc(amdgpu_sched_jobs * AMDGPU_MAX_RINGS,\r\nsizeof(struct fence*), GFP_KERNEL);\r\nif (!ctx->fences)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\r\nctx->rings[i].sequence = 1;\r\nctx->rings[i].fences = &ctx->fences[amdgpu_sched_jobs * i];\r\n}\r\nfor (i = 0; i < adev->num_rings; i++) {\r\nstruct amdgpu_ring *ring = adev->rings[i];\r\nstruct amd_sched_rq *rq;\r\nrq = &ring->sched.sched_rq[AMD_SCHED_PRIORITY_NORMAL];\r\nr = amd_sched_entity_init(&ring->sched, &ctx->rings[i].entity,\r\nrq, amdgpu_sched_jobs);\r\nif (r)\r\nbreak;\r\n}\r\nif (i < adev->num_rings) {\r\nfor (j = 0; j < i; j++)\r\namd_sched_entity_fini(&adev->rings[j]->sched,\r\n&ctx->rings[j].entity);\r\nkfree(ctx->fences);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic void amdgpu_ctx_fini(struct amdgpu_ctx *ctx)\r\n{\r\nstruct amdgpu_device *adev = ctx->adev;\r\nunsigned i, j;\r\nif (!adev)\r\nreturn;\r\nfor (i = 0; i < AMDGPU_MAX_RINGS; ++i)\r\nfor (j = 0; j < amdgpu_sched_jobs; ++j)\r\nfence_put(ctx->rings[i].fences[j]);\r\nkfree(ctx->fences);\r\nfor (i = 0; i < adev->num_rings; i++)\r\namd_sched_entity_fini(&adev->rings[i]->sched,\r\n&ctx->rings[i].entity);\r\n}\r\nstatic int amdgpu_ctx_alloc(struct amdgpu_device *adev,\r\nstruct amdgpu_fpriv *fpriv,\r\nuint32_t *id)\r\n{\r\nstruct amdgpu_ctx_mgr *mgr = &fpriv->ctx_mgr;\r\nstruct amdgpu_ctx *ctx;\r\nint r;\r\nctx = kmalloc(sizeof(*ctx), GFP_KERNEL);\r\nif (!ctx)\r\nreturn -ENOMEM;\r\nmutex_lock(&mgr->lock);\r\nr = idr_alloc(&mgr->ctx_handles, ctx, 1, 0, GFP_KERNEL);\r\nif (r < 0) {\r\nmutex_unlock(&mgr->lock);\r\nkfree(ctx);\r\nreturn r;\r\n}\r\n*id = (uint32_t)r;\r\nr = amdgpu_ctx_init(adev, ctx);\r\nif (r) {\r\nidr_remove(&mgr->ctx_handles, *id);\r\n*id = 0;\r\nkfree(ctx);\r\n}\r\nmutex_unlock(&mgr->lock);\r\nreturn r;\r\n}\r\nstatic void amdgpu_ctx_do_release(struct kref *ref)\r\n{\r\nstruct amdgpu_ctx *ctx;\r\nctx = container_of(ref, struct amdgpu_ctx, refcount);\r\namdgpu_ctx_fini(ctx);\r\nkfree(ctx);\r\n}\r\nstatic int amdgpu_ctx_free(struct amdgpu_fpriv *fpriv, uint32_t id)\r\n{\r\nstruct amdgpu_ctx_mgr *mgr = &fpriv->ctx_mgr;\r\nstruct amdgpu_ctx *ctx;\r\nmutex_lock(&mgr->lock);\r\nctx = idr_find(&mgr->ctx_handles, id);\r\nif (ctx) {\r\nidr_remove(&mgr->ctx_handles, id);\r\nkref_put(&ctx->refcount, amdgpu_ctx_do_release);\r\nmutex_unlock(&mgr->lock);\r\nreturn 0;\r\n}\r\nmutex_unlock(&mgr->lock);\r\nreturn -EINVAL;\r\n}\r\nstatic int amdgpu_ctx_query(struct amdgpu_device *adev,\r\nstruct amdgpu_fpriv *fpriv, uint32_t id,\r\nunion drm_amdgpu_ctx_out *out)\r\n{\r\nstruct amdgpu_ctx *ctx;\r\nstruct amdgpu_ctx_mgr *mgr;\r\nunsigned reset_counter;\r\nif (!fpriv)\r\nreturn -EINVAL;\r\nmgr = &fpriv->ctx_mgr;\r\nmutex_lock(&mgr->lock);\r\nctx = idr_find(&mgr->ctx_handles, id);\r\nif (!ctx) {\r\nmutex_unlock(&mgr->lock);\r\nreturn -EINVAL;\r\n}\r\nout->state.flags = 0x0;\r\nout->state.hangs = 0x0;\r\nreset_counter = atomic_read(&adev->gpu_reset_counter);\r\nif (ctx->reset_counter == reset_counter)\r\nout->state.reset_status = AMDGPU_CTX_NO_RESET;\r\nelse\r\nout->state.reset_status = AMDGPU_CTX_UNKNOWN_RESET;\r\nctx->reset_counter = reset_counter;\r\nmutex_unlock(&mgr->lock);\r\nreturn 0;\r\n}\r\nint amdgpu_ctx_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nint r;\r\nuint32_t id;\r\nunion drm_amdgpu_ctx *args = data;\r\nstruct amdgpu_device *adev = dev->dev_private;\r\nstruct amdgpu_fpriv *fpriv = filp->driver_priv;\r\nr = 0;\r\nid = args->in.ctx_id;\r\nswitch (args->in.op) {\r\ncase AMDGPU_CTX_OP_ALLOC_CTX:\r\nr = amdgpu_ctx_alloc(adev, fpriv, &id);\r\nargs->out.alloc.ctx_id = id;\r\nbreak;\r\ncase AMDGPU_CTX_OP_FREE_CTX:\r\nr = amdgpu_ctx_free(fpriv, id);\r\nbreak;\r\ncase AMDGPU_CTX_OP_QUERY_STATE:\r\nr = amdgpu_ctx_query(adev, fpriv, id, &args->out);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn r;\r\n}\r\nstruct amdgpu_ctx *amdgpu_ctx_get(struct amdgpu_fpriv *fpriv, uint32_t id)\r\n{\r\nstruct amdgpu_ctx *ctx;\r\nstruct amdgpu_ctx_mgr *mgr;\r\nif (!fpriv)\r\nreturn NULL;\r\nmgr = &fpriv->ctx_mgr;\r\nmutex_lock(&mgr->lock);\r\nctx = idr_find(&mgr->ctx_handles, id);\r\nif (ctx)\r\nkref_get(&ctx->refcount);\r\nmutex_unlock(&mgr->lock);\r\nreturn ctx;\r\n}\r\nint amdgpu_ctx_put(struct amdgpu_ctx *ctx)\r\n{\r\nif (ctx == NULL)\r\nreturn -EINVAL;\r\nkref_put(&ctx->refcount, amdgpu_ctx_do_release);\r\nreturn 0;\r\n}\r\nuint64_t amdgpu_ctx_add_fence(struct amdgpu_ctx *ctx, struct amdgpu_ring *ring,\r\nstruct fence *fence)\r\n{\r\nstruct amdgpu_ctx_ring *cring = & ctx->rings[ring->idx];\r\nuint64_t seq = cring->sequence;\r\nunsigned idx = 0;\r\nstruct fence *other = NULL;\r\nidx = seq & (amdgpu_sched_jobs - 1);\r\nother = cring->fences[idx];\r\nif (other) {\r\nsigned long r;\r\nr = fence_wait_timeout(other, false, MAX_SCHEDULE_TIMEOUT);\r\nif (r < 0)\r\nDRM_ERROR("Error (%ld) waiting for fence!\n", r);\r\n}\r\nfence_get(fence);\r\nspin_lock(&ctx->ring_lock);\r\ncring->fences[idx] = fence;\r\ncring->sequence++;\r\nspin_unlock(&ctx->ring_lock);\r\nfence_put(other);\r\nreturn seq;\r\n}\r\nstruct fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,\r\nstruct amdgpu_ring *ring, uint64_t seq)\r\n{\r\nstruct amdgpu_ctx_ring *cring = & ctx->rings[ring->idx];\r\nstruct fence *fence;\r\nspin_lock(&ctx->ring_lock);\r\nif (seq >= cring->sequence) {\r\nspin_unlock(&ctx->ring_lock);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nif (seq + amdgpu_sched_jobs < cring->sequence) {\r\nspin_unlock(&ctx->ring_lock);\r\nreturn NULL;\r\n}\r\nfence = fence_get(cring->fences[seq & (amdgpu_sched_jobs - 1)]);\r\nspin_unlock(&ctx->ring_lock);\r\nreturn fence;\r\n}\r\nvoid amdgpu_ctx_mgr_init(struct amdgpu_ctx_mgr *mgr)\r\n{\r\nmutex_init(&mgr->lock);\r\nidr_init(&mgr->ctx_handles);\r\n}\r\nvoid amdgpu_ctx_mgr_fini(struct amdgpu_ctx_mgr *mgr)\r\n{\r\nstruct amdgpu_ctx *ctx;\r\nstruct idr *idp;\r\nuint32_t id;\r\nidp = &mgr->ctx_handles;\r\nidr_for_each_entry(idp, ctx, id) {\r\nif (kref_put(&ctx->refcount, amdgpu_ctx_do_release) != 1)\r\nDRM_ERROR("ctx %p is still alive\n", ctx);\r\n}\r\nidr_destroy(&mgr->ctx_handles);\r\nmutex_destroy(&mgr->lock);\r\n}
