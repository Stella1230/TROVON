static void fq_flow_set_detached(struct fq_flow *f)\r\n{\r\nf->next = &detached;\r\nf->age = jiffies;\r\n}\r\nstatic bool fq_flow_is_detached(const struct fq_flow *f)\r\n{\r\nreturn f->next == &detached;\r\n}\r\nstatic void fq_flow_set_throttled(struct fq_sched_data *q, struct fq_flow *f)\r\n{\r\nstruct rb_node **p = &q->delayed.rb_node, *parent = NULL;\r\nwhile (*p) {\r\nstruct fq_flow *aux;\r\nparent = *p;\r\naux = container_of(parent, struct fq_flow, rate_node);\r\nif (f->time_next_packet >= aux->time_next_packet)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nrb_link_node(&f->rate_node, parent, p);\r\nrb_insert_color(&f->rate_node, &q->delayed);\r\nq->throttled_flows++;\r\nq->stat_throttled++;\r\nf->next = &throttled;\r\nif (q->time_next_delayed_flow > f->time_next_packet)\r\nq->time_next_delayed_flow = f->time_next_packet;\r\n}\r\nstatic void fq_flow_add_tail(struct fq_flow_head *head, struct fq_flow *flow)\r\n{\r\nif (head->first)\r\nhead->last->next = flow;\r\nelse\r\nhead->first = flow;\r\nhead->last = flow;\r\nflow->next = NULL;\r\n}\r\nstatic bool fq_gc_candidate(const struct fq_flow *f)\r\n{\r\nreturn fq_flow_is_detached(f) &&\r\ntime_after(jiffies, f->age + FQ_GC_AGE);\r\n}\r\nstatic void fq_gc(struct fq_sched_data *q,\r\nstruct rb_root *root,\r\nstruct sock *sk)\r\n{\r\nstruct fq_flow *f, *tofree[FQ_GC_MAX];\r\nstruct rb_node **p, *parent;\r\nint fcnt = 0;\r\np = &root->rb_node;\r\nparent = NULL;\r\nwhile (*p) {\r\nparent = *p;\r\nf = container_of(parent, struct fq_flow, fq_node);\r\nif (f->sk == sk)\r\nbreak;\r\nif (fq_gc_candidate(f)) {\r\ntofree[fcnt++] = f;\r\nif (fcnt == FQ_GC_MAX)\r\nbreak;\r\n}\r\nif (f->sk > sk)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nq->flows -= fcnt;\r\nq->inactive_flows -= fcnt;\r\nq->stat_gc_flows += fcnt;\r\nwhile (fcnt) {\r\nstruct fq_flow *f = tofree[--fcnt];\r\nrb_erase(&f->fq_node, root);\r\nkmem_cache_free(fq_flow_cachep, f);\r\n}\r\n}\r\nstatic struct fq_flow *fq_classify(struct sk_buff *skb, struct fq_sched_data *q)\r\n{\r\nstruct rb_node **p, *parent;\r\nstruct sock *sk = skb->sk;\r\nstruct rb_root *root;\r\nstruct fq_flow *f;\r\nif (unlikely((skb->priority & TC_PRIO_MAX) == TC_PRIO_CONTROL))\r\nreturn &q->internal;\r\nif (!sk || sk_listener(sk)) {\r\nunsigned long hash = skb_get_hash(skb) & q->orphan_mask;\r\nsk = (struct sock *)((hash << 1) | 1UL);\r\nskb_orphan(skb);\r\n}\r\nroot = &q->fq_root[hash_32((u32)(long)sk, q->fq_trees_log)];\r\nif (q->flows >= (2U << q->fq_trees_log) &&\r\nq->inactive_flows > q->flows/2)\r\nfq_gc(q, root, sk);\r\np = &root->rb_node;\r\nparent = NULL;\r\nwhile (*p) {\r\nparent = *p;\r\nf = container_of(parent, struct fq_flow, fq_node);\r\nif (f->sk == sk) {\r\nif (unlikely(skb->sk &&\r\nf->socket_hash != sk->sk_hash)) {\r\nf->credit = q->initial_quantum;\r\nf->socket_hash = sk->sk_hash;\r\nf->time_next_packet = 0ULL;\r\n}\r\nreturn f;\r\n}\r\nif (f->sk > sk)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nf = kmem_cache_zalloc(fq_flow_cachep, GFP_ATOMIC | __GFP_NOWARN);\r\nif (unlikely(!f)) {\r\nq->stat_allocation_errors++;\r\nreturn &q->internal;\r\n}\r\nfq_flow_set_detached(f);\r\nf->sk = sk;\r\nif (skb->sk)\r\nf->socket_hash = sk->sk_hash;\r\nf->credit = q->initial_quantum;\r\nrb_link_node(&f->fq_node, parent, p);\r\nrb_insert_color(&f->fq_node, root);\r\nq->flows++;\r\nq->inactive_flows++;\r\nreturn f;\r\n}\r\nstatic struct sk_buff *fq_dequeue_head(struct Qdisc *sch, struct fq_flow *flow)\r\n{\r\nstruct sk_buff *skb = flow->head;\r\nif (skb) {\r\nflow->head = skb->next;\r\nskb->next = NULL;\r\nflow->qlen--;\r\nqdisc_qstats_backlog_dec(sch, skb);\r\nsch->q.qlen--;\r\n}\r\nreturn skb;\r\n}\r\nstatic bool skb_is_retransmit(struct sk_buff *skb)\r\n{\r\nreturn false;\r\n}\r\nstatic void flow_queue_add(struct fq_flow *flow, struct sk_buff *skb)\r\n{\r\nstruct sk_buff *prev, *head = flow->head;\r\nskb->next = NULL;\r\nif (!head) {\r\nflow->head = skb;\r\nflow->tail = skb;\r\nreturn;\r\n}\r\nif (likely(!skb_is_retransmit(skb))) {\r\nflow->tail->next = skb;\r\nflow->tail = skb;\r\nreturn;\r\n}\r\nprev = NULL;\r\nwhile (skb_is_retransmit(head)) {\r\nprev = head;\r\nhead = head->next;\r\nif (!head)\r\nbreak;\r\n}\r\nif (!prev) {\r\nskb->next = flow->head;\r\nflow->head = skb;\r\n} else {\r\nif (prev == flow->tail)\r\nflow->tail = skb;\r\nelse\r\nskb->next = prev->next;\r\nprev->next = skb;\r\n}\r\n}\r\nstatic int fq_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nstruct fq_flow *f;\r\nif (unlikely(sch->q.qlen >= sch->limit))\r\nreturn qdisc_drop(skb, sch);\r\nf = fq_classify(skb, q);\r\nif (unlikely(f->qlen >= q->flow_plimit && f != &q->internal)) {\r\nq->stat_flows_plimit++;\r\nreturn qdisc_drop(skb, sch);\r\n}\r\nf->qlen++;\r\nif (skb_is_retransmit(skb))\r\nq->stat_tcp_retrans++;\r\nqdisc_qstats_backlog_inc(sch, skb);\r\nif (fq_flow_is_detached(f)) {\r\nfq_flow_add_tail(&q->new_flows, f);\r\nif (time_after(jiffies, f->age + q->flow_refill_delay))\r\nf->credit = max_t(u32, f->credit, q->quantum);\r\nq->inactive_flows--;\r\n}\r\nflow_queue_add(f, skb);\r\nif (unlikely(f == &q->internal)) {\r\nq->stat_internal_packets++;\r\n}\r\nsch->q.qlen++;\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic void fq_check_throttled(struct fq_sched_data *q, u64 now)\r\n{\r\nstruct rb_node *p;\r\nif (q->time_next_delayed_flow > now)\r\nreturn;\r\nq->time_next_delayed_flow = ~0ULL;\r\nwhile ((p = rb_first(&q->delayed)) != NULL) {\r\nstruct fq_flow *f = container_of(p, struct fq_flow, rate_node);\r\nif (f->time_next_packet > now) {\r\nq->time_next_delayed_flow = f->time_next_packet;\r\nbreak;\r\n}\r\nrb_erase(p, &q->delayed);\r\nq->throttled_flows--;\r\nfq_flow_add_tail(&q->old_flows, f);\r\n}\r\n}\r\nstatic struct sk_buff *fq_dequeue(struct Qdisc *sch)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nu64 now = ktime_get_ns();\r\nstruct fq_flow_head *head;\r\nstruct sk_buff *skb;\r\nstruct fq_flow *f;\r\nu32 rate;\r\nskb = fq_dequeue_head(sch, &q->internal);\r\nif (skb)\r\ngoto out;\r\nfq_check_throttled(q, now);\r\nbegin:\r\nhead = &q->new_flows;\r\nif (!head->first) {\r\nhead = &q->old_flows;\r\nif (!head->first) {\r\nif (q->time_next_delayed_flow != ~0ULL)\r\nqdisc_watchdog_schedule_ns(&q->watchdog,\r\nq->time_next_delayed_flow,\r\nfalse);\r\nreturn NULL;\r\n}\r\n}\r\nf = head->first;\r\nif (f->credit <= 0) {\r\nf->credit += q->quantum;\r\nhead->first = f->next;\r\nfq_flow_add_tail(&q->old_flows, f);\r\ngoto begin;\r\n}\r\nskb = f->head;\r\nif (unlikely(skb && now < f->time_next_packet &&\r\n!skb_is_tcp_pure_ack(skb))) {\r\nhead->first = f->next;\r\nfq_flow_set_throttled(q, f);\r\ngoto begin;\r\n}\r\nskb = fq_dequeue_head(sch, f);\r\nif (!skb) {\r\nhead->first = f->next;\r\nif ((head == &q->new_flows) && q->old_flows.first) {\r\nfq_flow_add_tail(&q->old_flows, f);\r\n} else {\r\nfq_flow_set_detached(f);\r\nq->inactive_flows++;\r\n}\r\ngoto begin;\r\n}\r\nprefetch(&skb->end);\r\nf->credit -= qdisc_pkt_len(skb);\r\nif (f->credit > 0 || !q->rate_enable)\r\ngoto out;\r\nif (skb_is_tcp_pure_ack(skb))\r\ngoto out;\r\nrate = q->flow_max_rate;\r\nif (skb->sk)\r\nrate = min(skb->sk->sk_pacing_rate, rate);\r\nif (rate != ~0U) {\r\nu32 plen = max(qdisc_pkt_len(skb), q->quantum);\r\nu64 len = (u64)plen * NSEC_PER_SEC;\r\nif (likely(rate))\r\ndo_div(len, rate);\r\nif (unlikely(len > NSEC_PER_SEC)) {\r\nlen = NSEC_PER_SEC;\r\nq->stat_pkts_too_long++;\r\n}\r\nf->time_next_packet = now + len;\r\n}\r\nout:\r\nqdisc_bstats_update(sch, skb);\r\nreturn skb;\r\n}\r\nstatic void fq_reset(struct Qdisc *sch)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nstruct rb_root *root;\r\nstruct sk_buff *skb;\r\nstruct rb_node *p;\r\nstruct fq_flow *f;\r\nunsigned int idx;\r\nwhile ((skb = fq_dequeue_head(sch, &q->internal)) != NULL)\r\nkfree_skb(skb);\r\nif (!q->fq_root)\r\nreturn;\r\nfor (idx = 0; idx < (1U << q->fq_trees_log); idx++) {\r\nroot = &q->fq_root[idx];\r\nwhile ((p = rb_first(root)) != NULL) {\r\nf = container_of(p, struct fq_flow, fq_node);\r\nrb_erase(p, root);\r\nwhile ((skb = fq_dequeue_head(sch, f)) != NULL)\r\nkfree_skb(skb);\r\nkmem_cache_free(fq_flow_cachep, f);\r\n}\r\n}\r\nq->new_flows.first = NULL;\r\nq->old_flows.first = NULL;\r\nq->delayed = RB_ROOT;\r\nq->flows = 0;\r\nq->inactive_flows = 0;\r\nq->throttled_flows = 0;\r\n}\r\nstatic void fq_rehash(struct fq_sched_data *q,\r\nstruct rb_root *old_array, u32 old_log,\r\nstruct rb_root *new_array, u32 new_log)\r\n{\r\nstruct rb_node *op, **np, *parent;\r\nstruct rb_root *oroot, *nroot;\r\nstruct fq_flow *of, *nf;\r\nint fcnt = 0;\r\nu32 idx;\r\nfor (idx = 0; idx < (1U << old_log); idx++) {\r\noroot = &old_array[idx];\r\nwhile ((op = rb_first(oroot)) != NULL) {\r\nrb_erase(op, oroot);\r\nof = container_of(op, struct fq_flow, fq_node);\r\nif (fq_gc_candidate(of)) {\r\nfcnt++;\r\nkmem_cache_free(fq_flow_cachep, of);\r\ncontinue;\r\n}\r\nnroot = &new_array[hash_32((u32)(long)of->sk, new_log)];\r\nnp = &nroot->rb_node;\r\nparent = NULL;\r\nwhile (*np) {\r\nparent = *np;\r\nnf = container_of(parent, struct fq_flow, fq_node);\r\nBUG_ON(nf->sk == of->sk);\r\nif (nf->sk > of->sk)\r\nnp = &parent->rb_right;\r\nelse\r\nnp = &parent->rb_left;\r\n}\r\nrb_link_node(&of->fq_node, parent, np);\r\nrb_insert_color(&of->fq_node, nroot);\r\n}\r\n}\r\nq->flows -= fcnt;\r\nq->inactive_flows -= fcnt;\r\nq->stat_gc_flows += fcnt;\r\n}\r\nstatic void *fq_alloc_node(size_t sz, int node)\r\n{\r\nvoid *ptr;\r\nptr = kmalloc_node(sz, GFP_KERNEL | __GFP_REPEAT | __GFP_NOWARN, node);\r\nif (!ptr)\r\nptr = vmalloc_node(sz, node);\r\nreturn ptr;\r\n}\r\nstatic void fq_free(void *addr)\r\n{\r\nkvfree(addr);\r\n}\r\nstatic int fq_resize(struct Qdisc *sch, u32 log)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nstruct rb_root *array;\r\nvoid *old_fq_root;\r\nu32 idx;\r\nif (q->fq_root && log == q->fq_trees_log)\r\nreturn 0;\r\narray = fq_alloc_node(sizeof(struct rb_root) << log,\r\nnetdev_queue_numa_node_read(sch->dev_queue));\r\nif (!array)\r\nreturn -ENOMEM;\r\nfor (idx = 0; idx < (1U << log); idx++)\r\narray[idx] = RB_ROOT;\r\nsch_tree_lock(sch);\r\nold_fq_root = q->fq_root;\r\nif (old_fq_root)\r\nfq_rehash(q, old_fq_root, q->fq_trees_log, array, log);\r\nq->fq_root = array;\r\nq->fq_trees_log = log;\r\nsch_tree_unlock(sch);\r\nfq_free(old_fq_root);\r\nreturn 0;\r\n}\r\nstatic int fq_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_FQ_MAX + 1];\r\nint err, drop_count = 0;\r\nunsigned drop_len = 0;\r\nu32 fq_log;\r\nif (!opt)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_FQ_MAX, opt, fq_policy);\r\nif (err < 0)\r\nreturn err;\r\nsch_tree_lock(sch);\r\nfq_log = q->fq_trees_log;\r\nif (tb[TCA_FQ_BUCKETS_LOG]) {\r\nu32 nval = nla_get_u32(tb[TCA_FQ_BUCKETS_LOG]);\r\nif (nval >= 1 && nval <= ilog2(256*1024))\r\nfq_log = nval;\r\nelse\r\nerr = -EINVAL;\r\n}\r\nif (tb[TCA_FQ_PLIMIT])\r\nsch->limit = nla_get_u32(tb[TCA_FQ_PLIMIT]);\r\nif (tb[TCA_FQ_FLOW_PLIMIT])\r\nq->flow_plimit = nla_get_u32(tb[TCA_FQ_FLOW_PLIMIT]);\r\nif (tb[TCA_FQ_QUANTUM]) {\r\nu32 quantum = nla_get_u32(tb[TCA_FQ_QUANTUM]);\r\nif (quantum > 0)\r\nq->quantum = quantum;\r\nelse\r\nerr = -EINVAL;\r\n}\r\nif (tb[TCA_FQ_INITIAL_QUANTUM])\r\nq->initial_quantum = nla_get_u32(tb[TCA_FQ_INITIAL_QUANTUM]);\r\nif (tb[TCA_FQ_FLOW_DEFAULT_RATE])\r\npr_warn_ratelimited("sch_fq: defrate %u ignored.\n",\r\nnla_get_u32(tb[TCA_FQ_FLOW_DEFAULT_RATE]));\r\nif (tb[TCA_FQ_FLOW_MAX_RATE])\r\nq->flow_max_rate = nla_get_u32(tb[TCA_FQ_FLOW_MAX_RATE]);\r\nif (tb[TCA_FQ_RATE_ENABLE]) {\r\nu32 enable = nla_get_u32(tb[TCA_FQ_RATE_ENABLE]);\r\nif (enable <= 1)\r\nq->rate_enable = enable;\r\nelse\r\nerr = -EINVAL;\r\n}\r\nif (tb[TCA_FQ_FLOW_REFILL_DELAY]) {\r\nu32 usecs_delay = nla_get_u32(tb[TCA_FQ_FLOW_REFILL_DELAY]) ;\r\nq->flow_refill_delay = usecs_to_jiffies(usecs_delay);\r\n}\r\nif (tb[TCA_FQ_ORPHAN_MASK])\r\nq->orphan_mask = nla_get_u32(tb[TCA_FQ_ORPHAN_MASK]);\r\nif (!err) {\r\nsch_tree_unlock(sch);\r\nerr = fq_resize(sch, fq_log);\r\nsch_tree_lock(sch);\r\n}\r\nwhile (sch->q.qlen > sch->limit) {\r\nstruct sk_buff *skb = fq_dequeue(sch);\r\nif (!skb)\r\nbreak;\r\ndrop_len += qdisc_pkt_len(skb);\r\nkfree_skb(skb);\r\ndrop_count++;\r\n}\r\nqdisc_tree_reduce_backlog(sch, drop_count, drop_len);\r\nsch_tree_unlock(sch);\r\nreturn err;\r\n}\r\nstatic void fq_destroy(struct Qdisc *sch)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nfq_reset(sch);\r\nfq_free(q->fq_root);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\n}\r\nstatic int fq_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nint err;\r\nsch->limit = 10000;\r\nq->flow_plimit = 100;\r\nq->quantum = 2 * psched_mtu(qdisc_dev(sch));\r\nq->initial_quantum = 10 * psched_mtu(qdisc_dev(sch));\r\nq->flow_refill_delay = msecs_to_jiffies(40);\r\nq->flow_max_rate = ~0U;\r\nq->rate_enable = 1;\r\nq->new_flows.first = NULL;\r\nq->old_flows.first = NULL;\r\nq->delayed = RB_ROOT;\r\nq->fq_root = NULL;\r\nq->fq_trees_log = ilog2(1024);\r\nq->orphan_mask = 1024 - 1;\r\nqdisc_watchdog_init(&q->watchdog, sch);\r\nif (opt)\r\nerr = fq_change(sch, opt);\r\nelse\r\nerr = fq_resize(sch, q->fq_trees_log);\r\nreturn err;\r\n}\r\nstatic int fq_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *opts;\r\nopts = nla_nest_start(skb, TCA_OPTIONS);\r\nif (opts == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put_u32(skb, TCA_FQ_PLIMIT, sch->limit) ||\r\nnla_put_u32(skb, TCA_FQ_FLOW_PLIMIT, q->flow_plimit) ||\r\nnla_put_u32(skb, TCA_FQ_QUANTUM, q->quantum) ||\r\nnla_put_u32(skb, TCA_FQ_INITIAL_QUANTUM, q->initial_quantum) ||\r\nnla_put_u32(skb, TCA_FQ_RATE_ENABLE, q->rate_enable) ||\r\nnla_put_u32(skb, TCA_FQ_FLOW_MAX_RATE, q->flow_max_rate) ||\r\nnla_put_u32(skb, TCA_FQ_FLOW_REFILL_DELAY,\r\njiffies_to_usecs(q->flow_refill_delay)) ||\r\nnla_put_u32(skb, TCA_FQ_ORPHAN_MASK, q->orphan_mask) ||\r\nnla_put_u32(skb, TCA_FQ_BUCKETS_LOG, q->fq_trees_log))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, opts);\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nstatic int fq_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\r\n{\r\nstruct fq_sched_data *q = qdisc_priv(sch);\r\nu64 now = ktime_get_ns();\r\nstruct tc_fq_qd_stats st = {\r\n.gc_flows = q->stat_gc_flows,\r\n.highprio_packets = q->stat_internal_packets,\r\n.tcp_retrans = q->stat_tcp_retrans,\r\n.throttled = q->stat_throttled,\r\n.flows_plimit = q->stat_flows_plimit,\r\n.pkts_too_long = q->stat_pkts_too_long,\r\n.allocation_errors = q->stat_allocation_errors,\r\n.flows = q->flows,\r\n.inactive_flows = q->inactive_flows,\r\n.throttled_flows = q->throttled_flows,\r\n.time_next_delayed_flow = q->time_next_delayed_flow - now,\r\n};\r\nreturn gnet_stats_copy_app(d, &st, sizeof(st));\r\n}\r\nstatic int __init fq_module_init(void)\r\n{\r\nint ret;\r\nfq_flow_cachep = kmem_cache_create("fq_flow_cache",\r\nsizeof(struct fq_flow),\r\n0, 0, NULL);\r\nif (!fq_flow_cachep)\r\nreturn -ENOMEM;\r\nret = register_qdisc(&fq_qdisc_ops);\r\nif (ret)\r\nkmem_cache_destroy(fq_flow_cachep);\r\nreturn ret;\r\n}\r\nstatic void __exit fq_module_exit(void)\r\n{\r\nunregister_qdisc(&fq_qdisc_ops);\r\nkmem_cache_destroy(fq_flow_cachep);\r\n}
