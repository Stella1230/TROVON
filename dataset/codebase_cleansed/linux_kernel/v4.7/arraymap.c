static void bpf_array_free_percpu(struct bpf_array *array)\r\n{\r\nint i;\r\nfor (i = 0; i < array->map.max_entries; i++)\r\nfree_percpu(array->pptrs[i]);\r\n}\r\nstatic int bpf_array_alloc_percpu(struct bpf_array *array)\r\n{\r\nvoid __percpu *ptr;\r\nint i;\r\nfor (i = 0; i < array->map.max_entries; i++) {\r\nptr = __alloc_percpu_gfp(array->elem_size, 8,\r\nGFP_USER | __GFP_NOWARN);\r\nif (!ptr) {\r\nbpf_array_free_percpu(array);\r\nreturn -ENOMEM;\r\n}\r\narray->pptrs[i] = ptr;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct bpf_map *array_map_alloc(union bpf_attr *attr)\r\n{\r\nbool percpu = attr->map_type == BPF_MAP_TYPE_PERCPU_ARRAY;\r\nstruct bpf_array *array;\r\nu64 array_size;\r\nu32 elem_size;\r\nif (attr->max_entries == 0 || attr->key_size != 4 ||\r\nattr->value_size == 0 || attr->map_flags)\r\nreturn ERR_PTR(-EINVAL);\r\nif (attr->value_size >= 1 << (KMALLOC_SHIFT_MAX - 1))\r\nreturn ERR_PTR(-E2BIG);\r\nelem_size = round_up(attr->value_size, 8);\r\narray_size = sizeof(*array);\r\nif (percpu)\r\narray_size += (u64) attr->max_entries * sizeof(void *);\r\nelse\r\narray_size += (u64) attr->max_entries * elem_size;\r\nif (array_size >= U32_MAX - PAGE_SIZE)\r\nreturn ERR_PTR(-ENOMEM);\r\narray = kzalloc(array_size, GFP_USER | __GFP_NOWARN);\r\nif (!array) {\r\narray = vzalloc(array_size);\r\nif (!array)\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\narray->map.map_type = attr->map_type;\r\narray->map.key_size = attr->key_size;\r\narray->map.value_size = attr->value_size;\r\narray->map.max_entries = attr->max_entries;\r\narray->elem_size = elem_size;\r\nif (!percpu)\r\ngoto out;\r\narray_size += (u64) attr->max_entries * elem_size * num_possible_cpus();\r\nif (array_size >= U32_MAX - PAGE_SIZE ||\r\nelem_size > PCPU_MIN_UNIT_SIZE || bpf_array_alloc_percpu(array)) {\r\nkvfree(array);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nout:\r\narray->map.pages = round_up(array_size, PAGE_SIZE) >> PAGE_SHIFT;\r\nreturn &array->map;\r\n}\r\nstatic void *array_map_lookup_elem(struct bpf_map *map, void *key)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nu32 index = *(u32 *)key;\r\nif (unlikely(index >= array->map.max_entries))\r\nreturn NULL;\r\nreturn array->value + array->elem_size * index;\r\n}\r\nstatic void *percpu_array_map_lookup_elem(struct bpf_map *map, void *key)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nu32 index = *(u32 *)key;\r\nif (unlikely(index >= array->map.max_entries))\r\nreturn NULL;\r\nreturn this_cpu_ptr(array->pptrs[index]);\r\n}\r\nint bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nu32 index = *(u32 *)key;\r\nvoid __percpu *pptr;\r\nint cpu, off = 0;\r\nu32 size;\r\nif (unlikely(index >= array->map.max_entries))\r\nreturn -ENOENT;\r\nsize = round_up(map->value_size, 8);\r\nrcu_read_lock();\r\npptr = array->pptrs[index];\r\nfor_each_possible_cpu(cpu) {\r\nbpf_long_memcpy(value + off, per_cpu_ptr(pptr, cpu), size);\r\noff += size;\r\n}\r\nrcu_read_unlock();\r\nreturn 0;\r\n}\r\nstatic int array_map_get_next_key(struct bpf_map *map, void *key, void *next_key)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nu32 index = *(u32 *)key;\r\nu32 *next = (u32 *)next_key;\r\nif (index >= array->map.max_entries) {\r\n*next = 0;\r\nreturn 0;\r\n}\r\nif (index == array->map.max_entries - 1)\r\nreturn -ENOENT;\r\n*next = index + 1;\r\nreturn 0;\r\n}\r\nstatic int array_map_update_elem(struct bpf_map *map, void *key, void *value,\r\nu64 map_flags)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nu32 index = *(u32 *)key;\r\nif (unlikely(map_flags > BPF_EXIST))\r\nreturn -EINVAL;\r\nif (unlikely(index >= array->map.max_entries))\r\nreturn -E2BIG;\r\nif (unlikely(map_flags == BPF_NOEXIST))\r\nreturn -EEXIST;\r\nif (array->map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY)\r\nmemcpy(this_cpu_ptr(array->pptrs[index]),\r\nvalue, map->value_size);\r\nelse\r\nmemcpy(array->value + array->elem_size * index,\r\nvalue, map->value_size);\r\nreturn 0;\r\n}\r\nint bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,\r\nu64 map_flags)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nu32 index = *(u32 *)key;\r\nvoid __percpu *pptr;\r\nint cpu, off = 0;\r\nu32 size;\r\nif (unlikely(map_flags > BPF_EXIST))\r\nreturn -EINVAL;\r\nif (unlikely(index >= array->map.max_entries))\r\nreturn -E2BIG;\r\nif (unlikely(map_flags == BPF_NOEXIST))\r\nreturn -EEXIST;\r\nsize = round_up(map->value_size, 8);\r\nrcu_read_lock();\r\npptr = array->pptrs[index];\r\nfor_each_possible_cpu(cpu) {\r\nbpf_long_memcpy(per_cpu_ptr(pptr, cpu), value + off, size);\r\noff += size;\r\n}\r\nrcu_read_unlock();\r\nreturn 0;\r\n}\r\nstatic int array_map_delete_elem(struct bpf_map *map, void *key)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic void array_map_free(struct bpf_map *map)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nsynchronize_rcu();\r\nif (array->map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY)\r\nbpf_array_free_percpu(array);\r\nkvfree(array);\r\n}\r\nstatic int __init register_array_map(void)\r\n{\r\nbpf_register_map_type(&array_type);\r\nbpf_register_map_type(&percpu_array_type);\r\nreturn 0;\r\n}\r\nstatic struct bpf_map *fd_array_map_alloc(union bpf_attr *attr)\r\n{\r\nif (attr->value_size != sizeof(u32))\r\nreturn ERR_PTR(-EINVAL);\r\nreturn array_map_alloc(attr);\r\n}\r\nstatic void fd_array_map_free(struct bpf_map *map)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nint i;\r\nsynchronize_rcu();\r\nfor (i = 0; i < array->map.max_entries; i++)\r\nBUG_ON(array->ptrs[i] != NULL);\r\nkvfree(array);\r\n}\r\nstatic void *fd_array_map_lookup_elem(struct bpf_map *map, void *key)\r\n{\r\nreturn NULL;\r\n}\r\nstatic int fd_array_map_update_elem(struct bpf_map *map, void *key,\r\nvoid *value, u64 map_flags)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nvoid *new_ptr, *old_ptr;\r\nu32 index = *(u32 *)key, ufd;\r\nif (map_flags != BPF_ANY)\r\nreturn -EINVAL;\r\nif (index >= array->map.max_entries)\r\nreturn -E2BIG;\r\nufd = *(u32 *)value;\r\nnew_ptr = map->ops->map_fd_get_ptr(map, ufd);\r\nif (IS_ERR(new_ptr))\r\nreturn PTR_ERR(new_ptr);\r\nold_ptr = xchg(array->ptrs + index, new_ptr);\r\nif (old_ptr)\r\nmap->ops->map_fd_put_ptr(old_ptr);\r\nreturn 0;\r\n}\r\nstatic int fd_array_map_delete_elem(struct bpf_map *map, void *key)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nvoid *old_ptr;\r\nu32 index = *(u32 *)key;\r\nif (index >= array->map.max_entries)\r\nreturn -E2BIG;\r\nold_ptr = xchg(array->ptrs + index, NULL);\r\nif (old_ptr) {\r\nmap->ops->map_fd_put_ptr(old_ptr);\r\nreturn 0;\r\n} else {\r\nreturn -ENOENT;\r\n}\r\n}\r\nstatic void *prog_fd_array_get_ptr(struct bpf_map *map, int fd)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nstruct bpf_prog *prog = bpf_prog_get(fd);\r\nif (IS_ERR(prog))\r\nreturn prog;\r\nif (!bpf_prog_array_compatible(array, prog)) {\r\nbpf_prog_put(prog);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nreturn prog;\r\n}\r\nstatic void prog_fd_array_put_ptr(void *ptr)\r\n{\r\nstruct bpf_prog *prog = ptr;\r\nbpf_prog_put_rcu(prog);\r\n}\r\nvoid bpf_fd_array_map_clear(struct bpf_map *map)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nint i;\r\nfor (i = 0; i < array->map.max_entries; i++)\r\nfd_array_map_delete_elem(map, &i);\r\n}\r\nstatic int __init register_prog_array_map(void)\r\n{\r\nbpf_register_map_type(&prog_array_type);\r\nreturn 0;\r\n}\r\nstatic void perf_event_array_map_free(struct bpf_map *map)\r\n{\r\nbpf_fd_array_map_clear(map);\r\nfd_array_map_free(map);\r\n}\r\nstatic void *perf_event_fd_array_get_ptr(struct bpf_map *map, int fd)\r\n{\r\nstruct perf_event *event;\r\nconst struct perf_event_attr *attr;\r\nstruct file *file;\r\nfile = perf_event_get(fd);\r\nif (IS_ERR(file))\r\nreturn file;\r\nevent = file->private_data;\r\nattr = perf_event_attrs(event);\r\nif (IS_ERR(attr))\r\ngoto err;\r\nif (attr->inherit)\r\ngoto err;\r\nif (attr->type == PERF_TYPE_RAW)\r\nreturn file;\r\nif (attr->type == PERF_TYPE_HARDWARE)\r\nreturn file;\r\nif (attr->type == PERF_TYPE_SOFTWARE &&\r\nattr->config == PERF_COUNT_SW_BPF_OUTPUT)\r\nreturn file;\r\nerr:\r\nfput(file);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic void perf_event_fd_array_put_ptr(void *ptr)\r\n{\r\nfput((struct file *)ptr);\r\n}\r\nstatic int __init register_perf_event_array_map(void)\r\n{\r\nbpf_register_map_type(&perf_event_array_type);\r\nreturn 0;\r\n}
