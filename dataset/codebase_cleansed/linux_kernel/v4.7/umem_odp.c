static void ib_umem_notifier_start_account(struct ib_umem *item)\r\n{\r\nmutex_lock(&item->odp_data->umem_mutex);\r\nif (item->odp_data->mn_counters_active) {\r\nint notifiers_count = item->odp_data->notifiers_count++;\r\nif (notifiers_count == 0)\r\nreinit_completion(&item->odp_data->notifier_completion);\r\n}\r\nmutex_unlock(&item->odp_data->umem_mutex);\r\n}\r\nstatic void ib_umem_notifier_end_account(struct ib_umem *item)\r\n{\r\nmutex_lock(&item->odp_data->umem_mutex);\r\nif (item->odp_data->mn_counters_active) {\r\n++item->odp_data->notifiers_seq;\r\nif (--item->odp_data->notifiers_count == 0)\r\ncomplete_all(&item->odp_data->notifier_completion);\r\n}\r\nmutex_unlock(&item->odp_data->umem_mutex);\r\n}\r\nstatic void ib_ucontext_notifier_start_account(struct ib_ucontext *context)\r\n{\r\natomic_inc(&context->notifier_count);\r\n}\r\nstatic void ib_ucontext_notifier_end_account(struct ib_ucontext *context)\r\n{\r\nint zero_notifiers = atomic_dec_and_test(&context->notifier_count);\r\nif (zero_notifiers &&\r\n!list_empty(&context->no_private_counters)) {\r\nstruct ib_umem_odp *odp_data, *next;\r\ndown_write(&context->umem_rwsem);\r\nif (!atomic_read(&context->notifier_count)) {\r\nlist_for_each_entry_safe(odp_data, next,\r\n&context->no_private_counters,\r\nno_private_counters) {\r\nmutex_lock(&odp_data->umem_mutex);\r\nodp_data->mn_counters_active = true;\r\nlist_del(&odp_data->no_private_counters);\r\ncomplete_all(&odp_data->notifier_completion);\r\nmutex_unlock(&odp_data->umem_mutex);\r\n}\r\n}\r\nup_write(&context->umem_rwsem);\r\n}\r\n}\r\nstatic int ib_umem_notifier_release_trampoline(struct ib_umem *item, u64 start,\r\nu64 end, void *cookie) {\r\nib_umem_notifier_start_account(item);\r\nitem->odp_data->dying = 1;\r\nsmp_wmb();\r\ncomplete_all(&item->odp_data->notifier_completion);\r\nitem->context->invalidate_range(item, ib_umem_start(item),\r\nib_umem_end(item));\r\nreturn 0;\r\n}\r\nstatic void ib_umem_notifier_release(struct mmu_notifier *mn,\r\nstruct mm_struct *mm)\r\n{\r\nstruct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);\r\nif (!context->invalidate_range)\r\nreturn;\r\nib_ucontext_notifier_start_account(context);\r\ndown_read(&context->umem_rwsem);\r\nrbt_ib_umem_for_each_in_range(&context->umem_tree, 0,\r\nULLONG_MAX,\r\nib_umem_notifier_release_trampoline,\r\nNULL);\r\nup_read(&context->umem_rwsem);\r\n}\r\nstatic int invalidate_page_trampoline(struct ib_umem *item, u64 start,\r\nu64 end, void *cookie)\r\n{\r\nib_umem_notifier_start_account(item);\r\nitem->context->invalidate_range(item, start, start + PAGE_SIZE);\r\nib_umem_notifier_end_account(item);\r\nreturn 0;\r\n}\r\nstatic void ib_umem_notifier_invalidate_page(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long address)\r\n{\r\nstruct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);\r\nif (!context->invalidate_range)\r\nreturn;\r\nib_ucontext_notifier_start_account(context);\r\ndown_read(&context->umem_rwsem);\r\nrbt_ib_umem_for_each_in_range(&context->umem_tree, address,\r\naddress + PAGE_SIZE,\r\ninvalidate_page_trampoline, NULL);\r\nup_read(&context->umem_rwsem);\r\nib_ucontext_notifier_end_account(context);\r\n}\r\nstatic int invalidate_range_start_trampoline(struct ib_umem *item, u64 start,\r\nu64 end, void *cookie)\r\n{\r\nib_umem_notifier_start_account(item);\r\nitem->context->invalidate_range(item, start, end);\r\nreturn 0;\r\n}\r\nstatic void ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nstruct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);\r\nif (!context->invalidate_range)\r\nreturn;\r\nib_ucontext_notifier_start_account(context);\r\ndown_read(&context->umem_rwsem);\r\nrbt_ib_umem_for_each_in_range(&context->umem_tree, start,\r\nend,\r\ninvalidate_range_start_trampoline, NULL);\r\nup_read(&context->umem_rwsem);\r\n}\r\nstatic int invalidate_range_end_trampoline(struct ib_umem *item, u64 start,\r\nu64 end, void *cookie)\r\n{\r\nib_umem_notifier_end_account(item);\r\nreturn 0;\r\n}\r\nstatic void ib_umem_notifier_invalidate_range_end(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nstruct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);\r\nif (!context->invalidate_range)\r\nreturn;\r\ndown_read(&context->umem_rwsem);\r\nrbt_ib_umem_for_each_in_range(&context->umem_tree, start,\r\nend,\r\ninvalidate_range_end_trampoline, NULL);\r\nup_read(&context->umem_rwsem);\r\nib_ucontext_notifier_end_account(context);\r\n}\r\nint ib_umem_odp_get(struct ib_ucontext *context, struct ib_umem *umem)\r\n{\r\nint ret_val;\r\nstruct pid *our_pid;\r\nstruct mm_struct *mm = get_task_mm(current);\r\nif (!mm)\r\nreturn -EINVAL;\r\nrcu_read_lock();\r\nour_pid = get_task_pid(current->group_leader, PIDTYPE_PID);\r\nrcu_read_unlock();\r\nput_pid(our_pid);\r\nif (context->tgid != our_pid) {\r\nret_val = -EINVAL;\r\ngoto out_mm;\r\n}\r\numem->hugetlb = 0;\r\numem->odp_data = kzalloc(sizeof(*umem->odp_data), GFP_KERNEL);\r\nif (!umem->odp_data) {\r\nret_val = -ENOMEM;\r\ngoto out_mm;\r\n}\r\numem->odp_data->umem = umem;\r\nmutex_init(&umem->odp_data->umem_mutex);\r\ninit_completion(&umem->odp_data->notifier_completion);\r\numem->odp_data->page_list = vzalloc(ib_umem_num_pages(umem) *\r\nsizeof(*umem->odp_data->page_list));\r\nif (!umem->odp_data->page_list) {\r\nret_val = -ENOMEM;\r\ngoto out_odp_data;\r\n}\r\numem->odp_data->dma_list = vzalloc(ib_umem_num_pages(umem) *\r\nsizeof(*umem->odp_data->dma_list));\r\nif (!umem->odp_data->dma_list) {\r\nret_val = -ENOMEM;\r\ngoto out_page_list;\r\n}\r\ndown_write(&context->umem_rwsem);\r\ncontext->odp_mrs_count++;\r\nif (likely(ib_umem_start(umem) != ib_umem_end(umem)))\r\nrbt_ib_umem_insert(&umem->odp_data->interval_tree,\r\n&context->umem_tree);\r\nif (likely(!atomic_read(&context->notifier_count)) ||\r\ncontext->odp_mrs_count == 1)\r\numem->odp_data->mn_counters_active = true;\r\nelse\r\nlist_add(&umem->odp_data->no_private_counters,\r\n&context->no_private_counters);\r\ndowngrade_write(&context->umem_rwsem);\r\nif (context->odp_mrs_count == 1) {\r\natomic_set(&context->notifier_count, 0);\r\nINIT_HLIST_NODE(&context->mn.hlist);\r\ncontext->mn.ops = &ib_umem_notifiers;\r\nlockdep_off();\r\nret_val = mmu_notifier_register(&context->mn, mm);\r\nlockdep_on();\r\nif (ret_val) {\r\npr_err("Failed to register mmu_notifier %d\n", ret_val);\r\nret_val = -EBUSY;\r\ngoto out_mutex;\r\n}\r\n}\r\nup_read(&context->umem_rwsem);\r\nmmput(mm);\r\nreturn 0;\r\nout_mutex:\r\nup_read(&context->umem_rwsem);\r\nvfree(umem->odp_data->dma_list);\r\nout_page_list:\r\nvfree(umem->odp_data->page_list);\r\nout_odp_data:\r\nkfree(umem->odp_data);\r\nout_mm:\r\nmmput(mm);\r\nreturn ret_val;\r\n}\r\nvoid ib_umem_odp_release(struct ib_umem *umem)\r\n{\r\nstruct ib_ucontext *context = umem->context;\r\nib_umem_odp_unmap_dma_pages(umem, ib_umem_start(umem),\r\nib_umem_end(umem));\r\ndown_write(&context->umem_rwsem);\r\nif (likely(ib_umem_start(umem) != ib_umem_end(umem)))\r\nrbt_ib_umem_remove(&umem->odp_data->interval_tree,\r\n&context->umem_tree);\r\ncontext->odp_mrs_count--;\r\nif (!umem->odp_data->mn_counters_active) {\r\nlist_del(&umem->odp_data->no_private_counters);\r\ncomplete_all(&umem->odp_data->notifier_completion);\r\n}\r\ndowngrade_write(&context->umem_rwsem);\r\nif (!context->odp_mrs_count) {\r\nstruct task_struct *owning_process = NULL;\r\nstruct mm_struct *owning_mm = NULL;\r\nowning_process = get_pid_task(context->tgid,\r\nPIDTYPE_PID);\r\nif (owning_process == NULL)\r\ngoto out;\r\nowning_mm = get_task_mm(owning_process);\r\nif (owning_mm == NULL)\r\ngoto out_put_task;\r\nmmu_notifier_unregister(&context->mn, owning_mm);\r\nmmput(owning_mm);\r\nout_put_task:\r\nput_task_struct(owning_process);\r\n}\r\nout:\r\nup_read(&context->umem_rwsem);\r\nvfree(umem->odp_data->dma_list);\r\nvfree(umem->odp_data->page_list);\r\nkfree(umem->odp_data);\r\nkfree(umem);\r\n}\r\nstatic int ib_umem_odp_map_dma_single_page(\r\nstruct ib_umem *umem,\r\nint page_index,\r\nu64 base_virt_addr,\r\nstruct page *page,\r\nu64 access_mask,\r\nunsigned long current_seq)\r\n{\r\nstruct ib_device *dev = umem->context->device;\r\ndma_addr_t dma_addr;\r\nint stored_page = 0;\r\nint remove_existing_mapping = 0;\r\nint ret = 0;\r\nif (ib_umem_mmu_notifier_retry(umem, current_seq)) {\r\nret = -EAGAIN;\r\ngoto out;\r\n}\r\nif (!(umem->odp_data->dma_list[page_index])) {\r\ndma_addr = ib_dma_map_page(dev,\r\npage,\r\n0, PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nif (ib_dma_mapping_error(dev, dma_addr)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\numem->odp_data->dma_list[page_index] = dma_addr | access_mask;\r\numem->odp_data->page_list[page_index] = page;\r\nstored_page = 1;\r\n} else if (umem->odp_data->page_list[page_index] == page) {\r\numem->odp_data->dma_list[page_index] |= access_mask;\r\n} else {\r\npr_err("error: got different pages in IB device and from get_user_pages. IB device page: %p, gup page: %p\n",\r\numem->odp_data->page_list[page_index], page);\r\nremove_existing_mapping = 1;\r\n}\r\nout:\r\nif (umem->context->invalidate_range || !stored_page)\r\nput_page(page);\r\nif (remove_existing_mapping && umem->context->invalidate_range) {\r\ninvalidate_page_trampoline(\r\numem,\r\nbase_virt_addr + (page_index * PAGE_SIZE),\r\nbase_virt_addr + ((page_index+1)*PAGE_SIZE),\r\nNULL);\r\nret = -EAGAIN;\r\n}\r\nreturn ret;\r\n}\r\nint ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,\r\nu64 access_mask, unsigned long current_seq)\r\n{\r\nstruct task_struct *owning_process = NULL;\r\nstruct mm_struct *owning_mm = NULL;\r\nstruct page **local_page_list = NULL;\r\nu64 off;\r\nint j, k, ret = 0, start_idx, npages = 0;\r\nu64 base_virt_addr;\r\nif (access_mask == 0)\r\nreturn -EINVAL;\r\nif (user_virt < ib_umem_start(umem) ||\r\nuser_virt + bcnt > ib_umem_end(umem))\r\nreturn -EFAULT;\r\nlocal_page_list = (struct page **)__get_free_page(GFP_KERNEL);\r\nif (!local_page_list)\r\nreturn -ENOMEM;\r\noff = user_virt & (~PAGE_MASK);\r\nuser_virt = user_virt & PAGE_MASK;\r\nbase_virt_addr = user_virt;\r\nbcnt += off;\r\nowning_process = get_pid_task(umem->context->tgid, PIDTYPE_PID);\r\nif (owning_process == NULL) {\r\nret = -EINVAL;\r\ngoto out_no_task;\r\n}\r\nowning_mm = get_task_mm(owning_process);\r\nif (owning_mm == NULL) {\r\nret = -EINVAL;\r\ngoto out_put_task;\r\n}\r\nstart_idx = (user_virt - ib_umem_start(umem)) >> PAGE_SHIFT;\r\nk = start_idx;\r\nwhile (bcnt > 0) {\r\nconst size_t gup_num_pages =\r\nmin_t(size_t, ALIGN(bcnt, PAGE_SIZE) / PAGE_SIZE,\r\nPAGE_SIZE / sizeof(struct page *));\r\ndown_read(&owning_mm->mmap_sem);\r\nnpages = get_user_pages_remote(owning_process, owning_mm,\r\nuser_virt, gup_num_pages,\r\naccess_mask & ODP_WRITE_ALLOWED_BIT,\r\n0, local_page_list, NULL);\r\nup_read(&owning_mm->mmap_sem);\r\nif (npages < 0)\r\nbreak;\r\nbcnt -= min_t(size_t, npages << PAGE_SHIFT, bcnt);\r\nuser_virt += npages << PAGE_SHIFT;\r\nmutex_lock(&umem->odp_data->umem_mutex);\r\nfor (j = 0; j < npages; ++j) {\r\nret = ib_umem_odp_map_dma_single_page(\r\numem, k, base_virt_addr, local_page_list[j],\r\naccess_mask, current_seq);\r\nif (ret < 0)\r\nbreak;\r\nk++;\r\n}\r\nmutex_unlock(&umem->odp_data->umem_mutex);\r\nif (ret < 0) {\r\nfor (++j; j < npages; ++j)\r\nput_page(local_page_list[j]);\r\nbreak;\r\n}\r\n}\r\nif (ret >= 0) {\r\nif (npages < 0 && k == start_idx)\r\nret = npages;\r\nelse\r\nret = k - start_idx;\r\n}\r\nmmput(owning_mm);\r\nout_put_task:\r\nput_task_struct(owning_process);\r\nout_no_task:\r\nfree_page((unsigned long)local_page_list);\r\nreturn ret;\r\n}\r\nvoid ib_umem_odp_unmap_dma_pages(struct ib_umem *umem, u64 virt,\r\nu64 bound)\r\n{\r\nint idx;\r\nu64 addr;\r\nstruct ib_device *dev = umem->context->device;\r\nvirt = max_t(u64, virt, ib_umem_start(umem));\r\nbound = min_t(u64, bound, ib_umem_end(umem));\r\nmutex_lock(&umem->odp_data->umem_mutex);\r\nfor (addr = virt; addr < bound; addr += (u64)umem->page_size) {\r\nidx = (addr - ib_umem_start(umem)) / PAGE_SIZE;\r\nif (umem->odp_data->page_list[idx]) {\r\nstruct page *page = umem->odp_data->page_list[idx];\r\ndma_addr_t dma = umem->odp_data->dma_list[idx];\r\ndma_addr_t dma_addr = dma & ODP_DMA_ADDR_MASK;\r\nWARN_ON(!dma_addr);\r\nib_dma_unmap_page(dev, dma_addr, PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nif (dma & ODP_WRITE_ALLOWED_BIT) {\r\nstruct page *head_page = compound_head(page);\r\nset_page_dirty(head_page);\r\n}\r\nif (!umem->context->invalidate_range)\r\nput_page(page);\r\numem->odp_data->page_list[idx] = NULL;\r\numem->odp_data->dma_list[idx] = 0;\r\n}\r\n}\r\nmutex_unlock(&umem->odp_data->umem_mutex);\r\n}
