void fscrypt_release_ctx(struct fscrypt_ctx *ctx)\r\n{\r\nunsigned long flags;\r\nif (ctx->flags & FS_WRITE_PATH_FL && ctx->w.bounce_page) {\r\nmempool_free(ctx->w.bounce_page, fscrypt_bounce_page_pool);\r\nctx->w.bounce_page = NULL;\r\n}\r\nctx->w.control_page = NULL;\r\nif (ctx->flags & FS_CTX_REQUIRES_FREE_ENCRYPT_FL) {\r\nkmem_cache_free(fscrypt_ctx_cachep, ctx);\r\n} else {\r\nspin_lock_irqsave(&fscrypt_ctx_lock, flags);\r\nlist_add(&ctx->free_list, &fscrypt_free_ctxs);\r\nspin_unlock_irqrestore(&fscrypt_ctx_lock, flags);\r\n}\r\n}\r\nstruct fscrypt_ctx *fscrypt_get_ctx(struct inode *inode, gfp_t gfp_flags)\r\n{\r\nstruct fscrypt_ctx *ctx = NULL;\r\nstruct fscrypt_info *ci = inode->i_crypt_info;\r\nunsigned long flags;\r\nif (ci == NULL)\r\nreturn ERR_PTR(-ENOKEY);\r\nspin_lock_irqsave(&fscrypt_ctx_lock, flags);\r\nctx = list_first_entry_or_null(&fscrypt_free_ctxs,\r\nstruct fscrypt_ctx, free_list);\r\nif (ctx)\r\nlist_del(&ctx->free_list);\r\nspin_unlock_irqrestore(&fscrypt_ctx_lock, flags);\r\nif (!ctx) {\r\nctx = kmem_cache_zalloc(fscrypt_ctx_cachep, gfp_flags);\r\nif (!ctx)\r\nreturn ERR_PTR(-ENOMEM);\r\nctx->flags |= FS_CTX_REQUIRES_FREE_ENCRYPT_FL;\r\n} else {\r\nctx->flags &= ~FS_CTX_REQUIRES_FREE_ENCRYPT_FL;\r\n}\r\nctx->flags &= ~FS_WRITE_PATH_FL;\r\nreturn ctx;\r\n}\r\nstatic void fscrypt_complete(struct crypto_async_request *req, int res)\r\n{\r\nstruct fscrypt_completion_result *ecr = req->data;\r\nif (res == -EINPROGRESS)\r\nreturn;\r\necr->res = res;\r\ncomplete(&ecr->completion);\r\n}\r\nstatic int do_page_crypto(struct inode *inode,\r\nfscrypt_direction_t rw, pgoff_t index,\r\nstruct page *src_page, struct page *dest_page,\r\ngfp_t gfp_flags)\r\n{\r\nu8 xts_tweak[FS_XTS_TWEAK_SIZE];\r\nstruct skcipher_request *req = NULL;\r\nDECLARE_FS_COMPLETION_RESULT(ecr);\r\nstruct scatterlist dst, src;\r\nstruct fscrypt_info *ci = inode->i_crypt_info;\r\nstruct crypto_skcipher *tfm = ci->ci_ctfm;\r\nint res = 0;\r\nreq = skcipher_request_alloc(tfm, gfp_flags);\r\nif (!req) {\r\nprintk_ratelimited(KERN_ERR\r\n"%s: crypto_request_alloc() failed\n",\r\n__func__);\r\nreturn -ENOMEM;\r\n}\r\nskcipher_request_set_callback(\r\nreq, CRYPTO_TFM_REQ_MAY_BACKLOG | CRYPTO_TFM_REQ_MAY_SLEEP,\r\nfscrypt_complete, &ecr);\r\nBUILD_BUG_ON(FS_XTS_TWEAK_SIZE < sizeof(index));\r\nmemcpy(xts_tweak, &index, sizeof(index));\r\nmemset(&xts_tweak[sizeof(index)], 0,\r\nFS_XTS_TWEAK_SIZE - sizeof(index));\r\nsg_init_table(&dst, 1);\r\nsg_set_page(&dst, dest_page, PAGE_SIZE, 0);\r\nsg_init_table(&src, 1);\r\nsg_set_page(&src, src_page, PAGE_SIZE, 0);\r\nskcipher_request_set_crypt(req, &src, &dst, PAGE_SIZE,\r\nxts_tweak);\r\nif (rw == FS_DECRYPT)\r\nres = crypto_skcipher_decrypt(req);\r\nelse\r\nres = crypto_skcipher_encrypt(req);\r\nif (res == -EINPROGRESS || res == -EBUSY) {\r\nBUG_ON(req->base.data != &ecr);\r\nwait_for_completion(&ecr.completion);\r\nres = ecr.res;\r\n}\r\nskcipher_request_free(req);\r\nif (res) {\r\nprintk_ratelimited(KERN_ERR\r\n"%s: crypto_skcipher_encrypt() returned %d\n",\r\n__func__, res);\r\nreturn res;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct page *alloc_bounce_page(struct fscrypt_ctx *ctx, gfp_t gfp_flags)\r\n{\r\nctx->w.bounce_page = mempool_alloc(fscrypt_bounce_page_pool, gfp_flags);\r\nif (ctx->w.bounce_page == NULL)\r\nreturn ERR_PTR(-ENOMEM);\r\nctx->flags |= FS_WRITE_PATH_FL;\r\nreturn ctx->w.bounce_page;\r\n}\r\nstruct page *fscrypt_encrypt_page(struct inode *inode,\r\nstruct page *plaintext_page, gfp_t gfp_flags)\r\n{\r\nstruct fscrypt_ctx *ctx;\r\nstruct page *ciphertext_page = NULL;\r\nint err;\r\nBUG_ON(!PageLocked(plaintext_page));\r\nctx = fscrypt_get_ctx(inode, gfp_flags);\r\nif (IS_ERR(ctx))\r\nreturn (struct page *)ctx;\r\nciphertext_page = alloc_bounce_page(ctx, gfp_flags);\r\nif (IS_ERR(ciphertext_page))\r\ngoto errout;\r\nctx->w.control_page = plaintext_page;\r\nerr = do_page_crypto(inode, FS_ENCRYPT, plaintext_page->index,\r\nplaintext_page, ciphertext_page,\r\ngfp_flags);\r\nif (err) {\r\nciphertext_page = ERR_PTR(err);\r\ngoto errout;\r\n}\r\nSetPagePrivate(ciphertext_page);\r\nset_page_private(ciphertext_page, (unsigned long)ctx);\r\nlock_page(ciphertext_page);\r\nreturn ciphertext_page;\r\nerrout:\r\nfscrypt_release_ctx(ctx);\r\nreturn ciphertext_page;\r\n}\r\nint fscrypt_decrypt_page(struct page *page)\r\n{\r\nBUG_ON(!PageLocked(page));\r\nreturn do_page_crypto(page->mapping->host,\r\nFS_DECRYPT, page->index, page, page, GFP_NOFS);\r\n}\r\nint fscrypt_zeroout_range(struct inode *inode, pgoff_t lblk,\r\nsector_t pblk, unsigned int len)\r\n{\r\nstruct fscrypt_ctx *ctx;\r\nstruct page *ciphertext_page = NULL;\r\nstruct bio *bio;\r\nint ret, err = 0;\r\nBUG_ON(inode->i_sb->s_blocksize != PAGE_SIZE);\r\nctx = fscrypt_get_ctx(inode, GFP_NOFS);\r\nif (IS_ERR(ctx))\r\nreturn PTR_ERR(ctx);\r\nciphertext_page = alloc_bounce_page(ctx, GFP_NOWAIT);\r\nif (IS_ERR(ciphertext_page)) {\r\nerr = PTR_ERR(ciphertext_page);\r\ngoto errout;\r\n}\r\nwhile (len--) {\r\nerr = do_page_crypto(inode, FS_ENCRYPT, lblk,\r\nZERO_PAGE(0), ciphertext_page,\r\nGFP_NOFS);\r\nif (err)\r\ngoto errout;\r\nbio = bio_alloc(GFP_NOWAIT, 1);\r\nif (!bio) {\r\nerr = -ENOMEM;\r\ngoto errout;\r\n}\r\nbio->bi_bdev = inode->i_sb->s_bdev;\r\nbio->bi_iter.bi_sector =\r\npblk << (inode->i_sb->s_blocksize_bits - 9);\r\nret = bio_add_page(bio, ciphertext_page,\r\ninode->i_sb->s_blocksize, 0);\r\nif (ret != inode->i_sb->s_blocksize) {\r\nWARN_ON(1);\r\nbio_put(bio);\r\nerr = -EIO;\r\ngoto errout;\r\n}\r\nerr = submit_bio_wait(WRITE, bio);\r\nif ((err == 0) && bio->bi_error)\r\nerr = -EIO;\r\nbio_put(bio);\r\nif (err)\r\ngoto errout;\r\nlblk++;\r\npblk++;\r\n}\r\nerr = 0;\r\nerrout:\r\nfscrypt_release_ctx(ctx);\r\nreturn err;\r\n}\r\nstatic int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\r\n{\r\nstruct dentry *dir;\r\nstruct fscrypt_info *ci;\r\nint dir_has_key, cached_with_key;\r\nif (flags & LOOKUP_RCU)\r\nreturn -ECHILD;\r\ndir = dget_parent(dentry);\r\nif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\r\ndput(dir);\r\nreturn 0;\r\n}\r\nci = d_inode(dir)->i_crypt_info;\r\nif (ci && ci->ci_keyring_key &&\r\n(ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\r\n(1 << KEY_FLAG_REVOKED) |\r\n(1 << KEY_FLAG_DEAD))))\r\nci = NULL;\r\nspin_lock(&dentry->d_lock);\r\ncached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\r\nspin_unlock(&dentry->d_lock);\r\ndir_has_key = (ci != NULL);\r\ndput(dir);\r\nif ((!cached_with_key && d_is_negative(dentry)) ||\r\n(!cached_with_key && dir_has_key) ||\r\n(cached_with_key && !dir_has_key))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void completion_pages(struct work_struct *work)\r\n{\r\nstruct fscrypt_ctx *ctx =\r\ncontainer_of(work, struct fscrypt_ctx, r.work);\r\nstruct bio *bio = ctx->r.bio;\r\nstruct bio_vec *bv;\r\nint i;\r\nbio_for_each_segment_all(bv, bio, i) {\r\nstruct page *page = bv->bv_page;\r\nint ret = fscrypt_decrypt_page(page);\r\nif (ret) {\r\nWARN_ON_ONCE(1);\r\nSetPageError(page);\r\n} else {\r\nSetPageUptodate(page);\r\n}\r\nunlock_page(page);\r\n}\r\nfscrypt_release_ctx(ctx);\r\nbio_put(bio);\r\n}\r\nvoid fscrypt_decrypt_bio_pages(struct fscrypt_ctx *ctx, struct bio *bio)\r\n{\r\nINIT_WORK(&ctx->r.work, completion_pages);\r\nctx->r.bio = bio;\r\nqueue_work(fscrypt_read_workqueue, &ctx->r.work);\r\n}\r\nvoid fscrypt_pullback_bio_page(struct page **page, bool restore)\r\n{\r\nstruct fscrypt_ctx *ctx;\r\nstruct page *bounce_page;\r\nif ((*page)->mapping)\r\nreturn;\r\nbounce_page = *page;\r\nctx = (struct fscrypt_ctx *)page_private(bounce_page);\r\n*page = ctx->w.control_page;\r\nif (restore)\r\nfscrypt_restore_control_page(bounce_page);\r\n}\r\nvoid fscrypt_restore_control_page(struct page *page)\r\n{\r\nstruct fscrypt_ctx *ctx;\r\nctx = (struct fscrypt_ctx *)page_private(page);\r\nset_page_private(page, (unsigned long)NULL);\r\nClearPagePrivate(page);\r\nunlock_page(page);\r\nfscrypt_release_ctx(ctx);\r\n}\r\nstatic void fscrypt_destroy(void)\r\n{\r\nstruct fscrypt_ctx *pos, *n;\r\nlist_for_each_entry_safe(pos, n, &fscrypt_free_ctxs, free_list)\r\nkmem_cache_free(fscrypt_ctx_cachep, pos);\r\nINIT_LIST_HEAD(&fscrypt_free_ctxs);\r\nmempool_destroy(fscrypt_bounce_page_pool);\r\nfscrypt_bounce_page_pool = NULL;\r\n}\r\nint fscrypt_initialize(void)\r\n{\r\nint i, res = -ENOMEM;\r\nif (fscrypt_bounce_page_pool)\r\nreturn 0;\r\nmutex_lock(&fscrypt_init_mutex);\r\nif (fscrypt_bounce_page_pool)\r\ngoto already_initialized;\r\nfor (i = 0; i < num_prealloc_crypto_ctxs; i++) {\r\nstruct fscrypt_ctx *ctx;\r\nctx = kmem_cache_zalloc(fscrypt_ctx_cachep, GFP_NOFS);\r\nif (!ctx)\r\ngoto fail;\r\nlist_add(&ctx->free_list, &fscrypt_free_ctxs);\r\n}\r\nfscrypt_bounce_page_pool =\r\nmempool_create_page_pool(num_prealloc_crypto_pages, 0);\r\nif (!fscrypt_bounce_page_pool)\r\ngoto fail;\r\nalready_initialized:\r\nmutex_unlock(&fscrypt_init_mutex);\r\nreturn 0;\r\nfail:\r\nfscrypt_destroy();\r\nmutex_unlock(&fscrypt_init_mutex);\r\nreturn res;\r\n}\r\nstatic int __init fscrypt_init(void)\r\n{\r\nfscrypt_read_workqueue = alloc_workqueue("fscrypt_read_queue",\r\nWQ_HIGHPRI, 0);\r\nif (!fscrypt_read_workqueue)\r\ngoto fail;\r\nfscrypt_ctx_cachep = KMEM_CACHE(fscrypt_ctx, SLAB_RECLAIM_ACCOUNT);\r\nif (!fscrypt_ctx_cachep)\r\ngoto fail_free_queue;\r\nfscrypt_info_cachep = KMEM_CACHE(fscrypt_info, SLAB_RECLAIM_ACCOUNT);\r\nif (!fscrypt_info_cachep)\r\ngoto fail_free_ctx;\r\nreturn 0;\r\nfail_free_ctx:\r\nkmem_cache_destroy(fscrypt_ctx_cachep);\r\nfail_free_queue:\r\ndestroy_workqueue(fscrypt_read_workqueue);\r\nfail:\r\nreturn -ENOMEM;\r\n}\r\nstatic void __exit fscrypt_exit(void)\r\n{\r\nfscrypt_destroy();\r\nif (fscrypt_read_workqueue)\r\ndestroy_workqueue(fscrypt_read_workqueue);\r\nkmem_cache_destroy(fscrypt_ctx_cachep);\r\nkmem_cache_destroy(fscrypt_info_cachep);\r\n}
