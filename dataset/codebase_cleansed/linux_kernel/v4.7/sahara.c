static inline void sahara_write(struct sahara_dev *dev, u32 data, u32 reg)\r\n{\r\nwritel(data, dev->regs_base + reg);\r\n}\r\nstatic inline unsigned int sahara_read(struct sahara_dev *dev, u32 reg)\r\n{\r\nreturn readl(dev->regs_base + reg);\r\n}\r\nstatic u32 sahara_aes_key_hdr(struct sahara_dev *dev)\r\n{\r\nu32 hdr = SAHARA_HDR_BASE | SAHARA_HDR_SKHA_ALG_AES |\r\nSAHARA_HDR_FORM_KEY | SAHARA_HDR_LLO |\r\nSAHARA_HDR_CHA_SKHA | SAHARA_HDR_PARITY_BIT;\r\nif (dev->flags & FLAGS_CBC) {\r\nhdr |= SAHARA_HDR_SKHA_MODE_CBC;\r\nhdr ^= SAHARA_HDR_PARITY_BIT;\r\n}\r\nif (dev->flags & FLAGS_ENCRYPT) {\r\nhdr |= SAHARA_HDR_SKHA_OP_ENC;\r\nhdr ^= SAHARA_HDR_PARITY_BIT;\r\n}\r\nreturn hdr;\r\n}\r\nstatic u32 sahara_aes_data_link_hdr(struct sahara_dev *dev)\r\n{\r\nreturn SAHARA_HDR_BASE | SAHARA_HDR_FORM_DATA |\r\nSAHARA_HDR_CHA_SKHA | SAHARA_HDR_PARITY_BIT;\r\n}\r\nstatic void sahara_decode_error(struct sahara_dev *dev, unsigned int error)\r\n{\r\nu8 source = SAHARA_ERRSTATUS_GET_SOURCE(error);\r\nu16 chasrc = ffs(SAHARA_ERRSTATUS_GET_CHASRC(error));\r\ndev_err(dev->device, "%s: Error Register = 0x%08x\n", __func__, error);\r\ndev_err(dev->device, " - %s.\n", sahara_err_src[source]);\r\nif (source == SAHARA_ERRSOURCE_DMA) {\r\nif (error & SAHARA_ERRSTATUS_DMA_DIR)\r\ndev_err(dev->device, " * DMA read.\n");\r\nelse\r\ndev_err(dev->device, " * DMA write.\n");\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_err_dmasize[SAHARA_ERRSTATUS_GET_DMASZ(error)]);\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_err_dmasrc[SAHARA_ERRSTATUS_GET_DMASRC(error)]);\r\n} else if (source == SAHARA_ERRSOURCE_CHA) {\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_cha_errsrc[chasrc]);\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_cha_err[SAHARA_ERRSTATUS_GET_CHAERR(error)]);\r\n}\r\ndev_err(dev->device, "\n");\r\n}\r\nstatic void sahara_decode_status(struct sahara_dev *dev, unsigned int status)\r\n{\r\nu8 state;\r\nif (!IS_ENABLED(DEBUG))\r\nreturn;\r\nstate = SAHARA_STATUS_GET_STATE(status);\r\ndev_dbg(dev->device, "%s: Status Register = 0x%08x\n",\r\n__func__, status);\r\ndev_dbg(dev->device, " - State = %d:\n", state);\r\nif (state & SAHARA_STATE_COMP_FLAG)\r\ndev_dbg(dev->device, " * Descriptor completed. IRQ pending.\n");\r\ndev_dbg(dev->device, " * %s.\n",\r\nsahara_state[state & ~SAHARA_STATE_COMP_FLAG]);\r\nif (status & SAHARA_STATUS_DAR_FULL)\r\ndev_dbg(dev->device, " - DAR Full.\n");\r\nif (status & SAHARA_STATUS_ERROR)\r\ndev_dbg(dev->device, " - Error.\n");\r\nif (status & SAHARA_STATUS_SECURE)\r\ndev_dbg(dev->device, " - Secure.\n");\r\nif (status & SAHARA_STATUS_FAIL)\r\ndev_dbg(dev->device, " - Fail.\n");\r\nif (status & SAHARA_STATUS_RNG_RESEED)\r\ndev_dbg(dev->device, " - RNG Reseed Request.\n");\r\nif (status & SAHARA_STATUS_ACTIVE_RNG)\r\ndev_dbg(dev->device, " - RNG Active.\n");\r\nif (status & SAHARA_STATUS_ACTIVE_MDHA)\r\ndev_dbg(dev->device, " - MDHA Active.\n");\r\nif (status & SAHARA_STATUS_ACTIVE_SKHA)\r\ndev_dbg(dev->device, " - SKHA Active.\n");\r\nif (status & SAHARA_STATUS_MODE_BATCH)\r\ndev_dbg(dev->device, " - Batch Mode.\n");\r\nelse if (status & SAHARA_STATUS_MODE_DEDICATED)\r\ndev_dbg(dev->device, " - Decidated Mode.\n");\r\nelse if (status & SAHARA_STATUS_MODE_DEBUG)\r\ndev_dbg(dev->device, " - Debug Mode.\n");\r\ndev_dbg(dev->device, " - Internal state = 0x%02x\n",\r\nSAHARA_STATUS_GET_ISTATE(status));\r\ndev_dbg(dev->device, "Current DAR: 0x%08x\n",\r\nsahara_read(dev, SAHARA_REG_CDAR));\r\ndev_dbg(dev->device, "Initial DAR: 0x%08x\n\n",\r\nsahara_read(dev, SAHARA_REG_IDAR));\r\n}\r\nstatic void sahara_dump_descriptors(struct sahara_dev *dev)\r\n{\r\nint i;\r\nif (!IS_ENABLED(DEBUG))\r\nreturn;\r\nfor (i = 0; i < SAHARA_MAX_HW_DESC; i++) {\r\ndev_dbg(dev->device, "Descriptor (%d) (%pad):\n",\r\ni, &dev->hw_phys_desc[i]);\r\ndev_dbg(dev->device, "\thdr = 0x%08x\n", dev->hw_desc[i]->hdr);\r\ndev_dbg(dev->device, "\tlen1 = %u\n", dev->hw_desc[i]->len1);\r\ndev_dbg(dev->device, "\tp1 = 0x%08x\n", dev->hw_desc[i]->p1);\r\ndev_dbg(dev->device, "\tlen2 = %u\n", dev->hw_desc[i]->len2);\r\ndev_dbg(dev->device, "\tp2 = 0x%08x\n", dev->hw_desc[i]->p2);\r\ndev_dbg(dev->device, "\tnext = 0x%08x\n",\r\ndev->hw_desc[i]->next);\r\n}\r\ndev_dbg(dev->device, "\n");\r\n}\r\nstatic void sahara_dump_links(struct sahara_dev *dev)\r\n{\r\nint i;\r\nif (!IS_ENABLED(DEBUG))\r\nreturn;\r\nfor (i = 0; i < SAHARA_MAX_HW_LINK; i++) {\r\ndev_dbg(dev->device, "Link (%d) (%pad):\n",\r\ni, &dev->hw_phys_link[i]);\r\ndev_dbg(dev->device, "\tlen = %u\n", dev->hw_link[i]->len);\r\ndev_dbg(dev->device, "\tp = 0x%08x\n", dev->hw_link[i]->p);\r\ndev_dbg(dev->device, "\tnext = 0x%08x\n",\r\ndev->hw_link[i]->next);\r\n}\r\ndev_dbg(dev->device, "\n");\r\n}\r\nstatic int sahara_hw_descriptor_create(struct sahara_dev *dev)\r\n{\r\nstruct sahara_ctx *ctx = dev->ctx;\r\nstruct scatterlist *sg;\r\nint ret;\r\nint i, j;\r\nint idx = 0;\r\nif (ctx->flags & FLAGS_NEW_KEY) {\r\nmemcpy(dev->key_base, ctx->key, ctx->keylen);\r\nctx->flags &= ~FLAGS_NEW_KEY;\r\nif (dev->flags & FLAGS_CBC) {\r\ndev->hw_desc[idx]->len1 = AES_BLOCK_SIZE;\r\ndev->hw_desc[idx]->p1 = dev->iv_phys_base;\r\n} else {\r\ndev->hw_desc[idx]->len1 = 0;\r\ndev->hw_desc[idx]->p1 = 0;\r\n}\r\ndev->hw_desc[idx]->len2 = ctx->keylen;\r\ndev->hw_desc[idx]->p2 = dev->key_phys_base;\r\ndev->hw_desc[idx]->next = dev->hw_phys_desc[1];\r\ndev->hw_desc[idx]->hdr = sahara_aes_key_hdr(dev);\r\nidx++;\r\n}\r\ndev->nb_in_sg = sg_nents_for_len(dev->in_sg, dev->total);\r\nif (dev->nb_in_sg < 0) {\r\ndev_err(dev->device, "Invalid numbers of src SG.\n");\r\nreturn dev->nb_in_sg;\r\n}\r\ndev->nb_out_sg = sg_nents_for_len(dev->out_sg, dev->total);\r\nif (dev->nb_out_sg < 0) {\r\ndev_err(dev->device, "Invalid numbers of dst SG.\n");\r\nreturn dev->nb_out_sg;\r\n}\r\nif ((dev->nb_in_sg + dev->nb_out_sg) > SAHARA_MAX_HW_LINK) {\r\ndev_err(dev->device, "not enough hw links (%d)\n",\r\ndev->nb_in_sg + dev->nb_out_sg);\r\nreturn -EINVAL;\r\n}\r\nret = dma_map_sg(dev->device, dev->in_sg, dev->nb_in_sg,\r\nDMA_TO_DEVICE);\r\nif (ret != dev->nb_in_sg) {\r\ndev_err(dev->device, "couldn't map in sg\n");\r\ngoto unmap_in;\r\n}\r\nret = dma_map_sg(dev->device, dev->out_sg, dev->nb_out_sg,\r\nDMA_FROM_DEVICE);\r\nif (ret != dev->nb_out_sg) {\r\ndev_err(dev->device, "couldn't map out sg\n");\r\ngoto unmap_out;\r\n}\r\ndev->hw_desc[idx]->p1 = dev->hw_phys_link[0];\r\nsg = dev->in_sg;\r\nfor (i = 0; i < dev->nb_in_sg; i++) {\r\ndev->hw_link[i]->len = sg->length;\r\ndev->hw_link[i]->p = sg->dma_address;\r\nif (i == (dev->nb_in_sg - 1)) {\r\ndev->hw_link[i]->next = 0;\r\n} else {\r\ndev->hw_link[i]->next = dev->hw_phys_link[i + 1];\r\nsg = sg_next(sg);\r\n}\r\n}\r\ndev->hw_desc[idx]->p2 = dev->hw_phys_link[i];\r\nsg = dev->out_sg;\r\nfor (j = i; j < dev->nb_out_sg + i; j++) {\r\ndev->hw_link[j]->len = sg->length;\r\ndev->hw_link[j]->p = sg->dma_address;\r\nif (j == (dev->nb_out_sg + i - 1)) {\r\ndev->hw_link[j]->next = 0;\r\n} else {\r\ndev->hw_link[j]->next = dev->hw_phys_link[j + 1];\r\nsg = sg_next(sg);\r\n}\r\n}\r\ndev->hw_desc[idx]->hdr = sahara_aes_data_link_hdr(dev);\r\ndev->hw_desc[idx]->len1 = dev->total;\r\ndev->hw_desc[idx]->len2 = dev->total;\r\ndev->hw_desc[idx]->next = 0;\r\nsahara_dump_descriptors(dev);\r\nsahara_dump_links(dev);\r\nsahara_write(dev, dev->hw_phys_desc[0], SAHARA_REG_DAR);\r\nreturn 0;\r\nunmap_out:\r\ndma_unmap_sg(dev->device, dev->out_sg, dev->nb_out_sg,\r\nDMA_TO_DEVICE);\r\nunmap_in:\r\ndma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\r\nDMA_FROM_DEVICE);\r\nreturn -EINVAL;\r\n}\r\nstatic int sahara_aes_process(struct ablkcipher_request *req)\r\n{\r\nstruct sahara_dev *dev = dev_ptr;\r\nstruct sahara_ctx *ctx;\r\nstruct sahara_aes_reqctx *rctx;\r\nint ret;\r\nunsigned long timeout;\r\ndev_dbg(dev->device,\r\n"dispatch request (nbytes=%d, src=%p, dst=%p)\n",\r\nreq->nbytes, req->src, req->dst);\r\ndev->total = req->nbytes;\r\ndev->in_sg = req->src;\r\ndev->out_sg = req->dst;\r\nrctx = ablkcipher_request_ctx(req);\r\nctx = crypto_ablkcipher_ctx(crypto_ablkcipher_reqtfm(req));\r\nrctx->mode &= FLAGS_MODE_MASK;\r\ndev->flags = (dev->flags & ~FLAGS_MODE_MASK) | rctx->mode;\r\nif ((dev->flags & FLAGS_CBC) && req->info)\r\nmemcpy(dev->iv_base, req->info, AES_KEYSIZE_128);\r\ndev->ctx = ctx;\r\nreinit_completion(&dev->dma_completion);\r\nret = sahara_hw_descriptor_create(dev);\r\nif (ret)\r\nreturn -EINVAL;\r\ntimeout = wait_for_completion_timeout(&dev->dma_completion,\r\nmsecs_to_jiffies(SAHARA_TIMEOUT_MS));\r\nif (!timeout) {\r\ndev_err(dev->device, "AES timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\ndma_unmap_sg(dev->device, dev->out_sg, dev->nb_out_sg,\r\nDMA_TO_DEVICE);\r\ndma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\r\nDMA_FROM_DEVICE);\r\nreturn 0;\r\n}\r\nstatic int sahara_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nint ret;\r\nctx->keylen = keylen;\r\nif (keylen == AES_KEYSIZE_128) {\r\nmemcpy(ctx->key, key, keylen);\r\nctx->flags |= FLAGS_NEW_KEY;\r\nreturn 0;\r\n}\r\nif (keylen != AES_KEYSIZE_128 &&\r\nkeylen != AES_KEYSIZE_192 && keylen != AES_KEYSIZE_256)\r\nreturn -EINVAL;\r\nctx->fallback->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nctx->fallback->base.crt_flags |=\r\n(tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);\r\nret = crypto_ablkcipher_setkey(ctx->fallback, key, keylen);\r\nif (ret) {\r\nstruct crypto_tfm *tfm_aux = crypto_ablkcipher_tfm(tfm);\r\ntfm_aux->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm_aux->crt_flags |=\r\n(ctx->fallback->base.crt_flags & CRYPTO_TFM_RES_MASK);\r\n}\r\nreturn ret;\r\n}\r\nstatic int sahara_aes_crypt(struct ablkcipher_request *req, unsigned long mode)\r\n{\r\nstruct sahara_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct sahara_dev *dev = dev_ptr;\r\nint err = 0;\r\ndev_dbg(dev->device, "nbytes: %d, enc: %d, cbc: %d\n",\r\nreq->nbytes, !!(mode & FLAGS_ENCRYPT), !!(mode & FLAGS_CBC));\r\nif (!IS_ALIGNED(req->nbytes, AES_BLOCK_SIZE)) {\r\ndev_err(dev->device,\r\n"request size is not exact amount of AES blocks\n");\r\nreturn -EINVAL;\r\n}\r\nrctx->mode = mode;\r\nmutex_lock(&dev->queue_mutex);\r\nerr = ablkcipher_enqueue_request(&dev->queue, req);\r\nmutex_unlock(&dev->queue_mutex);\r\nwake_up_process(dev->kthread);\r\nreturn err;\r\n}\r\nstatic int sahara_aes_ecb_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_encrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, FLAGS_ENCRYPT);\r\n}\r\nstatic int sahara_aes_ecb_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_decrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, 0);\r\n}\r\nstatic int sahara_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_encrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_CBC);\r\n}\r\nstatic int sahara_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_decrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, FLAGS_CBC);\r\n}\r\nstatic int sahara_aes_cra_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = crypto_tfm_alg_name(tfm);\r\nstruct sahara_ctx *ctx = crypto_tfm_ctx(tfm);\r\nctx->fallback = crypto_alloc_ablkcipher(name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ctx->fallback)) {\r\npr_err("Error allocating fallback algo %s\n", name);\r\nreturn PTR_ERR(ctx->fallback);\r\n}\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct sahara_aes_reqctx);\r\nreturn 0;\r\n}\r\nstatic void sahara_aes_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct sahara_ctx *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->fallback)\r\ncrypto_free_ablkcipher(ctx->fallback);\r\nctx->fallback = NULL;\r\n}\r\nstatic u32 sahara_sha_init_hdr(struct sahara_dev *dev,\r\nstruct sahara_sha_reqctx *rctx)\r\n{\r\nu32 hdr = 0;\r\nhdr = rctx->mode;\r\nif (rctx->first) {\r\nhdr |= SAHARA_HDR_MDHA_SET_MODE_HASH;\r\nhdr |= SAHARA_HDR_MDHA_INIT;\r\n} else {\r\nhdr |= SAHARA_HDR_MDHA_SET_MODE_MD_KEY;\r\n}\r\nif (rctx->last)\r\nhdr |= SAHARA_HDR_MDHA_PDATA;\r\nif (hweight_long(hdr) % 2 == 0)\r\nhdr |= SAHARA_HDR_PARITY_BIT;\r\nreturn hdr;\r\n}\r\nstatic int sahara_sha_hw_links_create(struct sahara_dev *dev,\r\nstruct sahara_sha_reqctx *rctx,\r\nint start)\r\n{\r\nstruct scatterlist *sg;\r\nunsigned int i;\r\nint ret;\r\ndev->in_sg = rctx->in_sg;\r\ndev->nb_in_sg = sg_nents_for_len(dev->in_sg, rctx->total);\r\nif (dev->nb_in_sg < 0) {\r\ndev_err(dev->device, "Invalid numbers of src SG.\n");\r\nreturn dev->nb_in_sg;\r\n}\r\nif ((dev->nb_in_sg) > SAHARA_MAX_HW_LINK) {\r\ndev_err(dev->device, "not enough hw links (%d)\n",\r\ndev->nb_in_sg + dev->nb_out_sg);\r\nreturn -EINVAL;\r\n}\r\nsg = dev->in_sg;\r\nret = dma_map_sg(dev->device, dev->in_sg, dev->nb_in_sg, DMA_TO_DEVICE);\r\nif (!ret)\r\nreturn -EFAULT;\r\nfor (i = start; i < dev->nb_in_sg + start; i++) {\r\ndev->hw_link[i]->len = sg->length;\r\ndev->hw_link[i]->p = sg->dma_address;\r\nif (i == (dev->nb_in_sg + start - 1)) {\r\ndev->hw_link[i]->next = 0;\r\n} else {\r\ndev->hw_link[i]->next = dev->hw_phys_link[i + 1];\r\nsg = sg_next(sg);\r\n}\r\n}\r\nreturn i;\r\n}\r\nstatic int sahara_sha_hw_data_descriptor_create(struct sahara_dev *dev,\r\nstruct sahara_sha_reqctx *rctx,\r\nstruct ahash_request *req,\r\nint index)\r\n{\r\nunsigned result_len;\r\nint i = index;\r\nif (rctx->first)\r\ndev->hw_desc[index]->hdr = sahara_sha_init_hdr(dev, rctx);\r\nelse\r\ndev->hw_desc[index]->hdr = SAHARA_HDR_MDHA_HASH;\r\ndev->hw_desc[index]->len1 = rctx->total;\r\nif (dev->hw_desc[index]->len1 == 0) {\r\ndev->hw_desc[index]->p1 = 0;\r\nrctx->sg_in_idx = 0;\r\n} else {\r\ndev->hw_desc[index]->p1 = dev->hw_phys_link[index];\r\ni = sahara_sha_hw_links_create(dev, rctx, index);\r\nrctx->sg_in_idx = index;\r\nif (i < 0)\r\nreturn i;\r\n}\r\ndev->hw_desc[index]->p2 = dev->hw_phys_link[i];\r\nresult_len = rctx->context_size;\r\ndev->hw_link[i]->p = dev->context_phys_base;\r\ndev->hw_link[i]->len = result_len;\r\ndev->hw_desc[index]->len2 = result_len;\r\ndev->hw_link[i]->next = 0;\r\nreturn 0;\r\n}\r\nstatic int sahara_sha_hw_context_descriptor_create(struct sahara_dev *dev,\r\nstruct sahara_sha_reqctx *rctx,\r\nstruct ahash_request *req,\r\nint index)\r\n{\r\ndev->hw_desc[index]->hdr = sahara_sha_init_hdr(dev, rctx);\r\ndev->hw_desc[index]->len1 = rctx->context_size;\r\ndev->hw_desc[index]->p1 = dev->hw_phys_link[index];\r\ndev->hw_desc[index]->len2 = 0;\r\ndev->hw_desc[index]->p2 = 0;\r\ndev->hw_link[index]->len = rctx->context_size;\r\ndev->hw_link[index]->p = dev->context_phys_base;\r\ndev->hw_link[index]->next = 0;\r\nreturn 0;\r\n}\r\nstatic int sahara_walk_and_recalc(struct scatterlist *sg, unsigned int nbytes)\r\n{\r\nif (!sg || !sg->length)\r\nreturn nbytes;\r\nwhile (nbytes && sg) {\r\nif (nbytes <= sg->length) {\r\nsg->length = nbytes;\r\nsg_mark_end(sg);\r\nbreak;\r\n}\r\nnbytes -= sg->length;\r\nsg = sg_next(sg);\r\n}\r\nreturn nbytes;\r\n}\r\nstatic int sahara_sha_prepare_request(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\r\nunsigned int hash_later;\r\nunsigned int block_size;\r\nunsigned int len;\r\nblock_size = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nlen = rctx->buf_cnt + req->nbytes;\r\nif (!rctx->last && (len < block_size)) {\r\nscatterwalk_map_and_copy(rctx->buf + rctx->buf_cnt, req->src,\r\n0, req->nbytes, 0);\r\nrctx->buf_cnt += req->nbytes;\r\nreturn 0;\r\n}\r\nif (rctx->buf_cnt)\r\nmemcpy(rctx->rembuf, rctx->buf, rctx->buf_cnt);\r\nhash_later = rctx->last ? 0 : len & (block_size - 1);\r\nif (hash_later) {\r\nunsigned int offset = req->nbytes - hash_later;\r\nscatterwalk_map_and_copy(rctx->buf, req->src, offset,\r\nhash_later, 0);\r\n}\r\nreq->nbytes = req->nbytes - hash_later;\r\nsahara_walk_and_recalc(req->src, req->nbytes);\r\nif (rctx->buf_cnt && req->nbytes) {\r\nsg_init_table(rctx->in_sg_chain, 2);\r\nsg_set_buf(rctx->in_sg_chain, rctx->rembuf, rctx->buf_cnt);\r\nsg_chain(rctx->in_sg_chain, 2, req->src);\r\nrctx->total = req->nbytes + rctx->buf_cnt;\r\nrctx->in_sg = rctx->in_sg_chain;\r\nreq->src = rctx->in_sg_chain;\r\n} else if (rctx->buf_cnt) {\r\nif (req->src)\r\nrctx->in_sg = req->src;\r\nelse\r\nrctx->in_sg = rctx->in_sg_chain;\r\nsg_init_one(rctx->in_sg, rctx->rembuf, rctx->buf_cnt);\r\nrctx->total = rctx->buf_cnt;\r\n} else {\r\nrctx->in_sg = req->src;\r\nrctx->total = req->nbytes;\r\nreq->src = rctx->in_sg;\r\n}\r\nrctx->buf_cnt = hash_later;\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int sahara_sha_process(struct ahash_request *req)\r\n{\r\nstruct sahara_dev *dev = dev_ptr;\r\nstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\r\nint ret;\r\nunsigned long timeout;\r\nret = sahara_sha_prepare_request(req);\r\nif (!ret)\r\nreturn ret;\r\nif (rctx->first) {\r\nsahara_sha_hw_data_descriptor_create(dev, rctx, req, 0);\r\ndev->hw_desc[0]->next = 0;\r\nrctx->first = 0;\r\n} else {\r\nmemcpy(dev->context_base, rctx->context, rctx->context_size);\r\nsahara_sha_hw_context_descriptor_create(dev, rctx, req, 0);\r\ndev->hw_desc[0]->next = dev->hw_phys_desc[1];\r\nsahara_sha_hw_data_descriptor_create(dev, rctx, req, 1);\r\ndev->hw_desc[1]->next = 0;\r\n}\r\nsahara_dump_descriptors(dev);\r\nsahara_dump_links(dev);\r\nreinit_completion(&dev->dma_completion);\r\nsahara_write(dev, dev->hw_phys_desc[0], SAHARA_REG_DAR);\r\ntimeout = wait_for_completion_timeout(&dev->dma_completion,\r\nmsecs_to_jiffies(SAHARA_TIMEOUT_MS));\r\nif (!timeout) {\r\ndev_err(dev->device, "SHA timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nif (rctx->sg_in_idx)\r\ndma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\r\nDMA_TO_DEVICE);\r\nmemcpy(rctx->context, dev->context_base, rctx->context_size);\r\nif (req->result)\r\nmemcpy(req->result, rctx->context, rctx->digest_size);\r\nreturn 0;\r\n}\r\nstatic int sahara_queue_manage(void *data)\r\n{\r\nstruct sahara_dev *dev = (struct sahara_dev *)data;\r\nstruct crypto_async_request *async_req;\r\nstruct crypto_async_request *backlog;\r\nint ret = 0;\r\ndo {\r\n__set_current_state(TASK_INTERRUPTIBLE);\r\nmutex_lock(&dev->queue_mutex);\r\nbacklog = crypto_get_backlog(&dev->queue);\r\nasync_req = crypto_dequeue_request(&dev->queue);\r\nmutex_unlock(&dev->queue_mutex);\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nif (async_req) {\r\nif (crypto_tfm_alg_type(async_req->tfm) ==\r\nCRYPTO_ALG_TYPE_AHASH) {\r\nstruct ahash_request *req =\r\nahash_request_cast(async_req);\r\nret = sahara_sha_process(req);\r\n} else {\r\nstruct ablkcipher_request *req =\r\nablkcipher_request_cast(async_req);\r\nret = sahara_aes_process(req);\r\n}\r\nasync_req->complete(async_req, ret);\r\ncontinue;\r\n}\r\nschedule();\r\n} while (!kthread_should_stop());\r\nreturn 0;\r\n}\r\nstatic int sahara_sha_enqueue(struct ahash_request *req, int last)\r\n{\r\nstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\r\nstruct sahara_dev *dev = dev_ptr;\r\nint ret;\r\nif (!req->nbytes && !last)\r\nreturn 0;\r\nrctx->last = last;\r\nif (!rctx->active) {\r\nrctx->active = 1;\r\nrctx->first = 1;\r\n}\r\nmutex_lock(&dev->queue_mutex);\r\nret = crypto_enqueue_request(&dev->queue, &req->base);\r\nmutex_unlock(&dev->queue_mutex);\r\nwake_up_process(dev->kthread);\r\nreturn ret;\r\n}\r\nstatic int sahara_sha_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\r\nmemset(rctx, 0, sizeof(*rctx));\r\nswitch (crypto_ahash_digestsize(tfm)) {\r\ncase SHA1_DIGEST_SIZE:\r\nrctx->mode |= SAHARA_HDR_MDHA_ALG_SHA1;\r\nrctx->digest_size = SHA1_DIGEST_SIZE;\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nrctx->mode |= SAHARA_HDR_MDHA_ALG_SHA256;\r\nrctx->digest_size = SHA256_DIGEST_SIZE;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nrctx->context_size = rctx->digest_size + 4;\r\nrctx->active = 0;\r\nreturn 0;\r\n}\r\nstatic int sahara_sha_update(struct ahash_request *req)\r\n{\r\nreturn sahara_sha_enqueue(req, 0);\r\n}\r\nstatic int sahara_sha_final(struct ahash_request *req)\r\n{\r\nreq->nbytes = 0;\r\nreturn sahara_sha_enqueue(req, 1);\r\n}\r\nstatic int sahara_sha_finup(struct ahash_request *req)\r\n{\r\nreturn sahara_sha_enqueue(req, 1);\r\n}\r\nstatic int sahara_sha_digest(struct ahash_request *req)\r\n{\r\nsahara_sha_init(req);\r\nreturn sahara_sha_finup(req);\r\n}\r\nstatic int sahara_sha_export(struct ahash_request *req, void *out)\r\n{\r\nstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\r\nmemcpy(out, rctx, sizeof(struct sahara_sha_reqctx));\r\nreturn 0;\r\n}\r\nstatic int sahara_sha_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct sahara_sha_reqctx *rctx = ahash_request_ctx(req);\r\nmemcpy(rctx, in, sizeof(struct sahara_sha_reqctx));\r\nreturn 0;\r\n}\r\nstatic int sahara_sha_cra_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = crypto_tfm_alg_name(tfm);\r\nstruct sahara_ctx *ctx = crypto_tfm_ctx(tfm);\r\nctx->shash_fallback = crypto_alloc_shash(name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ctx->shash_fallback)) {\r\npr_err("Error allocating fallback algo %s\n", name);\r\nreturn PTR_ERR(ctx->shash_fallback);\r\n}\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct sahara_sha_reqctx) +\r\nSHA_BUFFER_LEN + SHA256_BLOCK_SIZE);\r\nreturn 0;\r\n}\r\nstatic void sahara_sha_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct sahara_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_shash(ctx->shash_fallback);\r\nctx->shash_fallback = NULL;\r\n}\r\nstatic irqreturn_t sahara_irq_handler(int irq, void *data)\r\n{\r\nstruct sahara_dev *dev = (struct sahara_dev *)data;\r\nunsigned int stat = sahara_read(dev, SAHARA_REG_STATUS);\r\nunsigned int err = sahara_read(dev, SAHARA_REG_ERRSTATUS);\r\nsahara_write(dev, SAHARA_CMD_CLEAR_INT | SAHARA_CMD_CLEAR_ERR,\r\nSAHARA_REG_CMD);\r\nsahara_decode_status(dev, stat);\r\nif (SAHARA_STATUS_GET_STATE(stat) == SAHARA_STATE_BUSY) {\r\nreturn IRQ_NONE;\r\n} else if (SAHARA_STATUS_GET_STATE(stat) == SAHARA_STATE_COMPLETE) {\r\ndev->error = 0;\r\n} else {\r\nsahara_decode_error(dev, err);\r\ndev->error = -EINVAL;\r\n}\r\ncomplete(&dev->dma_completion);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int sahara_register_algs(struct sahara_dev *dev)\r\n{\r\nint err;\r\nunsigned int i, j, k, l;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\r\nINIT_LIST_HEAD(&aes_algs[i].cra_list);\r\nerr = crypto_register_alg(&aes_algs[i]);\r\nif (err)\r\ngoto err_aes_algs;\r\n}\r\nfor (k = 0; k < ARRAY_SIZE(sha_v3_algs); k++) {\r\nerr = crypto_register_ahash(&sha_v3_algs[k]);\r\nif (err)\r\ngoto err_sha_v3_algs;\r\n}\r\nif (dev->version > SAHARA_VERSION_3)\r\nfor (l = 0; l < ARRAY_SIZE(sha_v4_algs); l++) {\r\nerr = crypto_register_ahash(&sha_v4_algs[l]);\r\nif (err)\r\ngoto err_sha_v4_algs;\r\n}\r\nreturn 0;\r\nerr_sha_v4_algs:\r\nfor (j = 0; j < l; j++)\r\ncrypto_unregister_ahash(&sha_v4_algs[j]);\r\nerr_sha_v3_algs:\r\nfor (j = 0; j < k; j++)\r\ncrypto_unregister_ahash(&sha_v4_algs[j]);\r\nerr_aes_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_alg(&aes_algs[j]);\r\nreturn err;\r\n}\r\nstatic void sahara_unregister_algs(struct sahara_dev *dev)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++)\r\ncrypto_unregister_alg(&aes_algs[i]);\r\nfor (i = 0; i < ARRAY_SIZE(sha_v4_algs); i++)\r\ncrypto_unregister_ahash(&sha_v3_algs[i]);\r\nif (dev->version > SAHARA_VERSION_3)\r\nfor (i = 0; i < ARRAY_SIZE(sha_v4_algs); i++)\r\ncrypto_unregister_ahash(&sha_v4_algs[i]);\r\n}\r\nstatic int sahara_probe(struct platform_device *pdev)\r\n{\r\nstruct sahara_dev *dev;\r\nstruct resource *res;\r\nu32 version;\r\nint irq;\r\nint err;\r\nint i;\r\ndev = devm_kzalloc(&pdev->dev, sizeof(struct sahara_dev), GFP_KERNEL);\r\nif (dev == NULL) {\r\ndev_err(&pdev->dev, "unable to alloc data struct.\n");\r\nreturn -ENOMEM;\r\n}\r\ndev->device = &pdev->dev;\r\nplatform_set_drvdata(pdev, dev);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndev->regs_base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(dev->regs_base))\r\nreturn PTR_ERR(dev->regs_base);\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0) {\r\ndev_err(&pdev->dev, "failed to get irq resource\n");\r\nreturn irq;\r\n}\r\nerr = devm_request_irq(&pdev->dev, irq, sahara_irq_handler,\r\n0, dev_name(&pdev->dev), dev);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to request irq\n");\r\nreturn err;\r\n}\r\ndev->clk_ipg = devm_clk_get(&pdev->dev, "ipg");\r\nif (IS_ERR(dev->clk_ipg)) {\r\ndev_err(&pdev->dev, "Could not get ipg clock\n");\r\nreturn PTR_ERR(dev->clk_ipg);\r\n}\r\ndev->clk_ahb = devm_clk_get(&pdev->dev, "ahb");\r\nif (IS_ERR(dev->clk_ahb)) {\r\ndev_err(&pdev->dev, "Could not get ahb clock\n");\r\nreturn PTR_ERR(dev->clk_ahb);\r\n}\r\ndev->hw_desc[0] = dmam_alloc_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_DESC * sizeof(struct sahara_hw_desc),\r\n&dev->hw_phys_desc[0], GFP_KERNEL);\r\nif (!dev->hw_desc[0]) {\r\ndev_err(&pdev->dev, "Could not allocate hw descriptors\n");\r\nreturn -ENOMEM;\r\n}\r\ndev->hw_desc[1] = dev->hw_desc[0] + 1;\r\ndev->hw_phys_desc[1] = dev->hw_phys_desc[0] +\r\nsizeof(struct sahara_hw_desc);\r\ndev->key_base = dmam_alloc_coherent(&pdev->dev, 2 * AES_KEYSIZE_128,\r\n&dev->key_phys_base, GFP_KERNEL);\r\nif (!dev->key_base) {\r\ndev_err(&pdev->dev, "Could not allocate memory for key\n");\r\nreturn -ENOMEM;\r\n}\r\ndev->iv_base = dev->key_base + AES_KEYSIZE_128;\r\ndev->iv_phys_base = dev->key_phys_base + AES_KEYSIZE_128;\r\ndev->context_base = dmam_alloc_coherent(&pdev->dev,\r\nSHA256_DIGEST_SIZE + 4,\r\n&dev->context_phys_base, GFP_KERNEL);\r\nif (!dev->context_base) {\r\ndev_err(&pdev->dev, "Could not allocate memory for MDHA context\n");\r\nreturn -ENOMEM;\r\n}\r\ndev->hw_link[0] = dmam_alloc_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_LINK * sizeof(struct sahara_hw_link),\r\n&dev->hw_phys_link[0], GFP_KERNEL);\r\nif (!dev->hw_link[0]) {\r\ndev_err(&pdev->dev, "Could not allocate hw links\n");\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 1; i < SAHARA_MAX_HW_LINK; i++) {\r\ndev->hw_phys_link[i] = dev->hw_phys_link[i - 1] +\r\nsizeof(struct sahara_hw_link);\r\ndev->hw_link[i] = dev->hw_link[i - 1] + 1;\r\n}\r\ncrypto_init_queue(&dev->queue, SAHARA_QUEUE_LENGTH);\r\nspin_lock_init(&dev->lock);\r\nmutex_init(&dev->queue_mutex);\r\ndev_ptr = dev;\r\ndev->kthread = kthread_run(sahara_queue_manage, dev, "sahara_crypto");\r\nif (IS_ERR(dev->kthread)) {\r\nreturn PTR_ERR(dev->kthread);\r\n}\r\ninit_completion(&dev->dma_completion);\r\nerr = clk_prepare_enable(dev->clk_ipg);\r\nif (err)\r\nreturn err;\r\nerr = clk_prepare_enable(dev->clk_ahb);\r\nif (err)\r\ngoto clk_ipg_disable;\r\nversion = sahara_read(dev, SAHARA_REG_VERSION);\r\nif (of_device_is_compatible(pdev->dev.of_node, "fsl,imx27-sahara")) {\r\nif (version != SAHARA_VERSION_3)\r\nerr = -ENODEV;\r\n} else if (of_device_is_compatible(pdev->dev.of_node,\r\n"fsl,imx53-sahara")) {\r\nif (((version >> 8) & 0xff) != SAHARA_VERSION_4)\r\nerr = -ENODEV;\r\nversion = (version >> 8) & 0xff;\r\n}\r\nif (err == -ENODEV) {\r\ndev_err(&pdev->dev, "SAHARA version %d not supported\n",\r\nversion);\r\ngoto err_algs;\r\n}\r\ndev->version = version;\r\nsahara_write(dev, SAHARA_CMD_RESET | SAHARA_CMD_MODE_BATCH,\r\nSAHARA_REG_CMD);\r\nsahara_write(dev, SAHARA_CONTROL_SET_THROTTLE(0) |\r\nSAHARA_CONTROL_SET_MAXBURST(8) |\r\nSAHARA_CONTROL_RNG_AUTORSD |\r\nSAHARA_CONTROL_ENABLE_INT,\r\nSAHARA_REG_CONTROL);\r\nerr = sahara_register_algs(dev);\r\nif (err)\r\ngoto err_algs;\r\ndev_info(&pdev->dev, "SAHARA version %d initialized\n", version);\r\nreturn 0;\r\nerr_algs:\r\nkthread_stop(dev->kthread);\r\ndev_ptr = NULL;\r\nclk_disable_unprepare(dev->clk_ahb);\r\nclk_ipg_disable:\r\nclk_disable_unprepare(dev->clk_ipg);\r\nreturn err;\r\n}\r\nstatic int sahara_remove(struct platform_device *pdev)\r\n{\r\nstruct sahara_dev *dev = platform_get_drvdata(pdev);\r\nkthread_stop(dev->kthread);\r\nsahara_unregister_algs(dev);\r\nclk_disable_unprepare(dev->clk_ipg);\r\nclk_disable_unprepare(dev->clk_ahb);\r\ndev_ptr = NULL;\r\nreturn 0;\r\n}
