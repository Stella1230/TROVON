struct gmap *gmap_alloc(struct mm_struct *mm, unsigned long limit)\r\n{\r\nstruct gmap *gmap;\r\nstruct page *page;\r\nunsigned long *table;\r\nunsigned long etype, atype;\r\nif (limit < (1UL << 31)) {\r\nlimit = (1UL << 31) - 1;\r\natype = _ASCE_TYPE_SEGMENT;\r\netype = _SEGMENT_ENTRY_EMPTY;\r\n} else if (limit < (1UL << 42)) {\r\nlimit = (1UL << 42) - 1;\r\natype = _ASCE_TYPE_REGION3;\r\netype = _REGION3_ENTRY_EMPTY;\r\n} else if (limit < (1UL << 53)) {\r\nlimit = (1UL << 53) - 1;\r\natype = _ASCE_TYPE_REGION2;\r\netype = _REGION2_ENTRY_EMPTY;\r\n} else {\r\nlimit = -1UL;\r\natype = _ASCE_TYPE_REGION1;\r\netype = _REGION1_ENTRY_EMPTY;\r\n}\r\ngmap = kzalloc(sizeof(struct gmap), GFP_KERNEL);\r\nif (!gmap)\r\ngoto out;\r\nINIT_LIST_HEAD(&gmap->crst_list);\r\nINIT_RADIX_TREE(&gmap->guest_to_host, GFP_KERNEL);\r\nINIT_RADIX_TREE(&gmap->host_to_guest, GFP_ATOMIC);\r\nspin_lock_init(&gmap->guest_table_lock);\r\ngmap->mm = mm;\r\npage = alloc_pages(GFP_KERNEL, 2);\r\nif (!page)\r\ngoto out_free;\r\npage->index = 0;\r\nlist_add(&page->lru, &gmap->crst_list);\r\ntable = (unsigned long *) page_to_phys(page);\r\ncrst_table_init(table, etype);\r\ngmap->table = table;\r\ngmap->asce = atype | _ASCE_TABLE_LENGTH |\r\n_ASCE_USER_BITS | __pa(table);\r\ngmap->asce_end = limit;\r\ndown_write(&mm->mmap_sem);\r\nlist_add(&gmap->list, &mm->context.gmap_list);\r\nup_write(&mm->mmap_sem);\r\nreturn gmap;\r\nout_free:\r\nkfree(gmap);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic void gmap_flush_tlb(struct gmap *gmap)\r\n{\r\nif (MACHINE_HAS_IDTE)\r\n__tlb_flush_asce(gmap->mm, gmap->asce);\r\nelse\r\n__tlb_flush_global();\r\n}\r\nstatic void gmap_radix_tree_free(struct radix_tree_root *root)\r\n{\r\nstruct radix_tree_iter iter;\r\nunsigned long indices[16];\r\nunsigned long index;\r\nvoid **slot;\r\nint i, nr;\r\nindex = 0;\r\ndo {\r\nnr = 0;\r\nradix_tree_for_each_slot(slot, root, &iter, index) {\r\nindices[nr] = iter.index;\r\nif (++nr == 16)\r\nbreak;\r\n}\r\nfor (i = 0; i < nr; i++) {\r\nindex = indices[i];\r\nradix_tree_delete(root, index);\r\n}\r\n} while (nr > 0);\r\n}\r\nvoid gmap_free(struct gmap *gmap)\r\n{\r\nstruct page *page, *next;\r\nif (MACHINE_HAS_IDTE)\r\n__tlb_flush_asce(gmap->mm, gmap->asce);\r\nelse\r\n__tlb_flush_global();\r\nlist_for_each_entry_safe(page, next, &gmap->crst_list, lru)\r\n__free_pages(page, 2);\r\ngmap_radix_tree_free(&gmap->guest_to_host);\r\ngmap_radix_tree_free(&gmap->host_to_guest);\r\ndown_write(&gmap->mm->mmap_sem);\r\nlist_del(&gmap->list);\r\nup_write(&gmap->mm->mmap_sem);\r\nkfree(gmap);\r\n}\r\nvoid gmap_enable(struct gmap *gmap)\r\n{\r\nS390_lowcore.gmap = (unsigned long) gmap;\r\n}\r\nvoid gmap_disable(struct gmap *gmap)\r\n{\r\nS390_lowcore.gmap = 0UL;\r\n}\r\nstatic int gmap_alloc_table(struct gmap *gmap, unsigned long *table,\r\nunsigned long init, unsigned long gaddr)\r\n{\r\nstruct page *page;\r\nunsigned long *new;\r\npage = alloc_pages(GFP_KERNEL, 2);\r\nif (!page)\r\nreturn -ENOMEM;\r\nnew = (unsigned long *) page_to_phys(page);\r\ncrst_table_init(new, init);\r\nspin_lock(&gmap->mm->page_table_lock);\r\nif (*table & _REGION_ENTRY_INVALID) {\r\nlist_add(&page->lru, &gmap->crst_list);\r\n*table = (unsigned long) new | _REGION_ENTRY_LENGTH |\r\n(*table & _REGION_ENTRY_TYPE_MASK);\r\npage->index = gaddr;\r\npage = NULL;\r\n}\r\nspin_unlock(&gmap->mm->page_table_lock);\r\nif (page)\r\n__free_pages(page, 2);\r\nreturn 0;\r\n}\r\nstatic unsigned long __gmap_segment_gaddr(unsigned long *entry)\r\n{\r\nstruct page *page;\r\nunsigned long offset, mask;\r\noffset = (unsigned long) entry / sizeof(unsigned long);\r\noffset = (offset & (PTRS_PER_PMD - 1)) * PMD_SIZE;\r\nmask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);\r\npage = virt_to_page((void *)((unsigned long) entry & mask));\r\nreturn page->index + offset;\r\n}\r\nstatic int __gmap_unlink_by_vmaddr(struct gmap *gmap, unsigned long vmaddr)\r\n{\r\nunsigned long *entry;\r\nint flush = 0;\r\nspin_lock(&gmap->guest_table_lock);\r\nentry = radix_tree_delete(&gmap->host_to_guest, vmaddr >> PMD_SHIFT);\r\nif (entry) {\r\nflush = (*entry != _SEGMENT_ENTRY_INVALID);\r\n*entry = _SEGMENT_ENTRY_INVALID;\r\n}\r\nspin_unlock(&gmap->guest_table_lock);\r\nreturn flush;\r\n}\r\nstatic int __gmap_unmap_by_gaddr(struct gmap *gmap, unsigned long gaddr)\r\n{\r\nunsigned long vmaddr;\r\nvmaddr = (unsigned long) radix_tree_delete(&gmap->guest_to_host,\r\ngaddr >> PMD_SHIFT);\r\nreturn vmaddr ? __gmap_unlink_by_vmaddr(gmap, vmaddr) : 0;\r\n}\r\nint gmap_unmap_segment(struct gmap *gmap, unsigned long to, unsigned long len)\r\n{\r\nunsigned long off;\r\nint flush;\r\nif ((to | len) & (PMD_SIZE - 1))\r\nreturn -EINVAL;\r\nif (len == 0 || to + len < to)\r\nreturn -EINVAL;\r\nflush = 0;\r\ndown_write(&gmap->mm->mmap_sem);\r\nfor (off = 0; off < len; off += PMD_SIZE)\r\nflush |= __gmap_unmap_by_gaddr(gmap, to + off);\r\nup_write(&gmap->mm->mmap_sem);\r\nif (flush)\r\ngmap_flush_tlb(gmap);\r\nreturn 0;\r\n}\r\nint gmap_map_segment(struct gmap *gmap, unsigned long from,\r\nunsigned long to, unsigned long len)\r\n{\r\nunsigned long off;\r\nint flush;\r\nif ((from | to | len) & (PMD_SIZE - 1))\r\nreturn -EINVAL;\r\nif (len == 0 || from + len < from || to + len < to ||\r\nfrom + len - 1 > TASK_MAX_SIZE || to + len - 1 > gmap->asce_end)\r\nreturn -EINVAL;\r\nflush = 0;\r\ndown_write(&gmap->mm->mmap_sem);\r\nfor (off = 0; off < len; off += PMD_SIZE) {\r\nflush |= __gmap_unmap_by_gaddr(gmap, to + off);\r\nif (radix_tree_insert(&gmap->guest_to_host,\r\n(to + off) >> PMD_SHIFT,\r\n(void *) from + off))\r\nbreak;\r\n}\r\nup_write(&gmap->mm->mmap_sem);\r\nif (flush)\r\ngmap_flush_tlb(gmap);\r\nif (off >= len)\r\nreturn 0;\r\ngmap_unmap_segment(gmap, to, len);\r\nreturn -ENOMEM;\r\n}\r\nunsigned long __gmap_translate(struct gmap *gmap, unsigned long gaddr)\r\n{\r\nunsigned long vmaddr;\r\nvmaddr = (unsigned long)\r\nradix_tree_lookup(&gmap->guest_to_host, gaddr >> PMD_SHIFT);\r\nreturn vmaddr ? (vmaddr | (gaddr & ~PMD_MASK)) : -EFAULT;\r\n}\r\nunsigned long gmap_translate(struct gmap *gmap, unsigned long gaddr)\r\n{\r\nunsigned long rc;\r\ndown_read(&gmap->mm->mmap_sem);\r\nrc = __gmap_translate(gmap, gaddr);\r\nup_read(&gmap->mm->mmap_sem);\r\nreturn rc;\r\n}\r\nvoid gmap_unlink(struct mm_struct *mm, unsigned long *table,\r\nunsigned long vmaddr)\r\n{\r\nstruct gmap *gmap;\r\nint flush;\r\nlist_for_each_entry(gmap, &mm->context.gmap_list, list) {\r\nflush = __gmap_unlink_by_vmaddr(gmap, vmaddr);\r\nif (flush)\r\ngmap_flush_tlb(gmap);\r\n}\r\n}\r\nint __gmap_link(struct gmap *gmap, unsigned long gaddr, unsigned long vmaddr)\r\n{\r\nstruct mm_struct *mm;\r\nunsigned long *table;\r\nspinlock_t *ptl;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\nint rc;\r\ntable = gmap->table;\r\nif ((gmap->asce & _ASCE_TYPE_MASK) >= _ASCE_TYPE_REGION1) {\r\ntable += (gaddr >> 53) & 0x7ff;\r\nif ((*table & _REGION_ENTRY_INVALID) &&\r\ngmap_alloc_table(gmap, table, _REGION2_ENTRY_EMPTY,\r\ngaddr & 0xffe0000000000000UL))\r\nreturn -ENOMEM;\r\ntable = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);\r\n}\r\nif ((gmap->asce & _ASCE_TYPE_MASK) >= _ASCE_TYPE_REGION2) {\r\ntable += (gaddr >> 42) & 0x7ff;\r\nif ((*table & _REGION_ENTRY_INVALID) &&\r\ngmap_alloc_table(gmap, table, _REGION3_ENTRY_EMPTY,\r\ngaddr & 0xfffffc0000000000UL))\r\nreturn -ENOMEM;\r\ntable = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);\r\n}\r\nif ((gmap->asce & _ASCE_TYPE_MASK) >= _ASCE_TYPE_REGION3) {\r\ntable += (gaddr >> 31) & 0x7ff;\r\nif ((*table & _REGION_ENTRY_INVALID) &&\r\ngmap_alloc_table(gmap, table, _SEGMENT_ENTRY_EMPTY,\r\ngaddr & 0xffffffff80000000UL))\r\nreturn -ENOMEM;\r\ntable = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);\r\n}\r\ntable += (gaddr >> 20) & 0x7ff;\r\nmm = gmap->mm;\r\npgd = pgd_offset(mm, vmaddr);\r\nVM_BUG_ON(pgd_none(*pgd));\r\npud = pud_offset(pgd, vmaddr);\r\nVM_BUG_ON(pud_none(*pud));\r\npmd = pmd_offset(pud, vmaddr);\r\nVM_BUG_ON(pmd_none(*pmd));\r\nif (pmd_large(*pmd))\r\nreturn -EFAULT;\r\nrc = radix_tree_preload(GFP_KERNEL);\r\nif (rc)\r\nreturn rc;\r\nptl = pmd_lock(mm, pmd);\r\nspin_lock(&gmap->guest_table_lock);\r\nif (*table == _SEGMENT_ENTRY_INVALID) {\r\nrc = radix_tree_insert(&gmap->host_to_guest,\r\nvmaddr >> PMD_SHIFT, table);\r\nif (!rc)\r\n*table = pmd_val(*pmd);\r\n} else\r\nrc = 0;\r\nspin_unlock(&gmap->guest_table_lock);\r\nspin_unlock(ptl);\r\nradix_tree_preload_end();\r\nreturn rc;\r\n}\r\nint gmap_fault(struct gmap *gmap, unsigned long gaddr,\r\nunsigned int fault_flags)\r\n{\r\nunsigned long vmaddr;\r\nint rc;\r\nbool unlocked;\r\ndown_read(&gmap->mm->mmap_sem);\r\nretry:\r\nunlocked = false;\r\nvmaddr = __gmap_translate(gmap, gaddr);\r\nif (IS_ERR_VALUE(vmaddr)) {\r\nrc = vmaddr;\r\ngoto out_up;\r\n}\r\nif (fixup_user_fault(current, gmap->mm, vmaddr, fault_flags,\r\n&unlocked)) {\r\nrc = -EFAULT;\r\ngoto out_up;\r\n}\r\nif (unlocked)\r\ngoto retry;\r\nrc = __gmap_link(gmap, gaddr, vmaddr);\r\nout_up:\r\nup_read(&gmap->mm->mmap_sem);\r\nreturn rc;\r\n}\r\nvoid __gmap_zap(struct gmap *gmap, unsigned long gaddr)\r\n{\r\nunsigned long vmaddr;\r\nspinlock_t *ptl;\r\npte_t *ptep;\r\nvmaddr = (unsigned long) radix_tree_lookup(&gmap->guest_to_host,\r\ngaddr >> PMD_SHIFT);\r\nif (vmaddr) {\r\nvmaddr |= gaddr & ~PMD_MASK;\r\nptep = get_locked_pte(gmap->mm, vmaddr, &ptl);\r\nif (likely(ptep))\r\nptep_zap_unused(gmap->mm, vmaddr, ptep, 0);\r\npte_unmap_unlock(ptep, ptl);\r\n}\r\n}\r\nvoid gmap_discard(struct gmap *gmap, unsigned long from, unsigned long to)\r\n{\r\nunsigned long gaddr, vmaddr, size;\r\nstruct vm_area_struct *vma;\r\ndown_read(&gmap->mm->mmap_sem);\r\nfor (gaddr = from; gaddr < to;\r\ngaddr = (gaddr + PMD_SIZE) & PMD_MASK) {\r\nvmaddr = (unsigned long)\r\nradix_tree_lookup(&gmap->guest_to_host,\r\ngaddr >> PMD_SHIFT);\r\nif (!vmaddr)\r\ncontinue;\r\nvmaddr |= gaddr & ~PMD_MASK;\r\nvma = find_vma(gmap->mm, vmaddr);\r\nsize = min(to - gaddr, PMD_SIZE - (gaddr & ~PMD_MASK));\r\nzap_page_range(vma, vmaddr, size, NULL);\r\n}\r\nup_read(&gmap->mm->mmap_sem);\r\n}\r\nvoid gmap_register_ipte_notifier(struct gmap_notifier *nb)\r\n{\r\nspin_lock(&gmap_notifier_lock);\r\nlist_add(&nb->list, &gmap_notifier_list);\r\nspin_unlock(&gmap_notifier_lock);\r\n}\r\nvoid gmap_unregister_ipte_notifier(struct gmap_notifier *nb)\r\n{\r\nspin_lock(&gmap_notifier_lock);\r\nlist_del_init(&nb->list);\r\nspin_unlock(&gmap_notifier_lock);\r\n}\r\nint gmap_ipte_notify(struct gmap *gmap, unsigned long gaddr, unsigned long len)\r\n{\r\nunsigned long addr;\r\nspinlock_t *ptl;\r\npte_t *ptep;\r\nbool unlocked;\r\nint rc = 0;\r\nif ((gaddr & ~PAGE_MASK) || (len & ~PAGE_MASK))\r\nreturn -EINVAL;\r\ndown_read(&gmap->mm->mmap_sem);\r\nwhile (len) {\r\nunlocked = false;\r\naddr = __gmap_translate(gmap, gaddr);\r\nif (IS_ERR_VALUE(addr)) {\r\nrc = addr;\r\nbreak;\r\n}\r\nif (fixup_user_fault(current, gmap->mm, addr, FAULT_FLAG_WRITE,\r\n&unlocked)) {\r\nrc = -EFAULT;\r\nbreak;\r\n}\r\nif (unlocked)\r\ncontinue;\r\nrc = __gmap_link(gmap, gaddr, addr);\r\nif (rc)\r\nbreak;\r\nptep = get_locked_pte(gmap->mm, addr, &ptl);\r\nVM_BUG_ON(!ptep);\r\nif ((pte_val(*ptep) & (_PAGE_INVALID | _PAGE_PROTECT)) == 0) {\r\nptep_set_notify(gmap->mm, addr, ptep);\r\ngaddr += PAGE_SIZE;\r\nlen -= PAGE_SIZE;\r\n}\r\npte_unmap_unlock(ptep, ptl);\r\n}\r\nup_read(&gmap->mm->mmap_sem);\r\nreturn rc;\r\n}\r\nvoid ptep_notify(struct mm_struct *mm, unsigned long vmaddr, pte_t *pte)\r\n{\r\nunsigned long offset, gaddr;\r\nunsigned long *table;\r\nstruct gmap_notifier *nb;\r\nstruct gmap *gmap;\r\noffset = ((unsigned long) pte) & (255 * sizeof(pte_t));\r\noffset = offset * (4096 / sizeof(pte_t));\r\nspin_lock(&gmap_notifier_lock);\r\nlist_for_each_entry(gmap, &mm->context.gmap_list, list) {\r\ntable = radix_tree_lookup(&gmap->host_to_guest,\r\nvmaddr >> PMD_SHIFT);\r\nif (!table)\r\ncontinue;\r\ngaddr = __gmap_segment_gaddr(table) + offset;\r\nlist_for_each_entry(nb, &gmap_notifier_list, list)\r\nnb->notifier_call(gmap, gaddr);\r\n}\r\nspin_unlock(&gmap_notifier_lock);\r\n}\r\nstatic inline void thp_split_mm(struct mm_struct *mm)\r\n{\r\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\r\nstruct vm_area_struct *vma;\r\nunsigned long addr;\r\nfor (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {\r\nfor (addr = vma->vm_start;\r\naddr < vma->vm_end;\r\naddr += PAGE_SIZE)\r\nfollow_page(vma, addr, FOLL_SPLIT);\r\nvma->vm_flags &= ~VM_HUGEPAGE;\r\nvma->vm_flags |= VM_NOHUGEPAGE;\r\n}\r\nmm->def_flags |= VM_NOHUGEPAGE;\r\n#endif\r\n}\r\nint s390_enable_sie(void)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nif (mm_has_pgste(mm))\r\nreturn 0;\r\nif (!mm_alloc_pgste(mm))\r\nreturn -EINVAL;\r\ndown_write(&mm->mmap_sem);\r\nmm->context.has_pgste = 1;\r\nthp_split_mm(mm);\r\nup_write(&mm->mmap_sem);\r\nreturn 0;\r\n}\r\nstatic int __s390_enable_skey(pte_t *pte, unsigned long addr,\r\nunsigned long next, struct mm_walk *walk)\r\n{\r\nif (is_zero_pfn(pte_pfn(*pte)))\r\nptep_xchg_direct(walk->mm, addr, pte, __pte(_PAGE_INVALID));\r\nptep_zap_key(walk->mm, addr, pte);\r\nreturn 0;\r\n}\r\nint s390_enable_skey(void)\r\n{\r\nstruct mm_walk walk = { .pte_entry = __s390_enable_skey };\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nint rc = 0;\r\ndown_write(&mm->mmap_sem);\r\nif (mm_use_skey(mm))\r\ngoto out_up;\r\nmm->context.use_skey = 1;\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next) {\r\nif (ksm_madvise(vma, vma->vm_start, vma->vm_end,\r\nMADV_UNMERGEABLE, &vma->vm_flags)) {\r\nmm->context.use_skey = 0;\r\nrc = -ENOMEM;\r\ngoto out_up;\r\n}\r\n}\r\nmm->def_flags &= ~VM_MERGEABLE;\r\nwalk.mm = mm;\r\nwalk_page_range(0, TASK_SIZE, &walk);\r\nout_up:\r\nup_write(&mm->mmap_sem);\r\nreturn rc;\r\n}\r\nstatic int __s390_reset_cmma(pte_t *pte, unsigned long addr,\r\nunsigned long next, struct mm_walk *walk)\r\n{\r\nptep_zap_unused(walk->mm, addr, pte, 1);\r\nreturn 0;\r\n}\r\nvoid s390_reset_cmma(struct mm_struct *mm)\r\n{\r\nstruct mm_walk walk = { .pte_entry = __s390_reset_cmma };\r\ndown_write(&mm->mmap_sem);\r\nwalk.mm = mm;\r\nwalk_page_range(0, TASK_SIZE, &walk);\r\nup_write(&mm->mmap_sem);\r\n}
