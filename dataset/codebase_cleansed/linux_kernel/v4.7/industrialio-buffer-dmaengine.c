static struct dmaengine_buffer *iio_buffer_to_dmaengine_buffer(\r\nstruct iio_buffer *buffer)\r\n{\r\nreturn container_of(buffer, struct dmaengine_buffer, queue.buffer);\r\n}\r\nstatic void iio_dmaengine_buffer_block_done(void *data)\r\n{\r\nstruct iio_dma_buffer_block *block = data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&block->queue->list_lock, flags);\r\nlist_del(&block->head);\r\nspin_unlock_irqrestore(&block->queue->list_lock, flags);\r\niio_dma_buffer_block_done(block);\r\n}\r\nstatic int iio_dmaengine_buffer_submit_block(struct iio_dma_buffer_queue *queue,\r\nstruct iio_dma_buffer_block *block)\r\n{\r\nstruct dmaengine_buffer *dmaengine_buffer =\r\niio_buffer_to_dmaengine_buffer(&queue->buffer);\r\nstruct dma_async_tx_descriptor *desc;\r\ndma_cookie_t cookie;\r\nblock->bytes_used = min(block->size, dmaengine_buffer->max_size);\r\nblock->bytes_used = rounddown(block->bytes_used,\r\ndmaengine_buffer->align);\r\ndesc = dmaengine_prep_slave_single(dmaengine_buffer->chan,\r\nblock->phys_addr, block->bytes_used, DMA_DEV_TO_MEM,\r\nDMA_PREP_INTERRUPT);\r\nif (!desc)\r\nreturn -ENOMEM;\r\ndesc->callback = iio_dmaengine_buffer_block_done;\r\ndesc->callback_param = block;\r\ncookie = dmaengine_submit(desc);\r\nif (dma_submit_error(cookie))\r\nreturn dma_submit_error(cookie);\r\nspin_lock_irq(&dmaengine_buffer->queue.list_lock);\r\nlist_add_tail(&block->head, &dmaengine_buffer->active);\r\nspin_unlock_irq(&dmaengine_buffer->queue.list_lock);\r\ndma_async_issue_pending(dmaengine_buffer->chan);\r\nreturn 0;\r\n}\r\nstatic void iio_dmaengine_buffer_abort(struct iio_dma_buffer_queue *queue)\r\n{\r\nstruct dmaengine_buffer *dmaengine_buffer =\r\niio_buffer_to_dmaengine_buffer(&queue->buffer);\r\ndmaengine_terminate_sync(dmaengine_buffer->chan);\r\niio_dma_buffer_block_list_abort(queue, &dmaengine_buffer->active);\r\n}\r\nstatic void iio_dmaengine_buffer_release(struct iio_buffer *buf)\r\n{\r\nstruct dmaengine_buffer *dmaengine_buffer =\r\niio_buffer_to_dmaengine_buffer(buf);\r\niio_dma_buffer_release(&dmaengine_buffer->queue);\r\nkfree(dmaengine_buffer);\r\n}\r\nstruct iio_buffer *iio_dmaengine_buffer_alloc(struct device *dev,\r\nconst char *channel)\r\n{\r\nstruct dmaengine_buffer *dmaengine_buffer;\r\nunsigned int width, src_width, dest_width;\r\nstruct dma_slave_caps caps;\r\nstruct dma_chan *chan;\r\nint ret;\r\ndmaengine_buffer = kzalloc(sizeof(*dmaengine_buffer), GFP_KERNEL);\r\nif (!dmaengine_buffer)\r\nreturn ERR_PTR(-ENOMEM);\r\nchan = dma_request_slave_channel_reason(dev, channel);\r\nif (IS_ERR(chan)) {\r\nret = PTR_ERR(chan);\r\ngoto err_free;\r\n}\r\nret = dma_get_slave_caps(chan, &caps);\r\nif (ret < 0)\r\ngoto err_free;\r\nif (caps.src_addr_widths)\r\nsrc_width = __ffs(caps.src_addr_widths);\r\nelse\r\nsrc_width = 1;\r\nif (caps.dst_addr_widths)\r\ndest_width = __ffs(caps.dst_addr_widths);\r\nelse\r\ndest_width = 1;\r\nwidth = max(src_width, dest_width);\r\nINIT_LIST_HEAD(&dmaengine_buffer->active);\r\ndmaengine_buffer->chan = chan;\r\ndmaengine_buffer->align = width;\r\ndmaengine_buffer->max_size = dma_get_max_seg_size(chan->device->dev);\r\niio_dma_buffer_init(&dmaengine_buffer->queue, chan->device->dev,\r\n&iio_dmaengine_default_ops);\r\ndmaengine_buffer->queue.buffer.access = &iio_dmaengine_buffer_ops;\r\nreturn &dmaengine_buffer->queue.buffer;\r\nerr_free:\r\nkfree(dmaengine_buffer);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid iio_dmaengine_buffer_free(struct iio_buffer *buffer)\r\n{\r\nstruct dmaengine_buffer *dmaengine_buffer =\r\niio_buffer_to_dmaengine_buffer(buffer);\r\niio_dma_buffer_exit(&dmaengine_buffer->queue);\r\ndma_release_channel(dmaengine_buffer->chan);\r\niio_buffer_put(buffer);\r\n}
