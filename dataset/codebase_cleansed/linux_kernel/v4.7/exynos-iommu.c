static u32 lv1ent_offset(sysmmu_iova_t iova)\r\n{\r\nreturn iova >> SECT_ORDER;\r\n}\r\nstatic u32 lv2ent_offset(sysmmu_iova_t iova)\r\n{\r\nreturn (iova >> SPAGE_ORDER) & (NUM_LV2ENTRIES - 1);\r\n}\r\nstatic sysmmu_pte_t *section_entry(sysmmu_pte_t *pgtable, sysmmu_iova_t iova)\r\n{\r\nreturn pgtable + lv1ent_offset(iova);\r\n}\r\nstatic sysmmu_pte_t *page_entry(sysmmu_pte_t *sent, sysmmu_iova_t iova)\r\n{\r\nreturn (sysmmu_pte_t *)phys_to_virt(\r\nlv2table_base(sent)) + lv2ent_offset(iova);\r\n}\r\nstatic struct exynos_iommu_domain *to_exynos_domain(struct iommu_domain *dom)\r\n{\r\nreturn container_of(dom, struct exynos_iommu_domain, domain);\r\n}\r\nstatic bool set_sysmmu_active(struct sysmmu_drvdata *data)\r\n{\r\nreturn ++data->activations == 1;\r\n}\r\nstatic bool set_sysmmu_inactive(struct sysmmu_drvdata *data)\r\n{\r\nBUG_ON(data->activations < 1);\r\nreturn --data->activations == 0;\r\n}\r\nstatic bool is_sysmmu_active(struct sysmmu_drvdata *data)\r\n{\r\nreturn data->activations > 0;\r\n}\r\nstatic void sysmmu_unblock(struct sysmmu_drvdata *data)\r\n{\r\nwritel(CTRL_ENABLE, data->sfrbase + REG_MMU_CTRL);\r\n}\r\nstatic bool sysmmu_block(struct sysmmu_drvdata *data)\r\n{\r\nint i = 120;\r\nwritel(CTRL_BLOCK, data->sfrbase + REG_MMU_CTRL);\r\nwhile ((i > 0) && !(readl(data->sfrbase + REG_MMU_STATUS) & 1))\r\n--i;\r\nif (!(readl(data->sfrbase + REG_MMU_STATUS) & 1)) {\r\nsysmmu_unblock(data);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic void __sysmmu_tlb_invalidate(struct sysmmu_drvdata *data)\r\n{\r\nif (MMU_MAJ_VER(data->version) < 5)\r\nwritel(0x1, data->sfrbase + REG_MMU_FLUSH);\r\nelse\r\nwritel(0x1, data->sfrbase + REG_V5_MMU_FLUSH_ALL);\r\n}\r\nstatic void __sysmmu_tlb_invalidate_entry(struct sysmmu_drvdata *data,\r\nsysmmu_iova_t iova, unsigned int num_inv)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < num_inv; i++) {\r\nif (MMU_MAJ_VER(data->version) < 5)\r\nwritel((iova & SPAGE_MASK) | 1,\r\ndata->sfrbase + REG_MMU_FLUSH_ENTRY);\r\nelse\r\nwritel((iova & SPAGE_MASK) | 1,\r\ndata->sfrbase + REG_V5_MMU_FLUSH_ENTRY);\r\niova += SPAGE_SIZE;\r\n}\r\n}\r\nstatic void __sysmmu_set_ptbase(struct sysmmu_drvdata *data, phys_addr_t pgd)\r\n{\r\nif (MMU_MAJ_VER(data->version) < 5)\r\nwritel(pgd, data->sfrbase + REG_PT_BASE_ADDR);\r\nelse\r\nwritel(pgd >> PAGE_SHIFT,\r\ndata->sfrbase + REG_V5_PT_BASE_PFN);\r\n__sysmmu_tlb_invalidate(data);\r\n}\r\nstatic void __sysmmu_get_version(struct sysmmu_drvdata *data)\r\n{\r\nu32 ver;\r\nclk_enable(data->clk_master);\r\nclk_enable(data->clk);\r\nclk_enable(data->pclk);\r\nclk_enable(data->aclk);\r\nver = readl(data->sfrbase + REG_MMU_VERSION);\r\nif (ver == 0x80000001u)\r\ndata->version = MAKE_MMU_VER(1, 0);\r\nelse\r\ndata->version = MMU_RAW_VER(ver);\r\ndev_dbg(data->sysmmu, "hardware version: %d.%d\n",\r\nMMU_MAJ_VER(data->version), MMU_MIN_VER(data->version));\r\nclk_disable(data->aclk);\r\nclk_disable(data->pclk);\r\nclk_disable(data->clk);\r\nclk_disable(data->clk_master);\r\n}\r\nstatic void show_fault_information(struct sysmmu_drvdata *data,\r\nconst struct sysmmu_fault_info *finfo,\r\nsysmmu_iova_t fault_addr)\r\n{\r\nsysmmu_pte_t *ent;\r\ndev_err(data->sysmmu, "%s FAULT occurred at %#x (page table base: %pa)\n",\r\nfinfo->name, fault_addr, &data->pgtable);\r\nent = section_entry(phys_to_virt(data->pgtable), fault_addr);\r\ndev_err(data->sysmmu, "\tLv1 entry: %#x\n", *ent);\r\nif (lv1ent_page(ent)) {\r\nent = page_entry(ent, fault_addr);\r\ndev_err(data->sysmmu, "\t Lv2 entry: %#x\n", *ent);\r\n}\r\n}\r\nstatic irqreturn_t exynos_sysmmu_irq(int irq, void *dev_id)\r\n{\r\nstruct sysmmu_drvdata *data = dev_id;\r\nconst struct sysmmu_fault_info *finfo;\r\nunsigned int i, n, itype;\r\nsysmmu_iova_t fault_addr = -1;\r\nunsigned short reg_status, reg_clear;\r\nint ret = -ENOSYS;\r\nWARN_ON(!is_sysmmu_active(data));\r\nif (MMU_MAJ_VER(data->version) < 5) {\r\nreg_status = REG_INT_STATUS;\r\nreg_clear = REG_INT_CLEAR;\r\nfinfo = sysmmu_faults;\r\nn = ARRAY_SIZE(sysmmu_faults);\r\n} else {\r\nreg_status = REG_V5_INT_STATUS;\r\nreg_clear = REG_V5_INT_CLEAR;\r\nfinfo = sysmmu_v5_faults;\r\nn = ARRAY_SIZE(sysmmu_v5_faults);\r\n}\r\nspin_lock(&data->lock);\r\nclk_enable(data->clk_master);\r\nitype = __ffs(readl(data->sfrbase + reg_status));\r\nfor (i = 0; i < n; i++, finfo++)\r\nif (finfo->bit == itype)\r\nbreak;\r\nBUG_ON(i == n);\r\nfault_addr = readl(data->sfrbase + finfo->addr_reg);\r\nshow_fault_information(data, finfo, fault_addr);\r\nif (data->domain)\r\nret = report_iommu_fault(&data->domain->domain,\r\ndata->master, fault_addr, finfo->type);\r\nBUG_ON(ret != 0);\r\nwritel(1 << itype, data->sfrbase + reg_clear);\r\nsysmmu_unblock(data);\r\nclk_disable(data->clk_master);\r\nspin_unlock(&data->lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void __sysmmu_disable_nocount(struct sysmmu_drvdata *data)\r\n{\r\nclk_enable(data->clk_master);\r\nwritel(CTRL_DISABLE, data->sfrbase + REG_MMU_CTRL);\r\nwritel(0, data->sfrbase + REG_MMU_CFG);\r\nclk_disable(data->aclk);\r\nclk_disable(data->pclk);\r\nclk_disable(data->clk);\r\nclk_disable(data->clk_master);\r\n}\r\nstatic bool __sysmmu_disable(struct sysmmu_drvdata *data)\r\n{\r\nbool disabled;\r\nunsigned long flags;\r\nspin_lock_irqsave(&data->lock, flags);\r\ndisabled = set_sysmmu_inactive(data);\r\nif (disabled) {\r\ndata->pgtable = 0;\r\ndata->domain = NULL;\r\n__sysmmu_disable_nocount(data);\r\ndev_dbg(data->sysmmu, "Disabled\n");\r\n} else {\r\ndev_dbg(data->sysmmu, "%d times left to disable\n",\r\ndata->activations);\r\n}\r\nspin_unlock_irqrestore(&data->lock, flags);\r\nreturn disabled;\r\n}\r\nstatic void __sysmmu_init_config(struct sysmmu_drvdata *data)\r\n{\r\nunsigned int cfg;\r\nif (data->version <= MAKE_MMU_VER(3, 1))\r\ncfg = CFG_LRU | CFG_QOS(15);\r\nelse if (data->version <= MAKE_MMU_VER(3, 2))\r\ncfg = CFG_LRU | CFG_QOS(15) | CFG_FLPDCACHE | CFG_SYSSEL;\r\nelse\r\ncfg = CFG_QOS(15) | CFG_FLPDCACHE | CFG_ACGEN;\r\nwritel(cfg, data->sfrbase + REG_MMU_CFG);\r\n}\r\nstatic void __sysmmu_enable_nocount(struct sysmmu_drvdata *data)\r\n{\r\nclk_enable(data->clk_master);\r\nclk_enable(data->clk);\r\nclk_enable(data->pclk);\r\nclk_enable(data->aclk);\r\nwritel(CTRL_BLOCK, data->sfrbase + REG_MMU_CTRL);\r\n__sysmmu_init_config(data);\r\n__sysmmu_set_ptbase(data, data->pgtable);\r\nwritel(CTRL_ENABLE, data->sfrbase + REG_MMU_CTRL);\r\nclk_disable(data->clk_master);\r\n}\r\nstatic int __sysmmu_enable(struct sysmmu_drvdata *data, phys_addr_t pgtable,\r\nstruct exynos_iommu_domain *domain)\r\n{\r\nint ret = 0;\r\nunsigned long flags;\r\nspin_lock_irqsave(&data->lock, flags);\r\nif (set_sysmmu_active(data)) {\r\ndata->pgtable = pgtable;\r\ndata->domain = domain;\r\n__sysmmu_enable_nocount(data);\r\ndev_dbg(data->sysmmu, "Enabled\n");\r\n} else {\r\nret = (pgtable == data->pgtable) ? 1 : -EBUSY;\r\ndev_dbg(data->sysmmu, "already enabled\n");\r\n}\r\nif (WARN_ON(ret < 0))\r\nset_sysmmu_inactive(data);\r\nspin_unlock_irqrestore(&data->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void sysmmu_tlb_invalidate_flpdcache(struct sysmmu_drvdata *data,\r\nsysmmu_iova_t iova)\r\n{\r\nunsigned long flags;\r\nclk_enable(data->clk_master);\r\nspin_lock_irqsave(&data->lock, flags);\r\nif (is_sysmmu_active(data)) {\r\nif (data->version >= MAKE_MMU_VER(3, 3))\r\n__sysmmu_tlb_invalidate_entry(data, iova, 1);\r\n}\r\nspin_unlock_irqrestore(&data->lock, flags);\r\nclk_disable(data->clk_master);\r\n}\r\nstatic void sysmmu_tlb_invalidate_entry(struct sysmmu_drvdata *data,\r\nsysmmu_iova_t iova, size_t size)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&data->lock, flags);\r\nif (is_sysmmu_active(data)) {\r\nunsigned int num_inv = 1;\r\nclk_enable(data->clk_master);\r\nif (MMU_MAJ_VER(data->version) == 2)\r\nnum_inv = min_t(unsigned int, size / PAGE_SIZE, 64);\r\nif (sysmmu_block(data)) {\r\n__sysmmu_tlb_invalidate_entry(data, iova, num_inv);\r\nsysmmu_unblock(data);\r\n}\r\nclk_disable(data->clk_master);\r\n} else {\r\ndev_dbg(data->master,\r\n"disabled. Skipping TLB invalidation @ %#x\n", iova);\r\n}\r\nspin_unlock_irqrestore(&data->lock, flags);\r\n}\r\nstatic int __init exynos_sysmmu_probe(struct platform_device *pdev)\r\n{\r\nint irq, ret;\r\nstruct device *dev = &pdev->dev;\r\nstruct sysmmu_drvdata *data;\r\nstruct resource *res;\r\ndata = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndata->sfrbase = devm_ioremap_resource(dev, res);\r\nif (IS_ERR(data->sfrbase))\r\nreturn PTR_ERR(data->sfrbase);\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq <= 0) {\r\ndev_err(dev, "Unable to find IRQ resource\n");\r\nreturn irq;\r\n}\r\nret = devm_request_irq(dev, irq, exynos_sysmmu_irq, 0,\r\ndev_name(dev), data);\r\nif (ret) {\r\ndev_err(dev, "Unabled to register handler of irq %d\n", irq);\r\nreturn ret;\r\n}\r\ndata->clk = devm_clk_get(dev, "sysmmu");\r\nif (!IS_ERR(data->clk)) {\r\nret = clk_prepare(data->clk);\r\nif (ret) {\r\ndev_err(dev, "Failed to prepare clk\n");\r\nreturn ret;\r\n}\r\n} else {\r\ndata->clk = NULL;\r\n}\r\ndata->aclk = devm_clk_get(dev, "aclk");\r\nif (!IS_ERR(data->aclk)) {\r\nret = clk_prepare(data->aclk);\r\nif (ret) {\r\ndev_err(dev, "Failed to prepare aclk\n");\r\nreturn ret;\r\n}\r\n} else {\r\ndata->aclk = NULL;\r\n}\r\ndata->pclk = devm_clk_get(dev, "pclk");\r\nif (!IS_ERR(data->pclk)) {\r\nret = clk_prepare(data->pclk);\r\nif (ret) {\r\ndev_err(dev, "Failed to prepare pclk\n");\r\nreturn ret;\r\n}\r\n} else {\r\ndata->pclk = NULL;\r\n}\r\nif (!data->clk && (!data->aclk || !data->pclk)) {\r\ndev_err(dev, "Failed to get device clock(s)!\n");\r\nreturn -ENOSYS;\r\n}\r\ndata->clk_master = devm_clk_get(dev, "master");\r\nif (!IS_ERR(data->clk_master)) {\r\nret = clk_prepare(data->clk_master);\r\nif (ret) {\r\ndev_err(dev, "Failed to prepare master's clk\n");\r\nreturn ret;\r\n}\r\n} else {\r\ndata->clk_master = NULL;\r\n}\r\ndata->sysmmu = dev;\r\nspin_lock_init(&data->lock);\r\nplatform_set_drvdata(pdev, data);\r\n__sysmmu_get_version(data);\r\nif (PG_ENT_SHIFT < 0) {\r\nif (MMU_MAJ_VER(data->version) < 5)\r\nPG_ENT_SHIFT = SYSMMU_PG_ENT_SHIFT;\r\nelse\r\nPG_ENT_SHIFT = SYSMMU_V5_PG_ENT_SHIFT;\r\n}\r\npm_runtime_enable(dev);\r\nreturn 0;\r\n}\r\nstatic int exynos_sysmmu_suspend(struct device *dev)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev);\r\ndev_dbg(dev, "suspend\n");\r\nif (is_sysmmu_active(data)) {\r\n__sysmmu_disable_nocount(data);\r\npm_runtime_put(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int exynos_sysmmu_resume(struct device *dev)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev);\r\ndev_dbg(dev, "resume\n");\r\nif (is_sysmmu_active(data)) {\r\npm_runtime_get_sync(dev);\r\n__sysmmu_enable_nocount(data);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void update_pte(sysmmu_pte_t *ent, sysmmu_pte_t val)\r\n{\r\ndma_sync_single_for_cpu(dma_dev, virt_to_phys(ent), sizeof(*ent),\r\nDMA_TO_DEVICE);\r\n*ent = val;\r\ndma_sync_single_for_device(dma_dev, virt_to_phys(ent), sizeof(*ent),\r\nDMA_TO_DEVICE);\r\n}\r\nstatic struct iommu_domain *exynos_iommu_domain_alloc(unsigned type)\r\n{\r\nstruct exynos_iommu_domain *domain;\r\ndma_addr_t handle;\r\nint i;\r\nBUG_ON(PG_ENT_SHIFT < 0 || !dma_dev);\r\ndomain = kzalloc(sizeof(*domain), GFP_KERNEL);\r\nif (!domain)\r\nreturn NULL;\r\nif (type == IOMMU_DOMAIN_DMA) {\r\nif (iommu_get_dma_cookie(&domain->domain) != 0)\r\ngoto err_pgtable;\r\n} else if (type != IOMMU_DOMAIN_UNMANAGED) {\r\ngoto err_pgtable;\r\n}\r\ndomain->pgtable = (sysmmu_pte_t *)__get_free_pages(GFP_KERNEL, 2);\r\nif (!domain->pgtable)\r\ngoto err_dma_cookie;\r\ndomain->lv2entcnt = (short *)__get_free_pages(GFP_KERNEL | __GFP_ZERO, 1);\r\nif (!domain->lv2entcnt)\r\ngoto err_counter;\r\nfor (i = 0; i < NUM_LV1ENTRIES; i += 8) {\r\ndomain->pgtable[i + 0] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 1] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 2] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 3] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 4] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 5] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 6] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 7] = ZERO_LV2LINK;\r\n}\r\nhandle = dma_map_single(dma_dev, domain->pgtable, LV1TABLE_SIZE,\r\nDMA_TO_DEVICE);\r\nBUG_ON(handle != virt_to_phys(domain->pgtable));\r\nspin_lock_init(&domain->lock);\r\nspin_lock_init(&domain->pgtablelock);\r\nINIT_LIST_HEAD(&domain->clients);\r\ndomain->domain.geometry.aperture_start = 0;\r\ndomain->domain.geometry.aperture_end = ~0UL;\r\ndomain->domain.geometry.force_aperture = true;\r\nreturn &domain->domain;\r\nerr_counter:\r\nfree_pages((unsigned long)domain->pgtable, 2);\r\nerr_dma_cookie:\r\nif (type == IOMMU_DOMAIN_DMA)\r\niommu_put_dma_cookie(&domain->domain);\r\nerr_pgtable:\r\nkfree(domain);\r\nreturn NULL;\r\n}\r\nstatic void exynos_iommu_domain_free(struct iommu_domain *iommu_domain)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nstruct sysmmu_drvdata *data, *next;\r\nunsigned long flags;\r\nint i;\r\nWARN_ON(!list_empty(&domain->clients));\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_for_each_entry_safe(data, next, &domain->clients, domain_node) {\r\nif (__sysmmu_disable(data))\r\ndata->master = NULL;\r\nlist_del_init(&data->domain_node);\r\n}\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nif (iommu_domain->type == IOMMU_DOMAIN_DMA)\r\niommu_put_dma_cookie(iommu_domain);\r\ndma_unmap_single(dma_dev, virt_to_phys(domain->pgtable), LV1TABLE_SIZE,\r\nDMA_TO_DEVICE);\r\nfor (i = 0; i < NUM_LV1ENTRIES; i++)\r\nif (lv1ent_page(domain->pgtable + i)) {\r\nphys_addr_t base = lv2table_base(domain->pgtable + i);\r\ndma_unmap_single(dma_dev, base, LV2TABLE_SIZE,\r\nDMA_TO_DEVICE);\r\nkmem_cache_free(lv2table_kmem_cache,\r\nphys_to_virt(base));\r\n}\r\nfree_pages((unsigned long)domain->pgtable, 2);\r\nfree_pages((unsigned long)domain->lv2entcnt, 1);\r\nkfree(domain);\r\n}\r\nstatic void exynos_iommu_detach_device(struct iommu_domain *iommu_domain,\r\nstruct device *dev)\r\n{\r\nstruct exynos_iommu_owner *owner = dev->archdata.iommu;\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nphys_addr_t pagetable = virt_to_phys(domain->pgtable);\r\nstruct sysmmu_drvdata *data, *next;\r\nunsigned long flags;\r\nbool found = false;\r\nif (!has_sysmmu(dev) || owner->domain != iommu_domain)\r\nreturn;\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_for_each_entry_safe(data, next, &domain->clients, domain_node) {\r\nif (data->master == dev) {\r\nif (__sysmmu_disable(data)) {\r\ndata->master = NULL;\r\nlist_del_init(&data->domain_node);\r\n}\r\npm_runtime_put(data->sysmmu);\r\nfound = true;\r\n}\r\n}\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nowner->domain = NULL;\r\nif (found)\r\ndev_dbg(dev, "%s: Detached IOMMU with pgtable %pa\n",\r\n__func__, &pagetable);\r\nelse\r\ndev_err(dev, "%s: No IOMMU is attached\n", __func__);\r\n}\r\nstatic int exynos_iommu_attach_device(struct iommu_domain *iommu_domain,\r\nstruct device *dev)\r\n{\r\nstruct exynos_iommu_owner *owner = dev->archdata.iommu;\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nstruct sysmmu_drvdata *data;\r\nphys_addr_t pagetable = virt_to_phys(domain->pgtable);\r\nunsigned long flags;\r\nint ret = -ENODEV;\r\nif (!has_sysmmu(dev))\r\nreturn -ENODEV;\r\nif (owner->domain)\r\nexynos_iommu_detach_device(owner->domain, dev);\r\nlist_for_each_entry(data, &owner->controllers, owner_node) {\r\npm_runtime_get_sync(data->sysmmu);\r\nret = __sysmmu_enable(data, pagetable, domain);\r\nif (ret >= 0) {\r\ndata->master = dev;\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_add_tail(&data->domain_node, &domain->clients);\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\n}\r\n}\r\nif (ret < 0) {\r\ndev_err(dev, "%s: Failed to attach IOMMU with pgtable %pa\n",\r\n__func__, &pagetable);\r\nreturn ret;\r\n}\r\nowner->domain = iommu_domain;\r\ndev_dbg(dev, "%s: Attached IOMMU with pgtable %pa %s\n",\r\n__func__, &pagetable, (ret == 0) ? "" : ", again");\r\nreturn ret;\r\n}\r\nstatic sysmmu_pte_t *alloc_lv2entry(struct exynos_iommu_domain *domain,\r\nsysmmu_pte_t *sent, sysmmu_iova_t iova, short *pgcounter)\r\n{\r\nif (lv1ent_section(sent)) {\r\nWARN(1, "Trying mapping on %#08x mapped with 1MiB page", iova);\r\nreturn ERR_PTR(-EADDRINUSE);\r\n}\r\nif (lv1ent_fault(sent)) {\r\nsysmmu_pte_t *pent;\r\nbool need_flush_flpd_cache = lv1ent_zero(sent);\r\npent = kmem_cache_zalloc(lv2table_kmem_cache, GFP_ATOMIC);\r\nBUG_ON((uintptr_t)pent & (LV2TABLE_SIZE - 1));\r\nif (!pent)\r\nreturn ERR_PTR(-ENOMEM);\r\nupdate_pte(sent, mk_lv1ent_page(virt_to_phys(pent)));\r\nkmemleak_ignore(pent);\r\n*pgcounter = NUM_LV2ENTRIES;\r\ndma_map_single(dma_dev, pent, LV2TABLE_SIZE, DMA_TO_DEVICE);\r\nif (need_flush_flpd_cache) {\r\nstruct sysmmu_drvdata *data;\r\nspin_lock(&domain->lock);\r\nlist_for_each_entry(data, &domain->clients, domain_node)\r\nsysmmu_tlb_invalidate_flpdcache(data, iova);\r\nspin_unlock(&domain->lock);\r\n}\r\n}\r\nreturn page_entry(sent, iova);\r\n}\r\nstatic int lv1set_section(struct exynos_iommu_domain *domain,\r\nsysmmu_pte_t *sent, sysmmu_iova_t iova,\r\nphys_addr_t paddr, short *pgcnt)\r\n{\r\nif (lv1ent_section(sent)) {\r\nWARN(1, "Trying mapping on 1MiB@%#08x that is mapped",\r\niova);\r\nreturn -EADDRINUSE;\r\n}\r\nif (lv1ent_page(sent)) {\r\nif (*pgcnt != NUM_LV2ENTRIES) {\r\nWARN(1, "Trying mapping on 1MiB@%#08x that is mapped",\r\niova);\r\nreturn -EADDRINUSE;\r\n}\r\nkmem_cache_free(lv2table_kmem_cache, page_entry(sent, 0));\r\n*pgcnt = 0;\r\n}\r\nupdate_pte(sent, mk_lv1ent_sect(paddr));\r\nspin_lock(&domain->lock);\r\nif (lv1ent_page_zero(sent)) {\r\nstruct sysmmu_drvdata *data;\r\nlist_for_each_entry(data, &domain->clients, domain_node)\r\nsysmmu_tlb_invalidate_flpdcache(data, iova);\r\n}\r\nspin_unlock(&domain->lock);\r\nreturn 0;\r\n}\r\nstatic int lv2set_page(sysmmu_pte_t *pent, phys_addr_t paddr, size_t size,\r\nshort *pgcnt)\r\n{\r\nif (size == SPAGE_SIZE) {\r\nif (WARN_ON(!lv2ent_fault(pent)))\r\nreturn -EADDRINUSE;\r\nupdate_pte(pent, mk_lv2ent_spage(paddr));\r\n*pgcnt -= 1;\r\n} else {\r\nint i;\r\ndma_addr_t pent_base = virt_to_phys(pent);\r\ndma_sync_single_for_cpu(dma_dev, pent_base,\r\nsizeof(*pent) * SPAGES_PER_LPAGE,\r\nDMA_TO_DEVICE);\r\nfor (i = 0; i < SPAGES_PER_LPAGE; i++, pent++) {\r\nif (WARN_ON(!lv2ent_fault(pent))) {\r\nif (i > 0)\r\nmemset(pent - i, 0, sizeof(*pent) * i);\r\nreturn -EADDRINUSE;\r\n}\r\n*pent = mk_lv2ent_lpage(paddr);\r\n}\r\ndma_sync_single_for_device(dma_dev, pent_base,\r\nsizeof(*pent) * SPAGES_PER_LPAGE,\r\nDMA_TO_DEVICE);\r\n*pgcnt -= SPAGES_PER_LPAGE;\r\n}\r\nreturn 0;\r\n}\r\nstatic int exynos_iommu_map(struct iommu_domain *iommu_domain,\r\nunsigned long l_iova, phys_addr_t paddr, size_t size,\r\nint prot)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nsysmmu_pte_t *entry;\r\nsysmmu_iova_t iova = (sysmmu_iova_t)l_iova;\r\nunsigned long flags;\r\nint ret = -ENOMEM;\r\nBUG_ON(domain->pgtable == NULL);\r\nspin_lock_irqsave(&domain->pgtablelock, flags);\r\nentry = section_entry(domain->pgtable, iova);\r\nif (size == SECT_SIZE) {\r\nret = lv1set_section(domain, entry, iova, paddr,\r\n&domain->lv2entcnt[lv1ent_offset(iova)]);\r\n} else {\r\nsysmmu_pte_t *pent;\r\npent = alloc_lv2entry(domain, entry, iova,\r\n&domain->lv2entcnt[lv1ent_offset(iova)]);\r\nif (IS_ERR(pent))\r\nret = PTR_ERR(pent);\r\nelse\r\nret = lv2set_page(pent, paddr, size,\r\n&domain->lv2entcnt[lv1ent_offset(iova)]);\r\n}\r\nif (ret)\r\npr_err("%s: Failed(%d) to map %#zx bytes @ %#x\n",\r\n__func__, ret, size, iova);\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\nreturn ret;\r\n}\r\nstatic void exynos_iommu_tlb_invalidate_entry(struct exynos_iommu_domain *domain,\r\nsysmmu_iova_t iova, size_t size)\r\n{\r\nstruct sysmmu_drvdata *data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_for_each_entry(data, &domain->clients, domain_node)\r\nsysmmu_tlb_invalidate_entry(data, iova, size);\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\n}\r\nstatic size_t exynos_iommu_unmap(struct iommu_domain *iommu_domain,\r\nunsigned long l_iova, size_t size)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nsysmmu_iova_t iova = (sysmmu_iova_t)l_iova;\r\nsysmmu_pte_t *ent;\r\nsize_t err_pgsize;\r\nunsigned long flags;\r\nBUG_ON(domain->pgtable == NULL);\r\nspin_lock_irqsave(&domain->pgtablelock, flags);\r\nent = section_entry(domain->pgtable, iova);\r\nif (lv1ent_section(ent)) {\r\nif (WARN_ON(size < SECT_SIZE)) {\r\nerr_pgsize = SECT_SIZE;\r\ngoto err;\r\n}\r\nupdate_pte(ent, ZERO_LV2LINK);\r\nsize = SECT_SIZE;\r\ngoto done;\r\n}\r\nif (unlikely(lv1ent_fault(ent))) {\r\nif (size > SECT_SIZE)\r\nsize = SECT_SIZE;\r\ngoto done;\r\n}\r\nent = page_entry(ent, iova);\r\nif (unlikely(lv2ent_fault(ent))) {\r\nsize = SPAGE_SIZE;\r\ngoto done;\r\n}\r\nif (lv2ent_small(ent)) {\r\nupdate_pte(ent, 0);\r\nsize = SPAGE_SIZE;\r\ndomain->lv2entcnt[lv1ent_offset(iova)] += 1;\r\ngoto done;\r\n}\r\nif (WARN_ON(size < LPAGE_SIZE)) {\r\nerr_pgsize = LPAGE_SIZE;\r\ngoto err;\r\n}\r\ndma_sync_single_for_cpu(dma_dev, virt_to_phys(ent),\r\nsizeof(*ent) * SPAGES_PER_LPAGE,\r\nDMA_TO_DEVICE);\r\nmemset(ent, 0, sizeof(*ent) * SPAGES_PER_LPAGE);\r\ndma_sync_single_for_device(dma_dev, virt_to_phys(ent),\r\nsizeof(*ent) * SPAGES_PER_LPAGE,\r\nDMA_TO_DEVICE);\r\nsize = LPAGE_SIZE;\r\ndomain->lv2entcnt[lv1ent_offset(iova)] += SPAGES_PER_LPAGE;\r\ndone:\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\nexynos_iommu_tlb_invalidate_entry(domain, iova, size);\r\nreturn size;\r\nerr:\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\npr_err("%s: Failed: size(%#zx) @ %#x is smaller than page size %#zx\n",\r\n__func__, size, iova, err_pgsize);\r\nreturn 0;\r\n}\r\nstatic phys_addr_t exynos_iommu_iova_to_phys(struct iommu_domain *iommu_domain,\r\ndma_addr_t iova)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nsysmmu_pte_t *entry;\r\nunsigned long flags;\r\nphys_addr_t phys = 0;\r\nspin_lock_irqsave(&domain->pgtablelock, flags);\r\nentry = section_entry(domain->pgtable, iova);\r\nif (lv1ent_section(entry)) {\r\nphys = section_phys(entry) + section_offs(iova);\r\n} else if (lv1ent_page(entry)) {\r\nentry = page_entry(entry, iova);\r\nif (lv2ent_large(entry))\r\nphys = lpage_phys(entry) + lpage_offs(iova);\r\nelse if (lv2ent_small(entry))\r\nphys = spage_phys(entry) + spage_offs(iova);\r\n}\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\nreturn phys;\r\n}\r\nstatic struct iommu_group *get_device_iommu_group(struct device *dev)\r\n{\r\nstruct iommu_group *group;\r\ngroup = iommu_group_get(dev);\r\nif (!group)\r\ngroup = iommu_group_alloc();\r\nreturn group;\r\n}\r\nstatic int exynos_iommu_add_device(struct device *dev)\r\n{\r\nstruct iommu_group *group;\r\nif (!has_sysmmu(dev))\r\nreturn -ENODEV;\r\ngroup = iommu_group_get_for_dev(dev);\r\nif (IS_ERR(group))\r\nreturn PTR_ERR(group);\r\niommu_group_put(group);\r\nreturn 0;\r\n}\r\nstatic void exynos_iommu_remove_device(struct device *dev)\r\n{\r\nif (!has_sysmmu(dev))\r\nreturn;\r\niommu_group_remove_device(dev);\r\n}\r\nstatic int exynos_iommu_of_xlate(struct device *dev,\r\nstruct of_phandle_args *spec)\r\n{\r\nstruct exynos_iommu_owner *owner = dev->archdata.iommu;\r\nstruct platform_device *sysmmu = of_find_device_by_node(spec->np);\r\nstruct sysmmu_drvdata *data;\r\nif (!sysmmu)\r\nreturn -ENODEV;\r\ndata = platform_get_drvdata(sysmmu);\r\nif (!data)\r\nreturn -ENODEV;\r\nif (!owner) {\r\nowner = kzalloc(sizeof(*owner), GFP_KERNEL);\r\nif (!owner)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&owner->controllers);\r\ndev->archdata.iommu = owner;\r\n}\r\nlist_add_tail(&data->owner_node, &owner->controllers);\r\nreturn 0;\r\n}\r\nstatic int __init exynos_iommu_init(void)\r\n{\r\nint ret;\r\nlv2table_kmem_cache = kmem_cache_create("exynos-iommu-lv2table",\r\nLV2TABLE_SIZE, LV2TABLE_SIZE, 0, NULL);\r\nif (!lv2table_kmem_cache) {\r\npr_err("%s: Failed to create kmem cache\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nret = platform_driver_register(&exynos_sysmmu_driver);\r\nif (ret) {\r\npr_err("%s: Failed to register driver\n", __func__);\r\ngoto err_reg_driver;\r\n}\r\nzero_lv2_table = kmem_cache_zalloc(lv2table_kmem_cache, GFP_KERNEL);\r\nif (zero_lv2_table == NULL) {\r\npr_err("%s: Failed to allocate zero level2 page table\n",\r\n__func__);\r\nret = -ENOMEM;\r\ngoto err_zero_lv2;\r\n}\r\nret = bus_set_iommu(&platform_bus_type, &exynos_iommu_ops);\r\nif (ret) {\r\npr_err("%s: Failed to register exynos-iommu driver.\n",\r\n__func__);\r\ngoto err_set_iommu;\r\n}\r\ninit_done = true;\r\nreturn 0;\r\nerr_set_iommu:\r\nkmem_cache_free(lv2table_kmem_cache, zero_lv2_table);\r\nerr_zero_lv2:\r\nplatform_driver_unregister(&exynos_sysmmu_driver);\r\nerr_reg_driver:\r\nkmem_cache_destroy(lv2table_kmem_cache);\r\nreturn ret;\r\n}\r\nstatic int __init exynos_iommu_of_setup(struct device_node *np)\r\n{\r\nstruct platform_device *pdev;\r\nif (!init_done)\r\nexynos_iommu_init();\r\npdev = of_platform_device_create(np, NULL, platform_bus_type.dev_root);\r\nif (IS_ERR(pdev))\r\nreturn PTR_ERR(pdev);\r\nif (!dma_dev)\r\ndma_dev = &pdev->dev;\r\nof_iommu_set_ops(np, &exynos_iommu_ops);\r\nreturn 0;\r\n}
