static inline void htab_elem_set_ptr(struct htab_elem *l, u32 key_size,\r\nvoid __percpu *pptr)\r\n{\r\n*(void __percpu **)(l->key + key_size) = pptr;\r\n}\r\nstatic inline void __percpu *htab_elem_get_ptr(struct htab_elem *l, u32 key_size)\r\n{\r\nreturn *(void __percpu **)(l->key + key_size);\r\n}\r\nstatic struct htab_elem *get_htab_elem(struct bpf_htab *htab, int i)\r\n{\r\nreturn (struct htab_elem *) (htab->elems + i * htab->elem_size);\r\n}\r\nstatic void htab_free_elems(struct bpf_htab *htab)\r\n{\r\nint i;\r\nif (htab->map.map_type != BPF_MAP_TYPE_PERCPU_HASH)\r\ngoto free_elems;\r\nfor (i = 0; i < htab->map.max_entries; i++) {\r\nvoid __percpu *pptr;\r\npptr = htab_elem_get_ptr(get_htab_elem(htab, i),\r\nhtab->map.key_size);\r\nfree_percpu(pptr);\r\n}\r\nfree_elems:\r\nvfree(htab->elems);\r\n}\r\nstatic int prealloc_elems_and_freelist(struct bpf_htab *htab)\r\n{\r\nint err = -ENOMEM, i;\r\nhtab->elems = vzalloc(htab->elem_size * htab->map.max_entries);\r\nif (!htab->elems)\r\nreturn -ENOMEM;\r\nif (htab->map.map_type != BPF_MAP_TYPE_PERCPU_HASH)\r\ngoto skip_percpu_elems;\r\nfor (i = 0; i < htab->map.max_entries; i++) {\r\nu32 size = round_up(htab->map.value_size, 8);\r\nvoid __percpu *pptr;\r\npptr = __alloc_percpu_gfp(size, 8, GFP_USER | __GFP_NOWARN);\r\nif (!pptr)\r\ngoto free_elems;\r\nhtab_elem_set_ptr(get_htab_elem(htab, i), htab->map.key_size,\r\npptr);\r\n}\r\nskip_percpu_elems:\r\nerr = pcpu_freelist_init(&htab->freelist);\r\nif (err)\r\ngoto free_elems;\r\npcpu_freelist_populate(&htab->freelist, htab->elems, htab->elem_size,\r\nhtab->map.max_entries);\r\nreturn 0;\r\nfree_elems:\r\nhtab_free_elems(htab);\r\nreturn err;\r\n}\r\nstatic struct bpf_map *htab_map_alloc(union bpf_attr *attr)\r\n{\r\nbool percpu = attr->map_type == BPF_MAP_TYPE_PERCPU_HASH;\r\nstruct bpf_htab *htab;\r\nint err, i;\r\nu64 cost;\r\nif (attr->map_flags & ~BPF_F_NO_PREALLOC)\r\nreturn ERR_PTR(-EINVAL);\r\nhtab = kzalloc(sizeof(*htab), GFP_USER);\r\nif (!htab)\r\nreturn ERR_PTR(-ENOMEM);\r\nhtab->map.map_type = attr->map_type;\r\nhtab->map.key_size = attr->key_size;\r\nhtab->map.value_size = attr->value_size;\r\nhtab->map.max_entries = attr->max_entries;\r\nhtab->map.map_flags = attr->map_flags;\r\nerr = -EINVAL;\r\nif (htab->map.max_entries == 0 || htab->map.key_size == 0 ||\r\nhtab->map.value_size == 0)\r\ngoto free_htab;\r\nhtab->n_buckets = roundup_pow_of_two(htab->map.max_entries);\r\nerr = -E2BIG;\r\nif (htab->map.key_size > MAX_BPF_STACK)\r\ngoto free_htab;\r\nif (htab->map.value_size >= (1 << (KMALLOC_SHIFT_MAX - 1)) -\r\nMAX_BPF_STACK - sizeof(struct htab_elem))\r\ngoto free_htab;\r\nif (percpu && round_up(htab->map.value_size, 8) > PCPU_MIN_UNIT_SIZE)\r\ngoto free_htab;\r\nhtab->elem_size = sizeof(struct htab_elem) +\r\nround_up(htab->map.key_size, 8);\r\nif (percpu)\r\nhtab->elem_size += sizeof(void *);\r\nelse\r\nhtab->elem_size += round_up(htab->map.value_size, 8);\r\nif (htab->n_buckets == 0 ||\r\nhtab->n_buckets > U32_MAX / sizeof(struct bucket))\r\ngoto free_htab;\r\ncost = (u64) htab->n_buckets * sizeof(struct bucket) +\r\n(u64) htab->elem_size * htab->map.max_entries;\r\nif (percpu)\r\ncost += (u64) round_up(htab->map.value_size, 8) *\r\nnum_possible_cpus() * htab->map.max_entries;\r\nif (cost >= U32_MAX - PAGE_SIZE)\r\ngoto free_htab;\r\nhtab->map.pages = round_up(cost, PAGE_SIZE) >> PAGE_SHIFT;\r\nerr = bpf_map_precharge_memlock(htab->map.pages);\r\nif (err)\r\ngoto free_htab;\r\nerr = -ENOMEM;\r\nhtab->buckets = kmalloc_array(htab->n_buckets, sizeof(struct bucket),\r\nGFP_USER | __GFP_NOWARN);\r\nif (!htab->buckets) {\r\nhtab->buckets = vmalloc(htab->n_buckets * sizeof(struct bucket));\r\nif (!htab->buckets)\r\ngoto free_htab;\r\n}\r\nfor (i = 0; i < htab->n_buckets; i++) {\r\nINIT_HLIST_HEAD(&htab->buckets[i].head);\r\nraw_spin_lock_init(&htab->buckets[i].lock);\r\n}\r\nif (!(attr->map_flags & BPF_F_NO_PREALLOC)) {\r\nerr = prealloc_elems_and_freelist(htab);\r\nif (err)\r\ngoto free_buckets;\r\n}\r\nreturn &htab->map;\r\nfree_buckets:\r\nkvfree(htab->buckets);\r\nfree_htab:\r\nkfree(htab);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic inline u32 htab_map_hash(const void *key, u32 key_len)\r\n{\r\nreturn jhash(key, key_len, 0);\r\n}\r\nstatic inline struct bucket *__select_bucket(struct bpf_htab *htab, u32 hash)\r\n{\r\nreturn &htab->buckets[hash & (htab->n_buckets - 1)];\r\n}\r\nstatic inline struct hlist_head *select_bucket(struct bpf_htab *htab, u32 hash)\r\n{\r\nreturn &__select_bucket(htab, hash)->head;\r\n}\r\nstatic struct htab_elem *lookup_elem_raw(struct hlist_head *head, u32 hash,\r\nvoid *key, u32 key_size)\r\n{\r\nstruct htab_elem *l;\r\nhlist_for_each_entry_rcu(l, head, hash_node)\r\nif (l->hash == hash && !memcmp(&l->key, key, key_size))\r\nreturn l;\r\nreturn NULL;\r\n}\r\nstatic void *__htab_map_lookup_elem(struct bpf_map *map, void *key)\r\n{\r\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\r\nstruct hlist_head *head;\r\nstruct htab_elem *l;\r\nu32 hash, key_size;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nkey_size = map->key_size;\r\nhash = htab_map_hash(key, key_size);\r\nhead = select_bucket(htab, hash);\r\nl = lookup_elem_raw(head, hash, key, key_size);\r\nreturn l;\r\n}\r\nstatic void *htab_map_lookup_elem(struct bpf_map *map, void *key)\r\n{\r\nstruct htab_elem *l = __htab_map_lookup_elem(map, key);\r\nif (l)\r\nreturn l->key + round_up(map->key_size, 8);\r\nreturn NULL;\r\n}\r\nstatic int htab_map_get_next_key(struct bpf_map *map, void *key, void *next_key)\r\n{\r\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\r\nstruct hlist_head *head;\r\nstruct htab_elem *l, *next_l;\r\nu32 hash, key_size;\r\nint i;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nkey_size = map->key_size;\r\nhash = htab_map_hash(key, key_size);\r\nhead = select_bucket(htab, hash);\r\nl = lookup_elem_raw(head, hash, key, key_size);\r\nif (!l) {\r\ni = 0;\r\ngoto find_first_elem;\r\n}\r\nnext_l = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(&l->hash_node)),\r\nstruct htab_elem, hash_node);\r\nif (next_l) {\r\nmemcpy(next_key, next_l->key, key_size);\r\nreturn 0;\r\n}\r\ni = hash & (htab->n_buckets - 1);\r\ni++;\r\nfind_first_elem:\r\nfor (; i < htab->n_buckets; i++) {\r\nhead = select_bucket(htab, i);\r\nnext_l = hlist_entry_safe(rcu_dereference_raw(hlist_first_rcu(head)),\r\nstruct htab_elem, hash_node);\r\nif (next_l) {\r\nmemcpy(next_key, next_l->key, key_size);\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENOENT;\r\n}\r\nstatic void htab_elem_free(struct bpf_htab *htab, struct htab_elem *l)\r\n{\r\nif (htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH)\r\nfree_percpu(htab_elem_get_ptr(l, htab->map.key_size));\r\nkfree(l);\r\n}\r\nstatic void htab_elem_free_rcu(struct rcu_head *head)\r\n{\r\nstruct htab_elem *l = container_of(head, struct htab_elem, rcu);\r\nstruct bpf_htab *htab = l->htab;\r\npreempt_disable();\r\n__this_cpu_inc(bpf_prog_active);\r\nhtab_elem_free(htab, l);\r\n__this_cpu_dec(bpf_prog_active);\r\npreempt_enable();\r\n}\r\nstatic void free_htab_elem(struct bpf_htab *htab, struct htab_elem *l)\r\n{\r\nif (!(htab->map.map_flags & BPF_F_NO_PREALLOC)) {\r\npcpu_freelist_push(&htab->freelist, &l->fnode);\r\n} else {\r\natomic_dec(&htab->count);\r\nl->htab = htab;\r\ncall_rcu(&l->rcu, htab_elem_free_rcu);\r\n}\r\n}\r\nstatic struct htab_elem *alloc_htab_elem(struct bpf_htab *htab, void *key,\r\nvoid *value, u32 key_size, u32 hash,\r\nbool percpu, bool onallcpus)\r\n{\r\nu32 size = htab->map.value_size;\r\nbool prealloc = !(htab->map.map_flags & BPF_F_NO_PREALLOC);\r\nstruct htab_elem *l_new;\r\nvoid __percpu *pptr;\r\nif (prealloc) {\r\nl_new = (struct htab_elem *)pcpu_freelist_pop(&htab->freelist);\r\nif (!l_new)\r\nreturn ERR_PTR(-E2BIG);\r\n} else {\r\nif (atomic_inc_return(&htab->count) > htab->map.max_entries) {\r\natomic_dec(&htab->count);\r\nreturn ERR_PTR(-E2BIG);\r\n}\r\nl_new = kmalloc(htab->elem_size, GFP_ATOMIC | __GFP_NOWARN);\r\nif (!l_new)\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nmemcpy(l_new->key, key, key_size);\r\nif (percpu) {\r\nsize = round_up(size, 8);\r\nif (prealloc) {\r\npptr = htab_elem_get_ptr(l_new, key_size);\r\n} else {\r\npptr = __alloc_percpu_gfp(size, 8,\r\nGFP_ATOMIC | __GFP_NOWARN);\r\nif (!pptr) {\r\nkfree(l_new);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\n}\r\nif (!onallcpus) {\r\nmemcpy(this_cpu_ptr(pptr), value, htab->map.value_size);\r\n} else {\r\nint off = 0, cpu;\r\nfor_each_possible_cpu(cpu) {\r\nbpf_long_memcpy(per_cpu_ptr(pptr, cpu),\r\nvalue + off, size);\r\noff += size;\r\n}\r\n}\r\nif (!prealloc)\r\nhtab_elem_set_ptr(l_new, key_size, pptr);\r\n} else {\r\nmemcpy(l_new->key + round_up(key_size, 8), value, size);\r\n}\r\nl_new->hash = hash;\r\nreturn l_new;\r\n}\r\nstatic int check_flags(struct bpf_htab *htab, struct htab_elem *l_old,\r\nu64 map_flags)\r\n{\r\nif (l_old && map_flags == BPF_NOEXIST)\r\nreturn -EEXIST;\r\nif (!l_old && map_flags == BPF_EXIST)\r\nreturn -ENOENT;\r\nreturn 0;\r\n}\r\nstatic int htab_map_update_elem(struct bpf_map *map, void *key, void *value,\r\nu64 map_flags)\r\n{\r\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\r\nstruct htab_elem *l_new = NULL, *l_old;\r\nstruct hlist_head *head;\r\nunsigned long flags;\r\nstruct bucket *b;\r\nu32 key_size, hash;\r\nint ret;\r\nif (unlikely(map_flags > BPF_EXIST))\r\nreturn -EINVAL;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nkey_size = map->key_size;\r\nhash = htab_map_hash(key, key_size);\r\nb = __select_bucket(htab, hash);\r\nhead = &b->head;\r\nraw_spin_lock_irqsave(&b->lock, flags);\r\nl_old = lookup_elem_raw(head, hash, key, key_size);\r\nret = check_flags(htab, l_old, map_flags);\r\nif (ret)\r\ngoto err;\r\nl_new = alloc_htab_elem(htab, key, value, key_size, hash, false, false);\r\nif (IS_ERR(l_new)) {\r\nret = PTR_ERR(l_new);\r\ngoto err;\r\n}\r\nhlist_add_head_rcu(&l_new->hash_node, head);\r\nif (l_old) {\r\nhlist_del_rcu(&l_old->hash_node);\r\nfree_htab_elem(htab, l_old);\r\n}\r\nret = 0;\r\nerr:\r\nraw_spin_unlock_irqrestore(&b->lock, flags);\r\nreturn ret;\r\n}\r\nstatic int __htab_percpu_map_update_elem(struct bpf_map *map, void *key,\r\nvoid *value, u64 map_flags,\r\nbool onallcpus)\r\n{\r\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\r\nstruct htab_elem *l_new = NULL, *l_old;\r\nstruct hlist_head *head;\r\nunsigned long flags;\r\nstruct bucket *b;\r\nu32 key_size, hash;\r\nint ret;\r\nif (unlikely(map_flags > BPF_EXIST))\r\nreturn -EINVAL;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nkey_size = map->key_size;\r\nhash = htab_map_hash(key, key_size);\r\nb = __select_bucket(htab, hash);\r\nhead = &b->head;\r\nraw_spin_lock_irqsave(&b->lock, flags);\r\nl_old = lookup_elem_raw(head, hash, key, key_size);\r\nret = check_flags(htab, l_old, map_flags);\r\nif (ret)\r\ngoto err;\r\nif (l_old) {\r\nvoid __percpu *pptr = htab_elem_get_ptr(l_old, key_size);\r\nu32 size = htab->map.value_size;\r\nif (!onallcpus) {\r\nmemcpy(this_cpu_ptr(pptr), value, size);\r\n} else {\r\nint off = 0, cpu;\r\nsize = round_up(size, 8);\r\nfor_each_possible_cpu(cpu) {\r\nbpf_long_memcpy(per_cpu_ptr(pptr, cpu),\r\nvalue + off, size);\r\noff += size;\r\n}\r\n}\r\n} else {\r\nl_new = alloc_htab_elem(htab, key, value, key_size,\r\nhash, true, onallcpus);\r\nif (IS_ERR(l_new)) {\r\nret = PTR_ERR(l_new);\r\ngoto err;\r\n}\r\nhlist_add_head_rcu(&l_new->hash_node, head);\r\n}\r\nret = 0;\r\nerr:\r\nraw_spin_unlock_irqrestore(&b->lock, flags);\r\nreturn ret;\r\n}\r\nstatic int htab_percpu_map_update_elem(struct bpf_map *map, void *key,\r\nvoid *value, u64 map_flags)\r\n{\r\nreturn __htab_percpu_map_update_elem(map, key, value, map_flags, false);\r\n}\r\nstatic int htab_map_delete_elem(struct bpf_map *map, void *key)\r\n{\r\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\r\nstruct hlist_head *head;\r\nstruct bucket *b;\r\nstruct htab_elem *l;\r\nunsigned long flags;\r\nu32 hash, key_size;\r\nint ret = -ENOENT;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nkey_size = map->key_size;\r\nhash = htab_map_hash(key, key_size);\r\nb = __select_bucket(htab, hash);\r\nhead = &b->head;\r\nraw_spin_lock_irqsave(&b->lock, flags);\r\nl = lookup_elem_raw(head, hash, key, key_size);\r\nif (l) {\r\nhlist_del_rcu(&l->hash_node);\r\nfree_htab_elem(htab, l);\r\nret = 0;\r\n}\r\nraw_spin_unlock_irqrestore(&b->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void delete_all_elements(struct bpf_htab *htab)\r\n{\r\nint i;\r\nfor (i = 0; i < htab->n_buckets; i++) {\r\nstruct hlist_head *head = select_bucket(htab, i);\r\nstruct hlist_node *n;\r\nstruct htab_elem *l;\r\nhlist_for_each_entry_safe(l, n, head, hash_node) {\r\nhlist_del_rcu(&l->hash_node);\r\nhtab_elem_free(htab, l);\r\n}\r\n}\r\n}\r\nstatic void htab_map_free(struct bpf_map *map)\r\n{\r\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\r\nsynchronize_rcu();\r\nrcu_barrier();\r\nif (htab->map.map_flags & BPF_F_NO_PREALLOC) {\r\ndelete_all_elements(htab);\r\n} else {\r\nhtab_free_elems(htab);\r\npcpu_freelist_destroy(&htab->freelist);\r\n}\r\nkvfree(htab->buckets);\r\nkfree(htab);\r\n}\r\nstatic void *htab_percpu_map_lookup_elem(struct bpf_map *map, void *key)\r\n{\r\nstruct htab_elem *l = __htab_map_lookup_elem(map, key);\r\nif (l)\r\nreturn this_cpu_ptr(htab_elem_get_ptr(l, map->key_size));\r\nelse\r\nreturn NULL;\r\n}\r\nint bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value)\r\n{\r\nstruct htab_elem *l;\r\nvoid __percpu *pptr;\r\nint ret = -ENOENT;\r\nint cpu, off = 0;\r\nu32 size;\r\nsize = round_up(map->value_size, 8);\r\nrcu_read_lock();\r\nl = __htab_map_lookup_elem(map, key);\r\nif (!l)\r\ngoto out;\r\npptr = htab_elem_get_ptr(l, map->key_size);\r\nfor_each_possible_cpu(cpu) {\r\nbpf_long_memcpy(value + off,\r\nper_cpu_ptr(pptr, cpu), size);\r\noff += size;\r\n}\r\nret = 0;\r\nout:\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nint bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,\r\nu64 map_flags)\r\n{\r\nint ret;\r\nrcu_read_lock();\r\nret = __htab_percpu_map_update_elem(map, key, value, map_flags, true);\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int __init register_htab_map(void)\r\n{\r\nbpf_register_map_type(&htab_type);\r\nbpf_register_map_type(&htab_percpu_type);\r\nreturn 0;\r\n}
