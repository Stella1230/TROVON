static struct __btrfs_workqueue *\r\n__btrfs_alloc_workqueue(const char *name, unsigned int flags, int limit_active,\r\nint thresh)\r\n{\r\nstruct __btrfs_workqueue *ret = kzalloc(sizeof(*ret), GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->limit_active = limit_active;\r\natomic_set(&ret->pending, 0);\r\nif (thresh == 0)\r\nthresh = DFT_THRESHOLD;\r\nif (thresh < DFT_THRESHOLD) {\r\nret->current_active = limit_active;\r\nret->thresh = NO_THRESHOLD;\r\n} else {\r\nret->current_active = 1;\r\nret->thresh = thresh;\r\n}\r\nif (flags & WQ_HIGHPRI)\r\nret->normal_wq = alloc_workqueue("%s-%s-high", flags,\r\nret->current_active, "btrfs",\r\nname);\r\nelse\r\nret->normal_wq = alloc_workqueue("%s-%s", flags,\r\nret->current_active, "btrfs",\r\nname);\r\nif (!ret->normal_wq) {\r\nkfree(ret);\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&ret->ordered_list);\r\nspin_lock_init(&ret->list_lock);\r\nspin_lock_init(&ret->thres_lock);\r\ntrace_btrfs_workqueue_alloc(ret, name, flags & WQ_HIGHPRI);\r\nreturn ret;\r\n}\r\nstruct btrfs_workqueue *btrfs_alloc_workqueue(const char *name,\r\nunsigned int flags,\r\nint limit_active,\r\nint thresh)\r\n{\r\nstruct btrfs_workqueue *ret = kzalloc(sizeof(*ret), GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->normal = __btrfs_alloc_workqueue(name, flags & ~WQ_HIGHPRI,\r\nlimit_active, thresh);\r\nif (!ret->normal) {\r\nkfree(ret);\r\nreturn NULL;\r\n}\r\nif (flags & WQ_HIGHPRI) {\r\nret->high = __btrfs_alloc_workqueue(name, flags, limit_active,\r\nthresh);\r\nif (!ret->high) {\r\n__btrfs_destroy_workqueue(ret->normal);\r\nkfree(ret);\r\nreturn NULL;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic inline void thresh_queue_hook(struct __btrfs_workqueue *wq)\r\n{\r\nif (wq->thresh == NO_THRESHOLD)\r\nreturn;\r\natomic_inc(&wq->pending);\r\n}\r\nstatic inline void thresh_exec_hook(struct __btrfs_workqueue *wq)\r\n{\r\nint new_current_active;\r\nlong pending;\r\nint need_change = 0;\r\nif (wq->thresh == NO_THRESHOLD)\r\nreturn;\r\natomic_dec(&wq->pending);\r\nspin_lock(&wq->thres_lock);\r\nwq->count++;\r\nwq->count %= (wq->thresh / 4);\r\nif (!wq->count)\r\ngoto out;\r\nnew_current_active = wq->current_active;\r\npending = atomic_read(&wq->pending);\r\nif (pending > wq->thresh)\r\nnew_current_active++;\r\nif (pending < wq->thresh / 2)\r\nnew_current_active--;\r\nnew_current_active = clamp_val(new_current_active, 1, wq->limit_active);\r\nif (new_current_active != wq->current_active) {\r\nneed_change = 1;\r\nwq->current_active = new_current_active;\r\n}\r\nout:\r\nspin_unlock(&wq->thres_lock);\r\nif (need_change) {\r\nworkqueue_set_max_active(wq->normal_wq, wq->current_active);\r\n}\r\n}\r\nstatic void run_ordered_work(struct __btrfs_workqueue *wq)\r\n{\r\nstruct list_head *list = &wq->ordered_list;\r\nstruct btrfs_work *work;\r\nspinlock_t *lock = &wq->list_lock;\r\nunsigned long flags;\r\nwhile (1) {\r\nspin_lock_irqsave(lock, flags);\r\nif (list_empty(list))\r\nbreak;\r\nwork = list_entry(list->next, struct btrfs_work,\r\nordered_list);\r\nif (!test_bit(WORK_DONE_BIT, &work->flags))\r\nbreak;\r\nif (test_and_set_bit(WORK_ORDER_DONE_BIT, &work->flags))\r\nbreak;\r\ntrace_btrfs_ordered_sched(work);\r\nspin_unlock_irqrestore(lock, flags);\r\nwork->ordered_func(work);\r\nspin_lock_irqsave(lock, flags);\r\nlist_del(&work->ordered_list);\r\nspin_unlock_irqrestore(lock, flags);\r\nwork->ordered_free(work);\r\ntrace_btrfs_all_work_done(work);\r\n}\r\nspin_unlock_irqrestore(lock, flags);\r\n}\r\nstatic void normal_work_helper(struct btrfs_work *work)\r\n{\r\nstruct __btrfs_workqueue *wq;\r\nint need_order = 0;\r\nif (work->ordered_func)\r\nneed_order = 1;\r\nwq = work->wq;\r\ntrace_btrfs_work_sched(work);\r\nthresh_exec_hook(wq);\r\nwork->func(work);\r\nif (need_order) {\r\nset_bit(WORK_DONE_BIT, &work->flags);\r\nrun_ordered_work(wq);\r\n}\r\nif (!need_order)\r\ntrace_btrfs_all_work_done(work);\r\n}\r\nvoid btrfs_init_work(struct btrfs_work *work, btrfs_work_func_t uniq_func,\r\nbtrfs_func_t func,\r\nbtrfs_func_t ordered_func,\r\nbtrfs_func_t ordered_free)\r\n{\r\nwork->func = func;\r\nwork->ordered_func = ordered_func;\r\nwork->ordered_free = ordered_free;\r\nINIT_WORK(&work->normal_work, uniq_func);\r\nINIT_LIST_HEAD(&work->ordered_list);\r\nwork->flags = 0;\r\n}\r\nstatic inline void __btrfs_queue_work(struct __btrfs_workqueue *wq,\r\nstruct btrfs_work *work)\r\n{\r\nunsigned long flags;\r\nwork->wq = wq;\r\nthresh_queue_hook(wq);\r\nif (work->ordered_func) {\r\nspin_lock_irqsave(&wq->list_lock, flags);\r\nlist_add_tail(&work->ordered_list, &wq->ordered_list);\r\nspin_unlock_irqrestore(&wq->list_lock, flags);\r\n}\r\ntrace_btrfs_work_queued(work);\r\nqueue_work(wq->normal_wq, &work->normal_work);\r\n}\r\nvoid btrfs_queue_work(struct btrfs_workqueue *wq,\r\nstruct btrfs_work *work)\r\n{\r\nstruct __btrfs_workqueue *dest_wq;\r\nif (test_bit(WORK_HIGH_PRIO_BIT, &work->flags) && wq->high)\r\ndest_wq = wq->high;\r\nelse\r\ndest_wq = wq->normal;\r\n__btrfs_queue_work(dest_wq, work);\r\n}\r\nstatic inline void\r\n__btrfs_destroy_workqueue(struct __btrfs_workqueue *wq)\r\n{\r\ndestroy_workqueue(wq->normal_wq);\r\ntrace_btrfs_workqueue_destroy(wq);\r\nkfree(wq);\r\n}\r\nvoid btrfs_destroy_workqueue(struct btrfs_workqueue *wq)\r\n{\r\nif (!wq)\r\nreturn;\r\nif (wq->high)\r\n__btrfs_destroy_workqueue(wq->high);\r\n__btrfs_destroy_workqueue(wq->normal);\r\nkfree(wq);\r\n}\r\nvoid btrfs_workqueue_set_max(struct btrfs_workqueue *wq, int limit_active)\r\n{\r\nif (!wq)\r\nreturn;\r\nwq->normal->limit_active = limit_active;\r\nif (wq->high)\r\nwq->high->limit_active = limit_active;\r\n}\r\nvoid btrfs_set_work_high_priority(struct btrfs_work *work)\r\n{\r\nset_bit(WORK_HIGH_PRIO_BIT, &work->flags);\r\n}
