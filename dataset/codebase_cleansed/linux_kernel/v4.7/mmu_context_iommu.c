static long mm_iommu_adjust_locked_vm(struct mm_struct *mm,\r\nunsigned long npages, bool incr)\r\n{\r\nlong ret = 0, locked, lock_limit;\r\nif (!npages)\r\nreturn 0;\r\ndown_write(&mm->mmap_sem);\r\nif (incr) {\r\nlocked = mm->locked_vm + npages;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif (locked > lock_limit && !capable(CAP_IPC_LOCK))\r\nret = -ENOMEM;\r\nelse\r\nmm->locked_vm += npages;\r\n} else {\r\nif (WARN_ON_ONCE(npages > mm->locked_vm))\r\nnpages = mm->locked_vm;\r\nmm->locked_vm -= npages;\r\n}\r\npr_debug("[%d] RLIMIT_MEMLOCK HASH64 %c%ld %ld/%ld\n",\r\ncurrent->pid,\r\nincr ? '+' : '-',\r\nnpages << PAGE_SHIFT,\r\nmm->locked_vm << PAGE_SHIFT,\r\nrlimit(RLIMIT_MEMLOCK));\r\nup_write(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nbool mm_iommu_preregistered(void)\r\n{\r\nif (!current || !current->mm)\r\nreturn false;\r\nreturn !list_empty(&current->mm->context.iommu_group_mem_list);\r\n}\r\nlong mm_iommu_get(unsigned long ua, unsigned long entries,\r\nstruct mm_iommu_table_group_mem_t **pmem)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem;\r\nlong i, j, ret = 0, locked_entries = 0;\r\nstruct page *page = NULL;\r\nif (!current || !current->mm)\r\nreturn -ESRCH;\r\nmutex_lock(&mem_list_mutex);\r\nlist_for_each_entry_rcu(mem, &current->mm->context.iommu_group_mem_list,\r\nnext) {\r\nif ((mem->ua == ua) && (mem->entries == entries)) {\r\n++mem->used;\r\n*pmem = mem;\r\ngoto unlock_exit;\r\n}\r\nif ((mem->ua < (ua + (entries << PAGE_SHIFT))) &&\r\n(ua < (mem->ua +\r\n(mem->entries << PAGE_SHIFT)))) {\r\nret = -EINVAL;\r\ngoto unlock_exit;\r\n}\r\n}\r\nret = mm_iommu_adjust_locked_vm(current->mm, entries, true);\r\nif (ret)\r\ngoto unlock_exit;\r\nlocked_entries = entries;\r\nmem = kzalloc(sizeof(*mem), GFP_KERNEL);\r\nif (!mem) {\r\nret = -ENOMEM;\r\ngoto unlock_exit;\r\n}\r\nmem->hpas = vzalloc(entries * sizeof(mem->hpas[0]));\r\nif (!mem->hpas) {\r\nkfree(mem);\r\nret = -ENOMEM;\r\ngoto unlock_exit;\r\n}\r\nfor (i = 0; i < entries; ++i) {\r\nif (1 != get_user_pages_fast(ua + (i << PAGE_SHIFT),\r\n1, 1, &page)) {\r\nfor (j = 0; j < i; ++j)\r\nput_page(pfn_to_page(\r\nmem->hpas[j] >> PAGE_SHIFT));\r\nvfree(mem->hpas);\r\nkfree(mem);\r\nret = -EFAULT;\r\ngoto unlock_exit;\r\n}\r\nmem->hpas[i] = page_to_pfn(page) << PAGE_SHIFT;\r\n}\r\natomic64_set(&mem->mapped, 1);\r\nmem->used = 1;\r\nmem->ua = ua;\r\nmem->entries = entries;\r\n*pmem = mem;\r\nlist_add_rcu(&mem->next, &current->mm->context.iommu_group_mem_list);\r\nunlock_exit:\r\nif (locked_entries && ret)\r\nmm_iommu_adjust_locked_vm(current->mm, locked_entries, false);\r\nmutex_unlock(&mem_list_mutex);\r\nreturn ret;\r\n}\r\nstatic void mm_iommu_unpin(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nlong i;\r\nstruct page *page = NULL;\r\nfor (i = 0; i < mem->entries; ++i) {\r\nif (!mem->hpas[i])\r\ncontinue;\r\npage = pfn_to_page(mem->hpas[i] >> PAGE_SHIFT);\r\nif (!page)\r\ncontinue;\r\nput_page(page);\r\nmem->hpas[i] = 0;\r\n}\r\n}\r\nstatic void mm_iommu_do_free(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nmm_iommu_unpin(mem);\r\nvfree(mem->hpas);\r\nkfree(mem);\r\n}\r\nstatic void mm_iommu_free(struct rcu_head *head)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem = container_of(head,\r\nstruct mm_iommu_table_group_mem_t, rcu);\r\nmm_iommu_do_free(mem);\r\n}\r\nstatic void mm_iommu_release(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nlist_del_rcu(&mem->next);\r\nmm_iommu_adjust_locked_vm(current->mm, mem->entries, false);\r\ncall_rcu(&mem->rcu, mm_iommu_free);\r\n}\r\nlong mm_iommu_put(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nlong ret = 0;\r\nif (!current || !current->mm)\r\nreturn -ESRCH;\r\nmutex_lock(&mem_list_mutex);\r\nif (mem->used == 0) {\r\nret = -ENOENT;\r\ngoto unlock_exit;\r\n}\r\n--mem->used;\r\nif (mem->used)\r\ngoto unlock_exit;\r\nif (atomic_cmpxchg(&mem->mapped, 1, 0) != 1) {\r\n++mem->used;\r\nret = -EBUSY;\r\ngoto unlock_exit;\r\n}\r\nmm_iommu_release(mem);\r\nunlock_exit:\r\nmutex_unlock(&mem_list_mutex);\r\nreturn ret;\r\n}\r\nstruct mm_iommu_table_group_mem_t *mm_iommu_lookup(unsigned long ua,\r\nunsigned long size)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem, *ret = NULL;\r\nlist_for_each_entry_rcu(mem,\r\n&current->mm->context.iommu_group_mem_list,\r\nnext) {\r\nif ((mem->ua <= ua) &&\r\n(ua + size <= mem->ua +\r\n(mem->entries << PAGE_SHIFT))) {\r\nret = mem;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstruct mm_iommu_table_group_mem_t *mm_iommu_find(unsigned long ua,\r\nunsigned long entries)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem, *ret = NULL;\r\nlist_for_each_entry_rcu(mem,\r\n&current->mm->context.iommu_group_mem_list,\r\nnext) {\r\nif ((mem->ua == ua) && (mem->entries == entries)) {\r\nret = mem;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nlong mm_iommu_ua_to_hpa(struct mm_iommu_table_group_mem_t *mem,\r\nunsigned long ua, unsigned long *hpa)\r\n{\r\nconst long entry = (ua - mem->ua) >> PAGE_SHIFT;\r\nu64 *va = &mem->hpas[entry];\r\nif (entry >= mem->entries)\r\nreturn -EFAULT;\r\n*hpa = *va | (ua & ~PAGE_MASK);\r\nreturn 0;\r\n}\r\nlong mm_iommu_mapped_inc(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nif (atomic64_inc_not_zero(&mem->mapped))\r\nreturn 0;\r\nreturn -ENXIO;\r\n}\r\nvoid mm_iommu_mapped_dec(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\natomic64_add_unless(&mem->mapped, -1, 1);\r\n}\r\nvoid mm_iommu_init(mm_context_t *ctx)\r\n{\r\nINIT_LIST_HEAD_RCU(&ctx->iommu_group_mem_list);\r\n}\r\nvoid mm_iommu_cleanup(mm_context_t *ctx)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem, *tmp;\r\nlist_for_each_entry_safe(mem, tmp, &ctx->iommu_group_mem_list, next) {\r\nlist_del_rcu(&mem->next);\r\nmm_iommu_do_free(mem);\r\n}\r\n}
