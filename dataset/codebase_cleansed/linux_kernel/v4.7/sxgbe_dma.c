static int sxgbe_dma_init(void __iomem *ioaddr, int fix_burst, int burst_map)\r\n{\r\nu32 reg_val;\r\nreg_val = readl(ioaddr + SXGBE_DMA_SYSBUS_MODE_REG);\r\nif (!fix_burst)\r\nreg_val |= SXGBE_DMA_AXI_UNDEF_BURST;\r\nreg_val |= (burst_map << SXGBE_DMA_BLENMAP_LSHIFT);\r\nwritel(reg_val, ioaddr + SXGBE_DMA_SYSBUS_MODE_REG);\r\nreturn 0;\r\n}\r\nstatic void sxgbe_dma_channel_init(void __iomem *ioaddr, int cha_num,\r\nint fix_burst, int pbl, dma_addr_t dma_tx,\r\ndma_addr_t dma_rx, int t_rsize, int r_rsize)\r\n{\r\nu32 reg_val;\r\ndma_addr_t dma_addr;\r\nreg_val = readl(ioaddr + SXGBE_DMA_CHA_CTL_REG(cha_num));\r\nif (fix_burst) {\r\nreg_val |= SXGBE_DMA_PBL_X8MODE;\r\nwritel(reg_val, ioaddr + SXGBE_DMA_CHA_CTL_REG(cha_num));\r\nreg_val = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\r\nreg_val |= (pbl << SXGBE_DMA_TXPBL_LSHIFT);\r\nwritel(reg_val, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\r\nreg_val = readl(ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cha_num));\r\nreg_val |= (pbl << SXGBE_DMA_RXPBL_LSHIFT);\r\nwritel(reg_val, ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cha_num));\r\n}\r\nwritel(upper_32_bits(dma_tx),\r\nioaddr + SXGBE_DMA_CHA_TXDESC_HADD_REG(cha_num));\r\nwritel(lower_32_bits(dma_tx),\r\nioaddr + SXGBE_DMA_CHA_TXDESC_LADD_REG(cha_num));\r\nwritel(upper_32_bits(dma_rx),\r\nioaddr + SXGBE_DMA_CHA_RXDESC_HADD_REG(cha_num));\r\nwritel(lower_32_bits(dma_rx),\r\nioaddr + SXGBE_DMA_CHA_RXDESC_LADD_REG(cha_num));\r\ndma_addr = dma_tx + ((t_rsize - 1) * SXGBE_DESC_SIZE_BYTES);\r\nwritel(lower_32_bits(dma_addr),\r\nioaddr + SXGBE_DMA_CHA_TXDESC_TAILPTR_REG(cha_num));\r\ndma_addr = dma_rx + ((r_rsize - 1) * SXGBE_DESC_SIZE_BYTES);\r\nwritel(lower_32_bits(dma_addr),\r\nioaddr + SXGBE_DMA_CHA_RXDESC_LADD_REG(cha_num));\r\nwritel(t_rsize - 1, ioaddr + SXGBE_DMA_CHA_TXDESC_RINGLEN_REG(cha_num));\r\nwritel(r_rsize - 1, ioaddr + SXGBE_DMA_CHA_RXDESC_RINGLEN_REG(cha_num));\r\nwritel(SXGBE_DMA_ENA_INT,\r\nioaddr + SXGBE_DMA_CHA_INT_ENABLE_REG(cha_num));\r\n}\r\nstatic void sxgbe_enable_dma_transmission(void __iomem *ioaddr, int cha_num)\r\n{\r\nu32 tx_config;\r\ntx_config = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\r\ntx_config |= SXGBE_TX_START_DMA;\r\nwritel(tx_config, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\r\n}\r\nstatic void sxgbe_enable_dma_irq(void __iomem *ioaddr, int dma_cnum)\r\n{\r\nwritel(SXGBE_DMA_ENA_INT,\r\nioaddr + SXGBE_DMA_CHA_INT_ENABLE_REG(dma_cnum));\r\n}\r\nstatic void sxgbe_disable_dma_irq(void __iomem *ioaddr, int dma_cnum)\r\n{\r\nwritel(0, ioaddr + SXGBE_DMA_CHA_INT_ENABLE_REG(dma_cnum));\r\n}\r\nstatic void sxgbe_dma_start_tx(void __iomem *ioaddr, int tchannels)\r\n{\r\nint cnum;\r\nu32 tx_ctl_reg;\r\nfor (cnum = 0; cnum < tchannels; cnum++) {\r\ntx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\r\ntx_ctl_reg |= SXGBE_TX_ENABLE;\r\nwritel(tx_ctl_reg,\r\nioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\r\n}\r\n}\r\nstatic void sxgbe_dma_start_tx_queue(void __iomem *ioaddr, int dma_cnum)\r\n{\r\nu32 tx_ctl_reg;\r\ntx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\r\ntx_ctl_reg |= SXGBE_TX_ENABLE;\r\nwritel(tx_ctl_reg, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\r\n}\r\nstatic void sxgbe_dma_stop_tx_queue(void __iomem *ioaddr, int dma_cnum)\r\n{\r\nu32 tx_ctl_reg;\r\ntx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\r\ntx_ctl_reg &= ~(SXGBE_TX_ENABLE);\r\nwritel(tx_ctl_reg, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\r\n}\r\nstatic void sxgbe_dma_stop_tx(void __iomem *ioaddr, int tchannels)\r\n{\r\nint cnum;\r\nu32 tx_ctl_reg;\r\nfor (cnum = 0; cnum < tchannels; cnum++) {\r\ntx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\r\ntx_ctl_reg &= ~(SXGBE_TX_ENABLE);\r\nwritel(tx_ctl_reg, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\r\n}\r\n}\r\nstatic void sxgbe_dma_start_rx(void __iomem *ioaddr, int rchannels)\r\n{\r\nint cnum;\r\nu32 rx_ctl_reg;\r\nfor (cnum = 0; cnum < rchannels; cnum++) {\r\nrx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\r\nrx_ctl_reg |= SXGBE_RX_ENABLE;\r\nwritel(rx_ctl_reg,\r\nioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\r\n}\r\n}\r\nstatic void sxgbe_dma_stop_rx(void __iomem *ioaddr, int rchannels)\r\n{\r\nint cnum;\r\nu32 rx_ctl_reg;\r\nfor (cnum = 0; cnum < rchannels; cnum++) {\r\nrx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\r\nrx_ctl_reg &= ~(SXGBE_RX_ENABLE);\r\nwritel(rx_ctl_reg, ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\r\n}\r\n}\r\nstatic int sxgbe_tx_dma_int_status(void __iomem *ioaddr, int channel_no,\r\nstruct sxgbe_extra_stats *x)\r\n{\r\nu32 int_status = readl(ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\r\nu32 clear_val = 0;\r\nu32 ret_val = 0;\r\nif (likely(int_status & SXGBE_DMA_INT_STATUS_NIS)) {\r\nx->normal_irq_n++;\r\nif (int_status & SXGBE_DMA_INT_STATUS_TI) {\r\nret_val |= handle_tx;\r\nx->tx_normal_irq_n++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_TI;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_TBU) {\r\nx->tx_underflow_irq++;\r\nret_val |= tx_bump_tc;\r\nclear_val |= SXGBE_DMA_INT_STATUS_TBU;\r\n}\r\n} else if (unlikely(int_status & SXGBE_DMA_INT_STATUS_AIS)) {\r\nif (int_status & SXGBE_DMA_INT_STATUS_TPS) {\r\nret_val |= tx_hard_error;\r\nclear_val |= SXGBE_DMA_INT_STATUS_TPS;\r\nx->tx_process_stopped_irq++;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_FBE) {\r\nret_val |= tx_hard_error;\r\nx->fatal_bus_error_irq++;\r\nif (int_status & SXGBE_DMA_INT_STATUS_TEB0) {\r\nx->tx_read_transfer_err++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_TEB0;\r\n} else {\r\nx->tx_write_transfer_err++;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_TEB1) {\r\nx->tx_desc_access_err++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_TEB1;\r\n} else {\r\nx->tx_buffer_access_err++;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_TEB2) {\r\nx->tx_data_transfer_err++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_TEB2;\r\n}\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_CTXTERR) {\r\nx->tx_ctxt_desc_err++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_CTXTERR;\r\n}\r\n}\r\nwritel(clear_val, ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\r\nreturn ret_val;\r\n}\r\nstatic int sxgbe_rx_dma_int_status(void __iomem *ioaddr, int channel_no,\r\nstruct sxgbe_extra_stats *x)\r\n{\r\nu32 int_status = readl(ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\r\nu32 clear_val = 0;\r\nu32 ret_val = 0;\r\nif (likely(int_status & SXGBE_DMA_INT_STATUS_NIS)) {\r\nx->normal_irq_n++;\r\nif (int_status & SXGBE_DMA_INT_STATUS_RI) {\r\nret_val |= handle_rx;\r\nx->rx_normal_irq_n++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_RI;\r\n}\r\n} else if (unlikely(int_status & SXGBE_DMA_INT_STATUS_AIS)) {\r\nif (int_status & SXGBE_DMA_INT_STATUS_RBU) {\r\nret_val |= rx_bump_tc;\r\nclear_val |= SXGBE_DMA_INT_STATUS_RBU;\r\nx->rx_underflow_irq++;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_RPS) {\r\nret_val |= rx_hard_error;\r\nclear_val |= SXGBE_DMA_INT_STATUS_RPS;\r\nx->rx_process_stopped_irq++;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_FBE) {\r\nret_val |= rx_hard_error;\r\nx->fatal_bus_error_irq++;\r\nif (int_status & SXGBE_DMA_INT_STATUS_REB0) {\r\nx->rx_read_transfer_err++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_REB0;\r\n} else {\r\nx->rx_write_transfer_err++;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_REB1) {\r\nx->rx_desc_access_err++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_REB1;\r\n} else {\r\nx->rx_buffer_access_err++;\r\n}\r\nif (int_status & SXGBE_DMA_INT_STATUS_REB2) {\r\nx->rx_data_transfer_err++;\r\nclear_val |= SXGBE_DMA_INT_STATUS_REB2;\r\n}\r\n}\r\n}\r\nwritel(clear_val, ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\r\nreturn ret_val;\r\n}\r\nstatic void sxgbe_dma_rx_watchdog(void __iomem *ioaddr, u32 riwt)\r\n{\r\nu32 que_num;\r\nSXGBE_FOR_EACH_QUEUE(SXGBE_RX_QUEUES, que_num) {\r\nwritel(riwt,\r\nioaddr + SXGBE_DMA_CHA_INT_RXWATCHTMR_REG(que_num));\r\n}\r\n}\r\nstatic void sxgbe_enable_tso(void __iomem *ioaddr, u8 chan_num)\r\n{\r\nu32 ctrl;\r\nctrl = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(chan_num));\r\nctrl |= SXGBE_DMA_CHA_TXCTL_TSE_ENABLE;\r\nwritel(ctrl, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(chan_num));\r\n}\r\nconst struct sxgbe_dma_ops *sxgbe_get_dma_ops(void)\r\n{\r\nreturn &sxgbe_dma_ops;\r\n}
