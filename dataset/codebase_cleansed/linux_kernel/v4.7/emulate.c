static ulong reg_read(struct x86_emulate_ctxt *ctxt, unsigned nr)\r\n{\r\nif (!(ctxt->regs_valid & (1 << nr))) {\r\nctxt->regs_valid |= 1 << nr;\r\nctxt->_regs[nr] = ctxt->ops->read_gpr(ctxt, nr);\r\n}\r\nreturn ctxt->_regs[nr];\r\n}\r\nstatic ulong *reg_write(struct x86_emulate_ctxt *ctxt, unsigned nr)\r\n{\r\nctxt->regs_valid |= 1 << nr;\r\nctxt->regs_dirty |= 1 << nr;\r\nreturn &ctxt->_regs[nr];\r\n}\r\nstatic ulong *reg_rmw(struct x86_emulate_ctxt *ctxt, unsigned nr)\r\n{\r\nreg_read(ctxt, nr);\r\nreturn reg_write(ctxt, nr);\r\n}\r\nstatic void writeback_registers(struct x86_emulate_ctxt *ctxt)\r\n{\r\nunsigned reg;\r\nfor_each_set_bit(reg, (ulong *)&ctxt->regs_dirty, 16)\r\nctxt->ops->write_gpr(ctxt, reg, ctxt->_regs[reg]);\r\n}\r\nstatic void invalidate_registers(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->regs_dirty = 0;\r\nctxt->regs_valid = 0;\r\n}\r\nstatic int emulator_check_intercept(struct x86_emulate_ctxt *ctxt,\r\nenum x86_intercept intercept,\r\nenum x86_intercept_stage stage)\r\n{\r\nstruct x86_instruction_info info = {\r\n.intercept = intercept,\r\n.rep_prefix = ctxt->rep_prefix,\r\n.modrm_mod = ctxt->modrm_mod,\r\n.modrm_reg = ctxt->modrm_reg,\r\n.modrm_rm = ctxt->modrm_rm,\r\n.src_val = ctxt->src.val64,\r\n.dst_val = ctxt->dst.val64,\r\n.src_bytes = ctxt->src.bytes,\r\n.dst_bytes = ctxt->dst.bytes,\r\n.ad_bytes = ctxt->ad_bytes,\r\n.next_rip = ctxt->eip,\r\n};\r\nreturn ctxt->ops->intercept(ctxt, &info, stage);\r\n}\r\nstatic void assign_masked(ulong *dest, ulong src, ulong mask)\r\n{\r\n*dest = (*dest & ~mask) | (src & mask);\r\n}\r\nstatic void assign_register(unsigned long *reg, u64 val, int bytes)\r\n{\r\nswitch (bytes) {\r\ncase 1:\r\n*(u8 *)reg = (u8)val;\r\nbreak;\r\ncase 2:\r\n*(u16 *)reg = (u16)val;\r\nbreak;\r\ncase 4:\r\n*reg = (u32)val;\r\nbreak;\r\ncase 8:\r\n*reg = val;\r\nbreak;\r\n}\r\n}\r\nstatic inline unsigned long ad_mask(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn (1UL << (ctxt->ad_bytes << 3)) - 1;\r\n}\r\nstatic ulong stack_mask(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu16 sel;\r\nstruct desc_struct ss;\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nreturn ~0UL;\r\nctxt->ops->get_segment(ctxt, &sel, &ss, NULL, VCPU_SREG_SS);\r\nreturn ~0U >> ((ss.d ^ 1) * 16);\r\n}\r\nstatic int stack_size(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn (__fls(stack_mask(ctxt)) + 1) >> 3;\r\n}\r\nstatic inline unsigned long\r\naddress_mask(struct x86_emulate_ctxt *ctxt, unsigned long reg)\r\n{\r\nif (ctxt->ad_bytes == sizeof(unsigned long))\r\nreturn reg;\r\nelse\r\nreturn reg & ad_mask(ctxt);\r\n}\r\nstatic inline unsigned long\r\nregister_address(struct x86_emulate_ctxt *ctxt, int reg)\r\n{\r\nreturn address_mask(ctxt, reg_read(ctxt, reg));\r\n}\r\nstatic void masked_increment(ulong *reg, ulong mask, int inc)\r\n{\r\nassign_masked(reg, *reg + inc, mask);\r\n}\r\nstatic inline void\r\nregister_address_increment(struct x86_emulate_ctxt *ctxt, int reg, int inc)\r\n{\r\nulong *preg = reg_rmw(ctxt, reg);\r\nassign_register(preg, *preg + inc, ctxt->ad_bytes);\r\n}\r\nstatic void rsp_increment(struct x86_emulate_ctxt *ctxt, int inc)\r\n{\r\nmasked_increment(reg_rmw(ctxt, VCPU_REGS_RSP), stack_mask(ctxt), inc);\r\n}\r\nstatic u32 desc_limit_scaled(struct desc_struct *desc)\r\n{\r\nu32 limit = get_desc_limit(desc);\r\nreturn desc->g ? (limit << 12) | 0xfff : limit;\r\n}\r\nstatic unsigned long seg_base(struct x86_emulate_ctxt *ctxt, int seg)\r\n{\r\nif (ctxt->mode == X86EMUL_MODE_PROT64 && seg < VCPU_SREG_FS)\r\nreturn 0;\r\nreturn ctxt->ops->get_cached_segment_base(ctxt, seg);\r\n}\r\nstatic int emulate_exception(struct x86_emulate_ctxt *ctxt, int vec,\r\nu32 error, bool valid)\r\n{\r\nWARN_ON(vec > 0x1f);\r\nctxt->exception.vector = vec;\r\nctxt->exception.error_code = error;\r\nctxt->exception.error_code_valid = valid;\r\nreturn X86EMUL_PROPAGATE_FAULT;\r\n}\r\nstatic int emulate_db(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn emulate_exception(ctxt, DB_VECTOR, 0, false);\r\n}\r\nstatic int emulate_gp(struct x86_emulate_ctxt *ctxt, int err)\r\n{\r\nreturn emulate_exception(ctxt, GP_VECTOR, err, true);\r\n}\r\nstatic int emulate_ss(struct x86_emulate_ctxt *ctxt, int err)\r\n{\r\nreturn emulate_exception(ctxt, SS_VECTOR, err, true);\r\n}\r\nstatic int emulate_ud(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn emulate_exception(ctxt, UD_VECTOR, 0, false);\r\n}\r\nstatic int emulate_ts(struct x86_emulate_ctxt *ctxt, int err)\r\n{\r\nreturn emulate_exception(ctxt, TS_VECTOR, err, true);\r\n}\r\nstatic int emulate_de(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn emulate_exception(ctxt, DE_VECTOR, 0, false);\r\n}\r\nstatic int emulate_nm(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn emulate_exception(ctxt, NM_VECTOR, 0, false);\r\n}\r\nstatic u16 get_segment_selector(struct x86_emulate_ctxt *ctxt, unsigned seg)\r\n{\r\nu16 selector;\r\nstruct desc_struct desc;\r\nctxt->ops->get_segment(ctxt, &selector, &desc, NULL, seg);\r\nreturn selector;\r\n}\r\nstatic void set_segment_selector(struct x86_emulate_ctxt *ctxt, u16 selector,\r\nunsigned seg)\r\n{\r\nu16 dummy;\r\nu32 base3;\r\nstruct desc_struct desc;\r\nctxt->ops->get_segment(ctxt, &dummy, &desc, &base3, seg);\r\nctxt->ops->set_segment(ctxt, selector, &desc, base3, seg);\r\n}\r\nstatic bool insn_aligned(struct x86_emulate_ctxt *ctxt, unsigned size)\r\n{\r\nif (likely(size < 16))\r\nreturn false;\r\nif (ctxt->d & Aligned)\r\nreturn true;\r\nelse if (ctxt->d & Unaligned)\r\nreturn false;\r\nelse if (ctxt->d & Avx)\r\nreturn false;\r\nelse\r\nreturn true;\r\n}\r\nstatic __always_inline int __linearize(struct x86_emulate_ctxt *ctxt,\r\nstruct segmented_address addr,\r\nunsigned *max_size, unsigned size,\r\nbool write, bool fetch,\r\nenum x86emul_mode mode, ulong *linear)\r\n{\r\nstruct desc_struct desc;\r\nbool usable;\r\nulong la;\r\nu32 lim;\r\nu16 sel;\r\nla = seg_base(ctxt, addr.seg) + addr.ea;\r\n*max_size = 0;\r\nswitch (mode) {\r\ncase X86EMUL_MODE_PROT64:\r\n*linear = la;\r\nif (is_noncanonical_address(la))\r\ngoto bad;\r\n*max_size = min_t(u64, ~0u, (1ull << 48) - la);\r\nif (size > *max_size)\r\ngoto bad;\r\nbreak;\r\ndefault:\r\n*linear = la = (u32)la;\r\nusable = ctxt->ops->get_segment(ctxt, &sel, &desc, NULL,\r\naddr.seg);\r\nif (!usable)\r\ngoto bad;\r\nif ((((ctxt->mode != X86EMUL_MODE_REAL) && (desc.type & 8))\r\n|| !(desc.type & 2)) && write)\r\ngoto bad;\r\nif (!fetch && (desc.type & 8) && !(desc.type & 2))\r\ngoto bad;\r\nlim = desc_limit_scaled(&desc);\r\nif (!(desc.type & 8) && (desc.type & 4)) {\r\nif (addr.ea <= lim)\r\ngoto bad;\r\nlim = desc.d ? 0xffffffff : 0xffff;\r\n}\r\nif (addr.ea > lim)\r\ngoto bad;\r\nif (lim == 0xffffffff)\r\n*max_size = ~0u;\r\nelse {\r\n*max_size = (u64)lim + 1 - addr.ea;\r\nif (size > *max_size)\r\ngoto bad;\r\n}\r\nbreak;\r\n}\r\nif (insn_aligned(ctxt, size) && ((la & (size - 1)) != 0))\r\nreturn emulate_gp(ctxt, 0);\r\nreturn X86EMUL_CONTINUE;\r\nbad:\r\nif (addr.seg == VCPU_SREG_SS)\r\nreturn emulate_ss(ctxt, 0);\r\nelse\r\nreturn emulate_gp(ctxt, 0);\r\n}\r\nstatic int linearize(struct x86_emulate_ctxt *ctxt,\r\nstruct segmented_address addr,\r\nunsigned size, bool write,\r\nulong *linear)\r\n{\r\nunsigned max_size;\r\nreturn __linearize(ctxt, addr, &max_size, size, write, false,\r\nctxt->mode, linear);\r\n}\r\nstatic inline int assign_eip(struct x86_emulate_ctxt *ctxt, ulong dst,\r\nenum x86emul_mode mode)\r\n{\r\nulong linear;\r\nint rc;\r\nunsigned max_size;\r\nstruct segmented_address addr = { .seg = VCPU_SREG_CS,\r\n.ea = dst };\r\nif (ctxt->op_bytes != sizeof(unsigned long))\r\naddr.ea = dst & ((1UL << (ctxt->op_bytes << 3)) - 1);\r\nrc = __linearize(ctxt, addr, &max_size, 1, false, true, mode, &linear);\r\nif (rc == X86EMUL_CONTINUE)\r\nctxt->_eip = addr.ea;\r\nreturn rc;\r\n}\r\nstatic inline int assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)\r\n{\r\nreturn assign_eip(ctxt, dst, ctxt->mode);\r\n}\r\nstatic int assign_eip_far(struct x86_emulate_ctxt *ctxt, ulong dst,\r\nconst struct desc_struct *cs_desc)\r\n{\r\nenum x86emul_mode mode = ctxt->mode;\r\nint rc;\r\n#ifdef CONFIG_X86_64\r\nif (ctxt->mode >= X86EMUL_MODE_PROT16) {\r\nif (cs_desc->l) {\r\nu64 efer = 0;\r\nctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\r\nif (efer & EFER_LMA)\r\nmode = X86EMUL_MODE_PROT64;\r\n} else\r\nmode = X86EMUL_MODE_PROT32;\r\n}\r\n#endif\r\nif (mode == X86EMUL_MODE_PROT16 || mode == X86EMUL_MODE_PROT32)\r\nmode = cs_desc->d ? X86EMUL_MODE_PROT32 : X86EMUL_MODE_PROT16;\r\nrc = assign_eip(ctxt, dst, mode);\r\nif (rc == X86EMUL_CONTINUE)\r\nctxt->mode = mode;\r\nreturn rc;\r\n}\r\nstatic inline int jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)\r\n{\r\nreturn assign_eip_near(ctxt, ctxt->_eip + rel);\r\n}\r\nstatic int segmented_read_std(struct x86_emulate_ctxt *ctxt,\r\nstruct segmented_address addr,\r\nvoid *data,\r\nunsigned size)\r\n{\r\nint rc;\r\nulong linear;\r\nrc = linearize(ctxt, addr, size, false, &linear);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn ctxt->ops->read_std(ctxt, linear, data, size, &ctxt->exception);\r\n}\r\nstatic int __do_insn_fetch_bytes(struct x86_emulate_ctxt *ctxt, int op_size)\r\n{\r\nint rc;\r\nunsigned size, max_size;\r\nunsigned long linear;\r\nint cur_size = ctxt->fetch.end - ctxt->fetch.data;\r\nstruct segmented_address addr = { .seg = VCPU_SREG_CS,\r\n.ea = ctxt->eip + cur_size };\r\nrc = __linearize(ctxt, addr, &max_size, 0, false, true, ctxt->mode,\r\n&linear);\r\nif (unlikely(rc != X86EMUL_CONTINUE))\r\nreturn rc;\r\nsize = min_t(unsigned, 15UL ^ cur_size, max_size);\r\nsize = min_t(unsigned, size, PAGE_SIZE - offset_in_page(linear));\r\nif (unlikely(size < op_size))\r\nreturn emulate_gp(ctxt, 0);\r\nrc = ctxt->ops->fetch(ctxt, linear, ctxt->fetch.end,\r\nsize, &ctxt->exception);\r\nif (unlikely(rc != X86EMUL_CONTINUE))\r\nreturn rc;\r\nctxt->fetch.end += size;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic __always_inline int do_insn_fetch_bytes(struct x86_emulate_ctxt *ctxt,\r\nunsigned size)\r\n{\r\nunsigned done_size = ctxt->fetch.end - ctxt->fetch.ptr;\r\nif (unlikely(done_size < size))\r\nreturn __do_insn_fetch_bytes(ctxt, size - done_size);\r\nelse\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic void *decode_register(struct x86_emulate_ctxt *ctxt, u8 modrm_reg,\r\nint byteop)\r\n{\r\nvoid *p;\r\nint highbyte_regs = (ctxt->rex_prefix == 0) && byteop;\r\nif (highbyte_regs && modrm_reg >= 4 && modrm_reg < 8)\r\np = (unsigned char *)reg_rmw(ctxt, modrm_reg & 3) + 1;\r\nelse\r\np = reg_rmw(ctxt, modrm_reg);\r\nreturn p;\r\n}\r\nstatic int read_descriptor(struct x86_emulate_ctxt *ctxt,\r\nstruct segmented_address addr,\r\nu16 *size, unsigned long *address, int op_bytes)\r\n{\r\nint rc;\r\nif (op_bytes == 2)\r\nop_bytes = 3;\r\n*address = 0;\r\nrc = segmented_read_std(ctxt, addr, size, 2);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\naddr.ea += 2;\r\nrc = segmented_read_std(ctxt, addr, address, op_bytes);\r\nreturn rc;\r\n}\r\nstatic int em_bsf_c(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (ctxt->src.val == 0)\r\nctxt->dst.type = OP_NONE;\r\nreturn fastop(ctxt, em_bsf);\r\n}\r\nstatic int em_bsr_c(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (ctxt->src.val == 0)\r\nctxt->dst.type = OP_NONE;\r\nreturn fastop(ctxt, em_bsr);\r\n}\r\nstatic __always_inline u8 test_cc(unsigned int condition, unsigned long flags)\r\n{\r\nu8 rc;\r\nvoid (*fop)(void) = (void *)em_setcc + 4 * (condition & 0xf);\r\nflags = (flags & EFLAGS_MASK) | X86_EFLAGS_IF;\r\nasm("push %[flags]; popf; call *%[fastop]"\r\n: "=a"(rc) : [fastop]"r"(fop), [flags]"r"(flags));\r\nreturn rc;\r\n}\r\nstatic void fetch_register_operand(struct operand *op)\r\n{\r\nswitch (op->bytes) {\r\ncase 1:\r\nop->val = *(u8 *)op->addr.reg;\r\nbreak;\r\ncase 2:\r\nop->val = *(u16 *)op->addr.reg;\r\nbreak;\r\ncase 4:\r\nop->val = *(u32 *)op->addr.reg;\r\nbreak;\r\ncase 8:\r\nop->val = *(u64 *)op->addr.reg;\r\nbreak;\r\n}\r\n}\r\nstatic void read_sse_reg(struct x86_emulate_ctxt *ctxt, sse128_t *data, int reg)\r\n{\r\nctxt->ops->get_fpu(ctxt);\r\nswitch (reg) {\r\ncase 0: asm("movdqa %%xmm0, %0" : "=m"(*data)); break;\r\ncase 1: asm("movdqa %%xmm1, %0" : "=m"(*data)); break;\r\ncase 2: asm("movdqa %%xmm2, %0" : "=m"(*data)); break;\r\ncase 3: asm("movdqa %%xmm3, %0" : "=m"(*data)); break;\r\ncase 4: asm("movdqa %%xmm4, %0" : "=m"(*data)); break;\r\ncase 5: asm("movdqa %%xmm5, %0" : "=m"(*data)); break;\r\ncase 6: asm("movdqa %%xmm6, %0" : "=m"(*data)); break;\r\ncase 7: asm("movdqa %%xmm7, %0" : "=m"(*data)); break;\r\n#ifdef CONFIG_X86_64\r\ncase 8: asm("movdqa %%xmm8, %0" : "=m"(*data)); break;\r\ncase 9: asm("movdqa %%xmm9, %0" : "=m"(*data)); break;\r\ncase 10: asm("movdqa %%xmm10, %0" : "=m"(*data)); break;\r\ncase 11: asm("movdqa %%xmm11, %0" : "=m"(*data)); break;\r\ncase 12: asm("movdqa %%xmm12, %0" : "=m"(*data)); break;\r\ncase 13: asm("movdqa %%xmm13, %0" : "=m"(*data)); break;\r\ncase 14: asm("movdqa %%xmm14, %0" : "=m"(*data)); break;\r\ncase 15: asm("movdqa %%xmm15, %0" : "=m"(*data)); break;\r\n#endif\r\ndefault: BUG();\r\n}\r\nctxt->ops->put_fpu(ctxt);\r\n}\r\nstatic void write_sse_reg(struct x86_emulate_ctxt *ctxt, sse128_t *data,\r\nint reg)\r\n{\r\nctxt->ops->get_fpu(ctxt);\r\nswitch (reg) {\r\ncase 0: asm("movdqa %0, %%xmm0" : : "m"(*data)); break;\r\ncase 1: asm("movdqa %0, %%xmm1" : : "m"(*data)); break;\r\ncase 2: asm("movdqa %0, %%xmm2" : : "m"(*data)); break;\r\ncase 3: asm("movdqa %0, %%xmm3" : : "m"(*data)); break;\r\ncase 4: asm("movdqa %0, %%xmm4" : : "m"(*data)); break;\r\ncase 5: asm("movdqa %0, %%xmm5" : : "m"(*data)); break;\r\ncase 6: asm("movdqa %0, %%xmm6" : : "m"(*data)); break;\r\ncase 7: asm("movdqa %0, %%xmm7" : : "m"(*data)); break;\r\n#ifdef CONFIG_X86_64\r\ncase 8: asm("movdqa %0, %%xmm8" : : "m"(*data)); break;\r\ncase 9: asm("movdqa %0, %%xmm9" : : "m"(*data)); break;\r\ncase 10: asm("movdqa %0, %%xmm10" : : "m"(*data)); break;\r\ncase 11: asm("movdqa %0, %%xmm11" : : "m"(*data)); break;\r\ncase 12: asm("movdqa %0, %%xmm12" : : "m"(*data)); break;\r\ncase 13: asm("movdqa %0, %%xmm13" : : "m"(*data)); break;\r\ncase 14: asm("movdqa %0, %%xmm14" : : "m"(*data)); break;\r\ncase 15: asm("movdqa %0, %%xmm15" : : "m"(*data)); break;\r\n#endif\r\ndefault: BUG();\r\n}\r\nctxt->ops->put_fpu(ctxt);\r\n}\r\nstatic void read_mmx_reg(struct x86_emulate_ctxt *ctxt, u64 *data, int reg)\r\n{\r\nctxt->ops->get_fpu(ctxt);\r\nswitch (reg) {\r\ncase 0: asm("movq %%mm0, %0" : "=m"(*data)); break;\r\ncase 1: asm("movq %%mm1, %0" : "=m"(*data)); break;\r\ncase 2: asm("movq %%mm2, %0" : "=m"(*data)); break;\r\ncase 3: asm("movq %%mm3, %0" : "=m"(*data)); break;\r\ncase 4: asm("movq %%mm4, %0" : "=m"(*data)); break;\r\ncase 5: asm("movq %%mm5, %0" : "=m"(*data)); break;\r\ncase 6: asm("movq %%mm6, %0" : "=m"(*data)); break;\r\ncase 7: asm("movq %%mm7, %0" : "=m"(*data)); break;\r\ndefault: BUG();\r\n}\r\nctxt->ops->put_fpu(ctxt);\r\n}\r\nstatic void write_mmx_reg(struct x86_emulate_ctxt *ctxt, u64 *data, int reg)\r\n{\r\nctxt->ops->get_fpu(ctxt);\r\nswitch (reg) {\r\ncase 0: asm("movq %0, %%mm0" : : "m"(*data)); break;\r\ncase 1: asm("movq %0, %%mm1" : : "m"(*data)); break;\r\ncase 2: asm("movq %0, %%mm2" : : "m"(*data)); break;\r\ncase 3: asm("movq %0, %%mm3" : : "m"(*data)); break;\r\ncase 4: asm("movq %0, %%mm4" : : "m"(*data)); break;\r\ncase 5: asm("movq %0, %%mm5" : : "m"(*data)); break;\r\ncase 6: asm("movq %0, %%mm6" : : "m"(*data)); break;\r\ncase 7: asm("movq %0, %%mm7" : : "m"(*data)); break;\r\ndefault: BUG();\r\n}\r\nctxt->ops->put_fpu(ctxt);\r\n}\r\nstatic int em_fninit(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (ctxt->ops->get_cr(ctxt, 0) & (X86_CR0_TS | X86_CR0_EM))\r\nreturn emulate_nm(ctxt);\r\nctxt->ops->get_fpu(ctxt);\r\nasm volatile("fninit");\r\nctxt->ops->put_fpu(ctxt);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_fnstcw(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu16 fcw;\r\nif (ctxt->ops->get_cr(ctxt, 0) & (X86_CR0_TS | X86_CR0_EM))\r\nreturn emulate_nm(ctxt);\r\nctxt->ops->get_fpu(ctxt);\r\nasm volatile("fnstcw %0": "+m"(fcw));\r\nctxt->ops->put_fpu(ctxt);\r\nctxt->dst.val = fcw;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_fnstsw(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu16 fsw;\r\nif (ctxt->ops->get_cr(ctxt, 0) & (X86_CR0_TS | X86_CR0_EM))\r\nreturn emulate_nm(ctxt);\r\nctxt->ops->get_fpu(ctxt);\r\nasm volatile("fnstsw %0": "+m"(fsw));\r\nctxt->ops->put_fpu(ctxt);\r\nctxt->dst.val = fsw;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic void decode_register_operand(struct x86_emulate_ctxt *ctxt,\r\nstruct operand *op)\r\n{\r\nunsigned reg = ctxt->modrm_reg;\r\nif (!(ctxt->d & ModRM))\r\nreg = (ctxt->b & 7) | ((ctxt->rex_prefix & 1) << 3);\r\nif (ctxt->d & Sse) {\r\nop->type = OP_XMM;\r\nop->bytes = 16;\r\nop->addr.xmm = reg;\r\nread_sse_reg(ctxt, &op->vec_val, reg);\r\nreturn;\r\n}\r\nif (ctxt->d & Mmx) {\r\nreg &= 7;\r\nop->type = OP_MM;\r\nop->bytes = 8;\r\nop->addr.mm = reg;\r\nreturn;\r\n}\r\nop->type = OP_REG;\r\nop->bytes = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nop->addr.reg = decode_register(ctxt, reg, ctxt->d & ByteOp);\r\nfetch_register_operand(op);\r\nop->orig_val = op->val;\r\n}\r\nstatic void adjust_modrm_seg(struct x86_emulate_ctxt *ctxt, int base_reg)\r\n{\r\nif (base_reg == VCPU_REGS_RSP || base_reg == VCPU_REGS_RBP)\r\nctxt->modrm_seg = VCPU_SREG_SS;\r\n}\r\nstatic int decode_modrm(struct x86_emulate_ctxt *ctxt,\r\nstruct operand *op)\r\n{\r\nu8 sib;\r\nint index_reg, base_reg, scale;\r\nint rc = X86EMUL_CONTINUE;\r\nulong modrm_ea = 0;\r\nctxt->modrm_reg = ((ctxt->rex_prefix << 1) & 8);\r\nindex_reg = (ctxt->rex_prefix << 2) & 8;\r\nbase_reg = (ctxt->rex_prefix << 3) & 8;\r\nctxt->modrm_mod = (ctxt->modrm & 0xc0) >> 6;\r\nctxt->modrm_reg |= (ctxt->modrm & 0x38) >> 3;\r\nctxt->modrm_rm = base_reg | (ctxt->modrm & 0x07);\r\nctxt->modrm_seg = VCPU_SREG_DS;\r\nif (ctxt->modrm_mod == 3 || (ctxt->d & NoMod)) {\r\nop->type = OP_REG;\r\nop->bytes = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nop->addr.reg = decode_register(ctxt, ctxt->modrm_rm,\r\nctxt->d & ByteOp);\r\nif (ctxt->d & Sse) {\r\nop->type = OP_XMM;\r\nop->bytes = 16;\r\nop->addr.xmm = ctxt->modrm_rm;\r\nread_sse_reg(ctxt, &op->vec_val, ctxt->modrm_rm);\r\nreturn rc;\r\n}\r\nif (ctxt->d & Mmx) {\r\nop->type = OP_MM;\r\nop->bytes = 8;\r\nop->addr.mm = ctxt->modrm_rm & 7;\r\nreturn rc;\r\n}\r\nfetch_register_operand(op);\r\nreturn rc;\r\n}\r\nop->type = OP_MEM;\r\nif (ctxt->ad_bytes == 2) {\r\nunsigned bx = reg_read(ctxt, VCPU_REGS_RBX);\r\nunsigned bp = reg_read(ctxt, VCPU_REGS_RBP);\r\nunsigned si = reg_read(ctxt, VCPU_REGS_RSI);\r\nunsigned di = reg_read(ctxt, VCPU_REGS_RDI);\r\nswitch (ctxt->modrm_mod) {\r\ncase 0:\r\nif (ctxt->modrm_rm == 6)\r\nmodrm_ea += insn_fetch(u16, ctxt);\r\nbreak;\r\ncase 1:\r\nmodrm_ea += insn_fetch(s8, ctxt);\r\nbreak;\r\ncase 2:\r\nmodrm_ea += insn_fetch(u16, ctxt);\r\nbreak;\r\n}\r\nswitch (ctxt->modrm_rm) {\r\ncase 0:\r\nmodrm_ea += bx + si;\r\nbreak;\r\ncase 1:\r\nmodrm_ea += bx + di;\r\nbreak;\r\ncase 2:\r\nmodrm_ea += bp + si;\r\nbreak;\r\ncase 3:\r\nmodrm_ea += bp + di;\r\nbreak;\r\ncase 4:\r\nmodrm_ea += si;\r\nbreak;\r\ncase 5:\r\nmodrm_ea += di;\r\nbreak;\r\ncase 6:\r\nif (ctxt->modrm_mod != 0)\r\nmodrm_ea += bp;\r\nbreak;\r\ncase 7:\r\nmodrm_ea += bx;\r\nbreak;\r\n}\r\nif (ctxt->modrm_rm == 2 || ctxt->modrm_rm == 3 ||\r\n(ctxt->modrm_rm == 6 && ctxt->modrm_mod != 0))\r\nctxt->modrm_seg = VCPU_SREG_SS;\r\nmodrm_ea = (u16)modrm_ea;\r\n} else {\r\nif ((ctxt->modrm_rm & 7) == 4) {\r\nsib = insn_fetch(u8, ctxt);\r\nindex_reg |= (sib >> 3) & 7;\r\nbase_reg |= sib & 7;\r\nscale = sib >> 6;\r\nif ((base_reg & 7) == 5 && ctxt->modrm_mod == 0)\r\nmodrm_ea += insn_fetch(s32, ctxt);\r\nelse {\r\nmodrm_ea += reg_read(ctxt, base_reg);\r\nadjust_modrm_seg(ctxt, base_reg);\r\nif ((ctxt->d & IncSP) &&\r\nbase_reg == VCPU_REGS_RSP)\r\nmodrm_ea += ctxt->op_bytes;\r\n}\r\nif (index_reg != 4)\r\nmodrm_ea += reg_read(ctxt, index_reg) << scale;\r\n} else if ((ctxt->modrm_rm & 7) == 5 && ctxt->modrm_mod == 0) {\r\nmodrm_ea += insn_fetch(s32, ctxt);\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nctxt->rip_relative = 1;\r\n} else {\r\nbase_reg = ctxt->modrm_rm;\r\nmodrm_ea += reg_read(ctxt, base_reg);\r\nadjust_modrm_seg(ctxt, base_reg);\r\n}\r\nswitch (ctxt->modrm_mod) {\r\ncase 1:\r\nmodrm_ea += insn_fetch(s8, ctxt);\r\nbreak;\r\ncase 2:\r\nmodrm_ea += insn_fetch(s32, ctxt);\r\nbreak;\r\n}\r\n}\r\nop->addr.mem.ea = modrm_ea;\r\nif (ctxt->ad_bytes != 8)\r\nctxt->memop.addr.mem.ea = (u32)ctxt->memop.addr.mem.ea;\r\ndone:\r\nreturn rc;\r\n}\r\nstatic int decode_abs(struct x86_emulate_ctxt *ctxt,\r\nstruct operand *op)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nop->type = OP_MEM;\r\nswitch (ctxt->ad_bytes) {\r\ncase 2:\r\nop->addr.mem.ea = insn_fetch(u16, ctxt);\r\nbreak;\r\ncase 4:\r\nop->addr.mem.ea = insn_fetch(u32, ctxt);\r\nbreak;\r\ncase 8:\r\nop->addr.mem.ea = insn_fetch(u64, ctxt);\r\nbreak;\r\n}\r\ndone:\r\nreturn rc;\r\n}\r\nstatic void fetch_bit_operand(struct x86_emulate_ctxt *ctxt)\r\n{\r\nlong sv = 0, mask;\r\nif (ctxt->dst.type == OP_MEM && ctxt->src.type == OP_REG) {\r\nmask = ~((long)ctxt->dst.bytes * 8 - 1);\r\nif (ctxt->src.bytes == 2)\r\nsv = (s16)ctxt->src.val & (s16)mask;\r\nelse if (ctxt->src.bytes == 4)\r\nsv = (s32)ctxt->src.val & (s32)mask;\r\nelse\r\nsv = (s64)ctxt->src.val & (s64)mask;\r\nctxt->dst.addr.mem.ea = address_mask(ctxt,\r\nctxt->dst.addr.mem.ea + (sv >> 3));\r\n}\r\nctxt->src.val &= (ctxt->dst.bytes << 3) - 1;\r\n}\r\nstatic int read_emulated(struct x86_emulate_ctxt *ctxt,\r\nunsigned long addr, void *dest, unsigned size)\r\n{\r\nint rc;\r\nstruct read_cache *mc = &ctxt->mem_read;\r\nif (mc->pos < mc->end)\r\ngoto read_cached;\r\nWARN_ON((mc->end + size) >= sizeof(mc->data));\r\nrc = ctxt->ops->read_emulated(ctxt, addr, mc->data + mc->end, size,\r\n&ctxt->exception);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nmc->end += size;\r\nread_cached:\r\nmemcpy(dest, mc->data + mc->pos, size);\r\nmc->pos += size;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int segmented_read(struct x86_emulate_ctxt *ctxt,\r\nstruct segmented_address addr,\r\nvoid *data,\r\nunsigned size)\r\n{\r\nint rc;\r\nulong linear;\r\nrc = linearize(ctxt, addr, size, false, &linear);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn read_emulated(ctxt, linear, data, size);\r\n}\r\nstatic int segmented_write(struct x86_emulate_ctxt *ctxt,\r\nstruct segmented_address addr,\r\nconst void *data,\r\nunsigned size)\r\n{\r\nint rc;\r\nulong linear;\r\nrc = linearize(ctxt, addr, size, true, &linear);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn ctxt->ops->write_emulated(ctxt, linear, data, size,\r\n&ctxt->exception);\r\n}\r\nstatic int segmented_cmpxchg(struct x86_emulate_ctxt *ctxt,\r\nstruct segmented_address addr,\r\nconst void *orig_data, const void *data,\r\nunsigned size)\r\n{\r\nint rc;\r\nulong linear;\r\nrc = linearize(ctxt, addr, size, true, &linear);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn ctxt->ops->cmpxchg_emulated(ctxt, linear, orig_data, data,\r\nsize, &ctxt->exception);\r\n}\r\nstatic int pio_in_emulated(struct x86_emulate_ctxt *ctxt,\r\nunsigned int size, unsigned short port,\r\nvoid *dest)\r\n{\r\nstruct read_cache *rc = &ctxt->io_read;\r\nif (rc->pos == rc->end) {\r\nunsigned int in_page, n;\r\nunsigned int count = ctxt->rep_prefix ?\r\naddress_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) : 1;\r\nin_page = (ctxt->eflags & X86_EFLAGS_DF) ?\r\noffset_in_page(reg_read(ctxt, VCPU_REGS_RDI)) :\r\nPAGE_SIZE - offset_in_page(reg_read(ctxt, VCPU_REGS_RDI));\r\nn = min3(in_page, (unsigned int)sizeof(rc->data) / size, count);\r\nif (n == 0)\r\nn = 1;\r\nrc->pos = rc->end = 0;\r\nif (!ctxt->ops->pio_in_emulated(ctxt, size, port, rc->data, n))\r\nreturn 0;\r\nrc->end = n * size;\r\n}\r\nif (ctxt->rep_prefix && (ctxt->d & String) &&\r\n!(ctxt->eflags & X86_EFLAGS_DF)) {\r\nctxt->dst.data = rc->data + rc->pos;\r\nctxt->dst.type = OP_MEM_STR;\r\nctxt->dst.count = (rc->end - rc->pos) / size;\r\nrc->pos = rc->end;\r\n} else {\r\nmemcpy(dest, rc->data + rc->pos, size);\r\nrc->pos += size;\r\n}\r\nreturn 1;\r\n}\r\nstatic int read_interrupt_descriptor(struct x86_emulate_ctxt *ctxt,\r\nu16 index, struct desc_struct *desc)\r\n{\r\nstruct desc_ptr dt;\r\nulong addr;\r\nctxt->ops->get_idt(ctxt, &dt);\r\nif (dt.size < index * 8 + 7)\r\nreturn emulate_gp(ctxt, index << 3 | 0x2);\r\naddr = dt.address + index * 8;\r\nreturn ctxt->ops->read_std(ctxt, addr, desc, sizeof *desc,\r\n&ctxt->exception);\r\n}\r\nstatic void get_descriptor_table_ptr(struct x86_emulate_ctxt *ctxt,\r\nu16 selector, struct desc_ptr *dt)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nu32 base3 = 0;\r\nif (selector & 1 << 2) {\r\nstruct desc_struct desc;\r\nu16 sel;\r\nmemset (dt, 0, sizeof *dt);\r\nif (!ops->get_segment(ctxt, &sel, &desc, &base3,\r\nVCPU_SREG_LDTR))\r\nreturn;\r\ndt->size = desc_limit_scaled(&desc);\r\ndt->address = get_desc_base(&desc) | ((u64)base3 << 32);\r\n} else\r\nops->get_gdt(ctxt, dt);\r\n}\r\nstatic int get_descriptor_ptr(struct x86_emulate_ctxt *ctxt,\r\nu16 selector, ulong *desc_addr_p)\r\n{\r\nstruct desc_ptr dt;\r\nu16 index = selector >> 3;\r\nulong addr;\r\nget_descriptor_table_ptr(ctxt, selector, &dt);\r\nif (dt.size < index * 8 + 7)\r\nreturn emulate_gp(ctxt, selector & 0xfffc);\r\naddr = dt.address + index * 8;\r\n#ifdef CONFIG_X86_64\r\nif (addr >> 32 != 0) {\r\nu64 efer = 0;\r\nctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\r\nif (!(efer & EFER_LMA))\r\naddr &= (u32)-1;\r\n}\r\n#endif\r\n*desc_addr_p = addr;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int read_segment_descriptor(struct x86_emulate_ctxt *ctxt,\r\nu16 selector, struct desc_struct *desc,\r\nulong *desc_addr_p)\r\n{\r\nint rc;\r\nrc = get_descriptor_ptr(ctxt, selector, desc_addr_p);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn ctxt->ops->read_std(ctxt, *desc_addr_p, desc, sizeof(*desc),\r\n&ctxt->exception);\r\n}\r\nstatic int write_segment_descriptor(struct x86_emulate_ctxt *ctxt,\r\nu16 selector, struct desc_struct *desc)\r\n{\r\nint rc;\r\nulong addr;\r\nrc = get_descriptor_ptr(ctxt, selector, &addr);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn ctxt->ops->write_std(ctxt, addr, desc, sizeof *desc,\r\n&ctxt->exception);\r\n}\r\nstatic int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\r\nu16 selector, int seg, u8 cpl,\r\nenum x86_transfer_type transfer,\r\nstruct desc_struct *desc)\r\n{\r\nstruct desc_struct seg_desc, old_desc;\r\nu8 dpl, rpl;\r\nunsigned err_vec = GP_VECTOR;\r\nu32 err_code = 0;\r\nbool null_selector = !(selector & ~0x3);\r\nulong desc_addr;\r\nint ret;\r\nu16 dummy;\r\nu32 base3 = 0;\r\nmemset(&seg_desc, 0, sizeof seg_desc);\r\nif (ctxt->mode == X86EMUL_MODE_REAL) {\r\nctxt->ops->get_segment(ctxt, &dummy, &seg_desc, NULL, seg);\r\nset_desc_base(&seg_desc, selector << 4);\r\ngoto load;\r\n} else if (seg <= VCPU_SREG_GS && ctxt->mode == X86EMUL_MODE_VM86) {\r\nset_desc_base(&seg_desc, selector << 4);\r\nset_desc_limit(&seg_desc, 0xffff);\r\nseg_desc.type = 3;\r\nseg_desc.p = 1;\r\nseg_desc.s = 1;\r\nseg_desc.dpl = 3;\r\ngoto load;\r\n}\r\nrpl = selector & 3;\r\nif ((seg == VCPU_SREG_CS\r\n|| (seg == VCPU_SREG_SS\r\n&& (ctxt->mode != X86EMUL_MODE_PROT64 || rpl != cpl))\r\n|| seg == VCPU_SREG_TR)\r\n&& null_selector)\r\ngoto exception;\r\nif (seg == VCPU_SREG_TR && (selector & (1 << 2)))\r\ngoto exception;\r\nif (null_selector)\r\ngoto load;\r\nret = read_segment_descriptor(ctxt, selector, &seg_desc, &desc_addr);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nerr_code = selector & 0xfffc;\r\nerr_vec = (transfer == X86_TRANSFER_TASK_SWITCH) ? TS_VECTOR :\r\nGP_VECTOR;\r\nif (seg <= VCPU_SREG_GS && !seg_desc.s) {\r\nif (transfer == X86_TRANSFER_CALL_JMP)\r\nreturn X86EMUL_UNHANDLEABLE;\r\ngoto exception;\r\n}\r\nif (!seg_desc.p) {\r\nerr_vec = (seg == VCPU_SREG_SS) ? SS_VECTOR : NP_VECTOR;\r\ngoto exception;\r\n}\r\ndpl = seg_desc.dpl;\r\nswitch (seg) {\r\ncase VCPU_SREG_SS:\r\nif (rpl != cpl || (seg_desc.type & 0xa) != 0x2 || dpl != cpl)\r\ngoto exception;\r\nbreak;\r\ncase VCPU_SREG_CS:\r\nif (!(seg_desc.type & 8))\r\ngoto exception;\r\nif (seg_desc.type & 4) {\r\nif (dpl > cpl)\r\ngoto exception;\r\n} else {\r\nif (rpl > cpl || dpl != cpl)\r\ngoto exception;\r\n}\r\nif (seg_desc.d && seg_desc.l) {\r\nu64 efer = 0;\r\nctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\r\nif (efer & EFER_LMA)\r\ngoto exception;\r\n}\r\nselector = (selector & 0xfffc) | cpl;\r\nbreak;\r\ncase VCPU_SREG_TR:\r\nif (seg_desc.s || (seg_desc.type != 1 && seg_desc.type != 9))\r\ngoto exception;\r\nold_desc = seg_desc;\r\nseg_desc.type |= 2;\r\nret = ctxt->ops->cmpxchg_emulated(ctxt, desc_addr, &old_desc, &seg_desc,\r\nsizeof(seg_desc), &ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nbreak;\r\ncase VCPU_SREG_LDTR:\r\nif (seg_desc.s || seg_desc.type != 2)\r\ngoto exception;\r\nbreak;\r\ndefault:\r\nif ((seg_desc.type & 0xa) == 0x8 ||\r\n(((seg_desc.type & 0xc) != 0xc) &&\r\n(rpl > dpl && cpl > dpl)))\r\ngoto exception;\r\nbreak;\r\n}\r\nif (seg_desc.s) {\r\nif (!(seg_desc.type & 1)) {\r\nseg_desc.type |= 1;\r\nret = write_segment_descriptor(ctxt, selector,\r\n&seg_desc);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\n}\r\n} else if (ctxt->mode == X86EMUL_MODE_PROT64) {\r\nret = ctxt->ops->read_std(ctxt, desc_addr+8, &base3,\r\nsizeof(base3), &ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nif (is_noncanonical_address(get_desc_base(&seg_desc) |\r\n((u64)base3 << 32)))\r\nreturn emulate_gp(ctxt, 0);\r\n}\r\nload:\r\nctxt->ops->set_segment(ctxt, selector, &seg_desc, base3, seg);\r\nif (desc)\r\n*desc = seg_desc;\r\nreturn X86EMUL_CONTINUE;\r\nexception:\r\nreturn emulate_exception(ctxt, err_vec, err_code, true);\r\n}\r\nstatic int load_segment_descriptor(struct x86_emulate_ctxt *ctxt,\r\nu16 selector, int seg)\r\n{\r\nu8 cpl = ctxt->ops->cpl(ctxt);\r\nreturn __load_segment_descriptor(ctxt, selector, seg, cpl,\r\nX86_TRANSFER_NONE, NULL);\r\n}\r\nstatic void write_register_operand(struct operand *op)\r\n{\r\nreturn assign_register(op->addr.reg, op->val, op->bytes);\r\n}\r\nstatic int writeback(struct x86_emulate_ctxt *ctxt, struct operand *op)\r\n{\r\nswitch (op->type) {\r\ncase OP_REG:\r\nwrite_register_operand(op);\r\nbreak;\r\ncase OP_MEM:\r\nif (ctxt->lock_prefix)\r\nreturn segmented_cmpxchg(ctxt,\r\nop->addr.mem,\r\n&op->orig_val,\r\n&op->val,\r\nop->bytes);\r\nelse\r\nreturn segmented_write(ctxt,\r\nop->addr.mem,\r\n&op->val,\r\nop->bytes);\r\nbreak;\r\ncase OP_MEM_STR:\r\nreturn segmented_write(ctxt,\r\nop->addr.mem,\r\nop->data,\r\nop->bytes * op->count);\r\nbreak;\r\ncase OP_XMM:\r\nwrite_sse_reg(ctxt, &op->vec_val, op->addr.xmm);\r\nbreak;\r\ncase OP_MM:\r\nwrite_mmx_reg(ctxt, &op->mm_val, op->addr.mm);\r\nbreak;\r\ncase OP_NONE:\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int push(struct x86_emulate_ctxt *ctxt, void *data, int bytes)\r\n{\r\nstruct segmented_address addr;\r\nrsp_increment(ctxt, -bytes);\r\naddr.ea = reg_read(ctxt, VCPU_REGS_RSP) & stack_mask(ctxt);\r\naddr.seg = VCPU_SREG_SS;\r\nreturn segmented_write(ctxt, addr, data, bytes);\r\n}\r\nstatic int em_push(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->dst.type = OP_NONE;\r\nreturn push(ctxt, &ctxt->src.val, ctxt->op_bytes);\r\n}\r\nstatic int emulate_pop(struct x86_emulate_ctxt *ctxt,\r\nvoid *dest, int len)\r\n{\r\nint rc;\r\nstruct segmented_address addr;\r\naddr.ea = reg_read(ctxt, VCPU_REGS_RSP) & stack_mask(ctxt);\r\naddr.seg = VCPU_SREG_SS;\r\nrc = segmented_read(ctxt, addr, dest, len);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrsp_increment(ctxt, len);\r\nreturn rc;\r\n}\r\nstatic int em_pop(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn emulate_pop(ctxt, &ctxt->dst.val, ctxt->op_bytes);\r\n}\r\nstatic int emulate_popf(struct x86_emulate_ctxt *ctxt,\r\nvoid *dest, int len)\r\n{\r\nint rc;\r\nunsigned long val, change_mask;\r\nint iopl = (ctxt->eflags & X86_EFLAGS_IOPL) >> X86_EFLAGS_IOPL_BIT;\r\nint cpl = ctxt->ops->cpl(ctxt);\r\nrc = emulate_pop(ctxt, &val, len);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nchange_mask = X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\r\nX86_EFLAGS_ZF | X86_EFLAGS_SF | X86_EFLAGS_OF |\r\nX86_EFLAGS_TF | X86_EFLAGS_DF | X86_EFLAGS_NT |\r\nX86_EFLAGS_AC | X86_EFLAGS_ID;\r\nswitch(ctxt->mode) {\r\ncase X86EMUL_MODE_PROT64:\r\ncase X86EMUL_MODE_PROT32:\r\ncase X86EMUL_MODE_PROT16:\r\nif (cpl == 0)\r\nchange_mask |= X86_EFLAGS_IOPL;\r\nif (cpl <= iopl)\r\nchange_mask |= X86_EFLAGS_IF;\r\nbreak;\r\ncase X86EMUL_MODE_VM86:\r\nif (iopl < 3)\r\nreturn emulate_gp(ctxt, 0);\r\nchange_mask |= X86_EFLAGS_IF;\r\nbreak;\r\ndefault:\r\nchange_mask |= (X86_EFLAGS_IOPL | X86_EFLAGS_IF);\r\nbreak;\r\n}\r\n*(unsigned long *)dest =\r\n(ctxt->eflags & ~change_mask) | (val & change_mask);\r\nreturn rc;\r\n}\r\nstatic int em_popf(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->dst.type = OP_REG;\r\nctxt->dst.addr.reg = &ctxt->eflags;\r\nctxt->dst.bytes = ctxt->op_bytes;\r\nreturn emulate_popf(ctxt, &ctxt->dst.val, ctxt->op_bytes);\r\n}\r\nstatic int em_enter(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nunsigned frame_size = ctxt->src.val;\r\nunsigned nesting_level = ctxt->src2.val & 31;\r\nulong rbp;\r\nif (nesting_level)\r\nreturn X86EMUL_UNHANDLEABLE;\r\nrbp = reg_read(ctxt, VCPU_REGS_RBP);\r\nrc = push(ctxt, &rbp, stack_size(ctxt));\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nassign_masked(reg_rmw(ctxt, VCPU_REGS_RBP), reg_read(ctxt, VCPU_REGS_RSP),\r\nstack_mask(ctxt));\r\nassign_masked(reg_rmw(ctxt, VCPU_REGS_RSP),\r\nreg_read(ctxt, VCPU_REGS_RSP) - frame_size,\r\nstack_mask(ctxt));\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_leave(struct x86_emulate_ctxt *ctxt)\r\n{\r\nassign_masked(reg_rmw(ctxt, VCPU_REGS_RSP), reg_read(ctxt, VCPU_REGS_RBP),\r\nstack_mask(ctxt));\r\nreturn emulate_pop(ctxt, reg_rmw(ctxt, VCPU_REGS_RBP), ctxt->op_bytes);\r\n}\r\nstatic int em_push_sreg(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint seg = ctxt->src2.val;\r\nctxt->src.val = get_segment_selector(ctxt, seg);\r\nif (ctxt->op_bytes == 4) {\r\nrsp_increment(ctxt, -2);\r\nctxt->op_bytes = 2;\r\n}\r\nreturn em_push(ctxt);\r\n}\r\nstatic int em_pop_sreg(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint seg = ctxt->src2.val;\r\nunsigned long selector;\r\nint rc;\r\nrc = emulate_pop(ctxt, &selector, 2);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nif (ctxt->modrm_reg == VCPU_SREG_SS)\r\nctxt->interruptibility = KVM_X86_SHADOW_INT_MOV_SS;\r\nif (ctxt->op_bytes > 2)\r\nrsp_increment(ctxt, ctxt->op_bytes - 2);\r\nrc = load_segment_descriptor(ctxt, (u16)selector, seg);\r\nreturn rc;\r\n}\r\nstatic int em_pusha(struct x86_emulate_ctxt *ctxt)\r\n{\r\nunsigned long old_esp = reg_read(ctxt, VCPU_REGS_RSP);\r\nint rc = X86EMUL_CONTINUE;\r\nint reg = VCPU_REGS_RAX;\r\nwhile (reg <= VCPU_REGS_RDI) {\r\n(reg == VCPU_REGS_RSP) ?\r\n(ctxt->src.val = old_esp) : (ctxt->src.val = reg_read(ctxt, reg));\r\nrc = em_push(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\n++reg;\r\n}\r\nreturn rc;\r\n}\r\nstatic int em_pushf(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->src.val = (unsigned long)ctxt->eflags & ~X86_EFLAGS_VM;\r\nreturn em_push(ctxt);\r\n}\r\nstatic int em_popa(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nint reg = VCPU_REGS_RDI;\r\nu32 val;\r\nwhile (reg >= VCPU_REGS_RAX) {\r\nif (reg == VCPU_REGS_RSP) {\r\nrsp_increment(ctxt, ctxt->op_bytes);\r\n--reg;\r\n}\r\nrc = emulate_pop(ctxt, &val, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nbreak;\r\nassign_register(reg_rmw(ctxt, reg), val, ctxt->op_bytes);\r\n--reg;\r\n}\r\nreturn rc;\r\n}\r\nstatic int __emulate_int_real(struct x86_emulate_ctxt *ctxt, int irq)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nint rc;\r\nstruct desc_ptr dt;\r\ngva_t cs_addr;\r\ngva_t eip_addr;\r\nu16 cs, eip;\r\nctxt->src.val = ctxt->eflags;\r\nrc = em_push(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nctxt->eflags &= ~(X86_EFLAGS_IF | X86_EFLAGS_TF | X86_EFLAGS_AC);\r\nctxt->src.val = get_segment_selector(ctxt, VCPU_SREG_CS);\r\nrc = em_push(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nctxt->src.val = ctxt->_eip;\r\nrc = em_push(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nops->get_idt(ctxt, &dt);\r\neip_addr = dt.address + (irq << 2);\r\ncs_addr = dt.address + (irq << 2) + 2;\r\nrc = ops->read_std(ctxt, cs_addr, &cs, 2, &ctxt->exception);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = ops->read_std(ctxt, eip_addr, &eip, 2, &ctxt->exception);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = load_segment_descriptor(ctxt, cs, VCPU_SREG_CS);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nctxt->_eip = eip;\r\nreturn rc;\r\n}\r\nint emulate_int_real(struct x86_emulate_ctxt *ctxt, int irq)\r\n{\r\nint rc;\r\ninvalidate_registers(ctxt);\r\nrc = __emulate_int_real(ctxt, irq);\r\nif (rc == X86EMUL_CONTINUE)\r\nwriteback_registers(ctxt);\r\nreturn rc;\r\n}\r\nstatic int emulate_int(struct x86_emulate_ctxt *ctxt, int irq)\r\n{\r\nswitch(ctxt->mode) {\r\ncase X86EMUL_MODE_REAL:\r\nreturn __emulate_int_real(ctxt, irq);\r\ncase X86EMUL_MODE_VM86:\r\ncase X86EMUL_MODE_PROT16:\r\ncase X86EMUL_MODE_PROT32:\r\ncase X86EMUL_MODE_PROT64:\r\ndefault:\r\nreturn X86EMUL_UNHANDLEABLE;\r\n}\r\n}\r\nstatic int emulate_iret_real(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nunsigned long temp_eip = 0;\r\nunsigned long temp_eflags = 0;\r\nunsigned long cs = 0;\r\nunsigned long mask = X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\r\nX86_EFLAGS_ZF | X86_EFLAGS_SF | X86_EFLAGS_TF |\r\nX86_EFLAGS_IF | X86_EFLAGS_DF | X86_EFLAGS_OF |\r\nX86_EFLAGS_IOPL | X86_EFLAGS_NT | X86_EFLAGS_RF |\r\nX86_EFLAGS_AC | X86_EFLAGS_ID |\r\nX86_EFLAGS_FIXED;\r\nunsigned long vm86_mask = X86_EFLAGS_VM | X86_EFLAGS_VIF |\r\nX86_EFLAGS_VIP;\r\nrc = emulate_pop(ctxt, &temp_eip, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nif (temp_eip & ~0xffff)\r\nreturn emulate_gp(ctxt, 0);\r\nrc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = emulate_pop(ctxt, &temp_eflags, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nctxt->_eip = temp_eip;\r\nif (ctxt->op_bytes == 4)\r\nctxt->eflags = ((temp_eflags & mask) | (ctxt->eflags & vm86_mask));\r\nelse if (ctxt->op_bytes == 2) {\r\nctxt->eflags &= ~0xffff;\r\nctxt->eflags |= temp_eflags;\r\n}\r\nctxt->eflags &= ~EFLG_RESERVED_ZEROS_MASK;\r\nctxt->eflags |= X86_EFLAGS_FIXED;\r\nctxt->ops->set_nmi_mask(ctxt, false);\r\nreturn rc;\r\n}\r\nstatic int em_iret(struct x86_emulate_ctxt *ctxt)\r\n{\r\nswitch(ctxt->mode) {\r\ncase X86EMUL_MODE_REAL:\r\nreturn emulate_iret_real(ctxt);\r\ncase X86EMUL_MODE_VM86:\r\ncase X86EMUL_MODE_PROT16:\r\ncase X86EMUL_MODE_PROT32:\r\ncase X86EMUL_MODE_PROT64:\r\ndefault:\r\nreturn X86EMUL_UNHANDLEABLE;\r\n}\r\n}\r\nstatic int em_jmp_far(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nunsigned short sel, old_sel;\r\nstruct desc_struct old_desc, new_desc;\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nu8 cpl = ctxt->ops->cpl(ctxt);\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nops->get_segment(ctxt, &old_sel, &old_desc, NULL,\r\nVCPU_SREG_CS);\r\nmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\r\nrc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\r\nX86_TRANSFER_CALL_JMP,\r\n&new_desc);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\r\nif (rc != X86EMUL_CONTINUE) {\r\nWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\r\nops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);\r\nreturn rc;\r\n}\r\nreturn rc;\r\n}\r\nstatic int em_jmp_abs(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn assign_eip_near(ctxt, ctxt->src.val);\r\n}\r\nstatic int em_call_near_abs(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nlong int old_eip;\r\nold_eip = ctxt->_eip;\r\nrc = assign_eip_near(ctxt, ctxt->src.val);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nctxt->src.val = old_eip;\r\nrc = em_push(ctxt);\r\nreturn rc;\r\n}\r\nstatic int em_cmpxchg8b(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 old = ctxt->dst.orig_val64;\r\nif (ctxt->dst.bytes == 16)\r\nreturn X86EMUL_UNHANDLEABLE;\r\nif (((u32) (old >> 0) != (u32) reg_read(ctxt, VCPU_REGS_RAX)) ||\r\n((u32) (old >> 32) != (u32) reg_read(ctxt, VCPU_REGS_RDX))) {\r\n*reg_write(ctxt, VCPU_REGS_RAX) = (u32) (old >> 0);\r\n*reg_write(ctxt, VCPU_REGS_RDX) = (u32) (old >> 32);\r\nctxt->eflags &= ~X86_EFLAGS_ZF;\r\n} else {\r\nctxt->dst.val64 = ((u64)reg_read(ctxt, VCPU_REGS_RCX) << 32) |\r\n(u32) reg_read(ctxt, VCPU_REGS_RBX);\r\nctxt->eflags |= X86_EFLAGS_ZF;\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_ret(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nunsigned long eip;\r\nrc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn assign_eip_near(ctxt, eip);\r\n}\r\nstatic int em_ret_far(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nunsigned long eip, cs;\r\nu16 old_cs;\r\nint cpl = ctxt->ops->cpl(ctxt);\r\nstruct desc_struct old_desc, new_desc;\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nops->get_segment(ctxt, &old_cs, &old_desc, NULL,\r\nVCPU_SREG_CS);\r\nrc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = emulate_pop(ctxt, &cs, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nif (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)\r\nreturn X86EMUL_UNHANDLEABLE;\r\nrc = __load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS, cpl,\r\nX86_TRANSFER_RET,\r\n&new_desc);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = assign_eip_far(ctxt, eip, &new_desc);\r\nif (rc != X86EMUL_CONTINUE) {\r\nWARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);\r\nops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\r\n}\r\nreturn rc;\r\n}\r\nstatic int em_ret_far_imm(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nrc = em_ret_far(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrsp_increment(ctxt, ctxt->src.val);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_cmpxchg(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->dst.orig_val = ctxt->dst.val;\r\nctxt->dst.val = reg_read(ctxt, VCPU_REGS_RAX);\r\nctxt->src.orig_val = ctxt->src.val;\r\nctxt->src.val = ctxt->dst.orig_val;\r\nfastop(ctxt, em_cmp);\r\nif (ctxt->eflags & X86_EFLAGS_ZF) {\r\nctxt->src.type = OP_NONE;\r\nctxt->dst.val = ctxt->src.orig_val;\r\n} else {\r\nctxt->src.type = OP_REG;\r\nctxt->src.addr.reg = reg_rmw(ctxt, VCPU_REGS_RAX);\r\nctxt->src.val = ctxt->dst.orig_val;\r\nctxt->dst.val = ctxt->dst.orig_val;\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_lseg(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint seg = ctxt->src2.val;\r\nunsigned short sel;\r\nint rc;\r\nmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\r\nrc = load_segment_descriptor(ctxt, sel, seg);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nctxt->dst.val = ctxt->src.val;\r\nreturn rc;\r\n}\r\nstatic int emulator_has_longmode(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu32 eax, ebx, ecx, edx;\r\neax = 0x80000001;\r\necx = 0;\r\nctxt->ops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx);\r\nreturn edx & bit(X86_FEATURE_LM);\r\n}\r\nstatic void rsm_set_desc_flags(struct desc_struct *desc, u32 flags)\r\n{\r\ndesc->g = (flags >> 23) & 1;\r\ndesc->d = (flags >> 22) & 1;\r\ndesc->l = (flags >> 21) & 1;\r\ndesc->avl = (flags >> 20) & 1;\r\ndesc->p = (flags >> 15) & 1;\r\ndesc->dpl = (flags >> 13) & 3;\r\ndesc->s = (flags >> 12) & 1;\r\ndesc->type = (flags >> 8) & 15;\r\n}\r\nstatic int rsm_load_seg_32(struct x86_emulate_ctxt *ctxt, u64 smbase, int n)\r\n{\r\nstruct desc_struct desc;\r\nint offset;\r\nu16 selector;\r\nselector = GET_SMSTATE(u32, smbase, 0x7fa8 + n * 4);\r\nif (n < 3)\r\noffset = 0x7f84 + n * 12;\r\nelse\r\noffset = 0x7f2c + (n - 3) * 12;\r\nset_desc_base(&desc, GET_SMSTATE(u32, smbase, offset + 8));\r\nset_desc_limit(&desc, GET_SMSTATE(u32, smbase, offset + 4));\r\nrsm_set_desc_flags(&desc, GET_SMSTATE(u32, smbase, offset));\r\nctxt->ops->set_segment(ctxt, selector, &desc, 0, n);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int rsm_load_seg_64(struct x86_emulate_ctxt *ctxt, u64 smbase, int n)\r\n{\r\nstruct desc_struct desc;\r\nint offset;\r\nu16 selector;\r\nu32 base3;\r\noffset = 0x7e00 + n * 16;\r\nselector = GET_SMSTATE(u16, smbase, offset);\r\nrsm_set_desc_flags(&desc, GET_SMSTATE(u16, smbase, offset + 2) << 8);\r\nset_desc_limit(&desc, GET_SMSTATE(u32, smbase, offset + 4));\r\nset_desc_base(&desc, GET_SMSTATE(u32, smbase, offset + 8));\r\nbase3 = GET_SMSTATE(u32, smbase, offset + 12);\r\nctxt->ops->set_segment(ctxt, selector, &desc, base3, n);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int rsm_enter_protected_mode(struct x86_emulate_ctxt *ctxt,\r\nu64 cr0, u64 cr4)\r\n{\r\nint bad;\r\nbad = ctxt->ops->set_cr(ctxt, 4, cr4 & ~X86_CR4_PCIDE);\r\nif (bad)\r\nreturn X86EMUL_UNHANDLEABLE;\r\nbad = ctxt->ops->set_cr(ctxt, 0, cr0);\r\nif (bad)\r\nreturn X86EMUL_UNHANDLEABLE;\r\nif (cr4 & X86_CR4_PCIDE) {\r\nbad = ctxt->ops->set_cr(ctxt, 4, cr4);\r\nif (bad)\r\nreturn X86EMUL_UNHANDLEABLE;\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int rsm_load_state_32(struct x86_emulate_ctxt *ctxt, u64 smbase)\r\n{\r\nstruct desc_struct desc;\r\nstruct desc_ptr dt;\r\nu16 selector;\r\nu32 val, cr0, cr4;\r\nint i;\r\ncr0 = GET_SMSTATE(u32, smbase, 0x7ffc);\r\nctxt->ops->set_cr(ctxt, 3, GET_SMSTATE(u32, smbase, 0x7ff8));\r\nctxt->eflags = GET_SMSTATE(u32, smbase, 0x7ff4) | X86_EFLAGS_FIXED;\r\nctxt->_eip = GET_SMSTATE(u32, smbase, 0x7ff0);\r\nfor (i = 0; i < 8; i++)\r\n*reg_write(ctxt, i) = GET_SMSTATE(u32, smbase, 0x7fd0 + i * 4);\r\nval = GET_SMSTATE(u32, smbase, 0x7fcc);\r\nctxt->ops->set_dr(ctxt, 6, (val & DR6_VOLATILE) | DR6_FIXED_1);\r\nval = GET_SMSTATE(u32, smbase, 0x7fc8);\r\nctxt->ops->set_dr(ctxt, 7, (val & DR7_VOLATILE) | DR7_FIXED_1);\r\nselector = GET_SMSTATE(u32, smbase, 0x7fc4);\r\nset_desc_base(&desc, GET_SMSTATE(u32, smbase, 0x7f64));\r\nset_desc_limit(&desc, GET_SMSTATE(u32, smbase, 0x7f60));\r\nrsm_set_desc_flags(&desc, GET_SMSTATE(u32, smbase, 0x7f5c));\r\nctxt->ops->set_segment(ctxt, selector, &desc, 0, VCPU_SREG_TR);\r\nselector = GET_SMSTATE(u32, smbase, 0x7fc0);\r\nset_desc_base(&desc, GET_SMSTATE(u32, smbase, 0x7f80));\r\nset_desc_limit(&desc, GET_SMSTATE(u32, smbase, 0x7f7c));\r\nrsm_set_desc_flags(&desc, GET_SMSTATE(u32, smbase, 0x7f78));\r\nctxt->ops->set_segment(ctxt, selector, &desc, 0, VCPU_SREG_LDTR);\r\ndt.address = GET_SMSTATE(u32, smbase, 0x7f74);\r\ndt.size = GET_SMSTATE(u32, smbase, 0x7f70);\r\nctxt->ops->set_gdt(ctxt, &dt);\r\ndt.address = GET_SMSTATE(u32, smbase, 0x7f58);\r\ndt.size = GET_SMSTATE(u32, smbase, 0x7f54);\r\nctxt->ops->set_idt(ctxt, &dt);\r\nfor (i = 0; i < 6; i++) {\r\nint r = rsm_load_seg_32(ctxt, smbase, i);\r\nif (r != X86EMUL_CONTINUE)\r\nreturn r;\r\n}\r\ncr4 = GET_SMSTATE(u32, smbase, 0x7f14);\r\nctxt->ops->set_smbase(ctxt, GET_SMSTATE(u32, smbase, 0x7ef8));\r\nreturn rsm_enter_protected_mode(ctxt, cr0, cr4);\r\n}\r\nstatic int rsm_load_state_64(struct x86_emulate_ctxt *ctxt, u64 smbase)\r\n{\r\nstruct desc_struct desc;\r\nstruct desc_ptr dt;\r\nu64 val, cr0, cr4;\r\nu32 base3;\r\nu16 selector;\r\nint i, r;\r\nfor (i = 0; i < 16; i++)\r\n*reg_write(ctxt, i) = GET_SMSTATE(u64, smbase, 0x7ff8 - i * 8);\r\nctxt->_eip = GET_SMSTATE(u64, smbase, 0x7f78);\r\nctxt->eflags = GET_SMSTATE(u32, smbase, 0x7f70) | X86_EFLAGS_FIXED;\r\nval = GET_SMSTATE(u32, smbase, 0x7f68);\r\nctxt->ops->set_dr(ctxt, 6, (val & DR6_VOLATILE) | DR6_FIXED_1);\r\nval = GET_SMSTATE(u32, smbase, 0x7f60);\r\nctxt->ops->set_dr(ctxt, 7, (val & DR7_VOLATILE) | DR7_FIXED_1);\r\ncr0 = GET_SMSTATE(u64, smbase, 0x7f58);\r\nctxt->ops->set_cr(ctxt, 3, GET_SMSTATE(u64, smbase, 0x7f50));\r\ncr4 = GET_SMSTATE(u64, smbase, 0x7f48);\r\nctxt->ops->set_smbase(ctxt, GET_SMSTATE(u32, smbase, 0x7f00));\r\nval = GET_SMSTATE(u64, smbase, 0x7ed0);\r\nctxt->ops->set_msr(ctxt, MSR_EFER, val & ~EFER_LMA);\r\nselector = GET_SMSTATE(u32, smbase, 0x7e90);\r\nrsm_set_desc_flags(&desc, GET_SMSTATE(u32, smbase, 0x7e92) << 8);\r\nset_desc_limit(&desc, GET_SMSTATE(u32, smbase, 0x7e94));\r\nset_desc_base(&desc, GET_SMSTATE(u32, smbase, 0x7e98));\r\nbase3 = GET_SMSTATE(u32, smbase, 0x7e9c);\r\nctxt->ops->set_segment(ctxt, selector, &desc, base3, VCPU_SREG_TR);\r\ndt.size = GET_SMSTATE(u32, smbase, 0x7e84);\r\ndt.address = GET_SMSTATE(u64, smbase, 0x7e88);\r\nctxt->ops->set_idt(ctxt, &dt);\r\nselector = GET_SMSTATE(u32, smbase, 0x7e70);\r\nrsm_set_desc_flags(&desc, GET_SMSTATE(u32, smbase, 0x7e72) << 8);\r\nset_desc_limit(&desc, GET_SMSTATE(u32, smbase, 0x7e74));\r\nset_desc_base(&desc, GET_SMSTATE(u32, smbase, 0x7e78));\r\nbase3 = GET_SMSTATE(u32, smbase, 0x7e7c);\r\nctxt->ops->set_segment(ctxt, selector, &desc, base3, VCPU_SREG_LDTR);\r\ndt.size = GET_SMSTATE(u32, smbase, 0x7e64);\r\ndt.address = GET_SMSTATE(u64, smbase, 0x7e68);\r\nctxt->ops->set_gdt(ctxt, &dt);\r\nr = rsm_enter_protected_mode(ctxt, cr0, cr4);\r\nif (r != X86EMUL_CONTINUE)\r\nreturn r;\r\nfor (i = 0; i < 6; i++) {\r\nr = rsm_load_seg_64(ctxt, smbase, i);\r\nif (r != X86EMUL_CONTINUE)\r\nreturn r;\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_rsm(struct x86_emulate_ctxt *ctxt)\r\n{\r\nunsigned long cr0, cr4, efer;\r\nu64 smbase;\r\nint ret;\r\nif ((ctxt->emul_flags & X86EMUL_SMM_MASK) == 0)\r\nreturn emulate_ud(ctxt);\r\ncr4 = ctxt->ops->get_cr(ctxt, 4);\r\nif (emulator_has_longmode(ctxt)) {\r\nstruct desc_struct cs_desc;\r\nif (cr4 & X86_CR4_PCIDE) {\r\nctxt->ops->set_cr(ctxt, 4, cr4 & ~X86_CR4_PCIDE);\r\ncr4 &= ~X86_CR4_PCIDE;\r\n}\r\nmemset(&cs_desc, 0, sizeof(cs_desc));\r\ncs_desc.type = 0xb;\r\ncs_desc.s = cs_desc.g = cs_desc.p = 1;\r\nctxt->ops->set_segment(ctxt, 0, &cs_desc, 0, VCPU_SREG_CS);\r\n}\r\ncr0 = ctxt->ops->get_cr(ctxt, 0);\r\nif (cr0 & X86_CR0_PE)\r\nctxt->ops->set_cr(ctxt, 0, cr0 & ~(X86_CR0_PG | X86_CR0_PE));\r\nif (cr4 & X86_CR4_PAE)\r\nctxt->ops->set_cr(ctxt, 4, cr4 & ~X86_CR4_PAE);\r\nefer = 0;\r\nctxt->ops->set_msr(ctxt, MSR_EFER, efer);\r\nsmbase = ctxt->ops->get_smbase(ctxt);\r\nif (emulator_has_longmode(ctxt))\r\nret = rsm_load_state_64(ctxt, smbase + 0x8000);\r\nelse\r\nret = rsm_load_state_32(ctxt, smbase + 0x8000);\r\nif (ret != X86EMUL_CONTINUE) {\r\nreturn X86EMUL_UNHANDLEABLE;\r\n}\r\nif ((ctxt->emul_flags & X86EMUL_SMM_INSIDE_NMI_MASK) == 0)\r\nctxt->ops->set_nmi_mask(ctxt, false);\r\nctxt->emul_flags &= ~X86EMUL_SMM_INSIDE_NMI_MASK;\r\nctxt->emul_flags &= ~X86EMUL_SMM_MASK;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic void\r\nsetup_syscalls_segments(struct x86_emulate_ctxt *ctxt,\r\nstruct desc_struct *cs, struct desc_struct *ss)\r\n{\r\ncs->l = 0;\r\nset_desc_base(cs, 0);\r\ncs->g = 1;\r\nset_desc_limit(cs, 0xfffff);\r\ncs->type = 0x0b;\r\ncs->s = 1;\r\ncs->dpl = 0;\r\ncs->p = 1;\r\ncs->d = 1;\r\ncs->avl = 0;\r\nset_desc_base(ss, 0);\r\nset_desc_limit(ss, 0xfffff);\r\nss->g = 1;\r\nss->s = 1;\r\nss->type = 0x03;\r\nss->d = 1;\r\nss->dpl = 0;\r\nss->p = 1;\r\nss->l = 0;\r\nss->avl = 0;\r\n}\r\nstatic bool vendor_intel(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu32 eax, ebx, ecx, edx;\r\neax = ecx = 0;\r\nctxt->ops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx);\r\nreturn ebx == X86EMUL_CPUID_VENDOR_GenuineIntel_ebx\r\n&& ecx == X86EMUL_CPUID_VENDOR_GenuineIntel_ecx\r\n&& edx == X86EMUL_CPUID_VENDOR_GenuineIntel_edx;\r\n}\r\nstatic bool em_syscall_is_enabled(struct x86_emulate_ctxt *ctxt)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nu32 eax, ebx, ecx, edx;\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nreturn true;\r\neax = 0x00000000;\r\necx = 0x00000000;\r\nops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx);\r\nif (ebx == X86EMUL_CPUID_VENDOR_GenuineIntel_ebx &&\r\necx == X86EMUL_CPUID_VENDOR_GenuineIntel_ecx &&\r\nedx == X86EMUL_CPUID_VENDOR_GenuineIntel_edx)\r\nreturn false;\r\nif (ebx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ebx &&\r\necx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ecx &&\r\nedx == X86EMUL_CPUID_VENDOR_AuthenticAMD_edx)\r\nreturn true;\r\nif (ebx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ebx &&\r\necx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ecx &&\r\nedx == X86EMUL_CPUID_VENDOR_AMDisbetterI_edx)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic int em_syscall(struct x86_emulate_ctxt *ctxt)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nstruct desc_struct cs, ss;\r\nu64 msr_data;\r\nu16 cs_sel, ss_sel;\r\nu64 efer = 0;\r\nif (ctxt->mode == X86EMUL_MODE_REAL ||\r\nctxt->mode == X86EMUL_MODE_VM86)\r\nreturn emulate_ud(ctxt);\r\nif (!(em_syscall_is_enabled(ctxt)))\r\nreturn emulate_ud(ctxt);\r\nops->get_msr(ctxt, MSR_EFER, &efer);\r\nsetup_syscalls_segments(ctxt, &cs, &ss);\r\nif (!(efer & EFER_SCE))\r\nreturn emulate_ud(ctxt);\r\nops->get_msr(ctxt, MSR_STAR, &msr_data);\r\nmsr_data >>= 32;\r\ncs_sel = (u16)(msr_data & 0xfffc);\r\nss_sel = (u16)(msr_data + 8);\r\nif (efer & EFER_LMA) {\r\ncs.d = 0;\r\ncs.l = 1;\r\n}\r\nops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\r\nops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\r\n*reg_write(ctxt, VCPU_REGS_RCX) = ctxt->_eip;\r\nif (efer & EFER_LMA) {\r\n#ifdef CONFIG_X86_64\r\n*reg_write(ctxt, VCPU_REGS_R11) = ctxt->eflags;\r\nops->get_msr(ctxt,\r\nctxt->mode == X86EMUL_MODE_PROT64 ?\r\nMSR_LSTAR : MSR_CSTAR, &msr_data);\r\nctxt->_eip = msr_data;\r\nops->get_msr(ctxt, MSR_SYSCALL_MASK, &msr_data);\r\nctxt->eflags &= ~msr_data;\r\nctxt->eflags |= X86_EFLAGS_FIXED;\r\n#endif\r\n} else {\r\nops->get_msr(ctxt, MSR_STAR, &msr_data);\r\nctxt->_eip = (u32)msr_data;\r\nctxt->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IF);\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_sysenter(struct x86_emulate_ctxt *ctxt)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nstruct desc_struct cs, ss;\r\nu64 msr_data;\r\nu16 cs_sel, ss_sel;\r\nu64 efer = 0;\r\nops->get_msr(ctxt, MSR_EFER, &efer);\r\nif (ctxt->mode == X86EMUL_MODE_REAL)\r\nreturn emulate_gp(ctxt, 0);\r\nif ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)\r\n&& !vendor_intel(ctxt))\r\nreturn emulate_ud(ctxt);\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nreturn X86EMUL_UNHANDLEABLE;\r\nsetup_syscalls_segments(ctxt, &cs, &ss);\r\nops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\r\nif ((msr_data & 0xfffc) == 0x0)\r\nreturn emulate_gp(ctxt, 0);\r\nctxt->eflags &= ~(X86_EFLAGS_VM | X86_EFLAGS_IF);\r\ncs_sel = (u16)msr_data & ~SEGMENT_RPL_MASK;\r\nss_sel = cs_sel + 8;\r\nif (efer & EFER_LMA) {\r\ncs.d = 0;\r\ncs.l = 1;\r\n}\r\nops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\r\nops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\r\nops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);\r\nctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;\r\nops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);\r\n*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :\r\n(u32)msr_data;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_sysexit(struct x86_emulate_ctxt *ctxt)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nstruct desc_struct cs, ss;\r\nu64 msr_data, rcx, rdx;\r\nint usermode;\r\nu16 cs_sel = 0, ss_sel = 0;\r\nif (ctxt->mode == X86EMUL_MODE_REAL ||\r\nctxt->mode == X86EMUL_MODE_VM86)\r\nreturn emulate_gp(ctxt, 0);\r\nsetup_syscalls_segments(ctxt, &cs, &ss);\r\nif ((ctxt->rex_prefix & 0x8) != 0x0)\r\nusermode = X86EMUL_MODE_PROT64;\r\nelse\r\nusermode = X86EMUL_MODE_PROT32;\r\nrcx = reg_read(ctxt, VCPU_REGS_RCX);\r\nrdx = reg_read(ctxt, VCPU_REGS_RDX);\r\ncs.dpl = 3;\r\nss.dpl = 3;\r\nops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);\r\nswitch (usermode) {\r\ncase X86EMUL_MODE_PROT32:\r\ncs_sel = (u16)(msr_data + 16);\r\nif ((msr_data & 0xfffc) == 0x0)\r\nreturn emulate_gp(ctxt, 0);\r\nss_sel = (u16)(msr_data + 24);\r\nrcx = (u32)rcx;\r\nrdx = (u32)rdx;\r\nbreak;\r\ncase X86EMUL_MODE_PROT64:\r\ncs_sel = (u16)(msr_data + 32);\r\nif (msr_data == 0x0)\r\nreturn emulate_gp(ctxt, 0);\r\nss_sel = cs_sel + 8;\r\ncs.d = 0;\r\ncs.l = 1;\r\nif (is_noncanonical_address(rcx) ||\r\nis_noncanonical_address(rdx))\r\nreturn emulate_gp(ctxt, 0);\r\nbreak;\r\n}\r\ncs_sel |= SEGMENT_RPL_MASK;\r\nss_sel |= SEGMENT_RPL_MASK;\r\nops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);\r\nops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);\r\nctxt->_eip = rdx;\r\n*reg_write(ctxt, VCPU_REGS_RSP) = rcx;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic bool emulator_bad_iopl(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint iopl;\r\nif (ctxt->mode == X86EMUL_MODE_REAL)\r\nreturn false;\r\nif (ctxt->mode == X86EMUL_MODE_VM86)\r\nreturn true;\r\niopl = (ctxt->eflags & X86_EFLAGS_IOPL) >> X86_EFLAGS_IOPL_BIT;\r\nreturn ctxt->ops->cpl(ctxt) > iopl;\r\n}\r\nstatic bool emulator_io_port_access_allowed(struct x86_emulate_ctxt *ctxt,\r\nu16 port, u16 len)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nstruct desc_struct tr_seg;\r\nu32 base3;\r\nint r;\r\nu16 tr, io_bitmap_ptr, perm, bit_idx = port & 0x7;\r\nunsigned mask = (1 << len) - 1;\r\nunsigned long base;\r\nops->get_segment(ctxt, &tr, &tr_seg, &base3, VCPU_SREG_TR);\r\nif (!tr_seg.p)\r\nreturn false;\r\nif (desc_limit_scaled(&tr_seg) < 103)\r\nreturn false;\r\nbase = get_desc_base(&tr_seg);\r\n#ifdef CONFIG_X86_64\r\nbase |= ((u64)base3) << 32;\r\n#endif\r\nr = ops->read_std(ctxt, base + 102, &io_bitmap_ptr, 2, NULL);\r\nif (r != X86EMUL_CONTINUE)\r\nreturn false;\r\nif (io_bitmap_ptr + port/8 > desc_limit_scaled(&tr_seg))\r\nreturn false;\r\nr = ops->read_std(ctxt, base + io_bitmap_ptr + port/8, &perm, 2, NULL);\r\nif (r != X86EMUL_CONTINUE)\r\nreturn false;\r\nif ((perm >> bit_idx) & mask)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool emulator_io_permited(struct x86_emulate_ctxt *ctxt,\r\nu16 port, u16 len)\r\n{\r\nif (ctxt->perm_ok)\r\nreturn true;\r\nif (emulator_bad_iopl(ctxt))\r\nif (!emulator_io_port_access_allowed(ctxt, port, len))\r\nreturn false;\r\nctxt->perm_ok = true;\r\nreturn true;\r\n}\r\nstatic void string_registers_quirk(struct x86_emulate_ctxt *ctxt)\r\n{\r\n#ifdef CONFIG_X86_64\r\nif (ctxt->ad_bytes != 4 || !vendor_intel(ctxt))\r\nreturn;\r\n*reg_write(ctxt, VCPU_REGS_RCX) = 0;\r\nswitch (ctxt->b) {\r\ncase 0xa4:\r\ncase 0xa5:\r\n*reg_rmw(ctxt, VCPU_REGS_RSI) &= (u32)-1;\r\ncase 0xaa:\r\ncase 0xab:\r\n*reg_rmw(ctxt, VCPU_REGS_RDI) &= (u32)-1;\r\n}\r\n#endif\r\n}\r\nstatic void save_state_to_tss16(struct x86_emulate_ctxt *ctxt,\r\nstruct tss_segment_16 *tss)\r\n{\r\ntss->ip = ctxt->_eip;\r\ntss->flag = ctxt->eflags;\r\ntss->ax = reg_read(ctxt, VCPU_REGS_RAX);\r\ntss->cx = reg_read(ctxt, VCPU_REGS_RCX);\r\ntss->dx = reg_read(ctxt, VCPU_REGS_RDX);\r\ntss->bx = reg_read(ctxt, VCPU_REGS_RBX);\r\ntss->sp = reg_read(ctxt, VCPU_REGS_RSP);\r\ntss->bp = reg_read(ctxt, VCPU_REGS_RBP);\r\ntss->si = reg_read(ctxt, VCPU_REGS_RSI);\r\ntss->di = reg_read(ctxt, VCPU_REGS_RDI);\r\ntss->es = get_segment_selector(ctxt, VCPU_SREG_ES);\r\ntss->cs = get_segment_selector(ctxt, VCPU_SREG_CS);\r\ntss->ss = get_segment_selector(ctxt, VCPU_SREG_SS);\r\ntss->ds = get_segment_selector(ctxt, VCPU_SREG_DS);\r\ntss->ldt = get_segment_selector(ctxt, VCPU_SREG_LDTR);\r\n}\r\nstatic int load_state_from_tss16(struct x86_emulate_ctxt *ctxt,\r\nstruct tss_segment_16 *tss)\r\n{\r\nint ret;\r\nu8 cpl;\r\nctxt->_eip = tss->ip;\r\nctxt->eflags = tss->flag | 2;\r\n*reg_write(ctxt, VCPU_REGS_RAX) = tss->ax;\r\n*reg_write(ctxt, VCPU_REGS_RCX) = tss->cx;\r\n*reg_write(ctxt, VCPU_REGS_RDX) = tss->dx;\r\n*reg_write(ctxt, VCPU_REGS_RBX) = tss->bx;\r\n*reg_write(ctxt, VCPU_REGS_RSP) = tss->sp;\r\n*reg_write(ctxt, VCPU_REGS_RBP) = tss->bp;\r\n*reg_write(ctxt, VCPU_REGS_RSI) = tss->si;\r\n*reg_write(ctxt, VCPU_REGS_RDI) = tss->di;\r\nset_segment_selector(ctxt, tss->ldt, VCPU_SREG_LDTR);\r\nset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\r\nset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\r\nset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\r\nset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\r\ncpl = tss->cs & 3;\r\nret = __load_segment_descriptor(ctxt, tss->ldt, VCPU_SREG_LDTR, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int task_switch_16(struct x86_emulate_ctxt *ctxt,\r\nu16 tss_selector, u16 old_tss_sel,\r\nulong old_tss_base, struct desc_struct *new_desc)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nstruct tss_segment_16 tss_seg;\r\nint ret;\r\nu32 new_tss_base = get_desc_base(new_desc);\r\nret = ops->read_std(ctxt, old_tss_base, &tss_seg, sizeof tss_seg,\r\n&ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nsave_state_to_tss16(ctxt, &tss_seg);\r\nret = ops->write_std(ctxt, old_tss_base, &tss_seg, sizeof tss_seg,\r\n&ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = ops->read_std(ctxt, new_tss_base, &tss_seg, sizeof tss_seg,\r\n&ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nif (old_tss_sel != 0xffff) {\r\ntss_seg.prev_task_link = old_tss_sel;\r\nret = ops->write_std(ctxt, new_tss_base,\r\n&tss_seg.prev_task_link,\r\nsizeof tss_seg.prev_task_link,\r\n&ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\n}\r\nreturn load_state_from_tss16(ctxt, &tss_seg);\r\n}\r\nstatic void save_state_to_tss32(struct x86_emulate_ctxt *ctxt,\r\nstruct tss_segment_32 *tss)\r\n{\r\ntss->eip = ctxt->_eip;\r\ntss->eflags = ctxt->eflags;\r\ntss->eax = reg_read(ctxt, VCPU_REGS_RAX);\r\ntss->ecx = reg_read(ctxt, VCPU_REGS_RCX);\r\ntss->edx = reg_read(ctxt, VCPU_REGS_RDX);\r\ntss->ebx = reg_read(ctxt, VCPU_REGS_RBX);\r\ntss->esp = reg_read(ctxt, VCPU_REGS_RSP);\r\ntss->ebp = reg_read(ctxt, VCPU_REGS_RBP);\r\ntss->esi = reg_read(ctxt, VCPU_REGS_RSI);\r\ntss->edi = reg_read(ctxt, VCPU_REGS_RDI);\r\ntss->es = get_segment_selector(ctxt, VCPU_SREG_ES);\r\ntss->cs = get_segment_selector(ctxt, VCPU_SREG_CS);\r\ntss->ss = get_segment_selector(ctxt, VCPU_SREG_SS);\r\ntss->ds = get_segment_selector(ctxt, VCPU_SREG_DS);\r\ntss->fs = get_segment_selector(ctxt, VCPU_SREG_FS);\r\ntss->gs = get_segment_selector(ctxt, VCPU_SREG_GS);\r\n}\r\nstatic int load_state_from_tss32(struct x86_emulate_ctxt *ctxt,\r\nstruct tss_segment_32 *tss)\r\n{\r\nint ret;\r\nu8 cpl;\r\nif (ctxt->ops->set_cr(ctxt, 3, tss->cr3))\r\nreturn emulate_gp(ctxt, 0);\r\nctxt->_eip = tss->eip;\r\nctxt->eflags = tss->eflags | 2;\r\n*reg_write(ctxt, VCPU_REGS_RAX) = tss->eax;\r\n*reg_write(ctxt, VCPU_REGS_RCX) = tss->ecx;\r\n*reg_write(ctxt, VCPU_REGS_RDX) = tss->edx;\r\n*reg_write(ctxt, VCPU_REGS_RBX) = tss->ebx;\r\n*reg_write(ctxt, VCPU_REGS_RSP) = tss->esp;\r\n*reg_write(ctxt, VCPU_REGS_RBP) = tss->ebp;\r\n*reg_write(ctxt, VCPU_REGS_RSI) = tss->esi;\r\n*reg_write(ctxt, VCPU_REGS_RDI) = tss->edi;\r\nset_segment_selector(ctxt, tss->ldt_selector, VCPU_SREG_LDTR);\r\nset_segment_selector(ctxt, tss->es, VCPU_SREG_ES);\r\nset_segment_selector(ctxt, tss->cs, VCPU_SREG_CS);\r\nset_segment_selector(ctxt, tss->ss, VCPU_SREG_SS);\r\nset_segment_selector(ctxt, tss->ds, VCPU_SREG_DS);\r\nset_segment_selector(ctxt, tss->fs, VCPU_SREG_FS);\r\nset_segment_selector(ctxt, tss->gs, VCPU_SREG_GS);\r\nif (ctxt->eflags & X86_EFLAGS_VM) {\r\nctxt->mode = X86EMUL_MODE_VM86;\r\ncpl = 3;\r\n} else {\r\nctxt->mode = X86EMUL_MODE_PROT32;\r\ncpl = tss->cs & 3;\r\n}\r\nret = __load_segment_descriptor(ctxt, tss->ldt_selector, VCPU_SREG_LDTR,\r\ncpl, X86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->fs, VCPU_SREG_FS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = __load_segment_descriptor(ctxt, tss->gs, VCPU_SREG_GS, cpl,\r\nX86_TRANSFER_TASK_SWITCH, NULL);\r\nreturn ret;\r\n}\r\nstatic int task_switch_32(struct x86_emulate_ctxt *ctxt,\r\nu16 tss_selector, u16 old_tss_sel,\r\nulong old_tss_base, struct desc_struct *new_desc)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nstruct tss_segment_32 tss_seg;\r\nint ret;\r\nu32 new_tss_base = get_desc_base(new_desc);\r\nu32 eip_offset = offsetof(struct tss_segment_32, eip);\r\nu32 ldt_sel_offset = offsetof(struct tss_segment_32, ldt_selector);\r\nret = ops->read_std(ctxt, old_tss_base, &tss_seg, sizeof tss_seg,\r\n&ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nsave_state_to_tss32(ctxt, &tss_seg);\r\nret = ops->write_std(ctxt, old_tss_base + eip_offset, &tss_seg.eip,\r\nldt_sel_offset - eip_offset, &ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = ops->read_std(ctxt, new_tss_base, &tss_seg, sizeof tss_seg,\r\n&ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nif (old_tss_sel != 0xffff) {\r\ntss_seg.prev_task_link = old_tss_sel;\r\nret = ops->write_std(ctxt, new_tss_base,\r\n&tss_seg.prev_task_link,\r\nsizeof tss_seg.prev_task_link,\r\n&ctxt->exception);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\n}\r\nreturn load_state_from_tss32(ctxt, &tss_seg);\r\n}\r\nstatic int emulator_do_task_switch(struct x86_emulate_ctxt *ctxt,\r\nu16 tss_selector, int idt_index, int reason,\r\nbool has_error_code, u32 error_code)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nstruct desc_struct curr_tss_desc, next_tss_desc;\r\nint ret;\r\nu16 old_tss_sel = get_segment_selector(ctxt, VCPU_SREG_TR);\r\nulong old_tss_base =\r\nops->get_cached_segment_base(ctxt, VCPU_SREG_TR);\r\nu32 desc_limit;\r\nulong desc_addr, dr7;\r\nret = read_segment_descriptor(ctxt, tss_selector, &next_tss_desc, &desc_addr);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nret = read_segment_descriptor(ctxt, old_tss_sel, &curr_tss_desc, &desc_addr);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nif (reason == TASK_SWITCH_GATE) {\r\nif (idt_index != -1) {\r\nstruct desc_struct task_gate_desc;\r\nint dpl;\r\nret = read_interrupt_descriptor(ctxt, idt_index,\r\n&task_gate_desc);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\ndpl = task_gate_desc.dpl;\r\nif ((tss_selector & 3) > dpl || ops->cpl(ctxt) > dpl)\r\nreturn emulate_gp(ctxt, (idt_index << 3) | 0x2);\r\n}\r\n}\r\ndesc_limit = desc_limit_scaled(&next_tss_desc);\r\nif (!next_tss_desc.p ||\r\n((desc_limit < 0x67 && (next_tss_desc.type & 8)) ||\r\ndesc_limit < 0x2b)) {\r\nreturn emulate_ts(ctxt, tss_selector & 0xfffc);\r\n}\r\nif (reason == TASK_SWITCH_IRET || reason == TASK_SWITCH_JMP) {\r\ncurr_tss_desc.type &= ~(1 << 1);\r\nwrite_segment_descriptor(ctxt, old_tss_sel, &curr_tss_desc);\r\n}\r\nif (reason == TASK_SWITCH_IRET)\r\nctxt->eflags = ctxt->eflags & ~X86_EFLAGS_NT;\r\nif (reason != TASK_SWITCH_CALL && reason != TASK_SWITCH_GATE)\r\nold_tss_sel = 0xffff;\r\nif (next_tss_desc.type & 8)\r\nret = task_switch_32(ctxt, tss_selector, old_tss_sel,\r\nold_tss_base, &next_tss_desc);\r\nelse\r\nret = task_switch_16(ctxt, tss_selector, old_tss_sel,\r\nold_tss_base, &next_tss_desc);\r\nif (ret != X86EMUL_CONTINUE)\r\nreturn ret;\r\nif (reason == TASK_SWITCH_CALL || reason == TASK_SWITCH_GATE)\r\nctxt->eflags = ctxt->eflags | X86_EFLAGS_NT;\r\nif (reason != TASK_SWITCH_IRET) {\r\nnext_tss_desc.type |= (1 << 1);\r\nwrite_segment_descriptor(ctxt, tss_selector, &next_tss_desc);\r\n}\r\nops->set_cr(ctxt, 0, ops->get_cr(ctxt, 0) | X86_CR0_TS);\r\nops->set_segment(ctxt, tss_selector, &next_tss_desc, 0, VCPU_SREG_TR);\r\nif (has_error_code) {\r\nctxt->op_bytes = ctxt->ad_bytes = (next_tss_desc.type & 8) ? 4 : 2;\r\nctxt->lock_prefix = 0;\r\nctxt->src.val = (unsigned long) error_code;\r\nret = em_push(ctxt);\r\n}\r\nops->get_dr(ctxt, 7, &dr7);\r\nops->set_dr(ctxt, 7, dr7 & ~(DR_LOCAL_ENABLE_MASK | DR_LOCAL_SLOWDOWN));\r\nreturn ret;\r\n}\r\nint emulator_task_switch(struct x86_emulate_ctxt *ctxt,\r\nu16 tss_selector, int idt_index, int reason,\r\nbool has_error_code, u32 error_code)\r\n{\r\nint rc;\r\ninvalidate_registers(ctxt);\r\nctxt->_eip = ctxt->eip;\r\nctxt->dst.type = OP_NONE;\r\nrc = emulator_do_task_switch(ctxt, tss_selector, idt_index, reason,\r\nhas_error_code, error_code);\r\nif (rc == X86EMUL_CONTINUE) {\r\nctxt->eip = ctxt->_eip;\r\nwriteback_registers(ctxt);\r\n}\r\nreturn (rc == X86EMUL_UNHANDLEABLE) ? EMULATION_FAILED : EMULATION_OK;\r\n}\r\nstatic void string_addr_inc(struct x86_emulate_ctxt *ctxt, int reg,\r\nstruct operand *op)\r\n{\r\nint df = (ctxt->eflags & X86_EFLAGS_DF) ? -op->count : op->count;\r\nregister_address_increment(ctxt, reg, df * op->bytes);\r\nop->addr.mem.ea = register_address(ctxt, reg);\r\n}\r\nstatic int em_das(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu8 al, old_al;\r\nbool af, cf, old_cf;\r\ncf = ctxt->eflags & X86_EFLAGS_CF;\r\nal = ctxt->dst.val;\r\nold_al = al;\r\nold_cf = cf;\r\ncf = false;\r\naf = ctxt->eflags & X86_EFLAGS_AF;\r\nif ((al & 0x0f) > 9 || af) {\r\nal -= 6;\r\ncf = old_cf | (al >= 250);\r\naf = true;\r\n} else {\r\naf = false;\r\n}\r\nif (old_al > 0x99 || old_cf) {\r\nal -= 0x60;\r\ncf = true;\r\n}\r\nctxt->dst.val = al;\r\nctxt->src.type = OP_IMM;\r\nctxt->src.val = 0;\r\nctxt->src.bytes = 1;\r\nfastop(ctxt, em_or);\r\nctxt->eflags &= ~(X86_EFLAGS_AF | X86_EFLAGS_CF);\r\nif (cf)\r\nctxt->eflags |= X86_EFLAGS_CF;\r\nif (af)\r\nctxt->eflags |= X86_EFLAGS_AF;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_aam(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu8 al, ah;\r\nif (ctxt->src.val == 0)\r\nreturn emulate_de(ctxt);\r\nal = ctxt->dst.val & 0xff;\r\nah = al / ctxt->src.val;\r\nal %= ctxt->src.val;\r\nctxt->dst.val = (ctxt->dst.val & 0xffff0000) | al | (ah << 8);\r\nctxt->src.type = OP_IMM;\r\nctxt->src.val = 0;\r\nctxt->src.bytes = 1;\r\nfastop(ctxt, em_or);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_aad(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu8 al = ctxt->dst.val & 0xff;\r\nu8 ah = (ctxt->dst.val >> 8) & 0xff;\r\nal = (al + (ah * ctxt->src.val)) & 0xff;\r\nctxt->dst.val = (ctxt->dst.val & 0xffff0000) | al;\r\nctxt->src.type = OP_IMM;\r\nctxt->src.val = 0;\r\nctxt->src.bytes = 1;\r\nfastop(ctxt, em_or);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_call(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nlong rel = ctxt->src.val;\r\nctxt->src.val = (unsigned long)ctxt->_eip;\r\nrc = jmp_rel(ctxt, rel);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nreturn em_push(ctxt);\r\n}\r\nstatic int em_call_far(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu16 sel, old_cs;\r\nulong old_eip;\r\nint rc;\r\nstruct desc_struct old_desc, new_desc;\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nint cpl = ctxt->ops->cpl(ctxt);\r\nenum x86emul_mode prev_mode = ctxt->mode;\r\nold_eip = ctxt->_eip;\r\nops->get_segment(ctxt, &old_cs, &old_desc, NULL, VCPU_SREG_CS);\r\nmemcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);\r\nrc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,\r\nX86_TRANSFER_CALL_JMP, &new_desc);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto fail;\r\nctxt->src.val = old_cs;\r\nrc = em_push(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto fail;\r\nctxt->src.val = old_eip;\r\nrc = em_push(ctxt);\r\nif (rc != X86EMUL_CONTINUE) {\r\npr_warn_once("faulting far call emulation tainted memory\n");\r\ngoto fail;\r\n}\r\nreturn rc;\r\nfail:\r\nops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);\r\nctxt->mode = prev_mode;\r\nreturn rc;\r\n}\r\nstatic int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nunsigned long eip;\r\nrc = emulate_pop(ctxt, &eip, ctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrc = assign_eip_near(ctxt, eip);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nrsp_increment(ctxt, ctxt->src.val);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_xchg(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->src.val = ctxt->dst.val;\r\nwrite_register_operand(&ctxt->src);\r\nctxt->dst.val = ctxt->src.orig_val;\r\nctxt->lock_prefix = 1;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_imul_3op(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->dst.val = ctxt->src2.val;\r\nreturn fastop(ctxt, em_imul);\r\n}\r\nstatic int em_cwd(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->dst.type = OP_REG;\r\nctxt->dst.bytes = ctxt->src.bytes;\r\nctxt->dst.addr.reg = reg_rmw(ctxt, VCPU_REGS_RDX);\r\nctxt->dst.val = ~((ctxt->src.val >> (ctxt->src.bytes * 8 - 1)) - 1);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_rdtsc(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 tsc = 0;\r\nctxt->ops->get_msr(ctxt, MSR_IA32_TSC, &tsc);\r\n*reg_write(ctxt, VCPU_REGS_RAX) = (u32)tsc;\r\n*reg_write(ctxt, VCPU_REGS_RDX) = tsc >> 32;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_rdpmc(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 pmc;\r\nif (ctxt->ops->read_pmc(ctxt, reg_read(ctxt, VCPU_REGS_RCX), &pmc))\r\nreturn emulate_gp(ctxt, 0);\r\n*reg_write(ctxt, VCPU_REGS_RAX) = (u32)pmc;\r\n*reg_write(ctxt, VCPU_REGS_RDX) = pmc >> 32;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_mov(struct x86_emulate_ctxt *ctxt)\r\n{\r\nmemcpy(ctxt->dst.valptr, ctxt->src.valptr, sizeof(ctxt->src.valptr));\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_movbe(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu32 ebx, ecx, edx, eax = 1;\r\nu16 tmp;\r\nctxt->ops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx);\r\nif (!(ecx & FFL(MOVBE)))\r\nreturn emulate_ud(ctxt);\r\nswitch (ctxt->op_bytes) {\r\ncase 2:\r\ntmp = (u16)ctxt->src.val;\r\nctxt->dst.val &= ~0xffffUL;\r\nctxt->dst.val |= (unsigned long)swab16(tmp);\r\nbreak;\r\ncase 4:\r\nctxt->dst.val = swab32((u32)ctxt->src.val);\r\nbreak;\r\ncase 8:\r\nctxt->dst.val = swab64(ctxt->src.val);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_cr_write(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (ctxt->ops->set_cr(ctxt, ctxt->modrm_reg, ctxt->src.val))\r\nreturn emulate_gp(ctxt, 0);\r\nctxt->dst.type = OP_NONE;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_dr_write(struct x86_emulate_ctxt *ctxt)\r\n{\r\nunsigned long val;\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nval = ctxt->src.val & ~0ULL;\r\nelse\r\nval = ctxt->src.val & ~0U;\r\nif (ctxt->ops->set_dr(ctxt, ctxt->modrm_reg, val) < 0)\r\nreturn emulate_gp(ctxt, 0);\r\nctxt->dst.type = OP_NONE;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_wrmsr(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 msr_data;\r\nmsr_data = (u32)reg_read(ctxt, VCPU_REGS_RAX)\r\n| ((u64)reg_read(ctxt, VCPU_REGS_RDX) << 32);\r\nif (ctxt->ops->set_msr(ctxt, reg_read(ctxt, VCPU_REGS_RCX), msr_data))\r\nreturn emulate_gp(ctxt, 0);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_rdmsr(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 msr_data;\r\nif (ctxt->ops->get_msr(ctxt, reg_read(ctxt, VCPU_REGS_RCX), &msr_data))\r\nreturn emulate_gp(ctxt, 0);\r\n*reg_write(ctxt, VCPU_REGS_RAX) = (u32)msr_data;\r\n*reg_write(ctxt, VCPU_REGS_RDX) = msr_data >> 32;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_mov_rm_sreg(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (ctxt->modrm_reg > VCPU_SREG_GS)\r\nreturn emulate_ud(ctxt);\r\nctxt->dst.val = get_segment_selector(ctxt, ctxt->modrm_reg);\r\nif (ctxt->dst.bytes == 4 && ctxt->dst.type == OP_MEM)\r\nctxt->dst.bytes = 2;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_mov_sreg_rm(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu16 sel = ctxt->src.val;\r\nif (ctxt->modrm_reg == VCPU_SREG_CS || ctxt->modrm_reg > VCPU_SREG_GS)\r\nreturn emulate_ud(ctxt);\r\nif (ctxt->modrm_reg == VCPU_SREG_SS)\r\nctxt->interruptibility = KVM_X86_SHADOW_INT_MOV_SS;\r\nctxt->dst.type = OP_NONE;\r\nreturn load_segment_descriptor(ctxt, sel, ctxt->modrm_reg);\r\n}\r\nstatic int em_lldt(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu16 sel = ctxt->src.val;\r\nctxt->dst.type = OP_NONE;\r\nreturn load_segment_descriptor(ctxt, sel, VCPU_SREG_LDTR);\r\n}\r\nstatic int em_ltr(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu16 sel = ctxt->src.val;\r\nctxt->dst.type = OP_NONE;\r\nreturn load_segment_descriptor(ctxt, sel, VCPU_SREG_TR);\r\n}\r\nstatic int em_invlpg(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc;\r\nulong linear;\r\nrc = linearize(ctxt, ctxt->src.addr.mem, 1, false, &linear);\r\nif (rc == X86EMUL_CONTINUE)\r\nctxt->ops->invlpg(ctxt, linear);\r\nctxt->dst.type = OP_NONE;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_clts(struct x86_emulate_ctxt *ctxt)\r\n{\r\nulong cr0;\r\ncr0 = ctxt->ops->get_cr(ctxt, 0);\r\ncr0 &= ~X86_CR0_TS;\r\nctxt->ops->set_cr(ctxt, 0, cr0);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_hypercall(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc = ctxt->ops->fix_hypercall(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nctxt->_eip = ctxt->eip;\r\nctxt->dst.type = OP_NONE;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\r\nvoid (*get)(struct x86_emulate_ctxt *ctxt,\r\nstruct desc_ptr *ptr))\r\n{\r\nstruct desc_ptr desc_ptr;\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nctxt->op_bytes = 8;\r\nget(ctxt, &desc_ptr);\r\nif (ctxt->op_bytes == 2) {\r\nctxt->op_bytes = 4;\r\ndesc_ptr.address &= 0x00ffffff;\r\n}\r\nctxt->dst.type = OP_NONE;\r\nreturn segmented_write(ctxt, ctxt->dst.addr.mem,\r\n&desc_ptr, 2 + ctxt->op_bytes);\r\n}\r\nstatic int em_sgdt(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn emulate_store_desc_ptr(ctxt, ctxt->ops->get_gdt);\r\n}\r\nstatic int em_sidt(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn emulate_store_desc_ptr(ctxt, ctxt->ops->get_idt);\r\n}\r\nstatic int em_lgdt_lidt(struct x86_emulate_ctxt *ctxt, bool lgdt)\r\n{\r\nstruct desc_ptr desc_ptr;\r\nint rc;\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nctxt->op_bytes = 8;\r\nrc = read_descriptor(ctxt, ctxt->src.addr.mem,\r\n&desc_ptr.size, &desc_ptr.address,\r\nctxt->op_bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\nif (ctxt->mode == X86EMUL_MODE_PROT64 &&\r\nis_noncanonical_address(desc_ptr.address))\r\nreturn emulate_gp(ctxt, 0);\r\nif (lgdt)\r\nctxt->ops->set_gdt(ctxt, &desc_ptr);\r\nelse\r\nctxt->ops->set_idt(ctxt, &desc_ptr);\r\nctxt->dst.type = OP_NONE;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_lgdt(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn em_lgdt_lidt(ctxt, true);\r\n}\r\nstatic int em_lidt(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn em_lgdt_lidt(ctxt, false);\r\n}\r\nstatic int em_smsw(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (ctxt->dst.type == OP_MEM)\r\nctxt->dst.bytes = 2;\r\nctxt->dst.val = ctxt->ops->get_cr(ctxt, 0);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_lmsw(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->ops->set_cr(ctxt, 0, (ctxt->ops->get_cr(ctxt, 0) & ~0x0eul)\r\n| (ctxt->src.val & 0x0f));\r\nctxt->dst.type = OP_NONE;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_loop(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nregister_address_increment(ctxt, VCPU_REGS_RCX, -1);\r\nif ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&\r\n(ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))\r\nrc = jmp_rel(ctxt, ctxt->src.val);\r\nreturn rc;\r\n}\r\nstatic int em_jcxz(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)\r\nrc = jmp_rel(ctxt, ctxt->src.val);\r\nreturn rc;\r\n}\r\nstatic int em_in(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (!pio_in_emulated(ctxt, ctxt->dst.bytes, ctxt->src.val,\r\n&ctxt->dst.val))\r\nreturn X86EMUL_IO_NEEDED;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_out(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->ops->pio_out_emulated(ctxt, ctxt->src.bytes, ctxt->dst.val,\r\n&ctxt->src.val, 1);\r\nctxt->dst.type = OP_NONE;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_cli(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (emulator_bad_iopl(ctxt))\r\nreturn emulate_gp(ctxt, 0);\r\nctxt->eflags &= ~X86_EFLAGS_IF;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_sti(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (emulator_bad_iopl(ctxt))\r\nreturn emulate_gp(ctxt, 0);\r\nctxt->interruptibility = KVM_X86_SHADOW_INT_STI;\r\nctxt->eflags |= X86_EFLAGS_IF;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_cpuid(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu32 eax, ebx, ecx, edx;\r\neax = reg_read(ctxt, VCPU_REGS_RAX);\r\necx = reg_read(ctxt, VCPU_REGS_RCX);\r\nctxt->ops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx);\r\n*reg_write(ctxt, VCPU_REGS_RAX) = eax;\r\n*reg_write(ctxt, VCPU_REGS_RBX) = ebx;\r\n*reg_write(ctxt, VCPU_REGS_RCX) = ecx;\r\n*reg_write(ctxt, VCPU_REGS_RDX) = edx;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_sahf(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu32 flags;\r\nflags = X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF | X86_EFLAGS_ZF |\r\nX86_EFLAGS_SF;\r\nflags &= *reg_rmw(ctxt, VCPU_REGS_RAX) >> 8;\r\nctxt->eflags &= ~0xffUL;\r\nctxt->eflags |= flags | X86_EFLAGS_FIXED;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_lahf(struct x86_emulate_ctxt *ctxt)\r\n{\r\n*reg_rmw(ctxt, VCPU_REGS_RAX) &= ~0xff00UL;\r\n*reg_rmw(ctxt, VCPU_REGS_RAX) |= (ctxt->eflags & 0xff) << 8;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_bswap(struct x86_emulate_ctxt *ctxt)\r\n{\r\nswitch (ctxt->op_bytes) {\r\n#ifdef CONFIG_X86_64\r\ncase 8:\r\nasm("bswap %0" : "+r"(ctxt->dst.val));\r\nbreak;\r\n#endif\r\ndefault:\r\nasm("bswap %0" : "+r"(*(u32 *)&ctxt->dst.val));\r\nbreak;\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_clflush(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int em_movsxd(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->dst.val = (s32) ctxt->src.val;\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic bool valid_cr(int nr)\r\n{\r\nswitch (nr) {\r\ncase 0:\r\ncase 2 ... 4:\r\ncase 8:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic int check_cr_read(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (!valid_cr(ctxt->modrm_reg))\r\nreturn emulate_ud(ctxt);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int check_cr_write(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 new_val = ctxt->src.val64;\r\nint cr = ctxt->modrm_reg;\r\nu64 efer = 0;\r\nstatic u64 cr_reserved_bits[] = {\r\n0xffffffff00000000ULL,\r\n0, 0, 0,\r\nCR4_RESERVED_BITS,\r\n0, 0, 0,\r\nCR8_RESERVED_BITS,\r\n};\r\nif (!valid_cr(cr))\r\nreturn emulate_ud(ctxt);\r\nif (new_val & cr_reserved_bits[cr])\r\nreturn emulate_gp(ctxt, 0);\r\nswitch (cr) {\r\ncase 0: {\r\nu64 cr4;\r\nif (((new_val & X86_CR0_PG) && !(new_val & X86_CR0_PE)) ||\r\n((new_val & X86_CR0_NW) && !(new_val & X86_CR0_CD)))\r\nreturn emulate_gp(ctxt, 0);\r\ncr4 = ctxt->ops->get_cr(ctxt, 4);\r\nctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\r\nif ((new_val & X86_CR0_PG) && (efer & EFER_LME) &&\r\n!(cr4 & X86_CR4_PAE))\r\nreturn emulate_gp(ctxt, 0);\r\nbreak;\r\n}\r\ncase 3: {\r\nu64 rsvd = 0;\r\nctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\r\nif (efer & EFER_LMA)\r\nrsvd = CR3_L_MODE_RESERVED_BITS & ~CR3_PCID_INVD;\r\nif (new_val & rsvd)\r\nreturn emulate_gp(ctxt, 0);\r\nbreak;\r\n}\r\ncase 4: {\r\nctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\r\nif ((efer & EFER_LMA) && !(new_val & X86_CR4_PAE))\r\nreturn emulate_gp(ctxt, 0);\r\nbreak;\r\n}\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int check_dr7_gd(struct x86_emulate_ctxt *ctxt)\r\n{\r\nunsigned long dr7;\r\nctxt->ops->get_dr(ctxt, 7, &dr7);\r\nreturn dr7 & (1 << 13);\r\n}\r\nstatic int check_dr_read(struct x86_emulate_ctxt *ctxt)\r\n{\r\nint dr = ctxt->modrm_reg;\r\nu64 cr4;\r\nif (dr > 7)\r\nreturn emulate_ud(ctxt);\r\ncr4 = ctxt->ops->get_cr(ctxt, 4);\r\nif ((cr4 & X86_CR4_DE) && (dr == 4 || dr == 5))\r\nreturn emulate_ud(ctxt);\r\nif (check_dr7_gd(ctxt)) {\r\nulong dr6;\r\nctxt->ops->get_dr(ctxt, 6, &dr6);\r\ndr6 &= ~15;\r\ndr6 |= DR6_BD | DR6_RTM;\r\nctxt->ops->set_dr(ctxt, 6, dr6);\r\nreturn emulate_db(ctxt);\r\n}\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int check_dr_write(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 new_val = ctxt->src.val64;\r\nint dr = ctxt->modrm_reg;\r\nif ((dr == 6 || dr == 7) && (new_val & 0xffffffff00000000ULL))\r\nreturn emulate_gp(ctxt, 0);\r\nreturn check_dr_read(ctxt);\r\n}\r\nstatic int check_svme(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 efer;\r\nctxt->ops->get_msr(ctxt, MSR_EFER, &efer);\r\nif (!(efer & EFER_SVME))\r\nreturn emulate_ud(ctxt);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int check_svme_pa(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 rax = reg_read(ctxt, VCPU_REGS_RAX);\r\nif (rax & 0xffff000000000000ULL)\r\nreturn emulate_gp(ctxt, 0);\r\nreturn check_svme(ctxt);\r\n}\r\nstatic int check_rdtsc(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 cr4 = ctxt->ops->get_cr(ctxt, 4);\r\nif (cr4 & X86_CR4_TSD && ctxt->ops->cpl(ctxt))\r\nreturn emulate_ud(ctxt);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int check_rdpmc(struct x86_emulate_ctxt *ctxt)\r\n{\r\nu64 cr4 = ctxt->ops->get_cr(ctxt, 4);\r\nu64 rcx = reg_read(ctxt, VCPU_REGS_RCX);\r\nif ((!(cr4 & X86_CR4_PCE) && ctxt->ops->cpl(ctxt)) ||\r\nctxt->ops->check_pmc(ctxt, rcx))\r\nreturn emulate_gp(ctxt, 0);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int check_perm_in(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->dst.bytes = min(ctxt->dst.bytes, 4u);\r\nif (!emulator_io_permited(ctxt, ctxt->src.val, ctxt->dst.bytes))\r\nreturn emulate_gp(ctxt, 0);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic int check_perm_out(struct x86_emulate_ctxt *ctxt)\r\n{\r\nctxt->src.bytes = min(ctxt->src.bytes, 4u);\r\nif (!emulator_io_permited(ctxt, ctxt->dst.val, ctxt->src.bytes))\r\nreturn emulate_gp(ctxt, 0);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic unsigned imm_size(struct x86_emulate_ctxt *ctxt)\r\n{\r\nunsigned size;\r\nsize = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nif (size == 8)\r\nsize = 4;\r\nreturn size;\r\n}\r\nstatic int decode_imm(struct x86_emulate_ctxt *ctxt, struct operand *op,\r\nunsigned size, bool sign_extension)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nop->type = OP_IMM;\r\nop->bytes = size;\r\nop->addr.mem.ea = ctxt->_eip;\r\nswitch (op->bytes) {\r\ncase 1:\r\nop->val = insn_fetch(s8, ctxt);\r\nbreak;\r\ncase 2:\r\nop->val = insn_fetch(s16, ctxt);\r\nbreak;\r\ncase 4:\r\nop->val = insn_fetch(s32, ctxt);\r\nbreak;\r\ncase 8:\r\nop->val = insn_fetch(s64, ctxt);\r\nbreak;\r\n}\r\nif (!sign_extension) {\r\nswitch (op->bytes) {\r\ncase 1:\r\nop->val &= 0xff;\r\nbreak;\r\ncase 2:\r\nop->val &= 0xffff;\r\nbreak;\r\ncase 4:\r\nop->val &= 0xffffffff;\r\nbreak;\r\n}\r\n}\r\ndone:\r\nreturn rc;\r\n}\r\nstatic int decode_operand(struct x86_emulate_ctxt *ctxt, struct operand *op,\r\nunsigned d)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nswitch (d) {\r\ncase OpReg:\r\ndecode_register_operand(ctxt, op);\r\nbreak;\r\ncase OpImmUByte:\r\nrc = decode_imm(ctxt, op, 1, false);\r\nbreak;\r\ncase OpMem:\r\nctxt->memop.bytes = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nmem_common:\r\n*op = ctxt->memop;\r\nctxt->memopp = op;\r\nif (ctxt->d & BitOp)\r\nfetch_bit_operand(ctxt);\r\nop->orig_val = op->val;\r\nbreak;\r\ncase OpMem64:\r\nctxt->memop.bytes = (ctxt->op_bytes == 8) ? 16 : 8;\r\ngoto mem_common;\r\ncase OpAcc:\r\nop->type = OP_REG;\r\nop->bytes = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nop->addr.reg = reg_rmw(ctxt, VCPU_REGS_RAX);\r\nfetch_register_operand(op);\r\nop->orig_val = op->val;\r\nbreak;\r\ncase OpAccLo:\r\nop->type = OP_REG;\r\nop->bytes = (ctxt->d & ByteOp) ? 2 : ctxt->op_bytes;\r\nop->addr.reg = reg_rmw(ctxt, VCPU_REGS_RAX);\r\nfetch_register_operand(op);\r\nop->orig_val = op->val;\r\nbreak;\r\ncase OpAccHi:\r\nif (ctxt->d & ByteOp) {\r\nop->type = OP_NONE;\r\nbreak;\r\n}\r\nop->type = OP_REG;\r\nop->bytes = ctxt->op_bytes;\r\nop->addr.reg = reg_rmw(ctxt, VCPU_REGS_RDX);\r\nfetch_register_operand(op);\r\nop->orig_val = op->val;\r\nbreak;\r\ncase OpDI:\r\nop->type = OP_MEM;\r\nop->bytes = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nop->addr.mem.ea =\r\nregister_address(ctxt, VCPU_REGS_RDI);\r\nop->addr.mem.seg = VCPU_SREG_ES;\r\nop->val = 0;\r\nop->count = 1;\r\nbreak;\r\ncase OpDX:\r\nop->type = OP_REG;\r\nop->bytes = 2;\r\nop->addr.reg = reg_rmw(ctxt, VCPU_REGS_RDX);\r\nfetch_register_operand(op);\r\nbreak;\r\ncase OpCL:\r\nop->type = OP_IMM;\r\nop->bytes = 1;\r\nop->val = reg_read(ctxt, VCPU_REGS_RCX) & 0xff;\r\nbreak;\r\ncase OpImmByte:\r\nrc = decode_imm(ctxt, op, 1, true);\r\nbreak;\r\ncase OpOne:\r\nop->type = OP_IMM;\r\nop->bytes = 1;\r\nop->val = 1;\r\nbreak;\r\ncase OpImm:\r\nrc = decode_imm(ctxt, op, imm_size(ctxt), true);\r\nbreak;\r\ncase OpImm64:\r\nrc = decode_imm(ctxt, op, ctxt->op_bytes, true);\r\nbreak;\r\ncase OpMem8:\r\nctxt->memop.bytes = 1;\r\nif (ctxt->memop.type == OP_REG) {\r\nctxt->memop.addr.reg = decode_register(ctxt,\r\nctxt->modrm_rm, true);\r\nfetch_register_operand(&ctxt->memop);\r\n}\r\ngoto mem_common;\r\ncase OpMem16:\r\nctxt->memop.bytes = 2;\r\ngoto mem_common;\r\ncase OpMem32:\r\nctxt->memop.bytes = 4;\r\ngoto mem_common;\r\ncase OpImmU16:\r\nrc = decode_imm(ctxt, op, 2, false);\r\nbreak;\r\ncase OpImmU:\r\nrc = decode_imm(ctxt, op, imm_size(ctxt), false);\r\nbreak;\r\ncase OpSI:\r\nop->type = OP_MEM;\r\nop->bytes = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nop->addr.mem.ea =\r\nregister_address(ctxt, VCPU_REGS_RSI);\r\nop->addr.mem.seg = ctxt->seg_override;\r\nop->val = 0;\r\nop->count = 1;\r\nbreak;\r\ncase OpXLat:\r\nop->type = OP_MEM;\r\nop->bytes = (ctxt->d & ByteOp) ? 1 : ctxt->op_bytes;\r\nop->addr.mem.ea =\r\naddress_mask(ctxt,\r\nreg_read(ctxt, VCPU_REGS_RBX) +\r\n(reg_read(ctxt, VCPU_REGS_RAX) & 0xff));\r\nop->addr.mem.seg = ctxt->seg_override;\r\nop->val = 0;\r\nbreak;\r\ncase OpImmFAddr:\r\nop->type = OP_IMM;\r\nop->addr.mem.ea = ctxt->_eip;\r\nop->bytes = ctxt->op_bytes + 2;\r\ninsn_fetch_arr(op->valptr, op->bytes, ctxt);\r\nbreak;\r\ncase OpMemFAddr:\r\nctxt->memop.bytes = ctxt->op_bytes + 2;\r\ngoto mem_common;\r\ncase OpES:\r\nop->type = OP_IMM;\r\nop->val = VCPU_SREG_ES;\r\nbreak;\r\ncase OpCS:\r\nop->type = OP_IMM;\r\nop->val = VCPU_SREG_CS;\r\nbreak;\r\ncase OpSS:\r\nop->type = OP_IMM;\r\nop->val = VCPU_SREG_SS;\r\nbreak;\r\ncase OpDS:\r\nop->type = OP_IMM;\r\nop->val = VCPU_SREG_DS;\r\nbreak;\r\ncase OpFS:\r\nop->type = OP_IMM;\r\nop->val = VCPU_SREG_FS;\r\nbreak;\r\ncase OpGS:\r\nop->type = OP_IMM;\r\nop->val = VCPU_SREG_GS;\r\nbreak;\r\ncase OpImplicit:\r\ndefault:\r\nop->type = OP_NONE;\r\nbreak;\r\n}\r\ndone:\r\nreturn rc;\r\n}\r\nint x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\r\n{\r\nint rc = X86EMUL_CONTINUE;\r\nint mode = ctxt->mode;\r\nint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\r\nbool op_prefix = false;\r\nbool has_seg_override = false;\r\nstruct opcode opcode;\r\nctxt->memop.type = OP_NONE;\r\nctxt->memopp = NULL;\r\nctxt->_eip = ctxt->eip;\r\nctxt->fetch.ptr = ctxt->fetch.data;\r\nctxt->fetch.end = ctxt->fetch.data + insn_len;\r\nctxt->opcode_len = 1;\r\nif (insn_len > 0)\r\nmemcpy(ctxt->fetch.data, insn, insn_len);\r\nelse {\r\nrc = __do_insn_fetch_bytes(ctxt, 1);\r\nif (rc != X86EMUL_CONTINUE)\r\nreturn rc;\r\n}\r\nswitch (mode) {\r\ncase X86EMUL_MODE_REAL:\r\ncase X86EMUL_MODE_VM86:\r\ncase X86EMUL_MODE_PROT16:\r\ndef_op_bytes = def_ad_bytes = 2;\r\nbreak;\r\ncase X86EMUL_MODE_PROT32:\r\ndef_op_bytes = def_ad_bytes = 4;\r\nbreak;\r\n#ifdef CONFIG_X86_64\r\ncase X86EMUL_MODE_PROT64:\r\ndef_op_bytes = 4;\r\ndef_ad_bytes = 8;\r\nbreak;\r\n#endif\r\ndefault:\r\nreturn EMULATION_FAILED;\r\n}\r\nctxt->op_bytes = def_op_bytes;\r\nctxt->ad_bytes = def_ad_bytes;\r\nfor (;;) {\r\nswitch (ctxt->b = insn_fetch(u8, ctxt)) {\r\ncase 0x66:\r\nop_prefix = true;\r\nctxt->op_bytes = def_op_bytes ^ 6;\r\nbreak;\r\ncase 0x67:\r\nif (mode == X86EMUL_MODE_PROT64)\r\nctxt->ad_bytes = def_ad_bytes ^ 12;\r\nelse\r\nctxt->ad_bytes = def_ad_bytes ^ 6;\r\nbreak;\r\ncase 0x26:\r\ncase 0x2e:\r\ncase 0x36:\r\ncase 0x3e:\r\nhas_seg_override = true;\r\nctxt->seg_override = (ctxt->b >> 3) & 3;\r\nbreak;\r\ncase 0x64:\r\ncase 0x65:\r\nhas_seg_override = true;\r\nctxt->seg_override = ctxt->b & 7;\r\nbreak;\r\ncase 0x40 ... 0x4f:\r\nif (mode != X86EMUL_MODE_PROT64)\r\ngoto done_prefixes;\r\nctxt->rex_prefix = ctxt->b;\r\ncontinue;\r\ncase 0xf0:\r\nctxt->lock_prefix = 1;\r\nbreak;\r\ncase 0xf2:\r\ncase 0xf3:\r\nctxt->rep_prefix = ctxt->b;\r\nbreak;\r\ndefault:\r\ngoto done_prefixes;\r\n}\r\nctxt->rex_prefix = 0;\r\n}\r\ndone_prefixes:\r\nif (ctxt->rex_prefix & 8)\r\nctxt->op_bytes = 8;\r\nopcode = opcode_table[ctxt->b];\r\nif (ctxt->b == 0x0f) {\r\nctxt->opcode_len = 2;\r\nctxt->b = insn_fetch(u8, ctxt);\r\nopcode = twobyte_table[ctxt->b];\r\nif (ctxt->b == 0x38) {\r\nctxt->opcode_len = 3;\r\nctxt->b = insn_fetch(u8, ctxt);\r\nopcode = opcode_map_0f_38[ctxt->b];\r\n}\r\n}\r\nctxt->d = opcode.flags;\r\nif (ctxt->d & ModRM)\r\nctxt->modrm = insn_fetch(u8, ctxt);\r\nif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\r\n(mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\r\nctxt->d = NotImpl;\r\n}\r\nwhile (ctxt->d & GroupMask) {\r\nswitch (ctxt->d & GroupMask) {\r\ncase Group:\r\ngoffset = (ctxt->modrm >> 3) & 7;\r\nopcode = opcode.u.group[goffset];\r\nbreak;\r\ncase GroupDual:\r\ngoffset = (ctxt->modrm >> 3) & 7;\r\nif ((ctxt->modrm >> 6) == 3)\r\nopcode = opcode.u.gdual->mod3[goffset];\r\nelse\r\nopcode = opcode.u.gdual->mod012[goffset];\r\nbreak;\r\ncase RMExt:\r\ngoffset = ctxt->modrm & 7;\r\nopcode = opcode.u.group[goffset];\r\nbreak;\r\ncase Prefix:\r\nif (ctxt->rep_prefix && op_prefix)\r\nreturn EMULATION_FAILED;\r\nsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\r\nswitch (simd_prefix) {\r\ncase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\r\ncase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\r\ncase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\r\ncase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\r\n}\r\nbreak;\r\ncase Escape:\r\nif (ctxt->modrm > 0xbf)\r\nopcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\r\nelse\r\nopcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\r\nbreak;\r\ncase InstrDual:\r\nif ((ctxt->modrm >> 6) == 3)\r\nopcode = opcode.u.idual->mod3;\r\nelse\r\nopcode = opcode.u.idual->mod012;\r\nbreak;\r\ncase ModeDual:\r\nif (ctxt->mode == X86EMUL_MODE_PROT64)\r\nopcode = opcode.u.mdual->mode64;\r\nelse\r\nopcode = opcode.u.mdual->mode32;\r\nbreak;\r\ndefault:\r\nreturn EMULATION_FAILED;\r\n}\r\nctxt->d &= ~(u64)GroupMask;\r\nctxt->d |= opcode.flags;\r\n}\r\nif (ctxt->d == 0)\r\nreturn EMULATION_FAILED;\r\nctxt->execute = opcode.u.execute;\r\nif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\r\nreturn EMULATION_FAILED;\r\nif (unlikely(ctxt->d &\r\n(NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\r\nNo16))) {\r\nctxt->check_perm = opcode.check_perm;\r\nctxt->intercept = opcode.intercept;\r\nif (ctxt->d & NotImpl)\r\nreturn EMULATION_FAILED;\r\nif (mode == X86EMUL_MODE_PROT64) {\r\nif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\r\nctxt->op_bytes = 8;\r\nelse if (ctxt->d & NearBranch)\r\nctxt->op_bytes = 8;\r\n}\r\nif (ctxt->d & Op3264) {\r\nif (mode == X86EMUL_MODE_PROT64)\r\nctxt->op_bytes = 8;\r\nelse\r\nctxt->op_bytes = 4;\r\n}\r\nif ((ctxt->d & No16) && ctxt->op_bytes == 2)\r\nctxt->op_bytes = 4;\r\nif (ctxt->d & Sse)\r\nctxt->op_bytes = 16;\r\nelse if (ctxt->d & Mmx)\r\nctxt->op_bytes = 8;\r\n}\r\nif (ctxt->d & ModRM) {\r\nrc = decode_modrm(ctxt, &ctxt->memop);\r\nif (!has_seg_override) {\r\nhas_seg_override = true;\r\nctxt->seg_override = ctxt->modrm_seg;\r\n}\r\n} else if (ctxt->d & MemAbs)\r\nrc = decode_abs(ctxt, &ctxt->memop);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\nif (!has_seg_override)\r\nctxt->seg_override = VCPU_SREG_DS;\r\nctxt->memop.addr.mem.seg = ctxt->seg_override;\r\nrc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\nrc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\nrc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\r\nif (ctxt->rip_relative)\r\nctxt->memopp->addr.mem.ea = address_mask(ctxt,\r\nctxt->memopp->addr.mem.ea + ctxt->_eip);\r\ndone:\r\nreturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\r\n}\r\nbool x86_page_table_writing_insn(struct x86_emulate_ctxt *ctxt)\r\n{\r\nreturn ctxt->d & PageTable;\r\n}\r\nstatic bool string_insn_completed(struct x86_emulate_ctxt *ctxt)\r\n{\r\nif (((ctxt->b == 0xa6) || (ctxt->b == 0xa7) ||\r\n(ctxt->b == 0xae) || (ctxt->b == 0xaf))\r\n&& (((ctxt->rep_prefix == REPE_PREFIX) &&\r\n((ctxt->eflags & X86_EFLAGS_ZF) == 0))\r\n|| ((ctxt->rep_prefix == REPNE_PREFIX) &&\r\n((ctxt->eflags & X86_EFLAGS_ZF) == X86_EFLAGS_ZF))))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic int flush_pending_x87_faults(struct x86_emulate_ctxt *ctxt)\r\n{\r\nbool fault = false;\r\nctxt->ops->get_fpu(ctxt);\r\nasm volatile("1: fwait \n\t"\r\n"2: \n\t"\r\n".pushsection .fixup,\"ax\" \n\t"\r\n"3: \n\t"\r\n"movb $1, %[fault] \n\t"\r\n"jmp 2b \n\t"\r\n".popsection \n\t"\r\n_ASM_EXTABLE(1b, 3b)\r\n: [fault]"+qm"(fault));\r\nctxt->ops->put_fpu(ctxt);\r\nif (unlikely(fault))\r\nreturn emulate_exception(ctxt, MF_VECTOR, 0, false);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nstatic void fetch_possible_mmx_operand(struct x86_emulate_ctxt *ctxt,\r\nstruct operand *op)\r\n{\r\nif (op->type == OP_MM)\r\nread_mmx_reg(ctxt, &op->mm_val, op->addr.mm);\r\n}\r\nstatic int fastop(struct x86_emulate_ctxt *ctxt, void (*fop)(struct fastop *))\r\n{\r\nregister void *__sp asm(_ASM_SP);\r\nulong flags = (ctxt->eflags & EFLAGS_MASK) | X86_EFLAGS_IF;\r\nif (!(ctxt->d & ByteOp))\r\nfop += __ffs(ctxt->dst.bytes) * FASTOP_SIZE;\r\nasm("push %[flags]; popf; call *%[fastop]; pushf; pop %[flags]\n"\r\n: "+a"(ctxt->dst.val), "+d"(ctxt->src.val), [flags]"+D"(flags),\r\n[fastop]"+S"(fop), "+r"(__sp)\r\n: "c"(ctxt->src2.val));\r\nctxt->eflags = (ctxt->eflags & ~EFLAGS_MASK) | (flags & EFLAGS_MASK);\r\nif (!fop)\r\nreturn emulate_de(ctxt);\r\nreturn X86EMUL_CONTINUE;\r\n}\r\nvoid init_decode_cache(struct x86_emulate_ctxt *ctxt)\r\n{\r\nmemset(&ctxt->rip_relative, 0,\r\n(void *)&ctxt->modrm - (void *)&ctxt->rip_relative);\r\nctxt->io_read.pos = 0;\r\nctxt->io_read.end = 0;\r\nctxt->mem_read.end = 0;\r\n}\r\nint x86_emulate_insn(struct x86_emulate_ctxt *ctxt)\r\n{\r\nconst struct x86_emulate_ops *ops = ctxt->ops;\r\nint rc = X86EMUL_CONTINUE;\r\nint saved_dst_type = ctxt->dst.type;\r\nctxt->mem_read.pos = 0;\r\nif (ctxt->lock_prefix && (!(ctxt->d & Lock) || ctxt->dst.type != OP_MEM)) {\r\nrc = emulate_ud(ctxt);\r\ngoto done;\r\n}\r\nif ((ctxt->d & SrcMask) == SrcMemFAddr && ctxt->src.type != OP_MEM) {\r\nrc = emulate_ud(ctxt);\r\ngoto done;\r\n}\r\nif (unlikely(ctxt->d &\r\n(No64|Undefined|Sse|Mmx|Intercept|CheckPerm|Priv|Prot|String))) {\r\nif ((ctxt->mode == X86EMUL_MODE_PROT64 && (ctxt->d & No64)) ||\r\n(ctxt->d & Undefined)) {\r\nrc = emulate_ud(ctxt);\r\ngoto done;\r\n}\r\nif (((ctxt->d & (Sse|Mmx)) && ((ops->get_cr(ctxt, 0) & X86_CR0_EM)))\r\n|| ((ctxt->d & Sse) && !(ops->get_cr(ctxt, 4) & X86_CR4_OSFXSR))) {\r\nrc = emulate_ud(ctxt);\r\ngoto done;\r\n}\r\nif ((ctxt->d & (Sse|Mmx)) && (ops->get_cr(ctxt, 0) & X86_CR0_TS)) {\r\nrc = emulate_nm(ctxt);\r\ngoto done;\r\n}\r\nif (ctxt->d & Mmx) {\r\nrc = flush_pending_x87_faults(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\nfetch_possible_mmx_operand(ctxt, &ctxt->src);\r\nfetch_possible_mmx_operand(ctxt, &ctxt->src2);\r\nif (!(ctxt->d & Mov))\r\nfetch_possible_mmx_operand(ctxt, &ctxt->dst);\r\n}\r\nif (unlikely(ctxt->emul_flags & X86EMUL_GUEST_MASK) && ctxt->intercept) {\r\nrc = emulator_check_intercept(ctxt, ctxt->intercept,\r\nX86_ICPT_PRE_EXCEPT);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\n}\r\nif ((ctxt->d & Prot) && ctxt->mode < X86EMUL_MODE_PROT16) {\r\nrc = emulate_ud(ctxt);\r\ngoto done;\r\n}\r\nif ((ctxt->d & Priv) && ops->cpl(ctxt)) {\r\nif (ctxt->d & PrivUD)\r\nrc = emulate_ud(ctxt);\r\nelse\r\nrc = emulate_gp(ctxt, 0);\r\ngoto done;\r\n}\r\nif (ctxt->d & CheckPerm) {\r\nrc = ctxt->check_perm(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\n}\r\nif (unlikely(ctxt->emul_flags & X86EMUL_GUEST_MASK) && (ctxt->d & Intercept)) {\r\nrc = emulator_check_intercept(ctxt, ctxt->intercept,\r\nX86_ICPT_POST_EXCEPT);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\n}\r\nif (ctxt->rep_prefix && (ctxt->d & String)) {\r\nif (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0) {\r\nstring_registers_quirk(ctxt);\r\nctxt->eip = ctxt->_eip;\r\nctxt->eflags &= ~X86_EFLAGS_RF;\r\ngoto done;\r\n}\r\n}\r\n}\r\nif ((ctxt->src.type == OP_MEM) && !(ctxt->d & NoAccess)) {\r\nrc = segmented_read(ctxt, ctxt->src.addr.mem,\r\nctxt->src.valptr, ctxt->src.bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\nctxt->src.orig_val64 = ctxt->src.val64;\r\n}\r\nif (ctxt->src2.type == OP_MEM) {\r\nrc = segmented_read(ctxt, ctxt->src2.addr.mem,\r\n&ctxt->src2.val, ctxt->src2.bytes);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\n}\r\nif ((ctxt->d & DstMask) == ImplicitOps)\r\ngoto special_insn;\r\nif ((ctxt->dst.type == OP_MEM) && !(ctxt->d & Mov)) {\r\nrc = segmented_read(ctxt, ctxt->dst.addr.mem,\r\n&ctxt->dst.val, ctxt->dst.bytes);\r\nif (rc != X86EMUL_CONTINUE) {\r\nif (!(ctxt->d & NoWrite) &&\r\nrc == X86EMUL_PROPAGATE_FAULT &&\r\nctxt->exception.vector == PF_VECTOR)\r\nctxt->exception.error_code |= PFERR_WRITE_MASK;\r\ngoto done;\r\n}\r\n}\r\nctxt->dst.orig_val64 = ctxt->dst.val64;\r\nspecial_insn:\r\nif (unlikely(ctxt->emul_flags & X86EMUL_GUEST_MASK) && (ctxt->d & Intercept)) {\r\nrc = emulator_check_intercept(ctxt, ctxt->intercept,\r\nX86_ICPT_POST_MEMACCESS);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\n}\r\nif (ctxt->rep_prefix && (ctxt->d & String))\r\nctxt->eflags |= X86_EFLAGS_RF;\r\nelse\r\nctxt->eflags &= ~X86_EFLAGS_RF;\r\nif (ctxt->execute) {\r\nif (ctxt->d & Fastop) {\r\nvoid (*fop)(struct fastop *) = (void *)ctxt->execute;\r\nrc = fastop(ctxt, fop);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\ngoto writeback;\r\n}\r\nrc = ctxt->execute(ctxt);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\ngoto writeback;\r\n}\r\nif (ctxt->opcode_len == 2)\r\ngoto twobyte_insn;\r\nelse if (ctxt->opcode_len == 3)\r\ngoto threebyte_insn;\r\nswitch (ctxt->b) {\r\ncase 0x70 ... 0x7f:\r\nif (test_cc(ctxt->b, ctxt->eflags))\r\nrc = jmp_rel(ctxt, ctxt->src.val);\r\nbreak;\r\ncase 0x8d:\r\nctxt->dst.val = ctxt->src.addr.mem.ea;\r\nbreak;\r\ncase 0x90 ... 0x97:\r\nif (ctxt->dst.addr.reg == reg_rmw(ctxt, VCPU_REGS_RAX))\r\nctxt->dst.type = OP_NONE;\r\nelse\r\nrc = em_xchg(ctxt);\r\nbreak;\r\ncase 0x98:\r\nswitch (ctxt->op_bytes) {\r\ncase 2: ctxt->dst.val = (s8)ctxt->dst.val; break;\r\ncase 4: ctxt->dst.val = (s16)ctxt->dst.val; break;\r\ncase 8: ctxt->dst.val = (s32)ctxt->dst.val; break;\r\n}\r\nbreak;\r\ncase 0xcc:\r\nrc = emulate_int(ctxt, 3);\r\nbreak;\r\ncase 0xcd:\r\nrc = emulate_int(ctxt, ctxt->src.val);\r\nbreak;\r\ncase 0xce:\r\nif (ctxt->eflags & X86_EFLAGS_OF)\r\nrc = emulate_int(ctxt, 4);\r\nbreak;\r\ncase 0xe9:\r\ncase 0xeb:\r\nrc = jmp_rel(ctxt, ctxt->src.val);\r\nctxt->dst.type = OP_NONE;\r\nbreak;\r\ncase 0xf4:\r\nctxt->ops->halt(ctxt);\r\nbreak;\r\ncase 0xf5:\r\nctxt->eflags ^= X86_EFLAGS_CF;\r\nbreak;\r\ncase 0xf8:\r\nctxt->eflags &= ~X86_EFLAGS_CF;\r\nbreak;\r\ncase 0xf9:\r\nctxt->eflags |= X86_EFLAGS_CF;\r\nbreak;\r\ncase 0xfc:\r\nctxt->eflags &= ~X86_EFLAGS_DF;\r\nbreak;\r\ncase 0xfd:\r\nctxt->eflags |= X86_EFLAGS_DF;\r\nbreak;\r\ndefault:\r\ngoto cannot_emulate;\r\n}\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\nwriteback:\r\nif (ctxt->d & SrcWrite) {\r\nBUG_ON(ctxt->src.type == OP_MEM || ctxt->src.type == OP_MEM_STR);\r\nrc = writeback(ctxt, &ctxt->src);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\n}\r\nif (!(ctxt->d & NoWrite)) {\r\nrc = writeback(ctxt, &ctxt->dst);\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\n}\r\nctxt->dst.type = saved_dst_type;\r\nif ((ctxt->d & SrcMask) == SrcSI)\r\nstring_addr_inc(ctxt, VCPU_REGS_RSI, &ctxt->src);\r\nif ((ctxt->d & DstMask) == DstDI)\r\nstring_addr_inc(ctxt, VCPU_REGS_RDI, &ctxt->dst);\r\nif (ctxt->rep_prefix && (ctxt->d & String)) {\r\nunsigned int count;\r\nstruct read_cache *r = &ctxt->io_read;\r\nif ((ctxt->d & SrcMask) == SrcSI)\r\ncount = ctxt->src.count;\r\nelse\r\ncount = ctxt->dst.count;\r\nregister_address_increment(ctxt, VCPU_REGS_RCX, -count);\r\nif (!string_insn_completed(ctxt)) {\r\nif ((r->end != 0 || reg_read(ctxt, VCPU_REGS_RCX) & 0x3ff) &&\r\n(r->end == 0 || r->end != r->pos)) {\r\nctxt->mem_read.end = 0;\r\nwriteback_registers(ctxt);\r\nreturn EMULATION_RESTART;\r\n}\r\ngoto done;\r\n}\r\nctxt->eflags &= ~X86_EFLAGS_RF;\r\n}\r\nctxt->eip = ctxt->_eip;\r\ndone:\r\nif (rc == X86EMUL_PROPAGATE_FAULT) {\r\nWARN_ON(ctxt->exception.vector > 0x1f);\r\nctxt->have_exception = true;\r\n}\r\nif (rc == X86EMUL_INTERCEPTED)\r\nreturn EMULATION_INTERCEPTED;\r\nif (rc == X86EMUL_CONTINUE)\r\nwriteback_registers(ctxt);\r\nreturn (rc == X86EMUL_UNHANDLEABLE) ? EMULATION_FAILED : EMULATION_OK;\r\ntwobyte_insn:\r\nswitch (ctxt->b) {\r\ncase 0x09:\r\n(ctxt->ops->wbinvd)(ctxt);\r\nbreak;\r\ncase 0x08:\r\ncase 0x0d:\r\ncase 0x18:\r\ncase 0x1f:\r\nbreak;\r\ncase 0x20:\r\nctxt->dst.val = ops->get_cr(ctxt, ctxt->modrm_reg);\r\nbreak;\r\ncase 0x21:\r\nops->get_dr(ctxt, ctxt->modrm_reg, &ctxt->dst.val);\r\nbreak;\r\ncase 0x40 ... 0x4f:\r\nif (test_cc(ctxt->b, ctxt->eflags))\r\nctxt->dst.val = ctxt->src.val;\r\nelse if (ctxt->op_bytes != 4)\r\nctxt->dst.type = OP_NONE;\r\nbreak;\r\ncase 0x80 ... 0x8f:\r\nif (test_cc(ctxt->b, ctxt->eflags))\r\nrc = jmp_rel(ctxt, ctxt->src.val);\r\nbreak;\r\ncase 0x90 ... 0x9f:\r\nctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);\r\nbreak;\r\ncase 0xb6 ... 0xb7:\r\nctxt->dst.bytes = ctxt->op_bytes;\r\nctxt->dst.val = (ctxt->src.bytes == 1) ? (u8) ctxt->src.val\r\n: (u16) ctxt->src.val;\r\nbreak;\r\ncase 0xbe ... 0xbf:\r\nctxt->dst.bytes = ctxt->op_bytes;\r\nctxt->dst.val = (ctxt->src.bytes == 1) ? (s8) ctxt->src.val :\r\n(s16) ctxt->src.val;\r\nbreak;\r\ndefault:\r\ngoto cannot_emulate;\r\n}\r\nthreebyte_insn:\r\nif (rc != X86EMUL_CONTINUE)\r\ngoto done;\r\ngoto writeback;\r\ncannot_emulate:\r\nreturn EMULATION_FAILED;\r\n}\r\nvoid emulator_invalidate_register_cache(struct x86_emulate_ctxt *ctxt)\r\n{\r\ninvalidate_registers(ctxt);\r\n}\r\nvoid emulator_writeback_register_cache(struct x86_emulate_ctxt *ctxt)\r\n{\r\nwriteback_registers(ctxt);\r\n}
