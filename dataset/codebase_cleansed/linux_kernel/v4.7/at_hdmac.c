static inline unsigned int atc_get_xfer_width(dma_addr_t src, dma_addr_t dst,\r\nsize_t len)\r\n{\r\nunsigned int width;\r\nif (!((src | dst | len) & 3))\r\nwidth = 2;\r\nelse if (!((src | dst | len) & 1))\r\nwidth = 1;\r\nelse\r\nwidth = 0;\r\nreturn width;\r\n}\r\nstatic struct at_desc *atc_first_active(struct at_dma_chan *atchan)\r\n{\r\nreturn list_first_entry(&atchan->active_list,\r\nstruct at_desc, desc_node);\r\n}\r\nstatic struct at_desc *atc_first_queued(struct at_dma_chan *atchan)\r\n{\r\nreturn list_first_entry(&atchan->queue,\r\nstruct at_desc, desc_node);\r\n}\r\nstatic struct at_desc *atc_alloc_descriptor(struct dma_chan *chan,\r\ngfp_t gfp_flags)\r\n{\r\nstruct at_desc *desc = NULL;\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\ndma_addr_t phys;\r\ndesc = dma_pool_alloc(atdma->dma_desc_pool, gfp_flags, &phys);\r\nif (desc) {\r\nmemset(desc, 0, sizeof(struct at_desc));\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->txd, chan);\r\ndesc->txd.flags = DMA_CTRL_ACK;\r\ndesc->txd.tx_submit = atc_tx_submit;\r\ndesc->txd.phys = phys;\r\n}\r\nreturn desc;\r\n}\r\nstatic struct at_desc *atc_desc_get(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *desc, *_desc;\r\nstruct at_desc *ret = NULL;\r\nunsigned long flags;\r\nunsigned int i = 0;\r\nLIST_HEAD(tmp_list);\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &atchan->free_list, desc_node) {\r\ni++;\r\nif (async_tx_test_ack(&desc->txd)) {\r\nlist_del(&desc->desc_node);\r\nret = desc;\r\nbreak;\r\n}\r\ndev_dbg(chan2dev(&atchan->chan_common),\r\n"desc %p not ACKed\n", desc);\r\n}\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"scanned %u descriptors on freelist\n", i);\r\nif (!ret) {\r\nret = atc_alloc_descriptor(&atchan->chan_common, GFP_ATOMIC);\r\nif (ret) {\r\nspin_lock_irqsave(&atchan->lock, flags);\r\natchan->descs_allocated++;\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n} else {\r\ndev_err(chan2dev(&atchan->chan_common),\r\n"not enough descriptors available\n");\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void atc_desc_put(struct at_dma_chan *atchan, struct at_desc *desc)\r\n{\r\nif (desc) {\r\nstruct at_desc *child;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"moving child desc %p to freelist\n",\r\nchild);\r\nlist_splice_init(&desc->tx_list, &atchan->free_list);\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"moving desc %p to freelist\n", desc);\r\nlist_add(&desc->desc_node, &atchan->free_list);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n}\r\n}\r\nstatic void atc_desc_chain(struct at_desc **first, struct at_desc **prev,\r\nstruct at_desc *desc)\r\n{\r\nif (!(*first)) {\r\n*first = desc;\r\n} else {\r\n(*prev)->lli.dscr = desc->txd.phys;\r\nlist_add_tail(&desc->desc_node,\r\n&(*first)->tx_list);\r\n}\r\n*prev = desc;\r\n}\r\nstatic void atc_dostart(struct at_dma_chan *atchan, struct at_desc *first)\r\n{\r\nstruct at_dma *atdma = to_at_dma(atchan->chan_common.device);\r\nif (atc_chan_is_enabled(atchan)) {\r\ndev_err(chan2dev(&atchan->chan_common),\r\n"BUG: Attempted to start non-idle channel\n");\r\ndev_err(chan2dev(&atchan->chan_common),\r\n" channel: s0x%x d0x%x ctrl0x%x:0x%x l0x%x\n",\r\nchannel_readl(atchan, SADDR),\r\nchannel_readl(atchan, DADDR),\r\nchannel_readl(atchan, CTRLA),\r\nchannel_readl(atchan, CTRLB),\r\nchannel_readl(atchan, DSCR));\r\nreturn;\r\n}\r\nvdbg_dump_regs(atchan);\r\nchannel_writel(atchan, SADDR, 0);\r\nchannel_writel(atchan, DADDR, 0);\r\nchannel_writel(atchan, CTRLA, 0);\r\nchannel_writel(atchan, CTRLB, 0);\r\nchannel_writel(atchan, DSCR, first->txd.phys);\r\nchannel_writel(atchan, SPIP, ATC_SPIP_HOLE(first->src_hole) |\r\nATC_SPIP_BOUNDARY(first->boundary));\r\nchannel_writel(atchan, DPIP, ATC_DPIP_HOLE(first->dst_hole) |\r\nATC_DPIP_BOUNDARY(first->boundary));\r\ndma_writel(atdma, CHER, atchan->mask);\r\nvdbg_dump_regs(atchan);\r\n}\r\nstatic struct at_desc *atc_get_desc_by_cookie(struct at_dma_chan *atchan,\r\ndma_cookie_t cookie)\r\n{\r\nstruct at_desc *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, &atchan->queue, desc_node) {\r\nif (desc->txd.cookie == cookie)\r\nreturn desc;\r\n}\r\nlist_for_each_entry_safe(desc, _desc, &atchan->active_list, desc_node) {\r\nif (desc->txd.cookie == cookie)\r\nreturn desc;\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline int atc_calc_bytes_left(int current_len, u32 ctrla)\r\n{\r\nu32 btsize = (ctrla & ATC_BTSIZE_MAX);\r\nu32 src_width = ATC_REG_TO_SRC_WIDTH(ctrla);\r\nreturn current_len - (btsize << src_width);\r\n}\r\nstatic int atc_get_bytes_left(struct dma_chan *chan, dma_cookie_t cookie)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_desc *desc_first = atc_first_active(atchan);\r\nstruct at_desc *desc;\r\nint ret;\r\nu32 ctrla, dscr, trials;\r\ndesc = atc_get_desc_by_cookie(atchan, cookie);\r\nif (desc == NULL)\r\nreturn -EINVAL;\r\nelse if (desc != desc_first)\r\nreturn desc->total_len;\r\nret = desc_first->total_len;\r\nif (desc_first->lli.dscr) {\r\ndscr = channel_readl(atchan, DSCR);\r\nrmb();\r\nctrla = channel_readl(atchan, CTRLA);\r\nfor (trials = 0; trials < ATC_MAX_DSCR_TRIALS; ++trials) {\r\nu32 new_dscr;\r\nrmb();\r\nnew_dscr = channel_readl(atchan, DSCR);\r\nif (likely(new_dscr == dscr))\r\nbreak;\r\ndscr = new_dscr;\r\nrmb();\r\nctrla = channel_readl(atchan, CTRLA);\r\n}\r\nif (unlikely(trials >= ATC_MAX_DSCR_TRIALS))\r\nreturn -ETIMEDOUT;\r\nif (desc_first->lli.dscr == dscr)\r\nreturn atc_calc_bytes_left(ret, ctrla);\r\nret -= desc_first->len;\r\nlist_for_each_entry(desc, &desc_first->tx_list, desc_node) {\r\nif (desc->lli.dscr == dscr)\r\nbreak;\r\nret -= desc->len;\r\n}\r\nret = atc_calc_bytes_left(ret, ctrla);\r\n} else {\r\nctrla = channel_readl(atchan, CTRLA);\r\nret = atc_calc_bytes_left(ret, ctrla);\r\n}\r\nreturn ret;\r\n}\r\nstatic void\r\natc_chain_complete(struct at_dma_chan *atchan, struct at_desc *desc)\r\n{\r\nstruct dma_async_tx_descriptor *txd = &desc->txd;\r\nstruct at_dma *atdma = to_at_dma(atchan->chan_common.device);\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"descriptor %u complete\n", txd->cookie);\r\nif (!atc_chan_is_cyclic(atchan))\r\ndma_cookie_complete(txd);\r\nif (desc->memset_buffer) {\r\ndma_pool_free(atdma->memset_pool, desc->memset_vaddr,\r\ndesc->memset_paddr);\r\ndesc->memset_buffer = false;\r\n}\r\nlist_splice_init(&desc->tx_list, &atchan->free_list);\r\nlist_move(&desc->desc_node, &atchan->free_list);\r\ndma_descriptor_unmap(txd);\r\nif (!atc_chan_is_cyclic(atchan)) {\r\ndma_async_tx_callback callback = txd->callback;\r\nvoid *param = txd->callback_param;\r\nif (callback)\r\ncallback(param);\r\n}\r\ndma_run_dependencies(txd);\r\n}\r\nstatic void atc_complete_all(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\ndev_vdbg(chan2dev(&atchan->chan_common), "complete all\n");\r\nif (!list_empty(&atchan->queue))\r\natc_dostart(atchan, atc_first_queued(atchan));\r\nlist_splice_init(&atchan->active_list, &list);\r\nlist_splice_init(&atchan->queue, &atchan->active_list);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\natc_chain_complete(atchan, desc);\r\n}\r\nstatic void atc_advance_work(struct at_dma_chan *atchan)\r\n{\r\ndev_vdbg(chan2dev(&atchan->chan_common), "advance_work\n");\r\nif (atc_chan_is_enabled(atchan))\r\nreturn;\r\nif (list_empty(&atchan->active_list) ||\r\nlist_is_singular(&atchan->active_list)) {\r\natc_complete_all(atchan);\r\n} else {\r\natc_chain_complete(atchan, atc_first_active(atchan));\r\natc_dostart(atchan, atc_first_active(atchan));\r\n}\r\n}\r\nstatic void atc_handle_error(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *bad_desc;\r\nstruct at_desc *child;\r\nbad_desc = atc_first_active(atchan);\r\nlist_del_init(&bad_desc->desc_node);\r\nlist_splice_init(&atchan->queue, atchan->active_list.prev);\r\nif (!list_empty(&atchan->active_list))\r\natc_dostart(atchan, atc_first_active(atchan));\r\ndev_crit(chan2dev(&atchan->chan_common),\r\n"Bad descriptor submitted for DMA!\n");\r\ndev_crit(chan2dev(&atchan->chan_common),\r\n" cookie: %d\n", bad_desc->txd.cookie);\r\natc_dump_lli(atchan, &bad_desc->lli);\r\nlist_for_each_entry(child, &bad_desc->tx_list, desc_node)\r\natc_dump_lli(atchan, &child->lli);\r\natc_chain_complete(atchan, bad_desc);\r\n}\r\nstatic void atc_handle_cyclic(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *first = atc_first_active(atchan);\r\nstruct dma_async_tx_descriptor *txd = &first->txd;\r\ndma_async_tx_callback callback = txd->callback;\r\nvoid *param = txd->callback_param;\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"new cyclic period llp 0x%08x\n",\r\nchannel_readl(atchan, DSCR));\r\nif (callback)\r\ncallback(param);\r\n}\r\nstatic void atc_tasklet(unsigned long data)\r\n{\r\nstruct at_dma_chan *atchan = (struct at_dma_chan *)data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nif (test_and_clear_bit(ATC_IS_ERROR, &atchan->status))\r\natc_handle_error(atchan);\r\nelse if (atc_chan_is_cyclic(atchan))\r\natc_handle_cyclic(atchan);\r\nelse\r\natc_advance_work(atchan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n}\r\nstatic irqreturn_t at_dma_interrupt(int irq, void *dev_id)\r\n{\r\nstruct at_dma *atdma = (struct at_dma *)dev_id;\r\nstruct at_dma_chan *atchan;\r\nint i;\r\nu32 status, pending, imr;\r\nint ret = IRQ_NONE;\r\ndo {\r\nimr = dma_readl(atdma, EBCIMR);\r\nstatus = dma_readl(atdma, EBCISR);\r\npending = status & imr;\r\nif (!pending)\r\nbreak;\r\ndev_vdbg(atdma->dma_common.dev,\r\n"interrupt: status = 0x%08x, 0x%08x, 0x%08x\n",\r\nstatus, imr, pending);\r\nfor (i = 0; i < atdma->dma_common.chancnt; i++) {\r\natchan = &atdma->chan[i];\r\nif (pending & (AT_DMA_BTC(i) | AT_DMA_ERR(i))) {\r\nif (pending & AT_DMA_ERR(i)) {\r\ndma_writel(atdma, CHDR,\r\nAT_DMA_RES(i) | atchan->mask);\r\nset_bit(ATC_IS_ERROR, &atchan->status);\r\n}\r\ntasklet_schedule(&atchan->tasklet);\r\nret = IRQ_HANDLED;\r\n}\r\n}\r\n} while (pending);\r\nreturn ret;\r\n}\r\nstatic dma_cookie_t atc_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct at_desc *desc = txd_to_at_desc(tx);\r\nstruct at_dma_chan *atchan = to_at_dma_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\nif (list_empty(&atchan->active_list)) {\r\ndev_vdbg(chan2dev(tx->chan), "tx_submit: started %u\n",\r\ndesc->txd.cookie);\r\natc_dostart(atchan, desc);\r\nlist_add_tail(&desc->desc_node, &atchan->active_list);\r\n} else {\r\ndev_vdbg(chan2dev(tx->chan), "tx_submit: queued %u\n",\r\ndesc->txd.cookie);\r\nlist_add_tail(&desc->desc_node, &atchan->queue);\r\n}\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_interleaved(struct dma_chan *chan,\r\nstruct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct data_chunk *first = xt->sgl;\r\nstruct at_desc *desc = NULL;\r\nsize_t xfer_count;\r\nunsigned int dwidth;\r\nu32 ctrla;\r\nu32 ctrlb;\r\nsize_t len = 0;\r\nint i;\r\nif (unlikely(!xt || xt->numf != 1 || !xt->frame_size))\r\nreturn NULL;\r\ndev_info(chan2dev(chan),\r\n"%s: src=%pad, dest=%pad, numf=%d, frame_size=%d, flags=0x%lx\n",\r\n__func__, &xt->src_start, &xt->dst_start, xt->numf,\r\nxt->frame_size, flags);\r\nfor (i = 0; i < xt->frame_size; i++) {\r\nstruct data_chunk *chunk = xt->sgl + i;\r\nif ((chunk->size != xt->sgl->size) ||\r\n(dmaengine_get_dst_icg(xt, chunk) != dmaengine_get_dst_icg(xt, first)) ||\r\n(dmaengine_get_src_icg(xt, chunk) != dmaengine_get_src_icg(xt, first))) {\r\ndev_err(chan2dev(chan),\r\n"%s: the controller can transfer only identical chunks\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\nlen += chunk->size;\r\n}\r\ndwidth = atc_get_xfer_width(xt->src_start,\r\nxt->dst_start, len);\r\nxfer_count = len >> dwidth;\r\nif (xfer_count > ATC_BTSIZE_MAX) {\r\ndev_err(chan2dev(chan), "%s: buffer is too big\n", __func__);\r\nreturn NULL;\r\n}\r\nctrla = ATC_SRC_WIDTH(dwidth) |\r\nATC_DST_WIDTH(dwidth);\r\nctrlb = ATC_DEFAULT_CTRLB | ATC_IEN\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_DST_ADDR_MODE_INCR\r\n| ATC_SRC_PIP\r\n| ATC_DST_PIP\r\n| ATC_FC_MEM2MEM;\r\ndesc = atc_desc_get(atchan);\r\nif (!desc) {\r\ndev_err(chan2dev(chan),\r\n"%s: couldn't allocate our descriptor\n", __func__);\r\nreturn NULL;\r\n}\r\ndesc->lli.saddr = xt->src_start;\r\ndesc->lli.daddr = xt->dst_start;\r\ndesc->lli.ctrla = ctrla | xfer_count;\r\ndesc->lli.ctrlb = ctrlb;\r\ndesc->boundary = first->size >> dwidth;\r\ndesc->dst_hole = (dmaengine_get_dst_icg(xt, first) >> dwidth) + 1;\r\ndesc->src_hole = (dmaengine_get_src_icg(xt, first) >> dwidth) + 1;\r\ndesc->txd.cookie = -EBUSY;\r\ndesc->total_len = desc->len = len;\r\nset_desc_eol(desc);\r\ndesc->txd.flags = flags;\r\nreturn &desc->txd;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_desc *desc = NULL;\r\nstruct at_desc *first = NULL;\r\nstruct at_desc *prev = NULL;\r\nsize_t xfer_count;\r\nsize_t offset;\r\nunsigned int src_width;\r\nunsigned int dst_width;\r\nu32 ctrla;\r\nu32 ctrlb;\r\ndev_vdbg(chan2dev(chan), "prep_dma_memcpy: d%pad s%pad l0x%zx f0x%lx\n",\r\n&dest, &src, len, flags);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan), "prep_dma_memcpy: length is zero!\n");\r\nreturn NULL;\r\n}\r\nctrlb = ATC_DEFAULT_CTRLB | ATC_IEN\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_DST_ADDR_MODE_INCR\r\n| ATC_FC_MEM2MEM;\r\nsrc_width = dst_width = atc_get_xfer_width(src, dest, len);\r\nctrla = ATC_SRC_WIDTH(src_width) |\r\nATC_DST_WIDTH(dst_width);\r\nfor (offset = 0; offset < len; offset += xfer_count << src_width) {\r\nxfer_count = min_t(size_t, (len - offset) >> src_width,\r\nATC_BTSIZE_MAX);\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\ndesc->lli.saddr = src + offset;\r\ndesc->lli.daddr = dest + offset;\r\ndesc->lli.ctrla = ctrla | xfer_count;\r\ndesc->lli.ctrlb = ctrlb;\r\ndesc->txd.cookie = 0;\r\ndesc->len = xfer_count << src_width;\r\natc_desc_chain(&first, &prev, desc);\r\n}\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->total_len = len;\r\nset_desc_eol(desc);\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nerr_desc_get:\r\natc_desc_put(atchan, first);\r\nreturn NULL;\r\n}\r\nstatic struct at_desc *atc_create_memset_desc(struct dma_chan *chan,\r\ndma_addr_t psrc,\r\ndma_addr_t pdst,\r\nsize_t len)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_desc *desc;\r\nsize_t xfer_count;\r\nu32 ctrla = ATC_SRC_WIDTH(2) | ATC_DST_WIDTH(2);\r\nu32 ctrlb = ATC_DEFAULT_CTRLB | ATC_IEN |\r\nATC_SRC_ADDR_MODE_FIXED |\r\nATC_DST_ADDR_MODE_INCR |\r\nATC_FC_MEM2MEM;\r\nxfer_count = len >> 2;\r\nif (xfer_count > ATC_BTSIZE_MAX) {\r\ndev_err(chan2dev(chan), "%s: buffer is too big\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\ndesc = atc_desc_get(atchan);\r\nif (!desc) {\r\ndev_err(chan2dev(chan), "%s: can't get a descriptor\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\ndesc->lli.saddr = psrc;\r\ndesc->lli.daddr = pdst;\r\ndesc->lli.ctrla = ctrla | xfer_count;\r\ndesc->lli.ctrlb = ctrlb;\r\ndesc->txd.cookie = 0;\r\ndesc->len = len;\r\nreturn desc;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_memset(struct dma_chan *chan, dma_addr_t dest, int value,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nstruct at_desc *desc;\r\nvoid __iomem *vaddr;\r\ndma_addr_t paddr;\r\ndev_vdbg(chan2dev(chan), "%s: d%pad v0x%x l0x%zx f0x%lx\n", __func__,\r\n&dest, value, len, flags);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan), "%s: length is zero!\n", __func__);\r\nreturn NULL;\r\n}\r\nif (!is_dma_fill_aligned(chan->device, dest, 0, len)) {\r\ndev_dbg(chan2dev(chan), "%s: buffer is not aligned\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\nvaddr = dma_pool_alloc(atdma->memset_pool, GFP_ATOMIC, &paddr);\r\nif (!vaddr) {\r\ndev_err(chan2dev(chan), "%s: couldn't allocate buffer\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\n*(u32*)vaddr = value;\r\ndesc = atc_create_memset_desc(chan, paddr, dest, len);\r\nif (!desc) {\r\ndev_err(chan2dev(chan), "%s: couldn't get a descriptor\n",\r\n__func__);\r\ngoto err_free_buffer;\r\n}\r\ndesc->memset_paddr = paddr;\r\ndesc->memset_vaddr = vaddr;\r\ndesc->memset_buffer = true;\r\ndesc->txd.cookie = -EBUSY;\r\ndesc->total_len = len;\r\nset_desc_eol(desc);\r\ndesc->txd.flags = flags;\r\nreturn &desc->txd;\r\nerr_free_buffer:\r\ndma_pool_free(atdma->memset_pool, vaddr, paddr);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_memset_sg(struct dma_chan *chan,\r\nstruct scatterlist *sgl,\r\nunsigned int sg_len, int value,\r\nunsigned long flags)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nstruct at_desc *desc = NULL, *first = NULL, *prev = NULL;\r\nstruct scatterlist *sg;\r\nvoid __iomem *vaddr;\r\ndma_addr_t paddr;\r\nsize_t total_len = 0;\r\nint i;\r\ndev_vdbg(chan2dev(chan), "%s: v0x%x l0x%zx f0x%lx\n", __func__,\r\nvalue, sg_len, flags);\r\nif (unlikely(!sgl || !sg_len)) {\r\ndev_dbg(chan2dev(chan), "%s: scatterlist is empty!\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\nvaddr = dma_pool_alloc(atdma->memset_pool, GFP_ATOMIC, &paddr);\r\nif (!vaddr) {\r\ndev_err(chan2dev(chan), "%s: couldn't allocate buffer\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\n*(u32*)vaddr = value;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndma_addr_t dest = sg_dma_address(sg);\r\nsize_t len = sg_dma_len(sg);\r\ndev_vdbg(chan2dev(chan), "%s: d%pad, l0x%zx\n",\r\n__func__, &dest, len);\r\nif (!is_dma_fill_aligned(chan->device, dest, 0, len)) {\r\ndev_err(chan2dev(chan), "%s: buffer is not aligned\n",\r\n__func__);\r\ngoto err_put_desc;\r\n}\r\ndesc = atc_create_memset_desc(chan, paddr, dest, len);\r\nif (!desc)\r\ngoto err_put_desc;\r\natc_desc_chain(&first, &prev, desc);\r\ntotal_len += len;\r\n}\r\ndesc->memset_paddr = paddr;\r\ndesc->memset_vaddr = vaddr;\r\ndesc->memset_buffer = true;\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->total_len = total_len;\r\nset_desc_eol(desc);\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nerr_put_desc:\r\natc_desc_put(atchan, first);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma_slave *atslave = chan->private;\r\nstruct dma_slave_config *sconfig = &atchan->dma_sconfig;\r\nstruct at_desc *first = NULL;\r\nstruct at_desc *prev = NULL;\r\nu32 ctrla;\r\nu32 ctrlb;\r\ndma_addr_t reg;\r\nunsigned int reg_width;\r\nunsigned int mem_width;\r\nunsigned int i;\r\nstruct scatterlist *sg;\r\nsize_t total_len = 0;\r\ndev_vdbg(chan2dev(chan), "prep_slave_sg (%d): %s f0x%lx\n",\r\nsg_len,\r\ndirection == DMA_MEM_TO_DEV ? "TO DEVICE" : "FROM DEVICE",\r\nflags);\r\nif (unlikely(!atslave || !sg_len)) {\r\ndev_dbg(chan2dev(chan), "prep_slave_sg: sg length is zero!\n");\r\nreturn NULL;\r\n}\r\nctrla = ATC_SCSIZE(sconfig->src_maxburst)\r\n| ATC_DCSIZE(sconfig->dst_maxburst);\r\nctrlb = ATC_IEN;\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\nreg_width = convert_buswidth(sconfig->dst_addr_width);\r\nctrla |= ATC_DST_WIDTH(reg_width);\r\nctrlb |= ATC_DST_ADDR_MODE_FIXED\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_FC_MEM2PER\r\n| ATC_SIF(atchan->mem_if) | ATC_DIF(atchan->per_if);\r\nreg = sconfig->dst_addr;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct at_desc *desc;\r\nu32 len;\r\nu32 mem;\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\nmem = sg_dma_address(sg);\r\nlen = sg_dma_len(sg);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan),\r\n"prep_slave_sg: sg(%d) data length is zero\n", i);\r\ngoto err;\r\n}\r\nmem_width = 2;\r\nif (unlikely(mem & 3 || len & 3))\r\nmem_width = 0;\r\ndesc->lli.saddr = mem;\r\ndesc->lli.daddr = reg;\r\ndesc->lli.ctrla = ctrla\r\n| ATC_SRC_WIDTH(mem_width)\r\n| len >> mem_width;\r\ndesc->lli.ctrlb = ctrlb;\r\ndesc->len = len;\r\natc_desc_chain(&first, &prev, desc);\r\ntotal_len += len;\r\n}\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\nreg_width = convert_buswidth(sconfig->src_addr_width);\r\nctrla |= ATC_SRC_WIDTH(reg_width);\r\nctrlb |= ATC_DST_ADDR_MODE_INCR\r\n| ATC_SRC_ADDR_MODE_FIXED\r\n| ATC_FC_PER2MEM\r\n| ATC_SIF(atchan->per_if) | ATC_DIF(atchan->mem_if);\r\nreg = sconfig->src_addr;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct at_desc *desc;\r\nu32 len;\r\nu32 mem;\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\nmem = sg_dma_address(sg);\r\nlen = sg_dma_len(sg);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan),\r\n"prep_slave_sg: sg(%d) data length is zero\n", i);\r\ngoto err;\r\n}\r\nmem_width = 2;\r\nif (unlikely(mem & 3 || len & 3))\r\nmem_width = 0;\r\ndesc->lli.saddr = reg;\r\ndesc->lli.daddr = mem;\r\ndesc->lli.ctrla = ctrla\r\n| ATC_DST_WIDTH(mem_width)\r\n| len >> reg_width;\r\ndesc->lli.ctrlb = ctrlb;\r\ndesc->len = len;\r\natc_desc_chain(&first, &prev, desc);\r\ntotal_len += len;\r\n}\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nset_desc_eol(prev);\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->total_len = total_len;\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nerr_desc_get:\r\ndev_err(chan2dev(chan), "not enough descriptors available\n");\r\nerr:\r\natc_desc_put(atchan, first);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_sg(struct dma_chan *chan,\r\nstruct scatterlist *dst_sg, unsigned int dst_nents,\r\nstruct scatterlist *src_sg, unsigned int src_nents,\r\nunsigned long flags)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_desc *desc = NULL;\r\nstruct at_desc *first = NULL;\r\nstruct at_desc *prev = NULL;\r\nunsigned int src_width;\r\nunsigned int dst_width;\r\nsize_t xfer_count;\r\nu32 ctrla;\r\nu32 ctrlb;\r\nsize_t dst_len = 0, src_len = 0;\r\ndma_addr_t dst = 0, src = 0;\r\nsize_t len = 0, total_len = 0;\r\nif (unlikely(dst_nents == 0 || src_nents == 0))\r\nreturn NULL;\r\nif (unlikely(dst_sg == NULL || src_sg == NULL))\r\nreturn NULL;\r\nctrlb = ATC_DEFAULT_CTRLB | ATC_IEN\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_DST_ADDR_MODE_INCR\r\n| ATC_FC_MEM2MEM;\r\nwhile (true) {\r\nif (dst_len == 0) {\r\nif (!dst_sg || !dst_nents)\r\nbreak;\r\ndst = sg_dma_address(dst_sg);\r\ndst_len = sg_dma_len(dst_sg);\r\ndst_sg = sg_next(dst_sg);\r\ndst_nents--;\r\n}\r\nif (src_len == 0) {\r\nif (!src_sg || !src_nents)\r\nbreak;\r\nsrc = sg_dma_address(src_sg);\r\nsrc_len = sg_dma_len(src_sg);\r\nsrc_sg = sg_next(src_sg);\r\nsrc_nents--;\r\n}\r\nlen = min_t(size_t, src_len, dst_len);\r\nif (len == 0)\r\ncontinue;\r\nsrc_width = dst_width = atc_get_xfer_width(src, dst, len);\r\nctrla = ATC_SRC_WIDTH(src_width) |\r\nATC_DST_WIDTH(dst_width);\r\nxfer_count = len >> src_width;\r\nif (xfer_count > ATC_BTSIZE_MAX) {\r\nxfer_count = ATC_BTSIZE_MAX;\r\nlen = ATC_BTSIZE_MAX << src_width;\r\n}\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\ndesc->lli.saddr = src;\r\ndesc->lli.daddr = dst;\r\ndesc->lli.ctrla = ctrla | xfer_count;\r\ndesc->lli.ctrlb = ctrlb;\r\ndesc->txd.cookie = 0;\r\ndesc->len = len;\r\natc_desc_chain(&first, &prev, desc);\r\ndst_len -= len;\r\nsrc_len -= len;\r\ndst += len;\r\nsrc += len;\r\ntotal_len += len;\r\n}\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->total_len = total_len;\r\nset_desc_eol(desc);\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nerr_desc_get:\r\natc_desc_put(atchan, first);\r\nreturn NULL;\r\n}\r\nstatic int\r\natc_dma_cyclic_check_values(unsigned int reg_width, dma_addr_t buf_addr,\r\nsize_t period_len)\r\n{\r\nif (period_len > (ATC_BTSIZE_MAX << reg_width))\r\ngoto err_out;\r\nif (unlikely(period_len & ((1 << reg_width) - 1)))\r\ngoto err_out;\r\nif (unlikely(buf_addr & ((1 << reg_width) - 1)))\r\ngoto err_out;\r\nreturn 0;\r\nerr_out:\r\nreturn -EINVAL;\r\n}\r\nstatic int\r\natc_dma_cyclic_fill_desc(struct dma_chan *chan, struct at_desc *desc,\r\nunsigned int period_index, dma_addr_t buf_addr,\r\nunsigned int reg_width, size_t period_len,\r\nenum dma_transfer_direction direction)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct dma_slave_config *sconfig = &atchan->dma_sconfig;\r\nu32 ctrla;\r\nctrla = ATC_SCSIZE(sconfig->src_maxburst)\r\n| ATC_DCSIZE(sconfig->dst_maxburst)\r\n| ATC_DST_WIDTH(reg_width)\r\n| ATC_SRC_WIDTH(reg_width)\r\n| period_len >> reg_width;\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\ndesc->lli.saddr = buf_addr + (period_len * period_index);\r\ndesc->lli.daddr = sconfig->dst_addr;\r\ndesc->lli.ctrla = ctrla;\r\ndesc->lli.ctrlb = ATC_DST_ADDR_MODE_FIXED\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_FC_MEM2PER\r\n| ATC_SIF(atchan->mem_if)\r\n| ATC_DIF(atchan->per_if);\r\ndesc->len = period_len;\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\ndesc->lli.saddr = sconfig->src_addr;\r\ndesc->lli.daddr = buf_addr + (period_len * period_index);\r\ndesc->lli.ctrla = ctrla;\r\ndesc->lli.ctrlb = ATC_DST_ADDR_MODE_INCR\r\n| ATC_SRC_ADDR_MODE_FIXED\r\n| ATC_FC_PER2MEM\r\n| ATC_SIF(atchan->per_if)\r\n| ATC_DIF(atchan->mem_if);\r\ndesc->len = period_len;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma_slave *atslave = chan->private;\r\nstruct dma_slave_config *sconfig = &atchan->dma_sconfig;\r\nstruct at_desc *first = NULL;\r\nstruct at_desc *prev = NULL;\r\nunsigned long was_cyclic;\r\nunsigned int reg_width;\r\nunsigned int periods = buf_len / period_len;\r\nunsigned int i;\r\ndev_vdbg(chan2dev(chan), "prep_dma_cyclic: %s buf@%pad - %d (%d/%d)\n",\r\ndirection == DMA_MEM_TO_DEV ? "TO DEVICE" : "FROM DEVICE",\r\n&buf_addr,\r\nperiods, buf_len, period_len);\r\nif (unlikely(!atslave || !buf_len || !period_len)) {\r\ndev_dbg(chan2dev(chan), "prep_dma_cyclic: length is zero!\n");\r\nreturn NULL;\r\n}\r\nwas_cyclic = test_and_set_bit(ATC_IS_CYCLIC, &atchan->status);\r\nif (was_cyclic) {\r\ndev_dbg(chan2dev(chan), "prep_dma_cyclic: channel in use!\n");\r\nreturn NULL;\r\n}\r\nif (unlikely(!is_slave_direction(direction)))\r\ngoto err_out;\r\nif (sconfig->direction == DMA_MEM_TO_DEV)\r\nreg_width = convert_buswidth(sconfig->dst_addr_width);\r\nelse\r\nreg_width = convert_buswidth(sconfig->src_addr_width);\r\nif (atc_dma_cyclic_check_values(reg_width, buf_addr, period_len))\r\ngoto err_out;\r\nfor (i = 0; i < periods; i++) {\r\nstruct at_desc *desc;\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\nif (atc_dma_cyclic_fill_desc(chan, desc, i, buf_addr,\r\nreg_width, period_len, direction))\r\ngoto err_desc_get;\r\natc_desc_chain(&first, &prev, desc);\r\n}\r\nprev->lli.dscr = first->txd.phys;\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->total_len = buf_len;\r\nreturn &first->txd;\r\nerr_desc_get:\r\ndev_err(chan2dev(chan), "not enough descriptors available\n");\r\natc_desc_put(atchan, first);\r\nerr_out:\r\nclear_bit(ATC_IS_CYCLIC, &atchan->status);\r\nreturn NULL;\r\n}\r\nstatic int atc_config(struct dma_chan *chan,\r\nstruct dma_slave_config *sconfig)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\ndev_vdbg(chan2dev(chan), "%s\n", __func__);\r\nif (!chan->private)\r\nreturn -EINVAL;\r\nmemcpy(&atchan->dma_sconfig, sconfig, sizeof(*sconfig));\r\nconvert_burst(&atchan->dma_sconfig.src_maxburst);\r\nconvert_burst(&atchan->dma_sconfig.dst_maxburst);\r\nreturn 0;\r\n}\r\nstatic int atc_pause(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nint chan_id = atchan->chan_common.chan_id;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\ndev_vdbg(chan2dev(chan), "%s\n", __func__);\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndma_writel(atdma, CHER, AT_DMA_SUSP(chan_id));\r\nset_bit(ATC_IS_PAUSED, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int atc_resume(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nint chan_id = atchan->chan_common.chan_id;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\ndev_vdbg(chan2dev(chan), "%s\n", __func__);\r\nif (!atc_chan_is_paused(atchan))\r\nreturn 0;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndma_writel(atdma, CHDR, AT_DMA_RES(chan_id));\r\nclear_bit(ATC_IS_PAUSED, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int atc_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nint chan_id = atchan->chan_common.chan_id;\r\nstruct at_desc *desc, *_desc;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\ndev_vdbg(chan2dev(chan), "%s\n", __func__);\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndma_writel(atdma, CHDR, AT_DMA_RES(chan_id) | atchan->mask);\r\nwhile (dma_readl(atdma, CHSR) & atchan->mask)\r\ncpu_relax();\r\nlist_splice_init(&atchan->queue, &list);\r\nlist_splice_init(&atchan->active_list, &list);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\natc_chain_complete(atchan, desc);\r\nclear_bit(ATC_IS_PAUSED, &atchan->status);\r\nclear_bit(ATC_IS_CYCLIC, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic enum dma_status\r\natc_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nunsigned long flags;\r\nenum dma_status ret;\r\nint bytes = 0;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nif (!txstate)\r\nreturn DMA_ERROR;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nbytes = atc_get_bytes_left(chan, cookie);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nif (unlikely(bytes < 0)) {\r\ndev_vdbg(chan2dev(chan), "get residual bytes error\n");\r\nreturn DMA_ERROR;\r\n} else {\r\ndma_set_residue(txstate, bytes);\r\n}\r\ndev_vdbg(chan2dev(chan), "tx_status %d: cookie = %d residue = %d\n",\r\nret, cookie, bytes);\r\nreturn ret;\r\n}\r\nstatic void atc_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nunsigned long flags;\r\ndev_vdbg(chan2dev(chan), "issue_pending\n");\r\nif (atc_chan_is_cyclic(atchan))\r\nreturn;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\natc_advance_work(atchan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n}\r\nstatic int atc_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nstruct at_desc *desc;\r\nstruct at_dma_slave *atslave;\r\nunsigned long flags;\r\nint i;\r\nu32 cfg;\r\nLIST_HEAD(tmp_list);\r\ndev_vdbg(chan2dev(chan), "alloc_chan_resources\n");\r\nif (atc_chan_is_enabled(atchan)) {\r\ndev_dbg(chan2dev(chan), "DMA channel not idle ?\n");\r\nreturn -EIO;\r\n}\r\ncfg = ATC_DEFAULT_CFG;\r\natslave = chan->private;\r\nif (atslave) {\r\nBUG_ON(!atslave->dma_dev || atslave->dma_dev != atdma->dma_common.dev);\r\nif (atslave->cfg)\r\ncfg = atslave->cfg;\r\n}\r\nif (!list_empty(&atchan->free_list))\r\nreturn atchan->descs_allocated;\r\nfor (i = 0; i < init_nr_desc_per_channel; i++) {\r\ndesc = atc_alloc_descriptor(chan, GFP_KERNEL);\r\nif (!desc) {\r\ndev_err(atdma->dma_common.dev,\r\n"Only %d initial descriptors\n", i);\r\nbreak;\r\n}\r\nlist_add_tail(&desc->desc_node, &tmp_list);\r\n}\r\nspin_lock_irqsave(&atchan->lock, flags);\r\natchan->descs_allocated = i;\r\nlist_splice(&tmp_list, &atchan->free_list);\r\ndma_cookie_init(chan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nchannel_writel(atchan, CFG, cfg);\r\ndev_dbg(chan2dev(chan),\r\n"alloc_chan_resources: allocated %d descriptors\n",\r\natchan->descs_allocated);\r\nreturn atchan->descs_allocated;\r\n}\r\nstatic void atc_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nstruct at_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\ndev_dbg(chan2dev(chan), "free_chan_resources: (descs allocated=%u)\n",\r\natchan->descs_allocated);\r\nBUG_ON(!list_empty(&atchan->active_list));\r\nBUG_ON(!list_empty(&atchan->queue));\r\nBUG_ON(atc_chan_is_enabled(atchan));\r\nlist_for_each_entry_safe(desc, _desc, &atchan->free_list, desc_node) {\r\ndev_vdbg(chan2dev(chan), " freeing descriptor %p\n", desc);\r\nlist_del(&desc->desc_node);\r\ndma_pool_free(atdma->dma_desc_pool, desc, desc->txd.phys);\r\n}\r\nlist_splice_init(&atchan->free_list, &list);\r\natchan->descs_allocated = 0;\r\natchan->status = 0;\r\ndev_vdbg(chan2dev(chan), "free_chan_resources: done\n");\r\n}\r\nstatic bool at_dma_filter(struct dma_chan *chan, void *slave)\r\n{\r\nstruct at_dma_slave *atslave = slave;\r\nif (atslave->dma_dev == chan->device->dev) {\r\nchan->private = atslave;\r\nreturn true;\r\n} else {\r\nreturn false;\r\n}\r\n}\r\nstatic struct dma_chan *at_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *of_dma)\r\n{\r\nstruct dma_chan *chan;\r\nstruct at_dma_chan *atchan;\r\nstruct at_dma_slave *atslave;\r\ndma_cap_mask_t mask;\r\nunsigned int per_id;\r\nstruct platform_device *dmac_pdev;\r\nif (dma_spec->args_count != 2)\r\nreturn NULL;\r\ndmac_pdev = of_find_device_by_node(dma_spec->np);\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\natslave = devm_kzalloc(&dmac_pdev->dev, sizeof(*atslave), GFP_KERNEL);\r\nif (!atslave)\r\nreturn NULL;\r\natslave->cfg = ATC_DST_H2SEL_HW | ATC_SRC_H2SEL_HW;\r\nper_id = dma_spec->args[1] & AT91_DMA_CFG_PER_ID_MASK;\r\natslave->cfg |= ATC_DST_PER_MSB(per_id) | ATC_DST_PER(per_id)\r\n| ATC_SRC_PER_MSB(per_id) | ATC_SRC_PER(per_id);\r\nswitch (dma_spec->args[1] & AT91_DMA_CFG_FIFOCFG_MASK) {\r\ncase AT91_DMA_CFG_FIFOCFG_ALAP:\r\natslave->cfg |= ATC_FIFOCFG_LARGESTBURST;\r\nbreak;\r\ncase AT91_DMA_CFG_FIFOCFG_ASAP:\r\natslave->cfg |= ATC_FIFOCFG_ENOUGHSPACE;\r\nbreak;\r\ncase AT91_DMA_CFG_FIFOCFG_HALF:\r\ndefault:\r\natslave->cfg |= ATC_FIFOCFG_HALFFIFO;\r\n}\r\natslave->dma_dev = &dmac_pdev->dev;\r\nchan = dma_request_channel(mask, at_dma_filter, atslave);\r\nif (!chan)\r\nreturn NULL;\r\natchan = to_at_dma_chan(chan);\r\natchan->per_if = dma_spec->args[0] & 0xff;\r\natchan->mem_if = (dma_spec->args[0] >> 16) & 0xff;\r\nreturn chan;\r\n}\r\nstatic struct dma_chan *at_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *of_dma)\r\n{\r\nreturn NULL;\r\n}\r\nstatic inline const struct at_dma_platform_data * __init at_dma_get_driver_data(\r\nstruct platform_device *pdev)\r\n{\r\nif (pdev->dev.of_node) {\r\nconst struct of_device_id *match;\r\nmatch = of_match_node(atmel_dma_dt_ids, pdev->dev.of_node);\r\nif (match == NULL)\r\nreturn NULL;\r\nreturn match->data;\r\n}\r\nreturn (struct at_dma_platform_data *)\r\nplatform_get_device_id(pdev)->driver_data;\r\n}\r\nstatic void at_dma_off(struct at_dma *atdma)\r\n{\r\ndma_writel(atdma, EN, 0);\r\ndma_writel(atdma, EBCIDR, -1L);\r\nwhile (dma_readl(atdma, CHSR) & atdma->all_chan_mask)\r\ncpu_relax();\r\n}\r\nstatic int __init at_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *io;\r\nstruct at_dma *atdma;\r\nsize_t size;\r\nint irq;\r\nint err;\r\nint i;\r\nconst struct at_dma_platform_data *plat_dat;\r\ndma_cap_set(DMA_MEMCPY, at91sam9rl_config.cap_mask);\r\ndma_cap_set(DMA_SG, at91sam9rl_config.cap_mask);\r\ndma_cap_set(DMA_INTERLEAVE, at91sam9g45_config.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, at91sam9g45_config.cap_mask);\r\ndma_cap_set(DMA_MEMSET, at91sam9g45_config.cap_mask);\r\ndma_cap_set(DMA_MEMSET_SG, at91sam9g45_config.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, at91sam9g45_config.cap_mask);\r\ndma_cap_set(DMA_SLAVE, at91sam9g45_config.cap_mask);\r\ndma_cap_set(DMA_SG, at91sam9g45_config.cap_mask);\r\nplat_dat = at_dma_get_driver_data(pdev);\r\nif (!plat_dat)\r\nreturn -ENODEV;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!io)\r\nreturn -EINVAL;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0)\r\nreturn irq;\r\nsize = sizeof(struct at_dma);\r\nsize += plat_dat->nr_channels * sizeof(struct at_dma_chan);\r\natdma = kzalloc(size, GFP_KERNEL);\r\nif (!atdma)\r\nreturn -ENOMEM;\r\natdma->dma_common.cap_mask = plat_dat->cap_mask;\r\natdma->all_chan_mask = (1 << plat_dat->nr_channels) - 1;\r\nsize = resource_size(io);\r\nif (!request_mem_region(io->start, size, pdev->dev.driver->name)) {\r\nerr = -EBUSY;\r\ngoto err_kfree;\r\n}\r\natdma->regs = ioremap(io->start, size);\r\nif (!atdma->regs) {\r\nerr = -ENOMEM;\r\ngoto err_release_r;\r\n}\r\natdma->clk = clk_get(&pdev->dev, "dma_clk");\r\nif (IS_ERR(atdma->clk)) {\r\nerr = PTR_ERR(atdma->clk);\r\ngoto err_clk;\r\n}\r\nerr = clk_prepare_enable(atdma->clk);\r\nif (err)\r\ngoto err_clk_prepare;\r\nat_dma_off(atdma);\r\nerr = request_irq(irq, at_dma_interrupt, 0, "at_hdmac", atdma);\r\nif (err)\r\ngoto err_irq;\r\nplatform_set_drvdata(pdev, atdma);\r\natdma->dma_desc_pool = dma_pool_create("at_hdmac_desc_pool",\r\n&pdev->dev, sizeof(struct at_desc),\r\n4 , 0);\r\nif (!atdma->dma_desc_pool) {\r\ndev_err(&pdev->dev, "No memory for descriptors dma pool\n");\r\nerr = -ENOMEM;\r\ngoto err_desc_pool_create;\r\n}\r\natdma->memset_pool = dma_pool_create("at_hdmac_memset_pool",\r\n&pdev->dev, sizeof(int), 4, 0);\r\nif (!atdma->memset_pool) {\r\ndev_err(&pdev->dev, "No memory for memset dma pool\n");\r\nerr = -ENOMEM;\r\ngoto err_memset_pool_create;\r\n}\r\nwhile (dma_readl(atdma, EBCISR))\r\ncpu_relax();\r\nINIT_LIST_HEAD(&atdma->dma_common.channels);\r\nfor (i = 0; i < plat_dat->nr_channels; i++) {\r\nstruct at_dma_chan *atchan = &atdma->chan[i];\r\natchan->mem_if = AT_DMA_MEM_IF;\r\natchan->per_if = AT_DMA_PER_IF;\r\natchan->chan_common.device = &atdma->dma_common;\r\ndma_cookie_init(&atchan->chan_common);\r\nlist_add_tail(&atchan->chan_common.device_node,\r\n&atdma->dma_common.channels);\r\natchan->ch_regs = atdma->regs + ch_regs(i);\r\nspin_lock_init(&atchan->lock);\r\natchan->mask = 1 << i;\r\nINIT_LIST_HEAD(&atchan->active_list);\r\nINIT_LIST_HEAD(&atchan->queue);\r\nINIT_LIST_HEAD(&atchan->free_list);\r\ntasklet_init(&atchan->tasklet, atc_tasklet,\r\n(unsigned long)atchan);\r\natc_enable_chan_irq(atdma, i);\r\n}\r\natdma->dma_common.device_alloc_chan_resources = atc_alloc_chan_resources;\r\natdma->dma_common.device_free_chan_resources = atc_free_chan_resources;\r\natdma->dma_common.device_tx_status = atc_tx_status;\r\natdma->dma_common.device_issue_pending = atc_issue_pending;\r\natdma->dma_common.dev = &pdev->dev;\r\nif (dma_has_cap(DMA_INTERLEAVE, atdma->dma_common.cap_mask))\r\natdma->dma_common.device_prep_interleaved_dma = atc_prep_dma_interleaved;\r\nif (dma_has_cap(DMA_MEMCPY, atdma->dma_common.cap_mask))\r\natdma->dma_common.device_prep_dma_memcpy = atc_prep_dma_memcpy;\r\nif (dma_has_cap(DMA_MEMSET, atdma->dma_common.cap_mask)) {\r\natdma->dma_common.device_prep_dma_memset = atc_prep_dma_memset;\r\natdma->dma_common.device_prep_dma_memset_sg = atc_prep_dma_memset_sg;\r\natdma->dma_common.fill_align = DMAENGINE_ALIGN_4_BYTES;\r\n}\r\nif (dma_has_cap(DMA_SLAVE, atdma->dma_common.cap_mask)) {\r\natdma->dma_common.device_prep_slave_sg = atc_prep_slave_sg;\r\ndma_cap_set(DMA_CYCLIC, atdma->dma_common.cap_mask);\r\natdma->dma_common.device_prep_dma_cyclic = atc_prep_dma_cyclic;\r\natdma->dma_common.device_config = atc_config;\r\natdma->dma_common.device_pause = atc_pause;\r\natdma->dma_common.device_resume = atc_resume;\r\natdma->dma_common.device_terminate_all = atc_terminate_all;\r\natdma->dma_common.src_addr_widths = ATC_DMA_BUSWIDTHS;\r\natdma->dma_common.dst_addr_widths = ATC_DMA_BUSWIDTHS;\r\natdma->dma_common.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\natdma->dma_common.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\n}\r\nif (dma_has_cap(DMA_SG, atdma->dma_common.cap_mask))\r\natdma->dma_common.device_prep_dma_sg = atc_prep_dma_sg;\r\ndma_writel(atdma, EN, AT_DMA_ENABLE);\r\ndev_info(&pdev->dev, "Atmel AHB DMA Controller ( %s%s%s%s), %d channels\n",\r\ndma_has_cap(DMA_MEMCPY, atdma->dma_common.cap_mask) ? "cpy " : "",\r\ndma_has_cap(DMA_MEMSET, atdma->dma_common.cap_mask) ? "set " : "",\r\ndma_has_cap(DMA_SLAVE, atdma->dma_common.cap_mask) ? "slave " : "",\r\ndma_has_cap(DMA_SG, atdma->dma_common.cap_mask) ? "sg-cpy " : "",\r\nplat_dat->nr_channels);\r\ndma_async_device_register(&atdma->dma_common);\r\nif (pdev->dev.of_node) {\r\nerr = of_dma_controller_register(pdev->dev.of_node,\r\nat_dma_xlate, atdma);\r\nif (err) {\r\ndev_err(&pdev->dev, "could not register of_dma_controller\n");\r\ngoto err_of_dma_controller_register;\r\n}\r\n}\r\nreturn 0;\r\nerr_of_dma_controller_register:\r\ndma_async_device_unregister(&atdma->dma_common);\r\ndma_pool_destroy(atdma->memset_pool);\r\nerr_memset_pool_create:\r\ndma_pool_destroy(atdma->dma_desc_pool);\r\nerr_desc_pool_create:\r\nfree_irq(platform_get_irq(pdev, 0), atdma);\r\nerr_irq:\r\nclk_disable_unprepare(atdma->clk);\r\nerr_clk_prepare:\r\nclk_put(atdma->clk);\r\nerr_clk:\r\niounmap(atdma->regs);\r\natdma->regs = NULL;\r\nerr_release_r:\r\nrelease_mem_region(io->start, size);\r\nerr_kfree:\r\nkfree(atdma);\r\nreturn err;\r\n}\r\nstatic int at_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nstruct resource *io;\r\nat_dma_off(atdma);\r\ndma_async_device_unregister(&atdma->dma_common);\r\ndma_pool_destroy(atdma->memset_pool);\r\ndma_pool_destroy(atdma->dma_desc_pool);\r\nfree_irq(platform_get_irq(pdev, 0), atdma);\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\natc_disable_chan_irq(atdma, chan->chan_id);\r\ntasklet_kill(&atchan->tasklet);\r\nlist_del(&chan->device_node);\r\n}\r\nclk_disable_unprepare(atdma->clk);\r\nclk_put(atdma->clk);\r\niounmap(atdma->regs);\r\natdma->regs = NULL;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nrelease_mem_region(io->start, resource_size(io));\r\nkfree(atdma);\r\nreturn 0;\r\n}\r\nstatic void at_dma_shutdown(struct platform_device *pdev)\r\n{\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nat_dma_off(platform_get_drvdata(pdev));\r\nclk_disable_unprepare(atdma->clk);\r\n}\r\nstatic int at_dma_prepare(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nif (atc_chan_is_enabled(atchan) && !atc_chan_is_cyclic(atchan))\r\nreturn -EAGAIN;\r\n}\r\nreturn 0;\r\n}\r\nstatic void atc_suspend_cyclic(struct at_dma_chan *atchan)\r\n{\r\nstruct dma_chan *chan = &atchan->chan_common;\r\nif (!atc_chan_is_paused(atchan)) {\r\ndev_warn(chan2dev(chan),\r\n"cyclic channel not paused, should be done by channel user\n");\r\natc_pause(chan);\r\n}\r\natchan->save_dscr = channel_readl(atchan, DSCR);\r\nvdbg_dump_regs(atchan);\r\n}\r\nstatic int at_dma_suspend_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nif (atc_chan_is_cyclic(atchan))\r\natc_suspend_cyclic(atchan);\r\natchan->save_cfg = channel_readl(atchan, CFG);\r\n}\r\natdma->save_imr = dma_readl(atdma, EBCIMR);\r\nat_dma_off(atdma);\r\nclk_disable_unprepare(atdma->clk);\r\nreturn 0;\r\n}\r\nstatic void atc_resume_cyclic(struct at_dma_chan *atchan)\r\n{\r\nstruct at_dma *atdma = to_at_dma(atchan->chan_common.device);\r\nchannel_writel(atchan, SADDR, 0);\r\nchannel_writel(atchan, DADDR, 0);\r\nchannel_writel(atchan, CTRLA, 0);\r\nchannel_writel(atchan, CTRLB, 0);\r\nchannel_writel(atchan, DSCR, atchan->save_dscr);\r\ndma_writel(atdma, CHER, atchan->mask);\r\nvdbg_dump_regs(atchan);\r\n}\r\nstatic int at_dma_resume_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nclk_prepare_enable(atdma->clk);\r\ndma_writel(atdma, EN, AT_DMA_ENABLE);\r\nwhile (dma_readl(atdma, EBCISR))\r\ncpu_relax();\r\ndma_writel(atdma, EBCIER, atdma->save_imr);\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nchannel_writel(atchan, CFG, atchan->save_cfg);\r\nif (atc_chan_is_cyclic(atchan))\r\natc_resume_cyclic(atchan);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init at_dma_init(void)\r\n{\r\nreturn platform_driver_probe(&at_dma_driver, at_dma_probe);\r\n}\r\nstatic void __exit at_dma_exit(void)\r\n{\r\nplatform_driver_unregister(&at_dma_driver);\r\n}
