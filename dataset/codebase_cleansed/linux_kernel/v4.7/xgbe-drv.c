static int xgbe_alloc_channels(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel_mem, *channel;\r\nstruct xgbe_ring *tx_ring, *rx_ring;\r\nunsigned int count, i;\r\nint ret = -ENOMEM;\r\ncount = max_t(unsigned int, pdata->tx_ring_count, pdata->rx_ring_count);\r\nchannel_mem = kcalloc(count, sizeof(struct xgbe_channel), GFP_KERNEL);\r\nif (!channel_mem)\r\ngoto err_channel;\r\ntx_ring = kcalloc(pdata->tx_ring_count, sizeof(struct xgbe_ring),\r\nGFP_KERNEL);\r\nif (!tx_ring)\r\ngoto err_tx_ring;\r\nrx_ring = kcalloc(pdata->rx_ring_count, sizeof(struct xgbe_ring),\r\nGFP_KERNEL);\r\nif (!rx_ring)\r\ngoto err_rx_ring;\r\nfor (i = 0, channel = channel_mem; i < count; i++, channel++) {\r\nsnprintf(channel->name, sizeof(channel->name), "channel-%d", i);\r\nchannel->pdata = pdata;\r\nchannel->queue_index = i;\r\nchannel->dma_regs = pdata->xgmac_regs + DMA_CH_BASE +\r\n(DMA_CH_INC * i);\r\nif (pdata->per_channel_irq) {\r\nret = platform_get_irq(pdata->pdev, i + 1);\r\nif (ret < 0) {\r\nnetdev_err(pdata->netdev,\r\n"platform_get_irq %u failed\n",\r\ni + 1);\r\ngoto err_irq;\r\n}\r\nchannel->dma_irq = ret;\r\n}\r\nif (i < pdata->tx_ring_count) {\r\nspin_lock_init(&tx_ring->lock);\r\nchannel->tx_ring = tx_ring++;\r\n}\r\nif (i < pdata->rx_ring_count) {\r\nspin_lock_init(&rx_ring->lock);\r\nchannel->rx_ring = rx_ring++;\r\n}\r\nnetif_dbg(pdata, drv, pdata->netdev,\r\n"%s: dma_regs=%p, dma_irq=%d, tx=%p, rx=%p\n",\r\nchannel->name, channel->dma_regs, channel->dma_irq,\r\nchannel->tx_ring, channel->rx_ring);\r\n}\r\npdata->channel = channel_mem;\r\npdata->channel_count = count;\r\nreturn 0;\r\nerr_irq:\r\nkfree(rx_ring);\r\nerr_rx_ring:\r\nkfree(tx_ring);\r\nerr_tx_ring:\r\nkfree(channel_mem);\r\nerr_channel:\r\nreturn ret;\r\n}\r\nstatic void xgbe_free_channels(struct xgbe_prv_data *pdata)\r\n{\r\nif (!pdata->channel)\r\nreturn;\r\nkfree(pdata->channel->rx_ring);\r\nkfree(pdata->channel->tx_ring);\r\nkfree(pdata->channel);\r\npdata->channel = NULL;\r\npdata->channel_count = 0;\r\n}\r\nstatic inline unsigned int xgbe_tx_avail_desc(struct xgbe_ring *ring)\r\n{\r\nreturn (ring->rdesc_count - (ring->cur - ring->dirty));\r\n}\r\nstatic inline unsigned int xgbe_rx_dirty_desc(struct xgbe_ring *ring)\r\n{\r\nreturn (ring->cur - ring->dirty);\r\n}\r\nstatic int xgbe_maybe_stop_tx_queue(struct xgbe_channel *channel,\r\nstruct xgbe_ring *ring, unsigned int count)\r\n{\r\nstruct xgbe_prv_data *pdata = channel->pdata;\r\nif (count > xgbe_tx_avail_desc(ring)) {\r\nnetif_info(pdata, drv, pdata->netdev,\r\n"Tx queue stopped, not enough descriptors available\n");\r\nnetif_stop_subqueue(pdata->netdev, channel->queue_index);\r\nring->tx.queue_stopped = 1;\r\nif (ring->tx.xmit_more)\r\npdata->hw_if.tx_start_xmit(channel, ring);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nreturn 0;\r\n}\r\nstatic int xgbe_calc_rx_buf_size(struct net_device *netdev, unsigned int mtu)\r\n{\r\nunsigned int rx_buf_size;\r\nif (mtu > XGMAC_JUMBO_PACKET_MTU) {\r\nnetdev_alert(netdev, "MTU exceeds maximum supported value\n");\r\nreturn -EINVAL;\r\n}\r\nrx_buf_size = mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;\r\nrx_buf_size = clamp_val(rx_buf_size, XGBE_RX_MIN_BUF_SIZE, PAGE_SIZE);\r\nrx_buf_size = (rx_buf_size + XGBE_RX_BUF_ALIGN - 1) &\r\n~(XGBE_RX_BUF_ALIGN - 1);\r\nreturn rx_buf_size;\r\n}\r\nstatic void xgbe_enable_rx_tx_ints(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_channel *channel;\r\nenum xgbe_int int_id;\r\nunsigned int i;\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nif (channel->tx_ring && channel->rx_ring)\r\nint_id = XGMAC_INT_DMA_CH_SR_TI_RI;\r\nelse if (channel->tx_ring)\r\nint_id = XGMAC_INT_DMA_CH_SR_TI;\r\nelse if (channel->rx_ring)\r\nint_id = XGMAC_INT_DMA_CH_SR_RI;\r\nelse\r\ncontinue;\r\nhw_if->enable_int(channel, int_id);\r\n}\r\n}\r\nstatic void xgbe_disable_rx_tx_ints(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_channel *channel;\r\nenum xgbe_int int_id;\r\nunsigned int i;\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nif (channel->tx_ring && channel->rx_ring)\r\nint_id = XGMAC_INT_DMA_CH_SR_TI_RI;\r\nelse if (channel->tx_ring)\r\nint_id = XGMAC_INT_DMA_CH_SR_TI;\r\nelse if (channel->rx_ring)\r\nint_id = XGMAC_INT_DMA_CH_SR_RI;\r\nelse\r\ncontinue;\r\nhw_if->disable_int(channel, int_id);\r\n}\r\n}\r\nstatic irqreturn_t xgbe_isr(int irq, void *data)\r\n{\r\nstruct xgbe_prv_data *pdata = data;\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_channel *channel;\r\nunsigned int dma_isr, dma_ch_isr;\r\nunsigned int mac_isr, mac_tssr;\r\nunsigned int i;\r\ndma_isr = XGMAC_IOREAD(pdata, DMA_ISR);\r\nif (!dma_isr)\r\ngoto isr_done;\r\nnetif_dbg(pdata, intr, pdata->netdev, "DMA_ISR=%#010x\n", dma_isr);\r\nfor (i = 0; i < pdata->channel_count; i++) {\r\nif (!(dma_isr & (1 << i)))\r\ncontinue;\r\nchannel = pdata->channel + i;\r\ndma_ch_isr = XGMAC_DMA_IOREAD(channel, DMA_CH_SR);\r\nnetif_dbg(pdata, intr, pdata->netdev, "DMA_CH%u_ISR=%#010x\n",\r\ni, dma_ch_isr);\r\nif (!pdata->per_channel_irq &&\r\n(XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, TI) ||\r\nXGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, RI))) {\r\nif (napi_schedule_prep(&pdata->napi)) {\r\nxgbe_disable_rx_tx_ints(pdata);\r\n__napi_schedule_irqoff(&pdata->napi);\r\n}\r\n}\r\nif (XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, RBU))\r\npdata->ext_stats.rx_buffer_unavailable++;\r\nif (XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, FBE))\r\nschedule_work(&pdata->restart_work);\r\nXGMAC_DMA_IOWRITE(channel, DMA_CH_SR, dma_ch_isr);\r\n}\r\nif (XGMAC_GET_BITS(dma_isr, DMA_ISR, MACIS)) {\r\nmac_isr = XGMAC_IOREAD(pdata, MAC_ISR);\r\nif (XGMAC_GET_BITS(mac_isr, MAC_ISR, MMCTXIS))\r\nhw_if->tx_mmc_int(pdata);\r\nif (XGMAC_GET_BITS(mac_isr, MAC_ISR, MMCRXIS))\r\nhw_if->rx_mmc_int(pdata);\r\nif (XGMAC_GET_BITS(mac_isr, MAC_ISR, TSIS)) {\r\nmac_tssr = XGMAC_IOREAD(pdata, MAC_TSSR);\r\nif (XGMAC_GET_BITS(mac_tssr, MAC_TSSR, TXTSC)) {\r\npdata->tx_tstamp =\r\nhw_if->get_tx_tstamp(pdata);\r\nqueue_work(pdata->dev_workqueue,\r\n&pdata->tx_tstamp_work);\r\n}\r\n}\r\n}\r\nisr_done:\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t xgbe_dma_isr(int irq, void *data)\r\n{\r\nstruct xgbe_channel *channel = data;\r\nif (napi_schedule_prep(&channel->napi)) {\r\ndisable_irq_nosync(channel->dma_irq);\r\n__napi_schedule_irqoff(&channel->napi);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void xgbe_tx_timer(unsigned long data)\r\n{\r\nstruct xgbe_channel *channel = (struct xgbe_channel *)data;\r\nstruct xgbe_prv_data *pdata = channel->pdata;\r\nstruct napi_struct *napi;\r\nDBGPR("-->xgbe_tx_timer\n");\r\nnapi = (pdata->per_channel_irq) ? &channel->napi : &pdata->napi;\r\nif (napi_schedule_prep(napi)) {\r\nif (pdata->per_channel_irq)\r\ndisable_irq_nosync(channel->dma_irq);\r\nelse\r\nxgbe_disable_rx_tx_ints(pdata);\r\n__napi_schedule(napi);\r\n}\r\nchannel->tx_timer_active = 0;\r\nDBGPR("<--xgbe_tx_timer\n");\r\n}\r\nstatic void xgbe_service(struct work_struct *work)\r\n{\r\nstruct xgbe_prv_data *pdata = container_of(work,\r\nstruct xgbe_prv_data,\r\nservice_work);\r\npdata->phy_if.phy_status(pdata);\r\n}\r\nstatic void xgbe_service_timer(unsigned long data)\r\n{\r\nstruct xgbe_prv_data *pdata = (struct xgbe_prv_data *)data;\r\nqueue_work(pdata->dev_workqueue, &pdata->service_work);\r\nmod_timer(&pdata->service_timer, jiffies + HZ);\r\n}\r\nstatic void xgbe_init_timers(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nsetup_timer(&pdata->service_timer, xgbe_service_timer,\r\n(unsigned long)pdata);\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nif (!channel->tx_ring)\r\nbreak;\r\nsetup_timer(&channel->tx_timer, xgbe_tx_timer,\r\n(unsigned long)channel);\r\n}\r\n}\r\nstatic void xgbe_start_timers(struct xgbe_prv_data *pdata)\r\n{\r\nmod_timer(&pdata->service_timer, jiffies + HZ);\r\n}\r\nstatic void xgbe_stop_timers(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\ndel_timer_sync(&pdata->service_timer);\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nif (!channel->tx_ring)\r\nbreak;\r\ndel_timer_sync(&channel->tx_timer);\r\n}\r\n}\r\nvoid xgbe_get_all_hw_features(struct xgbe_prv_data *pdata)\r\n{\r\nunsigned int mac_hfr0, mac_hfr1, mac_hfr2;\r\nstruct xgbe_hw_features *hw_feat = &pdata->hw_feat;\r\nDBGPR("-->xgbe_get_all_hw_features\n");\r\nmac_hfr0 = XGMAC_IOREAD(pdata, MAC_HWF0R);\r\nmac_hfr1 = XGMAC_IOREAD(pdata, MAC_HWF1R);\r\nmac_hfr2 = XGMAC_IOREAD(pdata, MAC_HWF2R);\r\nmemset(hw_feat, 0, sizeof(*hw_feat));\r\nhw_feat->version = XGMAC_IOREAD(pdata, MAC_VR);\r\nhw_feat->gmii = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, GMIISEL);\r\nhw_feat->vlhash = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, VLHASH);\r\nhw_feat->sma = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, SMASEL);\r\nhw_feat->rwk = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, RWKSEL);\r\nhw_feat->mgk = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, MGKSEL);\r\nhw_feat->mmc = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, MMCSEL);\r\nhw_feat->aoe = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, ARPOFFSEL);\r\nhw_feat->ts = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, TSSEL);\r\nhw_feat->eee = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, EEESEL);\r\nhw_feat->tx_coe = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, TXCOESEL);\r\nhw_feat->rx_coe = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, RXCOESEL);\r\nhw_feat->addn_mac = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R,\r\nADDMACADRSEL);\r\nhw_feat->ts_src = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, TSSTSSEL);\r\nhw_feat->sa_vlan_ins = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, SAVLANINS);\r\nhw_feat->rx_fifo_size = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\r\nRXFIFOSIZE);\r\nhw_feat->tx_fifo_size = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\r\nTXFIFOSIZE);\r\nhw_feat->adv_ts_hi = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, ADVTHWORD);\r\nhw_feat->dma_width = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, ADDR64);\r\nhw_feat->dcb = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, DCBEN);\r\nhw_feat->sph = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, SPHEN);\r\nhw_feat->tso = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, TSOEN);\r\nhw_feat->dma_debug = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, DBGMEMA);\r\nhw_feat->rss = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, RSSEN);\r\nhw_feat->tc_cnt = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, NUMTC);\r\nhw_feat->hash_table_size = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\r\nHASHTBLSZ);\r\nhw_feat->l3l4_filter_num = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\r\nL3L4FNUM);\r\nhw_feat->rx_q_cnt = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, RXQCNT);\r\nhw_feat->tx_q_cnt = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, TXQCNT);\r\nhw_feat->rx_ch_cnt = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, RXCHCNT);\r\nhw_feat->tx_ch_cnt = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, TXCHCNT);\r\nhw_feat->pps_out_num = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, PPSOUTNUM);\r\nhw_feat->aux_snap_num = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, AUXSNAPNUM);\r\nswitch (hw_feat->hash_table_size) {\r\ncase 0:\r\nbreak;\r\ncase 1:\r\nhw_feat->hash_table_size = 64;\r\nbreak;\r\ncase 2:\r\nhw_feat->hash_table_size = 128;\r\nbreak;\r\ncase 3:\r\nhw_feat->hash_table_size = 256;\r\nbreak;\r\n}\r\nswitch (hw_feat->dma_width) {\r\ncase 0:\r\nhw_feat->dma_width = 32;\r\nbreak;\r\ncase 1:\r\nhw_feat->dma_width = 40;\r\nbreak;\r\ncase 2:\r\nhw_feat->dma_width = 48;\r\nbreak;\r\ndefault:\r\nhw_feat->dma_width = 32;\r\n}\r\nhw_feat->rx_q_cnt++;\r\nhw_feat->tx_q_cnt++;\r\nhw_feat->rx_ch_cnt++;\r\nhw_feat->tx_ch_cnt++;\r\nhw_feat->tc_cnt++;\r\nDBGPR("<--xgbe_get_all_hw_features\n");\r\n}\r\nstatic void xgbe_napi_enable(struct xgbe_prv_data *pdata, unsigned int add)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nif (pdata->per_channel_irq) {\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nif (add)\r\nnetif_napi_add(pdata->netdev, &channel->napi,\r\nxgbe_one_poll, NAPI_POLL_WEIGHT);\r\nnapi_enable(&channel->napi);\r\n}\r\n} else {\r\nif (add)\r\nnetif_napi_add(pdata->netdev, &pdata->napi,\r\nxgbe_all_poll, NAPI_POLL_WEIGHT);\r\nnapi_enable(&pdata->napi);\r\n}\r\n}\r\nstatic void xgbe_napi_disable(struct xgbe_prv_data *pdata, unsigned int del)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nif (pdata->per_channel_irq) {\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nnapi_disable(&channel->napi);\r\nif (del)\r\nnetif_napi_del(&channel->napi);\r\n}\r\n} else {\r\nnapi_disable(&pdata->napi);\r\nif (del)\r\nnetif_napi_del(&pdata->napi);\r\n}\r\n}\r\nstatic int xgbe_request_irqs(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nstruct net_device *netdev = pdata->netdev;\r\nunsigned int i;\r\nint ret;\r\nret = devm_request_irq(pdata->dev, pdata->dev_irq, xgbe_isr, 0,\r\nnetdev->name, pdata);\r\nif (ret) {\r\nnetdev_alert(netdev, "error requesting irq %d\n",\r\npdata->dev_irq);\r\nreturn ret;\r\n}\r\nif (!pdata->per_channel_irq)\r\nreturn 0;\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nsnprintf(channel->dma_irq_name,\r\nsizeof(channel->dma_irq_name) - 1,\r\n"%s-TxRx-%u", netdev_name(netdev),\r\nchannel->queue_index);\r\nret = devm_request_irq(pdata->dev, channel->dma_irq,\r\nxgbe_dma_isr, 0,\r\nchannel->dma_irq_name, channel);\r\nif (ret) {\r\nnetdev_alert(netdev, "error requesting irq %d\n",\r\nchannel->dma_irq);\r\ngoto err_irq;\r\n}\r\n}\r\nreturn 0;\r\nerr_irq:\r\nfor (i--, channel--; i < pdata->channel_count; i--, channel--)\r\ndevm_free_irq(pdata->dev, channel->dma_irq, channel);\r\ndevm_free_irq(pdata->dev, pdata->dev_irq, pdata);\r\nreturn ret;\r\n}\r\nstatic void xgbe_free_irqs(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\ndevm_free_irq(pdata->dev, pdata->dev_irq, pdata);\r\nif (!pdata->per_channel_irq)\r\nreturn;\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++)\r\ndevm_free_irq(pdata->dev, channel->dma_irq, channel);\r\n}\r\nvoid xgbe_init_tx_coalesce(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nDBGPR("-->xgbe_init_tx_coalesce\n");\r\npdata->tx_usecs = XGMAC_INIT_DMA_TX_USECS;\r\npdata->tx_frames = XGMAC_INIT_DMA_TX_FRAMES;\r\nhw_if->config_tx_coalesce(pdata);\r\nDBGPR("<--xgbe_init_tx_coalesce\n");\r\n}\r\nvoid xgbe_init_rx_coalesce(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nDBGPR("-->xgbe_init_rx_coalesce\n");\r\npdata->rx_riwt = hw_if->usec_to_riwt(pdata, XGMAC_INIT_DMA_RX_USECS);\r\npdata->rx_usecs = XGMAC_INIT_DMA_RX_USECS;\r\npdata->rx_frames = XGMAC_INIT_DMA_RX_FRAMES;\r\nhw_if->config_rx_coalesce(pdata);\r\nDBGPR("<--xgbe_init_rx_coalesce\n");\r\n}\r\nstatic void xgbe_free_tx_data(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_desc_if *desc_if = &pdata->desc_if;\r\nstruct xgbe_channel *channel;\r\nstruct xgbe_ring *ring;\r\nstruct xgbe_ring_data *rdata;\r\nunsigned int i, j;\r\nDBGPR("-->xgbe_free_tx_data\n");\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nring = channel->tx_ring;\r\nif (!ring)\r\nbreak;\r\nfor (j = 0; j < ring->rdesc_count; j++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, j);\r\ndesc_if->unmap_rdata(pdata, rdata);\r\n}\r\n}\r\nDBGPR("<--xgbe_free_tx_data\n");\r\n}\r\nstatic void xgbe_free_rx_data(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_desc_if *desc_if = &pdata->desc_if;\r\nstruct xgbe_channel *channel;\r\nstruct xgbe_ring *ring;\r\nstruct xgbe_ring_data *rdata;\r\nunsigned int i, j;\r\nDBGPR("-->xgbe_free_rx_data\n");\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nring = channel->rx_ring;\r\nif (!ring)\r\nbreak;\r\nfor (j = 0; j < ring->rdesc_count; j++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, j);\r\ndesc_if->unmap_rdata(pdata, rdata);\r\n}\r\n}\r\nDBGPR("<--xgbe_free_rx_data\n");\r\n}\r\nstatic int xgbe_phy_init(struct xgbe_prv_data *pdata)\r\n{\r\npdata->phy_link = -1;\r\npdata->phy_speed = SPEED_UNKNOWN;\r\nreturn pdata->phy_if.phy_reset(pdata);\r\n}\r\nint xgbe_powerdown(struct net_device *netdev, unsigned int caller)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nunsigned long flags;\r\nDBGPR("-->xgbe_powerdown\n");\r\nif (!netif_running(netdev) ||\r\n(caller == XGMAC_IOCTL_CONTEXT && pdata->power_down)) {\r\nnetdev_alert(netdev, "Device is already powered down\n");\r\nDBGPR("<--xgbe_powerdown\n");\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&pdata->lock, flags);\r\nif (caller == XGMAC_DRIVER_CONTEXT)\r\nnetif_device_detach(netdev);\r\nnetif_tx_stop_all_queues(netdev);\r\nxgbe_stop_timers(pdata);\r\nflush_workqueue(pdata->dev_workqueue);\r\nhw_if->powerdown_tx(pdata);\r\nhw_if->powerdown_rx(pdata);\r\nxgbe_napi_disable(pdata, 0);\r\npdata->power_down = 1;\r\nspin_unlock_irqrestore(&pdata->lock, flags);\r\nDBGPR("<--xgbe_powerdown\n");\r\nreturn 0;\r\n}\r\nint xgbe_powerup(struct net_device *netdev, unsigned int caller)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nunsigned long flags;\r\nDBGPR("-->xgbe_powerup\n");\r\nif (!netif_running(netdev) ||\r\n(caller == XGMAC_IOCTL_CONTEXT && !pdata->power_down)) {\r\nnetdev_alert(netdev, "Device is already powered up\n");\r\nDBGPR("<--xgbe_powerup\n");\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&pdata->lock, flags);\r\npdata->power_down = 0;\r\nxgbe_napi_enable(pdata, 0);\r\nhw_if->powerup_tx(pdata);\r\nhw_if->powerup_rx(pdata);\r\nif (caller == XGMAC_DRIVER_CONTEXT)\r\nnetif_device_attach(netdev);\r\nnetif_tx_start_all_queues(netdev);\r\nxgbe_start_timers(pdata);\r\nspin_unlock_irqrestore(&pdata->lock, flags);\r\nDBGPR("<--xgbe_powerup\n");\r\nreturn 0;\r\n}\r\nstatic int xgbe_start(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_phy_if *phy_if = &pdata->phy_if;\r\nstruct net_device *netdev = pdata->netdev;\r\nint ret;\r\nDBGPR("-->xgbe_start\n");\r\nhw_if->init(pdata);\r\nret = phy_if->phy_start(pdata);\r\nif (ret)\r\ngoto err_phy;\r\nxgbe_napi_enable(pdata, 1);\r\nret = xgbe_request_irqs(pdata);\r\nif (ret)\r\ngoto err_napi;\r\nhw_if->enable_tx(pdata);\r\nhw_if->enable_rx(pdata);\r\nnetif_tx_start_all_queues(netdev);\r\nxgbe_start_timers(pdata);\r\nqueue_work(pdata->dev_workqueue, &pdata->service_work);\r\nDBGPR("<--xgbe_start\n");\r\nreturn 0;\r\nerr_napi:\r\nxgbe_napi_disable(pdata, 1);\r\nphy_if->phy_stop(pdata);\r\nerr_phy:\r\nhw_if->exit(pdata);\r\nreturn ret;\r\n}\r\nstatic void xgbe_stop(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_phy_if *phy_if = &pdata->phy_if;\r\nstruct xgbe_channel *channel;\r\nstruct net_device *netdev = pdata->netdev;\r\nstruct netdev_queue *txq;\r\nunsigned int i;\r\nDBGPR("-->xgbe_stop\n");\r\nnetif_tx_stop_all_queues(netdev);\r\nxgbe_stop_timers(pdata);\r\nflush_workqueue(pdata->dev_workqueue);\r\nhw_if->disable_tx(pdata);\r\nhw_if->disable_rx(pdata);\r\nxgbe_free_irqs(pdata);\r\nxgbe_napi_disable(pdata, 1);\r\nphy_if->phy_stop(pdata);\r\nhw_if->exit(pdata);\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nif (!channel->tx_ring)\r\ncontinue;\r\ntxq = netdev_get_tx_queue(netdev, channel->queue_index);\r\nnetdev_tx_reset_queue(txq);\r\n}\r\nDBGPR("<--xgbe_stop\n");\r\n}\r\nstatic void xgbe_restart_dev(struct xgbe_prv_data *pdata)\r\n{\r\nDBGPR("-->xgbe_restart_dev\n");\r\nif (!netif_running(pdata->netdev))\r\nreturn;\r\nxgbe_stop(pdata);\r\nxgbe_free_tx_data(pdata);\r\nxgbe_free_rx_data(pdata);\r\nxgbe_start(pdata);\r\nDBGPR("<--xgbe_restart_dev\n");\r\n}\r\nstatic void xgbe_restart(struct work_struct *work)\r\n{\r\nstruct xgbe_prv_data *pdata = container_of(work,\r\nstruct xgbe_prv_data,\r\nrestart_work);\r\nrtnl_lock();\r\nxgbe_restart_dev(pdata);\r\nrtnl_unlock();\r\n}\r\nstatic void xgbe_tx_tstamp(struct work_struct *work)\r\n{\r\nstruct xgbe_prv_data *pdata = container_of(work,\r\nstruct xgbe_prv_data,\r\ntx_tstamp_work);\r\nstruct skb_shared_hwtstamps hwtstamps;\r\nu64 nsec;\r\nunsigned long flags;\r\nif (pdata->tx_tstamp) {\r\nnsec = timecounter_cyc2time(&pdata->tstamp_tc,\r\npdata->tx_tstamp);\r\nmemset(&hwtstamps, 0, sizeof(hwtstamps));\r\nhwtstamps.hwtstamp = ns_to_ktime(nsec);\r\nskb_tstamp_tx(pdata->tx_tstamp_skb, &hwtstamps);\r\n}\r\ndev_kfree_skb_any(pdata->tx_tstamp_skb);\r\nspin_lock_irqsave(&pdata->tstamp_lock, flags);\r\npdata->tx_tstamp_skb = NULL;\r\nspin_unlock_irqrestore(&pdata->tstamp_lock, flags);\r\n}\r\nstatic int xgbe_get_hwtstamp_settings(struct xgbe_prv_data *pdata,\r\nstruct ifreq *ifreq)\r\n{\r\nif (copy_to_user(ifreq->ifr_data, &pdata->tstamp_config,\r\nsizeof(pdata->tstamp_config)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int xgbe_set_hwtstamp_settings(struct xgbe_prv_data *pdata,\r\nstruct ifreq *ifreq)\r\n{\r\nstruct hwtstamp_config config;\r\nunsigned int mac_tscr;\r\nif (copy_from_user(&config, ifreq->ifr_data, sizeof(config)))\r\nreturn -EFAULT;\r\nif (config.flags)\r\nreturn -EINVAL;\r\nmac_tscr = 0;\r\nswitch (config.tx_type) {\r\ncase HWTSTAMP_TX_OFF:\r\nbreak;\r\ncase HWTSTAMP_TX_ON:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nswitch (config.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\nbreak;\r\ncase HWTSTAMP_FILTER_ALL:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENALL, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, SNAPTYPSEL, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSMSTRENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, AV8021ASMEN, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, SNAPTYPSEL, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, AV8021ASMEN, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, AV8021ASMEN, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSMSTRENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_EVENT:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, SNAPTYPSEL, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_SYNC:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSMSTRENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\r\nXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\npdata->hw_if.config_tstamp(pdata, mac_tscr);\r\nmemcpy(&pdata->tstamp_config, &config, sizeof(config));\r\nreturn 0;\r\n}\r\nstatic void xgbe_prep_tx_tstamp(struct xgbe_prv_data *pdata,\r\nstruct sk_buff *skb,\r\nstruct xgbe_packet_data *packet)\r\n{\r\nunsigned long flags;\r\nif (XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES, PTP)) {\r\nspin_lock_irqsave(&pdata->tstamp_lock, flags);\r\nif (pdata->tx_tstamp_skb) {\r\nXGMAC_SET_BITS(packet->attributes,\r\nTX_PACKET_ATTRIBUTES, PTP, 0);\r\n} else {\r\npdata->tx_tstamp_skb = skb_get(skb);\r\nskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\r\n}\r\nspin_unlock_irqrestore(&pdata->tstamp_lock, flags);\r\n}\r\nif (!XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES, PTP))\r\nskb_tx_timestamp(skb);\r\n}\r\nstatic void xgbe_prep_vlan(struct sk_buff *skb, struct xgbe_packet_data *packet)\r\n{\r\nif (skb_vlan_tag_present(skb))\r\npacket->vlan_ctag = skb_vlan_tag_get(skb);\r\n}\r\nstatic int xgbe_prep_tso(struct sk_buff *skb, struct xgbe_packet_data *packet)\r\n{\r\nint ret;\r\nif (!XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nTSO_ENABLE))\r\nreturn 0;\r\nret = skb_cow_head(skb, 0);\r\nif (ret)\r\nreturn ret;\r\npacket->header_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\npacket->tcp_header_len = tcp_hdrlen(skb);\r\npacket->tcp_payload_len = skb->len - packet->header_len;\r\npacket->mss = skb_shinfo(skb)->gso_size;\r\nDBGPR(" packet->header_len=%u\n", packet->header_len);\r\nDBGPR(" packet->tcp_header_len=%u, packet->tcp_payload_len=%u\n",\r\npacket->tcp_header_len, packet->tcp_payload_len);\r\nDBGPR(" packet->mss=%u\n", packet->mss);\r\npacket->tx_packets = skb_shinfo(skb)->gso_segs;\r\npacket->tx_bytes += (packet->tx_packets - 1) * packet->header_len;\r\nreturn 0;\r\n}\r\nstatic int xgbe_is_tso(struct sk_buff *skb)\r\n{\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nreturn 0;\r\nif (!skb_is_gso(skb))\r\nreturn 0;\r\nDBGPR(" TSO packet to be processed\n");\r\nreturn 1;\r\n}\r\nstatic void xgbe_packet_info(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring *ring, struct sk_buff *skb,\r\nstruct xgbe_packet_data *packet)\r\n{\r\nstruct skb_frag_struct *frag;\r\nunsigned int context_desc;\r\nunsigned int len;\r\nunsigned int i;\r\npacket->skb = skb;\r\ncontext_desc = 0;\r\npacket->rdesc_count = 0;\r\npacket->tx_packets = 1;\r\npacket->tx_bytes = skb->len;\r\nif (xgbe_is_tso(skb)) {\r\nif (skb_shinfo(skb)->gso_size != ring->tx.cur_mss) {\r\ncontext_desc = 1;\r\npacket->rdesc_count++;\r\n}\r\npacket->rdesc_count++;\r\nXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nTSO_ENABLE, 1);\r\nXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nCSUM_ENABLE, 1);\r\n} else if (skb->ip_summed == CHECKSUM_PARTIAL)\r\nXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nCSUM_ENABLE, 1);\r\nif (skb_vlan_tag_present(skb)) {\r\nif (skb_vlan_tag_get(skb) != ring->tx.cur_vlan_ctag)\r\nif (!context_desc) {\r\ncontext_desc = 1;\r\npacket->rdesc_count++;\r\n}\r\nXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nVLAN_CTAG, 1);\r\n}\r\nif ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\r\n(pdata->tstamp_config.tx_type == HWTSTAMP_TX_ON))\r\nXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nPTP, 1);\r\nfor (len = skb_headlen(skb); len;) {\r\npacket->rdesc_count++;\r\nlen -= min_t(unsigned int, len, XGBE_TX_MAX_BUF_SIZE);\r\n}\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nfrag = &skb_shinfo(skb)->frags[i];\r\nfor (len = skb_frag_size(frag); len; ) {\r\npacket->rdesc_count++;\r\nlen -= min_t(unsigned int, len, XGBE_TX_MAX_BUF_SIZE);\r\n}\r\n}\r\n}\r\nstatic int xgbe_open(struct net_device *netdev)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_desc_if *desc_if = &pdata->desc_if;\r\nint ret;\r\nDBGPR("-->xgbe_open\n");\r\nret = xgbe_phy_init(pdata);\r\nif (ret)\r\nreturn ret;\r\nret = clk_prepare_enable(pdata->sysclk);\r\nif (ret) {\r\nnetdev_alert(netdev, "dma clk_prepare_enable failed\n");\r\nreturn ret;\r\n}\r\nret = clk_prepare_enable(pdata->ptpclk);\r\nif (ret) {\r\nnetdev_alert(netdev, "ptp clk_prepare_enable failed\n");\r\ngoto err_sysclk;\r\n}\r\nret = xgbe_calc_rx_buf_size(netdev, netdev->mtu);\r\nif (ret < 0)\r\ngoto err_ptpclk;\r\npdata->rx_buf_size = ret;\r\nret = xgbe_alloc_channels(pdata);\r\nif (ret)\r\ngoto err_ptpclk;\r\nret = desc_if->alloc_ring_resources(pdata);\r\nif (ret)\r\ngoto err_channels;\r\nINIT_WORK(&pdata->service_work, xgbe_service);\r\nINIT_WORK(&pdata->restart_work, xgbe_restart);\r\nINIT_WORK(&pdata->tx_tstamp_work, xgbe_tx_tstamp);\r\nxgbe_init_timers(pdata);\r\nret = xgbe_start(pdata);\r\nif (ret)\r\ngoto err_rings;\r\nclear_bit(XGBE_DOWN, &pdata->dev_state);\r\nDBGPR("<--xgbe_open\n");\r\nreturn 0;\r\nerr_rings:\r\ndesc_if->free_ring_resources(pdata);\r\nerr_channels:\r\nxgbe_free_channels(pdata);\r\nerr_ptpclk:\r\nclk_disable_unprepare(pdata->ptpclk);\r\nerr_sysclk:\r\nclk_disable_unprepare(pdata->sysclk);\r\nreturn ret;\r\n}\r\nstatic int xgbe_close(struct net_device *netdev)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_desc_if *desc_if = &pdata->desc_if;\r\nDBGPR("-->xgbe_close\n");\r\nxgbe_stop(pdata);\r\ndesc_if->free_ring_resources(pdata);\r\nxgbe_free_channels(pdata);\r\nclk_disable_unprepare(pdata->ptpclk);\r\nclk_disable_unprepare(pdata->sysclk);\r\nset_bit(XGBE_DOWN, &pdata->dev_state);\r\nDBGPR("<--xgbe_close\n");\r\nreturn 0;\r\n}\r\nstatic int xgbe_xmit(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_desc_if *desc_if = &pdata->desc_if;\r\nstruct xgbe_channel *channel;\r\nstruct xgbe_ring *ring;\r\nstruct xgbe_packet_data *packet;\r\nstruct netdev_queue *txq;\r\nint ret;\r\nDBGPR("-->xgbe_xmit: skb->len = %d\n", skb->len);\r\nchannel = pdata->channel + skb->queue_mapping;\r\ntxq = netdev_get_tx_queue(netdev, channel->queue_index);\r\nring = channel->tx_ring;\r\npacket = &ring->packet_data;\r\nret = NETDEV_TX_OK;\r\nif (skb->len == 0) {\r\nnetif_err(pdata, tx_err, netdev,\r\n"empty skb received from stack\n");\r\ndev_kfree_skb_any(skb);\r\ngoto tx_netdev_return;\r\n}\r\nmemset(packet, 0, sizeof(*packet));\r\nxgbe_packet_info(pdata, ring, skb, packet);\r\nret = xgbe_maybe_stop_tx_queue(channel, ring, packet->rdesc_count);\r\nif (ret)\r\ngoto tx_netdev_return;\r\nret = xgbe_prep_tso(skb, packet);\r\nif (ret) {\r\nnetif_err(pdata, tx_err, netdev,\r\n"error processing TSO packet\n");\r\ndev_kfree_skb_any(skb);\r\ngoto tx_netdev_return;\r\n}\r\nxgbe_prep_vlan(skb, packet);\r\nif (!desc_if->map_tx_skb(channel, skb)) {\r\ndev_kfree_skb_any(skb);\r\ngoto tx_netdev_return;\r\n}\r\nxgbe_prep_tx_tstamp(pdata, skb, packet);\r\nnetdev_tx_sent_queue(txq, packet->tx_bytes);\r\nhw_if->dev_xmit(channel);\r\nif (netif_msg_pktdata(pdata))\r\nxgbe_print_pkt(netdev, skb, true);\r\nxgbe_maybe_stop_tx_queue(channel, ring, XGBE_TX_MAX_DESCS);\r\nret = NETDEV_TX_OK;\r\ntx_netdev_return:\r\nreturn ret;\r\n}\r\nstatic void xgbe_set_rx_mode(struct net_device *netdev)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nDBGPR("-->xgbe_set_rx_mode\n");\r\nhw_if->config_rx_mode(pdata);\r\nDBGPR("<--xgbe_set_rx_mode\n");\r\n}\r\nstatic int xgbe_set_mac_address(struct net_device *netdev, void *addr)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct sockaddr *saddr = addr;\r\nDBGPR("-->xgbe_set_mac_address\n");\r\nif (!is_valid_ether_addr(saddr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(netdev->dev_addr, saddr->sa_data, netdev->addr_len);\r\nhw_if->set_mac_address(pdata, netdev->dev_addr);\r\nDBGPR("<--xgbe_set_mac_address\n");\r\nreturn 0;\r\n}\r\nstatic int xgbe_ioctl(struct net_device *netdev, struct ifreq *ifreq, int cmd)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nint ret;\r\nswitch (cmd) {\r\ncase SIOCGHWTSTAMP:\r\nret = xgbe_get_hwtstamp_settings(pdata, ifreq);\r\nbreak;\r\ncase SIOCSHWTSTAMP:\r\nret = xgbe_set_hwtstamp_settings(pdata, ifreq);\r\nbreak;\r\ndefault:\r\nret = -EOPNOTSUPP;\r\n}\r\nreturn ret;\r\n}\r\nstatic int xgbe_change_mtu(struct net_device *netdev, int mtu)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nint ret;\r\nDBGPR("-->xgbe_change_mtu\n");\r\nret = xgbe_calc_rx_buf_size(netdev, mtu);\r\nif (ret < 0)\r\nreturn ret;\r\npdata->rx_buf_size = ret;\r\nnetdev->mtu = mtu;\r\nxgbe_restart_dev(pdata);\r\nDBGPR("<--xgbe_change_mtu\n");\r\nreturn 0;\r\n}\r\nstatic void xgbe_tx_timeout(struct net_device *netdev)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nnetdev_warn(netdev, "tx timeout, device restarting\n");\r\nschedule_work(&pdata->restart_work);\r\n}\r\nstatic struct rtnl_link_stats64 *xgbe_get_stats64(struct net_device *netdev,\r\nstruct rtnl_link_stats64 *s)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_mmc_stats *pstats = &pdata->mmc_stats;\r\nDBGPR("-->%s\n", __func__);\r\npdata->hw_if.read_mmc_stats(pdata);\r\ns->rx_packets = pstats->rxframecount_gb;\r\ns->rx_bytes = pstats->rxoctetcount_gb;\r\ns->rx_errors = pstats->rxframecount_gb -\r\npstats->rxbroadcastframes_g -\r\npstats->rxmulticastframes_g -\r\npstats->rxunicastframes_g;\r\ns->multicast = pstats->rxmulticastframes_g;\r\ns->rx_length_errors = pstats->rxlengtherror;\r\ns->rx_crc_errors = pstats->rxcrcerror;\r\ns->rx_fifo_errors = pstats->rxfifooverflow;\r\ns->tx_packets = pstats->txframecount_gb;\r\ns->tx_bytes = pstats->txoctetcount_gb;\r\ns->tx_errors = pstats->txframecount_gb - pstats->txframecount_g;\r\ns->tx_dropped = netdev->stats.tx_dropped;\r\nDBGPR("<--%s\n", __func__);\r\nreturn s;\r\n}\r\nstatic int xgbe_vlan_rx_add_vid(struct net_device *netdev, __be16 proto,\r\nu16 vid)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nDBGPR("-->%s\n", __func__);\r\nset_bit(vid, pdata->active_vlans);\r\nhw_if->update_vlan_hash_table(pdata);\r\nDBGPR("<--%s\n", __func__);\r\nreturn 0;\r\n}\r\nstatic int xgbe_vlan_rx_kill_vid(struct net_device *netdev, __be16 proto,\r\nu16 vid)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nDBGPR("-->%s\n", __func__);\r\nclear_bit(vid, pdata->active_vlans);\r\nhw_if->update_vlan_hash_table(pdata);\r\nDBGPR("<--%s\n", __func__);\r\nreturn 0;\r\n}\r\nstatic void xgbe_poll_controller(struct net_device *netdev)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nDBGPR("-->xgbe_poll_controller\n");\r\nif (pdata->per_channel_irq) {\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++)\r\nxgbe_dma_isr(channel->dma_irq, channel);\r\n} else {\r\ndisable_irq(pdata->dev_irq);\r\nxgbe_isr(pdata->dev_irq, pdata);\r\nenable_irq(pdata->dev_irq);\r\n}\r\nDBGPR("<--xgbe_poll_controller\n");\r\n}\r\nstatic int xgbe_setup_tc(struct net_device *netdev, u32 handle, __be16 proto,\r\nstruct tc_to_netdev *tc_to_netdev)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nu8 tc;\r\nif (tc_to_netdev->type != TC_SETUP_MQPRIO)\r\nreturn -EINVAL;\r\ntc = tc_to_netdev->tc;\r\nif (tc > pdata->hw_feat.tc_cnt)\r\nreturn -EINVAL;\r\npdata->num_tcs = tc;\r\npdata->hw_if.config_tc(pdata);\r\nreturn 0;\r\n}\r\nstatic int xgbe_set_features(struct net_device *netdev,\r\nnetdev_features_t features)\r\n{\r\nstruct xgbe_prv_data *pdata = netdev_priv(netdev);\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nnetdev_features_t rxhash, rxcsum, rxvlan, rxvlan_filter;\r\nint ret = 0;\r\nrxhash = pdata->netdev_features & NETIF_F_RXHASH;\r\nrxcsum = pdata->netdev_features & NETIF_F_RXCSUM;\r\nrxvlan = pdata->netdev_features & NETIF_F_HW_VLAN_CTAG_RX;\r\nrxvlan_filter = pdata->netdev_features & NETIF_F_HW_VLAN_CTAG_FILTER;\r\nif ((features & NETIF_F_RXHASH) && !rxhash)\r\nret = hw_if->enable_rss(pdata);\r\nelse if (!(features & NETIF_F_RXHASH) && rxhash)\r\nret = hw_if->disable_rss(pdata);\r\nif (ret)\r\nreturn ret;\r\nif ((features & NETIF_F_RXCSUM) && !rxcsum)\r\nhw_if->enable_rx_csum(pdata);\r\nelse if (!(features & NETIF_F_RXCSUM) && rxcsum)\r\nhw_if->disable_rx_csum(pdata);\r\nif ((features & NETIF_F_HW_VLAN_CTAG_RX) && !rxvlan)\r\nhw_if->enable_rx_vlan_stripping(pdata);\r\nelse if (!(features & NETIF_F_HW_VLAN_CTAG_RX) && rxvlan)\r\nhw_if->disable_rx_vlan_stripping(pdata);\r\nif ((features & NETIF_F_HW_VLAN_CTAG_FILTER) && !rxvlan_filter)\r\nhw_if->enable_rx_vlan_filtering(pdata);\r\nelse if (!(features & NETIF_F_HW_VLAN_CTAG_FILTER) && rxvlan_filter)\r\nhw_if->disable_rx_vlan_filtering(pdata);\r\npdata->netdev_features = features;\r\nDBGPR("<--xgbe_set_features\n");\r\nreturn 0;\r\n}\r\nstruct net_device_ops *xgbe_get_netdev_ops(void)\r\n{\r\nreturn (struct net_device_ops *)&xgbe_netdev_ops;\r\n}\r\nstatic void xgbe_rx_refresh(struct xgbe_channel *channel)\r\n{\r\nstruct xgbe_prv_data *pdata = channel->pdata;\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_desc_if *desc_if = &pdata->desc_if;\r\nstruct xgbe_ring *ring = channel->rx_ring;\r\nstruct xgbe_ring_data *rdata;\r\nwhile (ring->dirty != ring->cur) {\r\nrdata = XGBE_GET_DESC_DATA(ring, ring->dirty);\r\ndesc_if->unmap_rdata(pdata, rdata);\r\nif (desc_if->map_rx_buffer(pdata, ring, rdata))\r\nbreak;\r\nhw_if->rx_desc_reset(pdata, rdata, ring->dirty);\r\nring->dirty++;\r\n}\r\nwmb();\r\nrdata = XGBE_GET_DESC_DATA(ring, ring->dirty - 1);\r\nXGMAC_DMA_IOWRITE(channel, DMA_CH_RDTR_LO,\r\nlower_32_bits(rdata->rdesc_dma));\r\n}\r\nstatic struct sk_buff *xgbe_create_skb(struct xgbe_prv_data *pdata,\r\nstruct napi_struct *napi,\r\nstruct xgbe_ring_data *rdata,\r\nunsigned int len)\r\n{\r\nstruct sk_buff *skb;\r\nu8 *packet;\r\nunsigned int copy_len;\r\nskb = napi_alloc_skb(napi, rdata->rx.hdr.dma_len);\r\nif (!skb)\r\nreturn NULL;\r\ndma_sync_single_range_for_cpu(pdata->dev, rdata->rx.hdr.dma_base,\r\nrdata->rx.hdr.dma_off,\r\nrdata->rx.hdr.dma_len, DMA_FROM_DEVICE);\r\npacket = page_address(rdata->rx.hdr.pa.pages) +\r\nrdata->rx.hdr.pa.pages_offset;\r\ncopy_len = (rdata->rx.hdr_len) ? rdata->rx.hdr_len : len;\r\ncopy_len = min(rdata->rx.hdr.dma_len, copy_len);\r\nskb_copy_to_linear_data(skb, packet, copy_len);\r\nskb_put(skb, copy_len);\r\nlen -= copy_len;\r\nif (len) {\r\ndma_sync_single_range_for_cpu(pdata->dev,\r\nrdata->rx.buf.dma_base,\r\nrdata->rx.buf.dma_off,\r\nrdata->rx.buf.dma_len,\r\nDMA_FROM_DEVICE);\r\nskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\r\nrdata->rx.buf.pa.pages,\r\nrdata->rx.buf.pa.pages_offset,\r\nlen, rdata->rx.buf.dma_len);\r\nrdata->rx.buf.pa.pages = NULL;\r\n}\r\nreturn skb;\r\n}\r\nstatic int xgbe_tx_poll(struct xgbe_channel *channel)\r\n{\r\nstruct xgbe_prv_data *pdata = channel->pdata;\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_desc_if *desc_if = &pdata->desc_if;\r\nstruct xgbe_ring *ring = channel->tx_ring;\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_ring_desc *rdesc;\r\nstruct net_device *netdev = pdata->netdev;\r\nstruct netdev_queue *txq;\r\nint processed = 0;\r\nunsigned int tx_packets = 0, tx_bytes = 0;\r\nunsigned int cur;\r\nDBGPR("-->xgbe_tx_poll\n");\r\nif (!ring)\r\nreturn 0;\r\ncur = ring->cur;\r\nsmp_rmb();\r\ntxq = netdev_get_tx_queue(netdev, channel->queue_index);\r\nwhile ((processed < XGBE_TX_DESC_MAX_PROC) &&\r\n(ring->dirty != cur)) {\r\nrdata = XGBE_GET_DESC_DATA(ring, ring->dirty);\r\nrdesc = rdata->rdesc;\r\nif (!hw_if->tx_complete(rdesc))\r\nbreak;\r\ndma_rmb();\r\nif (netif_msg_tx_done(pdata))\r\nxgbe_dump_tx_desc(pdata, ring, ring->dirty, 1, 0);\r\nif (hw_if->is_last_desc(rdesc)) {\r\ntx_packets += rdata->tx.packets;\r\ntx_bytes += rdata->tx.bytes;\r\n}\r\ndesc_if->unmap_rdata(pdata, rdata);\r\nhw_if->tx_desc_reset(rdata);\r\nprocessed++;\r\nring->dirty++;\r\n}\r\nif (!processed)\r\nreturn 0;\r\nnetdev_tx_completed_queue(txq, tx_packets, tx_bytes);\r\nif ((ring->tx.queue_stopped == 1) &&\r\n(xgbe_tx_avail_desc(ring) > XGBE_TX_DESC_MIN_FREE)) {\r\nring->tx.queue_stopped = 0;\r\nnetif_tx_wake_queue(txq);\r\n}\r\nDBGPR("<--xgbe_tx_poll: processed=%d\n", processed);\r\nreturn processed;\r\n}\r\nstatic int xgbe_rx_poll(struct xgbe_channel *channel, int budget)\r\n{\r\nstruct xgbe_prv_data *pdata = channel->pdata;\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_ring *ring = channel->rx_ring;\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_packet_data *packet;\r\nstruct net_device *netdev = pdata->netdev;\r\nstruct napi_struct *napi;\r\nstruct sk_buff *skb;\r\nstruct skb_shared_hwtstamps *hwtstamps;\r\nunsigned int incomplete, error, context_next, context;\r\nunsigned int len, rdesc_len, max_len;\r\nunsigned int received = 0;\r\nint packet_count = 0;\r\nDBGPR("-->xgbe_rx_poll: budget=%d\n", budget);\r\nif (!ring)\r\nreturn 0;\r\nincomplete = 0;\r\ncontext_next = 0;\r\nnapi = (pdata->per_channel_irq) ? &channel->napi : &pdata->napi;\r\nrdata = XGBE_GET_DESC_DATA(ring, ring->cur);\r\npacket = &ring->packet_data;\r\nwhile (packet_count < budget) {\r\nDBGPR(" cur = %d\n", ring->cur);\r\nif (!received && rdata->state_saved) {\r\nskb = rdata->state.skb;\r\nerror = rdata->state.error;\r\nlen = rdata->state.len;\r\n} else {\r\nmemset(packet, 0, sizeof(*packet));\r\nskb = NULL;\r\nerror = 0;\r\nlen = 0;\r\n}\r\nread_again:\r\nrdata = XGBE_GET_DESC_DATA(ring, ring->cur);\r\nif (xgbe_rx_dirty_desc(ring) > (XGBE_RX_DESC_CNT >> 3))\r\nxgbe_rx_refresh(channel);\r\nif (hw_if->dev_read(channel))\r\nbreak;\r\nreceived++;\r\nring->cur++;\r\nincomplete = XGMAC_GET_BITS(packet->attributes,\r\nRX_PACKET_ATTRIBUTES,\r\nINCOMPLETE);\r\ncontext_next = XGMAC_GET_BITS(packet->attributes,\r\nRX_PACKET_ATTRIBUTES,\r\nCONTEXT_NEXT);\r\ncontext = XGMAC_GET_BITS(packet->attributes,\r\nRX_PACKET_ATTRIBUTES,\r\nCONTEXT);\r\nif ((incomplete || context_next) && error)\r\ngoto read_again;\r\nif (error || packet->errors) {\r\nif (packet->errors)\r\nnetif_err(pdata, rx_err, netdev,\r\n"error in received packet\n");\r\ndev_kfree_skb(skb);\r\ngoto next_packet;\r\n}\r\nif (!context) {\r\nrdesc_len = rdata->rx.len - len;\r\nlen += rdesc_len;\r\nif (rdesc_len && !skb) {\r\nskb = xgbe_create_skb(pdata, napi, rdata,\r\nrdesc_len);\r\nif (!skb)\r\nerror = 1;\r\n} else if (rdesc_len) {\r\ndma_sync_single_range_for_cpu(pdata->dev,\r\nrdata->rx.buf.dma_base,\r\nrdata->rx.buf.dma_off,\r\nrdata->rx.buf.dma_len,\r\nDMA_FROM_DEVICE);\r\nskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\r\nrdata->rx.buf.pa.pages,\r\nrdata->rx.buf.pa.pages_offset,\r\nrdesc_len,\r\nrdata->rx.buf.dma_len);\r\nrdata->rx.buf.pa.pages = NULL;\r\n}\r\n}\r\nif (incomplete || context_next)\r\ngoto read_again;\r\nif (!skb)\r\ngoto next_packet;\r\nmax_len = netdev->mtu + ETH_HLEN;\r\nif (!(netdev->features & NETIF_F_HW_VLAN_CTAG_RX) &&\r\n(skb->protocol == htons(ETH_P_8021Q)))\r\nmax_len += VLAN_HLEN;\r\nif (skb->len > max_len) {\r\nnetif_err(pdata, rx_err, netdev,\r\n"packet length exceeds configured MTU\n");\r\ndev_kfree_skb(skb);\r\ngoto next_packet;\r\n}\r\nif (netif_msg_pktdata(pdata))\r\nxgbe_print_pkt(netdev, skb, false);\r\nskb_checksum_none_assert(skb);\r\nif (XGMAC_GET_BITS(packet->attributes,\r\nRX_PACKET_ATTRIBUTES, CSUM_DONE))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nif (XGMAC_GET_BITS(packet->attributes,\r\nRX_PACKET_ATTRIBUTES, VLAN_CTAG))\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\r\npacket->vlan_ctag);\r\nif (XGMAC_GET_BITS(packet->attributes,\r\nRX_PACKET_ATTRIBUTES, RX_TSTAMP)) {\r\nu64 nsec;\r\nnsec = timecounter_cyc2time(&pdata->tstamp_tc,\r\npacket->rx_tstamp);\r\nhwtstamps = skb_hwtstamps(skb);\r\nhwtstamps->hwtstamp = ns_to_ktime(nsec);\r\n}\r\nif (XGMAC_GET_BITS(packet->attributes,\r\nRX_PACKET_ATTRIBUTES, RSS_HASH))\r\nskb_set_hash(skb, packet->rss_hash,\r\npacket->rss_hash_type);\r\nskb->dev = netdev;\r\nskb->protocol = eth_type_trans(skb, netdev);\r\nskb_record_rx_queue(skb, channel->queue_index);\r\nnapi_gro_receive(napi, skb);\r\nnext_packet:\r\npacket_count++;\r\n}\r\nif (received && (incomplete || context_next)) {\r\nrdata = XGBE_GET_DESC_DATA(ring, ring->cur);\r\nrdata->state_saved = 1;\r\nrdata->state.skb = skb;\r\nrdata->state.len = len;\r\nrdata->state.error = error;\r\n}\r\nDBGPR("<--xgbe_rx_poll: packet_count = %d\n", packet_count);\r\nreturn packet_count;\r\n}\r\nstatic int xgbe_one_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct xgbe_channel *channel = container_of(napi, struct xgbe_channel,\r\nnapi);\r\nint processed = 0;\r\nDBGPR("-->xgbe_one_poll: budget=%d\n", budget);\r\nxgbe_tx_poll(channel);\r\nprocessed = xgbe_rx_poll(channel, budget);\r\nif (processed < budget) {\r\nnapi_complete_done(napi, processed);\r\nenable_irq(channel->dma_irq);\r\n}\r\nDBGPR("<--xgbe_one_poll: received = %d\n", processed);\r\nreturn processed;\r\n}\r\nstatic int xgbe_all_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct xgbe_prv_data *pdata = container_of(napi, struct xgbe_prv_data,\r\nnapi);\r\nstruct xgbe_channel *channel;\r\nint ring_budget;\r\nint processed, last_processed;\r\nunsigned int i;\r\nDBGPR("-->xgbe_all_poll: budget=%d\n", budget);\r\nprocessed = 0;\r\nring_budget = budget / pdata->rx_ring_count;\r\ndo {\r\nlast_processed = processed;\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nxgbe_tx_poll(channel);\r\nif (ring_budget > (budget - processed))\r\nring_budget = budget - processed;\r\nprocessed += xgbe_rx_poll(channel, ring_budget);\r\n}\r\n} while ((processed < budget) && (processed != last_processed));\r\nif (processed < budget) {\r\nnapi_complete_done(napi, processed);\r\nxgbe_enable_rx_tx_ints(pdata);\r\n}\r\nDBGPR("<--xgbe_all_poll: received = %d\n", processed);\r\nreturn processed;\r\n}\r\nvoid xgbe_dump_tx_desc(struct xgbe_prv_data *pdata, struct xgbe_ring *ring,\r\nunsigned int idx, unsigned int count, unsigned int flag)\r\n{\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_ring_desc *rdesc;\r\nwhile (count--) {\r\nrdata = XGBE_GET_DESC_DATA(ring, idx);\r\nrdesc = rdata->rdesc;\r\nnetdev_dbg(pdata->netdev,\r\n"TX_NORMAL_DESC[%d %s] = %08x:%08x:%08x:%08x\n", idx,\r\n(flag == 1) ? "QUEUED FOR TX" : "TX BY DEVICE",\r\nle32_to_cpu(rdesc->desc0),\r\nle32_to_cpu(rdesc->desc1),\r\nle32_to_cpu(rdesc->desc2),\r\nle32_to_cpu(rdesc->desc3));\r\nidx++;\r\n}\r\n}\r\nvoid xgbe_dump_rx_desc(struct xgbe_prv_data *pdata, struct xgbe_ring *ring,\r\nunsigned int idx)\r\n{\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_ring_desc *rdesc;\r\nrdata = XGBE_GET_DESC_DATA(ring, idx);\r\nrdesc = rdata->rdesc;\r\nnetdev_dbg(pdata->netdev,\r\n"RX_NORMAL_DESC[%d RX BY DEVICE] = %08x:%08x:%08x:%08x\n",\r\nidx, le32_to_cpu(rdesc->desc0), le32_to_cpu(rdesc->desc1),\r\nle32_to_cpu(rdesc->desc2), le32_to_cpu(rdesc->desc3));\r\n}\r\nvoid xgbe_print_pkt(struct net_device *netdev, struct sk_buff *skb, bool tx_rx)\r\n{\r\nstruct ethhdr *eth = (struct ethhdr *)skb->data;\r\nunsigned char *buf = skb->data;\r\nunsigned char buffer[128];\r\nunsigned int i, j;\r\nnetdev_dbg(netdev, "\n************** SKB dump ****************\n");\r\nnetdev_dbg(netdev, "%s packet of %d bytes\n",\r\n(tx_rx ? "TX" : "RX"), skb->len);\r\nnetdev_dbg(netdev, "Dst MAC addr: %pM\n", eth->h_dest);\r\nnetdev_dbg(netdev, "Src MAC addr: %pM\n", eth->h_source);\r\nnetdev_dbg(netdev, "Protocol: %#06hx\n", ntohs(eth->h_proto));\r\nfor (i = 0, j = 0; i < skb->len;) {\r\nj += snprintf(buffer + j, sizeof(buffer) - j, "%02hhx",\r\nbuf[i++]);\r\nif ((i % 32) == 0) {\r\nnetdev_dbg(netdev, " %#06x: %s\n", i - 32, buffer);\r\nj = 0;\r\n} else if ((i % 16) == 0) {\r\nbuffer[j++] = ' ';\r\nbuffer[j++] = ' ';\r\n} else if ((i % 4) == 0) {\r\nbuffer[j++] = ' ';\r\n}\r\n}\r\nif (i % 32)\r\nnetdev_dbg(netdev, " %#06x: %s\n", i - (i % 32), buffer);\r\nnetdev_dbg(netdev, "\n************** SKB dump ****************\n");\r\n}
