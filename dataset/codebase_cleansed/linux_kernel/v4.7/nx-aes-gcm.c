static int gcm_aes_nx_set_key(struct crypto_aead *tfm,\r\nconst u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_aead_ctx(tfm);\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nstruct nx_csbcpb *csbcpb_aead = nx_ctx->csbcpb_aead;\r\nnx_ctx_init(nx_ctx, HCOP_FC_AES);\r\nswitch (key_len) {\r\ncase AES_KEYSIZE_128:\r\nNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_128);\r\nNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_128);\r\nnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_128];\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_192);\r\nNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_192);\r\nnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_192];\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_256);\r\nNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_256);\r\nnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_256];\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ncsbcpb->cpb.hdr.mode = NX_MODE_AES_GCM;\r\nmemcpy(csbcpb->cpb.aes_gcm.key, in_key, key_len);\r\ncsbcpb_aead->cpb.hdr.mode = NX_MODE_AES_GCA;\r\nmemcpy(csbcpb_aead->cpb.aes_gca.key, in_key, key_len);\r\nreturn 0;\r\n}\r\nstatic int gcm4106_aes_nx_set_key(struct crypto_aead *tfm,\r\nconst u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_aead_ctx(tfm);\r\nchar *nonce = nx_ctx->priv.gcm.nonce;\r\nint rc;\r\nif (key_len < 4)\r\nreturn -EINVAL;\r\nkey_len -= 4;\r\nrc = gcm_aes_nx_set_key(tfm, in_key, key_len);\r\nif (rc)\r\ngoto out;\r\nmemcpy(nonce, in_key + key_len, 4);\r\nout:\r\nreturn rc;\r\n}\r\nstatic int gcm4106_aes_nx_setauthsize(struct crypto_aead *tfm,\r\nunsigned int authsize)\r\n{\r\nswitch (authsize) {\r\ncase 8:\r\ncase 12:\r\ncase 16:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int nx_gca(struct nx_crypto_ctx *nx_ctx,\r\nstruct aead_request *req,\r\nu8 *out,\r\nunsigned int assoclen)\r\n{\r\nint rc;\r\nstruct nx_csbcpb *csbcpb_aead = nx_ctx->csbcpb_aead;\r\nstruct scatter_walk walk;\r\nstruct nx_sg *nx_sg = nx_ctx->in_sg;\r\nunsigned int nbytes = assoclen;\r\nunsigned int processed = 0, to_process;\r\nunsigned int max_sg_len;\r\nif (nbytes <= AES_BLOCK_SIZE) {\r\nscatterwalk_start(&walk, req->src);\r\nscatterwalk_copychunks(out, &walk, nbytes, SCATTERWALK_FROM_SG);\r\nscatterwalk_done(&walk, SCATTERWALK_FROM_SG, 0);\r\nreturn 0;\r\n}\r\nNX_CPB_FDM(csbcpb_aead) &= ~NX_FDM_CONTINUATION;\r\nmax_sg_len = min_t(u64, nx_driver.of.max_sg_len/sizeof(struct nx_sg),\r\nnx_ctx->ap->sglen);\r\nmax_sg_len = min_t(u64, max_sg_len,\r\nnx_ctx->ap->databytelen/NX_PAGE_SIZE);\r\ndo {\r\nto_process = min_t(u64, nbytes - processed,\r\nnx_ctx->ap->databytelen);\r\nto_process = min_t(u64, to_process,\r\nNX_PAGE_SIZE * (max_sg_len - 1));\r\nnx_sg = nx_walk_and_build(nx_ctx->in_sg, max_sg_len,\r\nreq->src, processed, &to_process);\r\nif ((to_process + processed) < nbytes)\r\nNX_CPB_FDM(csbcpb_aead) |= NX_FDM_INTERMEDIATE;\r\nelse\r\nNX_CPB_FDM(csbcpb_aead) &= ~NX_FDM_INTERMEDIATE;\r\nnx_ctx->op_aead.inlen = (nx_ctx->in_sg - nx_sg)\r\n* sizeof(struct nx_sg);\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op_aead,\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\nreturn rc;\r\nmemcpy(csbcpb_aead->cpb.aes_gca.in_pat,\r\ncsbcpb_aead->cpb.aes_gca.out_pat,\r\nAES_BLOCK_SIZE);\r\nNX_CPB_FDM(csbcpb_aead) |= NX_FDM_CONTINUATION;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(assoclen, &(nx_ctx->stats->aes_bytes));\r\nprocessed += to_process;\r\n} while (processed < nbytes);\r\nmemcpy(out, csbcpb_aead->cpb.aes_gca.out_pat, AES_BLOCK_SIZE);\r\nreturn rc;\r\n}\r\nstatic int gmac(struct aead_request *req, struct blkcipher_desc *desc,\r\nunsigned int assoclen)\r\n{\r\nint rc;\r\nstruct nx_crypto_ctx *nx_ctx =\r\ncrypto_aead_ctx(crypto_aead_reqtfm(req));\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nstruct nx_sg *nx_sg;\r\nunsigned int nbytes = assoclen;\r\nunsigned int processed = 0, to_process;\r\nunsigned int max_sg_len;\r\ncsbcpb->cpb.hdr.mode = NX_MODE_AES_GMAC;\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_CONTINUATION;\r\nmax_sg_len = min_t(u64, nx_driver.of.max_sg_len/sizeof(struct nx_sg),\r\nnx_ctx->ap->sglen);\r\nmax_sg_len = min_t(u64, max_sg_len,\r\nnx_ctx->ap->databytelen/NX_PAGE_SIZE);\r\nmemcpy(csbcpb->cpb.aes_gcm.iv_or_cnt, desc->info, AES_BLOCK_SIZE);\r\ndo {\r\nto_process = min_t(u64, nbytes - processed,\r\nnx_ctx->ap->databytelen);\r\nto_process = min_t(u64, to_process,\r\nNX_PAGE_SIZE * (max_sg_len - 1));\r\nnx_sg = nx_walk_and_build(nx_ctx->in_sg, max_sg_len,\r\nreq->src, processed, &to_process);\r\nif ((to_process + processed) < nbytes)\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\r\nelse\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\r\nnx_ctx->op.inlen = (nx_ctx->in_sg - nx_sg)\r\n* sizeof(struct nx_sg);\r\ncsbcpb->cpb.aes_gcm.bit_length_data = 0;\r\ncsbcpb->cpb.aes_gcm.bit_length_aad = 8 * nbytes;\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\ngoto out;\r\nmemcpy(csbcpb->cpb.aes_gcm.in_pat_or_aad,\r\ncsbcpb->cpb.aes_gcm.out_pat_or_mac, AES_BLOCK_SIZE);\r\nmemcpy(csbcpb->cpb.aes_gcm.in_s0,\r\ncsbcpb->cpb.aes_gcm.out_s0, AES_BLOCK_SIZE);\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(assoclen, &(nx_ctx->stats->aes_bytes));\r\nprocessed += to_process;\r\n} while (processed < nbytes);\r\nout:\r\ncsbcpb->cpb.hdr.mode = NX_MODE_AES_GCM;\r\nreturn rc;\r\n}\r\nstatic int gcm_empty(struct aead_request *req, struct blkcipher_desc *desc,\r\nint enc)\r\n{\r\nint rc;\r\nstruct nx_crypto_ctx *nx_ctx =\r\ncrypto_aead_ctx(crypto_aead_reqtfm(req));\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nchar out[AES_BLOCK_SIZE];\r\nstruct nx_sg *in_sg, *out_sg;\r\nint len;\r\ncsbcpb->cpb.hdr.mode = NX_MODE_AES_ECB;\r\nmemcpy(csbcpb->cpb.aes_ecb.key, csbcpb->cpb.aes_gcm.key,\r\nsizeof(csbcpb->cpb.aes_ecb.key));\r\nif (enc)\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_ENDE_ENCRYPT;\r\nelse\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_ENDE_ENCRYPT;\r\nlen = AES_BLOCK_SIZE;\r\nin_sg = nx_build_sg_list(nx_ctx->in_sg, (u8 *) desc->info,\r\n&len, nx_ctx->ap->sglen);\r\nif (len != AES_BLOCK_SIZE)\r\nreturn -EINVAL;\r\nlen = sizeof(out);\r\nout_sg = nx_build_sg_list(nx_ctx->out_sg, (u8 *) out, &len,\r\nnx_ctx->ap->sglen);\r\nif (len != sizeof(out))\r\nreturn -EINVAL;\r\nnx_ctx->op.inlen = (nx_ctx->in_sg - in_sg) * sizeof(struct nx_sg);\r\nnx_ctx->op.outlen = (nx_ctx->out_sg - out_sg) * sizeof(struct nx_sg);\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\r\ndesc->flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\ngoto out;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\nmemcpy(csbcpb->cpb.aes_gcm.out_pat_or_mac, out,\r\ncrypto_aead_authsize(crypto_aead_reqtfm(req)));\r\nout:\r\ncsbcpb->cpb.hdr.mode = NX_MODE_AES_GCM;\r\nmemset(csbcpb->cpb.aes_ecb.key, 0, sizeof(csbcpb->cpb.aes_ecb.key));\r\nreturn rc;\r\n}\r\nstatic int gcm_aes_nx_crypt(struct aead_request *req, int enc,\r\nunsigned int assoclen)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx =\r\ncrypto_aead_ctx(crypto_aead_reqtfm(req));\r\nstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nstruct blkcipher_desc desc;\r\nunsigned int nbytes = req->cryptlen;\r\nunsigned int processed = 0, to_process;\r\nunsigned long irq_flags;\r\nint rc = -EINVAL;\r\nspin_lock_irqsave(&nx_ctx->lock, irq_flags);\r\ndesc.info = rctx->iv;\r\n*(u32 *)(desc.info + NX_GCM_CTR_OFFSET) = 1;\r\nif (nbytes == 0) {\r\nif (assoclen == 0)\r\nrc = gcm_empty(req, &desc, enc);\r\nelse\r\nrc = gmac(req, &desc, assoclen);\r\nif (rc)\r\ngoto out;\r\nelse\r\ngoto mac;\r\n}\r\ncsbcpb->cpb.aes_gcm.bit_length_aad = assoclen * 8;\r\nif (assoclen) {\r\nrc = nx_gca(nx_ctx, req, csbcpb->cpb.aes_gcm.in_pat_or_aad,\r\nassoclen);\r\nif (rc)\r\ngoto out;\r\n}\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_CONTINUATION;\r\nif (enc) {\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_ENDE_ENCRYPT;\r\n} else {\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_ENDE_ENCRYPT;\r\nnbytes -= crypto_aead_authsize(crypto_aead_reqtfm(req));\r\n}\r\ndo {\r\nto_process = nbytes - processed;\r\ncsbcpb->cpb.aes_gcm.bit_length_data = nbytes * 8;\r\nrc = nx_build_sg_lists(nx_ctx, &desc, req->dst,\r\nreq->src, &to_process,\r\nprocessed + req->assoclen,\r\ncsbcpb->cpb.aes_gcm.iv_or_cnt);\r\nif (rc)\r\ngoto out;\r\nif ((to_process + processed) < nbytes)\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\r\nelse\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\ngoto out;\r\nmemcpy(desc.info, csbcpb->cpb.aes_gcm.out_cnt, AES_BLOCK_SIZE);\r\nmemcpy(csbcpb->cpb.aes_gcm.in_pat_or_aad,\r\ncsbcpb->cpb.aes_gcm.out_pat_or_mac, AES_BLOCK_SIZE);\r\nmemcpy(csbcpb->cpb.aes_gcm.in_s0,\r\ncsbcpb->cpb.aes_gcm.out_s0, AES_BLOCK_SIZE);\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(csbcpb->csb.processed_byte_count,\r\n&(nx_ctx->stats->aes_bytes));\r\nprocessed += to_process;\r\n} while (processed < nbytes);\r\nmac:\r\nif (enc) {\r\nscatterwalk_map_and_copy(\r\ncsbcpb->cpb.aes_gcm.out_pat_or_mac,\r\nreq->dst, req->assoclen + nbytes,\r\ncrypto_aead_authsize(crypto_aead_reqtfm(req)),\r\nSCATTERWALK_TO_SG);\r\n} else {\r\nu8 *itag = nx_ctx->priv.gcm.iauth_tag;\r\nu8 *otag = csbcpb->cpb.aes_gcm.out_pat_or_mac;\r\nscatterwalk_map_and_copy(\r\nitag, req->src, req->assoclen + nbytes,\r\ncrypto_aead_authsize(crypto_aead_reqtfm(req)),\r\nSCATTERWALK_FROM_SG);\r\nrc = crypto_memneq(itag, otag,\r\ncrypto_aead_authsize(crypto_aead_reqtfm(req))) ?\r\n-EBADMSG : 0;\r\n}\r\nout:\r\nspin_unlock_irqrestore(&nx_ctx->lock, irq_flags);\r\nreturn rc;\r\n}\r\nstatic int gcm_aes_nx_encrypt(struct aead_request *req)\r\n{\r\nstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\r\nchar *iv = rctx->iv;\r\nmemcpy(iv, req->iv, 12);\r\nreturn gcm_aes_nx_crypt(req, 1, req->assoclen);\r\n}\r\nstatic int gcm_aes_nx_decrypt(struct aead_request *req)\r\n{\r\nstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\r\nchar *iv = rctx->iv;\r\nmemcpy(iv, req->iv, 12);\r\nreturn gcm_aes_nx_crypt(req, 0, req->assoclen);\r\n}\r\nstatic int gcm4106_aes_nx_encrypt(struct aead_request *req)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx =\r\ncrypto_aead_ctx(crypto_aead_reqtfm(req));\r\nstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\r\nchar *iv = rctx->iv;\r\nchar *nonce = nx_ctx->priv.gcm.nonce;\r\nmemcpy(iv, nonce, NX_GCM4106_NONCE_LEN);\r\nmemcpy(iv + NX_GCM4106_NONCE_LEN, req->iv, 8);\r\nif (req->assoclen < 8)\r\nreturn -EINVAL;\r\nreturn gcm_aes_nx_crypt(req, 1, req->assoclen - 8);\r\n}\r\nstatic int gcm4106_aes_nx_decrypt(struct aead_request *req)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx =\r\ncrypto_aead_ctx(crypto_aead_reqtfm(req));\r\nstruct nx_gcm_rctx *rctx = aead_request_ctx(req);\r\nchar *iv = rctx->iv;\r\nchar *nonce = nx_ctx->priv.gcm.nonce;\r\nmemcpy(iv, nonce, NX_GCM4106_NONCE_LEN);\r\nmemcpy(iv + NX_GCM4106_NONCE_LEN, req->iv, 8);\r\nif (req->assoclen < 8)\r\nreturn -EINVAL;\r\nreturn gcm_aes_nx_crypt(req, 0, req->assoclen - 8);\r\n}
