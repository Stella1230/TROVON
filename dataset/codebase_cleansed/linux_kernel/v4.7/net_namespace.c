static struct net_generic *net_alloc_generic(void)\r\n{\r\nstruct net_generic *ng;\r\nsize_t generic_size = offsetof(struct net_generic, ptr[max_gen_ptrs]);\r\nng = kzalloc(generic_size, GFP_KERNEL);\r\nif (ng)\r\nng->len = max_gen_ptrs;\r\nreturn ng;\r\n}\r\nstatic int net_assign_generic(struct net *net, int id, void *data)\r\n{\r\nstruct net_generic *ng, *old_ng;\r\nBUG_ON(!mutex_is_locked(&net_mutex));\r\nBUG_ON(id == 0);\r\nold_ng = rcu_dereference_protected(net->gen,\r\nlockdep_is_held(&net_mutex));\r\nng = old_ng;\r\nif (old_ng->len >= id)\r\ngoto assign;\r\nng = net_alloc_generic();\r\nif (ng == NULL)\r\nreturn -ENOMEM;\r\nmemcpy(&ng->ptr, &old_ng->ptr, old_ng->len * sizeof(void*));\r\nrcu_assign_pointer(net->gen, ng);\r\nkfree_rcu(old_ng, rcu);\r\nassign:\r\nng->ptr[id - 1] = data;\r\nreturn 0;\r\n}\r\nstatic int ops_init(const struct pernet_operations *ops, struct net *net)\r\n{\r\nint err = -ENOMEM;\r\nvoid *data = NULL;\r\nif (ops->id && ops->size) {\r\ndata = kzalloc(ops->size, GFP_KERNEL);\r\nif (!data)\r\ngoto out;\r\nerr = net_assign_generic(net, *ops->id, data);\r\nif (err)\r\ngoto cleanup;\r\n}\r\nerr = 0;\r\nif (ops->init)\r\nerr = ops->init(net);\r\nif (!err)\r\nreturn 0;\r\ncleanup:\r\nkfree(data);\r\nout:\r\nreturn err;\r\n}\r\nstatic void ops_free(const struct pernet_operations *ops, struct net *net)\r\n{\r\nif (ops->id && ops->size) {\r\nint id = *ops->id;\r\nkfree(net_generic(net, id));\r\n}\r\n}\r\nstatic void ops_exit_list(const struct pernet_operations *ops,\r\nstruct list_head *net_exit_list)\r\n{\r\nstruct net *net;\r\nif (ops->exit) {\r\nlist_for_each_entry(net, net_exit_list, exit_list)\r\nops->exit(net);\r\n}\r\nif (ops->exit_batch)\r\nops->exit_batch(net_exit_list);\r\n}\r\nstatic void ops_free_list(const struct pernet_operations *ops,\r\nstruct list_head *net_exit_list)\r\n{\r\nstruct net *net;\r\nif (ops->size && ops->id) {\r\nlist_for_each_entry(net, net_exit_list, exit_list)\r\nops_free(ops, net);\r\n}\r\n}\r\nstatic int alloc_netid(struct net *net, struct net *peer, int reqid)\r\n{\r\nint min = 0, max = 0;\r\nif (reqid >= 0) {\r\nmin = reqid;\r\nmax = reqid + 1;\r\n}\r\nreturn idr_alloc(&net->netns_ids, peer, min, max, GFP_ATOMIC);\r\n}\r\nstatic int net_eq_idr(int id, void *net, void *peer)\r\n{\r\nif (net_eq(net, peer))\r\nreturn id ? : NET_ID_ZERO;\r\nreturn 0;\r\n}\r\nstatic int __peernet2id_alloc(struct net *net, struct net *peer, bool *alloc)\r\n{\r\nint id = idr_for_each(&net->netns_ids, net_eq_idr, peer);\r\nbool alloc_it = *alloc;\r\n*alloc = false;\r\nif (id == NET_ID_ZERO)\r\nreturn 0;\r\nif (id > 0)\r\nreturn id;\r\nif (alloc_it) {\r\nid = alloc_netid(net, peer, -1);\r\n*alloc = true;\r\nreturn id >= 0 ? id : NETNSA_NSID_NOT_ASSIGNED;\r\n}\r\nreturn NETNSA_NSID_NOT_ASSIGNED;\r\n}\r\nstatic int __peernet2id(struct net *net, struct net *peer)\r\n{\r\nbool no = false;\r\nreturn __peernet2id_alloc(net, peer, &no);\r\n}\r\nint peernet2id_alloc(struct net *net, struct net *peer)\r\n{\r\nunsigned long flags;\r\nbool alloc;\r\nint id;\r\nspin_lock_irqsave(&net->nsid_lock, flags);\r\nalloc = atomic_read(&peer->count) == 0 ? false : true;\r\nid = __peernet2id_alloc(net, peer, &alloc);\r\nspin_unlock_irqrestore(&net->nsid_lock, flags);\r\nif (alloc && id >= 0)\r\nrtnl_net_notifyid(net, RTM_NEWNSID, id);\r\nreturn id;\r\n}\r\nint peernet2id(struct net *net, struct net *peer)\r\n{\r\nunsigned long flags;\r\nint id;\r\nspin_lock_irqsave(&net->nsid_lock, flags);\r\nid = __peernet2id(net, peer);\r\nspin_unlock_irqrestore(&net->nsid_lock, flags);\r\nreturn id;\r\n}\r\nbool peernet_has_id(struct net *net, struct net *peer)\r\n{\r\nreturn peernet2id(net, peer) >= 0;\r\n}\r\nstruct net *get_net_ns_by_id(struct net *net, int id)\r\n{\r\nunsigned long flags;\r\nstruct net *peer;\r\nif (id < 0)\r\nreturn NULL;\r\nrcu_read_lock();\r\nspin_lock_irqsave(&net->nsid_lock, flags);\r\npeer = idr_find(&net->netns_ids, id);\r\nif (peer)\r\nget_net(peer);\r\nspin_unlock_irqrestore(&net->nsid_lock, flags);\r\nrcu_read_unlock();\r\nreturn peer;\r\n}\r\nstatic __net_init int setup_net(struct net *net, struct user_namespace *user_ns)\r\n{\r\nconst struct pernet_operations *ops, *saved_ops;\r\nint error = 0;\r\nLIST_HEAD(net_exit_list);\r\natomic_set(&net->count, 1);\r\natomic_set(&net->passive, 1);\r\nnet->dev_base_seq = 1;\r\nnet->user_ns = user_ns;\r\nidr_init(&net->netns_ids);\r\nspin_lock_init(&net->nsid_lock);\r\nlist_for_each_entry(ops, &pernet_list, list) {\r\nerror = ops_init(ops, net);\r\nif (error < 0)\r\ngoto out_undo;\r\n}\r\nout:\r\nreturn error;\r\nout_undo:\r\nlist_add(&net->exit_list, &net_exit_list);\r\nsaved_ops = ops;\r\nlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\r\nops_exit_list(ops, &net_exit_list);\r\nops = saved_ops;\r\nlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\r\nops_free_list(ops, &net_exit_list);\r\nrcu_barrier();\r\ngoto out;\r\n}\r\nstatic struct net *net_alloc(void)\r\n{\r\nstruct net *net = NULL;\r\nstruct net_generic *ng;\r\nng = net_alloc_generic();\r\nif (!ng)\r\ngoto out;\r\nnet = kmem_cache_zalloc(net_cachep, GFP_KERNEL);\r\nif (!net)\r\ngoto out_free;\r\nrcu_assign_pointer(net->gen, ng);\r\nout:\r\nreturn net;\r\nout_free:\r\nkfree(ng);\r\ngoto out;\r\n}\r\nstatic void net_free(struct net *net)\r\n{\r\nkfree(rcu_access_pointer(net->gen));\r\nkmem_cache_free(net_cachep, net);\r\n}\r\nvoid net_drop_ns(void *p)\r\n{\r\nstruct net *ns = p;\r\nif (ns && atomic_dec_and_test(&ns->passive))\r\nnet_free(ns);\r\n}\r\nstruct net *copy_net_ns(unsigned long flags,\r\nstruct user_namespace *user_ns, struct net *old_net)\r\n{\r\nstruct net *net;\r\nint rv;\r\nif (!(flags & CLONE_NEWNET))\r\nreturn get_net(old_net);\r\nnet = net_alloc();\r\nif (!net)\r\nreturn ERR_PTR(-ENOMEM);\r\nget_user_ns(user_ns);\r\nmutex_lock(&net_mutex);\r\nrv = setup_net(net, user_ns);\r\nif (rv == 0) {\r\nrtnl_lock();\r\nlist_add_tail_rcu(&net->list, &net_namespace_list);\r\nrtnl_unlock();\r\n}\r\nmutex_unlock(&net_mutex);\r\nif (rv < 0) {\r\nput_user_ns(user_ns);\r\nnet_drop_ns(net);\r\nreturn ERR_PTR(rv);\r\n}\r\nreturn net;\r\n}\r\nstatic void cleanup_net(struct work_struct *work)\r\n{\r\nconst struct pernet_operations *ops;\r\nstruct net *net, *tmp;\r\nstruct list_head net_kill_list;\r\nLIST_HEAD(net_exit_list);\r\nspin_lock_irq(&cleanup_list_lock);\r\nlist_replace_init(&cleanup_list, &net_kill_list);\r\nspin_unlock_irq(&cleanup_list_lock);\r\nmutex_lock(&net_mutex);\r\nrtnl_lock();\r\nlist_for_each_entry(net, &net_kill_list, cleanup_list) {\r\nlist_del_rcu(&net->list);\r\nlist_add_tail(&net->exit_list, &net_exit_list);\r\nfor_each_net(tmp) {\r\nint id;\r\nspin_lock_irq(&tmp->nsid_lock);\r\nid = __peernet2id(tmp, net);\r\nif (id >= 0)\r\nidr_remove(&tmp->netns_ids, id);\r\nspin_unlock_irq(&tmp->nsid_lock);\r\nif (id >= 0)\r\nrtnl_net_notifyid(tmp, RTM_DELNSID, id);\r\n}\r\nspin_lock_irq(&net->nsid_lock);\r\nidr_destroy(&net->netns_ids);\r\nspin_unlock_irq(&net->nsid_lock);\r\n}\r\nrtnl_unlock();\r\nsynchronize_rcu();\r\nlist_for_each_entry_reverse(ops, &pernet_list, list)\r\nops_exit_list(ops, &net_exit_list);\r\nlist_for_each_entry_reverse(ops, &pernet_list, list)\r\nops_free_list(ops, &net_exit_list);\r\nmutex_unlock(&net_mutex);\r\nrcu_barrier();\r\nlist_for_each_entry_safe(net, tmp, &net_exit_list, exit_list) {\r\nlist_del_init(&net->exit_list);\r\nput_user_ns(net->user_ns);\r\nnet_drop_ns(net);\r\n}\r\n}\r\nvoid __put_net(struct net *net)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&cleanup_list_lock, flags);\r\nlist_add(&net->cleanup_list, &cleanup_list);\r\nspin_unlock_irqrestore(&cleanup_list_lock, flags);\r\nqueue_work(netns_wq, &net_cleanup_work);\r\n}\r\nstruct net *get_net_ns_by_fd(int fd)\r\n{\r\nstruct file *file;\r\nstruct ns_common *ns;\r\nstruct net *net;\r\nfile = proc_ns_fget(fd);\r\nif (IS_ERR(file))\r\nreturn ERR_CAST(file);\r\nns = get_proc_ns(file_inode(file));\r\nif (ns->ops == &netns_operations)\r\nnet = get_net(container_of(ns, struct net, ns));\r\nelse\r\nnet = ERR_PTR(-EINVAL);\r\nfput(file);\r\nreturn net;\r\n}\r\nstruct net *get_net_ns_by_fd(int fd)\r\n{\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstruct net *get_net_ns_by_pid(pid_t pid)\r\n{\r\nstruct task_struct *tsk;\r\nstruct net *net;\r\nnet = ERR_PTR(-ESRCH);\r\nrcu_read_lock();\r\ntsk = find_task_by_vpid(pid);\r\nif (tsk) {\r\nstruct nsproxy *nsproxy;\r\ntask_lock(tsk);\r\nnsproxy = tsk->nsproxy;\r\nif (nsproxy)\r\nnet = get_net(nsproxy->net_ns);\r\ntask_unlock(tsk);\r\n}\r\nrcu_read_unlock();\r\nreturn net;\r\n}\r\nstatic __net_init int net_ns_net_init(struct net *net)\r\n{\r\n#ifdef CONFIG_NET_NS\r\nnet->ns.ops = &netns_operations;\r\n#endif\r\nreturn ns_alloc_inum(&net->ns);\r\n}\r\nstatic __net_exit void net_ns_net_exit(struct net *net)\r\n{\r\nns_free_inum(&net->ns);\r\n}\r\nstatic int rtnl_net_newid(struct sk_buff *skb, struct nlmsghdr *nlh)\r\n{\r\nstruct net *net = sock_net(skb->sk);\r\nstruct nlattr *tb[NETNSA_MAX + 1];\r\nunsigned long flags;\r\nstruct net *peer;\r\nint nsid, err;\r\nerr = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\r\nrtnl_net_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (!tb[NETNSA_NSID])\r\nreturn -EINVAL;\r\nnsid = nla_get_s32(tb[NETNSA_NSID]);\r\nif (tb[NETNSA_PID])\r\npeer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));\r\nelse if (tb[NETNSA_FD])\r\npeer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));\r\nelse\r\nreturn -EINVAL;\r\nif (IS_ERR(peer))\r\nreturn PTR_ERR(peer);\r\nspin_lock_irqsave(&net->nsid_lock, flags);\r\nif (__peernet2id(net, peer) >= 0) {\r\nspin_unlock_irqrestore(&net->nsid_lock, flags);\r\nerr = -EEXIST;\r\ngoto out;\r\n}\r\nerr = alloc_netid(net, peer, nsid);\r\nspin_unlock_irqrestore(&net->nsid_lock, flags);\r\nif (err >= 0) {\r\nrtnl_net_notifyid(net, RTM_NEWNSID, err);\r\nerr = 0;\r\n}\r\nout:\r\nput_net(peer);\r\nreturn err;\r\n}\r\nstatic int rtnl_net_get_size(void)\r\n{\r\nreturn NLMSG_ALIGN(sizeof(struct rtgenmsg))\r\n+ nla_total_size(sizeof(s32))\r\n;\r\n}\r\nstatic int rtnl_net_fill(struct sk_buff *skb, u32 portid, u32 seq, int flags,\r\nint cmd, struct net *net, int nsid)\r\n{\r\nstruct nlmsghdr *nlh;\r\nstruct rtgenmsg *rth;\r\nnlh = nlmsg_put(skb, portid, seq, cmd, sizeof(*rth), flags);\r\nif (!nlh)\r\nreturn -EMSGSIZE;\r\nrth = nlmsg_data(nlh);\r\nrth->rtgen_family = AF_UNSPEC;\r\nif (nla_put_s32(skb, NETNSA_NSID, nsid))\r\ngoto nla_put_failure;\r\nnlmsg_end(skb, nlh);\r\nreturn 0;\r\nnla_put_failure:\r\nnlmsg_cancel(skb, nlh);\r\nreturn -EMSGSIZE;\r\n}\r\nstatic int rtnl_net_getid(struct sk_buff *skb, struct nlmsghdr *nlh)\r\n{\r\nstruct net *net = sock_net(skb->sk);\r\nstruct nlattr *tb[NETNSA_MAX + 1];\r\nstruct sk_buff *msg;\r\nstruct net *peer;\r\nint err, id;\r\nerr = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\r\nrtnl_net_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[NETNSA_PID])\r\npeer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));\r\nelse if (tb[NETNSA_FD])\r\npeer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));\r\nelse\r\nreturn -EINVAL;\r\nif (IS_ERR(peer))\r\nreturn PTR_ERR(peer);\r\nmsg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);\r\nif (!msg) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nid = peernet2id(net, peer);\r\nerr = rtnl_net_fill(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq, 0,\r\nRTM_NEWNSID, net, id);\r\nif (err < 0)\r\ngoto err_out;\r\nerr = rtnl_unicast(msg, net, NETLINK_CB(skb).portid);\r\ngoto out;\r\nerr_out:\r\nnlmsg_free(msg);\r\nout:\r\nput_net(peer);\r\nreturn err;\r\n}\r\nstatic int rtnl_net_dumpid_one(int id, void *peer, void *data)\r\n{\r\nstruct rtnl_net_dump_cb *net_cb = (struct rtnl_net_dump_cb *)data;\r\nint ret;\r\nif (net_cb->idx < net_cb->s_idx)\r\ngoto cont;\r\nret = rtnl_net_fill(net_cb->skb, NETLINK_CB(net_cb->cb->skb).portid,\r\nnet_cb->cb->nlh->nlmsg_seq, NLM_F_MULTI,\r\nRTM_NEWNSID, net_cb->net, id);\r\nif (ret < 0)\r\nreturn ret;\r\ncont:\r\nnet_cb->idx++;\r\nreturn 0;\r\n}\r\nstatic int rtnl_net_dumpid(struct sk_buff *skb, struct netlink_callback *cb)\r\n{\r\nstruct net *net = sock_net(skb->sk);\r\nstruct rtnl_net_dump_cb net_cb = {\r\n.net = net,\r\n.skb = skb,\r\n.cb = cb,\r\n.idx = 0,\r\n.s_idx = cb->args[0],\r\n};\r\nunsigned long flags;\r\nspin_lock_irqsave(&net->nsid_lock, flags);\r\nidr_for_each(&net->netns_ids, rtnl_net_dumpid_one, &net_cb);\r\nspin_unlock_irqrestore(&net->nsid_lock, flags);\r\ncb->args[0] = net_cb.idx;\r\nreturn skb->len;\r\n}\r\nstatic void rtnl_net_notifyid(struct net *net, int cmd, int id)\r\n{\r\nstruct sk_buff *msg;\r\nint err = -ENOMEM;\r\nmsg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);\r\nif (!msg)\r\ngoto out;\r\nerr = rtnl_net_fill(msg, 0, 0, 0, cmd, net, id);\r\nif (err < 0)\r\ngoto err_out;\r\nrtnl_notify(msg, net, 0, RTNLGRP_NSID, NULL, 0);\r\nreturn;\r\nerr_out:\r\nnlmsg_free(msg);\r\nout:\r\nrtnl_set_sk_err(net, RTNLGRP_NSID, err);\r\n}\r\nstatic int __init net_ns_init(void)\r\n{\r\nstruct net_generic *ng;\r\n#ifdef CONFIG_NET_NS\r\nnet_cachep = kmem_cache_create("net_namespace", sizeof(struct net),\r\nSMP_CACHE_BYTES,\r\nSLAB_PANIC, NULL);\r\nnetns_wq = create_singlethread_workqueue("netns");\r\nif (!netns_wq)\r\npanic("Could not create netns workq");\r\n#endif\r\nng = net_alloc_generic();\r\nif (!ng)\r\npanic("Could not allocate generic netns");\r\nrcu_assign_pointer(init_net.gen, ng);\r\nmutex_lock(&net_mutex);\r\nif (setup_net(&init_net, &init_user_ns))\r\npanic("Could not setup the initial network namespace");\r\nrtnl_lock();\r\nlist_add_tail_rcu(&init_net.list, &net_namespace_list);\r\nrtnl_unlock();\r\nmutex_unlock(&net_mutex);\r\nregister_pernet_subsys(&net_ns_ops);\r\nrtnl_register(PF_UNSPEC, RTM_NEWNSID, rtnl_net_newid, NULL, NULL);\r\nrtnl_register(PF_UNSPEC, RTM_GETNSID, rtnl_net_getid, rtnl_net_dumpid,\r\nNULL);\r\nreturn 0;\r\n}\r\nstatic int __register_pernet_operations(struct list_head *list,\r\nstruct pernet_operations *ops)\r\n{\r\nstruct net *net;\r\nint error;\r\nLIST_HEAD(net_exit_list);\r\nlist_add_tail(&ops->list, list);\r\nif (ops->init || (ops->id && ops->size)) {\r\nfor_each_net(net) {\r\nerror = ops_init(ops, net);\r\nif (error)\r\ngoto out_undo;\r\nlist_add_tail(&net->exit_list, &net_exit_list);\r\n}\r\n}\r\nreturn 0;\r\nout_undo:\r\nlist_del(&ops->list);\r\nops_exit_list(ops, &net_exit_list);\r\nops_free_list(ops, &net_exit_list);\r\nreturn error;\r\n}\r\nstatic void __unregister_pernet_operations(struct pernet_operations *ops)\r\n{\r\nstruct net *net;\r\nLIST_HEAD(net_exit_list);\r\nlist_del(&ops->list);\r\nfor_each_net(net)\r\nlist_add_tail(&net->exit_list, &net_exit_list);\r\nops_exit_list(ops, &net_exit_list);\r\nops_free_list(ops, &net_exit_list);\r\n}\r\nstatic int __register_pernet_operations(struct list_head *list,\r\nstruct pernet_operations *ops)\r\n{\r\nreturn ops_init(ops, &init_net);\r\n}\r\nstatic void __unregister_pernet_operations(struct pernet_operations *ops)\r\n{\r\nLIST_HEAD(net_exit_list);\r\nlist_add(&init_net.exit_list, &net_exit_list);\r\nops_exit_list(ops, &net_exit_list);\r\nops_free_list(ops, &net_exit_list);\r\n}\r\nstatic int register_pernet_operations(struct list_head *list,\r\nstruct pernet_operations *ops)\r\n{\r\nint error;\r\nif (ops->id) {\r\nagain:\r\nerror = ida_get_new_above(&net_generic_ids, 1, ops->id);\r\nif (error < 0) {\r\nif (error == -EAGAIN) {\r\nida_pre_get(&net_generic_ids, GFP_KERNEL);\r\ngoto again;\r\n}\r\nreturn error;\r\n}\r\nmax_gen_ptrs = max_t(unsigned int, max_gen_ptrs, *ops->id);\r\n}\r\nerror = __register_pernet_operations(list, ops);\r\nif (error) {\r\nrcu_barrier();\r\nif (ops->id)\r\nida_remove(&net_generic_ids, *ops->id);\r\n}\r\nreturn error;\r\n}\r\nstatic void unregister_pernet_operations(struct pernet_operations *ops)\r\n{\r\n__unregister_pernet_operations(ops);\r\nrcu_barrier();\r\nif (ops->id)\r\nida_remove(&net_generic_ids, *ops->id);\r\n}\r\nint register_pernet_subsys(struct pernet_operations *ops)\r\n{\r\nint error;\r\nmutex_lock(&net_mutex);\r\nerror = register_pernet_operations(first_device, ops);\r\nmutex_unlock(&net_mutex);\r\nreturn error;\r\n}\r\nvoid unregister_pernet_subsys(struct pernet_operations *ops)\r\n{\r\nmutex_lock(&net_mutex);\r\nunregister_pernet_operations(ops);\r\nmutex_unlock(&net_mutex);\r\n}\r\nint register_pernet_device(struct pernet_operations *ops)\r\n{\r\nint error;\r\nmutex_lock(&net_mutex);\r\nerror = register_pernet_operations(&pernet_list, ops);\r\nif (!error && (first_device == &pernet_list))\r\nfirst_device = &ops->list;\r\nmutex_unlock(&net_mutex);\r\nreturn error;\r\n}\r\nvoid unregister_pernet_device(struct pernet_operations *ops)\r\n{\r\nmutex_lock(&net_mutex);\r\nif (&ops->list == first_device)\r\nfirst_device = first_device->next;\r\nunregister_pernet_operations(ops);\r\nmutex_unlock(&net_mutex);\r\n}\r\nstatic struct ns_common *netns_get(struct task_struct *task)\r\n{\r\nstruct net *net = NULL;\r\nstruct nsproxy *nsproxy;\r\ntask_lock(task);\r\nnsproxy = task->nsproxy;\r\nif (nsproxy)\r\nnet = get_net(nsproxy->net_ns);\r\ntask_unlock(task);\r\nreturn net ? &net->ns : NULL;\r\n}\r\nstatic inline struct net *to_net_ns(struct ns_common *ns)\r\n{\r\nreturn container_of(ns, struct net, ns);\r\n}\r\nstatic void netns_put(struct ns_common *ns)\r\n{\r\nput_net(to_net_ns(ns));\r\n}\r\nstatic int netns_install(struct nsproxy *nsproxy, struct ns_common *ns)\r\n{\r\nstruct net *net = to_net_ns(ns);\r\nif (!ns_capable(net->user_ns, CAP_SYS_ADMIN) ||\r\n!ns_capable(current_user_ns(), CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nput_net(nsproxy->net_ns);\r\nnsproxy->net_ns = get_net(net);\r\nreturn 0;\r\n}
