static int rio_mport_maint_rd(struct mport_cdev_priv *priv, void __user *arg,\r\nint local)\r\n{\r\nstruct rio_mport *mport = priv->md->mport;\r\nstruct rio_mport_maint_io maint_io;\r\nu32 *buffer;\r\nu32 offset;\r\nsize_t length;\r\nint ret, i;\r\nif (unlikely(copy_from_user(&maint_io, arg, sizeof(maint_io))))\r\nreturn -EFAULT;\r\nif ((maint_io.offset % 4) ||\r\n(maint_io.length == 0) || (maint_io.length % 4) ||\r\n(maint_io.length + maint_io.offset) > RIO_MAINT_SPACE_SZ)\r\nreturn -EINVAL;\r\nbuffer = vmalloc(maint_io.length);\r\nif (buffer == NULL)\r\nreturn -ENOMEM;\r\nlength = maint_io.length/sizeof(u32);\r\noffset = maint_io.offset;\r\nfor (i = 0; i < length; i++) {\r\nif (local)\r\nret = __rio_local_read_config_32(mport,\r\noffset, &buffer[i]);\r\nelse\r\nret = rio_mport_read_config_32(mport, maint_io.rioid,\r\nmaint_io.hopcount, offset, &buffer[i]);\r\nif (ret)\r\ngoto out;\r\noffset += 4;\r\n}\r\nif (unlikely(copy_to_user((void __user *)(uintptr_t)maint_io.buffer,\r\nbuffer, maint_io.length)))\r\nret = -EFAULT;\r\nout:\r\nvfree(buffer);\r\nreturn ret;\r\n}\r\nstatic int rio_mport_maint_wr(struct mport_cdev_priv *priv, void __user *arg,\r\nint local)\r\n{\r\nstruct rio_mport *mport = priv->md->mport;\r\nstruct rio_mport_maint_io maint_io;\r\nu32 *buffer;\r\nu32 offset;\r\nsize_t length;\r\nint ret = -EINVAL, i;\r\nif (unlikely(copy_from_user(&maint_io, arg, sizeof(maint_io))))\r\nreturn -EFAULT;\r\nif ((maint_io.offset % 4) ||\r\n(maint_io.length == 0) || (maint_io.length % 4) ||\r\n(maint_io.length + maint_io.offset) > RIO_MAINT_SPACE_SZ)\r\nreturn -EINVAL;\r\nbuffer = vmalloc(maint_io.length);\r\nif (buffer == NULL)\r\nreturn -ENOMEM;\r\nlength = maint_io.length;\r\nif (unlikely(copy_from_user(buffer,\r\n(void __user *)(uintptr_t)maint_io.buffer, length))) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\noffset = maint_io.offset;\r\nlength /= sizeof(u32);\r\nfor (i = 0; i < length; i++) {\r\nif (local)\r\nret = __rio_local_write_config_32(mport,\r\noffset, buffer[i]);\r\nelse\r\nret = rio_mport_write_config_32(mport, maint_io.rioid,\r\nmaint_io.hopcount,\r\noffset, buffer[i]);\r\nif (ret)\r\ngoto out;\r\noffset += 4;\r\n}\r\nout:\r\nvfree(buffer);\r\nreturn ret;\r\n}\r\nstatic int\r\nrio_mport_create_outbound_mapping(struct mport_dev *md, struct file *filp,\r\nu16 rioid, u64 raddr, u32 size,\r\ndma_addr_t *paddr)\r\n{\r\nstruct rio_mport *mport = md->mport;\r\nstruct rio_mport_mapping *map;\r\nint ret;\r\nrmcd_debug(OBW, "did=%d ra=0x%llx sz=0x%x", rioid, raddr, size);\r\nmap = kzalloc(sizeof(*map), GFP_KERNEL);\r\nif (map == NULL)\r\nreturn -ENOMEM;\r\nret = rio_map_outb_region(mport, rioid, raddr, size, 0, paddr);\r\nif (ret < 0)\r\ngoto err_map_outb;\r\nmap->dir = MAP_OUTBOUND;\r\nmap->rioid = rioid;\r\nmap->rio_addr = raddr;\r\nmap->size = size;\r\nmap->phys_addr = *paddr;\r\nmap->filp = filp;\r\nmap->md = md;\r\nkref_init(&map->ref);\r\nlist_add_tail(&map->node, &md->mappings);\r\nreturn 0;\r\nerr_map_outb:\r\nkfree(map);\r\nreturn ret;\r\n}\r\nstatic int\r\nrio_mport_get_outbound_mapping(struct mport_dev *md, struct file *filp,\r\nu16 rioid, u64 raddr, u32 size,\r\ndma_addr_t *paddr)\r\n{\r\nstruct rio_mport_mapping *map;\r\nint err = -ENOMEM;\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry(map, &md->mappings, node) {\r\nif (map->dir != MAP_OUTBOUND)\r\ncontinue;\r\nif (rioid == map->rioid &&\r\nraddr == map->rio_addr && size == map->size) {\r\n*paddr = map->phys_addr;\r\nerr = 0;\r\nbreak;\r\n} else if (rioid == map->rioid &&\r\nraddr < (map->rio_addr + map->size - 1) &&\r\n(raddr + size) > map->rio_addr) {\r\nerr = -EBUSY;\r\nbreak;\r\n}\r\n}\r\nif (err == -ENOMEM)\r\nerr = rio_mport_create_outbound_mapping(md, filp, rioid, raddr,\r\nsize, paddr);\r\nmutex_unlock(&md->buf_mutex);\r\nreturn err;\r\n}\r\nstatic int rio_mport_obw_map(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *data = priv->md;\r\nstruct rio_mmap map;\r\ndma_addr_t paddr;\r\nint ret;\r\nif (unlikely(copy_from_user(&map, arg, sizeof(map))))\r\nreturn -EFAULT;\r\nrmcd_debug(OBW, "did=%d ra=0x%llx sz=0x%llx",\r\nmap.rioid, map.rio_addr, map.length);\r\nret = rio_mport_get_outbound_mapping(data, filp, map.rioid,\r\nmap.rio_addr, map.length, &paddr);\r\nif (ret < 0) {\r\nrmcd_error("Failed to set OBW err= %d", ret);\r\nreturn ret;\r\n}\r\nmap.handle = paddr;\r\nif (unlikely(copy_to_user(arg, &map, sizeof(map))))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int rio_mport_obw_free(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *md = priv->md;\r\nu64 handle;\r\nstruct rio_mport_mapping *map, *_map;\r\nif (!md->mport->ops->unmap_outb)\r\nreturn -EPROTONOSUPPORT;\r\nif (copy_from_user(&handle, arg, sizeof(handle)))\r\nreturn -EFAULT;\r\nrmcd_debug(OBW, "h=0x%llx", handle);\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry_safe(map, _map, &md->mappings, node) {\r\nif (map->dir == MAP_OUTBOUND && map->phys_addr == handle) {\r\nif (map->filp == filp) {\r\nrmcd_debug(OBW, "kref_put h=0x%llx", handle);\r\nmap->filp = NULL;\r\nkref_put(&map->ref, mport_release_mapping);\r\n}\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&md->buf_mutex);\r\nreturn 0;\r\n}\r\nstatic int maint_hdid_set(struct mport_cdev_priv *priv, void __user *arg)\r\n{\r\nstruct mport_dev *md = priv->md;\r\nu16 hdid;\r\nif (copy_from_user(&hdid, arg, sizeof(hdid)))\r\nreturn -EFAULT;\r\nmd->mport->host_deviceid = hdid;\r\nmd->properties.hdid = hdid;\r\nrio_local_set_device_id(md->mport, hdid);\r\nrmcd_debug(MPORT, "Set host device Id to %d", hdid);\r\nreturn 0;\r\n}\r\nstatic int maint_comptag_set(struct mport_cdev_priv *priv, void __user *arg)\r\n{\r\nstruct mport_dev *md = priv->md;\r\nu32 comptag;\r\nif (copy_from_user(&comptag, arg, sizeof(comptag)))\r\nreturn -EFAULT;\r\nrio_local_write_config_32(md->mport, RIO_COMPONENT_TAG_CSR, comptag);\r\nrmcd_debug(MPORT, "Set host Component Tag to %d", comptag);\r\nreturn 0;\r\n}\r\nstatic void mport_release_def_dma(struct kref *dma_ref)\r\n{\r\nstruct mport_dev *md =\r\ncontainer_of(dma_ref, struct mport_dev, dma_ref);\r\nrmcd_debug(EXIT, "DMA_%d", md->dma_chan->chan_id);\r\nrio_release_dma(md->dma_chan);\r\nmd->dma_chan = NULL;\r\n}\r\nstatic void mport_release_dma(struct kref *dma_ref)\r\n{\r\nstruct mport_cdev_priv *priv =\r\ncontainer_of(dma_ref, struct mport_cdev_priv, dma_ref);\r\nrmcd_debug(EXIT, "DMA_%d", priv->dmach->chan_id);\r\ncomplete(&priv->comp);\r\n}\r\nstatic void dma_req_free(struct mport_dma_req *req)\r\n{\r\nstruct mport_cdev_priv *priv = req->priv;\r\nunsigned int i;\r\ndma_unmap_sg(req->dmach->device->dev,\r\nreq->sgt.sgl, req->sgt.nents, req->dir);\r\nsg_free_table(&req->sgt);\r\nif (req->page_list) {\r\nfor (i = 0; i < req->nr_pages; i++)\r\nput_page(req->page_list[i]);\r\nkfree(req->page_list);\r\n}\r\nif (req->map) {\r\nmutex_lock(&req->map->md->buf_mutex);\r\nkref_put(&req->map->ref, mport_release_mapping);\r\nmutex_unlock(&req->map->md->buf_mutex);\r\n}\r\nkref_put(&priv->dma_ref, mport_release_dma);\r\nkfree(req);\r\n}\r\nstatic void dma_xfer_callback(void *param)\r\n{\r\nstruct mport_dma_req *req = (struct mport_dma_req *)param;\r\nstruct mport_cdev_priv *priv = req->priv;\r\nreq->status = dma_async_is_tx_complete(priv->dmach, req->cookie,\r\nNULL, NULL);\r\ncomplete(&req->req_comp);\r\n}\r\nstatic void dma_faf_cleanup(struct work_struct *_work)\r\n{\r\nstruct mport_faf_work *work = container_of(_work,\r\nstruct mport_faf_work, work);\r\nstruct mport_dma_req *req = work->req;\r\ndma_req_free(req);\r\nkfree(work);\r\n}\r\nstatic void dma_faf_callback(void *param)\r\n{\r\nstruct mport_dma_req *req = (struct mport_dma_req *)param;\r\nstruct mport_faf_work *work;\r\nwork = kmalloc(sizeof(*work), GFP_ATOMIC);\r\nif (!work)\r\nreturn;\r\nINIT_WORK(&work->work, dma_faf_cleanup);\r\nwork->req = req;\r\nqueue_work(dma_wq, &work->work);\r\n}\r\nstatic struct dma_async_tx_descriptor\r\n*prep_dma_xfer(struct dma_chan *chan, struct rio_transfer_io *transfer,\r\nstruct sg_table *sgt, int nents, enum dma_transfer_direction dir,\r\nenum dma_ctrl_flags flags)\r\n{\r\nstruct rio_dma_data tx_data;\r\ntx_data.sg = sgt->sgl;\r\ntx_data.sg_len = nents;\r\ntx_data.rio_addr_u = 0;\r\ntx_data.rio_addr = transfer->rio_addr;\r\nif (dir == DMA_MEM_TO_DEV) {\r\nswitch (transfer->method) {\r\ncase RIO_EXCHANGE_NWRITE:\r\ntx_data.wr_type = RDW_ALL_NWRITE;\r\nbreak;\r\ncase RIO_EXCHANGE_NWRITE_R_ALL:\r\ntx_data.wr_type = RDW_ALL_NWRITE_R;\r\nbreak;\r\ncase RIO_EXCHANGE_NWRITE_R:\r\ntx_data.wr_type = RDW_LAST_NWRITE_R;\r\nbreak;\r\ncase RIO_EXCHANGE_DEFAULT:\r\ntx_data.wr_type = RDW_DEFAULT;\r\nbreak;\r\ndefault:\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\n}\r\nreturn rio_dma_prep_xfer(chan, transfer->rioid, &tx_data, dir, flags);\r\n}\r\nstatic int get_dma_channel(struct mport_cdev_priv *priv)\r\n{\r\nmutex_lock(&priv->dma_lock);\r\nif (!priv->dmach) {\r\npriv->dmach = rio_request_mport_dma(priv->md->mport);\r\nif (!priv->dmach) {\r\nif (priv->md->dma_chan) {\r\npriv->dmach = priv->md->dma_chan;\r\nkref_get(&priv->md->dma_ref);\r\n} else {\r\nrmcd_error("Failed to get DMA channel");\r\nmutex_unlock(&priv->dma_lock);\r\nreturn -ENODEV;\r\n}\r\n} else if (!priv->md->dma_chan) {\r\npriv->md->dma_chan = priv->dmach;\r\nkref_init(&priv->md->dma_ref);\r\nrmcd_debug(DMA, "Register DMA_chan %d as default",\r\npriv->dmach->chan_id);\r\n}\r\nkref_init(&priv->dma_ref);\r\ninit_completion(&priv->comp);\r\n}\r\nkref_get(&priv->dma_ref);\r\nmutex_unlock(&priv->dma_lock);\r\nreturn 0;\r\n}\r\nstatic void put_dma_channel(struct mport_cdev_priv *priv)\r\n{\r\nkref_put(&priv->dma_ref, mport_release_dma);\r\n}\r\nstatic int do_dma_request(struct mport_dma_req *req,\r\nstruct rio_transfer_io *xfer,\r\nenum rio_transfer_sync sync, int nents)\r\n{\r\nstruct mport_cdev_priv *priv;\r\nstruct sg_table *sgt;\r\nstruct dma_chan *chan;\r\nstruct dma_async_tx_descriptor *tx;\r\ndma_cookie_t cookie;\r\nunsigned long tmo = msecs_to_jiffies(dma_timeout);\r\nenum dma_transfer_direction dir;\r\nlong wret;\r\nint ret = 0;\r\npriv = req->priv;\r\nsgt = &req->sgt;\r\nchan = priv->dmach;\r\ndir = (req->dir == DMA_FROM_DEVICE) ? DMA_DEV_TO_MEM : DMA_MEM_TO_DEV;\r\nrmcd_debug(DMA, "%s(%d) uses %s for DMA_%s",\r\ncurrent->comm, task_pid_nr(current),\r\ndev_name(&chan->dev->device),\r\n(dir == DMA_DEV_TO_MEM)?"READ":"WRITE");\r\ntx = prep_dma_xfer(chan, xfer, sgt, nents, dir,\r\nDMA_CTRL_ACK | DMA_PREP_INTERRUPT);\r\nif (!tx) {\r\nrmcd_debug(DMA, "prep error for %s A:0x%llx L:0x%llx",\r\n(dir == DMA_DEV_TO_MEM)?"READ":"WRITE",\r\nxfer->rio_addr, xfer->length);\r\nret = -EIO;\r\ngoto err_out;\r\n} else if (IS_ERR(tx)) {\r\nret = PTR_ERR(tx);\r\nrmcd_debug(DMA, "prep error %d for %s A:0x%llx L:0x%llx", ret,\r\n(dir == DMA_DEV_TO_MEM)?"READ":"WRITE",\r\nxfer->rio_addr, xfer->length);\r\ngoto err_out;\r\n}\r\nif (sync == RIO_TRANSFER_FAF)\r\ntx->callback = dma_faf_callback;\r\nelse\r\ntx->callback = dma_xfer_callback;\r\ntx->callback_param = req;\r\nreq->dmach = chan;\r\nreq->sync = sync;\r\nreq->status = DMA_IN_PROGRESS;\r\ninit_completion(&req->req_comp);\r\ncookie = dmaengine_submit(tx);\r\nreq->cookie = cookie;\r\nrmcd_debug(DMA, "pid=%d DMA_%s tx_cookie = %d", task_pid_nr(current),\r\n(dir == DMA_DEV_TO_MEM)?"READ":"WRITE", cookie);\r\nif (dma_submit_error(cookie)) {\r\nrmcd_error("submit err=%d (addr:0x%llx len:0x%llx)",\r\ncookie, xfer->rio_addr, xfer->length);\r\nret = -EIO;\r\ngoto err_out;\r\n}\r\ndma_async_issue_pending(chan);\r\nif (sync == RIO_TRANSFER_ASYNC) {\r\nspin_lock(&priv->req_lock);\r\nlist_add_tail(&req->node, &priv->async_list);\r\nspin_unlock(&priv->req_lock);\r\nreturn cookie;\r\n} else if (sync == RIO_TRANSFER_FAF)\r\nreturn 0;\r\nwret = wait_for_completion_interruptible_timeout(&req->req_comp, tmo);\r\nif (wret == 0) {\r\nrmcd_error("%s(%d) timed out waiting for DMA_%s %d",\r\ncurrent->comm, task_pid_nr(current),\r\n(dir == DMA_DEV_TO_MEM)?"READ":"WRITE", cookie);\r\nreturn -ETIMEDOUT;\r\n} else if (wret == -ERESTARTSYS) {\r\nrmcd_error("%s(%d) wait for DMA_%s %d was interrupted",\r\ncurrent->comm, task_pid_nr(current),\r\n(dir == DMA_DEV_TO_MEM)?"READ":"WRITE", cookie);\r\nreturn -EINTR;\r\n}\r\nif (req->status != DMA_COMPLETE) {\r\nrmcd_error("%s(%d) DMA_%s %d completed with status %d (ret=%d)",\r\ncurrent->comm, task_pid_nr(current),\r\n(dir == DMA_DEV_TO_MEM)?"READ":"WRITE",\r\ncookie, req->status, ret);\r\nret = -EIO;\r\n}\r\nerr_out:\r\nreturn ret;\r\n}\r\nstatic int\r\nrio_dma_transfer(struct file *filp, u32 transfer_mode,\r\nenum rio_transfer_sync sync, enum dma_data_direction dir,\r\nstruct rio_transfer_io *xfer)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nunsigned long nr_pages = 0;\r\nstruct page **page_list = NULL;\r\nstruct mport_dma_req *req;\r\nstruct mport_dev *md = priv->md;\r\nstruct dma_chan *chan;\r\nint i, ret;\r\nint nents;\r\nif (xfer->length == 0)\r\nreturn -EINVAL;\r\nreq = kzalloc(sizeof(*req), GFP_KERNEL);\r\nif (!req)\r\nreturn -ENOMEM;\r\nret = get_dma_channel(priv);\r\nif (ret) {\r\nkfree(req);\r\nreturn ret;\r\n}\r\nif (xfer->loc_addr) {\r\nunsigned long offset;\r\nlong pinned;\r\noffset = (unsigned long)(uintptr_t)xfer->loc_addr & ~PAGE_MASK;\r\nnr_pages = PAGE_ALIGN(xfer->length + offset) >> PAGE_SHIFT;\r\npage_list = kmalloc_array(nr_pages,\r\nsizeof(*page_list), GFP_KERNEL);\r\nif (page_list == NULL) {\r\nret = -ENOMEM;\r\ngoto err_req;\r\n}\r\ndown_read(&current->mm->mmap_sem);\r\npinned = get_user_pages(\r\n(unsigned long)xfer->loc_addr & PAGE_MASK,\r\nnr_pages, dir == DMA_FROM_DEVICE, 0,\r\npage_list, NULL);\r\nup_read(&current->mm->mmap_sem);\r\nif (pinned != nr_pages) {\r\nif (pinned < 0) {\r\nrmcd_error("get_user_pages err=%ld", pinned);\r\nnr_pages = 0;\r\n} else\r\nrmcd_error("pinned %ld out of %ld pages",\r\npinned, nr_pages);\r\nret = -EFAULT;\r\ngoto err_pg;\r\n}\r\nret = sg_alloc_table_from_pages(&req->sgt, page_list, nr_pages,\r\noffset, xfer->length, GFP_KERNEL);\r\nif (ret) {\r\nrmcd_error("sg_alloc_table failed with err=%d", ret);\r\ngoto err_pg;\r\n}\r\nreq->page_list = page_list;\r\nreq->nr_pages = nr_pages;\r\n} else {\r\ndma_addr_t baddr;\r\nstruct rio_mport_mapping *map;\r\nbaddr = (dma_addr_t)xfer->handle;\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry(map, &md->mappings, node) {\r\nif (baddr >= map->phys_addr &&\r\nbaddr < (map->phys_addr + map->size)) {\r\nkref_get(&map->ref);\r\nreq->map = map;\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&md->buf_mutex);\r\nif (req->map == NULL) {\r\nret = -ENOMEM;\r\ngoto err_req;\r\n}\r\nif (xfer->length + xfer->offset > map->size) {\r\nret = -EINVAL;\r\ngoto err_req;\r\n}\r\nret = sg_alloc_table(&req->sgt, 1, GFP_KERNEL);\r\nif (unlikely(ret)) {\r\nrmcd_error("sg_alloc_table failed for internal buf");\r\ngoto err_req;\r\n}\r\nsg_set_buf(req->sgt.sgl,\r\nmap->virt_addr + (baddr - map->phys_addr) +\r\nxfer->offset, xfer->length);\r\n}\r\nreq->dir = dir;\r\nreq->filp = filp;\r\nreq->priv = priv;\r\nchan = priv->dmach;\r\nnents = dma_map_sg(chan->device->dev,\r\nreq->sgt.sgl, req->sgt.nents, dir);\r\nif (nents == -EFAULT) {\r\nrmcd_error("Failed to map SG list");\r\nreturn -EFAULT;\r\n}\r\nret = do_dma_request(req, xfer, sync, nents);\r\nif (ret >= 0) {\r\nif (sync == RIO_TRANSFER_SYNC)\r\ngoto sync_out;\r\nreturn ret;\r\n}\r\nif (ret == -ETIMEDOUT || ret == -EINTR) {\r\nspin_lock(&priv->req_lock);\r\nlist_add_tail(&req->node, &priv->pend_list);\r\nspin_unlock(&priv->req_lock);\r\nreturn ret;\r\n}\r\nrmcd_debug(DMA, "do_dma_request failed with err=%d", ret);\r\nsync_out:\r\ndma_unmap_sg(chan->device->dev, req->sgt.sgl, req->sgt.nents, dir);\r\nsg_free_table(&req->sgt);\r\nerr_pg:\r\nif (page_list) {\r\nfor (i = 0; i < nr_pages; i++)\r\nput_page(page_list[i]);\r\nkfree(page_list);\r\n}\r\nerr_req:\r\nif (req->map) {\r\nmutex_lock(&md->buf_mutex);\r\nkref_put(&req->map->ref, mport_release_mapping);\r\nmutex_unlock(&md->buf_mutex);\r\n}\r\nput_dma_channel(priv);\r\nkfree(req);\r\nreturn ret;\r\n}\r\nstatic int rio_mport_transfer_ioctl(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct rio_transaction transaction;\r\nstruct rio_transfer_io *transfer;\r\nenum dma_data_direction dir;\r\nint i, ret = 0;\r\nif (unlikely(copy_from_user(&transaction, arg, sizeof(transaction))))\r\nreturn -EFAULT;\r\nif (transaction.count != 1)\r\nreturn -EINVAL;\r\nif ((transaction.transfer_mode &\r\npriv->md->properties.transfer_mode) == 0)\r\nreturn -ENODEV;\r\ntransfer = vmalloc(transaction.count * sizeof(*transfer));\r\nif (!transfer)\r\nreturn -ENOMEM;\r\nif (unlikely(copy_from_user(transfer,\r\n(void __user *)(uintptr_t)transaction.block,\r\ntransaction.count * sizeof(*transfer)))) {\r\nret = -EFAULT;\r\ngoto out_free;\r\n}\r\ndir = (transaction.dir == RIO_TRANSFER_DIR_READ) ?\r\nDMA_FROM_DEVICE : DMA_TO_DEVICE;\r\nfor (i = 0; i < transaction.count && ret == 0; i++)\r\nret = rio_dma_transfer(filp, transaction.transfer_mode,\r\ntransaction.sync, dir, &transfer[i]);\r\nif (unlikely(copy_to_user((void __user *)(uintptr_t)transaction.block,\r\ntransfer,\r\ntransaction.count * sizeof(*transfer))))\r\nret = -EFAULT;\r\nout_free:\r\nvfree(transfer);\r\nreturn ret;\r\n}\r\nstatic int rio_mport_wait_for_async_dma(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv;\r\nstruct mport_dev *md;\r\nstruct rio_async_tx_wait w_param;\r\nstruct mport_dma_req *req;\r\ndma_cookie_t cookie;\r\nunsigned long tmo;\r\nlong wret;\r\nint found = 0;\r\nint ret;\r\npriv = (struct mport_cdev_priv *)filp->private_data;\r\nmd = priv->md;\r\nif (unlikely(copy_from_user(&w_param, arg, sizeof(w_param))))\r\nreturn -EFAULT;\r\ncookie = w_param.token;\r\nif (w_param.timeout)\r\ntmo = msecs_to_jiffies(w_param.timeout);\r\nelse\r\ntmo = msecs_to_jiffies(dma_timeout);\r\nspin_lock(&priv->req_lock);\r\nlist_for_each_entry(req, &priv->async_list, node) {\r\nif (req->cookie == cookie) {\r\nlist_del(&req->node);\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&priv->req_lock);\r\nif (!found)\r\nreturn -EAGAIN;\r\nwret = wait_for_completion_interruptible_timeout(&req->req_comp, tmo);\r\nif (wret == 0) {\r\nrmcd_error("%s(%d) timed out waiting for ASYNC DMA_%s",\r\ncurrent->comm, task_pid_nr(current),\r\n(req->dir == DMA_FROM_DEVICE)?"READ":"WRITE");\r\nret = -ETIMEDOUT;\r\ngoto err_tmo;\r\n} else if (wret == -ERESTARTSYS) {\r\nrmcd_error("%s(%d) wait for ASYNC DMA_%s was interrupted",\r\ncurrent->comm, task_pid_nr(current),\r\n(req->dir == DMA_FROM_DEVICE)?"READ":"WRITE");\r\nret = -EINTR;\r\ngoto err_tmo;\r\n}\r\nif (req->status != DMA_COMPLETE) {\r\nrmcd_error("%s(%d) ASYNC DMA_%s completion with status %d",\r\ncurrent->comm, task_pid_nr(current),\r\n(req->dir == DMA_FROM_DEVICE)?"READ":"WRITE",\r\nreq->status);\r\nret = -EIO;\r\n} else\r\nret = 0;\r\nif (req->status != DMA_IN_PROGRESS && req->status != DMA_PAUSED)\r\ndma_req_free(req);\r\nreturn ret;\r\nerr_tmo:\r\nspin_lock(&priv->req_lock);\r\nlist_add_tail(&req->node, &priv->async_list);\r\nspin_unlock(&priv->req_lock);\r\nreturn ret;\r\n}\r\nstatic int rio_mport_create_dma_mapping(struct mport_dev *md, struct file *filp,\r\nu64 size, struct rio_mport_mapping **mapping)\r\n{\r\nstruct rio_mport_mapping *map;\r\nmap = kzalloc(sizeof(*map), GFP_KERNEL);\r\nif (map == NULL)\r\nreturn -ENOMEM;\r\nmap->virt_addr = dma_alloc_coherent(md->mport->dev.parent, size,\r\n&map->phys_addr, GFP_KERNEL);\r\nif (map->virt_addr == NULL) {\r\nkfree(map);\r\nreturn -ENOMEM;\r\n}\r\nmap->dir = MAP_DMA;\r\nmap->size = size;\r\nmap->filp = filp;\r\nmap->md = md;\r\nkref_init(&map->ref);\r\nmutex_lock(&md->buf_mutex);\r\nlist_add_tail(&map->node, &md->mappings);\r\nmutex_unlock(&md->buf_mutex);\r\n*mapping = map;\r\nreturn 0;\r\n}\r\nstatic int rio_mport_alloc_dma(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *md = priv->md;\r\nstruct rio_dma_mem map;\r\nstruct rio_mport_mapping *mapping = NULL;\r\nint ret;\r\nif (unlikely(copy_from_user(&map, arg, sizeof(map))))\r\nreturn -EFAULT;\r\nret = rio_mport_create_dma_mapping(md, filp, map.length, &mapping);\r\nif (ret)\r\nreturn ret;\r\nmap.dma_handle = mapping->phys_addr;\r\nif (unlikely(copy_to_user(arg, &map, sizeof(map)))) {\r\nmutex_lock(&md->buf_mutex);\r\nkref_put(&mapping->ref, mport_release_mapping);\r\nmutex_unlock(&md->buf_mutex);\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rio_mport_free_dma(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *md = priv->md;\r\nu64 handle;\r\nint ret = -EFAULT;\r\nstruct rio_mport_mapping *map, *_map;\r\nif (copy_from_user(&handle, arg, sizeof(handle)))\r\nreturn -EFAULT;\r\nrmcd_debug(EXIT, "filp=%p", filp);\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry_safe(map, _map, &md->mappings, node) {\r\nif (map->dir == MAP_DMA && map->phys_addr == handle &&\r\nmap->filp == filp) {\r\nkref_put(&map->ref, mport_release_mapping);\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&md->buf_mutex);\r\nif (ret == -EFAULT) {\r\nrmcd_debug(DMA, "ERR no matching mapping");\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rio_mport_transfer_ioctl(struct file *filp, void *arg)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic int rio_mport_wait_for_async_dma(struct file *filp, void __user *arg)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic int rio_mport_alloc_dma(struct file *filp, void __user *arg)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic int rio_mport_free_dma(struct file *filp, void __user *arg)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic int\r\nrio_mport_create_inbound_mapping(struct mport_dev *md, struct file *filp,\r\nu64 raddr, u64 size,\r\nstruct rio_mport_mapping **mapping)\r\n{\r\nstruct rio_mport *mport = md->mport;\r\nstruct rio_mport_mapping *map;\r\nint ret;\r\nif (size > 0xffffffff)\r\nreturn -EINVAL;\r\nmap = kzalloc(sizeof(*map), GFP_KERNEL);\r\nif (map == NULL)\r\nreturn -ENOMEM;\r\nmap->virt_addr = dma_alloc_coherent(mport->dev.parent, size,\r\n&map->phys_addr, GFP_KERNEL);\r\nif (map->virt_addr == NULL) {\r\nret = -ENOMEM;\r\ngoto err_dma_alloc;\r\n}\r\nif (raddr == RIO_MAP_ANY_ADDR)\r\nraddr = map->phys_addr;\r\nret = rio_map_inb_region(mport, map->phys_addr, raddr, (u32)size, 0);\r\nif (ret < 0)\r\ngoto err_map_inb;\r\nmap->dir = MAP_INBOUND;\r\nmap->rio_addr = raddr;\r\nmap->size = size;\r\nmap->filp = filp;\r\nmap->md = md;\r\nkref_init(&map->ref);\r\nmutex_lock(&md->buf_mutex);\r\nlist_add_tail(&map->node, &md->mappings);\r\nmutex_unlock(&md->buf_mutex);\r\n*mapping = map;\r\nreturn 0;\r\nerr_map_inb:\r\ndma_free_coherent(mport->dev.parent, size,\r\nmap->virt_addr, map->phys_addr);\r\nerr_dma_alloc:\r\nkfree(map);\r\nreturn ret;\r\n}\r\nstatic int\r\nrio_mport_get_inbound_mapping(struct mport_dev *md, struct file *filp,\r\nu64 raddr, u64 size,\r\nstruct rio_mport_mapping **mapping)\r\n{\r\nstruct rio_mport_mapping *map;\r\nint err = -ENOMEM;\r\nif (raddr == RIO_MAP_ANY_ADDR)\r\ngoto get_new;\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry(map, &md->mappings, node) {\r\nif (map->dir != MAP_INBOUND)\r\ncontinue;\r\nif (raddr == map->rio_addr && size == map->size) {\r\n*mapping = map;\r\nerr = 0;\r\nbreak;\r\n} else if (raddr < (map->rio_addr + map->size - 1) &&\r\n(raddr + size) > map->rio_addr) {\r\nerr = -EBUSY;\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&md->buf_mutex);\r\nif (err != -ENOMEM)\r\nreturn err;\r\nget_new:\r\nreturn rio_mport_create_inbound_mapping(md, filp, raddr, size, mapping);\r\n}\r\nstatic int rio_mport_map_inbound(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *md = priv->md;\r\nstruct rio_mmap map;\r\nstruct rio_mport_mapping *mapping = NULL;\r\nint ret;\r\nif (!md->mport->ops->map_inb)\r\nreturn -EPROTONOSUPPORT;\r\nif (unlikely(copy_from_user(&map, arg, sizeof(map))))\r\nreturn -EFAULT;\r\nrmcd_debug(IBW, "%s filp=%p", dev_name(&priv->md->dev), filp);\r\nret = rio_mport_get_inbound_mapping(md, filp, map.rio_addr,\r\nmap.length, &mapping);\r\nif (ret)\r\nreturn ret;\r\nmap.handle = mapping->phys_addr;\r\nmap.rio_addr = mapping->rio_addr;\r\nif (unlikely(copy_to_user(arg, &map, sizeof(map)))) {\r\nif (ret == 0 && mapping->filp == filp) {\r\nmutex_lock(&md->buf_mutex);\r\nkref_put(&mapping->ref, mport_release_mapping);\r\nmutex_unlock(&md->buf_mutex);\r\n}\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rio_mport_inbound_free(struct file *filp, void __user *arg)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *md = priv->md;\r\nu64 handle;\r\nstruct rio_mport_mapping *map, *_map;\r\nrmcd_debug(IBW, "%s filp=%p", dev_name(&priv->md->dev), filp);\r\nif (!md->mport->ops->unmap_inb)\r\nreturn -EPROTONOSUPPORT;\r\nif (copy_from_user(&handle, arg, sizeof(handle)))\r\nreturn -EFAULT;\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry_safe(map, _map, &md->mappings, node) {\r\nif (map->dir == MAP_INBOUND && map->phys_addr == handle) {\r\nif (map->filp == filp) {\r\nmap->filp = NULL;\r\nkref_put(&map->ref, mport_release_mapping);\r\n}\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&md->buf_mutex);\r\nreturn 0;\r\n}\r\nstatic int maint_port_idx_get(struct mport_cdev_priv *priv, void __user *arg)\r\n{\r\nstruct mport_dev *md = priv->md;\r\nu32 port_idx = md->mport->index;\r\nrmcd_debug(MPORT, "port_index=%d", port_idx);\r\nif (copy_to_user(arg, &port_idx, sizeof(port_idx)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int rio_mport_add_event(struct mport_cdev_priv *priv,\r\nstruct rio_event *event)\r\n{\r\nint overflow;\r\nif (!(priv->event_mask & event->header))\r\nreturn -EACCES;\r\nspin_lock(&priv->fifo_lock);\r\noverflow = kfifo_avail(&priv->event_fifo) < sizeof(*event)\r\n|| kfifo_in(&priv->event_fifo, (unsigned char *)event,\r\nsizeof(*event)) != sizeof(*event);\r\nspin_unlock(&priv->fifo_lock);\r\nwake_up_interruptible(&priv->event_rx_wait);\r\nif (overflow) {\r\ndev_warn(&priv->md->dev, DRV_NAME ": event fifo overflow\n");\r\nreturn -EBUSY;\r\n}\r\nreturn 0;\r\n}\r\nstatic void rio_mport_doorbell_handler(struct rio_mport *mport, void *dev_id,\r\nu16 src, u16 dst, u16 info)\r\n{\r\nstruct mport_dev *data = dev_id;\r\nstruct mport_cdev_priv *priv;\r\nstruct rio_mport_db_filter *db_filter;\r\nstruct rio_event event;\r\nint handled;\r\nevent.header = RIO_DOORBELL;\r\nevent.u.doorbell.rioid = src;\r\nevent.u.doorbell.payload = info;\r\nhandled = 0;\r\nspin_lock(&data->db_lock);\r\nlist_for_each_entry(db_filter, &data->doorbells, data_node) {\r\nif (((db_filter->filter.rioid == RIO_INVALID_DESTID ||\r\ndb_filter->filter.rioid == src)) &&\r\ninfo >= db_filter->filter.low &&\r\ninfo <= db_filter->filter.high) {\r\npriv = db_filter->priv;\r\nrio_mport_add_event(priv, &event);\r\nhandled = 1;\r\n}\r\n}\r\nspin_unlock(&data->db_lock);\r\nif (!handled)\r\ndev_warn(&data->dev,\r\n"%s: spurious DB received from 0x%x, info=0x%04x\n",\r\n__func__, src, info);\r\n}\r\nstatic int rio_mport_add_db_filter(struct mport_cdev_priv *priv,\r\nvoid __user *arg)\r\n{\r\nstruct mport_dev *md = priv->md;\r\nstruct rio_mport_db_filter *db_filter;\r\nstruct rio_doorbell_filter filter;\r\nunsigned long flags;\r\nint ret;\r\nif (copy_from_user(&filter, arg, sizeof(filter)))\r\nreturn -EFAULT;\r\nif (filter.low > filter.high)\r\nreturn -EINVAL;\r\nret = rio_request_inb_dbell(md->mport, md, filter.low, filter.high,\r\nrio_mport_doorbell_handler);\r\nif (ret) {\r\nrmcd_error("%s failed to register IBDB, err=%d",\r\ndev_name(&md->dev), ret);\r\nreturn ret;\r\n}\r\ndb_filter = kzalloc(sizeof(*db_filter), GFP_KERNEL);\r\nif (db_filter == NULL) {\r\nrio_release_inb_dbell(md->mport, filter.low, filter.high);\r\nreturn -ENOMEM;\r\n}\r\ndb_filter->filter = filter;\r\ndb_filter->priv = priv;\r\nspin_lock_irqsave(&md->db_lock, flags);\r\nlist_add_tail(&db_filter->priv_node, &priv->db_filters);\r\nlist_add_tail(&db_filter->data_node, &md->doorbells);\r\nspin_unlock_irqrestore(&md->db_lock, flags);\r\nreturn 0;\r\n}\r\nstatic void rio_mport_delete_db_filter(struct rio_mport_db_filter *db_filter)\r\n{\r\nlist_del(&db_filter->data_node);\r\nlist_del(&db_filter->priv_node);\r\nkfree(db_filter);\r\n}\r\nstatic int rio_mport_remove_db_filter(struct mport_cdev_priv *priv,\r\nvoid __user *arg)\r\n{\r\nstruct rio_mport_db_filter *db_filter;\r\nstruct rio_doorbell_filter filter;\r\nunsigned long flags;\r\nint ret = -EINVAL;\r\nif (copy_from_user(&filter, arg, sizeof(filter)))\r\nreturn -EFAULT;\r\nif (filter.low > filter.high)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&priv->md->db_lock, flags);\r\nlist_for_each_entry(db_filter, &priv->db_filters, priv_node) {\r\nif (db_filter->filter.rioid == filter.rioid &&\r\ndb_filter->filter.low == filter.low &&\r\ndb_filter->filter.high == filter.high) {\r\nrio_mport_delete_db_filter(db_filter);\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&priv->md->db_lock, flags);\r\nif (!ret)\r\nrio_release_inb_dbell(priv->md->mport, filter.low, filter.high);\r\nreturn ret;\r\n}\r\nstatic int rio_mport_match_pw(union rio_pw_msg *msg,\r\nstruct rio_pw_filter *filter)\r\n{\r\nif ((msg->em.comptag & filter->mask) < filter->low ||\r\n(msg->em.comptag & filter->mask) > filter->high)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int rio_mport_pw_handler(struct rio_mport *mport, void *context,\r\nunion rio_pw_msg *msg, int step)\r\n{\r\nstruct mport_dev *md = context;\r\nstruct mport_cdev_priv *priv;\r\nstruct rio_mport_pw_filter *pw_filter;\r\nstruct rio_event event;\r\nint handled;\r\nevent.header = RIO_PORTWRITE;\r\nmemcpy(event.u.portwrite.payload, msg->raw, RIO_PW_MSG_SIZE);\r\nhandled = 0;\r\nspin_lock(&md->pw_lock);\r\nlist_for_each_entry(pw_filter, &md->portwrites, md_node) {\r\nif (rio_mport_match_pw(msg, &pw_filter->filter)) {\r\npriv = pw_filter->priv;\r\nrio_mport_add_event(priv, &event);\r\nhandled = 1;\r\n}\r\n}\r\nspin_unlock(&md->pw_lock);\r\nif (!handled) {\r\nprintk_ratelimited(KERN_WARNING DRV_NAME\r\n": mport%d received spurious PW from 0x%08x\n",\r\nmport->id, msg->em.comptag);\r\n}\r\nreturn 0;\r\n}\r\nstatic int rio_mport_add_pw_filter(struct mport_cdev_priv *priv,\r\nvoid __user *arg)\r\n{\r\nstruct mport_dev *md = priv->md;\r\nstruct rio_mport_pw_filter *pw_filter;\r\nstruct rio_pw_filter filter;\r\nunsigned long flags;\r\nint hadd = 0;\r\nif (copy_from_user(&filter, arg, sizeof(filter)))\r\nreturn -EFAULT;\r\npw_filter = kzalloc(sizeof(*pw_filter), GFP_KERNEL);\r\nif (pw_filter == NULL)\r\nreturn -ENOMEM;\r\npw_filter->filter = filter;\r\npw_filter->priv = priv;\r\nspin_lock_irqsave(&md->pw_lock, flags);\r\nif (list_empty(&md->portwrites))\r\nhadd = 1;\r\nlist_add_tail(&pw_filter->priv_node, &priv->pw_filters);\r\nlist_add_tail(&pw_filter->md_node, &md->portwrites);\r\nspin_unlock_irqrestore(&md->pw_lock, flags);\r\nif (hadd) {\r\nint ret;\r\nret = rio_add_mport_pw_handler(md->mport, md,\r\nrio_mport_pw_handler);\r\nif (ret) {\r\ndev_err(&md->dev,\r\n"%s: failed to add IB_PW handler, err=%d\n",\r\n__func__, ret);\r\nreturn ret;\r\n}\r\nrio_pw_enable(md->mport, 1);\r\n}\r\nreturn 0;\r\n}\r\nstatic void rio_mport_delete_pw_filter(struct rio_mport_pw_filter *pw_filter)\r\n{\r\nlist_del(&pw_filter->md_node);\r\nlist_del(&pw_filter->priv_node);\r\nkfree(pw_filter);\r\n}\r\nstatic int rio_mport_match_pw_filter(struct rio_pw_filter *a,\r\nstruct rio_pw_filter *b)\r\n{\r\nif ((a->mask == b->mask) && (a->low == b->low) && (a->high == b->high))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int rio_mport_remove_pw_filter(struct mport_cdev_priv *priv,\r\nvoid __user *arg)\r\n{\r\nstruct mport_dev *md = priv->md;\r\nstruct rio_mport_pw_filter *pw_filter;\r\nstruct rio_pw_filter filter;\r\nunsigned long flags;\r\nint ret = -EINVAL;\r\nint hdel = 0;\r\nif (copy_from_user(&filter, arg, sizeof(filter)))\r\nreturn -EFAULT;\r\nspin_lock_irqsave(&md->pw_lock, flags);\r\nlist_for_each_entry(pw_filter, &priv->pw_filters, priv_node) {\r\nif (rio_mport_match_pw_filter(&pw_filter->filter, &filter)) {\r\nrio_mport_delete_pw_filter(pw_filter);\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\nif (list_empty(&md->portwrites))\r\nhdel = 1;\r\nspin_unlock_irqrestore(&md->pw_lock, flags);\r\nif (hdel) {\r\nrio_del_mport_pw_handler(md->mport, priv->md,\r\nrio_mport_pw_handler);\r\nrio_pw_enable(md->mport, 0);\r\n}\r\nreturn ret;\r\n}\r\nstatic void rio_release_dev(struct device *dev)\r\n{\r\nstruct rio_dev *rdev;\r\nrdev = to_rio_dev(dev);\r\npr_info(DRV_PREFIX "%s: %s\n", __func__, rio_name(rdev));\r\nkfree(rdev);\r\n}\r\nstatic void rio_release_net(struct device *dev)\r\n{\r\nstruct rio_net *net;\r\nnet = to_rio_net(dev);\r\nrmcd_debug(RDEV, "net_%d", net->id);\r\nkfree(net);\r\n}\r\nstatic int rio_mport_add_riodev(struct mport_cdev_priv *priv,\r\nvoid __user *arg)\r\n{\r\nstruct mport_dev *md = priv->md;\r\nstruct rio_rdev_info dev_info;\r\nstruct rio_dev *rdev;\r\nstruct rio_switch *rswitch = NULL;\r\nstruct rio_mport *mport;\r\nsize_t size;\r\nu32 rval;\r\nu32 swpinfo = 0;\r\nu16 destid;\r\nu8 hopcount;\r\nint err;\r\nif (copy_from_user(&dev_info, arg, sizeof(dev_info)))\r\nreturn -EFAULT;\r\nrmcd_debug(RDEV, "name:%s ct:0x%x did:0x%x hc:0x%x", dev_info.name,\r\ndev_info.comptag, dev_info.destid, dev_info.hopcount);\r\nif (bus_find_device_by_name(&rio_bus_type, NULL, dev_info.name)) {\r\nrmcd_debug(RDEV, "device %s already exists", dev_info.name);\r\nreturn -EEXIST;\r\n}\r\nsize = sizeof(*rdev);\r\nmport = md->mport;\r\ndestid = dev_info.destid;\r\nhopcount = dev_info.hopcount;\r\nif (rio_mport_read_config_32(mport, destid, hopcount,\r\nRIO_PEF_CAR, &rval))\r\nreturn -EIO;\r\nif (rval & RIO_PEF_SWITCH) {\r\nrio_mport_read_config_32(mport, destid, hopcount,\r\nRIO_SWP_INFO_CAR, &swpinfo);\r\nsize += (RIO_GET_TOTAL_PORTS(swpinfo) *\r\nsizeof(rswitch->nextdev[0])) + sizeof(*rswitch);\r\n}\r\nrdev = kzalloc(size, GFP_KERNEL);\r\nif (rdev == NULL)\r\nreturn -ENOMEM;\r\nif (mport->net == NULL) {\r\nstruct rio_net *net;\r\nnet = rio_alloc_net(mport);\r\nif (!net) {\r\nerr = -ENOMEM;\r\nrmcd_debug(RDEV, "failed to allocate net object");\r\ngoto cleanup;\r\n}\r\nnet->id = mport->id;\r\nnet->hport = mport;\r\ndev_set_name(&net->dev, "rnet_%d", net->id);\r\nnet->dev.parent = &mport->dev;\r\nnet->dev.release = rio_release_net;\r\nerr = rio_add_net(net);\r\nif (err) {\r\nrmcd_debug(RDEV, "failed to register net, err=%d", err);\r\nkfree(net);\r\ngoto cleanup;\r\n}\r\n}\r\nrdev->net = mport->net;\r\nrdev->pef = rval;\r\nrdev->swpinfo = swpinfo;\r\nrio_mport_read_config_32(mport, destid, hopcount,\r\nRIO_DEV_ID_CAR, &rval);\r\nrdev->did = rval >> 16;\r\nrdev->vid = rval & 0xffff;\r\nrio_mport_read_config_32(mport, destid, hopcount, RIO_DEV_INFO_CAR,\r\n&rdev->device_rev);\r\nrio_mport_read_config_32(mport, destid, hopcount, RIO_ASM_ID_CAR,\r\n&rval);\r\nrdev->asm_did = rval >> 16;\r\nrdev->asm_vid = rval & 0xffff;\r\nrio_mport_read_config_32(mport, destid, hopcount, RIO_ASM_INFO_CAR,\r\n&rval);\r\nrdev->asm_rev = rval >> 16;\r\nif (rdev->pef & RIO_PEF_EXT_FEATURES) {\r\nrdev->efptr = rval & 0xffff;\r\nrdev->phys_efptr = rio_mport_get_physefb(mport, 0, destid,\r\nhopcount);\r\nrdev->em_efptr = rio_mport_get_feature(mport, 0, destid,\r\nhopcount, RIO_EFB_ERR_MGMNT);\r\n}\r\nrio_mport_read_config_32(mport, destid, hopcount, RIO_SRC_OPS_CAR,\r\n&rdev->src_ops);\r\nrio_mport_read_config_32(mport, destid, hopcount, RIO_DST_OPS_CAR,\r\n&rdev->dst_ops);\r\nrdev->comp_tag = dev_info.comptag;\r\nrdev->destid = destid;\r\nrdev->hopcount = hopcount;\r\nif (rdev->pef & RIO_PEF_SWITCH) {\r\nrswitch = rdev->rswitch;\r\nrswitch->route_table = NULL;\r\n}\r\nif (strlen(dev_info.name))\r\ndev_set_name(&rdev->dev, "%s", dev_info.name);\r\nelse if (rdev->pef & RIO_PEF_SWITCH)\r\ndev_set_name(&rdev->dev, "%02x:s:%04x", mport->id,\r\nrdev->comp_tag & RIO_CTAG_UDEVID);\r\nelse\r\ndev_set_name(&rdev->dev, "%02x:e:%04x", mport->id,\r\nrdev->comp_tag & RIO_CTAG_UDEVID);\r\nINIT_LIST_HEAD(&rdev->net_list);\r\nrdev->dev.parent = &mport->net->dev;\r\nrio_attach_device(rdev);\r\nrdev->dev.release = rio_release_dev;\r\nif (rdev->dst_ops & RIO_DST_OPS_DOORBELL)\r\nrio_init_dbell_res(&rdev->riores[RIO_DOORBELL_RESOURCE],\r\n0, 0xffff);\r\nerr = rio_add_device(rdev);\r\nif (err)\r\ngoto cleanup;\r\nrio_dev_get(rdev);\r\nreturn 0;\r\ncleanup:\r\nkfree(rdev);\r\nreturn err;\r\n}\r\nstatic int rio_mport_del_riodev(struct mport_cdev_priv *priv, void __user *arg)\r\n{\r\nstruct rio_rdev_info dev_info;\r\nstruct rio_dev *rdev = NULL;\r\nstruct device *dev;\r\nstruct rio_mport *mport;\r\nstruct rio_net *net;\r\nif (copy_from_user(&dev_info, arg, sizeof(dev_info)))\r\nreturn -EFAULT;\r\nmport = priv->md->mport;\r\nif (strlen(dev_info.name)) {\r\ndev = bus_find_device_by_name(&rio_bus_type, NULL,\r\ndev_info.name);\r\nif (dev)\r\nrdev = to_rio_dev(dev);\r\n} else {\r\ndo {\r\nrdev = rio_get_comptag(dev_info.comptag, rdev);\r\nif (rdev && rdev->dev.parent == &mport->net->dev &&\r\nrdev->destid == dev_info.destid &&\r\nrdev->hopcount == dev_info.hopcount)\r\nbreak;\r\n} while (rdev);\r\n}\r\nif (!rdev) {\r\nrmcd_debug(RDEV,\r\n"device name:%s ct:0x%x did:0x%x hc:0x%x not found",\r\ndev_info.name, dev_info.comptag, dev_info.destid,\r\ndev_info.hopcount);\r\nreturn -ENODEV;\r\n}\r\nnet = rdev->net;\r\nrio_dev_put(rdev);\r\nrio_del_device(rdev, RIO_DEVICE_SHUTDOWN);\r\nif (list_empty(&net->devices)) {\r\nrio_free_net(net);\r\nmport->net = NULL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mport_cdev_open(struct inode *inode, struct file *filp)\r\n{\r\nint ret;\r\nint minor = iminor(inode);\r\nstruct mport_dev *chdev;\r\nstruct mport_cdev_priv *priv;\r\nif (minor >= RIO_MAX_MPORTS) {\r\nrmcd_error("Invalid minor device number");\r\nreturn -EINVAL;\r\n}\r\nchdev = container_of(inode->i_cdev, struct mport_dev, cdev);\r\nrmcd_debug(INIT, "%s filp=%p", dev_name(&chdev->dev), filp);\r\nif (atomic_read(&chdev->active) == 0)\r\nreturn -ENODEV;\r\nget_device(&chdev->dev);\r\npriv = kzalloc(sizeof(*priv), GFP_KERNEL);\r\nif (!priv) {\r\nput_device(&chdev->dev);\r\nreturn -ENOMEM;\r\n}\r\npriv->md = chdev;\r\nmutex_lock(&chdev->file_mutex);\r\nlist_add_tail(&priv->list, &chdev->file_list);\r\nmutex_unlock(&chdev->file_mutex);\r\nINIT_LIST_HEAD(&priv->db_filters);\r\nINIT_LIST_HEAD(&priv->pw_filters);\r\nspin_lock_init(&priv->fifo_lock);\r\ninit_waitqueue_head(&priv->event_rx_wait);\r\nret = kfifo_alloc(&priv->event_fifo,\r\nsizeof(struct rio_event) * MPORT_EVENT_DEPTH,\r\nGFP_KERNEL);\r\nif (ret < 0) {\r\ndev_err(&chdev->dev, DRV_NAME ": kfifo_alloc failed\n");\r\nret = -ENOMEM;\r\ngoto err_fifo;\r\n}\r\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\r\nINIT_LIST_HEAD(&priv->async_list);\r\nINIT_LIST_HEAD(&priv->pend_list);\r\nspin_lock_init(&priv->req_lock);\r\nmutex_init(&priv->dma_lock);\r\n#endif\r\nfilp->private_data = priv;\r\ngoto out;\r\nerr_fifo:\r\nkfree(priv);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int mport_cdev_fasync(int fd, struct file *filp, int mode)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nreturn fasync_helper(fd, filp, mode, &priv->async_queue);\r\n}\r\nstatic void mport_cdev_release_dma(struct file *filp)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *md;\r\nstruct mport_dma_req *req, *req_next;\r\nunsigned long tmo = msecs_to_jiffies(dma_timeout);\r\nlong wret;\r\nLIST_HEAD(list);\r\nrmcd_debug(EXIT, "from filp=%p %s(%d)",\r\nfilp, current->comm, task_pid_nr(current));\r\nif (!priv->dmach) {\r\nrmcd_debug(EXIT, "No DMA channel for filp=%p", filp);\r\nreturn;\r\n}\r\nmd = priv->md;\r\nflush_workqueue(dma_wq);\r\nspin_lock(&priv->req_lock);\r\nif (!list_empty(&priv->async_list)) {\r\nrmcd_debug(EXIT, "async list not empty filp=%p %s(%d)",\r\nfilp, current->comm, task_pid_nr(current));\r\nlist_splice_init(&priv->async_list, &list);\r\n}\r\nspin_unlock(&priv->req_lock);\r\nif (!list_empty(&list)) {\r\nrmcd_debug(EXIT, "temp list not empty");\r\nlist_for_each_entry_safe(req, req_next, &list, node) {\r\nrmcd_debug(EXIT, "free req->filp=%p cookie=%d compl=%s",\r\nreq->filp, req->cookie,\r\ncompletion_done(&req->req_comp)?"yes":"no");\r\nlist_del(&req->node);\r\ndma_req_free(req);\r\n}\r\n}\r\nif (!list_empty(&priv->pend_list)) {\r\nrmcd_debug(EXIT, "Free pending DMA requests for filp=%p %s(%d)",\r\nfilp, current->comm, task_pid_nr(current));\r\nlist_for_each_entry_safe(req,\r\nreq_next, &priv->pend_list, node) {\r\nrmcd_debug(EXIT, "free req->filp=%p cookie=%d compl=%s",\r\nreq->filp, req->cookie,\r\ncompletion_done(&req->req_comp)?"yes":"no");\r\nlist_del(&req->node);\r\ndma_req_free(req);\r\n}\r\n}\r\nput_dma_channel(priv);\r\nwret = wait_for_completion_interruptible_timeout(&priv->comp, tmo);\r\nif (wret <= 0) {\r\nrmcd_error("%s(%d) failed waiting for DMA release err=%ld",\r\ncurrent->comm, task_pid_nr(current), wret);\r\n}\r\nspin_lock(&priv->req_lock);\r\nif (!list_empty(&priv->pend_list)) {\r\nrmcd_debug(EXIT, "ATTN: pending DMA requests, filp=%p %s(%d)",\r\nfilp, current->comm, task_pid_nr(current));\r\n}\r\nspin_unlock(&priv->req_lock);\r\nif (priv->dmach != priv->md->dma_chan) {\r\nrmcd_debug(EXIT, "Release DMA channel for filp=%p %s(%d)",\r\nfilp, current->comm, task_pid_nr(current));\r\nrio_release_dma(priv->dmach);\r\n} else {\r\nrmcd_debug(EXIT, "Adjust default DMA channel refcount");\r\nkref_put(&md->dma_ref, mport_release_def_dma);\r\n}\r\npriv->dmach = NULL;\r\n}\r\nstatic int mport_cdev_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *chdev;\r\nstruct rio_mport_pw_filter *pw_filter, *pw_filter_next;\r\nstruct rio_mport_db_filter *db_filter, *db_filter_next;\r\nstruct rio_mport_mapping *map, *_map;\r\nunsigned long flags;\r\nrmcd_debug(EXIT, "%s filp=%p", dev_name(&priv->md->dev), filp);\r\nchdev = priv->md;\r\nmport_cdev_release_dma(filp);\r\npriv->event_mask = 0;\r\nspin_lock_irqsave(&chdev->pw_lock, flags);\r\nif (!list_empty(&priv->pw_filters)) {\r\nlist_for_each_entry_safe(pw_filter, pw_filter_next,\r\n&priv->pw_filters, priv_node)\r\nrio_mport_delete_pw_filter(pw_filter);\r\n}\r\nspin_unlock_irqrestore(&chdev->pw_lock, flags);\r\nspin_lock_irqsave(&chdev->db_lock, flags);\r\nlist_for_each_entry_safe(db_filter, db_filter_next,\r\n&priv->db_filters, priv_node) {\r\nrio_mport_delete_db_filter(db_filter);\r\n}\r\nspin_unlock_irqrestore(&chdev->db_lock, flags);\r\nkfifo_free(&priv->event_fifo);\r\nmutex_lock(&chdev->buf_mutex);\r\nlist_for_each_entry_safe(map, _map, &chdev->mappings, node) {\r\nif (map->filp == filp) {\r\nrmcd_debug(EXIT, "release mapping %p filp=%p",\r\nmap->virt_addr, filp);\r\nkref_put(&map->ref, mport_release_mapping);\r\n}\r\n}\r\nmutex_unlock(&chdev->buf_mutex);\r\nmport_cdev_fasync(-1, filp, 0);\r\nfilp->private_data = NULL;\r\nmutex_lock(&chdev->file_mutex);\r\nlist_del(&priv->list);\r\nmutex_unlock(&chdev->file_mutex);\r\nput_device(&chdev->dev);\r\nkfree(priv);\r\nreturn 0;\r\n}\r\nstatic long mport_cdev_ioctl(struct file *filp,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nint err = -EINVAL;\r\nstruct mport_cdev_priv *data = filp->private_data;\r\nstruct mport_dev *md = data->md;\r\nif (atomic_read(&md->active) == 0)\r\nreturn -ENODEV;\r\nswitch (cmd) {\r\ncase RIO_MPORT_MAINT_READ_LOCAL:\r\nreturn rio_mport_maint_rd(data, (void __user *)arg, 1);\r\ncase RIO_MPORT_MAINT_WRITE_LOCAL:\r\nreturn rio_mport_maint_wr(data, (void __user *)arg, 1);\r\ncase RIO_MPORT_MAINT_READ_REMOTE:\r\nreturn rio_mport_maint_rd(data, (void __user *)arg, 0);\r\ncase RIO_MPORT_MAINT_WRITE_REMOTE:\r\nreturn rio_mport_maint_wr(data, (void __user *)arg, 0);\r\ncase RIO_MPORT_MAINT_HDID_SET:\r\nreturn maint_hdid_set(data, (void __user *)arg);\r\ncase RIO_MPORT_MAINT_COMPTAG_SET:\r\nreturn maint_comptag_set(data, (void __user *)arg);\r\ncase RIO_MPORT_MAINT_PORT_IDX_GET:\r\nreturn maint_port_idx_get(data, (void __user *)arg);\r\ncase RIO_MPORT_GET_PROPERTIES:\r\nmd->properties.hdid = md->mport->host_deviceid;\r\nif (copy_to_user((void __user *)arg, &(md->properties),\r\nsizeof(md->properties)))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase RIO_ENABLE_DOORBELL_RANGE:\r\nreturn rio_mport_add_db_filter(data, (void __user *)arg);\r\ncase RIO_DISABLE_DOORBELL_RANGE:\r\nreturn rio_mport_remove_db_filter(data, (void __user *)arg);\r\ncase RIO_ENABLE_PORTWRITE_RANGE:\r\nreturn rio_mport_add_pw_filter(data, (void __user *)arg);\r\ncase RIO_DISABLE_PORTWRITE_RANGE:\r\nreturn rio_mport_remove_pw_filter(data, (void __user *)arg);\r\ncase RIO_SET_EVENT_MASK:\r\ndata->event_mask = (u32)arg;\r\nreturn 0;\r\ncase RIO_GET_EVENT_MASK:\r\nif (copy_to_user((void __user *)arg, &data->event_mask,\r\nsizeof(u32)))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase RIO_MAP_OUTBOUND:\r\nreturn rio_mport_obw_map(filp, (void __user *)arg);\r\ncase RIO_MAP_INBOUND:\r\nreturn rio_mport_map_inbound(filp, (void __user *)arg);\r\ncase RIO_UNMAP_OUTBOUND:\r\nreturn rio_mport_obw_free(filp, (void __user *)arg);\r\ncase RIO_UNMAP_INBOUND:\r\nreturn rio_mport_inbound_free(filp, (void __user *)arg);\r\ncase RIO_ALLOC_DMA:\r\nreturn rio_mport_alloc_dma(filp, (void __user *)arg);\r\ncase RIO_FREE_DMA:\r\nreturn rio_mport_free_dma(filp, (void __user *)arg);\r\ncase RIO_WAIT_FOR_ASYNC:\r\nreturn rio_mport_wait_for_async_dma(filp, (void __user *)arg);\r\ncase RIO_TRANSFER:\r\nreturn rio_mport_transfer_ioctl(filp, (void __user *)arg);\r\ncase RIO_DEV_ADD:\r\nreturn rio_mport_add_riodev(data, (void __user *)arg);\r\ncase RIO_DEV_DEL:\r\nreturn rio_mport_del_riodev(data, (void __user *)arg);\r\ndefault:\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic void mport_release_mapping(struct kref *ref)\r\n{\r\nstruct rio_mport_mapping *map =\r\ncontainer_of(ref, struct rio_mport_mapping, ref);\r\nstruct rio_mport *mport = map->md->mport;\r\nrmcd_debug(MMAP, "type %d mapping @ %p (phys = %pad) for %s",\r\nmap->dir, map->virt_addr,\r\n&map->phys_addr, mport->name);\r\nlist_del(&map->node);\r\nswitch (map->dir) {\r\ncase MAP_INBOUND:\r\nrio_unmap_inb_region(mport, map->phys_addr);\r\ncase MAP_DMA:\r\ndma_free_coherent(mport->dev.parent, map->size,\r\nmap->virt_addr, map->phys_addr);\r\nbreak;\r\ncase MAP_OUTBOUND:\r\nrio_unmap_outb_region(mport, map->rioid, map->rio_addr);\r\nbreak;\r\n}\r\nkfree(map);\r\n}\r\nstatic void mport_mm_open(struct vm_area_struct *vma)\r\n{\r\nstruct rio_mport_mapping *map = vma->vm_private_data;\r\nrmcd_debug(MMAP, "0x%pad", &map->phys_addr);\r\nkref_get(&map->ref);\r\n}\r\nstatic void mport_mm_close(struct vm_area_struct *vma)\r\n{\r\nstruct rio_mport_mapping *map = vma->vm_private_data;\r\nrmcd_debug(MMAP, "0x%pad", &map->phys_addr);\r\nmutex_lock(&map->md->buf_mutex);\r\nkref_put(&map->ref, mport_release_mapping);\r\nmutex_unlock(&map->md->buf_mutex);\r\n}\r\nstatic int mport_cdev_mmap(struct file *filp, struct vm_area_struct *vma)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct mport_dev *md;\r\nsize_t size = vma->vm_end - vma->vm_start;\r\ndma_addr_t baddr;\r\nunsigned long offset;\r\nint found = 0, ret;\r\nstruct rio_mport_mapping *map;\r\nrmcd_debug(MMAP, "0x%x bytes at offset 0x%lx",\r\n(unsigned int)size, vma->vm_pgoff);\r\nmd = priv->md;\r\nbaddr = ((dma_addr_t)vma->vm_pgoff << PAGE_SHIFT);\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry(map, &md->mappings, node) {\r\nif (baddr >= map->phys_addr &&\r\nbaddr < (map->phys_addr + map->size)) {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&md->buf_mutex);\r\nif (!found)\r\nreturn -ENOMEM;\r\noffset = baddr - map->phys_addr;\r\nif (size + offset > map->size)\r\nreturn -EINVAL;\r\nvma->vm_pgoff = offset >> PAGE_SHIFT;\r\nrmcd_debug(MMAP, "MMAP adjusted offset = 0x%lx", vma->vm_pgoff);\r\nif (map->dir == MAP_INBOUND || map->dir == MAP_DMA)\r\nret = dma_mmap_coherent(md->mport->dev.parent, vma,\r\nmap->virt_addr, map->phys_addr, map->size);\r\nelse if (map->dir == MAP_OUTBOUND) {\r\nvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\r\nret = vm_iomap_memory(vma, map->phys_addr, map->size);\r\n} else {\r\nrmcd_error("Attempt to mmap unsupported mapping type");\r\nret = -EIO;\r\n}\r\nif (!ret) {\r\nvma->vm_private_data = map;\r\nvma->vm_ops = &vm_ops;\r\nmport_mm_open(vma);\r\n} else {\r\nrmcd_error("MMAP exit with err=%d", ret);\r\n}\r\nreturn ret;\r\n}\r\nstatic unsigned int mport_cdev_poll(struct file *filp, poll_table *wait)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\npoll_wait(filp, &priv->event_rx_wait, wait);\r\nif (kfifo_len(&priv->event_fifo))\r\nreturn POLLIN | POLLRDNORM;\r\nreturn 0;\r\n}\r\nstatic ssize_t mport_read(struct file *filp, char __user *buf, size_t count,\r\nloff_t *ppos)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nint copied;\r\nssize_t ret;\r\nif (!count)\r\nreturn 0;\r\nif (kfifo_is_empty(&priv->event_fifo) &&\r\n(filp->f_flags & O_NONBLOCK))\r\nreturn -EAGAIN;\r\nif (count % sizeof(struct rio_event))\r\nreturn -EINVAL;\r\nret = wait_event_interruptible(priv->event_rx_wait,\r\nkfifo_len(&priv->event_fifo) != 0);\r\nif (ret)\r\nreturn ret;\r\nwhile (ret < count) {\r\nif (kfifo_to_user(&priv->event_fifo, buf,\r\nsizeof(struct rio_event), &copied))\r\nreturn -EFAULT;\r\nret += copied;\r\nbuf += copied;\r\n}\r\nreturn ret;\r\n}\r\nstatic ssize_t mport_write(struct file *filp, const char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct mport_cdev_priv *priv = filp->private_data;\r\nstruct rio_mport *mport = priv->md->mport;\r\nstruct rio_event event;\r\nint len, ret;\r\nif (!count)\r\nreturn 0;\r\nif (count % sizeof(event))\r\nreturn -EINVAL;\r\nlen = 0;\r\nwhile ((count - len) >= (int)sizeof(event)) {\r\nif (copy_from_user(&event, buf, sizeof(event)))\r\nreturn -EFAULT;\r\nif (event.header != RIO_DOORBELL)\r\nreturn -EINVAL;\r\nret = rio_mport_send_doorbell(mport,\r\nevent.u.doorbell.rioid,\r\nevent.u.doorbell.payload);\r\nif (ret < 0)\r\nreturn ret;\r\nlen += sizeof(event);\r\nbuf += sizeof(event);\r\n}\r\nreturn len;\r\n}\r\nstatic void mport_device_release(struct device *dev)\r\n{\r\nstruct mport_dev *md;\r\nrmcd_debug(EXIT, "%s", dev_name(dev));\r\nmd = container_of(dev, struct mport_dev, dev);\r\nkfree(md);\r\n}\r\nstatic struct mport_dev *mport_cdev_add(struct rio_mport *mport)\r\n{\r\nint ret = 0;\r\nstruct mport_dev *md;\r\nstruct rio_mport_attr attr;\r\nmd = kzalloc(sizeof(*md), GFP_KERNEL);\r\nif (!md) {\r\nrmcd_error("Unable allocate a device object");\r\nreturn NULL;\r\n}\r\nmd->mport = mport;\r\nmutex_init(&md->buf_mutex);\r\nmutex_init(&md->file_mutex);\r\nINIT_LIST_HEAD(&md->file_list);\r\ncdev_init(&md->cdev, &mport_fops);\r\nmd->cdev.owner = THIS_MODULE;\r\nret = cdev_add(&md->cdev, MKDEV(MAJOR(dev_number), mport->id), 1);\r\nif (ret < 0) {\r\nkfree(md);\r\nrmcd_error("Unable to register a device, err=%d", ret);\r\nreturn NULL;\r\n}\r\nmd->dev.devt = md->cdev.dev;\r\nmd->dev.class = dev_class;\r\nmd->dev.parent = &mport->dev;\r\nmd->dev.release = mport_device_release;\r\ndev_set_name(&md->dev, DEV_NAME "%d", mport->id);\r\natomic_set(&md->active, 1);\r\nret = device_register(&md->dev);\r\nif (ret) {\r\nrmcd_error("Failed to register mport %d (err=%d)",\r\nmport->id, ret);\r\ngoto err_cdev;\r\n}\r\nget_device(&md->dev);\r\nINIT_LIST_HEAD(&md->doorbells);\r\nspin_lock_init(&md->db_lock);\r\nINIT_LIST_HEAD(&md->portwrites);\r\nspin_lock_init(&md->pw_lock);\r\nINIT_LIST_HEAD(&md->mappings);\r\nmd->properties.id = mport->id;\r\nmd->properties.sys_size = mport->sys_size;\r\nmd->properties.hdid = mport->host_deviceid;\r\nmd->properties.index = mport->index;\r\n#ifdef CONFIG_FSL_RIO\r\nmd->properties.transfer_mode |= RIO_TRANSFER_MODE_MAPPED;\r\n#else\r\nmd->properties.transfer_mode |= RIO_TRANSFER_MODE_TRANSFER;\r\n#endif\r\nret = rio_query_mport(mport, &attr);\r\nif (!ret) {\r\nmd->properties.flags = attr.flags;\r\nmd->properties.link_speed = attr.link_speed;\r\nmd->properties.link_width = attr.link_width;\r\nmd->properties.dma_max_sge = attr.dma_max_sge;\r\nmd->properties.dma_max_size = attr.dma_max_size;\r\nmd->properties.dma_align = attr.dma_align;\r\nmd->properties.cap_sys_size = 0;\r\nmd->properties.cap_transfer_mode = 0;\r\nmd->properties.cap_addr_size = 0;\r\n} else\r\npr_info(DRV_PREFIX "Failed to obtain info for %s cdev(%d:%d)\n",\r\nmport->name, MAJOR(dev_number), mport->id);\r\nmutex_lock(&mport_devs_lock);\r\nlist_add_tail(&md->node, &mport_devs);\r\nmutex_unlock(&mport_devs_lock);\r\npr_info(DRV_PREFIX "Added %s cdev(%d:%d)\n",\r\nmport->name, MAJOR(dev_number), mport->id);\r\nreturn md;\r\nerr_cdev:\r\ncdev_del(&md->cdev);\r\nkfree(md);\r\nreturn NULL;\r\n}\r\nstatic void mport_cdev_terminate_dma(struct mport_dev *md)\r\n{\r\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\r\nstruct mport_cdev_priv *client;\r\nrmcd_debug(DMA, "%s", dev_name(&md->dev));\r\nmutex_lock(&md->file_mutex);\r\nlist_for_each_entry(client, &md->file_list, list) {\r\nif (client->dmach) {\r\ndmaengine_terminate_all(client->dmach);\r\nrio_release_dma(client->dmach);\r\n}\r\n}\r\nmutex_unlock(&md->file_mutex);\r\nif (md->dma_chan) {\r\ndmaengine_terminate_all(md->dma_chan);\r\nrio_release_dma(md->dma_chan);\r\nmd->dma_chan = NULL;\r\n}\r\n#endif\r\n}\r\nstatic int mport_cdev_kill_fasync(struct mport_dev *md)\r\n{\r\nunsigned int files = 0;\r\nstruct mport_cdev_priv *client;\r\nmutex_lock(&md->file_mutex);\r\nlist_for_each_entry(client, &md->file_list, list) {\r\nif (client->async_queue)\r\nkill_fasync(&client->async_queue, SIGIO, POLL_HUP);\r\nfiles++;\r\n}\r\nmutex_unlock(&md->file_mutex);\r\nreturn files;\r\n}\r\nstatic void mport_cdev_remove(struct mport_dev *md)\r\n{\r\nstruct rio_mport_mapping *map, *_map;\r\nrmcd_debug(EXIT, "Remove %s cdev", md->mport->name);\r\natomic_set(&md->active, 0);\r\nmport_cdev_terminate_dma(md);\r\nrio_del_mport_pw_handler(md->mport, md, rio_mport_pw_handler);\r\ncdev_del(&(md->cdev));\r\nmport_cdev_kill_fasync(md);\r\nflush_workqueue(dma_wq);\r\nmutex_lock(&md->buf_mutex);\r\nlist_for_each_entry_safe(map, _map, &md->mappings, node) {\r\nkref_put(&map->ref, mport_release_mapping);\r\n}\r\nmutex_unlock(&md->buf_mutex);\r\nif (!list_empty(&md->mappings))\r\nrmcd_warn("WARNING: %s pending mappings on removal",\r\nmd->mport->name);\r\nrio_release_inb_dbell(md->mport, 0, 0x0fff);\r\ndevice_unregister(&md->dev);\r\nput_device(&md->dev);\r\n}\r\nstatic int mport_add_mport(struct device *dev,\r\nstruct class_interface *class_intf)\r\n{\r\nstruct rio_mport *mport = NULL;\r\nstruct mport_dev *chdev = NULL;\r\nmport = to_rio_mport(dev);\r\nif (!mport)\r\nreturn -ENODEV;\r\nchdev = mport_cdev_add(mport);\r\nif (!chdev)\r\nreturn -ENODEV;\r\nreturn 0;\r\n}\r\nstatic void mport_remove_mport(struct device *dev,\r\nstruct class_interface *class_intf)\r\n{\r\nstruct rio_mport *mport = NULL;\r\nstruct mport_dev *chdev;\r\nint found = 0;\r\nmport = to_rio_mport(dev);\r\nrmcd_debug(EXIT, "Remove %s", mport->name);\r\nmutex_lock(&mport_devs_lock);\r\nlist_for_each_entry(chdev, &mport_devs, node) {\r\nif (chdev->mport->id == mport->id) {\r\natomic_set(&chdev->active, 0);\r\nlist_del(&chdev->node);\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&mport_devs_lock);\r\nif (found)\r\nmport_cdev_remove(chdev);\r\n}\r\nstatic int __init mport_init(void)\r\n{\r\nint ret;\r\ndev_class = class_create(THIS_MODULE, DRV_NAME);\r\nif (IS_ERR(dev_class)) {\r\nrmcd_error("Unable to create " DRV_NAME " class");\r\nreturn PTR_ERR(dev_class);\r\n}\r\nret = alloc_chrdev_region(&dev_number, 0, RIO_MAX_MPORTS, DRV_NAME);\r\nif (ret < 0)\r\ngoto err_chr;\r\nrmcd_debug(INIT, "Registered class with major=%d", MAJOR(dev_number));\r\nret = class_interface_register(&rio_mport_interface);\r\nif (ret) {\r\nrmcd_error("class_interface_register() failed, err=%d", ret);\r\ngoto err_cli;\r\n}\r\ndma_wq = create_singlethread_workqueue("dma_wq");\r\nif (!dma_wq) {\r\nrmcd_error("failed to create DMA work queue");\r\nret = -ENOMEM;\r\ngoto err_wq;\r\n}\r\nreturn 0;\r\nerr_wq:\r\nclass_interface_unregister(&rio_mport_interface);\r\nerr_cli:\r\nunregister_chrdev_region(dev_number, RIO_MAX_MPORTS);\r\nerr_chr:\r\nclass_destroy(dev_class);\r\nreturn ret;\r\n}\r\nstatic void __exit mport_exit(void)\r\n{\r\nclass_interface_unregister(&rio_mport_interface);\r\nclass_destroy(dev_class);\r\nunregister_chrdev_region(dev_number, RIO_MAX_MPORTS);\r\ndestroy_workqueue(dma_wq);\r\n}
