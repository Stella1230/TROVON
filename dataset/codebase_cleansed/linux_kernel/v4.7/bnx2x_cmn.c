static void bnx2x_add_all_napi_cnic(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_rx_queue_cnic(bp, i) {\r\nnetif_napi_add(bp->dev, &bnx2x_fp(bp, i, napi),\r\nbnx2x_poll, NAPI_POLL_WEIGHT);\r\n}\r\n}\r\nstatic void bnx2x_add_all_napi(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_eth_queue(bp, i) {\r\nnetif_napi_add(bp->dev, &bnx2x_fp(bp, i, napi),\r\nbnx2x_poll, NAPI_POLL_WEIGHT);\r\n}\r\n}\r\nstatic int bnx2x_calc_num_queues(struct bnx2x *bp)\r\n{\r\nint nq = bnx2x_num_queues ? : netif_get_num_default_rss_queues();\r\nif (is_kdump_kernel())\r\nnq = 1;\r\nnq = clamp(nq, 1, BNX2X_MAX_QUEUES(bp));\r\nreturn nq;\r\n}\r\nstatic inline void bnx2x_move_fp(struct bnx2x *bp, int from, int to)\r\n{\r\nstruct bnx2x_fastpath *from_fp = &bp->fp[from];\r\nstruct bnx2x_fastpath *to_fp = &bp->fp[to];\r\nstruct bnx2x_sp_objs *from_sp_objs = &bp->sp_objs[from];\r\nstruct bnx2x_sp_objs *to_sp_objs = &bp->sp_objs[to];\r\nstruct bnx2x_fp_stats *from_fp_stats = &bp->fp_stats[from];\r\nstruct bnx2x_fp_stats *to_fp_stats = &bp->fp_stats[to];\r\nint old_max_eth_txqs, new_max_eth_txqs;\r\nint old_txdata_index = 0, new_txdata_index = 0;\r\nstruct bnx2x_agg_info *old_tpa_info = to_fp->tpa_info;\r\nfrom_fp->napi = to_fp->napi;\r\nmemcpy(to_fp, from_fp, sizeof(*to_fp));\r\nto_fp->index = to;\r\nto_fp->tpa_info = old_tpa_info;\r\nmemcpy(to_sp_objs, from_sp_objs, sizeof(*to_sp_objs));\r\nmemcpy(to_fp_stats, from_fp_stats, sizeof(*to_fp_stats));\r\nold_max_eth_txqs = BNX2X_NUM_ETH_QUEUES(bp) * (bp)->max_cos;\r\nnew_max_eth_txqs = (BNX2X_NUM_ETH_QUEUES(bp) - from + to) *\r\n(bp)->max_cos;\r\nif (from == FCOE_IDX(bp)) {\r\nold_txdata_index = old_max_eth_txqs + FCOE_TXQ_IDX_OFFSET;\r\nnew_txdata_index = new_max_eth_txqs + FCOE_TXQ_IDX_OFFSET;\r\n}\r\nmemcpy(&bp->bnx2x_txq[new_txdata_index],\r\n&bp->bnx2x_txq[old_txdata_index],\r\nsizeof(struct bnx2x_fp_txdata));\r\nto_fp->txdata_ptr[0] = &bp->bnx2x_txq[new_txdata_index];\r\n}\r\nvoid bnx2x_fill_fw_str(struct bnx2x *bp, char *buf, size_t buf_len)\r\n{\r\nif (IS_PF(bp)) {\r\nu8 phy_fw_ver[PHY_FW_VER_LEN];\r\nphy_fw_ver[0] = '\0';\r\nbnx2x_get_ext_phy_fw_version(&bp->link_params,\r\nphy_fw_ver, PHY_FW_VER_LEN);\r\nstrlcpy(buf, bp->fw_ver, buf_len);\r\nsnprintf(buf + strlen(bp->fw_ver), 32 - strlen(bp->fw_ver),\r\n"bc %d.%d.%d%s%s",\r\n(bp->common.bc_ver & 0xff0000) >> 16,\r\n(bp->common.bc_ver & 0xff00) >> 8,\r\n(bp->common.bc_ver & 0xff),\r\n((phy_fw_ver[0] != '\0') ? " phy " : ""), phy_fw_ver);\r\n} else {\r\nbnx2x_vf_fill_fw_str(bp, buf, buf_len);\r\n}\r\n}\r\nstatic void bnx2x_shrink_eth_fp(struct bnx2x *bp, int delta)\r\n{\r\nint i, cos, old_eth_num = BNX2X_NUM_ETH_QUEUES(bp);\r\nfor (cos = 1; cos < bp->max_cos; cos++) {\r\nfor (i = 0; i < old_eth_num - delta; i++) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[i];\r\nint new_idx = cos * (old_eth_num - delta) + i;\r\nmemcpy(&bp->bnx2x_txq[new_idx], fp->txdata_ptr[cos],\r\nsizeof(struct bnx2x_fp_txdata));\r\nfp->txdata_ptr[cos] = &bp->bnx2x_txq[new_idx];\r\n}\r\n}\r\n}\r\nstatic u16 bnx2x_free_tx_pkt(struct bnx2x *bp, struct bnx2x_fp_txdata *txdata,\r\nu16 idx, unsigned int *pkts_compl,\r\nunsigned int *bytes_compl)\r\n{\r\nstruct sw_tx_bd *tx_buf = &txdata->tx_buf_ring[idx];\r\nstruct eth_tx_start_bd *tx_start_bd;\r\nstruct eth_tx_bd *tx_data_bd;\r\nstruct sk_buff *skb = tx_buf->skb;\r\nu16 bd_idx = TX_BD(tx_buf->first_bd), new_cons;\r\nint nbd;\r\nu16 split_bd_len = 0;\r\nprefetch(&skb->end);\r\nDP(NETIF_MSG_TX_DONE, "fp[%d]: pkt_idx %d buff @(%p)->skb %p\n",\r\ntxdata->txq_index, idx, tx_buf, skb);\r\ntx_start_bd = &txdata->tx_desc_ring[bd_idx].start_bd;\r\nnbd = le16_to_cpu(tx_start_bd->nbd) - 1;\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif ((nbd - 1) > (MAX_SKB_FRAGS + 2)) {\r\nBNX2X_ERR("BAD nbd!\n");\r\nbnx2x_panic();\r\n}\r\n#endif\r\nnew_cons = nbd + tx_buf->first_bd;\r\nbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\r\n--nbd;\r\nbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\r\nif (tx_buf->flags & BNX2X_HAS_SECOND_PBD) {\r\n--nbd;\r\nbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\r\n}\r\nif (tx_buf->flags & BNX2X_TSO_SPLIT_BD) {\r\ntx_data_bd = &txdata->tx_desc_ring[bd_idx].reg_bd;\r\nsplit_bd_len = BD_UNMAP_LEN(tx_data_bd);\r\n--nbd;\r\nbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\r\n}\r\ndma_unmap_single(&bp->pdev->dev, BD_UNMAP_ADDR(tx_start_bd),\r\nBD_UNMAP_LEN(tx_start_bd) + split_bd_len,\r\nDMA_TO_DEVICE);\r\nwhile (nbd > 0) {\r\ntx_data_bd = &txdata->tx_desc_ring[bd_idx].reg_bd;\r\ndma_unmap_page(&bp->pdev->dev, BD_UNMAP_ADDR(tx_data_bd),\r\nBD_UNMAP_LEN(tx_data_bd), DMA_TO_DEVICE);\r\nif (--nbd)\r\nbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\r\n}\r\nWARN_ON(!skb);\r\nif (likely(skb)) {\r\n(*pkts_compl)++;\r\n(*bytes_compl) += skb->len;\r\ndev_kfree_skb_any(skb);\r\n}\r\ntx_buf->first_bd = 0;\r\ntx_buf->skb = NULL;\r\nreturn new_cons;\r\n}\r\nint bnx2x_tx_int(struct bnx2x *bp, struct bnx2x_fp_txdata *txdata)\r\n{\r\nstruct netdev_queue *txq;\r\nu16 hw_cons, sw_cons, bd_cons = txdata->tx_bd_cons;\r\nunsigned int pkts_compl = 0, bytes_compl = 0;\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (unlikely(bp->panic))\r\nreturn -1;\r\n#endif\r\ntxq = netdev_get_tx_queue(bp->dev, txdata->txq_index);\r\nhw_cons = le16_to_cpu(*txdata->tx_cons_sb);\r\nsw_cons = txdata->tx_pkt_cons;\r\nwhile (sw_cons != hw_cons) {\r\nu16 pkt_cons;\r\npkt_cons = TX_BD(sw_cons);\r\nDP(NETIF_MSG_TX_DONE,\r\n"queue[%d]: hw_cons %u sw_cons %u pkt_cons %u\n",\r\ntxdata->txq_index, hw_cons, sw_cons, pkt_cons);\r\nbd_cons = bnx2x_free_tx_pkt(bp, txdata, pkt_cons,\r\n&pkts_compl, &bytes_compl);\r\nsw_cons++;\r\n}\r\nnetdev_tx_completed_queue(txq, pkts_compl, bytes_compl);\r\ntxdata->tx_pkt_cons = sw_cons;\r\ntxdata->tx_bd_cons = bd_cons;\r\nsmp_mb();\r\nif (unlikely(netif_tx_queue_stopped(txq))) {\r\n__netif_tx_lock(txq, smp_processor_id());\r\nif ((netif_tx_queue_stopped(txq)) &&\r\n(bp->state == BNX2X_STATE_OPEN) &&\r\n(bnx2x_tx_avail(bp, txdata) >= MAX_DESC_PER_TX_PKT))\r\nnetif_tx_wake_queue(txq);\r\n__netif_tx_unlock(txq);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void bnx2x_update_last_max_sge(struct bnx2x_fastpath *fp,\r\nu16 idx)\r\n{\r\nu16 last_max = fp->last_max_sge;\r\nif (SUB_S16(idx, last_max) > 0)\r\nfp->last_max_sge = idx;\r\n}\r\nstatic inline void bnx2x_update_sge_prod(struct bnx2x_fastpath *fp,\r\nu16 sge_len,\r\nstruct eth_end_agg_rx_cqe *cqe)\r\n{\r\nstruct bnx2x *bp = fp->bp;\r\nu16 last_max, last_elem, first_elem;\r\nu16 delta = 0;\r\nu16 i;\r\nif (!sge_len)\r\nreturn;\r\nfor (i = 0; i < sge_len; i++)\r\nBIT_VEC64_CLEAR_BIT(fp->sge_mask,\r\nRX_SGE(le16_to_cpu(cqe->sgl_or_raw_data.sgl[i])));\r\nDP(NETIF_MSG_RX_STATUS, "fp_cqe->sgl[%d] = %d\n",\r\nsge_len - 1, le16_to_cpu(cqe->sgl_or_raw_data.sgl[sge_len - 1]));\r\nprefetch((void *)(fp->sge_mask));\r\nbnx2x_update_last_max_sge(fp,\r\nle16_to_cpu(cqe->sgl_or_raw_data.sgl[sge_len - 1]));\r\nlast_max = RX_SGE(fp->last_max_sge);\r\nlast_elem = last_max >> BIT_VEC64_ELEM_SHIFT;\r\nfirst_elem = RX_SGE(fp->rx_sge_prod) >> BIT_VEC64_ELEM_SHIFT;\r\nif (last_elem + 1 != first_elem)\r\nlast_elem++;\r\nfor (i = first_elem; i != last_elem; i = NEXT_SGE_MASK_ELEM(i)) {\r\nif (likely(fp->sge_mask[i]))\r\nbreak;\r\nfp->sge_mask[i] = BIT_VEC64_ELEM_ONE_MASK;\r\ndelta += BIT_VEC64_ELEM_SZ;\r\n}\r\nif (delta > 0) {\r\nfp->rx_sge_prod += delta;\r\nbnx2x_clear_sge_mask_next_elems(fp);\r\n}\r\nDP(NETIF_MSG_RX_STATUS,\r\n"fp->last_max_sge = %d fp->rx_sge_prod = %d\n",\r\nfp->last_max_sge, fp->rx_sge_prod);\r\n}\r\nstatic u32 bnx2x_get_rxhash(const struct bnx2x *bp,\r\nconst struct eth_fast_path_rx_cqe *cqe,\r\nenum pkt_hash_types *rxhash_type)\r\n{\r\nif ((bp->dev->features & NETIF_F_RXHASH) &&\r\n(cqe->status_flags & ETH_FAST_PATH_RX_CQE_RSS_HASH_FLG)) {\r\nenum eth_rss_hash_type htype;\r\nhtype = cqe->status_flags & ETH_FAST_PATH_RX_CQE_RSS_HASH_TYPE;\r\n*rxhash_type = ((htype == TCP_IPV4_HASH_TYPE) ||\r\n(htype == TCP_IPV6_HASH_TYPE)) ?\r\nPKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3;\r\nreturn le32_to_cpu(cqe->rss_hash_result);\r\n}\r\n*rxhash_type = PKT_HASH_TYPE_NONE;\r\nreturn 0;\r\n}\r\nstatic void bnx2x_tpa_start(struct bnx2x_fastpath *fp, u16 queue,\r\nu16 cons, u16 prod,\r\nstruct eth_fast_path_rx_cqe *cqe)\r\n{\r\nstruct bnx2x *bp = fp->bp;\r\nstruct sw_rx_bd *cons_rx_buf = &fp->rx_buf_ring[cons];\r\nstruct sw_rx_bd *prod_rx_buf = &fp->rx_buf_ring[prod];\r\nstruct eth_rx_bd *prod_bd = &fp->rx_desc_ring[prod];\r\ndma_addr_t mapping;\r\nstruct bnx2x_agg_info *tpa_info = &fp->tpa_info[queue];\r\nstruct sw_rx_bd *first_buf = &tpa_info->first_buf;\r\nif (tpa_info->tpa_state != BNX2X_TPA_STOP)\r\nBNX2X_ERR("start of bin not in stop [%d]\n", queue);\r\nmapping = dma_map_single(&bp->pdev->dev,\r\nfirst_buf->data + NET_SKB_PAD,\r\nfp->rx_buf_size, DMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\r\nbnx2x_reuse_rx_data(fp, cons, prod);\r\ntpa_info->tpa_state = BNX2X_TPA_ERROR;\r\nreturn;\r\n}\r\nprod_rx_buf->data = first_buf->data;\r\ndma_unmap_addr_set(prod_rx_buf, mapping, mapping);\r\nprod_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\r\nprod_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\r\n*first_buf = *cons_rx_buf;\r\ntpa_info->parsing_flags =\r\nle16_to_cpu(cqe->pars_flags.flags);\r\ntpa_info->vlan_tag = le16_to_cpu(cqe->vlan_tag);\r\ntpa_info->tpa_state = BNX2X_TPA_START;\r\ntpa_info->len_on_bd = le16_to_cpu(cqe->len_on_bd);\r\ntpa_info->placement_offset = cqe->placement_offset;\r\ntpa_info->rxhash = bnx2x_get_rxhash(bp, cqe, &tpa_info->rxhash_type);\r\nif (fp->mode == TPA_MODE_GRO) {\r\nu16 gro_size = le16_to_cpu(cqe->pkt_len_or_gro_seg_len);\r\ntpa_info->full_page = SGE_PAGES / gro_size * gro_size;\r\ntpa_info->gro_size = gro_size;\r\n}\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nfp->tpa_queue_used |= (1 << queue);\r\nDP(NETIF_MSG_RX_STATUS, "fp->tpa_queue_used = 0x%llx\n",\r\nfp->tpa_queue_used);\r\n#endif\r\n}\r\nstatic void bnx2x_set_gro_params(struct sk_buff *skb, u16 parsing_flags,\r\nu16 len_on_bd, unsigned int pkt_len,\r\nu16 num_of_coalesced_segs)\r\n{\r\nu16 hdrs_len = ETH_HLEN + sizeof(struct tcphdr);\r\nif (GET_FLAG(parsing_flags, PARSING_FLAGS_OVER_ETHERNET_PROTOCOL) ==\r\nPRS_FLAG_OVERETH_IPV6) {\r\nhdrs_len += sizeof(struct ipv6hdr);\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\r\n} else {\r\nhdrs_len += sizeof(struct iphdr);\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\r\n}\r\nif (parsing_flags & PARSING_FLAGS_TIME_STAMP_EXIST_FLAG)\r\nhdrs_len += TPA_TSTAMP_OPT_LEN;\r\nskb_shinfo(skb)->gso_size = len_on_bd - hdrs_len;\r\nNAPI_GRO_CB(skb)->count = num_of_coalesced_segs;\r\n}\r\nstatic int bnx2x_alloc_rx_sge(struct bnx2x *bp, struct bnx2x_fastpath *fp,\r\nu16 index, gfp_t gfp_mask)\r\n{\r\nstruct sw_rx_page *sw_buf = &fp->rx_page_ring[index];\r\nstruct eth_rx_sge *sge = &fp->rx_sge_ring[index];\r\nstruct bnx2x_alloc_pool *pool = &fp->page_pool;\r\ndma_addr_t mapping;\r\nif (!pool->page || (PAGE_SIZE - pool->offset) < SGE_PAGE_SIZE) {\r\nif (pool->page)\r\nput_page(pool->page);\r\npool->page = alloc_pages(gfp_mask, PAGES_PER_SGE_SHIFT);\r\nif (unlikely(!pool->page))\r\nreturn -ENOMEM;\r\npool->offset = 0;\r\n}\r\nmapping = dma_map_page(&bp->pdev->dev, pool->page,\r\npool->offset, SGE_PAGE_SIZE, DMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\r\nBNX2X_ERR("Can't map sge\n");\r\nreturn -ENOMEM;\r\n}\r\nget_page(pool->page);\r\nsw_buf->page = pool->page;\r\nsw_buf->offset = pool->offset;\r\ndma_unmap_addr_set(sw_buf, mapping, mapping);\r\nsge->addr_hi = cpu_to_le32(U64_HI(mapping));\r\nsge->addr_lo = cpu_to_le32(U64_LO(mapping));\r\npool->offset += SGE_PAGE_SIZE;\r\nreturn 0;\r\n}\r\nstatic int bnx2x_fill_frag_skb(struct bnx2x *bp, struct bnx2x_fastpath *fp,\r\nstruct bnx2x_agg_info *tpa_info,\r\nu16 pages,\r\nstruct sk_buff *skb,\r\nstruct eth_end_agg_rx_cqe *cqe,\r\nu16 cqe_idx)\r\n{\r\nstruct sw_rx_page *rx_pg, old_rx_pg;\r\nu32 i, frag_len, frag_size;\r\nint err, j, frag_id = 0;\r\nu16 len_on_bd = tpa_info->len_on_bd;\r\nu16 full_page = 0, gro_size = 0;\r\nfrag_size = le16_to_cpu(cqe->pkt_len) - len_on_bd;\r\nif (fp->mode == TPA_MODE_GRO) {\r\ngro_size = tpa_info->gro_size;\r\nfull_page = tpa_info->full_page;\r\n}\r\nif (frag_size)\r\nbnx2x_set_gro_params(skb, tpa_info->parsing_flags, len_on_bd,\r\nle16_to_cpu(cqe->pkt_len),\r\nle16_to_cpu(cqe->num_of_coalesced_segs));\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (pages > min_t(u32, 8, MAX_SKB_FRAGS) * SGE_PAGES) {\r\nBNX2X_ERR("SGL length is too long: %d. CQE index is %d\n",\r\npages, cqe_idx);\r\nBNX2X_ERR("cqe->pkt_len = %d\n", cqe->pkt_len);\r\nbnx2x_panic();\r\nreturn -EINVAL;\r\n}\r\n#endif\r\nfor (i = 0, j = 0; i < pages; i += PAGES_PER_SGE, j++) {\r\nu16 sge_idx = RX_SGE(le16_to_cpu(cqe->sgl_or_raw_data.sgl[j]));\r\nif (fp->mode == TPA_MODE_GRO)\r\nfrag_len = min_t(u32, frag_size, (u32)full_page);\r\nelse\r\nfrag_len = min_t(u32, frag_size, (u32)SGE_PAGES);\r\nrx_pg = &fp->rx_page_ring[sge_idx];\r\nold_rx_pg = *rx_pg;\r\nerr = bnx2x_alloc_rx_sge(bp, fp, sge_idx, GFP_ATOMIC);\r\nif (unlikely(err)) {\r\nbnx2x_fp_qstats(bp, fp)->rx_skb_alloc_failed++;\r\nreturn err;\r\n}\r\ndma_unmap_page(&bp->pdev->dev,\r\ndma_unmap_addr(&old_rx_pg, mapping),\r\nSGE_PAGE_SIZE, DMA_FROM_DEVICE);\r\nif (fp->mode == TPA_MODE_LRO)\r\nskb_fill_page_desc(skb, j, old_rx_pg.page,\r\nold_rx_pg.offset, frag_len);\r\nelse {\r\nint rem;\r\nint offset = 0;\r\nfor (rem = frag_len; rem > 0; rem -= gro_size) {\r\nint len = rem > gro_size ? gro_size : rem;\r\nskb_fill_page_desc(skb, frag_id++,\r\nold_rx_pg.page,\r\nold_rx_pg.offset + offset,\r\nlen);\r\nif (offset)\r\nget_page(old_rx_pg.page);\r\noffset += len;\r\n}\r\n}\r\nskb->data_len += frag_len;\r\nskb->truesize += SGE_PAGES;\r\nskb->len += frag_len;\r\nfrag_size -= frag_len;\r\n}\r\nreturn 0;\r\n}\r\nstatic void bnx2x_frag_free(const struct bnx2x_fastpath *fp, void *data)\r\n{\r\nif (fp->rx_frag_size)\r\nskb_free_frag(data);\r\nelse\r\nkfree(data);\r\n}\r\nstatic void *bnx2x_frag_alloc(const struct bnx2x_fastpath *fp, gfp_t gfp_mask)\r\n{\r\nif (fp->rx_frag_size) {\r\nif (unlikely(gfpflags_allow_blocking(gfp_mask)))\r\nreturn (void *)__get_free_page(gfp_mask);\r\nreturn netdev_alloc_frag(fp->rx_frag_size);\r\n}\r\nreturn kmalloc(fp->rx_buf_size + NET_SKB_PAD, gfp_mask);\r\n}\r\nstatic void bnx2x_gro_ip_csum(struct bnx2x *bp, struct sk_buff *skb)\r\n{\r\nconst struct iphdr *iph = ip_hdr(skb);\r\nstruct tcphdr *th;\r\nskb_set_transport_header(skb, sizeof(struct iphdr));\r\nth = tcp_hdr(skb);\r\nth->check = ~tcp_v4_check(skb->len - skb_transport_offset(skb),\r\niph->saddr, iph->daddr, 0);\r\n}\r\nstatic void bnx2x_gro_ipv6_csum(struct bnx2x *bp, struct sk_buff *skb)\r\n{\r\nstruct ipv6hdr *iph = ipv6_hdr(skb);\r\nstruct tcphdr *th;\r\nskb_set_transport_header(skb, sizeof(struct ipv6hdr));\r\nth = tcp_hdr(skb);\r\nth->check = ~tcp_v6_check(skb->len - skb_transport_offset(skb),\r\n&iph->saddr, &iph->daddr, 0);\r\n}\r\nstatic void bnx2x_gro_csum(struct bnx2x *bp, struct sk_buff *skb,\r\nvoid (*gro_func)(struct bnx2x*, struct sk_buff*))\r\n{\r\nskb_set_network_header(skb, 0);\r\ngro_func(bp, skb);\r\ntcp_gro_complete(skb);\r\n}\r\nstatic void bnx2x_gro_receive(struct bnx2x *bp, struct bnx2x_fastpath *fp,\r\nstruct sk_buff *skb)\r\n{\r\n#ifdef CONFIG_INET\r\nif (skb_shinfo(skb)->gso_size) {\r\nswitch (be16_to_cpu(skb->protocol)) {\r\ncase ETH_P_IP:\r\nbnx2x_gro_csum(bp, skb, bnx2x_gro_ip_csum);\r\nbreak;\r\ncase ETH_P_IPV6:\r\nbnx2x_gro_csum(bp, skb, bnx2x_gro_ipv6_csum);\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "Error: FW GRO supports only IPv4/IPv6, not 0x%04x\n",\r\nbe16_to_cpu(skb->protocol));\r\n}\r\n}\r\n#endif\r\nskb_record_rx_queue(skb, fp->rx_queue);\r\nnapi_gro_receive(&fp->napi, skb);\r\n}\r\nstatic void bnx2x_tpa_stop(struct bnx2x *bp, struct bnx2x_fastpath *fp,\r\nstruct bnx2x_agg_info *tpa_info,\r\nu16 pages,\r\nstruct eth_end_agg_rx_cqe *cqe,\r\nu16 cqe_idx)\r\n{\r\nstruct sw_rx_bd *rx_buf = &tpa_info->first_buf;\r\nu8 pad = tpa_info->placement_offset;\r\nu16 len = tpa_info->len_on_bd;\r\nstruct sk_buff *skb = NULL;\r\nu8 *new_data, *data = rx_buf->data;\r\nu8 old_tpa_state = tpa_info->tpa_state;\r\ntpa_info->tpa_state = BNX2X_TPA_STOP;\r\nif (old_tpa_state == BNX2X_TPA_ERROR)\r\ngoto drop;\r\nnew_data = bnx2x_frag_alloc(fp, GFP_ATOMIC);\r\ndma_unmap_single(&bp->pdev->dev, dma_unmap_addr(rx_buf, mapping),\r\nfp->rx_buf_size, DMA_FROM_DEVICE);\r\nif (likely(new_data))\r\nskb = build_skb(data, fp->rx_frag_size);\r\nif (likely(skb)) {\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (pad + len > fp->rx_buf_size) {\r\nBNX2X_ERR("skb_put is about to fail... pad %d len %d rx_buf_size %d\n",\r\npad, len, fp->rx_buf_size);\r\nbnx2x_panic();\r\nreturn;\r\n}\r\n#endif\r\nskb_reserve(skb, pad + NET_SKB_PAD);\r\nskb_put(skb, len);\r\nskb_set_hash(skb, tpa_info->rxhash, tpa_info->rxhash_type);\r\nskb->protocol = eth_type_trans(skb, bp->dev);\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nif (!bnx2x_fill_frag_skb(bp, fp, tpa_info, pages,\r\nskb, cqe, cqe_idx)) {\r\nif (tpa_info->parsing_flags & PARSING_FLAGS_VLAN)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tpa_info->vlan_tag);\r\nbnx2x_gro_receive(bp, fp, skb);\r\n} else {\r\nDP(NETIF_MSG_RX_STATUS,\r\n"Failed to allocate new pages - dropping packet!\n");\r\ndev_kfree_skb_any(skb);\r\n}\r\nrx_buf->data = new_data;\r\nreturn;\r\n}\r\nif (new_data)\r\nbnx2x_frag_free(fp, new_data);\r\ndrop:\r\nDP(NETIF_MSG_RX_STATUS,\r\n"Failed to allocate or map a new skb - dropping packet!\n");\r\nbnx2x_fp_stats(bp, fp)->eth_q_stats.rx_skb_alloc_failed++;\r\n}\r\nstatic int bnx2x_alloc_rx_data(struct bnx2x *bp, struct bnx2x_fastpath *fp,\r\nu16 index, gfp_t gfp_mask)\r\n{\r\nu8 *data;\r\nstruct sw_rx_bd *rx_buf = &fp->rx_buf_ring[index];\r\nstruct eth_rx_bd *rx_bd = &fp->rx_desc_ring[index];\r\ndma_addr_t mapping;\r\ndata = bnx2x_frag_alloc(fp, gfp_mask);\r\nif (unlikely(data == NULL))\r\nreturn -ENOMEM;\r\nmapping = dma_map_single(&bp->pdev->dev, data + NET_SKB_PAD,\r\nfp->rx_buf_size,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\r\nbnx2x_frag_free(fp, data);\r\nBNX2X_ERR("Can't map rx data\n");\r\nreturn -ENOMEM;\r\n}\r\nrx_buf->data = data;\r\ndma_unmap_addr_set(rx_buf, mapping, mapping);\r\nrx_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\r\nrx_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\r\nreturn 0;\r\n}\r\nstatic\r\nvoid bnx2x_csum_validate(struct sk_buff *skb, union eth_rx_cqe *cqe,\r\nstruct bnx2x_fastpath *fp,\r\nstruct bnx2x_eth_q_stats *qstats)\r\n{\r\nif (cqe->fast_path_cqe.status_flags &\r\nETH_FAST_PATH_RX_CQE_L4_XSUM_NO_VALIDATION_FLG)\r\nreturn;\r\nif (cqe->fast_path_cqe.type_error_flags &\r\n(ETH_FAST_PATH_RX_CQE_IP_BAD_XSUM_FLG |\r\nETH_FAST_PATH_RX_CQE_L4_BAD_XSUM_FLG))\r\nqstats->hw_csum_err++;\r\nelse\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n}\r\nstatic int bnx2x_rx_int(struct bnx2x_fastpath *fp, int budget)\r\n{\r\nstruct bnx2x *bp = fp->bp;\r\nu16 bd_cons, bd_prod, bd_prod_fw, comp_ring_cons;\r\nu16 sw_comp_cons, sw_comp_prod;\r\nint rx_pkt = 0;\r\nunion eth_rx_cqe *cqe;\r\nstruct eth_fast_path_rx_cqe *cqe_fp;\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (unlikely(bp->panic))\r\nreturn 0;\r\n#endif\r\nif (budget <= 0)\r\nreturn rx_pkt;\r\nbd_cons = fp->rx_bd_cons;\r\nbd_prod = fp->rx_bd_prod;\r\nbd_prod_fw = bd_prod;\r\nsw_comp_cons = fp->rx_comp_cons;\r\nsw_comp_prod = fp->rx_comp_prod;\r\ncomp_ring_cons = RCQ_BD(sw_comp_cons);\r\ncqe = &fp->rx_comp_ring[comp_ring_cons];\r\ncqe_fp = &cqe->fast_path_cqe;\r\nDP(NETIF_MSG_RX_STATUS,\r\n"queue[%d]: sw_comp_cons %u\n", fp->index, sw_comp_cons);\r\nwhile (BNX2X_IS_CQE_COMPLETED(cqe_fp)) {\r\nstruct sw_rx_bd *rx_buf = NULL;\r\nstruct sk_buff *skb;\r\nu8 cqe_fp_flags;\r\nenum eth_rx_cqe_type cqe_fp_type;\r\nu16 len, pad, queue;\r\nu8 *data;\r\nu32 rxhash;\r\nenum pkt_hash_types rxhash_type;\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (unlikely(bp->panic))\r\nreturn 0;\r\n#endif\r\nbd_prod = RX_BD(bd_prod);\r\nbd_cons = RX_BD(bd_cons);\r\nrmb();\r\ncqe_fp_flags = cqe_fp->type_error_flags;\r\ncqe_fp_type = cqe_fp_flags & ETH_FAST_PATH_RX_CQE_TYPE;\r\nDP(NETIF_MSG_RX_STATUS,\r\n"CQE type %x err %x status %x queue %x vlan %x len %u\n",\r\nCQE_TYPE(cqe_fp_flags),\r\ncqe_fp_flags, cqe_fp->status_flags,\r\nle32_to_cpu(cqe_fp->rss_hash_result),\r\nle16_to_cpu(cqe_fp->vlan_tag),\r\nle16_to_cpu(cqe_fp->pkt_len_or_gro_seg_len));\r\nif (unlikely(CQE_TYPE_SLOW(cqe_fp_type))) {\r\nbnx2x_sp_event(fp, cqe);\r\ngoto next_cqe;\r\n}\r\nrx_buf = &fp->rx_buf_ring[bd_cons];\r\ndata = rx_buf->data;\r\nif (!CQE_TYPE_FAST(cqe_fp_type)) {\r\nstruct bnx2x_agg_info *tpa_info;\r\nu16 frag_size, pages;\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (fp->mode == TPA_MODE_DISABLED &&\r\n(CQE_TYPE_START(cqe_fp_type) ||\r\nCQE_TYPE_STOP(cqe_fp_type)))\r\nBNX2X_ERR("START/STOP packet while TPA disabled, type %x\n",\r\nCQE_TYPE(cqe_fp_type));\r\n#endif\r\nif (CQE_TYPE_START(cqe_fp_type)) {\r\nu16 queue = cqe_fp->queue_index;\r\nDP(NETIF_MSG_RX_STATUS,\r\n"calling tpa_start on queue %d\n",\r\nqueue);\r\nbnx2x_tpa_start(fp, queue,\r\nbd_cons, bd_prod,\r\ncqe_fp);\r\ngoto next_rx;\r\n}\r\nqueue = cqe->end_agg_cqe.queue_index;\r\ntpa_info = &fp->tpa_info[queue];\r\nDP(NETIF_MSG_RX_STATUS,\r\n"calling tpa_stop on queue %d\n",\r\nqueue);\r\nfrag_size = le16_to_cpu(cqe->end_agg_cqe.pkt_len) -\r\ntpa_info->len_on_bd;\r\nif (fp->mode == TPA_MODE_GRO)\r\npages = (frag_size + tpa_info->full_page - 1) /\r\ntpa_info->full_page;\r\nelse\r\npages = SGE_PAGE_ALIGN(frag_size) >>\r\nSGE_PAGE_SHIFT;\r\nbnx2x_tpa_stop(bp, fp, tpa_info, pages,\r\n&cqe->end_agg_cqe, comp_ring_cons);\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (bp->panic)\r\nreturn 0;\r\n#endif\r\nbnx2x_update_sge_prod(fp, pages, &cqe->end_agg_cqe);\r\ngoto next_cqe;\r\n}\r\nlen = le16_to_cpu(cqe_fp->pkt_len_or_gro_seg_len);\r\npad = cqe_fp->placement_offset;\r\ndma_sync_single_for_cpu(&bp->pdev->dev,\r\ndma_unmap_addr(rx_buf, mapping),\r\npad + RX_COPY_THRESH,\r\nDMA_FROM_DEVICE);\r\npad += NET_SKB_PAD;\r\nprefetch(data + pad);\r\nif (unlikely(cqe_fp_flags & ETH_RX_ERROR_FALGS)) {\r\nDP(NETIF_MSG_RX_ERR | NETIF_MSG_RX_STATUS,\r\n"ERROR flags %x rx packet %u\n",\r\ncqe_fp_flags, sw_comp_cons);\r\nbnx2x_fp_qstats(bp, fp)->rx_err_discard_pkt++;\r\ngoto reuse_rx;\r\n}\r\nif ((bp->dev->mtu > ETH_MAX_PACKET_SIZE) &&\r\n(len <= RX_COPY_THRESH)) {\r\nskb = napi_alloc_skb(&fp->napi, len);\r\nif (skb == NULL) {\r\nDP(NETIF_MSG_RX_ERR | NETIF_MSG_RX_STATUS,\r\n"ERROR packet dropped because of alloc failure\n");\r\nbnx2x_fp_qstats(bp, fp)->rx_skb_alloc_failed++;\r\ngoto reuse_rx;\r\n}\r\nmemcpy(skb->data, data + pad, len);\r\nbnx2x_reuse_rx_data(fp, bd_cons, bd_prod);\r\n} else {\r\nif (likely(bnx2x_alloc_rx_data(bp, fp, bd_prod,\r\nGFP_ATOMIC) == 0)) {\r\ndma_unmap_single(&bp->pdev->dev,\r\ndma_unmap_addr(rx_buf, mapping),\r\nfp->rx_buf_size,\r\nDMA_FROM_DEVICE);\r\nskb = build_skb(data, fp->rx_frag_size);\r\nif (unlikely(!skb)) {\r\nbnx2x_frag_free(fp, data);\r\nbnx2x_fp_qstats(bp, fp)->\r\nrx_skb_alloc_failed++;\r\ngoto next_rx;\r\n}\r\nskb_reserve(skb, pad);\r\n} else {\r\nDP(NETIF_MSG_RX_ERR | NETIF_MSG_RX_STATUS,\r\n"ERROR packet dropped because of alloc failure\n");\r\nbnx2x_fp_qstats(bp, fp)->rx_skb_alloc_failed++;\r\nreuse_rx:\r\nbnx2x_reuse_rx_data(fp, bd_cons, bd_prod);\r\ngoto next_rx;\r\n}\r\n}\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, bp->dev);\r\nrxhash = bnx2x_get_rxhash(bp, cqe_fp, &rxhash_type);\r\nskb_set_hash(skb, rxhash, rxhash_type);\r\nskb_checksum_none_assert(skb);\r\nif (bp->dev->features & NETIF_F_RXCSUM)\r\nbnx2x_csum_validate(skb, cqe, fp,\r\nbnx2x_fp_qstats(bp, fp));\r\nskb_record_rx_queue(skb, fp->rx_queue);\r\nif (unlikely(cqe->fast_path_cqe.type_error_flags &\r\n(1 << ETH_FAST_PATH_RX_CQE_PTP_PKT_SHIFT)))\r\nbnx2x_set_rx_ts(bp, skb);\r\nif (le16_to_cpu(cqe_fp->pars_flags.flags) &\r\nPARSING_FLAGS_VLAN)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\r\nle16_to_cpu(cqe_fp->vlan_tag));\r\nnapi_gro_receive(&fp->napi, skb);\r\nnext_rx:\r\nrx_buf->data = NULL;\r\nbd_cons = NEXT_RX_IDX(bd_cons);\r\nbd_prod = NEXT_RX_IDX(bd_prod);\r\nbd_prod_fw = NEXT_RX_IDX(bd_prod_fw);\r\nrx_pkt++;\r\nnext_cqe:\r\nsw_comp_prod = NEXT_RCQ_IDX(sw_comp_prod);\r\nsw_comp_cons = NEXT_RCQ_IDX(sw_comp_cons);\r\nBNX2X_SEED_CQE(cqe_fp);\r\nif (rx_pkt == budget)\r\nbreak;\r\ncomp_ring_cons = RCQ_BD(sw_comp_cons);\r\ncqe = &fp->rx_comp_ring[comp_ring_cons];\r\ncqe_fp = &cqe->fast_path_cqe;\r\n}\r\nfp->rx_bd_cons = bd_cons;\r\nfp->rx_bd_prod = bd_prod_fw;\r\nfp->rx_comp_cons = sw_comp_cons;\r\nfp->rx_comp_prod = sw_comp_prod;\r\nbnx2x_update_rx_prod(bp, fp, bd_prod_fw, sw_comp_prod,\r\nfp->rx_sge_prod);\r\nreturn rx_pkt;\r\n}\r\nstatic irqreturn_t bnx2x_msix_fp_int(int irq, void *fp_cookie)\r\n{\r\nstruct bnx2x_fastpath *fp = fp_cookie;\r\nstruct bnx2x *bp = fp->bp;\r\nu8 cos;\r\nDP(NETIF_MSG_INTR,\r\n"got an MSI-X interrupt on IDX:SB [fp %d fw_sd %d igusb %d]\n",\r\nfp->index, fp->fw_sb_id, fp->igu_sb_id);\r\nbnx2x_ack_sb(bp, fp->igu_sb_id, USTORM_ID, 0, IGU_INT_DISABLE, 0);\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (unlikely(bp->panic))\r\nreturn IRQ_HANDLED;\r\n#endif\r\nfor_each_cos_in_tx_queue(fp, cos)\r\nprefetch(fp->txdata_ptr[cos]->tx_cons_sb);\r\nprefetch(&fp->sb_running_index[SM_RX_ID]);\r\nnapi_schedule_irqoff(&bnx2x_fp(bp, fp->index, napi));\r\nreturn IRQ_HANDLED;\r\n}\r\nvoid bnx2x_acquire_phy_lock(struct bnx2x *bp)\r\n{\r\nmutex_lock(&bp->port.phy_mutex);\r\nbnx2x_acquire_hw_lock(bp, HW_LOCK_RESOURCE_MDIO);\r\n}\r\nvoid bnx2x_release_phy_lock(struct bnx2x *bp)\r\n{\r\nbnx2x_release_hw_lock(bp, HW_LOCK_RESOURCE_MDIO);\r\nmutex_unlock(&bp->port.phy_mutex);\r\n}\r\nu16 bnx2x_get_mf_speed(struct bnx2x *bp)\r\n{\r\nu16 line_speed = bp->link_vars.line_speed;\r\nif (IS_MF(bp)) {\r\nu16 maxCfg = bnx2x_extract_max_cfg(bp,\r\nbp->mf_config[BP_VN(bp)]);\r\nif (IS_MF_PERCENT_BW(bp))\r\nline_speed = (line_speed * maxCfg) / 100;\r\nelse {\r\nu16 vn_max_rate = maxCfg * 100;\r\nif (vn_max_rate < line_speed)\r\nline_speed = vn_max_rate;\r\n}\r\n}\r\nreturn line_speed;\r\n}\r\nstatic void bnx2x_fill_report_data(struct bnx2x *bp,\r\nstruct bnx2x_link_report_data *data)\r\n{\r\nmemset(data, 0, sizeof(*data));\r\nif (IS_PF(bp)) {\r\ndata->line_speed = bnx2x_get_mf_speed(bp);\r\nif (!bp->link_vars.link_up || (bp->flags & MF_FUNC_DIS))\r\n__set_bit(BNX2X_LINK_REPORT_LINK_DOWN,\r\n&data->link_report_flags);\r\nif (!BNX2X_NUM_ETH_QUEUES(bp))\r\n__set_bit(BNX2X_LINK_REPORT_LINK_DOWN,\r\n&data->link_report_flags);\r\nif (bp->link_vars.duplex == DUPLEX_FULL)\r\n__set_bit(BNX2X_LINK_REPORT_FD,\r\n&data->link_report_flags);\r\nif (bp->link_vars.flow_ctrl & BNX2X_FLOW_CTRL_RX)\r\n__set_bit(BNX2X_LINK_REPORT_RX_FC_ON,\r\n&data->link_report_flags);\r\nif (bp->link_vars.flow_ctrl & BNX2X_FLOW_CTRL_TX)\r\n__set_bit(BNX2X_LINK_REPORT_TX_FC_ON,\r\n&data->link_report_flags);\r\n} else {\r\n*data = bp->vf_link_vars;\r\n}\r\n}\r\nvoid bnx2x_link_report(struct bnx2x *bp)\r\n{\r\nbnx2x_acquire_phy_lock(bp);\r\n__bnx2x_link_report(bp);\r\nbnx2x_release_phy_lock(bp);\r\n}\r\nvoid __bnx2x_link_report(struct bnx2x *bp)\r\n{\r\nstruct bnx2x_link_report_data cur_data;\r\nif (IS_PF(bp) && !CHIP_IS_E1(bp))\r\nbnx2x_read_mf_cfg(bp);\r\nbnx2x_fill_report_data(bp, &cur_data);\r\nif (!memcmp(&cur_data, &bp->last_reported_link, sizeof(cur_data)) ||\r\n(test_bit(BNX2X_LINK_REPORT_LINK_DOWN,\r\n&bp->last_reported_link.link_report_flags) &&\r\ntest_bit(BNX2X_LINK_REPORT_LINK_DOWN,\r\n&cur_data.link_report_flags)))\r\nreturn;\r\nbp->link_cnt++;\r\nmemcpy(&bp->last_reported_link, &cur_data, sizeof(cur_data));\r\nif (IS_PF(bp))\r\nbnx2x_iov_link_update(bp);\r\nif (test_bit(BNX2X_LINK_REPORT_LINK_DOWN,\r\n&cur_data.link_report_flags)) {\r\nnetif_carrier_off(bp->dev);\r\nnetdev_err(bp->dev, "NIC Link is Down\n");\r\nreturn;\r\n} else {\r\nconst char *duplex;\r\nconst char *flow;\r\nnetif_carrier_on(bp->dev);\r\nif (test_and_clear_bit(BNX2X_LINK_REPORT_FD,\r\n&cur_data.link_report_flags))\r\nduplex = "full";\r\nelse\r\nduplex = "half";\r\nif (cur_data.link_report_flags) {\r\nif (test_bit(BNX2X_LINK_REPORT_RX_FC_ON,\r\n&cur_data.link_report_flags)) {\r\nif (test_bit(BNX2X_LINK_REPORT_TX_FC_ON,\r\n&cur_data.link_report_flags))\r\nflow = "ON - receive & transmit";\r\nelse\r\nflow = "ON - receive";\r\n} else {\r\nflow = "ON - transmit";\r\n}\r\n} else {\r\nflow = "none";\r\n}\r\nnetdev_info(bp->dev, "NIC Link is Up, %d Mbps %s duplex, Flow control: %s\n",\r\ncur_data.line_speed, duplex, flow);\r\n}\r\n}\r\nstatic void bnx2x_set_next_page_sgl(struct bnx2x_fastpath *fp)\r\n{\r\nint i;\r\nfor (i = 1; i <= NUM_RX_SGE_PAGES; i++) {\r\nstruct eth_rx_sge *sge;\r\nsge = &fp->rx_sge_ring[RX_SGE_CNT * i - 2];\r\nsge->addr_hi =\r\ncpu_to_le32(U64_HI(fp->rx_sge_mapping +\r\nBCM_PAGE_SIZE*(i % NUM_RX_SGE_PAGES)));\r\nsge->addr_lo =\r\ncpu_to_le32(U64_LO(fp->rx_sge_mapping +\r\nBCM_PAGE_SIZE*(i % NUM_RX_SGE_PAGES)));\r\n}\r\n}\r\nstatic void bnx2x_free_tpa_pool(struct bnx2x *bp,\r\nstruct bnx2x_fastpath *fp, int last)\r\n{\r\nint i;\r\nfor (i = 0; i < last; i++) {\r\nstruct bnx2x_agg_info *tpa_info = &fp->tpa_info[i];\r\nstruct sw_rx_bd *first_buf = &tpa_info->first_buf;\r\nu8 *data = first_buf->data;\r\nif (data == NULL) {\r\nDP(NETIF_MSG_IFDOWN, "tpa bin %d empty on free\n", i);\r\ncontinue;\r\n}\r\nif (tpa_info->tpa_state == BNX2X_TPA_START)\r\ndma_unmap_single(&bp->pdev->dev,\r\ndma_unmap_addr(first_buf, mapping),\r\nfp->rx_buf_size, DMA_FROM_DEVICE);\r\nbnx2x_frag_free(fp, data);\r\nfirst_buf->data = NULL;\r\n}\r\n}\r\nvoid bnx2x_init_rx_rings_cnic(struct bnx2x *bp)\r\n{\r\nint j;\r\nfor_each_rx_queue_cnic(bp, j) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[j];\r\nfp->rx_bd_cons = 0;\r\nbnx2x_update_rx_prod(bp, fp, fp->rx_bd_prod, fp->rx_comp_prod,\r\nfp->rx_sge_prod);\r\n}\r\n}\r\nvoid bnx2x_init_rx_rings(struct bnx2x *bp)\r\n{\r\nint func = BP_FUNC(bp);\r\nu16 ring_prod;\r\nint i, j;\r\nfor_each_eth_queue(bp, j) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[j];\r\nDP(NETIF_MSG_IFUP,\r\n"mtu %d rx_buf_size %d\n", bp->dev->mtu, fp->rx_buf_size);\r\nif (fp->mode != TPA_MODE_DISABLED) {\r\nfor (i = 0; i < MAX_AGG_QS(bp); i++) {\r\nstruct bnx2x_agg_info *tpa_info =\r\n&fp->tpa_info[i];\r\nstruct sw_rx_bd *first_buf =\r\n&tpa_info->first_buf;\r\nfirst_buf->data =\r\nbnx2x_frag_alloc(fp, GFP_KERNEL);\r\nif (!first_buf->data) {\r\nBNX2X_ERR("Failed to allocate TPA skb pool for queue[%d] - disabling TPA on this queue!\n",\r\nj);\r\nbnx2x_free_tpa_pool(bp, fp, i);\r\nfp->mode = TPA_MODE_DISABLED;\r\nbreak;\r\n}\r\ndma_unmap_addr_set(first_buf, mapping, 0);\r\ntpa_info->tpa_state = BNX2X_TPA_STOP;\r\n}\r\nbnx2x_set_next_page_sgl(fp);\r\nbnx2x_init_sge_ring_bit_mask(fp);\r\nfor (i = 0, ring_prod = 0;\r\ni < MAX_RX_SGE_CNT*NUM_RX_SGE_PAGES; i++) {\r\nif (bnx2x_alloc_rx_sge(bp, fp, ring_prod,\r\nGFP_KERNEL) < 0) {\r\nBNX2X_ERR("was only able to allocate %d rx sges\n",\r\ni);\r\nBNX2X_ERR("disabling TPA for queue[%d]\n",\r\nj);\r\nbnx2x_free_rx_sge_range(bp, fp,\r\nring_prod);\r\nbnx2x_free_tpa_pool(bp, fp,\r\nMAX_AGG_QS(bp));\r\nfp->mode = TPA_MODE_DISABLED;\r\nring_prod = 0;\r\nbreak;\r\n}\r\nring_prod = NEXT_SGE_IDX(ring_prod);\r\n}\r\nfp->rx_sge_prod = ring_prod;\r\n}\r\n}\r\nfor_each_eth_queue(bp, j) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[j];\r\nfp->rx_bd_cons = 0;\r\nbnx2x_update_rx_prod(bp, fp, fp->rx_bd_prod, fp->rx_comp_prod,\r\nfp->rx_sge_prod);\r\nif (j != 0)\r\ncontinue;\r\nif (CHIP_IS_E1(bp)) {\r\nREG_WR(bp, BAR_USTRORM_INTMEM +\r\nUSTORM_MEM_WORKAROUND_ADDRESS_OFFSET(func),\r\nU64_LO(fp->rx_comp_mapping));\r\nREG_WR(bp, BAR_USTRORM_INTMEM +\r\nUSTORM_MEM_WORKAROUND_ADDRESS_OFFSET(func) + 4,\r\nU64_HI(fp->rx_comp_mapping));\r\n}\r\n}\r\n}\r\nstatic void bnx2x_free_tx_skbs_queue(struct bnx2x_fastpath *fp)\r\n{\r\nu8 cos;\r\nstruct bnx2x *bp = fp->bp;\r\nfor_each_cos_in_tx_queue(fp, cos) {\r\nstruct bnx2x_fp_txdata *txdata = fp->txdata_ptr[cos];\r\nunsigned pkts_compl = 0, bytes_compl = 0;\r\nu16 sw_prod = txdata->tx_pkt_prod;\r\nu16 sw_cons = txdata->tx_pkt_cons;\r\nwhile (sw_cons != sw_prod) {\r\nbnx2x_free_tx_pkt(bp, txdata, TX_BD(sw_cons),\r\n&pkts_compl, &bytes_compl);\r\nsw_cons++;\r\n}\r\nnetdev_tx_reset_queue(\r\nnetdev_get_tx_queue(bp->dev,\r\ntxdata->txq_index));\r\n}\r\n}\r\nstatic void bnx2x_free_tx_skbs_cnic(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_tx_queue_cnic(bp, i) {\r\nbnx2x_free_tx_skbs_queue(&bp->fp[i]);\r\n}\r\n}\r\nstatic void bnx2x_free_tx_skbs(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_eth_queue(bp, i) {\r\nbnx2x_free_tx_skbs_queue(&bp->fp[i]);\r\n}\r\n}\r\nstatic void bnx2x_free_rx_bds(struct bnx2x_fastpath *fp)\r\n{\r\nstruct bnx2x *bp = fp->bp;\r\nint i;\r\nif (fp->rx_buf_ring == NULL)\r\nreturn;\r\nfor (i = 0; i < NUM_RX_BD; i++) {\r\nstruct sw_rx_bd *rx_buf = &fp->rx_buf_ring[i];\r\nu8 *data = rx_buf->data;\r\nif (data == NULL)\r\ncontinue;\r\ndma_unmap_single(&bp->pdev->dev,\r\ndma_unmap_addr(rx_buf, mapping),\r\nfp->rx_buf_size, DMA_FROM_DEVICE);\r\nrx_buf->data = NULL;\r\nbnx2x_frag_free(fp, data);\r\n}\r\n}\r\nstatic void bnx2x_free_rx_skbs_cnic(struct bnx2x *bp)\r\n{\r\nint j;\r\nfor_each_rx_queue_cnic(bp, j) {\r\nbnx2x_free_rx_bds(&bp->fp[j]);\r\n}\r\n}\r\nstatic void bnx2x_free_rx_skbs(struct bnx2x *bp)\r\n{\r\nint j;\r\nfor_each_eth_queue(bp, j) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[j];\r\nbnx2x_free_rx_bds(fp);\r\nif (fp->mode != TPA_MODE_DISABLED)\r\nbnx2x_free_tpa_pool(bp, fp, MAX_AGG_QS(bp));\r\n}\r\n}\r\nstatic void bnx2x_free_skbs_cnic(struct bnx2x *bp)\r\n{\r\nbnx2x_free_tx_skbs_cnic(bp);\r\nbnx2x_free_rx_skbs_cnic(bp);\r\n}\r\nvoid bnx2x_free_skbs(struct bnx2x *bp)\r\n{\r\nbnx2x_free_tx_skbs(bp);\r\nbnx2x_free_rx_skbs(bp);\r\n}\r\nvoid bnx2x_update_max_mf_config(struct bnx2x *bp, u32 value)\r\n{\r\nu32 mf_cfg = bp->mf_config[BP_VN(bp)];\r\nif (value != bnx2x_extract_max_cfg(bp, mf_cfg)) {\r\nmf_cfg &= ~FUNC_MF_CFG_MAX_BW_MASK;\r\nmf_cfg |= (value << FUNC_MF_CFG_MAX_BW_SHIFT)\r\n& FUNC_MF_CFG_MAX_BW_MASK;\r\nbnx2x_fw_command(bp, DRV_MSG_CODE_SET_MF_BW, mf_cfg);\r\n}\r\n}\r\nstatic void bnx2x_free_msix_irqs(struct bnx2x *bp, int nvecs)\r\n{\r\nint i, offset = 0;\r\nif (nvecs == offset)\r\nreturn;\r\nif (IS_PF(bp)) {\r\nfree_irq(bp->msix_table[offset].vector, bp->dev);\r\nDP(NETIF_MSG_IFDOWN, "released sp irq (%d)\n",\r\nbp->msix_table[offset].vector);\r\noffset++;\r\n}\r\nif (CNIC_SUPPORT(bp)) {\r\nif (nvecs == offset)\r\nreturn;\r\noffset++;\r\n}\r\nfor_each_eth_queue(bp, i) {\r\nif (nvecs == offset)\r\nreturn;\r\nDP(NETIF_MSG_IFDOWN, "about to release fp #%d->%d irq\n",\r\ni, bp->msix_table[offset].vector);\r\nfree_irq(bp->msix_table[offset++].vector, &bp->fp[i]);\r\n}\r\n}\r\nvoid bnx2x_free_irq(struct bnx2x *bp)\r\n{\r\nif (bp->flags & USING_MSIX_FLAG &&\r\n!(bp->flags & USING_SINGLE_MSIX_FLAG)) {\r\nint nvecs = BNX2X_NUM_ETH_QUEUES(bp) + CNIC_SUPPORT(bp);\r\nif (IS_PF(bp))\r\nnvecs++;\r\nbnx2x_free_msix_irqs(bp, nvecs);\r\n} else {\r\nfree_irq(bp->dev->irq, bp->dev);\r\n}\r\n}\r\nint bnx2x_enable_msix(struct bnx2x *bp)\r\n{\r\nint msix_vec = 0, i, rc;\r\nif (IS_PF(bp)) {\r\nbp->msix_table[msix_vec].entry = msix_vec;\r\nBNX2X_DEV_INFO("msix_table[0].entry = %d (slowpath)\n",\r\nbp->msix_table[0].entry);\r\nmsix_vec++;\r\n}\r\nif (CNIC_SUPPORT(bp)) {\r\nbp->msix_table[msix_vec].entry = msix_vec;\r\nBNX2X_DEV_INFO("msix_table[%d].entry = %d (CNIC)\n",\r\nmsix_vec, bp->msix_table[msix_vec].entry);\r\nmsix_vec++;\r\n}\r\nfor_each_eth_queue(bp, i) {\r\nbp->msix_table[msix_vec].entry = msix_vec;\r\nBNX2X_DEV_INFO("msix_table[%d].entry = %d (fastpath #%u)\n",\r\nmsix_vec, msix_vec, i);\r\nmsix_vec++;\r\n}\r\nDP(BNX2X_MSG_SP, "about to request enable msix with %d vectors\n",\r\nmsix_vec);\r\nrc = pci_enable_msix_range(bp->pdev, &bp->msix_table[0],\r\nBNX2X_MIN_MSIX_VEC_CNT(bp), msix_vec);\r\nif (rc == -ENOSPC) {\r\nrc = pci_enable_msix_range(bp->pdev, &bp->msix_table[0], 1, 1);\r\nif (rc < 0) {\r\nBNX2X_DEV_INFO("Single MSI-X is not attainable rc %d\n",\r\nrc);\r\ngoto no_msix;\r\n}\r\nBNX2X_DEV_INFO("Using single MSI-X vector\n");\r\nbp->flags |= USING_SINGLE_MSIX_FLAG;\r\nBNX2X_DEV_INFO("set number of queues to 1\n");\r\nbp->num_ethernet_queues = 1;\r\nbp->num_queues = bp->num_ethernet_queues + bp->num_cnic_queues;\r\n} else if (rc < 0) {\r\nBNX2X_DEV_INFO("MSI-X is not attainable rc %d\n", rc);\r\ngoto no_msix;\r\n} else if (rc < msix_vec) {\r\nint diff = msix_vec - rc;\r\nBNX2X_DEV_INFO("Trying to use less MSI-X vectors: %d\n", rc);\r\nbp->num_ethernet_queues -= diff;\r\nbp->num_queues = bp->num_ethernet_queues + bp->num_cnic_queues;\r\nBNX2X_DEV_INFO("New queue configuration set: %d\n",\r\nbp->num_queues);\r\n}\r\nbp->flags |= USING_MSIX_FLAG;\r\nreturn 0;\r\nno_msix:\r\nif (rc == -ENOMEM)\r\nbp->flags |= DISABLE_MSI_FLAG;\r\nreturn rc;\r\n}\r\nstatic int bnx2x_req_msix_irqs(struct bnx2x *bp)\r\n{\r\nint i, rc, offset = 0;\r\nif (IS_PF(bp)) {\r\nrc = request_irq(bp->msix_table[offset++].vector,\r\nbnx2x_msix_sp_int, 0,\r\nbp->dev->name, bp->dev);\r\nif (rc) {\r\nBNX2X_ERR("request sp irq failed\n");\r\nreturn -EBUSY;\r\n}\r\n}\r\nif (CNIC_SUPPORT(bp))\r\noffset++;\r\nfor_each_eth_queue(bp, i) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[i];\r\nsnprintf(fp->name, sizeof(fp->name), "%s-fp-%d",\r\nbp->dev->name, i);\r\nrc = request_irq(bp->msix_table[offset].vector,\r\nbnx2x_msix_fp_int, 0, fp->name, fp);\r\nif (rc) {\r\nBNX2X_ERR("request fp #%d irq (%d) failed rc %d\n", i,\r\nbp->msix_table[offset].vector, rc);\r\nbnx2x_free_msix_irqs(bp, offset);\r\nreturn -EBUSY;\r\n}\r\noffset++;\r\n}\r\ni = BNX2X_NUM_ETH_QUEUES(bp);\r\nif (IS_PF(bp)) {\r\noffset = 1 + CNIC_SUPPORT(bp);\r\nnetdev_info(bp->dev,\r\n"using MSI-X IRQs: sp %d fp[%d] %d ... fp[%d] %d\n",\r\nbp->msix_table[0].vector,\r\n0, bp->msix_table[offset].vector,\r\ni - 1, bp->msix_table[offset + i - 1].vector);\r\n} else {\r\noffset = CNIC_SUPPORT(bp);\r\nnetdev_info(bp->dev,\r\n"using MSI-X IRQs: fp[%d] %d ... fp[%d] %d\n",\r\n0, bp->msix_table[offset].vector,\r\ni - 1, bp->msix_table[offset + i - 1].vector);\r\n}\r\nreturn 0;\r\n}\r\nint bnx2x_enable_msi(struct bnx2x *bp)\r\n{\r\nint rc;\r\nrc = pci_enable_msi(bp->pdev);\r\nif (rc) {\r\nBNX2X_DEV_INFO("MSI is not attainable\n");\r\nreturn -1;\r\n}\r\nbp->flags |= USING_MSI_FLAG;\r\nreturn 0;\r\n}\r\nstatic int bnx2x_req_irq(struct bnx2x *bp)\r\n{\r\nunsigned long flags;\r\nunsigned int irq;\r\nif (bp->flags & (USING_MSI_FLAG | USING_MSIX_FLAG))\r\nflags = 0;\r\nelse\r\nflags = IRQF_SHARED;\r\nif (bp->flags & USING_MSIX_FLAG)\r\nirq = bp->msix_table[0].vector;\r\nelse\r\nirq = bp->pdev->irq;\r\nreturn request_irq(irq, bnx2x_interrupt, flags, bp->dev->name, bp->dev);\r\n}\r\nstatic int bnx2x_setup_irqs(struct bnx2x *bp)\r\n{\r\nint rc = 0;\r\nif (bp->flags & USING_MSIX_FLAG &&\r\n!(bp->flags & USING_SINGLE_MSIX_FLAG)) {\r\nrc = bnx2x_req_msix_irqs(bp);\r\nif (rc)\r\nreturn rc;\r\n} else {\r\nrc = bnx2x_req_irq(bp);\r\nif (rc) {\r\nBNX2X_ERR("IRQ request failed rc %d, aborting\n", rc);\r\nreturn rc;\r\n}\r\nif (bp->flags & USING_MSI_FLAG) {\r\nbp->dev->irq = bp->pdev->irq;\r\nnetdev_info(bp->dev, "using MSI IRQ %d\n",\r\nbp->dev->irq);\r\n}\r\nif (bp->flags & USING_MSIX_FLAG) {\r\nbp->dev->irq = bp->msix_table[0].vector;\r\nnetdev_info(bp->dev, "using MSIX IRQ %d\n",\r\nbp->dev->irq);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void bnx2x_napi_enable_cnic(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_rx_queue_cnic(bp, i) {\r\nnapi_enable(&bnx2x_fp(bp, i, napi));\r\n}\r\n}\r\nstatic void bnx2x_napi_enable(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_eth_queue(bp, i) {\r\nnapi_enable(&bnx2x_fp(bp, i, napi));\r\n}\r\n}\r\nstatic void bnx2x_napi_disable_cnic(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_rx_queue_cnic(bp, i) {\r\nnapi_disable(&bnx2x_fp(bp, i, napi));\r\n}\r\n}\r\nstatic void bnx2x_napi_disable(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_eth_queue(bp, i) {\r\nnapi_disable(&bnx2x_fp(bp, i, napi));\r\n}\r\n}\r\nvoid bnx2x_netif_start(struct bnx2x *bp)\r\n{\r\nif (netif_running(bp->dev)) {\r\nbnx2x_napi_enable(bp);\r\nif (CNIC_LOADED(bp))\r\nbnx2x_napi_enable_cnic(bp);\r\nbnx2x_int_enable(bp);\r\nif (bp->state == BNX2X_STATE_OPEN)\r\nnetif_tx_wake_all_queues(bp->dev);\r\n}\r\n}\r\nvoid bnx2x_netif_stop(struct bnx2x *bp, int disable_hw)\r\n{\r\nbnx2x_int_disable_sync(bp, disable_hw);\r\nbnx2x_napi_disable(bp);\r\nif (CNIC_LOADED(bp))\r\nbnx2x_napi_disable_cnic(bp);\r\n}\r\nu16 bnx2x_select_queue(struct net_device *dev, struct sk_buff *skb,\r\nvoid *accel_priv, select_queue_fallback_t fallback)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nif (CNIC_LOADED(bp) && !NO_FCOE(bp)) {\r\nstruct ethhdr *hdr = (struct ethhdr *)skb->data;\r\nu16 ether_type = ntohs(hdr->h_proto);\r\nif (ether_type == ETH_P_8021Q) {\r\nstruct vlan_ethhdr *vhdr =\r\n(struct vlan_ethhdr *)skb->data;\r\nether_type = ntohs(vhdr->h_vlan_encapsulated_proto);\r\n}\r\nif ((ether_type == ETH_P_FCOE) || (ether_type == ETH_P_FIP))\r\nreturn bnx2x_fcoe_tx(bp, txq_index);\r\n}\r\nreturn fallback(dev, skb) % BNX2X_NUM_ETH_QUEUES(bp);\r\n}\r\nvoid bnx2x_set_num_queues(struct bnx2x *bp)\r\n{\r\nbp->num_ethernet_queues = bnx2x_calc_num_queues(bp);\r\nif (IS_MF_STORAGE_ONLY(bp))\r\nbp->num_ethernet_queues = 1;\r\nbp->num_cnic_queues = CNIC_SUPPORT(bp);\r\nbp->num_queues = bp->num_ethernet_queues + bp->num_cnic_queues;\r\nBNX2X_DEV_INFO("set number of queues to %d\n", bp->num_queues);\r\n}\r\nstatic int bnx2x_set_real_num_queues(struct bnx2x *bp, int include_cnic)\r\n{\r\nint rc, tx, rx;\r\ntx = BNX2X_NUM_ETH_QUEUES(bp) * bp->max_cos;\r\nrx = BNX2X_NUM_ETH_QUEUES(bp);\r\nif (include_cnic && !NO_FCOE(bp)) {\r\nrx++;\r\ntx++;\r\n}\r\nrc = netif_set_real_num_tx_queues(bp->dev, tx);\r\nif (rc) {\r\nBNX2X_ERR("Failed to set real number of Tx queues: %d\n", rc);\r\nreturn rc;\r\n}\r\nrc = netif_set_real_num_rx_queues(bp->dev, rx);\r\nif (rc) {\r\nBNX2X_ERR("Failed to set real number of Rx queues: %d\n", rc);\r\nreturn rc;\r\n}\r\nDP(NETIF_MSG_IFUP, "Setting real num queues to (tx, rx) (%d, %d)\n",\r\ntx, rx);\r\nreturn rc;\r\n}\r\nstatic void bnx2x_set_rx_buf_size(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_queue(bp, i) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[i];\r\nu32 mtu;\r\nif (IS_FCOE_IDX(i))\r\nmtu = BNX2X_FCOE_MINI_JUMBO_MTU;\r\nelse\r\nmtu = bp->dev->mtu;\r\nfp->rx_buf_size = BNX2X_FW_RX_ALIGN_START +\r\nIP_HEADER_ALIGNMENT_PADDING +\r\nETH_OVREHEAD +\r\nmtu +\r\nBNX2X_FW_RX_ALIGN_END;\r\nif (fp->rx_buf_size + NET_SKB_PAD <= PAGE_SIZE)\r\nfp->rx_frag_size = fp->rx_buf_size + NET_SKB_PAD;\r\nelse\r\nfp->rx_frag_size = 0;\r\n}\r\n}\r\nstatic int bnx2x_init_rss(struct bnx2x *bp)\r\n{\r\nint i;\r\nu8 num_eth_queues = BNX2X_NUM_ETH_QUEUES(bp);\r\nfor (i = 0; i < sizeof(bp->rss_conf_obj.ind_table); i++)\r\nbp->rss_conf_obj.ind_table[i] =\r\nbp->fp->cl_id +\r\nethtool_rxfh_indir_default(i, num_eth_queues);\r\nreturn bnx2x_config_rss_eth(bp, bp->port.pmf || !CHIP_IS_E1x(bp));\r\n}\r\nint bnx2x_rss(struct bnx2x *bp, struct bnx2x_rss_config_obj *rss_obj,\r\nbool config_hash, bool enable)\r\n{\r\nstruct bnx2x_config_rss_params params = {NULL};\r\nparams.rss_obj = rss_obj;\r\n__set_bit(RAMROD_COMP_WAIT, &params.ramrod_flags);\r\nif (enable) {\r\n__set_bit(BNX2X_RSS_MODE_REGULAR, &params.rss_flags);\r\n__set_bit(BNX2X_RSS_IPV4, &params.rss_flags);\r\n__set_bit(BNX2X_RSS_IPV4_TCP, &params.rss_flags);\r\n__set_bit(BNX2X_RSS_IPV6, &params.rss_flags);\r\n__set_bit(BNX2X_RSS_IPV6_TCP, &params.rss_flags);\r\nif (rss_obj->udp_rss_v4)\r\n__set_bit(BNX2X_RSS_IPV4_UDP, &params.rss_flags);\r\nif (rss_obj->udp_rss_v6)\r\n__set_bit(BNX2X_RSS_IPV6_UDP, &params.rss_flags);\r\nif (!CHIP_IS_E1x(bp)) {\r\n__set_bit(BNX2X_RSS_IPV4_VXLAN, &params.rss_flags);\r\n__set_bit(BNX2X_RSS_IPV6_VXLAN, &params.rss_flags);\r\n__set_bit(BNX2X_RSS_TUNN_INNER_HDRS, &params.rss_flags);\r\n}\r\n} else {\r\n__set_bit(BNX2X_RSS_MODE_DISABLED, &params.rss_flags);\r\n}\r\nparams.rss_result_mask = MULTI_MASK;\r\nmemcpy(params.ind_table, rss_obj->ind_table, sizeof(params.ind_table));\r\nif (config_hash) {\r\nnetdev_rss_key_fill(params.rss_key, T_ETH_RSS_KEY * 4);\r\n__set_bit(BNX2X_RSS_SET_SRCH, &params.rss_flags);\r\n}\r\nif (IS_PF(bp))\r\nreturn bnx2x_config_rss(bp, &params);\r\nelse\r\nreturn bnx2x_vfpf_config_rss(bp, &params);\r\n}\r\nstatic int bnx2x_init_hw(struct bnx2x *bp, u32 load_code)\r\n{\r\nstruct bnx2x_func_state_params func_params = {NULL};\r\n__set_bit(RAMROD_COMP_WAIT, &func_params.ramrod_flags);\r\nfunc_params.f_obj = &bp->func_obj;\r\nfunc_params.cmd = BNX2X_F_CMD_HW_INIT;\r\nfunc_params.params.hw_init.load_phase = load_code;\r\nreturn bnx2x_func_state_change(bp, &func_params);\r\n}\r\nvoid bnx2x_squeeze_objects(struct bnx2x *bp)\r\n{\r\nint rc;\r\nunsigned long ramrod_flags = 0, vlan_mac_flags = 0;\r\nstruct bnx2x_mcast_ramrod_params rparam = {NULL};\r\nstruct bnx2x_vlan_mac_obj *mac_obj = &bp->sp_objs->mac_obj;\r\n__set_bit(RAMROD_COMP_WAIT, &ramrod_flags);\r\n__set_bit(RAMROD_DRV_CLR_ONLY, &ramrod_flags);\r\n__set_bit(BNX2X_ETH_MAC, &vlan_mac_flags);\r\nrc = mac_obj->delete_all(bp, &bp->sp_objs->mac_obj, &vlan_mac_flags,\r\n&ramrod_flags);\r\nif (rc != 0)\r\nBNX2X_ERR("Failed to clean ETH MACs: %d\n", rc);\r\nvlan_mac_flags = 0;\r\n__set_bit(BNX2X_UC_LIST_MAC, &vlan_mac_flags);\r\nrc = mac_obj->delete_all(bp, mac_obj, &vlan_mac_flags,\r\n&ramrod_flags);\r\nif (rc != 0)\r\nBNX2X_ERR("Failed to clean UC list MACs: %d\n", rc);\r\nrparam.mcast_obj = &bp->mcast_obj;\r\n__set_bit(RAMROD_DRV_CLR_ONLY, &rparam.ramrod_flags);\r\nnetif_addr_lock_bh(bp->dev);\r\nrc = bnx2x_config_mcast(bp, &rparam, BNX2X_MCAST_CMD_DEL);\r\nif (rc < 0)\r\nBNX2X_ERR("Failed to add a new DEL command to a multi-cast object: %d\n",\r\nrc);\r\nrc = bnx2x_config_mcast(bp, &rparam, BNX2X_MCAST_CMD_CONT);\r\nwhile (rc != 0) {\r\nif (rc < 0) {\r\nBNX2X_ERR("Failed to clean multi-cast object: %d\n",\r\nrc);\r\nnetif_addr_unlock_bh(bp->dev);\r\nreturn;\r\n}\r\nrc = bnx2x_config_mcast(bp, &rparam, BNX2X_MCAST_CMD_CONT);\r\n}\r\nnetif_addr_unlock_bh(bp->dev);\r\n}\r\nstatic void bnx2x_free_fw_stats_mem(struct bnx2x *bp)\r\n{\r\nBNX2X_PCI_FREE(bp->fw_stats, bp->fw_stats_mapping,\r\nbp->fw_stats_data_sz + bp->fw_stats_req_sz);\r\nreturn;\r\n}\r\nstatic int bnx2x_alloc_fw_stats_mem(struct bnx2x *bp)\r\n{\r\nint num_groups, vf_headroom = 0;\r\nint is_fcoe_stats = NO_FCOE(bp) ? 0 : 1;\r\nu8 num_queue_stats = BNX2X_NUM_ETH_QUEUES(bp) + is_fcoe_stats;\r\nbp->fw_stats_num = 2 + is_fcoe_stats + num_queue_stats;\r\nif (IS_SRIOV(bp))\r\nvf_headroom = bnx2x_vf_headroom(bp);\r\nnum_groups =\r\n(((bp->fw_stats_num + vf_headroom) / STATS_QUERY_CMD_COUNT) +\r\n(((bp->fw_stats_num + vf_headroom) % STATS_QUERY_CMD_COUNT) ?\r\n1 : 0));\r\nDP(BNX2X_MSG_SP, "stats fw_stats_num %d, vf headroom %d, num_groups %d\n",\r\nbp->fw_stats_num, vf_headroom, num_groups);\r\nbp->fw_stats_req_sz = sizeof(struct stats_query_header) +\r\nnum_groups * sizeof(struct stats_query_cmd_group);\r\nbp->fw_stats_data_sz = sizeof(struct per_port_stats) +\r\nsizeof(struct per_pf_stats) +\r\nsizeof(struct fcoe_statistics_params) +\r\nsizeof(struct per_queue_stats) * num_queue_stats +\r\nsizeof(struct stats_counter);\r\nbp->fw_stats = BNX2X_PCI_ALLOC(&bp->fw_stats_mapping,\r\nbp->fw_stats_data_sz + bp->fw_stats_req_sz);\r\nif (!bp->fw_stats)\r\ngoto alloc_mem_err;\r\nbp->fw_stats_req = (struct bnx2x_fw_stats_req *)bp->fw_stats;\r\nbp->fw_stats_req_mapping = bp->fw_stats_mapping;\r\nbp->fw_stats_data = (struct bnx2x_fw_stats_data *)\r\n((u8 *)bp->fw_stats + bp->fw_stats_req_sz);\r\nbp->fw_stats_data_mapping = bp->fw_stats_mapping +\r\nbp->fw_stats_req_sz;\r\nDP(BNX2X_MSG_SP, "statistics request base address set to %x %x\n",\r\nU64_HI(bp->fw_stats_req_mapping),\r\nU64_LO(bp->fw_stats_req_mapping));\r\nDP(BNX2X_MSG_SP, "statistics data base address set to %x %x\n",\r\nU64_HI(bp->fw_stats_data_mapping),\r\nU64_LO(bp->fw_stats_data_mapping));\r\nreturn 0;\r\nalloc_mem_err:\r\nbnx2x_free_fw_stats_mem(bp);\r\nBNX2X_ERR("Can't allocate FW stats memory\n");\r\nreturn -ENOMEM;\r\n}\r\nstatic int bnx2x_nic_load_request(struct bnx2x *bp, u32 *load_code)\r\n{\r\nu32 param;\r\nbp->fw_seq =\r\n(SHMEM_RD(bp, func_mb[BP_FW_MB_IDX(bp)].drv_mb_header) &\r\nDRV_MSG_SEQ_NUMBER_MASK);\r\nBNX2X_DEV_INFO("fw_seq 0x%08x\n", bp->fw_seq);\r\nbp->fw_drv_pulse_wr_seq =\r\n(SHMEM_RD(bp, func_mb[BP_FW_MB_IDX(bp)].drv_pulse_mb) &\r\nDRV_PULSE_SEQ_MASK);\r\nBNX2X_DEV_INFO("drv_pulse 0x%x\n", bp->fw_drv_pulse_wr_seq);\r\nparam = DRV_MSG_CODE_LOAD_REQ_WITH_LFA;\r\nif (IS_MF_SD(bp) && bnx2x_port_after_undi(bp))\r\nparam |= DRV_MSG_CODE_LOAD_REQ_FORCE_LFA;\r\n(*load_code) = bnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_REQ, param);\r\nif (!(*load_code)) {\r\nBNX2X_ERR("MCP response failure, aborting\n");\r\nreturn -EBUSY;\r\n}\r\nif ((*load_code) == FW_MSG_CODE_DRV_LOAD_REFUSED) {\r\nBNX2X_ERR("MCP refused load request, aborting\n");\r\nreturn -EBUSY;\r\n}\r\nreturn 0;\r\n}\r\nint bnx2x_compare_fw_ver(struct bnx2x *bp, u32 load_code, bool print_err)\r\n{\r\nif (load_code != FW_MSG_CODE_DRV_LOAD_COMMON_CHIP &&\r\nload_code != FW_MSG_CODE_DRV_LOAD_COMMON) {\r\nu32 my_fw = (BCM_5710_FW_MAJOR_VERSION) +\r\n(BCM_5710_FW_MINOR_VERSION << 8) +\r\n(BCM_5710_FW_REVISION_VERSION << 16) +\r\n(BCM_5710_FW_ENGINEERING_VERSION << 24);\r\nu32 loaded_fw = REG_RD(bp, XSEM_REG_PRAM);\r\nDP(BNX2X_MSG_SP, "loaded fw %x, my fw %x\n",\r\nloaded_fw, my_fw);\r\nif (my_fw != loaded_fw) {\r\nif (print_err)\r\nBNX2X_ERR("bnx2x with FW %x was already loaded which mismatches my %x FW. Aborting\n",\r\nloaded_fw, my_fw);\r\nelse\r\nBNX2X_DEV_INFO("bnx2x with FW %x was already loaded which mismatches my %x FW, possibly due to MF UNDI\n",\r\nloaded_fw, my_fw);\r\nreturn -EBUSY;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int bnx2x_nic_load_no_mcp(struct bnx2x *bp, int port)\r\n{\r\nint path = BP_PATH(bp);\r\nDP(NETIF_MSG_IFUP, "NO MCP - load counts[%d] %d, %d, %d\n",\r\npath, bnx2x_load_count[path][0], bnx2x_load_count[path][1],\r\nbnx2x_load_count[path][2]);\r\nbnx2x_load_count[path][0]++;\r\nbnx2x_load_count[path][1 + port]++;\r\nDP(NETIF_MSG_IFUP, "NO MCP - new load counts[%d] %d, %d, %d\n",\r\npath, bnx2x_load_count[path][0], bnx2x_load_count[path][1],\r\nbnx2x_load_count[path][2]);\r\nif (bnx2x_load_count[path][0] == 1)\r\nreturn FW_MSG_CODE_DRV_LOAD_COMMON;\r\nelse if (bnx2x_load_count[path][1 + port] == 1)\r\nreturn FW_MSG_CODE_DRV_LOAD_PORT;\r\nelse\r\nreturn FW_MSG_CODE_DRV_LOAD_FUNCTION;\r\n}\r\nstatic void bnx2x_nic_load_pmf(struct bnx2x *bp, u32 load_code)\r\n{\r\nif ((load_code == FW_MSG_CODE_DRV_LOAD_COMMON) ||\r\n(load_code == FW_MSG_CODE_DRV_LOAD_COMMON_CHIP) ||\r\n(load_code == FW_MSG_CODE_DRV_LOAD_PORT)) {\r\nbp->port.pmf = 1;\r\nsmp_mb();\r\n} else {\r\nbp->port.pmf = 0;\r\n}\r\nDP(NETIF_MSG_LINK, "pmf %d\n", bp->port.pmf);\r\n}\r\nstatic void bnx2x_nic_load_afex_dcc(struct bnx2x *bp, int load_code)\r\n{\r\nif (((load_code == FW_MSG_CODE_DRV_LOAD_COMMON) ||\r\n(load_code == FW_MSG_CODE_DRV_LOAD_COMMON_CHIP)) &&\r\n(bp->common.shmem2_base)) {\r\nif (SHMEM2_HAS(bp, dcc_support))\r\nSHMEM2_WR(bp, dcc_support,\r\n(SHMEM_DCC_SUPPORT_DISABLE_ENABLE_PF_TLV |\r\nSHMEM_DCC_SUPPORT_BANDWIDTH_ALLOCATION_TLV));\r\nif (SHMEM2_HAS(bp, afex_driver_support))\r\nSHMEM2_WR(bp, afex_driver_support,\r\nSHMEM_AFEX_SUPPORTED_VERSION_ONE);\r\n}\r\nbp->afex_def_vlan_tag = -1;\r\n}\r\nstatic void bnx2x_bz_fp(struct bnx2x *bp, int index)\r\n{\r\nstruct bnx2x_fastpath *fp = &bp->fp[index];\r\nint cos;\r\nstruct napi_struct orig_napi = fp->napi;\r\nstruct bnx2x_agg_info *orig_tpa_info = fp->tpa_info;\r\nif (fp->tpa_info)\r\nmemset(fp->tpa_info, 0, ETH_MAX_AGGREGATION_QUEUES_E1H_E2 *\r\nsizeof(struct bnx2x_agg_info));\r\nmemset(fp, 0, sizeof(*fp));\r\nfp->napi = orig_napi;\r\nfp->tpa_info = orig_tpa_info;\r\nfp->bp = bp;\r\nfp->index = index;\r\nif (IS_ETH_FP(fp))\r\nfp->max_cos = bp->max_cos;\r\nelse\r\nfp->max_cos = 1;\r\nif (IS_FCOE_FP(fp))\r\nfp->txdata_ptr[0] = &bp->bnx2x_txq[FCOE_TXQ_IDX(bp)];\r\nif (IS_ETH_FP(fp))\r\nfor_each_cos_in_tx_queue(fp, cos)\r\nfp->txdata_ptr[cos] = &bp->bnx2x_txq[cos *\r\nBNX2X_NUM_ETH_QUEUES(bp) + index];\r\nif (bp->dev->features & NETIF_F_LRO)\r\nfp->mode = TPA_MODE_LRO;\r\nelse if (bp->dev->features & NETIF_F_GRO &&\r\nbnx2x_mtu_allows_gro(bp->dev->mtu))\r\nfp->mode = TPA_MODE_GRO;\r\nelse\r\nfp->mode = TPA_MODE_DISABLED;\r\nif (bp->disable_tpa || IS_FCOE_FP(fp))\r\nfp->mode = TPA_MODE_DISABLED;\r\n}\r\nvoid bnx2x_set_os_driver_state(struct bnx2x *bp, u32 state)\r\n{\r\nu32 cur;\r\nif (!IS_MF_BD(bp) || !SHMEM2_HAS(bp, os_driver_state) || IS_VF(bp))\r\nreturn;\r\ncur = SHMEM2_RD(bp, os_driver_state[BP_FW_MB_IDX(bp)]);\r\nDP(NETIF_MSG_IFUP, "Driver state %08x-->%08x\n",\r\ncur, state);\r\nSHMEM2_WR(bp, os_driver_state[BP_FW_MB_IDX(bp)], state);\r\n}\r\nint bnx2x_load_cnic(struct bnx2x *bp)\r\n{\r\nint i, rc, port = BP_PORT(bp);\r\nDP(NETIF_MSG_IFUP, "Starting CNIC-related load\n");\r\nmutex_init(&bp->cnic_mutex);\r\nif (IS_PF(bp)) {\r\nrc = bnx2x_alloc_mem_cnic(bp);\r\nif (rc) {\r\nBNX2X_ERR("Unable to allocate bp memory for cnic\n");\r\nLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic0);\r\n}\r\n}\r\nrc = bnx2x_alloc_fp_mem_cnic(bp);\r\nif (rc) {\r\nBNX2X_ERR("Unable to allocate memory for cnic fps\n");\r\nLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic0);\r\n}\r\nrc = bnx2x_set_real_num_queues(bp, 1);\r\nif (rc) {\r\nBNX2X_ERR("Unable to set real_num_queues including cnic\n");\r\nLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic0);\r\n}\r\nbnx2x_add_all_napi_cnic(bp);\r\nDP(NETIF_MSG_IFUP, "cnic napi added\n");\r\nbnx2x_napi_enable_cnic(bp);\r\nrc = bnx2x_init_hw_func_cnic(bp);\r\nif (rc)\r\nLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic1);\r\nbnx2x_nic_init_cnic(bp);\r\nif (IS_PF(bp)) {\r\nREG_WR(bp, TM_REG_EN_LINEAR0_TIMER + port*4, 1);\r\nfor_each_cnic_queue(bp, i) {\r\nrc = bnx2x_setup_queue(bp, &bp->fp[i], 0);\r\nif (rc) {\r\nBNX2X_ERR("Queue setup failed\n");\r\nLOAD_ERROR_EXIT(bp, load_error_cnic2);\r\n}\r\n}\r\n}\r\nbnx2x_set_rx_mode_inner(bp);\r\nbnx2x_get_iscsi_info(bp);\r\nbnx2x_setup_cnic_irq_info(bp);\r\nbnx2x_setup_cnic_info(bp);\r\nbp->cnic_loaded = true;\r\nif (bp->state == BNX2X_STATE_OPEN)\r\nbnx2x_cnic_notify(bp, CNIC_CTL_START_CMD);\r\nDP(NETIF_MSG_IFUP, "Ending successfully CNIC-related load\n");\r\nreturn 0;\r\n#ifndef BNX2X_STOP_ON_ERROR\r\nload_error_cnic2:\r\nREG_WR(bp, TM_REG_EN_LINEAR0_TIMER + port*4, 0);\r\nload_error_cnic1:\r\nbnx2x_napi_disable_cnic(bp);\r\nif (bnx2x_set_real_num_queues(bp, 0))\r\nBNX2X_ERR("Unable to set real_num_queues not including cnic\n");\r\nload_error_cnic0:\r\nBNX2X_ERR("CNIC-related load failed\n");\r\nbnx2x_free_fp_mem_cnic(bp);\r\nbnx2x_free_mem_cnic(bp);\r\nreturn rc;\r\n#endif\r\n}\r\nint bnx2x_nic_load(struct bnx2x *bp, int load_mode)\r\n{\r\nint port = BP_PORT(bp);\r\nint i, rc = 0, load_code = 0;\r\nDP(NETIF_MSG_IFUP, "Starting NIC load\n");\r\nDP(NETIF_MSG_IFUP,\r\n"CNIC is %s\n", CNIC_ENABLED(bp) ? "enabled" : "disabled");\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (unlikely(bp->panic)) {\r\nBNX2X_ERR("Can't load NIC when there is panic\n");\r\nreturn -EPERM;\r\n}\r\n#endif\r\nbp->state = BNX2X_STATE_OPENING_WAIT4_LOAD;\r\nmemset(&bp->last_reported_link, 0, sizeof(bp->last_reported_link));\r\n__set_bit(BNX2X_LINK_REPORT_LINK_DOWN,\r\n&bp->last_reported_link.link_report_flags);\r\nif (IS_PF(bp))\r\nbnx2x_ilt_set_info(bp);\r\nDP(NETIF_MSG_IFUP, "num queues: %d", bp->num_queues);\r\nfor_each_queue(bp, i)\r\nbnx2x_bz_fp(bp, i);\r\nmemset(bp->bnx2x_txq, 0, (BNX2X_MAX_RSS_COUNT(bp) * BNX2X_MULTI_TX_COS +\r\nbp->num_cnic_queues) *\r\nsizeof(struct bnx2x_fp_txdata));\r\nbp->fcoe_init = false;\r\nbnx2x_set_rx_buf_size(bp);\r\nif (IS_PF(bp)) {\r\nrc = bnx2x_alloc_mem(bp);\r\nif (rc) {\r\nBNX2X_ERR("Unable to allocate bp memory\n");\r\nreturn rc;\r\n}\r\n}\r\nrc = bnx2x_alloc_fp_mem(bp);\r\nif (rc) {\r\nBNX2X_ERR("Unable to allocate memory for fps\n");\r\nLOAD_ERROR_EXIT(bp, load_error0);\r\n}\r\nif (bnx2x_alloc_fw_stats_mem(bp))\r\nLOAD_ERROR_EXIT(bp, load_error0);\r\nif (IS_VF(bp)) {\r\nrc = bnx2x_vfpf_init(bp);\r\nif (rc)\r\nLOAD_ERROR_EXIT(bp, load_error0);\r\n}\r\nrc = bnx2x_set_real_num_queues(bp, 0);\r\nif (rc) {\r\nBNX2X_ERR("Unable to set real_num_queues\n");\r\nLOAD_ERROR_EXIT(bp, load_error0);\r\n}\r\nbnx2x_setup_tc(bp->dev, bp->max_cos);\r\nbnx2x_add_all_napi(bp);\r\nDP(NETIF_MSG_IFUP, "napi added\n");\r\nbnx2x_napi_enable(bp);\r\nif (IS_PF(bp)) {\r\nbnx2x_set_pf_load(bp);\r\nif (!BP_NOMCP(bp)) {\r\nrc = bnx2x_nic_load_request(bp, &load_code);\r\nif (rc)\r\nLOAD_ERROR_EXIT(bp, load_error1);\r\nrc = bnx2x_compare_fw_ver(bp, load_code, true);\r\nif (rc) {\r\nbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\r\nLOAD_ERROR_EXIT(bp, load_error2);\r\n}\r\n} else {\r\nload_code = bnx2x_nic_load_no_mcp(bp, port);\r\n}\r\nbnx2x_nic_load_pmf(bp, load_code);\r\nbnx2x__init_func_obj(bp);\r\nrc = bnx2x_init_hw(bp, load_code);\r\nif (rc) {\r\nBNX2X_ERR("HW init failed, aborting\n");\r\nbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\r\nLOAD_ERROR_EXIT(bp, load_error2);\r\n}\r\n}\r\nbnx2x_pre_irq_nic_init(bp);\r\nrc = bnx2x_setup_irqs(bp);\r\nif (rc) {\r\nBNX2X_ERR("setup irqs failed\n");\r\nif (IS_PF(bp))\r\nbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\r\nLOAD_ERROR_EXIT(bp, load_error2);\r\n}\r\nif (IS_PF(bp)) {\r\nbnx2x_post_irq_nic_init(bp, load_code);\r\nbnx2x_init_bp_objs(bp);\r\nbnx2x_iov_nic_init(bp);\r\nbp->afex_def_vlan_tag = -1;\r\nbnx2x_nic_load_afex_dcc(bp, load_code);\r\nbp->state = BNX2X_STATE_OPENING_WAIT4_PORT;\r\nrc = bnx2x_func_start(bp);\r\nif (rc) {\r\nBNX2X_ERR("Function start failed!\n");\r\nbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\n}\r\nif (!BP_NOMCP(bp)) {\r\nload_code = bnx2x_fw_command(bp,\r\nDRV_MSG_CODE_LOAD_DONE, 0);\r\nif (!load_code) {\r\nBNX2X_ERR("MCP response failure, aborting\n");\r\nrc = -EBUSY;\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\n}\r\n}\r\nbnx2x_update_coalesce(bp);\r\n}\r\nrc = bnx2x_setup_leading(bp);\r\nif (rc) {\r\nBNX2X_ERR("Setup leading failed!\n");\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\n}\r\nfor_each_nondefault_eth_queue(bp, i) {\r\nif (IS_PF(bp))\r\nrc = bnx2x_setup_queue(bp, &bp->fp[i], false);\r\nelse\r\nrc = bnx2x_vfpf_setup_q(bp, &bp->fp[i], false);\r\nif (rc) {\r\nBNX2X_ERR("Queue %d setup failed\n", i);\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\n}\r\n}\r\nrc = bnx2x_init_rss(bp);\r\nif (rc) {\r\nBNX2X_ERR("PF RSS init failed\n");\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\n}\r\nbp->state = BNX2X_STATE_OPEN;\r\nif (IS_PF(bp))\r\nrc = bnx2x_set_eth_mac(bp, true);\r\nelse\r\nrc = bnx2x_vfpf_config_mac(bp, bp->dev->dev_addr, bp->fp->index,\r\ntrue);\r\nif (rc) {\r\nBNX2X_ERR("Setting Ethernet MAC failed\n");\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\n}\r\nif (IS_PF(bp) && bp->pending_max) {\r\nbnx2x_update_max_mf_config(bp, bp->pending_max);\r\nbp->pending_max = 0;\r\n}\r\nif (bp->port.pmf) {\r\nrc = bnx2x_initial_phy_init(bp, load_mode);\r\nif (rc)\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\n}\r\nbp->link_params.feature_config_flags &= ~FEATURE_CONFIG_BOOT_FROM_SAN;\r\nrc = bnx2x_vlan_reconfigure_vid(bp);\r\nif (rc)\r\nLOAD_ERROR_EXIT(bp, load_error3);\r\nbnx2x_set_rx_mode_inner(bp);\r\nif (bp->flags & PTP_SUPPORTED) {\r\nbnx2x_init_ptp(bp);\r\nbnx2x_configure_ptp_filters(bp);\r\n}\r\nswitch (load_mode) {\r\ncase LOAD_NORMAL:\r\nnetif_tx_wake_all_queues(bp->dev);\r\nbreak;\r\ncase LOAD_OPEN:\r\nnetif_tx_start_all_queues(bp->dev);\r\nsmp_mb__after_atomic();\r\nbreak;\r\ncase LOAD_DIAG:\r\ncase LOAD_LOOPBACK_EXT:\r\nbp->state = BNX2X_STATE_DIAG;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (bp->port.pmf)\r\nbnx2x_update_drv_flags(bp, 1 << DRV_FLAGS_PORT_MASK, 0);\r\nelse\r\nbnx2x__link_status_update(bp);\r\nmod_timer(&bp->timer, jiffies + bp->current_interval);\r\nif (CNIC_ENABLED(bp))\r\nbnx2x_load_cnic(bp);\r\nif (IS_PF(bp))\r\nbnx2x_schedule_sp_rtnl(bp, BNX2X_SP_RTNL_GET_DRV_VERSION, 0);\r\nif (IS_PF(bp) && SHMEM2_HAS(bp, drv_capabilities_flag)) {\r\nu32 val;\r\nval = SHMEM2_RD(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)]);\r\nval &= ~DRV_FLAGS_MTU_MASK;\r\nval |= (bp->dev->mtu << DRV_FLAGS_MTU_SHIFT);\r\nSHMEM2_WR(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)],\r\nval | DRV_FLAGS_CAPABILITIES_LOADED_SUPPORTED |\r\nDRV_FLAGS_CAPABILITIES_LOADED_L2);\r\n}\r\nif (IS_PF(bp) && !bnx2x_wait_sp_comp(bp, ~0x0UL)) {\r\nBNX2X_ERR("Timeout waiting for SP elements to complete\n");\r\nbnx2x_nic_unload(bp, UNLOAD_CLOSE, false);\r\nreturn -EBUSY;\r\n}\r\nif (IS_PF(bp))\r\nbnx2x_update_mfw_dump(bp);\r\nif (bp->port.pmf && (bp->state != BNX2X_STATE_DIAG))\r\nbnx2x_dcbx_init(bp, false);\r\nif (!IS_MF_SD_STORAGE_PERSONALITY_ONLY(bp))\r\nbnx2x_set_os_driver_state(bp, OS_DRIVER_STATE_ACTIVE);\r\nDP(NETIF_MSG_IFUP, "Ending successfully NIC load\n");\r\nreturn 0;\r\n#ifndef BNX2X_STOP_ON_ERROR\r\nload_error3:\r\nif (IS_PF(bp)) {\r\nbnx2x_int_disable_sync(bp, 1);\r\nbnx2x_squeeze_objects(bp);\r\n}\r\nbnx2x_free_skbs(bp);\r\nfor_each_rx_queue(bp, i)\r\nbnx2x_free_rx_sge_range(bp, bp->fp + i, NUM_RX_SGE);\r\nbnx2x_free_irq(bp);\r\nload_error2:\r\nif (IS_PF(bp) && !BP_NOMCP(bp)) {\r\nbnx2x_fw_command(bp, DRV_MSG_CODE_UNLOAD_REQ_WOL_MCP, 0);\r\nbnx2x_fw_command(bp, DRV_MSG_CODE_UNLOAD_DONE, 0);\r\n}\r\nbp->port.pmf = 0;\r\nload_error1:\r\nbnx2x_napi_disable(bp);\r\nbnx2x_del_all_napi(bp);\r\nif (IS_PF(bp))\r\nbnx2x_clear_pf_load(bp);\r\nload_error0:\r\nbnx2x_free_fw_stats_mem(bp);\r\nbnx2x_free_fp_mem(bp);\r\nbnx2x_free_mem(bp);\r\nreturn rc;\r\n#endif\r\n}\r\nint bnx2x_drain_tx_queues(struct bnx2x *bp)\r\n{\r\nu8 rc = 0, cos, i;\r\nfor_each_tx_queue(bp, i) {\r\nstruct bnx2x_fastpath *fp = &bp->fp[i];\r\nfor_each_cos_in_tx_queue(fp, cos)\r\nrc = bnx2x_clean_tx_queue(bp, fp->txdata_ptr[cos]);\r\nif (rc)\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nint bnx2x_nic_unload(struct bnx2x *bp, int unload_mode, bool keep_link)\r\n{\r\nint i;\r\nbool global = false;\r\nDP(NETIF_MSG_IFUP, "Starting NIC unload\n");\r\nif (!IS_MF_SD_STORAGE_PERSONALITY_ONLY(bp))\r\nbnx2x_set_os_driver_state(bp, OS_DRIVER_STATE_DISABLED);\r\nif (IS_PF(bp) && SHMEM2_HAS(bp, drv_capabilities_flag)) {\r\nu32 val;\r\nval = SHMEM2_RD(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)]);\r\nSHMEM2_WR(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)],\r\nval & ~DRV_FLAGS_CAPABILITIES_LOADED_L2);\r\n}\r\nif (IS_PF(bp) && bp->recovery_state != BNX2X_RECOVERY_DONE &&\r\n(bp->state == BNX2X_STATE_CLOSED ||\r\nbp->state == BNX2X_STATE_ERROR)) {\r\nbp->recovery_state = BNX2X_RECOVERY_DONE;\r\nbp->is_leader = 0;\r\nbnx2x_release_leader_lock(bp);\r\nsmp_mb();\r\nDP(NETIF_MSG_IFDOWN, "Releasing a leadership...\n");\r\nBNX2X_ERR("Can't unload in closed or error state\n");\r\nreturn -EINVAL;\r\n}\r\nif (bp->state == BNX2X_STATE_CLOSED || bp->state == BNX2X_STATE_ERROR)\r\nreturn 0;\r\nbp->state = BNX2X_STATE_CLOSING_WAIT4_HALT;\r\nsmp_mb();\r\nbnx2x_iov_channel_down(bp);\r\nif (CNIC_LOADED(bp))\r\nbnx2x_cnic_notify(bp, CNIC_CTL_STOP_CMD);\r\nbnx2x_tx_disable(bp);\r\nnetdev_reset_tc(bp->dev);\r\nbp->rx_mode = BNX2X_RX_MODE_NONE;\r\ndel_timer_sync(&bp->timer);\r\nif (IS_PF(bp)) {\r\nbp->fw_drv_pulse_wr_seq |= DRV_PULSE_ALWAYS_ALIVE;\r\nbnx2x_drv_pulse(bp);\r\nbnx2x_stats_handle(bp, STATS_EVENT_STOP);\r\nbnx2x_save_statistics(bp);\r\n}\r\nif (unload_mode != UNLOAD_RECOVERY)\r\nbnx2x_drain_tx_queues(bp);\r\nif (IS_VF(bp))\r\nbnx2x_vfpf_close_vf(bp);\r\nelse if (unload_mode != UNLOAD_RECOVERY)\r\nbnx2x_chip_cleanup(bp, unload_mode, keep_link);\r\nelse {\r\nbnx2x_send_unload_req(bp, unload_mode);\r\nif (!CHIP_IS_E1x(bp))\r\nbnx2x_pf_disable(bp);\r\nbnx2x_netif_stop(bp, 1);\r\nbnx2x_del_all_napi(bp);\r\nif (CNIC_LOADED(bp))\r\nbnx2x_del_all_napi_cnic(bp);\r\nbnx2x_free_irq(bp);\r\nbnx2x_send_unload_done(bp, false);\r\n}\r\nif (IS_PF(bp))\r\nbnx2x_squeeze_objects(bp);\r\nbp->sp_state = 0;\r\nbp->port.pmf = 0;\r\nbp->sp_rtnl_state = 0;\r\nsmp_mb();\r\nbnx2x_free_skbs(bp);\r\nif (CNIC_LOADED(bp))\r\nbnx2x_free_skbs_cnic(bp);\r\nfor_each_rx_queue(bp, i)\r\nbnx2x_free_rx_sge_range(bp, bp->fp + i, NUM_RX_SGE);\r\nbnx2x_free_fp_mem(bp);\r\nif (CNIC_LOADED(bp))\r\nbnx2x_free_fp_mem_cnic(bp);\r\nif (IS_PF(bp)) {\r\nif (CNIC_LOADED(bp))\r\nbnx2x_free_mem_cnic(bp);\r\n}\r\nbnx2x_free_mem(bp);\r\nbp->state = BNX2X_STATE_CLOSED;\r\nbp->cnic_loaded = false;\r\nif (IS_PF(bp))\r\nbnx2x_update_mng_version(bp);\r\nif (IS_PF(bp) && bnx2x_chk_parity_attn(bp, &global, false)) {\r\nbnx2x_set_reset_in_progress(bp);\r\nif (global)\r\nbnx2x_set_reset_global(bp);\r\n}\r\nif (IS_PF(bp) &&\r\n!bnx2x_clear_pf_load(bp) &&\r\nbnx2x_reset_is_done(bp, BP_PATH(bp)))\r\nbnx2x_disable_close_the_gate(bp);\r\nDP(NETIF_MSG_IFUP, "Ending NIC unload\n");\r\nreturn 0;\r\n}\r\nint bnx2x_set_power_state(struct bnx2x *bp, pci_power_t state)\r\n{\r\nu16 pmcsr;\r\nif (!bp->pdev->pm_cap) {\r\nBNX2X_DEV_INFO("No power capability. Breaking.\n");\r\nreturn 0;\r\n}\r\npci_read_config_word(bp->pdev, bp->pdev->pm_cap + PCI_PM_CTRL, &pmcsr);\r\nswitch (state) {\r\ncase PCI_D0:\r\npci_write_config_word(bp->pdev, bp->pdev->pm_cap + PCI_PM_CTRL,\r\n((pmcsr & ~PCI_PM_CTRL_STATE_MASK) |\r\nPCI_PM_CTRL_PME_STATUS));\r\nif (pmcsr & PCI_PM_CTRL_STATE_MASK)\r\nmsleep(20);\r\nbreak;\r\ncase PCI_D3hot:\r\nif (atomic_read(&bp->pdev->enable_cnt) != 1)\r\nreturn 0;\r\nif (CHIP_REV_IS_SLOW(bp))\r\nreturn 0;\r\npmcsr &= ~PCI_PM_CTRL_STATE_MASK;\r\npmcsr |= 3;\r\nif (bp->wol)\r\npmcsr |= PCI_PM_CTRL_PME_ENABLE;\r\npci_write_config_word(bp->pdev, bp->pdev->pm_cap + PCI_PM_CTRL,\r\npmcsr);\r\nbreak;\r\ndefault:\r\ndev_err(&bp->pdev->dev, "Can't support state = %d\n", state);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int bnx2x_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct bnx2x_fastpath *fp = container_of(napi, struct bnx2x_fastpath,\r\nnapi);\r\nstruct bnx2x *bp = fp->bp;\r\nint rx_work_done;\r\nu8 cos;\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (unlikely(bp->panic)) {\r\nnapi_complete(napi);\r\nreturn 0;\r\n}\r\n#endif\r\nfor_each_cos_in_tx_queue(fp, cos)\r\nif (bnx2x_tx_queue_has_work(fp->txdata_ptr[cos]))\r\nbnx2x_tx_int(bp, fp->txdata_ptr[cos]);\r\nrx_work_done = (bnx2x_has_rx_work(fp)) ? bnx2x_rx_int(fp, budget) : 0;\r\nif (rx_work_done < budget) {\r\nif (IS_FCOE_FP(fp)) {\r\nnapi_complete(napi);\r\n} else {\r\nbnx2x_update_fpsb_idx(fp);\r\nrmb();\r\nif (!(bnx2x_has_rx_work(fp) || bnx2x_has_tx_work(fp))) {\r\nnapi_complete(napi);\r\nDP(NETIF_MSG_RX_STATUS,\r\n"Update index to %d\n", fp->fp_hc_idx);\r\nbnx2x_ack_sb(bp, fp->igu_sb_id, USTORM_ID,\r\nle16_to_cpu(fp->fp_hc_idx),\r\nIGU_INT_ENABLE, 1);\r\n} else {\r\nrx_work_done = budget;\r\n}\r\n}\r\n}\r\nreturn rx_work_done;\r\n}\r\nstatic u16 bnx2x_tx_split(struct bnx2x *bp,\r\nstruct bnx2x_fp_txdata *txdata,\r\nstruct sw_tx_bd *tx_buf,\r\nstruct eth_tx_start_bd **tx_bd, u16 hlen,\r\nu16 bd_prod)\r\n{\r\nstruct eth_tx_start_bd *h_tx_bd = *tx_bd;\r\nstruct eth_tx_bd *d_tx_bd;\r\ndma_addr_t mapping;\r\nint old_len = le16_to_cpu(h_tx_bd->nbytes);\r\nh_tx_bd->nbytes = cpu_to_le16(hlen);\r\nDP(NETIF_MSG_TX_QUEUED, "TSO split header size is %d (%x:%x)\n",\r\nh_tx_bd->nbytes, h_tx_bd->addr_hi, h_tx_bd->addr_lo);\r\nbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\r\nd_tx_bd = &txdata->tx_desc_ring[bd_prod].reg_bd;\r\nmapping = HILO_U64(le32_to_cpu(h_tx_bd->addr_hi),\r\nle32_to_cpu(h_tx_bd->addr_lo)) + hlen;\r\nd_tx_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\r\nd_tx_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\r\nd_tx_bd->nbytes = cpu_to_le16(old_len - hlen);\r\ntx_buf->flags |= BNX2X_TSO_SPLIT_BD;\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"TSO split data size is %d (%x:%x)\n",\r\nd_tx_bd->nbytes, d_tx_bd->addr_hi, d_tx_bd->addr_lo);\r\n*tx_bd = (struct eth_tx_start_bd *)d_tx_bd;\r\nreturn bd_prod;\r\n}\r\nstatic __le16 bnx2x_csum_fix(unsigned char *t_header, u16 csum, s8 fix)\r\n{\r\n__sum16 tsum = (__force __sum16) csum;\r\nif (fix > 0)\r\ntsum = ~csum_fold(csum_sub((__force __wsum) csum,\r\ncsum_partial(t_header - fix, fix, 0)));\r\nelse if (fix < 0)\r\ntsum = ~csum_fold(csum_add((__force __wsum) csum,\r\ncsum_partial(t_header, -fix, 0)));\r\nreturn bswab16(tsum);\r\n}\r\nstatic u32 bnx2x_xmit_type(struct bnx2x *bp, struct sk_buff *skb)\r\n{\r\nu32 rc;\r\n__u8 prot = 0;\r\n__be16 protocol;\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nreturn XMIT_PLAIN;\r\nprotocol = vlan_get_protocol(skb);\r\nif (protocol == htons(ETH_P_IPV6)) {\r\nrc = XMIT_CSUM_V6;\r\nprot = ipv6_hdr(skb)->nexthdr;\r\n} else {\r\nrc = XMIT_CSUM_V4;\r\nprot = ip_hdr(skb)->protocol;\r\n}\r\nif (!CHIP_IS_E1x(bp) && skb->encapsulation) {\r\nif (inner_ip_hdr(skb)->version == 6) {\r\nrc |= XMIT_CSUM_ENC_V6;\r\nif (inner_ipv6_hdr(skb)->nexthdr == IPPROTO_TCP)\r\nrc |= XMIT_CSUM_TCP;\r\n} else {\r\nrc |= XMIT_CSUM_ENC_V4;\r\nif (inner_ip_hdr(skb)->protocol == IPPROTO_TCP)\r\nrc |= XMIT_CSUM_TCP;\r\n}\r\n}\r\nif (prot == IPPROTO_TCP)\r\nrc |= XMIT_CSUM_TCP;\r\nif (skb_is_gso(skb)) {\r\nif (skb_is_gso_v6(skb)) {\r\nrc |= (XMIT_GSO_V6 | XMIT_CSUM_TCP);\r\nif (rc & XMIT_CSUM_ENC)\r\nrc |= XMIT_GSO_ENC_V6;\r\n} else {\r\nrc |= (XMIT_GSO_V4 | XMIT_CSUM_TCP);\r\nif (rc & XMIT_CSUM_ENC)\r\nrc |= XMIT_GSO_ENC_V4;\r\n}\r\n}\r\nreturn rc;\r\n}\r\nstatic int bnx2x_pkt_req_lin(struct bnx2x *bp, struct sk_buff *skb,\r\nu32 xmit_type)\r\n{\r\nint first_bd_sz = 0, num_tso_win_sub = BNX2X_NUM_TSO_WIN_SUB_BDS;\r\nint to_copy = 0, hlen = 0;\r\nif (xmit_type & XMIT_GSO_ENC)\r\nnum_tso_win_sub = BNX2X_NUM_VXLAN_TSO_WIN_SUB_BDS;\r\nif (skb_shinfo(skb)->nr_frags >= (MAX_FETCH_BD - num_tso_win_sub)) {\r\nif (xmit_type & XMIT_GSO) {\r\nunsigned short lso_mss = skb_shinfo(skb)->gso_size;\r\nint wnd_size = MAX_FETCH_BD - num_tso_win_sub;\r\nint num_wnds = skb_shinfo(skb)->nr_frags - wnd_size;\r\nint wnd_idx = 0;\r\nint frag_idx = 0;\r\nu32 wnd_sum = 0;\r\nif (xmit_type & XMIT_GSO_ENC)\r\nhlen = (int)(skb_inner_transport_header(skb) -\r\nskb->data) +\r\ninner_tcp_hdrlen(skb);\r\nelse\r\nhlen = (int)(skb_transport_header(skb) -\r\nskb->data) + tcp_hdrlen(skb);\r\nfirst_bd_sz = skb_headlen(skb) - hlen;\r\nwnd_sum = first_bd_sz;\r\nfor (frag_idx = 0; frag_idx < wnd_size - 1; frag_idx++)\r\nwnd_sum +=\r\nskb_frag_size(&skb_shinfo(skb)->frags[frag_idx]);\r\nif (first_bd_sz > 0) {\r\nif (unlikely(wnd_sum < lso_mss)) {\r\nto_copy = 1;\r\ngoto exit_lbl;\r\n}\r\nwnd_sum -= first_bd_sz;\r\n}\r\nfor (wnd_idx = 0; wnd_idx <= num_wnds; wnd_idx++) {\r\nwnd_sum +=\r\nskb_frag_size(&skb_shinfo(skb)->frags[wnd_idx + wnd_size - 1]);\r\nif (unlikely(wnd_sum < lso_mss)) {\r\nto_copy = 1;\r\nbreak;\r\n}\r\nwnd_sum -=\r\nskb_frag_size(&skb_shinfo(skb)->frags[wnd_idx]);\r\n}\r\n} else {\r\nto_copy = 1;\r\n}\r\n}\r\nexit_lbl:\r\nif (unlikely(to_copy))\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"Linearization IS REQUIRED for %s packet. num_frags %d hlen %d first_bd_sz %d\n",\r\n(xmit_type & XMIT_GSO) ? "LSO" : "non-LSO",\r\nskb_shinfo(skb)->nr_frags, hlen, first_bd_sz);\r\nreturn to_copy;\r\n}\r\nstatic void bnx2x_set_pbd_gso(struct sk_buff *skb,\r\nstruct eth_tx_parse_bd_e1x *pbd,\r\nu32 xmit_type)\r\n{\r\npbd->lso_mss = cpu_to_le16(skb_shinfo(skb)->gso_size);\r\npbd->tcp_send_seq = bswab32(tcp_hdr(skb)->seq);\r\npbd->tcp_flags = pbd_tcp_flags(tcp_hdr(skb));\r\nif (xmit_type & XMIT_GSO_V4) {\r\npbd->ip_id = bswab16(ip_hdr(skb)->id);\r\npbd->tcp_pseudo_csum =\r\nbswab16(~csum_tcpudp_magic(ip_hdr(skb)->saddr,\r\nip_hdr(skb)->daddr,\r\n0, IPPROTO_TCP, 0));\r\n} else {\r\npbd->tcp_pseudo_csum =\r\nbswab16(~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\r\n&ipv6_hdr(skb)->daddr,\r\n0, IPPROTO_TCP, 0));\r\n}\r\npbd->global_data |=\r\ncpu_to_le16(ETH_TX_PARSE_BD_E1X_PSEUDO_CS_WITHOUT_LEN);\r\n}\r\nstatic u8 bnx2x_set_pbd_csum_enc(struct bnx2x *bp, struct sk_buff *skb,\r\nu32 *parsing_data, u32 xmit_type)\r\n{\r\n*parsing_data |=\r\n((((u8 *)skb_inner_transport_header(skb) - skb->data) >> 1) <<\r\nETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W_SHIFT) &\r\nETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W;\r\nif (xmit_type & XMIT_CSUM_TCP) {\r\n*parsing_data |= ((inner_tcp_hdrlen(skb) / 4) <<\r\nETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW_SHIFT) &\r\nETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW;\r\nreturn skb_inner_transport_header(skb) +\r\ninner_tcp_hdrlen(skb) - skb->data;\r\n}\r\nreturn skb_inner_transport_header(skb) +\r\nsizeof(struct udphdr) - skb->data;\r\n}\r\nstatic u8 bnx2x_set_pbd_csum_e2(struct bnx2x *bp, struct sk_buff *skb,\r\nu32 *parsing_data, u32 xmit_type)\r\n{\r\n*parsing_data |=\r\n((((u8 *)skb_transport_header(skb) - skb->data) >> 1) <<\r\nETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W_SHIFT) &\r\nETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W;\r\nif (xmit_type & XMIT_CSUM_TCP) {\r\n*parsing_data |= ((tcp_hdrlen(skb) / 4) <<\r\nETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW_SHIFT) &\r\nETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW;\r\nreturn skb_transport_header(skb) + tcp_hdrlen(skb) - skb->data;\r\n}\r\nreturn skb_transport_header(skb) + sizeof(struct udphdr) - skb->data;\r\n}\r\nstatic void bnx2x_set_sbd_csum(struct bnx2x *bp, struct sk_buff *skb,\r\nstruct eth_tx_start_bd *tx_start_bd,\r\nu32 xmit_type)\r\n{\r\ntx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_L4_CSUM;\r\nif (xmit_type & (XMIT_CSUM_ENC_V6 | XMIT_CSUM_V6))\r\ntx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_IPV6;\r\nif (!(xmit_type & XMIT_CSUM_TCP))\r\ntx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_IS_UDP;\r\n}\r\nstatic u8 bnx2x_set_pbd_csum(struct bnx2x *bp, struct sk_buff *skb,\r\nstruct eth_tx_parse_bd_e1x *pbd,\r\nu32 xmit_type)\r\n{\r\nu8 hlen = (skb_network_header(skb) - skb->data) >> 1;\r\npbd->global_data =\r\ncpu_to_le16(hlen |\r\n((skb->protocol == cpu_to_be16(ETH_P_8021Q)) <<\r\nETH_TX_PARSE_BD_E1X_LLC_SNAP_EN_SHIFT));\r\npbd->ip_hlen_w = (skb_transport_header(skb) -\r\nskb_network_header(skb)) >> 1;\r\nhlen += pbd->ip_hlen_w;\r\nif (xmit_type & XMIT_CSUM_TCP)\r\nhlen += tcp_hdrlen(skb) / 2;\r\nelse\r\nhlen += sizeof(struct udphdr) / 2;\r\npbd->total_hlen_w = cpu_to_le16(hlen);\r\nhlen = hlen*2;\r\nif (xmit_type & XMIT_CSUM_TCP) {\r\npbd->tcp_pseudo_csum = bswab16(tcp_hdr(skb)->check);\r\n} else {\r\ns8 fix = SKB_CS_OFF(skb);\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"hlen %d fix %d csum before fix %x\n",\r\nle16_to_cpu(pbd->total_hlen_w), fix, SKB_CS(skb));\r\npbd->tcp_pseudo_csum =\r\nbnx2x_csum_fix(skb_transport_header(skb),\r\nSKB_CS(skb), fix);\r\nDP(NETIF_MSG_TX_QUEUED, "csum after fix %x\n",\r\npbd->tcp_pseudo_csum);\r\n}\r\nreturn hlen;\r\n}\r\nstatic void bnx2x_update_pbds_gso_enc(struct sk_buff *skb,\r\nstruct eth_tx_parse_bd_e2 *pbd_e2,\r\nstruct eth_tx_parse_2nd_bd *pbd2,\r\nu16 *global_data,\r\nu32 xmit_type)\r\n{\r\nu16 hlen_w = 0;\r\nu8 outerip_off, outerip_len = 0;\r\nhlen_w = (skb_inner_transport_header(skb) -\r\nskb_network_header(skb)) >> 1;\r\nhlen_w += inner_tcp_hdrlen(skb) >> 1;\r\npbd2->fw_ip_hdr_to_payload_w = hlen_w;\r\nif (xmit_type & XMIT_CSUM_V4) {\r\nstruct iphdr *iph = ip_hdr(skb);\r\nu32 csum = (__force u32)(~iph->check) -\r\n(__force u32)iph->tot_len -\r\n(__force u32)iph->frag_off;\r\nouterip_len = iph->ihl << 1;\r\npbd2->fw_ip_csum_wo_len_flags_frag =\r\nbswab16(csum_fold((__force __wsum)csum));\r\n} else {\r\npbd2->fw_ip_hdr_to_payload_w =\r\nhlen_w - ((sizeof(struct ipv6hdr)) >> 1);\r\npbd_e2->data.tunnel_data.flags |=\r\nETH_TUNNEL_DATA_IPV6_OUTER;\r\n}\r\npbd2->tcp_send_seq = bswab32(inner_tcp_hdr(skb)->seq);\r\npbd2->tcp_flags = pbd_tcp_flags(inner_tcp_hdr(skb));\r\nif (xmit_type & XMIT_CSUM_ENC_V4) {\r\npbd2->hw_ip_id = bswab16(inner_ip_hdr(skb)->id);\r\npbd_e2->data.tunnel_data.pseudo_csum =\r\nbswab16(~csum_tcpudp_magic(\r\ninner_ip_hdr(skb)->saddr,\r\ninner_ip_hdr(skb)->daddr,\r\n0, IPPROTO_TCP, 0));\r\n} else {\r\npbd_e2->data.tunnel_data.pseudo_csum =\r\nbswab16(~csum_ipv6_magic(\r\n&inner_ipv6_hdr(skb)->saddr,\r\n&inner_ipv6_hdr(skb)->daddr,\r\n0, IPPROTO_TCP, 0));\r\n}\r\nouterip_off = (skb_network_header(skb) - skb->data) >> 1;\r\n*global_data |=\r\nouterip_off |\r\n(outerip_len <<\r\nETH_TX_PARSE_2ND_BD_IP_HDR_LEN_OUTER_W_SHIFT) |\r\n((skb->protocol == cpu_to_be16(ETH_P_8021Q)) <<\r\nETH_TX_PARSE_2ND_BD_LLC_SNAP_EN_SHIFT);\r\nif (ip_hdr(skb)->protocol == IPPROTO_UDP) {\r\nSET_FLAG(*global_data, ETH_TX_PARSE_2ND_BD_TUNNEL_UDP_EXIST, 1);\r\npbd2->tunnel_udp_hdr_start_w = skb_transport_offset(skb) >> 1;\r\n}\r\n}\r\nstatic inline void bnx2x_set_ipv6_ext_e2(struct sk_buff *skb, u32 *parsing_data,\r\nu32 xmit_type)\r\n{\r\nstruct ipv6hdr *ipv6;\r\nif (!(xmit_type & (XMIT_GSO_ENC_V6 | XMIT_GSO_V6)))\r\nreturn;\r\nif (xmit_type & XMIT_GSO_ENC_V6)\r\nipv6 = inner_ipv6_hdr(skb);\r\nelse\r\nipv6 = ipv6_hdr(skb);\r\nif (ipv6->nexthdr == NEXTHDR_IPV6)\r\n*parsing_data |= ETH_TX_PARSE_BD_E2_IPV6_WITH_EXT_HDR;\r\n}\r\nnetdev_tx_t bnx2x_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nstruct netdev_queue *txq;\r\nstruct bnx2x_fp_txdata *txdata;\r\nstruct sw_tx_bd *tx_buf;\r\nstruct eth_tx_start_bd *tx_start_bd, *first_bd;\r\nstruct eth_tx_bd *tx_data_bd, *total_pkt_bd = NULL;\r\nstruct eth_tx_parse_bd_e1x *pbd_e1x = NULL;\r\nstruct eth_tx_parse_bd_e2 *pbd_e2 = NULL;\r\nstruct eth_tx_parse_2nd_bd *pbd2 = NULL;\r\nu32 pbd_e2_parsing_data = 0;\r\nu16 pkt_prod, bd_prod;\r\nint nbd, txq_index;\r\ndma_addr_t mapping;\r\nu32 xmit_type = bnx2x_xmit_type(bp, skb);\r\nint i;\r\nu8 hlen = 0;\r\n__le16 pkt_size = 0;\r\nstruct ethhdr *eth;\r\nu8 mac_type = UNICAST_ADDRESS;\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (unlikely(bp->panic))\r\nreturn NETDEV_TX_BUSY;\r\n#endif\r\ntxq_index = skb_get_queue_mapping(skb);\r\ntxq = netdev_get_tx_queue(dev, txq_index);\r\nBUG_ON(txq_index >= MAX_ETH_TXQ_IDX(bp) + (CNIC_LOADED(bp) ? 1 : 0));\r\ntxdata = &bp->bnx2x_txq[txq_index];\r\nif (unlikely(bnx2x_tx_avail(bp, txdata) <\r\nskb_shinfo(skb)->nr_frags +\r\nBDS_PER_TX_PKT +\r\nNEXT_CNT_PER_TX_PKT(MAX_BDS_PER_TX_PKT))) {\r\nif (txdata->tx_ring_size == 0) {\r\nstruct bnx2x_eth_q_stats *q_stats =\r\nbnx2x_fp_qstats(bp, txdata->parent_fp);\r\nq_stats->driver_filtered_tx_pkt++;\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nbnx2x_fp_qstats(bp, txdata->parent_fp)->driver_xoff++;\r\nnetif_tx_stop_queue(txq);\r\nBNX2X_ERR("BUG! Tx ring full when queue awake!\n");\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"queue[%d]: SKB: summed %x protocol %x protocol(%x,%x) gso type %x xmit_type %x len %d\n",\r\ntxq_index, skb->ip_summed, skb->protocol, ipv6_hdr(skb)->nexthdr,\r\nip_hdr(skb)->protocol, skb_shinfo(skb)->gso_type, xmit_type,\r\nskb->len);\r\neth = (struct ethhdr *)skb->data;\r\nif (unlikely(is_multicast_ether_addr(eth->h_dest))) {\r\nif (is_broadcast_ether_addr(eth->h_dest))\r\nmac_type = BROADCAST_ADDRESS;\r\nelse\r\nmac_type = MULTICAST_ADDRESS;\r\n}\r\n#if (MAX_SKB_FRAGS >= MAX_FETCH_BD - BDS_PER_TX_PKT)\r\nif (bnx2x_pkt_req_lin(bp, skb, xmit_type)) {\r\nbp->lin_cnt++;\r\nif (skb_linearize(skb) != 0) {\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"SKB linearization failed - silently dropping this SKB\n");\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\n}\r\n#endif\r\nmapping = dma_map_single(&bp->pdev->dev, skb->data,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"SKB mapping failed - silently dropping this SKB\n");\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\npkt_prod = txdata->tx_pkt_prod;\r\nbd_prod = TX_BD(txdata->tx_bd_prod);\r\ntx_buf = &txdata->tx_buf_ring[TX_BD(pkt_prod)];\r\ntx_start_bd = &txdata->tx_desc_ring[bd_prod].start_bd;\r\nfirst_bd = tx_start_bd;\r\ntx_start_bd->bd_flags.as_bitfield = ETH_TX_BD_FLAGS_START_BD;\r\nif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {\r\nif (!(bp->flags & TX_TIMESTAMPING_EN)) {\r\nBNX2X_ERR("Tx timestamping was not enabled, this packet will not be timestamped\n");\r\n} else if (bp->ptp_tx_skb) {\r\nBNX2X_ERR("The device supports only a single outstanding packet to timestamp, this packet will not be timestamped\n");\r\n} else {\r\nskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\r\nbp->ptp_tx_skb = skb_get(skb);\r\nbp->ptp_tx_start = jiffies;\r\nschedule_work(&bp->ptp_task);\r\n}\r\n}\r\ntx_start_bd->general_data = 1 << ETH_TX_START_BD_HDR_NBDS_SHIFT;\r\ntx_buf->first_bd = txdata->tx_bd_prod;\r\ntx_buf->skb = skb;\r\ntx_buf->flags = 0;\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"sending pkt %u @%p next_idx %u bd %u @%p\n",\r\npkt_prod, tx_buf, txdata->tx_pkt_prod, bd_prod, tx_start_bd);\r\nif (skb_vlan_tag_present(skb)) {\r\ntx_start_bd->vlan_or_ethertype =\r\ncpu_to_le16(skb_vlan_tag_get(skb));\r\ntx_start_bd->bd_flags.as_bitfield |=\r\n(X_ETH_OUTBAND_VLAN << ETH_TX_BD_FLAGS_VLAN_MODE_SHIFT);\r\n} else {\r\n#ifndef BNX2X_STOP_ON_ERROR\r\nif (IS_VF(bp))\r\n#endif\r\ntx_start_bd->vlan_or_ethertype =\r\ncpu_to_le16(ntohs(eth->h_proto));\r\n#ifndef BNX2X_STOP_ON_ERROR\r\nelse\r\ntx_start_bd->vlan_or_ethertype = cpu_to_le16(pkt_prod);\r\n#endif\r\n}\r\nnbd = 2;\r\nbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\r\nif (xmit_type & XMIT_CSUM)\r\nbnx2x_set_sbd_csum(bp, skb, tx_start_bd, xmit_type);\r\nif (!CHIP_IS_E1x(bp)) {\r\npbd_e2 = &txdata->tx_desc_ring[bd_prod].parse_bd_e2;\r\nmemset(pbd_e2, 0, sizeof(struct eth_tx_parse_bd_e2));\r\nif (xmit_type & XMIT_CSUM_ENC) {\r\nu16 global_data = 0;\r\nhlen = bnx2x_set_pbd_csum_enc(bp, skb,\r\n&pbd_e2_parsing_data,\r\nxmit_type);\r\nbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\r\npbd2 = &txdata->tx_desc_ring[bd_prod].parse_2nd_bd;\r\nmemset(pbd2, 0, sizeof(*pbd2));\r\npbd_e2->data.tunnel_data.ip_hdr_start_inner_w =\r\n(skb_inner_network_header(skb) -\r\nskb->data) >> 1;\r\nif (xmit_type & XMIT_GSO_ENC)\r\nbnx2x_update_pbds_gso_enc(skb, pbd_e2, pbd2,\r\n&global_data,\r\nxmit_type);\r\npbd2->global_data = cpu_to_le16(global_data);\r\nSET_FLAG(tx_start_bd->general_data,\r\nETH_TX_START_BD_PARSE_NBDS, 1);\r\nSET_FLAG(tx_start_bd->general_data,\r\nETH_TX_START_BD_TUNNEL_EXIST, 1);\r\ntx_buf->flags |= BNX2X_HAS_SECOND_PBD;\r\nnbd++;\r\n} else if (xmit_type & XMIT_CSUM) {\r\nhlen = bnx2x_set_pbd_csum_e2(bp, skb,\r\n&pbd_e2_parsing_data,\r\nxmit_type);\r\n}\r\nbnx2x_set_ipv6_ext_e2(skb, &pbd_e2_parsing_data, xmit_type);\r\nif (IS_VF(bp)) {\r\nbnx2x_set_fw_mac_addr(&pbd_e2->data.mac_addr.src_hi,\r\n&pbd_e2->data.mac_addr.src_mid,\r\n&pbd_e2->data.mac_addr.src_lo,\r\neth->h_source);\r\nbnx2x_set_fw_mac_addr(&pbd_e2->data.mac_addr.dst_hi,\r\n&pbd_e2->data.mac_addr.dst_mid,\r\n&pbd_e2->data.mac_addr.dst_lo,\r\neth->h_dest);\r\n} else {\r\nif (bp->flags & TX_SWITCHING)\r\nbnx2x_set_fw_mac_addr(\r\n&pbd_e2->data.mac_addr.dst_hi,\r\n&pbd_e2->data.mac_addr.dst_mid,\r\n&pbd_e2->data.mac_addr.dst_lo,\r\neth->h_dest);\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nbnx2x_set_fw_mac_addr(&pbd_e2->data.mac_addr.src_hi,\r\n&pbd_e2->data.mac_addr.src_mid,\r\n&pbd_e2->data.mac_addr.src_lo,\r\neth->h_source);\r\n#endif\r\n}\r\nSET_FLAG(pbd_e2_parsing_data,\r\nETH_TX_PARSE_BD_E2_ETH_ADDR_TYPE, mac_type);\r\n} else {\r\nu16 global_data = 0;\r\npbd_e1x = &txdata->tx_desc_ring[bd_prod].parse_bd_e1x;\r\nmemset(pbd_e1x, 0, sizeof(struct eth_tx_parse_bd_e1x));\r\nif (xmit_type & XMIT_CSUM)\r\nhlen = bnx2x_set_pbd_csum(bp, skb, pbd_e1x, xmit_type);\r\nSET_FLAG(global_data,\r\nETH_TX_PARSE_BD_E1X_ETH_ADDR_TYPE, mac_type);\r\npbd_e1x->global_data |= cpu_to_le16(global_data);\r\n}\r\ntx_start_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\r\ntx_start_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\r\ntx_start_bd->nbytes = cpu_to_le16(skb_headlen(skb));\r\npkt_size = tx_start_bd->nbytes;\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"first bd @%p addr (%x:%x) nbytes %d flags %x vlan %x\n",\r\ntx_start_bd, tx_start_bd->addr_hi, tx_start_bd->addr_lo,\r\nle16_to_cpu(tx_start_bd->nbytes),\r\ntx_start_bd->bd_flags.as_bitfield,\r\nle16_to_cpu(tx_start_bd->vlan_or_ethertype));\r\nif (xmit_type & XMIT_GSO) {\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"TSO packet len %d hlen %d total len %d tso size %d\n",\r\nskb->len, hlen, skb_headlen(skb),\r\nskb_shinfo(skb)->gso_size);\r\ntx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_SW_LSO;\r\nif (unlikely(skb_headlen(skb) > hlen)) {\r\nnbd++;\r\nbd_prod = bnx2x_tx_split(bp, txdata, tx_buf,\r\n&tx_start_bd, hlen,\r\nbd_prod);\r\n}\r\nif (!CHIP_IS_E1x(bp))\r\npbd_e2_parsing_data |=\r\n(skb_shinfo(skb)->gso_size <<\r\nETH_TX_PARSE_BD_E2_LSO_MSS_SHIFT) &\r\nETH_TX_PARSE_BD_E2_LSO_MSS;\r\nelse\r\nbnx2x_set_pbd_gso(skb, pbd_e1x, xmit_type);\r\n}\r\nif (pbd_e2_parsing_data)\r\npbd_e2->parsing_data = cpu_to_le32(pbd_e2_parsing_data);\r\ntx_data_bd = (struct eth_tx_bd *)tx_start_bd;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nmapping = skb_frag_dma_map(&bp->pdev->dev, frag, 0,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\r\nunsigned int pkts_compl = 0, bytes_compl = 0;\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"Unable to map page - dropping packet...\n");\r\nfirst_bd->nbd = cpu_to_le16(nbd);\r\nbnx2x_free_tx_pkt(bp, txdata,\r\nTX_BD(txdata->tx_pkt_prod),\r\n&pkts_compl, &bytes_compl);\r\nreturn NETDEV_TX_OK;\r\n}\r\nbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\r\ntx_data_bd = &txdata->tx_desc_ring[bd_prod].reg_bd;\r\nif (total_pkt_bd == NULL)\r\ntotal_pkt_bd = &txdata->tx_desc_ring[bd_prod].reg_bd;\r\ntx_data_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\r\ntx_data_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\r\ntx_data_bd->nbytes = cpu_to_le16(skb_frag_size(frag));\r\nle16_add_cpu(&pkt_size, skb_frag_size(frag));\r\nnbd++;\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"frag %d bd @%p addr (%x:%x) nbytes %d\n",\r\ni, tx_data_bd, tx_data_bd->addr_hi, tx_data_bd->addr_lo,\r\nle16_to_cpu(tx_data_bd->nbytes));\r\n}\r\nDP(NETIF_MSG_TX_QUEUED, "last bd @%p\n", tx_data_bd);\r\nfirst_bd->nbd = cpu_to_le16(nbd);\r\nbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\r\nif (TX_BD_POFF(bd_prod) < nbd)\r\nnbd++;\r\nif (total_pkt_bd != NULL)\r\ntotal_pkt_bd->total_pkt_bytes = pkt_size;\r\nif (pbd_e1x)\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"PBD (E1X) @%p ip_data %x ip_hlen %u ip_id %u lso_mss %u tcp_flags %x xsum %x seq %u hlen %u\n",\r\npbd_e1x, pbd_e1x->global_data, pbd_e1x->ip_hlen_w,\r\npbd_e1x->ip_id, pbd_e1x->lso_mss, pbd_e1x->tcp_flags,\r\npbd_e1x->tcp_pseudo_csum, pbd_e1x->tcp_send_seq,\r\nle16_to_cpu(pbd_e1x->total_hlen_w));\r\nif (pbd_e2)\r\nDP(NETIF_MSG_TX_QUEUED,\r\n"PBD (E2) @%p dst %x %x %x src %x %x %x parsing_data %x\n",\r\npbd_e2,\r\npbd_e2->data.mac_addr.dst_hi,\r\npbd_e2->data.mac_addr.dst_mid,\r\npbd_e2->data.mac_addr.dst_lo,\r\npbd_e2->data.mac_addr.src_hi,\r\npbd_e2->data.mac_addr.src_mid,\r\npbd_e2->data.mac_addr.src_lo,\r\npbd_e2->parsing_data);\r\nDP(NETIF_MSG_TX_QUEUED, "doorbell: nbd %d bd %u\n", nbd, bd_prod);\r\nnetdev_tx_sent_queue(txq, skb->len);\r\nskb_tx_timestamp(skb);\r\ntxdata->tx_pkt_prod++;\r\nwmb();\r\ntxdata->tx_db.data.prod += nbd;\r\nbarrier();\r\nDOORBELL(bp, txdata->cid, txdata->tx_db.raw);\r\nmmiowb();\r\ntxdata->tx_bd_prod += nbd;\r\nif (unlikely(bnx2x_tx_avail(bp, txdata) < MAX_DESC_PER_TX_PKT)) {\r\nnetif_tx_stop_queue(txq);\r\nsmp_mb();\r\nbnx2x_fp_qstats(bp, txdata->parent_fp)->driver_xoff++;\r\nif (bnx2x_tx_avail(bp, txdata) >= MAX_DESC_PER_TX_PKT)\r\nnetif_tx_wake_queue(txq);\r\n}\r\ntxdata->tx_pkt++;\r\nreturn NETDEV_TX_OK;\r\n}\r\nvoid bnx2x_get_c2s_mapping(struct bnx2x *bp, u8 *c2s_map, u8 *c2s_default)\r\n{\r\nint mfw_vn = BP_FW_MB_IDX(bp);\r\nu32 tmp;\r\nif (!IS_MF_BD(bp)) {\r\nint i;\r\nfor (i = 0; i < BNX2X_MAX_PRIORITY; i++)\r\nc2s_map[i] = i;\r\n*c2s_default = 0;\r\nreturn;\r\n}\r\ntmp = SHMEM2_RD(bp, c2s_pcp_map_lower[mfw_vn]);\r\ntmp = (__force u32)be32_to_cpu((__force __be32)tmp);\r\nc2s_map[0] = tmp & 0xff;\r\nc2s_map[1] = (tmp >> 8) & 0xff;\r\nc2s_map[2] = (tmp >> 16) & 0xff;\r\nc2s_map[3] = (tmp >> 24) & 0xff;\r\ntmp = SHMEM2_RD(bp, c2s_pcp_map_upper[mfw_vn]);\r\ntmp = (__force u32)be32_to_cpu((__force __be32)tmp);\r\nc2s_map[4] = tmp & 0xff;\r\nc2s_map[5] = (tmp >> 8) & 0xff;\r\nc2s_map[6] = (tmp >> 16) & 0xff;\r\nc2s_map[7] = (tmp >> 24) & 0xff;\r\ntmp = SHMEM2_RD(bp, c2s_pcp_map_default[mfw_vn]);\r\ntmp = (__force u32)be32_to_cpu((__force __be32)tmp);\r\n*c2s_default = (tmp >> (8 * mfw_vn)) & 0xff;\r\n}\r\nint bnx2x_setup_tc(struct net_device *dev, u8 num_tc)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nu8 c2s_map[BNX2X_MAX_PRIORITY], c2s_def;\r\nint cos, prio, count, offset;\r\nASSERT_RTNL();\r\nif (!num_tc) {\r\nnetdev_reset_tc(dev);\r\nreturn 0;\r\n}\r\nif (num_tc > bp->max_cos) {\r\nBNX2X_ERR("support for too many traffic classes requested: %d. Max supported is %d\n",\r\nnum_tc, bp->max_cos);\r\nreturn -EINVAL;\r\n}\r\nif (netdev_set_num_tc(dev, num_tc)) {\r\nBNX2X_ERR("failed to declare %d traffic classes\n", num_tc);\r\nreturn -EINVAL;\r\n}\r\nbnx2x_get_c2s_mapping(bp, c2s_map, &c2s_def);\r\nfor (prio = 0; prio < BNX2X_MAX_PRIORITY; prio++) {\r\nint outer_prio = c2s_map[prio];\r\nnetdev_set_prio_tc_map(dev, prio, bp->prio_to_cos[outer_prio]);\r\nDP(BNX2X_MSG_SP | NETIF_MSG_IFUP,\r\n"mapping priority %d to tc %d\n",\r\nouter_prio, bp->prio_to_cos[outer_prio]);\r\n}\r\nfor (cos = 0; cos < bp->max_cos; cos++) {\r\ncount = BNX2X_NUM_ETH_QUEUES(bp);\r\noffset = cos * BNX2X_NUM_NON_CNIC_QUEUES(bp);\r\nnetdev_set_tc_queue(dev, cos, count, offset);\r\nDP(BNX2X_MSG_SP | NETIF_MSG_IFUP,\r\n"mapping tc %d to offset %d count %d\n",\r\ncos, offset, count);\r\n}\r\nreturn 0;\r\n}\r\nint __bnx2x_setup_tc(struct net_device *dev, u32 handle, __be16 proto,\r\nstruct tc_to_netdev *tc)\r\n{\r\nif (tc->type != TC_SETUP_MQPRIO)\r\nreturn -EINVAL;\r\nreturn bnx2x_setup_tc(dev, tc->tc);\r\n}\r\nint bnx2x_change_mac_addr(struct net_device *dev, void *p)\r\n{\r\nstruct sockaddr *addr = p;\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nint rc = 0;\r\nif (!is_valid_ether_addr(addr->sa_data)) {\r\nBNX2X_ERR("Requested MAC address is not valid\n");\r\nreturn -EINVAL;\r\n}\r\nif (IS_MF_STORAGE_ONLY(bp)) {\r\nBNX2X_ERR("Can't change address on STORAGE ONLY function\n");\r\nreturn -EINVAL;\r\n}\r\nif (netif_running(dev)) {\r\nrc = bnx2x_set_eth_mac(bp, false);\r\nif (rc)\r\nreturn rc;\r\n}\r\nmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\r\nif (netif_running(dev))\r\nrc = bnx2x_set_eth_mac(bp, true);\r\nif (IS_PF(bp) && SHMEM2_HAS(bp, curr_cfg))\r\nSHMEM2_WR(bp, curr_cfg, CURR_CFG_MET_OS);\r\nreturn rc;\r\n}\r\nstatic void bnx2x_free_fp_mem_at(struct bnx2x *bp, int fp_index)\r\n{\r\nunion host_hc_status_block *sb = &bnx2x_fp(bp, fp_index, status_blk);\r\nstruct bnx2x_fastpath *fp = &bp->fp[fp_index];\r\nu8 cos;\r\nif (IS_FCOE_IDX(fp_index)) {\r\nmemset(sb, 0, sizeof(union host_hc_status_block));\r\nfp->status_blk_mapping = 0;\r\n} else {\r\nif (!CHIP_IS_E1x(bp))\r\nBNX2X_PCI_FREE(sb->e2_sb,\r\nbnx2x_fp(bp, fp_index,\r\nstatus_blk_mapping),\r\nsizeof(struct host_hc_status_block_e2));\r\nelse\r\nBNX2X_PCI_FREE(sb->e1x_sb,\r\nbnx2x_fp(bp, fp_index,\r\nstatus_blk_mapping),\r\nsizeof(struct host_hc_status_block_e1x));\r\n}\r\nif (!skip_rx_queue(bp, fp_index)) {\r\nbnx2x_free_rx_bds(fp);\r\nBNX2X_FREE(bnx2x_fp(bp, fp_index, rx_buf_ring));\r\nBNX2X_PCI_FREE(bnx2x_fp(bp, fp_index, rx_desc_ring),\r\nbnx2x_fp(bp, fp_index, rx_desc_mapping),\r\nsizeof(struct eth_rx_bd) * NUM_RX_BD);\r\nBNX2X_PCI_FREE(bnx2x_fp(bp, fp_index, rx_comp_ring),\r\nbnx2x_fp(bp, fp_index, rx_comp_mapping),\r\nsizeof(struct eth_fast_path_rx_cqe) *\r\nNUM_RCQ_BD);\r\nBNX2X_FREE(bnx2x_fp(bp, fp_index, rx_page_ring));\r\nBNX2X_PCI_FREE(bnx2x_fp(bp, fp_index, rx_sge_ring),\r\nbnx2x_fp(bp, fp_index, rx_sge_mapping),\r\nBCM_PAGE_SIZE * NUM_RX_SGE_PAGES);\r\n}\r\nif (!skip_tx_queue(bp, fp_index)) {\r\nfor_each_cos_in_tx_queue(fp, cos) {\r\nstruct bnx2x_fp_txdata *txdata = fp->txdata_ptr[cos];\r\nDP(NETIF_MSG_IFDOWN,\r\n"freeing tx memory of fp %d cos %d cid %d\n",\r\nfp_index, cos, txdata->cid);\r\nBNX2X_FREE(txdata->tx_buf_ring);\r\nBNX2X_PCI_FREE(txdata->tx_desc_ring,\r\ntxdata->tx_desc_mapping,\r\nsizeof(union eth_tx_bd_types) * NUM_TX_BD);\r\n}\r\n}\r\n}\r\nstatic void bnx2x_free_fp_mem_cnic(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_cnic_queue(bp, i)\r\nbnx2x_free_fp_mem_at(bp, i);\r\n}\r\nvoid bnx2x_free_fp_mem(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor_each_eth_queue(bp, i)\r\nbnx2x_free_fp_mem_at(bp, i);\r\n}\r\nstatic void set_sb_shortcuts(struct bnx2x *bp, int index)\r\n{\r\nunion host_hc_status_block status_blk = bnx2x_fp(bp, index, status_blk);\r\nif (!CHIP_IS_E1x(bp)) {\r\nbnx2x_fp(bp, index, sb_index_values) =\r\n(__le16 *)status_blk.e2_sb->sb.index_values;\r\nbnx2x_fp(bp, index, sb_running_index) =\r\n(__le16 *)status_blk.e2_sb->sb.running_index;\r\n} else {\r\nbnx2x_fp(bp, index, sb_index_values) =\r\n(__le16 *)status_blk.e1x_sb->sb.index_values;\r\nbnx2x_fp(bp, index, sb_running_index) =\r\n(__le16 *)status_blk.e1x_sb->sb.running_index;\r\n}\r\n}\r\nstatic int bnx2x_alloc_rx_bds(struct bnx2x_fastpath *fp,\r\nint rx_ring_size)\r\n{\r\nstruct bnx2x *bp = fp->bp;\r\nu16 ring_prod, cqe_ring_prod;\r\nint i, failure_cnt = 0;\r\nfp->rx_comp_cons = 0;\r\ncqe_ring_prod = ring_prod = 0;\r\nfor (i = 0; i < rx_ring_size; i++) {\r\nif (bnx2x_alloc_rx_data(bp, fp, ring_prod, GFP_KERNEL) < 0) {\r\nfailure_cnt++;\r\ncontinue;\r\n}\r\nring_prod = NEXT_RX_IDX(ring_prod);\r\ncqe_ring_prod = NEXT_RCQ_IDX(cqe_ring_prod);\r\nWARN_ON(ring_prod <= (i - failure_cnt));\r\n}\r\nif (failure_cnt)\r\nBNX2X_ERR("was only able to allocate %d rx skbs on queue[%d]\n",\r\ni - failure_cnt, fp->index);\r\nfp->rx_bd_prod = ring_prod;\r\nfp->rx_comp_prod = min_t(u16, NUM_RCQ_RINGS*RCQ_DESC_CNT,\r\ncqe_ring_prod);\r\nbnx2x_fp_stats(bp, fp)->eth_q_stats.rx_skb_alloc_failed += failure_cnt;\r\nreturn i - failure_cnt;\r\n}\r\nstatic void bnx2x_set_next_page_rx_cq(struct bnx2x_fastpath *fp)\r\n{\r\nint i;\r\nfor (i = 1; i <= NUM_RCQ_RINGS; i++) {\r\nstruct eth_rx_cqe_next_page *nextpg;\r\nnextpg = (struct eth_rx_cqe_next_page *)\r\n&fp->rx_comp_ring[RCQ_DESC_CNT * i - 1];\r\nnextpg->addr_hi =\r\ncpu_to_le32(U64_HI(fp->rx_comp_mapping +\r\nBCM_PAGE_SIZE*(i % NUM_RCQ_RINGS)));\r\nnextpg->addr_lo =\r\ncpu_to_le32(U64_LO(fp->rx_comp_mapping +\r\nBCM_PAGE_SIZE*(i % NUM_RCQ_RINGS)));\r\n}\r\n}\r\nstatic int bnx2x_alloc_fp_mem_at(struct bnx2x *bp, int index)\r\n{\r\nunion host_hc_status_block *sb;\r\nstruct bnx2x_fastpath *fp = &bp->fp[index];\r\nint ring_size = 0;\r\nu8 cos;\r\nint rx_ring_size = 0;\r\nif (!bp->rx_ring_size && IS_MF_STORAGE_ONLY(bp)) {\r\nrx_ring_size = MIN_RX_SIZE_NONTPA;\r\nbp->rx_ring_size = rx_ring_size;\r\n} else if (!bp->rx_ring_size) {\r\nrx_ring_size = MAX_RX_AVAIL/BNX2X_NUM_RX_QUEUES(bp);\r\nif (CHIP_IS_E3(bp)) {\r\nu32 cfg = SHMEM_RD(bp,\r\ndev_info.port_hw_config[BP_PORT(bp)].\r\ndefault_cfg);\r\nif ((cfg & PORT_HW_CFG_NET_SERDES_IF_MASK) ==\r\nPORT_HW_CFG_NET_SERDES_IF_SGMII)\r\nrx_ring_size /= 10;\r\n}\r\nrx_ring_size = max_t(int, bp->disable_tpa ? MIN_RX_SIZE_NONTPA :\r\nMIN_RX_SIZE_TPA, rx_ring_size);\r\nbp->rx_ring_size = rx_ring_size;\r\n} else\r\nrx_ring_size = bp->rx_ring_size;\r\nDP(BNX2X_MSG_SP, "calculated rx_ring_size %d\n", rx_ring_size);\r\nsb = &bnx2x_fp(bp, index, status_blk);\r\nif (!IS_FCOE_IDX(index)) {\r\nif (!CHIP_IS_E1x(bp)) {\r\nsb->e2_sb = BNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, status_blk_mapping),\r\nsizeof(struct host_hc_status_block_e2));\r\nif (!sb->e2_sb)\r\ngoto alloc_mem_err;\r\n} else {\r\nsb->e1x_sb = BNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, status_blk_mapping),\r\nsizeof(struct host_hc_status_block_e1x));\r\nif (!sb->e1x_sb)\r\ngoto alloc_mem_err;\r\n}\r\n}\r\nif (!IS_FCOE_IDX(index))\r\nset_sb_shortcuts(bp, index);\r\nif (!skip_tx_queue(bp, index)) {\r\nfor_each_cos_in_tx_queue(fp, cos) {\r\nstruct bnx2x_fp_txdata *txdata = fp->txdata_ptr[cos];\r\nDP(NETIF_MSG_IFUP,\r\n"allocating tx memory of fp %d cos %d\n",\r\nindex, cos);\r\ntxdata->tx_buf_ring = kcalloc(NUM_TX_BD,\r\nsizeof(struct sw_tx_bd),\r\nGFP_KERNEL);\r\nif (!txdata->tx_buf_ring)\r\ngoto alloc_mem_err;\r\ntxdata->tx_desc_ring = BNX2X_PCI_ALLOC(&txdata->tx_desc_mapping,\r\nsizeof(union eth_tx_bd_types) * NUM_TX_BD);\r\nif (!txdata->tx_desc_ring)\r\ngoto alloc_mem_err;\r\n}\r\n}\r\nif (!skip_rx_queue(bp, index)) {\r\nbnx2x_fp(bp, index, rx_buf_ring) =\r\nkcalloc(NUM_RX_BD, sizeof(struct sw_rx_bd), GFP_KERNEL);\r\nif (!bnx2x_fp(bp, index, rx_buf_ring))\r\ngoto alloc_mem_err;\r\nbnx2x_fp(bp, index, rx_desc_ring) =\r\nBNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, rx_desc_mapping),\r\nsizeof(struct eth_rx_bd) * NUM_RX_BD);\r\nif (!bnx2x_fp(bp, index, rx_desc_ring))\r\ngoto alloc_mem_err;\r\nbnx2x_fp(bp, index, rx_comp_ring) =\r\nBNX2X_PCI_FALLOC(&bnx2x_fp(bp, index, rx_comp_mapping),\r\nsizeof(struct eth_fast_path_rx_cqe) * NUM_RCQ_BD);\r\nif (!bnx2x_fp(bp, index, rx_comp_ring))\r\ngoto alloc_mem_err;\r\nbnx2x_fp(bp, index, rx_page_ring) =\r\nkcalloc(NUM_RX_SGE, sizeof(struct sw_rx_page),\r\nGFP_KERNEL);\r\nif (!bnx2x_fp(bp, index, rx_page_ring))\r\ngoto alloc_mem_err;\r\nbnx2x_fp(bp, index, rx_sge_ring) =\r\nBNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, rx_sge_mapping),\r\nBCM_PAGE_SIZE * NUM_RX_SGE_PAGES);\r\nif (!bnx2x_fp(bp, index, rx_sge_ring))\r\ngoto alloc_mem_err;\r\nbnx2x_set_next_page_rx_bd(fp);\r\nbnx2x_set_next_page_rx_cq(fp);\r\nring_size = bnx2x_alloc_rx_bds(fp, rx_ring_size);\r\nif (ring_size < rx_ring_size)\r\ngoto alloc_mem_err;\r\n}\r\nreturn 0;\r\nalloc_mem_err:\r\nBNX2X_ERR("Unable to allocate full memory for queue %d (size %d)\n",\r\nindex, ring_size);\r\nif (ring_size < (fp->mode == TPA_MODE_DISABLED ?\r\nMIN_RX_SIZE_NONTPA : MIN_RX_SIZE_TPA)) {\r\nbnx2x_free_fp_mem_at(bp, index);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic int bnx2x_alloc_fp_mem_cnic(struct bnx2x *bp)\r\n{\r\nif (!NO_FCOE(bp))\r\nif (bnx2x_alloc_fp_mem_at(bp, FCOE_IDX(bp)))\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic int bnx2x_alloc_fp_mem(struct bnx2x *bp)\r\n{\r\nint i;\r\nif (bnx2x_alloc_fp_mem_at(bp, 0))\r\nreturn -ENOMEM;\r\nfor_each_nondefault_eth_queue(bp, i)\r\nif (bnx2x_alloc_fp_mem_at(bp, i))\r\nbreak;\r\nif (i != BNX2X_NUM_ETH_QUEUES(bp)) {\r\nint delta = BNX2X_NUM_ETH_QUEUES(bp) - i;\r\nWARN_ON(delta < 0);\r\nbnx2x_shrink_eth_fp(bp, delta);\r\nif (CNIC_SUPPORT(bp))\r\nbnx2x_move_fp(bp, FCOE_IDX(bp), FCOE_IDX(bp) - delta);\r\nbp->num_ethernet_queues -= delta;\r\nbp->num_queues = bp->num_ethernet_queues +\r\nbp->num_cnic_queues;\r\nBNX2X_ERR("Adjusted num of queues from %d to %d\n",\r\nbp->num_queues + delta, bp->num_queues);\r\n}\r\nreturn 0;\r\n}\r\nvoid bnx2x_free_mem_bp(struct bnx2x *bp)\r\n{\r\nint i;\r\nfor (i = 0; i < bp->fp_array_size; i++)\r\nkfree(bp->fp[i].tpa_info);\r\nkfree(bp->fp);\r\nkfree(bp->sp_objs);\r\nkfree(bp->fp_stats);\r\nkfree(bp->bnx2x_txq);\r\nkfree(bp->msix_table);\r\nkfree(bp->ilt);\r\n}\r\nint bnx2x_alloc_mem_bp(struct bnx2x *bp)\r\n{\r\nstruct bnx2x_fastpath *fp;\r\nstruct msix_entry *tbl;\r\nstruct bnx2x_ilt *ilt;\r\nint msix_table_size = 0;\r\nint fp_array_size, txq_array_size;\r\nint i;\r\nmsix_table_size = bp->igu_sb_cnt;\r\nif (IS_PF(bp))\r\nmsix_table_size++;\r\nBNX2X_DEV_INFO("msix_table_size %d\n", msix_table_size);\r\nfp_array_size = BNX2X_MAX_RSS_COUNT(bp) + CNIC_SUPPORT(bp);\r\nbp->fp_array_size = fp_array_size;\r\nBNX2X_DEV_INFO("fp_array_size %d\n", bp->fp_array_size);\r\nfp = kcalloc(bp->fp_array_size, sizeof(*fp), GFP_KERNEL);\r\nif (!fp)\r\ngoto alloc_err;\r\nfor (i = 0; i < bp->fp_array_size; i++) {\r\nfp[i].tpa_info =\r\nkcalloc(ETH_MAX_AGGREGATION_QUEUES_E1H_E2,\r\nsizeof(struct bnx2x_agg_info), GFP_KERNEL);\r\nif (!(fp[i].tpa_info))\r\ngoto alloc_err;\r\n}\r\nbp->fp = fp;\r\nbp->sp_objs = kcalloc(bp->fp_array_size, sizeof(struct bnx2x_sp_objs),\r\nGFP_KERNEL);\r\nif (!bp->sp_objs)\r\ngoto alloc_err;\r\nbp->fp_stats = kcalloc(bp->fp_array_size, sizeof(struct bnx2x_fp_stats),\r\nGFP_KERNEL);\r\nif (!bp->fp_stats)\r\ngoto alloc_err;\r\ntxq_array_size =\r\nBNX2X_MAX_RSS_COUNT(bp) * BNX2X_MULTI_TX_COS + CNIC_SUPPORT(bp);\r\nBNX2X_DEV_INFO("txq_array_size %d", txq_array_size);\r\nbp->bnx2x_txq = kcalloc(txq_array_size, sizeof(struct bnx2x_fp_txdata),\r\nGFP_KERNEL);\r\nif (!bp->bnx2x_txq)\r\ngoto alloc_err;\r\ntbl = kcalloc(msix_table_size, sizeof(*tbl), GFP_KERNEL);\r\nif (!tbl)\r\ngoto alloc_err;\r\nbp->msix_table = tbl;\r\nilt = kzalloc(sizeof(*ilt), GFP_KERNEL);\r\nif (!ilt)\r\ngoto alloc_err;\r\nbp->ilt = ilt;\r\nreturn 0;\r\nalloc_err:\r\nbnx2x_free_mem_bp(bp);\r\nreturn -ENOMEM;\r\n}\r\nint bnx2x_reload_if_running(struct net_device *dev)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nif (unlikely(!netif_running(dev)))\r\nreturn 0;\r\nbnx2x_nic_unload(bp, UNLOAD_NORMAL, true);\r\nreturn bnx2x_nic_load(bp, LOAD_NORMAL);\r\n}\r\nint bnx2x_get_cur_phy_idx(struct bnx2x *bp)\r\n{\r\nu32 sel_phy_idx = 0;\r\nif (bp->link_params.num_phys <= 1)\r\nreturn INT_PHY;\r\nif (bp->link_vars.link_up) {\r\nsel_phy_idx = EXT_PHY1;\r\nif ((bp->link_vars.link_status & LINK_STATUS_SERDES_LINK) &&\r\n(bp->link_params.phy[EXT_PHY2].supported & SUPPORTED_FIBRE))\r\nsel_phy_idx = EXT_PHY2;\r\n} else {\r\nswitch (bnx2x_phy_selection(&bp->link_params)) {\r\ncase PORT_HW_CFG_PHY_SELECTION_HARDWARE_DEFAULT:\r\ncase PORT_HW_CFG_PHY_SELECTION_FIRST_PHY:\r\ncase PORT_HW_CFG_PHY_SELECTION_FIRST_PHY_PRIORITY:\r\nsel_phy_idx = EXT_PHY1;\r\nbreak;\r\ncase PORT_HW_CFG_PHY_SELECTION_SECOND_PHY:\r\ncase PORT_HW_CFG_PHY_SELECTION_SECOND_PHY_PRIORITY:\r\nsel_phy_idx = EXT_PHY2;\r\nbreak;\r\n}\r\n}\r\nreturn sel_phy_idx;\r\n}\r\nint bnx2x_get_link_cfg_idx(struct bnx2x *bp)\r\n{\r\nu32 sel_phy_idx = bnx2x_get_cur_phy_idx(bp);\r\nif (bp->link_params.multi_phy_config &\r\nPORT_HW_CFG_PHY_SWAPPED_ENABLED) {\r\nif (sel_phy_idx == EXT_PHY1)\r\nsel_phy_idx = EXT_PHY2;\r\nelse if (sel_phy_idx == EXT_PHY2)\r\nsel_phy_idx = EXT_PHY1;\r\n}\r\nreturn LINK_CONFIG_IDX(sel_phy_idx);\r\n}\r\nint bnx2x_fcoe_get_wwn(struct net_device *dev, u64 *wwn, int type)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nstruct cnic_eth_dev *cp = &bp->cnic_eth_dev;\r\nswitch (type) {\r\ncase NETDEV_FCOE_WWNN:\r\n*wwn = HILO_U64(cp->fcoe_wwn_node_name_hi,\r\ncp->fcoe_wwn_node_name_lo);\r\nbreak;\r\ncase NETDEV_FCOE_WWPN:\r\n*wwn = HILO_U64(cp->fcoe_wwn_port_name_hi,\r\ncp->fcoe_wwn_port_name_lo);\r\nbreak;\r\ndefault:\r\nBNX2X_ERR("Wrong WWN type requested - %d\n", type);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint bnx2x_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nif (pci_num_vf(bp->pdev)) {\r\nDP(BNX2X_MSG_IOV, "VFs are enabled, can not change MTU\n");\r\nreturn -EPERM;\r\n}\r\nif (bp->recovery_state != BNX2X_RECOVERY_DONE) {\r\nBNX2X_ERR("Can't perform change MTU during parity recovery\n");\r\nreturn -EAGAIN;\r\n}\r\nif ((new_mtu > ETH_MAX_JUMBO_PACKET_SIZE) ||\r\n((new_mtu + ETH_HLEN) < ETH_MIN_PACKET_SIZE)) {\r\nBNX2X_ERR("Can't support requested MTU size\n");\r\nreturn -EINVAL;\r\n}\r\ndev->mtu = new_mtu;\r\nif (IS_PF(bp) && SHMEM2_HAS(bp, curr_cfg))\r\nSHMEM2_WR(bp, curr_cfg, CURR_CFG_MET_OS);\r\nreturn bnx2x_reload_if_running(dev);\r\n}\r\nnetdev_features_t bnx2x_fix_features(struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nif (pci_num_vf(bp->pdev)) {\r\nnetdev_features_t changed = dev->features ^ features;\r\nif (!(features & NETIF_F_RXCSUM) && !bp->disable_tpa) {\r\nfeatures &= ~NETIF_F_RXCSUM;\r\nfeatures |= dev->features & NETIF_F_RXCSUM;\r\n}\r\nif (changed & NETIF_F_LOOPBACK) {\r\nfeatures &= ~NETIF_F_LOOPBACK;\r\nfeatures |= dev->features & NETIF_F_LOOPBACK;\r\n}\r\n}\r\nif (!(features & NETIF_F_RXCSUM)) {\r\nfeatures &= ~NETIF_F_LRO;\r\nfeatures &= ~NETIF_F_GRO;\r\n}\r\nreturn features;\r\n}\r\nint bnx2x_set_features(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\nnetdev_features_t changes = features ^ dev->features;\r\nbool bnx2x_reload = false;\r\nint rc;\r\nif (!pci_num_vf(bp->pdev)) {\r\nif (features & NETIF_F_LOOPBACK) {\r\nif (bp->link_params.loopback_mode != LOOPBACK_BMAC) {\r\nbp->link_params.loopback_mode = LOOPBACK_BMAC;\r\nbnx2x_reload = true;\r\n}\r\n} else {\r\nif (bp->link_params.loopback_mode != LOOPBACK_NONE) {\r\nbp->link_params.loopback_mode = LOOPBACK_NONE;\r\nbnx2x_reload = true;\r\n}\r\n}\r\n}\r\nif ((changes & NETIF_F_GRO) && (features & NETIF_F_LRO))\r\nchanges &= ~NETIF_F_GRO;\r\nif ((changes & NETIF_F_GRO) && bp->disable_tpa)\r\nchanges &= ~NETIF_F_GRO;\r\nif (changes)\r\nbnx2x_reload = true;\r\nif (bnx2x_reload) {\r\nif (bp->recovery_state == BNX2X_RECOVERY_DONE) {\r\ndev->features = features;\r\nrc = bnx2x_reload_if_running(dev);\r\nreturn rc ? rc : 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid bnx2x_tx_timeout(struct net_device *dev)\r\n{\r\nstruct bnx2x *bp = netdev_priv(dev);\r\n#ifdef BNX2X_STOP_ON_ERROR\r\nif (!bp->panic)\r\nbnx2x_panic();\r\n#endif\r\nbnx2x_schedule_sp_rtnl(bp, BNX2X_SP_RTNL_TX_TIMEOUT, 0);\r\n}\r\nint bnx2x_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct bnx2x *bp;\r\nif (!dev) {\r\ndev_err(&pdev->dev, "BAD net device from bnx2x_init_one\n");\r\nreturn -ENODEV;\r\n}\r\nbp = netdev_priv(dev);\r\nrtnl_lock();\r\npci_save_state(pdev);\r\nif (!netif_running(dev)) {\r\nrtnl_unlock();\r\nreturn 0;\r\n}\r\nnetif_device_detach(dev);\r\nbnx2x_nic_unload(bp, UNLOAD_CLOSE, false);\r\nbnx2x_set_power_state(bp, pci_choose_state(pdev, state));\r\nrtnl_unlock();\r\nreturn 0;\r\n}\r\nint bnx2x_resume(struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct bnx2x *bp;\r\nint rc;\r\nif (!dev) {\r\ndev_err(&pdev->dev, "BAD net device from bnx2x_init_one\n");\r\nreturn -ENODEV;\r\n}\r\nbp = netdev_priv(dev);\r\nif (bp->recovery_state != BNX2X_RECOVERY_DONE) {\r\nBNX2X_ERR("Handling parity error recovery. Try again later\n");\r\nreturn -EAGAIN;\r\n}\r\nrtnl_lock();\r\npci_restore_state(pdev);\r\nif (!netif_running(dev)) {\r\nrtnl_unlock();\r\nreturn 0;\r\n}\r\nbnx2x_set_power_state(bp, PCI_D0);\r\nnetif_device_attach(dev);\r\nrc = bnx2x_nic_load(bp, LOAD_OPEN);\r\nrtnl_unlock();\r\nreturn rc;\r\n}\r\nvoid bnx2x_set_ctx_validation(struct bnx2x *bp, struct eth_context *cxt,\r\nu32 cid)\r\n{\r\nif (!cxt) {\r\nBNX2X_ERR("bad context pointer %p\n", cxt);\r\nreturn;\r\n}\r\ncxt->ustorm_ag_context.cdu_usage =\r\nCDU_RSRVD_VALUE_TYPE_A(HW_CID(bp, cid),\r\nCDU_REGION_NUMBER_UCM_AG, ETH_CONNECTION_TYPE);\r\ncxt->xstorm_ag_context.cdu_reserved =\r\nCDU_RSRVD_VALUE_TYPE_A(HW_CID(bp, cid),\r\nCDU_REGION_NUMBER_XCM_AG, ETH_CONNECTION_TYPE);\r\n}\r\nstatic void storm_memset_hc_timeout(struct bnx2x *bp, u8 port,\r\nu8 fw_sb_id, u8 sb_index,\r\nu8 ticks)\r\n{\r\nu32 addr = BAR_CSTRORM_INTMEM +\r\nCSTORM_STATUS_BLOCK_DATA_TIMEOUT_OFFSET(fw_sb_id, sb_index);\r\nREG_WR8(bp, addr, ticks);\r\nDP(NETIF_MSG_IFUP,\r\n"port %x fw_sb_id %d sb_index %d ticks %d\n",\r\nport, fw_sb_id, sb_index, ticks);\r\n}\r\nstatic void storm_memset_hc_disable(struct bnx2x *bp, u8 port,\r\nu16 fw_sb_id, u8 sb_index,\r\nu8 disable)\r\n{\r\nu32 enable_flag = disable ? 0 : (1 << HC_INDEX_DATA_HC_ENABLED_SHIFT);\r\nu32 addr = BAR_CSTRORM_INTMEM +\r\nCSTORM_STATUS_BLOCK_DATA_FLAGS_OFFSET(fw_sb_id, sb_index);\r\nu8 flags = REG_RD8(bp, addr);\r\nflags &= ~HC_INDEX_DATA_HC_ENABLED;\r\nflags |= enable_flag;\r\nREG_WR8(bp, addr, flags);\r\nDP(NETIF_MSG_IFUP,\r\n"port %x fw_sb_id %d sb_index %d disable %d\n",\r\nport, fw_sb_id, sb_index, disable);\r\n}\r\nvoid bnx2x_update_coalesce_sb_index(struct bnx2x *bp, u8 fw_sb_id,\r\nu8 sb_index, u8 disable, u16 usec)\r\n{\r\nint port = BP_PORT(bp);\r\nu8 ticks = usec / BNX2X_BTR;\r\nstorm_memset_hc_timeout(bp, port, fw_sb_id, sb_index, ticks);\r\ndisable = disable ? 1 : (usec ? 0 : 1);\r\nstorm_memset_hc_disable(bp, port, fw_sb_id, sb_index, disable);\r\n}\r\nvoid bnx2x_schedule_sp_rtnl(struct bnx2x *bp, enum sp_rtnl_flag flag,\r\nu32 verbose)\r\n{\r\nsmp_mb__before_atomic();\r\nset_bit(flag, &bp->sp_rtnl_state);\r\nsmp_mb__after_atomic();\r\nDP((BNX2X_MSG_SP | verbose), "Scheduling sp_rtnl task [Flag: %d]\n",\r\nflag);\r\nschedule_delayed_work(&bp->sp_rtnl_task, 0);\r\n}
