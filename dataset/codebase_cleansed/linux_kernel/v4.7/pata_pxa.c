static void pxa_ata_dma_irq(void *d)\r\n{\r\nstruct pata_pxa_data *pd = d;\r\nenum dma_status status;\r\nstatus = dmaengine_tx_status(pd->dma_chan, pd->dma_cookie, NULL);\r\nif (status == DMA_ERROR || status == DMA_COMPLETE)\r\ncomplete(&pd->dma_done);\r\n}\r\nstatic void pxa_qc_prep(struct ata_queued_cmd *qc)\r\n{\r\nstruct pata_pxa_data *pd = qc->ap->private_data;\r\nstruct dma_async_tx_descriptor *tx;\r\nenum dma_transfer_direction dir;\r\nif (!(qc->flags & ATA_QCFLAG_DMAMAP))\r\nreturn;\r\ndir = (qc->dma_dir == DMA_TO_DEVICE ? DMA_MEM_TO_DEV : DMA_DEV_TO_MEM);\r\ntx = dmaengine_prep_slave_sg(pd->dma_chan, qc->sg, qc->n_elem, dir,\r\nDMA_PREP_INTERRUPT);\r\nif (!tx) {\r\nata_dev_err(qc->dev, "prep_slave_sg() failed\n");\r\nreturn;\r\n}\r\ntx->callback = pxa_ata_dma_irq;\r\ntx->callback_param = pd;\r\npd->dma_cookie = dmaengine_submit(tx);\r\n}\r\nstatic void pxa_bmdma_setup(struct ata_queued_cmd *qc)\r\n{\r\nqc->ap->ops->sff_exec_command(qc->ap, &qc->tf);\r\n}\r\nstatic void pxa_bmdma_start(struct ata_queued_cmd *qc)\r\n{\r\nstruct pata_pxa_data *pd = qc->ap->private_data;\r\ninit_completion(&pd->dma_done);\r\ndma_async_issue_pending(pd->dma_chan);\r\n}\r\nstatic void pxa_bmdma_stop(struct ata_queued_cmd *qc)\r\n{\r\nstruct pata_pxa_data *pd = qc->ap->private_data;\r\nenum dma_status status;\r\nstatus = dmaengine_tx_status(pd->dma_chan, pd->dma_cookie, NULL);\r\nif (status != DMA_ERROR && status != DMA_COMPLETE &&\r\nwait_for_completion_timeout(&pd->dma_done, HZ))\r\nata_dev_err(qc->dev, "Timeout waiting for DMA completion!");\r\ndmaengine_terminate_all(pd->dma_chan);\r\n}\r\nstatic unsigned char pxa_bmdma_status(struct ata_port *ap)\r\n{\r\nstruct pata_pxa_data *pd = ap->private_data;\r\nunsigned char ret = ATA_DMA_INTR;\r\nstruct dma_tx_state state;\r\nenum dma_status status;\r\nstatus = dmaengine_tx_status(pd->dma_chan, pd->dma_cookie, &state);\r\nif (status != DMA_COMPLETE)\r\nret |= ATA_DMA_ERR;\r\nreturn ret;\r\n}\r\nstatic void pxa_irq_clear(struct ata_port *ap)\r\n{\r\n}\r\nstatic int pxa_check_atapi_dma(struct ata_queued_cmd *qc)\r\n{\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int pxa_ata_probe(struct platform_device *pdev)\r\n{\r\nstruct ata_host *host;\r\nstruct ata_port *ap;\r\nstruct pata_pxa_data *data;\r\nstruct resource *cmd_res;\r\nstruct resource *ctl_res;\r\nstruct resource *dma_res;\r\nstruct resource *irq_res;\r\nstruct pata_pxa_pdata *pdata = dev_get_platdata(&pdev->dev);\r\nstruct dma_slave_config config;\r\ndma_cap_mask_t mask;\r\nstruct pxad_param param;\r\nint ret = 0;\r\nif (pdev->num_resources != 4) {\r\ndev_err(&pdev->dev, "invalid number of resources\n");\r\nreturn -EINVAL;\r\n}\r\ncmd_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (unlikely(cmd_res == NULL))\r\nreturn -EINVAL;\r\nctl_res = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (unlikely(ctl_res == NULL))\r\nreturn -EINVAL;\r\ndma_res = platform_get_resource(pdev, IORESOURCE_DMA, 0);\r\nif (unlikely(dma_res == NULL))\r\nreturn -EINVAL;\r\nirq_res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (unlikely(irq_res == NULL))\r\nreturn -EINVAL;\r\nhost = ata_host_alloc(&pdev->dev, 1);\r\nif (!host)\r\nreturn -ENOMEM;\r\nap = host->ports[0];\r\nap->ops = &pxa_ata_port_ops;\r\nap->pio_mask = ATA_PIO4;\r\nap->mwdma_mask = ATA_MWDMA2;\r\nap->ioaddr.cmd_addr = devm_ioremap(&pdev->dev, cmd_res->start,\r\nresource_size(cmd_res));\r\nap->ioaddr.ctl_addr = devm_ioremap(&pdev->dev, ctl_res->start,\r\nresource_size(ctl_res));\r\nap->ioaddr.bmdma_addr = devm_ioremap(&pdev->dev, dma_res->start,\r\nresource_size(dma_res));\r\nap->ioaddr.altstatus_addr = ap->ioaddr.ctl_addr;\r\nap->ioaddr.data_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_DATA << pdata->reg_shift);\r\nap->ioaddr.error_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_ERR << pdata->reg_shift);\r\nap->ioaddr.feature_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_FEATURE << pdata->reg_shift);\r\nap->ioaddr.nsect_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_NSECT << pdata->reg_shift);\r\nap->ioaddr.lbal_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_LBAL << pdata->reg_shift);\r\nap->ioaddr.lbam_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_LBAM << pdata->reg_shift);\r\nap->ioaddr.lbah_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_LBAH << pdata->reg_shift);\r\nap->ioaddr.device_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_DEVICE << pdata->reg_shift);\r\nap->ioaddr.status_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_STATUS << pdata->reg_shift);\r\nap->ioaddr.command_addr = ap->ioaddr.cmd_addr +\r\n(ATA_REG_CMD << pdata->reg_shift);\r\ndata = devm_kzalloc(&pdev->dev, sizeof(struct pata_pxa_data),\r\nGFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\nap->private_data = data;\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\nparam.prio = PXAD_PRIO_LOWEST;\r\nparam.drcmr = pdata->dma_dreq;\r\nmemset(&config, 0, sizeof(config));\r\nconfig.src_addr_width = DMA_SLAVE_BUSWIDTH_2_BYTES;\r\nconfig.dst_addr_width = DMA_SLAVE_BUSWIDTH_2_BYTES;\r\nconfig.src_addr = dma_res->start;\r\nconfig.dst_addr = dma_res->start;\r\nconfig.src_maxburst = 32;\r\nconfig.dst_maxburst = 32;\r\ndata->dma_chan =\r\ndma_request_slave_channel_compat(mask, pxad_filter_fn,\r\n&param, &pdev->dev, "data");\r\nif (!data->dma_chan)\r\nreturn -EBUSY;\r\nret = dmaengine_slave_config(data->dma_chan, &config);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "dma configuration failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nret = ata_host_activate(host, irq_res->start, ata_sff_interrupt,\r\npdata->irq_flags, &pxa_ata_sht);\r\nif (ret)\r\ndma_release_channel(data->dma_chan);\r\nreturn ret;\r\n}\r\nstatic int pxa_ata_remove(struct platform_device *pdev)\r\n{\r\nstruct ata_host *host = platform_get_drvdata(pdev);\r\nstruct pata_pxa_data *data = host->ports[0]->private_data;\r\ndma_release_channel(data->dma_chan);\r\nata_host_detach(host);\r\nreturn 0;\r\n}
