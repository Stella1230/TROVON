static void __ib_umem_release(struct ib_device *dev, struct ib_umem *umem, int dirty)\r\n{\r\nstruct scatterlist *sg;\r\nstruct page *page;\r\nint i;\r\nif (umem->nmap > 0)\r\nib_dma_unmap_sg(dev, umem->sg_head.sgl,\r\numem->nmap,\r\nDMA_BIDIRECTIONAL);\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->npages, i) {\r\npage = sg_page(sg);\r\nif (umem->writable && dirty)\r\nset_page_dirty_lock(page);\r\nput_page(page);\r\n}\r\nsg_free_table(&umem->sg_head);\r\nreturn;\r\n}\r\nstruct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,\r\nsize_t size, int access, int dmasync)\r\n{\r\nstruct ib_umem *umem;\r\nstruct page **page_list;\r\nstruct vm_area_struct **vma_list;\r\nunsigned long locked;\r\nunsigned long lock_limit;\r\nunsigned long cur_base;\r\nunsigned long npages;\r\nint ret;\r\nint i;\r\nDEFINE_DMA_ATTRS(attrs);\r\nstruct scatterlist *sg, *sg_list_start;\r\nint need_release = 0;\r\nif (dmasync)\r\ndma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);\r\nif (!size)\r\nreturn ERR_PTR(-EINVAL);\r\nif (((addr + size) < addr) ||\r\nPAGE_ALIGN(addr + size) < (addr + size))\r\nreturn ERR_PTR(-EINVAL);\r\nif (!can_do_mlock())\r\nreturn ERR_PTR(-EPERM);\r\numem = kzalloc(sizeof *umem, GFP_KERNEL);\r\nif (!umem)\r\nreturn ERR_PTR(-ENOMEM);\r\numem->context = context;\r\numem->length = size;\r\numem->address = addr;\r\numem->page_size = PAGE_SIZE;\r\numem->pid = get_task_pid(current, PIDTYPE_PID);\r\numem->writable = !!(access &\r\n(IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_ATOMIC | IB_ACCESS_MW_BIND));\r\nif (access & IB_ACCESS_ON_DEMAND) {\r\nret = ib_umem_odp_get(context, umem);\r\nif (ret) {\r\nkfree(umem);\r\nreturn ERR_PTR(ret);\r\n}\r\nreturn umem;\r\n}\r\numem->odp_data = NULL;\r\numem->hugetlb = 1;\r\npage_list = (struct page **) __get_free_page(GFP_KERNEL);\r\nif (!page_list) {\r\nkfree(umem);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvma_list = (struct vm_area_struct **) __get_free_page(GFP_KERNEL);\r\nif (!vma_list)\r\numem->hugetlb = 0;\r\nnpages = ib_umem_num_pages(umem);\r\ndown_write(&current->mm->mmap_sem);\r\nlocked = npages + current->mm->pinned_vm;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ncur_base = addr & PAGE_MASK;\r\nif (npages == 0) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nret = sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL);\r\nif (ret)\r\ngoto out;\r\nneed_release = 1;\r\nsg_list_start = umem->sg_head.sgl;\r\nwhile (npages) {\r\nret = get_user_pages(cur_base,\r\nmin_t(unsigned long, npages,\r\nPAGE_SIZE / sizeof (struct page *)),\r\n1, !umem->writable, page_list, vma_list);\r\nif (ret < 0)\r\ngoto out;\r\numem->npages += ret;\r\ncur_base += ret * PAGE_SIZE;\r\nnpages -= ret;\r\nfor_each_sg(sg_list_start, sg, ret, i) {\r\nif (vma_list && !is_vm_hugetlb_page(vma_list[i]))\r\numem->hugetlb = 0;\r\nsg_set_page(sg, page_list[i], PAGE_SIZE, 0);\r\n}\r\nsg_list_start = sg;\r\n}\r\numem->nmap = ib_dma_map_sg_attrs(context->device,\r\numem->sg_head.sgl,\r\numem->npages,\r\nDMA_BIDIRECTIONAL,\r\n&attrs);\r\nif (umem->nmap <= 0) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = 0;\r\nout:\r\nif (ret < 0) {\r\nif (need_release)\r\n__ib_umem_release(context->device, umem, 0);\r\nput_pid(umem->pid);\r\nkfree(umem);\r\n} else\r\ncurrent->mm->pinned_vm = locked;\r\nup_write(&current->mm->mmap_sem);\r\nif (vma_list)\r\nfree_page((unsigned long) vma_list);\r\nfree_page((unsigned long) page_list);\r\nreturn ret < 0 ? ERR_PTR(ret) : umem;\r\n}\r\nstatic void ib_umem_account(struct work_struct *work)\r\n{\r\nstruct ib_umem *umem = container_of(work, struct ib_umem, work);\r\ndown_write(&umem->mm->mmap_sem);\r\numem->mm->pinned_vm -= umem->diff;\r\nup_write(&umem->mm->mmap_sem);\r\nmmput(umem->mm);\r\nkfree(umem);\r\n}\r\nvoid ib_umem_release(struct ib_umem *umem)\r\n{\r\nstruct ib_ucontext *context = umem->context;\r\nstruct mm_struct *mm;\r\nstruct task_struct *task;\r\nunsigned long diff;\r\nif (umem->odp_data) {\r\nib_umem_odp_release(umem);\r\nreturn;\r\n}\r\n__ib_umem_release(umem->context->device, umem, 1);\r\ntask = get_pid_task(umem->pid, PIDTYPE_PID);\r\nput_pid(umem->pid);\r\nif (!task)\r\ngoto out;\r\nmm = get_task_mm(task);\r\nput_task_struct(task);\r\nif (!mm)\r\ngoto out;\r\ndiff = ib_umem_num_pages(umem);\r\nif (context->closing) {\r\nif (!down_write_trylock(&mm->mmap_sem)) {\r\nINIT_WORK(&umem->work, ib_umem_account);\r\numem->mm = mm;\r\numem->diff = diff;\r\nqueue_work(ib_wq, &umem->work);\r\nreturn;\r\n}\r\n} else\r\ndown_write(&mm->mmap_sem);\r\nmm->pinned_vm -= diff;\r\nup_write(&mm->mmap_sem);\r\nmmput(mm);\r\nout:\r\nkfree(umem);\r\n}\r\nint ib_umem_page_count(struct ib_umem *umem)\r\n{\r\nint shift;\r\nint i;\r\nint n;\r\nstruct scatterlist *sg;\r\nif (umem->odp_data)\r\nreturn ib_umem_num_pages(umem);\r\nshift = ilog2(umem->page_size);\r\nn = 0;\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->nmap, i)\r\nn += sg_dma_len(sg) >> shift;\r\nreturn n;\r\n}\r\nint ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,\r\nsize_t length)\r\n{\r\nsize_t end = offset + length;\r\nint ret;\r\nif (offset > umem->length || length > umem->length - offset) {\r\npr_err("ib_umem_copy_from not in range. offset: %zd umem length: %zd end: %zd\n",\r\noffset, umem->length, end);\r\nreturn -EINVAL;\r\n}\r\nret = sg_pcopy_to_buffer(umem->sg_head.sgl, umem->nmap, dst, length,\r\noffset + ib_umem_offset(umem));\r\nif (ret < 0)\r\nreturn ret;\r\nelse if (ret != length)\r\nreturn -EINVAL;\r\nelse\r\nreturn 0;\r\n}
