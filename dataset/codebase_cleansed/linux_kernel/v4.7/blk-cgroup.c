static bool blkcg_policy_enabled(struct request_queue *q,\r\nconst struct blkcg_policy *pol)\r\n{\r\nreturn pol && test_bit(pol->plid, q->blkcg_pols);\r\n}\r\nstatic void blkg_free(struct blkcg_gq *blkg)\r\n{\r\nint i;\r\nif (!blkg)\r\nreturn;\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++)\r\nif (blkg->pd[i])\r\nblkcg_policy[i]->pd_free_fn(blkg->pd[i]);\r\nif (blkg->blkcg != &blkcg_root)\r\nblk_exit_rl(&blkg->rl);\r\nblkg_rwstat_exit(&blkg->stat_ios);\r\nblkg_rwstat_exit(&blkg->stat_bytes);\r\nkfree(blkg);\r\n}\r\nstatic struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct request_queue *q,\r\ngfp_t gfp_mask)\r\n{\r\nstruct blkcg_gq *blkg;\r\nint i;\r\nblkg = kzalloc_node(sizeof(*blkg), gfp_mask, q->node);\r\nif (!blkg)\r\nreturn NULL;\r\nif (blkg_rwstat_init(&blkg->stat_bytes, gfp_mask) ||\r\nblkg_rwstat_init(&blkg->stat_ios, gfp_mask))\r\ngoto err_free;\r\nblkg->q = q;\r\nINIT_LIST_HEAD(&blkg->q_node);\r\nblkg->blkcg = blkcg;\r\natomic_set(&blkg->refcnt, 1);\r\nif (blkcg != &blkcg_root) {\r\nif (blk_init_rl(&blkg->rl, q, gfp_mask))\r\ngoto err_free;\r\nblkg->rl.blkg = blkg;\r\n}\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nstruct blkg_policy_data *pd;\r\nif (!blkcg_policy_enabled(q, pol))\r\ncontinue;\r\npd = pol->pd_alloc_fn(gfp_mask, q->node);\r\nif (!pd)\r\ngoto err_free;\r\nblkg->pd[i] = pd;\r\npd->blkg = blkg;\r\npd->plid = i;\r\n}\r\nreturn blkg;\r\nerr_free:\r\nblkg_free(blkg);\r\nreturn NULL;\r\n}\r\nstruct blkcg_gq *blkg_lookup_slowpath(struct blkcg *blkcg,\r\nstruct request_queue *q, bool update_hint)\r\n{\r\nstruct blkcg_gq *blkg;\r\nblkg = radix_tree_lookup(&blkcg->blkg_tree, q->id);\r\nif (blkg && blkg->q == q) {\r\nif (update_hint) {\r\nlockdep_assert_held(q->queue_lock);\r\nrcu_assign_pointer(blkcg->blkg_hint, blkg);\r\n}\r\nreturn blkg;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct blkcg_gq *blkg_create(struct blkcg *blkcg,\r\nstruct request_queue *q,\r\nstruct blkcg_gq *new_blkg)\r\n{\r\nstruct blkcg_gq *blkg;\r\nstruct bdi_writeback_congested *wb_congested;\r\nint i, ret;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nlockdep_assert_held(q->queue_lock);\r\nif (!css_tryget_online(&blkcg->css)) {\r\nret = -ENODEV;\r\ngoto err_free_blkg;\r\n}\r\nwb_congested = wb_congested_get_create(&q->backing_dev_info,\r\nblkcg->css.id, GFP_NOWAIT);\r\nif (!wb_congested) {\r\nret = -ENOMEM;\r\ngoto err_put_css;\r\n}\r\nif (!new_blkg) {\r\nnew_blkg = blkg_alloc(blkcg, q, GFP_NOWAIT);\r\nif (unlikely(!new_blkg)) {\r\nret = -ENOMEM;\r\ngoto err_put_congested;\r\n}\r\n}\r\nblkg = new_blkg;\r\nblkg->wb_congested = wb_congested;\r\nif (blkcg_parent(blkcg)) {\r\nblkg->parent = __blkg_lookup(blkcg_parent(blkcg), q, false);\r\nif (WARN_ON_ONCE(!blkg->parent)) {\r\nret = -ENODEV;\r\ngoto err_put_congested;\r\n}\r\nblkg_get(blkg->parent);\r\n}\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_init_fn)\r\npol->pd_init_fn(blkg->pd[i]);\r\n}\r\nspin_lock(&blkcg->lock);\r\nret = radix_tree_insert(&blkcg->blkg_tree, q->id, blkg);\r\nif (likely(!ret)) {\r\nhlist_add_head_rcu(&blkg->blkcg_node, &blkcg->blkg_list);\r\nlist_add(&blkg->q_node, &q->blkg_list);\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_online_fn)\r\npol->pd_online_fn(blkg->pd[i]);\r\n}\r\n}\r\nblkg->online = true;\r\nspin_unlock(&blkcg->lock);\r\nif (!ret)\r\nreturn blkg;\r\nblkg_put(blkg);\r\nreturn ERR_PTR(ret);\r\nerr_put_congested:\r\nwb_congested_put(wb_congested);\r\nerr_put_css:\r\ncss_put(&blkcg->css);\r\nerr_free_blkg:\r\nblkg_free(new_blkg);\r\nreturn ERR_PTR(ret);\r\n}\r\nstruct blkcg_gq *blkg_lookup_create(struct blkcg *blkcg,\r\nstruct request_queue *q)\r\n{\r\nstruct blkcg_gq *blkg;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nlockdep_assert_held(q->queue_lock);\r\nif (unlikely(blk_queue_bypass(q)))\r\nreturn ERR_PTR(blk_queue_dying(q) ? -ENODEV : -EBUSY);\r\nblkg = __blkg_lookup(blkcg, q, true);\r\nif (blkg)\r\nreturn blkg;\r\nwhile (true) {\r\nstruct blkcg *pos = blkcg;\r\nstruct blkcg *parent = blkcg_parent(blkcg);\r\nwhile (parent && !__blkg_lookup(parent, q, false)) {\r\npos = parent;\r\nparent = blkcg_parent(parent);\r\n}\r\nblkg = blkg_create(pos, q, NULL);\r\nif (pos == blkcg || IS_ERR(blkg))\r\nreturn blkg;\r\n}\r\n}\r\nstatic void blkg_destroy(struct blkcg_gq *blkg)\r\n{\r\nstruct blkcg *blkcg = blkg->blkcg;\r\nstruct blkcg_gq *parent = blkg->parent;\r\nint i;\r\nlockdep_assert_held(blkg->q->queue_lock);\r\nlockdep_assert_held(&blkcg->lock);\r\nWARN_ON_ONCE(list_empty(&blkg->q_node));\r\nWARN_ON_ONCE(hlist_unhashed(&blkg->blkcg_node));\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_offline_fn)\r\npol->pd_offline_fn(blkg->pd[i]);\r\n}\r\nif (parent) {\r\nblkg_rwstat_add_aux(&parent->stat_bytes, &blkg->stat_bytes);\r\nblkg_rwstat_add_aux(&parent->stat_ios, &blkg->stat_ios);\r\n}\r\nblkg->online = false;\r\nradix_tree_delete(&blkcg->blkg_tree, blkg->q->id);\r\nlist_del_init(&blkg->q_node);\r\nhlist_del_init_rcu(&blkg->blkcg_node);\r\nif (rcu_access_pointer(blkcg->blkg_hint) == blkg)\r\nrcu_assign_pointer(blkcg->blkg_hint, NULL);\r\nblkg_put(blkg);\r\n}\r\nstatic void blkg_destroy_all(struct request_queue *q)\r\n{\r\nstruct blkcg_gq *blkg, *n;\r\nlockdep_assert_held(q->queue_lock);\r\nlist_for_each_entry_safe(blkg, n, &q->blkg_list, q_node) {\r\nstruct blkcg *blkcg = blkg->blkcg;\r\nspin_lock(&blkcg->lock);\r\nblkg_destroy(blkg);\r\nspin_unlock(&blkcg->lock);\r\n}\r\nq->root_blkg = NULL;\r\nq->root_rl.blkg = NULL;\r\n}\r\nvoid __blkg_release_rcu(struct rcu_head *rcu_head)\r\n{\r\nstruct blkcg_gq *blkg = container_of(rcu_head, struct blkcg_gq, rcu_head);\r\ncss_put(&blkg->blkcg->css);\r\nif (blkg->parent)\r\nblkg_put(blkg->parent);\r\nwb_congested_put(blkg->wb_congested);\r\nblkg_free(blkg);\r\n}\r\nstruct request_list *__blk_queue_next_rl(struct request_list *rl,\r\nstruct request_queue *q)\r\n{\r\nstruct list_head *ent;\r\nstruct blkcg_gq *blkg;\r\nif (rl == &q->root_rl) {\r\nent = &q->blkg_list;\r\nif (list_empty(ent))\r\nreturn NULL;\r\n} else {\r\nblkg = container_of(rl, struct blkcg_gq, rl);\r\nent = &blkg->q_node;\r\n}\r\nent = ent->next;\r\nif (ent == &q->root_blkg->q_node)\r\nent = ent->next;\r\nif (ent == &q->blkg_list)\r\nreturn NULL;\r\nblkg = container_of(ent, struct blkcg_gq, q_node);\r\nreturn &blkg->rl;\r\n}\r\nstatic int blkcg_reset_stats(struct cgroup_subsys_state *css,\r\nstruct cftype *cftype, u64 val)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(css);\r\nstruct blkcg_gq *blkg;\r\nint i;\r\nmutex_lock(&blkcg_pol_mutex);\r\nspin_lock_irq(&blkcg->lock);\r\nhlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {\r\nblkg_rwstat_reset(&blkg->stat_bytes);\r\nblkg_rwstat_reset(&blkg->stat_ios);\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_reset_stats_fn)\r\npol->pd_reset_stats_fn(blkg->pd[i]);\r\n}\r\n}\r\nspin_unlock_irq(&blkcg->lock);\r\nmutex_unlock(&blkcg_pol_mutex);\r\nreturn 0;\r\n}\r\nconst char *blkg_dev_name(struct blkcg_gq *blkg)\r\n{\r\nif (blkg->q->backing_dev_info.dev)\r\nreturn dev_name(blkg->q->backing_dev_info.dev);\r\nreturn NULL;\r\n}\r\nvoid blkcg_print_blkgs(struct seq_file *sf, struct blkcg *blkcg,\r\nu64 (*prfill)(struct seq_file *,\r\nstruct blkg_policy_data *, int),\r\nconst struct blkcg_policy *pol, int data,\r\nbool show_total)\r\n{\r\nstruct blkcg_gq *blkg;\r\nu64 total = 0;\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\r\nspin_lock_irq(blkg->q->queue_lock);\r\nif (blkcg_policy_enabled(blkg->q, pol))\r\ntotal += prfill(sf, blkg->pd[pol->plid], data);\r\nspin_unlock_irq(blkg->q->queue_lock);\r\n}\r\nrcu_read_unlock();\r\nif (show_total)\r\nseq_printf(sf, "Total %llu\n", (unsigned long long)total);\r\n}\r\nu64 __blkg_prfill_u64(struct seq_file *sf, struct blkg_policy_data *pd, u64 v)\r\n{\r\nconst char *dname = blkg_dev_name(pd->blkg);\r\nif (!dname)\r\nreturn 0;\r\nseq_printf(sf, "%s %llu\n", dname, (unsigned long long)v);\r\nreturn v;\r\n}\r\nu64 __blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\r\nconst struct blkg_rwstat *rwstat)\r\n{\r\nstatic const char *rwstr[] = {\r\n[BLKG_RWSTAT_READ] = "Read",\r\n[BLKG_RWSTAT_WRITE] = "Write",\r\n[BLKG_RWSTAT_SYNC] = "Sync",\r\n[BLKG_RWSTAT_ASYNC] = "Async",\r\n};\r\nconst char *dname = blkg_dev_name(pd->blkg);\r\nu64 v;\r\nint i;\r\nif (!dname)\r\nreturn 0;\r\nfor (i = 0; i < BLKG_RWSTAT_NR; i++)\r\nseq_printf(sf, "%s %s %llu\n", dname, rwstr[i],\r\n(unsigned long long)atomic64_read(&rwstat->aux_cnt[i]));\r\nv = atomic64_read(&rwstat->aux_cnt[BLKG_RWSTAT_READ]) +\r\natomic64_read(&rwstat->aux_cnt[BLKG_RWSTAT_WRITE]);\r\nseq_printf(sf, "%s Total %llu\n", dname, (unsigned long long)v);\r\nreturn v;\r\n}\r\nu64 blkg_prfill_stat(struct seq_file *sf, struct blkg_policy_data *pd, int off)\r\n{\r\nreturn __blkg_prfill_u64(sf, pd, blkg_stat_read((void *)pd + off));\r\n}\r\nu64 blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct blkg_rwstat rwstat = blkg_rwstat_read((void *)pd + off);\r\nreturn __blkg_prfill_rwstat(sf, pd, &rwstat);\r\n}\r\nstatic u64 blkg_prfill_rwstat_field(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct blkg_rwstat rwstat = blkg_rwstat_read((void *)pd->blkg + off);\r\nreturn __blkg_prfill_rwstat(sf, pd, &rwstat);\r\n}\r\nint blkg_print_stat_bytes(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\nblkg_prfill_rwstat_field, (void *)seq_cft(sf)->private,\r\noffsetof(struct blkcg_gq, stat_bytes), true);\r\nreturn 0;\r\n}\r\nint blkg_print_stat_ios(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\nblkg_prfill_rwstat_field, (void *)seq_cft(sf)->private,\r\noffsetof(struct blkcg_gq, stat_ios), true);\r\nreturn 0;\r\n}\r\nstatic u64 blkg_prfill_rwstat_field_recursive(struct seq_file *sf,\r\nstruct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct blkg_rwstat rwstat = blkg_rwstat_recursive_sum(pd->blkg,\r\nNULL, off);\r\nreturn __blkg_prfill_rwstat(sf, pd, &rwstat);\r\n}\r\nint blkg_print_stat_bytes_recursive(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\nblkg_prfill_rwstat_field_recursive,\r\n(void *)seq_cft(sf)->private,\r\noffsetof(struct blkcg_gq, stat_bytes), true);\r\nreturn 0;\r\n}\r\nint blkg_print_stat_ios_recursive(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\r\nblkg_prfill_rwstat_field_recursive,\r\n(void *)seq_cft(sf)->private,\r\noffsetof(struct blkcg_gq, stat_ios), true);\r\nreturn 0;\r\n}\r\nu64 blkg_stat_recursive_sum(struct blkcg_gq *blkg,\r\nstruct blkcg_policy *pol, int off)\r\n{\r\nstruct blkcg_gq *pos_blkg;\r\nstruct cgroup_subsys_state *pos_css;\r\nu64 sum = 0;\r\nlockdep_assert_held(blkg->q->queue_lock);\r\nrcu_read_lock();\r\nblkg_for_each_descendant_pre(pos_blkg, pos_css, blkg) {\r\nstruct blkg_stat *stat;\r\nif (!pos_blkg->online)\r\ncontinue;\r\nif (pol)\r\nstat = (void *)blkg_to_pd(pos_blkg, pol) + off;\r\nelse\r\nstat = (void *)blkg + off;\r\nsum += blkg_stat_read(stat) + atomic64_read(&stat->aux_cnt);\r\n}\r\nrcu_read_unlock();\r\nreturn sum;\r\n}\r\nstruct blkg_rwstat blkg_rwstat_recursive_sum(struct blkcg_gq *blkg,\r\nstruct blkcg_policy *pol, int off)\r\n{\r\nstruct blkcg_gq *pos_blkg;\r\nstruct cgroup_subsys_state *pos_css;\r\nstruct blkg_rwstat sum = { };\r\nint i;\r\nlockdep_assert_held(blkg->q->queue_lock);\r\nrcu_read_lock();\r\nblkg_for_each_descendant_pre(pos_blkg, pos_css, blkg) {\r\nstruct blkg_rwstat *rwstat;\r\nif (!pos_blkg->online)\r\ncontinue;\r\nif (pol)\r\nrwstat = (void *)blkg_to_pd(pos_blkg, pol) + off;\r\nelse\r\nrwstat = (void *)pos_blkg + off;\r\nfor (i = 0; i < BLKG_RWSTAT_NR; i++)\r\natomic64_add(atomic64_read(&rwstat->aux_cnt[i]) +\r\npercpu_counter_sum_positive(&rwstat->cpu_cnt[i]),\r\n&sum.aux_cnt[i]);\r\n}\r\nrcu_read_unlock();\r\nreturn sum;\r\n}\r\nint blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,\r\nchar *input, struct blkg_conf_ctx *ctx)\r\n__acquires(rcu) __acquires(disk->queue->queue_lock)\r\n{\r\nstruct gendisk *disk;\r\nstruct blkcg_gq *blkg;\r\nstruct module *owner;\r\nunsigned int major, minor;\r\nint key_len, part, ret;\r\nchar *body;\r\nif (sscanf(input, "%u:%u%n", &major, &minor, &key_len) != 2)\r\nreturn -EINVAL;\r\nbody = input + key_len;\r\nif (!isspace(*body))\r\nreturn -EINVAL;\r\nbody = skip_spaces(body);\r\ndisk = get_gendisk(MKDEV(major, minor), &part);\r\nif (!disk)\r\nreturn -ENODEV;\r\nif (part) {\r\nowner = disk->fops->owner;\r\nput_disk(disk);\r\nmodule_put(owner);\r\nreturn -ENODEV;\r\n}\r\nrcu_read_lock();\r\nspin_lock_irq(disk->queue->queue_lock);\r\nif (blkcg_policy_enabled(disk->queue, pol))\r\nblkg = blkg_lookup_create(blkcg, disk->queue);\r\nelse\r\nblkg = ERR_PTR(-EOPNOTSUPP);\r\nif (IS_ERR(blkg)) {\r\nret = PTR_ERR(blkg);\r\nrcu_read_unlock();\r\nspin_unlock_irq(disk->queue->queue_lock);\r\nowner = disk->fops->owner;\r\nput_disk(disk);\r\nmodule_put(owner);\r\nif (ret == -EBUSY) {\r\nmsleep(10);\r\nret = restart_syscall();\r\n}\r\nreturn ret;\r\n}\r\nctx->disk = disk;\r\nctx->blkg = blkg;\r\nctx->body = body;\r\nreturn 0;\r\n}\r\nvoid blkg_conf_finish(struct blkg_conf_ctx *ctx)\r\n__releases(ctx->disk->queue->queue_lock) __releases(rcu)\r\n{\r\nstruct module *owner;\r\nspin_unlock_irq(ctx->disk->queue->queue_lock);\r\nrcu_read_unlock();\r\nowner = ctx->disk->fops->owner;\r\nput_disk(ctx->disk);\r\nmodule_put(owner);\r\n}\r\nstatic int blkcg_print_stat(struct seq_file *sf, void *v)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\r\nstruct blkcg_gq *blkg;\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\r\nconst char *dname;\r\nstruct blkg_rwstat rwstat;\r\nu64 rbytes, wbytes, rios, wios;\r\ndname = blkg_dev_name(blkg);\r\nif (!dname)\r\ncontinue;\r\nspin_lock_irq(blkg->q->queue_lock);\r\nrwstat = blkg_rwstat_recursive_sum(blkg, NULL,\r\noffsetof(struct blkcg_gq, stat_bytes));\r\nrbytes = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_READ]);\r\nwbytes = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_WRITE]);\r\nrwstat = blkg_rwstat_recursive_sum(blkg, NULL,\r\noffsetof(struct blkcg_gq, stat_ios));\r\nrios = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_READ]);\r\nwios = atomic64_read(&rwstat.aux_cnt[BLKG_RWSTAT_WRITE]);\r\nspin_unlock_irq(blkg->q->queue_lock);\r\nif (rbytes || wbytes || rios || wios)\r\nseq_printf(sf, "%s rbytes=%llu wbytes=%llu rios=%llu wios=%llu\n",\r\ndname, rbytes, wbytes, rios, wios);\r\n}\r\nrcu_read_unlock();\r\nreturn 0;\r\n}\r\nstatic void blkcg_css_offline(struct cgroup_subsys_state *css)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(css);\r\nspin_lock_irq(&blkcg->lock);\r\nwhile (!hlist_empty(&blkcg->blkg_list)) {\r\nstruct blkcg_gq *blkg = hlist_entry(blkcg->blkg_list.first,\r\nstruct blkcg_gq, blkcg_node);\r\nstruct request_queue *q = blkg->q;\r\nif (spin_trylock(q->queue_lock)) {\r\nblkg_destroy(blkg);\r\nspin_unlock(q->queue_lock);\r\n} else {\r\nspin_unlock_irq(&blkcg->lock);\r\ncpu_relax();\r\nspin_lock_irq(&blkcg->lock);\r\n}\r\n}\r\nspin_unlock_irq(&blkcg->lock);\r\nwb_blkcg_offline(blkcg);\r\n}\r\nstatic void blkcg_css_free(struct cgroup_subsys_state *css)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(css);\r\nint i;\r\nmutex_lock(&blkcg_pol_mutex);\r\nlist_del(&blkcg->all_blkcgs_node);\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++)\r\nif (blkcg->cpd[i])\r\nblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\r\nmutex_unlock(&blkcg_pol_mutex);\r\nkfree(blkcg);\r\n}\r\nstatic struct cgroup_subsys_state *\r\nblkcg_css_alloc(struct cgroup_subsys_state *parent_css)\r\n{\r\nstruct blkcg *blkcg;\r\nstruct cgroup_subsys_state *ret;\r\nint i;\r\nmutex_lock(&blkcg_pol_mutex);\r\nif (!parent_css) {\r\nblkcg = &blkcg_root;\r\n} else {\r\nblkcg = kzalloc(sizeof(*blkcg), GFP_KERNEL);\r\nif (!blkcg) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto free_blkcg;\r\n}\r\n}\r\nfor (i = 0; i < BLKCG_MAX_POLS ; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nstruct blkcg_policy_data *cpd;\r\nif (!pol || !pol->cpd_alloc_fn)\r\ncontinue;\r\ncpd = pol->cpd_alloc_fn(GFP_KERNEL);\r\nif (!cpd) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto free_pd_blkcg;\r\n}\r\nblkcg->cpd[i] = cpd;\r\ncpd->blkcg = blkcg;\r\ncpd->plid = i;\r\nif (pol->cpd_init_fn)\r\npol->cpd_init_fn(cpd);\r\n}\r\nspin_lock_init(&blkcg->lock);\r\nINIT_RADIX_TREE(&blkcg->blkg_tree, GFP_NOWAIT);\r\nINIT_HLIST_HEAD(&blkcg->blkg_list);\r\n#ifdef CONFIG_CGROUP_WRITEBACK\r\nINIT_LIST_HEAD(&blkcg->cgwb_list);\r\n#endif\r\nlist_add_tail(&blkcg->all_blkcgs_node, &all_blkcgs);\r\nmutex_unlock(&blkcg_pol_mutex);\r\nreturn &blkcg->css;\r\nfree_pd_blkcg:\r\nfor (i--; i >= 0; i--)\r\nif (blkcg->cpd[i])\r\nblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\r\nfree_blkcg:\r\nkfree(blkcg);\r\nmutex_unlock(&blkcg_pol_mutex);\r\nreturn ret;\r\n}\r\nint blkcg_init_queue(struct request_queue *q)\r\n{\r\nstruct blkcg_gq *new_blkg, *blkg;\r\nbool preloaded;\r\nint ret;\r\nnew_blkg = blkg_alloc(&blkcg_root, q, GFP_KERNEL);\r\nif (!new_blkg)\r\nreturn -ENOMEM;\r\npreloaded = !radix_tree_preload(GFP_KERNEL);\r\nrcu_read_lock();\r\nspin_lock_irq(q->queue_lock);\r\nblkg = blkg_create(&blkcg_root, q, new_blkg);\r\nspin_unlock_irq(q->queue_lock);\r\nrcu_read_unlock();\r\nif (preloaded)\r\nradix_tree_preload_end();\r\nif (IS_ERR(blkg)) {\r\nblkg_free(new_blkg);\r\nreturn PTR_ERR(blkg);\r\n}\r\nq->root_blkg = blkg;\r\nq->root_rl.blkg = blkg;\r\nret = blk_throtl_init(q);\r\nif (ret) {\r\nspin_lock_irq(q->queue_lock);\r\nblkg_destroy_all(q);\r\nspin_unlock_irq(q->queue_lock);\r\n}\r\nreturn ret;\r\n}\r\nvoid blkcg_drain_queue(struct request_queue *q)\r\n{\r\nlockdep_assert_held(q->queue_lock);\r\nif (!q->root_blkg)\r\nreturn;\r\nblk_throtl_drain(q);\r\n}\r\nvoid blkcg_exit_queue(struct request_queue *q)\r\n{\r\nspin_lock_irq(q->queue_lock);\r\nblkg_destroy_all(q);\r\nspin_unlock_irq(q->queue_lock);\r\nblk_throtl_exit(q);\r\n}\r\nstatic int blkcg_can_attach(struct cgroup_taskset *tset)\r\n{\r\nstruct task_struct *task;\r\nstruct cgroup_subsys_state *dst_css;\r\nstruct io_context *ioc;\r\nint ret = 0;\r\ncgroup_taskset_for_each(task, dst_css, tset) {\r\ntask_lock(task);\r\nioc = task->io_context;\r\nif (ioc && atomic_read(&ioc->nr_tasks) > 1)\r\nret = -EINVAL;\r\ntask_unlock(task);\r\nif (ret)\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic void blkcg_bind(struct cgroup_subsys_state *root_css)\r\n{\r\nint i;\r\nmutex_lock(&blkcg_pol_mutex);\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nstruct blkcg *blkcg;\r\nif (!pol || !pol->cpd_bind_fn)\r\ncontinue;\r\nlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node)\r\nif (blkcg->cpd[pol->plid])\r\npol->cpd_bind_fn(blkcg->cpd[pol->plid]);\r\n}\r\nmutex_unlock(&blkcg_pol_mutex);\r\n}\r\nint blkcg_activate_policy(struct request_queue *q,\r\nconst struct blkcg_policy *pol)\r\n{\r\nstruct blkg_policy_data *pd_prealloc = NULL;\r\nstruct blkcg_gq *blkg;\r\nint ret;\r\nif (blkcg_policy_enabled(q, pol))\r\nreturn 0;\r\nblk_queue_bypass_start(q);\r\npd_prealloc:\r\nif (!pd_prealloc) {\r\npd_prealloc = pol->pd_alloc_fn(GFP_KERNEL, q->node);\r\nif (!pd_prealloc) {\r\nret = -ENOMEM;\r\ngoto out_bypass_end;\r\n}\r\n}\r\nspin_lock_irq(q->queue_lock);\r\nlist_for_each_entry(blkg, &q->blkg_list, q_node) {\r\nstruct blkg_policy_data *pd;\r\nif (blkg->pd[pol->plid])\r\ncontinue;\r\npd = pol->pd_alloc_fn(GFP_NOWAIT, q->node);\r\nif (!pd)\r\nswap(pd, pd_prealloc);\r\nif (!pd) {\r\nspin_unlock_irq(q->queue_lock);\r\ngoto pd_prealloc;\r\n}\r\nblkg->pd[pol->plid] = pd;\r\npd->blkg = blkg;\r\npd->plid = pol->plid;\r\nif (pol->pd_init_fn)\r\npol->pd_init_fn(pd);\r\n}\r\n__set_bit(pol->plid, q->blkcg_pols);\r\nret = 0;\r\nspin_unlock_irq(q->queue_lock);\r\nout_bypass_end:\r\nblk_queue_bypass_end(q);\r\nif (pd_prealloc)\r\npol->pd_free_fn(pd_prealloc);\r\nreturn ret;\r\n}\r\nvoid blkcg_deactivate_policy(struct request_queue *q,\r\nconst struct blkcg_policy *pol)\r\n{\r\nstruct blkcg_gq *blkg;\r\nif (!blkcg_policy_enabled(q, pol))\r\nreturn;\r\nblk_queue_bypass_start(q);\r\nspin_lock_irq(q->queue_lock);\r\n__clear_bit(pol->plid, q->blkcg_pols);\r\nlist_for_each_entry(blkg, &q->blkg_list, q_node) {\r\nspin_lock(&blkg->blkcg->lock);\r\nif (blkg->pd[pol->plid]) {\r\nif (pol->pd_offline_fn)\r\npol->pd_offline_fn(blkg->pd[pol->plid]);\r\npol->pd_free_fn(blkg->pd[pol->plid]);\r\nblkg->pd[pol->plid] = NULL;\r\n}\r\nspin_unlock(&blkg->blkcg->lock);\r\n}\r\nspin_unlock_irq(q->queue_lock);\r\nblk_queue_bypass_end(q);\r\n}\r\nint blkcg_policy_register(struct blkcg_policy *pol)\r\n{\r\nstruct blkcg *blkcg;\r\nint i, ret;\r\nmutex_lock(&blkcg_pol_register_mutex);\r\nmutex_lock(&blkcg_pol_mutex);\r\nret = -ENOSPC;\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++)\r\nif (!blkcg_policy[i])\r\nbreak;\r\nif (i >= BLKCG_MAX_POLS)\r\ngoto err_unlock;\r\npol->plid = i;\r\nblkcg_policy[pol->plid] = pol;\r\nif (pol->cpd_alloc_fn) {\r\nlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\r\nstruct blkcg_policy_data *cpd;\r\ncpd = pol->cpd_alloc_fn(GFP_KERNEL);\r\nif (!cpd) {\r\nmutex_unlock(&blkcg_pol_mutex);\r\ngoto err_free_cpds;\r\n}\r\nblkcg->cpd[pol->plid] = cpd;\r\ncpd->blkcg = blkcg;\r\ncpd->plid = pol->plid;\r\npol->cpd_init_fn(cpd);\r\n}\r\n}\r\nmutex_unlock(&blkcg_pol_mutex);\r\nif (pol->dfl_cftypes)\r\nWARN_ON(cgroup_add_dfl_cftypes(&io_cgrp_subsys,\r\npol->dfl_cftypes));\r\nif (pol->legacy_cftypes)\r\nWARN_ON(cgroup_add_legacy_cftypes(&io_cgrp_subsys,\r\npol->legacy_cftypes));\r\nmutex_unlock(&blkcg_pol_register_mutex);\r\nreturn 0;\r\nerr_free_cpds:\r\nif (pol->cpd_alloc_fn) {\r\nlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\r\nif (blkcg->cpd[pol->plid]) {\r\npol->cpd_free_fn(blkcg->cpd[pol->plid]);\r\nblkcg->cpd[pol->plid] = NULL;\r\n}\r\n}\r\n}\r\nblkcg_policy[pol->plid] = NULL;\r\nerr_unlock:\r\nmutex_unlock(&blkcg_pol_mutex);\r\nmutex_unlock(&blkcg_pol_register_mutex);\r\nreturn ret;\r\n}\r\nvoid blkcg_policy_unregister(struct blkcg_policy *pol)\r\n{\r\nstruct blkcg *blkcg;\r\nmutex_lock(&blkcg_pol_register_mutex);\r\nif (WARN_ON(blkcg_policy[pol->plid] != pol))\r\ngoto out_unlock;\r\nif (pol->dfl_cftypes)\r\ncgroup_rm_cftypes(pol->dfl_cftypes);\r\nif (pol->legacy_cftypes)\r\ncgroup_rm_cftypes(pol->legacy_cftypes);\r\nmutex_lock(&blkcg_pol_mutex);\r\nif (pol->cpd_alloc_fn) {\r\nlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\r\nif (blkcg->cpd[pol->plid]) {\r\npol->cpd_free_fn(blkcg->cpd[pol->plid]);\r\nblkcg->cpd[pol->plid] = NULL;\r\n}\r\n}\r\n}\r\nblkcg_policy[pol->plid] = NULL;\r\nmutex_unlock(&blkcg_pol_mutex);\r\nout_unlock:\r\nmutex_unlock(&blkcg_pol_register_mutex);\r\n}
