static inline void tk_normalize_xtime(struct timekeeper *tk)\r\n{\r\nwhile (tk->tkr_mono.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_mono.shift)) {\r\ntk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;\r\ntk->xtime_sec++;\r\n}\r\n}\r\nstatic inline struct timespec64 tk_xtime(struct timekeeper *tk)\r\n{\r\nstruct timespec64 ts;\r\nts.tv_sec = tk->xtime_sec;\r\nts.tv_nsec = (long)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);\r\nreturn ts;\r\n}\r\nstatic void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)\r\n{\r\ntk->xtime_sec = ts->tv_sec;\r\ntk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;\r\n}\r\nstatic void tk_xtime_add(struct timekeeper *tk, const struct timespec64 *ts)\r\n{\r\ntk->xtime_sec += ts->tv_sec;\r\ntk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;\r\ntk_normalize_xtime(tk);\r\n}\r\nstatic void tk_set_wall_to_mono(struct timekeeper *tk, struct timespec64 wtm)\r\n{\r\nstruct timespec64 tmp;\r\nset_normalized_timespec64(&tmp, -tk->wall_to_monotonic.tv_sec,\r\n-tk->wall_to_monotonic.tv_nsec);\r\nWARN_ON_ONCE(tk->offs_real.tv64 != timespec64_to_ktime(tmp).tv64);\r\ntk->wall_to_monotonic = wtm;\r\nset_normalized_timespec64(&tmp, -wtm.tv_sec, -wtm.tv_nsec);\r\ntk->offs_real = timespec64_to_ktime(tmp);\r\ntk->offs_tai = ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0));\r\n}\r\nstatic inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)\r\n{\r\ntk->offs_boot = ktime_add(tk->offs_boot, delta);\r\n}\r\nstatic void timekeeping_check_update(struct timekeeper *tk, cycle_t offset)\r\n{\r\ncycle_t max_cycles = tk->tkr_mono.clock->max_cycles;\r\nconst char *name = tk->tkr_mono.clock->name;\r\nif (offset > max_cycles) {\r\nprintk_deferred("WARNING: timekeeping: Cycle offset (%lld) is larger than allowed by the '%s' clock's max_cycles value (%lld): time overflow danger\n",\r\noffset, name, max_cycles);\r\nprintk_deferred(" timekeeping: Your kernel is sick, but tries to cope by capping time updates\n");\r\n} else {\r\nif (offset > (max_cycles >> 1)) {\r\nprintk_deferred("INFO: timekeeping: Cycle offset (%lld) is larger than the '%s' clock's 50%% safety margin (%lld)\n",\r\noffset, name, max_cycles >> 1);\r\nprintk_deferred(" timekeeping: Your kernel is still fine, but is feeling a bit nervous\n");\r\n}\r\n}\r\nif (tk->underflow_seen) {\r\nif (jiffies - tk->last_warning > WARNING_FREQ) {\r\nprintk_deferred("WARNING: Underflow in clocksource '%s' observed, time update ignored.\n", name);\r\nprintk_deferred(" Please report this, consider using a different clocksource, if possible.\n");\r\nprintk_deferred(" Your kernel is probably still fine.\n");\r\ntk->last_warning = jiffies;\r\n}\r\ntk->underflow_seen = 0;\r\n}\r\nif (tk->overflow_seen) {\r\nif (jiffies - tk->last_warning > WARNING_FREQ) {\r\nprintk_deferred("WARNING: Overflow in clocksource '%s' observed, time update capped.\n", name);\r\nprintk_deferred(" Please report this, consider using a different clocksource, if possible.\n");\r\nprintk_deferred(" Your kernel is probably still fine.\n");\r\ntk->last_warning = jiffies;\r\n}\r\ntk->overflow_seen = 0;\r\n}\r\n}\r\nstatic inline cycle_t timekeeping_get_delta(struct tk_read_base *tkr)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\ncycle_t now, last, mask, max, delta;\r\nunsigned int seq;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnow = tkr->read(tkr->clock);\r\nlast = tkr->cycle_last;\r\nmask = tkr->mask;\r\nmax = tkr->clock->max_cycles;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\ndelta = clocksource_delta(now, last, mask);\r\nif (unlikely((~delta & mask) < (mask >> 3))) {\r\ntk->underflow_seen = 1;\r\ndelta = 0;\r\n}\r\nif (unlikely(delta > max)) {\r\ntk->overflow_seen = 1;\r\ndelta = tkr->clock->max_cycles;\r\n}\r\nreturn delta;\r\n}\r\nstatic inline void timekeeping_check_update(struct timekeeper *tk, cycle_t offset)\r\n{\r\n}\r\nstatic inline cycle_t timekeeping_get_delta(struct tk_read_base *tkr)\r\n{\r\ncycle_t cycle_now, delta;\r\ncycle_now = tkr->read(tkr->clock);\r\ndelta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);\r\nreturn delta;\r\n}\r\nstatic void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)\r\n{\r\ncycle_t interval;\r\nu64 tmp, ntpinterval;\r\nstruct clocksource *old_clock;\r\n++tk->cs_was_changed_seq;\r\nold_clock = tk->tkr_mono.clock;\r\ntk->tkr_mono.clock = clock;\r\ntk->tkr_mono.read = clock->read;\r\ntk->tkr_mono.mask = clock->mask;\r\ntk->tkr_mono.cycle_last = tk->tkr_mono.read(clock);\r\ntk->tkr_raw.clock = clock;\r\ntk->tkr_raw.read = clock->read;\r\ntk->tkr_raw.mask = clock->mask;\r\ntk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;\r\ntmp = NTP_INTERVAL_LENGTH;\r\ntmp <<= clock->shift;\r\nntpinterval = tmp;\r\ntmp += clock->mult/2;\r\ndo_div(tmp, clock->mult);\r\nif (tmp == 0)\r\ntmp = 1;\r\ninterval = (cycle_t) tmp;\r\ntk->cycle_interval = interval;\r\ntk->xtime_interval = (u64) interval * clock->mult;\r\ntk->xtime_remainder = ntpinterval - tk->xtime_interval;\r\ntk->raw_interval =\r\n((u64) interval * clock->mult) >> clock->shift;\r\nif (old_clock) {\r\nint shift_change = clock->shift - old_clock->shift;\r\nif (shift_change < 0)\r\ntk->tkr_mono.xtime_nsec >>= -shift_change;\r\nelse\r\ntk->tkr_mono.xtime_nsec <<= shift_change;\r\n}\r\ntk->tkr_raw.xtime_nsec = 0;\r\ntk->tkr_mono.shift = clock->shift;\r\ntk->tkr_raw.shift = clock->shift;\r\ntk->ntp_error = 0;\r\ntk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;\r\ntk->ntp_tick = ntpinterval << tk->ntp_error_shift;\r\ntk->tkr_mono.mult = clock->mult;\r\ntk->tkr_raw.mult = clock->mult;\r\ntk->ntp_err_mult = 0;\r\n}\r\nstatic u32 default_arch_gettimeoffset(void) { return 0; }\r\nstatic inline u32 arch_gettimeoffset(void) { return 0; }\r\nstatic inline s64 timekeeping_delta_to_ns(struct tk_read_base *tkr,\r\ncycle_t delta)\r\n{\r\ns64 nsec;\r\nnsec = delta * tkr->mult + tkr->xtime_nsec;\r\nnsec >>= tkr->shift;\r\nreturn nsec + arch_gettimeoffset();\r\n}\r\nstatic inline s64 timekeeping_get_ns(struct tk_read_base *tkr)\r\n{\r\ncycle_t delta;\r\ndelta = timekeeping_get_delta(tkr);\r\nreturn timekeeping_delta_to_ns(tkr, delta);\r\n}\r\nstatic inline s64 timekeeping_cycles_to_ns(struct tk_read_base *tkr,\r\ncycle_t cycles)\r\n{\r\ncycle_t delta;\r\ndelta = clocksource_delta(cycles, tkr->cycle_last, tkr->mask);\r\nreturn timekeeping_delta_to_ns(tkr, delta);\r\n}\r\nstatic void update_fast_timekeeper(struct tk_read_base *tkr, struct tk_fast *tkf)\r\n{\r\nstruct tk_read_base *base = tkf->base;\r\nraw_write_seqcount_latch(&tkf->seq);\r\nmemcpy(base, tkr, sizeof(*base));\r\nraw_write_seqcount_latch(&tkf->seq);\r\nmemcpy(base + 1, base, sizeof(*base));\r\n}\r\nstatic __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)\r\n{\r\nstruct tk_read_base *tkr;\r\nunsigned int seq;\r\nu64 now;\r\ndo {\r\nseq = raw_read_seqcount_latch(&tkf->seq);\r\ntkr = tkf->base + (seq & 0x01);\r\nnow = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);\r\n} while (read_seqcount_retry(&tkf->seq, seq));\r\nreturn now;\r\n}\r\nu64 ktime_get_mono_fast_ns(void)\r\n{\r\nreturn __ktime_get_fast_ns(&tk_fast_mono);\r\n}\r\nu64 ktime_get_raw_fast_ns(void)\r\n{\r\nreturn __ktime_get_fast_ns(&tk_fast_raw);\r\n}\r\nstatic cycle_t dummy_clock_read(struct clocksource *cs)\r\n{\r\nreturn cycles_at_suspend;\r\n}\r\nstatic void halt_fast_timekeeper(struct timekeeper *tk)\r\n{\r\nstatic struct tk_read_base tkr_dummy;\r\nstruct tk_read_base *tkr = &tk->tkr_mono;\r\nmemcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));\r\ncycles_at_suspend = tkr->read(tkr->clock);\r\ntkr_dummy.read = dummy_clock_read;\r\nupdate_fast_timekeeper(&tkr_dummy, &tk_fast_mono);\r\ntkr = &tk->tkr_raw;\r\nmemcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));\r\ntkr_dummy.read = dummy_clock_read;\r\nupdate_fast_timekeeper(&tkr_dummy, &tk_fast_raw);\r\n}\r\nstatic inline void update_vsyscall(struct timekeeper *tk)\r\n{\r\nstruct timespec xt, wm;\r\nxt = timespec64_to_timespec(tk_xtime(tk));\r\nwm = timespec64_to_timespec(tk->wall_to_monotonic);\r\nupdate_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,\r\ntk->tkr_mono.cycle_last);\r\n}\r\nstatic inline void old_vsyscall_fixup(struct timekeeper *tk)\r\n{\r\ns64 remainder;\r\nremainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);\r\ntk->tkr_mono.xtime_nsec -= remainder;\r\ntk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;\r\ntk->ntp_error += remainder << tk->ntp_error_shift;\r\ntk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;\r\n}\r\nstatic void update_pvclock_gtod(struct timekeeper *tk, bool was_set)\r\n{\r\nraw_notifier_call_chain(&pvclock_gtod_chain, was_set, tk);\r\n}\r\nint pvclock_gtod_register_notifier(struct notifier_block *nb)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nint ret;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nret = raw_notifier_chain_register(&pvclock_gtod_chain, nb);\r\nupdate_pvclock_gtod(tk, true);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nreturn ret;\r\n}\r\nint pvclock_gtod_unregister_notifier(struct notifier_block *nb)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nret = raw_notifier_chain_unregister(&pvclock_gtod_chain, nb);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nreturn ret;\r\n}\r\nstatic inline void tk_update_leap_state(struct timekeeper *tk)\r\n{\r\ntk->next_leap_ktime = ntp_get_next_leap();\r\nif (tk->next_leap_ktime.tv64 != KTIME_MAX)\r\ntk->next_leap_ktime = ktime_sub(tk->next_leap_ktime, tk->offs_real);\r\n}\r\nstatic inline void tk_update_ktime_data(struct timekeeper *tk)\r\n{\r\nu64 seconds;\r\nu32 nsec;\r\nseconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);\r\nnsec = (u32) tk->wall_to_monotonic.tv_nsec;\r\ntk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);\r\ntk->tkr_raw.base = timespec64_to_ktime(tk->raw_time);\r\nnsec += (u32)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);\r\nif (nsec >= NSEC_PER_SEC)\r\nseconds++;\r\ntk->ktime_sec = seconds;\r\n}\r\nstatic void timekeeping_update(struct timekeeper *tk, unsigned int action)\r\n{\r\nif (action & TK_CLEAR_NTP) {\r\ntk->ntp_error = 0;\r\nntp_clear();\r\n}\r\ntk_update_leap_state(tk);\r\ntk_update_ktime_data(tk);\r\nupdate_vsyscall(tk);\r\nupdate_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);\r\nupdate_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);\r\nupdate_fast_timekeeper(&tk->tkr_raw, &tk_fast_raw);\r\nif (action & TK_CLOCK_WAS_SET)\r\ntk->clock_was_set_seq++;\r\nif (action & TK_MIRROR)\r\nmemcpy(&shadow_timekeeper, &tk_core.timekeeper,\r\nsizeof(tk_core.timekeeper));\r\n}\r\nstatic void timekeeping_forward_now(struct timekeeper *tk)\r\n{\r\nstruct clocksource *clock = tk->tkr_mono.clock;\r\ncycle_t cycle_now, delta;\r\ns64 nsec;\r\ncycle_now = tk->tkr_mono.read(clock);\r\ndelta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask);\r\ntk->tkr_mono.cycle_last = cycle_now;\r\ntk->tkr_raw.cycle_last = cycle_now;\r\ntk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;\r\ntk->tkr_mono.xtime_nsec += (u64)arch_gettimeoffset() << tk->tkr_mono.shift;\r\ntk_normalize_xtime(tk);\r\nnsec = clocksource_cyc2ns(delta, tk->tkr_raw.mult, tk->tkr_raw.shift);\r\ntimespec64_add_ns(&tk->raw_time, nsec);\r\n}\r\nint __getnstimeofday64(struct timespec64 *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\ns64 nsecs = 0;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nts->tv_sec = tk->xtime_sec;\r\nnsecs = timekeeping_get_ns(&tk->tkr_mono);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nts->tv_nsec = 0;\r\ntimespec64_add_ns(ts, nsecs);\r\nif (unlikely(timekeeping_suspended))\r\nreturn -EAGAIN;\r\nreturn 0;\r\n}\r\nvoid getnstimeofday64(struct timespec64 *ts)\r\n{\r\nWARN_ON(__getnstimeofday64(ts));\r\n}\r\nktime_t ktime_get(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base;\r\ns64 nsecs;\r\nWARN_ON(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = tk->tkr_mono.base;\r\nnsecs = timekeeping_get_ns(&tk->tkr_mono);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nu32 ktime_get_resolution_ns(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nu32 nsecs;\r\nWARN_ON(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnsecs = tk->tkr_mono.mult >> tk->tkr_mono.shift;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn nsecs;\r\n}\r\nktime_t ktime_get_with_offset(enum tk_offsets offs)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base, *offset = offsets[offs];\r\ns64 nsecs;\r\nWARN_ON(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = ktime_add(tk->tkr_mono.base, *offset);\r\nnsecs = timekeeping_get_ns(&tk->tkr_mono);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)\r\n{\r\nktime_t *offset = offsets[offs];\r\nunsigned long seq;\r\nktime_t tconv;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\ntconv = ktime_add(tmono, *offset);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn tconv;\r\n}\r\nktime_t ktime_get_raw(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base;\r\ns64 nsecs;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = tk->tkr_raw.base;\r\nnsecs = timekeeping_get_ns(&tk->tkr_raw);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ktime_add_ns(base, nsecs);\r\n}\r\nvoid ktime_get_ts64(struct timespec64 *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 tomono;\r\ns64 nsec;\r\nunsigned int seq;\r\nWARN_ON(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nts->tv_sec = tk->xtime_sec;\r\nnsec = timekeeping_get_ns(&tk->tkr_mono);\r\ntomono = tk->wall_to_monotonic;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nts->tv_sec += tomono.tv_sec;\r\nts->tv_nsec = 0;\r\ntimespec64_add_ns(ts, nsec + tomono.tv_nsec);\r\n}\r\ntime64_t ktime_get_seconds(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nWARN_ON(timekeeping_suspended);\r\nreturn tk->ktime_sec;\r\n}\r\ntime64_t ktime_get_real_seconds(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\ntime64_t seconds;\r\nunsigned int seq;\r\nif (IS_ENABLED(CONFIG_64BIT))\r\nreturn tk->xtime_sec;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nseconds = tk->xtime_sec;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn seconds;\r\n}\r\ntime64_t __ktime_get_real_seconds(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nreturn tk->xtime_sec;\r\n}\r\nvoid ktime_get_snapshot(struct system_time_snapshot *systime_snapshot)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\nktime_t base_raw;\r\nktime_t base_real;\r\ns64 nsec_raw;\r\ns64 nsec_real;\r\ncycle_t now;\r\nWARN_ON_ONCE(timekeeping_suspended);\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnow = tk->tkr_mono.read(tk->tkr_mono.clock);\r\nsystime_snapshot->cs_was_changed_seq = tk->cs_was_changed_seq;\r\nsystime_snapshot->clock_was_set_seq = tk->clock_was_set_seq;\r\nbase_real = ktime_add(tk->tkr_mono.base,\r\ntk_core.timekeeper.offs_real);\r\nbase_raw = tk->tkr_raw.base;\r\nnsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono, now);\r\nnsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw, now);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nsystime_snapshot->cycles = now;\r\nsystime_snapshot->real = ktime_add_ns(base_real, nsec_real);\r\nsystime_snapshot->raw = ktime_add_ns(base_raw, nsec_raw);\r\n}\r\nstatic int scale64_check_overflow(u64 mult, u64 div, u64 *base)\r\n{\r\nu64 tmp, rem;\r\ntmp = div64_u64_rem(*base, div, &rem);\r\nif (((int)sizeof(u64)*8 - fls64(mult) < fls64(tmp)) ||\r\n((int)sizeof(u64)*8 - fls64(mult) < fls64(rem)))\r\nreturn -EOVERFLOW;\r\ntmp *= mult;\r\nrem *= mult;\r\ndo_div(rem, div);\r\n*base = tmp + rem;\r\nreturn 0;\r\n}\r\nstatic int adjust_historical_crosststamp(struct system_time_snapshot *history,\r\ncycle_t partial_history_cycles,\r\ncycle_t total_history_cycles,\r\nbool discontinuity,\r\nstruct system_device_crosststamp *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nu64 corr_raw, corr_real;\r\nbool interp_forward;\r\nint ret;\r\nif (total_history_cycles == 0 || partial_history_cycles == 0)\r\nreturn 0;\r\ninterp_forward = partial_history_cycles > total_history_cycles/2 ?\r\ntrue : false;\r\npartial_history_cycles = interp_forward ?\r\ntotal_history_cycles - partial_history_cycles :\r\npartial_history_cycles;\r\ncorr_raw = (u64)ktime_to_ns(\r\nktime_sub(ts->sys_monoraw, history->raw));\r\nret = scale64_check_overflow(partial_history_cycles,\r\ntotal_history_cycles, &corr_raw);\r\nif (ret)\r\nreturn ret;\r\nif (discontinuity) {\r\ncorr_real = mul_u64_u32_div\r\n(corr_raw, tk->tkr_mono.mult, tk->tkr_raw.mult);\r\n} else {\r\ncorr_real = (u64)ktime_to_ns(\r\nktime_sub(ts->sys_realtime, history->real));\r\nret = scale64_check_overflow(partial_history_cycles,\r\ntotal_history_cycles, &corr_real);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (interp_forward) {\r\nts->sys_monoraw = ktime_add_ns(history->raw, corr_raw);\r\nts->sys_realtime = ktime_add_ns(history->real, corr_real);\r\n} else {\r\nts->sys_monoraw = ktime_sub_ns(ts->sys_monoraw, corr_raw);\r\nts->sys_realtime = ktime_sub_ns(ts->sys_realtime, corr_real);\r\n}\r\nreturn 0;\r\n}\r\nstatic bool cycle_between(cycle_t before, cycle_t test, cycle_t after)\r\n{\r\nif (test > before && test < after)\r\nreturn true;\r\nif (test < before && before > after)\r\nreturn true;\r\nreturn false;\r\n}\r\nint get_device_system_crosststamp(int (*get_time_fn)\r\n(ktime_t *device_time,\r\nstruct system_counterval_t *sys_counterval,\r\nvoid *ctx),\r\nvoid *ctx,\r\nstruct system_time_snapshot *history_begin,\r\nstruct system_device_crosststamp *xtstamp)\r\n{\r\nstruct system_counterval_t system_counterval;\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\ncycle_t cycles, now, interval_start;\r\nunsigned int clock_was_set_seq = 0;\r\nktime_t base_real, base_raw;\r\ns64 nsec_real, nsec_raw;\r\nu8 cs_was_changed_seq;\r\nunsigned long seq;\r\nbool do_interp;\r\nint ret;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nret = get_time_fn(&xtstamp->device, &system_counterval, ctx);\r\nif (ret)\r\nreturn ret;\r\nif (tk->tkr_mono.clock != system_counterval.cs)\r\nreturn -ENODEV;\r\ncycles = system_counterval.cycles;\r\nnow = tk->tkr_mono.read(tk->tkr_mono.clock);\r\ninterval_start = tk->tkr_mono.cycle_last;\r\nif (!cycle_between(interval_start, cycles, now)) {\r\nclock_was_set_seq = tk->clock_was_set_seq;\r\ncs_was_changed_seq = tk->cs_was_changed_seq;\r\ncycles = interval_start;\r\ndo_interp = true;\r\n} else {\r\ndo_interp = false;\r\n}\r\nbase_real = ktime_add(tk->tkr_mono.base,\r\ntk_core.timekeeper.offs_real);\r\nbase_raw = tk->tkr_raw.base;\r\nnsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono,\r\nsystem_counterval.cycles);\r\nnsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw,\r\nsystem_counterval.cycles);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nxtstamp->sys_realtime = ktime_add_ns(base_real, nsec_real);\r\nxtstamp->sys_monoraw = ktime_add_ns(base_raw, nsec_raw);\r\nif (do_interp) {\r\ncycle_t partial_history_cycles, total_history_cycles;\r\nbool discontinuity;\r\nif (!history_begin ||\r\n!cycle_between(history_begin->cycles,\r\nsystem_counterval.cycles, cycles) ||\r\nhistory_begin->cs_was_changed_seq != cs_was_changed_seq)\r\nreturn -EINVAL;\r\npartial_history_cycles = cycles - system_counterval.cycles;\r\ntotal_history_cycles = cycles - history_begin->cycles;\r\ndiscontinuity =\r\nhistory_begin->clock_was_set_seq != clock_was_set_seq;\r\nret = adjust_historical_crosststamp(history_begin,\r\npartial_history_cycles,\r\ntotal_history_cycles,\r\ndiscontinuity, xtstamp);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nvoid do_gettimeofday(struct timeval *tv)\r\n{\r\nstruct timespec64 now;\r\ngetnstimeofday64(&now);\r\ntv->tv_sec = now.tv_sec;\r\ntv->tv_usec = now.tv_nsec/1000;\r\n}\r\nint do_settimeofday64(const struct timespec64 *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 ts_delta, xt;\r\nunsigned long flags;\r\nint ret = 0;\r\nif (!timespec64_valid_strict(ts))\r\nreturn -EINVAL;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\nxt = tk_xtime(tk);\r\nts_delta.tv_sec = ts->tv_sec - xt.tv_sec;\r\nts_delta.tv_nsec = ts->tv_nsec - xt.tv_nsec;\r\nif (timespec64_compare(&tk->wall_to_monotonic, &ts_delta) > 0) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\ntk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts_delta));\r\ntk_set_xtime(tk, ts);\r\nout:\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\nreturn ret;\r\n}\r\nint timekeeping_inject_offset(struct timespec *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nstruct timespec64 ts64, tmp;\r\nint ret = 0;\r\nif (!timespec_inject_offset_valid(ts))\r\nreturn -EINVAL;\r\nts64 = timespec_to_timespec64(*ts);\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\ntmp = timespec64_add(tk_xtime(tk), ts64);\r\nif (timespec64_compare(&tk->wall_to_monotonic, &ts64) > 0 ||\r\n!timespec64_valid_strict(&tmp)) {\r\nret = -EINVAL;\r\ngoto error;\r\n}\r\ntk_xtime_add(tk, &ts64);\r\ntk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts64));\r\nerror:\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\nreturn ret;\r\n}\r\ns32 timekeeping_get_tai_offset(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\ns32 ret;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nret = tk->tai_offset;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ret;\r\n}\r\nstatic void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)\r\n{\r\ntk->tai_offset = tai_offset;\r\ntk->offs_tai = ktime_add(tk->offs_real, ktime_set(tai_offset, 0));\r\n}\r\nvoid timekeeping_set_tai_offset(s32 tai_offset)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\n__timekeeping_set_tai_offset(tk, tai_offset);\r\ntimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\n}\r\nstatic int change_clocksource(void *data)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct clocksource *new, *old;\r\nunsigned long flags;\r\nnew = (struct clocksource *) data;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\nif (try_module_get(new->owner)) {\r\nif (!new->enable || new->enable(new) == 0) {\r\nold = tk->tkr_mono.clock;\r\ntk_setup_internals(tk, new);\r\nif (old->disable)\r\nold->disable(old);\r\nmodule_put(old->owner);\r\n} else {\r\nmodule_put(new->owner);\r\n}\r\n}\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nreturn 0;\r\n}\r\nint timekeeping_notify(struct clocksource *clock)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nif (tk->tkr_mono.clock == clock)\r\nreturn 0;\r\nstop_machine(change_clocksource, clock, NULL);\r\ntick_clock_notify();\r\nreturn tk->tkr_mono.clock == clock ? 0 : -1;\r\n}\r\nvoid getrawmonotonic64(struct timespec64 *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 ts64;\r\nunsigned long seq;\r\ns64 nsecs;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnsecs = timekeeping_get_ns(&tk->tkr_raw);\r\nts64 = tk->raw_time;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\ntimespec64_add_ns(&ts64, nsecs);\r\n*ts = ts64;\r\n}\r\nint timekeeping_valid_for_hres(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\nint ret;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nret = tk->tkr_mono.clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ret;\r\n}\r\nu64 timekeeping_max_deferment(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long seq;\r\nu64 ret;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nret = tk->tkr_mono.clock->max_idle_ns;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn ret;\r\n}\r\nvoid __weak read_persistent_clock(struct timespec *ts)\r\n{\r\nts->tv_sec = 0;\r\nts->tv_nsec = 0;\r\n}\r\nvoid __weak read_persistent_clock64(struct timespec64 *ts64)\r\n{\r\nstruct timespec ts;\r\nread_persistent_clock(&ts);\r\n*ts64 = timespec_to_timespec64(ts);\r\n}\r\nvoid __weak read_boot_clock64(struct timespec64 *ts)\r\n{\r\nts->tv_sec = 0;\r\nts->tv_nsec = 0;\r\n}\r\nvoid __init timekeeping_init(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct clocksource *clock;\r\nunsigned long flags;\r\nstruct timespec64 now, boot, tmp;\r\nread_persistent_clock64(&now);\r\nif (!timespec64_valid_strict(&now)) {\r\npr_warn("WARNING: Persistent clock returned invalid value!\n"\r\n" Check your CMOS/BIOS settings.\n");\r\nnow.tv_sec = 0;\r\nnow.tv_nsec = 0;\r\n} else if (now.tv_sec || now.tv_nsec)\r\npersistent_clock_exists = true;\r\nread_boot_clock64(&boot);\r\nif (!timespec64_valid_strict(&boot)) {\r\npr_warn("WARNING: Boot clock returned invalid value!\n"\r\n" Check your CMOS/BIOS settings.\n");\r\nboot.tv_sec = 0;\r\nboot.tv_nsec = 0;\r\n}\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\nntp_init();\r\nclock = clocksource_default_clock();\r\nif (clock->enable)\r\nclock->enable(clock);\r\ntk_setup_internals(tk, clock);\r\ntk_set_xtime(tk, &now);\r\ntk->raw_time.tv_sec = 0;\r\ntk->raw_time.tv_nsec = 0;\r\nif (boot.tv_sec == 0 && boot.tv_nsec == 0)\r\nboot = tk_xtime(tk);\r\nset_normalized_timespec64(&tmp, -boot.tv_sec, -boot.tv_nsec);\r\ntk_set_wall_to_mono(tk, tmp);\r\ntimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\n}\r\nstatic void __timekeeping_inject_sleeptime(struct timekeeper *tk,\r\nstruct timespec64 *delta)\r\n{\r\nif (!timespec64_valid_strict(delta)) {\r\nprintk_deferred(KERN_WARNING\r\n"__timekeeping_inject_sleeptime: Invalid "\r\n"sleep delta value!\n");\r\nreturn;\r\n}\r\ntk_xtime_add(tk, delta);\r\ntk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *delta));\r\ntk_update_sleep_time(tk, timespec64_to_ktime(*delta));\r\ntk_debug_account_sleep_time(delta);\r\n}\r\nbool timekeeping_rtc_skipresume(void)\r\n{\r\nreturn sleeptime_injected;\r\n}\r\nbool timekeeping_rtc_skipsuspend(void)\r\n{\r\nreturn persistent_clock_exists;\r\n}\r\nvoid timekeeping_inject_sleeptime64(struct timespec64 *delta)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\n__timekeeping_inject_sleeptime(tk, delta);\r\ntimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nclock_was_set();\r\n}\r\nvoid timekeeping_resume(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct clocksource *clock = tk->tkr_mono.clock;\r\nunsigned long flags;\r\nstruct timespec64 ts_new, ts_delta;\r\ncycle_t cycle_now, cycle_delta;\r\nsleeptime_injected = false;\r\nread_persistent_clock64(&ts_new);\r\nclockevents_resume();\r\nclocksource_resume();\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ncycle_now = tk->tkr_mono.read(clock);\r\nif ((clock->flags & CLOCK_SOURCE_SUSPEND_NONSTOP) &&\r\ncycle_now > tk->tkr_mono.cycle_last) {\r\nu64 num, max = ULLONG_MAX;\r\nu32 mult = clock->mult;\r\nu32 shift = clock->shift;\r\ns64 nsec = 0;\r\ncycle_delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last,\r\ntk->tkr_mono.mask);\r\ndo_div(max, mult);\r\nif (cycle_delta > max) {\r\nnum = div64_u64(cycle_delta, max);\r\nnsec = (((u64) max * mult) >> shift) * num;\r\ncycle_delta -= num * max;\r\n}\r\nnsec += ((u64) cycle_delta * mult) >> shift;\r\nts_delta = ns_to_timespec64(nsec);\r\nsleeptime_injected = true;\r\n} else if (timespec64_compare(&ts_new, &timekeeping_suspend_time) > 0) {\r\nts_delta = timespec64_sub(ts_new, timekeeping_suspend_time);\r\nsleeptime_injected = true;\r\n}\r\nif (sleeptime_injected)\r\n__timekeeping_inject_sleeptime(tk, &ts_delta);\r\ntk->tkr_mono.cycle_last = cycle_now;\r\ntk->tkr_raw.cycle_last = cycle_now;\r\ntk->ntp_error = 0;\r\ntimekeeping_suspended = 0;\r\ntimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\ntouch_softlockup_watchdog();\r\ntick_resume();\r\nhrtimers_resume();\r\n}\r\nint timekeeping_suspend(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nstruct timespec64 delta, delta_delta;\r\nstatic struct timespec64 old_delta;\r\nread_persistent_clock64(&timekeeping_suspend_time);\r\nif (timekeeping_suspend_time.tv_sec || timekeeping_suspend_time.tv_nsec)\r\npersistent_clock_exists = true;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_forward_now(tk);\r\ntimekeeping_suspended = 1;\r\nif (persistent_clock_exists) {\r\ndelta = timespec64_sub(tk_xtime(tk), timekeeping_suspend_time);\r\ndelta_delta = timespec64_sub(delta, old_delta);\r\nif (abs(delta_delta.tv_sec) >= 2) {\r\nold_delta = delta;\r\n} else {\r\ntimekeeping_suspend_time =\r\ntimespec64_add(timekeeping_suspend_time, delta_delta);\r\n}\r\n}\r\ntimekeeping_update(tk, TK_MIRROR);\r\nhalt_fast_timekeeper(tk);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\ntick_suspend();\r\nclocksource_suspend();\r\nclockevents_suspend();\r\nreturn 0;\r\n}\r\nstatic int __init timekeeping_init_ops(void)\r\n{\r\nregister_syscore_ops(&timekeeping_syscore_ops);\r\nreturn 0;\r\n}\r\nstatic __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,\r\ns64 offset,\r\nbool negative,\r\nint adj_scale)\r\n{\r\ns64 interval = tk->cycle_interval;\r\ns32 mult_adj = 1;\r\nif (negative) {\r\nmult_adj = -mult_adj;\r\ninterval = -interval;\r\noffset = -offset;\r\n}\r\nmult_adj <<= adj_scale;\r\ninterval <<= adj_scale;\r\noffset <<= adj_scale;\r\nif ((mult_adj > 0) && (tk->tkr_mono.mult + mult_adj < mult_adj)) {\r\nWARN_ON_ONCE(1);\r\nreturn;\r\n}\r\ntk->tkr_mono.mult += mult_adj;\r\ntk->xtime_interval += interval;\r\ntk->tkr_mono.xtime_nsec -= offset;\r\ntk->ntp_error -= (interval - offset) << tk->ntp_error_shift;\r\n}\r\nstatic __always_inline void timekeeping_freqadjust(struct timekeeper *tk,\r\ns64 offset)\r\n{\r\ns64 interval = tk->cycle_interval;\r\ns64 xinterval = tk->xtime_interval;\r\nu32 base = tk->tkr_mono.clock->mult;\r\nu32 max = tk->tkr_mono.clock->maxadj;\r\nu32 cur_adj = tk->tkr_mono.mult;\r\ns64 tick_error;\r\nbool negative;\r\nu32 adj_scale;\r\nif (tk->ntp_err_mult)\r\nxinterval -= tk->cycle_interval;\r\ntk->ntp_tick = ntp_tick_length();\r\ntick_error = ntp_tick_length() >> tk->ntp_error_shift;\r\ntick_error -= (xinterval + tk->xtime_remainder);\r\nif (likely((tick_error >= 0) && (tick_error <= interval)))\r\nreturn;\r\nnegative = (tick_error < 0);\r\nif (negative && (cur_adj - 1) <= (base - max))\r\nreturn;\r\nif (!negative && (cur_adj + 1) >= (base + max))\r\nreturn;\r\nadj_scale = 0;\r\ntick_error = abs(tick_error);\r\nwhile (tick_error > interval) {\r\nu32 adj = 1 << (adj_scale + 1);\r\nif (negative && (cur_adj - adj) <= (base - max))\r\nbreak;\r\nif (!negative && (cur_adj + adj) >= (base + max))\r\nbreak;\r\nadj_scale++;\r\ntick_error >>= 1;\r\n}\r\ntimekeeping_apply_adjustment(tk, offset, negative, adj_scale);\r\n}\r\nstatic void timekeeping_adjust(struct timekeeper *tk, s64 offset)\r\n{\r\ntimekeeping_freqadjust(tk, offset);\r\nif (!tk->ntp_err_mult && (tk->ntp_error > 0)) {\r\ntk->ntp_err_mult = 1;\r\ntimekeeping_apply_adjustment(tk, offset, 0, 0);\r\n} else if (tk->ntp_err_mult && (tk->ntp_error <= 0)) {\r\ntimekeeping_apply_adjustment(tk, offset, 1, 0);\r\ntk->ntp_err_mult = 0;\r\n}\r\nif (unlikely(tk->tkr_mono.clock->maxadj &&\r\n(abs(tk->tkr_mono.mult - tk->tkr_mono.clock->mult)\r\n> tk->tkr_mono.clock->maxadj))) {\r\nprintk_once(KERN_WARNING\r\n"Adjusting %s more than 11%% (%ld vs %ld)\n",\r\ntk->tkr_mono.clock->name, (long)tk->tkr_mono.mult,\r\n(long)tk->tkr_mono.clock->mult + tk->tkr_mono.clock->maxadj);\r\n}\r\nif (unlikely((s64)tk->tkr_mono.xtime_nsec < 0)) {\r\ns64 neg = -(s64)tk->tkr_mono.xtime_nsec;\r\ntk->tkr_mono.xtime_nsec = 0;\r\ntk->ntp_error += neg << tk->ntp_error_shift;\r\n}\r\n}\r\nstatic inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)\r\n{\r\nu64 nsecps = (u64)NSEC_PER_SEC << tk->tkr_mono.shift;\r\nunsigned int clock_set = 0;\r\nwhile (tk->tkr_mono.xtime_nsec >= nsecps) {\r\nint leap;\r\ntk->tkr_mono.xtime_nsec -= nsecps;\r\ntk->xtime_sec++;\r\nleap = second_overflow(tk->xtime_sec);\r\nif (unlikely(leap)) {\r\nstruct timespec64 ts;\r\ntk->xtime_sec += leap;\r\nts.tv_sec = leap;\r\nts.tv_nsec = 0;\r\ntk_set_wall_to_mono(tk,\r\ntimespec64_sub(tk->wall_to_monotonic, ts));\r\n__timekeeping_set_tai_offset(tk, tk->tai_offset - leap);\r\nclock_set = TK_CLOCK_WAS_SET;\r\n}\r\n}\r\nreturn clock_set;\r\n}\r\nstatic cycle_t logarithmic_accumulation(struct timekeeper *tk, cycle_t offset,\r\nu32 shift,\r\nunsigned int *clock_set)\r\n{\r\ncycle_t interval = tk->cycle_interval << shift;\r\nu64 raw_nsecs;\r\nif (offset < interval)\r\nreturn offset;\r\noffset -= interval;\r\ntk->tkr_mono.cycle_last += interval;\r\ntk->tkr_raw.cycle_last += interval;\r\ntk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;\r\n*clock_set |= accumulate_nsecs_to_secs(tk);\r\nraw_nsecs = (u64)tk->raw_interval << shift;\r\nraw_nsecs += tk->raw_time.tv_nsec;\r\nif (raw_nsecs >= NSEC_PER_SEC) {\r\nu64 raw_secs = raw_nsecs;\r\nraw_nsecs = do_div(raw_secs, NSEC_PER_SEC);\r\ntk->raw_time.tv_sec += raw_secs;\r\n}\r\ntk->raw_time.tv_nsec = raw_nsecs;\r\ntk->ntp_error += tk->ntp_tick << shift;\r\ntk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<\r\n(tk->ntp_error_shift + shift);\r\nreturn offset;\r\n}\r\nvoid update_wall_time(void)\r\n{\r\nstruct timekeeper *real_tk = &tk_core.timekeeper;\r\nstruct timekeeper *tk = &shadow_timekeeper;\r\ncycle_t offset;\r\nint shift = 0, maxshift;\r\nunsigned int clock_set = 0;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nif (unlikely(timekeeping_suspended))\r\ngoto out;\r\n#ifdef CONFIG_ARCH_USES_GETTIMEOFFSET\r\noffset = real_tk->cycle_interval;\r\n#else\r\noffset = clocksource_delta(tk->tkr_mono.read(tk->tkr_mono.clock),\r\ntk->tkr_mono.cycle_last, tk->tkr_mono.mask);\r\n#endif\r\nif (offset < real_tk->cycle_interval)\r\ngoto out;\r\ntimekeeping_check_update(real_tk, offset);\r\nshift = ilog2(offset) - ilog2(tk->cycle_interval);\r\nshift = max(0, shift);\r\nmaxshift = (64 - (ilog2(ntp_tick_length())+1)) - 1;\r\nshift = min(shift, maxshift);\r\nwhile (offset >= tk->cycle_interval) {\r\noffset = logarithmic_accumulation(tk, offset, shift,\r\n&clock_set);\r\nif (offset < tk->cycle_interval<<shift)\r\nshift--;\r\n}\r\ntimekeeping_adjust(tk, offset);\r\nold_vsyscall_fixup(tk);\r\nclock_set |= accumulate_nsecs_to_secs(tk);\r\nwrite_seqcount_begin(&tk_core.seq);\r\ntimekeeping_update(tk, clock_set);\r\nmemcpy(real_tk, tk, sizeof(*tk));\r\nwrite_seqcount_end(&tk_core.seq);\r\nout:\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nif (clock_set)\r\nclock_was_set_delayed();\r\n}\r\nvoid getboottime64(struct timespec64 *ts)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nktime_t t = ktime_sub(tk->offs_real, tk->offs_boot);\r\n*ts = ktime_to_timespec64(t);\r\n}\r\nunsigned long get_seconds(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nreturn tk->xtime_sec;\r\n}\r\nstruct timespec __current_kernel_time(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nreturn timespec64_to_timespec(tk_xtime(tk));\r\n}\r\nstruct timespec64 current_kernel_time64(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 now;\r\nunsigned long seq;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnow = tk_xtime(tk);\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn now;\r\n}\r\nstruct timespec64 get_monotonic_coarse64(void)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nstruct timespec64 now, mono;\r\nunsigned long seq;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nnow = tk_xtime(tk);\r\nmono = tk->wall_to_monotonic;\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nset_normalized_timespec64(&now, now.tv_sec + mono.tv_sec,\r\nnow.tv_nsec + mono.tv_nsec);\r\nreturn now;\r\n}\r\nvoid do_timer(unsigned long ticks)\r\n{\r\njiffies_64 += ticks;\r\ncalc_global_load(ticks);\r\n}\r\nktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,\r\nktime_t *offs_boot, ktime_t *offs_tai)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned int seq;\r\nktime_t base;\r\nu64 nsecs;\r\ndo {\r\nseq = read_seqcount_begin(&tk_core.seq);\r\nbase = tk->tkr_mono.base;\r\nnsecs = timekeeping_get_ns(&tk->tkr_mono);\r\nbase = ktime_add_ns(base, nsecs);\r\nif (*cwsseq != tk->clock_was_set_seq) {\r\n*cwsseq = tk->clock_was_set_seq;\r\n*offs_real = tk->offs_real;\r\n*offs_boot = tk->offs_boot;\r\n*offs_tai = tk->offs_tai;\r\n}\r\nif (unlikely(base.tv64 >= tk->next_leap_ktime.tv64))\r\n*offs_real = ktime_sub(tk->offs_real, ktime_set(1, 0));\r\n} while (read_seqcount_retry(&tk_core.seq, seq));\r\nreturn base;\r\n}\r\nint do_adjtimex(struct timex *txc)\r\n{\r\nstruct timekeeper *tk = &tk_core.timekeeper;\r\nunsigned long flags;\r\nstruct timespec64 ts;\r\ns32 orig_tai, tai;\r\nint ret;\r\nret = ntp_validate_timex(txc);\r\nif (ret)\r\nreturn ret;\r\nif (txc->modes & ADJ_SETOFFSET) {\r\nstruct timespec delta;\r\ndelta.tv_sec = txc->time.tv_sec;\r\ndelta.tv_nsec = txc->time.tv_usec;\r\nif (!(txc->modes & ADJ_NANO))\r\ndelta.tv_nsec *= 1000;\r\nret = timekeeping_inject_offset(&delta);\r\nif (ret)\r\nreturn ret;\r\n}\r\ngetnstimeofday64(&ts);\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\norig_tai = tai = tk->tai_offset;\r\nret = __do_adjtimex(txc, &ts, &tai);\r\nif (tai != orig_tai) {\r\n__timekeeping_set_tai_offset(tk, tai);\r\ntimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\r\n}\r\ntk_update_leap_state(tk);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\nif (tai != orig_tai)\r\nclock_was_set();\r\nntp_notify_cmos_timer();\r\nreturn ret;\r\n}\r\nvoid hardpps(const struct timespec64 *phase_ts, const struct timespec64 *raw_ts)\r\n{\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&timekeeper_lock, flags);\r\nwrite_seqcount_begin(&tk_core.seq);\r\n__hardpps(phase_ts, raw_ts);\r\nwrite_seqcount_end(&tk_core.seq);\r\nraw_spin_unlock_irqrestore(&timekeeper_lock, flags);\r\n}\r\nvoid xtime_update(unsigned long ticks)\r\n{\r\nwrite_seqlock(&jiffies_lock);\r\ndo_timer(ticks);\r\nwrite_sequnlock(&jiffies_lock);\r\nupdate_wall_time();\r\n}
