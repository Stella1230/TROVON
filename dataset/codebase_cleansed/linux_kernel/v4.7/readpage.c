static void completion_pages(struct work_struct *work)\r\n{\r\n#ifdef CONFIG_EXT4_FS_ENCRYPTION\r\nstruct ext4_crypto_ctx *ctx =\r\ncontainer_of(work, struct ext4_crypto_ctx, r.work);\r\nstruct bio *bio = ctx->r.bio;\r\nstruct bio_vec *bv;\r\nint i;\r\nbio_for_each_segment_all(bv, bio, i) {\r\nstruct page *page = bv->bv_page;\r\nint ret = ext4_decrypt(page);\r\nif (ret) {\r\nWARN_ON_ONCE(1);\r\nSetPageError(page);\r\n} else\r\nSetPageUptodate(page);\r\nunlock_page(page);\r\n}\r\next4_release_crypto_ctx(ctx);\r\nbio_put(bio);\r\n#else\r\nBUG();\r\n#endif\r\n}\r\nstatic inline bool ext4_bio_encrypted(struct bio *bio)\r\n{\r\n#ifdef CONFIG_EXT4_FS_ENCRYPTION\r\nreturn unlikely(bio->bi_private != NULL);\r\n#else\r\nreturn false;\r\n#endif\r\n}\r\nstatic void mpage_end_io(struct bio *bio)\r\n{\r\nstruct bio_vec *bv;\r\nint i;\r\nif (ext4_bio_encrypted(bio)) {\r\nstruct ext4_crypto_ctx *ctx = bio->bi_private;\r\nif (bio->bi_error) {\r\next4_release_crypto_ctx(ctx);\r\n} else {\r\nINIT_WORK(&ctx->r.work, completion_pages);\r\nctx->r.bio = bio;\r\nqueue_work(ext4_read_workqueue, &ctx->r.work);\r\nreturn;\r\n}\r\n}\r\nbio_for_each_segment_all(bv, bio, i) {\r\nstruct page *page = bv->bv_page;\r\nif (!bio->bi_error) {\r\nSetPageUptodate(page);\r\n} else {\r\nClearPageUptodate(page);\r\nSetPageError(page);\r\n}\r\nunlock_page(page);\r\n}\r\nbio_put(bio);\r\n}\r\nint ext4_mpage_readpages(struct address_space *mapping,\r\nstruct list_head *pages, struct page *page,\r\nunsigned nr_pages)\r\n{\r\nstruct bio *bio = NULL;\r\nunsigned page_idx;\r\nsector_t last_block_in_bio = 0;\r\nstruct inode *inode = mapping->host;\r\nconst unsigned blkbits = inode->i_blkbits;\r\nconst unsigned blocks_per_page = PAGE_SIZE >> blkbits;\r\nconst unsigned blocksize = 1 << blkbits;\r\nsector_t block_in_file;\r\nsector_t last_block;\r\nsector_t last_block_in_file;\r\nsector_t blocks[MAX_BUF_PER_PAGE];\r\nunsigned page_block;\r\nstruct block_device *bdev = inode->i_sb->s_bdev;\r\nint length;\r\nunsigned relative_block = 0;\r\nstruct ext4_map_blocks map;\r\nmap.m_pblk = 0;\r\nmap.m_lblk = 0;\r\nmap.m_len = 0;\r\nmap.m_flags = 0;\r\nfor (page_idx = 0; nr_pages; page_idx++, nr_pages--) {\r\nint fully_mapped = 1;\r\nunsigned first_hole = blocks_per_page;\r\nprefetchw(&page->flags);\r\nif (pages) {\r\npage = list_entry(pages->prev, struct page, lru);\r\nlist_del(&page->lru);\r\nif (add_to_page_cache_lru(page, mapping, page->index,\r\nmapping_gfp_constraint(mapping, GFP_KERNEL)))\r\ngoto next_page;\r\n}\r\nif (page_has_buffers(page))\r\ngoto confused;\r\nblock_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);\r\nlast_block = block_in_file + nr_pages * blocks_per_page;\r\nlast_block_in_file = (i_size_read(inode) + blocksize - 1) >> blkbits;\r\nif (last_block > last_block_in_file)\r\nlast_block = last_block_in_file;\r\npage_block = 0;\r\nif ((map.m_flags & EXT4_MAP_MAPPED) &&\r\nblock_in_file > map.m_lblk &&\r\nblock_in_file < (map.m_lblk + map.m_len)) {\r\nunsigned map_offset = block_in_file - map.m_lblk;\r\nunsigned last = map.m_len - map_offset;\r\nfor (relative_block = 0; ; relative_block++) {\r\nif (relative_block == last) {\r\nmap.m_flags &= ~EXT4_MAP_MAPPED;\r\nbreak;\r\n}\r\nif (page_block == blocks_per_page)\r\nbreak;\r\nblocks[page_block] = map.m_pblk + map_offset +\r\nrelative_block;\r\npage_block++;\r\nblock_in_file++;\r\n}\r\n}\r\nwhile (page_block < blocks_per_page) {\r\nif (block_in_file < last_block) {\r\nmap.m_lblk = block_in_file;\r\nmap.m_len = last_block - block_in_file;\r\nif (ext4_map_blocks(NULL, inode, &map, 0) < 0) {\r\nset_error_page:\r\nSetPageError(page);\r\nzero_user_segment(page, 0,\r\nPAGE_SIZE);\r\nunlock_page(page);\r\ngoto next_page;\r\n}\r\n}\r\nif ((map.m_flags & EXT4_MAP_MAPPED) == 0) {\r\nfully_mapped = 0;\r\nif (first_hole == blocks_per_page)\r\nfirst_hole = page_block;\r\npage_block++;\r\nblock_in_file++;\r\ncontinue;\r\n}\r\nif (first_hole != blocks_per_page)\r\ngoto confused;\r\nif (page_block && blocks[page_block-1] != map.m_pblk-1)\r\ngoto confused;\r\nfor (relative_block = 0; ; relative_block++) {\r\nif (relative_block == map.m_len) {\r\nmap.m_flags &= ~EXT4_MAP_MAPPED;\r\nbreak;\r\n} else if (page_block == blocks_per_page)\r\nbreak;\r\nblocks[page_block] = map.m_pblk+relative_block;\r\npage_block++;\r\nblock_in_file++;\r\n}\r\n}\r\nif (first_hole != blocks_per_page) {\r\nzero_user_segment(page, first_hole << blkbits,\r\nPAGE_SIZE);\r\nif (first_hole == 0) {\r\nSetPageUptodate(page);\r\nunlock_page(page);\r\ngoto next_page;\r\n}\r\n} else if (fully_mapped) {\r\nSetPageMappedToDisk(page);\r\n}\r\nif (fully_mapped && blocks_per_page == 1 &&\r\n!PageUptodate(page) && cleancache_get_page(page) == 0) {\r\nSetPageUptodate(page);\r\ngoto confused;\r\n}\r\nif (bio && (last_block_in_bio != blocks[0] - 1)) {\r\nsubmit_and_realloc:\r\nsubmit_bio(READ, bio);\r\nbio = NULL;\r\n}\r\nif (bio == NULL) {\r\nstruct ext4_crypto_ctx *ctx = NULL;\r\nif (ext4_encrypted_inode(inode) &&\r\nS_ISREG(inode->i_mode)) {\r\nctx = ext4_get_crypto_ctx(inode, GFP_NOFS);\r\nif (IS_ERR(ctx))\r\ngoto set_error_page;\r\n}\r\nbio = bio_alloc(GFP_KERNEL,\r\nmin_t(int, nr_pages, BIO_MAX_PAGES));\r\nif (!bio) {\r\nif (ctx)\r\next4_release_crypto_ctx(ctx);\r\ngoto set_error_page;\r\n}\r\nbio->bi_bdev = bdev;\r\nbio->bi_iter.bi_sector = blocks[0] << (blkbits - 9);\r\nbio->bi_end_io = mpage_end_io;\r\nbio->bi_private = ctx;\r\n}\r\nlength = first_hole << blkbits;\r\nif (bio_add_page(bio, page, length, 0) < length)\r\ngoto submit_and_realloc;\r\nif (((map.m_flags & EXT4_MAP_BOUNDARY) &&\r\n(relative_block == map.m_len)) ||\r\n(first_hole != blocks_per_page)) {\r\nsubmit_bio(READ, bio);\r\nbio = NULL;\r\n} else\r\nlast_block_in_bio = blocks[blocks_per_page - 1];\r\ngoto next_page;\r\nconfused:\r\nif (bio) {\r\nsubmit_bio(READ, bio);\r\nbio = NULL;\r\n}\r\nif (!PageUptodate(page))\r\nblock_read_full_page(page, ext4_get_block);\r\nelse\r\nunlock_page(page);\r\nnext_page:\r\nif (pages)\r\nput_page(page);\r\n}\r\nBUG_ON(pages && !list_empty(pages));\r\nif (bio)\r\nsubmit_bio(READ, bio);\r\nreturn 0;\r\n}
