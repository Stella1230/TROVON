static u32 edma_readl(struct fsl_edma_engine *edma, void __iomem *addr)\r\n{\r\nif (edma->big_endian)\r\nreturn ioread32be(addr);\r\nelse\r\nreturn ioread32(addr);\r\n}\r\nstatic void edma_writeb(struct fsl_edma_engine *edma, u8 val, void __iomem *addr)\r\n{\r\nif (edma->big_endian)\r\niowrite8(val, (void __iomem *)((unsigned long)addr ^ 0x3));\r\nelse\r\niowrite8(val, addr);\r\n}\r\nstatic void edma_writew(struct fsl_edma_engine *edma, u16 val, void __iomem *addr)\r\n{\r\nif (edma->big_endian)\r\niowrite16be(val, (void __iomem *)((unsigned long)addr ^ 0x2));\r\nelse\r\niowrite16(val, addr);\r\n}\r\nstatic void edma_writel(struct fsl_edma_engine *edma, u32 val, void __iomem *addr)\r\n{\r\nif (edma->big_endian)\r\niowrite32be(val, addr);\r\nelse\r\niowrite32(val, addr);\r\n}\r\nstatic struct fsl_edma_chan *to_fsl_edma_chan(struct dma_chan *chan)\r\n{\r\nreturn container_of(chan, struct fsl_edma_chan, vchan.chan);\r\n}\r\nstatic struct fsl_edma_desc *to_fsl_edma_desc(struct virt_dma_desc *vd)\r\n{\r\nreturn container_of(vd, struct fsl_edma_desc, vdesc);\r\n}\r\nstatic void fsl_edma_enable_request(struct fsl_edma_chan *fsl_chan)\r\n{\r\nvoid __iomem *addr = fsl_chan->edma->membase;\r\nu32 ch = fsl_chan->vchan.chan.chan_id;\r\nedma_writeb(fsl_chan->edma, EDMA_SEEI_SEEI(ch), addr + EDMA_SEEI);\r\nedma_writeb(fsl_chan->edma, ch, addr + EDMA_SERQ);\r\n}\r\nstatic void fsl_edma_disable_request(struct fsl_edma_chan *fsl_chan)\r\n{\r\nvoid __iomem *addr = fsl_chan->edma->membase;\r\nu32 ch = fsl_chan->vchan.chan.chan_id;\r\nedma_writeb(fsl_chan->edma, ch, addr + EDMA_CERQ);\r\nedma_writeb(fsl_chan->edma, EDMA_CEEI_CEEI(ch), addr + EDMA_CEEI);\r\n}\r\nstatic void fsl_edma_chan_mux(struct fsl_edma_chan *fsl_chan,\r\nunsigned int slot, bool enable)\r\n{\r\nu32 ch = fsl_chan->vchan.chan.chan_id;\r\nvoid __iomem *muxaddr;\r\nunsigned chans_per_mux, ch_off;\r\nchans_per_mux = fsl_chan->edma->n_chans / DMAMUX_NR;\r\nch_off = fsl_chan->vchan.chan.chan_id % chans_per_mux;\r\nmuxaddr = fsl_chan->edma->muxbase[ch / chans_per_mux];\r\nslot = EDMAMUX_CHCFG_SOURCE(slot);\r\nif (enable)\r\niowrite8(EDMAMUX_CHCFG_ENBL | slot, muxaddr + ch_off);\r\nelse\r\niowrite8(EDMAMUX_CHCFG_DIS, muxaddr + ch_off);\r\n}\r\nstatic unsigned int fsl_edma_get_tcd_attr(enum dma_slave_buswidth addr_width)\r\n{\r\nswitch (addr_width) {\r\ncase 1:\r\nreturn EDMA_TCD_ATTR_SSIZE_8BIT | EDMA_TCD_ATTR_DSIZE_8BIT;\r\ncase 2:\r\nreturn EDMA_TCD_ATTR_SSIZE_16BIT | EDMA_TCD_ATTR_DSIZE_16BIT;\r\ncase 4:\r\nreturn EDMA_TCD_ATTR_SSIZE_32BIT | EDMA_TCD_ATTR_DSIZE_32BIT;\r\ncase 8:\r\nreturn EDMA_TCD_ATTR_SSIZE_64BIT | EDMA_TCD_ATTR_DSIZE_64BIT;\r\ndefault:\r\nreturn EDMA_TCD_ATTR_SSIZE_32BIT | EDMA_TCD_ATTR_DSIZE_32BIT;\r\n}\r\n}\r\nstatic void fsl_edma_free_desc(struct virt_dma_desc *vdesc)\r\n{\r\nstruct fsl_edma_desc *fsl_desc;\r\nint i;\r\nfsl_desc = to_fsl_edma_desc(vdesc);\r\nfor (i = 0; i < fsl_desc->n_tcds; i++)\r\ndma_pool_free(fsl_desc->echan->tcd_pool, fsl_desc->tcd[i].vtcd,\r\nfsl_desc->tcd[i].ptcd);\r\nkfree(fsl_desc);\r\n}\r\nstatic int fsl_edma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\r\nfsl_edma_disable_request(fsl_chan);\r\nfsl_chan->edesc = NULL;\r\nfsl_chan->idle = true;\r\nvchan_get_all_descriptors(&fsl_chan->vchan, &head);\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\nvchan_dma_desc_free_list(&fsl_chan->vchan, &head);\r\nreturn 0;\r\n}\r\nstatic int fsl_edma_pause(struct dma_chan *chan)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\r\nif (fsl_chan->edesc) {\r\nfsl_edma_disable_request(fsl_chan);\r\nfsl_chan->status = DMA_PAUSED;\r\nfsl_chan->idle = true;\r\n}\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\nreturn 0;\r\n}\r\nstatic int fsl_edma_resume(struct dma_chan *chan)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\r\nif (fsl_chan->edesc) {\r\nfsl_edma_enable_request(fsl_chan);\r\nfsl_chan->status = DMA_IN_PROGRESS;\r\nfsl_chan->idle = false;\r\n}\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\nreturn 0;\r\n}\r\nstatic int fsl_edma_slave_config(struct dma_chan *chan,\r\nstruct dma_slave_config *cfg)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nfsl_chan->fsc.dir = cfg->direction;\r\nif (cfg->direction == DMA_DEV_TO_MEM) {\r\nfsl_chan->fsc.dev_addr = cfg->src_addr;\r\nfsl_chan->fsc.addr_width = cfg->src_addr_width;\r\nfsl_chan->fsc.burst = cfg->src_maxburst;\r\nfsl_chan->fsc.attr = fsl_edma_get_tcd_attr(cfg->src_addr_width);\r\n} else if (cfg->direction == DMA_MEM_TO_DEV) {\r\nfsl_chan->fsc.dev_addr = cfg->dst_addr;\r\nfsl_chan->fsc.addr_width = cfg->dst_addr_width;\r\nfsl_chan->fsc.burst = cfg->dst_maxburst;\r\nfsl_chan->fsc.attr = fsl_edma_get_tcd_attr(cfg->dst_addr_width);\r\n} else {\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic size_t fsl_edma_desc_residue(struct fsl_edma_chan *fsl_chan,\r\nstruct virt_dma_desc *vdesc, bool in_progress)\r\n{\r\nstruct fsl_edma_desc *edesc = fsl_chan->edesc;\r\nvoid __iomem *addr = fsl_chan->edma->membase;\r\nu32 ch = fsl_chan->vchan.chan.chan_id;\r\nenum dma_transfer_direction dir = fsl_chan->fsc.dir;\r\ndma_addr_t cur_addr, dma_addr;\r\nsize_t len, size;\r\nint i;\r\nfor (len = i = 0; i < fsl_chan->edesc->n_tcds; i++)\r\nlen += le32_to_cpu(edesc->tcd[i].vtcd->nbytes)\r\n* le16_to_cpu(edesc->tcd[i].vtcd->biter);\r\nif (!in_progress)\r\nreturn len;\r\nif (dir == DMA_MEM_TO_DEV)\r\ncur_addr = edma_readl(fsl_chan->edma, addr + EDMA_TCD_SADDR(ch));\r\nelse\r\ncur_addr = edma_readl(fsl_chan->edma, addr + EDMA_TCD_DADDR(ch));\r\nfor (i = 0; i < fsl_chan->edesc->n_tcds; i++) {\r\nsize = le32_to_cpu(edesc->tcd[i].vtcd->nbytes)\r\n* le16_to_cpu(edesc->tcd[i].vtcd->biter);\r\nif (dir == DMA_MEM_TO_DEV)\r\ndma_addr = le32_to_cpu(edesc->tcd[i].vtcd->saddr);\r\nelse\r\ndma_addr = le32_to_cpu(edesc->tcd[i].vtcd->daddr);\r\nlen -= size;\r\nif (cur_addr >= dma_addr && cur_addr < dma_addr + size) {\r\nlen += dma_addr + size - cur_addr;\r\nbreak;\r\n}\r\n}\r\nreturn len;\r\n}\r\nstatic enum dma_status fsl_edma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nstruct virt_dma_desc *vdesc;\r\nenum dma_status status;\r\nunsigned long flags;\r\nstatus = dma_cookie_status(chan, cookie, txstate);\r\nif (status == DMA_COMPLETE)\r\nreturn status;\r\nif (!txstate)\r\nreturn fsl_chan->status;\r\nspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\r\nvdesc = vchan_find_desc(&fsl_chan->vchan, cookie);\r\nif (fsl_chan->edesc && cookie == fsl_chan->edesc->vdesc.tx.cookie)\r\ntxstate->residue = fsl_edma_desc_residue(fsl_chan, vdesc, true);\r\nelse if (vdesc)\r\ntxstate->residue = fsl_edma_desc_residue(fsl_chan, vdesc, false);\r\nelse\r\ntxstate->residue = 0;\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\nreturn fsl_chan->status;\r\n}\r\nstatic void fsl_edma_set_tcd_regs(struct fsl_edma_chan *fsl_chan,\r\nstruct fsl_edma_hw_tcd *tcd)\r\n{\r\nstruct fsl_edma_engine *edma = fsl_chan->edma;\r\nvoid __iomem *addr = fsl_chan->edma->membase;\r\nu32 ch = fsl_chan->vchan.chan.chan_id;\r\nedma_writew(edma, 0, addr + EDMA_TCD_CSR(ch));\r\nedma_writel(edma, le32_to_cpu(tcd->saddr), addr + EDMA_TCD_SADDR(ch));\r\nedma_writel(edma, le32_to_cpu(tcd->daddr), addr + EDMA_TCD_DADDR(ch));\r\nedma_writew(edma, le16_to_cpu(tcd->attr), addr + EDMA_TCD_ATTR(ch));\r\nedma_writew(edma, le16_to_cpu(tcd->soff), addr + EDMA_TCD_SOFF(ch));\r\nedma_writel(edma, le32_to_cpu(tcd->nbytes), addr + EDMA_TCD_NBYTES(ch));\r\nedma_writel(edma, le32_to_cpu(tcd->slast), addr + EDMA_TCD_SLAST(ch));\r\nedma_writew(edma, le16_to_cpu(tcd->citer), addr + EDMA_TCD_CITER(ch));\r\nedma_writew(edma, le16_to_cpu(tcd->biter), addr + EDMA_TCD_BITER(ch));\r\nedma_writew(edma, le16_to_cpu(tcd->doff), addr + EDMA_TCD_DOFF(ch));\r\nedma_writel(edma, le32_to_cpu(tcd->dlast_sga), addr + EDMA_TCD_DLAST_SGA(ch));\r\nedma_writew(edma, le16_to_cpu(tcd->csr), addr + EDMA_TCD_CSR(ch));\r\n}\r\nstatic inline\r\nvoid fsl_edma_fill_tcd(struct fsl_edma_hw_tcd *tcd, u32 src, u32 dst,\r\nu16 attr, u16 soff, u32 nbytes, u32 slast, u16 citer,\r\nu16 biter, u16 doff, u32 dlast_sga, bool major_int,\r\nbool disable_req, bool enable_sg)\r\n{\r\nu16 csr = 0;\r\ntcd->saddr = cpu_to_le32(src);\r\ntcd->daddr = cpu_to_le32(dst);\r\ntcd->attr = cpu_to_le16(attr);\r\ntcd->soff = cpu_to_le16(EDMA_TCD_SOFF_SOFF(soff));\r\ntcd->nbytes = cpu_to_le32(EDMA_TCD_NBYTES_NBYTES(nbytes));\r\ntcd->slast = cpu_to_le32(EDMA_TCD_SLAST_SLAST(slast));\r\ntcd->citer = cpu_to_le16(EDMA_TCD_CITER_CITER(citer));\r\ntcd->doff = cpu_to_le16(EDMA_TCD_DOFF_DOFF(doff));\r\ntcd->dlast_sga = cpu_to_le32(EDMA_TCD_DLAST_SGA_DLAST_SGA(dlast_sga));\r\ntcd->biter = cpu_to_le16(EDMA_TCD_BITER_BITER(biter));\r\nif (major_int)\r\ncsr |= EDMA_TCD_CSR_INT_MAJOR;\r\nif (disable_req)\r\ncsr |= EDMA_TCD_CSR_D_REQ;\r\nif (enable_sg)\r\ncsr |= EDMA_TCD_CSR_E_SG;\r\ntcd->csr = cpu_to_le16(csr);\r\n}\r\nstatic struct fsl_edma_desc *fsl_edma_alloc_desc(struct fsl_edma_chan *fsl_chan,\r\nint sg_len)\r\n{\r\nstruct fsl_edma_desc *fsl_desc;\r\nint i;\r\nfsl_desc = kzalloc(sizeof(*fsl_desc) + sizeof(struct fsl_edma_sw_tcd) * sg_len,\r\nGFP_NOWAIT);\r\nif (!fsl_desc)\r\nreturn NULL;\r\nfsl_desc->echan = fsl_chan;\r\nfsl_desc->n_tcds = sg_len;\r\nfor (i = 0; i < sg_len; i++) {\r\nfsl_desc->tcd[i].vtcd = dma_pool_alloc(fsl_chan->tcd_pool,\r\nGFP_NOWAIT, &fsl_desc->tcd[i].ptcd);\r\nif (!fsl_desc->tcd[i].vtcd)\r\ngoto err;\r\n}\r\nreturn fsl_desc;\r\nerr:\r\nwhile (--i >= 0)\r\ndma_pool_free(fsl_chan->tcd_pool, fsl_desc->tcd[i].vtcd,\r\nfsl_desc->tcd[i].ptcd);\r\nkfree(fsl_desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *fsl_edma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t dma_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nstruct fsl_edma_desc *fsl_desc;\r\ndma_addr_t dma_buf_next;\r\nint sg_len, i;\r\nu32 src_addr, dst_addr, last_sg, nbytes;\r\nu16 soff, doff, iter;\r\nif (!is_slave_direction(fsl_chan->fsc.dir))\r\nreturn NULL;\r\nsg_len = buf_len / period_len;\r\nfsl_desc = fsl_edma_alloc_desc(fsl_chan, sg_len);\r\nif (!fsl_desc)\r\nreturn NULL;\r\nfsl_desc->iscyclic = true;\r\ndma_buf_next = dma_addr;\r\nnbytes = fsl_chan->fsc.addr_width * fsl_chan->fsc.burst;\r\niter = period_len / nbytes;\r\nfor (i = 0; i < sg_len; i++) {\r\nif (dma_buf_next >= dma_addr + buf_len)\r\ndma_buf_next = dma_addr;\r\nlast_sg = fsl_desc->tcd[(i + 1) % sg_len].ptcd;\r\nif (fsl_chan->fsc.dir == DMA_MEM_TO_DEV) {\r\nsrc_addr = dma_buf_next;\r\ndst_addr = fsl_chan->fsc.dev_addr;\r\nsoff = fsl_chan->fsc.addr_width;\r\ndoff = 0;\r\n} else {\r\nsrc_addr = fsl_chan->fsc.dev_addr;\r\ndst_addr = dma_buf_next;\r\nsoff = 0;\r\ndoff = fsl_chan->fsc.addr_width;\r\n}\r\nfsl_edma_fill_tcd(fsl_desc->tcd[i].vtcd, src_addr, dst_addr,\r\nfsl_chan->fsc.attr, soff, nbytes, 0, iter,\r\niter, doff, last_sg, true, false, true);\r\ndma_buf_next += period_len;\r\n}\r\nreturn vchan_tx_prep(&fsl_chan->vchan, &fsl_desc->vdesc, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *fsl_edma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nstruct fsl_edma_desc *fsl_desc;\r\nstruct scatterlist *sg;\r\nu32 src_addr, dst_addr, last_sg, nbytes;\r\nu16 soff, doff, iter;\r\nint i;\r\nif (!is_slave_direction(fsl_chan->fsc.dir))\r\nreturn NULL;\r\nfsl_desc = fsl_edma_alloc_desc(fsl_chan, sg_len);\r\nif (!fsl_desc)\r\nreturn NULL;\r\nfsl_desc->iscyclic = false;\r\nnbytes = fsl_chan->fsc.addr_width * fsl_chan->fsc.burst;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nlast_sg = fsl_desc->tcd[(i + 1) % sg_len].ptcd;\r\nif (fsl_chan->fsc.dir == DMA_MEM_TO_DEV) {\r\nsrc_addr = sg_dma_address(sg);\r\ndst_addr = fsl_chan->fsc.dev_addr;\r\nsoff = fsl_chan->fsc.addr_width;\r\ndoff = 0;\r\n} else {\r\nsrc_addr = fsl_chan->fsc.dev_addr;\r\ndst_addr = sg_dma_address(sg);\r\nsoff = 0;\r\ndoff = fsl_chan->fsc.addr_width;\r\n}\r\niter = sg_dma_len(sg) / nbytes;\r\nif (i < sg_len - 1) {\r\nlast_sg = fsl_desc->tcd[(i + 1)].ptcd;\r\nfsl_edma_fill_tcd(fsl_desc->tcd[i].vtcd, src_addr,\r\ndst_addr, fsl_chan->fsc.attr, soff,\r\nnbytes, 0, iter, iter, doff, last_sg,\r\nfalse, false, true);\r\n} else {\r\nlast_sg = 0;\r\nfsl_edma_fill_tcd(fsl_desc->tcd[i].vtcd, src_addr,\r\ndst_addr, fsl_chan->fsc.attr, soff,\r\nnbytes, 0, iter, iter, doff, last_sg,\r\ntrue, true, false);\r\n}\r\n}\r\nreturn vchan_tx_prep(&fsl_chan->vchan, &fsl_desc->vdesc, flags);\r\n}\r\nstatic void fsl_edma_xfer_desc(struct fsl_edma_chan *fsl_chan)\r\n{\r\nstruct virt_dma_desc *vdesc;\r\nvdesc = vchan_next_desc(&fsl_chan->vchan);\r\nif (!vdesc)\r\nreturn;\r\nfsl_chan->edesc = to_fsl_edma_desc(vdesc);\r\nfsl_edma_set_tcd_regs(fsl_chan, fsl_chan->edesc->tcd[0].vtcd);\r\nfsl_edma_enable_request(fsl_chan);\r\nfsl_chan->status = DMA_IN_PROGRESS;\r\nfsl_chan->idle = false;\r\n}\r\nstatic irqreturn_t fsl_edma_tx_handler(int irq, void *dev_id)\r\n{\r\nstruct fsl_edma_engine *fsl_edma = dev_id;\r\nunsigned int intr, ch;\r\nvoid __iomem *base_addr;\r\nstruct fsl_edma_chan *fsl_chan;\r\nbase_addr = fsl_edma->membase;\r\nintr = edma_readl(fsl_edma, base_addr + EDMA_INTR);\r\nif (!intr)\r\nreturn IRQ_NONE;\r\nfor (ch = 0; ch < fsl_edma->n_chans; ch++) {\r\nif (intr & (0x1 << ch)) {\r\nedma_writeb(fsl_edma, EDMA_CINT_CINT(ch),\r\nbase_addr + EDMA_CINT);\r\nfsl_chan = &fsl_edma->chans[ch];\r\nspin_lock(&fsl_chan->vchan.lock);\r\nif (!fsl_chan->edesc->iscyclic) {\r\nlist_del(&fsl_chan->edesc->vdesc.node);\r\nvchan_cookie_complete(&fsl_chan->edesc->vdesc);\r\nfsl_chan->edesc = NULL;\r\nfsl_chan->status = DMA_COMPLETE;\r\nfsl_chan->idle = true;\r\n} else {\r\nvchan_cyclic_callback(&fsl_chan->edesc->vdesc);\r\n}\r\nif (!fsl_chan->edesc)\r\nfsl_edma_xfer_desc(fsl_chan);\r\nspin_unlock(&fsl_chan->vchan.lock);\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t fsl_edma_err_handler(int irq, void *dev_id)\r\n{\r\nstruct fsl_edma_engine *fsl_edma = dev_id;\r\nunsigned int err, ch;\r\nerr = edma_readl(fsl_edma, fsl_edma->membase + EDMA_ERR);\r\nif (!err)\r\nreturn IRQ_NONE;\r\nfor (ch = 0; ch < fsl_edma->n_chans; ch++) {\r\nif (err & (0x1 << ch)) {\r\nfsl_edma_disable_request(&fsl_edma->chans[ch]);\r\nedma_writeb(fsl_edma, EDMA_CERR_CERR(ch),\r\nfsl_edma->membase + EDMA_CERR);\r\nfsl_edma->chans[ch].status = DMA_ERROR;\r\nfsl_edma->chans[ch].idle = true;\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t fsl_edma_irq_handler(int irq, void *dev_id)\r\n{\r\nif (fsl_edma_tx_handler(irq, dev_id) == IRQ_HANDLED)\r\nreturn IRQ_HANDLED;\r\nreturn fsl_edma_err_handler(irq, dev_id);\r\n}\r\nstatic void fsl_edma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\r\nif (unlikely(fsl_chan->pm_state != RUNNING)) {\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\nreturn;\r\n}\r\nif (vchan_issue_pending(&fsl_chan->vchan) && !fsl_chan->edesc)\r\nfsl_edma_xfer_desc(fsl_chan);\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\n}\r\nstatic struct dma_chan *fsl_edma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct fsl_edma_engine *fsl_edma = ofdma->of_dma_data;\r\nstruct dma_chan *chan, *_chan;\r\nstruct fsl_edma_chan *fsl_chan;\r\nunsigned long chans_per_mux = fsl_edma->n_chans / DMAMUX_NR;\r\nif (dma_spec->args_count != 2)\r\nreturn NULL;\r\nmutex_lock(&fsl_edma->fsl_edma_mutex);\r\nlist_for_each_entry_safe(chan, _chan, &fsl_edma->dma_dev.channels, device_node) {\r\nif (chan->client_count)\r\ncontinue;\r\nif ((chan->chan_id / chans_per_mux) == dma_spec->args[0]) {\r\nchan = dma_get_slave_channel(chan);\r\nif (chan) {\r\nchan->device->privatecnt++;\r\nfsl_chan = to_fsl_edma_chan(chan);\r\nfsl_chan->slave_id = dma_spec->args[1];\r\nfsl_edma_chan_mux(fsl_chan, fsl_chan->slave_id,\r\ntrue);\r\nmutex_unlock(&fsl_edma->fsl_edma_mutex);\r\nreturn chan;\r\n}\r\n}\r\n}\r\nmutex_unlock(&fsl_edma->fsl_edma_mutex);\r\nreturn NULL;\r\n}\r\nstatic int fsl_edma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nfsl_chan->tcd_pool = dma_pool_create("tcd_pool", chan->device->dev,\r\nsizeof(struct fsl_edma_hw_tcd),\r\n32, 0);\r\nreturn 0;\r\n}\r\nstatic void fsl_edma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\r\nfsl_edma_disable_request(fsl_chan);\r\nfsl_edma_chan_mux(fsl_chan, 0, false);\r\nfsl_chan->edesc = NULL;\r\nvchan_get_all_descriptors(&fsl_chan->vchan, &head);\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\nvchan_dma_desc_free_list(&fsl_chan->vchan, &head);\r\ndma_pool_destroy(fsl_chan->tcd_pool);\r\nfsl_chan->tcd_pool = NULL;\r\n}\r\nstatic int\r\nfsl_edma_irq_init(struct platform_device *pdev, struct fsl_edma_engine *fsl_edma)\r\n{\r\nint ret;\r\nfsl_edma->txirq = platform_get_irq_byname(pdev, "edma-tx");\r\nif (fsl_edma->txirq < 0) {\r\ndev_err(&pdev->dev, "Can't get edma-tx irq.\n");\r\nreturn fsl_edma->txirq;\r\n}\r\nfsl_edma->errirq = platform_get_irq_byname(pdev, "edma-err");\r\nif (fsl_edma->errirq < 0) {\r\ndev_err(&pdev->dev, "Can't get edma-err irq.\n");\r\nreturn fsl_edma->errirq;\r\n}\r\nif (fsl_edma->txirq == fsl_edma->errirq) {\r\nret = devm_request_irq(&pdev->dev, fsl_edma->txirq,\r\nfsl_edma_irq_handler, 0, "eDMA", fsl_edma);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Can't register eDMA IRQ.\n");\r\nreturn ret;\r\n}\r\n} else {\r\nret = devm_request_irq(&pdev->dev, fsl_edma->txirq,\r\nfsl_edma_tx_handler, 0, "eDMA tx", fsl_edma);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Can't register eDMA tx IRQ.\n");\r\nreturn ret;\r\n}\r\nret = devm_request_irq(&pdev->dev, fsl_edma->errirq,\r\nfsl_edma_err_handler, 0, "eDMA err", fsl_edma);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Can't register eDMA err IRQ.\n");\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int fsl_edma_probe(struct platform_device *pdev)\r\n{\r\nstruct device_node *np = pdev->dev.of_node;\r\nstruct fsl_edma_engine *fsl_edma;\r\nstruct fsl_edma_chan *fsl_chan;\r\nstruct resource *res;\r\nint len, chans;\r\nint ret, i;\r\nret = of_property_read_u32(np, "dma-channels", &chans);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Can't get dma-channels.\n");\r\nreturn ret;\r\n}\r\nlen = sizeof(*fsl_edma) + sizeof(*fsl_chan) * chans;\r\nfsl_edma = devm_kzalloc(&pdev->dev, len, GFP_KERNEL);\r\nif (!fsl_edma)\r\nreturn -ENOMEM;\r\nfsl_edma->n_chans = chans;\r\nmutex_init(&fsl_edma->fsl_edma_mutex);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nfsl_edma->membase = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(fsl_edma->membase))\r\nreturn PTR_ERR(fsl_edma->membase);\r\nfor (i = 0; i < DMAMUX_NR; i++) {\r\nchar clkname[32];\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1 + i);\r\nfsl_edma->muxbase[i] = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(fsl_edma->muxbase[i]))\r\nreturn PTR_ERR(fsl_edma->muxbase[i]);\r\nsprintf(clkname, "dmamux%d", i);\r\nfsl_edma->muxclk[i] = devm_clk_get(&pdev->dev, clkname);\r\nif (IS_ERR(fsl_edma->muxclk[i])) {\r\ndev_err(&pdev->dev, "Missing DMAMUX block clock.\n");\r\nreturn PTR_ERR(fsl_edma->muxclk[i]);\r\n}\r\nret = clk_prepare_enable(fsl_edma->muxclk[i]);\r\nif (ret) {\r\ndev_err(&pdev->dev, "DMAMUX clk block failed.\n");\r\nreturn ret;\r\n}\r\n}\r\nfsl_edma->big_endian = of_property_read_bool(np, "big-endian");\r\nINIT_LIST_HEAD(&fsl_edma->dma_dev.channels);\r\nfor (i = 0; i < fsl_edma->n_chans; i++) {\r\nstruct fsl_edma_chan *fsl_chan = &fsl_edma->chans[i];\r\nfsl_chan->edma = fsl_edma;\r\nfsl_chan->pm_state = RUNNING;\r\nfsl_chan->slave_id = 0;\r\nfsl_chan->idle = true;\r\nfsl_chan->vchan.desc_free = fsl_edma_free_desc;\r\nvchan_init(&fsl_chan->vchan, &fsl_edma->dma_dev);\r\nedma_writew(fsl_edma, 0x0, fsl_edma->membase + EDMA_TCD_CSR(i));\r\nfsl_edma_chan_mux(fsl_chan, 0, false);\r\n}\r\nedma_writel(fsl_edma, ~0, fsl_edma->membase + EDMA_INTR);\r\nret = fsl_edma_irq_init(pdev, fsl_edma);\r\nif (ret)\r\nreturn ret;\r\ndma_cap_set(DMA_PRIVATE, fsl_edma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_SLAVE, fsl_edma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, fsl_edma->dma_dev.cap_mask);\r\nfsl_edma->dma_dev.dev = &pdev->dev;\r\nfsl_edma->dma_dev.device_alloc_chan_resources\r\n= fsl_edma_alloc_chan_resources;\r\nfsl_edma->dma_dev.device_free_chan_resources\r\n= fsl_edma_free_chan_resources;\r\nfsl_edma->dma_dev.device_tx_status = fsl_edma_tx_status;\r\nfsl_edma->dma_dev.device_prep_slave_sg = fsl_edma_prep_slave_sg;\r\nfsl_edma->dma_dev.device_prep_dma_cyclic = fsl_edma_prep_dma_cyclic;\r\nfsl_edma->dma_dev.device_config = fsl_edma_slave_config;\r\nfsl_edma->dma_dev.device_pause = fsl_edma_pause;\r\nfsl_edma->dma_dev.device_resume = fsl_edma_resume;\r\nfsl_edma->dma_dev.device_terminate_all = fsl_edma_terminate_all;\r\nfsl_edma->dma_dev.device_issue_pending = fsl_edma_issue_pending;\r\nfsl_edma->dma_dev.src_addr_widths = FSL_EDMA_BUSWIDTHS;\r\nfsl_edma->dma_dev.dst_addr_widths = FSL_EDMA_BUSWIDTHS;\r\nfsl_edma->dma_dev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nplatform_set_drvdata(pdev, fsl_edma);\r\nret = dma_async_device_register(&fsl_edma->dma_dev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Can't register Freescale eDMA engine.\n");\r\nreturn ret;\r\n}\r\nret = of_dma_controller_register(np, fsl_edma_xlate, fsl_edma);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Can't register Freescale eDMA of_dma.\n");\r\ndma_async_device_unregister(&fsl_edma->dma_dev);\r\nreturn ret;\r\n}\r\nedma_writel(fsl_edma, EDMA_CR_ERGA | EDMA_CR_ERCA, fsl_edma->membase + EDMA_CR);\r\nreturn 0;\r\n}\r\nstatic int fsl_edma_remove(struct platform_device *pdev)\r\n{\r\nstruct device_node *np = pdev->dev.of_node;\r\nstruct fsl_edma_engine *fsl_edma = platform_get_drvdata(pdev);\r\nint i;\r\nof_dma_controller_free(np);\r\ndma_async_device_unregister(&fsl_edma->dma_dev);\r\nfor (i = 0; i < DMAMUX_NR; i++)\r\nclk_disable_unprepare(fsl_edma->muxclk[i]);\r\nreturn 0;\r\n}\r\nstatic int fsl_edma_suspend_late(struct device *dev)\r\n{\r\nstruct fsl_edma_engine *fsl_edma = dev_get_drvdata(dev);\r\nstruct fsl_edma_chan *fsl_chan;\r\nunsigned long flags;\r\nint i;\r\nfor (i = 0; i < fsl_edma->n_chans; i++) {\r\nfsl_chan = &fsl_edma->chans[i];\r\nspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\r\nif (unlikely(!fsl_chan->idle)) {\r\ndev_warn(dev, "WARN: There is non-idle channel.");\r\nfsl_edma_disable_request(fsl_chan);\r\nfsl_edma_chan_mux(fsl_chan, 0, false);\r\n}\r\nfsl_chan->pm_state = SUSPENDED;\r\nspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic int fsl_edma_resume_early(struct device *dev)\r\n{\r\nstruct fsl_edma_engine *fsl_edma = dev_get_drvdata(dev);\r\nstruct fsl_edma_chan *fsl_chan;\r\nint i;\r\nfor (i = 0; i < fsl_edma->n_chans; i++) {\r\nfsl_chan = &fsl_edma->chans[i];\r\nfsl_chan->pm_state = RUNNING;\r\nedma_writew(fsl_edma, 0x0, fsl_edma->membase + EDMA_TCD_CSR(i));\r\nif (fsl_chan->slave_id != 0)\r\nfsl_edma_chan_mux(fsl_chan, fsl_chan->slave_id, true);\r\n}\r\nedma_writel(fsl_edma, EDMA_CR_ERGA | EDMA_CR_ERCA,\r\nfsl_edma->membase + EDMA_CR);\r\nreturn 0;\r\n}\r\nstatic int __init fsl_edma_init(void)\r\n{\r\nreturn platform_driver_register(&fsl_edma_driver);\r\n}\r\nstatic void __exit fsl_edma_exit(void)\r\n{\r\nplatform_driver_unregister(&fsl_edma_driver);\r\n}
