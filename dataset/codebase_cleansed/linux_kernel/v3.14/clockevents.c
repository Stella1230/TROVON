static u64 cev_delta2ns(unsigned long latch, struct clock_event_device *evt,\r\nbool ismax)\r\n{\r\nu64 clc = (u64) latch << evt->shift;\r\nu64 rnd;\r\nif (unlikely(!evt->mult)) {\r\nevt->mult = 1;\r\nWARN_ON(1);\r\n}\r\nrnd = (u64) evt->mult - 1;\r\nif ((clc >> evt->shift) != (u64)latch)\r\nclc = ~0ULL;\r\nif ((~0ULL - clc > rnd) &&\r\n(!ismax || evt->mult <= (1U << evt->shift)))\r\nclc += rnd;\r\ndo_div(clc, evt->mult);\r\nreturn clc > 1000 ? clc : 1000;\r\n}\r\nu64 clockevent_delta2ns(unsigned long latch, struct clock_event_device *evt)\r\n{\r\nreturn cev_delta2ns(latch, evt, false);\r\n}\r\nvoid clockevents_set_mode(struct clock_event_device *dev,\r\nenum clock_event_mode mode)\r\n{\r\nif (dev->mode != mode) {\r\ndev->set_mode(mode, dev);\r\ndev->mode = mode;\r\nif (mode == CLOCK_EVT_MODE_ONESHOT) {\r\nif (unlikely(!dev->mult)) {\r\ndev->mult = 1;\r\nWARN_ON(1);\r\n}\r\n}\r\n}\r\n}\r\nvoid clockevents_shutdown(struct clock_event_device *dev)\r\n{\r\nclockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);\r\ndev->next_event.tv64 = KTIME_MAX;\r\n}\r\nstatic int clockevents_increase_min_delta(struct clock_event_device *dev)\r\n{\r\nif (dev->min_delta_ns >= MIN_DELTA_LIMIT) {\r\nprintk(KERN_WARNING "CE: Reprogramming failure. Giving up\n");\r\ndev->next_event.tv64 = KTIME_MAX;\r\nreturn -ETIME;\r\n}\r\nif (dev->min_delta_ns < 5000)\r\ndev->min_delta_ns = 5000;\r\nelse\r\ndev->min_delta_ns += dev->min_delta_ns >> 1;\r\nif (dev->min_delta_ns > MIN_DELTA_LIMIT)\r\ndev->min_delta_ns = MIN_DELTA_LIMIT;\r\nprintk(KERN_WARNING "CE: %s increased min_delta_ns to %llu nsec\n",\r\ndev->name ? dev->name : "?",\r\n(unsigned long long) dev->min_delta_ns);\r\nreturn 0;\r\n}\r\nstatic int clockevents_program_min_delta(struct clock_event_device *dev)\r\n{\r\nunsigned long long clc;\r\nint64_t delta;\r\nint i;\r\nfor (i = 0;;) {\r\ndelta = dev->min_delta_ns;\r\ndev->next_event = ktime_add_ns(ktime_get(), delta);\r\nif (dev->mode == CLOCK_EVT_MODE_SHUTDOWN)\r\nreturn 0;\r\ndev->retries++;\r\nclc = ((unsigned long long) delta * dev->mult) >> dev->shift;\r\nif (dev->set_next_event((unsigned long) clc, dev) == 0)\r\nreturn 0;\r\nif (++i > 2) {\r\nif (clockevents_increase_min_delta(dev))\r\nreturn -ETIME;\r\ni = 0;\r\n}\r\n}\r\n}\r\nstatic int clockevents_program_min_delta(struct clock_event_device *dev)\r\n{\r\nunsigned long long clc;\r\nint64_t delta;\r\ndelta = dev->min_delta_ns;\r\ndev->next_event = ktime_add_ns(ktime_get(), delta);\r\nif (dev->mode == CLOCK_EVT_MODE_SHUTDOWN)\r\nreturn 0;\r\ndev->retries++;\r\nclc = ((unsigned long long) delta * dev->mult) >> dev->shift;\r\nreturn dev->set_next_event((unsigned long) clc, dev);\r\n}\r\nint clockevents_program_event(struct clock_event_device *dev, ktime_t expires,\r\nbool force)\r\n{\r\nunsigned long long clc;\r\nint64_t delta;\r\nint rc;\r\nif (unlikely(expires.tv64 < 0)) {\r\nWARN_ON_ONCE(1);\r\nreturn -ETIME;\r\n}\r\ndev->next_event = expires;\r\nif (dev->mode == CLOCK_EVT_MODE_SHUTDOWN)\r\nreturn 0;\r\nif (dev->features & CLOCK_EVT_FEAT_KTIME)\r\nreturn dev->set_next_ktime(expires, dev);\r\ndelta = ktime_to_ns(ktime_sub(expires, ktime_get()));\r\nif (delta <= 0)\r\nreturn force ? clockevents_program_min_delta(dev) : -ETIME;\r\ndelta = min(delta, (int64_t) dev->max_delta_ns);\r\ndelta = max(delta, (int64_t) dev->min_delta_ns);\r\nclc = ((unsigned long long) delta * dev->mult) >> dev->shift;\r\nrc = dev->set_next_event((unsigned long) clc, dev);\r\nreturn (rc && force) ? clockevents_program_min_delta(dev) : rc;\r\n}\r\nstatic void clockevents_notify_released(void)\r\n{\r\nstruct clock_event_device *dev;\r\nwhile (!list_empty(&clockevents_released)) {\r\ndev = list_entry(clockevents_released.next,\r\nstruct clock_event_device, list);\r\nlist_del(&dev->list);\r\nlist_add(&dev->list, &clockevent_devices);\r\ntick_check_new_device(dev);\r\n}\r\n}\r\nstatic int clockevents_replace(struct clock_event_device *ced)\r\n{\r\nstruct clock_event_device *dev, *newdev = NULL;\r\nlist_for_each_entry(dev, &clockevent_devices, list) {\r\nif (dev == ced || dev->mode != CLOCK_EVT_MODE_UNUSED)\r\ncontinue;\r\nif (!tick_check_replacement(newdev, dev))\r\ncontinue;\r\nif (!try_module_get(dev->owner))\r\ncontinue;\r\nif (newdev)\r\nmodule_put(newdev->owner);\r\nnewdev = dev;\r\n}\r\nif (newdev) {\r\ntick_install_replacement(newdev);\r\nlist_del_init(&ced->list);\r\n}\r\nreturn newdev ? 0 : -EBUSY;\r\n}\r\nstatic int __clockevents_try_unbind(struct clock_event_device *ced, int cpu)\r\n{\r\nif (ced->mode == CLOCK_EVT_MODE_UNUSED) {\r\nlist_del_init(&ced->list);\r\nreturn 0;\r\n}\r\nreturn ced == per_cpu(tick_cpu_device, cpu).evtdev ? -EAGAIN : -EBUSY;\r\n}\r\nstatic void __clockevents_unbind(void *arg)\r\n{\r\nstruct ce_unbind *cu = arg;\r\nint res;\r\nraw_spin_lock(&clockevents_lock);\r\nres = __clockevents_try_unbind(cu->ce, smp_processor_id());\r\nif (res == -EAGAIN)\r\nres = clockevents_replace(cu->ce);\r\ncu->res = res;\r\nraw_spin_unlock(&clockevents_lock);\r\n}\r\nstatic int clockevents_unbind(struct clock_event_device *ced, int cpu)\r\n{\r\nstruct ce_unbind cu = { .ce = ced, .res = -ENODEV };\r\nsmp_call_function_single(cpu, __clockevents_unbind, &cu, 1);\r\nreturn cu.res;\r\n}\r\nint clockevents_unbind_device(struct clock_event_device *ced, int cpu)\r\n{\r\nint ret;\r\nmutex_lock(&clockevents_mutex);\r\nret = clockevents_unbind(ced, cpu);\r\nmutex_unlock(&clockevents_mutex);\r\nreturn ret;\r\n}\r\nvoid clockevents_register_device(struct clock_event_device *dev)\r\n{\r\nunsigned long flags;\r\nBUG_ON(dev->mode != CLOCK_EVT_MODE_UNUSED);\r\nif (!dev->cpumask) {\r\nWARN_ON(num_possible_cpus() > 1);\r\ndev->cpumask = cpumask_of(smp_processor_id());\r\n}\r\nraw_spin_lock_irqsave(&clockevents_lock, flags);\r\nlist_add(&dev->list, &clockevent_devices);\r\ntick_check_new_device(dev);\r\nclockevents_notify_released();\r\nraw_spin_unlock_irqrestore(&clockevents_lock, flags);\r\n}\r\nvoid clockevents_config(struct clock_event_device *dev, u32 freq)\r\n{\r\nu64 sec;\r\nif (!(dev->features & CLOCK_EVT_FEAT_ONESHOT))\r\nreturn;\r\nsec = dev->max_delta_ticks;\r\ndo_div(sec, freq);\r\nif (!sec)\r\nsec = 1;\r\nelse if (sec > 600 && dev->max_delta_ticks > UINT_MAX)\r\nsec = 600;\r\nclockevents_calc_mult_shift(dev, freq, sec);\r\ndev->min_delta_ns = cev_delta2ns(dev->min_delta_ticks, dev, false);\r\ndev->max_delta_ns = cev_delta2ns(dev->max_delta_ticks, dev, true);\r\n}\r\nvoid clockevents_config_and_register(struct clock_event_device *dev,\r\nu32 freq, unsigned long min_delta,\r\nunsigned long max_delta)\r\n{\r\ndev->min_delta_ticks = min_delta;\r\ndev->max_delta_ticks = max_delta;\r\nclockevents_config(dev, freq);\r\nclockevents_register_device(dev);\r\n}\r\nint clockevents_update_freq(struct clock_event_device *dev, u32 freq)\r\n{\r\nclockevents_config(dev, freq);\r\nif (dev->mode != CLOCK_EVT_MODE_ONESHOT)\r\nreturn 0;\r\nreturn clockevents_program_event(dev, dev->next_event, false);\r\n}\r\nvoid clockevents_handle_noop(struct clock_event_device *dev)\r\n{\r\n}\r\nvoid clockevents_exchange_device(struct clock_event_device *old,\r\nstruct clock_event_device *new)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (old) {\r\nmodule_put(old->owner);\r\nclockevents_set_mode(old, CLOCK_EVT_MODE_UNUSED);\r\nlist_del(&old->list);\r\nlist_add(&old->list, &clockevents_released);\r\n}\r\nif (new) {\r\nBUG_ON(new->mode != CLOCK_EVT_MODE_UNUSED);\r\nclockevents_shutdown(new);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid clockevents_suspend(void)\r\n{\r\nstruct clock_event_device *dev;\r\nlist_for_each_entry_reverse(dev, &clockevent_devices, list)\r\nif (dev->suspend)\r\ndev->suspend(dev);\r\n}\r\nvoid clockevents_resume(void)\r\n{\r\nstruct clock_event_device *dev;\r\nlist_for_each_entry(dev, &clockevent_devices, list)\r\nif (dev->resume)\r\ndev->resume(dev);\r\n}\r\nvoid clockevents_notify(unsigned long reason, void *arg)\r\n{\r\nstruct clock_event_device *dev, *tmp;\r\nunsigned long flags;\r\nint cpu;\r\nraw_spin_lock_irqsave(&clockevents_lock, flags);\r\nswitch (reason) {\r\ncase CLOCK_EVT_NOTIFY_BROADCAST_ON:\r\ncase CLOCK_EVT_NOTIFY_BROADCAST_OFF:\r\ncase CLOCK_EVT_NOTIFY_BROADCAST_FORCE:\r\ntick_broadcast_on_off(reason, arg);\r\nbreak;\r\ncase CLOCK_EVT_NOTIFY_BROADCAST_ENTER:\r\ncase CLOCK_EVT_NOTIFY_BROADCAST_EXIT:\r\ntick_broadcast_oneshot_control(reason);\r\nbreak;\r\ncase CLOCK_EVT_NOTIFY_CPU_DYING:\r\ntick_handover_do_timer(arg);\r\nbreak;\r\ncase CLOCK_EVT_NOTIFY_SUSPEND:\r\ntick_suspend();\r\ntick_suspend_broadcast();\r\nbreak;\r\ncase CLOCK_EVT_NOTIFY_RESUME:\r\ntick_resume();\r\nbreak;\r\ncase CLOCK_EVT_NOTIFY_CPU_DEAD:\r\ntick_shutdown_broadcast_oneshot(arg);\r\ntick_shutdown_broadcast(arg);\r\ntick_shutdown(arg);\r\nlist_for_each_entry_safe(dev, tmp, &clockevents_released, list)\r\nlist_del(&dev->list);\r\ncpu = *((int *)arg);\r\nlist_for_each_entry_safe(dev, tmp, &clockevent_devices, list) {\r\nif (cpumask_test_cpu(cpu, dev->cpumask) &&\r\ncpumask_weight(dev->cpumask) == 1 &&\r\n!tick_is_broadcast_device(dev)) {\r\nBUG_ON(dev->mode != CLOCK_EVT_MODE_UNUSED);\r\nlist_del(&dev->list);\r\n}\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nraw_spin_unlock_irqrestore(&clockevents_lock, flags);\r\n}\r\nstatic ssize_t sysfs_show_current_tick_dev(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct tick_device *td;\r\nssize_t count = 0;\r\nraw_spin_lock_irq(&clockevents_lock);\r\ntd = tick_get_tick_dev(dev);\r\nif (td && td->evtdev)\r\ncount = snprintf(buf, PAGE_SIZE, "%s\n", td->evtdev->name);\r\nraw_spin_unlock_irq(&clockevents_lock);\r\nreturn count;\r\n}\r\nstatic ssize_t sysfs_unbind_tick_dev(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nchar name[CS_NAME_LEN];\r\nssize_t ret = sysfs_get_uname(buf, name, count);\r\nstruct clock_event_device *ce;\r\nif (ret < 0)\r\nreturn ret;\r\nret = -ENODEV;\r\nmutex_lock(&clockevents_mutex);\r\nraw_spin_lock_irq(&clockevents_lock);\r\nlist_for_each_entry(ce, &clockevent_devices, list) {\r\nif (!strcmp(ce->name, name)) {\r\nret = __clockevents_try_unbind(ce, dev->id);\r\nbreak;\r\n}\r\n}\r\nraw_spin_unlock_irq(&clockevents_lock);\r\nif (ret == -EAGAIN)\r\nret = clockevents_unbind(ce, dev->id);\r\nmutex_unlock(&clockevents_mutex);\r\nreturn ret ? ret : count;\r\n}\r\nstatic struct tick_device *tick_get_tick_dev(struct device *dev)\r\n{\r\nreturn dev == &tick_bc_dev ? tick_get_broadcast_device() :\r\n&per_cpu(tick_cpu_device, dev->id);\r\n}\r\nstatic __init int tick_broadcast_init_sysfs(void)\r\n{\r\nint err = device_register(&tick_bc_dev);\r\nif (!err)\r\nerr = device_create_file(&tick_bc_dev, &dev_attr_current_device);\r\nreturn err;\r\n}\r\nstatic struct tick_device *tick_get_tick_dev(struct device *dev)\r\n{\r\nreturn &per_cpu(tick_cpu_device, dev->id);\r\n}\r\nstatic inline int tick_broadcast_init_sysfs(void) { return 0; }\r\nstatic int __init tick_init_sysfs(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct device *dev = &per_cpu(tick_percpu_dev, cpu);\r\nint err;\r\ndev->id = cpu;\r\ndev->bus = &clockevents_subsys;\r\nerr = device_register(dev);\r\nif (!err)\r\nerr = device_create_file(dev, &dev_attr_current_device);\r\nif (!err)\r\nerr = device_create_file(dev, &dev_attr_unbind_device);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn tick_broadcast_init_sysfs();\r\n}\r\nstatic int __init clockevents_init_sysfs(void)\r\n{\r\nint err = subsys_system_register(&clockevents_subsys, NULL);\r\nif (!err)\r\nerr = tick_init_sysfs();\r\nreturn err;\r\n}
