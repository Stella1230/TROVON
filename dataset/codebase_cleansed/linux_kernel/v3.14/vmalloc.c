static void free_work(struct work_struct *w)\r\n{\r\nstruct vfree_deferred *p = container_of(w, struct vfree_deferred, wq);\r\nstruct llist_node *llnode = llist_del_all(&p->list);\r\nwhile (llnode) {\r\nvoid *p = llnode;\r\nllnode = llist_next(llnode);\r\n__vunmap(p, 1);\r\n}\r\n}\r\nstatic void vunmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end)\r\n{\r\npte_t *pte;\r\npte = pte_offset_kernel(pmd, addr);\r\ndo {\r\npte_t ptent = ptep_get_and_clear(&init_mm, addr, pte);\r\nWARN_ON(!pte_none(ptent) && !pte_present(ptent));\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\n}\r\nstatic void vunmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end)\r\n{\r\npmd_t *pmd;\r\nunsigned long next;\r\npmd = pmd_offset(pud, addr);\r\ndo {\r\nnext = pmd_addr_end(addr, end);\r\nif (pmd_none_or_clear_bad(pmd))\r\ncontinue;\r\nvunmap_pte_range(pmd, addr, next);\r\n} while (pmd++, addr = next, addr != end);\r\n}\r\nstatic void vunmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end)\r\n{\r\npud_t *pud;\r\nunsigned long next;\r\npud = pud_offset(pgd, addr);\r\ndo {\r\nnext = pud_addr_end(addr, end);\r\nif (pud_none_or_clear_bad(pud))\r\ncontinue;\r\nvunmap_pmd_range(pud, addr, next);\r\n} while (pud++, addr = next, addr != end);\r\n}\r\nstatic void vunmap_page_range(unsigned long addr, unsigned long end)\r\n{\r\npgd_t *pgd;\r\nunsigned long next;\r\nBUG_ON(addr >= end);\r\npgd = pgd_offset_k(addr);\r\ndo {\r\nnext = pgd_addr_end(addr, end);\r\nif (pgd_none_or_clear_bad(pgd))\r\ncontinue;\r\nvunmap_pud_range(pgd, addr, next);\r\n} while (pgd++, addr = next, addr != end);\r\n}\r\nstatic int vmap_pte_range(pmd_t *pmd, unsigned long addr,\r\nunsigned long end, pgprot_t prot, struct page **pages, int *nr)\r\n{\r\npte_t *pte;\r\npte = pte_alloc_kernel(pmd, addr);\r\nif (!pte)\r\nreturn -ENOMEM;\r\ndo {\r\nstruct page *page = pages[*nr];\r\nif (WARN_ON(!pte_none(*pte)))\r\nreturn -EBUSY;\r\nif (WARN_ON(!page))\r\nreturn -ENOMEM;\r\nset_pte_at(&init_mm, addr, pte, mk_pte(page, prot));\r\n(*nr)++;\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\nreturn 0;\r\n}\r\nstatic int vmap_pmd_range(pud_t *pud, unsigned long addr,\r\nunsigned long end, pgprot_t prot, struct page **pages, int *nr)\r\n{\r\npmd_t *pmd;\r\nunsigned long next;\r\npmd = pmd_alloc(&init_mm, pud, addr);\r\nif (!pmd)\r\nreturn -ENOMEM;\r\ndo {\r\nnext = pmd_addr_end(addr, end);\r\nif (vmap_pte_range(pmd, addr, next, prot, pages, nr))\r\nreturn -ENOMEM;\r\n} while (pmd++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nstatic int vmap_pud_range(pgd_t *pgd, unsigned long addr,\r\nunsigned long end, pgprot_t prot, struct page **pages, int *nr)\r\n{\r\npud_t *pud;\r\nunsigned long next;\r\npud = pud_alloc(&init_mm, pgd, addr);\r\nif (!pud)\r\nreturn -ENOMEM;\r\ndo {\r\nnext = pud_addr_end(addr, end);\r\nif (vmap_pmd_range(pud, addr, next, prot, pages, nr))\r\nreturn -ENOMEM;\r\n} while (pud++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nstatic int vmap_page_range_noflush(unsigned long start, unsigned long end,\r\npgprot_t prot, struct page **pages)\r\n{\r\npgd_t *pgd;\r\nunsigned long next;\r\nunsigned long addr = start;\r\nint err = 0;\r\nint nr = 0;\r\nBUG_ON(addr >= end);\r\npgd = pgd_offset_k(addr);\r\ndo {\r\nnext = pgd_addr_end(addr, end);\r\nerr = vmap_pud_range(pgd, addr, next, prot, pages, &nr);\r\nif (err)\r\nreturn err;\r\n} while (pgd++, addr = next, addr != end);\r\nreturn nr;\r\n}\r\nstatic int vmap_page_range(unsigned long start, unsigned long end,\r\npgprot_t prot, struct page **pages)\r\n{\r\nint ret;\r\nret = vmap_page_range_noflush(start, end, prot, pages);\r\nflush_cache_vmap(start, end);\r\nreturn ret;\r\n}\r\nint is_vmalloc_or_module_addr(const void *x)\r\n{\r\n#if defined(CONFIG_MODULES) && defined(MODULES_VADDR)\r\nunsigned long addr = (unsigned long)x;\r\nif (addr >= MODULES_VADDR && addr < MODULES_END)\r\nreturn 1;\r\n#endif\r\nreturn is_vmalloc_addr(x);\r\n}\r\nstruct page *vmalloc_to_page(const void *vmalloc_addr)\r\n{\r\nunsigned long addr = (unsigned long) vmalloc_addr;\r\nstruct page *page = NULL;\r\npgd_t *pgd = pgd_offset_k(addr);\r\nVIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));\r\nif (!pgd_none(*pgd)) {\r\npud_t *pud = pud_offset(pgd, addr);\r\nif (!pud_none(*pud)) {\r\npmd_t *pmd = pmd_offset(pud, addr);\r\nif (!pmd_none(*pmd)) {\r\npte_t *ptep, pte;\r\nptep = pte_offset_map(pmd, addr);\r\npte = *ptep;\r\nif (pte_present(pte))\r\npage = pte_page(pte);\r\npte_unmap(ptep);\r\n}\r\n}\r\n}\r\nreturn page;\r\n}\r\nunsigned long vmalloc_to_pfn(const void *vmalloc_addr)\r\n{\r\nreturn page_to_pfn(vmalloc_to_page(vmalloc_addr));\r\n}\r\nstatic struct vmap_area *__find_vmap_area(unsigned long addr)\r\n{\r\nstruct rb_node *n = vmap_area_root.rb_node;\r\nwhile (n) {\r\nstruct vmap_area *va;\r\nva = rb_entry(n, struct vmap_area, rb_node);\r\nif (addr < va->va_start)\r\nn = n->rb_left;\r\nelse if (addr >= va->va_end)\r\nn = n->rb_right;\r\nelse\r\nreturn va;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void __insert_vmap_area(struct vmap_area *va)\r\n{\r\nstruct rb_node **p = &vmap_area_root.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct rb_node *tmp;\r\nwhile (*p) {\r\nstruct vmap_area *tmp_va;\r\nparent = *p;\r\ntmp_va = rb_entry(parent, struct vmap_area, rb_node);\r\nif (va->va_start < tmp_va->va_end)\r\np = &(*p)->rb_left;\r\nelse if (va->va_end > tmp_va->va_start)\r\np = &(*p)->rb_right;\r\nelse\r\nBUG();\r\n}\r\nrb_link_node(&va->rb_node, parent, p);\r\nrb_insert_color(&va->rb_node, &vmap_area_root);\r\ntmp = rb_prev(&va->rb_node);\r\nif (tmp) {\r\nstruct vmap_area *prev;\r\nprev = rb_entry(tmp, struct vmap_area, rb_node);\r\nlist_add_rcu(&va->list, &prev->list);\r\n} else\r\nlist_add_rcu(&va->list, &vmap_area_list);\r\n}\r\nstatic struct vmap_area *alloc_vmap_area(unsigned long size,\r\nunsigned long align,\r\nunsigned long vstart, unsigned long vend,\r\nint node, gfp_t gfp_mask)\r\n{\r\nstruct vmap_area *va;\r\nstruct rb_node *n;\r\nunsigned long addr;\r\nint purged = 0;\r\nstruct vmap_area *first;\r\nBUG_ON(!size);\r\nBUG_ON(size & ~PAGE_MASK);\r\nBUG_ON(!is_power_of_2(align));\r\nva = kmalloc_node(sizeof(struct vmap_area),\r\ngfp_mask & GFP_RECLAIM_MASK, node);\r\nif (unlikely(!va))\r\nreturn ERR_PTR(-ENOMEM);\r\nkmemleak_scan_area(&va->rb_node, SIZE_MAX, gfp_mask & GFP_RECLAIM_MASK);\r\nretry:\r\nspin_lock(&vmap_area_lock);\r\nif (!free_vmap_cache ||\r\nsize < cached_hole_size ||\r\nvstart < cached_vstart ||\r\nalign < cached_align) {\r\nnocache:\r\ncached_hole_size = 0;\r\nfree_vmap_cache = NULL;\r\n}\r\ncached_vstart = vstart;\r\ncached_align = align;\r\nif (free_vmap_cache) {\r\nfirst = rb_entry(free_vmap_cache, struct vmap_area, rb_node);\r\naddr = ALIGN(first->va_end, align);\r\nif (addr < vstart)\r\ngoto nocache;\r\nif (addr + size < addr)\r\ngoto overflow;\r\n} else {\r\naddr = ALIGN(vstart, align);\r\nif (addr + size < addr)\r\ngoto overflow;\r\nn = vmap_area_root.rb_node;\r\nfirst = NULL;\r\nwhile (n) {\r\nstruct vmap_area *tmp;\r\ntmp = rb_entry(n, struct vmap_area, rb_node);\r\nif (tmp->va_end >= addr) {\r\nfirst = tmp;\r\nif (tmp->va_start <= addr)\r\nbreak;\r\nn = n->rb_left;\r\n} else\r\nn = n->rb_right;\r\n}\r\nif (!first)\r\ngoto found;\r\n}\r\nwhile (addr + size > first->va_start && addr + size <= vend) {\r\nif (addr + cached_hole_size < first->va_start)\r\ncached_hole_size = first->va_start - addr;\r\naddr = ALIGN(first->va_end, align);\r\nif (addr + size < addr)\r\ngoto overflow;\r\nif (list_is_last(&first->list, &vmap_area_list))\r\ngoto found;\r\nfirst = list_entry(first->list.next,\r\nstruct vmap_area, list);\r\n}\r\nfound:\r\nif (addr + size > vend)\r\ngoto overflow;\r\nva->va_start = addr;\r\nva->va_end = addr + size;\r\nva->flags = 0;\r\n__insert_vmap_area(va);\r\nfree_vmap_cache = &va->rb_node;\r\nspin_unlock(&vmap_area_lock);\r\nBUG_ON(va->va_start & (align-1));\r\nBUG_ON(va->va_start < vstart);\r\nBUG_ON(va->va_end > vend);\r\nreturn va;\r\noverflow:\r\nspin_unlock(&vmap_area_lock);\r\nif (!purged) {\r\npurge_vmap_area_lazy();\r\npurged = 1;\r\ngoto retry;\r\n}\r\nif (printk_ratelimit())\r\nprintk(KERN_WARNING\r\n"vmap allocation for size %lu failed: "\r\n"use vmalloc=<size> to increase size.\n", size);\r\nkfree(va);\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nstatic void __free_vmap_area(struct vmap_area *va)\r\n{\r\nBUG_ON(RB_EMPTY_NODE(&va->rb_node));\r\nif (free_vmap_cache) {\r\nif (va->va_end < cached_vstart) {\r\nfree_vmap_cache = NULL;\r\n} else {\r\nstruct vmap_area *cache;\r\ncache = rb_entry(free_vmap_cache, struct vmap_area, rb_node);\r\nif (va->va_start <= cache->va_start) {\r\nfree_vmap_cache = rb_prev(&va->rb_node);\r\n}\r\n}\r\n}\r\nrb_erase(&va->rb_node, &vmap_area_root);\r\nRB_CLEAR_NODE(&va->rb_node);\r\nlist_del_rcu(&va->list);\r\nif (va->va_end > VMALLOC_START && va->va_end <= VMALLOC_END)\r\nvmap_area_pcpu_hole = max(vmap_area_pcpu_hole, va->va_end);\r\nkfree_rcu(va, rcu_head);\r\n}\r\nstatic void free_vmap_area(struct vmap_area *va)\r\n{\r\nspin_lock(&vmap_area_lock);\r\n__free_vmap_area(va);\r\nspin_unlock(&vmap_area_lock);\r\n}\r\nstatic void unmap_vmap_area(struct vmap_area *va)\r\n{\r\nvunmap_page_range(va->va_start, va->va_end);\r\n}\r\nstatic void vmap_debug_free_range(unsigned long start, unsigned long end)\r\n{\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nvunmap_page_range(start, end);\r\nflush_tlb_kernel_range(start, end);\r\n#endif\r\n}\r\nstatic unsigned long lazy_max_pages(void)\r\n{\r\nunsigned int log;\r\nlog = fls(num_online_cpus());\r\nreturn log * (32UL * 1024 * 1024 / PAGE_SIZE);\r\n}\r\nvoid set_iounmap_nonlazy(void)\r\n{\r\natomic_set(&vmap_lazy_nr, lazy_max_pages()+1);\r\n}\r\nstatic void __purge_vmap_area_lazy(unsigned long *start, unsigned long *end,\r\nint sync, int force_flush)\r\n{\r\nstatic DEFINE_SPINLOCK(purge_lock);\r\nLIST_HEAD(valist);\r\nstruct vmap_area *va;\r\nstruct vmap_area *n_va;\r\nint nr = 0;\r\nif (!sync && !force_flush) {\r\nif (!spin_trylock(&purge_lock))\r\nreturn;\r\n} else\r\nspin_lock(&purge_lock);\r\nif (sync)\r\npurge_fragmented_blocks_allcpus();\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(va, &vmap_area_list, list) {\r\nif (va->flags & VM_LAZY_FREE) {\r\nif (va->va_start < *start)\r\n*start = va->va_start;\r\nif (va->va_end > *end)\r\n*end = va->va_end;\r\nnr += (va->va_end - va->va_start) >> PAGE_SHIFT;\r\nlist_add_tail(&va->purge_list, &valist);\r\nva->flags |= VM_LAZY_FREEING;\r\nva->flags &= ~VM_LAZY_FREE;\r\n}\r\n}\r\nrcu_read_unlock();\r\nif (nr)\r\natomic_sub(nr, &vmap_lazy_nr);\r\nif (nr || force_flush)\r\nflush_tlb_kernel_range(*start, *end);\r\nif (nr) {\r\nspin_lock(&vmap_area_lock);\r\nlist_for_each_entry_safe(va, n_va, &valist, purge_list)\r\n__free_vmap_area(va);\r\nspin_unlock(&vmap_area_lock);\r\n}\r\nspin_unlock(&purge_lock);\r\n}\r\nstatic void try_purge_vmap_area_lazy(void)\r\n{\r\nunsigned long start = ULONG_MAX, end = 0;\r\n__purge_vmap_area_lazy(&start, &end, 0, 0);\r\n}\r\nstatic void purge_vmap_area_lazy(void)\r\n{\r\nunsigned long start = ULONG_MAX, end = 0;\r\n__purge_vmap_area_lazy(&start, &end, 1, 0);\r\n}\r\nstatic void free_vmap_area_noflush(struct vmap_area *va)\r\n{\r\nva->flags |= VM_LAZY_FREE;\r\natomic_add((va->va_end - va->va_start) >> PAGE_SHIFT, &vmap_lazy_nr);\r\nif (unlikely(atomic_read(&vmap_lazy_nr) > lazy_max_pages()))\r\ntry_purge_vmap_area_lazy();\r\n}\r\nstatic void free_unmap_vmap_area_noflush(struct vmap_area *va)\r\n{\r\nunmap_vmap_area(va);\r\nfree_vmap_area_noflush(va);\r\n}\r\nstatic void free_unmap_vmap_area(struct vmap_area *va)\r\n{\r\nflush_cache_vunmap(va->va_start, va->va_end);\r\nfree_unmap_vmap_area_noflush(va);\r\n}\r\nstatic struct vmap_area *find_vmap_area(unsigned long addr)\r\n{\r\nstruct vmap_area *va;\r\nspin_lock(&vmap_area_lock);\r\nva = __find_vmap_area(addr);\r\nspin_unlock(&vmap_area_lock);\r\nreturn va;\r\n}\r\nstatic void free_unmap_vmap_area_addr(unsigned long addr)\r\n{\r\nstruct vmap_area *va;\r\nva = find_vmap_area(addr);\r\nBUG_ON(!va);\r\nfree_unmap_vmap_area(va);\r\n}\r\nstatic unsigned long addr_to_vb_idx(unsigned long addr)\r\n{\r\naddr -= VMALLOC_START & ~(VMAP_BLOCK_SIZE-1);\r\naddr /= VMAP_BLOCK_SIZE;\r\nreturn addr;\r\n}\r\nstatic struct vmap_block *new_vmap_block(gfp_t gfp_mask)\r\n{\r\nstruct vmap_block_queue *vbq;\r\nstruct vmap_block *vb;\r\nstruct vmap_area *va;\r\nunsigned long vb_idx;\r\nint node, err;\r\nnode = numa_node_id();\r\nvb = kmalloc_node(sizeof(struct vmap_block),\r\ngfp_mask & GFP_RECLAIM_MASK, node);\r\nif (unlikely(!vb))\r\nreturn ERR_PTR(-ENOMEM);\r\nva = alloc_vmap_area(VMAP_BLOCK_SIZE, VMAP_BLOCK_SIZE,\r\nVMALLOC_START, VMALLOC_END,\r\nnode, gfp_mask);\r\nif (IS_ERR(va)) {\r\nkfree(vb);\r\nreturn ERR_CAST(va);\r\n}\r\nerr = radix_tree_preload(gfp_mask);\r\nif (unlikely(err)) {\r\nkfree(vb);\r\nfree_vmap_area(va);\r\nreturn ERR_PTR(err);\r\n}\r\nspin_lock_init(&vb->lock);\r\nvb->va = va;\r\nvb->free = VMAP_BBMAP_BITS;\r\nvb->dirty = 0;\r\nbitmap_zero(vb->dirty_map, VMAP_BBMAP_BITS);\r\nINIT_LIST_HEAD(&vb->free_list);\r\nvb_idx = addr_to_vb_idx(va->va_start);\r\nspin_lock(&vmap_block_tree_lock);\r\nerr = radix_tree_insert(&vmap_block_tree, vb_idx, vb);\r\nspin_unlock(&vmap_block_tree_lock);\r\nBUG_ON(err);\r\nradix_tree_preload_end();\r\nvbq = &get_cpu_var(vmap_block_queue);\r\nspin_lock(&vbq->lock);\r\nlist_add_rcu(&vb->free_list, &vbq->free);\r\nspin_unlock(&vbq->lock);\r\nput_cpu_var(vmap_block_queue);\r\nreturn vb;\r\n}\r\nstatic void free_vmap_block(struct vmap_block *vb)\r\n{\r\nstruct vmap_block *tmp;\r\nunsigned long vb_idx;\r\nvb_idx = addr_to_vb_idx(vb->va->va_start);\r\nspin_lock(&vmap_block_tree_lock);\r\ntmp = radix_tree_delete(&vmap_block_tree, vb_idx);\r\nspin_unlock(&vmap_block_tree_lock);\r\nBUG_ON(tmp != vb);\r\nfree_vmap_area_noflush(vb->va);\r\nkfree_rcu(vb, rcu_head);\r\n}\r\nstatic void purge_fragmented_blocks(int cpu)\r\n{\r\nLIST_HEAD(purge);\r\nstruct vmap_block *vb;\r\nstruct vmap_block *n_vb;\r\nstruct vmap_block_queue *vbq = &per_cpu(vmap_block_queue, cpu);\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(vb, &vbq->free, free_list) {\r\nif (!(vb->free + vb->dirty == VMAP_BBMAP_BITS && vb->dirty != VMAP_BBMAP_BITS))\r\ncontinue;\r\nspin_lock(&vb->lock);\r\nif (vb->free + vb->dirty == VMAP_BBMAP_BITS && vb->dirty != VMAP_BBMAP_BITS) {\r\nvb->free = 0;\r\nvb->dirty = VMAP_BBMAP_BITS;\r\nbitmap_fill(vb->dirty_map, VMAP_BBMAP_BITS);\r\nspin_lock(&vbq->lock);\r\nlist_del_rcu(&vb->free_list);\r\nspin_unlock(&vbq->lock);\r\nspin_unlock(&vb->lock);\r\nlist_add_tail(&vb->purge, &purge);\r\n} else\r\nspin_unlock(&vb->lock);\r\n}\r\nrcu_read_unlock();\r\nlist_for_each_entry_safe(vb, n_vb, &purge, purge) {\r\nlist_del(&vb->purge);\r\nfree_vmap_block(vb);\r\n}\r\n}\r\nstatic void purge_fragmented_blocks_allcpus(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\npurge_fragmented_blocks(cpu);\r\n}\r\nstatic void *vb_alloc(unsigned long size, gfp_t gfp_mask)\r\n{\r\nstruct vmap_block_queue *vbq;\r\nstruct vmap_block *vb;\r\nunsigned long addr = 0;\r\nunsigned int order;\r\nBUG_ON(size & ~PAGE_MASK);\r\nBUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);\r\nif (WARN_ON(size == 0)) {\r\nreturn NULL;\r\n}\r\norder = get_order(size);\r\nagain:\r\nrcu_read_lock();\r\nvbq = &get_cpu_var(vmap_block_queue);\r\nlist_for_each_entry_rcu(vb, &vbq->free, free_list) {\r\nint i;\r\nspin_lock(&vb->lock);\r\nif (vb->free < 1UL << order)\r\ngoto next;\r\ni = VMAP_BBMAP_BITS - vb->free;\r\naddr = vb->va->va_start + (i << PAGE_SHIFT);\r\nBUG_ON(addr_to_vb_idx(addr) !=\r\naddr_to_vb_idx(vb->va->va_start));\r\nvb->free -= 1UL << order;\r\nif (vb->free == 0) {\r\nspin_lock(&vbq->lock);\r\nlist_del_rcu(&vb->free_list);\r\nspin_unlock(&vbq->lock);\r\n}\r\nspin_unlock(&vb->lock);\r\nbreak;\r\nnext:\r\nspin_unlock(&vb->lock);\r\n}\r\nput_cpu_var(vmap_block_queue);\r\nrcu_read_unlock();\r\nif (!addr) {\r\nvb = new_vmap_block(gfp_mask);\r\nif (IS_ERR(vb))\r\nreturn vb;\r\ngoto again;\r\n}\r\nreturn (void *)addr;\r\n}\r\nstatic void vb_free(const void *addr, unsigned long size)\r\n{\r\nunsigned long offset;\r\nunsigned long vb_idx;\r\nunsigned int order;\r\nstruct vmap_block *vb;\r\nBUG_ON(size & ~PAGE_MASK);\r\nBUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);\r\nflush_cache_vunmap((unsigned long)addr, (unsigned long)addr + size);\r\norder = get_order(size);\r\noffset = (unsigned long)addr & (VMAP_BLOCK_SIZE - 1);\r\nvb_idx = addr_to_vb_idx((unsigned long)addr);\r\nrcu_read_lock();\r\nvb = radix_tree_lookup(&vmap_block_tree, vb_idx);\r\nrcu_read_unlock();\r\nBUG_ON(!vb);\r\nvunmap_page_range((unsigned long)addr, (unsigned long)addr + size);\r\nspin_lock(&vb->lock);\r\nBUG_ON(bitmap_allocate_region(vb->dirty_map, offset >> PAGE_SHIFT, order));\r\nvb->dirty += 1UL << order;\r\nif (vb->dirty == VMAP_BBMAP_BITS) {\r\nBUG_ON(vb->free);\r\nspin_unlock(&vb->lock);\r\nfree_vmap_block(vb);\r\n} else\r\nspin_unlock(&vb->lock);\r\n}\r\nvoid vm_unmap_aliases(void)\r\n{\r\nunsigned long start = ULONG_MAX, end = 0;\r\nint cpu;\r\nint flush = 0;\r\nif (unlikely(!vmap_initialized))\r\nreturn;\r\nfor_each_possible_cpu(cpu) {\r\nstruct vmap_block_queue *vbq = &per_cpu(vmap_block_queue, cpu);\r\nstruct vmap_block *vb;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(vb, &vbq->free, free_list) {\r\nint i, j;\r\nspin_lock(&vb->lock);\r\ni = find_first_bit(vb->dirty_map, VMAP_BBMAP_BITS);\r\nif (i < VMAP_BBMAP_BITS) {\r\nunsigned long s, e;\r\nj = find_last_bit(vb->dirty_map,\r\nVMAP_BBMAP_BITS);\r\nj = j + 1;\r\ns = vb->va->va_start + (i << PAGE_SHIFT);\r\ne = vb->va->va_start + (j << PAGE_SHIFT);\r\nflush = 1;\r\nif (s < start)\r\nstart = s;\r\nif (e > end)\r\nend = e;\r\n}\r\nspin_unlock(&vb->lock);\r\n}\r\nrcu_read_unlock();\r\n}\r\n__purge_vmap_area_lazy(&start, &end, 1, flush);\r\n}\r\nvoid vm_unmap_ram(const void *mem, unsigned int count)\r\n{\r\nunsigned long size = count << PAGE_SHIFT;\r\nunsigned long addr = (unsigned long)mem;\r\nBUG_ON(!addr);\r\nBUG_ON(addr < VMALLOC_START);\r\nBUG_ON(addr > VMALLOC_END);\r\nBUG_ON(addr & (PAGE_SIZE-1));\r\ndebug_check_no_locks_freed(mem, size);\r\nvmap_debug_free_range(addr, addr+size);\r\nif (likely(count <= VMAP_MAX_ALLOC))\r\nvb_free(mem, size);\r\nelse\r\nfree_unmap_vmap_area_addr(addr);\r\n}\r\nvoid *vm_map_ram(struct page **pages, unsigned int count, int node, pgprot_t prot)\r\n{\r\nunsigned long size = count << PAGE_SHIFT;\r\nunsigned long addr;\r\nvoid *mem;\r\nif (likely(count <= VMAP_MAX_ALLOC)) {\r\nmem = vb_alloc(size, GFP_KERNEL);\r\nif (IS_ERR(mem))\r\nreturn NULL;\r\naddr = (unsigned long)mem;\r\n} else {\r\nstruct vmap_area *va;\r\nva = alloc_vmap_area(size, PAGE_SIZE,\r\nVMALLOC_START, VMALLOC_END, node, GFP_KERNEL);\r\nif (IS_ERR(va))\r\nreturn NULL;\r\naddr = va->va_start;\r\nmem = (void *)addr;\r\n}\r\nif (vmap_page_range(addr, addr + size, prot, pages) < 0) {\r\nvm_unmap_ram(mem, count);\r\nreturn NULL;\r\n}\r\nreturn mem;\r\n}\r\nvoid __init vm_area_add_early(struct vm_struct *vm)\r\n{\r\nstruct vm_struct *tmp, **p;\r\nBUG_ON(vmap_initialized);\r\nfor (p = &vmlist; (tmp = *p) != NULL; p = &tmp->next) {\r\nif (tmp->addr >= vm->addr) {\r\nBUG_ON(tmp->addr < vm->addr + vm->size);\r\nbreak;\r\n} else\r\nBUG_ON(tmp->addr + tmp->size > vm->addr);\r\n}\r\nvm->next = *p;\r\n*p = vm;\r\n}\r\nvoid __init vm_area_register_early(struct vm_struct *vm, size_t align)\r\n{\r\nstatic size_t vm_init_off __initdata;\r\nunsigned long addr;\r\naddr = ALIGN(VMALLOC_START + vm_init_off, align);\r\nvm_init_off = PFN_ALIGN(addr + vm->size) - VMALLOC_START;\r\nvm->addr = (void *)addr;\r\nvm_area_add_early(vm);\r\n}\r\nvoid __init vmalloc_init(void)\r\n{\r\nstruct vmap_area *va;\r\nstruct vm_struct *tmp;\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nstruct vmap_block_queue *vbq;\r\nstruct vfree_deferred *p;\r\nvbq = &per_cpu(vmap_block_queue, i);\r\nspin_lock_init(&vbq->lock);\r\nINIT_LIST_HEAD(&vbq->free);\r\np = &per_cpu(vfree_deferred, i);\r\ninit_llist_head(&p->list);\r\nINIT_WORK(&p->wq, free_work);\r\n}\r\nfor (tmp = vmlist; tmp; tmp = tmp->next) {\r\nva = kzalloc(sizeof(struct vmap_area), GFP_NOWAIT);\r\nva->flags = VM_VM_AREA;\r\nva->va_start = (unsigned long)tmp->addr;\r\nva->va_end = va->va_start + tmp->size;\r\nva->vm = tmp;\r\n__insert_vmap_area(va);\r\n}\r\nvmap_area_pcpu_hole = VMALLOC_END;\r\nvmap_initialized = true;\r\n}\r\nint map_kernel_range_noflush(unsigned long addr, unsigned long size,\r\npgprot_t prot, struct page **pages)\r\n{\r\nreturn vmap_page_range_noflush(addr, addr + size, prot, pages);\r\n}\r\nvoid unmap_kernel_range_noflush(unsigned long addr, unsigned long size)\r\n{\r\nvunmap_page_range(addr, addr + size);\r\n}\r\nvoid unmap_kernel_range(unsigned long addr, unsigned long size)\r\n{\r\nunsigned long end = addr + size;\r\nflush_cache_vunmap(addr, end);\r\nvunmap_page_range(addr, end);\r\nflush_tlb_kernel_range(addr, end);\r\n}\r\nint map_vm_area(struct vm_struct *area, pgprot_t prot, struct page ***pages)\r\n{\r\nunsigned long addr = (unsigned long)area->addr;\r\nunsigned long end = addr + get_vm_area_size(area);\r\nint err;\r\nerr = vmap_page_range(addr, end, prot, *pages);\r\nif (err > 0) {\r\n*pages += err;\r\nerr = 0;\r\n}\r\nreturn err;\r\n}\r\nstatic void setup_vmalloc_vm(struct vm_struct *vm, struct vmap_area *va,\r\nunsigned long flags, const void *caller)\r\n{\r\nspin_lock(&vmap_area_lock);\r\nvm->flags = flags;\r\nvm->addr = (void *)va->va_start;\r\nvm->size = va->va_end - va->va_start;\r\nvm->caller = caller;\r\nva->vm = vm;\r\nva->flags |= VM_VM_AREA;\r\nspin_unlock(&vmap_area_lock);\r\n}\r\nstatic void clear_vm_uninitialized_flag(struct vm_struct *vm)\r\n{\r\nsmp_wmb();\r\nvm->flags &= ~VM_UNINITIALIZED;\r\n}\r\nstatic struct vm_struct *__get_vm_area_node(unsigned long size,\r\nunsigned long align, unsigned long flags, unsigned long start,\r\nunsigned long end, int node, gfp_t gfp_mask, const void *caller)\r\n{\r\nstruct vmap_area *va;\r\nstruct vm_struct *area;\r\nBUG_ON(in_interrupt());\r\nif (flags & VM_IOREMAP)\r\nalign = 1ul << clamp(fls(size), PAGE_SHIFT, IOREMAP_MAX_ORDER);\r\nsize = PAGE_ALIGN(size);\r\nif (unlikely(!size))\r\nreturn NULL;\r\narea = kzalloc_node(sizeof(*area), gfp_mask & GFP_RECLAIM_MASK, node);\r\nif (unlikely(!area))\r\nreturn NULL;\r\nsize += PAGE_SIZE;\r\nva = alloc_vmap_area(size, align, start, end, node, gfp_mask);\r\nif (IS_ERR(va)) {\r\nkfree(area);\r\nreturn NULL;\r\n}\r\nsetup_vmalloc_vm(area, va, flags, caller);\r\nreturn area;\r\n}\r\nstruct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,\r\nunsigned long start, unsigned long end)\r\n{\r\nreturn __get_vm_area_node(size, 1, flags, start, end, NUMA_NO_NODE,\r\nGFP_KERNEL, __builtin_return_address(0));\r\n}\r\nstruct vm_struct *__get_vm_area_caller(unsigned long size, unsigned long flags,\r\nunsigned long start, unsigned long end,\r\nconst void *caller)\r\n{\r\nreturn __get_vm_area_node(size, 1, flags, start, end, NUMA_NO_NODE,\r\nGFP_KERNEL, caller);\r\n}\r\nstruct vm_struct *get_vm_area(unsigned long size, unsigned long flags)\r\n{\r\nreturn __get_vm_area_node(size, 1, flags, VMALLOC_START, VMALLOC_END,\r\nNUMA_NO_NODE, GFP_KERNEL,\r\n__builtin_return_address(0));\r\n}\r\nstruct vm_struct *get_vm_area_caller(unsigned long size, unsigned long flags,\r\nconst void *caller)\r\n{\r\nreturn __get_vm_area_node(size, 1, flags, VMALLOC_START, VMALLOC_END,\r\nNUMA_NO_NODE, GFP_KERNEL, caller);\r\n}\r\nstruct vm_struct *find_vm_area(const void *addr)\r\n{\r\nstruct vmap_area *va;\r\nva = find_vmap_area((unsigned long)addr);\r\nif (va && va->flags & VM_VM_AREA)\r\nreturn va->vm;\r\nreturn NULL;\r\n}\r\nstruct vm_struct *remove_vm_area(const void *addr)\r\n{\r\nstruct vmap_area *va;\r\nva = find_vmap_area((unsigned long)addr);\r\nif (va && va->flags & VM_VM_AREA) {\r\nstruct vm_struct *vm = va->vm;\r\nspin_lock(&vmap_area_lock);\r\nva->vm = NULL;\r\nva->flags &= ~VM_VM_AREA;\r\nspin_unlock(&vmap_area_lock);\r\nvmap_debug_free_range(va->va_start, va->va_end);\r\nfree_unmap_vmap_area(va);\r\nvm->size -= PAGE_SIZE;\r\nreturn vm;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void __vunmap(const void *addr, int deallocate_pages)\r\n{\r\nstruct vm_struct *area;\r\nif (!addr)\r\nreturn;\r\nif (WARN(!PAGE_ALIGNED(addr), "Trying to vfree() bad address (%p)\n",\r\naddr))\r\nreturn;\r\narea = remove_vm_area(addr);\r\nif (unlikely(!area)) {\r\nWARN(1, KERN_ERR "Trying to vfree() nonexistent vm area (%p)\n",\r\naddr);\r\nreturn;\r\n}\r\ndebug_check_no_locks_freed(addr, area->size);\r\ndebug_check_no_obj_freed(addr, area->size);\r\nif (deallocate_pages) {\r\nint i;\r\nfor (i = 0; i < area->nr_pages; i++) {\r\nstruct page *page = area->pages[i];\r\nBUG_ON(!page);\r\n__free_page(page);\r\n}\r\nif (area->flags & VM_VPAGES)\r\nvfree(area->pages);\r\nelse\r\nkfree(area->pages);\r\n}\r\nkfree(area);\r\nreturn;\r\n}\r\nvoid vfree(const void *addr)\r\n{\r\nBUG_ON(in_nmi());\r\nkmemleak_free(addr);\r\nif (!addr)\r\nreturn;\r\nif (unlikely(in_interrupt())) {\r\nstruct vfree_deferred *p = &__get_cpu_var(vfree_deferred);\r\nif (llist_add((struct llist_node *)addr, &p->list))\r\nschedule_work(&p->wq);\r\n} else\r\n__vunmap(addr, 1);\r\n}\r\nvoid vunmap(const void *addr)\r\n{\r\nBUG_ON(in_interrupt());\r\nmight_sleep();\r\nif (addr)\r\n__vunmap(addr, 0);\r\n}\r\nvoid *vmap(struct page **pages, unsigned int count,\r\nunsigned long flags, pgprot_t prot)\r\n{\r\nstruct vm_struct *area;\r\nmight_sleep();\r\nif (count > totalram_pages)\r\nreturn NULL;\r\narea = get_vm_area_caller((count << PAGE_SHIFT), flags,\r\n__builtin_return_address(0));\r\nif (!area)\r\nreturn NULL;\r\nif (map_vm_area(area, prot, &pages)) {\r\nvunmap(area->addr);\r\nreturn NULL;\r\n}\r\nreturn area->addr;\r\n}\r\nstatic void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,\r\npgprot_t prot, int node)\r\n{\r\nconst int order = 0;\r\nstruct page **pages;\r\nunsigned int nr_pages, array_size, i;\r\ngfp_t nested_gfp = (gfp_mask & GFP_RECLAIM_MASK) | __GFP_ZERO;\r\nnr_pages = get_vm_area_size(area) >> PAGE_SHIFT;\r\narray_size = (nr_pages * sizeof(struct page *));\r\narea->nr_pages = nr_pages;\r\nif (array_size > PAGE_SIZE) {\r\npages = __vmalloc_node(array_size, 1, nested_gfp|__GFP_HIGHMEM,\r\nPAGE_KERNEL, node, area->caller);\r\narea->flags |= VM_VPAGES;\r\n} else {\r\npages = kmalloc_node(array_size, nested_gfp, node);\r\n}\r\narea->pages = pages;\r\nif (!area->pages) {\r\nremove_vm_area(area->addr);\r\nkfree(area);\r\nreturn NULL;\r\n}\r\nfor (i = 0; i < area->nr_pages; i++) {\r\nstruct page *page;\r\ngfp_t tmp_mask = gfp_mask | __GFP_NOWARN;\r\nif (node == NUMA_NO_NODE)\r\npage = alloc_page(tmp_mask);\r\nelse\r\npage = alloc_pages_node(node, tmp_mask, order);\r\nif (unlikely(!page)) {\r\narea->nr_pages = i;\r\ngoto fail;\r\n}\r\narea->pages[i] = page;\r\n}\r\nif (map_vm_area(area, prot, &pages))\r\ngoto fail;\r\nreturn area->addr;\r\nfail:\r\nwarn_alloc_failed(gfp_mask, order,\r\n"vmalloc: allocation failure, allocated %ld of %ld bytes\n",\r\n(area->nr_pages*PAGE_SIZE), area->size);\r\nvfree(area->addr);\r\nreturn NULL;\r\n}\r\nvoid *__vmalloc_node_range(unsigned long size, unsigned long align,\r\nunsigned long start, unsigned long end, gfp_t gfp_mask,\r\npgprot_t prot, int node, const void *caller)\r\n{\r\nstruct vm_struct *area;\r\nvoid *addr;\r\nunsigned long real_size = size;\r\nsize = PAGE_ALIGN(size);\r\nif (!size || (size >> PAGE_SHIFT) > totalram_pages)\r\ngoto fail;\r\narea = __get_vm_area_node(size, align, VM_ALLOC | VM_UNINITIALIZED,\r\nstart, end, node, gfp_mask, caller);\r\nif (!area)\r\ngoto fail;\r\naddr = __vmalloc_area_node(area, gfp_mask, prot, node);\r\nif (!addr)\r\nreturn NULL;\r\nclear_vm_uninitialized_flag(area);\r\nkmemleak_alloc(addr, real_size, 2, gfp_mask);\r\nreturn addr;\r\nfail:\r\nwarn_alloc_failed(gfp_mask, 0,\r\n"vmalloc: allocation failure: %lu bytes\n",\r\nreal_size);\r\nreturn NULL;\r\n}\r\nstatic void *__vmalloc_node(unsigned long size, unsigned long align,\r\ngfp_t gfp_mask, pgprot_t prot,\r\nint node, const void *caller)\r\n{\r\nreturn __vmalloc_node_range(size, align, VMALLOC_START, VMALLOC_END,\r\ngfp_mask, prot, node, caller);\r\n}\r\nvoid *__vmalloc(unsigned long size, gfp_t gfp_mask, pgprot_t prot)\r\n{\r\nreturn __vmalloc_node(size, 1, gfp_mask, prot, NUMA_NO_NODE,\r\n__builtin_return_address(0));\r\n}\r\nstatic inline void *__vmalloc_node_flags(unsigned long size,\r\nint node, gfp_t flags)\r\n{\r\nreturn __vmalloc_node(size, 1, flags, PAGE_KERNEL,\r\nnode, __builtin_return_address(0));\r\n}\r\nvoid *vmalloc(unsigned long size)\r\n{\r\nreturn __vmalloc_node_flags(size, NUMA_NO_NODE,\r\nGFP_KERNEL | __GFP_HIGHMEM);\r\n}\r\nvoid *vzalloc(unsigned long size)\r\n{\r\nreturn __vmalloc_node_flags(size, NUMA_NO_NODE,\r\nGFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO);\r\n}\r\nvoid *vmalloc_user(unsigned long size)\r\n{\r\nstruct vm_struct *area;\r\nvoid *ret;\r\nret = __vmalloc_node(size, SHMLBA,\r\nGFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO,\r\nPAGE_KERNEL, NUMA_NO_NODE,\r\n__builtin_return_address(0));\r\nif (ret) {\r\narea = find_vm_area(ret);\r\narea->flags |= VM_USERMAP;\r\n}\r\nreturn ret;\r\n}\r\nvoid *vmalloc_node(unsigned long size, int node)\r\n{\r\nreturn __vmalloc_node(size, 1, GFP_KERNEL | __GFP_HIGHMEM, PAGE_KERNEL,\r\nnode, __builtin_return_address(0));\r\n}\r\nvoid *vzalloc_node(unsigned long size, int node)\r\n{\r\nreturn __vmalloc_node_flags(size, node,\r\nGFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO);\r\n}\r\nvoid *vmalloc_exec(unsigned long size)\r\n{\r\nreturn __vmalloc_node(size, 1, GFP_KERNEL | __GFP_HIGHMEM, PAGE_KERNEL_EXEC,\r\nNUMA_NO_NODE, __builtin_return_address(0));\r\n}\r\nvoid *vmalloc_32(unsigned long size)\r\n{\r\nreturn __vmalloc_node(size, 1, GFP_VMALLOC32, PAGE_KERNEL,\r\nNUMA_NO_NODE, __builtin_return_address(0));\r\n}\r\nvoid *vmalloc_32_user(unsigned long size)\r\n{\r\nstruct vm_struct *area;\r\nvoid *ret;\r\nret = __vmalloc_node(size, 1, GFP_VMALLOC32 | __GFP_ZERO, PAGE_KERNEL,\r\nNUMA_NO_NODE, __builtin_return_address(0));\r\nif (ret) {\r\narea = find_vm_area(ret);\r\narea->flags |= VM_USERMAP;\r\n}\r\nreturn ret;\r\n}\r\nstatic int aligned_vread(char *buf, char *addr, unsigned long count)\r\n{\r\nstruct page *p;\r\nint copied = 0;\r\nwhile (count) {\r\nunsigned long offset, length;\r\noffset = (unsigned long)addr & ~PAGE_MASK;\r\nlength = PAGE_SIZE - offset;\r\nif (length > count)\r\nlength = count;\r\np = vmalloc_to_page(addr);\r\nif (p) {\r\nvoid *map = kmap_atomic(p);\r\nmemcpy(buf, map + offset, length);\r\nkunmap_atomic(map);\r\n} else\r\nmemset(buf, 0, length);\r\naddr += length;\r\nbuf += length;\r\ncopied += length;\r\ncount -= length;\r\n}\r\nreturn copied;\r\n}\r\nstatic int aligned_vwrite(char *buf, char *addr, unsigned long count)\r\n{\r\nstruct page *p;\r\nint copied = 0;\r\nwhile (count) {\r\nunsigned long offset, length;\r\noffset = (unsigned long)addr & ~PAGE_MASK;\r\nlength = PAGE_SIZE - offset;\r\nif (length > count)\r\nlength = count;\r\np = vmalloc_to_page(addr);\r\nif (p) {\r\nvoid *map = kmap_atomic(p);\r\nmemcpy(map + offset, buf, length);\r\nkunmap_atomic(map);\r\n}\r\naddr += length;\r\nbuf += length;\r\ncopied += length;\r\ncount -= length;\r\n}\r\nreturn copied;\r\n}\r\nlong vread(char *buf, char *addr, unsigned long count)\r\n{\r\nstruct vmap_area *va;\r\nstruct vm_struct *vm;\r\nchar *vaddr, *buf_start = buf;\r\nunsigned long buflen = count;\r\nunsigned long n;\r\nif ((unsigned long) addr + count < count)\r\ncount = -(unsigned long) addr;\r\nspin_lock(&vmap_area_lock);\r\nlist_for_each_entry(va, &vmap_area_list, list) {\r\nif (!count)\r\nbreak;\r\nif (!(va->flags & VM_VM_AREA))\r\ncontinue;\r\nvm = va->vm;\r\nvaddr = (char *) vm->addr;\r\nif (addr >= vaddr + get_vm_area_size(vm))\r\ncontinue;\r\nwhile (addr < vaddr) {\r\nif (count == 0)\r\ngoto finished;\r\n*buf = '\0';\r\nbuf++;\r\naddr++;\r\ncount--;\r\n}\r\nn = vaddr + get_vm_area_size(vm) - addr;\r\nif (n > count)\r\nn = count;\r\nif (!(vm->flags & VM_IOREMAP))\r\naligned_vread(buf, addr, n);\r\nelse\r\nmemset(buf, 0, n);\r\nbuf += n;\r\naddr += n;\r\ncount -= n;\r\n}\r\nfinished:\r\nspin_unlock(&vmap_area_lock);\r\nif (buf == buf_start)\r\nreturn 0;\r\nif (buf != buf_start + buflen)\r\nmemset(buf, 0, buflen - (buf - buf_start));\r\nreturn buflen;\r\n}\r\nlong vwrite(char *buf, char *addr, unsigned long count)\r\n{\r\nstruct vmap_area *va;\r\nstruct vm_struct *vm;\r\nchar *vaddr;\r\nunsigned long n, buflen;\r\nint copied = 0;\r\nif ((unsigned long) addr + count < count)\r\ncount = -(unsigned long) addr;\r\nbuflen = count;\r\nspin_lock(&vmap_area_lock);\r\nlist_for_each_entry(va, &vmap_area_list, list) {\r\nif (!count)\r\nbreak;\r\nif (!(va->flags & VM_VM_AREA))\r\ncontinue;\r\nvm = va->vm;\r\nvaddr = (char *) vm->addr;\r\nif (addr >= vaddr + get_vm_area_size(vm))\r\ncontinue;\r\nwhile (addr < vaddr) {\r\nif (count == 0)\r\ngoto finished;\r\nbuf++;\r\naddr++;\r\ncount--;\r\n}\r\nn = vaddr + get_vm_area_size(vm) - addr;\r\nif (n > count)\r\nn = count;\r\nif (!(vm->flags & VM_IOREMAP)) {\r\naligned_vwrite(buf, addr, n);\r\ncopied++;\r\n}\r\nbuf += n;\r\naddr += n;\r\ncount -= n;\r\n}\r\nfinished:\r\nspin_unlock(&vmap_area_lock);\r\nif (!copied)\r\nreturn 0;\r\nreturn buflen;\r\n}\r\nint remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,\r\nvoid *kaddr, unsigned long size)\r\n{\r\nstruct vm_struct *area;\r\nsize = PAGE_ALIGN(size);\r\nif (!PAGE_ALIGNED(uaddr) || !PAGE_ALIGNED(kaddr))\r\nreturn -EINVAL;\r\narea = find_vm_area(kaddr);\r\nif (!area)\r\nreturn -EINVAL;\r\nif (!(area->flags & VM_USERMAP))\r\nreturn -EINVAL;\r\nif (kaddr + size > area->addr + area->size)\r\nreturn -EINVAL;\r\ndo {\r\nstruct page *page = vmalloc_to_page(kaddr);\r\nint ret;\r\nret = vm_insert_page(vma, uaddr, page);\r\nif (ret)\r\nreturn ret;\r\nuaddr += PAGE_SIZE;\r\nkaddr += PAGE_SIZE;\r\nsize -= PAGE_SIZE;\r\n} while (size > 0);\r\nvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\r\nreturn 0;\r\n}\r\nint remap_vmalloc_range(struct vm_area_struct *vma, void *addr,\r\nunsigned long pgoff)\r\n{\r\nreturn remap_vmalloc_range_partial(vma, vma->vm_start,\r\naddr + (pgoff << PAGE_SHIFT),\r\nvma->vm_end - vma->vm_start);\r\n}\r\nstatic int f(pte_t *pte, pgtable_t table, unsigned long addr, void *data)\r\n{\r\npte_t ***p = data;\r\nif (p) {\r\n*(*p) = pte;\r\n(*p)++;\r\n}\r\nreturn 0;\r\n}\r\nstruct vm_struct *alloc_vm_area(size_t size, pte_t **ptes)\r\n{\r\nstruct vm_struct *area;\r\narea = get_vm_area_caller(size, VM_IOREMAP,\r\n__builtin_return_address(0));\r\nif (area == NULL)\r\nreturn NULL;\r\nif (apply_to_page_range(&init_mm, (unsigned long)area->addr,\r\nsize, f, ptes ? &ptes : NULL)) {\r\nfree_vm_area(area);\r\nreturn NULL;\r\n}\r\nreturn area;\r\n}\r\nvoid free_vm_area(struct vm_struct *area)\r\n{\r\nstruct vm_struct *ret;\r\nret = remove_vm_area(area->addr);\r\nBUG_ON(ret != area);\r\nkfree(area);\r\n}\r\nstatic struct vmap_area *node_to_va(struct rb_node *n)\r\n{\r\nreturn n ? rb_entry(n, struct vmap_area, rb_node) : NULL;\r\n}\r\nstatic bool pvm_find_next_prev(unsigned long end,\r\nstruct vmap_area **pnext,\r\nstruct vmap_area **pprev)\r\n{\r\nstruct rb_node *n = vmap_area_root.rb_node;\r\nstruct vmap_area *va = NULL;\r\nwhile (n) {\r\nva = rb_entry(n, struct vmap_area, rb_node);\r\nif (end < va->va_end)\r\nn = n->rb_left;\r\nelse if (end > va->va_end)\r\nn = n->rb_right;\r\nelse\r\nbreak;\r\n}\r\nif (!va)\r\nreturn false;\r\nif (va->va_end > end) {\r\n*pnext = va;\r\n*pprev = node_to_va(rb_prev(&(*pnext)->rb_node));\r\n} else {\r\n*pprev = va;\r\n*pnext = node_to_va(rb_next(&(*pprev)->rb_node));\r\n}\r\nreturn true;\r\n}\r\nstatic unsigned long pvm_determine_end(struct vmap_area **pnext,\r\nstruct vmap_area **pprev,\r\nunsigned long align)\r\n{\r\nconst unsigned long vmalloc_end = VMALLOC_END & ~(align - 1);\r\nunsigned long addr;\r\nif (*pnext)\r\naddr = min((*pnext)->va_start & ~(align - 1), vmalloc_end);\r\nelse\r\naddr = vmalloc_end;\r\nwhile (*pprev && (*pprev)->va_end > addr) {\r\n*pnext = *pprev;\r\n*pprev = node_to_va(rb_prev(&(*pnext)->rb_node));\r\n}\r\nreturn addr;\r\n}\r\nstruct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,\r\nconst size_t *sizes, int nr_vms,\r\nsize_t align)\r\n{\r\nconst unsigned long vmalloc_start = ALIGN(VMALLOC_START, align);\r\nconst unsigned long vmalloc_end = VMALLOC_END & ~(align - 1);\r\nstruct vmap_area **vas, *prev, *next;\r\nstruct vm_struct **vms;\r\nint area, area2, last_area, term_area;\r\nunsigned long base, start, end, last_end;\r\nbool purged = false;\r\nBUG_ON(align & ~PAGE_MASK || !is_power_of_2(align));\r\nfor (last_area = 0, area = 0; area < nr_vms; area++) {\r\nstart = offsets[area];\r\nend = start + sizes[area];\r\nBUG_ON(!IS_ALIGNED(offsets[area], align));\r\nBUG_ON(!IS_ALIGNED(sizes[area], align));\r\nif (start > offsets[last_area])\r\nlast_area = area;\r\nfor (area2 = 0; area2 < nr_vms; area2++) {\r\nunsigned long start2 = offsets[area2];\r\nunsigned long end2 = start2 + sizes[area2];\r\nif (area2 == area)\r\ncontinue;\r\nBUG_ON(start2 >= start && start2 < end);\r\nBUG_ON(end2 <= end && end2 > start);\r\n}\r\n}\r\nlast_end = offsets[last_area] + sizes[last_area];\r\nif (vmalloc_end - vmalloc_start < last_end) {\r\nWARN_ON(true);\r\nreturn NULL;\r\n}\r\nvms = kcalloc(nr_vms, sizeof(vms[0]), GFP_KERNEL);\r\nvas = kcalloc(nr_vms, sizeof(vas[0]), GFP_KERNEL);\r\nif (!vas || !vms)\r\ngoto err_free2;\r\nfor (area = 0; area < nr_vms; area++) {\r\nvas[area] = kzalloc(sizeof(struct vmap_area), GFP_KERNEL);\r\nvms[area] = kzalloc(sizeof(struct vm_struct), GFP_KERNEL);\r\nif (!vas[area] || !vms[area])\r\ngoto err_free;\r\n}\r\nretry:\r\nspin_lock(&vmap_area_lock);\r\narea = term_area = last_area;\r\nstart = offsets[area];\r\nend = start + sizes[area];\r\nif (!pvm_find_next_prev(vmap_area_pcpu_hole, &next, &prev)) {\r\nbase = vmalloc_end - last_end;\r\ngoto found;\r\n}\r\nbase = pvm_determine_end(&next, &prev, align) - end;\r\nwhile (true) {\r\nBUG_ON(next && next->va_end <= base + end);\r\nBUG_ON(prev && prev->va_end > base + end);\r\nif (base + last_end < vmalloc_start + last_end) {\r\nspin_unlock(&vmap_area_lock);\r\nif (!purged) {\r\npurge_vmap_area_lazy();\r\npurged = true;\r\ngoto retry;\r\n}\r\ngoto err_free;\r\n}\r\nif (next && next->va_start < base + end) {\r\nbase = pvm_determine_end(&next, &prev, align) - end;\r\nterm_area = area;\r\ncontinue;\r\n}\r\nif (prev && prev->va_end > base + start) {\r\nnext = prev;\r\nprev = node_to_va(rb_prev(&next->rb_node));\r\nbase = pvm_determine_end(&next, &prev, align) - end;\r\nterm_area = area;\r\ncontinue;\r\n}\r\narea = (area + nr_vms - 1) % nr_vms;\r\nif (area == term_area)\r\nbreak;\r\nstart = offsets[area];\r\nend = start + sizes[area];\r\npvm_find_next_prev(base + end, &next, &prev);\r\n}\r\nfound:\r\nfor (area = 0; area < nr_vms; area++) {\r\nstruct vmap_area *va = vas[area];\r\nva->va_start = base + offsets[area];\r\nva->va_end = va->va_start + sizes[area];\r\n__insert_vmap_area(va);\r\n}\r\nvmap_area_pcpu_hole = base + offsets[last_area];\r\nspin_unlock(&vmap_area_lock);\r\nfor (area = 0; area < nr_vms; area++)\r\nsetup_vmalloc_vm(vms[area], vas[area], VM_ALLOC,\r\npcpu_get_vm_areas);\r\nkfree(vas);\r\nreturn vms;\r\nerr_free:\r\nfor (area = 0; area < nr_vms; area++) {\r\nkfree(vas[area]);\r\nkfree(vms[area]);\r\n}\r\nerr_free2:\r\nkfree(vas);\r\nkfree(vms);\r\nreturn NULL;\r\n}\r\nvoid pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)\r\n{\r\nint i;\r\nfor (i = 0; i < nr_vms; i++)\r\nfree_vm_area(vms[i]);\r\nkfree(vms);\r\n}\r\nstatic void *s_start(struct seq_file *m, loff_t *pos)\r\n__acquires(&vmap_area_lock\r\nstatic void *s_next(struct seq_file *m, void *p, loff_t *pos)\r\n{\r\nstruct vmap_area *va = p, *next;\r\n++*pos;\r\nnext = list_entry(va->list.next, typeof(*va), list);\r\nif (&next->list != &vmap_area_list)\r\nreturn next;\r\nreturn NULL;\r\n}\r\nstatic void s_stop(struct seq_file *m, void *p)\r\n__releases(&vmap_area_lock\r\nstatic void show_numa_info(struct seq_file *m, struct vm_struct *v)\r\n{\r\nif (IS_ENABLED(CONFIG_NUMA)) {\r\nunsigned int nr, *counters = m->private;\r\nif (!counters)\r\nreturn;\r\nsmp_rmb();\r\nif (v->flags & VM_UNINITIALIZED)\r\nreturn;\r\nmemset(counters, 0, nr_node_ids * sizeof(unsigned int));\r\nfor (nr = 0; nr < v->nr_pages; nr++)\r\ncounters[page_to_nid(v->pages[nr])]++;\r\nfor_each_node_state(nr, N_HIGH_MEMORY)\r\nif (counters[nr])\r\nseq_printf(m, " N%u=%u", nr, counters[nr]);\r\n}\r\n}\r\nstatic int s_show(struct seq_file *m, void *p)\r\n{\r\nstruct vmap_area *va = p;\r\nstruct vm_struct *v;\r\nif (!(va->flags & VM_VM_AREA))\r\nreturn 0;\r\nv = va->vm;\r\nseq_printf(m, "0x%pK-0x%pK %7ld",\r\nv->addr, v->addr + v->size, v->size);\r\nif (v->caller)\r\nseq_printf(m, " %pS", v->caller);\r\nif (v->nr_pages)\r\nseq_printf(m, " pages=%d", v->nr_pages);\r\nif (v->phys_addr)\r\nseq_printf(m, " phys=%llx", (unsigned long long)v->phys_addr);\r\nif (v->flags & VM_IOREMAP)\r\nseq_printf(m, " ioremap");\r\nif (v->flags & VM_ALLOC)\r\nseq_printf(m, " vmalloc");\r\nif (v->flags & VM_MAP)\r\nseq_printf(m, " vmap");\r\nif (v->flags & VM_USERMAP)\r\nseq_printf(m, " user");\r\nif (v->flags & VM_VPAGES)\r\nseq_printf(m, " vpages");\r\nshow_numa_info(m, v);\r\nseq_putc(m, '\n');\r\nreturn 0;\r\n}\r\nstatic int vmalloc_open(struct inode *inode, struct file *file)\r\n{\r\nunsigned int *ptr = NULL;\r\nint ret;\r\nif (IS_ENABLED(CONFIG_NUMA)) {\r\nptr = kmalloc(nr_node_ids * sizeof(unsigned int), GFP_KERNEL);\r\nif (ptr == NULL)\r\nreturn -ENOMEM;\r\n}\r\nret = seq_open(file, &vmalloc_op);\r\nif (!ret) {\r\nstruct seq_file *m = file->private_data;\r\nm->private = ptr;\r\n} else\r\nkfree(ptr);\r\nreturn ret;\r\n}\r\nstatic int __init proc_vmalloc_init(void)\r\n{\r\nproc_create("vmallocinfo", S_IRUSR, NULL, &proc_vmalloc_operations);\r\nreturn 0;\r\n}\r\nvoid get_vmalloc_info(struct vmalloc_info *vmi)\r\n{\r\nstruct vmap_area *va;\r\nunsigned long free_area_size;\r\nunsigned long prev_end;\r\nvmi->used = 0;\r\nvmi->largest_chunk = 0;\r\nprev_end = VMALLOC_START;\r\nspin_lock(&vmap_area_lock);\r\nif (list_empty(&vmap_area_list)) {\r\nvmi->largest_chunk = VMALLOC_TOTAL;\r\ngoto out;\r\n}\r\nlist_for_each_entry(va, &vmap_area_list, list) {\r\nunsigned long addr = va->va_start;\r\nif (addr < VMALLOC_START)\r\ncontinue;\r\nif (addr >= VMALLOC_END)\r\nbreak;\r\nif (va->flags & (VM_LAZY_FREE | VM_LAZY_FREEING))\r\ncontinue;\r\nvmi->used += (va->va_end - va->va_start);\r\nfree_area_size = addr - prev_end;\r\nif (vmi->largest_chunk < free_area_size)\r\nvmi->largest_chunk = free_area_size;\r\nprev_end = va->va_end;\r\n}\r\nif (VMALLOC_END - prev_end > vmi->largest_chunk)\r\nvmi->largest_chunk = VMALLOC_END - prev_end;\r\nout:\r\nspin_unlock(&vmap_area_lock);\r\n}
