static int __init early_cma(char *p)\r\n{\r\npr_debug("%s(%s)\n", __func__, p);\r\nsize_cmdline = memparse(p, &p);\r\nreturn 0;\r\n}\r\nstatic phys_addr_t __init __maybe_unused cma_early_percent_memory(void)\r\n{\r\nstruct memblock_region *reg;\r\nunsigned long total_pages = 0;\r\nfor_each_memblock(memory, reg)\r\ntotal_pages += memblock_region_memory_end_pfn(reg) -\r\nmemblock_region_memory_base_pfn(reg);\r\nreturn (total_pages * CONFIG_CMA_SIZE_PERCENTAGE / 100) << PAGE_SHIFT;\r\n}\r\nstatic inline __maybe_unused phys_addr_t cma_early_percent_memory(void)\r\n{\r\nreturn 0;\r\n}\r\nvoid __init dma_contiguous_reserve(phys_addr_t limit)\r\n{\r\nphys_addr_t selected_size = 0;\r\npr_debug("%s(limit %08lx)\n", __func__, (unsigned long)limit);\r\nif (size_cmdline != -1) {\r\nselected_size = size_cmdline;\r\n} else {\r\n#ifdef CONFIG_CMA_SIZE_SEL_MBYTES\r\nselected_size = size_bytes;\r\n#elif defined(CONFIG_CMA_SIZE_SEL_PERCENTAGE)\r\nselected_size = cma_early_percent_memory();\r\n#elif defined(CONFIG_CMA_SIZE_SEL_MIN)\r\nselected_size = min(size_bytes, cma_early_percent_memory());\r\n#elif defined(CONFIG_CMA_SIZE_SEL_MAX)\r\nselected_size = max(size_bytes, cma_early_percent_memory());\r\n#endif\r\n}\r\nif (selected_size && !dma_contiguous_default_area) {\r\npr_debug("%s: reserving %ld MiB for global area\n", __func__,\r\n(unsigned long)selected_size / SZ_1M);\r\ndma_contiguous_reserve_area(selected_size, 0, limit,\r\n&dma_contiguous_default_area);\r\n}\r\n}\r\nstatic int __init cma_activate_area(struct cma *cma)\r\n{\r\nint bitmap_size = BITS_TO_LONGS(cma->count) * sizeof(long);\r\nunsigned long base_pfn = cma->base_pfn, pfn = base_pfn;\r\nunsigned i = cma->count >> pageblock_order;\r\nstruct zone *zone;\r\ncma->bitmap = kzalloc(bitmap_size, GFP_KERNEL);\r\nif (!cma->bitmap)\r\nreturn -ENOMEM;\r\nWARN_ON_ONCE(!pfn_valid(pfn));\r\nzone = page_zone(pfn_to_page(pfn));\r\ndo {\r\nunsigned j;\r\nbase_pfn = pfn;\r\nfor (j = pageblock_nr_pages; j; --j, pfn++) {\r\nWARN_ON_ONCE(!pfn_valid(pfn));\r\nif (page_zone(pfn_to_page(pfn)) != zone)\r\nreturn -EINVAL;\r\n}\r\ninit_cma_reserved_pageblock(pfn_to_page(base_pfn));\r\n} while (--i);\r\nreturn 0;\r\n}\r\nstatic int __init cma_init_reserved_areas(void)\r\n{\r\nint i;\r\nfor (i = 0; i < cma_area_count; i++) {\r\nint ret = cma_activate_area(&cma_areas[i]);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint __init dma_contiguous_reserve_area(phys_addr_t size, phys_addr_t base,\r\nphys_addr_t limit, struct cma **res_cma)\r\n{\r\nstruct cma *cma = &cma_areas[cma_area_count];\r\nphys_addr_t alignment;\r\nint ret = 0;\r\npr_debug("%s(size %lx, base %08lx, limit %08lx)\n", __func__,\r\n(unsigned long)size, (unsigned long)base,\r\n(unsigned long)limit);\r\nif (cma_area_count == ARRAY_SIZE(cma_areas)) {\r\npr_err("Not enough slots for CMA reserved regions!\n");\r\nreturn -ENOSPC;\r\n}\r\nif (!size)\r\nreturn -EINVAL;\r\nalignment = PAGE_SIZE << max(MAX_ORDER - 1, pageblock_order);\r\nbase = ALIGN(base, alignment);\r\nsize = ALIGN(size, alignment);\r\nlimit &= ~(alignment - 1);\r\nif (base) {\r\nif (memblock_is_region_reserved(base, size) ||\r\nmemblock_reserve(base, size) < 0) {\r\nret = -EBUSY;\r\ngoto err;\r\n}\r\n} else {\r\nphys_addr_t addr = __memblock_alloc_base(size, alignment, limit);\r\nif (!addr) {\r\nret = -ENOMEM;\r\ngoto err;\r\n} else {\r\nbase = addr;\r\n}\r\n}\r\ncma->base_pfn = PFN_DOWN(base);\r\ncma->count = size >> PAGE_SHIFT;\r\n*res_cma = cma;\r\ncma_area_count++;\r\npr_info("CMA: reserved %ld MiB at %08lx\n", (unsigned long)size / SZ_1M,\r\n(unsigned long)base);\r\ndma_contiguous_early_fixup(base, size);\r\nreturn 0;\r\nerr:\r\npr_err("CMA: failed to reserve %ld MiB\n", (unsigned long)size / SZ_1M);\r\nreturn ret;\r\n}\r\nstruct page *dma_alloc_from_contiguous(struct device *dev, int count,\r\nunsigned int align)\r\n{\r\nunsigned long mask, pfn, pageno, start = 0;\r\nstruct cma *cma = dev_get_cma_area(dev);\r\nstruct page *page = NULL;\r\nint ret;\r\nif (!cma || !cma->count)\r\nreturn NULL;\r\nif (align > CONFIG_CMA_ALIGNMENT)\r\nalign = CONFIG_CMA_ALIGNMENT;\r\npr_debug("%s(cma %p, count %d, align %d)\n", __func__, (void *)cma,\r\ncount, align);\r\nif (!count)\r\nreturn NULL;\r\nmask = (1 << align) - 1;\r\nmutex_lock(&cma_mutex);\r\nfor (;;) {\r\npageno = bitmap_find_next_zero_area(cma->bitmap, cma->count,\r\nstart, count, mask);\r\nif (pageno >= cma->count)\r\nbreak;\r\npfn = cma->base_pfn + pageno;\r\nret = alloc_contig_range(pfn, pfn + count, MIGRATE_CMA);\r\nif (ret == 0) {\r\nbitmap_set(cma->bitmap, pageno, count);\r\npage = pfn_to_page(pfn);\r\nbreak;\r\n} else if (ret != -EBUSY) {\r\nbreak;\r\n}\r\npr_debug("%s(): memory range at %p is busy, retrying\n",\r\n__func__, pfn_to_page(pfn));\r\nstart = pageno + mask + 1;\r\n}\r\nmutex_unlock(&cma_mutex);\r\npr_debug("%s(): returned %p\n", __func__, page);\r\nreturn page;\r\n}\r\nbool dma_release_from_contiguous(struct device *dev, struct page *pages,\r\nint count)\r\n{\r\nstruct cma *cma = dev_get_cma_area(dev);\r\nunsigned long pfn;\r\nif (!cma || !pages)\r\nreturn false;\r\npr_debug("%s(page %p)\n", __func__, (void *)pages);\r\npfn = page_to_pfn(pages);\r\nif (pfn < cma->base_pfn || pfn >= cma->base_pfn + cma->count)\r\nreturn false;\r\nVM_BUG_ON(pfn + count > cma->base_pfn + cma->count);\r\nmutex_lock(&cma_mutex);\r\nbitmap_clear(cma->bitmap, pfn - cma->base_pfn, count);\r\nfree_contig_range(pfn, count);\r\nmutex_unlock(&cma_mutex);\r\nreturn true;\r\n}
