void ehea_dump(void *adr, int len, char *msg)\r\n{\r\nint x;\r\nunsigned char *deb = adr;\r\nfor (x = 0; x < len; x += 16) {\r\npr_info("%s adr=%p ofs=%04x %016llx %016llx\n",\r\nmsg, deb, x, *((u64 *)&deb[0]), *((u64 *)&deb[8]));\r\ndeb += 16;\r\n}\r\n}\r\nstatic void ehea_schedule_port_reset(struct ehea_port *port)\r\n{\r\nif (!test_bit(__EHEA_DISABLE_PORT_RESET, &port->flags))\r\nschedule_work(&port->reset_task);\r\n}\r\nstatic void ehea_update_firmware_handles(void)\r\n{\r\nstruct ehea_fw_handle_entry *arr = NULL;\r\nstruct ehea_adapter *adapter;\r\nint num_adapters = 0;\r\nint num_ports = 0;\r\nint num_portres = 0;\r\nint i = 0;\r\nint num_fw_handles, k, l;\r\nmutex_lock(&ehea_fw_handles.lock);\r\nlist_for_each_entry(adapter, &adapter_list, list) {\r\nnum_adapters++;\r\nfor (k = 0; k < EHEA_MAX_PORTS; k++) {\r\nstruct ehea_port *port = adapter->port[k];\r\nif (!port || (port->state != EHEA_PORT_UP))\r\ncontinue;\r\nnum_ports++;\r\nnum_portres += port->num_def_qps;\r\n}\r\n}\r\nnum_fw_handles = num_adapters * EHEA_NUM_ADAPTER_FW_HANDLES +\r\nnum_ports * EHEA_NUM_PORT_FW_HANDLES +\r\nnum_portres * EHEA_NUM_PORTRES_FW_HANDLES;\r\nif (num_fw_handles) {\r\narr = kcalloc(num_fw_handles, sizeof(*arr), GFP_KERNEL);\r\nif (!arr)\r\ngoto out;\r\n} else\r\ngoto out_update;\r\nlist_for_each_entry(adapter, &adapter_list, list) {\r\nif (num_adapters == 0)\r\nbreak;\r\nfor (k = 0; k < EHEA_MAX_PORTS; k++) {\r\nstruct ehea_port *port = adapter->port[k];\r\nif (!port || (port->state != EHEA_PORT_UP) ||\r\n(num_ports == 0))\r\ncontinue;\r\nfor (l = 0; l < port->num_def_qps; l++) {\r\nstruct ehea_port_res *pr = &port->port_res[l];\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = pr->qp->fw_handle;\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = pr->send_cq->fw_handle;\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = pr->recv_cq->fw_handle;\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = pr->eq->fw_handle;\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = pr->send_mr.handle;\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = pr->recv_mr.handle;\r\n}\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = port->qp_eq->fw_handle;\r\nnum_ports--;\r\n}\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = adapter->neq->fw_handle;\r\nif (adapter->mr.handle) {\r\narr[i].adh = adapter->handle;\r\narr[i++].fwh = adapter->mr.handle;\r\n}\r\nnum_adapters--;\r\n}\r\nout_update:\r\nkfree(ehea_fw_handles.arr);\r\nehea_fw_handles.arr = arr;\r\nehea_fw_handles.num_entries = i;\r\nout:\r\nmutex_unlock(&ehea_fw_handles.lock);\r\n}\r\nstatic void ehea_update_bcmc_registrations(void)\r\n{\r\nunsigned long flags;\r\nstruct ehea_bcmc_reg_entry *arr = NULL;\r\nstruct ehea_adapter *adapter;\r\nstruct ehea_mc_list *mc_entry;\r\nint num_registrations = 0;\r\nint i = 0;\r\nint k;\r\nspin_lock_irqsave(&ehea_bcmc_regs.lock, flags);\r\nlist_for_each_entry(adapter, &adapter_list, list)\r\nfor (k = 0; k < EHEA_MAX_PORTS; k++) {\r\nstruct ehea_port *port = adapter->port[k];\r\nif (!port || (port->state != EHEA_PORT_UP))\r\ncontinue;\r\nnum_registrations += 2;\r\nlist_for_each_entry(mc_entry, &port->mc_list->list,list)\r\nnum_registrations += 2;\r\n}\r\nif (num_registrations) {\r\narr = kcalloc(num_registrations, sizeof(*arr), GFP_ATOMIC);\r\nif (!arr)\r\ngoto out;\r\n} else\r\ngoto out_update;\r\nlist_for_each_entry(adapter, &adapter_list, list) {\r\nfor (k = 0; k < EHEA_MAX_PORTS; k++) {\r\nstruct ehea_port *port = adapter->port[k];\r\nif (!port || (port->state != EHEA_PORT_UP))\r\ncontinue;\r\nif (num_registrations == 0)\r\ngoto out_update;\r\narr[i].adh = adapter->handle;\r\narr[i].port_id = port->logical_port_id;\r\narr[i].reg_type = EHEA_BCMC_BROADCAST |\r\nEHEA_BCMC_UNTAGGED;\r\narr[i++].macaddr = port->mac_addr;\r\narr[i].adh = adapter->handle;\r\narr[i].port_id = port->logical_port_id;\r\narr[i].reg_type = EHEA_BCMC_BROADCAST |\r\nEHEA_BCMC_VLANID_ALL;\r\narr[i++].macaddr = port->mac_addr;\r\nnum_registrations -= 2;\r\nlist_for_each_entry(mc_entry,\r\n&port->mc_list->list, list) {\r\nif (num_registrations == 0)\r\ngoto out_update;\r\narr[i].adh = adapter->handle;\r\narr[i].port_id = port->logical_port_id;\r\narr[i].reg_type = EHEA_BCMC_MULTICAST |\r\nEHEA_BCMC_UNTAGGED;\r\nif (mc_entry->macaddr == 0)\r\narr[i].reg_type |= EHEA_BCMC_SCOPE_ALL;\r\narr[i++].macaddr = mc_entry->macaddr;\r\narr[i].adh = adapter->handle;\r\narr[i].port_id = port->logical_port_id;\r\narr[i].reg_type = EHEA_BCMC_MULTICAST |\r\nEHEA_BCMC_VLANID_ALL;\r\nif (mc_entry->macaddr == 0)\r\narr[i].reg_type |= EHEA_BCMC_SCOPE_ALL;\r\narr[i++].macaddr = mc_entry->macaddr;\r\nnum_registrations -= 2;\r\n}\r\n}\r\n}\r\nout_update:\r\nkfree(ehea_bcmc_regs.arr);\r\nehea_bcmc_regs.arr = arr;\r\nehea_bcmc_regs.num_entries = i;\r\nout:\r\nspin_unlock_irqrestore(&ehea_bcmc_regs.lock, flags);\r\n}\r\nstatic struct rtnl_link_stats64 *ehea_get_stats64(struct net_device *dev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nu64 rx_packets = 0, tx_packets = 0, rx_bytes = 0, tx_bytes = 0;\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\nrx_packets += port->port_res[i].rx_packets;\r\nrx_bytes += port->port_res[i].rx_bytes;\r\n}\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\ntx_packets += port->port_res[i].tx_packets;\r\ntx_bytes += port->port_res[i].tx_bytes;\r\n}\r\nstats->tx_packets = tx_packets;\r\nstats->rx_bytes = rx_bytes;\r\nstats->tx_bytes = tx_bytes;\r\nstats->rx_packets = rx_packets;\r\nstats->multicast = port->stats.multicast;\r\nstats->rx_errors = port->stats.rx_errors;\r\nreturn stats;\r\n}\r\nstatic void ehea_update_stats(struct work_struct *work)\r\n{\r\nstruct ehea_port *port =\r\ncontainer_of(work, struct ehea_port, stats_work.work);\r\nstruct net_device *dev = port->netdev;\r\nstruct rtnl_link_stats64 *stats = &port->stats;\r\nstruct hcp_ehea_port_cb2 *cb2;\r\nu64 hret;\r\ncb2 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb2) {\r\nnetdev_err(dev, "No mem for cb2. Some interface statistics were not updated\n");\r\ngoto resched;\r\n}\r\nhret = ehea_h_query_ehea_port(port->adapter->handle,\r\nport->logical_port_id,\r\nH_PORT_CB2, H_PORT_CB2_ALL, cb2);\r\nif (hret != H_SUCCESS) {\r\nnetdev_err(dev, "query_ehea_port failed\n");\r\ngoto out_herr;\r\n}\r\nif (netif_msg_hw(port))\r\nehea_dump(cb2, sizeof(*cb2), "net_device_stats");\r\nstats->multicast = cb2->rxmcp;\r\nstats->rx_errors = cb2->rxuerr;\r\nout_herr:\r\nfree_page((unsigned long)cb2);\r\nresched:\r\nschedule_delayed_work(&port->stats_work,\r\nround_jiffies_relative(msecs_to_jiffies(1000)));\r\n}\r\nstatic void ehea_refill_rq1(struct ehea_port_res *pr, int index, int nr_of_wqes)\r\n{\r\nstruct sk_buff **skb_arr_rq1 = pr->rq1_skba.arr;\r\nstruct net_device *dev = pr->port->netdev;\r\nint max_index_mask = pr->rq1_skba.len - 1;\r\nint fill_wqes = pr->rq1_skba.os_skbs + nr_of_wqes;\r\nint adder = 0;\r\nint i;\r\npr->rq1_skba.os_skbs = 0;\r\nif (unlikely(test_bit(__EHEA_STOP_XFER, &ehea_driver_flags))) {\r\nif (nr_of_wqes > 0)\r\npr->rq1_skba.index = index;\r\npr->rq1_skba.os_skbs = fill_wqes;\r\nreturn;\r\n}\r\nfor (i = 0; i < fill_wqes; i++) {\r\nif (!skb_arr_rq1[index]) {\r\nskb_arr_rq1[index] = netdev_alloc_skb(dev,\r\nEHEA_L_PKT_SIZE);\r\nif (!skb_arr_rq1[index]) {\r\npr->rq1_skba.os_skbs = fill_wqes - i;\r\nbreak;\r\n}\r\n}\r\nindex--;\r\nindex &= max_index_mask;\r\nadder++;\r\n}\r\nif (adder == 0)\r\nreturn;\r\nehea_update_rq1a(pr->qp, adder);\r\n}\r\nstatic void ehea_init_fill_rq1(struct ehea_port_res *pr, int nr_rq1a)\r\n{\r\nstruct sk_buff **skb_arr_rq1 = pr->rq1_skba.arr;\r\nstruct net_device *dev = pr->port->netdev;\r\nint i;\r\nif (nr_rq1a > pr->rq1_skba.len) {\r\nnetdev_err(dev, "NR_RQ1A bigger than skb array len\n");\r\nreturn;\r\n}\r\nfor (i = 0; i < nr_rq1a; i++) {\r\nskb_arr_rq1[i] = netdev_alloc_skb(dev, EHEA_L_PKT_SIZE);\r\nif (!skb_arr_rq1[i])\r\nbreak;\r\n}\r\nehea_update_rq1a(pr->qp, i - 1);\r\n}\r\nstatic int ehea_refill_rq_def(struct ehea_port_res *pr,\r\nstruct ehea_q_skb_arr *q_skba, int rq_nr,\r\nint num_wqes, int wqe_type, int packet_size)\r\n{\r\nstruct net_device *dev = pr->port->netdev;\r\nstruct ehea_qp *qp = pr->qp;\r\nstruct sk_buff **skb_arr = q_skba->arr;\r\nstruct ehea_rwqe *rwqe;\r\nint i, index, max_index_mask, fill_wqes;\r\nint adder = 0;\r\nint ret = 0;\r\nfill_wqes = q_skba->os_skbs + num_wqes;\r\nq_skba->os_skbs = 0;\r\nif (unlikely(test_bit(__EHEA_STOP_XFER, &ehea_driver_flags))) {\r\nq_skba->os_skbs = fill_wqes;\r\nreturn ret;\r\n}\r\nindex = q_skba->index;\r\nmax_index_mask = q_skba->len - 1;\r\nfor (i = 0; i < fill_wqes; i++) {\r\nu64 tmp_addr;\r\nstruct sk_buff *skb;\r\nskb = netdev_alloc_skb_ip_align(dev, packet_size);\r\nif (!skb) {\r\nq_skba->os_skbs = fill_wqes - i;\r\nif (q_skba->os_skbs == q_skba->len - 2) {\r\nnetdev_info(pr->port->netdev,\r\n"rq%i ran dry - no mem for skb\n",\r\nrq_nr);\r\nret = -ENOMEM;\r\n}\r\nbreak;\r\n}\r\nskb_arr[index] = skb;\r\ntmp_addr = ehea_map_vaddr(skb->data);\r\nif (tmp_addr == -1) {\r\ndev_kfree_skb(skb);\r\nq_skba->os_skbs = fill_wqes - i;\r\nret = 0;\r\nbreak;\r\n}\r\nrwqe = ehea_get_next_rwqe(qp, rq_nr);\r\nrwqe->wr_id = EHEA_BMASK_SET(EHEA_WR_ID_TYPE, wqe_type)\r\n| EHEA_BMASK_SET(EHEA_WR_ID_INDEX, index);\r\nrwqe->sg_list[0].l_key = pr->recv_mr.lkey;\r\nrwqe->sg_list[0].vaddr = tmp_addr;\r\nrwqe->sg_list[0].len = packet_size;\r\nrwqe->data_segments = 1;\r\nindex++;\r\nindex &= max_index_mask;\r\nadder++;\r\n}\r\nq_skba->index = index;\r\nif (adder == 0)\r\ngoto out;\r\niosync();\r\nif (rq_nr == 2)\r\nehea_update_rq2a(pr->qp, adder);\r\nelse\r\nehea_update_rq3a(pr->qp, adder);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ehea_refill_rq2(struct ehea_port_res *pr, int nr_of_wqes)\r\n{\r\nreturn ehea_refill_rq_def(pr, &pr->rq2_skba, 2,\r\nnr_of_wqes, EHEA_RWQE2_TYPE,\r\nEHEA_RQ2_PKT_SIZE);\r\n}\r\nstatic int ehea_refill_rq3(struct ehea_port_res *pr, int nr_of_wqes)\r\n{\r\nreturn ehea_refill_rq_def(pr, &pr->rq3_skba, 3,\r\nnr_of_wqes, EHEA_RWQE3_TYPE,\r\nEHEA_MAX_PACKET_SIZE);\r\n}\r\nstatic inline int ehea_check_cqe(struct ehea_cqe *cqe, int *rq_num)\r\n{\r\n*rq_num = (cqe->type & EHEA_CQE_TYPE_RQ) >> 5;\r\nif ((cqe->status & EHEA_CQE_STAT_ERR_MASK) == 0)\r\nreturn 0;\r\nif (((cqe->status & EHEA_CQE_STAT_ERR_TCP) != 0) &&\r\n(cqe->header_length == 0))\r\nreturn 0;\r\nreturn -EINVAL;\r\n}\r\nstatic inline void ehea_fill_skb(struct net_device *dev,\r\nstruct sk_buff *skb, struct ehea_cqe *cqe,\r\nstruct ehea_port_res *pr)\r\n{\r\nint length = cqe->num_bytes_transfered - 4;\r\nskb_put(skb, length);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nif (cqe->status & EHEA_CQE_BLIND_CKSUM) {\r\nskb->ip_summed = CHECKSUM_COMPLETE;\r\nskb->csum = csum_unfold(~cqe->inet_checksum_value);\r\n} else\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb_record_rx_queue(skb, pr - &pr->port->port_res[0]);\r\n}\r\nstatic inline struct sk_buff *get_skb_by_index(struct sk_buff **skb_array,\r\nint arr_len,\r\nstruct ehea_cqe *cqe)\r\n{\r\nint skb_index = EHEA_BMASK_GET(EHEA_WR_ID_INDEX, cqe->wr_id);\r\nstruct sk_buff *skb;\r\nvoid *pref;\r\nint x;\r\nx = skb_index + 1;\r\nx &= (arr_len - 1);\r\npref = skb_array[x];\r\nif (pref) {\r\nprefetchw(pref);\r\nprefetchw(pref + EHEA_CACHE_LINE);\r\npref = (skb_array[x]->data);\r\nprefetch(pref);\r\nprefetch(pref + EHEA_CACHE_LINE);\r\nprefetch(pref + EHEA_CACHE_LINE * 2);\r\nprefetch(pref + EHEA_CACHE_LINE * 3);\r\n}\r\nskb = skb_array[skb_index];\r\nskb_array[skb_index] = NULL;\r\nreturn skb;\r\n}\r\nstatic inline struct sk_buff *get_skb_by_index_ll(struct sk_buff **skb_array,\r\nint arr_len, int wqe_index)\r\n{\r\nstruct sk_buff *skb;\r\nvoid *pref;\r\nint x;\r\nx = wqe_index + 1;\r\nx &= (arr_len - 1);\r\npref = skb_array[x];\r\nif (pref) {\r\nprefetchw(pref);\r\nprefetchw(pref + EHEA_CACHE_LINE);\r\npref = (skb_array[x]->data);\r\nprefetchw(pref);\r\nprefetchw(pref + EHEA_CACHE_LINE);\r\n}\r\nskb = skb_array[wqe_index];\r\nskb_array[wqe_index] = NULL;\r\nreturn skb;\r\n}\r\nstatic int ehea_treat_poll_error(struct ehea_port_res *pr, int rq,\r\nstruct ehea_cqe *cqe, int *processed_rq2,\r\nint *processed_rq3)\r\n{\r\nstruct sk_buff *skb;\r\nif (cqe->status & EHEA_CQE_STAT_ERR_TCP)\r\npr->p_stats.err_tcp_cksum++;\r\nif (cqe->status & EHEA_CQE_STAT_ERR_IP)\r\npr->p_stats.err_ip_cksum++;\r\nif (cqe->status & EHEA_CQE_STAT_ERR_CRC)\r\npr->p_stats.err_frame_crc++;\r\nif (rq == 2) {\r\n*processed_rq2 += 1;\r\nskb = get_skb_by_index(pr->rq2_skba.arr, pr->rq2_skba.len, cqe);\r\ndev_kfree_skb(skb);\r\n} else if (rq == 3) {\r\n*processed_rq3 += 1;\r\nskb = get_skb_by_index(pr->rq3_skba.arr, pr->rq3_skba.len, cqe);\r\ndev_kfree_skb(skb);\r\n}\r\nif (cqe->status & EHEA_CQE_STAT_FAT_ERR_MASK) {\r\nif (netif_msg_rx_err(pr->port)) {\r\npr_err("Critical receive error for QP %d. Resetting port.\n",\r\npr->qp->init_attr.qp_nr);\r\nehea_dump(cqe, sizeof(*cqe), "CQE");\r\n}\r\nehea_schedule_port_reset(pr->port);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ehea_proc_rwqes(struct net_device *dev,\r\nstruct ehea_port_res *pr,\r\nint budget)\r\n{\r\nstruct ehea_port *port = pr->port;\r\nstruct ehea_qp *qp = pr->qp;\r\nstruct ehea_cqe *cqe;\r\nstruct sk_buff *skb;\r\nstruct sk_buff **skb_arr_rq1 = pr->rq1_skba.arr;\r\nstruct sk_buff **skb_arr_rq2 = pr->rq2_skba.arr;\r\nstruct sk_buff **skb_arr_rq3 = pr->rq3_skba.arr;\r\nint skb_arr_rq1_len = pr->rq1_skba.len;\r\nint skb_arr_rq2_len = pr->rq2_skba.len;\r\nint skb_arr_rq3_len = pr->rq3_skba.len;\r\nint processed, processed_rq1, processed_rq2, processed_rq3;\r\nu64 processed_bytes = 0;\r\nint wqe_index, last_wqe_index, rq, port_reset;\r\nprocessed = processed_rq1 = processed_rq2 = processed_rq3 = 0;\r\nlast_wqe_index = 0;\r\ncqe = ehea_poll_rq1(qp, &wqe_index);\r\nwhile ((processed < budget) && cqe) {\r\nehea_inc_rq1(qp);\r\nprocessed_rq1++;\r\nprocessed++;\r\nif (netif_msg_rx_status(port))\r\nehea_dump(cqe, sizeof(*cqe), "CQE");\r\nlast_wqe_index = wqe_index;\r\nrmb();\r\nif (!ehea_check_cqe(cqe, &rq)) {\r\nif (rq == 1) {\r\nskb = get_skb_by_index_ll(skb_arr_rq1,\r\nskb_arr_rq1_len,\r\nwqe_index);\r\nif (unlikely(!skb)) {\r\nnetif_info(port, rx_err, dev,\r\n"LL rq1: skb=NULL\n");\r\nskb = netdev_alloc_skb(dev,\r\nEHEA_L_PKT_SIZE);\r\nif (!skb)\r\nbreak;\r\n}\r\nskb_copy_to_linear_data(skb, ((char *)cqe) + 64,\r\ncqe->num_bytes_transfered - 4);\r\nehea_fill_skb(dev, skb, cqe, pr);\r\n} else if (rq == 2) {\r\nskb = get_skb_by_index(skb_arr_rq2,\r\nskb_arr_rq2_len, cqe);\r\nif (unlikely(!skb)) {\r\nnetif_err(port, rx_err, dev,\r\n"rq2: skb=NULL\n");\r\nbreak;\r\n}\r\nehea_fill_skb(dev, skb, cqe, pr);\r\nprocessed_rq2++;\r\n} else {\r\nskb = get_skb_by_index(skb_arr_rq3,\r\nskb_arr_rq3_len, cqe);\r\nif (unlikely(!skb)) {\r\nnetif_err(port, rx_err, dev,\r\n"rq3: skb=NULL\n");\r\nbreak;\r\n}\r\nehea_fill_skb(dev, skb, cqe, pr);\r\nprocessed_rq3++;\r\n}\r\nprocessed_bytes += skb->len;\r\nif (cqe->status & EHEA_CQE_VLAN_TAG_XTRACT)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\r\ncqe->vlan_tag);\r\nnapi_gro_receive(&pr->napi, skb);\r\n} else {\r\npr->p_stats.poll_receive_errors++;\r\nport_reset = ehea_treat_poll_error(pr, rq, cqe,\r\n&processed_rq2,\r\n&processed_rq3);\r\nif (port_reset)\r\nbreak;\r\n}\r\ncqe = ehea_poll_rq1(qp, &wqe_index);\r\n}\r\npr->rx_packets += processed;\r\npr->rx_bytes += processed_bytes;\r\nehea_refill_rq1(pr, last_wqe_index, processed_rq1);\r\nehea_refill_rq2(pr, processed_rq2);\r\nehea_refill_rq3(pr, processed_rq3);\r\nreturn processed;\r\n}\r\nstatic void reset_sq_restart_flag(struct ehea_port *port)\r\n{\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\nstruct ehea_port_res *pr = &port->port_res[i];\r\npr->sq_restart_flag = 0;\r\n}\r\nwake_up(&port->restart_wq);\r\n}\r\nstatic void check_sqs(struct ehea_port *port)\r\n{\r\nstruct ehea_swqe *swqe;\r\nint swqe_index;\r\nint i, k;\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\nstruct ehea_port_res *pr = &port->port_res[i];\r\nint ret;\r\nk = 0;\r\nswqe = ehea_get_swqe(pr->qp, &swqe_index);\r\nmemset(swqe, 0, SWQE_HEADER_SIZE);\r\natomic_dec(&pr->swqe_avail);\r\nswqe->tx_control |= EHEA_SWQE_PURGE;\r\nswqe->wr_id = SWQE_RESTART_CHECK;\r\nswqe->tx_control |= EHEA_SWQE_SIGNALLED_COMPLETION;\r\nswqe->tx_control |= EHEA_SWQE_IMM_DATA_PRESENT;\r\nswqe->immediate_data_length = 80;\r\nehea_post_swqe(pr->qp, swqe);\r\nret = wait_event_timeout(port->restart_wq,\r\npr->sq_restart_flag == 0,\r\nmsecs_to_jiffies(100));\r\nif (!ret) {\r\npr_err("HW/SW queues out of sync\n");\r\nehea_schedule_port_reset(pr->port);\r\nreturn;\r\n}\r\n}\r\n}\r\nstatic struct ehea_cqe *ehea_proc_cqes(struct ehea_port_res *pr, int my_quota)\r\n{\r\nstruct sk_buff *skb;\r\nstruct ehea_cq *send_cq = pr->send_cq;\r\nstruct ehea_cqe *cqe;\r\nint quota = my_quota;\r\nint cqe_counter = 0;\r\nint swqe_av = 0;\r\nint index;\r\nstruct netdev_queue *txq = netdev_get_tx_queue(pr->port->netdev,\r\npr - &pr->port->port_res[0]);\r\ncqe = ehea_poll_cq(send_cq);\r\nwhile (cqe && (quota > 0)) {\r\nehea_inc_cq(send_cq);\r\ncqe_counter++;\r\nrmb();\r\nif (cqe->wr_id == SWQE_RESTART_CHECK) {\r\npr->sq_restart_flag = 1;\r\nswqe_av++;\r\nbreak;\r\n}\r\nif (cqe->status & EHEA_CQE_STAT_ERR_MASK) {\r\npr_err("Bad send completion status=0x%04X\n",\r\ncqe->status);\r\nif (netif_msg_tx_err(pr->port))\r\nehea_dump(cqe, sizeof(*cqe), "Send CQE");\r\nif (cqe->status & EHEA_CQE_STAT_RESET_MASK) {\r\npr_err("Resetting port\n");\r\nehea_schedule_port_reset(pr->port);\r\nbreak;\r\n}\r\n}\r\nif (netif_msg_tx_done(pr->port))\r\nehea_dump(cqe, sizeof(*cqe), "CQE");\r\nif (likely(EHEA_BMASK_GET(EHEA_WR_ID_TYPE, cqe->wr_id)\r\n== EHEA_SWQE2_TYPE)) {\r\nindex = EHEA_BMASK_GET(EHEA_WR_ID_INDEX, cqe->wr_id);\r\nskb = pr->sq_skba.arr[index];\r\ndev_kfree_skb(skb);\r\npr->sq_skba.arr[index] = NULL;\r\n}\r\nswqe_av += EHEA_BMASK_GET(EHEA_WR_ID_REFILL, cqe->wr_id);\r\nquota--;\r\ncqe = ehea_poll_cq(send_cq);\r\n}\r\nehea_update_feca(send_cq, cqe_counter);\r\natomic_add(swqe_av, &pr->swqe_avail);\r\nif (unlikely(netif_tx_queue_stopped(txq) &&\r\n(atomic_read(&pr->swqe_avail) >= pr->swqe_refill_th))) {\r\n__netif_tx_lock(txq, smp_processor_id());\r\nif (netif_tx_queue_stopped(txq) &&\r\n(atomic_read(&pr->swqe_avail) >= pr->swqe_refill_th))\r\nnetif_tx_wake_queue(txq);\r\n__netif_tx_unlock(txq);\r\n}\r\nwake_up(&pr->port->swqe_avail_wq);\r\nreturn cqe;\r\n}\r\nstatic int ehea_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct ehea_port_res *pr = container_of(napi, struct ehea_port_res,\r\nnapi);\r\nstruct net_device *dev = pr->port->netdev;\r\nstruct ehea_cqe *cqe;\r\nstruct ehea_cqe *cqe_skb = NULL;\r\nint wqe_index;\r\nint rx = 0;\r\ncqe_skb = ehea_proc_cqes(pr, EHEA_POLL_MAX_CQES);\r\nrx += ehea_proc_rwqes(dev, pr, budget - rx);\r\nwhile (rx != budget) {\r\nnapi_complete(napi);\r\nehea_reset_cq_ep(pr->recv_cq);\r\nehea_reset_cq_ep(pr->send_cq);\r\nehea_reset_cq_n1(pr->recv_cq);\r\nehea_reset_cq_n1(pr->send_cq);\r\nrmb();\r\ncqe = ehea_poll_rq1(pr->qp, &wqe_index);\r\ncqe_skb = ehea_poll_cq(pr->send_cq);\r\nif (!cqe && !cqe_skb)\r\nreturn rx;\r\nif (!napi_reschedule(napi))\r\nreturn rx;\r\ncqe_skb = ehea_proc_cqes(pr, EHEA_POLL_MAX_CQES);\r\nrx += ehea_proc_rwqes(dev, pr, budget - rx);\r\n}\r\nreturn rx;\r\n}\r\nstatic void ehea_netpoll(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++)\r\nnapi_schedule(&port->port_res[i].napi);\r\n}\r\nstatic irqreturn_t ehea_recv_irq_handler(int irq, void *param)\r\n{\r\nstruct ehea_port_res *pr = param;\r\nnapi_schedule(&pr->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ehea_qp_aff_irq_handler(int irq, void *param)\r\n{\r\nstruct ehea_port *port = param;\r\nstruct ehea_eqe *eqe;\r\nstruct ehea_qp *qp;\r\nu32 qp_token;\r\nu64 resource_type, aer, aerr;\r\nint reset_port = 0;\r\neqe = ehea_poll_eq(port->qp_eq);\r\nwhile (eqe) {\r\nqp_token = EHEA_BMASK_GET(EHEA_EQE_QP_TOKEN, eqe->entry);\r\npr_err("QP aff_err: entry=0x%llx, token=0x%x\n",\r\neqe->entry, qp_token);\r\nqp = port->port_res[qp_token].qp;\r\nresource_type = ehea_error_data(port->adapter, qp->fw_handle,\r\n&aer, &aerr);\r\nif (resource_type == EHEA_AER_RESTYPE_QP) {\r\nif ((aer & EHEA_AER_RESET_MASK) ||\r\n(aerr & EHEA_AERR_RESET_MASK))\r\nreset_port = 1;\r\n} else\r\nreset_port = 1;\r\neqe = ehea_poll_eq(port->qp_eq);\r\n}\r\nif (reset_port) {\r\npr_err("Resetting port\n");\r\nehea_schedule_port_reset(port);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic struct ehea_port *ehea_get_port(struct ehea_adapter *adapter,\r\nint logical_port)\r\n{\r\nint i;\r\nfor (i = 0; i < EHEA_MAX_PORTS; i++)\r\nif (adapter->port[i])\r\nif (adapter->port[i]->logical_port_id == logical_port)\r\nreturn adapter->port[i];\r\nreturn NULL;\r\n}\r\nint ehea_sense_port_attr(struct ehea_port *port)\r\n{\r\nint ret;\r\nu64 hret;\r\nstruct hcp_ehea_port_cb0 *cb0;\r\ncb0 = (void *)get_zeroed_page(GFP_ATOMIC);\r\nif (!cb0) {\r\npr_err("no mem for cb0\n");\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_port(port->adapter->handle,\r\nport->logical_port_id, H_PORT_CB0,\r\nEHEA_BMASK_SET(H_PORT_CB0_ALL, 0xFFFF),\r\ncb0);\r\nif (hret != H_SUCCESS) {\r\nret = -EIO;\r\ngoto out_free;\r\n}\r\nport->mac_addr = cb0->port_mac_addr << 16;\r\nif (!is_valid_ether_addr((u8 *)&port->mac_addr)) {\r\nret = -EADDRNOTAVAIL;\r\ngoto out_free;\r\n}\r\nswitch (cb0->port_speed) {\r\ncase H_SPEED_10M_H:\r\nport->port_speed = EHEA_SPEED_10M;\r\nport->full_duplex = 0;\r\nbreak;\r\ncase H_SPEED_10M_F:\r\nport->port_speed = EHEA_SPEED_10M;\r\nport->full_duplex = 1;\r\nbreak;\r\ncase H_SPEED_100M_H:\r\nport->port_speed = EHEA_SPEED_100M;\r\nport->full_duplex = 0;\r\nbreak;\r\ncase H_SPEED_100M_F:\r\nport->port_speed = EHEA_SPEED_100M;\r\nport->full_duplex = 1;\r\nbreak;\r\ncase H_SPEED_1G_F:\r\nport->port_speed = EHEA_SPEED_1G;\r\nport->full_duplex = 1;\r\nbreak;\r\ncase H_SPEED_10G_F:\r\nport->port_speed = EHEA_SPEED_10G;\r\nport->full_duplex = 1;\r\nbreak;\r\ndefault:\r\nport->port_speed = 0;\r\nport->full_duplex = 0;\r\nbreak;\r\n}\r\nport->autoneg = 1;\r\nport->num_mcs = cb0->num_default_qps;\r\nif (use_mcs)\r\nport->num_def_qps = cb0->num_default_qps;\r\nelse\r\nport->num_def_qps = 1;\r\nif (!port->num_def_qps) {\r\nret = -EINVAL;\r\ngoto out_free;\r\n}\r\nret = 0;\r\nout_free:\r\nif (ret || netif_msg_probe(port))\r\nehea_dump(cb0, sizeof(*cb0), "ehea_sense_port_attr");\r\nfree_page((unsigned long)cb0);\r\nout:\r\nreturn ret;\r\n}\r\nint ehea_set_portspeed(struct ehea_port *port, u32 port_speed)\r\n{\r\nstruct hcp_ehea_port_cb4 *cb4;\r\nu64 hret;\r\nint ret = 0;\r\ncb4 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb4) {\r\npr_err("no mem for cb4\n");\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ncb4->port_speed = port_speed;\r\nnetif_carrier_off(port->netdev);\r\nhret = ehea_h_modify_ehea_port(port->adapter->handle,\r\nport->logical_port_id,\r\nH_PORT_CB4, H_PORT_CB4_SPEED, cb4);\r\nif (hret == H_SUCCESS) {\r\nport->autoneg = port_speed == EHEA_SPEED_AUTONEG ? 1 : 0;\r\nhret = ehea_h_query_ehea_port(port->adapter->handle,\r\nport->logical_port_id,\r\nH_PORT_CB4, H_PORT_CB4_SPEED,\r\ncb4);\r\nif (hret == H_SUCCESS) {\r\nswitch (cb4->port_speed) {\r\ncase H_SPEED_10M_H:\r\nport->port_speed = EHEA_SPEED_10M;\r\nport->full_duplex = 0;\r\nbreak;\r\ncase H_SPEED_10M_F:\r\nport->port_speed = EHEA_SPEED_10M;\r\nport->full_duplex = 1;\r\nbreak;\r\ncase H_SPEED_100M_H:\r\nport->port_speed = EHEA_SPEED_100M;\r\nport->full_duplex = 0;\r\nbreak;\r\ncase H_SPEED_100M_F:\r\nport->port_speed = EHEA_SPEED_100M;\r\nport->full_duplex = 1;\r\nbreak;\r\ncase H_SPEED_1G_F:\r\nport->port_speed = EHEA_SPEED_1G;\r\nport->full_duplex = 1;\r\nbreak;\r\ncase H_SPEED_10G_F:\r\nport->port_speed = EHEA_SPEED_10G;\r\nport->full_duplex = 1;\r\nbreak;\r\ndefault:\r\nport->port_speed = 0;\r\nport->full_duplex = 0;\r\nbreak;\r\n}\r\n} else {\r\npr_err("Failed sensing port speed\n");\r\nret = -EIO;\r\n}\r\n} else {\r\nif (hret == H_AUTHORITY) {\r\npr_info("Hypervisor denied setting port speed\n");\r\nret = -EPERM;\r\n} else {\r\nret = -EIO;\r\npr_err("Failed setting port speed\n");\r\n}\r\n}\r\nif (!prop_carrier_state || (port->phy_link == EHEA_PHY_LINK_UP))\r\nnetif_carrier_on(port->netdev);\r\nfree_page((unsigned long)cb4);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void ehea_parse_eqe(struct ehea_adapter *adapter, u64 eqe)\r\n{\r\nint ret;\r\nu8 ec;\r\nu8 portnum;\r\nstruct ehea_port *port;\r\nstruct net_device *dev;\r\nec = EHEA_BMASK_GET(NEQE_EVENT_CODE, eqe);\r\nportnum = EHEA_BMASK_GET(NEQE_PORTNUM, eqe);\r\nport = ehea_get_port(adapter, portnum);\r\ndev = port->netdev;\r\nswitch (ec) {\r\ncase EHEA_EC_PORTSTATE_CHG:\r\nif (!port) {\r\nnetdev_err(dev, "unknown portnum %x\n", portnum);\r\nbreak;\r\n}\r\nif (EHEA_BMASK_GET(NEQE_PORT_UP, eqe)) {\r\nif (!netif_carrier_ok(dev)) {\r\nret = ehea_sense_port_attr(port);\r\nif (ret) {\r\nnetdev_err(dev, "failed resensing port attributes\n");\r\nbreak;\r\n}\r\nnetif_info(port, link, dev,\r\n"Logical port up: %dMbps %s Duplex\n",\r\nport->port_speed,\r\nport->full_duplex == 1 ?\r\n"Full" : "Half");\r\nnetif_carrier_on(dev);\r\nnetif_wake_queue(dev);\r\n}\r\n} else\r\nif (netif_carrier_ok(dev)) {\r\nnetif_info(port, link, dev,\r\n"Logical port down\n");\r\nnetif_carrier_off(dev);\r\nnetif_tx_disable(dev);\r\n}\r\nif (EHEA_BMASK_GET(NEQE_EXTSWITCH_PORT_UP, eqe)) {\r\nport->phy_link = EHEA_PHY_LINK_UP;\r\nnetif_info(port, link, dev,\r\n"Physical port up\n");\r\nif (prop_carrier_state)\r\nnetif_carrier_on(dev);\r\n} else {\r\nport->phy_link = EHEA_PHY_LINK_DOWN;\r\nnetif_info(port, link, dev,\r\n"Physical port down\n");\r\nif (prop_carrier_state)\r\nnetif_carrier_off(dev);\r\n}\r\nif (EHEA_BMASK_GET(NEQE_EXTSWITCH_PRIMARY, eqe))\r\nnetdev_info(dev,\r\n"External switch port is primary port\n");\r\nelse\r\nnetdev_info(dev,\r\n"External switch port is backup port\n");\r\nbreak;\r\ncase EHEA_EC_ADAPTER_MALFUNC:\r\nnetdev_err(dev, "Adapter malfunction\n");\r\nbreak;\r\ncase EHEA_EC_PORT_MALFUNC:\r\nnetdev_info(dev, "Port malfunction\n");\r\nnetif_carrier_off(dev);\r\nnetif_tx_disable(dev);\r\nbreak;\r\ndefault:\r\nnetdev_err(dev, "unknown event code %x, eqe=0x%llX\n", ec, eqe);\r\nbreak;\r\n}\r\n}\r\nstatic void ehea_neq_tasklet(unsigned long data)\r\n{\r\nstruct ehea_adapter *adapter = (struct ehea_adapter *)data;\r\nstruct ehea_eqe *eqe;\r\nu64 event_mask;\r\neqe = ehea_poll_eq(adapter->neq);\r\npr_debug("eqe=%p\n", eqe);\r\nwhile (eqe) {\r\npr_debug("*eqe=%lx\n", (unsigned long) eqe->entry);\r\nehea_parse_eqe(adapter, eqe->entry);\r\neqe = ehea_poll_eq(adapter->neq);\r\npr_debug("next eqe=%p\n", eqe);\r\n}\r\nevent_mask = EHEA_BMASK_SET(NELR_PORTSTATE_CHG, 1)\r\n| EHEA_BMASK_SET(NELR_ADAPTER_MALFUNC, 1)\r\n| EHEA_BMASK_SET(NELR_PORT_MALFUNC, 1);\r\nehea_h_reset_events(adapter->handle,\r\nadapter->neq->fw_handle, event_mask);\r\n}\r\nstatic irqreturn_t ehea_interrupt_neq(int irq, void *param)\r\n{\r\nstruct ehea_adapter *adapter = param;\r\ntasklet_hi_schedule(&adapter->neq_tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int ehea_fill_port_res(struct ehea_port_res *pr)\r\n{\r\nint ret;\r\nstruct ehea_qp_init_attr *init_attr = &pr->qp->init_attr;\r\nehea_init_fill_rq1(pr, pr->rq1_skba.len);\r\nret = ehea_refill_rq2(pr, init_attr->act_nr_rwqes_rq2 - 1);\r\nret |= ehea_refill_rq3(pr, init_attr->act_nr_rwqes_rq3 - 1);\r\nreturn ret;\r\n}\r\nstatic int ehea_reg_interrupts(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_port_res *pr;\r\nint i, ret;\r\nsnprintf(port->int_aff_name, EHEA_IRQ_NAME_SIZE - 1, "%s-aff",\r\ndev->name);\r\nret = ibmebus_request_irq(port->qp_eq->attr.ist1,\r\nehea_qp_aff_irq_handler,\r\n0, port->int_aff_name, port);\r\nif (ret) {\r\nnetdev_err(dev, "failed registering irq for qp_aff_irq_handler:ist=%X\n",\r\nport->qp_eq->attr.ist1);\r\ngoto out_free_qpeq;\r\n}\r\nnetif_info(port, ifup, dev,\r\n"irq_handle 0x%X for function qp_aff_irq_handler registered\n",\r\nport->qp_eq->attr.ist1);\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\npr = &port->port_res[i];\r\nsnprintf(pr->int_send_name, EHEA_IRQ_NAME_SIZE - 1,\r\n"%s-queue%d", dev->name, i);\r\nret = ibmebus_request_irq(pr->eq->attr.ist1,\r\nehea_recv_irq_handler,\r\n0, pr->int_send_name, pr);\r\nif (ret) {\r\nnetdev_err(dev, "failed registering irq for ehea_queue port_res_nr:%d, ist=%X\n",\r\ni, pr->eq->attr.ist1);\r\ngoto out_free_req;\r\n}\r\nnetif_info(port, ifup, dev,\r\n"irq_handle 0x%X for function ehea_queue_int %d registered\n",\r\npr->eq->attr.ist1, i);\r\n}\r\nout:\r\nreturn ret;\r\nout_free_req:\r\nwhile (--i >= 0) {\r\nu32 ist = port->port_res[i].eq->attr.ist1;\r\nibmebus_free_irq(ist, &port->port_res[i]);\r\n}\r\nout_free_qpeq:\r\nibmebus_free_irq(port->qp_eq->attr.ist1, port);\r\ni = port->num_def_qps;\r\ngoto out;\r\n}\r\nstatic void ehea_free_interrupts(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_port_res *pr;\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\npr = &port->port_res[i];\r\nibmebus_free_irq(pr->eq->attr.ist1, pr);\r\nnetif_info(port, intr, dev,\r\n"free send irq for res %d with handle 0x%X\n",\r\ni, pr->eq->attr.ist1);\r\n}\r\nibmebus_free_irq(port->qp_eq->attr.ist1, port);\r\nnetif_info(port, intr, dev,\r\n"associated event interrupt for handle 0x%X freed\n",\r\nport->qp_eq->attr.ist1);\r\n}\r\nstatic int ehea_configure_port(struct ehea_port *port)\r\n{\r\nint ret, i;\r\nu64 hret, mask;\r\nstruct hcp_ehea_port_cb0 *cb0;\r\nret = -ENOMEM;\r\ncb0 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb0)\r\ngoto out;\r\ncb0->port_rc = EHEA_BMASK_SET(PXLY_RC_VALID, 1)\r\n| EHEA_BMASK_SET(PXLY_RC_IP_CHKSUM, 1)\r\n| EHEA_BMASK_SET(PXLY_RC_TCP_UDP_CHKSUM, 1)\r\n| EHEA_BMASK_SET(PXLY_RC_VLAN_XTRACT, 1)\r\n| EHEA_BMASK_SET(PXLY_RC_VLAN_TAG_FILTER,\r\nPXLY_RC_VLAN_FILTER)\r\n| EHEA_BMASK_SET(PXLY_RC_JUMBO_FRAME, 1);\r\nfor (i = 0; i < port->num_mcs; i++)\r\nif (use_mcs)\r\ncb0->default_qpn_arr[i] =\r\nport->port_res[i].qp->init_attr.qp_nr;\r\nelse\r\ncb0->default_qpn_arr[i] =\r\nport->port_res[0].qp->init_attr.qp_nr;\r\nif (netif_msg_ifup(port))\r\nehea_dump(cb0, sizeof(*cb0), "ehea_configure_port");\r\nmask = EHEA_BMASK_SET(H_PORT_CB0_PRC, 1)\r\n| EHEA_BMASK_SET(H_PORT_CB0_DEFQPNARRAY, 1);\r\nhret = ehea_h_modify_ehea_port(port->adapter->handle,\r\nport->logical_port_id,\r\nH_PORT_CB0, mask, cb0);\r\nret = -EIO;\r\nif (hret != H_SUCCESS)\r\ngoto out_free;\r\nret = 0;\r\nout_free:\r\nfree_page((unsigned long)cb0);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ehea_gen_smrs(struct ehea_port_res *pr)\r\n{\r\nint ret;\r\nstruct ehea_adapter *adapter = pr->port->adapter;\r\nret = ehea_gen_smr(adapter, &adapter->mr, &pr->send_mr);\r\nif (ret)\r\ngoto out;\r\nret = ehea_gen_smr(adapter, &adapter->mr, &pr->recv_mr);\r\nif (ret)\r\ngoto out_free;\r\nreturn 0;\r\nout_free:\r\nehea_rem_mr(&pr->send_mr);\r\nout:\r\npr_err("Generating SMRS failed\n");\r\nreturn -EIO;\r\n}\r\nstatic int ehea_rem_smrs(struct ehea_port_res *pr)\r\n{\r\nif ((ehea_rem_mr(&pr->send_mr)) ||\r\n(ehea_rem_mr(&pr->recv_mr)))\r\nreturn -EIO;\r\nelse\r\nreturn 0;\r\n}\r\nstatic int ehea_init_q_skba(struct ehea_q_skb_arr *q_skba, int max_q_entries)\r\n{\r\nint arr_size = sizeof(void *) * max_q_entries;\r\nq_skba->arr = vzalloc(arr_size);\r\nif (!q_skba->arr)\r\nreturn -ENOMEM;\r\nq_skba->len = max_q_entries;\r\nq_skba->index = 0;\r\nq_skba->os_skbs = 0;\r\nreturn 0;\r\n}\r\nstatic int ehea_init_port_res(struct ehea_port *port, struct ehea_port_res *pr,\r\nstruct port_res_cfg *pr_cfg, int queue_token)\r\n{\r\nstruct ehea_adapter *adapter = port->adapter;\r\nenum ehea_eq_type eq_type = EHEA_EQ;\r\nstruct ehea_qp_init_attr *init_attr = NULL;\r\nint ret = -EIO;\r\nu64 tx_bytes, rx_bytes, tx_packets, rx_packets;\r\ntx_bytes = pr->tx_bytes;\r\ntx_packets = pr->tx_packets;\r\nrx_bytes = pr->rx_bytes;\r\nrx_packets = pr->rx_packets;\r\nmemset(pr, 0, sizeof(struct ehea_port_res));\r\npr->tx_bytes = rx_bytes;\r\npr->tx_packets = tx_packets;\r\npr->rx_bytes = rx_bytes;\r\npr->rx_packets = rx_packets;\r\npr->port = port;\r\npr->eq = ehea_create_eq(adapter, eq_type, EHEA_MAX_ENTRIES_EQ, 0);\r\nif (!pr->eq) {\r\npr_err("create_eq failed (eq)\n");\r\ngoto out_free;\r\n}\r\npr->recv_cq = ehea_create_cq(adapter, pr_cfg->max_entries_rcq,\r\npr->eq->fw_handle,\r\nport->logical_port_id);\r\nif (!pr->recv_cq) {\r\npr_err("create_cq failed (cq_recv)\n");\r\ngoto out_free;\r\n}\r\npr->send_cq = ehea_create_cq(adapter, pr_cfg->max_entries_scq,\r\npr->eq->fw_handle,\r\nport->logical_port_id);\r\nif (!pr->send_cq) {\r\npr_err("create_cq failed (cq_send)\n");\r\ngoto out_free;\r\n}\r\nif (netif_msg_ifup(port))\r\npr_info("Send CQ: act_nr_cqes=%d, Recv CQ: act_nr_cqes=%d\n",\r\npr->send_cq->attr.act_nr_of_cqes,\r\npr->recv_cq->attr.act_nr_of_cqes);\r\ninit_attr = kzalloc(sizeof(*init_attr), GFP_KERNEL);\r\nif (!init_attr) {\r\nret = -ENOMEM;\r\npr_err("no mem for ehea_qp_init_attr\n");\r\ngoto out_free;\r\n}\r\ninit_attr->low_lat_rq1 = 1;\r\ninit_attr->signalingtype = 1;\r\ninit_attr->rq_count = 3;\r\ninit_attr->qp_token = queue_token;\r\ninit_attr->max_nr_send_wqes = pr_cfg->max_entries_sq;\r\ninit_attr->max_nr_rwqes_rq1 = pr_cfg->max_entries_rq1;\r\ninit_attr->max_nr_rwqes_rq2 = pr_cfg->max_entries_rq2;\r\ninit_attr->max_nr_rwqes_rq3 = pr_cfg->max_entries_rq3;\r\ninit_attr->wqe_size_enc_sq = EHEA_SG_SQ;\r\ninit_attr->wqe_size_enc_rq1 = EHEA_SG_RQ1;\r\ninit_attr->wqe_size_enc_rq2 = EHEA_SG_RQ2;\r\ninit_attr->wqe_size_enc_rq3 = EHEA_SG_RQ3;\r\ninit_attr->rq2_threshold = EHEA_RQ2_THRESHOLD;\r\ninit_attr->rq3_threshold = EHEA_RQ3_THRESHOLD;\r\ninit_attr->port_nr = port->logical_port_id;\r\ninit_attr->send_cq_handle = pr->send_cq->fw_handle;\r\ninit_attr->recv_cq_handle = pr->recv_cq->fw_handle;\r\ninit_attr->aff_eq_handle = port->qp_eq->fw_handle;\r\npr->qp = ehea_create_qp(adapter, adapter->pd, init_attr);\r\nif (!pr->qp) {\r\npr_err("create_qp failed\n");\r\nret = -EIO;\r\ngoto out_free;\r\n}\r\nif (netif_msg_ifup(port))\r\npr_info("QP: qp_nr=%d\n act_nr_snd_wqe=%d\n nr_rwqe_rq1=%d\n nr_rwqe_rq2=%d\n nr_rwqe_rq3=%d\n",\r\ninit_attr->qp_nr,\r\ninit_attr->act_nr_send_wqes,\r\ninit_attr->act_nr_rwqes_rq1,\r\ninit_attr->act_nr_rwqes_rq2,\r\ninit_attr->act_nr_rwqes_rq3);\r\npr->sq_skba_size = init_attr->act_nr_send_wqes + 1;\r\nret = ehea_init_q_skba(&pr->sq_skba, pr->sq_skba_size);\r\nret |= ehea_init_q_skba(&pr->rq1_skba, init_attr->act_nr_rwqes_rq1 + 1);\r\nret |= ehea_init_q_skba(&pr->rq2_skba, init_attr->act_nr_rwqes_rq2 + 1);\r\nret |= ehea_init_q_skba(&pr->rq3_skba, init_attr->act_nr_rwqes_rq3 + 1);\r\nif (ret)\r\ngoto out_free;\r\npr->swqe_refill_th = init_attr->act_nr_send_wqes / 10;\r\nif (ehea_gen_smrs(pr) != 0) {\r\nret = -EIO;\r\ngoto out_free;\r\n}\r\natomic_set(&pr->swqe_avail, init_attr->act_nr_send_wqes - 1);\r\nkfree(init_attr);\r\nnetif_napi_add(pr->port->netdev, &pr->napi, ehea_poll, 64);\r\nret = 0;\r\ngoto out;\r\nout_free:\r\nkfree(init_attr);\r\nvfree(pr->sq_skba.arr);\r\nvfree(pr->rq1_skba.arr);\r\nvfree(pr->rq2_skba.arr);\r\nvfree(pr->rq3_skba.arr);\r\nehea_destroy_qp(pr->qp);\r\nehea_destroy_cq(pr->send_cq);\r\nehea_destroy_cq(pr->recv_cq);\r\nehea_destroy_eq(pr->eq);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ehea_clean_portres(struct ehea_port *port, struct ehea_port_res *pr)\r\n{\r\nint ret, i;\r\nif (pr->qp)\r\nnetif_napi_del(&pr->napi);\r\nret = ehea_destroy_qp(pr->qp);\r\nif (!ret) {\r\nehea_destroy_cq(pr->send_cq);\r\nehea_destroy_cq(pr->recv_cq);\r\nehea_destroy_eq(pr->eq);\r\nfor (i = 0; i < pr->rq1_skba.len; i++)\r\nif (pr->rq1_skba.arr[i])\r\ndev_kfree_skb(pr->rq1_skba.arr[i]);\r\nfor (i = 0; i < pr->rq2_skba.len; i++)\r\nif (pr->rq2_skba.arr[i])\r\ndev_kfree_skb(pr->rq2_skba.arr[i]);\r\nfor (i = 0; i < pr->rq3_skba.len; i++)\r\nif (pr->rq3_skba.arr[i])\r\ndev_kfree_skb(pr->rq3_skba.arr[i]);\r\nfor (i = 0; i < pr->sq_skba.len; i++)\r\nif (pr->sq_skba.arr[i])\r\ndev_kfree_skb(pr->sq_skba.arr[i]);\r\nvfree(pr->rq1_skba.arr);\r\nvfree(pr->rq2_skba.arr);\r\nvfree(pr->rq3_skba.arr);\r\nvfree(pr->sq_skba.arr);\r\nret = ehea_rem_smrs(pr);\r\n}\r\nreturn ret;\r\n}\r\nstatic void write_swqe2_immediate(struct sk_buff *skb, struct ehea_swqe *swqe,\r\nu32 lkey)\r\n{\r\nint skb_data_size = skb_headlen(skb);\r\nu8 *imm_data = &swqe->u.immdata_desc.immediate_data[0];\r\nstruct ehea_vsgentry *sg1entry = &swqe->u.immdata_desc.sg_entry;\r\nunsigned int immediate_len = SWQE2_MAX_IMM;\r\nswqe->descriptors = 0;\r\nif (skb_is_gso(skb)) {\r\nswqe->tx_control |= EHEA_SWQE_TSO;\r\nswqe->mss = skb_shinfo(skb)->gso_size;\r\nimmediate_len = ETH_HLEN + ip_hdrlen(skb) + tcp_hdrlen(skb);\r\n}\r\nif (skb_is_gso(skb) || skb_data_size >= SWQE2_MAX_IMM) {\r\nskb_copy_from_linear_data(skb, imm_data, immediate_len);\r\nswqe->immediate_data_length = immediate_len;\r\nif (skb_data_size > immediate_len) {\r\nsg1entry->l_key = lkey;\r\nsg1entry->len = skb_data_size - immediate_len;\r\nsg1entry->vaddr =\r\nehea_map_vaddr(skb->data + immediate_len);\r\nswqe->descriptors++;\r\n}\r\n} else {\r\nskb_copy_from_linear_data(skb, imm_data, skb_data_size);\r\nswqe->immediate_data_length = skb_data_size;\r\n}\r\n}\r\nstatic inline void write_swqe2_data(struct sk_buff *skb, struct net_device *dev,\r\nstruct ehea_swqe *swqe, u32 lkey)\r\n{\r\nstruct ehea_vsgentry *sg_list, *sg1entry, *sgentry;\r\nskb_frag_t *frag;\r\nint nfrags, sg1entry_contains_frag_data, i;\r\nnfrags = skb_shinfo(skb)->nr_frags;\r\nsg1entry = &swqe->u.immdata_desc.sg_entry;\r\nsg_list = (struct ehea_vsgentry *)&swqe->u.immdata_desc.sg_list;\r\nsg1entry_contains_frag_data = 0;\r\nwrite_swqe2_immediate(skb, swqe, lkey);\r\nif (nfrags > 0) {\r\nif (swqe->descriptors == 0) {\r\nfrag = &skb_shinfo(skb)->frags[0];\r\nsg1entry->l_key = lkey;\r\nsg1entry->len = skb_frag_size(frag);\r\nsg1entry->vaddr =\r\nehea_map_vaddr(skb_frag_address(frag));\r\nswqe->descriptors++;\r\nsg1entry_contains_frag_data = 1;\r\n}\r\nfor (i = sg1entry_contains_frag_data; i < nfrags; i++) {\r\nfrag = &skb_shinfo(skb)->frags[i];\r\nsgentry = &sg_list[i - sg1entry_contains_frag_data];\r\nsgentry->l_key = lkey;\r\nsgentry->len = skb_frag_size(frag);\r\nsgentry->vaddr = ehea_map_vaddr(skb_frag_address(frag));\r\nswqe->descriptors++;\r\n}\r\n}\r\n}\r\nstatic int ehea_broadcast_reg_helper(struct ehea_port *port, u32 hcallid)\r\n{\r\nint ret = 0;\r\nu64 hret;\r\nu8 reg_type;\r\nreg_type = EHEA_BCMC_BROADCAST | EHEA_BCMC_UNTAGGED;\r\nhret = ehea_h_reg_dereg_bcmc(port->adapter->handle,\r\nport->logical_port_id,\r\nreg_type, port->mac_addr, 0, hcallid);\r\nif (hret != H_SUCCESS) {\r\npr_err("%sregistering bc address failed (tagged)\n",\r\nhcallid == H_REG_BCMC ? "" : "de");\r\nret = -EIO;\r\ngoto out_herr;\r\n}\r\nreg_type = EHEA_BCMC_BROADCAST | EHEA_BCMC_VLANID_ALL;\r\nhret = ehea_h_reg_dereg_bcmc(port->adapter->handle,\r\nport->logical_port_id,\r\nreg_type, port->mac_addr, 0, hcallid);\r\nif (hret != H_SUCCESS) {\r\npr_err("%sregistering bc address failed (vlan)\n",\r\nhcallid == H_REG_BCMC ? "" : "de");\r\nret = -EIO;\r\n}\r\nout_herr:\r\nreturn ret;\r\n}\r\nstatic int ehea_set_mac_addr(struct net_device *dev, void *sa)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct sockaddr *mac_addr = sa;\r\nstruct hcp_ehea_port_cb0 *cb0;\r\nint ret;\r\nu64 hret;\r\nif (!is_valid_ether_addr(mac_addr->sa_data)) {\r\nret = -EADDRNOTAVAIL;\r\ngoto out;\r\n}\r\ncb0 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb0) {\r\npr_err("no mem for cb0\n");\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nmemcpy(&(cb0->port_mac_addr), &(mac_addr->sa_data[0]), ETH_ALEN);\r\ncb0->port_mac_addr = cb0->port_mac_addr >> 16;\r\nhret = ehea_h_modify_ehea_port(port->adapter->handle,\r\nport->logical_port_id, H_PORT_CB0,\r\nEHEA_BMASK_SET(H_PORT_CB0_MAC, 1), cb0);\r\nif (hret != H_SUCCESS) {\r\nret = -EIO;\r\ngoto out_free;\r\n}\r\nmemcpy(dev->dev_addr, mac_addr->sa_data, dev->addr_len);\r\nif (port->state == EHEA_PORT_UP) {\r\nret = ehea_broadcast_reg_helper(port, H_DEREG_BCMC);\r\nif (ret)\r\ngoto out_upregs;\r\n}\r\nport->mac_addr = cb0->port_mac_addr << 16;\r\nif (port->state == EHEA_PORT_UP) {\r\nret = ehea_broadcast_reg_helper(port, H_REG_BCMC);\r\nif (ret)\r\ngoto out_upregs;\r\n}\r\nret = 0;\r\nout_upregs:\r\nehea_update_bcmc_registrations();\r\nout_free:\r\nfree_page((unsigned long)cb0);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void ehea_promiscuous_error(u64 hret, int enable)\r\n{\r\nif (hret == H_AUTHORITY)\r\npr_info("Hypervisor denied %sabling promiscuous mode\n",\r\nenable == 1 ? "en" : "dis");\r\nelse\r\npr_err("failed %sabling promiscuous mode\n",\r\nenable == 1 ? "en" : "dis");\r\n}\r\nstatic void ehea_promiscuous(struct net_device *dev, int enable)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct hcp_ehea_port_cb7 *cb7;\r\nu64 hret;\r\nif (enable == port->promisc)\r\nreturn;\r\ncb7 = (void *)get_zeroed_page(GFP_ATOMIC);\r\nif (!cb7) {\r\npr_err("no mem for cb7\n");\r\ngoto out;\r\n}\r\ncb7->def_uc_qpn = enable == 1 ? port->port_res[0].qp->fw_handle : 0;\r\nhret = ehea_h_modify_ehea_port(port->adapter->handle,\r\nport->logical_port_id,\r\nH_PORT_CB7, H_PORT_CB7_DUCQPN, cb7);\r\nif (hret) {\r\nehea_promiscuous_error(hret, enable);\r\ngoto out;\r\n}\r\nport->promisc = enable;\r\nout:\r\nfree_page((unsigned long)cb7);\r\n}\r\nstatic u64 ehea_multicast_reg_helper(struct ehea_port *port, u64 mc_mac_addr,\r\nu32 hcallid)\r\n{\r\nu64 hret;\r\nu8 reg_type;\r\nreg_type = EHEA_BCMC_MULTICAST | EHEA_BCMC_UNTAGGED;\r\nif (mc_mac_addr == 0)\r\nreg_type |= EHEA_BCMC_SCOPE_ALL;\r\nhret = ehea_h_reg_dereg_bcmc(port->adapter->handle,\r\nport->logical_port_id,\r\nreg_type, mc_mac_addr, 0, hcallid);\r\nif (hret)\r\ngoto out;\r\nreg_type = EHEA_BCMC_MULTICAST | EHEA_BCMC_VLANID_ALL;\r\nif (mc_mac_addr == 0)\r\nreg_type |= EHEA_BCMC_SCOPE_ALL;\r\nhret = ehea_h_reg_dereg_bcmc(port->adapter->handle,\r\nport->logical_port_id,\r\nreg_type, mc_mac_addr, 0, hcallid);\r\nout:\r\nreturn hret;\r\n}\r\nstatic int ehea_drop_multicast_list(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_mc_list *mc_entry = port->mc_list;\r\nstruct list_head *pos;\r\nstruct list_head *temp;\r\nint ret = 0;\r\nu64 hret;\r\nlist_for_each_safe(pos, temp, &(port->mc_list->list)) {\r\nmc_entry = list_entry(pos, struct ehea_mc_list, list);\r\nhret = ehea_multicast_reg_helper(port, mc_entry->macaddr,\r\nH_DEREG_BCMC);\r\nif (hret) {\r\npr_err("failed deregistering mcast MAC\n");\r\nret = -EIO;\r\n}\r\nlist_del(pos);\r\nkfree(mc_entry);\r\n}\r\nreturn ret;\r\n}\r\nstatic void ehea_allmulti(struct net_device *dev, int enable)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nu64 hret;\r\nif (!port->allmulti) {\r\nif (enable) {\r\nehea_drop_multicast_list(dev);\r\nhret = ehea_multicast_reg_helper(port, 0, H_REG_BCMC);\r\nif (!hret)\r\nport->allmulti = 1;\r\nelse\r\nnetdev_err(dev,\r\n"failed enabling IFF_ALLMULTI\n");\r\n}\r\n} else {\r\nif (!enable) {\r\nhret = ehea_multicast_reg_helper(port, 0, H_DEREG_BCMC);\r\nif (!hret)\r\nport->allmulti = 0;\r\nelse\r\nnetdev_err(dev,\r\n"failed disabling IFF_ALLMULTI\n");\r\n}\r\n}\r\n}\r\nstatic void ehea_add_multicast_entry(struct ehea_port *port, u8 *mc_mac_addr)\r\n{\r\nstruct ehea_mc_list *ehea_mcl_entry;\r\nu64 hret;\r\nehea_mcl_entry = kzalloc(sizeof(*ehea_mcl_entry), GFP_ATOMIC);\r\nif (!ehea_mcl_entry)\r\nreturn;\r\nINIT_LIST_HEAD(&ehea_mcl_entry->list);\r\nmemcpy(&ehea_mcl_entry->macaddr, mc_mac_addr, ETH_ALEN);\r\nhret = ehea_multicast_reg_helper(port, ehea_mcl_entry->macaddr,\r\nH_REG_BCMC);\r\nif (!hret)\r\nlist_add(&ehea_mcl_entry->list, &port->mc_list->list);\r\nelse {\r\npr_err("failed registering mcast MAC\n");\r\nkfree(ehea_mcl_entry);\r\n}\r\n}\r\nstatic void ehea_set_multicast_list(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct netdev_hw_addr *ha;\r\nint ret;\r\nehea_promiscuous(dev, !!(dev->flags & IFF_PROMISC));\r\nif (dev->flags & IFF_ALLMULTI) {\r\nehea_allmulti(dev, 1);\r\ngoto out;\r\n}\r\nehea_allmulti(dev, 0);\r\nif (!netdev_mc_empty(dev)) {\r\nret = ehea_drop_multicast_list(dev);\r\nif (ret) {\r\nehea_allmulti(dev, 1);\r\n}\r\nif (netdev_mc_count(dev) > port->adapter->max_mc_mac) {\r\npr_info("Mcast registration limit reached (0x%llx). Use ALLMULTI!\n",\r\nport->adapter->max_mc_mac);\r\ngoto out;\r\n}\r\nnetdev_for_each_mc_addr(ha, dev)\r\nehea_add_multicast_entry(port, ha->addr);\r\n}\r\nout:\r\nehea_update_bcmc_registrations();\r\n}\r\nstatic int ehea_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nif ((new_mtu < 68) || (new_mtu > EHEA_MAX_PACKET_SIZE))\r\nreturn -EINVAL;\r\ndev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic void xmit_common(struct sk_buff *skb, struct ehea_swqe *swqe)\r\n{\r\nswqe->tx_control |= EHEA_SWQE_IMM_DATA_PRESENT | EHEA_SWQE_CRC;\r\nif (skb->protocol != htons(ETH_P_IP))\r\nreturn;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\nswqe->tx_control |= EHEA_SWQE_IP_CHECKSUM;\r\nswqe->ip_start = skb_network_offset(skb);\r\nswqe->ip_end = swqe->ip_start + ip_hdrlen(skb) - 1;\r\nswitch (ip_hdr(skb)->protocol) {\r\ncase IPPROTO_UDP:\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\nswqe->tx_control |= EHEA_SWQE_TCP_CHECKSUM;\r\nswqe->tcp_offset = swqe->ip_end + 1 +\r\noffsetof(struct udphdr, check);\r\nbreak;\r\ncase IPPROTO_TCP:\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\nswqe->tx_control |= EHEA_SWQE_TCP_CHECKSUM;\r\nswqe->tcp_offset = swqe->ip_end + 1 +\r\noffsetof(struct tcphdr, check);\r\nbreak;\r\n}\r\n}\r\nstatic void ehea_xmit2(struct sk_buff *skb, struct net_device *dev,\r\nstruct ehea_swqe *swqe, u32 lkey)\r\n{\r\nswqe->tx_control |= EHEA_SWQE_DESCRIPTORS_PRESENT;\r\nxmit_common(skb, swqe);\r\nwrite_swqe2_data(skb, dev, swqe, lkey);\r\n}\r\nstatic void ehea_xmit3(struct sk_buff *skb, struct net_device *dev,\r\nstruct ehea_swqe *swqe)\r\n{\r\nu8 *imm_data = &swqe->u.immdata_nodesc.immediate_data[0];\r\nxmit_common(skb, swqe);\r\nif (!skb->data_len)\r\nskb_copy_from_linear_data(skb, imm_data, skb->len);\r\nelse\r\nskb_copy_bits(skb, 0, imm_data, skb->len);\r\nswqe->immediate_data_length = skb->len;\r\ndev_kfree_skb(skb);\r\n}\r\nstatic int ehea_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_swqe *swqe;\r\nu32 lkey;\r\nint swqe_index;\r\nstruct ehea_port_res *pr;\r\nstruct netdev_queue *txq;\r\npr = &port->port_res[skb_get_queue_mapping(skb)];\r\ntxq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\r\nswqe = ehea_get_swqe(pr->qp, &swqe_index);\r\nmemset(swqe, 0, SWQE_HEADER_SIZE);\r\natomic_dec(&pr->swqe_avail);\r\nif (vlan_tx_tag_present(skb)) {\r\nswqe->tx_control |= EHEA_SWQE_VLAN_INSERT;\r\nswqe->vlan_tag = vlan_tx_tag_get(skb);\r\n}\r\npr->tx_packets++;\r\npr->tx_bytes += skb->len;\r\nif (skb->len <= SWQE3_MAX_IMM) {\r\nu32 sig_iv = port->sig_comp_iv;\r\nu32 swqe_num = pr->swqe_id_counter;\r\nehea_xmit3(skb, dev, swqe);\r\nswqe->wr_id = EHEA_BMASK_SET(EHEA_WR_ID_TYPE, EHEA_SWQE3_TYPE)\r\n| EHEA_BMASK_SET(EHEA_WR_ID_COUNT, swqe_num);\r\nif (pr->swqe_ll_count >= (sig_iv - 1)) {\r\nswqe->wr_id |= EHEA_BMASK_SET(EHEA_WR_ID_REFILL,\r\nsig_iv);\r\nswqe->tx_control |= EHEA_SWQE_SIGNALLED_COMPLETION;\r\npr->swqe_ll_count = 0;\r\n} else\r\npr->swqe_ll_count += 1;\r\n} else {\r\nswqe->wr_id =\r\nEHEA_BMASK_SET(EHEA_WR_ID_TYPE, EHEA_SWQE2_TYPE)\r\n| EHEA_BMASK_SET(EHEA_WR_ID_COUNT, pr->swqe_id_counter)\r\n| EHEA_BMASK_SET(EHEA_WR_ID_REFILL, 1)\r\n| EHEA_BMASK_SET(EHEA_WR_ID_INDEX, pr->sq_skba.index);\r\npr->sq_skba.arr[pr->sq_skba.index] = skb;\r\npr->sq_skba.index++;\r\npr->sq_skba.index &= (pr->sq_skba.len - 1);\r\nlkey = pr->send_mr.lkey;\r\nehea_xmit2(skb, dev, swqe, lkey);\r\nswqe->tx_control |= EHEA_SWQE_SIGNALLED_COMPLETION;\r\n}\r\npr->swqe_id_counter += 1;\r\nnetif_info(port, tx_queued, dev,\r\n"post swqe on QP %d\n", pr->qp->init_attr.qp_nr);\r\nif (netif_msg_tx_queued(port))\r\nehea_dump(swqe, 512, "swqe");\r\nif (unlikely(test_bit(__EHEA_STOP_XFER, &ehea_driver_flags))) {\r\nnetif_tx_stop_queue(txq);\r\nswqe->tx_control |= EHEA_SWQE_PURGE;\r\n}\r\nehea_post_swqe(pr->qp, swqe);\r\nif (unlikely(atomic_read(&pr->swqe_avail) <= 1)) {\r\npr->p_stats.queue_stopped++;\r\nnetif_tx_stop_queue(txq);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int ehea_vlan_rx_add_vid(struct net_device *dev, __be16 proto, u16 vid)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_adapter *adapter = port->adapter;\r\nstruct hcp_ehea_port_cb1 *cb1;\r\nint index;\r\nu64 hret;\r\nint err = 0;\r\ncb1 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb1) {\r\npr_err("no mem for cb1\n");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_port(adapter->handle, port->logical_port_id,\r\nH_PORT_CB1, H_PORT_CB1_ALL, cb1);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_port failed\n");\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nindex = (vid / 64);\r\ncb1->vlan_filter[index] |= ((u64)(0x8000000000000000 >> (vid & 0x3F)));\r\nhret = ehea_h_modify_ehea_port(adapter->handle, port->logical_port_id,\r\nH_PORT_CB1, H_PORT_CB1_ALL, cb1);\r\nif (hret != H_SUCCESS) {\r\npr_err("modify_ehea_port failed\n");\r\nerr = -EINVAL;\r\n}\r\nout:\r\nfree_page((unsigned long)cb1);\r\nreturn err;\r\n}\r\nstatic int ehea_vlan_rx_kill_vid(struct net_device *dev, __be16 proto, u16 vid)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_adapter *adapter = port->adapter;\r\nstruct hcp_ehea_port_cb1 *cb1;\r\nint index;\r\nu64 hret;\r\nint err = 0;\r\ncb1 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb1) {\r\npr_err("no mem for cb1\n");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_port(adapter->handle, port->logical_port_id,\r\nH_PORT_CB1, H_PORT_CB1_ALL, cb1);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_port failed\n");\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nindex = (vid / 64);\r\ncb1->vlan_filter[index] &= ~((u64)(0x8000000000000000 >> (vid & 0x3F)));\r\nhret = ehea_h_modify_ehea_port(adapter->handle, port->logical_port_id,\r\nH_PORT_CB1, H_PORT_CB1_ALL, cb1);\r\nif (hret != H_SUCCESS) {\r\npr_err("modify_ehea_port failed\n");\r\nerr = -EINVAL;\r\n}\r\nout:\r\nfree_page((unsigned long)cb1);\r\nreturn err;\r\n}\r\nstatic int ehea_activate_qp(struct ehea_adapter *adapter, struct ehea_qp *qp)\r\n{\r\nint ret = -EIO;\r\nu64 hret;\r\nu16 dummy16 = 0;\r\nu64 dummy64 = 0;\r\nstruct hcp_modify_qp_cb0 *cb0;\r\ncb0 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb0) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF), cb0);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_qp failed (1)\n");\r\ngoto out;\r\n}\r\ncb0->qp_ctl_reg = H_QP_CR_STATE_INITIALIZED;\r\nhret = ehea_h_modify_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_QP_CTL_REG, 1), cb0,\r\n&dummy64, &dummy64, &dummy16, &dummy16);\r\nif (hret != H_SUCCESS) {\r\npr_err("modify_ehea_qp failed (1)\n");\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF), cb0);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_qp failed (2)\n");\r\ngoto out;\r\n}\r\ncb0->qp_ctl_reg = H_QP_CR_ENABLED | H_QP_CR_STATE_INITIALIZED;\r\nhret = ehea_h_modify_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_QP_CTL_REG, 1), cb0,\r\n&dummy64, &dummy64, &dummy16, &dummy16);\r\nif (hret != H_SUCCESS) {\r\npr_err("modify_ehea_qp failed (2)\n");\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF), cb0);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_qp failed (3)\n");\r\ngoto out;\r\n}\r\ncb0->qp_ctl_reg = H_QP_CR_ENABLED | H_QP_CR_STATE_RDY2SND;\r\nhret = ehea_h_modify_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_QP_CTL_REG, 1), cb0,\r\n&dummy64, &dummy64, &dummy16, &dummy16);\r\nif (hret != H_SUCCESS) {\r\npr_err("modify_ehea_qp failed (3)\n");\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF), cb0);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_qp failed (4)\n");\r\ngoto out;\r\n}\r\nret = 0;\r\nout:\r\nfree_page((unsigned long)cb0);\r\nreturn ret;\r\n}\r\nstatic int ehea_port_res_setup(struct ehea_port *port, int def_qps)\r\n{\r\nint ret, i;\r\nstruct port_res_cfg pr_cfg, pr_cfg_small_rx;\r\nenum ehea_eq_type eq_type = EHEA_EQ;\r\nport->qp_eq = ehea_create_eq(port->adapter, eq_type,\r\nEHEA_MAX_ENTRIES_EQ, 1);\r\nif (!port->qp_eq) {\r\nret = -EINVAL;\r\npr_err("ehea_create_eq failed (qp_eq)\n");\r\ngoto out_kill_eq;\r\n}\r\npr_cfg.max_entries_rcq = rq1_entries + rq2_entries + rq3_entries;\r\npr_cfg.max_entries_scq = sq_entries * 2;\r\npr_cfg.max_entries_sq = sq_entries;\r\npr_cfg.max_entries_rq1 = rq1_entries;\r\npr_cfg.max_entries_rq2 = rq2_entries;\r\npr_cfg.max_entries_rq3 = rq3_entries;\r\npr_cfg_small_rx.max_entries_rcq = 1;\r\npr_cfg_small_rx.max_entries_scq = sq_entries;\r\npr_cfg_small_rx.max_entries_sq = sq_entries;\r\npr_cfg_small_rx.max_entries_rq1 = 1;\r\npr_cfg_small_rx.max_entries_rq2 = 1;\r\npr_cfg_small_rx.max_entries_rq3 = 1;\r\nfor (i = 0; i < def_qps; i++) {\r\nret = ehea_init_port_res(port, &port->port_res[i], &pr_cfg, i);\r\nif (ret)\r\ngoto out_clean_pr;\r\n}\r\nfor (i = def_qps; i < def_qps; i++) {\r\nret = ehea_init_port_res(port, &port->port_res[i],\r\n&pr_cfg_small_rx, i);\r\nif (ret)\r\ngoto out_clean_pr;\r\n}\r\nreturn 0;\r\nout_clean_pr:\r\nwhile (--i >= 0)\r\nehea_clean_portres(port, &port->port_res[i]);\r\nout_kill_eq:\r\nehea_destroy_eq(port->qp_eq);\r\nreturn ret;\r\n}\r\nstatic int ehea_clean_all_portres(struct ehea_port *port)\r\n{\r\nint ret = 0;\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++)\r\nret |= ehea_clean_portres(port, &port->port_res[i]);\r\nret |= ehea_destroy_eq(port->qp_eq);\r\nreturn ret;\r\n}\r\nstatic void ehea_remove_adapter_mr(struct ehea_adapter *adapter)\r\n{\r\nif (adapter->active_ports)\r\nreturn;\r\nehea_rem_mr(&adapter->mr);\r\n}\r\nstatic int ehea_add_adapter_mr(struct ehea_adapter *adapter)\r\n{\r\nif (adapter->active_ports)\r\nreturn 0;\r\nreturn ehea_reg_kernel_mr(adapter, &adapter->mr);\r\n}\r\nstatic int ehea_up(struct net_device *dev)\r\n{\r\nint ret, i;\r\nstruct ehea_port *port = netdev_priv(dev);\r\nif (port->state == EHEA_PORT_UP)\r\nreturn 0;\r\nret = ehea_port_res_setup(port, port->num_def_qps);\r\nif (ret) {\r\nnetdev_err(dev, "port_res_failed\n");\r\ngoto out;\r\n}\r\nret = ehea_configure_port(port);\r\nif (ret) {\r\nnetdev_err(dev, "ehea_configure_port failed. ret:%d\n", ret);\r\ngoto out_clean_pr;\r\n}\r\nret = ehea_reg_interrupts(dev);\r\nif (ret) {\r\nnetdev_err(dev, "reg_interrupts failed. ret:%d\n", ret);\r\ngoto out_clean_pr;\r\n}\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\nret = ehea_activate_qp(port->adapter, port->port_res[i].qp);\r\nif (ret) {\r\nnetdev_err(dev, "activate_qp failed\n");\r\ngoto out_free_irqs;\r\n}\r\n}\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\nret = ehea_fill_port_res(&port->port_res[i]);\r\nif (ret) {\r\nnetdev_err(dev, "out_free_irqs\n");\r\ngoto out_free_irqs;\r\n}\r\n}\r\nret = ehea_broadcast_reg_helper(port, H_REG_BCMC);\r\nif (ret) {\r\nret = -EIO;\r\ngoto out_free_irqs;\r\n}\r\nport->state = EHEA_PORT_UP;\r\nret = 0;\r\ngoto out;\r\nout_free_irqs:\r\nehea_free_interrupts(dev);\r\nout_clean_pr:\r\nehea_clean_all_portres(port);\r\nout:\r\nif (ret)\r\nnetdev_info(dev, "Failed starting. ret=%i\n", ret);\r\nehea_update_bcmc_registrations();\r\nehea_update_firmware_handles();\r\nreturn ret;\r\n}\r\nstatic void port_napi_disable(struct ehea_port *port)\r\n{\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++)\r\nnapi_disable(&port->port_res[i].napi);\r\n}\r\nstatic void port_napi_enable(struct ehea_port *port)\r\n{\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++)\r\nnapi_enable(&port->port_res[i].napi);\r\n}\r\nstatic int ehea_open(struct net_device *dev)\r\n{\r\nint ret;\r\nstruct ehea_port *port = netdev_priv(dev);\r\nmutex_lock(&port->port_lock);\r\nnetif_info(port, ifup, dev, "enabling port\n");\r\nret = ehea_up(dev);\r\nif (!ret) {\r\nport_napi_enable(port);\r\nnetif_tx_start_all_queues(dev);\r\n}\r\nmutex_unlock(&port->port_lock);\r\nschedule_delayed_work(&port->stats_work,\r\nround_jiffies_relative(msecs_to_jiffies(1000)));\r\nreturn ret;\r\n}\r\nstatic int ehea_down(struct net_device *dev)\r\n{\r\nint ret;\r\nstruct ehea_port *port = netdev_priv(dev);\r\nif (port->state == EHEA_PORT_DOWN)\r\nreturn 0;\r\nehea_drop_multicast_list(dev);\r\nehea_allmulti(dev, 0);\r\nehea_broadcast_reg_helper(port, H_DEREG_BCMC);\r\nehea_free_interrupts(dev);\r\nport->state = EHEA_PORT_DOWN;\r\nehea_update_bcmc_registrations();\r\nret = ehea_clean_all_portres(port);\r\nif (ret)\r\nnetdev_info(dev, "Failed freeing resources. ret=%i\n", ret);\r\nehea_update_firmware_handles();\r\nreturn ret;\r\n}\r\nstatic int ehea_stop(struct net_device *dev)\r\n{\r\nint ret;\r\nstruct ehea_port *port = netdev_priv(dev);\r\nnetif_info(port, ifdown, dev, "disabling port\n");\r\nset_bit(__EHEA_DISABLE_PORT_RESET, &port->flags);\r\ncancel_work_sync(&port->reset_task);\r\ncancel_delayed_work_sync(&port->stats_work);\r\nmutex_lock(&port->port_lock);\r\nnetif_tx_stop_all_queues(dev);\r\nport_napi_disable(port);\r\nret = ehea_down(dev);\r\nmutex_unlock(&port->port_lock);\r\nclear_bit(__EHEA_DISABLE_PORT_RESET, &port->flags);\r\nreturn ret;\r\n}\r\nstatic void ehea_purge_sq(struct ehea_qp *orig_qp)\r\n{\r\nstruct ehea_qp qp = *orig_qp;\r\nstruct ehea_qp_init_attr *init_attr = &qp.init_attr;\r\nstruct ehea_swqe *swqe;\r\nint wqe_index;\r\nint i;\r\nfor (i = 0; i < init_attr->act_nr_send_wqes; i++) {\r\nswqe = ehea_get_swqe(&qp, &wqe_index);\r\nswqe->tx_control |= EHEA_SWQE_PURGE;\r\n}\r\n}\r\nstatic void ehea_flush_sq(struct ehea_port *port)\r\n{\r\nint i;\r\nfor (i = 0; i < port->num_def_qps; i++) {\r\nstruct ehea_port_res *pr = &port->port_res[i];\r\nint swqe_max = pr->sq_skba_size - 2 - pr->swqe_ll_count;\r\nint ret;\r\nret = wait_event_timeout(port->swqe_avail_wq,\r\natomic_read(&pr->swqe_avail) >= swqe_max,\r\nmsecs_to_jiffies(100));\r\nif (!ret) {\r\npr_err("WARNING: sq not flushed completely\n");\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic int ehea_stop_qps(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_adapter *adapter = port->adapter;\r\nstruct hcp_modify_qp_cb0 *cb0;\r\nint ret = -EIO;\r\nint dret;\r\nint i;\r\nu64 hret;\r\nu64 dummy64 = 0;\r\nu16 dummy16 = 0;\r\ncb0 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb0) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nfor (i = 0; i < (port->num_def_qps); i++) {\r\nstruct ehea_port_res *pr = &port->port_res[i];\r\nstruct ehea_qp *qp = pr->qp;\r\nehea_purge_sq(qp);\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF),\r\ncb0);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_qp failed (1)\n");\r\ngoto out;\r\n}\r\ncb0->qp_ctl_reg = (cb0->qp_ctl_reg & H_QP_CR_RES_STATE) << 8;\r\ncb0->qp_ctl_reg &= ~H_QP_CR_ENABLED;\r\nhret = ehea_h_modify_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_QP_CTL_REG,\r\n1), cb0, &dummy64,\r\n&dummy64, &dummy16, &dummy16);\r\nif (hret != H_SUCCESS) {\r\npr_err("modify_ehea_qp failed (1)\n");\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF),\r\ncb0);\r\nif (hret != H_SUCCESS) {\r\npr_err("query_ehea_qp failed (2)\n");\r\ngoto out;\r\n}\r\ndret = ehea_rem_smrs(pr);\r\nif (dret) {\r\npr_err("unreg shared memory region failed\n");\r\ngoto out;\r\n}\r\n}\r\nret = 0;\r\nout:\r\nfree_page((unsigned long)cb0);\r\nreturn ret;\r\n}\r\nstatic void ehea_update_rqs(struct ehea_qp *orig_qp, struct ehea_port_res *pr)\r\n{\r\nstruct ehea_qp qp = *orig_qp;\r\nstruct ehea_qp_init_attr *init_attr = &qp.init_attr;\r\nstruct ehea_rwqe *rwqe;\r\nstruct sk_buff **skba_rq2 = pr->rq2_skba.arr;\r\nstruct sk_buff **skba_rq3 = pr->rq3_skba.arr;\r\nstruct sk_buff *skb;\r\nu32 lkey = pr->recv_mr.lkey;\r\nint i;\r\nint index;\r\nfor (i = 0; i < init_attr->act_nr_rwqes_rq2 + 1; i++) {\r\nrwqe = ehea_get_next_rwqe(&qp, 2);\r\nrwqe->sg_list[0].l_key = lkey;\r\nindex = EHEA_BMASK_GET(EHEA_WR_ID_INDEX, rwqe->wr_id);\r\nskb = skba_rq2[index];\r\nif (skb)\r\nrwqe->sg_list[0].vaddr = ehea_map_vaddr(skb->data);\r\n}\r\nfor (i = 0; i < init_attr->act_nr_rwqes_rq3 + 1; i++) {\r\nrwqe = ehea_get_next_rwqe(&qp, 3);\r\nrwqe->sg_list[0].l_key = lkey;\r\nindex = EHEA_BMASK_GET(EHEA_WR_ID_INDEX, rwqe->wr_id);\r\nskb = skba_rq3[index];\r\nif (skb)\r\nrwqe->sg_list[0].vaddr = ehea_map_vaddr(skb->data);\r\n}\r\n}\r\nstatic int ehea_restart_qps(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nstruct ehea_adapter *adapter = port->adapter;\r\nint ret = 0;\r\nint i;\r\nstruct hcp_modify_qp_cb0 *cb0;\r\nu64 hret;\r\nu64 dummy64 = 0;\r\nu16 dummy16 = 0;\r\ncb0 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb0) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nfor (i = 0; i < (port->num_def_qps); i++) {\r\nstruct ehea_port_res *pr = &port->port_res[i];\r\nstruct ehea_qp *qp = pr->qp;\r\nret = ehea_gen_smrs(pr);\r\nif (ret) {\r\nnetdev_err(dev, "creation of shared memory regions failed\n");\r\ngoto out;\r\n}\r\nehea_update_rqs(qp, pr);\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF),\r\ncb0);\r\nif (hret != H_SUCCESS) {\r\nnetdev_err(dev, "query_ehea_qp failed (1)\n");\r\ngoto out;\r\n}\r\ncb0->qp_ctl_reg = (cb0->qp_ctl_reg & H_QP_CR_RES_STATE) << 8;\r\ncb0->qp_ctl_reg |= H_QP_CR_ENABLED;\r\nhret = ehea_h_modify_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_QP_CTL_REG,\r\n1), cb0, &dummy64,\r\n&dummy64, &dummy16, &dummy16);\r\nif (hret != H_SUCCESS) {\r\nnetdev_err(dev, "modify_ehea_qp failed (1)\n");\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea_qp(adapter->handle, 0, qp->fw_handle,\r\nEHEA_BMASK_SET(H_QPCB0_ALL, 0xFFFF),\r\ncb0);\r\nif (hret != H_SUCCESS) {\r\nnetdev_err(dev, "query_ehea_qp failed (2)\n");\r\ngoto out;\r\n}\r\nehea_refill_rq1(pr, pr->rq1_skba.index, 0);\r\nehea_refill_rq2(pr, 0);\r\nehea_refill_rq3(pr, 0);\r\n}\r\nout:\r\nfree_page((unsigned long)cb0);\r\nreturn ret;\r\n}\r\nstatic void ehea_reset_port(struct work_struct *work)\r\n{\r\nint ret;\r\nstruct ehea_port *port =\r\ncontainer_of(work, struct ehea_port, reset_task);\r\nstruct net_device *dev = port->netdev;\r\nmutex_lock(&dlpar_mem_lock);\r\nport->resets++;\r\nmutex_lock(&port->port_lock);\r\nnetif_tx_disable(dev);\r\nport_napi_disable(port);\r\nehea_down(dev);\r\nret = ehea_up(dev);\r\nif (ret)\r\ngoto out;\r\nehea_set_multicast_list(dev);\r\nnetif_info(port, timer, dev, "reset successful\n");\r\nport_napi_enable(port);\r\nnetif_tx_wake_all_queues(dev);\r\nout:\r\nmutex_unlock(&port->port_lock);\r\nmutex_unlock(&dlpar_mem_lock);\r\n}\r\nstatic void ehea_rereg_mrs(void)\r\n{\r\nint ret, i;\r\nstruct ehea_adapter *adapter;\r\npr_info("LPAR memory changed - re-initializing driver\n");\r\nlist_for_each_entry(adapter, &adapter_list, list)\r\nif (adapter->active_ports) {\r\nfor (i = 0; i < EHEA_MAX_PORTS; i++) {\r\nstruct ehea_port *port = adapter->port[i];\r\nstruct net_device *dev;\r\nif (!port)\r\ncontinue;\r\ndev = port->netdev;\r\nif (dev->flags & IFF_UP) {\r\nmutex_lock(&port->port_lock);\r\nnetif_tx_disable(dev);\r\nehea_flush_sq(port);\r\nret = ehea_stop_qps(dev);\r\nif (ret) {\r\nmutex_unlock(&port->port_lock);\r\ngoto out;\r\n}\r\nport_napi_disable(port);\r\nmutex_unlock(&port->port_lock);\r\n}\r\nreset_sq_restart_flag(port);\r\n}\r\nret = ehea_rem_mr(&adapter->mr);\r\nif (ret) {\r\npr_err("unregister MR failed - driver inoperable!\n");\r\ngoto out;\r\n}\r\n}\r\nclear_bit(__EHEA_STOP_XFER, &ehea_driver_flags);\r\nlist_for_each_entry(adapter, &adapter_list, list)\r\nif (adapter->active_ports) {\r\nret = ehea_reg_kernel_mr(adapter, &adapter->mr);\r\nif (ret) {\r\npr_err("register MR failed - driver inoperable!\n");\r\ngoto out;\r\n}\r\nfor (i = 0; i < EHEA_MAX_PORTS; i++) {\r\nstruct ehea_port *port = adapter->port[i];\r\nif (port) {\r\nstruct net_device *dev = port->netdev;\r\nif (dev->flags & IFF_UP) {\r\nmutex_lock(&port->port_lock);\r\nret = ehea_restart_qps(dev);\r\nif (!ret) {\r\ncheck_sqs(port);\r\nport_napi_enable(port);\r\nnetif_tx_wake_all_queues(dev);\r\n} else {\r\nnetdev_err(dev, "Unable to restart QPS\n");\r\n}\r\nmutex_unlock(&port->port_lock);\r\n}\r\n}\r\n}\r\n}\r\npr_info("re-initializing driver complete\n");\r\nout:\r\nreturn;\r\n}\r\nstatic void ehea_tx_watchdog(struct net_device *dev)\r\n{\r\nstruct ehea_port *port = netdev_priv(dev);\r\nif (netif_carrier_ok(dev) &&\r\n!test_bit(__EHEA_STOP_XFER, &ehea_driver_flags))\r\nehea_schedule_port_reset(port);\r\n}\r\nstatic int ehea_sense_adapter_attr(struct ehea_adapter *adapter)\r\n{\r\nstruct hcp_query_ehea *cb;\r\nu64 hret;\r\nint ret;\r\ncb = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nhret = ehea_h_query_ehea(adapter->handle, cb);\r\nif (hret != H_SUCCESS) {\r\nret = -EIO;\r\ngoto out_herr;\r\n}\r\nadapter->max_mc_mac = cb->max_mc_mac - 1;\r\nret = 0;\r\nout_herr:\r\nfree_page((unsigned long)cb);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ehea_get_jumboframe_status(struct ehea_port *port, int *jumbo)\r\n{\r\nstruct hcp_ehea_port_cb4 *cb4;\r\nu64 hret;\r\nint ret = 0;\r\n*jumbo = 0;\r\ncb4 = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (!cb4) {\r\npr_err("no mem for cb4\n");\r\nret = -ENOMEM;\r\ngoto out;\r\n} else {\r\nhret = ehea_h_query_ehea_port(port->adapter->handle,\r\nport->logical_port_id,\r\nH_PORT_CB4,\r\nH_PORT_CB4_JUMBO, cb4);\r\nif (hret == H_SUCCESS) {\r\nif (cb4->jumbo_frame)\r\n*jumbo = 1;\r\nelse {\r\ncb4->jumbo_frame = 1;\r\nhret = ehea_h_modify_ehea_port(port->adapter->\r\nhandle,\r\nport->\r\nlogical_port_id,\r\nH_PORT_CB4,\r\nH_PORT_CB4_JUMBO,\r\ncb4);\r\nif (hret == H_SUCCESS)\r\n*jumbo = 1;\r\n}\r\n} else\r\nret = -EINVAL;\r\nfree_page((unsigned long)cb4);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic ssize_t ehea_show_port_id(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct ehea_port *port = container_of(dev, struct ehea_port, ofdev.dev);\r\nreturn sprintf(buf, "%d", port->logical_port_id);\r\n}\r\nstatic void logical_port_release(struct device *dev)\r\n{\r\nstruct ehea_port *port = container_of(dev, struct ehea_port, ofdev.dev);\r\nof_node_put(port->ofdev.dev.of_node);\r\n}\r\nstatic struct device *ehea_register_port(struct ehea_port *port,\r\nstruct device_node *dn)\r\n{\r\nint ret;\r\nport->ofdev.dev.of_node = of_node_get(dn);\r\nport->ofdev.dev.parent = &port->adapter->ofdev->dev;\r\nport->ofdev.dev.bus = &ibmebus_bus_type;\r\ndev_set_name(&port->ofdev.dev, "port%d", port_name_cnt++);\r\nport->ofdev.dev.release = logical_port_release;\r\nret = of_device_register(&port->ofdev);\r\nif (ret) {\r\npr_err("failed to register device. ret=%d\n", ret);\r\ngoto out;\r\n}\r\nret = device_create_file(&port->ofdev.dev, &dev_attr_log_port_id);\r\nif (ret) {\r\npr_err("failed to register attributes, ret=%d\n", ret);\r\ngoto out_unreg_of_dev;\r\n}\r\nreturn &port->ofdev.dev;\r\nout_unreg_of_dev:\r\nof_device_unregister(&port->ofdev);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic void ehea_unregister_port(struct ehea_port *port)\r\n{\r\ndevice_remove_file(&port->ofdev.dev, &dev_attr_log_port_id);\r\nof_device_unregister(&port->ofdev);\r\n}\r\nstatic struct ehea_port *ehea_setup_single_port(struct ehea_adapter *adapter,\r\nu32 logical_port_id,\r\nstruct device_node *dn)\r\n{\r\nint ret;\r\nstruct net_device *dev;\r\nstruct ehea_port *port;\r\nstruct device *port_dev;\r\nint jumbo;\r\ndev = alloc_etherdev_mq(sizeof(struct ehea_port), EHEA_MAX_PORT_RES);\r\nif (!dev) {\r\nret = -ENOMEM;\r\ngoto out_err;\r\n}\r\nport = netdev_priv(dev);\r\nmutex_init(&port->port_lock);\r\nport->state = EHEA_PORT_DOWN;\r\nport->sig_comp_iv = sq_entries / 10;\r\nport->adapter = adapter;\r\nport->netdev = dev;\r\nport->logical_port_id = logical_port_id;\r\nport->msg_enable = netif_msg_init(msg_level, EHEA_MSG_DEFAULT);\r\nport->mc_list = kzalloc(sizeof(struct ehea_mc_list), GFP_KERNEL);\r\nif (!port->mc_list) {\r\nret = -ENOMEM;\r\ngoto out_free_ethdev;\r\n}\r\nINIT_LIST_HEAD(&port->mc_list->list);\r\nret = ehea_sense_port_attr(port);\r\nif (ret)\r\ngoto out_free_mc_list;\r\nnetif_set_real_num_rx_queues(dev, port->num_def_qps);\r\nnetif_set_real_num_tx_queues(dev, port->num_def_qps);\r\nport_dev = ehea_register_port(port, dn);\r\nif (!port_dev)\r\ngoto out_free_mc_list;\r\nSET_NETDEV_DEV(dev, port_dev);\r\nmemcpy(dev->dev_addr, &port->mac_addr, ETH_ALEN);\r\ndev->netdev_ops = &ehea_netdev_ops;\r\nehea_set_ethtool_ops(dev);\r\ndev->hw_features = NETIF_F_SG | NETIF_F_TSO |\r\nNETIF_F_IP_CSUM | NETIF_F_HW_VLAN_CTAG_TX;\r\ndev->features = NETIF_F_SG | NETIF_F_TSO |\r\nNETIF_F_HIGHDMA | NETIF_F_IP_CSUM |\r\nNETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |\r\nNETIF_F_HW_VLAN_CTAG_FILTER | NETIF_F_RXCSUM;\r\ndev->vlan_features = NETIF_F_SG | NETIF_F_TSO | NETIF_F_HIGHDMA |\r\nNETIF_F_IP_CSUM;\r\ndev->watchdog_timeo = EHEA_WATCH_DOG_TIMEOUT;\r\nINIT_WORK(&port->reset_task, ehea_reset_port);\r\nINIT_DELAYED_WORK(&port->stats_work, ehea_update_stats);\r\ninit_waitqueue_head(&port->swqe_avail_wq);\r\ninit_waitqueue_head(&port->restart_wq);\r\nmemset(&port->stats, 0, sizeof(struct net_device_stats));\r\nret = register_netdev(dev);\r\nif (ret) {\r\npr_err("register_netdev failed. ret=%d\n", ret);\r\ngoto out_unreg_port;\r\n}\r\nret = ehea_get_jumboframe_status(port, &jumbo);\r\nif (ret)\r\nnetdev_err(dev, "failed determining jumbo frame status\n");\r\nnetdev_info(dev, "Jumbo frames are %sabled\n",\r\njumbo == 1 ? "en" : "dis");\r\nadapter->active_ports++;\r\nreturn port;\r\nout_unreg_port:\r\nehea_unregister_port(port);\r\nout_free_mc_list:\r\nkfree(port->mc_list);\r\nout_free_ethdev:\r\nfree_netdev(dev);\r\nout_err:\r\npr_err("setting up logical port with id=%d failed, ret=%d\n",\r\nlogical_port_id, ret);\r\nreturn NULL;\r\n}\r\nstatic void ehea_shutdown_single_port(struct ehea_port *port)\r\n{\r\nstruct ehea_adapter *adapter = port->adapter;\r\ncancel_work_sync(&port->reset_task);\r\ncancel_delayed_work_sync(&port->stats_work);\r\nunregister_netdev(port->netdev);\r\nehea_unregister_port(port);\r\nkfree(port->mc_list);\r\nfree_netdev(port->netdev);\r\nadapter->active_ports--;\r\n}\r\nstatic int ehea_setup_ports(struct ehea_adapter *adapter)\r\n{\r\nstruct device_node *lhea_dn;\r\nstruct device_node *eth_dn = NULL;\r\nconst u32 *dn_log_port_id;\r\nint i = 0;\r\nlhea_dn = adapter->ofdev->dev.of_node;\r\nwhile ((eth_dn = of_get_next_child(lhea_dn, eth_dn))) {\r\ndn_log_port_id = of_get_property(eth_dn, "ibm,hea-port-no",\r\nNULL);\r\nif (!dn_log_port_id) {\r\npr_err("bad device node: eth_dn name=%s\n",\r\neth_dn->full_name);\r\ncontinue;\r\n}\r\nif (ehea_add_adapter_mr(adapter)) {\r\npr_err("creating MR failed\n");\r\nof_node_put(eth_dn);\r\nreturn -EIO;\r\n}\r\nadapter->port[i] = ehea_setup_single_port(adapter,\r\n*dn_log_port_id,\r\neth_dn);\r\nif (adapter->port[i])\r\nnetdev_info(adapter->port[i]->netdev,\r\n"logical port id #%d\n", *dn_log_port_id);\r\nelse\r\nehea_remove_adapter_mr(adapter);\r\ni++;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct device_node *ehea_get_eth_dn(struct ehea_adapter *adapter,\r\nu32 logical_port_id)\r\n{\r\nstruct device_node *lhea_dn;\r\nstruct device_node *eth_dn = NULL;\r\nconst u32 *dn_log_port_id;\r\nlhea_dn = adapter->ofdev->dev.of_node;\r\nwhile ((eth_dn = of_get_next_child(lhea_dn, eth_dn))) {\r\ndn_log_port_id = of_get_property(eth_dn, "ibm,hea-port-no",\r\nNULL);\r\nif (dn_log_port_id)\r\nif (*dn_log_port_id == logical_port_id)\r\nreturn eth_dn;\r\n}\r\nreturn NULL;\r\n}\r\nstatic ssize_t ehea_probe_port(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct ehea_adapter *adapter = dev_get_drvdata(dev);\r\nstruct ehea_port *port;\r\nstruct device_node *eth_dn = NULL;\r\nint i;\r\nu32 logical_port_id;\r\nsscanf(buf, "%d", &logical_port_id);\r\nport = ehea_get_port(adapter, logical_port_id);\r\nif (port) {\r\nnetdev_info(port->netdev, "adding port with logical port id=%d failed: port already configured\n",\r\nlogical_port_id);\r\nreturn -EINVAL;\r\n}\r\neth_dn = ehea_get_eth_dn(adapter, logical_port_id);\r\nif (!eth_dn) {\r\npr_info("no logical port with id %d found\n", logical_port_id);\r\nreturn -EINVAL;\r\n}\r\nif (ehea_add_adapter_mr(adapter)) {\r\npr_err("creating MR failed\n");\r\nreturn -EIO;\r\n}\r\nport = ehea_setup_single_port(adapter, logical_port_id, eth_dn);\r\nof_node_put(eth_dn);\r\nif (port) {\r\nfor (i = 0; i < EHEA_MAX_PORTS; i++)\r\nif (!adapter->port[i]) {\r\nadapter->port[i] = port;\r\nbreak;\r\n}\r\nnetdev_info(port->netdev, "added: (logical port id=%d)\n",\r\nlogical_port_id);\r\n} else {\r\nehea_remove_adapter_mr(adapter);\r\nreturn -EIO;\r\n}\r\nreturn (ssize_t) count;\r\n}\r\nstatic ssize_t ehea_remove_port(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct ehea_adapter *adapter = dev_get_drvdata(dev);\r\nstruct ehea_port *port;\r\nint i;\r\nu32 logical_port_id;\r\nsscanf(buf, "%d", &logical_port_id);\r\nport = ehea_get_port(adapter, logical_port_id);\r\nif (port) {\r\nnetdev_info(port->netdev, "removed: (logical port id=%d)\n",\r\nlogical_port_id);\r\nehea_shutdown_single_port(port);\r\nfor (i = 0; i < EHEA_MAX_PORTS; i++)\r\nif (adapter->port[i] == port) {\r\nadapter->port[i] = NULL;\r\nbreak;\r\n}\r\n} else {\r\npr_err("removing port with logical port id=%d failed. port not configured.\n",\r\nlogical_port_id);\r\nreturn -EINVAL;\r\n}\r\nehea_remove_adapter_mr(adapter);\r\nreturn (ssize_t) count;\r\n}\r\nstatic int ehea_create_device_sysfs(struct platform_device *dev)\r\n{\r\nint ret = device_create_file(&dev->dev, &dev_attr_probe_port);\r\nif (ret)\r\ngoto out;\r\nret = device_create_file(&dev->dev, &dev_attr_remove_port);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void ehea_remove_device_sysfs(struct platform_device *dev)\r\n{\r\ndevice_remove_file(&dev->dev, &dev_attr_probe_port);\r\ndevice_remove_file(&dev->dev, &dev_attr_remove_port);\r\n}\r\nstatic int ehea_probe_adapter(struct platform_device *dev)\r\n{\r\nstruct ehea_adapter *adapter;\r\nconst u64 *adapter_handle;\r\nint ret;\r\nint i;\r\nif (!dev || !dev->dev.of_node) {\r\npr_err("Invalid ibmebus device probed\n");\r\nreturn -EINVAL;\r\n}\r\nadapter = kzalloc(sizeof(*adapter), GFP_KERNEL);\r\nif (!adapter) {\r\nret = -ENOMEM;\r\ndev_err(&dev->dev, "no mem for ehea_adapter\n");\r\ngoto out;\r\n}\r\nlist_add(&adapter->list, &adapter_list);\r\nadapter->ofdev = dev;\r\nadapter_handle = of_get_property(dev->dev.of_node, "ibm,hea-handle",\r\nNULL);\r\nif (adapter_handle)\r\nadapter->handle = *adapter_handle;\r\nif (!adapter->handle) {\r\ndev_err(&dev->dev, "failed getting handle for adapter"\r\n" '%s'\n", dev->dev.of_node->full_name);\r\nret = -ENODEV;\r\ngoto out_free_ad;\r\n}\r\nadapter->pd = EHEA_PD_ID;\r\nplatform_set_drvdata(dev, adapter);\r\nret = ehea_sense_adapter_attr(adapter);\r\nif (ret) {\r\ndev_err(&dev->dev, "sense_adapter_attr failed: %d\n", ret);\r\ngoto out_free_ad;\r\n}\r\nadapter->neq = ehea_create_eq(adapter,\r\nEHEA_NEQ, EHEA_MAX_ENTRIES_EQ, 1);\r\nif (!adapter->neq) {\r\nret = -EIO;\r\ndev_err(&dev->dev, "NEQ creation failed\n");\r\ngoto out_free_ad;\r\n}\r\ntasklet_init(&adapter->neq_tasklet, ehea_neq_tasklet,\r\n(unsigned long)adapter);\r\nret = ehea_create_device_sysfs(dev);\r\nif (ret)\r\ngoto out_kill_eq;\r\nret = ehea_setup_ports(adapter);\r\nif (ret) {\r\ndev_err(&dev->dev, "setup_ports failed\n");\r\ngoto out_rem_dev_sysfs;\r\n}\r\nret = ibmebus_request_irq(adapter->neq->attr.ist1,\r\nehea_interrupt_neq, 0,\r\n"ehea_neq", adapter);\r\nif (ret) {\r\ndev_err(&dev->dev, "requesting NEQ IRQ failed\n");\r\ngoto out_shutdown_ports;\r\n}\r\ntasklet_hi_schedule(&adapter->neq_tasklet);\r\nret = 0;\r\ngoto out;\r\nout_shutdown_ports:\r\nfor (i = 0; i < EHEA_MAX_PORTS; i++)\r\nif (adapter->port[i]) {\r\nehea_shutdown_single_port(adapter->port[i]);\r\nadapter->port[i] = NULL;\r\n}\r\nout_rem_dev_sysfs:\r\nehea_remove_device_sysfs(dev);\r\nout_kill_eq:\r\nehea_destroy_eq(adapter->neq);\r\nout_free_ad:\r\nlist_del(&adapter->list);\r\nkfree(adapter);\r\nout:\r\nehea_update_firmware_handles();\r\nreturn ret;\r\n}\r\nstatic int ehea_remove(struct platform_device *dev)\r\n{\r\nstruct ehea_adapter *adapter = platform_get_drvdata(dev);\r\nint i;\r\nfor (i = 0; i < EHEA_MAX_PORTS; i++)\r\nif (adapter->port[i]) {\r\nehea_shutdown_single_port(adapter->port[i]);\r\nadapter->port[i] = NULL;\r\n}\r\nehea_remove_device_sysfs(dev);\r\nibmebus_free_irq(adapter->neq->attr.ist1, adapter);\r\ntasklet_kill(&adapter->neq_tasklet);\r\nehea_destroy_eq(adapter->neq);\r\nehea_remove_adapter_mr(adapter);\r\nlist_del(&adapter->list);\r\nkfree(adapter);\r\nehea_update_firmware_handles();\r\nreturn 0;\r\n}\r\nstatic void ehea_crash_handler(void)\r\n{\r\nint i;\r\nif (ehea_fw_handles.arr)\r\nfor (i = 0; i < ehea_fw_handles.num_entries; i++)\r\nehea_h_free_resource(ehea_fw_handles.arr[i].adh,\r\nehea_fw_handles.arr[i].fwh,\r\nFORCE_FREE);\r\nif (ehea_bcmc_regs.arr)\r\nfor (i = 0; i < ehea_bcmc_regs.num_entries; i++)\r\nehea_h_reg_dereg_bcmc(ehea_bcmc_regs.arr[i].adh,\r\nehea_bcmc_regs.arr[i].port_id,\r\nehea_bcmc_regs.arr[i].reg_type,\r\nehea_bcmc_regs.arr[i].macaddr,\r\n0, H_DEREG_BCMC);\r\n}\r\nstatic int ehea_mem_notifier(struct notifier_block *nb,\r\nunsigned long action, void *data)\r\n{\r\nint ret = NOTIFY_BAD;\r\nstruct memory_notify *arg = data;\r\nmutex_lock(&dlpar_mem_lock);\r\nswitch (action) {\r\ncase MEM_CANCEL_OFFLINE:\r\npr_info("memory offlining canceled");\r\ncase MEM_ONLINE:\r\npr_info("memory is going online");\r\nset_bit(__EHEA_STOP_XFER, &ehea_driver_flags);\r\nif (ehea_add_sect_bmap(arg->start_pfn, arg->nr_pages))\r\ngoto out_unlock;\r\nehea_rereg_mrs();\r\nbreak;\r\ncase MEM_GOING_OFFLINE:\r\npr_info("memory is going offline");\r\nset_bit(__EHEA_STOP_XFER, &ehea_driver_flags);\r\nif (ehea_rem_sect_bmap(arg->start_pfn, arg->nr_pages))\r\ngoto out_unlock;\r\nehea_rereg_mrs();\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nehea_update_firmware_handles();\r\nret = NOTIFY_OK;\r\nout_unlock:\r\nmutex_unlock(&dlpar_mem_lock);\r\nreturn ret;\r\n}\r\nstatic int ehea_reboot_notifier(struct notifier_block *nb,\r\nunsigned long action, void *unused)\r\n{\r\nif (action == SYS_RESTART) {\r\npr_info("Reboot: freeing all eHEA resources\n");\r\nibmebus_unregister_driver(&ehea_driver);\r\n}\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic int check_module_parm(void)\r\n{\r\nint ret = 0;\r\nif ((rq1_entries < EHEA_MIN_ENTRIES_QP) ||\r\n(rq1_entries > EHEA_MAX_ENTRIES_RQ1)) {\r\npr_info("Bad parameter: rq1_entries\n");\r\nret = -EINVAL;\r\n}\r\nif ((rq2_entries < EHEA_MIN_ENTRIES_QP) ||\r\n(rq2_entries > EHEA_MAX_ENTRIES_RQ2)) {\r\npr_info("Bad parameter: rq2_entries\n");\r\nret = -EINVAL;\r\n}\r\nif ((rq3_entries < EHEA_MIN_ENTRIES_QP) ||\r\n(rq3_entries > EHEA_MAX_ENTRIES_RQ3)) {\r\npr_info("Bad parameter: rq3_entries\n");\r\nret = -EINVAL;\r\n}\r\nif ((sq_entries < EHEA_MIN_ENTRIES_QP) ||\r\n(sq_entries > EHEA_MAX_ENTRIES_SQ)) {\r\npr_info("Bad parameter: sq_entries\n");\r\nret = -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nstatic ssize_t ehea_show_capabilities(struct device_driver *drv,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%d", EHEA_CAPABILITIES);\r\n}\r\nstatic int __init ehea_module_init(void)\r\n{\r\nint ret;\r\npr_info("IBM eHEA ethernet device driver (Release %s)\n", DRV_VERSION);\r\nmemset(&ehea_fw_handles, 0, sizeof(ehea_fw_handles));\r\nmemset(&ehea_bcmc_regs, 0, sizeof(ehea_bcmc_regs));\r\nmutex_init(&ehea_fw_handles.lock);\r\nspin_lock_init(&ehea_bcmc_regs.lock);\r\nret = check_module_parm();\r\nif (ret)\r\ngoto out;\r\nret = ehea_create_busmap();\r\nif (ret)\r\ngoto out;\r\nret = register_reboot_notifier(&ehea_reboot_nb);\r\nif (ret)\r\npr_info("failed registering reboot notifier\n");\r\nret = register_memory_notifier(&ehea_mem_nb);\r\nif (ret)\r\npr_info("failed registering memory remove notifier\n");\r\nret = crash_shutdown_register(ehea_crash_handler);\r\nif (ret)\r\npr_info("failed registering crash handler\n");\r\nret = ibmebus_register_driver(&ehea_driver);\r\nif (ret) {\r\npr_err("failed registering eHEA device driver on ebus\n");\r\ngoto out2;\r\n}\r\nret = driver_create_file(&ehea_driver.driver,\r\n&driver_attr_capabilities);\r\nif (ret) {\r\npr_err("failed to register capabilities attribute, ret=%d\n",\r\nret);\r\ngoto out3;\r\n}\r\nreturn ret;\r\nout3:\r\nibmebus_unregister_driver(&ehea_driver);\r\nout2:\r\nunregister_memory_notifier(&ehea_mem_nb);\r\nunregister_reboot_notifier(&ehea_reboot_nb);\r\ncrash_shutdown_unregister(ehea_crash_handler);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void __exit ehea_module_exit(void)\r\n{\r\nint ret;\r\ndriver_remove_file(&ehea_driver.driver, &driver_attr_capabilities);\r\nibmebus_unregister_driver(&ehea_driver);\r\nunregister_reboot_notifier(&ehea_reboot_nb);\r\nret = crash_shutdown_unregister(ehea_crash_handler);\r\nif (ret)\r\npr_info("failed unregistering crash handler\n");\r\nunregister_memory_notifier(&ehea_mem_nb);\r\nkfree(ehea_fw_handles.arr);\r\nkfree(ehea_bcmc_regs.arr);\r\nehea_destroy_busmap();\r\n}
