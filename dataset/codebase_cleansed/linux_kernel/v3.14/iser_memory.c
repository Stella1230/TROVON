static int iser_start_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nint dma_nents;\r\nstruct ib_device *dev;\r\nchar *mem = NULL;\r\nstruct iser_data_buf *data = &iser_task->data[cmd_dir];\r\nunsigned long cmd_data_len = data->data_len;\r\nif (cmd_data_len > ISER_KMALLOC_THRESHOLD)\r\nmem = (void *)__get_free_pages(GFP_ATOMIC,\r\nilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);\r\nelse\r\nmem = kmalloc(cmd_data_len, GFP_ATOMIC);\r\nif (mem == NULL) {\r\niser_err("Failed to allocate mem size %d %d for copying sglist\n",\r\ndata->size,(int)cmd_data_len);\r\nreturn -ENOMEM;\r\n}\r\nif (cmd_dir == ISER_DIR_OUT) {\r\nstruct scatterlist *sgl = (struct scatterlist *)data->buf;\r\nstruct scatterlist *sg;\r\nint i;\r\nchar *p, *from;\r\np = mem;\r\nfor_each_sg(sgl, sg, data->size, i) {\r\nfrom = kmap_atomic(sg_page(sg));\r\nmemcpy(p,\r\nfrom + sg->offset,\r\nsg->length);\r\nkunmap_atomic(from);\r\np += sg->length;\r\n}\r\n}\r\nsg_init_one(&iser_task->data_copy[cmd_dir].sg_single, mem, cmd_data_len);\r\niser_task->data_copy[cmd_dir].buf =\r\n&iser_task->data_copy[cmd_dir].sg_single;\r\niser_task->data_copy[cmd_dir].size = 1;\r\niser_task->data_copy[cmd_dir].copy_buf = mem;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\ndma_nents = ib_dma_map_sg(dev,\r\n&iser_task->data_copy[cmd_dir].sg_single,\r\n1,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nBUG_ON(dma_nents == 0);\r\niser_task->data_copy[cmd_dir].dma_nents = dma_nents;\r\nreturn 0;\r\n}\r\nvoid iser_finalize_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_device *dev;\r\nstruct iser_data_buf *mem_copy;\r\nunsigned long cmd_data_len;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\nmem_copy = &iser_task->data_copy[cmd_dir];\r\nib_dma_unmap_sg(dev, &mem_copy->sg_single, 1,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nif (cmd_dir == ISER_DIR_IN) {\r\nchar *mem;\r\nstruct scatterlist *sgl, *sg;\r\nunsigned char *p, *to;\r\nunsigned int sg_size;\r\nint i;\r\nmem = mem_copy->copy_buf;\r\nsgl = (struct scatterlist *)iser_task->data[ISER_DIR_IN].buf;\r\nsg_size = iser_task->data[ISER_DIR_IN].size;\r\np = mem;\r\nfor_each_sg(sgl, sg, sg_size, i) {\r\nto = kmap_atomic(sg_page(sg));\r\nmemcpy(to + sg->offset,\r\np,\r\nsg->length);\r\nkunmap_atomic(to);\r\np += sg->length;\r\n}\r\n}\r\ncmd_data_len = iser_task->data[cmd_dir].data_len;\r\nif (cmd_data_len > ISER_KMALLOC_THRESHOLD)\r\nfree_pages((unsigned long)mem_copy->copy_buf,\r\nilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);\r\nelse\r\nkfree(mem_copy->copy_buf);\r\nmem_copy->copy_buf = NULL;\r\n}\r\nstatic int iser_sg_to_page_vec(struct iser_data_buf *data,\r\nstruct ib_device *ibdev, u64 *pages,\r\nint *offset, int *data_size)\r\n{\r\nstruct scatterlist *sg, *sgl = (struct scatterlist *)data->buf;\r\nu64 start_addr, end_addr, page, chunk_start = 0;\r\nunsigned long total_sz = 0;\r\nunsigned int dma_len;\r\nint i, new_chunk, cur_page, last_ent = data->dma_nents - 1;\r\n*offset = (u64) sgl[0].offset & ~MASK_4K;\r\nnew_chunk = 1;\r\ncur_page = 0;\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nstart_addr = ib_sg_dma_address(ibdev, sg);\r\nif (new_chunk)\r\nchunk_start = start_addr;\r\ndma_len = ib_sg_dma_len(ibdev, sg);\r\nend_addr = start_addr + dma_len;\r\ntotal_sz += dma_len;\r\nif (!IS_4K_ALIGNED(end_addr) && i < last_ent) {\r\nnew_chunk = 0;\r\ncontinue;\r\n}\r\nnew_chunk = 1;\r\npage = chunk_start & MASK_4K;\r\ndo {\r\npages[cur_page++] = page;\r\npage += SIZE_4K;\r\n} while (page < end_addr);\r\n}\r\n*data_size = total_sz;\r\niser_dbg("page_vec->data_size:%d cur_page %d\n",\r\n*data_size, cur_page);\r\nreturn cur_page;\r\n}\r\nstatic int iser_data_buf_aligned_len(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sgl, *sg, *next_sg = NULL;\r\nu64 start_addr, end_addr;\r\nint i, ret_len, start_check = 0;\r\nif (data->dma_nents == 1)\r\nreturn 1;\r\nsgl = (struct scatterlist *)data->buf;\r\nstart_addr = ib_sg_dma_address(ibdev, sgl);\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nif (start_check && !IS_4K_ALIGNED(start_addr))\r\nbreak;\r\nnext_sg = sg_next(sg);\r\nif (!next_sg)\r\nbreak;\r\nend_addr = start_addr + ib_sg_dma_len(ibdev, sg);\r\nstart_addr = ib_sg_dma_address(ibdev, next_sg);\r\nif (end_addr == start_addr) {\r\nstart_check = 0;\r\ncontinue;\r\n} else\r\nstart_check = 1;\r\nif (!IS_4K_ALIGNED(end_addr))\r\nbreak;\r\n}\r\nret_len = (next_sg) ? i : i+1;\r\niser_dbg("Found %d aligned entries out of %d in sg:0x%p\n",\r\nret_len, data->dma_nents, data);\r\nreturn ret_len;\r\n}\r\nstatic void iser_data_buf_dump(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sgl = (struct scatterlist *)data->buf;\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, data->dma_nents, i)\r\niser_dbg("sg[%d] dma_addr:0x%lX page:0x%p "\r\n"off:0x%x sz:0x%x dma_len:0x%x\n",\r\ni, (unsigned long)ib_sg_dma_address(ibdev, sg),\r\nsg_page(sg), sg->offset,\r\nsg->length, ib_sg_dma_len(ibdev, sg));\r\n}\r\nstatic void iser_dump_page_vec(struct iser_page_vec *page_vec)\r\n{\r\nint i;\r\niser_err("page vec length %d data size %d\n",\r\npage_vec->length, page_vec->data_size);\r\nfor (i = 0; i < page_vec->length; i++)\r\niser_err("%d %lx\n",i,(unsigned long)page_vec->pages[i]);\r\n}\r\nstatic void iser_page_vec_build(struct iser_data_buf *data,\r\nstruct iser_page_vec *page_vec,\r\nstruct ib_device *ibdev)\r\n{\r\nint page_vec_len = 0;\r\npage_vec->length = 0;\r\npage_vec->offset = 0;\r\niser_dbg("Translating sg sz: %d\n", data->dma_nents);\r\npage_vec_len = iser_sg_to_page_vec(data, ibdev, page_vec->pages,\r\n&page_vec->offset,\r\n&page_vec->data_size);\r\niser_dbg("sg len %d page_vec_len %d\n", data->dma_nents, page_vec_len);\r\npage_vec->length = page_vec_len;\r\nif (page_vec_len * SIZE_4K < page_vec->data_size) {\r\niser_err("page_vec too short to hold this SG\n");\r\niser_data_buf_dump(data, ibdev);\r\niser_dump_page_vec(page_vec);\r\nBUG();\r\n}\r\n}\r\nint iser_dma_map_task_data(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nenum iser_data_dir iser_dir,\r\nenum dma_data_direction dma_dir)\r\n{\r\nstruct ib_device *dev;\r\niser_task->dir[iser_dir] = 1;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\ndata->dma_nents = ib_dma_map_sg(dev, data->buf, data->size, dma_dir);\r\nif (data->dma_nents == 0) {\r\niser_err("dma_map_sg failed!!!\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid iser_dma_unmap_task_data(struct iscsi_iser_task *iser_task)\r\n{\r\nstruct ib_device *dev;\r\nstruct iser_data_buf *data;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\nif (iser_task->dir[ISER_DIR_IN]) {\r\ndata = &iser_task->data[ISER_DIR_IN];\r\nib_dma_unmap_sg(dev, data->buf, data->size, DMA_FROM_DEVICE);\r\n}\r\nif (iser_task->dir[ISER_DIR_OUT]) {\r\ndata = &iser_task->data[ISER_DIR_OUT];\r\nib_dma_unmap_sg(dev, data->buf, data->size, DMA_TO_DEVICE);\r\n}\r\n}\r\nstatic int fall_to_bounce_buf(struct iscsi_iser_task *iser_task,\r\nstruct ib_device *ibdev,\r\nenum iser_data_dir cmd_dir,\r\nint aligned_len)\r\n{\r\nstruct iscsi_conn *iscsi_conn = iser_task->iser_conn->iscsi_conn;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\niscsi_conn->fmr_unalign_cnt++;\r\niser_warn("rdma alignment violation (%d/%d aligned) or FMR not supported\n",\r\naligned_len, mem->size);\r\nif (iser_debug_level > 0)\r\niser_data_buf_dump(mem, ibdev);\r\niser_dma_unmap_task_data(iser_task);\r\nif (iser_start_rdma_unaligned_sg(iser_task, cmd_dir) != 0)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint iser_reg_rdma_mem_fmr(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct iser_conn *ib_conn = iser_task->iser_conn->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\nstruct iser_regd_buf *regd_buf;\r\nint aligned_len;\r\nint err;\r\nint i;\r\nstruct scatterlist *sg;\r\nregd_buf = &iser_task->rdma_regd[cmd_dir];\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, ibdev,\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\nmem = &iser_task->data_copy[cmd_dir];\r\n}\r\nif (mem->dma_nents == 1) {\r\nsg = (struct scatterlist *)mem->buf;\r\nregd_buf->reg.lkey = device->mr->lkey;\r\nregd_buf->reg.rkey = device->mr->rkey;\r\nregd_buf->reg.len = ib_sg_dma_len(ibdev, &sg[0]);\r\nregd_buf->reg.va = ib_sg_dma_address(ibdev, &sg[0]);\r\nregd_buf->reg.is_mr = 0;\r\niser_dbg("PHYSICAL Mem.register: lkey: 0x%08X rkey: 0x%08X "\r\n"va: 0x%08lX sz: %ld]\n",\r\n(unsigned int)regd_buf->reg.lkey,\r\n(unsigned int)regd_buf->reg.rkey,\r\n(unsigned long)regd_buf->reg.va,\r\n(unsigned long)regd_buf->reg.len);\r\n} else {\r\niser_page_vec_build(mem, ib_conn->fastreg.fmr.page_vec, ibdev);\r\nerr = iser_reg_page_vec(ib_conn, ib_conn->fastreg.fmr.page_vec,\r\n&regd_buf->reg);\r\nif (err && err != -EAGAIN) {\r\niser_data_buf_dump(mem, ibdev);\r\niser_err("mem->dma_nents = %d (dlength = 0x%x)\n",\r\nmem->dma_nents,\r\nntoh24(iser_task->desc.iscsi_header.dlength));\r\niser_err("page_vec: data_size = 0x%x, length = %d, offset = 0x%x\n",\r\nib_conn->fastreg.fmr.page_vec->data_size,\r\nib_conn->fastreg.fmr.page_vec->length,\r\nib_conn->fastreg.fmr.page_vec->offset);\r\nfor (i = 0; i < ib_conn->fastreg.fmr.page_vec->length; i++)\r\niser_err("page_vec[%d] = 0x%llx\n", i,\r\n(unsigned long long) ib_conn->fastreg.fmr.page_vec->pages[i]);\r\n}\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int iser_fast_reg_mr(struct fast_reg_descriptor *desc,\r\nstruct iser_conn *ib_conn,\r\nstruct iser_regd_buf *regd_buf,\r\nu32 offset, unsigned int data_size,\r\nunsigned int page_list_len)\r\n{\r\nstruct ib_send_wr fastreg_wr, inv_wr;\r\nstruct ib_send_wr *bad_wr, *wr = NULL;\r\nu8 key;\r\nint ret;\r\nif (!desc->valid) {\r\nmemset(&inv_wr, 0, sizeof(inv_wr));\r\ninv_wr.opcode = IB_WR_LOCAL_INV;\r\ninv_wr.send_flags = IB_SEND_SIGNALED;\r\ninv_wr.ex.invalidate_rkey = desc->data_mr->rkey;\r\nwr = &inv_wr;\r\nkey = (u8)(desc->data_mr->rkey & 0x000000FF);\r\nib_update_fast_reg_key(desc->data_mr, ++key);\r\n}\r\nmemset(&fastreg_wr, 0, sizeof(fastreg_wr));\r\nfastreg_wr.opcode = IB_WR_FAST_REG_MR;\r\nfastreg_wr.send_flags = IB_SEND_SIGNALED;\r\nfastreg_wr.wr.fast_reg.iova_start = desc->data_frpl->page_list[0] + offset;\r\nfastreg_wr.wr.fast_reg.page_list = desc->data_frpl;\r\nfastreg_wr.wr.fast_reg.page_list_len = page_list_len;\r\nfastreg_wr.wr.fast_reg.page_shift = SHIFT_4K;\r\nfastreg_wr.wr.fast_reg.length = data_size;\r\nfastreg_wr.wr.fast_reg.rkey = desc->data_mr->rkey;\r\nfastreg_wr.wr.fast_reg.access_flags = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_READ);\r\nif (!wr) {\r\nwr = &fastreg_wr;\r\natomic_inc(&ib_conn->post_send_buf_count);\r\n} else {\r\nwr->next = &fastreg_wr;\r\natomic_add(2, &ib_conn->post_send_buf_count);\r\n}\r\nret = ib_post_send(ib_conn->qp, wr, &bad_wr);\r\nif (ret) {\r\nif (bad_wr->next)\r\natomic_sub(2, &ib_conn->post_send_buf_count);\r\nelse\r\natomic_dec(&ib_conn->post_send_buf_count);\r\niser_err("fast registration failed, ret:%d\n", ret);\r\nreturn ret;\r\n}\r\ndesc->valid = false;\r\nregd_buf->reg.mem_h = desc;\r\nregd_buf->reg.lkey = desc->data_mr->lkey;\r\nregd_buf->reg.rkey = desc->data_mr->rkey;\r\nregd_buf->reg.va = desc->data_frpl->page_list[0] + offset;\r\nregd_buf->reg.len = data_size;\r\nregd_buf->reg.is_mr = 1;\r\nreturn ret;\r\n}\r\nint iser_reg_rdma_mem_frwr(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct iser_conn *ib_conn = iser_task->iser_conn->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\nstruct iser_regd_buf *regd_buf = &iser_task->rdma_regd[cmd_dir];\r\nstruct fast_reg_descriptor *desc;\r\nunsigned int data_size, page_list_len;\r\nint err, aligned_len;\r\nunsigned long flags;\r\nu32 offset;\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, ibdev,\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\nmem = &iser_task->data_copy[cmd_dir];\r\n}\r\nif (mem->dma_nents == 1) {\r\nstruct scatterlist *sg = (struct scatterlist *)mem->buf;\r\nregd_buf->reg.lkey = device->mr->lkey;\r\nregd_buf->reg.rkey = device->mr->rkey;\r\nregd_buf->reg.len = ib_sg_dma_len(ibdev, &sg[0]);\r\nregd_buf->reg.va = ib_sg_dma_address(ibdev, &sg[0]);\r\nregd_buf->reg.is_mr = 0;\r\n} else {\r\nspin_lock_irqsave(&ib_conn->lock, flags);\r\ndesc = list_first_entry(&ib_conn->fastreg.frwr.pool,\r\nstruct fast_reg_descriptor, list);\r\nlist_del(&desc->list);\r\nspin_unlock_irqrestore(&ib_conn->lock, flags);\r\npage_list_len = iser_sg_to_page_vec(mem, device->ib_device,\r\ndesc->data_frpl->page_list,\r\n&offset, &data_size);\r\nif (page_list_len * SIZE_4K < data_size) {\r\niser_err("fast reg page_list too short to hold this SG\n");\r\nerr = -EINVAL;\r\ngoto err_reg;\r\n}\r\nerr = iser_fast_reg_mr(desc, ib_conn, regd_buf,\r\noffset, data_size, page_list_len);\r\nif (err)\r\ngoto err_reg;\r\n}\r\nreturn 0;\r\nerr_reg:\r\nspin_lock_irqsave(&ib_conn->lock, flags);\r\nlist_add_tail(&desc->list, &ib_conn->fastreg.frwr.pool);\r\nspin_unlock_irqrestore(&ib_conn->lock, flags);\r\nreturn err;\r\n}
