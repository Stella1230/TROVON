static inline int\r\ngss_krb5_padding(int blocksize, int length)\r\n{\r\nreturn blocksize - (length % blocksize);\r\n}\r\nstatic inline void\r\ngss_krb5_add_padding(struct xdr_buf *buf, int offset, int blocksize)\r\n{\r\nint padding = gss_krb5_padding(blocksize, buf->len - offset);\r\nchar *p;\r\nstruct kvec *iov;\r\nif (buf->page_len || buf->tail[0].iov_len)\r\niov = &buf->tail[0];\r\nelse\r\niov = &buf->head[0];\r\np = iov->iov_base + iov->iov_len;\r\niov->iov_len += padding;\r\nbuf->len += padding;\r\nmemset(p, padding, padding);\r\n}\r\nstatic inline int\r\ngss_krb5_remove_padding(struct xdr_buf *buf, int blocksize)\r\n{\r\nu8 *ptr;\r\nu8 pad;\r\nsize_t len = buf->len;\r\nif (len <= buf->head[0].iov_len) {\r\npad = *(u8 *)(buf->head[0].iov_base + len - 1);\r\nif (pad > buf->head[0].iov_len)\r\nreturn -EINVAL;\r\nbuf->head[0].iov_len -= pad;\r\ngoto out;\r\n} else\r\nlen -= buf->head[0].iov_len;\r\nif (len <= buf->page_len) {\r\nunsigned int last = (buf->page_base + len - 1)\r\n>>PAGE_CACHE_SHIFT;\r\nunsigned int offset = (buf->page_base + len - 1)\r\n& (PAGE_CACHE_SIZE - 1);\r\nptr = kmap_atomic(buf->pages[last]);\r\npad = *(ptr + offset);\r\nkunmap_atomic(ptr);\r\ngoto out;\r\n} else\r\nlen -= buf->page_len;\r\nBUG_ON(len > buf->tail[0].iov_len);\r\npad = *(u8 *)(buf->tail[0].iov_base + len - 1);\r\nout:\r\nif (pad > blocksize)\r\nreturn -EINVAL;\r\nif (buf->len > pad)\r\nbuf->len -= pad;\r\nelse\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nvoid\r\ngss_krb5_make_confounder(char *p, u32 conflen)\r\n{\r\nstatic u64 i = 0;\r\nu64 *q = (u64 *)p;\r\nif (i == 0) {\r\ni = prandom_u32();\r\ni = (i << 32) | prandom_u32();\r\n}\r\nswitch (conflen) {\r\ncase 16:\r\n*q++ = i++;\r\ncase 8:\r\n*q++ = i++;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic u32\r\ngss_wrap_kerberos_v1(struct krb5_ctx *kctx, int offset,\r\nstruct xdr_buf *buf, struct page **pages)\r\n{\r\nchar cksumdata[GSS_KRB5_MAX_CKSUM_LEN];\r\nstruct xdr_netobj md5cksum = {.len = sizeof(cksumdata),\r\n.data = cksumdata};\r\nint blocksize = 0, plainlen;\r\nunsigned char *ptr, *msg_start;\r\ns32 now;\r\nint headlen;\r\nstruct page **tmp_pages;\r\nu32 seq_send;\r\nu8 *cksumkey;\r\nu32 conflen = kctx->gk5e->conflen;\r\ndprintk("RPC: %s\n", __func__);\r\nnow = get_seconds();\r\nblocksize = crypto_blkcipher_blocksize(kctx->enc);\r\ngss_krb5_add_padding(buf, offset, blocksize);\r\nBUG_ON((buf->len - offset) % blocksize);\r\nplainlen = conflen + buf->len - offset;\r\nheadlen = g_token_size(&kctx->mech_used,\r\nGSS_KRB5_TOK_HDR_LEN + kctx->gk5e->cksumlength + plainlen) -\r\n(buf->len - offset);\r\nptr = buf->head[0].iov_base + offset;\r\nxdr_extend_head(buf, offset, headlen);\r\nBUG_ON((buf->len - offset - headlen) % blocksize);\r\ng_make_token_header(&kctx->mech_used,\r\nGSS_KRB5_TOK_HDR_LEN +\r\nkctx->gk5e->cksumlength + plainlen, &ptr);\r\nptr[0] = (unsigned char) ((KG_TOK_WRAP_MSG >> 8) & 0xff);\r\nptr[1] = (unsigned char) (KG_TOK_WRAP_MSG & 0xff);\r\nmsg_start = ptr + GSS_KRB5_TOK_HDR_LEN + kctx->gk5e->cksumlength;\r\n*(__be16 *)(ptr + 2) = cpu_to_le16(kctx->gk5e->signalg);\r\nmemset(ptr + 4, 0xff, 4);\r\n*(__be16 *)(ptr + 4) = cpu_to_le16(kctx->gk5e->sealalg);\r\ngss_krb5_make_confounder(msg_start, conflen);\r\nif (kctx->gk5e->keyed_cksum)\r\ncksumkey = kctx->cksum;\r\nelse\r\ncksumkey = NULL;\r\ntmp_pages = buf->pages;\r\nbuf->pages = pages;\r\nif (make_checksum(kctx, ptr, 8, buf, offset + headlen - conflen,\r\ncksumkey, KG_USAGE_SEAL, &md5cksum))\r\nreturn GSS_S_FAILURE;\r\nbuf->pages = tmp_pages;\r\nmemcpy(ptr + GSS_KRB5_TOK_HDR_LEN, md5cksum.data, md5cksum.len);\r\nspin_lock(&krb5_seq_lock);\r\nseq_send = kctx->seq_send++;\r\nspin_unlock(&krb5_seq_lock);\r\nif ((krb5_make_seq_num(kctx, kctx->seq, kctx->initiate ? 0 : 0xff,\r\nseq_send, ptr + GSS_KRB5_TOK_HDR_LEN, ptr + 8)))\r\nreturn GSS_S_FAILURE;\r\nif (kctx->enctype == ENCTYPE_ARCFOUR_HMAC) {\r\nstruct crypto_blkcipher *cipher;\r\nint err;\r\ncipher = crypto_alloc_blkcipher(kctx->gk5e->encrypt_name, 0,\r\nCRYPTO_ALG_ASYNC);\r\nif (IS_ERR(cipher))\r\nreturn GSS_S_FAILURE;\r\nkrb5_rc4_setup_enc_key(kctx, cipher, seq_send);\r\nerr = gss_encrypt_xdr_buf(cipher, buf,\r\noffset + headlen - conflen, pages);\r\ncrypto_free_blkcipher(cipher);\r\nif (err)\r\nreturn GSS_S_FAILURE;\r\n} else {\r\nif (gss_encrypt_xdr_buf(kctx->enc, buf,\r\noffset + headlen - conflen, pages))\r\nreturn GSS_S_FAILURE;\r\n}\r\nreturn (kctx->endtime < now) ? GSS_S_CONTEXT_EXPIRED : GSS_S_COMPLETE;\r\n}\r\nstatic u32\r\ngss_unwrap_kerberos_v1(struct krb5_ctx *kctx, int offset, struct xdr_buf *buf)\r\n{\r\nint signalg;\r\nint sealalg;\r\nchar cksumdata[GSS_KRB5_MAX_CKSUM_LEN];\r\nstruct xdr_netobj md5cksum = {.len = sizeof(cksumdata),\r\n.data = cksumdata};\r\ns32 now;\r\nint direction;\r\ns32 seqnum;\r\nunsigned char *ptr;\r\nint bodysize;\r\nvoid *data_start, *orig_start;\r\nint data_len;\r\nint blocksize;\r\nu32 conflen = kctx->gk5e->conflen;\r\nint crypt_offset;\r\nu8 *cksumkey;\r\ndprintk("RPC: gss_unwrap_kerberos\n");\r\nptr = (u8 *)buf->head[0].iov_base + offset;\r\nif (g_verify_token_header(&kctx->mech_used, &bodysize, &ptr,\r\nbuf->len - offset))\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\nif ((ptr[0] != ((KG_TOK_WRAP_MSG >> 8) & 0xff)) ||\r\n(ptr[1] != (KG_TOK_WRAP_MSG & 0xff)))\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\nsignalg = ptr[2] + (ptr[3] << 8);\r\nif (signalg != kctx->gk5e->signalg)\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\nsealalg = ptr[4] + (ptr[5] << 8);\r\nif (sealalg != kctx->gk5e->sealalg)\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\nif ((ptr[6] != 0xff) || (ptr[7] != 0xff))\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\ncrypt_offset = ptr + (GSS_KRB5_TOK_HDR_LEN + kctx->gk5e->cksumlength) -\r\n(unsigned char *)buf->head[0].iov_base;\r\nif (krb5_get_seq_num(kctx, ptr + GSS_KRB5_TOK_HDR_LEN,\r\nptr + 8, &direction, &seqnum))\r\nreturn GSS_S_BAD_SIG;\r\nif ((kctx->initiate && direction != 0xff) ||\r\n(!kctx->initiate && direction != 0))\r\nreturn GSS_S_BAD_SIG;\r\nif (kctx->enctype == ENCTYPE_ARCFOUR_HMAC) {\r\nstruct crypto_blkcipher *cipher;\r\nint err;\r\ncipher = crypto_alloc_blkcipher(kctx->gk5e->encrypt_name, 0,\r\nCRYPTO_ALG_ASYNC);\r\nif (IS_ERR(cipher))\r\nreturn GSS_S_FAILURE;\r\nkrb5_rc4_setup_enc_key(kctx, cipher, seqnum);\r\nerr = gss_decrypt_xdr_buf(cipher, buf, crypt_offset);\r\ncrypto_free_blkcipher(cipher);\r\nif (err)\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\n} else {\r\nif (gss_decrypt_xdr_buf(kctx->enc, buf, crypt_offset))\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\n}\r\nif (kctx->gk5e->keyed_cksum)\r\ncksumkey = kctx->cksum;\r\nelse\r\ncksumkey = NULL;\r\nif (make_checksum(kctx, ptr, 8, buf, crypt_offset,\r\ncksumkey, KG_USAGE_SEAL, &md5cksum))\r\nreturn GSS_S_FAILURE;\r\nif (memcmp(md5cksum.data, ptr + GSS_KRB5_TOK_HDR_LEN,\r\nkctx->gk5e->cksumlength))\r\nreturn GSS_S_BAD_SIG;\r\nnow = get_seconds();\r\nif (now > kctx->endtime)\r\nreturn GSS_S_CONTEXT_EXPIRED;\r\nblocksize = crypto_blkcipher_blocksize(kctx->enc);\r\ndata_start = ptr + (GSS_KRB5_TOK_HDR_LEN + kctx->gk5e->cksumlength) +\r\nconflen;\r\norig_start = buf->head[0].iov_base + offset;\r\ndata_len = (buf->head[0].iov_base + buf->head[0].iov_len) - data_start;\r\nmemmove(orig_start, data_start, data_len);\r\nbuf->head[0].iov_len -= (data_start - orig_start);\r\nbuf->len -= (data_start - orig_start);\r\nif (gss_krb5_remove_padding(buf, blocksize))\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\nreturn GSS_S_COMPLETE;\r\n}\r\nstatic void rotate_buf_a_little(struct xdr_buf *buf, unsigned int shift)\r\n{\r\nchar head[LOCAL_BUF_LEN];\r\nchar tmp[LOCAL_BUF_LEN];\r\nunsigned int this_len, i;\r\nBUG_ON(shift > LOCAL_BUF_LEN);\r\nread_bytes_from_xdr_buf(buf, 0, head, shift);\r\nfor (i = 0; i + shift < buf->len; i += LOCAL_BUF_LEN) {\r\nthis_len = min(LOCAL_BUF_LEN, buf->len - (i + shift));\r\nread_bytes_from_xdr_buf(buf, i+shift, tmp, this_len);\r\nwrite_bytes_to_xdr_buf(buf, i, tmp, this_len);\r\n}\r\nwrite_bytes_to_xdr_buf(buf, buf->len - shift, head, shift);\r\n}\r\nstatic void _rotate_left(struct xdr_buf *buf, unsigned int shift)\r\n{\r\nint shifted = 0;\r\nint this_shift;\r\nshift %= buf->len;\r\nwhile (shifted < shift) {\r\nthis_shift = min(shift - shifted, LOCAL_BUF_LEN);\r\nrotate_buf_a_little(buf, this_shift);\r\nshifted += this_shift;\r\n}\r\n}\r\nstatic void rotate_left(u32 base, struct xdr_buf *buf, unsigned int shift)\r\n{\r\nstruct xdr_buf subbuf;\r\nxdr_buf_subsegment(buf, &subbuf, base, buf->len - base);\r\n_rotate_left(&subbuf, shift);\r\n}\r\nstatic u32\r\ngss_wrap_kerberos_v2(struct krb5_ctx *kctx, u32 offset,\r\nstruct xdr_buf *buf, struct page **pages)\r\n{\r\nint blocksize;\r\nu8 *ptr, *plainhdr;\r\ns32 now;\r\nu8 flags = 0x00;\r\n__be16 *be16ptr, ec = 0;\r\n__be64 *be64ptr;\r\nu32 err;\r\ndprintk("RPC: %s\n", __func__);\r\nif (kctx->gk5e->encrypt_v2 == NULL)\r\nreturn GSS_S_FAILURE;\r\nif (xdr_extend_head(buf, offset, GSS_KRB5_TOK_HDR_LEN))\r\nreturn GSS_S_FAILURE;\r\nptr = plainhdr = buf->head[0].iov_base + offset;\r\n*ptr++ = (unsigned char) ((KG2_TOK_WRAP>>8) & 0xff);\r\n*ptr++ = (unsigned char) (KG2_TOK_WRAP & 0xff);\r\nif ((kctx->flags & KRB5_CTX_FLAG_INITIATOR) == 0)\r\nflags |= KG2_TOKEN_FLAG_SENTBYACCEPTOR;\r\nif ((kctx->flags & KRB5_CTX_FLAG_ACCEPTOR_SUBKEY) != 0)\r\nflags |= KG2_TOKEN_FLAG_ACCEPTORSUBKEY;\r\nflags |= KG2_TOKEN_FLAG_SEALED;\r\n*ptr++ = flags;\r\n*ptr++ = 0xff;\r\nbe16ptr = (__be16 *)ptr;\r\nblocksize = crypto_blkcipher_blocksize(kctx->acceptor_enc);\r\n*be16ptr++ = cpu_to_be16(ec);\r\n*be16ptr++ = cpu_to_be16(0);\r\nbe64ptr = (__be64 *)be16ptr;\r\nspin_lock(&krb5_seq_lock);\r\n*be64ptr = cpu_to_be64(kctx->seq_send64++);\r\nspin_unlock(&krb5_seq_lock);\r\nerr = (*kctx->gk5e->encrypt_v2)(kctx, offset, buf, ec, pages);\r\nif (err)\r\nreturn err;\r\nnow = get_seconds();\r\nreturn (kctx->endtime < now) ? GSS_S_CONTEXT_EXPIRED : GSS_S_COMPLETE;\r\n}\r\nstatic u32\r\ngss_unwrap_kerberos_v2(struct krb5_ctx *kctx, int offset, struct xdr_buf *buf)\r\n{\r\ns32 now;\r\nu8 *ptr;\r\nu8 flags = 0x00;\r\nu16 ec, rrc;\r\nint err;\r\nu32 headskip, tailskip;\r\nu8 decrypted_hdr[GSS_KRB5_TOK_HDR_LEN];\r\nunsigned int movelen;\r\ndprintk("RPC: %s\n", __func__);\r\nif (kctx->gk5e->decrypt_v2 == NULL)\r\nreturn GSS_S_FAILURE;\r\nptr = buf->head[0].iov_base + offset;\r\nif (be16_to_cpu(*((__be16 *)ptr)) != KG2_TOK_WRAP)\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\nflags = ptr[2];\r\nif ((!kctx->initiate && (flags & KG2_TOKEN_FLAG_SENTBYACCEPTOR)) ||\r\n(kctx->initiate && !(flags & KG2_TOKEN_FLAG_SENTBYACCEPTOR)))\r\nreturn GSS_S_BAD_SIG;\r\nif ((flags & KG2_TOKEN_FLAG_SEALED) == 0) {\r\ndprintk("%s: token missing expected sealed flag\n", __func__);\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\n}\r\nif (ptr[3] != 0xff)\r\nreturn GSS_S_DEFECTIVE_TOKEN;\r\nec = be16_to_cpup((__be16 *)(ptr + 4));\r\nrrc = be16_to_cpup((__be16 *)(ptr + 6));\r\nif (rrc != 0)\r\nrotate_left(offset + 16, buf, rrc);\r\nerr = (*kctx->gk5e->decrypt_v2)(kctx, offset, buf,\r\n&headskip, &tailskip);\r\nif (err)\r\nreturn GSS_S_FAILURE;\r\nerr = read_bytes_from_xdr_buf(buf,\r\nbuf->len - GSS_KRB5_TOK_HDR_LEN - tailskip,\r\ndecrypted_hdr, GSS_KRB5_TOK_HDR_LEN);\r\nif (err) {\r\ndprintk("%s: error %u getting decrypted_hdr\n", __func__, err);\r\nreturn GSS_S_FAILURE;\r\n}\r\nif (memcmp(ptr, decrypted_hdr, 6)\r\n|| memcmp(ptr + 8, decrypted_hdr + 8, 8)) {\r\ndprintk("%s: token hdr, plaintext hdr mismatch!\n", __func__);\r\nreturn GSS_S_FAILURE;\r\n}\r\nnow = get_seconds();\r\nif (now > kctx->endtime)\r\nreturn GSS_S_CONTEXT_EXPIRED;\r\nmovelen = min_t(unsigned int, buf->head[0].iov_len, buf->len);\r\nmovelen -= offset + GSS_KRB5_TOK_HDR_LEN + headskip;\r\nBUG_ON(offset + GSS_KRB5_TOK_HDR_LEN + headskip + movelen >\r\nbuf->head[0].iov_len);\r\nmemmove(ptr, ptr + GSS_KRB5_TOK_HDR_LEN + headskip, movelen);\r\nbuf->head[0].iov_len -= GSS_KRB5_TOK_HDR_LEN + headskip;\r\nbuf->len -= GSS_KRB5_TOK_HDR_LEN + headskip;\r\nxdr_buf_trim(buf, ec + GSS_KRB5_TOK_HDR_LEN + tailskip);\r\nreturn GSS_S_COMPLETE;\r\n}\r\nu32\r\ngss_wrap_kerberos(struct gss_ctx *gctx, int offset,\r\nstruct xdr_buf *buf, struct page **pages)\r\n{\r\nstruct krb5_ctx *kctx = gctx->internal_ctx_id;\r\nswitch (kctx->enctype) {\r\ndefault:\r\nBUG();\r\ncase ENCTYPE_DES_CBC_RAW:\r\ncase ENCTYPE_DES3_CBC_RAW:\r\ncase ENCTYPE_ARCFOUR_HMAC:\r\nreturn gss_wrap_kerberos_v1(kctx, offset, buf, pages);\r\ncase ENCTYPE_AES128_CTS_HMAC_SHA1_96:\r\ncase ENCTYPE_AES256_CTS_HMAC_SHA1_96:\r\nreturn gss_wrap_kerberos_v2(kctx, offset, buf, pages);\r\n}\r\n}\r\nu32\r\ngss_unwrap_kerberos(struct gss_ctx *gctx, int offset, struct xdr_buf *buf)\r\n{\r\nstruct krb5_ctx *kctx = gctx->internal_ctx_id;\r\nswitch (kctx->enctype) {\r\ndefault:\r\nBUG();\r\ncase ENCTYPE_DES_CBC_RAW:\r\ncase ENCTYPE_DES3_CBC_RAW:\r\ncase ENCTYPE_ARCFOUR_HMAC:\r\nreturn gss_unwrap_kerberos_v1(kctx, offset, buf);\r\ncase ENCTYPE_AES128_CTS_HMAC_SHA1_96:\r\ncase ENCTYPE_AES256_CTS_HMAC_SHA1_96:\r\nreturn gss_unwrap_kerberos_v2(kctx, offset, buf);\r\n}\r\n}
