static void *esp_alloc_tmp(struct crypto_aead *aead, int nfrags, int seqhilen)\r\n{\r\nunsigned int len;\r\nlen = seqhilen;\r\nlen += crypto_aead_ivsize(aead);\r\nif (len) {\r\nlen += crypto_aead_alignmask(aead) &\r\n~(crypto_tfm_ctx_alignment() - 1);\r\nlen = ALIGN(len, crypto_tfm_ctx_alignment());\r\n}\r\nlen += sizeof(struct aead_givcrypt_request) + crypto_aead_reqsize(aead);\r\nlen = ALIGN(len, __alignof__(struct scatterlist));\r\nlen += sizeof(struct scatterlist) * nfrags;\r\nreturn kmalloc(len, GFP_ATOMIC);\r\n}\r\nstatic inline __be32 *esp_tmp_seqhi(void *tmp)\r\n{\r\nreturn PTR_ALIGN((__be32 *)tmp, __alignof__(__be32));\r\n}\r\nstatic inline u8 *esp_tmp_iv(struct crypto_aead *aead, void *tmp, int seqhilen)\r\n{\r\nreturn crypto_aead_ivsize(aead) ?\r\nPTR_ALIGN((u8 *)tmp + seqhilen,\r\ncrypto_aead_alignmask(aead) + 1) : tmp + seqhilen;\r\n}\r\nstatic inline struct aead_givcrypt_request *esp_tmp_givreq(\r\nstruct crypto_aead *aead, u8 *iv)\r\n{\r\nstruct aead_givcrypt_request *req;\r\nreq = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),\r\ncrypto_tfm_ctx_alignment());\r\naead_givcrypt_set_tfm(req, aead);\r\nreturn req;\r\n}\r\nstatic inline struct aead_request *esp_tmp_req(struct crypto_aead *aead, u8 *iv)\r\n{\r\nstruct aead_request *req;\r\nreq = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),\r\ncrypto_tfm_ctx_alignment());\r\naead_request_set_tfm(req, aead);\r\nreturn req;\r\n}\r\nstatic inline struct scatterlist *esp_req_sg(struct crypto_aead *aead,\r\nstruct aead_request *req)\r\n{\r\nreturn (void *)ALIGN((unsigned long)(req + 1) +\r\ncrypto_aead_reqsize(aead),\r\n__alignof__(struct scatterlist));\r\n}\r\nstatic inline struct scatterlist *esp_givreq_sg(\r\nstruct crypto_aead *aead, struct aead_givcrypt_request *req)\r\n{\r\nreturn (void *)ALIGN((unsigned long)(req + 1) +\r\ncrypto_aead_reqsize(aead),\r\n__alignof__(struct scatterlist));\r\n}\r\nstatic void esp_output_done(struct crypto_async_request *base, int err)\r\n{\r\nstruct sk_buff *skb = base->data;\r\nkfree(ESP_SKB_CB(skb)->tmp);\r\nxfrm_output_resume(skb, err);\r\n}\r\nstatic int esp_output(struct xfrm_state *x, struct sk_buff *skb)\r\n{\r\nint err;\r\nstruct ip_esp_hdr *esph;\r\nstruct crypto_aead *aead;\r\nstruct aead_givcrypt_request *req;\r\nstruct scatterlist *sg;\r\nstruct scatterlist *asg;\r\nstruct sk_buff *trailer;\r\nvoid *tmp;\r\nu8 *iv;\r\nu8 *tail;\r\nint blksize;\r\nint clen;\r\nint alen;\r\nint plen;\r\nint tfclen;\r\nint nfrags;\r\nint assoclen;\r\nint sglists;\r\nint seqhilen;\r\n__be32 *seqhi;\r\naead = x->data;\r\nalen = crypto_aead_authsize(aead);\r\ntfclen = 0;\r\nif (x->tfcpad) {\r\nstruct xfrm_dst *dst = (struct xfrm_dst *)skb_dst(skb);\r\nu32 padto;\r\npadto = min(x->tfcpad, esp4_get_mtu(x, dst->child_mtu_cached));\r\nif (skb->len < padto)\r\ntfclen = padto - skb->len;\r\n}\r\nblksize = ALIGN(crypto_aead_blocksize(aead), 4);\r\nclen = ALIGN(skb->len + 2 + tfclen, blksize);\r\nplen = clen - skb->len - tfclen;\r\nerr = skb_cow_data(skb, tfclen + plen + alen, &trailer);\r\nif (err < 0)\r\ngoto error;\r\nnfrags = err;\r\nassoclen = sizeof(*esph);\r\nsglists = 1;\r\nseqhilen = 0;\r\nif (x->props.flags & XFRM_STATE_ESN) {\r\nsglists += 2;\r\nseqhilen += sizeof(__be32);\r\nassoclen += seqhilen;\r\n}\r\ntmp = esp_alloc_tmp(aead, nfrags + sglists, seqhilen);\r\nif (!tmp) {\r\nerr = -ENOMEM;\r\ngoto error;\r\n}\r\nseqhi = esp_tmp_seqhi(tmp);\r\niv = esp_tmp_iv(aead, tmp, seqhilen);\r\nreq = esp_tmp_givreq(aead, iv);\r\nasg = esp_givreq_sg(aead, req);\r\nsg = asg + sglists;\r\ntail = skb_tail_pointer(trailer);\r\nif (tfclen) {\r\nmemset(tail, 0, tfclen);\r\ntail += tfclen;\r\n}\r\ndo {\r\nint i;\r\nfor (i = 0; i < plen - 2; i++)\r\ntail[i] = i + 1;\r\n} while (0);\r\ntail[plen - 2] = plen - 2;\r\ntail[plen - 1] = *skb_mac_header(skb);\r\npskb_put(skb, trailer, clen - skb->len + alen);\r\nskb_push(skb, -skb_network_offset(skb));\r\nesph = ip_esp_hdr(skb);\r\n*skb_mac_header(skb) = IPPROTO_ESP;\r\nif (x->encap) {\r\nstruct xfrm_encap_tmpl *encap = x->encap;\r\nstruct udphdr *uh;\r\n__be32 *udpdata32;\r\n__be16 sport, dport;\r\nint encap_type;\r\nspin_lock_bh(&x->lock);\r\nsport = encap->encap_sport;\r\ndport = encap->encap_dport;\r\nencap_type = encap->encap_type;\r\nspin_unlock_bh(&x->lock);\r\nuh = (struct udphdr *)esph;\r\nuh->source = sport;\r\nuh->dest = dport;\r\nuh->len = htons(skb->len - skb_transport_offset(skb));\r\nuh->check = 0;\r\nswitch (encap_type) {\r\ndefault:\r\ncase UDP_ENCAP_ESPINUDP:\r\nesph = (struct ip_esp_hdr *)(uh + 1);\r\nbreak;\r\ncase UDP_ENCAP_ESPINUDP_NON_IKE:\r\nudpdata32 = (__be32 *)(uh + 1);\r\nudpdata32[0] = udpdata32[1] = 0;\r\nesph = (struct ip_esp_hdr *)(udpdata32 + 2);\r\nbreak;\r\n}\r\n*skb_mac_header(skb) = IPPROTO_UDP;\r\n}\r\nesph->spi = x->id.spi;\r\nesph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);\r\nsg_init_table(sg, nfrags);\r\nskb_to_sgvec(skb, sg,\r\nesph->enc_data + crypto_aead_ivsize(aead) - skb->data,\r\nclen + alen);\r\nif ((x->props.flags & XFRM_STATE_ESN)) {\r\nsg_init_table(asg, 3);\r\nsg_set_buf(asg, &esph->spi, sizeof(__be32));\r\n*seqhi = htonl(XFRM_SKB_CB(skb)->seq.output.hi);\r\nsg_set_buf(asg + 1, seqhi, seqhilen);\r\nsg_set_buf(asg + 2, &esph->seq_no, sizeof(__be32));\r\n} else\r\nsg_init_one(asg, esph, sizeof(*esph));\r\naead_givcrypt_set_callback(req, 0, esp_output_done, skb);\r\naead_givcrypt_set_crypt(req, sg, sg, clen, iv);\r\naead_givcrypt_set_assoc(req, asg, assoclen);\r\naead_givcrypt_set_giv(req, esph->enc_data,\r\nXFRM_SKB_CB(skb)->seq.output.low);\r\nESP_SKB_CB(skb)->tmp = tmp;\r\nerr = crypto_aead_givencrypt(req);\r\nif (err == -EINPROGRESS)\r\ngoto error;\r\nif (err == -EBUSY)\r\nerr = NET_XMIT_DROP;\r\nkfree(tmp);\r\nerror:\r\nreturn err;\r\n}\r\nstatic int esp_input_done2(struct sk_buff *skb, int err)\r\n{\r\nconst struct iphdr *iph;\r\nstruct xfrm_state *x = xfrm_input_state(skb);\r\nstruct crypto_aead *aead = x->data;\r\nint alen = crypto_aead_authsize(aead);\r\nint hlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\r\nint elen = skb->len - hlen;\r\nint ihl;\r\nu8 nexthdr[2];\r\nint padlen;\r\nkfree(ESP_SKB_CB(skb)->tmp);\r\nif (unlikely(err))\r\ngoto out;\r\nif (skb_copy_bits(skb, skb->len-alen-2, nexthdr, 2))\r\nBUG();\r\nerr = -EINVAL;\r\npadlen = nexthdr[0];\r\nif (padlen + 2 + alen >= elen)\r\ngoto out;\r\niph = ip_hdr(skb);\r\nihl = iph->ihl * 4;\r\nif (x->encap) {\r\nstruct xfrm_encap_tmpl *encap = x->encap;\r\nstruct udphdr *uh = (void *)(skb_network_header(skb) + ihl);\r\nif (iph->saddr != x->props.saddr.a4 ||\r\nuh->source != encap->encap_sport) {\r\nxfrm_address_t ipaddr;\r\nipaddr.a4 = iph->saddr;\r\nkm_new_mapping(x, &ipaddr, uh->source);\r\n}\r\nif (x->props.mode == XFRM_MODE_TRANSPORT)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n}\r\npskb_trim(skb, skb->len - alen - padlen - 2);\r\n__skb_pull(skb, hlen);\r\nif (x->props.mode == XFRM_MODE_TUNNEL)\r\nskb_reset_transport_header(skb);\r\nelse\r\nskb_set_transport_header(skb, -ihl);\r\nerr = nexthdr[1];\r\nif (err == IPPROTO_NONE)\r\nerr = -EINVAL;\r\nout:\r\nreturn err;\r\n}\r\nstatic void esp_input_done(struct crypto_async_request *base, int err)\r\n{\r\nstruct sk_buff *skb = base->data;\r\nxfrm_input_resume(skb, esp_input_done2(skb, err));\r\n}\r\nstatic int esp_input(struct xfrm_state *x, struct sk_buff *skb)\r\n{\r\nstruct ip_esp_hdr *esph;\r\nstruct crypto_aead *aead = x->data;\r\nstruct aead_request *req;\r\nstruct sk_buff *trailer;\r\nint elen = skb->len - sizeof(*esph) - crypto_aead_ivsize(aead);\r\nint nfrags;\r\nint assoclen;\r\nint sglists;\r\nint seqhilen;\r\n__be32 *seqhi;\r\nvoid *tmp;\r\nu8 *iv;\r\nstruct scatterlist *sg;\r\nstruct scatterlist *asg;\r\nint err = -EINVAL;\r\nif (!pskb_may_pull(skb, sizeof(*esph) + crypto_aead_ivsize(aead)))\r\ngoto out;\r\nif (elen <= 0)\r\ngoto out;\r\nif ((err = skb_cow_data(skb, 0, &trailer)) < 0)\r\ngoto out;\r\nnfrags = err;\r\nassoclen = sizeof(*esph);\r\nsglists = 1;\r\nseqhilen = 0;\r\nif (x->props.flags & XFRM_STATE_ESN) {\r\nsglists += 2;\r\nseqhilen += sizeof(__be32);\r\nassoclen += seqhilen;\r\n}\r\nerr = -ENOMEM;\r\ntmp = esp_alloc_tmp(aead, nfrags + sglists, seqhilen);\r\nif (!tmp)\r\ngoto out;\r\nESP_SKB_CB(skb)->tmp = tmp;\r\nseqhi = esp_tmp_seqhi(tmp);\r\niv = esp_tmp_iv(aead, tmp, seqhilen);\r\nreq = esp_tmp_req(aead, iv);\r\nasg = esp_req_sg(aead, req);\r\nsg = asg + sglists;\r\nskb->ip_summed = CHECKSUM_NONE;\r\nesph = (struct ip_esp_hdr *)skb->data;\r\niv = esph->enc_data;\r\nsg_init_table(sg, nfrags);\r\nskb_to_sgvec(skb, sg, sizeof(*esph) + crypto_aead_ivsize(aead), elen);\r\nif ((x->props.flags & XFRM_STATE_ESN)) {\r\nsg_init_table(asg, 3);\r\nsg_set_buf(asg, &esph->spi, sizeof(__be32));\r\n*seqhi = XFRM_SKB_CB(skb)->seq.input.hi;\r\nsg_set_buf(asg + 1, seqhi, seqhilen);\r\nsg_set_buf(asg + 2, &esph->seq_no, sizeof(__be32));\r\n} else\r\nsg_init_one(asg, esph, sizeof(*esph));\r\naead_request_set_callback(req, 0, esp_input_done, skb);\r\naead_request_set_crypt(req, sg, sg, elen, iv);\r\naead_request_set_assoc(req, asg, assoclen);\r\nerr = crypto_aead_decrypt(req);\r\nif (err == -EINPROGRESS)\r\ngoto out;\r\nerr = esp_input_done2(skb, err);\r\nout:\r\nreturn err;\r\n}\r\nstatic u32 esp4_get_mtu(struct xfrm_state *x, int mtu)\r\n{\r\nstruct crypto_aead *aead = x->data;\r\nu32 blksize = ALIGN(crypto_aead_blocksize(aead), 4);\r\nunsigned int net_adj;\r\nswitch (x->props.mode) {\r\ncase XFRM_MODE_TRANSPORT:\r\ncase XFRM_MODE_BEET:\r\nnet_adj = sizeof(struct iphdr);\r\nbreak;\r\ncase XFRM_MODE_TUNNEL:\r\nnet_adj = 0;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn ((mtu - x->props.header_len - crypto_aead_authsize(aead) -\r\nnet_adj) & ~(blksize - 1)) + net_adj - 2;\r\n}\r\nstatic void esp4_err(struct sk_buff *skb, u32 info)\r\n{\r\nstruct net *net = dev_net(skb->dev);\r\nconst struct iphdr *iph = (const struct iphdr *)skb->data;\r\nstruct ip_esp_hdr *esph = (struct ip_esp_hdr *)(skb->data+(iph->ihl<<2));\r\nstruct xfrm_state *x;\r\nswitch (icmp_hdr(skb)->type) {\r\ncase ICMP_DEST_UNREACH:\r\nif (icmp_hdr(skb)->code != ICMP_FRAG_NEEDED)\r\nreturn;\r\ncase ICMP_REDIRECT:\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nx = xfrm_state_lookup(net, skb->mark, (const xfrm_address_t *)&iph->daddr,\r\nesph->spi, IPPROTO_ESP, AF_INET);\r\nif (!x)\r\nreturn;\r\nif (icmp_hdr(skb)->type == ICMP_DEST_UNREACH)\r\nipv4_update_pmtu(skb, net, info, 0, 0, IPPROTO_ESP, 0);\r\nelse\r\nipv4_redirect(skb, net, 0, 0, IPPROTO_ESP, 0);\r\nxfrm_state_put(x);\r\n}\r\nstatic void esp_destroy(struct xfrm_state *x)\r\n{\r\nstruct crypto_aead *aead = x->data;\r\nif (!aead)\r\nreturn;\r\ncrypto_free_aead(aead);\r\n}\r\nstatic int esp_init_aead(struct xfrm_state *x)\r\n{\r\nstruct crypto_aead *aead;\r\nint err;\r\naead = crypto_alloc_aead(x->aead->alg_name, 0, 0);\r\nerr = PTR_ERR(aead);\r\nif (IS_ERR(aead))\r\ngoto error;\r\nx->data = aead;\r\nerr = crypto_aead_setkey(aead, x->aead->alg_key,\r\n(x->aead->alg_key_len + 7) / 8);\r\nif (err)\r\ngoto error;\r\nerr = crypto_aead_setauthsize(aead, x->aead->alg_icv_len / 8);\r\nif (err)\r\ngoto error;\r\nerror:\r\nreturn err;\r\n}\r\nstatic int esp_init_authenc(struct xfrm_state *x)\r\n{\r\nstruct crypto_aead *aead;\r\nstruct crypto_authenc_key_param *param;\r\nstruct rtattr *rta;\r\nchar *key;\r\nchar *p;\r\nchar authenc_name[CRYPTO_MAX_ALG_NAME];\r\nunsigned int keylen;\r\nint err;\r\nerr = -EINVAL;\r\nif (x->ealg == NULL)\r\ngoto error;\r\nerr = -ENAMETOOLONG;\r\nif ((x->props.flags & XFRM_STATE_ESN)) {\r\nif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\r\n"authencesn(%s,%s)",\r\nx->aalg ? x->aalg->alg_name : "digest_null",\r\nx->ealg->alg_name) >= CRYPTO_MAX_ALG_NAME)\r\ngoto error;\r\n} else {\r\nif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\r\n"authenc(%s,%s)",\r\nx->aalg ? x->aalg->alg_name : "digest_null",\r\nx->ealg->alg_name) >= CRYPTO_MAX_ALG_NAME)\r\ngoto error;\r\n}\r\naead = crypto_alloc_aead(authenc_name, 0, 0);\r\nerr = PTR_ERR(aead);\r\nif (IS_ERR(aead))\r\ngoto error;\r\nx->data = aead;\r\nkeylen = (x->aalg ? (x->aalg->alg_key_len + 7) / 8 : 0) +\r\n(x->ealg->alg_key_len + 7) / 8 + RTA_SPACE(sizeof(*param));\r\nerr = -ENOMEM;\r\nkey = kmalloc(keylen, GFP_KERNEL);\r\nif (!key)\r\ngoto error;\r\np = key;\r\nrta = (void *)p;\r\nrta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;\r\nrta->rta_len = RTA_LENGTH(sizeof(*param));\r\nparam = RTA_DATA(rta);\r\np += RTA_SPACE(sizeof(*param));\r\nif (x->aalg) {\r\nstruct xfrm_algo_desc *aalg_desc;\r\nmemcpy(p, x->aalg->alg_key, (x->aalg->alg_key_len + 7) / 8);\r\np += (x->aalg->alg_key_len + 7) / 8;\r\naalg_desc = xfrm_aalg_get_byname(x->aalg->alg_name, 0);\r\nBUG_ON(!aalg_desc);\r\nerr = -EINVAL;\r\nif (aalg_desc->uinfo.auth.icv_fullbits/8 !=\r\ncrypto_aead_authsize(aead)) {\r\nNETDEBUG(KERN_INFO "ESP: %s digestsize %u != %hu\n",\r\nx->aalg->alg_name,\r\ncrypto_aead_authsize(aead),\r\naalg_desc->uinfo.auth.icv_fullbits/8);\r\ngoto free_key;\r\n}\r\nerr = crypto_aead_setauthsize(\r\naead, x->aalg->alg_trunc_len / 8);\r\nif (err)\r\ngoto free_key;\r\n}\r\nparam->enckeylen = cpu_to_be32((x->ealg->alg_key_len + 7) / 8);\r\nmemcpy(p, x->ealg->alg_key, (x->ealg->alg_key_len + 7) / 8);\r\nerr = crypto_aead_setkey(aead, key, keylen);\r\nfree_key:\r\nkfree(key);\r\nerror:\r\nreturn err;\r\n}\r\nstatic int esp_init_state(struct xfrm_state *x)\r\n{\r\nstruct crypto_aead *aead;\r\nu32 align;\r\nint err;\r\nx->data = NULL;\r\nif (x->aead)\r\nerr = esp_init_aead(x);\r\nelse\r\nerr = esp_init_authenc(x);\r\nif (err)\r\ngoto error;\r\naead = x->data;\r\nx->props.header_len = sizeof(struct ip_esp_hdr) +\r\ncrypto_aead_ivsize(aead);\r\nif (x->props.mode == XFRM_MODE_TUNNEL)\r\nx->props.header_len += sizeof(struct iphdr);\r\nelse if (x->props.mode == XFRM_MODE_BEET && x->sel.family != AF_INET6)\r\nx->props.header_len += IPV4_BEET_PHMAXLEN;\r\nif (x->encap) {\r\nstruct xfrm_encap_tmpl *encap = x->encap;\r\nswitch (encap->encap_type) {\r\ndefault:\r\ngoto error;\r\ncase UDP_ENCAP_ESPINUDP:\r\nx->props.header_len += sizeof(struct udphdr);\r\nbreak;\r\ncase UDP_ENCAP_ESPINUDP_NON_IKE:\r\nx->props.header_len += sizeof(struct udphdr) + 2 * sizeof(u32);\r\nbreak;\r\n}\r\n}\r\nalign = ALIGN(crypto_aead_blocksize(aead), 4);\r\nx->props.trailer_len = align + 1 + crypto_aead_authsize(aead);\r\nerror:\r\nreturn err;\r\n}\r\nstatic int __init esp4_init(void)\r\n{\r\nif (xfrm_register_type(&esp_type, AF_INET) < 0) {\r\npr_info("%s: can't add xfrm type\n", __func__);\r\nreturn -EAGAIN;\r\n}\r\nif (inet_add_protocol(&esp4_protocol, IPPROTO_ESP) < 0) {\r\npr_info("%s: can't add protocol\n", __func__);\r\nxfrm_unregister_type(&esp_type, AF_INET);\r\nreturn -EAGAIN;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __exit esp4_fini(void)\r\n{\r\nif (inet_del_protocol(&esp4_protocol, IPPROTO_ESP) < 0)\r\npr_info("%s: can't remove protocol\n", __func__);\r\nif (xfrm_unregister_type(&esp_type, AF_INET) < 0)\r\npr_info("%s: can't remove xfrm type\n", __func__);\r\n}
