static void ip_vs_lblc_rcu_free(struct rcu_head *head)\r\n{\r\nstruct ip_vs_lblc_entry *en = container_of(head,\r\nstruct ip_vs_lblc_entry,\r\nrcu_head);\r\nip_vs_dest_put_and_free(en->dest);\r\nkfree(en);\r\n}\r\nstatic inline void ip_vs_lblc_del(struct ip_vs_lblc_entry *en)\r\n{\r\nhlist_del_rcu(&en->list);\r\ncall_rcu(&en->rcu_head, ip_vs_lblc_rcu_free);\r\n}\r\nstatic inline unsigned int\r\nip_vs_lblc_hashkey(int af, const union nf_inet_addr *addr)\r\n{\r\n__be32 addr_fold = addr->ip;\r\n#ifdef CONFIG_IP_VS_IPV6\r\nif (af == AF_INET6)\r\naddr_fold = addr->ip6[0]^addr->ip6[1]^\r\naddr->ip6[2]^addr->ip6[3];\r\n#endif\r\nreturn (ntohl(addr_fold)*2654435761UL) & IP_VS_LBLC_TAB_MASK;\r\n}\r\nstatic void\r\nip_vs_lblc_hash(struct ip_vs_lblc_table *tbl, struct ip_vs_lblc_entry *en)\r\n{\r\nunsigned int hash = ip_vs_lblc_hashkey(en->af, &en->addr);\r\nhlist_add_head_rcu(&en->list, &tbl->bucket[hash]);\r\natomic_inc(&tbl->entries);\r\n}\r\nstatic inline struct ip_vs_lblc_entry *\r\nip_vs_lblc_get(int af, struct ip_vs_lblc_table *tbl,\r\nconst union nf_inet_addr *addr)\r\n{\r\nunsigned int hash = ip_vs_lblc_hashkey(af, addr);\r\nstruct ip_vs_lblc_entry *en;\r\nhlist_for_each_entry_rcu(en, &tbl->bucket[hash], list)\r\nif (ip_vs_addr_equal(af, &en->addr, addr))\r\nreturn en;\r\nreturn NULL;\r\n}\r\nstatic inline struct ip_vs_lblc_entry *\r\nip_vs_lblc_new(struct ip_vs_lblc_table *tbl, const union nf_inet_addr *daddr,\r\nstruct ip_vs_dest *dest)\r\n{\r\nstruct ip_vs_lblc_entry *en;\r\nen = ip_vs_lblc_get(dest->af, tbl, daddr);\r\nif (en) {\r\nif (en->dest == dest)\r\nreturn en;\r\nip_vs_lblc_del(en);\r\n}\r\nen = kmalloc(sizeof(*en), GFP_ATOMIC);\r\nif (!en)\r\nreturn NULL;\r\nen->af = dest->af;\r\nip_vs_addr_copy(dest->af, &en->addr, daddr);\r\nen->lastuse = jiffies;\r\nip_vs_dest_hold(dest);\r\nen->dest = dest;\r\nip_vs_lblc_hash(tbl, en);\r\nreturn en;\r\n}\r\nstatic void ip_vs_lblc_flush(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\nstruct ip_vs_lblc_entry *en;\r\nstruct hlist_node *next;\r\nint i;\r\nspin_lock_bh(&svc->sched_lock);\r\ntbl->dead = 1;\r\nfor (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nhlist_for_each_entry_safe(en, next, &tbl->bucket[i], list) {\r\nip_vs_lblc_del(en);\r\natomic_dec(&tbl->entries);\r\n}\r\n}\r\nspin_unlock_bh(&svc->sched_lock);\r\n}\r\nstatic int sysctl_lblc_expiration(struct ip_vs_service *svc)\r\n{\r\n#ifdef CONFIG_SYSCTL\r\nstruct netns_ipvs *ipvs = net_ipvs(svc->net);\r\nreturn ipvs->sysctl_lblc_expiration;\r\n#else\r\nreturn DEFAULT_EXPIRATION;\r\n#endif\r\n}\r\nstatic inline void ip_vs_lblc_full_check(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\nstruct ip_vs_lblc_entry *en;\r\nstruct hlist_node *next;\r\nunsigned long now = jiffies;\r\nint i, j;\r\nfor (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLC_TAB_MASK;\r\nspin_lock(&svc->sched_lock);\r\nhlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {\r\nif (time_before(now,\r\nen->lastuse +\r\nsysctl_lblc_expiration(svc)))\r\ncontinue;\r\nip_vs_lblc_del(en);\r\natomic_dec(&tbl->entries);\r\n}\r\nspin_unlock(&svc->sched_lock);\r\n}\r\ntbl->rover = j;\r\n}\r\nstatic void ip_vs_lblc_check_expire(unsigned long data)\r\n{\r\nstruct ip_vs_service *svc = (struct ip_vs_service *) data;\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\nunsigned long now = jiffies;\r\nint goal;\r\nint i, j;\r\nstruct ip_vs_lblc_entry *en;\r\nstruct hlist_node *next;\r\nif ((tbl->counter % COUNT_FOR_FULL_EXPIRATION) == 0) {\r\nip_vs_lblc_full_check(svc);\r\ntbl->counter = 1;\r\ngoto out;\r\n}\r\nif (atomic_read(&tbl->entries) <= tbl->max_size) {\r\ntbl->counter++;\r\ngoto out;\r\n}\r\ngoal = (atomic_read(&tbl->entries) - tbl->max_size)*4/3;\r\nif (goal > tbl->max_size/2)\r\ngoal = tbl->max_size/2;\r\nfor (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLC_TAB_MASK;\r\nspin_lock(&svc->sched_lock);\r\nhlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {\r\nif (time_before(now, en->lastuse + ENTRY_TIMEOUT))\r\ncontinue;\r\nip_vs_lblc_del(en);\r\natomic_dec(&tbl->entries);\r\ngoal--;\r\n}\r\nspin_unlock(&svc->sched_lock);\r\nif (goal <= 0)\r\nbreak;\r\n}\r\ntbl->rover = j;\r\nout:\r\nmod_timer(&tbl->periodic_timer, jiffies+CHECK_EXPIRE_INTERVAL);\r\n}\r\nstatic int ip_vs_lblc_init_svc(struct ip_vs_service *svc)\r\n{\r\nint i;\r\nstruct ip_vs_lblc_table *tbl;\r\ntbl = kmalloc(sizeof(*tbl), GFP_KERNEL);\r\nif (tbl == NULL)\r\nreturn -ENOMEM;\r\nsvc->sched_data = tbl;\r\nIP_VS_DBG(6, "LBLC hash table (memory=%Zdbytes) allocated for "\r\n"current service\n", sizeof(*tbl));\r\nfor (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nINIT_HLIST_HEAD(&tbl->bucket[i]);\r\n}\r\ntbl->max_size = IP_VS_LBLC_TAB_SIZE*16;\r\ntbl->rover = 0;\r\ntbl->counter = 1;\r\ntbl->dead = 0;\r\nsetup_timer(&tbl->periodic_timer, ip_vs_lblc_check_expire,\r\n(unsigned long)svc);\r\nmod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);\r\nreturn 0;\r\n}\r\nstatic void ip_vs_lblc_done_svc(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\ndel_timer_sync(&tbl->periodic_timer);\r\nip_vs_lblc_flush(svc);\r\nkfree_rcu(tbl, rcu_head);\r\nIP_VS_DBG(6, "LBLC hash table (memory=%Zdbytes) released\n",\r\nsizeof(*tbl));\r\n}\r\nstatic inline struct ip_vs_dest *\r\n__ip_vs_lblc_schedule(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_dest *dest, *least;\r\nint loh, doh;\r\nlist_for_each_entry_rcu(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\nif (atomic_read(&dest->weight) > 0) {\r\nleast = dest;\r\nloh = ip_vs_dest_conn_overhead(least);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry_continue_rcu(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif ((__s64)loh * atomic_read(&dest->weight) >\r\n(__s64)doh * atomic_read(&least->weight)) {\r\nleast = dest;\r\nloh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "LBLC: server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\nIP_VS_DBG_ADDR(least->af, &least->addr),\r\nntohs(least->port),\r\natomic_read(&least->activeconns),\r\natomic_read(&least->refcnt),\r\natomic_read(&least->weight), loh);\r\nreturn least;\r\n}\r\nstatic inline int\r\nis_overloaded(struct ip_vs_dest *dest, struct ip_vs_service *svc)\r\n{\r\nif (atomic_read(&dest->activeconns) > atomic_read(&dest->weight)) {\r\nstruct ip_vs_dest *d;\r\nlist_for_each_entry_rcu(d, &svc->destinations, n_list) {\r\nif (atomic_read(&d->activeconns)*2\r\n< atomic_read(&d->weight)) {\r\nreturn 1;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic struct ip_vs_dest *\r\nip_vs_lblc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb,\r\nstruct ip_vs_iphdr *iph)\r\n{\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\nstruct ip_vs_dest *dest = NULL;\r\nstruct ip_vs_lblc_entry *en;\r\nIP_VS_DBG(6, "%s(): Scheduling...\n", __func__);\r\nen = ip_vs_lblc_get(svc->af, tbl, &iph->daddr);\r\nif (en) {\r\nen->lastuse = jiffies;\r\ndest = en->dest;\r\nif ((dest->flags & IP_VS_DEST_F_AVAILABLE) &&\r\natomic_read(&dest->weight) > 0 && !is_overloaded(dest, svc))\r\ngoto out;\r\n}\r\ndest = __ip_vs_lblc_schedule(svc);\r\nif (!dest) {\r\nip_vs_scheduler_err(svc, "no destination available");\r\nreturn NULL;\r\n}\r\nspin_lock_bh(&svc->sched_lock);\r\nif (!tbl->dead)\r\nip_vs_lblc_new(tbl, &iph->daddr, dest);\r\nspin_unlock_bh(&svc->sched_lock);\r\nout:\r\nIP_VS_DBG_BUF(6, "LBLC: destination IP address %s --> server %s:%d\n",\r\nIP_VS_DBG_ADDR(svc->af, &iph->daddr),\r\nIP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port));\r\nreturn dest;\r\n}\r\nstatic int __net_init __ip_vs_lblc_init(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nif (!ipvs)\r\nreturn -ENOENT;\r\nif (!net_eq(net, &init_net)) {\r\nipvs->lblc_ctl_table = kmemdup(vs_vars_table,\r\nsizeof(vs_vars_table),\r\nGFP_KERNEL);\r\nif (ipvs->lblc_ctl_table == NULL)\r\nreturn -ENOMEM;\r\nif (net->user_ns != &init_user_ns)\r\nipvs->lblc_ctl_table[0].procname = NULL;\r\n} else\r\nipvs->lblc_ctl_table = vs_vars_table;\r\nipvs->sysctl_lblc_expiration = DEFAULT_EXPIRATION;\r\nipvs->lblc_ctl_table[0].data = &ipvs->sysctl_lblc_expiration;\r\nipvs->lblc_ctl_header =\r\nregister_net_sysctl(net, "net/ipv4/vs", ipvs->lblc_ctl_table);\r\nif (!ipvs->lblc_ctl_header) {\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblc_ctl_table);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __net_exit __ip_vs_lblc_exit(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nunregister_net_sysctl_table(ipvs->lblc_ctl_header);\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblc_ctl_table);\r\n}\r\nstatic int __net_init __ip_vs_lblc_init(struct net *net) { return 0; }\r\nstatic void __net_exit __ip_vs_lblc_exit(struct net *net) { }\r\nstatic int __init ip_vs_lblc_init(void)\r\n{\r\nint ret;\r\nret = register_pernet_subsys(&ip_vs_lblc_ops);\r\nif (ret)\r\nreturn ret;\r\nret = register_ip_vs_scheduler(&ip_vs_lblc_scheduler);\r\nif (ret)\r\nunregister_pernet_subsys(&ip_vs_lblc_ops);\r\nreturn ret;\r\n}\r\nstatic void __exit ip_vs_lblc_cleanup(void)\r\n{\r\nunregister_ip_vs_scheduler(&ip_vs_lblc_scheduler);\r\nunregister_pernet_subsys(&ip_vs_lblc_ops);\r\nrcu_barrier();\r\n}
