static void *get_wqe(struct mthca_srq *srq, int n)\r\n{\r\nif (srq->is_direct)\r\nreturn srq->queue.direct.buf + (n << srq->wqe_shift);\r\nelse\r\nreturn srq->queue.page_list[(n << srq->wqe_shift) >> PAGE_SHIFT].buf +\r\n((n << srq->wqe_shift) & (PAGE_SIZE - 1));\r\n}\r\nstatic inline int *wqe_to_link(void *wqe)\r\n{\r\nreturn (int *) (wqe + offsetof(struct mthca_next_seg, imm));\r\n}\r\nstatic void mthca_tavor_init_srq_context(struct mthca_dev *dev,\r\nstruct mthca_pd *pd,\r\nstruct mthca_srq *srq,\r\nstruct mthca_tavor_srq_context *context)\r\n{\r\nmemset(context, 0, sizeof *context);\r\ncontext->wqe_base_ds = cpu_to_be64(1 << (srq->wqe_shift - 4));\r\ncontext->state_pd = cpu_to_be32(pd->pd_num);\r\ncontext->lkey = cpu_to_be32(srq->mr.ibmr.lkey);\r\nif (pd->ibpd.uobject)\r\ncontext->uar =\r\ncpu_to_be32(to_mucontext(pd->ibpd.uobject->context)->uar.index);\r\nelse\r\ncontext->uar = cpu_to_be32(dev->driver_uar.index);\r\n}\r\nstatic void mthca_arbel_init_srq_context(struct mthca_dev *dev,\r\nstruct mthca_pd *pd,\r\nstruct mthca_srq *srq,\r\nstruct mthca_arbel_srq_context *context)\r\n{\r\nint logsize, max;\r\nmemset(context, 0, sizeof *context);\r\nmax = srq->max;\r\nlogsize = ilog2(max);\r\ncontext->state_logsize_srqn = cpu_to_be32(logsize << 24 | srq->srqn);\r\ncontext->lkey = cpu_to_be32(srq->mr.ibmr.lkey);\r\ncontext->db_index = cpu_to_be32(srq->db_index);\r\ncontext->logstride_usrpage = cpu_to_be32((srq->wqe_shift - 4) << 29);\r\nif (pd->ibpd.uobject)\r\ncontext->logstride_usrpage |=\r\ncpu_to_be32(to_mucontext(pd->ibpd.uobject->context)->uar.index);\r\nelse\r\ncontext->logstride_usrpage |= cpu_to_be32(dev->driver_uar.index);\r\ncontext->eq_pd = cpu_to_be32(MTHCA_EQ_ASYNC << 24 | pd->pd_num);\r\n}\r\nstatic void mthca_free_srq_buf(struct mthca_dev *dev, struct mthca_srq *srq)\r\n{\r\nmthca_buf_free(dev, srq->max << srq->wqe_shift, &srq->queue,\r\nsrq->is_direct, &srq->mr);\r\nkfree(srq->wrid);\r\n}\r\nstatic int mthca_alloc_srq_buf(struct mthca_dev *dev, struct mthca_pd *pd,\r\nstruct mthca_srq *srq)\r\n{\r\nstruct mthca_data_seg *scatter;\r\nvoid *wqe;\r\nint err;\r\nint i;\r\nif (pd->ibpd.uobject)\r\nreturn 0;\r\nsrq->wrid = kmalloc(srq->max * sizeof (u64), GFP_KERNEL);\r\nif (!srq->wrid)\r\nreturn -ENOMEM;\r\nerr = mthca_buf_alloc(dev, srq->max << srq->wqe_shift,\r\nMTHCA_MAX_DIRECT_SRQ_SIZE,\r\n&srq->queue, &srq->is_direct, pd, 1, &srq->mr);\r\nif (err) {\r\nkfree(srq->wrid);\r\nreturn err;\r\n}\r\nfor (i = 0; i < srq->max; ++i) {\r\nstruct mthca_next_seg *next;\r\nnext = wqe = get_wqe(srq, i);\r\nif (i < srq->max - 1) {\r\n*wqe_to_link(wqe) = i + 1;\r\nnext->nda_op = htonl(((i + 1) << srq->wqe_shift) | 1);\r\n} else {\r\n*wqe_to_link(wqe) = -1;\r\nnext->nda_op = 0;\r\n}\r\nfor (scatter = wqe + sizeof (struct mthca_next_seg);\r\n(void *) scatter < wqe + (1 << srq->wqe_shift);\r\n++scatter)\r\nscatter->lkey = cpu_to_be32(MTHCA_INVAL_LKEY);\r\n}\r\nsrq->last = get_wqe(srq, srq->max - 1);\r\nreturn 0;\r\n}\r\nint mthca_alloc_srq(struct mthca_dev *dev, struct mthca_pd *pd,\r\nstruct ib_srq_attr *attr, struct mthca_srq *srq)\r\n{\r\nstruct mthca_mailbox *mailbox;\r\nint ds;\r\nint err;\r\nif (attr->max_wr > dev->limits.max_srq_wqes ||\r\nattr->max_sge > dev->limits.max_srq_sge)\r\nreturn -EINVAL;\r\nsrq->max = attr->max_wr;\r\nsrq->max_gs = attr->max_sge;\r\nsrq->counter = 0;\r\nif (mthca_is_memfree(dev))\r\nsrq->max = roundup_pow_of_two(srq->max + 1);\r\nelse\r\nsrq->max = srq->max + 1;\r\nds = max(64UL,\r\nroundup_pow_of_two(sizeof (struct mthca_next_seg) +\r\nsrq->max_gs * sizeof (struct mthca_data_seg)));\r\nif (!mthca_is_memfree(dev) && (ds > dev->limits.max_desc_sz))\r\nreturn -EINVAL;\r\nsrq->wqe_shift = ilog2(ds);\r\nsrq->srqn = mthca_alloc(&dev->srq_table.alloc);\r\nif (srq->srqn == -1)\r\nreturn -ENOMEM;\r\nif (mthca_is_memfree(dev)) {\r\nerr = mthca_table_get(dev, dev->srq_table.table, srq->srqn);\r\nif (err)\r\ngoto err_out;\r\nif (!pd->ibpd.uobject) {\r\nsrq->db_index = mthca_alloc_db(dev, MTHCA_DB_TYPE_SRQ,\r\nsrq->srqn, &srq->db);\r\nif (srq->db_index < 0) {\r\nerr = -ENOMEM;\r\ngoto err_out_icm;\r\n}\r\n}\r\n}\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox)) {\r\nerr = PTR_ERR(mailbox);\r\ngoto err_out_db;\r\n}\r\nerr = mthca_alloc_srq_buf(dev, pd, srq);\r\nif (err)\r\ngoto err_out_mailbox;\r\nspin_lock_init(&srq->lock);\r\nsrq->refcount = 1;\r\ninit_waitqueue_head(&srq->wait);\r\nmutex_init(&srq->mutex);\r\nif (mthca_is_memfree(dev))\r\nmthca_arbel_init_srq_context(dev, pd, srq, mailbox->buf);\r\nelse\r\nmthca_tavor_init_srq_context(dev, pd, srq, mailbox->buf);\r\nerr = mthca_SW2HW_SRQ(dev, mailbox, srq->srqn);\r\nif (err) {\r\nmthca_warn(dev, "SW2HW_SRQ failed (%d)\n", err);\r\ngoto err_out_free_buf;\r\n}\r\nspin_lock_irq(&dev->srq_table.lock);\r\nif (mthca_array_set(&dev->srq_table.srq,\r\nsrq->srqn & (dev->limits.num_srqs - 1),\r\nsrq)) {\r\nspin_unlock_irq(&dev->srq_table.lock);\r\ngoto err_out_free_srq;\r\n}\r\nspin_unlock_irq(&dev->srq_table.lock);\r\nmthca_free_mailbox(dev, mailbox);\r\nsrq->first_free = 0;\r\nsrq->last_free = srq->max - 1;\r\nattr->max_wr = srq->max - 1;\r\nattr->max_sge = srq->max_gs;\r\nreturn 0;\r\nerr_out_free_srq:\r\nerr = mthca_HW2SW_SRQ(dev, mailbox, srq->srqn);\r\nif (err)\r\nmthca_warn(dev, "HW2SW_SRQ failed (%d)\n", err);\r\nerr_out_free_buf:\r\nif (!pd->ibpd.uobject)\r\nmthca_free_srq_buf(dev, srq);\r\nerr_out_mailbox:\r\nmthca_free_mailbox(dev, mailbox);\r\nerr_out_db:\r\nif (!pd->ibpd.uobject && mthca_is_memfree(dev))\r\nmthca_free_db(dev, MTHCA_DB_TYPE_SRQ, srq->db_index);\r\nerr_out_icm:\r\nmthca_table_put(dev, dev->srq_table.table, srq->srqn);\r\nerr_out:\r\nmthca_free(&dev->srq_table.alloc, srq->srqn);\r\nreturn err;\r\n}\r\nstatic inline int get_srq_refcount(struct mthca_dev *dev, struct mthca_srq *srq)\r\n{\r\nint c;\r\nspin_lock_irq(&dev->srq_table.lock);\r\nc = srq->refcount;\r\nspin_unlock_irq(&dev->srq_table.lock);\r\nreturn c;\r\n}\r\nvoid mthca_free_srq(struct mthca_dev *dev, struct mthca_srq *srq)\r\n{\r\nstruct mthca_mailbox *mailbox;\r\nint err;\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox)) {\r\nmthca_warn(dev, "No memory for mailbox to free SRQ.\n");\r\nreturn;\r\n}\r\nerr = mthca_HW2SW_SRQ(dev, mailbox, srq->srqn);\r\nif (err)\r\nmthca_warn(dev, "HW2SW_SRQ failed (%d)\n", err);\r\nspin_lock_irq(&dev->srq_table.lock);\r\nmthca_array_clear(&dev->srq_table.srq,\r\nsrq->srqn & (dev->limits.num_srqs - 1));\r\n--srq->refcount;\r\nspin_unlock_irq(&dev->srq_table.lock);\r\nwait_event(srq->wait, !get_srq_refcount(dev, srq));\r\nif (!srq->ibsrq.uobject) {\r\nmthca_free_srq_buf(dev, srq);\r\nif (mthca_is_memfree(dev))\r\nmthca_free_db(dev, MTHCA_DB_TYPE_SRQ, srq->db_index);\r\n}\r\nmthca_table_put(dev, dev->srq_table.table, srq->srqn);\r\nmthca_free(&dev->srq_table.alloc, srq->srqn);\r\nmthca_free_mailbox(dev, mailbox);\r\n}\r\nint mthca_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,\r\nenum ib_srq_attr_mask attr_mask, struct ib_udata *udata)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibsrq->device);\r\nstruct mthca_srq *srq = to_msrq(ibsrq);\r\nint ret = 0;\r\nif (attr_mask & IB_SRQ_MAX_WR)\r\nreturn -EINVAL;\r\nif (attr_mask & IB_SRQ_LIMIT) {\r\nu32 max_wr = mthca_is_memfree(dev) ? srq->max - 1 : srq->max;\r\nif (attr->srq_limit > max_wr)\r\nreturn -EINVAL;\r\nmutex_lock(&srq->mutex);\r\nret = mthca_ARM_SRQ(dev, srq->srqn, attr->srq_limit);\r\nmutex_unlock(&srq->mutex);\r\n}\r\nreturn ret;\r\n}\r\nint mthca_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibsrq->device);\r\nstruct mthca_srq *srq = to_msrq(ibsrq);\r\nstruct mthca_mailbox *mailbox;\r\nstruct mthca_arbel_srq_context *arbel_ctx;\r\nstruct mthca_tavor_srq_context *tavor_ctx;\r\nint err;\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox))\r\nreturn PTR_ERR(mailbox);\r\nerr = mthca_QUERY_SRQ(dev, srq->srqn, mailbox);\r\nif (err)\r\ngoto out;\r\nif (mthca_is_memfree(dev)) {\r\narbel_ctx = mailbox->buf;\r\nsrq_attr->srq_limit = be16_to_cpu(arbel_ctx->limit_watermark);\r\n} else {\r\ntavor_ctx = mailbox->buf;\r\nsrq_attr->srq_limit = be16_to_cpu(tavor_ctx->limit_watermark);\r\n}\r\nsrq_attr->max_wr = srq->max - 1;\r\nsrq_attr->max_sge = srq->max_gs;\r\nout:\r\nmthca_free_mailbox(dev, mailbox);\r\nreturn err;\r\n}\r\nvoid mthca_srq_event(struct mthca_dev *dev, u32 srqn,\r\nenum ib_event_type event_type)\r\n{\r\nstruct mthca_srq *srq;\r\nstruct ib_event event;\r\nspin_lock(&dev->srq_table.lock);\r\nsrq = mthca_array_get(&dev->srq_table.srq, srqn & (dev->limits.num_srqs - 1));\r\nif (srq)\r\n++srq->refcount;\r\nspin_unlock(&dev->srq_table.lock);\r\nif (!srq) {\r\nmthca_warn(dev, "Async event for bogus SRQ %08x\n", srqn);\r\nreturn;\r\n}\r\nif (!srq->ibsrq.event_handler)\r\ngoto out;\r\nevent.device = &dev->ib_dev;\r\nevent.event = event_type;\r\nevent.element.srq = &srq->ibsrq;\r\nsrq->ibsrq.event_handler(&event, srq->ibsrq.srq_context);\r\nout:\r\nspin_lock(&dev->srq_table.lock);\r\nif (!--srq->refcount)\r\nwake_up(&srq->wait);\r\nspin_unlock(&dev->srq_table.lock);\r\n}\r\nvoid mthca_free_srq_wqe(struct mthca_srq *srq, u32 wqe_addr)\r\n{\r\nint ind;\r\nstruct mthca_next_seg *last_free;\r\nind = wqe_addr >> srq->wqe_shift;\r\nspin_lock(&srq->lock);\r\nlast_free = get_wqe(srq, srq->last_free);\r\n*wqe_to_link(last_free) = ind;\r\nlast_free->nda_op = htonl((ind << srq->wqe_shift) | 1);\r\n*wqe_to_link(get_wqe(srq, ind)) = -1;\r\nsrq->last_free = ind;\r\nspin_unlock(&srq->lock);\r\n}\r\nint mthca_tavor_post_srq_recv(struct ib_srq *ibsrq, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibsrq->device);\r\nstruct mthca_srq *srq = to_msrq(ibsrq);\r\nunsigned long flags;\r\nint err = 0;\r\nint first_ind;\r\nint ind;\r\nint next_ind;\r\nint nreq;\r\nint i;\r\nvoid *wqe;\r\nvoid *prev_wqe;\r\nspin_lock_irqsave(&srq->lock, flags);\r\nfirst_ind = srq->first_free;\r\nfor (nreq = 0; wr; wr = wr->next) {\r\nind = srq->first_free;\r\nwqe = get_wqe(srq, ind);\r\nnext_ind = *wqe_to_link(wqe);\r\nif (unlikely(next_ind < 0)) {\r\nmthca_err(dev, "SRQ %06x full\n", srq->srqn);\r\nerr = -ENOMEM;\r\n*bad_wr = wr;\r\nbreak;\r\n}\r\nprev_wqe = srq->last;\r\nsrq->last = wqe;\r\n((struct mthca_next_seg *) wqe)->ee_nds = 0;\r\nwqe += sizeof (struct mthca_next_seg);\r\nif (unlikely(wr->num_sge > srq->max_gs)) {\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\nsrq->last = prev_wqe;\r\nbreak;\r\n}\r\nfor (i = 0; i < wr->num_sge; ++i) {\r\nmthca_set_data_seg(wqe, wr->sg_list + i);\r\nwqe += sizeof (struct mthca_data_seg);\r\n}\r\nif (i < srq->max_gs)\r\nmthca_set_data_seg_inval(wqe);\r\n((struct mthca_next_seg *) prev_wqe)->ee_nds =\r\ncpu_to_be32(MTHCA_NEXT_DBD);\r\nsrq->wrid[ind] = wr->wr_id;\r\nsrq->first_free = next_ind;\r\n++nreq;\r\nif (unlikely(nreq == MTHCA_TAVOR_MAX_WQES_PER_RECV_DB)) {\r\nnreq = 0;\r\nwmb();\r\nmthca_write64(first_ind << srq->wqe_shift, srq->srqn << 8,\r\ndev->kar + MTHCA_RECEIVE_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\nfirst_ind = srq->first_free;\r\n}\r\n}\r\nif (likely(nreq)) {\r\nwmb();\r\nmthca_write64(first_ind << srq->wqe_shift, (srq->srqn << 8) | nreq,\r\ndev->kar + MTHCA_RECEIVE_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\n}\r\nmmiowb();\r\nspin_unlock_irqrestore(&srq->lock, flags);\r\nreturn err;\r\n}\r\nint mthca_arbel_post_srq_recv(struct ib_srq *ibsrq, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibsrq->device);\r\nstruct mthca_srq *srq = to_msrq(ibsrq);\r\nunsigned long flags;\r\nint err = 0;\r\nint ind;\r\nint next_ind;\r\nint nreq;\r\nint i;\r\nvoid *wqe;\r\nspin_lock_irqsave(&srq->lock, flags);\r\nfor (nreq = 0; wr; ++nreq, wr = wr->next) {\r\nind = srq->first_free;\r\nwqe = get_wqe(srq, ind);\r\nnext_ind = *wqe_to_link(wqe);\r\nif (unlikely(next_ind < 0)) {\r\nmthca_err(dev, "SRQ %06x full\n", srq->srqn);\r\nerr = -ENOMEM;\r\n*bad_wr = wr;\r\nbreak;\r\n}\r\n((struct mthca_next_seg *) wqe)->ee_nds = 0;\r\nwqe += sizeof (struct mthca_next_seg);\r\nif (unlikely(wr->num_sge > srq->max_gs)) {\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\nbreak;\r\n}\r\nfor (i = 0; i < wr->num_sge; ++i) {\r\nmthca_set_data_seg(wqe, wr->sg_list + i);\r\nwqe += sizeof (struct mthca_data_seg);\r\n}\r\nif (i < srq->max_gs)\r\nmthca_set_data_seg_inval(wqe);\r\nsrq->wrid[ind] = wr->wr_id;\r\nsrq->first_free = next_ind;\r\n}\r\nif (likely(nreq)) {\r\nsrq->counter += nreq;\r\nwmb();\r\n*srq->db = cpu_to_be32(srq->counter);\r\n}\r\nspin_unlock_irqrestore(&srq->lock, flags);\r\nreturn err;\r\n}\r\nint mthca_max_srq_sge(struct mthca_dev *dev)\r\n{\r\nif (mthca_is_memfree(dev))\r\nreturn dev->limits.max_sg;\r\nreturn min_t(int, dev->limits.max_sg,\r\n((1 << (fls(dev->limits.max_desc_sz) - 1)) -\r\nsizeof (struct mthca_next_seg)) /\r\nsizeof (struct mthca_data_seg));\r\n}\r\nint mthca_init_srq_table(struct mthca_dev *dev)\r\n{\r\nint err;\r\nif (!(dev->mthca_flags & MTHCA_FLAG_SRQ))\r\nreturn 0;\r\nspin_lock_init(&dev->srq_table.lock);\r\nerr = mthca_alloc_init(&dev->srq_table.alloc,\r\ndev->limits.num_srqs,\r\ndev->limits.num_srqs - 1,\r\ndev->limits.reserved_srqs);\r\nif (err)\r\nreturn err;\r\nerr = mthca_array_init(&dev->srq_table.srq,\r\ndev->limits.num_srqs);\r\nif (err)\r\nmthca_alloc_cleanup(&dev->srq_table.alloc);\r\nreturn err;\r\n}\r\nvoid mthca_cleanup_srq_table(struct mthca_dev *dev)\r\n{\r\nif (!(dev->mthca_flags & MTHCA_FLAG_SRQ))\r\nreturn;\r\nmthca_array_cleanup(&dev->srq_table.srq, dev->limits.num_srqs);\r\nmthca_alloc_cleanup(&dev->srq_table.alloc);\r\n}
