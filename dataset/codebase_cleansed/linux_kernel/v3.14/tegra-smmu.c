static bool inline smmu_valid_reg(struct smmu_device *smmu,\r\nvoid __iomem *addr)\r\n{\r\nint i;\r\nfor (i = 0; i < smmu->nregs; i++) {\r\nif (addr < smmu->regs[i])\r\nbreak;\r\nif (addr <= smmu->rege[i])\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic inline u32 smmu_read(struct smmu_device *smmu, size_t offs)\r\n{\r\nvoid __iomem *addr = smmu->regbase + offs;\r\nBUG_ON(!smmu_valid_reg(smmu, addr));\r\nreturn readl(addr);\r\n}\r\nstatic inline void smmu_write(struct smmu_device *smmu, u32 val, size_t offs)\r\n{\r\nvoid __iomem *addr = smmu->regbase + offs;\r\nBUG_ON(!smmu_valid_reg(smmu, addr));\r\nwritel(val, addr);\r\n}\r\nstatic int __smmu_client_set_hwgrp(struct smmu_client *c,\r\nunsigned long map, int on)\r\n{\r\nint i;\r\nstruct smmu_as *as = c->as;\r\nu32 val, offs, mask = SMMU_ASID_ENABLE(as->asid);\r\nstruct smmu_device *smmu = as->smmu;\r\nWARN_ON(!on && map);\r\nif (on && !map)\r\nreturn -EINVAL;\r\nif (!on)\r\nmap = smmu_client_hwgrp(c);\r\nfor_each_set_bit(i, &map, HWGRP_COUNT) {\r\noffs = HWGRP_ASID_REG(i);\r\nval = smmu_read(smmu, offs);\r\nif (on) {\r\nif (WARN_ON(val & mask))\r\ngoto err_hw_busy;\r\nval |= mask;\r\n} else {\r\nWARN_ON((val & mask) == mask);\r\nval &= ~mask;\r\n}\r\nsmmu_write(smmu, val, offs);\r\n}\r\nFLUSH_SMMU_REGS(smmu);\r\nc->hwgrp = map;\r\nreturn 0;\r\nerr_hw_busy:\r\nfor_each_set_bit(i, &map, HWGRP_COUNT) {\r\noffs = HWGRP_ASID_REG(i);\r\nval = smmu_read(smmu, offs);\r\nval &= ~mask;\r\nsmmu_write(smmu, val, offs);\r\n}\r\nreturn -EBUSY;\r\n}\r\nstatic int smmu_client_set_hwgrp(struct smmu_client *c, u32 map, int on)\r\n{\r\nu32 val;\r\nunsigned long flags;\r\nstruct smmu_as *as = c->as;\r\nstruct smmu_device *smmu = as->smmu;\r\nspin_lock_irqsave(&smmu->lock, flags);\r\nval = __smmu_client_set_hwgrp(c, map, on);\r\nspin_unlock_irqrestore(&smmu->lock, flags);\r\nreturn val;\r\n}\r\nstatic void smmu_flush_regs(struct smmu_device *smmu, int enable)\r\n{\r\nu32 val;\r\nsmmu_write(smmu, SMMU_PTC_FLUSH_TYPE_ALL, SMMU_PTC_FLUSH);\r\nFLUSH_SMMU_REGS(smmu);\r\nval = SMMU_TLB_FLUSH_VA_MATCH_ALL |\r\nSMMU_TLB_FLUSH_ASID_MATCH_disable;\r\nsmmu_write(smmu, val, SMMU_TLB_FLUSH);\r\nif (enable)\r\nsmmu_write(smmu, SMMU_CONFIG_ENABLE, SMMU_CONFIG);\r\nFLUSH_SMMU_REGS(smmu);\r\n}\r\nstatic int smmu_setup_regs(struct smmu_device *smmu)\r\n{\r\nint i;\r\nu32 val;\r\nfor (i = 0; i < smmu->num_as; i++) {\r\nstruct smmu_as *as = &smmu->as[i];\r\nstruct smmu_client *c;\r\nsmmu_write(smmu, SMMU_PTB_ASID_CUR(as->asid), SMMU_PTB_ASID);\r\nval = as->pdir_page ?\r\nSMMU_MK_PDIR(as->pdir_page, as->pdir_attr) :\r\nSMMU_PTB_DATA_RESET_VAL;\r\nsmmu_write(smmu, val, SMMU_PTB_DATA);\r\nlist_for_each_entry(c, &as->client, list)\r\n__smmu_client_set_hwgrp(c, c->hwgrp, 1);\r\n}\r\nsmmu_write(smmu, smmu->translation_enable_0, SMMU_TRANSLATION_ENABLE_0);\r\nsmmu_write(smmu, smmu->translation_enable_1, SMMU_TRANSLATION_ENABLE_1);\r\nsmmu_write(smmu, smmu->translation_enable_2, SMMU_TRANSLATION_ENABLE_2);\r\nsmmu_write(smmu, smmu->asid_security, SMMU_ASID_SECURITY);\r\nsmmu_write(smmu, SMMU_TLB_CONFIG_RESET_VAL, SMMU_CACHE_CONFIG(_TLB));\r\nsmmu_write(smmu, SMMU_PTC_CONFIG_RESET_VAL, SMMU_CACHE_CONFIG(_PTC));\r\nsmmu_flush_regs(smmu, 1);\r\nreturn tegra_ahb_enable_smmu(smmu->ahb);\r\n}\r\nstatic void flush_ptc_and_tlb(struct smmu_device *smmu,\r\nstruct smmu_as *as, dma_addr_t iova,\r\nunsigned long *pte, struct page *page, int is_pde)\r\n{\r\nu32 val;\r\nunsigned long tlb_flush_va = is_pde\r\n? SMMU_TLB_FLUSH_VA(iova, SECTION)\r\n: SMMU_TLB_FLUSH_VA(iova, GROUP);\r\nval = SMMU_PTC_FLUSH_TYPE_ADR | VA_PAGE_TO_PA(pte, page);\r\nsmmu_write(smmu, val, SMMU_PTC_FLUSH);\r\nFLUSH_SMMU_REGS(smmu);\r\nval = tlb_flush_va |\r\nSMMU_TLB_FLUSH_ASID_MATCH__ENABLE |\r\n(as->asid << SMMU_TLB_FLUSH_ASID_SHIFT);\r\nsmmu_write(smmu, val, SMMU_TLB_FLUSH);\r\nFLUSH_SMMU_REGS(smmu);\r\n}\r\nstatic void free_ptbl(struct smmu_as *as, dma_addr_t iova)\r\n{\r\nunsigned long pdn = SMMU_ADDR_TO_PDN(iova);\r\nunsigned long *pdir = (unsigned long *)page_address(as->pdir_page);\r\nif (pdir[pdn] != _PDE_VACANT(pdn)) {\r\ndev_dbg(as->smmu->dev, "pdn: %lx\n", pdn);\r\nClearPageReserved(SMMU_EX_PTBL_PAGE(pdir[pdn]));\r\n__free_page(SMMU_EX_PTBL_PAGE(pdir[pdn]));\r\npdir[pdn] = _PDE_VACANT(pdn);\r\nFLUSH_CPU_DCACHE(&pdir[pdn], as->pdir_page, sizeof pdir[pdn]);\r\nflush_ptc_and_tlb(as->smmu, as, iova, &pdir[pdn],\r\nas->pdir_page, 1);\r\n}\r\n}\r\nstatic void free_pdir(struct smmu_as *as)\r\n{\r\nunsigned addr;\r\nint count;\r\nstruct device *dev = as->smmu->dev;\r\nif (!as->pdir_page)\r\nreturn;\r\naddr = as->smmu->iovmm_base;\r\ncount = as->smmu->page_count;\r\nwhile (count-- > 0) {\r\nfree_ptbl(as, addr);\r\naddr += SMMU_PAGE_SIZE * SMMU_PTBL_COUNT;\r\n}\r\nClearPageReserved(as->pdir_page);\r\n__free_page(as->pdir_page);\r\nas->pdir_page = NULL;\r\ndevm_kfree(dev, as->pte_count);\r\nas->pte_count = NULL;\r\n}\r\nstatic unsigned long *locate_pte(struct smmu_as *as,\r\ndma_addr_t iova, bool allocate,\r\nstruct page **ptbl_page_p,\r\nunsigned int **count)\r\n{\r\nunsigned long ptn = SMMU_ADDR_TO_PFN(iova);\r\nunsigned long pdn = SMMU_ADDR_TO_PDN(iova);\r\nunsigned long *pdir = page_address(as->pdir_page);\r\nunsigned long *ptbl;\r\nif (pdir[pdn] != _PDE_VACANT(pdn)) {\r\n*ptbl_page_p = SMMU_EX_PTBL_PAGE(pdir[pdn]);\r\nptbl = page_address(*ptbl_page_p);\r\n} else if (!allocate) {\r\nreturn NULL;\r\n} else {\r\nint pn;\r\nunsigned long addr = SMMU_PDN_TO_ADDR(pdn);\r\ndev_dbg(as->smmu->dev, "New PTBL pdn: %lx\n", pdn);\r\n*ptbl_page_p = alloc_page(GFP_ATOMIC);\r\nif (!*ptbl_page_p) {\r\ndev_err(as->smmu->dev,\r\n"failed to allocate smmu_device page table\n");\r\nreturn NULL;\r\n}\r\nSetPageReserved(*ptbl_page_p);\r\nptbl = (unsigned long *)page_address(*ptbl_page_p);\r\nfor (pn = 0; pn < SMMU_PTBL_COUNT;\r\npn++, addr += SMMU_PAGE_SIZE) {\r\nptbl[pn] = _PTE_VACANT(addr);\r\n}\r\nFLUSH_CPU_DCACHE(ptbl, *ptbl_page_p, SMMU_PTBL_SIZE);\r\npdir[pdn] = SMMU_MK_PDE(*ptbl_page_p,\r\nas->pde_attr | _PDE_NEXT);\r\nFLUSH_CPU_DCACHE(&pdir[pdn], as->pdir_page, sizeof pdir[pdn]);\r\nflush_ptc_and_tlb(as->smmu, as, iova, &pdir[pdn],\r\nas->pdir_page, 1);\r\n}\r\n*count = &as->pte_count[pdn];\r\nreturn &ptbl[ptn % SMMU_PTBL_COUNT];\r\n}\r\nstatic void put_signature(struct smmu_as *as,\r\ndma_addr_t iova, unsigned long pfn)\r\n{\r\nstruct page *page;\r\nunsigned long *vaddr;\r\npage = pfn_to_page(pfn);\r\nvaddr = page_address(page);\r\nif (!vaddr)\r\nreturn;\r\nvaddr[0] = iova;\r\nvaddr[1] = pfn << PAGE_SHIFT;\r\nFLUSH_CPU_DCACHE(vaddr, page, sizeof(vaddr[0]) * 2);\r\n}\r\nstatic inline void put_signature(struct smmu_as *as,\r\nunsigned long addr, unsigned long pfn)\r\n{\r\n}\r\nstatic int alloc_pdir(struct smmu_as *as)\r\n{\r\nunsigned long *pdir, flags;\r\nint pdn, err = 0;\r\nu32 val;\r\nstruct smmu_device *smmu = as->smmu;\r\nstruct page *page;\r\nunsigned int *cnt;\r\ncnt = devm_kzalloc(smmu->dev,\r\nsizeof(cnt[0]) * SMMU_PDIR_COUNT,\r\nGFP_KERNEL);\r\npage = alloc_page(GFP_KERNEL | __GFP_DMA);\r\nspin_lock_irqsave(&as->lock, flags);\r\nif (as->pdir_page) {\r\nerr = -EAGAIN;\r\ngoto err_out;\r\n}\r\nif (!page || !cnt) {\r\ndev_err(smmu->dev, "failed to allocate at %s\n", __func__);\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\nas->pdir_page = page;\r\nas->pte_count = cnt;\r\nSetPageReserved(as->pdir_page);\r\npdir = page_address(as->pdir_page);\r\nfor (pdn = 0; pdn < SMMU_PDIR_COUNT; pdn++)\r\npdir[pdn] = _PDE_VACANT(pdn);\r\nFLUSH_CPU_DCACHE(pdir, as->pdir_page, SMMU_PDIR_SIZE);\r\nval = SMMU_PTC_FLUSH_TYPE_ADR | VA_PAGE_TO_PA(pdir, as->pdir_page);\r\nsmmu_write(smmu, val, SMMU_PTC_FLUSH);\r\nFLUSH_SMMU_REGS(as->smmu);\r\nval = SMMU_TLB_FLUSH_VA_MATCH_ALL |\r\nSMMU_TLB_FLUSH_ASID_MATCH__ENABLE |\r\n(as->asid << SMMU_TLB_FLUSH_ASID_SHIFT);\r\nsmmu_write(smmu, val, SMMU_TLB_FLUSH);\r\nFLUSH_SMMU_REGS(as->smmu);\r\nspin_unlock_irqrestore(&as->lock, flags);\r\nreturn 0;\r\nerr_out:\r\nspin_unlock_irqrestore(&as->lock, flags);\r\ndevm_kfree(smmu->dev, cnt);\r\nif (page)\r\n__free_page(page);\r\nreturn err;\r\n}\r\nstatic void __smmu_iommu_unmap(struct smmu_as *as, dma_addr_t iova)\r\n{\r\nunsigned long *pte;\r\nstruct page *page;\r\nunsigned int *count;\r\npte = locate_pte(as, iova, false, &page, &count);\r\nif (WARN_ON(!pte))\r\nreturn;\r\nif (WARN_ON(*pte == _PTE_VACANT(iova)))\r\nreturn;\r\n*pte = _PTE_VACANT(iova);\r\nFLUSH_CPU_DCACHE(pte, page, sizeof(*pte));\r\nflush_ptc_and_tlb(as->smmu, as, iova, pte, page, 0);\r\nif (!--(*count))\r\nfree_ptbl(as, iova);\r\n}\r\nstatic void __smmu_iommu_map_pfn(struct smmu_as *as, dma_addr_t iova,\r\nunsigned long pfn)\r\n{\r\nstruct smmu_device *smmu = as->smmu;\r\nunsigned long *pte;\r\nunsigned int *count;\r\nstruct page *page;\r\npte = locate_pte(as, iova, true, &page, &count);\r\nif (WARN_ON(!pte))\r\nreturn;\r\nif (*pte == _PTE_VACANT(iova))\r\n(*count)++;\r\n*pte = SMMU_PFN_TO_PTE(pfn, as->pte_attr);\r\nif (unlikely((*pte == _PTE_VACANT(iova))))\r\n(*count)--;\r\nFLUSH_CPU_DCACHE(pte, page, sizeof(*pte));\r\nflush_ptc_and_tlb(smmu, as, iova, pte, page, 0);\r\nput_signature(as, iova, pfn);\r\n}\r\nstatic int smmu_iommu_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t pa, size_t bytes, int prot)\r\n{\r\nstruct smmu_as *as = domain->priv;\r\nunsigned long pfn = __phys_to_pfn(pa);\r\nunsigned long flags;\r\ndev_dbg(as->smmu->dev, "[%d] %08lx:%pa\n", as->asid, iova, &pa);\r\nif (!pfn_valid(pfn))\r\nreturn -ENOMEM;\r\nspin_lock_irqsave(&as->lock, flags);\r\n__smmu_iommu_map_pfn(as, iova, pfn);\r\nspin_unlock_irqrestore(&as->lock, flags);\r\nreturn 0;\r\n}\r\nstatic size_t smmu_iommu_unmap(struct iommu_domain *domain, unsigned long iova,\r\nsize_t bytes)\r\n{\r\nstruct smmu_as *as = domain->priv;\r\nunsigned long flags;\r\ndev_dbg(as->smmu->dev, "[%d] %08lx\n", as->asid, iova);\r\nspin_lock_irqsave(&as->lock, flags);\r\n__smmu_iommu_unmap(as, iova);\r\nspin_unlock_irqrestore(&as->lock, flags);\r\nreturn SMMU_PAGE_SIZE;\r\n}\r\nstatic phys_addr_t smmu_iommu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct smmu_as *as = domain->priv;\r\nunsigned long *pte;\r\nunsigned int *count;\r\nstruct page *page;\r\nunsigned long pfn;\r\nunsigned long flags;\r\nspin_lock_irqsave(&as->lock, flags);\r\npte = locate_pte(as, iova, true, &page, &count);\r\npfn = *pte & SMMU_PFN_MASK;\r\nWARN_ON(!pfn_valid(pfn));\r\ndev_dbg(as->smmu->dev,\r\n"iova:%08llx pfn:%08lx asid:%d\n", (unsigned long long)iova,\r\npfn, as->asid);\r\nspin_unlock_irqrestore(&as->lock, flags);\r\nreturn PFN_PHYS(pfn);\r\n}\r\nstatic int smmu_iommu_domain_has_cap(struct iommu_domain *domain,\r\nunsigned long cap)\r\n{\r\nreturn 0;\r\n}\r\nstatic int smmu_iommu_attach_dev(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct smmu_as *as = domain->priv;\r\nstruct smmu_device *smmu = as->smmu;\r\nstruct smmu_client *client, *c;\r\nu32 map;\r\nint err;\r\nclient = devm_kzalloc(smmu->dev, sizeof(*c), GFP_KERNEL);\r\nif (!client)\r\nreturn -ENOMEM;\r\nclient->dev = dev;\r\nclient->as = as;\r\nmap = (unsigned long)dev->platform_data;\r\nif (!map)\r\nreturn -EINVAL;\r\nerr = smmu_client_enable_hwgrp(client, map);\r\nif (err)\r\ngoto err_hwgrp;\r\nspin_lock(&as->client_lock);\r\nlist_for_each_entry(c, &as->client, list) {\r\nif (c->dev == dev) {\r\ndev_err(smmu->dev,\r\n"%s is already attached\n", dev_name(c->dev));\r\nerr = -EINVAL;\r\ngoto err_client;\r\n}\r\n}\r\nlist_add(&client->list, &as->client);\r\nspin_unlock(&as->client_lock);\r\nif (map & HWG_AVPC) {\r\nstruct page *page;\r\npage = as->smmu->avp_vector_page;\r\n__smmu_iommu_map_pfn(as, 0, page_to_pfn(page));\r\npr_info("Reserve \"page zero\" for AVP vectors using a common dummy\n");\r\n}\r\ndev_dbg(smmu->dev, "%s is attached\n", dev_name(dev));\r\nreturn 0;\r\nerr_client:\r\nsmmu_client_disable_hwgrp(client);\r\nspin_unlock(&as->client_lock);\r\nerr_hwgrp:\r\ndevm_kfree(smmu->dev, client);\r\nreturn err;\r\n}\r\nstatic void smmu_iommu_detach_dev(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct smmu_as *as = domain->priv;\r\nstruct smmu_device *smmu = as->smmu;\r\nstruct smmu_client *c;\r\nspin_lock(&as->client_lock);\r\nlist_for_each_entry(c, &as->client, list) {\r\nif (c->dev == dev) {\r\nsmmu_client_disable_hwgrp(c);\r\nlist_del(&c->list);\r\ndevm_kfree(smmu->dev, c);\r\nc->as = NULL;\r\ndev_dbg(smmu->dev,\r\n"%s is detached\n", dev_name(c->dev));\r\ngoto out;\r\n}\r\n}\r\ndev_err(smmu->dev, "Couldn't find %s\n", dev_name(dev));\r\nout:\r\nspin_unlock(&as->client_lock);\r\n}\r\nstatic int smmu_iommu_domain_init(struct iommu_domain *domain)\r\n{\r\nint i, err = -EAGAIN;\r\nunsigned long flags;\r\nstruct smmu_as *as;\r\nstruct smmu_device *smmu = smmu_handle;\r\nfor (i = 0; i < smmu->num_as; i++) {\r\nas = &smmu->as[i];\r\nif (as->pdir_page)\r\ncontinue;\r\nerr = alloc_pdir(as);\r\nif (!err)\r\ngoto found;\r\nif (err != -EAGAIN)\r\nbreak;\r\n}\r\nif (i == smmu->num_as)\r\ndev_err(smmu->dev, "no free AS\n");\r\nreturn err;\r\nfound:\r\nspin_lock_irqsave(&smmu->lock, flags);\r\nsmmu_write(smmu, SMMU_PTB_ASID_CUR(as->asid), SMMU_PTB_ASID);\r\nsmmu_write(smmu,\r\nSMMU_MK_PDIR(as->pdir_page, as->pdir_attr), SMMU_PTB_DATA);\r\nFLUSH_SMMU_REGS(smmu);\r\nspin_unlock_irqrestore(&smmu->lock, flags);\r\ndomain->priv = as;\r\ndomain->geometry.aperture_start = smmu->iovmm_base;\r\ndomain->geometry.aperture_end = smmu->iovmm_base +\r\nsmmu->page_count * SMMU_PAGE_SIZE - 1;\r\ndomain->geometry.force_aperture = true;\r\ndev_dbg(smmu->dev, "smmu_as@%p\n", as);\r\nreturn 0;\r\n}\r\nstatic void smmu_iommu_domain_destroy(struct iommu_domain *domain)\r\n{\r\nstruct smmu_as *as = domain->priv;\r\nstruct smmu_device *smmu = as->smmu;\r\nunsigned long flags;\r\nspin_lock_irqsave(&as->lock, flags);\r\nif (as->pdir_page) {\r\nspin_lock(&smmu->lock);\r\nsmmu_write(smmu, SMMU_PTB_ASID_CUR(as->asid), SMMU_PTB_ASID);\r\nsmmu_write(smmu, SMMU_PTB_DATA_RESET_VAL, SMMU_PTB_DATA);\r\nFLUSH_SMMU_REGS(smmu);\r\nspin_unlock(&smmu->lock);\r\nfree_pdir(as);\r\n}\r\nif (!list_empty(&as->client)) {\r\nstruct smmu_client *c;\r\nlist_for_each_entry(c, &as->client, list)\r\nsmmu_iommu_detach_dev(domain, c->dev);\r\n}\r\nspin_unlock_irqrestore(&as->lock, flags);\r\ndomain->priv = NULL;\r\ndev_dbg(smmu->dev, "smmu_as@%p\n", as);\r\n}\r\nstatic ssize_t smmu_debugfs_stats_write(struct file *file,\r\nconst char __user *buffer,\r\nsize_t count, loff_t *pos)\r\n{\r\nstruct smmu_debugfs_info *info;\r\nstruct smmu_device *smmu;\r\nint i;\r\nenum {\r\n_OFF = 0,\r\n_ON,\r\n_RESET,\r\n};\r\nconst char * const command[] = {\r\n[_OFF] = "off",\r\n[_ON] = "on",\r\n[_RESET] = "reset",\r\n};\r\nchar str[] = "reset";\r\nu32 val;\r\nsize_t offs;\r\ncount = min_t(size_t, count, sizeof(str));\r\nif (copy_from_user(str, buffer, count))\r\nreturn -EINVAL;\r\nfor (i = 0; i < ARRAY_SIZE(command); i++)\r\nif (strncmp(str, command[i],\r\nstrlen(command[i])) == 0)\r\nbreak;\r\nif (i == ARRAY_SIZE(command))\r\nreturn -EINVAL;\r\ninfo = file_inode(file)->i_private;\r\nsmmu = info->smmu;\r\noffs = SMMU_CACHE_CONFIG(info->cache);\r\nval = smmu_read(smmu, offs);\r\nswitch (i) {\r\ncase _OFF:\r\nval &= ~SMMU_CACHE_CONFIG_STATS_ENABLE;\r\nval &= ~SMMU_CACHE_CONFIG_STATS_TEST;\r\nsmmu_write(smmu, val, offs);\r\nbreak;\r\ncase _ON:\r\nval |= SMMU_CACHE_CONFIG_STATS_ENABLE;\r\nval &= ~SMMU_CACHE_CONFIG_STATS_TEST;\r\nsmmu_write(smmu, val, offs);\r\nbreak;\r\ncase _RESET:\r\nval |= SMMU_CACHE_CONFIG_STATS_TEST;\r\nsmmu_write(smmu, val, offs);\r\nval &= ~SMMU_CACHE_CONFIG_STATS_TEST;\r\nsmmu_write(smmu, val, offs);\r\nbreak;\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\ndev_dbg(smmu->dev, "%s() %08x, %08x @%08x\n", __func__,\r\nval, smmu_read(smmu, offs), offs);\r\nreturn count;\r\n}\r\nstatic int smmu_debugfs_stats_show(struct seq_file *s, void *v)\r\n{\r\nstruct smmu_debugfs_info *info = s->private;\r\nstruct smmu_device *smmu = info->smmu;\r\nint i;\r\nconst char * const stats[] = { "hit", "miss", };\r\nfor (i = 0; i < ARRAY_SIZE(stats); i++) {\r\nu32 val;\r\nsize_t offs;\r\noffs = SMMU_STATS_CACHE_COUNT(info->mc, info->cache, i);\r\nval = smmu_read(smmu, offs);\r\nseq_printf(s, "%s:%08x ", stats[i], val);\r\ndev_dbg(smmu->dev, "%s() %s %08x @%08x\n", __func__,\r\nstats[i], val, offs);\r\n}\r\nseq_printf(s, "\n");\r\nreturn 0;\r\n}\r\nstatic int smmu_debugfs_stats_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, smmu_debugfs_stats_show, inode->i_private);\r\n}\r\nstatic void smmu_debugfs_delete(struct smmu_device *smmu)\r\n{\r\ndebugfs_remove_recursive(smmu->debugfs_root);\r\nkfree(smmu->debugfs_info);\r\n}\r\nstatic void smmu_debugfs_create(struct smmu_device *smmu)\r\n{\r\nint i;\r\nsize_t bytes;\r\nstruct dentry *root;\r\nbytes = ARRAY_SIZE(smmu_debugfs_mc) * ARRAY_SIZE(smmu_debugfs_cache) *\r\nsizeof(*smmu->debugfs_info);\r\nsmmu->debugfs_info = kmalloc(bytes, GFP_KERNEL);\r\nif (!smmu->debugfs_info)\r\nreturn;\r\nroot = debugfs_create_dir(dev_name(smmu->dev), NULL);\r\nif (!root)\r\ngoto err_out;\r\nsmmu->debugfs_root = root;\r\nfor (i = 0; i < ARRAY_SIZE(smmu_debugfs_mc); i++) {\r\nint j;\r\nstruct dentry *mc;\r\nmc = debugfs_create_dir(smmu_debugfs_mc[i], root);\r\nif (!mc)\r\ngoto err_out;\r\nfor (j = 0; j < ARRAY_SIZE(smmu_debugfs_cache); j++) {\r\nstruct dentry *cache;\r\nstruct smmu_debugfs_info *info;\r\ninfo = smmu->debugfs_info;\r\ninfo += i * ARRAY_SIZE(smmu_debugfs_mc) + j;\r\ninfo->smmu = smmu;\r\ninfo->mc = i;\r\ninfo->cache = j;\r\ncache = debugfs_create_file(smmu_debugfs_cache[j],\r\nS_IWUGO | S_IRUGO, mc,\r\n(void *)info,\r\n&smmu_debugfs_stats_fops);\r\nif (!cache)\r\ngoto err_out;\r\n}\r\n}\r\nreturn;\r\nerr_out:\r\nsmmu_debugfs_delete(smmu);\r\n}\r\nstatic int tegra_smmu_suspend(struct device *dev)\r\n{\r\nstruct smmu_device *smmu = dev_get_drvdata(dev);\r\nsmmu->translation_enable_0 = smmu_read(smmu, SMMU_TRANSLATION_ENABLE_0);\r\nsmmu->translation_enable_1 = smmu_read(smmu, SMMU_TRANSLATION_ENABLE_1);\r\nsmmu->translation_enable_2 = smmu_read(smmu, SMMU_TRANSLATION_ENABLE_2);\r\nsmmu->asid_security = smmu_read(smmu, SMMU_ASID_SECURITY);\r\nreturn 0;\r\n}\r\nstatic int tegra_smmu_resume(struct device *dev)\r\n{\r\nstruct smmu_device *smmu = dev_get_drvdata(dev);\r\nunsigned long flags;\r\nint err;\r\nspin_lock_irqsave(&smmu->lock, flags);\r\nerr = smmu_setup_regs(smmu);\r\nspin_unlock_irqrestore(&smmu->lock, flags);\r\nreturn err;\r\n}\r\nstatic int tegra_smmu_probe(struct platform_device *pdev)\r\n{\r\nstruct smmu_device *smmu;\r\nstruct device *dev = &pdev->dev;\r\nint i, asids, err = 0;\r\ndma_addr_t uninitialized_var(base);\r\nsize_t bytes, uninitialized_var(size);\r\nif (smmu_handle)\r\nreturn -EIO;\r\nBUILD_BUG_ON(PAGE_SHIFT != SMMU_PAGE_SHIFT);\r\nif (of_property_read_u32(dev->of_node, "nvidia,#asids", &asids))\r\nreturn -ENODEV;\r\nbytes = sizeof(*smmu) + asids * sizeof(*smmu->as);\r\nsmmu = devm_kzalloc(dev, bytes, GFP_KERNEL);\r\nif (!smmu) {\r\ndev_err(dev, "failed to allocate smmu_device\n");\r\nreturn -ENOMEM;\r\n}\r\nsmmu->nregs = pdev->num_resources;\r\nsmmu->regs = devm_kzalloc(dev, 2 * smmu->nregs * sizeof(*smmu->regs),\r\nGFP_KERNEL);\r\nsmmu->rege = smmu->regs + smmu->nregs;\r\nif (!smmu->regs)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < smmu->nregs; i++) {\r\nstruct resource *res;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, i);\r\nsmmu->regs[i] = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(smmu->regs[i]))\r\nreturn PTR_ERR(smmu->regs[i]);\r\nsmmu->rege[i] = smmu->regs[i] + resource_size(res) - 1;\r\n}\r\nsmmu->regbase = (void __iomem *)((u32)smmu->regs[0] & PAGE_MASK);\r\nerr = of_get_dma_window(dev->of_node, NULL, 0, NULL, &base, &size);\r\nif (err)\r\nreturn -ENODEV;\r\nif (size & SMMU_PAGE_MASK)\r\nreturn -EINVAL;\r\nsize >>= SMMU_PAGE_SHIFT;\r\nif (!size)\r\nreturn -EINVAL;\r\nsmmu->ahb = of_parse_phandle(dev->of_node, "nvidia,ahb", 0);\r\nif (!smmu->ahb)\r\nreturn -ENODEV;\r\nsmmu->dev = dev;\r\nsmmu->num_as = asids;\r\nsmmu->iovmm_base = base;\r\nsmmu->page_count = size;\r\nsmmu->translation_enable_0 = ~0;\r\nsmmu->translation_enable_1 = ~0;\r\nsmmu->translation_enable_2 = ~0;\r\nsmmu->asid_security = 0;\r\nfor (i = 0; i < smmu->num_as; i++) {\r\nstruct smmu_as *as = &smmu->as[i];\r\nas->smmu = smmu;\r\nas->asid = i;\r\nas->pdir_attr = _PDIR_ATTR;\r\nas->pde_attr = _PDE_ATTR;\r\nas->pte_attr = _PTE_ATTR;\r\nspin_lock_init(&as->lock);\r\nspin_lock_init(&as->client_lock);\r\nINIT_LIST_HEAD(&as->client);\r\n}\r\nspin_lock_init(&smmu->lock);\r\nerr = smmu_setup_regs(smmu);\r\nif (err)\r\nreturn err;\r\nplatform_set_drvdata(pdev, smmu);\r\nsmmu->avp_vector_page = alloc_page(GFP_KERNEL);\r\nif (!smmu->avp_vector_page)\r\nreturn -ENOMEM;\r\nsmmu_debugfs_create(smmu);\r\nsmmu_handle = smmu;\r\nbus_set_iommu(&platform_bus_type, &smmu_iommu_ops);\r\nreturn 0;\r\n}\r\nstatic int tegra_smmu_remove(struct platform_device *pdev)\r\n{\r\nstruct smmu_device *smmu = platform_get_drvdata(pdev);\r\nint i;\r\nsmmu_debugfs_delete(smmu);\r\nsmmu_write(smmu, SMMU_CONFIG_DISABLE, SMMU_CONFIG);\r\nfor (i = 0; i < smmu->num_as; i++)\r\nfree_pdir(&smmu->as[i]);\r\n__free_page(smmu->avp_vector_page);\r\nsmmu_handle = NULL;\r\nreturn 0;\r\n}\r\nstatic int tegra_smmu_init(void)\r\n{\r\nreturn platform_driver_register(&tegra_smmu_driver);\r\n}\r\nstatic void __exit tegra_smmu_exit(void)\r\n{\r\nplatform_driver_unregister(&tegra_smmu_driver);\r\n}
