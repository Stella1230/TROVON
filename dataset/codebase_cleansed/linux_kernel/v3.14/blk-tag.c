struct request *blk_queue_find_tag(struct request_queue *q, int tag)\r\n{\r\nreturn blk_map_queue_find_tag(q->queue_tags, tag);\r\n}\r\nstatic int __blk_free_tags(struct blk_queue_tag *bqt)\r\n{\r\nint retval;\r\nretval = atomic_dec_and_test(&bqt->refcnt);\r\nif (retval) {\r\nBUG_ON(find_first_bit(bqt->tag_map, bqt->max_depth) <\r\nbqt->max_depth);\r\nkfree(bqt->tag_index);\r\nbqt->tag_index = NULL;\r\nkfree(bqt->tag_map);\r\nbqt->tag_map = NULL;\r\nkfree(bqt);\r\n}\r\nreturn retval;\r\n}\r\nvoid __blk_queue_free_tags(struct request_queue *q)\r\n{\r\nstruct blk_queue_tag *bqt = q->queue_tags;\r\nif (!bqt)\r\nreturn;\r\n__blk_free_tags(bqt);\r\nq->queue_tags = NULL;\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_QUEUED, q);\r\n}\r\nvoid blk_free_tags(struct blk_queue_tag *bqt)\r\n{\r\nif (unlikely(!__blk_free_tags(bqt)))\r\nBUG();\r\n}\r\nvoid blk_queue_free_tags(struct request_queue *q)\r\n{\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_QUEUED, q);\r\n}\r\nstatic int\r\ninit_tag_map(struct request_queue *q, struct blk_queue_tag *tags, int depth)\r\n{\r\nstruct request **tag_index;\r\nunsigned long *tag_map;\r\nint nr_ulongs;\r\nif (q && depth > q->nr_requests * 2) {\r\ndepth = q->nr_requests * 2;\r\nprintk(KERN_ERR "%s: adjusted depth to %d\n",\r\n__func__, depth);\r\n}\r\ntag_index = kzalloc(depth * sizeof(struct request *), GFP_ATOMIC);\r\nif (!tag_index)\r\ngoto fail;\r\nnr_ulongs = ALIGN(depth, BITS_PER_LONG) / BITS_PER_LONG;\r\ntag_map = kzalloc(nr_ulongs * sizeof(unsigned long), GFP_ATOMIC);\r\nif (!tag_map)\r\ngoto fail;\r\ntags->real_max_depth = depth;\r\ntags->max_depth = depth;\r\ntags->tag_index = tag_index;\r\ntags->tag_map = tag_map;\r\nreturn 0;\r\nfail:\r\nkfree(tag_index);\r\nreturn -ENOMEM;\r\n}\r\nstatic struct blk_queue_tag *__blk_queue_init_tags(struct request_queue *q,\r\nint depth)\r\n{\r\nstruct blk_queue_tag *tags;\r\ntags = kmalloc(sizeof(struct blk_queue_tag), GFP_ATOMIC);\r\nif (!tags)\r\ngoto fail;\r\nif (init_tag_map(q, tags, depth))\r\ngoto fail;\r\natomic_set(&tags->refcnt, 1);\r\nreturn tags;\r\nfail:\r\nkfree(tags);\r\nreturn NULL;\r\n}\r\nstruct blk_queue_tag *blk_init_tags(int depth)\r\n{\r\nreturn __blk_queue_init_tags(NULL, depth);\r\n}\r\nint blk_queue_init_tags(struct request_queue *q, int depth,\r\nstruct blk_queue_tag *tags)\r\n{\r\nint rc;\r\nBUG_ON(tags && q->queue_tags && tags != q->queue_tags);\r\nif (!tags && !q->queue_tags) {\r\ntags = __blk_queue_init_tags(q, depth);\r\nif (!tags)\r\nreturn -ENOMEM;\r\n} else if (q->queue_tags) {\r\nrc = blk_queue_resize_tags(q, depth);\r\nif (rc)\r\nreturn rc;\r\nqueue_flag_set(QUEUE_FLAG_QUEUED, q);\r\nreturn 0;\r\n} else\r\natomic_inc(&tags->refcnt);\r\nq->queue_tags = tags;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_QUEUED, q);\r\nINIT_LIST_HEAD(&q->tag_busy_list);\r\nreturn 0;\r\n}\r\nint blk_queue_resize_tags(struct request_queue *q, int new_depth)\r\n{\r\nstruct blk_queue_tag *bqt = q->queue_tags;\r\nstruct request **tag_index;\r\nunsigned long *tag_map;\r\nint max_depth, nr_ulongs;\r\nif (!bqt)\r\nreturn -ENXIO;\r\nif (new_depth <= bqt->real_max_depth) {\r\nbqt->max_depth = new_depth;\r\nreturn 0;\r\n}\r\nif (atomic_read(&bqt->refcnt) != 1)\r\nreturn -EBUSY;\r\ntag_index = bqt->tag_index;\r\ntag_map = bqt->tag_map;\r\nmax_depth = bqt->real_max_depth;\r\nif (init_tag_map(q, bqt, new_depth))\r\nreturn -ENOMEM;\r\nmemcpy(bqt->tag_index, tag_index, max_depth * sizeof(struct request *));\r\nnr_ulongs = ALIGN(max_depth, BITS_PER_LONG) / BITS_PER_LONG;\r\nmemcpy(bqt->tag_map, tag_map, nr_ulongs * sizeof(unsigned long));\r\nkfree(tag_index);\r\nkfree(tag_map);\r\nreturn 0;\r\n}\r\nvoid blk_queue_end_tag(struct request_queue *q, struct request *rq)\r\n{\r\nstruct blk_queue_tag *bqt = q->queue_tags;\r\nunsigned tag = rq->tag;\r\nBUG_ON(tag >= bqt->real_max_depth);\r\nlist_del_init(&rq->queuelist);\r\nrq->cmd_flags &= ~REQ_QUEUED;\r\nrq->tag = -1;\r\nif (unlikely(bqt->tag_index[tag] == NULL))\r\nprintk(KERN_ERR "%s: tag %d is missing\n",\r\n__func__, tag);\r\nbqt->tag_index[tag] = NULL;\r\nif (unlikely(!test_bit(tag, bqt->tag_map))) {\r\nprintk(KERN_ERR "%s: attempt to clear non-busy tag (%d)\n",\r\n__func__, tag);\r\nreturn;\r\n}\r\nclear_bit_unlock(tag, bqt->tag_map);\r\n}\r\nint blk_queue_start_tag(struct request_queue *q, struct request *rq)\r\n{\r\nstruct blk_queue_tag *bqt = q->queue_tags;\r\nunsigned max_depth;\r\nint tag;\r\nif (unlikely((rq->cmd_flags & REQ_QUEUED))) {\r\nprintk(KERN_ERR\r\n"%s: request %p for device [%s] already tagged %d",\r\n__func__, rq,\r\nrq->rq_disk ? rq->rq_disk->disk_name : "?", rq->tag);\r\nBUG();\r\n}\r\nmax_depth = bqt->max_depth;\r\nif (!rq_is_sync(rq) && max_depth > 1) {\r\nswitch (max_depth) {\r\ncase 2:\r\nmax_depth = 1;\r\nbreak;\r\ncase 3:\r\nmax_depth = 2;\r\nbreak;\r\ndefault:\r\nmax_depth -= 2;\r\n}\r\nif (q->in_flight[BLK_RW_ASYNC] > max_depth)\r\nreturn 1;\r\n}\r\ndo {\r\ntag = find_first_zero_bit(bqt->tag_map, max_depth);\r\nif (tag >= max_depth)\r\nreturn 1;\r\n} while (test_and_set_bit_lock(tag, bqt->tag_map));\r\nrq->cmd_flags |= REQ_QUEUED;\r\nrq->tag = tag;\r\nbqt->tag_index[tag] = rq;\r\nblk_start_request(rq);\r\nlist_add(&rq->queuelist, &q->tag_busy_list);\r\nreturn 0;\r\n}\r\nvoid blk_queue_invalidate_tags(struct request_queue *q)\r\n{\r\nstruct list_head *tmp, *n;\r\nlist_for_each_safe(tmp, n, &q->tag_busy_list)\r\nblk_requeue_request(q, list_entry_rq(tmp));\r\n}
