static struct drm_i915_gem_object *dma_buf_to_obj(struct dma_buf *buf)\r\n{\r\nreturn to_intel_bo(buf->priv);\r\n}\r\nstatic struct sg_table *i915_gem_map_dma_buf(struct dma_buf_attachment *attachment,\r\nenum dma_data_direction dir)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(attachment->dmabuf);\r\nstruct sg_table *st;\r\nstruct scatterlist *src, *dst;\r\nint ret, i;\r\nret = i915_mutex_lock_interruptible(obj->base.dev);\r\nif (ret)\r\ngoto err;\r\nret = i915_gem_object_get_pages(obj);\r\nif (ret)\r\ngoto err_unlock;\r\ni915_gem_object_pin_pages(obj);\r\nst = kmalloc(sizeof(struct sg_table), GFP_KERNEL);\r\nif (st == NULL) {\r\nret = -ENOMEM;\r\ngoto err_unpin;\r\n}\r\nret = sg_alloc_table(st, obj->pages->nents, GFP_KERNEL);\r\nif (ret)\r\ngoto err_free;\r\nsrc = obj->pages->sgl;\r\ndst = st->sgl;\r\nfor (i = 0; i < obj->pages->nents; i++) {\r\nsg_set_page(dst, sg_page(src), src->length, 0);\r\ndst = sg_next(dst);\r\nsrc = sg_next(src);\r\n}\r\nif (!dma_map_sg(attachment->dev, st->sgl, st->nents, dir)) {\r\nret =-ENOMEM;\r\ngoto err_free_sg;\r\n}\r\nmutex_unlock(&obj->base.dev->struct_mutex);\r\nreturn st;\r\nerr_free_sg:\r\nsg_free_table(st);\r\nerr_free:\r\nkfree(st);\r\nerr_unpin:\r\ni915_gem_object_unpin_pages(obj);\r\nerr_unlock:\r\nmutex_unlock(&obj->base.dev->struct_mutex);\r\nerr:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void i915_gem_unmap_dma_buf(struct dma_buf_attachment *attachment,\r\nstruct sg_table *sg,\r\nenum dma_data_direction dir)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(attachment->dmabuf);\r\nmutex_lock(&obj->base.dev->struct_mutex);\r\ndma_unmap_sg(attachment->dev, sg->sgl, sg->nents, dir);\r\nsg_free_table(sg);\r\nkfree(sg);\r\ni915_gem_object_unpin_pages(obj);\r\nmutex_unlock(&obj->base.dev->struct_mutex);\r\n}\r\nstatic void *i915_gem_dmabuf_vmap(struct dma_buf *dma_buf)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nstruct drm_device *dev = obj->base.dev;\r\nstruct sg_page_iter sg_iter;\r\nstruct page **pages;\r\nint ret, i;\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nif (obj->dma_buf_vmapping) {\r\nobj->vmapping_count++;\r\ngoto out_unlock;\r\n}\r\nret = i915_gem_object_get_pages(obj);\r\nif (ret)\r\ngoto err;\r\ni915_gem_object_pin_pages(obj);\r\nret = -ENOMEM;\r\npages = drm_malloc_ab(obj->base.size >> PAGE_SHIFT, sizeof(*pages));\r\nif (pages == NULL)\r\ngoto err_unpin;\r\ni = 0;\r\nfor_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0)\r\npages[i++] = sg_page_iter_page(&sg_iter);\r\nobj->dma_buf_vmapping = vmap(pages, i, 0, PAGE_KERNEL);\r\ndrm_free_large(pages);\r\nif (!obj->dma_buf_vmapping)\r\ngoto err_unpin;\r\nobj->vmapping_count = 1;\r\nout_unlock:\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn obj->dma_buf_vmapping;\r\nerr_unpin:\r\ni915_gem_object_unpin_pages(obj);\r\nerr:\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void i915_gem_dmabuf_vunmap(struct dma_buf *dma_buf, void *vaddr)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nstruct drm_device *dev = obj->base.dev;\r\nint ret;\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\nreturn;\r\nif (--obj->vmapping_count == 0) {\r\nvunmap(obj->dma_buf_vmapping);\r\nobj->dma_buf_vmapping = NULL;\r\ni915_gem_object_unpin_pages(obj);\r\n}\r\nmutex_unlock(&dev->struct_mutex);\r\n}\r\nstatic void *i915_gem_dmabuf_kmap_atomic(struct dma_buf *dma_buf, unsigned long page_num)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void i915_gem_dmabuf_kunmap_atomic(struct dma_buf *dma_buf, unsigned long page_num, void *addr)\r\n{\r\n}\r\nstatic void *i915_gem_dmabuf_kmap(struct dma_buf *dma_buf, unsigned long page_num)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void i915_gem_dmabuf_kunmap(struct dma_buf *dma_buf, unsigned long page_num, void *addr)\r\n{\r\n}\r\nstatic int i915_gem_dmabuf_mmap(struct dma_buf *dma_buf, struct vm_area_struct *vma)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int i915_gem_begin_cpu_access(struct dma_buf *dma_buf, size_t start, size_t length, enum dma_data_direction direction)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nstruct drm_device *dev = obj->base.dev;\r\nint ret;\r\nbool write = (direction == DMA_BIDIRECTIONAL || direction == DMA_TO_DEVICE);\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\nreturn ret;\r\nret = i915_gem_object_set_to_cpu_domain(obj, write);\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn ret;\r\n}\r\nstruct dma_buf *i915_gem_prime_export(struct drm_device *dev,\r\nstruct drm_gem_object *gem_obj, int flags)\r\n{\r\nreturn dma_buf_export(gem_obj, &i915_dmabuf_ops, gem_obj->size, flags);\r\n}\r\nstatic int i915_gem_object_get_pages_dmabuf(struct drm_i915_gem_object *obj)\r\n{\r\nstruct sg_table *sg;\r\nsg = dma_buf_map_attachment(obj->base.import_attach, DMA_BIDIRECTIONAL);\r\nif (IS_ERR(sg))\r\nreturn PTR_ERR(sg);\r\nobj->pages = sg;\r\nobj->has_dma_mapping = true;\r\nreturn 0;\r\n}\r\nstatic void i915_gem_object_put_pages_dmabuf(struct drm_i915_gem_object *obj)\r\n{\r\ndma_buf_unmap_attachment(obj->base.import_attach,\r\nobj->pages, DMA_BIDIRECTIONAL);\r\nobj->has_dma_mapping = false;\r\n}\r\nstruct drm_gem_object *i915_gem_prime_import(struct drm_device *dev,\r\nstruct dma_buf *dma_buf)\r\n{\r\nstruct dma_buf_attachment *attach;\r\nstruct drm_i915_gem_object *obj;\r\nint ret;\r\nif (dma_buf->ops == &i915_dmabuf_ops) {\r\nobj = dma_buf_to_obj(dma_buf);\r\nif (obj->base.dev == dev) {\r\ndrm_gem_object_reference(&obj->base);\r\nreturn &obj->base;\r\n}\r\n}\r\nattach = dma_buf_attach(dma_buf, dev->dev);\r\nif (IS_ERR(attach))\r\nreturn ERR_CAST(attach);\r\nget_dma_buf(dma_buf);\r\nobj = i915_gem_object_alloc(dev);\r\nif (obj == NULL) {\r\nret = -ENOMEM;\r\ngoto fail_detach;\r\n}\r\ndrm_gem_private_object_init(dev, &obj->base, dma_buf->size);\r\ni915_gem_object_init(obj, &i915_gem_object_dmabuf_ops);\r\nobj->base.import_attach = attach;\r\nreturn &obj->base;\r\nfail_detach:\r\ndma_buf_detach(dma_buf, attach);\r\ndma_buf_put(dma_buf);\r\nreturn ERR_PTR(ret);\r\n}
