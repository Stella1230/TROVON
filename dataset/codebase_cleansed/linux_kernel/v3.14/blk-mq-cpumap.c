static void show_map(unsigned int *map, unsigned int nr)\r\n{\r\nint i;\r\npr_info("blk-mq: CPU -> queue map\n");\r\nfor_each_online_cpu(i)\r\npr_info(" CPU%2u -> Queue %u\n", i, map[i]);\r\n}\r\nstatic int cpu_to_queue_index(unsigned int nr_cpus, unsigned int nr_queues,\r\nconst int cpu)\r\n{\r\nreturn cpu / ((nr_cpus + nr_queues - 1) / nr_queues);\r\n}\r\nstatic int get_first_sibling(unsigned int cpu)\r\n{\r\nunsigned int ret;\r\nret = cpumask_first(topology_thread_cpumask(cpu));\r\nif (ret < nr_cpu_ids)\r\nreturn ret;\r\nreturn cpu;\r\n}\r\nint blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues)\r\n{\r\nunsigned int i, nr_cpus, nr_uniq_cpus, queue, first_sibling;\r\ncpumask_var_t cpus;\r\nif (!alloc_cpumask_var(&cpus, GFP_ATOMIC))\r\nreturn 1;\r\ncpumask_clear(cpus);\r\nnr_cpus = nr_uniq_cpus = 0;\r\nfor_each_online_cpu(i) {\r\nnr_cpus++;\r\nfirst_sibling = get_first_sibling(i);\r\nif (!cpumask_test_cpu(first_sibling, cpus))\r\nnr_uniq_cpus++;\r\ncpumask_set_cpu(i, cpus);\r\n}\r\nqueue = 0;\r\nfor_each_possible_cpu(i) {\r\nif (!cpu_online(i)) {\r\nmap[i] = 0;\r\ncontinue;\r\n}\r\nif (nr_queues >= nr_cpus || nr_cpus == nr_uniq_cpus) {\r\nmap[i] = cpu_to_queue_index(nr_cpus, nr_queues, queue);\r\nqueue++;\r\ncontinue;\r\n}\r\nfirst_sibling = get_first_sibling(i);\r\nif (first_sibling == i) {\r\nmap[i] = cpu_to_queue_index(nr_uniq_cpus, nr_queues,\r\nqueue);\r\nqueue++;\r\n} else\r\nmap[i] = map[first_sibling];\r\n}\r\nshow_map(map, nr_cpus);\r\nfree_cpumask_var(cpus);\r\nreturn 0;\r\n}\r\nunsigned int *blk_mq_make_queue_map(struct blk_mq_reg *reg)\r\n{\r\nunsigned int *map;\r\nmap = kzalloc_node(sizeof(*map) * num_possible_cpus(), GFP_KERNEL,\r\nreg->numa_node);\r\nif (!map)\r\nreturn NULL;\r\nif (!blk_mq_update_queue_map(map, reg->nr_hw_queues))\r\nreturn map;\r\nkfree(map);\r\nreturn NULL;\r\n}
