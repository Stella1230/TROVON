static u32 mthca_buddy_alloc(struct mthca_buddy *buddy, int order)\r\n{\r\nint o;\r\nint m;\r\nu32 seg;\r\nspin_lock(&buddy->lock);\r\nfor (o = order; o <= buddy->max_order; ++o)\r\nif (buddy->num_free[o]) {\r\nm = 1 << (buddy->max_order - o);\r\nseg = find_first_bit(buddy->bits[o], m);\r\nif (seg < m)\r\ngoto found;\r\n}\r\nspin_unlock(&buddy->lock);\r\nreturn -1;\r\nfound:\r\nclear_bit(seg, buddy->bits[o]);\r\n--buddy->num_free[o];\r\nwhile (o > order) {\r\n--o;\r\nseg <<= 1;\r\nset_bit(seg ^ 1, buddy->bits[o]);\r\n++buddy->num_free[o];\r\n}\r\nspin_unlock(&buddy->lock);\r\nseg <<= order;\r\nreturn seg;\r\n}\r\nstatic void mthca_buddy_free(struct mthca_buddy *buddy, u32 seg, int order)\r\n{\r\nseg >>= order;\r\nspin_lock(&buddy->lock);\r\nwhile (test_bit(seg ^ 1, buddy->bits[order])) {\r\nclear_bit(seg ^ 1, buddy->bits[order]);\r\n--buddy->num_free[order];\r\nseg >>= 1;\r\n++order;\r\n}\r\nset_bit(seg, buddy->bits[order]);\r\n++buddy->num_free[order];\r\nspin_unlock(&buddy->lock);\r\n}\r\nstatic int mthca_buddy_init(struct mthca_buddy *buddy, int max_order)\r\n{\r\nint i, s;\r\nbuddy->max_order = max_order;\r\nspin_lock_init(&buddy->lock);\r\nbuddy->bits = kzalloc((buddy->max_order + 1) * sizeof (long *),\r\nGFP_KERNEL);\r\nbuddy->num_free = kcalloc((buddy->max_order + 1), sizeof *buddy->num_free,\r\nGFP_KERNEL);\r\nif (!buddy->bits || !buddy->num_free)\r\ngoto err_out;\r\nfor (i = 0; i <= buddy->max_order; ++i) {\r\ns = BITS_TO_LONGS(1 << (buddy->max_order - i));\r\nbuddy->bits[i] = kmalloc(s * sizeof (long), GFP_KERNEL);\r\nif (!buddy->bits[i])\r\ngoto err_out_free;\r\nbitmap_zero(buddy->bits[i],\r\n1 << (buddy->max_order - i));\r\n}\r\nset_bit(0, buddy->bits[buddy->max_order]);\r\nbuddy->num_free[buddy->max_order] = 1;\r\nreturn 0;\r\nerr_out_free:\r\nfor (i = 0; i <= buddy->max_order; ++i)\r\nkfree(buddy->bits[i]);\r\nerr_out:\r\nkfree(buddy->bits);\r\nkfree(buddy->num_free);\r\nreturn -ENOMEM;\r\n}\r\nstatic void mthca_buddy_cleanup(struct mthca_buddy *buddy)\r\n{\r\nint i;\r\nfor (i = 0; i <= buddy->max_order; ++i)\r\nkfree(buddy->bits[i]);\r\nkfree(buddy->bits);\r\nkfree(buddy->num_free);\r\n}\r\nstatic u32 mthca_alloc_mtt_range(struct mthca_dev *dev, int order,\r\nstruct mthca_buddy *buddy)\r\n{\r\nu32 seg = mthca_buddy_alloc(buddy, order);\r\nif (seg == -1)\r\nreturn -1;\r\nif (mthca_is_memfree(dev))\r\nif (mthca_table_get_range(dev, dev->mr_table.mtt_table, seg,\r\nseg + (1 << order) - 1)) {\r\nmthca_buddy_free(buddy, seg, order);\r\nseg = -1;\r\n}\r\nreturn seg;\r\n}\r\nstatic struct mthca_mtt *__mthca_alloc_mtt(struct mthca_dev *dev, int size,\r\nstruct mthca_buddy *buddy)\r\n{\r\nstruct mthca_mtt *mtt;\r\nint i;\r\nif (size <= 0)\r\nreturn ERR_PTR(-EINVAL);\r\nmtt = kmalloc(sizeof *mtt, GFP_KERNEL);\r\nif (!mtt)\r\nreturn ERR_PTR(-ENOMEM);\r\nmtt->buddy = buddy;\r\nmtt->order = 0;\r\nfor (i = dev->limits.mtt_seg_size / 8; i < size; i <<= 1)\r\n++mtt->order;\r\nmtt->first_seg = mthca_alloc_mtt_range(dev, mtt->order, buddy);\r\nif (mtt->first_seg == -1) {\r\nkfree(mtt);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nreturn mtt;\r\n}\r\nstruct mthca_mtt *mthca_alloc_mtt(struct mthca_dev *dev, int size)\r\n{\r\nreturn __mthca_alloc_mtt(dev, size, &dev->mr_table.mtt_buddy);\r\n}\r\nvoid mthca_free_mtt(struct mthca_dev *dev, struct mthca_mtt *mtt)\r\n{\r\nif (!mtt)\r\nreturn;\r\nmthca_buddy_free(mtt->buddy, mtt->first_seg, mtt->order);\r\nmthca_table_put_range(dev, dev->mr_table.mtt_table,\r\nmtt->first_seg,\r\nmtt->first_seg + (1 << mtt->order) - 1);\r\nkfree(mtt);\r\n}\r\nstatic int __mthca_write_mtt(struct mthca_dev *dev, struct mthca_mtt *mtt,\r\nint start_index, u64 *buffer_list, int list_len)\r\n{\r\nstruct mthca_mailbox *mailbox;\r\n__be64 *mtt_entry;\r\nint err = 0;\r\nint i;\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox))\r\nreturn PTR_ERR(mailbox);\r\nmtt_entry = mailbox->buf;\r\nwhile (list_len > 0) {\r\nmtt_entry[0] = cpu_to_be64(dev->mr_table.mtt_base +\r\nmtt->first_seg * dev->limits.mtt_seg_size +\r\nstart_index * 8);\r\nmtt_entry[1] = 0;\r\nfor (i = 0; i < list_len && i < MTHCA_MAILBOX_SIZE / 8 - 2; ++i)\r\nmtt_entry[i + 2] = cpu_to_be64(buffer_list[i] |\r\nMTHCA_MTT_FLAG_PRESENT);\r\nif (i & 1)\r\nmtt_entry[i + 2] = 0;\r\nerr = mthca_WRITE_MTT(dev, mailbox, (i + 1) & ~1);\r\nif (err) {\r\nmthca_warn(dev, "WRITE_MTT failed (%d)\n", err);\r\ngoto out;\r\n}\r\nlist_len -= i;\r\nstart_index += i;\r\nbuffer_list += i;\r\n}\r\nout:\r\nmthca_free_mailbox(dev, mailbox);\r\nreturn err;\r\n}\r\nint mthca_write_mtt_size(struct mthca_dev *dev)\r\n{\r\nif (dev->mr_table.fmr_mtt_buddy != &dev->mr_table.mtt_buddy ||\r\n!(dev->mthca_flags & MTHCA_FLAG_FMR))\r\nreturn PAGE_SIZE / sizeof (u64) - 2;\r\nreturn mthca_is_memfree(dev) ? (PAGE_SIZE / sizeof (u64)) : 0x7ffffff;\r\n}\r\nstatic void mthca_tavor_write_mtt_seg(struct mthca_dev *dev,\r\nstruct mthca_mtt *mtt, int start_index,\r\nu64 *buffer_list, int list_len)\r\n{\r\nu64 __iomem *mtts;\r\nint i;\r\nmtts = dev->mr_table.tavor_fmr.mtt_base + mtt->first_seg * dev->limits.mtt_seg_size +\r\nstart_index * sizeof (u64);\r\nfor (i = 0; i < list_len; ++i)\r\nmthca_write64_raw(cpu_to_be64(buffer_list[i] | MTHCA_MTT_FLAG_PRESENT),\r\nmtts + i);\r\n}\r\nstatic void mthca_arbel_write_mtt_seg(struct mthca_dev *dev,\r\nstruct mthca_mtt *mtt, int start_index,\r\nu64 *buffer_list, int list_len)\r\n{\r\n__be64 *mtts;\r\ndma_addr_t dma_handle;\r\nint i;\r\nint s = start_index * sizeof (u64);\r\nBUG_ON(s / PAGE_SIZE != (s + list_len * sizeof(u64) - 1) / PAGE_SIZE);\r\nBUG_ON(s % dev->limits.mtt_seg_size);\r\nmtts = mthca_table_find(dev->mr_table.mtt_table, mtt->first_seg +\r\ns / dev->limits.mtt_seg_size, &dma_handle);\r\nBUG_ON(!mtts);\r\ndma_sync_single_for_cpu(&dev->pdev->dev, dma_handle,\r\nlist_len * sizeof (u64), DMA_TO_DEVICE);\r\nfor (i = 0; i < list_len; ++i)\r\nmtts[i] = cpu_to_be64(buffer_list[i] | MTHCA_MTT_FLAG_PRESENT);\r\ndma_sync_single_for_device(&dev->pdev->dev, dma_handle,\r\nlist_len * sizeof (u64), DMA_TO_DEVICE);\r\n}\r\nint mthca_write_mtt(struct mthca_dev *dev, struct mthca_mtt *mtt,\r\nint start_index, u64 *buffer_list, int list_len)\r\n{\r\nint size = mthca_write_mtt_size(dev);\r\nint chunk;\r\nif (dev->mr_table.fmr_mtt_buddy != &dev->mr_table.mtt_buddy ||\r\n!(dev->mthca_flags & MTHCA_FLAG_FMR))\r\nreturn __mthca_write_mtt(dev, mtt, start_index, buffer_list, list_len);\r\nwhile (list_len > 0) {\r\nchunk = min(size, list_len);\r\nif (mthca_is_memfree(dev))\r\nmthca_arbel_write_mtt_seg(dev, mtt, start_index,\r\nbuffer_list, chunk);\r\nelse\r\nmthca_tavor_write_mtt_seg(dev, mtt, start_index,\r\nbuffer_list, chunk);\r\nlist_len -= chunk;\r\nstart_index += chunk;\r\nbuffer_list += chunk;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline u32 tavor_hw_index_to_key(u32 ind)\r\n{\r\nreturn ind;\r\n}\r\nstatic inline u32 tavor_key_to_hw_index(u32 key)\r\n{\r\nreturn key;\r\n}\r\nstatic inline u32 arbel_hw_index_to_key(u32 ind)\r\n{\r\nreturn (ind >> 24) | (ind << 8);\r\n}\r\nstatic inline u32 arbel_key_to_hw_index(u32 key)\r\n{\r\nreturn (key << 24) | (key >> 8);\r\n}\r\nstatic inline u32 hw_index_to_key(struct mthca_dev *dev, u32 ind)\r\n{\r\nif (mthca_is_memfree(dev))\r\nreturn arbel_hw_index_to_key(ind);\r\nelse\r\nreturn tavor_hw_index_to_key(ind);\r\n}\r\nstatic inline u32 key_to_hw_index(struct mthca_dev *dev, u32 key)\r\n{\r\nif (mthca_is_memfree(dev))\r\nreturn arbel_key_to_hw_index(key);\r\nelse\r\nreturn tavor_key_to_hw_index(key);\r\n}\r\nstatic inline u32 adjust_key(struct mthca_dev *dev, u32 key)\r\n{\r\nif (dev->mthca_flags & MTHCA_FLAG_SINAI_OPT)\r\nreturn ((key << 20) & 0x800000) | (key & 0x7fffff);\r\nelse\r\nreturn key;\r\n}\r\nint mthca_mr_alloc(struct mthca_dev *dev, u32 pd, int buffer_size_shift,\r\nu64 iova, u64 total_size, u32 access, struct mthca_mr *mr)\r\n{\r\nstruct mthca_mailbox *mailbox;\r\nstruct mthca_mpt_entry *mpt_entry;\r\nu32 key;\r\nint i;\r\nint err;\r\nWARN_ON(buffer_size_shift >= 32);\r\nkey = mthca_alloc(&dev->mr_table.mpt_alloc);\r\nif (key == -1)\r\nreturn -ENOMEM;\r\nkey = adjust_key(dev, key);\r\nmr->ibmr.rkey = mr->ibmr.lkey = hw_index_to_key(dev, key);\r\nif (mthca_is_memfree(dev)) {\r\nerr = mthca_table_get(dev, dev->mr_table.mpt_table, key);\r\nif (err)\r\ngoto err_out_mpt_free;\r\n}\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox)) {\r\nerr = PTR_ERR(mailbox);\r\ngoto err_out_table;\r\n}\r\nmpt_entry = mailbox->buf;\r\nmpt_entry->flags = cpu_to_be32(MTHCA_MPT_FLAG_SW_OWNS |\r\nMTHCA_MPT_FLAG_MIO |\r\nMTHCA_MPT_FLAG_REGION |\r\naccess);\r\nif (!mr->mtt)\r\nmpt_entry->flags |= cpu_to_be32(MTHCA_MPT_FLAG_PHYSICAL);\r\nmpt_entry->page_size = cpu_to_be32(buffer_size_shift - 12);\r\nmpt_entry->key = cpu_to_be32(key);\r\nmpt_entry->pd = cpu_to_be32(pd);\r\nmpt_entry->start = cpu_to_be64(iova);\r\nmpt_entry->length = cpu_to_be64(total_size);\r\nmemset(&mpt_entry->lkey, 0,\r\nsizeof *mpt_entry - offsetof(struct mthca_mpt_entry, lkey));\r\nif (mr->mtt)\r\nmpt_entry->mtt_seg =\r\ncpu_to_be64(dev->mr_table.mtt_base +\r\nmr->mtt->first_seg * dev->limits.mtt_seg_size);\r\nif (0) {\r\nmthca_dbg(dev, "Dumping MPT entry %08x:\n", mr->ibmr.lkey);\r\nfor (i = 0; i < sizeof (struct mthca_mpt_entry) / 4; ++i) {\r\nif (i % 4 == 0)\r\nprintk("[%02x] ", i * 4);\r\nprintk(" %08x", be32_to_cpu(((__be32 *) mpt_entry)[i]));\r\nif ((i + 1) % 4 == 0)\r\nprintk("\n");\r\n}\r\n}\r\nerr = mthca_SW2HW_MPT(dev, mailbox,\r\nkey & (dev->limits.num_mpts - 1));\r\nif (err) {\r\nmthca_warn(dev, "SW2HW_MPT failed (%d)\n", err);\r\ngoto err_out_mailbox;\r\n}\r\nmthca_free_mailbox(dev, mailbox);\r\nreturn err;\r\nerr_out_mailbox:\r\nmthca_free_mailbox(dev, mailbox);\r\nerr_out_table:\r\nmthca_table_put(dev, dev->mr_table.mpt_table, key);\r\nerr_out_mpt_free:\r\nmthca_free(&dev->mr_table.mpt_alloc, key);\r\nreturn err;\r\n}\r\nint mthca_mr_alloc_notrans(struct mthca_dev *dev, u32 pd,\r\nu32 access, struct mthca_mr *mr)\r\n{\r\nmr->mtt = NULL;\r\nreturn mthca_mr_alloc(dev, pd, 12, 0, ~0ULL, access, mr);\r\n}\r\nint mthca_mr_alloc_phys(struct mthca_dev *dev, u32 pd,\r\nu64 *buffer_list, int buffer_size_shift,\r\nint list_len, u64 iova, u64 total_size,\r\nu32 access, struct mthca_mr *mr)\r\n{\r\nint err;\r\nmr->mtt = mthca_alloc_mtt(dev, list_len);\r\nif (IS_ERR(mr->mtt))\r\nreturn PTR_ERR(mr->mtt);\r\nerr = mthca_write_mtt(dev, mr->mtt, 0, buffer_list, list_len);\r\nif (err) {\r\nmthca_free_mtt(dev, mr->mtt);\r\nreturn err;\r\n}\r\nerr = mthca_mr_alloc(dev, pd, buffer_size_shift, iova,\r\ntotal_size, access, mr);\r\nif (err)\r\nmthca_free_mtt(dev, mr->mtt);\r\nreturn err;\r\n}\r\nstatic void mthca_free_region(struct mthca_dev *dev, u32 lkey)\r\n{\r\nmthca_table_put(dev, dev->mr_table.mpt_table,\r\nkey_to_hw_index(dev, lkey));\r\nmthca_free(&dev->mr_table.mpt_alloc, key_to_hw_index(dev, lkey));\r\n}\r\nvoid mthca_free_mr(struct mthca_dev *dev, struct mthca_mr *mr)\r\n{\r\nint err;\r\nerr = mthca_HW2SW_MPT(dev, NULL,\r\nkey_to_hw_index(dev, mr->ibmr.lkey) &\r\n(dev->limits.num_mpts - 1));\r\nif (err)\r\nmthca_warn(dev, "HW2SW_MPT failed (%d)\n", err);\r\nmthca_free_region(dev, mr->ibmr.lkey);\r\nmthca_free_mtt(dev, mr->mtt);\r\n}\r\nint mthca_fmr_alloc(struct mthca_dev *dev, u32 pd,\r\nu32 access, struct mthca_fmr *mr)\r\n{\r\nstruct mthca_mpt_entry *mpt_entry;\r\nstruct mthca_mailbox *mailbox;\r\nu64 mtt_seg;\r\nu32 key, idx;\r\nint list_len = mr->attr.max_pages;\r\nint err = -ENOMEM;\r\nint i;\r\nif (mr->attr.page_shift < 12 || mr->attr.page_shift >= 32)\r\nreturn -EINVAL;\r\nif (mthca_is_memfree(dev) &&\r\nmr->attr.max_pages * sizeof *mr->mem.arbel.mtts > PAGE_SIZE)\r\nreturn -EINVAL;\r\nmr->maps = 0;\r\nkey = mthca_alloc(&dev->mr_table.mpt_alloc);\r\nif (key == -1)\r\nreturn -ENOMEM;\r\nkey = adjust_key(dev, key);\r\nidx = key & (dev->limits.num_mpts - 1);\r\nmr->ibmr.rkey = mr->ibmr.lkey = hw_index_to_key(dev, key);\r\nif (mthca_is_memfree(dev)) {\r\nerr = mthca_table_get(dev, dev->mr_table.mpt_table, key);\r\nif (err)\r\ngoto err_out_mpt_free;\r\nmr->mem.arbel.mpt = mthca_table_find(dev->mr_table.mpt_table, key, NULL);\r\nBUG_ON(!mr->mem.arbel.mpt);\r\n} else\r\nmr->mem.tavor.mpt = dev->mr_table.tavor_fmr.mpt_base +\r\nsizeof *(mr->mem.tavor.mpt) * idx;\r\nmr->mtt = __mthca_alloc_mtt(dev, list_len, dev->mr_table.fmr_mtt_buddy);\r\nif (IS_ERR(mr->mtt)) {\r\nerr = PTR_ERR(mr->mtt);\r\ngoto err_out_table;\r\n}\r\nmtt_seg = mr->mtt->first_seg * dev->limits.mtt_seg_size;\r\nif (mthca_is_memfree(dev)) {\r\nmr->mem.arbel.mtts = mthca_table_find(dev->mr_table.mtt_table,\r\nmr->mtt->first_seg,\r\n&mr->mem.arbel.dma_handle);\r\nBUG_ON(!mr->mem.arbel.mtts);\r\n} else\r\nmr->mem.tavor.mtts = dev->mr_table.tavor_fmr.mtt_base + mtt_seg;\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox)) {\r\nerr = PTR_ERR(mailbox);\r\ngoto err_out_free_mtt;\r\n}\r\nmpt_entry = mailbox->buf;\r\nmpt_entry->flags = cpu_to_be32(MTHCA_MPT_FLAG_SW_OWNS |\r\nMTHCA_MPT_FLAG_MIO |\r\nMTHCA_MPT_FLAG_REGION |\r\naccess);\r\nmpt_entry->page_size = cpu_to_be32(mr->attr.page_shift - 12);\r\nmpt_entry->key = cpu_to_be32(key);\r\nmpt_entry->pd = cpu_to_be32(pd);\r\nmemset(&mpt_entry->start, 0,\r\nsizeof *mpt_entry - offsetof(struct mthca_mpt_entry, start));\r\nmpt_entry->mtt_seg = cpu_to_be64(dev->mr_table.mtt_base + mtt_seg);\r\nif (0) {\r\nmthca_dbg(dev, "Dumping MPT entry %08x:\n", mr->ibmr.lkey);\r\nfor (i = 0; i < sizeof (struct mthca_mpt_entry) / 4; ++i) {\r\nif (i % 4 == 0)\r\nprintk("[%02x] ", i * 4);\r\nprintk(" %08x", be32_to_cpu(((__be32 *) mpt_entry)[i]));\r\nif ((i + 1) % 4 == 0)\r\nprintk("\n");\r\n}\r\n}\r\nerr = mthca_SW2HW_MPT(dev, mailbox,\r\nkey & (dev->limits.num_mpts - 1));\r\nif (err) {\r\nmthca_warn(dev, "SW2HW_MPT failed (%d)\n", err);\r\ngoto err_out_mailbox_free;\r\n}\r\nmthca_free_mailbox(dev, mailbox);\r\nreturn 0;\r\nerr_out_mailbox_free:\r\nmthca_free_mailbox(dev, mailbox);\r\nerr_out_free_mtt:\r\nmthca_free_mtt(dev, mr->mtt);\r\nerr_out_table:\r\nmthca_table_put(dev, dev->mr_table.mpt_table, key);\r\nerr_out_mpt_free:\r\nmthca_free(&dev->mr_table.mpt_alloc, key);\r\nreturn err;\r\n}\r\nint mthca_free_fmr(struct mthca_dev *dev, struct mthca_fmr *fmr)\r\n{\r\nif (fmr->maps)\r\nreturn -EBUSY;\r\nmthca_free_region(dev, fmr->ibmr.lkey);\r\nmthca_free_mtt(dev, fmr->mtt);\r\nreturn 0;\r\n}\r\nstatic inline int mthca_check_fmr(struct mthca_fmr *fmr, u64 *page_list,\r\nint list_len, u64 iova)\r\n{\r\nint i, page_mask;\r\nif (list_len > fmr->attr.max_pages)\r\nreturn -EINVAL;\r\npage_mask = (1 << fmr->attr.page_shift) - 1;\r\nif (iova & page_mask)\r\nreturn -EINVAL;\r\nif (0)\r\nfor (i = 0; i < list_len; ++i) {\r\nif (page_list[i] & ~page_mask)\r\nreturn -EINVAL;\r\n}\r\nif (fmr->maps >= fmr->attr.max_maps)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nint mthca_tavor_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,\r\nint list_len, u64 iova)\r\n{\r\nstruct mthca_fmr *fmr = to_mfmr(ibfmr);\r\nstruct mthca_dev *dev = to_mdev(ibfmr->device);\r\nstruct mthca_mpt_entry mpt_entry;\r\nu32 key;\r\nint i, err;\r\nerr = mthca_check_fmr(fmr, page_list, list_len, iova);\r\nif (err)\r\nreturn err;\r\n++fmr->maps;\r\nkey = tavor_key_to_hw_index(fmr->ibmr.lkey);\r\nkey += dev->limits.num_mpts;\r\nfmr->ibmr.lkey = fmr->ibmr.rkey = tavor_hw_index_to_key(key);\r\nwriteb(MTHCA_MPT_STATUS_SW, fmr->mem.tavor.mpt);\r\nfor (i = 0; i < list_len; ++i) {\r\n__be64 mtt_entry = cpu_to_be64(page_list[i] |\r\nMTHCA_MTT_FLAG_PRESENT);\r\nmthca_write64_raw(mtt_entry, fmr->mem.tavor.mtts + i);\r\n}\r\nmpt_entry.lkey = cpu_to_be32(key);\r\nmpt_entry.length = cpu_to_be64(list_len * (1ull << fmr->attr.page_shift));\r\nmpt_entry.start = cpu_to_be64(iova);\r\n__raw_writel((__force u32) mpt_entry.lkey, &fmr->mem.tavor.mpt->key);\r\nmemcpy_toio(&fmr->mem.tavor.mpt->start, &mpt_entry.start,\r\noffsetof(struct mthca_mpt_entry, window_count) -\r\noffsetof(struct mthca_mpt_entry, start));\r\nwriteb(MTHCA_MPT_STATUS_HW, fmr->mem.tavor.mpt);\r\nreturn 0;\r\n}\r\nint mthca_arbel_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,\r\nint list_len, u64 iova)\r\n{\r\nstruct mthca_fmr *fmr = to_mfmr(ibfmr);\r\nstruct mthca_dev *dev = to_mdev(ibfmr->device);\r\nu32 key;\r\nint i, err;\r\nerr = mthca_check_fmr(fmr, page_list, list_len, iova);\r\nif (err)\r\nreturn err;\r\n++fmr->maps;\r\nkey = arbel_key_to_hw_index(fmr->ibmr.lkey);\r\nif (dev->mthca_flags & MTHCA_FLAG_SINAI_OPT)\r\nkey += SINAI_FMR_KEY_INC;\r\nelse\r\nkey += dev->limits.num_mpts;\r\nfmr->ibmr.lkey = fmr->ibmr.rkey = arbel_hw_index_to_key(key);\r\n*(u8 *) fmr->mem.arbel.mpt = MTHCA_MPT_STATUS_SW;\r\nwmb();\r\ndma_sync_single_for_cpu(&dev->pdev->dev, fmr->mem.arbel.dma_handle,\r\nlist_len * sizeof(u64), DMA_TO_DEVICE);\r\nfor (i = 0; i < list_len; ++i)\r\nfmr->mem.arbel.mtts[i] = cpu_to_be64(page_list[i] |\r\nMTHCA_MTT_FLAG_PRESENT);\r\ndma_sync_single_for_device(&dev->pdev->dev, fmr->mem.arbel.dma_handle,\r\nlist_len * sizeof(u64), DMA_TO_DEVICE);\r\nfmr->mem.arbel.mpt->key = cpu_to_be32(key);\r\nfmr->mem.arbel.mpt->lkey = cpu_to_be32(key);\r\nfmr->mem.arbel.mpt->length = cpu_to_be64(list_len * (1ull << fmr->attr.page_shift));\r\nfmr->mem.arbel.mpt->start = cpu_to_be64(iova);\r\nwmb();\r\n*(u8 *) fmr->mem.arbel.mpt = MTHCA_MPT_STATUS_HW;\r\nwmb();\r\nreturn 0;\r\n}\r\nvoid mthca_tavor_fmr_unmap(struct mthca_dev *dev, struct mthca_fmr *fmr)\r\n{\r\nif (!fmr->maps)\r\nreturn;\r\nfmr->maps = 0;\r\nwriteb(MTHCA_MPT_STATUS_SW, fmr->mem.tavor.mpt);\r\n}\r\nvoid mthca_arbel_fmr_unmap(struct mthca_dev *dev, struct mthca_fmr *fmr)\r\n{\r\nif (!fmr->maps)\r\nreturn;\r\nfmr->maps = 0;\r\n*(u8 *) fmr->mem.arbel.mpt = MTHCA_MPT_STATUS_SW;\r\n}\r\nint mthca_init_mr_table(struct mthca_dev *dev)\r\n{\r\nphys_addr_t addr;\r\nint mpts, mtts, err, i;\r\nerr = mthca_alloc_init(&dev->mr_table.mpt_alloc,\r\ndev->limits.num_mpts,\r\n~0, dev->limits.reserved_mrws);\r\nif (err)\r\nreturn err;\r\nif (!mthca_is_memfree(dev) &&\r\n(dev->mthca_flags & MTHCA_FLAG_DDR_HIDDEN))\r\ndev->limits.fmr_reserved_mtts = 0;\r\nelse\r\ndev->mthca_flags |= MTHCA_FLAG_FMR;\r\nif (dev->mthca_flags & MTHCA_FLAG_SINAI_OPT)\r\nmthca_dbg(dev, "Memory key throughput optimization activated.\n");\r\nerr = mthca_buddy_init(&dev->mr_table.mtt_buddy,\r\nfls(dev->limits.num_mtt_segs - 1));\r\nif (err)\r\ngoto err_mtt_buddy;\r\ndev->mr_table.tavor_fmr.mpt_base = NULL;\r\ndev->mr_table.tavor_fmr.mtt_base = NULL;\r\nif (dev->limits.fmr_reserved_mtts) {\r\ni = fls(dev->limits.fmr_reserved_mtts - 1);\r\nif (i >= 31) {\r\nmthca_warn(dev, "Unable to reserve 2^31 FMR MTTs.\n");\r\nerr = -EINVAL;\r\ngoto err_fmr_mpt;\r\n}\r\nmpts = mtts = 1 << i;\r\n} else {\r\nmtts = dev->limits.num_mtt_segs;\r\nmpts = dev->limits.num_mpts;\r\n}\r\nif (!mthca_is_memfree(dev) &&\r\n(dev->mthca_flags & MTHCA_FLAG_FMR)) {\r\naddr = pci_resource_start(dev->pdev, 4) +\r\n((pci_resource_len(dev->pdev, 4) - 1) &\r\ndev->mr_table.mpt_base);\r\ndev->mr_table.tavor_fmr.mpt_base =\r\nioremap(addr, mpts * sizeof(struct mthca_mpt_entry));\r\nif (!dev->mr_table.tavor_fmr.mpt_base) {\r\nmthca_warn(dev, "MPT ioremap for FMR failed.\n");\r\nerr = -ENOMEM;\r\ngoto err_fmr_mpt;\r\n}\r\naddr = pci_resource_start(dev->pdev, 4) +\r\n((pci_resource_len(dev->pdev, 4) - 1) &\r\ndev->mr_table.mtt_base);\r\ndev->mr_table.tavor_fmr.mtt_base =\r\nioremap(addr, mtts * dev->limits.mtt_seg_size);\r\nif (!dev->mr_table.tavor_fmr.mtt_base) {\r\nmthca_warn(dev, "MTT ioremap for FMR failed.\n");\r\nerr = -ENOMEM;\r\ngoto err_fmr_mtt;\r\n}\r\n}\r\nif (dev->limits.fmr_reserved_mtts) {\r\nerr = mthca_buddy_init(&dev->mr_table.tavor_fmr.mtt_buddy, fls(mtts - 1));\r\nif (err)\r\ngoto err_fmr_mtt_buddy;\r\nerr = mthca_buddy_alloc(&dev->mr_table.mtt_buddy, fls(mtts - 1));\r\nif (err)\r\ngoto err_reserve_fmr;\r\ndev->mr_table.fmr_mtt_buddy =\r\n&dev->mr_table.tavor_fmr.mtt_buddy;\r\n} else\r\ndev->mr_table.fmr_mtt_buddy = &dev->mr_table.mtt_buddy;\r\nif (dev->limits.reserved_mtts) {\r\ni = fls(dev->limits.reserved_mtts - 1);\r\nif (mthca_alloc_mtt_range(dev, i,\r\ndev->mr_table.fmr_mtt_buddy) == -1) {\r\nmthca_warn(dev, "MTT table of order %d is too small.\n",\r\ndev->mr_table.fmr_mtt_buddy->max_order);\r\nerr = -ENOMEM;\r\ngoto err_reserve_mtts;\r\n}\r\n}\r\nreturn 0;\r\nerr_reserve_mtts:\r\nerr_reserve_fmr:\r\nif (dev->limits.fmr_reserved_mtts)\r\nmthca_buddy_cleanup(&dev->mr_table.tavor_fmr.mtt_buddy);\r\nerr_fmr_mtt_buddy:\r\nif (dev->mr_table.tavor_fmr.mtt_base)\r\niounmap(dev->mr_table.tavor_fmr.mtt_base);\r\nerr_fmr_mtt:\r\nif (dev->mr_table.tavor_fmr.mpt_base)\r\niounmap(dev->mr_table.tavor_fmr.mpt_base);\r\nerr_fmr_mpt:\r\nmthca_buddy_cleanup(&dev->mr_table.mtt_buddy);\r\nerr_mtt_buddy:\r\nmthca_alloc_cleanup(&dev->mr_table.mpt_alloc);\r\nreturn err;\r\n}\r\nvoid mthca_cleanup_mr_table(struct mthca_dev *dev)\r\n{\r\nif (dev->limits.fmr_reserved_mtts)\r\nmthca_buddy_cleanup(&dev->mr_table.tavor_fmr.mtt_buddy);\r\nmthca_buddy_cleanup(&dev->mr_table.mtt_buddy);\r\nif (dev->mr_table.tavor_fmr.mtt_base)\r\niounmap(dev->mr_table.tavor_fmr.mtt_base);\r\nif (dev->mr_table.tavor_fmr.mpt_base)\r\niounmap(dev->mr_table.tavor_fmr.mpt_base);\r\nmthca_alloc_cleanup(&dev->mr_table.mpt_alloc);\r\n}
