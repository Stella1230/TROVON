static inline void\r\nsh64_setup_dtlb_cache_slot(unsigned long eaddr, unsigned long asid,\r\nunsigned long paddr)\r\n{\r\nlocal_irq_disable();\r\nsh64_setup_tlb_slot(dtlb_cache_slot, eaddr, asid, paddr);\r\n}\r\nstatic inline void sh64_teardown_dtlb_cache_slot(void)\r\n{\r\nsh64_teardown_tlb_slot(dtlb_cache_slot);\r\nlocal_irq_enable();\r\n}\r\nstatic inline void sh64_icache_inv_all(void)\r\n{\r\nunsigned long long addr, flag, data;\r\nunsigned long flags;\r\naddr = ICCR0;\r\nflag = ICCR0_ICI;\r\ndata = 0;\r\nlocal_irq_save(flags);\r\n__asm__ __volatile__ (\r\n"getcfg %3, 0, %0\n\t"\r\n"or %0, %2, %0\n\t"\r\n"putcfg %3, 0, %0\n\t"\r\n"synci"\r\n: "=&r" (data)\r\n: "0" (data), "r" (flag), "r" (addr));\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void sh64_icache_inv_kernel_range(unsigned long start, unsigned long end)\r\n{\r\nunsigned long long ullend, addr, aligned_start;\r\naligned_start = (unsigned long long)(signed long long)(signed long) start;\r\naddr = L1_CACHE_ALIGN(aligned_start);\r\nullend = (unsigned long long) (signed long long) (signed long) end;\r\nwhile (addr <= ullend) {\r\n__asm__ __volatile__ ("icbi %0, 0" : : "r" (addr));\r\naddr += L1_CACHE_BYTES;\r\n}\r\n}\r\nstatic void sh64_icache_inv_user_page(struct vm_area_struct *vma, unsigned long eaddr)\r\n{\r\nunsigned int cpu = smp_processor_id();\r\nunsigned long long addr, end_addr;\r\nunsigned long flags = 0;\r\nunsigned long running_asid, vma_asid;\r\naddr = eaddr;\r\nend_addr = addr + PAGE_SIZE;\r\nrunning_asid = get_asid();\r\nvma_asid = cpu_asid(cpu, vma->vm_mm);\r\nif (running_asid != vma_asid) {\r\nlocal_irq_save(flags);\r\nswitch_and_save_asid(vma_asid);\r\n}\r\nwhile (addr < end_addr) {\r\n__asm__ __volatile__("icbi %0, 0" : : "r" (addr));\r\n__asm__ __volatile__("icbi %0, 32" : : "r" (addr));\r\n__asm__ __volatile__("icbi %0, 64" : : "r" (addr));\r\n__asm__ __volatile__("icbi %0, 96" : : "r" (addr));\r\naddr += 128;\r\n}\r\nif (running_asid != vma_asid) {\r\nswitch_and_save_asid(running_asid);\r\nlocal_irq_restore(flags);\r\n}\r\n}\r\nstatic void sh64_icache_inv_user_page_range(struct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nint n_pages;\r\nif (!mm)\r\nreturn;\r\nn_pages = ((end - start) >> PAGE_SHIFT);\r\nif (n_pages >= 64) {\r\nsh64_icache_inv_all();\r\n} else {\r\nunsigned long aligned_start;\r\nunsigned long eaddr;\r\nunsigned long after_last_page_start;\r\nunsigned long mm_asid, current_asid;\r\nunsigned long flags = 0;\r\nmm_asid = cpu_asid(smp_processor_id(), mm);\r\ncurrent_asid = get_asid();\r\nif (mm_asid != current_asid) {\r\nlocal_irq_save(flags);\r\nswitch_and_save_asid(mm_asid);\r\n}\r\naligned_start = start & PAGE_MASK;\r\nafter_last_page_start = PAGE_SIZE + ((end - 1) & PAGE_MASK);\r\nwhile (aligned_start < after_last_page_start) {\r\nstruct vm_area_struct *vma;\r\nunsigned long vma_end;\r\nvma = find_vma(mm, aligned_start);\r\nif (!vma || (aligned_start <= vma->vm_end)) {\r\naligned_start += PAGE_SIZE;\r\ncontinue;\r\n}\r\nvma_end = vma->vm_end;\r\nif (vma->vm_flags & VM_EXEC) {\r\neaddr = aligned_start;\r\nwhile (eaddr < vma_end) {\r\nsh64_icache_inv_user_page(vma, eaddr);\r\neaddr += PAGE_SIZE;\r\n}\r\n}\r\naligned_start = vma->vm_end;\r\n}\r\nif (mm_asid != current_asid) {\r\nswitch_and_save_asid(current_asid);\r\nlocal_irq_restore(flags);\r\n}\r\n}\r\n}\r\nstatic void sh64_icache_inv_current_user_range(unsigned long start, unsigned long end)\r\n{\r\nunsigned long long aligned_start;\r\nunsigned long long ull_end;\r\nunsigned long long addr;\r\null_end = end;\r\naligned_start = L1_CACHE_ALIGN(start);\r\naddr = aligned_start;\r\nwhile (addr < ull_end) {\r\n__asm__ __volatile__ ("icbi %0, 0" : : "r" (addr));\r\n__asm__ __volatile__ ("nop");\r\n__asm__ __volatile__ ("nop");\r\naddr += L1_CACHE_BYTES;\r\n}\r\n}\r\nstatic void inline sh64_dcache_purge_sets(int sets_to_purge_base, int n_sets)\r\n{\r\nint dummy_buffer_base_set;\r\nunsigned long long eaddr, eaddr0, eaddr1;\r\nint j;\r\nint set_offset;\r\ndummy_buffer_base_set = ((int)&dummy_alloco_area &\r\ncpu_data->dcache.entry_mask) >>\r\ncpu_data->dcache.entry_shift;\r\nset_offset = sets_to_purge_base - dummy_buffer_base_set;\r\nfor (j = 0; j < n_sets; j++, set_offset++) {\r\nset_offset &= (cpu_data->dcache.sets - 1);\r\neaddr0 = (unsigned long long)dummy_alloco_area +\r\n(set_offset << cpu_data->dcache.entry_shift);\r\neaddr1 = eaddr0 + cpu_data->dcache.way_size *\r\ncpu_data->dcache.ways;\r\nfor (eaddr = eaddr0; eaddr < eaddr1;\r\neaddr += cpu_data->dcache.way_size) {\r\n__asm__ __volatile__ ("alloco %0, 0" : : "r" (eaddr));\r\n__asm__ __volatile__ ("synco");\r\n}\r\neaddr1 = eaddr0 + cpu_data->dcache.way_size *\r\ncpu_data->dcache.ways;\r\nfor (eaddr = eaddr0; eaddr < eaddr1;\r\neaddr += cpu_data->dcache.way_size) {\r\nif (test_bit(SH_CACHE_MODE_WT, &(cpu_data->dcache.flags)))\r\n__raw_readb((unsigned long)eaddr);\r\n}\r\n}\r\n}\r\nstatic void sh64_dcache_purge_all(void)\r\n{\r\nsh64_dcache_purge_sets(0, cpu_data->dcache.sets);\r\n}\r\nstatic void sh64_dcache_purge_coloured_phy_page(unsigned long paddr,\r\nunsigned long eaddr)\r\n{\r\nunsigned long long magic_page_start;\r\nunsigned long long magic_eaddr, magic_eaddr_end;\r\nmagic_page_start = MAGIC_PAGE0_START + (eaddr & CACHE_OC_SYN_MASK);\r\nsh64_setup_dtlb_cache_slot(magic_page_start, get_asid(), paddr);\r\nmagic_eaddr = magic_page_start;\r\nmagic_eaddr_end = magic_eaddr + PAGE_SIZE;\r\nwhile (magic_eaddr < magic_eaddr_end) {\r\n__asm__ __volatile__ ("ocbp %0, 0" : : "r" (magic_eaddr));\r\nmagic_eaddr += L1_CACHE_BYTES;\r\n}\r\nsh64_teardown_dtlb_cache_slot();\r\n}\r\nstatic void sh64_dcache_purge_phy_page(unsigned long paddr)\r\n{\r\nunsigned long long eaddr_start, eaddr, eaddr_end;\r\nint i;\r\neaddr_start = MAGIC_PAGE0_START;\r\nfor (i = 0; i < (1 << CACHE_OC_N_SYNBITS); i++) {\r\nsh64_setup_dtlb_cache_slot(eaddr_start, get_asid(), paddr);\r\neaddr = eaddr_start;\r\neaddr_end = eaddr + PAGE_SIZE;\r\nwhile (eaddr < eaddr_end) {\r\n__asm__ __volatile__ ("ocbp %0, 0" : : "r" (eaddr));\r\neaddr += L1_CACHE_BYTES;\r\n}\r\nsh64_teardown_dtlb_cache_slot();\r\neaddr_start += PAGE_SIZE;\r\n}\r\n}\r\nstatic void sh64_dcache_purge_user_pages(struct mm_struct *mm,\r\nunsigned long addr, unsigned long end)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\npte_t entry;\r\nspinlock_t *ptl;\r\nunsigned long paddr;\r\nif (!mm)\r\nreturn;\r\npgd = pgd_offset(mm, addr);\r\nif (pgd_bad(*pgd))\r\nreturn;\r\npud = pud_offset(pgd, addr);\r\nif (pud_none(*pud) || pud_bad(*pud))\r\nreturn;\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd) || pmd_bad(*pmd))\r\nreturn;\r\npte = pte_offset_map_lock(mm, pmd, addr, &ptl);\r\ndo {\r\nentry = *pte;\r\nif (pte_none(entry) || !pte_present(entry))\r\ncontinue;\r\npaddr = pte_val(entry) & PAGE_MASK;\r\nsh64_dcache_purge_coloured_phy_page(paddr, addr);\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\npte_unmap_unlock(pte - 1, ptl);\r\n}\r\nstatic void sh64_dcache_purge_user_range(struct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nint n_pages = ((end - start) >> PAGE_SHIFT);\r\nif (n_pages >= 64 || ((start ^ (end - 1)) & PMD_MASK)) {\r\nsh64_dcache_purge_all();\r\n} else {\r\nstart &= PAGE_MASK;\r\nend = PAGE_ALIGN(end);\r\nsh64_dcache_purge_user_pages(mm, start, end);\r\n}\r\n}\r\nstatic void sh5_flush_cache_all(void *unused)\r\n{\r\nsh64_dcache_purge_all();\r\nsh64_icache_inv_all();\r\n}\r\nstatic void sh5_flush_cache_mm(void *unused)\r\n{\r\nsh64_dcache_purge_all();\r\n}\r\nstatic void sh5_flush_cache_range(void *args)\r\n{\r\nstruct flusher_data *data = args;\r\nstruct vm_area_struct *vma;\r\nunsigned long start, end;\r\nvma = data->vma;\r\nstart = data->addr1;\r\nend = data->addr2;\r\nsh64_dcache_purge_user_range(vma->vm_mm, start, end);\r\nsh64_icache_inv_user_page_range(vma->vm_mm, start, end);\r\n}\r\nstatic void sh5_flush_cache_page(void *args)\r\n{\r\nstruct flusher_data *data = args;\r\nstruct vm_area_struct *vma;\r\nunsigned long eaddr, pfn;\r\nvma = data->vma;\r\neaddr = data->addr1;\r\npfn = data->addr2;\r\nsh64_dcache_purge_phy_page(pfn << PAGE_SHIFT);\r\nif (vma->vm_flags & VM_EXEC)\r\nsh64_icache_inv_user_page(vma, eaddr);\r\n}\r\nstatic void sh5_flush_dcache_page(void *page)\r\n{\r\nsh64_dcache_purge_phy_page(page_to_phys((struct page *)page));\r\nwmb();\r\n}\r\nstatic void sh5_flush_icache_range(void *args)\r\n{\r\nstruct flusher_data *data = args;\r\nunsigned long start, end;\r\nstart = data->addr1;\r\nend = data->addr2;\r\n__flush_purge_region((void *)start, end);\r\nwmb();\r\nsh64_icache_inv_kernel_range(start, end);\r\n}\r\nstatic void sh5_flush_cache_sigtramp(void *vaddr)\r\n{\r\nunsigned long end = (unsigned long)vaddr + L1_CACHE_BYTES;\r\n__flush_wback_region(vaddr, L1_CACHE_BYTES);\r\nwmb();\r\nsh64_icache_inv_current_user_range((unsigned long)vaddr, end);\r\n}\r\nvoid __init sh5_cache_init(void)\r\n{\r\nlocal_flush_cache_all = sh5_flush_cache_all;\r\nlocal_flush_cache_mm = sh5_flush_cache_mm;\r\nlocal_flush_cache_dup_mm = sh5_flush_cache_mm;\r\nlocal_flush_cache_page = sh5_flush_cache_page;\r\nlocal_flush_cache_range = sh5_flush_cache_range;\r\nlocal_flush_dcache_page = sh5_flush_dcache_page;\r\nlocal_flush_icache_range = sh5_flush_icache_range;\r\nlocal_flush_cache_sigtramp = sh5_flush_cache_sigtramp;\r\ndtlb_cache_slot = sh64_get_wired_dtlb_entry();\r\nsh4__flush_region_init();\r\n}
