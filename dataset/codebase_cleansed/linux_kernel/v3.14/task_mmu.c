void task_mem(struct seq_file *m, struct mm_struct *mm)\r\n{\r\nunsigned long data, text, lib, swap;\r\nunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\r\nhiwater_vm = total_vm = mm->total_vm;\r\nif (hiwater_vm < mm->hiwater_vm)\r\nhiwater_vm = mm->hiwater_vm;\r\nhiwater_rss = total_rss = get_mm_rss(mm);\r\nif (hiwater_rss < mm->hiwater_rss)\r\nhiwater_rss = mm->hiwater_rss;\r\ndata = mm->total_vm - mm->shared_vm - mm->stack_vm;\r\ntext = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK)) >> 10;\r\nlib = (mm->exec_vm << (PAGE_SHIFT-10)) - text;\r\nswap = get_mm_counter(mm, MM_SWAPENTS);\r\nseq_printf(m,\r\n"VmPeak:\t%8lu kB\n"\r\n"VmSize:\t%8lu kB\n"\r\n"VmLck:\t%8lu kB\n"\r\n"VmPin:\t%8lu kB\n"\r\n"VmHWM:\t%8lu kB\n"\r\n"VmRSS:\t%8lu kB\n"\r\n"VmData:\t%8lu kB\n"\r\n"VmStk:\t%8lu kB\n"\r\n"VmExe:\t%8lu kB\n"\r\n"VmLib:\t%8lu kB\n"\r\n"VmPTE:\t%8lu kB\n"\r\n"VmSwap:\t%8lu kB\n",\r\nhiwater_vm << (PAGE_SHIFT-10),\r\ntotal_vm << (PAGE_SHIFT-10),\r\nmm->locked_vm << (PAGE_SHIFT-10),\r\nmm->pinned_vm << (PAGE_SHIFT-10),\r\nhiwater_rss << (PAGE_SHIFT-10),\r\ntotal_rss << (PAGE_SHIFT-10),\r\ndata << (PAGE_SHIFT-10),\r\nmm->stack_vm << (PAGE_SHIFT-10), text, lib,\r\n(PTRS_PER_PTE * sizeof(pte_t) *\r\natomic_long_read(&mm->nr_ptes)) >> 10,\r\nswap << (PAGE_SHIFT-10));\r\n}\r\nunsigned long task_vsize(struct mm_struct *mm)\r\n{\r\nreturn PAGE_SIZE * mm->total_vm;\r\n}\r\nunsigned long task_statm(struct mm_struct *mm,\r\nunsigned long *shared, unsigned long *text,\r\nunsigned long *data, unsigned long *resident)\r\n{\r\n*shared = get_mm_counter(mm, MM_FILEPAGES);\r\n*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\r\n>> PAGE_SHIFT;\r\n*data = mm->total_vm - mm->shared_vm;\r\n*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\r\nreturn mm->total_vm;\r\n}\r\nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\r\n{\r\nstruct task_struct *task = priv->task;\r\ntask_lock(task);\r\npriv->task_mempolicy = task->mempolicy;\r\nmpol_get(priv->task_mempolicy);\r\ntask_unlock(task);\r\n}\r\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\r\n{\r\nmpol_put(priv->task_mempolicy);\r\n}\r\nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\r\n{\r\n}\r\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\r\n{\r\n}\r\nstatic void vma_stop(struct proc_maps_private *priv, struct vm_area_struct *vma)\r\n{\r\nif (vma && vma != priv->tail_vma) {\r\nstruct mm_struct *mm = vma->vm_mm;\r\nrelease_task_mempolicy(priv);\r\nup_read(&mm->mmap_sem);\r\nmmput(mm);\r\n}\r\n}\r\nstatic void *m_start(struct seq_file *m, loff_t *pos)\r\n{\r\nstruct proc_maps_private *priv = m->private;\r\nunsigned long last_addr = m->version;\r\nstruct mm_struct *mm;\r\nstruct vm_area_struct *vma, *tail_vma = NULL;\r\nloff_t l = *pos;\r\npriv->task = NULL;\r\npriv->tail_vma = NULL;\r\nif (last_addr == -1UL)\r\nreturn NULL;\r\npriv->task = get_pid_task(priv->pid, PIDTYPE_PID);\r\nif (!priv->task)\r\nreturn ERR_PTR(-ESRCH);\r\nmm = mm_access(priv->task, PTRACE_MODE_READ);\r\nif (!mm || IS_ERR(mm))\r\nreturn mm;\r\ndown_read(&mm->mmap_sem);\r\ntail_vma = get_gate_vma(priv->task->mm);\r\npriv->tail_vma = tail_vma;\r\nhold_task_mempolicy(priv);\r\nvma = find_vma(mm, last_addr);\r\nif (last_addr && vma) {\r\nvma = vma->vm_next;\r\ngoto out;\r\n}\r\nvma = NULL;\r\nif ((unsigned long)l < mm->map_count) {\r\nvma = mm->mmap;\r\nwhile (l-- && vma)\r\nvma = vma->vm_next;\r\ngoto out;\r\n}\r\nif (l != mm->map_count)\r\ntail_vma = NULL;\r\nout:\r\nif (vma)\r\nreturn vma;\r\nrelease_task_mempolicy(priv);\r\nm->version = (tail_vma != NULL)? 0: -1UL;\r\nup_read(&mm->mmap_sem);\r\nmmput(mm);\r\nreturn tail_vma;\r\n}\r\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\r\n{\r\nstruct proc_maps_private *priv = m->private;\r\nstruct vm_area_struct *vma = v;\r\nstruct vm_area_struct *tail_vma = priv->tail_vma;\r\n(*pos)++;\r\nif (vma && (vma != tail_vma) && vma->vm_next)\r\nreturn vma->vm_next;\r\nvma_stop(priv, vma);\r\nreturn (vma != tail_vma)? tail_vma: NULL;\r\n}\r\nstatic void m_stop(struct seq_file *m, void *v)\r\n{\r\nstruct proc_maps_private *priv = m->private;\r\nstruct vm_area_struct *vma = v;\r\nif (!IS_ERR(vma))\r\nvma_stop(priv, vma);\r\nif (priv->task)\r\nput_task_struct(priv->task);\r\n}\r\nstatic int do_maps_open(struct inode *inode, struct file *file,\r\nconst struct seq_operations *ops)\r\n{\r\nstruct proc_maps_private *priv;\r\nint ret = -ENOMEM;\r\npriv = kzalloc(sizeof(*priv), GFP_KERNEL);\r\nif (priv) {\r\npriv->pid = proc_pid(inode);\r\nret = seq_open(file, ops);\r\nif (!ret) {\r\nstruct seq_file *m = file->private_data;\r\nm->private = priv;\r\n} else {\r\nkfree(priv);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void\r\nshow_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct file *file = vma->vm_file;\r\nstruct proc_maps_private *priv = m->private;\r\nstruct task_struct *task = priv->task;\r\nvm_flags_t flags = vma->vm_flags;\r\nunsigned long ino = 0;\r\nunsigned long long pgoff = 0;\r\nunsigned long start, end;\r\ndev_t dev = 0;\r\nconst char *name = NULL;\r\nif (file) {\r\nstruct inode *inode = file_inode(vma->vm_file);\r\ndev = inode->i_sb->s_dev;\r\nino = inode->i_ino;\r\npgoff = ((loff_t)vma->vm_pgoff) << PAGE_SHIFT;\r\n}\r\nstart = vma->vm_start;\r\nif (stack_guard_page_start(vma, start))\r\nstart += PAGE_SIZE;\r\nend = vma->vm_end;\r\nif (stack_guard_page_end(vma, end))\r\nend -= PAGE_SIZE;\r\nseq_setwidth(m, 25 + sizeof(void *) * 6 - 1);\r\nseq_printf(m, "%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu ",\r\nstart,\r\nend,\r\nflags & VM_READ ? 'r' : '-',\r\nflags & VM_WRITE ? 'w' : '-',\r\nflags & VM_EXEC ? 'x' : '-',\r\nflags & VM_MAYSHARE ? 's' : 'p',\r\npgoff,\r\nMAJOR(dev), MINOR(dev), ino);\r\nif (file) {\r\nseq_pad(m, ' ');\r\nseq_path(m, &file->f_path, "\n");\r\ngoto done;\r\n}\r\nname = arch_vma_name(vma);\r\nif (!name) {\r\npid_t tid;\r\nif (!mm) {\r\nname = "[vdso]";\r\ngoto done;\r\n}\r\nif (vma->vm_start <= mm->brk &&\r\nvma->vm_end >= mm->start_brk) {\r\nname = "[heap]";\r\ngoto done;\r\n}\r\ntid = vm_is_stack(task, vma, is_pid);\r\nif (tid != 0) {\r\nif (!is_pid || (vma->vm_start <= mm->start_stack &&\r\nvma->vm_end >= mm->start_stack)) {\r\nname = "[stack]";\r\n} else {\r\nseq_pad(m, ' ');\r\nseq_printf(m, "[stack:%d]", tid);\r\n}\r\n}\r\n}\r\ndone:\r\nif (name) {\r\nseq_pad(m, ' ');\r\nseq_puts(m, name);\r\n}\r\nseq_putc(m, '\n');\r\n}\r\nstatic int show_map(struct seq_file *m, void *v, int is_pid)\r\n{\r\nstruct vm_area_struct *vma = v;\r\nstruct proc_maps_private *priv = m->private;\r\nstruct task_struct *task = priv->task;\r\nshow_map_vma(m, vma, is_pid);\r\nif (m->count < m->size)\r\nm->version = (vma != get_gate_vma(task->mm))\r\n? vma->vm_start : 0;\r\nreturn 0;\r\n}\r\nstatic int show_pid_map(struct seq_file *m, void *v)\r\n{\r\nreturn show_map(m, v, 1);\r\n}\r\nstatic int show_tid_map(struct seq_file *m, void *v)\r\n{\r\nreturn show_map(m, v, 0);\r\n}\r\nstatic int pid_maps_open(struct inode *inode, struct file *file)\r\n{\r\nreturn do_maps_open(inode, file, &proc_pid_maps_op);\r\n}\r\nstatic int tid_maps_open(struct inode *inode, struct file *file)\r\n{\r\nreturn do_maps_open(inode, file, &proc_tid_maps_op);\r\n}\r\nstatic void smaps_pte_entry(pte_t ptent, unsigned long addr,\r\nunsigned long ptent_size, struct mm_walk *walk)\r\n{\r\nstruct mem_size_stats *mss = walk->private;\r\nstruct vm_area_struct *vma = mss->vma;\r\npgoff_t pgoff = linear_page_index(vma, addr);\r\nstruct page *page = NULL;\r\nint mapcount;\r\nif (pte_present(ptent)) {\r\npage = vm_normal_page(vma, addr, ptent);\r\n} else if (is_swap_pte(ptent)) {\r\nswp_entry_t swpent = pte_to_swp_entry(ptent);\r\nif (!non_swap_entry(swpent))\r\nmss->swap += ptent_size;\r\nelse if (is_migration_entry(swpent))\r\npage = migration_entry_to_page(swpent);\r\n} else if (pte_file(ptent)) {\r\nif (pte_to_pgoff(ptent) != pgoff)\r\nmss->nonlinear += ptent_size;\r\n}\r\nif (!page)\r\nreturn;\r\nif (PageAnon(page))\r\nmss->anonymous += ptent_size;\r\nif (page->index != pgoff)\r\nmss->nonlinear += ptent_size;\r\nmss->resident += ptent_size;\r\nif (pte_young(ptent) || PageReferenced(page))\r\nmss->referenced += ptent_size;\r\nmapcount = page_mapcount(page);\r\nif (mapcount >= 2) {\r\nif (pte_dirty(ptent) || PageDirty(page))\r\nmss->shared_dirty += ptent_size;\r\nelse\r\nmss->shared_clean += ptent_size;\r\nmss->pss += (ptent_size << PSS_SHIFT) / mapcount;\r\n} else {\r\nif (pte_dirty(ptent) || PageDirty(page))\r\nmss->private_dirty += ptent_size;\r\nelse\r\nmss->private_clean += ptent_size;\r\nmss->pss += (ptent_size << PSS_SHIFT);\r\n}\r\n}\r\nstatic int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\r\nstruct mm_walk *walk)\r\n{\r\nstruct mem_size_stats *mss = walk->private;\r\nstruct vm_area_struct *vma = mss->vma;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nif (pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {\r\nsmaps_pte_entry(*(pte_t *)pmd, addr, HPAGE_PMD_SIZE, walk);\r\nspin_unlock(ptl);\r\nmss->anonymous_thp += HPAGE_PMD_SIZE;\r\nreturn 0;\r\n}\r\nif (pmd_trans_unstable(pmd))\r\nreturn 0;\r\npte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\r\nfor (; addr != end; pte++, addr += PAGE_SIZE)\r\nsmaps_pte_entry(*pte, addr, PAGE_SIZE, walk);\r\npte_unmap_unlock(pte - 1, ptl);\r\ncond_resched();\r\nreturn 0;\r\n}\r\nstatic void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)\r\n{\r\nstatic const char mnemonics[BITS_PER_LONG][2] = {\r\n[0 ... (BITS_PER_LONG-1)] = "??",\r\n[ilog2(VM_READ)] = "rd",\r\n[ilog2(VM_WRITE)] = "wr",\r\n[ilog2(VM_EXEC)] = "ex",\r\n[ilog2(VM_SHARED)] = "sh",\r\n[ilog2(VM_MAYREAD)] = "mr",\r\n[ilog2(VM_MAYWRITE)] = "mw",\r\n[ilog2(VM_MAYEXEC)] = "me",\r\n[ilog2(VM_MAYSHARE)] = "ms",\r\n[ilog2(VM_GROWSDOWN)] = "gd",\r\n[ilog2(VM_PFNMAP)] = "pf",\r\n[ilog2(VM_DENYWRITE)] = "dw",\r\n[ilog2(VM_LOCKED)] = "lo",\r\n[ilog2(VM_IO)] = "io",\r\n[ilog2(VM_SEQ_READ)] = "sr",\r\n[ilog2(VM_RAND_READ)] = "rr",\r\n[ilog2(VM_DONTCOPY)] = "dc",\r\n[ilog2(VM_DONTEXPAND)] = "de",\r\n[ilog2(VM_ACCOUNT)] = "ac",\r\n[ilog2(VM_NORESERVE)] = "nr",\r\n[ilog2(VM_HUGETLB)] = "ht",\r\n[ilog2(VM_NONLINEAR)] = "nl",\r\n[ilog2(VM_ARCH_1)] = "ar",\r\n[ilog2(VM_DONTDUMP)] = "dd",\r\n#ifdef CONFIG_MEM_SOFT_DIRTY\r\n[ilog2(VM_SOFTDIRTY)] = "sd",\r\n#endif\r\n[ilog2(VM_MIXEDMAP)] = "mm",\r\n[ilog2(VM_HUGEPAGE)] = "hg",\r\n[ilog2(VM_NOHUGEPAGE)] = "nh",\r\n[ilog2(VM_MERGEABLE)] = "mg",\r\n};\r\nsize_t i;\r\nseq_puts(m, "VmFlags: ");\r\nfor (i = 0; i < BITS_PER_LONG; i++) {\r\nif (vma->vm_flags & (1UL << i)) {\r\nseq_printf(m, "%c%c ",\r\nmnemonics[i][0], mnemonics[i][1]);\r\n}\r\n}\r\nseq_putc(m, '\n');\r\n}\r\nstatic int show_smap(struct seq_file *m, void *v, int is_pid)\r\n{\r\nstruct proc_maps_private *priv = m->private;\r\nstruct task_struct *task = priv->task;\r\nstruct vm_area_struct *vma = v;\r\nstruct mem_size_stats mss;\r\nstruct mm_walk smaps_walk = {\r\n.pmd_entry = smaps_pte_range,\r\n.mm = vma->vm_mm,\r\n.private = &mss,\r\n};\r\nmemset(&mss, 0, sizeof mss);\r\nmss.vma = vma;\r\nif (vma->vm_mm && !is_vm_hugetlb_page(vma))\r\nwalk_page_range(vma->vm_start, vma->vm_end, &smaps_walk);\r\nshow_map_vma(m, vma, is_pid);\r\nseq_printf(m,\r\n"Size: %8lu kB\n"\r\n"Rss: %8lu kB\n"\r\n"Pss: %8lu kB\n"\r\n"Shared_Clean: %8lu kB\n"\r\n"Shared_Dirty: %8lu kB\n"\r\n"Private_Clean: %8lu kB\n"\r\n"Private_Dirty: %8lu kB\n"\r\n"Referenced: %8lu kB\n"\r\n"Anonymous: %8lu kB\n"\r\n"AnonHugePages: %8lu kB\n"\r\n"Swap: %8lu kB\n"\r\n"KernelPageSize: %8lu kB\n"\r\n"MMUPageSize: %8lu kB\n"\r\n"Locked: %8lu kB\n",\r\n(vma->vm_end - vma->vm_start) >> 10,\r\nmss.resident >> 10,\r\n(unsigned long)(mss.pss >> (10 + PSS_SHIFT)),\r\nmss.shared_clean >> 10,\r\nmss.shared_dirty >> 10,\r\nmss.private_clean >> 10,\r\nmss.private_dirty >> 10,\r\nmss.referenced >> 10,\r\nmss.anonymous >> 10,\r\nmss.anonymous_thp >> 10,\r\nmss.swap >> 10,\r\nvma_kernel_pagesize(vma) >> 10,\r\nvma_mmu_pagesize(vma) >> 10,\r\n(vma->vm_flags & VM_LOCKED) ?\r\n(unsigned long)(mss.pss >> (10 + PSS_SHIFT)) : 0);\r\nif (vma->vm_flags & VM_NONLINEAR)\r\nseq_printf(m, "Nonlinear: %8lu kB\n",\r\nmss.nonlinear >> 10);\r\nshow_smap_vma_flags(m, vma);\r\nif (m->count < m->size)\r\nm->version = (vma != get_gate_vma(task->mm))\r\n? vma->vm_start : 0;\r\nreturn 0;\r\n}\r\nstatic int show_pid_smap(struct seq_file *m, void *v)\r\n{\r\nreturn show_smap(m, v, 1);\r\n}\r\nstatic int show_tid_smap(struct seq_file *m, void *v)\r\n{\r\nreturn show_smap(m, v, 0);\r\n}\r\nstatic int pid_smaps_open(struct inode *inode, struct file *file)\r\n{\r\nreturn do_maps_open(inode, file, &proc_pid_smaps_op);\r\n}\r\nstatic int tid_smaps_open(struct inode *inode, struct file *file)\r\n{\r\nreturn do_maps_open(inode, file, &proc_tid_smaps_op);\r\n}\r\nstatic inline void clear_soft_dirty(struct vm_area_struct *vma,\r\nunsigned long addr, pte_t *pte)\r\n{\r\n#ifdef CONFIG_MEM_SOFT_DIRTY\r\npte_t ptent = *pte;\r\nif (pte_present(ptent)) {\r\nptent = pte_wrprotect(ptent);\r\nptent = pte_clear_flags(ptent, _PAGE_SOFT_DIRTY);\r\n} else if (is_swap_pte(ptent)) {\r\nptent = pte_swp_clear_soft_dirty(ptent);\r\n} else if (pte_file(ptent)) {\r\nptent = pte_file_clear_soft_dirty(ptent);\r\n}\r\nif (vma->vm_flags & VM_SOFTDIRTY)\r\nvma->vm_flags &= ~VM_SOFTDIRTY;\r\nset_pte_at(vma->vm_mm, addr, pte, ptent);\r\n#endif\r\n}\r\nstatic int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,\r\nunsigned long end, struct mm_walk *walk)\r\n{\r\nstruct clear_refs_private *cp = walk->private;\r\nstruct vm_area_struct *vma = cp->vma;\r\npte_t *pte, ptent;\r\nspinlock_t *ptl;\r\nstruct page *page;\r\nsplit_huge_page_pmd(vma, addr, pmd);\r\nif (pmd_trans_unstable(pmd))\r\nreturn 0;\r\npte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\r\nfor (; addr != end; pte++, addr += PAGE_SIZE) {\r\nptent = *pte;\r\nif (cp->type == CLEAR_REFS_SOFT_DIRTY) {\r\nclear_soft_dirty(vma, addr, pte);\r\ncontinue;\r\n}\r\nif (!pte_present(ptent))\r\ncontinue;\r\npage = vm_normal_page(vma, addr, ptent);\r\nif (!page)\r\ncontinue;\r\nptep_test_and_clear_young(vma, addr, pte);\r\nClearPageReferenced(page);\r\n}\r\npte_unmap_unlock(pte - 1, ptl);\r\ncond_resched();\r\nreturn 0;\r\n}\r\nstatic ssize_t clear_refs_write(struct file *file, const char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct task_struct *task;\r\nchar buffer[PROC_NUMBUF];\r\nstruct mm_struct *mm;\r\nstruct vm_area_struct *vma;\r\nenum clear_refs_types type;\r\nint itype;\r\nint rv;\r\nmemset(buffer, 0, sizeof(buffer));\r\nif (count > sizeof(buffer) - 1)\r\ncount = sizeof(buffer) - 1;\r\nif (copy_from_user(buffer, buf, count))\r\nreturn -EFAULT;\r\nrv = kstrtoint(strstrip(buffer), 10, &itype);\r\nif (rv < 0)\r\nreturn rv;\r\ntype = (enum clear_refs_types)itype;\r\nif (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)\r\nreturn -EINVAL;\r\nif (type == CLEAR_REFS_SOFT_DIRTY) {\r\nsoft_dirty_cleared = true;\r\npr_warn_once("The pagemap bits 55-60 has changed their meaning! "\r\n"See the linux/Documentation/vm/pagemap.txt for details.\n");\r\n}\r\ntask = get_proc_task(file_inode(file));\r\nif (!task)\r\nreturn -ESRCH;\r\nmm = get_task_mm(task);\r\nif (mm) {\r\nstruct clear_refs_private cp = {\r\n.type = type,\r\n};\r\nstruct mm_walk clear_refs_walk = {\r\n.pmd_entry = clear_refs_pte_range,\r\n.mm = mm,\r\n.private = &cp,\r\n};\r\ndown_read(&mm->mmap_sem);\r\nif (type == CLEAR_REFS_SOFT_DIRTY)\r\nmmu_notifier_invalidate_range_start(mm, 0, -1);\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next) {\r\ncp.vma = vma;\r\nif (is_vm_hugetlb_page(vma))\r\ncontinue;\r\nif (type == CLEAR_REFS_ANON && vma->vm_file)\r\ncontinue;\r\nif (type == CLEAR_REFS_MAPPED && !vma->vm_file)\r\ncontinue;\r\nwalk_page_range(vma->vm_start, vma->vm_end,\r\n&clear_refs_walk);\r\n}\r\nif (type == CLEAR_REFS_SOFT_DIRTY)\r\nmmu_notifier_invalidate_range_end(mm, 0, -1);\r\nflush_tlb_mm(mm);\r\nup_read(&mm->mmap_sem);\r\nmmput(mm);\r\n}\r\nput_task_struct(task);\r\nreturn count;\r\n}\r\nstatic inline pagemap_entry_t make_pme(u64 val)\r\n{\r\nreturn (pagemap_entry_t) { .pme = val };\r\n}\r\nstatic int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,\r\nstruct pagemapread *pm)\r\n{\r\npm->buffer[pm->pos++] = *pme;\r\nif (pm->pos >= pm->len)\r\nreturn PM_END_OF_BUFFER;\r\nreturn 0;\r\n}\r\nstatic int pagemap_pte_hole(unsigned long start, unsigned long end,\r\nstruct mm_walk *walk)\r\n{\r\nstruct pagemapread *pm = walk->private;\r\nunsigned long addr;\r\nint err = 0;\r\npagemap_entry_t pme = make_pme(PM_NOT_PRESENT(pm->v2));\r\nfor (addr = start; addr < end; addr += PAGE_SIZE) {\r\nerr = add_to_pagemap(addr, &pme, pm);\r\nif (err)\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic void pte_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,\r\nstruct vm_area_struct *vma, unsigned long addr, pte_t pte)\r\n{\r\nu64 frame, flags;\r\nstruct page *page = NULL;\r\nint flags2 = 0;\r\nif (pte_present(pte)) {\r\nframe = pte_pfn(pte);\r\nflags = PM_PRESENT;\r\npage = vm_normal_page(vma, addr, pte);\r\nif (pte_soft_dirty(pte))\r\nflags2 |= __PM_SOFT_DIRTY;\r\n} else if (is_swap_pte(pte)) {\r\nswp_entry_t entry;\r\nif (pte_swp_soft_dirty(pte))\r\nflags2 |= __PM_SOFT_DIRTY;\r\nentry = pte_to_swp_entry(pte);\r\nframe = swp_type(entry) |\r\n(swp_offset(entry) << MAX_SWAPFILES_SHIFT);\r\nflags = PM_SWAP;\r\nif (is_migration_entry(entry))\r\npage = migration_entry_to_page(entry);\r\n} else {\r\nif (vma->vm_flags & VM_SOFTDIRTY)\r\nflags2 |= __PM_SOFT_DIRTY;\r\n*pme = make_pme(PM_NOT_PRESENT(pm->v2) | PM_STATUS2(pm->v2, flags2));\r\nreturn;\r\n}\r\nif (page && !PageAnon(page))\r\nflags |= PM_FILE;\r\nif ((vma->vm_flags & VM_SOFTDIRTY))\r\nflags2 |= __PM_SOFT_DIRTY;\r\n*pme = make_pme(PM_PFRAME(frame) | PM_STATUS2(pm->v2, flags2) | flags);\r\n}\r\nstatic void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,\r\npmd_t pmd, int offset, int pmd_flags2)\r\n{\r\nif (pmd_present(pmd))\r\n*pme = make_pme(PM_PFRAME(pmd_pfn(pmd) + offset)\r\n| PM_STATUS2(pm->v2, pmd_flags2) | PM_PRESENT);\r\nelse\r\n*pme = make_pme(PM_NOT_PRESENT(pm->v2) | PM_STATUS2(pm->v2, pmd_flags2));\r\n}\r\nstatic inline void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,\r\npmd_t pmd, int offset, int pmd_flags2)\r\n{\r\n}\r\nstatic int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\r\nstruct mm_walk *walk)\r\n{\r\nstruct vm_area_struct *vma;\r\nstruct pagemapread *pm = walk->private;\r\nspinlock_t *ptl;\r\npte_t *pte;\r\nint err = 0;\r\npagemap_entry_t pme = make_pme(PM_NOT_PRESENT(pm->v2));\r\nvma = find_vma(walk->mm, addr);\r\nif (vma && pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {\r\nint pmd_flags2;\r\nif ((vma->vm_flags & VM_SOFTDIRTY) || pmd_soft_dirty(*pmd))\r\npmd_flags2 = __PM_SOFT_DIRTY;\r\nelse\r\npmd_flags2 = 0;\r\nfor (; addr != end; addr += PAGE_SIZE) {\r\nunsigned long offset;\r\noffset = (addr & ~PAGEMAP_WALK_MASK) >>\r\nPAGE_SHIFT;\r\nthp_pmd_to_pagemap_entry(&pme, pm, *pmd, offset, pmd_flags2);\r\nerr = add_to_pagemap(addr, &pme, pm);\r\nif (err)\r\nbreak;\r\n}\r\nspin_unlock(ptl);\r\nreturn err;\r\n}\r\nif (pmd_trans_unstable(pmd))\r\nreturn 0;\r\nfor (; addr != end; addr += PAGE_SIZE) {\r\nint flags2;\r\nif (vma && (addr >= vma->vm_end)) {\r\nvma = find_vma(walk->mm, addr);\r\nif (vma && (vma->vm_flags & VM_SOFTDIRTY))\r\nflags2 = __PM_SOFT_DIRTY;\r\nelse\r\nflags2 = 0;\r\npme = make_pme(PM_NOT_PRESENT(pm->v2) | PM_STATUS2(pm->v2, flags2));\r\n}\r\nif (vma && (vma->vm_start <= addr) &&\r\n!is_vm_hugetlb_page(vma)) {\r\npte = pte_offset_map(pmd, addr);\r\npte_to_pagemap_entry(&pme, pm, vma, addr, *pte);\r\npte_unmap(pte);\r\n}\r\nerr = add_to_pagemap(addr, &pme, pm);\r\nif (err)\r\nreturn err;\r\n}\r\ncond_resched();\r\nreturn err;\r\n}\r\nstatic void huge_pte_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,\r\npte_t pte, int offset, int flags2)\r\n{\r\nif (pte_present(pte))\r\n*pme = make_pme(PM_PFRAME(pte_pfn(pte) + offset) |\r\nPM_STATUS2(pm->v2, flags2) |\r\nPM_PRESENT);\r\nelse\r\n*pme = make_pme(PM_NOT_PRESENT(pm->v2) |\r\nPM_STATUS2(pm->v2, flags2));\r\n}\r\nstatic int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,\r\nunsigned long addr, unsigned long end,\r\nstruct mm_walk *walk)\r\n{\r\nstruct pagemapread *pm = walk->private;\r\nstruct vm_area_struct *vma;\r\nint err = 0;\r\nint flags2;\r\npagemap_entry_t pme;\r\nvma = find_vma(walk->mm, addr);\r\nWARN_ON_ONCE(!vma);\r\nif (vma && (vma->vm_flags & VM_SOFTDIRTY))\r\nflags2 = __PM_SOFT_DIRTY;\r\nelse\r\nflags2 = 0;\r\nfor (; addr != end; addr += PAGE_SIZE) {\r\nint offset = (addr & ~hmask) >> PAGE_SHIFT;\r\nhuge_pte_to_pagemap_entry(&pme, pm, *pte, offset, flags2);\r\nerr = add_to_pagemap(addr, &pme, pm);\r\nif (err)\r\nreturn err;\r\n}\r\ncond_resched();\r\nreturn err;\r\n}\r\nstatic ssize_t pagemap_read(struct file *file, char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct task_struct *task = get_proc_task(file_inode(file));\r\nstruct mm_struct *mm;\r\nstruct pagemapread pm;\r\nint ret = -ESRCH;\r\nstruct mm_walk pagemap_walk = {};\r\nunsigned long src;\r\nunsigned long svpfn;\r\nunsigned long start_vaddr;\r\nunsigned long end_vaddr;\r\nint copied = 0;\r\nif (!task)\r\ngoto out;\r\nret = -EINVAL;\r\nif ((*ppos % PM_ENTRY_BYTES) || (count % PM_ENTRY_BYTES))\r\ngoto out_task;\r\nret = 0;\r\nif (!count)\r\ngoto out_task;\r\npm.v2 = soft_dirty_cleared;\r\npm.len = (PAGEMAP_WALK_SIZE >> PAGE_SHIFT);\r\npm.buffer = kmalloc(pm.len * PM_ENTRY_BYTES, GFP_TEMPORARY);\r\nret = -ENOMEM;\r\nif (!pm.buffer)\r\ngoto out_task;\r\nmm = mm_access(task, PTRACE_MODE_READ);\r\nret = PTR_ERR(mm);\r\nif (!mm || IS_ERR(mm))\r\ngoto out_free;\r\npagemap_walk.pmd_entry = pagemap_pte_range;\r\npagemap_walk.pte_hole = pagemap_pte_hole;\r\n#ifdef CONFIG_HUGETLB_PAGE\r\npagemap_walk.hugetlb_entry = pagemap_hugetlb_range;\r\n#endif\r\npagemap_walk.mm = mm;\r\npagemap_walk.private = &pm;\r\nsrc = *ppos;\r\nsvpfn = src / PM_ENTRY_BYTES;\r\nstart_vaddr = svpfn << PAGE_SHIFT;\r\nend_vaddr = TASK_SIZE_OF(task);\r\nif (svpfn > TASK_SIZE_OF(task) >> PAGE_SHIFT)\r\nstart_vaddr = end_vaddr;\r\nret = 0;\r\nwhile (count && (start_vaddr < end_vaddr)) {\r\nint len;\r\nunsigned long end;\r\npm.pos = 0;\r\nend = (start_vaddr + PAGEMAP_WALK_SIZE) & PAGEMAP_WALK_MASK;\r\nif (end < start_vaddr || end > end_vaddr)\r\nend = end_vaddr;\r\ndown_read(&mm->mmap_sem);\r\nret = walk_page_range(start_vaddr, end, &pagemap_walk);\r\nup_read(&mm->mmap_sem);\r\nstart_vaddr = end;\r\nlen = min(count, PM_ENTRY_BYTES * pm.pos);\r\nif (copy_to_user(buf, pm.buffer, len)) {\r\nret = -EFAULT;\r\ngoto out_mm;\r\n}\r\ncopied += len;\r\nbuf += len;\r\ncount -= len;\r\n}\r\n*ppos += copied;\r\nif (!ret || ret == PM_END_OF_BUFFER)\r\nret = copied;\r\nout_mm:\r\nmmput(mm);\r\nout_free:\r\nkfree(pm.buffer);\r\nout_task:\r\nput_task_struct(task);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int pagemap_open(struct inode *inode, struct file *file)\r\n{\r\npr_warn_once("Bits 55-60 of /proc/PID/pagemap entries are about "\r\n"to stop being page-shift some time soon. See the "\r\n"linux/Documentation/vm/pagemap.txt for details.\n");\r\nreturn 0;\r\n}\r\nstatic void gather_stats(struct page *page, struct numa_maps *md, int pte_dirty,\r\nunsigned long nr_pages)\r\n{\r\nint count = page_mapcount(page);\r\nmd->pages += nr_pages;\r\nif (pte_dirty || PageDirty(page))\r\nmd->dirty += nr_pages;\r\nif (PageSwapCache(page))\r\nmd->swapcache += nr_pages;\r\nif (PageActive(page) || PageUnevictable(page))\r\nmd->active += nr_pages;\r\nif (PageWriteback(page))\r\nmd->writeback += nr_pages;\r\nif (PageAnon(page))\r\nmd->anon += nr_pages;\r\nif (count > md->mapcount_max)\r\nmd->mapcount_max = count;\r\nmd->node[page_to_nid(page)] += nr_pages;\r\n}\r\nstatic struct page *can_gather_numa_stats(pte_t pte, struct vm_area_struct *vma,\r\nunsigned long addr)\r\n{\r\nstruct page *page;\r\nint nid;\r\nif (!pte_present(pte))\r\nreturn NULL;\r\npage = vm_normal_page(vma, addr, pte);\r\nif (!page)\r\nreturn NULL;\r\nif (PageReserved(page))\r\nreturn NULL;\r\nnid = page_to_nid(page);\r\nif (!node_isset(nid, node_states[N_MEMORY]))\r\nreturn NULL;\r\nreturn page;\r\n}\r\nstatic int gather_pte_stats(pmd_t *pmd, unsigned long addr,\r\nunsigned long end, struct mm_walk *walk)\r\n{\r\nstruct numa_maps *md;\r\nspinlock_t *ptl;\r\npte_t *orig_pte;\r\npte_t *pte;\r\nmd = walk->private;\r\nif (pmd_trans_huge_lock(pmd, md->vma, &ptl) == 1) {\r\npte_t huge_pte = *(pte_t *)pmd;\r\nstruct page *page;\r\npage = can_gather_numa_stats(huge_pte, md->vma, addr);\r\nif (page)\r\ngather_stats(page, md, pte_dirty(huge_pte),\r\nHPAGE_PMD_SIZE/PAGE_SIZE);\r\nspin_unlock(ptl);\r\nreturn 0;\r\n}\r\nif (pmd_trans_unstable(pmd))\r\nreturn 0;\r\norig_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\r\ndo {\r\nstruct page *page = can_gather_numa_stats(*pte, md->vma, addr);\r\nif (!page)\r\ncontinue;\r\ngather_stats(page, md, pte_dirty(*pte), 1);\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\npte_unmap_unlock(orig_pte, ptl);\r\nreturn 0;\r\n}\r\nstatic int gather_hugetbl_stats(pte_t *pte, unsigned long hmask,\r\nunsigned long addr, unsigned long end, struct mm_walk *walk)\r\n{\r\nstruct numa_maps *md;\r\nstruct page *page;\r\nif (pte_none(*pte))\r\nreturn 0;\r\npage = pte_page(*pte);\r\nif (!page)\r\nreturn 0;\r\nmd = walk->private;\r\ngather_stats(page, md, pte_dirty(*pte), 1);\r\nreturn 0;\r\n}\r\nstatic int gather_hugetbl_stats(pte_t *pte, unsigned long hmask,\r\nunsigned long addr, unsigned long end, struct mm_walk *walk)\r\n{\r\nreturn 0;\r\n}\r\nstatic int show_numa_map(struct seq_file *m, void *v, int is_pid)\r\n{\r\nstruct numa_maps_private *numa_priv = m->private;\r\nstruct proc_maps_private *proc_priv = &numa_priv->proc_maps;\r\nstruct vm_area_struct *vma = v;\r\nstruct numa_maps *md = &numa_priv->md;\r\nstruct file *file = vma->vm_file;\r\nstruct task_struct *task = proc_priv->task;\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct mm_walk walk = {};\r\nstruct mempolicy *pol;\r\nchar buffer[64];\r\nint nid;\r\nif (!mm)\r\nreturn 0;\r\nmemset(md, 0, sizeof(*md));\r\nmd->vma = vma;\r\nwalk.hugetlb_entry = gather_hugetbl_stats;\r\nwalk.pmd_entry = gather_pte_stats;\r\nwalk.private = md;\r\nwalk.mm = mm;\r\npol = get_vma_policy(task, vma, vma->vm_start);\r\nmpol_to_str(buffer, sizeof(buffer), pol);\r\nmpol_cond_put(pol);\r\nseq_printf(m, "%08lx %s", vma->vm_start, buffer);\r\nif (file) {\r\nseq_printf(m, " file=");\r\nseq_path(m, &file->f_path, "\n\t= ");\r\n} else if (vma->vm_start <= mm->brk && vma->vm_end >= mm->start_brk) {\r\nseq_printf(m, " heap");\r\n} else {\r\npid_t tid = vm_is_stack(task, vma, is_pid);\r\nif (tid != 0) {\r\nif (!is_pid || (vma->vm_start <= mm->start_stack &&\r\nvma->vm_end >= mm->start_stack))\r\nseq_printf(m, " stack");\r\nelse\r\nseq_printf(m, " stack:%d", tid);\r\n}\r\n}\r\nif (is_vm_hugetlb_page(vma))\r\nseq_printf(m, " huge");\r\nwalk_page_range(vma->vm_start, vma->vm_end, &walk);\r\nif (!md->pages)\r\ngoto out;\r\nif (md->anon)\r\nseq_printf(m, " anon=%lu", md->anon);\r\nif (md->dirty)\r\nseq_printf(m, " dirty=%lu", md->dirty);\r\nif (md->pages != md->anon && md->pages != md->dirty)\r\nseq_printf(m, " mapped=%lu", md->pages);\r\nif (md->mapcount_max > 1)\r\nseq_printf(m, " mapmax=%lu", md->mapcount_max);\r\nif (md->swapcache)\r\nseq_printf(m, " swapcache=%lu", md->swapcache);\r\nif (md->active < md->pages && !is_vm_hugetlb_page(vma))\r\nseq_printf(m, " active=%lu", md->active);\r\nif (md->writeback)\r\nseq_printf(m, " writeback=%lu", md->writeback);\r\nfor_each_node_state(nid, N_MEMORY)\r\nif (md->node[nid])\r\nseq_printf(m, " N%d=%lu", nid, md->node[nid]);\r\nout:\r\nseq_putc(m, '\n');\r\nif (m->count < m->size)\r\nm->version = (vma != proc_priv->tail_vma) ? vma->vm_start : 0;\r\nreturn 0;\r\n}\r\nstatic int show_pid_numa_map(struct seq_file *m, void *v)\r\n{\r\nreturn show_numa_map(m, v, 1);\r\n}\r\nstatic int show_tid_numa_map(struct seq_file *m, void *v)\r\n{\r\nreturn show_numa_map(m, v, 0);\r\n}\r\nstatic int numa_maps_open(struct inode *inode, struct file *file,\r\nconst struct seq_operations *ops)\r\n{\r\nstruct numa_maps_private *priv;\r\nint ret = -ENOMEM;\r\npriv = kzalloc(sizeof(*priv), GFP_KERNEL);\r\nif (priv) {\r\npriv->proc_maps.pid = proc_pid(inode);\r\nret = seq_open(file, ops);\r\nif (!ret) {\r\nstruct seq_file *m = file->private_data;\r\nm->private = priv;\r\n} else {\r\nkfree(priv);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int pid_numa_maps_open(struct inode *inode, struct file *file)\r\n{\r\nreturn numa_maps_open(inode, file, &proc_pid_numa_maps_op);\r\n}\r\nstatic int tid_numa_maps_open(struct inode *inode, struct file *file)\r\n{\r\nreturn numa_maps_open(inode, file, &proc_tid_numa_maps_op);\r\n}
