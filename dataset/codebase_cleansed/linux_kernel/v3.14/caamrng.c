static inline void rng_unmap_buf(struct device *jrdev, struct buf_data *bd)\r\n{\r\nif (bd->addr)\r\ndma_unmap_single(jrdev, bd->addr, RN_BUF_SIZE,\r\nDMA_FROM_DEVICE);\r\n}\r\nstatic inline void rng_unmap_ctx(struct caam_rng_ctx *ctx)\r\n{\r\nstruct device *jrdev = ctx->jrdev;\r\nif (ctx->sh_desc_dma)\r\ndma_unmap_single(jrdev, ctx->sh_desc_dma, DESC_RNG_LEN,\r\nDMA_TO_DEVICE);\r\nrng_unmap_buf(jrdev, &ctx->bufs[0]);\r\nrng_unmap_buf(jrdev, &ctx->bufs[1]);\r\n}\r\nstatic void rng_done(struct device *jrdev, u32 *desc, u32 err, void *context)\r\n{\r\nstruct buf_data *bd;\r\nbd = (struct buf_data *)((char *)desc -\r\noffsetof(struct buf_data, hw_desc));\r\nif (err) {\r\nchar tmp[CAAM_ERROR_STR_MAX];\r\ndev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));\r\n}\r\natomic_set(&bd->empty, BUF_NOT_EMPTY);\r\ncomplete(&bd->filled);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "rng refreshed buf@: ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, bd->buf, RN_BUF_SIZE, 1);\r\n#endif\r\n}\r\nstatic inline int submit_job(struct caam_rng_ctx *ctx, int to_current)\r\n{\r\nstruct buf_data *bd = &ctx->bufs[!(to_current ^ ctx->current_buf)];\r\nstruct device *jrdev = ctx->jrdev;\r\nu32 *desc = bd->hw_desc;\r\nint err;\r\ndev_dbg(jrdev, "submitting job %d\n", !(to_current ^ ctx->current_buf));\r\ninit_completion(&bd->filled);\r\nerr = caam_jr_enqueue(jrdev, desc, rng_done, ctx);\r\nif (err)\r\ncomplete(&bd->filled);\r\nelse\r\natomic_inc(&bd->empty);\r\nreturn err;\r\n}\r\nstatic int caam_read(struct hwrng *rng, void *data, size_t max, bool wait)\r\n{\r\nstruct caam_rng_ctx *ctx = &rng_ctx;\r\nstruct buf_data *bd = &ctx->bufs[ctx->current_buf];\r\nint next_buf_idx, copied_idx;\r\nint err;\r\nif (atomic_read(&bd->empty)) {\r\nif (atomic_read(&bd->empty) == BUF_EMPTY) {\r\nerr = submit_job(ctx, 1);\r\nif (err)\r\nreturn 0;\r\n}\r\nif (!wait)\r\nreturn 0;\r\nif (atomic_read(&bd->empty))\r\nwait_for_completion(&bd->filled);\r\n}\r\nnext_buf_idx = ctx->cur_buf_idx + max;\r\ndev_dbg(ctx->jrdev, "%s: start reading at buffer %d, idx %d\n",\r\n__func__, ctx->current_buf, ctx->cur_buf_idx);\r\nif (next_buf_idx < RN_BUF_SIZE) {\r\nmemcpy(data, bd->buf + ctx->cur_buf_idx, max);\r\nctx->cur_buf_idx = next_buf_idx;\r\nreturn max;\r\n}\r\ncopied_idx = RN_BUF_SIZE - ctx->cur_buf_idx;\r\nmemcpy(data, bd->buf + ctx->cur_buf_idx, copied_idx);\r\nctx->cur_buf_idx = 0;\r\natomic_set(&bd->empty, BUF_EMPTY);\r\nsubmit_job(ctx, 1);\r\nctx->current_buf = !ctx->current_buf;\r\ndev_dbg(ctx->jrdev, "switched to buffer %d\n", ctx->current_buf);\r\nreturn copied_idx + caam_read(rng, data + copied_idx,\r\nmax - copied_idx, false);\r\n}\r\nstatic inline void rng_create_sh_desc(struct caam_rng_ctx *ctx)\r\n{\r\nstruct device *jrdev = ctx->jrdev;\r\nu32 *desc = ctx->sh_desc;\r\ninit_sh_desc(desc, HDR_SHARE_SERIAL);\r\nappend_cmd(desc, SET_OK_NO_PROP_ERRORS | CMD_LOAD);\r\nappend_operation(desc, OP_ALG_ALGSEL_RNG | OP_TYPE_CLASS1_ALG);\r\nappend_seq_fifo_store(desc, RN_BUF_SIZE, FIFOST_TYPE_RNGSTORE);\r\nctx->sh_desc_dma = dma_map_single(jrdev, desc, desc_bytes(desc),\r\nDMA_TO_DEVICE);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "rng shdesc@: ", DUMP_PREFIX_ADDRESS, 16, 4,\r\ndesc, desc_bytes(desc), 1);\r\n#endif\r\n}\r\nstatic inline void rng_create_job_desc(struct caam_rng_ctx *ctx, int buf_id)\r\n{\r\nstruct device *jrdev = ctx->jrdev;\r\nstruct buf_data *bd = &ctx->bufs[buf_id];\r\nu32 *desc = bd->hw_desc;\r\nint sh_len = desc_len(ctx->sh_desc);\r\ninit_job_desc_shared(desc, ctx->sh_desc_dma, sh_len, HDR_SHARE_DEFER |\r\nHDR_REVERSE);\r\nbd->addr = dma_map_single(jrdev, bd->buf, RN_BUF_SIZE, DMA_FROM_DEVICE);\r\nappend_seq_out_ptr_intlen(desc, bd->addr, RN_BUF_SIZE, 0);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "rng job desc@: ", DUMP_PREFIX_ADDRESS, 16, 4,\r\ndesc, desc_bytes(desc), 1);\r\n#endif\r\n}\r\nstatic void caam_cleanup(struct hwrng *rng)\r\n{\r\nint i;\r\nstruct buf_data *bd;\r\nfor (i = 0; i < 2; i++) {\r\nbd = &rng_ctx.bufs[i];\r\nif (atomic_read(&bd->empty) == BUF_PENDING)\r\nwait_for_completion(&bd->filled);\r\n}\r\nrng_unmap_ctx(&rng_ctx);\r\n}\r\nstatic void caam_init_buf(struct caam_rng_ctx *ctx, int buf_id)\r\n{\r\nstruct buf_data *bd = &ctx->bufs[buf_id];\r\nrng_create_job_desc(ctx, buf_id);\r\natomic_set(&bd->empty, BUF_EMPTY);\r\nsubmit_job(ctx, buf_id == ctx->current_buf);\r\nwait_for_completion(&bd->filled);\r\n}\r\nstatic void caam_init_rng(struct caam_rng_ctx *ctx, struct device *jrdev)\r\n{\r\nctx->jrdev = jrdev;\r\nrng_create_sh_desc(ctx);\r\nctx->current_buf = 0;\r\nctx->cur_buf_idx = 0;\r\ncaam_init_buf(ctx, 0);\r\ncaam_init_buf(ctx, 1);\r\n}\r\nstatic void __exit caam_rng_exit(void)\r\n{\r\ncaam_jr_free(rng_ctx.jrdev);\r\nhwrng_unregister(&caam_rng);\r\n}\r\nstatic int __init caam_rng_init(void)\r\n{\r\nstruct device *dev;\r\ndev = caam_jr_alloc();\r\nif (IS_ERR(dev)) {\r\npr_err("Job Ring Device allocation for transform failed\n");\r\nreturn PTR_ERR(dev);\r\n}\r\ncaam_init_rng(&rng_ctx, dev);\r\ndev_info(dev, "registering rng-caam\n");\r\nreturn hwrng_register(&caam_rng);\r\n}
