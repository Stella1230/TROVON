static void tcp_write_err(struct sock *sk)\r\n{\r\nsk->sk_err = sk->sk_err_soft ? : ETIMEDOUT;\r\nsk->sk_error_report(sk);\r\ntcp_done(sk);\r\nNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONTIMEOUT);\r\n}\r\nstatic int tcp_out_of_resources(struct sock *sk, int do_reset)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nint shift = 0;\r\nif ((s32)(tcp_time_stamp - tp->lsndtime) > 2*TCP_RTO_MAX || !do_reset)\r\nshift++;\r\nif (sk->sk_err_soft)\r\nshift++;\r\nif (tcp_check_oom(sk, shift)) {\r\nif ((s32)(tcp_time_stamp - tp->lsndtime) <= TCP_TIMEWAIT_LEN ||\r\n(!tp->snd_wnd && !tp->packets_out))\r\ndo_reset = 1;\r\nif (do_reset)\r\ntcp_send_active_reset(sk, GFP_ATOMIC);\r\ntcp_done(sk);\r\nNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONMEMORY);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int tcp_orphan_retries(struct sock *sk, int alive)\r\n{\r\nint retries = sysctl_tcp_orphan_retries;\r\nif (sk->sk_err_soft && !alive)\r\nretries = 0;\r\nif (retries == 0 && alive)\r\nretries = 8;\r\nreturn retries;\r\n}\r\nstatic void tcp_mtu_probing(struct inet_connection_sock *icsk, struct sock *sk)\r\n{\r\nif (sysctl_tcp_mtu_probing) {\r\nif (!icsk->icsk_mtup.enabled) {\r\nicsk->icsk_mtup.enabled = 1;\r\ntcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\r\n} else {\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nint mss;\r\nmss = tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low) >> 1;\r\nmss = min(sysctl_tcp_base_mss, mss);\r\nmss = max(mss, 68 - tp->tcp_header_len);\r\nicsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\r\ntcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\r\n}\r\n}\r\n}\r\nstatic bool retransmits_timed_out(struct sock *sk,\r\nunsigned int boundary,\r\nunsigned int timeout,\r\nbool syn_set)\r\n{\r\nunsigned int linear_backoff_thresh, start_ts;\r\nunsigned int rto_base = syn_set ? TCP_TIMEOUT_INIT : TCP_RTO_MIN;\r\nif (!inet_csk(sk)->icsk_retransmits)\r\nreturn false;\r\nif (unlikely(!tcp_sk(sk)->retrans_stamp))\r\nstart_ts = TCP_SKB_CB(tcp_write_queue_head(sk))->when;\r\nelse\r\nstart_ts = tcp_sk(sk)->retrans_stamp;\r\nif (likely(timeout == 0)) {\r\nlinear_backoff_thresh = ilog2(TCP_RTO_MAX/rto_base);\r\nif (boundary <= linear_backoff_thresh)\r\ntimeout = ((2 << boundary) - 1) * rto_base;\r\nelse\r\ntimeout = ((2 << linear_backoff_thresh) - 1) * rto_base +\r\n(boundary - linear_backoff_thresh) * TCP_RTO_MAX;\r\n}\r\nreturn (tcp_time_stamp - start_ts) >= timeout;\r\n}\r\nstatic int tcp_write_timeout(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nint retry_until;\r\nbool do_reset, syn_set = false;\r\nif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\r\nif (icsk->icsk_retransmits) {\r\ndst_negative_advice(sk);\r\nif (tp->syn_fastopen || tp->syn_data)\r\ntcp_fastopen_cache_set(sk, 0, NULL, true);\r\n}\r\nretry_until = icsk->icsk_syn_retries ? : sysctl_tcp_syn_retries;\r\nsyn_set = true;\r\n} else {\r\nif (retransmits_timed_out(sk, sysctl_tcp_retries1, 0, 0)) {\r\ntcp_mtu_probing(icsk, sk);\r\ndst_negative_advice(sk);\r\n}\r\nretry_until = sysctl_tcp_retries2;\r\nif (sock_flag(sk, SOCK_DEAD)) {\r\nconst int alive = (icsk->icsk_rto < TCP_RTO_MAX);\r\nretry_until = tcp_orphan_retries(sk, alive);\r\ndo_reset = alive ||\r\n!retransmits_timed_out(sk, retry_until, 0, 0);\r\nif (tcp_out_of_resources(sk, do_reset))\r\nreturn 1;\r\n}\r\n}\r\nif (retransmits_timed_out(sk, retry_until,\r\nsyn_set ? 0 : icsk->icsk_user_timeout, syn_set)) {\r\ntcp_write_err(sk);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid tcp_delack_timer_handler(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nsk_mem_reclaim_partial(sk);\r\nif (sk->sk_state == TCP_CLOSE || !(icsk->icsk_ack.pending & ICSK_ACK_TIMER))\r\ngoto out;\r\nif (time_after(icsk->icsk_ack.timeout, jiffies)) {\r\nsk_reset_timer(sk, &icsk->icsk_delack_timer, icsk->icsk_ack.timeout);\r\ngoto out;\r\n}\r\nicsk->icsk_ack.pending &= ~ICSK_ACK_TIMER;\r\nif (!skb_queue_empty(&tp->ucopy.prequeue)) {\r\nstruct sk_buff *skb;\r\nNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPSCHEDULERFAILED);\r\nwhile ((skb = __skb_dequeue(&tp->ucopy.prequeue)) != NULL)\r\nsk_backlog_rcv(sk, skb);\r\ntp->ucopy.memory = 0;\r\n}\r\nif (inet_csk_ack_scheduled(sk)) {\r\nif (!icsk->icsk_ack.pingpong) {\r\nicsk->icsk_ack.ato = min(icsk->icsk_ack.ato << 1, icsk->icsk_rto);\r\n} else {\r\nicsk->icsk_ack.pingpong = 0;\r\nicsk->icsk_ack.ato = TCP_ATO_MIN;\r\n}\r\ntcp_send_ack(sk);\r\nNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_DELAYEDACKS);\r\n}\r\nout:\r\nif (sk_under_memory_pressure(sk))\r\nsk_mem_reclaim(sk);\r\n}\r\nstatic void tcp_delack_timer(unsigned long data)\r\n{\r\nstruct sock *sk = (struct sock *)data;\r\nbh_lock_sock(sk);\r\nif (!sock_owned_by_user(sk)) {\r\ntcp_delack_timer_handler(sk);\r\n} else {\r\ninet_csk(sk)->icsk_ack.blocked = 1;\r\nNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_DELAYEDACKLOCKED);\r\nif (!test_and_set_bit(TCP_DELACK_TIMER_DEFERRED, &tcp_sk(sk)->tsq_flags))\r\nsock_hold(sk);\r\n}\r\nbh_unlock_sock(sk);\r\nsock_put(sk);\r\n}\r\nstatic void tcp_probe_timer(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nint max_probes;\r\nif (tp->packets_out || !tcp_send_head(sk)) {\r\nicsk->icsk_probes_out = 0;\r\nreturn;\r\n}\r\nmax_probes = sysctl_tcp_retries2;\r\nif (sock_flag(sk, SOCK_DEAD)) {\r\nconst int alive = ((icsk->icsk_rto << icsk->icsk_backoff) < TCP_RTO_MAX);\r\nmax_probes = tcp_orphan_retries(sk, alive);\r\nif (tcp_out_of_resources(sk, alive || icsk->icsk_probes_out <= max_probes))\r\nreturn;\r\n}\r\nif (icsk->icsk_probes_out > max_probes) {\r\ntcp_write_err(sk);\r\n} else {\r\ntcp_send_probe0(sk);\r\n}\r\n}\r\nstatic void tcp_fastopen_synack_timer(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nint max_retries = icsk->icsk_syn_retries ? :\r\nsysctl_tcp_synack_retries + 1;\r\nstruct request_sock *req;\r\nreq = tcp_sk(sk)->fastopen_rsk;\r\nreq->rsk_ops->syn_ack_timeout(sk, req);\r\nif (req->num_timeout >= max_retries) {\r\ntcp_write_err(sk);\r\nreturn;\r\n}\r\ninet_rtx_syn_ack(sk, req);\r\nreq->num_timeout++;\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\r\nTCP_TIMEOUT_INIT << req->num_timeout, TCP_RTO_MAX);\r\n}\r\nvoid tcp_retransmit_timer(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nif (tp->fastopen_rsk) {\r\nWARN_ON_ONCE(sk->sk_state != TCP_SYN_RECV &&\r\nsk->sk_state != TCP_FIN_WAIT1);\r\ntcp_fastopen_synack_timer(sk);\r\nreturn;\r\n}\r\nif (!tp->packets_out)\r\ngoto out;\r\nWARN_ON(tcp_write_queue_empty(sk));\r\ntp->tlp_high_seq = 0;\r\nif (!tp->snd_wnd && !sock_flag(sk, SOCK_DEAD) &&\r\n!((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))) {\r\nstruct inet_sock *inet = inet_sk(sk);\r\nif (sk->sk_family == AF_INET) {\r\nLIMIT_NETDEBUG(KERN_DEBUG pr_fmt("Peer %pI4:%u/%u unexpectedly shrunk window %u:%u (repaired)\n"),\r\n&inet->inet_daddr,\r\nntohs(inet->inet_dport), inet->inet_num,\r\ntp->snd_una, tp->snd_nxt);\r\n}\r\n#if IS_ENABLED(CONFIG_IPV6)\r\nelse if (sk->sk_family == AF_INET6) {\r\nLIMIT_NETDEBUG(KERN_DEBUG pr_fmt("Peer %pI6:%u/%u unexpectedly shrunk window %u:%u (repaired)\n"),\r\n&sk->sk_v6_daddr,\r\nntohs(inet->inet_dport), inet->inet_num,\r\ntp->snd_una, tp->snd_nxt);\r\n}\r\n#endif\r\nif (tcp_time_stamp - tp->rcv_tstamp > TCP_RTO_MAX) {\r\ntcp_write_err(sk);\r\ngoto out;\r\n}\r\ntcp_enter_loss(sk, 0);\r\ntcp_retransmit_skb(sk, tcp_write_queue_head(sk));\r\n__sk_dst_reset(sk);\r\ngoto out_reset_timer;\r\n}\r\nif (tcp_write_timeout(sk))\r\ngoto out;\r\nif (icsk->icsk_retransmits == 0) {\r\nint mib_idx;\r\nif (icsk->icsk_ca_state == TCP_CA_Recovery) {\r\nif (tcp_is_sack(tp))\r\nmib_idx = LINUX_MIB_TCPSACKRECOVERYFAIL;\r\nelse\r\nmib_idx = LINUX_MIB_TCPRENORECOVERYFAIL;\r\n} else if (icsk->icsk_ca_state == TCP_CA_Loss) {\r\nmib_idx = LINUX_MIB_TCPLOSSFAILURES;\r\n} else if ((icsk->icsk_ca_state == TCP_CA_Disorder) ||\r\ntp->sacked_out) {\r\nif (tcp_is_sack(tp))\r\nmib_idx = LINUX_MIB_TCPSACKFAILURES;\r\nelse\r\nmib_idx = LINUX_MIB_TCPRENOFAILURES;\r\n} else {\r\nmib_idx = LINUX_MIB_TCPTIMEOUTS;\r\n}\r\nNET_INC_STATS_BH(sock_net(sk), mib_idx);\r\n}\r\ntcp_enter_loss(sk, 0);\r\nif (tcp_retransmit_skb(sk, tcp_write_queue_head(sk)) > 0) {\r\nif (!icsk->icsk_retransmits)\r\nicsk->icsk_retransmits = 1;\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\r\nmin(icsk->icsk_rto, TCP_RESOURCE_PROBE_INTERVAL),\r\nTCP_RTO_MAX);\r\ngoto out;\r\n}\r\nicsk->icsk_backoff++;\r\nicsk->icsk_retransmits++;\r\nout_reset_timer:\r\nif (sk->sk_state == TCP_ESTABLISHED &&\r\n(tp->thin_lto || sysctl_tcp_thin_linear_timeouts) &&\r\ntcp_stream_is_thin(tp) &&\r\nicsk->icsk_retransmits <= TCP_THIN_LINEAR_RETRIES) {\r\nicsk->icsk_backoff = 0;\r\nicsk->icsk_rto = min(__tcp_set_rto(tp), TCP_RTO_MAX);\r\n} else {\r\nicsk->icsk_rto = min(icsk->icsk_rto << 1, TCP_RTO_MAX);\r\n}\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS, icsk->icsk_rto, TCP_RTO_MAX);\r\nif (retransmits_timed_out(sk, sysctl_tcp_retries1 + 1, 0, 0))\r\n__sk_dst_reset(sk);\r\nout:;\r\n}\r\nvoid tcp_write_timer_handler(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nint event;\r\nif (sk->sk_state == TCP_CLOSE || !icsk->icsk_pending)\r\ngoto out;\r\nif (time_after(icsk->icsk_timeout, jiffies)) {\r\nsk_reset_timer(sk, &icsk->icsk_retransmit_timer, icsk->icsk_timeout);\r\ngoto out;\r\n}\r\nevent = icsk->icsk_pending;\r\nswitch (event) {\r\ncase ICSK_TIME_EARLY_RETRANS:\r\ntcp_resume_early_retransmit(sk);\r\nbreak;\r\ncase ICSK_TIME_LOSS_PROBE:\r\ntcp_send_loss_probe(sk);\r\nbreak;\r\ncase ICSK_TIME_RETRANS:\r\nicsk->icsk_pending = 0;\r\ntcp_retransmit_timer(sk);\r\nbreak;\r\ncase ICSK_TIME_PROBE0:\r\nicsk->icsk_pending = 0;\r\ntcp_probe_timer(sk);\r\nbreak;\r\n}\r\nout:\r\nsk_mem_reclaim(sk);\r\n}\r\nstatic void tcp_write_timer(unsigned long data)\r\n{\r\nstruct sock *sk = (struct sock *)data;\r\nbh_lock_sock(sk);\r\nif (!sock_owned_by_user(sk)) {\r\ntcp_write_timer_handler(sk);\r\n} else {\r\nif (!test_and_set_bit(TCP_WRITE_TIMER_DEFERRED, &tcp_sk(sk)->tsq_flags))\r\nsock_hold(sk);\r\n}\r\nbh_unlock_sock(sk);\r\nsock_put(sk);\r\n}\r\nstatic void tcp_synack_timer(struct sock *sk)\r\n{\r\ninet_csk_reqsk_queue_prune(sk, TCP_SYNQ_INTERVAL,\r\nTCP_TIMEOUT_INIT, TCP_RTO_MAX);\r\n}\r\nvoid tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req)\r\n{\r\nNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPTIMEOUTS);\r\n}\r\nvoid tcp_set_keepalive(struct sock *sk, int val)\r\n{\r\nif ((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))\r\nreturn;\r\nif (val && !sock_flag(sk, SOCK_KEEPOPEN))\r\ninet_csk_reset_keepalive_timer(sk, keepalive_time_when(tcp_sk(sk)));\r\nelse if (!val)\r\ninet_csk_delete_keepalive_timer(sk);\r\n}\r\nstatic void tcp_keepalive_timer (unsigned long data)\r\n{\r\nstruct sock *sk = (struct sock *) data;\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nu32 elapsed;\r\nbh_lock_sock(sk);\r\nif (sock_owned_by_user(sk)) {\r\ninet_csk_reset_keepalive_timer (sk, HZ/20);\r\ngoto out;\r\n}\r\nif (sk->sk_state == TCP_LISTEN) {\r\ntcp_synack_timer(sk);\r\ngoto out;\r\n}\r\nif (sk->sk_state == TCP_FIN_WAIT2 && sock_flag(sk, SOCK_DEAD)) {\r\nif (tp->linger2 >= 0) {\r\nconst int tmo = tcp_fin_time(sk) - TCP_TIMEWAIT_LEN;\r\nif (tmo > 0) {\r\ntcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\r\ngoto out;\r\n}\r\n}\r\ntcp_send_active_reset(sk, GFP_ATOMIC);\r\ngoto death;\r\n}\r\nif (!sock_flag(sk, SOCK_KEEPOPEN) || sk->sk_state == TCP_CLOSE)\r\ngoto out;\r\nelapsed = keepalive_time_when(tp);\r\nif (tp->packets_out || tcp_send_head(sk))\r\ngoto resched;\r\nelapsed = keepalive_time_elapsed(tp);\r\nif (elapsed >= keepalive_time_when(tp)) {\r\nif ((icsk->icsk_user_timeout != 0 &&\r\nelapsed >= icsk->icsk_user_timeout &&\r\nicsk->icsk_probes_out > 0) ||\r\n(icsk->icsk_user_timeout == 0 &&\r\nicsk->icsk_probes_out >= keepalive_probes(tp))) {\r\ntcp_send_active_reset(sk, GFP_ATOMIC);\r\ntcp_write_err(sk);\r\ngoto out;\r\n}\r\nif (tcp_write_wakeup(sk) <= 0) {\r\nicsk->icsk_probes_out++;\r\nelapsed = keepalive_intvl_when(tp);\r\n} else {\r\nelapsed = TCP_RESOURCE_PROBE_INTERVAL;\r\n}\r\n} else {\r\nelapsed = keepalive_time_when(tp) - elapsed;\r\n}\r\nsk_mem_reclaim(sk);\r\nresched:\r\ninet_csk_reset_keepalive_timer (sk, elapsed);\r\ngoto out;\r\ndeath:\r\ntcp_done(sk);\r\nout:\r\nbh_unlock_sock(sk);\r\nsock_put(sk);\r\n}\r\nvoid tcp_init_xmit_timers(struct sock *sk)\r\n{\r\ninet_csk_init_xmit_timers(sk, &tcp_write_timer, &tcp_delack_timer,\r\n&tcp_keepalive_timer);\r\n}
