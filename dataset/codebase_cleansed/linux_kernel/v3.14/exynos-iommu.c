static unsigned long *section_entry(unsigned long *pgtable, unsigned long iova)\r\n{\r\nreturn pgtable + lv1ent_offset(iova);\r\n}\r\nstatic unsigned long *page_entry(unsigned long *sent, unsigned long iova)\r\n{\r\nreturn (unsigned long *)__va(lv2table_base(sent)) + lv2ent_offset(iova);\r\n}\r\nstatic bool set_sysmmu_active(struct sysmmu_drvdata *data)\r\n{\r\nreturn ++data->activations == 1;\r\n}\r\nstatic bool set_sysmmu_inactive(struct sysmmu_drvdata *data)\r\n{\r\nBUG_ON(data->activations < 1);\r\nreturn --data->activations == 0;\r\n}\r\nstatic bool is_sysmmu_active(struct sysmmu_drvdata *data)\r\n{\r\nreturn data->activations > 0;\r\n}\r\nstatic void sysmmu_unblock(void __iomem *sfrbase)\r\n{\r\n__raw_writel(CTRL_ENABLE, sfrbase + REG_MMU_CTRL);\r\n}\r\nstatic bool sysmmu_block(void __iomem *sfrbase)\r\n{\r\nint i = 120;\r\n__raw_writel(CTRL_BLOCK, sfrbase + REG_MMU_CTRL);\r\nwhile ((i > 0) && !(__raw_readl(sfrbase + REG_MMU_STATUS) & 1))\r\n--i;\r\nif (!(__raw_readl(sfrbase + REG_MMU_STATUS) & 1)) {\r\nsysmmu_unblock(sfrbase);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic void __sysmmu_tlb_invalidate(void __iomem *sfrbase)\r\n{\r\n__raw_writel(0x1, sfrbase + REG_MMU_FLUSH);\r\n}\r\nstatic void __sysmmu_tlb_invalidate_entry(void __iomem *sfrbase,\r\nunsigned long iova)\r\n{\r\n__raw_writel((iova & SPAGE_MASK) | 1, sfrbase + REG_MMU_FLUSH_ENTRY);\r\n}\r\nstatic void __sysmmu_set_ptbase(void __iomem *sfrbase,\r\nunsigned long pgd)\r\n{\r\n__raw_writel(0x1, sfrbase + REG_MMU_CFG);\r\n__raw_writel(pgd, sfrbase + REG_PT_BASE_ADDR);\r\n__sysmmu_tlb_invalidate(sfrbase);\r\n}\r\nstatic void __sysmmu_set_prefbuf(void __iomem *sfrbase, unsigned long base,\r\nunsigned long size, int idx)\r\n{\r\n__raw_writel(base, sfrbase + REG_PB0_SADDR + idx * 8);\r\n__raw_writel(size - 1 + base, sfrbase + REG_PB0_EADDR + idx * 8);\r\n}\r\nstatic void __set_fault_handler(struct sysmmu_drvdata *data,\r\nsysmmu_fault_handler_t handler)\r\n{\r\nunsigned long flags;\r\nwrite_lock_irqsave(&data->lock, flags);\r\ndata->fault_handler = handler;\r\nwrite_unlock_irqrestore(&data->lock, flags);\r\n}\r\nvoid exynos_sysmmu_set_fault_handler(struct device *dev,\r\nsysmmu_fault_handler_t handler)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev->archdata.iommu);\r\n__set_fault_handler(data, handler);\r\n}\r\nstatic int default_fault_handler(enum exynos_sysmmu_inttype itype,\r\nunsigned long pgtable_base, unsigned long fault_addr)\r\n{\r\nunsigned long *ent;\r\nif ((itype >= SYSMMU_FAULTS_NUM) || (itype < SYSMMU_PAGEFAULT))\r\nitype = SYSMMU_FAULT_UNKNOWN;\r\npr_err("%s occurred at 0x%lx(Page table base: 0x%lx)\n",\r\nsysmmu_fault_name[itype], fault_addr, pgtable_base);\r\nent = section_entry(__va(pgtable_base), fault_addr);\r\npr_err("\tLv1 entry: 0x%lx\n", *ent);\r\nif (lv1ent_page(ent)) {\r\nent = page_entry(ent, fault_addr);\r\npr_err("\t Lv2 entry: 0x%lx\n", *ent);\r\n}\r\npr_err("Generating Kernel OOPS... because it is unrecoverable.\n");\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic irqreturn_t exynos_sysmmu_irq(int irq, void *dev_id)\r\n{\r\nstruct sysmmu_drvdata *data = dev_id;\r\nstruct resource *irqres;\r\nstruct platform_device *pdev;\r\nenum exynos_sysmmu_inttype itype;\r\nunsigned long addr = -1;\r\nint i, ret = -ENOSYS;\r\nread_lock(&data->lock);\r\nWARN_ON(!is_sysmmu_active(data));\r\npdev = to_platform_device(data->sysmmu);\r\nfor (i = 0; i < (pdev->num_resources / 2); i++) {\r\nirqres = platform_get_resource(pdev, IORESOURCE_IRQ, i);\r\nif (irqres && ((int)irqres->start == irq))\r\nbreak;\r\n}\r\nif (i == pdev->num_resources) {\r\nitype = SYSMMU_FAULT_UNKNOWN;\r\n} else {\r\nitype = (enum exynos_sysmmu_inttype)\r\n__ffs(__raw_readl(data->sfrbases[i] + REG_INT_STATUS));\r\nif (WARN_ON(!((itype >= 0) && (itype < SYSMMU_FAULT_UNKNOWN))))\r\nitype = SYSMMU_FAULT_UNKNOWN;\r\nelse\r\naddr = __raw_readl(\r\ndata->sfrbases[i] + fault_reg_offset[itype]);\r\n}\r\nif (data->domain)\r\nret = report_iommu_fault(data->domain, data->dev,\r\naddr, itype);\r\nif ((ret == -ENOSYS) && data->fault_handler) {\r\nunsigned long base = data->pgtable;\r\nif (itype != SYSMMU_FAULT_UNKNOWN)\r\nbase = __raw_readl(\r\ndata->sfrbases[i] + REG_PT_BASE_ADDR);\r\nret = data->fault_handler(itype, base, addr);\r\n}\r\nif (!ret && (itype != SYSMMU_FAULT_UNKNOWN))\r\n__raw_writel(1 << itype, data->sfrbases[i] + REG_INT_CLEAR);\r\nelse\r\ndev_dbg(data->sysmmu, "(%s) %s is not handled.\n",\r\ndata->dbgname, sysmmu_fault_name[itype]);\r\nif (itype != SYSMMU_FAULT_UNKNOWN)\r\nsysmmu_unblock(data->sfrbases[i]);\r\nread_unlock(&data->lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic bool __exynos_sysmmu_disable(struct sysmmu_drvdata *data)\r\n{\r\nunsigned long flags;\r\nbool disabled = false;\r\nint i;\r\nwrite_lock_irqsave(&data->lock, flags);\r\nif (!set_sysmmu_inactive(data))\r\ngoto finish;\r\nfor (i = 0; i < data->nsfrs; i++)\r\n__raw_writel(CTRL_DISABLE, data->sfrbases[i] + REG_MMU_CTRL);\r\nif (data->clk[1])\r\nclk_disable(data->clk[1]);\r\nif (data->clk[0])\r\nclk_disable(data->clk[0]);\r\ndisabled = true;\r\ndata->pgtable = 0;\r\ndata->domain = NULL;\r\nfinish:\r\nwrite_unlock_irqrestore(&data->lock, flags);\r\nif (disabled)\r\ndev_dbg(data->sysmmu, "(%s) Disabled\n", data->dbgname);\r\nelse\r\ndev_dbg(data->sysmmu, "(%s) %d times left to be disabled\n",\r\ndata->dbgname, data->activations);\r\nreturn disabled;\r\n}\r\nstatic int __exynos_sysmmu_enable(struct sysmmu_drvdata *data,\r\nunsigned long pgtable, struct iommu_domain *domain)\r\n{\r\nint i, ret = 0;\r\nunsigned long flags;\r\nwrite_lock_irqsave(&data->lock, flags);\r\nif (!set_sysmmu_active(data)) {\r\nif (WARN_ON(pgtable != data->pgtable)) {\r\nret = -EBUSY;\r\nset_sysmmu_inactive(data);\r\n} else {\r\nret = 1;\r\n}\r\ndev_dbg(data->sysmmu, "(%s) Already enabled\n", data->dbgname);\r\ngoto finish;\r\n}\r\nif (data->clk[0])\r\nclk_enable(data->clk[0]);\r\nif (data->clk[1])\r\nclk_enable(data->clk[1]);\r\ndata->pgtable = pgtable;\r\nfor (i = 0; i < data->nsfrs; i++) {\r\n__sysmmu_set_ptbase(data->sfrbases[i], pgtable);\r\nif ((readl(data->sfrbases[i] + REG_MMU_VERSION) >> 28) == 3) {\r\n__raw_writel((1 << 12) | (2 << 28),\r\ndata->sfrbases[i] + REG_MMU_CFG);\r\n__sysmmu_set_prefbuf(data->sfrbases[i], 0, -1, 0);\r\n__sysmmu_set_prefbuf(data->sfrbases[i], 0, -1, 1);\r\n}\r\n__raw_writel(CTRL_ENABLE, data->sfrbases[i] + REG_MMU_CTRL);\r\n}\r\ndata->domain = domain;\r\ndev_dbg(data->sysmmu, "(%s) Enabled\n", data->dbgname);\r\nfinish:\r\nwrite_unlock_irqrestore(&data->lock, flags);\r\nreturn ret;\r\n}\r\nint exynos_sysmmu_enable(struct device *dev, unsigned long pgtable)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev->archdata.iommu);\r\nint ret;\r\nBUG_ON(!memblock_is_memory(pgtable));\r\nret = pm_runtime_get_sync(data->sysmmu);\r\nif (ret < 0) {\r\ndev_dbg(data->sysmmu, "(%s) Failed to enable\n", data->dbgname);\r\nreturn ret;\r\n}\r\nret = __exynos_sysmmu_enable(data, pgtable, NULL);\r\nif (WARN_ON(ret < 0)) {\r\npm_runtime_put(data->sysmmu);\r\ndev_err(data->sysmmu,\r\n"(%s) Already enabled with page table %#lx\n",\r\ndata->dbgname, data->pgtable);\r\n} else {\r\ndata->dev = dev;\r\n}\r\nreturn ret;\r\n}\r\nstatic bool exynos_sysmmu_disable(struct device *dev)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev->archdata.iommu);\r\nbool disabled;\r\ndisabled = __exynos_sysmmu_disable(data);\r\npm_runtime_put(data->sysmmu);\r\nreturn disabled;\r\n}\r\nstatic void sysmmu_tlb_invalidate_entry(struct device *dev, unsigned long iova)\r\n{\r\nunsigned long flags;\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev->archdata.iommu);\r\nread_lock_irqsave(&data->lock, flags);\r\nif (is_sysmmu_active(data)) {\r\nint i;\r\nfor (i = 0; i < data->nsfrs; i++) {\r\nif (sysmmu_block(data->sfrbases[i])) {\r\n__sysmmu_tlb_invalidate_entry(\r\ndata->sfrbases[i], iova);\r\nsysmmu_unblock(data->sfrbases[i]);\r\n}\r\n}\r\n} else {\r\ndev_dbg(data->sysmmu,\r\n"(%s) Disabled. Skipping invalidating TLB.\n",\r\ndata->dbgname);\r\n}\r\nread_unlock_irqrestore(&data->lock, flags);\r\n}\r\nvoid exynos_sysmmu_tlb_invalidate(struct device *dev)\r\n{\r\nunsigned long flags;\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev->archdata.iommu);\r\nread_lock_irqsave(&data->lock, flags);\r\nif (is_sysmmu_active(data)) {\r\nint i;\r\nfor (i = 0; i < data->nsfrs; i++) {\r\nif (sysmmu_block(data->sfrbases[i])) {\r\n__sysmmu_tlb_invalidate(data->sfrbases[i]);\r\nsysmmu_unblock(data->sfrbases[i]);\r\n}\r\n}\r\n} else {\r\ndev_dbg(data->sysmmu,\r\n"(%s) Disabled. Skipping invalidating TLB.\n",\r\ndata->dbgname);\r\n}\r\nread_unlock_irqrestore(&data->lock, flags);\r\n}\r\nstatic int exynos_sysmmu_probe(struct platform_device *pdev)\r\n{\r\nint i, ret;\r\nstruct device *dev;\r\nstruct sysmmu_drvdata *data;\r\ndev = &pdev->dev;\r\ndata = kzalloc(sizeof(*data), GFP_KERNEL);\r\nif (!data) {\r\ndev_dbg(dev, "Not enough memory\n");\r\nret = -ENOMEM;\r\ngoto err_alloc;\r\n}\r\nret = dev_set_drvdata(dev, data);\r\nif (ret) {\r\ndev_dbg(dev, "Unabled to initialize driver data\n");\r\ngoto err_init;\r\n}\r\ndata->nsfrs = pdev->num_resources / 2;\r\ndata->sfrbases = kmalloc(sizeof(*data->sfrbases) * data->nsfrs,\r\nGFP_KERNEL);\r\nif (data->sfrbases == NULL) {\r\ndev_dbg(dev, "Not enough memory\n");\r\nret = -ENOMEM;\r\ngoto err_init;\r\n}\r\nfor (i = 0; i < data->nsfrs; i++) {\r\nstruct resource *res;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, i);\r\nif (!res) {\r\ndev_dbg(dev, "Unable to find IOMEM region\n");\r\nret = -ENOENT;\r\ngoto err_res;\r\n}\r\ndata->sfrbases[i] = ioremap(res->start, resource_size(res));\r\nif (!data->sfrbases[i]) {\r\ndev_dbg(dev, "Unable to map IOMEM @ PA:%#x\n",\r\nres->start);\r\nret = -ENOENT;\r\ngoto err_res;\r\n}\r\n}\r\nfor (i = 0; i < data->nsfrs; i++) {\r\nret = platform_get_irq(pdev, i);\r\nif (ret <= 0) {\r\ndev_dbg(dev, "Unable to find IRQ resource\n");\r\ngoto err_irq;\r\n}\r\nret = request_irq(ret, exynos_sysmmu_irq, 0,\r\ndev_name(dev), data);\r\nif (ret) {\r\ndev_dbg(dev, "Unabled to register interrupt handler\n");\r\ngoto err_irq;\r\n}\r\n}\r\nif (dev_get_platdata(dev)) {\r\nchar *deli, *beg;\r\nstruct sysmmu_platform_data *platdata = dev_get_platdata(dev);\r\nbeg = platdata->clockname;\r\nfor (deli = beg; (*deli != '\0') && (*deli != ','); deli++)\r\n;\r\nif (*deli == '\0')\r\ndeli = NULL;\r\nelse\r\n*deli = '\0';\r\ndata->clk[0] = clk_get(dev, beg);\r\nif (IS_ERR(data->clk[0])) {\r\ndata->clk[0] = NULL;\r\ndev_dbg(dev, "No clock descriptor registered\n");\r\n}\r\nif (data->clk[0] && deli) {\r\n*deli = ',';\r\ndata->clk[1] = clk_get(dev, deli + 1);\r\nif (IS_ERR(data->clk[1]))\r\ndata->clk[1] = NULL;\r\n}\r\ndata->dbgname = platdata->dbgname;\r\n}\r\ndata->sysmmu = dev;\r\nrwlock_init(&data->lock);\r\nINIT_LIST_HEAD(&data->node);\r\n__set_fault_handler(data, &default_fault_handler);\r\nif (dev->parent)\r\npm_runtime_enable(dev);\r\ndev_dbg(dev, "(%s) Initialized\n", data->dbgname);\r\nreturn 0;\r\nerr_irq:\r\nwhile (i-- > 0) {\r\nint irq;\r\nirq = platform_get_irq(pdev, i);\r\nfree_irq(irq, data);\r\n}\r\nerr_res:\r\nwhile (data->nsfrs-- > 0)\r\niounmap(data->sfrbases[data->nsfrs]);\r\nkfree(data->sfrbases);\r\nerr_init:\r\nkfree(data);\r\nerr_alloc:\r\ndev_err(dev, "Failed to initialize\n");\r\nreturn ret;\r\n}\r\nstatic inline void pgtable_flush(void *vastart, void *vaend)\r\n{\r\ndmac_flush_range(vastart, vaend);\r\nouter_flush_range(virt_to_phys(vastart),\r\nvirt_to_phys(vaend));\r\n}\r\nstatic int exynos_iommu_domain_init(struct iommu_domain *domain)\r\n{\r\nstruct exynos_iommu_domain *priv;\r\npriv = kzalloc(sizeof(*priv), GFP_KERNEL);\r\nif (!priv)\r\nreturn -ENOMEM;\r\npriv->pgtable = (unsigned long *)__get_free_pages(\r\nGFP_KERNEL | __GFP_ZERO, 2);\r\nif (!priv->pgtable)\r\ngoto err_pgtable;\r\npriv->lv2entcnt = (short *)__get_free_pages(\r\nGFP_KERNEL | __GFP_ZERO, 1);\r\nif (!priv->lv2entcnt)\r\ngoto err_counter;\r\npgtable_flush(priv->pgtable, priv->pgtable + NUM_LV1ENTRIES);\r\nspin_lock_init(&priv->lock);\r\nspin_lock_init(&priv->pgtablelock);\r\nINIT_LIST_HEAD(&priv->clients);\r\ndomain->geometry.aperture_start = 0;\r\ndomain->geometry.aperture_end = ~0UL;\r\ndomain->geometry.force_aperture = true;\r\ndomain->priv = priv;\r\nreturn 0;\r\nerr_counter:\r\nfree_pages((unsigned long)priv->pgtable, 2);\r\nerr_pgtable:\r\nkfree(priv);\r\nreturn -ENOMEM;\r\n}\r\nstatic void exynos_iommu_domain_destroy(struct iommu_domain *domain)\r\n{\r\nstruct exynos_iommu_domain *priv = domain->priv;\r\nstruct sysmmu_drvdata *data;\r\nunsigned long flags;\r\nint i;\r\nWARN_ON(!list_empty(&priv->clients));\r\nspin_lock_irqsave(&priv->lock, flags);\r\nlist_for_each_entry(data, &priv->clients, node) {\r\nwhile (!exynos_sysmmu_disable(data->dev))\r\n;\r\n}\r\nspin_unlock_irqrestore(&priv->lock, flags);\r\nfor (i = 0; i < NUM_LV1ENTRIES; i++)\r\nif (lv1ent_page(priv->pgtable + i))\r\nkfree(__va(lv2table_base(priv->pgtable + i)));\r\nfree_pages((unsigned long)priv->pgtable, 2);\r\nfree_pages((unsigned long)priv->lv2entcnt, 1);\r\nkfree(domain->priv);\r\ndomain->priv = NULL;\r\n}\r\nstatic int exynos_iommu_attach_device(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev->archdata.iommu);\r\nstruct exynos_iommu_domain *priv = domain->priv;\r\nunsigned long flags;\r\nint ret;\r\nret = pm_runtime_get_sync(data->sysmmu);\r\nif (ret < 0)\r\nreturn ret;\r\nret = 0;\r\nspin_lock_irqsave(&priv->lock, flags);\r\nret = __exynos_sysmmu_enable(data, __pa(priv->pgtable), domain);\r\nif (ret == 0) {\r\nBUG_ON(!list_empty(&data->node));\r\ndata->dev = dev;\r\nlist_add_tail(&data->node, &priv->clients);\r\n}\r\nspin_unlock_irqrestore(&priv->lock, flags);\r\nif (ret < 0) {\r\ndev_err(dev, "%s: Failed to attach IOMMU with pgtable %#lx\n",\r\n__func__, __pa(priv->pgtable));\r\npm_runtime_put(data->sysmmu);\r\n} else if (ret > 0) {\r\ndev_dbg(dev, "%s: IOMMU with pgtable 0x%lx already attached\n",\r\n__func__, __pa(priv->pgtable));\r\n} else {\r\ndev_dbg(dev, "%s: Attached new IOMMU with pgtable 0x%lx\n",\r\n__func__, __pa(priv->pgtable));\r\n}\r\nreturn ret;\r\n}\r\nstatic void exynos_iommu_detach_device(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev->archdata.iommu);\r\nstruct exynos_iommu_domain *priv = domain->priv;\r\nstruct list_head *pos;\r\nunsigned long flags;\r\nbool found = false;\r\nspin_lock_irqsave(&priv->lock, flags);\r\nlist_for_each(pos, &priv->clients) {\r\nif (list_entry(pos, struct sysmmu_drvdata, node) == data) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found)\r\ngoto finish;\r\nif (__exynos_sysmmu_disable(data)) {\r\ndev_dbg(dev, "%s: Detached IOMMU with pgtable %#lx\n",\r\n__func__, __pa(priv->pgtable));\r\nlist_del_init(&data->node);\r\n} else {\r\ndev_dbg(dev, "%s: Detaching IOMMU with pgtable %#lx delayed",\r\n__func__, __pa(priv->pgtable));\r\n}\r\nfinish:\r\nspin_unlock_irqrestore(&priv->lock, flags);\r\nif (found)\r\npm_runtime_put(data->sysmmu);\r\n}\r\nstatic unsigned long *alloc_lv2entry(unsigned long *sent, unsigned long iova,\r\nshort *pgcounter)\r\n{\r\nif (lv1ent_fault(sent)) {\r\nunsigned long *pent;\r\npent = kzalloc(LV2TABLE_SIZE, GFP_ATOMIC);\r\nBUG_ON((unsigned long)pent & (LV2TABLE_SIZE - 1));\r\nif (!pent)\r\nreturn NULL;\r\n*sent = mk_lv1ent_page(__pa(pent));\r\n*pgcounter = NUM_LV2ENTRIES;\r\npgtable_flush(pent, pent + NUM_LV2ENTRIES);\r\npgtable_flush(sent, sent + 1);\r\n}\r\nreturn page_entry(sent, iova);\r\n}\r\nstatic int lv1set_section(unsigned long *sent, phys_addr_t paddr, short *pgcnt)\r\n{\r\nif (lv1ent_section(sent))\r\nreturn -EADDRINUSE;\r\nif (lv1ent_page(sent)) {\r\nif (*pgcnt != NUM_LV2ENTRIES)\r\nreturn -EADDRINUSE;\r\nkfree(page_entry(sent, 0));\r\n*pgcnt = 0;\r\n}\r\n*sent = mk_lv1ent_sect(paddr);\r\npgtable_flush(sent, sent + 1);\r\nreturn 0;\r\n}\r\nstatic int lv2set_page(unsigned long *pent, phys_addr_t paddr, size_t size,\r\nshort *pgcnt)\r\n{\r\nif (size == SPAGE_SIZE) {\r\nif (!lv2ent_fault(pent))\r\nreturn -EADDRINUSE;\r\n*pent = mk_lv2ent_spage(paddr);\r\npgtable_flush(pent, pent + 1);\r\n*pgcnt -= 1;\r\n} else {\r\nint i;\r\nfor (i = 0; i < SPAGES_PER_LPAGE; i++, pent++) {\r\nif (!lv2ent_fault(pent)) {\r\nmemset(pent, 0, sizeof(*pent) * i);\r\nreturn -EADDRINUSE;\r\n}\r\n*pent = mk_lv2ent_lpage(paddr);\r\n}\r\npgtable_flush(pent - SPAGES_PER_LPAGE, pent);\r\n*pgcnt -= SPAGES_PER_LPAGE;\r\n}\r\nreturn 0;\r\n}\r\nstatic int exynos_iommu_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct exynos_iommu_domain *priv = domain->priv;\r\nunsigned long *entry;\r\nunsigned long flags;\r\nint ret = -ENOMEM;\r\nBUG_ON(priv->pgtable == NULL);\r\nspin_lock_irqsave(&priv->pgtablelock, flags);\r\nentry = section_entry(priv->pgtable, iova);\r\nif (size == SECT_SIZE) {\r\nret = lv1set_section(entry, paddr,\r\n&priv->lv2entcnt[lv1ent_offset(iova)]);\r\n} else {\r\nunsigned long *pent;\r\npent = alloc_lv2entry(entry, iova,\r\n&priv->lv2entcnt[lv1ent_offset(iova)]);\r\nif (!pent)\r\nret = -ENOMEM;\r\nelse\r\nret = lv2set_page(pent, paddr, size,\r\n&priv->lv2entcnt[lv1ent_offset(iova)]);\r\n}\r\nif (ret) {\r\npr_debug("%s: Failed to map iova 0x%lx/0x%x bytes\n",\r\n__func__, iova, size);\r\n}\r\nspin_unlock_irqrestore(&priv->pgtablelock, flags);\r\nreturn ret;\r\n}\r\nstatic size_t exynos_iommu_unmap(struct iommu_domain *domain,\r\nunsigned long iova, size_t size)\r\n{\r\nstruct exynos_iommu_domain *priv = domain->priv;\r\nstruct sysmmu_drvdata *data;\r\nunsigned long flags;\r\nunsigned long *ent;\r\nBUG_ON(priv->pgtable == NULL);\r\nspin_lock_irqsave(&priv->pgtablelock, flags);\r\nent = section_entry(priv->pgtable, iova);\r\nif (lv1ent_section(ent)) {\r\nBUG_ON(size < SECT_SIZE);\r\n*ent = 0;\r\npgtable_flush(ent, ent + 1);\r\nsize = SECT_SIZE;\r\ngoto done;\r\n}\r\nif (unlikely(lv1ent_fault(ent))) {\r\nif (size > SECT_SIZE)\r\nsize = SECT_SIZE;\r\ngoto done;\r\n}\r\nent = page_entry(ent, iova);\r\nif (unlikely(lv2ent_fault(ent))) {\r\nsize = SPAGE_SIZE;\r\ngoto done;\r\n}\r\nif (lv2ent_small(ent)) {\r\n*ent = 0;\r\nsize = SPAGE_SIZE;\r\npriv->lv2entcnt[lv1ent_offset(iova)] += 1;\r\ngoto done;\r\n}\r\nBUG_ON(size < LPAGE_SIZE);\r\nmemset(ent, 0, sizeof(*ent) * SPAGES_PER_LPAGE);\r\nsize = LPAGE_SIZE;\r\npriv->lv2entcnt[lv1ent_offset(iova)] += SPAGES_PER_LPAGE;\r\ndone:\r\nspin_unlock_irqrestore(&priv->pgtablelock, flags);\r\nspin_lock_irqsave(&priv->lock, flags);\r\nlist_for_each_entry(data, &priv->clients, node)\r\nsysmmu_tlb_invalidate_entry(data->dev, iova);\r\nspin_unlock_irqrestore(&priv->lock, flags);\r\nreturn size;\r\n}\r\nstatic phys_addr_t exynos_iommu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct exynos_iommu_domain *priv = domain->priv;\r\nunsigned long *entry;\r\nunsigned long flags;\r\nphys_addr_t phys = 0;\r\nspin_lock_irqsave(&priv->pgtablelock, flags);\r\nentry = section_entry(priv->pgtable, iova);\r\nif (lv1ent_section(entry)) {\r\nphys = section_phys(entry) + section_offs(iova);\r\n} else if (lv1ent_page(entry)) {\r\nentry = page_entry(entry, iova);\r\nif (lv2ent_large(entry))\r\nphys = lpage_phys(entry) + lpage_offs(iova);\r\nelse if (lv2ent_small(entry))\r\nphys = spage_phys(entry) + spage_offs(iova);\r\n}\r\nspin_unlock_irqrestore(&priv->pgtablelock, flags);\r\nreturn phys;\r\n}\r\nstatic int __init exynos_iommu_init(void)\r\n{\r\nint ret;\r\nret = platform_driver_register(&exynos_sysmmu_driver);\r\nif (ret == 0)\r\nbus_set_iommu(&platform_bus_type, &exynos_iommu_ops);\r\nreturn ret;\r\n}
