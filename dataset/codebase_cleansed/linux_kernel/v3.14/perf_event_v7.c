static inline u32 armv7_pmnc_read(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p15, 0, %0, c9, c12, 0" : "=r"(val));\r\nreturn val;\r\n}\r\nstatic inline void armv7_pmnc_write(u32 val)\r\n{\r\nval &= ARMV7_PMNC_MASK;\r\nisb();\r\nasm volatile("mcr p15, 0, %0, c9, c12, 0" : : "r"(val));\r\n}\r\nstatic inline int armv7_pmnc_has_overflowed(u32 pmnc)\r\n{\r\nreturn pmnc & ARMV7_OVERFLOWED_MASK;\r\n}\r\nstatic inline int armv7_pmnc_counter_valid(struct arm_pmu *cpu_pmu, int idx)\r\n{\r\nreturn idx >= ARMV7_IDX_CYCLE_COUNTER &&\r\nidx <= ARMV7_IDX_COUNTER_LAST(cpu_pmu);\r\n}\r\nstatic inline int armv7_pmnc_counter_has_overflowed(u32 pmnc, int idx)\r\n{\r\nreturn pmnc & BIT(ARMV7_IDX_TO_COUNTER(idx));\r\n}\r\nstatic inline int armv7_pmnc_select_counter(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c12, 5" : : "r" (counter));\r\nisb();\r\nreturn idx;\r\n}\r\nstatic inline u32 armv7pmu_read_counter(struct perf_event *event)\r\n{\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nu32 value = 0;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx))\r\npr_err("CPU%u reading wrong counter %d\n",\r\nsmp_processor_id(), idx);\r\nelse if (idx == ARMV7_IDX_CYCLE_COUNTER)\r\nasm volatile("mrc p15, 0, %0, c9, c13, 0" : "=r" (value));\r\nelse if (armv7_pmnc_select_counter(idx) == idx)\r\nasm volatile("mrc p15, 0, %0, c9, c13, 2" : "=r" (value));\r\nreturn value;\r\n}\r\nstatic inline void armv7pmu_write_counter(struct perf_event *event, u32 value)\r\n{\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx))\r\npr_err("CPU%u writing wrong counter %d\n",\r\nsmp_processor_id(), idx);\r\nelse if (idx == ARMV7_IDX_CYCLE_COUNTER)\r\nasm volatile("mcr p15, 0, %0, c9, c13, 0" : : "r" (value));\r\nelse if (armv7_pmnc_select_counter(idx) == idx)\r\nasm volatile("mcr p15, 0, %0, c9, c13, 2" : : "r" (value));\r\n}\r\nstatic inline void armv7_pmnc_write_evtsel(int idx, u32 val)\r\n{\r\nif (armv7_pmnc_select_counter(idx) == idx) {\r\nval &= ARMV7_EVTYPE_MASK;\r\nasm volatile("mcr p15, 0, %0, c9, c13, 1" : : "r" (val));\r\n}\r\n}\r\nstatic inline int armv7_pmnc_enable_counter(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c12, 1" : : "r" (BIT(counter)));\r\nreturn idx;\r\n}\r\nstatic inline int armv7_pmnc_disable_counter(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c12, 2" : : "r" (BIT(counter)));\r\nreturn idx;\r\n}\r\nstatic inline int armv7_pmnc_enable_intens(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c14, 1" : : "r" (BIT(counter)));\r\nreturn idx;\r\n}\r\nstatic inline int armv7_pmnc_disable_intens(int idx)\r\n{\r\nu32 counter = ARMV7_IDX_TO_COUNTER(idx);\r\nasm volatile("mcr p15, 0, %0, c9, c14, 2" : : "r" (BIT(counter)));\r\nisb();\r\nasm volatile("mcr p15, 0, %0, c9, c12, 3" : : "r" (BIT(counter)));\r\nisb();\r\nreturn idx;\r\n}\r\nstatic inline u32 armv7_pmnc_getreset_flags(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p15, 0, %0, c9, c12, 3" : "=r" (val));\r\nval &= ARMV7_FLAG_MASK;\r\nasm volatile("mcr p15, 0, %0, c9, c12, 3" : : "r" (val));\r\nreturn val;\r\n}\r\nstatic void armv7_pmnc_dump_regs(struct arm_pmu *cpu_pmu)\r\n{\r\nu32 val;\r\nunsigned int cnt;\r\nprintk(KERN_INFO "PMNC registers dump:\n");\r\nasm volatile("mrc p15, 0, %0, c9, c12, 0" : "=r" (val));\r\nprintk(KERN_INFO "PMNC =0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c12, 1" : "=r" (val));\r\nprintk(KERN_INFO "CNTENS=0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c14, 1" : "=r" (val));\r\nprintk(KERN_INFO "INTENS=0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c12, 3" : "=r" (val));\r\nprintk(KERN_INFO "FLAGS =0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c12, 5" : "=r" (val));\r\nprintk(KERN_INFO "SELECT=0x%08x\n", val);\r\nasm volatile("mrc p15, 0, %0, c9, c13, 0" : "=r" (val));\r\nprintk(KERN_INFO "CCNT =0x%08x\n", val);\r\nfor (cnt = ARMV7_IDX_COUNTER0;\r\ncnt <= ARMV7_IDX_COUNTER_LAST(cpu_pmu); cnt++) {\r\narmv7_pmnc_select_counter(cnt);\r\nasm volatile("mrc p15, 0, %0, c9, c13, 2" : "=r" (val));\r\nprintk(KERN_INFO "CNT[%d] count =0x%08x\n",\r\nARMV7_IDX_TO_COUNTER(cnt), val);\r\nasm volatile("mrc p15, 0, %0, c9, c13, 1" : "=r" (val));\r\nprintk(KERN_INFO "CNT[%d] evtsel=0x%08x\n",\r\nARMV7_IDX_TO_COUNTER(cnt), val);\r\n}\r\n}\r\nstatic void armv7pmu_enable_event(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx)) {\r\npr_err("CPU%u enabling wrong PMNC counter IRQ enable %d\n",\r\nsmp_processor_id(), idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_disable_counter(idx);\r\nif (cpu_pmu->set_event_filter || idx != ARMV7_IDX_CYCLE_COUNTER)\r\narmv7_pmnc_write_evtsel(idx, hwc->config_base);\r\narmv7_pmnc_enable_intens(idx);\r\narmv7_pmnc_enable_counter(idx);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void armv7pmu_disable_event(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nif (!armv7_pmnc_counter_valid(cpu_pmu, idx)) {\r\npr_err("CPU%u disabling wrong PMNC counter IRQ enable %d\n",\r\nsmp_processor_id(), idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_disable_counter(idx);\r\narmv7_pmnc_disable_intens(idx);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic irqreturn_t armv7pmu_handle_irq(int irq_num, void *dev)\r\n{\r\nu32 pmnc;\r\nstruct perf_sample_data data;\r\nstruct arm_pmu *cpu_pmu = (struct arm_pmu *)dev;\r\nstruct pmu_hw_events *cpuc = cpu_pmu->get_hw_events();\r\nstruct pt_regs *regs;\r\nint idx;\r\npmnc = armv7_pmnc_getreset_flags();\r\nif (!armv7_pmnc_has_overflowed(pmnc))\r\nreturn IRQ_NONE;\r\nregs = get_irq_regs();\r\nfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\r\nstruct perf_event *event = cpuc->events[idx];\r\nstruct hw_perf_event *hwc;\r\nif (!event)\r\ncontinue;\r\nif (!armv7_pmnc_counter_has_overflowed(pmnc, idx))\r\ncontinue;\r\nhwc = &event->hw;\r\narmpmu_event_update(event);\r\nperf_sample_data_init(&data, 0, hwc->last_period);\r\nif (!armpmu_event_set_period(event))\r\ncontinue;\r\nif (perf_event_overflow(event, &data, regs))\r\ncpu_pmu->disable(event);\r\n}\r\nirq_work_run();\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void armv7pmu_start(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_write(armv7_pmnc_read() | ARMV7_PMNC_E);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void armv7pmu_stop(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\narmv7_pmnc_write(armv7_pmnc_read() & ~ARMV7_PMNC_E);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic int armv7pmu_get_event_idx(struct pmu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nint idx;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned long evtype = hwc->config_base & ARMV7_EVTYPE_EVENT;\r\nif (evtype == ARMV7_PERFCTR_CPU_CYCLES) {\r\nif (test_and_set_bit(ARMV7_IDX_CYCLE_COUNTER, cpuc->used_mask))\r\nreturn -EAGAIN;\r\nreturn ARMV7_IDX_CYCLE_COUNTER;\r\n}\r\nfor (idx = ARMV7_IDX_COUNTER0; idx < cpu_pmu->num_events; ++idx) {\r\nif (!test_and_set_bit(idx, cpuc->used_mask))\r\nreturn idx;\r\n}\r\nreturn -EAGAIN;\r\n}\r\nstatic int armv7pmu_set_event_filter(struct hw_perf_event *event,\r\nstruct perf_event_attr *attr)\r\n{\r\nunsigned long config_base = 0;\r\nif (attr->exclude_idle)\r\nreturn -EPERM;\r\nif (attr->exclude_user)\r\nconfig_base |= ARMV7_EXCLUDE_USER;\r\nif (attr->exclude_kernel)\r\nconfig_base |= ARMV7_EXCLUDE_PL1;\r\nif (!attr->exclude_hv)\r\nconfig_base |= ARMV7_INCLUDE_HYP;\r\nevent->config_base = config_base;\r\nreturn 0;\r\n}\r\nstatic void armv7pmu_reset(void *info)\r\n{\r\nstruct arm_pmu *cpu_pmu = (struct arm_pmu *)info;\r\nu32 idx, nb_cnt = cpu_pmu->num_events;\r\nfor (idx = ARMV7_IDX_CYCLE_COUNTER; idx < nb_cnt; ++idx) {\r\narmv7_pmnc_disable_counter(idx);\r\narmv7_pmnc_disable_intens(idx);\r\n}\r\narmv7_pmnc_write(ARMV7_PMNC_P | ARMV7_PMNC_C);\r\n}\r\nstatic int armv7_a8_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a8_perf_map,\r\n&armv7_a8_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a9_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a9_perf_map,\r\n&armv7_a9_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a5_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a5_perf_map,\r\n&armv7_a5_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a15_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a15_perf_map,\r\n&armv7_a15_perf_cache_map, 0xFF);\r\n}\r\nstatic int armv7_a7_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &armv7_a7_perf_map,\r\n&armv7_a7_perf_cache_map, 0xFF);\r\n}\r\nstatic void armv7pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\ncpu_pmu->handle_irq = armv7pmu_handle_irq;\r\ncpu_pmu->enable = armv7pmu_enable_event;\r\ncpu_pmu->disable = armv7pmu_disable_event;\r\ncpu_pmu->read_counter = armv7pmu_read_counter;\r\ncpu_pmu->write_counter = armv7pmu_write_counter;\r\ncpu_pmu->get_event_idx = armv7pmu_get_event_idx;\r\ncpu_pmu->start = armv7pmu_start;\r\ncpu_pmu->stop = armv7pmu_stop;\r\ncpu_pmu->reset = armv7pmu_reset;\r\ncpu_pmu->max_period = (1LLU << 32) - 1;\r\n}\r\nstatic u32 armv7_read_num_pmnc_events(void)\r\n{\r\nu32 nb_cnt;\r\nnb_cnt = (armv7_pmnc_read() >> ARMV7_PMNC_N_SHIFT) & ARMV7_PMNC_N_MASK;\r\nreturn nb_cnt + 1;\r\n}\r\nstatic int armv7_a8_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "ARMv7 Cortex-A8";\r\ncpu_pmu->map_event = armv7_a8_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\nreturn 0;\r\n}\r\nstatic int armv7_a9_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "ARMv7 Cortex-A9";\r\ncpu_pmu->map_event = armv7_a9_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\nreturn 0;\r\n}\r\nstatic int armv7_a5_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "ARMv7 Cortex-A5";\r\ncpu_pmu->map_event = armv7_a5_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\nreturn 0;\r\n}\r\nstatic int armv7_a15_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "ARMv7 Cortex-A15";\r\ncpu_pmu->map_event = armv7_a15_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\ncpu_pmu->set_event_filter = armv7pmu_set_event_filter;\r\nreturn 0;\r\n}\r\nstatic int armv7_a7_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\narmv7pmu_init(cpu_pmu);\r\ncpu_pmu->name = "ARMv7 Cortex-A7";\r\ncpu_pmu->map_event = armv7_a7_map_event;\r\ncpu_pmu->num_events = armv7_read_num_pmnc_events();\r\ncpu_pmu->set_event_filter = armv7pmu_set_event_filter;\r\nreturn 0;\r\n}\r\nstatic inline int armv7_a8_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a9_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a5_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a15_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int armv7_a7_pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}
