static u32 get_ccsidr(u32 csselr)\r\n{\r\nu32 ccsidr;\r\nlocal_irq_disable();\r\nasm volatile("msr csselr_el1, %x0" : : "r" (csselr));\r\nisb();\r\nasm volatile("mrs %0, ccsidr_el1" : "=r" (ccsidr));\r\nlocal_irq_enable();\r\nreturn ccsidr;\r\n}\r\nstatic void do_dc_cisw(u32 val)\r\n{\r\nasm volatile("dc cisw, %x0" : : "r" (val));\r\ndsb();\r\n}\r\nstatic void do_dc_csw(u32 val)\r\n{\r\nasm volatile("dc csw, %x0" : : "r" (val));\r\ndsb();\r\n}\r\nstatic bool access_dcsw(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nunsigned long val;\r\nint cpu;\r\nif (!p->is_write)\r\nreturn read_from_write_only(vcpu, p);\r\ncpu = get_cpu();\r\ncpumask_setall(&vcpu->arch.require_dcache_flush);\r\ncpumask_clear_cpu(cpu, &vcpu->arch.require_dcache_flush);\r\nif (cpu != vcpu->arch.last_pcpu) {\r\nflush_cache_all();\r\ngoto done;\r\n}\r\nval = *vcpu_reg(vcpu, p->Rt);\r\nswitch (p->CRm) {\r\ncase 6:\r\ncase 14:\r\ndo_dc_cisw(val);\r\nbreak;\r\ncase 10:\r\ndo_dc_csw(val);\r\nbreak;\r\n}\r\ndone:\r\nput_cpu();\r\nreturn true;\r\n}\r\nstatic bool pm_fake(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write)\r\nreturn ignore_write(vcpu, p);\r\nelse\r\nreturn read_zero(vcpu, p);\r\n}\r\nstatic void reset_amair_el1(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\r\n{\r\nu64 amair;\r\nasm volatile("mrs %0, amair_el1\n" : "=r" (amair));\r\nvcpu_sys_reg(vcpu, AMAIR_EL1) = amair;\r\n}\r\nstatic void reset_mpidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\r\n{\r\nvcpu_sys_reg(vcpu, MPIDR_EL1) = (1UL << 31) | (vcpu->vcpu_id & 0xff);\r\n}\r\nvoid kvm_register_target_sys_reg_table(unsigned int target,\r\nstruct kvm_sys_reg_target_table *table)\r\n{\r\ntarget_tables[target] = table;\r\n}\r\nstatic const struct sys_reg_desc *get_target_table(unsigned target,\r\nbool mode_is_64,\r\nsize_t *num)\r\n{\r\nstruct kvm_sys_reg_target_table *table;\r\ntable = target_tables[target];\r\nif (mode_is_64) {\r\n*num = table->table64.num;\r\nreturn table->table64.table;\r\n} else {\r\n*num = table->table32.num;\r\nreturn table->table32.table;\r\n}\r\n}\r\nstatic const struct sys_reg_desc *find_reg(const struct sys_reg_params *params,\r\nconst struct sys_reg_desc table[],\r\nunsigned int num)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < num; i++) {\r\nconst struct sys_reg_desc *r = &table[i];\r\nif (params->Op0 != r->Op0)\r\ncontinue;\r\nif (params->Op1 != r->Op1)\r\ncontinue;\r\nif (params->CRn != r->CRn)\r\ncontinue;\r\nif (params->CRm != r->CRm)\r\ncontinue;\r\nif (params->Op2 != r->Op2)\r\ncontinue;\r\nreturn r;\r\n}\r\nreturn NULL;\r\n}\r\nint kvm_handle_cp14_load_store(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nint kvm_handle_cp14_access(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nstatic void emulate_cp15(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_params *params)\r\n{\r\nsize_t num;\r\nconst struct sys_reg_desc *table, *r;\r\ntable = get_target_table(vcpu->arch.target, false, &num);\r\nr = find_reg(params, table, num);\r\nif (!r)\r\nr = find_reg(params, cp15_regs, ARRAY_SIZE(cp15_regs));\r\nif (likely(r)) {\r\nBUG_ON(!r->access);\r\nif (likely(r->access(vcpu, params, r))) {\r\nkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\r\nreturn;\r\n}\r\n}\r\nkvm_err("Unsupported guest CP15 access at: %08lx\n", *vcpu_pc(vcpu));\r\nprint_sys_reg_instr(params);\r\nkvm_inject_undefined(vcpu);\r\n}\r\nint kvm_handle_cp15_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nstruct sys_reg_params params;\r\nu32 hsr = kvm_vcpu_get_hsr(vcpu);\r\nint Rt2 = (hsr >> 10) & 0xf;\r\nparams.CRm = (hsr >> 1) & 0xf;\r\nparams.Rt = (hsr >> 5) & 0xf;\r\nparams.is_write = ((hsr & 1) == 0);\r\nparams.Op0 = 0;\r\nparams.Op1 = (hsr >> 16) & 0xf;\r\nparams.Op2 = 0;\r\nparams.CRn = 0;\r\nif (params.is_write) {\r\nu64 val = *vcpu_reg(vcpu, params.Rt);\r\nval &= 0xffffffff;\r\nval |= *vcpu_reg(vcpu, Rt2) << 32;\r\n*vcpu_reg(vcpu, params.Rt) = val;\r\n}\r\nemulate_cp15(vcpu, &params);\r\nif (!params.is_write) {\r\nu64 val = *vcpu_reg(vcpu, params.Rt);\r\nval >>= 32;\r\n*vcpu_reg(vcpu, Rt2) = val;\r\n}\r\nreturn 1;\r\n}\r\nint kvm_handle_cp15_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nstruct sys_reg_params params;\r\nu32 hsr = kvm_vcpu_get_hsr(vcpu);\r\nparams.CRm = (hsr >> 1) & 0xf;\r\nparams.Rt = (hsr >> 5) & 0xf;\r\nparams.is_write = ((hsr & 1) == 0);\r\nparams.CRn = (hsr >> 10) & 0xf;\r\nparams.Op0 = 0;\r\nparams.Op1 = (hsr >> 14) & 0x7;\r\nparams.Op2 = (hsr >> 17) & 0x7;\r\nemulate_cp15(vcpu, &params);\r\nreturn 1;\r\n}\r\nstatic int emulate_sys_reg(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_params *params)\r\n{\r\nsize_t num;\r\nconst struct sys_reg_desc *table, *r;\r\ntable = get_target_table(vcpu->arch.target, true, &num);\r\nr = find_reg(params, table, num);\r\nif (!r)\r\nr = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\r\nif (likely(r)) {\r\nBUG_ON(!r->access);\r\nif (likely(r->access(vcpu, params, r))) {\r\nkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\r\nreturn 1;\r\n}\r\n} else {\r\nkvm_err("Unsupported guest sys_reg access at: %lx\n",\r\n*vcpu_pc(vcpu));\r\nprint_sys_reg_instr(params);\r\n}\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nstatic void reset_sys_reg_descs(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *table, size_t num)\r\n{\r\nunsigned long i;\r\nfor (i = 0; i < num; i++)\r\nif (table[i].reset)\r\ntable[i].reset(vcpu, &table[i]);\r\n}\r\nint kvm_handle_sys_reg(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nstruct sys_reg_params params;\r\nunsigned long esr = kvm_vcpu_get_hsr(vcpu);\r\nparams.Op0 = (esr >> 20) & 3;\r\nparams.Op1 = (esr >> 14) & 0x7;\r\nparams.CRn = (esr >> 10) & 0xf;\r\nparams.CRm = (esr >> 1) & 0xf;\r\nparams.Op2 = (esr >> 17) & 0x7;\r\nparams.Rt = (esr >> 5) & 0x1f;\r\nparams.is_write = !(esr & 1);\r\nreturn emulate_sys_reg(vcpu, &params);\r\n}\r\nstatic bool index_to_params(u64 id, struct sys_reg_params *params)\r\n{\r\nswitch (id & KVM_REG_SIZE_MASK) {\r\ncase KVM_REG_SIZE_U64:\r\nif (id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK\r\n| KVM_REG_ARM_COPROC_MASK\r\n| KVM_REG_ARM64_SYSREG_OP0_MASK\r\n| KVM_REG_ARM64_SYSREG_OP1_MASK\r\n| KVM_REG_ARM64_SYSREG_CRN_MASK\r\n| KVM_REG_ARM64_SYSREG_CRM_MASK\r\n| KVM_REG_ARM64_SYSREG_OP2_MASK))\r\nreturn false;\r\nparams->Op0 = ((id & KVM_REG_ARM64_SYSREG_OP0_MASK)\r\n>> KVM_REG_ARM64_SYSREG_OP0_SHIFT);\r\nparams->Op1 = ((id & KVM_REG_ARM64_SYSREG_OP1_MASK)\r\n>> KVM_REG_ARM64_SYSREG_OP1_SHIFT);\r\nparams->CRn = ((id & KVM_REG_ARM64_SYSREG_CRN_MASK)\r\n>> KVM_REG_ARM64_SYSREG_CRN_SHIFT);\r\nparams->CRm = ((id & KVM_REG_ARM64_SYSREG_CRM_MASK)\r\n>> KVM_REG_ARM64_SYSREG_CRM_SHIFT);\r\nparams->Op2 = ((id & KVM_REG_ARM64_SYSREG_OP2_MASK)\r\n>> KVM_REG_ARM64_SYSREG_OP2_SHIFT);\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic const struct sys_reg_desc *index_to_sys_reg_desc(struct kvm_vcpu *vcpu,\r\nu64 id)\r\n{\r\nsize_t num;\r\nconst struct sys_reg_desc *table, *r;\r\nstruct sys_reg_params params;\r\nif ((id & KVM_REG_ARM_COPROC_MASK) != KVM_REG_ARM64_SYSREG)\r\nreturn NULL;\r\nif (!index_to_params(id, &params))\r\nreturn NULL;\r\ntable = get_target_table(vcpu->arch.target, true, &num);\r\nr = find_reg(&params, table, num);\r\nif (!r)\r\nr = find_reg(&params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\r\nif (r && !r->reg)\r\nr = NULL;\r\nreturn r;\r\n}\r\nstatic int reg_from_user(void *val, const void __user *uaddr, u64 id)\r\n{\r\nif (copy_from_user(val, uaddr, KVM_REG_SIZE(id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int reg_to_user(void __user *uaddr, const void *val, u64 id)\r\n{\r\nif (copy_to_user(uaddr, val, KVM_REG_SIZE(id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int get_invariant_sys_reg(u64 id, void __user *uaddr)\r\n{\r\nstruct sys_reg_params params;\r\nconst struct sys_reg_desc *r;\r\nif (!index_to_params(id, &params))\r\nreturn -ENOENT;\r\nr = find_reg(&params, invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs));\r\nif (!r)\r\nreturn -ENOENT;\r\nreturn reg_to_user(uaddr, &r->val, id);\r\n}\r\nstatic int set_invariant_sys_reg(u64 id, void __user *uaddr)\r\n{\r\nstruct sys_reg_params params;\r\nconst struct sys_reg_desc *r;\r\nint err;\r\nu64 val = 0;\r\nif (!index_to_params(id, &params))\r\nreturn -ENOENT;\r\nr = find_reg(&params, invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs));\r\nif (!r)\r\nreturn -ENOENT;\r\nerr = reg_from_user(&val, uaddr, id);\r\nif (err)\r\nreturn err;\r\nif (r->val != val)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic bool is_valid_cache(u32 val)\r\n{\r\nu32 level, ctype;\r\nif (val >= CSSELR_MAX)\r\nreturn -ENOENT;\r\nlevel = (val >> 1);\r\nctype = (cache_levels >> (level * 3)) & 7;\r\nswitch (ctype) {\r\ncase 0:\r\nreturn false;\r\ncase 1:\r\nreturn (val & 1);\r\ncase 2:\r\ncase 4:\r\nreturn !(val & 1);\r\ncase 3:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic int demux_c15_get(u64 id, void __user *uaddr)\r\n{\r\nu32 val;\r\nu32 __user *uval = uaddr;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\r\ncase KVM_REG_ARM_DEMUX_ID_CCSIDR:\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\r\n>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\r\nif (!is_valid_cache(val))\r\nreturn -ENOENT;\r\nreturn put_user(get_ccsidr(val), uval);\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nstatic int demux_c15_set(u64 id, void __user *uaddr)\r\n{\r\nu32 val, newval;\r\nu32 __user *uval = uaddr;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\r\ncase KVM_REG_ARM_DEMUX_ID_CCSIDR:\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\r\n>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\r\nif (!is_valid_cache(val))\r\nreturn -ENOENT;\r\nif (get_user(newval, uval))\r\nreturn -EFAULT;\r\nif (newval != get_ccsidr(val))\r\nreturn -EINVAL;\r\nreturn 0;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nint kvm_arm_sys_reg_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\r\n{\r\nconst struct sys_reg_desc *r;\r\nvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\r\nreturn demux_c15_get(reg->id, uaddr);\r\nif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\r\nreturn -ENOENT;\r\nr = index_to_sys_reg_desc(vcpu, reg->id);\r\nif (!r)\r\nreturn get_invariant_sys_reg(reg->id, uaddr);\r\nreturn reg_to_user(uaddr, &vcpu_sys_reg(vcpu, r->reg), reg->id);\r\n}\r\nint kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\r\n{\r\nconst struct sys_reg_desc *r;\r\nvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\r\nreturn demux_c15_set(reg->id, uaddr);\r\nif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\r\nreturn -ENOENT;\r\nr = index_to_sys_reg_desc(vcpu, reg->id);\r\nif (!r)\r\nreturn set_invariant_sys_reg(reg->id, uaddr);\r\nreturn reg_from_user(&vcpu_sys_reg(vcpu, r->reg), uaddr, reg->id);\r\n}\r\nstatic unsigned int num_demux_regs(void)\r\n{\r\nunsigned int i, count = 0;\r\nfor (i = 0; i < CSSELR_MAX; i++)\r\nif (is_valid_cache(i))\r\ncount++;\r\nreturn count;\r\n}\r\nstatic int write_demux_regids(u64 __user *uindices)\r\n{\r\nu64 val = KVM_REG_ARM | KVM_REG_SIZE_U32 | KVM_REG_ARM_DEMUX;\r\nunsigned int i;\r\nval |= KVM_REG_ARM_DEMUX_ID_CCSIDR;\r\nfor (i = 0; i < CSSELR_MAX; i++) {\r\nif (!is_valid_cache(i))\r\ncontinue;\r\nif (put_user(val | i, uindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nreturn 0;\r\n}\r\nstatic u64 sys_reg_to_index(const struct sys_reg_desc *reg)\r\n{\r\nreturn (KVM_REG_ARM64 | KVM_REG_SIZE_U64 |\r\nKVM_REG_ARM64_SYSREG |\r\n(reg->Op0 << KVM_REG_ARM64_SYSREG_OP0_SHIFT) |\r\n(reg->Op1 << KVM_REG_ARM64_SYSREG_OP1_SHIFT) |\r\n(reg->CRn << KVM_REG_ARM64_SYSREG_CRN_SHIFT) |\r\n(reg->CRm << KVM_REG_ARM64_SYSREG_CRM_SHIFT) |\r\n(reg->Op2 << KVM_REG_ARM64_SYSREG_OP2_SHIFT));\r\n}\r\nstatic bool copy_reg_to_user(const struct sys_reg_desc *reg, u64 __user **uind)\r\n{\r\nif (!*uind)\r\nreturn true;\r\nif (put_user(sys_reg_to_index(reg), *uind))\r\nreturn false;\r\n(*uind)++;\r\nreturn true;\r\n}\r\nstatic int walk_sys_regs(struct kvm_vcpu *vcpu, u64 __user *uind)\r\n{\r\nconst struct sys_reg_desc *i1, *i2, *end1, *end2;\r\nunsigned int total = 0;\r\nsize_t num;\r\ni1 = get_target_table(vcpu->arch.target, true, &num);\r\nend1 = i1 + num;\r\ni2 = sys_reg_descs;\r\nend2 = sys_reg_descs + ARRAY_SIZE(sys_reg_descs);\r\nBUG_ON(i1 == end1 || i2 == end2);\r\nwhile (i1 || i2) {\r\nint cmp = cmp_sys_reg(i1, i2);\r\nif (cmp <= 0) {\r\nif (i1->reg) {\r\nif (!copy_reg_to_user(i1, &uind))\r\nreturn -EFAULT;\r\ntotal++;\r\n}\r\n} else {\r\nif (i2->reg) {\r\nif (!copy_reg_to_user(i2, &uind))\r\nreturn -EFAULT;\r\ntotal++;\r\n}\r\n}\r\nif (cmp <= 0 && ++i1 == end1)\r\ni1 = NULL;\r\nif (cmp >= 0 && ++i2 == end2)\r\ni2 = NULL;\r\n}\r\nreturn total;\r\n}\r\nunsigned long kvm_arm_num_sys_reg_descs(struct kvm_vcpu *vcpu)\r\n{\r\nreturn ARRAY_SIZE(invariant_sys_regs)\r\n+ num_demux_regs()\r\n+ walk_sys_regs(vcpu, (u64 __user *)NULL);\r\n}\r\nint kvm_arm_copy_sys_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\r\n{\r\nunsigned int i;\r\nint err;\r\nfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++) {\r\nif (put_user(sys_reg_to_index(&invariant_sys_regs[i]), uindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nerr = walk_sys_regs(vcpu, uindices);\r\nif (err < 0)\r\nreturn err;\r\nuindices += err;\r\nreturn write_demux_regids(uindices);\r\n}\r\nvoid kvm_sys_reg_table_init(void)\r\n{\r\nunsigned int i;\r\nstruct sys_reg_desc clidr;\r\nfor (i = 1; i < ARRAY_SIZE(sys_reg_descs); i++)\r\nBUG_ON(cmp_sys_reg(&sys_reg_descs[i-1], &sys_reg_descs[i]) >= 0);\r\nfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++)\r\ninvariant_sys_regs[i].reset(NULL, &invariant_sys_regs[i]);\r\nget_clidr_el1(NULL, &clidr);\r\ncache_levels = clidr.val;\r\nfor (i = 0; i < 7; i++)\r\nif (((cache_levels >> (i*3)) & 7) == 0)\r\nbreak;\r\ncache_levels &= (1 << (i*3))-1;\r\n}\r\nvoid kvm_reset_sys_regs(struct kvm_vcpu *vcpu)\r\n{\r\nsize_t num;\r\nconst struct sys_reg_desc *table;\r\nmemset(&vcpu->arch.ctxt.sys_regs, 0x42, sizeof(vcpu->arch.ctxt.sys_regs));\r\nreset_sys_reg_descs(vcpu, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\r\ntable = get_target_table(vcpu->arch.target, true, &num);\r\nreset_sys_reg_descs(vcpu, table, num);\r\nfor (num = 1; num < NR_SYS_REGS; num++)\r\nif (vcpu_sys_reg(vcpu, num) == 0x4242424242424242)\r\npanic("Didn't reset vcpu_sys_reg(%zi)", num);\r\n}
