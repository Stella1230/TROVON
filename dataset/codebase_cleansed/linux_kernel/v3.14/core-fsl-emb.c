static inline int perf_intr_is_nmi(struct pt_regs *regs)\r\n{\r\n#ifdef __powerpc64__\r\nreturn !regs->softe;\r\n#else\r\nreturn 0;\r\n#endif\r\n}\r\nstatic unsigned long read_pmc(int idx)\r\n{\r\nunsigned long val;\r\nswitch (idx) {\r\ncase 0:\r\nval = mfpmr(PMRN_PMC0);\r\nbreak;\r\ncase 1:\r\nval = mfpmr(PMRN_PMC1);\r\nbreak;\r\ncase 2:\r\nval = mfpmr(PMRN_PMC2);\r\nbreak;\r\ncase 3:\r\nval = mfpmr(PMRN_PMC3);\r\nbreak;\r\ncase 4:\r\nval = mfpmr(PMRN_PMC4);\r\nbreak;\r\ncase 5:\r\nval = mfpmr(PMRN_PMC5);\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "oops trying to read PMC%d\n", idx);\r\nval = 0;\r\n}\r\nreturn val;\r\n}\r\nstatic void write_pmc(int idx, unsigned long val)\r\n{\r\nswitch (idx) {\r\ncase 0:\r\nmtpmr(PMRN_PMC0, val);\r\nbreak;\r\ncase 1:\r\nmtpmr(PMRN_PMC1, val);\r\nbreak;\r\ncase 2:\r\nmtpmr(PMRN_PMC2, val);\r\nbreak;\r\ncase 3:\r\nmtpmr(PMRN_PMC3, val);\r\nbreak;\r\ncase 4:\r\nmtpmr(PMRN_PMC4, val);\r\nbreak;\r\ncase 5:\r\nmtpmr(PMRN_PMC5, val);\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "oops trying to write PMC%d\n", idx);\r\n}\r\nisync();\r\n}\r\nstatic void write_pmlca(int idx, unsigned long val)\r\n{\r\nswitch (idx) {\r\ncase 0:\r\nmtpmr(PMRN_PMLCA0, val);\r\nbreak;\r\ncase 1:\r\nmtpmr(PMRN_PMLCA1, val);\r\nbreak;\r\ncase 2:\r\nmtpmr(PMRN_PMLCA2, val);\r\nbreak;\r\ncase 3:\r\nmtpmr(PMRN_PMLCA3, val);\r\nbreak;\r\ncase 4:\r\nmtpmr(PMRN_PMLCA4, val);\r\nbreak;\r\ncase 5:\r\nmtpmr(PMRN_PMLCA5, val);\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "oops trying to write PMLCA%d\n", idx);\r\n}\r\nisync();\r\n}\r\nstatic void write_pmlcb(int idx, unsigned long val)\r\n{\r\nswitch (idx) {\r\ncase 0:\r\nmtpmr(PMRN_PMLCB0, val);\r\nbreak;\r\ncase 1:\r\nmtpmr(PMRN_PMLCB1, val);\r\nbreak;\r\ncase 2:\r\nmtpmr(PMRN_PMLCB2, val);\r\nbreak;\r\ncase 3:\r\nmtpmr(PMRN_PMLCB3, val);\r\nbreak;\r\ncase 4:\r\nmtpmr(PMRN_PMLCB4, val);\r\nbreak;\r\ncase 5:\r\nmtpmr(PMRN_PMLCB5, val);\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "oops trying to write PMLCB%d\n", idx);\r\n}\r\nisync();\r\n}\r\nstatic void fsl_emb_pmu_read(struct perf_event *event)\r\n{\r\ns64 val, delta, prev;\r\nif (event->hw.state & PERF_HES_STOPPED)\r\nreturn;\r\ndo {\r\nprev = local64_read(&event->hw.prev_count);\r\nbarrier();\r\nval = read_pmc(event->hw.idx);\r\n} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\r\ndelta = (val - prev) & 0xfffffffful;\r\nlocal64_add(delta, &event->count);\r\nlocal64_sub(delta, &event->hw.period_left);\r\n}\r\nstatic void fsl_emb_pmu_disable(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\nif (!cpuhw->disabled) {\r\ncpuhw->disabled = 1;\r\nif (!cpuhw->pmcs_enabled) {\r\nppc_enable_pmcs();\r\ncpuhw->pmcs_enabled = 1;\r\n}\r\nif (atomic_read(&num_events)) {\r\nmtpmr(PMRN_PMGC0, PMGC0_FAC);\r\nisync();\r\n}\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void fsl_emb_pmu_enable(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\nif (!cpuhw->disabled)\r\ngoto out;\r\ncpuhw->disabled = 0;\r\nppc_set_pmu_inuse(cpuhw->n_events != 0);\r\nif (cpuhw->n_events > 0) {\r\nmtpmr(PMRN_PMGC0, PMGC0_PMIE | PMGC0_FCECE);\r\nisync();\r\n}\r\nout:\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int collect_events(struct perf_event *group, int max_count,\r\nstruct perf_event *ctrs[])\r\n{\r\nint n = 0;\r\nstruct perf_event *event;\r\nif (!is_software_event(group)) {\r\nif (n >= max_count)\r\nreturn -1;\r\nctrs[n] = group;\r\nn++;\r\n}\r\nlist_for_each_entry(event, &group->sibling_list, group_entry) {\r\nif (!is_software_event(event) &&\r\nevent->state != PERF_EVENT_STATE_OFF) {\r\nif (n >= max_count)\r\nreturn -1;\r\nctrs[n] = event;\r\nn++;\r\n}\r\n}\r\nreturn n;\r\n}\r\nstatic int fsl_emb_pmu_add(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nint ret = -EAGAIN;\r\nint num_counters = ppmu->n_counter;\r\nu64 val;\r\nint i;\r\nperf_pmu_disable(event->pmu);\r\ncpuhw = &get_cpu_var(cpu_hw_events);\r\nif (event->hw.config & FSL_EMB_EVENT_RESTRICTED)\r\nnum_counters = ppmu->n_restricted;\r\nfor (i = num_counters - 1; i >= 0; i--) {\r\nif (cpuhw->event[i])\r\ncontinue;\r\nbreak;\r\n}\r\nif (i < 0)\r\ngoto out;\r\nevent->hw.idx = i;\r\ncpuhw->event[i] = event;\r\n++cpuhw->n_events;\r\nval = 0;\r\nif (event->hw.sample_period) {\r\ns64 left = local64_read(&event->hw.period_left);\r\nif (left < 0x80000000L)\r\nval = 0x80000000L - left;\r\n}\r\nlocal64_set(&event->hw.prev_count, val);\r\nif (!(flags & PERF_EF_START)) {\r\nevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nval = 0;\r\n}\r\nwrite_pmc(i, val);\r\nperf_event_update_userpage(event);\r\nwrite_pmlcb(i, event->hw.config >> 32);\r\nwrite_pmlca(i, event->hw.config_base);\r\nret = 0;\r\nout:\r\nput_cpu_var(cpu_hw_events);\r\nperf_pmu_enable(event->pmu);\r\nreturn ret;\r\n}\r\nstatic void fsl_emb_pmu_del(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nint i = event->hw.idx;\r\nperf_pmu_disable(event->pmu);\r\nif (i < 0)\r\ngoto out;\r\nfsl_emb_pmu_read(event);\r\ncpuhw = &get_cpu_var(cpu_hw_events);\r\nWARN_ON(event != cpuhw->event[event->hw.idx]);\r\nwrite_pmlca(i, 0);\r\nwrite_pmlcb(i, 0);\r\nwrite_pmc(i, 0);\r\ncpuhw->event[i] = NULL;\r\nevent->hw.idx = -1;\r\ncpuhw->n_events--;\r\nout:\r\nperf_pmu_enable(event->pmu);\r\nput_cpu_var(cpu_hw_events);\r\n}\r\nstatic void fsl_emb_pmu_start(struct perf_event *event, int ef_flags)\r\n{\r\nunsigned long flags;\r\ns64 left;\r\nif (event->hw.idx < 0 || !event->hw.sample_period)\r\nreturn;\r\nif (!(event->hw.state & PERF_HES_STOPPED))\r\nreturn;\r\nif (ef_flags & PERF_EF_RELOAD)\r\nWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\nevent->hw.state = 0;\r\nleft = local64_read(&event->hw.period_left);\r\nwrite_pmc(event->hw.idx, left);\r\nperf_event_update_userpage(event);\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void fsl_emb_pmu_stop(struct perf_event *event, int ef_flags)\r\n{\r\nunsigned long flags;\r\nif (event->hw.idx < 0 || !event->hw.sample_period)\r\nreturn;\r\nif (event->hw.state & PERF_HES_STOPPED)\r\nreturn;\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\nfsl_emb_pmu_read(event);\r\nevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nwrite_pmc(event->hw.idx, 0);\r\nperf_event_update_userpage(event);\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void hw_perf_event_destroy(struct perf_event *event)\r\n{\r\nif (!atomic_add_unless(&num_events, -1, 1)) {\r\nmutex_lock(&pmc_reserve_mutex);\r\nif (atomic_dec_return(&num_events) == 0)\r\nrelease_pmc_hardware();\r\nmutex_unlock(&pmc_reserve_mutex);\r\n}\r\n}\r\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\r\n{\r\nunsigned long type, op, result;\r\nint ev;\r\nif (!ppmu->cache_events)\r\nreturn -EINVAL;\r\ntype = config & 0xff;\r\nop = (config >> 8) & 0xff;\r\nresult = (config >> 16) & 0xff;\r\nif (type >= PERF_COUNT_HW_CACHE_MAX ||\r\nop >= PERF_COUNT_HW_CACHE_OP_MAX ||\r\nresult >= PERF_COUNT_HW_CACHE_RESULT_MAX)\r\nreturn -EINVAL;\r\nev = (*ppmu->cache_events)[type][op][result];\r\nif (ev == 0)\r\nreturn -EOPNOTSUPP;\r\nif (ev == -1)\r\nreturn -EINVAL;\r\n*eventp = ev;\r\nreturn 0;\r\n}\r\nstatic int fsl_emb_pmu_event_init(struct perf_event *event)\r\n{\r\nu64 ev;\r\nstruct perf_event *events[MAX_HWEVENTS];\r\nint n;\r\nint err;\r\nint num_restricted;\r\nint i;\r\nif (ppmu->n_counter > MAX_HWEVENTS) {\r\nWARN(1, "No. of perf counters (%d) is higher than max array size(%d)\n",\r\nppmu->n_counter, MAX_HWEVENTS);\r\nppmu->n_counter = MAX_HWEVENTS;\r\n}\r\nswitch (event->attr.type) {\r\ncase PERF_TYPE_HARDWARE:\r\nev = event->attr.config;\r\nif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\r\nreturn -EOPNOTSUPP;\r\nev = ppmu->generic_events[ev];\r\nbreak;\r\ncase PERF_TYPE_HW_CACHE:\r\nerr = hw_perf_cache_event(event->attr.config, &ev);\r\nif (err)\r\nreturn err;\r\nbreak;\r\ncase PERF_TYPE_RAW:\r\nev = event->attr.config;\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nevent->hw.config = ppmu->xlate_event(ev);\r\nif (!(event->hw.config & FSL_EMB_EVENT_VALID))\r\nreturn -EINVAL;\r\nn = 0;\r\nif (event->group_leader != event) {\r\nn = collect_events(event->group_leader,\r\nppmu->n_counter - 1, events);\r\nif (n < 0)\r\nreturn -EINVAL;\r\n}\r\nif (event->hw.config & FSL_EMB_EVENT_RESTRICTED) {\r\nnum_restricted = 0;\r\nfor (i = 0; i < n; i++) {\r\nif (events[i]->hw.config & FSL_EMB_EVENT_RESTRICTED)\r\nnum_restricted++;\r\n}\r\nif (num_restricted >= ppmu->n_restricted)\r\nreturn -EINVAL;\r\n}\r\nevent->hw.idx = -1;\r\nevent->hw.config_base = PMLCA_CE | PMLCA_FCM1 |\r\n(u32)((ev << 16) & PMLCA_EVENT_MASK);\r\nif (event->attr.exclude_user)\r\nevent->hw.config_base |= PMLCA_FCU;\r\nif (event->attr.exclude_kernel)\r\nevent->hw.config_base |= PMLCA_FCS;\r\nif (event->attr.exclude_idle)\r\nreturn -ENOTSUPP;\r\nevent->hw.last_period = event->hw.sample_period;\r\nlocal64_set(&event->hw.period_left, event->hw.last_period);\r\nerr = 0;\r\nif (!atomic_inc_not_zero(&num_events)) {\r\nmutex_lock(&pmc_reserve_mutex);\r\nif (atomic_read(&num_events) == 0 &&\r\nreserve_pmc_hardware(perf_event_interrupt))\r\nerr = -EBUSY;\r\nelse\r\natomic_inc(&num_events);\r\nmutex_unlock(&pmc_reserve_mutex);\r\nmtpmr(PMRN_PMGC0, PMGC0_FAC);\r\nisync();\r\n}\r\nevent->destroy = hw_perf_event_destroy;\r\nreturn err;\r\n}\r\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\r\nstruct pt_regs *regs)\r\n{\r\nu64 period = event->hw.sample_period;\r\ns64 prev, delta, left;\r\nint record = 0;\r\nif (event->hw.state & PERF_HES_STOPPED) {\r\nwrite_pmc(event->hw.idx, 0);\r\nreturn;\r\n}\r\nprev = local64_read(&event->hw.prev_count);\r\ndelta = (val - prev) & 0xfffffffful;\r\nlocal64_add(delta, &event->count);\r\nval = 0;\r\nleft = local64_read(&event->hw.period_left) - delta;\r\nif (period) {\r\nif (left <= 0) {\r\nleft += period;\r\nif (left <= 0)\r\nleft = period;\r\nrecord = 1;\r\nevent->hw.last_period = event->hw.sample_period;\r\n}\r\nif (left < 0x80000000LL)\r\nval = 0x80000000LL - left;\r\n}\r\nwrite_pmc(event->hw.idx, val);\r\nlocal64_set(&event->hw.prev_count, val);\r\nlocal64_set(&event->hw.period_left, left);\r\nperf_event_update_userpage(event);\r\nif (record) {\r\nstruct perf_sample_data data;\r\nperf_sample_data_init(&data, 0, event->hw.last_period);\r\nif (perf_event_overflow(event, &data, regs))\r\nfsl_emb_pmu_stop(event, 0);\r\n}\r\n}\r\nstatic void perf_event_interrupt(struct pt_regs *regs)\r\n{\r\nint i;\r\nstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\r\nstruct perf_event *event;\r\nunsigned long val;\r\nint found = 0;\r\nint nmi;\r\nnmi = perf_intr_is_nmi(regs);\r\nif (nmi)\r\nnmi_enter();\r\nelse\r\nirq_enter();\r\nfor (i = 0; i < ppmu->n_counter; ++i) {\r\nevent = cpuhw->event[i];\r\nval = read_pmc(i);\r\nif ((int)val < 0) {\r\nif (event) {\r\nfound = 1;\r\nrecord_and_restart(event, val, regs);\r\n} else {\r\nwrite_pmc(i, 0);\r\n}\r\n}\r\n}\r\nmtmsr(mfmsr() | MSR_PMM);\r\nmtpmr(PMRN_PMGC0, PMGC0_PMIE | PMGC0_FCECE);\r\nisync();\r\nif (nmi)\r\nnmi_exit();\r\nelse\r\nirq_exit();\r\n}\r\nvoid hw_perf_event_setup(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\r\nmemset(cpuhw, 0, sizeof(*cpuhw));\r\n}\r\nint register_fsl_emb_pmu(struct fsl_emb_pmu *pmu)\r\n{\r\nif (ppmu)\r\nreturn -EBUSY;\r\nppmu = pmu;\r\npr_info("%s performance monitor hardware support registered\n",\r\npmu->name);\r\nperf_pmu_register(&fsl_emb_pmu, "cpu", PERF_TYPE_RAW);\r\nreturn 0;\r\n}
