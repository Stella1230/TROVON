static inline u32\r\nxscale1pmu_read_pmnc(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p14, 0, %0, c0, c0, 0" : "=r" (val));\r\nreturn val;\r\n}\r\nstatic inline void\r\nxscale1pmu_write_pmnc(u32 val)\r\n{\r\nval &= 0xffff77f;\r\nasm volatile("mcr p14, 0, %0, c0, c0, 0" : : "r" (val));\r\n}\r\nstatic inline int\r\nxscale1_pmnc_counter_has_overflowed(unsigned long pmnc,\r\nenum xscale_counters counter)\r\n{\r\nint ret = 0;\r\nswitch (counter) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nret = pmnc & XSCALE1_CCOUNT_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nret = pmnc & XSCALE1_COUNT0_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nret = pmnc & XSCALE1_COUNT1_OVERFLOW;\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "invalid counter number (%d)\n", counter);\r\n}\r\nreturn ret;\r\n}\r\nstatic irqreturn_t\r\nxscale1pmu_handle_irq(int irq_num, void *dev)\r\n{\r\nunsigned long pmnc;\r\nstruct perf_sample_data data;\r\nstruct arm_pmu *cpu_pmu = (struct arm_pmu *)dev;\r\nstruct pmu_hw_events *cpuc = cpu_pmu->get_hw_events();\r\nstruct pt_regs *regs;\r\nint idx;\r\npmnc = xscale1pmu_read_pmnc();\r\nxscale1pmu_write_pmnc(pmnc & ~XSCALE_PMU_ENABLE);\r\nif (!(pmnc & XSCALE1_OVERFLOWED_MASK))\r\nreturn IRQ_NONE;\r\nregs = get_irq_regs();\r\nfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\r\nstruct perf_event *event = cpuc->events[idx];\r\nstruct hw_perf_event *hwc;\r\nif (!event)\r\ncontinue;\r\nif (!xscale1_pmnc_counter_has_overflowed(pmnc, idx))\r\ncontinue;\r\nhwc = &event->hw;\r\narmpmu_event_update(event);\r\nperf_sample_data_init(&data, 0, hwc->last_period);\r\nif (!armpmu_event_set_period(event))\r\ncontinue;\r\nif (perf_event_overflow(event, &data, regs))\r\ncpu_pmu->disable(event);\r\n}\r\nirq_work_run();\r\npmnc = xscale1pmu_read_pmnc() | XSCALE_PMU_ENABLE;\r\nxscale1pmu_write_pmnc(pmnc);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void xscale1pmu_enable_event(struct perf_event *event)\r\n{\r\nunsigned long val, mask, evt, flags;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nswitch (idx) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nmask = 0;\r\nevt = XSCALE1_CCOUNT_INT_EN;\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nmask = XSCALE1_COUNT0_EVT_MASK;\r\nevt = (hwc->config_base << XSCALE1_COUNT0_EVT_SHFT) |\r\nXSCALE1_COUNT0_INT_EN;\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nmask = XSCALE1_COUNT1_EVT_MASK;\r\nevt = (hwc->config_base << XSCALE1_COUNT1_EVT_SHFT) |\r\nXSCALE1_COUNT1_INT_EN;\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "invalid counter number (%d)\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = xscale1pmu_read_pmnc();\r\nval &= ~mask;\r\nval |= evt;\r\nxscale1pmu_write_pmnc(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void xscale1pmu_disable_event(struct perf_event *event)\r\n{\r\nunsigned long val, mask, evt, flags;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nswitch (idx) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nmask = XSCALE1_CCOUNT_INT_EN;\r\nevt = 0;\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nmask = XSCALE1_COUNT0_INT_EN | XSCALE1_COUNT0_EVT_MASK;\r\nevt = XSCALE_PERFCTR_UNUSED << XSCALE1_COUNT0_EVT_SHFT;\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nmask = XSCALE1_COUNT1_INT_EN | XSCALE1_COUNT1_EVT_MASK;\r\nevt = XSCALE_PERFCTR_UNUSED << XSCALE1_COUNT1_EVT_SHFT;\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "invalid counter number (%d)\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = xscale1pmu_read_pmnc();\r\nval &= ~mask;\r\nval |= evt;\r\nxscale1pmu_write_pmnc(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic int\r\nxscale1pmu_get_event_idx(struct pmu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (XSCALE_PERFCTR_CCNT == hwc->config_base) {\r\nif (test_and_set_bit(XSCALE_CYCLE_COUNTER, cpuc->used_mask))\r\nreturn -EAGAIN;\r\nreturn XSCALE_CYCLE_COUNTER;\r\n} else {\r\nif (!test_and_set_bit(XSCALE_COUNTER1, cpuc->used_mask))\r\nreturn XSCALE_COUNTER1;\r\nif (!test_and_set_bit(XSCALE_COUNTER0, cpuc->used_mask))\r\nreturn XSCALE_COUNTER0;\r\nreturn -EAGAIN;\r\n}\r\n}\r\nstatic void xscale1pmu_start(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags, val;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = xscale1pmu_read_pmnc();\r\nval |= XSCALE_PMU_ENABLE;\r\nxscale1pmu_write_pmnc(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void xscale1pmu_stop(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags, val;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = xscale1pmu_read_pmnc();\r\nval &= ~XSCALE_PMU_ENABLE;\r\nxscale1pmu_write_pmnc(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic inline u32 xscale1pmu_read_counter(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint counter = hwc->idx;\r\nu32 val = 0;\r\nswitch (counter) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nasm volatile("mrc p14, 0, %0, c1, c0, 0" : "=r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nasm volatile("mrc p14, 0, %0, c2, c0, 0" : "=r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nasm volatile("mrc p14, 0, %0, c3, c0, 0" : "=r" (val));\r\nbreak;\r\n}\r\nreturn val;\r\n}\r\nstatic inline void xscale1pmu_write_counter(struct perf_event *event, u32 val)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint counter = hwc->idx;\r\nswitch (counter) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nasm volatile("mcr p14, 0, %0, c1, c0, 0" : : "r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nasm volatile("mcr p14, 0, %0, c2, c0, 0" : : "r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nasm volatile("mcr p14, 0, %0, c3, c0, 0" : : "r" (val));\r\nbreak;\r\n}\r\n}\r\nstatic int xscale_map_event(struct perf_event *event)\r\n{\r\nreturn armpmu_map_event(event, &xscale_perf_map,\r\n&xscale_perf_cache_map, 0xFF);\r\n}\r\nstatic int xscale1pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\ncpu_pmu->name = "xscale1";\r\ncpu_pmu->handle_irq = xscale1pmu_handle_irq;\r\ncpu_pmu->enable = xscale1pmu_enable_event;\r\ncpu_pmu->disable = xscale1pmu_disable_event;\r\ncpu_pmu->read_counter = xscale1pmu_read_counter;\r\ncpu_pmu->write_counter = xscale1pmu_write_counter;\r\ncpu_pmu->get_event_idx = xscale1pmu_get_event_idx;\r\ncpu_pmu->start = xscale1pmu_start;\r\ncpu_pmu->stop = xscale1pmu_stop;\r\ncpu_pmu->map_event = xscale_map_event;\r\ncpu_pmu->num_events = 3;\r\ncpu_pmu->max_period = (1LLU << 32) - 1;\r\nreturn 0;\r\n}\r\nstatic inline u32\r\nxscale2pmu_read_pmnc(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p14, 0, %0, c0, c1, 0" : "=r" (val));\r\nreturn val & 0xff000009;\r\n}\r\nstatic inline void\r\nxscale2pmu_write_pmnc(u32 val)\r\n{\r\nval &= 0xf;\r\nasm volatile("mcr p14, 0, %0, c0, c1, 0" : : "r" (val));\r\n}\r\nstatic inline u32\r\nxscale2pmu_read_overflow_flags(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p14, 0, %0, c5, c1, 0" : "=r" (val));\r\nreturn val;\r\n}\r\nstatic inline void\r\nxscale2pmu_write_overflow_flags(u32 val)\r\n{\r\nasm volatile("mcr p14, 0, %0, c5, c1, 0" : : "r" (val));\r\n}\r\nstatic inline u32\r\nxscale2pmu_read_event_select(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p14, 0, %0, c8, c1, 0" : "=r" (val));\r\nreturn val;\r\n}\r\nstatic inline void\r\nxscale2pmu_write_event_select(u32 val)\r\n{\r\nasm volatile("mcr p14, 0, %0, c8, c1, 0" : : "r"(val));\r\n}\r\nstatic inline u32\r\nxscale2pmu_read_int_enable(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p14, 0, %0, c4, c1, 0" : "=r" (val));\r\nreturn val;\r\n}\r\nstatic void\r\nxscale2pmu_write_int_enable(u32 val)\r\n{\r\nasm volatile("mcr p14, 0, %0, c4, c1, 0" : : "r" (val));\r\n}\r\nstatic inline int\r\nxscale2_pmnc_counter_has_overflowed(unsigned long of_flags,\r\nenum xscale_counters counter)\r\n{\r\nint ret = 0;\r\nswitch (counter) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nret = of_flags & XSCALE2_CCOUNT_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nret = of_flags & XSCALE2_COUNT0_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nret = of_flags & XSCALE2_COUNT1_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER2:\r\nret = of_flags & XSCALE2_COUNT2_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER3:\r\nret = of_flags & XSCALE2_COUNT3_OVERFLOW;\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "invalid counter number (%d)\n", counter);\r\n}\r\nreturn ret;\r\n}\r\nstatic irqreturn_t\r\nxscale2pmu_handle_irq(int irq_num, void *dev)\r\n{\r\nunsigned long pmnc, of_flags;\r\nstruct perf_sample_data data;\r\nstruct arm_pmu *cpu_pmu = (struct arm_pmu *)dev;\r\nstruct pmu_hw_events *cpuc = cpu_pmu->get_hw_events();\r\nstruct pt_regs *regs;\r\nint idx;\r\npmnc = xscale2pmu_read_pmnc();\r\nxscale2pmu_write_pmnc(pmnc & ~XSCALE_PMU_ENABLE);\r\nof_flags = xscale2pmu_read_overflow_flags();\r\nif (!(of_flags & XSCALE2_OVERFLOWED_MASK))\r\nreturn IRQ_NONE;\r\nxscale2pmu_write_overflow_flags(of_flags);\r\nregs = get_irq_regs();\r\nfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\r\nstruct perf_event *event = cpuc->events[idx];\r\nstruct hw_perf_event *hwc;\r\nif (!event)\r\ncontinue;\r\nif (!xscale2_pmnc_counter_has_overflowed(of_flags, idx))\r\ncontinue;\r\nhwc = &event->hw;\r\narmpmu_event_update(event);\r\nperf_sample_data_init(&data, 0, hwc->last_period);\r\nif (!armpmu_event_set_period(event))\r\ncontinue;\r\nif (perf_event_overflow(event, &data, regs))\r\ncpu_pmu->disable(event);\r\n}\r\nirq_work_run();\r\npmnc = xscale2pmu_read_pmnc() | XSCALE_PMU_ENABLE;\r\nxscale2pmu_write_pmnc(pmnc);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void xscale2pmu_enable_event(struct perf_event *event)\r\n{\r\nunsigned long flags, ien, evtsel;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nien = xscale2pmu_read_int_enable();\r\nevtsel = xscale2pmu_read_event_select();\r\nswitch (idx) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nien |= XSCALE2_CCOUNT_INT_EN;\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nien |= XSCALE2_COUNT0_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT0_EVT_MASK;\r\nevtsel |= hwc->config_base << XSCALE2_COUNT0_EVT_SHFT;\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nien |= XSCALE2_COUNT1_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT1_EVT_MASK;\r\nevtsel |= hwc->config_base << XSCALE2_COUNT1_EVT_SHFT;\r\nbreak;\r\ncase XSCALE_COUNTER2:\r\nien |= XSCALE2_COUNT2_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT2_EVT_MASK;\r\nevtsel |= hwc->config_base << XSCALE2_COUNT2_EVT_SHFT;\r\nbreak;\r\ncase XSCALE_COUNTER3:\r\nien |= XSCALE2_COUNT3_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT3_EVT_MASK;\r\nevtsel |= hwc->config_base << XSCALE2_COUNT3_EVT_SHFT;\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "invalid counter number (%d)\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nxscale2pmu_write_event_select(evtsel);\r\nxscale2pmu_write_int_enable(ien);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void xscale2pmu_disable_event(struct perf_event *event)\r\n{\r\nunsigned long flags, ien, evtsel, of_flags;\r\nstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nint idx = hwc->idx;\r\nien = xscale2pmu_read_int_enable();\r\nevtsel = xscale2pmu_read_event_select();\r\nswitch (idx) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nien &= ~XSCALE2_CCOUNT_INT_EN;\r\nof_flags = XSCALE2_CCOUNT_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nien &= ~XSCALE2_COUNT0_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT0_EVT_MASK;\r\nevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT0_EVT_SHFT;\r\nof_flags = XSCALE2_COUNT0_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nien &= ~XSCALE2_COUNT1_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT1_EVT_MASK;\r\nevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT1_EVT_SHFT;\r\nof_flags = XSCALE2_COUNT1_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER2:\r\nien &= ~XSCALE2_COUNT2_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT2_EVT_MASK;\r\nevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT2_EVT_SHFT;\r\nof_flags = XSCALE2_COUNT2_OVERFLOW;\r\nbreak;\r\ncase XSCALE_COUNTER3:\r\nien &= ~XSCALE2_COUNT3_INT_EN;\r\nevtsel &= ~XSCALE2_COUNT3_EVT_MASK;\r\nevtsel |= XSCALE_PERFCTR_UNUSED << XSCALE2_COUNT3_EVT_SHFT;\r\nof_flags = XSCALE2_COUNT3_OVERFLOW;\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "invalid counter number (%d)\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nxscale2pmu_write_event_select(evtsel);\r\nxscale2pmu_write_int_enable(ien);\r\nxscale2pmu_write_overflow_flags(of_flags);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic int\r\nxscale2pmu_get_event_idx(struct pmu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nint idx = xscale1pmu_get_event_idx(cpuc, event);\r\nif (idx >= 0)\r\ngoto out;\r\nif (!test_and_set_bit(XSCALE_COUNTER3, cpuc->used_mask))\r\nidx = XSCALE_COUNTER3;\r\nelse if (!test_and_set_bit(XSCALE_COUNTER2, cpuc->used_mask))\r\nidx = XSCALE_COUNTER2;\r\nout:\r\nreturn idx;\r\n}\r\nstatic void xscale2pmu_start(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags, val;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = xscale2pmu_read_pmnc() & ~XSCALE_PMU_CNT64;\r\nval |= XSCALE_PMU_ENABLE;\r\nxscale2pmu_write_pmnc(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void xscale2pmu_stop(struct arm_pmu *cpu_pmu)\r\n{\r\nunsigned long flags, val;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = xscale2pmu_read_pmnc();\r\nval &= ~XSCALE_PMU_ENABLE;\r\nxscale2pmu_write_pmnc(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic inline u32 xscale2pmu_read_counter(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint counter = hwc->idx;\r\nu32 val = 0;\r\nswitch (counter) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nasm volatile("mrc p14, 0, %0, c1, c1, 0" : "=r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nasm volatile("mrc p14, 0, %0, c0, c2, 0" : "=r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nasm volatile("mrc p14, 0, %0, c1, c2, 0" : "=r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER2:\r\nasm volatile("mrc p14, 0, %0, c2, c2, 0" : "=r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER3:\r\nasm volatile("mrc p14, 0, %0, c3, c2, 0" : "=r" (val));\r\nbreak;\r\n}\r\nreturn val;\r\n}\r\nstatic inline void xscale2pmu_write_counter(struct perf_event *event, u32 val)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint counter = hwc->idx;\r\nswitch (counter) {\r\ncase XSCALE_CYCLE_COUNTER:\r\nasm volatile("mcr p14, 0, %0, c1, c1, 0" : : "r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER0:\r\nasm volatile("mcr p14, 0, %0, c0, c2, 0" : : "r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER1:\r\nasm volatile("mcr p14, 0, %0, c1, c2, 0" : : "r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER2:\r\nasm volatile("mcr p14, 0, %0, c2, c2, 0" : : "r" (val));\r\nbreak;\r\ncase XSCALE_COUNTER3:\r\nasm volatile("mcr p14, 0, %0, c3, c2, 0" : : "r" (val));\r\nbreak;\r\n}\r\n}\r\nstatic int xscale2pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\ncpu_pmu->name = "xscale2";\r\ncpu_pmu->handle_irq = xscale2pmu_handle_irq;\r\ncpu_pmu->enable = xscale2pmu_enable_event;\r\ncpu_pmu->disable = xscale2pmu_disable_event;\r\ncpu_pmu->read_counter = xscale2pmu_read_counter;\r\ncpu_pmu->write_counter = xscale2pmu_write_counter;\r\ncpu_pmu->get_event_idx = xscale2pmu_get_event_idx;\r\ncpu_pmu->start = xscale2pmu_start;\r\ncpu_pmu->stop = xscale2pmu_stop;\r\ncpu_pmu->map_event = xscale_map_event;\r\ncpu_pmu->num_events = 5;\r\ncpu_pmu->max_period = (1LLU << 32) - 1;\r\nreturn 0;\r\n}\r\nstatic inline int xscale1pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}\r\nstatic inline int xscale2pmu_init(struct arm_pmu *cpu_pmu)\r\n{\r\nreturn -ENODEV;\r\n}
