static void vfio_lock_acct_bg(struct work_struct *work)\r\n{\r\nstruct vwork *vwork = container_of(work, struct vwork, work);\r\nstruct mm_struct *mm;\r\nmm = vwork->mm;\r\ndown_write(&mm->mmap_sem);\r\nmm->locked_vm += vwork->npage;\r\nup_write(&mm->mmap_sem);\r\nmmput(mm);\r\nkfree(vwork);\r\n}\r\nstatic void vfio_lock_acct(long npage)\r\n{\r\nstruct vwork *vwork;\r\nstruct mm_struct *mm;\r\nif (!current->mm)\r\nreturn;\r\nif (down_write_trylock(&current->mm->mmap_sem)) {\r\ncurrent->mm->locked_vm += npage;\r\nup_write(&current->mm->mmap_sem);\r\nreturn;\r\n}\r\nvwork = kmalloc(sizeof(struct vwork), GFP_KERNEL);\r\nif (!vwork)\r\nreturn;\r\nmm = get_task_mm(current);\r\nif (!mm) {\r\nkfree(vwork);\r\nreturn;\r\n}\r\nINIT_WORK(&vwork->work, vfio_lock_acct_bg);\r\nvwork->mm = mm;\r\nvwork->npage = npage;\r\nschedule_work(&vwork->work);\r\n}\r\nstatic bool is_invalid_reserved_pfn(unsigned long pfn)\r\n{\r\nif (pfn_valid(pfn)) {\r\nbool reserved;\r\nstruct page *tail = pfn_to_page(pfn);\r\nstruct page *head = compound_trans_head(tail);\r\nreserved = !!(PageReserved(head));\r\nif (head != tail) {\r\nsmp_rmb();\r\nif (PageTail(tail))\r\nreturn reserved;\r\n}\r\nreturn PageReserved(tail);\r\n}\r\nreturn true;\r\n}\r\nstatic int put_pfn(unsigned long pfn, int prot)\r\n{\r\nif (!is_invalid_reserved_pfn(pfn)) {\r\nstruct page *page = pfn_to_page(pfn);\r\nif (prot & IOMMU_WRITE)\r\nSetPageDirty(page);\r\nput_page(page);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic long __vfio_dma_do_unmap(struct vfio_iommu *iommu, dma_addr_t iova,\r\nlong npage, int prot)\r\n{\r\nlong i, unlocked = 0;\r\nfor (i = 0; i < npage; i++, iova += PAGE_SIZE) {\r\nunsigned long pfn;\r\npfn = iommu_iova_to_phys(iommu->domain, iova) >> PAGE_SHIFT;\r\nif (pfn) {\r\niommu_unmap(iommu->domain, iova, PAGE_SIZE);\r\nunlocked += put_pfn(pfn, prot);\r\n}\r\n}\r\nreturn unlocked;\r\n}\r\nstatic void vfio_dma_unmap(struct vfio_iommu *iommu, dma_addr_t iova,\r\nlong npage, int prot)\r\n{\r\nlong unlocked;\r\nunlocked = __vfio_dma_do_unmap(iommu, iova, npage, prot);\r\nvfio_lock_acct(-unlocked);\r\n}\r\nstatic int vaddr_get_pfn(unsigned long vaddr, int prot, unsigned long *pfn)\r\n{\r\nstruct page *page[1];\r\nstruct vm_area_struct *vma;\r\nint ret = -EFAULT;\r\nif (get_user_pages_fast(vaddr, 1, !!(prot & IOMMU_WRITE), page) == 1) {\r\n*pfn = page_to_pfn(page[0]);\r\nreturn 0;\r\n}\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma_intersection(current->mm, vaddr, vaddr + 1);\r\nif (vma && vma->vm_flags & VM_PFNMAP) {\r\n*pfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nif (is_invalid_reserved_pfn(*pfn))\r\nret = 0;\r\n}\r\nup_read(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic int __vfio_dma_map(struct vfio_iommu *iommu, dma_addr_t iova,\r\nunsigned long vaddr, long npage, int prot)\r\n{\r\ndma_addr_t start = iova;\r\nlong i, locked = 0;\r\nint ret;\r\nfor (i = 0; i < npage; i++, iova += PAGE_SIZE)\r\nif (iommu_iova_to_phys(iommu->domain, iova))\r\nreturn -EBUSY;\r\niova = start;\r\nif (iommu->cache)\r\nprot |= IOMMU_CACHE;\r\nfor (i = 0; i < npage; i++, iova += PAGE_SIZE, vaddr += PAGE_SIZE) {\r\nunsigned long pfn = 0;\r\nret = vaddr_get_pfn(vaddr, prot, &pfn);\r\nif (ret) {\r\n__vfio_dma_do_unmap(iommu, start, i, prot);\r\nreturn ret;\r\n}\r\nif (!is_invalid_reserved_pfn(pfn))\r\nlocked++;\r\nret = iommu_map(iommu->domain, iova,\r\n(phys_addr_t)pfn << PAGE_SHIFT,\r\nPAGE_SIZE, prot);\r\nif (ret) {\r\nput_pfn(pfn, prot);\r\n__vfio_dma_do_unmap(iommu, start, i, prot);\r\nreturn ret;\r\n}\r\n}\r\nvfio_lock_acct(locked);\r\nreturn 0;\r\n}\r\nstatic inline bool ranges_overlap(dma_addr_t start1, size_t size1,\r\ndma_addr_t start2, size_t size2)\r\n{\r\nif (start1 < start2)\r\nreturn (start2 - start1 < size1);\r\nelse if (start2 < start1)\r\nreturn (start1 - start2 < size2);\r\nreturn (size1 > 0 && size2 > 0);\r\n}\r\nstatic struct vfio_dma *vfio_find_dma(struct vfio_iommu *iommu,\r\ndma_addr_t start, size_t size)\r\n{\r\nstruct vfio_dma *dma;\r\nlist_for_each_entry(dma, &iommu->dma_list, next) {\r\nif (ranges_overlap(dma->iova, NPAGE_TO_SIZE(dma->npage),\r\nstart, size))\r\nreturn dma;\r\n}\r\nreturn NULL;\r\n}\r\nstatic long vfio_remove_dma_overlap(struct vfio_iommu *iommu, dma_addr_t start,\r\nsize_t size, struct vfio_dma *dma)\r\n{\r\nstruct vfio_dma *split;\r\nlong npage_lo, npage_hi;\r\nif (start <= dma->iova &&\r\nstart + size >= dma->iova + NPAGE_TO_SIZE(dma->npage)) {\r\nvfio_dma_unmap(iommu, dma->iova, dma->npage, dma->prot);\r\nlist_del(&dma->next);\r\nnpage_lo = dma->npage;\r\nkfree(dma);\r\nreturn npage_lo;\r\n}\r\nif (start <= dma->iova) {\r\nsize_t overlap;\r\noverlap = start + size - dma->iova;\r\nnpage_lo = overlap >> PAGE_SHIFT;\r\nvfio_dma_unmap(iommu, dma->iova, npage_lo, dma->prot);\r\ndma->iova += overlap;\r\ndma->vaddr += overlap;\r\ndma->npage -= npage_lo;\r\nreturn npage_lo;\r\n}\r\nif (start + size >= dma->iova + NPAGE_TO_SIZE(dma->npage)) {\r\nsize_t overlap;\r\noverlap = dma->iova + NPAGE_TO_SIZE(dma->npage) - start;\r\nnpage_hi = overlap >> PAGE_SHIFT;\r\nvfio_dma_unmap(iommu, start, npage_hi, dma->prot);\r\ndma->npage -= npage_hi;\r\nreturn npage_hi;\r\n}\r\nnpage_lo = (start - dma->iova) >> PAGE_SHIFT;\r\nnpage_hi = dma->npage - (size >> PAGE_SHIFT) - npage_lo;\r\nsplit = kzalloc(sizeof *split, GFP_KERNEL);\r\nif (!split)\r\nreturn -ENOMEM;\r\nvfio_dma_unmap(iommu, start, size >> PAGE_SHIFT, dma->prot);\r\ndma->npage = npage_lo;\r\nsplit->npage = npage_hi;\r\nsplit->iova = start + size;\r\nsplit->vaddr = dma->vaddr + NPAGE_TO_SIZE(npage_lo) + size;\r\nsplit->prot = dma->prot;\r\nlist_add(&split->next, &iommu->dma_list);\r\nreturn size >> PAGE_SHIFT;\r\n}\r\nstatic int vfio_dma_do_unmap(struct vfio_iommu *iommu,\r\nstruct vfio_iommu_type1_dma_unmap *unmap)\r\n{\r\nlong ret = 0, npage = unmap->size >> PAGE_SHIFT;\r\nstruct vfio_dma *dma, *tmp;\r\nuint64_t mask;\r\nmask = ((uint64_t)1 << __ffs(iommu->domain->ops->pgsize_bitmap)) - 1;\r\nif (unmap->iova & mask)\r\nreturn -EINVAL;\r\nif (unmap->size & mask)\r\nreturn -EINVAL;\r\nWARN_ON(mask & PAGE_MASK);\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry_safe(dma, tmp, &iommu->dma_list, next) {\r\nif (ranges_overlap(dma->iova, NPAGE_TO_SIZE(dma->npage),\r\nunmap->iova, unmap->size)) {\r\nret = vfio_remove_dma_overlap(iommu, unmap->iova,\r\nunmap->size, dma);\r\nif (ret > 0)\r\nnpage -= ret;\r\nif (ret < 0 || npage == 0)\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&iommu->lock);\r\nreturn ret > 0 ? 0 : (int)ret;\r\n}\r\nstatic int vfio_dma_do_map(struct vfio_iommu *iommu,\r\nstruct vfio_iommu_type1_dma_map *map)\r\n{\r\nstruct vfio_dma *dma, *pdma = NULL;\r\ndma_addr_t iova = map->iova;\r\nunsigned long locked, lock_limit, vaddr = map->vaddr;\r\nsize_t size = map->size;\r\nint ret = 0, prot = 0;\r\nuint64_t mask;\r\nlong npage;\r\nmask = ((uint64_t)1 << __ffs(iommu->domain->ops->pgsize_bitmap)) - 1;\r\nif (map->flags & VFIO_DMA_MAP_FLAG_WRITE)\r\nprot |= IOMMU_WRITE;\r\nif (map->flags & VFIO_DMA_MAP_FLAG_READ)\r\nprot |= IOMMU_READ;\r\nif (!prot)\r\nreturn -EINVAL;\r\nif (vaddr & mask)\r\nreturn -EINVAL;\r\nif (iova & mask)\r\nreturn -EINVAL;\r\nif (size & mask)\r\nreturn -EINVAL;\r\nWARN_ON(mask & PAGE_MASK);\r\nif (iova + size && iova + size < iova)\r\nreturn -EINVAL;\r\nif (vaddr + size && vaddr + size < vaddr)\r\nreturn -EINVAL;\r\nnpage = size >> PAGE_SHIFT;\r\nif (!npage)\r\nreturn -EINVAL;\r\nmutex_lock(&iommu->lock);\r\nif (vfio_find_dma(iommu, iova, size)) {\r\nret = -EBUSY;\r\ngoto out_lock;\r\n}\r\nlocked = current->mm->locked_vm + npage;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif (locked > lock_limit && !capable(CAP_IPC_LOCK)) {\r\npr_warn("%s: RLIMIT_MEMLOCK (%ld) exceeded\n",\r\n__func__, rlimit(RLIMIT_MEMLOCK));\r\nret = -ENOMEM;\r\ngoto out_lock;\r\n}\r\nret = __vfio_dma_map(iommu, iova, vaddr, npage, prot);\r\nif (ret)\r\ngoto out_lock;\r\nif (iova) {\r\ndma = vfio_find_dma(iommu, iova - 1, 1);\r\nif (dma && dma->prot == prot &&\r\ndma->vaddr + NPAGE_TO_SIZE(dma->npage) == vaddr) {\r\ndma->npage += npage;\r\niova = dma->iova;\r\nvaddr = dma->vaddr;\r\nnpage = dma->npage;\r\nsize = NPAGE_TO_SIZE(npage);\r\npdma = dma;\r\n}\r\n}\r\nif (iova + size) {\r\ndma = vfio_find_dma(iommu, iova + size, 1);\r\nif (dma && dma->prot == prot &&\r\ndma->vaddr == vaddr + size) {\r\ndma->npage += npage;\r\ndma->iova = iova;\r\ndma->vaddr = vaddr;\r\nif (pdma) {\r\nlist_del(&pdma->next);\r\nkfree(pdma);\r\n}\r\npdma = dma;\r\n}\r\n}\r\nif (!pdma) {\r\ndma = kzalloc(sizeof *dma, GFP_KERNEL);\r\nif (!dma) {\r\nret = -ENOMEM;\r\nvfio_dma_unmap(iommu, iova, npage, prot);\r\ngoto out_lock;\r\n}\r\ndma->npage = npage;\r\ndma->iova = iova;\r\ndma->vaddr = vaddr;\r\ndma->prot = prot;\r\nlist_add(&dma->next, &iommu->dma_list);\r\n}\r\nout_lock:\r\nmutex_unlock(&iommu->lock);\r\nreturn ret;\r\n}\r\nstatic int vfio_iommu_type1_attach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_group *group, *tmp;\r\nint ret;\r\ngroup = kzalloc(sizeof(*group), GFP_KERNEL);\r\nif (!group)\r\nreturn -ENOMEM;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(tmp, &iommu->group_list, next) {\r\nif (tmp->iommu_group == iommu_group) {\r\nmutex_unlock(&iommu->lock);\r\nkfree(group);\r\nreturn -EINVAL;\r\n}\r\n}\r\nret = iommu_attach_group(iommu->domain, iommu_group);\r\nif (ret) {\r\nmutex_unlock(&iommu->lock);\r\nkfree(group);\r\nreturn ret;\r\n}\r\ngroup->iommu_group = iommu_group;\r\nlist_add(&group->next, &iommu->group_list);\r\nmutex_unlock(&iommu->lock);\r\nreturn 0;\r\n}\r\nstatic void vfio_iommu_type1_detach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_group *group;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(group, &iommu->group_list, next) {\r\nif (group->iommu_group == iommu_group) {\r\niommu_detach_group(iommu->domain, iommu_group);\r\nlist_del(&group->next);\r\nkfree(group);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&iommu->lock);\r\n}\r\nstatic void *vfio_iommu_type1_open(unsigned long arg)\r\n{\r\nstruct vfio_iommu *iommu;\r\nif (arg != VFIO_TYPE1_IOMMU)\r\nreturn ERR_PTR(-EINVAL);\r\niommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\r\nif (!iommu)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_LIST_HEAD(&iommu->group_list);\r\nINIT_LIST_HEAD(&iommu->dma_list);\r\nmutex_init(&iommu->lock);\r\niommu->domain = iommu_domain_alloc(&pci_bus_type);\r\nif (!iommu->domain) {\r\nkfree(iommu);\r\nreturn ERR_PTR(-EIO);\r\n}\r\nif (!allow_unsafe_interrupts &&\r\n!iommu_domain_has_cap(iommu->domain, IOMMU_CAP_INTR_REMAP)) {\r\npr_warn("%s: No interrupt remapping support. Use the module param \"allow_unsafe_interrupts\" to enable VFIO IOMMU support on this platform\n",\r\n__func__);\r\niommu_domain_free(iommu->domain);\r\nkfree(iommu);\r\nreturn ERR_PTR(-EPERM);\r\n}\r\nreturn iommu;\r\n}\r\nstatic void vfio_iommu_type1_release(void *iommu_data)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_group *group, *group_tmp;\r\nstruct vfio_dma *dma, *dma_tmp;\r\nlist_for_each_entry_safe(group, group_tmp, &iommu->group_list, next) {\r\niommu_detach_group(iommu->domain, group->iommu_group);\r\nlist_del(&group->next);\r\nkfree(group);\r\n}\r\nlist_for_each_entry_safe(dma, dma_tmp, &iommu->dma_list, next) {\r\nvfio_dma_unmap(iommu, dma->iova, dma->npage, dma->prot);\r\nlist_del(&dma->next);\r\nkfree(dma);\r\n}\r\niommu_domain_free(iommu->domain);\r\niommu->domain = NULL;\r\nkfree(iommu);\r\n}\r\nstatic long vfio_iommu_type1_ioctl(void *iommu_data,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nunsigned long minsz;\r\nif (cmd == VFIO_CHECK_EXTENSION) {\r\nswitch (arg) {\r\ncase VFIO_TYPE1_IOMMU:\r\nreturn 1;\r\ndefault:\r\nreturn 0;\r\n}\r\n} else if (cmd == VFIO_IOMMU_GET_INFO) {\r\nstruct vfio_iommu_type1_info info;\r\nminsz = offsetofend(struct vfio_iommu_type1_info, iova_pgsizes);\r\nif (copy_from_user(&info, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (info.argsz < minsz)\r\nreturn -EINVAL;\r\ninfo.flags = 0;\r\ninfo.iova_pgsizes = iommu->domain->ops->pgsize_bitmap;\r\nreturn copy_to_user((void __user *)arg, &info, minsz);\r\n} else if (cmd == VFIO_IOMMU_MAP_DMA) {\r\nstruct vfio_iommu_type1_dma_map map;\r\nuint32_t mask = VFIO_DMA_MAP_FLAG_READ |\r\nVFIO_DMA_MAP_FLAG_WRITE;\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_map, size);\r\nif (copy_from_user(&map, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (map.argsz < minsz || map.flags & ~mask)\r\nreturn -EINVAL;\r\nreturn vfio_dma_do_map(iommu, &map);\r\n} else if (cmd == VFIO_IOMMU_UNMAP_DMA) {\r\nstruct vfio_iommu_type1_dma_unmap unmap;\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_unmap, size);\r\nif (copy_from_user(&unmap, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (unmap.argsz < minsz || unmap.flags)\r\nreturn -EINVAL;\r\nreturn vfio_dma_do_unmap(iommu, &unmap);\r\n}\r\nreturn -ENOTTY;\r\n}\r\nstatic int __init vfio_iommu_type1_init(void)\r\n{\r\nif (!iommu_present(&pci_bus_type))\r\nreturn -ENODEV;\r\nreturn vfio_register_iommu_driver(&vfio_iommu_driver_ops_type1);\r\n}\r\nstatic void __exit vfio_iommu_type1_cleanup(void)\r\n{\r\nvfio_unregister_iommu_driver(&vfio_iommu_driver_ops_type1);\r\n}
