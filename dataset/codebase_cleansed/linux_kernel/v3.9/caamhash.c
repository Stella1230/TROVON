static inline void map_seq_out_ptr_ctx(u32 *desc, struct device *jrdev,\r\nstruct caam_hash_state *state,\r\nint ctx_len)\r\n{\r\nstate->ctx_dma = dma_map_single(jrdev, state->caam_ctx,\r\nctx_len, DMA_FROM_DEVICE);\r\nappend_seq_out_ptr(desc, state->ctx_dma, ctx_len, 0);\r\n}\r\nstatic inline dma_addr_t map_seq_out_ptr_result(u32 *desc, struct device *jrdev,\r\nu8 *result, int digestsize)\r\n{\r\ndma_addr_t dst_dma;\r\ndst_dma = dma_map_single(jrdev, result, digestsize, DMA_FROM_DEVICE);\r\nappend_seq_out_ptr(desc, dst_dma, digestsize, 0);\r\nreturn dst_dma;\r\n}\r\nstatic inline dma_addr_t buf_map_to_sec4_sg(struct device *jrdev,\r\nstruct sec4_sg_entry *sec4_sg,\r\nu8 *buf, int buflen)\r\n{\r\ndma_addr_t buf_dma;\r\nbuf_dma = dma_map_single(jrdev, buf, buflen, DMA_TO_DEVICE);\r\ndma_to_sec4_sg_one(sec4_sg, buf_dma, buflen, 0);\r\nreturn buf_dma;\r\n}\r\nstatic inline void src_map_to_sec4_sg(struct device *jrdev,\r\nstruct scatterlist *src, int src_nents,\r\nstruct sec4_sg_entry *sec4_sg,\r\nbool chained)\r\n{\r\ndma_map_sg_chained(jrdev, src, src_nents, DMA_TO_DEVICE, chained);\r\nsg_to_sec4_sg_last(src, src_nents, sec4_sg, 0);\r\n}\r\nstatic inline dma_addr_t\r\ntry_buf_map_to_sec4_sg(struct device *jrdev, struct sec4_sg_entry *sec4_sg,\r\nu8 *buf, dma_addr_t buf_dma, int buflen,\r\nint last_buflen)\r\n{\r\nif (buf_dma && !dma_mapping_error(jrdev, buf_dma))\r\ndma_unmap_single(jrdev, buf_dma, last_buflen, DMA_TO_DEVICE);\r\nif (buflen)\r\nbuf_dma = buf_map_to_sec4_sg(jrdev, sec4_sg, buf, buflen);\r\nelse\r\nbuf_dma = 0;\r\nreturn buf_dma;\r\n}\r\nstatic inline void ctx_map_to_sec4_sg(u32 *desc, struct device *jrdev,\r\nstruct caam_hash_state *state,\r\nint ctx_len,\r\nstruct sec4_sg_entry *sec4_sg,\r\nu32 flag)\r\n{\r\nstate->ctx_dma = dma_map_single(jrdev, state->caam_ctx, ctx_len, flag);\r\ndma_to_sec4_sg_one(sec4_sg, state->ctx_dma, ctx_len, 0);\r\n}\r\nstatic inline void append_key_ahash(u32 *desc, struct caam_hash_ctx *ctx)\r\n{\r\nappend_key_as_imm(desc, ctx->key, ctx->split_key_pad_len,\r\nctx->split_key_len, CLASS_2 |\r\nKEY_DEST_MDHA_SPLIT | KEY_ENC);\r\n}\r\nstatic inline void init_sh_desc_key_ahash(u32 *desc, struct caam_hash_ctx *ctx)\r\n{\r\nu32 *key_jump_cmd;\r\ninit_sh_desc(desc, HDR_SHARE_SERIAL);\r\nif (ctx->split_key_len) {\r\nkey_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |\r\nJUMP_COND_SHRD);\r\nappend_key_ahash(desc, ctx);\r\nset_jump_tgt_here(desc, key_jump_cmd);\r\n}\r\nappend_cmd(desc, SET_OK_NO_PROP_ERRORS | CMD_LOAD);\r\n}\r\nstatic inline void ahash_append_load_str(u32 *desc, int digestsize)\r\n{\r\nappend_math_add(desc, VARSEQINLEN, SEQINLEN, REG0, CAAM_CMD_SZ);\r\nappend_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_LAST2 |\r\nFIFOLD_TYPE_MSG | KEY_VLF);\r\nappend_seq_store(desc, digestsize, LDST_CLASS_2_CCB |\r\nLDST_SRCDST_BYTE_CONTEXT);\r\n}\r\nstatic inline void ahash_ctx_data_to_out(u32 *desc, u32 op, u32 state,\r\nint digestsize,\r\nstruct caam_hash_ctx *ctx)\r\n{\r\ninit_sh_desc_key_ahash(desc, ctx);\r\nappend_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_BYTE_CONTEXT |\r\nLDST_CLASS_2_CCB | ctx->ctx_len);\r\nappend_operation(desc, op | state | OP_ALG_ENCRYPT);\r\nahash_append_load_str(desc, digestsize);\r\n}\r\nstatic inline void ahash_data_to_out(u32 *desc, u32 op, u32 state,\r\nint digestsize, struct caam_hash_ctx *ctx)\r\n{\r\ninit_sh_desc_key_ahash(desc, ctx);\r\nappend_operation(desc, op | state | OP_ALG_ENCRYPT);\r\nahash_append_load_str(desc, digestsize);\r\n}\r\nstatic int ahash_set_sh_desc(struct crypto_ahash *ahash)\r\n{\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct device *jrdev = ctx->jrdev;\r\nu32 have_key = 0;\r\nu32 *desc;\r\nif (ctx->split_key_len)\r\nhave_key = OP_ALG_AAI_HMAC_PRECOMP;\r\ndesc = ctx->sh_desc_update;\r\ninit_sh_desc(desc, HDR_SHARE_SERIAL);\r\nappend_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_BYTE_CONTEXT |\r\nLDST_CLASS_2_CCB | ctx->ctx_len);\r\nappend_operation(desc, ctx->alg_type | OP_ALG_AS_UPDATE |\r\nOP_ALG_ENCRYPT);\r\nahash_append_load_str(desc, ctx->ctx_len);\r\nctx->sh_desc_update_dma = dma_map_single(jrdev, desc, desc_bytes(desc),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, ctx->sh_desc_update_dma)) {\r\ndev_err(jrdev, "unable to map shared descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ahash update shdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\ndesc = ctx->sh_desc_update_first;\r\nahash_data_to_out(desc, have_key | ctx->alg_type, OP_ALG_AS_INIT,\r\nctx->ctx_len, ctx);\r\nctx->sh_desc_update_first_dma = dma_map_single(jrdev, desc,\r\ndesc_bytes(desc),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, ctx->sh_desc_update_first_dma)) {\r\ndev_err(jrdev, "unable to map shared descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ahash update first shdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\ndesc = ctx->sh_desc_fin;\r\nahash_ctx_data_to_out(desc, have_key | ctx->alg_type,\r\nOP_ALG_AS_FINALIZE, digestsize, ctx);\r\nctx->sh_desc_fin_dma = dma_map_single(jrdev, desc, desc_bytes(desc),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, ctx->sh_desc_fin_dma)) {\r\ndev_err(jrdev, "unable to map shared descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ahash final shdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\ndesc = ctx->sh_desc_finup;\r\nahash_ctx_data_to_out(desc, have_key | ctx->alg_type,\r\nOP_ALG_AS_FINALIZE, digestsize, ctx);\r\nctx->sh_desc_finup_dma = dma_map_single(jrdev, desc, desc_bytes(desc),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, ctx->sh_desc_finup_dma)) {\r\ndev_err(jrdev, "unable to map shared descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ahash finup shdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\ndesc = ctx->sh_desc_digest;\r\nahash_data_to_out(desc, have_key | ctx->alg_type, OP_ALG_AS_INITFINAL,\r\ndigestsize, ctx);\r\nctx->sh_desc_digest_dma = dma_map_single(jrdev, desc,\r\ndesc_bytes(desc),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, ctx->sh_desc_digest_dma)) {\r\ndev_err(jrdev, "unable to map shared descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ahash digest shdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic u32 gen_split_hash_key(struct caam_hash_ctx *ctx, const u8 *key_in,\r\nu32 keylen)\r\n{\r\nreturn gen_split_key(ctx->jrdev, ctx->key, ctx->split_key_len,\r\nctx->split_key_pad_len, key_in, keylen,\r\nctx->alg_op);\r\n}\r\nstatic u32 hash_digest_key(struct caam_hash_ctx *ctx, const u8 *key_in,\r\nu32 *keylen, u8 *key_out, u32 digestsize)\r\n{\r\nstruct device *jrdev = ctx->jrdev;\r\nu32 *desc;\r\nstruct split_key_result result;\r\ndma_addr_t src_dma, dst_dma;\r\nint ret = 0;\r\ndesc = kmalloc(CAAM_CMD_SZ * 6 + CAAM_PTR_SZ * 2, GFP_KERNEL | GFP_DMA);\r\nif (!desc) {\r\ndev_err(jrdev, "unable to allocate key input memory\n");\r\nreturn -ENOMEM;\r\n}\r\ninit_job_desc(desc, 0);\r\nsrc_dma = dma_map_single(jrdev, (void *)key_in, *keylen,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, src_dma)) {\r\ndev_err(jrdev, "unable to map key input memory\n");\r\nkfree(desc);\r\nreturn -ENOMEM;\r\n}\r\ndst_dma = dma_map_single(jrdev, (void *)key_out, digestsize,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(jrdev, dst_dma)) {\r\ndev_err(jrdev, "unable to map key output memory\n");\r\ndma_unmap_single(jrdev, src_dma, *keylen, DMA_TO_DEVICE);\r\nkfree(desc);\r\nreturn -ENOMEM;\r\n}\r\nappend_operation(desc, ctx->alg_type | OP_ALG_ENCRYPT |\r\nOP_ALG_AS_INITFINAL);\r\nappend_seq_in_ptr(desc, src_dma, *keylen, 0);\r\nappend_seq_fifo_load(desc, *keylen, FIFOLD_CLASS_CLASS2 |\r\nFIFOLD_TYPE_LAST2 | FIFOLD_TYPE_MSG);\r\nappend_seq_out_ptr(desc, dst_dma, digestsize, 0);\r\nappend_seq_store(desc, digestsize, LDST_CLASS_2_CCB |\r\nLDST_SRCDST_BYTE_CONTEXT);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "key_in@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, key_in, *keylen, 1);\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nresult.err = 0;\r\ninit_completion(&result.completion);\r\nret = caam_jr_enqueue(jrdev, desc, split_key_done, &result);\r\nif (!ret) {\r\nwait_for_completion_interruptible(&result.completion);\r\nret = result.err;\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "digested key@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, key_in,\r\ndigestsize, 1);\r\n#endif\r\n}\r\n*keylen = digestsize;\r\ndma_unmap_single(jrdev, src_dma, *keylen, DMA_TO_DEVICE);\r\ndma_unmap_single(jrdev, dst_dma, digestsize, DMA_FROM_DEVICE);\r\nkfree(desc);\r\nreturn ret;\r\n}\r\nstatic int ahash_setkey(struct crypto_ahash *ahash,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstatic const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct device *jrdev = ctx->jrdev;\r\nint blocksize = crypto_tfm_alg_blocksize(&ahash->base);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nint ret = 0;\r\nu8 *hashed_key = NULL;\r\n#ifdef DEBUG\r\nprintk(KERN_ERR "keylen %d\n", keylen);\r\n#endif\r\nif (keylen > blocksize) {\r\nhashed_key = kmalloc(sizeof(u8) * digestsize, GFP_KERNEL |\r\nGFP_DMA);\r\nif (!hashed_key)\r\nreturn -ENOMEM;\r\nret = hash_digest_key(ctx, key, &keylen, hashed_key,\r\ndigestsize);\r\nif (ret)\r\ngoto badkey;\r\nkey = hashed_key;\r\n}\r\nctx->split_key_len = mdpadlen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>\r\nOP_ALG_ALGSEL_SHIFT] * 2;\r\nctx->split_key_pad_len = ALIGN(ctx->split_key_len, 16);\r\n#ifdef DEBUG\r\nprintk(KERN_ERR "split_key_len %d split_key_pad_len %d\n",\r\nctx->split_key_len, ctx->split_key_pad_len);\r\nprint_hex_dump(KERN_ERR, "key in @"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);\r\n#endif\r\nret = gen_split_hash_key(ctx, key, keylen);\r\nif (ret)\r\ngoto badkey;\r\nctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, ctx->key_dma)) {\r\ndev_err(jrdev, "unable to map key i/o memory\n");\r\nreturn -ENOMEM;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx.key@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, ctx->key,\r\nctx->split_key_pad_len, 1);\r\n#endif\r\nret = ahash_set_sh_desc(ahash);\r\nif (ret) {\r\ndma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len,\r\nDMA_TO_DEVICE);\r\n}\r\nkfree(hashed_key);\r\nreturn ret;\r\nbadkey:\r\nkfree(hashed_key);\r\ncrypto_ahash_set_flags(ahash, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic inline void ahash_unmap(struct device *dev,\r\nstruct ahash_edesc *edesc,\r\nstruct ahash_request *req, int dst_len)\r\n{\r\nif (edesc->src_nents)\r\ndma_unmap_sg_chained(dev, req->src, edesc->src_nents,\r\nDMA_TO_DEVICE, edesc->chained);\r\nif (edesc->dst_dma)\r\ndma_unmap_single(dev, edesc->dst_dma, dst_len, DMA_FROM_DEVICE);\r\nif (edesc->sec4_sg_bytes)\r\ndma_unmap_single(dev, edesc->sec4_sg_dma,\r\nedesc->sec4_sg_bytes, DMA_TO_DEVICE);\r\n}\r\nstatic inline void ahash_unmap_ctx(struct device *dev,\r\nstruct ahash_edesc *edesc,\r\nstruct ahash_request *req, int dst_len, u32 flag)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nif (state->ctx_dma)\r\ndma_unmap_single(dev, state->ctx_dma, ctx->ctx_len, flag);\r\nahash_unmap(dev, edesc, req, dst_len);\r\n}\r\nstatic void ahash_done(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\n#ifdef DEBUG\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = (struct ahash_edesc *)((char *)desc -\r\noffsetof(struct ahash_edesc, hw_desc));\r\nif (err) {\r\nchar tmp[CAAM_ERROR_STR_MAX];\r\ndev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));\r\n}\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic void ahash_done_bi(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\n#ifdef DEBUG\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = (struct ahash_edesc *)((char *)desc -\r\noffsetof(struct ahash_edesc, hw_desc));\r\nif (err) {\r\nchar tmp[CAAM_ERROR_STR_MAX];\r\ndev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));\r\n}\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_BIDIRECTIONAL);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic void ahash_done_ctx_src(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\n#ifdef DEBUG\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = (struct ahash_edesc *)((char *)desc -\r\noffsetof(struct ahash_edesc, hw_desc));\r\nif (err) {\r\nchar tmp[CAAM_ERROR_STR_MAX];\r\ndev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));\r\n}\r\nahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_FROM_DEVICE);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic void ahash_done_ctx_dst(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\n#ifdef DEBUG\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = (struct ahash_edesc *)((char *)desc -\r\noffsetof(struct ahash_edesc, hw_desc));\r\nif (err) {\r\nchar tmp[CAAM_ERROR_STR_MAX];\r\ndev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));\r\n}\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_TO_DEVICE);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic int ahash_update_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = state->current_buf ? state->buf_1 : state->buf_0;\r\nint *buflen = state->current_buf ? &state->buflen_1 : &state->buflen_0;\r\nu8 *next_buf = state->current_buf ? state->buf_0 : state->buf_1;\r\nint *next_buflen = state->current_buf ? &state->buflen_0 :\r\n&state->buflen_1, last_buflen;\r\nint in_len = *buflen + req->nbytes, to_hash;\r\nu32 *sh_desc = ctx->sh_desc_update, *desc;\r\ndma_addr_t ptr = ctx->sh_desc_update_dma;\r\nint src_nents, sec4_sg_bytes, sec4_sg_src_index;\r\nstruct ahash_edesc *edesc;\r\nbool chained = false;\r\nint ret = 0;\r\nint sh_len;\r\nlast_buflen = *next_buflen;\r\n*next_buflen = in_len & (crypto_tfm_alg_blocksize(&ahash->base) - 1);\r\nto_hash = in_len - *next_buflen;\r\nif (to_hash) {\r\nsrc_nents = __sg_count(req->src, req->nbytes - (*next_buflen),\r\n&chained);\r\nsec4_sg_src_index = 1 + (*buflen ? 1 : 0);\r\nsec4_sg_bytes = (sec4_sg_src_index + src_nents) *\r\nsizeof(struct sec4_sg_entry);\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + DESC_JOB_IO_LEN +\r\nsec4_sg_bytes, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev,\r\n"could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nedesc->src_nents = src_nents;\r\nedesc->chained = chained;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->sec4_sg = (void *)edesc + sizeof(struct ahash_edesc) +\r\nDESC_JOB_IO_LEN;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes,\r\nDMA_TO_DEVICE);\r\nctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len,\r\nedesc->sec4_sg, DMA_BIDIRECTIONAL);\r\nstate->buf_dma = try_buf_map_to_sec4_sg(jrdev,\r\nedesc->sec4_sg + 1,\r\nbuf, state->buf_dma,\r\n*buflen, last_buflen);\r\nif (src_nents) {\r\nsrc_map_to_sec4_sg(jrdev, req->src, src_nents,\r\nedesc->sec4_sg + sec4_sg_src_index,\r\nchained);\r\nif (*next_buflen) {\r\nsg_copy_part(next_buf, req->src, to_hash -\r\n*buflen, req->nbytes);\r\nstate->current_buf = !state->current_buf;\r\n}\r\n} else {\r\n(edesc->sec4_sg + sec4_sg_src_index - 1)->len |=\r\nSEC4_SG_LEN_FIN;\r\n}\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER |\r\nHDR_REVERSE);\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, ctx->ctx_len +\r\nto_hash, LDST_SGF);\r\nappend_seq_out_ptr(desc, state->ctx_dma, ctx->ctx_len, 0);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_bi, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len,\r\nDMA_BIDIRECTIONAL);\r\nkfree(edesc);\r\n}\r\n} else if (*next_buflen) {\r\nsg_copy(buf + *buflen, req->src, req->nbytes);\r\n*buflen = *next_buflen;\r\n*next_buflen = last_buflen;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "buf@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, buf, *buflen, 1);\r\nprint_hex_dump(KERN_ERR, "next buf@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, next_buf,\r\n*next_buflen, 1);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int ahash_final_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = state->current_buf ? state->buf_1 : state->buf_0;\r\nint buflen = state->current_buf ? state->buflen_1 : state->buflen_0;\r\nint last_buflen = state->current_buf ? state->buflen_0 :\r\nstate->buflen_1;\r\nu32 *sh_desc = ctx->sh_desc_fin, *desc;\r\ndma_addr_t ptr = ctx->sh_desc_fin_dma;\r\nint sec4_sg_bytes;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nint ret = 0;\r\nint sh_len;\r\nsec4_sg_bytes = (1 + (buflen ? 1 : 0)) * sizeof(struct sec4_sg_entry);\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + DESC_JOB_IO_LEN +\r\nsec4_sg_bytes, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev, "could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER | HDR_REVERSE);\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->sec4_sg = (void *)edesc + sizeof(struct ahash_edesc) +\r\nDESC_JOB_IO_LEN;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes, DMA_TO_DEVICE);\r\nedesc->src_nents = 0;\r\nctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len, edesc->sec4_sg,\r\nDMA_TO_DEVICE);\r\nstate->buf_dma = try_buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1,\r\nbuf, state->buf_dma, buflen,\r\nlast_buflen);\r\n(edesc->sec4_sg + sec4_sg_bytes - 1)->len |= SEC4_SG_LEN_FIN;\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, ctx->ctx_len + buflen,\r\nLDST_SGF);\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_src, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_FROM_DEVICE);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ahash_finup_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = state->current_buf ? state->buf_1 : state->buf_0;\r\nint buflen = state->current_buf ? state->buflen_1 : state->buflen_0;\r\nint last_buflen = state->current_buf ? state->buflen_0 :\r\nstate->buflen_1;\r\nu32 *sh_desc = ctx->sh_desc_finup, *desc;\r\ndma_addr_t ptr = ctx->sh_desc_finup_dma;\r\nint sec4_sg_bytes, sec4_sg_src_index;\r\nint src_nents;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nbool chained = false;\r\nint ret = 0;\r\nint sh_len;\r\nsrc_nents = __sg_count(req->src, req->nbytes, &chained);\r\nsec4_sg_src_index = 1 + (buflen ? 1 : 0);\r\nsec4_sg_bytes = (sec4_sg_src_index + src_nents) *\r\nsizeof(struct sec4_sg_entry);\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + DESC_JOB_IO_LEN +\r\nsec4_sg_bytes, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev, "could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER | HDR_REVERSE);\r\nedesc->src_nents = src_nents;\r\nedesc->chained = chained;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->sec4_sg = (void *)edesc + sizeof(struct ahash_edesc) +\r\nDESC_JOB_IO_LEN;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes, DMA_TO_DEVICE);\r\nctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len, edesc->sec4_sg,\r\nDMA_TO_DEVICE);\r\nstate->buf_dma = try_buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1,\r\nbuf, state->buf_dma, buflen,\r\nlast_buflen);\r\nsrc_map_to_sec4_sg(jrdev, req->src, src_nents, edesc->sec4_sg +\r\nsec4_sg_src_index, chained);\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, ctx->ctx_len +\r\nbuflen + req->nbytes, LDST_SGF);\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_src, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_FROM_DEVICE);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ahash_digest(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu32 *sh_desc = ctx->sh_desc_digest, *desc;\r\ndma_addr_t ptr = ctx->sh_desc_digest_dma;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nint src_nents, sec4_sg_bytes;\r\ndma_addr_t src_dma;\r\nstruct ahash_edesc *edesc;\r\nbool chained = false;\r\nint ret = 0;\r\nu32 options;\r\nint sh_len;\r\nsrc_nents = sg_count(req->src, req->nbytes, &chained);\r\ndma_map_sg_chained(jrdev, req->src, src_nents ? : 1, DMA_TO_DEVICE,\r\nchained);\r\nsec4_sg_bytes = src_nents * sizeof(struct sec4_sg_entry);\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + sec4_sg_bytes +\r\nDESC_JOB_IO_LEN, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev, "could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nedesc->sec4_sg = (void *)edesc + sizeof(struct ahash_edesc) +\r\nDESC_JOB_IO_LEN;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes, DMA_TO_DEVICE);\r\nedesc->src_nents = src_nents;\r\nedesc->chained = chained;\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER | HDR_REVERSE);\r\nif (src_nents) {\r\nsg_to_sec4_sg_last(req->src, src_nents, edesc->sec4_sg, 0);\r\nsrc_dma = edesc->sec4_sg_dma;\r\noptions = LDST_SGF;\r\n} else {\r\nsrc_dma = sg_dma_address(req->src);\r\noptions = 0;\r\n}\r\nappend_seq_in_ptr(desc, src_dma, req->nbytes, options);\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ahash_final_no_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = state->current_buf ? state->buf_1 : state->buf_0;\r\nint buflen = state->current_buf ? state->buflen_1 : state->buflen_0;\r\nu32 *sh_desc = ctx->sh_desc_digest, *desc;\r\ndma_addr_t ptr = ctx->sh_desc_digest_dma;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nint ret = 0;\r\nint sh_len;\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + DESC_JOB_IO_LEN,\r\nGFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev, "could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER | HDR_REVERSE);\r\nstate->buf_dma = dma_map_single(jrdev, buf, buflen, DMA_TO_DEVICE);\r\nappend_seq_in_ptr(desc, state->buf_dma, buflen, 0);\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\nedesc->src_nents = 0;\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ahash_update_no_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = state->current_buf ? state->buf_1 : state->buf_0;\r\nint *buflen = state->current_buf ? &state->buflen_1 : &state->buflen_0;\r\nu8 *next_buf = state->current_buf ? state->buf_0 : state->buf_1;\r\nint *next_buflen = state->current_buf ? &state->buflen_0 :\r\n&state->buflen_1;\r\nint in_len = *buflen + req->nbytes, to_hash;\r\nint sec4_sg_bytes, src_nents;\r\nstruct ahash_edesc *edesc;\r\nu32 *desc, *sh_desc = ctx->sh_desc_update_first;\r\ndma_addr_t ptr = ctx->sh_desc_update_first_dma;\r\nbool chained = false;\r\nint ret = 0;\r\nint sh_len;\r\n*next_buflen = in_len & (crypto_tfm_alg_blocksize(&ahash->base) - 1);\r\nto_hash = in_len - *next_buflen;\r\nif (to_hash) {\r\nsrc_nents = __sg_count(req->src, req->nbytes - (*next_buflen),\r\n&chained);\r\nsec4_sg_bytes = (1 + src_nents) *\r\nsizeof(struct sec4_sg_entry);\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + DESC_JOB_IO_LEN +\r\nsec4_sg_bytes, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev,\r\n"could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nedesc->src_nents = src_nents;\r\nedesc->chained = chained;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->sec4_sg = (void *)edesc + sizeof(struct ahash_edesc) +\r\nDESC_JOB_IO_LEN;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes,\r\nDMA_TO_DEVICE);\r\nstate->buf_dma = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg,\r\nbuf, *buflen);\r\nsrc_map_to_sec4_sg(jrdev, req->src, src_nents,\r\nedesc->sec4_sg + 1, chained);\r\nif (*next_buflen) {\r\nsg_copy_part(next_buf, req->src, to_hash - *buflen,\r\nreq->nbytes);\r\nstate->current_buf = !state->current_buf;\r\n}\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER |\r\nHDR_REVERSE);\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, to_hash, LDST_SGF);\r\nmap_seq_out_ptr_ctx(desc, jrdev, state, ctx->ctx_len);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_dst, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\nstate->update = ahash_update_ctx;\r\nstate->finup = ahash_finup_ctx;\r\nstate->final = ahash_final_ctx;\r\n} else {\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len,\r\nDMA_TO_DEVICE);\r\nkfree(edesc);\r\n}\r\n} else if (*next_buflen) {\r\nsg_copy(buf + *buflen, req->src, req->nbytes);\r\n*buflen = *next_buflen;\r\n*next_buflen = 0;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "buf@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, buf, *buflen, 1);\r\nprint_hex_dump(KERN_ERR, "next buf@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, next_buf,\r\n*next_buflen, 1);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int ahash_finup_no_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = state->current_buf ? state->buf_1 : state->buf_0;\r\nint buflen = state->current_buf ? state->buflen_1 : state->buflen_0;\r\nint last_buflen = state->current_buf ? state->buflen_0 :\r\nstate->buflen_1;\r\nu32 *sh_desc = ctx->sh_desc_digest, *desc;\r\ndma_addr_t ptr = ctx->sh_desc_digest_dma;\r\nint sec4_sg_bytes, sec4_sg_src_index, src_nents;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nbool chained = false;\r\nint sh_len;\r\nint ret = 0;\r\nsrc_nents = __sg_count(req->src, req->nbytes, &chained);\r\nsec4_sg_src_index = 2;\r\nsec4_sg_bytes = (sec4_sg_src_index + src_nents) *\r\nsizeof(struct sec4_sg_entry);\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + DESC_JOB_IO_LEN +\r\nsec4_sg_bytes, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev, "could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER | HDR_REVERSE);\r\nedesc->src_nents = src_nents;\r\nedesc->chained = chained;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->sec4_sg = (void *)edesc + sizeof(struct ahash_edesc) +\r\nDESC_JOB_IO_LEN;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes, DMA_TO_DEVICE);\r\nstate->buf_dma = try_buf_map_to_sec4_sg(jrdev, edesc->sec4_sg, buf,\r\nstate->buf_dma, buflen,\r\nlast_buflen);\r\nsrc_map_to_sec4_sg(jrdev, req->src, src_nents, edesc->sec4_sg + 1,\r\nchained);\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, buflen +\r\nreq->nbytes, LDST_SGF);\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ahash_update_first(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *next_buf = state->buf_0 + state->current_buf *\r\nCAAM_MAX_HASH_BLOCK_SIZE;\r\nint *next_buflen = &state->buflen_0 + state->current_buf;\r\nint to_hash;\r\nu32 *sh_desc = ctx->sh_desc_update_first, *desc;\r\ndma_addr_t ptr = ctx->sh_desc_update_first_dma;\r\nint sec4_sg_bytes, src_nents;\r\ndma_addr_t src_dma;\r\nu32 options;\r\nstruct ahash_edesc *edesc;\r\nbool chained = false;\r\nint ret = 0;\r\nint sh_len;\r\n*next_buflen = req->nbytes & (crypto_tfm_alg_blocksize(&ahash->base) -\r\n1);\r\nto_hash = req->nbytes - *next_buflen;\r\nif (to_hash) {\r\nsrc_nents = sg_count(req->src, req->nbytes - (*next_buflen),\r\n&chained);\r\ndma_map_sg_chained(jrdev, req->src, src_nents ? : 1,\r\nDMA_TO_DEVICE, chained);\r\nsec4_sg_bytes = src_nents * sizeof(struct sec4_sg_entry);\r\nedesc = kmalloc(sizeof(struct ahash_edesc) + DESC_JOB_IO_LEN +\r\nsec4_sg_bytes, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(jrdev,\r\n"could not allocate extended descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\nedesc->src_nents = src_nents;\r\nedesc->chained = chained;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->sec4_sg = (void *)edesc + sizeof(struct ahash_edesc) +\r\nDESC_JOB_IO_LEN;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes,\r\nDMA_TO_DEVICE);\r\nif (src_nents) {\r\nsg_to_sec4_sg_last(req->src, src_nents,\r\nedesc->sec4_sg, 0);\r\nsrc_dma = edesc->sec4_sg_dma;\r\noptions = LDST_SGF;\r\n} else {\r\nsrc_dma = sg_dma_address(req->src);\r\noptions = 0;\r\n}\r\nif (*next_buflen)\r\nsg_copy_part(next_buf, req->src, to_hash, req->nbytes);\r\nsh_len = desc_len(sh_desc);\r\ndesc = edesc->hw_desc;\r\ninit_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER |\r\nHDR_REVERSE);\r\nappend_seq_in_ptr(desc, src_dma, to_hash, options);\r\nmap_seq_out_ptr_ctx(desc, jrdev, state, ctx->ctx_len);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_dst,\r\nreq);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\nstate->update = ahash_update_ctx;\r\nstate->finup = ahash_finup_ctx;\r\nstate->final = ahash_final_ctx;\r\n} else {\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len,\r\nDMA_TO_DEVICE);\r\nkfree(edesc);\r\n}\r\n} else if (*next_buflen) {\r\nstate->update = ahash_update_no_ctx;\r\nstate->finup = ahash_finup_no_ctx;\r\nstate->final = ahash_final_no_ctx;\r\nsg_copy(next_buf, req->src, req->nbytes);\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "next buf@"xstr(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, next_buf,\r\n*next_buflen, 1);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int ahash_finup_first(struct ahash_request *req)\r\n{\r\nreturn ahash_digest(req);\r\n}\r\nstatic int ahash_init(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstate->update = ahash_update_first;\r\nstate->finup = ahash_finup_first;\r\nstate->final = ahash_final_no_ctx;\r\nstate->current_buf = 0;\r\nreturn 0;\r\n}\r\nstatic int ahash_update(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nreturn state->update(req);\r\n}\r\nstatic int ahash_finup(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nreturn state->finup(req);\r\n}\r\nstatic int ahash_final(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nreturn state->final(req);\r\n}\r\nstatic int ahash_export(struct ahash_request *req, void *out)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nmemcpy(out, ctx, sizeof(struct caam_hash_ctx));\r\nmemcpy(out + sizeof(struct caam_hash_ctx), state,\r\nsizeof(struct caam_hash_state));\r\nreturn 0;\r\n}\r\nstatic int ahash_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nmemcpy(ctx, in, sizeof(struct caam_hash_ctx));\r\nmemcpy(state, in + sizeof(struct caam_hash_ctx),\r\nsizeof(struct caam_hash_state));\r\nreturn 0;\r\n}\r\nstatic int caam_hash_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nstruct crypto_alg *base = tfm->__crt_alg;\r\nstruct hash_alg_common *halg =\r\ncontainer_of(base, struct hash_alg_common, base);\r\nstruct ahash_alg *alg =\r\ncontainer_of(halg, struct ahash_alg, halg);\r\nstruct caam_hash_alg *caam_hash =\r\ncontainer_of(alg, struct caam_hash_alg, ahash_alg);\r\nstruct caam_hash_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct caam_drv_private *priv = dev_get_drvdata(caam_hash->ctrldev);\r\nstatic const u8 runninglen[] = { HASH_MSG_LEN + MD5_DIGEST_SIZE,\r\nHASH_MSG_LEN + SHA1_DIGEST_SIZE,\r\nHASH_MSG_LEN + 32,\r\nHASH_MSG_LEN + SHA256_DIGEST_SIZE,\r\nHASH_MSG_LEN + 64,\r\nHASH_MSG_LEN + SHA512_DIGEST_SIZE };\r\nint tgt_jr = atomic_inc_return(&priv->tfm_count);\r\nint ret = 0;\r\nctx->jrdev = priv->jrdev[tgt_jr % priv->total_jobrs];\r\nctx->alg_type = OP_TYPE_CLASS2_ALG | caam_hash->alg_type;\r\nctx->alg_op = OP_TYPE_CLASS2_ALG | caam_hash->alg_op;\r\nctx->ctx_len = runninglen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>\r\nOP_ALG_ALGSEL_SHIFT];\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct caam_hash_state));\r\nret = ahash_set_sh_desc(ahash);\r\nreturn ret;\r\n}\r\nstatic void caam_hash_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct caam_hash_ctx *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->sh_desc_update_dma &&\r\n!dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_dma))\r\ndma_unmap_single(ctx->jrdev, ctx->sh_desc_update_dma,\r\ndesc_bytes(ctx->sh_desc_update),\r\nDMA_TO_DEVICE);\r\nif (ctx->sh_desc_update_first_dma &&\r\n!dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_first_dma))\r\ndma_unmap_single(ctx->jrdev, ctx->sh_desc_update_first_dma,\r\ndesc_bytes(ctx->sh_desc_update_first),\r\nDMA_TO_DEVICE);\r\nif (ctx->sh_desc_fin_dma &&\r\n!dma_mapping_error(ctx->jrdev, ctx->sh_desc_fin_dma))\r\ndma_unmap_single(ctx->jrdev, ctx->sh_desc_fin_dma,\r\ndesc_bytes(ctx->sh_desc_fin), DMA_TO_DEVICE);\r\nif (ctx->sh_desc_digest_dma &&\r\n!dma_mapping_error(ctx->jrdev, ctx->sh_desc_digest_dma))\r\ndma_unmap_single(ctx->jrdev, ctx->sh_desc_digest_dma,\r\ndesc_bytes(ctx->sh_desc_digest),\r\nDMA_TO_DEVICE);\r\nif (ctx->sh_desc_finup_dma &&\r\n!dma_mapping_error(ctx->jrdev, ctx->sh_desc_finup_dma))\r\ndma_unmap_single(ctx->jrdev, ctx->sh_desc_finup_dma,\r\ndesc_bytes(ctx->sh_desc_finup), DMA_TO_DEVICE);\r\n}\r\nstatic void __exit caam_algapi_hash_exit(void)\r\n{\r\nstruct device_node *dev_node;\r\nstruct platform_device *pdev;\r\nstruct device *ctrldev;\r\nstruct caam_drv_private *priv;\r\nstruct caam_hash_alg *t_alg, *n;\r\ndev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");\r\nif (!dev_node) {\r\ndev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");\r\nif (!dev_node)\r\nreturn;\r\n}\r\npdev = of_find_device_by_node(dev_node);\r\nif (!pdev)\r\nreturn;\r\nctrldev = &pdev->dev;\r\nof_node_put(dev_node);\r\npriv = dev_get_drvdata(ctrldev);\r\nif (!priv->hash_list.next)\r\nreturn;\r\nlist_for_each_entry_safe(t_alg, n, &priv->hash_list, entry) {\r\ncrypto_unregister_ahash(&t_alg->ahash_alg);\r\nlist_del(&t_alg->entry);\r\nkfree(t_alg);\r\n}\r\n}\r\nstatic struct caam_hash_alg *\r\ncaam_hash_alloc(struct device *ctrldev, struct caam_hash_template *template,\r\nbool keyed)\r\n{\r\nstruct caam_hash_alg *t_alg;\r\nstruct ahash_alg *halg;\r\nstruct crypto_alg *alg;\r\nt_alg = kzalloc(sizeof(struct caam_hash_alg), GFP_KERNEL);\r\nif (!t_alg) {\r\ndev_err(ctrldev, "failed to allocate t_alg\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nt_alg->ahash_alg = template->template_ahash;\r\nhalg = &t_alg->ahash_alg;\r\nalg = &halg->halg.base;\r\nif (keyed) {\r\nsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->hmac_name);\r\nsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->hmac_driver_name);\r\n} else {\r\nsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->name);\r\nsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->driver_name);\r\n}\r\nalg->cra_module = THIS_MODULE;\r\nalg->cra_init = caam_hash_cra_init;\r\nalg->cra_exit = caam_hash_cra_exit;\r\nalg->cra_ctxsize = sizeof(struct caam_hash_ctx);\r\nalg->cra_priority = CAAM_CRA_PRIORITY;\r\nalg->cra_blocksize = template->blocksize;\r\nalg->cra_alignmask = 0;\r\nalg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_TYPE_AHASH;\r\nalg->cra_type = &crypto_ahash_type;\r\nt_alg->alg_type = template->alg_type;\r\nt_alg->alg_op = template->alg_op;\r\nt_alg->ctrldev = ctrldev;\r\nreturn t_alg;\r\n}\r\nstatic int __init caam_algapi_hash_init(void)\r\n{\r\nstruct device_node *dev_node;\r\nstruct platform_device *pdev;\r\nstruct device *ctrldev;\r\nstruct caam_drv_private *priv;\r\nint i = 0, err = 0;\r\ndev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");\r\nif (!dev_node) {\r\ndev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");\r\nif (!dev_node)\r\nreturn -ENODEV;\r\n}\r\npdev = of_find_device_by_node(dev_node);\r\nif (!pdev)\r\nreturn -ENODEV;\r\nctrldev = &pdev->dev;\r\npriv = dev_get_drvdata(ctrldev);\r\nof_node_put(dev_node);\r\nINIT_LIST_HEAD(&priv->hash_list);\r\natomic_set(&priv->tfm_count, -1);\r\nfor (i = 0; i < ARRAY_SIZE(driver_hash); i++) {\r\nstruct caam_hash_alg *t_alg;\r\nt_alg = caam_hash_alloc(ctrldev, &driver_hash[i], true);\r\nif (IS_ERR(t_alg)) {\r\nerr = PTR_ERR(t_alg);\r\ndev_warn(ctrldev, "%s alg allocation failed\n",\r\ndriver_hash[i].driver_name);\r\ncontinue;\r\n}\r\nerr = crypto_register_ahash(&t_alg->ahash_alg);\r\nif (err) {\r\ndev_warn(ctrldev, "%s alg registration failed\n",\r\nt_alg->ahash_alg.halg.base.cra_driver_name);\r\nkfree(t_alg);\r\n} else\r\nlist_add_tail(&t_alg->entry, &priv->hash_list);\r\nt_alg = caam_hash_alloc(ctrldev, &driver_hash[i], false);\r\nif (IS_ERR(t_alg)) {\r\nerr = PTR_ERR(t_alg);\r\ndev_warn(ctrldev, "%s alg allocation failed\n",\r\ndriver_hash[i].driver_name);\r\ncontinue;\r\n}\r\nerr = crypto_register_ahash(&t_alg->ahash_alg);\r\nif (err) {\r\ndev_warn(ctrldev, "%s alg registration failed\n",\r\nt_alg->ahash_alg.halg.base.cra_driver_name);\r\nkfree(t_alg);\r\n} else\r\nlist_add_tail(&t_alg->entry, &priv->hash_list);\r\n}\r\nreturn err;\r\n}
