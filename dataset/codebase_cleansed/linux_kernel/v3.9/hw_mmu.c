hw_status hw_mmu_enable(void __iomem *base_address)\r\n{\r\nhw_status status = 0;\r\nMMUMMU_CNTLMMU_ENABLE_WRITE32(base_address, HW_SET);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_disable(void __iomem *base_address)\r\n{\r\nhw_status status = 0;\r\nMMUMMU_CNTLMMU_ENABLE_WRITE32(base_address, HW_CLEAR);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_num_locked_set(void __iomem *base_address,\r\nu32 num_locked_entries)\r\n{\r\nhw_status status = 0;\r\nMMUMMU_LOCK_BASE_VALUE_WRITE32(base_address, num_locked_entries);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_victim_num_set(void __iomem *base_address,\r\nu32 victim_entry_num)\r\n{\r\nhw_status status = 0;\r\nMMUMMU_LOCK_CURRENT_VICTIM_WRITE32(base_address, victim_entry_num);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_event_ack(void __iomem *base_address, u32 irq_mask)\r\n{\r\nhw_status status = 0;\r\nMMUMMU_IRQSTATUS_WRITE_REGISTER32(base_address, irq_mask);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_event_disable(void __iomem *base_address, u32 irq_mask)\r\n{\r\nhw_status status = 0;\r\nu32 irq_reg;\r\nirq_reg = MMUMMU_IRQENABLE_READ_REGISTER32(base_address);\r\nMMUMMU_IRQENABLE_WRITE_REGISTER32(base_address, irq_reg & ~irq_mask);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_event_enable(void __iomem *base_address, u32 irq_mask)\r\n{\r\nhw_status status = 0;\r\nu32 irq_reg;\r\nirq_reg = MMUMMU_IRQENABLE_READ_REGISTER32(base_address);\r\nMMUMMU_IRQENABLE_WRITE_REGISTER32(base_address, irq_reg | irq_mask);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_event_status(void __iomem *base_address, u32 *irq_mask)\r\n{\r\nhw_status status = 0;\r\n*irq_mask = MMUMMU_IRQSTATUS_READ_REGISTER32(base_address);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_fault_addr_read(void __iomem *base_address, u32 *addr)\r\n{\r\nhw_status status = 0;\r\n*addr = MMUMMU_FAULT_AD_READ_REGISTER32(base_address);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_ttb_set(void __iomem *base_address, u32 ttb_phys_addr)\r\n{\r\nhw_status status = 0;\r\nu32 load_ttb;\r\nload_ttb = ttb_phys_addr & ~0x7FUL;\r\nMMUMMU_TTB_WRITE_REGISTER32(base_address, load_ttb);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_twl_enable(void __iomem *base_address)\r\n{\r\nhw_status status = 0;\r\nMMUMMU_CNTLTWL_ENABLE_WRITE32(base_address, HW_SET);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_twl_disable(void __iomem *base_address)\r\n{\r\nhw_status status = 0;\r\nMMUMMU_CNTLTWL_ENABLE_WRITE32(base_address, HW_CLEAR);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_tlb_add(void __iomem *base_address,\r\nu32 physical_addr,\r\nu32 virtual_addr,\r\nu32 page_sz,\r\nu32 entry_num,\r\nstruct hw_mmu_map_attrs_t *map_attrs,\r\ns8 preserved_bit, s8 valid_bit)\r\n{\r\nhw_status status = 0;\r\nu32 lock_reg;\r\nu32 virtual_addr_tag;\r\nenum hw_mmu_page_size_t mmu_pg_size;\r\nswitch (page_sz) {\r\ncase HW_PAGE_SIZE4KB:\r\nmmu_pg_size = HW_MMU_SMALL_PAGE;\r\nbreak;\r\ncase HW_PAGE_SIZE64KB:\r\nmmu_pg_size = HW_MMU_LARGE_PAGE;\r\nbreak;\r\ncase HW_PAGE_SIZE1MB:\r\nmmu_pg_size = HW_MMU_SECTION;\r\nbreak;\r\ncase HW_PAGE_SIZE16MB:\r\nmmu_pg_size = HW_MMU_SUPERSECTION;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nlock_reg = MMUMMU_LOCK_READ_REGISTER32(base_address);\r\nvirtual_addr_tag = ((virtual_addr & MMU_ADDR_MASK) >> 12);\r\nmmu_set_cam_entry(base_address, mmu_pg_size, preserved_bit, valid_bit,\r\nvirtual_addr_tag);\r\nmmu_set_ram_entry(base_address, physical_addr, map_attrs->endianism,\r\nmap_attrs->element_size, map_attrs->mixed_size);\r\nMMUMMU_LOCK_CURRENT_VICTIM_WRITE32(base_address, entry_num);\r\nMMUMMU_LD_TLB_WRITE_REGISTER32(base_address, MMU_LOAD_TLB);\r\nMMUMMU_LOCK_WRITE_REGISTER32(base_address, lock_reg);\r\nreturn status;\r\n}\r\nhw_status hw_mmu_pte_set(const u32 pg_tbl_va,\r\nu32 physical_addr,\r\nu32 virtual_addr,\r\nu32 page_sz, struct hw_mmu_map_attrs_t *map_attrs)\r\n{\r\nhw_status status = 0;\r\nu32 pte_addr, pte_val;\r\ns32 num_entries = 1;\r\nswitch (page_sz) {\r\ncase HW_PAGE_SIZE4KB:\r\npte_addr = hw_mmu_pte_addr_l2(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_SMALL_PAGE_MASK);\r\npte_val =\r\n((physical_addr & MMU_SMALL_PAGE_MASK) |\r\n(map_attrs->endianism << 9) | (map_attrs->\r\nelement_size << 4) |\r\n(map_attrs->mixed_size << 11) | 2);\r\nbreak;\r\ncase HW_PAGE_SIZE64KB:\r\nnum_entries = 16;\r\npte_addr = hw_mmu_pte_addr_l2(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_LARGE_PAGE_MASK);\r\npte_val =\r\n((physical_addr & MMU_LARGE_PAGE_MASK) |\r\n(map_attrs->endianism << 9) | (map_attrs->\r\nelement_size << 4) |\r\n(map_attrs->mixed_size << 11) | 1);\r\nbreak;\r\ncase HW_PAGE_SIZE1MB:\r\npte_addr = hw_mmu_pte_addr_l1(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_SECTION_ADDR_MASK);\r\npte_val =\r\n((((physical_addr & MMU_SECTION_ADDR_MASK) |\r\n(map_attrs->endianism << 15) | (map_attrs->\r\nelement_size << 10) |\r\n(map_attrs->mixed_size << 17)) & ~0x40000) | 0x2);\r\nbreak;\r\ncase HW_PAGE_SIZE16MB:\r\nnum_entries = 16;\r\npte_addr = hw_mmu_pte_addr_l1(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_SSECTION_ADDR_MASK);\r\npte_val =\r\n(((physical_addr & MMU_SSECTION_ADDR_MASK) |\r\n(map_attrs->endianism << 15) | (map_attrs->\r\nelement_size << 10) |\r\n(map_attrs->mixed_size << 17)\r\n) | 0x40000 | 0x2);\r\nbreak;\r\ncase HW_MMU_COARSE_PAGE_SIZE:\r\npte_addr = hw_mmu_pte_addr_l1(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_SECTION_ADDR_MASK);\r\npte_val = (physical_addr & MMU_PAGE_TABLE_MASK) | 1;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nwhile (--num_entries >= 0)\r\n((u32 *) pte_addr)[num_entries] = pte_val;\r\nreturn status;\r\n}\r\nhw_status hw_mmu_pte_clear(const u32 pg_tbl_va, u32 virtual_addr, u32 page_size)\r\n{\r\nhw_status status = 0;\r\nu32 pte_addr;\r\ns32 num_entries = 1;\r\nswitch (page_size) {\r\ncase HW_PAGE_SIZE4KB:\r\npte_addr = hw_mmu_pte_addr_l2(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_SMALL_PAGE_MASK);\r\nbreak;\r\ncase HW_PAGE_SIZE64KB:\r\nnum_entries = 16;\r\npte_addr = hw_mmu_pte_addr_l2(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_LARGE_PAGE_MASK);\r\nbreak;\r\ncase HW_PAGE_SIZE1MB:\r\ncase HW_MMU_COARSE_PAGE_SIZE:\r\npte_addr = hw_mmu_pte_addr_l1(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_SECTION_ADDR_MASK);\r\nbreak;\r\ncase HW_PAGE_SIZE16MB:\r\nnum_entries = 16;\r\npte_addr = hw_mmu_pte_addr_l1(pg_tbl_va,\r\nvirtual_addr &\r\nMMU_SSECTION_ADDR_MASK);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nwhile (--num_entries >= 0)\r\n((u32 *) pte_addr)[num_entries] = 0;\r\nreturn status;\r\n}\r\nstatic hw_status mmu_set_cam_entry(void __iomem *base_address,\r\nconst u32 page_sz,\r\nconst u32 preserved_bit,\r\nconst u32 valid_bit,\r\nconst u32 virtual_addr_tag)\r\n{\r\nhw_status status = 0;\r\nu32 mmu_cam_reg;\r\nmmu_cam_reg = (virtual_addr_tag << 12);\r\nmmu_cam_reg = (mmu_cam_reg) | (page_sz) | (valid_bit << 2) |\r\n(preserved_bit << 3);\r\nMMUMMU_CAM_WRITE_REGISTER32(base_address, mmu_cam_reg);\r\nreturn status;\r\n}\r\nstatic hw_status mmu_set_ram_entry(void __iomem *base_address,\r\nconst u32 physical_addr,\r\nenum hw_endianism_t endianism,\r\nenum hw_element_size_t element_size,\r\nenum hw_mmu_mixed_size_t mixed_size)\r\n{\r\nhw_status status = 0;\r\nu32 mmu_ram_reg;\r\nmmu_ram_reg = (physical_addr & MMU_ADDR_MASK);\r\nmmu_ram_reg = (mmu_ram_reg) | ((endianism << 9) | (element_size << 7) |\r\n(mixed_size << 6));\r\nMMUMMU_RAM_WRITE_REGISTER32(base_address, mmu_ram_reg);\r\nreturn status;\r\n}\r\nvoid hw_mmu_tlb_flush_all(void __iomem *base)\r\n{\r\n__raw_writel(1, base + MMU_GFLUSH);\r\n}
