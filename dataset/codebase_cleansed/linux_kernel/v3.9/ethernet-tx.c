static inline int32_t cvm_oct_adjust_skb_to_free(int32_t skb_to_free, int fau)\r\n{\r\nint32_t undo;\r\nundo = skb_to_free > 0 ? MAX_SKB_TO_FREE : skb_to_free + MAX_SKB_TO_FREE;\r\nif (undo > 0)\r\ncvmx_fau_atomic_add32(fau, -undo);\r\nskb_to_free = -skb_to_free > MAX_SKB_TO_FREE ? MAX_SKB_TO_FREE : -skb_to_free;\r\nreturn skb_to_free;\r\n}\r\nstatic void cvm_oct_kick_tx_poll_watchdog(void)\r\n{\r\nunion cvmx_ciu_timx ciu_timx;\r\nciu_timx.u64 = 0;\r\nciu_timx.s.one_shot = 1;\r\nciu_timx.s.len = cvm_oct_tx_poll_interval;\r\ncvmx_write_csr(CVMX_CIU_TIMX(1), ciu_timx.u64);\r\n}\r\nvoid cvm_oct_free_tx_skbs(struct net_device *dev)\r\n{\r\nint32_t skb_to_free;\r\nint qos, queues_per_port;\r\nint total_freed = 0;\r\nint total_remaining = 0;\r\nunsigned long flags;\r\nstruct octeon_ethernet *priv = netdev_priv(dev);\r\nqueues_per_port = cvmx_pko_get_num_queues(priv->port);\r\nfor (qos = 0; qos < queues_per_port; qos++) {\r\nif (skb_queue_len(&priv->tx_free_list[qos]) == 0)\r\ncontinue;\r\nskb_to_free = cvmx_fau_fetch_and_add32(priv->fau+qos*4, MAX_SKB_TO_FREE);\r\nskb_to_free = cvm_oct_adjust_skb_to_free(skb_to_free, priv->fau+qos*4);\r\ntotal_freed += skb_to_free;\r\nif (skb_to_free > 0) {\r\nstruct sk_buff *to_free_list = NULL;\r\nspin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);\r\nwhile (skb_to_free > 0) {\r\nstruct sk_buff *t = __skb_dequeue(&priv->tx_free_list[qos]);\r\nt->next = to_free_list;\r\nto_free_list = t;\r\nskb_to_free--;\r\n}\r\nspin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);\r\nwhile (to_free_list) {\r\nstruct sk_buff *t = to_free_list;\r\nto_free_list = to_free_list->next;\r\ndev_kfree_skb_any(t);\r\n}\r\n}\r\ntotal_remaining += skb_queue_len(&priv->tx_free_list[qos]);\r\n}\r\nif (total_freed >= 0 && netif_queue_stopped(dev))\r\nnetif_wake_queue(dev);\r\nif (total_remaining)\r\ncvm_oct_kick_tx_poll_watchdog();\r\n}\r\nint cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\ncvmx_pko_command_word0_t pko_command;\r\nunion cvmx_buf_ptr hw_buffer;\r\nuint64_t old_scratch;\r\nuint64_t old_scratch2;\r\nint qos;\r\nint i;\r\nenum {QUEUE_CORE, QUEUE_HW, QUEUE_DROP} queue_type;\r\nstruct octeon_ethernet *priv = netdev_priv(dev);\r\nstruct sk_buff *to_free_list;\r\nint32_t skb_to_free;\r\nint32_t buffers_to_free;\r\nu32 total_to_clean;\r\nunsigned long flags;\r\n#if REUSE_SKBUFFS_WITHOUT_FREE\r\nunsigned char *fpa_head;\r\n#endif\r\nprefetch(priv);\r\nif ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||\r\n(CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1)) {\r\nqos = GET_SKBUFF_QOS(skb);\r\nif (qos <= 0)\r\nqos = 0;\r\nelse if (qos >= cvmx_pko_get_num_queues(priv->port))\r\nqos = 0;\r\n} else\r\nqos = 0;\r\nif (USE_ASYNC_IOBDMA) {\r\nCVMX_SYNCIOBDMA;\r\nold_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);\r\nold_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);\r\ncvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,\r\nFAU_NUM_PACKET_BUFFERS_TO_FREE,\r\n0);\r\ncvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,\r\npriv->fau + qos * 4,\r\nMAX_SKB_TO_FREE);\r\n}\r\nif (unlikely(skb_shinfo(skb)->nr_frags > 5)) {\r\nif (unlikely(__skb_linearize(skb))) {\r\nqueue_type = QUEUE_DROP;\r\nif (USE_ASYNC_IOBDMA) {\r\nCVMX_SYNCIOBDMA;\r\nskb_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH);\r\n} else {\r\nskb_to_free = cvmx_fau_fetch_and_add32(priv->fau + qos * 4,\r\nMAX_SKB_TO_FREE);\r\n}\r\nskb_to_free = cvm_oct_adjust_skb_to_free(skb_to_free, priv->fau + qos * 4);\r\nspin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);\r\ngoto skip_xmit;\r\n}\r\n}\r\nif ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {\r\nunion cvmx_gmxx_prtx_cfg gmx_prt_cfg;\r\nint interface = INTERFACE(priv->port);\r\nint index = INDEX(priv->port);\r\nif (interface < 2) {\r\ngmx_prt_cfg.u64 =\r\ncvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));\r\nif (gmx_prt_cfg.s.duplex == 0) {\r\nint add_bytes = 64 - skb->len;\r\nif ((skb_tail_pointer(skb) + add_bytes) <=\r\nskb_end_pointer(skb))\r\nmemset(__skb_put(skb, add_bytes), 0,\r\nadd_bytes);\r\n}\r\n}\r\n}\r\npko_command.u64 = 0;\r\npko_command.s.n2 = 1;\r\npko_command.s.segs = 1;\r\npko_command.s.total_bytes = skb->len;\r\npko_command.s.size0 = CVMX_FAU_OP_SIZE_32;\r\npko_command.s.subone0 = 1;\r\npko_command.s.dontfree = 1;\r\nhw_buffer.u64 = 0;\r\nif (skb_shinfo(skb)->nr_frags == 0) {\r\nhw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);\r\nhw_buffer.s.pool = 0;\r\nhw_buffer.s.size = skb->len;\r\n} else {\r\nhw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);\r\nhw_buffer.s.pool = 0;\r\nhw_buffer.s.size = skb_headlen(skb);\r\nCVM_OCT_SKB_CB(skb)[0] = hw_buffer.u64;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nstruct skb_frag_struct *fs = skb_shinfo(skb)->frags + i;\r\nhw_buffer.s.addr = XKPHYS_TO_PHYS((u64)(page_address(fs->page.p) + fs->page_offset));\r\nhw_buffer.s.size = fs->size;\r\nCVM_OCT_SKB_CB(skb)[i + 1] = hw_buffer.u64;\r\n}\r\nhw_buffer.s.addr = XKPHYS_TO_PHYS((u64)CVM_OCT_SKB_CB(skb));\r\nhw_buffer.s.size = skb_shinfo(skb)->nr_frags + 1;\r\npko_command.s.segs = skb_shinfo(skb)->nr_frags + 1;\r\npko_command.s.gather = 1;\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\n#if REUSE_SKBUFFS_WITHOUT_FREE\r\nfpa_head = skb->head + 256 - ((unsigned long)skb->head & 0x7f);\r\nif (unlikely(skb->data < fpa_head)) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\nif (unlikely\r\n((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE)) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\nif (unlikely(skb_shared(skb))) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\nif (unlikely(skb_cloned(skb))) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\nif (unlikely(skb_header_cloned(skb))) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\nif (unlikely(skb->destructor)) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\nif (unlikely(skb_shinfo(skb)->nr_frags)) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\nif (unlikely\r\n(skb->truesize !=\r\nsizeof(*skb) + skb_end_offset(skb))) {\r\ngoto dont_put_skbuff_in_hw;\r\n}\r\npko_command.s.dontfree = 0;\r\nhw_buffer.s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);\r\n*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;\r\ndst_release(skb_dst(skb));\r\nskb_dst_set(skb, NULL);\r\n#ifdef CONFIG_XFRM\r\nsecpath_put(skb->sp);\r\nskb->sp = NULL;\r\n#endif\r\nnf_reset(skb);\r\n#ifdef CONFIG_NET_SCHED\r\nskb->tc_index = 0;\r\n#ifdef CONFIG_NET_CLS_ACT\r\nskb->tc_verd = 0;\r\n#endif\r\n#endif\r\n#endif\r\ndont_put_skbuff_in_hw:\r\nif (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&\r\n(ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&\r\n((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1 << 14))\r\n&& ((ip_hdr(skb)->protocol == IPPROTO_TCP)\r\n|| (ip_hdr(skb)->protocol == IPPROTO_UDP))) {\r\npko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;\r\n}\r\nif (USE_ASYNC_IOBDMA) {\r\nCVMX_SYNCIOBDMA;\r\nskb_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH);\r\nbuffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);\r\n} else {\r\nskb_to_free = cvmx_fau_fetch_and_add32(priv->fau + qos * 4,\r\nMAX_SKB_TO_FREE);\r\nbuffers_to_free =\r\ncvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);\r\n}\r\nskb_to_free = cvm_oct_adjust_skb_to_free(skb_to_free, priv->fau+qos*4);\r\nif ((buffers_to_free < -100) && !pko_command.s.dontfree)\r\npko_command.s.dontfree = 1;\r\nif (pko_command.s.dontfree) {\r\nqueue_type = QUEUE_CORE;\r\npko_command.s.reg0 = priv->fau+qos*4;\r\n} else {\r\nqueue_type = QUEUE_HW;\r\n}\r\nif (USE_ASYNC_IOBDMA)\r\ncvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH, FAU_TOTAL_TX_TO_CLEAN, 1);\r\nspin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);\r\nif (unlikely(skb_queue_len(&priv->tx_free_list[qos]) >= MAX_OUT_QUEUE_DEPTH)) {\r\nif (dev->tx_queue_len != 0) {\r\nspin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);\r\nnetif_stop_queue(dev);\r\nspin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);\r\n} else {\r\nqueue_type = QUEUE_DROP;\r\ngoto skip_xmit;\r\n}\r\n}\r\ncvmx_pko_send_packet_prepare(priv->port, priv->queue + qos,\r\nCVMX_PKO_LOCK_NONE);\r\nif (unlikely(cvmx_pko_send_packet_finish(priv->port,\r\npriv->queue + qos,\r\npko_command, hw_buffer,\r\nCVMX_PKO_LOCK_NONE))) {\r\nprintk_ratelimited("%s: Failed to send the packet\n", dev->name);\r\nqueue_type = QUEUE_DROP;\r\n}\r\nskip_xmit:\r\nto_free_list = NULL;\r\nswitch (queue_type) {\r\ncase QUEUE_DROP:\r\nskb->next = to_free_list;\r\nto_free_list = skb;\r\npriv->stats.tx_dropped++;\r\nbreak;\r\ncase QUEUE_HW:\r\ncvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -1);\r\nbreak;\r\ncase QUEUE_CORE:\r\n__skb_queue_tail(&priv->tx_free_list[qos], skb);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nwhile (skb_to_free > 0) {\r\nstruct sk_buff *t = __skb_dequeue(&priv->tx_free_list[qos]);\r\nt->next = to_free_list;\r\nto_free_list = t;\r\nskb_to_free--;\r\n}\r\nspin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);\r\nwhile (to_free_list) {\r\nstruct sk_buff *t = to_free_list;\r\nto_free_list = to_free_list->next;\r\ndev_kfree_skb_any(t);\r\n}\r\nif (USE_ASYNC_IOBDMA) {\r\nCVMX_SYNCIOBDMA;\r\ntotal_to_clean = cvmx_scratch_read64(CVMX_SCR_SCRATCH);\r\ncvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);\r\ncvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);\r\n} else {\r\ntotal_to_clean = cvmx_fau_fetch_and_add32(FAU_TOTAL_TX_TO_CLEAN, 1);\r\n}\r\nif (total_to_clean & 0x3ff) {\r\ntasklet_schedule(&cvm_oct_tx_cleanup_tasklet);\r\n}\r\ncvm_oct_kick_tx_poll_watchdog();\r\nreturn NETDEV_TX_OK;\r\n}\r\nint cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct octeon_ethernet *priv = netdev_priv(dev);\r\nvoid *packet_buffer;\r\nvoid *copy_location;\r\ncvmx_wqe_t *work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);\r\nif (unlikely(work == NULL)) {\r\nprintk_ratelimited("%s: Failed to allocate a work "\r\n"queue entry\n", dev->name);\r\npriv->stats.tx_dropped++;\r\ndev_kfree_skb(skb);\r\nreturn 0;\r\n}\r\npacket_buffer = cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL);\r\nif (unlikely(packet_buffer == NULL)) {\r\nprintk_ratelimited("%s: Failed to allocate a packet buffer\n",\r\ndev->name);\r\ncvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));\r\npriv->stats.tx_dropped++;\r\ndev_kfree_skb(skb);\r\nreturn 0;\r\n}\r\ncopy_location = packet_buffer + sizeof(uint64_t);\r\ncopy_location += ((CVMX_HELPER_FIRST_MBUFF_SKIP + 7) & 0xfff8) + 6;\r\nmemcpy(copy_location, skb->data, skb->len);\r\nwork->hw_chksum = skb->csum;\r\nwork->len = skb->len;\r\nwork->ipprt = priv->port;\r\nwork->qos = priv->port & 0x7;\r\nwork->grp = pow_send_group;\r\nwork->tag_type = CVMX_HELPER_INPUT_TAG_TYPE;\r\nwork->tag = pow_send_group;\r\nwork->word2.u64 = 0;\r\nwork->word2.s.bufs = 1;\r\nwork->packet_ptr.u64 = 0;\r\nwork->packet_ptr.s.addr = cvmx_ptr_to_phys(copy_location);\r\nwork->packet_ptr.s.pool = CVMX_FPA_PACKET_POOL;\r\nwork->packet_ptr.s.size = CVMX_FPA_PACKET_POOL_SIZE;\r\nwork->packet_ptr.s.back = (copy_location - packet_buffer) >> 7;\r\nif (skb->protocol == htons(ETH_P_IP)) {\r\nwork->word2.s.ip_offset = 14;\r\n#if 0\r\nwork->word2.s.vlan_valid = 0;\r\nwork->word2.s.vlan_cfi = 0;\r\nwork->word2.s.vlan_id = 0;\r\nwork->word2.s.dec_ipcomp = 0;\r\n#endif\r\nwork->word2.s.tcp_or_udp =\r\n(ip_hdr(skb)->protocol == IPPROTO_TCP)\r\n|| (ip_hdr(skb)->protocol == IPPROTO_UDP);\r\n#if 0\r\nwork->word2.s.dec_ipsec = 0;\r\nwork->word2.s.is_v6 = 0;\r\nwork->word2.s.software = 0;\r\nwork->word2.s.L4_error = 0;\r\n#endif\r\nwork->word2.s.is_frag = !((ip_hdr(skb)->frag_off == 0)\r\n|| (ip_hdr(skb)->frag_off ==\r\n1 << 14));\r\n#if 0\r\nwork->word2.s.IP_exc = 0;\r\n#endif\r\nwork->word2.s.is_bcast = (skb->pkt_type == PACKET_BROADCAST);\r\nwork->word2.s.is_mcast = (skb->pkt_type == PACKET_MULTICAST);\r\n#if 0\r\nwork->word2.s.not_IP = 0;\r\nwork->word2.s.rcv_error = 0;\r\nwork->word2.s.err_code = 0;\r\n#endif\r\nmemcpy(work->packet_data, skb->data + 10,\r\nsizeof(work->packet_data));\r\n} else {\r\n#if 0\r\nwork->word2.snoip.vlan_valid = 0;\r\nwork->word2.snoip.vlan_cfi = 0;\r\nwork->word2.snoip.vlan_id = 0;\r\nwork->word2.snoip.software = 0;\r\n#endif\r\nwork->word2.snoip.is_rarp = skb->protocol == htons(ETH_P_RARP);\r\nwork->word2.snoip.is_arp = skb->protocol == htons(ETH_P_ARP);\r\nwork->word2.snoip.is_bcast =\r\n(skb->pkt_type == PACKET_BROADCAST);\r\nwork->word2.snoip.is_mcast =\r\n(skb->pkt_type == PACKET_MULTICAST);\r\nwork->word2.snoip.not_IP = 1;\r\n#if 0\r\nwork->word2.snoip.rcv_error = 0;\r\nwork->word2.snoip.err_code = 0;\r\n#endif\r\nmemcpy(work->packet_data, skb->data, sizeof(work->packet_data));\r\n}\r\ncvmx_pow_work_submit(work, work->tag, work->tag_type, work->qos,\r\nwork->grp);\r\npriv->stats.tx_packets++;\r\npriv->stats.tx_bytes += skb->len;\r\ndev_kfree_skb(skb);\r\nreturn 0;\r\n}\r\nvoid cvm_oct_tx_shutdown_dev(struct net_device *dev)\r\n{\r\nstruct octeon_ethernet *priv = netdev_priv(dev);\r\nunsigned long flags;\r\nint qos;\r\nfor (qos = 0; qos < 16; qos++) {\r\nspin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);\r\nwhile (skb_queue_len(&priv->tx_free_list[qos]))\r\ndev_kfree_skb_any(__skb_dequeue\r\n(&priv->tx_free_list[qos]));\r\nspin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);\r\n}\r\n}\r\nstatic void cvm_oct_tx_do_cleanup(unsigned long arg)\r\n{\r\nint port;\r\nfor (port = 0; port < TOTAL_NUMBER_OF_PORTS; port++) {\r\nif (cvm_oct_device[port]) {\r\nstruct net_device *dev = cvm_oct_device[port];\r\ncvm_oct_free_tx_skbs(dev);\r\n}\r\n}\r\n}\r\nstatic irqreturn_t cvm_oct_tx_cleanup_watchdog(int cpl, void *dev_id)\r\n{\r\ncvmx_write_csr(CVMX_CIU_TIMX(1), 0);\r\ntasklet_schedule(&cvm_oct_tx_cleanup_tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nvoid cvm_oct_tx_initialize(void)\r\n{\r\nint i;\r\ncvmx_write_csr(CVMX_CIU_TIMX(1), 0);\r\ni = request_irq(OCTEON_IRQ_TIMER1,\r\ncvm_oct_tx_cleanup_watchdog, 0,\r\n"Ethernet", cvm_oct_device);\r\nif (i)\r\npanic("Could not acquire Ethernet IRQ %d\n", OCTEON_IRQ_TIMER1);\r\n}\r\nvoid cvm_oct_tx_shutdown(void)\r\n{\r\nfree_irq(OCTEON_IRQ_TIMER1, cvm_oct_device);\r\n}
