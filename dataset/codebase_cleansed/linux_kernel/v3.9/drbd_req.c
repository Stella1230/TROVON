static void _drbd_start_io_acct(struct drbd_conf *mdev, struct drbd_request *req, struct bio *bio)\r\n{\r\nconst int rw = bio_data_dir(bio);\r\nint cpu;\r\ncpu = part_stat_lock();\r\npart_round_stats(cpu, &mdev->vdisk->part0);\r\npart_stat_inc(cpu, &mdev->vdisk->part0, ios[rw]);\r\npart_stat_add(cpu, &mdev->vdisk->part0, sectors[rw], bio_sectors(bio));\r\n(void) cpu;\r\npart_inc_in_flight(&mdev->vdisk->part0, rw);\r\npart_stat_unlock();\r\n}\r\nstatic void _drbd_end_io_acct(struct drbd_conf *mdev, struct drbd_request *req)\r\n{\r\nint rw = bio_data_dir(req->master_bio);\r\nunsigned long duration = jiffies - req->start_time;\r\nint cpu;\r\ncpu = part_stat_lock();\r\npart_stat_add(cpu, &mdev->vdisk->part0, ticks[rw], duration);\r\npart_round_stats(cpu, &mdev->vdisk->part0);\r\npart_dec_in_flight(&mdev->vdisk->part0, rw);\r\npart_stat_unlock();\r\n}\r\nstatic struct drbd_request *drbd_req_new(struct drbd_conf *mdev,\r\nstruct bio *bio_src)\r\n{\r\nstruct drbd_request *req;\r\nreq = mempool_alloc(drbd_request_mempool, GFP_NOIO);\r\nif (!req)\r\nreturn NULL;\r\ndrbd_req_make_private_bio(req, bio_src);\r\nreq->rq_state = bio_data_dir(bio_src) == WRITE ? RQ_WRITE : 0;\r\nreq->w.mdev = mdev;\r\nreq->master_bio = bio_src;\r\nreq->epoch = 0;\r\ndrbd_clear_interval(&req->i);\r\nreq->i.sector = bio_src->bi_sector;\r\nreq->i.size = bio_src->bi_size;\r\nreq->i.local = true;\r\nreq->i.waiting = false;\r\nINIT_LIST_HEAD(&req->tl_requests);\r\nINIT_LIST_HEAD(&req->w.list);\r\natomic_set(&req->completion_ref, 1);\r\nkref_init(&req->kref);\r\nreturn req;\r\n}\r\nvoid drbd_req_destroy(struct kref *kref)\r\n{\r\nstruct drbd_request *req = container_of(kref, struct drbd_request, kref);\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nconst unsigned s = req->rq_state;\r\nif ((req->master_bio && !(s & RQ_POSTPONED)) ||\r\natomic_read(&req->completion_ref) ||\r\n(s & RQ_LOCAL_PENDING) ||\r\n((s & RQ_NET_MASK) && !(s & RQ_NET_DONE))) {\r\ndev_err(DEV, "drbd_req_destroy: Logic BUG rq_state = 0x%x, completion_ref = %d\n",\r\ns, atomic_read(&req->completion_ref));\r\nreturn;\r\n}\r\nlist_del_init(&req->tl_requests);\r\nif (s & RQ_WRITE) {\r\nif ((s & (RQ_POSTPONED|RQ_LOCAL_MASK|RQ_NET_MASK)) != RQ_POSTPONED) {\r\nif (!(s & RQ_NET_OK) || !(s & RQ_LOCAL_OK))\r\ndrbd_set_out_of_sync(mdev, req->i.sector, req->i.size);\r\nif ((s & RQ_NET_OK) && (s & RQ_LOCAL_OK) && (s & RQ_NET_SIS))\r\ndrbd_set_in_sync(mdev, req->i.sector, req->i.size);\r\n}\r\nif (s & RQ_IN_ACT_LOG) {\r\nif (get_ldev_if_state(mdev, D_FAILED)) {\r\ndrbd_al_complete_io(mdev, &req->i);\r\nput_ldev(mdev);\r\n} else if (__ratelimit(&drbd_ratelimit_state)) {\r\ndev_warn(DEV, "Should have called drbd_al_complete_io(, %llu, %u), "\r\n"but my Disk seems to have failed :(\n",\r\n(unsigned long long) req->i.sector, req->i.size);\r\n}\r\n}\r\n}\r\nmempool_free(req, drbd_request_mempool);\r\n}\r\nstatic void wake_all_senders(struct drbd_tconn *tconn) {\r\nwake_up(&tconn->sender_work.q_wait);\r\n}\r\nvoid start_new_tl_epoch(struct drbd_tconn *tconn)\r\n{\r\nif (tconn->current_tle_writes == 0)\r\nreturn;\r\ntconn->current_tle_writes = 0;\r\natomic_inc(&tconn->current_tle_nr);\r\nwake_all_senders(tconn);\r\n}\r\nvoid complete_master_bio(struct drbd_conf *mdev,\r\nstruct bio_and_error *m)\r\n{\r\nbio_endio(m->bio, m->error);\r\ndec_ap_bio(mdev);\r\n}\r\nstatic void drbd_remove_request_interval(struct rb_root *root,\r\nstruct drbd_request *req)\r\n{\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nstruct drbd_interval *i = &req->i;\r\ndrbd_remove_interval(root, i);\r\nif (i->waiting)\r\nwake_up(&mdev->misc_wait);\r\n}\r\nstatic\r\nvoid drbd_req_complete(struct drbd_request *req, struct bio_and_error *m)\r\n{\r\nconst unsigned s = req->rq_state;\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nint rw;\r\nint error, ok;\r\nif ((s & RQ_LOCAL_PENDING && !(s & RQ_LOCAL_ABORTED)) ||\r\n(s & RQ_NET_QUEUED) || (s & RQ_NET_PENDING) ||\r\n(s & RQ_COMPLETION_SUSP)) {\r\ndev_err(DEV, "drbd_req_complete: Logic BUG rq_state = 0x%x\n", s);\r\nreturn;\r\n}\r\nif (!req->master_bio) {\r\ndev_err(DEV, "drbd_req_complete: Logic BUG, master_bio == NULL!\n");\r\nreturn;\r\n}\r\nrw = bio_rw(req->master_bio);\r\nok = (s & RQ_LOCAL_OK) || (s & RQ_NET_OK);\r\nerror = PTR_ERR(req->private_bio);\r\nif (!drbd_interval_empty(&req->i)) {\r\nstruct rb_root *root;\r\nif (rw == WRITE)\r\nroot = &mdev->write_requests;\r\nelse\r\nroot = &mdev->read_requests;\r\ndrbd_remove_request_interval(root, req);\r\n} else if (!(s & RQ_POSTPONED))\r\nD_ASSERT((s & (RQ_NET_MASK & ~RQ_NET_DONE)) == 0);\r\nif (rw == WRITE &&\r\nreq->epoch == atomic_read(&mdev->tconn->current_tle_nr))\r\nstart_new_tl_epoch(mdev->tconn);\r\n_drbd_end_io_acct(mdev, req);\r\nif (!ok && rw == READ && !list_empty(&req->tl_requests))\r\nreq->rq_state |= RQ_POSTPONED;\r\nif (!(req->rq_state & RQ_POSTPONED)) {\r\nm->error = ok ? 0 : (error ?: -EIO);\r\nm->bio = req->master_bio;\r\nreq->master_bio = NULL;\r\n}\r\n}\r\nstatic int drbd_req_put_completion_ref(struct drbd_request *req, struct bio_and_error *m, int put)\r\n{\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nD_ASSERT(m || (req->rq_state & RQ_POSTPONED));\r\nif (!atomic_sub_and_test(put, &req->completion_ref))\r\nreturn 0;\r\ndrbd_req_complete(req, m);\r\nif (req->rq_state & RQ_POSTPONED) {\r\ndrbd_restart_request(req);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void mod_rq_state(struct drbd_request *req, struct bio_and_error *m,\r\nint clear, int set)\r\n{\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nunsigned s = req->rq_state;\r\nint c_put = 0;\r\nint k_put = 0;\r\nif (drbd_suspended(mdev) && !((s | clear) & RQ_COMPLETION_SUSP))\r\nset |= RQ_COMPLETION_SUSP;\r\nreq->rq_state &= ~clear;\r\nreq->rq_state |= set;\r\nif (req->rq_state == s)\r\nreturn;\r\nif (!(s & RQ_LOCAL_PENDING) && (set & RQ_LOCAL_PENDING))\r\natomic_inc(&req->completion_ref);\r\nif (!(s & RQ_NET_PENDING) && (set & RQ_NET_PENDING)) {\r\ninc_ap_pending(mdev);\r\natomic_inc(&req->completion_ref);\r\n}\r\nif (!(s & RQ_NET_QUEUED) && (set & RQ_NET_QUEUED))\r\natomic_inc(&req->completion_ref);\r\nif (!(s & RQ_EXP_BARR_ACK) && (set & RQ_EXP_BARR_ACK))\r\nkref_get(&req->kref);\r\nif (!(s & RQ_NET_SENT) && (set & RQ_NET_SENT))\r\natomic_add(req->i.size >> 9, &mdev->ap_in_flight);\r\nif (!(s & RQ_COMPLETION_SUSP) && (set & RQ_COMPLETION_SUSP))\r\natomic_inc(&req->completion_ref);\r\nif ((s & RQ_COMPLETION_SUSP) && (clear & RQ_COMPLETION_SUSP))\r\n++c_put;\r\nif (!(s & RQ_LOCAL_ABORTED) && (set & RQ_LOCAL_ABORTED)) {\r\nD_ASSERT(req->rq_state & RQ_LOCAL_PENDING);\r\nkref_get(&req->kref);\r\n++c_put;\r\n}\r\nif ((s & RQ_LOCAL_PENDING) && (clear & RQ_LOCAL_PENDING)) {\r\nif (req->rq_state & RQ_LOCAL_ABORTED)\r\n++k_put;\r\nelse\r\n++c_put;\r\n}\r\nif ((s & RQ_NET_PENDING) && (clear & RQ_NET_PENDING)) {\r\ndec_ap_pending(mdev);\r\n++c_put;\r\n}\r\nif ((s & RQ_NET_QUEUED) && (clear & RQ_NET_QUEUED))\r\n++c_put;\r\nif ((s & RQ_EXP_BARR_ACK) && !(s & RQ_NET_DONE) && (set & RQ_NET_DONE)) {\r\nif (req->rq_state & RQ_NET_SENT)\r\natomic_sub(req->i.size >> 9, &mdev->ap_in_flight);\r\n++k_put;\r\n}\r\nif (k_put || c_put) {\r\nint at_least = k_put + !!c_put;\r\nint refcount = atomic_read(&req->kref.refcount);\r\nif (refcount < at_least)\r\ndev_err(DEV,\r\n"mod_rq_state: Logic BUG: %x -> %x: refcount = %d, should be >= %d\n",\r\ns, req->rq_state, refcount, at_least);\r\n}\r\nif (req->i.waiting)\r\nwake_up(&mdev->misc_wait);\r\nif (c_put)\r\nk_put += drbd_req_put_completion_ref(req, m, c_put);\r\nif (k_put)\r\nkref_sub(&req->kref, k_put, drbd_req_destroy);\r\n}\r\nstatic void drbd_report_io_error(struct drbd_conf *mdev, struct drbd_request *req)\r\n{\r\nchar b[BDEVNAME_SIZE];\r\nif (!__ratelimit(&drbd_ratelimit_state))\r\nreturn;\r\ndev_warn(DEV, "local %s IO error sector %llu+%u on %s\n",\r\n(req->rq_state & RQ_WRITE) ? "WRITE" : "READ",\r\n(unsigned long long)req->i.sector,\r\nreq->i.size >> 9,\r\nbdevname(mdev->ldev->backing_bdev, b));\r\n}\r\nint __req_mod(struct drbd_request *req, enum drbd_req_event what,\r\nstruct bio_and_error *m)\r\n{\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nstruct net_conf *nc;\r\nint p, rv = 0;\r\nif (m)\r\nm->bio = NULL;\r\nswitch (what) {\r\ndefault:\r\ndev_err(DEV, "LOGIC BUG in %s:%u\n", __FILE__ , __LINE__);\r\nbreak;\r\ncase TO_BE_SENT:\r\nD_ASSERT(!(req->rq_state & RQ_NET_MASK));\r\nrcu_read_lock();\r\nnc = rcu_dereference(mdev->tconn->net_conf);\r\np = nc->wire_protocol;\r\nrcu_read_unlock();\r\nreq->rq_state |=\r\np == DRBD_PROT_C ? RQ_EXP_WRITE_ACK :\r\np == DRBD_PROT_B ? RQ_EXP_RECEIVE_ACK : 0;\r\nmod_rq_state(req, m, 0, RQ_NET_PENDING);\r\nbreak;\r\ncase TO_BE_SUBMITTED:\r\nD_ASSERT(!(req->rq_state & RQ_LOCAL_MASK));\r\nmod_rq_state(req, m, 0, RQ_LOCAL_PENDING);\r\nbreak;\r\ncase COMPLETED_OK:\r\nif (req->rq_state & RQ_WRITE)\r\nmdev->writ_cnt += req->i.size >> 9;\r\nelse\r\nmdev->read_cnt += req->i.size >> 9;\r\nmod_rq_state(req, m, RQ_LOCAL_PENDING,\r\nRQ_LOCAL_COMPLETED|RQ_LOCAL_OK);\r\nbreak;\r\ncase ABORT_DISK_IO:\r\nmod_rq_state(req, m, 0, RQ_LOCAL_ABORTED);\r\nbreak;\r\ncase WRITE_COMPLETED_WITH_ERROR:\r\ndrbd_report_io_error(mdev, req);\r\n__drbd_chk_io_error(mdev, DRBD_WRITE_ERROR);\r\nmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\r\nbreak;\r\ncase READ_COMPLETED_WITH_ERROR:\r\ndrbd_set_out_of_sync(mdev, req->i.sector, req->i.size);\r\ndrbd_report_io_error(mdev, req);\r\n__drbd_chk_io_error(mdev, DRBD_READ_ERROR);\r\ncase READ_AHEAD_COMPLETED_WITH_ERROR:\r\nmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\r\nbreak;\r\ncase QUEUE_FOR_NET_READ:\r\nD_ASSERT(drbd_interval_empty(&req->i));\r\ndrbd_insert_interval(&mdev->read_requests, &req->i);\r\nset_bit(UNPLUG_REMOTE, &mdev->flags);\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nD_ASSERT((req->rq_state & RQ_LOCAL_MASK) == 0);\r\nmod_rq_state(req, m, 0, RQ_NET_QUEUED);\r\nreq->w.cb = w_send_read_req;\r\ndrbd_queue_work(&mdev->tconn->sender_work, &req->w);\r\nbreak;\r\ncase QUEUE_FOR_NET_WRITE:\r\nD_ASSERT(drbd_interval_empty(&req->i));\r\ndrbd_insert_interval(&mdev->write_requests, &req->i);\r\nset_bit(UNPLUG_REMOTE, &mdev->flags);\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nmod_rq_state(req, m, 0, RQ_NET_QUEUED|RQ_EXP_BARR_ACK);\r\nreq->w.cb = w_send_dblock;\r\ndrbd_queue_work(&mdev->tconn->sender_work, &req->w);\r\nrcu_read_lock();\r\nnc = rcu_dereference(mdev->tconn->net_conf);\r\np = nc->max_epoch_size;\r\nrcu_read_unlock();\r\nif (mdev->tconn->current_tle_writes >= p)\r\nstart_new_tl_epoch(mdev->tconn);\r\nbreak;\r\ncase QUEUE_FOR_SEND_OOS:\r\nmod_rq_state(req, m, 0, RQ_NET_QUEUED);\r\nreq->w.cb = w_send_out_of_sync;\r\ndrbd_queue_work(&mdev->tconn->sender_work, &req->w);\r\nbreak;\r\ncase READ_RETRY_REMOTE_CANCELED:\r\ncase SEND_CANCELED:\r\ncase SEND_FAILED:\r\nmod_rq_state(req, m, RQ_NET_QUEUED, 0);\r\nbreak;\r\ncase HANDED_OVER_TO_NETWORK:\r\nif (bio_data_dir(req->master_bio) == WRITE &&\r\n!(req->rq_state & (RQ_EXP_RECEIVE_ACK | RQ_EXP_WRITE_ACK))) {\r\nif (req->rq_state & RQ_NET_PENDING)\r\nmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_OK);\r\n}\r\nmod_rq_state(req, m, RQ_NET_QUEUED, RQ_NET_SENT);\r\nbreak;\r\ncase OOS_HANDED_TO_NETWORK:\r\nmod_rq_state(req, m, RQ_NET_QUEUED, RQ_NET_DONE);\r\nbreak;\r\ncase CONNECTION_LOST_WHILE_PENDING:\r\nmod_rq_state(req, m,\r\nRQ_NET_OK|RQ_NET_PENDING|RQ_COMPLETION_SUSP,\r\nRQ_NET_DONE);\r\nbreak;\r\ncase CONFLICT_RESOLVED:\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nD_ASSERT(req->rq_state & RQ_EXP_WRITE_ACK);\r\nmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_DONE|RQ_NET_OK);\r\nbreak;\r\ncase WRITE_ACKED_BY_PEER_AND_SIS:\r\nreq->rq_state |= RQ_NET_SIS;\r\ncase WRITE_ACKED_BY_PEER:\r\nD_ASSERT(req->rq_state & RQ_EXP_WRITE_ACK);\r\ngoto ack_common;\r\ncase RECV_ACKED_BY_PEER:\r\nD_ASSERT(req->rq_state & RQ_EXP_RECEIVE_ACK);\r\nack_common:\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_OK);\r\nbreak;\r\ncase POSTPONE_WRITE:\r\nD_ASSERT(req->rq_state & RQ_EXP_WRITE_ACK);\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nreq->rq_state |= RQ_POSTPONED;\r\nif (req->i.waiting)\r\nwake_up(&mdev->misc_wait);\r\nbreak;\r\ncase NEG_ACKED:\r\nmod_rq_state(req, m, RQ_NET_OK|RQ_NET_PENDING, 0);\r\nbreak;\r\ncase FAIL_FROZEN_DISK_IO:\r\nif (!(req->rq_state & RQ_LOCAL_COMPLETED))\r\nbreak;\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP, 0);\r\nbreak;\r\ncase RESTART_FROZEN_DISK_IO:\r\nif (!(req->rq_state & RQ_LOCAL_COMPLETED))\r\nbreak;\r\nmod_rq_state(req, m,\r\nRQ_COMPLETION_SUSP|RQ_LOCAL_COMPLETED,\r\nRQ_LOCAL_PENDING);\r\nrv = MR_READ;\r\nif (bio_data_dir(req->master_bio) == WRITE)\r\nrv = MR_WRITE;\r\nget_ldev(mdev);\r\nreq->w.cb = w_restart_disk_io;\r\ndrbd_queue_work(&mdev->tconn->sender_work, &req->w);\r\nbreak;\r\ncase RESEND:\r\nif (!(req->rq_state & RQ_WRITE) && !req->w.cb) {\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP, 0);\r\nbreak;\r\n}\r\nif (!(req->rq_state & RQ_NET_OK)) {\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP, RQ_NET_QUEUED|RQ_NET_PENDING);\r\nif (req->w.cb) {\r\ndrbd_queue_work(&mdev->tconn->sender_work, &req->w);\r\nrv = req->rq_state & RQ_WRITE ? MR_WRITE : MR_READ;\r\n}\r\nbreak;\r\n}\r\ncase BARRIER_ACKED:\r\nif (!(req->rq_state & RQ_WRITE))\r\nbreak;\r\nif (req->rq_state & RQ_NET_PENDING) {\r\ndev_err(DEV, "FIXME (BARRIER_ACKED but pending)\n");\r\n}\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP,\r\n(req->rq_state & RQ_NET_MASK) ? RQ_NET_DONE : 0);\r\nbreak;\r\ncase DATA_RECEIVED:\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_OK|RQ_NET_DONE);\r\nbreak;\r\n};\r\nreturn rv;\r\n}\r\nstatic bool drbd_may_do_local_read(struct drbd_conf *mdev, sector_t sector, int size)\r\n{\r\nunsigned long sbnr, ebnr;\r\nsector_t esector, nr_sectors;\r\nif (mdev->state.disk == D_UP_TO_DATE)\r\nreturn true;\r\nif (mdev->state.disk != D_INCONSISTENT)\r\nreturn false;\r\nesector = sector + (size >> 9) - 1;\r\nnr_sectors = drbd_get_capacity(mdev->this_bdev);\r\nD_ASSERT(sector < nr_sectors);\r\nD_ASSERT(esector < nr_sectors);\r\nsbnr = BM_SECT_TO_BIT(sector);\r\nebnr = BM_SECT_TO_BIT(esector);\r\nreturn drbd_bm_count_bits(mdev, sbnr, ebnr) == 0;\r\n}\r\nstatic bool remote_due_to_read_balancing(struct drbd_conf *mdev, sector_t sector,\r\nenum drbd_read_balancing rbm)\r\n{\r\nstruct backing_dev_info *bdi;\r\nint stripe_shift;\r\nswitch (rbm) {\r\ncase RB_CONGESTED_REMOTE:\r\nbdi = &mdev->ldev->backing_bdev->bd_disk->queue->backing_dev_info;\r\nreturn bdi_read_congested(bdi);\r\ncase RB_LEAST_PENDING:\r\nreturn atomic_read(&mdev->local_cnt) >\r\natomic_read(&mdev->ap_pending_cnt) + atomic_read(&mdev->rs_pending_cnt);\r\ncase RB_32K_STRIPING:\r\ncase RB_64K_STRIPING:\r\ncase RB_128K_STRIPING:\r\ncase RB_256K_STRIPING:\r\ncase RB_512K_STRIPING:\r\ncase RB_1M_STRIPING:\r\nstripe_shift = (rbm - RB_32K_STRIPING + 15);\r\nreturn (sector >> (stripe_shift - 9)) & 1;\r\ncase RB_ROUND_ROBIN:\r\nreturn test_and_change_bit(READ_BALANCE_RR, &mdev->flags);\r\ncase RB_PREFER_REMOTE:\r\nreturn true;\r\ncase RB_PREFER_LOCAL:\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic void complete_conflicting_writes(struct drbd_request *req)\r\n{\r\nDEFINE_WAIT(wait);\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nstruct drbd_interval *i;\r\nsector_t sector = req->i.sector;\r\nint size = req->i.size;\r\ni = drbd_find_overlap(&mdev->write_requests, sector, size);\r\nif (!i)\r\nreturn;\r\nfor (;;) {\r\nprepare_to_wait(&mdev->misc_wait, &wait, TASK_UNINTERRUPTIBLE);\r\ni = drbd_find_overlap(&mdev->write_requests, sector, size);\r\nif (!i)\r\nbreak;\r\ni->waiting = true;\r\nspin_unlock_irq(&mdev->tconn->req_lock);\r\nschedule();\r\nspin_lock_irq(&mdev->tconn->req_lock);\r\n}\r\nfinish_wait(&mdev->misc_wait, &wait);\r\n}\r\nstatic void maybe_pull_ahead(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_tconn *tconn = mdev->tconn;\r\nstruct net_conf *nc;\r\nbool congested = false;\r\nenum drbd_on_congestion on_congestion;\r\nnc = rcu_dereference(tconn->net_conf);\r\non_congestion = nc ? nc->on_congestion : OC_BLOCK;\r\nif (on_congestion == OC_BLOCK ||\r\ntconn->agreed_pro_version < 96)\r\nreturn;\r\nif (!get_ldev_if_state(mdev, D_UP_TO_DATE))\r\nreturn;\r\nif (nc->cong_fill &&\r\natomic_read(&mdev->ap_in_flight) >= nc->cong_fill) {\r\ndev_info(DEV, "Congestion-fill threshold reached\n");\r\ncongested = true;\r\n}\r\nif (mdev->act_log->used >= nc->cong_extents) {\r\ndev_info(DEV, "Congestion-extents threshold reached\n");\r\ncongested = true;\r\n}\r\nif (congested) {\r\nstart_new_tl_epoch(mdev->tconn);\r\nif (on_congestion == OC_PULL_AHEAD)\r\n_drbd_set_state(_NS(mdev, conn, C_AHEAD), 0, NULL);\r\nelse\r\n_drbd_set_state(_NS(mdev, conn, C_DISCONNECTING), 0, NULL);\r\n}\r\nput_ldev(mdev);\r\n}\r\nstatic bool do_remote_read(struct drbd_request *req)\r\n{\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nenum drbd_read_balancing rbm;\r\nif (req->private_bio) {\r\nif (!drbd_may_do_local_read(mdev,\r\nreq->i.sector, req->i.size)) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(mdev);\r\n}\r\n}\r\nif (mdev->state.pdsk != D_UP_TO_DATE)\r\nreturn false;\r\nif (req->private_bio == NULL)\r\nreturn true;\r\nrcu_read_lock();\r\nrbm = rcu_dereference(mdev->ldev->disk_conf)->read_balancing;\r\nrcu_read_unlock();\r\nif (rbm == RB_PREFER_LOCAL && req->private_bio)\r\nreturn false;\r\nif (remote_due_to_read_balancing(mdev, req->i.sector, rbm)) {\r\nif (req->private_bio) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(mdev);\r\n}\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int drbd_process_write_request(struct drbd_request *req)\r\n{\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nint remote, send_oos;\r\nrcu_read_lock();\r\nremote = drbd_should_do_remote(mdev->state);\r\nif (remote) {\r\nmaybe_pull_ahead(mdev);\r\nremote = drbd_should_do_remote(mdev->state);\r\n}\r\nsend_oos = drbd_should_send_out_of_sync(mdev->state);\r\nrcu_read_unlock();\r\nif (unlikely(req->i.size == 0)) {\r\nD_ASSERT(req->master_bio->bi_rw & REQ_FLUSH);\r\nif (remote)\r\nstart_new_tl_epoch(mdev->tconn);\r\nreturn 0;\r\n}\r\nif (!remote && !send_oos)\r\nreturn 0;\r\nD_ASSERT(!(remote && send_oos));\r\nif (remote) {\r\n_req_mod(req, TO_BE_SENT);\r\n_req_mod(req, QUEUE_FOR_NET_WRITE);\r\n} else if (drbd_set_out_of_sync(mdev, req->i.sector, req->i.size))\r\n_req_mod(req, QUEUE_FOR_SEND_OOS);\r\nreturn remote;\r\n}\r\nstatic void\r\ndrbd_submit_req_private_bio(struct drbd_request *req)\r\n{\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nstruct bio *bio = req->private_bio;\r\nconst int rw = bio_rw(bio);\r\nbio->bi_bdev = mdev->ldev->backing_bdev;\r\nif (get_ldev(mdev)) {\r\nif (drbd_insert_fault(mdev,\r\nrw == WRITE ? DRBD_FAULT_DT_WR\r\n: rw == READ ? DRBD_FAULT_DT_RD\r\n: DRBD_FAULT_DT_RA))\r\nbio_endio(bio, -EIO);\r\nelse\r\ngeneric_make_request(bio);\r\nput_ldev(mdev);\r\n} else\r\nbio_endio(bio, -EIO);\r\n}\r\nvoid __drbd_make_request(struct drbd_conf *mdev, struct bio *bio, unsigned long start_time)\r\n{\r\nconst int rw = bio_rw(bio);\r\nstruct bio_and_error m = { NULL, };\r\nstruct drbd_request *req;\r\nbool no_remote = false;\r\nreq = drbd_req_new(mdev, bio);\r\nif (!req) {\r\ndec_ap_bio(mdev);\r\ndev_err(DEV, "could not kmalloc() req\n");\r\nbio_endio(bio, -ENOMEM);\r\nreturn;\r\n}\r\nreq->start_time = start_time;\r\nif (!get_ldev(mdev)) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\n}\r\nif (rw == WRITE && req->private_bio && req->i.size\r\n&& !test_bit(AL_SUSPENDED, &mdev->flags)) {\r\nreq->rq_state |= RQ_IN_ACT_LOG;\r\ndrbd_al_begin_io(mdev, &req->i);\r\n}\r\nspin_lock_irq(&mdev->tconn->req_lock);\r\nif (rw == WRITE) {\r\ncomplete_conflicting_writes(req);\r\n}\r\nif (drbd_suspended(mdev)) {\r\nreq->rq_state |= RQ_POSTPONED;\r\nif (req->private_bio) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(mdev);\r\n}\r\ngoto out;\r\n}\r\n_drbd_start_io_acct(mdev, req, bio);\r\nif (rw != WRITE) {\r\nif (!do_remote_read(req) && !req->private_bio)\r\ngoto nodata;\r\n}\r\nreq->epoch = atomic_read(&mdev->tconn->current_tle_nr);\r\nif (likely(req->i.size!=0)) {\r\nif (rw == WRITE)\r\nmdev->tconn->current_tle_writes++;\r\nlist_add_tail(&req->tl_requests, &mdev->tconn->transfer_log);\r\n}\r\nif (rw == WRITE) {\r\nif (!drbd_process_write_request(req))\r\nno_remote = true;\r\n} else {\r\nif (req->private_bio == NULL) {\r\n_req_mod(req, TO_BE_SENT);\r\n_req_mod(req, QUEUE_FOR_NET_READ);\r\n} else\r\nno_remote = true;\r\n}\r\nif (req->private_bio) {\r\n_req_mod(req, TO_BE_SUBMITTED);\r\nspin_unlock_irq(&mdev->tconn->req_lock);\r\ndrbd_submit_req_private_bio(req);\r\nspin_lock_irq(&mdev->tconn->req_lock);\r\n} else if (no_remote) {\r\nnodata:\r\nif (__ratelimit(&drbd_ratelimit_state))\r\ndev_err(DEV, "IO ERROR: neither local nor remote data, sector %llu+%u\n",\r\n(unsigned long long)req->i.sector, req->i.size >> 9);\r\n}\r\nout:\r\nif (drbd_req_put_completion_ref(req, &m, 1))\r\nkref_put(&req->kref, drbd_req_destroy);\r\nspin_unlock_irq(&mdev->tconn->req_lock);\r\nif (m.bio)\r\ncomplete_master_bio(mdev, &m);\r\nreturn;\r\n}\r\nvoid drbd_make_request(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *) q->queuedata;\r\nunsigned long start_time;\r\nstart_time = jiffies;\r\nD_ASSERT(IS_ALIGNED(bio->bi_size, 512));\r\ninc_ap_bio(mdev);\r\n__drbd_make_request(mdev, bio, start_time);\r\n}\r\nint drbd_merge_bvec(struct request_queue *q, struct bvec_merge_data *bvm, struct bio_vec *bvec)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *) q->queuedata;\r\nunsigned int bio_size = bvm->bi_size;\r\nint limit = DRBD_MAX_BIO_SIZE;\r\nint backing_limit;\r\nif (bio_size && get_ldev(mdev)) {\r\nstruct request_queue * const b =\r\nmdev->ldev->backing_bdev->bd_disk->queue;\r\nif (b->merge_bvec_fn) {\r\nbacking_limit = b->merge_bvec_fn(b, bvm, bvec);\r\nlimit = min(limit, backing_limit);\r\n}\r\nput_ldev(mdev);\r\n}\r\nreturn limit;\r\n}\r\nstruct drbd_request *find_oldest_request(struct drbd_tconn *tconn)\r\n{\r\nstruct drbd_request *r;\r\nlist_for_each_entry(r, &tconn->transfer_log, tl_requests) {\r\nif (atomic_read(&r->completion_ref))\r\nreturn r;\r\n}\r\nreturn NULL;\r\n}\r\nvoid request_timer_fn(unsigned long data)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *) data;\r\nstruct drbd_tconn *tconn = mdev->tconn;\r\nstruct drbd_request *req;\r\nstruct net_conf *nc;\r\nunsigned long ent = 0, dt = 0, et, nt;\r\nunsigned long now;\r\nrcu_read_lock();\r\nnc = rcu_dereference(tconn->net_conf);\r\nif (nc && mdev->state.conn >= C_WF_REPORT_PARAMS)\r\nent = nc->timeout * HZ/10 * nc->ko_count;\r\nif (get_ldev(mdev)) {\r\ndt = rcu_dereference(mdev->ldev->disk_conf)->disk_timeout * HZ / 10;\r\nput_ldev(mdev);\r\n}\r\nrcu_read_unlock();\r\net = min_not_zero(dt, ent);\r\nif (!et)\r\nreturn;\r\nnow = jiffies;\r\nspin_lock_irq(&tconn->req_lock);\r\nreq = find_oldest_request(tconn);\r\nif (!req) {\r\nspin_unlock_irq(&tconn->req_lock);\r\nmod_timer(&mdev->request_timer, now + et);\r\nreturn;\r\n}\r\nif (ent && req->rq_state & RQ_NET_PENDING &&\r\ntime_after(now, req->start_time + ent) &&\r\n!time_in_range(now, tconn->last_reconnect_jif, tconn->last_reconnect_jif + ent)) {\r\ndev_warn(DEV, "Remote failed to finish a request within ko-count * timeout\n");\r\n_drbd_set_state(_NS(mdev, conn, C_TIMEOUT), CS_VERBOSE | CS_HARD, NULL);\r\n}\r\nif (dt && req->rq_state & RQ_LOCAL_PENDING && req->w.mdev == mdev &&\r\ntime_after(now, req->start_time + dt) &&\r\n!time_in_range(now, mdev->last_reattach_jif, mdev->last_reattach_jif + dt)) {\r\ndev_warn(DEV, "Local backing device failed to meet the disk-timeout\n");\r\n__drbd_chk_io_error(mdev, DRBD_FORCE_DETACH);\r\n}\r\nnt = (time_after(now, req->start_time + et) ? now : req->start_time) + et;\r\nspin_unlock_irq(&tconn->req_lock);\r\nmod_timer(&mdev->request_timer, nt);\r\n}
