static inline u64 async_mask(struct mthca_dev *dev)\r\n{\r\nreturn dev->mthca_flags & MTHCA_FLAG_SRQ ?\r\nMTHCA_ASYNC_EVENT_MASK | MTHCA_SRQ_EVENT_MASK :\r\nMTHCA_ASYNC_EVENT_MASK;\r\n}\r\nstatic inline void tavor_set_eq_ci(struct mthca_dev *dev, struct mthca_eq *eq, u32 ci)\r\n{\r\nwmb();\r\nmthca_write64(MTHCA_EQ_DB_SET_CI | eq->eqn, ci & (eq->nent - 1),\r\ndev->kar + MTHCA_EQ_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\n}\r\nstatic inline void arbel_set_eq_ci(struct mthca_dev *dev, struct mthca_eq *eq, u32 ci)\r\n{\r\nwmb();\r\n__raw_writel((__force u32) cpu_to_be32(ci),\r\ndev->eq_regs.arbel.eq_set_ci_base + eq->eqn * 8);\r\nmb();\r\n}\r\nstatic inline void set_eq_ci(struct mthca_dev *dev, struct mthca_eq *eq, u32 ci)\r\n{\r\nif (mthca_is_memfree(dev))\r\narbel_set_eq_ci(dev, eq, ci);\r\nelse\r\ntavor_set_eq_ci(dev, eq, ci);\r\n}\r\nstatic inline void tavor_eq_req_not(struct mthca_dev *dev, int eqn)\r\n{\r\nmthca_write64(MTHCA_EQ_DB_REQ_NOT | eqn, 0,\r\ndev->kar + MTHCA_EQ_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\n}\r\nstatic inline void arbel_eq_req_not(struct mthca_dev *dev, u32 eqn_mask)\r\n{\r\nwritel(eqn_mask, dev->eq_regs.arbel.eq_arm);\r\n}\r\nstatic inline void disarm_cq(struct mthca_dev *dev, int eqn, int cqn)\r\n{\r\nif (!mthca_is_memfree(dev)) {\r\nmthca_write64(MTHCA_EQ_DB_DISARM_CQ | eqn, cqn,\r\ndev->kar + MTHCA_EQ_DOORBELL,\r\nMTHCA_GET_DOORBELL_LOCK(&dev->doorbell_lock));\r\n}\r\n}\r\nstatic inline struct mthca_eqe *get_eqe(struct mthca_eq *eq, u32 entry)\r\n{\r\nunsigned long off = (entry & (eq->nent - 1)) * MTHCA_EQ_ENTRY_SIZE;\r\nreturn eq->page_list[off / PAGE_SIZE].buf + off % PAGE_SIZE;\r\n}\r\nstatic inline struct mthca_eqe *next_eqe_sw(struct mthca_eq *eq)\r\n{\r\nstruct mthca_eqe *eqe;\r\neqe = get_eqe(eq, eq->cons_index);\r\nreturn (MTHCA_EQ_ENTRY_OWNER_HW & eqe->owner) ? NULL : eqe;\r\n}\r\nstatic inline void set_eqe_hw(struct mthca_eqe *eqe)\r\n{\r\neqe->owner = MTHCA_EQ_ENTRY_OWNER_HW;\r\n}\r\nstatic void port_change(struct mthca_dev *dev, int port, int active)\r\n{\r\nstruct ib_event record;\r\nmthca_dbg(dev, "Port change to %s for port %d\n",\r\nactive ? "active" : "down", port);\r\nrecord.device = &dev->ib_dev;\r\nrecord.event = active ? IB_EVENT_PORT_ACTIVE : IB_EVENT_PORT_ERR;\r\nrecord.element.port_num = port;\r\nib_dispatch_event(&record);\r\n}\r\nstatic int mthca_eq_int(struct mthca_dev *dev, struct mthca_eq *eq)\r\n{\r\nstruct mthca_eqe *eqe;\r\nint disarm_cqn;\r\nint eqes_found = 0;\r\nint set_ci = 0;\r\nwhile ((eqe = next_eqe_sw(eq))) {\r\nrmb();\r\nswitch (eqe->type) {\r\ncase MTHCA_EVENT_TYPE_COMP:\r\ndisarm_cqn = be32_to_cpu(eqe->event.comp.cqn) & 0xffffff;\r\ndisarm_cq(dev, eq->eqn, disarm_cqn);\r\nmthca_cq_completion(dev, disarm_cqn);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_PATH_MIG:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_PATH_MIG);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_COMM_EST:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_COMM_EST);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_SQ_DRAINED:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_SQ_DRAINED);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_SRQ_QP_LAST_WQE:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_QP_LAST_WQE_REACHED);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_SRQ_LIMIT:\r\nmthca_srq_event(dev, be32_to_cpu(eqe->event.srq.srqn) & 0xffffff,\r\nIB_EVENT_SRQ_LIMIT_REACHED);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_WQ_CATAS_ERROR:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_QP_FATAL);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_PATH_MIG_FAILED:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_PATH_MIG_ERR);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_WQ_INVAL_REQ_ERROR:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_QP_REQ_ERR);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_WQ_ACCESS_ERROR:\r\nmthca_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\nIB_EVENT_QP_ACCESS_ERR);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_CMD:\r\nmthca_cmd_event(dev,\r\nbe16_to_cpu(eqe->event.cmd.token),\r\neqe->event.cmd.status,\r\nbe64_to_cpu(eqe->event.cmd.out_param));\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_PORT_CHANGE:\r\nport_change(dev,\r\n(be32_to_cpu(eqe->event.port_change.port) >> 28) & 3,\r\neqe->subtype == 0x4);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_CQ_ERROR:\r\nmthca_warn(dev, "CQ %s on CQN %06x\n",\r\neqe->event.cq_err.syndrome == 1 ?\r\n"overrun" : "access violation",\r\nbe32_to_cpu(eqe->event.cq_err.cqn) & 0xffffff);\r\nmthca_cq_event(dev, be32_to_cpu(eqe->event.cq_err.cqn),\r\nIB_EVENT_CQ_ERR);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_EQ_OVERFLOW:\r\nmthca_warn(dev, "EQ overrun on EQN %d\n", eq->eqn);\r\nbreak;\r\ncase MTHCA_EVENT_TYPE_EEC_CATAS_ERROR:\r\ncase MTHCA_EVENT_TYPE_SRQ_CATAS_ERROR:\r\ncase MTHCA_EVENT_TYPE_LOCAL_CATAS_ERROR:\r\ncase MTHCA_EVENT_TYPE_ECC_DETECT:\r\ndefault:\r\nmthca_warn(dev, "Unhandled event %02x(%02x) on EQ %d\n",\r\neqe->type, eqe->subtype, eq->eqn);\r\nbreak;\r\n};\r\nset_eqe_hw(eqe);\r\n++eq->cons_index;\r\neqes_found = 1;\r\n++set_ci;\r\nif (unlikely(set_ci >= MTHCA_NUM_SPARE_EQE)) {\r\nset_eq_ci(dev, eq, eq->cons_index);\r\nset_ci = 0;\r\n}\r\n}\r\nreturn eqes_found;\r\n}\r\nstatic irqreturn_t mthca_tavor_interrupt(int irq, void *dev_ptr)\r\n{\r\nstruct mthca_dev *dev = dev_ptr;\r\nu32 ecr;\r\nint i;\r\nif (dev->eq_table.clr_mask)\r\nwritel(dev->eq_table.clr_mask, dev->eq_table.clr_int);\r\necr = readl(dev->eq_regs.tavor.ecr_base + 4);\r\nif (!ecr)\r\nreturn IRQ_NONE;\r\nwritel(ecr, dev->eq_regs.tavor.ecr_base +\r\nMTHCA_ECR_CLR_BASE - MTHCA_ECR_BASE + 4);\r\nfor (i = 0; i < MTHCA_NUM_EQ; ++i)\r\nif (ecr & dev->eq_table.eq[i].eqn_mask) {\r\nif (mthca_eq_int(dev, &dev->eq_table.eq[i]))\r\ntavor_set_eq_ci(dev, &dev->eq_table.eq[i],\r\ndev->eq_table.eq[i].cons_index);\r\ntavor_eq_req_not(dev, dev->eq_table.eq[i].eqn);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t mthca_tavor_msi_x_interrupt(int irq, void *eq_ptr)\r\n{\r\nstruct mthca_eq *eq = eq_ptr;\r\nstruct mthca_dev *dev = eq->dev;\r\nmthca_eq_int(dev, eq);\r\ntavor_set_eq_ci(dev, eq, eq->cons_index);\r\ntavor_eq_req_not(dev, eq->eqn);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t mthca_arbel_interrupt(int irq, void *dev_ptr)\r\n{\r\nstruct mthca_dev *dev = dev_ptr;\r\nint work = 0;\r\nint i;\r\nif (dev->eq_table.clr_mask)\r\nwritel(dev->eq_table.clr_mask, dev->eq_table.clr_int);\r\nfor (i = 0; i < MTHCA_NUM_EQ; ++i)\r\nif (mthca_eq_int(dev, &dev->eq_table.eq[i])) {\r\nwork = 1;\r\narbel_set_eq_ci(dev, &dev->eq_table.eq[i],\r\ndev->eq_table.eq[i].cons_index);\r\n}\r\narbel_eq_req_not(dev, dev->eq_table.arm_mask);\r\nreturn IRQ_RETVAL(work);\r\n}\r\nstatic irqreturn_t mthca_arbel_msi_x_interrupt(int irq, void *eq_ptr)\r\n{\r\nstruct mthca_eq *eq = eq_ptr;\r\nstruct mthca_dev *dev = eq->dev;\r\nmthca_eq_int(dev, eq);\r\narbel_set_eq_ci(dev, eq, eq->cons_index);\r\narbel_eq_req_not(dev, eq->eqn_mask);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int mthca_create_eq(struct mthca_dev *dev,\r\nint nent,\r\nu8 intr,\r\nstruct mthca_eq *eq)\r\n{\r\nint npages;\r\nu64 *dma_list = NULL;\r\ndma_addr_t t;\r\nstruct mthca_mailbox *mailbox;\r\nstruct mthca_eq_context *eq_context;\r\nint err = -ENOMEM;\r\nint i;\r\neq->dev = dev;\r\neq->nent = roundup_pow_of_two(max(nent, 2));\r\nnpages = ALIGN(eq->nent * MTHCA_EQ_ENTRY_SIZE, PAGE_SIZE) / PAGE_SIZE;\r\neq->page_list = kmalloc(npages * sizeof *eq->page_list,\r\nGFP_KERNEL);\r\nif (!eq->page_list)\r\ngoto err_out;\r\nfor (i = 0; i < npages; ++i)\r\neq->page_list[i].buf = NULL;\r\ndma_list = kmalloc(npages * sizeof *dma_list, GFP_KERNEL);\r\nif (!dma_list)\r\ngoto err_out_free;\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox))\r\ngoto err_out_free;\r\neq_context = mailbox->buf;\r\nfor (i = 0; i < npages; ++i) {\r\neq->page_list[i].buf = dma_alloc_coherent(&dev->pdev->dev,\r\nPAGE_SIZE, &t, GFP_KERNEL);\r\nif (!eq->page_list[i].buf)\r\ngoto err_out_free_pages;\r\ndma_list[i] = t;\r\ndma_unmap_addr_set(&eq->page_list[i], mapping, t);\r\nclear_page(eq->page_list[i].buf);\r\n}\r\nfor (i = 0; i < eq->nent; ++i)\r\nset_eqe_hw(get_eqe(eq, i));\r\neq->eqn = mthca_alloc(&dev->eq_table.alloc);\r\nif (eq->eqn == -1)\r\ngoto err_out_free_pages;\r\nerr = mthca_mr_alloc_phys(dev, dev->driver_pd.pd_num,\r\ndma_list, PAGE_SHIFT, npages,\r\n0, npages * PAGE_SIZE,\r\nMTHCA_MPT_FLAG_LOCAL_WRITE |\r\nMTHCA_MPT_FLAG_LOCAL_READ,\r\n&eq->mr);\r\nif (err)\r\ngoto err_out_free_eq;\r\nmemset(eq_context, 0, sizeof *eq_context);\r\neq_context->flags = cpu_to_be32(MTHCA_EQ_STATUS_OK |\r\nMTHCA_EQ_OWNER_HW |\r\nMTHCA_EQ_STATE_ARMED |\r\nMTHCA_EQ_FLAG_TR);\r\nif (mthca_is_memfree(dev))\r\neq_context->flags |= cpu_to_be32(MTHCA_EQ_STATE_ARBEL);\r\neq_context->logsize_usrpage = cpu_to_be32((ffs(eq->nent) - 1) << 24);\r\nif (mthca_is_memfree(dev)) {\r\neq_context->arbel_pd = cpu_to_be32(dev->driver_pd.pd_num);\r\n} else {\r\neq_context->logsize_usrpage |= cpu_to_be32(dev->driver_uar.index);\r\neq_context->tavor_pd = cpu_to_be32(dev->driver_pd.pd_num);\r\n}\r\neq_context->intr = intr;\r\neq_context->lkey = cpu_to_be32(eq->mr.ibmr.lkey);\r\nerr = mthca_SW2HW_EQ(dev, mailbox, eq->eqn);\r\nif (err) {\r\nmthca_warn(dev, "SW2HW_EQ returned %d\n", err);\r\ngoto err_out_free_mr;\r\n}\r\nkfree(dma_list);\r\nmthca_free_mailbox(dev, mailbox);\r\neq->eqn_mask = swab32(1 << eq->eqn);\r\neq->cons_index = 0;\r\ndev->eq_table.arm_mask |= eq->eqn_mask;\r\nmthca_dbg(dev, "Allocated EQ %d with %d entries\n",\r\neq->eqn, eq->nent);\r\nreturn err;\r\nerr_out_free_mr:\r\nmthca_free_mr(dev, &eq->mr);\r\nerr_out_free_eq:\r\nmthca_free(&dev->eq_table.alloc, eq->eqn);\r\nerr_out_free_pages:\r\nfor (i = 0; i < npages; ++i)\r\nif (eq->page_list[i].buf)\r\ndma_free_coherent(&dev->pdev->dev, PAGE_SIZE,\r\neq->page_list[i].buf,\r\ndma_unmap_addr(&eq->page_list[i],\r\nmapping));\r\nmthca_free_mailbox(dev, mailbox);\r\nerr_out_free:\r\nkfree(eq->page_list);\r\nkfree(dma_list);\r\nerr_out:\r\nreturn err;\r\n}\r\nstatic void mthca_free_eq(struct mthca_dev *dev,\r\nstruct mthca_eq *eq)\r\n{\r\nstruct mthca_mailbox *mailbox;\r\nint err;\r\nint npages = (eq->nent * MTHCA_EQ_ENTRY_SIZE + PAGE_SIZE - 1) /\r\nPAGE_SIZE;\r\nint i;\r\nmailbox = mthca_alloc_mailbox(dev, GFP_KERNEL);\r\nif (IS_ERR(mailbox))\r\nreturn;\r\nerr = mthca_HW2SW_EQ(dev, mailbox, eq->eqn);\r\nif (err)\r\nmthca_warn(dev, "HW2SW_EQ returned %d\n", err);\r\ndev->eq_table.arm_mask &= ~eq->eqn_mask;\r\nif (0) {\r\nmthca_dbg(dev, "Dumping EQ context %02x:\n", eq->eqn);\r\nfor (i = 0; i < sizeof (struct mthca_eq_context) / 4; ++i) {\r\nif (i % 4 == 0)\r\nprintk("[%02x] ", i * 4);\r\nprintk(" %08x", be32_to_cpup(mailbox->buf + i * 4));\r\nif ((i + 1) % 4 == 0)\r\nprintk("\n");\r\n}\r\n}\r\nmthca_free_mr(dev, &eq->mr);\r\nfor (i = 0; i < npages; ++i)\r\npci_free_consistent(dev->pdev, PAGE_SIZE,\r\neq->page_list[i].buf,\r\ndma_unmap_addr(&eq->page_list[i], mapping));\r\nkfree(eq->page_list);\r\nmthca_free_mailbox(dev, mailbox);\r\n}\r\nstatic void mthca_free_irqs(struct mthca_dev *dev)\r\n{\r\nint i;\r\nif (dev->eq_table.have_irq)\r\nfree_irq(dev->pdev->irq, dev);\r\nfor (i = 0; i < MTHCA_NUM_EQ; ++i)\r\nif (dev->eq_table.eq[i].have_irq) {\r\nfree_irq(dev->eq_table.eq[i].msi_x_vector,\r\ndev->eq_table.eq + i);\r\ndev->eq_table.eq[i].have_irq = 0;\r\n}\r\n}\r\nstatic int mthca_map_reg(struct mthca_dev *dev,\r\nunsigned long offset, unsigned long size,\r\nvoid __iomem **map)\r\n{\r\nphys_addr_t base = pci_resource_start(dev->pdev, 0);\r\n*map = ioremap(base + offset, size);\r\nif (!*map)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic int mthca_map_eq_regs(struct mthca_dev *dev)\r\n{\r\nif (mthca_is_memfree(dev)) {\r\nif (mthca_map_reg(dev, (pci_resource_len(dev->pdev, 0) - 1) &\r\ndev->fw.arbel.clr_int_base, MTHCA_CLR_INT_SIZE,\r\n&dev->clr_base)) {\r\nmthca_err(dev, "Couldn't map interrupt clear register, "\r\n"aborting.\n");\r\nreturn -ENOMEM;\r\n}\r\nif (mthca_map_reg(dev, ((pci_resource_len(dev->pdev, 0) - 1) &\r\ndev->fw.arbel.eq_arm_base) + 4, 4,\r\n&dev->eq_regs.arbel.eq_arm)) {\r\nmthca_err(dev, "Couldn't map EQ arm register, aborting.\n");\r\niounmap(dev->clr_base);\r\nreturn -ENOMEM;\r\n}\r\nif (mthca_map_reg(dev, (pci_resource_len(dev->pdev, 0) - 1) &\r\ndev->fw.arbel.eq_set_ci_base,\r\nMTHCA_EQ_SET_CI_SIZE,\r\n&dev->eq_regs.arbel.eq_set_ci_base)) {\r\nmthca_err(dev, "Couldn't map EQ CI register, aborting.\n");\r\niounmap(dev->eq_regs.arbel.eq_arm);\r\niounmap(dev->clr_base);\r\nreturn -ENOMEM;\r\n}\r\n} else {\r\nif (mthca_map_reg(dev, MTHCA_CLR_INT_BASE, MTHCA_CLR_INT_SIZE,\r\n&dev->clr_base)) {\r\nmthca_err(dev, "Couldn't map interrupt clear register, "\r\n"aborting.\n");\r\nreturn -ENOMEM;\r\n}\r\nif (mthca_map_reg(dev, MTHCA_ECR_BASE,\r\nMTHCA_ECR_SIZE + MTHCA_ECR_CLR_SIZE,\r\n&dev->eq_regs.tavor.ecr_base)) {\r\nmthca_err(dev, "Couldn't map ecr register, "\r\n"aborting.\n");\r\niounmap(dev->clr_base);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void mthca_unmap_eq_regs(struct mthca_dev *dev)\r\n{\r\nif (mthca_is_memfree(dev)) {\r\niounmap(dev->eq_regs.arbel.eq_set_ci_base);\r\niounmap(dev->eq_regs.arbel.eq_arm);\r\niounmap(dev->clr_base);\r\n} else {\r\niounmap(dev->eq_regs.tavor.ecr_base);\r\niounmap(dev->clr_base);\r\n}\r\n}\r\nint mthca_map_eq_icm(struct mthca_dev *dev, u64 icm_virt)\r\n{\r\nint ret;\r\ndev->eq_table.icm_virt = icm_virt;\r\ndev->eq_table.icm_page = alloc_page(GFP_HIGHUSER);\r\nif (!dev->eq_table.icm_page)\r\nreturn -ENOMEM;\r\ndev->eq_table.icm_dma = pci_map_page(dev->pdev, dev->eq_table.icm_page, 0,\r\nPAGE_SIZE, PCI_DMA_BIDIRECTIONAL);\r\nif (pci_dma_mapping_error(dev->pdev, dev->eq_table.icm_dma)) {\r\n__free_page(dev->eq_table.icm_page);\r\nreturn -ENOMEM;\r\n}\r\nret = mthca_MAP_ICM_page(dev, dev->eq_table.icm_dma, icm_virt);\r\nif (ret) {\r\npci_unmap_page(dev->pdev, dev->eq_table.icm_dma, PAGE_SIZE,\r\nPCI_DMA_BIDIRECTIONAL);\r\n__free_page(dev->eq_table.icm_page);\r\n}\r\nreturn ret;\r\n}\r\nvoid mthca_unmap_eq_icm(struct mthca_dev *dev)\r\n{\r\nmthca_UNMAP_ICM(dev, dev->eq_table.icm_virt, 1);\r\npci_unmap_page(dev->pdev, dev->eq_table.icm_dma, PAGE_SIZE,\r\nPCI_DMA_BIDIRECTIONAL);\r\n__free_page(dev->eq_table.icm_page);\r\n}\r\nint mthca_init_eq_table(struct mthca_dev *dev)\r\n{\r\nint err;\r\nu8 intr;\r\nint i;\r\nerr = mthca_alloc_init(&dev->eq_table.alloc,\r\ndev->limits.num_eqs,\r\ndev->limits.num_eqs - 1,\r\ndev->limits.reserved_eqs);\r\nif (err)\r\nreturn err;\r\nerr = mthca_map_eq_regs(dev);\r\nif (err)\r\ngoto err_out_free;\r\nif (dev->mthca_flags & MTHCA_FLAG_MSI_X) {\r\ndev->eq_table.clr_mask = 0;\r\n} else {\r\ndev->eq_table.clr_mask =\r\nswab32(1 << (dev->eq_table.inta_pin & 31));\r\ndev->eq_table.clr_int = dev->clr_base +\r\n(dev->eq_table.inta_pin < 32 ? 4 : 0);\r\n}\r\ndev->eq_table.arm_mask = 0;\r\nintr = dev->eq_table.inta_pin;\r\nerr = mthca_create_eq(dev, dev->limits.num_cqs + MTHCA_NUM_SPARE_EQE,\r\n(dev->mthca_flags & MTHCA_FLAG_MSI_X) ? 128 : intr,\r\n&dev->eq_table.eq[MTHCA_EQ_COMP]);\r\nif (err)\r\ngoto err_out_unmap;\r\nerr = mthca_create_eq(dev, MTHCA_NUM_ASYNC_EQE + MTHCA_NUM_SPARE_EQE,\r\n(dev->mthca_flags & MTHCA_FLAG_MSI_X) ? 129 : intr,\r\n&dev->eq_table.eq[MTHCA_EQ_ASYNC]);\r\nif (err)\r\ngoto err_out_comp;\r\nerr = mthca_create_eq(dev, MTHCA_NUM_CMD_EQE + MTHCA_NUM_SPARE_EQE,\r\n(dev->mthca_flags & MTHCA_FLAG_MSI_X) ? 130 : intr,\r\n&dev->eq_table.eq[MTHCA_EQ_CMD]);\r\nif (err)\r\ngoto err_out_async;\r\nif (dev->mthca_flags & MTHCA_FLAG_MSI_X) {\r\nstatic const char *eq_name[] = {\r\n[MTHCA_EQ_COMP] = DRV_NAME "-comp",\r\n[MTHCA_EQ_ASYNC] = DRV_NAME "-async",\r\n[MTHCA_EQ_CMD] = DRV_NAME "-cmd"\r\n};\r\nfor (i = 0; i < MTHCA_NUM_EQ; ++i) {\r\nsnprintf(dev->eq_table.eq[i].irq_name,\r\nIB_DEVICE_NAME_MAX,\r\n"%s@pci:%s", eq_name[i],\r\npci_name(dev->pdev));\r\nerr = request_irq(dev->eq_table.eq[i].msi_x_vector,\r\nmthca_is_memfree(dev) ?\r\nmthca_arbel_msi_x_interrupt :\r\nmthca_tavor_msi_x_interrupt,\r\n0, dev->eq_table.eq[i].irq_name,\r\ndev->eq_table.eq + i);\r\nif (err)\r\ngoto err_out_cmd;\r\ndev->eq_table.eq[i].have_irq = 1;\r\n}\r\n} else {\r\nsnprintf(dev->eq_table.eq[0].irq_name, IB_DEVICE_NAME_MAX,\r\nDRV_NAME "@pci:%s", pci_name(dev->pdev));\r\nerr = request_irq(dev->pdev->irq,\r\nmthca_is_memfree(dev) ?\r\nmthca_arbel_interrupt :\r\nmthca_tavor_interrupt,\r\nIRQF_SHARED, dev->eq_table.eq[0].irq_name, dev);\r\nif (err)\r\ngoto err_out_cmd;\r\ndev->eq_table.have_irq = 1;\r\n}\r\nerr = mthca_MAP_EQ(dev, async_mask(dev),\r\n0, dev->eq_table.eq[MTHCA_EQ_ASYNC].eqn);\r\nif (err)\r\nmthca_warn(dev, "MAP_EQ for async EQ %d failed (%d)\n",\r\ndev->eq_table.eq[MTHCA_EQ_ASYNC].eqn, err);\r\nerr = mthca_MAP_EQ(dev, MTHCA_CMD_EVENT_MASK,\r\n0, dev->eq_table.eq[MTHCA_EQ_CMD].eqn);\r\nif (err)\r\nmthca_warn(dev, "MAP_EQ for cmd EQ %d failed (%d)\n",\r\ndev->eq_table.eq[MTHCA_EQ_CMD].eqn, err);\r\nfor (i = 0; i < MTHCA_NUM_EQ; ++i)\r\nif (mthca_is_memfree(dev))\r\narbel_eq_req_not(dev, dev->eq_table.eq[i].eqn_mask);\r\nelse\r\ntavor_eq_req_not(dev, dev->eq_table.eq[i].eqn);\r\nreturn 0;\r\nerr_out_cmd:\r\nmthca_free_irqs(dev);\r\nmthca_free_eq(dev, &dev->eq_table.eq[MTHCA_EQ_CMD]);\r\nerr_out_async:\r\nmthca_free_eq(dev, &dev->eq_table.eq[MTHCA_EQ_ASYNC]);\r\nerr_out_comp:\r\nmthca_free_eq(dev, &dev->eq_table.eq[MTHCA_EQ_COMP]);\r\nerr_out_unmap:\r\nmthca_unmap_eq_regs(dev);\r\nerr_out_free:\r\nmthca_alloc_cleanup(&dev->eq_table.alloc);\r\nreturn err;\r\n}\r\nvoid mthca_cleanup_eq_table(struct mthca_dev *dev)\r\n{\r\nint i;\r\nmthca_free_irqs(dev);\r\nmthca_MAP_EQ(dev, async_mask(dev),\r\n1, dev->eq_table.eq[MTHCA_EQ_ASYNC].eqn);\r\nmthca_MAP_EQ(dev, MTHCA_CMD_EVENT_MASK,\r\n1, dev->eq_table.eq[MTHCA_EQ_CMD].eqn);\r\nfor (i = 0; i < MTHCA_NUM_EQ; ++i)\r\nmthca_free_eq(dev, &dev->eq_table.eq[i]);\r\nmthca_unmap_eq_regs(dev);\r\nmthca_alloc_cleanup(&dev->eq_table.alloc);\r\n}
