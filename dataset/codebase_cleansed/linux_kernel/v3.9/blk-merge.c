static unsigned int __blk_recalc_rq_segments(struct request_queue *q,\r\nstruct bio *bio)\r\n{\r\nstruct bio_vec *bv, *bvprv = NULL;\r\nint cluster, i, high, highprv = 1;\r\nunsigned int seg_size, nr_phys_segs;\r\nstruct bio *fbio, *bbio;\r\nif (!bio)\r\nreturn 0;\r\nfbio = bio;\r\ncluster = blk_queue_cluster(q);\r\nseg_size = 0;\r\nnr_phys_segs = 0;\r\nfor_each_bio(bio) {\r\nbio_for_each_segment(bv, bio, i) {\r\nhigh = page_to_pfn(bv->bv_page) > queue_bounce_pfn(q);\r\nif (high || highprv)\r\ngoto new_segment;\r\nif (cluster) {\r\nif (seg_size + bv->bv_len\r\n> queue_max_segment_size(q))\r\ngoto new_segment;\r\nif (!BIOVEC_PHYS_MERGEABLE(bvprv, bv))\r\ngoto new_segment;\r\nif (!BIOVEC_SEG_BOUNDARY(q, bvprv, bv))\r\ngoto new_segment;\r\nseg_size += bv->bv_len;\r\nbvprv = bv;\r\ncontinue;\r\n}\r\nnew_segment:\r\nif (nr_phys_segs == 1 && seg_size >\r\nfbio->bi_seg_front_size)\r\nfbio->bi_seg_front_size = seg_size;\r\nnr_phys_segs++;\r\nbvprv = bv;\r\nseg_size = bv->bv_len;\r\nhighprv = high;\r\n}\r\nbbio = bio;\r\n}\r\nif (nr_phys_segs == 1 && seg_size > fbio->bi_seg_front_size)\r\nfbio->bi_seg_front_size = seg_size;\r\nif (seg_size > bbio->bi_seg_back_size)\r\nbbio->bi_seg_back_size = seg_size;\r\nreturn nr_phys_segs;\r\n}\r\nvoid blk_recalc_rq_segments(struct request *rq)\r\n{\r\nrq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);\r\n}\r\nvoid blk_recount_segments(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct bio *nxt = bio->bi_next;\r\nbio->bi_next = NULL;\r\nbio->bi_phys_segments = __blk_recalc_rq_segments(q, bio);\r\nbio->bi_next = nxt;\r\nbio->bi_flags |= (1 << BIO_SEG_VALID);\r\n}\r\nstatic int blk_phys_contig_segment(struct request_queue *q, struct bio *bio,\r\nstruct bio *nxt)\r\n{\r\nif (!blk_queue_cluster(q))\r\nreturn 0;\r\nif (bio->bi_seg_back_size + nxt->bi_seg_front_size >\r\nqueue_max_segment_size(q))\r\nreturn 0;\r\nif (!bio_has_data(bio))\r\nreturn 1;\r\nif (!BIOVEC_PHYS_MERGEABLE(__BVEC_END(bio), __BVEC_START(nxt)))\r\nreturn 0;\r\nif (BIO_SEG_BOUNDARY(q, bio, nxt))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void\r\n__blk_segment_map_sg(struct request_queue *q, struct bio_vec *bvec,\r\nstruct scatterlist *sglist, struct bio_vec **bvprv,\r\nstruct scatterlist **sg, int *nsegs, int *cluster)\r\n{\r\nint nbytes = bvec->bv_len;\r\nif (*bvprv && *cluster) {\r\nif ((*sg)->length + nbytes > queue_max_segment_size(q))\r\ngoto new_segment;\r\nif (!BIOVEC_PHYS_MERGEABLE(*bvprv, bvec))\r\ngoto new_segment;\r\nif (!BIOVEC_SEG_BOUNDARY(q, *bvprv, bvec))\r\ngoto new_segment;\r\n(*sg)->length += nbytes;\r\n} else {\r\nnew_segment:\r\nif (!*sg)\r\n*sg = sglist;\r\nelse {\r\n(*sg)->page_link &= ~0x02;\r\n*sg = sg_next(*sg);\r\n}\r\nsg_set_page(*sg, bvec->bv_page, nbytes, bvec->bv_offset);\r\n(*nsegs)++;\r\n}\r\n*bvprv = bvec;\r\n}\r\nint blk_rq_map_sg(struct request_queue *q, struct request *rq,\r\nstruct scatterlist *sglist)\r\n{\r\nstruct bio_vec *bvec, *bvprv;\r\nstruct req_iterator iter;\r\nstruct scatterlist *sg;\r\nint nsegs, cluster;\r\nnsegs = 0;\r\ncluster = blk_queue_cluster(q);\r\nbvprv = NULL;\r\nsg = NULL;\r\nrq_for_each_segment(bvec, rq, iter) {\r\n__blk_segment_map_sg(q, bvec, sglist, &bvprv, &sg,\r\n&nsegs, &cluster);\r\n}\r\nif (unlikely(rq->cmd_flags & REQ_COPY_USER) &&\r\n(blk_rq_bytes(rq) & q->dma_pad_mask)) {\r\nunsigned int pad_len =\r\n(q->dma_pad_mask & ~blk_rq_bytes(rq)) + 1;\r\nsg->length += pad_len;\r\nrq->extra_len += pad_len;\r\n}\r\nif (q->dma_drain_size && q->dma_drain_needed(rq)) {\r\nif (rq->cmd_flags & REQ_WRITE)\r\nmemset(q->dma_drain_buffer, 0, q->dma_drain_size);\r\nsg->page_link &= ~0x02;\r\nsg = sg_next(sg);\r\nsg_set_page(sg, virt_to_page(q->dma_drain_buffer),\r\nq->dma_drain_size,\r\n((unsigned long)q->dma_drain_buffer) &\r\n(PAGE_SIZE - 1));\r\nnsegs++;\r\nrq->extra_len += q->dma_drain_size;\r\n}\r\nif (sg)\r\nsg_mark_end(sg);\r\nreturn nsegs;\r\n}\r\nint blk_bio_map_sg(struct request_queue *q, struct bio *bio,\r\nstruct scatterlist *sglist)\r\n{\r\nstruct bio_vec *bvec, *bvprv;\r\nstruct scatterlist *sg;\r\nint nsegs, cluster;\r\nunsigned long i;\r\nnsegs = 0;\r\ncluster = blk_queue_cluster(q);\r\nbvprv = NULL;\r\nsg = NULL;\r\nbio_for_each_segment(bvec, bio, i) {\r\n__blk_segment_map_sg(q, bvec, sglist, &bvprv, &sg,\r\n&nsegs, &cluster);\r\n}\r\nif (sg)\r\nsg_mark_end(sg);\r\nBUG_ON(bio->bi_phys_segments && nsegs > bio->bi_phys_segments);\r\nreturn nsegs;\r\n}\r\nstatic inline int ll_new_hw_segment(struct request_queue *q,\r\nstruct request *req,\r\nstruct bio *bio)\r\n{\r\nint nr_phys_segs = bio_phys_segments(q, bio);\r\nif (req->nr_phys_segments + nr_phys_segs > queue_max_segments(q))\r\ngoto no_merge;\r\nif (bio_integrity(bio) && blk_integrity_merge_bio(q, req, bio))\r\ngoto no_merge;\r\nreq->nr_phys_segments += nr_phys_segs;\r\nreturn 1;\r\nno_merge:\r\nreq->cmd_flags |= REQ_NOMERGE;\r\nif (req == q->last_merge)\r\nq->last_merge = NULL;\r\nreturn 0;\r\n}\r\nint ll_back_merge_fn(struct request_queue *q, struct request *req,\r\nstruct bio *bio)\r\n{\r\nif (blk_rq_sectors(req) + bio_sectors(bio) >\r\nblk_rq_get_max_sectors(req)) {\r\nreq->cmd_flags |= REQ_NOMERGE;\r\nif (req == q->last_merge)\r\nq->last_merge = NULL;\r\nreturn 0;\r\n}\r\nif (!bio_flagged(req->biotail, BIO_SEG_VALID))\r\nblk_recount_segments(q, req->biotail);\r\nif (!bio_flagged(bio, BIO_SEG_VALID))\r\nblk_recount_segments(q, bio);\r\nreturn ll_new_hw_segment(q, req, bio);\r\n}\r\nint ll_front_merge_fn(struct request_queue *q, struct request *req,\r\nstruct bio *bio)\r\n{\r\nif (blk_rq_sectors(req) + bio_sectors(bio) >\r\nblk_rq_get_max_sectors(req)) {\r\nreq->cmd_flags |= REQ_NOMERGE;\r\nif (req == q->last_merge)\r\nq->last_merge = NULL;\r\nreturn 0;\r\n}\r\nif (!bio_flagged(bio, BIO_SEG_VALID))\r\nblk_recount_segments(q, bio);\r\nif (!bio_flagged(req->bio, BIO_SEG_VALID))\r\nblk_recount_segments(q, req->bio);\r\nreturn ll_new_hw_segment(q, req, bio);\r\n}\r\nstatic int ll_merge_requests_fn(struct request_queue *q, struct request *req,\r\nstruct request *next)\r\n{\r\nint total_phys_segments;\r\nunsigned int seg_size =\r\nreq->biotail->bi_seg_back_size + next->bio->bi_seg_front_size;\r\nif (req->special || next->special)\r\nreturn 0;\r\nif ((blk_rq_sectors(req) + blk_rq_sectors(next)) >\r\nblk_rq_get_max_sectors(req))\r\nreturn 0;\r\ntotal_phys_segments = req->nr_phys_segments + next->nr_phys_segments;\r\nif (blk_phys_contig_segment(q, req->biotail, next->bio)) {\r\nif (req->nr_phys_segments == 1)\r\nreq->bio->bi_seg_front_size = seg_size;\r\nif (next->nr_phys_segments == 1)\r\nnext->biotail->bi_seg_back_size = seg_size;\r\ntotal_phys_segments--;\r\n}\r\nif (total_phys_segments > queue_max_segments(q))\r\nreturn 0;\r\nif (blk_integrity_rq(req) && blk_integrity_merge_rq(q, req, next))\r\nreturn 0;\r\nreq->nr_phys_segments = total_phys_segments;\r\nreturn 1;\r\n}\r\nvoid blk_rq_set_mixed_merge(struct request *rq)\r\n{\r\nunsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;\r\nstruct bio *bio;\r\nif (rq->cmd_flags & REQ_MIXED_MERGE)\r\nreturn;\r\nfor (bio = rq->bio; bio; bio = bio->bi_next) {\r\nWARN_ON_ONCE((bio->bi_rw & REQ_FAILFAST_MASK) &&\r\n(bio->bi_rw & REQ_FAILFAST_MASK) != ff);\r\nbio->bi_rw |= ff;\r\n}\r\nrq->cmd_flags |= REQ_MIXED_MERGE;\r\n}\r\nstatic void blk_account_io_merge(struct request *req)\r\n{\r\nif (blk_do_io_stat(req)) {\r\nstruct hd_struct *part;\r\nint cpu;\r\ncpu = part_stat_lock();\r\npart = req->part;\r\npart_round_stats(cpu, part);\r\npart_dec_in_flight(part, rq_data_dir(req));\r\nhd_struct_put(part);\r\npart_stat_unlock();\r\n}\r\n}\r\nstatic int attempt_merge(struct request_queue *q, struct request *req,\r\nstruct request *next)\r\n{\r\nif (!rq_mergeable(req) || !rq_mergeable(next))\r\nreturn 0;\r\nif (!blk_check_merge_flags(req->cmd_flags, next->cmd_flags))\r\nreturn 0;\r\nif (blk_rq_pos(req) + blk_rq_sectors(req) != blk_rq_pos(next))\r\nreturn 0;\r\nif (rq_data_dir(req) != rq_data_dir(next)\r\n|| req->rq_disk != next->rq_disk\r\n|| next->special)\r\nreturn 0;\r\nif (req->cmd_flags & REQ_WRITE_SAME &&\r\n!blk_write_same_mergeable(req->bio, next->bio))\r\nreturn 0;\r\nif (!ll_merge_requests_fn(q, req, next))\r\nreturn 0;\r\nif ((req->cmd_flags | next->cmd_flags) & REQ_MIXED_MERGE ||\r\n(req->cmd_flags & REQ_FAILFAST_MASK) !=\r\n(next->cmd_flags & REQ_FAILFAST_MASK)) {\r\nblk_rq_set_mixed_merge(req);\r\nblk_rq_set_mixed_merge(next);\r\n}\r\nif (time_after(req->start_time, next->start_time))\r\nreq->start_time = next->start_time;\r\nreq->biotail->bi_next = next->bio;\r\nreq->biotail = next->biotail;\r\nreq->__data_len += blk_rq_bytes(next);\r\nelv_merge_requests(q, req, next);\r\nblk_account_io_merge(next);\r\nreq->ioprio = ioprio_best(req->ioprio, next->ioprio);\r\nif (blk_rq_cpu_valid(next))\r\nreq->cpu = next->cpu;\r\nnext->bio = NULL;\r\n__blk_put_request(q, next);\r\nreturn 1;\r\n}\r\nint attempt_back_merge(struct request_queue *q, struct request *rq)\r\n{\r\nstruct request *next = elv_latter_request(q, rq);\r\nif (next)\r\nreturn attempt_merge(q, rq, next);\r\nreturn 0;\r\n}\r\nint attempt_front_merge(struct request_queue *q, struct request *rq)\r\n{\r\nstruct request *prev = elv_former_request(q, rq);\r\nif (prev)\r\nreturn attempt_merge(q, prev, rq);\r\nreturn 0;\r\n}\r\nint blk_attempt_req_merge(struct request_queue *q, struct request *rq,\r\nstruct request *next)\r\n{\r\nreturn attempt_merge(q, rq, next);\r\n}\r\nbool blk_rq_merge_ok(struct request *rq, struct bio *bio)\r\n{\r\nif (!rq_mergeable(rq) || !bio_mergeable(bio))\r\nreturn false;\r\nif (!blk_check_merge_flags(rq->cmd_flags, bio->bi_rw))\r\nreturn false;\r\nif (bio_data_dir(bio) != rq_data_dir(rq))\r\nreturn false;\r\nif (rq->rq_disk != bio->bi_bdev->bd_disk || rq->special)\r\nreturn false;\r\nif (bio_integrity(bio) != blk_integrity_rq(rq))\r\nreturn false;\r\nif (rq->cmd_flags & REQ_WRITE_SAME &&\r\n!blk_write_same_mergeable(rq->bio, bio))\r\nreturn false;\r\nreturn true;\r\n}\r\nint blk_try_merge(struct request *rq, struct bio *bio)\r\n{\r\nif (blk_rq_pos(rq) + blk_rq_sectors(rq) == bio->bi_sector)\r\nreturn ELEVATOR_BACK_MERGE;\r\nelse if (blk_rq_pos(rq) - bio_sectors(bio) == bio->bi_sector)\r\nreturn ELEVATOR_FRONT_MERGE;\r\nreturn ELEVATOR_NO_MERGE;\r\n}
