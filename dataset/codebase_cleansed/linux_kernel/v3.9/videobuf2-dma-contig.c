static void vb2_dc_sgt_foreach_page(struct sg_table *sgt,\r\nvoid (*cb)(struct page *pg))\r\n{\r\nstruct scatterlist *s;\r\nunsigned int i;\r\nfor_each_sg(sgt->sgl, s, sgt->orig_nents, i) {\r\nstruct page *page = sg_page(s);\r\nunsigned int n_pages = PAGE_ALIGN(s->offset + s->length)\r\n>> PAGE_SHIFT;\r\nunsigned int j;\r\nfor (j = 0; j < n_pages; ++j, ++page)\r\ncb(page);\r\n}\r\n}\r\nstatic unsigned long vb2_dc_get_contiguous_size(struct sg_table *sgt)\r\n{\r\nstruct scatterlist *s;\r\ndma_addr_t expected = sg_dma_address(sgt->sgl);\r\nunsigned int i;\r\nunsigned long size = 0;\r\nfor_each_sg(sgt->sgl, s, sgt->nents, i) {\r\nif (sg_dma_address(s) != expected)\r\nbreak;\r\nexpected = sg_dma_address(s) + sg_dma_len(s);\r\nsize += sg_dma_len(s);\r\n}\r\nreturn size;\r\n}\r\nstatic void *vb2_dc_cookie(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nreturn &buf->dma_addr;\r\n}\r\nstatic void *vb2_dc_vaddr(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nreturn buf->vaddr;\r\n}\r\nstatic unsigned int vb2_dc_num_users(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nreturn atomic_read(&buf->refcount);\r\n}\r\nstatic void vb2_dc_prepare(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nstruct sg_table *sgt = buf->dma_sgt;\r\nif (!sgt || buf->db_attach)\r\nreturn;\r\ndma_sync_sg_for_device(buf->dev, sgt->sgl, sgt->nents, buf->dma_dir);\r\n}\r\nstatic void vb2_dc_finish(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nstruct sg_table *sgt = buf->dma_sgt;\r\nif (!sgt || buf->db_attach)\r\nreturn;\r\ndma_sync_sg_for_cpu(buf->dev, sgt->sgl, sgt->nents, buf->dma_dir);\r\n}\r\nstatic void vb2_dc_put(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nif (!atomic_dec_and_test(&buf->refcount))\r\nreturn;\r\nif (buf->sgt_base) {\r\nsg_free_table(buf->sgt_base);\r\nkfree(buf->sgt_base);\r\n}\r\ndma_free_coherent(buf->dev, buf->size, buf->vaddr, buf->dma_addr);\r\nput_device(buf->dev);\r\nkfree(buf);\r\n}\r\nstatic void *vb2_dc_alloc(void *alloc_ctx, unsigned long size)\r\n{\r\nstruct vb2_dc_conf *conf = alloc_ctx;\r\nstruct device *dev = conf->dev;\r\nstruct vb2_dc_buf *buf;\r\nbuf = kzalloc(sizeof *buf, GFP_KERNEL);\r\nif (!buf)\r\nreturn ERR_PTR(-ENOMEM);\r\nsize = PAGE_ALIGN(size);\r\nbuf->vaddr = dma_alloc_coherent(dev, size, &buf->dma_addr, GFP_KERNEL);\r\nif (!buf->vaddr) {\r\ndev_err(dev, "dma_alloc_coherent of size %ld failed\n", size);\r\nkfree(buf);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nbuf->dev = get_device(dev);\r\nbuf->size = size;\r\nbuf->handler.refcount = &buf->refcount;\r\nbuf->handler.put = vb2_dc_put;\r\nbuf->handler.arg = buf;\r\natomic_inc(&buf->refcount);\r\nreturn buf;\r\n}\r\nstatic int vb2_dc_mmap(void *buf_priv, struct vm_area_struct *vma)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nint ret;\r\nif (!buf) {\r\nprintk(KERN_ERR "No buffer to map\n");\r\nreturn -EINVAL;\r\n}\r\nvma->vm_pgoff = 0;\r\nret = dma_mmap_coherent(buf->dev, vma, buf->vaddr,\r\nbuf->dma_addr, buf->size);\r\nif (ret) {\r\npr_err("Remapping memory failed, error: %d\n", ret);\r\nreturn ret;\r\n}\r\nvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\r\nvma->vm_private_data = &buf->handler;\r\nvma->vm_ops = &vb2_common_vm_ops;\r\nvma->vm_ops->open(vma);\r\npr_debug("%s: mapped dma addr 0x%08lx at 0x%08lx, size %ld\n",\r\n__func__, (unsigned long)buf->dma_addr, vma->vm_start,\r\nbuf->size);\r\nreturn 0;\r\n}\r\nstatic int vb2_dc_dmabuf_ops_attach(struct dma_buf *dbuf, struct device *dev,\r\nstruct dma_buf_attachment *dbuf_attach)\r\n{\r\nstruct vb2_dc_attachment *attach;\r\nunsigned int i;\r\nstruct scatterlist *rd, *wr;\r\nstruct sg_table *sgt;\r\nstruct vb2_dc_buf *buf = dbuf->priv;\r\nint ret;\r\nattach = kzalloc(sizeof(*attach), GFP_KERNEL);\r\nif (!attach)\r\nreturn -ENOMEM;\r\nsgt = &attach->sgt;\r\nret = sg_alloc_table(sgt, buf->sgt_base->orig_nents, GFP_KERNEL);\r\nif (ret) {\r\nkfree(attach);\r\nreturn -ENOMEM;\r\n}\r\nrd = buf->sgt_base->sgl;\r\nwr = sgt->sgl;\r\nfor (i = 0; i < sgt->orig_nents; ++i) {\r\nsg_set_page(wr, sg_page(rd), rd->length, rd->offset);\r\nrd = sg_next(rd);\r\nwr = sg_next(wr);\r\n}\r\nattach->dir = DMA_NONE;\r\ndbuf_attach->priv = attach;\r\nreturn 0;\r\n}\r\nstatic void vb2_dc_dmabuf_ops_detach(struct dma_buf *dbuf,\r\nstruct dma_buf_attachment *db_attach)\r\n{\r\nstruct vb2_dc_attachment *attach = db_attach->priv;\r\nstruct sg_table *sgt;\r\nif (!attach)\r\nreturn;\r\nsgt = &attach->sgt;\r\nif (attach->dir != DMA_NONE)\r\ndma_unmap_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\nattach->dir);\r\nsg_free_table(sgt);\r\nkfree(attach);\r\ndb_attach->priv = NULL;\r\n}\r\nstatic struct sg_table *vb2_dc_dmabuf_ops_map(\r\nstruct dma_buf_attachment *db_attach, enum dma_data_direction dir)\r\n{\r\nstruct vb2_dc_attachment *attach = db_attach->priv;\r\nstruct mutex *lock = &db_attach->dmabuf->lock;\r\nstruct sg_table *sgt;\r\nint ret;\r\nmutex_lock(lock);\r\nsgt = &attach->sgt;\r\nif (attach->dir == dir) {\r\nmutex_unlock(lock);\r\nreturn sgt;\r\n}\r\nif (attach->dir != DMA_NONE) {\r\ndma_unmap_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\nattach->dir);\r\nattach->dir = DMA_NONE;\r\n}\r\nret = dma_map_sg(db_attach->dev, sgt->sgl, sgt->orig_nents, dir);\r\nif (ret <= 0) {\r\npr_err("failed to map scatterlist\n");\r\nmutex_unlock(lock);\r\nreturn ERR_PTR(-EIO);\r\n}\r\nattach->dir = dir;\r\nmutex_unlock(lock);\r\nreturn sgt;\r\n}\r\nstatic void vb2_dc_dmabuf_ops_unmap(struct dma_buf_attachment *db_attach,\r\nstruct sg_table *sgt, enum dma_data_direction dir)\r\n{\r\n}\r\nstatic void vb2_dc_dmabuf_ops_release(struct dma_buf *dbuf)\r\n{\r\nvb2_dc_put(dbuf->priv);\r\n}\r\nstatic void *vb2_dc_dmabuf_ops_kmap(struct dma_buf *dbuf, unsigned long pgnum)\r\n{\r\nstruct vb2_dc_buf *buf = dbuf->priv;\r\nreturn buf->vaddr + pgnum * PAGE_SIZE;\r\n}\r\nstatic void *vb2_dc_dmabuf_ops_vmap(struct dma_buf *dbuf)\r\n{\r\nstruct vb2_dc_buf *buf = dbuf->priv;\r\nreturn buf->vaddr;\r\n}\r\nstatic int vb2_dc_dmabuf_ops_mmap(struct dma_buf *dbuf,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn vb2_dc_mmap(dbuf->priv, vma);\r\n}\r\nstatic struct sg_table *vb2_dc_get_base_sgt(struct vb2_dc_buf *buf)\r\n{\r\nint ret;\r\nstruct sg_table *sgt;\r\nsgt = kmalloc(sizeof(*sgt), GFP_KERNEL);\r\nif (!sgt) {\r\ndev_err(buf->dev, "failed to alloc sg table\n");\r\nreturn NULL;\r\n}\r\nret = dma_get_sgtable(buf->dev, sgt, buf->vaddr, buf->dma_addr,\r\nbuf->size);\r\nif (ret < 0) {\r\ndev_err(buf->dev, "failed to get scatterlist from DMA API\n");\r\nkfree(sgt);\r\nreturn NULL;\r\n}\r\nreturn sgt;\r\n}\r\nstatic struct dma_buf *vb2_dc_get_dmabuf(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nstruct dma_buf *dbuf;\r\nif (!buf->sgt_base)\r\nbuf->sgt_base = vb2_dc_get_base_sgt(buf);\r\nif (WARN_ON(!buf->sgt_base))\r\nreturn NULL;\r\ndbuf = dma_buf_export(buf, &vb2_dc_dmabuf_ops, buf->size, 0);\r\nif (IS_ERR(dbuf))\r\nreturn NULL;\r\natomic_inc(&buf->refcount);\r\nreturn dbuf;\r\n}\r\nstatic inline int vma_is_io(struct vm_area_struct *vma)\r\n{\r\nreturn !!(vma->vm_flags & (VM_IO | VM_PFNMAP));\r\n}\r\nstatic int vb2_dc_get_user_pages(unsigned long start, struct page **pages,\r\nint n_pages, struct vm_area_struct *vma, int write)\r\n{\r\nif (vma_is_io(vma)) {\r\nunsigned int i;\r\nfor (i = 0; i < n_pages; ++i, start += PAGE_SIZE) {\r\nunsigned long pfn;\r\nint ret = follow_pfn(vma, start, &pfn);\r\nif (ret) {\r\npr_err("no page for address %lu\n", start);\r\nreturn ret;\r\n}\r\npages[i] = pfn_to_page(pfn);\r\n}\r\n} else {\r\nint n;\r\nn = get_user_pages(current, current->mm, start & PAGE_MASK,\r\nn_pages, write, 1, pages, NULL);\r\nn = max(n, 0);\r\nif (n != n_pages) {\r\npr_err("got only %d of %d user pages\n", n, n_pages);\r\nwhile (n)\r\nput_page(pages[--n]);\r\nreturn -EFAULT;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void vb2_dc_put_dirty_page(struct page *page)\r\n{\r\nset_page_dirty_lock(page);\r\nput_page(page);\r\n}\r\nstatic void vb2_dc_put_userptr(void *buf_priv)\r\n{\r\nstruct vb2_dc_buf *buf = buf_priv;\r\nstruct sg_table *sgt = buf->dma_sgt;\r\ndma_unmap_sg(buf->dev, sgt->sgl, sgt->orig_nents, buf->dma_dir);\r\nif (!vma_is_io(buf->vma))\r\nvb2_dc_sgt_foreach_page(sgt, vb2_dc_put_dirty_page);\r\nsg_free_table(sgt);\r\nkfree(sgt);\r\nvb2_put_vma(buf->vma);\r\nkfree(buf);\r\n}\r\nstatic void *vb2_dc_get_userptr(void *alloc_ctx, unsigned long vaddr,\r\nunsigned long size, int write)\r\n{\r\nstruct vb2_dc_conf *conf = alloc_ctx;\r\nstruct vb2_dc_buf *buf;\r\nunsigned long start;\r\nunsigned long end;\r\nunsigned long offset;\r\nstruct page **pages;\r\nint n_pages;\r\nint ret = 0;\r\nstruct vm_area_struct *vma;\r\nstruct sg_table *sgt;\r\nunsigned long contig_size;\r\nunsigned long dma_align = dma_get_cache_alignment();\r\nif (!IS_ALIGNED(vaddr | size, dma_align)) {\r\npr_debug("user data must be aligned to %lu bytes\n", dma_align);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nif (!size) {\r\npr_debug("size is zero\n");\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nbuf = kzalloc(sizeof *buf, GFP_KERNEL);\r\nif (!buf)\r\nreturn ERR_PTR(-ENOMEM);\r\nbuf->dev = conf->dev;\r\nbuf->dma_dir = write ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\r\nstart = vaddr & PAGE_MASK;\r\noffset = vaddr & ~PAGE_MASK;\r\nend = PAGE_ALIGN(vaddr + size);\r\nn_pages = (end - start) >> PAGE_SHIFT;\r\npages = kmalloc(n_pages * sizeof(pages[0]), GFP_KERNEL);\r\nif (!pages) {\r\nret = -ENOMEM;\r\npr_err("failed to allocate pages table\n");\r\ngoto fail_buf;\r\n}\r\nvma = find_vma(current->mm, vaddr);\r\nif (!vma) {\r\npr_err("no vma for address %lu\n", vaddr);\r\nret = -EFAULT;\r\ngoto fail_pages;\r\n}\r\nif (vma->vm_end < vaddr + size) {\r\npr_err("vma at %lu is too small for %lu bytes\n", vaddr, size);\r\nret = -EFAULT;\r\ngoto fail_pages;\r\n}\r\nbuf->vma = vb2_get_vma(vma);\r\nif (!buf->vma) {\r\npr_err("failed to copy vma\n");\r\nret = -ENOMEM;\r\ngoto fail_pages;\r\n}\r\nret = vb2_dc_get_user_pages(start, pages, n_pages, vma, write);\r\nif (ret) {\r\npr_err("failed to get user pages\n");\r\ngoto fail_vma;\r\n}\r\nsgt = kzalloc(sizeof(*sgt), GFP_KERNEL);\r\nif (!sgt) {\r\npr_err("failed to allocate sg table\n");\r\nret = -ENOMEM;\r\ngoto fail_get_user_pages;\r\n}\r\nret = sg_alloc_table_from_pages(sgt, pages, n_pages,\r\noffset, size, GFP_KERNEL);\r\nif (ret) {\r\npr_err("failed to initialize sg table\n");\r\ngoto fail_sgt;\r\n}\r\nkfree(pages);\r\npages = NULL;\r\nsgt->nents = dma_map_sg(buf->dev, sgt->sgl, sgt->orig_nents,\r\nbuf->dma_dir);\r\nif (sgt->nents <= 0) {\r\npr_err("failed to map scatterlist\n");\r\nret = -EIO;\r\ngoto fail_sgt_init;\r\n}\r\ncontig_size = vb2_dc_get_contiguous_size(sgt);\r\nif (contig_size < size) {\r\npr_err("contiguous mapping is too small %lu/%lu\n",\r\ncontig_size, size);\r\nret = -EFAULT;\r\ngoto fail_map_sg;\r\n}\r\nbuf->dma_addr = sg_dma_address(sgt->sgl);\r\nbuf->size = size;\r\nbuf->dma_sgt = sgt;\r\nreturn buf;\r\nfail_map_sg:\r\ndma_unmap_sg(buf->dev, sgt->sgl, sgt->orig_nents, buf->dma_dir);\r\nfail_sgt_init:\r\nif (!vma_is_io(buf->vma))\r\nvb2_dc_sgt_foreach_page(sgt, put_page);\r\nsg_free_table(sgt);\r\nfail_sgt:\r\nkfree(sgt);\r\nfail_get_user_pages:\r\nif (pages && !vma_is_io(buf->vma))\r\nwhile (n_pages)\r\nput_page(pages[--n_pages]);\r\nfail_vma:\r\nvb2_put_vma(buf->vma);\r\nfail_pages:\r\nkfree(pages);\r\nfail_buf:\r\nkfree(buf);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic int vb2_dc_map_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_dc_buf *buf = mem_priv;\r\nstruct sg_table *sgt;\r\nunsigned long contig_size;\r\nif (WARN_ON(!buf->db_attach)) {\r\npr_err("trying to pin a non attached buffer\n");\r\nreturn -EINVAL;\r\n}\r\nif (WARN_ON(buf->dma_sgt)) {\r\npr_err("dmabuf buffer is already pinned\n");\r\nreturn 0;\r\n}\r\nsgt = dma_buf_map_attachment(buf->db_attach, buf->dma_dir);\r\nif (IS_ERR_OR_NULL(sgt)) {\r\npr_err("Error getting dmabuf scatterlist\n");\r\nreturn -EINVAL;\r\n}\r\ncontig_size = vb2_dc_get_contiguous_size(sgt);\r\nif (contig_size < buf->size) {\r\npr_err("contiguous chunk is too small %lu/%lu b\n",\r\ncontig_size, buf->size);\r\ndma_buf_unmap_attachment(buf->db_attach, sgt, buf->dma_dir);\r\nreturn -EFAULT;\r\n}\r\nbuf->dma_addr = sg_dma_address(sgt->sgl);\r\nbuf->dma_sgt = sgt;\r\nreturn 0;\r\n}\r\nstatic void vb2_dc_unmap_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_dc_buf *buf = mem_priv;\r\nstruct sg_table *sgt = buf->dma_sgt;\r\nif (WARN_ON(!buf->db_attach)) {\r\npr_err("trying to unpin a not attached buffer\n");\r\nreturn;\r\n}\r\nif (WARN_ON(!sgt)) {\r\npr_err("dmabuf buffer is already unpinned\n");\r\nreturn;\r\n}\r\ndma_buf_unmap_attachment(buf->db_attach, sgt, buf->dma_dir);\r\nbuf->dma_addr = 0;\r\nbuf->dma_sgt = NULL;\r\n}\r\nstatic void vb2_dc_detach_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_dc_buf *buf = mem_priv;\r\nif (WARN_ON(buf->dma_addr))\r\nvb2_dc_unmap_dmabuf(buf);\r\ndma_buf_detach(buf->db_attach->dmabuf, buf->db_attach);\r\nkfree(buf);\r\n}\r\nstatic void *vb2_dc_attach_dmabuf(void *alloc_ctx, struct dma_buf *dbuf,\r\nunsigned long size, int write)\r\n{\r\nstruct vb2_dc_conf *conf = alloc_ctx;\r\nstruct vb2_dc_buf *buf;\r\nstruct dma_buf_attachment *dba;\r\nif (dbuf->size < size)\r\nreturn ERR_PTR(-EFAULT);\r\nbuf = kzalloc(sizeof(*buf), GFP_KERNEL);\r\nif (!buf)\r\nreturn ERR_PTR(-ENOMEM);\r\nbuf->dev = conf->dev;\r\ndba = dma_buf_attach(dbuf, buf->dev);\r\nif (IS_ERR(dba)) {\r\npr_err("failed to attach dmabuf\n");\r\nkfree(buf);\r\nreturn dba;\r\n}\r\nbuf->dma_dir = write ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\r\nbuf->size = size;\r\nbuf->db_attach = dba;\r\nreturn buf;\r\n}\r\nvoid *vb2_dma_contig_init_ctx(struct device *dev)\r\n{\r\nstruct vb2_dc_conf *conf;\r\nconf = kzalloc(sizeof *conf, GFP_KERNEL);\r\nif (!conf)\r\nreturn ERR_PTR(-ENOMEM);\r\nconf->dev = dev;\r\nreturn conf;\r\n}\r\nvoid vb2_dma_contig_cleanup_ctx(void *alloc_ctx)\r\n{\r\nkfree(alloc_ctx);\r\n}
