static u64 uncore_msr_read_counter(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nu64 count;\r\nrdmsrl(event->hw.event_base, count);\r\nreturn count;\r\n}\r\nstatic struct event_constraint *\r\nuncore_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &event->hw.branch_reg;\r\nunsigned long flags;\r\nbool ok = false;\r\nif (reg1->idx == EXTRA_REG_NONE ||\r\n(!uncore_box_is_fake(box) && reg1->alloc))\r\nreturn NULL;\r\ner = &box->shared_regs[reg1->idx];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nif (!atomic_read(&er->ref) ||\r\n(er->config1 == reg1->config && er->config2 == reg2->config)) {\r\natomic_inc(&er->ref);\r\ner->config1 = reg1->config;\r\ner->config2 = reg2->config;\r\nok = true;\r\n}\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nif (ok) {\r\nif (!uncore_box_is_fake(box))\r\nreg1->alloc = 1;\r\nreturn NULL;\r\n}\r\nreturn &constraint_empty;\r\n}\r\nstatic void uncore_put_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nif (uncore_box_is_fake(box) || !reg1->alloc)\r\nreturn;\r\ner = &box->shared_regs[reg1->idx];\r\natomic_dec(&er->ref);\r\nreg1->alloc = 0;\r\n}\r\nstatic void snbep_uncore_pci_disable_box(struct intel_uncore_box *box)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nint box_ctl = uncore_pci_box_ctl(box);\r\nu32 config = 0;\r\nif (!pci_read_config_dword(pdev, box_ctl, &config)) {\r\nconfig |= SNBEP_PMON_BOX_CTL_FRZ;\r\npci_write_config_dword(pdev, box_ctl, config);\r\n}\r\n}\r\nstatic void snbep_uncore_pci_enable_box(struct intel_uncore_box *box)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nint box_ctl = uncore_pci_box_ctl(box);\r\nu32 config = 0;\r\nif (!pci_read_config_dword(pdev, box_ctl, &config)) {\r\nconfig &= ~SNBEP_PMON_BOX_CTL_FRZ;\r\npci_write_config_dword(pdev, box_ctl, config);\r\n}\r\n}\r\nstatic void snbep_uncore_pci_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\npci_write_config_dword(pdev, hwc->config_base, hwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nstatic void snbep_uncore_pci_disable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\npci_write_config_dword(pdev, hwc->config_base, hwc->config);\r\n}\r\nstatic u64 snbep_uncore_pci_read_counter(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 count = 0;\r\npci_read_config_dword(pdev, hwc->event_base, (u32 *)&count);\r\npci_read_config_dword(pdev, hwc->event_base + 4, (u32 *)&count + 1);\r\nreturn count;\r\n}\r\nstatic void snbep_uncore_pci_init_box(struct intel_uncore_box *box)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\npci_write_config_dword(pdev, SNBEP_PCI_PMON_BOX_CTL, SNBEP_PMON_BOX_CTL_INT);\r\n}\r\nstatic void snbep_uncore_msr_disable_box(struct intel_uncore_box *box)\r\n{\r\nu64 config;\r\nunsigned msr;\r\nmsr = uncore_msr_box_ctl(box);\r\nif (msr) {\r\nrdmsrl(msr, config);\r\nconfig |= SNBEP_PMON_BOX_CTL_FRZ;\r\nwrmsrl(msr, config);\r\n}\r\n}\r\nstatic void snbep_uncore_msr_enable_box(struct intel_uncore_box *box)\r\n{\r\nu64 config;\r\nunsigned msr;\r\nmsr = uncore_msr_box_ctl(box);\r\nif (msr) {\r\nrdmsrl(msr, config);\r\nconfig &= ~SNBEP_PMON_BOX_CTL_FRZ;\r\nwrmsrl(msr, config);\r\n}\r\n}\r\nstatic void snbep_uncore_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nif (reg1->idx != EXTRA_REG_NONE)\r\nwrmsrl(reg1->reg, reg1->config);\r\nwrmsrl(hwc->config_base, hwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nstatic void snbep_uncore_msr_disable_event(struct intel_uncore_box *box,\r\nstruct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nwrmsrl(hwc->config_base, hwc->config);\r\n}\r\nstatic void snbep_uncore_msr_init_box(struct intel_uncore_box *box)\r\n{\r\nunsigned msr = uncore_msr_box_ctl(box);\r\nif (msr)\r\nwrmsrl(msr, SNBEP_PMON_BOX_CTL_INT);\r\n}\r\nstatic int snbep_uncore_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nif (box->pmu->type == &snbep_uncore_cbox) {\r\nreg1->reg = SNBEP_C0_MSR_PMON_BOX_FILTER +\r\nSNBEP_CBO_MSR_OFFSET * box->pmu->pmu_idx;\r\nreg1->config = event->attr.config1 &\r\nSNBEP_CB0_MSR_PMON_BOX_FILTER_MASK;\r\n} else {\r\nif (box->pmu->type == &snbep_uncore_pcu) {\r\nreg1->reg = SNBEP_PCU_MSR_PMON_BOX_FILTER;\r\nreg1->config = event->attr.config1 & SNBEP_PCU_MSR_PMON_BOX_FILTER_MASK;\r\n} else {\r\nreturn 0;\r\n}\r\n}\r\nreg1->idx = 0;\r\nreturn 0;\r\n}\r\nstatic int snbep_pci2phy_map_init(void)\r\n{\r\nstruct pci_dev *ubox_dev = NULL;\r\nint i, bus, nodeid;\r\nint err = 0;\r\nu32 config = 0;\r\nwhile (1) {\r\nubox_dev = pci_get_device(PCI_VENDOR_ID_INTEL,\r\nPCI_DEVICE_ID_INTEL_JAKETOWN_UBOX,\r\nubox_dev);\r\nif (!ubox_dev)\r\nbreak;\r\nbus = ubox_dev->bus->number;\r\nerr = pci_read_config_dword(ubox_dev, 0x40, &config);\r\nif (err)\r\nbreak;\r\nnodeid = config;\r\nerr = pci_read_config_dword(ubox_dev, 0x54, &config);\r\nif (err)\r\nbreak;\r\nfor (i = 0; i < 8; i++) {\r\nif (nodeid == ((config >> (3 * i)) & 0x7)) {\r\npcibus_to_physid[bus] = i;\r\nbreak;\r\n}\r\n}\r\n};\r\nif (ubox_dev)\r\npci_dev_put(ubox_dev);\r\nreturn err ? pcibios_err_to_errno(err) : 0;\r\n}\r\nstatic void snb_uncore_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (hwc->idx < UNCORE_PMC_IDX_FIXED)\r\nwrmsrl(hwc->config_base, hwc->config | SNB_UNC_CTL_EN);\r\nelse\r\nwrmsrl(hwc->config_base, SNB_UNC_CTL_EN);\r\n}\r\nstatic void snb_uncore_msr_disable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nwrmsrl(event->hw.config_base, 0);\r\n}\r\nstatic void snb_uncore_msr_init_box(struct intel_uncore_box *box)\r\n{\r\nif (box->pmu->pmu_idx == 0) {\r\nwrmsrl(SNB_UNC_PERF_GLOBAL_CTL,\r\nSNB_UNC_GLOBAL_CTL_EN | SNB_UNC_GLOBAL_CTL_CORE_ALL);\r\n}\r\n}\r\nstatic void nhm_uncore_msr_disable_box(struct intel_uncore_box *box)\r\n{\r\nwrmsrl(NHM_UNC_PERF_GLOBAL_CTL, 0);\r\n}\r\nstatic void nhm_uncore_msr_enable_box(struct intel_uncore_box *box)\r\n{\r\nwrmsrl(NHM_UNC_PERF_GLOBAL_CTL, NHM_UNC_GLOBAL_CTL_EN_PC_ALL | NHM_UNC_GLOBAL_CTL_EN_FC);\r\n}\r\nstatic void nhm_uncore_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (hwc->idx < UNCORE_PMC_IDX_FIXED)\r\nwrmsrl(hwc->config_base, hwc->config | SNB_UNC_CTL_EN);\r\nelse\r\nwrmsrl(hwc->config_base, NHM_UNC_FIXED_CTR_CTL_EN);\r\n}\r\nstatic void nhmex_uncore_msr_init_box(struct intel_uncore_box *box)\r\n{\r\nwrmsrl(NHMEX_U_MSR_PMON_GLOBAL_CTL, NHMEX_U_PMON_GLOBAL_EN_ALL);\r\n}\r\nstatic void nhmex_uncore_msr_disable_box(struct intel_uncore_box *box)\r\n{\r\nunsigned msr = uncore_msr_box_ctl(box);\r\nu64 config;\r\nif (msr) {\r\nrdmsrl(msr, config);\r\nconfig &= ~((1ULL << uncore_num_counters(box)) - 1);\r\nif (uncore_msr_fixed_ctl(box))\r\nconfig &= ~NHMEX_W_PMON_GLOBAL_FIXED_EN;\r\nwrmsrl(msr, config);\r\n}\r\n}\r\nstatic void nhmex_uncore_msr_enable_box(struct intel_uncore_box *box)\r\n{\r\nunsigned msr = uncore_msr_box_ctl(box);\r\nu64 config;\r\nif (msr) {\r\nrdmsrl(msr, config);\r\nconfig |= (1ULL << uncore_num_counters(box)) - 1;\r\nif (uncore_msr_fixed_ctl(box))\r\nconfig |= NHMEX_W_PMON_GLOBAL_FIXED_EN;\r\nwrmsrl(msr, config);\r\n}\r\n}\r\nstatic void nhmex_uncore_msr_disable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nwrmsrl(event->hw.config_base, 0);\r\n}\r\nstatic void nhmex_uncore_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (hwc->idx >= UNCORE_PMC_IDX_FIXED)\r\nwrmsrl(hwc->config_base, NHMEX_PMON_CTL_EN_BIT0);\r\nelse if (box->pmu->type->event_mask & NHMEX_PMON_CTL_EN_BIT0)\r\nwrmsrl(hwc->config_base, hwc->config | NHMEX_PMON_CTL_EN_BIT22);\r\nelse\r\nwrmsrl(hwc->config_base, hwc->config | NHMEX_PMON_CTL_EN_BIT0);\r\n}\r\nstatic int nhmex_bbox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nint ctr, ev_sel;\r\nctr = (hwc->config & NHMEX_B_PMON_CTR_MASK) >>\r\nNHMEX_B_PMON_CTR_SHIFT;\r\nev_sel = (hwc->config & NHMEX_B_PMON_CTL_EV_SEL_MASK) >>\r\nNHMEX_B_PMON_CTL_EV_SEL_SHIFT;\r\nif ((ctr == 0 && ev_sel > 0x3) || (ctr == 1 && ev_sel > 0x6) ||\r\n(ctr == 2 && ev_sel != 0x4) || ctr == 3)\r\nreturn 0;\r\nif (box->pmu->pmu_idx == 0)\r\nreg1->reg = NHMEX_B0_MSR_MATCH;\r\nelse\r\nreg1->reg = NHMEX_B1_MSR_MATCH;\r\nreg1->idx = 0;\r\nreg1->config = event->attr.config1;\r\nreg2->config = event->attr.config2;\r\nreturn 0;\r\n}\r\nstatic void nhmex_bbox_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nif (reg1->idx != EXTRA_REG_NONE) {\r\nwrmsrl(reg1->reg, reg1->config);\r\nwrmsrl(reg1->reg + 1, reg2->config);\r\n}\r\nwrmsrl(hwc->config_base, NHMEX_PMON_CTL_EN_BIT0 |\r\n(hwc->config & NHMEX_B_PMON_CTL_EV_SEL_MASK));\r\n}\r\nstatic int nhmex_sbox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nif ((hwc->config & NHMEX_PMON_CTL_EV_SEL_MASK) !=\r\nNHMEX_S_EVENT_TO_R_PROG_EV)\r\nreturn 0;\r\nif (box->pmu->pmu_idx == 0)\r\nreg1->reg = NHMEX_S0_MSR_MM_CFG;\r\nelse\r\nreg1->reg = NHMEX_S1_MSR_MM_CFG;\r\nreg1->idx = 0;\r\nreg1->config = event->attr.config1;\r\nreg2->config = event->attr.config2;\r\nreturn 0;\r\n}\r\nstatic void nhmex_sbox_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nif (reg1->idx != EXTRA_REG_NONE) {\r\nwrmsrl(reg1->reg, 0);\r\nwrmsrl(reg1->reg + 1, reg1->config);\r\nwrmsrl(reg1->reg + 2, reg2->config);\r\nwrmsrl(reg1->reg, NHMEX_S_PMON_MM_CFG_EN);\r\n}\r\nwrmsrl(hwc->config_base, hwc->config | NHMEX_PMON_CTL_EN_BIT22);\r\n}\r\nstatic bool nhmex_mbox_get_shared_reg(struct intel_uncore_box *box, int idx, u64 config)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nunsigned long flags;\r\nbool ret = false;\r\nu64 mask;\r\nif (idx < EXTRA_REG_NHMEX_M_ZDP_CTL_FVC) {\r\ner = &box->shared_regs[idx];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nif (!atomic_read(&er->ref) || er->config == config) {\r\natomic_inc(&er->ref);\r\ner->config = config;\r\nret = true;\r\n}\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nreturn ret;\r\n}\r\nidx -= EXTRA_REG_NHMEX_M_ZDP_CTL_FVC;\r\nif (WARN_ON_ONCE(idx >= 4))\r\nreturn false;\r\nif (uncore_nhmex)\r\nmask = NHMEX_M_PMON_ZDP_CTL_FVC_MASK;\r\nelse\r\nmask = WSMEX_M_PMON_ZDP_CTL_FVC_MASK;\r\ner = &box->shared_regs[EXTRA_REG_NHMEX_M_ZDP_CTL_FVC];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nif (__BITS_VALUE(atomic_read(&er->ref), idx, 8)) {\r\nif (uncore_nhmex)\r\nmask |= NHMEX_M_PMON_ZDP_CTL_FVC_EVENT_MASK(idx);\r\nelse\r\nmask |= WSMEX_M_PMON_ZDP_CTL_FVC_EVENT_MASK(idx);\r\n}\r\nif (!atomic_read(&er->ref) || !((er->config ^ config) & mask)) {\r\natomic_add(1 << (idx * 8), &er->ref);\r\nif (uncore_nhmex)\r\nmask = NHMEX_M_PMON_ZDP_CTL_FVC_MASK |\r\nNHMEX_M_PMON_ZDP_CTL_FVC_EVENT_MASK(idx);\r\nelse\r\nmask = WSMEX_M_PMON_ZDP_CTL_FVC_MASK |\r\nWSMEX_M_PMON_ZDP_CTL_FVC_EVENT_MASK(idx);\r\ner->config &= ~mask;\r\ner->config |= (config & mask);\r\nret = true;\r\n}\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void nhmex_mbox_put_shared_reg(struct intel_uncore_box *box, int idx)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nif (idx < EXTRA_REG_NHMEX_M_ZDP_CTL_FVC) {\r\ner = &box->shared_regs[idx];\r\natomic_dec(&er->ref);\r\nreturn;\r\n}\r\nidx -= EXTRA_REG_NHMEX_M_ZDP_CTL_FVC;\r\ner = &box->shared_regs[EXTRA_REG_NHMEX_M_ZDP_CTL_FVC];\r\natomic_sub(1 << (idx * 8), &er->ref);\r\n}\r\nu64 nhmex_mbox_alter_er(struct perf_event *event, int new_idx, bool modify)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nint idx, orig_idx = __BITS_VALUE(reg1->idx, 0, 8);\r\nu64 config = reg1->config;\r\nidx = orig_idx - EXTRA_REG_NHMEX_M_ZDP_CTL_FVC;\r\nif (uncore_nhmex)\r\nconfig &= NHMEX_M_PMON_ZDP_CTL_FVC_EVENT_MASK(idx);\r\nelse\r\nconfig &= WSMEX_M_PMON_ZDP_CTL_FVC_EVENT_MASK(idx);\r\nif (new_idx > orig_idx) {\r\nidx = new_idx - orig_idx;\r\nconfig <<= 3 * idx;\r\n} else {\r\nidx = orig_idx - new_idx;\r\nconfig >>= 3 * idx;\r\n}\r\nif (uncore_nhmex)\r\nconfig |= NHMEX_M_PMON_ZDP_CTL_FVC_MASK & reg1->config;\r\nelse\r\nconfig |= WSMEX_M_PMON_ZDP_CTL_FVC_MASK & reg1->config;\r\nconfig |= NHMEX_M_PMON_ZDP_CTL_FVC_MASK & reg1->config;\r\nif (modify) {\r\nif (new_idx > orig_idx)\r\nhwc->config += idx << NHMEX_M_PMON_CTL_INC_SEL_SHIFT;\r\nelse\r\nhwc->config -= idx << NHMEX_M_PMON_CTL_INC_SEL_SHIFT;\r\nreg1->config = config;\r\nreg1->idx = ~0xff | new_idx;\r\n}\r\nreturn config;\r\n}\r\nstatic struct event_constraint *\r\nnhmex_mbox_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &event->hw.branch_reg;\r\nint i, idx[2], alloc = 0;\r\nu64 config1 = reg1->config;\r\nidx[0] = __BITS_VALUE(reg1->idx, 0, 8);\r\nidx[1] = __BITS_VALUE(reg1->idx, 1, 8);\r\nagain:\r\nfor (i = 0; i < 2; i++) {\r\nif (!uncore_box_is_fake(box) && (reg1->alloc & (0x1 << i)))\r\nidx[i] = 0xff;\r\nif (idx[i] == 0xff)\r\ncontinue;\r\nif (!nhmex_mbox_get_shared_reg(box, idx[i],\r\n__BITS_VALUE(config1, i, 32)))\r\ngoto fail;\r\nalloc |= (0x1 << i);\r\n}\r\nif (reg2->idx != EXTRA_REG_NONE &&\r\n(uncore_box_is_fake(box) || !reg2->alloc) &&\r\n!nhmex_mbox_get_shared_reg(box, reg2->idx, reg2->config))\r\ngoto fail;\r\nif (!uncore_box_is_fake(box)) {\r\nif (idx[0] != 0xff && idx[0] != __BITS_VALUE(reg1->idx, 0, 8))\r\nnhmex_mbox_alter_er(event, idx[0], true);\r\nreg1->alloc |= alloc;\r\nif (reg2->idx != EXTRA_REG_NONE)\r\nreg2->alloc = 1;\r\n}\r\nreturn NULL;\r\nfail:\r\nif (idx[0] != 0xff && !(alloc & 0x1) &&\r\nidx[0] >= EXTRA_REG_NHMEX_M_ZDP_CTL_FVC) {\r\nBUG_ON(__BITS_VALUE(reg1->idx, 1, 8) != 0xff);\r\nidx[0] -= EXTRA_REG_NHMEX_M_ZDP_CTL_FVC;\r\nidx[0] = (idx[0] + 1) % 4;\r\nidx[0] += EXTRA_REG_NHMEX_M_ZDP_CTL_FVC;\r\nif (idx[0] != __BITS_VALUE(reg1->idx, 0, 8)) {\r\nconfig1 = nhmex_mbox_alter_er(event, idx[0], false);\r\ngoto again;\r\n}\r\n}\r\nif (alloc & 0x1)\r\nnhmex_mbox_put_shared_reg(box, idx[0]);\r\nif (alloc & 0x2)\r\nnhmex_mbox_put_shared_reg(box, idx[1]);\r\nreturn &constraint_empty;\r\n}\r\nstatic void nhmex_mbox_put_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &event->hw.branch_reg;\r\nif (uncore_box_is_fake(box))\r\nreturn;\r\nif (reg1->alloc & 0x1)\r\nnhmex_mbox_put_shared_reg(box, __BITS_VALUE(reg1->idx, 0, 8));\r\nif (reg1->alloc & 0x2)\r\nnhmex_mbox_put_shared_reg(box, __BITS_VALUE(reg1->idx, 1, 8));\r\nreg1->alloc = 0;\r\nif (reg2->alloc) {\r\nnhmex_mbox_put_shared_reg(box, reg2->idx);\r\nreg2->alloc = 0;\r\n}\r\n}\r\nstatic int nhmex_mbox_extra_reg_idx(struct extra_reg *er)\r\n{\r\nif (er->idx < EXTRA_REG_NHMEX_M_ZDP_CTL_FVC)\r\nreturn er->idx;\r\nreturn er->idx + (er->event >> NHMEX_M_PMON_CTL_INC_SEL_SHIFT) - 0xd;\r\n}\r\nstatic int nhmex_mbox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_type *type = box->pmu->type;\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &event->hw.branch_reg;\r\nstruct extra_reg *er;\r\nunsigned msr;\r\nint reg_idx = 0;\r\nfor (er = nhmex_uncore_mbox_extra_regs; er->msr; er++) {\r\nif (er->event != (event->hw.config & er->config_mask))\r\ncontinue;\r\nif (event->attr.config1 & ~er->valid_mask)\r\nreturn -EINVAL;\r\nmsr = er->msr + type->msr_offset * box->pmu->pmu_idx;\r\nif (WARN_ON_ONCE(msr >= 0xffff || er->idx >= 0xff))\r\nreturn -EINVAL;\r\nif (er->idx == EXTRA_REG_NHMEX_M_PLD)\r\nreg_idx = 1;\r\nelse if (WARN_ON_ONCE(reg_idx > 0))\r\nreturn -EINVAL;\r\nreg1->idx &= ~(0xff << (reg_idx * 8));\r\nreg1->reg &= ~(0xffff << (reg_idx * 16));\r\nreg1->idx |= nhmex_mbox_extra_reg_idx(er) << (reg_idx * 8);\r\nreg1->reg |= msr << (reg_idx * 16);\r\nreg1->config = event->attr.config1;\r\nreg_idx++;\r\n}\r\nif (reg_idx == 2) {\r\nreg2->idx = EXTRA_REG_NHMEX_M_FILTER;\r\nif (event->attr.config2 & NHMEX_M_PMON_MM_CFG_EN)\r\nreg2->config = event->attr.config2;\r\nelse\r\nreg2->config = ~0ULL;\r\nif (box->pmu->pmu_idx == 0)\r\nreg2->reg = NHMEX_M0_MSR_PMU_MM_CFG;\r\nelse\r\nreg2->reg = NHMEX_M1_MSR_PMU_MM_CFG;\r\n}\r\nreturn 0;\r\n}\r\nstatic u64 nhmex_mbox_shared_reg_config(struct intel_uncore_box *box, int idx)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nunsigned long flags;\r\nu64 config;\r\nif (idx < EXTRA_REG_NHMEX_M_ZDP_CTL_FVC)\r\nreturn box->shared_regs[idx].config;\r\ner = &box->shared_regs[EXTRA_REG_NHMEX_M_ZDP_CTL_FVC];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nconfig = er->config;\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nreturn config;\r\n}\r\nstatic void nhmex_mbox_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nint idx;\r\nidx = __BITS_VALUE(reg1->idx, 0, 8);\r\nif (idx != 0xff)\r\nwrmsrl(__BITS_VALUE(reg1->reg, 0, 16),\r\nnhmex_mbox_shared_reg_config(box, idx));\r\nidx = __BITS_VALUE(reg1->idx, 1, 8);\r\nif (idx != 0xff)\r\nwrmsrl(__BITS_VALUE(reg1->reg, 1, 16),\r\nnhmex_mbox_shared_reg_config(box, idx));\r\nif (reg2->idx != EXTRA_REG_NONE) {\r\nwrmsrl(reg2->reg, 0);\r\nif (reg2->config != ~0ULL) {\r\nwrmsrl(reg2->reg + 1,\r\nreg2->config & NHMEX_M_PMON_ADDR_MATCH_MASK);\r\nwrmsrl(reg2->reg + 2, NHMEX_M_PMON_ADDR_MASK_MASK &\r\n(reg2->config >> NHMEX_M_PMON_ADDR_MASK_SHIFT));\r\nwrmsrl(reg2->reg, NHMEX_M_PMON_MM_CFG_EN);\r\n}\r\n}\r\nwrmsrl(hwc->config_base, hwc->config | NHMEX_PMON_CTL_EN_BIT0);\r\n}\r\nvoid nhmex_rbox_alter_er(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nif (reg1->idx % 2) {\r\nreg1->idx--;\r\nhwc->config -= 1 << NHMEX_R_PMON_CTL_EV_SEL_SHIFT;\r\n} else {\r\nreg1->idx++;\r\nhwc->config += 1 << NHMEX_R_PMON_CTL_EV_SEL_SHIFT;\r\n}\r\nswitch (reg1->idx % 6) {\r\ncase 2:\r\nreg1->config >>= 8;\r\nbreak;\r\ncase 3:\r\nreg1->config <<= 8;\r\nbreak;\r\n};\r\n}\r\nstatic struct event_constraint *\r\nnhmex_rbox_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nstruct intel_uncore_extra_reg *er;\r\nunsigned long flags;\r\nint idx, er_idx;\r\nu64 config1;\r\nbool ok = false;\r\nif (!uncore_box_is_fake(box) && reg1->alloc)\r\nreturn NULL;\r\nidx = reg1->idx % 6;\r\nconfig1 = reg1->config;\r\nagain:\r\ner_idx = idx;\r\nif (er_idx > 2)\r\ner_idx--;\r\ner_idx += (reg1->idx / 6) * 5;\r\ner = &box->shared_regs[er_idx];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nif (idx < 2) {\r\nif (!atomic_read(&er->ref) || er->config == reg1->config) {\r\natomic_inc(&er->ref);\r\ner->config = reg1->config;\r\nok = true;\r\n}\r\n} else if (idx == 2 || idx == 3) {\r\nu64 mask = 0xff << ((idx - 2) * 8);\r\nif (!__BITS_VALUE(atomic_read(&er->ref), idx - 2, 8) ||\r\n!((er->config ^ config1) & mask)) {\r\natomic_add(1 << ((idx - 2) * 8), &er->ref);\r\ner->config &= ~mask;\r\ner->config |= config1 & mask;\r\nok = true;\r\n}\r\n} else {\r\nif (!atomic_read(&er->ref) ||\r\n(er->config == (hwc->config >> 32) &&\r\ner->config1 == reg1->config &&\r\ner->config2 == reg2->config)) {\r\natomic_inc(&er->ref);\r\ner->config = (hwc->config >> 32);\r\ner->config1 = reg1->config;\r\ner->config2 = reg2->config;\r\nok = true;\r\n}\r\n}\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nif (!ok) {\r\nif (idx % 2)\r\nidx--;\r\nelse\r\nidx++;\r\nif (idx != reg1->idx % 6) {\r\nif (idx == 2)\r\nconfig1 >>= 8;\r\nelse if (idx == 3)\r\nconfig1 <<= 8;\r\ngoto again;\r\n}\r\n} else {\r\nif (!uncore_box_is_fake(box)) {\r\nif (idx != reg1->idx % 6)\r\nnhmex_rbox_alter_er(box, event);\r\nreg1->alloc = 1;\r\n}\r\nreturn NULL;\r\n}\r\nreturn &constraint_empty;\r\n}\r\nstatic void nhmex_rbox_put_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nint idx, er_idx;\r\nif (uncore_box_is_fake(box) || !reg1->alloc)\r\nreturn;\r\nidx = reg1->idx % 6;\r\ner_idx = idx;\r\nif (er_idx > 2)\r\ner_idx--;\r\ner_idx += (reg1->idx / 6) * 5;\r\ner = &box->shared_regs[er_idx];\r\nif (idx == 2 || idx == 3)\r\natomic_sub(1 << ((idx - 2) * 8), &er->ref);\r\nelse\r\natomic_dec(&er->ref);\r\nreg1->alloc = 0;\r\n}\r\nstatic int nhmex_rbox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &event->hw.branch_reg;\r\nint idx;\r\nidx = (event->hw.config & NHMEX_R_PMON_CTL_EV_SEL_MASK) >>\r\nNHMEX_R_PMON_CTL_EV_SEL_SHIFT;\r\nif (idx >= 0x18)\r\nreturn -EINVAL;\r\nreg1->idx = idx;\r\nreg1->config = event->attr.config1;\r\nswitch (idx % 6) {\r\ncase 4:\r\ncase 5:\r\nhwc->config |= event->attr.config & (~0ULL << 32);\r\nreg2->config = event->attr.config2;\r\nbreak;\r\n};\r\nreturn 0;\r\n}\r\nstatic u64 nhmex_rbox_shared_reg_config(struct intel_uncore_box *box, int idx)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nunsigned long flags;\r\nu64 config;\r\ner = &box->shared_regs[idx];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nconfig = er->config;\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nreturn config;\r\n}\r\nstatic void nhmex_rbox_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nint idx, port;\r\nidx = reg1->idx;\r\nport = idx / 6 + box->pmu->pmu_idx * 4;\r\nswitch (idx % 6) {\r\ncase 0:\r\nwrmsrl(NHMEX_R_MSR_PORTN_IPERF_CFG0(port), reg1->config);\r\nbreak;\r\ncase 1:\r\nwrmsrl(NHMEX_R_MSR_PORTN_IPERF_CFG1(port), reg1->config);\r\nbreak;\r\ncase 2:\r\ncase 3:\r\nwrmsrl(NHMEX_R_MSR_PORTN_QLX_CFG(port),\r\nnhmex_rbox_shared_reg_config(box, 2 + (idx / 6) * 5));\r\nbreak;\r\ncase 4:\r\nwrmsrl(NHMEX_R_MSR_PORTN_XBR_SET1_MM_CFG(port),\r\nhwc->config >> 32);\r\nwrmsrl(NHMEX_R_MSR_PORTN_XBR_SET1_MATCH(port), reg1->config);\r\nwrmsrl(NHMEX_R_MSR_PORTN_XBR_SET1_MASK(port), reg2->config);\r\nbreak;\r\ncase 5:\r\nwrmsrl(NHMEX_R_MSR_PORTN_XBR_SET2_MM_CFG(port),\r\nhwc->config >> 32);\r\nwrmsrl(NHMEX_R_MSR_PORTN_XBR_SET2_MATCH(port), reg1->config);\r\nwrmsrl(NHMEX_R_MSR_PORTN_XBR_SET2_MASK(port), reg2->config);\r\nbreak;\r\n};\r\nwrmsrl(hwc->config_base, NHMEX_PMON_CTL_EN_BIT0 |\r\n(hwc->config & NHMEX_R_PMON_CTL_EV_SEL_MASK));\r\n}\r\nstatic void uncore_assign_hw_event(struct intel_uncore_box *box, struct perf_event *event, int idx)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nhwc->idx = idx;\r\nhwc->last_tag = ++box->tags[idx];\r\nif (hwc->idx == UNCORE_PMC_IDX_FIXED) {\r\nhwc->event_base = uncore_fixed_ctr(box);\r\nhwc->config_base = uncore_fixed_ctl(box);\r\nreturn;\r\n}\r\nhwc->config_base = uncore_event_ctl(box, hwc->idx);\r\nhwc->event_base = uncore_perf_ctr(box, hwc->idx);\r\n}\r\nstatic void uncore_perf_event_update(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nu64 prev_count, new_count, delta;\r\nint shift;\r\nif (event->hw.idx >= UNCORE_PMC_IDX_FIXED)\r\nshift = 64 - uncore_fixed_ctr_bits(box);\r\nelse\r\nshift = 64 - uncore_perf_ctr_bits(box);\r\nagain:\r\nprev_count = local64_read(&event->hw.prev_count);\r\nnew_count = uncore_read_counter(box, event);\r\nif (local64_xchg(&event->hw.prev_count, new_count) != prev_count)\r\ngoto again;\r\ndelta = (new_count << shift) - (prev_count << shift);\r\ndelta >>= shift;\r\nlocal64_add(delta, &event->count);\r\n}\r\nstatic enum hrtimer_restart uncore_pmu_hrtimer(struct hrtimer *hrtimer)\r\n{\r\nstruct intel_uncore_box *box;\r\nunsigned long flags;\r\nint bit;\r\nbox = container_of(hrtimer, struct intel_uncore_box, hrtimer);\r\nif (!box->n_active || box->cpu != smp_processor_id())\r\nreturn HRTIMER_NORESTART;\r\nlocal_irq_save(flags);\r\nfor_each_set_bit(bit, box->active_mask, UNCORE_PMC_IDX_MAX)\r\nuncore_perf_event_update(box, box->events[bit]);\r\nlocal_irq_restore(flags);\r\nhrtimer_forward_now(hrtimer, ns_to_ktime(UNCORE_PMU_HRTIMER_INTERVAL));\r\nreturn HRTIMER_RESTART;\r\n}\r\nstatic void uncore_pmu_start_hrtimer(struct intel_uncore_box *box)\r\n{\r\n__hrtimer_start_range_ns(&box->hrtimer,\r\nns_to_ktime(UNCORE_PMU_HRTIMER_INTERVAL), 0,\r\nHRTIMER_MODE_REL_PINNED, 0);\r\n}\r\nstatic void uncore_pmu_cancel_hrtimer(struct intel_uncore_box *box)\r\n{\r\nhrtimer_cancel(&box->hrtimer);\r\n}\r\nstatic void uncore_pmu_init_hrtimer(struct intel_uncore_box *box)\r\n{\r\nhrtimer_init(&box->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nbox->hrtimer.function = uncore_pmu_hrtimer;\r\n}\r\nstruct intel_uncore_box *uncore_alloc_box(struct intel_uncore_type *type, int cpu)\r\n{\r\nstruct intel_uncore_box *box;\r\nint i, size;\r\nsize = sizeof(*box) + type->num_shared_regs * sizeof(struct intel_uncore_extra_reg);\r\nbox = kmalloc_node(size, GFP_KERNEL | __GFP_ZERO, cpu_to_node(cpu));\r\nif (!box)\r\nreturn NULL;\r\nfor (i = 0; i < type->num_shared_regs; i++)\r\nraw_spin_lock_init(&box->shared_regs[i].lock);\r\nuncore_pmu_init_hrtimer(box);\r\natomic_set(&box->refcnt, 1);\r\nbox->cpu = -1;\r\nbox->phys_id = -1;\r\nreturn box;\r\n}\r\nstatic struct intel_uncore_box *\r\nuncore_pmu_to_box(struct intel_uncore_pmu *pmu, int cpu)\r\n{\r\nstruct intel_uncore_box *box;\r\nbox = *per_cpu_ptr(pmu->box, cpu);\r\nif (box)\r\nreturn box;\r\nraw_spin_lock(&uncore_box_lock);\r\nlist_for_each_entry(box, &pmu->box_list, list) {\r\nif (box->phys_id == topology_physical_package_id(cpu)) {\r\natomic_inc(&box->refcnt);\r\n*per_cpu_ptr(pmu->box, cpu) = box;\r\nbreak;\r\n}\r\n}\r\nraw_spin_unlock(&uncore_box_lock);\r\nreturn *per_cpu_ptr(pmu->box, cpu);\r\n}\r\nstatic struct intel_uncore_pmu *uncore_event_to_pmu(struct perf_event *event)\r\n{\r\nreturn container_of(event->pmu, struct intel_uncore_pmu, pmu);\r\n}\r\nstatic struct intel_uncore_box *uncore_event_to_box(struct perf_event *event)\r\n{\r\nreturn uncore_pmu_to_box(uncore_event_to_pmu(event), smp_processor_id());\r\n}\r\nstatic int\r\nuncore_collect_events(struct intel_uncore_box *box, struct perf_event *leader, bool dogrp)\r\n{\r\nstruct perf_event *event;\r\nint n, max_count;\r\nmax_count = box->pmu->type->num_counters;\r\nif (box->pmu->type->fixed_ctl)\r\nmax_count++;\r\nif (box->n_events >= max_count)\r\nreturn -EINVAL;\r\nn = box->n_events;\r\nbox->event_list[n] = leader;\r\nn++;\r\nif (!dogrp)\r\nreturn n;\r\nlist_for_each_entry(event, &leader->sibling_list, group_entry) {\r\nif (event->state <= PERF_EVENT_STATE_OFF)\r\ncontinue;\r\nif (n >= max_count)\r\nreturn -EINVAL;\r\nbox->event_list[n] = event;\r\nn++;\r\n}\r\nreturn n;\r\n}\r\nstatic struct event_constraint *\r\nuncore_get_event_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_type *type = box->pmu->type;\r\nstruct event_constraint *c;\r\nif (type->ops->get_constraint) {\r\nc = type->ops->get_constraint(box, event);\r\nif (c)\r\nreturn c;\r\n}\r\nif (event->hw.config == ~0ULL)\r\nreturn &constraint_fixed;\r\nif (type->constraints) {\r\nfor_each_event_constraint(c, type->constraints) {\r\nif ((event->hw.config & c->cmask) == c->code)\r\nreturn c;\r\n}\r\n}\r\nreturn &type->unconstrainted;\r\n}\r\nstatic void uncore_put_event_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nif (box->pmu->type->ops->put_constraint)\r\nbox->pmu->type->ops->put_constraint(box, event);\r\n}\r\nstatic int uncore_assign_events(struct intel_uncore_box *box, int assign[], int n)\r\n{\r\nunsigned long used_mask[BITS_TO_LONGS(UNCORE_PMC_IDX_MAX)];\r\nstruct event_constraint *c, *constraints[UNCORE_PMC_IDX_MAX];\r\nint i, wmin, wmax, ret = 0;\r\nstruct hw_perf_event *hwc;\r\nbitmap_zero(used_mask, UNCORE_PMC_IDX_MAX);\r\nfor (i = 0, wmin = UNCORE_PMC_IDX_MAX, wmax = 0; i < n; i++) {\r\nc = uncore_get_event_constraint(box, box->event_list[i]);\r\nconstraints[i] = c;\r\nwmin = min(wmin, c->weight);\r\nwmax = max(wmax, c->weight);\r\n}\r\nfor (i = 0; i < n; i++) {\r\nhwc = &box->event_list[i]->hw;\r\nc = constraints[i];\r\nif (hwc->idx == -1)\r\nbreak;\r\nif (!test_bit(hwc->idx, c->idxmsk))\r\nbreak;\r\nif (test_bit(hwc->idx, used_mask))\r\nbreak;\r\n__set_bit(hwc->idx, used_mask);\r\nif (assign)\r\nassign[i] = hwc->idx;\r\n}\r\nif (i != n)\r\nret = perf_assign_events(constraints, n, wmin, wmax, assign);\r\nif (!assign || ret) {\r\nfor (i = 0; i < n; i++)\r\nuncore_put_event_constraint(box, box->event_list[i]);\r\n}\r\nreturn ret ? -EINVAL : 0;\r\n}\r\nstatic void uncore_pmu_event_start(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nint idx = event->hw.idx;\r\nif (WARN_ON_ONCE(!(event->hw.state & PERF_HES_STOPPED)))\r\nreturn;\r\nif (WARN_ON_ONCE(idx == -1 || idx >= UNCORE_PMC_IDX_MAX))\r\nreturn;\r\nevent->hw.state = 0;\r\nbox->events[idx] = event;\r\nbox->n_active++;\r\n__set_bit(idx, box->active_mask);\r\nlocal64_set(&event->hw.prev_count, uncore_read_counter(box, event));\r\nuncore_enable_event(box, event);\r\nif (box->n_active == 1) {\r\nuncore_enable_box(box);\r\nuncore_pmu_start_hrtimer(box);\r\n}\r\n}\r\nstatic void uncore_pmu_event_stop(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (__test_and_clear_bit(hwc->idx, box->active_mask)) {\r\nuncore_disable_event(box, event);\r\nbox->n_active--;\r\nbox->events[hwc->idx] = NULL;\r\nWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\r\nhwc->state |= PERF_HES_STOPPED;\r\nif (box->n_active == 0) {\r\nuncore_disable_box(box);\r\nuncore_pmu_cancel_hrtimer(box);\r\n}\r\n}\r\nif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\r\nuncore_perf_event_update(box, event);\r\nhwc->state |= PERF_HES_UPTODATE;\r\n}\r\n}\r\nstatic int uncore_pmu_event_add(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint assign[UNCORE_PMC_IDX_MAX];\r\nint i, n, ret;\r\nif (!box)\r\nreturn -ENODEV;\r\nret = n = uncore_collect_events(box, event, false);\r\nif (ret < 0)\r\nreturn ret;\r\nhwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\r\nif (!(flags & PERF_EF_START))\r\nhwc->state |= PERF_HES_ARCH;\r\nret = uncore_assign_events(box, assign, n);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < box->n_events; i++) {\r\nevent = box->event_list[i];\r\nhwc = &event->hw;\r\nif (hwc->idx == assign[i] &&\r\nhwc->last_tag == box->tags[assign[i]])\r\ncontinue;\r\nif (hwc->state & PERF_HES_STOPPED)\r\nhwc->state |= PERF_HES_ARCH;\r\nuncore_pmu_event_stop(event, PERF_EF_UPDATE);\r\n}\r\nfor (i = 0; i < n; i++) {\r\nevent = box->event_list[i];\r\nhwc = &event->hw;\r\nif (hwc->idx != assign[i] ||\r\nhwc->last_tag != box->tags[assign[i]])\r\nuncore_assign_hw_event(box, event, assign[i]);\r\nelse if (i < box->n_events)\r\ncontinue;\r\nif (hwc->state & PERF_HES_ARCH)\r\ncontinue;\r\nuncore_pmu_event_start(event, 0);\r\n}\r\nbox->n_events = n;\r\nreturn 0;\r\n}\r\nstatic void uncore_pmu_event_del(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nint i;\r\nuncore_pmu_event_stop(event, PERF_EF_UPDATE);\r\nfor (i = 0; i < box->n_events; i++) {\r\nif (event == box->event_list[i]) {\r\nuncore_put_event_constraint(box, event);\r\nwhile (++i < box->n_events)\r\nbox->event_list[i - 1] = box->event_list[i];\r\n--box->n_events;\r\nbreak;\r\n}\r\n}\r\nevent->hw.idx = -1;\r\nevent->hw.last_tag = ~0ULL;\r\n}\r\nstatic void uncore_pmu_event_read(struct perf_event *event)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nuncore_perf_event_update(box, event);\r\n}\r\nstatic int uncore_validate_group(struct intel_uncore_pmu *pmu,\r\nstruct perf_event *event)\r\n{\r\nstruct perf_event *leader = event->group_leader;\r\nstruct intel_uncore_box *fake_box;\r\nint ret = -EINVAL, n;\r\nfake_box = uncore_alloc_box(pmu->type, smp_processor_id());\r\nif (!fake_box)\r\nreturn -ENOMEM;\r\nfake_box->pmu = pmu;\r\nn = uncore_collect_events(fake_box, leader, true);\r\nif (n < 0)\r\ngoto out;\r\nfake_box->n_events = n;\r\nn = uncore_collect_events(fake_box, event, false);\r\nif (n < 0)\r\ngoto out;\r\nfake_box->n_events = n;\r\nret = uncore_assign_events(fake_box, NULL, n);\r\nout:\r\nkfree(fake_box);\r\nreturn ret;\r\n}\r\nint uncore_pmu_event_init(struct perf_event *event)\r\n{\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint ret;\r\nif (event->attr.type != event->pmu->type)\r\nreturn -ENOENT;\r\npmu = uncore_event_to_pmu(event);\r\nif (pmu->func_id < 0)\r\nreturn -ENOENT;\r\nif (event->attr.exclude_user || event->attr.exclude_kernel ||\r\nevent->attr.exclude_hv || event->attr.exclude_idle)\r\nreturn -EINVAL;\r\nif (hwc->sample_period)\r\nreturn -EINVAL;\r\nif (event->cpu < 0)\r\nreturn -EINVAL;\r\nbox = uncore_pmu_to_box(pmu, event->cpu);\r\nif (!box || box->cpu < 0)\r\nreturn -EINVAL;\r\nevent->cpu = box->cpu;\r\nevent->hw.idx = -1;\r\nevent->hw.last_tag = ~0ULL;\r\nevent->hw.extra_reg.idx = EXTRA_REG_NONE;\r\nevent->hw.branch_reg.idx = EXTRA_REG_NONE;\r\nif (event->attr.config == UNCORE_FIXED_EVENT) {\r\nif (!pmu->type->fixed_ctl)\r\nreturn -EINVAL;\r\nif (pmu->type->single_fixed && pmu->pmu_idx > 0)\r\nreturn -EINVAL;\r\nhwc->config = ~0ULL;\r\n} else {\r\nhwc->config = event->attr.config & pmu->type->event_mask;\r\nif (pmu->type->ops->hw_config) {\r\nret = pmu->type->ops->hw_config(box, event);\r\nif (ret)\r\nreturn ret;\r\n}\r\n}\r\nif (event->group_leader != event)\r\nret = uncore_validate_group(pmu, event);\r\nelse\r\nret = 0;\r\nreturn ret;\r\n}\r\nstatic ssize_t uncore_get_attr_cpumask(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nint n = cpulist_scnprintf(buf, PAGE_SIZE - 2, &uncore_cpu_mask);\r\nbuf[n++] = '\n';\r\nbuf[n] = '\0';\r\nreturn n;\r\n}\r\nstatic int __init uncore_pmu_register(struct intel_uncore_pmu *pmu)\r\n{\r\nint ret;\r\npmu->pmu = (struct pmu) {\r\n.attr_groups = pmu->type->attr_groups,\r\n.task_ctx_nr = perf_invalid_context,\r\n.event_init = uncore_pmu_event_init,\r\n.add = uncore_pmu_event_add,\r\n.del = uncore_pmu_event_del,\r\n.start = uncore_pmu_event_start,\r\n.stop = uncore_pmu_event_stop,\r\n.read = uncore_pmu_event_read,\r\n};\r\nif (pmu->type->num_boxes == 1) {\r\nif (strlen(pmu->type->name) > 0)\r\nsprintf(pmu->name, "uncore_%s", pmu->type->name);\r\nelse\r\nsprintf(pmu->name, "uncore");\r\n} else {\r\nsprintf(pmu->name, "uncore_%s_%d", pmu->type->name,\r\npmu->pmu_idx);\r\n}\r\nret = perf_pmu_register(&pmu->pmu, pmu->name, -1);\r\nreturn ret;\r\n}\r\nstatic void __init uncore_type_exit(struct intel_uncore_type *type)\r\n{\r\nint i;\r\nfor (i = 0; i < type->num_boxes; i++)\r\nfree_percpu(type->pmus[i].box);\r\nkfree(type->pmus);\r\ntype->pmus = NULL;\r\nkfree(type->events_group);\r\ntype->events_group = NULL;\r\n}\r\nstatic void __init uncore_types_exit(struct intel_uncore_type **types)\r\n{\r\nint i;\r\nfor (i = 0; types[i]; i++)\r\nuncore_type_exit(types[i]);\r\n}\r\nstatic int __init uncore_type_init(struct intel_uncore_type *type)\r\n{\r\nstruct intel_uncore_pmu *pmus;\r\nstruct attribute_group *events_group;\r\nstruct attribute **attrs;\r\nint i, j;\r\npmus = kzalloc(sizeof(*pmus) * type->num_boxes, GFP_KERNEL);\r\nif (!pmus)\r\nreturn -ENOMEM;\r\ntype->unconstrainted = (struct event_constraint)\r\n__EVENT_CONSTRAINT(0, (1ULL << type->num_counters) - 1,\r\n0, type->num_counters, 0);\r\nfor (i = 0; i < type->num_boxes; i++) {\r\npmus[i].func_id = -1;\r\npmus[i].pmu_idx = i;\r\npmus[i].type = type;\r\nINIT_LIST_HEAD(&pmus[i].box_list);\r\npmus[i].box = alloc_percpu(struct intel_uncore_box *);\r\nif (!pmus[i].box)\r\ngoto fail;\r\n}\r\nif (type->event_descs) {\r\ni = 0;\r\nwhile (type->event_descs[i].attr.attr.name)\r\ni++;\r\nevents_group = kzalloc(sizeof(struct attribute *) * (i + 1) +\r\nsizeof(*events_group), GFP_KERNEL);\r\nif (!events_group)\r\ngoto fail;\r\nattrs = (struct attribute **)(events_group + 1);\r\nevents_group->name = "events";\r\nevents_group->attrs = attrs;\r\nfor (j = 0; j < i; j++)\r\nattrs[j] = &type->event_descs[j].attr.attr;\r\ntype->events_group = events_group;\r\n}\r\ntype->pmu_group = &uncore_pmu_attr_group;\r\ntype->pmus = pmus;\r\nreturn 0;\r\nfail:\r\nuncore_type_exit(type);\r\nreturn -ENOMEM;\r\n}\r\nstatic int __init uncore_types_init(struct intel_uncore_type **types)\r\n{\r\nint i, ret;\r\nfor (i = 0; types[i]; i++) {\r\nret = uncore_type_init(types[i]);\r\nif (ret)\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nwhile (--i >= 0)\r\nuncore_type_exit(types[i]);\r\nreturn ret;\r\n}\r\nstatic int uncore_pci_add(struct intel_uncore_type *type, struct pci_dev *pdev)\r\n{\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nint i, phys_id;\r\nphys_id = pcibus_to_physid[pdev->bus->number];\r\nif (phys_id < 0)\r\nreturn -ENODEV;\r\nbox = uncore_alloc_box(type, 0);\r\nif (!box)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < type->num_boxes; i++) {\r\npmu = &type->pmus[i];\r\nif (pmu->func_id == pdev->devfn)\r\nbreak;\r\nif (pmu->func_id < 0) {\r\npmu->func_id = pdev->devfn;\r\nbreak;\r\n}\r\npmu = NULL;\r\n}\r\nif (!pmu) {\r\nkfree(box);\r\nreturn -EINVAL;\r\n}\r\nbox->phys_id = phys_id;\r\nbox->pci_dev = pdev;\r\nbox->pmu = pmu;\r\nuncore_box_init(box);\r\npci_set_drvdata(pdev, box);\r\nraw_spin_lock(&uncore_box_lock);\r\nlist_add_tail(&box->list, &pmu->box_list);\r\nraw_spin_unlock(&uncore_box_lock);\r\nreturn 0;\r\n}\r\nstatic void uncore_pci_remove(struct pci_dev *pdev)\r\n{\r\nstruct intel_uncore_box *box = pci_get_drvdata(pdev);\r\nstruct intel_uncore_pmu *pmu = box->pmu;\r\nint cpu, phys_id = pcibus_to_physid[pdev->bus->number];\r\nif (WARN_ON_ONCE(phys_id != box->phys_id))\r\nreturn;\r\nraw_spin_lock(&uncore_box_lock);\r\nlist_del(&box->list);\r\nraw_spin_unlock(&uncore_box_lock);\r\nfor_each_possible_cpu(cpu) {\r\nif (*per_cpu_ptr(pmu->box, cpu) == box) {\r\n*per_cpu_ptr(pmu->box, cpu) = NULL;\r\natomic_dec(&box->refcnt);\r\n}\r\n}\r\nWARN_ON_ONCE(atomic_read(&box->refcnt) != 1);\r\nkfree(box);\r\n}\r\nstatic int uncore_pci_probe(struct pci_dev *pdev,\r\nconst struct pci_device_id *id)\r\n{\r\nstruct intel_uncore_type *type;\r\ntype = (struct intel_uncore_type *)id->driver_data;\r\nreturn uncore_pci_add(type, pdev);\r\n}\r\nstatic int __init uncore_pci_init(void)\r\n{\r\nint ret;\r\nswitch (boot_cpu_data.x86_model) {\r\ncase 45:\r\nret = snbep_pci2phy_map_init();\r\nif (ret)\r\nreturn ret;\r\npci_uncores = snbep_pci_uncores;\r\nuncore_pci_driver = &snbep_uncore_pci_driver;\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nret = uncore_types_init(pci_uncores);\r\nif (ret)\r\nreturn ret;\r\nuncore_pci_driver->probe = uncore_pci_probe;\r\nuncore_pci_driver->remove = uncore_pci_remove;\r\nret = pci_register_driver(uncore_pci_driver);\r\nif (ret == 0)\r\npcidrv_registered = true;\r\nelse\r\nuncore_types_exit(pci_uncores);\r\nreturn ret;\r\n}\r\nstatic void __init uncore_pci_exit(void)\r\n{\r\nif (pcidrv_registered) {\r\npcidrv_registered = false;\r\npci_unregister_driver(uncore_pci_driver);\r\nuncore_types_exit(pci_uncores);\r\n}\r\n}\r\nstatic void __cpuinit uncore_cpu_dying(int cpu)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nint i, j;\r\nfor (i = 0; msr_uncores[i]; i++) {\r\ntype = msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nbox = *per_cpu_ptr(pmu->box, cpu);\r\n*per_cpu_ptr(pmu->box, cpu) = NULL;\r\nif (box && atomic_dec_and_test(&box->refcnt))\r\nkfree(box);\r\n}\r\n}\r\n}\r\nstatic int __cpuinit uncore_cpu_starting(int cpu)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box, *exist;\r\nint i, j, k, phys_id;\r\nphys_id = topology_physical_package_id(cpu);\r\nfor (i = 0; msr_uncores[i]; i++) {\r\ntype = msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nbox = *per_cpu_ptr(pmu->box, cpu);\r\nif (box && box->phys_id >= 0) {\r\nuncore_box_init(box);\r\ncontinue;\r\n}\r\nfor_each_online_cpu(k) {\r\nexist = *per_cpu_ptr(pmu->box, k);\r\nif (exist && exist->phys_id == phys_id) {\r\natomic_inc(&exist->refcnt);\r\n*per_cpu_ptr(pmu->box, cpu) = exist;\r\nkfree(box);\r\nbox = NULL;\r\nbreak;\r\n}\r\n}\r\nif (box) {\r\nbox->phys_id = phys_id;\r\nuncore_box_init(box);\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int __cpuinit uncore_cpu_prepare(int cpu, int phys_id)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nint i, j;\r\nfor (i = 0; msr_uncores[i]; i++) {\r\ntype = msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nif (pmu->func_id < 0)\r\npmu->func_id = j;\r\nbox = uncore_alloc_box(type, cpu);\r\nif (!box)\r\nreturn -ENOMEM;\r\nbox->pmu = pmu;\r\nbox->phys_id = phys_id;\r\n*per_cpu_ptr(pmu->box, cpu) = box;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void __cpuinit\r\nuncore_change_context(struct intel_uncore_type **uncores, int old_cpu, int new_cpu)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nint i, j;\r\nfor (i = 0; uncores[i]; i++) {\r\ntype = uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nif (old_cpu < 0)\r\nbox = uncore_pmu_to_box(pmu, new_cpu);\r\nelse\r\nbox = uncore_pmu_to_box(pmu, old_cpu);\r\nif (!box)\r\ncontinue;\r\nif (old_cpu < 0) {\r\nWARN_ON_ONCE(box->cpu != -1);\r\nbox->cpu = new_cpu;\r\ncontinue;\r\n}\r\nWARN_ON_ONCE(box->cpu != old_cpu);\r\nif (new_cpu >= 0) {\r\nuncore_pmu_cancel_hrtimer(box);\r\nperf_pmu_migrate_context(&pmu->pmu,\r\nold_cpu, new_cpu);\r\nbox->cpu = new_cpu;\r\n} else {\r\nbox->cpu = -1;\r\n}\r\n}\r\n}\r\n}\r\nstatic void __cpuinit uncore_event_exit_cpu(int cpu)\r\n{\r\nint i, phys_id, target;\r\nif (!cpumask_test_and_clear_cpu(cpu, &uncore_cpu_mask))\r\nreturn;\r\nphys_id = topology_physical_package_id(cpu);\r\ntarget = -1;\r\nfor_each_online_cpu(i) {\r\nif (i == cpu)\r\ncontinue;\r\nif (phys_id == topology_physical_package_id(i)) {\r\ntarget = i;\r\nbreak;\r\n}\r\n}\r\nif (target >= 0)\r\ncpumask_set_cpu(target, &uncore_cpu_mask);\r\nuncore_change_context(msr_uncores, cpu, target);\r\nuncore_change_context(pci_uncores, cpu, target);\r\n}\r\nstatic void __cpuinit uncore_event_init_cpu(int cpu)\r\n{\r\nint i, phys_id;\r\nphys_id = topology_physical_package_id(cpu);\r\nfor_each_cpu(i, &uncore_cpu_mask) {\r\nif (phys_id == topology_physical_package_id(i))\r\nreturn;\r\n}\r\ncpumask_set_cpu(cpu, &uncore_cpu_mask);\r\nuncore_change_context(msr_uncores, -1, cpu);\r\nuncore_change_context(pci_uncores, -1, cpu);\r\n}\r\nstatic int\r\n__cpuinit uncore_cpu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (long)hcpu;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_UP_PREPARE:\r\nuncore_cpu_prepare(cpu, -1);\r\nbreak;\r\ncase CPU_STARTING:\r\nuncore_cpu_starting(cpu);\r\nbreak;\r\ncase CPU_UP_CANCELED:\r\ncase CPU_DYING:\r\nuncore_cpu_dying(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_DOWN_FAILED:\r\ncase CPU_STARTING:\r\nuncore_event_init_cpu(cpu);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\nuncore_event_exit_cpu(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void __init uncore_cpu_setup(void *dummy)\r\n{\r\nuncore_cpu_starting(smp_processor_id());\r\n}\r\nstatic int __init uncore_cpu_init(void)\r\n{\r\nint ret, cpu, max_cores;\r\nmax_cores = boot_cpu_data.x86_max_cores;\r\nswitch (boot_cpu_data.x86_model) {\r\ncase 26:\r\ncase 30:\r\ncase 37:\r\ncase 44:\r\nmsr_uncores = nhm_msr_uncores;\r\nbreak;\r\ncase 42:\r\nif (snb_uncore_cbox.num_boxes > max_cores)\r\nsnb_uncore_cbox.num_boxes = max_cores;\r\nmsr_uncores = snb_msr_uncores;\r\nbreak;\r\ncase 45:\r\nif (snbep_uncore_cbox.num_boxes > max_cores)\r\nsnbep_uncore_cbox.num_boxes = max_cores;\r\nmsr_uncores = snbep_msr_uncores;\r\nbreak;\r\ncase 46:\r\nuncore_nhmex = true;\r\ncase 47:\r\nif (!uncore_nhmex)\r\nnhmex_uncore_mbox.event_descs = wsmex_uncore_mbox_events;\r\nif (nhmex_uncore_cbox.num_boxes > max_cores)\r\nnhmex_uncore_cbox.num_boxes = max_cores;\r\nmsr_uncores = nhmex_msr_uncores;\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nret = uncore_types_init(msr_uncores);\r\nif (ret)\r\nreturn ret;\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu) {\r\nint i, phys_id = topology_physical_package_id(cpu);\r\nfor_each_cpu(i, &uncore_cpu_mask) {\r\nif (phys_id == topology_physical_package_id(i)) {\r\nphys_id = -1;\r\nbreak;\r\n}\r\n}\r\nif (phys_id < 0)\r\ncontinue;\r\nuncore_cpu_prepare(cpu, phys_id);\r\nuncore_event_init_cpu(cpu);\r\n}\r\non_each_cpu(uncore_cpu_setup, NULL, 1);\r\nregister_cpu_notifier(&uncore_cpu_nb);\r\nput_online_cpus();\r\nreturn 0;\r\n}\r\nstatic int __init uncore_pmus_register(void)\r\n{\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_type *type;\r\nint i, j;\r\nfor (i = 0; msr_uncores[i]; i++) {\r\ntype = msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nuncore_pmu_register(pmu);\r\n}\r\n}\r\nfor (i = 0; pci_uncores[i]; i++) {\r\ntype = pci_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nuncore_pmu_register(pmu);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init intel_uncore_init(void)\r\n{\r\nint ret;\r\nif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)\r\nreturn -ENODEV;\r\nif (cpu_has_hypervisor)\r\nreturn -ENODEV;\r\nret = uncore_pci_init();\r\nif (ret)\r\ngoto fail;\r\nret = uncore_cpu_init();\r\nif (ret) {\r\nuncore_pci_exit();\r\ngoto fail;\r\n}\r\nuncore_pmus_register();\r\nreturn 0;\r\nfail:\r\nreturn ret;\r\n}
