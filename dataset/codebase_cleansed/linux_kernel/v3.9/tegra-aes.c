static inline u32 aes_readl(struct tegra_aes_dev *dd, u32 offset)\r\n{\r\nreturn readl(dd->io_base + offset);\r\n}\r\nstatic inline void aes_writel(struct tegra_aes_dev *dd, u32 val, u32 offset)\r\n{\r\nwritel(val, dd->io_base + offset);\r\n}\r\nstatic int aes_start_crypt(struct tegra_aes_dev *dd, u32 in_addr, u32 out_addr,\r\nint nblocks, int mode, bool upd_iv)\r\n{\r\nu32 cmdq[AES_HW_MAX_ICQ_LENGTH];\r\nint i, eng_busy, icq_empty, ret;\r\nu32 value;\r\naes_writel(dd, 0xFFFFFFFF, TEGRA_AES_INTR_STATUS);\r\naes_writel(dd, 0x33, TEGRA_AES_INT_ENB);\r\ncmdq[0] = CMD_DMASETUP << CMDQ_OPCODE_SHIFT;\r\ncmdq[1] = in_addr;\r\ncmdq[2] = CMD_BLKSTARTENGINE << CMDQ_OPCODE_SHIFT | (nblocks-1);\r\ncmdq[3] = CMD_DMACOMPLETE << CMDQ_OPCODE_SHIFT;\r\nvalue = aes_readl(dd, TEGRA_AES_CMDQUE_CONTROL);\r\nvalue &= ~TEGRA_AES_CMDQ_CTRL_SRC_STM_SEL_FIELD;\r\nvalue &= ~TEGRA_AES_CMDQ_CTRL_DST_STM_SEL_FIELD;\r\nvalue |= TEGRA_AES_CMDQ_CTRL_SRC_STM_SEL_FIELD |\r\nTEGRA_AES_CMDQ_CTRL_DST_STM_SEL_FIELD |\r\nTEGRA_AES_CMDQ_CTRL_ICMDQEN_FIELD;\r\naes_writel(dd, value, TEGRA_AES_CMDQUE_CONTROL);\r\ndev_dbg(dd->dev, "cmd_q_ctrl=0x%x", value);\r\nvalue = (0x1 << TEGRA_AES_SECURE_INPUT_ALG_SEL_SHIFT) |\r\n((dd->ctx->keylen * 8) <<\r\nTEGRA_AES_SECURE_INPUT_KEY_LEN_SHIFT) |\r\n((u32)upd_iv << TEGRA_AES_SECURE_IV_SELECT_SHIFT);\r\nif (mode & FLAGS_CBC) {\r\nvalue |= ((((mode & FLAGS_ENCRYPT) ? 2 : 3)\r\n<< TEGRA_AES_SECURE_XOR_POS_SHIFT) |\r\n(((mode & FLAGS_ENCRYPT) ? 2 : 3)\r\n<< TEGRA_AES_SECURE_VCTRAM_SEL_SHIFT) |\r\n((mode & FLAGS_ENCRYPT) ? 1 : 0)\r\n<< TEGRA_AES_SECURE_CORE_SEL_SHIFT);\r\n} else if (mode & FLAGS_OFB) {\r\nvalue |= ((TEGRA_AES_SECURE_XOR_POS_FIELD) |\r\n(2 << TEGRA_AES_SECURE_INPUT_SEL_SHIFT) |\r\n(TEGRA_AES_SECURE_CORE_SEL_FIELD));\r\n} else if (mode & FLAGS_RNG) {\r\nvalue |= (((mode & FLAGS_ENCRYPT) ? 1 : 0)\r\n<< TEGRA_AES_SECURE_CORE_SEL_SHIFT |\r\nTEGRA_AES_SECURE_RNG_ENB_FIELD);\r\n} else {\r\nvalue |= (((mode & FLAGS_ENCRYPT) ? 1 : 0)\r\n<< TEGRA_AES_SECURE_CORE_SEL_SHIFT);\r\n}\r\ndev_dbg(dd->dev, "secure_in_sel=0x%x", value);\r\naes_writel(dd, value, TEGRA_AES_SECURE_INPUT_SELECT);\r\naes_writel(dd, out_addr, TEGRA_AES_SECURE_DEST_ADDR);\r\nINIT_COMPLETION(dd->op_complete);\r\nfor (i = 0; i < AES_HW_MAX_ICQ_LENGTH - 1; i++) {\r\ndo {\r\nvalue = aes_readl(dd, TEGRA_AES_INTR_STATUS);\r\neng_busy = value & TEGRA_AES_ENGINE_BUSY_FIELD;\r\nicq_empty = value & TEGRA_AES_ICQ_EMPTY_FIELD;\r\n} while (eng_busy & (!icq_empty));\r\naes_writel(dd, cmdq[i], TEGRA_AES_ICMDQUE_WR);\r\n}\r\nret = wait_for_completion_timeout(&dd->op_complete,\r\nmsecs_to_jiffies(150));\r\nif (ret == 0) {\r\ndev_err(dd->dev, "timed out (0x%x)\n",\r\naes_readl(dd, TEGRA_AES_INTR_STATUS));\r\nreturn -ETIMEDOUT;\r\n}\r\naes_writel(dd, cmdq[AES_HW_MAX_ICQ_LENGTH - 1], TEGRA_AES_ICMDQUE_WR);\r\nreturn 0;\r\n}\r\nstatic void aes_release_key_slot(struct tegra_aes_slot *slot)\r\n{\r\nif (slot->slot_num == SSK_SLOT_NUM)\r\nreturn;\r\nspin_lock(&list_lock);\r\nlist_add_tail(&slot->node, &dev_list);\r\nslot = NULL;\r\nspin_unlock(&list_lock);\r\n}\r\nstatic struct tegra_aes_slot *aes_find_key_slot(void)\r\n{\r\nstruct tegra_aes_slot *slot = NULL;\r\nstruct list_head *new_head;\r\nint empty;\r\nspin_lock(&list_lock);\r\nempty = list_empty(&dev_list);\r\nif (!empty) {\r\nslot = list_entry(&dev_list, struct tegra_aes_slot, node);\r\nnew_head = dev_list.next;\r\nlist_del(&dev_list);\r\ndev_list.next = new_head->next;\r\ndev_list.prev = NULL;\r\n}\r\nspin_unlock(&list_lock);\r\nreturn slot;\r\n}\r\nstatic int aes_set_key(struct tegra_aes_dev *dd)\r\n{\r\nu32 value, cmdq[2];\r\nstruct tegra_aes_ctx *ctx = dd->ctx;\r\nint eng_busy, icq_empty, dma_busy;\r\nbool use_ssk = false;\r\nif (!dd->ctx->slot) {\r\ndev_dbg(dd->dev, "using ssk");\r\ndd->ctx->slot = &ssk;\r\nuse_ssk = true;\r\n}\r\nvalue = aes_readl(dd, TEGRA_AES_SECURE_CONFIG_EXT);\r\nvalue &= ~TEGRA_AES_SECURE_KEY_SCH_DIS_FIELD;\r\naes_writel(dd, value, TEGRA_AES_SECURE_CONFIG_EXT);\r\nvalue = aes_readl(dd, TEGRA_AES_SECURE_CONFIG);\r\nvalue &= ~TEGRA_AES_SECURE_KEY_INDEX_FIELD;\r\nvalue |= (ctx->slot->slot_num << TEGRA_AES_SECURE_KEY_INDEX_SHIFT);\r\naes_writel(dd, value, TEGRA_AES_SECURE_CONFIG);\r\nif (use_ssk)\r\nreturn 0;\r\ncmdq[0] = CMD_MEMDMAVD << CMDQ_OPCODE_SHIFT |\r\nMEMDMA_DIR_DTOVRAM << MEMDMA_DIR_SHIFT |\r\nAES_HW_KEY_TABLE_LENGTH_BYTES / sizeof(u32) <<\r\nMEMDMA_NUM_WORDS_SHIFT;\r\ncmdq[1] = (u32)dd->ivkey_phys_base;\r\naes_writel(dd, cmdq[0], TEGRA_AES_ICMDQUE_WR);\r\naes_writel(dd, cmdq[1], TEGRA_AES_ICMDQUE_WR);\r\ndo {\r\nvalue = aes_readl(dd, TEGRA_AES_INTR_STATUS);\r\neng_busy = value & TEGRA_AES_ENGINE_BUSY_FIELD;\r\nicq_empty = value & TEGRA_AES_ICQ_EMPTY_FIELD;\r\ndma_busy = value & TEGRA_AES_DMA_BUSY_FIELD;\r\n} while (eng_busy & (!icq_empty) & dma_busy);\r\nvalue = CMD_SETTABLE << CMDQ_OPCODE_SHIFT |\r\nSUBCMD_CRYPTO_TABLE_SEL << CMDQ_TABLESEL_SHIFT |\r\nSUBCMD_VRAM_SEL << CMDQ_VRAMSEL_SHIFT |\r\n(SUBCMD_KEY_TABLE_SEL | ctx->slot->slot_num) <<\r\nCMDQ_KEYTABLEID_SHIFT;\r\naes_writel(dd, value, TEGRA_AES_ICMDQUE_WR);\r\ndo {\r\nvalue = aes_readl(dd, TEGRA_AES_INTR_STATUS);\r\neng_busy = value & TEGRA_AES_ENGINE_BUSY_FIELD;\r\nicq_empty = value & TEGRA_AES_ICQ_EMPTY_FIELD;\r\n} while (eng_busy & (!icq_empty));\r\nreturn 0;\r\n}\r\nstatic int tegra_aes_handle_req(struct tegra_aes_dev *dd)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct crypto_ablkcipher *tfm;\r\nstruct tegra_aes_ctx *ctx;\r\nstruct tegra_aes_reqctx *rctx;\r\nstruct ablkcipher_request *req;\r\nunsigned long flags;\r\nint dma_max = AES_HW_DMA_BUFFER_SIZE_BYTES;\r\nint ret = 0, nblocks, total;\r\nint count = 0;\r\ndma_addr_t addr_in, addr_out;\r\nstruct scatterlist *in_sg, *out_sg;\r\nif (!dd)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&dd->lock, flags);\r\nbacklog = crypto_get_backlog(&dd->queue);\r\nasync_req = crypto_dequeue_request(&dd->queue);\r\nif (!async_req)\r\nclear_bit(FLAGS_BUSY, &dd->flags);\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nif (!async_req)\r\nreturn -ENODATA;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ablkcipher_request_cast(async_req);\r\ndev_dbg(dd->dev, "%s: get new req\n", __func__);\r\nif (!req->src || !req->dst)\r\nreturn -EINVAL;\r\nmutex_lock(&aes_lock);\r\ndd->req = req;\r\ndd->total = req->nbytes;\r\ndd->in_offset = 0;\r\ndd->in_sg = req->src;\r\ndd->out_offset = 0;\r\ndd->out_sg = req->dst;\r\nin_sg = dd->in_sg;\r\nout_sg = dd->out_sg;\r\ntotal = dd->total;\r\ntfm = crypto_ablkcipher_reqtfm(req);\r\nrctx = ablkcipher_request_ctx(req);\r\nctx = crypto_ablkcipher_ctx(tfm);\r\nrctx->mode &= FLAGS_MODE_MASK;\r\ndd->flags = (dd->flags & ~FLAGS_MODE_MASK) | rctx->mode;\r\ndd->iv = (u8 *)req->info;\r\ndd->ivlen = crypto_ablkcipher_ivsize(tfm);\r\nctx->dd = dd;\r\ndd->ctx = ctx;\r\nif (ctx->flags & FLAGS_NEW_KEY) {\r\nmemcpy(dd->ivkey_base, ctx->key, ctx->keylen);\r\nmemset(dd->ivkey_base + ctx->keylen, 0, AES_HW_KEY_TABLE_LENGTH_BYTES - ctx->keylen);\r\naes_set_key(dd);\r\nctx->flags &= ~FLAGS_NEW_KEY;\r\n}\r\nif (((dd->flags & FLAGS_CBC) || (dd->flags & FLAGS_OFB)) && dd->iv) {\r\nmemcpy(dd->buf_in, dd->iv, dd->ivlen);\r\nret = aes_start_crypt(dd, (u32)dd->dma_buf_in,\r\ndd->dma_buf_out, 1, FLAGS_CBC, false);\r\nif (ret < 0) {\r\ndev_err(dd->dev, "aes_start_crypt fail(%d)\n", ret);\r\ngoto out;\r\n}\r\n}\r\nwhile (total) {\r\ndev_dbg(dd->dev, "remain: %d\n", total);\r\nret = dma_map_sg(dd->dev, in_sg, 1, DMA_TO_DEVICE);\r\nif (!ret) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\ngoto out;\r\n}\r\nret = dma_map_sg(dd->dev, out_sg, 1, DMA_FROM_DEVICE);\r\nif (!ret) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\ndma_unmap_sg(dd->dev, dd->in_sg,\r\n1, DMA_TO_DEVICE);\r\ngoto out;\r\n}\r\naddr_in = sg_dma_address(in_sg);\r\naddr_out = sg_dma_address(out_sg);\r\ndd->flags |= FLAGS_FAST;\r\ncount = min_t(int, sg_dma_len(in_sg), dma_max);\r\nWARN_ON(sg_dma_len(in_sg) != sg_dma_len(out_sg));\r\nnblocks = DIV_ROUND_UP(count, AES_BLOCK_SIZE);\r\nret = aes_start_crypt(dd, addr_in, addr_out, nblocks,\r\ndd->flags, true);\r\ndma_unmap_sg(dd->dev, out_sg, 1, DMA_FROM_DEVICE);\r\ndma_unmap_sg(dd->dev, in_sg, 1, DMA_TO_DEVICE);\r\nif (ret < 0) {\r\ndev_err(dd->dev, "aes_start_crypt fail(%d)\n", ret);\r\ngoto out;\r\n}\r\ndd->flags &= ~FLAGS_FAST;\r\ndev_dbg(dd->dev, "out: copied %d\n", count);\r\ntotal -= count;\r\nin_sg = sg_next(in_sg);\r\nout_sg = sg_next(out_sg);\r\nWARN_ON(((total != 0) && (!in_sg || !out_sg)));\r\n}\r\nout:\r\nmutex_unlock(&aes_lock);\r\ndd->total = total;\r\nif (dd->req->base.complete)\r\ndd->req->base.complete(&dd->req->base, ret);\r\ndev_dbg(dd->dev, "%s: exit\n", __func__);\r\nreturn ret;\r\n}\r\nstatic int tegra_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct tegra_aes_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct tegra_aes_dev *dd = aes_dev;\r\nstruct tegra_aes_slot *key_slot;\r\nif ((keylen != AES_KEYSIZE_128) && (keylen != AES_KEYSIZE_192) &&\r\n(keylen != AES_KEYSIZE_256)) {\r\ndev_err(dd->dev, "unsupported key size\n");\r\ncrypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\ndev_dbg(dd->dev, "keylen: %d\n", keylen);\r\nctx->dd = dd;\r\nif (key) {\r\nif (!ctx->slot) {\r\nkey_slot = aes_find_key_slot();\r\nif (!key_slot) {\r\ndev_err(dd->dev, "no empty slot\n");\r\nreturn -ENOMEM;\r\n}\r\nctx->slot = key_slot;\r\n}\r\nmemcpy(ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\n}\r\nctx->flags |= FLAGS_NEW_KEY;\r\ndev_dbg(dd->dev, "done\n");\r\nreturn 0;\r\n}\r\nstatic void aes_workqueue_handler(struct work_struct *work)\r\n{\r\nstruct tegra_aes_dev *dd = aes_dev;\r\nint ret;\r\nret = clk_prepare_enable(dd->aes_clk);\r\nif (ret)\r\nBUG_ON("clock enable failed");\r\ndo {\r\nret = tegra_aes_handle_req(dd);\r\n} while (!ret);\r\nclk_disable_unprepare(dd->aes_clk);\r\n}\r\nstatic irqreturn_t aes_irq(int irq, void *dev_id)\r\n{\r\nstruct tegra_aes_dev *dd = (struct tegra_aes_dev *)dev_id;\r\nu32 value = aes_readl(dd, TEGRA_AES_INTR_STATUS);\r\nint busy = test_bit(FLAGS_BUSY, &dd->flags);\r\nif (!busy) {\r\ndev_dbg(dd->dev, "spurious interrupt\n");\r\nreturn IRQ_NONE;\r\n}\r\ndev_dbg(dd->dev, "irq_stat: 0x%x\n", value);\r\nif (value & TEGRA_AES_INT_ERROR_MASK)\r\naes_writel(dd, TEGRA_AES_INT_ERROR_MASK, TEGRA_AES_INTR_STATUS);\r\nif (!(value & TEGRA_AES_ENGINE_BUSY_FIELD))\r\ncomplete(&dd->op_complete);\r\nelse\r\nreturn IRQ_NONE;\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int tegra_aes_crypt(struct ablkcipher_request *req, unsigned long mode)\r\n{\r\nstruct tegra_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct tegra_aes_dev *dd = aes_dev;\r\nunsigned long flags;\r\nint err = 0;\r\nint busy;\r\ndev_dbg(dd->dev, "nbytes: %d, enc: %d, cbc: %d, ofb: %d\n",\r\nreq->nbytes, !!(mode & FLAGS_ENCRYPT),\r\n!!(mode & FLAGS_CBC), !!(mode & FLAGS_OFB));\r\nrctx->mode = mode;\r\nspin_lock_irqsave(&dd->lock, flags);\r\nerr = ablkcipher_enqueue_request(&dd->queue, req);\r\nbusy = test_and_set_bit(FLAGS_BUSY, &dd->flags);\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nif (!busy)\r\nqueue_work(aes_wq, &aes_work);\r\nreturn err;\r\n}\r\nstatic int tegra_aes_ecb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn tegra_aes_crypt(req, FLAGS_ENCRYPT);\r\n}\r\nstatic int tegra_aes_ecb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn tegra_aes_crypt(req, 0);\r\n}\r\nstatic int tegra_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn tegra_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_CBC);\r\n}\r\nstatic int tegra_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn tegra_aes_crypt(req, FLAGS_CBC);\r\n}\r\nstatic int tegra_aes_ofb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn tegra_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_OFB);\r\n}\r\nstatic int tegra_aes_ofb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn tegra_aes_crypt(req, FLAGS_OFB);\r\n}\r\nstatic int tegra_aes_get_random(struct crypto_rng *tfm, u8 *rdata,\r\nunsigned int dlen)\r\n{\r\nstruct tegra_aes_dev *dd = aes_dev;\r\nstruct tegra_aes_ctx *ctx = &rng_ctx;\r\nint ret, i;\r\nu8 *dest = rdata, *dt = dd->dt;\r\nmutex_lock(&aes_lock);\r\nret = clk_prepare_enable(dd->aes_clk);\r\nif (ret) {\r\nmutex_unlock(&aes_lock);\r\nreturn ret;\r\n}\r\nctx->dd = dd;\r\ndd->ctx = ctx;\r\ndd->flags = FLAGS_ENCRYPT | FLAGS_RNG;\r\nmemcpy(dd->buf_in, dt, DEFAULT_RNG_BLK_SZ);\r\nret = aes_start_crypt(dd, (u32)dd->dma_buf_in,\r\n(u32)dd->dma_buf_out, 1, dd->flags, true);\r\nif (ret < 0) {\r\ndev_err(dd->dev, "aes_start_crypt fail(%d)\n", ret);\r\ndlen = ret;\r\ngoto out;\r\n}\r\nmemcpy(dest, dd->buf_out, dlen);\r\nfor (i = DEFAULT_RNG_BLK_SZ - 1; i >= 0; i--) {\r\ndt[i] += 1;\r\nif (dt[i] != 0)\r\nbreak;\r\n}\r\nout:\r\nclk_disable_unprepare(dd->aes_clk);\r\nmutex_unlock(&aes_lock);\r\ndev_dbg(dd->dev, "%s: done\n", __func__);\r\nreturn dlen;\r\n}\r\nstatic int tegra_aes_rng_reset(struct crypto_rng *tfm, u8 *seed,\r\nunsigned int slen)\r\n{\r\nstruct tegra_aes_dev *dd = aes_dev;\r\nstruct tegra_aes_ctx *ctx = &rng_ctx;\r\nstruct tegra_aes_slot *key_slot;\r\nstruct timespec ts;\r\nint ret = 0;\r\nu64 nsec, tmp[2];\r\nu8 *dt;\r\nif (!ctx || !dd) {\r\ndev_err(dd->dev, "ctx=0x%x, dd=0x%x\n",\r\n(unsigned int)ctx, (unsigned int)dd);\r\nreturn -EINVAL;\r\n}\r\nif (slen < (DEFAULT_RNG_BLK_SZ + AES_KEYSIZE_128)) {\r\ndev_err(dd->dev, "seed size invalid");\r\nreturn -ENOMEM;\r\n}\r\nmutex_lock(&aes_lock);\r\nif (!ctx->slot) {\r\nkey_slot = aes_find_key_slot();\r\nif (!key_slot) {\r\ndev_err(dd->dev, "no empty slot\n");\r\nmutex_unlock(&aes_lock);\r\nreturn -ENOMEM;\r\n}\r\nctx->slot = key_slot;\r\n}\r\nctx->dd = dd;\r\ndd->ctx = ctx;\r\ndd->ctr = 0;\r\nctx->keylen = AES_KEYSIZE_128;\r\nctx->flags |= FLAGS_NEW_KEY;\r\nmemcpy(dd->ivkey_base, seed + DEFAULT_RNG_BLK_SZ, AES_KEYSIZE_128);\r\nmemset(dd->ivkey_base + AES_KEYSIZE_128, 0, AES_HW_KEY_TABLE_LENGTH_BYTES - AES_KEYSIZE_128);\r\ndd->iv = seed;\r\ndd->ivlen = slen;\r\ndd->flags = FLAGS_ENCRYPT | FLAGS_RNG;\r\nret = clk_prepare_enable(dd->aes_clk);\r\nif (ret) {\r\nmutex_unlock(&aes_lock);\r\nreturn ret;\r\n}\r\naes_set_key(dd);\r\nmemcpy(dd->buf_in, dd->iv, DEFAULT_RNG_BLK_SZ);\r\nret = aes_start_crypt(dd, (u32)dd->dma_buf_in,\r\ndd->dma_buf_out, 1, FLAGS_CBC, false);\r\nif (ret < 0) {\r\ndev_err(dd->dev, "aes_start_crypt fail(%d)\n", ret);\r\ngoto out;\r\n}\r\nif (dd->ivlen >= (2 * DEFAULT_RNG_BLK_SZ + AES_KEYSIZE_128)) {\r\ndt = dd->iv + DEFAULT_RNG_BLK_SZ + AES_KEYSIZE_128;\r\n} else {\r\ngetnstimeofday(&ts);\r\nnsec = timespec_to_ns(&ts);\r\ndo_div(nsec, 1000);\r\nnsec ^= dd->ctr << 56;\r\ndd->ctr++;\r\ntmp[0] = nsec;\r\ntmp[1] = tegra_chip_uid();\r\ndt = (u8 *)tmp;\r\n}\r\nmemcpy(dd->dt, dt, DEFAULT_RNG_BLK_SZ);\r\nout:\r\nclk_disable_unprepare(dd->aes_clk);\r\nmutex_unlock(&aes_lock);\r\ndev_dbg(dd->dev, "%s: done\n", __func__);\r\nreturn ret;\r\n}\r\nstatic int tegra_aes_cra_init(struct crypto_tfm *tfm)\r\n{\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct tegra_aes_reqctx);\r\nreturn 0;\r\n}\r\nvoid tegra_aes_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct tegra_aes_ctx *ctx =\r\ncrypto_ablkcipher_ctx((struct crypto_ablkcipher *)tfm);\r\nif (ctx && ctx->slot)\r\naes_release_key_slot(ctx->slot);\r\n}\r\nstatic int tegra_aes_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct tegra_aes_dev *dd;\r\nstruct resource *res;\r\nint err = -ENOMEM, i = 0, j;\r\ndd = devm_kzalloc(dev, sizeof(struct tegra_aes_dev), GFP_KERNEL);\r\nif (dd == NULL) {\r\ndev_err(dev, "unable to alloc data struct.\n");\r\nreturn err;\r\n}\r\ndd->dev = dev;\r\nplatform_set_drvdata(pdev, dd);\r\ndd->slots = devm_kzalloc(dev, sizeof(struct tegra_aes_slot) *\r\nAES_NR_KEYSLOTS, GFP_KERNEL);\r\nif (dd->slots == NULL) {\r\ndev_err(dev, "unable to alloc slot struct.\n");\r\ngoto out;\r\n}\r\nspin_lock_init(&dd->lock);\r\ncrypto_init_queue(&dd->queue, TEGRA_AES_QUEUE_LENGTH);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res) {\r\ndev_err(dev, "invalid resource type: base\n");\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nif (!devm_request_mem_region(&pdev->dev, res->start,\r\nresource_size(res),\r\ndev_name(&pdev->dev))) {\r\ndev_err(&pdev->dev, "Couldn't request MEM resource\n");\r\nreturn -ENODEV;\r\n}\r\ndd->io_base = devm_ioremap(dev, res->start, resource_size(res));\r\nif (!dd->io_base) {\r\ndev_err(dev, "can't ioremap register space\n");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\ndd->aes_clk = clk_get(dev, "vde");\r\nif (IS_ERR(dd->aes_clk)) {\r\ndev_err(dev, "iclock intialization failed.\n");\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nerr = clk_set_rate(dd->aes_clk, ULONG_MAX);\r\nif (err) {\r\ndev_err(dd->dev, "iclk set_rate fail(%d)\n", err);\r\ngoto out;\r\n}\r\ndd->ivkey_base = dma_alloc_coherent(dev, AES_HW_KEY_TABLE_LENGTH_BYTES,\r\n&dd->ivkey_phys_base,\r\nGFP_KERNEL);\r\nif (!dd->ivkey_base) {\r\ndev_err(dev, "can not allocate iv/key buffer\n");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\ndd->buf_in = dma_alloc_coherent(dev, AES_HW_DMA_BUFFER_SIZE_BYTES,\r\n&dd->dma_buf_in, GFP_KERNEL);\r\nif (!dd->buf_in) {\r\ndev_err(dev, "can not allocate dma-in buffer\n");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\ndd->buf_out = dma_alloc_coherent(dev, AES_HW_DMA_BUFFER_SIZE_BYTES,\r\n&dd->dma_buf_out, GFP_KERNEL);\r\nif (!dd->buf_out) {\r\ndev_err(dev, "can not allocate dma-out buffer\n");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\ninit_completion(&dd->op_complete);\r\naes_wq = alloc_workqueue("tegra_aes_wq", WQ_HIGHPRI | WQ_UNBOUND, 1);\r\nif (!aes_wq) {\r\ndev_err(dev, "alloc_workqueue failed\n");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nres = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!res) {\r\ndev_err(dev, "invalid resource type: base\n");\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\ndd->irq = res->start;\r\nerr = devm_request_irq(dev, dd->irq, aes_irq, IRQF_TRIGGER_HIGH |\r\nIRQF_SHARED, "tegra-aes", dd);\r\nif (err) {\r\ndev_err(dev, "request_irq failed\n");\r\ngoto out;\r\n}\r\nmutex_init(&aes_lock);\r\nINIT_LIST_HEAD(&dev_list);\r\nspin_lock_init(&list_lock);\r\nspin_lock(&list_lock);\r\nfor (i = 0; i < AES_NR_KEYSLOTS; i++) {\r\nif (i == SSK_SLOT_NUM)\r\ncontinue;\r\ndd->slots[i].slot_num = i;\r\nINIT_LIST_HEAD(&dd->slots[i].node);\r\nlist_add_tail(&dd->slots[i].node, &dev_list);\r\n}\r\nspin_unlock(&list_lock);\r\naes_dev = dd;\r\nfor (i = 0; i < ARRAY_SIZE(algs); i++) {\r\nalgs[i].cra_priority = 300;\r\nalgs[i].cra_ctxsize = sizeof(struct tegra_aes_ctx);\r\nalgs[i].cra_module = THIS_MODULE;\r\nalgs[i].cra_init = tegra_aes_cra_init;\r\nalgs[i].cra_exit = tegra_aes_cra_exit;\r\nerr = crypto_register_alg(&algs[i]);\r\nif (err)\r\ngoto out;\r\n}\r\ndev_info(dev, "registered");\r\nreturn 0;\r\nout:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_alg(&algs[j]);\r\nif (dd->ivkey_base)\r\ndma_free_coherent(dev, AES_HW_KEY_TABLE_LENGTH_BYTES,\r\ndd->ivkey_base, dd->ivkey_phys_base);\r\nif (dd->buf_in)\r\ndma_free_coherent(dev, AES_HW_DMA_BUFFER_SIZE_BYTES,\r\ndd->buf_in, dd->dma_buf_in);\r\nif (dd->buf_out)\r\ndma_free_coherent(dev, AES_HW_DMA_BUFFER_SIZE_BYTES,\r\ndd->buf_out, dd->dma_buf_out);\r\nif (!IS_ERR(dd->aes_clk))\r\nclk_put(dd->aes_clk);\r\nif (aes_wq)\r\ndestroy_workqueue(aes_wq);\r\nspin_lock(&list_lock);\r\nlist_del(&dev_list);\r\nspin_unlock(&list_lock);\r\naes_dev = NULL;\r\ndev_err(dev, "%s: initialization failed.\n", __func__);\r\nreturn err;\r\n}\r\nstatic int tegra_aes_remove(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct tegra_aes_dev *dd = platform_get_drvdata(pdev);\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(algs); i++)\r\ncrypto_unregister_alg(&algs[i]);\r\ncancel_work_sync(&aes_work);\r\ndestroy_workqueue(aes_wq);\r\nspin_lock(&list_lock);\r\nlist_del(&dev_list);\r\nspin_unlock(&list_lock);\r\ndma_free_coherent(dev, AES_HW_KEY_TABLE_LENGTH_BYTES,\r\ndd->ivkey_base, dd->ivkey_phys_base);\r\ndma_free_coherent(dev, AES_HW_DMA_BUFFER_SIZE_BYTES,\r\ndd->buf_in, dd->dma_buf_in);\r\ndma_free_coherent(dev, AES_HW_DMA_BUFFER_SIZE_BYTES,\r\ndd->buf_out, dd->dma_buf_out);\r\nclk_put(dd->aes_clk);\r\naes_dev = NULL;\r\nreturn 0;\r\n}
