static int\r\nxpc_setup_partitions_uv(void)\r\n{\r\nshort partid;\r\nstruct xpc_partition_uv *part_uv;\r\nfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\r\npart_uv = &xpc_partitions[partid].sn.uv;\r\nmutex_init(&part_uv->cached_activate_gru_mq_desc_mutex);\r\nspin_lock_init(&part_uv->flags_lock);\r\npart_uv->remote_act_state = XPC_P_AS_INACTIVE;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nxpc_teardown_partitions_uv(void)\r\n{\r\nshort partid;\r\nstruct xpc_partition_uv *part_uv;\r\nunsigned long irq_flags;\r\nfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\r\npart_uv = &xpc_partitions[partid].sn.uv;\r\nif (part_uv->cached_activate_gru_mq_desc != NULL) {\r\nmutex_lock(&part_uv->cached_activate_gru_mq_desc_mutex);\r\nspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\r\npart_uv->flags &= ~XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV;\r\nspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\r\nkfree(part_uv->cached_activate_gru_mq_desc);\r\npart_uv->cached_activate_gru_mq_desc = NULL;\r\nmutex_unlock(&part_uv->\r\ncached_activate_gru_mq_desc_mutex);\r\n}\r\n}\r\n}\r\nstatic int\r\nxpc_get_gru_mq_irq_uv(struct xpc_gru_mq_uv *mq, int cpu, char *irq_name)\r\n{\r\nint mmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\r\n#if defined CONFIG_X86_64\r\nmq->irq = uv_setup_irq(irq_name, cpu, mq->mmr_blade, mq->mmr_offset,\r\nUV_AFFINITY_CPU);\r\nif (mq->irq < 0)\r\nreturn mq->irq;\r\nmq->mmr_value = uv_read_global_mmr64(mmr_pnode, mq->mmr_offset);\r\n#elif defined CONFIG_IA64_GENERIC || defined CONFIG_IA64_SGI_UV\r\nif (strcmp(irq_name, XPC_ACTIVATE_IRQ_NAME) == 0)\r\nmq->irq = SGI_XPC_ACTIVATE;\r\nelse if (strcmp(irq_name, XPC_NOTIFY_IRQ_NAME) == 0)\r\nmq->irq = SGI_XPC_NOTIFY;\r\nelse\r\nreturn -EINVAL;\r\nmq->mmr_value = (unsigned long)cpu_physical_id(cpu) << 32 | mq->irq;\r\nuv_write_global_mmr64(mmr_pnode, mq->mmr_offset, mq->mmr_value);\r\n#else\r\n#error not a supported configuration\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void\r\nxpc_release_gru_mq_irq_uv(struct xpc_gru_mq_uv *mq)\r\n{\r\n#if defined CONFIG_X86_64\r\nuv_teardown_irq(mq->irq);\r\n#elif defined CONFIG_IA64_GENERIC || defined CONFIG_IA64_SGI_UV\r\nint mmr_pnode;\r\nunsigned long mmr_value;\r\nmmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\r\nmmr_value = 1UL << 16;\r\nuv_write_global_mmr64(mmr_pnode, mq->mmr_offset, mmr_value);\r\n#else\r\n#error not a supported configuration\r\n#endif\r\n}\r\nstatic int\r\nxpc_gru_mq_watchlist_alloc_uv(struct xpc_gru_mq_uv *mq)\r\n{\r\nint ret;\r\n#if defined CONFIG_IA64_GENERIC || defined CONFIG_IA64_SGI_UV\r\nint mmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\r\nret = sn_mq_watchlist_alloc(mmr_pnode, (void *)uv_gpa(mq->address),\r\nmq->order, &mq->mmr_offset);\r\nif (ret < 0) {\r\ndev_err(xpc_part, "sn_mq_watchlist_alloc() failed, ret=%d\n",\r\nret);\r\nreturn -EBUSY;\r\n}\r\n#elif defined CONFIG_X86_64\r\nret = uv_bios_mq_watchlist_alloc(uv_gpa(mq->address),\r\nmq->order, &mq->mmr_offset);\r\nif (ret < 0) {\r\ndev_err(xpc_part, "uv_bios_mq_watchlist_alloc() failed, "\r\n"ret=%d\n", ret);\r\nreturn ret;\r\n}\r\n#else\r\n#error not a supported configuration\r\n#endif\r\nmq->watchlist_num = ret;\r\nreturn 0;\r\n}\r\nstatic void\r\nxpc_gru_mq_watchlist_free_uv(struct xpc_gru_mq_uv *mq)\r\n{\r\nint ret;\r\nint mmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\r\n#if defined CONFIG_X86_64\r\nret = uv_bios_mq_watchlist_free(mmr_pnode, mq->watchlist_num);\r\nBUG_ON(ret != BIOS_STATUS_SUCCESS);\r\n#elif defined CONFIG_IA64_GENERIC || defined CONFIG_IA64_SGI_UV\r\nret = sn_mq_watchlist_free(mmr_pnode, mq->watchlist_num);\r\nBUG_ON(ret != SALRET_OK);\r\n#else\r\n#error not a supported configuration\r\n#endif\r\n}\r\nstatic struct xpc_gru_mq_uv *\r\nxpc_create_gru_mq_uv(unsigned int mq_size, int cpu, char *irq_name,\r\nirq_handler_t irq_handler)\r\n{\r\nenum xp_retval xp_ret;\r\nint ret;\r\nint nid;\r\nint nasid;\r\nint pg_order;\r\nstruct page *page;\r\nstruct xpc_gru_mq_uv *mq;\r\nstruct uv_IO_APIC_route_entry *mmr_value;\r\nmq = kmalloc(sizeof(struct xpc_gru_mq_uv), GFP_KERNEL);\r\nif (mq == NULL) {\r\ndev_err(xpc_part, "xpc_create_gru_mq_uv() failed to kmalloc() "\r\n"a xpc_gru_mq_uv structure\n");\r\nret = -ENOMEM;\r\ngoto out_0;\r\n}\r\nmq->gru_mq_desc = kzalloc(sizeof(struct gru_message_queue_desc),\r\nGFP_KERNEL);\r\nif (mq->gru_mq_desc == NULL) {\r\ndev_err(xpc_part, "xpc_create_gru_mq_uv() failed to kmalloc() "\r\n"a gru_message_queue_desc structure\n");\r\nret = -ENOMEM;\r\ngoto out_1;\r\n}\r\npg_order = get_order(mq_size);\r\nmq->order = pg_order + PAGE_SHIFT;\r\nmq_size = 1UL << mq->order;\r\nmq->mmr_blade = uv_cpu_to_blade_id(cpu);\r\nnid = cpu_to_node(cpu);\r\npage = alloc_pages_exact_node(nid,\r\nGFP_KERNEL | __GFP_ZERO | GFP_THISNODE,\r\npg_order);\r\nif (page == NULL) {\r\ndev_err(xpc_part, "xpc_create_gru_mq_uv() failed to alloc %d "\r\n"bytes of memory on nid=%d for GRU mq\n", mq_size, nid);\r\nret = -ENOMEM;\r\ngoto out_2;\r\n}\r\nmq->address = page_address(page);\r\nret = xpc_gru_mq_watchlist_alloc_uv(mq);\r\nif (ret != 0)\r\ngoto out_3;\r\nret = xpc_get_gru_mq_irq_uv(mq, cpu, irq_name);\r\nif (ret != 0)\r\ngoto out_4;\r\nret = request_irq(mq->irq, irq_handler, 0, irq_name, NULL);\r\nif (ret != 0) {\r\ndev_err(xpc_part, "request_irq(irq=%d) returned error=%d\n",\r\nmq->irq, -ret);\r\ngoto out_5;\r\n}\r\nnasid = UV_PNODE_TO_NASID(uv_cpu_to_pnode(cpu));\r\nmmr_value = (struct uv_IO_APIC_route_entry *)&mq->mmr_value;\r\nret = gru_create_message_queue(mq->gru_mq_desc, mq->address, mq_size,\r\nnasid, mmr_value->vector, mmr_value->dest);\r\nif (ret != 0) {\r\ndev_err(xpc_part, "gru_create_message_queue() returned "\r\n"error=%d\n", ret);\r\nret = -EINVAL;\r\ngoto out_6;\r\n}\r\nxp_ret = xp_expand_memprotect(xp_pa(mq->address), mq_size);\r\nif (xp_ret != xpSuccess) {\r\nret = -EACCES;\r\ngoto out_6;\r\n}\r\nreturn mq;\r\nout_6:\r\nfree_irq(mq->irq, NULL);\r\nout_5:\r\nxpc_release_gru_mq_irq_uv(mq);\r\nout_4:\r\nxpc_gru_mq_watchlist_free_uv(mq);\r\nout_3:\r\nfree_pages((unsigned long)mq->address, pg_order);\r\nout_2:\r\nkfree(mq->gru_mq_desc);\r\nout_1:\r\nkfree(mq);\r\nout_0:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void\r\nxpc_destroy_gru_mq_uv(struct xpc_gru_mq_uv *mq)\r\n{\r\nunsigned int mq_size;\r\nint pg_order;\r\nint ret;\r\nmq_size = 1UL << mq->order;\r\nret = xp_restrict_memprotect(xp_pa(mq->address), mq_size);\r\nBUG_ON(ret != xpSuccess);\r\nfree_irq(mq->irq, NULL);\r\nxpc_release_gru_mq_irq_uv(mq);\r\nxpc_gru_mq_watchlist_free_uv(mq);\r\npg_order = mq->order - PAGE_SHIFT;\r\nfree_pages((unsigned long)mq->address, pg_order);\r\nkfree(mq);\r\n}\r\nstatic enum xp_retval\r\nxpc_send_gru_msg(struct gru_message_queue_desc *gru_mq_desc, void *msg,\r\nsize_t msg_size)\r\n{\r\nenum xp_retval xp_ret;\r\nint ret;\r\nwhile (1) {\r\nret = gru_send_message_gpa(gru_mq_desc, msg, msg_size);\r\nif (ret == MQE_OK) {\r\nxp_ret = xpSuccess;\r\nbreak;\r\n}\r\nif (ret == MQE_QUEUE_FULL) {\r\ndev_dbg(xpc_chan, "gru_send_message_gpa() returned "\r\n"error=MQE_QUEUE_FULL\n");\r\n(void)msleep_interruptible(10);\r\n} else if (ret == MQE_CONGESTION) {\r\ndev_dbg(xpc_chan, "gru_send_message_gpa() returned "\r\n"error=MQE_CONGESTION\n");\r\n} else {\r\ndev_err(xpc_chan, "gru_send_message_gpa() returned "\r\n"error=%d\n", ret);\r\nxp_ret = xpGruSendMqError;\r\nbreak;\r\n}\r\n}\r\nreturn xp_ret;\r\n}\r\nstatic void\r\nxpc_process_activate_IRQ_rcvd_uv(void)\r\n{\r\nunsigned long irq_flags;\r\nshort partid;\r\nstruct xpc_partition *part;\r\nu8 act_state_req;\r\nDBUG_ON(xpc_activate_IRQ_rcvd == 0);\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\r\npart = &xpc_partitions[partid];\r\nif (part->sn.uv.act_state_req == 0)\r\ncontinue;\r\nxpc_activate_IRQ_rcvd--;\r\nBUG_ON(xpc_activate_IRQ_rcvd < 0);\r\nact_state_req = part->sn.uv.act_state_req;\r\npart->sn.uv.act_state_req = 0;\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (act_state_req == XPC_P_ASR_ACTIVATE_UV) {\r\nif (part->act_state == XPC_P_AS_INACTIVE)\r\nxpc_activate_partition(part);\r\nelse if (part->act_state == XPC_P_AS_DEACTIVATING)\r\nXPC_DEACTIVATE_PARTITION(part, xpReactivating);\r\n} else if (act_state_req == XPC_P_ASR_REACTIVATE_UV) {\r\nif (part->act_state == XPC_P_AS_INACTIVE)\r\nxpc_activate_partition(part);\r\nelse\r\nXPC_DEACTIVATE_PARTITION(part, xpReactivating);\r\n} else if (act_state_req == XPC_P_ASR_DEACTIVATE_UV) {\r\nXPC_DEACTIVATE_PARTITION(part, part->sn.uv.reason);\r\n} else {\r\nBUG();\r\n}\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (xpc_activate_IRQ_rcvd == 0)\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\n}\r\nstatic void\r\nxpc_handle_activate_mq_msg_uv(struct xpc_partition *part,\r\nstruct xpc_activate_mq_msghdr_uv *msg_hdr,\r\nint part_setup,\r\nint *wakeup_hb_checker)\r\n{\r\nunsigned long irq_flags;\r\nstruct xpc_partition_uv *part_uv = &part->sn.uv;\r\nstruct xpc_openclose_args *args;\r\npart_uv->remote_act_state = msg_hdr->act_state;\r\nswitch (msg_hdr->type) {\r\ncase XPC_ACTIVATE_MQ_MSG_SYNC_ACT_STATE_UV:\r\nbreak;\r\ncase XPC_ACTIVATE_MQ_MSG_ACTIVATE_REQ_UV: {\r\nstruct xpc_activate_mq_msg_activate_req_uv *msg;\r\nmsg = container_of(msg_hdr, struct\r\nxpc_activate_mq_msg_activate_req_uv, hdr);\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (part_uv->act_state_req == 0)\r\nxpc_activate_IRQ_rcvd++;\r\npart_uv->act_state_req = XPC_P_ASR_ACTIVATE_UV;\r\npart->remote_rp_pa = msg->rp_gpa;\r\npart->remote_rp_ts_jiffies = msg_hdr->rp_ts_jiffies;\r\npart_uv->heartbeat_gpa = msg->heartbeat_gpa;\r\nif (msg->activate_gru_mq_desc_gpa !=\r\npart_uv->activate_gru_mq_desc_gpa) {\r\nspin_lock(&part_uv->flags_lock);\r\npart_uv->flags &= ~XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV;\r\nspin_unlock(&part_uv->flags_lock);\r\npart_uv->activate_gru_mq_desc_gpa =\r\nmsg->activate_gru_mq_desc_gpa;\r\n}\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\n(*wakeup_hb_checker)++;\r\nbreak;\r\n}\r\ncase XPC_ACTIVATE_MQ_MSG_DEACTIVATE_REQ_UV: {\r\nstruct xpc_activate_mq_msg_deactivate_req_uv *msg;\r\nmsg = container_of(msg_hdr, struct\r\nxpc_activate_mq_msg_deactivate_req_uv, hdr);\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (part_uv->act_state_req == 0)\r\nxpc_activate_IRQ_rcvd++;\r\npart_uv->act_state_req = XPC_P_ASR_DEACTIVATE_UV;\r\npart_uv->reason = msg->reason;\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\n(*wakeup_hb_checker)++;\r\nreturn;\r\n}\r\ncase XPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREQUEST_UV: {\r\nstruct xpc_activate_mq_msg_chctl_closerequest_uv *msg;\r\nif (!part_setup)\r\nbreak;\r\nmsg = container_of(msg_hdr, struct\r\nxpc_activate_mq_msg_chctl_closerequest_uv,\r\nhdr);\r\nargs = &part->remote_openclose_args[msg->ch_number];\r\nargs->reason = msg->reason;\r\nspin_lock_irqsave(&part->chctl_lock, irq_flags);\r\npart->chctl.flags[msg->ch_number] |= XPC_CHCTL_CLOSEREQUEST;\r\nspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\r\nxpc_wakeup_channel_mgr(part);\r\nbreak;\r\n}\r\ncase XPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREPLY_UV: {\r\nstruct xpc_activate_mq_msg_chctl_closereply_uv *msg;\r\nif (!part_setup)\r\nbreak;\r\nmsg = container_of(msg_hdr, struct\r\nxpc_activate_mq_msg_chctl_closereply_uv,\r\nhdr);\r\nspin_lock_irqsave(&part->chctl_lock, irq_flags);\r\npart->chctl.flags[msg->ch_number] |= XPC_CHCTL_CLOSEREPLY;\r\nspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\r\nxpc_wakeup_channel_mgr(part);\r\nbreak;\r\n}\r\ncase XPC_ACTIVATE_MQ_MSG_CHCTL_OPENREQUEST_UV: {\r\nstruct xpc_activate_mq_msg_chctl_openrequest_uv *msg;\r\nif (!part_setup)\r\nbreak;\r\nmsg = container_of(msg_hdr, struct\r\nxpc_activate_mq_msg_chctl_openrequest_uv,\r\nhdr);\r\nargs = &part->remote_openclose_args[msg->ch_number];\r\nargs->entry_size = msg->entry_size;\r\nargs->local_nentries = msg->local_nentries;\r\nspin_lock_irqsave(&part->chctl_lock, irq_flags);\r\npart->chctl.flags[msg->ch_number] |= XPC_CHCTL_OPENREQUEST;\r\nspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\r\nxpc_wakeup_channel_mgr(part);\r\nbreak;\r\n}\r\ncase XPC_ACTIVATE_MQ_MSG_CHCTL_OPENREPLY_UV: {\r\nstruct xpc_activate_mq_msg_chctl_openreply_uv *msg;\r\nif (!part_setup)\r\nbreak;\r\nmsg = container_of(msg_hdr, struct\r\nxpc_activate_mq_msg_chctl_openreply_uv, hdr);\r\nargs = &part->remote_openclose_args[msg->ch_number];\r\nargs->remote_nentries = msg->remote_nentries;\r\nargs->local_nentries = msg->local_nentries;\r\nargs->local_msgqueue_pa = msg->notify_gru_mq_desc_gpa;\r\nspin_lock_irqsave(&part->chctl_lock, irq_flags);\r\npart->chctl.flags[msg->ch_number] |= XPC_CHCTL_OPENREPLY;\r\nspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\r\nxpc_wakeup_channel_mgr(part);\r\nbreak;\r\n}\r\ncase XPC_ACTIVATE_MQ_MSG_CHCTL_OPENCOMPLETE_UV: {\r\nstruct xpc_activate_mq_msg_chctl_opencomplete_uv *msg;\r\nif (!part_setup)\r\nbreak;\r\nmsg = container_of(msg_hdr, struct\r\nxpc_activate_mq_msg_chctl_opencomplete_uv, hdr);\r\nspin_lock_irqsave(&part->chctl_lock, irq_flags);\r\npart->chctl.flags[msg->ch_number] |= XPC_CHCTL_OPENCOMPLETE;\r\nspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\r\nxpc_wakeup_channel_mgr(part);\r\n}\r\ncase XPC_ACTIVATE_MQ_MSG_MARK_ENGAGED_UV:\r\nspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\r\npart_uv->flags |= XPC_P_ENGAGED_UV;\r\nspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\r\nbreak;\r\ncase XPC_ACTIVATE_MQ_MSG_MARK_DISENGAGED_UV:\r\nspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\r\npart_uv->flags &= ~XPC_P_ENGAGED_UV;\r\nspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\r\nbreak;\r\ndefault:\r\ndev_err(xpc_part, "received unknown activate_mq msg type=%d "\r\n"from partition=%d\n", msg_hdr->type, XPC_PARTID(part));\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (part_uv->act_state_req == 0)\r\nxpc_activate_IRQ_rcvd++;\r\npart_uv->act_state_req = XPC_P_ASR_DEACTIVATE_UV;\r\npart_uv->reason = xpBadMsgType;\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\n(*wakeup_hb_checker)++;\r\nreturn;\r\n}\r\nif (msg_hdr->rp_ts_jiffies != part->remote_rp_ts_jiffies &&\r\npart->remote_rp_ts_jiffies != 0) {\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (part_uv->act_state_req == 0)\r\nxpc_activate_IRQ_rcvd++;\r\npart_uv->act_state_req = XPC_P_ASR_REACTIVATE_UV;\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\n(*wakeup_hb_checker)++;\r\n}\r\n}\r\nstatic irqreturn_t\r\nxpc_handle_activate_IRQ_uv(int irq, void *dev_id)\r\n{\r\nstruct xpc_activate_mq_msghdr_uv *msg_hdr;\r\nshort partid;\r\nstruct xpc_partition *part;\r\nint wakeup_hb_checker = 0;\r\nint part_referenced;\r\nwhile (1) {\r\nmsg_hdr = gru_get_next_message(xpc_activate_mq_uv->gru_mq_desc);\r\nif (msg_hdr == NULL)\r\nbreak;\r\npartid = msg_hdr->partid;\r\nif (partid < 0 || partid >= XP_MAX_NPARTITIONS_UV) {\r\ndev_err(xpc_part, "xpc_handle_activate_IRQ_uv() "\r\n"received invalid partid=0x%x in message\n",\r\npartid);\r\n} else {\r\npart = &xpc_partitions[partid];\r\npart_referenced = xpc_part_ref(part);\r\nxpc_handle_activate_mq_msg_uv(part, msg_hdr,\r\npart_referenced,\r\n&wakeup_hb_checker);\r\nif (part_referenced)\r\nxpc_part_deref(part);\r\n}\r\ngru_free_message(xpc_activate_mq_uv->gru_mq_desc, msg_hdr);\r\n}\r\nif (wakeup_hb_checker)\r\nwake_up_interruptible(&xpc_activate_IRQ_wq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic enum xp_retval\r\nxpc_cache_remote_gru_mq_desc_uv(struct gru_message_queue_desc *gru_mq_desc,\r\nunsigned long gru_mq_desc_gpa)\r\n{\r\nenum xp_retval ret;\r\nret = xp_remote_memcpy(uv_gpa(gru_mq_desc), gru_mq_desc_gpa,\r\nsizeof(struct gru_message_queue_desc));\r\nif (ret == xpSuccess)\r\ngru_mq_desc->mq = NULL;\r\nreturn ret;\r\n}\r\nstatic enum xp_retval\r\nxpc_send_activate_IRQ_uv(struct xpc_partition *part, void *msg, size_t msg_size,\r\nint msg_type)\r\n{\r\nstruct xpc_activate_mq_msghdr_uv *msg_hdr = msg;\r\nstruct xpc_partition_uv *part_uv = &part->sn.uv;\r\nstruct gru_message_queue_desc *gru_mq_desc;\r\nunsigned long irq_flags;\r\nenum xp_retval ret;\r\nDBUG_ON(msg_size > XPC_ACTIVATE_MSG_SIZE_UV);\r\nmsg_hdr->type = msg_type;\r\nmsg_hdr->partid = xp_partition_id;\r\nmsg_hdr->act_state = part->act_state;\r\nmsg_hdr->rp_ts_jiffies = xpc_rsvd_page->ts_jiffies;\r\nmutex_lock(&part_uv->cached_activate_gru_mq_desc_mutex);\r\nagain:\r\nif (!(part_uv->flags & XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV)) {\r\ngru_mq_desc = part_uv->cached_activate_gru_mq_desc;\r\nif (gru_mq_desc == NULL) {\r\ngru_mq_desc = kmalloc(sizeof(struct\r\ngru_message_queue_desc),\r\nGFP_KERNEL);\r\nif (gru_mq_desc == NULL) {\r\nret = xpNoMemory;\r\ngoto done;\r\n}\r\npart_uv->cached_activate_gru_mq_desc = gru_mq_desc;\r\n}\r\nret = xpc_cache_remote_gru_mq_desc_uv(gru_mq_desc,\r\npart_uv->\r\nactivate_gru_mq_desc_gpa);\r\nif (ret != xpSuccess)\r\ngoto done;\r\nspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\r\npart_uv->flags |= XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV;\r\nspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\r\n}\r\nret = xpc_send_gru_msg(part_uv->cached_activate_gru_mq_desc, msg,\r\nmsg_size);\r\nif (ret != xpSuccess) {\r\nsmp_rmb();\r\nif (!(part_uv->flags & XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV))\r\ngoto again;\r\n}\r\ndone:\r\nmutex_unlock(&part_uv->cached_activate_gru_mq_desc_mutex);\r\nreturn ret;\r\n}\r\nstatic void\r\nxpc_send_activate_IRQ_part_uv(struct xpc_partition *part, void *msg,\r\nsize_t msg_size, int msg_type)\r\n{\r\nenum xp_retval ret;\r\nret = xpc_send_activate_IRQ_uv(part, msg, msg_size, msg_type);\r\nif (unlikely(ret != xpSuccess))\r\nXPC_DEACTIVATE_PARTITION(part, ret);\r\n}\r\nstatic void\r\nxpc_send_activate_IRQ_ch_uv(struct xpc_channel *ch, unsigned long *irq_flags,\r\nvoid *msg, size_t msg_size, int msg_type)\r\n{\r\nstruct xpc_partition *part = &xpc_partitions[ch->partid];\r\nenum xp_retval ret;\r\nret = xpc_send_activate_IRQ_uv(part, msg, msg_size, msg_type);\r\nif (unlikely(ret != xpSuccess)) {\r\nif (irq_flags != NULL)\r\nspin_unlock_irqrestore(&ch->lock, *irq_flags);\r\nXPC_DEACTIVATE_PARTITION(part, ret);\r\nif (irq_flags != NULL)\r\nspin_lock_irqsave(&ch->lock, *irq_flags);\r\n}\r\n}\r\nstatic void\r\nxpc_send_local_activate_IRQ_uv(struct xpc_partition *part, int act_state_req)\r\n{\r\nunsigned long irq_flags;\r\nstruct xpc_partition_uv *part_uv = &part->sn.uv;\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (part_uv->act_state_req == 0)\r\nxpc_activate_IRQ_rcvd++;\r\npart_uv->act_state_req = act_state_req;\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nwake_up_interruptible(&xpc_activate_IRQ_wq);\r\n}\r\nstatic enum xp_retval\r\nxpc_get_partition_rsvd_page_pa_uv(void *buf, u64 *cookie, unsigned long *rp_pa,\r\nsize_t *len)\r\n{\r\ns64 status;\r\nenum xp_retval ret;\r\n#if defined CONFIG_X86_64\r\nstatus = uv_bios_reserved_page_pa((u64)buf, cookie, (u64 *)rp_pa,\r\n(u64 *)len);\r\nif (status == BIOS_STATUS_SUCCESS)\r\nret = xpSuccess;\r\nelse if (status == BIOS_STATUS_MORE_PASSES)\r\nret = xpNeedMoreInfo;\r\nelse\r\nret = xpBiosError;\r\n#elif defined CONFIG_IA64_GENERIC || defined CONFIG_IA64_SGI_UV\r\nstatus = sn_partition_reserved_page_pa((u64)buf, cookie, rp_pa, len);\r\nif (status == SALRET_OK)\r\nret = xpSuccess;\r\nelse if (status == SALRET_MORE_PASSES)\r\nret = xpNeedMoreInfo;\r\nelse\r\nret = xpSalError;\r\n#else\r\n#error not a supported configuration\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int\r\nxpc_setup_rsvd_page_uv(struct xpc_rsvd_page *rp)\r\n{\r\nxpc_heartbeat_uv =\r\n&xpc_partitions[sn_partition_id].sn.uv.cached_heartbeat;\r\nrp->sn.uv.heartbeat_gpa = uv_gpa(xpc_heartbeat_uv);\r\nrp->sn.uv.activate_gru_mq_desc_gpa =\r\nuv_gpa(xpc_activate_mq_uv->gru_mq_desc);\r\nreturn 0;\r\n}\r\nstatic void\r\nxpc_allow_hb_uv(short partid)\r\n{\r\n}\r\nstatic void\r\nxpc_disallow_hb_uv(short partid)\r\n{\r\n}\r\nstatic void\r\nxpc_disallow_all_hbs_uv(void)\r\n{\r\n}\r\nstatic void\r\nxpc_increment_heartbeat_uv(void)\r\n{\r\nxpc_heartbeat_uv->value++;\r\n}\r\nstatic void\r\nxpc_offline_heartbeat_uv(void)\r\n{\r\nxpc_increment_heartbeat_uv();\r\nxpc_heartbeat_uv->offline = 1;\r\n}\r\nstatic void\r\nxpc_online_heartbeat_uv(void)\r\n{\r\nxpc_increment_heartbeat_uv();\r\nxpc_heartbeat_uv->offline = 0;\r\n}\r\nstatic void\r\nxpc_heartbeat_init_uv(void)\r\n{\r\nxpc_heartbeat_uv->value = 1;\r\nxpc_heartbeat_uv->offline = 0;\r\n}\r\nstatic void\r\nxpc_heartbeat_exit_uv(void)\r\n{\r\nxpc_offline_heartbeat_uv();\r\n}\r\nstatic enum xp_retval\r\nxpc_get_remote_heartbeat_uv(struct xpc_partition *part)\r\n{\r\nstruct xpc_partition_uv *part_uv = &part->sn.uv;\r\nenum xp_retval ret;\r\nret = xp_remote_memcpy(uv_gpa(&part_uv->cached_heartbeat),\r\npart_uv->heartbeat_gpa,\r\nsizeof(struct xpc_heartbeat_uv));\r\nif (ret != xpSuccess)\r\nreturn ret;\r\nif (part_uv->cached_heartbeat.value == part->last_heartbeat &&\r\n!part_uv->cached_heartbeat.offline) {\r\nret = xpNoHeartbeat;\r\n} else {\r\npart->last_heartbeat = part_uv->cached_heartbeat.value;\r\n}\r\nreturn ret;\r\n}\r\nstatic void\r\nxpc_request_partition_activation_uv(struct xpc_rsvd_page *remote_rp,\r\nunsigned long remote_rp_gpa, int nasid)\r\n{\r\nshort partid = remote_rp->SAL_partid;\r\nstruct xpc_partition *part = &xpc_partitions[partid];\r\nstruct xpc_activate_mq_msg_activate_req_uv msg;\r\npart->remote_rp_pa = remote_rp_gpa;\r\npart->remote_rp_ts_jiffies = remote_rp->ts_jiffies;\r\npart->sn.uv.heartbeat_gpa = remote_rp->sn.uv.heartbeat_gpa;\r\npart->sn.uv.activate_gru_mq_desc_gpa =\r\nremote_rp->sn.uv.activate_gru_mq_desc_gpa;\r\nif (part->sn.uv.remote_act_state == XPC_P_AS_INACTIVE) {\r\nmsg.rp_gpa = uv_gpa(xpc_rsvd_page);\r\nmsg.heartbeat_gpa = xpc_rsvd_page->sn.uv.heartbeat_gpa;\r\nmsg.activate_gru_mq_desc_gpa =\r\nxpc_rsvd_page->sn.uv.activate_gru_mq_desc_gpa;\r\nxpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_ACTIVATE_REQ_UV);\r\n}\r\nif (part->act_state == XPC_P_AS_INACTIVE)\r\nxpc_send_local_activate_IRQ_uv(part, XPC_P_ASR_ACTIVATE_UV);\r\n}\r\nstatic void\r\nxpc_request_partition_reactivation_uv(struct xpc_partition *part)\r\n{\r\nxpc_send_local_activate_IRQ_uv(part, XPC_P_ASR_ACTIVATE_UV);\r\n}\r\nstatic void\r\nxpc_request_partition_deactivation_uv(struct xpc_partition *part)\r\n{\r\nstruct xpc_activate_mq_msg_deactivate_req_uv msg;\r\nif (part->sn.uv.remote_act_state != XPC_P_AS_DEACTIVATING &&\r\npart->sn.uv.remote_act_state != XPC_P_AS_INACTIVE) {\r\nmsg.reason = part->reason;\r\nxpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_DEACTIVATE_REQ_UV);\r\n}\r\n}\r\nstatic void\r\nxpc_cancel_partition_deactivation_request_uv(struct xpc_partition *part)\r\n{\r\nreturn;\r\n}\r\nstatic void\r\nxpc_init_fifo_uv(struct xpc_fifo_head_uv *head)\r\n{\r\nhead->first = NULL;\r\nhead->last = NULL;\r\nspin_lock_init(&head->lock);\r\nhead->n_entries = 0;\r\n}\r\nstatic void *\r\nxpc_get_fifo_entry_uv(struct xpc_fifo_head_uv *head)\r\n{\r\nunsigned long irq_flags;\r\nstruct xpc_fifo_entry_uv *first;\r\nspin_lock_irqsave(&head->lock, irq_flags);\r\nfirst = head->first;\r\nif (head->first != NULL) {\r\nhead->first = first->next;\r\nif (head->first == NULL)\r\nhead->last = NULL;\r\nhead->n_entries--;\r\nBUG_ON(head->n_entries < 0);\r\nfirst->next = NULL;\r\n}\r\nspin_unlock_irqrestore(&head->lock, irq_flags);\r\nreturn first;\r\n}\r\nstatic void\r\nxpc_put_fifo_entry_uv(struct xpc_fifo_head_uv *head,\r\nstruct xpc_fifo_entry_uv *last)\r\n{\r\nunsigned long irq_flags;\r\nlast->next = NULL;\r\nspin_lock_irqsave(&head->lock, irq_flags);\r\nif (head->last != NULL)\r\nhead->last->next = last;\r\nelse\r\nhead->first = last;\r\nhead->last = last;\r\nhead->n_entries++;\r\nspin_unlock_irqrestore(&head->lock, irq_flags);\r\n}\r\nstatic int\r\nxpc_n_of_fifo_entries_uv(struct xpc_fifo_head_uv *head)\r\n{\r\nreturn head->n_entries;\r\n}\r\nstatic enum xp_retval\r\nxpc_setup_ch_structures_uv(struct xpc_partition *part)\r\n{\r\nstruct xpc_channel_uv *ch_uv;\r\nint ch_number;\r\nfor (ch_number = 0; ch_number < part->nchannels; ch_number++) {\r\nch_uv = &part->channels[ch_number].sn.uv;\r\nxpc_init_fifo_uv(&ch_uv->msg_slot_free_list);\r\nxpc_init_fifo_uv(&ch_uv->recv_msg_list);\r\n}\r\nreturn xpSuccess;\r\n}\r\nstatic void\r\nxpc_teardown_ch_structures_uv(struct xpc_partition *part)\r\n{\r\nreturn;\r\n}\r\nstatic enum xp_retval\r\nxpc_make_first_contact_uv(struct xpc_partition *part)\r\n{\r\nstruct xpc_activate_mq_msg_uv msg;\r\nxpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_SYNC_ACT_STATE_UV);\r\nwhile (!((part->sn.uv.remote_act_state == XPC_P_AS_ACTIVATING) ||\r\n(part->sn.uv.remote_act_state == XPC_P_AS_ACTIVE))) {\r\ndev_dbg(xpc_part, "waiting to make first contact with "\r\n"partition %d\n", XPC_PARTID(part));\r\n(void)msleep_interruptible(250);\r\nif (part->act_state == XPC_P_AS_DEACTIVATING)\r\nreturn part->reason;\r\n}\r\nreturn xpSuccess;\r\n}\r\nstatic u64\r\nxpc_get_chctl_all_flags_uv(struct xpc_partition *part)\r\n{\r\nunsigned long irq_flags;\r\nunion xpc_channel_ctl_flags chctl;\r\nspin_lock_irqsave(&part->chctl_lock, irq_flags);\r\nchctl = part->chctl;\r\nif (chctl.all_flags != 0)\r\npart->chctl.all_flags = 0;\r\nspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\r\nreturn chctl.all_flags;\r\n}\r\nstatic enum xp_retval\r\nxpc_allocate_send_msg_slot_uv(struct xpc_channel *ch)\r\n{\r\nstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\r\nstruct xpc_send_msg_slot_uv *msg_slot;\r\nunsigned long irq_flags;\r\nint nentries;\r\nint entry;\r\nsize_t nbytes;\r\nfor (nentries = ch->local_nentries; nentries > 0; nentries--) {\r\nnbytes = nentries * sizeof(struct xpc_send_msg_slot_uv);\r\nch_uv->send_msg_slots = kzalloc(nbytes, GFP_KERNEL);\r\nif (ch_uv->send_msg_slots == NULL)\r\ncontinue;\r\nfor (entry = 0; entry < nentries; entry++) {\r\nmsg_slot = &ch_uv->send_msg_slots[entry];\r\nmsg_slot->msg_slot_number = entry;\r\nxpc_put_fifo_entry_uv(&ch_uv->msg_slot_free_list,\r\n&msg_slot->next);\r\n}\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nif (nentries < ch->local_nentries)\r\nch->local_nentries = nentries;\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\nreturn xpSuccess;\r\n}\r\nreturn xpNoMemory;\r\n}\r\nstatic enum xp_retval\r\nxpc_allocate_recv_msg_slot_uv(struct xpc_channel *ch)\r\n{\r\nstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\r\nstruct xpc_notify_mq_msg_uv *msg_slot;\r\nunsigned long irq_flags;\r\nint nentries;\r\nint entry;\r\nsize_t nbytes;\r\nfor (nentries = ch->remote_nentries; nentries > 0; nentries--) {\r\nnbytes = nentries * ch->entry_size;\r\nch_uv->recv_msg_slots = kzalloc(nbytes, GFP_KERNEL);\r\nif (ch_uv->recv_msg_slots == NULL)\r\ncontinue;\r\nfor (entry = 0; entry < nentries; entry++) {\r\nmsg_slot = ch_uv->recv_msg_slots +\r\nentry * ch->entry_size;\r\nmsg_slot->hdr.msg_slot_number = entry;\r\n}\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nif (nentries < ch->remote_nentries)\r\nch->remote_nentries = nentries;\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\nreturn xpSuccess;\r\n}\r\nreturn xpNoMemory;\r\n}\r\nstatic enum xp_retval\r\nxpc_setup_msg_structures_uv(struct xpc_channel *ch)\r\n{\r\nstatic enum xp_retval ret;\r\nstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\r\nDBUG_ON(ch->flags & XPC_C_SETUP);\r\nch_uv->cached_notify_gru_mq_desc = kmalloc(sizeof(struct\r\ngru_message_queue_desc),\r\nGFP_KERNEL);\r\nif (ch_uv->cached_notify_gru_mq_desc == NULL)\r\nreturn xpNoMemory;\r\nret = xpc_allocate_send_msg_slot_uv(ch);\r\nif (ret == xpSuccess) {\r\nret = xpc_allocate_recv_msg_slot_uv(ch);\r\nif (ret != xpSuccess) {\r\nkfree(ch_uv->send_msg_slots);\r\nxpc_init_fifo_uv(&ch_uv->msg_slot_free_list);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void\r\nxpc_teardown_msg_structures_uv(struct xpc_channel *ch)\r\n{\r\nstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\r\nDBUG_ON(!spin_is_locked(&ch->lock));\r\nkfree(ch_uv->cached_notify_gru_mq_desc);\r\nch_uv->cached_notify_gru_mq_desc = NULL;\r\nif (ch->flags & XPC_C_SETUP) {\r\nxpc_init_fifo_uv(&ch_uv->msg_slot_free_list);\r\nkfree(ch_uv->send_msg_slots);\r\nxpc_init_fifo_uv(&ch_uv->recv_msg_list);\r\nkfree(ch_uv->recv_msg_slots);\r\n}\r\n}\r\nstatic void\r\nxpc_send_chctl_closerequest_uv(struct xpc_channel *ch, unsigned long *irq_flags)\r\n{\r\nstruct xpc_activate_mq_msg_chctl_closerequest_uv msg;\r\nmsg.ch_number = ch->number;\r\nmsg.reason = ch->reason;\r\nxpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREQUEST_UV);\r\n}\r\nstatic void\r\nxpc_send_chctl_closereply_uv(struct xpc_channel *ch, unsigned long *irq_flags)\r\n{\r\nstruct xpc_activate_mq_msg_chctl_closereply_uv msg;\r\nmsg.ch_number = ch->number;\r\nxpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREPLY_UV);\r\n}\r\nstatic void\r\nxpc_send_chctl_openrequest_uv(struct xpc_channel *ch, unsigned long *irq_flags)\r\n{\r\nstruct xpc_activate_mq_msg_chctl_openrequest_uv msg;\r\nmsg.ch_number = ch->number;\r\nmsg.entry_size = ch->entry_size;\r\nmsg.local_nentries = ch->local_nentries;\r\nxpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_CHCTL_OPENREQUEST_UV);\r\n}\r\nstatic void\r\nxpc_send_chctl_openreply_uv(struct xpc_channel *ch, unsigned long *irq_flags)\r\n{\r\nstruct xpc_activate_mq_msg_chctl_openreply_uv msg;\r\nmsg.ch_number = ch->number;\r\nmsg.local_nentries = ch->local_nentries;\r\nmsg.remote_nentries = ch->remote_nentries;\r\nmsg.notify_gru_mq_desc_gpa = uv_gpa(xpc_notify_mq_uv->gru_mq_desc);\r\nxpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_CHCTL_OPENREPLY_UV);\r\n}\r\nstatic void\r\nxpc_send_chctl_opencomplete_uv(struct xpc_channel *ch, unsigned long *irq_flags)\r\n{\r\nstruct xpc_activate_mq_msg_chctl_opencomplete_uv msg;\r\nmsg.ch_number = ch->number;\r\nxpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_CHCTL_OPENCOMPLETE_UV);\r\n}\r\nstatic void\r\nxpc_send_chctl_local_msgrequest_uv(struct xpc_partition *part, int ch_number)\r\n{\r\nunsigned long irq_flags;\r\nspin_lock_irqsave(&part->chctl_lock, irq_flags);\r\npart->chctl.flags[ch_number] |= XPC_CHCTL_MSGREQUEST;\r\nspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\r\nxpc_wakeup_channel_mgr(part);\r\n}\r\nstatic enum xp_retval\r\nxpc_save_remote_msgqueue_pa_uv(struct xpc_channel *ch,\r\nunsigned long gru_mq_desc_gpa)\r\n{\r\nstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\r\nDBUG_ON(ch_uv->cached_notify_gru_mq_desc == NULL);\r\nreturn xpc_cache_remote_gru_mq_desc_uv(ch_uv->cached_notify_gru_mq_desc,\r\ngru_mq_desc_gpa);\r\n}\r\nstatic void\r\nxpc_indicate_partition_engaged_uv(struct xpc_partition *part)\r\n{\r\nstruct xpc_activate_mq_msg_uv msg;\r\nxpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_MARK_ENGAGED_UV);\r\n}\r\nstatic void\r\nxpc_indicate_partition_disengaged_uv(struct xpc_partition *part)\r\n{\r\nstruct xpc_activate_mq_msg_uv msg;\r\nxpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\r\nXPC_ACTIVATE_MQ_MSG_MARK_DISENGAGED_UV);\r\n}\r\nstatic void\r\nxpc_assume_partition_disengaged_uv(short partid)\r\n{\r\nstruct xpc_partition_uv *part_uv = &xpc_partitions[partid].sn.uv;\r\nunsigned long irq_flags;\r\nspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\r\npart_uv->flags &= ~XPC_P_ENGAGED_UV;\r\nspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\r\n}\r\nstatic int\r\nxpc_partition_engaged_uv(short partid)\r\n{\r\nreturn (xpc_partitions[partid].sn.uv.flags & XPC_P_ENGAGED_UV) != 0;\r\n}\r\nstatic int\r\nxpc_any_partition_engaged_uv(void)\r\n{\r\nstruct xpc_partition_uv *part_uv;\r\nshort partid;\r\nfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\r\npart_uv = &xpc_partitions[partid].sn.uv;\r\nif ((part_uv->flags & XPC_P_ENGAGED_UV) != 0)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic enum xp_retval\r\nxpc_allocate_msg_slot_uv(struct xpc_channel *ch, u32 flags,\r\nstruct xpc_send_msg_slot_uv **address_of_msg_slot)\r\n{\r\nenum xp_retval ret;\r\nstruct xpc_send_msg_slot_uv *msg_slot;\r\nstruct xpc_fifo_entry_uv *entry;\r\nwhile (1) {\r\nentry = xpc_get_fifo_entry_uv(&ch->sn.uv.msg_slot_free_list);\r\nif (entry != NULL)\r\nbreak;\r\nif (flags & XPC_NOWAIT)\r\nreturn xpNoWait;\r\nret = xpc_allocate_msg_wait(ch);\r\nif (ret != xpInterrupted && ret != xpTimeout)\r\nreturn ret;\r\n}\r\nmsg_slot = container_of(entry, struct xpc_send_msg_slot_uv, next);\r\n*address_of_msg_slot = msg_slot;\r\nreturn xpSuccess;\r\n}\r\nstatic void\r\nxpc_free_msg_slot_uv(struct xpc_channel *ch,\r\nstruct xpc_send_msg_slot_uv *msg_slot)\r\n{\r\nxpc_put_fifo_entry_uv(&ch->sn.uv.msg_slot_free_list, &msg_slot->next);\r\nif (atomic_read(&ch->n_on_msg_allocate_wq) > 0)\r\nwake_up(&ch->msg_allocate_wq);\r\n}\r\nstatic void\r\nxpc_notify_sender_uv(struct xpc_channel *ch,\r\nstruct xpc_send_msg_slot_uv *msg_slot,\r\nenum xp_retval reason)\r\n{\r\nxpc_notify_func func = msg_slot->func;\r\nif (func != NULL && cmpxchg(&msg_slot->func, func, NULL) == func) {\r\natomic_dec(&ch->n_to_notify);\r\ndev_dbg(xpc_chan, "msg_slot->func() called, msg_slot=0x%p "\r\n"msg_slot_number=%d partid=%d channel=%d\n", msg_slot,\r\nmsg_slot->msg_slot_number, ch->partid, ch->number);\r\nfunc(reason, ch->partid, ch->number, msg_slot->key);\r\ndev_dbg(xpc_chan, "msg_slot->func() returned, msg_slot=0x%p "\r\n"msg_slot_number=%d partid=%d channel=%d\n", msg_slot,\r\nmsg_slot->msg_slot_number, ch->partid, ch->number);\r\n}\r\n}\r\nstatic void\r\nxpc_handle_notify_mq_ack_uv(struct xpc_channel *ch,\r\nstruct xpc_notify_mq_msg_uv *msg)\r\n{\r\nstruct xpc_send_msg_slot_uv *msg_slot;\r\nint entry = msg->hdr.msg_slot_number % ch->local_nentries;\r\nmsg_slot = &ch->sn.uv.send_msg_slots[entry];\r\nBUG_ON(msg_slot->msg_slot_number != msg->hdr.msg_slot_number);\r\nmsg_slot->msg_slot_number += ch->local_nentries;\r\nif (msg_slot->func != NULL)\r\nxpc_notify_sender_uv(ch, msg_slot, xpMsgDelivered);\r\nxpc_free_msg_slot_uv(ch, msg_slot);\r\n}\r\nstatic void\r\nxpc_handle_notify_mq_msg_uv(struct xpc_partition *part,\r\nstruct xpc_notify_mq_msg_uv *msg)\r\n{\r\nstruct xpc_partition_uv *part_uv = &part->sn.uv;\r\nstruct xpc_channel *ch;\r\nstruct xpc_channel_uv *ch_uv;\r\nstruct xpc_notify_mq_msg_uv *msg_slot;\r\nunsigned long irq_flags;\r\nint ch_number = msg->hdr.ch_number;\r\nif (unlikely(ch_number >= part->nchannels)) {\r\ndev_err(xpc_part, "xpc_handle_notify_IRQ_uv() received invalid "\r\n"channel number=0x%x in message from partid=%d\n",\r\nch_number, XPC_PARTID(part));\r\nspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nif (part_uv->act_state_req == 0)\r\nxpc_activate_IRQ_rcvd++;\r\npart_uv->act_state_req = XPC_P_ASR_DEACTIVATE_UV;\r\npart_uv->reason = xpBadChannelNumber;\r\nspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\r\nwake_up_interruptible(&xpc_activate_IRQ_wq);\r\nreturn;\r\n}\r\nch = &part->channels[ch_number];\r\nxpc_msgqueue_ref(ch);\r\nif (!(ch->flags & XPC_C_CONNECTED)) {\r\nxpc_msgqueue_deref(ch);\r\nreturn;\r\n}\r\nif (msg->hdr.size == 0) {\r\nxpc_handle_notify_mq_ack_uv(ch, msg);\r\nxpc_msgqueue_deref(ch);\r\nreturn;\r\n}\r\nch_uv = &ch->sn.uv;\r\nmsg_slot = ch_uv->recv_msg_slots +\r\n(msg->hdr.msg_slot_number % ch->remote_nentries) * ch->entry_size;\r\nBUG_ON(msg_slot->hdr.size != 0);\r\nmemcpy(msg_slot, msg, msg->hdr.size);\r\nxpc_put_fifo_entry_uv(&ch_uv->recv_msg_list, &msg_slot->hdr.u.next);\r\nif (ch->flags & XPC_C_CONNECTEDCALLOUT_MADE) {\r\nif (atomic_read(&ch->kthreads_idle) > 0)\r\nwake_up_nr(&ch->idle_wq, 1);\r\nelse\r\nxpc_send_chctl_local_msgrequest_uv(part, ch->number);\r\n}\r\nxpc_msgqueue_deref(ch);\r\n}\r\nstatic irqreturn_t\r\nxpc_handle_notify_IRQ_uv(int irq, void *dev_id)\r\n{\r\nstruct xpc_notify_mq_msg_uv *msg;\r\nshort partid;\r\nstruct xpc_partition *part;\r\nwhile ((msg = gru_get_next_message(xpc_notify_mq_uv->gru_mq_desc)) !=\r\nNULL) {\r\npartid = msg->hdr.partid;\r\nif (partid < 0 || partid >= XP_MAX_NPARTITIONS_UV) {\r\ndev_err(xpc_part, "xpc_handle_notify_IRQ_uv() received "\r\n"invalid partid=0x%x in message\n", partid);\r\n} else {\r\npart = &xpc_partitions[partid];\r\nif (xpc_part_ref(part)) {\r\nxpc_handle_notify_mq_msg_uv(part, msg);\r\nxpc_part_deref(part);\r\n}\r\n}\r\ngru_free_message(xpc_notify_mq_uv->gru_mq_desc, msg);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int\r\nxpc_n_of_deliverable_payloads_uv(struct xpc_channel *ch)\r\n{\r\nreturn xpc_n_of_fifo_entries_uv(&ch->sn.uv.recv_msg_list);\r\n}\r\nstatic void\r\nxpc_process_msg_chctl_flags_uv(struct xpc_partition *part, int ch_number)\r\n{\r\nstruct xpc_channel *ch = &part->channels[ch_number];\r\nint ndeliverable_payloads;\r\nxpc_msgqueue_ref(ch);\r\nndeliverable_payloads = xpc_n_of_deliverable_payloads_uv(ch);\r\nif (ndeliverable_payloads > 0 &&\r\n(ch->flags & XPC_C_CONNECTED) &&\r\n(ch->flags & XPC_C_CONNECTEDCALLOUT_MADE)) {\r\nxpc_activate_kthreads(ch, ndeliverable_payloads);\r\n}\r\nxpc_msgqueue_deref(ch);\r\n}\r\nstatic enum xp_retval\r\nxpc_send_payload_uv(struct xpc_channel *ch, u32 flags, void *payload,\r\nu16 payload_size, u8 notify_type, xpc_notify_func func,\r\nvoid *key)\r\n{\r\nenum xp_retval ret = xpSuccess;\r\nstruct xpc_send_msg_slot_uv *msg_slot = NULL;\r\nstruct xpc_notify_mq_msg_uv *msg;\r\nu8 msg_buffer[XPC_NOTIFY_MSG_SIZE_UV];\r\nsize_t msg_size;\r\nDBUG_ON(notify_type != XPC_N_CALL);\r\nmsg_size = sizeof(struct xpc_notify_mq_msghdr_uv) + payload_size;\r\nif (msg_size > ch->entry_size)\r\nreturn xpPayloadTooBig;\r\nxpc_msgqueue_ref(ch);\r\nif (ch->flags & XPC_C_DISCONNECTING) {\r\nret = ch->reason;\r\ngoto out_1;\r\n}\r\nif (!(ch->flags & XPC_C_CONNECTED)) {\r\nret = xpNotConnected;\r\ngoto out_1;\r\n}\r\nret = xpc_allocate_msg_slot_uv(ch, flags, &msg_slot);\r\nif (ret != xpSuccess)\r\ngoto out_1;\r\nif (func != NULL) {\r\natomic_inc(&ch->n_to_notify);\r\nmsg_slot->key = key;\r\nsmp_wmb();\r\nmsg_slot->func = func;\r\nif (ch->flags & XPC_C_DISCONNECTING) {\r\nret = ch->reason;\r\ngoto out_2;\r\n}\r\n}\r\nmsg = (struct xpc_notify_mq_msg_uv *)&msg_buffer;\r\nmsg->hdr.partid = xp_partition_id;\r\nmsg->hdr.ch_number = ch->number;\r\nmsg->hdr.size = msg_size;\r\nmsg->hdr.msg_slot_number = msg_slot->msg_slot_number;\r\nmemcpy(&msg->payload, payload, payload_size);\r\nret = xpc_send_gru_msg(ch->sn.uv.cached_notify_gru_mq_desc, msg,\r\nmsg_size);\r\nif (ret == xpSuccess)\r\ngoto out_1;\r\nXPC_DEACTIVATE_PARTITION(&xpc_partitions[ch->partid], ret);\r\nout_2:\r\nif (func != NULL) {\r\nif (cmpxchg(&msg_slot->func, func, NULL) != func) {\r\nret = xpSuccess;\r\ngoto out_1;\r\n}\r\nmsg_slot->key = NULL;\r\natomic_dec(&ch->n_to_notify);\r\n}\r\nxpc_free_msg_slot_uv(ch, msg_slot);\r\nout_1:\r\nxpc_msgqueue_deref(ch);\r\nreturn ret;\r\n}\r\nstatic void\r\nxpc_notify_senders_of_disconnect_uv(struct xpc_channel *ch)\r\n{\r\nstruct xpc_send_msg_slot_uv *msg_slot;\r\nint entry;\r\nDBUG_ON(!(ch->flags & XPC_C_DISCONNECTING));\r\nfor (entry = 0; entry < ch->local_nentries; entry++) {\r\nif (atomic_read(&ch->n_to_notify) == 0)\r\nbreak;\r\nmsg_slot = &ch->sn.uv.send_msg_slots[entry];\r\nif (msg_slot->func != NULL)\r\nxpc_notify_sender_uv(ch, msg_slot, ch->reason);\r\n}\r\n}\r\nstatic void *\r\nxpc_get_deliverable_payload_uv(struct xpc_channel *ch)\r\n{\r\nstruct xpc_fifo_entry_uv *entry;\r\nstruct xpc_notify_mq_msg_uv *msg;\r\nvoid *payload = NULL;\r\nif (!(ch->flags & XPC_C_DISCONNECTING)) {\r\nentry = xpc_get_fifo_entry_uv(&ch->sn.uv.recv_msg_list);\r\nif (entry != NULL) {\r\nmsg = container_of(entry, struct xpc_notify_mq_msg_uv,\r\nhdr.u.next);\r\npayload = &msg->payload;\r\n}\r\n}\r\nreturn payload;\r\n}\r\nstatic void\r\nxpc_received_payload_uv(struct xpc_channel *ch, void *payload)\r\n{\r\nstruct xpc_notify_mq_msg_uv *msg;\r\nenum xp_retval ret;\r\nmsg = container_of(payload, struct xpc_notify_mq_msg_uv, payload);\r\nmsg->hdr.partid = xp_partition_id;\r\nmsg->hdr.size = 0;\r\nret = xpc_send_gru_msg(ch->sn.uv.cached_notify_gru_mq_desc, msg,\r\nsizeof(struct xpc_notify_mq_msghdr_uv));\r\nif (ret != xpSuccess)\r\nXPC_DEACTIVATE_PARTITION(&xpc_partitions[ch->partid], ret);\r\n}\r\nstatic int\r\nxpc_init_mq_node(int nid)\r\n{\r\nint cpu;\r\nget_online_cpus();\r\nfor_each_cpu(cpu, cpumask_of_node(nid)) {\r\nxpc_activate_mq_uv =\r\nxpc_create_gru_mq_uv(XPC_ACTIVATE_MQ_SIZE_UV, nid,\r\nXPC_ACTIVATE_IRQ_NAME,\r\nxpc_handle_activate_IRQ_uv);\r\nif (!IS_ERR(xpc_activate_mq_uv))\r\nbreak;\r\n}\r\nif (IS_ERR(xpc_activate_mq_uv)) {\r\nput_online_cpus();\r\nreturn PTR_ERR(xpc_activate_mq_uv);\r\n}\r\nfor_each_cpu(cpu, cpumask_of_node(nid)) {\r\nxpc_notify_mq_uv =\r\nxpc_create_gru_mq_uv(XPC_NOTIFY_MQ_SIZE_UV, nid,\r\nXPC_NOTIFY_IRQ_NAME,\r\nxpc_handle_notify_IRQ_uv);\r\nif (!IS_ERR(xpc_notify_mq_uv))\r\nbreak;\r\n}\r\nif (IS_ERR(xpc_notify_mq_uv)) {\r\nxpc_destroy_gru_mq_uv(xpc_activate_mq_uv);\r\nput_online_cpus();\r\nreturn PTR_ERR(xpc_notify_mq_uv);\r\n}\r\nput_online_cpus();\r\nreturn 0;\r\n}\r\nint\r\nxpc_init_uv(void)\r\n{\r\nint nid;\r\nint ret = 0;\r\nxpc_arch_ops = xpc_arch_ops_uv;\r\nif (sizeof(struct xpc_notify_mq_msghdr_uv) > XPC_MSG_HDR_MAX_SIZE) {\r\ndev_err(xpc_part, "xpc_notify_mq_msghdr_uv is larger than %d\n",\r\nXPC_MSG_HDR_MAX_SIZE);\r\nreturn -E2BIG;\r\n}\r\nif (xpc_mq_node < 0)\r\nfor_each_online_node(nid) {\r\nret = xpc_init_mq_node(nid);\r\nif (!ret)\r\nbreak;\r\n}\r\nelse\r\nret = xpc_init_mq_node(xpc_mq_node);\r\nif (ret < 0)\r\ndev_err(xpc_part, "xpc_init_mq_node() returned error=%d\n",\r\n-ret);\r\nreturn ret;\r\n}\r\nvoid\r\nxpc_exit_uv(void)\r\n{\r\nxpc_destroy_gru_mq_uv(xpc_notify_mq_uv);\r\nxpc_destroy_gru_mq_uv(xpc_activate_mq_uv);\r\n}
