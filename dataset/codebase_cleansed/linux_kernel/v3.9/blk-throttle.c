static inline struct throtl_grp *pd_to_tg(struct blkg_policy_data *pd)\r\n{\r\nreturn pd ? container_of(pd, struct throtl_grp, pd) : NULL;\r\n}\r\nstatic inline struct throtl_grp *blkg_to_tg(struct blkcg_gq *blkg)\r\n{\r\nreturn pd_to_tg(blkg_to_pd(blkg, &blkcg_policy_throtl));\r\n}\r\nstatic inline struct blkcg_gq *tg_to_blkg(struct throtl_grp *tg)\r\n{\r\nreturn pd_to_blkg(&tg->pd);\r\n}\r\nstatic inline struct throtl_grp *td_root_tg(struct throtl_data *td)\r\n{\r\nreturn blkg_to_tg(td->queue->root_blkg);\r\n}\r\nstatic inline unsigned int total_nr_queued(struct throtl_data *td)\r\n{\r\nreturn td->nr_queued[0] + td->nr_queued[1];\r\n}\r\nstatic void tg_stats_alloc_fn(struct work_struct *work)\r\n{\r\nstatic struct tg_stats_cpu *stats_cpu;\r\nstruct delayed_work *dwork = to_delayed_work(work);\r\nbool empty = false;\r\nalloc_stats:\r\nif (!stats_cpu) {\r\nstats_cpu = alloc_percpu(struct tg_stats_cpu);\r\nif (!stats_cpu) {\r\nschedule_delayed_work(dwork, msecs_to_jiffies(10));\r\nreturn;\r\n}\r\n}\r\nspin_lock_irq(&tg_stats_alloc_lock);\r\nif (!list_empty(&tg_stats_alloc_list)) {\r\nstruct throtl_grp *tg = list_first_entry(&tg_stats_alloc_list,\r\nstruct throtl_grp,\r\nstats_alloc_node);\r\nswap(tg->stats_cpu, stats_cpu);\r\nlist_del_init(&tg->stats_alloc_node);\r\n}\r\nempty = list_empty(&tg_stats_alloc_list);\r\nspin_unlock_irq(&tg_stats_alloc_lock);\r\nif (!empty)\r\ngoto alloc_stats;\r\n}\r\nstatic void throtl_pd_init(struct blkcg_gq *blkg)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nunsigned long flags;\r\nRB_CLEAR_NODE(&tg->rb_node);\r\nbio_list_init(&tg->bio_lists[0]);\r\nbio_list_init(&tg->bio_lists[1]);\r\ntg->limits_changed = false;\r\ntg->bps[READ] = -1;\r\ntg->bps[WRITE] = -1;\r\ntg->iops[READ] = -1;\r\ntg->iops[WRITE] = -1;\r\nspin_lock_irqsave(&tg_stats_alloc_lock, flags);\r\nlist_add(&tg->stats_alloc_node, &tg_stats_alloc_list);\r\nschedule_delayed_work(&tg_stats_alloc_work, 0);\r\nspin_unlock_irqrestore(&tg_stats_alloc_lock, flags);\r\n}\r\nstatic void throtl_pd_exit(struct blkcg_gq *blkg)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nunsigned long flags;\r\nspin_lock_irqsave(&tg_stats_alloc_lock, flags);\r\nlist_del_init(&tg->stats_alloc_node);\r\nspin_unlock_irqrestore(&tg_stats_alloc_lock, flags);\r\nfree_percpu(tg->stats_cpu);\r\n}\r\nstatic void throtl_pd_reset_stats(struct blkcg_gq *blkg)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nint cpu;\r\nif (tg->stats_cpu == NULL)\r\nreturn;\r\nfor_each_possible_cpu(cpu) {\r\nstruct tg_stats_cpu *sc = per_cpu_ptr(tg->stats_cpu, cpu);\r\nblkg_rwstat_reset(&sc->service_bytes);\r\nblkg_rwstat_reset(&sc->serviced);\r\n}\r\n}\r\nstatic struct throtl_grp *throtl_lookup_tg(struct throtl_data *td,\r\nstruct blkcg *blkcg)\r\n{\r\nif (blkcg == &blkcg_root)\r\nreturn td_root_tg(td);\r\nreturn blkg_to_tg(blkg_lookup(blkcg, td->queue));\r\n}\r\nstatic struct throtl_grp *throtl_lookup_create_tg(struct throtl_data *td,\r\nstruct blkcg *blkcg)\r\n{\r\nstruct request_queue *q = td->queue;\r\nstruct throtl_grp *tg = NULL;\r\nif (blkcg == &blkcg_root) {\r\ntg = td_root_tg(td);\r\n} else {\r\nstruct blkcg_gq *blkg;\r\nblkg = blkg_lookup_create(blkcg, q);\r\nif (!IS_ERR(blkg))\r\ntg = blkg_to_tg(blkg);\r\nelse if (!blk_queue_dying(q))\r\ntg = td_root_tg(td);\r\n}\r\nreturn tg;\r\n}\r\nstatic struct throtl_grp *throtl_rb_first(struct throtl_rb_root *root)\r\n{\r\nif (!root->count)\r\nreturn NULL;\r\nif (!root->left)\r\nroot->left = rb_first(&root->rb);\r\nif (root->left)\r\nreturn rb_entry_tg(root->left);\r\nreturn NULL;\r\n}\r\nstatic void rb_erase_init(struct rb_node *n, struct rb_root *root)\r\n{\r\nrb_erase(n, root);\r\nRB_CLEAR_NODE(n);\r\n}\r\nstatic void throtl_rb_erase(struct rb_node *n, struct throtl_rb_root *root)\r\n{\r\nif (root->left == n)\r\nroot->left = NULL;\r\nrb_erase_init(n, &root->rb);\r\n--root->count;\r\n}\r\nstatic void update_min_dispatch_time(struct throtl_rb_root *st)\r\n{\r\nstruct throtl_grp *tg;\r\ntg = throtl_rb_first(st);\r\nif (!tg)\r\nreturn;\r\nst->min_disptime = tg->disptime;\r\n}\r\nstatic void\r\ntg_service_tree_add(struct throtl_rb_root *st, struct throtl_grp *tg)\r\n{\r\nstruct rb_node **node = &st->rb.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct throtl_grp *__tg;\r\nunsigned long key = tg->disptime;\r\nint left = 1;\r\nwhile (*node != NULL) {\r\nparent = *node;\r\n__tg = rb_entry_tg(parent);\r\nif (time_before(key, __tg->disptime))\r\nnode = &parent->rb_left;\r\nelse {\r\nnode = &parent->rb_right;\r\nleft = 0;\r\n}\r\n}\r\nif (left)\r\nst->left = &tg->rb_node;\r\nrb_link_node(&tg->rb_node, parent, node);\r\nrb_insert_color(&tg->rb_node, &st->rb);\r\n}\r\nstatic void __throtl_enqueue_tg(struct throtl_data *td, struct throtl_grp *tg)\r\n{\r\nstruct throtl_rb_root *st = &td->tg_service_tree;\r\ntg_service_tree_add(st, tg);\r\nthrotl_mark_tg_on_rr(tg);\r\nst->count++;\r\n}\r\nstatic void throtl_enqueue_tg(struct throtl_data *td, struct throtl_grp *tg)\r\n{\r\nif (!throtl_tg_on_rr(tg))\r\n__throtl_enqueue_tg(td, tg);\r\n}\r\nstatic void __throtl_dequeue_tg(struct throtl_data *td, struct throtl_grp *tg)\r\n{\r\nthrotl_rb_erase(&tg->rb_node, &td->tg_service_tree);\r\nthrotl_clear_tg_on_rr(tg);\r\n}\r\nstatic void throtl_dequeue_tg(struct throtl_data *td, struct throtl_grp *tg)\r\n{\r\nif (throtl_tg_on_rr(tg))\r\n__throtl_dequeue_tg(td, tg);\r\n}\r\nstatic void throtl_schedule_next_dispatch(struct throtl_data *td)\r\n{\r\nstruct throtl_rb_root *st = &td->tg_service_tree;\r\nif (!total_nr_queued(td))\r\nreturn;\r\nBUG_ON(!st->count);\r\nupdate_min_dispatch_time(st);\r\nif (time_before_eq(st->min_disptime, jiffies))\r\nthrotl_schedule_delayed_work(td, 0);\r\nelse\r\nthrotl_schedule_delayed_work(td, (st->min_disptime - jiffies));\r\n}\r\nstatic inline void\r\nthrotl_start_new_slice(struct throtl_data *td, struct throtl_grp *tg, bool rw)\r\n{\r\ntg->bytes_disp[rw] = 0;\r\ntg->io_disp[rw] = 0;\r\ntg->slice_start[rw] = jiffies;\r\ntg->slice_end[rw] = jiffies + throtl_slice;\r\nthrotl_log_tg(td, tg, "[%c] new slice start=%lu end=%lu jiffies=%lu",\r\nrw == READ ? 'R' : 'W', tg->slice_start[rw],\r\ntg->slice_end[rw], jiffies);\r\n}\r\nstatic inline void throtl_set_slice_end(struct throtl_data *td,\r\nstruct throtl_grp *tg, bool rw, unsigned long jiffy_end)\r\n{\r\ntg->slice_end[rw] = roundup(jiffy_end, throtl_slice);\r\n}\r\nstatic inline void throtl_extend_slice(struct throtl_data *td,\r\nstruct throtl_grp *tg, bool rw, unsigned long jiffy_end)\r\n{\r\ntg->slice_end[rw] = roundup(jiffy_end, throtl_slice);\r\nthrotl_log_tg(td, tg, "[%c] extend slice start=%lu end=%lu jiffies=%lu",\r\nrw == READ ? 'R' : 'W', tg->slice_start[rw],\r\ntg->slice_end[rw], jiffies);\r\n}\r\nstatic bool\r\nthrotl_slice_used(struct throtl_data *td, struct throtl_grp *tg, bool rw)\r\n{\r\nif (time_in_range(jiffies, tg->slice_start[rw], tg->slice_end[rw]))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic inline void\r\nthrotl_trim_slice(struct throtl_data *td, struct throtl_grp *tg, bool rw)\r\n{\r\nunsigned long nr_slices, time_elapsed, io_trim;\r\nu64 bytes_trim, tmp;\r\nBUG_ON(time_before(tg->slice_end[rw], tg->slice_start[rw]));\r\nif (throtl_slice_used(td, tg, rw))\r\nreturn;\r\nthrotl_set_slice_end(td, tg, rw, jiffies + throtl_slice);\r\ntime_elapsed = jiffies - tg->slice_start[rw];\r\nnr_slices = time_elapsed / throtl_slice;\r\nif (!nr_slices)\r\nreturn;\r\ntmp = tg->bps[rw] * throtl_slice * nr_slices;\r\ndo_div(tmp, HZ);\r\nbytes_trim = tmp;\r\nio_trim = (tg->iops[rw] * throtl_slice * nr_slices)/HZ;\r\nif (!bytes_trim && !io_trim)\r\nreturn;\r\nif (tg->bytes_disp[rw] >= bytes_trim)\r\ntg->bytes_disp[rw] -= bytes_trim;\r\nelse\r\ntg->bytes_disp[rw] = 0;\r\nif (tg->io_disp[rw] >= io_trim)\r\ntg->io_disp[rw] -= io_trim;\r\nelse\r\ntg->io_disp[rw] = 0;\r\ntg->slice_start[rw] += nr_slices * throtl_slice;\r\nthrotl_log_tg(td, tg, "[%c] trim slice nr=%lu bytes=%llu io=%lu"\r\n" start=%lu end=%lu jiffies=%lu",\r\nrw == READ ? 'R' : 'W', nr_slices, bytes_trim, io_trim,\r\ntg->slice_start[rw], tg->slice_end[rw], jiffies);\r\n}\r\nstatic bool tg_with_in_iops_limit(struct throtl_data *td, struct throtl_grp *tg,\r\nstruct bio *bio, unsigned long *wait)\r\n{\r\nbool rw = bio_data_dir(bio);\r\nunsigned int io_allowed;\r\nunsigned long jiffy_elapsed, jiffy_wait, jiffy_elapsed_rnd;\r\nu64 tmp;\r\njiffy_elapsed = jiffy_elapsed_rnd = jiffies - tg->slice_start[rw];\r\nif (!jiffy_elapsed)\r\njiffy_elapsed_rnd = throtl_slice;\r\njiffy_elapsed_rnd = roundup(jiffy_elapsed_rnd, throtl_slice);\r\ntmp = (u64)tg->iops[rw] * jiffy_elapsed_rnd;\r\ndo_div(tmp, HZ);\r\nif (tmp > UINT_MAX)\r\nio_allowed = UINT_MAX;\r\nelse\r\nio_allowed = tmp;\r\nif (tg->io_disp[rw] + 1 <= io_allowed) {\r\nif (wait)\r\n*wait = 0;\r\nreturn 1;\r\n}\r\njiffy_wait = ((tg->io_disp[rw] + 1) * HZ)/tg->iops[rw] + 1;\r\nif (jiffy_wait > jiffy_elapsed)\r\njiffy_wait = jiffy_wait - jiffy_elapsed;\r\nelse\r\njiffy_wait = 1;\r\nif (wait)\r\n*wait = jiffy_wait;\r\nreturn 0;\r\n}\r\nstatic bool tg_with_in_bps_limit(struct throtl_data *td, struct throtl_grp *tg,\r\nstruct bio *bio, unsigned long *wait)\r\n{\r\nbool rw = bio_data_dir(bio);\r\nu64 bytes_allowed, extra_bytes, tmp;\r\nunsigned long jiffy_elapsed, jiffy_wait, jiffy_elapsed_rnd;\r\njiffy_elapsed = jiffy_elapsed_rnd = jiffies - tg->slice_start[rw];\r\nif (!jiffy_elapsed)\r\njiffy_elapsed_rnd = throtl_slice;\r\njiffy_elapsed_rnd = roundup(jiffy_elapsed_rnd, throtl_slice);\r\ntmp = tg->bps[rw] * jiffy_elapsed_rnd;\r\ndo_div(tmp, HZ);\r\nbytes_allowed = tmp;\r\nif (tg->bytes_disp[rw] + bio->bi_size <= bytes_allowed) {\r\nif (wait)\r\n*wait = 0;\r\nreturn 1;\r\n}\r\nextra_bytes = tg->bytes_disp[rw] + bio->bi_size - bytes_allowed;\r\njiffy_wait = div64_u64(extra_bytes * HZ, tg->bps[rw]);\r\nif (!jiffy_wait)\r\njiffy_wait = 1;\r\njiffy_wait = jiffy_wait + (jiffy_elapsed_rnd - jiffy_elapsed);\r\nif (wait)\r\n*wait = jiffy_wait;\r\nreturn 0;\r\n}\r\nstatic bool tg_no_rule_group(struct throtl_grp *tg, bool rw) {\r\nif (tg->bps[rw] == -1 && tg->iops[rw] == -1)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic bool tg_may_dispatch(struct throtl_data *td, struct throtl_grp *tg,\r\nstruct bio *bio, unsigned long *wait)\r\n{\r\nbool rw = bio_data_dir(bio);\r\nunsigned long bps_wait = 0, iops_wait = 0, max_wait = 0;\r\nBUG_ON(tg->nr_queued[rw] && bio != bio_list_peek(&tg->bio_lists[rw]));\r\nif (tg->bps[rw] == -1 && tg->iops[rw] == -1) {\r\nif (wait)\r\n*wait = 0;\r\nreturn 1;\r\n}\r\nif (throtl_slice_used(td, tg, rw))\r\nthrotl_start_new_slice(td, tg, rw);\r\nelse {\r\nif (time_before(tg->slice_end[rw], jiffies + throtl_slice))\r\nthrotl_extend_slice(td, tg, rw, jiffies + throtl_slice);\r\n}\r\nif (tg_with_in_bps_limit(td, tg, bio, &bps_wait)\r\n&& tg_with_in_iops_limit(td, tg, bio, &iops_wait)) {\r\nif (wait)\r\n*wait = 0;\r\nreturn 1;\r\n}\r\nmax_wait = max(bps_wait, iops_wait);\r\nif (wait)\r\n*wait = max_wait;\r\nif (time_before(tg->slice_end[rw], jiffies + max_wait))\r\nthrotl_extend_slice(td, tg, rw, jiffies + max_wait);\r\nreturn 0;\r\n}\r\nstatic void throtl_update_dispatch_stats(struct blkcg_gq *blkg, u64 bytes,\r\nint rw)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nstruct tg_stats_cpu *stats_cpu;\r\nunsigned long flags;\r\nif (tg->stats_cpu == NULL)\r\nreturn;\r\nlocal_irq_save(flags);\r\nstats_cpu = this_cpu_ptr(tg->stats_cpu);\r\nblkg_rwstat_add(&stats_cpu->serviced, rw, 1);\r\nblkg_rwstat_add(&stats_cpu->service_bytes, rw, bytes);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void throtl_charge_bio(struct throtl_grp *tg, struct bio *bio)\r\n{\r\nbool rw = bio_data_dir(bio);\r\ntg->bytes_disp[rw] += bio->bi_size;\r\ntg->io_disp[rw]++;\r\nthrotl_update_dispatch_stats(tg_to_blkg(tg), bio->bi_size, bio->bi_rw);\r\n}\r\nstatic void throtl_add_bio_tg(struct throtl_data *td, struct throtl_grp *tg,\r\nstruct bio *bio)\r\n{\r\nbool rw = bio_data_dir(bio);\r\nbio_list_add(&tg->bio_lists[rw], bio);\r\nblkg_get(tg_to_blkg(tg));\r\ntg->nr_queued[rw]++;\r\ntd->nr_queued[rw]++;\r\nthrotl_enqueue_tg(td, tg);\r\n}\r\nstatic void tg_update_disptime(struct throtl_data *td, struct throtl_grp *tg)\r\n{\r\nunsigned long read_wait = -1, write_wait = -1, min_wait = -1, disptime;\r\nstruct bio *bio;\r\nif ((bio = bio_list_peek(&tg->bio_lists[READ])))\r\ntg_may_dispatch(td, tg, bio, &read_wait);\r\nif ((bio = bio_list_peek(&tg->bio_lists[WRITE])))\r\ntg_may_dispatch(td, tg, bio, &write_wait);\r\nmin_wait = min(read_wait, write_wait);\r\ndisptime = jiffies + min_wait;\r\nthrotl_dequeue_tg(td, tg);\r\ntg->disptime = disptime;\r\nthrotl_enqueue_tg(td, tg);\r\n}\r\nstatic void tg_dispatch_one_bio(struct throtl_data *td, struct throtl_grp *tg,\r\nbool rw, struct bio_list *bl)\r\n{\r\nstruct bio *bio;\r\nbio = bio_list_pop(&tg->bio_lists[rw]);\r\ntg->nr_queued[rw]--;\r\nblkg_put(tg_to_blkg(tg));\r\nBUG_ON(td->nr_queued[rw] <= 0);\r\ntd->nr_queued[rw]--;\r\nthrotl_charge_bio(tg, bio);\r\nbio_list_add(bl, bio);\r\nbio->bi_rw |= REQ_THROTTLED;\r\nthrotl_trim_slice(td, tg, rw);\r\n}\r\nstatic int throtl_dispatch_tg(struct throtl_data *td, struct throtl_grp *tg,\r\nstruct bio_list *bl)\r\n{\r\nunsigned int nr_reads = 0, nr_writes = 0;\r\nunsigned int max_nr_reads = throtl_grp_quantum*3/4;\r\nunsigned int max_nr_writes = throtl_grp_quantum - max_nr_reads;\r\nstruct bio *bio;\r\nwhile ((bio = bio_list_peek(&tg->bio_lists[READ]))\r\n&& tg_may_dispatch(td, tg, bio, NULL)) {\r\ntg_dispatch_one_bio(td, tg, bio_data_dir(bio), bl);\r\nnr_reads++;\r\nif (nr_reads >= max_nr_reads)\r\nbreak;\r\n}\r\nwhile ((bio = bio_list_peek(&tg->bio_lists[WRITE]))\r\n&& tg_may_dispatch(td, tg, bio, NULL)) {\r\ntg_dispatch_one_bio(td, tg, bio_data_dir(bio), bl);\r\nnr_writes++;\r\nif (nr_writes >= max_nr_writes)\r\nbreak;\r\n}\r\nreturn nr_reads + nr_writes;\r\n}\r\nstatic int throtl_select_dispatch(struct throtl_data *td, struct bio_list *bl)\r\n{\r\nunsigned int nr_disp = 0;\r\nstruct throtl_grp *tg;\r\nstruct throtl_rb_root *st = &td->tg_service_tree;\r\nwhile (1) {\r\ntg = throtl_rb_first(st);\r\nif (!tg)\r\nbreak;\r\nif (time_before(jiffies, tg->disptime))\r\nbreak;\r\nthrotl_dequeue_tg(td, tg);\r\nnr_disp += throtl_dispatch_tg(td, tg, bl);\r\nif (tg->nr_queued[0] || tg->nr_queued[1]) {\r\ntg_update_disptime(td, tg);\r\nthrotl_enqueue_tg(td, tg);\r\n}\r\nif (nr_disp >= throtl_quantum)\r\nbreak;\r\n}\r\nreturn nr_disp;\r\n}\r\nstatic void throtl_process_limit_change(struct throtl_data *td)\r\n{\r\nstruct request_queue *q = td->queue;\r\nstruct blkcg_gq *blkg, *n;\r\nif (!td->limits_changed)\r\nreturn;\r\nxchg(&td->limits_changed, false);\r\nthrotl_log(td, "limits changed");\r\nlist_for_each_entry_safe(blkg, n, &q->blkg_list, q_node) {\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nif (!tg->limits_changed)\r\ncontinue;\r\nif (!xchg(&tg->limits_changed, false))\r\ncontinue;\r\nthrotl_log_tg(td, tg, "limit change rbps=%llu wbps=%llu"\r\n" riops=%u wiops=%u", tg->bps[READ], tg->bps[WRITE],\r\ntg->iops[READ], tg->iops[WRITE]);\r\nthrotl_start_new_slice(td, tg, 0);\r\nthrotl_start_new_slice(td, tg, 1);\r\nif (throtl_tg_on_rr(tg))\r\ntg_update_disptime(td, tg);\r\n}\r\n}\r\nstatic int throtl_dispatch(struct request_queue *q)\r\n{\r\nstruct throtl_data *td = q->td;\r\nunsigned int nr_disp = 0;\r\nstruct bio_list bio_list_on_stack;\r\nstruct bio *bio;\r\nstruct blk_plug plug;\r\nspin_lock_irq(q->queue_lock);\r\nthrotl_process_limit_change(td);\r\nif (!total_nr_queued(td))\r\ngoto out;\r\nbio_list_init(&bio_list_on_stack);\r\nthrotl_log(td, "dispatch nr_queued=%u read=%u write=%u",\r\ntotal_nr_queued(td), td->nr_queued[READ],\r\ntd->nr_queued[WRITE]);\r\nnr_disp = throtl_select_dispatch(td, &bio_list_on_stack);\r\nif (nr_disp)\r\nthrotl_log(td, "bios disp=%u", nr_disp);\r\nthrotl_schedule_next_dispatch(td);\r\nout:\r\nspin_unlock_irq(q->queue_lock);\r\nif (nr_disp) {\r\nblk_start_plug(&plug);\r\nwhile((bio = bio_list_pop(&bio_list_on_stack)))\r\ngeneric_make_request(bio);\r\nblk_finish_plug(&plug);\r\n}\r\nreturn nr_disp;\r\n}\r\nvoid blk_throtl_work(struct work_struct *work)\r\n{\r\nstruct throtl_data *td = container_of(work, struct throtl_data,\r\nthrotl_work.work);\r\nstruct request_queue *q = td->queue;\r\nthrotl_dispatch(q);\r\n}\r\nstatic void\r\nthrotl_schedule_delayed_work(struct throtl_data *td, unsigned long delay)\r\n{\r\nstruct delayed_work *dwork = &td->throtl_work;\r\nif (total_nr_queued(td) || td->limits_changed) {\r\nmod_delayed_work(kthrotld_workqueue, dwork, delay);\r\nthrotl_log(td, "schedule work. delay=%lu jiffies=%lu",\r\ndelay, jiffies);\r\n}\r\n}\r\nstatic u64 tg_prfill_cpu_rwstat(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct throtl_grp *tg = pd_to_tg(pd);\r\nstruct blkg_rwstat rwstat = { }, tmp;\r\nint i, cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct tg_stats_cpu *sc = per_cpu_ptr(tg->stats_cpu, cpu);\r\ntmp = blkg_rwstat_read((void *)sc + off);\r\nfor (i = 0; i < BLKG_RWSTAT_NR; i++)\r\nrwstat.cnt[i] += tmp.cnt[i];\r\n}\r\nreturn __blkg_prfill_rwstat(sf, pd, &rwstat);\r\n}\r\nstatic int tg_print_cpu_rwstat(struct cgroup *cgrp, struct cftype *cft,\r\nstruct seq_file *sf)\r\n{\r\nstruct blkcg *blkcg = cgroup_to_blkcg(cgrp);\r\nblkcg_print_blkgs(sf, blkcg, tg_prfill_cpu_rwstat, &blkcg_policy_throtl,\r\ncft->private, true);\r\nreturn 0;\r\n}\r\nstatic u64 tg_prfill_conf_u64(struct seq_file *sf, struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct throtl_grp *tg = pd_to_tg(pd);\r\nu64 v = *(u64 *)((void *)tg + off);\r\nif (v == -1)\r\nreturn 0;\r\nreturn __blkg_prfill_u64(sf, pd, v);\r\n}\r\nstatic u64 tg_prfill_conf_uint(struct seq_file *sf, struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct throtl_grp *tg = pd_to_tg(pd);\r\nunsigned int v = *(unsigned int *)((void *)tg + off);\r\nif (v == -1)\r\nreturn 0;\r\nreturn __blkg_prfill_u64(sf, pd, v);\r\n}\r\nstatic int tg_print_conf_u64(struct cgroup *cgrp, struct cftype *cft,\r\nstruct seq_file *sf)\r\n{\r\nblkcg_print_blkgs(sf, cgroup_to_blkcg(cgrp), tg_prfill_conf_u64,\r\n&blkcg_policy_throtl, cft->private, false);\r\nreturn 0;\r\n}\r\nstatic int tg_print_conf_uint(struct cgroup *cgrp, struct cftype *cft,\r\nstruct seq_file *sf)\r\n{\r\nblkcg_print_blkgs(sf, cgroup_to_blkcg(cgrp), tg_prfill_conf_uint,\r\n&blkcg_policy_throtl, cft->private, false);\r\nreturn 0;\r\n}\r\nstatic int tg_set_conf(struct cgroup *cgrp, struct cftype *cft, const char *buf,\r\nbool is_u64)\r\n{\r\nstruct blkcg *blkcg = cgroup_to_blkcg(cgrp);\r\nstruct blkg_conf_ctx ctx;\r\nstruct throtl_grp *tg;\r\nstruct throtl_data *td;\r\nint ret;\r\nret = blkg_conf_prep(blkcg, &blkcg_policy_throtl, buf, &ctx);\r\nif (ret)\r\nreturn ret;\r\ntg = blkg_to_tg(ctx.blkg);\r\ntd = ctx.blkg->q->td;\r\nif (!ctx.v)\r\nctx.v = -1;\r\nif (is_u64)\r\n*(u64 *)((void *)tg + cft->private) = ctx.v;\r\nelse\r\n*(unsigned int *)((void *)tg + cft->private) = ctx.v;\r\nxchg(&tg->limits_changed, true);\r\nxchg(&td->limits_changed, true);\r\nthrotl_schedule_delayed_work(td, 0);\r\nblkg_conf_finish(&ctx);\r\nreturn 0;\r\n}\r\nstatic int tg_set_conf_u64(struct cgroup *cgrp, struct cftype *cft,\r\nconst char *buf)\r\n{\r\nreturn tg_set_conf(cgrp, cft, buf, true);\r\n}\r\nstatic int tg_set_conf_uint(struct cgroup *cgrp, struct cftype *cft,\r\nconst char *buf)\r\n{\r\nreturn tg_set_conf(cgrp, cft, buf, false);\r\n}\r\nstatic void throtl_shutdown_wq(struct request_queue *q)\r\n{\r\nstruct throtl_data *td = q->td;\r\ncancel_delayed_work_sync(&td->throtl_work);\r\n}\r\nbool blk_throtl_bio(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct throtl_data *td = q->td;\r\nstruct throtl_grp *tg;\r\nbool rw = bio_data_dir(bio), update_disptime = true;\r\nstruct blkcg *blkcg;\r\nbool throttled = false;\r\nif (bio->bi_rw & REQ_THROTTLED) {\r\nbio->bi_rw &= ~REQ_THROTTLED;\r\ngoto out;\r\n}\r\nrcu_read_lock();\r\nblkcg = bio_blkcg(bio);\r\ntg = throtl_lookup_tg(td, blkcg);\r\nif (tg) {\r\nif (tg_no_rule_group(tg, rw)) {\r\nthrotl_update_dispatch_stats(tg_to_blkg(tg),\r\nbio->bi_size, bio->bi_rw);\r\ngoto out_unlock_rcu;\r\n}\r\n}\r\nspin_lock_irq(q->queue_lock);\r\ntg = throtl_lookup_create_tg(td, blkcg);\r\nif (unlikely(!tg))\r\ngoto out_unlock;\r\nif (tg->nr_queued[rw]) {\r\nupdate_disptime = false;\r\ngoto queue_bio;\r\n}\r\nif (tg_may_dispatch(td, tg, bio, NULL)) {\r\nthrotl_charge_bio(tg, bio);\r\nthrotl_trim_slice(td, tg, rw);\r\ngoto out_unlock;\r\n}\r\nqueue_bio:\r\nthrotl_log_tg(td, tg, "[%c] bio. bdisp=%llu sz=%u bps=%llu"\r\n" iodisp=%u iops=%u queued=%d/%d",\r\nrw == READ ? 'R' : 'W',\r\ntg->bytes_disp[rw], bio->bi_size, tg->bps[rw],\r\ntg->io_disp[rw], tg->iops[rw],\r\ntg->nr_queued[READ], tg->nr_queued[WRITE]);\r\nbio_associate_current(bio);\r\nthrotl_add_bio_tg(q->td, tg, bio);\r\nthrottled = true;\r\nif (update_disptime) {\r\ntg_update_disptime(td, tg);\r\nthrotl_schedule_next_dispatch(td);\r\n}\r\nout_unlock:\r\nspin_unlock_irq(q->queue_lock);\r\nout_unlock_rcu:\r\nrcu_read_unlock();\r\nout:\r\nreturn throttled;\r\n}\r\nvoid blk_throtl_drain(struct request_queue *q)\r\n__releases(q->queue_lock) __acquires(q->queue_lock)\r\n{\r\nstruct throtl_data *td = q->td;\r\nstruct throtl_rb_root *st = &td->tg_service_tree;\r\nstruct throtl_grp *tg;\r\nstruct bio_list bl;\r\nstruct bio *bio;\r\nqueue_lockdep_assert_held(q);\r\nbio_list_init(&bl);\r\nwhile ((tg = throtl_rb_first(st))) {\r\nthrotl_dequeue_tg(td, tg);\r\nwhile ((bio = bio_list_peek(&tg->bio_lists[READ])))\r\ntg_dispatch_one_bio(td, tg, bio_data_dir(bio), &bl);\r\nwhile ((bio = bio_list_peek(&tg->bio_lists[WRITE])))\r\ntg_dispatch_one_bio(td, tg, bio_data_dir(bio), &bl);\r\n}\r\nspin_unlock_irq(q->queue_lock);\r\nwhile ((bio = bio_list_pop(&bl)))\r\ngeneric_make_request(bio);\r\nspin_lock_irq(q->queue_lock);\r\n}\r\nint blk_throtl_init(struct request_queue *q)\r\n{\r\nstruct throtl_data *td;\r\nint ret;\r\ntd = kzalloc_node(sizeof(*td), GFP_KERNEL, q->node);\r\nif (!td)\r\nreturn -ENOMEM;\r\ntd->tg_service_tree = THROTL_RB_ROOT;\r\ntd->limits_changed = false;\r\nINIT_DELAYED_WORK(&td->throtl_work, blk_throtl_work);\r\nq->td = td;\r\ntd->queue = q;\r\nret = blkcg_activate_policy(q, &blkcg_policy_throtl);\r\nif (ret)\r\nkfree(td);\r\nreturn ret;\r\n}\r\nvoid blk_throtl_exit(struct request_queue *q)\r\n{\r\nBUG_ON(!q->td);\r\nthrotl_shutdown_wq(q);\r\nblkcg_deactivate_policy(q, &blkcg_policy_throtl);\r\nkfree(q->td);\r\n}\r\nstatic int __init throtl_init(void)\r\n{\r\nkthrotld_workqueue = alloc_workqueue("kthrotld", WQ_MEM_RECLAIM, 0);\r\nif (!kthrotld_workqueue)\r\npanic("Failed to create kthrotld\n");\r\nreturn blkcg_policy_register(&blkcg_policy_throtl);\r\n}
