static inline enum direction decode_direction(unsigned int insn)\r\n{\r\nunsigned long tmp = (insn >> 21) & 1;\r\nif (!tmp)\r\nreturn load;\r\nelse {\r\nswitch ((insn>>19)&0xf) {\r\ncase 15:\r\nreturn both;\r\ndefault:\r\nreturn store;\r\n}\r\n}\r\n}\r\nstatic inline int decode_access_size(struct pt_regs *regs, unsigned int insn)\r\n{\r\nunsigned int tmp;\r\ntmp = ((insn >> 19) & 0xf);\r\nif (tmp == 11 || tmp == 14)\r\nreturn 8;\r\ntmp &= 3;\r\nif (!tmp)\r\nreturn 4;\r\nelse if (tmp == 3)\r\nreturn 16;\r\nelse if (tmp == 2)\r\nreturn 2;\r\nelse {\r\nprintk("Impossible unaligned trap. insn=%08x\n", insn);\r\ndie_if_kernel("Byte sized unaligned access?!?!", regs);\r\nreturn 0;\r\n}\r\n}\r\nstatic inline int decode_asi(unsigned int insn, struct pt_regs *regs)\r\n{\r\nif (insn & 0x800000) {\r\nif (insn & 0x2000)\r\nreturn (unsigned char)(regs->tstate >> 24);\r\nelse\r\nreturn (unsigned char)(insn >> 5);\r\n} else\r\nreturn ASI_P;\r\n}\r\nstatic inline int decode_signedness(unsigned int insn)\r\n{\r\nreturn (insn & 0x400000);\r\n}\r\nstatic inline void maybe_flush_windows(unsigned int rs1, unsigned int rs2,\r\nunsigned int rd, int from_kernel)\r\n{\r\nif (rs2 >= 16 || rs1 >= 16 || rd >= 16) {\r\nif (from_kernel != 0)\r\n__asm__ __volatile__("flushw");\r\nelse\r\nflushw_user();\r\n}\r\n}\r\nstatic inline long sign_extend_imm13(long imm)\r\n{\r\nreturn imm << 51 >> 51;\r\n}\r\nstatic unsigned long fetch_reg(unsigned int reg, struct pt_regs *regs)\r\n{\r\nunsigned long value, fp;\r\nif (reg < 16)\r\nreturn (!reg ? 0 : regs->u_regs[reg]);\r\nfp = regs->u_regs[UREG_FP];\r\nif (regs->tstate & TSTATE_PRIV) {\r\nstruct reg_window *win;\r\nwin = (struct reg_window *)(fp + STACK_BIAS);\r\nvalue = win->locals[reg - 16];\r\n} else if (!test_thread_64bit_stack(fp)) {\r\nstruct reg_window32 __user *win32;\r\nwin32 = (struct reg_window32 __user *)((unsigned long)((u32)fp));\r\nget_user(value, &win32->locals[reg - 16]);\r\n} else {\r\nstruct reg_window __user *win;\r\nwin = (struct reg_window __user *)(fp + STACK_BIAS);\r\nget_user(value, &win->locals[reg - 16]);\r\n}\r\nreturn value;\r\n}\r\nstatic unsigned long *fetch_reg_addr(unsigned int reg, struct pt_regs *regs)\r\n{\r\nunsigned long fp;\r\nif (reg < 16)\r\nreturn &regs->u_regs[reg];\r\nfp = regs->u_regs[UREG_FP];\r\nif (regs->tstate & TSTATE_PRIV) {\r\nstruct reg_window *win;\r\nwin = (struct reg_window *)(fp + STACK_BIAS);\r\nreturn &win->locals[reg - 16];\r\n} else if (!test_thread_64bit_stack(fp)) {\r\nstruct reg_window32 *win32;\r\nwin32 = (struct reg_window32 *)((unsigned long)((u32)fp));\r\nreturn (unsigned long *)&win32->locals[reg - 16];\r\n} else {\r\nstruct reg_window *win;\r\nwin = (struct reg_window *)(fp + STACK_BIAS);\r\nreturn &win->locals[reg - 16];\r\n}\r\n}\r\nunsigned long compute_effective_address(struct pt_regs *regs,\r\nunsigned int insn, unsigned int rd)\r\n{\r\nunsigned int rs1 = (insn >> 14) & 0x1f;\r\nunsigned int rs2 = insn & 0x1f;\r\nint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\r\nif (insn & 0x2000) {\r\nmaybe_flush_windows(rs1, 0, rd, from_kernel);\r\nreturn (fetch_reg(rs1, regs) + sign_extend_imm13(insn));\r\n} else {\r\nmaybe_flush_windows(rs1, rs2, rd, from_kernel);\r\nreturn (fetch_reg(rs1, regs) + fetch_reg(rs2, regs));\r\n}\r\n}\r\nstatic void __used unaligned_panic(char *str, struct pt_regs *regs)\r\n{\r\ndie_if_kernel(str, regs);\r\n}\r\nstatic inline int do_int_store(int reg_num, int size, unsigned long *dst_addr,\r\nstruct pt_regs *regs, int asi, int orig_asi)\r\n{\r\nunsigned long zero = 0;\r\nunsigned long *src_val_p = &zero;\r\nunsigned long src_val;\r\nif (size == 16) {\r\nsize = 8;\r\nzero = (((long)(reg_num ?\r\n(unsigned)fetch_reg(reg_num, regs) : 0)) << 32) |\r\n(unsigned)fetch_reg(reg_num + 1, regs);\r\n} else if (reg_num) {\r\nsrc_val_p = fetch_reg_addr(reg_num, regs);\r\n}\r\nsrc_val = *src_val_p;\r\nif (unlikely(asi != orig_asi)) {\r\nswitch (size) {\r\ncase 2:\r\nsrc_val = swab16(src_val);\r\nbreak;\r\ncase 4:\r\nsrc_val = swab32(src_val);\r\nbreak;\r\ncase 8:\r\nsrc_val = swab64(src_val);\r\nbreak;\r\ncase 16:\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\n}\r\nreturn __do_int_store(dst_addr, size, src_val, asi);\r\n}\r\nstatic inline void advance(struct pt_regs *regs)\r\n{\r\nregs->tpc = regs->tnpc;\r\nregs->tnpc += 4;\r\nif (test_thread_flag(TIF_32BIT)) {\r\nregs->tpc &= 0xffffffff;\r\nregs->tnpc &= 0xffffffff;\r\n}\r\n}\r\nstatic inline int floating_point_load_or_store_p(unsigned int insn)\r\n{\r\nreturn (insn >> 24) & 1;\r\n}\r\nstatic inline int ok_for_kernel(unsigned int insn)\r\n{\r\nreturn !floating_point_load_or_store_p(insn);\r\n}\r\nstatic void kernel_mna_trap_fault(int fixup_tstate_asi)\r\n{\r\nstruct pt_regs *regs = current_thread_info()->kern_una_regs;\r\nunsigned int insn = current_thread_info()->kern_una_insn;\r\nconst struct exception_table_entry *entry;\r\nentry = search_exception_tables(regs->tpc);\r\nif (!entry) {\r\nunsigned long address;\r\naddress = compute_effective_address(regs, insn,\r\n((insn >> 25) & 0x1f));\r\nif (address < PAGE_SIZE) {\r\nprintk(KERN_ALERT "Unable to handle kernel NULL "\r\n"pointer dereference in mna handler");\r\n} else\r\nprintk(KERN_ALERT "Unable to handle kernel paging "\r\n"request in mna handler");\r\nprintk(KERN_ALERT " at virtual address %016lx\n",address);\r\nprintk(KERN_ALERT "current->{active_,}mm->context = %016lx\n",\r\n(current->mm ? CTX_HWBITS(current->mm->context) :\r\nCTX_HWBITS(current->active_mm->context)));\r\nprintk(KERN_ALERT "current->{active_,}mm->pgd = %016lx\n",\r\n(current->mm ? (unsigned long) current->mm->pgd :\r\n(unsigned long) current->active_mm->pgd));\r\ndie_if_kernel("Oops", regs);\r\n}\r\nregs->tpc = entry->fixup;\r\nregs->tnpc = regs->tpc + 4;\r\nif (fixup_tstate_asi) {\r\nregs->tstate &= ~TSTATE_ASI;\r\nregs->tstate |= (ASI_AIUS << 24UL);\r\n}\r\n}\r\nstatic void log_unaligned(struct pt_regs *regs)\r\n{\r\nstatic DEFINE_RATELIMIT_STATE(ratelimit, 5 * HZ, 5);\r\nif (__ratelimit(&ratelimit)) {\r\nprintk("Kernel unaligned access at TPC[%lx] %pS\n",\r\nregs->tpc, (void *) regs->tpc);\r\n}\r\n}\r\nasmlinkage void kernel_unaligned_trap(struct pt_regs *regs, unsigned int insn)\r\n{\r\nenum direction dir = decode_direction(insn);\r\nint size = decode_access_size(regs, insn);\r\nint orig_asi, asi;\r\ncurrent_thread_info()->kern_una_regs = regs;\r\ncurrent_thread_info()->kern_una_insn = insn;\r\norig_asi = asi = decode_asi(insn, regs);\r\nif (asi == ASI_AIUS) {\r\nkernel_mna_trap_fault(0);\r\nreturn;\r\n}\r\nlog_unaligned(regs);\r\nif (!ok_for_kernel(insn) || dir == both) {\r\nprintk("Unsupported unaligned load/store trap for kernel "\r\n"at <%016lx>.\n", regs->tpc);\r\nunaligned_panic("Kernel does fpu/atomic "\r\n"unaligned load/store.", regs);\r\nkernel_mna_trap_fault(0);\r\n} else {\r\nunsigned long addr, *reg_addr;\r\nint err;\r\naddr = compute_effective_address(regs, insn,\r\n((insn >> 25) & 0x1f));\r\nperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, addr);\r\nswitch (asi) {\r\ncase ASI_NL:\r\ncase ASI_AIUPL:\r\ncase ASI_AIUSL:\r\ncase ASI_PL:\r\ncase ASI_SL:\r\ncase ASI_PNFL:\r\ncase ASI_SNFL:\r\nasi &= ~0x08;\r\nbreak;\r\n}\r\nswitch (dir) {\r\ncase load:\r\nreg_addr = fetch_reg_addr(((insn>>25)&0x1f), regs);\r\nerr = do_int_load(reg_addr, size,\r\n(unsigned long *) addr,\r\ndecode_signedness(insn), asi);\r\nif (likely(!err) && unlikely(asi != orig_asi)) {\r\nunsigned long val_in = *reg_addr;\r\nswitch (size) {\r\ncase 2:\r\nval_in = swab16(val_in);\r\nbreak;\r\ncase 4:\r\nval_in = swab32(val_in);\r\nbreak;\r\ncase 8:\r\nval_in = swab64(val_in);\r\nbreak;\r\ncase 16:\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\n*reg_addr = val_in;\r\n}\r\nbreak;\r\ncase store:\r\nerr = do_int_store(((insn>>25)&0x1f), size,\r\n(unsigned long *) addr, regs,\r\nasi, orig_asi);\r\nbreak;\r\ndefault:\r\npanic("Impossible kernel unaligned trap.");\r\n}\r\nif (unlikely(err))\r\nkernel_mna_trap_fault(1);\r\nelse\r\nadvance(regs);\r\n}\r\n}\r\nint handle_popc(u32 insn, struct pt_regs *regs)\r\n{\r\nint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\r\nint ret, rd = ((insn >> 25) & 0x1f);\r\nu64 value;\r\nperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\r\nif (insn & 0x2000) {\r\nmaybe_flush_windows(0, 0, rd, from_kernel);\r\nvalue = sign_extend_imm13(insn);\r\n} else {\r\nmaybe_flush_windows(0, insn & 0x1f, rd, from_kernel);\r\nvalue = fetch_reg(insn & 0x1f, regs);\r\n}\r\nret = hweight64(value);\r\nif (rd < 16) {\r\nif (rd)\r\nregs->u_regs[rd] = ret;\r\n} else {\r\nunsigned long fp = regs->u_regs[UREG_FP];\r\nif (!test_thread_64bit_stack(fp)) {\r\nstruct reg_window32 __user *win32;\r\nwin32 = (struct reg_window32 __user *)((unsigned long)((u32)fp));\r\nput_user(ret, &win32->locals[rd - 16]);\r\n} else {\r\nstruct reg_window __user *win;\r\nwin = (struct reg_window __user *)(fp + STACK_BIAS);\r\nput_user(ret, &win->locals[rd - 16]);\r\n}\r\n}\r\nadvance(regs);\r\nreturn 1;\r\n}\r\nint handle_ldf_stq(u32 insn, struct pt_regs *regs)\r\n{\r\nunsigned long addr = compute_effective_address(regs, insn, 0);\r\nint freg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\r\nstruct fpustate *f = FPUSTATE;\r\nint asi = decode_asi(insn, regs);\r\nint flag = (freg < 32) ? FPRS_DL : FPRS_DU;\r\nperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\r\nsave_and_clear_fpu();\r\ncurrent_thread_info()->xfsr[0] &= ~0x1c000;\r\nif (freg & 3) {\r\ncurrent_thread_info()->xfsr[0] |= (6 << 14) ;\r\ndo_fpother(regs);\r\nreturn 0;\r\n}\r\nif (insn & 0x200000) {\r\nu64 first = 0, second = 0;\r\nif (current_thread_info()->fpsaved[0] & flag) {\r\nfirst = *(u64 *)&f->regs[freg];\r\nsecond = *(u64 *)&f->regs[freg+2];\r\n}\r\nif (asi < 0x80) {\r\ndo_privact(regs);\r\nreturn 1;\r\n}\r\nswitch (asi) {\r\ncase ASI_P:\r\ncase ASI_S: break;\r\ncase ASI_PL:\r\ncase ASI_SL:\r\n{\r\nu64 tmp = __swab64p(&first);\r\nfirst = __swab64p(&second);\r\nsecond = tmp;\r\nbreak;\r\n}\r\ndefault:\r\nif (tlb_type == hypervisor)\r\nsun4v_data_access_exception(regs, addr, 0);\r\nelse\r\nspitfire_data_access_exception(regs, 0, addr);\r\nreturn 1;\r\n}\r\nif (put_user (first >> 32, (u32 __user *)addr) ||\r\n__put_user ((u32)first, (u32 __user *)(addr + 4)) ||\r\n__put_user (second >> 32, (u32 __user *)(addr + 8)) ||\r\n__put_user ((u32)second, (u32 __user *)(addr + 12))) {\r\nif (tlb_type == hypervisor)\r\nsun4v_data_access_exception(regs, addr, 0);\r\nelse\r\nspitfire_data_access_exception(regs, 0, addr);\r\nreturn 1;\r\n}\r\n} else {\r\nu32 data[4] __attribute__ ((aligned(8)));\r\nint size, i;\r\nint err;\r\nif (asi < 0x80) {\r\ndo_privact(regs);\r\nreturn 1;\r\n} else if (asi > ASI_SNFL) {\r\nif (tlb_type == hypervisor)\r\nsun4v_data_access_exception(regs, addr, 0);\r\nelse\r\nspitfire_data_access_exception(regs, 0, addr);\r\nreturn 1;\r\n}\r\nswitch (insn & 0x180000) {\r\ncase 0x000000: size = 1; break;\r\ncase 0x100000: size = 4; break;\r\ndefault: size = 2; break;\r\n}\r\nfor (i = 0; i < size; i++)\r\ndata[i] = 0;\r\nerr = get_user (data[0], (u32 __user *) addr);\r\nif (!err) {\r\nfor (i = 1; i < size; i++)\r\nerr |= __get_user (data[i], (u32 __user *)(addr + 4*i));\r\n}\r\nif (err && !(asi & 0x2 )) {\r\nif (tlb_type == hypervisor)\r\nsun4v_data_access_exception(regs, addr, 0);\r\nelse\r\nspitfire_data_access_exception(regs, 0, addr);\r\nreturn 1;\r\n}\r\nif (asi & 0x8) {\r\nu64 tmp;\r\nswitch (size) {\r\ncase 1: data[0] = le32_to_cpup(data + 0); break;\r\ndefault:*(u64 *)(data + 0) = le64_to_cpup((u64 *)(data + 0));\r\nbreak;\r\ncase 4: tmp = le64_to_cpup((u64 *)(data + 0));\r\n*(u64 *)(data + 0) = le64_to_cpup((u64 *)(data + 2));\r\n*(u64 *)(data + 2) = tmp;\r\nbreak;\r\n}\r\n}\r\nif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\r\ncurrent_thread_info()->fpsaved[0] = FPRS_FEF;\r\ncurrent_thread_info()->gsr[0] = 0;\r\n}\r\nif (!(current_thread_info()->fpsaved[0] & flag)) {\r\nif (freg < 32)\r\nmemset(f->regs, 0, 32*sizeof(u32));\r\nelse\r\nmemset(f->regs+32, 0, 32*sizeof(u32));\r\n}\r\nmemcpy(f->regs + freg, data, size * 4);\r\ncurrent_thread_info()->fpsaved[0] |= flag;\r\n}\r\nadvance(regs);\r\nreturn 1;\r\n}\r\nvoid handle_ld_nf(u32 insn, struct pt_regs *regs)\r\n{\r\nint rd = ((insn >> 25) & 0x1f);\r\nint from_kernel = (regs->tstate & TSTATE_PRIV) != 0;\r\nunsigned long *reg;\r\nperf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);\r\nmaybe_flush_windows(0, 0, rd, from_kernel);\r\nreg = fetch_reg_addr(rd, regs);\r\nif (from_kernel || rd < 16) {\r\nreg[0] = 0;\r\nif ((insn & 0x780000) == 0x180000)\r\nreg[1] = 0;\r\n} else if (!test_thread_64bit_stack(regs->u_regs[UREG_FP])) {\r\nput_user(0, (int __user *) reg);\r\nif ((insn & 0x780000) == 0x180000)\r\nput_user(0, ((int __user *) reg) + 1);\r\n} else {\r\nput_user(0, (unsigned long __user *) reg);\r\nif ((insn & 0x780000) == 0x180000)\r\nput_user(0, (unsigned long __user *) reg + 1);\r\n}\r\nadvance(regs);\r\n}\r\nvoid handle_lddfmna(struct pt_regs *regs, unsigned long sfar, unsigned long sfsr)\r\n{\r\nunsigned long pc = regs->tpc;\r\nunsigned long tstate = regs->tstate;\r\nu32 insn;\r\nu64 value;\r\nu8 freg;\r\nint flag;\r\nstruct fpustate *f = FPUSTATE;\r\nif (tstate & TSTATE_PRIV)\r\ndie_if_kernel("lddfmna from kernel", regs);\r\nperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, sfar);\r\nif (test_thread_flag(TIF_32BIT))\r\npc = (u32)pc;\r\nif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\r\nint asi = decode_asi(insn, regs);\r\nu32 first, second;\r\nint err;\r\nif ((asi > ASI_SNFL) ||\r\n(asi < ASI_P))\r\ngoto daex;\r\nfirst = second = 0;\r\nerr = get_user(first, (u32 __user *)sfar);\r\nif (!err)\r\nerr = get_user(second, (u32 __user *)(sfar + 4));\r\nif (err) {\r\nif (!(asi & 0x2))\r\ngoto daex;\r\nfirst = second = 0;\r\n}\r\nsave_and_clear_fpu();\r\nfreg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\r\nvalue = (((u64)first) << 32) | second;\r\nif (asi & 0x8)\r\nvalue = __swab64p(&value);\r\nflag = (freg < 32) ? FPRS_DL : FPRS_DU;\r\nif (!(current_thread_info()->fpsaved[0] & FPRS_FEF)) {\r\ncurrent_thread_info()->fpsaved[0] = FPRS_FEF;\r\ncurrent_thread_info()->gsr[0] = 0;\r\n}\r\nif (!(current_thread_info()->fpsaved[0] & flag)) {\r\nif (freg < 32)\r\nmemset(f->regs, 0, 32*sizeof(u32));\r\nelse\r\nmemset(f->regs+32, 0, 32*sizeof(u32));\r\n}\r\n*(u64 *)(f->regs + freg) = value;\r\ncurrent_thread_info()->fpsaved[0] |= flag;\r\n} else {\r\ndaex:\r\nif (tlb_type == hypervisor)\r\nsun4v_data_access_exception(regs, sfar, sfsr);\r\nelse\r\nspitfire_data_access_exception(regs, sfsr, sfar);\r\nreturn;\r\n}\r\nadvance(regs);\r\n}\r\nvoid handle_stdfmna(struct pt_regs *regs, unsigned long sfar, unsigned long sfsr)\r\n{\r\nunsigned long pc = regs->tpc;\r\nunsigned long tstate = regs->tstate;\r\nu32 insn;\r\nu64 value;\r\nu8 freg;\r\nint flag;\r\nstruct fpustate *f = FPUSTATE;\r\nif (tstate & TSTATE_PRIV)\r\ndie_if_kernel("stdfmna from kernel", regs);\r\nperf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, sfar);\r\nif (test_thread_flag(TIF_32BIT))\r\npc = (u32)pc;\r\nif (get_user(insn, (u32 __user *) pc) != -EFAULT) {\r\nint asi = decode_asi(insn, regs);\r\nfreg = ((insn >> 25) & 0x1e) | ((insn >> 20) & 0x20);\r\nvalue = 0;\r\nflag = (freg < 32) ? FPRS_DL : FPRS_DU;\r\nif ((asi > ASI_SNFL) ||\r\n(asi < ASI_P))\r\ngoto daex;\r\nsave_and_clear_fpu();\r\nif (current_thread_info()->fpsaved[0] & flag)\r\nvalue = *(u64 *)&f->regs[freg];\r\nswitch (asi) {\r\ncase ASI_P:\r\ncase ASI_S: break;\r\ncase ASI_PL:\r\ncase ASI_SL:\r\nvalue = __swab64p(&value); break;\r\ndefault: goto daex;\r\n}\r\nif (put_user (value >> 32, (u32 __user *) sfar) ||\r\n__put_user ((u32)value, (u32 __user *)(sfar + 4)))\r\ngoto daex;\r\n} else {\r\ndaex:\r\nif (tlb_type == hypervisor)\r\nsun4v_data_access_exception(regs, sfar, sfsr);\r\nelse\r\nspitfire_data_access_exception(regs, sfsr, sfar);\r\nreturn;\r\n}\r\nadvance(regs);\r\n}
