int nx_hcall_sync(struct nx_crypto_ctx *nx_ctx,\r\nstruct vio_pfo_op *op,\r\nu32 may_sleep)\r\n{\r\nint rc, retries = 10;\r\nstruct vio_dev *viodev = nx_driver.viodev;\r\natomic_inc(&(nx_ctx->stats->sync_ops));\r\ndo {\r\nrc = vio_h_cop_sync(viodev, op);\r\n} while ((rc == -EBUSY && !may_sleep && retries--) ||\r\n(rc == -EBUSY && may_sleep && cond_resched()));\r\nif (rc) {\r\ndev_dbg(&viodev->dev, "vio_h_cop_sync failed: rc: %d "\r\n"hcall rc: %ld\n", rc, op->hcall_err);\r\natomic_inc(&(nx_ctx->stats->errors));\r\natomic_set(&(nx_ctx->stats->last_error), op->hcall_err);\r\natomic_set(&(nx_ctx->stats->last_error_pid), current->pid);\r\n}\r\nreturn rc;\r\n}\r\nstruct nx_sg *nx_build_sg_list(struct nx_sg *sg_head,\r\nu8 *start_addr,\r\nunsigned int len,\r\nu32 sgmax)\r\n{\r\nunsigned int sg_len = 0;\r\nstruct nx_sg *sg;\r\nu64 sg_addr = (u64)start_addr;\r\nu64 end_addr;\r\nif (is_vmalloc_addr(start_addr))\r\nsg_addr = page_to_phys(vmalloc_to_page(start_addr))\r\n+ offset_in_page(sg_addr);\r\nelse\r\nsg_addr = __pa(sg_addr);\r\nend_addr = sg_addr + len;\r\nfor (sg = sg_head; sg_len < len; sg++) {\r\nsg->addr = sg_addr;\r\nsg_addr = min_t(u64, NX_PAGE_NUM(sg_addr + NX_PAGE_SIZE), end_addr);\r\nsg->len = sg_addr - sg->addr;\r\nsg_len += sg->len;\r\nif ((sg - sg_head) == sgmax) {\r\npr_err("nx: scatter/gather list overflow, pid: %d\n",\r\ncurrent->pid);\r\nreturn NULL;\r\n}\r\n}\r\nreturn sg;\r\n}\r\nstruct nx_sg *nx_walk_and_build(struct nx_sg *nx_dst,\r\nunsigned int sglen,\r\nstruct scatterlist *sg_src,\r\nunsigned int start,\r\nunsigned int src_len)\r\n{\r\nstruct scatter_walk walk;\r\nstruct nx_sg *nx_sg = nx_dst;\r\nunsigned int n, offset = 0, len = src_len;\r\nchar *dst;\r\nfor (;;) {\r\nscatterwalk_start(&walk, sg_src);\r\nif (start < offset + sg_src->length)\r\nbreak;\r\noffset += sg_src->length;\r\nsg_src = scatterwalk_sg_next(sg_src);\r\n}\r\nscatterwalk_advance(&walk, start - offset);\r\nwhile (len && nx_sg) {\r\nn = scatterwalk_clamp(&walk, len);\r\nif (!n) {\r\nscatterwalk_start(&walk, sg_next(walk.sg));\r\nn = scatterwalk_clamp(&walk, len);\r\n}\r\ndst = scatterwalk_map(&walk);\r\nnx_sg = nx_build_sg_list(nx_sg, dst, n, sglen);\r\nlen -= n;\r\nscatterwalk_unmap(dst);\r\nscatterwalk_advance(&walk, n);\r\nscatterwalk_done(&walk, SCATTERWALK_FROM_SG, len);\r\n}\r\nreturn nx_sg;\r\n}\r\nint nx_build_sg_lists(struct nx_crypto_ctx *nx_ctx,\r\nstruct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src,\r\nunsigned int nbytes,\r\nu8 *iv)\r\n{\r\nstruct nx_sg *nx_insg = nx_ctx->in_sg;\r\nstruct nx_sg *nx_outsg = nx_ctx->out_sg;\r\nstruct blkcipher_walk walk;\r\nint rc;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nrc = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);\r\nif (rc)\r\ngoto out;\r\nif (iv)\r\nmemcpy(iv, walk.iv, AES_BLOCK_SIZE);\r\nwhile (walk.nbytes) {\r\nnx_insg = nx_build_sg_list(nx_insg, walk.src.virt.addr,\r\nwalk.nbytes, nx_ctx->ap->sglen);\r\nnx_outsg = nx_build_sg_list(nx_outsg, walk.dst.virt.addr,\r\nwalk.nbytes, nx_ctx->ap->sglen);\r\nrc = blkcipher_walk_done(desc, &walk, 0);\r\nif (rc)\r\nbreak;\r\n}\r\nif (walk.nbytes) {\r\nnx_insg = nx_build_sg_list(nx_insg, walk.src.virt.addr,\r\nwalk.nbytes, nx_ctx->ap->sglen);\r\nnx_outsg = nx_build_sg_list(nx_outsg, walk.dst.virt.addr,\r\nwalk.nbytes, nx_ctx->ap->sglen);\r\nrc = 0;\r\n}\r\nnx_ctx->op.inlen = (nx_ctx->in_sg - nx_insg) * sizeof(struct nx_sg);\r\nnx_ctx->op.outlen = (nx_ctx->out_sg - nx_outsg) * sizeof(struct nx_sg);\r\nout:\r\nreturn rc;\r\n}\r\nvoid nx_ctx_init(struct nx_crypto_ctx *nx_ctx, unsigned int function)\r\n{\r\nmemset(nx_ctx->kmem, 0, nx_ctx->kmem_len);\r\nnx_ctx->csbcpb->csb.valid |= NX_CSB_VALID_BIT;\r\nnx_ctx->op.flags = function;\r\nnx_ctx->op.csbcpb = __pa(nx_ctx->csbcpb);\r\nnx_ctx->op.in = __pa(nx_ctx->in_sg);\r\nnx_ctx->op.out = __pa(nx_ctx->out_sg);\r\nif (nx_ctx->csbcpb_aead) {\r\nnx_ctx->csbcpb_aead->csb.valid |= NX_CSB_VALID_BIT;\r\nnx_ctx->op_aead.flags = function;\r\nnx_ctx->op_aead.csbcpb = __pa(nx_ctx->csbcpb_aead);\r\nnx_ctx->op_aead.in = __pa(nx_ctx->in_sg);\r\nnx_ctx->op_aead.out = __pa(nx_ctx->out_sg);\r\n}\r\n}\r\nstatic void nx_of_update_status(struct device *dev,\r\nstruct property *p,\r\nstruct nx_of *props)\r\n{\r\nif (!strncmp(p->value, "okay", p->length)) {\r\nprops->status = NX_WAITING;\r\nprops->flags |= NX_OF_FLAG_STATUS_SET;\r\n} else {\r\ndev_info(dev, "%s: status '%s' is not 'okay'\n", __func__,\r\n(char *)p->value);\r\n}\r\n}\r\nstatic void nx_of_update_sglen(struct device *dev,\r\nstruct property *p,\r\nstruct nx_of *props)\r\n{\r\nif (p->length != sizeof(props->max_sg_len)) {\r\ndev_err(dev, "%s: unexpected format for "\r\n"ibm,max-sg-len property\n", __func__);\r\ndev_dbg(dev, "%s: ibm,max-sg-len is %d bytes "\r\n"long, expected %zd bytes\n", __func__,\r\np->length, sizeof(props->max_sg_len));\r\nreturn;\r\n}\r\nprops->max_sg_len = *(u32 *)p->value;\r\nprops->flags |= NX_OF_FLAG_MAXSGLEN_SET;\r\n}\r\nstatic void nx_of_update_msc(struct device *dev,\r\nstruct property *p,\r\nstruct nx_of *props)\r\n{\r\nstruct msc_triplet *trip;\r\nstruct max_sync_cop *msc;\r\nunsigned int bytes_so_far, i, lenp;\r\nmsc = (struct max_sync_cop *)p->value;\r\nlenp = p->length;\r\nbytes_so_far = 0;\r\nwhile ((bytes_so_far + sizeof(struct max_sync_cop)) <= lenp) {\r\nbytes_so_far += sizeof(struct max_sync_cop);\r\ntrip = msc->trip;\r\nfor (i = 0;\r\n((bytes_so_far + sizeof(struct msc_triplet)) <= lenp) &&\r\ni < msc->triplets;\r\ni++) {\r\nif (msc->fc > NX_MAX_FC || msc->mode > NX_MAX_MODE) {\r\ndev_err(dev, "unknown function code/mode "\r\n"combo: %d/%d (ignored)\n", msc->fc,\r\nmsc->mode);\r\ngoto next_loop;\r\n}\r\nswitch (trip->keybitlen) {\r\ncase 128:\r\ncase 160:\r\nprops->ap[msc->fc][msc->mode][0].databytelen =\r\ntrip->databytelen;\r\nprops->ap[msc->fc][msc->mode][0].sglen =\r\ntrip->sglen;\r\nbreak;\r\ncase 192:\r\nprops->ap[msc->fc][msc->mode][1].databytelen =\r\ntrip->databytelen;\r\nprops->ap[msc->fc][msc->mode][1].sglen =\r\ntrip->sglen;\r\nbreak;\r\ncase 256:\r\nif (msc->fc == NX_FC_AES) {\r\nprops->ap[msc->fc][msc->mode][2].\r\ndatabytelen = trip->databytelen;\r\nprops->ap[msc->fc][msc->mode][2].sglen =\r\ntrip->sglen;\r\n} else if (msc->fc == NX_FC_AES_HMAC ||\r\nmsc->fc == NX_FC_SHA) {\r\nprops->ap[msc->fc][msc->mode][1].\r\ndatabytelen = trip->databytelen;\r\nprops->ap[msc->fc][msc->mode][1].sglen =\r\ntrip->sglen;\r\n} else {\r\ndev_warn(dev, "unknown function "\r\n"code/key bit len combo"\r\n": (%u/256)\n", msc->fc);\r\n}\r\nbreak;\r\ncase 512:\r\nprops->ap[msc->fc][msc->mode][2].databytelen =\r\ntrip->databytelen;\r\nprops->ap[msc->fc][msc->mode][2].sglen =\r\ntrip->sglen;\r\nbreak;\r\ndefault:\r\ndev_warn(dev, "unknown function code/key bit "\r\n"len combo: (%u/%u)\n", msc->fc,\r\ntrip->keybitlen);\r\nbreak;\r\n}\r\nnext_loop:\r\nbytes_so_far += sizeof(struct msc_triplet);\r\ntrip++;\r\n}\r\nmsc = (struct max_sync_cop *)trip;\r\n}\r\nprops->flags |= NX_OF_FLAG_MAXSYNCCOP_SET;\r\n}\r\nstatic void nx_of_init(struct device *dev, struct nx_of *props)\r\n{\r\nstruct device_node *base_node = dev->of_node;\r\nstruct property *p;\r\np = of_find_property(base_node, "status", NULL);\r\nif (!p)\r\ndev_info(dev, "%s: property 'status' not found\n", __func__);\r\nelse\r\nnx_of_update_status(dev, p, props);\r\np = of_find_property(base_node, "ibm,max-sg-len", NULL);\r\nif (!p)\r\ndev_info(dev, "%s: property 'ibm,max-sg-len' not found\n",\r\n__func__);\r\nelse\r\nnx_of_update_sglen(dev, p, props);\r\np = of_find_property(base_node, "ibm,max-sync-cop", NULL);\r\nif (!p)\r\ndev_info(dev, "%s: property 'ibm,max-sync-cop' not found\n",\r\n__func__);\r\nelse\r\nnx_of_update_msc(dev, p, props);\r\n}\r\nstatic int nx_register_algs(void)\r\n{\r\nint rc = -1;\r\nif (nx_driver.of.flags != NX_OF_FLAG_MASK_READY)\r\ngoto out;\r\nmemset(&nx_driver.stats, 0, sizeof(struct nx_stats));\r\nrc = NX_DEBUGFS_INIT(&nx_driver);\r\nif (rc)\r\ngoto out;\r\nrc = crypto_register_alg(&nx_ecb_aes_alg);\r\nif (rc)\r\ngoto out;\r\nrc = crypto_register_alg(&nx_cbc_aes_alg);\r\nif (rc)\r\ngoto out_unreg_ecb;\r\nrc = crypto_register_alg(&nx_ctr_aes_alg);\r\nif (rc)\r\ngoto out_unreg_cbc;\r\nrc = crypto_register_alg(&nx_ctr3686_aes_alg);\r\nif (rc)\r\ngoto out_unreg_ctr;\r\nrc = crypto_register_alg(&nx_gcm_aes_alg);\r\nif (rc)\r\ngoto out_unreg_ctr3686;\r\nrc = crypto_register_alg(&nx_gcm4106_aes_alg);\r\nif (rc)\r\ngoto out_unreg_gcm;\r\nrc = crypto_register_alg(&nx_ccm_aes_alg);\r\nif (rc)\r\ngoto out_unreg_gcm4106;\r\nrc = crypto_register_alg(&nx_ccm4309_aes_alg);\r\nif (rc)\r\ngoto out_unreg_ccm;\r\nrc = crypto_register_shash(&nx_shash_sha256_alg);\r\nif (rc)\r\ngoto out_unreg_ccm4309;\r\nrc = crypto_register_shash(&nx_shash_sha512_alg);\r\nif (rc)\r\ngoto out_unreg_s256;\r\nrc = crypto_register_shash(&nx_shash_aes_xcbc_alg);\r\nif (rc)\r\ngoto out_unreg_s512;\r\nnx_driver.of.status = NX_OKAY;\r\ngoto out;\r\nout_unreg_s512:\r\ncrypto_unregister_shash(&nx_shash_sha512_alg);\r\nout_unreg_s256:\r\ncrypto_unregister_shash(&nx_shash_sha256_alg);\r\nout_unreg_ccm4309:\r\ncrypto_unregister_alg(&nx_ccm4309_aes_alg);\r\nout_unreg_ccm:\r\ncrypto_unregister_alg(&nx_ccm_aes_alg);\r\nout_unreg_gcm4106:\r\ncrypto_unregister_alg(&nx_gcm4106_aes_alg);\r\nout_unreg_gcm:\r\ncrypto_unregister_alg(&nx_gcm_aes_alg);\r\nout_unreg_ctr3686:\r\ncrypto_unregister_alg(&nx_ctr3686_aes_alg);\r\nout_unreg_ctr:\r\ncrypto_unregister_alg(&nx_ctr_aes_alg);\r\nout_unreg_cbc:\r\ncrypto_unregister_alg(&nx_cbc_aes_alg);\r\nout_unreg_ecb:\r\ncrypto_unregister_alg(&nx_ecb_aes_alg);\r\nout:\r\nreturn rc;\r\n}\r\nstatic int nx_crypto_ctx_init(struct nx_crypto_ctx *nx_ctx, u32 fc, u32 mode)\r\n{\r\nif (nx_driver.of.status != NX_OKAY) {\r\npr_err("Attempt to initialize NX crypto context while device "\r\n"is not available!\n");\r\nreturn -ENODEV;\r\n}\r\nif (mode == NX_MODE_AES_GCM || mode == NX_MODE_AES_CCM)\r\nnx_ctx->kmem_len = (4 * NX_PAGE_SIZE) +\r\nsizeof(struct nx_csbcpb);\r\nelse\r\nnx_ctx->kmem_len = (3 * NX_PAGE_SIZE) +\r\nsizeof(struct nx_csbcpb);\r\nnx_ctx->kmem = kmalloc(nx_ctx->kmem_len, GFP_KERNEL);\r\nif (!nx_ctx->kmem)\r\nreturn -ENOMEM;\r\nnx_ctx->csbcpb = (struct nx_csbcpb *)(round_up((u64)nx_ctx->kmem,\r\n(u64)NX_PAGE_SIZE));\r\nnx_ctx->in_sg = (struct nx_sg *)((u8 *)nx_ctx->csbcpb + NX_PAGE_SIZE);\r\nnx_ctx->out_sg = (struct nx_sg *)((u8 *)nx_ctx->in_sg + NX_PAGE_SIZE);\r\nif (mode == NX_MODE_AES_GCM || mode == NX_MODE_AES_CCM)\r\nnx_ctx->csbcpb_aead =\r\n(struct nx_csbcpb *)((u8 *)nx_ctx->out_sg +\r\nNX_PAGE_SIZE);\r\nnx_ctx->stats = &nx_driver.stats;\r\nmemcpy(nx_ctx->props, nx_driver.of.ap[fc][mode],\r\nsizeof(struct alg_props) * 3);\r\nreturn 0;\r\n}\r\nint nx_crypto_ctx_aes_ccm_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_CCM);\r\n}\r\nint nx_crypto_ctx_aes_gcm_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_GCM);\r\n}\r\nint nx_crypto_ctx_aes_ctr_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_CTR);\r\n}\r\nint nx_crypto_ctx_aes_cbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_CBC);\r\n}\r\nint nx_crypto_ctx_aes_ecb_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_ECB);\r\n}\r\nint nx_crypto_ctx_sha_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_SHA, NX_MODE_SHA);\r\n}\r\nint nx_crypto_ctx_aes_xcbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_XCBC_MAC);\r\n}\r\nvoid nx_crypto_ctx_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(tfm);\r\nkzfree(nx_ctx->kmem);\r\nnx_ctx->csbcpb = NULL;\r\nnx_ctx->csbcpb_aead = NULL;\r\nnx_ctx->in_sg = NULL;\r\nnx_ctx->out_sg = NULL;\r\n}\r\nstatic int nx_probe(struct vio_dev *viodev, const struct vio_device_id *id)\r\n{\r\ndev_dbg(&viodev->dev, "driver probed: %s resource id: 0x%x\n",\r\nviodev->name, viodev->resource_id);\r\nif (nx_driver.viodev) {\r\ndev_err(&viodev->dev, "%s: Attempt to register more than one "\r\n"instance of the hardware\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nnx_driver.viodev = viodev;\r\nnx_of_init(&viodev->dev, &nx_driver.of);\r\nreturn nx_register_algs();\r\n}\r\nstatic int nx_remove(struct vio_dev *viodev)\r\n{\r\ndev_dbg(&viodev->dev, "entering nx_remove for UA 0x%x\n",\r\nviodev->unit_address);\r\nif (nx_driver.of.status == NX_OKAY) {\r\nNX_DEBUGFS_FINI(&nx_driver);\r\ncrypto_unregister_alg(&nx_ccm_aes_alg);\r\ncrypto_unregister_alg(&nx_ccm4309_aes_alg);\r\ncrypto_unregister_alg(&nx_gcm_aes_alg);\r\ncrypto_unregister_alg(&nx_gcm4106_aes_alg);\r\ncrypto_unregister_alg(&nx_ctr_aes_alg);\r\ncrypto_unregister_alg(&nx_ctr3686_aes_alg);\r\ncrypto_unregister_alg(&nx_cbc_aes_alg);\r\ncrypto_unregister_alg(&nx_ecb_aes_alg);\r\ncrypto_unregister_shash(&nx_shash_sha256_alg);\r\ncrypto_unregister_shash(&nx_shash_sha512_alg);\r\ncrypto_unregister_shash(&nx_shash_aes_xcbc_alg);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init nx_init(void)\r\n{\r\nreturn vio_register_driver(&nx_driver.viodriver);\r\n}\r\nstatic void __exit nx_fini(void)\r\n{\r\nvio_unregister_driver(&nx_driver.viodriver);\r\n}
