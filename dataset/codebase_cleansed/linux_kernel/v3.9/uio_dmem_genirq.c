static int uio_dmem_genirq_open(struct uio_info *info, struct inode *inode)\r\n{\r\nstruct uio_dmem_genirq_platdata *priv = info->priv;\r\nstruct uio_mem *uiomem;\r\nint ret = 0;\r\nint dmem_region = priv->dmem_region_start;\r\nuiomem = &priv->uioinfo->mem[priv->dmem_region_start];\r\nmutex_lock(&priv->alloc_lock);\r\nwhile (!priv->refcnt && uiomem < &priv->uioinfo->mem[MAX_UIO_MAPS]) {\r\nvoid *addr;\r\nif (!uiomem->size)\r\nbreak;\r\naddr = dma_alloc_coherent(&priv->pdev->dev, uiomem->size,\r\n(dma_addr_t *)&uiomem->addr, GFP_KERNEL);\r\nif (!addr) {\r\nuiomem->addr = DMEM_MAP_ERROR;\r\n}\r\npriv->dmem_region_vaddr[dmem_region++] = addr;\r\n++uiomem;\r\n}\r\npriv->refcnt++;\r\nmutex_unlock(&priv->alloc_lock);\r\npm_runtime_get_sync(&priv->pdev->dev);\r\nreturn ret;\r\n}\r\nstatic int uio_dmem_genirq_release(struct uio_info *info, struct inode *inode)\r\n{\r\nstruct uio_dmem_genirq_platdata *priv = info->priv;\r\nstruct uio_mem *uiomem;\r\nint dmem_region = priv->dmem_region_start;\r\npm_runtime_put_sync(&priv->pdev->dev);\r\nuiomem = &priv->uioinfo->mem[priv->dmem_region_start];\r\nmutex_lock(&priv->alloc_lock);\r\npriv->refcnt--;\r\nwhile (!priv->refcnt && uiomem < &priv->uioinfo->mem[MAX_UIO_MAPS]) {\r\nif (!uiomem->size)\r\nbreak;\r\nif (priv->dmem_region_vaddr[dmem_region]) {\r\ndma_free_coherent(&priv->pdev->dev, uiomem->size,\r\npriv->dmem_region_vaddr[dmem_region],\r\nuiomem->addr);\r\n}\r\nuiomem->addr = DMEM_MAP_ERROR;\r\n++dmem_region;\r\n++uiomem;\r\n}\r\nmutex_unlock(&priv->alloc_lock);\r\nreturn 0;\r\n}\r\nstatic irqreturn_t uio_dmem_genirq_handler(int irq, struct uio_info *dev_info)\r\n{\r\nstruct uio_dmem_genirq_platdata *priv = dev_info->priv;\r\nif (!test_and_set_bit(0, &priv->flags))\r\ndisable_irq_nosync(irq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int uio_dmem_genirq_irqcontrol(struct uio_info *dev_info, s32 irq_on)\r\n{\r\nstruct uio_dmem_genirq_platdata *priv = dev_info->priv;\r\nunsigned long flags;\r\nspin_lock_irqsave(&priv->lock, flags);\r\nif (irq_on) {\r\nif (test_and_clear_bit(0, &priv->flags))\r\nenable_irq(dev_info->irq);\r\n} else {\r\nif (!test_and_set_bit(0, &priv->flags))\r\ndisable_irq(dev_info->irq);\r\n}\r\nspin_unlock_irqrestore(&priv->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int uio_dmem_genirq_probe(struct platform_device *pdev)\r\n{\r\nstruct uio_dmem_genirq_pdata *pdata = pdev->dev.platform_data;\r\nstruct uio_info *uioinfo = &pdata->uioinfo;\r\nstruct uio_dmem_genirq_platdata *priv;\r\nstruct uio_mem *uiomem;\r\nint ret = -EINVAL;\r\nint i;\r\nif (pdev->dev.of_node) {\r\nint irq;\r\nuioinfo = kzalloc(sizeof(*uioinfo), GFP_KERNEL);\r\nif (!uioinfo) {\r\nret = -ENOMEM;\r\ndev_err(&pdev->dev, "unable to kmalloc\n");\r\ngoto bad2;\r\n}\r\nuioinfo->name = pdev->dev.of_node->name;\r\nuioinfo->version = "devicetree";\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq == -ENXIO)\r\nuioinfo->irq = UIO_IRQ_NONE;\r\nelse\r\nuioinfo->irq = irq;\r\n}\r\nif (!uioinfo || !uioinfo->name || !uioinfo->version) {\r\ndev_err(&pdev->dev, "missing platform_data\n");\r\ngoto bad0;\r\n}\r\nif (uioinfo->handler || uioinfo->irqcontrol ||\r\nuioinfo->irq_flags & IRQF_SHARED) {\r\ndev_err(&pdev->dev, "interrupt configuration error\n");\r\ngoto bad0;\r\n}\r\npriv = kzalloc(sizeof(*priv), GFP_KERNEL);\r\nif (!priv) {\r\nret = -ENOMEM;\r\ndev_err(&pdev->dev, "unable to kmalloc\n");\r\ngoto bad0;\r\n}\r\ndma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(32));\r\npriv->uioinfo = uioinfo;\r\nspin_lock_init(&priv->lock);\r\npriv->flags = 0;\r\npriv->pdev = pdev;\r\nmutex_init(&priv->alloc_lock);\r\nif (!uioinfo->irq) {\r\nret = platform_get_irq(pdev, 0);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "failed to get IRQ\n");\r\ngoto bad0;\r\n}\r\nuioinfo->irq = ret;\r\n}\r\nuiomem = &uioinfo->mem[0];\r\nfor (i = 0; i < pdev->num_resources; ++i) {\r\nstruct resource *r = &pdev->resource[i];\r\nif (r->flags != IORESOURCE_MEM)\r\ncontinue;\r\nif (uiomem >= &uioinfo->mem[MAX_UIO_MAPS]) {\r\ndev_warn(&pdev->dev, "device has more than "\r\n__stringify(MAX_UIO_MAPS)\r\n" I/O memory resources.\n");\r\nbreak;\r\n}\r\nuiomem->memtype = UIO_MEM_PHYS;\r\nuiomem->addr = r->start;\r\nuiomem->size = resource_size(r);\r\n++uiomem;\r\n}\r\npriv->dmem_region_start = i;\r\npriv->num_dmem_regions = pdata->num_dynamic_regions;\r\nfor (i = 0; i < pdata->num_dynamic_regions; ++i) {\r\nif (uiomem >= &uioinfo->mem[MAX_UIO_MAPS]) {\r\ndev_warn(&pdev->dev, "device has more than "\r\n__stringify(MAX_UIO_MAPS)\r\n" dynamic and fixed memory regions.\n");\r\nbreak;\r\n}\r\nuiomem->memtype = UIO_MEM_PHYS;\r\nuiomem->addr = DMEM_MAP_ERROR;\r\nuiomem->size = pdata->dynamic_region_sizes[i];\r\n++uiomem;\r\n}\r\nwhile (uiomem < &uioinfo->mem[MAX_UIO_MAPS]) {\r\nuiomem->size = 0;\r\n++uiomem;\r\n}\r\nuioinfo->handler = uio_dmem_genirq_handler;\r\nuioinfo->irqcontrol = uio_dmem_genirq_irqcontrol;\r\nuioinfo->open = uio_dmem_genirq_open;\r\nuioinfo->release = uio_dmem_genirq_release;\r\nuioinfo->priv = priv;\r\npm_runtime_enable(&pdev->dev);\r\nret = uio_register_device(&pdev->dev, priv->uioinfo);\r\nif (ret) {\r\ndev_err(&pdev->dev, "unable to register uio device\n");\r\ngoto bad1;\r\n}\r\nplatform_set_drvdata(pdev, priv);\r\nreturn 0;\r\nbad1:\r\nkfree(priv);\r\npm_runtime_disable(&pdev->dev);\r\nbad0:\r\nif (pdev->dev.of_node)\r\nkfree(uioinfo);\r\nbad2:\r\nreturn ret;\r\n}\r\nstatic int uio_dmem_genirq_remove(struct platform_device *pdev)\r\n{\r\nstruct uio_dmem_genirq_platdata *priv = platform_get_drvdata(pdev);\r\nuio_unregister_device(priv->uioinfo);\r\npm_runtime_disable(&pdev->dev);\r\npriv->uioinfo->handler = NULL;\r\npriv->uioinfo->irqcontrol = NULL;\r\nif (pdev->dev.of_node)\r\nkfree(priv->uioinfo);\r\nkfree(priv);\r\nreturn 0;\r\n}\r\nstatic int uio_dmem_genirq_runtime_nop(struct device *dev)\r\n{\r\nreturn 0;\r\n}
