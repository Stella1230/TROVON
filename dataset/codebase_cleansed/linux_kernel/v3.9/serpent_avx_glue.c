static void serpent_crypt_ctr(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\nbe128 ctrblk;\r\nle128_to_be128(&ctrblk, iv);\r\nle128_inc(iv);\r\n__serpent_encrypt(ctx, (u8 *)&ctrblk, (u8 *)&ctrblk);\r\nu128_xor(dst, src, (u128 *)&ctrblk);\r\n}\r\nstatic int ecb_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ecb_crypt_128bit(&serpent_enc, desc, dst, src, nbytes);\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ecb_crypt_128bit(&serpent_dec, desc, dst, src, nbytes);\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_cbc_encrypt_128bit(GLUE_FUNC_CAST(__serpent_encrypt), desc,\r\ndst, src, nbytes);\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_cbc_decrypt_128bit(&serpent_dec_cbc, desc, dst, src,\r\nnbytes);\r\n}\r\nstatic int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ctr_crypt_128bit(&serpent_ctr, desc, dst, src, nbytes);\r\n}\r\nstatic inline bool serpent_fpu_begin(bool fpu_enabled, unsigned int nbytes)\r\n{\r\nreturn glue_fpu_begin(SERPENT_BLOCK_SIZE, SERPENT_PARALLEL_BLOCKS,\r\nNULL, fpu_enabled, nbytes);\r\n}\r\nstatic inline void serpent_fpu_end(bool fpu_enabled)\r\n{\r\nglue_fpu_end(fpu_enabled);\r\n}\r\nstatic void encrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes == bsize * SERPENT_PARALLEL_BLOCKS) {\r\nserpent_ecb_enc_8way_avx(ctx->ctx, srcdst, srcdst);\r\nreturn;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_encrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic void decrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes == bsize * SERPENT_PARALLEL_BLOCKS) {\r\nserpent_ecb_dec_8way_avx(ctx->ctx, srcdst, srcdst);\r\nreturn;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_decrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic int lrw_serpent_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nerr = __serpent_setkey(&ctx->serpent_ctx, key, keylen -\r\nSERPENT_BLOCK_SIZE);\r\nif (err)\r\nreturn err;\r\nreturn lrw_init_table(&ctx->lrw_table, key + keylen -\r\nSERPENT_BLOCK_SIZE);\r\n}\r\nstatic int lrw_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int lrw_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic void lrw_exit_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nlrw_free_table(&ctx->lrw_table);\r\n}\r\nstatic int xts_serpent_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_tfm_ctx(tfm);\r\nu32 *flags = &tfm->crt_flags;\r\nint err;\r\nif (keylen % 2) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nerr = __serpent_setkey(&ctx->crypt_ctx, key, keylen / 2);\r\nif (err)\r\nreturn err;\r\nreturn __serpent_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2);\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->crypt_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = &ctx->tweak_ctx,\r\n.tweak_fn = XTS_TWEAK_CAST(__serpent_encrypt),\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->crypt_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = &ctx->tweak_ctx,\r\n.tweak_fn = XTS_TWEAK_CAST(__serpent_encrypt),\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int __init serpent_init(void)\r\n{\r\nu64 xcr0;\r\nif (!cpu_has_avx || !cpu_has_osxsave) {\r\nprintk(KERN_INFO "AVX instructions are not detected.\n");\r\nreturn -ENODEV;\r\n}\r\nxcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\r\nif ((xcr0 & (XSTATE_SSE | XSTATE_YMM)) != (XSTATE_SSE | XSTATE_YMM)) {\r\nprintk(KERN_INFO "AVX detected but unusable.\n");\r\nreturn -ENODEV;\r\n}\r\nreturn crypto_register_algs(serpent_algs, ARRAY_SIZE(serpent_algs));\r\n}\r\nstatic void __exit serpent_exit(void)\r\n{\r\ncrypto_unregister_algs(serpent_algs, ARRAY_SIZE(serpent_algs));\r\n}
