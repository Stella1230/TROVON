static inline int __movsl_is_ok(unsigned long a1, unsigned long a2, unsigned long n)\r\n{\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nif (n >= 64 && ((a1 ^ a2) & movsl_mask.mask))\r\nreturn 0;\r\n#endif\r\nreturn 1;\r\n}\r\nunsigned long\r\nclear_user(void __user *to, unsigned long n)\r\n{\r\nmight_fault();\r\nif (access_ok(VERIFY_WRITE, to, n))\r\n__do_clear_user(to, n);\r\nreturn n;\r\n}\r\nunsigned long\r\n__clear_user(void __user *to, unsigned long n)\r\n{\r\n__do_clear_user(to, n);\r\nreturn n;\r\n}\r\nstatic unsigned long\r\n__copy_user_intel(void __user *to, const void *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"1: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 3f\n"\r\n"2: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"3: movl 0(%4), %%eax\n"\r\n"4: movl 4(%4), %%edx\n"\r\n"5: movl %%eax, 0(%3)\n"\r\n"6: movl %%edx, 4(%3)\n"\r\n"7: movl 8(%4), %%eax\n"\r\n"8: movl 12(%4),%%edx\n"\r\n"9: movl %%eax, 8(%3)\n"\r\n"10: movl %%edx, 12(%3)\n"\r\n"11: movl 16(%4), %%eax\n"\r\n"12: movl 20(%4), %%edx\n"\r\n"13: movl %%eax, 16(%3)\n"\r\n"14: movl %%edx, 20(%3)\n"\r\n"15: movl 24(%4), %%eax\n"\r\n"16: movl 28(%4), %%edx\n"\r\n"17: movl %%eax, 24(%3)\n"\r\n"18: movl %%edx, 28(%3)\n"\r\n"19: movl 32(%4), %%eax\n"\r\n"20: movl 36(%4), %%edx\n"\r\n"21: movl %%eax, 32(%3)\n"\r\n"22: movl %%edx, 36(%3)\n"\r\n"23: movl 40(%4), %%eax\n"\r\n"24: movl 44(%4), %%edx\n"\r\n"25: movl %%eax, 40(%3)\n"\r\n"26: movl %%edx, 44(%3)\n"\r\n"27: movl 48(%4), %%eax\n"\r\n"28: movl 52(%4), %%edx\n"\r\n"29: movl %%eax, 48(%3)\n"\r\n"30: movl %%edx, 52(%3)\n"\r\n"31: movl 56(%4), %%eax\n"\r\n"32: movl 60(%4), %%edx\n"\r\n"33: movl %%eax, 56(%3)\n"\r\n"34: movl %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 1b\n"\r\n"35: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"99: rep; movsl\n"\r\n"36: movl %%eax, %0\n"\r\n"37: rep; movsb\n"\r\n"100:\n"\r\n".section .fixup,\"ax\"\n"\r\n"101: lea 0(%%eax,%0,4),%0\n"\r\n" jmp 100b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(1b,100b)\r\n_ASM_EXTABLE(2b,100b)\r\n_ASM_EXTABLE(3b,100b)\r\n_ASM_EXTABLE(4b,100b)\r\n_ASM_EXTABLE(5b,100b)\r\n_ASM_EXTABLE(6b,100b)\r\n_ASM_EXTABLE(7b,100b)\r\n_ASM_EXTABLE(8b,100b)\r\n_ASM_EXTABLE(9b,100b)\r\n_ASM_EXTABLE(10b,100b)\r\n_ASM_EXTABLE(11b,100b)\r\n_ASM_EXTABLE(12b,100b)\r\n_ASM_EXTABLE(13b,100b)\r\n_ASM_EXTABLE(14b,100b)\r\n_ASM_EXTABLE(15b,100b)\r\n_ASM_EXTABLE(16b,100b)\r\n_ASM_EXTABLE(17b,100b)\r\n_ASM_EXTABLE(18b,100b)\r\n_ASM_EXTABLE(19b,100b)\r\n_ASM_EXTABLE(20b,100b)\r\n_ASM_EXTABLE(21b,100b)\r\n_ASM_EXTABLE(22b,100b)\r\n_ASM_EXTABLE(23b,100b)\r\n_ASM_EXTABLE(24b,100b)\r\n_ASM_EXTABLE(25b,100b)\r\n_ASM_EXTABLE(26b,100b)\r\n_ASM_EXTABLE(27b,100b)\r\n_ASM_EXTABLE(28b,100b)\r\n_ASM_EXTABLE(29b,100b)\r\n_ASM_EXTABLE(30b,100b)\r\n_ASM_EXTABLE(31b,100b)\r\n_ASM_EXTABLE(32b,100b)\r\n_ASM_EXTABLE(33b,100b)\r\n_ASM_EXTABLE(34b,100b)\r\n_ASM_EXTABLE(35b,100b)\r\n_ASM_EXTABLE(36b,100b)\r\n_ASM_EXTABLE(37b,100b)\r\n_ASM_EXTABLE(99b,101b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nstatic unsigned long\r\n__copy_user_zeroing_intel(void *to, const void __user *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movl %%eax, 0(%3)\n"\r\n" movl %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movl %%eax, 8(%3)\n"\r\n" movl %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movl %%eax, 16(%3)\n"\r\n" movl %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movl %%eax, 24(%3)\n"\r\n" movl %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movl %%eax, 32(%3)\n"\r\n" movl %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movl %%eax, 40(%3)\n"\r\n" movl %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movl %%eax, 48(%3)\n"\r\n" movl %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movl %%eax, 56(%3)\n"\r\n" movl %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: pushl %0\n"\r\n" pushl %%eax\n"\r\n" xorl %%eax,%%eax\n"\r\n" rep; stosb\n"\r\n" popl %%eax\n"\r\n" popl %0\n"\r\n" jmp 8b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(0b,16b)\r\n_ASM_EXTABLE(1b,16b)\r\n_ASM_EXTABLE(2b,16b)\r\n_ASM_EXTABLE(21b,16b)\r\n_ASM_EXTABLE(3b,16b)\r\n_ASM_EXTABLE(31b,16b)\r\n_ASM_EXTABLE(4b,16b)\r\n_ASM_EXTABLE(41b,16b)\r\n_ASM_EXTABLE(10b,16b)\r\n_ASM_EXTABLE(51b,16b)\r\n_ASM_EXTABLE(11b,16b)\r\n_ASM_EXTABLE(61b,16b)\r\n_ASM_EXTABLE(12b,16b)\r\n_ASM_EXTABLE(71b,16b)\r\n_ASM_EXTABLE(13b,16b)\r\n_ASM_EXTABLE(81b,16b)\r\n_ASM_EXTABLE(14b,16b)\r\n_ASM_EXTABLE(91b,16b)\r\n_ASM_EXTABLE(6b,9b)\r\n_ASM_EXTABLE(7b,16b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nstatic unsigned long __copy_user_zeroing_intel_nocache(void *to,\r\nconst void __user *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movnti %%eax, 0(%3)\n"\r\n" movnti %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movnti %%eax, 8(%3)\n"\r\n" movnti %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movnti %%eax, 16(%3)\n"\r\n" movnti %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movnti %%eax, 24(%3)\n"\r\n" movnti %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movnti %%eax, 32(%3)\n"\r\n" movnti %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movnti %%eax, 40(%3)\n"\r\n" movnti %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movnti %%eax, 48(%3)\n"\r\n" movnti %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movnti %%eax, 56(%3)\n"\r\n" movnti %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n" sfence \n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: pushl %0\n"\r\n" pushl %%eax\n"\r\n" xorl %%eax,%%eax\n"\r\n" rep; stosb\n"\r\n" popl %%eax\n"\r\n" popl %0\n"\r\n" jmp 8b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(0b,16b)\r\n_ASM_EXTABLE(1b,16b)\r\n_ASM_EXTABLE(2b,16b)\r\n_ASM_EXTABLE(21b,16b)\r\n_ASM_EXTABLE(3b,16b)\r\n_ASM_EXTABLE(31b,16b)\r\n_ASM_EXTABLE(4b,16b)\r\n_ASM_EXTABLE(41b,16b)\r\n_ASM_EXTABLE(10b,16b)\r\n_ASM_EXTABLE(51b,16b)\r\n_ASM_EXTABLE(11b,16b)\r\n_ASM_EXTABLE(61b,16b)\r\n_ASM_EXTABLE(12b,16b)\r\n_ASM_EXTABLE(71b,16b)\r\n_ASM_EXTABLE(13b,16b)\r\n_ASM_EXTABLE(81b,16b)\r\n_ASM_EXTABLE(14b,16b)\r\n_ASM_EXTABLE(91b,16b)\r\n_ASM_EXTABLE(6b,9b)\r\n_ASM_EXTABLE(7b,16b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nstatic unsigned long __copy_user_intel_nocache(void *to,\r\nconst void __user *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movnti %%eax, 0(%3)\n"\r\n" movnti %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movnti %%eax, 8(%3)\n"\r\n" movnti %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movnti %%eax, 16(%3)\n"\r\n" movnti %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movnti %%eax, 24(%3)\n"\r\n" movnti %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movnti %%eax, 32(%3)\n"\r\n" movnti %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movnti %%eax, 40(%3)\n"\r\n" movnti %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movnti %%eax, 48(%3)\n"\r\n" movnti %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movnti %%eax, 56(%3)\n"\r\n" movnti %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n" sfence \n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: jmp 8b\n"\r\n".previous\n"\r\n_ASM_EXTABLE(0b,16b)\r\n_ASM_EXTABLE(1b,16b)\r\n_ASM_EXTABLE(2b,16b)\r\n_ASM_EXTABLE(21b,16b)\r\n_ASM_EXTABLE(3b,16b)\r\n_ASM_EXTABLE(31b,16b)\r\n_ASM_EXTABLE(4b,16b)\r\n_ASM_EXTABLE(41b,16b)\r\n_ASM_EXTABLE(10b,16b)\r\n_ASM_EXTABLE(51b,16b)\r\n_ASM_EXTABLE(11b,16b)\r\n_ASM_EXTABLE(61b,16b)\r\n_ASM_EXTABLE(12b,16b)\r\n_ASM_EXTABLE(71b,16b)\r\n_ASM_EXTABLE(13b,16b)\r\n_ASM_EXTABLE(81b,16b)\r\n_ASM_EXTABLE(14b,16b)\r\n_ASM_EXTABLE(91b,16b)\r\n_ASM_EXTABLE(6b,9b)\r\n_ASM_EXTABLE(7b,16b)\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nunsigned long __copy_to_user_ll(void __user *to, const void *from,\r\nunsigned long n)\r\n{\r\nstac();\r\nif (movsl_is_ok(to, from, n))\r\n__copy_user(to, from, n);\r\nelse\r\nn = __copy_user_intel(to, from, n);\r\nclac();\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\nstac();\r\nif (movsl_is_ok(to, from, n))\r\n__copy_user_zeroing(to, from, n);\r\nelse\r\nn = __copy_user_zeroing_intel(to, from, n);\r\nclac();\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll_nozero(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\nstac();\r\nif (movsl_is_ok(to, from, n))\r\n__copy_user(to, from, n);\r\nelse\r\nn = __copy_user_intel((void __user *)to,\r\n(const void *)from, n);\r\nclac();\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll_nocache(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\nstac();\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nif (n > 64 && cpu_has_xmm2)\r\nn = __copy_user_zeroing_intel_nocache(to, from, n);\r\nelse\r\n__copy_user_zeroing(to, from, n);\r\n#else\r\n__copy_user_zeroing(to, from, n);\r\n#endif\r\nclac();\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\nstac();\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nif (n > 64 && cpu_has_xmm2)\r\nn = __copy_user_intel_nocache(to, from, n);\r\nelse\r\n__copy_user(to, from, n);\r\n#else\r\n__copy_user(to, from, n);\r\n#endif\r\nclac();\r\nreturn n;\r\n}\r\nunsigned long\r\ncopy_to_user(void __user *to, const void *from, unsigned long n)\r\n{\r\nif (access_ok(VERIFY_WRITE, to, n))\r\nn = __copy_to_user(to, from, n);\r\nreturn n;\r\n}\r\nunsigned long\r\n_copy_from_user(void *to, const void __user *from, unsigned long n)\r\n{\r\nif (access_ok(VERIFY_READ, from, n))\r\nn = __copy_from_user(to, from, n);\r\nelse\r\nmemset(to, 0, n);\r\nreturn n;\r\n}\r\nvoid copy_from_user_overflow(void)\r\n{\r\nWARN(1, "Buffer overflow detected!\n");\r\n}
