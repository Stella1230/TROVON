u32 mlx4_bitmap_alloc(struct mlx4_bitmap *bitmap)\r\n{\r\nu32 obj;\r\nspin_lock(&bitmap->lock);\r\nobj = find_next_zero_bit(bitmap->table, bitmap->max, bitmap->last);\r\nif (obj >= bitmap->max) {\r\nbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\r\n& bitmap->mask;\r\nobj = find_first_zero_bit(bitmap->table, bitmap->max);\r\n}\r\nif (obj < bitmap->max) {\r\nset_bit(obj, bitmap->table);\r\nbitmap->last = (obj + 1);\r\nif (bitmap->last == bitmap->max)\r\nbitmap->last = 0;\r\nobj |= bitmap->top;\r\n} else\r\nobj = -1;\r\nif (obj != -1)\r\n--bitmap->avail;\r\nspin_unlock(&bitmap->lock);\r\nreturn obj;\r\n}\r\nvoid mlx4_bitmap_free(struct mlx4_bitmap *bitmap, u32 obj)\r\n{\r\nmlx4_bitmap_free_range(bitmap, obj, 1);\r\n}\r\nu32 mlx4_bitmap_alloc_range(struct mlx4_bitmap *bitmap, int cnt, int align)\r\n{\r\nu32 obj;\r\nif (likely(cnt == 1 && align == 1))\r\nreturn mlx4_bitmap_alloc(bitmap);\r\nspin_lock(&bitmap->lock);\r\nobj = bitmap_find_next_zero_area(bitmap->table, bitmap->max,\r\nbitmap->last, cnt, align - 1);\r\nif (obj >= bitmap->max) {\r\nbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\r\n& bitmap->mask;\r\nobj = bitmap_find_next_zero_area(bitmap->table, bitmap->max,\r\n0, cnt, align - 1);\r\n}\r\nif (obj < bitmap->max) {\r\nbitmap_set(bitmap->table, obj, cnt);\r\nif (obj == bitmap->last) {\r\nbitmap->last = (obj + cnt);\r\nif (bitmap->last >= bitmap->max)\r\nbitmap->last = 0;\r\n}\r\nobj |= bitmap->top;\r\n} else\r\nobj = -1;\r\nif (obj != -1)\r\nbitmap->avail -= cnt;\r\nspin_unlock(&bitmap->lock);\r\nreturn obj;\r\n}\r\nu32 mlx4_bitmap_avail(struct mlx4_bitmap *bitmap)\r\n{\r\nreturn bitmap->avail;\r\n}\r\nvoid mlx4_bitmap_free_range(struct mlx4_bitmap *bitmap, u32 obj, int cnt)\r\n{\r\nobj &= bitmap->max + bitmap->reserved_top - 1;\r\nspin_lock(&bitmap->lock);\r\nbitmap_clear(bitmap->table, obj, cnt);\r\nbitmap->avail += cnt;\r\nspin_unlock(&bitmap->lock);\r\n}\r\nint mlx4_bitmap_init(struct mlx4_bitmap *bitmap, u32 num, u32 mask,\r\nu32 reserved_bot, u32 reserved_top)\r\n{\r\nif (num != roundup_pow_of_two(num))\r\nreturn -EINVAL;\r\nbitmap->last = 0;\r\nbitmap->top = 0;\r\nbitmap->max = num - reserved_top;\r\nbitmap->mask = mask;\r\nbitmap->reserved_top = reserved_top;\r\nbitmap->avail = num - reserved_top - reserved_bot;\r\nspin_lock_init(&bitmap->lock);\r\nbitmap->table = kzalloc(BITS_TO_LONGS(bitmap->max) *\r\nsizeof (long), GFP_KERNEL);\r\nif (!bitmap->table)\r\nreturn -ENOMEM;\r\nbitmap_set(bitmap->table, 0, reserved_bot);\r\nreturn 0;\r\n}\r\nvoid mlx4_bitmap_cleanup(struct mlx4_bitmap *bitmap)\r\n{\r\nkfree(bitmap->table);\r\n}\r\nint mlx4_buf_alloc(struct mlx4_dev *dev, int size, int max_direct,\r\nstruct mlx4_buf *buf)\r\n{\r\ndma_addr_t t;\r\nif (size <= max_direct) {\r\nbuf->nbufs = 1;\r\nbuf->npages = 1;\r\nbuf->page_shift = get_order(size) + PAGE_SHIFT;\r\nbuf->direct.buf = dma_alloc_coherent(&dev->pdev->dev,\r\nsize, &t, GFP_KERNEL);\r\nif (!buf->direct.buf)\r\nreturn -ENOMEM;\r\nbuf->direct.map = t;\r\nwhile (t & ((1 << buf->page_shift) - 1)) {\r\n--buf->page_shift;\r\nbuf->npages *= 2;\r\n}\r\nmemset(buf->direct.buf, 0, size);\r\n} else {\r\nint i;\r\nbuf->direct.buf = NULL;\r\nbuf->nbufs = (size + PAGE_SIZE - 1) / PAGE_SIZE;\r\nbuf->npages = buf->nbufs;\r\nbuf->page_shift = PAGE_SHIFT;\r\nbuf->page_list = kcalloc(buf->nbufs, sizeof(*buf->page_list),\r\nGFP_KERNEL);\r\nif (!buf->page_list)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < buf->nbufs; ++i) {\r\nbuf->page_list[i].buf =\r\ndma_alloc_coherent(&dev->pdev->dev, PAGE_SIZE,\r\n&t, GFP_KERNEL);\r\nif (!buf->page_list[i].buf)\r\ngoto err_free;\r\nbuf->page_list[i].map = t;\r\nmemset(buf->page_list[i].buf, 0, PAGE_SIZE);\r\n}\r\nif (BITS_PER_LONG == 64) {\r\nstruct page **pages;\r\npages = kmalloc(sizeof *pages * buf->nbufs, GFP_KERNEL);\r\nif (!pages)\r\ngoto err_free;\r\nfor (i = 0; i < buf->nbufs; ++i)\r\npages[i] = virt_to_page(buf->page_list[i].buf);\r\nbuf->direct.buf = vmap(pages, buf->nbufs, VM_MAP, PAGE_KERNEL);\r\nkfree(pages);\r\nif (!buf->direct.buf)\r\ngoto err_free;\r\n}\r\n}\r\nreturn 0;\r\nerr_free:\r\nmlx4_buf_free(dev, size, buf);\r\nreturn -ENOMEM;\r\n}\r\nvoid mlx4_buf_free(struct mlx4_dev *dev, int size, struct mlx4_buf *buf)\r\n{\r\nint i;\r\nif (buf->nbufs == 1)\r\ndma_free_coherent(&dev->pdev->dev, size, buf->direct.buf,\r\nbuf->direct.map);\r\nelse {\r\nif (BITS_PER_LONG == 64 && buf->direct.buf)\r\nvunmap(buf->direct.buf);\r\nfor (i = 0; i < buf->nbufs; ++i)\r\nif (buf->page_list[i].buf)\r\ndma_free_coherent(&dev->pdev->dev, PAGE_SIZE,\r\nbuf->page_list[i].buf,\r\nbuf->page_list[i].map);\r\nkfree(buf->page_list);\r\n}\r\n}\r\nstatic struct mlx4_db_pgdir *mlx4_alloc_db_pgdir(struct device *dma_device)\r\n{\r\nstruct mlx4_db_pgdir *pgdir;\r\npgdir = kzalloc(sizeof *pgdir, GFP_KERNEL);\r\nif (!pgdir)\r\nreturn NULL;\r\nbitmap_fill(pgdir->order1, MLX4_DB_PER_PAGE / 2);\r\npgdir->bits[0] = pgdir->order0;\r\npgdir->bits[1] = pgdir->order1;\r\npgdir->db_page = dma_alloc_coherent(dma_device, PAGE_SIZE,\r\n&pgdir->db_dma, GFP_KERNEL);\r\nif (!pgdir->db_page) {\r\nkfree(pgdir);\r\nreturn NULL;\r\n}\r\nreturn pgdir;\r\n}\r\nstatic int mlx4_alloc_db_from_pgdir(struct mlx4_db_pgdir *pgdir,\r\nstruct mlx4_db *db, int order)\r\n{\r\nint o;\r\nint i;\r\nfor (o = order; o <= 1; ++o) {\r\ni = find_first_bit(pgdir->bits[o], MLX4_DB_PER_PAGE >> o);\r\nif (i < MLX4_DB_PER_PAGE >> o)\r\ngoto found;\r\n}\r\nreturn -ENOMEM;\r\nfound:\r\nclear_bit(i, pgdir->bits[o]);\r\ni <<= o;\r\nif (o > order)\r\nset_bit(i ^ 1, pgdir->bits[order]);\r\ndb->u.pgdir = pgdir;\r\ndb->index = i;\r\ndb->db = pgdir->db_page + db->index;\r\ndb->dma = pgdir->db_dma + db->index * 4;\r\ndb->order = order;\r\nreturn 0;\r\n}\r\nint mlx4_db_alloc(struct mlx4_dev *dev, struct mlx4_db *db, int order)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_db_pgdir *pgdir;\r\nint ret = 0;\r\nmutex_lock(&priv->pgdir_mutex);\r\nlist_for_each_entry(pgdir, &priv->pgdir_list, list)\r\nif (!mlx4_alloc_db_from_pgdir(pgdir, db, order))\r\ngoto out;\r\npgdir = mlx4_alloc_db_pgdir(&(dev->pdev->dev));\r\nif (!pgdir) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nlist_add(&pgdir->list, &priv->pgdir_list);\r\nWARN_ON(mlx4_alloc_db_from_pgdir(pgdir, db, order));\r\nout:\r\nmutex_unlock(&priv->pgdir_mutex);\r\nreturn ret;\r\n}\r\nvoid mlx4_db_free(struct mlx4_dev *dev, struct mlx4_db *db)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint o;\r\nint i;\r\nmutex_lock(&priv->pgdir_mutex);\r\no = db->order;\r\ni = db->index;\r\nif (db->order == 0 && test_bit(i ^ 1, db->u.pgdir->order0)) {\r\nclear_bit(i ^ 1, db->u.pgdir->order0);\r\n++o;\r\n}\r\ni >>= o;\r\nset_bit(i, db->u.pgdir->bits[o]);\r\nif (bitmap_full(db->u.pgdir->order1, MLX4_DB_PER_PAGE / 2)) {\r\ndma_free_coherent(&(dev->pdev->dev), PAGE_SIZE,\r\ndb->u.pgdir->db_page, db->u.pgdir->db_dma);\r\nlist_del(&db->u.pgdir->list);\r\nkfree(db->u.pgdir);\r\n}\r\nmutex_unlock(&priv->pgdir_mutex);\r\n}\r\nint mlx4_alloc_hwq_res(struct mlx4_dev *dev, struct mlx4_hwq_resources *wqres,\r\nint size, int max_direct)\r\n{\r\nint err;\r\nerr = mlx4_db_alloc(dev, &wqres->db, 1);\r\nif (err)\r\nreturn err;\r\n*wqres->db.db = 0;\r\nerr = mlx4_buf_alloc(dev, size, max_direct, &wqres->buf);\r\nif (err)\r\ngoto err_db;\r\nerr = mlx4_mtt_init(dev, wqres->buf.npages, wqres->buf.page_shift,\r\n&wqres->mtt);\r\nif (err)\r\ngoto err_buf;\r\nerr = mlx4_buf_write_mtt(dev, &wqres->mtt, &wqres->buf);\r\nif (err)\r\ngoto err_mtt;\r\nreturn 0;\r\nerr_mtt:\r\nmlx4_mtt_cleanup(dev, &wqres->mtt);\r\nerr_buf:\r\nmlx4_buf_free(dev, size, &wqres->buf);\r\nerr_db:\r\nmlx4_db_free(dev, &wqres->db);\r\nreturn err;\r\n}\r\nvoid mlx4_free_hwq_res(struct mlx4_dev *dev, struct mlx4_hwq_resources *wqres,\r\nint size)\r\n{\r\nmlx4_mtt_cleanup(dev, &wqres->mtt);\r\nmlx4_buf_free(dev, size, &wqres->buf);\r\nmlx4_db_free(dev, &wqres->db);\r\n}
