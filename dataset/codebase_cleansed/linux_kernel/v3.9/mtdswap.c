static loff_t mtdswap_eb_offset(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nreturn (loff_t)(eb - d->eb_data) * d->mtd->erasesize;\r\n}\r\nstatic void mtdswap_eb_detach(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nunsigned int oldidx;\r\nstruct mtdswap_tree *tp;\r\nif (eb->root) {\r\ntp = container_of(eb->root, struct mtdswap_tree, root);\r\noldidx = tp - &d->trees[0];\r\nd->trees[oldidx].count--;\r\nrb_erase(&eb->rb, eb->root);\r\n}\r\n}\r\nstatic void __mtdswap_rb_add(struct rb_root *root, struct swap_eb *eb)\r\n{\r\nstruct rb_node **p, *parent = NULL;\r\nstruct swap_eb *cur;\r\np = &root->rb_node;\r\nwhile (*p) {\r\nparent = *p;\r\ncur = rb_entry(parent, struct swap_eb, rb);\r\nif (eb->erase_count > cur->erase_count)\r\np = &(*p)->rb_right;\r\nelse\r\np = &(*p)->rb_left;\r\n}\r\nrb_link_node(&eb->rb, parent, p);\r\nrb_insert_color(&eb->rb, root);\r\n}\r\nstatic void mtdswap_rb_add(struct mtdswap_dev *d, struct swap_eb *eb, int idx)\r\n{\r\nstruct rb_root *root;\r\nif (eb->root == &d->trees[idx].root)\r\nreturn;\r\nmtdswap_eb_detach(d, eb);\r\nroot = &d->trees[idx].root;\r\n__mtdswap_rb_add(root, eb);\r\neb->root = root;\r\nd->trees[idx].count++;\r\n}\r\nstatic struct rb_node *mtdswap_rb_index(struct rb_root *root, unsigned int idx)\r\n{\r\nstruct rb_node *p;\r\nunsigned int i;\r\np = rb_first(root);\r\ni = 0;\r\nwhile (i < idx && p) {\r\np = rb_next(p);\r\ni++;\r\n}\r\nreturn p;\r\n}\r\nstatic int mtdswap_handle_badblock(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nint ret;\r\nloff_t offset;\r\nd->spare_eblks--;\r\neb->flags |= EBLOCK_BAD;\r\nmtdswap_eb_detach(d, eb);\r\neb->root = NULL;\r\nif (!mtd_can_have_bb(d->mtd))\r\nreturn 1;\r\noffset = mtdswap_eb_offset(d, eb);\r\ndev_warn(d->dev, "Marking bad block at %08llx\n", offset);\r\nret = mtd_block_markbad(d->mtd, offset);\r\nif (ret) {\r\ndev_warn(d->dev, "Mark block bad failed for block at %08llx "\r\n"error %d\n", offset, ret);\r\nreturn ret;\r\n}\r\nreturn 1;\r\n}\r\nstatic int mtdswap_handle_write_error(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nunsigned int marked = eb->flags & EBLOCK_FAILED;\r\nstruct swap_eb *curr_write = d->curr_write;\r\neb->flags |= EBLOCK_FAILED;\r\nif (curr_write == eb) {\r\nd->curr_write = NULL;\r\nif (!marked && d->curr_write_pos != 0) {\r\nmtdswap_rb_add(d, eb, MTDSWAP_FAILING);\r\nreturn 0;\r\n}\r\n}\r\nreturn mtdswap_handle_badblock(d, eb);\r\n}\r\nstatic int mtdswap_read_oob(struct mtdswap_dev *d, loff_t from,\r\nstruct mtd_oob_ops *ops)\r\n{\r\nint ret = mtd_read_oob(d->mtd, from, ops);\r\nif (mtd_is_bitflip(ret))\r\nreturn ret;\r\nif (ret) {\r\ndev_warn(d->dev, "Read OOB failed %d for block at %08llx\n",\r\nret, from);\r\nreturn ret;\r\n}\r\nif (ops->oobretlen < ops->ooblen) {\r\ndev_warn(d->dev, "Read OOB return short read (%zd bytes not "\r\n"%zd) for block at %08llx\n",\r\nops->oobretlen, ops->ooblen, from);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtdswap_read_markers(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nstruct mtdswap_oobdata *data, *data2;\r\nint ret;\r\nloff_t offset;\r\nstruct mtd_oob_ops ops;\r\noffset = mtdswap_eb_offset(d, eb);\r\nif (mtd_can_have_bb(d->mtd) && mtd_block_isbad(d->mtd, offset))\r\nreturn MTDSWAP_SCANNED_BAD;\r\nops.ooblen = 2 * d->mtd->ecclayout->oobavail;\r\nops.oobbuf = d->oob_buf;\r\nops.ooboffs = 0;\r\nops.datbuf = NULL;\r\nops.mode = MTD_OPS_AUTO_OOB;\r\nret = mtdswap_read_oob(d, offset, &ops);\r\nif (ret && !mtd_is_bitflip(ret))\r\nreturn ret;\r\ndata = (struct mtdswap_oobdata *)d->oob_buf;\r\ndata2 = (struct mtdswap_oobdata *)\r\n(d->oob_buf + d->mtd->ecclayout->oobavail);\r\nif (le16_to_cpu(data->magic) == MTDSWAP_MAGIC_CLEAN) {\r\neb->erase_count = le32_to_cpu(data->count);\r\nif (mtd_is_bitflip(ret))\r\nret = MTDSWAP_SCANNED_BITFLIP;\r\nelse {\r\nif (le16_to_cpu(data2->magic) == MTDSWAP_MAGIC_DIRTY)\r\nret = MTDSWAP_SCANNED_DIRTY;\r\nelse\r\nret = MTDSWAP_SCANNED_CLEAN;\r\n}\r\n} else {\r\neb->flags |= EBLOCK_NOMAGIC;\r\nret = MTDSWAP_SCANNED_DIRTY;\r\n}\r\nreturn ret;\r\n}\r\nstatic int mtdswap_write_marker(struct mtdswap_dev *d, struct swap_eb *eb,\r\nu16 marker)\r\n{\r\nstruct mtdswap_oobdata n;\r\nint ret;\r\nloff_t offset;\r\nstruct mtd_oob_ops ops;\r\nops.ooboffs = 0;\r\nops.oobbuf = (uint8_t *)&n;\r\nops.mode = MTD_OPS_AUTO_OOB;\r\nops.datbuf = NULL;\r\nif (marker == MTDSWAP_TYPE_CLEAN) {\r\nn.magic = cpu_to_le16(MTDSWAP_MAGIC_CLEAN);\r\nn.count = cpu_to_le32(eb->erase_count);\r\nops.ooblen = MTDSWAP_OOBSIZE;\r\noffset = mtdswap_eb_offset(d, eb);\r\n} else {\r\nn.magic = cpu_to_le16(MTDSWAP_MAGIC_DIRTY);\r\nops.ooblen = sizeof(n.magic);\r\noffset = mtdswap_eb_offset(d, eb) + d->mtd->writesize;\r\n}\r\nret = mtd_write_oob(d->mtd, offset, &ops);\r\nif (ret) {\r\ndev_warn(d->dev, "Write OOB failed for block at %08llx "\r\n"error %d\n", offset, ret);\r\nif (ret == -EIO || mtd_is_eccerr(ret))\r\nmtdswap_handle_write_error(d, eb);\r\nreturn ret;\r\n}\r\nif (ops.oobretlen != ops.ooblen) {\r\ndev_warn(d->dev, "Short OOB write for block at %08llx: "\r\n"%zd not %zd\n",\r\noffset, ops.oobretlen, ops.ooblen);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mtdswap_check_counts(struct mtdswap_dev *d)\r\n{\r\nstruct rb_root hist_root = RB_ROOT;\r\nstruct rb_node *medrb;\r\nstruct swap_eb *eb;\r\nunsigned int i, cnt, median;\r\ncnt = 0;\r\nfor (i = 0; i < d->eblks; i++) {\r\neb = d->eb_data + i;\r\nif (eb->flags & (EBLOCK_NOMAGIC | EBLOCK_BAD | EBLOCK_READERR))\r\ncontinue;\r\n__mtdswap_rb_add(&hist_root, eb);\r\ncnt++;\r\n}\r\nif (cnt == 0)\r\nreturn;\r\nmedrb = mtdswap_rb_index(&hist_root, cnt / 2);\r\nmedian = rb_entry(medrb, struct swap_eb, rb)->erase_count;\r\nd->max_erase_count = MTDSWAP_ECNT_MAX(&hist_root);\r\nfor (i = 0; i < d->eblks; i++) {\r\neb = d->eb_data + i;\r\nif (eb->flags & (EBLOCK_NOMAGIC | EBLOCK_READERR))\r\neb->erase_count = median;\r\nif (eb->flags & (EBLOCK_NOMAGIC | EBLOCK_BAD | EBLOCK_READERR))\r\ncontinue;\r\nrb_erase(&eb->rb, &hist_root);\r\n}\r\n}\r\nstatic void mtdswap_scan_eblks(struct mtdswap_dev *d)\r\n{\r\nint status;\r\nunsigned int i, idx;\r\nstruct swap_eb *eb;\r\nfor (i = 0; i < d->eblks; i++) {\r\neb = d->eb_data + i;\r\nstatus = mtdswap_read_markers(d, eb);\r\nif (status < 0)\r\neb->flags |= EBLOCK_READERR;\r\nelse if (status == MTDSWAP_SCANNED_BAD) {\r\neb->flags |= EBLOCK_BAD;\r\ncontinue;\r\n}\r\nswitch (status) {\r\ncase MTDSWAP_SCANNED_CLEAN:\r\nidx = MTDSWAP_CLEAN;\r\nbreak;\r\ncase MTDSWAP_SCANNED_DIRTY:\r\ncase MTDSWAP_SCANNED_BITFLIP:\r\nidx = MTDSWAP_DIRTY;\r\nbreak;\r\ndefault:\r\nidx = MTDSWAP_FAILING;\r\n}\r\neb->flags |= (idx << EBLOCK_IDX_SHIFT);\r\n}\r\nmtdswap_check_counts(d);\r\nfor (i = 0; i < d->eblks; i++) {\r\neb = d->eb_data + i;\r\nif (eb->flags & EBLOCK_BAD)\r\ncontinue;\r\nidx = eb->flags >> EBLOCK_IDX_SHIFT;\r\nmtdswap_rb_add(d, eb, idx);\r\n}\r\n}\r\nstatic void mtdswap_store_eb(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nunsigned int weight = eb->active_count;\r\nunsigned int maxweight = d->pages_per_eblk;\r\nif (eb == d->curr_write)\r\nreturn;\r\nif (eb->flags & EBLOCK_BITFLIP)\r\nmtdswap_rb_add(d, eb, MTDSWAP_BITFLIP);\r\nelse if (eb->flags & (EBLOCK_READERR | EBLOCK_FAILED))\r\nmtdswap_rb_add(d, eb, MTDSWAP_FAILING);\r\nif (weight == maxweight)\r\nmtdswap_rb_add(d, eb, MTDSWAP_USED);\r\nelse if (weight == 0)\r\nmtdswap_rb_add(d, eb, MTDSWAP_DIRTY);\r\nelse if (weight > (maxweight/2))\r\nmtdswap_rb_add(d, eb, MTDSWAP_LOWFRAG);\r\nelse\r\nmtdswap_rb_add(d, eb, MTDSWAP_HIFRAG);\r\n}\r\nstatic void mtdswap_erase_callback(struct erase_info *done)\r\n{\r\nwait_queue_head_t *wait_q = (wait_queue_head_t *)done->priv;\r\nwake_up(wait_q);\r\n}\r\nstatic int mtdswap_erase_block(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nstruct mtd_info *mtd = d->mtd;\r\nstruct erase_info erase;\r\nwait_queue_head_t wq;\r\nunsigned int retries = 0;\r\nint ret;\r\neb->erase_count++;\r\nif (eb->erase_count > d->max_erase_count)\r\nd->max_erase_count = eb->erase_count;\r\nretry:\r\ninit_waitqueue_head(&wq);\r\nmemset(&erase, 0, sizeof(struct erase_info));\r\nerase.mtd = mtd;\r\nerase.callback = mtdswap_erase_callback;\r\nerase.addr = mtdswap_eb_offset(d, eb);\r\nerase.len = mtd->erasesize;\r\nerase.priv = (u_long)&wq;\r\nret = mtd_erase(mtd, &erase);\r\nif (ret) {\r\nif (retries++ < MTDSWAP_ERASE_RETRIES) {\r\ndev_warn(d->dev,\r\n"erase of erase block %#llx on %s failed",\r\nerase.addr, mtd->name);\r\nyield();\r\ngoto retry;\r\n}\r\ndev_err(d->dev, "Cannot erase erase block %#llx on %s\n",\r\nerase.addr, mtd->name);\r\nmtdswap_handle_badblock(d, eb);\r\nreturn -EIO;\r\n}\r\nret = wait_event_interruptible(wq, erase.state == MTD_ERASE_DONE ||\r\nerase.state == MTD_ERASE_FAILED);\r\nif (ret) {\r\ndev_err(d->dev, "Interrupted erase block %#llx erassure on %s",\r\nerase.addr, mtd->name);\r\nreturn -EINTR;\r\n}\r\nif (erase.state == MTD_ERASE_FAILED) {\r\nif (retries++ < MTDSWAP_ERASE_RETRIES) {\r\ndev_warn(d->dev,\r\n"erase of erase block %#llx on %s failed",\r\nerase.addr, mtd->name);\r\nyield();\r\ngoto retry;\r\n}\r\nmtdswap_handle_badblock(d, eb);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtdswap_map_free_block(struct mtdswap_dev *d, unsigned int page,\r\nunsigned int *block)\r\n{\r\nint ret;\r\nstruct swap_eb *old_eb = d->curr_write;\r\nstruct rb_root *clean_root;\r\nstruct swap_eb *eb;\r\nif (old_eb == NULL || d->curr_write_pos >= d->pages_per_eblk) {\r\ndo {\r\nif (TREE_EMPTY(d, CLEAN))\r\nreturn -ENOSPC;\r\nclean_root = TREE_ROOT(d, CLEAN);\r\neb = rb_entry(rb_first(clean_root), struct swap_eb, rb);\r\nrb_erase(&eb->rb, clean_root);\r\neb->root = NULL;\r\nTREE_COUNT(d, CLEAN)--;\r\nret = mtdswap_write_marker(d, eb, MTDSWAP_TYPE_DIRTY);\r\n} while (ret == -EIO || mtd_is_eccerr(ret));\r\nif (ret)\r\nreturn ret;\r\nd->curr_write_pos = 0;\r\nd->curr_write = eb;\r\nif (old_eb)\r\nmtdswap_store_eb(d, old_eb);\r\n}\r\n*block = (d->curr_write - d->eb_data) * d->pages_per_eblk +\r\nd->curr_write_pos;\r\nd->curr_write->active_count++;\r\nd->revmap[*block] = page;\r\nd->curr_write_pos++;\r\nreturn 0;\r\n}\r\nstatic unsigned int mtdswap_free_page_cnt(struct mtdswap_dev *d)\r\n{\r\nreturn TREE_COUNT(d, CLEAN) * d->pages_per_eblk +\r\nd->pages_per_eblk - d->curr_write_pos;\r\n}\r\nstatic unsigned int mtdswap_enough_free_pages(struct mtdswap_dev *d)\r\n{\r\nreturn mtdswap_free_page_cnt(d) > d->pages_per_eblk;\r\n}\r\nstatic int mtdswap_write_block(struct mtdswap_dev *d, char *buf,\r\nunsigned int page, unsigned int *bp, int gc_context)\r\n{\r\nstruct mtd_info *mtd = d->mtd;\r\nstruct swap_eb *eb;\r\nsize_t retlen;\r\nloff_t writepos;\r\nint ret;\r\nretry:\r\nif (!gc_context)\r\nwhile (!mtdswap_enough_free_pages(d))\r\nif (mtdswap_gc(d, 0) > 0)\r\nreturn -ENOSPC;\r\nret = mtdswap_map_free_block(d, page, bp);\r\neb = d->eb_data + (*bp / d->pages_per_eblk);\r\nif (ret == -EIO || mtd_is_eccerr(ret)) {\r\nd->curr_write = NULL;\r\neb->active_count--;\r\nd->revmap[*bp] = PAGE_UNDEF;\r\ngoto retry;\r\n}\r\nif (ret < 0)\r\nreturn ret;\r\nwritepos = (loff_t)*bp << PAGE_SHIFT;\r\nret = mtd_write(mtd, writepos, PAGE_SIZE, &retlen, buf);\r\nif (ret == -EIO || mtd_is_eccerr(ret)) {\r\nd->curr_write_pos--;\r\neb->active_count--;\r\nd->revmap[*bp] = PAGE_UNDEF;\r\nmtdswap_handle_write_error(d, eb);\r\ngoto retry;\r\n}\r\nif (ret < 0) {\r\ndev_err(d->dev, "Write to MTD device failed: %d (%zd written)",\r\nret, retlen);\r\ngoto err;\r\n}\r\nif (retlen != PAGE_SIZE) {\r\ndev_err(d->dev, "Short write to MTD device: %zd written",\r\nretlen);\r\nret = -EIO;\r\ngoto err;\r\n}\r\nreturn ret;\r\nerr:\r\nd->curr_write_pos--;\r\neb->active_count--;\r\nd->revmap[*bp] = PAGE_UNDEF;\r\nreturn ret;\r\n}\r\nstatic int mtdswap_move_block(struct mtdswap_dev *d, unsigned int oldblock,\r\nunsigned int *newblock)\r\n{\r\nstruct mtd_info *mtd = d->mtd;\r\nstruct swap_eb *eb, *oldeb;\r\nint ret;\r\nsize_t retlen;\r\nunsigned int page, retries;\r\nloff_t readpos;\r\npage = d->revmap[oldblock];\r\nreadpos = (loff_t) oldblock << PAGE_SHIFT;\r\nretries = 0;\r\nretry:\r\nret = mtd_read(mtd, readpos, PAGE_SIZE, &retlen, d->page_buf);\r\nif (ret < 0 && !mtd_is_bitflip(ret)) {\r\noldeb = d->eb_data + oldblock / d->pages_per_eblk;\r\noldeb->flags |= EBLOCK_READERR;\r\ndev_err(d->dev, "Read Error: %d (block %u)\n", ret,\r\noldblock);\r\nretries++;\r\nif (retries < MTDSWAP_IO_RETRIES)\r\ngoto retry;\r\ngoto read_error;\r\n}\r\nif (retlen != PAGE_SIZE) {\r\ndev_err(d->dev, "Short read: %zd (block %u)\n", retlen,\r\noldblock);\r\nret = -EIO;\r\ngoto read_error;\r\n}\r\nret = mtdswap_write_block(d, d->page_buf, page, newblock, 1);\r\nif (ret < 0) {\r\nd->page_data[page] = BLOCK_ERROR;\r\ndev_err(d->dev, "Write error: %d\n", ret);\r\nreturn ret;\r\n}\r\neb = d->eb_data + *newblock / d->pages_per_eblk;\r\nd->page_data[page] = *newblock;\r\nd->revmap[oldblock] = PAGE_UNDEF;\r\neb = d->eb_data + oldblock / d->pages_per_eblk;\r\neb->active_count--;\r\nreturn 0;\r\nread_error:\r\nd->page_data[page] = BLOCK_ERROR;\r\nd->revmap[oldblock] = PAGE_UNDEF;\r\nreturn ret;\r\n}\r\nstatic int mtdswap_gc_eblock(struct mtdswap_dev *d, struct swap_eb *eb)\r\n{\r\nunsigned int i, block, eblk_base, newblock;\r\nint ret, errcode;\r\nerrcode = 0;\r\neblk_base = (eb - d->eb_data) * d->pages_per_eblk;\r\nfor (i = 0; i < d->pages_per_eblk; i++) {\r\nif (d->spare_eblks < MIN_SPARE_EBLOCKS)\r\nreturn -ENOSPC;\r\nblock = eblk_base + i;\r\nif (d->revmap[block] == PAGE_UNDEF)\r\ncontinue;\r\nret = mtdswap_move_block(d, block, &newblock);\r\nif (ret < 0 && !errcode)\r\nerrcode = ret;\r\n}\r\nreturn errcode;\r\n}\r\nstatic int __mtdswap_choose_gc_tree(struct mtdswap_dev *d)\r\n{\r\nint idx, stopat;\r\nif (TREE_COUNT(d, CLEAN) < LOW_FRAG_GC_TRESHOLD)\r\nstopat = MTDSWAP_LOWFRAG;\r\nelse\r\nstopat = MTDSWAP_HIFRAG;\r\nfor (idx = MTDSWAP_BITFLIP; idx >= stopat; idx--)\r\nif (d->trees[idx].root.rb_node != NULL)\r\nreturn idx;\r\nreturn -1;\r\n}\r\nstatic int mtdswap_wlfreq(unsigned int maxdiff)\r\n{\r\nunsigned int h, x, y, dist, base;\r\ndist = maxdiff - MAX_ERASE_DIFF;\r\nif (dist > COLLECT_NONDIRTY_BASE)\r\ndist = COLLECT_NONDIRTY_BASE;\r\nh = COLLECT_NONDIRTY_FREQ1 - COLLECT_NONDIRTY_FREQ2;\r\nbase = COLLECT_NONDIRTY_BASE;\r\nx = dist - base;\r\ny = (x * h + base / 2) / base;\r\nreturn COLLECT_NONDIRTY_FREQ2 + y;\r\n}\r\nstatic int mtdswap_choose_wl_tree(struct mtdswap_dev *d)\r\n{\r\nstatic unsigned int pick_cnt;\r\nunsigned int i, idx = -1, wear, max;\r\nstruct rb_root *root;\r\nmax = 0;\r\nfor (i = 0; i <= MTDSWAP_DIRTY; i++) {\r\nroot = &d->trees[i].root;\r\nif (root->rb_node == NULL)\r\ncontinue;\r\nwear = d->max_erase_count - MTDSWAP_ECNT_MIN(root);\r\nif (wear > max) {\r\nmax = wear;\r\nidx = i;\r\n}\r\n}\r\nif (max > MAX_ERASE_DIFF && pick_cnt >= mtdswap_wlfreq(max) - 1) {\r\npick_cnt = 0;\r\nreturn idx;\r\n}\r\npick_cnt++;\r\nreturn -1;\r\n}\r\nstatic int mtdswap_choose_gc_tree(struct mtdswap_dev *d,\r\nunsigned int background)\r\n{\r\nint idx;\r\nif (TREE_NONEMPTY(d, FAILING) &&\r\n(background || (TREE_EMPTY(d, CLEAN) && TREE_EMPTY(d, DIRTY))))\r\nreturn MTDSWAP_FAILING;\r\nidx = mtdswap_choose_wl_tree(d);\r\nif (idx >= MTDSWAP_CLEAN)\r\nreturn idx;\r\nreturn __mtdswap_choose_gc_tree(d);\r\n}\r\nstatic struct swap_eb *mtdswap_pick_gc_eblk(struct mtdswap_dev *d,\r\nunsigned int background)\r\n{\r\nstruct rb_root *rp = NULL;\r\nstruct swap_eb *eb = NULL;\r\nint idx;\r\nif (background && TREE_COUNT(d, CLEAN) > CLEAN_BLOCK_THRESHOLD &&\r\nTREE_EMPTY(d, DIRTY) && TREE_EMPTY(d, FAILING))\r\nreturn NULL;\r\nidx = mtdswap_choose_gc_tree(d, background);\r\nif (idx < 0)\r\nreturn NULL;\r\nrp = &d->trees[idx].root;\r\neb = rb_entry(rb_first(rp), struct swap_eb, rb);\r\nrb_erase(&eb->rb, rp);\r\neb->root = NULL;\r\nd->trees[idx].count--;\r\nreturn eb;\r\n}\r\nstatic unsigned int mtdswap_test_patt(unsigned int i)\r\n{\r\nreturn i % 2 ? 0x55555555 : 0xAAAAAAAA;\r\n}\r\nstatic unsigned int mtdswap_eblk_passes(struct mtdswap_dev *d,\r\nstruct swap_eb *eb)\r\n{\r\nstruct mtd_info *mtd = d->mtd;\r\nunsigned int test, i, j, patt, mtd_pages;\r\nloff_t base, pos;\r\nunsigned int *p1 = (unsigned int *)d->page_buf;\r\nunsigned char *p2 = (unsigned char *)d->oob_buf;\r\nstruct mtd_oob_ops ops;\r\nint ret;\r\nops.mode = MTD_OPS_AUTO_OOB;\r\nops.len = mtd->writesize;\r\nops.ooblen = mtd->ecclayout->oobavail;\r\nops.ooboffs = 0;\r\nops.datbuf = d->page_buf;\r\nops.oobbuf = d->oob_buf;\r\nbase = mtdswap_eb_offset(d, eb);\r\nmtd_pages = d->pages_per_eblk * PAGE_SIZE / mtd->writesize;\r\nfor (test = 0; test < 2; test++) {\r\npos = base;\r\nfor (i = 0; i < mtd_pages; i++) {\r\npatt = mtdswap_test_patt(test + i);\r\nmemset(d->page_buf, patt, mtd->writesize);\r\nmemset(d->oob_buf, patt, mtd->ecclayout->oobavail);\r\nret = mtd_write_oob(mtd, pos, &ops);\r\nif (ret)\r\ngoto error;\r\npos += mtd->writesize;\r\n}\r\npos = base;\r\nfor (i = 0; i < mtd_pages; i++) {\r\nret = mtd_read_oob(mtd, pos, &ops);\r\nif (ret)\r\ngoto error;\r\npatt = mtdswap_test_patt(test + i);\r\nfor (j = 0; j < mtd->writesize/sizeof(int); j++)\r\nif (p1[j] != patt)\r\ngoto error;\r\nfor (j = 0; j < mtd->ecclayout->oobavail; j++)\r\nif (p2[j] != (unsigned char)patt)\r\ngoto error;\r\npos += mtd->writesize;\r\n}\r\nret = mtdswap_erase_block(d, eb);\r\nif (ret)\r\ngoto error;\r\n}\r\neb->flags &= ~EBLOCK_READERR;\r\nreturn 1;\r\nerror:\r\nmtdswap_handle_badblock(d, eb);\r\nreturn 0;\r\n}\r\nstatic int mtdswap_gc(struct mtdswap_dev *d, unsigned int background)\r\n{\r\nstruct swap_eb *eb;\r\nint ret;\r\nif (d->spare_eblks < MIN_SPARE_EBLOCKS)\r\nreturn 1;\r\neb = mtdswap_pick_gc_eblk(d, background);\r\nif (!eb)\r\nreturn 1;\r\nret = mtdswap_gc_eblock(d, eb);\r\nif (ret == -ENOSPC)\r\nreturn 1;\r\nif (eb->flags & EBLOCK_FAILED) {\r\nmtdswap_handle_badblock(d, eb);\r\nreturn 0;\r\n}\r\neb->flags &= ~EBLOCK_BITFLIP;\r\nret = mtdswap_erase_block(d, eb);\r\nif ((eb->flags & EBLOCK_READERR) &&\r\n(ret || !mtdswap_eblk_passes(d, eb)))\r\nreturn 0;\r\nif (ret == 0)\r\nret = mtdswap_write_marker(d, eb, MTDSWAP_TYPE_CLEAN);\r\nif (ret == 0)\r\nmtdswap_rb_add(d, eb, MTDSWAP_CLEAN);\r\nelse if (ret != -EIO && !mtd_is_eccerr(ret))\r\nmtdswap_rb_add(d, eb, MTDSWAP_DIRTY);\r\nreturn 0;\r\n}\r\nstatic void mtdswap_background(struct mtd_blktrans_dev *dev)\r\n{\r\nstruct mtdswap_dev *d = MTDSWAP_MBD_TO_MTDSWAP(dev);\r\nint ret;\r\nwhile (1) {\r\nret = mtdswap_gc(d, 1);\r\nif (ret || mtd_blktrans_cease_background(dev))\r\nreturn;\r\n}\r\n}\r\nstatic void mtdswap_cleanup(struct mtdswap_dev *d)\r\n{\r\nvfree(d->eb_data);\r\nvfree(d->revmap);\r\nvfree(d->page_data);\r\nkfree(d->oob_buf);\r\nkfree(d->page_buf);\r\n}\r\nstatic int mtdswap_flush(struct mtd_blktrans_dev *dev)\r\n{\r\nstruct mtdswap_dev *d = MTDSWAP_MBD_TO_MTDSWAP(dev);\r\nmtd_sync(d->mtd);\r\nreturn 0;\r\n}\r\nstatic unsigned int mtdswap_badblocks(struct mtd_info *mtd, uint64_t size)\r\n{\r\nloff_t offset;\r\nunsigned int badcnt;\r\nbadcnt = 0;\r\nif (mtd_can_have_bb(mtd))\r\nfor (offset = 0; offset < size; offset += mtd->erasesize)\r\nif (mtd_block_isbad(mtd, offset))\r\nbadcnt++;\r\nreturn badcnt;\r\n}\r\nstatic int mtdswap_writesect(struct mtd_blktrans_dev *dev,\r\nunsigned long page, char *buf)\r\n{\r\nstruct mtdswap_dev *d = MTDSWAP_MBD_TO_MTDSWAP(dev);\r\nunsigned int newblock, mapped;\r\nstruct swap_eb *eb;\r\nint ret;\r\nd->sect_write_count++;\r\nif (d->spare_eblks < MIN_SPARE_EBLOCKS)\r\nreturn -ENOSPC;\r\nif (header) {\r\nif (unlikely(page == 0))\r\nreturn 0;\r\npage--;\r\n}\r\nmapped = d->page_data[page];\r\nif (mapped <= BLOCK_MAX) {\r\neb = d->eb_data + (mapped / d->pages_per_eblk);\r\neb->active_count--;\r\nmtdswap_store_eb(d, eb);\r\nd->page_data[page] = BLOCK_UNDEF;\r\nd->revmap[mapped] = PAGE_UNDEF;\r\n}\r\nret = mtdswap_write_block(d, buf, page, &newblock, 0);\r\nd->mtd_write_count++;\r\nif (ret < 0)\r\nreturn ret;\r\neb = d->eb_data + (newblock / d->pages_per_eblk);\r\nd->page_data[page] = newblock;\r\nreturn 0;\r\n}\r\nstatic int mtdswap_auto_header(struct mtdswap_dev *d, char *buf)\r\n{\r\nunion swap_header *hd = (union swap_header *)(buf);\r\nmemset(buf, 0, PAGE_SIZE - 10);\r\nhd->info.version = 1;\r\nhd->info.last_page = d->mbd_dev->size - 1;\r\nhd->info.nr_badpages = 0;\r\nmemcpy(buf + PAGE_SIZE - 10, "SWAPSPACE2", 10);\r\nreturn 0;\r\n}\r\nstatic int mtdswap_readsect(struct mtd_blktrans_dev *dev,\r\nunsigned long page, char *buf)\r\n{\r\nstruct mtdswap_dev *d = MTDSWAP_MBD_TO_MTDSWAP(dev);\r\nstruct mtd_info *mtd = d->mtd;\r\nunsigned int realblock, retries;\r\nloff_t readpos;\r\nstruct swap_eb *eb;\r\nsize_t retlen;\r\nint ret;\r\nd->sect_read_count++;\r\nif (header) {\r\nif (unlikely(page == 0))\r\nreturn mtdswap_auto_header(d, buf);\r\npage--;\r\n}\r\nrealblock = d->page_data[page];\r\nif (realblock > BLOCK_MAX) {\r\nmemset(buf, 0x0, PAGE_SIZE);\r\nif (realblock == BLOCK_UNDEF)\r\nreturn 0;\r\nelse\r\nreturn -EIO;\r\n}\r\neb = d->eb_data + (realblock / d->pages_per_eblk);\r\nBUG_ON(d->revmap[realblock] == PAGE_UNDEF);\r\nreadpos = (loff_t)realblock << PAGE_SHIFT;\r\nretries = 0;\r\nretry:\r\nret = mtd_read(mtd, readpos, PAGE_SIZE, &retlen, buf);\r\nd->mtd_read_count++;\r\nif (mtd_is_bitflip(ret)) {\r\neb->flags |= EBLOCK_BITFLIP;\r\nmtdswap_rb_add(d, eb, MTDSWAP_BITFLIP);\r\nret = 0;\r\n}\r\nif (ret < 0) {\r\ndev_err(d->dev, "Read error %d\n", ret);\r\neb->flags |= EBLOCK_READERR;\r\nmtdswap_rb_add(d, eb, MTDSWAP_FAILING);\r\nretries++;\r\nif (retries < MTDSWAP_IO_RETRIES)\r\ngoto retry;\r\nreturn ret;\r\n}\r\nif (retlen != PAGE_SIZE) {\r\ndev_err(d->dev, "Short read %zd\n", retlen);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtdswap_discard(struct mtd_blktrans_dev *dev, unsigned long first,\r\nunsigned nr_pages)\r\n{\r\nstruct mtdswap_dev *d = MTDSWAP_MBD_TO_MTDSWAP(dev);\r\nunsigned long page;\r\nstruct swap_eb *eb;\r\nunsigned int mapped;\r\nd->discard_count++;\r\nfor (page = first; page < first + nr_pages; page++) {\r\nmapped = d->page_data[page];\r\nif (mapped <= BLOCK_MAX) {\r\neb = d->eb_data + (mapped / d->pages_per_eblk);\r\neb->active_count--;\r\nmtdswap_store_eb(d, eb);\r\nd->page_data[page] = BLOCK_UNDEF;\r\nd->revmap[mapped] = PAGE_UNDEF;\r\nd->discard_page_count++;\r\n} else if (mapped == BLOCK_ERROR) {\r\nd->page_data[page] = BLOCK_UNDEF;\r\nd->discard_page_count++;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtdswap_show(struct seq_file *s, void *data)\r\n{\r\nstruct mtdswap_dev *d = (struct mtdswap_dev *) s->private;\r\nunsigned long sum;\r\nunsigned int count[MTDSWAP_TREE_CNT];\r\nunsigned int min[MTDSWAP_TREE_CNT];\r\nunsigned int max[MTDSWAP_TREE_CNT];\r\nunsigned int i, cw = 0, cwp = 0, cwecount = 0, bb_cnt, mapped, pages;\r\nuint64_t use_size;\r\nchar *name[] = {"clean", "used", "low", "high", "dirty", "bitflip",\r\n"failing"};\r\nmutex_lock(&d->mbd_dev->lock);\r\nfor (i = 0; i < MTDSWAP_TREE_CNT; i++) {\r\nstruct rb_root *root = &d->trees[i].root;\r\nif (root->rb_node) {\r\ncount[i] = d->trees[i].count;\r\nmin[i] = rb_entry(rb_first(root), struct swap_eb,\r\nrb)->erase_count;\r\nmax[i] = rb_entry(rb_last(root), struct swap_eb,\r\nrb)->erase_count;\r\n} else\r\ncount[i] = 0;\r\n}\r\nif (d->curr_write) {\r\ncw = 1;\r\ncwp = d->curr_write_pos;\r\ncwecount = d->curr_write->erase_count;\r\n}\r\nsum = 0;\r\nfor (i = 0; i < d->eblks; i++)\r\nsum += d->eb_data[i].erase_count;\r\nuse_size = (uint64_t)d->eblks * d->mtd->erasesize;\r\nbb_cnt = mtdswap_badblocks(d->mtd, use_size);\r\nmapped = 0;\r\npages = d->mbd_dev->size;\r\nfor (i = 0; i < pages; i++)\r\nif (d->page_data[i] != BLOCK_UNDEF)\r\nmapped++;\r\nmutex_unlock(&d->mbd_dev->lock);\r\nfor (i = 0; i < MTDSWAP_TREE_CNT; i++) {\r\nif (!count[i])\r\ncontinue;\r\nif (min[i] != max[i])\r\nseq_printf(s, "%s:\t%5d erase blocks, erased min %d, "\r\n"max %d times\n",\r\nname[i], count[i], min[i], max[i]);\r\nelse\r\nseq_printf(s, "%s:\t%5d erase blocks, all erased %d "\r\n"times\n", name[i], count[i], min[i]);\r\n}\r\nif (bb_cnt)\r\nseq_printf(s, "bad:\t%5u erase blocks\n", bb_cnt);\r\nif (cw)\r\nseq_printf(s, "current erase block: %u pages used, %u free, "\r\n"erased %u times\n",\r\ncwp, d->pages_per_eblk - cwp, cwecount);\r\nseq_printf(s, "total erasures: %lu\n", sum);\r\nseq_printf(s, "\n");\r\nseq_printf(s, "mtdswap_readsect count: %llu\n", d->sect_read_count);\r\nseq_printf(s, "mtdswap_writesect count: %llu\n", d->sect_write_count);\r\nseq_printf(s, "mtdswap_discard count: %llu\n", d->discard_count);\r\nseq_printf(s, "mtd read count: %llu\n", d->mtd_read_count);\r\nseq_printf(s, "mtd write count: %llu\n", d->mtd_write_count);\r\nseq_printf(s, "discarded pages count: %llu\n", d->discard_page_count);\r\nseq_printf(s, "\n");\r\nseq_printf(s, "total pages: %u\n", pages);\r\nseq_printf(s, "pages mapped: %u\n", mapped);\r\nreturn 0;\r\n}\r\nstatic int mtdswap_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, mtdswap_show, inode->i_private);\r\n}\r\nstatic int mtdswap_add_debugfs(struct mtdswap_dev *d)\r\n{\r\nstruct gendisk *gd = d->mbd_dev->disk;\r\nstruct device *dev = disk_to_dev(gd);\r\nstruct dentry *root;\r\nstruct dentry *dent;\r\nroot = debugfs_create_dir(gd->disk_name, NULL);\r\nif (IS_ERR(root))\r\nreturn 0;\r\nif (!root) {\r\ndev_err(dev, "failed to initialize debugfs\n");\r\nreturn -1;\r\n}\r\nd->debugfs_root = root;\r\ndent = debugfs_create_file("stats", S_IRUSR, root, d,\r\n&mtdswap_fops);\r\nif (!dent) {\r\ndev_err(d->dev, "debugfs_create_file failed\n");\r\ndebugfs_remove_recursive(root);\r\nd->debugfs_root = NULL;\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtdswap_init(struct mtdswap_dev *d, unsigned int eblocks,\r\nunsigned int spare_cnt)\r\n{\r\nstruct mtd_info *mtd = d->mbd_dev->mtd;\r\nunsigned int i, eblk_bytes, pages, blocks;\r\nint ret = -ENOMEM;\r\nd->mtd = mtd;\r\nd->eblks = eblocks;\r\nd->spare_eblks = spare_cnt;\r\nd->pages_per_eblk = mtd->erasesize >> PAGE_SHIFT;\r\npages = d->mbd_dev->size;\r\nblocks = eblocks * d->pages_per_eblk;\r\nfor (i = 0; i < MTDSWAP_TREE_CNT; i++)\r\nd->trees[i].root = RB_ROOT;\r\nd->page_data = vmalloc(sizeof(int)*pages);\r\nif (!d->page_data)\r\ngoto page_data_fail;\r\nd->revmap = vmalloc(sizeof(int)*blocks);\r\nif (!d->revmap)\r\ngoto revmap_fail;\r\neblk_bytes = sizeof(struct swap_eb)*d->eblks;\r\nd->eb_data = vzalloc(eblk_bytes);\r\nif (!d->eb_data)\r\ngoto eb_data_fail;\r\nfor (i = 0; i < pages; i++)\r\nd->page_data[i] = BLOCK_UNDEF;\r\nfor (i = 0; i < blocks; i++)\r\nd->revmap[i] = PAGE_UNDEF;\r\nd->page_buf = kmalloc(PAGE_SIZE, GFP_KERNEL);\r\nif (!d->page_buf)\r\ngoto page_buf_fail;\r\nd->oob_buf = kmalloc(2 * mtd->ecclayout->oobavail, GFP_KERNEL);\r\nif (!d->oob_buf)\r\ngoto oob_buf_fail;\r\nmtdswap_scan_eblks(d);\r\nreturn 0;\r\noob_buf_fail:\r\nkfree(d->page_buf);\r\npage_buf_fail:\r\nvfree(d->eb_data);\r\neb_data_fail:\r\nvfree(d->revmap);\r\nrevmap_fail:\r\nvfree(d->page_data);\r\npage_data_fail:\r\nprintk(KERN_ERR "%s: init failed (%d)\n", MTDSWAP_PREFIX, ret);\r\nreturn ret;\r\n}\r\nstatic void mtdswap_add_mtd(struct mtd_blktrans_ops *tr, struct mtd_info *mtd)\r\n{\r\nstruct mtdswap_dev *d;\r\nstruct mtd_blktrans_dev *mbd_dev;\r\nchar *parts;\r\nchar *this_opt;\r\nunsigned long part;\r\nunsigned int eblocks, eavailable, bad_blocks, spare_cnt;\r\nuint64_t swap_size, use_size, size_limit;\r\nstruct nand_ecclayout *oinfo;\r\nint ret;\r\nparts = &partitions[0];\r\nif (!*parts)\r\nreturn;\r\nwhile ((this_opt = strsep(&parts, ",")) != NULL) {\r\nif (strict_strtoul(this_opt, 0, &part) < 0)\r\nreturn;\r\nif (mtd->index == part)\r\nbreak;\r\n}\r\nif (mtd->index != part)\r\nreturn;\r\nif (mtd->erasesize < PAGE_SIZE || mtd->erasesize % PAGE_SIZE) {\r\nprintk(KERN_ERR "%s: Erase size %u not multiple of PAGE_SIZE "\r\n"%lu\n", MTDSWAP_PREFIX, mtd->erasesize, PAGE_SIZE);\r\nreturn;\r\n}\r\nif (PAGE_SIZE % mtd->writesize || mtd->writesize > PAGE_SIZE) {\r\nprintk(KERN_ERR "%s: PAGE_SIZE %lu not multiple of write size"\r\n" %u\n", MTDSWAP_PREFIX, PAGE_SIZE, mtd->writesize);\r\nreturn;\r\n}\r\noinfo = mtd->ecclayout;\r\nif (!oinfo) {\r\nprintk(KERN_ERR "%s: mtd%d does not have OOB\n",\r\nMTDSWAP_PREFIX, mtd->index);\r\nreturn;\r\n}\r\nif (!mtd->oobsize || oinfo->oobavail < MTDSWAP_OOBSIZE) {\r\nprintk(KERN_ERR "%s: Not enough free bytes in OOB, "\r\n"%d available, %zu needed.\n",\r\nMTDSWAP_PREFIX, oinfo->oobavail, MTDSWAP_OOBSIZE);\r\nreturn;\r\n}\r\nif (spare_eblocks > 100)\r\nspare_eblocks = 100;\r\nuse_size = mtd->size;\r\nsize_limit = (uint64_t) BLOCK_MAX * PAGE_SIZE;\r\nif (mtd->size > size_limit) {\r\nprintk(KERN_WARNING "%s: Device too large. Limiting size to "\r\n"%llu bytes\n", MTDSWAP_PREFIX, size_limit);\r\nuse_size = size_limit;\r\n}\r\neblocks = mtd_div_by_eb(use_size, mtd);\r\nuse_size = eblocks * mtd->erasesize;\r\nbad_blocks = mtdswap_badblocks(mtd, use_size);\r\neavailable = eblocks - bad_blocks;\r\nif (eavailable < MIN_ERASE_BLOCKS) {\r\nprintk(KERN_ERR "%s: Not enough erase blocks. %u available, "\r\n"%d needed\n", MTDSWAP_PREFIX, eavailable,\r\nMIN_ERASE_BLOCKS);\r\nreturn;\r\n}\r\nspare_cnt = div_u64((uint64_t)eavailable * spare_eblocks, 100);\r\nif (spare_cnt < MIN_SPARE_EBLOCKS)\r\nspare_cnt = MIN_SPARE_EBLOCKS;\r\nif (spare_cnt > eavailable - 1)\r\nspare_cnt = eavailable - 1;\r\nswap_size = (uint64_t)(eavailable - spare_cnt) * mtd->erasesize +\r\n(header ? PAGE_SIZE : 0);\r\nprintk(KERN_INFO "%s: Enabling MTD swap on device %lu, size %llu KB, "\r\n"%u spare, %u bad blocks\n",\r\nMTDSWAP_PREFIX, part, swap_size / 1024, spare_cnt, bad_blocks);\r\nd = kzalloc(sizeof(struct mtdswap_dev), GFP_KERNEL);\r\nif (!d)\r\nreturn;\r\nmbd_dev = kzalloc(sizeof(struct mtd_blktrans_dev), GFP_KERNEL);\r\nif (!mbd_dev) {\r\nkfree(d);\r\nreturn;\r\n}\r\nd->mbd_dev = mbd_dev;\r\nmbd_dev->priv = d;\r\nmbd_dev->mtd = mtd;\r\nmbd_dev->devnum = mtd->index;\r\nmbd_dev->size = swap_size >> PAGE_SHIFT;\r\nmbd_dev->tr = tr;\r\nif (!(mtd->flags & MTD_WRITEABLE))\r\nmbd_dev->readonly = 1;\r\nif (mtdswap_init(d, eblocks, spare_cnt) < 0)\r\ngoto init_failed;\r\nif (add_mtd_blktrans_dev(mbd_dev) < 0)\r\ngoto cleanup;\r\nd->dev = disk_to_dev(mbd_dev->disk);\r\nret = mtdswap_add_debugfs(d);\r\nif (ret < 0)\r\ngoto debugfs_failed;\r\nreturn;\r\ndebugfs_failed:\r\ndel_mtd_blktrans_dev(mbd_dev);\r\ncleanup:\r\nmtdswap_cleanup(d);\r\ninit_failed:\r\nkfree(mbd_dev);\r\nkfree(d);\r\n}\r\nstatic void mtdswap_remove_dev(struct mtd_blktrans_dev *dev)\r\n{\r\nstruct mtdswap_dev *d = MTDSWAP_MBD_TO_MTDSWAP(dev);\r\ndebugfs_remove_recursive(d->debugfs_root);\r\ndel_mtd_blktrans_dev(dev);\r\nmtdswap_cleanup(d);\r\nkfree(d);\r\n}\r\nstatic int __init mtdswap_modinit(void)\r\n{\r\nreturn register_mtd_blktrans(&mtdswap_ops);\r\n}\r\nstatic void __exit mtdswap_modexit(void)\r\n{\r\nderegister_mtd_blktrans(&mtdswap_ops);\r\n}
