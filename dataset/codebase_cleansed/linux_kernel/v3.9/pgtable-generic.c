int ptep_set_access_flags(struct vm_area_struct *vma,\r\nunsigned long address, pte_t *ptep,\r\npte_t entry, int dirty)\r\n{\r\nint changed = !pte_same(*ptep, entry);\r\nif (changed) {\r\nset_pte_at(vma->vm_mm, address, ptep, entry);\r\nflush_tlb_fix_spurious_fault(vma, address);\r\n}\r\nreturn changed;\r\n}\r\nint pmdp_set_access_flags(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp,\r\npmd_t entry, int dirty)\r\n{\r\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\r\nint changed = !pmd_same(*pmdp, entry);\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\nif (changed) {\r\nset_pmd_at(vma->vm_mm, address, pmdp, entry);\r\nflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\r\n}\r\nreturn changed;\r\n#else\r\nBUG();\r\nreturn 0;\r\n#endif\r\n}\r\nint ptep_clear_flush_young(struct vm_area_struct *vma,\r\nunsigned long address, pte_t *ptep)\r\n{\r\nint young;\r\nyoung = ptep_test_and_clear_young(vma, address, ptep);\r\nif (young)\r\nflush_tlb_page(vma, address);\r\nreturn young;\r\n}\r\nint pmdp_clear_flush_young(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp)\r\n{\r\nint young;\r\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\n#else\r\nBUG();\r\n#endif\r\nyoung = pmdp_test_and_clear_young(vma, address, pmdp);\r\nif (young)\r\nflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\r\nreturn young;\r\n}\r\npte_t ptep_clear_flush(struct vm_area_struct *vma, unsigned long address,\r\npte_t *ptep)\r\n{\r\npte_t pte;\r\npte = ptep_get_and_clear((vma)->vm_mm, address, ptep);\r\nif (pte_accessible(pte))\r\nflush_tlb_page(vma, address);\r\nreturn pte;\r\n}\r\npmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,\r\npmd_t *pmdp)\r\n{\r\npmd_t pmd;\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\npmd = pmdp_get_and_clear(vma->vm_mm, address, pmdp);\r\nflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\r\nreturn pmd;\r\n}\r\nvoid pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,\r\npmd_t *pmdp)\r\n{\r\npmd_t pmd = pmd_mksplitting(*pmdp);\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\nset_pmd_at(vma->vm_mm, address, pmdp, pmd);\r\nflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\r\n}\r\nvoid pgtable_trans_huge_deposit(struct mm_struct *mm, pgtable_t pgtable)\r\n{\r\nassert_spin_locked(&mm->page_table_lock);\r\nif (!mm->pmd_huge_pte)\r\nINIT_LIST_HEAD(&pgtable->lru);\r\nelse\r\nlist_add(&pgtable->lru, &mm->pmd_huge_pte->lru);\r\nmm->pmd_huge_pte = pgtable;\r\n}\r\npgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm)\r\n{\r\npgtable_t pgtable;\r\nassert_spin_locked(&mm->page_table_lock);\r\npgtable = mm->pmd_huge_pte;\r\nif (list_empty(&pgtable->lru))\r\nmm->pmd_huge_pte = NULL;\r\nelse {\r\nmm->pmd_huge_pte = list_entry(pgtable->lru.next,\r\nstruct page, lru);\r\nlist_del(&pgtable->lru);\r\n}\r\nreturn pgtable;\r\n}\r\nvoid pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,\r\npmd_t *pmdp)\r\n{\r\nset_pmd_at(vma->vm_mm, address, pmdp, pmd_mknotpresent(*pmdp));\r\nflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\r\n}
