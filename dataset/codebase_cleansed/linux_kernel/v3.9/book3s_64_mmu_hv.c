long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)\r\n{\r\nunsigned long hpt;\r\nstruct revmap_entry *rev;\r\nstruct kvmppc_linear_info *li;\r\nlong order = kvm_hpt_order;\r\nif (htab_orderp) {\r\norder = *htab_orderp;\r\nif (order < PPC_MIN_HPT_ORDER)\r\norder = PPC_MIN_HPT_ORDER;\r\n}\r\nhpt = 0;\r\nif (order != kvm_hpt_order) {\r\nhpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT|\r\n__GFP_NOWARN, order - PAGE_SHIFT);\r\nif (!hpt)\r\n--order;\r\n}\r\nif (!hpt) {\r\nli = kvm_alloc_hpt();\r\nif (li) {\r\nhpt = (ulong)li->base_virt;\r\nkvm->arch.hpt_li = li;\r\norder = kvm_hpt_order;\r\n}\r\n}\r\nwhile (!hpt && order > PPC_MIN_HPT_ORDER) {\r\nhpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT|\r\n__GFP_NOWARN, order - PAGE_SHIFT);\r\nif (!hpt)\r\n--order;\r\n}\r\nif (!hpt)\r\nreturn -ENOMEM;\r\nkvm->arch.hpt_virt = hpt;\r\nkvm->arch.hpt_order = order;\r\nkvm->arch.hpt_npte = 1ul << (order - 4);\r\nkvm->arch.hpt_mask = (1ul << (order - 7)) - 1;\r\nrev = vmalloc(sizeof(struct revmap_entry) * kvm->arch.hpt_npte);\r\nif (!rev) {\r\npr_err("kvmppc_alloc_hpt: Couldn't alloc reverse map array\n");\r\ngoto out_freehpt;\r\n}\r\nkvm->arch.revmap = rev;\r\nkvm->arch.sdr1 = __pa(hpt) | (order - 18);\r\npr_info("KVM guest htab at %lx (order %ld), LPID %x\n",\r\nhpt, order, kvm->arch.lpid);\r\nif (htab_orderp)\r\n*htab_orderp = order;\r\nreturn 0;\r\nout_freehpt:\r\nif (kvm->arch.hpt_li)\r\nkvm_release_hpt(kvm->arch.hpt_li);\r\nelse\r\nfree_pages(hpt, order - PAGE_SHIFT);\r\nreturn -ENOMEM;\r\n}\r\nlong kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp)\r\n{\r\nlong err = -EBUSY;\r\nlong order;\r\nmutex_lock(&kvm->lock);\r\nif (kvm->arch.rma_setup_done) {\r\nkvm->arch.rma_setup_done = 0;\r\nsmp_mb();\r\nif (atomic_read(&kvm->arch.vcpus_running)) {\r\nkvm->arch.rma_setup_done = 1;\r\ngoto out;\r\n}\r\n}\r\nif (kvm->arch.hpt_virt) {\r\norder = kvm->arch.hpt_order;\r\nmemset((void *)kvm->arch.hpt_virt, 0, 1ul << order);\r\nkvmppc_rmap_reset(kvm);\r\ncpumask_setall(&kvm->arch.need_tlb_flush);\r\n*htab_orderp = order;\r\nerr = 0;\r\n} else {\r\nerr = kvmppc_alloc_hpt(kvm, htab_orderp);\r\norder = *htab_orderp;\r\n}\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn err;\r\n}\r\nvoid kvmppc_free_hpt(struct kvm *kvm)\r\n{\r\nkvmppc_free_lpid(kvm->arch.lpid);\r\nvfree(kvm->arch.revmap);\r\nif (kvm->arch.hpt_li)\r\nkvm_release_hpt(kvm->arch.hpt_li);\r\nelse\r\nfree_pages(kvm->arch.hpt_virt,\r\nkvm->arch.hpt_order - PAGE_SHIFT);\r\n}\r\nstatic inline unsigned long hpte0_pgsize_encoding(unsigned long pgsize)\r\n{\r\nreturn (pgsize > 0x1000) ? HPTE_V_LARGE : 0;\r\n}\r\nstatic inline unsigned long hpte1_pgsize_encoding(unsigned long pgsize)\r\n{\r\nreturn (pgsize == 0x10000) ? 0x1000 : 0;\r\n}\r\nvoid kvmppc_map_vrma(struct kvm_vcpu *vcpu, struct kvm_memory_slot *memslot,\r\nunsigned long porder)\r\n{\r\nunsigned long i;\r\nunsigned long npages;\r\nunsigned long hp_v, hp_r;\r\nunsigned long addr, hash;\r\nunsigned long psize;\r\nunsigned long hp0, hp1;\r\nunsigned long idx_ret;\r\nlong ret;\r\nstruct kvm *kvm = vcpu->kvm;\r\npsize = 1ul << porder;\r\nnpages = memslot->npages >> (porder - PAGE_SHIFT);\r\nif (npages > 1ul << (40 - porder))\r\nnpages = 1ul << (40 - porder);\r\nif (npages > kvm->arch.hpt_mask + 1)\r\nnpages = kvm->arch.hpt_mask + 1;\r\nhp0 = HPTE_V_1TB_SEG | (VRMA_VSID << (40 - 16)) |\r\nHPTE_V_BOLTED | hpte0_pgsize_encoding(psize);\r\nhp1 = hpte1_pgsize_encoding(psize) |\r\nHPTE_R_R | HPTE_R_C | HPTE_R_M | PP_RWXX;\r\nfor (i = 0; i < npages; ++i) {\r\naddr = i << porder;\r\nhash = (i ^ (VRMA_VSID ^ (VRMA_VSID << 25))) & kvm->arch.hpt_mask;\r\nhash = (hash << 3) + 7;\r\nhp_v = hp0 | ((addr >> 16) & ~0x7fUL);\r\nhp_r = hp1 | addr;\r\nret = kvmppc_virtmode_do_h_enter(kvm, H_EXACT, hash, hp_v, hp_r,\r\n&idx_ret);\r\nif (ret != H_SUCCESS) {\r\npr_err("KVM: map_vrma at %lx failed, ret=%ld\n",\r\naddr, ret);\r\nbreak;\r\n}\r\n}\r\n}\r\nint kvmppc_mmu_hv_init(void)\r\n{\r\nunsigned long host_lpid, rsvd_lpid;\r\nif (!cpu_has_feature(CPU_FTR_HVMODE))\r\nreturn -EINVAL;\r\nif (cpu_has_feature(CPU_FTR_ARCH_206)) {\r\nhost_lpid = mfspr(SPRN_LPID);\r\nrsvd_lpid = LPID_RSVD;\r\n} else {\r\nhost_lpid = 0;\r\nrsvd_lpid = MAX_LPID_970;\r\n}\r\nkvmppc_init_lpid(rsvd_lpid + 1);\r\nkvmppc_claim_lpid(host_lpid);\r\nkvmppc_claim_lpid(rsvd_lpid);\r\nreturn 0;\r\n}\r\nvoid kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic void kvmppc_mmu_book3s_64_hv_reset_msr(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_set_msr(vcpu, MSR_SF | MSR_ME);\r\n}\r\nstatic long kvmppc_get_guest_page(struct kvm *kvm, unsigned long gfn,\r\nstruct kvm_memory_slot *memslot,\r\nunsigned long psize)\r\n{\r\nunsigned long start;\r\nlong np, err;\r\nstruct page *page, *hpage, *pages[1];\r\nunsigned long s, pgsize;\r\nunsigned long *physp;\r\nunsigned int is_io, got, pgorder;\r\nstruct vm_area_struct *vma;\r\nunsigned long pfn, i, npages;\r\nphysp = memslot->arch.slot_phys;\r\nif (!physp)\r\nreturn -EINVAL;\r\nif (physp[gfn - memslot->base_gfn])\r\nreturn 0;\r\nis_io = 0;\r\ngot = 0;\r\npage = NULL;\r\npgsize = psize;\r\nerr = -EINVAL;\r\nstart = gfn_to_hva_memslot(memslot, gfn);\r\nnp = get_user_pages_fast(start, 1, 1, pages);\r\nif (np != 1) {\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, start);\r\nif (!vma || vma->vm_start > start ||\r\nstart + psize > vma->vm_end ||\r\n!(vma->vm_flags & VM_PFNMAP))\r\ngoto up_err;\r\nis_io = hpte_cache_bits(pgprot_val(vma->vm_page_prot));\r\npfn = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\nif (psize > PAGE_SIZE && (pfn & ((psize >> PAGE_SHIFT) - 1)))\r\ngoto up_err;\r\nup_read(&current->mm->mmap_sem);\r\n} else {\r\npage = pages[0];\r\ngot = KVMPPC_GOT_PAGE;\r\ns = PAGE_SIZE;\r\nif (PageHuge(page)) {\r\nhpage = compound_head(page);\r\ns <<= compound_order(hpage);\r\nif (s > psize && slot_is_aligned(memslot, s) &&\r\n!(memslot->userspace_addr & (s - 1))) {\r\nstart &= ~(s - 1);\r\npgsize = s;\r\nget_page(hpage);\r\nput_page(page);\r\npage = hpage;\r\n}\r\n}\r\nif (s < psize)\r\ngoto out;\r\npfn = page_to_pfn(page);\r\n}\r\nnpages = pgsize >> PAGE_SHIFT;\r\npgorder = __ilog2(npages);\r\nphysp += (gfn - memslot->base_gfn) & ~(npages - 1);\r\nspin_lock(&kvm->arch.slot_phys_lock);\r\nfor (i = 0; i < npages; ++i) {\r\nif (!physp[i]) {\r\nphysp[i] = ((pfn + i) << PAGE_SHIFT) +\r\ngot + is_io + pgorder;\r\ngot = 0;\r\n}\r\n}\r\nspin_unlock(&kvm->arch.slot_phys_lock);\r\nerr = 0;\r\nout:\r\nif (got)\r\nput_page(page);\r\nreturn err;\r\nup_err:\r\nup_read(&current->mm->mmap_sem);\r\nreturn err;\r\n}\r\nlong kvmppc_virtmode_do_h_enter(struct kvm *kvm, unsigned long flags,\r\nlong pte_index, unsigned long pteh,\r\nunsigned long ptel, unsigned long *pte_idx_ret)\r\n{\r\nunsigned long psize, gpa, gfn;\r\nstruct kvm_memory_slot *memslot;\r\nlong ret;\r\nif (kvm->arch.using_mmu_notifiers)\r\ngoto do_insert;\r\npsize = hpte_page_size(pteh, ptel);\r\nif (!psize)\r\nreturn H_PARAMETER;\r\npteh &= ~(HPTE_V_HVLOCK | HPTE_V_ABSENT | HPTE_V_VALID);\r\ngpa = (ptel & HPTE_R_RPN) & ~(psize - 1);\r\ngfn = gpa >> PAGE_SHIFT;\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (memslot && !(memslot->flags & KVM_MEMSLOT_INVALID)) {\r\nif (!slot_is_aligned(memslot, psize))\r\nreturn H_PARAMETER;\r\nif (kvmppc_get_guest_page(kvm, gfn, memslot, psize) < 0)\r\nreturn H_PARAMETER;\r\n}\r\ndo_insert:\r\nrcu_read_lock_sched();\r\nret = kvmppc_do_h_enter(kvm, flags, pte_index, pteh, ptel,\r\ncurrent->mm->pgd, false, pte_idx_ret);\r\nrcu_read_unlock_sched();\r\nif (ret == H_TOO_HARD) {\r\npr_err("KVM: Oops, kvmppc_h_enter returned too hard!\n");\r\nret = H_RESOURCE;\r\n}\r\nreturn ret;\r\n}\r\nlong kvmppc_virtmode_h_enter(struct kvm_vcpu *vcpu, unsigned long flags,\r\nlong pte_index, unsigned long pteh,\r\nunsigned long ptel)\r\n{\r\nreturn kvmppc_virtmode_do_h_enter(vcpu->kvm, flags, pte_index,\r\npteh, ptel, &vcpu->arch.gpr[4]);\r\n}\r\nstatic struct kvmppc_slb *kvmppc_mmu_book3s_hv_find_slbe(struct kvm_vcpu *vcpu,\r\ngva_t eaddr)\r\n{\r\nu64 mask;\r\nint i;\r\nfor (i = 0; i < vcpu->arch.slb_nr; i++) {\r\nif (!(vcpu->arch.slb[i].orige & SLB_ESID_V))\r\ncontinue;\r\nif (vcpu->arch.slb[i].origv & SLB_VSID_B_1T)\r\nmask = ESID_MASK_1T;\r\nelse\r\nmask = ESID_MASK;\r\nif (((vcpu->arch.slb[i].orige ^ eaddr) & mask) == 0)\r\nreturn &vcpu->arch.slb[i];\r\n}\r\nreturn NULL;\r\n}\r\nstatic unsigned long kvmppc_mmu_get_real_addr(unsigned long v, unsigned long r,\r\nunsigned long ea)\r\n{\r\nunsigned long ra_mask;\r\nra_mask = hpte_page_size(v, r) - 1;\r\nreturn (r & HPTE_R_RPN & ~ra_mask) | (ea & ra_mask);\r\n}\r\nstatic int kvmppc_mmu_book3s_64_hv_xlate(struct kvm_vcpu *vcpu, gva_t eaddr,\r\nstruct kvmppc_pte *gpte, bool data)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct kvmppc_slb *slbe;\r\nunsigned long slb_v;\r\nunsigned long pp, key;\r\nunsigned long v, gr;\r\nunsigned long *hptep;\r\nint index;\r\nint virtmode = vcpu->arch.shregs.msr & (data ? MSR_DR : MSR_IR);\r\nif (virtmode) {\r\nslbe = kvmppc_mmu_book3s_hv_find_slbe(vcpu, eaddr);\r\nif (!slbe)\r\nreturn -EINVAL;\r\nslb_v = slbe->origv;\r\n} else {\r\nslb_v = vcpu->kvm->arch.vrma_slb_v;\r\n}\r\nindex = kvmppc_hv_find_lock_hpte(kvm, eaddr, slb_v,\r\nHPTE_V_VALID | HPTE_V_ABSENT);\r\nif (index < 0)\r\nreturn -ENOENT;\r\nhptep = (unsigned long *)(kvm->arch.hpt_virt + (index << 4));\r\nv = hptep[0] & ~HPTE_V_HVLOCK;\r\ngr = kvm->arch.revmap[index].guest_rpte;\r\nasm volatile("lwsync" : : : "memory");\r\nhptep[0] = v;\r\ngpte->eaddr = eaddr;\r\ngpte->vpage = ((v & HPTE_V_AVPN) << 4) | ((eaddr >> 12) & 0xfff);\r\npp = gr & (HPTE_R_PP0 | HPTE_R_PP);\r\nkey = (vcpu->arch.shregs.msr & MSR_PR) ? SLB_VSID_KP : SLB_VSID_KS;\r\nkey &= slb_v;\r\ngpte->may_read = hpte_read_permission(pp, key);\r\ngpte->may_write = hpte_write_permission(pp, key);\r\ngpte->may_execute = gpte->may_read && !(gr & (HPTE_R_N | HPTE_R_G));\r\nif (data && virtmode && cpu_has_feature(CPU_FTR_ARCH_206)) {\r\nint amrfield = hpte_get_skey_perm(gr, vcpu->arch.amr);\r\nif (amrfield & 1)\r\ngpte->may_read = 0;\r\nif (amrfield & 2)\r\ngpte->may_write = 0;\r\n}\r\ngpte->raddr = kvmppc_mmu_get_real_addr(v, gr, eaddr);\r\nreturn 0;\r\n}\r\nstatic int instruction_is_store(unsigned int instr)\r\n{\r\nunsigned int mask;\r\nmask = 0x10000000;\r\nif ((instr & 0xfc000000) == 0x7c000000)\r\nmask = 0x100;\r\nreturn (instr & mask) != 0;\r\n}\r\nstatic int kvmppc_hv_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned long gpa, gva_t ea, int is_store)\r\n{\r\nint ret;\r\nu32 last_inst;\r\nunsigned long srr0 = kvmppc_get_pc(vcpu);\r\nif (vcpu->arch.last_inst == KVM_INST_FETCH_FAILED) {\r\nret = kvmppc_ld(vcpu, &srr0, sizeof(u32), &last_inst, false);\r\nif (ret != EMULATE_DONE || last_inst == KVM_INST_FETCH_FAILED)\r\nreturn RESUME_GUEST;\r\nvcpu->arch.last_inst = last_inst;\r\n}\r\nif (instruction_is_store(vcpu->arch.last_inst) != !!is_store)\r\nreturn RESUME_GUEST;\r\nvcpu->arch.paddr_accessed = gpa;\r\nvcpu->arch.vaddr_accessed = ea;\r\nreturn kvmppc_emulate_mmio(run, vcpu);\r\n}\r\nint kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned long ea, unsigned long dsisr)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long *hptep, hpte[3], r;\r\nunsigned long mmu_seq, psize, pte_size;\r\nunsigned long gpa, gfn, hva, pfn;\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long *rmap;\r\nstruct revmap_entry *rev;\r\nstruct page *page, *pages[1];\r\nlong index, ret, npages;\r\nunsigned long is_io;\r\nunsigned int writing, write_ok;\r\nstruct vm_area_struct *vma;\r\nunsigned long rcbits;\r\nif (ea != vcpu->arch.pgfault_addr)\r\nreturn RESUME_GUEST;\r\nindex = vcpu->arch.pgfault_index;\r\nhptep = (unsigned long *)(kvm->arch.hpt_virt + (index << 4));\r\nrev = &kvm->arch.revmap[index];\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptep, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nhpte[0] = hptep[0] & ~HPTE_V_HVLOCK;\r\nhpte[1] = hptep[1];\r\nhpte[2] = r = rev->guest_rpte;\r\nasm volatile("lwsync" : : : "memory");\r\nhptep[0] = hpte[0];\r\npreempt_enable();\r\nif (hpte[0] != vcpu->arch.pgfault_hpte[0] ||\r\nhpte[1] != vcpu->arch.pgfault_hpte[1])\r\nreturn RESUME_GUEST;\r\npsize = hpte_page_size(hpte[0], r);\r\ngpa = (r & HPTE_R_RPN & ~(psize - 1)) | (ea & (psize - 1));\r\ngfn = gpa >> PAGE_SHIFT;\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID))\r\nreturn kvmppc_hv_emulate_mmio(run, vcpu, gpa, ea,\r\ndsisr & DSISR_ISSTORE);\r\nif (!kvm->arch.using_mmu_notifiers)\r\nreturn -EFAULT;\r\nmmu_seq = kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\nis_io = 0;\r\npfn = 0;\r\npage = NULL;\r\npte_size = PAGE_SIZE;\r\nwriting = (dsisr & DSISR_ISSTORE) != 0;\r\nwrite_ok = writing;\r\nhva = gfn_to_hva_memslot(memslot, gfn);\r\nnpages = get_user_pages_fast(hva, 1, writing, pages);\r\nif (npages < 1) {\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, hva);\r\nif (vma && vma->vm_start <= hva && hva + psize <= vma->vm_end &&\r\n(vma->vm_flags & VM_PFNMAP)) {\r\npfn = vma->vm_pgoff +\r\n((hva - vma->vm_start) >> PAGE_SHIFT);\r\npte_size = psize;\r\nis_io = hpte_cache_bits(pgprot_val(vma->vm_page_prot));\r\nwrite_ok = vma->vm_flags & VM_WRITE;\r\n}\r\nup_read(&current->mm->mmap_sem);\r\nif (!pfn)\r\nreturn -EFAULT;\r\n} else {\r\npage = pages[0];\r\nif (PageHuge(page)) {\r\npage = compound_head(page);\r\npte_size <<= compound_order(page);\r\n}\r\nif (!writing && hpte_is_writable(r)) {\r\npte_t *ptep, pte;\r\nrcu_read_lock_sched();\r\nptep = find_linux_pte_or_hugepte(current->mm->pgd,\r\nhva, NULL);\r\nif (ptep && pte_present(*ptep)) {\r\npte = kvmppc_read_update_linux_pte(ptep, 1);\r\nif (pte_write(pte))\r\nwrite_ok = 1;\r\n}\r\nrcu_read_unlock_sched();\r\n}\r\npfn = page_to_pfn(page);\r\n}\r\nret = -EFAULT;\r\nif (psize > pte_size)\r\ngoto out_put;\r\nif (!hpte_cache_flags_ok(r, is_io)) {\r\nif (is_io)\r\nreturn -EFAULT;\r\nr = (r & ~(HPTE_R_W|HPTE_R_I|HPTE_R_G)) | HPTE_R_M;\r\n}\r\nr = (r & ~(HPTE_R_PP0 - pte_size)) | (pfn << PAGE_SHIFT);\r\nif (hpte_is_writable(r) && !write_ok)\r\nr = hpte_make_readonly(r);\r\nret = RESUME_GUEST;\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptep, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nif ((hptep[0] & ~HPTE_V_HVLOCK) != hpte[0] || hptep[1] != hpte[1] ||\r\nrev->guest_rpte != hpte[2])\r\ngoto out_unlock;\r\nhpte[0] = (hpte[0] & ~HPTE_V_ABSENT) | HPTE_V_VALID;\r\nrmap = &memslot->arch.rmap[gfn - memslot->base_gfn];\r\nlock_rmap(rmap);\r\nret = RESUME_GUEST;\r\nif (mmu_notifier_retry(vcpu->kvm, mmu_seq)) {\r\nunlock_rmap(rmap);\r\ngoto out_unlock;\r\n}\r\nrcbits = *rmap >> KVMPPC_RMAP_RC_SHIFT;\r\nr &= rcbits | ~(HPTE_R_R | HPTE_R_C);\r\nif (hptep[0] & HPTE_V_VALID) {\r\nunlock_rmap(rmap);\r\nhptep[0] |= HPTE_V_ABSENT;\r\nkvmppc_invalidate_hpte(kvm, hptep, index);\r\nr |= hptep[1] & (HPTE_R_R | HPTE_R_C);\r\n} else {\r\nkvmppc_add_revmap_chain(kvm, rev, rmap, index, 0);\r\n}\r\nhptep[1] = r;\r\neieio();\r\nhptep[0] = hpte[0];\r\nasm volatile("ptesync" : : : "memory");\r\npreempt_enable();\r\nif (page && hpte_is_writable(r))\r\nSetPageDirty(page);\r\nout_put:\r\nif (page) {\r\nput_page(pages[0]);\r\n}\r\nreturn ret;\r\nout_unlock:\r\nhptep[0] &= ~HPTE_V_HVLOCK;\r\npreempt_enable();\r\ngoto out_put;\r\n}\r\nstatic void kvmppc_rmap_reset(struct kvm *kvm)\r\n{\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nint srcu_idx;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nslots = kvm->memslots;\r\nkvm_for_each_memslot(memslot, slots) {\r\nmemset(memslot->arch.rmap, 0,\r\nmemslot->npages * sizeof(*memslot->arch.rmap));\r\n}\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\n}\r\nstatic int kvm_handle_hva_range(struct kvm *kvm,\r\nunsigned long start,\r\nunsigned long end,\r\nint (*handler)(struct kvm *kvm,\r\nunsigned long *rmapp,\r\nunsigned long gfn))\r\n{\r\nint ret;\r\nint retval = 0;\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nunsigned long hva_start, hva_end;\r\ngfn_t gfn, gfn_end;\r\nhva_start = max(start, memslot->userspace_addr);\r\nhva_end = min(end, memslot->userspace_addr +\r\n(memslot->npages << PAGE_SHIFT));\r\nif (hva_start >= hva_end)\r\ncontinue;\r\ngfn = hva_to_gfn_memslot(hva_start, memslot);\r\ngfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);\r\nfor (; gfn < gfn_end; ++gfn) {\r\ngfn_t gfn_offset = gfn - memslot->base_gfn;\r\nret = handler(kvm, &memslot->arch.rmap[gfn_offset], gfn);\r\nretval |= ret;\r\n}\r\n}\r\nreturn retval;\r\n}\r\nstatic int kvm_handle_hva(struct kvm *kvm, unsigned long hva,\r\nint (*handler)(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn))\r\n{\r\nreturn kvm_handle_hva_range(kvm, hva, hva + 1, handler);\r\n}\r\nstatic int kvm_unmap_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long h, i, j;\r\nunsigned long *hptep;\r\nunsigned long ptel, psize, rcbits;\r\nfor (;;) {\r\nlock_rmap(rmapp);\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nbreak;\r\n}\r\ni = *rmapp & KVMPPC_RMAP_INDEX;\r\nhptep = (unsigned long *) (kvm->arch.hpt_virt + (i << 4));\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (hptep[0] & HPTE_V_HVLOCK)\r\ncpu_relax();\r\ncontinue;\r\n}\r\nj = rev[i].forw;\r\nif (j == i) {\r\n*rmapp &= ~(KVMPPC_RMAP_PRESENT | KVMPPC_RMAP_INDEX);\r\n} else {\r\nh = rev[i].back;\r\nrev[h].forw = j;\r\nrev[j].back = h;\r\nrev[i].forw = rev[i].back = i;\r\n*rmapp = (*rmapp & ~KVMPPC_RMAP_INDEX) | j;\r\n}\r\nptel = rev[i].guest_rpte;\r\npsize = hpte_page_size(hptep[0], ptel);\r\nif ((hptep[0] & HPTE_V_VALID) &&\r\nhpte_rpn(ptel, psize) == gfn) {\r\nif (kvm->arch.using_mmu_notifiers)\r\nhptep[0] |= HPTE_V_ABSENT;\r\nkvmppc_invalidate_hpte(kvm, hptep, i);\r\nrcbits = hptep[1] & (HPTE_R_R | HPTE_R_C);\r\n*rmapp |= rcbits << KVMPPC_RMAP_RC_SHIFT;\r\nrev[i].guest_rpte = ptel | rcbits;\r\n}\r\nunlock_rmap(rmapp);\r\nhptep[0] &= ~HPTE_V_HVLOCK;\r\n}\r\nreturn 0;\r\n}\r\nint kvm_unmap_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nif (kvm->arch.using_mmu_notifiers)\r\nkvm_handle_hva(kvm, hva, kvm_unmap_rmapp);\r\nreturn 0;\r\n}\r\nint kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nif (kvm->arch.using_mmu_notifiers)\r\nkvm_handle_hva_range(kvm, start, end, kvm_unmap_rmapp);\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_flush_memslot(struct kvm *kvm, struct kvm_memory_slot *memslot)\r\n{\r\nunsigned long *rmapp;\r\nunsigned long gfn;\r\nunsigned long n;\r\nrmapp = memslot->arch.rmap;\r\ngfn = memslot->base_gfn;\r\nfor (n = memslot->npages; n; --n) {\r\nif (*rmapp & KVMPPC_RMAP_PRESENT)\r\nkvm_unmap_rmapp(kvm, rmapp, gfn);\r\n++rmapp;\r\n++gfn;\r\n}\r\n}\r\nstatic int kvm_age_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long head, i, j;\r\nunsigned long *hptep;\r\nint ret = 0;\r\nretry:\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED) {\r\n*rmapp &= ~KVMPPC_RMAP_REFERENCED;\r\nret = 1;\r\n}\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nhptep = (unsigned long *) (kvm->arch.hpt_virt + (i << 4));\r\nj = rev[i].forw;\r\nif (!(hptep[1] & HPTE_R_R))\r\ncontinue;\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (hptep[0] & HPTE_V_HVLOCK)\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif ((hptep[0] & HPTE_V_VALID) && (hptep[1] & HPTE_R_R)) {\r\nkvmppc_clear_ref_hpte(kvm, hptep, i);\r\nrev[i].guest_rpte |= HPTE_R_R;\r\nret = 1;\r\n}\r\nhptep[0] &= ~HPTE_V_HVLOCK;\r\n} while ((i = j) != head);\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\nint kvm_age_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nif (!kvm->arch.using_mmu_notifiers)\r\nreturn 0;\r\nreturn kvm_handle_hva(kvm, hva, kvm_age_rmapp);\r\n}\r\nstatic int kvm_test_age_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long head, i, j;\r\nunsigned long *hp;\r\nint ret = 1;\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED)\r\nreturn 1;\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED)\r\ngoto out;\r\nif (*rmapp & KVMPPC_RMAP_PRESENT) {\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nhp = (unsigned long *)(kvm->arch.hpt_virt + (i << 4));\r\nj = rev[i].forw;\r\nif (hp[1] & HPTE_R_R)\r\ngoto out;\r\n} while ((i = j) != head);\r\n}\r\nret = 0;\r\nout:\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\nint kvm_test_age_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nif (!kvm->arch.using_mmu_notifiers)\r\nreturn 0;\r\nreturn kvm_handle_hva(kvm, hva, kvm_test_age_rmapp);\r\n}\r\nvoid kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)\r\n{\r\nif (!kvm->arch.using_mmu_notifiers)\r\nreturn;\r\nkvm_handle_hva(kvm, hva, kvm_unmap_rmapp);\r\n}\r\nstatic int kvm_test_clear_dirty(struct kvm *kvm, unsigned long *rmapp)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long head, i, j;\r\nunsigned long *hptep;\r\nint ret = 0;\r\nretry:\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_CHANGED) {\r\n*rmapp &= ~KVMPPC_RMAP_CHANGED;\r\nret = 1;\r\n}\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nhptep = (unsigned long *) (kvm->arch.hpt_virt + (i << 4));\r\nj = rev[i].forw;\r\nif (!(hptep[1] & HPTE_R_C))\r\ncontinue;\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (hptep[0] & HPTE_V_HVLOCK)\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif ((hptep[0] & HPTE_V_VALID) && (hptep[1] & HPTE_R_C)) {\r\nhptep[0] |= HPTE_V_ABSENT;\r\nkvmppc_invalidate_hpte(kvm, hptep, i);\r\nhptep[1] &= ~HPTE_R_C;\r\neieio();\r\nhptep[0] = (hptep[0] & ~HPTE_V_ABSENT) | HPTE_V_VALID;\r\nrev[i].guest_rpte |= HPTE_R_C;\r\nret = 1;\r\n}\r\nhptep[0] &= ~HPTE_V_HVLOCK;\r\n} while ((i = j) != head);\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\nlong kvmppc_hv_get_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long *map)\r\n{\r\nunsigned long i;\r\nunsigned long *rmapp;\r\npreempt_disable();\r\nrmapp = memslot->arch.rmap;\r\nfor (i = 0; i < memslot->npages; ++i) {\r\nif (kvm_test_clear_dirty(kvm, rmapp) && map)\r\n__set_bit_le(i, map);\r\n++rmapp;\r\n}\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nvoid *kvmppc_pin_guest_page(struct kvm *kvm, unsigned long gpa,\r\nunsigned long *nb_ret)\r\n{\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long gfn = gpa >> PAGE_SHIFT;\r\nstruct page *page, *pages[1];\r\nint npages;\r\nunsigned long hva, psize, offset;\r\nunsigned long pa;\r\nunsigned long *physp;\r\nint srcu_idx;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID))\r\ngoto err;\r\nif (!kvm->arch.using_mmu_notifiers) {\r\nphysp = memslot->arch.slot_phys;\r\nif (!physp)\r\ngoto err;\r\nphysp += gfn - memslot->base_gfn;\r\npa = *physp;\r\nif (!pa) {\r\nif (kvmppc_get_guest_page(kvm, gfn, memslot,\r\nPAGE_SIZE) < 0)\r\ngoto err;\r\npa = *physp;\r\n}\r\npage = pfn_to_page(pa >> PAGE_SHIFT);\r\nget_page(page);\r\n} else {\r\nhva = gfn_to_hva_memslot(memslot, gfn);\r\nnpages = get_user_pages_fast(hva, 1, 1, pages);\r\nif (npages < 1)\r\ngoto err;\r\npage = pages[0];\r\n}\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\npsize = PAGE_SIZE;\r\nif (PageHuge(page)) {\r\npage = compound_head(page);\r\npsize <<= compound_order(page);\r\n}\r\noffset = gpa & (psize - 1);\r\nif (nb_ret)\r\n*nb_ret = psize - offset;\r\nreturn page_address(page) + offset;\r\nerr:\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\nreturn NULL;\r\n}\r\nvoid kvmppc_unpin_guest_page(struct kvm *kvm, void *va)\r\n{\r\nstruct page *page = virt_to_page(va);\r\nput_page(page);\r\n}\r\nstatic long record_hpte(unsigned long flags, unsigned long *hptp,\r\nunsigned long *hpte, struct revmap_entry *revp,\r\nint want_valid, int first_pass)\r\n{\r\nunsigned long v, r;\r\nint ok = 1;\r\nint valid, dirty;\r\ndirty = !!(revp->guest_rpte & HPTE_GR_MODIFIED);\r\nif (!first_pass && !dirty)\r\nreturn 0;\r\nvalid = 0;\r\nif (hptp[0] & (HPTE_V_VALID | HPTE_V_ABSENT)) {\r\nvalid = 1;\r\nif ((flags & KVM_GET_HTAB_BOLTED_ONLY) &&\r\n!(hptp[0] & HPTE_V_BOLTED))\r\nvalid = 0;\r\n}\r\nif (valid != want_valid)\r\nreturn 0;\r\nv = r = 0;\r\nif (valid || dirty) {\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptp, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = hptp[0];\r\nif (v & HPTE_V_ABSENT) {\r\nv &= ~HPTE_V_ABSENT;\r\nv |= HPTE_V_VALID;\r\n}\r\nvalid = !!(v & HPTE_V_VALID);\r\nif ((flags & KVM_GET_HTAB_BOLTED_ONLY) && !(v & HPTE_V_BOLTED))\r\nvalid = 0;\r\nr = revp->guest_rpte | (hptp[1] & (HPTE_R_R | HPTE_R_C));\r\ndirty = !!(revp->guest_rpte & HPTE_GR_MODIFIED);\r\nif (valid == want_valid && dirty) {\r\nr &= ~HPTE_GR_MODIFIED;\r\nrevp->guest_rpte = r;\r\n}\r\nasm volatile(PPC_RELEASE_BARRIER "" : : : "memory");\r\nhptp[0] &= ~HPTE_V_HVLOCK;\r\npreempt_enable();\r\nif (!(valid == want_valid && (first_pass || dirty)))\r\nok = 0;\r\n}\r\nhpte[0] = v;\r\nhpte[1] = r;\r\nreturn ok;\r\n}\r\nstatic ssize_t kvm_htab_read(struct file *file, char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct kvm_htab_ctx *ctx = file->private_data;\r\nstruct kvm *kvm = ctx->kvm;\r\nstruct kvm_get_htab_header hdr;\r\nunsigned long *hptp;\r\nstruct revmap_entry *revp;\r\nunsigned long i, nb, nw;\r\nunsigned long __user *lbuf;\r\nstruct kvm_get_htab_header __user *hptr;\r\nunsigned long flags;\r\nint first_pass;\r\nunsigned long hpte[2];\r\nif (!access_ok(VERIFY_WRITE, buf, count))\r\nreturn -EFAULT;\r\nfirst_pass = ctx->first_pass;\r\nflags = ctx->flags;\r\ni = ctx->index;\r\nhptp = (unsigned long *)(kvm->arch.hpt_virt + (i * HPTE_SIZE));\r\nrevp = kvm->arch.revmap + i;\r\nlbuf = (unsigned long __user *)buf;\r\nnb = 0;\r\nwhile (nb + sizeof(hdr) + HPTE_SIZE < count) {\r\nhptr = (struct kvm_get_htab_header __user *)buf;\r\nhdr.n_valid = 0;\r\nhdr.n_invalid = 0;\r\nnw = nb;\r\nnb += sizeof(hdr);\r\nlbuf = (unsigned long __user *)(buf + sizeof(hdr));\r\nif (!first_pass) {\r\nwhile (i < kvm->arch.hpt_npte &&\r\n!(revp->guest_rpte & HPTE_GR_MODIFIED)) {\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\n}\r\nhdr.index = i;\r\nwhile (i < kvm->arch.hpt_npte &&\r\nhdr.n_valid < 0xffff &&\r\nnb + HPTE_SIZE < count &&\r\nrecord_hpte(flags, hptp, hpte, revp, 1, first_pass)) {\r\n++hdr.n_valid;\r\nif (__put_user(hpte[0], lbuf) ||\r\n__put_user(hpte[1], lbuf + 1))\r\nreturn -EFAULT;\r\nnb += HPTE_SIZE;\r\nlbuf += 2;\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\nwhile (i < kvm->arch.hpt_npte &&\r\nhdr.n_invalid < 0xffff &&\r\nrecord_hpte(flags, hptp, hpte, revp, 0, first_pass)) {\r\n++hdr.n_invalid;\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\nif (hdr.n_valid || hdr.n_invalid) {\r\nif (__copy_to_user(hptr, &hdr, sizeof(hdr)))\r\nreturn -EFAULT;\r\nnw = nb;\r\nbuf = (char __user *)lbuf;\r\n} else {\r\nnb = nw;\r\n}\r\nif (i >= kvm->arch.hpt_npte) {\r\ni = 0;\r\nctx->first_pass = 0;\r\nbreak;\r\n}\r\n}\r\nctx->index = i;\r\nreturn nb;\r\n}\r\nstatic ssize_t kvm_htab_write(struct file *file, const char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct kvm_htab_ctx *ctx = file->private_data;\r\nstruct kvm *kvm = ctx->kvm;\r\nstruct kvm_get_htab_header hdr;\r\nunsigned long i, j;\r\nunsigned long v, r;\r\nunsigned long __user *lbuf;\r\nunsigned long *hptp;\r\nunsigned long tmp[2];\r\nssize_t nb;\r\nlong int err, ret;\r\nint rma_setup;\r\nif (!access_ok(VERIFY_READ, buf, count))\r\nreturn -EFAULT;\r\nmutex_lock(&kvm->lock);\r\nrma_setup = kvm->arch.rma_setup_done;\r\nif (rma_setup) {\r\nkvm->arch.rma_setup_done = 0;\r\nsmp_mb();\r\nif (atomic_read(&kvm->arch.vcpus_running)) {\r\nkvm->arch.rma_setup_done = 1;\r\nmutex_unlock(&kvm->lock);\r\nreturn -EBUSY;\r\n}\r\n}\r\nerr = 0;\r\nfor (nb = 0; nb + sizeof(hdr) <= count; ) {\r\nerr = -EFAULT;\r\nif (__copy_from_user(&hdr, buf, sizeof(hdr)))\r\nbreak;\r\nerr = 0;\r\nif (nb + hdr.n_valid * HPTE_SIZE > count)\r\nbreak;\r\nnb += sizeof(hdr);\r\nbuf += sizeof(hdr);\r\nerr = -EINVAL;\r\ni = hdr.index;\r\nif (i >= kvm->arch.hpt_npte ||\r\ni + hdr.n_valid + hdr.n_invalid > kvm->arch.hpt_npte)\r\nbreak;\r\nhptp = (unsigned long *)(kvm->arch.hpt_virt + (i * HPTE_SIZE));\r\nlbuf = (unsigned long __user *)buf;\r\nfor (j = 0; j < hdr.n_valid; ++j) {\r\nerr = -EFAULT;\r\nif (__get_user(v, lbuf) || __get_user(r, lbuf + 1))\r\ngoto out;\r\nerr = -EINVAL;\r\nif (!(v & HPTE_V_VALID))\r\ngoto out;\r\nlbuf += 2;\r\nnb += HPTE_SIZE;\r\nif (hptp[0] & (HPTE_V_VALID | HPTE_V_ABSENT))\r\nkvmppc_do_h_remove(kvm, 0, i, 0, tmp);\r\nerr = -EIO;\r\nret = kvmppc_virtmode_do_h_enter(kvm, H_EXACT, i, v, r,\r\ntmp);\r\nif (ret != H_SUCCESS) {\r\npr_err("kvm_htab_write ret %ld i=%ld v=%lx "\r\n"r=%lx\n", ret, i, v, r);\r\ngoto out;\r\n}\r\nif (!rma_setup && is_vrma_hpte(v)) {\r\nunsigned long psize = hpte_page_size(v, r);\r\nunsigned long senc = slb_pgsize_encoding(psize);\r\nunsigned long lpcr;\r\nkvm->arch.vrma_slb_v = senc | SLB_VSID_B_1T |\r\n(VRMA_VSID << SLB_VSID_SHIFT_1T);\r\nlpcr = kvm->arch.lpcr & ~LPCR_VRMASD;\r\nlpcr |= senc << (LPCR_VRMASD_SH - 4);\r\nkvm->arch.lpcr = lpcr;\r\nrma_setup = 1;\r\n}\r\n++i;\r\nhptp += 2;\r\n}\r\nfor (j = 0; j < hdr.n_invalid; ++j) {\r\nif (hptp[0] & (HPTE_V_VALID | HPTE_V_ABSENT))\r\nkvmppc_do_h_remove(kvm, 0, i, 0, tmp);\r\n++i;\r\nhptp += 2;\r\n}\r\nerr = 0;\r\n}\r\nout:\r\nsmp_wmb();\r\nkvm->arch.rma_setup_done = rma_setup;\r\nmutex_unlock(&kvm->lock);\r\nif (err)\r\nreturn err;\r\nreturn nb;\r\n}\r\nstatic int kvm_htab_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct kvm_htab_ctx *ctx = filp->private_data;\r\nfilp->private_data = NULL;\r\nif (!(ctx->flags & KVM_GET_HTAB_WRITE))\r\natomic_dec(&ctx->kvm->arch.hpte_mod_interest);\r\nkvm_put_kvm(ctx->kvm);\r\nkfree(ctx);\r\nreturn 0;\r\n}\r\nint kvm_vm_ioctl_get_htab_fd(struct kvm *kvm, struct kvm_get_htab_fd *ghf)\r\n{\r\nint ret;\r\nstruct kvm_htab_ctx *ctx;\r\nint rwflag;\r\nif (ghf->flags & ~(KVM_GET_HTAB_BOLTED_ONLY | KVM_GET_HTAB_WRITE))\r\nreturn -EINVAL;\r\nctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\r\nif (!ctx)\r\nreturn -ENOMEM;\r\nkvm_get_kvm(kvm);\r\nctx->kvm = kvm;\r\nctx->index = ghf->start_index;\r\nctx->flags = ghf->flags;\r\nctx->first_pass = 1;\r\nrwflag = (ghf->flags & KVM_GET_HTAB_WRITE) ? O_WRONLY : O_RDONLY;\r\nret = anon_inode_getfd("kvm-htab", &kvm_htab_fops, ctx, rwflag);\r\nif (ret < 0) {\r\nkvm_put_kvm(kvm);\r\nreturn ret;\r\n}\r\nif (rwflag == O_RDONLY) {\r\nmutex_lock(&kvm->slots_lock);\r\natomic_inc(&kvm->arch.hpte_mod_interest);\r\nsynchronize_srcu_expedited(&kvm->srcu);\r\nmutex_unlock(&kvm->slots_lock);\r\n}\r\nreturn ret;\r\n}\r\nvoid kvmppc_mmu_book3s_hv_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_mmu *mmu = &vcpu->arch.mmu;\r\nif (cpu_has_feature(CPU_FTR_ARCH_206))\r\nvcpu->arch.slb_nr = 32;\r\nelse\r\nvcpu->arch.slb_nr = 64;\r\nmmu->xlate = kvmppc_mmu_book3s_64_hv_xlate;\r\nmmu->reset_msr = kvmppc_mmu_book3s_64_hv_reset_msr;\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_SLB;\r\n}
