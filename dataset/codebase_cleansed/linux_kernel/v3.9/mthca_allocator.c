u32 mthca_alloc(struct mthca_alloc *alloc)\r\n{\r\nunsigned long flags;\r\nu32 obj;\r\nspin_lock_irqsave(&alloc->lock, flags);\r\nobj = find_next_zero_bit(alloc->table, alloc->max, alloc->last);\r\nif (obj >= alloc->max) {\r\nalloc->top = (alloc->top + alloc->max) & alloc->mask;\r\nobj = find_first_zero_bit(alloc->table, alloc->max);\r\n}\r\nif (obj < alloc->max) {\r\nset_bit(obj, alloc->table);\r\nobj |= alloc->top;\r\n} else\r\nobj = -1;\r\nspin_unlock_irqrestore(&alloc->lock, flags);\r\nreturn obj;\r\n}\r\nvoid mthca_free(struct mthca_alloc *alloc, u32 obj)\r\n{\r\nunsigned long flags;\r\nobj &= alloc->max - 1;\r\nspin_lock_irqsave(&alloc->lock, flags);\r\nclear_bit(obj, alloc->table);\r\nalloc->last = min(alloc->last, obj);\r\nalloc->top = (alloc->top + alloc->max) & alloc->mask;\r\nspin_unlock_irqrestore(&alloc->lock, flags);\r\n}\r\nint mthca_alloc_init(struct mthca_alloc *alloc, u32 num, u32 mask,\r\nu32 reserved)\r\n{\r\nint i;\r\nif (num != 1 << (ffs(num) - 1))\r\nreturn -EINVAL;\r\nalloc->last = 0;\r\nalloc->top = 0;\r\nalloc->max = num;\r\nalloc->mask = mask;\r\nspin_lock_init(&alloc->lock);\r\nalloc->table = kmalloc(BITS_TO_LONGS(num) * sizeof (long),\r\nGFP_KERNEL);\r\nif (!alloc->table)\r\nreturn -ENOMEM;\r\nbitmap_zero(alloc->table, num);\r\nfor (i = 0; i < reserved; ++i)\r\nset_bit(i, alloc->table);\r\nreturn 0;\r\n}\r\nvoid mthca_alloc_cleanup(struct mthca_alloc *alloc)\r\n{\r\nkfree(alloc->table);\r\n}\r\nvoid *mthca_array_get(struct mthca_array *array, int index)\r\n{\r\nint p = (index * sizeof (void *)) >> PAGE_SHIFT;\r\nif (array->page_list[p].page)\r\nreturn array->page_list[p].page[index & MTHCA_ARRAY_MASK];\r\nelse\r\nreturn NULL;\r\n}\r\nint mthca_array_set(struct mthca_array *array, int index, void *value)\r\n{\r\nint p = (index * sizeof (void *)) >> PAGE_SHIFT;\r\nif (!array->page_list[p].page)\r\narray->page_list[p].page = (void **) get_zeroed_page(GFP_ATOMIC);\r\nif (!array->page_list[p].page)\r\nreturn -ENOMEM;\r\narray->page_list[p].page[index & MTHCA_ARRAY_MASK] = value;\r\n++array->page_list[p].used;\r\nreturn 0;\r\n}\r\nvoid mthca_array_clear(struct mthca_array *array, int index)\r\n{\r\nint p = (index * sizeof (void *)) >> PAGE_SHIFT;\r\nif (--array->page_list[p].used == 0) {\r\nfree_page((unsigned long) array->page_list[p].page);\r\narray->page_list[p].page = NULL;\r\n} else\r\narray->page_list[p].page[index & MTHCA_ARRAY_MASK] = NULL;\r\nif (array->page_list[p].used < 0)\r\npr_debug("Array %p index %d page %d with ref count %d < 0\n",\r\narray, index, p, array->page_list[p].used);\r\n}\r\nint mthca_array_init(struct mthca_array *array, int nent)\r\n{\r\nint npage = (nent * sizeof (void *) + PAGE_SIZE - 1) / PAGE_SIZE;\r\nint i;\r\narray->page_list = kmalloc(npage * sizeof *array->page_list, GFP_KERNEL);\r\nif (!array->page_list)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < npage; ++i) {\r\narray->page_list[i].page = NULL;\r\narray->page_list[i].used = 0;\r\n}\r\nreturn 0;\r\n}\r\nvoid mthca_array_cleanup(struct mthca_array *array, int nent)\r\n{\r\nint i;\r\nfor (i = 0; i < (nent * sizeof (void *) + PAGE_SIZE - 1) / PAGE_SIZE; ++i)\r\nfree_page((unsigned long) array->page_list[i].page);\r\nkfree(array->page_list);\r\n}\r\nint mthca_buf_alloc(struct mthca_dev *dev, int size, int max_direct,\r\nunion mthca_buf *buf, int *is_direct, struct mthca_pd *pd,\r\nint hca_write, struct mthca_mr *mr)\r\n{\r\nint err = -ENOMEM;\r\nint npages, shift;\r\nu64 *dma_list = NULL;\r\ndma_addr_t t;\r\nint i;\r\nif (size <= max_direct) {\r\n*is_direct = 1;\r\nnpages = 1;\r\nshift = get_order(size) + PAGE_SHIFT;\r\nbuf->direct.buf = dma_alloc_coherent(&dev->pdev->dev,\r\nsize, &t, GFP_KERNEL);\r\nif (!buf->direct.buf)\r\nreturn -ENOMEM;\r\ndma_unmap_addr_set(&buf->direct, mapping, t);\r\nmemset(buf->direct.buf, 0, size);\r\nwhile (t & ((1 << shift) - 1)) {\r\n--shift;\r\nnpages *= 2;\r\n}\r\ndma_list = kmalloc(npages * sizeof *dma_list, GFP_KERNEL);\r\nif (!dma_list)\r\ngoto err_free;\r\nfor (i = 0; i < npages; ++i)\r\ndma_list[i] = t + i * (1 << shift);\r\n} else {\r\n*is_direct = 0;\r\nnpages = (size + PAGE_SIZE - 1) / PAGE_SIZE;\r\nshift = PAGE_SHIFT;\r\ndma_list = kmalloc(npages * sizeof *dma_list, GFP_KERNEL);\r\nif (!dma_list)\r\nreturn -ENOMEM;\r\nbuf->page_list = kmalloc(npages * sizeof *buf->page_list,\r\nGFP_KERNEL);\r\nif (!buf->page_list)\r\ngoto err_out;\r\nfor (i = 0; i < npages; ++i)\r\nbuf->page_list[i].buf = NULL;\r\nfor (i = 0; i < npages; ++i) {\r\nbuf->page_list[i].buf =\r\ndma_alloc_coherent(&dev->pdev->dev, PAGE_SIZE,\r\n&t, GFP_KERNEL);\r\nif (!buf->page_list[i].buf)\r\ngoto err_free;\r\ndma_list[i] = t;\r\ndma_unmap_addr_set(&buf->page_list[i], mapping, t);\r\nclear_page(buf->page_list[i].buf);\r\n}\r\n}\r\nerr = mthca_mr_alloc_phys(dev, pd->pd_num,\r\ndma_list, shift, npages,\r\n0, size,\r\nMTHCA_MPT_FLAG_LOCAL_READ |\r\n(hca_write ? MTHCA_MPT_FLAG_LOCAL_WRITE : 0),\r\nmr);\r\nif (err)\r\ngoto err_free;\r\nkfree(dma_list);\r\nreturn 0;\r\nerr_free:\r\nmthca_buf_free(dev, size, buf, *is_direct, NULL);\r\nerr_out:\r\nkfree(dma_list);\r\nreturn err;\r\n}\r\nvoid mthca_buf_free(struct mthca_dev *dev, int size, union mthca_buf *buf,\r\nint is_direct, struct mthca_mr *mr)\r\n{\r\nint i;\r\nif (mr)\r\nmthca_free_mr(dev, mr);\r\nif (is_direct)\r\ndma_free_coherent(&dev->pdev->dev, size, buf->direct.buf,\r\ndma_unmap_addr(&buf->direct, mapping));\r\nelse {\r\nfor (i = 0; i < (size + PAGE_SIZE - 1) / PAGE_SIZE; ++i)\r\ndma_free_coherent(&dev->pdev->dev, PAGE_SIZE,\r\nbuf->page_list[i].buf,\r\ndma_unmap_addr(&buf->page_list[i],\r\nmapping));\r\nkfree(buf->page_list);\r\n}\r\n}
