static void\r\nvxge_hw_vpath_set_zero_rx_frm_len(struct vxge_hw_vpath_reg __iomem *vp_reg)\r\n{\r\nu64 val64;\r\nval64 = readq(&vp_reg->rxmac_vcfg0);\r\nval64 &= ~VXGE_HW_RXMAC_VCFG0_RTS_MAX_FRM_LEN(0x3fff);\r\nwriteq(val64, &vp_reg->rxmac_vcfg0);\r\nval64 = readq(&vp_reg->rxmac_vcfg0);\r\n}\r\nint vxge_hw_vpath_wait_receive_idle(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nu64 val64, rxd_count, rxd_spat;\r\nint count = 0, total_count = 0;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nvp_reg = vpath->vp_reg;\r\nvxge_hw_vpath_set_zero_rx_frm_len(vp_reg);\r\nval64 = readq(&vp_reg->prc_cfg6);\r\nrxd_spat = VXGE_HW_PRC_CFG6_GET_RXD_SPAT(val64) + 1;\r\nrxd_spat *= 2;\r\ndo {\r\nmdelay(1);\r\nrxd_count = readq(&vp_reg->prc_rxd_doorbell);\r\nval64 = readq(&vp_reg->frm_in_progress_cnt);\r\nif ((rxd_count <= rxd_spat) || (val64 > 0))\r\ncount = 0;\r\nelse\r\ncount++;\r\ntotal_count++;\r\n} while ((count < VXGE_HW_MIN_SUCCESSIVE_IDLE_COUNT) &&\r\n(total_count < VXGE_HW_MAX_POLLING_COUNT));\r\nif (total_count >= VXGE_HW_MAX_POLLING_COUNT)\r\nprintk(KERN_ALERT "%s: Still Receiving traffic. Abort wait\n",\r\n__func__);\r\nreturn total_count;\r\n}\r\nvoid vxge_hw_device_wait_receive_idle(struct __vxge_hw_device *hldev)\r\n{\r\nint i, total_count = 0;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpaths_deployed & vxge_mBIT(i)))\r\ncontinue;\r\ntotal_count += vxge_hw_vpath_wait_receive_idle(hldev, i);\r\nif (total_count >= VXGE_HW_MAX_POLLING_COUNT)\r\nbreak;\r\n}\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_register_poll(void __iomem *reg, u64 mask, u32 max_millis)\r\n{\r\nu64 val64;\r\nu32 i = 0;\r\nenum vxge_hw_status ret = VXGE_HW_FAIL;\r\nudelay(10);\r\ndo {\r\nval64 = readq(reg);\r\nif (!(val64 & mask))\r\nreturn VXGE_HW_OK;\r\nudelay(100);\r\n} while (++i <= 9);\r\ni = 0;\r\ndo {\r\nval64 = readq(reg);\r\nif (!(val64 & mask))\r\nreturn VXGE_HW_OK;\r\nmdelay(1);\r\n} while (++i <= max_millis);\r\nreturn ret;\r\n}\r\nstatic inline enum vxge_hw_status\r\n__vxge_hw_pio_mem_write64(u64 val64, void __iomem *addr,\r\nu64 mask, u32 max_millis)\r\n{\r\n__vxge_hw_pio_mem_write32_lower((u32)vxge_bVALn(val64, 32, 32), addr);\r\nwmb();\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(val64, 0, 32), addr);\r\nwmb();\r\nreturn __vxge_hw_device_register_poll(addr, mask, max_millis);\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_hw_vpath_fw_api(struct __vxge_hw_virtualpath *vpath, u32 action,\r\nu32 fw_memo, u32 offset, u64 *data0, u64 *data1,\r\nu64 *steer_ctrl)\r\n{\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg = vpath->vp_reg;\r\nenum vxge_hw_status status;\r\nu64 val64;\r\nu32 retry = 0, max_retry = 3;\r\nspin_lock(&vpath->lock);\r\nif (!vpath->vp_open) {\r\nspin_unlock(&vpath->lock);\r\nmax_retry = 100;\r\n}\r\nwriteq(*data0, &vp_reg->rts_access_steer_data0);\r\nwriteq(*data1, &vp_reg->rts_access_steer_data1);\r\nwmb();\r\nval64 = VXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION(action) |\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL(fw_memo) |\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_OFFSET(offset) |\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_STROBE |\r\n*steer_ctrl;\r\nstatus = __vxge_hw_pio_mem_write64(val64,\r\n&vp_reg->rts_access_steer_ctrl,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_STROBE,\r\nVXGE_HW_DEF_DEVICE_POLL_MILLIS);\r\nwhile ((status != VXGE_HW_OK) && retry++ < max_retry) {\r\nif (!vpath->vp_open)\r\nmsleep(20);\r\nstatus = __vxge_hw_device_register_poll(\r\n&vp_reg->rts_access_steer_ctrl,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_STROBE,\r\nVXGE_HW_DEF_DEVICE_POLL_MILLIS);\r\n}\r\nif (status != VXGE_HW_OK)\r\ngoto out;\r\nval64 = readq(&vp_reg->rts_access_steer_ctrl);\r\nif (val64 & VXGE_HW_RTS_ACCESS_STEER_CTRL_RMACJ_STATUS) {\r\n*data0 = readq(&vp_reg->rts_access_steer_data0);\r\n*data1 = readq(&vp_reg->rts_access_steer_data1);\r\n*steer_ctrl = val64;\r\n} else\r\nstatus = VXGE_HW_FAIL;\r\nout:\r\nif (vpath->vp_open)\r\nspin_unlock(&vpath->lock);\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_upgrade_read_version(struct __vxge_hw_device *hldev, u32 *major,\r\nu32 *minor, u32 *build)\r\n{\r\nu64 data0 = 0, data1 = 0, steer_ctrl = 0;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status;\r\nvpath = &hldev->virtual_paths[hldev->first_vp_id];\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_FW_UPGRADE_ACTION,\r\nVXGE_HW_FW_UPGRADE_MEMO,\r\nVXGE_HW_FW_UPGRADE_OFFSET_READ,\r\n&data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\nreturn status;\r\n*major = VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_MAJOR(data0);\r\n*minor = VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_MINOR(data0);\r\n*build = VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_BUILD(data0);\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_flash_fw(struct __vxge_hw_device *hldev)\r\n{\r\nu64 data0 = 0, data1 = 0, steer_ctrl = 0;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status;\r\nu32 ret;\r\nvpath = &hldev->virtual_paths[hldev->first_vp_id];\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_FW_UPGRADE_ACTION,\r\nVXGE_HW_FW_UPGRADE_MEMO,\r\nVXGE_HW_FW_UPGRADE_OFFSET_COMMIT,\r\n&data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR, "%s: FW upgrade failed", __func__);\r\ngoto exit;\r\n}\r\nret = VXGE_HW_RTS_ACCESS_STEER_CTRL_GET_ACTION(steer_ctrl) & 0x7F;\r\nif (ret != 1) {\r\nvxge_debug_init(VXGE_ERR, "%s: FW commit failed with error %d",\r\n__func__, ret);\r\nstatus = VXGE_HW_FAIL;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_update_fw_image(struct __vxge_hw_device *hldev, const u8 *fwdata, int size)\r\n{\r\nu64 data0 = 0, data1 = 0, steer_ctrl = 0;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status;\r\nint ret_code, sec_code;\r\nvpath = &hldev->virtual_paths[hldev->first_vp_id];\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_FW_UPGRADE_ACTION,\r\nVXGE_HW_FW_UPGRADE_MEMO,\r\nVXGE_HW_FW_UPGRADE_OFFSET_START,\r\n&data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR, " %s: Upgrade start cmd failed",\r\n__func__);\r\nreturn status;\r\n}\r\nfor (; size > 0; size -= VXGE_HW_FW_UPGRADE_BLK_SIZE) {\r\nsteer_ctrl = 0;\r\ndata0 = *((u64 *)fwdata);\r\ndata1 = *((u64 *)fwdata + 1);\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_FW_UPGRADE_ACTION,\r\nVXGE_HW_FW_UPGRADE_MEMO,\r\nVXGE_HW_FW_UPGRADE_OFFSET_SEND,\r\n&data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR, "%s: Upgrade send failed",\r\n__func__);\r\ngoto out;\r\n}\r\nret_code = VXGE_HW_UPGRADE_GET_RET_ERR_CODE(data0);\r\nswitch (ret_code) {\r\ncase VXGE_HW_FW_UPGRADE_OK:\r\nbreak;\r\ncase VXGE_FW_UPGRADE_BYTES2SKIP:\r\nfwdata += (data0 >> 8) & 0xFFFFFFFF;\r\nbreak;\r\ncase VXGE_HW_FW_UPGRADE_DONE:\r\ngoto out;\r\ncase VXGE_HW_FW_UPGRADE_ERR:\r\nsec_code = VXGE_HW_UPGRADE_GET_SEC_ERR_CODE(data0);\r\nswitch (sec_code) {\r\ncase VXGE_HW_FW_UPGRADE_ERR_CORRUPT_DATA_1:\r\ncase VXGE_HW_FW_UPGRADE_ERR_CORRUPT_DATA_7:\r\nprintk(KERN_ERR\r\n"corrupted data from .ncf file\n");\r\nbreak;\r\ncase VXGE_HW_FW_UPGRADE_ERR_INV_NCF_FILE_3:\r\ncase VXGE_HW_FW_UPGRADE_ERR_INV_NCF_FILE_4:\r\ncase VXGE_HW_FW_UPGRADE_ERR_INV_NCF_FILE_5:\r\ncase VXGE_HW_FW_UPGRADE_ERR_INV_NCF_FILE_6:\r\ncase VXGE_HW_FW_UPGRADE_ERR_INV_NCF_FILE_8:\r\nprintk(KERN_ERR "invalid .ncf file\n");\r\nbreak;\r\ncase VXGE_HW_FW_UPGRADE_ERR_BUFFER_OVERFLOW:\r\nprintk(KERN_ERR "buffer overflow\n");\r\nbreak;\r\ncase VXGE_HW_FW_UPGRADE_ERR_FAILED_TO_FLASH:\r\nprintk(KERN_ERR "failed to flash the image\n");\r\nbreak;\r\ncase VXGE_HW_FW_UPGRADE_ERR_GENERIC_ERROR_UNKNOWN:\r\nprintk(KERN_ERR\r\n"generic error. Unknown error type\n");\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "Unknown error of type %d\n",\r\nsec_code);\r\nbreak;\r\n}\r\nstatus = VXGE_HW_FAIL;\r\ngoto out;\r\ndefault:\r\nprintk(KERN_ERR "Unknown FW error: %d\n", ret_code);\r\nstatus = VXGE_HW_FAIL;\r\ngoto out;\r\n}\r\nfwdata += VXGE_HW_FW_UPGRADE_BLK_SIZE;\r\n}\r\nout:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_eprom_img_ver_get(struct __vxge_hw_device *hldev,\r\nstruct eprom_image *img)\r\n{\r\nu64 data0 = 0, data1 = 0, steer_ctrl = 0;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status;\r\nint i;\r\nvpath = &hldev->virtual_paths[hldev->first_vp_id];\r\nfor (i = 0; i < VXGE_HW_MAX_ROM_IMAGES; i++) {\r\ndata0 = VXGE_HW_RTS_ACCESS_STEER_ROM_IMAGE_INDEX(i);\r\ndata1 = steer_ctrl = 0;\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_FW_API_GET_EPROM_REV,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_FW_MEMO,\r\n0, &data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\nbreak;\r\nimg[i].is_valid = VXGE_HW_GET_EPROM_IMAGE_VALID(data0);\r\nimg[i].index = VXGE_HW_GET_EPROM_IMAGE_INDEX(data0);\r\nimg[i].type = VXGE_HW_GET_EPROM_IMAGE_TYPE(data0);\r\nimg[i].version = VXGE_HW_GET_EPROM_IMAGE_REV(data0);\r\n}\r\nreturn status;\r\n}\r\nstatic void __vxge_hw_channel_free(struct __vxge_hw_channel *channel)\r\n{\r\nkfree(channel->work_arr);\r\nkfree(channel->free_arr);\r\nkfree(channel->reserve_arr);\r\nkfree(channel->orig_arr);\r\nkfree(channel);\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_channel_initialize(struct __vxge_hw_channel *channel)\r\n{\r\nu32 i;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nvpath = channel->vph->vpath;\r\nif ((channel->reserve_arr != NULL) && (channel->orig_arr != NULL)) {\r\nfor (i = 0; i < channel->length; i++)\r\nchannel->orig_arr[i] = channel->reserve_arr[i];\r\n}\r\nswitch (channel->type) {\r\ncase VXGE_HW_CHANNEL_TYPE_FIFO:\r\nvpath->fifoh = (struct __vxge_hw_fifo *)channel;\r\nchannel->stats = &((struct __vxge_hw_fifo *)\r\nchannel)->stats->common_stats;\r\nbreak;\r\ncase VXGE_HW_CHANNEL_TYPE_RING:\r\nvpath->ringh = (struct __vxge_hw_ring *)channel;\r\nchannel->stats = &((struct __vxge_hw_ring *)\r\nchannel)->stats->common_stats;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_channel_reset(struct __vxge_hw_channel *channel)\r\n{\r\nu32 i;\r\nfor (i = 0; i < channel->length; i++) {\r\nif (channel->reserve_arr != NULL)\r\nchannel->reserve_arr[i] = channel->orig_arr[i];\r\nif (channel->free_arr != NULL)\r\nchannel->free_arr[i] = NULL;\r\nif (channel->work_arr != NULL)\r\nchannel->work_arr[i] = NULL;\r\n}\r\nchannel->free_ptr = channel->length;\r\nchannel->reserve_ptr = channel->length;\r\nchannel->reserve_top = 0;\r\nchannel->post_index = 0;\r\nchannel->compl_index = 0;\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic void __vxge_hw_device_pci_e_init(struct __vxge_hw_device *hldev)\r\n{\r\nu16 cmd = 0;\r\npci_read_config_word(hldev->pdev, PCI_COMMAND, &cmd);\r\ncmd |= 0x140;\r\npci_write_config_word(hldev->pdev, PCI_COMMAND, cmd);\r\npci_save_state(hldev->pdev);\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_vpath_reset_in_prog_check(u64 __iomem *vpath_rst_in_prog)\r\n{\r\nenum vxge_hw_status status;\r\nstatus = __vxge_hw_device_register_poll(vpath_rst_in_prog,\r\nVXGE_HW_VPATH_RST_IN_PROG_VPATH_RST_IN_PROG(0x1ffff),\r\nVXGE_HW_DEF_DEVICE_POLL_MILLIS);\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_legacy_swapper_set(struct vxge_hw_legacy_reg __iomem *legacy_reg)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nval64 = readq(&legacy_reg->toc_swapper_fb);\r\nwmb();\r\nswitch (val64) {\r\ncase VXGE_HW_SWAPPER_INITIAL_VALUE:\r\nreturn status;\r\ncase VXGE_HW_SWAPPER_BYTE_SWAPPED_BIT_FLIPPED:\r\nwriteq(VXGE_HW_SWAPPER_READ_BYTE_SWAP_ENABLE,\r\n&legacy_reg->pifm_rd_swap_en);\r\nwriteq(VXGE_HW_SWAPPER_READ_BIT_FLAP_ENABLE,\r\n&legacy_reg->pifm_rd_flip_en);\r\nwriteq(VXGE_HW_SWAPPER_WRITE_BYTE_SWAP_ENABLE,\r\n&legacy_reg->pifm_wr_swap_en);\r\nwriteq(VXGE_HW_SWAPPER_WRITE_BIT_FLAP_ENABLE,\r\n&legacy_reg->pifm_wr_flip_en);\r\nbreak;\r\ncase VXGE_HW_SWAPPER_BYTE_SWAPPED:\r\nwriteq(VXGE_HW_SWAPPER_READ_BYTE_SWAP_ENABLE,\r\n&legacy_reg->pifm_rd_swap_en);\r\nwriteq(VXGE_HW_SWAPPER_WRITE_BYTE_SWAP_ENABLE,\r\n&legacy_reg->pifm_wr_swap_en);\r\nbreak;\r\ncase VXGE_HW_SWAPPER_BIT_FLIPPED:\r\nwriteq(VXGE_HW_SWAPPER_READ_BIT_FLAP_ENABLE,\r\n&legacy_reg->pifm_rd_flip_en);\r\nwriteq(VXGE_HW_SWAPPER_WRITE_BIT_FLAP_ENABLE,\r\n&legacy_reg->pifm_wr_flip_en);\r\nbreak;\r\n}\r\nwmb();\r\nval64 = readq(&legacy_reg->toc_swapper_fb);\r\nif (val64 != VXGE_HW_SWAPPER_INITIAL_VALUE)\r\nstatus = VXGE_HW_ERR_SWAPPER_CTRL;\r\nreturn status;\r\n}\r\nstatic struct vxge_hw_toc_reg __iomem *\r\n__vxge_hw_device_toc_get(void __iomem *bar0)\r\n{\r\nu64 val64;\r\nstruct vxge_hw_toc_reg __iomem *toc = NULL;\r\nenum vxge_hw_status status;\r\nstruct vxge_hw_legacy_reg __iomem *legacy_reg =\r\n(struct vxge_hw_legacy_reg __iomem *)bar0;\r\nstatus = __vxge_hw_legacy_swapper_set(legacy_reg);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nval64 = readq(&legacy_reg->toc_first_pointer);\r\ntoc = bar0 + val64;\r\nexit:\r\nreturn toc;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_reg_addr_get(struct __vxge_hw_device *hldev)\r\n{\r\nu64 val64;\r\nu32 i;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nhldev->legacy_reg = hldev->bar0;\r\nhldev->toc_reg = __vxge_hw_device_toc_get(hldev->bar0);\r\nif (hldev->toc_reg == NULL) {\r\nstatus = VXGE_HW_FAIL;\r\ngoto exit;\r\n}\r\nval64 = readq(&hldev->toc_reg->toc_common_pointer);\r\nhldev->common_reg = hldev->bar0 + val64;\r\nval64 = readq(&hldev->toc_reg->toc_mrpcim_pointer);\r\nhldev->mrpcim_reg = hldev->bar0 + val64;\r\nfor (i = 0; i < VXGE_HW_TITAN_SRPCIM_REG_SPACES; i++) {\r\nval64 = readq(&hldev->toc_reg->toc_srpcim_pointer[i]);\r\nhldev->srpcim_reg[i] = hldev->bar0 + val64;\r\n}\r\nfor (i = 0; i < VXGE_HW_TITAN_VPMGMT_REG_SPACES; i++) {\r\nval64 = readq(&hldev->toc_reg->toc_vpmgmt_pointer[i]);\r\nhldev->vpmgmt_reg[i] = hldev->bar0 + val64;\r\n}\r\nfor (i = 0; i < VXGE_HW_TITAN_VPATH_REG_SPACES; i++) {\r\nval64 = readq(&hldev->toc_reg->toc_vpath_pointer[i]);\r\nhldev->vpath_reg[i] = hldev->bar0 + val64;\r\n}\r\nval64 = readq(&hldev->toc_reg->toc_kdfc);\r\nswitch (VXGE_HW_TOC_GET_KDFC_INITIAL_BIR(val64)) {\r\ncase 0:\r\nhldev->kdfc = hldev->bar0 + VXGE_HW_TOC_GET_KDFC_INITIAL_OFFSET(val64) ;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nstatus = __vxge_hw_device_vpath_reset_in_prog_check(\r\n(u64 __iomem *)&hldev->common_reg->vpath_rst_in_prog);\r\nexit:\r\nreturn status;\r\n}\r\nstatic u32\r\n__vxge_hw_device_access_rights_get(u32 host_type, u32 func_id)\r\n{\r\nu32 access_rights = VXGE_HW_DEVICE_ACCESS_RIGHT_VPATH;\r\nswitch (host_type) {\r\ncase VXGE_HW_NO_MR_NO_SR_NORMAL_FUNCTION:\r\nif (func_id == 0) {\r\naccess_rights |= VXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM |\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_SRPCIM;\r\n}\r\nbreak;\r\ncase VXGE_HW_MR_NO_SR_VH0_BASE_FUNCTION:\r\naccess_rights |= VXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM |\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_SRPCIM;\r\nbreak;\r\ncase VXGE_HW_NO_MR_SR_VH0_FUNCTION0:\r\naccess_rights |= VXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM |\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_SRPCIM;\r\nbreak;\r\ncase VXGE_HW_NO_MR_SR_VH0_VIRTUAL_FUNCTION:\r\ncase VXGE_HW_SR_VH_VIRTUAL_FUNCTION:\r\ncase VXGE_HW_MR_SR_VH0_INVALID_CONFIG:\r\nbreak;\r\ncase VXGE_HW_SR_VH_FUNCTION0:\r\ncase VXGE_HW_VH_NORMAL_FUNCTION:\r\naccess_rights |= VXGE_HW_DEVICE_ACCESS_RIGHT_SRPCIM;\r\nbreak;\r\n}\r\nreturn access_rights;\r\n}\r\nenum vxge_hw_status\r\n__vxge_hw_device_is_privilaged(u32 host_type, u32 func_id)\r\n{\r\nif (__vxge_hw_device_access_rights_get(host_type,\r\nfunc_id) &\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM)\r\nreturn VXGE_HW_OK;\r\nelse\r\nreturn VXGE_HW_ERR_PRIVILAGED_OPEARATION;\r\n}\r\nstatic u32\r\n__vxge_hw_vpath_func_id_get(struct vxge_hw_vpmgmt_reg __iomem *vpmgmt_reg)\r\n{\r\nu64 val64;\r\nval64 = readq(&vpmgmt_reg->vpath_to_func_map_cfg1);\r\nreturn\r\n(u32)VXGE_HW_VPATH_TO_FUNC_MAP_CFG1_GET_VPATH_TO_FUNC_MAP_CFG1(val64);\r\n}\r\nstatic void __vxge_hw_device_host_info_get(struct __vxge_hw_device *hldev)\r\n{\r\nu64 val64;\r\nu32 i;\r\nval64 = readq(&hldev->common_reg->host_type_assignments);\r\nhldev->host_type =\r\n(u32)VXGE_HW_HOST_TYPE_ASSIGNMENTS_GET_HOST_TYPE_ASSIGNMENTS(val64);\r\nhldev->vpath_assignments = readq(&hldev->common_reg->vpath_assignments);\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpath_assignments & vxge_mBIT(i)))\r\ncontinue;\r\nhldev->func_id =\r\n__vxge_hw_vpath_func_id_get(hldev->vpmgmt_reg[i]);\r\nhldev->access_rights = __vxge_hw_device_access_rights_get(\r\nhldev->host_type, hldev->func_id);\r\nhldev->virtual_paths[i].vp_open = VXGE_HW_VP_NOT_OPEN;\r\nhldev->virtual_paths[i].vp_reg = hldev->vpath_reg[i];\r\nhldev->first_vp_id = i;\r\nbreak;\r\n}\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_verify_pci_e_info(struct __vxge_hw_device *hldev)\r\n{\r\nstruct pci_dev *dev = hldev->pdev;\r\nu16 lnk;\r\npcie_capability_read_word(dev, PCI_EXP_LNKSTA, &lnk);\r\nif ((lnk & PCI_EXP_LNKSTA_CLS) != 1)\r\nreturn VXGE_HW_ERR_INVALID_PCI_INFO;\r\nswitch ((lnk & PCI_EXP_LNKSTA_NLW) >> 4) {\r\ncase PCIE_LNK_WIDTH_RESRV:\r\ncase PCIE_LNK_X1:\r\ncase PCIE_LNK_X2:\r\ncase PCIE_LNK_X4:\r\ncase PCIE_LNK_X8:\r\nbreak;\r\ndefault:\r\nreturn VXGE_HW_ERR_INVALID_PCI_INFO;\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_initialize(struct __vxge_hw_device *hldev)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (VXGE_HW_OK == __vxge_hw_device_is_privilaged(hldev->host_type,\r\nhldev->func_id)) {\r\nstatus = __vxge_hw_verify_pci_e_info(hldev);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_fw_ver_get(struct __vxge_hw_virtualpath *vpath,\r\nstruct vxge_hw_device_hw_info *hw_info)\r\n{\r\nstruct vxge_hw_device_version *fw_version = &hw_info->fw_version;\r\nstruct vxge_hw_device_date *fw_date = &hw_info->fw_date;\r\nstruct vxge_hw_device_version *flash_version = &hw_info->flash_version;\r\nstruct vxge_hw_device_date *flash_date = &hw_info->flash_date;\r\nu64 data0, data1 = 0, steer_ctrl = 0;\r\nenum vxge_hw_status status;\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_READ_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_FW_MEMO,\r\n0, &data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nfw_date->day =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_DAY(data0);\r\nfw_date->month =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_MONTH(data0);\r\nfw_date->year =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_YEAR(data0);\r\nsnprintf(fw_date->date, VXGE_HW_FW_STRLEN, "%2.2d/%2.2d/%4.4d",\r\nfw_date->month, fw_date->day, fw_date->year);\r\nfw_version->major =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_MAJOR(data0);\r\nfw_version->minor =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_MINOR(data0);\r\nfw_version->build =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_FW_VER_BUILD(data0);\r\nsnprintf(fw_version->version, VXGE_HW_FW_STRLEN, "%d.%d.%d",\r\nfw_version->major, fw_version->minor, fw_version->build);\r\nflash_date->day =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA1_GET_FLASH_VER_DAY(data1);\r\nflash_date->month =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA1_GET_FLASH_VER_MONTH(data1);\r\nflash_date->year =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA1_GET_FLASH_VER_YEAR(data1);\r\nsnprintf(flash_date->date, VXGE_HW_FW_STRLEN, "%2.2d/%2.2d/%4.4d",\r\nflash_date->month, flash_date->day, flash_date->year);\r\nflash_version->major =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA1_GET_FLASH_VER_MAJOR(data1);\r\nflash_version->minor =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA1_GET_FLASH_VER_MINOR(data1);\r\nflash_version->build =\r\n(u32) VXGE_HW_RTS_ACCESS_STEER_DATA1_GET_FLASH_VER_BUILD(data1);\r\nsnprintf(flash_version->version, VXGE_HW_FW_STRLEN, "%d.%d.%d",\r\nflash_version->major, flash_version->minor,\r\nflash_version->build);\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_card_info_get(struct __vxge_hw_virtualpath *vpath,\r\nstruct vxge_hw_device_hw_info *hw_info)\r\n{\r\nenum vxge_hw_status status;\r\nu64 data0, data1 = 0, steer_ctrl = 0;\r\nu8 *serial_number = hw_info->serial_number;\r\nu8 *part_number = hw_info->part_number;\r\nu8 *product_desc = hw_info->product_desc;\r\nu32 i, j = 0;\r\ndata0 = VXGE_HW_RTS_ACCESS_STEER_DATA0_MEMO_ITEM_SERIAL_NUMBER;\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_READ_MEMO_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_FW_MEMO,\r\n0, &data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\nreturn status;\r\n((u64 *)serial_number)[0] = be64_to_cpu(data0);\r\n((u64 *)serial_number)[1] = be64_to_cpu(data1);\r\ndata0 = VXGE_HW_RTS_ACCESS_STEER_DATA0_MEMO_ITEM_PART_NUMBER;\r\ndata1 = steer_ctrl = 0;\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_READ_MEMO_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_FW_MEMO,\r\n0, &data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\nreturn status;\r\n((u64 *)part_number)[0] = be64_to_cpu(data0);\r\n((u64 *)part_number)[1] = be64_to_cpu(data1);\r\nfor (i = VXGE_HW_RTS_ACCESS_STEER_DATA0_MEMO_ITEM_DESC_0;\r\ni <= VXGE_HW_RTS_ACCESS_STEER_DATA0_MEMO_ITEM_DESC_3; i++) {\r\ndata0 = i;\r\ndata1 = steer_ctrl = 0;\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_READ_MEMO_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_FW_MEMO,\r\n0, &data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\nreturn status;\r\n((u64 *)product_desc)[j++] = be64_to_cpu(data0);\r\n((u64 *)product_desc)[j++] = be64_to_cpu(data1);\r\n}\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_pci_func_mode_get(struct __vxge_hw_virtualpath *vpath,\r\nstruct vxge_hw_device_hw_info *hw_info)\r\n{\r\nu64 data0, data1 = 0, steer_ctrl = 0;\r\nenum vxge_hw_status status;\r\ndata0 = 0;\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_FW_API_GET_FUNC_MODE,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_FW_MEMO,\r\n0, &data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\nreturn status;\r\nhw_info->function_mode = VXGE_HW_GET_FUNC_MODE_VAL(data0);\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_addr_get(struct __vxge_hw_virtualpath *vpath,\r\nu8 *macaddr, u8 *macaddr_mask)\r\n{\r\nu64 action = VXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_LIST_FIRST_ENTRY,\r\ndata0 = 0, data1 = 0, steer_ctrl = 0;\r\nenum vxge_hw_status status;\r\nint i;\r\ndo {\r\nstatus = vxge_hw_vpath_fw_api(vpath, action,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_DA,\r\n0, &data0, &data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\ndata0 = VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_DA_MAC_ADDR(data0);\r\ndata1 = VXGE_HW_RTS_ACCESS_STEER_DATA1_GET_DA_MAC_ADDR_MASK(\r\ndata1);\r\nfor (i = ETH_ALEN; i > 0; i--) {\r\nmacaddr[i - 1] = (u8) (data0 & 0xFF);\r\ndata0 >>= 8;\r\nmacaddr_mask[i - 1] = (u8) (data1 & 0xFF);\r\ndata1 >>= 8;\r\n}\r\naction = VXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_LIST_NEXT_ENTRY;\r\ndata0 = 0, data1 = 0, steer_ctrl = 0;\r\n} while (!is_valid_ether_addr(macaddr));\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_device_hw_info_get(void __iomem *bar0,\r\nstruct vxge_hw_device_hw_info *hw_info)\r\n{\r\nu32 i;\r\nu64 val64;\r\nstruct vxge_hw_toc_reg __iomem *toc;\r\nstruct vxge_hw_mrpcim_reg __iomem *mrpcim_reg;\r\nstruct vxge_hw_common_reg __iomem *common_reg;\r\nstruct vxge_hw_vpmgmt_reg __iomem *vpmgmt_reg;\r\nenum vxge_hw_status status;\r\nstruct __vxge_hw_virtualpath vpath;\r\nmemset(hw_info, 0, sizeof(struct vxge_hw_device_hw_info));\r\ntoc = __vxge_hw_device_toc_get(bar0);\r\nif (toc == NULL) {\r\nstatus = VXGE_HW_ERR_CRITICAL;\r\ngoto exit;\r\n}\r\nval64 = readq(&toc->toc_common_pointer);\r\ncommon_reg = bar0 + val64;\r\nstatus = __vxge_hw_device_vpath_reset_in_prog_check(\r\n(u64 __iomem *)&common_reg->vpath_rst_in_prog);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nhw_info->vpath_mask = readq(&common_reg->vpath_assignments);\r\nval64 = readq(&common_reg->host_type_assignments);\r\nhw_info->host_type =\r\n(u32)VXGE_HW_HOST_TYPE_ASSIGNMENTS_GET_HOST_TYPE_ASSIGNMENTS(val64);\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!((hw_info->vpath_mask) & vxge_mBIT(i)))\r\ncontinue;\r\nval64 = readq(&toc->toc_vpmgmt_pointer[i]);\r\nvpmgmt_reg = bar0 + val64;\r\nhw_info->func_id = __vxge_hw_vpath_func_id_get(vpmgmt_reg);\r\nif (__vxge_hw_device_access_rights_get(hw_info->host_type,\r\nhw_info->func_id) &\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM) {\r\nval64 = readq(&toc->toc_mrpcim_pointer);\r\nmrpcim_reg = bar0 + val64;\r\nwriteq(0, &mrpcim_reg->xgmac_gen_fw_memo_mask);\r\nwmb();\r\n}\r\nval64 = readq(&toc->toc_vpath_pointer[i]);\r\nspin_lock_init(&vpath.lock);\r\nvpath.vp_reg = bar0 + val64;\r\nvpath.vp_open = VXGE_HW_VP_NOT_OPEN;\r\nstatus = __vxge_hw_vpath_pci_func_mode_get(&vpath, hw_info);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_fw_ver_get(&vpath, hw_info);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_card_info_get(&vpath, hw_info);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nbreak;\r\n}\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!((hw_info->vpath_mask) & vxge_mBIT(i)))\r\ncontinue;\r\nval64 = readq(&toc->toc_vpath_pointer[i]);\r\nvpath.vp_reg = bar0 + val64;\r\nvpath.vp_open = VXGE_HW_VP_NOT_OPEN;\r\nstatus = __vxge_hw_vpath_addr_get(&vpath,\r\nhw_info->mac_addrs[i],\r\nhw_info->mac_addr_masks[i]);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic void __vxge_hw_blockpool_destroy(struct __vxge_hw_blockpool *blockpool)\r\n{\r\nstruct __vxge_hw_device *hldev;\r\nstruct list_head *p, *n;\r\nu16 ret;\r\nif (blockpool == NULL) {\r\nret = 1;\r\ngoto exit;\r\n}\r\nhldev = blockpool->hldev;\r\nlist_for_each_safe(p, n, &blockpool->free_block_list) {\r\npci_unmap_single(hldev->pdev,\r\n((struct __vxge_hw_blockpool_entry *)p)->dma_addr,\r\n((struct __vxge_hw_blockpool_entry *)p)->length,\r\nPCI_DMA_BIDIRECTIONAL);\r\nvxge_os_dma_free(hldev->pdev,\r\n((struct __vxge_hw_blockpool_entry *)p)->memblock,\r\n&((struct __vxge_hw_blockpool_entry *)p)->acc_handle);\r\nlist_del(&((struct __vxge_hw_blockpool_entry *)p)->item);\r\nkfree(p);\r\nblockpool->pool_size--;\r\n}\r\nlist_for_each_safe(p, n, &blockpool->free_entry_list) {\r\nlist_del(&((struct __vxge_hw_blockpool_entry *)p)->item);\r\nkfree((void *)p);\r\n}\r\nret = 0;\r\nexit:\r\nreturn;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_blockpool_create(struct __vxge_hw_device *hldev,\r\nstruct __vxge_hw_blockpool *blockpool,\r\nu32 pool_size,\r\nu32 pool_max)\r\n{\r\nu32 i;\r\nstruct __vxge_hw_blockpool_entry *entry = NULL;\r\nvoid *memblock;\r\ndma_addr_t dma_addr;\r\nstruct pci_dev *dma_handle;\r\nstruct pci_dev *acc_handle;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (blockpool == NULL) {\r\nstatus = VXGE_HW_FAIL;\r\ngoto blockpool_create_exit;\r\n}\r\nblockpool->hldev = hldev;\r\nblockpool->block_size = VXGE_HW_BLOCK_SIZE;\r\nblockpool->pool_size = 0;\r\nblockpool->pool_max = pool_max;\r\nblockpool->req_out = 0;\r\nINIT_LIST_HEAD(&blockpool->free_block_list);\r\nINIT_LIST_HEAD(&blockpool->free_entry_list);\r\nfor (i = 0; i < pool_size + pool_max; i++) {\r\nentry = kzalloc(sizeof(struct __vxge_hw_blockpool_entry),\r\nGFP_KERNEL);\r\nif (entry == NULL) {\r\n__vxge_hw_blockpool_destroy(blockpool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto blockpool_create_exit;\r\n}\r\nlist_add(&entry->item, &blockpool->free_entry_list);\r\n}\r\nfor (i = 0; i < pool_size; i++) {\r\nmemblock = vxge_os_dma_malloc(\r\nhldev->pdev,\r\nVXGE_HW_BLOCK_SIZE,\r\n&dma_handle,\r\n&acc_handle);\r\nif (memblock == NULL) {\r\n__vxge_hw_blockpool_destroy(blockpool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto blockpool_create_exit;\r\n}\r\ndma_addr = pci_map_single(hldev->pdev, memblock,\r\nVXGE_HW_BLOCK_SIZE, PCI_DMA_BIDIRECTIONAL);\r\nif (unlikely(pci_dma_mapping_error(hldev->pdev,\r\ndma_addr))) {\r\nvxge_os_dma_free(hldev->pdev, memblock, &acc_handle);\r\n__vxge_hw_blockpool_destroy(blockpool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto blockpool_create_exit;\r\n}\r\nif (!list_empty(&blockpool->free_entry_list))\r\nentry = (struct __vxge_hw_blockpool_entry *)\r\nlist_first_entry(&blockpool->free_entry_list,\r\nstruct __vxge_hw_blockpool_entry,\r\nitem);\r\nif (entry == NULL)\r\nentry =\r\nkzalloc(sizeof(struct __vxge_hw_blockpool_entry),\r\nGFP_KERNEL);\r\nif (entry != NULL) {\r\nlist_del(&entry->item);\r\nentry->length = VXGE_HW_BLOCK_SIZE;\r\nentry->memblock = memblock;\r\nentry->dma_addr = dma_addr;\r\nentry->acc_handle = acc_handle;\r\nentry->dma_handle = dma_handle;\r\nlist_add(&entry->item,\r\n&blockpool->free_block_list);\r\nblockpool->pool_size++;\r\n} else {\r\n__vxge_hw_blockpool_destroy(blockpool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto blockpool_create_exit;\r\n}\r\n}\r\nblockpool_create_exit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_fifo_config_check(struct vxge_hw_fifo_config *fifo_config)\r\n{\r\nif ((fifo_config->fifo_blocks < VXGE_HW_MIN_FIFO_BLOCKS) ||\r\n(fifo_config->fifo_blocks > VXGE_HW_MAX_FIFO_BLOCKS))\r\nreturn VXGE_HW_BADCFG_FIFO_BLOCKS;\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_vpath_config_check(struct vxge_hw_vp_config *vp_config)\r\n{\r\nenum vxge_hw_status status;\r\nif ((vp_config->min_bandwidth < VXGE_HW_VPATH_BANDWIDTH_MIN) ||\r\n(vp_config->min_bandwidth > VXGE_HW_VPATH_BANDWIDTH_MAX))\r\nreturn VXGE_HW_BADCFG_VPATH_MIN_BANDWIDTH;\r\nstatus = __vxge_hw_device_fifo_config_check(&vp_config->fifo);\r\nif (status != VXGE_HW_OK)\r\nreturn status;\r\nif ((vp_config->mtu != VXGE_HW_VPATH_USE_FLASH_DEFAULT_INITIAL_MTU) &&\r\n((vp_config->mtu < VXGE_HW_VPATH_MIN_INITIAL_MTU) ||\r\n(vp_config->mtu > VXGE_HW_VPATH_MAX_INITIAL_MTU)))\r\nreturn VXGE_HW_BADCFG_VPATH_MTU;\r\nif ((vp_config->rpa_strip_vlan_tag !=\r\nVXGE_HW_VPATH_RPA_STRIP_VLAN_TAG_USE_FLASH_DEFAULT) &&\r\n(vp_config->rpa_strip_vlan_tag !=\r\nVXGE_HW_VPATH_RPA_STRIP_VLAN_TAG_ENABLE) &&\r\n(vp_config->rpa_strip_vlan_tag !=\r\nVXGE_HW_VPATH_RPA_STRIP_VLAN_TAG_DISABLE))\r\nreturn VXGE_HW_BADCFG_VPATH_RPA_STRIP_VLAN_TAG;\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_config_check(struct vxge_hw_device_config *new_config)\r\n{\r\nu32 i;\r\nenum vxge_hw_status status;\r\nif ((new_config->intr_mode != VXGE_HW_INTR_MODE_IRQLINE) &&\r\n(new_config->intr_mode != VXGE_HW_INTR_MODE_MSIX) &&\r\n(new_config->intr_mode != VXGE_HW_INTR_MODE_MSIX_ONE_SHOT) &&\r\n(new_config->intr_mode != VXGE_HW_INTR_MODE_DEF))\r\nreturn VXGE_HW_BADCFG_INTR_MODE;\r\nif ((new_config->rts_mac_en != VXGE_HW_RTS_MAC_DISABLE) &&\r\n(new_config->rts_mac_en != VXGE_HW_RTS_MAC_ENABLE))\r\nreturn VXGE_HW_BADCFG_RTS_MAC_EN;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nstatus = __vxge_hw_device_vpath_config_check(\r\n&new_config->vp_config[i]);\r\nif (status != VXGE_HW_OK)\r\nreturn status;\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_device_initialize(\r\nstruct __vxge_hw_device **devh,\r\nstruct vxge_hw_device_attr *attr,\r\nstruct vxge_hw_device_config *device_config)\r\n{\r\nu32 i;\r\nu32 nblocks = 0;\r\nstruct __vxge_hw_device *hldev = NULL;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstatus = __vxge_hw_device_config_check(device_config);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nhldev = vzalloc(sizeof(struct __vxge_hw_device));\r\nif (hldev == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nhldev->magic = VXGE_HW_DEVICE_MAGIC;\r\nvxge_hw_device_debug_set(hldev, VXGE_ERR, VXGE_COMPONENT_ALL);\r\nmemcpy(&hldev->config, device_config,\r\nsizeof(struct vxge_hw_device_config));\r\nhldev->bar0 = attr->bar0;\r\nhldev->pdev = attr->pdev;\r\nhldev->uld_callbacks = attr->uld_callbacks;\r\n__vxge_hw_device_pci_e_init(hldev);\r\nstatus = __vxge_hw_device_reg_addr_get(hldev);\r\nif (status != VXGE_HW_OK) {\r\nvfree(hldev);\r\ngoto exit;\r\n}\r\n__vxge_hw_device_host_info_get(hldev);\r\nnblocks++;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpath_assignments & vxge_mBIT(i)))\r\ncontinue;\r\nif (device_config->vp_config[i].ring.enable ==\r\nVXGE_HW_RING_ENABLE)\r\nnblocks += device_config->vp_config[i].ring.ring_blocks;\r\nif (device_config->vp_config[i].fifo.enable ==\r\nVXGE_HW_FIFO_ENABLE)\r\nnblocks += device_config->vp_config[i].fifo.fifo_blocks;\r\nnblocks++;\r\n}\r\nif (__vxge_hw_blockpool_create(hldev,\r\n&hldev->block_pool,\r\ndevice_config->dma_blockpool_initial + nblocks,\r\ndevice_config->dma_blockpool_max + nblocks) != VXGE_HW_OK) {\r\nvxge_hw_device_terminate(hldev);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_device_initialize(hldev);\r\nif (status != VXGE_HW_OK) {\r\nvxge_hw_device_terminate(hldev);\r\ngoto exit;\r\n}\r\n*devh = hldev;\r\nexit:\r\nreturn status;\r\n}\r\nvoid\r\nvxge_hw_device_terminate(struct __vxge_hw_device *hldev)\r\n{\r\nvxge_assert(hldev->magic == VXGE_HW_DEVICE_MAGIC);\r\nhldev->magic = VXGE_HW_DEVICE_DEAD;\r\n__vxge_hw_blockpool_destroy(&hldev->block_pool);\r\nvfree(hldev);\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_stats_access(struct __vxge_hw_virtualpath *vpath,\r\nu32 operation, u32 offset, u64 *stat)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto vpath_stats_access_exit;\r\n}\r\nvp_reg = vpath->vp_reg;\r\nval64 = VXGE_HW_XMAC_STATS_ACCESS_CMD_OP(operation) |\r\nVXGE_HW_XMAC_STATS_ACCESS_CMD_STROBE |\r\nVXGE_HW_XMAC_STATS_ACCESS_CMD_OFFSET_SEL(offset);\r\nstatus = __vxge_hw_pio_mem_write64(val64,\r\n&vp_reg->xmac_stats_access_cmd,\r\nVXGE_HW_XMAC_STATS_ACCESS_CMD_STROBE,\r\nvpath->hldev->config.device_poll_millis);\r\nif ((status == VXGE_HW_OK) && (operation == VXGE_HW_STATS_OP_READ))\r\n*stat = readq(&vp_reg->xmac_stats_access_data);\r\nelse\r\n*stat = 0;\r\nvpath_stats_access_exit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_xmac_tx_stats_get(struct __vxge_hw_virtualpath *vpath,\r\nstruct vxge_hw_xmac_vpath_tx_stats *vpath_tx_stats)\r\n{\r\nu64 *val64;\r\nint i;\r\nu32 offset = VXGE_HW_STATS_VPATH_TX_OFFSET;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nval64 = (u64 *)vpath_tx_stats;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nfor (i = 0; i < sizeof(struct vxge_hw_xmac_vpath_tx_stats) / 8; i++) {\r\nstatus = __vxge_hw_vpath_stats_access(vpath,\r\nVXGE_HW_STATS_OP_READ,\r\noffset, val64);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\noffset++;\r\nval64++;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_xmac_rx_stats_get(struct __vxge_hw_virtualpath *vpath,\r\nstruct vxge_hw_xmac_vpath_rx_stats *vpath_rx_stats)\r\n{\r\nu64 *val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nint i;\r\nu32 offset = VXGE_HW_STATS_VPATH_RX_OFFSET;\r\nval64 = (u64 *) vpath_rx_stats;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nfor (i = 0; i < sizeof(struct vxge_hw_xmac_vpath_rx_stats) / 8; i++) {\r\nstatus = __vxge_hw_vpath_stats_access(vpath,\r\nVXGE_HW_STATS_OP_READ,\r\noffset >> 3, val64);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\noffset += 8;\r\nval64++;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_stats_get(struct __vxge_hw_virtualpath *vpath,\r\nstruct vxge_hw_vpath_stats_hw_info *hw_stats)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nvp_reg = vpath->vp_reg;\r\nval64 = readq(&vp_reg->vpath_debug_stats0);\r\nhw_stats->ini_num_mwr_sent =\r\n(u32)VXGE_HW_VPATH_DEBUG_STATS0_GET_INI_NUM_MWR_SENT(val64);\r\nval64 = readq(&vp_reg->vpath_debug_stats1);\r\nhw_stats->ini_num_mrd_sent =\r\n(u32)VXGE_HW_VPATH_DEBUG_STATS1_GET_INI_NUM_MRD_SENT(val64);\r\nval64 = readq(&vp_reg->vpath_debug_stats2);\r\nhw_stats->ini_num_cpl_rcvd =\r\n(u32)VXGE_HW_VPATH_DEBUG_STATS2_GET_INI_NUM_CPL_RCVD(val64);\r\nval64 = readq(&vp_reg->vpath_debug_stats3);\r\nhw_stats->ini_num_mwr_byte_sent =\r\nVXGE_HW_VPATH_DEBUG_STATS3_GET_INI_NUM_MWR_BYTE_SENT(val64);\r\nval64 = readq(&vp_reg->vpath_debug_stats4);\r\nhw_stats->ini_num_cpl_byte_rcvd =\r\nVXGE_HW_VPATH_DEBUG_STATS4_GET_INI_NUM_CPL_BYTE_RCVD(val64);\r\nval64 = readq(&vp_reg->vpath_debug_stats5);\r\nhw_stats->wrcrdtarb_xoff =\r\n(u32)VXGE_HW_VPATH_DEBUG_STATS5_GET_WRCRDTARB_XOFF(val64);\r\nval64 = readq(&vp_reg->vpath_debug_stats6);\r\nhw_stats->rdcrdtarb_xoff =\r\n(u32)VXGE_HW_VPATH_DEBUG_STATS6_GET_RDCRDTARB_XOFF(val64);\r\nval64 = readq(&vp_reg->vpath_genstats_count01);\r\nhw_stats->vpath_genstats_count0 =\r\n(u32)VXGE_HW_VPATH_GENSTATS_COUNT01_GET_PPIF_VPATH_GENSTATS_COUNT0(\r\nval64);\r\nval64 = readq(&vp_reg->vpath_genstats_count01);\r\nhw_stats->vpath_genstats_count1 =\r\n(u32)VXGE_HW_VPATH_GENSTATS_COUNT01_GET_PPIF_VPATH_GENSTATS_COUNT1(\r\nval64);\r\nval64 = readq(&vp_reg->vpath_genstats_count23);\r\nhw_stats->vpath_genstats_count2 =\r\n(u32)VXGE_HW_VPATH_GENSTATS_COUNT23_GET_PPIF_VPATH_GENSTATS_COUNT2(\r\nval64);\r\nval64 = readq(&vp_reg->vpath_genstats_count01);\r\nhw_stats->vpath_genstats_count3 =\r\n(u32)VXGE_HW_VPATH_GENSTATS_COUNT23_GET_PPIF_VPATH_GENSTATS_COUNT3(\r\nval64);\r\nval64 = readq(&vp_reg->vpath_genstats_count4);\r\nhw_stats->vpath_genstats_count4 =\r\n(u32)VXGE_HW_VPATH_GENSTATS_COUNT4_GET_PPIF_VPATH_GENSTATS_COUNT4(\r\nval64);\r\nval64 = readq(&vp_reg->vpath_genstats_count5);\r\nhw_stats->vpath_genstats_count5 =\r\n(u32)VXGE_HW_VPATH_GENSTATS_COUNT5_GET_PPIF_VPATH_GENSTATS_COUNT5(\r\nval64);\r\nstatus = __vxge_hw_vpath_xmac_tx_stats_get(vpath, &hw_stats->tx_stats);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_xmac_rx_stats_get(vpath, &hw_stats->rx_stats);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nVXGE_HW_VPATH_STATS_PIO_READ(\r\nVXGE_HW_STATS_VPATH_PROG_EVENT_VNUM0_OFFSET);\r\nhw_stats->prog_event_vnum0 =\r\n(u32)VXGE_HW_STATS_GET_VPATH_PROG_EVENT_VNUM0(val64);\r\nhw_stats->prog_event_vnum1 =\r\n(u32)VXGE_HW_STATS_GET_VPATH_PROG_EVENT_VNUM1(val64);\r\nVXGE_HW_VPATH_STATS_PIO_READ(\r\nVXGE_HW_STATS_VPATH_PROG_EVENT_VNUM2_OFFSET);\r\nhw_stats->prog_event_vnum2 =\r\n(u32)VXGE_HW_STATS_GET_VPATH_PROG_EVENT_VNUM2(val64);\r\nhw_stats->prog_event_vnum3 =\r\n(u32)VXGE_HW_STATS_GET_VPATH_PROG_EVENT_VNUM3(val64);\r\nval64 = readq(&vp_reg->rx_multi_cast_stats);\r\nhw_stats->rx_multi_cast_frame_discard =\r\n(u16)VXGE_HW_RX_MULTI_CAST_STATS_GET_FRAME_DISCARD(val64);\r\nval64 = readq(&vp_reg->rx_frm_transferred);\r\nhw_stats->rx_frm_transferred =\r\n(u32)VXGE_HW_RX_FRM_TRANSFERRED_GET_RX_FRM_TRANSFERRED(val64);\r\nval64 = readq(&vp_reg->rxd_returned);\r\nhw_stats->rxd_returned =\r\n(u16)VXGE_HW_RXD_RETURNED_GET_RXD_RETURNED(val64);\r\nval64 = readq(&vp_reg->dbg_stats_rx_mpa);\r\nhw_stats->rx_mpa_len_fail_frms =\r\n(u16)VXGE_HW_DBG_STATS_GET_RX_MPA_LEN_FAIL_FRMS(val64);\r\nhw_stats->rx_mpa_mrk_fail_frms =\r\n(u16)VXGE_HW_DBG_STATS_GET_RX_MPA_MRK_FAIL_FRMS(val64);\r\nhw_stats->rx_mpa_crc_fail_frms =\r\n(u16)VXGE_HW_DBG_STATS_GET_RX_MPA_CRC_FAIL_FRMS(val64);\r\nval64 = readq(&vp_reg->dbg_stats_rx_fau);\r\nhw_stats->rx_permitted_frms =\r\n(u16)VXGE_HW_DBG_STATS_GET_RX_FAU_RX_PERMITTED_FRMS(val64);\r\nhw_stats->rx_vp_reset_discarded_frms =\r\n(u16)VXGE_HW_DBG_STATS_GET_RX_FAU_RX_VP_RESET_DISCARDED_FRMS(val64);\r\nhw_stats->rx_wol_frms =\r\n(u16)VXGE_HW_DBG_STATS_GET_RX_FAU_RX_WOL_FRMS(val64);\r\nval64 = readq(&vp_reg->tx_vp_reset_discarded_frms);\r\nhw_stats->tx_vp_reset_discarded_frms =\r\n(u16)VXGE_HW_TX_VP_RESET_DISCARDED_FRMS_GET_TX_VP_RESET_DISCARDED_FRMS(\r\nval64);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_device_stats_get(struct __vxge_hw_device *hldev,\r\nstruct vxge_hw_device_stats_hw_info *hw_stats)\r\n{\r\nu32 i;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpaths_deployed & vxge_mBIT(i)) ||\r\n(hldev->virtual_paths[i].vp_open ==\r\nVXGE_HW_VP_NOT_OPEN))\r\ncontinue;\r\nmemcpy(hldev->virtual_paths[i].hw_stats_sav,\r\nhldev->virtual_paths[i].hw_stats,\r\nsizeof(struct vxge_hw_vpath_stats_hw_info));\r\nstatus = __vxge_hw_vpath_stats_get(\r\n&hldev->virtual_paths[i],\r\nhldev->virtual_paths[i].hw_stats);\r\n}\r\nmemcpy(hw_stats, &hldev->stats.hw_dev_info_stats,\r\nsizeof(struct vxge_hw_device_stats_hw_info));\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_driver_stats_get(\r\nstruct __vxge_hw_device *hldev,\r\nstruct vxge_hw_device_stats_sw_info *sw_stats)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nmemcpy(sw_stats, &hldev->stats.sw_dev_info_stats,\r\nsizeof(struct vxge_hw_device_stats_sw_info));\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_mrpcim_stats_access(struct __vxge_hw_device *hldev,\r\nu32 operation, u32 location, u32 offset, u64 *stat)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstatus = __vxge_hw_device_is_privilaged(hldev->host_type,\r\nhldev->func_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nval64 = VXGE_HW_XMAC_STATS_SYS_CMD_OP(operation) |\r\nVXGE_HW_XMAC_STATS_SYS_CMD_STROBE |\r\nVXGE_HW_XMAC_STATS_SYS_CMD_LOC_SEL(location) |\r\nVXGE_HW_XMAC_STATS_SYS_CMD_OFFSET_SEL(offset);\r\nstatus = __vxge_hw_pio_mem_write64(val64,\r\n&hldev->mrpcim_reg->xmac_stats_sys_cmd,\r\nVXGE_HW_XMAC_STATS_SYS_CMD_STROBE,\r\nhldev->config.device_poll_millis);\r\nif ((status == VXGE_HW_OK) && (operation == VXGE_HW_STATS_OP_READ))\r\n*stat = readq(&hldev->mrpcim_reg->xmac_stats_sys_data);\r\nelse\r\n*stat = 0;\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_hw_device_xmac_aggr_stats_get(struct __vxge_hw_device *hldev, u32 port,\r\nstruct vxge_hw_xmac_aggr_stats *aggr_stats)\r\n{\r\nu64 *val64;\r\nint i;\r\nu32 offset = VXGE_HW_STATS_AGGRn_OFFSET;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nval64 = (u64 *)aggr_stats;\r\nstatus = __vxge_hw_device_is_privilaged(hldev->host_type,\r\nhldev->func_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nfor (i = 0; i < sizeof(struct vxge_hw_xmac_aggr_stats) / 8; i++) {\r\nstatus = vxge_hw_mrpcim_stats_access(hldev,\r\nVXGE_HW_STATS_OP_READ,\r\nVXGE_HW_STATS_LOC_AGGR,\r\n((offset + (104 * port)) >> 3), val64);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\noffset += 8;\r\nval64++;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_hw_device_xmac_port_stats_get(struct __vxge_hw_device *hldev, u32 port,\r\nstruct vxge_hw_xmac_port_stats *port_stats)\r\n{\r\nu64 *val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nint i;\r\nu32 offset = 0x0;\r\nval64 = (u64 *) port_stats;\r\nstatus = __vxge_hw_device_is_privilaged(hldev->host_type,\r\nhldev->func_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nfor (i = 0; i < sizeof(struct vxge_hw_xmac_port_stats) / 8; i++) {\r\nstatus = vxge_hw_mrpcim_stats_access(hldev,\r\nVXGE_HW_STATS_OP_READ,\r\nVXGE_HW_STATS_LOC_AGGR,\r\n((offset + (608 * port)) >> 3), val64);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\noffset += 8;\r\nval64++;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_device_xmac_stats_get(struct __vxge_hw_device *hldev,\r\nstruct vxge_hw_xmac_stats *xmac_stats)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nu32 i;\r\nstatus = vxge_hw_device_xmac_aggr_stats_get(hldev,\r\n0, &xmac_stats->aggr_stats[0]);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = vxge_hw_device_xmac_aggr_stats_get(hldev,\r\n1, &xmac_stats->aggr_stats[1]);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nfor (i = 0; i <= VXGE_HW_MAC_MAX_MAC_PORT_ID; i++) {\r\nstatus = vxge_hw_device_xmac_port_stats_get(hldev,\r\ni, &xmac_stats->port_stats[i]);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpaths_deployed & vxge_mBIT(i)))\r\ncontinue;\r\nstatus = __vxge_hw_vpath_xmac_tx_stats_get(\r\n&hldev->virtual_paths[i],\r\n&xmac_stats->vpath_tx_stats[i]);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_xmac_rx_stats_get(\r\n&hldev->virtual_paths[i],\r\n&xmac_stats->vpath_rx_stats[i]);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nvoid vxge_hw_device_debug_set(struct __vxge_hw_device *hldev,\r\nenum vxge_debug_level level, u32 mask)\r\n{\r\nif (hldev == NULL)\r\nreturn;\r\n#if defined(VXGE_DEBUG_TRACE_MASK) || \\r\ndefined(VXGE_DEBUG_ERR_MASK)\r\nhldev->debug_module_mask = mask;\r\nhldev->debug_level = level;\r\n#endif\r\n#if defined(VXGE_DEBUG_ERR_MASK)\r\nhldev->level_err = level & VXGE_ERR;\r\n#endif\r\n#if defined(VXGE_DEBUG_TRACE_MASK)\r\nhldev->level_trace = level & VXGE_TRACE;\r\n#endif\r\n}\r\nu32 vxge_hw_device_error_level_get(struct __vxge_hw_device *hldev)\r\n{\r\n#if defined(VXGE_DEBUG_ERR_MASK)\r\nif (hldev == NULL)\r\nreturn VXGE_ERR;\r\nelse\r\nreturn hldev->level_err;\r\n#else\r\nreturn 0;\r\n#endif\r\n}\r\nu32 vxge_hw_device_trace_level_get(struct __vxge_hw_device *hldev)\r\n{\r\n#if defined(VXGE_DEBUG_TRACE_MASK)\r\nif (hldev == NULL)\r\nreturn VXGE_TRACE;\r\nelse\r\nreturn hldev->level_trace;\r\n#else\r\nreturn 0;\r\n#endif\r\n}\r\nenum vxge_hw_status vxge_hw_device_getpause_data(struct __vxge_hw_device *hldev,\r\nu32 port, u32 *tx, u32 *rx)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((hldev == NULL) || (hldev->magic != VXGE_HW_DEVICE_MAGIC)) {\r\nstatus = VXGE_HW_ERR_INVALID_DEVICE;\r\ngoto exit;\r\n}\r\nif (port > VXGE_HW_MAC_MAX_MAC_PORT_ID) {\r\nstatus = VXGE_HW_ERR_INVALID_PORT;\r\ngoto exit;\r\n}\r\nif (!(hldev->access_rights & VXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM)) {\r\nstatus = VXGE_HW_ERR_PRIVILAGED_OPEARATION;\r\ngoto exit;\r\n}\r\nval64 = readq(&hldev->mrpcim_reg->rxmac_pause_cfg_port[port]);\r\nif (val64 & VXGE_HW_RXMAC_PAUSE_CFG_PORT_GEN_EN)\r\n*tx = 1;\r\nif (val64 & VXGE_HW_RXMAC_PAUSE_CFG_PORT_RCV_EN)\r\n*rx = 1;\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_device_setpause_data(struct __vxge_hw_device *hldev,\r\nu32 port, u32 tx, u32 rx)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((hldev == NULL) || (hldev->magic != VXGE_HW_DEVICE_MAGIC)) {\r\nstatus = VXGE_HW_ERR_INVALID_DEVICE;\r\ngoto exit;\r\n}\r\nif (port > VXGE_HW_MAC_MAX_MAC_PORT_ID) {\r\nstatus = VXGE_HW_ERR_INVALID_PORT;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_device_is_privilaged(hldev->host_type,\r\nhldev->func_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nval64 = readq(&hldev->mrpcim_reg->rxmac_pause_cfg_port[port]);\r\nif (tx)\r\nval64 |= VXGE_HW_RXMAC_PAUSE_CFG_PORT_GEN_EN;\r\nelse\r\nval64 &= ~VXGE_HW_RXMAC_PAUSE_CFG_PORT_GEN_EN;\r\nif (rx)\r\nval64 |= VXGE_HW_RXMAC_PAUSE_CFG_PORT_RCV_EN;\r\nelse\r\nval64 &= ~VXGE_HW_RXMAC_PAUSE_CFG_PORT_RCV_EN;\r\nwriteq(val64, &hldev->mrpcim_reg->rxmac_pause_cfg_port[port]);\r\nexit:\r\nreturn status;\r\n}\r\nu16 vxge_hw_device_link_width_get(struct __vxge_hw_device *hldev)\r\n{\r\nstruct pci_dev *dev = hldev->pdev;\r\nu16 lnk;\r\npcie_capability_read_word(dev, PCI_EXP_LNKSTA, &lnk);\r\nreturn (lnk & VXGE_HW_PCI_EXP_LNKCAP_LNK_WIDTH) >> 4;\r\n}\r\nstatic inline u32\r\n__vxge_hw_ring_block_memblock_idx(u8 *block)\r\n{\r\nreturn (u32)*((u64 *)(block + VXGE_HW_RING_MEMBLOCK_IDX_OFFSET));\r\n}\r\nstatic inline void\r\n__vxge_hw_ring_block_memblock_idx_set(u8 *block, u32 memblock_idx)\r\n{\r\n*((u64 *)(block + VXGE_HW_RING_MEMBLOCK_IDX_OFFSET)) = memblock_idx;\r\n}\r\nstatic inline void\r\n__vxge_hw_ring_block_next_pointer_set(u8 *block, dma_addr_t dma_next)\r\n{\r\n*((u64 *)(block + VXGE_HW_RING_NEXT_BLOCK_POINTER_OFFSET)) = dma_next;\r\n}\r\nstatic u64 __vxge_hw_ring_first_block_address_get(struct __vxge_hw_ring *ring)\r\n{\r\nstruct vxge_hw_mempool_dma *dma_object;\r\ndma_object = ring->mempool->memblocks_dma_arr;\r\nvxge_assert(dma_object != NULL);\r\nreturn dma_object->addr;\r\n}\r\nstatic dma_addr_t __vxge_hw_ring_item_dma_addr(struct vxge_hw_mempool *mempoolh,\r\nvoid *item)\r\n{\r\nu32 memblock_idx;\r\nvoid *memblock;\r\nstruct vxge_hw_mempool_dma *memblock_dma_object;\r\nptrdiff_t dma_item_offset;\r\nmemblock_idx = __vxge_hw_ring_block_memblock_idx(item);\r\nmemblock = mempoolh->memblocks_arr[memblock_idx];\r\nmemblock_dma_object = mempoolh->memblocks_dma_arr + memblock_idx;\r\ndma_item_offset = (u8 *)item - (u8 *)memblock;\r\nreturn memblock_dma_object->addr + dma_item_offset;\r\n}\r\nstatic void __vxge_hw_ring_rxdblock_link(struct vxge_hw_mempool *mempoolh,\r\nstruct __vxge_hw_ring *ring, u32 from,\r\nu32 to)\r\n{\r\nu8 *to_item , *from_item;\r\ndma_addr_t to_dma;\r\nfrom_item = mempoolh->items_arr[from];\r\nvxge_assert(from_item);\r\nto_item = mempoolh->items_arr[to];\r\nvxge_assert(to_item);\r\nto_dma = __vxge_hw_ring_item_dma_addr(mempoolh, to_item);\r\n__vxge_hw_ring_block_next_pointer_set(from_item, to_dma);\r\n}\r\nstatic void\r\n__vxge_hw_ring_mempool_item_alloc(struct vxge_hw_mempool *mempoolh,\r\nu32 memblock_index,\r\nstruct vxge_hw_mempool_dma *dma_object,\r\nu32 index, u32 is_last)\r\n{\r\nu32 i;\r\nvoid *item = mempoolh->items_arr[index];\r\nstruct __vxge_hw_ring *ring =\r\n(struct __vxge_hw_ring *)mempoolh->userdata;\r\nfor (i = 0; i < ring->rxds_per_block; i++) {\r\nvoid *rxdblock_priv;\r\nvoid *uld_priv;\r\nstruct vxge_hw_ring_rxd_1 *rxdp;\r\nu32 reserve_index = ring->channel.reserve_ptr -\r\n(index * ring->rxds_per_block + i + 1);\r\nu32 memblock_item_idx;\r\nring->channel.reserve_arr[reserve_index] = ((u8 *)item) +\r\ni * ring->rxd_size;\r\nrxdblock_priv = __vxge_hw_mempool_item_priv(mempoolh,\r\nmemblock_index, item,\r\n&memblock_item_idx);\r\nrxdp = ring->channel.reserve_arr[reserve_index];\r\nuld_priv = ((u8 *)rxdblock_priv + ring->rxd_priv_size * i);\r\nrxdp->host_control = (u64)(size_t)uld_priv;\r\n}\r\n__vxge_hw_ring_block_memblock_idx_set(item, memblock_index);\r\nif (is_last) {\r\n__vxge_hw_ring_rxdblock_link(mempoolh, ring, index, 0);\r\n}\r\nif (index > 0) {\r\n__vxge_hw_ring_rxdblock_link(mempoolh, ring, index - 1, index);\r\n}\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_ring_replenish(struct __vxge_hw_ring *ring)\r\n{\r\nvoid *rxd;\r\nstruct __vxge_hw_channel *channel;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nchannel = &ring->channel;\r\nwhile (vxge_hw_channel_dtr_count(channel) > 0) {\r\nstatus = vxge_hw_ring_rxd_reserve(ring, &rxd);\r\nvxge_assert(status == VXGE_HW_OK);\r\nif (ring->rxd_init) {\r\nstatus = ring->rxd_init(rxd, channel->userdata);\r\nif (status != VXGE_HW_OK) {\r\nvxge_hw_ring_rxd_free(ring, rxd);\r\ngoto exit;\r\n}\r\n}\r\nvxge_hw_ring_rxd_post(ring, rxd);\r\n}\r\nstatus = VXGE_HW_OK;\r\nexit:\r\nreturn status;\r\n}\r\nstatic struct __vxge_hw_channel *\r\n__vxge_hw_channel_allocate(struct __vxge_hw_vpath_handle *vph,\r\nenum __vxge_hw_channel_type type,\r\nu32 length, u32 per_dtr_space,\r\nvoid *userdata)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nstruct __vxge_hw_device *hldev;\r\nint size = 0;\r\nu32 vp_id;\r\nhldev = vph->vpath->hldev;\r\nvp_id = vph->vpath->vp_id;\r\nswitch (type) {\r\ncase VXGE_HW_CHANNEL_TYPE_FIFO:\r\nsize = sizeof(struct __vxge_hw_fifo);\r\nbreak;\r\ncase VXGE_HW_CHANNEL_TYPE_RING:\r\nsize = sizeof(struct __vxge_hw_ring);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nchannel = kzalloc(size, GFP_KERNEL);\r\nif (channel == NULL)\r\ngoto exit0;\r\nINIT_LIST_HEAD(&channel->item);\r\nchannel->common_reg = hldev->common_reg;\r\nchannel->first_vp_id = hldev->first_vp_id;\r\nchannel->type = type;\r\nchannel->devh = hldev;\r\nchannel->vph = vph;\r\nchannel->userdata = userdata;\r\nchannel->per_dtr_space = per_dtr_space;\r\nchannel->length = length;\r\nchannel->vp_id = vp_id;\r\nchannel->work_arr = kzalloc(sizeof(void *)*length, GFP_KERNEL);\r\nif (channel->work_arr == NULL)\r\ngoto exit1;\r\nchannel->free_arr = kzalloc(sizeof(void *)*length, GFP_KERNEL);\r\nif (channel->free_arr == NULL)\r\ngoto exit1;\r\nchannel->free_ptr = length;\r\nchannel->reserve_arr = kzalloc(sizeof(void *)*length, GFP_KERNEL);\r\nif (channel->reserve_arr == NULL)\r\ngoto exit1;\r\nchannel->reserve_ptr = length;\r\nchannel->reserve_top = 0;\r\nchannel->orig_arr = kzalloc(sizeof(void *)*length, GFP_KERNEL);\r\nif (channel->orig_arr == NULL)\r\ngoto exit1;\r\nreturn channel;\r\nexit1:\r\n__vxge_hw_channel_free(channel);\r\nexit0:\r\nreturn NULL;\r\n}\r\nstatic void vxge_hw_blockpool_block_add(struct __vxge_hw_device *devh,\r\nvoid *block_addr,\r\nu32 length,\r\nstruct pci_dev *dma_h,\r\nstruct pci_dev *acc_handle)\r\n{\r\nstruct __vxge_hw_blockpool *blockpool;\r\nstruct __vxge_hw_blockpool_entry *entry = NULL;\r\ndma_addr_t dma_addr;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nu32 req_out;\r\nblockpool = &devh->block_pool;\r\nif (block_addr == NULL) {\r\nblockpool->req_out--;\r\nstatus = VXGE_HW_FAIL;\r\ngoto exit;\r\n}\r\ndma_addr = pci_map_single(devh->pdev, block_addr, length,\r\nPCI_DMA_BIDIRECTIONAL);\r\nif (unlikely(pci_dma_mapping_error(devh->pdev, dma_addr))) {\r\nvxge_os_dma_free(devh->pdev, block_addr, &acc_handle);\r\nblockpool->req_out--;\r\nstatus = VXGE_HW_FAIL;\r\ngoto exit;\r\n}\r\nif (!list_empty(&blockpool->free_entry_list))\r\nentry = (struct __vxge_hw_blockpool_entry *)\r\nlist_first_entry(&blockpool->free_entry_list,\r\nstruct __vxge_hw_blockpool_entry,\r\nitem);\r\nif (entry == NULL)\r\nentry = vmalloc(sizeof(struct __vxge_hw_blockpool_entry));\r\nelse\r\nlist_del(&entry->item);\r\nif (entry != NULL) {\r\nentry->length = length;\r\nentry->memblock = block_addr;\r\nentry->dma_addr = dma_addr;\r\nentry->acc_handle = acc_handle;\r\nentry->dma_handle = dma_h;\r\nlist_add(&entry->item, &blockpool->free_block_list);\r\nblockpool->pool_size++;\r\nstatus = VXGE_HW_OK;\r\n} else\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\nblockpool->req_out--;\r\nreq_out = blockpool->req_out;\r\nexit:\r\nreturn;\r\n}\r\nstatic inline void\r\nvxge_os_dma_malloc_async(struct pci_dev *pdev, void *devh, unsigned long size)\r\n{\r\ngfp_t flags;\r\nvoid *vaddr;\r\nif (in_interrupt())\r\nflags = GFP_ATOMIC | GFP_DMA;\r\nelse\r\nflags = GFP_KERNEL | GFP_DMA;\r\nvaddr = kmalloc((size), flags);\r\nvxge_hw_blockpool_block_add(devh, vaddr, size, pdev, pdev);\r\n}\r\nstatic\r\nvoid __vxge_hw_blockpool_blocks_add(struct __vxge_hw_blockpool *blockpool)\r\n{\r\nu32 nreq = 0, i;\r\nif ((blockpool->pool_size + blockpool->req_out) <\r\nVXGE_HW_MIN_DMA_BLOCK_POOL_SIZE) {\r\nnreq = VXGE_HW_INCR_DMA_BLOCK_POOL_SIZE;\r\nblockpool->req_out += nreq;\r\n}\r\nfor (i = 0; i < nreq; i++)\r\nvxge_os_dma_malloc_async(\r\n(blockpool->hldev)->pdev,\r\nblockpool->hldev, VXGE_HW_BLOCK_SIZE);\r\n}\r\nstatic void *__vxge_hw_blockpool_malloc(struct __vxge_hw_device *devh, u32 size,\r\nstruct vxge_hw_mempool_dma *dma_object)\r\n{\r\nstruct __vxge_hw_blockpool_entry *entry = NULL;\r\nstruct __vxge_hw_blockpool *blockpool;\r\nvoid *memblock = NULL;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nblockpool = &devh->block_pool;\r\nif (size != blockpool->block_size) {\r\nmemblock = vxge_os_dma_malloc(devh->pdev, size,\r\n&dma_object->handle,\r\n&dma_object->acc_handle);\r\nif (memblock == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\ndma_object->addr = pci_map_single(devh->pdev, memblock, size,\r\nPCI_DMA_BIDIRECTIONAL);\r\nif (unlikely(pci_dma_mapping_error(devh->pdev,\r\ndma_object->addr))) {\r\nvxge_os_dma_free(devh->pdev, memblock,\r\n&dma_object->acc_handle);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\n} else {\r\nif (!list_empty(&blockpool->free_block_list))\r\nentry = (struct __vxge_hw_blockpool_entry *)\r\nlist_first_entry(&blockpool->free_block_list,\r\nstruct __vxge_hw_blockpool_entry,\r\nitem);\r\nif (entry != NULL) {\r\nlist_del(&entry->item);\r\ndma_object->addr = entry->dma_addr;\r\ndma_object->handle = entry->dma_handle;\r\ndma_object->acc_handle = entry->acc_handle;\r\nmemblock = entry->memblock;\r\nlist_add(&entry->item,\r\n&blockpool->free_entry_list);\r\nblockpool->pool_size--;\r\n}\r\nif (memblock != NULL)\r\n__vxge_hw_blockpool_blocks_add(blockpool);\r\n}\r\nexit:\r\nreturn memblock;\r\n}\r\nstatic void\r\n__vxge_hw_blockpool_blocks_remove(struct __vxge_hw_blockpool *blockpool)\r\n{\r\nstruct list_head *p, *n;\r\nlist_for_each_safe(p, n, &blockpool->free_block_list) {\r\nif (blockpool->pool_size < blockpool->pool_max)\r\nbreak;\r\npci_unmap_single(\r\n(blockpool->hldev)->pdev,\r\n((struct __vxge_hw_blockpool_entry *)p)->dma_addr,\r\n((struct __vxge_hw_blockpool_entry *)p)->length,\r\nPCI_DMA_BIDIRECTIONAL);\r\nvxge_os_dma_free(\r\n(blockpool->hldev)->pdev,\r\n((struct __vxge_hw_blockpool_entry *)p)->memblock,\r\n&((struct __vxge_hw_blockpool_entry *)p)->acc_handle);\r\nlist_del(&((struct __vxge_hw_blockpool_entry *)p)->item);\r\nlist_add(p, &blockpool->free_entry_list);\r\nblockpool->pool_size--;\r\n}\r\n}\r\nstatic void __vxge_hw_blockpool_free(struct __vxge_hw_device *devh,\r\nvoid *memblock, u32 size,\r\nstruct vxge_hw_mempool_dma *dma_object)\r\n{\r\nstruct __vxge_hw_blockpool_entry *entry = NULL;\r\nstruct __vxge_hw_blockpool *blockpool;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nblockpool = &devh->block_pool;\r\nif (size != blockpool->block_size) {\r\npci_unmap_single(devh->pdev, dma_object->addr, size,\r\nPCI_DMA_BIDIRECTIONAL);\r\nvxge_os_dma_free(devh->pdev, memblock, &dma_object->acc_handle);\r\n} else {\r\nif (!list_empty(&blockpool->free_entry_list))\r\nentry = (struct __vxge_hw_blockpool_entry *)\r\nlist_first_entry(&blockpool->free_entry_list,\r\nstruct __vxge_hw_blockpool_entry,\r\nitem);\r\nif (entry == NULL)\r\nentry = vmalloc(sizeof(\r\nstruct __vxge_hw_blockpool_entry));\r\nelse\r\nlist_del(&entry->item);\r\nif (entry != NULL) {\r\nentry->length = size;\r\nentry->memblock = memblock;\r\nentry->dma_addr = dma_object->addr;\r\nentry->acc_handle = dma_object->acc_handle;\r\nentry->dma_handle = dma_object->handle;\r\nlist_add(&entry->item,\r\n&blockpool->free_block_list);\r\nblockpool->pool_size++;\r\nstatus = VXGE_HW_OK;\r\n} else\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\nif (status == VXGE_HW_OK)\r\n__vxge_hw_blockpool_blocks_remove(blockpool);\r\n}\r\n}\r\nstatic void __vxge_hw_mempool_destroy(struct vxge_hw_mempool *mempool)\r\n{\r\nu32 i, j;\r\nstruct __vxge_hw_device *devh = mempool->devh;\r\nfor (i = 0; i < mempool->memblocks_allocated; i++) {\r\nstruct vxge_hw_mempool_dma *dma_object;\r\nvxge_assert(mempool->memblocks_arr[i]);\r\nvxge_assert(mempool->memblocks_dma_arr + i);\r\ndma_object = mempool->memblocks_dma_arr + i;\r\nfor (j = 0; j < mempool->items_per_memblock; j++) {\r\nu32 index = i * mempool->items_per_memblock + j;\r\nif (index >= mempool->items_current)\r\nbreak;\r\n}\r\nvfree(mempool->memblocks_priv_arr[i]);\r\n__vxge_hw_blockpool_free(devh, mempool->memblocks_arr[i],\r\nmempool->memblock_size, dma_object);\r\n}\r\nvfree(mempool->items_arr);\r\nvfree(mempool->memblocks_dma_arr);\r\nvfree(mempool->memblocks_priv_arr);\r\nvfree(mempool->memblocks_arr);\r\nvfree(mempool);\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_mempool_grow(struct vxge_hw_mempool *mempool, u32 num_allocate,\r\nu32 *num_allocated)\r\n{\r\nu32 i, first_time = mempool->memblocks_allocated == 0 ? 1 : 0;\r\nu32 n_items = mempool->items_per_memblock;\r\nu32 start_block_idx = mempool->memblocks_allocated;\r\nu32 end_block_idx = mempool->memblocks_allocated + num_allocate;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\n*num_allocated = 0;\r\nif (end_block_idx > mempool->memblocks_max) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nfor (i = start_block_idx; i < end_block_idx; i++) {\r\nu32 j;\r\nu32 is_last = ((end_block_idx - 1) == i);\r\nstruct vxge_hw_mempool_dma *dma_object =\r\nmempool->memblocks_dma_arr + i;\r\nvoid *the_memblock;\r\nmempool->memblocks_priv_arr[i] =\r\nvzalloc(mempool->items_priv_size * n_items);\r\nif (mempool->memblocks_priv_arr[i] == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nmempool->memblocks_arr[i] =\r\n__vxge_hw_blockpool_malloc(mempool->devh,\r\nmempool->memblock_size, dma_object);\r\nif (mempool->memblocks_arr[i] == NULL) {\r\nvfree(mempool->memblocks_priv_arr[i]);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\n(*num_allocated)++;\r\nmempool->memblocks_allocated++;\r\nmemset(mempool->memblocks_arr[i], 0, mempool->memblock_size);\r\nthe_memblock = mempool->memblocks_arr[i];\r\nfor (j = 0; j < n_items; j++) {\r\nu32 index = i * n_items + j;\r\nif (first_time && index >= mempool->items_initial)\r\nbreak;\r\nmempool->items_arr[index] =\r\n((char *)the_memblock + j*mempool->item_size);\r\nif (mempool->item_func_alloc != NULL)\r\nmempool->item_func_alloc(mempool, i,\r\ndma_object, index, is_last);\r\nmempool->items_current = index + 1;\r\n}\r\nif (first_time && mempool->items_current ==\r\nmempool->items_initial)\r\nbreak;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic struct vxge_hw_mempool *\r\n__vxge_hw_mempool_create(struct __vxge_hw_device *devh,\r\nu32 memblock_size,\r\nu32 item_size,\r\nu32 items_priv_size,\r\nu32 items_initial,\r\nu32 items_max,\r\nconst struct vxge_hw_mempool_cbs *mp_callback,\r\nvoid *userdata)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nu32 memblocks_to_allocate;\r\nstruct vxge_hw_mempool *mempool = NULL;\r\nu32 allocated;\r\nif (memblock_size < item_size) {\r\nstatus = VXGE_HW_FAIL;\r\ngoto exit;\r\n}\r\nmempool = vzalloc(sizeof(struct vxge_hw_mempool));\r\nif (mempool == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nmempool->devh = devh;\r\nmempool->memblock_size = memblock_size;\r\nmempool->items_max = items_max;\r\nmempool->items_initial = items_initial;\r\nmempool->item_size = item_size;\r\nmempool->items_priv_size = items_priv_size;\r\nmempool->item_func_alloc = mp_callback->item_func_alloc;\r\nmempool->userdata = userdata;\r\nmempool->memblocks_allocated = 0;\r\nmempool->items_per_memblock = memblock_size / item_size;\r\nmempool->memblocks_max = (items_max + mempool->items_per_memblock - 1) /\r\nmempool->items_per_memblock;\r\nmempool->memblocks_arr =\r\nvzalloc(sizeof(void *) * mempool->memblocks_max);\r\nif (mempool->memblocks_arr == NULL) {\r\n__vxge_hw_mempool_destroy(mempool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\nmempool = NULL;\r\ngoto exit;\r\n}\r\nmempool->memblocks_priv_arr =\r\nvzalloc(sizeof(void *) * mempool->memblocks_max);\r\nif (mempool->memblocks_priv_arr == NULL) {\r\n__vxge_hw_mempool_destroy(mempool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\nmempool = NULL;\r\ngoto exit;\r\n}\r\nmempool->memblocks_dma_arr =\r\nvzalloc(sizeof(struct vxge_hw_mempool_dma) *\r\nmempool->memblocks_max);\r\nif (mempool->memblocks_dma_arr == NULL) {\r\n__vxge_hw_mempool_destroy(mempool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\nmempool = NULL;\r\ngoto exit;\r\n}\r\nmempool->items_arr = vzalloc(sizeof(void *) * mempool->items_max);\r\nif (mempool->items_arr == NULL) {\r\n__vxge_hw_mempool_destroy(mempool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\nmempool = NULL;\r\ngoto exit;\r\n}\r\nmemblocks_to_allocate = (mempool->items_initial +\r\nmempool->items_per_memblock - 1) /\r\nmempool->items_per_memblock;\r\nstatus = __vxge_hw_mempool_grow(mempool, memblocks_to_allocate,\r\n&allocated);\r\nif (status != VXGE_HW_OK) {\r\n__vxge_hw_mempool_destroy(mempool);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\nmempool = NULL;\r\ngoto exit;\r\n}\r\nexit:\r\nreturn mempool;\r\n}\r\nstatic enum vxge_hw_status __vxge_hw_ring_abort(struct __vxge_hw_ring *ring)\r\n{\r\nvoid *rxdh;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &ring->channel;\r\nfor (;;) {\r\nvxge_hw_channel_dtr_try_complete(channel, &rxdh);\r\nif (rxdh == NULL)\r\nbreak;\r\nvxge_hw_channel_dtr_complete(channel);\r\nif (ring->rxd_term)\r\nring->rxd_term(rxdh, VXGE_HW_RXD_STATE_POSTED,\r\nchannel->userdata);\r\nvxge_hw_channel_dtr_free(channel, rxdh);\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status __vxge_hw_ring_reset(struct __vxge_hw_ring *ring)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &ring->channel;\r\n__vxge_hw_ring_abort(ring);\r\nstatus = __vxge_hw_channel_reset(channel);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nif (ring->rxd_init) {\r\nstatus = vxge_hw_ring_replenish(ring);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_ring_delete(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nstruct __vxge_hw_ring *ring = vp->vpath->ringh;\r\n__vxge_hw_ring_abort(ring);\r\nif (ring->mempool)\r\n__vxge_hw_mempool_destroy(ring->mempool);\r\nvp->vpath->ringh = NULL;\r\n__vxge_hw_channel_free(&ring->channel);\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_ring_create(struct __vxge_hw_vpath_handle *vp,\r\nstruct vxge_hw_ring_attr *attr)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_ring *ring;\r\nu32 ring_length;\r\nstruct vxge_hw_ring_config *config;\r\nstruct __vxge_hw_device *hldev;\r\nu32 vp_id;\r\nstatic const struct vxge_hw_mempool_cbs ring_mp_callback = {\r\n.item_func_alloc = __vxge_hw_ring_mempool_item_alloc,\r\n};\r\nif ((vp == NULL) || (attr == NULL)) {\r\nstatus = VXGE_HW_FAIL;\r\ngoto exit;\r\n}\r\nhldev = vp->vpath->hldev;\r\nvp_id = vp->vpath->vp_id;\r\nconfig = &hldev->config.vp_config[vp_id].ring;\r\nring_length = config->ring_blocks *\r\nvxge_hw_ring_rxds_per_block_get(config->buffer_mode);\r\nring = (struct __vxge_hw_ring *)__vxge_hw_channel_allocate(vp,\r\nVXGE_HW_CHANNEL_TYPE_RING,\r\nring_length,\r\nattr->per_rxd_space,\r\nattr->userdata);\r\nif (ring == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nvp->vpath->ringh = ring;\r\nring->vp_id = vp_id;\r\nring->vp_reg = vp->vpath->vp_reg;\r\nring->common_reg = hldev->common_reg;\r\nring->stats = &vp->vpath->sw_stats->ring_stats;\r\nring->config = config;\r\nring->callback = attr->callback;\r\nring->rxd_init = attr->rxd_init;\r\nring->rxd_term = attr->rxd_term;\r\nring->buffer_mode = config->buffer_mode;\r\nring->tim_rti_cfg1_saved = vp->vpath->tim_rti_cfg1_saved;\r\nring->tim_rti_cfg3_saved = vp->vpath->tim_rti_cfg3_saved;\r\nring->rxds_limit = config->rxds_limit;\r\nring->rxd_size = vxge_hw_ring_rxd_size_get(config->buffer_mode);\r\nring->rxd_priv_size =\r\nsizeof(struct __vxge_hw_ring_rxd_priv) + attr->per_rxd_space;\r\nring->per_rxd_space = attr->per_rxd_space;\r\nring->rxd_priv_size =\r\n((ring->rxd_priv_size + VXGE_CACHE_LINE_SIZE - 1) /\r\nVXGE_CACHE_LINE_SIZE) * VXGE_CACHE_LINE_SIZE;\r\nring->rxds_per_block =\r\nvxge_hw_ring_rxds_per_block_get(config->buffer_mode);\r\nring->rxdblock_priv_size = ring->rxd_priv_size * ring->rxds_per_block;\r\nring->mempool = __vxge_hw_mempool_create(hldev,\r\nVXGE_HW_BLOCK_SIZE,\r\nVXGE_HW_BLOCK_SIZE,\r\nring->rxdblock_priv_size,\r\nring->config->ring_blocks,\r\nring->config->ring_blocks,\r\n&ring_mp_callback,\r\nring);\r\nif (ring->mempool == NULL) {\r\n__vxge_hw_ring_delete(vp);\r\nreturn VXGE_HW_ERR_OUT_OF_MEMORY;\r\n}\r\nstatus = __vxge_hw_channel_initialize(&ring->channel);\r\nif (status != VXGE_HW_OK) {\r\n__vxge_hw_ring_delete(vp);\r\ngoto exit;\r\n}\r\nif (ring->rxd_init) {\r\nstatus = vxge_hw_ring_replenish(ring);\r\nif (status != VXGE_HW_OK) {\r\n__vxge_hw_ring_delete(vp);\r\ngoto exit;\r\n}\r\n}\r\nring->stats->common_stats.usage_cnt = 0;\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_device_config_default_get(struct vxge_hw_device_config *device_config)\r\n{\r\nu32 i;\r\ndevice_config->dma_blockpool_initial =\r\nVXGE_HW_INITIAL_DMA_BLOCK_POOL_SIZE;\r\ndevice_config->dma_blockpool_max = VXGE_HW_MAX_DMA_BLOCK_POOL_SIZE;\r\ndevice_config->intr_mode = VXGE_HW_INTR_MODE_DEF;\r\ndevice_config->rth_en = VXGE_HW_RTH_DEFAULT;\r\ndevice_config->rth_it_type = VXGE_HW_RTH_IT_TYPE_DEFAULT;\r\ndevice_config->device_poll_millis = VXGE_HW_DEF_DEVICE_POLL_MILLIS;\r\ndevice_config->rts_mac_en = VXGE_HW_RTS_MAC_DEFAULT;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\ndevice_config->vp_config[i].vp_id = i;\r\ndevice_config->vp_config[i].min_bandwidth =\r\nVXGE_HW_VPATH_BANDWIDTH_DEFAULT;\r\ndevice_config->vp_config[i].ring.enable = VXGE_HW_RING_DEFAULT;\r\ndevice_config->vp_config[i].ring.ring_blocks =\r\nVXGE_HW_DEF_RING_BLOCKS;\r\ndevice_config->vp_config[i].ring.buffer_mode =\r\nVXGE_HW_RING_RXD_BUFFER_MODE_DEFAULT;\r\ndevice_config->vp_config[i].ring.scatter_mode =\r\nVXGE_HW_RING_SCATTER_MODE_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].ring.rxds_limit =\r\nVXGE_HW_DEF_RING_RXDS_LIMIT;\r\ndevice_config->vp_config[i].fifo.enable = VXGE_HW_FIFO_ENABLE;\r\ndevice_config->vp_config[i].fifo.fifo_blocks =\r\nVXGE_HW_MIN_FIFO_BLOCKS;\r\ndevice_config->vp_config[i].fifo.max_frags =\r\nVXGE_HW_MAX_FIFO_FRAGS;\r\ndevice_config->vp_config[i].fifo.memblock_size =\r\nVXGE_HW_DEF_FIFO_MEMBLOCK_SIZE;\r\ndevice_config->vp_config[i].fifo.alignment_size =\r\nVXGE_HW_DEF_FIFO_ALIGNMENT_SIZE;\r\ndevice_config->vp_config[i].fifo.intr =\r\nVXGE_HW_FIFO_QUEUE_INTR_DEFAULT;\r\ndevice_config->vp_config[i].fifo.no_snoop_bits =\r\nVXGE_HW_FIFO_NO_SNOOP_DEFAULT;\r\ndevice_config->vp_config[i].tti.intr_enable =\r\nVXGE_HW_TIM_INTR_DEFAULT;\r\ndevice_config->vp_config[i].tti.btimer_val =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.timer_ac_en =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.timer_ci_en =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.timer_ri_en =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.rtimer_val =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.util_sel =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.ltimer_val =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.urange_a =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.uec_a =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.urange_b =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.uec_b =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.urange_c =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.uec_c =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].tti.uec_d =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.intr_enable =\r\nVXGE_HW_TIM_INTR_DEFAULT;\r\ndevice_config->vp_config[i].rti.btimer_val =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.timer_ac_en =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.timer_ci_en =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.timer_ri_en =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.rtimer_val =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.util_sel =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.ltimer_val =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.urange_a =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.uec_a =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.urange_b =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.uec_b =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.urange_c =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.uec_c =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].rti.uec_d =\r\nVXGE_HW_USE_FLASH_DEFAULT;\r\ndevice_config->vp_config[i].mtu =\r\nVXGE_HW_VPATH_USE_FLASH_DEFAULT_INITIAL_MTU;\r\ndevice_config->vp_config[i].rpa_strip_vlan_tag =\r\nVXGE_HW_VPATH_RPA_STRIP_VLAN_TAG_USE_FLASH_DEFAULT;\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_swapper_set(struct vxge_hw_vpath_reg __iomem *vpath_reg)\r\n{\r\n#ifndef __BIG_ENDIAN\r\nu64 val64;\r\nval64 = readq(&vpath_reg->vpath_general_cfg1);\r\nwmb();\r\nval64 |= VXGE_HW_VPATH_GENERAL_CFG1_CTL_BYTE_SWAPEN;\r\nwriteq(val64, &vpath_reg->vpath_general_cfg1);\r\nwmb();\r\n#endif\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_kdfc_swapper_set(struct vxge_hw_legacy_reg __iomem *legacy_reg,\r\nstruct vxge_hw_vpath_reg __iomem *vpath_reg)\r\n{\r\nu64 val64;\r\nval64 = readq(&legacy_reg->pifm_wr_swap_en);\r\nif (val64 == VXGE_HW_SWAPPER_WRITE_BYTE_SWAP_ENABLE) {\r\nval64 = readq(&vpath_reg->kdfcctl_cfg0);\r\nwmb();\r\nval64 |= VXGE_HW_KDFCCTL_CFG0_BYTE_SWAPEN_FIFO0 |\r\nVXGE_HW_KDFCCTL_CFG0_BYTE_SWAPEN_FIFO1 |\r\nVXGE_HW_KDFCCTL_CFG0_BYTE_SWAPEN_FIFO2;\r\nwriteq(val64, &vpath_reg->kdfcctl_cfg0);\r\nwmb();\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_mgmt_reg_read(struct __vxge_hw_device *hldev,\r\nenum vxge_hw_mgmt_reg_type type,\r\nu32 index, u32 offset, u64 *value)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((hldev == NULL) || (hldev->magic != VXGE_HW_DEVICE_MAGIC)) {\r\nstatus = VXGE_HW_ERR_INVALID_DEVICE;\r\ngoto exit;\r\n}\r\nswitch (type) {\r\ncase vxge_hw_mgmt_reg_type_legacy:\r\nif (offset > sizeof(struct vxge_hw_legacy_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\n*value = readq((void __iomem *)hldev->legacy_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_toc:\r\nif (offset > sizeof(struct vxge_hw_toc_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\n*value = readq((void __iomem *)hldev->toc_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_common:\r\nif (offset > sizeof(struct vxge_hw_common_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\n*value = readq((void __iomem *)hldev->common_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_mrpcim:\r\nif (!(hldev->access_rights &\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM)) {\r\nstatus = VXGE_HW_ERR_PRIVILAGED_OPEARATION;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_mrpcim_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\n*value = readq((void __iomem *)hldev->mrpcim_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_srpcim:\r\nif (!(hldev->access_rights &\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_SRPCIM)) {\r\nstatus = VXGE_HW_ERR_PRIVILAGED_OPEARATION;\r\nbreak;\r\n}\r\nif (index > VXGE_HW_TITAN_SRPCIM_REG_SPACES - 1) {\r\nstatus = VXGE_HW_ERR_INVALID_INDEX;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_srpcim_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\n*value = readq((void __iomem *)hldev->srpcim_reg[index] +\r\noffset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_vpmgmt:\r\nif ((index > VXGE_HW_TITAN_VPMGMT_REG_SPACES - 1) ||\r\n(!(hldev->vpath_assignments & vxge_mBIT(index)))) {\r\nstatus = VXGE_HW_ERR_INVALID_INDEX;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_vpmgmt_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\n*value = readq((void __iomem *)hldev->vpmgmt_reg[index] +\r\noffset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_vpath:\r\nif ((index > VXGE_HW_TITAN_VPATH_REG_SPACES - 1) ||\r\n(!(hldev->vpath_assignments & vxge_mBIT(index)))) {\r\nstatus = VXGE_HW_ERR_INVALID_INDEX;\r\nbreak;\r\n}\r\nif (index > VXGE_HW_TITAN_VPATH_REG_SPACES - 1) {\r\nstatus = VXGE_HW_ERR_INVALID_INDEX;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_vpath_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\n*value = readq((void __iomem *)hldev->vpath_reg[index] +\r\noffset);\r\nbreak;\r\ndefault:\r\nstatus = VXGE_HW_ERR_INVALID_TYPE;\r\nbreak;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_strip_fcs_check(struct __vxge_hw_device *hldev, u64 vpath_mask)\r\n{\r\nstruct vxge_hw_vpmgmt_reg __iomem *vpmgmt_reg;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nint i = 0, j = 0;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!((vpath_mask) & vxge_mBIT(i)))\r\ncontinue;\r\nvpmgmt_reg = hldev->vpmgmt_reg[i];\r\nfor (j = 0; j < VXGE_HW_MAC_MAX_MAC_PORT_ID; j++) {\r\nif (readq(&vpmgmt_reg->rxmac_cfg0_port_vpmgmt_clone[j])\r\n& VXGE_HW_RXMAC_CFG0_PORT_VPMGMT_CLONE_STRIP_FCS)\r\nreturn VXGE_HW_FAIL;\r\n}\r\n}\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_mgmt_reg_write(struct __vxge_hw_device *hldev,\r\nenum vxge_hw_mgmt_reg_type type,\r\nu32 index, u32 offset, u64 value)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((hldev == NULL) || (hldev->magic != VXGE_HW_DEVICE_MAGIC)) {\r\nstatus = VXGE_HW_ERR_INVALID_DEVICE;\r\ngoto exit;\r\n}\r\nswitch (type) {\r\ncase vxge_hw_mgmt_reg_type_legacy:\r\nif (offset > sizeof(struct vxge_hw_legacy_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\nwriteq(value, (void __iomem *)hldev->legacy_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_toc:\r\nif (offset > sizeof(struct vxge_hw_toc_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\nwriteq(value, (void __iomem *)hldev->toc_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_common:\r\nif (offset > sizeof(struct vxge_hw_common_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\nwriteq(value, (void __iomem *)hldev->common_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_mrpcim:\r\nif (!(hldev->access_rights &\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM)) {\r\nstatus = VXGE_HW_ERR_PRIVILAGED_OPEARATION;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_mrpcim_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\nwriteq(value, (void __iomem *)hldev->mrpcim_reg + offset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_srpcim:\r\nif (!(hldev->access_rights &\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_SRPCIM)) {\r\nstatus = VXGE_HW_ERR_PRIVILAGED_OPEARATION;\r\nbreak;\r\n}\r\nif (index > VXGE_HW_TITAN_SRPCIM_REG_SPACES - 1) {\r\nstatus = VXGE_HW_ERR_INVALID_INDEX;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_srpcim_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\nwriteq(value, (void __iomem *)hldev->srpcim_reg[index] +\r\noffset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_vpmgmt:\r\nif ((index > VXGE_HW_TITAN_VPMGMT_REG_SPACES - 1) ||\r\n(!(hldev->vpath_assignments & vxge_mBIT(index)))) {\r\nstatus = VXGE_HW_ERR_INVALID_INDEX;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_vpmgmt_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\nwriteq(value, (void __iomem *)hldev->vpmgmt_reg[index] +\r\noffset);\r\nbreak;\r\ncase vxge_hw_mgmt_reg_type_vpath:\r\nif ((index > VXGE_HW_TITAN_VPATH_REG_SPACES-1) ||\r\n(!(hldev->vpath_assignments & vxge_mBIT(index)))) {\r\nstatus = VXGE_HW_ERR_INVALID_INDEX;\r\nbreak;\r\n}\r\nif (offset > sizeof(struct vxge_hw_vpath_reg) - 8) {\r\nstatus = VXGE_HW_ERR_INVALID_OFFSET;\r\nbreak;\r\n}\r\nwriteq(value, (void __iomem *)hldev->vpath_reg[index] +\r\noffset);\r\nbreak;\r\ndefault:\r\nstatus = VXGE_HW_ERR_INVALID_TYPE;\r\nbreak;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status __vxge_hw_fifo_abort(struct __vxge_hw_fifo *fifo)\r\n{\r\nvoid *txdlh;\r\nfor (;;) {\r\nvxge_hw_channel_dtr_try_complete(&fifo->channel, &txdlh);\r\nif (txdlh == NULL)\r\nbreak;\r\nvxge_hw_channel_dtr_complete(&fifo->channel);\r\nif (fifo->txdl_term) {\r\nfifo->txdl_term(txdlh,\r\nVXGE_HW_TXDL_STATE_POSTED,\r\nfifo->channel.userdata);\r\n}\r\nvxge_hw_channel_dtr_free(&fifo->channel, txdlh);\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status __vxge_hw_fifo_reset(struct __vxge_hw_fifo *fifo)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\n__vxge_hw_fifo_abort(fifo);\r\nstatus = __vxge_hw_channel_reset(&fifo->channel);\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_fifo_delete(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nstruct __vxge_hw_fifo *fifo = vp->vpath->fifoh;\r\n__vxge_hw_fifo_abort(fifo);\r\nif (fifo->mempool)\r\n__vxge_hw_mempool_destroy(fifo->mempool);\r\nvp->vpath->fifoh = NULL;\r\n__vxge_hw_channel_free(&fifo->channel);\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic void\r\n__vxge_hw_fifo_mempool_item_alloc(\r\nstruct vxge_hw_mempool *mempoolh,\r\nu32 memblock_index, struct vxge_hw_mempool_dma *dma_object,\r\nu32 index, u32 is_last)\r\n{\r\nu32 memblock_item_idx;\r\nstruct __vxge_hw_fifo_txdl_priv *txdl_priv;\r\nstruct vxge_hw_fifo_txd *txdp =\r\n(struct vxge_hw_fifo_txd *)mempoolh->items_arr[index];\r\nstruct __vxge_hw_fifo *fifo =\r\n(struct __vxge_hw_fifo *)mempoolh->userdata;\r\nvoid *memblock = mempoolh->memblocks_arr[memblock_index];\r\nvxge_assert(txdp);\r\ntxdp->host_control = (u64) (size_t)\r\n__vxge_hw_mempool_item_priv(mempoolh, memblock_index, txdp,\r\n&memblock_item_idx);\r\ntxdl_priv = __vxge_hw_fifo_txdl_priv(fifo, txdp);\r\nvxge_assert(txdl_priv);\r\nfifo->channel.reserve_arr[fifo->channel.reserve_ptr - 1 - index] = txdp;\r\ntxdl_priv->dma_offset = (char *)txdp - (char *)memblock;\r\ntxdl_priv->dma_addr = dma_object->addr + txdl_priv->dma_offset;\r\ntxdl_priv->dma_handle = dma_object->handle;\r\ntxdl_priv->memblock = memblock;\r\ntxdl_priv->first_txdp = txdp;\r\ntxdl_priv->next_txdl_priv = NULL;\r\ntxdl_priv->alloc_frags = 0;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_fifo_create(struct __vxge_hw_vpath_handle *vp,\r\nstruct vxge_hw_fifo_attr *attr)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_fifo *fifo;\r\nstruct vxge_hw_fifo_config *config;\r\nu32 txdl_size, txdl_per_memblock;\r\nstruct vxge_hw_mempool_cbs fifo_mp_callback;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nif ((vp == NULL) || (attr == NULL)) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nconfig = &vpath->hldev->config.vp_config[vpath->vp_id].fifo;\r\ntxdl_size = config->max_frags * sizeof(struct vxge_hw_fifo_txd);\r\ntxdl_per_memblock = config->memblock_size / txdl_size;\r\nfifo = (struct __vxge_hw_fifo *)__vxge_hw_channel_allocate(vp,\r\nVXGE_HW_CHANNEL_TYPE_FIFO,\r\nconfig->fifo_blocks * txdl_per_memblock,\r\nattr->per_txdl_space, attr->userdata);\r\nif (fifo == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nvpath->fifoh = fifo;\r\nfifo->nofl_db = vpath->nofl_db;\r\nfifo->vp_id = vpath->vp_id;\r\nfifo->vp_reg = vpath->vp_reg;\r\nfifo->stats = &vpath->sw_stats->fifo_stats;\r\nfifo->config = config;\r\nfifo->interrupt_type = VXGE_HW_FIFO_TXD_INT_TYPE_UTILZ;\r\nfifo->tim_tti_cfg1_saved = vpath->tim_tti_cfg1_saved;\r\nfifo->tim_tti_cfg3_saved = vpath->tim_tti_cfg3_saved;\r\nif (fifo->config->intr)\r\nfifo->interrupt_type = VXGE_HW_FIFO_TXD_INT_TYPE_PER_LIST;\r\nfifo->no_snoop_bits = config->no_snoop_bits;\r\nfifo->priv_size =\r\nsizeof(struct __vxge_hw_fifo_txdl_priv) + attr->per_txdl_space;\r\nfifo->priv_size = ((fifo->priv_size + VXGE_CACHE_LINE_SIZE - 1) /\r\nVXGE_CACHE_LINE_SIZE) * VXGE_CACHE_LINE_SIZE;\r\nfifo->per_txdl_space = attr->per_txdl_space;\r\nfifo->txdl_size = txdl_size;\r\nfifo->txdl_per_memblock = txdl_per_memblock;\r\nfifo->txdl_term = attr->txdl_term;\r\nfifo->callback = attr->callback;\r\nif (fifo->txdl_per_memblock == 0) {\r\n__vxge_hw_fifo_delete(vp);\r\nstatus = VXGE_HW_ERR_INVALID_BLOCK_SIZE;\r\ngoto exit;\r\n}\r\nfifo_mp_callback.item_func_alloc = __vxge_hw_fifo_mempool_item_alloc;\r\nfifo->mempool =\r\n__vxge_hw_mempool_create(vpath->hldev,\r\nfifo->config->memblock_size,\r\nfifo->txdl_size,\r\nfifo->priv_size,\r\n(fifo->config->fifo_blocks * fifo->txdl_per_memblock),\r\n(fifo->config->fifo_blocks * fifo->txdl_per_memblock),\r\n&fifo_mp_callback,\r\nfifo);\r\nif (fifo->mempool == NULL) {\r\n__vxge_hw_fifo_delete(vp);\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_channel_initialize(&fifo->channel);\r\nif (status != VXGE_HW_OK) {\r\n__vxge_hw_fifo_delete(vp);\r\ngoto exit;\r\n}\r\nvxge_assert(fifo->channel.reserve_ptr);\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_pci_read(struct __vxge_hw_virtualpath *vpath,\r\nu32 phy_func_0, u32 offset, u32 *val)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg = vpath->vp_reg;\r\nval64 = VXGE_HW_PCI_CONFIG_ACCESS_CFG1_ADDRESS(offset);\r\nif (phy_func_0)\r\nval64 |= VXGE_HW_PCI_CONFIG_ACCESS_CFG1_SEL_FUNC0;\r\nwriteq(val64, &vp_reg->pci_config_access_cfg1);\r\nwmb();\r\nwriteq(VXGE_HW_PCI_CONFIG_ACCESS_CFG2_REQ,\r\n&vp_reg->pci_config_access_cfg2);\r\nwmb();\r\nstatus = __vxge_hw_device_register_poll(\r\n&vp_reg->pci_config_access_cfg2,\r\nVXGE_HW_INTR_MASK_ALL, VXGE_HW_DEF_DEVICE_POLL_MILLIS);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nval64 = readq(&vp_reg->pci_config_access_status);\r\nif (val64 & VXGE_HW_PCI_CONFIG_ACCESS_STATUS_ACCESS_ERR) {\r\nstatus = VXGE_HW_FAIL;\r\n*val = 0;\r\n} else\r\n*val = (u32)vxge_bVALn(val64, 32, 32);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_device_flick_link_led(struct __vxge_hw_device *hldev, u64 on_off)\r\n{\r\nstruct __vxge_hw_virtualpath *vpath;\r\nu64 data0, data1 = 0, steer_ctrl = 0;\r\nenum vxge_hw_status status;\r\nif (hldev == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_DEVICE;\r\ngoto exit;\r\n}\r\nvpath = &hldev->virtual_paths[hldev->first_vp_id];\r\ndata0 = on_off;\r\nstatus = vxge_hw_vpath_fw_api(vpath,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_LED_CONTROL,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_FW_MEMO,\r\n0, &data0, &data1, &steer_ctrl);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\n__vxge_hw_vpath_rts_table_get(struct __vxge_hw_vpath_handle *vp,\r\nu32 action, u32 rts_table, u32 offset,\r\nu64 *data0, u64 *data1)\r\n{\r\nenum vxge_hw_status status;\r\nu64 steer_ctrl = 0;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nif ((rts_table ==\r\nVXGE_HW_RTS_ACS_STEER_CTRL_DATA_STRUCT_SEL_RTH_SOLO_IT) ||\r\n(rts_table ==\r\nVXGE_HW_RTS_ACS_STEER_CTRL_DATA_STRUCT_SEL_RTH_MULTI_IT) ||\r\n(rts_table ==\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_RTH_MASK) ||\r\n(rts_table ==\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_RTH_KEY)) {\r\nsteer_ctrl = VXGE_HW_RTS_ACCESS_STEER_CTRL_TABLE_SEL;\r\n}\r\nstatus = vxge_hw_vpath_fw_api(vp->vpath, action, rts_table, offset,\r\ndata0, data1, &steer_ctrl);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nif ((rts_table != VXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_DA) &&\r\n(rts_table !=\r\nVXGE_HW_RTS_ACS_STEER_CTRL_DATA_STRUCT_SEL_RTH_MULTI_IT))\r\n*data1 = 0;\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\n__vxge_hw_vpath_rts_table_set(struct __vxge_hw_vpath_handle *vp, u32 action,\r\nu32 rts_table, u32 offset, u64 steer_data0,\r\nu64 steer_data1)\r\n{\r\nu64 data0, data1 = 0, steer_ctrl = 0;\r\nenum vxge_hw_status status;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\ndata0 = steer_data0;\r\nif ((rts_table == VXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_DA) ||\r\n(rts_table ==\r\nVXGE_HW_RTS_ACS_STEER_CTRL_DATA_STRUCT_SEL_RTH_MULTI_IT))\r\ndata1 = steer_data1;\r\nstatus = vxge_hw_vpath_fw_api(vp->vpath, action, rts_table, offset,\r\n&data0, &data1, &steer_ctrl);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_rts_rth_set(\r\nstruct __vxge_hw_vpath_handle *vp,\r\nenum vxge_hw_rth_algoritms algorithm,\r\nstruct vxge_hw_rth_hash_types *hash_type,\r\nu16 bucket_size)\r\n{\r\nu64 data0, data1;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_rts_table_get(vp,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_READ_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_RTH_GEN_CFG,\r\n0, &data0, &data1);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\ndata0 &= ~(VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_BUCKET_SIZE(0xf) |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_ALG_SEL(0x3));\r\ndata0 |= VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_RTH_EN |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_BUCKET_SIZE(bucket_size) |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_ALG_SEL(algorithm);\r\nif (hash_type->hash_type_tcpipv4_en)\r\ndata0 |= VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_RTH_TCP_IPV4_EN;\r\nif (hash_type->hash_type_ipv4_en)\r\ndata0 |= VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_RTH_IPV4_EN;\r\nif (hash_type->hash_type_tcpipv6_en)\r\ndata0 |= VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_RTH_TCP_IPV6_EN;\r\nif (hash_type->hash_type_ipv6_en)\r\ndata0 |= VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_RTH_IPV6_EN;\r\nif (hash_type->hash_type_tcpipv6ex_en)\r\ndata0 |=\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_RTH_TCP_IPV6_EX_EN;\r\nif (hash_type->hash_type_ipv6ex_en)\r\ndata0 |= VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_RTH_IPV6_EX_EN;\r\nif (VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_RTH_GEN_ACTIVE_TABLE(data0))\r\ndata0 &= ~VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_ACTIVE_TABLE;\r\nelse\r\ndata0 |= VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_GEN_ACTIVE_TABLE;\r\nstatus = __vxge_hw_vpath_rts_table_set(vp,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_WRITE_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_RTH_GEN_CFG,\r\n0, data0, 0);\r\nexit:\r\nreturn status;\r\n}\r\nstatic void\r\nvxge_hw_rts_rth_data0_data1_get(u32 j, u64 *data0, u64 *data1,\r\nu16 flag, u8 *itable)\r\n{\r\nswitch (flag) {\r\ncase 1:\r\n*data0 = VXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_ITEM0_BUCKET_NUM(j)|\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_ITEM0_ENTRY_EN |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_ITEM0_BUCKET_DATA(\r\nitable[j]);\r\ncase 2:\r\n*data0 |=\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_ITEM1_BUCKET_NUM(j)|\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_ITEM1_ENTRY_EN |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_ITEM1_BUCKET_DATA(\r\nitable[j]);\r\ncase 3:\r\n*data1 = VXGE_HW_RTS_ACCESS_STEER_DATA1_RTH_ITEM0_BUCKET_NUM(j)|\r\nVXGE_HW_RTS_ACCESS_STEER_DATA1_RTH_ITEM0_ENTRY_EN |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA1_RTH_ITEM0_BUCKET_DATA(\r\nitable[j]);\r\ncase 4:\r\n*data1 |=\r\nVXGE_HW_RTS_ACCESS_STEER_DATA1_RTH_ITEM1_BUCKET_NUM(j)|\r\nVXGE_HW_RTS_ACCESS_STEER_DATA1_RTH_ITEM1_ENTRY_EN |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA1_RTH_ITEM1_BUCKET_DATA(\r\nitable[j]);\r\ndefault:\r\nreturn;\r\n}\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_rts_rth_itable_set(\r\nstruct __vxge_hw_vpath_handle **vpath_handles,\r\nu32 vpath_count,\r\nu8 *mtable,\r\nu8 *itable,\r\nu32 itable_size)\r\n{\r\nu32 i, j, action, rts_table;\r\nu64 data0;\r\nu64 data1;\r\nu32 max_entries;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_vpath_handle *vp = vpath_handles[0];\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nmax_entries = (((u32)1) << itable_size);\r\nif (vp->vpath->hldev->config.rth_it_type\r\n== VXGE_HW_RTH_IT_TYPE_SOLO_IT) {\r\naction = VXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_WRITE_ENTRY;\r\nrts_table =\r\nVXGE_HW_RTS_ACS_STEER_CTRL_DATA_STRUCT_SEL_RTH_SOLO_IT;\r\nfor (j = 0; j < max_entries; j++) {\r\ndata1 = 0;\r\ndata0 =\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_SOLO_IT_BUCKET_DATA(\r\nitable[j]);\r\nstatus = __vxge_hw_vpath_rts_table_set(vpath_handles[0],\r\naction, rts_table, j, data0, data1);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\nfor (j = 0; j < max_entries; j++) {\r\ndata1 = 0;\r\ndata0 =\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_SOLO_IT_ENTRY_EN |\r\nVXGE_HW_RTS_ACCESS_STEER_DATA0_RTH_SOLO_IT_BUCKET_DATA(\r\nitable[j]);\r\nstatus = __vxge_hw_vpath_rts_table_set(\r\nvpath_handles[mtable[itable[j]]], action,\r\nrts_table, j, data0, data1);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\n} else {\r\naction = VXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_WRITE_ENTRY;\r\nrts_table =\r\nVXGE_HW_RTS_ACS_STEER_CTRL_DATA_STRUCT_SEL_RTH_MULTI_IT;\r\nfor (i = 0; i < vpath_count; i++) {\r\nfor (j = 0; j < max_entries;) {\r\ndata0 = 0;\r\ndata1 = 0;\r\nwhile (j < max_entries) {\r\nif (mtable[itable[j]] != i) {\r\nj++;\r\ncontinue;\r\n}\r\nvxge_hw_rts_rth_data0_data1_get(j,\r\n&data0, &data1, 1, itable);\r\nj++;\r\nbreak;\r\n}\r\nwhile (j < max_entries) {\r\nif (mtable[itable[j]] != i) {\r\nj++;\r\ncontinue;\r\n}\r\nvxge_hw_rts_rth_data0_data1_get(j,\r\n&data0, &data1, 2, itable);\r\nj++;\r\nbreak;\r\n}\r\nwhile (j < max_entries) {\r\nif (mtable[itable[j]] != i) {\r\nj++;\r\ncontinue;\r\n}\r\nvxge_hw_rts_rth_data0_data1_get(j,\r\n&data0, &data1, 3, itable);\r\nj++;\r\nbreak;\r\n}\r\nwhile (j < max_entries) {\r\nif (mtable[itable[j]] != i) {\r\nj++;\r\ncontinue;\r\n}\r\nvxge_hw_rts_rth_data0_data1_get(j,\r\n&data0, &data1, 4, itable);\r\nj++;\r\nbreak;\r\n}\r\nif (data0 != 0) {\r\nstatus = __vxge_hw_vpath_rts_table_set(\r\nvpath_handles[i],\r\naction, rts_table,\r\n0, data0, data1);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\n}\r\n}\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_check_leak(struct __vxge_hw_ring *ring)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nu64 rxd_new_count, rxd_spat;\r\nif (ring == NULL)\r\nreturn status;\r\nrxd_new_count = readl(&ring->vp_reg->prc_rxd_doorbell);\r\nrxd_spat = readq(&ring->vp_reg->prc_cfg6);\r\nrxd_spat = VXGE_HW_PRC_CFG6_RXD_SPAT(rxd_spat);\r\nif (rxd_new_count >= rxd_spat)\r\nstatus = VXGE_HW_FAIL;\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_mgmt_read(\r\nstruct __vxge_hw_device *hldev,\r\nstruct __vxge_hw_virtualpath *vpath)\r\n{\r\nu32 i, mtu = 0, max_pyld = 0;\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nfor (i = 0; i < VXGE_HW_MAC_MAX_MAC_PORT_ID; i++) {\r\nval64 = readq(&vpath->vpmgmt_reg->\r\nrxmac_cfg0_port_vpmgmt_clone[i]);\r\nmax_pyld =\r\n(u32)\r\nVXGE_HW_RXMAC_CFG0_PORT_VPMGMT_CLONE_GET_MAX_PYLD_LEN\r\n(val64);\r\nif (mtu < max_pyld)\r\nmtu = max_pyld;\r\n}\r\nvpath->max_mtu = mtu + VXGE_HW_MAC_HEADER_MAX_SIZE;\r\nval64 = readq(&vpath->vpmgmt_reg->xmac_vsport_choices_vp);\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (val64 & vxge_mBIT(i))\r\nvpath->vsport_number = i;\r\n}\r\nval64 = readq(&vpath->vpmgmt_reg->xgmac_gen_status_vpmgmt_clone);\r\nif (val64 & VXGE_HW_XGMAC_GEN_STATUS_VPMGMT_CLONE_XMACJ_NTWK_OK)\r\nVXGE_HW_DEVICE_LINK_STATE_SET(vpath->hldev, VXGE_HW_LINK_UP);\r\nelse\r\nVXGE_HW_DEVICE_LINK_STATE_SET(vpath->hldev, VXGE_HW_LINK_DOWN);\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_reset_check(struct __vxge_hw_virtualpath *vpath)\r\n{\r\nenum vxge_hw_status status;\r\nstatus = __vxge_hw_device_register_poll(\r\n&vpath->hldev->common_reg->vpath_rst_in_prog,\r\nVXGE_HW_VPATH_RST_IN_PROG_VPATH_RST_IN_PROG(\r\n1 << (16 - vpath->vp_id)),\r\nvpath->hldev->config.device_poll_millis);\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_reset(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nval64 = VXGE_HW_CMN_RSTHDLR_CFG0_SW_RESET_VPATH(1 << (16 - vp_id));\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(val64, 0, 32),\r\n&hldev->common_reg->cmn_rsthdlr_cfg0);\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_sw_reset(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nif (vpath->ringh) {\r\nstatus = __vxge_hw_ring_reset(vpath->ringh);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\n}\r\nif (vpath->fifoh)\r\nstatus = __vxge_hw_fifo_reset(vpath->fifoh);\r\nexit:\r\nreturn status;\r\n}\r\nstatic void\r\n__vxge_hw_vpath_prc_configure(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nstruct vxge_hw_vp_config *vp_config;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nvp_reg = vpath->vp_reg;\r\nvp_config = vpath->vp_config;\r\nif (vp_config->ring.enable == VXGE_HW_RING_DISABLE)\r\nreturn;\r\nval64 = readq(&vp_reg->prc_cfg1);\r\nval64 |= VXGE_HW_PRC_CFG1_RTI_TINT_DISABLE;\r\nwriteq(val64, &vp_reg->prc_cfg1);\r\nval64 = readq(&vpath->vp_reg->prc_cfg6);\r\nval64 |= VXGE_HW_PRC_CFG6_DOORBELL_MODE_EN;\r\nwriteq(val64, &vpath->vp_reg->prc_cfg6);\r\nval64 = readq(&vp_reg->prc_cfg7);\r\nif (vpath->vp_config->ring.scatter_mode !=\r\nVXGE_HW_RING_SCATTER_MODE_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_PRC_CFG7_SCATTER_MODE(0x3);\r\nswitch (vpath->vp_config->ring.scatter_mode) {\r\ncase VXGE_HW_RING_SCATTER_MODE_A:\r\nval64 |= VXGE_HW_PRC_CFG7_SCATTER_MODE(\r\nVXGE_HW_PRC_CFG7_SCATTER_MODE_A);\r\nbreak;\r\ncase VXGE_HW_RING_SCATTER_MODE_B:\r\nval64 |= VXGE_HW_PRC_CFG7_SCATTER_MODE(\r\nVXGE_HW_PRC_CFG7_SCATTER_MODE_B);\r\nbreak;\r\ncase VXGE_HW_RING_SCATTER_MODE_C:\r\nval64 |= VXGE_HW_PRC_CFG7_SCATTER_MODE(\r\nVXGE_HW_PRC_CFG7_SCATTER_MODE_C);\r\nbreak;\r\n}\r\n}\r\nwriteq(val64, &vp_reg->prc_cfg7);\r\nwriteq(VXGE_HW_PRC_CFG5_RXD0_ADD(\r\n__vxge_hw_ring_first_block_address_get(\r\nvpath->ringh) >> 3), &vp_reg->prc_cfg5);\r\nval64 = readq(&vp_reg->prc_cfg4);\r\nval64 |= VXGE_HW_PRC_CFG4_IN_SVC;\r\nval64 &= ~VXGE_HW_PRC_CFG4_RING_MODE(0x3);\r\nval64 |= VXGE_HW_PRC_CFG4_RING_MODE(\r\nVXGE_HW_PRC_CFG4_RING_MODE_ONE_BUFFER);\r\nif (hldev->config.rth_en == VXGE_HW_RTH_DISABLE)\r\nval64 |= VXGE_HW_PRC_CFG4_RTH_DISABLE;\r\nelse\r\nval64 &= ~VXGE_HW_PRC_CFG4_RTH_DISABLE;\r\nwriteq(val64, &vp_reg->prc_cfg4);\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_kdfc_configure(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nu64 val64;\r\nu64 vpath_stride;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nvp_reg = vpath->vp_reg;\r\nstatus = __vxge_hw_kdfc_swapper_set(hldev->legacy_reg, vp_reg);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nval64 = readq(&vp_reg->kdfc_drbl_triplet_total);\r\nvpath->max_kdfc_db =\r\n(u32)VXGE_HW_KDFC_DRBL_TRIPLET_TOTAL_GET_KDFC_MAX_SIZE(\r\nval64+1)/2;\r\nif (vpath->vp_config->fifo.enable == VXGE_HW_FIFO_ENABLE) {\r\nvpath->max_nofl_db = vpath->max_kdfc_db;\r\nif (vpath->max_nofl_db <\r\n((vpath->vp_config->fifo.memblock_size /\r\n(vpath->vp_config->fifo.max_frags *\r\nsizeof(struct vxge_hw_fifo_txd))) *\r\nvpath->vp_config->fifo.fifo_blocks)) {\r\nreturn VXGE_HW_BADCFG_FIFO_BLOCKS;\r\n}\r\nval64 = VXGE_HW_KDFC_FIFO_TRPL_PARTITION_LENGTH_0(\r\n(vpath->max_nofl_db*2)-1);\r\n}\r\nwriteq(val64, &vp_reg->kdfc_fifo_trpl_partition);\r\nwriteq(VXGE_HW_KDFC_FIFO_TRPL_CTRL_TRIPLET_ENABLE,\r\n&vp_reg->kdfc_fifo_trpl_ctrl);\r\nval64 = readq(&vp_reg->kdfc_trpl_fifo_0_ctrl);\r\nval64 &= ~(VXGE_HW_KDFC_TRPL_FIFO_0_CTRL_MODE(0x3) |\r\nVXGE_HW_KDFC_TRPL_FIFO_0_CTRL_SELECT(0xFF));\r\nval64 |= VXGE_HW_KDFC_TRPL_FIFO_0_CTRL_MODE(\r\nVXGE_HW_KDFC_TRPL_FIFO_0_CTRL_MODE_NON_OFFLOAD_ONLY) |\r\n#ifndef __BIG_ENDIAN\r\nVXGE_HW_KDFC_TRPL_FIFO_0_CTRL_SWAP_EN |\r\n#endif\r\nVXGE_HW_KDFC_TRPL_FIFO_0_CTRL_SELECT(0);\r\nwriteq(val64, &vp_reg->kdfc_trpl_fifo_0_ctrl);\r\nwriteq((u64)0, &vp_reg->kdfc_trpl_fifo_0_wb_address);\r\nwmb();\r\nvpath_stride = readq(&hldev->toc_reg->toc_kdfc_vpath_stride);\r\nvpath->nofl_db =\r\n(struct __vxge_hw_non_offload_db_wrapper __iomem *)\r\n(hldev->kdfc + (vp_id *\r\nVXGE_HW_TOC_KDFC_VPATH_STRIDE_GET_TOC_KDFC_VPATH_STRIDE(\r\nvpath_stride)));\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_mac_configure(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nstruct vxge_hw_vp_config *vp_config;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nvp_reg = vpath->vp_reg;\r\nvp_config = vpath->vp_config;\r\nwriteq(VXGE_HW_XMAC_VSPORT_CHOICE_VSPORT_NUMBER(\r\nvpath->vsport_number), &vp_reg->xmac_vsport_choice);\r\nif (vp_config->ring.enable == VXGE_HW_RING_ENABLE) {\r\nval64 = readq(&vp_reg->xmac_rpa_vcfg);\r\nif (vp_config->rpa_strip_vlan_tag !=\r\nVXGE_HW_VPATH_RPA_STRIP_VLAN_TAG_USE_FLASH_DEFAULT) {\r\nif (vp_config->rpa_strip_vlan_tag)\r\nval64 |= VXGE_HW_XMAC_RPA_VCFG_STRIP_VLAN_TAG;\r\nelse\r\nval64 &= ~VXGE_HW_XMAC_RPA_VCFG_STRIP_VLAN_TAG;\r\n}\r\nwriteq(val64, &vp_reg->xmac_rpa_vcfg);\r\nval64 = readq(&vp_reg->rxmac_vcfg0);\r\nif (vp_config->mtu !=\r\nVXGE_HW_VPATH_USE_FLASH_DEFAULT_INITIAL_MTU) {\r\nval64 &= ~VXGE_HW_RXMAC_VCFG0_RTS_MAX_FRM_LEN(0x3fff);\r\nif ((vp_config->mtu +\r\nVXGE_HW_MAC_HEADER_MAX_SIZE) < vpath->max_mtu)\r\nval64 |= VXGE_HW_RXMAC_VCFG0_RTS_MAX_FRM_LEN(\r\nvp_config->mtu +\r\nVXGE_HW_MAC_HEADER_MAX_SIZE);\r\nelse\r\nval64 |= VXGE_HW_RXMAC_VCFG0_RTS_MAX_FRM_LEN(\r\nvpath->max_mtu);\r\n}\r\nwriteq(val64, &vp_reg->rxmac_vcfg0);\r\nval64 = readq(&vp_reg->rxmac_vcfg1);\r\nval64 &= ~(VXGE_HW_RXMAC_VCFG1_RTS_RTH_MULTI_IT_BD_MODE(0x3) |\r\nVXGE_HW_RXMAC_VCFG1_RTS_RTH_MULTI_IT_EN_MODE);\r\nif (hldev->config.rth_it_type ==\r\nVXGE_HW_RTH_IT_TYPE_MULTI_IT) {\r\nval64 |= VXGE_HW_RXMAC_VCFG1_RTS_RTH_MULTI_IT_BD_MODE(\r\n0x2) |\r\nVXGE_HW_RXMAC_VCFG1_RTS_RTH_MULTI_IT_EN_MODE;\r\n}\r\nwriteq(val64, &vp_reg->rxmac_vcfg1);\r\n}\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_tim_configure(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nstruct vxge_hw_vp_config *config;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nvp_reg = vpath->vp_reg;\r\nconfig = vpath->vp_config;\r\nwriteq(0, &vp_reg->tim_dest_addr);\r\nwriteq(0, &vp_reg->tim_vpath_map);\r\nwriteq(0, &vp_reg->tim_bitmap);\r\nwriteq(0, &vp_reg->tim_remap);\r\nif (config->ring.enable == VXGE_HW_RING_ENABLE)\r\nwriteq(VXGE_HW_TIM_RING_ASSN_INT_NUM(\r\n(vp_id * VXGE_HW_MAX_INTR_PER_VP) +\r\nVXGE_HW_VPATH_INTR_RX), &vp_reg->tim_ring_assn);\r\nval64 = readq(&vp_reg->tim_pci_cfg);\r\nval64 |= VXGE_HW_TIM_PCI_CFG_ADD_PAD;\r\nwriteq(val64, &vp_reg->tim_pci_cfg);\r\nif (config->fifo.enable == VXGE_HW_FIFO_ENABLE) {\r\nval64 = readq(&vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_TX]);\r\nif (config->tti.btimer_val != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_BTIMER_VAL(\r\n0x3ffffff);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_BTIMER_VAL(\r\nconfig->tti.btimer_val);\r\n}\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_BITMP_EN;\r\nif (config->tti.timer_ac_en != VXGE_HW_USE_FLASH_DEFAULT) {\r\nif (config->tti.timer_ac_en)\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_TIMER_AC;\r\nelse\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_TIMER_AC;\r\n}\r\nif (config->tti.timer_ci_en != VXGE_HW_USE_FLASH_DEFAULT) {\r\nif (config->tti.timer_ci_en)\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_TIMER_CI;\r\nelse\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_TIMER_CI;\r\n}\r\nif (config->tti.urange_a != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_URNG_A(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_URNG_A(\r\nconfig->tti.urange_a);\r\n}\r\nif (config->tti.urange_b != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_URNG_B(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_URNG_B(\r\nconfig->tti.urange_b);\r\n}\r\nif (config->tti.urange_c != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_URNG_C(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_URNG_C(\r\nconfig->tti.urange_c);\r\n}\r\nwriteq(val64, &vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_TX]);\r\nvpath->tim_tti_cfg1_saved = val64;\r\nval64 = readq(&vp_reg->tim_cfg2_int_num[VXGE_HW_VPATH_INTR_TX]);\r\nif (config->tti.uec_a != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_A(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_A(\r\nconfig->tti.uec_a);\r\n}\r\nif (config->tti.uec_b != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_B(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_B(\r\nconfig->tti.uec_b);\r\n}\r\nif (config->tti.uec_c != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_C(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_C(\r\nconfig->tti.uec_c);\r\n}\r\nif (config->tti.uec_d != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_D(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_D(\r\nconfig->tti.uec_d);\r\n}\r\nwriteq(val64, &vp_reg->tim_cfg2_int_num[VXGE_HW_VPATH_INTR_TX]);\r\nval64 = readq(&vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_TX]);\r\nif (config->tti.timer_ri_en != VXGE_HW_USE_FLASH_DEFAULT) {\r\nif (config->tti.timer_ri_en)\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_TIMER_RI;\r\nelse\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_TIMER_RI;\r\n}\r\nif (config->tti.rtimer_val != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(\r\n0x3ffffff);\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(\r\nconfig->tti.rtimer_val);\r\n}\r\nif (config->tti.util_sel != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_UTIL_SEL(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_UTIL_SEL(vp_id);\r\n}\r\nif (config->tti.ltimer_val != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_LTIMER_VAL(\r\n0x3ffffff);\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_LTIMER_VAL(\r\nconfig->tti.ltimer_val);\r\n}\r\nwriteq(val64, &vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_TX]);\r\nvpath->tim_tti_cfg3_saved = val64;\r\n}\r\nif (config->ring.enable == VXGE_HW_RING_ENABLE) {\r\nval64 = readq(&vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_RX]);\r\nif (config->rti.btimer_val != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_BTIMER_VAL(\r\n0x3ffffff);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_BTIMER_VAL(\r\nconfig->rti.btimer_val);\r\n}\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_BITMP_EN;\r\nif (config->rti.timer_ac_en != VXGE_HW_USE_FLASH_DEFAULT) {\r\nif (config->rti.timer_ac_en)\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_TIMER_AC;\r\nelse\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_TIMER_AC;\r\n}\r\nif (config->rti.timer_ci_en != VXGE_HW_USE_FLASH_DEFAULT) {\r\nif (config->rti.timer_ci_en)\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_TIMER_CI;\r\nelse\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_TIMER_CI;\r\n}\r\nif (config->rti.urange_a != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_URNG_A(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_URNG_A(\r\nconfig->rti.urange_a);\r\n}\r\nif (config->rti.urange_b != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_URNG_B(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_URNG_B(\r\nconfig->rti.urange_b);\r\n}\r\nif (config->rti.urange_c != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG1_INT_NUM_URNG_C(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_URNG_C(\r\nconfig->rti.urange_c);\r\n}\r\nwriteq(val64, &vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_RX]);\r\nvpath->tim_rti_cfg1_saved = val64;\r\nval64 = readq(&vp_reg->tim_cfg2_int_num[VXGE_HW_VPATH_INTR_RX]);\r\nif (config->rti.uec_a != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_A(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_A(\r\nconfig->rti.uec_a);\r\n}\r\nif (config->rti.uec_b != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_B(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_B(\r\nconfig->rti.uec_b);\r\n}\r\nif (config->rti.uec_c != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_C(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_C(\r\nconfig->rti.uec_c);\r\n}\r\nif (config->rti.uec_d != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG2_INT_NUM_UEC_D(0xffff);\r\nval64 |= VXGE_HW_TIM_CFG2_INT_NUM_UEC_D(\r\nconfig->rti.uec_d);\r\n}\r\nwriteq(val64, &vp_reg->tim_cfg2_int_num[VXGE_HW_VPATH_INTR_RX]);\r\nval64 = readq(&vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_RX]);\r\nif (config->rti.timer_ri_en != VXGE_HW_USE_FLASH_DEFAULT) {\r\nif (config->rti.timer_ri_en)\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_TIMER_RI;\r\nelse\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_TIMER_RI;\r\n}\r\nif (config->rti.rtimer_val != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(\r\n0x3ffffff);\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(\r\nconfig->rti.rtimer_val);\r\n}\r\nif (config->rti.util_sel != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_UTIL_SEL(0x3f);\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_UTIL_SEL(vp_id);\r\n}\r\nif (config->rti.ltimer_val != VXGE_HW_USE_FLASH_DEFAULT) {\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_LTIMER_VAL(\r\n0x3ffffff);\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_LTIMER_VAL(\r\nconfig->rti.ltimer_val);\r\n}\r\nwriteq(val64, &vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_RX]);\r\nvpath->tim_rti_cfg3_saved = val64;\r\n}\r\nval64 = 0;\r\nwriteq(val64, &vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_EINTA]);\r\nwriteq(val64, &vp_reg->tim_cfg2_int_num[VXGE_HW_VPATH_INTR_EINTA]);\r\nwriteq(val64, &vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_EINTA]);\r\nwriteq(val64, &vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_BMAP]);\r\nwriteq(val64, &vp_reg->tim_cfg2_int_num[VXGE_HW_VPATH_INTR_BMAP]);\r\nwriteq(val64, &vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_BMAP]);\r\nval64 = VXGE_HW_TIM_WRKLD_CLC_WRKLD_EVAL_PRD(150);\r\nval64 |= VXGE_HW_TIM_WRKLD_CLC_WRKLD_EVAL_DIV(0);\r\nval64 |= VXGE_HW_TIM_WRKLD_CLC_CNT_RX_TX(3);\r\nwriteq(val64, &vp_reg->tim_wrkld_clc);\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_initialize(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nu64 val64;\r\nu32 val32;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nif (!(hldev->vpath_assignments & vxge_mBIT(vp_id))) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_AVAILABLE;\r\ngoto exit;\r\n}\r\nvp_reg = vpath->vp_reg;\r\nstatus = __vxge_hw_vpath_swapper_set(vpath->vp_reg);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_mac_configure(hldev, vp_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_kdfc_configure(hldev, vp_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_tim_configure(hldev, vp_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nval64 = readq(&vp_reg->rtdma_rd_optimization_ctrl);\r\nstatus = __vxge_hw_vpath_pci_read(vpath, 1, 0x78, &val32);\r\nif (status == VXGE_HW_OK) {\r\nval32 = (val32 & VXGE_HW_PCI_EXP_DEVCTL_READRQ) >> 12;\r\nval64 &=\r\n~(VXGE_HW_RTDMA_RD_OPTIMIZATION_CTRL_FB_FILL_THRESH(7));\r\nval64 |=\r\nVXGE_HW_RTDMA_RD_OPTIMIZATION_CTRL_FB_FILL_THRESH(val32);\r\nval64 |= VXGE_HW_RTDMA_RD_OPTIMIZATION_CTRL_FB_WAIT_FOR_SPACE;\r\n}\r\nval64 &= ~(VXGE_HW_RTDMA_RD_OPTIMIZATION_CTRL_FB_ADDR_BDRY(7));\r\nval64 |=\r\nVXGE_HW_RTDMA_RD_OPTIMIZATION_CTRL_FB_ADDR_BDRY(\r\nVXGE_HW_MAX_PAYLOAD_SIZE_512);\r\nval64 |= VXGE_HW_RTDMA_RD_OPTIMIZATION_CTRL_FB_ADDR_BDRY_EN;\r\nwriteq(val64, &vp_reg->rtdma_rd_optimization_ctrl);\r\nexit:\r\nreturn status;\r\n}\r\nstatic void __vxge_hw_vp_terminate(struct __vxge_hw_device *hldev, u32 vp_id)\r\n{\r\nstruct __vxge_hw_virtualpath *vpath;\r\nvpath = &hldev->virtual_paths[vp_id];\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN)\r\ngoto exit;\r\nVXGE_HW_DEVICE_TIM_INT_MASK_RESET(vpath->hldev->tim_int_mask0,\r\nvpath->hldev->tim_int_mask1, vpath->vp_id);\r\nhldev->stats.hw_dev_info_stats.vpath_info[vpath->vp_id] = NULL;\r\nspin_lock(&vpath->lock);\r\nvpath->vp_open = VXGE_HW_VP_NOT_OPEN;\r\nspin_unlock(&vpath->lock);\r\nvpath->vpmgmt_reg = NULL;\r\nvpath->nofl_db = NULL;\r\nvpath->max_mtu = 0;\r\nvpath->vsport_number = 0;\r\nvpath->max_kdfc_db = 0;\r\nvpath->max_nofl_db = 0;\r\nvpath->ringh = NULL;\r\nvpath->fifoh = NULL;\r\nmemset(&vpath->vpath_handles, 0, sizeof(struct list_head));\r\nvpath->stats_block = 0;\r\nvpath->hw_stats = NULL;\r\nvpath->hw_stats_sav = NULL;\r\nvpath->sw_stats = NULL;\r\nexit:\r\nreturn;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vp_initialize(struct __vxge_hw_device *hldev, u32 vp_id,\r\nstruct vxge_hw_vp_config *config)\r\n{\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (!(hldev->vpath_assignments & vxge_mBIT(vp_id))) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_AVAILABLE;\r\ngoto exit;\r\n}\r\nvpath = &hldev->virtual_paths[vp_id];\r\nspin_lock_init(&vpath->lock);\r\nvpath->vp_id = vp_id;\r\nvpath->vp_open = VXGE_HW_VP_OPEN;\r\nvpath->hldev = hldev;\r\nvpath->vp_config = config;\r\nvpath->vp_reg = hldev->vpath_reg[vp_id];\r\nvpath->vpmgmt_reg = hldev->vpmgmt_reg[vp_id];\r\n__vxge_hw_vpath_reset(hldev, vp_id);\r\nstatus = __vxge_hw_vpath_reset_check(vpath);\r\nif (status != VXGE_HW_OK) {\r\nmemset(vpath, 0, sizeof(struct __vxge_hw_virtualpath));\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_mgmt_read(hldev, vpath);\r\nif (status != VXGE_HW_OK) {\r\nmemset(vpath, 0, sizeof(struct __vxge_hw_virtualpath));\r\ngoto exit;\r\n}\r\nINIT_LIST_HEAD(&vpath->vpath_handles);\r\nvpath->sw_stats = &hldev->stats.sw_dev_info_stats.vpath_info[vp_id];\r\nVXGE_HW_DEVICE_TIM_INT_MASK_SET(hldev->tim_int_mask0,\r\nhldev->tim_int_mask1, vp_id);\r\nstatus = __vxge_hw_vpath_initialize(hldev, vp_id);\r\nif (status != VXGE_HW_OK)\r\n__vxge_hw_vp_terminate(hldev, vp_id);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_mtu_set(struct __vxge_hw_vpath_handle *vp, u32 new_mtu)\r\n{\r\nu64 val64;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nnew_mtu += VXGE_HW_MAC_HEADER_MAX_SIZE;\r\nif ((new_mtu < VXGE_HW_MIN_MTU) || (new_mtu > vpath->max_mtu))\r\nstatus = VXGE_HW_ERR_INVALID_MTU_SIZE;\r\nval64 = readq(&vpath->vp_reg->rxmac_vcfg0);\r\nval64 &= ~VXGE_HW_RXMAC_VCFG0_RTS_MAX_FRM_LEN(0x3fff);\r\nval64 |= VXGE_HW_RXMAC_VCFG0_RTS_MAX_FRM_LEN(new_mtu);\r\nwriteq(val64, &vpath->vp_reg->rxmac_vcfg0);\r\nvpath->vp_config->mtu = new_mtu - VXGE_HW_MAC_HEADER_MAX_SIZE;\r\nexit:\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_hw_vpath_stats_enable(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nvpath = vp->vpath;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nmemcpy(vpath->hw_stats_sav, vpath->hw_stats,\r\nsizeof(struct vxge_hw_vpath_stats_hw_info));\r\nstatus = __vxge_hw_vpath_stats_get(vpath, vpath->hw_stats);\r\nexit:\r\nreturn status;\r\n}\r\nstatic struct __vxge_hw_blockpool_entry *\r\n__vxge_hw_blockpool_block_allocate(struct __vxge_hw_device *devh, u32 size)\r\n{\r\nstruct __vxge_hw_blockpool_entry *entry = NULL;\r\nstruct __vxge_hw_blockpool *blockpool;\r\nblockpool = &devh->block_pool;\r\nif (size == blockpool->block_size) {\r\nif (!list_empty(&blockpool->free_block_list))\r\nentry = (struct __vxge_hw_blockpool_entry *)\r\nlist_first_entry(&blockpool->free_block_list,\r\nstruct __vxge_hw_blockpool_entry,\r\nitem);\r\nif (entry != NULL) {\r\nlist_del(&entry->item);\r\nblockpool->pool_size--;\r\n}\r\n}\r\nif (entry != NULL)\r\n__vxge_hw_blockpool_blocks_add(blockpool);\r\nreturn entry;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_open(struct __vxge_hw_device *hldev,\r\nstruct vxge_hw_vpath_attr *attr,\r\nstruct __vxge_hw_vpath_handle **vpath_handle)\r\n{\r\nstruct __vxge_hw_virtualpath *vpath;\r\nstruct __vxge_hw_vpath_handle *vp;\r\nenum vxge_hw_status status;\r\nvpath = &hldev->virtual_paths[attr->vp_id];\r\nif (vpath->vp_open == VXGE_HW_VP_OPEN) {\r\nstatus = VXGE_HW_ERR_INVALID_STATE;\r\ngoto vpath_open_exit1;\r\n}\r\nstatus = __vxge_hw_vp_initialize(hldev, attr->vp_id,\r\n&hldev->config.vp_config[attr->vp_id]);\r\nif (status != VXGE_HW_OK)\r\ngoto vpath_open_exit1;\r\nvp = vzalloc(sizeof(struct __vxge_hw_vpath_handle));\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto vpath_open_exit2;\r\n}\r\nvp->vpath = vpath;\r\nif (vpath->vp_config->fifo.enable == VXGE_HW_FIFO_ENABLE) {\r\nstatus = __vxge_hw_fifo_create(vp, &attr->fifo_attr);\r\nif (status != VXGE_HW_OK)\r\ngoto vpath_open_exit6;\r\n}\r\nif (vpath->vp_config->ring.enable == VXGE_HW_RING_ENABLE) {\r\nstatus = __vxge_hw_ring_create(vp, &attr->ring_attr);\r\nif (status != VXGE_HW_OK)\r\ngoto vpath_open_exit7;\r\n__vxge_hw_vpath_prc_configure(hldev, attr->vp_id);\r\n}\r\nvpath->fifoh->tx_intr_num =\r\n(attr->vp_id * VXGE_HW_MAX_INTR_PER_VP) +\r\nVXGE_HW_VPATH_INTR_TX;\r\nvpath->stats_block = __vxge_hw_blockpool_block_allocate(hldev,\r\nVXGE_HW_BLOCK_SIZE);\r\nif (vpath->stats_block == NULL) {\r\nstatus = VXGE_HW_ERR_OUT_OF_MEMORY;\r\ngoto vpath_open_exit8;\r\n}\r\nvpath->hw_stats = vpath->stats_block->memblock;\r\nmemset(vpath->hw_stats, 0,\r\nsizeof(struct vxge_hw_vpath_stats_hw_info));\r\nhldev->stats.hw_dev_info_stats.vpath_info[attr->vp_id] =\r\nvpath->hw_stats;\r\nvpath->hw_stats_sav =\r\n&hldev->stats.hw_dev_info_stats.vpath_info_sav[attr->vp_id];\r\nmemset(vpath->hw_stats_sav, 0,\r\nsizeof(struct vxge_hw_vpath_stats_hw_info));\r\nwriteq(vpath->stats_block->dma_addr, &vpath->vp_reg->stats_cfg);\r\nstatus = vxge_hw_vpath_stats_enable(vp);\r\nif (status != VXGE_HW_OK)\r\ngoto vpath_open_exit8;\r\nlist_add(&vp->item, &vpath->vpath_handles);\r\nhldev->vpaths_deployed |= vxge_mBIT(vpath->vp_id);\r\n*vpath_handle = vp;\r\nattr->fifo_attr.userdata = vpath->fifoh;\r\nattr->ring_attr.userdata = vpath->ringh;\r\nreturn VXGE_HW_OK;\r\nvpath_open_exit8:\r\nif (vpath->ringh != NULL)\r\n__vxge_hw_ring_delete(vp);\r\nvpath_open_exit7:\r\nif (vpath->fifoh != NULL)\r\n__vxge_hw_fifo_delete(vp);\r\nvpath_open_exit6:\r\nvfree(vp);\r\nvpath_open_exit2:\r\n__vxge_hw_vp_terminate(hldev, attr->vp_id);\r\nvpath_open_exit1:\r\nreturn status;\r\n}\r\nvoid vxge_hw_vpath_rx_doorbell_init(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nstruct __vxge_hw_virtualpath *vpath = vp->vpath;\r\nstruct __vxge_hw_ring *ring = vpath->ringh;\r\nstruct vxgedev *vdev = netdev_priv(vpath->hldev->ndev);\r\nu64 new_count, val64, val164;\r\nif (vdev->titan1) {\r\nnew_count = readq(&vpath->vp_reg->rxdmem_size);\r\nnew_count &= 0x1fff;\r\n} else\r\nnew_count = ring->config->ring_blocks * VXGE_HW_BLOCK_SIZE / 8;\r\nval164 = VXGE_HW_RXDMEM_SIZE_PRC_RXDMEM_SIZE(new_count);\r\nwriteq(VXGE_HW_PRC_RXD_DOORBELL_NEW_QW_CNT(val164),\r\n&vpath->vp_reg->prc_rxd_doorbell);\r\nreadl(&vpath->vp_reg->prc_rxd_doorbell);\r\nval164 /= 2;\r\nval64 = readq(&vpath->vp_reg->prc_cfg6);\r\nval64 = VXGE_HW_PRC_CFG6_RXD_SPAT(val64);\r\nval64 &= 0x1ff;\r\nnew_count -= (val64 + 1);\r\nval64 = min(val164, new_count) / 4;\r\nring->rxds_limit = min(ring->rxds_limit, val64);\r\nif (ring->rxds_limit < 4)\r\nring->rxds_limit = 4;\r\n}\r\nstatic void\r\n__vxge_hw_blockpool_block_free(struct __vxge_hw_device *devh,\r\nstruct __vxge_hw_blockpool_entry *entry)\r\n{\r\nstruct __vxge_hw_blockpool *blockpool;\r\nblockpool = &devh->block_pool;\r\nif (entry->length == blockpool->block_size) {\r\nlist_add(&entry->item, &blockpool->free_block_list);\r\nblockpool->pool_size++;\r\n}\r\n__vxge_hw_blockpool_blocks_remove(blockpool);\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_close(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nstruct __vxge_hw_virtualpath *vpath = NULL;\r\nstruct __vxge_hw_device *devh = NULL;\r\nu32 vp_id = vp->vpath->vp_id;\r\nu32 is_empty = TRUE;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nvpath = vp->vpath;\r\ndevh = vpath->hldev;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto vpath_close_exit;\r\n}\r\nlist_del(&vp->item);\r\nif (!list_empty(&vpath->vpath_handles)) {\r\nlist_add(&vp->item, &vpath->vpath_handles);\r\nis_empty = FALSE;\r\n}\r\nif (!is_empty) {\r\nstatus = VXGE_HW_FAIL;\r\ngoto vpath_close_exit;\r\n}\r\ndevh->vpaths_deployed &= ~vxge_mBIT(vp_id);\r\nif (vpath->ringh != NULL)\r\n__vxge_hw_ring_delete(vp);\r\nif (vpath->fifoh != NULL)\r\n__vxge_hw_fifo_delete(vp);\r\nif (vpath->stats_block != NULL)\r\n__vxge_hw_blockpool_block_free(devh, vpath->stats_block);\r\nvfree(vp);\r\n__vxge_hw_vp_terminate(devh, vp_id);\r\nvpath_close_exit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_reset(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nenum vxge_hw_status status;\r\nu32 vp_id;\r\nstruct __vxge_hw_virtualpath *vpath = vp->vpath;\r\nvp_id = vpath->vp_id;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_reset(vpath->hldev, vp_id);\r\nif (status == VXGE_HW_OK)\r\nvpath->sw_stats->soft_reset_cnt++;\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_recover_from_reset(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nstruct __vxge_hw_virtualpath *vpath = NULL;\r\nenum vxge_hw_status status;\r\nstruct __vxge_hw_device *hldev;\r\nu32 vp_id;\r\nvp_id = vp->vpath->vp_id;\r\nvpath = vp->vpath;\r\nhldev = vpath->hldev;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_reset_check(vpath);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_sw_reset(hldev, vp_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nstatus = __vxge_hw_vpath_initialize(hldev, vp_id);\r\nif (status != VXGE_HW_OK)\r\ngoto exit;\r\nif (vpath->ringh != NULL)\r\n__vxge_hw_vpath_prc_configure(hldev, vp_id);\r\nmemset(vpath->hw_stats, 0,\r\nsizeof(struct vxge_hw_vpath_stats_hw_info));\r\nmemset(vpath->hw_stats_sav, 0,\r\nsizeof(struct vxge_hw_vpath_stats_hw_info));\r\nwriteq(vpath->stats_block->dma_addr,\r\n&vpath->vp_reg->stats_cfg);\r\nstatus = vxge_hw_vpath_stats_enable(vp);\r\nexit:\r\nreturn status;\r\n}\r\nvoid\r\nvxge_hw_vpath_enable(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nstruct __vxge_hw_device *hldev;\r\nu64 val64;\r\nhldev = vp->vpath->hldev;\r\nval64 = VXGE_HW_CMN_RSTHDLR_CFG1_CLR_VPATH_RESET(\r\n1 << (16 - vp->vpath->vp_id));\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(val64, 0, 32),\r\n&hldev->common_reg->cmn_rsthdlr_cfg1);\r\n}
