static int perf_trace_event_perm(struct ftrace_event_call *tp_event,\r\nstruct perf_event *p_event)\r\n{\r\nif (ftrace_event_is_function(tp_event) &&\r\nperf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\r\nreturn 0;\r\nif (p_event->attach_state == PERF_ATTACH_TASK) {\r\nif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\r\nreturn 0;\r\n}\r\nif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int perf_trace_event_reg(struct ftrace_event_call *tp_event,\r\nstruct perf_event *p_event)\r\n{\r\nstruct hlist_head __percpu *list;\r\nint ret = -ENOMEM;\r\nint cpu;\r\np_event->tp_event = tp_event;\r\nif (tp_event->perf_refcount++ > 0)\r\nreturn 0;\r\nlist = alloc_percpu(struct hlist_head);\r\nif (!list)\r\ngoto fail;\r\nfor_each_possible_cpu(cpu)\r\nINIT_HLIST_HEAD(per_cpu_ptr(list, cpu));\r\ntp_event->perf_events = list;\r\nif (!total_ref_count) {\r\nchar __percpu *buf;\r\nint i;\r\nfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\r\nbuf = (char __percpu *)alloc_percpu(perf_trace_t);\r\nif (!buf)\r\ngoto fail;\r\nperf_trace_buf[i] = buf;\r\n}\r\n}\r\nret = tp_event->class->reg(tp_event, TRACE_REG_PERF_REGISTER, NULL);\r\nif (ret)\r\ngoto fail;\r\ntotal_ref_count++;\r\nreturn 0;\r\nfail:\r\nif (!total_ref_count) {\r\nint i;\r\nfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\r\nfree_percpu(perf_trace_buf[i]);\r\nperf_trace_buf[i] = NULL;\r\n}\r\n}\r\nif (!--tp_event->perf_refcount) {\r\nfree_percpu(tp_event->perf_events);\r\ntp_event->perf_events = NULL;\r\n}\r\nreturn ret;\r\n}\r\nstatic void perf_trace_event_unreg(struct perf_event *p_event)\r\n{\r\nstruct ftrace_event_call *tp_event = p_event->tp_event;\r\nint i;\r\nif (--tp_event->perf_refcount > 0)\r\ngoto out;\r\ntp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER, NULL);\r\ntracepoint_synchronize_unregister();\r\nfree_percpu(tp_event->perf_events);\r\ntp_event->perf_events = NULL;\r\nif (!--total_ref_count) {\r\nfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\r\nfree_percpu(perf_trace_buf[i]);\r\nperf_trace_buf[i] = NULL;\r\n}\r\n}\r\nout:\r\nmodule_put(tp_event->mod);\r\n}\r\nstatic int perf_trace_event_open(struct perf_event *p_event)\r\n{\r\nstruct ftrace_event_call *tp_event = p_event->tp_event;\r\nreturn tp_event->class->reg(tp_event, TRACE_REG_PERF_OPEN, p_event);\r\n}\r\nstatic void perf_trace_event_close(struct perf_event *p_event)\r\n{\r\nstruct ftrace_event_call *tp_event = p_event->tp_event;\r\ntp_event->class->reg(tp_event, TRACE_REG_PERF_CLOSE, p_event);\r\n}\r\nstatic int perf_trace_event_init(struct ftrace_event_call *tp_event,\r\nstruct perf_event *p_event)\r\n{\r\nint ret;\r\nret = perf_trace_event_perm(tp_event, p_event);\r\nif (ret)\r\nreturn ret;\r\nret = perf_trace_event_reg(tp_event, p_event);\r\nif (ret)\r\nreturn ret;\r\nret = perf_trace_event_open(p_event);\r\nif (ret) {\r\nperf_trace_event_unreg(p_event);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint perf_trace_init(struct perf_event *p_event)\r\n{\r\nstruct ftrace_event_call *tp_event;\r\nint event_id = p_event->attr.config;\r\nint ret = -EINVAL;\r\nmutex_lock(&event_mutex);\r\nlist_for_each_entry(tp_event, &ftrace_events, list) {\r\nif (tp_event->event.type == event_id &&\r\ntp_event->class && tp_event->class->reg &&\r\ntry_module_get(tp_event->mod)) {\r\nret = perf_trace_event_init(tp_event, p_event);\r\nif (ret)\r\nmodule_put(tp_event->mod);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&event_mutex);\r\nreturn ret;\r\n}\r\nvoid perf_trace_destroy(struct perf_event *p_event)\r\n{\r\nmutex_lock(&event_mutex);\r\nperf_trace_event_close(p_event);\r\nperf_trace_event_unreg(p_event);\r\nmutex_unlock(&event_mutex);\r\n}\r\nint perf_trace_add(struct perf_event *p_event, int flags)\r\n{\r\nstruct ftrace_event_call *tp_event = p_event->tp_event;\r\nstruct hlist_head __percpu *pcpu_list;\r\nstruct hlist_head *list;\r\npcpu_list = tp_event->perf_events;\r\nif (WARN_ON_ONCE(!pcpu_list))\r\nreturn -EINVAL;\r\nif (!(flags & PERF_EF_START))\r\np_event->hw.state = PERF_HES_STOPPED;\r\nlist = this_cpu_ptr(pcpu_list);\r\nhlist_add_head_rcu(&p_event->hlist_entry, list);\r\nreturn tp_event->class->reg(tp_event, TRACE_REG_PERF_ADD, p_event);\r\n}\r\nvoid perf_trace_del(struct perf_event *p_event, int flags)\r\n{\r\nstruct ftrace_event_call *tp_event = p_event->tp_event;\r\nhlist_del_rcu(&p_event->hlist_entry);\r\ntp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event);\r\n}\r\n__kprobes void *perf_trace_buf_prepare(int size, unsigned short type,\r\nstruct pt_regs *regs, int *rctxp)\r\n{\r\nstruct trace_entry *entry;\r\nunsigned long flags;\r\nchar *raw_data;\r\nint pc;\r\nBUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));\r\npc = preempt_count();\r\n*rctxp = perf_swevent_get_recursion_context();\r\nif (*rctxp < 0)\r\nreturn NULL;\r\nraw_data = this_cpu_ptr(perf_trace_buf[*rctxp]);\r\nmemset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));\r\nentry = (struct trace_entry *)raw_data;\r\nlocal_save_flags(flags);\r\ntracing_generic_entry_update(entry, flags, pc);\r\nentry->type = type;\r\nreturn raw_data;\r\n}\r\nstatic void\r\nperf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,\r\nstruct ftrace_ops *ops, struct pt_regs *pt_regs)\r\n{\r\nstruct ftrace_entry *entry;\r\nstruct hlist_head *head;\r\nstruct pt_regs regs;\r\nint rctx;\r\n#define ENTRY_SIZE (ALIGN(sizeof(struct ftrace_entry) + sizeof(u32), \\r\nsizeof(u64)) - sizeof(u32))\r\nBUILD_BUG_ON(ENTRY_SIZE > PERF_MAX_TRACE_SIZE);\r\nperf_fetch_caller_regs(&regs);\r\nentry = perf_trace_buf_prepare(ENTRY_SIZE, TRACE_FN, NULL, &rctx);\r\nif (!entry)\r\nreturn;\r\nentry->ip = ip;\r\nentry->parent_ip = parent_ip;\r\nhead = this_cpu_ptr(event_function.perf_events);\r\nperf_trace_buf_submit(entry, ENTRY_SIZE, rctx, 0,\r\n1, &regs, head, NULL);\r\n#undef ENTRY_SIZE\r\n}\r\nstatic int perf_ftrace_function_register(struct perf_event *event)\r\n{\r\nstruct ftrace_ops *ops = &event->ftrace_ops;\r\nops->flags |= FTRACE_OPS_FL_CONTROL;\r\nops->func = perf_ftrace_function_call;\r\nreturn register_ftrace_function(ops);\r\n}\r\nstatic int perf_ftrace_function_unregister(struct perf_event *event)\r\n{\r\nstruct ftrace_ops *ops = &event->ftrace_ops;\r\nint ret = unregister_ftrace_function(ops);\r\nftrace_free_filter(ops);\r\nreturn ret;\r\n}\r\nstatic void perf_ftrace_function_enable(struct perf_event *event)\r\n{\r\nftrace_function_local_enable(&event->ftrace_ops);\r\n}\r\nstatic void perf_ftrace_function_disable(struct perf_event *event)\r\n{\r\nftrace_function_local_disable(&event->ftrace_ops);\r\n}\r\nint perf_ftrace_event_register(struct ftrace_event_call *call,\r\nenum trace_reg type, void *data)\r\n{\r\nswitch (type) {\r\ncase TRACE_REG_REGISTER:\r\ncase TRACE_REG_UNREGISTER:\r\nbreak;\r\ncase TRACE_REG_PERF_REGISTER:\r\ncase TRACE_REG_PERF_UNREGISTER:\r\nreturn 0;\r\ncase TRACE_REG_PERF_OPEN:\r\nreturn perf_ftrace_function_register(data);\r\ncase TRACE_REG_PERF_CLOSE:\r\nreturn perf_ftrace_function_unregister(data);\r\ncase TRACE_REG_PERF_ADD:\r\nperf_ftrace_function_enable(data);\r\nreturn 0;\r\ncase TRACE_REG_PERF_DEL:\r\nperf_ftrace_function_disable(data);\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}
