static void\r\nnouveau_sgdma_destroy(struct ttm_tt *ttm)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nif (ttm) {\r\nttm_dma_tt_fini(&nvbe->ttm);\r\nkfree(nvbe);\r\n}\r\n}\r\nstatic int\r\nnv04_sgdma_bind(struct ttm_tt *ttm, struct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct nouveau_mem *node = mem->mm_node;\r\nu64 size = mem->num_pages << 12;\r\nif (ttm->sg) {\r\nnode->sg = ttm->sg;\r\nnouveau_vm_map_sg_table(&node->vma[0], 0, size, node);\r\n} else {\r\nnode->pages = nvbe->ttm.dma_address;\r\nnouveau_vm_map_sg(&node->vma[0], 0, size, node);\r\n}\r\nnvbe->node = node;\r\nreturn 0;\r\n}\r\nstatic int\r\nnv04_sgdma_unbind(struct ttm_tt *ttm)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nnouveau_vm_unmap(&nvbe->node->vma[0]);\r\nreturn 0;\r\n}\r\nstatic int\r\nnv50_sgdma_bind(struct ttm_tt *ttm, struct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct nouveau_mem *node = mem->mm_node;\r\nif (ttm->sg) {\r\nnode->sg = ttm->sg;\r\n} else\r\nnode->pages = nvbe->ttm.dma_address;\r\nreturn 0;\r\n}\r\nstatic int\r\nnv50_sgdma_unbind(struct ttm_tt *ttm)\r\n{\r\nreturn 0;\r\n}\r\nstruct ttm_tt *\r\nnouveau_sgdma_create_ttm(struct ttm_bo_device *bdev,\r\nunsigned long size, uint32_t page_flags,\r\nstruct page *dummy_read_page)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nstruct nouveau_sgdma_be *nvbe;\r\nnvbe = kzalloc(sizeof(*nvbe), GFP_KERNEL);\r\nif (!nvbe)\r\nreturn NULL;\r\nnvbe->dev = drm->dev;\r\nif (nv_device(drm->device)->card_type < NV_50)\r\nnvbe->ttm.ttm.func = &nv04_sgdma_backend;\r\nelse\r\nnvbe->ttm.ttm.func = &nv50_sgdma_backend;\r\nif (ttm_dma_tt_init(&nvbe->ttm, bdev, size, page_flags, dummy_read_page)) {\r\nkfree(nvbe);\r\nreturn NULL;\r\n}\r\nreturn &nvbe->ttm.ttm;\r\n}
