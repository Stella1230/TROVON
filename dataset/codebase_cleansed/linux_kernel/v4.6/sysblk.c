static inline int scan_ppa_idx(int row, int blkid)\r\n{\r\nreturn (row * MAX_BLKS_PR_SYSBLK) + blkid;\r\n}\r\nvoid nvm_sysblk_to_cpu(struct nvm_sb_info *info, struct nvm_system_block *sb)\r\n{\r\ninfo->seqnr = be32_to_cpu(sb->seqnr);\r\ninfo->erase_cnt = be32_to_cpu(sb->erase_cnt);\r\ninfo->version = be16_to_cpu(sb->version);\r\nstrncpy(info->mmtype, sb->mmtype, NVM_MMTYPE_LEN);\r\ninfo->fs_ppa.ppa = be64_to_cpu(sb->fs_ppa);\r\n}\r\nvoid nvm_cpu_to_sysblk(struct nvm_system_block *sb, struct nvm_sb_info *info)\r\n{\r\nsb->magic = cpu_to_be32(NVM_SYSBLK_MAGIC);\r\nsb->seqnr = cpu_to_be32(info->seqnr);\r\nsb->erase_cnt = cpu_to_be32(info->erase_cnt);\r\nsb->version = cpu_to_be16(info->version);\r\nstrncpy(sb->mmtype, info->mmtype, NVM_MMTYPE_LEN);\r\nsb->fs_ppa = cpu_to_be64(info->fs_ppa.ppa);\r\n}\r\nstatic int nvm_setup_sysblks(struct nvm_dev *dev, struct ppa_addr *sysblk_ppas)\r\n{\r\nint nr_rows = min_t(int, MAX_SYSBLKS, dev->nr_chnls);\r\nint i;\r\nfor (i = 0; i < nr_rows; i++)\r\nsysblk_ppas[i].ppa = 0;\r\nswitch (dev->nr_chnls) {\r\ncase 2:\r\nsysblk_ppas[1].g.ch = 1;\r\ncase 1:\r\nsysblk_ppas[0].g.ch = 0;\r\nbreak;\r\ndefault:\r\nsysblk_ppas[0].g.ch = 0;\r\nsysblk_ppas[1].g.ch = dev->nr_chnls / 2;\r\nsysblk_ppas[2].g.ch = dev->nr_chnls - 1;\r\nbreak;\r\n}\r\nreturn nr_rows;\r\n}\r\nvoid nvm_setup_sysblk_scan(struct nvm_dev *dev, struct sysblk_scan *s,\r\nstruct ppa_addr *sysblk_ppas)\r\n{\r\nmemset(s, 0, sizeof(struct sysblk_scan));\r\ns->nr_rows = nvm_setup_sysblks(dev, sysblk_ppas);\r\n}\r\nstatic int sysblk_get_host_blks(struct ppa_addr ppa, int nr_blks, u8 *blks,\r\nvoid *private)\r\n{\r\nstruct sysblk_scan *s = private;\r\nint i, nr_sysblk = 0;\r\nfor (i = 0; i < nr_blks; i++) {\r\nif (blks[i] != NVM_BLK_T_HOST)\r\ncontinue;\r\nif (s->nr_ppas == MAX_BLKS_PR_SYSBLK * MAX_SYSBLKS) {\r\npr_err("nvm: too many host blks\n");\r\nreturn -EINVAL;\r\n}\r\nppa.g.blk = i;\r\ns->ppas[scan_ppa_idx(s->row, nr_sysblk)] = ppa;\r\ns->nr_ppas++;\r\nnr_sysblk++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int nvm_get_all_sysblks(struct nvm_dev *dev, struct sysblk_scan *s,\r\nstruct ppa_addr *ppas, nvm_bb_update_fn *fn)\r\n{\r\nstruct ppa_addr dppa;\r\nint i, ret;\r\ns->nr_ppas = 0;\r\nfor (i = 0; i < s->nr_rows; i++) {\r\ndppa = generic_to_dev_addr(dev, ppas[i]);\r\ns->row = i;\r\nret = dev->ops->get_bb_tbl(dev, dppa, dev->blks_per_lun, fn, s);\r\nif (ret) {\r\npr_err("nvm: failed bb tbl for ppa (%u %u)\n",\r\nppas[i].g.ch,\r\nppas[i].g.blk);\r\nreturn ret;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int nvm_scan_block(struct nvm_dev *dev, struct ppa_addr *ppa,\r\nstruct nvm_system_block *sblk)\r\n{\r\nstruct nvm_system_block *cur;\r\nint pg, cursz, ret, found = 0;\r\ncursz = dev->sec_size * dev->sec_per_pg * dev->nr_planes;\r\ncur = kmalloc(cursz, GFP_KERNEL);\r\nif (!cur)\r\nreturn -ENOMEM;\r\nfor (pg = 0; pg < dev->lps_per_blk; pg++) {\r\nppa->g.pg = ppa_to_slc(dev, pg);\r\nret = nvm_submit_ppa(dev, ppa, 1, NVM_OP_PREAD, NVM_IO_SLC_MODE,\r\ncur, cursz);\r\nif (ret) {\r\nif (ret == NVM_RSP_ERR_EMPTYPAGE) {\r\npr_debug("nvm: sysblk scan empty ppa (%u %u %u %u)\n",\r\nppa->g.ch,\r\nppa->g.lun,\r\nppa->g.blk,\r\nppa->g.pg);\r\nbreak;\r\n}\r\npr_err("nvm: read failed (%x) for ppa (%u %u %u %u)",\r\nret,\r\nppa->g.ch,\r\nppa->g.lun,\r\nppa->g.blk,\r\nppa->g.pg);\r\nbreak;\r\n}\r\nif (be32_to_cpu(cur->magic) != NVM_SYSBLK_MAGIC) {\r\npr_debug("nvm: scan break for ppa (%u %u %u %u)\n",\r\nppa->g.ch,\r\nppa->g.lun,\r\nppa->g.blk,\r\nppa->g.pg);\r\nbreak;\r\n}\r\nif (be32_to_cpu(cur->seqnr) < be32_to_cpu(sblk->seqnr))\r\ncontinue;\r\nmemcpy(sblk, cur, sizeof(struct nvm_system_block));\r\nfound = 1;\r\n}\r\nkfree(cur);\r\nreturn found;\r\n}\r\nstatic int nvm_set_bb_tbl(struct nvm_dev *dev, struct sysblk_scan *s, int type)\r\n{\r\nstruct nvm_rq rqd;\r\nint ret;\r\nif (s->nr_ppas > dev->ops->max_phys_sect) {\r\npr_err("nvm: unable to update all sysblocks atomically\n");\r\nreturn -EINVAL;\r\n}\r\nmemset(&rqd, 0, sizeof(struct nvm_rq));\r\nnvm_set_rqd_ppalist(dev, &rqd, s->ppas, s->nr_ppas);\r\nnvm_generic_to_addr_mode(dev, &rqd);\r\nret = dev->ops->set_bb_tbl(dev, &rqd, type);\r\nnvm_free_rqd_ppalist(dev, &rqd);\r\nif (ret) {\r\npr_err("nvm: sysblk failed bb mark\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int sysblk_get_free_blks(struct ppa_addr ppa, int nr_blks, u8 *blks,\r\nvoid *private)\r\n{\r\nstruct sysblk_scan *s = private;\r\nstruct ppa_addr *sppa;\r\nint i, blkid = 0;\r\nfor (i = 0; i < nr_blks; i++) {\r\nif (blks[i] == NVM_BLK_T_HOST)\r\nreturn -EEXIST;\r\nif (blks[i] != NVM_BLK_T_FREE)\r\ncontinue;\r\nsppa = &s->ppas[scan_ppa_idx(s->row, blkid)];\r\nsppa->g.ch = ppa.g.ch;\r\nsppa->g.lun = ppa.g.lun;\r\nsppa->g.blk = i;\r\ns->nr_ppas++;\r\nblkid++;\r\npr_debug("nvm: use (%u %u %u) as sysblk\n",\r\nsppa->g.ch, sppa->g.lun, sppa->g.blk);\r\nif (blkid > MAX_BLKS_PR_SYSBLK - 1)\r\nreturn 0;\r\n}\r\npr_err("nvm: sysblk failed get sysblk\n");\r\nreturn -EINVAL;\r\n}\r\nstatic int nvm_write_and_verify(struct nvm_dev *dev, struct nvm_sb_info *info,\r\nstruct sysblk_scan *s)\r\n{\r\nstruct nvm_system_block nvmsb;\r\nvoid *buf;\r\nint i, sect, ret, bufsz;\r\nstruct ppa_addr *ppas;\r\nnvm_cpu_to_sysblk(&nvmsb, info);\r\nbufsz = dev->sec_size * dev->sec_per_pg * dev->nr_planes;\r\nbuf = kzalloc(bufsz, GFP_KERNEL);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nmemcpy(buf, &nvmsb, sizeof(struct nvm_system_block));\r\nppas = kcalloc(dev->sec_per_pg, sizeof(struct ppa_addr), GFP_KERNEL);\r\nif (!ppas) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nfor (i = 0; i < s->nr_rows; i++) {\r\nppas[0] = s->ppas[scan_ppa_idx(i, s->act_blk[i])];\r\npr_debug("nvm: writing sysblk to ppa (%u %u %u %u)\n",\r\nppas[0].g.ch,\r\nppas[0].g.lun,\r\nppas[0].g.blk,\r\nppas[0].g.pg);\r\nif (dev->sec_per_pg > 1) {\r\nfor (sect = 1; sect < dev->sec_per_pg; sect++) {\r\nppas[sect].ppa = ppas[0].ppa;\r\nppas[sect].g.sec = sect;\r\n}\r\n}\r\nret = nvm_submit_ppa(dev, ppas, dev->sec_per_pg, NVM_OP_PWRITE,\r\nNVM_IO_SLC_MODE, buf, bufsz);\r\nif (ret) {\r\npr_err("nvm: sysblk failed program (%u %u %u)\n",\r\nppas[0].g.ch,\r\nppas[0].g.lun,\r\nppas[0].g.blk);\r\nbreak;\r\n}\r\nret = nvm_submit_ppa(dev, ppas, dev->sec_per_pg, NVM_OP_PREAD,\r\nNVM_IO_SLC_MODE, buf, bufsz);\r\nif (ret) {\r\npr_err("nvm: sysblk failed read (%u %u %u)\n",\r\nppas[0].g.ch,\r\nppas[0].g.lun,\r\nppas[0].g.blk);\r\nbreak;\r\n}\r\nif (memcmp(buf, &nvmsb, sizeof(struct nvm_system_block))) {\r\npr_err("nvm: sysblk failed verify (%u %u %u)\n",\r\nppas[0].g.ch,\r\nppas[0].g.lun,\r\nppas[0].g.blk);\r\nret = -EINVAL;\r\nbreak;\r\n}\r\n}\r\nkfree(ppas);\r\nerr:\r\nkfree(buf);\r\nreturn ret;\r\n}\r\nstatic int nvm_prepare_new_sysblks(struct nvm_dev *dev, struct sysblk_scan *s)\r\n{\r\nint i, ret;\r\nunsigned long nxt_blk;\r\nstruct ppa_addr *ppa;\r\nfor (i = 0; i < s->nr_rows; i++) {\r\nnxt_blk = (s->act_blk[i] + 1) % MAX_BLKS_PR_SYSBLK;\r\nppa = &s->ppas[scan_ppa_idx(i, nxt_blk)];\r\nppa->g.pg = ppa_to_slc(dev, 0);\r\nret = nvm_erase_ppa(dev, ppa, 1);\r\nif (ret)\r\nreturn ret;\r\ns->act_blk[i] = nxt_blk;\r\n}\r\nreturn 0;\r\n}\r\nint nvm_get_sysblock(struct nvm_dev *dev, struct nvm_sb_info *info)\r\n{\r\nstruct ppa_addr sysblk_ppas[MAX_SYSBLKS];\r\nstruct sysblk_scan s;\r\nstruct nvm_system_block *cur;\r\nint i, j, found = 0;\r\nint ret = -ENOMEM;\r\nif (!dev->ops->get_bb_tbl)\r\nreturn -EINVAL;\r\nnvm_setup_sysblk_scan(dev, &s, sysblk_ppas);\r\nmutex_lock(&dev->mlock);\r\nret = nvm_get_all_sysblks(dev, &s, sysblk_ppas, sysblk_get_host_blks);\r\nif (ret)\r\ngoto err_sysblk;\r\nif (!s.nr_ppas)\r\ngoto err_sysblk;\r\ncur = kzalloc(sizeof(struct nvm_system_block), GFP_KERNEL);\r\nif (!cur)\r\ngoto err_sysblk;\r\nfor (i = 0; i < s.nr_rows; i++) {\r\nfor (j = 0; j < MAX_BLKS_PR_SYSBLK; j++) {\r\nstruct ppa_addr ppa = s.ppas[scan_ppa_idx(i, j)];\r\nret = nvm_scan_block(dev, &ppa, cur);\r\nif (ret > 0)\r\nfound = 1;\r\nelse if (ret < 0)\r\nbreak;\r\n}\r\n}\r\nnvm_sysblk_to_cpu(info, cur);\r\nkfree(cur);\r\nerr_sysblk:\r\nmutex_unlock(&dev->mlock);\r\nif (found)\r\nreturn 1;\r\nreturn ret;\r\n}\r\nint nvm_update_sysblock(struct nvm_dev *dev, struct nvm_sb_info *new)\r\n{\r\nstruct ppa_addr sysblk_ppas[MAX_SYSBLKS];\r\nstruct sysblk_scan s;\r\nstruct nvm_system_block *cur;\r\nint i, j, ppaidx, found = 0;\r\nint ret = -ENOMEM;\r\nif (!dev->ops->get_bb_tbl)\r\nreturn -EINVAL;\r\nnvm_setup_sysblk_scan(dev, &s, sysblk_ppas);\r\nmutex_lock(&dev->mlock);\r\nret = nvm_get_all_sysblks(dev, &s, sysblk_ppas, sysblk_get_host_blks);\r\nif (ret)\r\ngoto err_sysblk;\r\ncur = kzalloc(sizeof(struct nvm_system_block), GFP_KERNEL);\r\nif (!cur)\r\ngoto err_sysblk;\r\nfor (i = 0; i < s.nr_rows; i++) {\r\nfound = 0;\r\nfor (j = 0; j < MAX_BLKS_PR_SYSBLK; j++) {\r\nppaidx = scan_ppa_idx(i, j);\r\nret = nvm_scan_block(dev, &s.ppas[ppaidx], cur);\r\nif (ret > 0) {\r\ns.act_blk[i] = j;\r\nfound = 1;\r\n} else if (ret < 0)\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\npr_err("nvm: no valid sysblks found to update\n");\r\nret = -EINVAL;\r\ngoto err_cur;\r\n}\r\nfor (i = 1; i < s.nr_rows; i++) {\r\nstruct ppa_addr l = s.ppas[scan_ppa_idx(0, s.act_blk[0])];\r\nstruct ppa_addr r = s.ppas[scan_ppa_idx(i, s.act_blk[i])];\r\nif (l.g.pg != r.g.pg) {\r\npr_err("nvm: sysblks not on same page. Previous update failed.\n");\r\nret = -EINVAL;\r\ngoto err_cur;\r\n}\r\n}\r\nif ((new->seqnr - 1) != be32_to_cpu(cur->seqnr)) {\r\npr_err("nvm: seq is not sequential\n");\r\nret = -EINVAL;\r\ngoto err_cur;\r\n}\r\nif (s.ppas[scan_ppa_idx(0, s.act_blk[0])].g.pg ==\r\ndev->lps_per_blk - 1) {\r\nret = nvm_prepare_new_sysblks(dev, &s);\r\nif (ret)\r\ngoto err_cur;\r\n}\r\nret = nvm_write_and_verify(dev, new, &s);\r\nerr_cur:\r\nkfree(cur);\r\nerr_sysblk:\r\nmutex_unlock(&dev->mlock);\r\nreturn ret;\r\n}\r\nint nvm_init_sysblock(struct nvm_dev *dev, struct nvm_sb_info *info)\r\n{\r\nstruct ppa_addr sysblk_ppas[MAX_SYSBLKS];\r\nstruct sysblk_scan s;\r\nint ret;\r\nif (!dev->ops->get_bb_tbl || !dev->ops->set_bb_tbl)\r\nreturn -EINVAL;\r\nif (!(dev->mccap & NVM_ID_CAP_SLC) || !dev->lps_per_blk) {\r\npr_err("nvm: memory does not support SLC access\n");\r\nreturn -EINVAL;\r\n}\r\nnvm_setup_sysblk_scan(dev, &s, sysblk_ppas);\r\nmutex_lock(&dev->mlock);\r\nret = nvm_get_all_sysblks(dev, &s, sysblk_ppas, sysblk_get_free_blks);\r\nif (ret)\r\ngoto err_mark;\r\nret = nvm_set_bb_tbl(dev, &s, NVM_BLK_T_HOST);\r\nif (ret)\r\ngoto err_mark;\r\nret = nvm_write_and_verify(dev, info, &s);\r\nerr_mark:\r\nmutex_unlock(&dev->mlock);\r\nreturn ret;\r\n}\r\nstatic int factory_nblks(int nblks)\r\n{\r\nreturn (nblks + (BITS_PER_LONG - 1)) & ~(BITS_PER_LONG - 1);\r\n}\r\nstatic unsigned int factory_blk_offset(struct nvm_dev *dev, int ch, int lun)\r\n{\r\nint nblks = factory_nblks(dev->blks_per_lun);\r\nreturn ((ch * dev->luns_per_chnl * nblks) + (lun * nblks)) /\r\nBITS_PER_LONG;\r\n}\r\nstatic int nvm_factory_blks(struct ppa_addr ppa, int nr_blks, u8 *blks,\r\nvoid *private)\r\n{\r\nstruct factory_blks *f = private;\r\nstruct nvm_dev *dev = f->dev;\r\nint i, lunoff;\r\nlunoff = factory_blk_offset(dev, ppa.g.ch, ppa.g.lun);\r\nfor (i = 0; i < nr_blks; i++) {\r\nswitch (blks[i]) {\r\ncase NVM_BLK_T_FREE:\r\nif (f->flags & NVM_FACTORY_ERASE_ONLY_USER)\r\nset_bit(i, &f->blks[lunoff]);\r\nbreak;\r\ncase NVM_BLK_T_HOST:\r\nif (!(f->flags & NVM_FACTORY_RESET_HOST_BLKS))\r\nset_bit(i, &f->blks[lunoff]);\r\nbreak;\r\ncase NVM_BLK_T_GRWN_BAD:\r\nif (!(f->flags & NVM_FACTORY_RESET_GRWN_BBLKS))\r\nset_bit(i, &f->blks[lunoff]);\r\nbreak;\r\ndefault:\r\nset_bit(i, &f->blks[lunoff]);\r\nbreak;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int nvm_fact_get_blks(struct nvm_dev *dev, struct ppa_addr *erase_list,\r\nint max_ppas, struct factory_blks *f)\r\n{\r\nstruct ppa_addr ppa;\r\nint ch, lun, blkid, idx, done = 0, ppa_cnt = 0;\r\nunsigned long *offset;\r\nwhile (!done) {\r\ndone = 1;\r\nfor (ch = 0; ch < dev->nr_chnls; ch++) {\r\nfor (lun = 0; lun < dev->luns_per_chnl; lun++) {\r\nidx = factory_blk_offset(dev, ch, lun);\r\noffset = &f->blks[idx];\r\nblkid = find_first_zero_bit(offset,\r\ndev->blks_per_lun);\r\nif (blkid >= dev->blks_per_lun)\r\ncontinue;\r\nset_bit(blkid, offset);\r\nppa.ppa = 0;\r\nppa.g.ch = ch;\r\nppa.g.lun = lun;\r\nppa.g.blk = blkid;\r\npr_debug("nvm: erase ppa (%u %u %u)\n",\r\nppa.g.ch,\r\nppa.g.lun,\r\nppa.g.blk);\r\nerase_list[ppa_cnt] = ppa;\r\nppa_cnt++;\r\ndone = 0;\r\nif (ppa_cnt == max_ppas)\r\nreturn ppa_cnt;\r\n}\r\n}\r\n}\r\nreturn ppa_cnt;\r\n}\r\nstatic int nvm_fact_get_bb_tbl(struct nvm_dev *dev, struct ppa_addr ppa,\r\nnvm_bb_update_fn *fn, void *priv)\r\n{\r\nstruct ppa_addr dev_ppa;\r\nint ret;\r\ndev_ppa = generic_to_dev_addr(dev, ppa);\r\nret = dev->ops->get_bb_tbl(dev, dev_ppa, dev->blks_per_lun, fn, priv);\r\nif (ret)\r\npr_err("nvm: failed bb tbl for ch%u lun%u\n",\r\nppa.g.ch, ppa.g.blk);\r\nreturn ret;\r\n}\r\nstatic int nvm_fact_select_blks(struct nvm_dev *dev, struct factory_blks *f)\r\n{\r\nint ch, lun, ret;\r\nstruct ppa_addr ppa;\r\nppa.ppa = 0;\r\nfor (ch = 0; ch < dev->nr_chnls; ch++) {\r\nfor (lun = 0; lun < dev->luns_per_chnl; lun++) {\r\nppa.g.ch = ch;\r\nppa.g.lun = lun;\r\nret = nvm_fact_get_bb_tbl(dev, ppa, nvm_factory_blks,\r\nf);\r\nif (ret)\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint nvm_dev_factory(struct nvm_dev *dev, int flags)\r\n{\r\nstruct factory_blks f;\r\nstruct ppa_addr *ppas;\r\nint ppa_cnt, ret = -ENOMEM;\r\nint max_ppas = dev->ops->max_phys_sect / dev->nr_planes;\r\nstruct ppa_addr sysblk_ppas[MAX_SYSBLKS];\r\nstruct sysblk_scan s;\r\nf.blks = kzalloc(factory_nblks(dev->blks_per_lun) * dev->nr_luns,\r\nGFP_KERNEL);\r\nif (!f.blks)\r\nreturn ret;\r\nppas = kcalloc(max_ppas, sizeof(struct ppa_addr), GFP_KERNEL);\r\nif (!ppas)\r\ngoto err_blks;\r\nf.dev = dev;\r\nf.flags = flags;\r\nret = nvm_fact_select_blks(dev, &f);\r\nif (ret)\r\ngoto err_ppas;\r\nwhile ((ppa_cnt = nvm_fact_get_blks(dev, ppas, max_ppas, &f)) > 0)\r\nnvm_erase_ppa(dev, ppas, ppa_cnt);\r\nif (flags & NVM_FACTORY_RESET_HOST_BLKS) {\r\nnvm_setup_sysblk_scan(dev, &s, sysblk_ppas);\r\nmutex_lock(&dev->mlock);\r\nret = nvm_get_all_sysblks(dev, &s, sysblk_ppas,\r\nsysblk_get_host_blks);\r\nif (!ret)\r\nret = nvm_set_bb_tbl(dev, &s, NVM_BLK_T_FREE);\r\nmutex_unlock(&dev->mlock);\r\n}\r\nerr_ppas:\r\nkfree(ppas);\r\nerr_blks:\r\nkfree(f.blks);\r\nreturn ret;\r\n}
