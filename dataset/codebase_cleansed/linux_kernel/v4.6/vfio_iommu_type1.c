static struct vfio_dma *vfio_find_dma(struct vfio_iommu *iommu,\r\ndma_addr_t start, size_t size)\r\n{\r\nstruct rb_node *node = iommu->dma_list.rb_node;\r\nwhile (node) {\r\nstruct vfio_dma *dma = rb_entry(node, struct vfio_dma, node);\r\nif (start + size <= dma->iova)\r\nnode = node->rb_left;\r\nelse if (start >= dma->iova + dma->size)\r\nnode = node->rb_right;\r\nelse\r\nreturn dma;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void vfio_link_dma(struct vfio_iommu *iommu, struct vfio_dma *new)\r\n{\r\nstruct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;\r\nstruct vfio_dma *dma;\r\nwhile (*link) {\r\nparent = *link;\r\ndma = rb_entry(parent, struct vfio_dma, node);\r\nif (new->iova + new->size <= dma->iova)\r\nlink = &(*link)->rb_left;\r\nelse\r\nlink = &(*link)->rb_right;\r\n}\r\nrb_link_node(&new->node, parent, link);\r\nrb_insert_color(&new->node, &iommu->dma_list);\r\n}\r\nstatic void vfio_unlink_dma(struct vfio_iommu *iommu, struct vfio_dma *old)\r\n{\r\nrb_erase(&old->node, &iommu->dma_list);\r\n}\r\nstatic void vfio_lock_acct_bg(struct work_struct *work)\r\n{\r\nstruct vwork *vwork = container_of(work, struct vwork, work);\r\nstruct mm_struct *mm;\r\nmm = vwork->mm;\r\ndown_write(&mm->mmap_sem);\r\nmm->locked_vm += vwork->npage;\r\nup_write(&mm->mmap_sem);\r\nmmput(mm);\r\nkfree(vwork);\r\n}\r\nstatic void vfio_lock_acct(long npage)\r\n{\r\nstruct vwork *vwork;\r\nstruct mm_struct *mm;\r\nif (!current->mm || !npage)\r\nreturn;\r\nif (down_write_trylock(&current->mm->mmap_sem)) {\r\ncurrent->mm->locked_vm += npage;\r\nup_write(&current->mm->mmap_sem);\r\nreturn;\r\n}\r\nvwork = kmalloc(sizeof(struct vwork), GFP_KERNEL);\r\nif (!vwork)\r\nreturn;\r\nmm = get_task_mm(current);\r\nif (!mm) {\r\nkfree(vwork);\r\nreturn;\r\n}\r\nINIT_WORK(&vwork->work, vfio_lock_acct_bg);\r\nvwork->mm = mm;\r\nvwork->npage = npage;\r\nschedule_work(&vwork->work);\r\n}\r\nstatic bool is_invalid_reserved_pfn(unsigned long pfn)\r\n{\r\nif (pfn_valid(pfn)) {\r\nbool reserved;\r\nstruct page *tail = pfn_to_page(pfn);\r\nstruct page *head = compound_head(tail);\r\nreserved = !!(PageReserved(head));\r\nif (head != tail) {\r\nsmp_rmb();\r\nif (PageTail(tail))\r\nreturn reserved;\r\n}\r\nreturn PageReserved(tail);\r\n}\r\nreturn true;\r\n}\r\nstatic int put_pfn(unsigned long pfn, int prot)\r\n{\r\nif (!is_invalid_reserved_pfn(pfn)) {\r\nstruct page *page = pfn_to_page(pfn);\r\nif (prot & IOMMU_WRITE)\r\nSetPageDirty(page);\r\nput_page(page);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int vaddr_get_pfn(unsigned long vaddr, int prot, unsigned long *pfn)\r\n{\r\nstruct page *page[1];\r\nstruct vm_area_struct *vma;\r\nint ret = -EFAULT;\r\nif (get_user_pages_fast(vaddr, 1, !!(prot & IOMMU_WRITE), page) == 1) {\r\n*pfn = page_to_pfn(page[0]);\r\nreturn 0;\r\n}\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma_intersection(current->mm, vaddr, vaddr + 1);\r\nif (vma && vma->vm_flags & VM_PFNMAP) {\r\n*pfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nif (is_invalid_reserved_pfn(*pfn))\r\nret = 0;\r\n}\r\nup_read(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic long vfio_pin_pages(unsigned long vaddr, long npage,\r\nint prot, unsigned long *pfn_base)\r\n{\r\nunsigned long limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nbool lock_cap = capable(CAP_IPC_LOCK);\r\nlong ret, i;\r\nbool rsvd;\r\nif (!current->mm)\r\nreturn -ENODEV;\r\nret = vaddr_get_pfn(vaddr, prot, pfn_base);\r\nif (ret)\r\nreturn ret;\r\nrsvd = is_invalid_reserved_pfn(*pfn_base);\r\nif (!rsvd && !lock_cap && current->mm->locked_vm + 1 > limit) {\r\nput_pfn(*pfn_base, prot);\r\npr_warn("%s: RLIMIT_MEMLOCK (%ld) exceeded\n", __func__,\r\nlimit << PAGE_SHIFT);\r\nreturn -ENOMEM;\r\n}\r\nif (unlikely(disable_hugepages)) {\r\nif (!rsvd)\r\nvfio_lock_acct(1);\r\nreturn 1;\r\n}\r\nfor (i = 1, vaddr += PAGE_SIZE; i < npage; i++, vaddr += PAGE_SIZE) {\r\nunsigned long pfn = 0;\r\nret = vaddr_get_pfn(vaddr, prot, &pfn);\r\nif (ret)\r\nbreak;\r\nif (pfn != *pfn_base + i ||\r\nrsvd != is_invalid_reserved_pfn(pfn)) {\r\nput_pfn(pfn, prot);\r\nbreak;\r\n}\r\nif (!rsvd && !lock_cap &&\r\ncurrent->mm->locked_vm + i + 1 > limit) {\r\nput_pfn(pfn, prot);\r\npr_warn("%s: RLIMIT_MEMLOCK (%ld) exceeded\n",\r\n__func__, limit << PAGE_SHIFT);\r\nbreak;\r\n}\r\n}\r\nif (!rsvd)\r\nvfio_lock_acct(i);\r\nreturn i;\r\n}\r\nstatic long vfio_unpin_pages(unsigned long pfn, long npage,\r\nint prot, bool do_accounting)\r\n{\r\nunsigned long unlocked = 0;\r\nlong i;\r\nfor (i = 0; i < npage; i++)\r\nunlocked += put_pfn(pfn++, prot);\r\nif (do_accounting)\r\nvfio_lock_acct(-unlocked);\r\nreturn unlocked;\r\n}\r\nstatic void vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma)\r\n{\r\ndma_addr_t iova = dma->iova, end = dma->iova + dma->size;\r\nstruct vfio_domain *domain, *d;\r\nlong unlocked = 0;\r\nif (!dma->size)\r\nreturn;\r\ndomain = d = list_first_entry(&iommu->domain_list,\r\nstruct vfio_domain, next);\r\nlist_for_each_entry_continue(d, &iommu->domain_list, next) {\r\niommu_unmap(d->domain, dma->iova, dma->size);\r\ncond_resched();\r\n}\r\nwhile (iova < end) {\r\nsize_t unmapped, len;\r\nphys_addr_t phys, next;\r\nphys = iommu_iova_to_phys(domain->domain, iova);\r\nif (WARN_ON(!phys)) {\r\niova += PAGE_SIZE;\r\ncontinue;\r\n}\r\nfor (len = PAGE_SIZE;\r\n!domain->fgsp && iova + len < end; len += PAGE_SIZE) {\r\nnext = iommu_iova_to_phys(domain->domain, iova + len);\r\nif (next != phys + len)\r\nbreak;\r\n}\r\nunmapped = iommu_unmap(domain->domain, iova, len);\r\nif (WARN_ON(!unmapped))\r\nbreak;\r\nunlocked += vfio_unpin_pages(phys >> PAGE_SHIFT,\r\nunmapped >> PAGE_SHIFT,\r\ndma->prot, false);\r\niova += unmapped;\r\ncond_resched();\r\n}\r\nvfio_lock_acct(-unlocked);\r\n}\r\nstatic void vfio_remove_dma(struct vfio_iommu *iommu, struct vfio_dma *dma)\r\n{\r\nvfio_unmap_unpin(iommu, dma);\r\nvfio_unlink_dma(iommu, dma);\r\nkfree(dma);\r\n}\r\nstatic unsigned long vfio_pgsize_bitmap(struct vfio_iommu *iommu)\r\n{\r\nstruct vfio_domain *domain;\r\nunsigned long bitmap = ULONG_MAX;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(domain, &iommu->domain_list, next)\r\nbitmap &= domain->domain->ops->pgsize_bitmap;\r\nmutex_unlock(&iommu->lock);\r\nif (bitmap & ~PAGE_MASK) {\r\nbitmap &= PAGE_MASK;\r\nbitmap |= PAGE_SIZE;\r\n}\r\nreturn bitmap;\r\n}\r\nstatic int vfio_dma_do_unmap(struct vfio_iommu *iommu,\r\nstruct vfio_iommu_type1_dma_unmap *unmap)\r\n{\r\nuint64_t mask;\r\nstruct vfio_dma *dma;\r\nsize_t unmapped = 0;\r\nint ret = 0;\r\nmask = ((uint64_t)1 << __ffs(vfio_pgsize_bitmap(iommu))) - 1;\r\nif (unmap->iova & mask)\r\nreturn -EINVAL;\r\nif (!unmap->size || unmap->size & mask)\r\nreturn -EINVAL;\r\nWARN_ON(mask & PAGE_MASK);\r\nmutex_lock(&iommu->lock);\r\nif (iommu->v2) {\r\ndma = vfio_find_dma(iommu, unmap->iova, 0);\r\nif (dma && dma->iova != unmap->iova) {\r\nret = -EINVAL;\r\ngoto unlock;\r\n}\r\ndma = vfio_find_dma(iommu, unmap->iova + unmap->size - 1, 0);\r\nif (dma && dma->iova + dma->size != unmap->iova + unmap->size) {\r\nret = -EINVAL;\r\ngoto unlock;\r\n}\r\n}\r\nwhile ((dma = vfio_find_dma(iommu, unmap->iova, unmap->size))) {\r\nif (!iommu->v2 && unmap->iova > dma->iova)\r\nbreak;\r\nunmapped += dma->size;\r\nvfio_remove_dma(iommu, dma);\r\n}\r\nunlock:\r\nmutex_unlock(&iommu->lock);\r\nunmap->size = unmapped;\r\nreturn ret;\r\n}\r\nstatic int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,\r\nunsigned long pfn, long npage, int prot)\r\n{\r\nlong i;\r\nint ret;\r\nfor (i = 0; i < npage; i++, pfn++, iova += PAGE_SIZE) {\r\nret = iommu_map(domain->domain, iova,\r\n(phys_addr_t)pfn << PAGE_SHIFT,\r\nPAGE_SIZE, prot | domain->prot);\r\nif (ret)\r\nbreak;\r\n}\r\nfor (; i < npage && i > 0; i--, iova -= PAGE_SIZE)\r\niommu_unmap(domain->domain, iova, PAGE_SIZE);\r\nreturn ret;\r\n}\r\nstatic int vfio_iommu_map(struct vfio_iommu *iommu, dma_addr_t iova,\r\nunsigned long pfn, long npage, int prot)\r\n{\r\nstruct vfio_domain *d;\r\nint ret;\r\nlist_for_each_entry(d, &iommu->domain_list, next) {\r\nret = iommu_map(d->domain, iova, (phys_addr_t)pfn << PAGE_SHIFT,\r\nnpage << PAGE_SHIFT, prot | d->prot);\r\nif (ret) {\r\nif (ret != -EBUSY ||\r\nmap_try_harder(d, iova, pfn, npage, prot))\r\ngoto unwind;\r\n}\r\ncond_resched();\r\n}\r\nreturn 0;\r\nunwind:\r\nlist_for_each_entry_continue_reverse(d, &iommu->domain_list, next)\r\niommu_unmap(d->domain, iova, npage << PAGE_SHIFT);\r\nreturn ret;\r\n}\r\nstatic int vfio_dma_do_map(struct vfio_iommu *iommu,\r\nstruct vfio_iommu_type1_dma_map *map)\r\n{\r\ndma_addr_t iova = map->iova;\r\nunsigned long vaddr = map->vaddr;\r\nsize_t size = map->size;\r\nlong npage;\r\nint ret = 0, prot = 0;\r\nuint64_t mask;\r\nstruct vfio_dma *dma;\r\nunsigned long pfn;\r\nif (map->size != size || map->vaddr != vaddr || map->iova != iova)\r\nreturn -EINVAL;\r\nmask = ((uint64_t)1 << __ffs(vfio_pgsize_bitmap(iommu))) - 1;\r\nWARN_ON(mask & PAGE_MASK);\r\nif (map->flags & VFIO_DMA_MAP_FLAG_WRITE)\r\nprot |= IOMMU_WRITE;\r\nif (map->flags & VFIO_DMA_MAP_FLAG_READ)\r\nprot |= IOMMU_READ;\r\nif (!prot || !size || (size | iova | vaddr) & mask)\r\nreturn -EINVAL;\r\nif (iova + size - 1 < iova || vaddr + size - 1 < vaddr)\r\nreturn -EINVAL;\r\nmutex_lock(&iommu->lock);\r\nif (vfio_find_dma(iommu, iova, size)) {\r\nmutex_unlock(&iommu->lock);\r\nreturn -EEXIST;\r\n}\r\ndma = kzalloc(sizeof(*dma), GFP_KERNEL);\r\nif (!dma) {\r\nmutex_unlock(&iommu->lock);\r\nreturn -ENOMEM;\r\n}\r\ndma->iova = iova;\r\ndma->vaddr = vaddr;\r\ndma->prot = prot;\r\nvfio_link_dma(iommu, dma);\r\nwhile (size) {\r\nnpage = vfio_pin_pages(vaddr + dma->size,\r\nsize >> PAGE_SHIFT, prot, &pfn);\r\nif (npage <= 0) {\r\nWARN_ON(!npage);\r\nret = (int)npage;\r\nbreak;\r\n}\r\nret = vfio_iommu_map(iommu, iova + dma->size, pfn, npage, prot);\r\nif (ret) {\r\nvfio_unpin_pages(pfn, npage, prot, true);\r\nbreak;\r\n}\r\nsize -= npage << PAGE_SHIFT;\r\ndma->size += npage << PAGE_SHIFT;\r\n}\r\nif (ret)\r\nvfio_remove_dma(iommu, dma);\r\nmutex_unlock(&iommu->lock);\r\nreturn ret;\r\n}\r\nstatic int vfio_bus_type(struct device *dev, void *data)\r\n{\r\nstruct bus_type **bus = data;\r\nif (*bus && *bus != dev->bus)\r\nreturn -EINVAL;\r\n*bus = dev->bus;\r\nreturn 0;\r\n}\r\nstatic int vfio_iommu_replay(struct vfio_iommu *iommu,\r\nstruct vfio_domain *domain)\r\n{\r\nstruct vfio_domain *d;\r\nstruct rb_node *n;\r\nint ret;\r\nd = list_first_entry(&iommu->domain_list, struct vfio_domain, next);\r\nn = rb_first(&iommu->dma_list);\r\nif (WARN_ON(n && !d))\r\nreturn -EINVAL;\r\nfor (; n; n = rb_next(n)) {\r\nstruct vfio_dma *dma;\r\ndma_addr_t iova;\r\ndma = rb_entry(n, struct vfio_dma, node);\r\niova = dma->iova;\r\nwhile (iova < dma->iova + dma->size) {\r\nphys_addr_t phys = iommu_iova_to_phys(d->domain, iova);\r\nsize_t size;\r\nif (WARN_ON(!phys)) {\r\niova += PAGE_SIZE;\r\ncontinue;\r\n}\r\nsize = PAGE_SIZE;\r\nwhile (iova + size < dma->iova + dma->size &&\r\nphys + size == iommu_iova_to_phys(d->domain,\r\niova + size))\r\nsize += PAGE_SIZE;\r\nret = iommu_map(domain->domain, iova, phys,\r\nsize, dma->prot | domain->prot);\r\nif (ret)\r\nreturn ret;\r\niova += size;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void vfio_test_domain_fgsp(struct vfio_domain *domain)\r\n{\r\nstruct page *pages;\r\nint ret, order = get_order(PAGE_SIZE * 2);\r\npages = alloc_pages(GFP_KERNEL | __GFP_ZERO, order);\r\nif (!pages)\r\nreturn;\r\nret = iommu_map(domain->domain, 0, page_to_phys(pages), PAGE_SIZE * 2,\r\nIOMMU_READ | IOMMU_WRITE | domain->prot);\r\nif (!ret) {\r\nsize_t unmapped = iommu_unmap(domain->domain, 0, PAGE_SIZE);\r\nif (unmapped == PAGE_SIZE)\r\niommu_unmap(domain->domain, PAGE_SIZE, PAGE_SIZE);\r\nelse\r\ndomain->fgsp = true;\r\n}\r\n__free_pages(pages, order);\r\n}\r\nstatic int vfio_iommu_type1_attach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_group *group, *g;\r\nstruct vfio_domain *domain, *d;\r\nstruct bus_type *bus = NULL;\r\nint ret;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(d, &iommu->domain_list, next) {\r\nlist_for_each_entry(g, &d->group_list, next) {\r\nif (g->iommu_group != iommu_group)\r\ncontinue;\r\nmutex_unlock(&iommu->lock);\r\nreturn -EINVAL;\r\n}\r\n}\r\ngroup = kzalloc(sizeof(*group), GFP_KERNEL);\r\ndomain = kzalloc(sizeof(*domain), GFP_KERNEL);\r\nif (!group || !domain) {\r\nret = -ENOMEM;\r\ngoto out_free;\r\n}\r\ngroup->iommu_group = iommu_group;\r\nret = iommu_group_for_each_dev(iommu_group, &bus, vfio_bus_type);\r\nif (ret)\r\ngoto out_free;\r\ndomain->domain = iommu_domain_alloc(bus);\r\nif (!domain->domain) {\r\nret = -EIO;\r\ngoto out_free;\r\n}\r\nif (iommu->nesting) {\r\nint attr = 1;\r\nret = iommu_domain_set_attr(domain->domain, DOMAIN_ATTR_NESTING,\r\n&attr);\r\nif (ret)\r\ngoto out_domain;\r\n}\r\nret = iommu_attach_group(domain->domain, iommu_group);\r\nif (ret)\r\ngoto out_domain;\r\nINIT_LIST_HEAD(&domain->group_list);\r\nlist_add(&group->next, &domain->group_list);\r\nif (!allow_unsafe_interrupts &&\r\n!iommu_capable(bus, IOMMU_CAP_INTR_REMAP)) {\r\npr_warn("%s: No interrupt remapping support. Use the module param \"allow_unsafe_interrupts\" to enable VFIO IOMMU support on this platform\n",\r\n__func__);\r\nret = -EPERM;\r\ngoto out_detach;\r\n}\r\nif (iommu_capable(bus, IOMMU_CAP_CACHE_COHERENCY))\r\ndomain->prot |= IOMMU_CACHE;\r\nlist_for_each_entry(d, &iommu->domain_list, next) {\r\nif (d->domain->ops == domain->domain->ops &&\r\nd->prot == domain->prot) {\r\niommu_detach_group(domain->domain, iommu_group);\r\nif (!iommu_attach_group(d->domain, iommu_group)) {\r\nlist_add(&group->next, &d->group_list);\r\niommu_domain_free(domain->domain);\r\nkfree(domain);\r\nmutex_unlock(&iommu->lock);\r\nreturn 0;\r\n}\r\nret = iommu_attach_group(domain->domain, iommu_group);\r\nif (ret)\r\ngoto out_domain;\r\n}\r\n}\r\nvfio_test_domain_fgsp(domain);\r\nret = vfio_iommu_replay(iommu, domain);\r\nif (ret)\r\ngoto out_detach;\r\nlist_add(&domain->next, &iommu->domain_list);\r\nmutex_unlock(&iommu->lock);\r\nreturn 0;\r\nout_detach:\r\niommu_detach_group(domain->domain, iommu_group);\r\nout_domain:\r\niommu_domain_free(domain->domain);\r\nout_free:\r\nkfree(domain);\r\nkfree(group);\r\nmutex_unlock(&iommu->lock);\r\nreturn ret;\r\n}\r\nstatic void vfio_iommu_unmap_unpin_all(struct vfio_iommu *iommu)\r\n{\r\nstruct rb_node *node;\r\nwhile ((node = rb_first(&iommu->dma_list)))\r\nvfio_remove_dma(iommu, rb_entry(node, struct vfio_dma, node));\r\n}\r\nstatic void vfio_iommu_type1_detach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_domain *domain;\r\nstruct vfio_group *group;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(domain, &iommu->domain_list, next) {\r\nlist_for_each_entry(group, &domain->group_list, next) {\r\nif (group->iommu_group != iommu_group)\r\ncontinue;\r\niommu_detach_group(domain->domain, iommu_group);\r\nlist_del(&group->next);\r\nkfree(group);\r\nif (list_empty(&domain->group_list)) {\r\nif (list_is_singular(&iommu->domain_list))\r\nvfio_iommu_unmap_unpin_all(iommu);\r\niommu_domain_free(domain->domain);\r\nlist_del(&domain->next);\r\nkfree(domain);\r\n}\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nmutex_unlock(&iommu->lock);\r\n}\r\nstatic void *vfio_iommu_type1_open(unsigned long arg)\r\n{\r\nstruct vfio_iommu *iommu;\r\niommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\r\nif (!iommu)\r\nreturn ERR_PTR(-ENOMEM);\r\nswitch (arg) {\r\ncase VFIO_TYPE1_IOMMU:\r\nbreak;\r\ncase VFIO_TYPE1_NESTING_IOMMU:\r\niommu->nesting = true;\r\ncase VFIO_TYPE1v2_IOMMU:\r\niommu->v2 = true;\r\nbreak;\r\ndefault:\r\nkfree(iommu);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nINIT_LIST_HEAD(&iommu->domain_list);\r\niommu->dma_list = RB_ROOT;\r\nmutex_init(&iommu->lock);\r\nreturn iommu;\r\n}\r\nstatic void vfio_iommu_type1_release(void *iommu_data)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_domain *domain, *domain_tmp;\r\nstruct vfio_group *group, *group_tmp;\r\nvfio_iommu_unmap_unpin_all(iommu);\r\nlist_for_each_entry_safe(domain, domain_tmp,\r\n&iommu->domain_list, next) {\r\nlist_for_each_entry_safe(group, group_tmp,\r\n&domain->group_list, next) {\r\niommu_detach_group(domain->domain, group->iommu_group);\r\nlist_del(&group->next);\r\nkfree(group);\r\n}\r\niommu_domain_free(domain->domain);\r\nlist_del(&domain->next);\r\nkfree(domain);\r\n}\r\nkfree(iommu);\r\n}\r\nstatic int vfio_domains_have_iommu_cache(struct vfio_iommu *iommu)\r\n{\r\nstruct vfio_domain *domain;\r\nint ret = 1;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(domain, &iommu->domain_list, next) {\r\nif (!(domain->prot & IOMMU_CACHE)) {\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&iommu->lock);\r\nreturn ret;\r\n}\r\nstatic long vfio_iommu_type1_ioctl(void *iommu_data,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nunsigned long minsz;\r\nif (cmd == VFIO_CHECK_EXTENSION) {\r\nswitch (arg) {\r\ncase VFIO_TYPE1_IOMMU:\r\ncase VFIO_TYPE1v2_IOMMU:\r\ncase VFIO_TYPE1_NESTING_IOMMU:\r\nreturn 1;\r\ncase VFIO_DMA_CC_IOMMU:\r\nif (!iommu)\r\nreturn 0;\r\nreturn vfio_domains_have_iommu_cache(iommu);\r\ndefault:\r\nreturn 0;\r\n}\r\n} else if (cmd == VFIO_IOMMU_GET_INFO) {\r\nstruct vfio_iommu_type1_info info;\r\nminsz = offsetofend(struct vfio_iommu_type1_info, iova_pgsizes);\r\nif (copy_from_user(&info, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (info.argsz < minsz)\r\nreturn -EINVAL;\r\ninfo.flags = VFIO_IOMMU_INFO_PGSIZES;\r\ninfo.iova_pgsizes = vfio_pgsize_bitmap(iommu);\r\nreturn copy_to_user((void __user *)arg, &info, minsz) ?\r\n-EFAULT : 0;\r\n} else if (cmd == VFIO_IOMMU_MAP_DMA) {\r\nstruct vfio_iommu_type1_dma_map map;\r\nuint32_t mask = VFIO_DMA_MAP_FLAG_READ |\r\nVFIO_DMA_MAP_FLAG_WRITE;\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_map, size);\r\nif (copy_from_user(&map, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (map.argsz < minsz || map.flags & ~mask)\r\nreturn -EINVAL;\r\nreturn vfio_dma_do_map(iommu, &map);\r\n} else if (cmd == VFIO_IOMMU_UNMAP_DMA) {\r\nstruct vfio_iommu_type1_dma_unmap unmap;\r\nlong ret;\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_unmap, size);\r\nif (copy_from_user(&unmap, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (unmap.argsz < minsz || unmap.flags)\r\nreturn -EINVAL;\r\nret = vfio_dma_do_unmap(iommu, &unmap);\r\nif (ret)\r\nreturn ret;\r\nreturn copy_to_user((void __user *)arg, &unmap, minsz) ?\r\n-EFAULT : 0;\r\n}\r\nreturn -ENOTTY;\r\n}\r\nstatic int __init vfio_iommu_type1_init(void)\r\n{\r\nreturn vfio_register_iommu_driver(&vfio_iommu_driver_ops_type1);\r\n}\r\nstatic void __exit vfio_iommu_type1_cleanup(void)\r\n{\r\nvfio_unregister_iommu_driver(&vfio_iommu_driver_ops_type1);\r\n}
