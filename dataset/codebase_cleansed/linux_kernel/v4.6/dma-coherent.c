void dma_cache_sync(struct device *dev, void *vaddr, size_t size, int direction)\r\n{\r\nif (PXSEG(vaddr) == P2SEG)\r\nreturn;\r\nswitch (direction) {\r\ncase DMA_FROM_DEVICE:\r\ninvalidate_dcache_region(vaddr, size);\r\nbreak;\r\ncase DMA_TO_DEVICE:\r\nclean_dcache_region(vaddr, size);\r\nbreak;\r\ncase DMA_BIDIRECTIONAL:\r\nflush_dcache_region(vaddr, size);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic struct page *__dma_alloc(struct device *dev, size_t size,\r\ndma_addr_t *handle, gfp_t gfp)\r\n{\r\nstruct page *page, *free, *end;\r\nint order;\r\ngfp &= ~(__GFP_COMP);\r\nsize = PAGE_ALIGN(size);\r\norder = get_order(size);\r\npage = alloc_pages(gfp, order);\r\nif (!page)\r\nreturn NULL;\r\nsplit_page(page, order);\r\ninvalidate_dcache_region(phys_to_virt(page_to_phys(page)), size);\r\n*handle = page_to_bus(page);\r\nfree = page + (size >> PAGE_SHIFT);\r\nend = page + (1 << order);\r\nwhile (free < end) {\r\n__free_page(free);\r\nfree++;\r\n}\r\nreturn page;\r\n}\r\nstatic void __dma_free(struct device *dev, size_t size,\r\nstruct page *page, dma_addr_t handle)\r\n{\r\nstruct page *end = page + (PAGE_ALIGN(size) >> PAGE_SHIFT);\r\nwhile (page < end)\r\n__free_page(page++);\r\n}\r\nstatic void *avr32_dma_alloc(struct device *dev, size_t size,\r\ndma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\r\n{\r\nstruct page *page;\r\ndma_addr_t phys;\r\npage = __dma_alloc(dev, size, handle, gfp);\r\nif (!page)\r\nreturn NULL;\r\nphys = page_to_phys(page);\r\nif (dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs)) {\r\n*handle = phys;\r\nreturn __ioremap(phys, size, _PAGE_BUFFER);\r\n} else {\r\nreturn phys_to_uncached(phys);\r\n}\r\n}\r\nstatic void avr32_dma_free(struct device *dev, size_t size,\r\nvoid *cpu_addr, dma_addr_t handle, struct dma_attrs *attrs)\r\n{\r\nstruct page *page;\r\nif (dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs)) {\r\niounmap(cpu_addr);\r\npage = phys_to_page(handle);\r\n} else {\r\nvoid *addr = phys_to_cached(uncached_to_phys(cpu_addr));\r\npr_debug("avr32_dma_free addr %p (phys %08lx) size %u\n",\r\ncpu_addr, (unsigned long)handle, (unsigned)size);\r\nBUG_ON(!virt_addr_valid(addr));\r\npage = virt_to_page(addr);\r\n}\r\n__dma_free(dev, size, page, handle);\r\n}\r\nstatic dma_addr_t avr32_dma_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction direction, struct dma_attrs *attrs)\r\n{\r\nvoid *cpu_addr = page_address(page) + offset;\r\ndma_cache_sync(dev, cpu_addr, size, direction);\r\nreturn virt_to_bus(cpu_addr);\r\n}\r\nstatic int avr32_dma_map_sg(struct device *dev, struct scatterlist *sglist,\r\nint nents, enum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nint i;\r\nstruct scatterlist *sg;\r\nfor_each_sg(sglist, sg, nents, i) {\r\nchar *virt;\r\nsg->dma_address = page_to_bus(sg_page(sg)) + sg->offset;\r\nvirt = sg_virt(sg);\r\ndma_cache_sync(dev, virt, sg->length, direction);\r\n}\r\nreturn nents;\r\n}\r\nstatic void avr32_dma_sync_single_for_device(struct device *dev,\r\ndma_addr_t dma_handle, size_t size,\r\nenum dma_data_direction direction)\r\n{\r\ndma_cache_sync(dev, bus_to_virt(dma_handle), size, direction);\r\n}\r\nstatic void avr32_dma_sync_sg_for_device(struct device *dev,\r\nstruct scatterlist *sglist, int nents,\r\nenum dma_data_direction direction)\r\n{\r\nint i;\r\nstruct scatterlist *sg;\r\nfor_each_sg(sglist, sg, nents, i)\r\ndma_cache_sync(dev, sg_virt(sg), sg->length, direction);\r\n}
