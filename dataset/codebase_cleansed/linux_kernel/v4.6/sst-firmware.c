static inline void sst_memcpy32(volatile void __iomem *dest, void *src, u32 bytes)\r\n{\r\nu32 tmp = 0;\r\nint i, m, n;\r\nconst u8 *src_byte = src;\r\nm = bytes / 4;\r\nn = bytes % 4;\r\n__iowrite32_copy((void *)dest, src, m);\r\nif (n) {\r\nfor (i = 0; i < n; i++)\r\ntmp |= (u32)*(src_byte + m * 4 + i) << (i * 8);\r\n__iowrite32_copy((void *)(dest + m * 4), &tmp, 1);\r\n}\r\n}\r\nstatic void sst_dma_transfer_complete(void *arg)\r\n{\r\nstruct sst_dsp *sst = (struct sst_dsp *)arg;\r\ndev_dbg(sst->dev, "DMA: callback\n");\r\n}\r\nstatic int sst_dsp_dma_copy(struct sst_dsp *sst, dma_addr_t dest_addr,\r\ndma_addr_t src_addr, size_t size)\r\n{\r\nstruct dma_async_tx_descriptor *desc;\r\nstruct sst_dma *dma = sst->dma;\r\nif (dma->ch == NULL) {\r\ndev_err(sst->dev, "error: no DMA channel\n");\r\nreturn -ENODEV;\r\n}\r\ndev_dbg(sst->dev, "DMA: src: 0x%lx dest 0x%lx size %zu\n",\r\n(unsigned long)src_addr, (unsigned long)dest_addr, size);\r\ndesc = dma->ch->device->device_prep_dma_memcpy(dma->ch, dest_addr,\r\nsrc_addr, size, DMA_CTRL_ACK);\r\nif (!desc){\r\ndev_err(sst->dev, "error: dma prep memcpy failed\n");\r\nreturn -EINVAL;\r\n}\r\ndesc->callback = sst_dma_transfer_complete;\r\ndesc->callback_param = sst;\r\ndesc->tx_submit(desc);\r\ndma_wait_for_async_tx(desc);\r\nreturn 0;\r\n}\r\nint sst_dsp_dma_copyto(struct sst_dsp *sst, dma_addr_t dest_addr,\r\ndma_addr_t src_addr, size_t size)\r\n{\r\nreturn sst_dsp_dma_copy(sst, dest_addr | SST_HSW_MASK_DMA_ADDR_DSP,\r\nsrc_addr, size);\r\n}\r\nint sst_dsp_dma_copyfrom(struct sst_dsp *sst, dma_addr_t dest_addr,\r\ndma_addr_t src_addr, size_t size)\r\n{\r\nreturn sst_dsp_dma_copy(sst, dest_addr,\r\nsrc_addr | SST_HSW_MASK_DMA_ADDR_DSP, size);\r\n}\r\nstatic void block_list_remove(struct sst_dsp *dsp,\r\nstruct list_head *block_list)\r\n{\r\nstruct sst_mem_block *block, *tmp;\r\nint err;\r\nlist_for_each_entry(block, block_list, module_list) {\r\nif (block->ops && block->ops->disable) {\r\nerr = block->ops->disable(block);\r\nif (err < 0)\r\ndev_err(dsp->dev,\r\n"error: cant disable block %d:%d\n",\r\nblock->type, block->index);\r\n}\r\n}\r\nlist_for_each_entry_safe(block, tmp, block_list, module_list) {\r\nlist_del(&block->module_list);\r\nlist_move(&block->list, &dsp->free_block_list);\r\ndev_dbg(dsp->dev, "block freed %d:%d at offset 0x%x\n",\r\nblock->type, block->index, block->offset);\r\n}\r\n}\r\nstatic int block_list_prepare(struct sst_dsp *dsp,\r\nstruct list_head *block_list)\r\n{\r\nstruct sst_mem_block *block;\r\nint ret = 0;\r\nlist_for_each_entry(block, block_list, module_list) {\r\nif (block->ops && block->ops->enable && !block->users) {\r\nret = block->ops->enable(block);\r\nif (ret < 0) {\r\ndev_err(dsp->dev,\r\n"error: cant disable block %d:%d\n",\r\nblock->type, block->index);\r\ngoto err;\r\n}\r\n}\r\n}\r\nreturn ret;\r\nerr:\r\nlist_for_each_entry(block, block_list, module_list) {\r\nif (block->ops && block->ops->disable)\r\nblock->ops->disable(block);\r\n}\r\nreturn ret;\r\n}\r\nstatic struct dw_dma_chip *dw_probe(struct device *dev, struct resource *mem,\r\nint irq)\r\n{\r\nstruct dw_dma_chip *chip;\r\nint err;\r\nchip = devm_kzalloc(dev, sizeof(*chip), GFP_KERNEL);\r\nif (!chip)\r\nreturn ERR_PTR(-ENOMEM);\r\nchip->irq = irq;\r\nchip->regs = devm_ioremap_resource(dev, mem);\r\nif (IS_ERR(chip->regs))\r\nreturn ERR_CAST(chip->regs);\r\nerr = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(31));\r\nif (err)\r\nreturn ERR_PTR(err);\r\nchip->dev = dev;\r\nerr = dw_dma_probe(chip, NULL);\r\nif (err)\r\nreturn ERR_PTR(err);\r\nreturn chip;\r\n}\r\nstatic void dw_remove(struct dw_dma_chip *chip)\r\n{\r\ndw_dma_remove(chip);\r\n}\r\nstatic bool dma_chan_filter(struct dma_chan *chan, void *param)\r\n{\r\nstruct sst_dsp *dsp = (struct sst_dsp *)param;\r\nreturn chan->device->dev == dsp->dma_dev;\r\n}\r\nint sst_dsp_dma_get_channel(struct sst_dsp *dsp, int chan_id)\r\n{\r\nstruct sst_dma *dma = dsp->dma;\r\nstruct dma_slave_config slave;\r\ndma_cap_mask_t mask;\r\nint ret;\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\ndma_cap_set(DMA_MEMCPY, mask);\r\ndma->ch = dma_request_channel(mask, dma_chan_filter, dsp);\r\nif (dma->ch == NULL) {\r\ndev_err(dsp->dev, "error: DMA request channel failed\n");\r\nreturn -EIO;\r\n}\r\nmemset(&slave, 0, sizeof(slave));\r\nslave.direction = DMA_MEM_TO_DEV;\r\nslave.src_addr_width =\r\nslave.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\nslave.src_maxburst = slave.dst_maxburst = SST_DSP_DMA_MAX_BURST;\r\nret = dmaengine_slave_config(dma->ch, &slave);\r\nif (ret) {\r\ndev_err(dsp->dev, "error: unable to set DMA slave config %d\n",\r\nret);\r\ndma_release_channel(dma->ch);\r\ndma->ch = NULL;\r\n}\r\nreturn ret;\r\n}\r\nvoid sst_dsp_dma_put_channel(struct sst_dsp *dsp)\r\n{\r\nstruct sst_dma *dma = dsp->dma;\r\nif (!dma->ch)\r\nreturn;\r\ndma_release_channel(dma->ch);\r\ndma->ch = NULL;\r\n}\r\nint sst_dma_new(struct sst_dsp *sst)\r\n{\r\nstruct sst_pdata *sst_pdata = sst->pdata;\r\nstruct sst_dma *dma;\r\nstruct resource mem;\r\nconst char *dma_dev_name;\r\nint ret = 0;\r\nif (sst->pdata->resindex_dma_base == -1)\r\nreturn 0;\r\nswitch (sst->pdata->dma_engine) {\r\ncase SST_DMA_TYPE_DW:\r\ndma_dev_name = "dw_dmac";\r\nbreak;\r\ndefault:\r\ndev_err(sst->dev, "error: invalid DMA engine %d\n",\r\nsst->pdata->dma_engine);\r\nreturn -EINVAL;\r\n}\r\ndma = devm_kzalloc(sst->dev, sizeof(struct sst_dma), GFP_KERNEL);\r\nif (!dma)\r\nreturn -ENOMEM;\r\ndma->sst = sst;\r\nmemset(&mem, 0, sizeof(mem));\r\nmem.start = sst->addr.lpe_base + sst_pdata->dma_base;\r\nmem.end = sst->addr.lpe_base + sst_pdata->dma_base + sst_pdata->dma_size - 1;\r\nmem.flags = IORESOURCE_MEM;\r\ndma->chip = dw_probe(sst->dma_dev, &mem, sst_pdata->irq);\r\nif (IS_ERR(dma->chip)) {\r\ndev_err(sst->dev, "error: DMA device register failed\n");\r\nret = PTR_ERR(dma->chip);\r\ngoto err_dma_dev;\r\n}\r\nsst->dma = dma;\r\nsst->fw_use_dma = true;\r\nreturn 0;\r\nerr_dma_dev:\r\ndevm_kfree(sst->dev, dma);\r\nreturn ret;\r\n}\r\nvoid sst_dma_free(struct sst_dma *dma)\r\n{\r\nif (dma == NULL)\r\nreturn;\r\nif (dma->ch)\r\ndma_release_channel(dma->ch);\r\nif (dma->chip)\r\ndw_remove(dma->chip);\r\n}\r\nstruct sst_fw *sst_fw_new(struct sst_dsp *dsp,\r\nconst struct firmware *fw, void *private)\r\n{\r\nstruct sst_fw *sst_fw;\r\nint err;\r\nif (!dsp->ops->parse_fw)\r\nreturn NULL;\r\nsst_fw = kzalloc(sizeof(*sst_fw), GFP_KERNEL);\r\nif (sst_fw == NULL)\r\nreturn NULL;\r\nsst_fw->dsp = dsp;\r\nsst_fw->private = private;\r\nsst_fw->size = fw->size;\r\nsst_fw->dma_buf = dma_alloc_coherent(dsp->dma_dev, sst_fw->size,\r\n&sst_fw->dmable_fw_paddr, GFP_DMA | GFP_KERNEL);\r\nif (!sst_fw->dma_buf) {\r\ndev_err(dsp->dev, "error: DMA alloc failed\n");\r\nkfree(sst_fw);\r\nreturn NULL;\r\n}\r\nmemcpy((void *)sst_fw->dma_buf, (void *)fw->data, fw->size);\r\nif (dsp->fw_use_dma) {\r\nerr = sst_dsp_dma_get_channel(dsp, 0);\r\nif (err < 0)\r\ngoto chan_err;\r\n}\r\nerr = dsp->ops->parse_fw(sst_fw);\r\nif (err < 0) {\r\ndev_err(dsp->dev, "error: parse fw failed %d\n", err);\r\ngoto parse_err;\r\n}\r\nif (dsp->fw_use_dma)\r\nsst_dsp_dma_put_channel(dsp);\r\nmutex_lock(&dsp->mutex);\r\nlist_add(&sst_fw->list, &dsp->fw_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn sst_fw;\r\nparse_err:\r\nif (dsp->fw_use_dma)\r\nsst_dsp_dma_put_channel(dsp);\r\nchan_err:\r\ndma_free_coherent(dsp->dma_dev, sst_fw->size,\r\nsst_fw->dma_buf,\r\nsst_fw->dmable_fw_paddr);\r\nsst_fw->dma_buf = NULL;\r\nkfree(sst_fw);\r\nreturn NULL;\r\n}\r\nint sst_fw_reload(struct sst_fw *sst_fw)\r\n{\r\nstruct sst_dsp *dsp = sst_fw->dsp;\r\nint ret;\r\ndev_dbg(dsp->dev, "reloading firmware\n");\r\nret = dsp->ops->parse_fw(sst_fw);\r\nif (ret < 0)\r\ndev_err(dsp->dev, "error: parse fw failed %d\n", ret);\r\nreturn ret;\r\n}\r\nvoid sst_fw_unload(struct sst_fw *sst_fw)\r\n{\r\nstruct sst_dsp *dsp = sst_fw->dsp;\r\nstruct sst_module *module, *mtmp;\r\nstruct sst_module_runtime *runtime, *rtmp;\r\ndev_dbg(dsp->dev, "unloading firmware\n");\r\nmutex_lock(&dsp->mutex);\r\nlist_for_each_entry_safe(module, mtmp, &dsp->module_list, list) {\r\nif (module->sst_fw == sst_fw) {\r\nlist_for_each_entry_safe(runtime, rtmp, &module->runtime_list, list) {\r\nblock_list_remove(dsp, &runtime->block_list);\r\nlist_del(&runtime->list);\r\nkfree(runtime);\r\n}\r\nblock_list_remove(dsp, &module->block_list);\r\nlist_del(&module->list);\r\nkfree(module);\r\n}\r\n}\r\nblock_list_remove(dsp, &dsp->scratch_block_list);\r\nmutex_unlock(&dsp->mutex);\r\n}\r\nvoid sst_fw_free(struct sst_fw *sst_fw)\r\n{\r\nstruct sst_dsp *dsp = sst_fw->dsp;\r\nmutex_lock(&dsp->mutex);\r\nlist_del(&sst_fw->list);\r\nmutex_unlock(&dsp->mutex);\r\nif (sst_fw->dma_buf)\r\ndma_free_coherent(dsp->dma_dev, sst_fw->size, sst_fw->dma_buf,\r\nsst_fw->dmable_fw_paddr);\r\nkfree(sst_fw);\r\n}\r\nvoid sst_fw_free_all(struct sst_dsp *dsp)\r\n{\r\nstruct sst_fw *sst_fw, *t;\r\nmutex_lock(&dsp->mutex);\r\nlist_for_each_entry_safe(sst_fw, t, &dsp->fw_list, list) {\r\nlist_del(&sst_fw->list);\r\ndma_free_coherent(dsp->dev, sst_fw->size, sst_fw->dma_buf,\r\nsst_fw->dmable_fw_paddr);\r\nkfree(sst_fw);\r\n}\r\nmutex_unlock(&dsp->mutex);\r\n}\r\nstruct sst_module *sst_module_new(struct sst_fw *sst_fw,\r\nstruct sst_module_template *template, void *private)\r\n{\r\nstruct sst_dsp *dsp = sst_fw->dsp;\r\nstruct sst_module *sst_module;\r\nsst_module = kzalloc(sizeof(*sst_module), GFP_KERNEL);\r\nif (sst_module == NULL)\r\nreturn NULL;\r\nsst_module->id = template->id;\r\nsst_module->dsp = dsp;\r\nsst_module->sst_fw = sst_fw;\r\nsst_module->scratch_size = template->scratch_size;\r\nsst_module->persistent_size = template->persistent_size;\r\nsst_module->entry = template->entry;\r\nsst_module->state = SST_MODULE_STATE_UNLOADED;\r\nINIT_LIST_HEAD(&sst_module->block_list);\r\nINIT_LIST_HEAD(&sst_module->runtime_list);\r\nmutex_lock(&dsp->mutex);\r\nlist_add(&sst_module->list, &dsp->module_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn sst_module;\r\n}\r\nvoid sst_module_free(struct sst_module *sst_module)\r\n{\r\nstruct sst_dsp *dsp = sst_module->dsp;\r\nmutex_lock(&dsp->mutex);\r\nlist_del(&sst_module->list);\r\nmutex_unlock(&dsp->mutex);\r\nkfree(sst_module);\r\n}\r\nstruct sst_module_runtime *sst_module_runtime_new(struct sst_module *module,\r\nint id, void *private)\r\n{\r\nstruct sst_dsp *dsp = module->dsp;\r\nstruct sst_module_runtime *runtime;\r\nruntime = kzalloc(sizeof(*runtime), GFP_KERNEL);\r\nif (runtime == NULL)\r\nreturn NULL;\r\nruntime->id = id;\r\nruntime->dsp = dsp;\r\nruntime->module = module;\r\nINIT_LIST_HEAD(&runtime->block_list);\r\nmutex_lock(&dsp->mutex);\r\nlist_add(&runtime->list, &module->runtime_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn runtime;\r\n}\r\nvoid sst_module_runtime_free(struct sst_module_runtime *runtime)\r\n{\r\nstruct sst_dsp *dsp = runtime->dsp;\r\nmutex_lock(&dsp->mutex);\r\nlist_del(&runtime->list);\r\nmutex_unlock(&dsp->mutex);\r\nkfree(runtime);\r\n}\r\nstatic struct sst_mem_block *find_block(struct sst_dsp *dsp,\r\nstruct sst_block_allocator *ba)\r\n{\r\nstruct sst_mem_block *block;\r\nlist_for_each_entry(block, &dsp->free_block_list, list) {\r\nif (block->type == ba->type && block->offset == ba->offset)\r\nreturn block;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int block_alloc_contiguous(struct sst_dsp *dsp,\r\nstruct sst_block_allocator *ba, struct list_head *block_list)\r\n{\r\nstruct list_head tmp = LIST_HEAD_INIT(tmp);\r\nstruct sst_mem_block *block;\r\nu32 block_start = SST_HSW_BLOCK_ANY;\r\nint size = ba->size, offset = ba->offset;\r\nwhile (ba->size > 0) {\r\nblock = find_block(dsp, ba);\r\nif (!block) {\r\nlist_splice(&tmp, &dsp->free_block_list);\r\nba->size = size;\r\nba->offset = offset;\r\nreturn -ENOMEM;\r\n}\r\nlist_move_tail(&block->list, &tmp);\r\nba->offset += block->size;\r\nba->size -= block->size;\r\n}\r\nba->size = size;\r\nba->offset = offset;\r\nlist_for_each_entry(block, &tmp, list) {\r\nif (block->offset < block_start)\r\nblock_start = block->offset;\r\nlist_add(&block->module_list, block_list);\r\ndev_dbg(dsp->dev, "block allocated %d:%d at offset 0x%x\n",\r\nblock->type, block->index, block->offset);\r\n}\r\nlist_splice(&tmp, &dsp->used_block_list);\r\nreturn 0;\r\n}\r\nstatic int block_alloc(struct sst_dsp *dsp, struct sst_block_allocator *ba,\r\nstruct list_head *block_list)\r\n{\r\nstruct sst_mem_block *block, *tmp;\r\nint ret = 0;\r\nif (ba->size == 0)\r\nreturn 0;\r\nlist_for_each_entry_safe(block, tmp, &dsp->free_block_list, list) {\r\nif (block->type != ba->type)\r\ncontinue;\r\nif (ba->size > block->size)\r\ncontinue;\r\nba->offset = block->offset;\r\nblock->bytes_used = ba->size % block->size;\r\nlist_add(&block->module_list, block_list);\r\nlist_move(&block->list, &dsp->used_block_list);\r\ndev_dbg(dsp->dev, "block allocated %d:%d at offset 0x%x\n",\r\nblock->type, block->index, block->offset);\r\nreturn 0;\r\n}\r\nlist_for_each_entry_safe(block, tmp, &dsp->free_block_list, list) {\r\nif (block->type != ba->type)\r\ncontinue;\r\nif (ba->size > block->size) {\r\nba->offset = block->offset;\r\nret = block_alloc_contiguous(dsp, ba, block_list);\r\nif (ret == 0)\r\nreturn ret;\r\n}\r\n}\r\nreturn -ENOMEM;\r\n}\r\nint sst_alloc_blocks(struct sst_dsp *dsp, struct sst_block_allocator *ba,\r\nstruct list_head *block_list)\r\n{\r\nint ret;\r\ndev_dbg(dsp->dev, "block request 0x%x bytes at offset 0x%x type %d\n",\r\nba->size, ba->offset, ba->type);\r\nmutex_lock(&dsp->mutex);\r\nret = block_alloc(dsp, ba, block_list);\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: can't alloc blocks %d\n", ret);\r\ngoto out;\r\n}\r\nret = block_list_prepare(dsp, block_list);\r\nif (ret < 0)\r\ndev_err(dsp->dev, "error: prepare failed\n");\r\nout:\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\n}\r\nint sst_free_blocks(struct sst_dsp *dsp, struct list_head *block_list)\r\n{\r\nmutex_lock(&dsp->mutex);\r\nblock_list_remove(dsp, block_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn 0;\r\n}\r\nstatic int block_alloc_fixed(struct sst_dsp *dsp, struct sst_block_allocator *ba,\r\nstruct list_head *block_list)\r\n{\r\nstruct sst_mem_block *block, *tmp;\r\nstruct sst_block_allocator ba_tmp = *ba;\r\nu32 end = ba->offset + ba->size, block_end;\r\nint err;\r\nif (ba->type != SST_MEM_IRAM && ba->type != SST_MEM_DRAM)\r\nreturn 0;\r\nlist_for_each_entry_safe(block, tmp, block_list, module_list) {\r\nif (block->type != ba->type)\r\ncontinue;\r\nblock_end = block->offset + block->size;\r\nif (ba->offset >= block->offset && end <= block_end)\r\nreturn 0;\r\nif (ba->offset >= block->offset && ba->offset < block_end) {\r\nba_tmp.size -= block_end - ba->offset;\r\nba_tmp.offset = block_end;\r\nerr = block_alloc_contiguous(dsp, &ba_tmp, block_list);\r\nif (err < 0)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\n}\r\nlist_for_each_entry_safe(block, tmp, &dsp->free_block_list, list) {\r\nblock_end = block->offset + block->size;\r\nif (block->type != ba->type)\r\ncontinue;\r\nif (ba->offset >= block->offset && end <= block_end) {\r\nlist_move(&block->list, &dsp->used_block_list);\r\nlist_add(&block->module_list, block_list);\r\ndev_dbg(dsp->dev, "block allocated %d:%d at offset 0x%x\n",\r\nblock->type, block->index, block->offset);\r\nreturn 0;\r\n}\r\nif (ba->offset >= block->offset && ba->offset < block_end) {\r\nlist_move(&block->list, &dsp->used_block_list);\r\nlist_add(&block->module_list, block_list);\r\nba_tmp.size -= block_end - ba->offset;\r\nba_tmp.offset = block_end;\r\nerr = block_alloc_contiguous(dsp, &ba_tmp, block_list);\r\nif (err < 0)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENOMEM;\r\n}\r\nint sst_module_alloc_blocks(struct sst_module *module)\r\n{\r\nstruct sst_dsp *dsp = module->dsp;\r\nstruct sst_fw *sst_fw = module->sst_fw;\r\nstruct sst_block_allocator ba;\r\nint ret;\r\nmemset(&ba, 0, sizeof(ba));\r\nba.size = module->size;\r\nba.type = module->type;\r\nba.offset = module->offset;\r\ndev_dbg(dsp->dev, "block request 0x%x bytes at offset 0x%x type %d\n",\r\nba.size, ba.offset, ba.type);\r\nmutex_lock(&dsp->mutex);\r\nret = block_alloc_fixed(dsp, &ba, &module->block_list);\r\nif (ret < 0) {\r\ndev_err(dsp->dev,\r\n"error: no free blocks for section at offset 0x%x size 0x%x\n",\r\nmodule->offset, module->size);\r\nmutex_unlock(&dsp->mutex);\r\nreturn -ENOMEM;\r\n}\r\nret = block_list_prepare(dsp, &module->block_list);\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: fw module prepare failed\n");\r\ngoto err;\r\n}\r\nif (dsp->fw_use_dma) {\r\nret = sst_dsp_dma_copyto(dsp,\r\ndsp->addr.lpe_base + module->offset,\r\nsst_fw->dmable_fw_paddr + module->data_offset,\r\nmodule->size);\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: module copy failed\n");\r\ngoto err;\r\n}\r\n} else\r\nsst_memcpy32(dsp->addr.lpe + module->offset, module->data,\r\nmodule->size);\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\nerr:\r\nblock_list_remove(dsp, &module->block_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\n}\r\nint sst_module_free_blocks(struct sst_module *module)\r\n{\r\nstruct sst_dsp *dsp = module->dsp;\r\nmutex_lock(&dsp->mutex);\r\nblock_list_remove(dsp, &module->block_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn 0;\r\n}\r\nint sst_module_runtime_alloc_blocks(struct sst_module_runtime *runtime,\r\nint offset)\r\n{\r\nstruct sst_dsp *dsp = runtime->dsp;\r\nstruct sst_module *module = runtime->module;\r\nstruct sst_block_allocator ba;\r\nint ret;\r\nif (module->persistent_size == 0)\r\nreturn 0;\r\nmemset(&ba, 0, sizeof(ba));\r\nba.size = module->persistent_size;\r\nba.type = SST_MEM_DRAM;\r\nmutex_lock(&dsp->mutex);\r\nif (offset != 0) {\r\nba.offset = offset;\r\ndev_dbg(dsp->dev, "persistent fixed block request 0x%x bytes type %d offset 0x%x\n",\r\nba.size, ba.type, ba.offset);\r\nret = block_alloc_fixed(dsp, &ba, &runtime->block_list);\r\n} else {\r\ndev_dbg(dsp->dev, "persistent block request 0x%x bytes type %d\n",\r\nba.size, ba.type);\r\nret = block_alloc(dsp, &ba, &runtime->block_list);\r\n}\r\nif (ret < 0) {\r\ndev_err(dsp->dev,\r\n"error: no free blocks for runtime module size 0x%x\n",\r\nmodule->persistent_size);\r\nmutex_unlock(&dsp->mutex);\r\nreturn -ENOMEM;\r\n}\r\nruntime->persistent_offset = ba.offset;\r\nret = block_list_prepare(dsp, &runtime->block_list);\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: runtime block prepare failed\n");\r\ngoto err;\r\n}\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\nerr:\r\nblock_list_remove(dsp, &module->block_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\n}\r\nint sst_module_runtime_free_blocks(struct sst_module_runtime *runtime)\r\n{\r\nstruct sst_dsp *dsp = runtime->dsp;\r\nmutex_lock(&dsp->mutex);\r\nblock_list_remove(dsp, &runtime->block_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn 0;\r\n}\r\nint sst_module_runtime_save(struct sst_module_runtime *runtime,\r\nstruct sst_module_runtime_context *context)\r\n{\r\nstruct sst_dsp *dsp = runtime->dsp;\r\nstruct sst_module *module = runtime->module;\r\nint ret = 0;\r\ndev_dbg(dsp->dev, "saving runtime %d memory at 0x%x size 0x%x\n",\r\nruntime->id, runtime->persistent_offset,\r\nmodule->persistent_size);\r\ncontext->buffer = dma_alloc_coherent(dsp->dma_dev,\r\nmodule->persistent_size,\r\n&context->dma_buffer, GFP_DMA | GFP_KERNEL);\r\nif (!context->buffer) {\r\ndev_err(dsp->dev, "error: DMA context alloc failed\n");\r\nreturn -ENOMEM;\r\n}\r\nmutex_lock(&dsp->mutex);\r\nif (dsp->fw_use_dma) {\r\nret = sst_dsp_dma_get_channel(dsp, 0);\r\nif (ret < 0)\r\ngoto err;\r\nret = sst_dsp_dma_copyfrom(dsp, context->dma_buffer,\r\ndsp->addr.lpe_base + runtime->persistent_offset,\r\nmodule->persistent_size);\r\nsst_dsp_dma_put_channel(dsp);\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: context copy failed\n");\r\ngoto err;\r\n}\r\n} else\r\nsst_memcpy32(context->buffer, dsp->addr.lpe +\r\nruntime->persistent_offset,\r\nmodule->persistent_size);\r\nerr:\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\n}\r\nint sst_module_runtime_restore(struct sst_module_runtime *runtime,\r\nstruct sst_module_runtime_context *context)\r\n{\r\nstruct sst_dsp *dsp = runtime->dsp;\r\nstruct sst_module *module = runtime->module;\r\nint ret = 0;\r\ndev_dbg(dsp->dev, "restoring runtime %d memory at 0x%x size 0x%x\n",\r\nruntime->id, runtime->persistent_offset,\r\nmodule->persistent_size);\r\nmutex_lock(&dsp->mutex);\r\nif (!context->buffer) {\r\ndev_info(dsp->dev, "no context buffer need to restore!\n");\r\ngoto err;\r\n}\r\nif (dsp->fw_use_dma) {\r\nret = sst_dsp_dma_get_channel(dsp, 0);\r\nif (ret < 0)\r\ngoto err;\r\nret = sst_dsp_dma_copyto(dsp,\r\ndsp->addr.lpe_base + runtime->persistent_offset,\r\ncontext->dma_buffer, module->persistent_size);\r\nsst_dsp_dma_put_channel(dsp);\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: module copy failed\n");\r\ngoto err;\r\n}\r\n} else\r\nsst_memcpy32(dsp->addr.lpe + runtime->persistent_offset,\r\ncontext->buffer, module->persistent_size);\r\ndma_free_coherent(dsp->dma_dev, module->persistent_size,\r\ncontext->buffer, context->dma_buffer);\r\ncontext->buffer = NULL;\r\nerr:\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\n}\r\nstruct sst_mem_block *sst_mem_block_register(struct sst_dsp *dsp, u32 offset,\r\nu32 size, enum sst_mem_type type, const struct sst_block_ops *ops,\r\nu32 index, void *private)\r\n{\r\nstruct sst_mem_block *block;\r\nblock = kzalloc(sizeof(*block), GFP_KERNEL);\r\nif (block == NULL)\r\nreturn NULL;\r\nblock->offset = offset;\r\nblock->size = size;\r\nblock->index = index;\r\nblock->type = type;\r\nblock->dsp = dsp;\r\nblock->private = private;\r\nblock->ops = ops;\r\nmutex_lock(&dsp->mutex);\r\nlist_add(&block->list, &dsp->free_block_list);\r\nmutex_unlock(&dsp->mutex);\r\nreturn block;\r\n}\r\nvoid sst_mem_block_unregister_all(struct sst_dsp *dsp)\r\n{\r\nstruct sst_mem_block *block, *tmp;\r\nmutex_lock(&dsp->mutex);\r\nlist_for_each_entry_safe(block, tmp, &dsp->used_block_list, list) {\r\nlist_del(&block->list);\r\nkfree(block);\r\n}\r\nlist_for_each_entry_safe(block, tmp, &dsp->free_block_list, list) {\r\nlist_del(&block->list);\r\nkfree(block);\r\n}\r\nmutex_unlock(&dsp->mutex);\r\n}\r\nint sst_block_alloc_scratch(struct sst_dsp *dsp)\r\n{\r\nstruct sst_module *module;\r\nstruct sst_block_allocator ba;\r\nint ret;\r\nmutex_lock(&dsp->mutex);\r\ndsp->scratch_size = 0;\r\nlist_for_each_entry(module, &dsp->module_list, list) {\r\ndev_dbg(dsp->dev, "module %d scratch req 0x%x bytes\n",\r\nmodule->id, module->scratch_size);\r\nif (dsp->scratch_size < module->scratch_size)\r\ndsp->scratch_size = module->scratch_size;\r\n}\r\ndev_dbg(dsp->dev, "scratch buffer required is 0x%x bytes\n",\r\ndsp->scratch_size);\r\nif (dsp->scratch_size == 0) {\r\ndev_info(dsp->dev, "no modules need scratch buffer\n");\r\nmutex_unlock(&dsp->mutex);\r\nreturn 0;\r\n}\r\ndev_dbg(dsp->dev, "allocating scratch blocks\n");\r\nba.size = dsp->scratch_size;\r\nba.type = SST_MEM_DRAM;\r\nif (dsp->scratch_offset != 0) {\r\ndev_dbg(dsp->dev, "block request 0x%x bytes type %d at 0x%x\n",\r\nba.size, ba.type, ba.offset);\r\nba.offset = dsp->scratch_offset;\r\nret = block_alloc_fixed(dsp, &ba, &dsp->scratch_block_list);\r\n} else {\r\ndev_dbg(dsp->dev, "block request 0x%x bytes type %d\n",\r\nba.size, ba.type);\r\nba.offset = 0;\r\nret = block_alloc(dsp, &ba, &dsp->scratch_block_list);\r\n}\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: can't alloc scratch blocks\n");\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\n}\r\nret = block_list_prepare(dsp, &dsp->scratch_block_list);\r\nif (ret < 0) {\r\ndev_err(dsp->dev, "error: scratch block prepare failed\n");\r\nmutex_unlock(&dsp->mutex);\r\nreturn ret;\r\n}\r\ndsp->scratch_offset = ba.offset;\r\nmutex_unlock(&dsp->mutex);\r\nreturn dsp->scratch_size;\r\n}\r\nvoid sst_block_free_scratch(struct sst_dsp *dsp)\r\n{\r\nmutex_lock(&dsp->mutex);\r\nblock_list_remove(dsp, &dsp->scratch_block_list);\r\nmutex_unlock(&dsp->mutex);\r\n}\r\nstruct sst_module *sst_module_get_from_id(struct sst_dsp *dsp, u32 id)\r\n{\r\nstruct sst_module *module;\r\nmutex_lock(&dsp->mutex);\r\nlist_for_each_entry(module, &dsp->module_list, list) {\r\nif (module->id == id) {\r\nmutex_unlock(&dsp->mutex);\r\nreturn module;\r\n}\r\n}\r\nmutex_unlock(&dsp->mutex);\r\nreturn NULL;\r\n}\r\nstruct sst_module_runtime *sst_module_runtime_get_from_id(\r\nstruct sst_module *module, u32 id)\r\n{\r\nstruct sst_module_runtime *runtime;\r\nstruct sst_dsp *dsp = module->dsp;\r\nmutex_lock(&dsp->mutex);\r\nlist_for_each_entry(runtime, &module->runtime_list, list) {\r\nif (runtime->id == id) {\r\nmutex_unlock(&dsp->mutex);\r\nreturn runtime;\r\n}\r\n}\r\nmutex_unlock(&dsp->mutex);\r\nreturn NULL;\r\n}\r\nu32 sst_dsp_get_offset(struct sst_dsp *dsp, u32 offset,\r\nenum sst_mem_type type)\r\n{\r\nswitch (type) {\r\ncase SST_MEM_IRAM:\r\nreturn offset - dsp->addr.iram_offset +\r\ndsp->addr.dsp_iram_offset;\r\ncase SST_MEM_DRAM:\r\nreturn offset - dsp->addr.dram_offset +\r\ndsp->addr.dsp_dram_offset;\r\ndefault:\r\nreturn 0;\r\n}\r\n}
