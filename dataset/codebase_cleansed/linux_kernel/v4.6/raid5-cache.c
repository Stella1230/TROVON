static sector_t r5l_ring_add(struct r5l_log *log, sector_t start, sector_t inc)\r\n{\r\nstart += inc;\r\nif (start >= log->device_size)\r\nstart = start - log->device_size;\r\nreturn start;\r\n}\r\nstatic sector_t r5l_ring_distance(struct r5l_log *log, sector_t start,\r\nsector_t end)\r\n{\r\nif (end >= start)\r\nreturn end - start;\r\nelse\r\nreturn end + log->device_size - start;\r\n}\r\nstatic bool r5l_has_free_space(struct r5l_log *log, sector_t size)\r\n{\r\nsector_t used_size;\r\nused_size = r5l_ring_distance(log, log->last_checkpoint,\r\nlog->log_start);\r\nreturn log->device_size > used_size + size;\r\n}\r\nstatic void __r5l_set_io_unit_state(struct r5l_io_unit *io,\r\nenum r5l_io_unit_state state)\r\n{\r\nif (WARN_ON(io->state >= state))\r\nreturn;\r\nio->state = state;\r\n}\r\nstatic void r5l_io_run_stripes(struct r5l_io_unit *io)\r\n{\r\nstruct stripe_head *sh, *next;\r\nlist_for_each_entry_safe(sh, next, &io->stripe_list, log_list) {\r\nlist_del_init(&sh->log_list);\r\nset_bit(STRIPE_HANDLE, &sh->state);\r\nraid5_release_stripe(sh);\r\n}\r\n}\r\nstatic void r5l_log_run_stripes(struct r5l_log *log)\r\n{\r\nstruct r5l_io_unit *io, *next;\r\nassert_spin_locked(&log->io_list_lock);\r\nlist_for_each_entry_safe(io, next, &log->running_ios, log_sibling) {\r\nif (io->state < IO_UNIT_IO_END)\r\nbreak;\r\nlist_move_tail(&io->log_sibling, &log->finished_ios);\r\nr5l_io_run_stripes(io);\r\n}\r\n}\r\nstatic void r5l_move_to_end_ios(struct r5l_log *log)\r\n{\r\nstruct r5l_io_unit *io, *next;\r\nassert_spin_locked(&log->io_list_lock);\r\nlist_for_each_entry_safe(io, next, &log->running_ios, log_sibling) {\r\nif (io->state < IO_UNIT_IO_END)\r\nbreak;\r\nlist_move_tail(&io->log_sibling, &log->io_end_ios);\r\n}\r\n}\r\nstatic void r5l_log_endio(struct bio *bio)\r\n{\r\nstruct r5l_io_unit *io = bio->bi_private;\r\nstruct r5l_log *log = io->log;\r\nunsigned long flags;\r\nif (bio->bi_error)\r\nmd_error(log->rdev->mddev, log->rdev);\r\nbio_put(bio);\r\nmempool_free(io->meta_page, log->meta_pool);\r\nspin_lock_irqsave(&log->io_list_lock, flags);\r\n__r5l_set_io_unit_state(io, IO_UNIT_IO_END);\r\nif (log->need_cache_flush)\r\nr5l_move_to_end_ios(log);\r\nelse\r\nr5l_log_run_stripes(log);\r\nspin_unlock_irqrestore(&log->io_list_lock, flags);\r\nif (log->need_cache_flush)\r\nmd_wakeup_thread(log->rdev->mddev->thread);\r\n}\r\nstatic void r5l_submit_current_io(struct r5l_log *log)\r\n{\r\nstruct r5l_io_unit *io = log->current_io;\r\nstruct r5l_meta_block *block;\r\nunsigned long flags;\r\nu32 crc;\r\nif (!io)\r\nreturn;\r\nblock = page_address(io->meta_page);\r\nblock->meta_size = cpu_to_le32(io->meta_offset);\r\ncrc = crc32c_le(log->uuid_checksum, block, PAGE_SIZE);\r\nblock->checksum = cpu_to_le32(crc);\r\nlog->current_io = NULL;\r\nspin_lock_irqsave(&log->io_list_lock, flags);\r\n__r5l_set_io_unit_state(io, IO_UNIT_IO_START);\r\nspin_unlock_irqrestore(&log->io_list_lock, flags);\r\nsubmit_bio(WRITE, io->current_bio);\r\n}\r\nstatic struct bio *r5l_bio_alloc(struct r5l_log *log)\r\n{\r\nstruct bio *bio = bio_alloc_bioset(GFP_NOIO, BIO_MAX_PAGES, log->bs);\r\nbio->bi_rw = WRITE;\r\nbio->bi_bdev = log->rdev->bdev;\r\nbio->bi_iter.bi_sector = log->rdev->data_offset + log->log_start;\r\nreturn bio;\r\n}\r\nstatic void r5_reserve_log_entry(struct r5l_log *log, struct r5l_io_unit *io)\r\n{\r\nlog->log_start = r5l_ring_add(log, log->log_start, BLOCK_SECTORS);\r\nif (log->log_start == 0)\r\nio->need_split_bio = true;\r\nio->log_end = log->log_start;\r\n}\r\nstatic struct r5l_io_unit *r5l_new_meta(struct r5l_log *log)\r\n{\r\nstruct r5l_io_unit *io;\r\nstruct r5l_meta_block *block;\r\nio = mempool_alloc(log->io_pool, GFP_ATOMIC);\r\nif (!io)\r\nreturn NULL;\r\nmemset(io, 0, sizeof(*io));\r\nio->log = log;\r\nINIT_LIST_HEAD(&io->log_sibling);\r\nINIT_LIST_HEAD(&io->stripe_list);\r\nio->state = IO_UNIT_RUNNING;\r\nio->meta_page = mempool_alloc(log->meta_pool, GFP_NOIO);\r\nblock = page_address(io->meta_page);\r\nclear_page(block);\r\nblock->magic = cpu_to_le32(R5LOG_MAGIC);\r\nblock->version = R5LOG_VERSION;\r\nblock->seq = cpu_to_le64(log->seq);\r\nblock->position = cpu_to_le64(log->log_start);\r\nio->log_start = log->log_start;\r\nio->meta_offset = sizeof(struct r5l_meta_block);\r\nio->seq = log->seq++;\r\nio->current_bio = r5l_bio_alloc(log);\r\nio->current_bio->bi_end_io = r5l_log_endio;\r\nio->current_bio->bi_private = io;\r\nbio_add_page(io->current_bio, io->meta_page, PAGE_SIZE, 0);\r\nr5_reserve_log_entry(log, io);\r\nspin_lock_irq(&log->io_list_lock);\r\nlist_add_tail(&io->log_sibling, &log->running_ios);\r\nspin_unlock_irq(&log->io_list_lock);\r\nreturn io;\r\n}\r\nstatic int r5l_get_meta(struct r5l_log *log, unsigned int payload_size)\r\n{\r\nif (log->current_io &&\r\nlog->current_io->meta_offset + payload_size > PAGE_SIZE)\r\nr5l_submit_current_io(log);\r\nif (!log->current_io) {\r\nlog->current_io = r5l_new_meta(log);\r\nif (!log->current_io)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void r5l_append_payload_meta(struct r5l_log *log, u16 type,\r\nsector_t location,\r\nu32 checksum1, u32 checksum2,\r\nbool checksum2_valid)\r\n{\r\nstruct r5l_io_unit *io = log->current_io;\r\nstruct r5l_payload_data_parity *payload;\r\npayload = page_address(io->meta_page) + io->meta_offset;\r\npayload->header.type = cpu_to_le16(type);\r\npayload->header.flags = cpu_to_le16(0);\r\npayload->size = cpu_to_le32((1 + !!checksum2_valid) <<\r\n(PAGE_SHIFT - 9));\r\npayload->location = cpu_to_le64(location);\r\npayload->checksum[0] = cpu_to_le32(checksum1);\r\nif (checksum2_valid)\r\npayload->checksum[1] = cpu_to_le32(checksum2);\r\nio->meta_offset += sizeof(struct r5l_payload_data_parity) +\r\nsizeof(__le32) * (1 + !!checksum2_valid);\r\n}\r\nstatic void r5l_append_payload_page(struct r5l_log *log, struct page *page)\r\n{\r\nstruct r5l_io_unit *io = log->current_io;\r\nif (io->need_split_bio) {\r\nstruct bio *prev = io->current_bio;\r\nio->current_bio = r5l_bio_alloc(log);\r\nbio_chain(io->current_bio, prev);\r\nsubmit_bio(WRITE, prev);\r\n}\r\nif (!bio_add_page(io->current_bio, page, PAGE_SIZE, 0))\r\nBUG();\r\nr5_reserve_log_entry(log, io);\r\n}\r\nstatic int r5l_log_stripe(struct r5l_log *log, struct stripe_head *sh,\r\nint data_pages, int parity_pages)\r\n{\r\nint i;\r\nint meta_size;\r\nint ret;\r\nstruct r5l_io_unit *io;\r\nmeta_size =\r\n((sizeof(struct r5l_payload_data_parity) + sizeof(__le32))\r\n* data_pages) +\r\nsizeof(struct r5l_payload_data_parity) +\r\nsizeof(__le32) * parity_pages;\r\nret = r5l_get_meta(log, meta_size);\r\nif (ret)\r\nreturn ret;\r\nio = log->current_io;\r\nfor (i = 0; i < sh->disks; i++) {\r\nif (!test_bit(R5_Wantwrite, &sh->dev[i].flags))\r\ncontinue;\r\nif (i == sh->pd_idx || i == sh->qd_idx)\r\ncontinue;\r\nr5l_append_payload_meta(log, R5LOG_PAYLOAD_DATA,\r\nraid5_compute_blocknr(sh, i, 0),\r\nsh->dev[i].log_checksum, 0, false);\r\nr5l_append_payload_page(log, sh->dev[i].page);\r\n}\r\nif (sh->qd_idx >= 0) {\r\nr5l_append_payload_meta(log, R5LOG_PAYLOAD_PARITY,\r\nsh->sector, sh->dev[sh->pd_idx].log_checksum,\r\nsh->dev[sh->qd_idx].log_checksum, true);\r\nr5l_append_payload_page(log, sh->dev[sh->pd_idx].page);\r\nr5l_append_payload_page(log, sh->dev[sh->qd_idx].page);\r\n} else {\r\nr5l_append_payload_meta(log, R5LOG_PAYLOAD_PARITY,\r\nsh->sector, sh->dev[sh->pd_idx].log_checksum,\r\n0, false);\r\nr5l_append_payload_page(log, sh->dev[sh->pd_idx].page);\r\n}\r\nlist_add_tail(&sh->log_list, &io->stripe_list);\r\natomic_inc(&io->pending_stripe);\r\nsh->log_io = io;\r\nreturn 0;\r\n}\r\nint r5l_write_stripe(struct r5l_log *log, struct stripe_head *sh)\r\n{\r\nint write_disks = 0;\r\nint data_pages, parity_pages;\r\nint meta_size;\r\nint reserve;\r\nint i;\r\nint ret = 0;\r\nif (!log)\r\nreturn -EAGAIN;\r\nif (sh->log_io || !test_bit(R5_Wantwrite, &sh->dev[sh->pd_idx].flags) ||\r\ntest_bit(STRIPE_SYNCING, &sh->state)) {\r\nclear_bit(STRIPE_LOG_TRAPPED, &sh->state);\r\nreturn -EAGAIN;\r\n}\r\nfor (i = 0; i < sh->disks; i++) {\r\nvoid *addr;\r\nif (!test_bit(R5_Wantwrite, &sh->dev[i].flags))\r\ncontinue;\r\nwrite_disks++;\r\nif (test_bit(STRIPE_LOG_TRAPPED, &sh->state))\r\ncontinue;\r\naddr = kmap_atomic(sh->dev[i].page);\r\nsh->dev[i].log_checksum = crc32c_le(log->uuid_checksum,\r\naddr, PAGE_SIZE);\r\nkunmap_atomic(addr);\r\n}\r\nparity_pages = 1 + !!(sh->qd_idx >= 0);\r\ndata_pages = write_disks - parity_pages;\r\nmeta_size =\r\n((sizeof(struct r5l_payload_data_parity) + sizeof(__le32))\r\n* data_pages) +\r\nsizeof(struct r5l_payload_data_parity) +\r\nsizeof(__le32) * parity_pages;\r\nif (meta_size + sizeof(struct r5l_meta_block) > PAGE_SIZE)\r\nreturn -EINVAL;\r\nset_bit(STRIPE_LOG_TRAPPED, &sh->state);\r\nclear_bit(STRIPE_DELAYED, &sh->state);\r\natomic_inc(&sh->count);\r\nmutex_lock(&log->io_mutex);\r\nreserve = (1 + write_disks) << (PAGE_SHIFT - 9);\r\nif (!r5l_has_free_space(log, reserve)) {\r\nspin_lock(&log->no_space_stripes_lock);\r\nlist_add_tail(&sh->log_list, &log->no_space_stripes);\r\nspin_unlock(&log->no_space_stripes_lock);\r\nr5l_wake_reclaim(log, reserve);\r\n} else {\r\nret = r5l_log_stripe(log, sh, data_pages, parity_pages);\r\nif (ret) {\r\nspin_lock_irq(&log->io_list_lock);\r\nlist_add_tail(&sh->log_list, &log->no_mem_stripes);\r\nspin_unlock_irq(&log->io_list_lock);\r\n}\r\n}\r\nmutex_unlock(&log->io_mutex);\r\nreturn 0;\r\n}\r\nvoid r5l_write_stripe_run(struct r5l_log *log)\r\n{\r\nif (!log)\r\nreturn;\r\nmutex_lock(&log->io_mutex);\r\nr5l_submit_current_io(log);\r\nmutex_unlock(&log->io_mutex);\r\n}\r\nint r5l_handle_flush_request(struct r5l_log *log, struct bio *bio)\r\n{\r\nif (!log)\r\nreturn -ENODEV;\r\nif (bio->bi_iter.bi_size == 0) {\r\nbio_endio(bio);\r\nreturn 0;\r\n}\r\nbio->bi_rw &= ~REQ_FLUSH;\r\nreturn -EAGAIN;\r\n}\r\nstatic void r5l_run_no_space_stripes(struct r5l_log *log)\r\n{\r\nstruct stripe_head *sh;\r\nspin_lock(&log->no_space_stripes_lock);\r\nwhile (!list_empty(&log->no_space_stripes)) {\r\nsh = list_first_entry(&log->no_space_stripes,\r\nstruct stripe_head, log_list);\r\nlist_del_init(&sh->log_list);\r\nset_bit(STRIPE_HANDLE, &sh->state);\r\nraid5_release_stripe(sh);\r\n}\r\nspin_unlock(&log->no_space_stripes_lock);\r\n}\r\nstatic sector_t r5l_reclaimable_space(struct r5l_log *log)\r\n{\r\nreturn r5l_ring_distance(log, log->last_checkpoint,\r\nlog->next_checkpoint);\r\n}\r\nstatic void r5l_run_no_mem_stripe(struct r5l_log *log)\r\n{\r\nstruct stripe_head *sh;\r\nassert_spin_locked(&log->io_list_lock);\r\nif (!list_empty(&log->no_mem_stripes)) {\r\nsh = list_first_entry(&log->no_mem_stripes,\r\nstruct stripe_head, log_list);\r\nlist_del_init(&sh->log_list);\r\nset_bit(STRIPE_HANDLE, &sh->state);\r\nraid5_release_stripe(sh);\r\n}\r\n}\r\nstatic bool r5l_complete_finished_ios(struct r5l_log *log)\r\n{\r\nstruct r5l_io_unit *io, *next;\r\nbool found = false;\r\nassert_spin_locked(&log->io_list_lock);\r\nlist_for_each_entry_safe(io, next, &log->finished_ios, log_sibling) {\r\nif (io->state < IO_UNIT_STRIPE_END)\r\nbreak;\r\nlog->next_checkpoint = io->log_start;\r\nlog->next_cp_seq = io->seq;\r\nlist_del(&io->log_sibling);\r\nmempool_free(io, log->io_pool);\r\nr5l_run_no_mem_stripe(log);\r\nfound = true;\r\n}\r\nreturn found;\r\n}\r\nstatic void __r5l_stripe_write_finished(struct r5l_io_unit *io)\r\n{\r\nstruct r5l_log *log = io->log;\r\nunsigned long flags;\r\nspin_lock_irqsave(&log->io_list_lock, flags);\r\n__r5l_set_io_unit_state(io, IO_UNIT_STRIPE_END);\r\nif (!r5l_complete_finished_ios(log)) {\r\nspin_unlock_irqrestore(&log->io_list_lock, flags);\r\nreturn;\r\n}\r\nif (r5l_reclaimable_space(log) > log->max_free_space)\r\nr5l_wake_reclaim(log, 0);\r\nspin_unlock_irqrestore(&log->io_list_lock, flags);\r\nwake_up(&log->iounit_wait);\r\n}\r\nvoid r5l_stripe_write_finished(struct stripe_head *sh)\r\n{\r\nstruct r5l_io_unit *io;\r\nio = sh->log_io;\r\nsh->log_io = NULL;\r\nif (io && atomic_dec_and_test(&io->pending_stripe))\r\n__r5l_stripe_write_finished(io);\r\n}\r\nstatic void r5l_log_flush_endio(struct bio *bio)\r\n{\r\nstruct r5l_log *log = container_of(bio, struct r5l_log,\r\nflush_bio);\r\nunsigned long flags;\r\nstruct r5l_io_unit *io;\r\nif (bio->bi_error)\r\nmd_error(log->rdev->mddev, log->rdev);\r\nspin_lock_irqsave(&log->io_list_lock, flags);\r\nlist_for_each_entry(io, &log->flushing_ios, log_sibling)\r\nr5l_io_run_stripes(io);\r\nlist_splice_tail_init(&log->flushing_ios, &log->finished_ios);\r\nspin_unlock_irqrestore(&log->io_list_lock, flags);\r\n}\r\nvoid r5l_flush_stripe_to_raid(struct r5l_log *log)\r\n{\r\nbool do_flush;\r\nif (!log || !log->need_cache_flush)\r\nreturn;\r\nspin_lock_irq(&log->io_list_lock);\r\nif (!list_empty(&log->flushing_ios)) {\r\nspin_unlock_irq(&log->io_list_lock);\r\nreturn;\r\n}\r\nlist_splice_tail_init(&log->io_end_ios, &log->flushing_ios);\r\ndo_flush = !list_empty(&log->flushing_ios);\r\nspin_unlock_irq(&log->io_list_lock);\r\nif (!do_flush)\r\nreturn;\r\nbio_reset(&log->flush_bio);\r\nlog->flush_bio.bi_bdev = log->rdev->bdev;\r\nlog->flush_bio.bi_end_io = r5l_log_flush_endio;\r\nsubmit_bio(WRITE_FLUSH, &log->flush_bio);\r\n}\r\nstatic void r5l_write_super_and_discard_space(struct r5l_log *log,\r\nsector_t end)\r\n{\r\nstruct block_device *bdev = log->rdev->bdev;\r\nstruct mddev *mddev;\r\nr5l_write_super(log, end);\r\nif (!blk_queue_discard(bdev_get_queue(bdev)))\r\nreturn;\r\nmddev = log->rdev->mddev;\r\nif (!log->in_teardown) {\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\nset_bit(MD_CHANGE_PENDING, &mddev->flags);\r\nmd_wakeup_thread(mddev->thread);\r\nwait_event(mddev->sb_wait,\r\n!test_bit(MD_CHANGE_PENDING, &mddev->flags) ||\r\nlog->in_teardown);\r\nif (log->in_teardown)\r\nmd_update_sb(mddev, 1);\r\n} else {\r\nWARN_ON(!mddev_is_locked(mddev));\r\nmd_update_sb(mddev, 1);\r\n}\r\nif (log->last_checkpoint < end) {\r\nblkdev_issue_discard(bdev,\r\nlog->last_checkpoint + log->rdev->data_offset,\r\nend - log->last_checkpoint, GFP_NOIO, 0);\r\n} else {\r\nblkdev_issue_discard(bdev,\r\nlog->last_checkpoint + log->rdev->data_offset,\r\nlog->device_size - log->last_checkpoint,\r\nGFP_NOIO, 0);\r\nblkdev_issue_discard(bdev, log->rdev->data_offset, end,\r\nGFP_NOIO, 0);\r\n}\r\n}\r\nstatic void r5l_do_reclaim(struct r5l_log *log)\r\n{\r\nsector_t reclaim_target = xchg(&log->reclaim_target, 0);\r\nsector_t reclaimable;\r\nsector_t next_checkpoint;\r\nu64 next_cp_seq;\r\nspin_lock_irq(&log->io_list_lock);\r\nwhile (1) {\r\nreclaimable = r5l_reclaimable_space(log);\r\nif (reclaimable >= reclaim_target ||\r\n(list_empty(&log->running_ios) &&\r\nlist_empty(&log->io_end_ios) &&\r\nlist_empty(&log->flushing_ios) &&\r\nlist_empty(&log->finished_ios)))\r\nbreak;\r\nmd_wakeup_thread(log->rdev->mddev->thread);\r\nwait_event_lock_irq(log->iounit_wait,\r\nr5l_reclaimable_space(log) > reclaimable,\r\nlog->io_list_lock);\r\n}\r\nnext_checkpoint = log->next_checkpoint;\r\nnext_cp_seq = log->next_cp_seq;\r\nspin_unlock_irq(&log->io_list_lock);\r\nBUG_ON(reclaimable < 0);\r\nif (reclaimable == 0)\r\nreturn;\r\nr5l_write_super_and_discard_space(log, next_checkpoint);\r\nmutex_lock(&log->io_mutex);\r\nlog->last_checkpoint = next_checkpoint;\r\nlog->last_cp_seq = next_cp_seq;\r\nmutex_unlock(&log->io_mutex);\r\nr5l_run_no_space_stripes(log);\r\n}\r\nstatic void r5l_reclaim_thread(struct md_thread *thread)\r\n{\r\nstruct mddev *mddev = thread->mddev;\r\nstruct r5conf *conf = mddev->private;\r\nstruct r5l_log *log = conf->log;\r\nif (!log)\r\nreturn;\r\nr5l_do_reclaim(log);\r\n}\r\nstatic void r5l_wake_reclaim(struct r5l_log *log, sector_t space)\r\n{\r\nunsigned long target;\r\nunsigned long new = (unsigned long)space;\r\ndo {\r\ntarget = log->reclaim_target;\r\nif (new < target)\r\nreturn;\r\n} while (cmpxchg(&log->reclaim_target, target, new) != target);\r\nmd_wakeup_thread(log->reclaim_thread);\r\n}\r\nvoid r5l_quiesce(struct r5l_log *log, int state)\r\n{\r\nstruct mddev *mddev;\r\nif (!log || state == 2)\r\nreturn;\r\nif (state == 0) {\r\nlog->in_teardown = 0;\r\nif (log->reclaim_thread)\r\nreturn;\r\nlog->reclaim_thread = md_register_thread(r5l_reclaim_thread,\r\nlog->rdev->mddev, "reclaim");\r\n} else if (state == 1) {\r\nlog->in_teardown = 1;\r\nmddev = log->rdev->mddev;\r\nwake_up(&mddev->sb_wait);\r\nr5l_wake_reclaim(log, -1L);\r\nmd_unregister_thread(&log->reclaim_thread);\r\nr5l_do_reclaim(log);\r\n}\r\n}\r\nbool r5l_log_disk_error(struct r5conf *conf)\r\n{\r\nstruct r5l_log *log;\r\nbool ret;\r\nrcu_read_lock();\r\nlog = rcu_dereference(conf->log);\r\nif (!log)\r\nret = test_bit(MD_HAS_JOURNAL, &conf->mddev->flags);\r\nelse\r\nret = test_bit(Faulty, &log->rdev->flags);\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int r5l_read_meta_block(struct r5l_log *log,\r\nstruct r5l_recovery_ctx *ctx)\r\n{\r\nstruct page *page = ctx->meta_page;\r\nstruct r5l_meta_block *mb;\r\nu32 crc, stored_crc;\r\nif (!sync_page_io(log->rdev, ctx->pos, PAGE_SIZE, page, READ, false))\r\nreturn -EIO;\r\nmb = page_address(page);\r\nstored_crc = le32_to_cpu(mb->checksum);\r\nmb->checksum = 0;\r\nif (le32_to_cpu(mb->magic) != R5LOG_MAGIC ||\r\nle64_to_cpu(mb->seq) != ctx->seq ||\r\nmb->version != R5LOG_VERSION ||\r\nle64_to_cpu(mb->position) != ctx->pos)\r\nreturn -EINVAL;\r\ncrc = crc32c_le(log->uuid_checksum, mb, PAGE_SIZE);\r\nif (stored_crc != crc)\r\nreturn -EINVAL;\r\nif (le32_to_cpu(mb->meta_size) > PAGE_SIZE)\r\nreturn -EINVAL;\r\nctx->meta_total_blocks = BLOCK_SECTORS;\r\nreturn 0;\r\n}\r\nstatic int r5l_recovery_flush_one_stripe(struct r5l_log *log,\r\nstruct r5l_recovery_ctx *ctx,\r\nsector_t stripe_sect,\r\nint *offset, sector_t *log_offset)\r\n{\r\nstruct r5conf *conf = log->rdev->mddev->private;\r\nstruct stripe_head *sh;\r\nstruct r5l_payload_data_parity *payload;\r\nint disk_index;\r\nsh = raid5_get_active_stripe(conf, stripe_sect, 0, 0, 0);\r\nwhile (1) {\r\npayload = page_address(ctx->meta_page) + *offset;\r\nif (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_DATA) {\r\nraid5_compute_sector(conf,\r\nle64_to_cpu(payload->location), 0,\r\n&disk_index, sh);\r\nsync_page_io(log->rdev, *log_offset, PAGE_SIZE,\r\nsh->dev[disk_index].page, READ, false);\r\nsh->dev[disk_index].log_checksum =\r\nle32_to_cpu(payload->checksum[0]);\r\nset_bit(R5_Wantwrite, &sh->dev[disk_index].flags);\r\nctx->meta_total_blocks += BLOCK_SECTORS;\r\n} else {\r\ndisk_index = sh->pd_idx;\r\nsync_page_io(log->rdev, *log_offset, PAGE_SIZE,\r\nsh->dev[disk_index].page, READ, false);\r\nsh->dev[disk_index].log_checksum =\r\nle32_to_cpu(payload->checksum[0]);\r\nset_bit(R5_Wantwrite, &sh->dev[disk_index].flags);\r\nif (sh->qd_idx >= 0) {\r\ndisk_index = sh->qd_idx;\r\nsync_page_io(log->rdev,\r\nr5l_ring_add(log, *log_offset, BLOCK_SECTORS),\r\nPAGE_SIZE, sh->dev[disk_index].page,\r\nREAD, false);\r\nsh->dev[disk_index].log_checksum =\r\nle32_to_cpu(payload->checksum[1]);\r\nset_bit(R5_Wantwrite,\r\n&sh->dev[disk_index].flags);\r\n}\r\nctx->meta_total_blocks += BLOCK_SECTORS * conf->max_degraded;\r\n}\r\n*log_offset = r5l_ring_add(log, *log_offset,\r\nle32_to_cpu(payload->size));\r\n*offset += sizeof(struct r5l_payload_data_parity) +\r\nsizeof(__le32) *\r\n(le32_to_cpu(payload->size) >> (PAGE_SHIFT - 9));\r\nif (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_PARITY)\r\nbreak;\r\n}\r\nfor (disk_index = 0; disk_index < sh->disks; disk_index++) {\r\nvoid *addr;\r\nu32 checksum;\r\nif (!test_bit(R5_Wantwrite, &sh->dev[disk_index].flags))\r\ncontinue;\r\naddr = kmap_atomic(sh->dev[disk_index].page);\r\nchecksum = crc32c_le(log->uuid_checksum, addr, PAGE_SIZE);\r\nkunmap_atomic(addr);\r\nif (checksum != sh->dev[disk_index].log_checksum)\r\ngoto error;\r\n}\r\nfor (disk_index = 0; disk_index < sh->disks; disk_index++) {\r\nstruct md_rdev *rdev, *rrdev;\r\nif (!test_and_clear_bit(R5_Wantwrite,\r\n&sh->dev[disk_index].flags))\r\ncontinue;\r\nrdev = rcu_dereference(conf->disks[disk_index].rdev);\r\nif (rdev)\r\nsync_page_io(rdev, stripe_sect, PAGE_SIZE,\r\nsh->dev[disk_index].page, WRITE, false);\r\nrrdev = rcu_dereference(conf->disks[disk_index].replacement);\r\nif (rrdev)\r\nsync_page_io(rrdev, stripe_sect, PAGE_SIZE,\r\nsh->dev[disk_index].page, WRITE, false);\r\n}\r\nraid5_release_stripe(sh);\r\nreturn 0;\r\nerror:\r\nfor (disk_index = 0; disk_index < sh->disks; disk_index++)\r\nsh->dev[disk_index].flags = 0;\r\nraid5_release_stripe(sh);\r\nreturn -EINVAL;\r\n}\r\nstatic int r5l_recovery_flush_one_meta(struct r5l_log *log,\r\nstruct r5l_recovery_ctx *ctx)\r\n{\r\nstruct r5conf *conf = log->rdev->mddev->private;\r\nstruct r5l_payload_data_parity *payload;\r\nstruct r5l_meta_block *mb;\r\nint offset;\r\nsector_t log_offset;\r\nsector_t stripe_sector;\r\nmb = page_address(ctx->meta_page);\r\noffset = sizeof(struct r5l_meta_block);\r\nlog_offset = r5l_ring_add(log, ctx->pos, BLOCK_SECTORS);\r\nwhile (offset < le32_to_cpu(mb->meta_size)) {\r\nint dd;\r\npayload = (void *)mb + offset;\r\nstripe_sector = raid5_compute_sector(conf,\r\nle64_to_cpu(payload->location), 0, &dd, NULL);\r\nif (r5l_recovery_flush_one_stripe(log, ctx, stripe_sector,\r\n&offset, &log_offset))\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void r5l_recovery_flush_log(struct r5l_log *log,\r\nstruct r5l_recovery_ctx *ctx)\r\n{\r\nwhile (1) {\r\nif (r5l_read_meta_block(log, ctx))\r\nreturn;\r\nif (r5l_recovery_flush_one_meta(log, ctx))\r\nreturn;\r\nctx->seq++;\r\nctx->pos = r5l_ring_add(log, ctx->pos, ctx->meta_total_blocks);\r\n}\r\n}\r\nstatic int r5l_log_write_empty_meta_block(struct r5l_log *log, sector_t pos,\r\nu64 seq)\r\n{\r\nstruct page *page;\r\nstruct r5l_meta_block *mb;\r\nu32 crc;\r\npage = alloc_page(GFP_KERNEL | __GFP_ZERO);\r\nif (!page)\r\nreturn -ENOMEM;\r\nmb = page_address(page);\r\nmb->magic = cpu_to_le32(R5LOG_MAGIC);\r\nmb->version = R5LOG_VERSION;\r\nmb->meta_size = cpu_to_le32(sizeof(struct r5l_meta_block));\r\nmb->seq = cpu_to_le64(seq);\r\nmb->position = cpu_to_le64(pos);\r\ncrc = crc32c_le(log->uuid_checksum, mb, PAGE_SIZE);\r\nmb->checksum = cpu_to_le32(crc);\r\nif (!sync_page_io(log->rdev, pos, PAGE_SIZE, page, WRITE_FUA, false)) {\r\n__free_page(page);\r\nreturn -EIO;\r\n}\r\n__free_page(page);\r\nreturn 0;\r\n}\r\nstatic int r5l_recovery_log(struct r5l_log *log)\r\n{\r\nstruct r5l_recovery_ctx ctx;\r\nctx.pos = log->last_checkpoint;\r\nctx.seq = log->last_cp_seq;\r\nctx.meta_page = alloc_page(GFP_KERNEL);\r\nif (!ctx.meta_page)\r\nreturn -ENOMEM;\r\nr5l_recovery_flush_log(log, &ctx);\r\n__free_page(ctx.meta_page);\r\nif (ctx.seq > log->last_cp_seq + 1) {\r\nint ret;\r\nret = r5l_log_write_empty_meta_block(log, ctx.pos, ctx.seq + 10);\r\nif (ret)\r\nreturn ret;\r\nlog->seq = ctx.seq + 11;\r\nlog->log_start = r5l_ring_add(log, ctx.pos, BLOCK_SECTORS);\r\nr5l_write_super(log, ctx.pos);\r\n} else {\r\nlog->log_start = ctx.pos;\r\nlog->seq = ctx.seq;\r\n}\r\nreturn 0;\r\n}\r\nstatic void r5l_write_super(struct r5l_log *log, sector_t cp)\r\n{\r\nstruct mddev *mddev = log->rdev->mddev;\r\nlog->rdev->journal_tail = cp;\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\n}\r\nstatic int r5l_load_log(struct r5l_log *log)\r\n{\r\nstruct md_rdev *rdev = log->rdev;\r\nstruct page *page;\r\nstruct r5l_meta_block *mb;\r\nsector_t cp = log->rdev->journal_tail;\r\nu32 stored_crc, expected_crc;\r\nbool create_super = false;\r\nint ret;\r\nif (cp >= rdev->sectors || round_down(cp, BLOCK_SECTORS) != cp)\r\ncp = 0;\r\npage = alloc_page(GFP_KERNEL);\r\nif (!page)\r\nreturn -ENOMEM;\r\nif (!sync_page_io(rdev, cp, PAGE_SIZE, page, READ, false)) {\r\nret = -EIO;\r\ngoto ioerr;\r\n}\r\nmb = page_address(page);\r\nif (le32_to_cpu(mb->magic) != R5LOG_MAGIC ||\r\nmb->version != R5LOG_VERSION) {\r\ncreate_super = true;\r\ngoto create;\r\n}\r\nstored_crc = le32_to_cpu(mb->checksum);\r\nmb->checksum = 0;\r\nexpected_crc = crc32c_le(log->uuid_checksum, mb, PAGE_SIZE);\r\nif (stored_crc != expected_crc) {\r\ncreate_super = true;\r\ngoto create;\r\n}\r\nif (le64_to_cpu(mb->position) != cp) {\r\ncreate_super = true;\r\ngoto create;\r\n}\r\ncreate:\r\nif (create_super) {\r\nlog->last_cp_seq = prandom_u32();\r\ncp = 0;\r\nr5l_write_super(log, cp);\r\n} else\r\nlog->last_cp_seq = le64_to_cpu(mb->seq);\r\nlog->device_size = round_down(rdev->sectors, BLOCK_SECTORS);\r\nlog->max_free_space = log->device_size >> RECLAIM_MAX_FREE_SPACE_SHIFT;\r\nif (log->max_free_space > RECLAIM_MAX_FREE_SPACE)\r\nlog->max_free_space = RECLAIM_MAX_FREE_SPACE;\r\nlog->last_checkpoint = cp;\r\n__free_page(page);\r\nreturn r5l_recovery_log(log);\r\nioerr:\r\n__free_page(page);\r\nreturn ret;\r\n}\r\nint r5l_init_log(struct r5conf *conf, struct md_rdev *rdev)\r\n{\r\nstruct r5l_log *log;\r\nif (PAGE_SIZE != 4096)\r\nreturn -EINVAL;\r\nlog = kzalloc(sizeof(*log), GFP_KERNEL);\r\nif (!log)\r\nreturn -ENOMEM;\r\nlog->rdev = rdev;\r\nlog->need_cache_flush = (rdev->bdev->bd_disk->queue->flush_flags != 0);\r\nlog->uuid_checksum = crc32c_le(~0, rdev->mddev->uuid,\r\nsizeof(rdev->mddev->uuid));\r\nmutex_init(&log->io_mutex);\r\nspin_lock_init(&log->io_list_lock);\r\nINIT_LIST_HEAD(&log->running_ios);\r\nINIT_LIST_HEAD(&log->io_end_ios);\r\nINIT_LIST_HEAD(&log->flushing_ios);\r\nINIT_LIST_HEAD(&log->finished_ios);\r\nbio_init(&log->flush_bio);\r\nlog->io_kc = KMEM_CACHE(r5l_io_unit, 0);\r\nif (!log->io_kc)\r\ngoto io_kc;\r\nlog->io_pool = mempool_create_slab_pool(R5L_POOL_SIZE, log->io_kc);\r\nif (!log->io_pool)\r\ngoto io_pool;\r\nlog->bs = bioset_create(R5L_POOL_SIZE, 0);\r\nif (!log->bs)\r\ngoto io_bs;\r\nlog->meta_pool = mempool_create_page_pool(R5L_POOL_SIZE, 0);\r\nif (!log->meta_pool)\r\ngoto out_mempool;\r\nlog->reclaim_thread = md_register_thread(r5l_reclaim_thread,\r\nlog->rdev->mddev, "reclaim");\r\nif (!log->reclaim_thread)\r\ngoto reclaim_thread;\r\ninit_waitqueue_head(&log->iounit_wait);\r\nINIT_LIST_HEAD(&log->no_mem_stripes);\r\nINIT_LIST_HEAD(&log->no_space_stripes);\r\nspin_lock_init(&log->no_space_stripes_lock);\r\nif (r5l_load_log(log))\r\ngoto error;\r\nrcu_assign_pointer(conf->log, log);\r\nset_bit(MD_HAS_JOURNAL, &conf->mddev->flags);\r\nreturn 0;\r\nerror:\r\nmd_unregister_thread(&log->reclaim_thread);\r\nreclaim_thread:\r\nmempool_destroy(log->meta_pool);\r\nout_mempool:\r\nbioset_free(log->bs);\r\nio_bs:\r\nmempool_destroy(log->io_pool);\r\nio_pool:\r\nkmem_cache_destroy(log->io_kc);\r\nio_kc:\r\nkfree(log);\r\nreturn -EINVAL;\r\n}\r\nvoid r5l_exit_log(struct r5l_log *log)\r\n{\r\nmd_unregister_thread(&log->reclaim_thread);\r\nmempool_destroy(log->meta_pool);\r\nbioset_free(log->bs);\r\nmempool_destroy(log->io_pool);\r\nkmem_cache_destroy(log->io_kc);\r\nkfree(log);\r\n}
