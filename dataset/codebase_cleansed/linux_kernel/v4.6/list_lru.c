static void list_lru_register(struct list_lru *lru)\r\n{\r\nmutex_lock(&list_lrus_mutex);\r\nlist_add(&lru->list, &list_lrus);\r\nmutex_unlock(&list_lrus_mutex);\r\n}\r\nstatic void list_lru_unregister(struct list_lru *lru)\r\n{\r\nmutex_lock(&list_lrus_mutex);\r\nlist_del(&lru->list);\r\nmutex_unlock(&list_lrus_mutex);\r\n}\r\nstatic void list_lru_register(struct list_lru *lru)\r\n{\r\n}\r\nstatic void list_lru_unregister(struct list_lru *lru)\r\n{\r\n}\r\nstatic inline bool list_lru_memcg_aware(struct list_lru *lru)\r\n{\r\nreturn !!lru->node[0].memcg_lrus;\r\n}\r\nstatic inline struct list_lru_one *\r\nlist_lru_from_memcg_idx(struct list_lru_node *nlru, int idx)\r\n{\r\nlockdep_assert_held(&nlru->lock);\r\nif (nlru->memcg_lrus && idx >= 0)\r\nreturn nlru->memcg_lrus->lru[idx];\r\nreturn &nlru->lru;\r\n}\r\ninline struct list_lru_one *\r\nlist_lru_from_kmem(struct list_lru_node *nlru, void *ptr)\r\n{\r\nstruct mem_cgroup *memcg;\r\nif (!nlru->memcg_lrus)\r\nreturn &nlru->lru;\r\nmemcg = mem_cgroup_from_kmem(ptr);\r\nif (!memcg)\r\nreturn &nlru->lru;\r\nreturn list_lru_from_memcg_idx(nlru, memcg_cache_id(memcg));\r\n}\r\nstatic inline bool list_lru_memcg_aware(struct list_lru *lru)\r\n{\r\nreturn false;\r\n}\r\nstatic inline struct list_lru_one *\r\nlist_lru_from_memcg_idx(struct list_lru_node *nlru, int idx)\r\n{\r\nreturn &nlru->lru;\r\n}\r\nstatic inline struct list_lru_one *\r\nlist_lru_from_kmem(struct list_lru_node *nlru, void *ptr)\r\n{\r\nreturn &nlru->lru;\r\n}\r\nbool list_lru_add(struct list_lru *lru, struct list_head *item)\r\n{\r\nint nid = page_to_nid(virt_to_page(item));\r\nstruct list_lru_node *nlru = &lru->node[nid];\r\nstruct list_lru_one *l;\r\nspin_lock(&nlru->lock);\r\nif (list_empty(item)) {\r\nl = list_lru_from_kmem(nlru, item);\r\nlist_add_tail(item, &l->list);\r\nl->nr_items++;\r\nspin_unlock(&nlru->lock);\r\nreturn true;\r\n}\r\nspin_unlock(&nlru->lock);\r\nreturn false;\r\n}\r\nbool list_lru_del(struct list_lru *lru, struct list_head *item)\r\n{\r\nint nid = page_to_nid(virt_to_page(item));\r\nstruct list_lru_node *nlru = &lru->node[nid];\r\nstruct list_lru_one *l;\r\nspin_lock(&nlru->lock);\r\nif (!list_empty(item)) {\r\nl = list_lru_from_kmem(nlru, item);\r\nlist_del_init(item);\r\nl->nr_items--;\r\nspin_unlock(&nlru->lock);\r\nreturn true;\r\n}\r\nspin_unlock(&nlru->lock);\r\nreturn false;\r\n}\r\nvoid list_lru_isolate(struct list_lru_one *list, struct list_head *item)\r\n{\r\nlist_del_init(item);\r\nlist->nr_items--;\r\n}\r\nvoid list_lru_isolate_move(struct list_lru_one *list, struct list_head *item,\r\nstruct list_head *head)\r\n{\r\nlist_move(item, head);\r\nlist->nr_items--;\r\n}\r\nstatic unsigned long __list_lru_count_one(struct list_lru *lru,\r\nint nid, int memcg_idx)\r\n{\r\nstruct list_lru_node *nlru = &lru->node[nid];\r\nstruct list_lru_one *l;\r\nunsigned long count;\r\nspin_lock(&nlru->lock);\r\nl = list_lru_from_memcg_idx(nlru, memcg_idx);\r\ncount = l->nr_items;\r\nspin_unlock(&nlru->lock);\r\nreturn count;\r\n}\r\nunsigned long list_lru_count_one(struct list_lru *lru,\r\nint nid, struct mem_cgroup *memcg)\r\n{\r\nreturn __list_lru_count_one(lru, nid, memcg_cache_id(memcg));\r\n}\r\nunsigned long list_lru_count_node(struct list_lru *lru, int nid)\r\n{\r\nlong count = 0;\r\nint memcg_idx;\r\ncount += __list_lru_count_one(lru, nid, -1);\r\nif (list_lru_memcg_aware(lru)) {\r\nfor_each_memcg_cache_index(memcg_idx)\r\ncount += __list_lru_count_one(lru, nid, memcg_idx);\r\n}\r\nreturn count;\r\n}\r\nstatic unsigned long\r\n__list_lru_walk_one(struct list_lru *lru, int nid, int memcg_idx,\r\nlist_lru_walk_cb isolate, void *cb_arg,\r\nunsigned long *nr_to_walk)\r\n{\r\nstruct list_lru_node *nlru = &lru->node[nid];\r\nstruct list_lru_one *l;\r\nstruct list_head *item, *n;\r\nunsigned long isolated = 0;\r\nspin_lock(&nlru->lock);\r\nl = list_lru_from_memcg_idx(nlru, memcg_idx);\r\nrestart:\r\nlist_for_each_safe(item, n, &l->list) {\r\nenum lru_status ret;\r\nif (!*nr_to_walk)\r\nbreak;\r\n--*nr_to_walk;\r\nret = isolate(item, l, &nlru->lock, cb_arg);\r\nswitch (ret) {\r\ncase LRU_REMOVED_RETRY:\r\nassert_spin_locked(&nlru->lock);\r\ncase LRU_REMOVED:\r\nisolated++;\r\nif (ret == LRU_REMOVED_RETRY)\r\ngoto restart;\r\nbreak;\r\ncase LRU_ROTATE:\r\nlist_move_tail(item, &l->list);\r\nbreak;\r\ncase LRU_SKIP:\r\nbreak;\r\ncase LRU_RETRY:\r\nassert_spin_locked(&nlru->lock);\r\ngoto restart;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nspin_unlock(&nlru->lock);\r\nreturn isolated;\r\n}\r\nunsigned long\r\nlist_lru_walk_one(struct list_lru *lru, int nid, struct mem_cgroup *memcg,\r\nlist_lru_walk_cb isolate, void *cb_arg,\r\nunsigned long *nr_to_walk)\r\n{\r\nreturn __list_lru_walk_one(lru, nid, memcg_cache_id(memcg),\r\nisolate, cb_arg, nr_to_walk);\r\n}\r\nunsigned long list_lru_walk_node(struct list_lru *lru, int nid,\r\nlist_lru_walk_cb isolate, void *cb_arg,\r\nunsigned long *nr_to_walk)\r\n{\r\nlong isolated = 0;\r\nint memcg_idx;\r\nisolated += __list_lru_walk_one(lru, nid, -1, isolate, cb_arg,\r\nnr_to_walk);\r\nif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {\r\nfor_each_memcg_cache_index(memcg_idx) {\r\nisolated += __list_lru_walk_one(lru, nid, memcg_idx,\r\nisolate, cb_arg, nr_to_walk);\r\nif (*nr_to_walk <= 0)\r\nbreak;\r\n}\r\n}\r\nreturn isolated;\r\n}\r\nstatic void init_one_lru(struct list_lru_one *l)\r\n{\r\nINIT_LIST_HEAD(&l->list);\r\nl->nr_items = 0;\r\n}\r\nstatic void __memcg_destroy_list_lru_node(struct list_lru_memcg *memcg_lrus,\r\nint begin, int end)\r\n{\r\nint i;\r\nfor (i = begin; i < end; i++)\r\nkfree(memcg_lrus->lru[i]);\r\n}\r\nstatic int __memcg_init_list_lru_node(struct list_lru_memcg *memcg_lrus,\r\nint begin, int end)\r\n{\r\nint i;\r\nfor (i = begin; i < end; i++) {\r\nstruct list_lru_one *l;\r\nl = kmalloc(sizeof(struct list_lru_one), GFP_KERNEL);\r\nif (!l)\r\ngoto fail;\r\ninit_one_lru(l);\r\nmemcg_lrus->lru[i] = l;\r\n}\r\nreturn 0;\r\nfail:\r\n__memcg_destroy_list_lru_node(memcg_lrus, begin, i - 1);\r\nreturn -ENOMEM;\r\n}\r\nstatic int memcg_init_list_lru_node(struct list_lru_node *nlru)\r\n{\r\nint size = memcg_nr_cache_ids;\r\nnlru->memcg_lrus = kmalloc(size * sizeof(void *), GFP_KERNEL);\r\nif (!nlru->memcg_lrus)\r\nreturn -ENOMEM;\r\nif (__memcg_init_list_lru_node(nlru->memcg_lrus, 0, size)) {\r\nkfree(nlru->memcg_lrus);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void memcg_destroy_list_lru_node(struct list_lru_node *nlru)\r\n{\r\n__memcg_destroy_list_lru_node(nlru->memcg_lrus, 0, memcg_nr_cache_ids);\r\nkfree(nlru->memcg_lrus);\r\n}\r\nstatic int memcg_update_list_lru_node(struct list_lru_node *nlru,\r\nint old_size, int new_size)\r\n{\r\nstruct list_lru_memcg *old, *new;\r\nBUG_ON(old_size > new_size);\r\nold = nlru->memcg_lrus;\r\nnew = kmalloc(new_size * sizeof(void *), GFP_KERNEL);\r\nif (!new)\r\nreturn -ENOMEM;\r\nif (__memcg_init_list_lru_node(new, old_size, new_size)) {\r\nkfree(new);\r\nreturn -ENOMEM;\r\n}\r\nmemcpy(new, old, old_size * sizeof(void *));\r\nspin_lock_irq(&nlru->lock);\r\nnlru->memcg_lrus = new;\r\nspin_unlock_irq(&nlru->lock);\r\nkfree(old);\r\nreturn 0;\r\n}\r\nstatic void memcg_cancel_update_list_lru_node(struct list_lru_node *nlru,\r\nint old_size, int new_size)\r\n{\r\n__memcg_destroy_list_lru_node(nlru->memcg_lrus, old_size, new_size);\r\n}\r\nstatic int memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)\r\n{\r\nint i;\r\nif (!memcg_aware)\r\nreturn 0;\r\nfor_each_node(i) {\r\nif (memcg_init_list_lru_node(&lru->node[i]))\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nfor (i = i - 1; i >= 0; i--) {\r\nif (!lru->node[i].memcg_lrus)\r\ncontinue;\r\nmemcg_destroy_list_lru_node(&lru->node[i]);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void memcg_destroy_list_lru(struct list_lru *lru)\r\n{\r\nint i;\r\nif (!list_lru_memcg_aware(lru))\r\nreturn;\r\nfor_each_node(i)\r\nmemcg_destroy_list_lru_node(&lru->node[i]);\r\n}\r\nstatic int memcg_update_list_lru(struct list_lru *lru,\r\nint old_size, int new_size)\r\n{\r\nint i;\r\nif (!list_lru_memcg_aware(lru))\r\nreturn 0;\r\nfor_each_node(i) {\r\nif (memcg_update_list_lru_node(&lru->node[i],\r\nold_size, new_size))\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nfor (i = i - 1; i >= 0; i--) {\r\nif (!lru->node[i].memcg_lrus)\r\ncontinue;\r\nmemcg_cancel_update_list_lru_node(&lru->node[i],\r\nold_size, new_size);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void memcg_cancel_update_list_lru(struct list_lru *lru,\r\nint old_size, int new_size)\r\n{\r\nint i;\r\nif (!list_lru_memcg_aware(lru))\r\nreturn;\r\nfor_each_node(i)\r\nmemcg_cancel_update_list_lru_node(&lru->node[i],\r\nold_size, new_size);\r\n}\r\nint memcg_update_all_list_lrus(int new_size)\r\n{\r\nint ret = 0;\r\nstruct list_lru *lru;\r\nint old_size = memcg_nr_cache_ids;\r\nmutex_lock(&list_lrus_mutex);\r\nlist_for_each_entry(lru, &list_lrus, list) {\r\nret = memcg_update_list_lru(lru, old_size, new_size);\r\nif (ret)\r\ngoto fail;\r\n}\r\nout:\r\nmutex_unlock(&list_lrus_mutex);\r\nreturn ret;\r\nfail:\r\nlist_for_each_entry_continue_reverse(lru, &list_lrus, list)\r\nmemcg_cancel_update_list_lru(lru, old_size, new_size);\r\ngoto out;\r\n}\r\nstatic void memcg_drain_list_lru_node(struct list_lru_node *nlru,\r\nint src_idx, int dst_idx)\r\n{\r\nstruct list_lru_one *src, *dst;\r\nspin_lock_irq(&nlru->lock);\r\nsrc = list_lru_from_memcg_idx(nlru, src_idx);\r\ndst = list_lru_from_memcg_idx(nlru, dst_idx);\r\nlist_splice_init(&src->list, &dst->list);\r\ndst->nr_items += src->nr_items;\r\nsrc->nr_items = 0;\r\nspin_unlock_irq(&nlru->lock);\r\n}\r\nstatic void memcg_drain_list_lru(struct list_lru *lru,\r\nint src_idx, int dst_idx)\r\n{\r\nint i;\r\nif (!list_lru_memcg_aware(lru))\r\nreturn;\r\nfor_each_node(i)\r\nmemcg_drain_list_lru_node(&lru->node[i], src_idx, dst_idx);\r\n}\r\nvoid memcg_drain_all_list_lrus(int src_idx, int dst_idx)\r\n{\r\nstruct list_lru *lru;\r\nmutex_lock(&list_lrus_mutex);\r\nlist_for_each_entry(lru, &list_lrus, list)\r\nmemcg_drain_list_lru(lru, src_idx, dst_idx);\r\nmutex_unlock(&list_lrus_mutex);\r\n}\r\nstatic int memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)\r\n{\r\nreturn 0;\r\n}\r\nstatic void memcg_destroy_list_lru(struct list_lru *lru)\r\n{\r\n}\r\nint __list_lru_init(struct list_lru *lru, bool memcg_aware,\r\nstruct lock_class_key *key)\r\n{\r\nint i;\r\nsize_t size = sizeof(*lru->node) * nr_node_ids;\r\nint err = -ENOMEM;\r\nmemcg_get_cache_ids();\r\nlru->node = kzalloc(size, GFP_KERNEL);\r\nif (!lru->node)\r\ngoto out;\r\nfor_each_node(i) {\r\nspin_lock_init(&lru->node[i].lock);\r\nif (key)\r\nlockdep_set_class(&lru->node[i].lock, key);\r\ninit_one_lru(&lru->node[i].lru);\r\n}\r\nerr = memcg_init_list_lru(lru, memcg_aware);\r\nif (err) {\r\nkfree(lru->node);\r\ngoto out;\r\n}\r\nlist_lru_register(lru);\r\nout:\r\nmemcg_put_cache_ids();\r\nreturn err;\r\n}\r\nvoid list_lru_destroy(struct list_lru *lru)\r\n{\r\nif (!lru->node)\r\nreturn;\r\nmemcg_get_cache_ids();\r\nlist_lru_unregister(lru);\r\nmemcg_destroy_list_lru(lru);\r\nkfree(lru->node);\r\nlru->node = NULL;\r\nmemcg_put_cache_ids();\r\n}
