static pm_callback_t __rpm_get_callback(struct device *dev, size_t cb_offset)\r\n{\r\npm_callback_t cb;\r\nconst struct dev_pm_ops *ops;\r\nif (dev->pm_domain)\r\nops = &dev->pm_domain->ops;\r\nelse if (dev->type && dev->type->pm)\r\nops = dev->type->pm;\r\nelse if (dev->class && dev->class->pm)\r\nops = dev->class->pm;\r\nelse if (dev->bus && dev->bus->pm)\r\nops = dev->bus->pm;\r\nelse\r\nops = NULL;\r\nif (ops)\r\ncb = *(pm_callback_t *)((void *)ops + cb_offset);\r\nelse\r\ncb = NULL;\r\nif (!cb && dev->driver && dev->driver->pm)\r\ncb = *(pm_callback_t *)((void *)dev->driver->pm + cb_offset);\r\nreturn cb;\r\n}\r\nvoid update_pm_runtime_accounting(struct device *dev)\r\n{\r\nunsigned long now = jiffies;\r\nunsigned long delta;\r\ndelta = now - dev->power.accounting_timestamp;\r\ndev->power.accounting_timestamp = now;\r\nif (dev->power.disable_depth > 0)\r\nreturn;\r\nif (dev->power.runtime_status == RPM_SUSPENDED)\r\ndev->power.suspended_jiffies += delta;\r\nelse\r\ndev->power.active_jiffies += delta;\r\n}\r\nstatic void __update_runtime_status(struct device *dev, enum rpm_status status)\r\n{\r\nupdate_pm_runtime_accounting(dev);\r\ndev->power.runtime_status = status;\r\n}\r\nstatic void pm_runtime_deactivate_timer(struct device *dev)\r\n{\r\nif (dev->power.timer_expires > 0) {\r\ndel_timer(&dev->power.suspend_timer);\r\ndev->power.timer_expires = 0;\r\n}\r\n}\r\nstatic void pm_runtime_cancel_pending(struct device *dev)\r\n{\r\npm_runtime_deactivate_timer(dev);\r\ndev->power.request = RPM_REQ_NONE;\r\n}\r\nunsigned long pm_runtime_autosuspend_expiration(struct device *dev)\r\n{\r\nint autosuspend_delay;\r\nlong elapsed;\r\nunsigned long last_busy;\r\nunsigned long expires = 0;\r\nif (!dev->power.use_autosuspend)\r\ngoto out;\r\nautosuspend_delay = ACCESS_ONCE(dev->power.autosuspend_delay);\r\nif (autosuspend_delay < 0)\r\ngoto out;\r\nlast_busy = ACCESS_ONCE(dev->power.last_busy);\r\nelapsed = jiffies - last_busy;\r\nif (elapsed < 0)\r\ngoto out;\r\nexpires = last_busy + msecs_to_jiffies(autosuspend_delay);\r\nif (autosuspend_delay >= 1000)\r\nexpires = round_jiffies(expires);\r\nexpires += !expires;\r\nif (elapsed >= expires - last_busy)\r\nexpires = 0;\r\nout:\r\nreturn expires;\r\n}\r\nstatic int dev_memalloc_noio(struct device *dev, void *data)\r\n{\r\nreturn dev->power.memalloc_noio;\r\n}\r\nvoid pm_runtime_set_memalloc_noio(struct device *dev, bool enable)\r\n{\r\nstatic DEFINE_MUTEX(dev_hotplug_mutex);\r\nmutex_lock(&dev_hotplug_mutex);\r\nfor (;;) {\r\nbool enabled;\r\nspin_lock_irq(&dev->power.lock);\r\nenabled = dev->power.memalloc_noio;\r\ndev->power.memalloc_noio = enable;\r\nspin_unlock_irq(&dev->power.lock);\r\nif (enabled && enable)\r\nbreak;\r\ndev = dev->parent;\r\nif (!dev || (!enable &&\r\ndevice_for_each_child(dev, NULL,\r\ndev_memalloc_noio)))\r\nbreak;\r\n}\r\nmutex_unlock(&dev_hotplug_mutex);\r\n}\r\nstatic int rpm_check_suspend_allowed(struct device *dev)\r\n{\r\nint retval = 0;\r\nif (dev->power.runtime_error)\r\nretval = -EINVAL;\r\nelse if (dev->power.disable_depth > 0)\r\nretval = -EACCES;\r\nelse if (atomic_read(&dev->power.usage_count) > 0)\r\nretval = -EAGAIN;\r\nelse if (!pm_children_suspended(dev))\r\nretval = -EBUSY;\r\nelse if ((dev->power.deferred_resume\r\n&& dev->power.runtime_status == RPM_SUSPENDING)\r\n|| (dev->power.request_pending\r\n&& dev->power.request == RPM_REQ_RESUME))\r\nretval = -EAGAIN;\r\nelse if (__dev_pm_qos_read_value(dev) < 0)\r\nretval = -EPERM;\r\nelse if (dev->power.runtime_status == RPM_SUSPENDED)\r\nretval = 1;\r\nreturn retval;\r\n}\r\nstatic int __rpm_callback(int (*cb)(struct device *), struct device *dev)\r\n__releases(&dev->power.lock\r\nstatic int rpm_idle(struct device *dev, int rpmflags)\r\n{\r\nint (*callback)(struct device *);\r\nint retval;\r\ntrace_rpm_idle(dev, rpmflags);\r\nretval = rpm_check_suspend_allowed(dev);\r\nif (retval < 0)\r\n;\r\nelse if (dev->power.runtime_status != RPM_ACTIVE)\r\nretval = -EAGAIN;\r\nelse if (dev->power.request_pending &&\r\ndev->power.request > RPM_REQ_IDLE)\r\nretval = -EAGAIN;\r\nelse if (dev->power.idle_notification)\r\nretval = -EINPROGRESS;\r\nif (retval)\r\ngoto out;\r\ndev->power.request = RPM_REQ_NONE;\r\nif (dev->power.no_callbacks)\r\ngoto out;\r\nif (rpmflags & RPM_ASYNC) {\r\ndev->power.request = RPM_REQ_IDLE;\r\nif (!dev->power.request_pending) {\r\ndev->power.request_pending = true;\r\nqueue_work(pm_wq, &dev->power.work);\r\n}\r\ntrace_rpm_return_int(dev, _THIS_IP_, 0);\r\nreturn 0;\r\n}\r\ndev->power.idle_notification = true;\r\ncallback = RPM_GET_CALLBACK(dev, runtime_idle);\r\nif (callback)\r\nretval = __rpm_callback(callback, dev);\r\ndev->power.idle_notification = false;\r\nwake_up_all(&dev->power.wait_queue);\r\nout:\r\ntrace_rpm_return_int(dev, _THIS_IP_, retval);\r\nreturn retval ? retval : rpm_suspend(dev, rpmflags | RPM_AUTO);\r\n}\r\nstatic int rpm_callback(int (*cb)(struct device *), struct device *dev)\r\n{\r\nint retval;\r\nif (!cb)\r\nreturn -ENOSYS;\r\nif (dev->power.memalloc_noio) {\r\nunsigned int noio_flag;\r\nnoio_flag = memalloc_noio_save();\r\nretval = __rpm_callback(cb, dev);\r\nmemalloc_noio_restore(noio_flag);\r\n} else {\r\nretval = __rpm_callback(cb, dev);\r\n}\r\ndev->power.runtime_error = retval;\r\nreturn retval != -EACCES ? retval : -EIO;\r\n}\r\nstatic int rpm_suspend(struct device *dev, int rpmflags)\r\n__releases(&dev->power.lock\r\nstatic int rpm_resume(struct device *dev, int rpmflags)\r\n__releases(&dev->power.lock\r\nstatic void pm_runtime_work(struct work_struct *work)\r\n{\r\nstruct device *dev = container_of(work, struct device, power.work);\r\nenum rpm_request req;\r\nspin_lock_irq(&dev->power.lock);\r\nif (!dev->power.request_pending)\r\ngoto out;\r\nreq = dev->power.request;\r\ndev->power.request = RPM_REQ_NONE;\r\ndev->power.request_pending = false;\r\nswitch (req) {\r\ncase RPM_REQ_NONE:\r\nbreak;\r\ncase RPM_REQ_IDLE:\r\nrpm_idle(dev, RPM_NOWAIT);\r\nbreak;\r\ncase RPM_REQ_SUSPEND:\r\nrpm_suspend(dev, RPM_NOWAIT);\r\nbreak;\r\ncase RPM_REQ_AUTOSUSPEND:\r\nrpm_suspend(dev, RPM_NOWAIT | RPM_AUTO);\r\nbreak;\r\ncase RPM_REQ_RESUME:\r\nrpm_resume(dev, RPM_NOWAIT);\r\nbreak;\r\n}\r\nout:\r\nspin_unlock_irq(&dev->power.lock);\r\n}\r\nstatic void pm_suspend_timer_fn(unsigned long data)\r\n{\r\nstruct device *dev = (struct device *)data;\r\nunsigned long flags;\r\nunsigned long expires;\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nexpires = dev->power.timer_expires;\r\nif (expires > 0 && !time_after(expires, jiffies)) {\r\ndev->power.timer_expires = 0;\r\nrpm_suspend(dev, dev->power.timer_autosuspends ?\r\n(RPM_ASYNC | RPM_AUTO) : RPM_ASYNC);\r\n}\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\n}\r\nint pm_schedule_suspend(struct device *dev, unsigned int delay)\r\n{\r\nunsigned long flags;\r\nint retval;\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nif (!delay) {\r\nretval = rpm_suspend(dev, RPM_ASYNC);\r\ngoto out;\r\n}\r\nretval = rpm_check_suspend_allowed(dev);\r\nif (retval)\r\ngoto out;\r\npm_runtime_cancel_pending(dev);\r\ndev->power.timer_expires = jiffies + msecs_to_jiffies(delay);\r\ndev->power.timer_expires += !dev->power.timer_expires;\r\ndev->power.timer_autosuspends = 0;\r\nmod_timer(&dev->power.suspend_timer, dev->power.timer_expires);\r\nout:\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\nreturn retval;\r\n}\r\nint __pm_runtime_idle(struct device *dev, int rpmflags)\r\n{\r\nunsigned long flags;\r\nint retval;\r\nmight_sleep_if(!(rpmflags & RPM_ASYNC) && !dev->power.irq_safe);\r\nif (rpmflags & RPM_GET_PUT) {\r\nif (!atomic_dec_and_test(&dev->power.usage_count))\r\nreturn 0;\r\n}\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nretval = rpm_idle(dev, rpmflags);\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\nreturn retval;\r\n}\r\nint __pm_runtime_suspend(struct device *dev, int rpmflags)\r\n{\r\nunsigned long flags;\r\nint retval;\r\nmight_sleep_if(!(rpmflags & RPM_ASYNC) && !dev->power.irq_safe);\r\nif (rpmflags & RPM_GET_PUT) {\r\nif (!atomic_dec_and_test(&dev->power.usage_count))\r\nreturn 0;\r\n}\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nretval = rpm_suspend(dev, rpmflags);\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\nreturn retval;\r\n}\r\nint __pm_runtime_resume(struct device *dev, int rpmflags)\r\n{\r\nunsigned long flags;\r\nint retval;\r\nmight_sleep_if(!(rpmflags & RPM_ASYNC) && !dev->power.irq_safe);\r\nif (rpmflags & RPM_GET_PUT)\r\natomic_inc(&dev->power.usage_count);\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nretval = rpm_resume(dev, rpmflags);\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\nreturn retval;\r\n}\r\nint pm_runtime_get_if_in_use(struct device *dev)\r\n{\r\nunsigned long flags;\r\nint retval;\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nretval = dev->power.disable_depth > 0 ? -EINVAL :\r\ndev->power.runtime_status == RPM_ACTIVE\r\n&& atomic_inc_not_zero(&dev->power.usage_count);\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\nreturn retval;\r\n}\r\nint __pm_runtime_set_status(struct device *dev, unsigned int status)\r\n{\r\nstruct device *parent = dev->parent;\r\nunsigned long flags;\r\nbool notify_parent = false;\r\nint error = 0;\r\nif (status != RPM_ACTIVE && status != RPM_SUSPENDED)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nif (!dev->power.runtime_error && !dev->power.disable_depth) {\r\nerror = -EAGAIN;\r\ngoto out;\r\n}\r\nif (dev->power.runtime_status == status)\r\ngoto out_set;\r\nif (status == RPM_SUSPENDED) {\r\nif (parent) {\r\natomic_add_unless(&parent->power.child_count, -1, 0);\r\nnotify_parent = !parent->power.ignore_children;\r\n}\r\ngoto out_set;\r\n}\r\nif (parent) {\r\nspin_lock_nested(&parent->power.lock, SINGLE_DEPTH_NESTING);\r\nif (!parent->power.disable_depth\r\n&& !parent->power.ignore_children\r\n&& parent->power.runtime_status != RPM_ACTIVE)\r\nerror = -EBUSY;\r\nelse if (dev->power.runtime_status == RPM_SUSPENDED)\r\natomic_inc(&parent->power.child_count);\r\nspin_unlock(&parent->power.lock);\r\nif (error)\r\ngoto out;\r\n}\r\nout_set:\r\n__update_runtime_status(dev, status);\r\ndev->power.runtime_error = 0;\r\nout:\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\nif (notify_parent)\r\npm_request_idle(parent);\r\nreturn error;\r\n}\r\nstatic void __pm_runtime_barrier(struct device *dev)\r\n{\r\npm_runtime_deactivate_timer(dev);\r\nif (dev->power.request_pending) {\r\ndev->power.request = RPM_REQ_NONE;\r\nspin_unlock_irq(&dev->power.lock);\r\ncancel_work_sync(&dev->power.work);\r\nspin_lock_irq(&dev->power.lock);\r\ndev->power.request_pending = false;\r\n}\r\nif (dev->power.runtime_status == RPM_SUSPENDING\r\n|| dev->power.runtime_status == RPM_RESUMING\r\n|| dev->power.idle_notification) {\r\nDEFINE_WAIT(wait);\r\nfor (;;) {\r\nprepare_to_wait(&dev->power.wait_queue, &wait,\r\nTASK_UNINTERRUPTIBLE);\r\nif (dev->power.runtime_status != RPM_SUSPENDING\r\n&& dev->power.runtime_status != RPM_RESUMING\r\n&& !dev->power.idle_notification)\r\nbreak;\r\nspin_unlock_irq(&dev->power.lock);\r\nschedule();\r\nspin_lock_irq(&dev->power.lock);\r\n}\r\nfinish_wait(&dev->power.wait_queue, &wait);\r\n}\r\n}\r\nint pm_runtime_barrier(struct device *dev)\r\n{\r\nint retval = 0;\r\npm_runtime_get_noresume(dev);\r\nspin_lock_irq(&dev->power.lock);\r\nif (dev->power.request_pending\r\n&& dev->power.request == RPM_REQ_RESUME) {\r\nrpm_resume(dev, 0);\r\nretval = 1;\r\n}\r\n__pm_runtime_barrier(dev);\r\nspin_unlock_irq(&dev->power.lock);\r\npm_runtime_put_noidle(dev);\r\nreturn retval;\r\n}\r\nvoid __pm_runtime_disable(struct device *dev, bool check_resume)\r\n{\r\nspin_lock_irq(&dev->power.lock);\r\nif (dev->power.disable_depth > 0) {\r\ndev->power.disable_depth++;\r\ngoto out;\r\n}\r\nif (check_resume && dev->power.request_pending\r\n&& dev->power.request == RPM_REQ_RESUME) {\r\npm_runtime_get_noresume(dev);\r\nrpm_resume(dev, 0);\r\npm_runtime_put_noidle(dev);\r\n}\r\nif (!dev->power.disable_depth++)\r\n__pm_runtime_barrier(dev);\r\nout:\r\nspin_unlock_irq(&dev->power.lock);\r\n}\r\nvoid pm_runtime_enable(struct device *dev)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->power.lock, flags);\r\nif (dev->power.disable_depth > 0)\r\ndev->power.disable_depth--;\r\nelse\r\ndev_warn(dev, "Unbalanced %s!\n", __func__);\r\nspin_unlock_irqrestore(&dev->power.lock, flags);\r\n}\r\nvoid pm_runtime_forbid(struct device *dev)\r\n{\r\nspin_lock_irq(&dev->power.lock);\r\nif (!dev->power.runtime_auto)\r\ngoto out;\r\ndev->power.runtime_auto = false;\r\natomic_inc(&dev->power.usage_count);\r\nrpm_resume(dev, 0);\r\nout:\r\nspin_unlock_irq(&dev->power.lock);\r\n}\r\nvoid pm_runtime_allow(struct device *dev)\r\n{\r\nspin_lock_irq(&dev->power.lock);\r\nif (dev->power.runtime_auto)\r\ngoto out;\r\ndev->power.runtime_auto = true;\r\nif (atomic_dec_and_test(&dev->power.usage_count))\r\nrpm_idle(dev, RPM_AUTO);\r\nout:\r\nspin_unlock_irq(&dev->power.lock);\r\n}\r\nvoid pm_runtime_no_callbacks(struct device *dev)\r\n{\r\nspin_lock_irq(&dev->power.lock);\r\ndev->power.no_callbacks = 1;\r\nspin_unlock_irq(&dev->power.lock);\r\nif (device_is_registered(dev))\r\nrpm_sysfs_remove(dev);\r\n}\r\nvoid pm_runtime_irq_safe(struct device *dev)\r\n{\r\nif (dev->parent)\r\npm_runtime_get_sync(dev->parent);\r\nspin_lock_irq(&dev->power.lock);\r\ndev->power.irq_safe = 1;\r\nspin_unlock_irq(&dev->power.lock);\r\n}\r\nstatic void update_autosuspend(struct device *dev, int old_delay, int old_use)\r\n{\r\nint delay = dev->power.autosuspend_delay;\r\nif (dev->power.use_autosuspend && delay < 0) {\r\nif (!old_use || old_delay >= 0) {\r\natomic_inc(&dev->power.usage_count);\r\nrpm_resume(dev, 0);\r\n}\r\n}\r\nelse {\r\nif (old_use && old_delay < 0)\r\natomic_dec(&dev->power.usage_count);\r\nrpm_idle(dev, RPM_AUTO);\r\n}\r\n}\r\nvoid pm_runtime_set_autosuspend_delay(struct device *dev, int delay)\r\n{\r\nint old_delay, old_use;\r\nspin_lock_irq(&dev->power.lock);\r\nold_delay = dev->power.autosuspend_delay;\r\nold_use = dev->power.use_autosuspend;\r\ndev->power.autosuspend_delay = delay;\r\nupdate_autosuspend(dev, old_delay, old_use);\r\nspin_unlock_irq(&dev->power.lock);\r\n}\r\nvoid __pm_runtime_use_autosuspend(struct device *dev, bool use)\r\n{\r\nint old_delay, old_use;\r\nspin_lock_irq(&dev->power.lock);\r\nold_delay = dev->power.autosuspend_delay;\r\nold_use = dev->power.use_autosuspend;\r\ndev->power.use_autosuspend = use;\r\nupdate_autosuspend(dev, old_delay, old_use);\r\nspin_unlock_irq(&dev->power.lock);\r\n}\r\nvoid pm_runtime_init(struct device *dev)\r\n{\r\ndev->power.runtime_status = RPM_SUSPENDED;\r\ndev->power.idle_notification = false;\r\ndev->power.disable_depth = 1;\r\natomic_set(&dev->power.usage_count, 0);\r\ndev->power.runtime_error = 0;\r\natomic_set(&dev->power.child_count, 0);\r\npm_suspend_ignore_children(dev, false);\r\ndev->power.runtime_auto = true;\r\ndev->power.request_pending = false;\r\ndev->power.request = RPM_REQ_NONE;\r\ndev->power.deferred_resume = false;\r\ndev->power.accounting_timestamp = jiffies;\r\nINIT_WORK(&dev->power.work, pm_runtime_work);\r\ndev->power.timer_expires = 0;\r\nsetup_timer(&dev->power.suspend_timer, pm_suspend_timer_fn,\r\n(unsigned long)dev);\r\ninit_waitqueue_head(&dev->power.wait_queue);\r\n}\r\nvoid pm_runtime_reinit(struct device *dev)\r\n{\r\nif (!pm_runtime_enabled(dev)) {\r\nif (dev->power.runtime_status == RPM_ACTIVE)\r\npm_runtime_set_suspended(dev);\r\nif (dev->power.irq_safe) {\r\nspin_lock_irq(&dev->power.lock);\r\ndev->power.irq_safe = 0;\r\nspin_unlock_irq(&dev->power.lock);\r\nif (dev->parent)\r\npm_runtime_put(dev->parent);\r\n}\r\n}\r\n}\r\nvoid pm_runtime_remove(struct device *dev)\r\n{\r\n__pm_runtime_disable(dev, false);\r\npm_runtime_reinit(dev);\r\n}\r\nint pm_runtime_force_suspend(struct device *dev)\r\n{\r\nint (*callback)(struct device *);\r\nint ret = 0;\r\npm_runtime_disable(dev);\r\nif (pm_runtime_status_suspended(dev))\r\nreturn 0;\r\ncallback = RPM_GET_CALLBACK(dev, runtime_suspend);\r\nif (!callback) {\r\nret = -ENOSYS;\r\ngoto err;\r\n}\r\nret = callback(dev);\r\nif (ret)\r\ngoto err;\r\npm_runtime_set_suspended(dev);\r\nreturn 0;\r\nerr:\r\npm_runtime_enable(dev);\r\nreturn ret;\r\n}\r\nint pm_runtime_force_resume(struct device *dev)\r\n{\r\nint (*callback)(struct device *);\r\nint ret = 0;\r\ncallback = RPM_GET_CALLBACK(dev, runtime_resume);\r\nif (!callback) {\r\nret = -ENOSYS;\r\ngoto out;\r\n}\r\nret = callback(dev);\r\nif (ret)\r\ngoto out;\r\npm_runtime_set_active(dev);\r\npm_runtime_mark_last_busy(dev);\r\nout:\r\npm_runtime_enable(dev);\r\nreturn ret;\r\n}
