static u32 head_hashfn(struct rhashtable *ht,\r\nconst struct bucket_table *tbl,\r\nconst struct rhash_head *he)\r\n{\r\nreturn rht_head_hashfn(ht, tbl, he, ht->p);\r\n}\r\nint lockdep_rht_mutex_is_held(struct rhashtable *ht)\r\n{\r\nreturn (debug_locks) ? lockdep_is_held(&ht->mutex) : 1;\r\n}\r\nint lockdep_rht_bucket_is_held(const struct bucket_table *tbl, u32 hash)\r\n{\r\nspinlock_t *lock = rht_bucket_lock(tbl, hash);\r\nreturn (debug_locks) ? lockdep_is_held(lock) : 1;\r\n}\r\nstatic int alloc_bucket_locks(struct rhashtable *ht, struct bucket_table *tbl,\r\ngfp_t gfp)\r\n{\r\nunsigned int i, size;\r\n#if defined(CONFIG_PROVE_LOCKING)\r\nunsigned int nr_pcpus = 2;\r\n#else\r\nunsigned int nr_pcpus = num_possible_cpus();\r\n#endif\r\nnr_pcpus = min_t(unsigned int, nr_pcpus, 32UL);\r\nsize = roundup_pow_of_two(nr_pcpus * ht->p.locks_mul);\r\nsize = min_t(unsigned int, size, tbl->size >> 1);\r\nif (sizeof(spinlock_t) != 0) {\r\n#ifdef CONFIG_NUMA\r\nif (size * sizeof(spinlock_t) > PAGE_SIZE &&\r\ngfp == GFP_KERNEL)\r\ntbl->locks = vmalloc(size * sizeof(spinlock_t));\r\nelse\r\n#endif\r\ntbl->locks = kmalloc_array(size, sizeof(spinlock_t),\r\ngfp);\r\nif (!tbl->locks)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < size; i++)\r\nspin_lock_init(&tbl->locks[i]);\r\n}\r\ntbl->locks_mask = size - 1;\r\nreturn 0;\r\n}\r\nstatic void bucket_table_free(const struct bucket_table *tbl)\r\n{\r\nif (tbl)\r\nkvfree(tbl->locks);\r\nkvfree(tbl);\r\n}\r\nstatic void bucket_table_free_rcu(struct rcu_head *head)\r\n{\r\nbucket_table_free(container_of(head, struct bucket_table, rcu));\r\n}\r\nstatic struct bucket_table *bucket_table_alloc(struct rhashtable *ht,\r\nsize_t nbuckets,\r\ngfp_t gfp)\r\n{\r\nstruct bucket_table *tbl = NULL;\r\nsize_t size;\r\nint i;\r\nsize = sizeof(*tbl) + nbuckets * sizeof(tbl->buckets[0]);\r\nif (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER) ||\r\ngfp != GFP_KERNEL)\r\ntbl = kzalloc(size, gfp | __GFP_NOWARN | __GFP_NORETRY);\r\nif (tbl == NULL && gfp == GFP_KERNEL)\r\ntbl = vzalloc(size);\r\nif (tbl == NULL)\r\nreturn NULL;\r\ntbl->size = nbuckets;\r\nif (alloc_bucket_locks(ht, tbl, gfp) < 0) {\r\nbucket_table_free(tbl);\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&tbl->walkers);\r\nget_random_bytes(&tbl->hash_rnd, sizeof(tbl->hash_rnd));\r\nfor (i = 0; i < nbuckets; i++)\r\nINIT_RHT_NULLS_HEAD(tbl->buckets[i], ht, i);\r\nreturn tbl;\r\n}\r\nstatic struct bucket_table *rhashtable_last_table(struct rhashtable *ht,\r\nstruct bucket_table *tbl)\r\n{\r\nstruct bucket_table *new_tbl;\r\ndo {\r\nnew_tbl = tbl;\r\ntbl = rht_dereference_rcu(tbl->future_tbl, ht);\r\n} while (tbl);\r\nreturn new_tbl;\r\n}\r\nstatic int rhashtable_rehash_one(struct rhashtable *ht, unsigned int old_hash)\r\n{\r\nstruct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);\r\nstruct bucket_table *new_tbl = rhashtable_last_table(ht,\r\nrht_dereference_rcu(old_tbl->future_tbl, ht));\r\nstruct rhash_head __rcu **pprev = &old_tbl->buckets[old_hash];\r\nint err = -ENOENT;\r\nstruct rhash_head *head, *next, *entry;\r\nspinlock_t *new_bucket_lock;\r\nunsigned int new_hash;\r\nrht_for_each(entry, old_tbl, old_hash) {\r\nerr = 0;\r\nnext = rht_dereference_bucket(entry->next, old_tbl, old_hash);\r\nif (rht_is_a_nulls(next))\r\nbreak;\r\npprev = &entry->next;\r\n}\r\nif (err)\r\ngoto out;\r\nnew_hash = head_hashfn(ht, new_tbl, entry);\r\nnew_bucket_lock = rht_bucket_lock(new_tbl, new_hash);\r\nspin_lock_nested(new_bucket_lock, SINGLE_DEPTH_NESTING);\r\nhead = rht_dereference_bucket(new_tbl->buckets[new_hash],\r\nnew_tbl, new_hash);\r\nRCU_INIT_POINTER(entry->next, head);\r\nrcu_assign_pointer(new_tbl->buckets[new_hash], entry);\r\nspin_unlock(new_bucket_lock);\r\nrcu_assign_pointer(*pprev, next);\r\nout:\r\nreturn err;\r\n}\r\nstatic void rhashtable_rehash_chain(struct rhashtable *ht,\r\nunsigned int old_hash)\r\n{\r\nstruct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);\r\nspinlock_t *old_bucket_lock;\r\nold_bucket_lock = rht_bucket_lock(old_tbl, old_hash);\r\nspin_lock_bh(old_bucket_lock);\r\nwhile (!rhashtable_rehash_one(ht, old_hash))\r\n;\r\nold_tbl->rehash++;\r\nspin_unlock_bh(old_bucket_lock);\r\n}\r\nstatic int rhashtable_rehash_attach(struct rhashtable *ht,\r\nstruct bucket_table *old_tbl,\r\nstruct bucket_table *new_tbl)\r\n{\r\nspin_lock_bh(old_tbl->locks);\r\nif (rcu_access_pointer(old_tbl->future_tbl)) {\r\nspin_unlock_bh(old_tbl->locks);\r\nreturn -EEXIST;\r\n}\r\nrcu_assign_pointer(old_tbl->future_tbl, new_tbl);\r\nspin_unlock_bh(old_tbl->locks);\r\nreturn 0;\r\n}\r\nstatic int rhashtable_rehash_table(struct rhashtable *ht)\r\n{\r\nstruct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);\r\nstruct bucket_table *new_tbl;\r\nstruct rhashtable_walker *walker;\r\nunsigned int old_hash;\r\nnew_tbl = rht_dereference(old_tbl->future_tbl, ht);\r\nif (!new_tbl)\r\nreturn 0;\r\nfor (old_hash = 0; old_hash < old_tbl->size; old_hash++)\r\nrhashtable_rehash_chain(ht, old_hash);\r\nrcu_assign_pointer(ht->tbl, new_tbl);\r\nspin_lock(&ht->lock);\r\nlist_for_each_entry(walker, &old_tbl->walkers, list)\r\nwalker->tbl = NULL;\r\nspin_unlock(&ht->lock);\r\ncall_rcu(&old_tbl->rcu, bucket_table_free_rcu);\r\nreturn rht_dereference(new_tbl->future_tbl, ht) ? -EAGAIN : 0;\r\n}\r\nstatic int rhashtable_expand(struct rhashtable *ht)\r\n{\r\nstruct bucket_table *new_tbl, *old_tbl = rht_dereference(ht->tbl, ht);\r\nint err;\r\nASSERT_RHT_MUTEX(ht);\r\nold_tbl = rhashtable_last_table(ht, old_tbl);\r\nnew_tbl = bucket_table_alloc(ht, old_tbl->size * 2, GFP_KERNEL);\r\nif (new_tbl == NULL)\r\nreturn -ENOMEM;\r\nerr = rhashtable_rehash_attach(ht, old_tbl, new_tbl);\r\nif (err)\r\nbucket_table_free(new_tbl);\r\nreturn err;\r\n}\r\nstatic int rhashtable_shrink(struct rhashtable *ht)\r\n{\r\nstruct bucket_table *new_tbl, *old_tbl = rht_dereference(ht->tbl, ht);\r\nunsigned int size;\r\nint err;\r\nASSERT_RHT_MUTEX(ht);\r\nsize = roundup_pow_of_two(atomic_read(&ht->nelems) * 3 / 2);\r\nif (size < ht->p.min_size)\r\nsize = ht->p.min_size;\r\nif (old_tbl->size <= size)\r\nreturn 0;\r\nif (rht_dereference(old_tbl->future_tbl, ht))\r\nreturn -EEXIST;\r\nnew_tbl = bucket_table_alloc(ht, size, GFP_KERNEL);\r\nif (new_tbl == NULL)\r\nreturn -ENOMEM;\r\nerr = rhashtable_rehash_attach(ht, old_tbl, new_tbl);\r\nif (err)\r\nbucket_table_free(new_tbl);\r\nreturn err;\r\n}\r\nstatic void rht_deferred_worker(struct work_struct *work)\r\n{\r\nstruct rhashtable *ht;\r\nstruct bucket_table *tbl;\r\nint err = 0;\r\nht = container_of(work, struct rhashtable, run_work);\r\nmutex_lock(&ht->mutex);\r\ntbl = rht_dereference(ht->tbl, ht);\r\ntbl = rhashtable_last_table(ht, tbl);\r\nif (rht_grow_above_75(ht, tbl))\r\nrhashtable_expand(ht);\r\nelse if (ht->p.automatic_shrinking && rht_shrink_below_30(ht, tbl))\r\nrhashtable_shrink(ht);\r\nerr = rhashtable_rehash_table(ht);\r\nmutex_unlock(&ht->mutex);\r\nif (err)\r\nschedule_work(&ht->run_work);\r\n}\r\nstatic bool rhashtable_check_elasticity(struct rhashtable *ht,\r\nstruct bucket_table *tbl,\r\nunsigned int hash)\r\n{\r\nunsigned int elasticity = ht->elasticity;\r\nstruct rhash_head *head;\r\nrht_for_each(head, tbl, hash)\r\nif (!--elasticity)\r\nreturn true;\r\nreturn false;\r\n}\r\nint rhashtable_insert_rehash(struct rhashtable *ht,\r\nstruct bucket_table *tbl)\r\n{\r\nstruct bucket_table *old_tbl;\r\nstruct bucket_table *new_tbl;\r\nunsigned int size;\r\nint err;\r\nold_tbl = rht_dereference_rcu(ht->tbl, ht);\r\nsize = tbl->size;\r\nerr = -EBUSY;\r\nif (rht_grow_above_75(ht, tbl))\r\nsize *= 2;\r\nelse if (old_tbl != tbl)\r\ngoto fail;\r\nerr = -ENOMEM;\r\nnew_tbl = bucket_table_alloc(ht, size, GFP_ATOMIC);\r\nif (new_tbl == NULL)\r\ngoto fail;\r\nerr = rhashtable_rehash_attach(ht, tbl, new_tbl);\r\nif (err) {\r\nbucket_table_free(new_tbl);\r\nif (err == -EEXIST)\r\nerr = 0;\r\n} else\r\nschedule_work(&ht->run_work);\r\nreturn err;\r\nfail:\r\nif (likely(rcu_dereference_raw(tbl->future_tbl)))\r\nreturn 0;\r\nif (err == -ENOMEM)\r\nschedule_work(&ht->run_work);\r\nreturn err;\r\n}\r\nstruct bucket_table *rhashtable_insert_slow(struct rhashtable *ht,\r\nconst void *key,\r\nstruct rhash_head *obj,\r\nstruct bucket_table *tbl)\r\n{\r\nstruct rhash_head *head;\r\nunsigned int hash;\r\nint err;\r\ntbl = rhashtable_last_table(ht, tbl);\r\nhash = head_hashfn(ht, tbl, obj);\r\nspin_lock_nested(rht_bucket_lock(tbl, hash), SINGLE_DEPTH_NESTING);\r\nerr = -EEXIST;\r\nif (key && rhashtable_lookup_fast(ht, key, ht->p))\r\ngoto exit;\r\nerr = -E2BIG;\r\nif (unlikely(rht_grow_above_max(ht, tbl)))\r\ngoto exit;\r\nerr = -EAGAIN;\r\nif (rhashtable_check_elasticity(ht, tbl, hash) ||\r\nrht_grow_above_100(ht, tbl))\r\ngoto exit;\r\nerr = 0;\r\nhead = rht_dereference_bucket(tbl->buckets[hash], tbl, hash);\r\nRCU_INIT_POINTER(obj->next, head);\r\nrcu_assign_pointer(tbl->buckets[hash], obj);\r\natomic_inc(&ht->nelems);\r\nexit:\r\nspin_unlock(rht_bucket_lock(tbl, hash));\r\nif (err == 0)\r\nreturn NULL;\r\nelse if (err == -EAGAIN)\r\nreturn tbl;\r\nelse\r\nreturn ERR_PTR(err);\r\n}\r\nint rhashtable_walk_init(struct rhashtable *ht, struct rhashtable_iter *iter)\r\n{\r\niter->ht = ht;\r\niter->p = NULL;\r\niter->slot = 0;\r\niter->skip = 0;\r\niter->walker = kmalloc(sizeof(*iter->walker), GFP_KERNEL);\r\nif (!iter->walker)\r\nreturn -ENOMEM;\r\nspin_lock(&ht->lock);\r\niter->walker->tbl =\r\nrcu_dereference_protected(ht->tbl, lockdep_is_held(&ht->lock));\r\nlist_add(&iter->walker->list, &iter->walker->tbl->walkers);\r\nspin_unlock(&ht->lock);\r\nreturn 0;\r\n}\r\nvoid rhashtable_walk_exit(struct rhashtable_iter *iter)\r\n{\r\nspin_lock(&iter->ht->lock);\r\nif (iter->walker->tbl)\r\nlist_del(&iter->walker->list);\r\nspin_unlock(&iter->ht->lock);\r\nkfree(iter->walker);\r\n}\r\nint rhashtable_walk_start(struct rhashtable_iter *iter)\r\n__acquires(RCU)\r\n{\r\nstruct rhashtable *ht = iter->ht;\r\nrcu_read_lock();\r\nspin_lock(&ht->lock);\r\nif (iter->walker->tbl)\r\nlist_del(&iter->walker->list);\r\nspin_unlock(&ht->lock);\r\nif (!iter->walker->tbl) {\r\niter->walker->tbl = rht_dereference_rcu(ht->tbl, ht);\r\nreturn -EAGAIN;\r\n}\r\nreturn 0;\r\n}\r\nvoid *rhashtable_walk_next(struct rhashtable_iter *iter)\r\n{\r\nstruct bucket_table *tbl = iter->walker->tbl;\r\nstruct rhashtable *ht = iter->ht;\r\nstruct rhash_head *p = iter->p;\r\nif (p) {\r\np = rht_dereference_bucket_rcu(p->next, tbl, iter->slot);\r\ngoto next;\r\n}\r\nfor (; iter->slot < tbl->size; iter->slot++) {\r\nint skip = iter->skip;\r\nrht_for_each_rcu(p, tbl, iter->slot) {\r\nif (!skip)\r\nbreak;\r\nskip--;\r\n}\r\nnext:\r\nif (!rht_is_a_nulls(p)) {\r\niter->skip++;\r\niter->p = p;\r\nreturn rht_obj(ht, p);\r\n}\r\niter->skip = 0;\r\n}\r\niter->p = NULL;\r\nsmp_rmb();\r\niter->walker->tbl = rht_dereference_rcu(tbl->future_tbl, ht);\r\nif (iter->walker->tbl) {\r\niter->slot = 0;\r\niter->skip = 0;\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nreturn NULL;\r\n}\r\nvoid rhashtable_walk_stop(struct rhashtable_iter *iter)\r\n__releases(RCU)\r\n{\r\nstruct rhashtable *ht;\r\nstruct bucket_table *tbl = iter->walker->tbl;\r\nif (!tbl)\r\ngoto out;\r\nht = iter->ht;\r\nspin_lock(&ht->lock);\r\nif (tbl->rehash < tbl->size)\r\nlist_add(&iter->walker->list, &tbl->walkers);\r\nelse\r\niter->walker->tbl = NULL;\r\nspin_unlock(&ht->lock);\r\niter->p = NULL;\r\nout:\r\nrcu_read_unlock();\r\n}\r\nstatic size_t rounded_hashtable_size(const struct rhashtable_params *params)\r\n{\r\nreturn max(roundup_pow_of_two(params->nelem_hint * 4 / 3),\r\n(unsigned long)params->min_size);\r\n}\r\nstatic u32 rhashtable_jhash2(const void *key, u32 length, u32 seed)\r\n{\r\nreturn jhash2(key, length, seed);\r\n}\r\nint rhashtable_init(struct rhashtable *ht,\r\nconst struct rhashtable_params *params)\r\n{\r\nstruct bucket_table *tbl;\r\nsize_t size;\r\nsize = HASH_DEFAULT_SIZE;\r\nif ((!params->key_len && !params->obj_hashfn) ||\r\n(params->obj_hashfn && !params->obj_cmpfn))\r\nreturn -EINVAL;\r\nif (params->nulls_base && params->nulls_base < (1U << RHT_BASE_SHIFT))\r\nreturn -EINVAL;\r\nmemset(ht, 0, sizeof(*ht));\r\nmutex_init(&ht->mutex);\r\nspin_lock_init(&ht->lock);\r\nmemcpy(&ht->p, params, sizeof(*params));\r\nif (params->min_size)\r\nht->p.min_size = roundup_pow_of_two(params->min_size);\r\nif (params->max_size)\r\nht->p.max_size = rounddown_pow_of_two(params->max_size);\r\nif (params->insecure_max_entries)\r\nht->p.insecure_max_entries =\r\nrounddown_pow_of_two(params->insecure_max_entries);\r\nelse\r\nht->p.insecure_max_entries = ht->p.max_size * 2;\r\nht->p.min_size = max(ht->p.min_size, HASH_MIN_SIZE);\r\nif (params->nelem_hint)\r\nsize = rounded_hashtable_size(&ht->p);\r\nif (!params->insecure_elasticity)\r\nht->elasticity = 16;\r\nif (params->locks_mul)\r\nht->p.locks_mul = roundup_pow_of_two(params->locks_mul);\r\nelse\r\nht->p.locks_mul = BUCKET_LOCKS_PER_CPU;\r\nht->key_len = ht->p.key_len;\r\nif (!params->hashfn) {\r\nht->p.hashfn = jhash;\r\nif (!(ht->key_len & (sizeof(u32) - 1))) {\r\nht->key_len /= sizeof(u32);\r\nht->p.hashfn = rhashtable_jhash2;\r\n}\r\n}\r\ntbl = bucket_table_alloc(ht, size, GFP_KERNEL);\r\nif (tbl == NULL)\r\nreturn -ENOMEM;\r\natomic_set(&ht->nelems, 0);\r\nRCU_INIT_POINTER(ht->tbl, tbl);\r\nINIT_WORK(&ht->run_work, rht_deferred_worker);\r\nreturn 0;\r\n}\r\nvoid rhashtable_free_and_destroy(struct rhashtable *ht,\r\nvoid (*free_fn)(void *ptr, void *arg),\r\nvoid *arg)\r\n{\r\nconst struct bucket_table *tbl;\r\nunsigned int i;\r\ncancel_work_sync(&ht->run_work);\r\nmutex_lock(&ht->mutex);\r\ntbl = rht_dereference(ht->tbl, ht);\r\nif (free_fn) {\r\nfor (i = 0; i < tbl->size; i++) {\r\nstruct rhash_head *pos, *next;\r\nfor (pos = rht_dereference(tbl->buckets[i], ht),\r\nnext = !rht_is_a_nulls(pos) ?\r\nrht_dereference(pos->next, ht) : NULL;\r\n!rht_is_a_nulls(pos);\r\npos = next,\r\nnext = !rht_is_a_nulls(pos) ?\r\nrht_dereference(pos->next, ht) : NULL)\r\nfree_fn(rht_obj(ht, pos), arg);\r\n}\r\n}\r\nbucket_table_free(tbl);\r\nmutex_unlock(&ht->mutex);\r\n}\r\nvoid rhashtable_destroy(struct rhashtable *ht)\r\n{\r\nreturn rhashtable_free_and_destroy(ht, NULL, NULL);\r\n}
