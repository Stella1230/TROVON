static struct mlx5_core_rsc_common *mlx5_get_rsc(struct mlx5_core_dev *dev,\r\nu32 rsn)\r\n{\r\nstruct mlx5_qp_table *table = &dev->priv.qp_table;\r\nstruct mlx5_core_rsc_common *common;\r\nspin_lock(&table->lock);\r\ncommon = radix_tree_lookup(&table->tree, rsn);\r\nif (common)\r\natomic_inc(&common->refcount);\r\nspin_unlock(&table->lock);\r\nif (!common) {\r\nmlx5_core_warn(dev, "Async event for bogus resource 0x%x\n",\r\nrsn);\r\nreturn NULL;\r\n}\r\nreturn common;\r\n}\r\nvoid mlx5_core_put_rsc(struct mlx5_core_rsc_common *common)\r\n{\r\nif (atomic_dec_and_test(&common->refcount))\r\ncomplete(&common->free);\r\n}\r\nstatic u64 qp_allowed_event_types(void)\r\n{\r\nu64 mask;\r\nmask = BIT(MLX5_EVENT_TYPE_PATH_MIG) |\r\nBIT(MLX5_EVENT_TYPE_COMM_EST) |\r\nBIT(MLX5_EVENT_TYPE_SQ_DRAINED) |\r\nBIT(MLX5_EVENT_TYPE_SRQ_LAST_WQE) |\r\nBIT(MLX5_EVENT_TYPE_WQ_CATAS_ERROR) |\r\nBIT(MLX5_EVENT_TYPE_PATH_MIG_FAILED) |\r\nBIT(MLX5_EVENT_TYPE_WQ_INVAL_REQ_ERROR) |\r\nBIT(MLX5_EVENT_TYPE_WQ_ACCESS_ERROR);\r\nreturn mask;\r\n}\r\nstatic u64 rq_allowed_event_types(void)\r\n{\r\nu64 mask;\r\nmask = BIT(MLX5_EVENT_TYPE_SRQ_LAST_WQE) |\r\nBIT(MLX5_EVENT_TYPE_WQ_CATAS_ERROR);\r\nreturn mask;\r\n}\r\nstatic u64 sq_allowed_event_types(void)\r\n{\r\nreturn BIT(MLX5_EVENT_TYPE_WQ_CATAS_ERROR);\r\n}\r\nstatic bool is_event_type_allowed(int rsc_type, int event_type)\r\n{\r\nswitch (rsc_type) {\r\ncase MLX5_EVENT_QUEUE_TYPE_QP:\r\nreturn BIT(event_type) & qp_allowed_event_types();\r\ncase MLX5_EVENT_QUEUE_TYPE_RQ:\r\nreturn BIT(event_type) & rq_allowed_event_types();\r\ncase MLX5_EVENT_QUEUE_TYPE_SQ:\r\nreturn BIT(event_type) & sq_allowed_event_types();\r\ndefault:\r\nWARN(1, "Event arrived for unknown resource type");\r\nreturn false;\r\n}\r\n}\r\nvoid mlx5_rsc_event(struct mlx5_core_dev *dev, u32 rsn, int event_type)\r\n{\r\nstruct mlx5_core_rsc_common *common = mlx5_get_rsc(dev, rsn);\r\nstruct mlx5_core_qp *qp;\r\nif (!common)\r\nreturn;\r\nif (!is_event_type_allowed((rsn >> MLX5_USER_INDEX_LEN), event_type)) {\r\nmlx5_core_warn(dev, "event 0x%.2x is not allowed on resource 0x%.8x\n",\r\nevent_type, rsn);\r\nreturn;\r\n}\r\nswitch (common->res) {\r\ncase MLX5_RES_QP:\r\ncase MLX5_RES_RQ:\r\ncase MLX5_RES_SQ:\r\nqp = (struct mlx5_core_qp *)common;\r\nqp->event(qp, event_type);\r\nbreak;\r\ndefault:\r\nmlx5_core_warn(dev, "invalid resource type for 0x%x\n", rsn);\r\n}\r\nmlx5_core_put_rsc(common);\r\n}\r\nvoid mlx5_eq_pagefault(struct mlx5_core_dev *dev, struct mlx5_eqe *eqe)\r\n{\r\nstruct mlx5_eqe_page_fault *pf_eqe = &eqe->data.page_fault;\r\nint qpn = be32_to_cpu(pf_eqe->flags_qpn) & MLX5_QPN_MASK;\r\nstruct mlx5_core_rsc_common *common = mlx5_get_rsc(dev, qpn);\r\nstruct mlx5_core_qp *qp =\r\ncontainer_of(common, struct mlx5_core_qp, common);\r\nstruct mlx5_pagefault pfault;\r\nif (!qp) {\r\nmlx5_core_warn(dev, "ODP event for non-existent QP %06x\n",\r\nqpn);\r\nreturn;\r\n}\r\npfault.event_subtype = eqe->sub_type;\r\npfault.flags = (be32_to_cpu(pf_eqe->flags_qpn) >> MLX5_QPN_BITS) &\r\n(MLX5_PFAULT_REQUESTOR | MLX5_PFAULT_WRITE | MLX5_PFAULT_RDMA);\r\npfault.bytes_committed = be32_to_cpu(\r\npf_eqe->bytes_committed);\r\nmlx5_core_dbg(dev,\r\n"PAGE_FAULT: subtype: 0x%02x, flags: 0x%02x,\n",\r\neqe->sub_type, pfault.flags);\r\nswitch (eqe->sub_type) {\r\ncase MLX5_PFAULT_SUBTYPE_RDMA:\r\npfault.rdma.r_key =\r\nbe32_to_cpu(pf_eqe->rdma.r_key);\r\npfault.rdma.packet_size =\r\nbe16_to_cpu(pf_eqe->rdma.packet_length);\r\npfault.rdma.rdma_op_len =\r\nbe32_to_cpu(pf_eqe->rdma.rdma_op_len);\r\npfault.rdma.rdma_va =\r\nbe64_to_cpu(pf_eqe->rdma.rdma_va);\r\nmlx5_core_dbg(dev,\r\n"PAGE_FAULT: qpn: 0x%06x, r_key: 0x%08x,\n",\r\nqpn, pfault.rdma.r_key);\r\nmlx5_core_dbg(dev,\r\n"PAGE_FAULT: rdma_op_len: 0x%08x,\n",\r\npfault.rdma.rdma_op_len);\r\nmlx5_core_dbg(dev,\r\n"PAGE_FAULT: rdma_va: 0x%016llx,\n",\r\npfault.rdma.rdma_va);\r\nmlx5_core_dbg(dev,\r\n"PAGE_FAULT: bytes_committed: 0x%06x\n",\r\npfault.bytes_committed);\r\nbreak;\r\ncase MLX5_PFAULT_SUBTYPE_WQE:\r\npfault.wqe.wqe_index =\r\nbe16_to_cpu(pf_eqe->wqe.wqe_index);\r\npfault.wqe.packet_size =\r\nbe16_to_cpu(pf_eqe->wqe.packet_length);\r\nmlx5_core_dbg(dev,\r\n"PAGE_FAULT: qpn: 0x%06x, wqe_index: 0x%04x,\n",\r\nqpn, pfault.wqe.wqe_index);\r\nmlx5_core_dbg(dev,\r\n"PAGE_FAULT: bytes_committed: 0x%06x\n",\r\npfault.bytes_committed);\r\nbreak;\r\ndefault:\r\nmlx5_core_warn(dev,\r\n"Unsupported page fault event sub-type: 0x%02hhx, QP %06x\n",\r\neqe->sub_type, qpn);\r\n}\r\nif (qp->pfault_handler) {\r\nqp->pfault_handler(qp, &pfault);\r\n} else {\r\nmlx5_core_err(dev,\r\n"ODP event for QP %08x, without a fault handler in QP\n",\r\nqpn);\r\n}\r\nmlx5_core_put_rsc(common);\r\n}\r\nstatic int create_qprqsq_common(struct mlx5_core_dev *dev,\r\nstruct mlx5_core_qp *qp,\r\nint rsc_type)\r\n{\r\nstruct mlx5_qp_table *table = &dev->priv.qp_table;\r\nint err;\r\nqp->common.res = rsc_type;\r\nspin_lock_irq(&table->lock);\r\nerr = radix_tree_insert(&table->tree,\r\nqp->qpn | (rsc_type << MLX5_USER_INDEX_LEN),\r\nqp);\r\nspin_unlock_irq(&table->lock);\r\nif (err)\r\nreturn err;\r\natomic_set(&qp->common.refcount, 1);\r\ninit_completion(&qp->common.free);\r\nqp->pid = current->pid;\r\nreturn 0;\r\n}\r\nstatic void destroy_qprqsq_common(struct mlx5_core_dev *dev,\r\nstruct mlx5_core_qp *qp)\r\n{\r\nstruct mlx5_qp_table *table = &dev->priv.qp_table;\r\nunsigned long flags;\r\nspin_lock_irqsave(&table->lock, flags);\r\nradix_tree_delete(&table->tree,\r\nqp->qpn | (qp->common.res << MLX5_USER_INDEX_LEN));\r\nspin_unlock_irqrestore(&table->lock, flags);\r\nmlx5_core_put_rsc((struct mlx5_core_rsc_common *)qp);\r\nwait_for_completion(&qp->common.free);\r\n}\r\nint mlx5_core_create_qp(struct mlx5_core_dev *dev,\r\nstruct mlx5_core_qp *qp,\r\nstruct mlx5_create_qp_mbox_in *in,\r\nint inlen)\r\n{\r\nstruct mlx5_create_qp_mbox_out out;\r\nstruct mlx5_destroy_qp_mbox_in din;\r\nstruct mlx5_destroy_qp_mbox_out dout;\r\nint err;\r\nmemset(&out, 0, sizeof(out));\r\nin->hdr.opcode = cpu_to_be16(MLX5_CMD_OP_CREATE_QP);\r\nerr = mlx5_cmd_exec(dev, in, inlen, &out, sizeof(out));\r\nif (err) {\r\nmlx5_core_warn(dev, "ret %d\n", err);\r\nreturn err;\r\n}\r\nif (out.hdr.status) {\r\nmlx5_core_warn(dev, "current num of QPs 0x%x\n",\r\natomic_read(&dev->num_qps));\r\nreturn mlx5_cmd_status_to_err(&out.hdr);\r\n}\r\nqp->qpn = be32_to_cpu(out.qpn) & 0xffffff;\r\nmlx5_core_dbg(dev, "qpn = 0x%x\n", qp->qpn);\r\nerr = create_qprqsq_common(dev, qp, MLX5_RES_QP);\r\nif (err)\r\ngoto err_cmd;\r\nerr = mlx5_debug_qp_add(dev, qp);\r\nif (err)\r\nmlx5_core_dbg(dev, "failed adding QP 0x%x to debug file system\n",\r\nqp->qpn);\r\natomic_inc(&dev->num_qps);\r\nreturn 0;\r\nerr_cmd:\r\nmemset(&din, 0, sizeof(din));\r\nmemset(&dout, 0, sizeof(dout));\r\ndin.hdr.opcode = cpu_to_be16(MLX5_CMD_OP_DESTROY_QP);\r\ndin.qpn = cpu_to_be32(qp->qpn);\r\nmlx5_cmd_exec(dev, &din, sizeof(din), &out, sizeof(dout));\r\nreturn err;\r\n}\r\nint mlx5_core_destroy_qp(struct mlx5_core_dev *dev,\r\nstruct mlx5_core_qp *qp)\r\n{\r\nstruct mlx5_destroy_qp_mbox_in in;\r\nstruct mlx5_destroy_qp_mbox_out out;\r\nint err;\r\nmlx5_debug_qp_remove(dev, qp);\r\ndestroy_qprqsq_common(dev, qp);\r\nmemset(&in, 0, sizeof(in));\r\nmemset(&out, 0, sizeof(out));\r\nin.hdr.opcode = cpu_to_be16(MLX5_CMD_OP_DESTROY_QP);\r\nin.qpn = cpu_to_be32(qp->qpn);\r\nerr = mlx5_cmd_exec(dev, &in, sizeof(in), &out, sizeof(out));\r\nif (err)\r\nreturn err;\r\nif (out.hdr.status)\r\nreturn mlx5_cmd_status_to_err(&out.hdr);\r\natomic_dec(&dev->num_qps);\r\nreturn 0;\r\n}\r\nint mlx5_core_qp_modify(struct mlx5_core_dev *dev, u16 operation,\r\nstruct mlx5_modify_qp_mbox_in *in, int sqd_event,\r\nstruct mlx5_core_qp *qp)\r\n{\r\nstruct mlx5_modify_qp_mbox_out out;\r\nint err = 0;\r\nmemset(&out, 0, sizeof(out));\r\nin->hdr.opcode = cpu_to_be16(operation);\r\nin->qpn = cpu_to_be32(qp->qpn);\r\nerr = mlx5_cmd_exec(dev, in, sizeof(*in), &out, sizeof(out));\r\nif (err)\r\nreturn err;\r\nreturn mlx5_cmd_status_to_err(&out.hdr);\r\n}\r\nvoid mlx5_init_qp_table(struct mlx5_core_dev *dev)\r\n{\r\nstruct mlx5_qp_table *table = &dev->priv.qp_table;\r\nmemset(table, 0, sizeof(*table));\r\nspin_lock_init(&table->lock);\r\nINIT_RADIX_TREE(&table->tree, GFP_ATOMIC);\r\nmlx5_qp_debugfs_init(dev);\r\n}\r\nvoid mlx5_cleanup_qp_table(struct mlx5_core_dev *dev)\r\n{\r\nmlx5_qp_debugfs_cleanup(dev);\r\n}\r\nint mlx5_core_qp_query(struct mlx5_core_dev *dev, struct mlx5_core_qp *qp,\r\nstruct mlx5_query_qp_mbox_out *out, int outlen)\r\n{\r\nstruct mlx5_query_qp_mbox_in in;\r\nint err;\r\nmemset(&in, 0, sizeof(in));\r\nmemset(out, 0, outlen);\r\nin.hdr.opcode = cpu_to_be16(MLX5_CMD_OP_QUERY_QP);\r\nin.qpn = cpu_to_be32(qp->qpn);\r\nerr = mlx5_cmd_exec(dev, &in, sizeof(in), out, outlen);\r\nif (err)\r\nreturn err;\r\nif (out->hdr.status)\r\nreturn mlx5_cmd_status_to_err(&out->hdr);\r\nreturn err;\r\n}\r\nint mlx5_core_xrcd_alloc(struct mlx5_core_dev *dev, u32 *xrcdn)\r\n{\r\nstruct mlx5_alloc_xrcd_mbox_in in;\r\nstruct mlx5_alloc_xrcd_mbox_out out;\r\nint err;\r\nmemset(&in, 0, sizeof(in));\r\nmemset(&out, 0, sizeof(out));\r\nin.hdr.opcode = cpu_to_be16(MLX5_CMD_OP_ALLOC_XRCD);\r\nerr = mlx5_cmd_exec(dev, &in, sizeof(in), &out, sizeof(out));\r\nif (err)\r\nreturn err;\r\nif (out.hdr.status)\r\nerr = mlx5_cmd_status_to_err(&out.hdr);\r\nelse\r\n*xrcdn = be32_to_cpu(out.xrcdn);\r\nreturn err;\r\n}\r\nint mlx5_core_xrcd_dealloc(struct mlx5_core_dev *dev, u32 xrcdn)\r\n{\r\nstruct mlx5_dealloc_xrcd_mbox_in in;\r\nstruct mlx5_dealloc_xrcd_mbox_out out;\r\nint err;\r\nmemset(&in, 0, sizeof(in));\r\nmemset(&out, 0, sizeof(out));\r\nin.hdr.opcode = cpu_to_be16(MLX5_CMD_OP_DEALLOC_XRCD);\r\nin.xrcdn = cpu_to_be32(xrcdn);\r\nerr = mlx5_cmd_exec(dev, &in, sizeof(in), &out, sizeof(out));\r\nif (err)\r\nreturn err;\r\nif (out.hdr.status)\r\nerr = mlx5_cmd_status_to_err(&out.hdr);\r\nreturn err;\r\n}\r\nint mlx5_core_page_fault_resume(struct mlx5_core_dev *dev, u32 qpn,\r\nu8 flags, int error)\r\n{\r\nstruct mlx5_page_fault_resume_mbox_in in;\r\nstruct mlx5_page_fault_resume_mbox_out out;\r\nint err;\r\nmemset(&in, 0, sizeof(in));\r\nmemset(&out, 0, sizeof(out));\r\nin.hdr.opcode = cpu_to_be16(MLX5_CMD_OP_PAGE_FAULT_RESUME);\r\nin.hdr.opmod = 0;\r\nflags &= (MLX5_PAGE_FAULT_RESUME_REQUESTOR |\r\nMLX5_PAGE_FAULT_RESUME_WRITE |\r\nMLX5_PAGE_FAULT_RESUME_RDMA);\r\nflags |= (error ? MLX5_PAGE_FAULT_RESUME_ERROR : 0);\r\nin.flags_qpn = cpu_to_be32((qpn & MLX5_QPN_MASK) |\r\n(flags << MLX5_QPN_BITS));\r\nerr = mlx5_cmd_exec(dev, &in, sizeof(in), &out, sizeof(out));\r\nif (err)\r\nreturn err;\r\nif (out.hdr.status)\r\nerr = mlx5_cmd_status_to_err(&out.hdr);\r\nreturn err;\r\n}\r\nint mlx5_core_create_rq_tracked(struct mlx5_core_dev *dev, u32 *in, int inlen,\r\nstruct mlx5_core_qp *rq)\r\n{\r\nint err;\r\nu32 rqn;\r\nerr = mlx5_core_create_rq(dev, in, inlen, &rqn);\r\nif (err)\r\nreturn err;\r\nrq->qpn = rqn;\r\nerr = create_qprqsq_common(dev, rq, MLX5_RES_RQ);\r\nif (err)\r\ngoto err_destroy_rq;\r\nreturn 0;\r\nerr_destroy_rq:\r\nmlx5_core_destroy_rq(dev, rq->qpn);\r\nreturn err;\r\n}\r\nvoid mlx5_core_destroy_rq_tracked(struct mlx5_core_dev *dev,\r\nstruct mlx5_core_qp *rq)\r\n{\r\ndestroy_qprqsq_common(dev, rq);\r\nmlx5_core_destroy_rq(dev, rq->qpn);\r\n}\r\nint mlx5_core_create_sq_tracked(struct mlx5_core_dev *dev, u32 *in, int inlen,\r\nstruct mlx5_core_qp *sq)\r\n{\r\nint err;\r\nu32 sqn;\r\nerr = mlx5_core_create_sq(dev, in, inlen, &sqn);\r\nif (err)\r\nreturn err;\r\nsq->qpn = sqn;\r\nerr = create_qprqsq_common(dev, sq, MLX5_RES_SQ);\r\nif (err)\r\ngoto err_destroy_sq;\r\nreturn 0;\r\nerr_destroy_sq:\r\nmlx5_core_destroy_sq(dev, sq->qpn);\r\nreturn err;\r\n}\r\nvoid mlx5_core_destroy_sq_tracked(struct mlx5_core_dev *dev,\r\nstruct mlx5_core_qp *sq)\r\n{\r\ndestroy_qprqsq_common(dev, sq);\r\nmlx5_core_destroy_sq(dev, sq->qpn);\r\n}
