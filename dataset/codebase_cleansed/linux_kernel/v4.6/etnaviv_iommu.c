static struct etnaviv_iommu_domain *to_etnaviv_domain(struct iommu_domain *domain)\r\n{\r\nreturn container_of(domain, struct etnaviv_iommu_domain, domain);\r\n}\r\nstatic int pgtable_alloc(struct etnaviv_iommu_domain_pgtable *pgtable,\r\nsize_t size)\r\n{\r\npgtable->pgtable = dma_alloc_coherent(NULL, size, &pgtable->paddr, GFP_KERNEL);\r\nif (!pgtable->pgtable)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void pgtable_free(struct etnaviv_iommu_domain_pgtable *pgtable,\r\nsize_t size)\r\n{\r\ndma_free_coherent(NULL, size, pgtable->pgtable, pgtable->paddr);\r\n}\r\nstatic u32 pgtable_read(struct etnaviv_iommu_domain_pgtable *pgtable,\r\nunsigned long iova)\r\n{\r\nunsigned int index = (iova - GPU_MEM_START) / SZ_4K;\r\nphys_addr_t paddr;\r\npaddr = pgtable->pgtable[index];\r\nreturn paddr;\r\n}\r\nstatic void pgtable_write(struct etnaviv_iommu_domain_pgtable *pgtable,\r\nunsigned long iova, phys_addr_t paddr)\r\n{\r\nunsigned int index = (iova - GPU_MEM_START) / SZ_4K;\r\npgtable->pgtable[index] = paddr;\r\n}\r\nstatic int __etnaviv_iommu_init(struct etnaviv_iommu_domain *etnaviv_domain)\r\n{\r\nu32 *p;\r\nint ret, i;\r\netnaviv_domain->bad_page_cpu = dma_alloc_coherent(etnaviv_domain->dev,\r\nSZ_4K,\r\n&etnaviv_domain->bad_page_dma,\r\nGFP_KERNEL);\r\nif (!etnaviv_domain->bad_page_cpu)\r\nreturn -ENOMEM;\r\np = etnaviv_domain->bad_page_cpu;\r\nfor (i = 0; i < SZ_4K / 4; i++)\r\n*p++ = 0xdead55aa;\r\nret = pgtable_alloc(&etnaviv_domain->pgtable, PT_SIZE);\r\nif (ret < 0) {\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->bad_page_cpu,\r\netnaviv_domain->bad_page_dma);\r\nreturn ret;\r\n}\r\nfor (i = 0; i < PT_ENTRIES; i++)\r\netnaviv_domain->pgtable.pgtable[i] =\r\netnaviv_domain->bad_page_dma;\r\nspin_lock_init(&etnaviv_domain->map_lock);\r\nreturn 0;\r\n}\r\nstatic void etnaviv_domain_free(struct iommu_domain *domain)\r\n{\r\nstruct etnaviv_iommu_domain *etnaviv_domain = to_etnaviv_domain(domain);\r\npgtable_free(&etnaviv_domain->pgtable, PT_SIZE);\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->bad_page_cpu,\r\netnaviv_domain->bad_page_dma);\r\nkfree(etnaviv_domain);\r\n}\r\nstatic int etnaviv_iommuv1_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct etnaviv_iommu_domain *etnaviv_domain = to_etnaviv_domain(domain);\r\nif (size != SZ_4K)\r\nreturn -EINVAL;\r\nspin_lock(&etnaviv_domain->map_lock);\r\npgtable_write(&etnaviv_domain->pgtable, iova, paddr);\r\nspin_unlock(&etnaviv_domain->map_lock);\r\nreturn 0;\r\n}\r\nstatic size_t etnaviv_iommuv1_unmap(struct iommu_domain *domain,\r\nunsigned long iova, size_t size)\r\n{\r\nstruct etnaviv_iommu_domain *etnaviv_domain = to_etnaviv_domain(domain);\r\nif (size != SZ_4K)\r\nreturn -EINVAL;\r\nspin_lock(&etnaviv_domain->map_lock);\r\npgtable_write(&etnaviv_domain->pgtable, iova,\r\netnaviv_domain->bad_page_dma);\r\nspin_unlock(&etnaviv_domain->map_lock);\r\nreturn SZ_4K;\r\n}\r\nstatic phys_addr_t etnaviv_iommu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct etnaviv_iommu_domain *etnaviv_domain = to_etnaviv_domain(domain);\r\nreturn pgtable_read(&etnaviv_domain->pgtable, iova);\r\n}\r\nstatic size_t etnaviv_iommuv1_dump_size(struct iommu_domain *domain)\r\n{\r\nreturn PT_SIZE;\r\n}\r\nstatic void etnaviv_iommuv1_dump(struct iommu_domain *domain, void *buf)\r\n{\r\nstruct etnaviv_iommu_domain *etnaviv_domain = to_etnaviv_domain(domain);\r\nmemcpy(buf, etnaviv_domain->pgtable.pgtable, PT_SIZE);\r\n}\r\nvoid etnaviv_iommu_domain_restore(struct etnaviv_gpu *gpu,\r\nstruct iommu_domain *domain)\r\n{\r\nstruct etnaviv_iommu_domain *etnaviv_domain = to_etnaviv_domain(domain);\r\nu32 pgtable;\r\npgtable = (u32)etnaviv_domain->pgtable.paddr;\r\ngpu_write(gpu, VIVS_MC_MMU_FE_PAGE_TABLE, pgtable);\r\ngpu_write(gpu, VIVS_MC_MMU_TX_PAGE_TABLE, pgtable);\r\ngpu_write(gpu, VIVS_MC_MMU_PE_PAGE_TABLE, pgtable);\r\ngpu_write(gpu, VIVS_MC_MMU_PEZ_PAGE_TABLE, pgtable);\r\ngpu_write(gpu, VIVS_MC_MMU_RA_PAGE_TABLE, pgtable);\r\n}\r\nstruct iommu_domain *etnaviv_iommu_domain_alloc(struct etnaviv_gpu *gpu)\r\n{\r\nstruct etnaviv_iommu_domain *etnaviv_domain;\r\nint ret;\r\netnaviv_domain = kzalloc(sizeof(*etnaviv_domain), GFP_KERNEL);\r\nif (!etnaviv_domain)\r\nreturn NULL;\r\netnaviv_domain->dev = gpu->dev;\r\netnaviv_domain->domain.type = __IOMMU_DOMAIN_PAGING;\r\netnaviv_domain->domain.ops = &etnaviv_iommu_ops.ops;\r\netnaviv_domain->domain.geometry.aperture_start = GPU_MEM_START;\r\netnaviv_domain->domain.geometry.aperture_end = GPU_MEM_START + PT_ENTRIES * SZ_4K - 1;\r\nret = __etnaviv_iommu_init(etnaviv_domain);\r\nif (ret)\r\ngoto out_free;\r\nreturn &etnaviv_domain->domain;\r\nout_free:\r\nkfree(etnaviv_domain);\r\nreturn NULL;\r\n}
