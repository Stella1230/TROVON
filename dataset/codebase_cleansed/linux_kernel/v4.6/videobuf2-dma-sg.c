static int vb2_dma_sg_alloc_compacted(struct vb2_dma_sg_buf *buf,\r\ngfp_t gfp_flags)\r\n{\r\nunsigned int last_page = 0;\r\nint size = buf->size;\r\nwhile (size > 0) {\r\nstruct page *pages;\r\nint order;\r\nint i;\r\norder = get_order(size);\r\nif ((PAGE_SIZE << order) > size)\r\norder--;\r\npages = NULL;\r\nwhile (!pages) {\r\npages = alloc_pages(GFP_KERNEL | __GFP_ZERO |\r\n__GFP_NOWARN | gfp_flags, order);\r\nif (pages)\r\nbreak;\r\nif (order == 0) {\r\nwhile (last_page--)\r\n__free_page(buf->pages[last_page]);\r\nreturn -ENOMEM;\r\n}\r\norder--;\r\n}\r\nsplit_page(pages, order);\r\nfor (i = 0; i < (1 << order); i++)\r\nbuf->pages[last_page++] = &pages[i];\r\nsize -= PAGE_SIZE << order;\r\n}\r\nreturn 0;\r\n}\r\nstatic void *vb2_dma_sg_alloc(void *alloc_ctx, unsigned long size,\r\nenum dma_data_direction dma_dir, gfp_t gfp_flags)\r\n{\r\nstruct vb2_dma_sg_conf *conf = alloc_ctx;\r\nstruct vb2_dma_sg_buf *buf;\r\nstruct sg_table *sgt;\r\nint ret;\r\nint num_pages;\r\nDEFINE_DMA_ATTRS(attrs);\r\ndma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, &attrs);\r\nif (WARN_ON(alloc_ctx == NULL))\r\nreturn NULL;\r\nbuf = kzalloc(sizeof *buf, GFP_KERNEL);\r\nif (!buf)\r\nreturn NULL;\r\nbuf->vaddr = NULL;\r\nbuf->dma_dir = dma_dir;\r\nbuf->offset = 0;\r\nbuf->size = size;\r\nbuf->num_pages = size >> PAGE_SHIFT;\r\nbuf->dma_sgt = &buf->sg_table;\r\nbuf->pages = kzalloc(buf->num_pages * sizeof(struct page *),\r\nGFP_KERNEL);\r\nif (!buf->pages)\r\ngoto fail_pages_array_alloc;\r\nret = vb2_dma_sg_alloc_compacted(buf, gfp_flags);\r\nif (ret)\r\ngoto fail_pages_alloc;\r\nret = sg_alloc_table_from_pages(buf->dma_sgt, buf->pages,\r\nbuf->num_pages, 0, size, GFP_KERNEL);\r\nif (ret)\r\ngoto fail_table_alloc;\r\nbuf->dev = get_device(conf->dev);\r\nsgt = &buf->sg_table;\r\nsgt->nents = dma_map_sg_attrs(buf->dev, sgt->sgl, sgt->orig_nents,\r\nbuf->dma_dir, &attrs);\r\nif (!sgt->nents)\r\ngoto fail_map;\r\nbuf->handler.refcount = &buf->refcount;\r\nbuf->handler.put = vb2_dma_sg_put;\r\nbuf->handler.arg = buf;\r\natomic_inc(&buf->refcount);\r\ndprintk(1, "%s: Allocated buffer of %d pages\n",\r\n__func__, buf->num_pages);\r\nreturn buf;\r\nfail_map:\r\nput_device(buf->dev);\r\nsg_free_table(buf->dma_sgt);\r\nfail_table_alloc:\r\nnum_pages = buf->num_pages;\r\nwhile (num_pages--)\r\n__free_page(buf->pages[num_pages]);\r\nfail_pages_alloc:\r\nkfree(buf->pages);\r\nfail_pages_array_alloc:\r\nkfree(buf);\r\nreturn NULL;\r\n}\r\nstatic void vb2_dma_sg_put(void *buf_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nstruct sg_table *sgt = &buf->sg_table;\r\nint i = buf->num_pages;\r\nif (atomic_dec_and_test(&buf->refcount)) {\r\nDEFINE_DMA_ATTRS(attrs);\r\ndma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, &attrs);\r\ndprintk(1, "%s: Freeing buffer of %d pages\n", __func__,\r\nbuf->num_pages);\r\ndma_unmap_sg_attrs(buf->dev, sgt->sgl, sgt->orig_nents,\r\nbuf->dma_dir, &attrs);\r\nif (buf->vaddr)\r\nvm_unmap_ram(buf->vaddr, buf->num_pages);\r\nsg_free_table(buf->dma_sgt);\r\nwhile (--i >= 0)\r\n__free_page(buf->pages[i]);\r\nkfree(buf->pages);\r\nput_device(buf->dev);\r\nkfree(buf);\r\n}\r\n}\r\nstatic void vb2_dma_sg_prepare(void *buf_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nstruct sg_table *sgt = buf->dma_sgt;\r\nif (buf->db_attach)\r\nreturn;\r\ndma_sync_sg_for_device(buf->dev, sgt->sgl, sgt->orig_nents,\r\nbuf->dma_dir);\r\n}\r\nstatic void vb2_dma_sg_finish(void *buf_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nstruct sg_table *sgt = buf->dma_sgt;\r\nif (buf->db_attach)\r\nreturn;\r\ndma_sync_sg_for_cpu(buf->dev, sgt->sgl, sgt->orig_nents, buf->dma_dir);\r\n}\r\nstatic void *vb2_dma_sg_get_userptr(void *alloc_ctx, unsigned long vaddr,\r\nunsigned long size,\r\nenum dma_data_direction dma_dir)\r\n{\r\nstruct vb2_dma_sg_conf *conf = alloc_ctx;\r\nstruct vb2_dma_sg_buf *buf;\r\nstruct sg_table *sgt;\r\nDEFINE_DMA_ATTRS(attrs);\r\nstruct frame_vector *vec;\r\ndma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, &attrs);\r\nbuf = kzalloc(sizeof *buf, GFP_KERNEL);\r\nif (!buf)\r\nreturn NULL;\r\nbuf->vaddr = NULL;\r\nbuf->dev = conf->dev;\r\nbuf->dma_dir = dma_dir;\r\nbuf->offset = vaddr & ~PAGE_MASK;\r\nbuf->size = size;\r\nbuf->dma_sgt = &buf->sg_table;\r\nvec = vb2_create_framevec(vaddr, size, buf->dma_dir == DMA_FROM_DEVICE);\r\nif (IS_ERR(vec))\r\ngoto userptr_fail_pfnvec;\r\nbuf->vec = vec;\r\nbuf->pages = frame_vector_pages(vec);\r\nif (IS_ERR(buf->pages))\r\ngoto userptr_fail_sgtable;\r\nbuf->num_pages = frame_vector_count(vec);\r\nif (sg_alloc_table_from_pages(buf->dma_sgt, buf->pages,\r\nbuf->num_pages, buf->offset, size, 0))\r\ngoto userptr_fail_sgtable;\r\nsgt = &buf->sg_table;\r\nsgt->nents = dma_map_sg_attrs(buf->dev, sgt->sgl, sgt->orig_nents,\r\nbuf->dma_dir, &attrs);\r\nif (!sgt->nents)\r\ngoto userptr_fail_map;\r\nreturn buf;\r\nuserptr_fail_map:\r\nsg_free_table(&buf->sg_table);\r\nuserptr_fail_sgtable:\r\nvb2_destroy_framevec(vec);\r\nuserptr_fail_pfnvec:\r\nkfree(buf);\r\nreturn NULL;\r\n}\r\nstatic void vb2_dma_sg_put_userptr(void *buf_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nstruct sg_table *sgt = &buf->sg_table;\r\nint i = buf->num_pages;\r\nDEFINE_DMA_ATTRS(attrs);\r\ndma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, &attrs);\r\ndprintk(1, "%s: Releasing userspace buffer of %d pages\n",\r\n__func__, buf->num_pages);\r\ndma_unmap_sg_attrs(buf->dev, sgt->sgl, sgt->orig_nents, buf->dma_dir,\r\n&attrs);\r\nif (buf->vaddr)\r\nvm_unmap_ram(buf->vaddr, buf->num_pages);\r\nsg_free_table(buf->dma_sgt);\r\nwhile (--i >= 0) {\r\nif (buf->dma_dir == DMA_FROM_DEVICE)\r\nset_page_dirty_lock(buf->pages[i]);\r\n}\r\nvb2_destroy_framevec(buf->vec);\r\nkfree(buf);\r\n}\r\nstatic void *vb2_dma_sg_vaddr(void *buf_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nBUG_ON(!buf);\r\nif (!buf->vaddr) {\r\nif (buf->db_attach)\r\nbuf->vaddr = dma_buf_vmap(buf->db_attach->dmabuf);\r\nelse\r\nbuf->vaddr = vm_map_ram(buf->pages,\r\nbuf->num_pages, -1, PAGE_KERNEL);\r\n}\r\nreturn buf->vaddr ? buf->vaddr + buf->offset : NULL;\r\n}\r\nstatic unsigned int vb2_dma_sg_num_users(void *buf_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nreturn atomic_read(&buf->refcount);\r\n}\r\nstatic int vb2_dma_sg_mmap(void *buf_priv, struct vm_area_struct *vma)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nunsigned long uaddr = vma->vm_start;\r\nunsigned long usize = vma->vm_end - vma->vm_start;\r\nint i = 0;\r\nif (!buf) {\r\nprintk(KERN_ERR "No memory to map\n");\r\nreturn -EINVAL;\r\n}\r\ndo {\r\nint ret;\r\nret = vm_insert_page(vma, uaddr, buf->pages[i++]);\r\nif (ret) {\r\nprintk(KERN_ERR "Remapping memory, error: %d\n", ret);\r\nreturn ret;\r\n}\r\nuaddr += PAGE_SIZE;\r\nusize -= PAGE_SIZE;\r\n} while (usize > 0);\r\nvma->vm_private_data = &buf->handler;\r\nvma->vm_ops = &vb2_common_vm_ops;\r\nvma->vm_ops->open(vma);\r\nreturn 0;\r\n}\r\nstatic int vb2_dma_sg_dmabuf_ops_attach(struct dma_buf *dbuf, struct device *dev,\r\nstruct dma_buf_attachment *dbuf_attach)\r\n{\r\nstruct vb2_dma_sg_attachment *attach;\r\nunsigned int i;\r\nstruct scatterlist *rd, *wr;\r\nstruct sg_table *sgt;\r\nstruct vb2_dma_sg_buf *buf = dbuf->priv;\r\nint ret;\r\nattach = kzalloc(sizeof(*attach), GFP_KERNEL);\r\nif (!attach)\r\nreturn -ENOMEM;\r\nsgt = &attach->sgt;\r\nret = sg_alloc_table(sgt, buf->dma_sgt->orig_nents, GFP_KERNEL);\r\nif (ret) {\r\nkfree(attach);\r\nreturn -ENOMEM;\r\n}\r\nrd = buf->dma_sgt->sgl;\r\nwr = sgt->sgl;\r\nfor (i = 0; i < sgt->orig_nents; ++i) {\r\nsg_set_page(wr, sg_page(rd), rd->length, rd->offset);\r\nrd = sg_next(rd);\r\nwr = sg_next(wr);\r\n}\r\nattach->dma_dir = DMA_NONE;\r\ndbuf_attach->priv = attach;\r\nreturn 0;\r\n}\r\nstatic void vb2_dma_sg_dmabuf_ops_detach(struct dma_buf *dbuf,\r\nstruct dma_buf_attachment *db_attach)\r\n{\r\nstruct vb2_dma_sg_attachment *attach = db_attach->priv;\r\nstruct sg_table *sgt;\r\nif (!attach)\r\nreturn;\r\nsgt = &attach->sgt;\r\nif (attach->dma_dir != DMA_NONE)\r\ndma_unmap_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\nattach->dma_dir);\r\nsg_free_table(sgt);\r\nkfree(attach);\r\ndb_attach->priv = NULL;\r\n}\r\nstatic struct sg_table *vb2_dma_sg_dmabuf_ops_map(\r\nstruct dma_buf_attachment *db_attach, enum dma_data_direction dma_dir)\r\n{\r\nstruct vb2_dma_sg_attachment *attach = db_attach->priv;\r\nstruct mutex *lock = &db_attach->dmabuf->lock;\r\nstruct sg_table *sgt;\r\nmutex_lock(lock);\r\nsgt = &attach->sgt;\r\nif (attach->dma_dir == dma_dir) {\r\nmutex_unlock(lock);\r\nreturn sgt;\r\n}\r\nif (attach->dma_dir != DMA_NONE) {\r\ndma_unmap_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\nattach->dma_dir);\r\nattach->dma_dir = DMA_NONE;\r\n}\r\nsgt->nents = dma_map_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\ndma_dir);\r\nif (!sgt->nents) {\r\npr_err("failed to map scatterlist\n");\r\nmutex_unlock(lock);\r\nreturn ERR_PTR(-EIO);\r\n}\r\nattach->dma_dir = dma_dir;\r\nmutex_unlock(lock);\r\nreturn sgt;\r\n}\r\nstatic void vb2_dma_sg_dmabuf_ops_unmap(struct dma_buf_attachment *db_attach,\r\nstruct sg_table *sgt, enum dma_data_direction dma_dir)\r\n{\r\n}\r\nstatic void vb2_dma_sg_dmabuf_ops_release(struct dma_buf *dbuf)\r\n{\r\nvb2_dma_sg_put(dbuf->priv);\r\n}\r\nstatic void *vb2_dma_sg_dmabuf_ops_kmap(struct dma_buf *dbuf, unsigned long pgnum)\r\n{\r\nstruct vb2_dma_sg_buf *buf = dbuf->priv;\r\nreturn buf->vaddr ? buf->vaddr + pgnum * PAGE_SIZE : NULL;\r\n}\r\nstatic void *vb2_dma_sg_dmabuf_ops_vmap(struct dma_buf *dbuf)\r\n{\r\nstruct vb2_dma_sg_buf *buf = dbuf->priv;\r\nreturn vb2_dma_sg_vaddr(buf);\r\n}\r\nstatic int vb2_dma_sg_dmabuf_ops_mmap(struct dma_buf *dbuf,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn vb2_dma_sg_mmap(dbuf->priv, vma);\r\n}\r\nstatic struct dma_buf *vb2_dma_sg_get_dmabuf(void *buf_priv, unsigned long flags)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nstruct dma_buf *dbuf;\r\nDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\r\nexp_info.ops = &vb2_dma_sg_dmabuf_ops;\r\nexp_info.size = buf->size;\r\nexp_info.flags = flags;\r\nexp_info.priv = buf;\r\nif (WARN_ON(!buf->dma_sgt))\r\nreturn NULL;\r\ndbuf = dma_buf_export(&exp_info);\r\nif (IS_ERR(dbuf))\r\nreturn NULL;\r\natomic_inc(&buf->refcount);\r\nreturn dbuf;\r\n}\r\nstatic int vb2_dma_sg_map_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = mem_priv;\r\nstruct sg_table *sgt;\r\nif (WARN_ON(!buf->db_attach)) {\r\npr_err("trying to pin a non attached buffer\n");\r\nreturn -EINVAL;\r\n}\r\nif (WARN_ON(buf->dma_sgt)) {\r\npr_err("dmabuf buffer is already pinned\n");\r\nreturn 0;\r\n}\r\nsgt = dma_buf_map_attachment(buf->db_attach, buf->dma_dir);\r\nif (IS_ERR(sgt)) {\r\npr_err("Error getting dmabuf scatterlist\n");\r\nreturn -EINVAL;\r\n}\r\nbuf->dma_sgt = sgt;\r\nbuf->vaddr = NULL;\r\nreturn 0;\r\n}\r\nstatic void vb2_dma_sg_unmap_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = mem_priv;\r\nstruct sg_table *sgt = buf->dma_sgt;\r\nif (WARN_ON(!buf->db_attach)) {\r\npr_err("trying to unpin a not attached buffer\n");\r\nreturn;\r\n}\r\nif (WARN_ON(!sgt)) {\r\npr_err("dmabuf buffer is already unpinned\n");\r\nreturn;\r\n}\r\nif (buf->vaddr) {\r\ndma_buf_vunmap(buf->db_attach->dmabuf, buf->vaddr);\r\nbuf->vaddr = NULL;\r\n}\r\ndma_buf_unmap_attachment(buf->db_attach, sgt, buf->dma_dir);\r\nbuf->dma_sgt = NULL;\r\n}\r\nstatic void vb2_dma_sg_detach_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = mem_priv;\r\nif (WARN_ON(buf->dma_sgt))\r\nvb2_dma_sg_unmap_dmabuf(buf);\r\ndma_buf_detach(buf->db_attach->dmabuf, buf->db_attach);\r\nkfree(buf);\r\n}\r\nstatic void *vb2_dma_sg_attach_dmabuf(void *alloc_ctx, struct dma_buf *dbuf,\r\nunsigned long size, enum dma_data_direction dma_dir)\r\n{\r\nstruct vb2_dma_sg_conf *conf = alloc_ctx;\r\nstruct vb2_dma_sg_buf *buf;\r\nstruct dma_buf_attachment *dba;\r\nif (dbuf->size < size)\r\nreturn ERR_PTR(-EFAULT);\r\nbuf = kzalloc(sizeof(*buf), GFP_KERNEL);\r\nif (!buf)\r\nreturn ERR_PTR(-ENOMEM);\r\nbuf->dev = conf->dev;\r\ndba = dma_buf_attach(dbuf, buf->dev);\r\nif (IS_ERR(dba)) {\r\npr_err("failed to attach dmabuf\n");\r\nkfree(buf);\r\nreturn dba;\r\n}\r\nbuf->dma_dir = dma_dir;\r\nbuf->size = size;\r\nbuf->db_attach = dba;\r\nreturn buf;\r\n}\r\nstatic void *vb2_dma_sg_cookie(void *buf_priv)\r\n{\r\nstruct vb2_dma_sg_buf *buf = buf_priv;\r\nreturn buf->dma_sgt;\r\n}\r\nvoid *vb2_dma_sg_init_ctx(struct device *dev)\r\n{\r\nstruct vb2_dma_sg_conf *conf;\r\nconf = kzalloc(sizeof(*conf), GFP_KERNEL);\r\nif (!conf)\r\nreturn ERR_PTR(-ENOMEM);\r\nconf->dev = dev;\r\nreturn conf;\r\n}\r\nvoid vb2_dma_sg_cleanup_ctx(void *alloc_ctx)\r\n{\r\nif (!IS_ERR_OR_NULL(alloc_ctx))\r\nkfree(alloc_ctx);\r\n}
