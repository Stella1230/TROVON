static long h_reg_sub_crq(unsigned long unit_address, unsigned long token,\r\nunsigned long length, unsigned long *number,\r\nunsigned long *irq)\r\n{\r\nunsigned long retbuf[PLPAR_HCALL_BUFSIZE];\r\nlong rc;\r\nrc = plpar_hcall(H_REG_SUB_CRQ, retbuf, unit_address, token, length);\r\n*number = retbuf[0];\r\n*irq = retbuf[1];\r\nreturn rc;\r\n}\r\nstatic void init_rx_pool(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_rx_pool *rx_pool, int num, int index,\r\nint buff_size, int active)\r\n{\r\nnetdev_dbg(adapter->netdev,\r\n"Initializing rx_pool %d, %d buffs, %d bytes each\n",\r\nindex, num, buff_size);\r\nrx_pool->size = num;\r\nrx_pool->index = index;\r\nrx_pool->buff_size = buff_size;\r\nrx_pool->active = active;\r\n}\r\nstatic int alloc_long_term_buff(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_long_term_buff *ltb, int size)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nltb->size = size;\r\nltb->buff = dma_alloc_coherent(dev, ltb->size, &ltb->addr,\r\nGFP_KERNEL);\r\nif (!ltb->buff) {\r\ndev_err(dev, "Couldn't alloc long term buffer\n");\r\nreturn -ENOMEM;\r\n}\r\nltb->map_id = adapter->map_id;\r\nadapter->map_id++;\r\nsend_request_map(adapter, ltb->addr,\r\nltb->size, ltb->map_id);\r\ninit_completion(&adapter->fw_done);\r\nwait_for_completion(&adapter->fw_done);\r\nreturn 0;\r\n}\r\nstatic void free_long_term_buff(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_long_term_buff *ltb)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\ndma_free_coherent(dev, ltb->size, ltb->buff, ltb->addr);\r\nsend_request_unmap(adapter, ltb->map_id);\r\n}\r\nstatic int alloc_rx_pool(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_rx_pool *pool)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nint i;\r\npool->free_map = kcalloc(pool->size, sizeof(int), GFP_KERNEL);\r\nif (!pool->free_map)\r\nreturn -ENOMEM;\r\npool->rx_buff = kcalloc(pool->size, sizeof(struct ibmvnic_rx_buff),\r\nGFP_KERNEL);\r\nif (!pool->rx_buff) {\r\ndev_err(dev, "Couldn't alloc rx buffers\n");\r\nkfree(pool->free_map);\r\nreturn -ENOMEM;\r\n}\r\nif (alloc_long_term_buff(adapter, &pool->long_term_buff,\r\npool->size * pool->buff_size)) {\r\nkfree(pool->free_map);\r\nkfree(pool->rx_buff);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < pool->size; ++i)\r\npool->free_map[i] = i;\r\natomic_set(&pool->available, 0);\r\npool->next_alloc = 0;\r\npool->next_free = 0;\r\nreturn 0;\r\n}\r\nstatic void replenish_rx_pool(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_rx_pool *pool)\r\n{\r\nint count = pool->size - atomic_read(&pool->available);\r\nstruct device *dev = &adapter->vdev->dev;\r\nint buffers_added = 0;\r\nunsigned long lpar_rc;\r\nunion sub_crq sub_crq;\r\nstruct sk_buff *skb;\r\nunsigned int offset;\r\ndma_addr_t dma_addr;\r\nunsigned char *dst;\r\nu64 *handle_array;\r\nint shift = 0;\r\nint index;\r\nint i;\r\nhandle_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +\r\nbe32_to_cpu(adapter->login_rsp_buf->\r\noff_rxadd_subcrqs));\r\nfor (i = 0; i < count; ++i) {\r\nskb = alloc_skb(pool->buff_size, GFP_ATOMIC);\r\nif (!skb) {\r\ndev_err(dev, "Couldn't replenish rx buff\n");\r\nadapter->replenish_no_mem++;\r\nbreak;\r\n}\r\nindex = pool->free_map[pool->next_free];\r\nif (pool->rx_buff[index].skb)\r\ndev_err(dev, "Inconsistent free_map!\n");\r\noffset = index * pool->buff_size;\r\ndst = pool->long_term_buff.buff + offset;\r\nmemset(dst, 0, pool->buff_size);\r\ndma_addr = pool->long_term_buff.addr + offset;\r\npool->rx_buff[index].data = dst;\r\npool->free_map[pool->next_free] = IBMVNIC_INVALID_MAP;\r\npool->rx_buff[index].dma = dma_addr;\r\npool->rx_buff[index].skb = skb;\r\npool->rx_buff[index].pool_index = pool->index;\r\npool->rx_buff[index].size = pool->buff_size;\r\nmemset(&sub_crq, 0, sizeof(sub_crq));\r\nsub_crq.rx_add.first = IBMVNIC_CRQ_CMD;\r\nsub_crq.rx_add.correlator =\r\ncpu_to_be64((u64)&pool->rx_buff[index]);\r\nsub_crq.rx_add.ioba = cpu_to_be32(dma_addr);\r\nsub_crq.rx_add.map_id = pool->long_term_buff.map_id;\r\n#ifdef __LITTLE_ENDIAN__\r\nshift = 8;\r\n#endif\r\nsub_crq.rx_add.len = cpu_to_be32(pool->buff_size << shift);\r\nlpar_rc = send_subcrq(adapter, handle_array[pool->index],\r\n&sub_crq);\r\nif (lpar_rc != H_SUCCESS)\r\ngoto failure;\r\nbuffers_added++;\r\nadapter->replenish_add_buff_success++;\r\npool->next_free = (pool->next_free + 1) % pool->size;\r\n}\r\natomic_add(buffers_added, &pool->available);\r\nreturn;\r\nfailure:\r\ndev_info(dev, "replenish pools failure\n");\r\npool->free_map[pool->next_free] = index;\r\npool->rx_buff[index].skb = NULL;\r\nif (!dma_mapping_error(dev, dma_addr))\r\ndma_unmap_single(dev, dma_addr, pool->buff_size,\r\nDMA_FROM_DEVICE);\r\ndev_kfree_skb_any(skb);\r\nadapter->replenish_add_buff_failure++;\r\natomic_add(buffers_added, &pool->available);\r\n}\r\nstatic void replenish_pools(struct ibmvnic_adapter *adapter)\r\n{\r\nint i;\r\nif (adapter->migrated)\r\nreturn;\r\nadapter->replenish_task_cycles++;\r\nfor (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);\r\ni++) {\r\nif (adapter->rx_pool[i].active)\r\nreplenish_rx_pool(adapter, &adapter->rx_pool[i]);\r\n}\r\n}\r\nstatic void free_rx_pool(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_rx_pool *pool)\r\n{\r\nint i;\r\nkfree(pool->free_map);\r\npool->free_map = NULL;\r\nif (!pool->rx_buff)\r\nreturn;\r\nfor (i = 0; i < pool->size; i++) {\r\nif (pool->rx_buff[i].skb) {\r\ndev_kfree_skb_any(pool->rx_buff[i].skb);\r\npool->rx_buff[i].skb = NULL;\r\n}\r\n}\r\nkfree(pool->rx_buff);\r\npool->rx_buff = NULL;\r\n}\r\nstatic int ibmvnic_open(struct net_device *netdev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_tx_pool *tx_pool;\r\nunion ibmvnic_crq crq;\r\nint rxadd_subcrqs;\r\nu64 *size_array;\r\nint tx_subcrqs;\r\nint i, j;\r\nrxadd_subcrqs =\r\nbe32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);\r\ntx_subcrqs =\r\nbe32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);\r\nsize_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +\r\nbe32_to_cpu(adapter->login_rsp_buf->\r\noff_rxadd_buff_size));\r\nadapter->map_id = 1;\r\nadapter->napi = kcalloc(adapter->req_rx_queues,\r\nsizeof(struct napi_struct), GFP_KERNEL);\r\nif (!adapter->napi)\r\ngoto alloc_napi_failed;\r\nfor (i = 0; i < adapter->req_rx_queues; i++) {\r\nnetif_napi_add(netdev, &adapter->napi[i], ibmvnic_poll,\r\nNAPI_POLL_WEIGHT);\r\nnapi_enable(&adapter->napi[i]);\r\n}\r\nadapter->rx_pool =\r\nkcalloc(rxadd_subcrqs, sizeof(struct ibmvnic_rx_pool), GFP_KERNEL);\r\nif (!adapter->rx_pool)\r\ngoto rx_pool_arr_alloc_failed;\r\nsend_map_query(adapter);\r\nfor (i = 0; i < rxadd_subcrqs; i++) {\r\ninit_rx_pool(adapter, &adapter->rx_pool[i],\r\nIBMVNIC_BUFFS_PER_POOL, i,\r\nbe64_to_cpu(size_array[i]), 1);\r\nif (alloc_rx_pool(adapter, &adapter->rx_pool[i])) {\r\ndev_err(dev, "Couldn't alloc rx pool\n");\r\ngoto rx_pool_alloc_failed;\r\n}\r\n}\r\nadapter->tx_pool =\r\nkcalloc(tx_subcrqs, sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);\r\nif (!adapter->tx_pool)\r\ngoto tx_pool_arr_alloc_failed;\r\nfor (i = 0; i < tx_subcrqs; i++) {\r\ntx_pool = &adapter->tx_pool[i];\r\ntx_pool->tx_buff =\r\nkcalloc(adapter->max_tx_entries_per_subcrq,\r\nsizeof(struct ibmvnic_tx_buff), GFP_KERNEL);\r\nif (!tx_pool->tx_buff)\r\ngoto tx_pool_alloc_failed;\r\nif (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,\r\nadapter->max_tx_entries_per_subcrq *\r\nadapter->req_mtu))\r\ngoto tx_ltb_alloc_failed;\r\ntx_pool->free_map =\r\nkcalloc(adapter->max_tx_entries_per_subcrq,\r\nsizeof(int), GFP_KERNEL);\r\nif (!tx_pool->free_map)\r\ngoto tx_fm_alloc_failed;\r\nfor (j = 0; j < adapter->max_tx_entries_per_subcrq; j++)\r\ntx_pool->free_map[j] = j;\r\ntx_pool->consumer_index = 0;\r\ntx_pool->producer_index = 0;\r\n}\r\nadapter->bounce_buffer_size =\r\n(netdev->mtu + ETH_HLEN - 1) / PAGE_SIZE + 1;\r\nadapter->bounce_buffer = kmalloc(adapter->bounce_buffer_size,\r\nGFP_KERNEL);\r\nif (!adapter->bounce_buffer)\r\ngoto bounce_alloc_failed;\r\nadapter->bounce_buffer_dma = dma_map_single(dev, adapter->bounce_buffer,\r\nadapter->bounce_buffer_size,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, adapter->bounce_buffer_dma)) {\r\ndev_err(dev, "Couldn't map tx bounce buffer\n");\r\ngoto bounce_map_failed;\r\n}\r\nreplenish_pools(adapter);\r\nfor (i = 0; i < adapter->req_rx_queues; i++)\r\nenable_scrq_irq(adapter, adapter->rx_scrq[i]);\r\nfor (i = 0; i < adapter->req_tx_queues; i++)\r\nenable_scrq_irq(adapter, adapter->tx_scrq[i]);\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.logical_link_state.first = IBMVNIC_CRQ_CMD;\r\ncrq.logical_link_state.cmd = LOGICAL_LINK_STATE;\r\ncrq.logical_link_state.link_state = IBMVNIC_LOGICAL_LNK_UP;\r\nibmvnic_send_crq(adapter, &crq);\r\nnetif_start_queue(netdev);\r\nreturn 0;\r\nbounce_map_failed:\r\nkfree(adapter->bounce_buffer);\r\nbounce_alloc_failed:\r\ni = tx_subcrqs - 1;\r\nkfree(adapter->tx_pool[i].free_map);\r\ntx_fm_alloc_failed:\r\nfree_long_term_buff(adapter, &adapter->tx_pool[i].long_term_buff);\r\ntx_ltb_alloc_failed:\r\nkfree(adapter->tx_pool[i].tx_buff);\r\ntx_pool_alloc_failed:\r\nfor (j = 0; j < i; j++) {\r\nkfree(adapter->tx_pool[j].tx_buff);\r\nfree_long_term_buff(adapter,\r\n&adapter->tx_pool[j].long_term_buff);\r\nkfree(adapter->tx_pool[j].free_map);\r\n}\r\nkfree(adapter->tx_pool);\r\nadapter->tx_pool = NULL;\r\ntx_pool_arr_alloc_failed:\r\ni = rxadd_subcrqs;\r\nrx_pool_alloc_failed:\r\nfor (j = 0; j < i; j++) {\r\nfree_rx_pool(adapter, &adapter->rx_pool[j]);\r\nfree_long_term_buff(adapter,\r\n&adapter->rx_pool[j].long_term_buff);\r\n}\r\nkfree(adapter->rx_pool);\r\nadapter->rx_pool = NULL;\r\nrx_pool_arr_alloc_failed:\r\nfor (i = 0; i < adapter->req_rx_queues; i++)\r\nnapi_enable(&adapter->napi[i]);\r\nalloc_napi_failed:\r\nreturn -ENOMEM;\r\n}\r\nstatic int ibmvnic_close(struct net_device *netdev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nstruct device *dev = &adapter->vdev->dev;\r\nunion ibmvnic_crq crq;\r\nint i;\r\nadapter->closing = true;\r\nfor (i = 0; i < adapter->req_rx_queues; i++)\r\nnapi_disable(&adapter->napi[i]);\r\nnetif_stop_queue(netdev);\r\nif (adapter->bounce_buffer) {\r\nif (!dma_mapping_error(dev, adapter->bounce_buffer_dma)) {\r\ndma_unmap_single(&adapter->vdev->dev,\r\nadapter->bounce_buffer_dma,\r\nadapter->bounce_buffer_size,\r\nDMA_BIDIRECTIONAL);\r\nadapter->bounce_buffer_dma = DMA_ERROR_CODE;\r\n}\r\nkfree(adapter->bounce_buffer);\r\nadapter->bounce_buffer = NULL;\r\n}\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.logical_link_state.first = IBMVNIC_CRQ_CMD;\r\ncrq.logical_link_state.cmd = LOGICAL_LINK_STATE;\r\ncrq.logical_link_state.link_state = IBMVNIC_LOGICAL_LNK_DN;\r\nibmvnic_send_crq(adapter, &crq);\r\nfor (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);\r\ni++) {\r\nkfree(adapter->tx_pool[i].tx_buff);\r\nfree_long_term_buff(adapter,\r\n&adapter->tx_pool[i].long_term_buff);\r\nkfree(adapter->tx_pool[i].free_map);\r\n}\r\nkfree(adapter->tx_pool);\r\nadapter->tx_pool = NULL;\r\nfor (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);\r\ni++) {\r\nfree_rx_pool(adapter, &adapter->rx_pool[i]);\r\nfree_long_term_buff(adapter,\r\n&adapter->rx_pool[i].long_term_buff);\r\n}\r\nkfree(adapter->rx_pool);\r\nadapter->rx_pool = NULL;\r\nadapter->closing = false;\r\nreturn 0;\r\n}\r\nstatic int ibmvnic_xmit(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nint queue_num = skb_get_queue_mapping(skb);\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_tx_buff *tx_buff = NULL;\r\nstruct ibmvnic_tx_pool *tx_pool;\r\nunsigned int tx_send_failed = 0;\r\nunsigned int tx_map_failed = 0;\r\nunsigned int tx_dropped = 0;\r\nunsigned int tx_packets = 0;\r\nunsigned int tx_bytes = 0;\r\ndma_addr_t data_dma_addr;\r\nstruct netdev_queue *txq;\r\nbool used_bounce = false;\r\nunsigned long lpar_rc;\r\nunion sub_crq tx_crq;\r\nunsigned int offset;\r\nunsigned char *dst;\r\nu64 *handle_array;\r\nint index = 0;\r\nint ret = 0;\r\ntx_pool = &adapter->tx_pool[queue_num];\r\ntxq = netdev_get_tx_queue(netdev, skb_get_queue_mapping(skb));\r\nhandle_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +\r\nbe32_to_cpu(adapter->login_rsp_buf->\r\noff_txsubm_subcrqs));\r\nif (adapter->migrated) {\r\ntx_send_failed++;\r\ntx_dropped++;\r\nret = NETDEV_TX_BUSY;\r\ngoto out;\r\n}\r\nindex = tx_pool->free_map[tx_pool->consumer_index];\r\noffset = index * adapter->req_mtu;\r\ndst = tx_pool->long_term_buff.buff + offset;\r\nmemset(dst, 0, adapter->req_mtu);\r\nskb_copy_from_linear_data(skb, dst, skb->len);\r\ndata_dma_addr = tx_pool->long_term_buff.addr + offset;\r\ntx_pool->consumer_index =\r\n(tx_pool->consumer_index + 1) %\r\nadapter->max_tx_entries_per_subcrq;\r\ntx_buff = &tx_pool->tx_buff[index];\r\ntx_buff->skb = skb;\r\ntx_buff->data_dma[0] = data_dma_addr;\r\ntx_buff->data_len[0] = skb->len;\r\ntx_buff->index = index;\r\ntx_buff->pool_index = queue_num;\r\ntx_buff->last_frag = true;\r\ntx_buff->used_bounce = used_bounce;\r\nmemset(&tx_crq, 0, sizeof(tx_crq));\r\ntx_crq.v1.first = IBMVNIC_CRQ_CMD;\r\ntx_crq.v1.type = IBMVNIC_TX_DESC;\r\ntx_crq.v1.n_crq_elem = 1;\r\ntx_crq.v1.n_sge = 1;\r\ntx_crq.v1.flags1 = IBMVNIC_TX_COMP_NEEDED;\r\ntx_crq.v1.correlator = cpu_to_be32(index);\r\ntx_crq.v1.dma_reg = cpu_to_be16(tx_pool->long_term_buff.map_id);\r\ntx_crq.v1.sge_len = cpu_to_be32(skb->len);\r\ntx_crq.v1.ioba = cpu_to_be64(data_dma_addr);\r\nif (adapter->vlan_header_insertion) {\r\ntx_crq.v1.flags2 |= IBMVNIC_TX_VLAN_INSERT;\r\ntx_crq.v1.vlan_id = cpu_to_be16(skb->vlan_tci);\r\n}\r\nif (skb->protocol == htons(ETH_P_IP)) {\r\nif (ip_hdr(skb)->version == 4)\r\ntx_crq.v1.flags1 |= IBMVNIC_TX_PROT_IPV4;\r\nelse if (ip_hdr(skb)->version == 6)\r\ntx_crq.v1.flags1 |= IBMVNIC_TX_PROT_IPV6;\r\nif (ip_hdr(skb)->protocol == IPPROTO_TCP)\r\ntx_crq.v1.flags1 |= IBMVNIC_TX_PROT_TCP;\r\nelse if (ip_hdr(skb)->protocol != IPPROTO_TCP)\r\ntx_crq.v1.flags1 |= IBMVNIC_TX_PROT_UDP;\r\n}\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\ntx_crq.v1.flags1 |= IBMVNIC_TX_CHKSUM_OFFLOAD;\r\nlpar_rc = send_subcrq(adapter, handle_array[0], &tx_crq);\r\nif (lpar_rc != H_SUCCESS) {\r\ndev_err(dev, "tx failed with code %ld\n", lpar_rc);\r\nif (tx_pool->consumer_index == 0)\r\ntx_pool->consumer_index =\r\nadapter->max_tx_entries_per_subcrq - 1;\r\nelse\r\ntx_pool->consumer_index--;\r\ntx_send_failed++;\r\ntx_dropped++;\r\nret = NETDEV_TX_BUSY;\r\ngoto out;\r\n}\r\ntx_packets++;\r\ntx_bytes += skb->len;\r\ntxq->trans_start = jiffies;\r\nret = NETDEV_TX_OK;\r\nout:\r\nnetdev->stats.tx_dropped += tx_dropped;\r\nnetdev->stats.tx_bytes += tx_bytes;\r\nnetdev->stats.tx_packets += tx_packets;\r\nadapter->tx_send_failed += tx_send_failed;\r\nadapter->tx_map_failed += tx_map_failed;\r\nreturn ret;\r\n}\r\nstatic void ibmvnic_set_multi(struct net_device *netdev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nstruct netdev_hw_addr *ha;\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.request_capability.first = IBMVNIC_CRQ_CMD;\r\ncrq.request_capability.cmd = REQUEST_CAPABILITY;\r\nif (netdev->flags & IFF_PROMISC) {\r\nif (!adapter->promisc_supported)\r\nreturn;\r\n} else {\r\nif (netdev->flags & IFF_ALLMULTI) {\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.multicast_ctrl.first = IBMVNIC_CRQ_CMD;\r\ncrq.multicast_ctrl.cmd = MULTICAST_CTRL;\r\ncrq.multicast_ctrl.flags = IBMVNIC_ENABLE_ALL;\r\nibmvnic_send_crq(adapter, &crq);\r\n} else if (netdev_mc_empty(netdev)) {\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.multicast_ctrl.first = IBMVNIC_CRQ_CMD;\r\ncrq.multicast_ctrl.cmd = MULTICAST_CTRL;\r\ncrq.multicast_ctrl.flags = IBMVNIC_DISABLE_ALL;\r\nibmvnic_send_crq(adapter, &crq);\r\n} else {\r\nnetdev_for_each_mc_addr(ha, netdev) {\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.multicast_ctrl.first = IBMVNIC_CRQ_CMD;\r\ncrq.multicast_ctrl.cmd = MULTICAST_CTRL;\r\ncrq.multicast_ctrl.flags = IBMVNIC_ENABLE_MC;\r\nether_addr_copy(&crq.multicast_ctrl.mac_addr[0],\r\nha->addr);\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\n}\r\n}\r\n}\r\nstatic int ibmvnic_set_mac(struct net_device *netdev, void *p)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nstruct sockaddr *addr = p;\r\nunion ibmvnic_crq crq;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.change_mac_addr.first = IBMVNIC_CRQ_CMD;\r\ncrq.change_mac_addr.cmd = CHANGE_MAC_ADDR;\r\nether_addr_copy(&crq.change_mac_addr.mac_addr[0], addr->sa_data);\r\nibmvnic_send_crq(adapter, &crq);\r\nreturn 0;\r\n}\r\nstatic int ibmvnic_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nif (new_mtu > adapter->req_mtu || new_mtu < adapter->min_mtu)\r\nreturn -EINVAL;\r\nnetdev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic void ibmvnic_tx_timeout(struct net_device *dev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(dev);\r\nint rc;\r\nrelease_sub_crqs(adapter);\r\nrc = ibmvnic_reset_crq(adapter);\r\nif (rc)\r\ndev_err(&adapter->vdev->dev, "Adapter timeout, reset failed\n");\r\nelse\r\nibmvnic_send_crq_init(adapter);\r\n}\r\nstatic void remove_buff_from_pool(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_rx_buff *rx_buff)\r\n{\r\nstruct ibmvnic_rx_pool *pool = &adapter->rx_pool[rx_buff->pool_index];\r\nrx_buff->skb = NULL;\r\npool->free_map[pool->next_alloc] = (int)(rx_buff - pool->rx_buff);\r\npool->next_alloc = (pool->next_alloc + 1) % pool->size;\r\natomic_dec(&pool->available);\r\n}\r\nstatic int ibmvnic_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct net_device *netdev = napi->dev;\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nint scrq_num = (int)(napi - adapter->napi);\r\nint frames_processed = 0;\r\nrestart_poll:\r\nwhile (frames_processed < budget) {\r\nstruct sk_buff *skb;\r\nstruct ibmvnic_rx_buff *rx_buff;\r\nunion sub_crq *next;\r\nu32 length;\r\nu16 offset;\r\nu8 flags = 0;\r\nif (!pending_scrq(adapter, adapter->rx_scrq[scrq_num]))\r\nbreak;\r\nnext = ibmvnic_next_scrq(adapter, adapter->rx_scrq[scrq_num]);\r\nrx_buff =\r\n(struct ibmvnic_rx_buff *)be64_to_cpu(next->\r\nrx_comp.correlator);\r\nif (next->rx_comp.rc) {\r\nnetdev_err(netdev, "rx error %x\n", next->rx_comp.rc);\r\nnext->rx_comp.first = 0;\r\nremove_buff_from_pool(adapter, rx_buff);\r\nbreak;\r\n}\r\nlength = be32_to_cpu(next->rx_comp.len);\r\noffset = be16_to_cpu(next->rx_comp.off_frame_data);\r\nflags = next->rx_comp.flags;\r\nskb = rx_buff->skb;\r\nskb_copy_to_linear_data(skb, rx_buff->data + offset,\r\nlength);\r\nskb->vlan_tci = be16_to_cpu(next->rx_comp.vlan_tci);\r\nnext->rx_comp.first = 0;\r\nremove_buff_from_pool(adapter, rx_buff);\r\nskb_put(skb, length);\r\nskb->protocol = eth_type_trans(skb, netdev);\r\nif (flags & IBMVNIC_IP_CHKSUM_GOOD &&\r\nflags & IBMVNIC_TCP_UDP_CHKSUM_GOOD) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n}\r\nlength = skb->len;\r\nnapi_gro_receive(napi, skb);\r\nnetdev->stats.rx_packets++;\r\nnetdev->stats.rx_bytes += length;\r\nframes_processed++;\r\n}\r\nreplenish_pools(adapter);\r\nif (frames_processed < budget) {\r\nenable_scrq_irq(adapter, adapter->rx_scrq[scrq_num]);\r\nnapi_complete(napi);\r\nif (pending_scrq(adapter, adapter->rx_scrq[scrq_num]) &&\r\nnapi_reschedule(napi)) {\r\ndisable_scrq_irq(adapter, adapter->rx_scrq[scrq_num]);\r\ngoto restart_poll;\r\n}\r\n}\r\nreturn frames_processed;\r\n}\r\nstatic void ibmvnic_netpoll_controller(struct net_device *dev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(dev);\r\nint i;\r\nreplenish_pools(netdev_priv(dev));\r\nfor (i = 0; i < adapter->req_rx_queues; i++)\r\nibmvnic_interrupt_rx(adapter->rx_scrq[i]->irq,\r\nadapter->rx_scrq[i]);\r\n}\r\nstatic int ibmvnic_get_settings(struct net_device *netdev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\ncmd->supported = (SUPPORTED_1000baseT_Full | SUPPORTED_Autoneg |\r\nSUPPORTED_FIBRE);\r\ncmd->advertising = (ADVERTISED_1000baseT_Full | ADVERTISED_Autoneg |\r\nADVERTISED_FIBRE);\r\nethtool_cmd_speed_set(cmd, SPEED_1000);\r\ncmd->duplex = DUPLEX_FULL;\r\ncmd->port = PORT_FIBRE;\r\ncmd->phy_address = 0;\r\ncmd->transceiver = XCVR_INTERNAL;\r\ncmd->autoneg = AUTONEG_ENABLE;\r\ncmd->maxtxpkt = 0;\r\ncmd->maxrxpkt = 1;\r\nreturn 0;\r\n}\r\nstatic void ibmvnic_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstrlcpy(info->driver, ibmvnic_driver_name, sizeof(info->driver));\r\nstrlcpy(info->version, IBMVNIC_DRIVER_VERSION, sizeof(info->version));\r\n}\r\nstatic u32 ibmvnic_get_msglevel(struct net_device *netdev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nreturn adapter->msg_enable;\r\n}\r\nstatic void ibmvnic_set_msglevel(struct net_device *netdev, u32 data)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nadapter->msg_enable = data;\r\n}\r\nstatic u32 ibmvnic_get_link(struct net_device *netdev)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nreturn adapter->logical_link_state;\r\n}\r\nstatic void ibmvnic_get_ringparam(struct net_device *netdev,\r\nstruct ethtool_ringparam *ring)\r\n{\r\nring->rx_max_pending = 0;\r\nring->tx_max_pending = 0;\r\nring->rx_mini_max_pending = 0;\r\nring->rx_jumbo_max_pending = 0;\r\nring->rx_pending = 0;\r\nring->tx_pending = 0;\r\nring->rx_mini_pending = 0;\r\nring->rx_jumbo_pending = 0;\r\n}\r\nstatic void ibmvnic_get_strings(struct net_device *dev, u32 stringset, u8 *data)\r\n{\r\nint i;\r\nif (stringset != ETH_SS_STATS)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(ibmvnic_stats); i++, data += ETH_GSTRING_LEN)\r\nmemcpy(data, ibmvnic_stats[i].name, ETH_GSTRING_LEN);\r\n}\r\nstatic int ibmvnic_get_sset_count(struct net_device *dev, int sset)\r\n{\r\nswitch (sset) {\r\ncase ETH_SS_STATS:\r\nreturn ARRAY_SIZE(ibmvnic_stats);\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void ibmvnic_get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *stats, u64 *data)\r\n{\r\nstruct ibmvnic_adapter *adapter = netdev_priv(dev);\r\nunion ibmvnic_crq crq;\r\nint i;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.request_statistics.first = IBMVNIC_CRQ_CMD;\r\ncrq.request_statistics.cmd = REQUEST_STATISTICS;\r\ncrq.request_statistics.ioba = cpu_to_be32(adapter->stats_token);\r\ncrq.request_statistics.len =\r\ncpu_to_be32(sizeof(struct ibmvnic_statistics));\r\nibmvnic_send_crq(adapter, &crq);\r\ninit_completion(&adapter->stats_done);\r\nwait_for_completion(&adapter->stats_done);\r\nfor (i = 0; i < ARRAY_SIZE(ibmvnic_stats); i++)\r\ndata[i] = IBMVNIC_GET_STAT(adapter, ibmvnic_stats[i].offset);\r\n}\r\nstatic void release_sub_crq_queue(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_sub_crq_queue *scrq)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nlong rc;\r\nnetdev_dbg(adapter->netdev, "Releasing sub-CRQ\n");\r\ndo {\r\nrc = plpar_hcall_norets(H_FREE_SUB_CRQ,\r\nadapter->vdev->unit_address,\r\nscrq->crq_num);\r\n} while (rc == H_BUSY || H_IS_LONG_BUSY(rc));\r\ndma_unmap_single(dev, scrq->msg_token, 4 * PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nfree_pages((unsigned long)scrq->msgs, 2);\r\nkfree(scrq);\r\n}\r\nstatic struct ibmvnic_sub_crq_queue *init_sub_crq_queue(struct ibmvnic_adapter\r\n*adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_sub_crq_queue *scrq;\r\nint rc;\r\nscrq = kmalloc(sizeof(*scrq), GFP_ATOMIC);\r\nif (!scrq)\r\nreturn NULL;\r\nscrq->msgs = (union sub_crq *)__get_free_pages(GFP_KERNEL, 2);\r\nmemset(scrq->msgs, 0, 4 * PAGE_SIZE);\r\nif (!scrq->msgs) {\r\ndev_warn(dev, "Couldn't allocate crq queue messages page\n");\r\ngoto zero_page_failed;\r\n}\r\nscrq->msg_token = dma_map_single(dev, scrq->msgs, 4 * PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(dev, scrq->msg_token)) {\r\ndev_warn(dev, "Couldn't map crq queue messages page\n");\r\ngoto map_failed;\r\n}\r\nrc = h_reg_sub_crq(adapter->vdev->unit_address, scrq->msg_token,\r\n4 * PAGE_SIZE, &scrq->crq_num, &scrq->hw_irq);\r\nif (rc == H_RESOURCE)\r\nrc = ibmvnic_reset_crq(adapter);\r\nif (rc == H_CLOSED) {\r\ndev_warn(dev, "Partner adapter not ready, waiting.\n");\r\n} else if (rc) {\r\ndev_warn(dev, "Error %d registering sub-crq\n", rc);\r\ngoto reg_failed;\r\n}\r\nscrq->irq = irq_create_mapping(NULL, scrq->hw_irq);\r\nif (scrq->irq == NO_IRQ) {\r\ndev_err(dev, "Error mapping irq\n");\r\ngoto map_irq_failed;\r\n}\r\nscrq->adapter = adapter;\r\nscrq->size = 4 * PAGE_SIZE / sizeof(*scrq->msgs);\r\nscrq->cur = 0;\r\nscrq->rx_skb_top = NULL;\r\nspin_lock_init(&scrq->lock);\r\nnetdev_dbg(adapter->netdev,\r\n"sub-crq initialized, num %lx, hw_irq=%lx, irq=%x\n",\r\nscrq->crq_num, scrq->hw_irq, scrq->irq);\r\nreturn scrq;\r\nmap_irq_failed:\r\ndo {\r\nrc = plpar_hcall_norets(H_FREE_SUB_CRQ,\r\nadapter->vdev->unit_address,\r\nscrq->crq_num);\r\n} while (rc == H_BUSY || H_IS_LONG_BUSY(rc));\r\nreg_failed:\r\ndma_unmap_single(dev, scrq->msg_token, 4 * PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nmap_failed:\r\nfree_pages((unsigned long)scrq->msgs, 2);\r\nzero_page_failed:\r\nkfree(scrq);\r\nreturn NULL;\r\n}\r\nstatic void release_sub_crqs(struct ibmvnic_adapter *adapter)\r\n{\r\nint i;\r\nif (adapter->tx_scrq) {\r\nfor (i = 0; i < adapter->req_tx_queues; i++)\r\nif (adapter->tx_scrq[i]) {\r\nfree_irq(adapter->tx_scrq[i]->irq,\r\nadapter->tx_scrq[i]);\r\nrelease_sub_crq_queue(adapter,\r\nadapter->tx_scrq[i]);\r\n}\r\nadapter->tx_scrq = NULL;\r\n}\r\nif (adapter->rx_scrq) {\r\nfor (i = 0; i < adapter->req_rx_queues; i++)\r\nif (adapter->rx_scrq[i]) {\r\nfree_irq(adapter->rx_scrq[i]->irq,\r\nadapter->rx_scrq[i]);\r\nrelease_sub_crq_queue(adapter,\r\nadapter->rx_scrq[i]);\r\n}\r\nadapter->rx_scrq = NULL;\r\n}\r\nadapter->requested_caps = 0;\r\n}\r\nstatic int disable_scrq_irq(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_sub_crq_queue *scrq)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nunsigned long rc;\r\nrc = plpar_hcall_norets(H_VIOCTL, adapter->vdev->unit_address,\r\nH_DISABLE_VIO_INTERRUPT, scrq->hw_irq, 0, 0);\r\nif (rc)\r\ndev_err(dev, "Couldn't disable scrq irq 0x%lx. rc=%ld\n",\r\nscrq->hw_irq, rc);\r\nreturn rc;\r\n}\r\nstatic int enable_scrq_irq(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_sub_crq_queue *scrq)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nunsigned long rc;\r\nif (scrq->hw_irq > 0x100000000ULL) {\r\ndev_err(dev, "bad hw_irq = %lx\n", scrq->hw_irq);\r\nreturn 1;\r\n}\r\nrc = plpar_hcall_norets(H_VIOCTL, adapter->vdev->unit_address,\r\nH_ENABLE_VIO_INTERRUPT, scrq->hw_irq, 0, 0);\r\nif (rc)\r\ndev_err(dev, "Couldn't enable scrq irq 0x%lx. rc=%ld\n",\r\nscrq->hw_irq, rc);\r\nreturn rc;\r\n}\r\nstatic int ibmvnic_complete_tx(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_sub_crq_queue *scrq)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_tx_buff *txbuff;\r\nunion sub_crq *next;\r\nint index;\r\nint i, j;\r\nrestart_loop:\r\nwhile (pending_scrq(adapter, scrq)) {\r\nunsigned int pool = scrq->pool_index;\r\nnext = ibmvnic_next_scrq(adapter, scrq);\r\nfor (i = 0; i < next->tx_comp.num_comps; i++) {\r\nif (next->tx_comp.rcs[i]) {\r\ndev_err(dev, "tx error %x\n",\r\nnext->tx_comp.rcs[i]);\r\ncontinue;\r\n}\r\nindex = be32_to_cpu(next->tx_comp.correlators[i]);\r\ntxbuff = &adapter->tx_pool[pool].tx_buff[index];\r\nfor (j = 0; j < IBMVNIC_MAX_FRAGS_PER_CRQ; j++) {\r\nif (!txbuff->data_dma[j])\r\ncontinue;\r\ntxbuff->data_dma[j] = 0;\r\ntxbuff->used_bounce = false;\r\n}\r\nif (txbuff->last_frag)\r\ndev_kfree_skb_any(txbuff->skb);\r\nadapter->tx_pool[pool].free_map[adapter->tx_pool[pool].\r\nproducer_index] = index;\r\nadapter->tx_pool[pool].producer_index =\r\n(adapter->tx_pool[pool].producer_index + 1) %\r\nadapter->max_tx_entries_per_subcrq;\r\n}\r\nnext->tx_comp.first = 0;\r\n}\r\nenable_scrq_irq(adapter, scrq);\r\nif (pending_scrq(adapter, scrq)) {\r\ndisable_scrq_irq(adapter, scrq);\r\ngoto restart_loop;\r\n}\r\nreturn 0;\r\n}\r\nstatic irqreturn_t ibmvnic_interrupt_tx(int irq, void *instance)\r\n{\r\nstruct ibmvnic_sub_crq_queue *scrq = instance;\r\nstruct ibmvnic_adapter *adapter = scrq->adapter;\r\ndisable_scrq_irq(adapter, scrq);\r\nibmvnic_complete_tx(adapter, scrq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ibmvnic_interrupt_rx(int irq, void *instance)\r\n{\r\nstruct ibmvnic_sub_crq_queue *scrq = instance;\r\nstruct ibmvnic_adapter *adapter = scrq->adapter;\r\nif (napi_schedule_prep(&adapter->napi[scrq->scrq_num])) {\r\ndisable_scrq_irq(adapter, scrq);\r\n__napi_schedule(&adapter->napi[scrq->scrq_num]);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void init_sub_crqs(struct ibmvnic_adapter *adapter, int retry)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_sub_crq_queue **allqueues;\r\nint registered_queues = 0;\r\nunion ibmvnic_crq crq;\r\nint total_queues;\r\nint more = 0;\r\nint i, j;\r\nint rc;\r\nif (!retry) {\r\nint entries_page = 4 * PAGE_SIZE / (sizeof(u64) * 4);\r\nif (adapter->min_tx_entries_per_subcrq > entries_page ||\r\nadapter->min_rx_add_entries_per_subcrq > entries_page) {\r\ndev_err(dev, "Fatal, invalid entries per sub-crq\n");\r\ngoto allqueues_failed;\r\n}\r\nadapter->req_tx_entries_per_subcrq =\r\nadapter->max_tx_entries_per_subcrq > entries_page ?\r\nentries_page : adapter->max_tx_entries_per_subcrq;\r\nadapter->req_rx_add_entries_per_subcrq =\r\nadapter->max_rx_add_entries_per_subcrq > entries_page ?\r\nentries_page : adapter->max_rx_add_entries_per_subcrq;\r\nadapter->req_tx_queues = adapter->min_tx_queues;\r\nadapter->req_rx_queues = adapter->min_rx_queues;\r\nadapter->req_rx_add_queues = adapter->min_rx_add_queues;\r\nadapter->req_mtu = adapter->max_mtu;\r\n}\r\ntotal_queues = adapter->req_tx_queues + adapter->req_rx_queues;\r\nallqueues = kcalloc(total_queues, sizeof(*allqueues), GFP_ATOMIC);\r\nif (!allqueues)\r\ngoto allqueues_failed;\r\nfor (i = 0; i < total_queues; i++) {\r\nallqueues[i] = init_sub_crq_queue(adapter);\r\nif (!allqueues[i]) {\r\ndev_warn(dev, "Couldn't allocate all sub-crqs\n");\r\nbreak;\r\n}\r\nregistered_queues++;\r\n}\r\nif (registered_queues <\r\nadapter->min_tx_queues + adapter->min_rx_queues) {\r\ndev_err(dev, "Fatal: Couldn't init min number of sub-crqs\n");\r\ngoto tx_failed;\r\n}\r\nfor (i = 0; i < total_queues - registered_queues + more ; i++) {\r\nnetdev_dbg(adapter->netdev, "Reducing number of queues\n");\r\nswitch (i % 3) {\r\ncase 0:\r\nif (adapter->req_rx_queues > adapter->min_rx_queues)\r\nadapter->req_rx_queues--;\r\nelse\r\nmore++;\r\nbreak;\r\ncase 1:\r\nif (adapter->req_tx_queues > adapter->min_tx_queues)\r\nadapter->req_tx_queues--;\r\nelse\r\nmore++;\r\nbreak;\r\n}\r\n}\r\nadapter->tx_scrq = kcalloc(adapter->req_tx_queues,\r\nsizeof(*adapter->tx_scrq), GFP_ATOMIC);\r\nif (!adapter->tx_scrq)\r\ngoto tx_failed;\r\nfor (i = 0; i < adapter->req_tx_queues; i++) {\r\nadapter->tx_scrq[i] = allqueues[i];\r\nadapter->tx_scrq[i]->pool_index = i;\r\nrc = request_irq(adapter->tx_scrq[i]->irq, ibmvnic_interrupt_tx,\r\n0, "ibmvnic_tx", adapter->tx_scrq[i]);\r\nif (rc) {\r\ndev_err(dev, "Couldn't register tx irq 0x%x. rc=%d\n",\r\nadapter->tx_scrq[i]->irq, rc);\r\ngoto req_tx_irq_failed;\r\n}\r\n}\r\nadapter->rx_scrq = kcalloc(adapter->req_rx_queues,\r\nsizeof(*adapter->rx_scrq), GFP_ATOMIC);\r\nif (!adapter->rx_scrq)\r\ngoto rx_failed;\r\nfor (i = 0; i < adapter->req_rx_queues; i++) {\r\nadapter->rx_scrq[i] = allqueues[i + adapter->req_tx_queues];\r\nadapter->rx_scrq[i]->scrq_num = i;\r\nrc = request_irq(adapter->rx_scrq[i]->irq, ibmvnic_interrupt_rx,\r\n0, "ibmvnic_rx", adapter->rx_scrq[i]);\r\nif (rc) {\r\ndev_err(dev, "Couldn't register rx irq 0x%x. rc=%d\n",\r\nadapter->rx_scrq[i]->irq, rc);\r\ngoto req_rx_irq_failed;\r\n}\r\n}\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.request_capability.first = IBMVNIC_CRQ_CMD;\r\ncrq.request_capability.cmd = REQUEST_CAPABILITY;\r\ncrq.request_capability.capability = cpu_to_be16(REQ_TX_QUEUES);\r\ncrq.request_capability.number = cpu_to_be64(adapter->req_tx_queues);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.request_capability.capability = cpu_to_be16(REQ_RX_QUEUES);\r\ncrq.request_capability.number = cpu_to_be64(adapter->req_rx_queues);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.request_capability.capability = cpu_to_be16(REQ_RX_ADD_QUEUES);\r\ncrq.request_capability.number = cpu_to_be64(adapter->req_rx_add_queues);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.request_capability.capability =\r\ncpu_to_be16(REQ_TX_ENTRIES_PER_SUBCRQ);\r\ncrq.request_capability.number =\r\ncpu_to_be64(adapter->req_tx_entries_per_subcrq);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.request_capability.capability =\r\ncpu_to_be16(REQ_RX_ADD_ENTRIES_PER_SUBCRQ);\r\ncrq.request_capability.number =\r\ncpu_to_be64(adapter->req_rx_add_entries_per_subcrq);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.request_capability.capability = cpu_to_be16(REQ_MTU);\r\ncrq.request_capability.number = cpu_to_be64(adapter->req_mtu);\r\nibmvnic_send_crq(adapter, &crq);\r\nif (adapter->netdev->flags & IFF_PROMISC) {\r\nif (adapter->promisc_supported) {\r\ncrq.request_capability.capability =\r\ncpu_to_be16(PROMISC_REQUESTED);\r\ncrq.request_capability.number = cpu_to_be64(1);\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\n} else {\r\ncrq.request_capability.capability =\r\ncpu_to_be16(PROMISC_REQUESTED);\r\ncrq.request_capability.number = cpu_to_be64(0);\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\nkfree(allqueues);\r\nreturn;\r\nreq_rx_irq_failed:\r\nfor (j = 0; j < i; j++)\r\nfree_irq(adapter->rx_scrq[j]->irq, adapter->rx_scrq[j]);\r\ni = adapter->req_tx_queues;\r\nreq_tx_irq_failed:\r\nfor (j = 0; j < i; j++)\r\nfree_irq(adapter->tx_scrq[j]->irq, adapter->tx_scrq[j]);\r\nkfree(adapter->rx_scrq);\r\nadapter->rx_scrq = NULL;\r\nrx_failed:\r\nkfree(adapter->tx_scrq);\r\nadapter->tx_scrq = NULL;\r\ntx_failed:\r\nfor (i = 0; i < registered_queues; i++)\r\nrelease_sub_crq_queue(adapter, allqueues[i]);\r\nkfree(allqueues);\r\nallqueues_failed:\r\nibmvnic_remove(adapter->vdev);\r\n}\r\nstatic int pending_scrq(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_sub_crq_queue *scrq)\r\n{\r\nunion sub_crq *entry = &scrq->msgs[scrq->cur];\r\nif (entry->generic.first & IBMVNIC_CRQ_CMD_RSP || adapter->closing)\r\nreturn 1;\r\nelse\r\nreturn 0;\r\n}\r\nstatic union sub_crq *ibmvnic_next_scrq(struct ibmvnic_adapter *adapter,\r\nstruct ibmvnic_sub_crq_queue *scrq)\r\n{\r\nunion sub_crq *entry;\r\nunsigned long flags;\r\nspin_lock_irqsave(&scrq->lock, flags);\r\nentry = &scrq->msgs[scrq->cur];\r\nif (entry->generic.first & IBMVNIC_CRQ_CMD_RSP) {\r\nif (++scrq->cur == scrq->size)\r\nscrq->cur = 0;\r\n} else {\r\nentry = NULL;\r\n}\r\nspin_unlock_irqrestore(&scrq->lock, flags);\r\nreturn entry;\r\n}\r\nstatic union ibmvnic_crq *ibmvnic_next_crq(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct ibmvnic_crq_queue *queue = &adapter->crq;\r\nunion ibmvnic_crq *crq;\r\ncrq = &queue->msgs[queue->cur];\r\nif (crq->generic.first & IBMVNIC_CRQ_CMD_RSP) {\r\nif (++queue->cur == queue->size)\r\nqueue->cur = 0;\r\n} else {\r\ncrq = NULL;\r\n}\r\nreturn crq;\r\n}\r\nstatic int send_subcrq(struct ibmvnic_adapter *adapter, u64 remote_handle,\r\nunion sub_crq *sub_crq)\r\n{\r\nunsigned int ua = adapter->vdev->unit_address;\r\nstruct device *dev = &adapter->vdev->dev;\r\nu64 *u64_crq = (u64 *)sub_crq;\r\nint rc;\r\nnetdev_dbg(adapter->netdev,\r\n"Sending sCRQ %016lx: %016lx %016lx %016lx %016lx\n",\r\n(unsigned long int)cpu_to_be64(remote_handle),\r\n(unsigned long int)cpu_to_be64(u64_crq[0]),\r\n(unsigned long int)cpu_to_be64(u64_crq[1]),\r\n(unsigned long int)cpu_to_be64(u64_crq[2]),\r\n(unsigned long int)cpu_to_be64(u64_crq[3]));\r\nmb();\r\nrc = plpar_hcall_norets(H_SEND_SUB_CRQ, ua,\r\ncpu_to_be64(remote_handle),\r\ncpu_to_be64(u64_crq[0]),\r\ncpu_to_be64(u64_crq[1]),\r\ncpu_to_be64(u64_crq[2]),\r\ncpu_to_be64(u64_crq[3]));\r\nif (rc) {\r\nif (rc == H_CLOSED)\r\ndev_warn(dev, "CRQ Queue closed\n");\r\ndev_err(dev, "Send error (rc=%d)\n", rc);\r\n}\r\nreturn rc;\r\n}\r\nstatic int ibmvnic_send_crq(struct ibmvnic_adapter *adapter,\r\nunion ibmvnic_crq *crq)\r\n{\r\nunsigned int ua = adapter->vdev->unit_address;\r\nstruct device *dev = &adapter->vdev->dev;\r\nu64 *u64_crq = (u64 *)crq;\r\nint rc;\r\nnetdev_dbg(adapter->netdev, "Sending CRQ: %016lx %016lx\n",\r\n(unsigned long int)cpu_to_be64(u64_crq[0]),\r\n(unsigned long int)cpu_to_be64(u64_crq[1]));\r\nmb();\r\nrc = plpar_hcall_norets(H_SEND_CRQ, ua,\r\ncpu_to_be64(u64_crq[0]),\r\ncpu_to_be64(u64_crq[1]));\r\nif (rc) {\r\nif (rc == H_CLOSED)\r\ndev_warn(dev, "CRQ Queue closed\n");\r\ndev_warn(dev, "Send error (rc=%d)\n", rc);\r\n}\r\nreturn rc;\r\n}\r\nstatic int ibmvnic_send_crq_init(struct ibmvnic_adapter *adapter)\r\n{\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.generic.first = IBMVNIC_CRQ_INIT_CMD;\r\ncrq.generic.cmd = IBMVNIC_CRQ_INIT;\r\nnetdev_dbg(adapter->netdev, "Sending CRQ init\n");\r\nreturn ibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic int ibmvnic_send_crq_init_complete(struct ibmvnic_adapter *adapter)\r\n{\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.generic.first = IBMVNIC_CRQ_INIT_CMD;\r\ncrq.generic.cmd = IBMVNIC_CRQ_INIT_COMPLETE;\r\nnetdev_dbg(adapter->netdev, "Sending CRQ init complete\n");\r\nreturn ibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic int send_version_xchg(struct ibmvnic_adapter *adapter)\r\n{\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.version_exchange.first = IBMVNIC_CRQ_CMD;\r\ncrq.version_exchange.cmd = VERSION_EXCHANGE;\r\ncrq.version_exchange.version = cpu_to_be16(ibmvnic_version);\r\nreturn ibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic void send_login(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct ibmvnic_login_rsp_buffer *login_rsp_buffer;\r\nstruct ibmvnic_login_buffer *login_buffer;\r\nstruct ibmvnic_inflight_cmd *inflight_cmd;\r\nstruct device *dev = &adapter->vdev->dev;\r\ndma_addr_t rsp_buffer_token;\r\ndma_addr_t buffer_token;\r\nsize_t rsp_buffer_size;\r\nunion ibmvnic_crq crq;\r\nunsigned long flags;\r\nsize_t buffer_size;\r\n__be64 *tx_list_p;\r\n__be64 *rx_list_p;\r\nint i;\r\nbuffer_size =\r\nsizeof(struct ibmvnic_login_buffer) +\r\nsizeof(u64) * (adapter->req_tx_queues + adapter->req_rx_queues);\r\nlogin_buffer = kmalloc(buffer_size, GFP_ATOMIC);\r\nif (!login_buffer)\r\ngoto buf_alloc_failed;\r\nbuffer_token = dma_map_single(dev, login_buffer, buffer_size,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, buffer_token)) {\r\ndev_err(dev, "Couldn't map login buffer\n");\r\ngoto buf_map_failed;\r\n}\r\nrsp_buffer_size =\r\nsizeof(struct ibmvnic_login_rsp_buffer) +\r\nsizeof(u64) * (adapter->req_tx_queues +\r\nadapter->req_rx_queues *\r\nadapter->req_rx_add_queues + adapter->\r\nreq_rx_add_queues) +\r\nsizeof(u8) * (IBMVNIC_TX_DESC_VERSIONS);\r\nlogin_rsp_buffer = kmalloc(rsp_buffer_size, GFP_ATOMIC);\r\nif (!login_rsp_buffer)\r\ngoto buf_rsp_alloc_failed;\r\nrsp_buffer_token = dma_map_single(dev, login_rsp_buffer,\r\nrsp_buffer_size, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(dev, rsp_buffer_token)) {\r\ndev_err(dev, "Couldn't map login rsp buffer\n");\r\ngoto buf_rsp_map_failed;\r\n}\r\ninflight_cmd = kmalloc(sizeof(*inflight_cmd), GFP_ATOMIC);\r\nif (!inflight_cmd) {\r\ndev_err(dev, "Couldn't allocate inflight_cmd\n");\r\ngoto inflight_alloc_failed;\r\n}\r\nadapter->login_buf = login_buffer;\r\nadapter->login_buf_token = buffer_token;\r\nadapter->login_buf_sz = buffer_size;\r\nadapter->login_rsp_buf = login_rsp_buffer;\r\nadapter->login_rsp_buf_token = rsp_buffer_token;\r\nadapter->login_rsp_buf_sz = rsp_buffer_size;\r\nlogin_buffer->len = cpu_to_be32(buffer_size);\r\nlogin_buffer->version = cpu_to_be32(INITIAL_VERSION_LB);\r\nlogin_buffer->num_txcomp_subcrqs = cpu_to_be32(adapter->req_tx_queues);\r\nlogin_buffer->off_txcomp_subcrqs =\r\ncpu_to_be32(sizeof(struct ibmvnic_login_buffer));\r\nlogin_buffer->num_rxcomp_subcrqs = cpu_to_be32(adapter->req_rx_queues);\r\nlogin_buffer->off_rxcomp_subcrqs =\r\ncpu_to_be32(sizeof(struct ibmvnic_login_buffer) +\r\nsizeof(u64) * adapter->req_tx_queues);\r\nlogin_buffer->login_rsp_ioba = cpu_to_be32(rsp_buffer_token);\r\nlogin_buffer->login_rsp_len = cpu_to_be32(rsp_buffer_size);\r\ntx_list_p = (__be64 *)((char *)login_buffer +\r\nsizeof(struct ibmvnic_login_buffer));\r\nrx_list_p = (__be64 *)((char *)login_buffer +\r\nsizeof(struct ibmvnic_login_buffer) +\r\nsizeof(u64) * adapter->req_tx_queues);\r\nfor (i = 0; i < adapter->req_tx_queues; i++) {\r\nif (adapter->tx_scrq[i]) {\r\ntx_list_p[i] = cpu_to_be64(adapter->tx_scrq[i]->\r\ncrq_num);\r\n}\r\n}\r\nfor (i = 0; i < adapter->req_rx_queues; i++) {\r\nif (adapter->rx_scrq[i]) {\r\nrx_list_p[i] = cpu_to_be64(adapter->rx_scrq[i]->\r\ncrq_num);\r\n}\r\n}\r\nnetdev_dbg(adapter->netdev, "Login Buffer:\n");\r\nfor (i = 0; i < (adapter->login_buf_sz - 1) / 8 + 1; i++) {\r\nnetdev_dbg(adapter->netdev, "%016lx\n",\r\n((unsigned long int *)(adapter->login_buf))[i]);\r\n}\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.login.first = IBMVNIC_CRQ_CMD;\r\ncrq.login.cmd = LOGIN;\r\ncrq.login.ioba = cpu_to_be32(buffer_token);\r\ncrq.login.len = cpu_to_be32(buffer_size);\r\nmemcpy(&inflight_cmd->crq, &crq, sizeof(crq));\r\nspin_lock_irqsave(&adapter->inflight_lock, flags);\r\nlist_add_tail(&inflight_cmd->list, &adapter->inflight);\r\nspin_unlock_irqrestore(&adapter->inflight_lock, flags);\r\nibmvnic_send_crq(adapter, &crq);\r\nreturn;\r\ninflight_alloc_failed:\r\ndma_unmap_single(dev, rsp_buffer_token, rsp_buffer_size,\r\nDMA_FROM_DEVICE);\r\nbuf_rsp_map_failed:\r\nkfree(login_rsp_buffer);\r\nbuf_rsp_alloc_failed:\r\ndma_unmap_single(dev, buffer_token, buffer_size, DMA_TO_DEVICE);\r\nbuf_map_failed:\r\nkfree(login_buffer);\r\nbuf_alloc_failed:\r\nreturn;\r\n}\r\nstatic void send_request_map(struct ibmvnic_adapter *adapter, dma_addr_t addr,\r\nu32 len, u8 map_id)\r\n{\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.request_map.first = IBMVNIC_CRQ_CMD;\r\ncrq.request_map.cmd = REQUEST_MAP;\r\ncrq.request_map.map_id = map_id;\r\ncrq.request_map.ioba = cpu_to_be32(addr);\r\ncrq.request_map.len = cpu_to_be32(len);\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic void send_request_unmap(struct ibmvnic_adapter *adapter, u8 map_id)\r\n{\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.request_unmap.first = IBMVNIC_CRQ_CMD;\r\ncrq.request_unmap.cmd = REQUEST_UNMAP;\r\ncrq.request_unmap.map_id = map_id;\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic void send_map_query(struct ibmvnic_adapter *adapter)\r\n{\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.query_map.first = IBMVNIC_CRQ_CMD;\r\ncrq.query_map.cmd = QUERY_MAP;\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic void send_cap_queries(struct ibmvnic_adapter *adapter)\r\n{\r\nunion ibmvnic_crq crq;\r\natomic_set(&adapter->running_cap_queries, 0);\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.query_capability.first = IBMVNIC_CRQ_CMD;\r\ncrq.query_capability.cmd = QUERY_CAPABILITY;\r\ncrq.query_capability.capability = cpu_to_be16(MIN_TX_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MIN_RX_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MIN_RX_ADD_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MAX_TX_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MAX_RX_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MAX_RX_ADD_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability =\r\ncpu_to_be16(MIN_TX_ENTRIES_PER_SUBCRQ);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability =\r\ncpu_to_be16(MIN_RX_ADD_ENTRIES_PER_SUBCRQ);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability =\r\ncpu_to_be16(MAX_TX_ENTRIES_PER_SUBCRQ);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability =\r\ncpu_to_be16(MAX_RX_ADD_ENTRIES_PER_SUBCRQ);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(TCP_IP_OFFLOAD);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(PROMISC_SUPPORTED);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MIN_MTU);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MAX_MTU);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MAX_MULTICAST_FILTERS);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(VLAN_HEADER_INSERTION);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(MAX_TX_SG_ENTRIES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(RX_SG_SUPPORTED);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(OPT_TX_COMP_SUB_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(OPT_RX_COMP_QUEUES);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability =\r\ncpu_to_be16(OPT_RX_BUFADD_Q_PER_RX_COMP_Q);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability =\r\ncpu_to_be16(OPT_TX_ENTRIES_PER_SUBCRQ);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability =\r\ncpu_to_be16(OPT_RXBA_ENTRIES_PER_SUBCRQ);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\ncrq.query_capability.capability = cpu_to_be16(TX_RX_DESC_REQ);\r\natomic_inc(&adapter->running_cap_queries);\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic void handle_query_ip_offload_rsp(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_query_ip_offload_buffer *buf = &adapter->ip_offload_buf;\r\nunion ibmvnic_crq crq;\r\nint i;\r\ndma_unmap_single(dev, adapter->ip_offload_tok,\r\nsizeof(adapter->ip_offload_buf), DMA_FROM_DEVICE);\r\nnetdev_dbg(adapter->netdev, "Query IP Offload Buffer:\n");\r\nfor (i = 0; i < (sizeof(adapter->ip_offload_buf) - 1) / 8 + 1; i++)\r\nnetdev_dbg(adapter->netdev, "%016lx\n",\r\n((unsigned long int *)(buf))[i]);\r\nnetdev_dbg(adapter->netdev, "ipv4_chksum = %d\n", buf->ipv4_chksum);\r\nnetdev_dbg(adapter->netdev, "ipv6_chksum = %d\n", buf->ipv6_chksum);\r\nnetdev_dbg(adapter->netdev, "tcp_ipv4_chksum = %d\n",\r\nbuf->tcp_ipv4_chksum);\r\nnetdev_dbg(adapter->netdev, "tcp_ipv6_chksum = %d\n",\r\nbuf->tcp_ipv6_chksum);\r\nnetdev_dbg(adapter->netdev, "udp_ipv4_chksum = %d\n",\r\nbuf->udp_ipv4_chksum);\r\nnetdev_dbg(adapter->netdev, "udp_ipv6_chksum = %d\n",\r\nbuf->udp_ipv6_chksum);\r\nnetdev_dbg(adapter->netdev, "large_tx_ipv4 = %d\n",\r\nbuf->large_tx_ipv4);\r\nnetdev_dbg(adapter->netdev, "large_tx_ipv6 = %d\n",\r\nbuf->large_tx_ipv6);\r\nnetdev_dbg(adapter->netdev, "large_rx_ipv4 = %d\n",\r\nbuf->large_rx_ipv4);\r\nnetdev_dbg(adapter->netdev, "large_rx_ipv6 = %d\n",\r\nbuf->large_rx_ipv6);\r\nnetdev_dbg(adapter->netdev, "max_ipv4_hdr_sz = %d\n",\r\nbuf->max_ipv4_header_size);\r\nnetdev_dbg(adapter->netdev, "max_ipv6_hdr_sz = %d\n",\r\nbuf->max_ipv6_header_size);\r\nnetdev_dbg(adapter->netdev, "max_tcp_hdr_size = %d\n",\r\nbuf->max_tcp_header_size);\r\nnetdev_dbg(adapter->netdev, "max_udp_hdr_size = %d\n",\r\nbuf->max_udp_header_size);\r\nnetdev_dbg(adapter->netdev, "max_large_tx_size = %d\n",\r\nbuf->max_large_tx_size);\r\nnetdev_dbg(adapter->netdev, "max_large_rx_size = %d\n",\r\nbuf->max_large_rx_size);\r\nnetdev_dbg(adapter->netdev, "ipv6_ext_hdr = %d\n",\r\nbuf->ipv6_extension_header);\r\nnetdev_dbg(adapter->netdev, "tcp_pseudosum_req = %d\n",\r\nbuf->tcp_pseudosum_req);\r\nnetdev_dbg(adapter->netdev, "num_ipv6_ext_hd = %d\n",\r\nbuf->num_ipv6_ext_headers);\r\nnetdev_dbg(adapter->netdev, "off_ipv6_ext_hd = %d\n",\r\nbuf->off_ipv6_ext_headers);\r\nadapter->ip_offload_ctrl_tok =\r\ndma_map_single(dev, &adapter->ip_offload_ctrl,\r\nsizeof(adapter->ip_offload_ctrl), DMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, adapter->ip_offload_ctrl_tok)) {\r\ndev_err(dev, "Couldn't map ip offload control buffer\n");\r\nreturn;\r\n}\r\nadapter->ip_offload_ctrl.version = cpu_to_be32(INITIAL_VERSION_IOB);\r\nadapter->ip_offload_ctrl.tcp_ipv4_chksum = buf->tcp_ipv4_chksum;\r\nadapter->ip_offload_ctrl.udp_ipv4_chksum = buf->udp_ipv4_chksum;\r\nadapter->ip_offload_ctrl.tcp_ipv6_chksum = buf->tcp_ipv6_chksum;\r\nadapter->ip_offload_ctrl.udp_ipv6_chksum = buf->udp_ipv6_chksum;\r\nadapter->ip_offload_ctrl.large_tx_ipv4 = 0;\r\nadapter->ip_offload_ctrl.large_tx_ipv6 = 0;\r\nadapter->ip_offload_ctrl.large_rx_ipv4 = 0;\r\nadapter->ip_offload_ctrl.large_rx_ipv6 = 0;\r\nadapter->netdev->features = NETIF_F_GSO;\r\nif (buf->tcp_ipv4_chksum || buf->udp_ipv4_chksum)\r\nadapter->netdev->features |= NETIF_F_IP_CSUM;\r\nif (buf->tcp_ipv6_chksum || buf->udp_ipv6_chksum)\r\nadapter->netdev->features |= NETIF_F_IPV6_CSUM;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.control_ip_offload.first = IBMVNIC_CRQ_CMD;\r\ncrq.control_ip_offload.cmd = CONTROL_IP_OFFLOAD;\r\ncrq.control_ip_offload.len =\r\ncpu_to_be32(sizeof(adapter->ip_offload_ctrl));\r\ncrq.control_ip_offload.ioba = cpu_to_be32(adapter->ip_offload_ctrl_tok);\r\nibmvnic_send_crq(adapter, &crq);\r\n}\r\nstatic void handle_error_info_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_error_buff *error_buff;\r\nunsigned long flags;\r\nbool found = false;\r\nint i;\r\nif (!crq->request_error_rsp.rc.code) {\r\ndev_info(dev, "Request Error Rsp returned with rc=%x\n",\r\ncrq->request_error_rsp.rc.code);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&adapter->error_list_lock, flags);\r\nlist_for_each_entry(error_buff, &adapter->errors, list)\r\nif (error_buff->error_id == crq->request_error_rsp.error_id) {\r\nfound = true;\r\nlist_del(&error_buff->list);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&adapter->error_list_lock, flags);\r\nif (!found) {\r\ndev_err(dev, "Couldn't find error id %x\n",\r\ncrq->request_error_rsp.error_id);\r\nreturn;\r\n}\r\ndev_err(dev, "Detailed info for error id %x:",\r\ncrq->request_error_rsp.error_id);\r\nfor (i = 0; i < error_buff->len; i++) {\r\npr_cont("%02x", (int)error_buff->buff[i]);\r\nif (i % 8 == 7)\r\npr_cont(" ");\r\n}\r\npr_cont("\n");\r\ndma_unmap_single(dev, error_buff->dma, error_buff->len,\r\nDMA_FROM_DEVICE);\r\nkfree(error_buff->buff);\r\nkfree(error_buff);\r\n}\r\nstatic void handle_dump_size_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nint len = be32_to_cpu(crq->request_dump_size_rsp.len);\r\nstruct ibmvnic_inflight_cmd *inflight_cmd;\r\nstruct device *dev = &adapter->vdev->dev;\r\nunion ibmvnic_crq newcrq;\r\nunsigned long flags;\r\nadapter->dump_data = kmalloc(len, GFP_KERNEL);\r\nif (!adapter->dump_data) {\r\ncomplete(&adapter->fw_done);\r\nreturn;\r\n}\r\nadapter->dump_data_token = dma_map_single(dev, adapter->dump_data, len,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(dev, adapter->dump_data_token)) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\ndev_err(dev, "Couldn't map dump data\n");\r\nkfree(adapter->dump_data);\r\ncomplete(&adapter->fw_done);\r\nreturn;\r\n}\r\ninflight_cmd = kmalloc(sizeof(*inflight_cmd), GFP_ATOMIC);\r\nif (!inflight_cmd) {\r\ndma_unmap_single(dev, adapter->dump_data_token, len,\r\nDMA_FROM_DEVICE);\r\nkfree(adapter->dump_data);\r\ncomplete(&adapter->fw_done);\r\nreturn;\r\n}\r\nmemset(&newcrq, 0, sizeof(newcrq));\r\nnewcrq.request_dump.first = IBMVNIC_CRQ_CMD;\r\nnewcrq.request_dump.cmd = REQUEST_DUMP;\r\nnewcrq.request_dump.ioba = cpu_to_be32(adapter->dump_data_token);\r\nnewcrq.request_dump.len = cpu_to_be32(adapter->dump_data_size);\r\nmemcpy(&inflight_cmd->crq, &newcrq, sizeof(newcrq));\r\nspin_lock_irqsave(&adapter->inflight_lock, flags);\r\nlist_add_tail(&inflight_cmd->list, &adapter->inflight);\r\nspin_unlock_irqrestore(&adapter->inflight_lock, flags);\r\nibmvnic_send_crq(adapter, &newcrq);\r\n}\r\nstatic void handle_error_indication(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nint detail_len = be32_to_cpu(crq->error_indication.detail_error_sz);\r\nstruct ibmvnic_inflight_cmd *inflight_cmd;\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_error_buff *error_buff;\r\nunion ibmvnic_crq new_crq;\r\nunsigned long flags;\r\ndev_err(dev, "Firmware reports %serror id %x, cause %d\n",\r\ncrq->error_indication.\r\nflags & IBMVNIC_FATAL_ERROR ? "FATAL " : "",\r\ncrq->error_indication.error_id,\r\ncrq->error_indication.error_cause);\r\nerror_buff = kmalloc(sizeof(*error_buff), GFP_ATOMIC);\r\nif (!error_buff)\r\nreturn;\r\nerror_buff->buff = kmalloc(detail_len, GFP_ATOMIC);\r\nif (!error_buff->buff) {\r\nkfree(error_buff);\r\nreturn;\r\n}\r\nerror_buff->dma = dma_map_single(dev, error_buff->buff, detail_len,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(dev, error_buff->dma)) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\ndev_err(dev, "Couldn't map error buffer\n");\r\nkfree(error_buff->buff);\r\nkfree(error_buff);\r\nreturn;\r\n}\r\ninflight_cmd = kmalloc(sizeof(*inflight_cmd), GFP_ATOMIC);\r\nif (!inflight_cmd) {\r\ndma_unmap_single(dev, error_buff->dma, detail_len,\r\nDMA_FROM_DEVICE);\r\nkfree(error_buff->buff);\r\nkfree(error_buff);\r\nreturn;\r\n}\r\nerror_buff->len = detail_len;\r\nerror_buff->error_id = crq->error_indication.error_id;\r\nspin_lock_irqsave(&adapter->error_list_lock, flags);\r\nlist_add_tail(&error_buff->list, &adapter->errors);\r\nspin_unlock_irqrestore(&adapter->error_list_lock, flags);\r\nmemset(&new_crq, 0, sizeof(new_crq));\r\nnew_crq.request_error_info.first = IBMVNIC_CRQ_CMD;\r\nnew_crq.request_error_info.cmd = REQUEST_ERROR_INFO;\r\nnew_crq.request_error_info.ioba = cpu_to_be32(error_buff->dma);\r\nnew_crq.request_error_info.len = cpu_to_be32(detail_len);\r\nnew_crq.request_error_info.error_id = crq->error_indication.error_id;\r\nmemcpy(&inflight_cmd->crq, &crq, sizeof(crq));\r\nspin_lock_irqsave(&adapter->inflight_lock, flags);\r\nlist_add_tail(&inflight_cmd->list, &adapter->inflight);\r\nspin_unlock_irqrestore(&adapter->inflight_lock, flags);\r\nibmvnic_send_crq(adapter, &new_crq);\r\n}\r\nstatic void handle_change_mac_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct device *dev = &adapter->vdev->dev;\r\nlong rc;\r\nrc = crq->change_mac_addr_rsp.rc.code;\r\nif (rc) {\r\ndev_err(dev, "Error %ld in CHANGE_MAC_ADDR_RSP\n", rc);\r\nreturn;\r\n}\r\nmemcpy(netdev->dev_addr, &crq->change_mac_addr_rsp.mac_addr[0],\r\nETH_ALEN);\r\n}\r\nstatic void handle_request_cap_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nu64 *req_value;\r\nchar *name;\r\nswitch (be16_to_cpu(crq->request_capability_rsp.capability)) {\r\ncase REQ_TX_QUEUES:\r\nreq_value = &adapter->req_tx_queues;\r\nname = "tx";\r\nbreak;\r\ncase REQ_RX_QUEUES:\r\nreq_value = &adapter->req_rx_queues;\r\nname = "rx";\r\nbreak;\r\ncase REQ_RX_ADD_QUEUES:\r\nreq_value = &adapter->req_rx_add_queues;\r\nname = "rx_add";\r\nbreak;\r\ncase REQ_TX_ENTRIES_PER_SUBCRQ:\r\nreq_value = &adapter->req_tx_entries_per_subcrq;\r\nname = "tx_entries_per_subcrq";\r\nbreak;\r\ncase REQ_RX_ADD_ENTRIES_PER_SUBCRQ:\r\nreq_value = &adapter->req_rx_add_entries_per_subcrq;\r\nname = "rx_add_entries_per_subcrq";\r\nbreak;\r\ncase REQ_MTU:\r\nreq_value = &adapter->req_mtu;\r\nname = "mtu";\r\nbreak;\r\ncase PROMISC_REQUESTED:\r\nreq_value = &adapter->promisc;\r\nname = "promisc";\r\nbreak;\r\ndefault:\r\ndev_err(dev, "Got invalid cap request rsp %d\n",\r\ncrq->request_capability.capability);\r\nreturn;\r\n}\r\nswitch (crq->request_capability_rsp.rc.code) {\r\ncase SUCCESS:\r\nbreak;\r\ncase PARTIALSUCCESS:\r\ndev_info(dev, "req=%lld, rsp=%ld in %s queue, retrying.\n",\r\n*req_value,\r\n(long int)be32_to_cpu(crq->request_capability_rsp.\r\nnumber), name);\r\nrelease_sub_crqs(adapter);\r\n*req_value = be32_to_cpu(crq->request_capability_rsp.number);\r\ncomplete(&adapter->init_done);\r\nreturn;\r\ndefault:\r\ndev_err(dev, "Error %d in request cap rsp\n",\r\ncrq->request_capability_rsp.rc.code);\r\nreturn;\r\n}\r\nif (++adapter->requested_caps == 7) {\r\nunion ibmvnic_crq newcrq;\r\nint buf_sz = sizeof(struct ibmvnic_query_ip_offload_buffer);\r\nstruct ibmvnic_query_ip_offload_buffer *ip_offload_buf =\r\n&adapter->ip_offload_buf;\r\nadapter->ip_offload_tok = dma_map_single(dev, ip_offload_buf,\r\nbuf_sz,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(dev, adapter->ip_offload_tok)) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\ndev_err(dev, "Couldn't map offload buffer\n");\r\nreturn;\r\n}\r\nmemset(&newcrq, 0, sizeof(newcrq));\r\nnewcrq.query_ip_offload.first = IBMVNIC_CRQ_CMD;\r\nnewcrq.query_ip_offload.cmd = QUERY_IP_OFFLOAD;\r\nnewcrq.query_ip_offload.len = cpu_to_be32(buf_sz);\r\nnewcrq.query_ip_offload.ioba =\r\ncpu_to_be32(adapter->ip_offload_tok);\r\nibmvnic_send_crq(adapter, &newcrq);\r\n}\r\n}\r\nstatic int handle_login_rsp(union ibmvnic_crq *login_rsp_crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_login_rsp_buffer *login_rsp = adapter->login_rsp_buf;\r\nstruct ibmvnic_login_buffer *login = adapter->login_buf;\r\nunion ibmvnic_crq crq;\r\nint i;\r\ndma_unmap_single(dev, adapter->login_buf_token, adapter->login_buf_sz,\r\nDMA_BIDIRECTIONAL);\r\ndma_unmap_single(dev, adapter->login_rsp_buf_token,\r\nadapter->login_rsp_buf_sz, DMA_BIDIRECTIONAL);\r\nnetdev_dbg(adapter->netdev, "Login Response Buffer:\n");\r\nfor (i = 0; i < (adapter->login_rsp_buf_sz - 1) / 8 + 1; i++) {\r\nnetdev_dbg(adapter->netdev, "%016lx\n",\r\n((unsigned long int *)(adapter->login_rsp_buf))[i]);\r\n}\r\nif (login->num_txcomp_subcrqs != login_rsp->num_txsubm_subcrqs ||\r\n(be32_to_cpu(login->num_rxcomp_subcrqs) *\r\nadapter->req_rx_add_queues !=\r\nbe32_to_cpu(login_rsp->num_rxadd_subcrqs))) {\r\ndev_err(dev, "FATAL: Inconsistent login and login rsp\n");\r\nibmvnic_remove(adapter->vdev);\r\nreturn -EIO;\r\n}\r\ncomplete(&adapter->init_done);\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.request_ras_comp_num.first = IBMVNIC_CRQ_CMD;\r\ncrq.request_ras_comp_num.cmd = REQUEST_RAS_COMP_NUM;\r\nibmvnic_send_crq(adapter, &crq);\r\nreturn 0;\r\n}\r\nstatic void handle_request_map_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nu8 map_id = crq->request_map_rsp.map_id;\r\nint tx_subcrqs;\r\nint rx_subcrqs;\r\nlong rc;\r\nint i;\r\ntx_subcrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);\r\nrx_subcrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);\r\nrc = crq->request_map_rsp.rc.code;\r\nif (rc) {\r\ndev_err(dev, "Error %ld in REQUEST_MAP_RSP\n", rc);\r\nadapter->map_id--;\r\nfor (i = 0; i < tx_subcrqs; i++) {\r\nif (adapter->tx_pool[i].long_term_buff.map_id == map_id)\r\nadapter->tx_pool[i].long_term_buff.map_id = 0;\r\n}\r\nfor (i = 0; i < rx_subcrqs; i++) {\r\nif (adapter->rx_pool[i].long_term_buff.map_id == map_id)\r\nadapter->rx_pool[i].long_term_buff.map_id = 0;\r\n}\r\n}\r\ncomplete(&adapter->fw_done);\r\n}\r\nstatic void handle_request_unmap_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nlong rc;\r\nrc = crq->request_unmap_rsp.rc.code;\r\nif (rc)\r\ndev_err(dev, "Error %ld in REQUEST_UNMAP_RSP\n", rc);\r\n}\r\nstatic void handle_query_map_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct device *dev = &adapter->vdev->dev;\r\nlong rc;\r\nrc = crq->query_map_rsp.rc.code;\r\nif (rc) {\r\ndev_err(dev, "Error %ld in QUERY_MAP_RSP\n", rc);\r\nreturn;\r\n}\r\nnetdev_dbg(netdev, "page_size = %d\ntot_pages = %d\nfree_pages = %d\n",\r\ncrq->query_map_rsp.page_size, crq->query_map_rsp.tot_pages,\r\ncrq->query_map_rsp.free_pages);\r\n}\r\nstatic void handle_query_cap_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct device *dev = &adapter->vdev->dev;\r\nlong rc;\r\natomic_dec(&adapter->running_cap_queries);\r\nnetdev_dbg(netdev, "Outstanding queries: %d\n",\r\natomic_read(&adapter->running_cap_queries));\r\nrc = crq->query_capability.rc.code;\r\nif (rc) {\r\ndev_err(dev, "Error %ld in QUERY_CAP_RSP\n", rc);\r\ngoto out;\r\n}\r\nswitch (be16_to_cpu(crq->query_capability.capability)) {\r\ncase MIN_TX_QUEUES:\r\nadapter->min_tx_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "min_tx_queues = %lld\n",\r\nadapter->min_tx_queues);\r\nbreak;\r\ncase MIN_RX_QUEUES:\r\nadapter->min_rx_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "min_rx_queues = %lld\n",\r\nadapter->min_rx_queues);\r\nbreak;\r\ncase MIN_RX_ADD_QUEUES:\r\nadapter->min_rx_add_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "min_rx_add_queues = %lld\n",\r\nadapter->min_rx_add_queues);\r\nbreak;\r\ncase MAX_TX_QUEUES:\r\nadapter->max_tx_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_tx_queues = %lld\n",\r\nadapter->max_tx_queues);\r\nbreak;\r\ncase MAX_RX_QUEUES:\r\nadapter->max_rx_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_rx_queues = %lld\n",\r\nadapter->max_rx_queues);\r\nbreak;\r\ncase MAX_RX_ADD_QUEUES:\r\nadapter->max_rx_add_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_rx_add_queues = %lld\n",\r\nadapter->max_rx_add_queues);\r\nbreak;\r\ncase MIN_TX_ENTRIES_PER_SUBCRQ:\r\nadapter->min_tx_entries_per_subcrq =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "min_tx_entries_per_subcrq = %lld\n",\r\nadapter->min_tx_entries_per_subcrq);\r\nbreak;\r\ncase MIN_RX_ADD_ENTRIES_PER_SUBCRQ:\r\nadapter->min_rx_add_entries_per_subcrq =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "min_rx_add_entrs_per_subcrq = %lld\n",\r\nadapter->min_rx_add_entries_per_subcrq);\r\nbreak;\r\ncase MAX_TX_ENTRIES_PER_SUBCRQ:\r\nadapter->max_tx_entries_per_subcrq =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_tx_entries_per_subcrq = %lld\n",\r\nadapter->max_tx_entries_per_subcrq);\r\nbreak;\r\ncase MAX_RX_ADD_ENTRIES_PER_SUBCRQ:\r\nadapter->max_rx_add_entries_per_subcrq =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_rx_add_entrs_per_subcrq = %lld\n",\r\nadapter->max_rx_add_entries_per_subcrq);\r\nbreak;\r\ncase TCP_IP_OFFLOAD:\r\nadapter->tcp_ip_offload =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "tcp_ip_offload = %lld\n",\r\nadapter->tcp_ip_offload);\r\nbreak;\r\ncase PROMISC_SUPPORTED:\r\nadapter->promisc_supported =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "promisc_supported = %lld\n",\r\nadapter->promisc_supported);\r\nbreak;\r\ncase MIN_MTU:\r\nadapter->min_mtu = be64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "min_mtu = %lld\n", adapter->min_mtu);\r\nbreak;\r\ncase MAX_MTU:\r\nadapter->max_mtu = be64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_mtu = %lld\n", adapter->max_mtu);\r\nbreak;\r\ncase MAX_MULTICAST_FILTERS:\r\nadapter->max_multicast_filters =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_multicast_filters = %lld\n",\r\nadapter->max_multicast_filters);\r\nbreak;\r\ncase VLAN_HEADER_INSERTION:\r\nadapter->vlan_header_insertion =\r\nbe64_to_cpu(crq->query_capability.number);\r\nif (adapter->vlan_header_insertion)\r\nnetdev->features |= NETIF_F_HW_VLAN_STAG_TX;\r\nnetdev_dbg(netdev, "vlan_header_insertion = %lld\n",\r\nadapter->vlan_header_insertion);\r\nbreak;\r\ncase MAX_TX_SG_ENTRIES:\r\nadapter->max_tx_sg_entries =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "max_tx_sg_entries = %lld\n",\r\nadapter->max_tx_sg_entries);\r\nbreak;\r\ncase RX_SG_SUPPORTED:\r\nadapter->rx_sg_supported =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "rx_sg_supported = %lld\n",\r\nadapter->rx_sg_supported);\r\nbreak;\r\ncase OPT_TX_COMP_SUB_QUEUES:\r\nadapter->opt_tx_comp_sub_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "opt_tx_comp_sub_queues = %lld\n",\r\nadapter->opt_tx_comp_sub_queues);\r\nbreak;\r\ncase OPT_RX_COMP_QUEUES:\r\nadapter->opt_rx_comp_queues =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "opt_rx_comp_queues = %lld\n",\r\nadapter->opt_rx_comp_queues);\r\nbreak;\r\ncase OPT_RX_BUFADD_Q_PER_RX_COMP_Q:\r\nadapter->opt_rx_bufadd_q_per_rx_comp_q =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "opt_rx_bufadd_q_per_rx_comp_q = %lld\n",\r\nadapter->opt_rx_bufadd_q_per_rx_comp_q);\r\nbreak;\r\ncase OPT_TX_ENTRIES_PER_SUBCRQ:\r\nadapter->opt_tx_entries_per_subcrq =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "opt_tx_entries_per_subcrq = %lld\n",\r\nadapter->opt_tx_entries_per_subcrq);\r\nbreak;\r\ncase OPT_RXBA_ENTRIES_PER_SUBCRQ:\r\nadapter->opt_rxba_entries_per_subcrq =\r\nbe64_to_cpu(crq->query_capability.number);\r\nnetdev_dbg(netdev, "opt_rxba_entries_per_subcrq = %lld\n",\r\nadapter->opt_rxba_entries_per_subcrq);\r\nbreak;\r\ncase TX_RX_DESC_REQ:\r\nadapter->tx_rx_desc_req = crq->query_capability.number;\r\nnetdev_dbg(netdev, "tx_rx_desc_req = %llx\n",\r\nadapter->tx_rx_desc_req);\r\nbreak;\r\ndefault:\r\nnetdev_err(netdev, "Got invalid cap rsp %d\n",\r\ncrq->query_capability.capability);\r\n}\r\nout:\r\nif (atomic_read(&adapter->running_cap_queries) == 0)\r\ncomplete(&adapter->init_done);\r\n}\r\nstatic void handle_control_ras_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nu8 correlator = crq->control_ras_rsp.correlator;\r\nstruct device *dev = &adapter->vdev->dev;\r\nbool found = false;\r\nint i;\r\nif (crq->control_ras_rsp.rc.code) {\r\ndev_warn(dev, "Control ras failed rc=%d\n",\r\ncrq->control_ras_rsp.rc.code);\r\nreturn;\r\n}\r\nfor (i = 0; i < adapter->ras_comp_num; i++) {\r\nif (adapter->ras_comps[i].correlator == correlator) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\ndev_warn(dev, "Correlator not found on control_ras_rsp\n");\r\nreturn;\r\n}\r\nswitch (crq->control_ras_rsp.op) {\r\ncase IBMVNIC_TRACE_LEVEL:\r\nadapter->ras_comps[i].trace_level = crq->control_ras.level;\r\nbreak;\r\ncase IBMVNIC_ERROR_LEVEL:\r\nadapter->ras_comps[i].error_check_level =\r\ncrq->control_ras.level;\r\nbreak;\r\ncase IBMVNIC_TRACE_PAUSE:\r\nadapter->ras_comp_int[i].paused = 1;\r\nbreak;\r\ncase IBMVNIC_TRACE_RESUME:\r\nadapter->ras_comp_int[i].paused = 0;\r\nbreak;\r\ncase IBMVNIC_TRACE_ON:\r\nadapter->ras_comps[i].trace_on = 1;\r\nbreak;\r\ncase IBMVNIC_TRACE_OFF:\r\nadapter->ras_comps[i].trace_on = 0;\r\nbreak;\r\ncase IBMVNIC_CHG_TRACE_BUFF_SZ:\r\n((u8 *)(&adapter->ras_comps[i].trace_buff_size))[0] = 0;\r\n((u8 *)(&adapter->ras_comps[i].trace_buff_size))[1] =\r\ncrq->control_ras_rsp.trace_buff_sz[0];\r\n((u8 *)(&adapter->ras_comps[i].trace_buff_size))[2] =\r\ncrq->control_ras_rsp.trace_buff_sz[1];\r\n((u8 *)(&adapter->ras_comps[i].trace_buff_size))[3] =\r\ncrq->control_ras_rsp.trace_buff_sz[2];\r\nbreak;\r\ndefault:\r\ndev_err(dev, "invalid op %d on control_ras_rsp",\r\ncrq->control_ras_rsp.op);\r\n}\r\n}\r\nstatic int ibmvnic_fw_comp_open(struct inode *inode, struct file *file)\r\n{\r\nfile->private_data = inode->i_private;\r\nreturn 0;\r\n}\r\nstatic ssize_t trace_read(struct file *file, char __user *user_buf, size_t len,\r\nloff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_fw_trace_entry *trace;\r\nint num = ras_comp_int->num;\r\nunion ibmvnic_crq crq;\r\ndma_addr_t trace_tok;\r\nif (*ppos >= be32_to_cpu(adapter->ras_comps[num].trace_buff_size))\r\nreturn 0;\r\ntrace =\r\ndma_alloc_coherent(dev,\r\nbe32_to_cpu(adapter->ras_comps[num].\r\ntrace_buff_size), &trace_tok,\r\nGFP_KERNEL);\r\nif (!trace) {\r\ndev_err(dev, "Couldn't alloc trace buffer\n");\r\nreturn 0;\r\n}\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.collect_fw_trace.first = IBMVNIC_CRQ_CMD;\r\ncrq.collect_fw_trace.cmd = COLLECT_FW_TRACE;\r\ncrq.collect_fw_trace.correlator = adapter->ras_comps[num].correlator;\r\ncrq.collect_fw_trace.ioba = cpu_to_be32(trace_tok);\r\ncrq.collect_fw_trace.len = adapter->ras_comps[num].trace_buff_size;\r\nibmvnic_send_crq(adapter, &crq);\r\ninit_completion(&adapter->fw_done);\r\nwait_for_completion(&adapter->fw_done);\r\nif (*ppos + len > be32_to_cpu(adapter->ras_comps[num].trace_buff_size))\r\nlen =\r\nbe32_to_cpu(adapter->ras_comps[num].trace_buff_size) -\r\n*ppos;\r\ncopy_to_user(user_buf, &((u8 *)trace)[*ppos], len);\r\ndma_free_coherent(dev,\r\nbe32_to_cpu(adapter->ras_comps[num].trace_buff_size),\r\ntrace, trace_tok);\r\n*ppos += len;\r\nreturn len;\r\n}\r\nstatic ssize_t paused_read(struct file *file, char __user *user_buf, size_t len,\r\nloff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nchar buff[5];\r\nint size;\r\nsize = sprintf(buff, "%d\n", adapter->ras_comp_int[num].paused);\r\nif (*ppos >= size)\r\nreturn 0;\r\ncopy_to_user(user_buf, buff, size);\r\n*ppos += size;\r\nreturn size;\r\n}\r\nstatic ssize_t paused_write(struct file *file, const char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nunion ibmvnic_crq crq;\r\nunsigned long val;\r\nchar buff[9];\r\ncopy_from_user(buff, user_buf, sizeof(buff));\r\nval = kstrtoul(buff, 10, NULL);\r\nadapter->ras_comp_int[num].paused = val ? 1 : 0;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.control_ras.first = IBMVNIC_CRQ_CMD;\r\ncrq.control_ras.cmd = CONTROL_RAS;\r\ncrq.control_ras.correlator = adapter->ras_comps[num].correlator;\r\ncrq.control_ras.op = val ? IBMVNIC_TRACE_PAUSE : IBMVNIC_TRACE_RESUME;\r\nibmvnic_send_crq(adapter, &crq);\r\nreturn len;\r\n}\r\nstatic ssize_t tracing_read(struct file *file, char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nchar buff[5];\r\nint size;\r\nsize = sprintf(buff, "%d\n", adapter->ras_comps[num].trace_on);\r\nif (*ppos >= size)\r\nreturn 0;\r\ncopy_to_user(user_buf, buff, size);\r\n*ppos += size;\r\nreturn size;\r\n}\r\nstatic ssize_t tracing_write(struct file *file, const char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nunion ibmvnic_crq crq;\r\nunsigned long val;\r\nchar buff[9];\r\ncopy_from_user(buff, user_buf, sizeof(buff));\r\nval = kstrtoul(buff, 10, NULL);\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.control_ras.first = IBMVNIC_CRQ_CMD;\r\ncrq.control_ras.cmd = CONTROL_RAS;\r\ncrq.control_ras.correlator = adapter->ras_comps[num].correlator;\r\ncrq.control_ras.op = val ? IBMVNIC_TRACE_ON : IBMVNIC_TRACE_OFF;\r\nreturn len;\r\n}\r\nstatic ssize_t error_level_read(struct file *file, char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nchar buff[5];\r\nint size;\r\nsize = sprintf(buff, "%d\n", adapter->ras_comps[num].error_check_level);\r\nif (*ppos >= size)\r\nreturn 0;\r\ncopy_to_user(user_buf, buff, size);\r\n*ppos += size;\r\nreturn size;\r\n}\r\nstatic ssize_t error_level_write(struct file *file, const char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nunion ibmvnic_crq crq;\r\nunsigned long val;\r\nchar buff[9];\r\ncopy_from_user(buff, user_buf, sizeof(buff));\r\nval = kstrtoul(buff, 10, NULL);\r\nif (val > 9)\r\nval = 9;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.control_ras.first = IBMVNIC_CRQ_CMD;\r\ncrq.control_ras.cmd = CONTROL_RAS;\r\ncrq.control_ras.correlator = adapter->ras_comps[num].correlator;\r\ncrq.control_ras.op = IBMVNIC_ERROR_LEVEL;\r\ncrq.control_ras.level = val;\r\nibmvnic_send_crq(adapter, &crq);\r\nreturn len;\r\n}\r\nstatic ssize_t trace_level_read(struct file *file, char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nchar buff[5];\r\nint size;\r\nsize = sprintf(buff, "%d\n", adapter->ras_comps[num].trace_level);\r\nif (*ppos >= size)\r\nreturn 0;\r\ncopy_to_user(user_buf, buff, size);\r\n*ppos += size;\r\nreturn size;\r\n}\r\nstatic ssize_t trace_level_write(struct file *file, const char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nunion ibmvnic_crq crq;\r\nunsigned long val;\r\nchar buff[9];\r\ncopy_from_user(buff, user_buf, sizeof(buff));\r\nval = kstrtoul(buff, 10, NULL);\r\nif (val > 9)\r\nval = 9;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.control_ras.first = IBMVNIC_CRQ_CMD;\r\ncrq.control_ras.cmd = CONTROL_RAS;\r\ncrq.control_ras.correlator =\r\nadapter->ras_comps[ras_comp_int->num].correlator;\r\ncrq.control_ras.op = IBMVNIC_TRACE_LEVEL;\r\ncrq.control_ras.level = val;\r\nibmvnic_send_crq(adapter, &crq);\r\nreturn len;\r\n}\r\nstatic ssize_t trace_buff_size_read(struct file *file, char __user *user_buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nint num = ras_comp_int->num;\r\nchar buff[9];\r\nint size;\r\nsize = sprintf(buff, "%d\n", adapter->ras_comps[num].trace_buff_size);\r\nif (*ppos >= size)\r\nreturn 0;\r\ncopy_to_user(user_buf, buff, size);\r\n*ppos += size;\r\nreturn size;\r\n}\r\nstatic ssize_t trace_buff_size_write(struct file *file,\r\nconst char __user *user_buf, size_t len,\r\nloff_t *ppos)\r\n{\r\nstruct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;\r\nstruct ibmvnic_adapter *adapter = ras_comp_int->adapter;\r\nunion ibmvnic_crq crq;\r\nunsigned long val;\r\nchar buff[9];\r\ncopy_from_user(buff, user_buf, sizeof(buff));\r\nval = kstrtoul(buff, 10, NULL);\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.control_ras.first = IBMVNIC_CRQ_CMD;\r\ncrq.control_ras.cmd = CONTROL_RAS;\r\ncrq.control_ras.correlator =\r\nadapter->ras_comps[ras_comp_int->num].correlator;\r\ncrq.control_ras.op = IBMVNIC_CHG_TRACE_BUFF_SZ;\r\ncrq.control_ras.trace_buff_sz[0] = ((u8 *)(&val))[5];\r\ncrq.control_ras.trace_buff_sz[1] = ((u8 *)(&val))[6];\r\ncrq.control_ras.trace_buff_sz[2] = ((u8 *)(&val))[7];\r\nibmvnic_send_crq(adapter, &crq);\r\nreturn len;\r\n}\r\nstatic void handle_request_ras_comps_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct dentry *dir_ent;\r\nstruct dentry *ent;\r\nint i;\r\ndebugfs_remove_recursive(adapter->ras_comps_ent);\r\nadapter->ras_comps_ent = debugfs_create_dir("ras_comps",\r\nadapter->debugfs_dir);\r\nif (!adapter->ras_comps_ent || IS_ERR(adapter->ras_comps_ent)) {\r\ndev_info(dev, "debugfs create ras_comps dir failed\n");\r\nreturn;\r\n}\r\nfor (i = 0; i < adapter->ras_comp_num; i++) {\r\ndir_ent = debugfs_create_dir(adapter->ras_comps[i].name,\r\nadapter->ras_comps_ent);\r\nif (!dir_ent || IS_ERR(dir_ent)) {\r\ndev_info(dev, "debugfs create %s dir failed\n",\r\nadapter->ras_comps[i].name);\r\ncontinue;\r\n}\r\nadapter->ras_comp_int[i].adapter = adapter;\r\nadapter->ras_comp_int[i].num = i;\r\nadapter->ras_comp_int[i].desc_blob.data =\r\n&adapter->ras_comps[i].description;\r\nadapter->ras_comp_int[i].desc_blob.size =\r\nsizeof(adapter->ras_comps[i].description);\r\nent = debugfs_create_blob("description", S_IRUGO, dir_ent,\r\n&adapter->ras_comp_int[i].desc_blob);\r\nent = debugfs_create_file("trace_buf_size", S_IRUGO | S_IWUSR,\r\ndir_ent, &adapter->ras_comp_int[i],\r\n&trace_size_ops);\r\nent = debugfs_create_file("trace_level",\r\nS_IRUGO |\r\n(adapter->ras_comps[i].trace_level !=\r\n0xFF ? S_IWUSR : 0),\r\ndir_ent, &adapter->ras_comp_int[i],\r\n&trace_level_ops);\r\nent = debugfs_create_file("error_level",\r\nS_IRUGO |\r\n(adapter->\r\nras_comps[i].error_check_level !=\r\n0xFF ? S_IWUSR : 0),\r\ndir_ent, &adapter->ras_comp_int[i],\r\n&trace_level_ops);\r\nent = debugfs_create_file("tracing", S_IRUGO | S_IWUSR,\r\ndir_ent, &adapter->ras_comp_int[i],\r\n&tracing_ops);\r\nent = debugfs_create_file("paused", S_IRUGO | S_IWUSR,\r\ndir_ent, &adapter->ras_comp_int[i],\r\n&paused_ops);\r\nent = debugfs_create_file("trace", S_IRUGO, dir_ent,\r\n&adapter->ras_comp_int[i],\r\n&trace_ops);\r\n}\r\n}\r\nstatic void handle_request_ras_comp_num_rsp(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nint len = adapter->ras_comp_num * sizeof(struct ibmvnic_fw_component);\r\nstruct device *dev = &adapter->vdev->dev;\r\nunion ibmvnic_crq newcrq;\r\nadapter->ras_comps = dma_alloc_coherent(dev, len,\r\n&adapter->ras_comps_tok,\r\nGFP_KERNEL);\r\nif (!adapter->ras_comps) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\ndev_err(dev, "Couldn't alloc fw comps buffer\n");\r\nreturn;\r\n}\r\nadapter->ras_comp_int = kmalloc(adapter->ras_comp_num *\r\nsizeof(struct ibmvnic_fw_comp_internal),\r\nGFP_KERNEL);\r\nif (!adapter->ras_comp_int)\r\ndma_free_coherent(dev, len, adapter->ras_comps,\r\nadapter->ras_comps_tok);\r\nmemset(&newcrq, 0, sizeof(newcrq));\r\nnewcrq.request_ras_comps.first = IBMVNIC_CRQ_CMD;\r\nnewcrq.request_ras_comps.cmd = REQUEST_RAS_COMPS;\r\nnewcrq.request_ras_comps.ioba = cpu_to_be32(adapter->ras_comps_tok);\r\nnewcrq.request_ras_comps.len = cpu_to_be32(len);\r\nibmvnic_send_crq(adapter, &newcrq);\r\n}\r\nstatic void ibmvnic_free_inflight(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct ibmvnic_inflight_cmd *inflight_cmd;\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct ibmvnic_error_buff *error_buff;\r\nunsigned long flags;\r\nunsigned long flags2;\r\nspin_lock_irqsave(&adapter->inflight_lock, flags);\r\nlist_for_each_entry(inflight_cmd, &adapter->inflight, list) {\r\nswitch (inflight_cmd->crq.generic.cmd) {\r\ncase LOGIN:\r\ndma_unmap_single(dev, adapter->login_buf_token,\r\nadapter->login_buf_sz,\r\nDMA_BIDIRECTIONAL);\r\ndma_unmap_single(dev, adapter->login_rsp_buf_token,\r\nadapter->login_rsp_buf_sz,\r\nDMA_BIDIRECTIONAL);\r\nkfree(adapter->login_rsp_buf);\r\nkfree(adapter->login_buf);\r\nbreak;\r\ncase REQUEST_DUMP:\r\ncomplete(&adapter->fw_done);\r\nbreak;\r\ncase REQUEST_ERROR_INFO:\r\nspin_lock_irqsave(&adapter->error_list_lock, flags2);\r\nlist_for_each_entry(error_buff, &adapter->errors,\r\nlist) {\r\ndma_unmap_single(dev, error_buff->dma,\r\nerror_buff->len,\r\nDMA_FROM_DEVICE);\r\nkfree(error_buff->buff);\r\nlist_del(&error_buff->list);\r\nkfree(error_buff);\r\n}\r\nspin_unlock_irqrestore(&adapter->error_list_lock,\r\nflags2);\r\nbreak;\r\n}\r\nlist_del(&inflight_cmd->list);\r\nkfree(inflight_cmd);\r\n}\r\nspin_unlock_irqrestore(&adapter->inflight_lock, flags);\r\n}\r\nstatic void ibmvnic_handle_crq(union ibmvnic_crq *crq,\r\nstruct ibmvnic_adapter *adapter)\r\n{\r\nstruct ibmvnic_generic_crq *gen_crq = &crq->generic;\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct device *dev = &adapter->vdev->dev;\r\nlong rc;\r\nnetdev_dbg(netdev, "Handling CRQ: %016lx %016lx\n",\r\n((unsigned long int *)crq)[0],\r\n((unsigned long int *)crq)[1]);\r\nswitch (gen_crq->first) {\r\ncase IBMVNIC_CRQ_INIT_RSP:\r\nswitch (gen_crq->cmd) {\r\ncase IBMVNIC_CRQ_INIT:\r\ndev_info(dev, "Partner initialized\n");\r\nrc = ibmvnic_send_crq_init_complete(adapter);\r\nif (rc == 0)\r\nsend_version_xchg(adapter);\r\nelse\r\ndev_err(dev, "Can't send initrsp rc=%ld\n", rc);\r\nbreak;\r\ncase IBMVNIC_CRQ_INIT_COMPLETE:\r\ndev_info(dev, "Partner initialization complete\n");\r\nsend_version_xchg(adapter);\r\nbreak;\r\ndefault:\r\ndev_err(dev, "Unknown crq cmd: %d\n", gen_crq->cmd);\r\n}\r\nreturn;\r\ncase IBMVNIC_CRQ_XPORT_EVENT:\r\nif (gen_crq->cmd == IBMVNIC_PARTITION_MIGRATED) {\r\ndev_info(dev, "Re-enabling adapter\n");\r\nadapter->migrated = true;\r\nibmvnic_free_inflight(adapter);\r\nrelease_sub_crqs(adapter);\r\nrc = ibmvnic_reenable_crq_queue(adapter);\r\nif (rc)\r\ndev_err(dev, "Error after enable rc=%ld\n", rc);\r\nadapter->migrated = false;\r\nrc = ibmvnic_send_crq_init(adapter);\r\nif (rc)\r\ndev_err(dev, "Error sending init rc=%ld\n", rc);\r\n} else {\r\ndev_err(dev, "Virtual Adapter failed (rc=%d)\n",\r\ngen_crq->cmd);\r\nibmvnic_free_inflight(adapter);\r\nrelease_sub_crqs(adapter);\r\n}\r\nreturn;\r\ncase IBMVNIC_CRQ_CMD_RSP:\r\nbreak;\r\ndefault:\r\ndev_err(dev, "Got an invalid msg type 0x%02x\n",\r\ngen_crq->first);\r\nreturn;\r\n}\r\nswitch (gen_crq->cmd) {\r\ncase VERSION_EXCHANGE_RSP:\r\nrc = crq->version_exchange_rsp.rc.code;\r\nif (rc) {\r\ndev_err(dev, "Error %ld in VERSION_EXCHG_RSP\n", rc);\r\nbreak;\r\n}\r\ndev_info(dev, "Partner protocol version is %d\n",\r\ncrq->version_exchange_rsp.version);\r\nif (be16_to_cpu(crq->version_exchange_rsp.version) <\r\nibmvnic_version)\r\nibmvnic_version =\r\nbe16_to_cpu(crq->version_exchange_rsp.version);\r\nsend_cap_queries(adapter);\r\nbreak;\r\ncase QUERY_CAPABILITY_RSP:\r\nhandle_query_cap_rsp(crq, adapter);\r\nbreak;\r\ncase QUERY_MAP_RSP:\r\nhandle_query_map_rsp(crq, adapter);\r\nbreak;\r\ncase REQUEST_MAP_RSP:\r\nhandle_request_map_rsp(crq, adapter);\r\nbreak;\r\ncase REQUEST_UNMAP_RSP:\r\nhandle_request_unmap_rsp(crq, adapter);\r\nbreak;\r\ncase REQUEST_CAPABILITY_RSP:\r\nhandle_request_cap_rsp(crq, adapter);\r\nbreak;\r\ncase LOGIN_RSP:\r\nnetdev_dbg(netdev, "Got Login Response\n");\r\nhandle_login_rsp(crq, adapter);\r\nbreak;\r\ncase LOGICAL_LINK_STATE_RSP:\r\nnetdev_dbg(netdev, "Got Logical Link State Response\n");\r\nadapter->logical_link_state =\r\ncrq->logical_link_state_rsp.link_state;\r\nbreak;\r\ncase LINK_STATE_INDICATION:\r\nnetdev_dbg(netdev, "Got Logical Link State Indication\n");\r\nadapter->phys_link_state =\r\ncrq->link_state_indication.phys_link_state;\r\nadapter->logical_link_state =\r\ncrq->link_state_indication.logical_link_state;\r\nbreak;\r\ncase CHANGE_MAC_ADDR_RSP:\r\nnetdev_dbg(netdev, "Got MAC address change Response\n");\r\nhandle_change_mac_rsp(crq, adapter);\r\nbreak;\r\ncase ERROR_INDICATION:\r\nnetdev_dbg(netdev, "Got Error Indication\n");\r\nhandle_error_indication(crq, adapter);\r\nbreak;\r\ncase REQUEST_ERROR_RSP:\r\nnetdev_dbg(netdev, "Got Error Detail Response\n");\r\nhandle_error_info_rsp(crq, adapter);\r\nbreak;\r\ncase REQUEST_STATISTICS_RSP:\r\nnetdev_dbg(netdev, "Got Statistics Response\n");\r\ncomplete(&adapter->stats_done);\r\nbreak;\r\ncase REQUEST_DUMP_SIZE_RSP:\r\nnetdev_dbg(netdev, "Got Request Dump Size Response\n");\r\nhandle_dump_size_rsp(crq, adapter);\r\nbreak;\r\ncase REQUEST_DUMP_RSP:\r\nnetdev_dbg(netdev, "Got Request Dump Response\n");\r\ncomplete(&adapter->fw_done);\r\nbreak;\r\ncase QUERY_IP_OFFLOAD_RSP:\r\nnetdev_dbg(netdev, "Got Query IP offload Response\n");\r\nhandle_query_ip_offload_rsp(adapter);\r\nbreak;\r\ncase MULTICAST_CTRL_RSP:\r\nnetdev_dbg(netdev, "Got multicast control Response\n");\r\nbreak;\r\ncase CONTROL_IP_OFFLOAD_RSP:\r\nnetdev_dbg(netdev, "Got Control IP offload Response\n");\r\ndma_unmap_single(dev, adapter->ip_offload_ctrl_tok,\r\nsizeof(adapter->ip_offload_ctrl),\r\nDMA_TO_DEVICE);\r\nsend_login(adapter);\r\nbreak;\r\ncase REQUEST_RAS_COMP_NUM_RSP:\r\nnetdev_dbg(netdev, "Got Request RAS Comp Num Response\n");\r\nif (crq->request_ras_comp_num_rsp.rc.code == 10) {\r\nnetdev_dbg(netdev, "Request RAS Comp Num not supported\n");\r\nbreak;\r\n}\r\nadapter->ras_comp_num =\r\nbe32_to_cpu(crq->request_ras_comp_num_rsp.num_components);\r\nhandle_request_ras_comp_num_rsp(crq, adapter);\r\nbreak;\r\ncase REQUEST_RAS_COMPS_RSP:\r\nnetdev_dbg(netdev, "Got Request RAS Comps Response\n");\r\nhandle_request_ras_comps_rsp(crq, adapter);\r\nbreak;\r\ncase CONTROL_RAS_RSP:\r\nnetdev_dbg(netdev, "Got Control RAS Response\n");\r\nhandle_control_ras_rsp(crq, adapter);\r\nbreak;\r\ncase COLLECT_FW_TRACE_RSP:\r\nnetdev_dbg(netdev, "Got Collect firmware trace Response\n");\r\ncomplete(&adapter->fw_done);\r\nbreak;\r\ndefault:\r\nnetdev_err(netdev, "Got an invalid cmd type 0x%02x\n",\r\ngen_crq->cmd);\r\n}\r\n}\r\nstatic irqreturn_t ibmvnic_interrupt(int irq, void *instance)\r\n{\r\nstruct ibmvnic_adapter *adapter = instance;\r\nstruct ibmvnic_crq_queue *queue = &adapter->crq;\r\nstruct vio_dev *vdev = adapter->vdev;\r\nunion ibmvnic_crq *crq;\r\nunsigned long flags;\r\nbool done = false;\r\nspin_lock_irqsave(&queue->lock, flags);\r\nvio_disable_interrupts(vdev);\r\nwhile (!done) {\r\nwhile ((crq = ibmvnic_next_crq(adapter)) != NULL) {\r\nibmvnic_handle_crq(crq, adapter);\r\ncrq->generic.first = 0;\r\n}\r\nvio_enable_interrupts(vdev);\r\ncrq = ibmvnic_next_crq(adapter);\r\nif (crq) {\r\nvio_disable_interrupts(vdev);\r\nibmvnic_handle_crq(crq, adapter);\r\ncrq->generic.first = 0;\r\n} else {\r\ndone = true;\r\n}\r\n}\r\nspin_unlock_irqrestore(&queue->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int ibmvnic_reenable_crq_queue(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct vio_dev *vdev = adapter->vdev;\r\nint rc;\r\ndo {\r\nrc = plpar_hcall_norets(H_ENABLE_CRQ, vdev->unit_address);\r\n} while (rc == H_IN_PROGRESS || rc == H_BUSY || H_IS_LONG_BUSY(rc));\r\nif (rc)\r\ndev_err(&vdev->dev, "Error enabling adapter (rc=%d)\n", rc);\r\nreturn rc;\r\n}\r\nstatic int ibmvnic_reset_crq(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct ibmvnic_crq_queue *crq = &adapter->crq;\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct vio_dev *vdev = adapter->vdev;\r\nint rc;\r\ndo {\r\nrc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\r\n} while (rc == H_BUSY || H_IS_LONG_BUSY(rc));\r\nmemset(crq->msgs, 0, PAGE_SIZE);\r\ncrq->cur = 0;\r\nrc = plpar_hcall_norets(H_REG_CRQ, vdev->unit_address,\r\ncrq->msg_token, PAGE_SIZE);\r\nif (rc == H_CLOSED)\r\ndev_warn(dev, "Partner adapter not ready\n");\r\nelse if (rc != 0)\r\ndev_warn(dev, "Couldn't register crq (rc=%d)\n", rc);\r\nreturn rc;\r\n}\r\nstatic void ibmvnic_release_crq_queue(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct ibmvnic_crq_queue *crq = &adapter->crq;\r\nstruct vio_dev *vdev = adapter->vdev;\r\nlong rc;\r\nnetdev_dbg(adapter->netdev, "Releasing CRQ\n");\r\nfree_irq(vdev->irq, adapter);\r\ndo {\r\nrc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\r\n} while (rc == H_BUSY || H_IS_LONG_BUSY(rc));\r\ndma_unmap_single(&vdev->dev, crq->msg_token, PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nfree_page((unsigned long)crq->msgs);\r\n}\r\nstatic int ibmvnic_init_crq_queue(struct ibmvnic_adapter *adapter)\r\n{\r\nstruct ibmvnic_crq_queue *crq = &adapter->crq;\r\nstruct device *dev = &adapter->vdev->dev;\r\nstruct vio_dev *vdev = adapter->vdev;\r\nint rc, retrc = -ENOMEM;\r\ncrq->msgs = (union ibmvnic_crq *)get_zeroed_page(GFP_KERNEL);\r\nif (!crq->msgs)\r\nreturn -ENOMEM;\r\ncrq->size = PAGE_SIZE / sizeof(*crq->msgs);\r\ncrq->msg_token = dma_map_single(dev, crq->msgs, PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(dev, crq->msg_token))\r\ngoto map_failed;\r\nrc = plpar_hcall_norets(H_REG_CRQ, vdev->unit_address,\r\ncrq->msg_token, PAGE_SIZE);\r\nif (rc == H_RESOURCE)\r\nrc = ibmvnic_reset_crq(adapter);\r\nretrc = rc;\r\nif (rc == H_CLOSED) {\r\ndev_warn(dev, "Partner adapter not ready\n");\r\n} else if (rc) {\r\ndev_warn(dev, "Error %d opening adapter\n", rc);\r\ngoto reg_crq_failed;\r\n}\r\nretrc = 0;\r\nnetdev_dbg(adapter->netdev, "registering irq 0x%x\n", vdev->irq);\r\nrc = request_irq(vdev->irq, ibmvnic_interrupt, 0, IBMVNIC_NAME,\r\nadapter);\r\nif (rc) {\r\ndev_err(dev, "Couldn't register irq 0x%x. rc=%d\n",\r\nvdev->irq, rc);\r\ngoto req_irq_failed;\r\n}\r\nrc = vio_enable_interrupts(vdev);\r\nif (rc) {\r\ndev_err(dev, "Error %d enabling interrupts\n", rc);\r\ngoto req_irq_failed;\r\n}\r\ncrq->cur = 0;\r\nspin_lock_init(&crq->lock);\r\nreturn retrc;\r\nreq_irq_failed:\r\ndo {\r\nrc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\r\n} while (rc == H_BUSY || H_IS_LONG_BUSY(rc));\r\nreg_crq_failed:\r\ndma_unmap_single(dev, crq->msg_token, PAGE_SIZE, DMA_BIDIRECTIONAL);\r\nmap_failed:\r\nfree_page((unsigned long)crq->msgs);\r\nreturn retrc;\r\n}\r\nstatic int ibmvnic_dump_show(struct seq_file *seq, void *v)\r\n{\r\nstruct net_device *netdev = seq->private;\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nstruct device *dev = &adapter->vdev->dev;\r\nunion ibmvnic_crq crq;\r\nmemset(&crq, 0, sizeof(crq));\r\ncrq.request_dump_size.first = IBMVNIC_CRQ_CMD;\r\ncrq.request_dump_size.cmd = REQUEST_DUMP_SIZE;\r\nibmvnic_send_crq(adapter, &crq);\r\ninit_completion(&adapter->fw_done);\r\nwait_for_completion(&adapter->fw_done);\r\nseq_write(seq, adapter->dump_data, adapter->dump_data_size);\r\ndma_unmap_single(dev, adapter->dump_data_token, adapter->dump_data_size,\r\nDMA_BIDIRECTIONAL);\r\nkfree(adapter->dump_data);\r\nreturn 0;\r\n}\r\nstatic int ibmvnic_dump_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, ibmvnic_dump_show, inode->i_private);\r\n}\r\nstatic int ibmvnic_probe(struct vio_dev *dev, const struct vio_device_id *id)\r\n{\r\nstruct ibmvnic_adapter *adapter;\r\nstruct net_device *netdev;\r\nunsigned char *mac_addr_p;\r\nstruct dentry *ent;\r\nchar buf[16];\r\nint rc;\r\ndev_dbg(&dev->dev, "entering ibmvnic_probe for UA 0x%x\n",\r\ndev->unit_address);\r\nmac_addr_p = (unsigned char *)vio_get_attribute(dev,\r\nVETH_MAC_ADDR, NULL);\r\nif (!mac_addr_p) {\r\ndev_err(&dev->dev,\r\n"(%s:%3.3d) ERROR: Can't find MAC_ADDR attribute\n",\r\n__FILE__, __LINE__);\r\nreturn 0;\r\n}\r\nnetdev = alloc_etherdev_mq(sizeof(struct ibmvnic_adapter),\r\nIBMVNIC_MAX_TX_QUEUES);\r\nif (!netdev)\r\nreturn -ENOMEM;\r\nadapter = netdev_priv(netdev);\r\ndev_set_drvdata(&dev->dev, netdev);\r\nadapter->vdev = dev;\r\nadapter->netdev = netdev;\r\nether_addr_copy(adapter->mac_addr, mac_addr_p);\r\nether_addr_copy(netdev->dev_addr, adapter->mac_addr);\r\nnetdev->irq = dev->irq;\r\nnetdev->netdev_ops = &ibmvnic_netdev_ops;\r\nnetdev->ethtool_ops = &ibmvnic_ethtool_ops;\r\nSET_NETDEV_DEV(netdev, &dev->dev);\r\nspin_lock_init(&adapter->stats_lock);\r\nrc = ibmvnic_init_crq_queue(adapter);\r\nif (rc) {\r\ndev_err(&dev->dev, "Couldn't initialize crq. rc=%d\n", rc);\r\ngoto free_netdev;\r\n}\r\nINIT_LIST_HEAD(&adapter->errors);\r\nINIT_LIST_HEAD(&adapter->inflight);\r\nspin_lock_init(&adapter->error_list_lock);\r\nspin_lock_init(&adapter->inflight_lock);\r\nadapter->stats_token = dma_map_single(&dev->dev, &adapter->stats,\r\nsizeof(struct ibmvnic_statistics),\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(&dev->dev, adapter->stats_token)) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\ndev_err(&dev->dev, "Couldn't map stats buffer\n");\r\ngoto free_crq;\r\n}\r\nsnprintf(buf, sizeof(buf), "ibmvnic_%x", dev->unit_address);\r\nent = debugfs_create_dir(buf, NULL);\r\nif (!ent || IS_ERR(ent)) {\r\ndev_info(&dev->dev, "debugfs create directory failed\n");\r\nadapter->debugfs_dir = NULL;\r\n} else {\r\nadapter->debugfs_dir = ent;\r\nent = debugfs_create_file("dump", S_IRUGO, adapter->debugfs_dir,\r\nnetdev, &ibmvnic_dump_ops);\r\nif (!ent || IS_ERR(ent)) {\r\ndev_info(&dev->dev,\r\n"debugfs create dump file failed\n");\r\nadapter->debugfs_dump = NULL;\r\n} else {\r\nadapter->debugfs_dump = ent;\r\n}\r\n}\r\nibmvnic_send_crq_init(adapter);\r\ninit_completion(&adapter->init_done);\r\nwait_for_completion(&adapter->init_done);\r\ninit_sub_crqs(adapter, 0);\r\nreinit_completion(&adapter->init_done);\r\nwait_for_completion(&adapter->init_done);\r\nwhile (!adapter->tx_scrq || !adapter->rx_scrq) {\r\ninit_sub_crqs(adapter, 1);\r\nreinit_completion(&adapter->init_done);\r\nwait_for_completion(&adapter->init_done);\r\n}\r\nnetdev->real_num_tx_queues = adapter->req_tx_queues;\r\nrc = register_netdev(netdev);\r\nif (rc) {\r\ndev_err(&dev->dev, "failed to register netdev rc=%d\n", rc);\r\ngoto free_debugfs;\r\n}\r\ndev_info(&dev->dev, "ibmvnic registered\n");\r\nreturn 0;\r\nfree_debugfs:\r\nif (adapter->debugfs_dir && !IS_ERR(adapter->debugfs_dir))\r\ndebugfs_remove_recursive(adapter->debugfs_dir);\r\nfree_crq:\r\nibmvnic_release_crq_queue(adapter);\r\nfree_netdev:\r\nfree_netdev(netdev);\r\nreturn rc;\r\n}\r\nstatic int ibmvnic_remove(struct vio_dev *dev)\r\n{\r\nstruct net_device *netdev = dev_get_drvdata(&dev->dev);\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nunregister_netdev(netdev);\r\nrelease_sub_crqs(adapter);\r\nibmvnic_release_crq_queue(adapter);\r\nif (adapter->debugfs_dir && !IS_ERR(adapter->debugfs_dir))\r\ndebugfs_remove_recursive(adapter->debugfs_dir);\r\nif (adapter->ras_comps)\r\ndma_free_coherent(&dev->dev,\r\nadapter->ras_comp_num *\r\nsizeof(struct ibmvnic_fw_component),\r\nadapter->ras_comps, adapter->ras_comps_tok);\r\nkfree(adapter->ras_comp_int);\r\nfree_netdev(netdev);\r\ndev_set_drvdata(&dev->dev, NULL);\r\nreturn 0;\r\n}\r\nstatic unsigned long ibmvnic_get_desired_dma(struct vio_dev *vdev)\r\n{\r\nstruct net_device *netdev = dev_get_drvdata(&vdev->dev);\r\nstruct ibmvnic_adapter *adapter;\r\nstruct iommu_table *tbl;\r\nunsigned long ret = 0;\r\nint i;\r\ntbl = get_iommu_table_base(&vdev->dev);\r\nif (!netdev)\r\nreturn IOMMU_PAGE_ALIGN(IBMVNIC_IO_ENTITLEMENT_DEFAULT, tbl);\r\nadapter = netdev_priv(netdev);\r\nret += PAGE_SIZE;\r\nret += adapter->bounce_buffer_size;\r\nret += IOMMU_PAGE_ALIGN(sizeof(struct ibmvnic_statistics), tbl);\r\nfor (i = 0; i < adapter->req_tx_queues + adapter->req_rx_queues; i++)\r\nret += 4 * PAGE_SIZE;\r\nfor (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);\r\ni++)\r\nret += adapter->rx_pool[i].size *\r\nIOMMU_PAGE_ALIGN(adapter->rx_pool[i].buff_size, tbl);\r\nreturn ret;\r\n}\r\nstatic int ibmvnic_resume(struct device *dev)\r\n{\r\nstruct net_device *netdev = dev_get_drvdata(dev);\r\nstruct ibmvnic_adapter *adapter = netdev_priv(netdev);\r\nint i;\r\nfor (i = 0; i < adapter->req_rx_queues; i++)\r\nibmvnic_interrupt_rx(adapter->rx_scrq[i]->irq,\r\nadapter->rx_scrq[i]);\r\nreturn 0;\r\n}\r\nstatic int __init ibmvnic_module_init(void)\r\n{\r\npr_info("%s: %s %s\n", ibmvnic_driver_name, ibmvnic_driver_string,\r\nIBMVNIC_DRIVER_VERSION);\r\nreturn vio_register_driver(&ibmvnic_driver);\r\n}\r\nstatic void __exit ibmvnic_module_exit(void)\r\n{\r\nvio_unregister_driver(&ibmvnic_driver);\r\n}
