static inline void deprecated_attr_warn(const char *name)\r\n{\r\npr_warn_once("%d (%s) Attribute %s (and others) will be removed. %s\n",\r\ntask_pid_nr(current),\r\ncurrent->comm,\r\nname,\r\n"See zram documentation.");\r\n}\r\nstatic inline bool init_done(struct zram *zram)\r\n{\r\nreturn zram->disksize;\r\n}\r\nstatic inline struct zram *dev_to_zram(struct device *dev)\r\n{\r\nreturn (struct zram *)dev_to_disk(dev)->private_data;\r\n}\r\nstatic int zram_test_flag(struct zram_meta *meta, u32 index,\r\nenum zram_pageflags flag)\r\n{\r\nreturn meta->table[index].value & BIT(flag);\r\n}\r\nstatic void zram_set_flag(struct zram_meta *meta, u32 index,\r\nenum zram_pageflags flag)\r\n{\r\nmeta->table[index].value |= BIT(flag);\r\n}\r\nstatic void zram_clear_flag(struct zram_meta *meta, u32 index,\r\nenum zram_pageflags flag)\r\n{\r\nmeta->table[index].value &= ~BIT(flag);\r\n}\r\nstatic size_t zram_get_obj_size(struct zram_meta *meta, u32 index)\r\n{\r\nreturn meta->table[index].value & (BIT(ZRAM_FLAG_SHIFT) - 1);\r\n}\r\nstatic void zram_set_obj_size(struct zram_meta *meta,\r\nu32 index, size_t size)\r\n{\r\nunsigned long flags = meta->table[index].value >> ZRAM_FLAG_SHIFT;\r\nmeta->table[index].value = (flags << ZRAM_FLAG_SHIFT) | size;\r\n}\r\nstatic inline bool is_partial_io(struct bio_vec *bvec)\r\n{\r\nreturn bvec->bv_len != PAGE_SIZE;\r\n}\r\nstatic inline bool valid_io_request(struct zram *zram,\r\nsector_t start, unsigned int size)\r\n{\r\nu64 end, bound;\r\nif (unlikely(start & (ZRAM_SECTOR_PER_LOGICAL_BLOCK - 1)))\r\nreturn false;\r\nif (unlikely(size & (ZRAM_LOGICAL_BLOCK_SIZE - 1)))\r\nreturn false;\r\nend = start + (size >> SECTOR_SHIFT);\r\nbound = zram->disksize >> SECTOR_SHIFT;\r\nif (unlikely(start >= bound || end > bound || start > end))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic void update_position(u32 *index, int *offset, struct bio_vec *bvec)\r\n{\r\nif (*offset + bvec->bv_len >= PAGE_SIZE)\r\n(*index)++;\r\n*offset = (*offset + bvec->bv_len) % PAGE_SIZE;\r\n}\r\nstatic inline void update_used_max(struct zram *zram,\r\nconst unsigned long pages)\r\n{\r\nunsigned long old_max, cur_max;\r\nold_max = atomic_long_read(&zram->stats.max_used_pages);\r\ndo {\r\ncur_max = old_max;\r\nif (pages > cur_max)\r\nold_max = atomic_long_cmpxchg(\r\n&zram->stats.max_used_pages, cur_max, pages);\r\n} while (old_max != cur_max);\r\n}\r\nstatic bool page_zero_filled(void *ptr)\r\n{\r\nunsigned int pos;\r\nunsigned long *page;\r\npage = (unsigned long *)ptr;\r\nfor (pos = 0; pos != PAGE_SIZE / sizeof(*page); pos++) {\r\nif (page[pos])\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic void handle_zero_page(struct bio_vec *bvec)\r\n{\r\nstruct page *page = bvec->bv_page;\r\nvoid *user_mem;\r\nuser_mem = kmap_atomic(page);\r\nif (is_partial_io(bvec))\r\nmemset(user_mem + bvec->bv_offset, 0, bvec->bv_len);\r\nelse\r\nclear_page(user_mem);\r\nkunmap_atomic(user_mem);\r\nflush_dcache_page(page);\r\n}\r\nstatic ssize_t initstate_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nu32 val;\r\nstruct zram *zram = dev_to_zram(dev);\r\ndown_read(&zram->init_lock);\r\nval = init_done(zram);\r\nup_read(&zram->init_lock);\r\nreturn scnprintf(buf, PAGE_SIZE, "%u\n", val);\r\n}\r\nstatic ssize_t disksize_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct zram *zram = dev_to_zram(dev);\r\nreturn scnprintf(buf, PAGE_SIZE, "%llu\n", zram->disksize);\r\n}\r\nstatic ssize_t orig_data_size_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct zram *zram = dev_to_zram(dev);\r\ndeprecated_attr_warn("orig_data_size");\r\nreturn scnprintf(buf, PAGE_SIZE, "%llu\n",\r\n(u64)(atomic64_read(&zram->stats.pages_stored)) << PAGE_SHIFT);\r\n}\r\nstatic ssize_t mem_used_total_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nu64 val = 0;\r\nstruct zram *zram = dev_to_zram(dev);\r\ndeprecated_attr_warn("mem_used_total");\r\ndown_read(&zram->init_lock);\r\nif (init_done(zram)) {\r\nstruct zram_meta *meta = zram->meta;\r\nval = zs_get_total_pages(meta->mem_pool);\r\n}\r\nup_read(&zram->init_lock);\r\nreturn scnprintf(buf, PAGE_SIZE, "%llu\n", val << PAGE_SHIFT);\r\n}\r\nstatic ssize_t mem_limit_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nu64 val;\r\nstruct zram *zram = dev_to_zram(dev);\r\ndeprecated_attr_warn("mem_limit");\r\ndown_read(&zram->init_lock);\r\nval = zram->limit_pages;\r\nup_read(&zram->init_lock);\r\nreturn scnprintf(buf, PAGE_SIZE, "%llu\n", val << PAGE_SHIFT);\r\n}\r\nstatic ssize_t mem_limit_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nu64 limit;\r\nchar *tmp;\r\nstruct zram *zram = dev_to_zram(dev);\r\nlimit = memparse(buf, &tmp);\r\nif (buf == tmp)\r\nreturn -EINVAL;\r\ndown_write(&zram->init_lock);\r\nzram->limit_pages = PAGE_ALIGN(limit) >> PAGE_SHIFT;\r\nup_write(&zram->init_lock);\r\nreturn len;\r\n}\r\nstatic ssize_t mem_used_max_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nu64 val = 0;\r\nstruct zram *zram = dev_to_zram(dev);\r\ndeprecated_attr_warn("mem_used_max");\r\ndown_read(&zram->init_lock);\r\nif (init_done(zram))\r\nval = atomic_long_read(&zram->stats.max_used_pages);\r\nup_read(&zram->init_lock);\r\nreturn scnprintf(buf, PAGE_SIZE, "%llu\n", val << PAGE_SHIFT);\r\n}\r\nstatic ssize_t mem_used_max_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nint err;\r\nunsigned long val;\r\nstruct zram *zram = dev_to_zram(dev);\r\nerr = kstrtoul(buf, 10, &val);\r\nif (err || val != 0)\r\nreturn -EINVAL;\r\ndown_read(&zram->init_lock);\r\nif (init_done(zram)) {\r\nstruct zram_meta *meta = zram->meta;\r\natomic_long_set(&zram->stats.max_used_pages,\r\nzs_get_total_pages(meta->mem_pool));\r\n}\r\nup_read(&zram->init_lock);\r\nreturn len;\r\n}\r\nstatic ssize_t max_comp_streams_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nint val;\r\nstruct zram *zram = dev_to_zram(dev);\r\ndown_read(&zram->init_lock);\r\nval = zram->max_comp_streams;\r\nup_read(&zram->init_lock);\r\nreturn scnprintf(buf, PAGE_SIZE, "%d\n", val);\r\n}\r\nstatic ssize_t max_comp_streams_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nint num;\r\nstruct zram *zram = dev_to_zram(dev);\r\nint ret;\r\nret = kstrtoint(buf, 0, &num);\r\nif (ret < 0)\r\nreturn ret;\r\nif (num < 1)\r\nreturn -EINVAL;\r\ndown_write(&zram->init_lock);\r\nif (init_done(zram)) {\r\nif (!zcomp_set_max_streams(zram->comp, num)) {\r\npr_info("Cannot change max compression streams\n");\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nzram->max_comp_streams = num;\r\nret = len;\r\nout:\r\nup_write(&zram->init_lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t comp_algorithm_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nsize_t sz;\r\nstruct zram *zram = dev_to_zram(dev);\r\ndown_read(&zram->init_lock);\r\nsz = zcomp_available_show(zram->compressor, buf);\r\nup_read(&zram->init_lock);\r\nreturn sz;\r\n}\r\nstatic ssize_t comp_algorithm_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nstruct zram *zram = dev_to_zram(dev);\r\nsize_t sz;\r\nif (!zcomp_available_algorithm(buf))\r\nreturn -EINVAL;\r\ndown_write(&zram->init_lock);\r\nif (init_done(zram)) {\r\nup_write(&zram->init_lock);\r\npr_info("Can't change algorithm for initialized device\n");\r\nreturn -EBUSY;\r\n}\r\nstrlcpy(zram->compressor, buf, sizeof(zram->compressor));\r\nsz = strlen(zram->compressor);\r\nif (sz > 0 && zram->compressor[sz - 1] == '\n')\r\nzram->compressor[sz - 1] = 0x00;\r\nup_write(&zram->init_lock);\r\nreturn len;\r\n}\r\nstatic ssize_t compact_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nstruct zram *zram = dev_to_zram(dev);\r\nstruct zram_meta *meta;\r\ndown_read(&zram->init_lock);\r\nif (!init_done(zram)) {\r\nup_read(&zram->init_lock);\r\nreturn -EINVAL;\r\n}\r\nmeta = zram->meta;\r\nzs_compact(meta->mem_pool);\r\nup_read(&zram->init_lock);\r\nreturn len;\r\n}\r\nstatic ssize_t io_stat_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct zram *zram = dev_to_zram(dev);\r\nssize_t ret;\r\ndown_read(&zram->init_lock);\r\nret = scnprintf(buf, PAGE_SIZE,\r\n"%8llu %8llu %8llu %8llu\n",\r\n(u64)atomic64_read(&zram->stats.failed_reads),\r\n(u64)atomic64_read(&zram->stats.failed_writes),\r\n(u64)atomic64_read(&zram->stats.invalid_io),\r\n(u64)atomic64_read(&zram->stats.notify_free));\r\nup_read(&zram->init_lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t mm_stat_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct zram *zram = dev_to_zram(dev);\r\nstruct zs_pool_stats pool_stats;\r\nu64 orig_size, mem_used = 0;\r\nlong max_used;\r\nssize_t ret;\r\nmemset(&pool_stats, 0x00, sizeof(struct zs_pool_stats));\r\ndown_read(&zram->init_lock);\r\nif (init_done(zram)) {\r\nmem_used = zs_get_total_pages(zram->meta->mem_pool);\r\nzs_pool_stats(zram->meta->mem_pool, &pool_stats);\r\n}\r\norig_size = atomic64_read(&zram->stats.pages_stored);\r\nmax_used = atomic_long_read(&zram->stats.max_used_pages);\r\nret = scnprintf(buf, PAGE_SIZE,\r\n"%8llu %8llu %8llu %8lu %8ld %8llu %8lu\n",\r\norig_size << PAGE_SHIFT,\r\n(u64)atomic64_read(&zram->stats.compr_data_size),\r\nmem_used << PAGE_SHIFT,\r\nzram->limit_pages << PAGE_SHIFT,\r\nmax_used << PAGE_SHIFT,\r\n(u64)atomic64_read(&zram->stats.zero_pages),\r\npool_stats.pages_compacted);\r\nup_read(&zram->init_lock);\r\nreturn ret;\r\n}\r\nstatic inline bool zram_meta_get(struct zram *zram)\r\n{\r\nif (atomic_inc_not_zero(&zram->refcount))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline void zram_meta_put(struct zram *zram)\r\n{\r\natomic_dec(&zram->refcount);\r\n}\r\nstatic void zram_meta_free(struct zram_meta *meta, u64 disksize)\r\n{\r\nsize_t num_pages = disksize >> PAGE_SHIFT;\r\nsize_t index;\r\nfor (index = 0; index < num_pages; index++) {\r\nunsigned long handle = meta->table[index].handle;\r\nif (!handle)\r\ncontinue;\r\nzs_free(meta->mem_pool, handle);\r\n}\r\nzs_destroy_pool(meta->mem_pool);\r\nvfree(meta->table);\r\nkfree(meta);\r\n}\r\nstatic struct zram_meta *zram_meta_alloc(char *pool_name, u64 disksize)\r\n{\r\nsize_t num_pages;\r\nstruct zram_meta *meta = kmalloc(sizeof(*meta), GFP_KERNEL);\r\nif (!meta)\r\nreturn NULL;\r\nnum_pages = disksize >> PAGE_SHIFT;\r\nmeta->table = vzalloc(num_pages * sizeof(*meta->table));\r\nif (!meta->table) {\r\npr_err("Error allocating zram address table\n");\r\ngoto out_error;\r\n}\r\nmeta->mem_pool = zs_create_pool(pool_name, GFP_NOIO | __GFP_HIGHMEM);\r\nif (!meta->mem_pool) {\r\npr_err("Error creating memory pool\n");\r\ngoto out_error;\r\n}\r\nreturn meta;\r\nout_error:\r\nvfree(meta->table);\r\nkfree(meta);\r\nreturn NULL;\r\n}\r\nstatic void zram_free_page(struct zram *zram, size_t index)\r\n{\r\nstruct zram_meta *meta = zram->meta;\r\nunsigned long handle = meta->table[index].handle;\r\nif (unlikely(!handle)) {\r\nif (zram_test_flag(meta, index, ZRAM_ZERO)) {\r\nzram_clear_flag(meta, index, ZRAM_ZERO);\r\natomic64_dec(&zram->stats.zero_pages);\r\n}\r\nreturn;\r\n}\r\nzs_free(meta->mem_pool, handle);\r\natomic64_sub(zram_get_obj_size(meta, index),\r\n&zram->stats.compr_data_size);\r\natomic64_dec(&zram->stats.pages_stored);\r\nmeta->table[index].handle = 0;\r\nzram_set_obj_size(meta, index, 0);\r\n}\r\nstatic int zram_decompress_page(struct zram *zram, char *mem, u32 index)\r\n{\r\nint ret = 0;\r\nunsigned char *cmem;\r\nstruct zram_meta *meta = zram->meta;\r\nunsigned long handle;\r\nsize_t size;\r\nbit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);\r\nhandle = meta->table[index].handle;\r\nsize = zram_get_obj_size(meta, index);\r\nif (!handle || zram_test_flag(meta, index, ZRAM_ZERO)) {\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\nclear_page(mem);\r\nreturn 0;\r\n}\r\ncmem = zs_map_object(meta->mem_pool, handle, ZS_MM_RO);\r\nif (size == PAGE_SIZE)\r\ncopy_page(mem, cmem);\r\nelse\r\nret = zcomp_decompress(zram->comp, cmem, size, mem);\r\nzs_unmap_object(meta->mem_pool, handle);\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\nif (unlikely(ret)) {\r\npr_err("Decompression failed! err=%d, page=%u\n", ret, index);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,\r\nu32 index, int offset)\r\n{\r\nint ret;\r\nstruct page *page;\r\nunsigned char *user_mem, *uncmem = NULL;\r\nstruct zram_meta *meta = zram->meta;\r\npage = bvec->bv_page;\r\nbit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);\r\nif (unlikely(!meta->table[index].handle) ||\r\nzram_test_flag(meta, index, ZRAM_ZERO)) {\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\nhandle_zero_page(bvec);\r\nreturn 0;\r\n}\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\nif (is_partial_io(bvec))\r\nuncmem = kmalloc(PAGE_SIZE, GFP_NOIO);\r\nuser_mem = kmap_atomic(page);\r\nif (!is_partial_io(bvec))\r\nuncmem = user_mem;\r\nif (!uncmem) {\r\npr_err("Unable to allocate temp memory\n");\r\nret = -ENOMEM;\r\ngoto out_cleanup;\r\n}\r\nret = zram_decompress_page(zram, uncmem, index);\r\nif (unlikely(ret))\r\ngoto out_cleanup;\r\nif (is_partial_io(bvec))\r\nmemcpy(user_mem + bvec->bv_offset, uncmem + offset,\r\nbvec->bv_len);\r\nflush_dcache_page(page);\r\nret = 0;\r\nout_cleanup:\r\nkunmap_atomic(user_mem);\r\nif (is_partial_io(bvec))\r\nkfree(uncmem);\r\nreturn ret;\r\n}\r\nstatic int zram_bvec_write(struct zram *zram, struct bio_vec *bvec, u32 index,\r\nint offset)\r\n{\r\nint ret = 0;\r\nsize_t clen;\r\nunsigned long handle;\r\nstruct page *page;\r\nunsigned char *user_mem, *cmem, *src, *uncmem = NULL;\r\nstruct zram_meta *meta = zram->meta;\r\nstruct zcomp_strm *zstrm = NULL;\r\nunsigned long alloced_pages;\r\npage = bvec->bv_page;\r\nif (is_partial_io(bvec)) {\r\nuncmem = kmalloc(PAGE_SIZE, GFP_NOIO);\r\nif (!uncmem) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = zram_decompress_page(zram, uncmem, index);\r\nif (ret)\r\ngoto out;\r\n}\r\nzstrm = zcomp_strm_find(zram->comp);\r\nuser_mem = kmap_atomic(page);\r\nif (is_partial_io(bvec)) {\r\nmemcpy(uncmem + offset, user_mem + bvec->bv_offset,\r\nbvec->bv_len);\r\nkunmap_atomic(user_mem);\r\nuser_mem = NULL;\r\n} else {\r\nuncmem = user_mem;\r\n}\r\nif (page_zero_filled(uncmem)) {\r\nif (user_mem)\r\nkunmap_atomic(user_mem);\r\nbit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);\r\nzram_free_page(zram, index);\r\nzram_set_flag(meta, index, ZRAM_ZERO);\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\natomic64_inc(&zram->stats.zero_pages);\r\nret = 0;\r\ngoto out;\r\n}\r\nret = zcomp_compress(zram->comp, zstrm, uncmem, &clen);\r\nif (!is_partial_io(bvec)) {\r\nkunmap_atomic(user_mem);\r\nuser_mem = NULL;\r\nuncmem = NULL;\r\n}\r\nif (unlikely(ret)) {\r\npr_err("Compression failed! err=%d\n", ret);\r\ngoto out;\r\n}\r\nsrc = zstrm->buffer;\r\nif (unlikely(clen > max_zpage_size)) {\r\nclen = PAGE_SIZE;\r\nif (is_partial_io(bvec))\r\nsrc = uncmem;\r\n}\r\nhandle = zs_malloc(meta->mem_pool, clen);\r\nif (!handle) {\r\npr_err("Error allocating memory for compressed page: %u, size=%zu\n",\r\nindex, clen);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nalloced_pages = zs_get_total_pages(meta->mem_pool);\r\nupdate_used_max(zram, alloced_pages);\r\nif (zram->limit_pages && alloced_pages > zram->limit_pages) {\r\nzs_free(meta->mem_pool, handle);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ncmem = zs_map_object(meta->mem_pool, handle, ZS_MM_WO);\r\nif ((clen == PAGE_SIZE) && !is_partial_io(bvec)) {\r\nsrc = kmap_atomic(page);\r\ncopy_page(cmem, src);\r\nkunmap_atomic(src);\r\n} else {\r\nmemcpy(cmem, src, clen);\r\n}\r\nzcomp_strm_release(zram->comp, zstrm);\r\nzstrm = NULL;\r\nzs_unmap_object(meta->mem_pool, handle);\r\nbit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);\r\nzram_free_page(zram, index);\r\nmeta->table[index].handle = handle;\r\nzram_set_obj_size(meta, index, clen);\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\natomic64_add(clen, &zram->stats.compr_data_size);\r\natomic64_inc(&zram->stats.pages_stored);\r\nout:\r\nif (zstrm)\r\nzcomp_strm_release(zram->comp, zstrm);\r\nif (is_partial_io(bvec))\r\nkfree(uncmem);\r\nreturn ret;\r\n}\r\nstatic void zram_bio_discard(struct zram *zram, u32 index,\r\nint offset, struct bio *bio)\r\n{\r\nsize_t n = bio->bi_iter.bi_size;\r\nstruct zram_meta *meta = zram->meta;\r\nif (offset) {\r\nif (n <= (PAGE_SIZE - offset))\r\nreturn;\r\nn -= (PAGE_SIZE - offset);\r\nindex++;\r\n}\r\nwhile (n >= PAGE_SIZE) {\r\nbit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);\r\nzram_free_page(zram, index);\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\natomic64_inc(&zram->stats.notify_free);\r\nindex++;\r\nn -= PAGE_SIZE;\r\n}\r\n}\r\nstatic int zram_bvec_rw(struct zram *zram, struct bio_vec *bvec, u32 index,\r\nint offset, int rw)\r\n{\r\nunsigned long start_time = jiffies;\r\nint ret;\r\ngeneric_start_io_acct(rw, bvec->bv_len >> SECTOR_SHIFT,\r\n&zram->disk->part0);\r\nif (rw == READ) {\r\natomic64_inc(&zram->stats.num_reads);\r\nret = zram_bvec_read(zram, bvec, index, offset);\r\n} else {\r\natomic64_inc(&zram->stats.num_writes);\r\nret = zram_bvec_write(zram, bvec, index, offset);\r\n}\r\ngeneric_end_io_acct(rw, &zram->disk->part0, start_time);\r\nif (unlikely(ret)) {\r\nif (rw == READ)\r\natomic64_inc(&zram->stats.failed_reads);\r\nelse\r\natomic64_inc(&zram->stats.failed_writes);\r\n}\r\nreturn ret;\r\n}\r\nstatic void __zram_make_request(struct zram *zram, struct bio *bio)\r\n{\r\nint offset, rw;\r\nu32 index;\r\nstruct bio_vec bvec;\r\nstruct bvec_iter iter;\r\nindex = bio->bi_iter.bi_sector >> SECTORS_PER_PAGE_SHIFT;\r\noffset = (bio->bi_iter.bi_sector &\r\n(SECTORS_PER_PAGE - 1)) << SECTOR_SHIFT;\r\nif (unlikely(bio->bi_rw & REQ_DISCARD)) {\r\nzram_bio_discard(zram, index, offset, bio);\r\nbio_endio(bio);\r\nreturn;\r\n}\r\nrw = bio_data_dir(bio);\r\nbio_for_each_segment(bvec, bio, iter) {\r\nint max_transfer_size = PAGE_SIZE - offset;\r\nif (bvec.bv_len > max_transfer_size) {\r\nstruct bio_vec bv;\r\nbv.bv_page = bvec.bv_page;\r\nbv.bv_len = max_transfer_size;\r\nbv.bv_offset = bvec.bv_offset;\r\nif (zram_bvec_rw(zram, &bv, index, offset, rw) < 0)\r\ngoto out;\r\nbv.bv_len = bvec.bv_len - max_transfer_size;\r\nbv.bv_offset += max_transfer_size;\r\nif (zram_bvec_rw(zram, &bv, index + 1, 0, rw) < 0)\r\ngoto out;\r\n} else\r\nif (zram_bvec_rw(zram, &bvec, index, offset, rw) < 0)\r\ngoto out;\r\nupdate_position(&index, &offset, &bvec);\r\n}\r\nbio_endio(bio);\r\nreturn;\r\nout:\r\nbio_io_error(bio);\r\n}\r\nstatic blk_qc_t zram_make_request(struct request_queue *queue, struct bio *bio)\r\n{\r\nstruct zram *zram = queue->queuedata;\r\nif (unlikely(!zram_meta_get(zram)))\r\ngoto error;\r\nblk_queue_split(queue, &bio, queue->bio_split);\r\nif (!valid_io_request(zram, bio->bi_iter.bi_sector,\r\nbio->bi_iter.bi_size)) {\r\natomic64_inc(&zram->stats.invalid_io);\r\ngoto put_zram;\r\n}\r\n__zram_make_request(zram, bio);\r\nzram_meta_put(zram);\r\nreturn BLK_QC_T_NONE;\r\nput_zram:\r\nzram_meta_put(zram);\r\nerror:\r\nbio_io_error(bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic void zram_slot_free_notify(struct block_device *bdev,\r\nunsigned long index)\r\n{\r\nstruct zram *zram;\r\nstruct zram_meta *meta;\r\nzram = bdev->bd_disk->private_data;\r\nmeta = zram->meta;\r\nbit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);\r\nzram_free_page(zram, index);\r\nbit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);\r\natomic64_inc(&zram->stats.notify_free);\r\n}\r\nstatic int zram_rw_page(struct block_device *bdev, sector_t sector,\r\nstruct page *page, int rw)\r\n{\r\nint offset, err = -EIO;\r\nu32 index;\r\nstruct zram *zram;\r\nstruct bio_vec bv;\r\nzram = bdev->bd_disk->private_data;\r\nif (unlikely(!zram_meta_get(zram)))\r\ngoto out;\r\nif (!valid_io_request(zram, sector, PAGE_SIZE)) {\r\natomic64_inc(&zram->stats.invalid_io);\r\nerr = -EINVAL;\r\ngoto put_zram;\r\n}\r\nindex = sector >> SECTORS_PER_PAGE_SHIFT;\r\noffset = sector & (SECTORS_PER_PAGE - 1) << SECTOR_SHIFT;\r\nbv.bv_page = page;\r\nbv.bv_len = PAGE_SIZE;\r\nbv.bv_offset = 0;\r\nerr = zram_bvec_rw(zram, &bv, index, offset, rw);\r\nput_zram:\r\nzram_meta_put(zram);\r\nout:\r\nif (err == 0)\r\npage_endio(page, rw, 0);\r\nreturn err;\r\n}\r\nstatic void zram_reset_device(struct zram *zram)\r\n{\r\nstruct zram_meta *meta;\r\nstruct zcomp *comp;\r\nu64 disksize;\r\ndown_write(&zram->init_lock);\r\nzram->limit_pages = 0;\r\nif (!init_done(zram)) {\r\nup_write(&zram->init_lock);\r\nreturn;\r\n}\r\nmeta = zram->meta;\r\ncomp = zram->comp;\r\ndisksize = zram->disksize;\r\nzram_meta_put(zram);\r\nwait_event(zram->io_done, atomic_read(&zram->refcount) == 0);\r\nmemset(&zram->stats, 0, sizeof(zram->stats));\r\nzram->disksize = 0;\r\nzram->max_comp_streams = 1;\r\nset_capacity(zram->disk, 0);\r\npart_stat_set_all(&zram->disk->part0, 0);\r\nup_write(&zram->init_lock);\r\nzram_meta_free(meta, disksize);\r\nzcomp_destroy(comp);\r\n}\r\nstatic ssize_t disksize_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nu64 disksize;\r\nstruct zcomp *comp;\r\nstruct zram_meta *meta;\r\nstruct zram *zram = dev_to_zram(dev);\r\nint err;\r\ndisksize = memparse(buf, NULL);\r\nif (!disksize)\r\nreturn -EINVAL;\r\ndisksize = PAGE_ALIGN(disksize);\r\nmeta = zram_meta_alloc(zram->disk->disk_name, disksize);\r\nif (!meta)\r\nreturn -ENOMEM;\r\ncomp = zcomp_create(zram->compressor, zram->max_comp_streams);\r\nif (IS_ERR(comp)) {\r\npr_err("Cannot initialise %s compressing backend\n",\r\nzram->compressor);\r\nerr = PTR_ERR(comp);\r\ngoto out_free_meta;\r\n}\r\ndown_write(&zram->init_lock);\r\nif (init_done(zram)) {\r\npr_info("Cannot change disksize for initialized device\n");\r\nerr = -EBUSY;\r\ngoto out_destroy_comp;\r\n}\r\ninit_waitqueue_head(&zram->io_done);\r\natomic_set(&zram->refcount, 1);\r\nzram->meta = meta;\r\nzram->comp = comp;\r\nzram->disksize = disksize;\r\nset_capacity(zram->disk, zram->disksize >> SECTOR_SHIFT);\r\nup_write(&zram->init_lock);\r\nrevalidate_disk(zram->disk);\r\nreturn len;\r\nout_destroy_comp:\r\nup_write(&zram->init_lock);\r\nzcomp_destroy(comp);\r\nout_free_meta:\r\nzram_meta_free(meta, disksize);\r\nreturn err;\r\n}\r\nstatic ssize_t reset_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t len)\r\n{\r\nint ret;\r\nunsigned short do_reset;\r\nstruct zram *zram;\r\nstruct block_device *bdev;\r\nret = kstrtou16(buf, 10, &do_reset);\r\nif (ret)\r\nreturn ret;\r\nif (!do_reset)\r\nreturn -EINVAL;\r\nzram = dev_to_zram(dev);\r\nbdev = bdget_disk(zram->disk, 0);\r\nif (!bdev)\r\nreturn -ENOMEM;\r\nmutex_lock(&bdev->bd_mutex);\r\nif (bdev->bd_openers || zram->claim) {\r\nmutex_unlock(&bdev->bd_mutex);\r\nbdput(bdev);\r\nreturn -EBUSY;\r\n}\r\nzram->claim = true;\r\nmutex_unlock(&bdev->bd_mutex);\r\nfsync_bdev(bdev);\r\nzram_reset_device(zram);\r\nrevalidate_disk(zram->disk);\r\nbdput(bdev);\r\nmutex_lock(&bdev->bd_mutex);\r\nzram->claim = false;\r\nmutex_unlock(&bdev->bd_mutex);\r\nreturn len;\r\n}\r\nstatic int zram_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nint ret = 0;\r\nstruct zram *zram;\r\nWARN_ON(!mutex_is_locked(&bdev->bd_mutex));\r\nzram = bdev->bd_disk->private_data;\r\nif (zram->claim)\r\nret = -EBUSY;\r\nreturn ret;\r\n}\r\nstatic int zram_add(void)\r\n{\r\nstruct zram *zram;\r\nstruct request_queue *queue;\r\nint ret, device_id;\r\nzram = kzalloc(sizeof(struct zram), GFP_KERNEL);\r\nif (!zram)\r\nreturn -ENOMEM;\r\nret = idr_alloc(&zram_index_idr, zram, 0, 0, GFP_KERNEL);\r\nif (ret < 0)\r\ngoto out_free_dev;\r\ndevice_id = ret;\r\ninit_rwsem(&zram->init_lock);\r\nqueue = blk_alloc_queue(GFP_KERNEL);\r\nif (!queue) {\r\npr_err("Error allocating disk queue for device %d\n",\r\ndevice_id);\r\nret = -ENOMEM;\r\ngoto out_free_idr;\r\n}\r\nblk_queue_make_request(queue, zram_make_request);\r\nzram->disk = alloc_disk(1);\r\nif (!zram->disk) {\r\npr_err("Error allocating disk structure for device %d\n",\r\ndevice_id);\r\nret = -ENOMEM;\r\ngoto out_free_queue;\r\n}\r\nzram->disk->major = zram_major;\r\nzram->disk->first_minor = device_id;\r\nzram->disk->fops = &zram_devops;\r\nzram->disk->queue = queue;\r\nzram->disk->queue->queuedata = zram;\r\nzram->disk->private_data = zram;\r\nsnprintf(zram->disk->disk_name, 16, "zram%d", device_id);\r\nset_capacity(zram->disk, 0);\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, zram->disk->queue);\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, zram->disk->queue);\r\nblk_queue_physical_block_size(zram->disk->queue, PAGE_SIZE);\r\nblk_queue_logical_block_size(zram->disk->queue,\r\nZRAM_LOGICAL_BLOCK_SIZE);\r\nblk_queue_io_min(zram->disk->queue, PAGE_SIZE);\r\nblk_queue_io_opt(zram->disk->queue, PAGE_SIZE);\r\nzram->disk->queue->limits.discard_granularity = PAGE_SIZE;\r\nblk_queue_max_discard_sectors(zram->disk->queue, UINT_MAX);\r\nif (ZRAM_LOGICAL_BLOCK_SIZE == PAGE_SIZE)\r\nzram->disk->queue->limits.discard_zeroes_data = 1;\r\nelse\r\nzram->disk->queue->limits.discard_zeroes_data = 0;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, zram->disk->queue);\r\nadd_disk(zram->disk);\r\nret = sysfs_create_group(&disk_to_dev(zram->disk)->kobj,\r\n&zram_disk_attr_group);\r\nif (ret < 0) {\r\npr_err("Error creating sysfs group for device %d\n",\r\ndevice_id);\r\ngoto out_free_disk;\r\n}\r\nstrlcpy(zram->compressor, default_compressor, sizeof(zram->compressor));\r\nzram->meta = NULL;\r\nzram->max_comp_streams = 1;\r\npr_info("Added device: %s\n", zram->disk->disk_name);\r\nreturn device_id;\r\nout_free_disk:\r\ndel_gendisk(zram->disk);\r\nput_disk(zram->disk);\r\nout_free_queue:\r\nblk_cleanup_queue(queue);\r\nout_free_idr:\r\nidr_remove(&zram_index_idr, device_id);\r\nout_free_dev:\r\nkfree(zram);\r\nreturn ret;\r\n}\r\nstatic int zram_remove(struct zram *zram)\r\n{\r\nstruct block_device *bdev;\r\nbdev = bdget_disk(zram->disk, 0);\r\nif (!bdev)\r\nreturn -ENOMEM;\r\nmutex_lock(&bdev->bd_mutex);\r\nif (bdev->bd_openers || zram->claim) {\r\nmutex_unlock(&bdev->bd_mutex);\r\nbdput(bdev);\r\nreturn -EBUSY;\r\n}\r\nzram->claim = true;\r\nmutex_unlock(&bdev->bd_mutex);\r\nsysfs_remove_group(&disk_to_dev(zram->disk)->kobj,\r\n&zram_disk_attr_group);\r\nfsync_bdev(bdev);\r\nzram_reset_device(zram);\r\nbdput(bdev);\r\npr_info("Removed device: %s\n", zram->disk->disk_name);\r\nblk_cleanup_queue(zram->disk->queue);\r\ndel_gendisk(zram->disk);\r\nput_disk(zram->disk);\r\nkfree(zram);\r\nreturn 0;\r\n}\r\nstatic ssize_t hot_add_show(struct class *class,\r\nstruct class_attribute *attr,\r\nchar *buf)\r\n{\r\nint ret;\r\nmutex_lock(&zram_index_mutex);\r\nret = zram_add();\r\nmutex_unlock(&zram_index_mutex);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn scnprintf(buf, PAGE_SIZE, "%d\n", ret);\r\n}\r\nstatic ssize_t hot_remove_store(struct class *class,\r\nstruct class_attribute *attr,\r\nconst char *buf,\r\nsize_t count)\r\n{\r\nstruct zram *zram;\r\nint ret, dev_id;\r\nret = kstrtoint(buf, 10, &dev_id);\r\nif (ret)\r\nreturn ret;\r\nif (dev_id < 0)\r\nreturn -EINVAL;\r\nmutex_lock(&zram_index_mutex);\r\nzram = idr_find(&zram_index_idr, dev_id);\r\nif (zram) {\r\nret = zram_remove(zram);\r\nidr_remove(&zram_index_idr, dev_id);\r\n} else {\r\nret = -ENODEV;\r\n}\r\nmutex_unlock(&zram_index_mutex);\r\nreturn ret ? ret : count;\r\n}\r\nstatic int zram_remove_cb(int id, void *ptr, void *data)\r\n{\r\nzram_remove(ptr);\r\nreturn 0;\r\n}\r\nstatic void destroy_devices(void)\r\n{\r\nclass_unregister(&zram_control_class);\r\nidr_for_each(&zram_index_idr, &zram_remove_cb, NULL);\r\nidr_destroy(&zram_index_idr);\r\nunregister_blkdev(zram_major, "zram");\r\n}\r\nstatic int __init zram_init(void)\r\n{\r\nint ret;\r\nret = class_register(&zram_control_class);\r\nif (ret) {\r\npr_err("Unable to register zram-control class\n");\r\nreturn ret;\r\n}\r\nzram_major = register_blkdev(0, "zram");\r\nif (zram_major <= 0) {\r\npr_err("Unable to get major number\n");\r\nclass_unregister(&zram_control_class);\r\nreturn -EBUSY;\r\n}\r\nwhile (num_devices != 0) {\r\nmutex_lock(&zram_index_mutex);\r\nret = zram_add();\r\nmutex_unlock(&zram_index_mutex);\r\nif (ret < 0)\r\ngoto out_error;\r\nnum_devices--;\r\n}\r\nreturn 0;\r\nout_error:\r\ndestroy_devices();\r\nreturn ret;\r\n}\r\nstatic void __exit zram_exit(void)\r\n{\r\ndestroy_devices();\r\n}
