static void iommu_flushall(struct iommu_map_table *iommu_map_table)\r\n{\r\nstruct iommu *iommu = container_of(iommu_map_table, struct iommu, tbl);\r\nif (iommu->iommu_flushinv) {\r\niommu_write(iommu->iommu_flushinv, ~(u64)0);\r\n} else {\r\nunsigned long tag;\r\nint entry;\r\ntag = iommu->iommu_tags;\r\nfor (entry = 0; entry < 16; entry++) {\r\niommu_write(tag, 0);\r\ntag += 8;\r\n}\r\n(void) iommu_read(iommu->write_complete_reg);\r\n}\r\n}\r\nstatic inline void iopte_make_dummy(struct iommu *iommu, iopte_t *iopte)\r\n{\r\nunsigned long val = iopte_val(*iopte);\r\nval &= ~IOPTE_PAGE;\r\nval |= iommu->dummy_page_pa;\r\niopte_val(*iopte) = val;\r\n}\r\nint iommu_table_init(struct iommu *iommu, int tsbsize,\r\nu32 dma_offset, u32 dma_addr_mask,\r\nint numa_node)\r\n{\r\nunsigned long i, order, sz, num_tsb_entries;\r\nstruct page *page;\r\nnum_tsb_entries = tsbsize / sizeof(iopte_t);\r\nspin_lock_init(&iommu->lock);\r\niommu->ctx_lowest_free = 1;\r\niommu->tbl.table_map_base = dma_offset;\r\niommu->dma_addr_mask = dma_addr_mask;\r\nsz = num_tsb_entries / 8;\r\nsz = (sz + 7UL) & ~7UL;\r\niommu->tbl.map = kmalloc_node(sz, GFP_KERNEL, numa_node);\r\nif (!iommu->tbl.map)\r\nreturn -ENOMEM;\r\nmemset(iommu->tbl.map, 0, sz);\r\niommu_tbl_pool_init(&iommu->tbl, num_tsb_entries, IO_PAGE_SHIFT,\r\n(tlb_type != hypervisor ? iommu_flushall : NULL),\r\nfalse, 1, false);\r\npage = alloc_pages_node(numa_node, GFP_KERNEL, 0);\r\nif (!page) {\r\nprintk(KERN_ERR "IOMMU: Error, gfp(dummy_page) failed.\n");\r\ngoto out_free_map;\r\n}\r\niommu->dummy_page = (unsigned long) page_address(page);\r\nmemset((void *)iommu->dummy_page, 0, PAGE_SIZE);\r\niommu->dummy_page_pa = (unsigned long) __pa(iommu->dummy_page);\r\norder = get_order(tsbsize);\r\npage = alloc_pages_node(numa_node, GFP_KERNEL, order);\r\nif (!page) {\r\nprintk(KERN_ERR "IOMMU: Error, gfp(tsb) failed.\n");\r\ngoto out_free_dummy_page;\r\n}\r\niommu->page_table = (iopte_t *)page_address(page);\r\nfor (i = 0; i < num_tsb_entries; i++)\r\niopte_make_dummy(iommu, &iommu->page_table[i]);\r\nreturn 0;\r\nout_free_dummy_page:\r\nfree_page(iommu->dummy_page);\r\niommu->dummy_page = 0UL;\r\nout_free_map:\r\nkfree(iommu->tbl.map);\r\niommu->tbl.map = NULL;\r\nreturn -ENOMEM;\r\n}\r\nstatic inline iopte_t *alloc_npages(struct device *dev,\r\nstruct iommu *iommu,\r\nunsigned long npages)\r\n{\r\nunsigned long entry;\r\nentry = iommu_tbl_range_alloc(dev, &iommu->tbl, npages, NULL,\r\n(unsigned long)(-1), 0);\r\nif (unlikely(entry == IOMMU_ERROR_CODE))\r\nreturn NULL;\r\nreturn iommu->page_table + entry;\r\n}\r\nstatic int iommu_alloc_ctx(struct iommu *iommu)\r\n{\r\nint lowest = iommu->ctx_lowest_free;\r\nint n = find_next_zero_bit(iommu->ctx_bitmap, IOMMU_NUM_CTXS, lowest);\r\nif (unlikely(n == IOMMU_NUM_CTXS)) {\r\nn = find_next_zero_bit(iommu->ctx_bitmap, lowest, 1);\r\nif (unlikely(n == lowest)) {\r\nprintk(KERN_WARNING "IOMMU: Ran out of contexts.\n");\r\nn = 0;\r\n}\r\n}\r\nif (n)\r\n__set_bit(n, iommu->ctx_bitmap);\r\nreturn n;\r\n}\r\nstatic inline void iommu_free_ctx(struct iommu *iommu, int ctx)\r\n{\r\nif (likely(ctx)) {\r\n__clear_bit(ctx, iommu->ctx_bitmap);\r\nif (ctx < iommu->ctx_lowest_free)\r\niommu->ctx_lowest_free = ctx;\r\n}\r\n}\r\nstatic void *dma_4u_alloc_coherent(struct device *dev, size_t size,\r\ndma_addr_t *dma_addrp, gfp_t gfp,\r\nstruct dma_attrs *attrs)\r\n{\r\nunsigned long order, first_page;\r\nstruct iommu *iommu;\r\nstruct page *page;\r\nint npages, nid;\r\niopte_t *iopte;\r\nvoid *ret;\r\nsize = IO_PAGE_ALIGN(size);\r\norder = get_order(size);\r\nif (order >= 10)\r\nreturn NULL;\r\nnid = dev->archdata.numa_node;\r\npage = alloc_pages_node(nid, gfp, order);\r\nif (unlikely(!page))\r\nreturn NULL;\r\nfirst_page = (unsigned long) page_address(page);\r\nmemset((char *)first_page, 0, PAGE_SIZE << order);\r\niommu = dev->archdata.iommu;\r\niopte = alloc_npages(dev, iommu, size >> IO_PAGE_SHIFT);\r\nif (unlikely(iopte == NULL)) {\r\nfree_pages(first_page, order);\r\nreturn NULL;\r\n}\r\n*dma_addrp = (iommu->tbl.table_map_base +\r\n((iopte - iommu->page_table) << IO_PAGE_SHIFT));\r\nret = (void *) first_page;\r\nnpages = size >> IO_PAGE_SHIFT;\r\nfirst_page = __pa(first_page);\r\nwhile (npages--) {\r\niopte_val(*iopte) = (IOPTE_CONSISTENT(0UL) |\r\nIOPTE_WRITE |\r\n(first_page & IOPTE_PAGE));\r\niopte++;\r\nfirst_page += IO_PAGE_SIZE;\r\n}\r\nreturn ret;\r\n}\r\nstatic void dma_4u_free_coherent(struct device *dev, size_t size,\r\nvoid *cpu, dma_addr_t dvma,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct iommu *iommu;\r\nunsigned long order, npages;\r\nnpages = IO_PAGE_ALIGN(size) >> IO_PAGE_SHIFT;\r\niommu = dev->archdata.iommu;\r\niommu_tbl_range_free(&iommu->tbl, dvma, npages, IOMMU_ERROR_CODE);\r\norder = get_order(size);\r\nif (order < 10)\r\nfree_pages((unsigned long)cpu, order);\r\n}\r\nstatic dma_addr_t dma_4u_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t sz,\r\nenum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct iommu *iommu;\r\nstruct strbuf *strbuf;\r\niopte_t *base;\r\nunsigned long flags, npages, oaddr;\r\nunsigned long i, base_paddr, ctx;\r\nu32 bus_addr, ret;\r\nunsigned long iopte_protection;\r\niommu = dev->archdata.iommu;\r\nstrbuf = dev->archdata.stc;\r\nif (unlikely(direction == DMA_NONE))\r\ngoto bad_no_ctx;\r\noaddr = (unsigned long)(page_address(page) + offset);\r\nnpages = IO_PAGE_ALIGN(oaddr + sz) - (oaddr & IO_PAGE_MASK);\r\nnpages >>= IO_PAGE_SHIFT;\r\nbase = alloc_npages(dev, iommu, npages);\r\nspin_lock_irqsave(&iommu->lock, flags);\r\nctx = 0;\r\nif (iommu->iommu_ctxflush)\r\nctx = iommu_alloc_ctx(iommu);\r\nspin_unlock_irqrestore(&iommu->lock, flags);\r\nif (unlikely(!base))\r\ngoto bad;\r\nbus_addr = (iommu->tbl.table_map_base +\r\n((base - iommu->page_table) << IO_PAGE_SHIFT));\r\nret = bus_addr | (oaddr & ~IO_PAGE_MASK);\r\nbase_paddr = __pa(oaddr & IO_PAGE_MASK);\r\nif (strbuf->strbuf_enabled)\r\niopte_protection = IOPTE_STREAMING(ctx);\r\nelse\r\niopte_protection = IOPTE_CONSISTENT(ctx);\r\nif (direction != DMA_TO_DEVICE)\r\niopte_protection |= IOPTE_WRITE;\r\nfor (i = 0; i < npages; i++, base++, base_paddr += IO_PAGE_SIZE)\r\niopte_val(*base) = iopte_protection | base_paddr;\r\nreturn ret;\r\nbad:\r\niommu_free_ctx(iommu, ctx);\r\nbad_no_ctx:\r\nif (printk_ratelimit())\r\nWARN_ON(1);\r\nreturn DMA_ERROR_CODE;\r\n}\r\nstatic void strbuf_flush(struct strbuf *strbuf, struct iommu *iommu,\r\nu32 vaddr, unsigned long ctx, unsigned long npages,\r\nenum dma_data_direction direction)\r\n{\r\nint limit;\r\nif (strbuf->strbuf_ctxflush &&\r\niommu->iommu_ctxflush) {\r\nunsigned long matchreg, flushreg;\r\nu64 val;\r\nflushreg = strbuf->strbuf_ctxflush;\r\nmatchreg = STC_CTXMATCH_ADDR(strbuf, ctx);\r\niommu_write(flushreg, ctx);\r\nval = iommu_read(matchreg);\r\nval &= 0xffff;\r\nif (!val)\r\ngoto do_flush_sync;\r\nwhile (val) {\r\nif (val & 0x1)\r\niommu_write(flushreg, ctx);\r\nval >>= 1;\r\n}\r\nval = iommu_read(matchreg);\r\nif (unlikely(val)) {\r\nprintk(KERN_WARNING "strbuf_flush: ctx flush "\r\n"timeout matchreg[%llx] ctx[%lx]\n",\r\nval, ctx);\r\ngoto do_page_flush;\r\n}\r\n} else {\r\nunsigned long i;\r\ndo_page_flush:\r\nfor (i = 0; i < npages; i++, vaddr += IO_PAGE_SIZE)\r\niommu_write(strbuf->strbuf_pflush, vaddr);\r\n}\r\ndo_flush_sync:\r\nif (direction == DMA_TO_DEVICE)\r\nreturn;\r\nSTC_FLUSHFLAG_INIT(strbuf);\r\niommu_write(strbuf->strbuf_fsync, strbuf->strbuf_flushflag_pa);\r\n(void) iommu_read(iommu->write_complete_reg);\r\nlimit = 100000;\r\nwhile (!STC_FLUSHFLAG_SET(strbuf)) {\r\nlimit--;\r\nif (!limit)\r\nbreak;\r\nudelay(1);\r\nrmb();\r\n}\r\nif (!limit)\r\nprintk(KERN_WARNING "strbuf_flush: flushflag timeout "\r\n"vaddr[%08x] ctx[%lx] npages[%ld]\n",\r\nvaddr, ctx, npages);\r\n}\r\nstatic void dma_4u_unmap_page(struct device *dev, dma_addr_t bus_addr,\r\nsize_t sz, enum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct iommu *iommu;\r\nstruct strbuf *strbuf;\r\niopte_t *base;\r\nunsigned long flags, npages, ctx, i;\r\nif (unlikely(direction == DMA_NONE)) {\r\nif (printk_ratelimit())\r\nWARN_ON(1);\r\nreturn;\r\n}\r\niommu = dev->archdata.iommu;\r\nstrbuf = dev->archdata.stc;\r\nnpages = IO_PAGE_ALIGN(bus_addr + sz) - (bus_addr & IO_PAGE_MASK);\r\nnpages >>= IO_PAGE_SHIFT;\r\nbase = iommu->page_table +\r\n((bus_addr - iommu->tbl.table_map_base) >> IO_PAGE_SHIFT);\r\nbus_addr &= IO_PAGE_MASK;\r\nspin_lock_irqsave(&iommu->lock, flags);\r\nctx = 0;\r\nif (iommu->iommu_ctxflush)\r\nctx = (iopte_val(*base) & IOPTE_CONTEXT) >> 47UL;\r\nif (strbuf->strbuf_enabled)\r\nstrbuf_flush(strbuf, iommu, bus_addr, ctx,\r\nnpages, direction);\r\nfor (i = 0; i < npages; i++)\r\niopte_make_dummy(iommu, base + i);\r\niommu_free_ctx(iommu, ctx);\r\nspin_unlock_irqrestore(&iommu->lock, flags);\r\niommu_tbl_range_free(&iommu->tbl, bus_addr, npages, IOMMU_ERROR_CODE);\r\n}\r\nstatic int dma_4u_map_sg(struct device *dev, struct scatterlist *sglist,\r\nint nelems, enum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *s, *outs, *segstart;\r\nunsigned long flags, handle, prot, ctx;\r\ndma_addr_t dma_next = 0, dma_addr;\r\nunsigned int max_seg_size;\r\nunsigned long seg_boundary_size;\r\nint outcount, incount, i;\r\nstruct strbuf *strbuf;\r\nstruct iommu *iommu;\r\nunsigned long base_shift;\r\nBUG_ON(direction == DMA_NONE);\r\niommu = dev->archdata.iommu;\r\nstrbuf = dev->archdata.stc;\r\nif (nelems == 0 || !iommu)\r\nreturn 0;\r\nspin_lock_irqsave(&iommu->lock, flags);\r\nctx = 0;\r\nif (iommu->iommu_ctxflush)\r\nctx = iommu_alloc_ctx(iommu);\r\nif (strbuf->strbuf_enabled)\r\nprot = IOPTE_STREAMING(ctx);\r\nelse\r\nprot = IOPTE_CONSISTENT(ctx);\r\nif (direction != DMA_TO_DEVICE)\r\nprot |= IOPTE_WRITE;\r\nouts = s = segstart = &sglist[0];\r\noutcount = 1;\r\nincount = nelems;\r\nhandle = 0;\r\nouts->dma_length = 0;\r\nmax_seg_size = dma_get_max_seg_size(dev);\r\nseg_boundary_size = ALIGN(dma_get_seg_boundary(dev) + 1,\r\nIO_PAGE_SIZE) >> IO_PAGE_SHIFT;\r\nbase_shift = iommu->tbl.table_map_base >> IO_PAGE_SHIFT;\r\nfor_each_sg(sglist, s, nelems, i) {\r\nunsigned long paddr, npages, entry, out_entry = 0, slen;\r\niopte_t *base;\r\nslen = s->length;\r\nif (slen == 0) {\r\ndma_next = 0;\r\ncontinue;\r\n}\r\npaddr = (unsigned long) SG_ENT_PHYS_ADDRESS(s);\r\nnpages = iommu_num_pages(paddr, slen, IO_PAGE_SIZE);\r\nentry = iommu_tbl_range_alloc(dev, &iommu->tbl, npages,\r\n&handle, (unsigned long)(-1), 0);\r\nif (unlikely(entry == IOMMU_ERROR_CODE)) {\r\nif (printk_ratelimit())\r\nprintk(KERN_INFO "iommu_alloc failed, iommu %p paddr %lx"\r\n" npages %lx\n", iommu, paddr, npages);\r\ngoto iommu_map_failed;\r\n}\r\nbase = iommu->page_table + entry;\r\ndma_addr = iommu->tbl.table_map_base +\r\n(entry << IO_PAGE_SHIFT);\r\ndma_addr |= (s->offset & ~IO_PAGE_MASK);\r\npaddr &= IO_PAGE_MASK;\r\nwhile (npages--) {\r\niopte_val(*base) = prot | paddr;\r\nbase++;\r\npaddr += IO_PAGE_SIZE;\r\n}\r\nif (segstart != s) {\r\nif ((dma_addr != dma_next) ||\r\n(outs->dma_length + s->length > max_seg_size) ||\r\n(is_span_boundary(out_entry, base_shift,\r\nseg_boundary_size, outs, s))) {\r\nsegstart = s;\r\noutcount++;\r\nouts = sg_next(outs);\r\n} else {\r\nouts->dma_length += s->length;\r\n}\r\n}\r\nif (segstart == s) {\r\nouts->dma_address = dma_addr;\r\nouts->dma_length = slen;\r\nout_entry = entry;\r\n}\r\ndma_next = dma_addr + slen;\r\n}\r\nspin_unlock_irqrestore(&iommu->lock, flags);\r\nif (outcount < incount) {\r\nouts = sg_next(outs);\r\nouts->dma_address = DMA_ERROR_CODE;\r\nouts->dma_length = 0;\r\n}\r\nreturn outcount;\r\niommu_map_failed:\r\nfor_each_sg(sglist, s, nelems, i) {\r\nif (s->dma_length != 0) {\r\nunsigned long vaddr, npages, entry, j;\r\niopte_t *base;\r\nvaddr = s->dma_address & IO_PAGE_MASK;\r\nnpages = iommu_num_pages(s->dma_address, s->dma_length,\r\nIO_PAGE_SIZE);\r\nentry = (vaddr - iommu->tbl.table_map_base)\r\n>> IO_PAGE_SHIFT;\r\nbase = iommu->page_table + entry;\r\nfor (j = 0; j < npages; j++)\r\niopte_make_dummy(iommu, base + j);\r\niommu_tbl_range_free(&iommu->tbl, vaddr, npages,\r\nIOMMU_ERROR_CODE);\r\ns->dma_address = DMA_ERROR_CODE;\r\ns->dma_length = 0;\r\n}\r\nif (s == outs)\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&iommu->lock, flags);\r\nreturn 0;\r\n}\r\nstatic unsigned long fetch_sg_ctx(struct iommu *iommu, struct scatterlist *sg)\r\n{\r\nunsigned long ctx = 0;\r\nif (iommu->iommu_ctxflush) {\r\niopte_t *base;\r\nu32 bus_addr;\r\nstruct iommu_map_table *tbl = &iommu->tbl;\r\nbus_addr = sg->dma_address & IO_PAGE_MASK;\r\nbase = iommu->page_table +\r\n((bus_addr - tbl->table_map_base) >> IO_PAGE_SHIFT);\r\nctx = (iopte_val(*base) & IOPTE_CONTEXT) >> 47UL;\r\n}\r\nreturn ctx;\r\n}\r\nstatic void dma_4u_unmap_sg(struct device *dev, struct scatterlist *sglist,\r\nint nelems, enum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nunsigned long flags, ctx;\r\nstruct scatterlist *sg;\r\nstruct strbuf *strbuf;\r\nstruct iommu *iommu;\r\nBUG_ON(direction == DMA_NONE);\r\niommu = dev->archdata.iommu;\r\nstrbuf = dev->archdata.stc;\r\nctx = fetch_sg_ctx(iommu, sglist);\r\nspin_lock_irqsave(&iommu->lock, flags);\r\nsg = sglist;\r\nwhile (nelems--) {\r\ndma_addr_t dma_handle = sg->dma_address;\r\nunsigned int len = sg->dma_length;\r\nunsigned long npages, entry;\r\niopte_t *base;\r\nint i;\r\nif (!len)\r\nbreak;\r\nnpages = iommu_num_pages(dma_handle, len, IO_PAGE_SIZE);\r\nentry = ((dma_handle - iommu->tbl.table_map_base)\r\n>> IO_PAGE_SHIFT);\r\nbase = iommu->page_table + entry;\r\ndma_handle &= IO_PAGE_MASK;\r\nif (strbuf->strbuf_enabled)\r\nstrbuf_flush(strbuf, iommu, dma_handle, ctx,\r\nnpages, direction);\r\nfor (i = 0; i < npages; i++)\r\niopte_make_dummy(iommu, base + i);\r\niommu_tbl_range_free(&iommu->tbl, dma_handle, npages,\r\nIOMMU_ERROR_CODE);\r\nsg = sg_next(sg);\r\n}\r\niommu_free_ctx(iommu, ctx);\r\nspin_unlock_irqrestore(&iommu->lock, flags);\r\n}\r\nstatic void dma_4u_sync_single_for_cpu(struct device *dev,\r\ndma_addr_t bus_addr, size_t sz,\r\nenum dma_data_direction direction)\r\n{\r\nstruct iommu *iommu;\r\nstruct strbuf *strbuf;\r\nunsigned long flags, ctx, npages;\r\niommu = dev->archdata.iommu;\r\nstrbuf = dev->archdata.stc;\r\nif (!strbuf->strbuf_enabled)\r\nreturn;\r\nspin_lock_irqsave(&iommu->lock, flags);\r\nnpages = IO_PAGE_ALIGN(bus_addr + sz) - (bus_addr & IO_PAGE_MASK);\r\nnpages >>= IO_PAGE_SHIFT;\r\nbus_addr &= IO_PAGE_MASK;\r\nctx = 0;\r\nif (iommu->iommu_ctxflush &&\r\nstrbuf->strbuf_ctxflush) {\r\niopte_t *iopte;\r\nstruct iommu_map_table *tbl = &iommu->tbl;\r\niopte = iommu->page_table +\r\n((bus_addr - tbl->table_map_base)>>IO_PAGE_SHIFT);\r\nctx = (iopte_val(*iopte) & IOPTE_CONTEXT) >> 47UL;\r\n}\r\nstrbuf_flush(strbuf, iommu, bus_addr, ctx, npages, direction);\r\nspin_unlock_irqrestore(&iommu->lock, flags);\r\n}\r\nstatic void dma_4u_sync_sg_for_cpu(struct device *dev,\r\nstruct scatterlist *sglist, int nelems,\r\nenum dma_data_direction direction)\r\n{\r\nstruct iommu *iommu;\r\nstruct strbuf *strbuf;\r\nunsigned long flags, ctx, npages, i;\r\nstruct scatterlist *sg, *sgprv;\r\nu32 bus_addr;\r\niommu = dev->archdata.iommu;\r\nstrbuf = dev->archdata.stc;\r\nif (!strbuf->strbuf_enabled)\r\nreturn;\r\nspin_lock_irqsave(&iommu->lock, flags);\r\nctx = 0;\r\nif (iommu->iommu_ctxflush &&\r\nstrbuf->strbuf_ctxflush) {\r\niopte_t *iopte;\r\nstruct iommu_map_table *tbl = &iommu->tbl;\r\niopte = iommu->page_table + ((sglist[0].dma_address -\r\ntbl->table_map_base) >> IO_PAGE_SHIFT);\r\nctx = (iopte_val(*iopte) & IOPTE_CONTEXT) >> 47UL;\r\n}\r\nbus_addr = sglist[0].dma_address & IO_PAGE_MASK;\r\nsgprv = NULL;\r\nfor_each_sg(sglist, sg, nelems, i) {\r\nif (sg->dma_length == 0)\r\nbreak;\r\nsgprv = sg;\r\n}\r\nnpages = (IO_PAGE_ALIGN(sgprv->dma_address + sgprv->dma_length)\r\n- bus_addr) >> IO_PAGE_SHIFT;\r\nstrbuf_flush(strbuf, iommu, bus_addr, ctx, npages, direction);\r\nspin_unlock_irqrestore(&iommu->lock, flags);\r\n}\r\nint dma_supported(struct device *dev, u64 device_mask)\r\n{\r\nstruct iommu *iommu = dev->archdata.iommu;\r\nu64 dma_addr_mask = iommu->dma_addr_mask;\r\nif (device_mask >= (1UL << 32UL))\r\nreturn 0;\r\nif ((device_mask & dma_addr_mask) == dma_addr_mask)\r\nreturn 1;\r\n#ifdef CONFIG_PCI\r\nif (dev_is_pci(dev))\r\nreturn pci64_dma_supported(to_pci_dev(dev), device_mask);\r\n#endif\r\nreturn 0;\r\n}
