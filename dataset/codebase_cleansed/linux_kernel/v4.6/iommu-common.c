static inline bool need_flush(struct iommu_map_table *iommu)\r\n{\r\nreturn ((iommu->flags & IOMMU_NEED_FLUSH) != 0);\r\n}\r\nstatic inline void set_flush(struct iommu_map_table *iommu)\r\n{\r\niommu->flags |= IOMMU_NEED_FLUSH;\r\n}\r\nstatic inline void clear_flush(struct iommu_map_table *iommu)\r\n{\r\niommu->flags &= ~IOMMU_NEED_FLUSH;\r\n}\r\nstatic void setup_iommu_pool_hash(void)\r\n{\r\nunsigned int i;\r\nstatic bool do_once;\r\nif (do_once)\r\nreturn;\r\ndo_once = true;\r\nfor_each_possible_cpu(i)\r\nper_cpu(iommu_hash_common, i) = hash_32(i, IOMMU_POOL_HASHBITS);\r\n}\r\nvoid iommu_tbl_pool_init(struct iommu_map_table *iommu,\r\nunsigned long num_entries,\r\nu32 table_shift,\r\nvoid (*lazy_flush)(struct iommu_map_table *),\r\nbool large_pool, u32 npools,\r\nbool skip_span_boundary_check)\r\n{\r\nunsigned int start, i;\r\nstruct iommu_pool *p = &(iommu->large_pool);\r\nsetup_iommu_pool_hash();\r\nif (npools == 0)\r\niommu->nr_pools = IOMMU_NR_POOLS;\r\nelse\r\niommu->nr_pools = npools;\r\nBUG_ON(npools > IOMMU_NR_POOLS);\r\niommu->table_shift = table_shift;\r\niommu->lazy_flush = lazy_flush;\r\nstart = 0;\r\nif (skip_span_boundary_check)\r\niommu->flags |= IOMMU_NO_SPAN_BOUND;\r\nif (large_pool)\r\niommu->flags |= IOMMU_HAS_LARGE_POOL;\r\nif (!large_pool)\r\niommu->poolsize = num_entries/iommu->nr_pools;\r\nelse\r\niommu->poolsize = (num_entries * 3 / 4)/iommu->nr_pools;\r\nfor (i = 0; i < iommu->nr_pools; i++) {\r\nspin_lock_init(&(iommu->pools[i].lock));\r\niommu->pools[i].start = start;\r\niommu->pools[i].hint = start;\r\nstart += iommu->poolsize;\r\niommu->pools[i].end = start - 1;\r\n}\r\nif (!large_pool)\r\nreturn;\r\nspin_lock_init(&(p->lock));\r\np->start = start;\r\np->hint = p->start;\r\np->end = num_entries;\r\n}\r\nunsigned long iommu_tbl_range_alloc(struct device *dev,\r\nstruct iommu_map_table *iommu,\r\nunsigned long npages,\r\nunsigned long *handle,\r\nunsigned long mask,\r\nunsigned int align_order)\r\n{\r\nunsigned int pool_hash = __this_cpu_read(iommu_hash_common);\r\nunsigned long n, end, start, limit, boundary_size;\r\nstruct iommu_pool *pool;\r\nint pass = 0;\r\nunsigned int pool_nr;\r\nunsigned int npools = iommu->nr_pools;\r\nunsigned long flags;\r\nbool large_pool = ((iommu->flags & IOMMU_HAS_LARGE_POOL) != 0);\r\nbool largealloc = (large_pool && npages > iommu_large_alloc);\r\nunsigned long shift;\r\nunsigned long align_mask = 0;\r\nif (align_order > 0)\r\nalign_mask = ~0ul >> (BITS_PER_LONG - align_order);\r\nif (unlikely(npages == 0)) {\r\nWARN_ON_ONCE(1);\r\nreturn IOMMU_ERROR_CODE;\r\n}\r\nif (largealloc) {\r\npool = &(iommu->large_pool);\r\npool_nr = 0;\r\n} else {\r\npool_nr = pool_hash & (npools - 1);\r\npool = &(iommu->pools[pool_nr]);\r\n}\r\nspin_lock_irqsave(&pool->lock, flags);\r\nagain:\r\nif (pass == 0 && handle && *handle &&\r\n(*handle >= pool->start) && (*handle < pool->end))\r\nstart = *handle;\r\nelse\r\nstart = pool->hint;\r\nlimit = pool->end;\r\nif (start >= limit)\r\nstart = pool->start;\r\nshift = iommu->table_map_base >> iommu->table_shift;\r\nif (limit + shift > mask) {\r\nlimit = mask - shift + 1;\r\nif ((start & mask) >= limit || pass > 0) {\r\nspin_unlock(&(pool->lock));\r\npool = &(iommu->pools[0]);\r\nspin_lock(&(pool->lock));\r\nstart = pool->start;\r\n} else {\r\nstart &= mask;\r\n}\r\n}\r\nif (dev)\r\nboundary_size = ALIGN(dma_get_seg_boundary(dev) + 1,\r\n1 << iommu->table_shift);\r\nelse\r\nboundary_size = ALIGN(1ULL << 32, 1 << iommu->table_shift);\r\nboundary_size = boundary_size >> iommu->table_shift;\r\nif ((iommu->flags & IOMMU_NO_SPAN_BOUND) != 0) {\r\nshift = 0;\r\nboundary_size = iommu->poolsize * iommu->nr_pools;\r\n}\r\nn = iommu_area_alloc(iommu->map, limit, start, npages, shift,\r\nboundary_size, align_mask);\r\nif (n == -1) {\r\nif (likely(pass == 0)) {\r\npool->hint = pool->start;\r\nset_flush(iommu);\r\npass++;\r\ngoto again;\r\n} else if (!largealloc && pass <= iommu->nr_pools) {\r\nspin_unlock(&(pool->lock));\r\npool_nr = (pool_nr + 1) & (iommu->nr_pools - 1);\r\npool = &(iommu->pools[pool_nr]);\r\nspin_lock(&(pool->lock));\r\npool->hint = pool->start;\r\nset_flush(iommu);\r\npass++;\r\ngoto again;\r\n} else {\r\nn = IOMMU_ERROR_CODE;\r\ngoto bail;\r\n}\r\n}\r\nif (iommu->lazy_flush &&\r\n(n < pool->hint || need_flush(iommu))) {\r\nclear_flush(iommu);\r\niommu->lazy_flush(iommu);\r\n}\r\nend = n + npages;\r\npool->hint = end;\r\nif (handle)\r\n*handle = end;\r\nbail:\r\nspin_unlock_irqrestore(&(pool->lock), flags);\r\nreturn n;\r\n}\r\nstatic struct iommu_pool *get_pool(struct iommu_map_table *tbl,\r\nunsigned long entry)\r\n{\r\nstruct iommu_pool *p;\r\nunsigned long largepool_start = tbl->large_pool.start;\r\nbool large_pool = ((tbl->flags & IOMMU_HAS_LARGE_POOL) != 0);\r\nif (large_pool && entry >= largepool_start) {\r\np = &tbl->large_pool;\r\n} else {\r\nunsigned int pool_nr = entry / tbl->poolsize;\r\nBUG_ON(pool_nr >= tbl->nr_pools);\r\np = &tbl->pools[pool_nr];\r\n}\r\nreturn p;\r\n}\r\nvoid iommu_tbl_range_free(struct iommu_map_table *iommu, u64 dma_addr,\r\nunsigned long npages, unsigned long entry)\r\n{\r\nstruct iommu_pool *pool;\r\nunsigned long flags;\r\nunsigned long shift = iommu->table_shift;\r\nif (entry == IOMMU_ERROR_CODE)\r\nentry = (dma_addr - iommu->table_map_base) >> shift;\r\npool = get_pool(iommu, entry);\r\nspin_lock_irqsave(&(pool->lock), flags);\r\nbitmap_clear(iommu->map, entry, npages);\r\nspin_unlock_irqrestore(&(pool->lock), flags);\r\n}
