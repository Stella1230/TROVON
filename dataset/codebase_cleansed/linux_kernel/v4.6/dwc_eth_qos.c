static void dwceqos_reset_state(struct net_local *lp)\r\n{\r\nlp->link = 0;\r\nlp->speed = 0;\r\nlp->duplex = DUPLEX_UNKNOWN;\r\nlp->flowcontrol.rx_current = 0;\r\nlp->flowcontrol.tx_current = 0;\r\nlp->eee_active = 0;\r\nlp->eee_enabled = 0;\r\n}\r\nstatic void print_descriptor(struct net_local *lp, int index, int tx)\r\n{\r\nstruct dwceqos_dma_desc *dd;\r\nif (tx)\r\ndd = (struct dwceqos_dma_desc *)&lp->tx_descs[index];\r\nelse\r\ndd = (struct dwceqos_dma_desc *)&lp->rx_descs[index];\r\npr_info("%s DMA Descriptor #%d@%p Contents:\n", tx ? "TX" : "RX",\r\nindex, dd);\r\npr_info("0x%08x 0x%08x 0x%08x 0x%08x\n", dd->des0, dd->des1, dd->des2,\r\ndd->des3);\r\n}\r\nstatic void print_status(struct net_local *lp)\r\n{\r\nsize_t desci, i;\r\npr_info("tx_free %zu, tx_cur %zu, tx_next %zu\n", lp->tx_free,\r\nlp->tx_cur, lp->tx_next);\r\nprint_descriptor(lp, lp->rx_cur, 0);\r\nfor (desci = (lp->tx_cur - 10) % DWCEQOS_TX_DCNT, i = 0;\r\ni < DWCEQOS_TX_DCNT;\r\n++i) {\r\nprint_descriptor(lp, desci, 1);\r\ndesci = (desci + 1) % DWCEQOS_TX_DCNT;\r\n}\r\npr_info("DMA_Debug_Status0: 0x%08x\n",\r\ndwceqos_read(lp, REG_DWCEQOS_DMA_DEBUG_ST0));\r\npr_info("DMA_CH0_Status: 0x%08x\n",\r\ndwceqos_read(lp, REG_DWCEQOS_DMA_IS));\r\npr_info("DMA_CH0_Current_App_TxDesc: 0x%08x\n",\r\ndwceqos_read(lp, 0x1144));\r\npr_info("DMA_CH0_Current_App_TxBuff: 0x%08x\n",\r\ndwceqos_read(lp, 0x1154));\r\npr_info("MTL_Debug_Status: 0x%08x\n",\r\ndwceqos_read(lp, REG_DWCEQOS_MTL_DEBUG_ST));\r\npr_info("MTL_TXQ0_Debug_Status: 0x%08x\n",\r\ndwceqos_read(lp, REG_DWCEQOS_MTL_TXQ0_DEBUG_ST));\r\npr_info("MTL_RXQ0_Debug_Status: 0x%08x\n",\r\ndwceqos_read(lp, REG_DWCEQOS_MTL_RXQ0_DEBUG_ST));\r\npr_info("Current TX DMA: 0x%08x, RX DMA: 0x%08x\n",\r\ndwceqos_read(lp, REG_DWCEQOS_DMA_CH0_CUR_TXDESC),\r\ndwceqos_read(lp, REG_DWCEQOS_DMA_CH0_CUR_RXDESC));\r\n}\r\nstatic void dwceqos_mdio_set_csr(struct net_local *lp)\r\n{\r\nint rate = clk_get_rate(lp->apb_pclk);\r\nif (rate <= 20000000)\r\nlp->csr_val = DWCEQOS_MAC_MDIO_ADDR_CR_20;\r\nelse if (rate <= 35000000)\r\nlp->csr_val = DWCEQOS_MAC_MDIO_ADDR_CR_35;\r\nelse if (rate <= 60000000)\r\nlp->csr_val = DWCEQOS_MAC_MDIO_ADDR_CR_60;\r\nelse if (rate <= 100000000)\r\nlp->csr_val = DWCEQOS_MAC_MDIO_ADDR_CR_100;\r\nelse if (rate <= 150000000)\r\nlp->csr_val = DWCEQOS_MAC_MDIO_ADDR_CR_150;\r\nelse if (rate <= 250000000)\r\nlp->csr_val = DWCEQOS_MAC_MDIO_ADDR_CR_250;\r\n}\r\nstatic int dwceqos_mdio_read(struct mii_bus *bus, int mii_id, int phyreg)\r\n{\r\nstruct net_local *lp = bus->priv;\r\nu32 regval;\r\nint i;\r\nint data;\r\nregval = DWCEQOS_MDIO_PHYADDR(mii_id) |\r\nDWCEQOS_MDIO_PHYREG(phyreg) |\r\nDWCEQOS_MAC_MDIO_ADDR_CR(lp->csr_val) |\r\nDWCEQOS_MAC_MDIO_ADDR_GB |\r\nDWCEQOS_MAC_MDIO_ADDR_GOC_READ;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_MDIO_ADDR, regval);\r\nfor (i = 0; i < 5; ++i) {\r\nusleep_range(64, 128);\r\nif (!(dwceqos_read(lp, REG_DWCEQOS_MAC_MDIO_ADDR) &\r\nDWCEQOS_MAC_MDIO_ADDR_GB))\r\nbreak;\r\n}\r\ndata = dwceqos_read(lp, REG_DWCEQOS_MAC_MDIO_DATA);\r\nif (i == 5) {\r\nnetdev_warn(lp->ndev, "MDIO read timed out\n");\r\ndata = 0xffff;\r\n}\r\nreturn data & 0xffff;\r\n}\r\nstatic int dwceqos_mdio_write(struct mii_bus *bus, int mii_id, int phyreg,\r\nu16 value)\r\n{\r\nstruct net_local *lp = bus->priv;\r\nu32 regval;\r\nint i;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_MDIO_DATA, value);\r\nregval = DWCEQOS_MDIO_PHYADDR(mii_id) |\r\nDWCEQOS_MDIO_PHYREG(phyreg) |\r\nDWCEQOS_MAC_MDIO_ADDR_CR(lp->csr_val) |\r\nDWCEQOS_MAC_MDIO_ADDR_GB |\r\nDWCEQOS_MAC_MDIO_ADDR_GOC_WRITE;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_MDIO_ADDR, regval);\r\nfor (i = 0; i < 5; ++i) {\r\nusleep_range(64, 128);\r\nif (!(dwceqos_read(lp, REG_DWCEQOS_MAC_MDIO_ADDR) &\r\nDWCEQOS_MAC_MDIO_ADDR_GB))\r\nbreak;\r\n}\r\nif (i == 5)\r\nnetdev_warn(lp->ndev, "MDIO write timed out\n");\r\nreturn 0;\r\n}\r\nstatic int dwceqos_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct phy_device *phydev = lp->phy_dev;\r\nif (!netif_running(ndev))\r\nreturn -EINVAL;\r\nif (!phydev)\r\nreturn -ENODEV;\r\nswitch (cmd) {\r\ncase SIOCGMIIPHY:\r\ncase SIOCGMIIREG:\r\ncase SIOCSMIIREG:\r\nreturn phy_mii_ioctl(phydev, rq, cmd);\r\ndefault:\r\ndev_info(&lp->pdev->dev, "ioctl %X not implemented.\n", cmd);\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void dwceqos_link_down(struct net_local *lp)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS);\r\nregval &= ~DWCEQOS_MAC_LPI_CTRL_STATUS_PLS;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_link_up(struct net_local *lp)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS);\r\nregval |= DWCEQOS_MAC_LPI_CTRL_STATUS_PLS;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\nlp->eee_active = !phy_init_eee(lp->phy_dev, 0);\r\nif (!lp->eee_active && lp->eee_enabled) {\r\nlp->eee_enabled = 0;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS);\r\nregval &= ~DWCEQOS_LPI_CTRL_ENABLE_EEE;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\n}\r\nstatic void dwceqos_set_speed(struct net_local *lp)\r\n{\r\nstruct phy_device *phydev = lp->phy_dev;\r\nu32 regval;\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_CFG);\r\nregval &= ~(DWCEQOS_MAC_CFG_PS | DWCEQOS_MAC_CFG_FES |\r\nDWCEQOS_MAC_CFG_DM);\r\nif (phydev->duplex)\r\nregval |= DWCEQOS_MAC_CFG_DM;\r\nif (phydev->speed == SPEED_10) {\r\nregval |= DWCEQOS_MAC_CFG_PS;\r\n} else if (phydev->speed == SPEED_100) {\r\nregval |= DWCEQOS_MAC_CFG_PS |\r\nDWCEQOS_MAC_CFG_FES;\r\n} else if (phydev->speed != SPEED_1000) {\r\nnetdev_err(lp->ndev,\r\n"unknown PHY speed %d\n",\r\nphydev->speed);\r\nreturn;\r\n}\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_CFG, regval);\r\n}\r\nstatic void dwceqos_adjust_link(struct net_device *ndev)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct phy_device *phydev = lp->phy_dev;\r\nint status_change = 0;\r\nif (lp->phy_defer)\r\nreturn;\r\nif (phydev->link) {\r\nif ((lp->speed != phydev->speed) ||\r\n(lp->duplex != phydev->duplex)) {\r\ndwceqos_set_speed(lp);\r\nlp->speed = phydev->speed;\r\nlp->duplex = phydev->duplex;\r\nstatus_change = 1;\r\n}\r\nif (lp->flowcontrol.autoneg) {\r\nlp->flowcontrol.rx = phydev->pause ||\r\nphydev->asym_pause;\r\nlp->flowcontrol.tx = phydev->pause ||\r\nphydev->asym_pause;\r\n}\r\nif (lp->flowcontrol.rx != lp->flowcontrol.rx_current) {\r\nif (netif_msg_link(lp))\r\nnetdev_dbg(ndev, "set rx flow to %d\n",\r\nlp->flowcontrol.rx);\r\ndwceqos_set_rx_flowcontrol(lp, lp->flowcontrol.rx);\r\nlp->flowcontrol.rx_current = lp->flowcontrol.rx;\r\n}\r\nif (lp->flowcontrol.tx != lp->flowcontrol.tx_current) {\r\nif (netif_msg_link(lp))\r\nnetdev_dbg(ndev, "set tx flow to %d\n",\r\nlp->flowcontrol.tx);\r\ndwceqos_set_tx_flowcontrol(lp, lp->flowcontrol.tx);\r\nlp->flowcontrol.tx_current = lp->flowcontrol.tx;\r\n}\r\n}\r\nif (phydev->link != lp->link) {\r\nlp->link = phydev->link;\r\nstatus_change = 1;\r\n}\r\nif (status_change) {\r\nif (phydev->link) {\r\nlp->ndev->trans_start = jiffies;\r\ndwceqos_link_up(lp);\r\n} else {\r\ndwceqos_link_down(lp);\r\n}\r\nphy_print_status(phydev);\r\n}\r\n}\r\nstatic int dwceqos_mii_probe(struct net_device *ndev)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct phy_device *phydev = NULL;\r\nif (lp->phy_node) {\r\nphydev = of_phy_connect(lp->ndev,\r\nlp->phy_node,\r\n&dwceqos_adjust_link,\r\n0,\r\nlp->phy_interface);\r\nif (!phydev) {\r\nnetdev_err(ndev, "no PHY found\n");\r\nreturn -1;\r\n}\r\n} else {\r\nnetdev_err(ndev, "no PHY configured\n");\r\nreturn -ENODEV;\r\n}\r\nif (netif_msg_probe(lp))\r\nphy_attached_info(phydev);\r\nphydev->supported &= PHY_GBIT_FEATURES;\r\nlp->link = 0;\r\nlp->speed = 0;\r\nlp->duplex = DUPLEX_UNKNOWN;\r\nlp->phy_dev = phydev;\r\nreturn 0;\r\n}\r\nstatic void dwceqos_alloc_rxring_desc(struct net_local *lp, int index)\r\n{\r\nstruct sk_buff *new_skb;\r\ndma_addr_t new_skb_baddr = 0;\r\nnew_skb = netdev_alloc_skb(lp->ndev, DWCEQOS_RX_BUF_SIZE);\r\nif (!new_skb) {\r\nnetdev_err(lp->ndev, "alloc_skb error for desc %d\n", index);\r\ngoto err_out;\r\n}\r\nnew_skb_baddr = dma_map_single(lp->ndev->dev.parent,\r\nnew_skb->data, DWCEQOS_RX_BUF_SIZE,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(lp->ndev->dev.parent, new_skb_baddr)) {\r\nnetdev_err(lp->ndev, "DMA map error\n");\r\ndev_kfree_skb(new_skb);\r\nnew_skb = NULL;\r\ngoto err_out;\r\n}\r\nlp->rx_descs[index].des0 = new_skb_baddr;\r\nlp->rx_descs[index].des1 = 0;\r\nlp->rx_descs[index].des2 = 0;\r\nlp->rx_descs[index].des3 = DWCEQOS_DMA_RDES3_INTE |\r\nDWCEQOS_DMA_RDES3_BUF1V |\r\nDWCEQOS_DMA_RDES3_OWN;\r\nlp->rx_skb[index].mapping = new_skb_baddr;\r\nlp->rx_skb[index].len = DWCEQOS_RX_BUF_SIZE;\r\nerr_out:\r\nlp->rx_skb[index].skb = new_skb;\r\n}\r\nstatic void dwceqos_clean_rings(struct net_local *lp)\r\n{\r\nint i;\r\nif (lp->rx_skb) {\r\nfor (i = 0; i < DWCEQOS_RX_DCNT; i++) {\r\nif (lp->rx_skb[i].skb) {\r\ndma_unmap_single(lp->ndev->dev.parent,\r\nlp->rx_skb[i].mapping,\r\nlp->rx_skb[i].len,\r\nDMA_FROM_DEVICE);\r\ndev_kfree_skb(lp->rx_skb[i].skb);\r\nlp->rx_skb[i].skb = NULL;\r\nlp->rx_skb[i].mapping = 0;\r\n}\r\n}\r\n}\r\nif (lp->tx_skb) {\r\nfor (i = 0; i < DWCEQOS_TX_DCNT; i++) {\r\nif (lp->tx_skb[i].skb) {\r\ndev_kfree_skb(lp->tx_skb[i].skb);\r\nlp->tx_skb[i].skb = NULL;\r\n}\r\nif (lp->tx_skb[i].mapping) {\r\ndma_unmap_single(lp->ndev->dev.parent,\r\nlp->tx_skb[i].mapping,\r\nlp->tx_skb[i].len,\r\nDMA_TO_DEVICE);\r\nlp->tx_skb[i].mapping = 0;\r\n}\r\n}\r\n}\r\n}\r\nstatic void dwceqos_descriptor_free(struct net_local *lp)\r\n{\r\nint size;\r\ndwceqos_clean_rings(lp);\r\nkfree(lp->tx_skb);\r\nlp->tx_skb = NULL;\r\nkfree(lp->rx_skb);\r\nlp->rx_skb = NULL;\r\nsize = DWCEQOS_RX_DCNT * sizeof(struct dwceqos_dma_desc);\r\nif (lp->rx_descs) {\r\ndma_free_coherent(lp->ndev->dev.parent, size,\r\n(void *)(lp->rx_descs), lp->rx_descs_addr);\r\nlp->rx_descs = NULL;\r\n}\r\nsize = DWCEQOS_TX_DCNT * sizeof(struct dwceqos_dma_desc);\r\nif (lp->tx_descs) {\r\ndma_free_coherent(lp->ndev->dev.parent, size,\r\n(void *)(lp->tx_descs), lp->tx_descs_addr);\r\nlp->tx_descs = NULL;\r\n}\r\n}\r\nstatic int dwceqos_descriptor_init(struct net_local *lp)\r\n{\r\nint size;\r\nu32 i;\r\nlp->gso_size = 0;\r\nlp->tx_skb = NULL;\r\nlp->rx_skb = NULL;\r\nlp->rx_descs = NULL;\r\nlp->tx_descs = NULL;\r\nlp->rx_cur = 0;\r\nlp->tx_cur = 0;\r\nlp->tx_next = 0;\r\nlp->tx_free = DWCEQOS_TX_DCNT;\r\nsize = DWCEQOS_RX_DCNT * sizeof(struct ring_desc);\r\nlp->rx_skb = kzalloc(size, GFP_KERNEL);\r\nif (!lp->rx_skb)\r\ngoto err_out;\r\nsize = DWCEQOS_TX_DCNT * sizeof(struct ring_desc);\r\nlp->tx_skb = kzalloc(size, GFP_KERNEL);\r\nif (!lp->tx_skb)\r\ngoto err_out;\r\nsize = DWCEQOS_RX_DCNT * sizeof(struct dwceqos_dma_desc);\r\nlp->rx_descs = dma_alloc_coherent(lp->ndev->dev.parent, size,\r\n&lp->rx_descs_addr, GFP_KERNEL);\r\nif (!lp->rx_descs)\r\ngoto err_out;\r\nlp->rx_descs_tail_addr = lp->rx_descs_addr +\r\nsizeof(struct dwceqos_dma_desc) * DWCEQOS_RX_DCNT;\r\nsize = DWCEQOS_TX_DCNT * sizeof(struct dwceqos_dma_desc);\r\nlp->tx_descs = dma_alloc_coherent(lp->ndev->dev.parent, size,\r\n&lp->tx_descs_addr, GFP_KERNEL);\r\nif (!lp->tx_descs)\r\ngoto err_out;\r\nlp->tx_descs_tail_addr = lp->tx_descs_addr +\r\nsizeof(struct dwceqos_dma_desc) * DWCEQOS_TX_DCNT;\r\nfor (i = 0; i < DWCEQOS_RX_DCNT; ++i) {\r\ndwceqos_alloc_rxring_desc(lp, i);\r\nif (!(lp->rx_skb[lp->rx_cur].skb))\r\ngoto err_out;\r\n}\r\nfor (i = 0; i < DWCEQOS_TX_DCNT; ++i) {\r\nlp->tx_descs[i].des0 = 0;\r\nlp->tx_descs[i].des1 = 0;\r\nlp->tx_descs[i].des2 = 0;\r\nlp->tx_descs[i].des3 = 0;\r\n}\r\nwmb();\r\nreturn 0;\r\nerr_out:\r\ndwceqos_descriptor_free(lp);\r\nreturn -ENOMEM;\r\n}\r\nstatic int dwceqos_packet_avail(struct net_local *lp)\r\n{\r\nreturn !(lp->rx_descs[lp->rx_cur].des3 & DWCEQOS_DMA_RDES3_OWN);\r\n}\r\nstatic void dwceqos_get_hwfeatures(struct net_local *lp)\r\n{\r\nlp->feature0 = dwceqos_read(lp, REG_DWCEQOS_MAC_HW_FEATURE0);\r\nlp->feature1 = dwceqos_read(lp, REG_DWCEQOS_MAC_HW_FEATURE1);\r\nlp->feature2 = dwceqos_read(lp, REG_DWCEQOS_MAC_HW_FEATURE2);\r\n}\r\nstatic void dwceqos_dma_enable_txirq(struct net_local *lp)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_DMA_CH0_IE);\r\nregval |= DWCEQOS_DMA_CH0_IE_TIE;\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_IE, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_dma_disable_txirq(struct net_local *lp)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_DMA_CH0_IE);\r\nregval &= ~DWCEQOS_DMA_CH0_IE_TIE;\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_IE, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_dma_enable_rxirq(struct net_local *lp)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_DMA_CH0_IE);\r\nregval |= DWCEQOS_DMA_CH0_IE_RIE;\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_IE, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_dma_disable_rxirq(struct net_local *lp)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_DMA_CH0_IE);\r\nregval &= ~DWCEQOS_DMA_CH0_IE_RIE;\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_IE, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_enable_mmc_interrupt(struct net_local *lp)\r\n{\r\ndwceqos_write(lp, REG_DWCEQOS_MMC_RXIRQMASK, 0);\r\ndwceqos_write(lp, REG_DWCEQOS_MMC_TXIRQMASK, 0);\r\n}\r\nstatic int dwceqos_mii_init(struct net_local *lp)\r\n{\r\nint ret = -ENXIO;\r\nstruct resource res;\r\nstruct device_node *mdionode;\r\nmdionode = of_get_child_by_name(lp->pdev->dev.of_node, "mdio");\r\nif (!mdionode)\r\nreturn 0;\r\nlp->mii_bus = mdiobus_alloc();\r\nif (!lp->mii_bus) {\r\nret = -ENOMEM;\r\ngoto err_out;\r\n}\r\nlp->mii_bus->name = "DWCEQOS MII bus";\r\nlp->mii_bus->read = &dwceqos_mdio_read;\r\nlp->mii_bus->write = &dwceqos_mdio_write;\r\nlp->mii_bus->priv = lp;\r\nlp->mii_bus->parent = &lp->ndev->dev;\r\nof_address_to_resource(lp->pdev->dev.of_node, 0, &res);\r\nsnprintf(lp->mii_bus->id, MII_BUS_ID_SIZE, "%.8llx",\r\n(unsigned long long)res.start);\r\nif (of_mdiobus_register(lp->mii_bus, mdionode))\r\ngoto err_out_free_mdiobus;\r\nreturn 0;\r\nerr_out_free_mdiobus:\r\nmdiobus_free(lp->mii_bus);\r\nerr_out:\r\nof_node_put(mdionode);\r\nreturn ret;\r\n}\r\nstatic void dwceqos_reset_hw(struct net_local *lp)\r\n{\r\nint i = 5000;\r\nu32 reg;\r\nreg = dwceqos_read(lp, REG_DWCEQOS_MAC_CFG);\r\nreg &= ~(DWCEQOS_MAC_CFG_PS | DWCEQOS_MAC_CFG_FES);\r\nreg |= DWCEQOS_MAC_CFG_DM;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_CFG, reg);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_MODE, DWCEQOS_DMA_MODE_SWR);\r\ndo {\r\nudelay(100);\r\ni--;\r\nreg = dwceqos_read(lp, REG_DWCEQOS_DMA_MODE);\r\n} while ((reg & DWCEQOS_DMA_MODE_SWR) && i);\r\nif (!i)\r\nnetdev_err(lp->ndev, "DMA reset timed out!\n");\r\n}\r\nstatic void dwceqos_fatal_bus_error(struct net_local *lp, u32 dma_status)\r\n{\r\nif (dma_status & DWCEQOS_DMA_CH0_IS_TEB) {\r\nnetdev_err(lp->ndev, "txdma bus error %s %s (status=%08x)\n",\r\ndma_status & DWCEQOS_DMA_CH0_IS_TX_ERR_READ ?\r\n"read" : "write",\r\ndma_status & DWCEQOS_DMA_CH0_IS_TX_ERR_DESCR ?\r\n"descr" : "data",\r\ndma_status);\r\nprint_status(lp);\r\n}\r\nif (dma_status & DWCEQOS_DMA_CH0_IS_REB) {\r\nnetdev_err(lp->ndev, "rxdma bus error %s %s (status=%08x)\n",\r\ndma_status & DWCEQOS_DMA_CH0_IS_RX_ERR_READ ?\r\n"read" : "write",\r\ndma_status & DWCEQOS_DMA_CH0_IS_RX_ERR_DESCR ?\r\n"descr" : "data",\r\ndma_status);\r\nprint_status(lp);\r\n}\r\n}\r\nstatic void dwceqos_mmc_interrupt(struct net_local *lp)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->stats_lock, flags);\r\ndwceqos_read_mmc_counters(lp,\r\ndwceqos_read(lp, REG_DWCEQOS_MMC_RXIRQ),\r\ndwceqos_read(lp, REG_DWCEQOS_MMC_TXIRQ));\r\nspin_unlock_irqrestore(&lp->stats_lock, flags);\r\n}\r\nstatic void dwceqos_mac_interrupt(struct net_local *lp)\r\n{\r\nu32 cause;\r\ncause = dwceqos_read(lp, REG_DWCEQOS_MAC_IS);\r\nif (cause & DWCEQOS_MAC_IS_MMC_INT)\r\ndwceqos_mmc_interrupt(lp);\r\n}\r\nstatic irqreturn_t dwceqos_interrupt(int irq, void *dev_id)\r\n{\r\nstruct net_device *ndev = dev_id;\r\nstruct net_local *lp = netdev_priv(ndev);\r\nu32 cause;\r\nu32 dma_status;\r\nirqreturn_t ret = IRQ_NONE;\r\ncause = dwceqos_read(lp, REG_DWCEQOS_DMA_IS);\r\nif (cause & DWCEQOS_DMA_IS_DC0IS) {\r\ndma_status = dwceqos_read(lp, REG_DWCEQOS_DMA_CH0_STA);\r\nif (dma_status & DWCEQOS_DMA_CH0_IS_TI) {\r\ntasklet_schedule(&lp->tx_bdreclaim_tasklet);\r\ndwceqos_dma_disable_txirq(lp);\r\n}\r\nif (dma_status & DWCEQOS_DMA_CH0_IS_RI) {\r\ndwceqos_dma_disable_rxirq(lp);\r\nnapi_schedule(&lp->napi);\r\n}\r\nif (unlikely(dma_status & DWCEQOS_DMA_CH0_IS_FBE)) {\r\ndwceqos_fatal_bus_error(lp, dma_status);\r\ndma_status |= DWCEQOS_DMA_CH0_IS_TEB |\r\nDWCEQOS_DMA_CH0_IS_REB;\r\n}\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_STA, dma_status);\r\nret = IRQ_HANDLED;\r\n}\r\nif (cause & DWCEQOS_DMA_IS_MTLIS) {\r\nu32 val = dwceqos_read(lp, REG_DWCEQOS_MTL_Q0_ISCTRL);\r\ndwceqos_write(lp, REG_DWCEQOS_MTL_Q0_ISCTRL, val);\r\nret = IRQ_HANDLED;\r\n}\r\nif (cause & DWCEQOS_DMA_IS_MACIS) {\r\ndwceqos_mac_interrupt(lp);\r\nret = IRQ_HANDLED;\r\n}\r\nreturn ret;\r\n}\r\nstatic void dwceqos_set_rx_flowcontrol(struct net_local *lp, bool enable)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_RX_FLOW_CTRL);\r\nif (enable)\r\nregval |= DWCEQOS_MAC_RX_FLOW_CTRL_RFE;\r\nelse\r\nregval &= ~DWCEQOS_MAC_RX_FLOW_CTRL_RFE;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_RX_FLOW_CTRL, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_set_tx_flowcontrol(struct net_local *lp, bool enable)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MTL_RXQ0_OPER);\r\nif (enable)\r\nregval |= DWCEQOS_MTL_RXQ_EHFC;\r\nelse\r\nregval &= ~DWCEQOS_MTL_RXQ_EHFC;\r\ndwceqos_write(lp, REG_DWCEQOS_MTL_RXQ0_OPER, regval);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_Q0_TX_FLOW);\r\nif (enable)\r\nregval |= DWCEQOS_MAC_Q0_TX_FLOW_TFE;\r\nelse\r\nregval &= ~DWCEQOS_MAC_Q0_TX_FLOW_TFE;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_Q0_TX_FLOW, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_configure_flow_control(struct net_local *lp)\r\n{\r\nu32 regval;\r\nunsigned long flags;\r\nint RQS, RFD, RFA;\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MTL_RXQ0_OPER);\r\nRQS = ((regval >> 20) & 0x3FF) + 1;\r\nRQS /= 2;\r\nRFD = RQS / 2 - 2;\r\nRFA = RQS / 8 - 2;\r\nregval = (regval & 0xFFF000FF) | (RFD << 14) | (RFA << 8);\r\nif (RFD >= 0 && RFA >= 0) {\r\ndwceqos_write(lp, REG_DWCEQOS_MTL_RXQ0_OPER, regval);\r\n} else {\r\nnetdev_warn(lp->ndev,\r\n"FIFO too small for flow control.");\r\n}\r\nregval = DWCEQOS_MAC_Q0_TX_FLOW_PT(256) |\r\nDWCEQOS_MAC_Q0_TX_FLOW_PLT_4_SLOTS;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_Q0_TX_FLOW, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nstatic void dwceqos_configure_clock(struct net_local *lp)\r\n{\r\nunsigned long rate_mhz = clk_get_rate(lp->apb_pclk) / 1000000;\r\nBUG_ON(!rate_mhz);\r\ndwceqos_write(lp,\r\nREG_DWCEQOS_MAC_1US_TIC_COUNTER,\r\nDWCEQOS_MAC_1US_TIC_COUNTER_VAL(rate_mhz - 1));\r\n}\r\nstatic void dwceqos_configure_bus(struct net_local *lp)\r\n{\r\nu32 sysbus_reg;\r\nsysbus_reg = DWCEQOS_DMA_SYSBUS_MODE_AAL;\r\nif (lp->bus_cfg.en_lpi)\r\nsysbus_reg |= DWCEQOS_DMA_SYSBUS_MODE_EN_LPI;\r\nif (lp->bus_cfg.burst_map)\r\nsysbus_reg |= DWCEQOS_DMA_SYSBUS_MODE_BURST(\r\nlp->bus_cfg.burst_map);\r\nelse\r\nsysbus_reg |= DWCEQOS_DMA_SYSBUS_MODE_BURST(\r\nDWCEQOS_DMA_SYSBUS_MODE_BURST_DEFAULT);\r\nif (lp->bus_cfg.read_requests)\r\nsysbus_reg |= DWCEQOS_DMA_SYSBUS_MODE_RD_OSR_LIMIT(\r\nlp->bus_cfg.read_requests - 1);\r\nelse\r\nsysbus_reg |= DWCEQOS_DMA_SYSBUS_MODE_RD_OSR_LIMIT(\r\nDWCEQOS_DMA_SYSBUS_MODE_RD_OSR_LIMIT_DEFAULT);\r\nif (lp->bus_cfg.write_requests)\r\nsysbus_reg |= DWCEQOS_DMA_SYSBUS_MODE_WR_OSR_LIMIT(\r\nlp->bus_cfg.write_requests - 1);\r\nelse\r\nsysbus_reg |= DWCEQOS_DMA_SYSBUS_MODE_WR_OSR_LIMIT(\r\nDWCEQOS_DMA_SYSBUS_MODE_WR_OSR_LIMIT_DEFAULT);\r\nif (netif_msg_hw(lp))\r\nnetdev_dbg(lp->ndev, "SysbusMode %#X\n", sysbus_reg);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_SYSBUS_MODE, sysbus_reg);\r\n}\r\nstatic void dwceqos_init_hw(struct net_local *lp)\r\n{\r\nu32 regval;\r\nu32 buswidth;\r\nu32 dma_skip;\r\ndwceqos_reset_hw(lp);\r\ndwceqos_configure_bus(lp);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_TXDESC_TAIL, 0xF);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_DMA_CH0_TXDESC_TAIL);\r\nbuswidth = (regval ^ 0xF) + 1;\r\ndma_skip = (sizeof(struct dwceqos_dma_desc) - 16) / buswidth;\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_CTRL,\r\nDWCEQOS_DMA_CH_CTRL_DSL(dma_skip) |\r\nDWCEQOS_DMA_CH_CTRL_PBLX8);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_TXDESC_LEN, DWCEQOS_TX_DCNT - 1);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_RXDESC_LEN, DWCEQOS_RX_DCNT - 1);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_TXDESC_LIST,\r\n(u32)lp->tx_descs_addr);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_RXDESC_LIST,\r\n(u32)lp->rx_descs_addr);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_TXDESC_TAIL,\r\nlp->tx_descs_tail_addr);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_RXDESC_TAIL,\r\nlp->rx_descs_tail_addr);\r\nif (lp->bus_cfg.tx_pbl)\r\nregval = DWCEQOS_DMA_CH_CTRL_PBL(lp->bus_cfg.tx_pbl);\r\nelse\r\nregval = DWCEQOS_DMA_CH_CTRL_PBL(2);\r\nif (lp->feature1 & DWCEQOS_MAC_HW_FEATURE1_TSOEN)\r\nregval |= DWCEQOS_DMA_CH_TX_TSE;\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_TX_CTRL, regval);\r\nif (lp->bus_cfg.rx_pbl)\r\nregval = DWCEQOS_DMA_CH_CTRL_PBL(lp->bus_cfg.rx_pbl);\r\nelse\r\nregval = DWCEQOS_DMA_CH_CTRL_PBL(2);\r\nregval |= DWCEQOS_DMA_CH_RX_CTRL_BUFSIZE(DWCEQOS_DWCEQOS_RX_BUF_SIZE);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_RX_CTRL, regval);\r\nregval |= DWCEQOS_DMA_CH_CTRL_START;\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_RX_CTRL, regval);\r\nregval = DWCEQOS_MTL_SCHALG_STRICT;\r\ndwceqos_write(lp, REG_DWCEQOS_MTL_OPER, regval);\r\nregval = DWCEQOS_MTL_TXQ_SIZE(\r\nDWCEQOS_MAC_HW_FEATURE1_TXFIFOSIZE(lp->feature1)) |\r\nDWCEQOS_MTL_TXQ_TXQEN | DWCEQOS_MTL_TXQ_TSF |\r\nDWCEQOS_MTL_TXQ_TTC512;\r\ndwceqos_write(lp, REG_DWCEQOS_MTL_TXQ0_OPER, regval);\r\nregval = DWCEQOS_MTL_RXQ_SIZE(\r\nDWCEQOS_MAC_HW_FEATURE1_RXFIFOSIZE(lp->feature1)) |\r\nDWCEQOS_MTL_RXQ_FUP | DWCEQOS_MTL_RXQ_FEP | DWCEQOS_MTL_RXQ_RSF;\r\ndwceqos_write(lp, REG_DWCEQOS_MTL_RXQ0_OPER, regval);\r\ndwceqos_configure_flow_control(lp);\r\ndwceqos_set_umac_addr(lp, lp->ndev->dev_addr, 0);\r\nlp->eee_enabled = 0;\r\ndwceqos_configure_clock(lp);\r\ndwceqos_write(lp, REG_DWCEQOS_MMC_RXIRQMASK, ~0u);\r\ndwceqos_write(lp, REG_DWCEQOS_MMC_TXIRQMASK, ~0u);\r\nlp->mmc_rx_counters_mask = dwceqos_read(lp, REG_DWCEQOS_MMC_RXIRQMASK);\r\nlp->mmc_tx_counters_mask = dwceqos_read(lp, REG_DWCEQOS_MMC_TXIRQMASK);\r\ndwceqos_write(lp, REG_DWCEQOS_MMC_CTRL, DWCEQOS_MMC_CTRL_CNTRST |\r\nDWCEQOS_MMC_CTRL_RSTONRD);\r\ndwceqos_enable_mmc_interrupt(lp);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_IE,\r\nDWCEQOS_DMA_CH0_IE_NIE |\r\nDWCEQOS_DMA_CH0_IE_RIE | DWCEQOS_DMA_CH0_IE_TIE |\r\nDWCEQOS_DMA_CH0_IE_AIE |\r\nDWCEQOS_DMA_CH0_IE_FBEE);\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_IE, 0);\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_CFG, DWCEQOS_MAC_CFG_IPC |\r\nDWCEQOS_MAC_CFG_DM | DWCEQOS_MAC_CFG_TE | DWCEQOS_MAC_CFG_RE);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_DMA_CH0_TX_CTRL);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_TX_CTRL,\r\nregval | DWCEQOS_DMA_CH_CTRL_START);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_CFG);\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_CFG,\r\nregval | DWCEQOS_MAC_CFG_TE | DWCEQOS_MAC_CFG_RE);\r\nlp->phy_defer = false;\r\nmutex_lock(&lp->phy_dev->lock);\r\nphy_read_status(lp->phy_dev);\r\ndwceqos_adjust_link(lp->ndev);\r\nmutex_unlock(&lp->phy_dev->lock);\r\n}\r\nstatic void dwceqos_tx_reclaim(unsigned long data)\r\n{\r\nstruct net_device *ndev = (struct net_device *)data;\r\nstruct net_local *lp = netdev_priv(ndev);\r\nunsigned int tx_bytes = 0;\r\nunsigned int tx_packets = 0;\r\nspin_lock(&lp->tx_lock);\r\nwhile (lp->tx_free < DWCEQOS_TX_DCNT) {\r\nstruct dwceqos_dma_desc *dd = &lp->tx_descs[lp->tx_cur];\r\nstruct ring_desc *rd = &lp->tx_skb[lp->tx_cur];\r\nif (dd->des3 & DWCEQOS_DMA_TDES3_OWN)\r\nbreak;\r\nif (rd->mapping)\r\ndma_unmap_single(ndev->dev.parent, rd->mapping, rd->len,\r\nDMA_TO_DEVICE);\r\nif (unlikely(rd->skb)) {\r\n++tx_packets;\r\ntx_bytes += rd->skb->len;\r\ndev_consume_skb_any(rd->skb);\r\n}\r\nrd->skb = NULL;\r\nrd->mapping = 0;\r\nlp->tx_free++;\r\nlp->tx_cur = (lp->tx_cur + 1) % DWCEQOS_TX_DCNT;\r\nif ((dd->des3 & DWCEQOS_DMA_TDES3_LD) &&\r\n(dd->des3 & DWCEQOS_DMA_RDES3_ES)) {\r\nif (netif_msg_tx_err(lp))\r\nnetdev_err(ndev, "TX Error, TDES3 = 0x%x\n",\r\ndd->des3);\r\nif (netif_msg_hw(lp))\r\nprint_status(lp);\r\n}\r\n}\r\nspin_unlock(&lp->tx_lock);\r\nnetdev_completed_queue(ndev, tx_packets, tx_bytes);\r\ndwceqos_dma_enable_txirq(lp);\r\nnetif_wake_queue(ndev);\r\n}\r\nstatic int dwceqos_rx(struct net_local *lp, int budget)\r\n{\r\nstruct sk_buff *skb;\r\nu32 tot_size = 0;\r\nunsigned int n_packets = 0;\r\nunsigned int n_descs = 0;\r\nu32 len;\r\nstruct dwceqos_dma_desc *dd;\r\nstruct sk_buff *new_skb;\r\ndma_addr_t new_skb_baddr = 0;\r\nwhile (n_descs < budget) {\r\nif (!dwceqos_packet_avail(lp))\r\nbreak;\r\nnew_skb = netdev_alloc_skb(lp->ndev, DWCEQOS_RX_BUF_SIZE);\r\nif (!new_skb) {\r\nnetdev_err(lp->ndev, "no memory for new sk_buff\n");\r\nbreak;\r\n}\r\nnew_skb_baddr = (u32)dma_map_single(lp->ndev->dev.parent,\r\nnew_skb->data,\r\nDWCEQOS_RX_BUF_SIZE,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(lp->ndev->dev.parent, new_skb_baddr)) {\r\nnetdev_err(lp->ndev, "DMA map error\n");\r\ndev_kfree_skb(new_skb);\r\nbreak;\r\n}\r\ndma_rmb();\r\ndd = &lp->rx_descs[lp->rx_cur];\r\nlen = DWCEQOS_DMA_RDES3_PL(dd->des3);\r\nskb = lp->rx_skb[lp->rx_cur].skb;\r\ndma_unmap_single(lp->ndev->dev.parent,\r\nlp->rx_skb[lp->rx_cur].mapping,\r\nlp->rx_skb[lp->rx_cur].len, DMA_FROM_DEVICE);\r\nif ((dd->des3 & DWCEQOS_DMA_RDES3_ES) ||\r\n(dd->des1 & DWCEQOS_DMA_RDES1_IPCE)) {\r\ndev_kfree_skb(skb);\r\nskb = NULL;\r\n} else {\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, lp->ndev);\r\nswitch (dd->des1 & DWCEQOS_DMA_RDES1_PT) {\r\ncase DWCEQOS_DMA_RDES1_PT_UDP:\r\ncase DWCEQOS_DMA_RDES1_PT_TCP:\r\ncase DWCEQOS_DMA_RDES1_PT_ICMP:\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nbreak;\r\ndefault:\r\nskb->ip_summed = CHECKSUM_NONE;\r\nbreak;\r\n}\r\n}\r\nif (unlikely(!skb)) {\r\nif (netif_msg_rx_err(lp))\r\nnetdev_dbg(lp->ndev, "rx error: des3=%X\n",\r\nlp->rx_descs[lp->rx_cur].des3);\r\n} else {\r\ntot_size += skb->len;\r\nn_packets++;\r\nnetif_receive_skb(skb);\r\n}\r\nlp->rx_descs[lp->rx_cur].des0 = new_skb_baddr;\r\nlp->rx_descs[lp->rx_cur].des1 = 0;\r\nlp->rx_descs[lp->rx_cur].des2 = 0;\r\nwmb();\r\nlp->rx_descs[lp->rx_cur].des3 = DWCEQOS_DMA_RDES3_INTE |\r\nDWCEQOS_DMA_RDES3_OWN |\r\nDWCEQOS_DMA_RDES3_BUF1V;\r\nlp->rx_skb[lp->rx_cur].mapping = new_skb_baddr;\r\nlp->rx_skb[lp->rx_cur].len = DWCEQOS_RX_BUF_SIZE;\r\nlp->rx_skb[lp->rx_cur].skb = new_skb;\r\nn_descs++;\r\nlp->rx_cur = (lp->rx_cur + 1) % DWCEQOS_RX_DCNT;\r\n}\r\nwmb();\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_STA, DWCEQOS_DMA_CH0_IS_RI);\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_RXDESC_TAIL,\r\nlp->rx_descs_tail_addr);\r\nreturn n_descs;\r\n}\r\nstatic int dwceqos_rx_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct net_local *lp = container_of(napi, struct net_local, napi);\r\nint work_done = 0;\r\nwork_done = dwceqos_rx(lp, budget - work_done);\r\nif (!dwceqos_packet_avail(lp) && work_done < budget) {\r\nnapi_complete(napi);\r\ndwceqos_dma_enable_rxirq(lp);\r\n} else {\r\nwork_done = budget;\r\n}\r\nreturn work_done;\r\n}\r\nstatic void dwceqos_reinit_for_txtimeout(struct work_struct *data)\r\n{\r\nstruct net_local *lp = container_of(data, struct net_local,\r\ntxtimeout_reinit);\r\nnetdev_err(lp->ndev, "transmit timeout %d s, resetting...\n",\r\nDWCEQOS_TX_TIMEOUT);\r\nif (netif_msg_hw(lp))\r\nprint_status(lp);\r\nrtnl_lock();\r\ndwceqos_stop(lp->ndev);\r\ndwceqos_open(lp->ndev);\r\nrtnl_unlock();\r\n}\r\nstatic inline int dwceqos_probe_config_dt(struct platform_device *pdev)\r\n{\r\nstruct net_device *ndev;\r\nstruct net_local *lp;\r\nconst void *mac_address;\r\nstruct dwceqos_bus_cfg *bus_cfg;\r\nstruct device_node *np = pdev->dev.of_node;\r\nndev = platform_get_drvdata(pdev);\r\nlp = netdev_priv(ndev);\r\nbus_cfg = &lp->bus_cfg;\r\nmac_address = of_get_mac_address(pdev->dev.of_node);\r\nif (mac_address)\r\nether_addr_copy(ndev->dev_addr, mac_address);\r\nlp->en_tx_lpi_clockgating = of_property_read_bool(np,\r\n"snps,en-tx-lpi-clockgating");\r\nbus_cfg->en_lpi = of_property_read_bool(np, "snps,en-lpi");\r\nof_property_read_u32(np, "snps,write-requests",\r\n&bus_cfg->write_requests);\r\nof_property_read_u32(np, "snps,read-requests", &bus_cfg->read_requests);\r\nof_property_read_u32(np, "snps,burst-map", &bus_cfg->burst_map);\r\nof_property_read_u32(np, "snps,txpbl", &bus_cfg->tx_pbl);\r\nof_property_read_u32(np, "snps,rxpbl", &bus_cfg->rx_pbl);\r\nnetdev_dbg(ndev, "BusCfg: lpi:%u wr:%u rr:%u bm:%X rxpbl:%u txpbl:%d\n",\r\nbus_cfg->en_lpi,\r\nbus_cfg->write_requests,\r\nbus_cfg->read_requests,\r\nbus_cfg->burst_map,\r\nbus_cfg->rx_pbl,\r\nbus_cfg->tx_pbl);\r\nreturn 0;\r\n}\r\nstatic int dwceqos_open(struct net_device *ndev)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nint res;\r\ndwceqos_reset_state(lp);\r\nres = dwceqos_descriptor_init(lp);\r\nif (res) {\r\nnetdev_err(ndev, "Unable to allocate DMA memory, rc %d\n", res);\r\nreturn res;\r\n}\r\nnetdev_reset_queue(ndev);\r\nlp->phy_defer = true;\r\nphy_start(lp->phy_dev);\r\ndwceqos_init_hw(lp);\r\nnapi_enable(&lp->napi);\r\nnetif_start_queue(ndev);\r\ntasklet_enable(&lp->tx_bdreclaim_tasklet);\r\nreturn 0;\r\n}\r\nstatic bool dweqos_is_tx_dma_suspended(struct net_local *lp)\r\n{\r\nu32 reg;\r\nreg = dwceqos_read(lp, REG_DWCEQOS_DMA_DEBUG_ST0);\r\nreg = DMA_GET_TX_STATE_CH0(reg);\r\nreturn reg == DMA_TX_CH_SUSPENDED;\r\n}\r\nstatic void dwceqos_drain_dma(struct net_local *lp)\r\n{\r\nsize_t limit = (DWCEQOS_TX_DCNT * 1250) / 100;\r\nwhile (!dweqos_is_tx_dma_suspended(lp) && limit--)\r\nusleep_range(100, 200);\r\n}\r\nstatic int dwceqos_stop(struct net_device *ndev)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\ntasklet_disable(&lp->tx_bdreclaim_tasklet);\r\nnapi_disable(&lp->napi);\r\nnetif_tx_lock_bh(lp->ndev);\r\nnetif_stop_queue(ndev);\r\nnetif_tx_unlock_bh(lp->ndev);\r\ndwceqos_drain_dma(lp);\r\ndwceqos_reset_hw(lp);\r\nphy_stop(lp->phy_dev);\r\ndwceqos_descriptor_free(lp);\r\nreturn 0;\r\n}\r\nstatic void dwceqos_dmadesc_set_ctx(struct net_local *lp,\r\nunsigned short gso_size)\r\n{\r\nstruct dwceqos_dma_desc *dd = &lp->tx_descs[lp->tx_next];\r\ndd->des0 = 0;\r\ndd->des1 = 0;\r\ndd->des2 = gso_size;\r\ndd->des3 = DWCEQOS_DMA_TDES3_CTXT | DWCEQOS_DMA_TDES3_TCMSSV;\r\nlp->tx_next = (lp->tx_next + 1) % DWCEQOS_TX_DCNT;\r\n}\r\nstatic void dwceqos_tx_poll_demand(struct net_local *lp)\r\n{\r\ndwceqos_write(lp, REG_DWCEQOS_DMA_CH0_TXDESC_TAIL,\r\nlp->tx_descs_tail_addr);\r\n}\r\nstatic void dwceqos_tx_prepare(struct sk_buff *skb, struct net_local *lp,\r\nstruct dwceqos_tx *tx)\r\n{\r\nsize_t n = 1;\r\nsize_t i;\r\nif (skb_is_gso(skb) && skb_shinfo(skb)->gso_size != lp->gso_size)\r\n++n;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; ++i) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nn += (skb_frag_size(frag) + BYTES_PER_DMA_DESC - 1) /\r\nBYTES_PER_DMA_DESC;\r\n}\r\ntx->nr_descriptors = n;\r\ntx->initial_descriptor = lp->tx_next;\r\ntx->last_descriptor = lp->tx_next;\r\ntx->prev_gso_size = lp->gso_size;\r\ntx->network_header_len = skb_transport_offset(skb);\r\nif (skb_is_gso(skb))\r\ntx->network_header_len += tcp_hdrlen(skb);\r\n}\r\nstatic int dwceqos_tx_linear(struct sk_buff *skb, struct net_local *lp,\r\nstruct dwceqos_tx *tx)\r\n{\r\nstruct ring_desc *rd;\r\nstruct dwceqos_dma_desc *dd;\r\nsize_t payload_len;\r\ndma_addr_t dma_handle;\r\nif (skb_is_gso(skb) && skb_shinfo(skb)->gso_size != lp->gso_size) {\r\ndwceqos_dmadesc_set_ctx(lp, skb_shinfo(skb)->gso_size);\r\nlp->gso_size = skb_shinfo(skb)->gso_size;\r\n}\r\ndma_handle = dma_map_single(lp->ndev->dev.parent, skb->data,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\nif (dma_mapping_error(lp->ndev->dev.parent, dma_handle)) {\r\nnetdev_err(lp->ndev, "TX DMA Mapping error\n");\r\nreturn -ENOMEM;\r\n}\r\nrd = &lp->tx_skb[lp->tx_next];\r\ndd = &lp->tx_descs[lp->tx_next];\r\nrd->skb = NULL;\r\nrd->len = skb_headlen(skb);\r\nrd->mapping = dma_handle;\r\ndd->des0 = dma_handle;\r\nif (skb_is_gso(skb)) {\r\npayload_len = skb_headlen(skb) - tx->network_header_len;\r\nif (payload_len)\r\ndd->des1 = dma_handle + tx->network_header_len;\r\ndd->des2 = tx->network_header_len |\r\nDWCEQOS_DMA_DES2_B2L(payload_len);\r\ndd->des3 = DWCEQOS_DMA_TDES3_TSE |\r\nDWCEQOS_DMA_DES3_THL((tcp_hdrlen(skb) / 4)) |\r\n(skb->len - tx->network_header_len);\r\n} else {\r\ndd->des1 = 0;\r\ndd->des2 = skb_headlen(skb);\r\ndd->des3 = skb->len;\r\nswitch (skb->ip_summed) {\r\ncase CHECKSUM_PARTIAL:\r\ndd->des3 |= DWCEQOS_DMA_TDES3_CA;\r\ncase CHECKSUM_NONE:\r\ncase CHECKSUM_UNNECESSARY:\r\ncase CHECKSUM_COMPLETE:\r\ndefault:\r\nbreak;\r\n}\r\n}\r\ndd->des3 |= DWCEQOS_DMA_TDES3_FD;\r\nif (lp->tx_next != tx->initial_descriptor)\r\ndd->des3 |= DWCEQOS_DMA_TDES3_OWN;\r\ntx->last_descriptor = lp->tx_next;\r\nlp->tx_next = (lp->tx_next + 1) % DWCEQOS_TX_DCNT;\r\nreturn 0;\r\n}\r\nstatic int dwceqos_tx_frags(struct sk_buff *skb, struct net_local *lp,\r\nstruct dwceqos_tx *tx)\r\n{\r\nstruct ring_desc *rd = NULL;\r\nstruct dwceqos_dma_desc *dd;\r\ndma_addr_t dma_handle;\r\nsize_t i;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; ++i) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nsize_t frag_size;\r\nsize_t consumed_size;\r\ndma_handle = skb_frag_dma_map(lp->ndev->dev.parent, frag, 0,\r\nskb_frag_size(frag),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(lp->ndev->dev.parent, dma_handle)) {\r\nnetdev_err(lp->ndev, "DMA Mapping error\n");\r\nreturn -ENOMEM;\r\n}\r\nfrag_size = skb_frag_size(frag);\r\nconsumed_size = 0;\r\nwhile (consumed_size < frag_size) {\r\nsize_t dma_size = min_t(size_t, 16376,\r\nfrag_size - consumed_size);\r\nrd = &lp->tx_skb[lp->tx_next];\r\nmemset(rd, 0, sizeof(*rd));\r\ndd = &lp->tx_descs[lp->tx_next];\r\ndd->des0 = dma_handle + consumed_size;\r\ndd->des1 = 0;\r\ndd->des2 = dma_size;\r\nif (skb_is_gso(skb))\r\ndd->des3 = (skb->len - tx->network_header_len);\r\nelse\r\ndd->des3 = skb->len;\r\ndd->des3 |= DWCEQOS_DMA_TDES3_OWN;\r\ntx->last_descriptor = lp->tx_next;\r\nlp->tx_next = (lp->tx_next + 1) % DWCEQOS_TX_DCNT;\r\nconsumed_size += dma_size;\r\n}\r\nrd->len = skb_frag_size(frag);\r\nrd->mapping = dma_handle;\r\n}\r\nreturn 0;\r\n}\r\nstatic void dwceqos_tx_finalize(struct sk_buff *skb, struct net_local *lp,\r\nstruct dwceqos_tx *tx)\r\n{\r\nlp->tx_descs[tx->last_descriptor].des3 |= DWCEQOS_DMA_TDES3_LD;\r\nlp->tx_descs[tx->last_descriptor].des2 |= DWCEQOS_DMA_TDES2_IOC;\r\nlp->tx_skb[tx->last_descriptor].skb = skb;\r\nwmb();\r\nlp->tx_descs[tx->initial_descriptor].des3 |= DWCEQOS_DMA_TDES3_OWN;\r\nwmb();\r\ndwceqos_tx_poll_demand(lp);\r\n}\r\nstatic void dwceqos_tx_rollback(struct net_local *lp, struct dwceqos_tx *tx)\r\n{\r\nsize_t i = tx->initial_descriptor;\r\nwhile (i != lp->tx_next) {\r\nif (lp->tx_skb[i].mapping)\r\ndma_unmap_single(lp->ndev->dev.parent,\r\nlp->tx_skb[i].mapping,\r\nlp->tx_skb[i].len,\r\nDMA_TO_DEVICE);\r\nlp->tx_skb[i].mapping = 0;\r\nlp->tx_skb[i].skb = NULL;\r\nmemset(&lp->tx_descs[i], 0, sizeof(lp->tx_descs[i]));\r\ni = (i + 1) % DWCEQOS_TX_DCNT;\r\n}\r\nlp->tx_next = tx->initial_descriptor;\r\nlp->gso_size = tx->prev_gso_size;\r\n}\r\nstatic int dwceqos_start_xmit(struct sk_buff *skb, struct net_device *ndev)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct dwceqos_tx trans;\r\nint err;\r\ndwceqos_tx_prepare(skb, lp, &trans);\r\nif (lp->tx_free < trans.nr_descriptors) {\r\nnetif_stop_queue(ndev);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nerr = dwceqos_tx_linear(skb, lp, &trans);\r\nif (err)\r\ngoto tx_error;\r\nerr = dwceqos_tx_frags(skb, lp, &trans);\r\nif (err)\r\ngoto tx_error;\r\nWARN_ON(lp->tx_next !=\r\n((trans.initial_descriptor + trans.nr_descriptors) %\r\nDWCEQOS_TX_DCNT));\r\nspin_lock_bh(&lp->tx_lock);\r\nlp->tx_free -= trans.nr_descriptors;\r\ndwceqos_tx_finalize(skb, lp, &trans);\r\nnetdev_sent_queue(ndev, skb->len);\r\nspin_unlock_bh(&lp->tx_lock);\r\nndev->trans_start = jiffies;\r\nreturn 0;\r\ntx_error:\r\ndwceqos_tx_rollback(lp, &trans);\r\ndev_kfree_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int dwceqos_set_mac_address(struct net_device *ndev, void *addr)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct sockaddr *hwaddr = (struct sockaddr *)addr;\r\nif (netif_running(ndev))\r\nreturn -EBUSY;\r\nif (!is_valid_ether_addr(hwaddr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(ndev->dev_addr, hwaddr->sa_data, ndev->addr_len);\r\ndwceqos_set_umac_addr(lp, lp->ndev->dev_addr, 0);\r\nreturn 0;\r\n}\r\nstatic void dwceqos_tx_timeout(struct net_device *ndev)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nqueue_work(lp->txtimeout_handler_wq, &lp->txtimeout_reinit);\r\n}\r\nstatic void dwceqos_set_umac_addr(struct net_local *lp, unsigned char *addr,\r\nunsigned int reg_n)\r\n{\r\nunsigned long data;\r\ndata = (addr[5] << 8) | addr[4];\r\ndwceqos_write(lp, DWCEQOS_ADDR_HIGH(reg_n),\r\ndata | DWCEQOS_MAC_MAC_ADDR_HI_EN);\r\ndata = (addr[3] << 24) | (addr[2] << 16) | (addr[1] << 8) | addr[0];\r\ndwceqos_write(lp, DWCEQOS_ADDR_LOW(reg_n), data);\r\n}\r\nstatic void dwceqos_disable_umac_addr(struct net_local *lp, unsigned int reg_n)\r\n{\r\nif (reg_n != 0)\r\ndwceqos_write(lp, DWCEQOS_ADDR_HIGH(reg_n), 0);\r\n}\r\nstatic void dwceqos_set_rx_mode(struct net_device *ndev)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nu32 regval = 0;\r\nu32 mc_filter[2];\r\nint reg = 1;\r\nstruct netdev_hw_addr *ha;\r\nunsigned int max_mac_addr;\r\nmax_mac_addr = DWCEQOS_MAX_PERFECT_ADDRESSES(lp->feature1);\r\nif (ndev->flags & IFF_PROMISC) {\r\nregval = DWCEQOS_MAC_PKT_FILT_PR;\r\n} else if (((netdev_mc_count(ndev) > DWCEQOS_HASH_TABLE_SIZE) ||\r\n(ndev->flags & IFF_ALLMULTI))) {\r\nregval = DWCEQOS_MAC_PKT_FILT_PM;\r\ndwceqos_write(lp, REG_DWCEQOS_HASTABLE_LO, 0xffffffff);\r\ndwceqos_write(lp, REG_DWCEQOS_HASTABLE_HI, 0xffffffff);\r\n} else if (!netdev_mc_empty(ndev)) {\r\nregval = DWCEQOS_MAC_PKT_FILT_HMC;\r\nmemset(mc_filter, 0, sizeof(mc_filter));\r\nnetdev_for_each_mc_addr(ha, ndev) {\r\nint bit_nr = bitrev32(~crc32_le(~0, ha->addr, 6)) >> 26;\r\nmc_filter[bit_nr >> 5] |= 1 << (bit_nr & 31);\r\n}\r\ndwceqos_write(lp, REG_DWCEQOS_HASTABLE_LO, mc_filter[0]);\r\ndwceqos_write(lp, REG_DWCEQOS_HASTABLE_HI, mc_filter[1]);\r\n}\r\nif (netdev_uc_count(ndev) > max_mac_addr) {\r\nregval |= DWCEQOS_MAC_PKT_FILT_PR;\r\n} else {\r\nnetdev_for_each_uc_addr(ha, ndev) {\r\ndwceqos_set_umac_addr(lp, ha->addr, reg);\r\nreg++;\r\n}\r\nfor (; reg < DWCEQOS_MAX_PERFECT_ADDRESSES(lp->feature1); reg++)\r\ndwceqos_disable_umac_addr(lp, reg);\r\n}\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_PKT_FILT, regval);\r\n}\r\nstatic void dwceqos_poll_controller(struct net_device *ndev)\r\n{\r\ndisable_irq(ndev->irq);\r\ndwceqos_interrupt(ndev->irq, ndev);\r\nenable_irq(ndev->irq);\r\n}\r\nstatic void dwceqos_read_mmc_counters(struct net_local *lp, u32 rx_mask,\r\nu32 tx_mask)\r\n{\r\nif (tx_mask & BIT(27))\r\nlp->mmc_counters.txlpitranscntr +=\r\ndwceqos_read(lp, DWC_MMC_TXLPITRANSCNTR);\r\nif (tx_mask & BIT(26))\r\nlp->mmc_counters.txpiuscntr +=\r\ndwceqos_read(lp, DWC_MMC_TXLPIUSCNTR);\r\nif (tx_mask & BIT(25))\r\nlp->mmc_counters.txoversize_g +=\r\ndwceqos_read(lp, DWC_MMC_TXOVERSIZE_G);\r\nif (tx_mask & BIT(24))\r\nlp->mmc_counters.txvlanpackets_g +=\r\ndwceqos_read(lp, DWC_MMC_TXVLANPACKETS_G);\r\nif (tx_mask & BIT(23))\r\nlp->mmc_counters.txpausepackets +=\r\ndwceqos_read(lp, DWC_MMC_TXPAUSEPACKETS);\r\nif (tx_mask & BIT(22))\r\nlp->mmc_counters.txexcessdef +=\r\ndwceqos_read(lp, DWC_MMC_TXEXCESSDEF);\r\nif (tx_mask & BIT(21))\r\nlp->mmc_counters.txpacketcount_g +=\r\ndwceqos_read(lp, DWC_MMC_TXPACKETCOUNT_G);\r\nif (tx_mask & BIT(20))\r\nlp->mmc_counters.txoctetcount_g +=\r\ndwceqos_read(lp, DWC_MMC_TXOCTETCOUNT_G);\r\nif (tx_mask & BIT(19))\r\nlp->mmc_counters.txcarriererror +=\r\ndwceqos_read(lp, DWC_MMC_TXCARRIERERROR);\r\nif (tx_mask & BIT(18))\r\nlp->mmc_counters.txexcesscol +=\r\ndwceqos_read(lp, DWC_MMC_TXEXCESSCOL);\r\nif (tx_mask & BIT(17))\r\nlp->mmc_counters.txlatecol +=\r\ndwceqos_read(lp, DWC_MMC_TXLATECOL);\r\nif (tx_mask & BIT(16))\r\nlp->mmc_counters.txdeferred +=\r\ndwceqos_read(lp, DWC_MMC_TXDEFERRED);\r\nif (tx_mask & BIT(15))\r\nlp->mmc_counters.txmulticol_g +=\r\ndwceqos_read(lp, DWC_MMC_TXMULTICOL_G);\r\nif (tx_mask & BIT(14))\r\nlp->mmc_counters.txsinglecol_g +=\r\ndwceqos_read(lp, DWC_MMC_TXSINGLECOL_G);\r\nif (tx_mask & BIT(13))\r\nlp->mmc_counters.txunderflowerror +=\r\ndwceqos_read(lp, DWC_MMC_TXUNDERFLOWERROR);\r\nif (tx_mask & BIT(12))\r\nlp->mmc_counters.txbroadcastpackets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TXBROADCASTPACKETS_GB);\r\nif (tx_mask & BIT(11))\r\nlp->mmc_counters.txmulticastpackets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TXMULTICASTPACKETS_GB);\r\nif (tx_mask & BIT(10))\r\nlp->mmc_counters.txunicastpackets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TXUNICASTPACKETS_GB);\r\nif (tx_mask & BIT(9))\r\nlp->mmc_counters.tx1024tomaxoctets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TX1024TOMAXOCTETS_GB);\r\nif (tx_mask & BIT(8))\r\nlp->mmc_counters.tx512to1023octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TX512TO1023OCTETS_GB);\r\nif (tx_mask & BIT(7))\r\nlp->mmc_counters.tx256to511octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TX256TO511OCTETS_GB);\r\nif (tx_mask & BIT(6))\r\nlp->mmc_counters.tx128to255octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TX128TO255OCTETS_GB);\r\nif (tx_mask & BIT(5))\r\nlp->mmc_counters.tx65to127octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TX65TO127OCTETS_GB);\r\nif (tx_mask & BIT(4))\r\nlp->mmc_counters.tx64octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_TX64OCTETS_GB);\r\nif (tx_mask & BIT(3))\r\nlp->mmc_counters.txmulticastpackets_g +=\r\ndwceqos_read(lp, DWC_MMC_TXMULTICASTPACKETS_G);\r\nif (tx_mask & BIT(2))\r\nlp->mmc_counters.txbroadcastpackets_g +=\r\ndwceqos_read(lp, DWC_MMC_TXBROADCASTPACKETS_G);\r\nif (tx_mask & BIT(1))\r\nlp->mmc_counters.txpacketcount_gb +=\r\ndwceqos_read(lp, DWC_MMC_TXPACKETCOUNT_GB);\r\nif (tx_mask & BIT(0))\r\nlp->mmc_counters.txoctetcount_gb +=\r\ndwceqos_read(lp, DWC_MMC_TXOCTETCOUNT_GB);\r\nif (rx_mask & BIT(27))\r\nlp->mmc_counters.rxlpitranscntr +=\r\ndwceqos_read(lp, DWC_MMC_RXLPITRANSCNTR);\r\nif (rx_mask & BIT(26))\r\nlp->mmc_counters.rxlpiuscntr +=\r\ndwceqos_read(lp, DWC_MMC_RXLPIUSCNTR);\r\nif (rx_mask & BIT(25))\r\nlp->mmc_counters.rxctrlpackets_g +=\r\ndwceqos_read(lp, DWC_MMC_RXCTRLPACKETS_G);\r\nif (rx_mask & BIT(24))\r\nlp->mmc_counters.rxrcverror +=\r\ndwceqos_read(lp, DWC_MMC_RXRCVERROR);\r\nif (rx_mask & BIT(23))\r\nlp->mmc_counters.rxwatchdog +=\r\ndwceqos_read(lp, DWC_MMC_RXWATCHDOG);\r\nif (rx_mask & BIT(22))\r\nlp->mmc_counters.rxvlanpackets_gb +=\r\ndwceqos_read(lp, DWC_MMC_RXVLANPACKETS_GB);\r\nif (rx_mask & BIT(21))\r\nlp->mmc_counters.rxfifooverflow +=\r\ndwceqos_read(lp, DWC_MMC_RXFIFOOVERFLOW);\r\nif (rx_mask & BIT(20))\r\nlp->mmc_counters.rxpausepackets +=\r\ndwceqos_read(lp, DWC_MMC_RXPAUSEPACKETS);\r\nif (rx_mask & BIT(19))\r\nlp->mmc_counters.rxoutofrangetype +=\r\ndwceqos_read(lp, DWC_MMC_RXOUTOFRANGETYPE);\r\nif (rx_mask & BIT(18))\r\nlp->mmc_counters.rxlengtherror +=\r\ndwceqos_read(lp, DWC_MMC_RXLENGTHERROR);\r\nif (rx_mask & BIT(17))\r\nlp->mmc_counters.rxunicastpackets_g +=\r\ndwceqos_read(lp, DWC_MMC_RXUNICASTPACKETS_G);\r\nif (rx_mask & BIT(16))\r\nlp->mmc_counters.rx1024tomaxoctets_gb +=\r\ndwceqos_read(lp, DWC_MMC_RX1024TOMAXOCTETS_GB);\r\nif (rx_mask & BIT(15))\r\nlp->mmc_counters.rx512to1023octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_RX512TO1023OCTETS_GB);\r\nif (rx_mask & BIT(14))\r\nlp->mmc_counters.rx256to511octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_RX256TO511OCTETS_GB);\r\nif (rx_mask & BIT(13))\r\nlp->mmc_counters.rx128to255octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_RX128TO255OCTETS_GB);\r\nif (rx_mask & BIT(12))\r\nlp->mmc_counters.rx65to127octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_RX65TO127OCTETS_GB);\r\nif (rx_mask & BIT(11))\r\nlp->mmc_counters.rx64octets_gb +=\r\ndwceqos_read(lp, DWC_MMC_RX64OCTETS_GB);\r\nif (rx_mask & BIT(10))\r\nlp->mmc_counters.rxoversize_g +=\r\ndwceqos_read(lp, DWC_MMC_RXOVERSIZE_G);\r\nif (rx_mask & BIT(9))\r\nlp->mmc_counters.rxundersize_g +=\r\ndwceqos_read(lp, DWC_MMC_RXUNDERSIZE_G);\r\nif (rx_mask & BIT(8))\r\nlp->mmc_counters.rxjabbererror +=\r\ndwceqos_read(lp, DWC_MMC_RXJABBERERROR);\r\nif (rx_mask & BIT(7))\r\nlp->mmc_counters.rxrunterror +=\r\ndwceqos_read(lp, DWC_MMC_RXRUNTERROR);\r\nif (rx_mask & BIT(6))\r\nlp->mmc_counters.rxalignmenterror +=\r\ndwceqos_read(lp, DWC_MMC_RXALIGNMENTERROR);\r\nif (rx_mask & BIT(5))\r\nlp->mmc_counters.rxcrcerror +=\r\ndwceqos_read(lp, DWC_MMC_RXCRCERROR);\r\nif (rx_mask & BIT(4))\r\nlp->mmc_counters.rxmulticastpackets_g +=\r\ndwceqos_read(lp, DWC_MMC_RXMULTICASTPACKETS_G);\r\nif (rx_mask & BIT(3))\r\nlp->mmc_counters.rxbroadcastpackets_g +=\r\ndwceqos_read(lp, DWC_MMC_RXBROADCASTPACKETS_G);\r\nif (rx_mask & BIT(2))\r\nlp->mmc_counters.rxoctetcount_g +=\r\ndwceqos_read(lp, DWC_MMC_RXOCTETCOUNT_G);\r\nif (rx_mask & BIT(1))\r\nlp->mmc_counters.rxoctetcount_gb +=\r\ndwceqos_read(lp, DWC_MMC_RXOCTETCOUNT_GB);\r\nif (rx_mask & BIT(0))\r\nlp->mmc_counters.rxpacketcount_gb +=\r\ndwceqos_read(lp, DWC_MMC_RXPACKETCOUNT_GB);\r\n}\r\nstatic struct rtnl_link_stats64*\r\ndwceqos_get_stats64(struct net_device *ndev, struct rtnl_link_stats64 *s)\r\n{\r\nunsigned long flags;\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct dwceqos_mmc_counters *hwstats = &lp->mmc_counters;\r\nspin_lock_irqsave(&lp->stats_lock, flags);\r\ndwceqos_read_mmc_counters(lp, lp->mmc_rx_counters_mask,\r\nlp->mmc_tx_counters_mask);\r\nspin_unlock_irqrestore(&lp->stats_lock, flags);\r\ns->rx_packets = hwstats->rxpacketcount_gb;\r\ns->rx_bytes = hwstats->rxoctetcount_gb;\r\ns->rx_errors = hwstats->rxpacketcount_gb -\r\nhwstats->rxbroadcastpackets_g -\r\nhwstats->rxmulticastpackets_g -\r\nhwstats->rxunicastpackets_g;\r\ns->multicast = hwstats->rxmulticastpackets_g;\r\ns->rx_length_errors = hwstats->rxlengtherror;\r\ns->rx_crc_errors = hwstats->rxcrcerror;\r\ns->rx_fifo_errors = hwstats->rxfifooverflow;\r\ns->tx_packets = hwstats->txpacketcount_gb;\r\ns->tx_bytes = hwstats->txoctetcount_gb;\r\nif (lp->mmc_tx_counters_mask & BIT(21))\r\ns->tx_errors = hwstats->txpacketcount_gb -\r\nhwstats->txpacketcount_g;\r\nelse\r\ns->tx_errors = hwstats->txunderflowerror +\r\nhwstats->txcarriererror;\r\nreturn s;\r\n}\r\nstatic int\r\ndwceqos_get_settings(struct net_device *ndev, struct ethtool_cmd *ecmd)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct phy_device *phydev = lp->phy_dev;\r\nif (!phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_gset(phydev, ecmd);\r\n}\r\nstatic int\r\ndwceqos_set_settings(struct net_device *ndev, struct ethtool_cmd *ecmd)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nstruct phy_device *phydev = lp->phy_dev;\r\nif (!phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_sset(phydev, ecmd);\r\n}\r\nstatic void\r\ndwceqos_get_drvinfo(struct net_device *ndev, struct ethtool_drvinfo *ed)\r\n{\r\nconst struct net_local *lp = netdev_priv(ndev);\r\nstrcpy(ed->driver, lp->pdev->dev.driver->name);\r\nstrcpy(ed->version, DRIVER_VERSION);\r\n}\r\nstatic void dwceqos_get_pauseparam(struct net_device *ndev,\r\nstruct ethtool_pauseparam *pp)\r\n{\r\nconst struct net_local *lp = netdev_priv(ndev);\r\npp->autoneg = lp->flowcontrol.autoneg;\r\npp->tx_pause = lp->flowcontrol.tx;\r\npp->rx_pause = lp->flowcontrol.rx;\r\n}\r\nstatic int dwceqos_set_pauseparam(struct net_device *ndev,\r\nstruct ethtool_pauseparam *pp)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nint ret = 0;\r\nlp->flowcontrol.autoneg = pp->autoneg;\r\nif (pp->autoneg) {\r\nlp->phy_dev->advertising |= ADVERTISED_Pause;\r\nlp->phy_dev->advertising |= ADVERTISED_Asym_Pause;\r\n} else {\r\nlp->phy_dev->advertising &= ~ADVERTISED_Pause;\r\nlp->phy_dev->advertising &= ~ADVERTISED_Asym_Pause;\r\nlp->flowcontrol.rx = pp->rx_pause;\r\nlp->flowcontrol.tx = pp->tx_pause;\r\n}\r\nif (netif_running(ndev))\r\nret = phy_start_aneg(lp->phy_dev);\r\nreturn ret;\r\n}\r\nstatic void dwceqos_get_strings(struct net_device *ndev, u32 stringset,\r\nu8 *data)\r\n{\r\nsize_t i;\r\nif (stringset != ETH_SS_STATS)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(dwceqos_ethtool_stats); ++i) {\r\nmemcpy(data, dwceqos_ethtool_stats[i].stat_name,\r\nETH_GSTRING_LEN);\r\ndata += ETH_GSTRING_LEN;\r\n}\r\n}\r\nstatic void dwceqos_get_ethtool_stats(struct net_device *ndev,\r\nstruct ethtool_stats *stats, u64 *data)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nunsigned long flags;\r\nsize_t i;\r\nu8 *mmcstat = (u8 *)&lp->mmc_counters;\r\nspin_lock_irqsave(&lp->stats_lock, flags);\r\ndwceqos_read_mmc_counters(lp, lp->mmc_rx_counters_mask,\r\nlp->mmc_tx_counters_mask);\r\nspin_unlock_irqrestore(&lp->stats_lock, flags);\r\nfor (i = 0; i < ARRAY_SIZE(dwceqos_ethtool_stats); ++i) {\r\nmemcpy(data,\r\nmmcstat + dwceqos_ethtool_stats[i].offset,\r\nsizeof(u64));\r\ndata++;\r\n}\r\n}\r\nstatic int dwceqos_get_sset_count(struct net_device *ndev, int sset)\r\n{\r\nif (sset == ETH_SS_STATS)\r\nreturn ARRAY_SIZE(dwceqos_ethtool_stats);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic void dwceqos_get_regs(struct net_device *dev, struct ethtool_regs *regs,\r\nvoid *space)\r\n{\r\nconst struct net_local *lp = netdev_priv(dev);\r\nu32 *reg_space = (u32 *)space;\r\nint reg_offset;\r\nint reg_ix = 0;\r\nfor (reg_offset = START_MAC_REG_OFFSET;\r\nreg_offset <= MAX_DMA_REG_OFFSET; reg_offset += 4) {\r\nreg_space[reg_ix] = dwceqos_read(lp, reg_offset);\r\nreg_ix++;\r\n}\r\nfor (reg_offset = START_MTL_REG_OFFSET;\r\nreg_offset <= MAX_MTL_REG_OFFSET; reg_offset += 4) {\r\nreg_space[reg_ix] = dwceqos_read(lp, reg_offset);\r\nreg_ix++;\r\n}\r\nfor (reg_offset = START_DMA_REG_OFFSET;\r\nreg_offset <= MAX_DMA_REG_OFFSET; reg_offset += 4) {\r\nreg_space[reg_ix] = dwceqos_read(lp, reg_offset);\r\nreg_ix++;\r\n}\r\nBUG_ON(4 * reg_ix > REG_SPACE_SIZE);\r\n}\r\nstatic int dwceqos_get_regs_len(struct net_device *dev)\r\n{\r\nreturn REG_SPACE_SIZE;\r\n}\r\nstatic inline const char *dwceqos_get_rx_lpi_state(u32 lpi_ctrl)\r\n{\r\nreturn (lpi_ctrl & DWCEQOS_MAC_LPI_CTRL_STATUS_RLPIST) ? "on" : "off";\r\n}\r\nstatic inline const char *dwceqos_get_tx_lpi_state(u32 lpi_ctrl)\r\n{\r\nreturn (lpi_ctrl & DWCEQOS_MAC_LPI_CTRL_STATUS_TLPIST) ? "on" : "off";\r\n}\r\nstatic int dwceqos_get_eee(struct net_device *ndev, struct ethtool_eee *edata)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nu32 lpi_status;\r\nu32 lpi_enabled;\r\nif (!(lp->feature0 & DWCEQOS_MAC_HW_FEATURE0_EEESEL))\r\nreturn -EOPNOTSUPP;\r\nedata->eee_active = lp->eee_active;\r\nedata->eee_enabled = lp->eee_enabled;\r\nedata->tx_lpi_timer = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_ENTRY_TIMER);\r\nlpi_status = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS);\r\nlpi_enabled = !!(lpi_status & DWCEQOS_MAC_LPI_CTRL_STATUS_LIPTXA);\r\nedata->tx_lpi_enabled = lpi_enabled;\r\nif (netif_msg_hw(lp)) {\r\nu32 regval;\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS);\r\nnetdev_info(lp->ndev, "MAC LPI State: RX:%s TX:%s\n",\r\ndwceqos_get_rx_lpi_state(regval),\r\ndwceqos_get_tx_lpi_state(regval));\r\n}\r\nreturn phy_ethtool_get_eee(lp->phy_dev, edata);\r\n}\r\nstatic int dwceqos_set_eee(struct net_device *ndev, struct ethtool_eee *edata)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nu32 regval;\r\nunsigned long flags;\r\nif (!(lp->feature0 & DWCEQOS_MAC_HW_FEATURE0_EEESEL))\r\nreturn -EOPNOTSUPP;\r\nif (edata->eee_enabled && !lp->eee_active)\r\nreturn -EOPNOTSUPP;\r\nif (edata->tx_lpi_enabled) {\r\nif (edata->tx_lpi_timer < DWCEQOS_LPI_TIMER_MIN ||\r\nedata->tx_lpi_timer > DWCEQOS_LPI_TIMER_MAX)\r\nreturn -EINVAL;\r\n}\r\nlp->eee_enabled = edata->eee_enabled;\r\nif (edata->eee_enabled && edata->tx_lpi_enabled) {\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_LPI_ENTRY_TIMER,\r\nedata->tx_lpi_timer);\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS);\r\nregval |= DWCEQOS_LPI_CTRL_ENABLE_EEE;\r\nif (lp->en_tx_lpi_clockgating)\r\nregval |= DWCEQOS_MAC_LPI_CTRL_STATUS_LPITCSE;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n} else {\r\nspin_lock_irqsave(&lp->hw_lock, flags);\r\nregval = dwceqos_read(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS);\r\nregval &= ~DWCEQOS_LPI_CTRL_ENABLE_EEE;\r\ndwceqos_write(lp, REG_DWCEQOS_MAC_LPI_CTRL_STATUS, regval);\r\nspin_unlock_irqrestore(&lp->hw_lock, flags);\r\n}\r\nreturn phy_ethtool_set_eee(lp->phy_dev, edata);\r\n}\r\nstatic u32 dwceqos_get_msglevel(struct net_device *ndev)\r\n{\r\nconst struct net_local *lp = netdev_priv(ndev);\r\nreturn lp->msg_enable;\r\n}\r\nstatic void dwceqos_set_msglevel(struct net_device *ndev, u32 msglevel)\r\n{\r\nstruct net_local *lp = netdev_priv(ndev);\r\nlp->msg_enable = msglevel;\r\n}\r\nstatic int dwceqos_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *r_mem = NULL;\r\nstruct net_device *ndev;\r\nstruct net_local *lp;\r\nint ret = -ENXIO;\r\nr_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!r_mem) {\r\ndev_err(&pdev->dev, "no IO resource defined.\n");\r\nreturn -ENXIO;\r\n}\r\nndev = alloc_etherdev(sizeof(*lp));\r\nif (!ndev) {\r\ndev_err(&pdev->dev, "etherdev allocation failed.\n");\r\nreturn -ENOMEM;\r\n}\r\nSET_NETDEV_DEV(ndev, &pdev->dev);\r\nlp = netdev_priv(ndev);\r\nlp->ndev = ndev;\r\nlp->pdev = pdev;\r\nlp->msg_enable = netif_msg_init(debug, DWCEQOS_MSG_DEFAULT);\r\nspin_lock_init(&lp->tx_lock);\r\nspin_lock_init(&lp->hw_lock);\r\nspin_lock_init(&lp->stats_lock);\r\nlp->apb_pclk = devm_clk_get(&pdev->dev, "apb_pclk");\r\nif (IS_ERR(lp->apb_pclk)) {\r\ndev_err(&pdev->dev, "apb_pclk clock not found.\n");\r\nret = PTR_ERR(lp->apb_pclk);\r\ngoto err_out_free_netdev;\r\n}\r\nret = clk_prepare_enable(lp->apb_pclk);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Unable to enable APER clock.\n");\r\ngoto err_out_free_netdev;\r\n}\r\nlp->baseaddr = devm_ioremap_resource(&pdev->dev, r_mem);\r\nif (IS_ERR(lp->baseaddr)) {\r\ndev_err(&pdev->dev, "failed to map baseaddress.\n");\r\nret = PTR_ERR(lp->baseaddr);\r\ngoto err_out_clk_dis_aper;\r\n}\r\nndev->irq = platform_get_irq(pdev, 0);\r\nndev->watchdog_timeo = DWCEQOS_TX_TIMEOUT * HZ;\r\nndev->netdev_ops = &netdev_ops;\r\nndev->ethtool_ops = &dwceqos_ethtool_ops;\r\nndev->base_addr = r_mem->start;\r\ndwceqos_get_hwfeatures(lp);\r\ndwceqos_mdio_set_csr(lp);\r\nndev->hw_features = NETIF_F_SG;\r\nif (lp->feature1 & DWCEQOS_MAC_HW_FEATURE1_TSOEN)\r\nndev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;\r\nif (lp->feature0 & DWCEQOS_MAC_HW_FEATURE0_TXCOESEL)\r\nndev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\r\nif (lp->feature0 & DWCEQOS_MAC_HW_FEATURE0_RXCOESEL)\r\nndev->hw_features |= NETIF_F_RXCSUM;\r\nndev->features = ndev->hw_features;\r\nnetif_napi_add(ndev, &lp->napi, dwceqos_rx_poll, NAPI_POLL_WEIGHT);\r\nret = register_netdev(ndev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Cannot register net device, aborting.\n");\r\ngoto err_out_clk_dis_aper;\r\n}\r\nlp->phy_ref_clk = devm_clk_get(&pdev->dev, "phy_ref_clk");\r\nif (IS_ERR(lp->phy_ref_clk)) {\r\ndev_err(&pdev->dev, "phy_ref_clk clock not found.\n");\r\nret = PTR_ERR(lp->phy_ref_clk);\r\ngoto err_out_unregister_netdev;\r\n}\r\nret = clk_prepare_enable(lp->phy_ref_clk);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Unable to enable device clock.\n");\r\ngoto err_out_unregister_netdev;\r\n}\r\nlp->phy_node = of_parse_phandle(lp->pdev->dev.of_node,\r\n"phy-handle", 0);\r\nif (!lp->phy_node && of_phy_is_fixed_link(lp->pdev->dev.of_node)) {\r\nret = of_phy_register_fixed_link(lp->pdev->dev.of_node);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "invalid fixed-link");\r\ngoto err_out_unregister_netdev;\r\n}\r\nlp->phy_node = of_node_get(lp->pdev->dev.of_node);\r\n}\r\nret = of_get_phy_mode(lp->pdev->dev.of_node);\r\nif (ret < 0) {\r\ndev_err(&lp->pdev->dev, "error in getting phy i/f\n");\r\ngoto err_out_unregister_clk_notifier;\r\n}\r\nlp->phy_interface = ret;\r\nret = dwceqos_mii_init(lp);\r\nif (ret) {\r\ndev_err(&lp->pdev->dev, "error in dwceqos_mii_init\n");\r\ngoto err_out_unregister_clk_notifier;\r\n}\r\nret = dwceqos_mii_probe(ndev);\r\nif (ret != 0) {\r\nnetdev_err(ndev, "mii_probe fail.\n");\r\nret = -ENXIO;\r\ngoto err_out_unregister_clk_notifier;\r\n}\r\ndwceqos_set_umac_addr(lp, lp->ndev->dev_addr, 0);\r\ntasklet_init(&lp->tx_bdreclaim_tasklet, dwceqos_tx_reclaim,\r\n(unsigned long)ndev);\r\ntasklet_disable(&lp->tx_bdreclaim_tasklet);\r\nlp->txtimeout_handler_wq = create_singlethread_workqueue(DRIVER_NAME);\r\nINIT_WORK(&lp->txtimeout_reinit, dwceqos_reinit_for_txtimeout);\r\nplatform_set_drvdata(pdev, ndev);\r\nret = dwceqos_probe_config_dt(pdev);\r\nif (ret) {\r\ndev_err(&lp->pdev->dev, "Unable to retrieve DT, error %d\n",\r\nret);\r\ngoto err_out_unregister_clk_notifier;\r\n}\r\ndev_info(&lp->pdev->dev, "pdev->id %d, baseaddr 0x%08lx, irq %d\n",\r\npdev->id, ndev->base_addr, ndev->irq);\r\nret = devm_request_irq(&pdev->dev, ndev->irq, &dwceqos_interrupt, 0,\r\nndev->name, ndev);\r\nif (ret) {\r\ndev_err(&lp->pdev->dev, "Unable to request IRQ %d, error %d\n",\r\nndev->irq, ret);\r\ngoto err_out_unregister_clk_notifier;\r\n}\r\nif (netif_msg_probe(lp))\r\nnetdev_dbg(ndev, "net_local@%p\n", lp);\r\nreturn 0;\r\nerr_out_unregister_clk_notifier:\r\nclk_disable_unprepare(lp->phy_ref_clk);\r\nerr_out_unregister_netdev:\r\nunregister_netdev(ndev);\r\nerr_out_clk_dis_aper:\r\nclk_disable_unprepare(lp->apb_pclk);\r\nerr_out_free_netdev:\r\nof_node_put(lp->phy_node);\r\nfree_netdev(ndev);\r\nplatform_set_drvdata(pdev, NULL);\r\nreturn ret;\r\n}\r\nstatic int dwceqos_remove(struct platform_device *pdev)\r\n{\r\nstruct net_device *ndev = platform_get_drvdata(pdev);\r\nstruct net_local *lp;\r\nif (ndev) {\r\nlp = netdev_priv(ndev);\r\nif (lp->phy_dev)\r\nphy_disconnect(lp->phy_dev);\r\nmdiobus_unregister(lp->mii_bus);\r\nmdiobus_free(lp->mii_bus);\r\nunregister_netdev(ndev);\r\nclk_disable_unprepare(lp->phy_ref_clk);\r\nclk_disable_unprepare(lp->apb_pclk);\r\nfree_netdev(ndev);\r\n}\r\nreturn 0;\r\n}
