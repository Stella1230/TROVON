static inline\r\nstruct mdp5_kms *get_kms(struct mdp5_ctl_manager *ctl_mgr)\r\n{\r\nstruct msm_drm_private *priv = ctl_mgr->dev->dev_private;\r\nreturn to_mdp5_kms(to_mdp_kms(priv->kms));\r\n}\r\nstatic inline\r\nvoid ctl_write(struct mdp5_ctl *ctl, u32 reg, u32 data)\r\n{\r\nstruct mdp5_kms *mdp5_kms = get_kms(ctl->ctlm);\r\n(void)ctl->reg_offset;\r\nmdp5_write(mdp5_kms, reg, data);\r\n}\r\nstatic inline\r\nu32 ctl_read(struct mdp5_ctl *ctl, u32 reg)\r\n{\r\nstruct mdp5_kms *mdp5_kms = get_kms(ctl->ctlm);\r\n(void)ctl->reg_offset;\r\nreturn mdp5_read(mdp5_kms, reg);\r\n}\r\nstatic void set_display_intf(struct mdp5_kms *mdp5_kms,\r\nstruct mdp5_interface *intf)\r\n{\r\nunsigned long flags;\r\nu32 intf_sel;\r\nspin_lock_irqsave(&mdp5_kms->resource_lock, flags);\r\nintf_sel = mdp5_read(mdp5_kms, REG_MDP5_MDP_DISP_INTF_SEL(0));\r\nswitch (intf->num) {\r\ncase 0:\r\nintf_sel &= ~MDP5_MDP_DISP_INTF_SEL_INTF0__MASK;\r\nintf_sel |= MDP5_MDP_DISP_INTF_SEL_INTF0(intf->type);\r\nbreak;\r\ncase 1:\r\nintf_sel &= ~MDP5_MDP_DISP_INTF_SEL_INTF1__MASK;\r\nintf_sel |= MDP5_MDP_DISP_INTF_SEL_INTF1(intf->type);\r\nbreak;\r\ncase 2:\r\nintf_sel &= ~MDP5_MDP_DISP_INTF_SEL_INTF2__MASK;\r\nintf_sel |= MDP5_MDP_DISP_INTF_SEL_INTF2(intf->type);\r\nbreak;\r\ncase 3:\r\nintf_sel &= ~MDP5_MDP_DISP_INTF_SEL_INTF3__MASK;\r\nintf_sel |= MDP5_MDP_DISP_INTF_SEL_INTF3(intf->type);\r\nbreak;\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\nmdp5_write(mdp5_kms, REG_MDP5_MDP_DISP_INTF_SEL(0), intf_sel);\r\nspin_unlock_irqrestore(&mdp5_kms->resource_lock, flags);\r\n}\r\nstatic void set_ctl_op(struct mdp5_ctl *ctl, struct mdp5_interface *intf)\r\n{\r\nunsigned long flags;\r\nu32 ctl_op = 0;\r\nif (!mdp5_cfg_intf_is_virtual(intf->type))\r\nctl_op |= MDP5_CTL_OP_INTF_NUM(INTF0 + intf->num);\r\nswitch (intf->type) {\r\ncase INTF_DSI:\r\nif (intf->mode == MDP5_INTF_DSI_MODE_COMMAND)\r\nctl_op |= MDP5_CTL_OP_CMD_MODE;\r\nbreak;\r\ncase INTF_WB:\r\nif (intf->mode == MDP5_INTF_WB_MODE_LINE)\r\nctl_op |= MDP5_CTL_OP_MODE(MODE_WB_2_LINE);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_lock_irqsave(&ctl->hw_lock, flags);\r\nctl_write(ctl, REG_MDP5_CTL_OP(ctl->id), ctl_op);\r\nspin_unlock_irqrestore(&ctl->hw_lock, flags);\r\n}\r\nint mdp5_ctl_set_pipeline(struct mdp5_ctl *ctl,\r\nstruct mdp5_interface *intf, int lm)\r\n{\r\nstruct mdp5_ctl_manager *ctl_mgr = ctl->ctlm;\r\nstruct mdp5_kms *mdp5_kms = get_kms(ctl_mgr);\r\nif (unlikely(WARN_ON(intf->num != ctl->pipeline.intf.num))) {\r\ndev_err(mdp5_kms->dev->dev,\r\n"CTL %d is allocated by INTF %d, but used by INTF %d\n",\r\nctl->id, ctl->pipeline.intf.num, intf->num);\r\nreturn -EINVAL;\r\n}\r\nctl->lm = lm;\r\nmemcpy(&ctl->pipeline.intf, intf, sizeof(*intf));\r\nctl->pipeline.start_mask = mdp_ctl_flush_mask_lm(ctl->lm) |\r\nmdp_ctl_flush_mask_encoder(intf);\r\nif (!mdp5_cfg_intf_is_virtual(intf->type))\r\nset_display_intf(mdp5_kms, intf);\r\nset_ctl_op(ctl, intf);\r\nreturn 0;\r\n}\r\nstatic bool start_signal_needed(struct mdp5_ctl *ctl)\r\n{\r\nstruct op_mode *pipeline = &ctl->pipeline;\r\nif (!pipeline->encoder_enabled || pipeline->start_mask != 0)\r\nreturn false;\r\nswitch (pipeline->intf.type) {\r\ncase INTF_WB:\r\nreturn true;\r\ncase INTF_DSI:\r\nreturn pipeline->intf.mode == MDP5_INTF_DSI_MODE_COMMAND;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic void send_start_signal(struct mdp5_ctl *ctl)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ctl->hw_lock, flags);\r\nctl_write(ctl, REG_MDP5_CTL_START(ctl->id), 1);\r\nspin_unlock_irqrestore(&ctl->hw_lock, flags);\r\n}\r\nstatic void refill_start_mask(struct mdp5_ctl *ctl)\r\n{\r\nstruct op_mode *pipeline = &ctl->pipeline;\r\nstruct mdp5_interface *intf = &ctl->pipeline.intf;\r\npipeline->start_mask = mdp_ctl_flush_mask_lm(ctl->lm);\r\nif (intf->type == INTF_WB)\r\npipeline->start_mask |= mdp_ctl_flush_mask_encoder(intf);\r\n}\r\nint mdp5_ctl_set_encoder_state(struct mdp5_ctl *ctl, bool enabled)\r\n{\r\nif (WARN_ON(!ctl))\r\nreturn -EINVAL;\r\nctl->pipeline.encoder_enabled = enabled;\r\nDBG("intf_%d: %s", ctl->pipeline.intf.num, enabled ? "on" : "off");\r\nif (start_signal_needed(ctl)) {\r\nsend_start_signal(ctl);\r\nrefill_start_mask(ctl);\r\n}\r\nreturn 0;\r\n}\r\nint mdp5_ctl_set_cursor(struct mdp5_ctl *ctl, int cursor_id, bool enable)\r\n{\r\nstruct mdp5_ctl_manager *ctl_mgr = ctl->ctlm;\r\nunsigned long flags;\r\nu32 blend_cfg;\r\nint lm = ctl->lm;\r\nif (unlikely(WARN_ON(lm < 0))) {\r\ndev_err(ctl_mgr->dev->dev, "CTL %d cannot find LM: %d",\r\nctl->id, lm);\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&ctl->hw_lock, flags);\r\nblend_cfg = ctl_read(ctl, REG_MDP5_CTL_LAYER_REG(ctl->id, lm));\r\nif (enable)\r\nblend_cfg |= MDP5_CTL_LAYER_REG_CURSOR_OUT;\r\nelse\r\nblend_cfg &= ~MDP5_CTL_LAYER_REG_CURSOR_OUT;\r\nctl_write(ctl, REG_MDP5_CTL_LAYER_REG(ctl->id, lm), blend_cfg);\r\nctl->cursor_on = enable;\r\nspin_unlock_irqrestore(&ctl->hw_lock, flags);\r\nctl->pending_ctl_trigger = mdp_ctl_flush_mask_cursor(cursor_id);\r\nreturn 0;\r\n}\r\nstatic u32 mdp_ctl_blend_mask(enum mdp5_pipe pipe,\r\nenum mdp_mixer_stage_id stage)\r\n{\r\nswitch (pipe) {\r\ncase SSPP_VIG0: return MDP5_CTL_LAYER_REG_VIG0(stage);\r\ncase SSPP_VIG1: return MDP5_CTL_LAYER_REG_VIG1(stage);\r\ncase SSPP_VIG2: return MDP5_CTL_LAYER_REG_VIG2(stage);\r\ncase SSPP_RGB0: return MDP5_CTL_LAYER_REG_RGB0(stage);\r\ncase SSPP_RGB1: return MDP5_CTL_LAYER_REG_RGB1(stage);\r\ncase SSPP_RGB2: return MDP5_CTL_LAYER_REG_RGB2(stage);\r\ncase SSPP_DMA0: return MDP5_CTL_LAYER_REG_DMA0(stage);\r\ncase SSPP_DMA1: return MDP5_CTL_LAYER_REG_DMA1(stage);\r\ncase SSPP_VIG3: return MDP5_CTL_LAYER_REG_VIG3(stage);\r\ncase SSPP_RGB3: return MDP5_CTL_LAYER_REG_RGB3(stage);\r\ndefault: return 0;\r\n}\r\n}\r\nstatic u32 mdp_ctl_blend_ext_mask(enum mdp5_pipe pipe,\r\nenum mdp_mixer_stage_id stage)\r\n{\r\nif (stage < STAGE6)\r\nreturn 0;\r\nswitch (pipe) {\r\ncase SSPP_VIG0: return MDP5_CTL_LAYER_EXT_REG_VIG0_BIT3;\r\ncase SSPP_VIG1: return MDP5_CTL_LAYER_EXT_REG_VIG1_BIT3;\r\ncase SSPP_VIG2: return MDP5_CTL_LAYER_EXT_REG_VIG2_BIT3;\r\ncase SSPP_RGB0: return MDP5_CTL_LAYER_EXT_REG_RGB0_BIT3;\r\ncase SSPP_RGB1: return MDP5_CTL_LAYER_EXT_REG_RGB1_BIT3;\r\ncase SSPP_RGB2: return MDP5_CTL_LAYER_EXT_REG_RGB2_BIT3;\r\ncase SSPP_DMA0: return MDP5_CTL_LAYER_EXT_REG_DMA0_BIT3;\r\ncase SSPP_DMA1: return MDP5_CTL_LAYER_EXT_REG_DMA1_BIT3;\r\ncase SSPP_VIG3: return MDP5_CTL_LAYER_EXT_REG_VIG3_BIT3;\r\ncase SSPP_RGB3: return MDP5_CTL_LAYER_EXT_REG_RGB3_BIT3;\r\ndefault: return 0;\r\n}\r\n}\r\nint mdp5_ctl_blend(struct mdp5_ctl *ctl, u8 *stage, u32 stage_cnt,\r\nu32 ctl_blend_op_flags)\r\n{\r\nunsigned long flags;\r\nu32 blend_cfg = 0, blend_ext_cfg = 0;\r\nint i, start_stage;\r\nif (ctl_blend_op_flags & MDP5_CTL_BLEND_OP_FLAG_BORDER_OUT) {\r\nstart_stage = STAGE0;\r\nblend_cfg |= MDP5_CTL_LAYER_REG_BORDER_COLOR;\r\n} else {\r\nstart_stage = STAGE_BASE;\r\n}\r\nfor (i = start_stage; i < start_stage + stage_cnt; i++) {\r\nblend_cfg |= mdp_ctl_blend_mask(stage[i], i);\r\nblend_ext_cfg |= mdp_ctl_blend_ext_mask(stage[i], i);\r\n}\r\nspin_lock_irqsave(&ctl->hw_lock, flags);\r\nif (ctl->cursor_on)\r\nblend_cfg |= MDP5_CTL_LAYER_REG_CURSOR_OUT;\r\nctl_write(ctl, REG_MDP5_CTL_LAYER_REG(ctl->id, ctl->lm), blend_cfg);\r\nctl_write(ctl, REG_MDP5_CTL_LAYER_EXT_REG(ctl->id, ctl->lm), blend_ext_cfg);\r\nspin_unlock_irqrestore(&ctl->hw_lock, flags);\r\nctl->pending_ctl_trigger = mdp_ctl_flush_mask_lm(ctl->lm);\r\nDBG("lm%d: blend config = 0x%08x. ext_cfg = 0x%08x", ctl->lm,\r\nblend_cfg, blend_ext_cfg);\r\nreturn 0;\r\n}\r\nu32 mdp_ctl_flush_mask_encoder(struct mdp5_interface *intf)\r\n{\r\nif (intf->type == INTF_WB)\r\nreturn MDP5_CTL_FLUSH_WB;\r\nswitch (intf->num) {\r\ncase 0: return MDP5_CTL_FLUSH_TIMING_0;\r\ncase 1: return MDP5_CTL_FLUSH_TIMING_1;\r\ncase 2: return MDP5_CTL_FLUSH_TIMING_2;\r\ncase 3: return MDP5_CTL_FLUSH_TIMING_3;\r\ndefault: return 0;\r\n}\r\n}\r\nu32 mdp_ctl_flush_mask_cursor(int cursor_id)\r\n{\r\nswitch (cursor_id) {\r\ncase 0: return MDP5_CTL_FLUSH_CURSOR_0;\r\ncase 1: return MDP5_CTL_FLUSH_CURSOR_1;\r\ndefault: return 0;\r\n}\r\n}\r\nu32 mdp_ctl_flush_mask_pipe(enum mdp5_pipe pipe)\r\n{\r\nswitch (pipe) {\r\ncase SSPP_VIG0: return MDP5_CTL_FLUSH_VIG0;\r\ncase SSPP_VIG1: return MDP5_CTL_FLUSH_VIG1;\r\ncase SSPP_VIG2: return MDP5_CTL_FLUSH_VIG2;\r\ncase SSPP_RGB0: return MDP5_CTL_FLUSH_RGB0;\r\ncase SSPP_RGB1: return MDP5_CTL_FLUSH_RGB1;\r\ncase SSPP_RGB2: return MDP5_CTL_FLUSH_RGB2;\r\ncase SSPP_DMA0: return MDP5_CTL_FLUSH_DMA0;\r\ncase SSPP_DMA1: return MDP5_CTL_FLUSH_DMA1;\r\ncase SSPP_VIG3: return MDP5_CTL_FLUSH_VIG3;\r\ncase SSPP_RGB3: return MDP5_CTL_FLUSH_RGB3;\r\ndefault: return 0;\r\n}\r\n}\r\nu32 mdp_ctl_flush_mask_lm(int lm)\r\n{\r\nswitch (lm) {\r\ncase 0: return MDP5_CTL_FLUSH_LM0;\r\ncase 1: return MDP5_CTL_FLUSH_LM1;\r\ncase 2: return MDP5_CTL_FLUSH_LM2;\r\ncase 5: return MDP5_CTL_FLUSH_LM5;\r\ndefault: return 0;\r\n}\r\n}\r\nstatic u32 fix_sw_flush(struct mdp5_ctl *ctl, u32 flush_mask)\r\n{\r\nstruct mdp5_ctl_manager *ctl_mgr = ctl->ctlm;\r\nu32 sw_mask = 0;\r\n#define BIT_NEEDS_SW_FIX(bit) \\r\n(!(ctl_mgr->flush_hw_mask & bit) && (flush_mask & bit))\r\nif (BIT_NEEDS_SW_FIX(MDP5_CTL_FLUSH_CURSOR_0))\r\nsw_mask |= mdp_ctl_flush_mask_lm(ctl->lm);\r\nreturn sw_mask;\r\n}\r\nstatic void fix_for_single_flush(struct mdp5_ctl *ctl, u32 *flush_mask,\r\nu32 *flush_id)\r\n{\r\nstruct mdp5_ctl_manager *ctl_mgr = ctl->ctlm;\r\nif (ctl->pair) {\r\nDBG("CTL %d FLUSH pending mask %x", ctl->id, *flush_mask);\r\nctl->flush_pending = true;\r\nctl_mgr->single_flush_pending_mask |= (*flush_mask);\r\n*flush_mask = 0;\r\nif (ctl->pair->flush_pending) {\r\n*flush_id = min_t(u32, ctl->id, ctl->pair->id);\r\n*flush_mask = ctl_mgr->single_flush_pending_mask;\r\nctl->flush_pending = false;\r\nctl->pair->flush_pending = false;\r\nctl_mgr->single_flush_pending_mask = 0;\r\nDBG("Single FLUSH mask %x,ID %d", *flush_mask,\r\n*flush_id);\r\n}\r\n}\r\n}\r\nu32 mdp5_ctl_commit(struct mdp5_ctl *ctl, u32 flush_mask)\r\n{\r\nstruct mdp5_ctl_manager *ctl_mgr = ctl->ctlm;\r\nstruct op_mode *pipeline = &ctl->pipeline;\r\nunsigned long flags;\r\nu32 flush_id = ctl->id;\r\nu32 curr_ctl_flush_mask;\r\npipeline->start_mask &= ~flush_mask;\r\nVERB("flush_mask=%x, start_mask=%x, trigger=%x", flush_mask,\r\npipeline->start_mask, ctl->pending_ctl_trigger);\r\nif (ctl->pending_ctl_trigger & flush_mask) {\r\nflush_mask |= MDP5_CTL_FLUSH_CTL;\r\nctl->pending_ctl_trigger = 0;\r\n}\r\nflush_mask |= fix_sw_flush(ctl, flush_mask);\r\nflush_mask &= ctl_mgr->flush_hw_mask;\r\ncurr_ctl_flush_mask = flush_mask;\r\nfix_for_single_flush(ctl, &flush_mask, &flush_id);\r\nif (flush_mask) {\r\nspin_lock_irqsave(&ctl->hw_lock, flags);\r\nctl_write(ctl, REG_MDP5_CTL_FLUSH(flush_id), flush_mask);\r\nspin_unlock_irqrestore(&ctl->hw_lock, flags);\r\n}\r\nif (start_signal_needed(ctl)) {\r\nsend_start_signal(ctl);\r\nrefill_start_mask(ctl);\r\n}\r\nreturn curr_ctl_flush_mask;\r\n}\r\nu32 mdp5_ctl_get_commit_status(struct mdp5_ctl *ctl)\r\n{\r\nreturn ctl_read(ctl, REG_MDP5_CTL_FLUSH(ctl->id));\r\n}\r\nint mdp5_ctl_get_ctl_id(struct mdp5_ctl *ctl)\r\n{\r\nreturn WARN_ON(!ctl) ? -EINVAL : ctl->id;\r\n}\r\nint mdp5_ctl_pair(struct mdp5_ctl *ctlx, struct mdp5_ctl *ctly, bool enable)\r\n{\r\nstruct mdp5_ctl_manager *ctl_mgr = ctlx->ctlm;\r\nstruct mdp5_kms *mdp5_kms = get_kms(ctl_mgr);\r\nif (!ctl_mgr->single_flush_supported)\r\nreturn 0;\r\nif (!enable) {\r\nctlx->pair = NULL;\r\nctly->pair = NULL;\r\nmdp5_write(mdp5_kms, REG_MDP5_MDP_SPARE_0(0), 0);\r\nreturn 0;\r\n} else if ((ctlx->pair != NULL) || (ctly->pair != NULL)) {\r\ndev_err(ctl_mgr->dev->dev, "CTLs already paired\n");\r\nreturn -EINVAL;\r\n} else if (!(ctlx->status & ctly->status & CTL_STAT_BOOKED)) {\r\ndev_err(ctl_mgr->dev->dev, "Only pair booked CTLs\n");\r\nreturn -EINVAL;\r\n}\r\nctlx->pair = ctly;\r\nctly->pair = ctlx;\r\nmdp5_write(mdp5_kms, REG_MDP5_MDP_SPARE_0(0),\r\nMDP5_MDP_SPARE_0_SPLIT_DPL_SINGLE_FLUSH_EN);\r\nreturn 0;\r\n}\r\nstruct mdp5_ctl *mdp5_ctlm_request(struct mdp5_ctl_manager *ctl_mgr,\r\nint intf_num)\r\n{\r\nstruct mdp5_ctl *ctl = NULL;\r\nconst u32 checkm = CTL_STAT_BUSY | CTL_STAT_BOOKED;\r\nu32 match = ((intf_num == 1) || (intf_num == 2)) ? CTL_STAT_BOOKED : 0;\r\nunsigned long flags;\r\nint c;\r\nspin_lock_irqsave(&ctl_mgr->pool_lock, flags);\r\nfor (c = 0; c < ctl_mgr->nctl; c++)\r\nif ((ctl_mgr->ctls[c].status & checkm) == match)\r\ngoto found;\r\ndev_warn(ctl_mgr->dev->dev,\r\n"fall back to the other CTL category for INTF %d!\n", intf_num);\r\nmatch ^= CTL_STAT_BOOKED;\r\nfor (c = 0; c < ctl_mgr->nctl; c++)\r\nif ((ctl_mgr->ctls[c].status & checkm) == match)\r\ngoto found;\r\ndev_err(ctl_mgr->dev->dev, "No more CTL available!");\r\ngoto unlock;\r\nfound:\r\nctl = &ctl_mgr->ctls[c];\r\nctl->pipeline.intf.num = intf_num;\r\nctl->lm = -1;\r\nctl->status |= CTL_STAT_BUSY;\r\nctl->pending_ctl_trigger = 0;\r\nDBG("CTL %d allocated", ctl->id);\r\nunlock:\r\nspin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);\r\nreturn ctl;\r\n}\r\nvoid mdp5_ctlm_hw_reset(struct mdp5_ctl_manager *ctl_mgr)\r\n{\r\nunsigned long flags;\r\nint c;\r\nfor (c = 0; c < ctl_mgr->nctl; c++) {\r\nstruct mdp5_ctl *ctl = &ctl_mgr->ctls[c];\r\nspin_lock_irqsave(&ctl->hw_lock, flags);\r\nctl_write(ctl, REG_MDP5_CTL_OP(ctl->id), 0);\r\nspin_unlock_irqrestore(&ctl->hw_lock, flags);\r\n}\r\n}\r\nvoid mdp5_ctlm_destroy(struct mdp5_ctl_manager *ctl_mgr)\r\n{\r\nkfree(ctl_mgr);\r\n}\r\nstruct mdp5_ctl_manager *mdp5_ctlm_init(struct drm_device *dev,\r\nvoid __iomem *mmio_base, struct mdp5_cfg_handler *cfg_hnd)\r\n{\r\nstruct mdp5_ctl_manager *ctl_mgr;\r\nconst struct mdp5_cfg_hw *hw_cfg = mdp5_cfg_get_hw_config(cfg_hnd);\r\nint rev = mdp5_cfg_get_hw_rev(cfg_hnd);\r\nconst struct mdp5_ctl_block *ctl_cfg = &hw_cfg->ctl;\r\nunsigned long flags;\r\nint c, ret;\r\nctl_mgr = kzalloc(sizeof(*ctl_mgr), GFP_KERNEL);\r\nif (!ctl_mgr) {\r\ndev_err(dev->dev, "failed to allocate CTL manager\n");\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nif (unlikely(WARN_ON(ctl_cfg->count > MAX_CTL))) {\r\ndev_err(dev->dev, "Increase static pool size to at least %d\n",\r\nctl_cfg->count);\r\nret = -ENOSPC;\r\ngoto fail;\r\n}\r\nctl_mgr->dev = dev;\r\nctl_mgr->nlm = hw_cfg->lm.count;\r\nctl_mgr->nctl = ctl_cfg->count;\r\nctl_mgr->flush_hw_mask = ctl_cfg->flush_hw_mask;\r\nspin_lock_init(&ctl_mgr->pool_lock);\r\nspin_lock_irqsave(&ctl_mgr->pool_lock, flags);\r\nfor (c = 0; c < ctl_mgr->nctl; c++) {\r\nstruct mdp5_ctl *ctl = &ctl_mgr->ctls[c];\r\nif (WARN_ON(!ctl_cfg->base[c])) {\r\ndev_err(dev->dev, "CTL_%d: base is null!\n", c);\r\nret = -EINVAL;\r\nspin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);\r\ngoto fail;\r\n}\r\nctl->ctlm = ctl_mgr;\r\nctl->id = c;\r\nctl->reg_offset = ctl_cfg->base[c];\r\nctl->status = 0;\r\nspin_lock_init(&ctl->hw_lock);\r\n}\r\nif (rev >= 3) {\r\nctl_mgr->single_flush_supported = true;\r\nctl_mgr->ctls[0].status |= CTL_STAT_BOOKED;\r\nctl_mgr->ctls[1].status |= CTL_STAT_BOOKED;\r\n}\r\nspin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);\r\nDBG("Pool of %d CTLs created.", ctl_mgr->nctl);\r\nreturn ctl_mgr;\r\nfail:\r\nif (ctl_mgr)\r\nmdp5_ctlm_destroy(ctl_mgr);\r\nreturn ERR_PTR(ret);\r\n}
