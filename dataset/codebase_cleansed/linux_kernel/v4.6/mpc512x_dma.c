static inline struct mpc_dma_chan *dma_chan_to_mpc_dma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct mpc_dma_chan, chan);\r\n}\r\nstatic inline struct mpc_dma *dma_chan_to_mpc_dma(struct dma_chan *c)\r\n{\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(c);\r\nreturn container_of(mchan, struct mpc_dma, channels[c->chan_id]);\r\n}\r\nstatic void mpc_dma_execute(struct mpc_dma_chan *mchan)\r\n{\r\nstruct mpc_dma *mdma = dma_chan_to_mpc_dma(&mchan->chan);\r\nstruct mpc_dma_desc *first = NULL;\r\nstruct mpc_dma_desc *prev = NULL;\r\nstruct mpc_dma_desc *mdesc;\r\nint cid = mchan->chan.chan_id;\r\nwhile (!list_empty(&mchan->queued)) {\r\nmdesc = list_first_entry(&mchan->queued,\r\nstruct mpc_dma_desc, node);\r\nif (mdesc->will_access_peripheral) {\r\nif (list_empty(&mchan->active))\r\nlist_move_tail(&mdesc->node, &mchan->active);\r\nbreak;\r\n} else {\r\nlist_move_tail(&mdesc->node, &mchan->active);\r\n}\r\n}\r\nlist_for_each_entry(mdesc, &mchan->active, node) {\r\nif (!first)\r\nfirst = mdesc;\r\nif (!prev) {\r\nprev = mdesc;\r\ncontinue;\r\n}\r\nprev->tcd->dlast_sga = mdesc->tcd_paddr;\r\nprev->tcd->e_sg = 1;\r\nmdesc->tcd->start = 1;\r\nprev = mdesc;\r\n}\r\nprev->tcd->int_maj = 1;\r\nmemcpy_toio(&mdma->tcd[cid], first->tcd, sizeof(struct mpc_dma_tcd));\r\nif (first != prev)\r\nmdma->tcd[cid].e_sg = 1;\r\nif (mdma->is_mpc8308) {\r\nout_8(&mdma->regs->dmassrt, cid);\r\n} else if (first->will_access_peripheral) {\r\nout_8(&mdma->regs->dmaserq, cid);\r\n} else {\r\nout_8(&mdma->regs->dmassrt, cid);\r\n}\r\n}\r\nstatic void mpc_dma_irq_process(struct mpc_dma *mdma, u32 is, u32 es, int off)\r\n{\r\nstruct mpc_dma_chan *mchan;\r\nstruct mpc_dma_desc *mdesc;\r\nu32 status = is | es;\r\nint ch;\r\nwhile ((ch = fls(status) - 1) >= 0) {\r\nstatus &= ~(1 << ch);\r\nmchan = &mdma->channels[ch + off];\r\nspin_lock(&mchan->lock);\r\nout_8(&mdma->regs->dmacint, ch + off);\r\nout_8(&mdma->regs->dmacerr, ch + off);\r\nif (es & (1 << ch))\r\nlist_for_each_entry(mdesc, &mchan->active, node)\r\nmdesc->error = -EIO;\r\nlist_splice_tail_init(&mchan->active, &mchan->completed);\r\nif (!list_empty(&mchan->queued))\r\nmpc_dma_execute(mchan);\r\nspin_unlock(&mchan->lock);\r\n}\r\n}\r\nstatic irqreturn_t mpc_dma_irq(int irq, void *data)\r\n{\r\nstruct mpc_dma *mdma = data;\r\nuint es;\r\nes = in_be32(&mdma->regs->dmaes);\r\nspin_lock(&mdma->error_status_lock);\r\nif ((es & MPC_DMA_DMAES_VLD) && mdma->error_status == 0)\r\nmdma->error_status = es;\r\nspin_unlock(&mdma->error_status_lock);\r\nif (mdma->dma.chancnt > 32) {\r\nmpc_dma_irq_process(mdma, in_be32(&mdma->regs->dmainth),\r\nin_be32(&mdma->regs->dmaerrh), 32);\r\n}\r\nmpc_dma_irq_process(mdma, in_be32(&mdma->regs->dmaintl),\r\nin_be32(&mdma->regs->dmaerrl), 0);\r\ntasklet_schedule(&mdma->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void mpc_dma_process_completed(struct mpc_dma *mdma)\r\n{\r\ndma_cookie_t last_cookie = 0;\r\nstruct mpc_dma_chan *mchan;\r\nstruct mpc_dma_desc *mdesc;\r\nstruct dma_async_tx_descriptor *desc;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\nint i;\r\nfor (i = 0; i < mdma->dma.chancnt; i++) {\r\nmchan = &mdma->channels[i];\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nif (!list_empty(&mchan->completed))\r\nlist_splice_tail_init(&mchan->completed, &list);\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\nif (list_empty(&list))\r\ncontinue;\r\nlist_for_each_entry(mdesc, &list, node) {\r\ndesc = &mdesc->desc;\r\nif (desc->callback)\r\ndesc->callback(desc->callback_param);\r\nlast_cookie = desc->cookie;\r\ndma_run_dependencies(desc);\r\n}\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nlist_splice_tail_init(&list, &mchan->free);\r\nmchan->chan.completed_cookie = last_cookie;\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\n}\r\n}\r\nstatic void mpc_dma_tasklet(unsigned long data)\r\n{\r\nstruct mpc_dma *mdma = (void *)data;\r\nunsigned long flags;\r\nuint es;\r\nspin_lock_irqsave(&mdma->error_status_lock, flags);\r\nes = mdma->error_status;\r\nmdma->error_status = 0;\r\nspin_unlock_irqrestore(&mdma->error_status_lock, flags);\r\nif (es) {\r\ndev_err(mdma->dma.dev,\r\n"Hardware reported following error(s) on channel %u:\n",\r\nMPC_DMA_DMAES_ERRCHN(es));\r\nif (es & MPC_DMA_DMAES_GPE)\r\ndev_err(mdma->dma.dev, "- Group Priority Error\n");\r\nif (es & MPC_DMA_DMAES_CPE)\r\ndev_err(mdma->dma.dev, "- Channel Priority Error\n");\r\nif (es & MPC_DMA_DMAES_SAE)\r\ndev_err(mdma->dma.dev, "- Source Address Error\n");\r\nif (es & MPC_DMA_DMAES_SOE)\r\ndev_err(mdma->dma.dev, "- Source Offset"\r\n" Configuration Error\n");\r\nif (es & MPC_DMA_DMAES_DAE)\r\ndev_err(mdma->dma.dev, "- Destination Address"\r\n" Error\n");\r\nif (es & MPC_DMA_DMAES_DOE)\r\ndev_err(mdma->dma.dev, "- Destination Offset"\r\n" Configuration Error\n");\r\nif (es & MPC_DMA_DMAES_NCE)\r\ndev_err(mdma->dma.dev, "- NBytes/Citter"\r\n" Configuration Error\n");\r\nif (es & MPC_DMA_DMAES_SGE)\r\ndev_err(mdma->dma.dev, "- Scatter/Gather"\r\n" Configuration Error\n");\r\nif (es & MPC_DMA_DMAES_SBE)\r\ndev_err(mdma->dma.dev, "- Source Bus Error\n");\r\nif (es & MPC_DMA_DMAES_DBE)\r\ndev_err(mdma->dma.dev, "- Destination Bus Error\n");\r\n}\r\nmpc_dma_process_completed(mdma);\r\n}\r\nstatic dma_cookie_t mpc_dma_tx_submit(struct dma_async_tx_descriptor *txd)\r\n{\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(txd->chan);\r\nstruct mpc_dma_desc *mdesc;\r\nunsigned long flags;\r\ndma_cookie_t cookie;\r\nmdesc = container_of(txd, struct mpc_dma_desc, desc);\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nlist_move_tail(&mdesc->node, &mchan->queued);\r\nif (list_empty(&mchan->active))\r\nmpc_dma_execute(mchan);\r\ncookie = dma_cookie_assign(txd);\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic int mpc_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct mpc_dma *mdma = dma_chan_to_mpc_dma(chan);\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(chan);\r\nstruct mpc_dma_desc *mdesc;\r\nstruct mpc_dma_tcd *tcd;\r\ndma_addr_t tcd_paddr;\r\nunsigned long flags;\r\nLIST_HEAD(descs);\r\nint i;\r\ntcd = dma_alloc_coherent(mdma->dma.dev,\r\nMPC_DMA_DESCRIPTORS * sizeof(struct mpc_dma_tcd),\r\n&tcd_paddr, GFP_KERNEL);\r\nif (!tcd)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < MPC_DMA_DESCRIPTORS; i++) {\r\nmdesc = kzalloc(sizeof(struct mpc_dma_desc), GFP_KERNEL);\r\nif (!mdesc) {\r\ndev_notice(mdma->dma.dev, "Memory allocation error. "\r\n"Allocated only %u descriptors\n", i);\r\nbreak;\r\n}\r\ndma_async_tx_descriptor_init(&mdesc->desc, chan);\r\nmdesc->desc.flags = DMA_CTRL_ACK;\r\nmdesc->desc.tx_submit = mpc_dma_tx_submit;\r\nmdesc->tcd = &tcd[i];\r\nmdesc->tcd_paddr = tcd_paddr + (i * sizeof(struct mpc_dma_tcd));\r\nlist_add_tail(&mdesc->node, &descs);\r\n}\r\nif (i == 0) {\r\ndma_free_coherent(mdma->dma.dev,\r\nMPC_DMA_DESCRIPTORS * sizeof(struct mpc_dma_tcd),\r\ntcd, tcd_paddr);\r\nreturn -ENOMEM;\r\n}\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nmchan->tcd = tcd;\r\nmchan->tcd_paddr = tcd_paddr;\r\nlist_splice_tail_init(&descs, &mchan->free);\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\nout_8(&mdma->regs->dmaseei, chan->chan_id);\r\nreturn 0;\r\n}\r\nstatic void mpc_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct mpc_dma *mdma = dma_chan_to_mpc_dma(chan);\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(chan);\r\nstruct mpc_dma_desc *mdesc, *tmp;\r\nstruct mpc_dma_tcd *tcd;\r\ndma_addr_t tcd_paddr;\r\nunsigned long flags;\r\nLIST_HEAD(descs);\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nBUG_ON(!list_empty(&mchan->prepared));\r\nBUG_ON(!list_empty(&mchan->queued));\r\nBUG_ON(!list_empty(&mchan->active));\r\nBUG_ON(!list_empty(&mchan->completed));\r\nlist_splice_tail_init(&mchan->free, &descs);\r\ntcd = mchan->tcd;\r\ntcd_paddr = mchan->tcd_paddr;\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\ndma_free_coherent(mdma->dma.dev,\r\nMPC_DMA_DESCRIPTORS * sizeof(struct mpc_dma_tcd),\r\ntcd, tcd_paddr);\r\nlist_for_each_entry_safe(mdesc, tmp, &descs, node)\r\nkfree(mdesc);\r\nout_8(&mdma->regs->dmaceei, chan->chan_id);\r\n}\r\nstatic void mpc_dma_issue_pending(struct dma_chan *chan)\r\n{\r\n}\r\nstatic enum dma_status\r\nmpc_dma_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nreturn dma_cookie_status(chan, cookie, txstate);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmpc_dma_prep_memcpy(struct dma_chan *chan, dma_addr_t dst, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct mpc_dma *mdma = dma_chan_to_mpc_dma(chan);\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(chan);\r\nstruct mpc_dma_desc *mdesc = NULL;\r\nstruct mpc_dma_tcd *tcd;\r\nunsigned long iflags;\r\nspin_lock_irqsave(&mchan->lock, iflags);\r\nif (!list_empty(&mchan->free)) {\r\nmdesc = list_first_entry(&mchan->free, struct mpc_dma_desc,\r\nnode);\r\nlist_del(&mdesc->node);\r\n}\r\nspin_unlock_irqrestore(&mchan->lock, iflags);\r\nif (!mdesc) {\r\nmpc_dma_process_completed(mdma);\r\nreturn NULL;\r\n}\r\nmdesc->error = 0;\r\nmdesc->will_access_peripheral = 0;\r\ntcd = mdesc->tcd;\r\nmemset(tcd, 0, sizeof(struct mpc_dma_tcd));\r\nif (IS_ALIGNED(src | dst | len, 32)) {\r\ntcd->ssize = MPC_DMA_TSIZE_32;\r\ntcd->dsize = MPC_DMA_TSIZE_32;\r\ntcd->soff = 32;\r\ntcd->doff = 32;\r\n} else if (!mdma->is_mpc8308 && IS_ALIGNED(src | dst | len, 16)) {\r\ntcd->ssize = MPC_DMA_TSIZE_16;\r\ntcd->dsize = MPC_DMA_TSIZE_16;\r\ntcd->soff = 16;\r\ntcd->doff = 16;\r\n} else if (IS_ALIGNED(src | dst | len, 4)) {\r\ntcd->ssize = MPC_DMA_TSIZE_4;\r\ntcd->dsize = MPC_DMA_TSIZE_4;\r\ntcd->soff = 4;\r\ntcd->doff = 4;\r\n} else if (IS_ALIGNED(src | dst | len, 2)) {\r\ntcd->ssize = MPC_DMA_TSIZE_2;\r\ntcd->dsize = MPC_DMA_TSIZE_2;\r\ntcd->soff = 2;\r\ntcd->doff = 2;\r\n} else {\r\ntcd->ssize = MPC_DMA_TSIZE_1;\r\ntcd->dsize = MPC_DMA_TSIZE_1;\r\ntcd->soff = 1;\r\ntcd->doff = 1;\r\n}\r\ntcd->saddr = src;\r\ntcd->daddr = dst;\r\ntcd->nbytes = len;\r\ntcd->biter = 1;\r\ntcd->citer = 1;\r\nspin_lock_irqsave(&mchan->lock, iflags);\r\nlist_add_tail(&mdesc->node, &mchan->prepared);\r\nspin_unlock_irqrestore(&mchan->lock, iflags);\r\nreturn &mdesc->desc;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmpc_dma_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct mpc_dma *mdma = dma_chan_to_mpc_dma(chan);\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(chan);\r\nstruct mpc_dma_desc *mdesc = NULL;\r\ndma_addr_t per_paddr;\r\nu32 tcd_nunits;\r\nstruct mpc_dma_tcd *tcd;\r\nunsigned long iflags;\r\nstruct scatterlist *sg;\r\nsize_t len;\r\nint iter, i;\r\nif (sg_len != 1)\r\nreturn NULL;\r\nif (!is_slave_direction(direction))\r\nreturn NULL;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nspin_lock_irqsave(&mchan->lock, iflags);\r\nmdesc = list_first_entry(&mchan->free,\r\nstruct mpc_dma_desc, node);\r\nif (!mdesc) {\r\nspin_unlock_irqrestore(&mchan->lock, iflags);\r\nmpc_dma_process_completed(mdma);\r\nreturn NULL;\r\n}\r\nlist_del(&mdesc->node);\r\nif (direction == DMA_DEV_TO_MEM) {\r\nper_paddr = mchan->src_per_paddr;\r\ntcd_nunits = mchan->src_tcd_nunits;\r\n} else {\r\nper_paddr = mchan->dst_per_paddr;\r\ntcd_nunits = mchan->dst_tcd_nunits;\r\n}\r\nspin_unlock_irqrestore(&mchan->lock, iflags);\r\nif (per_paddr == 0 || tcd_nunits == 0)\r\ngoto err_prep;\r\nmdesc->error = 0;\r\nmdesc->will_access_peripheral = 1;\r\ntcd = mdesc->tcd;\r\nmemset(tcd, 0, sizeof(struct mpc_dma_tcd));\r\nif (!IS_ALIGNED(sg_dma_address(sg), 4))\r\ngoto err_prep;\r\nif (direction == DMA_DEV_TO_MEM) {\r\ntcd->saddr = per_paddr;\r\ntcd->daddr = sg_dma_address(sg);\r\ntcd->soff = 0;\r\ntcd->doff = 4;\r\n} else {\r\ntcd->saddr = sg_dma_address(sg);\r\ntcd->daddr = per_paddr;\r\ntcd->soff = 4;\r\ntcd->doff = 0;\r\n}\r\ntcd->ssize = MPC_DMA_TSIZE_4;\r\ntcd->dsize = MPC_DMA_TSIZE_4;\r\nlen = sg_dma_len(sg);\r\ntcd->nbytes = tcd_nunits * 4;\r\nif (!IS_ALIGNED(len, tcd->nbytes))\r\ngoto err_prep;\r\niter = len / tcd->nbytes;\r\nif (iter >= 1 << 15) {\r\ngoto err_prep;\r\n}\r\ntcd->biter = iter & 0x1ff;\r\ntcd->biter_linkch = iter >> 9;\r\ntcd->citer = tcd->biter;\r\ntcd->citer_linkch = tcd->biter_linkch;\r\ntcd->e_sg = 0;\r\ntcd->d_req = 1;\r\nspin_lock_irqsave(&mchan->lock, iflags);\r\nlist_add_tail(&mdesc->node, &mchan->prepared);\r\nspin_unlock_irqrestore(&mchan->lock, iflags);\r\n}\r\nreturn &mdesc->desc;\r\nerr_prep:\r\nspin_lock_irqsave(&mchan->lock, iflags);\r\nlist_add_tail(&mdesc->node, &mchan->free);\r\nspin_unlock_irqrestore(&mchan->lock, iflags);\r\nreturn NULL;\r\n}\r\nstatic int mpc_dma_device_config(struct dma_chan *chan,\r\nstruct dma_slave_config *cfg)\r\n{\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(chan);\r\nunsigned long flags;\r\nif (cfg->src_addr_width != DMA_SLAVE_BUSWIDTH_4_BYTES ||\r\ncfg->dst_addr_width != DMA_SLAVE_BUSWIDTH_4_BYTES ||\r\n!IS_ALIGNED(cfg->src_addr, 4) ||\r\n!IS_ALIGNED(cfg->dst_addr, 4)) {\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nmchan->src_per_paddr = cfg->src_addr;\r\nmchan->src_tcd_nunits = cfg->src_maxburst;\r\nmchan->dst_per_paddr = cfg->dst_addr;\r\nmchan->dst_tcd_nunits = cfg->dst_maxburst;\r\nif (mchan->src_tcd_nunits == 0)\r\nmchan->src_tcd_nunits = 1;\r\nif (mchan->dst_tcd_nunits == 0)\r\nmchan->dst_tcd_nunits = 1;\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int mpc_dma_device_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct mpc_dma_chan *mchan = dma_chan_to_mpc_dma_chan(chan);\r\nstruct mpc_dma *mdma = dma_chan_to_mpc_dma(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nout_8(&mdma->regs->dmacerq, chan->chan_id);\r\nlist_splice_tail_init(&mchan->prepared, &mchan->free);\r\nlist_splice_tail_init(&mchan->queued, &mchan->free);\r\nlist_splice_tail_init(&mchan->active, &mchan->free);\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int mpc_dma_probe(struct platform_device *op)\r\n{\r\nstruct device_node *dn = op->dev.of_node;\r\nstruct device *dev = &op->dev;\r\nstruct dma_device *dma;\r\nstruct mpc_dma *mdma;\r\nstruct mpc_dma_chan *mchan;\r\nstruct resource res;\r\nulong regs_start, regs_size;\r\nint retval, i;\r\nu8 chancnt;\r\nmdma = devm_kzalloc(dev, sizeof(struct mpc_dma), GFP_KERNEL);\r\nif (!mdma) {\r\ndev_err(dev, "Memory exhausted!\n");\r\nretval = -ENOMEM;\r\ngoto err;\r\n}\r\nmdma->irq = irq_of_parse_and_map(dn, 0);\r\nif (mdma->irq == NO_IRQ) {\r\ndev_err(dev, "Error mapping IRQ!\n");\r\nretval = -EINVAL;\r\ngoto err;\r\n}\r\nif (of_device_is_compatible(dn, "fsl,mpc8308-dma")) {\r\nmdma->is_mpc8308 = 1;\r\nmdma->irq2 = irq_of_parse_and_map(dn, 1);\r\nif (mdma->irq2 == NO_IRQ) {\r\ndev_err(dev, "Error mapping IRQ!\n");\r\nretval = -EINVAL;\r\ngoto err_dispose1;\r\n}\r\n}\r\nretval = of_address_to_resource(dn, 0, &res);\r\nif (retval) {\r\ndev_err(dev, "Error parsing memory region!\n");\r\ngoto err_dispose2;\r\n}\r\nregs_start = res.start;\r\nregs_size = resource_size(&res);\r\nif (!devm_request_mem_region(dev, regs_start, regs_size, DRV_NAME)) {\r\ndev_err(dev, "Error requesting memory region!\n");\r\nretval = -EBUSY;\r\ngoto err_dispose2;\r\n}\r\nmdma->regs = devm_ioremap(dev, regs_start, regs_size);\r\nif (!mdma->regs) {\r\ndev_err(dev, "Error mapping memory region!\n");\r\nretval = -ENOMEM;\r\ngoto err_dispose2;\r\n}\r\nmdma->tcd = (struct mpc_dma_tcd *)((u8 *)(mdma->regs)\r\n+ MPC_DMA_TCD_OFFSET);\r\nretval = request_irq(mdma->irq, &mpc_dma_irq, 0, DRV_NAME, mdma);\r\nif (retval) {\r\ndev_err(dev, "Error requesting IRQ!\n");\r\nretval = -EINVAL;\r\ngoto err_dispose2;\r\n}\r\nif (mdma->is_mpc8308) {\r\nretval = request_irq(mdma->irq2, &mpc_dma_irq, 0,\r\nDRV_NAME, mdma);\r\nif (retval) {\r\ndev_err(dev, "Error requesting IRQ2!\n");\r\nretval = -EINVAL;\r\ngoto err_free1;\r\n}\r\n}\r\nspin_lock_init(&mdma->error_status_lock);\r\ndma = &mdma->dma;\r\ndma->dev = dev;\r\ndma->device_alloc_chan_resources = mpc_dma_alloc_chan_resources;\r\ndma->device_free_chan_resources = mpc_dma_free_chan_resources;\r\ndma->device_issue_pending = mpc_dma_issue_pending;\r\ndma->device_tx_status = mpc_dma_tx_status;\r\ndma->device_prep_dma_memcpy = mpc_dma_prep_memcpy;\r\ndma->device_prep_slave_sg = mpc_dma_prep_slave_sg;\r\ndma->device_config = mpc_dma_device_config;\r\ndma->device_terminate_all = mpc_dma_device_terminate_all;\r\nINIT_LIST_HEAD(&dma->channels);\r\ndma_cap_set(DMA_MEMCPY, dma->cap_mask);\r\ndma_cap_set(DMA_SLAVE, dma->cap_mask);\r\nif (mdma->is_mpc8308)\r\nchancnt = MPC8308_DMACHAN_MAX;\r\nelse\r\nchancnt = MPC512x_DMACHAN_MAX;\r\nfor (i = 0; i < chancnt; i++) {\r\nmchan = &mdma->channels[i];\r\nmchan->chan.device = dma;\r\ndma_cookie_init(&mchan->chan);\r\nINIT_LIST_HEAD(&mchan->free);\r\nINIT_LIST_HEAD(&mchan->prepared);\r\nINIT_LIST_HEAD(&mchan->queued);\r\nINIT_LIST_HEAD(&mchan->active);\r\nINIT_LIST_HEAD(&mchan->completed);\r\nspin_lock_init(&mchan->lock);\r\nlist_add_tail(&mchan->chan.device_node, &dma->channels);\r\n}\r\ntasklet_init(&mdma->tasklet, mpc_dma_tasklet, (unsigned long)mdma);\r\nif (mdma->is_mpc8308) {\r\nout_be32(&mdma->regs->dmacr, MPC_DMA_DMACR_ERCA);\r\nout_be32(&mdma->regs->dmagpor, MPC_DMA_DMAGPOR_SNOOP_ENABLE);\r\nout_be32(&mdma->regs->dmaeeil, 0);\r\nout_be32(&mdma->regs->dmaintl, 0xFFFF);\r\nout_be32(&mdma->regs->dmaerrl, 0xFFFF);\r\n} else {\r\nout_be32(&mdma->regs->dmacr, MPC_DMA_DMACR_EDCG |\r\nMPC_DMA_DMACR_ERGA | MPC_DMA_DMACR_ERCA);\r\nout_be32(&mdma->regs->dmaerqh, 0);\r\nout_be32(&mdma->regs->dmaerql, 0);\r\nout_be32(&mdma->regs->dmaeeih, 0);\r\nout_be32(&mdma->regs->dmaeeil, 0);\r\nout_be32(&mdma->regs->dmainth, 0xFFFFFFFF);\r\nout_be32(&mdma->regs->dmaintl, 0xFFFFFFFF);\r\nout_be32(&mdma->regs->dmaerrh, 0xFFFFFFFF);\r\nout_be32(&mdma->regs->dmaerrl, 0xFFFFFFFF);\r\nout_be32(&mdma->regs->dmaihsa, 0);\r\nout_be32(&mdma->regs->dmailsa, 0);\r\n}\r\ndev_set_drvdata(dev, mdma);\r\nretval = dma_async_device_register(dma);\r\nif (retval)\r\ngoto err_free2;\r\nif (dev->of_node) {\r\nretval = of_dma_controller_register(dev->of_node,\r\nof_dma_xlate_by_chan_id, mdma);\r\nif (retval)\r\ndev_warn(dev, "Could not register for OF lookup\n");\r\n}\r\nreturn 0;\r\nerr_free2:\r\nif (mdma->is_mpc8308)\r\nfree_irq(mdma->irq2, mdma);\r\nerr_free1:\r\nfree_irq(mdma->irq, mdma);\r\nerr_dispose2:\r\nif (mdma->is_mpc8308)\r\nirq_dispose_mapping(mdma->irq2);\r\nerr_dispose1:\r\nirq_dispose_mapping(mdma->irq);\r\nerr:\r\nreturn retval;\r\n}\r\nstatic int mpc_dma_remove(struct platform_device *op)\r\n{\r\nstruct device *dev = &op->dev;\r\nstruct mpc_dma *mdma = dev_get_drvdata(dev);\r\nif (dev->of_node)\r\nof_dma_controller_free(dev->of_node);\r\ndma_async_device_unregister(&mdma->dma);\r\nif (mdma->is_mpc8308) {\r\nfree_irq(mdma->irq2, mdma);\r\nirq_dispose_mapping(mdma->irq2);\r\n}\r\nfree_irq(mdma->irq, mdma);\r\nirq_dispose_mapping(mdma->irq);\r\nreturn 0;\r\n}
