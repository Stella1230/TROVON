static void dmar_register_drhd_unit(struct dmar_drhd_unit *drhd)\r\n{\r\nif (drhd->include_all)\r\nlist_add_tail_rcu(&drhd->list, &dmar_drhd_units);\r\nelse\r\nlist_add_rcu(&drhd->list, &dmar_drhd_units);\r\n}\r\nvoid *dmar_alloc_dev_scope(void *start, void *end, int *cnt)\r\n{\r\nstruct acpi_dmar_device_scope *scope;\r\n*cnt = 0;\r\nwhile (start < end) {\r\nscope = start;\r\nif (scope->entry_type == ACPI_DMAR_SCOPE_TYPE_NAMESPACE ||\r\nscope->entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT ||\r\nscope->entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE)\r\n(*cnt)++;\r\nelse if (scope->entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC &&\r\nscope->entry_type != ACPI_DMAR_SCOPE_TYPE_HPET) {\r\npr_warn("Unsupported device scope\n");\r\n}\r\nstart += scope->length;\r\n}\r\nif (*cnt == 0)\r\nreturn NULL;\r\nreturn kcalloc(*cnt, sizeof(struct dmar_dev_scope), GFP_KERNEL);\r\n}\r\nvoid dmar_free_dev_scope(struct dmar_dev_scope **devices, int *cnt)\r\n{\r\nint i;\r\nstruct device *tmp_dev;\r\nif (*devices && *cnt) {\r\nfor_each_active_dev_scope(*devices, *cnt, i, tmp_dev)\r\nput_device(tmp_dev);\r\nkfree(*devices);\r\n}\r\n*devices = NULL;\r\n*cnt = 0;\r\n}\r\nstatic struct dmar_pci_notify_info *\r\ndmar_alloc_pci_notify_info(struct pci_dev *dev, unsigned long event)\r\n{\r\nint level = 0;\r\nsize_t size;\r\nstruct pci_dev *tmp;\r\nstruct dmar_pci_notify_info *info;\r\nBUG_ON(dev->is_virtfn);\r\nif (event == BUS_NOTIFY_ADD_DEVICE)\r\nfor (tmp = dev; tmp; tmp = tmp->bus->self)\r\nlevel++;\r\nsize = sizeof(*info) + level * sizeof(struct acpi_dmar_pci_path);\r\nif (size <= sizeof(dmar_pci_notify_info_buf)) {\r\ninfo = (struct dmar_pci_notify_info *)dmar_pci_notify_info_buf;\r\n} else {\r\ninfo = kzalloc(size, GFP_KERNEL);\r\nif (!info) {\r\npr_warn("Out of memory when allocating notify_info "\r\n"for %s.\n", pci_name(dev));\r\nif (dmar_dev_scope_status == 0)\r\ndmar_dev_scope_status = -ENOMEM;\r\nreturn NULL;\r\n}\r\n}\r\ninfo->event = event;\r\ninfo->dev = dev;\r\ninfo->seg = pci_domain_nr(dev->bus);\r\ninfo->level = level;\r\nif (event == BUS_NOTIFY_ADD_DEVICE) {\r\nfor (tmp = dev; tmp; tmp = tmp->bus->self) {\r\nlevel--;\r\ninfo->path[level].bus = tmp->bus->number;\r\ninfo->path[level].device = PCI_SLOT(tmp->devfn);\r\ninfo->path[level].function = PCI_FUNC(tmp->devfn);\r\nif (pci_is_root_bus(tmp->bus))\r\ninfo->bus = tmp->bus->number;\r\n}\r\n}\r\nreturn info;\r\n}\r\nstatic inline void dmar_free_pci_notify_info(struct dmar_pci_notify_info *info)\r\n{\r\nif ((void *)info != dmar_pci_notify_info_buf)\r\nkfree(info);\r\n}\r\nstatic bool dmar_match_pci_path(struct dmar_pci_notify_info *info, int bus,\r\nstruct acpi_dmar_pci_path *path, int count)\r\n{\r\nint i;\r\nif (info->bus != bus)\r\ngoto fallback;\r\nif (info->level != count)\r\ngoto fallback;\r\nfor (i = 0; i < count; i++) {\r\nif (path[i].device != info->path[i].device ||\r\npath[i].function != info->path[i].function)\r\ngoto fallback;\r\n}\r\nreturn true;\r\nfallback:\r\nif (count != 1)\r\nreturn false;\r\ni = info->level - 1;\r\nif (bus == info->path[i].bus &&\r\npath[0].device == info->path[i].device &&\r\npath[0].function == info->path[i].function) {\r\npr_info(FW_BUG "RMRR entry for device %02x:%02x.%x is broken - applying workaround\n",\r\nbus, path[0].device, path[0].function);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nint dmar_insert_dev_scope(struct dmar_pci_notify_info *info,\r\nvoid *start, void*end, u16 segment,\r\nstruct dmar_dev_scope *devices,\r\nint devices_cnt)\r\n{\r\nint i, level;\r\nstruct device *tmp, *dev = &info->dev->dev;\r\nstruct acpi_dmar_device_scope *scope;\r\nstruct acpi_dmar_pci_path *path;\r\nif (segment != info->seg)\r\nreturn 0;\r\nfor (; start < end; start += scope->length) {\r\nscope = start;\r\nif (scope->entry_type != ACPI_DMAR_SCOPE_TYPE_ENDPOINT &&\r\nscope->entry_type != ACPI_DMAR_SCOPE_TYPE_BRIDGE)\r\ncontinue;\r\npath = (struct acpi_dmar_pci_path *)(scope + 1);\r\nlevel = (scope->length - sizeof(*scope)) / sizeof(*path);\r\nif (!dmar_match_pci_path(info, scope->bus, path, level))\r\ncontinue;\r\nif ((scope->entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT) ^\r\n(info->dev->hdr_type == PCI_HEADER_TYPE_NORMAL)) {\r\npr_warn("Device scope type does not match for %s\n",\r\npci_name(info->dev));\r\nreturn -EINVAL;\r\n}\r\nfor_each_dev_scope(devices, devices_cnt, i, tmp)\r\nif (tmp == NULL) {\r\ndevices[i].bus = info->dev->bus->number;\r\ndevices[i].devfn = info->dev->devfn;\r\nrcu_assign_pointer(devices[i].dev,\r\nget_device(dev));\r\nreturn 1;\r\n}\r\nBUG_ON(i >= devices_cnt);\r\n}\r\nreturn 0;\r\n}\r\nint dmar_remove_dev_scope(struct dmar_pci_notify_info *info, u16 segment,\r\nstruct dmar_dev_scope *devices, int count)\r\n{\r\nint index;\r\nstruct device *tmp;\r\nif (info->seg != segment)\r\nreturn 0;\r\nfor_each_active_dev_scope(devices, count, index, tmp)\r\nif (tmp == &info->dev->dev) {\r\nRCU_INIT_POINTER(devices[index].dev, NULL);\r\nsynchronize_rcu();\r\nput_device(tmp);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int dmar_pci_bus_add_dev(struct dmar_pci_notify_info *info)\r\n{\r\nint ret = 0;\r\nstruct dmar_drhd_unit *dmaru;\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nfor_each_drhd_unit(dmaru) {\r\nif (dmaru->include_all)\r\ncontinue;\r\ndrhd = container_of(dmaru->hdr,\r\nstruct acpi_dmar_hardware_unit, header);\r\nret = dmar_insert_dev_scope(info, (void *)(drhd + 1),\r\n((void *)drhd) + drhd->header.length,\r\ndmaru->segment,\r\ndmaru->devices, dmaru->devices_cnt);\r\nif (ret != 0)\r\nbreak;\r\n}\r\nif (ret >= 0)\r\nret = dmar_iommu_notify_scope_dev(info);\r\nif (ret < 0 && dmar_dev_scope_status == 0)\r\ndmar_dev_scope_status = ret;\r\nreturn ret;\r\n}\r\nstatic void dmar_pci_bus_del_dev(struct dmar_pci_notify_info *info)\r\n{\r\nstruct dmar_drhd_unit *dmaru;\r\nfor_each_drhd_unit(dmaru)\r\nif (dmar_remove_dev_scope(info, dmaru->segment,\r\ndmaru->devices, dmaru->devices_cnt))\r\nbreak;\r\ndmar_iommu_notify_scope_dev(info);\r\n}\r\nstatic int dmar_pci_bus_notifier(struct notifier_block *nb,\r\nunsigned long action, void *data)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(data);\r\nstruct dmar_pci_notify_info *info;\r\nif (pdev->is_virtfn)\r\nreturn NOTIFY_DONE;\r\nif (action != BUS_NOTIFY_ADD_DEVICE &&\r\naction != BUS_NOTIFY_REMOVED_DEVICE)\r\nreturn NOTIFY_DONE;\r\ninfo = dmar_alloc_pci_notify_info(pdev, action);\r\nif (!info)\r\nreturn NOTIFY_DONE;\r\ndown_write(&dmar_global_lock);\r\nif (action == BUS_NOTIFY_ADD_DEVICE)\r\ndmar_pci_bus_add_dev(info);\r\nelse if (action == BUS_NOTIFY_REMOVED_DEVICE)\r\ndmar_pci_bus_del_dev(info);\r\nup_write(&dmar_global_lock);\r\ndmar_free_pci_notify_info(info);\r\nreturn NOTIFY_OK;\r\n}\r\nstatic struct dmar_drhd_unit *\r\ndmar_find_dmaru(struct acpi_dmar_hardware_unit *drhd)\r\n{\r\nstruct dmar_drhd_unit *dmaru;\r\nlist_for_each_entry_rcu(dmaru, &dmar_drhd_units, list)\r\nif (dmaru->segment == drhd->segment &&\r\ndmaru->reg_base_addr == drhd->address)\r\nreturn dmaru;\r\nreturn NULL;\r\n}\r\nstatic int dmar_parse_one_drhd(struct acpi_dmar_header *header, void *arg)\r\n{\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nstruct dmar_drhd_unit *dmaru;\r\nint ret = 0;\r\ndrhd = (struct acpi_dmar_hardware_unit *)header;\r\ndmaru = dmar_find_dmaru(drhd);\r\nif (dmaru)\r\ngoto out;\r\ndmaru = kzalloc(sizeof(*dmaru) + header->length, GFP_KERNEL);\r\nif (!dmaru)\r\nreturn -ENOMEM;\r\ndmaru->hdr = (void *)(dmaru + 1);\r\nmemcpy(dmaru->hdr, header, header->length);\r\ndmaru->reg_base_addr = drhd->address;\r\ndmaru->segment = drhd->segment;\r\ndmaru->include_all = drhd->flags & 0x1;\r\ndmaru->devices = dmar_alloc_dev_scope((void *)(drhd + 1),\r\n((void *)drhd) + drhd->header.length,\r\n&dmaru->devices_cnt);\r\nif (dmaru->devices_cnt && dmaru->devices == NULL) {\r\nkfree(dmaru);\r\nreturn -ENOMEM;\r\n}\r\nret = alloc_iommu(dmaru);\r\nif (ret) {\r\ndmar_free_dev_scope(&dmaru->devices,\r\n&dmaru->devices_cnt);\r\nkfree(dmaru);\r\nreturn ret;\r\n}\r\ndmar_register_drhd_unit(dmaru);\r\nout:\r\nif (arg)\r\n(*(int *)arg)++;\r\nreturn 0;\r\n}\r\nstatic void dmar_free_drhd(struct dmar_drhd_unit *dmaru)\r\n{\r\nif (dmaru->devices && dmaru->devices_cnt)\r\ndmar_free_dev_scope(&dmaru->devices, &dmaru->devices_cnt);\r\nif (dmaru->iommu)\r\nfree_iommu(dmaru->iommu);\r\nkfree(dmaru);\r\n}\r\nstatic int __init dmar_parse_one_andd(struct acpi_dmar_header *header,\r\nvoid *arg)\r\n{\r\nstruct acpi_dmar_andd *andd = (void *)header;\r\nif (strnlen(andd->device_name, header->length - 8) == header->length - 8) {\r\nWARN_TAINT(1, TAINT_FIRMWARE_WORKAROUND,\r\n"Your BIOS is broken; ANDD object name is not NUL-terminated\n"\r\n"BIOS vendor: %s; Ver: %s; Product Version: %s\n",\r\ndmi_get_system_info(DMI_BIOS_VENDOR),\r\ndmi_get_system_info(DMI_BIOS_VERSION),\r\ndmi_get_system_info(DMI_PRODUCT_VERSION));\r\nreturn -EINVAL;\r\n}\r\npr_info("ANDD device: %x name: %s\n", andd->device_number,\r\nandd->device_name);\r\nreturn 0;\r\n}\r\nstatic int dmar_parse_one_rhsa(struct acpi_dmar_header *header, void *arg)\r\n{\r\nstruct acpi_dmar_rhsa *rhsa;\r\nstruct dmar_drhd_unit *drhd;\r\nrhsa = (struct acpi_dmar_rhsa *)header;\r\nfor_each_drhd_unit(drhd) {\r\nif (drhd->reg_base_addr == rhsa->base_address) {\r\nint node = acpi_map_pxm_to_node(rhsa->proximity_domain);\r\nif (!node_online(node))\r\nnode = -1;\r\ndrhd->iommu->node = node;\r\nreturn 0;\r\n}\r\n}\r\nWARN_TAINT(\r\n1, TAINT_FIRMWARE_WORKAROUND,\r\n"Your BIOS is broken; RHSA refers to non-existent DMAR unit at %llx\n"\r\n"BIOS vendor: %s; Ver: %s; Product Version: %s\n",\r\ndrhd->reg_base_addr,\r\ndmi_get_system_info(DMI_BIOS_VENDOR),\r\ndmi_get_system_info(DMI_BIOS_VERSION),\r\ndmi_get_system_info(DMI_PRODUCT_VERSION));\r\nreturn 0;\r\n}\r\nstatic void __init\r\ndmar_table_print_dmar_entry(struct acpi_dmar_header *header)\r\n{\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nstruct acpi_dmar_reserved_memory *rmrr;\r\nstruct acpi_dmar_atsr *atsr;\r\nstruct acpi_dmar_rhsa *rhsa;\r\nswitch (header->type) {\r\ncase ACPI_DMAR_TYPE_HARDWARE_UNIT:\r\ndrhd = container_of(header, struct acpi_dmar_hardware_unit,\r\nheader);\r\npr_info("DRHD base: %#016Lx flags: %#x\n",\r\n(unsigned long long)drhd->address, drhd->flags);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_RESERVED_MEMORY:\r\nrmrr = container_of(header, struct acpi_dmar_reserved_memory,\r\nheader);\r\npr_info("RMRR base: %#016Lx end: %#016Lx\n",\r\n(unsigned long long)rmrr->base_address,\r\n(unsigned long long)rmrr->end_address);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_ROOT_ATS:\r\natsr = container_of(header, struct acpi_dmar_atsr, header);\r\npr_info("ATSR flags: %#x\n", atsr->flags);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_HARDWARE_AFFINITY:\r\nrhsa = container_of(header, struct acpi_dmar_rhsa, header);\r\npr_info("RHSA base: %#016Lx proximity domain: %#x\n",\r\n(unsigned long long)rhsa->base_address,\r\nrhsa->proximity_domain);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_NAMESPACE:\r\nbreak;\r\n}\r\n}\r\nstatic int __init dmar_table_detect(void)\r\n{\r\nacpi_status status = AE_OK;\r\nstatus = acpi_get_table_with_size(ACPI_SIG_DMAR, 0,\r\n(struct acpi_table_header **)&dmar_tbl,\r\n&dmar_tbl_size);\r\nif (ACPI_SUCCESS(status) && !dmar_tbl) {\r\npr_warn("Unable to map DMAR\n");\r\nstatus = AE_NOT_FOUND;\r\n}\r\nreturn (ACPI_SUCCESS(status) ? 1 : 0);\r\n}\r\nstatic int dmar_walk_remapping_entries(struct acpi_dmar_header *start,\r\nsize_t len, struct dmar_res_callback *cb)\r\n{\r\nint ret = 0;\r\nstruct acpi_dmar_header *iter, *next;\r\nstruct acpi_dmar_header *end = ((void *)start) + len;\r\nfor (iter = start; iter < end && ret == 0; iter = next) {\r\nnext = (void *)iter + iter->length;\r\nif (iter->length == 0) {\r\npr_debug(FW_BUG "Invalid 0-length structure\n");\r\nbreak;\r\n} else if (next > end) {\r\npr_warn(FW_BUG "Record passes table end\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nif (cb->print_entry)\r\ndmar_table_print_dmar_entry(iter);\r\nif (iter->type >= ACPI_DMAR_TYPE_RESERVED) {\r\npr_debug("Unknown DMAR structure type %d\n",\r\niter->type);\r\n} else if (cb->cb[iter->type]) {\r\nret = cb->cb[iter->type](iter, cb->arg[iter->type]);\r\n} else if (!cb->ignore_unhandled) {\r\npr_warn("No handler for DMAR structure type %d\n",\r\niter->type);\r\nret = -EINVAL;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic inline int dmar_walk_dmar_table(struct acpi_table_dmar *dmar,\r\nstruct dmar_res_callback *cb)\r\n{\r\nreturn dmar_walk_remapping_entries((void *)(dmar + 1),\r\ndmar->header.length - sizeof(*dmar), cb);\r\n}\r\nstatic int __init\r\nparse_dmar_table(void)\r\n{\r\nstruct acpi_table_dmar *dmar;\r\nint ret = 0;\r\nint drhd_count = 0;\r\nstruct dmar_res_callback cb = {\r\n.print_entry = true,\r\n.ignore_unhandled = true,\r\n.arg[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &drhd_count,\r\n.cb[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &dmar_parse_one_drhd,\r\n.cb[ACPI_DMAR_TYPE_RESERVED_MEMORY] = &dmar_parse_one_rmrr,\r\n.cb[ACPI_DMAR_TYPE_ROOT_ATS] = &dmar_parse_one_atsr,\r\n.cb[ACPI_DMAR_TYPE_HARDWARE_AFFINITY] = &dmar_parse_one_rhsa,\r\n.cb[ACPI_DMAR_TYPE_NAMESPACE] = &dmar_parse_one_andd,\r\n};\r\ndmar_table_detect();\r\ndmar_tbl = tboot_get_dmar_table(dmar_tbl);\r\ndmar = (struct acpi_table_dmar *)dmar_tbl;\r\nif (!dmar)\r\nreturn -ENODEV;\r\nif (dmar->width < PAGE_SHIFT - 1) {\r\npr_warn("Invalid DMAR haw\n");\r\nreturn -EINVAL;\r\n}\r\npr_info("Host address width %d\n", dmar->width + 1);\r\nret = dmar_walk_dmar_table(dmar, &cb);\r\nif (ret == 0 && drhd_count == 0)\r\npr_warn(FW_BUG "No DRHD structure found in DMAR table\n");\r\nreturn ret;\r\n}\r\nstatic int dmar_pci_device_match(struct dmar_dev_scope devices[],\r\nint cnt, struct pci_dev *dev)\r\n{\r\nint index;\r\nstruct device *tmp;\r\nwhile (dev) {\r\nfor_each_active_dev_scope(devices, cnt, index, tmp)\r\nif (dev_is_pci(tmp) && dev == to_pci_dev(tmp))\r\nreturn 1;\r\ndev = dev->bus->self;\r\n}\r\nreturn 0;\r\n}\r\nstruct dmar_drhd_unit *\r\ndmar_find_matched_drhd_unit(struct pci_dev *dev)\r\n{\r\nstruct dmar_drhd_unit *dmaru;\r\nstruct acpi_dmar_hardware_unit *drhd;\r\ndev = pci_physfn(dev);\r\nrcu_read_lock();\r\nfor_each_drhd_unit(dmaru) {\r\ndrhd = container_of(dmaru->hdr,\r\nstruct acpi_dmar_hardware_unit,\r\nheader);\r\nif (dmaru->include_all &&\r\ndrhd->segment == pci_domain_nr(dev->bus))\r\ngoto out;\r\nif (dmar_pci_device_match(dmaru->devices,\r\ndmaru->devices_cnt, dev))\r\ngoto out;\r\n}\r\ndmaru = NULL;\r\nout:\r\nrcu_read_unlock();\r\nreturn dmaru;\r\n}\r\nstatic void __init dmar_acpi_insert_dev_scope(u8 device_number,\r\nstruct acpi_device *adev)\r\n{\r\nstruct dmar_drhd_unit *dmaru;\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nstruct acpi_dmar_device_scope *scope;\r\nstruct device *tmp;\r\nint i;\r\nstruct acpi_dmar_pci_path *path;\r\nfor_each_drhd_unit(dmaru) {\r\ndrhd = container_of(dmaru->hdr,\r\nstruct acpi_dmar_hardware_unit,\r\nheader);\r\nfor (scope = (void *)(drhd + 1);\r\n(unsigned long)scope < ((unsigned long)drhd) + drhd->header.length;\r\nscope = ((void *)scope) + scope->length) {\r\nif (scope->entry_type != ACPI_DMAR_SCOPE_TYPE_NAMESPACE)\r\ncontinue;\r\nif (scope->enumeration_id != device_number)\r\ncontinue;\r\npath = (void *)(scope + 1);\r\npr_info("ACPI device \"%s\" under DMAR at %llx as %02x:%02x.%d\n",\r\ndev_name(&adev->dev), dmaru->reg_base_addr,\r\nscope->bus, path->device, path->function);\r\nfor_each_dev_scope(dmaru->devices, dmaru->devices_cnt, i, tmp)\r\nif (tmp == NULL) {\r\ndmaru->devices[i].bus = scope->bus;\r\ndmaru->devices[i].devfn = PCI_DEVFN(path->device,\r\npath->function);\r\nrcu_assign_pointer(dmaru->devices[i].dev,\r\nget_device(&adev->dev));\r\nreturn;\r\n}\r\nBUG_ON(i >= dmaru->devices_cnt);\r\n}\r\n}\r\npr_warn("No IOMMU scope found for ANDD enumeration ID %d (%s)\n",\r\ndevice_number, dev_name(&adev->dev));\r\n}\r\nstatic int __init dmar_acpi_dev_scope_init(void)\r\n{\r\nstruct acpi_dmar_andd *andd;\r\nif (dmar_tbl == NULL)\r\nreturn -ENODEV;\r\nfor (andd = (void *)dmar_tbl + sizeof(struct acpi_table_dmar);\r\n((unsigned long)andd) < ((unsigned long)dmar_tbl) + dmar_tbl->length;\r\nandd = ((void *)andd) + andd->header.length) {\r\nif (andd->header.type == ACPI_DMAR_TYPE_NAMESPACE) {\r\nacpi_handle h;\r\nstruct acpi_device *adev;\r\nif (!ACPI_SUCCESS(acpi_get_handle(ACPI_ROOT_OBJECT,\r\nandd->device_name,\r\n&h))) {\r\npr_err("Failed to find handle for ACPI object %s\n",\r\nandd->device_name);\r\ncontinue;\r\n}\r\nif (acpi_bus_get_device(h, &adev)) {\r\npr_err("Failed to get device for ACPI object %s\n",\r\nandd->device_name);\r\ncontinue;\r\n}\r\ndmar_acpi_insert_dev_scope(andd->device_number, adev);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint __init dmar_dev_scope_init(void)\r\n{\r\nstruct pci_dev *dev = NULL;\r\nstruct dmar_pci_notify_info *info;\r\nif (dmar_dev_scope_status != 1)\r\nreturn dmar_dev_scope_status;\r\nif (list_empty(&dmar_drhd_units)) {\r\ndmar_dev_scope_status = -ENODEV;\r\n} else {\r\ndmar_dev_scope_status = 0;\r\ndmar_acpi_dev_scope_init();\r\nfor_each_pci_dev(dev) {\r\nif (dev->is_virtfn)\r\ncontinue;\r\ninfo = dmar_alloc_pci_notify_info(dev,\r\nBUS_NOTIFY_ADD_DEVICE);\r\nif (!info) {\r\nreturn dmar_dev_scope_status;\r\n} else {\r\ndmar_pci_bus_add_dev(info);\r\ndmar_free_pci_notify_info(info);\r\n}\r\n}\r\nbus_register_notifier(&pci_bus_type, &dmar_pci_bus_nb);\r\n}\r\nreturn dmar_dev_scope_status;\r\n}\r\nint __init dmar_table_init(void)\r\n{\r\nstatic int dmar_table_initialized;\r\nint ret;\r\nif (dmar_table_initialized == 0) {\r\nret = parse_dmar_table();\r\nif (ret < 0) {\r\nif (ret != -ENODEV)\r\npr_info("Parse DMAR table failure.\n");\r\n} else if (list_empty(&dmar_drhd_units)) {\r\npr_info("No DMAR devices found\n");\r\nret = -ENODEV;\r\n}\r\nif (ret < 0)\r\ndmar_table_initialized = ret;\r\nelse\r\ndmar_table_initialized = 1;\r\n}\r\nreturn dmar_table_initialized < 0 ? dmar_table_initialized : 0;\r\n}\r\nstatic void warn_invalid_dmar(u64 addr, const char *message)\r\n{\r\nWARN_TAINT_ONCE(\r\n1, TAINT_FIRMWARE_WORKAROUND,\r\n"Your BIOS is broken; DMAR reported at address %llx%s!\n"\r\n"BIOS vendor: %s; Ver: %s; Product Version: %s\n",\r\naddr, message,\r\ndmi_get_system_info(DMI_BIOS_VENDOR),\r\ndmi_get_system_info(DMI_BIOS_VERSION),\r\ndmi_get_system_info(DMI_PRODUCT_VERSION));\r\n}\r\nstatic int __ref\r\ndmar_validate_one_drhd(struct acpi_dmar_header *entry, void *arg)\r\n{\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nvoid __iomem *addr;\r\nu64 cap, ecap;\r\ndrhd = (void *)entry;\r\nif (!drhd->address) {\r\nwarn_invalid_dmar(0, "");\r\nreturn -EINVAL;\r\n}\r\nif (arg)\r\naddr = ioremap(drhd->address, VTD_PAGE_SIZE);\r\nelse\r\naddr = early_ioremap(drhd->address, VTD_PAGE_SIZE);\r\nif (!addr) {\r\npr_warn("Can't validate DRHD address: %llx\n", drhd->address);\r\nreturn -EINVAL;\r\n}\r\ncap = dmar_readq(addr + DMAR_CAP_REG);\r\necap = dmar_readq(addr + DMAR_ECAP_REG);\r\nif (arg)\r\niounmap(addr);\r\nelse\r\nearly_iounmap(addr, VTD_PAGE_SIZE);\r\nif (cap == (uint64_t)-1 && ecap == (uint64_t)-1) {\r\nwarn_invalid_dmar(drhd->address, " returns all ones");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint __init detect_intel_iommu(void)\r\n{\r\nint ret;\r\nstruct dmar_res_callback validate_drhd_cb = {\r\n.cb[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &dmar_validate_one_drhd,\r\n.ignore_unhandled = true,\r\n};\r\ndown_write(&dmar_global_lock);\r\nret = dmar_table_detect();\r\nif (ret)\r\nret = !dmar_walk_dmar_table((struct acpi_table_dmar *)dmar_tbl,\r\n&validate_drhd_cb);\r\nif (ret && !no_iommu && !iommu_detected && !dmar_disabled) {\r\niommu_detected = 1;\r\npci_request_acs();\r\n}\r\n#ifdef CONFIG_X86\r\nif (ret)\r\nx86_init.iommu.iommu_init = intel_iommu_init;\r\n#endif\r\nearly_acpi_os_unmap_memory((void __iomem *)dmar_tbl, dmar_tbl_size);\r\ndmar_tbl = NULL;\r\nup_write(&dmar_global_lock);\r\nreturn ret ? 1 : -ENODEV;\r\n}\r\nstatic void unmap_iommu(struct intel_iommu *iommu)\r\n{\r\niounmap(iommu->reg);\r\nrelease_mem_region(iommu->reg_phys, iommu->reg_size);\r\n}\r\nstatic int map_iommu(struct intel_iommu *iommu, u64 phys_addr)\r\n{\r\nint map_size, err=0;\r\niommu->reg_phys = phys_addr;\r\niommu->reg_size = VTD_PAGE_SIZE;\r\nif (!request_mem_region(iommu->reg_phys, iommu->reg_size, iommu->name)) {\r\npr_err("Can't reserve memory\n");\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\niommu->reg = ioremap(iommu->reg_phys, iommu->reg_size);\r\nif (!iommu->reg) {\r\npr_err("Can't map the region\n");\r\nerr = -ENOMEM;\r\ngoto release;\r\n}\r\niommu->cap = dmar_readq(iommu->reg + DMAR_CAP_REG);\r\niommu->ecap = dmar_readq(iommu->reg + DMAR_ECAP_REG);\r\nif (iommu->cap == (uint64_t)-1 && iommu->ecap == (uint64_t)-1) {\r\nerr = -EINVAL;\r\nwarn_invalid_dmar(phys_addr, " returns all ones");\r\ngoto unmap;\r\n}\r\nmap_size = max_t(int, ecap_max_iotlb_offset(iommu->ecap),\r\ncap_max_fault_reg_offset(iommu->cap));\r\nmap_size = VTD_PAGE_ALIGN(map_size);\r\nif (map_size > iommu->reg_size) {\r\niounmap(iommu->reg);\r\nrelease_mem_region(iommu->reg_phys, iommu->reg_size);\r\niommu->reg_size = map_size;\r\nif (!request_mem_region(iommu->reg_phys, iommu->reg_size,\r\niommu->name)) {\r\npr_err("Can't reserve memory\n");\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\niommu->reg = ioremap(iommu->reg_phys, iommu->reg_size);\r\nif (!iommu->reg) {\r\npr_err("Can't map the region\n");\r\nerr = -ENOMEM;\r\ngoto release;\r\n}\r\n}\r\nerr = 0;\r\ngoto out;\r\nunmap:\r\niounmap(iommu->reg);\r\nrelease:\r\nrelease_mem_region(iommu->reg_phys, iommu->reg_size);\r\nout:\r\nreturn err;\r\n}\r\nstatic int dmar_alloc_seq_id(struct intel_iommu *iommu)\r\n{\r\niommu->seq_id = find_first_zero_bit(dmar_seq_ids,\r\nDMAR_UNITS_SUPPORTED);\r\nif (iommu->seq_id >= DMAR_UNITS_SUPPORTED) {\r\niommu->seq_id = -1;\r\n} else {\r\nset_bit(iommu->seq_id, dmar_seq_ids);\r\nsprintf(iommu->name, "dmar%d", iommu->seq_id);\r\n}\r\nreturn iommu->seq_id;\r\n}\r\nstatic void dmar_free_seq_id(struct intel_iommu *iommu)\r\n{\r\nif (iommu->seq_id >= 0) {\r\nclear_bit(iommu->seq_id, dmar_seq_ids);\r\niommu->seq_id = -1;\r\n}\r\n}\r\nstatic int alloc_iommu(struct dmar_drhd_unit *drhd)\r\n{\r\nstruct intel_iommu *iommu;\r\nu32 ver, sts;\r\nint agaw = 0;\r\nint msagaw = 0;\r\nint err;\r\nif (!drhd->reg_base_addr) {\r\nwarn_invalid_dmar(0, "");\r\nreturn -EINVAL;\r\n}\r\niommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\r\nif (!iommu)\r\nreturn -ENOMEM;\r\nif (dmar_alloc_seq_id(iommu) < 0) {\r\npr_err("Failed to allocate seq_id\n");\r\nerr = -ENOSPC;\r\ngoto error;\r\n}\r\nerr = map_iommu(iommu, drhd->reg_base_addr);\r\nif (err) {\r\npr_err("Failed to map %s\n", iommu->name);\r\ngoto error_free_seq_id;\r\n}\r\nerr = -EINVAL;\r\nagaw = iommu_calculate_agaw(iommu);\r\nif (agaw < 0) {\r\npr_err("Cannot get a valid agaw for iommu (seq_id = %d)\n",\r\niommu->seq_id);\r\ngoto err_unmap;\r\n}\r\nmsagaw = iommu_calculate_max_sagaw(iommu);\r\nif (msagaw < 0) {\r\npr_err("Cannot get a valid max agaw for iommu (seq_id = %d)\n",\r\niommu->seq_id);\r\ngoto err_unmap;\r\n}\r\niommu->agaw = agaw;\r\niommu->msagaw = msagaw;\r\niommu->segment = drhd->segment;\r\niommu->node = -1;\r\nver = readl(iommu->reg + DMAR_VER_REG);\r\npr_info("%s: reg_base_addr %llx ver %d:%d cap %llx ecap %llx\n",\r\niommu->name,\r\n(unsigned long long)drhd->reg_base_addr,\r\nDMAR_VER_MAJOR(ver), DMAR_VER_MINOR(ver),\r\n(unsigned long long)iommu->cap,\r\n(unsigned long long)iommu->ecap);\r\nsts = readl(iommu->reg + DMAR_GSTS_REG);\r\nif (sts & DMA_GSTS_IRES)\r\niommu->gcmd |= DMA_GCMD_IRE;\r\nif (sts & DMA_GSTS_TES)\r\niommu->gcmd |= DMA_GCMD_TE;\r\nif (sts & DMA_GSTS_QIES)\r\niommu->gcmd |= DMA_GCMD_QIE;\r\nraw_spin_lock_init(&iommu->register_lock);\r\nif (intel_iommu_enabled) {\r\niommu->iommu_dev = iommu_device_create(NULL, iommu,\r\nintel_iommu_groups,\r\n"%s", iommu->name);\r\nif (IS_ERR(iommu->iommu_dev)) {\r\nerr = PTR_ERR(iommu->iommu_dev);\r\ngoto err_unmap;\r\n}\r\n}\r\ndrhd->iommu = iommu;\r\nreturn 0;\r\nerr_unmap:\r\nunmap_iommu(iommu);\r\nerror_free_seq_id:\r\ndmar_free_seq_id(iommu);\r\nerror:\r\nkfree(iommu);\r\nreturn err;\r\n}\r\nstatic void free_iommu(struct intel_iommu *iommu)\r\n{\r\niommu_device_destroy(iommu->iommu_dev);\r\nif (iommu->irq) {\r\nif (iommu->pr_irq) {\r\nfree_irq(iommu->pr_irq, iommu);\r\ndmar_free_hwirq(iommu->pr_irq);\r\niommu->pr_irq = 0;\r\n}\r\nfree_irq(iommu->irq, iommu);\r\ndmar_free_hwirq(iommu->irq);\r\niommu->irq = 0;\r\n}\r\nif (iommu->qi) {\r\nfree_page((unsigned long)iommu->qi->desc);\r\nkfree(iommu->qi->desc_status);\r\nkfree(iommu->qi);\r\n}\r\nif (iommu->reg)\r\nunmap_iommu(iommu);\r\ndmar_free_seq_id(iommu);\r\nkfree(iommu);\r\n}\r\nstatic inline void reclaim_free_desc(struct q_inval *qi)\r\n{\r\nwhile (qi->desc_status[qi->free_tail] == QI_DONE ||\r\nqi->desc_status[qi->free_tail] == QI_ABORT) {\r\nqi->desc_status[qi->free_tail] = QI_FREE;\r\nqi->free_tail = (qi->free_tail + 1) % QI_LENGTH;\r\nqi->free_cnt++;\r\n}\r\n}\r\nstatic int qi_check_fault(struct intel_iommu *iommu, int index)\r\n{\r\nu32 fault;\r\nint head, tail;\r\nstruct q_inval *qi = iommu->qi;\r\nint wait_index = (index + 1) % QI_LENGTH;\r\nif (qi->desc_status[wait_index] == QI_ABORT)\r\nreturn -EAGAIN;\r\nfault = readl(iommu->reg + DMAR_FSTS_REG);\r\nif (fault & DMA_FSTS_IQE) {\r\nhead = readl(iommu->reg + DMAR_IQH_REG);\r\nif ((head >> DMAR_IQ_SHIFT) == index) {\r\npr_err("VT-d detected invalid descriptor: "\r\n"low=%llx, high=%llx\n",\r\n(unsigned long long)qi->desc[index].low,\r\n(unsigned long long)qi->desc[index].high);\r\nmemcpy(&qi->desc[index], &qi->desc[wait_index],\r\nsizeof(struct qi_desc));\r\n__iommu_flush_cache(iommu, &qi->desc[index],\r\nsizeof(struct qi_desc));\r\nwritel(DMA_FSTS_IQE, iommu->reg + DMAR_FSTS_REG);\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (fault & DMA_FSTS_ITE) {\r\nhead = readl(iommu->reg + DMAR_IQH_REG);\r\nhead = ((head >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;\r\nhead |= 1;\r\ntail = readl(iommu->reg + DMAR_IQT_REG);\r\ntail = ((tail >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;\r\nwritel(DMA_FSTS_ITE, iommu->reg + DMAR_FSTS_REG);\r\ndo {\r\nif (qi->desc_status[head] == QI_IN_USE)\r\nqi->desc_status[head] = QI_ABORT;\r\nhead = (head - 2 + QI_LENGTH) % QI_LENGTH;\r\n} while (head != tail);\r\nif (qi->desc_status[wait_index] == QI_ABORT)\r\nreturn -EAGAIN;\r\n}\r\nif (fault & DMA_FSTS_ICE)\r\nwritel(DMA_FSTS_ICE, iommu->reg + DMAR_FSTS_REG);\r\nreturn 0;\r\n}\r\nint qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu)\r\n{\r\nint rc;\r\nstruct q_inval *qi = iommu->qi;\r\nstruct qi_desc *hw, wait_desc;\r\nint wait_index, index;\r\nunsigned long flags;\r\nif (!qi)\r\nreturn 0;\r\nhw = qi->desc;\r\nrestart:\r\nrc = 0;\r\nraw_spin_lock_irqsave(&qi->q_lock, flags);\r\nwhile (qi->free_cnt < 3) {\r\nraw_spin_unlock_irqrestore(&qi->q_lock, flags);\r\ncpu_relax();\r\nraw_spin_lock_irqsave(&qi->q_lock, flags);\r\n}\r\nindex = qi->free_head;\r\nwait_index = (index + 1) % QI_LENGTH;\r\nqi->desc_status[index] = qi->desc_status[wait_index] = QI_IN_USE;\r\nhw[index] = *desc;\r\nwait_desc.low = QI_IWD_STATUS_DATA(QI_DONE) |\r\nQI_IWD_STATUS_WRITE | QI_IWD_TYPE;\r\nwait_desc.high = virt_to_phys(&qi->desc_status[wait_index]);\r\nhw[wait_index] = wait_desc;\r\n__iommu_flush_cache(iommu, &hw[index], sizeof(struct qi_desc));\r\n__iommu_flush_cache(iommu, &hw[wait_index], sizeof(struct qi_desc));\r\nqi->free_head = (qi->free_head + 2) % QI_LENGTH;\r\nqi->free_cnt -= 2;\r\nwritel(qi->free_head << DMAR_IQ_SHIFT, iommu->reg + DMAR_IQT_REG);\r\nwhile (qi->desc_status[wait_index] != QI_DONE) {\r\nrc = qi_check_fault(iommu, index);\r\nif (rc)\r\nbreak;\r\nraw_spin_unlock(&qi->q_lock);\r\ncpu_relax();\r\nraw_spin_lock(&qi->q_lock);\r\n}\r\nqi->desc_status[index] = QI_DONE;\r\nreclaim_free_desc(qi);\r\nraw_spin_unlock_irqrestore(&qi->q_lock, flags);\r\nif (rc == -EAGAIN)\r\ngoto restart;\r\nreturn rc;\r\n}\r\nvoid qi_global_iec(struct intel_iommu *iommu)\r\n{\r\nstruct qi_desc desc;\r\ndesc.low = QI_IEC_TYPE;\r\ndesc.high = 0;\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid qi_flush_context(struct intel_iommu *iommu, u16 did, u16 sid, u8 fm,\r\nu64 type)\r\n{\r\nstruct qi_desc desc;\r\ndesc.low = QI_CC_FM(fm) | QI_CC_SID(sid) | QI_CC_DID(did)\r\n| QI_CC_GRAN(type) | QI_CC_TYPE;\r\ndesc.high = 0;\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,\r\nunsigned int size_order, u64 type)\r\n{\r\nu8 dw = 0, dr = 0;\r\nstruct qi_desc desc;\r\nint ih = 0;\r\nif (cap_write_drain(iommu->cap))\r\ndw = 1;\r\nif (cap_read_drain(iommu->cap))\r\ndr = 1;\r\ndesc.low = QI_IOTLB_DID(did) | QI_IOTLB_DR(dr) | QI_IOTLB_DW(dw)\r\n| QI_IOTLB_GRAN(type) | QI_IOTLB_TYPE;\r\ndesc.high = QI_IOTLB_ADDR(addr) | QI_IOTLB_IH(ih)\r\n| QI_IOTLB_AM(size_order);\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 qdep,\r\nu64 addr, unsigned mask)\r\n{\r\nstruct qi_desc desc;\r\nif (mask) {\r\nBUG_ON(addr & ((1 << (VTD_PAGE_SHIFT + mask)) - 1));\r\naddr |= (1 << (VTD_PAGE_SHIFT + mask - 1)) - 1;\r\ndesc.high = QI_DEV_IOTLB_ADDR(addr) | QI_DEV_IOTLB_SIZE;\r\n} else\r\ndesc.high = QI_DEV_IOTLB_ADDR(addr);\r\nif (qdep >= QI_DEV_IOTLB_MAX_INVS)\r\nqdep = 0;\r\ndesc.low = QI_DEV_IOTLB_SID(sid) | QI_DEV_IOTLB_QDEP(qdep) |\r\nQI_DIOTLB_TYPE;\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid dmar_disable_qi(struct intel_iommu *iommu)\r\n{\r\nunsigned long flags;\r\nu32 sts;\r\ncycles_t start_time = get_cycles();\r\nif (!ecap_qis(iommu->ecap))\r\nreturn;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flags);\r\nsts = readl(iommu->reg + DMAR_GSTS_REG);\r\nif (!(sts & DMA_GSTS_QIES))\r\ngoto end;\r\nwhile ((readl(iommu->reg + DMAR_IQT_REG) !=\r\nreadl(iommu->reg + DMAR_IQH_REG)) &&\r\n(DMAR_OPERATION_TIMEOUT > (get_cycles() - start_time)))\r\ncpu_relax();\r\niommu->gcmd &= ~DMA_GCMD_QIE;\r\nwritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\r\nIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, readl,\r\n!(sts & DMA_GSTS_QIES), sts);\r\nend:\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flags);\r\n}\r\nstatic void __dmar_enable_qi(struct intel_iommu *iommu)\r\n{\r\nu32 sts;\r\nunsigned long flags;\r\nstruct q_inval *qi = iommu->qi;\r\nqi->free_head = qi->free_tail = 0;\r\nqi->free_cnt = QI_LENGTH;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flags);\r\nwritel(0, iommu->reg + DMAR_IQT_REG);\r\ndmar_writeq(iommu->reg + DMAR_IQA_REG, virt_to_phys(qi->desc));\r\niommu->gcmd |= DMA_GCMD_QIE;\r\nwritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\r\nIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, readl, (sts & DMA_GSTS_QIES), sts);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flags);\r\n}\r\nint dmar_enable_qi(struct intel_iommu *iommu)\r\n{\r\nstruct q_inval *qi;\r\nstruct page *desc_page;\r\nif (!ecap_qis(iommu->ecap))\r\nreturn -ENOENT;\r\nif (iommu->qi)\r\nreturn 0;\r\niommu->qi = kmalloc(sizeof(*qi), GFP_ATOMIC);\r\nif (!iommu->qi)\r\nreturn -ENOMEM;\r\nqi = iommu->qi;\r\ndesc_page = alloc_pages_node(iommu->node, GFP_ATOMIC | __GFP_ZERO, 0);\r\nif (!desc_page) {\r\nkfree(qi);\r\niommu->qi = NULL;\r\nreturn -ENOMEM;\r\n}\r\nqi->desc = page_address(desc_page);\r\nqi->desc_status = kzalloc(QI_LENGTH * sizeof(int), GFP_ATOMIC);\r\nif (!qi->desc_status) {\r\nfree_page((unsigned long) qi->desc);\r\nkfree(qi);\r\niommu->qi = NULL;\r\nreturn -ENOMEM;\r\n}\r\nraw_spin_lock_init(&qi->q_lock);\r\n__dmar_enable_qi(iommu);\r\nreturn 0;\r\n}\r\nstatic const char *dmar_get_fault_reason(u8 fault_reason, int *fault_type)\r\n{\r\nif (fault_reason >= 0x20 && (fault_reason - 0x20 <\r\nARRAY_SIZE(irq_remap_fault_reasons))) {\r\n*fault_type = INTR_REMAP;\r\nreturn irq_remap_fault_reasons[fault_reason - 0x20];\r\n} else if (fault_reason < ARRAY_SIZE(dma_remap_fault_reasons)) {\r\n*fault_type = DMA_REMAP;\r\nreturn dma_remap_fault_reasons[fault_reason];\r\n} else {\r\n*fault_type = UNKNOWN;\r\nreturn "Unknown";\r\n}\r\n}\r\nstatic inline int dmar_msi_reg(struct intel_iommu *iommu, int irq)\r\n{\r\nif (iommu->irq == irq)\r\nreturn DMAR_FECTL_REG;\r\nelse if (iommu->pr_irq == irq)\r\nreturn DMAR_PECTL_REG;\r\nelse\r\nBUG();\r\n}\r\nvoid dmar_msi_unmask(struct irq_data *data)\r\n{\r\nstruct intel_iommu *iommu = irq_data_get_irq_handler_data(data);\r\nint reg = dmar_msi_reg(iommu, data->irq);\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nwritel(0, iommu->reg + reg);\r\nreadl(iommu->reg + reg);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nvoid dmar_msi_mask(struct irq_data *data)\r\n{\r\nstruct intel_iommu *iommu = irq_data_get_irq_handler_data(data);\r\nint reg = dmar_msi_reg(iommu, data->irq);\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nwritel(DMA_FECTL_IM, iommu->reg + reg);\r\nreadl(iommu->reg + reg);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nvoid dmar_msi_write(int irq, struct msi_msg *msg)\r\n{\r\nstruct intel_iommu *iommu = irq_get_handler_data(irq);\r\nint reg = dmar_msi_reg(iommu, irq);\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nwritel(msg->data, iommu->reg + reg + 4);\r\nwritel(msg->address_lo, iommu->reg + reg + 8);\r\nwritel(msg->address_hi, iommu->reg + reg + 12);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nvoid dmar_msi_read(int irq, struct msi_msg *msg)\r\n{\r\nstruct intel_iommu *iommu = irq_get_handler_data(irq);\r\nint reg = dmar_msi_reg(iommu, irq);\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nmsg->data = readl(iommu->reg + reg + 4);\r\nmsg->address_lo = readl(iommu->reg + reg + 8);\r\nmsg->address_hi = readl(iommu->reg + reg + 12);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nstatic int dmar_fault_do_one(struct intel_iommu *iommu, int type,\r\nu8 fault_reason, u16 source_id, unsigned long long addr)\r\n{\r\nconst char *reason;\r\nint fault_type;\r\nreason = dmar_get_fault_reason(fault_reason, &fault_type);\r\nif (fault_type == INTR_REMAP)\r\npr_err("INTR-REMAP: Request device [[%02x:%02x.%d] "\r\n"fault index %llx\n"\r\n"INTR-REMAP:[fault reason %02d] %s\n",\r\n(source_id >> 8), PCI_SLOT(source_id & 0xFF),\r\nPCI_FUNC(source_id & 0xFF), addr >> 48,\r\nfault_reason, reason);\r\nelse\r\npr_err("DMAR:[%s] Request device [%02x:%02x.%d] "\r\n"fault addr %llx \n"\r\n"DMAR:[fault reason %02d] %s\n",\r\n(type ? "DMA Read" : "DMA Write"),\r\n(source_id >> 8), PCI_SLOT(source_id & 0xFF),\r\nPCI_FUNC(source_id & 0xFF), addr, fault_reason, reason);\r\nreturn 0;\r\n}\r\nirqreturn_t dmar_fault(int irq, void *dev_id)\r\n{\r\nstruct intel_iommu *iommu = dev_id;\r\nint reg, fault_index;\r\nu32 fault_status;\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nfault_status = readl(iommu->reg + DMAR_FSTS_REG);\r\nif (fault_status)\r\npr_err("DRHD: handling fault status reg %x\n", fault_status);\r\nif (!(fault_status & DMA_FSTS_PPF))\r\ngoto unlock_exit;\r\nfault_index = dma_fsts_fault_record_index(fault_status);\r\nreg = cap_fault_reg_offset(iommu->cap);\r\nwhile (1) {\r\nu8 fault_reason;\r\nu16 source_id;\r\nu64 guest_addr;\r\nint type;\r\nu32 data;\r\ndata = readl(iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN + 12);\r\nif (!(data & DMA_FRCD_F))\r\nbreak;\r\nfault_reason = dma_frcd_fault_reason(data);\r\ntype = dma_frcd_type(data);\r\ndata = readl(iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN + 8);\r\nsource_id = dma_frcd_source_id(data);\r\nguest_addr = dmar_readq(iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN);\r\nguest_addr = dma_frcd_page_addr(guest_addr);\r\nwritel(DMA_FRCD_F, iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN + 12);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\ndmar_fault_do_one(iommu, type, fault_reason,\r\nsource_id, guest_addr);\r\nfault_index++;\r\nif (fault_index >= cap_num_fault_regs(iommu->cap))\r\nfault_index = 0;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\n}\r\nwritel(DMA_FSTS_PFO | DMA_FSTS_PPF, iommu->reg + DMAR_FSTS_REG);\r\nunlock_exit:\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\nreturn IRQ_HANDLED;\r\n}\r\nint dmar_set_interrupt(struct intel_iommu *iommu)\r\n{\r\nint irq, ret;\r\nif (iommu->irq)\r\nreturn 0;\r\nirq = dmar_alloc_hwirq(iommu->seq_id, iommu->node, iommu);\r\nif (irq > 0) {\r\niommu->irq = irq;\r\n} else {\r\npr_err("No free IRQ vectors\n");\r\nreturn -EINVAL;\r\n}\r\nret = request_irq(irq, dmar_fault, IRQF_NO_THREAD, iommu->name, iommu);\r\nif (ret)\r\npr_err("Can't request irq\n");\r\nreturn ret;\r\n}\r\nint __init enable_drhd_fault_handling(void)\r\n{\r\nstruct dmar_drhd_unit *drhd;\r\nstruct intel_iommu *iommu;\r\nfor_each_iommu(iommu, drhd) {\r\nu32 fault_status;\r\nint ret = dmar_set_interrupt(iommu);\r\nif (ret) {\r\npr_err("DRHD %Lx: failed to enable fault, interrupt, ret %d\n",\r\n(unsigned long long)drhd->reg_base_addr, ret);\r\nreturn -1;\r\n}\r\ndmar_fault(iommu->irq, iommu);\r\nfault_status = readl(iommu->reg + DMAR_FSTS_REG);\r\nwritel(fault_status, iommu->reg + DMAR_FSTS_REG);\r\n}\r\nreturn 0;\r\n}\r\nint dmar_reenable_qi(struct intel_iommu *iommu)\r\n{\r\nif (!ecap_qis(iommu->ecap))\r\nreturn -ENOENT;\r\nif (!iommu->qi)\r\nreturn -ENOENT;\r\ndmar_disable_qi(iommu);\r\n__dmar_enable_qi(iommu);\r\nreturn 0;\r\n}\r\nint __init dmar_ir_support(void)\r\n{\r\nstruct acpi_table_dmar *dmar;\r\ndmar = (struct acpi_table_dmar *)dmar_tbl;\r\nif (!dmar)\r\nreturn 0;\r\nreturn dmar->flags & 0x1;\r\n}\r\nstatic inline bool dmar_in_use(void)\r\n{\r\nreturn irq_remapping_enabled || intel_iommu_enabled;\r\n}\r\nstatic int __init dmar_free_unused_resources(void)\r\n{\r\nstruct dmar_drhd_unit *dmaru, *dmaru_n;\r\nif (dmar_in_use())\r\nreturn 0;\r\nif (dmar_dev_scope_status != 1 && !list_empty(&dmar_drhd_units))\r\nbus_unregister_notifier(&pci_bus_type, &dmar_pci_bus_nb);\r\ndown_write(&dmar_global_lock);\r\nlist_for_each_entry_safe(dmaru, dmaru_n, &dmar_drhd_units, list) {\r\nlist_del(&dmaru->list);\r\ndmar_free_drhd(dmaru);\r\n}\r\nup_write(&dmar_global_lock);\r\nreturn 0;\r\n}\r\nstatic inline bool dmar_detect_dsm(acpi_handle handle, int func)\r\n{\r\nreturn acpi_check_dsm(handle, dmar_hp_uuid, DMAR_DSM_REV_ID, 1 << func);\r\n}\r\nstatic int dmar_walk_dsm_resource(acpi_handle handle, int func,\r\ndmar_res_handler_t handler, void *arg)\r\n{\r\nint ret = -ENODEV;\r\nunion acpi_object *obj;\r\nstruct acpi_dmar_header *start;\r\nstruct dmar_res_callback callback;\r\nstatic int res_type[] = {\r\n[DMAR_DSM_FUNC_DRHD] = ACPI_DMAR_TYPE_HARDWARE_UNIT,\r\n[DMAR_DSM_FUNC_ATSR] = ACPI_DMAR_TYPE_ROOT_ATS,\r\n[DMAR_DSM_FUNC_RHSA] = ACPI_DMAR_TYPE_HARDWARE_AFFINITY,\r\n};\r\nif (!dmar_detect_dsm(handle, func))\r\nreturn 0;\r\nobj = acpi_evaluate_dsm_typed(handle, dmar_hp_uuid, DMAR_DSM_REV_ID,\r\nfunc, NULL, ACPI_TYPE_BUFFER);\r\nif (!obj)\r\nreturn -ENODEV;\r\nmemset(&callback, 0, sizeof(callback));\r\ncallback.cb[res_type[func]] = handler;\r\ncallback.arg[res_type[func]] = arg;\r\nstart = (struct acpi_dmar_header *)obj->buffer.pointer;\r\nret = dmar_walk_remapping_entries(start, obj->buffer.length, &callback);\r\nACPI_FREE(obj);\r\nreturn ret;\r\n}\r\nstatic int dmar_hp_add_drhd(struct acpi_dmar_header *header, void *arg)\r\n{\r\nint ret;\r\nstruct dmar_drhd_unit *dmaru;\r\ndmaru = dmar_find_dmaru((struct acpi_dmar_hardware_unit *)header);\r\nif (!dmaru)\r\nreturn -ENODEV;\r\nret = dmar_ir_hotplug(dmaru, true);\r\nif (ret == 0)\r\nret = dmar_iommu_hotplug(dmaru, true);\r\nreturn ret;\r\n}\r\nstatic int dmar_hp_remove_drhd(struct acpi_dmar_header *header, void *arg)\r\n{\r\nint i, ret;\r\nstruct device *dev;\r\nstruct dmar_drhd_unit *dmaru;\r\ndmaru = dmar_find_dmaru((struct acpi_dmar_hardware_unit *)header);\r\nif (!dmaru)\r\nreturn 0;\r\nif (!dmaru->include_all && dmaru->devices && dmaru->devices_cnt)\r\nfor_each_active_dev_scope(dmaru->devices,\r\ndmaru->devices_cnt, i, dev)\r\nreturn -EBUSY;\r\nret = dmar_ir_hotplug(dmaru, false);\r\nif (ret == 0)\r\nret = dmar_iommu_hotplug(dmaru, false);\r\nreturn ret;\r\n}\r\nstatic int dmar_hp_release_drhd(struct acpi_dmar_header *header, void *arg)\r\n{\r\nstruct dmar_drhd_unit *dmaru;\r\ndmaru = dmar_find_dmaru((struct acpi_dmar_hardware_unit *)header);\r\nif (dmaru) {\r\nlist_del_rcu(&dmaru->list);\r\nsynchronize_rcu();\r\ndmar_free_drhd(dmaru);\r\n}\r\nreturn 0;\r\n}\r\nstatic int dmar_hotplug_insert(acpi_handle handle)\r\n{\r\nint ret;\r\nint drhd_count = 0;\r\nret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_validate_one_drhd, (void *)1);\r\nif (ret)\r\ngoto out;\r\nret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_parse_one_drhd, (void *)&drhd_count);\r\nif (ret == 0 && drhd_count == 0) {\r\npr_warn(FW_BUG "No DRHD structures in buffer returned by _DSM method\n");\r\ngoto out;\r\n} else if (ret) {\r\ngoto release_drhd;\r\n}\r\nret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_RHSA,\r\n&dmar_parse_one_rhsa, NULL);\r\nif (ret)\r\ngoto release_drhd;\r\nret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\r\n&dmar_parse_one_atsr, NULL);\r\nif (ret)\r\ngoto release_atsr;\r\nret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_hp_add_drhd, NULL);\r\nif (!ret)\r\nreturn 0;\r\ndmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_hp_remove_drhd, NULL);\r\nrelease_atsr:\r\ndmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\r\n&dmar_release_one_atsr, NULL);\r\nrelease_drhd:\r\ndmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_hp_release_drhd, NULL);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int dmar_hotplug_remove(acpi_handle handle)\r\n{\r\nint ret;\r\nret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\r\n&dmar_check_one_atsr, NULL);\r\nif (ret)\r\nreturn ret;\r\nret = dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_hp_remove_drhd, NULL);\r\nif (ret == 0) {\r\nWARN_ON(dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_ATSR,\r\n&dmar_release_one_atsr, NULL));\r\nWARN_ON(dmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_hp_release_drhd, NULL));\r\n} else {\r\ndmar_walk_dsm_resource(handle, DMAR_DSM_FUNC_DRHD,\r\n&dmar_hp_add_drhd, NULL);\r\n}\r\nreturn ret;\r\n}\r\nstatic acpi_status dmar_get_dsm_handle(acpi_handle handle, u32 lvl,\r\nvoid *context, void **retval)\r\n{\r\nacpi_handle *phdl = retval;\r\nif (dmar_detect_dsm(handle, DMAR_DSM_FUNC_DRHD)) {\r\n*phdl = handle;\r\nreturn AE_CTRL_TERMINATE;\r\n}\r\nreturn AE_OK;\r\n}\r\nstatic int dmar_device_hotplug(acpi_handle handle, bool insert)\r\n{\r\nint ret;\r\nacpi_handle tmp = NULL;\r\nacpi_status status;\r\nif (!dmar_in_use())\r\nreturn 0;\r\nif (dmar_detect_dsm(handle, DMAR_DSM_FUNC_DRHD)) {\r\ntmp = handle;\r\n} else {\r\nstatus = acpi_walk_namespace(ACPI_TYPE_DEVICE, handle,\r\nACPI_UINT32_MAX,\r\ndmar_get_dsm_handle,\r\nNULL, NULL, &tmp);\r\nif (ACPI_FAILURE(status)) {\r\npr_warn("Failed to locate _DSM method.\n");\r\nreturn -ENXIO;\r\n}\r\n}\r\nif (tmp == NULL)\r\nreturn 0;\r\ndown_write(&dmar_global_lock);\r\nif (insert)\r\nret = dmar_hotplug_insert(tmp);\r\nelse\r\nret = dmar_hotplug_remove(tmp);\r\nup_write(&dmar_global_lock);\r\nreturn ret;\r\n}\r\nint dmar_device_add(acpi_handle handle)\r\n{\r\nreturn dmar_device_hotplug(handle, true);\r\n}\r\nint dmar_device_remove(acpi_handle handle)\r\n{\r\nreturn dmar_device_hotplug(handle, false);\r\n}
