static void add_sgi_source(struct kvm_vcpu *vcpu, int irq, int source)\r\n{\r\nvcpu->kvm->arch.vgic.vm_ops.add_sgi_source(vcpu, irq, source);\r\n}\r\nstatic bool queue_sgi(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nreturn vcpu->kvm->arch.vgic.vm_ops.queue_sgi(vcpu, irq);\r\n}\r\nint kvm_vgic_map_resources(struct kvm *kvm)\r\n{\r\nreturn kvm->arch.vgic.vm_ops.map_resources(kvm, vgic);\r\n}\r\nstatic int vgic_init_bitmap(struct vgic_bitmap *b, int nr_cpus, int nr_irqs)\r\n{\r\nint nr_longs;\r\nnr_longs = nr_cpus + BITS_TO_LONGS(nr_irqs - VGIC_NR_PRIVATE_IRQS);\r\nb->private = kzalloc(sizeof(unsigned long) * nr_longs, GFP_KERNEL);\r\nif (!b->private)\r\nreturn -ENOMEM;\r\nb->shared = b->private + nr_cpus;\r\nreturn 0;\r\n}\r\nstatic void vgic_free_bitmap(struct vgic_bitmap *b)\r\n{\r\nkfree(b->private);\r\nb->private = NULL;\r\nb->shared = NULL;\r\n}\r\nstatic unsigned long *u64_to_bitmask(u64 *val)\r\n{\r\n#if defined(CONFIG_CPU_BIG_ENDIAN) && BITS_PER_LONG == 32\r\n*val = (*val >> 32) | (*val << 32);\r\n#endif\r\nreturn (unsigned long *)val;\r\n}\r\nu32 *vgic_bitmap_get_reg(struct vgic_bitmap *x, int cpuid, u32 offset)\r\n{\r\noffset >>= 2;\r\nif (!offset)\r\nreturn (u32 *)(x->private + cpuid) + REG_OFFSET_SWIZZLE;\r\nelse\r\nreturn (u32 *)(x->shared) + ((offset - 1) ^ REG_OFFSET_SWIZZLE);\r\n}\r\nstatic int vgic_bitmap_get_irq_val(struct vgic_bitmap *x,\r\nint cpuid, int irq)\r\n{\r\nif (irq < VGIC_NR_PRIVATE_IRQS)\r\nreturn test_bit(irq, x->private + cpuid);\r\nreturn test_bit(irq - VGIC_NR_PRIVATE_IRQS, x->shared);\r\n}\r\nvoid vgic_bitmap_set_irq_val(struct vgic_bitmap *x, int cpuid,\r\nint irq, int val)\r\n{\r\nunsigned long *reg;\r\nif (irq < VGIC_NR_PRIVATE_IRQS) {\r\nreg = x->private + cpuid;\r\n} else {\r\nreg = x->shared;\r\nirq -= VGIC_NR_PRIVATE_IRQS;\r\n}\r\nif (val)\r\nset_bit(irq, reg);\r\nelse\r\nclear_bit(irq, reg);\r\n}\r\nstatic unsigned long *vgic_bitmap_get_cpu_map(struct vgic_bitmap *x, int cpuid)\r\n{\r\nreturn x->private + cpuid;\r\n}\r\nunsigned long *vgic_bitmap_get_shared_map(struct vgic_bitmap *x)\r\n{\r\nreturn x->shared;\r\n}\r\nstatic int vgic_init_bytemap(struct vgic_bytemap *x, int nr_cpus, int nr_irqs)\r\n{\r\nint size;\r\nsize = nr_cpus * VGIC_NR_PRIVATE_IRQS;\r\nsize += nr_irqs - VGIC_NR_PRIVATE_IRQS;\r\nx->private = kzalloc(size, GFP_KERNEL);\r\nif (!x->private)\r\nreturn -ENOMEM;\r\nx->shared = x->private + nr_cpus * VGIC_NR_PRIVATE_IRQS / sizeof(u32);\r\nreturn 0;\r\n}\r\nstatic void vgic_free_bytemap(struct vgic_bytemap *b)\r\n{\r\nkfree(b->private);\r\nb->private = NULL;\r\nb->shared = NULL;\r\n}\r\nu32 *vgic_bytemap_get_reg(struct vgic_bytemap *x, int cpuid, u32 offset)\r\n{\r\nu32 *reg;\r\nif (offset < VGIC_NR_PRIVATE_IRQS) {\r\nreg = x->private;\r\noffset += cpuid * VGIC_NR_PRIVATE_IRQS;\r\n} else {\r\nreg = x->shared;\r\noffset -= VGIC_NR_PRIVATE_IRQS;\r\n}\r\nreturn reg + (offset / sizeof(u32));\r\n}\r\nstatic bool vgic_irq_is_edge(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nint irq_val;\r\nirq_val = vgic_bitmap_get_irq_val(&dist->irq_cfg, vcpu->vcpu_id, irq);\r\nreturn irq_val == VGIC_CFG_EDGE;\r\n}\r\nstatic int vgic_irq_is_enabled(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_enabled, vcpu->vcpu_id, irq);\r\n}\r\nstatic int vgic_irq_is_queued(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_queued, vcpu->vcpu_id, irq);\r\n}\r\nstatic int vgic_irq_is_active(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_active, vcpu->vcpu_id, irq);\r\n}\r\nstatic void vgic_irq_set_queued(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_queued, vcpu->vcpu_id, irq, 1);\r\n}\r\nstatic void vgic_irq_clear_queued(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_queued, vcpu->vcpu_id, irq, 0);\r\n}\r\nstatic void vgic_irq_set_active(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_active, vcpu->vcpu_id, irq, 1);\r\n}\r\nstatic void vgic_irq_clear_active(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_active, vcpu->vcpu_id, irq, 0);\r\n}\r\nstatic int vgic_dist_irq_get_level(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_level, vcpu->vcpu_id, irq);\r\n}\r\nstatic void vgic_dist_irq_set_level(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_level, vcpu->vcpu_id, irq, 1);\r\n}\r\nstatic void vgic_dist_irq_clear_level(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_level, vcpu->vcpu_id, irq, 0);\r\n}\r\nstatic int vgic_dist_irq_soft_pend(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_soft_pend, vcpu->vcpu_id, irq);\r\n}\r\nstatic void vgic_dist_irq_clear_soft_pend(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_soft_pend, vcpu->vcpu_id, irq, 0);\r\nif (!vgic_dist_irq_get_level(vcpu, irq)) {\r\nvgic_dist_irq_clear_pending(vcpu, irq);\r\nif (!compute_pending_for_cpu(vcpu))\r\nclear_bit(vcpu->vcpu_id, dist->irq_pending_on_cpu);\r\n}\r\n}\r\nstatic int vgic_dist_irq_is_pending(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_pending, vcpu->vcpu_id, irq);\r\n}\r\nvoid vgic_dist_irq_set_pending(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_pending, vcpu->vcpu_id, irq, 1);\r\n}\r\nvoid vgic_dist_irq_clear_pending(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_pending, vcpu->vcpu_id, irq, 0);\r\n}\r\nstatic void vgic_cpu_irq_set(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nif (irq < VGIC_NR_PRIVATE_IRQS)\r\nset_bit(irq, vcpu->arch.vgic_cpu.pending_percpu);\r\nelse\r\nset_bit(irq - VGIC_NR_PRIVATE_IRQS,\r\nvcpu->arch.vgic_cpu.pending_shared);\r\n}\r\nvoid vgic_cpu_irq_clear(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nif (irq < VGIC_NR_PRIVATE_IRQS)\r\nclear_bit(irq, vcpu->arch.vgic_cpu.pending_percpu);\r\nelse\r\nclear_bit(irq - VGIC_NR_PRIVATE_IRQS,\r\nvcpu->arch.vgic_cpu.pending_shared);\r\n}\r\nstatic bool vgic_can_sample_irq(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nreturn !vgic_irq_is_queued(vcpu, irq);\r\n}\r\nvoid vgic_reg_access(struct kvm_exit_mmio *mmio, u32 *reg,\r\nphys_addr_t offset, int mode)\r\n{\r\nint word_offset = (offset & 3) * 8;\r\nu32 mask = (1UL << (mmio->len * 8)) - 1;\r\nu32 regval;\r\nif (reg) {\r\nregval = *reg;\r\n} else {\r\nBUG_ON(mode != (ACCESS_READ_RAZ | ACCESS_WRITE_IGNORED));\r\nregval = 0;\r\n}\r\nif (mmio->is_write) {\r\nu32 data = mmio_data_read(mmio, mask) << word_offset;\r\nswitch (ACCESS_WRITE_MASK(mode)) {\r\ncase ACCESS_WRITE_IGNORED:\r\nreturn;\r\ncase ACCESS_WRITE_SETBIT:\r\nregval |= data;\r\nbreak;\r\ncase ACCESS_WRITE_CLEARBIT:\r\nregval &= ~data;\r\nbreak;\r\ncase ACCESS_WRITE_VALUE:\r\nregval = (regval & ~(mask << word_offset)) | data;\r\nbreak;\r\n}\r\n*reg = regval;\r\n} else {\r\nswitch (ACCESS_READ_MASK(mode)) {\r\ncase ACCESS_READ_RAZ:\r\nregval = 0;\r\ncase ACCESS_READ_VALUE:\r\nmmio_data_write(mmio, mask, regval >> word_offset);\r\n}\r\n}\r\n}\r\nbool handle_mmio_raz_wi(struct kvm_vcpu *vcpu, struct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nbool vgic_handle_enable_reg(struct kvm *kvm, struct kvm_exit_mmio *mmio,\r\nphys_addr_t offset, int vcpu_id, int access)\r\n{\r\nu32 *reg;\r\nint mode = ACCESS_READ_VALUE | access;\r\nstruct kvm_vcpu *target_vcpu = kvm_get_vcpu(kvm, vcpu_id);\r\nreg = vgic_bitmap_get_reg(&kvm->arch.vgic.irq_enabled, vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset, mode);\r\nif (mmio->is_write) {\r\nif (access & ACCESS_WRITE_CLEARBIT) {\r\nif (offset < 4)\r\n*reg |= 0xffff;\r\nvgic_retire_disabled_irqs(target_vcpu);\r\n}\r\nvgic_update_state(kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nbool vgic_handle_set_pending_reg(struct kvm *kvm,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset, int vcpu_id)\r\n{\r\nu32 *reg, orig;\r\nu32 level_mask;\r\nint mode = ACCESS_READ_VALUE | ACCESS_WRITE_SETBIT;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nreg = vgic_bitmap_get_reg(&dist->irq_cfg, vcpu_id, offset);\r\nlevel_mask = (~(*reg));\r\nreg = vgic_bitmap_get_reg(&dist->irq_pending, vcpu_id, offset);\r\norig = *reg;\r\nvgic_reg_access(mmio, reg, offset, mode);\r\nif (mmio->is_write) {\r\nreg = vgic_bitmap_get_reg(&dist->irq_soft_pend,\r\nvcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset, mode);\r\n*reg &= level_mask;\r\nif (offset < 2) {\r\n*reg &= ~0xffff;\r\n*reg |= orig & 0xffff;\r\n}\r\nvgic_update_state(kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nbool vgic_handle_clear_pending_reg(struct kvm *kvm,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset, int vcpu_id)\r\n{\r\nu32 *level_active;\r\nu32 *reg, orig;\r\nint mode = ACCESS_READ_VALUE | ACCESS_WRITE_CLEARBIT;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nreg = vgic_bitmap_get_reg(&dist->irq_pending, vcpu_id, offset);\r\norig = *reg;\r\nvgic_reg_access(mmio, reg, offset, mode);\r\nif (mmio->is_write) {\r\nlevel_active = vgic_bitmap_get_reg(&dist->irq_level,\r\nvcpu_id, offset);\r\nreg = vgic_bitmap_get_reg(&dist->irq_pending, vcpu_id, offset);\r\n*reg |= *level_active;\r\nif (offset < 2) {\r\n*reg &= ~0xffff;\r\n*reg |= orig & 0xffff;\r\n}\r\nreg = vgic_bitmap_get_reg(&dist->irq_soft_pend,\r\nvcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset, mode);\r\nvgic_update_state(kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nbool vgic_handle_set_active_reg(struct kvm *kvm,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset, int vcpu_id)\r\n{\r\nu32 *reg;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nreg = vgic_bitmap_get_reg(&dist->irq_active, vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_SETBIT);\r\nif (mmio->is_write) {\r\nvgic_update_state(kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nbool vgic_handle_clear_active_reg(struct kvm *kvm,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset, int vcpu_id)\r\n{\r\nu32 *reg;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nreg = vgic_bitmap_get_reg(&dist->irq_active, vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_CLEARBIT);\r\nif (mmio->is_write) {\r\nvgic_update_state(kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic u32 vgic_cfg_expand(u16 val)\r\n{\r\nu32 res = 0;\r\nint i;\r\nfor (i = 0; i < 16; i++)\r\nres |= ((val >> i) & VGIC_CFG_EDGE) << (2 * i + 1);\r\nreturn res;\r\n}\r\nstatic u16 vgic_cfg_compress(u32 val)\r\n{\r\nu16 res = 0;\r\nint i;\r\nfor (i = 0; i < 16; i++)\r\nres |= ((val >> (i * 2 + 1)) & VGIC_CFG_EDGE) << i;\r\nreturn res;\r\n}\r\nbool vgic_handle_cfg_reg(u32 *reg, struct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 val;\r\nif (offset & 4)\r\nval = *reg >> 16;\r\nelse\r\nval = *reg & 0xffff;\r\nval = vgic_cfg_expand(val);\r\nvgic_reg_access(mmio, &val, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nif (offset < 8)\r\nreturn false;\r\nval = vgic_cfg_compress(val);\r\nif (offset & 4) {\r\n*reg &= 0xffff;\r\n*reg |= val << 16;\r\n} else {\r\n*reg &= 0xffff << 16;\r\n*reg |= val;\r\n}\r\n}\r\nreturn false;\r\n}\r\nvoid vgic_unqueue_irqs(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nu64 elrsr = vgic_get_elrsr(vcpu);\r\nunsigned long *elrsr_ptr = u64_to_bitmask(&elrsr);\r\nint i;\r\nfor_each_clear_bit(i, elrsr_ptr, vgic_cpu->nr_lr) {\r\nstruct vgic_lr lr = vgic_get_lr(vcpu, i);\r\nBUG_ON(!(lr.state & LR_STATE_MASK));\r\nif (lr.irq < VGIC_NR_SGIS)\r\nadd_sgi_source(vcpu, lr.irq, lr.source);\r\nif (lr.state & LR_STATE_ACTIVE)\r\nvgic_irq_set_active(vcpu, lr.irq);\r\nvgic_retire_lr(i, vcpu);\r\nvgic_update_state(vcpu->kvm);\r\n}\r\n}\r\nconst\r\nstruct vgic_io_range *vgic_find_range(const struct vgic_io_range *ranges,\r\nint len, gpa_t offset)\r\n{\r\nwhile (ranges->len) {\r\nif (offset >= ranges->base &&\r\n(offset + len) <= (ranges->base + ranges->len))\r\nreturn ranges;\r\nranges++;\r\n}\r\nreturn NULL;\r\n}\r\nstatic bool vgic_validate_access(const struct vgic_dist *dist,\r\nconst struct vgic_io_range *range,\r\nunsigned long offset)\r\n{\r\nint irq;\r\nif (!range->bits_per_irq)\r\nreturn true;\r\nirq = offset * 8 / range->bits_per_irq;\r\nif (irq >= dist->nr_irqs)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool call_range_handler(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nunsigned long offset,\r\nconst struct vgic_io_range *range)\r\n{\r\nstruct kvm_exit_mmio mmio32;\r\nbool ret;\r\nif (likely(mmio->len <= 4))\r\nreturn range->handle_mmio(vcpu, mmio, offset);\r\nmmio32.len = 4;\r\nmmio32.is_write = mmio->is_write;\r\nmmio32.private = mmio->private;\r\nmmio32.phys_addr = mmio->phys_addr + 4;\r\nmmio32.data = &((u32 *)mmio->data)[1];\r\nret = range->handle_mmio(vcpu, &mmio32, offset + 4);\r\nmmio32.phys_addr = mmio->phys_addr;\r\nmmio32.data = &((u32 *)mmio->data)[0];\r\nret |= range->handle_mmio(vcpu, &mmio32, offset);\r\nreturn ret;\r\n}\r\nstatic int vgic_handle_mmio_access(struct kvm_vcpu *vcpu,\r\nstruct kvm_io_device *this, gpa_t addr,\r\nint len, void *val, bool is_write)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nstruct vgic_io_device *iodev = container_of(this,\r\nstruct vgic_io_device, dev);\r\nstruct kvm_run *run = vcpu->run;\r\nconst struct vgic_io_range *range;\r\nstruct kvm_exit_mmio mmio;\r\nbool updated_state;\r\ngpa_t offset;\r\noffset = addr - iodev->addr;\r\nrange = vgic_find_range(iodev->reg_ranges, len, offset);\r\nif (unlikely(!range || !range->handle_mmio)) {\r\npr_warn("Unhandled access %d %08llx %d\n", is_write, addr, len);\r\nreturn -ENXIO;\r\n}\r\nmmio.phys_addr = addr;\r\nmmio.len = len;\r\nmmio.is_write = is_write;\r\nmmio.data = val;\r\nmmio.private = iodev->redist_vcpu;\r\nspin_lock(&dist->lock);\r\noffset -= range->base;\r\nif (vgic_validate_access(dist, range, offset)) {\r\nupdated_state = call_range_handler(vcpu, &mmio, offset, range);\r\n} else {\r\nif (!is_write)\r\nmemset(val, 0, len);\r\nupdated_state = false;\r\n}\r\nspin_unlock(&dist->lock);\r\nrun->mmio.is_write = is_write;\r\nrun->mmio.len = len;\r\nrun->mmio.phys_addr = addr;\r\nmemcpy(run->mmio.data, val, len);\r\nkvm_handle_mmio_return(vcpu, run);\r\nif (updated_state)\r\nvgic_kick_vcpus(vcpu->kvm);\r\nreturn 0;\r\n}\r\nstatic int vgic_handle_mmio_read(struct kvm_vcpu *vcpu,\r\nstruct kvm_io_device *this,\r\ngpa_t addr, int len, void *val)\r\n{\r\nreturn vgic_handle_mmio_access(vcpu, this, addr, len, val, false);\r\n}\r\nstatic int vgic_handle_mmio_write(struct kvm_vcpu *vcpu,\r\nstruct kvm_io_device *this,\r\ngpa_t addr, int len, const void *val)\r\n{\r\nreturn vgic_handle_mmio_access(vcpu, this, addr, len, (void *)val,\r\ntrue);\r\n}\r\nint vgic_register_kvm_io_dev(struct kvm *kvm, gpa_t base, int len,\r\nconst struct vgic_io_range *ranges,\r\nint redist_vcpu_id,\r\nstruct vgic_io_device *iodev)\r\n{\r\nstruct kvm_vcpu *vcpu = NULL;\r\nint ret;\r\nif (redist_vcpu_id >= 0)\r\nvcpu = kvm_get_vcpu(kvm, redist_vcpu_id);\r\niodev->addr = base;\r\niodev->len = len;\r\niodev->reg_ranges = ranges;\r\niodev->redist_vcpu = vcpu;\r\nkvm_iodevice_init(&iodev->dev, &vgic_io_ops);\r\nmutex_lock(&kvm->slots_lock);\r\nret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, base, len,\r\n&iodev->dev);\r\nmutex_unlock(&kvm->slots_lock);\r\nif (ret)\r\niodev->dev.ops = NULL;\r\nreturn ret;\r\n}\r\nstatic int vgic_nr_shared_irqs(struct vgic_dist *dist)\r\n{\r\nreturn dist->nr_irqs - VGIC_NR_PRIVATE_IRQS;\r\n}\r\nstatic int compute_active_for_cpu(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nunsigned long *active, *enabled, *act_percpu, *act_shared;\r\nunsigned long active_private, active_shared;\r\nint nr_shared = vgic_nr_shared_irqs(dist);\r\nint vcpu_id;\r\nvcpu_id = vcpu->vcpu_id;\r\nact_percpu = vcpu->arch.vgic_cpu.active_percpu;\r\nact_shared = vcpu->arch.vgic_cpu.active_shared;\r\nactive = vgic_bitmap_get_cpu_map(&dist->irq_active, vcpu_id);\r\nenabled = vgic_bitmap_get_cpu_map(&dist->irq_enabled, vcpu_id);\r\nbitmap_and(act_percpu, active, enabled, VGIC_NR_PRIVATE_IRQS);\r\nactive = vgic_bitmap_get_shared_map(&dist->irq_active);\r\nenabled = vgic_bitmap_get_shared_map(&dist->irq_enabled);\r\nbitmap_and(act_shared, active, enabled, nr_shared);\r\nbitmap_and(act_shared, act_shared,\r\nvgic_bitmap_get_shared_map(&dist->irq_spi_target[vcpu_id]),\r\nnr_shared);\r\nactive_private = find_first_bit(act_percpu, VGIC_NR_PRIVATE_IRQS);\r\nactive_shared = find_first_bit(act_shared, nr_shared);\r\nreturn (active_private < VGIC_NR_PRIVATE_IRQS ||\r\nactive_shared < nr_shared);\r\n}\r\nstatic int compute_pending_for_cpu(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nunsigned long *pending, *enabled, *pend_percpu, *pend_shared;\r\nunsigned long pending_private, pending_shared;\r\nint nr_shared = vgic_nr_shared_irqs(dist);\r\nint vcpu_id;\r\nvcpu_id = vcpu->vcpu_id;\r\npend_percpu = vcpu->arch.vgic_cpu.pending_percpu;\r\npend_shared = vcpu->arch.vgic_cpu.pending_shared;\r\nif (!dist->enabled) {\r\nbitmap_zero(pend_percpu, VGIC_NR_PRIVATE_IRQS);\r\nbitmap_zero(pend_shared, nr_shared);\r\nreturn 0;\r\n}\r\npending = vgic_bitmap_get_cpu_map(&dist->irq_pending, vcpu_id);\r\nenabled = vgic_bitmap_get_cpu_map(&dist->irq_enabled, vcpu_id);\r\nbitmap_and(pend_percpu, pending, enabled, VGIC_NR_PRIVATE_IRQS);\r\npending = vgic_bitmap_get_shared_map(&dist->irq_pending);\r\nenabled = vgic_bitmap_get_shared_map(&dist->irq_enabled);\r\nbitmap_and(pend_shared, pending, enabled, nr_shared);\r\nbitmap_and(pend_shared, pend_shared,\r\nvgic_bitmap_get_shared_map(&dist->irq_spi_target[vcpu_id]),\r\nnr_shared);\r\npending_private = find_first_bit(pend_percpu, VGIC_NR_PRIVATE_IRQS);\r\npending_shared = find_first_bit(pend_shared, nr_shared);\r\nreturn (pending_private < VGIC_NR_PRIVATE_IRQS ||\r\npending_shared < vgic_nr_shared_irqs(dist));\r\n}\r\nvoid vgic_update_state(struct kvm *kvm)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint c;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nif (compute_pending_for_cpu(vcpu))\r\nset_bit(c, dist->irq_pending_on_cpu);\r\nif (compute_active_for_cpu(vcpu))\r\nset_bit(c, dist->irq_active_on_cpu);\r\nelse\r\nclear_bit(c, dist->irq_active_on_cpu);\r\n}\r\n}\r\nstatic struct vgic_lr vgic_get_lr(const struct kvm_vcpu *vcpu, int lr)\r\n{\r\nreturn vgic_ops->get_lr(vcpu, lr);\r\n}\r\nstatic void vgic_set_lr(struct kvm_vcpu *vcpu, int lr,\r\nstruct vgic_lr vlr)\r\n{\r\nvgic_ops->set_lr(vcpu, lr, vlr);\r\n}\r\nstatic inline u64 vgic_get_elrsr(struct kvm_vcpu *vcpu)\r\n{\r\nreturn vgic_ops->get_elrsr(vcpu);\r\n}\r\nstatic inline u64 vgic_get_eisr(struct kvm_vcpu *vcpu)\r\n{\r\nreturn vgic_ops->get_eisr(vcpu);\r\n}\r\nstatic inline void vgic_clear_eisr(struct kvm_vcpu *vcpu)\r\n{\r\nvgic_ops->clear_eisr(vcpu);\r\n}\r\nstatic inline u32 vgic_get_interrupt_status(struct kvm_vcpu *vcpu)\r\n{\r\nreturn vgic_ops->get_interrupt_status(vcpu);\r\n}\r\nstatic inline void vgic_enable_underflow(struct kvm_vcpu *vcpu)\r\n{\r\nvgic_ops->enable_underflow(vcpu);\r\n}\r\nstatic inline void vgic_disable_underflow(struct kvm_vcpu *vcpu)\r\n{\r\nvgic_ops->disable_underflow(vcpu);\r\n}\r\nvoid vgic_get_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcr)\r\n{\r\nvgic_ops->get_vmcr(vcpu, vmcr);\r\n}\r\nvoid vgic_set_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcr)\r\n{\r\nvgic_ops->set_vmcr(vcpu, vmcr);\r\n}\r\nstatic inline void vgic_enable(struct kvm_vcpu *vcpu)\r\n{\r\nvgic_ops->enable(vcpu);\r\n}\r\nstatic void vgic_retire_lr(int lr_nr, struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_lr vlr = vgic_get_lr(vcpu, lr_nr);\r\nvgic_irq_clear_queued(vcpu, vlr.irq);\r\nif (vlr.state & LR_STATE_PENDING) {\r\nvgic_dist_irq_set_pending(vcpu, vlr.irq);\r\nvlr.hwirq = 0;\r\n}\r\nvlr.state = 0;\r\nvgic_set_lr(vcpu, lr_nr, vlr);\r\n}\r\nstatic bool dist_active_irq(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn test_bit(vcpu->vcpu_id, dist->irq_active_on_cpu);\r\n}\r\nbool kvm_vgic_map_is_active(struct kvm_vcpu *vcpu, struct irq_phys_map *map)\r\n{\r\nint i;\r\nfor (i = 0; i < vcpu->arch.vgic_cpu.nr_lr; i++) {\r\nstruct vgic_lr vlr = vgic_get_lr(vcpu, i);\r\nif (vlr.irq == map->virt_irq && vlr.state & LR_STATE_ACTIVE)\r\nreturn true;\r\n}\r\nreturn vgic_irq_is_active(vcpu, map->virt_irq);\r\n}\r\nstatic void vgic_retire_disabled_irqs(struct kvm_vcpu *vcpu)\r\n{\r\nu64 elrsr = vgic_get_elrsr(vcpu);\r\nunsigned long *elrsr_ptr = u64_to_bitmask(&elrsr);\r\nint lr;\r\nfor_each_clear_bit(lr, elrsr_ptr, vgic->nr_lr) {\r\nstruct vgic_lr vlr = vgic_get_lr(vcpu, lr);\r\nif (!vgic_irq_is_enabled(vcpu, vlr.irq))\r\nvgic_retire_lr(lr, vcpu);\r\n}\r\n}\r\nstatic void vgic_queue_irq_to_lr(struct kvm_vcpu *vcpu, int irq,\r\nint lr_nr, struct vgic_lr vlr)\r\n{\r\nif (vgic_irq_is_active(vcpu, irq)) {\r\nvlr.state |= LR_STATE_ACTIVE;\r\nkvm_debug("Set active, clear distributor: 0x%x\n", vlr.state);\r\nvgic_irq_clear_active(vcpu, irq);\r\nvgic_update_state(vcpu->kvm);\r\n} else {\r\nWARN_ON(!vgic_dist_irq_is_pending(vcpu, irq));\r\nvlr.state |= LR_STATE_PENDING;\r\nkvm_debug("Set pending: 0x%x\n", vlr.state);\r\n}\r\nif (!vgic_irq_is_edge(vcpu, irq))\r\nvlr.state |= LR_EOI_INT;\r\nif (vlr.irq >= VGIC_NR_SGIS) {\r\nstruct irq_phys_map *map;\r\nmap = vgic_irq_map_search(vcpu, irq);\r\nif (map) {\r\nvlr.hwirq = map->phys_irq;\r\nvlr.state |= LR_HW;\r\nvlr.state &= ~LR_EOI_INT;\r\nvgic_irq_set_queued(vcpu, irq);\r\n}\r\n}\r\nvgic_set_lr(vcpu, lr_nr, vlr);\r\n}\r\nbool vgic_queue_irq(struct kvm_vcpu *vcpu, u8 sgi_source_id, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nu64 elrsr = vgic_get_elrsr(vcpu);\r\nunsigned long *elrsr_ptr = u64_to_bitmask(&elrsr);\r\nstruct vgic_lr vlr;\r\nint lr;\r\nBUG_ON(sgi_source_id & ~7);\r\nBUG_ON(sgi_source_id && irq >= VGIC_NR_SGIS);\r\nBUG_ON(irq >= dist->nr_irqs);\r\nkvm_debug("Queue IRQ%d\n", irq);\r\nfor_each_clear_bit(lr, elrsr_ptr, vgic->nr_lr) {\r\nvlr = vgic_get_lr(vcpu, lr);\r\nif (vlr.irq == irq && vlr.source == sgi_source_id) {\r\nkvm_debug("LR%d piggyback for IRQ%d\n", lr, vlr.irq);\r\nvgic_queue_irq_to_lr(vcpu, irq, lr, vlr);\r\nreturn true;\r\n}\r\n}\r\nlr = find_first_bit(elrsr_ptr, vgic->nr_lr);\r\nif (lr >= vgic->nr_lr)\r\nreturn false;\r\nkvm_debug("LR%d allocated for IRQ%d %x\n", lr, irq, sgi_source_id);\r\nvlr.irq = irq;\r\nvlr.source = sgi_source_id;\r\nvlr.state = 0;\r\nvgic_queue_irq_to_lr(vcpu, irq, lr, vlr);\r\nreturn true;\r\n}\r\nstatic bool vgic_queue_hwirq(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nif (!vgic_can_sample_irq(vcpu, irq))\r\nreturn true;\r\nif (vgic_queue_irq(vcpu, 0, irq)) {\r\nif (vgic_irq_is_edge(vcpu, irq)) {\r\nvgic_dist_irq_clear_pending(vcpu, irq);\r\nvgic_cpu_irq_clear(vcpu, irq);\r\n} else {\r\nvgic_irq_set_queued(vcpu, irq);\r\n}\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void __kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nunsigned long *pa_percpu, *pa_shared;\r\nint i, vcpu_id;\r\nint overflow = 0;\r\nint nr_shared = vgic_nr_shared_irqs(dist);\r\nvcpu_id = vcpu->vcpu_id;\r\npa_percpu = vcpu->arch.vgic_cpu.pend_act_percpu;\r\npa_shared = vcpu->arch.vgic_cpu.pend_act_shared;\r\nbitmap_or(pa_percpu, vgic_cpu->pending_percpu, vgic_cpu->active_percpu,\r\nVGIC_NR_PRIVATE_IRQS);\r\nbitmap_or(pa_shared, vgic_cpu->pending_shared, vgic_cpu->active_shared,\r\nnr_shared);\r\nif (!kvm_vgic_vcpu_pending_irq(vcpu) && !dist_active_irq(vcpu))\r\ngoto epilog;\r\nfor_each_set_bit(i, pa_percpu, VGIC_NR_SGIS) {\r\nif (!queue_sgi(vcpu, i))\r\noverflow = 1;\r\n}\r\nfor_each_set_bit_from(i, pa_percpu, VGIC_NR_PRIVATE_IRQS) {\r\nif (!vgic_queue_hwirq(vcpu, i))\r\noverflow = 1;\r\n}\r\nfor_each_set_bit(i, pa_shared, nr_shared) {\r\nif (!vgic_queue_hwirq(vcpu, i + VGIC_NR_PRIVATE_IRQS))\r\noverflow = 1;\r\n}\r\nepilog:\r\nif (overflow) {\r\nvgic_enable_underflow(vcpu);\r\n} else {\r\nvgic_disable_underflow(vcpu);\r\nclear_bit(vcpu_id, dist->irq_pending_on_cpu);\r\n}\r\n}\r\nstatic int process_queued_irq(struct kvm_vcpu *vcpu,\r\nint lr, struct vgic_lr vlr)\r\n{\r\nint pending = 0;\r\nvgic_dist_irq_clear_soft_pend(vcpu, vlr.irq);\r\nvgic_irq_clear_queued(vcpu, vlr.irq);\r\nif (vgic_irq_is_edge(vcpu, vlr.irq)) {\r\nBUG_ON(!(vlr.state & LR_HW));\r\npending = vgic_dist_irq_is_pending(vcpu, vlr.irq);\r\n} else {\r\nif (vgic_dist_irq_get_level(vcpu, vlr.irq)) {\r\nvgic_cpu_irq_set(vcpu, vlr.irq);\r\npending = 1;\r\n} else {\r\nvgic_dist_irq_clear_pending(vcpu, vlr.irq);\r\nvgic_cpu_irq_clear(vcpu, vlr.irq);\r\n}\r\n}\r\nvlr.state = 0;\r\nvlr.hwirq = 0;\r\nvgic_set_lr(vcpu, lr, vlr);\r\nreturn pending;\r\n}\r\nstatic bool vgic_process_maintenance(struct kvm_vcpu *vcpu)\r\n{\r\nu32 status = vgic_get_interrupt_status(vcpu);\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nstruct kvm *kvm = vcpu->kvm;\r\nint level_pending = 0;\r\nkvm_debug("STATUS = %08x\n", status);\r\nif (status & INT_STATUS_EOI) {\r\nu64 eisr = vgic_get_eisr(vcpu);\r\nunsigned long *eisr_ptr = u64_to_bitmask(&eisr);\r\nint lr;\r\nfor_each_set_bit(lr, eisr_ptr, vgic->nr_lr) {\r\nstruct vgic_lr vlr = vgic_get_lr(vcpu, lr);\r\nWARN_ON(vgic_irq_is_edge(vcpu, vlr.irq));\r\nWARN_ON(vlr.state & LR_STATE_MASK);\r\nkvm_notify_acked_irq(kvm, 0,\r\nvlr.irq - VGIC_NR_PRIVATE_IRQS);\r\nspin_lock(&dist->lock);\r\nlevel_pending |= process_queued_irq(vcpu, lr, vlr);\r\nspin_unlock(&dist->lock);\r\n}\r\n}\r\nif (status & INT_STATUS_UNDERFLOW)\r\nvgic_disable_underflow(vcpu);\r\nvgic_clear_eisr(vcpu);\r\nreturn level_pending;\r\n}\r\nstatic bool vgic_sync_hwirq(struct kvm_vcpu *vcpu, int lr, struct vgic_lr vlr)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nbool level_pending;\r\nif (!(vlr.state & LR_HW))\r\nreturn false;\r\nif (vlr.state & LR_STATE_ACTIVE)\r\nreturn false;\r\nspin_lock(&dist->lock);\r\nlevel_pending = process_queued_irq(vcpu, lr, vlr);\r\nspin_unlock(&dist->lock);\r\nreturn level_pending;\r\n}\r\nstatic void __kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nu64 elrsr;\r\nunsigned long *elrsr_ptr;\r\nint lr, pending;\r\nbool level_pending;\r\nlevel_pending = vgic_process_maintenance(vcpu);\r\nfor (lr = 0; lr < vgic->nr_lr; lr++) {\r\nstruct vgic_lr vlr = vgic_get_lr(vcpu, lr);\r\nlevel_pending |= vgic_sync_hwirq(vcpu, lr, vlr);\r\nBUG_ON(vlr.irq >= dist->nr_irqs);\r\n}\r\nelrsr = vgic_get_elrsr(vcpu);\r\nelrsr_ptr = u64_to_bitmask(&elrsr);\r\npending = find_first_zero_bit(elrsr_ptr, vgic->nr_lr);\r\nif (level_pending || pending < vgic->nr_lr)\r\nset_bit(vcpu->vcpu_id, dist->irq_pending_on_cpu);\r\n}\r\nvoid kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nif (!irqchip_in_kernel(vcpu->kvm))\r\nreturn;\r\nspin_lock(&dist->lock);\r\n__kvm_vgic_flush_hwstate(vcpu);\r\nspin_unlock(&dist->lock);\r\n}\r\nvoid kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nif (!irqchip_in_kernel(vcpu->kvm))\r\nreturn;\r\n__kvm_vgic_sync_hwstate(vcpu);\r\n}\r\nint kvm_vgic_vcpu_pending_irq(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nif (!irqchip_in_kernel(vcpu->kvm))\r\nreturn 0;\r\nreturn test_bit(vcpu->vcpu_id, dist->irq_pending_on_cpu);\r\n}\r\nvoid vgic_kick_vcpus(struct kvm *kvm)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nint c;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nif (kvm_vgic_vcpu_pending_irq(vcpu))\r\nkvm_vcpu_kick(vcpu);\r\n}\r\n}\r\nstatic int vgic_validate_injection(struct kvm_vcpu *vcpu, int irq, int level)\r\n{\r\nint edge_triggered = vgic_irq_is_edge(vcpu, irq);\r\nif (edge_triggered) {\r\nint state = vgic_dist_irq_is_pending(vcpu, irq);\r\nreturn level > state;\r\n} else {\r\nint state = vgic_dist_irq_get_level(vcpu, irq);\r\nreturn level != state;\r\n}\r\n}\r\nstatic int vgic_update_irq_pending(struct kvm *kvm, int cpuid,\r\nstruct irq_phys_map *map,\r\nunsigned int irq_num, bool level)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint edge_triggered, level_triggered;\r\nint enabled;\r\nbool ret = true, can_inject = true;\r\ntrace_vgic_update_irq_pending(cpuid, irq_num, level);\r\nif (irq_num >= min(kvm->arch.vgic.nr_irqs, 1020))\r\nreturn -EINVAL;\r\nspin_lock(&dist->lock);\r\nvcpu = kvm_get_vcpu(kvm, cpuid);\r\nedge_triggered = vgic_irq_is_edge(vcpu, irq_num);\r\nlevel_triggered = !edge_triggered;\r\nif (!vgic_validate_injection(vcpu, irq_num, level)) {\r\nret = false;\r\ngoto out;\r\n}\r\nif (irq_num >= VGIC_NR_PRIVATE_IRQS) {\r\ncpuid = dist->irq_spi_cpu[irq_num - VGIC_NR_PRIVATE_IRQS];\r\nif (cpuid == VCPU_NOT_ALLOCATED) {\r\ncpuid = 0;\r\ncan_inject = false;\r\n}\r\nvcpu = kvm_get_vcpu(kvm, cpuid);\r\n}\r\nkvm_debug("Inject IRQ%d level %d CPU%d\n", irq_num, level, cpuid);\r\nif (level) {\r\nif (level_triggered)\r\nvgic_dist_irq_set_level(vcpu, irq_num);\r\nvgic_dist_irq_set_pending(vcpu, irq_num);\r\n} else {\r\nif (level_triggered) {\r\nvgic_dist_irq_clear_level(vcpu, irq_num);\r\nif (!vgic_dist_irq_soft_pend(vcpu, irq_num)) {\r\nvgic_dist_irq_clear_pending(vcpu, irq_num);\r\nvgic_cpu_irq_clear(vcpu, irq_num);\r\nif (!compute_pending_for_cpu(vcpu))\r\nclear_bit(cpuid, dist->irq_pending_on_cpu);\r\n}\r\n}\r\nret = false;\r\ngoto out;\r\n}\r\nenabled = vgic_irq_is_enabled(vcpu, irq_num);\r\nif (!enabled || !can_inject) {\r\nret = false;\r\ngoto out;\r\n}\r\nif (!vgic_can_sample_irq(vcpu, irq_num)) {\r\nret = false;\r\ngoto out;\r\n}\r\nif (level) {\r\nvgic_cpu_irq_set(vcpu, irq_num);\r\nset_bit(cpuid, dist->irq_pending_on_cpu);\r\n}\r\nout:\r\nspin_unlock(&dist->lock);\r\nif (ret) {\r\nkvm_vcpu_kick(kvm_get_vcpu(kvm, cpuid));\r\n}\r\nreturn 0;\r\n}\r\nstatic int vgic_lazy_init(struct kvm *kvm)\r\n{\r\nint ret = 0;\r\nif (unlikely(!vgic_initialized(kvm))) {\r\nif (kvm->arch.vgic.vgic_model != KVM_DEV_TYPE_ARM_VGIC_V2)\r\nreturn -EBUSY;\r\nmutex_lock(&kvm->lock);\r\nret = vgic_init(kvm);\r\nmutex_unlock(&kvm->lock);\r\n}\r\nreturn ret;\r\n}\r\nint kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int irq_num,\r\nbool level)\r\n{\r\nstruct irq_phys_map *map;\r\nint ret;\r\nret = vgic_lazy_init(kvm);\r\nif (ret)\r\nreturn ret;\r\nmap = vgic_irq_map_search(kvm_get_vcpu(kvm, cpuid), irq_num);\r\nif (map)\r\nreturn -EINVAL;\r\nreturn vgic_update_irq_pending(kvm, cpuid, NULL, irq_num, level);\r\n}\r\nint kvm_vgic_inject_mapped_irq(struct kvm *kvm, int cpuid,\r\nstruct irq_phys_map *map, bool level)\r\n{\r\nint ret;\r\nret = vgic_lazy_init(kvm);\r\nif (ret)\r\nreturn ret;\r\nreturn vgic_update_irq_pending(kvm, cpuid, map, map->virt_irq, level);\r\n}\r\nstatic irqreturn_t vgic_maintenance_handler(int irq, void *data)\r\n{\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic struct list_head *vgic_get_irq_phys_map_list(struct kvm_vcpu *vcpu,\r\nint virt_irq)\r\n{\r\nif (virt_irq < VGIC_NR_PRIVATE_IRQS)\r\nreturn &vcpu->arch.vgic_cpu.irq_phys_map_list;\r\nelse\r\nreturn &vcpu->kvm->arch.vgic.irq_phys_map_list;\r\n}\r\nstruct irq_phys_map *kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu,\r\nint virt_irq, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nstruct list_head *root = vgic_get_irq_phys_map_list(vcpu, virt_irq);\r\nstruct irq_phys_map *map;\r\nstruct irq_phys_map_entry *entry;\r\nstruct irq_desc *desc;\r\nstruct irq_data *data;\r\nint phys_irq;\r\ndesc = irq_to_desc(irq);\r\nif (!desc) {\r\nkvm_err("%s: no interrupt descriptor\n", __func__);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\ndata = irq_desc_get_irq_data(desc);\r\nwhile (data->parent_data)\r\ndata = data->parent_data;\r\nphys_irq = data->hwirq;\r\nentry = kzalloc(sizeof(*entry), GFP_KERNEL);\r\nif (!entry)\r\nreturn ERR_PTR(-ENOMEM);\r\nspin_lock(&dist->irq_phys_map_lock);\r\nmap = vgic_irq_map_search(vcpu, virt_irq);\r\nif (map) {\r\nif (map->phys_irq != phys_irq ||\r\nmap->irq != irq)\r\nmap = ERR_PTR(-EINVAL);\r\ngoto out;\r\n}\r\nmap = &entry->map;\r\nmap->virt_irq = virt_irq;\r\nmap->phys_irq = phys_irq;\r\nmap->irq = irq;\r\nlist_add_tail_rcu(&entry->entry, root);\r\nout:\r\nspin_unlock(&dist->irq_phys_map_lock);\r\nif (IS_ERR(map) || map != &entry->map)\r\nkfree(entry);\r\nreturn map;\r\n}\r\nstatic struct irq_phys_map *vgic_irq_map_search(struct kvm_vcpu *vcpu,\r\nint virt_irq)\r\n{\r\nstruct list_head *root = vgic_get_irq_phys_map_list(vcpu, virt_irq);\r\nstruct irq_phys_map_entry *entry;\r\nstruct irq_phys_map *map;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(entry, root, entry) {\r\nmap = &entry->map;\r\nif (map->virt_irq == virt_irq) {\r\nrcu_read_unlock();\r\nreturn map;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn NULL;\r\n}\r\nstatic void vgic_free_phys_irq_map_rcu(struct rcu_head *rcu)\r\n{\r\nstruct irq_phys_map_entry *entry;\r\nentry = container_of(rcu, struct irq_phys_map_entry, rcu);\r\nkfree(entry);\r\n}\r\nint kvm_vgic_unmap_phys_irq(struct kvm_vcpu *vcpu, struct irq_phys_map *map)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nstruct irq_phys_map_entry *entry;\r\nstruct list_head *root;\r\nif (!map)\r\nreturn -EINVAL;\r\nroot = vgic_get_irq_phys_map_list(vcpu, map->virt_irq);\r\nspin_lock(&dist->irq_phys_map_lock);\r\nlist_for_each_entry(entry, root, entry) {\r\nif (&entry->map == map) {\r\nlist_del_rcu(&entry->entry);\r\ncall_rcu(&entry->rcu, vgic_free_phys_irq_map_rcu);\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&dist->irq_phys_map_lock);\r\nreturn 0;\r\n}\r\nstatic void vgic_destroy_irq_phys_map(struct kvm *kvm, struct list_head *root)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct irq_phys_map_entry *entry;\r\nspin_lock(&dist->irq_phys_map_lock);\r\nlist_for_each_entry(entry, root, entry) {\r\nlist_del_rcu(&entry->entry);\r\ncall_rcu(&entry->rcu, vgic_free_phys_irq_map_rcu);\r\n}\r\nspin_unlock(&dist->irq_phys_map_lock);\r\n}\r\nvoid kvm_vgic_vcpu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nkfree(vgic_cpu->pending_shared);\r\nkfree(vgic_cpu->active_shared);\r\nkfree(vgic_cpu->pend_act_shared);\r\nvgic_destroy_irq_phys_map(vcpu->kvm, &vgic_cpu->irq_phys_map_list);\r\nvgic_cpu->pending_shared = NULL;\r\nvgic_cpu->active_shared = NULL;\r\nvgic_cpu->pend_act_shared = NULL;\r\n}\r\nstatic int vgic_vcpu_init_maps(struct kvm_vcpu *vcpu, int nr_irqs)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nint nr_longs = BITS_TO_LONGS(nr_irqs - VGIC_NR_PRIVATE_IRQS);\r\nint sz = nr_longs * sizeof(unsigned long);\r\nvgic_cpu->pending_shared = kzalloc(sz, GFP_KERNEL);\r\nvgic_cpu->active_shared = kzalloc(sz, GFP_KERNEL);\r\nvgic_cpu->pend_act_shared = kzalloc(sz, GFP_KERNEL);\r\nif (!vgic_cpu->pending_shared\r\n|| !vgic_cpu->active_shared\r\n|| !vgic_cpu->pend_act_shared) {\r\nkvm_vgic_vcpu_destroy(vcpu);\r\nreturn -ENOMEM;\r\n}\r\nvgic_cpu->nr_lr = vgic->nr_lr;\r\nreturn 0;\r\n}\r\nvoid kvm_vgic_vcpu_early_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nINIT_LIST_HEAD(&vgic_cpu->irq_phys_map_list);\r\n}\r\nint kvm_vgic_get_max_vcpus(void)\r\n{\r\nreturn vgic->max_gic_vcpus;\r\n}\r\nvoid kvm_vgic_destroy(struct kvm *kvm)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint i;\r\nkvm_for_each_vcpu(i, vcpu, kvm)\r\nkvm_vgic_vcpu_destroy(vcpu);\r\nvgic_free_bitmap(&dist->irq_enabled);\r\nvgic_free_bitmap(&dist->irq_level);\r\nvgic_free_bitmap(&dist->irq_pending);\r\nvgic_free_bitmap(&dist->irq_soft_pend);\r\nvgic_free_bitmap(&dist->irq_queued);\r\nvgic_free_bitmap(&dist->irq_cfg);\r\nvgic_free_bytemap(&dist->irq_priority);\r\nif (dist->irq_spi_target) {\r\nfor (i = 0; i < dist->nr_cpus; i++)\r\nvgic_free_bitmap(&dist->irq_spi_target[i]);\r\n}\r\nkfree(dist->irq_sgi_sources);\r\nkfree(dist->irq_spi_cpu);\r\nkfree(dist->irq_spi_mpidr);\r\nkfree(dist->irq_spi_target);\r\nkfree(dist->irq_pending_on_cpu);\r\nkfree(dist->irq_active_on_cpu);\r\nvgic_destroy_irq_phys_map(kvm, &dist->irq_phys_map_list);\r\ndist->irq_sgi_sources = NULL;\r\ndist->irq_spi_cpu = NULL;\r\ndist->irq_spi_target = NULL;\r\ndist->irq_pending_on_cpu = NULL;\r\ndist->irq_active_on_cpu = NULL;\r\ndist->nr_cpus = 0;\r\n}\r\nint vgic_init(struct kvm *kvm)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint nr_cpus, nr_irqs;\r\nint ret, i, vcpu_id;\r\nif (vgic_initialized(kvm))\r\nreturn 0;\r\nnr_cpus = dist->nr_cpus = atomic_read(&kvm->online_vcpus);\r\nif (!nr_cpus)\r\nreturn -ENODEV;\r\nif (!dist->nr_irqs)\r\ndist->nr_irqs = VGIC_NR_IRQS_LEGACY;\r\nnr_irqs = dist->nr_irqs;\r\nret = vgic_init_bitmap(&dist->irq_enabled, nr_cpus, nr_irqs);\r\nret |= vgic_init_bitmap(&dist->irq_level, nr_cpus, nr_irqs);\r\nret |= vgic_init_bitmap(&dist->irq_pending, nr_cpus, nr_irqs);\r\nret |= vgic_init_bitmap(&dist->irq_soft_pend, nr_cpus, nr_irqs);\r\nret |= vgic_init_bitmap(&dist->irq_queued, nr_cpus, nr_irqs);\r\nret |= vgic_init_bitmap(&dist->irq_active, nr_cpus, nr_irqs);\r\nret |= vgic_init_bitmap(&dist->irq_cfg, nr_cpus, nr_irqs);\r\nret |= vgic_init_bytemap(&dist->irq_priority, nr_cpus, nr_irqs);\r\nif (ret)\r\ngoto out;\r\ndist->irq_sgi_sources = kzalloc(nr_cpus * VGIC_NR_SGIS, GFP_KERNEL);\r\ndist->irq_spi_cpu = kzalloc(nr_irqs - VGIC_NR_PRIVATE_IRQS, GFP_KERNEL);\r\ndist->irq_spi_target = kzalloc(sizeof(*dist->irq_spi_target) * nr_cpus,\r\nGFP_KERNEL);\r\ndist->irq_pending_on_cpu = kzalloc(BITS_TO_LONGS(nr_cpus) * sizeof(long),\r\nGFP_KERNEL);\r\ndist->irq_active_on_cpu = kzalloc(BITS_TO_LONGS(nr_cpus) * sizeof(long),\r\nGFP_KERNEL);\r\nif (!dist->irq_sgi_sources ||\r\n!dist->irq_spi_cpu ||\r\n!dist->irq_spi_target ||\r\n!dist->irq_pending_on_cpu ||\r\n!dist->irq_active_on_cpu) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nfor (i = 0; i < nr_cpus; i++)\r\nret |= vgic_init_bitmap(&dist->irq_spi_target[i],\r\nnr_cpus, nr_irqs);\r\nif (ret)\r\ngoto out;\r\nret = kvm->arch.vgic.vm_ops.init_model(kvm);\r\nif (ret)\r\ngoto out;\r\nkvm_for_each_vcpu(vcpu_id, vcpu, kvm) {\r\nret = vgic_vcpu_init_maps(vcpu, nr_irqs);\r\nif (ret) {\r\nkvm_err("VGIC: Failed to allocate vcpu memory\n");\r\nbreak;\r\n}\r\nfor (i = 0; i < VGIC_NR_PRIVATE_IRQS; i++) {\r\nif (i < VGIC_NR_SGIS) {\r\nvgic_bitmap_set_irq_val(&dist->irq_enabled,\r\nvcpu->vcpu_id, i, 1);\r\nvgic_bitmap_set_irq_val(&dist->irq_cfg,\r\nvcpu->vcpu_id, i,\r\nVGIC_CFG_EDGE);\r\n} else if (i < VGIC_NR_PRIVATE_IRQS) {\r\nvgic_bitmap_set_irq_val(&dist->irq_cfg,\r\nvcpu->vcpu_id, i,\r\nVGIC_CFG_LEVEL);\r\n}\r\n}\r\nvgic_enable(vcpu);\r\n}\r\nout:\r\nif (ret)\r\nkvm_vgic_destroy(kvm);\r\nreturn ret;\r\n}\r\nstatic int init_vgic_model(struct kvm *kvm, int type)\r\n{\r\nswitch (type) {\r\ncase KVM_DEV_TYPE_ARM_VGIC_V2:\r\nvgic_v2_init_emulation(kvm);\r\nbreak;\r\n#ifdef CONFIG_KVM_ARM_VGIC_V3\r\ncase KVM_DEV_TYPE_ARM_VGIC_V3:\r\nvgic_v3_init_emulation(kvm);\r\nbreak;\r\n#endif\r\ndefault:\r\nreturn -ENODEV;\r\n}\r\nif (atomic_read(&kvm->online_vcpus) > kvm->arch.max_vcpus)\r\nreturn -E2BIG;\r\nreturn 0;\r\n}\r\nvoid kvm_vgic_early_init(struct kvm *kvm)\r\n{\r\nspin_lock_init(&kvm->arch.vgic.lock);\r\nspin_lock_init(&kvm->arch.vgic.irq_phys_map_lock);\r\nINIT_LIST_HEAD(&kvm->arch.vgic.irq_phys_map_list);\r\n}\r\nint kvm_vgic_create(struct kvm *kvm, u32 type)\r\n{\r\nint i, vcpu_lock_idx = -1, ret;\r\nstruct kvm_vcpu *vcpu;\r\nmutex_lock(&kvm->lock);\r\nif (irqchip_in_kernel(kvm)) {\r\nret = -EEXIST;\r\ngoto out;\r\n}\r\nif (type == KVM_DEV_TYPE_ARM_VGIC_V2 && !vgic->can_emulate_gicv2) {\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\nret = -EBUSY;\r\nkvm_for_each_vcpu(i, vcpu, kvm) {\r\nif (!mutex_trylock(&vcpu->mutex))\r\ngoto out_unlock;\r\nvcpu_lock_idx = i;\r\n}\r\nkvm_for_each_vcpu(i, vcpu, kvm) {\r\nif (vcpu->arch.has_run_once)\r\ngoto out_unlock;\r\n}\r\nret = 0;\r\nret = init_vgic_model(kvm, type);\r\nif (ret)\r\ngoto out_unlock;\r\nkvm->arch.vgic.in_kernel = true;\r\nkvm->arch.vgic.vgic_model = type;\r\nkvm->arch.vgic.vctrl_base = vgic->vctrl_base;\r\nkvm->arch.vgic.vgic_dist_base = VGIC_ADDR_UNDEF;\r\nkvm->arch.vgic.vgic_cpu_base = VGIC_ADDR_UNDEF;\r\nkvm->arch.vgic.vgic_redist_base = VGIC_ADDR_UNDEF;\r\nout_unlock:\r\nfor (; vcpu_lock_idx >= 0; vcpu_lock_idx--) {\r\nvcpu = kvm_get_vcpu(kvm, vcpu_lock_idx);\r\nmutex_unlock(&vcpu->mutex);\r\n}\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nstatic int vgic_ioaddr_overlap(struct kvm *kvm)\r\n{\r\nphys_addr_t dist = kvm->arch.vgic.vgic_dist_base;\r\nphys_addr_t cpu = kvm->arch.vgic.vgic_cpu_base;\r\nif (IS_VGIC_ADDR_UNDEF(dist) || IS_VGIC_ADDR_UNDEF(cpu))\r\nreturn 0;\r\nif ((dist <= cpu && dist + KVM_VGIC_V2_DIST_SIZE > cpu) ||\r\n(cpu <= dist && cpu + KVM_VGIC_V2_CPU_SIZE > dist))\r\nreturn -EBUSY;\r\nreturn 0;\r\n}\r\nstatic int vgic_ioaddr_assign(struct kvm *kvm, phys_addr_t *ioaddr,\r\nphys_addr_t addr, phys_addr_t size)\r\n{\r\nint ret;\r\nif (addr & ~KVM_PHYS_MASK)\r\nreturn -E2BIG;\r\nif (addr & (SZ_4K - 1))\r\nreturn -EINVAL;\r\nif (!IS_VGIC_ADDR_UNDEF(*ioaddr))\r\nreturn -EEXIST;\r\nif (addr + size < addr)\r\nreturn -EINVAL;\r\n*ioaddr = addr;\r\nret = vgic_ioaddr_overlap(kvm);\r\nif (ret)\r\n*ioaddr = VGIC_ADDR_UNDEF;\r\nreturn ret;\r\n}\r\nint kvm_vgic_addr(struct kvm *kvm, unsigned long type, u64 *addr, bool write)\r\n{\r\nint r = 0;\r\nstruct vgic_dist *vgic = &kvm->arch.vgic;\r\nint type_needed;\r\nphys_addr_t *addr_ptr, block_size;\r\nphys_addr_t alignment;\r\nmutex_lock(&kvm->lock);\r\nswitch (type) {\r\ncase KVM_VGIC_V2_ADDR_TYPE_DIST:\r\ntype_needed = KVM_DEV_TYPE_ARM_VGIC_V2;\r\naddr_ptr = &vgic->vgic_dist_base;\r\nblock_size = KVM_VGIC_V2_DIST_SIZE;\r\nalignment = SZ_4K;\r\nbreak;\r\ncase KVM_VGIC_V2_ADDR_TYPE_CPU:\r\ntype_needed = KVM_DEV_TYPE_ARM_VGIC_V2;\r\naddr_ptr = &vgic->vgic_cpu_base;\r\nblock_size = KVM_VGIC_V2_CPU_SIZE;\r\nalignment = SZ_4K;\r\nbreak;\r\n#ifdef CONFIG_KVM_ARM_VGIC_V3\r\ncase KVM_VGIC_V3_ADDR_TYPE_DIST:\r\ntype_needed = KVM_DEV_TYPE_ARM_VGIC_V3;\r\naddr_ptr = &vgic->vgic_dist_base;\r\nblock_size = KVM_VGIC_V3_DIST_SIZE;\r\nalignment = SZ_64K;\r\nbreak;\r\ncase KVM_VGIC_V3_ADDR_TYPE_REDIST:\r\ntype_needed = KVM_DEV_TYPE_ARM_VGIC_V3;\r\naddr_ptr = &vgic->vgic_redist_base;\r\nblock_size = KVM_VGIC_V3_REDIST_SIZE;\r\nalignment = SZ_64K;\r\nbreak;\r\n#endif\r\ndefault:\r\nr = -ENODEV;\r\ngoto out;\r\n}\r\nif (vgic->vgic_model != type_needed) {\r\nr = -ENODEV;\r\ngoto out;\r\n}\r\nif (write) {\r\nif (!IS_ALIGNED(*addr, alignment))\r\nr = -EINVAL;\r\nelse\r\nr = vgic_ioaddr_assign(kvm, addr_ptr, *addr,\r\nblock_size);\r\n} else {\r\n*addr = *addr_ptr;\r\n}\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nint vgic_set_common_attr(struct kvm_device *dev, struct kvm_device_attr *attr)\r\n{\r\nint r;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_ADDR: {\r\nu64 __user *uaddr = (u64 __user *)(long)attr->addr;\r\nu64 addr;\r\nunsigned long type = (unsigned long)attr->attr;\r\nif (copy_from_user(&addr, uaddr, sizeof(addr)))\r\nreturn -EFAULT;\r\nr = kvm_vgic_addr(dev->kvm, type, &addr, true);\r\nreturn (r == -ENODEV) ? -ENXIO : r;\r\n}\r\ncase KVM_DEV_ARM_VGIC_GRP_NR_IRQS: {\r\nu32 __user *uaddr = (u32 __user *)(long)attr->addr;\r\nu32 val;\r\nint ret = 0;\r\nif (get_user(val, uaddr))\r\nreturn -EFAULT;\r\nif (val < (VGIC_NR_PRIVATE_IRQS + 32) ||\r\nval > VGIC_MAX_IRQS ||\r\n(val & 31))\r\nreturn -EINVAL;\r\nmutex_lock(&dev->kvm->lock);\r\nif (vgic_ready(dev->kvm) || dev->kvm->arch.vgic.nr_irqs)\r\nret = -EBUSY;\r\nelse\r\ndev->kvm->arch.vgic.nr_irqs = val;\r\nmutex_unlock(&dev->kvm->lock);\r\nreturn ret;\r\n}\r\ncase KVM_DEV_ARM_VGIC_GRP_CTRL: {\r\nswitch (attr->attr) {\r\ncase KVM_DEV_ARM_VGIC_CTRL_INIT:\r\nr = vgic_init(dev->kvm);\r\nreturn r;\r\n}\r\nbreak;\r\n}\r\n}\r\nreturn -ENXIO;\r\n}\r\nint vgic_get_common_attr(struct kvm_device *dev, struct kvm_device_attr *attr)\r\n{\r\nint r = -ENXIO;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_ADDR: {\r\nu64 __user *uaddr = (u64 __user *)(long)attr->addr;\r\nu64 addr;\r\nunsigned long type = (unsigned long)attr->attr;\r\nr = kvm_vgic_addr(dev->kvm, type, &addr, false);\r\nif (r)\r\nreturn (r == -ENODEV) ? -ENXIO : r;\r\nif (copy_to_user(uaddr, &addr, sizeof(addr)))\r\nreturn -EFAULT;\r\nbreak;\r\n}\r\ncase KVM_DEV_ARM_VGIC_GRP_NR_IRQS: {\r\nu32 __user *uaddr = (u32 __user *)(long)attr->addr;\r\nr = put_user(dev->kvm->arch.vgic.nr_irqs, uaddr);\r\nbreak;\r\n}\r\n}\r\nreturn r;\r\n}\r\nint vgic_has_attr_regs(const struct vgic_io_range *ranges, phys_addr_t offset)\r\n{\r\nif (vgic_find_range(ranges, 4, offset))\r\nreturn 0;\r\nelse\r\nreturn -ENXIO;\r\n}\r\nstatic void vgic_init_maintenance_interrupt(void *info)\r\n{\r\nenable_percpu_irq(vgic->maint_irq, 0);\r\n}\r\nstatic int vgic_cpu_notify(struct notifier_block *self,\r\nunsigned long action, void *cpu)\r\n{\r\nswitch (action) {\r\ncase CPU_STARTING:\r\ncase CPU_STARTING_FROZEN:\r\nvgic_init_maintenance_interrupt(NULL);\r\nbreak;\r\ncase CPU_DYING:\r\ncase CPU_DYING_FROZEN:\r\ndisable_percpu_irq(vgic->maint_irq);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nint kvm_vgic_hyp_init(void)\r\n{\r\nconst struct of_device_id *matched_id;\r\nconst int (*vgic_probe)(struct device_node *,const struct vgic_ops **,\r\nconst struct vgic_params **);\r\nstruct device_node *vgic_node;\r\nint ret;\r\nvgic_node = of_find_matching_node_and_match(NULL,\r\nvgic_ids, &matched_id);\r\nif (!vgic_node) {\r\nkvm_err("error: no compatible GIC node found\n");\r\nreturn -ENODEV;\r\n}\r\nvgic_probe = matched_id->data;\r\nret = vgic_probe(vgic_node, &vgic_ops, &vgic);\r\nif (ret)\r\nreturn ret;\r\nret = request_percpu_irq(vgic->maint_irq, vgic_maintenance_handler,\r\n"vgic", kvm_get_running_vcpus());\r\nif (ret) {\r\nkvm_err("Cannot register interrupt %d\n", vgic->maint_irq);\r\nreturn ret;\r\n}\r\nret = __register_cpu_notifier(&vgic_cpu_nb);\r\nif (ret) {\r\nkvm_err("Cannot register vgic CPU notifier\n");\r\ngoto out_free_irq;\r\n}\r\non_each_cpu(vgic_init_maintenance_interrupt, NULL, 1);\r\nreturn 0;\r\nout_free_irq:\r\nfree_percpu_irq(vgic->maint_irq, kvm_get_running_vcpus());\r\nreturn ret;\r\n}\r\nint kvm_irq_map_gsi(struct kvm *kvm,\r\nstruct kvm_kernel_irq_routing_entry *entries,\r\nint gsi)\r\n{\r\nreturn 0;\r\n}\r\nint kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)\r\n{\r\nreturn pin;\r\n}\r\nint kvm_set_irq(struct kvm *kvm, int irq_source_id,\r\nu32 irq, int level, bool line_status)\r\n{\r\nunsigned int spi = irq + VGIC_NR_PRIVATE_IRQS;\r\ntrace_kvm_set_irq(irq, level, irq_source_id);\r\nBUG_ON(!vgic_initialized(kvm));\r\nreturn kvm_vgic_inject_irq(kvm, 0, spi, level);\r\n}\r\nint kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,\r\nstruct kvm *kvm, int irq_source_id,\r\nint level, bool line_status)\r\n{\r\nreturn 0;\r\n}
