static unsigned long *in_exception_stack(unsigned cpu, unsigned long stack,\r\nunsigned *usedp, char **idp)\r\n{\r\nunsigned k;\r\nfor (k = 0; k < N_EXCEPTION_STACKS; k++) {\r\nunsigned long end = per_cpu(orig_ist, cpu).ist[k];\r\nif (stack >= end)\r\ncontinue;\r\nif (stack >= end - EXCEPTION_STKSZ) {\r\nif (*usedp & (1U << k))\r\nbreak;\r\n*usedp |= 1U << k;\r\n*idp = x86_stack_ids[k];\r\nreturn (unsigned long *)end;\r\n}\r\n#if DEBUG_STKSZ > EXCEPTION_STKSZ\r\nif (k == DEBUG_STACK - 1 && stack >= end - DEBUG_STKSZ) {\r\nunsigned j = N_EXCEPTION_STACKS - 1;\r\ndo {\r\n++j;\r\nend -= EXCEPTION_STKSZ;\r\nx86_stack_ids[j][4] = '1' +\r\n(j - N_EXCEPTION_STACKS);\r\n} while (stack < end - EXCEPTION_STKSZ);\r\nif (*usedp & (1U << j))\r\nbreak;\r\n*usedp |= 1U << j;\r\n*idp = x86_stack_ids[j];\r\nreturn (unsigned long *)end;\r\n}\r\n#endif\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline int\r\nin_irq_stack(unsigned long *stack, unsigned long *irq_stack,\r\nunsigned long *irq_stack_end)\r\n{\r\nreturn (stack >= irq_stack && stack < irq_stack_end);\r\n}\r\nstatic enum stack_type\r\nanalyze_stack(int cpu, struct task_struct *task, unsigned long *stack,\r\nunsigned long **stack_end, unsigned long *irq_stack,\r\nunsigned *used, char **id)\r\n{\r\nunsigned long addr;\r\naddr = ((unsigned long)stack & (~(THREAD_SIZE - 1)));\r\nif ((unsigned long)task_stack_page(task) == addr)\r\nreturn STACK_IS_NORMAL;\r\n*stack_end = in_exception_stack(cpu, (unsigned long)stack,\r\nused, id);\r\nif (*stack_end)\r\nreturn STACK_IS_EXCEPTION;\r\nif (!irq_stack)\r\nreturn STACK_IS_NORMAL;\r\n*stack_end = irq_stack;\r\nirq_stack = irq_stack - irq_stack_size;\r\nif (in_irq_stack(stack, irq_stack, *stack_end))\r\nreturn STACK_IS_IRQ;\r\nreturn STACK_IS_UNKNOWN;\r\n}\r\nvoid dump_trace(struct task_struct *task, struct pt_regs *regs,\r\nunsigned long *stack, unsigned long bp,\r\nconst struct stacktrace_ops *ops, void *data)\r\n{\r\nconst unsigned cpu = get_cpu();\r\nstruct thread_info *tinfo;\r\nunsigned long *irq_stack = (unsigned long *)per_cpu(irq_stack_ptr, cpu);\r\nunsigned long dummy;\r\nunsigned used = 0;\r\nint graph = 0;\r\nint done = 0;\r\nif (!task)\r\ntask = current;\r\nif (!stack) {\r\nif (regs)\r\nstack = (unsigned long *)regs->sp;\r\nelse if (task != current)\r\nstack = (unsigned long *)task->thread.sp;\r\nelse\r\nstack = &dummy;\r\n}\r\nif (!bp)\r\nbp = stack_frame(task, regs);\r\ntinfo = task_thread_info(task);\r\nwhile (!done) {\r\nunsigned long *stack_end;\r\nenum stack_type stype;\r\nchar *id;\r\nstype = analyze_stack(cpu, task, stack, &stack_end,\r\nirq_stack, &used, &id);\r\ndone = 1;\r\nswitch (stype) {\r\ncase STACK_IS_NORMAL:\r\nbreak;\r\ncase STACK_IS_EXCEPTION:\r\nif (ops->stack(data, id) < 0)\r\nbreak;\r\nbp = ops->walk_stack(tinfo, stack, bp, ops,\r\ndata, stack_end, &graph);\r\nops->stack(data, "<EOE>");\r\nstack = (unsigned long *) stack_end[-2];\r\ndone = 0;\r\nbreak;\r\ncase STACK_IS_IRQ:\r\nif (ops->stack(data, "IRQ") < 0)\r\nbreak;\r\nbp = ops->walk_stack(tinfo, stack, bp,\r\nops, data, stack_end, &graph);\r\nstack = (unsigned long *) (stack_end[-1]);\r\nirq_stack = NULL;\r\nops->stack(data, "EOI");\r\ndone = 0;\r\nbreak;\r\ncase STACK_IS_UNKNOWN:\r\nops->stack(data, "UNK");\r\nbreak;\r\n}\r\n}\r\nbp = ops->walk_stack(tinfo, stack, bp, ops, data, NULL, &graph);\r\nput_cpu();\r\n}\r\nvoid\r\nshow_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,\r\nunsigned long *sp, unsigned long bp, char *log_lvl)\r\n{\r\nunsigned long *irq_stack_end;\r\nunsigned long *irq_stack;\r\nunsigned long *stack;\r\nint cpu;\r\nint i;\r\npreempt_disable();\r\ncpu = smp_processor_id();\r\nirq_stack_end = (unsigned long *)(per_cpu(irq_stack_ptr, cpu));\r\nirq_stack = (unsigned long *)(per_cpu(irq_stack_ptr, cpu) - IRQ_STACK_SIZE);\r\nif (sp == NULL) {\r\nif (task)\r\nsp = (unsigned long *)task->thread.sp;\r\nelse\r\nsp = (unsigned long *)&sp;\r\n}\r\nstack = sp;\r\nfor (i = 0; i < kstack_depth_to_print; i++) {\r\nif (stack >= irq_stack && stack <= irq_stack_end) {\r\nif (stack == irq_stack_end) {\r\nstack = (unsigned long *) (irq_stack_end[-1]);\r\npr_cont(" <EOI> ");\r\n}\r\n} else {\r\nif (kstack_end(stack))\r\nbreak;\r\n}\r\nif ((i % STACKSLOTS_PER_LINE) == 0) {\r\nif (i != 0)\r\npr_cont("\n");\r\nprintk("%s %016lx", log_lvl, *stack++);\r\n} else\r\npr_cont(" %016lx", *stack++);\r\ntouch_nmi_watchdog();\r\n}\r\npreempt_enable();\r\npr_cont("\n");\r\nshow_trace_log_lvl(task, regs, sp, bp, log_lvl);\r\n}\r\nvoid show_regs(struct pt_regs *regs)\r\n{\r\nint i;\r\nunsigned long sp;\r\nsp = regs->sp;\r\nshow_regs_print_info(KERN_DEFAULT);\r\n__show_regs(regs, 1);\r\nif (!user_mode(regs)) {\r\nunsigned int code_prologue = code_bytes * 43 / 64;\r\nunsigned int code_len = code_bytes;\r\nunsigned char c;\r\nu8 *ip;\r\nprintk(KERN_DEFAULT "Stack:\n");\r\nshow_stack_log_lvl(NULL, regs, (unsigned long *)sp,\r\n0, KERN_DEFAULT);\r\nprintk(KERN_DEFAULT "Code: ");\r\nip = (u8 *)regs->ip - code_prologue;\r\nif (ip < (u8 *)PAGE_OFFSET || probe_kernel_address(ip, c)) {\r\nip = (u8 *)regs->ip;\r\ncode_len = code_len - code_prologue + 1;\r\n}\r\nfor (i = 0; i < code_len; i++, ip++) {\r\nif (ip < (u8 *)PAGE_OFFSET ||\r\nprobe_kernel_address(ip, c)) {\r\npr_cont(" Bad RIP value.");\r\nbreak;\r\n}\r\nif (ip == (u8 *)regs->ip)\r\npr_cont("<%02x> ", c);\r\nelse\r\npr_cont("%02x ", c);\r\n}\r\n}\r\npr_cont("\n");\r\n}\r\nint is_valid_bugaddr(unsigned long ip)\r\n{\r\nunsigned short ud2;\r\nif (__copy_from_user(&ud2, (const void __user *) ip, sizeof(ud2)))\r\nreturn 0;\r\nreturn ud2 == 0x0b0f;\r\n}
