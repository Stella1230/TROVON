void nfp_net_get_fw_version(struct nfp_net_fw_version *fw_ver,\r\nvoid __iomem *ctrl_bar)\r\n{\r\nu32 reg;\r\nreg = readl(ctrl_bar + NFP_NET_CFG_VERSION);\r\nput_unaligned_le32(reg, fw_ver);\r\n}\r\nint nfp_net_reconfig(struct nfp_net *nn, u32 update)\r\n{\r\nint cnt, ret = 0;\r\nu32 new;\r\nspin_lock_bh(&nn->reconfig_lock);\r\nnn_writel(nn, NFP_NET_CFG_UPDATE, update);\r\nnn_pci_flush(nn);\r\nnfp_qcp_wr_ptr_add(nn->qcp_cfg, 1);\r\nfor (cnt = 0; ; cnt++) {\r\nnew = nn_readl(nn, NFP_NET_CFG_UPDATE);\r\nif (new == 0)\r\nbreak;\r\nif (new & NFP_NET_CFG_UPDATE_ERR) {\r\nnn_err(nn, "Reconfig error: 0x%08x\n", new);\r\nret = -EIO;\r\nbreak;\r\n} else if (cnt >= NFP_NET_POLL_TIMEOUT) {\r\nnn_err(nn, "Reconfig timeout for 0x%08x after %dms\n",\r\nupdate, cnt);\r\nret = -EIO;\r\nbreak;\r\n}\r\nmdelay(1);\r\n}\r\nspin_unlock_bh(&nn->reconfig_lock);\r\nreturn ret;\r\n}\r\nstatic void nfp_net_irq_unmask_msix(struct nfp_net *nn, unsigned int entry_nr)\r\n{\r\nstruct list_head *msi_head = &nn->pdev->dev.msi_list;\r\nstruct msi_desc *entry;\r\nu32 off;\r\nentry = list_first_entry(msi_head, struct msi_desc, list);\r\noff = (PCI_MSIX_ENTRY_SIZE * entry_nr) +\r\nPCI_MSIX_ENTRY_VECTOR_CTRL;\r\nwritel(0, entry->mask_base + off);\r\nreadl(entry->mask_base);\r\n}\r\nstatic void nfp_net_irq_unmask(struct nfp_net *nn, unsigned int entry_nr)\r\n{\r\nif (nn->ctrl & NFP_NET_CFG_CTRL_MSIXAUTO) {\r\nnfp_net_irq_unmask_msix(nn, entry_nr);\r\nreturn;\r\n}\r\nnn_writeb(nn, NFP_NET_CFG_ICR(entry_nr), NFP_NET_CFG_ICR_UNMASKED);\r\nnn_pci_flush(nn);\r\n}\r\nstatic int nfp_net_msix_alloc(struct nfp_net *nn, int nr_vecs)\r\n{\r\nstruct pci_dev *pdev = nn->pdev;\r\nint nvecs;\r\nint i;\r\nfor (i = 0; i < nr_vecs; i++)\r\nnn->irq_entries[i].entry = i;\r\nnvecs = pci_enable_msix_range(pdev, nn->irq_entries,\r\nNFP_NET_NON_Q_VECTORS + 1, nr_vecs);\r\nif (nvecs < 0) {\r\nnn_warn(nn, "Failed to enable MSI-X. Wanted %d-%d (err=%d)\n",\r\nNFP_NET_NON_Q_VECTORS + 1, nr_vecs, nvecs);\r\nreturn 0;\r\n}\r\nreturn nvecs;\r\n}\r\nstatic int nfp_net_irqs_wanted(struct nfp_net *nn)\r\n{\r\nint ncpus;\r\nint vecs;\r\nncpus = num_online_cpus();\r\nvecs = max_t(int, nn->num_tx_rings, nn->num_rx_rings);\r\nvecs = min_t(int, vecs, ncpus);\r\nreturn vecs + NFP_NET_NON_Q_VECTORS;\r\n}\r\nint nfp_net_irqs_alloc(struct nfp_net *nn)\r\n{\r\nint wanted_irqs;\r\nwanted_irqs = nfp_net_irqs_wanted(nn);\r\nnn->num_irqs = nfp_net_msix_alloc(nn, wanted_irqs);\r\nif (nn->num_irqs == 0) {\r\nnn_err(nn, "Failed to allocate MSI-X IRQs\n");\r\nreturn 0;\r\n}\r\nnn->num_r_vecs = nn->num_irqs - NFP_NET_NON_Q_VECTORS;\r\nif (nn->num_irqs < wanted_irqs)\r\nnn_warn(nn, "Unable to allocate %d vectors. Got %d instead\n",\r\nwanted_irqs, nn->num_irqs);\r\nreturn nn->num_irqs;\r\n}\r\nvoid nfp_net_irqs_disable(struct nfp_net *nn)\r\n{\r\npci_disable_msix(nn->pdev);\r\n}\r\nstatic irqreturn_t nfp_net_irq_rxtx(int irq, void *data)\r\n{\r\nstruct nfp_net_r_vector *r_vec = data;\r\nnapi_schedule_irqoff(&r_vec->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void nfp_net_read_link_status(struct nfp_net *nn)\r\n{\r\nunsigned long flags;\r\nbool link_up;\r\nu32 sts;\r\nspin_lock_irqsave(&nn->link_status_lock, flags);\r\nsts = nn_readl(nn, NFP_NET_CFG_STS);\r\nlink_up = !!(sts & NFP_NET_CFG_STS_LINK);\r\nif (nn->link_up == link_up)\r\ngoto out;\r\nnn->link_up = link_up;\r\nif (nn->link_up) {\r\nnetif_carrier_on(nn->netdev);\r\nnetdev_info(nn->netdev, "NIC Link is Up\n");\r\n} else {\r\nnetif_carrier_off(nn->netdev);\r\nnetdev_info(nn->netdev, "NIC Link is Down\n");\r\n}\r\nout:\r\nspin_unlock_irqrestore(&nn->link_status_lock, flags);\r\n}\r\nstatic irqreturn_t nfp_net_irq_lsc(int irq, void *data)\r\n{\r\nstruct nfp_net *nn = data;\r\nnfp_net_read_link_status(nn);\r\nnfp_net_irq_unmask(nn, NFP_NET_IRQ_LSC_IDX);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t nfp_net_irq_exn(int irq, void *data)\r\n{\r\nstruct nfp_net *nn = data;\r\nnn_err(nn, "%s: UNIMPLEMENTED.\n", __func__);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void nfp_net_tx_ring_init(struct nfp_net_tx_ring *tx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = tx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\ntx_ring->qcidx = tx_ring->idx * nn->stride_tx;\r\ntx_ring->qcp_q = nn->tx_bar + NFP_QCP_QUEUE_OFF(tx_ring->qcidx);\r\n}\r\nstatic void nfp_net_rx_ring_init(struct nfp_net_rx_ring *rx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = rx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nrx_ring->fl_qcidx = rx_ring->idx * nn->stride_rx;\r\nrx_ring->rx_qcidx = rx_ring->fl_qcidx + (nn->stride_rx - 1);\r\nrx_ring->qcp_fl = nn->rx_bar + NFP_QCP_QUEUE_OFF(rx_ring->fl_qcidx);\r\nrx_ring->qcp_rx = nn->rx_bar + NFP_QCP_QUEUE_OFF(rx_ring->rx_qcidx);\r\n}\r\nstatic void nfp_net_irqs_assign(struct net_device *netdev)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nstruct nfp_net_r_vector *r_vec;\r\nint r;\r\nif (nn->num_tx_rings > nn->num_r_vecs) {\r\nnn_warn(nn, "More rings (%d) than vectors (%d).\n",\r\nnn->num_tx_rings, nn->num_r_vecs);\r\nnn->num_tx_rings = nn->num_r_vecs;\r\nnn->num_rx_rings = nn->num_r_vecs;\r\n}\r\nnn->lsc_handler = nfp_net_irq_lsc;\r\nnn->exn_handler = nfp_net_irq_exn;\r\nfor (r = 0; r < nn->num_r_vecs; r++) {\r\nr_vec = &nn->r_vecs[r];\r\nr_vec->nfp_net = nn;\r\nr_vec->handler = nfp_net_irq_rxtx;\r\nr_vec->irq_idx = NFP_NET_NON_Q_VECTORS + r;\r\ncpumask_set_cpu(r, &r_vec->affinity_mask);\r\nr_vec->tx_ring = &nn->tx_rings[r];\r\nnn->tx_rings[r].idx = r;\r\nnn->tx_rings[r].r_vec = r_vec;\r\nnfp_net_tx_ring_init(r_vec->tx_ring);\r\nr_vec->rx_ring = &nn->rx_rings[r];\r\nnn->rx_rings[r].idx = r;\r\nnn->rx_rings[r].r_vec = r_vec;\r\nnfp_net_rx_ring_init(r_vec->rx_ring);\r\n}\r\n}\r\nstatic int\r\nnfp_net_aux_irq_request(struct nfp_net *nn, u32 ctrl_offset,\r\nconst char *format, char *name, size_t name_sz,\r\nunsigned int vector_idx, irq_handler_t handler)\r\n{\r\nstruct msix_entry *entry;\r\nint err;\r\nentry = &nn->irq_entries[vector_idx];\r\nsnprintf(name, name_sz, format, netdev_name(nn->netdev));\r\nerr = request_irq(entry->vector, handler, 0, name, nn);\r\nif (err) {\r\nnn_err(nn, "Failed to request IRQ %d (err=%d).\n",\r\nentry->vector, err);\r\nreturn err;\r\n}\r\nnn_writeb(nn, ctrl_offset, vector_idx);\r\nreturn 0;\r\n}\r\nstatic void nfp_net_aux_irq_free(struct nfp_net *nn, u32 ctrl_offset,\r\nunsigned int vector_idx)\r\n{\r\nnn_writeb(nn, ctrl_offset, 0xff);\r\nfree_irq(nn->irq_entries[vector_idx].vector, nn);\r\n}\r\nstatic inline int nfp_net_tx_full(struct nfp_net_tx_ring *tx_ring, int dcnt)\r\n{\r\nreturn (tx_ring->wr_p - tx_ring->rd_p) >= (tx_ring->cnt - dcnt);\r\n}\r\nstatic int nfp_net_tx_ring_should_wake(struct nfp_net_tx_ring *tx_ring)\r\n{\r\nreturn !nfp_net_tx_full(tx_ring, MAX_SKB_FRAGS * 4);\r\n}\r\nstatic int nfp_net_tx_ring_should_stop(struct nfp_net_tx_ring *tx_ring)\r\n{\r\nreturn nfp_net_tx_full(tx_ring, MAX_SKB_FRAGS + 1);\r\n}\r\nstatic void nfp_net_tx_ring_stop(struct netdev_queue *nd_q,\r\nstruct nfp_net_tx_ring *tx_ring)\r\n{\r\nnetif_tx_stop_queue(nd_q);\r\nsmp_mb();\r\nif (unlikely(nfp_net_tx_ring_should_wake(tx_ring)))\r\nnetif_tx_start_queue(nd_q);\r\n}\r\nstatic void nfp_net_tx_tso(struct nfp_net *nn, struct nfp_net_r_vector *r_vec,\r\nstruct nfp_net_tx_buf *txbuf,\r\nstruct nfp_net_tx_desc *txd, struct sk_buff *skb)\r\n{\r\nu32 hdrlen;\r\nu16 mss;\r\nif (!skb_is_gso(skb))\r\nreturn;\r\nif (!skb->encapsulation)\r\nhdrlen = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nelse\r\nhdrlen = skb_inner_transport_header(skb) - skb->data +\r\ninner_tcp_hdrlen(skb);\r\ntxbuf->pkt_cnt = skb_shinfo(skb)->gso_segs;\r\ntxbuf->real_len += hdrlen * (txbuf->pkt_cnt - 1);\r\nmss = skb_shinfo(skb)->gso_size & PCIE_DESC_TX_MSS_MASK;\r\ntxd->l4_offset = hdrlen;\r\ntxd->mss = cpu_to_le16(mss);\r\ntxd->flags |= PCIE_DESC_TX_LSO;\r\nu64_stats_update_begin(&r_vec->tx_sync);\r\nr_vec->tx_lso++;\r\nu64_stats_update_end(&r_vec->tx_sync);\r\n}\r\nstatic void nfp_net_tx_csum(struct nfp_net *nn, struct nfp_net_r_vector *r_vec,\r\nstruct nfp_net_tx_buf *txbuf,\r\nstruct nfp_net_tx_desc *txd, struct sk_buff *skb)\r\n{\r\nstruct ipv6hdr *ipv6h;\r\nstruct iphdr *iph;\r\nu8 l4_hdr;\r\nif (!(nn->ctrl & NFP_NET_CFG_CTRL_TXCSUM))\r\nreturn;\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nreturn;\r\ntxd->flags |= PCIE_DESC_TX_CSUM;\r\nif (skb->encapsulation)\r\ntxd->flags |= PCIE_DESC_TX_ENCAP;\r\niph = skb->encapsulation ? inner_ip_hdr(skb) : ip_hdr(skb);\r\nipv6h = skb->encapsulation ? inner_ipv6_hdr(skb) : ipv6_hdr(skb);\r\nif (iph->version == 4) {\r\ntxd->flags |= PCIE_DESC_TX_IP4_CSUM;\r\nl4_hdr = iph->protocol;\r\n} else if (ipv6h->version == 6) {\r\nl4_hdr = ipv6h->nexthdr;\r\n} else {\r\nnn_warn_ratelimit(nn, "partial checksum but ipv=%x!\n",\r\niph->version);\r\nreturn;\r\n}\r\nswitch (l4_hdr) {\r\ncase IPPROTO_TCP:\r\ntxd->flags |= PCIE_DESC_TX_TCP_CSUM;\r\nbreak;\r\ncase IPPROTO_UDP:\r\ntxd->flags |= PCIE_DESC_TX_UDP_CSUM;\r\nbreak;\r\ndefault:\r\nnn_warn_ratelimit(nn, "partial checksum but l4 proto=%x!\n",\r\nl4_hdr);\r\nreturn;\r\n}\r\nu64_stats_update_begin(&r_vec->tx_sync);\r\nif (skb->encapsulation)\r\nr_vec->hw_csum_tx_inner += txbuf->pkt_cnt;\r\nelse\r\nr_vec->hw_csum_tx += txbuf->pkt_cnt;\r\nu64_stats_update_end(&r_vec->tx_sync);\r\n}\r\nstatic int nfp_net_tx(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nconst struct skb_frag_struct *frag;\r\nstruct nfp_net_r_vector *r_vec;\r\nstruct nfp_net_tx_desc *txd, txdg;\r\nstruct nfp_net_tx_buf *txbuf;\r\nstruct nfp_net_tx_ring *tx_ring;\r\nstruct netdev_queue *nd_q;\r\ndma_addr_t dma_addr;\r\nunsigned int fsize;\r\nint f, nr_frags;\r\nint wr_idx;\r\nu16 qidx;\r\nqidx = skb_get_queue_mapping(skb);\r\ntx_ring = &nn->tx_rings[qidx];\r\nr_vec = tx_ring->r_vec;\r\nnd_q = netdev_get_tx_queue(nn->netdev, qidx);\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\nif (unlikely(nfp_net_tx_full(tx_ring, nr_frags + 1))) {\r\nnn_warn_ratelimit(nn, "TX ring %d busy. wrp=%u rdp=%u\n",\r\nqidx, tx_ring->wr_p, tx_ring->rd_p);\r\nnetif_tx_stop_queue(nd_q);\r\nu64_stats_update_begin(&r_vec->tx_sync);\r\nr_vec->tx_busy++;\r\nu64_stats_update_end(&r_vec->tx_sync);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\ndma_addr = dma_map_single(&nn->pdev->dev, skb->data, skb_headlen(skb),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&nn->pdev->dev, dma_addr))\r\ngoto err_free;\r\nwr_idx = tx_ring->wr_p % tx_ring->cnt;\r\ntxbuf = &tx_ring->txbufs[wr_idx];\r\ntxbuf->skb = skb;\r\ntxbuf->dma_addr = dma_addr;\r\ntxbuf->fidx = -1;\r\ntxbuf->pkt_cnt = 1;\r\ntxbuf->real_len = skb->len;\r\ntxd = &tx_ring->txds[wr_idx];\r\ntxd->offset_eop = (nr_frags == 0) ? PCIE_DESC_TX_EOP : 0;\r\ntxd->dma_len = cpu_to_le16(skb_headlen(skb));\r\nnfp_desc_set_dma_addr(txd, dma_addr);\r\ntxd->data_len = cpu_to_le16(skb->len);\r\ntxd->flags = 0;\r\ntxd->mss = 0;\r\ntxd->l4_offset = 0;\r\nnfp_net_tx_tso(nn, r_vec, txbuf, txd, skb);\r\nnfp_net_tx_csum(nn, r_vec, txbuf, txd, skb);\r\nif (skb_vlan_tag_present(skb) && nn->ctrl & NFP_NET_CFG_CTRL_TXVLAN) {\r\ntxd->flags |= PCIE_DESC_TX_VLAN;\r\ntxd->vlan = cpu_to_le16(skb_vlan_tag_get(skb));\r\n}\r\nif (nr_frags > 0) {\r\ntxdg = *txd;\r\nfor (f = 0; f < nr_frags; f++) {\r\nfrag = &skb_shinfo(skb)->frags[f];\r\nfsize = skb_frag_size(frag);\r\ndma_addr = skb_frag_dma_map(&nn->pdev->dev, frag, 0,\r\nfsize, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&nn->pdev->dev, dma_addr))\r\ngoto err_unmap;\r\nwr_idx = (wr_idx + 1) % tx_ring->cnt;\r\ntx_ring->txbufs[wr_idx].skb = skb;\r\ntx_ring->txbufs[wr_idx].dma_addr = dma_addr;\r\ntx_ring->txbufs[wr_idx].fidx = f;\r\ntxd = &tx_ring->txds[wr_idx];\r\n*txd = txdg;\r\ntxd->dma_len = cpu_to_le16(fsize);\r\nnfp_desc_set_dma_addr(txd, dma_addr);\r\ntxd->offset_eop =\r\n(f == nr_frags - 1) ? PCIE_DESC_TX_EOP : 0;\r\n}\r\nu64_stats_update_begin(&r_vec->tx_sync);\r\nr_vec->tx_gather++;\r\nu64_stats_update_end(&r_vec->tx_sync);\r\n}\r\nnetdev_tx_sent_queue(nd_q, txbuf->real_len);\r\ntx_ring->wr_p += nr_frags + 1;\r\nif (nfp_net_tx_ring_should_stop(tx_ring))\r\nnfp_net_tx_ring_stop(nd_q, tx_ring);\r\ntx_ring->wr_ptr_add += nr_frags + 1;\r\nif (!skb->xmit_more || netif_xmit_stopped(nd_q)) {\r\nwmb();\r\nnfp_qcp_wr_ptr_add(tx_ring->qcp_q, tx_ring->wr_ptr_add);\r\ntx_ring->wr_ptr_add = 0;\r\n}\r\nskb_tx_timestamp(skb);\r\nreturn NETDEV_TX_OK;\r\nerr_unmap:\r\n--f;\r\nwhile (f >= 0) {\r\nfrag = &skb_shinfo(skb)->frags[f];\r\ndma_unmap_page(&nn->pdev->dev,\r\ntx_ring->txbufs[wr_idx].dma_addr,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\ntx_ring->txbufs[wr_idx].skb = NULL;\r\ntx_ring->txbufs[wr_idx].dma_addr = 0;\r\ntx_ring->txbufs[wr_idx].fidx = -2;\r\nwr_idx = wr_idx - 1;\r\nif (wr_idx < 0)\r\nwr_idx += tx_ring->cnt;\r\n}\r\ndma_unmap_single(&nn->pdev->dev, tx_ring->txbufs[wr_idx].dma_addr,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\ntx_ring->txbufs[wr_idx].skb = NULL;\r\ntx_ring->txbufs[wr_idx].dma_addr = 0;\r\ntx_ring->txbufs[wr_idx].fidx = -2;\r\nerr_free:\r\nnn_warn_ratelimit(nn, "Failed to map DMA TX buffer\n");\r\nu64_stats_update_begin(&r_vec->tx_sync);\r\nr_vec->tx_errors++;\r\nu64_stats_update_end(&r_vec->tx_sync);\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void nfp_net_tx_complete(struct nfp_net_tx_ring *tx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = tx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nconst struct skb_frag_struct *frag;\r\nstruct netdev_queue *nd_q;\r\nu32 done_pkts = 0, done_bytes = 0;\r\nstruct sk_buff *skb;\r\nint todo, nr_frags;\r\nu32 qcp_rd_p;\r\nint fidx;\r\nint idx;\r\nqcp_rd_p = nfp_qcp_rd_ptr_read(tx_ring->qcp_q);\r\nif (qcp_rd_p == tx_ring->qcp_rd_p)\r\nreturn;\r\nif (qcp_rd_p > tx_ring->qcp_rd_p)\r\ntodo = qcp_rd_p - tx_ring->qcp_rd_p;\r\nelse\r\ntodo = qcp_rd_p + tx_ring->cnt - tx_ring->qcp_rd_p;\r\nwhile (todo--) {\r\nidx = tx_ring->rd_p % tx_ring->cnt;\r\ntx_ring->rd_p++;\r\nskb = tx_ring->txbufs[idx].skb;\r\nif (!skb)\r\ncontinue;\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\nfidx = tx_ring->txbufs[idx].fidx;\r\nif (fidx == -1) {\r\ndma_unmap_single(&nn->pdev->dev,\r\ntx_ring->txbufs[idx].dma_addr,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\ndone_pkts += tx_ring->txbufs[idx].pkt_cnt;\r\ndone_bytes += tx_ring->txbufs[idx].real_len;\r\n} else {\r\nfrag = &skb_shinfo(skb)->frags[fidx];\r\ndma_unmap_page(&nn->pdev->dev,\r\ntx_ring->txbufs[idx].dma_addr,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\n}\r\nif (fidx == nr_frags - 1)\r\ndev_kfree_skb_any(skb);\r\ntx_ring->txbufs[idx].dma_addr = 0;\r\ntx_ring->txbufs[idx].skb = NULL;\r\ntx_ring->txbufs[idx].fidx = -2;\r\n}\r\ntx_ring->qcp_rd_p = qcp_rd_p;\r\nu64_stats_update_begin(&r_vec->tx_sync);\r\nr_vec->tx_bytes += done_bytes;\r\nr_vec->tx_pkts += done_pkts;\r\nu64_stats_update_end(&r_vec->tx_sync);\r\nnd_q = netdev_get_tx_queue(nn->netdev, tx_ring->idx);\r\nnetdev_tx_completed_queue(nd_q, done_pkts, done_bytes);\r\nif (nfp_net_tx_ring_should_wake(tx_ring)) {\r\nsmp_mb();\r\nif (unlikely(netif_tx_queue_stopped(nd_q)))\r\nnetif_tx_wake_queue(nd_q);\r\n}\r\nWARN_ONCE(tx_ring->wr_p - tx_ring->rd_p > tx_ring->cnt,\r\n"TX ring corruption rd_p=%u wr_p=%u cnt=%u\n",\r\ntx_ring->rd_p, tx_ring->wr_p, tx_ring->cnt);\r\n}\r\nstatic void nfp_net_tx_flush(struct nfp_net_tx_ring *tx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = tx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nstruct pci_dev *pdev = nn->pdev;\r\nconst struct skb_frag_struct *frag;\r\nstruct netdev_queue *nd_q;\r\nstruct sk_buff *skb;\r\nint nr_frags;\r\nint fidx;\r\nint idx;\r\nwhile (tx_ring->rd_p != tx_ring->wr_p) {\r\nidx = tx_ring->rd_p % tx_ring->cnt;\r\nskb = tx_ring->txbufs[idx].skb;\r\nif (skb) {\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\nfidx = tx_ring->txbufs[idx].fidx;\r\nif (fidx == -1) {\r\ndma_unmap_single(&pdev->dev,\r\ntx_ring->txbufs[idx].dma_addr,\r\nskb_headlen(skb),\r\nDMA_TO_DEVICE);\r\n} else {\r\nfrag = &skb_shinfo(skb)->frags[fidx];\r\ndma_unmap_page(&pdev->dev,\r\ntx_ring->txbufs[idx].dma_addr,\r\nskb_frag_size(frag),\r\nDMA_TO_DEVICE);\r\n}\r\nif (fidx == nr_frags - 1)\r\ndev_kfree_skb_any(skb);\r\ntx_ring->txbufs[idx].dma_addr = 0;\r\ntx_ring->txbufs[idx].skb = NULL;\r\ntx_ring->txbufs[idx].fidx = -2;\r\n}\r\nmemset(&tx_ring->txds[idx], 0, sizeof(tx_ring->txds[idx]));\r\ntx_ring->qcp_rd_p++;\r\ntx_ring->rd_p++;\r\n}\r\nnd_q = netdev_get_tx_queue(nn->netdev, tx_ring->idx);\r\nnetdev_tx_reset_queue(nd_q);\r\n}\r\nstatic void nfp_net_tx_timeout(struct net_device *netdev)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nint i;\r\nfor (i = 0; i < nn->num_tx_rings; i++) {\r\nif (!netif_tx_queue_stopped(netdev_get_tx_queue(netdev, i)))\r\ncontinue;\r\nnn_warn(nn, "TX timeout on ring: %d\n", i);\r\n}\r\nnn_warn(nn, "TX watchdog timeout\n");\r\n}\r\nstatic inline int nfp_net_rx_space(struct nfp_net_rx_ring *rx_ring)\r\n{\r\nreturn (rx_ring->cnt - 1) - (rx_ring->wr_p - rx_ring->rd_p);\r\n}\r\nstatic struct sk_buff *\r\nnfp_net_rx_alloc_one(struct nfp_net_rx_ring *rx_ring, dma_addr_t *dma_addr)\r\n{\r\nstruct nfp_net *nn = rx_ring->r_vec->nfp_net;\r\nstruct sk_buff *skb;\r\nskb = netdev_alloc_skb(nn->netdev, nn->fl_bufsz);\r\nif (!skb) {\r\nnn_warn_ratelimit(nn, "Failed to alloc receive SKB\n");\r\nreturn NULL;\r\n}\r\n*dma_addr = dma_map_single(&nn->pdev->dev, skb->data,\r\nnn->fl_bufsz, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(&nn->pdev->dev, *dma_addr)) {\r\ndev_kfree_skb_any(skb);\r\nnn_warn_ratelimit(nn, "Failed to map DMA RX buffer\n");\r\nreturn NULL;\r\n}\r\nreturn skb;\r\n}\r\nstatic void nfp_net_rx_give_one(struct nfp_net_rx_ring *rx_ring,\r\nstruct sk_buff *skb, dma_addr_t dma_addr)\r\n{\r\nunsigned int wr_idx;\r\nwr_idx = rx_ring->wr_p % rx_ring->cnt;\r\nrx_ring->rxbufs[wr_idx].skb = skb;\r\nrx_ring->rxbufs[wr_idx].dma_addr = dma_addr;\r\nrx_ring->rxds[wr_idx].fld.reserved = 0;\r\nrx_ring->rxds[wr_idx].fld.meta_len_dd = 0;\r\nnfp_desc_set_dma_addr(&rx_ring->rxds[wr_idx].fld, dma_addr);\r\nrx_ring->wr_p++;\r\nrx_ring->wr_ptr_add++;\r\nif (rx_ring->wr_ptr_add >= NFP_NET_FL_BATCH) {\r\nwmb();\r\nnfp_qcp_wr_ptr_add(rx_ring->qcp_fl, rx_ring->wr_ptr_add);\r\nrx_ring->wr_ptr_add = 0;\r\n}\r\n}\r\nstatic void nfp_net_rx_flush(struct nfp_net_rx_ring *rx_ring)\r\n{\r\nstruct nfp_net *nn = rx_ring->r_vec->nfp_net;\r\nstruct pci_dev *pdev = nn->pdev;\r\nint idx;\r\nwhile (rx_ring->rd_p != rx_ring->wr_p) {\r\nidx = rx_ring->rd_p % rx_ring->cnt;\r\nif (rx_ring->rxbufs[idx].skb) {\r\ndma_unmap_single(&pdev->dev,\r\nrx_ring->rxbufs[idx].dma_addr,\r\nnn->fl_bufsz, DMA_FROM_DEVICE);\r\ndev_kfree_skb_any(rx_ring->rxbufs[idx].skb);\r\nrx_ring->rxbufs[idx].dma_addr = 0;\r\nrx_ring->rxbufs[idx].skb = NULL;\r\n}\r\nmemset(&rx_ring->rxds[idx], 0, sizeof(rx_ring->rxds[idx]));\r\nrx_ring->rd_p++;\r\n}\r\n}\r\nstatic int nfp_net_rx_fill_freelist(struct nfp_net_rx_ring *rx_ring)\r\n{\r\nstruct sk_buff *skb;\r\ndma_addr_t dma_addr;\r\nwhile (nfp_net_rx_space(rx_ring)) {\r\nskb = nfp_net_rx_alloc_one(rx_ring, &dma_addr);\r\nif (!skb) {\r\nnfp_net_rx_flush(rx_ring);\r\nreturn -ENOMEM;\r\n}\r\nnfp_net_rx_give_one(rx_ring, skb, dma_addr);\r\n}\r\nreturn 0;\r\n}\r\nstatic int nfp_net_rx_csum_has_errors(u16 flags)\r\n{\r\nu16 csum_all_checked, csum_all_ok;\r\ncsum_all_checked = flags & __PCIE_DESC_RX_CSUM_ALL;\r\ncsum_all_ok = flags & __PCIE_DESC_RX_CSUM_ALL_OK;\r\nreturn csum_all_checked != (csum_all_ok << PCIE_DESC_RX_CSUM_OK_SHIFT);\r\n}\r\nstatic void nfp_net_rx_csum(struct nfp_net *nn, struct nfp_net_r_vector *r_vec,\r\nstruct nfp_net_rx_desc *rxd, struct sk_buff *skb)\r\n{\r\nskb_checksum_none_assert(skb);\r\nif (!(nn->netdev->features & NETIF_F_RXCSUM))\r\nreturn;\r\nif (nfp_net_rx_csum_has_errors(le16_to_cpu(rxd->rxd.flags))) {\r\nu64_stats_update_begin(&r_vec->rx_sync);\r\nr_vec->hw_csum_rx_error++;\r\nu64_stats_update_end(&r_vec->rx_sync);\r\nreturn;\r\n}\r\nif (rxd->rxd.flags & PCIE_DESC_RX_TCP_CSUM_OK ||\r\nrxd->rxd.flags & PCIE_DESC_RX_UDP_CSUM_OK) {\r\n__skb_incr_checksum_unnecessary(skb);\r\nu64_stats_update_begin(&r_vec->rx_sync);\r\nr_vec->hw_csum_rx_ok++;\r\nu64_stats_update_end(&r_vec->rx_sync);\r\n}\r\nif (rxd->rxd.flags & PCIE_DESC_RX_I_TCP_CSUM_OK ||\r\nrxd->rxd.flags & PCIE_DESC_RX_I_UDP_CSUM_OK) {\r\n__skb_incr_checksum_unnecessary(skb);\r\nu64_stats_update_begin(&r_vec->rx_sync);\r\nr_vec->hw_csum_rx_inner_ok++;\r\nu64_stats_update_end(&r_vec->rx_sync);\r\n}\r\n}\r\nstatic void nfp_net_set_hash(struct net_device *netdev, struct sk_buff *skb,\r\nstruct nfp_net_rx_desc *rxd)\r\n{\r\nstruct nfp_net_rx_hash *rx_hash;\r\nif (!(rxd->rxd.flags & PCIE_DESC_RX_RSS) ||\r\n!(netdev->features & NETIF_F_RXHASH))\r\nreturn;\r\nrx_hash = (struct nfp_net_rx_hash *)(skb->data - sizeof(*rx_hash));\r\nswitch (be32_to_cpu(rx_hash->hash_type)) {\r\ncase NFP_NET_RSS_IPV4:\r\ncase NFP_NET_RSS_IPV6:\r\ncase NFP_NET_RSS_IPV6_EX:\r\nskb_set_hash(skb, be32_to_cpu(rx_hash->hash), PKT_HASH_TYPE_L3);\r\nbreak;\r\ndefault:\r\nskb_set_hash(skb, be32_to_cpu(rx_hash->hash), PKT_HASH_TYPE_L4);\r\nbreak;\r\n}\r\n}\r\nstatic int nfp_net_rx(struct nfp_net_rx_ring *rx_ring, int budget)\r\n{\r\nstruct nfp_net_r_vector *r_vec = rx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nunsigned int data_len, meta_len;\r\nint avail = 0, pkts_polled = 0;\r\nstruct sk_buff *skb, *new_skb;\r\nstruct nfp_net_rx_desc *rxd;\r\ndma_addr_t new_dma_addr;\r\nu32 qcp_wr_p;\r\nint idx;\r\nif (nn->is_nfp3200) {\r\nqcp_wr_p = nfp_qcp_wr_ptr_read(rx_ring->qcp_rx);\r\nidx = rx_ring->rd_p % rx_ring->cnt;\r\nif (qcp_wr_p == idx)\r\nreturn 0;\r\nif (qcp_wr_p > idx)\r\navail = qcp_wr_p - idx;\r\nelse\r\navail = qcp_wr_p + rx_ring->cnt - idx;\r\n} else {\r\navail = budget + 1;\r\n}\r\nwhile (avail > 0 && pkts_polled < budget) {\r\nidx = rx_ring->rd_p % rx_ring->cnt;\r\nrxd = &rx_ring->rxds[idx];\r\nif (!(rxd->rxd.meta_len_dd & PCIE_DESC_RX_DD)) {\r\nif (nn->is_nfp3200)\r\nnn_dbg(nn, "RX descriptor not valid (DD)%d:%u rxd[0]=%#x rxd[1]=%#x\n",\r\nrx_ring->idx, idx,\r\nrxd->vals[0], rxd->vals[1]);\r\nbreak;\r\n}\r\ndma_rmb();\r\nrx_ring->rd_p++;\r\npkts_polled++;\r\navail--;\r\nskb = rx_ring->rxbufs[idx].skb;\r\nnew_skb = nfp_net_rx_alloc_one(rx_ring, &new_dma_addr);\r\nif (!new_skb) {\r\nnfp_net_rx_give_one(rx_ring, rx_ring->rxbufs[idx].skb,\r\nrx_ring->rxbufs[idx].dma_addr);\r\nu64_stats_update_begin(&r_vec->rx_sync);\r\nr_vec->rx_drops++;\r\nu64_stats_update_end(&r_vec->rx_sync);\r\ncontinue;\r\n}\r\ndma_unmap_single(&nn->pdev->dev,\r\nrx_ring->rxbufs[idx].dma_addr,\r\nnn->fl_bufsz, DMA_FROM_DEVICE);\r\nnfp_net_rx_give_one(rx_ring, new_skb, new_dma_addr);\r\nmeta_len = rxd->rxd.meta_len_dd & PCIE_DESC_RX_META_LEN_MASK;\r\ndata_len = le16_to_cpu(rxd->rxd.data_len);\r\nif (WARN_ON_ONCE(data_len > nn->fl_bufsz)) {\r\ndev_kfree_skb_any(skb);\r\ncontinue;\r\n}\r\nif (nn->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC) {\r\nskb_reserve(skb, meta_len);\r\n} else {\r\nskb_reserve(skb, nn->rx_offset);\r\n}\r\nskb_put(skb, data_len - meta_len);\r\nnfp_net_set_hash(nn->netdev, skb, rxd);\r\nif (skb_put_padto(skb, 60))\r\nbreak;\r\nu64_stats_update_begin(&r_vec->rx_sync);\r\nr_vec->rx_pkts++;\r\nr_vec->rx_bytes += skb->len;\r\nu64_stats_update_end(&r_vec->rx_sync);\r\nskb_record_rx_queue(skb, rx_ring->idx);\r\nskb->protocol = eth_type_trans(skb, nn->netdev);\r\nnfp_net_rx_csum(nn, r_vec, rxd, skb);\r\nif (rxd->rxd.flags & PCIE_DESC_RX_VLAN)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\r\nle16_to_cpu(rxd->rxd.vlan));\r\nnapi_gro_receive(&rx_ring->r_vec->napi, skb);\r\n}\r\nif (nn->is_nfp3200)\r\nnfp_qcp_rd_ptr_add(rx_ring->qcp_rx, pkts_polled);\r\nreturn pkts_polled;\r\n}\r\nstatic int nfp_net_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct nfp_net_r_vector *r_vec =\r\ncontainer_of(napi, struct nfp_net_r_vector, napi);\r\nstruct nfp_net_rx_ring *rx_ring = r_vec->rx_ring;\r\nstruct nfp_net_tx_ring *tx_ring = r_vec->tx_ring;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nstruct netdev_queue *txq;\r\nunsigned int pkts_polled;\r\ntx_ring = &nn->tx_rings[rx_ring->idx];\r\ntxq = netdev_get_tx_queue(nn->netdev, tx_ring->idx);\r\nnfp_net_tx_complete(tx_ring);\r\npkts_polled = nfp_net_rx(rx_ring, budget);\r\nif (pkts_polled < budget) {\r\nnapi_complete_done(napi, pkts_polled);\r\nnfp_net_irq_unmask(nn, r_vec->irq_idx);\r\n}\r\nreturn pkts_polled;\r\n}\r\nstatic void nfp_net_tx_ring_free(struct nfp_net_tx_ring *tx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = tx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nstruct pci_dev *pdev = nn->pdev;\r\nnn_writeq(nn, NFP_NET_CFG_TXR_ADDR(tx_ring->idx), 0);\r\nnn_writeb(nn, NFP_NET_CFG_TXR_SZ(tx_ring->idx), 0);\r\nnn_writeb(nn, NFP_NET_CFG_TXR_VEC(tx_ring->idx), 0);\r\nkfree(tx_ring->txbufs);\r\nif (tx_ring->txds)\r\ndma_free_coherent(&pdev->dev, tx_ring->size,\r\ntx_ring->txds, tx_ring->dma);\r\ntx_ring->cnt = 0;\r\ntx_ring->wr_p = 0;\r\ntx_ring->rd_p = 0;\r\ntx_ring->qcp_rd_p = 0;\r\ntx_ring->wr_ptr_add = 0;\r\ntx_ring->txbufs = NULL;\r\ntx_ring->txds = NULL;\r\ntx_ring->dma = 0;\r\ntx_ring->size = 0;\r\n}\r\nstatic int nfp_net_tx_ring_alloc(struct nfp_net_tx_ring *tx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = tx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nstruct pci_dev *pdev = nn->pdev;\r\nint sz;\r\ntx_ring->cnt = nn->txd_cnt;\r\ntx_ring->size = sizeof(*tx_ring->txds) * tx_ring->cnt;\r\ntx_ring->txds = dma_zalloc_coherent(&pdev->dev, tx_ring->size,\r\n&tx_ring->dma, GFP_KERNEL);\r\nif (!tx_ring->txds)\r\ngoto err_alloc;\r\nsz = sizeof(*tx_ring->txbufs) * tx_ring->cnt;\r\ntx_ring->txbufs = kzalloc(sz, GFP_KERNEL);\r\nif (!tx_ring->txbufs)\r\ngoto err_alloc;\r\nnn_writeq(nn, NFP_NET_CFG_TXR_ADDR(tx_ring->idx), tx_ring->dma);\r\nnn_writeb(nn, NFP_NET_CFG_TXR_SZ(tx_ring->idx), ilog2(tx_ring->cnt));\r\nnn_writeb(nn, NFP_NET_CFG_TXR_VEC(tx_ring->idx), r_vec->irq_idx);\r\nnetif_set_xps_queue(nn->netdev, &r_vec->affinity_mask, tx_ring->idx);\r\nnn_dbg(nn, "TxQ%02d: QCidx=%02d cnt=%d dma=%#llx host=%p\n",\r\ntx_ring->idx, tx_ring->qcidx,\r\ntx_ring->cnt, (unsigned long long)tx_ring->dma, tx_ring->txds);\r\nreturn 0;\r\nerr_alloc:\r\nnfp_net_tx_ring_free(tx_ring);\r\nreturn -ENOMEM;\r\n}\r\nstatic void nfp_net_rx_ring_free(struct nfp_net_rx_ring *rx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = rx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nstruct pci_dev *pdev = nn->pdev;\r\nnn_writeq(nn, NFP_NET_CFG_RXR_ADDR(rx_ring->idx), 0);\r\nnn_writeb(nn, NFP_NET_CFG_RXR_SZ(rx_ring->idx), 0);\r\nnn_writeb(nn, NFP_NET_CFG_RXR_VEC(rx_ring->idx), 0);\r\nkfree(rx_ring->rxbufs);\r\nif (rx_ring->rxds)\r\ndma_free_coherent(&pdev->dev, rx_ring->size,\r\nrx_ring->rxds, rx_ring->dma);\r\nrx_ring->cnt = 0;\r\nrx_ring->wr_p = 0;\r\nrx_ring->rd_p = 0;\r\nrx_ring->wr_ptr_add = 0;\r\nrx_ring->rxbufs = NULL;\r\nrx_ring->rxds = NULL;\r\nrx_ring->dma = 0;\r\nrx_ring->size = 0;\r\n}\r\nstatic int nfp_net_rx_ring_alloc(struct nfp_net_rx_ring *rx_ring)\r\n{\r\nstruct nfp_net_r_vector *r_vec = rx_ring->r_vec;\r\nstruct nfp_net *nn = r_vec->nfp_net;\r\nstruct pci_dev *pdev = nn->pdev;\r\nint sz;\r\nrx_ring->cnt = nn->rxd_cnt;\r\nrx_ring->size = sizeof(*rx_ring->rxds) * rx_ring->cnt;\r\nrx_ring->rxds = dma_zalloc_coherent(&pdev->dev, rx_ring->size,\r\n&rx_ring->dma, GFP_KERNEL);\r\nif (!rx_ring->rxds)\r\ngoto err_alloc;\r\nsz = sizeof(*rx_ring->rxbufs) * rx_ring->cnt;\r\nrx_ring->rxbufs = kzalloc(sz, GFP_KERNEL);\r\nif (!rx_ring->rxbufs)\r\ngoto err_alloc;\r\nnn_writeq(nn, NFP_NET_CFG_RXR_ADDR(rx_ring->idx), rx_ring->dma);\r\nnn_writeb(nn, NFP_NET_CFG_RXR_SZ(rx_ring->idx), ilog2(rx_ring->cnt));\r\nnn_writeb(nn, NFP_NET_CFG_RXR_VEC(rx_ring->idx), r_vec->irq_idx);\r\nnn_dbg(nn, "RxQ%02d: FlQCidx=%02d RxQCidx=%02d cnt=%d dma=%#llx host=%p\n",\r\nrx_ring->idx, rx_ring->fl_qcidx, rx_ring->rx_qcidx,\r\nrx_ring->cnt, (unsigned long long)rx_ring->dma, rx_ring->rxds);\r\nreturn 0;\r\nerr_alloc:\r\nnfp_net_rx_ring_free(rx_ring);\r\nreturn -ENOMEM;\r\n}\r\nstatic void __nfp_net_free_rings(struct nfp_net *nn, unsigned int n_free)\r\n{\r\nstruct nfp_net_r_vector *r_vec;\r\nstruct msix_entry *entry;\r\nwhile (n_free--) {\r\nr_vec = &nn->r_vecs[n_free];\r\nentry = &nn->irq_entries[r_vec->irq_idx];\r\nnfp_net_rx_ring_free(r_vec->rx_ring);\r\nnfp_net_tx_ring_free(r_vec->tx_ring);\r\nirq_set_affinity_hint(entry->vector, NULL);\r\nfree_irq(entry->vector, r_vec);\r\nnetif_napi_del(&r_vec->napi);\r\n}\r\n}\r\nstatic void nfp_net_free_rings(struct nfp_net *nn)\r\n{\r\n__nfp_net_free_rings(nn, nn->num_r_vecs);\r\n}\r\nstatic int nfp_net_alloc_rings(struct nfp_net *nn)\r\n{\r\nstruct nfp_net_r_vector *r_vec;\r\nstruct msix_entry *entry;\r\nint err;\r\nint r;\r\nfor (r = 0; r < nn->num_r_vecs; r++) {\r\nr_vec = &nn->r_vecs[r];\r\nentry = &nn->irq_entries[r_vec->irq_idx];\r\nnetif_napi_add(nn->netdev, &r_vec->napi,\r\nnfp_net_poll, NAPI_POLL_WEIGHT);\r\nsnprintf(r_vec->name, sizeof(r_vec->name),\r\n"%s-rxtx-%d", nn->netdev->name, r);\r\nerr = request_irq(entry->vector, r_vec->handler, 0,\r\nr_vec->name, r_vec);\r\nif (err) {\r\nnn_dbg(nn, "Error requesting IRQ %d\n", entry->vector);\r\ngoto err_napi_del;\r\n}\r\nirq_set_affinity_hint(entry->vector, &r_vec->affinity_mask);\r\nnn_dbg(nn, "RV%02d: irq=%03d/%03d\n",\r\nr, entry->vector, entry->entry);\r\nerr = nfp_net_tx_ring_alloc(r_vec->tx_ring);\r\nif (err)\r\ngoto err_free_irq;\r\nerr = nfp_net_rx_ring_alloc(r_vec->rx_ring);\r\nif (err)\r\ngoto err_free_tx;\r\n}\r\nreturn 0;\r\nerr_free_tx:\r\nnfp_net_tx_ring_free(r_vec->tx_ring);\r\nerr_free_irq:\r\nirq_set_affinity_hint(entry->vector, NULL);\r\nfree_irq(entry->vector, r_vec);\r\nerr_napi_del:\r\nnetif_napi_del(&r_vec->napi);\r\n__nfp_net_free_rings(nn, r);\r\nreturn err;\r\n}\r\nvoid nfp_net_rss_write_itbl(struct nfp_net *nn)\r\n{\r\nint i;\r\nfor (i = 0; i < NFP_NET_CFG_RSS_ITBL_SZ; i += 4)\r\nnn_writel(nn, NFP_NET_CFG_RSS_ITBL + i,\r\nget_unaligned_le32(nn->rss_itbl + i));\r\n}\r\nvoid nfp_net_rss_write_key(struct nfp_net *nn)\r\n{\r\nint i;\r\nfor (i = 0; i < NFP_NET_CFG_RSS_KEY_SZ; i += 4)\r\nnn_writel(nn, NFP_NET_CFG_RSS_KEY + i,\r\nget_unaligned_le32(nn->rss_key + i));\r\n}\r\nvoid nfp_net_coalesce_write_cfg(struct nfp_net *nn)\r\n{\r\nu8 i;\r\nu32 factor;\r\nu32 value;\r\nfactor = nn->me_freq_mhz / 16;\r\nvalue = (nn->rx_coalesce_max_frames << 16) |\r\n(factor * nn->rx_coalesce_usecs);\r\nfor (i = 0; i < nn->num_r_vecs; i++)\r\nnn_writel(nn, NFP_NET_CFG_RXR_IRQ_MOD(i), value);\r\nvalue = (nn->tx_coalesce_max_frames << 16) |\r\n(factor * nn->tx_coalesce_usecs);\r\nfor (i = 0; i < nn->num_r_vecs; i++)\r\nnn_writel(nn, NFP_NET_CFG_TXR_IRQ_MOD(i), value);\r\n}\r\nstatic void nfp_net_write_mac_addr(struct nfp_net *nn, const u8 *mac)\r\n{\r\nnn_writel(nn, NFP_NET_CFG_MACADDR + 0,\r\nget_unaligned_be32(nn->netdev->dev_addr));\r\nnn_writel(nn, NFP_NET_CFG_MACADDR + 4,\r\nget_unaligned_be16(nn->netdev->dev_addr + 4) << 16);\r\n}\r\nstatic void nfp_net_clear_config_and_disable(struct nfp_net *nn)\r\n{\r\nu32 new_ctrl, update;\r\nint err;\r\nnew_ctrl = nn->ctrl;\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_ENABLE;\r\nupdate = NFP_NET_CFG_UPDATE_GEN;\r\nupdate |= NFP_NET_CFG_UPDATE_MSIX;\r\nupdate |= NFP_NET_CFG_UPDATE_RING;\r\nif (nn->cap & NFP_NET_CFG_CTRL_RINGCFG)\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_RINGCFG;\r\nnn_writeq(nn, NFP_NET_CFG_TXRS_ENABLE, 0);\r\nnn_writeq(nn, NFP_NET_CFG_RXRS_ENABLE, 0);\r\nnn_writel(nn, NFP_NET_CFG_CTRL, new_ctrl);\r\nerr = nfp_net_reconfig(nn, update);\r\nif (err) {\r\nnn_err(nn, "Could not disable device: %d\n", err);\r\nreturn;\r\n}\r\nnn->ctrl = new_ctrl;\r\n}\r\nstatic int nfp_net_start_vec(struct nfp_net *nn, struct nfp_net_r_vector *r_vec)\r\n{\r\nunsigned int irq_vec;\r\nint err = 0;\r\nirq_vec = nn->irq_entries[r_vec->irq_idx].vector;\r\ndisable_irq(irq_vec);\r\nerr = nfp_net_rx_fill_freelist(r_vec->rx_ring);\r\nif (err) {\r\nnn_err(nn, "RV%02d: couldn't allocate enough buffers\n",\r\nr_vec->irq_idx);\r\ngoto out;\r\n}\r\nnapi_enable(&r_vec->napi);\r\nout:\r\nenable_irq(irq_vec);\r\nreturn err;\r\n}\r\nstatic int nfp_net_netdev_open(struct net_device *netdev)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nint err, r;\r\nu32 update = 0;\r\nu32 new_ctrl;\r\nif (nn->ctrl & NFP_NET_CFG_CTRL_ENABLE) {\r\nnn_err(nn, "Dev is already enabled: 0x%08x\n", nn->ctrl);\r\nreturn -EBUSY;\r\n}\r\nnew_ctrl = nn->ctrl;\r\nerr = nfp_net_aux_irq_request(nn, NFP_NET_CFG_EXN, "%s-exn",\r\nnn->exn_name, sizeof(nn->exn_name),\r\nNFP_NET_IRQ_EXN_IDX, nn->exn_handler);\r\nif (err)\r\nreturn err;\r\nerr = nfp_net_alloc_rings(nn);\r\nif (err)\r\ngoto err_free_exn;\r\nerr = netif_set_real_num_tx_queues(netdev, nn->num_tx_rings);\r\nif (err)\r\ngoto err_free_rings;\r\nerr = netif_set_real_num_rx_queues(netdev, nn->num_rx_rings);\r\nif (err)\r\ngoto err_free_rings;\r\nif (nn->cap & NFP_NET_CFG_CTRL_RSS) {\r\nnfp_net_rss_write_key(nn);\r\nnfp_net_rss_write_itbl(nn);\r\nnn_writel(nn, NFP_NET_CFG_RSS_CTRL, nn->rss_cfg);\r\nupdate |= NFP_NET_CFG_UPDATE_RSS;\r\n}\r\nif (nn->cap & NFP_NET_CFG_CTRL_IRQMOD) {\r\nnfp_net_coalesce_write_cfg(nn);\r\nnew_ctrl |= NFP_NET_CFG_CTRL_IRQMOD;\r\nupdate |= NFP_NET_CFG_UPDATE_IRQMOD;\r\n}\r\nnn_writeq(nn, NFP_NET_CFG_TXRS_ENABLE, nn->num_tx_rings == 64 ?\r\n0xffffffffffffffffULL : ((u64)1 << nn->num_tx_rings) - 1);\r\nnn_writeq(nn, NFP_NET_CFG_RXRS_ENABLE, nn->num_rx_rings == 64 ?\r\n0xffffffffffffffffULL : ((u64)1 << nn->num_rx_rings) - 1);\r\nnfp_net_write_mac_addr(nn, netdev->dev_addr);\r\nnn_writel(nn, NFP_NET_CFG_MTU, netdev->mtu);\r\nnn_writel(nn, NFP_NET_CFG_FLBUFSZ, nn->fl_bufsz);\r\nnew_ctrl |= NFP_NET_CFG_CTRL_ENABLE;\r\nupdate |= NFP_NET_CFG_UPDATE_GEN;\r\nupdate |= NFP_NET_CFG_UPDATE_MSIX;\r\nupdate |= NFP_NET_CFG_UPDATE_RING;\r\nif (nn->cap & NFP_NET_CFG_CTRL_RINGCFG)\r\nnew_ctrl |= NFP_NET_CFG_CTRL_RINGCFG;\r\nnn_writel(nn, NFP_NET_CFG_CTRL, new_ctrl);\r\nerr = nfp_net_reconfig(nn, update);\r\nif (err)\r\ngoto err_clear_config;\r\nnn->ctrl = new_ctrl;\r\nif (nn->ctrl & NFP_NET_CFG_CTRL_VXLAN) {\r\nmemset(&nn->vxlan_ports, 0, sizeof(nn->vxlan_ports));\r\nmemset(&nn->vxlan_usecnt, 0, sizeof(nn->vxlan_usecnt));\r\nvxlan_get_rx_port(netdev);\r\n}\r\nfor (r = 0; r < nn->num_r_vecs; r++) {\r\nerr = nfp_net_start_vec(nn, &nn->r_vecs[r]);\r\nif (err)\r\ngoto err_disable_napi;\r\n}\r\nnetif_tx_wake_all_queues(netdev);\r\nerr = nfp_net_aux_irq_request(nn, NFP_NET_CFG_LSC, "%s-lsc",\r\nnn->lsc_name, sizeof(nn->lsc_name),\r\nNFP_NET_IRQ_LSC_IDX, nn->lsc_handler);\r\nif (err)\r\ngoto err_stop_tx;\r\nnfp_net_read_link_status(nn);\r\nreturn 0;\r\nerr_stop_tx:\r\nnetif_tx_disable(netdev);\r\nfor (r = 0; r < nn->num_r_vecs; r++)\r\nnfp_net_tx_flush(nn->r_vecs[r].tx_ring);\r\nerr_disable_napi:\r\nwhile (r--) {\r\nnapi_disable(&nn->r_vecs[r].napi);\r\nnfp_net_rx_flush(nn->r_vecs[r].rx_ring);\r\n}\r\nerr_clear_config:\r\nnfp_net_clear_config_and_disable(nn);\r\nerr_free_rings:\r\nnfp_net_free_rings(nn);\r\nerr_free_exn:\r\nnfp_net_aux_irq_free(nn, NFP_NET_CFG_EXN, NFP_NET_IRQ_EXN_IDX);\r\nreturn err;\r\n}\r\nstatic int nfp_net_netdev_close(struct net_device *netdev)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nint r;\r\nif (!(nn->ctrl & NFP_NET_CFG_CTRL_ENABLE)) {\r\nnn_err(nn, "Dev is not up: 0x%08x\n", nn->ctrl);\r\nreturn 0;\r\n}\r\nnfp_net_aux_irq_free(nn, NFP_NET_CFG_LSC, NFP_NET_IRQ_LSC_IDX);\r\nnetif_carrier_off(netdev);\r\nnn->link_up = false;\r\nfor (r = 0; r < nn->num_r_vecs; r++)\r\nnapi_disable(&nn->r_vecs[r].napi);\r\nnetif_tx_disable(netdev);\r\nnfp_net_clear_config_and_disable(nn);\r\nfor (r = 0; r < nn->num_r_vecs; r++) {\r\nnfp_net_rx_flush(nn->r_vecs[r].rx_ring);\r\nnfp_net_tx_flush(nn->r_vecs[r].tx_ring);\r\n}\r\nnfp_net_free_rings(nn);\r\nnfp_net_aux_irq_free(nn, NFP_NET_CFG_EXN, NFP_NET_IRQ_EXN_IDX);\r\nnn_dbg(nn, "%s down", netdev->name);\r\nreturn 0;\r\n}\r\nstatic void nfp_net_set_rx_mode(struct net_device *netdev)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nu32 new_ctrl;\r\nnew_ctrl = nn->ctrl;\r\nif (netdev->flags & IFF_PROMISC) {\r\nif (nn->cap & NFP_NET_CFG_CTRL_PROMISC)\r\nnew_ctrl |= NFP_NET_CFG_CTRL_PROMISC;\r\nelse\r\nnn_warn(nn, "FW does not support promiscuous mode\n");\r\n} else {\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_PROMISC;\r\n}\r\nif (new_ctrl == nn->ctrl)\r\nreturn;\r\nnn_writel(nn, NFP_NET_CFG_CTRL, new_ctrl);\r\nif (nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN))\r\nreturn;\r\nnn->ctrl = new_ctrl;\r\n}\r\nstatic int nfp_net_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nu32 tmp;\r\nnn_dbg(nn, "New MTU = %d\n", new_mtu);\r\nif (new_mtu < 68 || new_mtu > nn->max_mtu) {\r\nnn_err(nn, "New MTU (%d) is not valid\n", new_mtu);\r\nreturn -EINVAL;\r\n}\r\nnetdev->mtu = new_mtu;\r\ntmp = new_mtu + ETH_HLEN + VLAN_HLEN + NFP_NET_MAX_PREPEND;\r\nnn->fl_bufsz = roundup(tmp, 1024);\r\nif (netif_running(netdev)) {\r\nnfp_net_netdev_close(netdev);\r\nnfp_net_netdev_open(netdev);\r\n}\r\nreturn 0;\r\n}\r\nstatic struct rtnl_link_stats64 *nfp_net_stat64(struct net_device *netdev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nint r;\r\nfor (r = 0; r < nn->num_r_vecs; r++) {\r\nstruct nfp_net_r_vector *r_vec = &nn->r_vecs[r];\r\nu64 data[3];\r\nunsigned int start;\r\ndo {\r\nstart = u64_stats_fetch_begin(&r_vec->rx_sync);\r\ndata[0] = r_vec->rx_pkts;\r\ndata[1] = r_vec->rx_bytes;\r\ndata[2] = r_vec->rx_drops;\r\n} while (u64_stats_fetch_retry(&r_vec->rx_sync, start));\r\nstats->rx_packets += data[0];\r\nstats->rx_bytes += data[1];\r\nstats->rx_dropped += data[2];\r\ndo {\r\nstart = u64_stats_fetch_begin(&r_vec->tx_sync);\r\ndata[0] = r_vec->tx_pkts;\r\ndata[1] = r_vec->tx_bytes;\r\ndata[2] = r_vec->tx_errors;\r\n} while (u64_stats_fetch_retry(&r_vec->tx_sync, start));\r\nstats->tx_packets += data[0];\r\nstats->tx_bytes += data[1];\r\nstats->tx_errors += data[2];\r\n}\r\nreturn stats;\r\n}\r\nstatic int nfp_net_set_features(struct net_device *netdev,\r\nnetdev_features_t features)\r\n{\r\nnetdev_features_t changed = netdev->features ^ features;\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nu32 new_ctrl;\r\nint err;\r\nnew_ctrl = nn->ctrl;\r\nif (changed & NETIF_F_RXCSUM) {\r\nif (features & NETIF_F_RXCSUM)\r\nnew_ctrl |= NFP_NET_CFG_CTRL_RXCSUM;\r\nelse\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_RXCSUM;\r\n}\r\nif (changed & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\r\nif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM))\r\nnew_ctrl |= NFP_NET_CFG_CTRL_TXCSUM;\r\nelse\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_TXCSUM;\r\n}\r\nif (changed & (NETIF_F_TSO | NETIF_F_TSO6)) {\r\nif (features & (NETIF_F_TSO | NETIF_F_TSO6))\r\nnew_ctrl |= NFP_NET_CFG_CTRL_LSO;\r\nelse\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_LSO;\r\n}\r\nif (changed & NETIF_F_HW_VLAN_CTAG_RX) {\r\nif (features & NETIF_F_HW_VLAN_CTAG_RX)\r\nnew_ctrl |= NFP_NET_CFG_CTRL_RXVLAN;\r\nelse\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_RXVLAN;\r\n}\r\nif (changed & NETIF_F_HW_VLAN_CTAG_TX) {\r\nif (features & NETIF_F_HW_VLAN_CTAG_TX)\r\nnew_ctrl |= NFP_NET_CFG_CTRL_TXVLAN;\r\nelse\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_TXVLAN;\r\n}\r\nif (changed & NETIF_F_SG) {\r\nif (features & NETIF_F_SG)\r\nnew_ctrl |= NFP_NET_CFG_CTRL_GATHER;\r\nelse\r\nnew_ctrl &= ~NFP_NET_CFG_CTRL_GATHER;\r\n}\r\nnn_dbg(nn, "Feature change 0x%llx -> 0x%llx (changed=0x%llx)\n",\r\nnetdev->features, features, changed);\r\nif (new_ctrl == nn->ctrl)\r\nreturn 0;\r\nnn_dbg(nn, "NIC ctrl: 0x%x -> 0x%x\n", nn->ctrl, new_ctrl);\r\nnn_writel(nn, NFP_NET_CFG_CTRL, new_ctrl);\r\nerr = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);\r\nif (err)\r\nreturn err;\r\nnn->ctrl = new_ctrl;\r\nreturn 0;\r\n}\r\nstatic netdev_features_t\r\nnfp_net_features_check(struct sk_buff *skb, struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nu8 l4_hdr;\r\nfeatures &= vlan_features_check(skb, features);\r\nif (!skb->encapsulation)\r\nreturn features;\r\nif (skb_is_gso(skb)) {\r\nu32 hdrlen;\r\nhdrlen = skb_inner_transport_header(skb) - skb->data +\r\ninner_tcp_hdrlen(skb);\r\nif (unlikely(hdrlen > NFP_NET_LSO_MAX_HDR_SZ))\r\nfeatures &= ~NETIF_F_GSO_MASK;\r\n}\r\nswitch (vlan_get_protocol(skb)) {\r\ncase htons(ETH_P_IP):\r\nl4_hdr = ip_hdr(skb)->protocol;\r\nbreak;\r\ncase htons(ETH_P_IPV6):\r\nl4_hdr = ipv6_hdr(skb)->nexthdr;\r\nbreak;\r\ndefault:\r\nreturn features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\r\n}\r\nif (skb->inner_protocol_type != ENCAP_TYPE_ETHER ||\r\nskb->inner_protocol != htons(ETH_P_TEB) ||\r\n(l4_hdr != IPPROTO_UDP && l4_hdr != IPPROTO_GRE) ||\r\n(l4_hdr == IPPROTO_UDP &&\r\n(skb_inner_mac_header(skb) - skb_transport_header(skb) !=\r\nsizeof(struct udphdr) + sizeof(struct vxlanhdr))))\r\nreturn features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\r\nreturn features;\r\n}\r\nstatic void nfp_net_set_vxlan_port(struct nfp_net *nn, int idx, __be16 port)\r\n{\r\nint i;\r\nnn->vxlan_ports[idx] = port;\r\nif (!(nn->ctrl & NFP_NET_CFG_CTRL_VXLAN))\r\nreturn;\r\nBUILD_BUG_ON(NFP_NET_N_VXLAN_PORTS & 1);\r\nfor (i = 0; i < NFP_NET_N_VXLAN_PORTS; i += 2)\r\nnn_writel(nn, NFP_NET_CFG_VXLAN_PORT + i * sizeof(port),\r\nbe16_to_cpu(nn->vxlan_ports[i + 1]) << 16 |\r\nbe16_to_cpu(nn->vxlan_ports[i]));\r\nnfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_VXLAN);\r\n}\r\nstatic int nfp_net_find_vxlan_idx(struct nfp_net *nn, __be16 port)\r\n{\r\nint i, free_idx = -ENOSPC;\r\nfor (i = 0; i < NFP_NET_N_VXLAN_PORTS; i++) {\r\nif (nn->vxlan_ports[i] == port)\r\nreturn i;\r\nif (!nn->vxlan_usecnt[i])\r\nfree_idx = i;\r\n}\r\nreturn free_idx;\r\n}\r\nstatic void nfp_net_add_vxlan_port(struct net_device *netdev,\r\nsa_family_t sa_family, __be16 port)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nint idx;\r\nidx = nfp_net_find_vxlan_idx(nn, port);\r\nif (idx == -ENOSPC)\r\nreturn;\r\nif (!nn->vxlan_usecnt[idx]++)\r\nnfp_net_set_vxlan_port(nn, idx, port);\r\n}\r\nstatic void nfp_net_del_vxlan_port(struct net_device *netdev,\r\nsa_family_t sa_family, __be16 port)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nint idx;\r\nidx = nfp_net_find_vxlan_idx(nn, port);\r\nif (!nn->vxlan_usecnt[idx] || idx == -ENOSPC)\r\nreturn;\r\nif (!--nn->vxlan_usecnt[idx])\r\nnfp_net_set_vxlan_port(nn, idx, 0);\r\n}\r\nvoid nfp_net_info(struct nfp_net *nn)\r\n{\r\nnn_info(nn, "Netronome %s %sNetdev: TxQs=%d/%d RxQs=%d/%d\n",\r\nnn->is_nfp3200 ? "NFP-32xx" : "NFP-6xxx",\r\nnn->is_vf ? "VF " : "",\r\nnn->num_tx_rings, nn->max_tx_rings,\r\nnn->num_rx_rings, nn->max_rx_rings);\r\nnn_info(nn, "VER: %d.%d.%d.%d, Maximum supported MTU: %d\n",\r\nnn->fw_ver.resv, nn->fw_ver.class,\r\nnn->fw_ver.major, nn->fw_ver.minor,\r\nnn->max_mtu);\r\nnn_info(nn, "CAP: %#x %s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\n",\r\nnn->cap,\r\nnn->cap & NFP_NET_CFG_CTRL_PROMISC ? "PROMISC " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_L2BC ? "L2BCFILT " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_L2MC ? "L2MCFILT " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_RXCSUM ? "RXCSUM " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_TXCSUM ? "TXCSUM " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_RXVLAN ? "RXVLAN " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_TXVLAN ? "TXVLAN " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_SCATTER ? "SCATTER " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_GATHER ? "GATHER " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_LSO ? "TSO " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_RSS ? "RSS " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_L2SWITCH ? "L2SWITCH " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_MSIXAUTO ? "AUTOMASK " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_IRQMOD ? "IRQMOD " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_VXLAN ? "VXLAN " : "",\r\nnn->cap & NFP_NET_CFG_CTRL_NVGRE ? "NVGRE " : "");\r\n}\r\nstruct nfp_net *nfp_net_netdev_alloc(struct pci_dev *pdev,\r\nint max_tx_rings, int max_rx_rings)\r\n{\r\nstruct net_device *netdev;\r\nstruct nfp_net *nn;\r\nint nqs;\r\nnetdev = alloc_etherdev_mqs(sizeof(struct nfp_net),\r\nmax_tx_rings, max_rx_rings);\r\nif (!netdev)\r\nreturn ERR_PTR(-ENOMEM);\r\nSET_NETDEV_DEV(netdev, &pdev->dev);\r\nnn = netdev_priv(netdev);\r\nnn->netdev = netdev;\r\nnn->pdev = pdev;\r\nnn->max_tx_rings = max_tx_rings;\r\nnn->max_rx_rings = max_rx_rings;\r\nnqs = netif_get_num_default_rss_queues();\r\nnn->num_tx_rings = min_t(int, nqs, max_tx_rings);\r\nnn->num_rx_rings = min_t(int, nqs, max_rx_rings);\r\nnn->txd_cnt = NFP_NET_TX_DESCS_DEFAULT;\r\nnn->rxd_cnt = NFP_NET_RX_DESCS_DEFAULT;\r\nspin_lock_init(&nn->reconfig_lock);\r\nspin_lock_init(&nn->link_status_lock);\r\nreturn nn;\r\n}\r\nvoid nfp_net_netdev_free(struct nfp_net *nn)\r\n{\r\nfree_netdev(nn->netdev);\r\n}\r\nstatic void nfp_net_rss_init(struct nfp_net *nn)\r\n{\r\nint i;\r\nnetdev_rss_key_fill(nn->rss_key, NFP_NET_CFG_RSS_KEY_SZ);\r\nfor (i = 0; i < sizeof(nn->rss_itbl); i++)\r\nnn->rss_itbl[i] =\r\nethtool_rxfh_indir_default(i, nn->num_rx_rings);\r\nnn->rss_cfg = NFP_NET_CFG_RSS_IPV4_TCP |\r\nNFP_NET_CFG_RSS_IPV6_TCP |\r\nNFP_NET_CFG_RSS_TOEPLITZ |\r\nNFP_NET_CFG_RSS_MASK;\r\n}\r\nstatic void nfp_net_irqmod_init(struct nfp_net *nn)\r\n{\r\nnn->rx_coalesce_usecs = 50;\r\nnn->rx_coalesce_max_frames = 64;\r\nnn->tx_coalesce_usecs = 50;\r\nnn->tx_coalesce_max_frames = 64;\r\n}\r\nint nfp_net_netdev_init(struct net_device *netdev)\r\n{\r\nstruct nfp_net *nn = netdev_priv(netdev);\r\nint err;\r\nnn->cap = nn_readl(nn, NFP_NET_CFG_CAP);\r\nnn->max_mtu = nn_readl(nn, NFP_NET_CFG_MAX_MTU);\r\nnfp_net_write_mac_addr(nn, nn->netdev->dev_addr);\r\nif (nn->max_mtu < NFP_NET_DEFAULT_MTU)\r\nnetdev->mtu = nn->max_mtu;\r\nelse\r\nnetdev->mtu = NFP_NET_DEFAULT_MTU;\r\nnn->fl_bufsz = NFP_NET_DEFAULT_RX_BUFSZ;\r\nnetdev->hw_features = NETIF_F_HIGHDMA;\r\nif (nn->cap & NFP_NET_CFG_CTRL_RXCSUM) {\r\nnetdev->hw_features |= NETIF_F_RXCSUM;\r\nnn->ctrl |= NFP_NET_CFG_CTRL_RXCSUM;\r\n}\r\nif (nn->cap & NFP_NET_CFG_CTRL_TXCSUM) {\r\nnetdev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\r\nnn->ctrl |= NFP_NET_CFG_CTRL_TXCSUM;\r\n}\r\nif (nn->cap & NFP_NET_CFG_CTRL_GATHER) {\r\nnetdev->hw_features |= NETIF_F_SG;\r\nnn->ctrl |= NFP_NET_CFG_CTRL_GATHER;\r\n}\r\nif ((nn->cap & NFP_NET_CFG_CTRL_LSO) && nn->fw_ver.major > 2) {\r\nnetdev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;\r\nnn->ctrl |= NFP_NET_CFG_CTRL_LSO;\r\n}\r\nif (nn->cap & NFP_NET_CFG_CTRL_RSS) {\r\nnetdev->hw_features |= NETIF_F_RXHASH;\r\nnfp_net_rss_init(nn);\r\nnn->ctrl |= NFP_NET_CFG_CTRL_RSS;\r\n}\r\nif (nn->cap & NFP_NET_CFG_CTRL_VXLAN &&\r\nnn->cap & NFP_NET_CFG_CTRL_NVGRE) {\r\nif (nn->cap & NFP_NET_CFG_CTRL_LSO)\r\nnetdev->hw_features |= NETIF_F_GSO_GRE |\r\nNETIF_F_GSO_UDP_TUNNEL;\r\nnn->ctrl |= NFP_NET_CFG_CTRL_VXLAN | NFP_NET_CFG_CTRL_NVGRE;\r\nnetdev->hw_enc_features = netdev->hw_features;\r\n}\r\nnetdev->vlan_features = netdev->hw_features;\r\nif (nn->cap & NFP_NET_CFG_CTRL_RXVLAN) {\r\nnetdev->hw_features |= NETIF_F_HW_VLAN_CTAG_RX;\r\nnn->ctrl |= NFP_NET_CFG_CTRL_RXVLAN;\r\n}\r\nif (nn->cap & NFP_NET_CFG_CTRL_TXVLAN) {\r\nnetdev->hw_features |= NETIF_F_HW_VLAN_CTAG_TX;\r\nnn->ctrl |= NFP_NET_CFG_CTRL_TXVLAN;\r\n}\r\nnetdev->features = netdev->hw_features;\r\nnetdev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);\r\nif (nn->cap & NFP_NET_CFG_CTRL_L2BC)\r\nnn->ctrl |= NFP_NET_CFG_CTRL_L2BC;\r\nif (nn->cap & NFP_NET_CFG_CTRL_L2MC)\r\nnn->ctrl |= NFP_NET_CFG_CTRL_L2MC;\r\nif (nn->cap & NFP_NET_CFG_CTRL_IRQMOD) {\r\nnfp_net_irqmod_init(nn);\r\nnn->ctrl |= NFP_NET_CFG_CTRL_IRQMOD;\r\n}\r\nif (nn->is_nfp3200 && nn->cap & NFP_NET_CFG_CTRL_MSIXAUTO)\r\nnn->ctrl |= NFP_NET_CFG_CTRL_MSIXAUTO;\r\nif (nn->fw_ver.major >= 2)\r\nnn->rx_offset = nn_readl(nn, NFP_NET_CFG_RX_OFFSET);\r\nelse\r\nnn->rx_offset = NFP_NET_RX_OFFSET;\r\nnn->qcp_cfg = nn->tx_bar + NFP_QCP_QUEUE_ADDR_SZ;\r\nnn_writel(nn, NFP_NET_CFG_CTRL, 0);\r\nnn_writeq(nn, NFP_NET_CFG_TXRS_ENABLE, 0);\r\nnn_writeq(nn, NFP_NET_CFG_RXRS_ENABLE, 0);\r\nerr = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_RING |\r\nNFP_NET_CFG_UPDATE_GEN);\r\nif (err)\r\nreturn err;\r\nether_setup(netdev);\r\nnetdev->netdev_ops = &nfp_net_netdev_ops;\r\nnetdev->watchdog_timeo = msecs_to_jiffies(5 * 1000);\r\nnetif_carrier_off(netdev);\r\nnfp_net_set_ethtool_ops(netdev);\r\nnfp_net_irqs_assign(netdev);\r\nreturn register_netdev(netdev);\r\n}\r\nvoid nfp_net_netdev_clean(struct net_device *netdev)\r\n{\r\nunregister_netdev(netdev);\r\n}
