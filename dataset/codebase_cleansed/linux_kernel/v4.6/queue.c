static int mmc_prep_request(struct request_queue *q, struct request *req)\r\n{\r\nstruct mmc_queue *mq = q->queuedata;\r\nif (req->cmd_type != REQ_TYPE_FS && !(req->cmd_flags & REQ_DISCARD)) {\r\nblk_dump_rq_flags(req, "MMC bad request");\r\nreturn BLKPREP_KILL;\r\n}\r\nif (mq && (mmc_card_removed(mq->card) || mmc_access_rpmb(mq)))\r\nreturn BLKPREP_KILL;\r\nreq->cmd_flags |= REQ_DONTPREP;\r\nreturn BLKPREP_OK;\r\n}\r\nstatic int mmc_queue_thread(void *d)\r\n{\r\nstruct mmc_queue *mq = d;\r\nstruct request_queue *q = mq->queue;\r\ncurrent->flags |= PF_MEMALLOC;\r\ndown(&mq->thread_sem);\r\ndo {\r\nstruct request *req = NULL;\r\nunsigned int cmd_flags = 0;\r\nspin_lock_irq(q->queue_lock);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nreq = blk_fetch_request(q);\r\nmq->mqrq_cur->req = req;\r\nspin_unlock_irq(q->queue_lock);\r\nif (req || mq->mqrq_prev->req) {\r\nset_current_state(TASK_RUNNING);\r\ncmd_flags = req ? req->cmd_flags : 0;\r\nmq->issue_fn(mq, req);\r\ncond_resched();\r\nif (mq->flags & MMC_QUEUE_NEW_REQUEST) {\r\nmq->flags &= ~MMC_QUEUE_NEW_REQUEST;\r\ncontinue;\r\n}\r\nif (cmd_flags & MMC_REQ_SPECIAL_MASK)\r\nmq->mqrq_cur->req = NULL;\r\nmq->mqrq_prev->brq.mrq.data = NULL;\r\nmq->mqrq_prev->req = NULL;\r\nswap(mq->mqrq_prev, mq->mqrq_cur);\r\n} else {\r\nif (kthread_should_stop()) {\r\nset_current_state(TASK_RUNNING);\r\nbreak;\r\n}\r\nup(&mq->thread_sem);\r\nschedule();\r\ndown(&mq->thread_sem);\r\n}\r\n} while (1);\r\nup(&mq->thread_sem);\r\nreturn 0;\r\n}\r\nstatic void mmc_request_fn(struct request_queue *q)\r\n{\r\nstruct mmc_queue *mq = q->queuedata;\r\nstruct request *req;\r\nunsigned long flags;\r\nstruct mmc_context_info *cntx;\r\nif (!mq) {\r\nwhile ((req = blk_fetch_request(q)) != NULL) {\r\nreq->cmd_flags |= REQ_QUIET;\r\n__blk_end_request_all(req, -EIO);\r\n}\r\nreturn;\r\n}\r\ncntx = &mq->card->host->context_info;\r\nif (!mq->mqrq_cur->req && mq->mqrq_prev->req) {\r\nspin_lock_irqsave(&cntx->lock, flags);\r\nif (cntx->is_waiting_last_req) {\r\ncntx->is_new_req = true;\r\nwake_up_interruptible(&cntx->wait);\r\n}\r\nspin_unlock_irqrestore(&cntx->lock, flags);\r\n} else if (!mq->mqrq_cur->req && !mq->mqrq_prev->req)\r\nwake_up_process(mq->thread);\r\n}\r\nstatic struct scatterlist *mmc_alloc_sg(int sg_len, int *err)\r\n{\r\nstruct scatterlist *sg;\r\nsg = kmalloc(sizeof(struct scatterlist)*sg_len, GFP_KERNEL);\r\nif (!sg)\r\n*err = -ENOMEM;\r\nelse {\r\n*err = 0;\r\nsg_init_table(sg, sg_len);\r\n}\r\nreturn sg;\r\n}\r\nstatic void mmc_queue_setup_discard(struct request_queue *q,\r\nstruct mmc_card *card)\r\n{\r\nunsigned max_discard;\r\nmax_discard = mmc_calc_max_discard(card);\r\nif (!max_discard)\r\nreturn;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\r\nblk_queue_max_discard_sectors(q, max_discard);\r\nif (card->erased_byte == 0 && !mmc_can_discard(card))\r\nq->limits.discard_zeroes_data = 1;\r\nq->limits.discard_granularity = card->pref_erase << 9;\r\nif (card->pref_erase > max_discard)\r\nq->limits.discard_granularity = 0;\r\nif (mmc_can_secure_erase_trim(card))\r\nqueue_flag_set_unlocked(QUEUE_FLAG_SECDISCARD, q);\r\n}\r\nint mmc_init_queue(struct mmc_queue *mq, struct mmc_card *card,\r\nspinlock_t *lock, const char *subname)\r\n{\r\nstruct mmc_host *host = card->host;\r\nu64 limit = BLK_BOUNCE_HIGH;\r\nint ret;\r\nstruct mmc_queue_req *mqrq_cur = &mq->mqrq[0];\r\nstruct mmc_queue_req *mqrq_prev = &mq->mqrq[1];\r\nif (mmc_dev(host)->dma_mask && *mmc_dev(host)->dma_mask)\r\nlimit = (u64)dma_max_pfn(mmc_dev(host)) << PAGE_SHIFT;\r\nmq->card = card;\r\nmq->queue = blk_init_queue(mmc_request_fn, lock);\r\nif (!mq->queue)\r\nreturn -ENOMEM;\r\nmq->mqrq_cur = mqrq_cur;\r\nmq->mqrq_prev = mqrq_prev;\r\nmq->queue->queuedata = mq;\r\nblk_queue_prep_rq(mq->queue, mmc_prep_request);\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, mq->queue);\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, mq->queue);\r\nif (mmc_can_erase(card))\r\nmmc_queue_setup_discard(mq->queue, card);\r\n#ifdef CONFIG_MMC_BLOCK_BOUNCE\r\nif (host->max_segs == 1) {\r\nunsigned int bouncesz;\r\nbouncesz = MMC_QUEUE_BOUNCESZ;\r\nif (bouncesz > host->max_req_size)\r\nbouncesz = host->max_req_size;\r\nif (bouncesz > host->max_seg_size)\r\nbouncesz = host->max_seg_size;\r\nif (bouncesz > (host->max_blk_count * 512))\r\nbouncesz = host->max_blk_count * 512;\r\nif (bouncesz > 512) {\r\nmqrq_cur->bounce_buf = kmalloc(bouncesz, GFP_KERNEL);\r\nif (!mqrq_cur->bounce_buf) {\r\npr_warn("%s: unable to allocate bounce cur buffer\n",\r\nmmc_card_name(card));\r\n} else {\r\nmqrq_prev->bounce_buf =\r\nkmalloc(bouncesz, GFP_KERNEL);\r\nif (!mqrq_prev->bounce_buf) {\r\npr_warn("%s: unable to allocate bounce prev buffer\n",\r\nmmc_card_name(card));\r\nkfree(mqrq_cur->bounce_buf);\r\nmqrq_cur->bounce_buf = NULL;\r\n}\r\n}\r\n}\r\nif (mqrq_cur->bounce_buf && mqrq_prev->bounce_buf) {\r\nblk_queue_bounce_limit(mq->queue, BLK_BOUNCE_ANY);\r\nblk_queue_max_hw_sectors(mq->queue, bouncesz / 512);\r\nblk_queue_max_segments(mq->queue, bouncesz / 512);\r\nblk_queue_max_segment_size(mq->queue, bouncesz);\r\nmqrq_cur->sg = mmc_alloc_sg(1, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_cur->bounce_sg =\r\nmmc_alloc_sg(bouncesz / 512, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_prev->sg = mmc_alloc_sg(1, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_prev->bounce_sg =\r\nmmc_alloc_sg(bouncesz / 512, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\n}\r\n}\r\n#endif\r\nif (!mqrq_cur->bounce_buf && !mqrq_prev->bounce_buf) {\r\nblk_queue_bounce_limit(mq->queue, limit);\r\nblk_queue_max_hw_sectors(mq->queue,\r\nmin(host->max_blk_count, host->max_req_size / 512));\r\nblk_queue_max_segments(mq->queue, host->max_segs);\r\nblk_queue_max_segment_size(mq->queue, host->max_seg_size);\r\nmqrq_cur->sg = mmc_alloc_sg(host->max_segs, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_prev->sg = mmc_alloc_sg(host->max_segs, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\n}\r\nsema_init(&mq->thread_sem, 1);\r\nmq->thread = kthread_run(mmc_queue_thread, mq, "mmcqd/%d%s",\r\nhost->index, subname ? subname : "");\r\nif (IS_ERR(mq->thread)) {\r\nret = PTR_ERR(mq->thread);\r\ngoto free_bounce_sg;\r\n}\r\nreturn 0;\r\nfree_bounce_sg:\r\nkfree(mqrq_cur->bounce_sg);\r\nmqrq_cur->bounce_sg = NULL;\r\nkfree(mqrq_prev->bounce_sg);\r\nmqrq_prev->bounce_sg = NULL;\r\ncleanup_queue:\r\nkfree(mqrq_cur->sg);\r\nmqrq_cur->sg = NULL;\r\nkfree(mqrq_cur->bounce_buf);\r\nmqrq_cur->bounce_buf = NULL;\r\nkfree(mqrq_prev->sg);\r\nmqrq_prev->sg = NULL;\r\nkfree(mqrq_prev->bounce_buf);\r\nmqrq_prev->bounce_buf = NULL;\r\nblk_cleanup_queue(mq->queue);\r\nreturn ret;\r\n}\r\nvoid mmc_cleanup_queue(struct mmc_queue *mq)\r\n{\r\nstruct request_queue *q = mq->queue;\r\nunsigned long flags;\r\nstruct mmc_queue_req *mqrq_cur = mq->mqrq_cur;\r\nstruct mmc_queue_req *mqrq_prev = mq->mqrq_prev;\r\nmmc_queue_resume(mq);\r\nkthread_stop(mq->thread);\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nq->queuedata = NULL;\r\nblk_start_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\nkfree(mqrq_cur->bounce_sg);\r\nmqrq_cur->bounce_sg = NULL;\r\nkfree(mqrq_cur->sg);\r\nmqrq_cur->sg = NULL;\r\nkfree(mqrq_cur->bounce_buf);\r\nmqrq_cur->bounce_buf = NULL;\r\nkfree(mqrq_prev->bounce_sg);\r\nmqrq_prev->bounce_sg = NULL;\r\nkfree(mqrq_prev->sg);\r\nmqrq_prev->sg = NULL;\r\nkfree(mqrq_prev->bounce_buf);\r\nmqrq_prev->bounce_buf = NULL;\r\nmq->card = NULL;\r\n}\r\nint mmc_packed_init(struct mmc_queue *mq, struct mmc_card *card)\r\n{\r\nstruct mmc_queue_req *mqrq_cur = &mq->mqrq[0];\r\nstruct mmc_queue_req *mqrq_prev = &mq->mqrq[1];\r\nint ret = 0;\r\nmqrq_cur->packed = kzalloc(sizeof(struct mmc_packed), GFP_KERNEL);\r\nif (!mqrq_cur->packed) {\r\npr_warn("%s: unable to allocate packed cmd for mqrq_cur\n",\r\nmmc_card_name(card));\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nmqrq_prev->packed = kzalloc(sizeof(struct mmc_packed), GFP_KERNEL);\r\nif (!mqrq_prev->packed) {\r\npr_warn("%s: unable to allocate packed cmd for mqrq_prev\n",\r\nmmc_card_name(card));\r\nkfree(mqrq_cur->packed);\r\nmqrq_cur->packed = NULL;\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nINIT_LIST_HEAD(&mqrq_cur->packed->list);\r\nINIT_LIST_HEAD(&mqrq_prev->packed->list);\r\nout:\r\nreturn ret;\r\n}\r\nvoid mmc_packed_clean(struct mmc_queue *mq)\r\n{\r\nstruct mmc_queue_req *mqrq_cur = &mq->mqrq[0];\r\nstruct mmc_queue_req *mqrq_prev = &mq->mqrq[1];\r\nkfree(mqrq_cur->packed);\r\nmqrq_cur->packed = NULL;\r\nkfree(mqrq_prev->packed);\r\nmqrq_prev->packed = NULL;\r\n}\r\nvoid mmc_queue_suspend(struct mmc_queue *mq)\r\n{\r\nstruct request_queue *q = mq->queue;\r\nunsigned long flags;\r\nif (!(mq->flags & MMC_QUEUE_SUSPENDED)) {\r\nmq->flags |= MMC_QUEUE_SUSPENDED;\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_stop_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\ndown(&mq->thread_sem);\r\n}\r\n}\r\nvoid mmc_queue_resume(struct mmc_queue *mq)\r\n{\r\nstruct request_queue *q = mq->queue;\r\nunsigned long flags;\r\nif (mq->flags & MMC_QUEUE_SUSPENDED) {\r\nmq->flags &= ~MMC_QUEUE_SUSPENDED;\r\nup(&mq->thread_sem);\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_start_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\n}\r\n}\r\nstatic unsigned int mmc_queue_packed_map_sg(struct mmc_queue *mq,\r\nstruct mmc_packed *packed,\r\nstruct scatterlist *sg,\r\nenum mmc_packed_type cmd_type)\r\n{\r\nstruct scatterlist *__sg = sg;\r\nunsigned int sg_len = 0;\r\nstruct request *req;\r\nif (mmc_packed_wr(cmd_type)) {\r\nunsigned int hdr_sz = mmc_large_sector(mq->card) ? 4096 : 512;\r\nunsigned int max_seg_sz = queue_max_segment_size(mq->queue);\r\nunsigned int len, remain, offset = 0;\r\nu8 *buf = (u8 *)packed->cmd_hdr;\r\nremain = hdr_sz;\r\ndo {\r\nlen = min(remain, max_seg_sz);\r\nsg_set_buf(__sg, buf + offset, len);\r\noffset += len;\r\nremain -= len;\r\nsg_unmark_end(__sg++);\r\nsg_len++;\r\n} while (remain);\r\n}\r\nlist_for_each_entry(req, &packed->list, queuelist) {\r\nsg_len += blk_rq_map_sg(mq->queue, req, __sg);\r\n__sg = sg + (sg_len - 1);\r\nsg_unmark_end(__sg++);\r\n}\r\nsg_mark_end(sg + (sg_len - 1));\r\nreturn sg_len;\r\n}\r\nunsigned int mmc_queue_map_sg(struct mmc_queue *mq, struct mmc_queue_req *mqrq)\r\n{\r\nunsigned int sg_len;\r\nsize_t buflen;\r\nstruct scatterlist *sg;\r\nenum mmc_packed_type cmd_type;\r\nint i;\r\ncmd_type = mqrq->cmd_type;\r\nif (!mqrq->bounce_buf) {\r\nif (mmc_packed_cmd(cmd_type))\r\nreturn mmc_queue_packed_map_sg(mq, mqrq->packed,\r\nmqrq->sg, cmd_type);\r\nelse\r\nreturn blk_rq_map_sg(mq->queue, mqrq->req, mqrq->sg);\r\n}\r\nBUG_ON(!mqrq->bounce_sg);\r\nif (mmc_packed_cmd(cmd_type))\r\nsg_len = mmc_queue_packed_map_sg(mq, mqrq->packed,\r\nmqrq->bounce_sg, cmd_type);\r\nelse\r\nsg_len = blk_rq_map_sg(mq->queue, mqrq->req, mqrq->bounce_sg);\r\nmqrq->bounce_sg_len = sg_len;\r\nbuflen = 0;\r\nfor_each_sg(mqrq->bounce_sg, sg, sg_len, i)\r\nbuflen += sg->length;\r\nsg_init_one(mqrq->sg, mqrq->bounce_buf, buflen);\r\nreturn 1;\r\n}\r\nvoid mmc_queue_bounce_pre(struct mmc_queue_req *mqrq)\r\n{\r\nif (!mqrq->bounce_buf)\r\nreturn;\r\nif (rq_data_dir(mqrq->req) != WRITE)\r\nreturn;\r\nsg_copy_to_buffer(mqrq->bounce_sg, mqrq->bounce_sg_len,\r\nmqrq->bounce_buf, mqrq->sg[0].length);\r\n}\r\nvoid mmc_queue_bounce_post(struct mmc_queue_req *mqrq)\r\n{\r\nif (!mqrq->bounce_buf)\r\nreturn;\r\nif (rq_data_dir(mqrq->req) != READ)\r\nreturn;\r\nsg_copy_from_buffer(mqrq->bounce_sg, mqrq->bounce_sg_len,\r\nmqrq->bounce_buf, mqrq->sg[0].length);\r\n}
