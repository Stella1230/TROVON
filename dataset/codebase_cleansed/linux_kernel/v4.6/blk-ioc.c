void get_io_context(struct io_context *ioc)\r\n{\r\nBUG_ON(atomic_long_read(&ioc->refcount) <= 0);\r\natomic_long_inc(&ioc->refcount);\r\n}\r\nstatic void icq_free_icq_rcu(struct rcu_head *head)\r\n{\r\nstruct io_cq *icq = container_of(head, struct io_cq, __rcu_head);\r\nkmem_cache_free(icq->__rcu_icq_cache, icq);\r\n}\r\nstatic void ioc_exit_icq(struct io_cq *icq)\r\n{\r\nstruct elevator_type *et = icq->q->elevator->type;\r\nif (icq->flags & ICQ_EXITED)\r\nreturn;\r\nif (et->ops.elevator_exit_icq_fn)\r\net->ops.elevator_exit_icq_fn(icq);\r\nicq->flags |= ICQ_EXITED;\r\n}\r\nstatic void ioc_destroy_icq(struct io_cq *icq)\r\n{\r\nstruct io_context *ioc = icq->ioc;\r\nstruct request_queue *q = icq->q;\r\nstruct elevator_type *et = q->elevator->type;\r\nlockdep_assert_held(&ioc->lock);\r\nlockdep_assert_held(q->queue_lock);\r\nradix_tree_delete(&ioc->icq_tree, icq->q->id);\r\nhlist_del_init(&icq->ioc_node);\r\nlist_del_init(&icq->q_node);\r\nif (rcu_access_pointer(ioc->icq_hint) == icq)\r\nrcu_assign_pointer(ioc->icq_hint, NULL);\r\nioc_exit_icq(icq);\r\nicq->__rcu_icq_cache = et->icq_cache;\r\ncall_rcu(&icq->__rcu_head, icq_free_icq_rcu);\r\n}\r\nstatic void ioc_release_fn(struct work_struct *work)\r\n{\r\nstruct io_context *ioc = container_of(work, struct io_context,\r\nrelease_work);\r\nunsigned long flags;\r\nspin_lock_irqsave_nested(&ioc->lock, flags, 1);\r\nwhile (!hlist_empty(&ioc->icq_list)) {\r\nstruct io_cq *icq = hlist_entry(ioc->icq_list.first,\r\nstruct io_cq, ioc_node);\r\nstruct request_queue *q = icq->q;\r\nif (spin_trylock(q->queue_lock)) {\r\nioc_destroy_icq(icq);\r\nspin_unlock(q->queue_lock);\r\n} else {\r\nspin_unlock_irqrestore(&ioc->lock, flags);\r\ncpu_relax();\r\nspin_lock_irqsave_nested(&ioc->lock, flags, 1);\r\n}\r\n}\r\nspin_unlock_irqrestore(&ioc->lock, flags);\r\nkmem_cache_free(iocontext_cachep, ioc);\r\n}\r\nvoid put_io_context(struct io_context *ioc)\r\n{\r\nunsigned long flags;\r\nbool free_ioc = false;\r\nif (ioc == NULL)\r\nreturn;\r\nBUG_ON(atomic_long_read(&ioc->refcount) <= 0);\r\nif (atomic_long_dec_and_test(&ioc->refcount)) {\r\nspin_lock_irqsave(&ioc->lock, flags);\r\nif (!hlist_empty(&ioc->icq_list))\r\nqueue_work(system_power_efficient_wq,\r\n&ioc->release_work);\r\nelse\r\nfree_ioc = true;\r\nspin_unlock_irqrestore(&ioc->lock, flags);\r\n}\r\nif (free_ioc)\r\nkmem_cache_free(iocontext_cachep, ioc);\r\n}\r\nvoid put_io_context_active(struct io_context *ioc)\r\n{\r\nunsigned long flags;\r\nstruct io_cq *icq;\r\nif (!atomic_dec_and_test(&ioc->active_ref)) {\r\nput_io_context(ioc);\r\nreturn;\r\n}\r\nretry:\r\nspin_lock_irqsave_nested(&ioc->lock, flags, 1);\r\nhlist_for_each_entry(icq, &ioc->icq_list, ioc_node) {\r\nif (icq->flags & ICQ_EXITED)\r\ncontinue;\r\nif (spin_trylock(icq->q->queue_lock)) {\r\nioc_exit_icq(icq);\r\nspin_unlock(icq->q->queue_lock);\r\n} else {\r\nspin_unlock_irqrestore(&ioc->lock, flags);\r\ncpu_relax();\r\ngoto retry;\r\n}\r\n}\r\nspin_unlock_irqrestore(&ioc->lock, flags);\r\nput_io_context(ioc);\r\n}\r\nvoid exit_io_context(struct task_struct *task)\r\n{\r\nstruct io_context *ioc;\r\ntask_lock(task);\r\nioc = task->io_context;\r\ntask->io_context = NULL;\r\ntask_unlock(task);\r\natomic_dec(&ioc->nr_tasks);\r\nput_io_context_active(ioc);\r\n}\r\nvoid ioc_clear_queue(struct request_queue *q)\r\n{\r\nlockdep_assert_held(q->queue_lock);\r\nwhile (!list_empty(&q->icq_list)) {\r\nstruct io_cq *icq = list_entry(q->icq_list.next,\r\nstruct io_cq, q_node);\r\nstruct io_context *ioc = icq->ioc;\r\nspin_lock(&ioc->lock);\r\nioc_destroy_icq(icq);\r\nspin_unlock(&ioc->lock);\r\n}\r\n}\r\nint create_task_io_context(struct task_struct *task, gfp_t gfp_flags, int node)\r\n{\r\nstruct io_context *ioc;\r\nint ret;\r\nioc = kmem_cache_alloc_node(iocontext_cachep, gfp_flags | __GFP_ZERO,\r\nnode);\r\nif (unlikely(!ioc))\r\nreturn -ENOMEM;\r\natomic_long_set(&ioc->refcount, 1);\r\natomic_set(&ioc->nr_tasks, 1);\r\natomic_set(&ioc->active_ref, 1);\r\nspin_lock_init(&ioc->lock);\r\nINIT_RADIX_TREE(&ioc->icq_tree, GFP_ATOMIC | __GFP_HIGH);\r\nINIT_HLIST_HEAD(&ioc->icq_list);\r\nINIT_WORK(&ioc->release_work, ioc_release_fn);\r\ntask_lock(task);\r\nif (!task->io_context &&\r\n(task == current || !(task->flags & PF_EXITING)))\r\ntask->io_context = ioc;\r\nelse\r\nkmem_cache_free(iocontext_cachep, ioc);\r\nret = task->io_context ? 0 : -EBUSY;\r\ntask_unlock(task);\r\nreturn ret;\r\n}\r\nstruct io_context *get_task_io_context(struct task_struct *task,\r\ngfp_t gfp_flags, int node)\r\n{\r\nstruct io_context *ioc;\r\nmight_sleep_if(gfpflags_allow_blocking(gfp_flags));\r\ndo {\r\ntask_lock(task);\r\nioc = task->io_context;\r\nif (likely(ioc)) {\r\nget_io_context(ioc);\r\ntask_unlock(task);\r\nreturn ioc;\r\n}\r\ntask_unlock(task);\r\n} while (!create_task_io_context(task, gfp_flags, node));\r\nreturn NULL;\r\n}\r\nstruct io_cq *ioc_lookup_icq(struct io_context *ioc, struct request_queue *q)\r\n{\r\nstruct io_cq *icq;\r\nlockdep_assert_held(q->queue_lock);\r\nrcu_read_lock();\r\nicq = rcu_dereference(ioc->icq_hint);\r\nif (icq && icq->q == q)\r\ngoto out;\r\nicq = radix_tree_lookup(&ioc->icq_tree, q->id);\r\nif (icq && icq->q == q)\r\nrcu_assign_pointer(ioc->icq_hint, icq);\r\nelse\r\nicq = NULL;\r\nout:\r\nrcu_read_unlock();\r\nreturn icq;\r\n}\r\nstruct io_cq *ioc_create_icq(struct io_context *ioc, struct request_queue *q,\r\ngfp_t gfp_mask)\r\n{\r\nstruct elevator_type *et = q->elevator->type;\r\nstruct io_cq *icq;\r\nicq = kmem_cache_alloc_node(et->icq_cache, gfp_mask | __GFP_ZERO,\r\nq->node);\r\nif (!icq)\r\nreturn NULL;\r\nif (radix_tree_maybe_preload(gfp_mask) < 0) {\r\nkmem_cache_free(et->icq_cache, icq);\r\nreturn NULL;\r\n}\r\nicq->ioc = ioc;\r\nicq->q = q;\r\nINIT_LIST_HEAD(&icq->q_node);\r\nINIT_HLIST_NODE(&icq->ioc_node);\r\nspin_lock_irq(q->queue_lock);\r\nspin_lock(&ioc->lock);\r\nif (likely(!radix_tree_insert(&ioc->icq_tree, q->id, icq))) {\r\nhlist_add_head(&icq->ioc_node, &ioc->icq_list);\r\nlist_add(&icq->q_node, &q->icq_list);\r\nif (et->ops.elevator_init_icq_fn)\r\net->ops.elevator_init_icq_fn(icq);\r\n} else {\r\nkmem_cache_free(et->icq_cache, icq);\r\nicq = ioc_lookup_icq(ioc, q);\r\nif (!icq)\r\nprintk(KERN_ERR "cfq: icq link failed!\n");\r\n}\r\nspin_unlock(&ioc->lock);\r\nspin_unlock_irq(q->queue_lock);\r\nradix_tree_preload_end();\r\nreturn icq;\r\n}\r\nstatic int __init blk_ioc_init(void)\r\n{\r\niocontext_cachep = kmem_cache_create("blkdev_ioc",\r\nsizeof(struct io_context), 0, SLAB_PANIC, NULL);\r\nreturn 0;\r\n}
