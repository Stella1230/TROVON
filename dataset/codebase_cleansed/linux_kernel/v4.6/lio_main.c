static void octeon_droq_bh(unsigned long pdev)\r\n{\r\nint q_no;\r\nint reschedule = 0;\r\nstruct octeon_device *oct = (struct octeon_device *)pdev;\r\nstruct octeon_device_priv *oct_priv =\r\n(struct octeon_device_priv *)oct->priv;\r\nfor (q_no = 0; q_no < MAX_OCTEON_OUTPUT_QUEUES; q_no++) {\r\nif (!(oct->io_qmask.oq & (1UL << q_no)))\r\ncontinue;\r\nreschedule |= octeon_droq_process_packets(oct, oct->droq[q_no],\r\nMAX_PACKET_BUDGET);\r\n}\r\nif (reschedule)\r\ntasklet_schedule(&oct_priv->droq_tasklet);\r\n}\r\nstatic int lio_wait_for_oq_pkts(struct octeon_device *oct)\r\n{\r\nstruct octeon_device_priv *oct_priv =\r\n(struct octeon_device_priv *)oct->priv;\r\nint retry = 100, pkt_cnt = 0, pending_pkts = 0;\r\nint i;\r\ndo {\r\npending_pkts = 0;\r\nfor (i = 0; i < MAX_OCTEON_OUTPUT_QUEUES; i++) {\r\nif (!(oct->io_qmask.oq & (1UL << i)))\r\ncontinue;\r\npkt_cnt += octeon_droq_check_hw_for_pkts(oct,\r\noct->droq[i]);\r\n}\r\nif (pkt_cnt > 0) {\r\npending_pkts += pkt_cnt;\r\ntasklet_schedule(&oct_priv->droq_tasklet);\r\n}\r\npkt_cnt = 0;\r\nschedule_timeout_uninterruptible(1);\r\n} while (retry-- && pending_pkts);\r\nreturn pkt_cnt;\r\n}\r\nvoid octeon_report_tx_completion_to_bql(void *txq, unsigned int pkts_compl,\r\nunsigned int bytes_compl)\r\n{\r\nstruct netdev_queue *netdev_queue = txq;\r\nnetdev_tx_completed_queue(netdev_queue, pkts_compl, bytes_compl);\r\n}\r\nvoid octeon_update_tx_completion_counters(void *buf, int reqtype,\r\nunsigned int *pkts_compl,\r\nunsigned int *bytes_compl)\r\n{\r\nstruct octnet_buf_free_info *finfo;\r\nstruct sk_buff *skb = NULL;\r\nstruct octeon_soft_command *sc;\r\nswitch (reqtype) {\r\ncase REQTYPE_NORESP_NET:\r\ncase REQTYPE_NORESP_NET_SG:\r\nfinfo = buf;\r\nskb = finfo->skb;\r\nbreak;\r\ncase REQTYPE_RESP_NET_SG:\r\ncase REQTYPE_RESP_NET:\r\nsc = buf;\r\nskb = sc->callback_arg;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\n(*pkts_compl)++;\r\n*bytes_compl += skb->len;\r\n}\r\nvoid octeon_report_sent_bytes_to_bql(void *buf, int reqtype)\r\n{\r\nstruct octnet_buf_free_info *finfo;\r\nstruct sk_buff *skb;\r\nstruct octeon_soft_command *sc;\r\nstruct netdev_queue *txq;\r\nswitch (reqtype) {\r\ncase REQTYPE_NORESP_NET:\r\ncase REQTYPE_NORESP_NET_SG:\r\nfinfo = buf;\r\nskb = finfo->skb;\r\nbreak;\r\ncase REQTYPE_RESP_NET_SG:\r\ncase REQTYPE_RESP_NET:\r\nsc = buf;\r\nskb = sc->callback_arg;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\ntxq = netdev_get_tx_queue(skb->dev, skb_get_queue_mapping(skb));\r\nnetdev_tx_sent_queue(txq, skb->len);\r\n}\r\nint octeon_console_debug_enabled(u32 console)\r\n{\r\nreturn (console_bitmask >> (console)) & 0x1;\r\n}\r\nstatic void force_io_queues_off(struct octeon_device *oct)\r\n{\r\nif ((oct->chip_id == OCTEON_CN66XX) ||\r\n(oct->chip_id == OCTEON_CN68XX)) {\r\nocteon_write_csr(oct, CN6XXX_SLI_PKT_INSTR_ENB, 0);\r\nocteon_write_csr(oct, CN6XXX_SLI_PKT_OUT_ENB, 0);\r\n}\r\n}\r\nstatic int wait_for_pending_requests(struct octeon_device *oct)\r\n{\r\nint i, pcount = 0;\r\nfor (i = 0; i < 100; i++) {\r\npcount =\r\natomic_read(&oct->response_list\r\n[OCTEON_ORDERED_SC_LIST].pending_req_count);\r\nif (pcount)\r\nschedule_timeout_uninterruptible(HZ / 10);\r\nelse\r\nbreak;\r\n}\r\nif (pcount)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline void pcierror_quiesce_device(struct octeon_device *oct)\r\n{\r\nint i;\r\nforce_io_queues_off(oct);\r\nschedule_timeout_uninterruptible(100);\r\nif (wait_for_pending_requests(oct))\r\ndev_err(&oct->pci_dev->dev, "There were pending requests\n");\r\nfor (i = 0; i < MAX_OCTEON_INSTR_QUEUES; i++) {\r\nstruct octeon_instr_queue *iq;\r\nif (!(oct->io_qmask.iq & (1UL << i)))\r\ncontinue;\r\niq = oct->instr_queue[i];\r\nif (atomic_read(&iq->instr_pending)) {\r\nspin_lock_bh(&iq->lock);\r\niq->fill_cnt = 0;\r\niq->octeon_read_index = iq->host_write_index;\r\niq->stats.instr_processed +=\r\natomic_read(&iq->instr_pending);\r\nlio_process_iq_request_list(oct, iq);\r\nspin_unlock_bh(&iq->lock);\r\n}\r\n}\r\nlio_process_ordered_list(oct, 1);\r\n}\r\nstatic void cleanup_aer_uncorrect_error_status(struct pci_dev *dev)\r\n{\r\nint pos = 0x100;\r\nu32 status, mask;\r\npr_info("%s :\n", __func__);\r\npci_read_config_dword(dev, pos + PCI_ERR_UNCOR_STATUS, &status);\r\npci_read_config_dword(dev, pos + PCI_ERR_UNCOR_SEVER, &mask);\r\nif (dev->error_state == pci_channel_io_normal)\r\nstatus &= ~mask;\r\nelse\r\nstatus &= mask;\r\npci_write_config_dword(dev, pos + PCI_ERR_UNCOR_STATUS, status);\r\n}\r\nstatic void stop_pci_io(struct octeon_device *oct)\r\n{\r\natomic_set(&oct->status, OCT_DEV_IN_RESET);\r\npci_disable_device(oct->pci_dev);\r\noct->fn_list.disable_interrupt(oct->chip);\r\npcierror_quiesce_device(oct);\r\nfree_irq(oct->pci_dev->irq, oct);\r\nif (oct->flags & LIO_FLAG_MSI_ENABLED)\r\npci_disable_msi(oct->pci_dev);\r\ndev_dbg(&oct->pci_dev->dev, "Device state is now %s\n",\r\nlio_get_state_string(&oct->status));\r\ncleanup_aer_uncorrect_error_status(oct->pci_dev);\r\n}\r\nstatic pci_ers_result_t liquidio_pcie_error_detected(struct pci_dev *pdev,\r\npci_channel_state_t state)\r\n{\r\nstruct octeon_device *oct = pci_get_drvdata(pdev);\r\nif (state == pci_channel_io_normal) {\r\ndev_err(&oct->pci_dev->dev, "Non-correctable non-fatal error reported:\n");\r\ncleanup_aer_uncorrect_error_status(oct->pci_dev);\r\nreturn PCI_ERS_RESULT_CAN_RECOVER;\r\n}\r\ndev_err(&oct->pci_dev->dev, "Non-correctable FATAL reported by PCI AER driver\n");\r\nstop_pci_io(oct);\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\nstatic pci_ers_result_t liquidio_pcie_mmio_enabled(struct pci_dev *pdev)\r\n{\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n}\r\nstatic pci_ers_result_t liquidio_pcie_slot_reset(struct pci_dev *pdev)\r\n{\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n}\r\nstatic void liquidio_pcie_resume(struct pci_dev *pdev)\r\n{\r\n}\r\nstatic int liquidio_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nreturn 0;\r\n}\r\nstatic int liquidio_resume(struct pci_dev *pdev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int liquidio_init_pci(void)\r\n{\r\nreturn pci_register_driver(&liquidio_pci_driver);\r\n}\r\nstatic void liquidio_deinit_pci(void)\r\n{\r\npci_unregister_driver(&liquidio_pci_driver);\r\n}\r\nstatic inline int ifstate_check(struct lio *lio, int state_flag)\r\n{\r\nreturn atomic_read(&lio->ifstate) & state_flag;\r\n}\r\nstatic inline void ifstate_set(struct lio *lio, int state_flag)\r\n{\r\natomic_set(&lio->ifstate, (atomic_read(&lio->ifstate) | state_flag));\r\n}\r\nstatic inline void ifstate_reset(struct lio *lio, int state_flag)\r\n{\r\natomic_set(&lio->ifstate, (atomic_read(&lio->ifstate) & ~(state_flag)));\r\n}\r\nstatic inline void txqs_stop(struct net_device *netdev)\r\n{\r\nif (netif_is_multiqueue(netdev)) {\r\nint i;\r\nfor (i = 0; i < netdev->num_tx_queues; i++)\r\nnetif_stop_subqueue(netdev, i);\r\n} else {\r\nnetif_stop_queue(netdev);\r\n}\r\n}\r\nstatic inline void txqs_start(struct net_device *netdev)\r\n{\r\nif (netif_is_multiqueue(netdev)) {\r\nint i;\r\nfor (i = 0; i < netdev->num_tx_queues; i++)\r\nnetif_start_subqueue(netdev, i);\r\n} else {\r\nnetif_start_queue(netdev);\r\n}\r\n}\r\nstatic inline void txqs_wake(struct net_device *netdev)\r\n{\r\nif (netif_is_multiqueue(netdev)) {\r\nint i;\r\nfor (i = 0; i < netdev->num_tx_queues; i++)\r\nnetif_wake_subqueue(netdev, i);\r\n} else {\r\nnetif_wake_queue(netdev);\r\n}\r\n}\r\nstatic void stop_txq(struct net_device *netdev)\r\n{\r\ntxqs_stop(netdev);\r\n}\r\nstatic void start_txq(struct net_device *netdev)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nif (lio->linfo.link.s.status) {\r\ntxqs_start(netdev);\r\nreturn;\r\n}\r\n}\r\nstatic inline void wake_q(struct net_device *netdev, int q)\r\n{\r\nif (netif_is_multiqueue(netdev))\r\nnetif_wake_subqueue(netdev, q);\r\nelse\r\nnetif_wake_queue(netdev);\r\n}\r\nstatic inline void stop_q(struct net_device *netdev, int q)\r\n{\r\nif (netif_is_multiqueue(netdev))\r\nnetif_stop_subqueue(netdev, q);\r\nelse\r\nnetif_stop_queue(netdev);\r\n}\r\nstatic inline int check_txq_status(struct lio *lio)\r\n{\r\nint ret_val = 0;\r\nif (netif_is_multiqueue(lio->netdev)) {\r\nint numqs = lio->netdev->num_tx_queues;\r\nint q, iq = 0;\r\nfor (q = 0; q < numqs; q++) {\r\niq = lio->linfo.txpciq[q & (lio->linfo.num_txpciq - 1)];\r\nif (octnet_iq_is_full(lio->oct_dev, iq))\r\ncontinue;\r\nwake_q(lio->netdev, q);\r\nret_val++;\r\n}\r\n} else {\r\nif (octnet_iq_is_full(lio->oct_dev, lio->txq))\r\nreturn 0;\r\nwake_q(lio->netdev, lio->txq);\r\nret_val = 1;\r\n}\r\nreturn ret_val;\r\n}\r\nstatic inline struct list_head *list_delete_head(struct list_head *root)\r\n{\r\nstruct list_head *node;\r\nif ((root->prev == root) && (root->next == root))\r\nnode = NULL;\r\nelse\r\nnode = root->next;\r\nif (node)\r\nlist_del(node);\r\nreturn node;\r\n}\r\nstatic void delete_glist(struct lio *lio)\r\n{\r\nstruct octnic_gather *g;\r\ndo {\r\ng = (struct octnic_gather *)\r\nlist_delete_head(&lio->glist);\r\nif (g) {\r\nif (g->sg)\r\nkfree((void *)((unsigned long)g->sg -\r\ng->adjust));\r\nkfree(g);\r\n}\r\n} while (g);\r\n}\r\nstatic int setup_glist(struct lio *lio)\r\n{\r\nint i;\r\nstruct octnic_gather *g;\r\nINIT_LIST_HEAD(&lio->glist);\r\nfor (i = 0; i < lio->tx_qsize; i++) {\r\ng = kzalloc(sizeof(*g), GFP_KERNEL);\r\nif (!g)\r\nbreak;\r\ng->sg_size =\r\n((ROUNDUP4(OCTNIC_MAX_SG) >> 2) * OCT_SG_ENTRY_SIZE);\r\ng->sg = kmalloc(g->sg_size + 8, GFP_KERNEL);\r\nif (!g->sg) {\r\nkfree(g);\r\nbreak;\r\n}\r\nif (((unsigned long)g->sg) & 7) {\r\ng->adjust = 8 - (((unsigned long)g->sg) & 7);\r\ng->sg = (struct octeon_sg_entry *)\r\n((unsigned long)g->sg + g->adjust);\r\n}\r\nlist_add_tail(&g->list, &lio->glist);\r\n}\r\nif (i == lio->tx_qsize)\r\nreturn 0;\r\ndelete_glist(lio);\r\nreturn 1;\r\n}\r\nstatic void print_link_info(struct net_device *netdev)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nif (atomic_read(&lio->ifstate) & LIO_IFSTATE_REGISTERED) {\r\nstruct oct_link_info *linfo = &lio->linfo;\r\nif (linfo->link.s.status) {\r\nnetif_info(lio, link, lio->netdev, "%d Mbps %s Duplex UP\n",\r\nlinfo->link.s.speed,\r\n(linfo->link.s.duplex) ? "Full" : "Half");\r\n} else {\r\nnetif_info(lio, link, lio->netdev, "Link Down\n");\r\n}\r\n}\r\n}\r\nstatic inline void update_link_status(struct net_device *netdev,\r\nunion oct_link_status *ls)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nif ((lio->intf_open) && (lio->linfo.link.u64 != ls->u64)) {\r\nlio->linfo.link.u64 = ls->u64;\r\nprint_link_info(netdev);\r\nif (lio->linfo.link.s.status) {\r\nnetif_carrier_on(netdev);\r\ntxqs_wake(netdev);\r\n} else {\r\nnetif_carrier_off(netdev);\r\nstop_txq(netdev);\r\n}\r\n}\r\n}\r\nstatic\r\nvoid liquidio_schedule_droq_pkt_handlers(struct octeon_device *oct)\r\n{\r\nstruct octeon_device_priv *oct_priv =\r\n(struct octeon_device_priv *)oct->priv;\r\nu64 oq_no;\r\nstruct octeon_droq *droq;\r\nif (oct->int_status & OCT_DEV_INTR_PKT_DATA) {\r\nfor (oq_no = 0; oq_no < MAX_OCTEON_OUTPUT_QUEUES; oq_no++) {\r\nif (!(oct->droq_intr & (1 << oq_no)))\r\ncontinue;\r\ndroq = oct->droq[oq_no];\r\nif (droq->ops.poll_mode) {\r\ndroq->ops.napi_fn(droq);\r\noct_priv->napi_mask |= (1 << oq_no);\r\n} else {\r\ntasklet_schedule(&oct_priv->droq_tasklet);\r\n}\r\n}\r\n}\r\n}\r\nstatic int octeon_setup_interrupt(struct octeon_device *oct)\r\n{\r\nint irqret, err;\r\nerr = pci_enable_msi(oct->pci_dev);\r\nif (err)\r\ndev_warn(&oct->pci_dev->dev, "Reverting to legacy interrupts. Error: %d\n",\r\nerr);\r\nelse\r\noct->flags |= LIO_FLAG_MSI_ENABLED;\r\nirqret = request_irq(oct->pci_dev->irq, liquidio_intr_handler,\r\nIRQF_SHARED, "octeon", oct);\r\nif (irqret) {\r\nif (oct->flags & LIO_FLAG_MSI_ENABLED)\r\npci_disable_msi(oct->pci_dev);\r\ndev_err(&oct->pci_dev->dev, "Request IRQ failed with code: %d\n",\r\nirqret);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int liquidio_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct octeon_device *oct_dev = NULL;\r\nstruct handshake *hs;\r\noct_dev = octeon_allocate_device(pdev->device,\r\nsizeof(struct octeon_device_priv));\r\nif (!oct_dev) {\r\ndev_err(&pdev->dev, "Unable to allocate device\n");\r\nreturn -ENOMEM;\r\n}\r\ndev_info(&pdev->dev, "Initializing device %x:%x.\n",\r\n(u32)pdev->vendor, (u32)pdev->device);\r\npci_set_drvdata(pdev, oct_dev);\r\noct_dev->pci_dev = (void *)pdev;\r\nhs = &handshake[oct_dev->octeon_id];\r\ninit_completion(&hs->init);\r\ninit_completion(&hs->started);\r\nhs->pci_dev = pdev;\r\nif (oct_dev->octeon_id == 0)\r\ncomplete(&first_stage);\r\nif (octeon_device_init(oct_dev)) {\r\nliquidio_remove(pdev);\r\nreturn -ENOMEM;\r\n}\r\ndev_dbg(&oct_dev->pci_dev->dev, "Device is ready\n");\r\nreturn 0;\r\n}\r\nstatic void octeon_destroy_resources(struct octeon_device *oct)\r\n{\r\nint i;\r\nstruct octeon_device_priv *oct_priv =\r\n(struct octeon_device_priv *)oct->priv;\r\nstruct handshake *hs;\r\nswitch (atomic_read(&oct->status)) {\r\ncase OCT_DEV_RUNNING:\r\ncase OCT_DEV_CORE_OK:\r\natomic_set(&oct->status, OCT_DEV_IN_RESET);\r\noct->app_mode = CVM_DRV_INVALID_APP;\r\ndev_dbg(&oct->pci_dev->dev, "Device state is now %s\n",\r\nlio_get_state_string(&oct->status));\r\nschedule_timeout_uninterruptible(HZ / 10);\r\ncase OCT_DEV_HOST_OK:\r\ncase OCT_DEV_CONSOLE_INIT_DONE:\r\nocteon_remove_consoles(oct);\r\ncase OCT_DEV_IO_QUEUES_DONE:\r\nif (wait_for_pending_requests(oct))\r\ndev_err(&oct->pci_dev->dev, "There were pending requests\n");\r\nif (lio_wait_for_instr_fetch(oct))\r\ndev_err(&oct->pci_dev->dev, "IQ had pending instructions\n");\r\noct->fn_list.disable_io_queues(oct);\r\nif (lio_wait_for_oq_pkts(oct))\r\ndev_err(&oct->pci_dev->dev, "OQ had pending packets\n");\r\noct->fn_list.disable_interrupt(oct->chip);\r\nfree_irq(oct->pci_dev->irq, oct);\r\nif (oct->flags & LIO_FLAG_MSI_ENABLED)\r\npci_disable_msi(oct->pci_dev);\r\noct->fn_list.soft_reset(oct);\r\npci_disable_device(oct->pci_dev);\r\ncase OCT_DEV_IN_RESET:\r\ncase OCT_DEV_DROQ_INIT_DONE:\r\nmdelay(100);\r\nfor (i = 0; i < MAX_OCTEON_OUTPUT_QUEUES; i++) {\r\nif (!(oct->io_qmask.oq & (1UL << i)))\r\ncontinue;\r\nocteon_delete_droq(oct, i);\r\n}\r\nfor (i = 0; i < MAX_OCTEON_DEVICES; i++) {\r\nhs = &handshake[i];\r\nif (hs->pci_dev) {\r\nhandshake[oct->octeon_id].init_ok = 0;\r\ncomplete(&handshake[oct->octeon_id].init);\r\nhandshake[oct->octeon_id].started_ok = 0;\r\ncomplete(&handshake[oct->octeon_id].started);\r\n}\r\n}\r\ncase OCT_DEV_RESP_LIST_INIT_DONE:\r\nocteon_delete_response_list(oct);\r\ncase OCT_DEV_SC_BUFF_POOL_INIT_DONE:\r\nocteon_free_sc_buffer_pool(oct);\r\ncase OCT_DEV_INSTR_QUEUE_INIT_DONE:\r\nfor (i = 0; i < MAX_OCTEON_INSTR_QUEUES; i++) {\r\nif (!(oct->io_qmask.iq & (1UL << i)))\r\ncontinue;\r\nocteon_delete_instr_queue(oct, i);\r\n}\r\ncase OCT_DEV_DISPATCH_INIT_DONE:\r\nocteon_delete_dispatch_list(oct);\r\ncancel_delayed_work_sync(&oct->nic_poll_work.work);\r\ncase OCT_DEV_PCI_MAP_DONE:\r\nocteon_unmap_pci_barx(oct, 0);\r\nocteon_unmap_pci_barx(oct, 1);\r\ncase OCT_DEV_BEGIN_STATE:\r\nbreak;\r\n}\r\ntasklet_kill(&oct_priv->droq_tasklet);\r\n}\r\nstatic void send_rx_ctrl_cmd(struct lio *lio, int start_stop)\r\n{\r\nstruct octnic_ctrl_pkt nctrl;\r\nstruct octnic_ctrl_params nparams;\r\nmemset(&nctrl, 0, sizeof(struct octnic_ctrl_pkt));\r\nnctrl.ncmd.s.cmd = OCTNET_CMD_RX_CTL;\r\nnctrl.ncmd.s.param1 = lio->linfo.ifidx;\r\nnctrl.ncmd.s.param2 = start_stop;\r\nnctrl.netpndev = (u64)lio->netdev;\r\nnparams.resp_order = OCTEON_RESP_NORESPONSE;\r\nif (octnet_send_nic_ctrl_pkt(lio->oct_dev, &nctrl, nparams) < 0)\r\nnetif_info(lio, rx_err, lio->netdev, "Failed to send RX Control message\n");\r\n}\r\nstatic void liquidio_destroy_nic_device(struct octeon_device *oct, int ifidx)\r\n{\r\nstruct net_device *netdev = oct->props[ifidx].netdev;\r\nstruct lio *lio;\r\nif (!netdev) {\r\ndev_err(&oct->pci_dev->dev, "%s No netdevice ptr for index %d\n",\r\n__func__, ifidx);\r\nreturn;\r\n}\r\nlio = GET_LIO(netdev);\r\ndev_dbg(&oct->pci_dev->dev, "NIC device cleanup\n");\r\nsend_rx_ctrl_cmd(lio, 0);\r\nif (atomic_read(&lio->ifstate) & LIO_IFSTATE_RUNNING)\r\ntxqs_stop(netdev);\r\nif (atomic_read(&lio->ifstate) & LIO_IFSTATE_REGISTERED)\r\nunregister_netdev(netdev);\r\ndelete_glist(lio);\r\nfree_netdev(netdev);\r\noct->props[ifidx].netdev = NULL;\r\n}\r\nstatic int liquidio_stop_nic_module(struct octeon_device *oct)\r\n{\r\nint i, j;\r\nstruct lio *lio;\r\ndev_dbg(&oct->pci_dev->dev, "Stopping network interfaces\n");\r\nif (!oct->ifcount) {\r\ndev_err(&oct->pci_dev->dev, "Init for Octeon was not completed\n");\r\nreturn 1;\r\n}\r\nfor (i = 0; i < oct->ifcount; i++) {\r\nlio = GET_LIO(oct->props[i].netdev);\r\nfor (j = 0; j < lio->linfo.num_rxpciq; j++)\r\nocteon_unregister_droq_ops(oct, lio->linfo.rxpciq[j]);\r\n}\r\nfor (i = 0; i < oct->ifcount; i++)\r\nliquidio_destroy_nic_device(oct, i);\r\ndev_dbg(&oct->pci_dev->dev, "Network interfaces stopped\n");\r\nreturn 0;\r\n}\r\nstatic void liquidio_remove(struct pci_dev *pdev)\r\n{\r\nstruct octeon_device *oct_dev = pci_get_drvdata(pdev);\r\ndev_dbg(&oct_dev->pci_dev->dev, "Stopping device\n");\r\nif (oct_dev->app_mode && (oct_dev->app_mode == CVM_DRV_NIC_APP))\r\nliquidio_stop_nic_module(oct_dev);\r\nocteon_destroy_resources(oct_dev);\r\ndev_info(&oct_dev->pci_dev->dev, "Device removed\n");\r\nocteon_free_device_mem(oct_dev);\r\n}\r\nstatic int octeon_chip_specific_setup(struct octeon_device *oct)\r\n{\r\nu32 dev_id, rev_id;\r\nint ret = 1;\r\npci_read_config_dword(oct->pci_dev, 0, &dev_id);\r\npci_read_config_dword(oct->pci_dev, 8, &rev_id);\r\noct->rev_id = rev_id & 0xff;\r\nswitch (dev_id) {\r\ncase OCTEON_CN68XX_PCIID:\r\noct->chip_id = OCTEON_CN68XX;\r\nret = lio_setup_cn68xx_octeon_device(oct);\r\nbreak;\r\ncase OCTEON_CN66XX_PCIID:\r\noct->chip_id = OCTEON_CN66XX;\r\nret = lio_setup_cn66xx_octeon_device(oct);\r\nbreak;\r\ndefault:\r\ndev_err(&oct->pci_dev->dev, "Unknown device found (dev_id: %x)\n",\r\ndev_id);\r\n}\r\nif (!ret)\r\ndev_info(&oct->pci_dev->dev, "CN68XX PASS%d.%d %s\n",\r\nOCTEON_MAJOR_REV(oct),\r\nOCTEON_MINOR_REV(oct),\r\nocteon_get_conf(oct)->card_name);\r\nreturn ret;\r\n}\r\nstatic int octeon_pci_os_setup(struct octeon_device *oct)\r\n{\r\nif (pci_enable_device(oct->pci_dev)) {\r\ndev_err(&oct->pci_dev->dev, "pci_enable_device failed\n");\r\nreturn 1;\r\n}\r\nif (dma_set_mask_and_coherent(&oct->pci_dev->dev, DMA_BIT_MASK(64))) {\r\ndev_err(&oct->pci_dev->dev, "Unexpected DMA device capability\n");\r\nreturn 1;\r\n}\r\npci_set_master(oct->pci_dev);\r\nreturn 0;\r\n}\r\nstatic inline int check_txq_state(struct lio *lio, struct sk_buff *skb)\r\n{\r\nint q = 0, iq = 0;\r\nif (netif_is_multiqueue(lio->netdev)) {\r\nq = skb->queue_mapping;\r\niq = lio->linfo.txpciq[(q & (lio->linfo.num_txpciq - 1))];\r\n} else {\r\niq = lio->txq;\r\n}\r\nif (octnet_iq_is_full(lio->oct_dev, iq))\r\nreturn 0;\r\nwake_q(lio->netdev, q);\r\nreturn 1;\r\n}\r\nstatic void free_netbuf(void *buf)\r\n{\r\nstruct sk_buff *skb;\r\nstruct octnet_buf_free_info *finfo;\r\nstruct lio *lio;\r\nfinfo = (struct octnet_buf_free_info *)buf;\r\nskb = finfo->skb;\r\nlio = finfo->lio;\r\ndma_unmap_single(&lio->oct_dev->pci_dev->dev, finfo->dptr, skb->len,\r\nDMA_TO_DEVICE);\r\ncheck_txq_state(lio, skb);\r\nrecv_buffer_free((struct sk_buff *)skb);\r\n}\r\nstatic void free_netsgbuf(void *buf)\r\n{\r\nstruct octnet_buf_free_info *finfo;\r\nstruct sk_buff *skb;\r\nstruct lio *lio;\r\nstruct octnic_gather *g;\r\nint i, frags;\r\nfinfo = (struct octnet_buf_free_info *)buf;\r\nskb = finfo->skb;\r\nlio = finfo->lio;\r\ng = finfo->g;\r\nfrags = skb_shinfo(skb)->nr_frags;\r\ndma_unmap_single(&lio->oct_dev->pci_dev->dev,\r\ng->sg[0].ptr[0], (skb->len - skb->data_len),\r\nDMA_TO_DEVICE);\r\ni = 1;\r\nwhile (frags--) {\r\nstruct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i - 1];\r\npci_unmap_page((lio->oct_dev)->pci_dev,\r\ng->sg[(i >> 2)].ptr[(i & 3)],\r\nfrag->size, DMA_TO_DEVICE);\r\ni++;\r\n}\r\ndma_unmap_single(&lio->oct_dev->pci_dev->dev,\r\nfinfo->dptr, g->sg_size,\r\nDMA_TO_DEVICE);\r\nspin_lock(&lio->lock);\r\nlist_add_tail(&g->list, &lio->glist);\r\nspin_unlock(&lio->lock);\r\ncheck_txq_state(lio, skb);\r\nrecv_buffer_free((struct sk_buff *)skb);\r\n}\r\nstatic void free_netsgbuf_with_resp(void *buf)\r\n{\r\nstruct octeon_soft_command *sc;\r\nstruct octnet_buf_free_info *finfo;\r\nstruct sk_buff *skb;\r\nstruct lio *lio;\r\nstruct octnic_gather *g;\r\nint i, frags;\r\nsc = (struct octeon_soft_command *)buf;\r\nskb = (struct sk_buff *)sc->callback_arg;\r\nfinfo = (struct octnet_buf_free_info *)&skb->cb;\r\nlio = finfo->lio;\r\ng = finfo->g;\r\nfrags = skb_shinfo(skb)->nr_frags;\r\ndma_unmap_single(&lio->oct_dev->pci_dev->dev,\r\ng->sg[0].ptr[0], (skb->len - skb->data_len),\r\nDMA_TO_DEVICE);\r\ni = 1;\r\nwhile (frags--) {\r\nstruct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i - 1];\r\npci_unmap_page((lio->oct_dev)->pci_dev,\r\ng->sg[(i >> 2)].ptr[(i & 3)],\r\nfrag->size, DMA_TO_DEVICE);\r\ni++;\r\n}\r\ndma_unmap_single(&lio->oct_dev->pci_dev->dev,\r\nfinfo->dptr, g->sg_size,\r\nDMA_TO_DEVICE);\r\nspin_lock(&lio->lock);\r\nlist_add_tail(&g->list, &lio->glist);\r\nspin_unlock(&lio->lock);\r\ncheck_txq_state(lio, skb);\r\n}\r\nstatic int liquidio_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)\r\n{\r\nstruct lio *lio = container_of(ptp, struct lio, ptp_info);\r\nstruct octeon_device *oct = (struct octeon_device *)lio->oct_dev;\r\nu64 comp, delta;\r\nunsigned long flags;\r\nbool neg_adj = false;\r\nif (ppb < 0) {\r\nneg_adj = true;\r\nppb = -ppb;\r\n}\r\ndelta = (u64)ppb << 32;\r\ndo_div(delta, oct->coproc_clock_rate);\r\nspin_lock_irqsave(&lio->ptp_lock, flags);\r\ncomp = lio_pci_readq(oct, CN6XXX_MIO_PTP_CLOCK_COMP);\r\nif (neg_adj)\r\ncomp -= delta;\r\nelse\r\ncomp += delta;\r\nlio_pci_writeq(oct, comp, CN6XXX_MIO_PTP_CLOCK_COMP);\r\nspin_unlock_irqrestore(&lio->ptp_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int liquidio_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)\r\n{\r\nunsigned long flags;\r\nstruct lio *lio = container_of(ptp, struct lio, ptp_info);\r\nspin_lock_irqsave(&lio->ptp_lock, flags);\r\nlio->ptp_adjust += delta;\r\nspin_unlock_irqrestore(&lio->ptp_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int liquidio_ptp_gettime(struct ptp_clock_info *ptp,\r\nstruct timespec64 *ts)\r\n{\r\nu64 ns;\r\nunsigned long flags;\r\nstruct lio *lio = container_of(ptp, struct lio, ptp_info);\r\nstruct octeon_device *oct = (struct octeon_device *)lio->oct_dev;\r\nspin_lock_irqsave(&lio->ptp_lock, flags);\r\nns = lio_pci_readq(oct, CN6XXX_MIO_PTP_CLOCK_HI);\r\nns += lio->ptp_adjust;\r\nspin_unlock_irqrestore(&lio->ptp_lock, flags);\r\n*ts = ns_to_timespec64(ns);\r\nreturn 0;\r\n}\r\nstatic int liquidio_ptp_settime(struct ptp_clock_info *ptp,\r\nconst struct timespec64 *ts)\r\n{\r\nu64 ns;\r\nunsigned long flags;\r\nstruct lio *lio = container_of(ptp, struct lio, ptp_info);\r\nstruct octeon_device *oct = (struct octeon_device *)lio->oct_dev;\r\nns = timespec_to_ns(ts);\r\nspin_lock_irqsave(&lio->ptp_lock, flags);\r\nlio_pci_writeq(oct, ns, CN6XXX_MIO_PTP_CLOCK_HI);\r\nlio->ptp_adjust = 0;\r\nspin_unlock_irqrestore(&lio->ptp_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int liquidio_ptp_enable(struct ptp_clock_info *ptp,\r\nstruct ptp_clock_request *rq, int on)\r\n{\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic void oct_ptp_open(struct net_device *netdev)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = (struct octeon_device *)lio->oct_dev;\r\nspin_lock_init(&lio->ptp_lock);\r\nsnprintf(lio->ptp_info.name, 16, "%s", netdev->name);\r\nlio->ptp_info.owner = THIS_MODULE;\r\nlio->ptp_info.max_adj = 250000000;\r\nlio->ptp_info.n_alarm = 0;\r\nlio->ptp_info.n_ext_ts = 0;\r\nlio->ptp_info.n_per_out = 0;\r\nlio->ptp_info.pps = 0;\r\nlio->ptp_info.adjfreq = liquidio_ptp_adjfreq;\r\nlio->ptp_info.adjtime = liquidio_ptp_adjtime;\r\nlio->ptp_info.gettime64 = liquidio_ptp_gettime;\r\nlio->ptp_info.settime64 = liquidio_ptp_settime;\r\nlio->ptp_info.enable = liquidio_ptp_enable;\r\nlio->ptp_adjust = 0;\r\nlio->ptp_clock = ptp_clock_register(&lio->ptp_info,\r\n&oct->pci_dev->dev);\r\nif (IS_ERR(lio->ptp_clock))\r\nlio->ptp_clock = NULL;\r\n}\r\nstatic void liquidio_ptp_init(struct octeon_device *oct)\r\n{\r\nu64 clock_comp, cfg;\r\nclock_comp = (u64)NSEC_PER_SEC << 32;\r\ndo_div(clock_comp, oct->coproc_clock_rate);\r\nlio_pci_writeq(oct, clock_comp, CN6XXX_MIO_PTP_CLOCK_COMP);\r\ncfg = lio_pci_readq(oct, CN6XXX_MIO_PTP_CLOCK_CFG);\r\nlio_pci_writeq(oct, cfg | 0x01, CN6XXX_MIO_PTP_CLOCK_CFG);\r\n}\r\nstatic int load_firmware(struct octeon_device *oct)\r\n{\r\nint ret = 0;\r\nconst struct firmware *fw;\r\nchar fw_name[LIO_MAX_FW_FILENAME_LEN];\r\nchar *tmp_fw_type;\r\nif (strncmp(fw_type, LIO_FW_NAME_TYPE_NONE,\r\nsizeof(LIO_FW_NAME_TYPE_NONE)) == 0) {\r\ndev_info(&oct->pci_dev->dev, "Skipping firmware load\n");\r\nreturn ret;\r\n}\r\nif (fw_type[0] == '\0')\r\ntmp_fw_type = LIO_FW_NAME_TYPE_NIC;\r\nelse\r\ntmp_fw_type = fw_type;\r\nsprintf(fw_name, "%s%s%s_%s%s", LIO_FW_DIR, LIO_FW_BASE_NAME,\r\nocteon_get_conf(oct)->card_name, tmp_fw_type,\r\nLIO_FW_NAME_SUFFIX);\r\nret = request_firmware(&fw, fw_name, &oct->pci_dev->dev);\r\nif (ret) {\r\ndev_err(&oct->pci_dev->dev, "Request firmware failed. Could not find file %s.\n.",\r\nfw_name);\r\nreturn ret;\r\n}\r\nret = octeon_download_firmware(oct, fw->data, fw->size);\r\nrelease_firmware(fw);\r\nreturn ret;\r\n}\r\nstatic int octeon_setup_droq(struct octeon_device *oct, int q_no, int num_descs,\r\nint desc_size, void *app_ctx)\r\n{\r\nint ret_val = 0;\r\ndev_dbg(&oct->pci_dev->dev, "Creating Droq: %d\n", q_no);\r\nret_val = octeon_create_droq(oct, q_no, num_descs, desc_size, app_ctx);\r\nif (ret_val < 0)\r\nreturn ret_val;\r\nif (ret_val == 1) {\r\ndev_dbg(&oct->pci_dev->dev, "Using default droq %d\n", q_no);\r\nreturn 0;\r\n}\r\nocteon_set_droq_pkt_op(oct, q_no, 1);\r\nwritel(oct->droq[q_no]->max_count,\r\noct->droq[q_no]->pkts_credit_reg);\r\nreturn ret_val;\r\n}\r\nstatic void if_cfg_callback(struct octeon_device *oct,\r\nu32 status,\r\nvoid *buf)\r\n{\r\nstruct octeon_soft_command *sc = (struct octeon_soft_command *)buf;\r\nstruct liquidio_if_cfg_resp *resp;\r\nstruct liquidio_if_cfg_context *ctx;\r\nresp = (struct liquidio_if_cfg_resp *)sc->virtrptr;\r\nctx = (struct liquidio_if_cfg_context *)sc->ctxptr;\r\noct = lio_get_device(ctx->octeon_id);\r\nif (resp->status)\r\ndev_err(&oct->pci_dev->dev, "nic if cfg instruction failed. Status: %llx\n",\r\nCVM_CAST64(resp->status));\r\nACCESS_ONCE(ctx->cond) = 1;\r\nwmb();\r\nwake_up_interruptible(&ctx->wc);\r\n}\r\nstatic u16 select_q(struct net_device *dev, struct sk_buff *skb,\r\nvoid *accel_priv, select_queue_fallback_t fallback)\r\n{\r\nint qindex;\r\nstruct lio *lio;\r\nlio = GET_LIO(dev);\r\nqindex = skb_rx_queue_recorded(skb) ?\r\nskb_get_rx_queue(skb) : smp_processor_id();\r\nreturn (u16)(qindex & (lio->linfo.num_txpciq - 1));\r\n}\r\nstatic void\r\nliquidio_push_packet(u32 octeon_id,\r\nvoid *skbuff,\r\nu32 len,\r\nunion octeon_rh *rh,\r\nvoid *param)\r\n{\r\nstruct napi_struct *napi = param;\r\nstruct octeon_device *oct = lio_get_device(octeon_id);\r\nstruct sk_buff *skb = (struct sk_buff *)skbuff;\r\nstruct skb_shared_hwtstamps *shhwtstamps;\r\nu64 ns;\r\nstruct net_device *netdev =\r\n(struct net_device *)oct->props[rh->r_dh.link].netdev;\r\nstruct octeon_droq *droq = container_of(param, struct octeon_droq,\r\nnapi);\r\nif (netdev) {\r\nint packet_was_received;\r\nstruct lio *lio = GET_LIO(netdev);\r\nif (!ifstate_check(lio, LIO_IFSTATE_RUNNING)) {\r\nrecv_buffer_free(skb);\r\ndroq->stats.rx_dropped++;\r\nreturn;\r\n}\r\nskb->dev = netdev;\r\nif (rh->r_dh.has_hwtstamp) {\r\nif (ifstate_check(lio,\r\nLIO_IFSTATE_RX_TIMESTAMP_ENABLED)) {\r\nmemcpy(&ns, (skb->data), sizeof(ns));\r\nshhwtstamps = skb_hwtstamps(skb);\r\nshhwtstamps->hwtstamp =\r\nns_to_ktime(ns + lio->ptp_adjust);\r\n}\r\nskb_pull(skb, sizeof(ns));\r\n}\r\nskb->protocol = eth_type_trans(skb, skb->dev);\r\nif ((netdev->features & NETIF_F_RXCSUM) &&\r\n(rh->r_dh.csum_verified == CNNIC_CSUM_VERIFIED))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nelse\r\nskb->ip_summed = CHECKSUM_NONE;\r\npacket_was_received = napi_gro_receive(napi, skb) != GRO_DROP;\r\nif (packet_was_received) {\r\ndroq->stats.rx_bytes_received += len;\r\ndroq->stats.rx_pkts_received++;\r\nnetdev->last_rx = jiffies;\r\n} else {\r\ndroq->stats.rx_dropped++;\r\nnetif_info(lio, rx_err, lio->netdev,\r\n"droq:%d error rx_dropped:%llu\n",\r\ndroq->q_no, droq->stats.rx_dropped);\r\n}\r\n} else {\r\nrecv_buffer_free(skb);\r\n}\r\n}\r\nstatic void napi_schedule_wrapper(void *param)\r\n{\r\nstruct napi_struct *napi = param;\r\nnapi_schedule(napi);\r\n}\r\nstatic void liquidio_napi_drv_callback(void *arg)\r\n{\r\nstruct octeon_droq *droq = arg;\r\nint this_cpu = smp_processor_id();\r\nif (droq->cpu_id == this_cpu) {\r\nnapi_schedule(&droq->napi);\r\n} else {\r\nstruct call_single_data *csd = &droq->csd;\r\ncsd->func = napi_schedule_wrapper;\r\ncsd->info = &droq->napi;\r\ncsd->flags = 0;\r\nsmp_call_function_single_async(droq->cpu_id, csd);\r\n}\r\n}\r\nstatic int liquidio_napi_do_rx(struct octeon_droq *droq, int budget)\r\n{\r\nint work_done;\r\nstruct lio *lio = GET_LIO(droq->napi.dev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nwork_done = octeon_process_droq_poll_cmd(oct, droq->q_no,\r\nPOLL_EVENT_PROCESS_PKTS,\r\nbudget);\r\nif (work_done < 0) {\r\nnetif_info(lio, rx_err, lio->netdev,\r\n"Receive work_done < 0, rxq:%d\n", droq->q_no);\r\ngoto octnet_napi_finish;\r\n}\r\nif (work_done > budget)\r\ndev_err(&oct->pci_dev->dev, ">>>> %s work_done: %d budget: %d\n",\r\n__func__, work_done, budget);\r\nreturn work_done;\r\noctnet_napi_finish:\r\nnapi_complete(&droq->napi);\r\nocteon_process_droq_poll_cmd(oct, droq->q_no, POLL_EVENT_ENABLE_INTR,\r\n0);\r\nreturn 0;\r\n}\r\nstatic int liquidio_napi_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct octeon_droq *droq;\r\nint work_done;\r\ndroq = container_of(napi, struct octeon_droq, napi);\r\nwork_done = liquidio_napi_do_rx(droq, budget);\r\nif (work_done < budget) {\r\nnapi_complete(napi);\r\nocteon_process_droq_poll_cmd(droq->oct_dev, droq->q_no,\r\nPOLL_EVENT_ENABLE_INTR, 0);\r\nreturn 0;\r\n}\r\nreturn work_done;\r\n}\r\nstatic inline int setup_io_queues(struct octeon_device *octeon_dev,\r\nstruct net_device *net_device)\r\n{\r\nstatic int first_time = 1;\r\nstatic struct octeon_droq_ops droq_ops;\r\nstatic int cpu_id;\r\nstatic int cpu_id_modulus;\r\nstruct octeon_droq *droq;\r\nstruct napi_struct *napi;\r\nint q, q_no, retval = 0;\r\nstruct lio *lio;\r\nint num_tx_descs;\r\nlio = GET_LIO(net_device);\r\nif (first_time) {\r\nfirst_time = 0;\r\nmemset(&droq_ops, 0, sizeof(struct octeon_droq_ops));\r\ndroq_ops.fptr = liquidio_push_packet;\r\ndroq_ops.poll_mode = 1;\r\ndroq_ops.napi_fn = liquidio_napi_drv_callback;\r\ncpu_id = 0;\r\ncpu_id_modulus = num_present_cpus();\r\n}\r\nfor (q = 0; q < lio->linfo.num_rxpciq; q++) {\r\nq_no = lio->linfo.rxpciq[q];\r\nretval = octeon_setup_droq(octeon_dev, q_no,\r\nCFG_GET_NUM_RX_DESCS_NIC_IF\r\n(octeon_get_conf(octeon_dev),\r\nlio->ifidx),\r\nCFG_GET_NUM_RX_BUF_SIZE_NIC_IF\r\n(octeon_get_conf(octeon_dev),\r\nlio->ifidx), NULL);\r\nif (retval) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n" %s : Runtime DROQ(RxQ) creation failed.\n",\r\n__func__);\r\nreturn 1;\r\n}\r\ndroq = octeon_dev->droq[q_no];\r\nnapi = &droq->napi;\r\nnetif_napi_add(net_device, napi, liquidio_napi_poll, 64);\r\ndroq->cpu_id = cpu_id;\r\ncpu_id++;\r\nif (cpu_id >= cpu_id_modulus)\r\ncpu_id = 0;\r\nocteon_register_droq_ops(octeon_dev, q_no, &droq_ops);\r\n}\r\nfor (q = 0; q < lio->linfo.num_txpciq; q++) {\r\nnum_tx_descs = CFG_GET_NUM_TX_DESCS_NIC_IF(octeon_get_conf\r\n(octeon_dev),\r\nlio->ifidx);\r\nretval = octeon_setup_iq(octeon_dev, lio->linfo.txpciq[q],\r\nnum_tx_descs,\r\nnetdev_get_tx_queue(net_device, q));\r\nif (retval) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n" %s : Runtime IQ(TxQ) creation failed.\n",\r\n__func__);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void octnet_poll_check_txq_status(struct work_struct *work)\r\n{\r\nstruct cavium_wk *wk = (struct cavium_wk *)work;\r\nstruct lio *lio = (struct lio *)wk->ctxptr;\r\nif (!ifstate_check(lio, LIO_IFSTATE_RUNNING))\r\nreturn;\r\ncheck_txq_status(lio);\r\nqueue_delayed_work(lio->txq_status_wq.wq,\r\n&lio->txq_status_wq.wk.work, msecs_to_jiffies(1));\r\n}\r\nstatic inline void setup_tx_poll_fn(struct net_device *netdev)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nlio->txq_status_wq.wq = create_workqueue("txq-status");\r\nif (!lio->txq_status_wq.wq) {\r\ndev_err(&oct->pci_dev->dev, "unable to create cavium txq status wq\n");\r\nreturn;\r\n}\r\nINIT_DELAYED_WORK(&lio->txq_status_wq.wk.work,\r\noctnet_poll_check_txq_status);\r\nlio->txq_status_wq.wk.ctxptr = lio;\r\nqueue_delayed_work(lio->txq_status_wq.wq,\r\n&lio->txq_status_wq.wk.work, msecs_to_jiffies(1));\r\n}\r\nstatic int liquidio_open(struct net_device *netdev)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nstruct napi_struct *napi, *n;\r\nlist_for_each_entry_safe(napi, n, &netdev->napi_list, dev_list)\r\nnapi_enable(napi);\r\noct_ptp_open(netdev);\r\nifstate_set(lio, LIO_IFSTATE_RUNNING);\r\nsetup_tx_poll_fn(netdev);\r\nstart_txq(netdev);\r\nnetif_info(lio, ifup, lio->netdev, "Interface Open, ready for traffic\n");\r\ntry_module_get(THIS_MODULE);\r\nsend_rx_ctrl_cmd(lio, 1);\r\nlio->intf_open = 1;\r\ndev_info(&oct->pci_dev->dev, "%s interface is opened\n",\r\nnetdev->name);\r\nreturn 0;\r\n}\r\nstatic int liquidio_stop(struct net_device *netdev)\r\n{\r\nstruct napi_struct *napi, *n;\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nnetif_info(lio, ifdown, lio->netdev, "Stopping interface!\n");\r\nlio->intf_open = 0;\r\nlio->linfo.link.s.status = 0;\r\nnetif_carrier_off(netdev);\r\nsend_rx_ctrl_cmd(lio, 0);\r\ncancel_delayed_work_sync(&lio->txq_status_wq.wk.work);\r\nflush_workqueue(lio->txq_status_wq.wq);\r\ndestroy_workqueue(lio->txq_status_wq.wq);\r\nif (lio->ptp_clock) {\r\nptp_clock_unregister(lio->ptp_clock);\r\nlio->ptp_clock = NULL;\r\n}\r\nifstate_reset(lio, LIO_IFSTATE_RUNNING);\r\nset_bit(__LINK_STATE_START, &lio->netdev->state);\r\nlist_for_each_entry_safe(napi, n, &netdev->napi_list, dev_list)\r\nnapi_disable(napi);\r\ntxqs_stop(netdev);\r\ndev_info(&oct->pci_dev->dev, "%s interface is stopped\n", netdev->name);\r\nmodule_put(THIS_MODULE);\r\nreturn 0;\r\n}\r\nvoid liquidio_link_ctrl_cmd_completion(void *nctrl_ptr)\r\n{\r\nstruct octnic_ctrl_pkt *nctrl = (struct octnic_ctrl_pkt *)nctrl_ptr;\r\nstruct net_device *netdev = (struct net_device *)nctrl->netpndev;\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nswitch (nctrl->ncmd.s.cmd) {\r\ncase OCTNET_CMD_CHANGE_DEVFLAGS:\r\ncase OCTNET_CMD_SET_MULTI_LIST:\r\nbreak;\r\ncase OCTNET_CMD_CHANGE_MACADDR:\r\nnetif_info(lio, probe, lio->netdev, " MACAddr changed to 0x%llx\n",\r\nCVM_CAST64(nctrl->udd[0]));\r\ndev_info(&oct->pci_dev->dev, "%s MACAddr changed to 0x%llx\n",\r\nnetdev->name, CVM_CAST64(nctrl->udd[0]));\r\nmemcpy(netdev->dev_addr, ((u8 *)&nctrl->udd[0]) + 2, ETH_ALEN);\r\nbreak;\r\ncase OCTNET_CMD_CHANGE_MTU:\r\nnetif_info(lio, probe, lio->netdev, " MTU Changed from %d to %d\n",\r\nnetdev->mtu, nctrl->ncmd.s.param2);\r\ndev_info(&oct->pci_dev->dev, "%s MTU Changed from %d to %d\n",\r\nnetdev->name, netdev->mtu,\r\nnctrl->ncmd.s.param2);\r\nnetdev->mtu = nctrl->ncmd.s.param2;\r\nbreak;\r\ncase OCTNET_CMD_GPIO_ACCESS:\r\nnetif_info(lio, probe, lio->netdev, "LED Flashing visual identification\n");\r\nbreak;\r\ncase OCTNET_CMD_LRO_ENABLE:\r\ndev_info(&oct->pci_dev->dev, "%s LRO Enabled\n", netdev->name);\r\nbreak;\r\ncase OCTNET_CMD_LRO_DISABLE:\r\ndev_info(&oct->pci_dev->dev, "%s LRO Disabled\n",\r\nnetdev->name);\r\nbreak;\r\ncase OCTNET_CMD_VERBOSE_ENABLE:\r\ndev_info(&oct->pci_dev->dev, "%s LRO Enabled\n", netdev->name);\r\nbreak;\r\ncase OCTNET_CMD_VERBOSE_DISABLE:\r\ndev_info(&oct->pci_dev->dev, "%s LRO Disabled\n",\r\nnetdev->name);\r\nbreak;\r\ncase OCTNET_CMD_SET_SETTINGS:\r\ndev_info(&oct->pci_dev->dev, "%s settings changed\n",\r\nnetdev->name);\r\nbreak;\r\ndefault:\r\ndev_err(&oct->pci_dev->dev, "%s Unknown cmd %d\n", __func__,\r\nnctrl->ncmd.s.cmd);\r\n}\r\n}\r\nstatic inline enum octnet_ifflags get_new_flags(struct net_device *netdev)\r\n{\r\nenum octnet_ifflags f = OCTNET_IFFLAG_UNICAST;\r\nif (netdev->flags & IFF_PROMISC)\r\nf |= OCTNET_IFFLAG_PROMISC;\r\nif (netdev->flags & IFF_ALLMULTI)\r\nf |= OCTNET_IFFLAG_ALLMULTI;\r\nif (netdev->flags & IFF_MULTICAST) {\r\nf |= OCTNET_IFFLAG_MULTICAST;\r\nif (netdev_mc_count(netdev) > MAX_OCTEON_MULTICAST_ADDR)\r\nf |= OCTNET_IFFLAG_ALLMULTI;\r\n}\r\nif (netdev->flags & IFF_BROADCAST)\r\nf |= OCTNET_IFFLAG_BROADCAST;\r\nreturn f;\r\n}\r\nstatic void liquidio_set_mcast_list(struct net_device *netdev)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nstruct octnic_ctrl_pkt nctrl;\r\nstruct octnic_ctrl_params nparams;\r\nstruct netdev_hw_addr *ha;\r\nu64 *mc;\r\nint ret, i;\r\nint mc_count = min(netdev_mc_count(netdev), MAX_OCTEON_MULTICAST_ADDR);\r\nmemset(&nctrl, 0, sizeof(struct octnic_ctrl_pkt));\r\nnctrl.ncmd.u64 = 0;\r\nnctrl.ncmd.s.cmd = OCTNET_CMD_SET_MULTI_LIST;\r\nnctrl.ncmd.s.param1 = lio->linfo.ifidx;\r\nnctrl.ncmd.s.param2 = get_new_flags(netdev);\r\nnctrl.ncmd.s.param3 = mc_count;\r\nnctrl.ncmd.s.more = mc_count;\r\nnctrl.netpndev = (u64)netdev;\r\nnctrl.cb_fn = liquidio_link_ctrl_cmd_completion;\r\ni = 0;\r\nmc = &nctrl.udd[0];\r\nnetdev_for_each_mc_addr(ha, netdev) {\r\n*mc = 0;\r\nmemcpy(((u8 *)mc) + 2, ha->addr, ETH_ALEN);\r\nif (++mc > &nctrl.udd[mc_count])\r\nbreak;\r\n}\r\nnctrl.wait_time = 0;\r\nnparams.resp_order = OCTEON_RESP_NORESPONSE;\r\nret = octnet_send_nic_ctrl_pkt(lio->oct_dev, &nctrl, nparams);\r\nif (ret < 0) {\r\ndev_err(&oct->pci_dev->dev, "DEVFLAGS change failed in core (ret: 0x%x)\n",\r\nret);\r\n}\r\n}\r\nstatic int liquidio_set_mac(struct net_device *netdev, void *p)\r\n{\r\nint ret = 0;\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nstruct sockaddr *addr = (struct sockaddr *)p;\r\nstruct octnic_ctrl_pkt nctrl;\r\nstruct octnic_ctrl_params nparams;\r\nif ((!is_valid_ether_addr(addr->sa_data)) ||\r\n(ifstate_check(lio, LIO_IFSTATE_RUNNING)))\r\nreturn -EADDRNOTAVAIL;\r\nmemset(&nctrl, 0, sizeof(struct octnic_ctrl_pkt));\r\nnctrl.ncmd.u64 = 0;\r\nnctrl.ncmd.s.cmd = OCTNET_CMD_CHANGE_MACADDR;\r\nnctrl.ncmd.s.param1 = lio->linfo.ifidx;\r\nnctrl.ncmd.s.param2 = 0;\r\nnctrl.ncmd.s.more = 1;\r\nnctrl.netpndev = (u64)netdev;\r\nnctrl.cb_fn = liquidio_link_ctrl_cmd_completion;\r\nnctrl.wait_time = 100;\r\nnctrl.udd[0] = 0;\r\nmemcpy((u8 *)&nctrl.udd[0] + 2, addr->sa_data, ETH_ALEN);\r\nnparams.resp_order = OCTEON_RESP_ORDERED;\r\nret = octnet_send_nic_ctrl_pkt(lio->oct_dev, &nctrl, nparams);\r\nif (ret < 0) {\r\ndev_err(&oct->pci_dev->dev, "MAC Address change failed\n");\r\nreturn -ENOMEM;\r\n}\r\nmemcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);\r\nmemcpy(((u8 *)&lio->linfo.hw_addr) + 2, addr->sa_data, ETH_ALEN);\r\nreturn 0;\r\n}\r\nstatic struct net_device_stats *liquidio_get_stats(struct net_device *netdev)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct net_device_stats *stats = &netdev->stats;\r\nstruct octeon_device *oct;\r\nu64 pkts = 0, drop = 0, bytes = 0;\r\nstruct oct_droq_stats *oq_stats;\r\nstruct oct_iq_stats *iq_stats;\r\nint i, iq_no, oq_no;\r\noct = lio->oct_dev;\r\nfor (i = 0; i < lio->linfo.num_txpciq; i++) {\r\niq_no = lio->linfo.txpciq[i];\r\niq_stats = &oct->instr_queue[iq_no]->stats;\r\npkts += iq_stats->tx_done;\r\ndrop += iq_stats->tx_dropped;\r\nbytes += iq_stats->tx_tot_bytes;\r\n}\r\nstats->tx_packets = pkts;\r\nstats->tx_bytes = bytes;\r\nstats->tx_dropped = drop;\r\npkts = 0;\r\ndrop = 0;\r\nbytes = 0;\r\nfor (i = 0; i < lio->linfo.num_rxpciq; i++) {\r\noq_no = lio->linfo.rxpciq[i];\r\noq_stats = &oct->droq[oq_no]->stats;\r\npkts += oq_stats->rx_pkts_received;\r\ndrop += (oq_stats->rx_dropped +\r\noq_stats->dropped_nodispatch +\r\noq_stats->dropped_toomany +\r\noq_stats->dropped_nomem);\r\nbytes += oq_stats->rx_bytes_received;\r\n}\r\nstats->rx_bytes = bytes;\r\nstats->rx_packets = pkts;\r\nstats->rx_dropped = drop;\r\nreturn stats;\r\n}\r\nstatic int liquidio_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nstruct octnic_ctrl_pkt nctrl;\r\nstruct octnic_ctrl_params nparams;\r\nint max_frm_size = new_mtu + OCTNET_FRM_HEADER_SIZE;\r\nint ret = 0;\r\nif ((max_frm_size < OCTNET_MIN_FRM_SIZE) ||\r\n(max_frm_size > OCTNET_MAX_FRM_SIZE)) {\r\ndev_err(&oct->pci_dev->dev, "Invalid MTU: %d\n", new_mtu);\r\ndev_err(&oct->pci_dev->dev, "Valid range %d and %d\n",\r\n(OCTNET_MIN_FRM_SIZE - OCTNET_FRM_HEADER_SIZE),\r\n(OCTNET_MAX_FRM_SIZE - OCTNET_FRM_HEADER_SIZE));\r\nreturn -EINVAL;\r\n}\r\nmemset(&nctrl, 0, sizeof(struct octnic_ctrl_pkt));\r\nnctrl.ncmd.u64 = 0;\r\nnctrl.ncmd.s.cmd = OCTNET_CMD_CHANGE_MTU;\r\nnctrl.ncmd.s.param1 = lio->linfo.ifidx;\r\nnctrl.ncmd.s.param2 = new_mtu;\r\nnctrl.wait_time = 100;\r\nnctrl.netpndev = (u64)netdev;\r\nnctrl.cb_fn = liquidio_link_ctrl_cmd_completion;\r\nnparams.resp_order = OCTEON_RESP_ORDERED;\r\nret = octnet_send_nic_ctrl_pkt(lio->oct_dev, &nctrl, nparams);\r\nif (ret < 0) {\r\ndev_err(&oct->pci_dev->dev, "Failed to set MTU\n");\r\nreturn -1;\r\n}\r\nlio->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic int hwtstamp_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct hwtstamp_config conf;\r\nstruct lio *lio = GET_LIO(netdev);\r\nif (copy_from_user(&conf, ifr->ifr_data, sizeof(conf)))\r\nreturn -EFAULT;\r\nif (conf.flags)\r\nreturn -EINVAL;\r\nswitch (conf.tx_type) {\r\ncase HWTSTAMP_TX_ON:\r\ncase HWTSTAMP_TX_OFF:\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nswitch (conf.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\nbreak;\r\ncase HWTSTAMP_FILTER_ALL:\r\ncase HWTSTAMP_FILTER_SOME:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\r\nconf.rx_filter = HWTSTAMP_FILTER_ALL;\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nif (conf.rx_filter == HWTSTAMP_FILTER_ALL)\r\nifstate_set(lio, LIO_IFSTATE_RX_TIMESTAMP_ENABLED);\r\nelse\r\nifstate_reset(lio, LIO_IFSTATE_RX_TIMESTAMP_ENABLED);\r\nreturn copy_to_user(ifr->ifr_data, &conf, sizeof(conf)) ? -EFAULT : 0;\r\n}\r\nstatic int liquidio_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)\r\n{\r\nswitch (cmd) {\r\ncase SIOCSHWTSTAMP:\r\nreturn hwtstamp_ioctl(netdev, ifr, cmd);\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void handle_timestamp(struct octeon_device *oct,\r\nu32 status,\r\nvoid *buf)\r\n{\r\nstruct octnet_buf_free_info *finfo;\r\nstruct octeon_soft_command *sc;\r\nstruct oct_timestamp_resp *resp;\r\nstruct lio *lio;\r\nstruct sk_buff *skb = (struct sk_buff *)buf;\r\nfinfo = (struct octnet_buf_free_info *)skb->cb;\r\nlio = finfo->lio;\r\nsc = finfo->sc;\r\noct = lio->oct_dev;\r\nresp = (struct oct_timestamp_resp *)sc->virtrptr;\r\nif (status != OCTEON_REQUEST_DONE) {\r\ndev_err(&oct->pci_dev->dev, "Tx timestamp instruction failed. Status: %llx\n",\r\nCVM_CAST64(status));\r\nresp->timestamp = 0;\r\n}\r\nocteon_swap_8B_data(&resp->timestamp, 1);\r\nif (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) != 0)) {\r\nstruct skb_shared_hwtstamps ts;\r\nu64 ns = resp->timestamp;\r\nnetif_info(lio, tx_done, lio->netdev,\r\n"Got resulting SKBTX_HW_TSTAMP skb=%p ns=%016llu\n",\r\nskb, (unsigned long long)ns);\r\nts.hwtstamp = ns_to_ktime(ns + lio->ptp_adjust);\r\nskb_tstamp_tx(skb, &ts);\r\n}\r\nocteon_free_soft_command(oct, sc);\r\nrecv_buffer_free(skb);\r\n}\r\nstatic inline int send_nic_timestamp_pkt(struct octeon_device *oct,\r\nstruct octnic_data_pkt *ndata,\r\nstruct octnet_buf_free_info *finfo,\r\nint xmit_more)\r\n{\r\nint retval;\r\nstruct octeon_soft_command *sc;\r\nstruct octeon_instr_ih *ih;\r\nstruct octeon_instr_rdp *rdp;\r\nstruct lio *lio;\r\nint ring_doorbell;\r\nlio = finfo->lio;\r\nsc = octeon_alloc_soft_command_resp(oct, &ndata->cmd,\r\nsizeof(struct oct_timestamp_resp));\r\nfinfo->sc = sc;\r\nif (!sc) {\r\ndev_err(&oct->pci_dev->dev, "No memory for timestamped data packet\n");\r\nreturn IQ_SEND_FAILED;\r\n}\r\nif (ndata->reqtype == REQTYPE_NORESP_NET)\r\nndata->reqtype = REQTYPE_RESP_NET;\r\nelse if (ndata->reqtype == REQTYPE_NORESP_NET_SG)\r\nndata->reqtype = REQTYPE_RESP_NET_SG;\r\nsc->callback = handle_timestamp;\r\nsc->callback_arg = finfo->skb;\r\nsc->iq_no = ndata->q_no;\r\nih = (struct octeon_instr_ih *)&sc->cmd.ih;\r\nrdp = (struct octeon_instr_rdp *)&sc->cmd.rdp;\r\nring_doorbell = !xmit_more;\r\nretval = octeon_send_command(oct, sc->iq_no, ring_doorbell, &sc->cmd,\r\nsc, ih->dlengsz, ndata->reqtype);\r\nif (retval) {\r\ndev_err(&oct->pci_dev->dev, "timestamp data packet failed status: %x\n",\r\nretval);\r\nocteon_free_soft_command(oct, sc);\r\n} else {\r\nnetif_info(lio, tx_queued, lio->netdev, "Queued timestamp packet\n");\r\n}\r\nreturn retval;\r\n}\r\nstatic inline int is_ipv4(struct sk_buff *skb)\r\n{\r\nreturn (skb->protocol == htons(ETH_P_IP)) &&\r\n(ip_hdr(skb)->version == 4);\r\n}\r\nstatic inline int is_vlan(struct sk_buff *skb)\r\n{\r\nreturn skb->protocol == htons(ETH_P_8021Q);\r\n}\r\nstatic inline int is_ip_fragmented(struct sk_buff *skb)\r\n{\r\nreturn (ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET)) ? 1 : 0;\r\n}\r\nstatic inline int is_ipv6(struct sk_buff *skb)\r\n{\r\nreturn (skb->protocol == htons(ETH_P_IPV6)) &&\r\n(ipv6_hdr(skb)->version == 6);\r\n}\r\nstatic inline int is_with_extn_hdr(struct sk_buff *skb)\r\n{\r\nreturn (ipv6_hdr(skb)->nexthdr != IPPROTO_TCP) &&\r\n(ipv6_hdr(skb)->nexthdr != IPPROTO_UDP);\r\n}\r\nstatic inline int is_tcpudp(struct sk_buff *skb)\r\n{\r\nreturn (ip_hdr(skb)->protocol == IPPROTO_TCP) ||\r\n(ip_hdr(skb)->protocol == IPPROTO_UDP);\r\n}\r\nstatic inline u32 get_ipv4_5tuple_tag(struct sk_buff *skb)\r\n{\r\nu32 tag;\r\nstruct iphdr *iphdr = ip_hdr(skb);\r\ntag = crc32(0, &iphdr->protocol, 1);\r\ntag = crc32(tag, (u8 *)&iphdr->saddr, 8);\r\ntag = crc32(tag, skb_transport_header(skb), 4);\r\nreturn tag;\r\n}\r\nstatic inline u32 get_ipv6_5tuple_tag(struct sk_buff *skb)\r\n{\r\nu32 tag;\r\nstruct ipv6hdr *ipv6hdr = ipv6_hdr(skb);\r\ntag = crc32(0, &ipv6hdr->nexthdr, 1);\r\ntag = crc32(tag, (u8 *)&ipv6hdr->saddr, 32);\r\ntag = crc32(tag, skb_transport_header(skb), 4);\r\nreturn tag;\r\n}\r\nstatic int liquidio_xmit(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct lio *lio;\r\nstruct octnet_buf_free_info *finfo;\r\nunion octnic_cmd_setup cmdsetup;\r\nstruct octnic_data_pkt ndata;\r\nstruct octeon_device *oct;\r\nstruct oct_iq_stats *stats;\r\nint cpu = 0, status = 0;\r\nint q_idx = 0, iq_no = 0;\r\nint xmit_more;\r\nu32 tag = 0;\r\nlio = GET_LIO(netdev);\r\noct = lio->oct_dev;\r\nif (netif_is_multiqueue(netdev)) {\r\ncpu = skb->queue_mapping;\r\nq_idx = (cpu & (lio->linfo.num_txpciq - 1));\r\niq_no = lio->linfo.txpciq[q_idx];\r\n} else {\r\niq_no = lio->txq;\r\n}\r\nstats = &oct->instr_queue[iq_no]->stats;\r\nif (!(atomic_read(&lio->ifstate) & LIO_IFSTATE_RUNNING) ||\r\n(!lio->linfo.link.s.status) ||\r\n(skb->len <= 0)) {\r\nnetif_info(lio, tx_err, lio->netdev,\r\n"Transmit failed link_status : %d\n",\r\nlio->linfo.link.s.status);\r\ngoto lio_xmit_failed;\r\n}\r\nfinfo = (struct octnet_buf_free_info *)skb->cb;\r\nfinfo->lio = lio;\r\nfinfo->skb = skb;\r\nfinfo->sc = NULL;\r\nmemset(&ndata, 0, sizeof(struct octnic_data_pkt));\r\nndata.buf = (void *)finfo;\r\nndata.q_no = iq_no;\r\nif (netif_is_multiqueue(netdev)) {\r\nif (octnet_iq_is_full(oct, ndata.q_no)) {\r\nnetif_info(lio, tx_err, lio->netdev, "Transmit failed iq:%d full\n",\r\nndata.q_no);\r\nstats->tx_iq_busy++;\r\nreturn NETDEV_TX_BUSY;\r\n}\r\n} else {\r\nif (octnet_iq_is_full(oct, lio->txq)) {\r\nstats->tx_iq_busy++;\r\nnetif_info(lio, tx_err, lio->netdev, "Transmit failed iq:%d full\n",\r\nndata.q_no);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\n}\r\nndata.datasize = skb->len;\r\ncmdsetup.u64 = 0;\r\ncmdsetup.s.ifidx = lio->linfo.ifidx;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nif (is_ipv4(skb) && !is_ip_fragmented(skb) && is_tcpudp(skb)) {\r\ntag = get_ipv4_5tuple_tag(skb);\r\ncmdsetup.s.cksum_offset = sizeof(struct ethhdr) + 1;\r\nif (ip_hdr(skb)->ihl > 5)\r\ncmdsetup.s.ipv4opts_ipv6exthdr =\r\nOCT_PKT_PARAM_IPV4OPTS;\r\n} else if (is_ipv6(skb)) {\r\ntag = get_ipv6_5tuple_tag(skb);\r\ncmdsetup.s.cksum_offset = sizeof(struct ethhdr) + 1;\r\nif (is_with_extn_hdr(skb))\r\ncmdsetup.s.ipv4opts_ipv6exthdr =\r\nOCT_PKT_PARAM_IPV6EXTHDR;\r\n} else if (is_vlan(skb)) {\r\nif (vlan_eth_hdr(skb)->h_vlan_encapsulated_proto\r\n== htons(ETH_P_IP) &&\r\n!is_ip_fragmented(skb) && is_tcpudp(skb)) {\r\ntag = get_ipv4_5tuple_tag(skb);\r\ncmdsetup.s.cksum_offset =\r\nsizeof(struct vlan_ethhdr) + 1;\r\nif (ip_hdr(skb)->ihl > 5)\r\ncmdsetup.s.ipv4opts_ipv6exthdr =\r\nOCT_PKT_PARAM_IPV4OPTS;\r\n} else if (vlan_eth_hdr(skb)->h_vlan_encapsulated_proto\r\n== htons(ETH_P_IPV6)) {\r\ntag = get_ipv6_5tuple_tag(skb);\r\ncmdsetup.s.cksum_offset =\r\nsizeof(struct vlan_ethhdr) + 1;\r\nif (is_with_extn_hdr(skb))\r\ncmdsetup.s.ipv4opts_ipv6exthdr =\r\nOCT_PKT_PARAM_IPV6EXTHDR;\r\n}\r\n}\r\n}\r\nif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {\r\nskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\r\ncmdsetup.s.timestamp = 1;\r\n}\r\nif (skb_shinfo(skb)->nr_frags == 0) {\r\ncmdsetup.s.u.datasize = skb->len;\r\noctnet_prepare_pci_cmd(&ndata.cmd, &cmdsetup, tag);\r\nndata.cmd.dptr = dma_map_single(&oct->pci_dev->dev,\r\nskb->data,\r\nskb->len,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&oct->pci_dev->dev, ndata.cmd.dptr)) {\r\ndev_err(&oct->pci_dev->dev, "%s DMA mapping error 1\n",\r\n__func__);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nfinfo->dptr = ndata.cmd.dptr;\r\nndata.reqtype = REQTYPE_NORESP_NET;\r\n} else {\r\nint i, frags;\r\nstruct skb_frag_struct *frag;\r\nstruct octnic_gather *g;\r\nspin_lock(&lio->lock);\r\ng = (struct octnic_gather *)list_delete_head(&lio->glist);\r\nspin_unlock(&lio->lock);\r\nif (!g) {\r\nnetif_info(lio, tx_err, lio->netdev,\r\n"Transmit scatter gather: glist null!\n");\r\ngoto lio_xmit_failed;\r\n}\r\ncmdsetup.s.gather = 1;\r\ncmdsetup.s.u.gatherptrs = (skb_shinfo(skb)->nr_frags + 1);\r\noctnet_prepare_pci_cmd(&ndata.cmd, &cmdsetup, tag);\r\nmemset(g->sg, 0, g->sg_size);\r\ng->sg[0].ptr[0] = dma_map_single(&oct->pci_dev->dev,\r\nskb->data,\r\n(skb->len - skb->data_len),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&oct->pci_dev->dev, g->sg[0].ptr[0])) {\r\ndev_err(&oct->pci_dev->dev, "%s DMA mapping error 2\n",\r\n__func__);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nadd_sg_size(&g->sg[0], (skb->len - skb->data_len), 0);\r\nfrags = skb_shinfo(skb)->nr_frags;\r\ni = 1;\r\nwhile (frags--) {\r\nfrag = &skb_shinfo(skb)->frags[i - 1];\r\ng->sg[(i >> 2)].ptr[(i & 3)] =\r\ndma_map_page(&oct->pci_dev->dev,\r\nfrag->page.p,\r\nfrag->page_offset,\r\nfrag->size,\r\nDMA_TO_DEVICE);\r\nadd_sg_size(&g->sg[(i >> 2)], frag->size, (i & 3));\r\ni++;\r\n}\r\nndata.cmd.dptr = dma_map_single(&oct->pci_dev->dev,\r\ng->sg, g->sg_size,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&oct->pci_dev->dev, ndata.cmd.dptr)) {\r\ndev_err(&oct->pci_dev->dev, "%s DMA mapping error 3\n",\r\n__func__);\r\ndma_unmap_single(&oct->pci_dev->dev, g->sg[0].ptr[0],\r\nskb->len - skb->data_len,\r\nDMA_TO_DEVICE);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nfinfo->dptr = ndata.cmd.dptr;\r\nfinfo->g = g;\r\nndata.reqtype = REQTYPE_NORESP_NET_SG;\r\n}\r\nif (skb_shinfo(skb)->gso_size) {\r\nstruct octeon_instr_irh *irh =\r\n(struct octeon_instr_irh *)&ndata.cmd.irh;\r\nunion tx_info *tx_info = (union tx_info *)&ndata.cmd.ossp[0];\r\nirh->len = 1;\r\ntx_info->s.gso_size = skb_shinfo(skb)->gso_size;\r\ntx_info->s.gso_segs = skb_shinfo(skb)->gso_segs;\r\n}\r\nxmit_more = skb->xmit_more;\r\nif (unlikely(cmdsetup.s.timestamp))\r\nstatus = send_nic_timestamp_pkt(oct, &ndata, finfo, xmit_more);\r\nelse\r\nstatus = octnet_send_nic_data_pkt(oct, &ndata, xmit_more);\r\nif (status == IQ_SEND_FAILED)\r\ngoto lio_xmit_failed;\r\nnetif_info(lio, tx_queued, lio->netdev, "Transmit queued successfully\n");\r\nif (status == IQ_SEND_STOP)\r\nstop_q(lio->netdev, q_idx);\r\nnetdev->trans_start = jiffies;\r\nstats->tx_done++;\r\nstats->tx_tot_bytes += skb->len;\r\nreturn NETDEV_TX_OK;\r\nlio_xmit_failed:\r\nstats->tx_dropped++;\r\nnetif_info(lio, tx_err, lio->netdev, "IQ%d Transmit dropped:%llu\n",\r\niq_no, stats->tx_dropped);\r\ndma_unmap_single(&oct->pci_dev->dev, ndata.cmd.dptr,\r\nndata.datasize, DMA_TO_DEVICE);\r\nrecv_buffer_free(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void liquidio_tx_timeout(struct net_device *netdev)\r\n{\r\nstruct lio *lio;\r\nlio = GET_LIO(netdev);\r\nnetif_info(lio, tx_err, lio->netdev,\r\n"Transmit timeout tx_dropped:%ld, waking up queues now!!\n",\r\nnetdev->stats.tx_dropped);\r\nnetdev->trans_start = jiffies;\r\ntxqs_wake(netdev);\r\n}\r\nint liquidio_set_feature(struct net_device *netdev, int cmd)\r\n{\r\nstruct lio *lio = GET_LIO(netdev);\r\nstruct octeon_device *oct = lio->oct_dev;\r\nstruct octnic_ctrl_pkt nctrl;\r\nstruct octnic_ctrl_params nparams;\r\nint ret = 0;\r\nmemset(&nctrl, 0, sizeof(struct octnic_ctrl_pkt));\r\nnctrl.ncmd.u64 = 0;\r\nnctrl.ncmd.s.cmd = cmd;\r\nnctrl.ncmd.s.param1 = lio->linfo.ifidx;\r\nnctrl.ncmd.s.param2 = OCTNIC_LROIPV4 | OCTNIC_LROIPV6;\r\nnctrl.wait_time = 100;\r\nnctrl.netpndev = (u64)netdev;\r\nnctrl.cb_fn = liquidio_link_ctrl_cmd_completion;\r\nnparams.resp_order = OCTEON_RESP_NORESPONSE;\r\nret = octnet_send_nic_ctrl_pkt(lio->oct_dev, &nctrl, nparams);\r\nif (ret < 0) {\r\ndev_err(&oct->pci_dev->dev, "Feature change failed in core (ret: 0x%x)\n",\r\nret);\r\n}\r\nreturn ret;\r\n}\r\nstatic netdev_features_t liquidio_fix_features(struct net_device *netdev,\r\nnetdev_features_t request)\r\n{\r\nstruct lio *lio = netdev_priv(netdev);\r\nif ((request & NETIF_F_RXCSUM) &&\r\n!(lio->dev_capability & NETIF_F_RXCSUM))\r\nrequest &= ~NETIF_F_RXCSUM;\r\nif ((request & NETIF_F_HW_CSUM) &&\r\n!(lio->dev_capability & NETIF_F_HW_CSUM))\r\nrequest &= ~NETIF_F_HW_CSUM;\r\nif ((request & NETIF_F_TSO) && !(lio->dev_capability & NETIF_F_TSO))\r\nrequest &= ~NETIF_F_TSO;\r\nif ((request & NETIF_F_TSO6) && !(lio->dev_capability & NETIF_F_TSO6))\r\nrequest &= ~NETIF_F_TSO6;\r\nif ((request & NETIF_F_LRO) && !(lio->dev_capability & NETIF_F_LRO))\r\nrequest &= ~NETIF_F_LRO;\r\nif (!(request & NETIF_F_RXCSUM) && (netdev->features & NETIF_F_LRO) &&\r\n(lio->dev_capability & NETIF_F_LRO))\r\nrequest &= ~NETIF_F_LRO;\r\nreturn request;\r\n}\r\nstatic int liquidio_set_features(struct net_device *netdev,\r\nnetdev_features_t features)\r\n{\r\nstruct lio *lio = netdev_priv(netdev);\r\nif (!((netdev->features ^ features) & NETIF_F_LRO))\r\nreturn 0;\r\nif ((features & NETIF_F_LRO) && (lio->dev_capability & NETIF_F_LRO))\r\nliquidio_set_feature(netdev, OCTNET_CMD_LRO_ENABLE);\r\nelse if (!(features & NETIF_F_LRO) &&\r\n(lio->dev_capability & NETIF_F_LRO))\r\nliquidio_set_feature(netdev, OCTNET_CMD_LRO_DISABLE);\r\nreturn 0;\r\n}\r\nstatic int __init liquidio_init(void)\r\n{\r\nint i;\r\nstruct handshake *hs;\r\ninit_completion(&first_stage);\r\nocteon_init_device_list(conf_type);\r\nif (liquidio_init_pci())\r\nreturn -EINVAL;\r\nwait_for_completion_timeout(&first_stage, msecs_to_jiffies(1000));\r\nfor (i = 0; i < MAX_OCTEON_DEVICES; i++) {\r\nhs = &handshake[i];\r\nif (hs->pci_dev) {\r\nwait_for_completion(&hs->init);\r\nif (!hs->init_ok) {\r\ndev_err(&hs->pci_dev->dev,\r\n"Failed to init device\n");\r\nliquidio_deinit_pci();\r\nreturn -EIO;\r\n}\r\n}\r\n}\r\nfor (i = 0; i < MAX_OCTEON_DEVICES; i++) {\r\nhs = &handshake[i];\r\nif (hs->pci_dev) {\r\nwait_for_completion_timeout(&hs->started,\r\nmsecs_to_jiffies(30000));\r\nif (!hs->started_ok) {\r\ndev_err(&hs->pci_dev->dev,\r\n"Firmware failed to start\n");\r\nliquidio_deinit_pci();\r\nreturn -EIO;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int lio_nic_info(struct octeon_recv_info *recv_info, void *buf)\r\n{\r\nstruct octeon_device *oct = (struct octeon_device *)buf;\r\nstruct octeon_recv_pkt *recv_pkt = recv_info->recv_pkt;\r\nint ifidx = 0;\r\nunion oct_link_status *ls;\r\nint i;\r\nif ((recv_pkt->buffer_size[0] != sizeof(*ls)) ||\r\n(recv_pkt->rh.r_nic_info.ifidx > oct->ifcount)) {\r\ndev_err(&oct->pci_dev->dev, "Malformed NIC_INFO, len=%d, ifidx=%d\n",\r\nrecv_pkt->buffer_size[0],\r\nrecv_pkt->rh.r_nic_info.ifidx);\r\ngoto nic_info_err;\r\n}\r\nifidx = recv_pkt->rh.r_nic_info.ifidx;\r\nls = (union oct_link_status *)get_rbd(recv_pkt->buffer_ptr[0]);\r\nocteon_swap_8B_data((u64 *)ls, (sizeof(union oct_link_status)) >> 3);\r\nupdate_link_status(oct->props[ifidx].netdev, ls);\r\nnic_info_err:\r\nfor (i = 0; i < recv_pkt->buffer_count; i++)\r\nrecv_buffer_free(recv_pkt->buffer_ptr[i]);\r\nocteon_free_recv_info(recv_info);\r\nreturn 0;\r\n}\r\nstatic int setup_nic_devices(struct octeon_device *octeon_dev)\r\n{\r\nstruct lio *lio = NULL;\r\nstruct net_device *netdev;\r\nu8 mac[6], i, j;\r\nstruct octeon_soft_command *sc;\r\nstruct liquidio_if_cfg_context *ctx;\r\nstruct liquidio_if_cfg_resp *resp;\r\nstruct octdev_props *props;\r\nint retval, num_iqueues, num_oqueues, q_no;\r\nu64 q_mask;\r\nint num_cpus = num_online_cpus();\r\nunion oct_nic_if_cfg if_cfg;\r\nunsigned int base_queue;\r\nunsigned int gmx_port_id;\r\nu32 resp_size, ctx_size;\r\nocteon_register_dispatch_fn(octeon_dev, OPCODE_NIC,\r\nOPCODE_NIC_INFO,\r\nlio_nic_info, octeon_dev);\r\nocteon_register_reqtype_free_fn(octeon_dev, REQTYPE_NORESP_NET,\r\nfree_netbuf);\r\nocteon_register_reqtype_free_fn(octeon_dev, REQTYPE_NORESP_NET_SG,\r\nfree_netsgbuf);\r\nocteon_register_reqtype_free_fn(octeon_dev, REQTYPE_RESP_NET_SG,\r\nfree_netsgbuf_with_resp);\r\nfor (i = 0; i < octeon_dev->ifcount; i++) {\r\nresp_size = sizeof(struct liquidio_if_cfg_resp);\r\nctx_size = sizeof(struct liquidio_if_cfg_context);\r\nsc = (struct octeon_soft_command *)\r\nocteon_alloc_soft_command(octeon_dev, 0,\r\nresp_size, ctx_size);\r\nresp = (struct liquidio_if_cfg_resp *)sc->virtrptr;\r\nctx = (struct liquidio_if_cfg_context *)sc->ctxptr;\r\nnum_iqueues =\r\nCFG_GET_NUM_TXQS_NIC_IF(octeon_get_conf(octeon_dev), i);\r\nnum_oqueues =\r\nCFG_GET_NUM_RXQS_NIC_IF(octeon_get_conf(octeon_dev), i);\r\nbase_queue =\r\nCFG_GET_BASE_QUE_NIC_IF(octeon_get_conf(octeon_dev), i);\r\ngmx_port_id =\r\nCFG_GET_GMXID_NIC_IF(octeon_get_conf(octeon_dev), i);\r\nif (num_iqueues > num_cpus)\r\nnum_iqueues = num_cpus;\r\nif (num_oqueues > num_cpus)\r\nnum_oqueues = num_cpus;\r\ndev_dbg(&octeon_dev->pci_dev->dev,\r\n"requesting config for interface %d, iqs %d, oqs %d\n",\r\ni, num_iqueues, num_oqueues);\r\nACCESS_ONCE(ctx->cond) = 0;\r\nctx->octeon_id = lio_get_device_id(octeon_dev);\r\ninit_waitqueue_head(&ctx->wc);\r\nif_cfg.u64 = 0;\r\nif_cfg.s.num_iqueues = num_iqueues;\r\nif_cfg.s.num_oqueues = num_oqueues;\r\nif_cfg.s.base_queue = base_queue;\r\nif_cfg.s.gmx_port_id = gmx_port_id;\r\nocteon_prepare_soft_command(octeon_dev, sc, OPCODE_NIC,\r\nOPCODE_NIC_IF_CFG, i,\r\nif_cfg.u64, 0);\r\nsc->callback = if_cfg_callback;\r\nsc->callback_arg = sc;\r\nsc->wait_time = 1000;\r\nretval = octeon_send_soft_command(octeon_dev, sc);\r\nif (retval) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n"iq/oq config failed status: %x\n",\r\nretval);\r\ngoto setup_nic_dev_fail;\r\n}\r\nsleep_cond(&ctx->wc, &ctx->cond);\r\nretval = resp->status;\r\nif (retval) {\r\ndev_err(&octeon_dev->pci_dev->dev, "iq/oq config failed\n");\r\ngoto setup_nic_dev_fail;\r\n}\r\nocteon_swap_8B_data((u64 *)(&resp->cfg_info),\r\n(sizeof(struct liquidio_if_cfg_info)) >> 3);\r\nnum_iqueues = hweight64(resp->cfg_info.iqmask);\r\nnum_oqueues = hweight64(resp->cfg_info.oqmask);\r\nif (!(num_iqueues) || !(num_oqueues)) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n"Got bad iqueues (%016llx) or oqueues (%016llx) from firmware.\n",\r\nresp->cfg_info.iqmask,\r\nresp->cfg_info.oqmask);\r\ngoto setup_nic_dev_fail;\r\n}\r\ndev_dbg(&octeon_dev->pci_dev->dev,\r\n"interface %d, iqmask %016llx, oqmask %016llx, numiqueues %d, numoqueues %d\n",\r\ni, resp->cfg_info.iqmask, resp->cfg_info.oqmask,\r\nnum_iqueues, num_oqueues);\r\nnetdev = alloc_etherdev_mq(LIO_SIZE, num_iqueues);\r\nif (!netdev) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Device allocation failed\n");\r\ngoto setup_nic_dev_fail;\r\n}\r\nprops = &octeon_dev->props[i];\r\nprops->netdev = netdev;\r\nif (num_iqueues > 1)\r\nlionetdevops.ndo_select_queue = select_q;\r\nnetdev->netdev_ops = &lionetdevops;\r\nlio = GET_LIO(netdev);\r\nmemset(lio, 0, sizeof(struct lio));\r\nlio->linfo.ifidx = resp->cfg_info.ifidx;\r\nlio->ifidx = resp->cfg_info.ifidx;\r\nlio->linfo.num_rxpciq = num_oqueues;\r\nlio->linfo.num_txpciq = num_iqueues;\r\nq_mask = resp->cfg_info.oqmask;\r\nfor (j = 0; j < num_oqueues; j++) {\r\nq_no = __ffs64(q_mask);\r\nq_mask &= (~(1UL << q_no));\r\nlio->linfo.rxpciq[j] = q_no;\r\n}\r\nq_mask = resp->cfg_info.iqmask;\r\nfor (j = 0; j < num_iqueues; j++) {\r\nq_no = __ffs64(q_mask);\r\nq_mask &= (~(1UL << q_no));\r\nlio->linfo.txpciq[j] = q_no;\r\n}\r\nlio->linfo.hw_addr = resp->cfg_info.linfo.hw_addr;\r\nlio->linfo.gmxport = resp->cfg_info.linfo.gmxport;\r\nlio->linfo.link.u64 = resp->cfg_info.linfo.link.u64;\r\nlio->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\r\nlio->dev_capability = NETIF_F_HIGHDMA\r\n| NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM\r\n| NETIF_F_SG | NETIF_F_RXCSUM\r\n| NETIF_F_TSO | NETIF_F_TSO6\r\n| NETIF_F_LRO;\r\nnetif_set_gso_max_size(netdev, OCTNIC_GSO_MAX_SIZE);\r\nnetdev->features = lio->dev_capability;\r\nnetdev->vlan_features = lio->dev_capability;\r\nnetdev->hw_features = lio->dev_capability;\r\nlio->oct_dev = octeon_dev;\r\nlio->octprops = props;\r\nlio->netdev = netdev;\r\nspin_lock_init(&lio->lock);\r\ndev_dbg(&octeon_dev->pci_dev->dev,\r\n"if%d gmx: %d hw_addr: 0x%llx\n", i,\r\nlio->linfo.gmxport, CVM_CAST64(lio->linfo.hw_addr));\r\nocteon_swap_8B_data(&lio->linfo.hw_addr, 1);\r\nfor (j = 0; j < 6; j++)\r\nmac[j] = *((u8 *)(((u8 *)&lio->linfo.hw_addr) + 2 + j));\r\nether_addr_copy(netdev->dev_addr, mac);\r\nif (setup_io_queues(octeon_dev, netdev)) {\r\ndev_err(&octeon_dev->pci_dev->dev, "I/O queues creation failed\n");\r\ngoto setup_nic_dev_fail;\r\n}\r\nifstate_set(lio, LIO_IFSTATE_DROQ_OPS);\r\nlio->txq = lio->linfo.txpciq[0];\r\nlio->rxq = lio->linfo.rxpciq[0];\r\nlio->tx_qsize = octeon_get_tx_qsize(octeon_dev, lio->txq);\r\nlio->rx_qsize = octeon_get_rx_qsize(octeon_dev, lio->rxq);\r\nif (setup_glist(lio)) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n"Gather list allocation failed\n");\r\ngoto setup_nic_dev_fail;\r\n}\r\nliquidio_set_ethtool_ops(netdev);\r\nliquidio_set_feature(netdev, OCTNET_CMD_LRO_ENABLE);\r\nif ((debug != -1) && (debug & NETIF_MSG_HW))\r\nliquidio_set_feature(netdev, OCTNET_CMD_VERBOSE_ENABLE);\r\nif (register_netdev(netdev)) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Device registration failed\n");\r\ngoto setup_nic_dev_fail;\r\n}\r\ndev_dbg(&octeon_dev->pci_dev->dev,\r\n"Setup NIC ifidx:%d mac:%02x%02x%02x%02x%02x%02x\n",\r\ni, mac[0], mac[1], mac[2], mac[3], mac[4], mac[5]);\r\nnetif_carrier_off(netdev);\r\nif (lio->linfo.link.s.status) {\r\nnetif_carrier_on(netdev);\r\nstart_txq(netdev);\r\n} else {\r\nnetif_carrier_off(netdev);\r\n}\r\nifstate_set(lio, LIO_IFSTATE_REGISTERED);\r\ndev_dbg(&octeon_dev->pci_dev->dev,\r\n"NIC ifidx:%d Setup successful\n", i);\r\nocteon_free_soft_command(octeon_dev, sc);\r\n}\r\nreturn 0;\r\nsetup_nic_dev_fail:\r\nocteon_free_soft_command(octeon_dev, sc);\r\nwhile (i--) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n"NIC ifidx:%d Setup failed\n", i);\r\nliquidio_destroy_nic_device(octeon_dev, i);\r\n}\r\nreturn -ENODEV;\r\n}\r\nstatic int liquidio_init_nic_module(struct octeon_device *oct)\r\n{\r\nstruct oct_intrmod_cfg *intrmod_cfg;\r\nint retval = 0;\r\nint num_nic_ports = CFG_GET_NUM_NIC_PORTS(octeon_get_conf(oct));\r\ndev_dbg(&oct->pci_dev->dev, "Initializing network interfaces\n");\r\noct->ifcount = num_nic_ports;\r\nmemset(oct->props, 0,\r\nsizeof(struct octdev_props) * num_nic_ports);\r\nretval = setup_nic_devices(oct);\r\nif (retval) {\r\ndev_err(&oct->pci_dev->dev, "Setup NIC devices failed\n");\r\ngoto octnet_init_failure;\r\n}\r\nliquidio_ptp_init(oct);\r\nintrmod_cfg = &((struct octeon_device *)oct)->intrmod;\r\nintrmod_cfg->intrmod_enable = 1;\r\nintrmod_cfg->intrmod_check_intrvl = LIO_INTRMOD_CHECK_INTERVAL;\r\nintrmod_cfg->intrmod_maxpkt_ratethr = LIO_INTRMOD_MAXPKT_RATETHR;\r\nintrmod_cfg->intrmod_minpkt_ratethr = LIO_INTRMOD_MINPKT_RATETHR;\r\nintrmod_cfg->intrmod_maxcnt_trigger = LIO_INTRMOD_MAXCNT_TRIGGER;\r\nintrmod_cfg->intrmod_maxtmr_trigger = LIO_INTRMOD_MAXTMR_TRIGGER;\r\nintrmod_cfg->intrmod_mintmr_trigger = LIO_INTRMOD_MINTMR_TRIGGER;\r\nintrmod_cfg->intrmod_mincnt_trigger = LIO_INTRMOD_MINCNT_TRIGGER;\r\ndev_dbg(&oct->pci_dev->dev, "Network interfaces ready\n");\r\nreturn retval;\r\noctnet_init_failure:\r\noct->ifcount = 0;\r\nreturn retval;\r\n}\r\nstatic void nic_starter(struct work_struct *work)\r\n{\r\nstruct octeon_device *oct;\r\nstruct cavium_wk *wk = (struct cavium_wk *)work;\r\noct = (struct octeon_device *)wk->ctxptr;\r\nif (atomic_read(&oct->status) == OCT_DEV_RUNNING)\r\nreturn;\r\nif (atomic_read(&oct->status) != OCT_DEV_CORE_OK) {\r\nschedule_delayed_work(&oct->nic_poll_work.work,\r\nLIQUIDIO_STARTER_POLL_INTERVAL_MS);\r\nreturn;\r\n}\r\natomic_set(&oct->status, OCT_DEV_RUNNING);\r\nif (oct->app_mode && oct->app_mode == CVM_DRV_NIC_APP) {\r\ndev_dbg(&oct->pci_dev->dev, "Starting NIC module\n");\r\nif (liquidio_init_nic_module(oct))\r\ndev_err(&oct->pci_dev->dev, "NIC initialization failed\n");\r\nelse\r\nhandshake[oct->octeon_id].started_ok = 1;\r\n} else {\r\ndev_err(&oct->pci_dev->dev,\r\n"Unexpected application running on NIC (%d). Check firmware.\n",\r\noct->app_mode);\r\n}\r\ncomplete(&handshake[oct->octeon_id].started);\r\n}\r\nstatic int octeon_device_init(struct octeon_device *octeon_dev)\r\n{\r\nint j, ret;\r\nstruct octeon_device_priv *oct_priv =\r\n(struct octeon_device_priv *)octeon_dev->priv;\r\natomic_set(&octeon_dev->status, OCT_DEV_BEGIN_STATE);\r\nif (octeon_pci_os_setup(octeon_dev))\r\nreturn 1;\r\nif (octeon_chip_specific_setup(octeon_dev)) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Chip specific setup failed\n");\r\nreturn 1;\r\n}\r\natomic_set(&octeon_dev->status, OCT_DEV_PCI_MAP_DONE);\r\nocteon_dev->app_mode = CVM_DRV_INVALID_APP;\r\nif (octeon_dev->fn_list.soft_reset(octeon_dev))\r\nreturn 1;\r\nif (octeon_init_dispatch_list(octeon_dev))\r\nreturn 1;\r\nocteon_register_dispatch_fn(octeon_dev, OPCODE_NIC,\r\nOPCODE_NIC_CORE_DRV_ACTIVE,\r\nocteon_core_drv_init,\r\nocteon_dev);\r\nINIT_DELAYED_WORK(&octeon_dev->nic_poll_work.work, nic_starter);\r\nocteon_dev->nic_poll_work.ctxptr = (void *)octeon_dev;\r\nschedule_delayed_work(&octeon_dev->nic_poll_work.work,\r\nLIQUIDIO_STARTER_POLL_INTERVAL_MS);\r\natomic_set(&octeon_dev->status, OCT_DEV_DISPATCH_INIT_DONE);\r\nocteon_set_io_queues_off(octeon_dev);\r\nif (octeon_setup_instr_queues(octeon_dev)) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n"instruction queue initialization failed\n");\r\nfor (j = 0; j < octeon_dev->num_iqs; j++)\r\nocteon_delete_instr_queue(octeon_dev, j);\r\nreturn 1;\r\n}\r\natomic_set(&octeon_dev->status, OCT_DEV_INSTR_QUEUE_INIT_DONE);\r\nif (octeon_setup_sc_buffer_pool(octeon_dev)) {\r\ndev_err(&octeon_dev->pci_dev->dev, "sc buffer pool allocation failed\n");\r\nreturn 1;\r\n}\r\natomic_set(&octeon_dev->status, OCT_DEV_SC_BUFF_POOL_INIT_DONE);\r\nif (octeon_setup_response_list(octeon_dev)) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Response list allocation failed\n");\r\nreturn 1;\r\n}\r\natomic_set(&octeon_dev->status, OCT_DEV_RESP_LIST_INIT_DONE);\r\nif (octeon_setup_output_queues(octeon_dev)) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Output queue initialization failed\n");\r\nfor (j = 0; j < octeon_dev->num_oqs; j++)\r\nocteon_delete_droq(octeon_dev, j);\r\n}\r\natomic_set(&octeon_dev->status, OCT_DEV_DROQ_INIT_DONE);\r\nret = octeon_dev->fn_list.setup_device_regs(octeon_dev);\r\nif (ret) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n"Failed to configure device registers\n");\r\nreturn ret;\r\n}\r\ndev_dbg(&octeon_dev->pci_dev->dev, "Initializing droq tasklet\n");\r\ntasklet_init(&oct_priv->droq_tasklet, octeon_droq_bh,\r\n(unsigned long)octeon_dev);\r\nocteon_setup_interrupt(octeon_dev);\r\nocteon_dev->fn_list.enable_interrupt(octeon_dev->chip);\r\nocteon_dev->fn_list.enable_io_queues(octeon_dev);\r\natomic_set(&octeon_dev->status, OCT_DEV_IO_QUEUES_DONE);\r\ndev_dbg(&octeon_dev->pci_dev->dev, "Waiting for DDR initialization...\n");\r\nif (ddr_timeout == 0) {\r\ndev_info(&octeon_dev->pci_dev->dev,\r\n"WAITING. Set ddr_timeout to non-zero value to proceed with initialization.\n");\r\n}\r\nschedule_timeout_uninterruptible(HZ * LIO_RESET_SECS);\r\nret = octeon_wait_for_ddr_init(octeon_dev, &ddr_timeout);\r\nif (ret) {\r\ndev_err(&octeon_dev->pci_dev->dev,\r\n"DDR not initialized. Please confirm that board is configured to boot from Flash, ret: %d\n",\r\nret);\r\nreturn 1;\r\n}\r\nif (octeon_wait_for_bootloader(octeon_dev, 1000) != 0) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Board not responding\n");\r\nreturn 1;\r\n}\r\ndev_dbg(&octeon_dev->pci_dev->dev, "Initializing consoles\n");\r\nret = octeon_init_consoles(octeon_dev);\r\nif (ret) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Could not access board consoles\n");\r\nreturn 1;\r\n}\r\nret = octeon_add_console(octeon_dev, 0);\r\nif (ret) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Could not access board console\n");\r\nreturn 1;\r\n}\r\natomic_set(&octeon_dev->status, OCT_DEV_CONSOLE_INIT_DONE);\r\ndev_dbg(&octeon_dev->pci_dev->dev, "Loading firmware\n");\r\nret = load_firmware(octeon_dev);\r\nif (ret) {\r\ndev_err(&octeon_dev->pci_dev->dev, "Could not load firmware to board\n");\r\nreturn 1;\r\n}\r\nhandshake[octeon_dev->octeon_id].init_ok = 1;\r\ncomplete(&handshake[octeon_dev->octeon_id].init);\r\natomic_set(&octeon_dev->status, OCT_DEV_HOST_OK);\r\nfor (j = 0; j < octeon_dev->num_oqs; j++)\r\nwritel(octeon_dev->droq[j]->max_count,\r\nocteon_dev->droq[j]->pkts_credit_reg);\r\nreturn 0;\r\n}\r\nstatic void __exit liquidio_exit(void)\r\n{\r\nliquidio_deinit_pci();\r\npr_info("LiquidIO network module is now unloaded\n");\r\n}
