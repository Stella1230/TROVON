void radeon_gem_object_free(struct drm_gem_object *gobj)\r\n{\r\nstruct radeon_bo *robj = gem_to_radeon_bo(gobj);\r\nif (robj) {\r\nif (robj->gem_base.import_attach)\r\ndrm_prime_gem_destroy(&robj->gem_base, robj->tbo.sg);\r\nradeon_mn_unregister(robj);\r\nradeon_bo_unref(&robj);\r\n}\r\n}\r\nint radeon_gem_object_create(struct radeon_device *rdev, unsigned long size,\r\nint alignment, int initial_domain,\r\nu32 flags, bool kernel,\r\nstruct drm_gem_object **obj)\r\n{\r\nstruct radeon_bo *robj;\r\nunsigned long max_size;\r\nint r;\r\n*obj = NULL;\r\nif (alignment < PAGE_SIZE) {\r\nalignment = PAGE_SIZE;\r\n}\r\nmax_size = rdev->mc.gtt_size - rdev->gart_pin_size;\r\nif (size > max_size) {\r\nDRM_DEBUG("Allocation size %ldMb bigger than %ldMb limit\n",\r\nsize >> 20, max_size >> 20);\r\nreturn -ENOMEM;\r\n}\r\nretry:\r\nr = radeon_bo_create(rdev, size, alignment, kernel, initial_domain,\r\nflags, NULL, NULL, &robj);\r\nif (r) {\r\nif (r != -ERESTARTSYS) {\r\nif (initial_domain == RADEON_GEM_DOMAIN_VRAM) {\r\ninitial_domain |= RADEON_GEM_DOMAIN_GTT;\r\ngoto retry;\r\n}\r\nDRM_ERROR("Failed to allocate GEM object (%ld, %d, %u, %d)\n",\r\nsize, initial_domain, alignment, r);\r\n}\r\nreturn r;\r\n}\r\n*obj = &robj->gem_base;\r\nrobj->pid = task_pid_nr(current);\r\nmutex_lock(&rdev->gem.mutex);\r\nlist_add_tail(&robj->list, &rdev->gem.objects);\r\nmutex_unlock(&rdev->gem.mutex);\r\nreturn 0;\r\n}\r\nstatic int radeon_gem_set_domain(struct drm_gem_object *gobj,\r\nuint32_t rdomain, uint32_t wdomain)\r\n{\r\nstruct radeon_bo *robj;\r\nuint32_t domain;\r\nlong r;\r\nrobj = gem_to_radeon_bo(gobj);\r\ndomain = wdomain;\r\nif (!domain) {\r\ndomain = rdomain;\r\n}\r\nif (!domain) {\r\nprintk(KERN_WARNING "Set domain without domain !\n");\r\nreturn 0;\r\n}\r\nif (domain == RADEON_GEM_DOMAIN_CPU) {\r\nr = reservation_object_wait_timeout_rcu(robj->tbo.resv, true, true, 30 * HZ);\r\nif (!r)\r\nr = -EBUSY;\r\nif (r < 0 && r != -EINTR) {\r\nprintk(KERN_ERR "Failed to wait for object: %li\n", r);\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint radeon_gem_init(struct radeon_device *rdev)\r\n{\r\nINIT_LIST_HEAD(&rdev->gem.objects);\r\nreturn 0;\r\n}\r\nvoid radeon_gem_fini(struct radeon_device *rdev)\r\n{\r\nradeon_bo_force_delete(rdev);\r\n}\r\nint radeon_gem_object_open(struct drm_gem_object *obj, struct drm_file *file_priv)\r\n{\r\nstruct radeon_bo *rbo = gem_to_radeon_bo(obj);\r\nstruct radeon_device *rdev = rbo->rdev;\r\nstruct radeon_fpriv *fpriv = file_priv->driver_priv;\r\nstruct radeon_vm *vm = &fpriv->vm;\r\nstruct radeon_bo_va *bo_va;\r\nint r;\r\nif ((rdev->family < CHIP_CAYMAN) ||\r\n(!rdev->accel_working)) {\r\nreturn 0;\r\n}\r\nr = radeon_bo_reserve(rbo, false);\r\nif (r) {\r\nreturn r;\r\n}\r\nbo_va = radeon_vm_bo_find(vm, rbo);\r\nif (!bo_va) {\r\nbo_va = radeon_vm_bo_add(rdev, vm, rbo);\r\n} else {\r\n++bo_va->ref_count;\r\n}\r\nradeon_bo_unreserve(rbo);\r\nreturn 0;\r\n}\r\nvoid radeon_gem_object_close(struct drm_gem_object *obj,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct radeon_bo *rbo = gem_to_radeon_bo(obj);\r\nstruct radeon_device *rdev = rbo->rdev;\r\nstruct radeon_fpriv *fpriv = file_priv->driver_priv;\r\nstruct radeon_vm *vm = &fpriv->vm;\r\nstruct radeon_bo_va *bo_va;\r\nint r;\r\nif ((rdev->family < CHIP_CAYMAN) ||\r\n(!rdev->accel_working)) {\r\nreturn;\r\n}\r\nr = radeon_bo_reserve(rbo, true);\r\nif (r) {\r\ndev_err(rdev->dev, "leaking bo va because "\r\n"we fail to reserve bo (%d)\n", r);\r\nreturn;\r\n}\r\nbo_va = radeon_vm_bo_find(vm, rbo);\r\nif (bo_va) {\r\nif (--bo_va->ref_count == 0) {\r\nradeon_vm_bo_rmv(rdev, bo_va);\r\n}\r\n}\r\nradeon_bo_unreserve(rbo);\r\n}\r\nstatic int radeon_gem_handle_lockup(struct radeon_device *rdev, int r)\r\n{\r\nif (r == -EDEADLK) {\r\nr = radeon_gpu_reset(rdev);\r\nif (!r)\r\nr = -EAGAIN;\r\n}\r\nreturn r;\r\n}\r\nint radeon_gem_info_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct drm_radeon_gem_info *args = data;\r\nstruct ttm_mem_type_manager *man;\r\nman = &rdev->mman.bdev.man[TTM_PL_VRAM];\r\nargs->vram_size = rdev->mc.real_vram_size;\r\nargs->vram_visible = (u64)man->size << PAGE_SHIFT;\r\nargs->vram_visible -= rdev->vram_pin_size;\r\nargs->gart_size = rdev->mc.gtt_size;\r\nargs->gart_size -= rdev->gart_pin_size;\r\nreturn 0;\r\n}\r\nint radeon_gem_pread_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nDRM_ERROR("unimplemented %s\n", __func__);\r\nreturn -ENOSYS;\r\n}\r\nint radeon_gem_pwrite_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nDRM_ERROR("unimplemented %s\n", __func__);\r\nreturn -ENOSYS;\r\n}\r\nint radeon_gem_create_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct drm_radeon_gem_create *args = data;\r\nstruct drm_gem_object *gobj;\r\nuint32_t handle;\r\nint r;\r\ndown_read(&rdev->exclusive_lock);\r\nargs->size = roundup(args->size, PAGE_SIZE);\r\nr = radeon_gem_object_create(rdev, args->size, args->alignment,\r\nargs->initial_domain, args->flags,\r\nfalse, &gobj);\r\nif (r) {\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_gem_handle_lockup(rdev, r);\r\nreturn r;\r\n}\r\nr = drm_gem_handle_create(filp, gobj, &handle);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nif (r) {\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_gem_handle_lockup(rdev, r);\r\nreturn r;\r\n}\r\nargs->handle = handle;\r\nup_read(&rdev->exclusive_lock);\r\nreturn 0;\r\n}\r\nint radeon_gem_userptr_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct drm_radeon_gem_userptr *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *bo;\r\nuint32_t handle;\r\nint r;\r\nif (offset_in_page(args->addr | args->size))\r\nreturn -EINVAL;\r\nif (args->flags & ~(RADEON_GEM_USERPTR_READONLY |\r\nRADEON_GEM_USERPTR_ANONONLY | RADEON_GEM_USERPTR_VALIDATE |\r\nRADEON_GEM_USERPTR_REGISTER))\r\nreturn -EINVAL;\r\nif (args->flags & RADEON_GEM_USERPTR_READONLY) {\r\nif (rdev->family < CHIP_R600)\r\nreturn -EINVAL;\r\n} else if (!(args->flags & RADEON_GEM_USERPTR_ANONONLY) ||\r\n!(args->flags & RADEON_GEM_USERPTR_REGISTER)) {\r\nreturn -EACCES;\r\n}\r\ndown_read(&rdev->exclusive_lock);\r\nr = radeon_gem_object_create(rdev, args->size, 0,\r\nRADEON_GEM_DOMAIN_CPU, 0,\r\nfalse, &gobj);\r\nif (r)\r\ngoto handle_lockup;\r\nbo = gem_to_radeon_bo(gobj);\r\nr = radeon_ttm_tt_set_userptr(bo->tbo.ttm, args->addr, args->flags);\r\nif (r)\r\ngoto release_object;\r\nif (args->flags & RADEON_GEM_USERPTR_REGISTER) {\r\nr = radeon_mn_register(bo, args->addr);\r\nif (r)\r\ngoto release_object;\r\n}\r\nif (args->flags & RADEON_GEM_USERPTR_VALIDATE) {\r\ndown_read(&current->mm->mmap_sem);\r\nr = radeon_bo_reserve(bo, true);\r\nif (r) {\r\nup_read(&current->mm->mmap_sem);\r\ngoto release_object;\r\n}\r\nradeon_ttm_placement_from_domain(bo, RADEON_GEM_DOMAIN_GTT);\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);\r\nradeon_bo_unreserve(bo);\r\nup_read(&current->mm->mmap_sem);\r\nif (r)\r\ngoto release_object;\r\n}\r\nr = drm_gem_handle_create(filp, gobj, &handle);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nif (r)\r\ngoto handle_lockup;\r\nargs->handle = handle;\r\nup_read(&rdev->exclusive_lock);\r\nreturn 0;\r\nrelease_object:\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nhandle_lockup:\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_gem_handle_lockup(rdev, r);\r\nreturn r;\r\n}\r\nint radeon_gem_set_domain_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct drm_radeon_gem_set_domain *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *robj;\r\nint r;\r\ndown_read(&rdev->exclusive_lock);\r\ngobj = drm_gem_object_lookup(dev, filp, args->handle);\r\nif (gobj == NULL) {\r\nup_read(&rdev->exclusive_lock);\r\nreturn -ENOENT;\r\n}\r\nrobj = gem_to_radeon_bo(gobj);\r\nr = radeon_gem_set_domain(gobj, args->read_domains, args->write_domain);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_gem_handle_lockup(robj->rdev, r);\r\nreturn r;\r\n}\r\nint radeon_mode_dumb_mmap(struct drm_file *filp,\r\nstruct drm_device *dev,\r\nuint32_t handle, uint64_t *offset_p)\r\n{\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *robj;\r\ngobj = drm_gem_object_lookup(dev, filp, handle);\r\nif (gobj == NULL) {\r\nreturn -ENOENT;\r\n}\r\nrobj = gem_to_radeon_bo(gobj);\r\nif (radeon_ttm_tt_has_userptr(robj->tbo.ttm)) {\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn -EPERM;\r\n}\r\n*offset_p = radeon_bo_mmap_offset(robj);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn 0;\r\n}\r\nint radeon_gem_mmap_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct drm_radeon_gem_mmap *args = data;\r\nreturn radeon_mode_dumb_mmap(filp, dev, args->handle, &args->addr_ptr);\r\n}\r\nint radeon_gem_busy_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct drm_radeon_gem_busy *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *robj;\r\nint r;\r\nuint32_t cur_placement = 0;\r\ngobj = drm_gem_object_lookup(dev, filp, args->handle);\r\nif (gobj == NULL) {\r\nreturn -ENOENT;\r\n}\r\nrobj = gem_to_radeon_bo(gobj);\r\nr = reservation_object_test_signaled_rcu(robj->tbo.resv, true);\r\nif (r == 0)\r\nr = -EBUSY;\r\nelse\r\nr = 0;\r\ncur_placement = ACCESS_ONCE(robj->tbo.mem.mem_type);\r\nargs->domain = radeon_mem_type_to_domain(cur_placement);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn r;\r\n}\r\nint radeon_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct drm_radeon_gem_wait_idle *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *robj;\r\nint r = 0;\r\nuint32_t cur_placement = 0;\r\nlong ret;\r\ngobj = drm_gem_object_lookup(dev, filp, args->handle);\r\nif (gobj == NULL) {\r\nreturn -ENOENT;\r\n}\r\nrobj = gem_to_radeon_bo(gobj);\r\nret = reservation_object_wait_timeout_rcu(robj->tbo.resv, true, true, 30 * HZ);\r\nif (ret == 0)\r\nr = -EBUSY;\r\nelse if (ret < 0)\r\nr = ret;\r\ncur_placement = ACCESS_ONCE(robj->tbo.mem.mem_type);\r\nif (rdev->asic->mmio_hdp_flush &&\r\nradeon_mem_type_to_domain(cur_placement) == RADEON_GEM_DOMAIN_VRAM)\r\nrobj->rdev->asic->mmio_hdp_flush(rdev);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nr = radeon_gem_handle_lockup(rdev, r);\r\nreturn r;\r\n}\r\nint radeon_gem_set_tiling_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct drm_radeon_gem_set_tiling *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *robj;\r\nint r = 0;\r\nDRM_DEBUG("%d \n", args->handle);\r\ngobj = drm_gem_object_lookup(dev, filp, args->handle);\r\nif (gobj == NULL)\r\nreturn -ENOENT;\r\nrobj = gem_to_radeon_bo(gobj);\r\nr = radeon_bo_set_tiling_flags(robj, args->tiling_flags, args->pitch);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn r;\r\n}\r\nint radeon_gem_get_tiling_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct drm_radeon_gem_get_tiling *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *rbo;\r\nint r = 0;\r\nDRM_DEBUG("\n");\r\ngobj = drm_gem_object_lookup(dev, filp, args->handle);\r\nif (gobj == NULL)\r\nreturn -ENOENT;\r\nrbo = gem_to_radeon_bo(gobj);\r\nr = radeon_bo_reserve(rbo, false);\r\nif (unlikely(r != 0))\r\ngoto out;\r\nradeon_bo_get_tiling_flags(rbo, &args->tiling_flags, &args->pitch);\r\nradeon_bo_unreserve(rbo);\r\nout:\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn r;\r\n}\r\nstatic void radeon_gem_va_update_vm(struct radeon_device *rdev,\r\nstruct radeon_bo_va *bo_va)\r\n{\r\nstruct ttm_validate_buffer tv, *entry;\r\nstruct radeon_bo_list *vm_bos;\r\nstruct ww_acquire_ctx ticket;\r\nstruct list_head list;\r\nunsigned domain;\r\nint r;\r\nINIT_LIST_HEAD(&list);\r\ntv.bo = &bo_va->bo->tbo;\r\ntv.shared = true;\r\nlist_add(&tv.head, &list);\r\nvm_bos = radeon_vm_get_bos(rdev, bo_va->vm, &list);\r\nif (!vm_bos)\r\nreturn;\r\nr = ttm_eu_reserve_buffers(&ticket, &list, true, NULL);\r\nif (r)\r\ngoto error_free;\r\nlist_for_each_entry(entry, &list, head) {\r\ndomain = radeon_mem_type_to_domain(entry->bo->mem.mem_type);\r\nif (domain == RADEON_GEM_DOMAIN_CPU)\r\ngoto error_unreserve;\r\n}\r\nmutex_lock(&bo_va->vm->mutex);\r\nr = radeon_vm_clear_freed(rdev, bo_va->vm);\r\nif (r)\r\ngoto error_unlock;\r\nif (bo_va->it.start)\r\nr = radeon_vm_bo_update(rdev, bo_va, &bo_va->bo->tbo.mem);\r\nerror_unlock:\r\nmutex_unlock(&bo_va->vm->mutex);\r\nerror_unreserve:\r\nttm_eu_backoff_reservation(&ticket, &list);\r\nerror_free:\r\ndrm_free_large(vm_bos);\r\nif (r && r != -ERESTARTSYS)\r\nDRM_ERROR("Couldn't update BO_VA (%d)\n", r);\r\n}\r\nint radeon_gem_va_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct drm_radeon_gem_va *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct radeon_fpriv *fpriv = filp->driver_priv;\r\nstruct radeon_bo *rbo;\r\nstruct radeon_bo_va *bo_va;\r\nu32 invalid_flags;\r\nint r = 0;\r\nif (!rdev->vm_manager.enabled) {\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\nreturn -ENOTTY;\r\n}\r\nif (args->vm_id) {\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\nreturn -EINVAL;\r\n}\r\nif (args->offset < RADEON_VA_RESERVED_SIZE) {\r\ndev_err(&dev->pdev->dev,\r\n"offset 0x%lX is in reserved area 0x%X\n",\r\n(unsigned long)args->offset,\r\nRADEON_VA_RESERVED_SIZE);\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\nreturn -EINVAL;\r\n}\r\ninvalid_flags = RADEON_VM_PAGE_VALID | RADEON_VM_PAGE_SYSTEM;\r\nif ((args->flags & invalid_flags)) {\r\ndev_err(&dev->pdev->dev, "invalid flags 0x%08X vs 0x%08X\n",\r\nargs->flags, invalid_flags);\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\nreturn -EINVAL;\r\n}\r\nswitch (args->operation) {\r\ncase RADEON_VA_MAP:\r\ncase RADEON_VA_UNMAP:\r\nbreak;\r\ndefault:\r\ndev_err(&dev->pdev->dev, "unsupported operation %d\n",\r\nargs->operation);\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\nreturn -EINVAL;\r\n}\r\ngobj = drm_gem_object_lookup(dev, filp, args->handle);\r\nif (gobj == NULL) {\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\nreturn -ENOENT;\r\n}\r\nrbo = gem_to_radeon_bo(gobj);\r\nr = radeon_bo_reserve(rbo, false);\r\nif (r) {\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn r;\r\n}\r\nbo_va = radeon_vm_bo_find(&fpriv->vm, rbo);\r\nif (!bo_va) {\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\nradeon_bo_unreserve(rbo);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn -ENOENT;\r\n}\r\nswitch (args->operation) {\r\ncase RADEON_VA_MAP:\r\nif (bo_va->it.start) {\r\nargs->operation = RADEON_VA_RESULT_VA_EXIST;\r\nargs->offset = bo_va->it.start * RADEON_GPU_PAGE_SIZE;\r\nradeon_bo_unreserve(rbo);\r\ngoto out;\r\n}\r\nr = radeon_vm_bo_set_addr(rdev, bo_va, args->offset, args->flags);\r\nbreak;\r\ncase RADEON_VA_UNMAP:\r\nr = radeon_vm_bo_set_addr(rdev, bo_va, 0, 0);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (!r)\r\nradeon_gem_va_update_vm(rdev, bo_va);\r\nargs->operation = RADEON_VA_RESULT_OK;\r\nif (r) {\r\nargs->operation = RADEON_VA_RESULT_ERROR;\r\n}\r\nout:\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn r;\r\n}\r\nint radeon_gem_op_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *filp)\r\n{\r\nstruct drm_radeon_gem_op *args = data;\r\nstruct drm_gem_object *gobj;\r\nstruct radeon_bo *robj;\r\nint r;\r\ngobj = drm_gem_object_lookup(dev, filp, args->handle);\r\nif (gobj == NULL) {\r\nreturn -ENOENT;\r\n}\r\nrobj = gem_to_radeon_bo(gobj);\r\nr = -EPERM;\r\nif (radeon_ttm_tt_has_userptr(robj->tbo.ttm))\r\ngoto out;\r\nr = radeon_bo_reserve(robj, false);\r\nif (unlikely(r))\r\ngoto out;\r\nswitch (args->op) {\r\ncase RADEON_GEM_OP_GET_INITIAL_DOMAIN:\r\nargs->value = robj->initial_domain;\r\nbreak;\r\ncase RADEON_GEM_OP_SET_INITIAL_DOMAIN:\r\nrobj->initial_domain = args->value & (RADEON_GEM_DOMAIN_VRAM |\r\nRADEON_GEM_DOMAIN_GTT |\r\nRADEON_GEM_DOMAIN_CPU);\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\n}\r\nradeon_bo_unreserve(robj);\r\nout:\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nreturn r;\r\n}\r\nint radeon_mode_dumb_create(struct drm_file *file_priv,\r\nstruct drm_device *dev,\r\nstruct drm_mode_create_dumb *args)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct drm_gem_object *gobj;\r\nuint32_t handle;\r\nint r;\r\nargs->pitch = radeon_align_pitch(rdev, args->width, args->bpp, 0) * ((args->bpp + 1) / 8);\r\nargs->size = args->pitch * args->height;\r\nargs->size = ALIGN(args->size, PAGE_SIZE);\r\nr = radeon_gem_object_create(rdev, args->size, 0,\r\nRADEON_GEM_DOMAIN_VRAM, 0,\r\nfalse, &gobj);\r\nif (r)\r\nreturn -ENOMEM;\r\nr = drm_gem_handle_create(file_priv, gobj, &handle);\r\ndrm_gem_object_unreference_unlocked(gobj);\r\nif (r) {\r\nreturn r;\r\n}\r\nargs->handle = handle;\r\nreturn 0;\r\n}\r\nstatic int radeon_debugfs_gem_info(struct seq_file *m, void *data)\r\n{\r\nstruct drm_info_node *node = (struct drm_info_node *)m->private;\r\nstruct drm_device *dev = node->minor->dev;\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct radeon_bo *rbo;\r\nunsigned i = 0;\r\nmutex_lock(&rdev->gem.mutex);\r\nlist_for_each_entry(rbo, &rdev->gem.objects, list) {\r\nunsigned domain;\r\nconst char *placement;\r\ndomain = radeon_mem_type_to_domain(rbo->tbo.mem.mem_type);\r\nswitch (domain) {\r\ncase RADEON_GEM_DOMAIN_VRAM:\r\nplacement = "VRAM";\r\nbreak;\r\ncase RADEON_GEM_DOMAIN_GTT:\r\nplacement = " GTT";\r\nbreak;\r\ncase RADEON_GEM_DOMAIN_CPU:\r\ndefault:\r\nplacement = " CPU";\r\nbreak;\r\n}\r\nseq_printf(m, "bo[0x%08x] %8ldkB %8ldMB %s pid %8ld\n",\r\ni, radeon_bo_size(rbo) >> 10, radeon_bo_size(rbo) >> 20,\r\nplacement, (unsigned long)rbo->pid);\r\ni++;\r\n}\r\nmutex_unlock(&rdev->gem.mutex);\r\nreturn 0;\r\n}\r\nint radeon_gem_debugfs_init(struct radeon_device *rdev)\r\n{\r\n#if defined(CONFIG_DEBUG_FS)\r\nreturn radeon_debugfs_add_files(rdev, radeon_debugfs_gem_list, 1);\r\n#endif\r\nreturn 0;\r\n}
