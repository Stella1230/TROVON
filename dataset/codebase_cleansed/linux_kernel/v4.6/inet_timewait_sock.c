void inet_twsk_bind_unhash(struct inet_timewait_sock *tw,\r\nstruct inet_hashinfo *hashinfo)\r\n{\r\nstruct inet_bind_bucket *tb = tw->tw_tb;\r\nif (!tb)\r\nreturn;\r\n__hlist_del(&tw->tw_bind_node);\r\ntw->tw_tb = NULL;\r\ninet_bind_bucket_destroy(hashinfo->bind_bucket_cachep, tb);\r\n__sock_put((struct sock *)tw);\r\n}\r\nstatic void inet_twsk_kill(struct inet_timewait_sock *tw)\r\n{\r\nstruct inet_hashinfo *hashinfo = tw->tw_dr->hashinfo;\r\nspinlock_t *lock = inet_ehash_lockp(hashinfo, tw->tw_hash);\r\nstruct inet_bind_hashbucket *bhead;\r\nspin_lock(lock);\r\nsk_nulls_del_node_init_rcu((struct sock *)tw);\r\nspin_unlock(lock);\r\nbhead = &hashinfo->bhash[inet_bhashfn(twsk_net(tw), tw->tw_num,\r\nhashinfo->bhash_size)];\r\nspin_lock(&bhead->lock);\r\ninet_twsk_bind_unhash(tw, hashinfo);\r\nspin_unlock(&bhead->lock);\r\natomic_dec(&tw->tw_dr->tw_count);\r\ninet_twsk_put(tw);\r\n}\r\nvoid inet_twsk_free(struct inet_timewait_sock *tw)\r\n{\r\nstruct module *owner = tw->tw_prot->owner;\r\ntwsk_destructor((struct sock *)tw);\r\n#ifdef SOCK_REFCNT_DEBUG\r\npr_debug("%s timewait_sock %p released\n", tw->tw_prot->name, tw);\r\n#endif\r\nkmem_cache_free(tw->tw_prot->twsk_prot->twsk_slab, tw);\r\nmodule_put(owner);\r\n}\r\nvoid inet_twsk_put(struct inet_timewait_sock *tw)\r\n{\r\nif (atomic_dec_and_test(&tw->tw_refcnt))\r\ninet_twsk_free(tw);\r\n}\r\nstatic void inet_twsk_add_node_rcu(struct inet_timewait_sock *tw,\r\nstruct hlist_nulls_head *list)\r\n{\r\nhlist_nulls_add_head_rcu(&tw->tw_node, list);\r\n}\r\nstatic void inet_twsk_add_bind_node(struct inet_timewait_sock *tw,\r\nstruct hlist_head *list)\r\n{\r\nhlist_add_head(&tw->tw_bind_node, list);\r\n}\r\nvoid __inet_twsk_hashdance(struct inet_timewait_sock *tw, struct sock *sk,\r\nstruct inet_hashinfo *hashinfo)\r\n{\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct inet_ehash_bucket *ehead = inet_ehash_bucket(hashinfo, sk->sk_hash);\r\nspinlock_t *lock = inet_ehash_lockp(hashinfo, sk->sk_hash);\r\nstruct inet_bind_hashbucket *bhead;\r\nbhead = &hashinfo->bhash[inet_bhashfn(twsk_net(tw), inet->inet_num,\r\nhashinfo->bhash_size)];\r\nspin_lock(&bhead->lock);\r\ntw->tw_tb = icsk->icsk_bind_hash;\r\nWARN_ON(!icsk->icsk_bind_hash);\r\ninet_twsk_add_bind_node(tw, &tw->tw_tb->owners);\r\nspin_unlock(&bhead->lock);\r\nspin_lock(lock);\r\natomic_set(&tw->tw_refcnt, 4);\r\ninet_twsk_add_node_rcu(tw, &ehead->chain);\r\nif (__sk_nulls_del_node_init_rcu(sk))\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\r\nspin_unlock(lock);\r\n}\r\nstatic void tw_timer_handler(unsigned long data)\r\n{\r\nstruct inet_timewait_sock *tw = (struct inet_timewait_sock *)data;\r\nif (tw->tw_kill)\r\nNET_INC_STATS_BH(twsk_net(tw), LINUX_MIB_TIMEWAITKILLED);\r\nelse\r\nNET_INC_STATS_BH(twsk_net(tw), LINUX_MIB_TIMEWAITED);\r\ninet_twsk_kill(tw);\r\n}\r\nstruct inet_timewait_sock *inet_twsk_alloc(const struct sock *sk,\r\nstruct inet_timewait_death_row *dr,\r\nconst int state)\r\n{\r\nstruct inet_timewait_sock *tw;\r\nif (atomic_read(&dr->tw_count) >= dr->sysctl_max_tw_buckets)\r\nreturn NULL;\r\ntw = kmem_cache_alloc(sk->sk_prot_creator->twsk_prot->twsk_slab,\r\nGFP_ATOMIC);\r\nif (tw) {\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nkmemcheck_annotate_bitfield(tw, flags);\r\ntw->tw_dr = dr;\r\ntw->tw_daddr = inet->inet_daddr;\r\ntw->tw_rcv_saddr = inet->inet_rcv_saddr;\r\ntw->tw_bound_dev_if = sk->sk_bound_dev_if;\r\ntw->tw_tos = inet->tos;\r\ntw->tw_num = inet->inet_num;\r\ntw->tw_state = TCP_TIME_WAIT;\r\ntw->tw_substate = state;\r\ntw->tw_sport = inet->inet_sport;\r\ntw->tw_dport = inet->inet_dport;\r\ntw->tw_family = sk->sk_family;\r\ntw->tw_reuse = sk->sk_reuse;\r\ntw->tw_hash = sk->sk_hash;\r\ntw->tw_ipv6only = 0;\r\ntw->tw_transparent = inet->transparent;\r\ntw->tw_prot = sk->sk_prot_creator;\r\natomic64_set(&tw->tw_cookie, atomic64_read(&sk->sk_cookie));\r\ntwsk_net_set(tw, sock_net(sk));\r\nsetup_timer(&tw->tw_timer, tw_timer_handler, (unsigned long)tw);\r\natomic_set(&tw->tw_refcnt, 0);\r\n__module_get(tw->tw_prot->owner);\r\n}\r\nreturn tw;\r\n}\r\nvoid inet_twsk_deschedule_put(struct inet_timewait_sock *tw)\r\n{\r\nif (del_timer_sync(&tw->tw_timer))\r\ninet_twsk_kill(tw);\r\ninet_twsk_put(tw);\r\n}\r\nvoid __inet_twsk_schedule(struct inet_timewait_sock *tw, int timeo, bool rearm)\r\n{\r\ntw->tw_kill = timeo <= 4*HZ;\r\nif (!rearm) {\r\nBUG_ON(mod_timer_pinned(&tw->tw_timer, jiffies + timeo));\r\natomic_inc(&tw->tw_dr->tw_count);\r\n} else {\r\nmod_timer_pending(&tw->tw_timer, jiffies + timeo);\r\n}\r\n}\r\nvoid inet_twsk_purge(struct inet_hashinfo *hashinfo,\r\nstruct inet_timewait_death_row *twdr, int family)\r\n{\r\nstruct inet_timewait_sock *tw;\r\nstruct sock *sk;\r\nstruct hlist_nulls_node *node;\r\nunsigned int slot;\r\nfor (slot = 0; slot <= hashinfo->ehash_mask; slot++) {\r\nstruct inet_ehash_bucket *head = &hashinfo->ehash[slot];\r\nrestart_rcu:\r\ncond_resched();\r\nrcu_read_lock();\r\nrestart:\r\nsk_nulls_for_each_rcu(sk, node, &head->chain) {\r\nif (sk->sk_state != TCP_TIME_WAIT)\r\ncontinue;\r\ntw = inet_twsk(sk);\r\nif ((tw->tw_family != family) ||\r\natomic_read(&twsk_net(tw)->count))\r\ncontinue;\r\nif (unlikely(!atomic_inc_not_zero(&tw->tw_refcnt)))\r\ncontinue;\r\nif (unlikely((tw->tw_family != family) ||\r\natomic_read(&twsk_net(tw)->count))) {\r\ninet_twsk_put(tw);\r\ngoto restart;\r\n}\r\nrcu_read_unlock();\r\nlocal_bh_disable();\r\ninet_twsk_deschedule_put(tw);\r\nlocal_bh_enable();\r\ngoto restart_rcu;\r\n}\r\nif (get_nulls_value(node) != slot)\r\ngoto restart;\r\nrcu_read_unlock();\r\n}\r\n}
