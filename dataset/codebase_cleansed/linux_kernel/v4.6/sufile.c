static inline struct nilfs_sufile_info *NILFS_SUI(struct inode *sufile)\r\n{\r\nreturn (struct nilfs_sufile_info *)NILFS_MDT(sufile);\r\n}\r\nstatic inline unsigned long\r\nnilfs_sufile_segment_usages_per_block(const struct inode *sufile)\r\n{\r\nreturn NILFS_MDT(sufile)->mi_entries_per_block;\r\n}\r\nstatic unsigned long\r\nnilfs_sufile_get_blkoff(const struct inode *sufile, __u64 segnum)\r\n{\r\n__u64 t = segnum + NILFS_MDT(sufile)->mi_first_entry_offset;\r\ndo_div(t, nilfs_sufile_segment_usages_per_block(sufile));\r\nreturn (unsigned long)t;\r\n}\r\nstatic unsigned long\r\nnilfs_sufile_get_offset(const struct inode *sufile, __u64 segnum)\r\n{\r\n__u64 t = segnum + NILFS_MDT(sufile)->mi_first_entry_offset;\r\nreturn do_div(t, nilfs_sufile_segment_usages_per_block(sufile));\r\n}\r\nstatic unsigned long\r\nnilfs_sufile_segment_usages_in_block(const struct inode *sufile, __u64 curr,\r\n__u64 max)\r\n{\r\nreturn min_t(unsigned long,\r\nnilfs_sufile_segment_usages_per_block(sufile) -\r\nnilfs_sufile_get_offset(sufile, curr),\r\nmax - curr + 1);\r\n}\r\nstatic struct nilfs_segment_usage *\r\nnilfs_sufile_block_get_segment_usage(const struct inode *sufile, __u64 segnum,\r\nstruct buffer_head *bh, void *kaddr)\r\n{\r\nreturn kaddr + bh_offset(bh) +\r\nnilfs_sufile_get_offset(sufile, segnum) *\r\nNILFS_MDT(sufile)->mi_entry_size;\r\n}\r\nstatic inline int nilfs_sufile_get_header_block(struct inode *sufile,\r\nstruct buffer_head **bhp)\r\n{\r\nreturn nilfs_mdt_get_block(sufile, 0, 0, NULL, bhp);\r\n}\r\nstatic inline int\r\nnilfs_sufile_get_segment_usage_block(struct inode *sufile, __u64 segnum,\r\nint create, struct buffer_head **bhp)\r\n{\r\nreturn nilfs_mdt_get_block(sufile,\r\nnilfs_sufile_get_blkoff(sufile, segnum),\r\ncreate, NULL, bhp);\r\n}\r\nstatic int nilfs_sufile_delete_segment_usage_block(struct inode *sufile,\r\n__u64 segnum)\r\n{\r\nreturn nilfs_mdt_delete_block(sufile,\r\nnilfs_sufile_get_blkoff(sufile, segnum));\r\n}\r\nstatic void nilfs_sufile_mod_counter(struct buffer_head *header_bh,\r\nu64 ncleanadd, u64 ndirtyadd)\r\n{\r\nstruct nilfs_sufile_header *header;\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(header_bh->b_page);\r\nheader = kaddr + bh_offset(header_bh);\r\nle64_add_cpu(&header->sh_ncleansegs, ncleanadd);\r\nle64_add_cpu(&header->sh_ndirtysegs, ndirtyadd);\r\nkunmap_atomic(kaddr);\r\nmark_buffer_dirty(header_bh);\r\n}\r\nunsigned long nilfs_sufile_get_ncleansegs(struct inode *sufile)\r\n{\r\nreturn NILFS_SUI(sufile)->ncleansegs;\r\n}\r\nint nilfs_sufile_updatev(struct inode *sufile, __u64 *segnumv, size_t nsegs,\r\nint create, size_t *ndone,\r\nvoid (*dofunc)(struct inode *, __u64,\r\nstruct buffer_head *,\r\nstruct buffer_head *))\r\n{\r\nstruct buffer_head *header_bh, *bh;\r\nunsigned long blkoff, prev_blkoff;\r\n__u64 *seg;\r\nsize_t nerr = 0, n = 0;\r\nint ret = 0;\r\nif (unlikely(nsegs == 0))\r\ngoto out;\r\ndown_write(&NILFS_MDT(sufile)->mi_sem);\r\nfor (seg = segnumv; seg < segnumv + nsegs; seg++) {\r\nif (unlikely(*seg >= nilfs_sufile_get_nsegments(sufile))) {\r\nprintk(KERN_WARNING\r\n"%s: invalid segment number: %llu\n", __func__,\r\n(unsigned long long)*seg);\r\nnerr++;\r\n}\r\n}\r\nif (nerr > 0) {\r\nret = -EINVAL;\r\ngoto out_sem;\r\n}\r\nret = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (ret < 0)\r\ngoto out_sem;\r\nseg = segnumv;\r\nblkoff = nilfs_sufile_get_blkoff(sufile, *seg);\r\nret = nilfs_mdt_get_block(sufile, blkoff, create, NULL, &bh);\r\nif (ret < 0)\r\ngoto out_header;\r\nfor (;;) {\r\ndofunc(sufile, *seg, header_bh, bh);\r\nif (++seg >= segnumv + nsegs)\r\nbreak;\r\nprev_blkoff = blkoff;\r\nblkoff = nilfs_sufile_get_blkoff(sufile, *seg);\r\nif (blkoff == prev_blkoff)\r\ncontinue;\r\nbrelse(bh);\r\nret = nilfs_mdt_get_block(sufile, blkoff, create, NULL, &bh);\r\nif (unlikely(ret < 0))\r\ngoto out_header;\r\n}\r\nbrelse(bh);\r\nout_header:\r\nn = seg - segnumv;\r\nbrelse(header_bh);\r\nout_sem:\r\nup_write(&NILFS_MDT(sufile)->mi_sem);\r\nout:\r\nif (ndone)\r\n*ndone = n;\r\nreturn ret;\r\n}\r\nint nilfs_sufile_update(struct inode *sufile, __u64 segnum, int create,\r\nvoid (*dofunc)(struct inode *, __u64,\r\nstruct buffer_head *,\r\nstruct buffer_head *))\r\n{\r\nstruct buffer_head *header_bh, *bh;\r\nint ret;\r\nif (unlikely(segnum >= nilfs_sufile_get_nsegments(sufile))) {\r\nprintk(KERN_WARNING "%s: invalid segment number: %llu\n",\r\n__func__, (unsigned long long)segnum);\r\nreturn -EINVAL;\r\n}\r\ndown_write(&NILFS_MDT(sufile)->mi_sem);\r\nret = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (ret < 0)\r\ngoto out_sem;\r\nret = nilfs_sufile_get_segment_usage_block(sufile, segnum, create, &bh);\r\nif (!ret) {\r\ndofunc(sufile, segnum, header_bh, bh);\r\nbrelse(bh);\r\n}\r\nbrelse(header_bh);\r\nout_sem:\r\nup_write(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nint nilfs_sufile_set_alloc_range(struct inode *sufile, __u64 start, __u64 end)\r\n{\r\nstruct nilfs_sufile_info *sui = NILFS_SUI(sufile);\r\n__u64 nsegs;\r\nint ret = -ERANGE;\r\ndown_write(&NILFS_MDT(sufile)->mi_sem);\r\nnsegs = nilfs_sufile_get_nsegments(sufile);\r\nif (start <= end && end < nsegs) {\r\nsui->allocmin = start;\r\nsui->allocmax = end;\r\nret = 0;\r\n}\r\nup_write(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nint nilfs_sufile_alloc(struct inode *sufile, __u64 *segnump)\r\n{\r\nstruct buffer_head *header_bh, *su_bh;\r\nstruct nilfs_sufile_header *header;\r\nstruct nilfs_segment_usage *su;\r\nstruct nilfs_sufile_info *sui = NILFS_SUI(sufile);\r\nsize_t susz = NILFS_MDT(sufile)->mi_entry_size;\r\n__u64 segnum, maxsegnum, last_alloc;\r\nvoid *kaddr;\r\nunsigned long nsegments, nsus, cnt;\r\nint ret, j;\r\ndown_write(&NILFS_MDT(sufile)->mi_sem);\r\nret = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (ret < 0)\r\ngoto out_sem;\r\nkaddr = kmap_atomic(header_bh->b_page);\r\nheader = kaddr + bh_offset(header_bh);\r\nlast_alloc = le64_to_cpu(header->sh_last_alloc);\r\nkunmap_atomic(kaddr);\r\nnsegments = nilfs_sufile_get_nsegments(sufile);\r\nmaxsegnum = sui->allocmax;\r\nsegnum = last_alloc + 1;\r\nif (segnum < sui->allocmin || segnum > sui->allocmax)\r\nsegnum = sui->allocmin;\r\nfor (cnt = 0; cnt < nsegments; cnt += nsus) {\r\nif (segnum > maxsegnum) {\r\nif (cnt < sui->allocmax - sui->allocmin + 1) {\r\nsegnum = sui->allocmin;\r\nmaxsegnum = last_alloc;\r\n} else if (segnum > sui->allocmin &&\r\nsui->allocmax + 1 < nsegments) {\r\nsegnum = sui->allocmax + 1;\r\nmaxsegnum = nsegments - 1;\r\n} else if (sui->allocmin > 0) {\r\nsegnum = 0;\r\nmaxsegnum = sui->allocmin - 1;\r\n} else {\r\nbreak;\r\n}\r\n}\r\ntrace_nilfs2_segment_usage_check(sufile, segnum, cnt);\r\nret = nilfs_sufile_get_segment_usage_block(sufile, segnum, 1,\r\n&su_bh);\r\nif (ret < 0)\r\ngoto out_header;\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(\r\nsufile, segnum, su_bh, kaddr);\r\nnsus = nilfs_sufile_segment_usages_in_block(\r\nsufile, segnum, maxsegnum);\r\nfor (j = 0; j < nsus; j++, su = (void *)su + susz, segnum++) {\r\nif (!nilfs_segment_usage_clean(su))\r\ncontinue;\r\nnilfs_segment_usage_set_dirty(su);\r\nkunmap_atomic(kaddr);\r\nkaddr = kmap_atomic(header_bh->b_page);\r\nheader = kaddr + bh_offset(header_bh);\r\nle64_add_cpu(&header->sh_ncleansegs, -1);\r\nle64_add_cpu(&header->sh_ndirtysegs, 1);\r\nheader->sh_last_alloc = cpu_to_le64(segnum);\r\nkunmap_atomic(kaddr);\r\nsui->ncleansegs--;\r\nmark_buffer_dirty(header_bh);\r\nmark_buffer_dirty(su_bh);\r\nnilfs_mdt_mark_dirty(sufile);\r\nbrelse(su_bh);\r\n*segnump = segnum;\r\ntrace_nilfs2_segment_usage_allocated(sufile, segnum);\r\ngoto out_header;\r\n}\r\nkunmap_atomic(kaddr);\r\nbrelse(su_bh);\r\n}\r\nret = -ENOSPC;\r\nout_header:\r\nbrelse(header_bh);\r\nout_sem:\r\nup_write(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nvoid nilfs_sufile_do_cancel_free(struct inode *sufile, __u64 segnum,\r\nstruct buffer_head *header_bh,\r\nstruct buffer_head *su_bh)\r\n{\r\nstruct nilfs_segment_usage *su;\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(sufile, segnum, su_bh, kaddr);\r\nif (unlikely(!nilfs_segment_usage_clean(su))) {\r\nprintk(KERN_WARNING "%s: segment %llu must be clean\n",\r\n__func__, (unsigned long long)segnum);\r\nkunmap_atomic(kaddr);\r\nreturn;\r\n}\r\nnilfs_segment_usage_set_dirty(su);\r\nkunmap_atomic(kaddr);\r\nnilfs_sufile_mod_counter(header_bh, -1, 1);\r\nNILFS_SUI(sufile)->ncleansegs--;\r\nmark_buffer_dirty(su_bh);\r\nnilfs_mdt_mark_dirty(sufile);\r\n}\r\nvoid nilfs_sufile_do_scrap(struct inode *sufile, __u64 segnum,\r\nstruct buffer_head *header_bh,\r\nstruct buffer_head *su_bh)\r\n{\r\nstruct nilfs_segment_usage *su;\r\nvoid *kaddr;\r\nint clean, dirty;\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(sufile, segnum, su_bh, kaddr);\r\nif (su->su_flags == cpu_to_le32(1UL << NILFS_SEGMENT_USAGE_DIRTY) &&\r\nsu->su_nblocks == cpu_to_le32(0)) {\r\nkunmap_atomic(kaddr);\r\nreturn;\r\n}\r\nclean = nilfs_segment_usage_clean(su);\r\ndirty = nilfs_segment_usage_dirty(su);\r\nsu->su_lastmod = cpu_to_le64(0);\r\nsu->su_nblocks = cpu_to_le32(0);\r\nsu->su_flags = cpu_to_le32(1UL << NILFS_SEGMENT_USAGE_DIRTY);\r\nkunmap_atomic(kaddr);\r\nnilfs_sufile_mod_counter(header_bh, clean ? (u64)-1 : 0, dirty ? 0 : 1);\r\nNILFS_SUI(sufile)->ncleansegs -= clean;\r\nmark_buffer_dirty(su_bh);\r\nnilfs_mdt_mark_dirty(sufile);\r\n}\r\nvoid nilfs_sufile_do_free(struct inode *sufile, __u64 segnum,\r\nstruct buffer_head *header_bh,\r\nstruct buffer_head *su_bh)\r\n{\r\nstruct nilfs_segment_usage *su;\r\nvoid *kaddr;\r\nint sudirty;\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(sufile, segnum, su_bh, kaddr);\r\nif (nilfs_segment_usage_clean(su)) {\r\nprintk(KERN_WARNING "%s: segment %llu is already clean\n",\r\n__func__, (unsigned long long)segnum);\r\nkunmap_atomic(kaddr);\r\nreturn;\r\n}\r\nWARN_ON(nilfs_segment_usage_error(su));\r\nWARN_ON(!nilfs_segment_usage_dirty(su));\r\nsudirty = nilfs_segment_usage_dirty(su);\r\nnilfs_segment_usage_set_clean(su);\r\nkunmap_atomic(kaddr);\r\nmark_buffer_dirty(su_bh);\r\nnilfs_sufile_mod_counter(header_bh, 1, sudirty ? (u64)-1 : 0);\r\nNILFS_SUI(sufile)->ncleansegs++;\r\nnilfs_mdt_mark_dirty(sufile);\r\ntrace_nilfs2_segment_usage_freed(sufile, segnum);\r\n}\r\nint nilfs_sufile_mark_dirty(struct inode *sufile, __u64 segnum)\r\n{\r\nstruct buffer_head *bh;\r\nint ret;\r\nret = nilfs_sufile_get_segment_usage_block(sufile, segnum, 0, &bh);\r\nif (!ret) {\r\nmark_buffer_dirty(bh);\r\nnilfs_mdt_mark_dirty(sufile);\r\nbrelse(bh);\r\n}\r\nreturn ret;\r\n}\r\nint nilfs_sufile_set_segment_usage(struct inode *sufile, __u64 segnum,\r\nunsigned long nblocks, time_t modtime)\r\n{\r\nstruct buffer_head *bh;\r\nstruct nilfs_segment_usage *su;\r\nvoid *kaddr;\r\nint ret;\r\ndown_write(&NILFS_MDT(sufile)->mi_sem);\r\nret = nilfs_sufile_get_segment_usage_block(sufile, segnum, 0, &bh);\r\nif (ret < 0)\r\ngoto out_sem;\r\nkaddr = kmap_atomic(bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(sufile, segnum, bh, kaddr);\r\nWARN_ON(nilfs_segment_usage_error(su));\r\nif (modtime)\r\nsu->su_lastmod = cpu_to_le64(modtime);\r\nsu->su_nblocks = cpu_to_le32(nblocks);\r\nkunmap_atomic(kaddr);\r\nmark_buffer_dirty(bh);\r\nnilfs_mdt_mark_dirty(sufile);\r\nbrelse(bh);\r\nout_sem:\r\nup_write(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nint nilfs_sufile_get_stat(struct inode *sufile, struct nilfs_sustat *sustat)\r\n{\r\nstruct buffer_head *header_bh;\r\nstruct nilfs_sufile_header *header;\r\nstruct the_nilfs *nilfs = sufile->i_sb->s_fs_info;\r\nvoid *kaddr;\r\nint ret;\r\ndown_read(&NILFS_MDT(sufile)->mi_sem);\r\nret = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (ret < 0)\r\ngoto out_sem;\r\nkaddr = kmap_atomic(header_bh->b_page);\r\nheader = kaddr + bh_offset(header_bh);\r\nsustat->ss_nsegs = nilfs_sufile_get_nsegments(sufile);\r\nsustat->ss_ncleansegs = le64_to_cpu(header->sh_ncleansegs);\r\nsustat->ss_ndirtysegs = le64_to_cpu(header->sh_ndirtysegs);\r\nsustat->ss_ctime = nilfs->ns_ctime;\r\nsustat->ss_nongc_ctime = nilfs->ns_nongc_ctime;\r\nspin_lock(&nilfs->ns_last_segment_lock);\r\nsustat->ss_prot_seq = nilfs->ns_prot_seq;\r\nspin_unlock(&nilfs->ns_last_segment_lock);\r\nkunmap_atomic(kaddr);\r\nbrelse(header_bh);\r\nout_sem:\r\nup_read(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nvoid nilfs_sufile_do_set_error(struct inode *sufile, __u64 segnum,\r\nstruct buffer_head *header_bh,\r\nstruct buffer_head *su_bh)\r\n{\r\nstruct nilfs_segment_usage *su;\r\nvoid *kaddr;\r\nint suclean;\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(sufile, segnum, su_bh, kaddr);\r\nif (nilfs_segment_usage_error(su)) {\r\nkunmap_atomic(kaddr);\r\nreturn;\r\n}\r\nsuclean = nilfs_segment_usage_clean(su);\r\nnilfs_segment_usage_set_error(su);\r\nkunmap_atomic(kaddr);\r\nif (suclean) {\r\nnilfs_sufile_mod_counter(header_bh, -1, 0);\r\nNILFS_SUI(sufile)->ncleansegs--;\r\n}\r\nmark_buffer_dirty(su_bh);\r\nnilfs_mdt_mark_dirty(sufile);\r\n}\r\nstatic int nilfs_sufile_truncate_range(struct inode *sufile,\r\n__u64 start, __u64 end)\r\n{\r\nstruct the_nilfs *nilfs = sufile->i_sb->s_fs_info;\r\nstruct buffer_head *header_bh;\r\nstruct buffer_head *su_bh;\r\nstruct nilfs_segment_usage *su, *su2;\r\nsize_t susz = NILFS_MDT(sufile)->mi_entry_size;\r\nunsigned long segusages_per_block;\r\nunsigned long nsegs, ncleaned;\r\n__u64 segnum;\r\nvoid *kaddr;\r\nssize_t n, nc;\r\nint ret;\r\nint j;\r\nnsegs = nilfs_sufile_get_nsegments(sufile);\r\nret = -EINVAL;\r\nif (start > end || start >= nsegs)\r\ngoto out;\r\nret = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (ret < 0)\r\ngoto out;\r\nsegusages_per_block = nilfs_sufile_segment_usages_per_block(sufile);\r\nncleaned = 0;\r\nfor (segnum = start; segnum <= end; segnum += n) {\r\nn = min_t(unsigned long,\r\nsegusages_per_block -\r\nnilfs_sufile_get_offset(sufile, segnum),\r\nend - segnum + 1);\r\nret = nilfs_sufile_get_segment_usage_block(sufile, segnum, 0,\r\n&su_bh);\r\nif (ret < 0) {\r\nif (ret != -ENOENT)\r\ngoto out_header;\r\ncontinue;\r\n}\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(\r\nsufile, segnum, su_bh, kaddr);\r\nsu2 = su;\r\nfor (j = 0; j < n; j++, su = (void *)su + susz) {\r\nif ((le32_to_cpu(su->su_flags) &\r\n~(1UL << NILFS_SEGMENT_USAGE_ERROR)) ||\r\nnilfs_segment_is_active(nilfs, segnum + j)) {\r\nret = -EBUSY;\r\nkunmap_atomic(kaddr);\r\nbrelse(su_bh);\r\ngoto out_header;\r\n}\r\n}\r\nnc = 0;\r\nfor (su = su2, j = 0; j < n; j++, su = (void *)su + susz) {\r\nif (nilfs_segment_usage_error(su)) {\r\nnilfs_segment_usage_set_clean(su);\r\nnc++;\r\n}\r\n}\r\nkunmap_atomic(kaddr);\r\nif (nc > 0) {\r\nmark_buffer_dirty(su_bh);\r\nncleaned += nc;\r\n}\r\nbrelse(su_bh);\r\nif (n == segusages_per_block) {\r\nnilfs_sufile_delete_segment_usage_block(sufile, segnum);\r\n}\r\n}\r\nret = 0;\r\nout_header:\r\nif (ncleaned > 0) {\r\nNILFS_SUI(sufile)->ncleansegs += ncleaned;\r\nnilfs_sufile_mod_counter(header_bh, ncleaned, 0);\r\nnilfs_mdt_mark_dirty(sufile);\r\n}\r\nbrelse(header_bh);\r\nout:\r\nreturn ret;\r\n}\r\nint nilfs_sufile_resize(struct inode *sufile, __u64 newnsegs)\r\n{\r\nstruct the_nilfs *nilfs = sufile->i_sb->s_fs_info;\r\nstruct buffer_head *header_bh;\r\nstruct nilfs_sufile_header *header;\r\nstruct nilfs_sufile_info *sui = NILFS_SUI(sufile);\r\nvoid *kaddr;\r\nunsigned long nsegs, nrsvsegs;\r\nint ret = 0;\r\ndown_write(&NILFS_MDT(sufile)->mi_sem);\r\nnsegs = nilfs_sufile_get_nsegments(sufile);\r\nif (nsegs == newnsegs)\r\ngoto out;\r\nret = -ENOSPC;\r\nnrsvsegs = nilfs_nrsvsegs(nilfs, newnsegs);\r\nif (newnsegs < nsegs && nsegs - newnsegs + nrsvsegs > sui->ncleansegs)\r\ngoto out;\r\nret = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (ret < 0)\r\ngoto out;\r\nif (newnsegs > nsegs) {\r\nsui->ncleansegs += newnsegs - nsegs;\r\n} else {\r\nret = nilfs_sufile_truncate_range(sufile, newnsegs, nsegs - 1);\r\nif (ret < 0)\r\ngoto out_header;\r\nsui->ncleansegs -= nsegs - newnsegs;\r\n}\r\nkaddr = kmap_atomic(header_bh->b_page);\r\nheader = kaddr + bh_offset(header_bh);\r\nheader->sh_ncleansegs = cpu_to_le64(sui->ncleansegs);\r\nkunmap_atomic(kaddr);\r\nmark_buffer_dirty(header_bh);\r\nnilfs_mdt_mark_dirty(sufile);\r\nnilfs_set_nsegments(nilfs, newnsegs);\r\nout_header:\r\nbrelse(header_bh);\r\nout:\r\nup_write(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nssize_t nilfs_sufile_get_suinfo(struct inode *sufile, __u64 segnum, void *buf,\r\nunsigned sisz, size_t nsi)\r\n{\r\nstruct buffer_head *su_bh;\r\nstruct nilfs_segment_usage *su;\r\nstruct nilfs_suinfo *si = buf;\r\nsize_t susz = NILFS_MDT(sufile)->mi_entry_size;\r\nstruct the_nilfs *nilfs = sufile->i_sb->s_fs_info;\r\nvoid *kaddr;\r\nunsigned long nsegs, segusages_per_block;\r\nssize_t n;\r\nint ret, i, j;\r\ndown_read(&NILFS_MDT(sufile)->mi_sem);\r\nsegusages_per_block = nilfs_sufile_segment_usages_per_block(sufile);\r\nnsegs = min_t(unsigned long,\r\nnilfs_sufile_get_nsegments(sufile) - segnum,\r\nnsi);\r\nfor (i = 0; i < nsegs; i += n, segnum += n) {\r\nn = min_t(unsigned long,\r\nsegusages_per_block -\r\nnilfs_sufile_get_offset(sufile, segnum),\r\nnsegs - i);\r\nret = nilfs_sufile_get_segment_usage_block(sufile, segnum, 0,\r\n&su_bh);\r\nif (ret < 0) {\r\nif (ret != -ENOENT)\r\ngoto out;\r\nmemset(si, 0, sisz * n);\r\nsi = (void *)si + sisz * n;\r\ncontinue;\r\n}\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(\r\nsufile, segnum, su_bh, kaddr);\r\nfor (j = 0; j < n;\r\nj++, su = (void *)su + susz, si = (void *)si + sisz) {\r\nsi->sui_lastmod = le64_to_cpu(su->su_lastmod);\r\nsi->sui_nblocks = le32_to_cpu(su->su_nblocks);\r\nsi->sui_flags = le32_to_cpu(su->su_flags) &\r\n~(1UL << NILFS_SEGMENT_USAGE_ACTIVE);\r\nif (nilfs_segment_is_active(nilfs, segnum + j))\r\nsi->sui_flags |=\r\n(1UL << NILFS_SEGMENT_USAGE_ACTIVE);\r\n}\r\nkunmap_atomic(kaddr);\r\nbrelse(su_bh);\r\n}\r\nret = nsegs;\r\nout:\r\nup_read(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nssize_t nilfs_sufile_set_suinfo(struct inode *sufile, void *buf,\r\nunsigned supsz, size_t nsup)\r\n{\r\nstruct the_nilfs *nilfs = sufile->i_sb->s_fs_info;\r\nstruct buffer_head *header_bh, *bh;\r\nstruct nilfs_suinfo_update *sup, *supend = buf + supsz * nsup;\r\nstruct nilfs_segment_usage *su;\r\nvoid *kaddr;\r\nunsigned long blkoff, prev_blkoff;\r\nint cleansi, cleansu, dirtysi, dirtysu;\r\nlong ncleaned = 0, ndirtied = 0;\r\nint ret = 0;\r\nif (unlikely(nsup == 0))\r\nreturn ret;\r\nfor (sup = buf; sup < supend; sup = (void *)sup + supsz) {\r\nif (sup->sup_segnum >= nilfs->ns_nsegments\r\n|| (sup->sup_flags &\r\n(~0UL << __NR_NILFS_SUINFO_UPDATE_FIELDS))\r\n|| (nilfs_suinfo_update_nblocks(sup) &&\r\nsup->sup_sui.sui_nblocks >\r\nnilfs->ns_blocks_per_segment))\r\nreturn -EINVAL;\r\n}\r\ndown_write(&NILFS_MDT(sufile)->mi_sem);\r\nret = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (ret < 0)\r\ngoto out_sem;\r\nsup = buf;\r\nblkoff = nilfs_sufile_get_blkoff(sufile, sup->sup_segnum);\r\nret = nilfs_mdt_get_block(sufile, blkoff, 1, NULL, &bh);\r\nif (ret < 0)\r\ngoto out_header;\r\nfor (;;) {\r\nkaddr = kmap_atomic(bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(\r\nsufile, sup->sup_segnum, bh, kaddr);\r\nif (nilfs_suinfo_update_lastmod(sup))\r\nsu->su_lastmod = cpu_to_le64(sup->sup_sui.sui_lastmod);\r\nif (nilfs_suinfo_update_nblocks(sup))\r\nsu->su_nblocks = cpu_to_le32(sup->sup_sui.sui_nblocks);\r\nif (nilfs_suinfo_update_flags(sup)) {\r\nsup->sup_sui.sui_flags &=\r\n~(1UL << NILFS_SEGMENT_USAGE_ACTIVE);\r\ncleansi = nilfs_suinfo_clean(&sup->sup_sui);\r\ncleansu = nilfs_segment_usage_clean(su);\r\ndirtysi = nilfs_suinfo_dirty(&sup->sup_sui);\r\ndirtysu = nilfs_segment_usage_dirty(su);\r\nif (cleansi && !cleansu)\r\n++ncleaned;\r\nelse if (!cleansi && cleansu)\r\n--ncleaned;\r\nif (dirtysi && !dirtysu)\r\n++ndirtied;\r\nelse if (!dirtysi && dirtysu)\r\n--ndirtied;\r\nsu->su_flags = cpu_to_le32(sup->sup_sui.sui_flags);\r\n}\r\nkunmap_atomic(kaddr);\r\nsup = (void *)sup + supsz;\r\nif (sup >= supend)\r\nbreak;\r\nprev_blkoff = blkoff;\r\nblkoff = nilfs_sufile_get_blkoff(sufile, sup->sup_segnum);\r\nif (blkoff == prev_blkoff)\r\ncontinue;\r\nmark_buffer_dirty(bh);\r\nput_bh(bh);\r\nret = nilfs_mdt_get_block(sufile, blkoff, 1, NULL, &bh);\r\nif (unlikely(ret < 0))\r\ngoto out_mark;\r\n}\r\nmark_buffer_dirty(bh);\r\nput_bh(bh);\r\nout_mark:\r\nif (ncleaned || ndirtied) {\r\nnilfs_sufile_mod_counter(header_bh, (u64)ncleaned,\r\n(u64)ndirtied);\r\nNILFS_SUI(sufile)->ncleansegs += ncleaned;\r\n}\r\nnilfs_mdt_mark_dirty(sufile);\r\nout_header:\r\nput_bh(header_bh);\r\nout_sem:\r\nup_write(&NILFS_MDT(sufile)->mi_sem);\r\nreturn ret;\r\n}\r\nint nilfs_sufile_trim_fs(struct inode *sufile, struct fstrim_range *range)\r\n{\r\nstruct the_nilfs *nilfs = sufile->i_sb->s_fs_info;\r\nstruct buffer_head *su_bh;\r\nstruct nilfs_segment_usage *su;\r\nvoid *kaddr;\r\nsize_t n, i, susz = NILFS_MDT(sufile)->mi_entry_size;\r\nsector_t seg_start, seg_end, start_block, end_block;\r\nsector_t start = 0, nblocks = 0;\r\nu64 segnum, segnum_end, minlen, len, max_blocks, ndiscarded = 0;\r\nint ret = 0;\r\nunsigned int sects_per_block;\r\nsects_per_block = (1 << nilfs->ns_blocksize_bits) /\r\nbdev_logical_block_size(nilfs->ns_bdev);\r\nlen = range->len >> nilfs->ns_blocksize_bits;\r\nminlen = range->minlen >> nilfs->ns_blocksize_bits;\r\nmax_blocks = ((u64)nilfs->ns_nsegments * nilfs->ns_blocks_per_segment);\r\nif (!len || range->start >= max_blocks << nilfs->ns_blocksize_bits)\r\nreturn -EINVAL;\r\nstart_block = (range->start + nilfs->ns_blocksize - 1) >>\r\nnilfs->ns_blocksize_bits;\r\nif (max_blocks - start_block < len)\r\nend_block = max_blocks - 1;\r\nelse\r\nend_block = start_block + len - 1;\r\nsegnum = nilfs_get_segnum_of_block(nilfs, start_block);\r\nsegnum_end = nilfs_get_segnum_of_block(nilfs, end_block);\r\ndown_read(&NILFS_MDT(sufile)->mi_sem);\r\nwhile (segnum <= segnum_end) {\r\nn = nilfs_sufile_segment_usages_in_block(sufile, segnum,\r\nsegnum_end);\r\nret = nilfs_sufile_get_segment_usage_block(sufile, segnum, 0,\r\n&su_bh);\r\nif (ret < 0) {\r\nif (ret != -ENOENT)\r\ngoto out_sem;\r\nsegnum += n;\r\ncontinue;\r\n}\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(sufile, segnum,\r\nsu_bh, kaddr);\r\nfor (i = 0; i < n; ++i, ++segnum, su = (void *)su + susz) {\r\nif (!nilfs_segment_usage_clean(su))\r\ncontinue;\r\nnilfs_get_segment_range(nilfs, segnum, &seg_start,\r\n&seg_end);\r\nif (!nblocks) {\r\nstart = seg_start;\r\nnblocks = seg_end - seg_start + 1;\r\ncontinue;\r\n}\r\nif (start + nblocks == seg_start) {\r\nnblocks += seg_end - seg_start + 1;\r\ncontinue;\r\n}\r\nif (start < start_block) {\r\nnblocks -= start_block - start;\r\nstart = start_block;\r\n}\r\nif (nblocks >= minlen) {\r\nkunmap_atomic(kaddr);\r\nret = blkdev_issue_discard(nilfs->ns_bdev,\r\nstart * sects_per_block,\r\nnblocks * sects_per_block,\r\nGFP_NOFS, 0);\r\nif (ret < 0) {\r\nput_bh(su_bh);\r\ngoto out_sem;\r\n}\r\nndiscarded += nblocks;\r\nkaddr = kmap_atomic(su_bh->b_page);\r\nsu = nilfs_sufile_block_get_segment_usage(\r\nsufile, segnum, su_bh, kaddr);\r\n}\r\nstart = seg_start;\r\nnblocks = seg_end - seg_start + 1;\r\n}\r\nkunmap_atomic(kaddr);\r\nput_bh(su_bh);\r\n}\r\nif (nblocks) {\r\nif (start < start_block) {\r\nnblocks -= start_block - start;\r\nstart = start_block;\r\n}\r\nif (start + nblocks > end_block + 1)\r\nnblocks = end_block - start + 1;\r\nif (nblocks >= minlen) {\r\nret = blkdev_issue_discard(nilfs->ns_bdev,\r\nstart * sects_per_block,\r\nnblocks * sects_per_block,\r\nGFP_NOFS, 0);\r\nif (!ret)\r\nndiscarded += nblocks;\r\n}\r\n}\r\nout_sem:\r\nup_read(&NILFS_MDT(sufile)->mi_sem);\r\nrange->len = ndiscarded << nilfs->ns_blocksize_bits;\r\nreturn ret;\r\n}\r\nint nilfs_sufile_read(struct super_block *sb, size_t susize,\r\nstruct nilfs_inode *raw_inode, struct inode **inodep)\r\n{\r\nstruct inode *sufile;\r\nstruct nilfs_sufile_info *sui;\r\nstruct buffer_head *header_bh;\r\nstruct nilfs_sufile_header *header;\r\nvoid *kaddr;\r\nint err;\r\nif (susize > sb->s_blocksize) {\r\nprintk(KERN_ERR\r\n"NILFS: too large segment usage size: %zu bytes.\n",\r\nsusize);\r\nreturn -EINVAL;\r\n} else if (susize < NILFS_MIN_SEGMENT_USAGE_SIZE) {\r\nprintk(KERN_ERR\r\n"NILFS: too small segment usage size: %zu bytes.\n",\r\nsusize);\r\nreturn -EINVAL;\r\n}\r\nsufile = nilfs_iget_locked(sb, NULL, NILFS_SUFILE_INO);\r\nif (unlikely(!sufile))\r\nreturn -ENOMEM;\r\nif (!(sufile->i_state & I_NEW))\r\ngoto out;\r\nerr = nilfs_mdt_init(sufile, NILFS_MDT_GFP, sizeof(*sui));\r\nif (err)\r\ngoto failed;\r\nnilfs_mdt_set_entry_size(sufile, susize,\r\nsizeof(struct nilfs_sufile_header));\r\nerr = nilfs_read_inode_common(sufile, raw_inode);\r\nif (err)\r\ngoto failed;\r\nerr = nilfs_sufile_get_header_block(sufile, &header_bh);\r\nif (err)\r\ngoto failed;\r\nsui = NILFS_SUI(sufile);\r\nkaddr = kmap_atomic(header_bh->b_page);\r\nheader = kaddr + bh_offset(header_bh);\r\nsui->ncleansegs = le64_to_cpu(header->sh_ncleansegs);\r\nkunmap_atomic(kaddr);\r\nbrelse(header_bh);\r\nsui->allocmax = nilfs_sufile_get_nsegments(sufile) - 1;\r\nsui->allocmin = 0;\r\nunlock_new_inode(sufile);\r\nout:\r\n*inodep = sufile;\r\nreturn 0;\r\nfailed:\r\niget_failed(sufile);\r\nreturn err;\r\n}
