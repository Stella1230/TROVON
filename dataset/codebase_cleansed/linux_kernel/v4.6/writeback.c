static void __update_writeback_rate(struct cached_dev *dc)\r\n{\r\nstruct cache_set *c = dc->disk.c;\r\nuint64_t cache_sectors = c->nbuckets * c->sb.bucket_size;\r\nuint64_t cache_dirty_target =\r\ndiv_u64(cache_sectors * dc->writeback_percent, 100);\r\nint64_t target = div64_u64(cache_dirty_target * bdev_sectors(dc->bdev),\r\nc->cached_dev_sectors);\r\nint64_t dirty = bcache_dev_sectors_dirty(&dc->disk);\r\nint64_t derivative = dirty - dc->disk.sectors_dirty_last;\r\nint64_t proportional = dirty - target;\r\nint64_t change;\r\ndc->disk.sectors_dirty_last = dirty;\r\nproportional *= dc->writeback_rate_update_seconds;\r\nproportional = div_s64(proportional, dc->writeback_rate_p_term_inverse);\r\nderivative = div_s64(derivative, dc->writeback_rate_update_seconds);\r\nderivative = ewma_add(dc->disk.sectors_dirty_derivative, derivative,\r\n(dc->writeback_rate_d_term /\r\ndc->writeback_rate_update_seconds) ?: 1, 0);\r\nderivative *= dc->writeback_rate_d_term;\r\nderivative = div_s64(derivative, dc->writeback_rate_p_term_inverse);\r\nchange = proportional + derivative;\r\nif (change > 0 &&\r\ntime_after64(local_clock(),\r\ndc->writeback_rate.next + NSEC_PER_MSEC))\r\nchange = 0;\r\ndc->writeback_rate.rate =\r\nclamp_t(int64_t, (int64_t) dc->writeback_rate.rate + change,\r\n1, NSEC_PER_MSEC);\r\ndc->writeback_rate_proportional = proportional;\r\ndc->writeback_rate_derivative = derivative;\r\ndc->writeback_rate_change = change;\r\ndc->writeback_rate_target = target;\r\n}\r\nstatic void update_writeback_rate(struct work_struct *work)\r\n{\r\nstruct cached_dev *dc = container_of(to_delayed_work(work),\r\nstruct cached_dev,\r\nwriteback_rate_update);\r\ndown_read(&dc->writeback_lock);\r\nif (atomic_read(&dc->has_dirty) &&\r\ndc->writeback_percent)\r\n__update_writeback_rate(dc);\r\nup_read(&dc->writeback_lock);\r\nschedule_delayed_work(&dc->writeback_rate_update,\r\ndc->writeback_rate_update_seconds * HZ);\r\n}\r\nstatic unsigned writeback_delay(struct cached_dev *dc, unsigned sectors)\r\n{\r\nif (test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags) ||\r\n!dc->writeback_percent)\r\nreturn 0;\r\nreturn bch_next_delay(&dc->writeback_rate, sectors);\r\n}\r\nstatic void dirty_init(struct keybuf_key *w)\r\n{\r\nstruct dirty_io *io = w->private;\r\nstruct bio *bio = &io->bio;\r\nbio_init(bio);\r\nif (!io->dc->writeback_percent)\r\nbio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));\r\nbio->bi_iter.bi_size = KEY_SIZE(&w->key) << 9;\r\nbio->bi_max_vecs = DIV_ROUND_UP(KEY_SIZE(&w->key), PAGE_SECTORS);\r\nbio->bi_private = w;\r\nbio->bi_io_vec = bio->bi_inline_vecs;\r\nbch_bio_map(bio, NULL);\r\n}\r\nstatic void dirty_io_destructor(struct closure *cl)\r\n{\r\nstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\r\nkfree(io);\r\n}\r\nstatic void write_dirty_finish(struct closure *cl)\r\n{\r\nstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\r\nstruct keybuf_key *w = io->bio.bi_private;\r\nstruct cached_dev *dc = io->dc;\r\nstruct bio_vec *bv;\r\nint i;\r\nbio_for_each_segment_all(bv, &io->bio, i)\r\n__free_page(bv->bv_page);\r\nif (KEY_DIRTY(&w->key)) {\r\nint ret;\r\nunsigned i;\r\nstruct keylist keys;\r\nbch_keylist_init(&keys);\r\nbkey_copy(keys.top, &w->key);\r\nSET_KEY_DIRTY(keys.top, false);\r\nbch_keylist_push(&keys);\r\nfor (i = 0; i < KEY_PTRS(&w->key); i++)\r\natomic_inc(&PTR_BUCKET(dc->disk.c, &w->key, i)->pin);\r\nret = bch_btree_insert(dc->disk.c, &keys, NULL, &w->key);\r\nif (ret)\r\ntrace_bcache_writeback_collision(&w->key);\r\natomic_long_inc(ret\r\n? &dc->disk.c->writeback_keys_failed\r\n: &dc->disk.c->writeback_keys_done);\r\n}\r\nbch_keybuf_del(&dc->writeback_keys, w);\r\nup(&dc->in_flight);\r\nclosure_return_with_destructor(cl, dirty_io_destructor);\r\n}\r\nstatic void dirty_endio(struct bio *bio)\r\n{\r\nstruct keybuf_key *w = bio->bi_private;\r\nstruct dirty_io *io = w->private;\r\nif (bio->bi_error)\r\nSET_KEY_DIRTY(&w->key, false);\r\nclosure_put(&io->cl);\r\n}\r\nstatic void write_dirty(struct closure *cl)\r\n{\r\nstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\r\nstruct keybuf_key *w = io->bio.bi_private;\r\ndirty_init(w);\r\nio->bio.bi_rw = WRITE;\r\nio->bio.bi_iter.bi_sector = KEY_START(&w->key);\r\nio->bio.bi_bdev = io->dc->bdev;\r\nio->bio.bi_end_io = dirty_endio;\r\nclosure_bio_submit(&io->bio, cl);\r\ncontinue_at(cl, write_dirty_finish, system_wq);\r\n}\r\nstatic void read_dirty_endio(struct bio *bio)\r\n{\r\nstruct keybuf_key *w = bio->bi_private;\r\nstruct dirty_io *io = w->private;\r\nbch_count_io_errors(PTR_CACHE(io->dc->disk.c, &w->key, 0),\r\nbio->bi_error, "reading dirty data from cache");\r\ndirty_endio(bio);\r\n}\r\nstatic void read_dirty_submit(struct closure *cl)\r\n{\r\nstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\r\nclosure_bio_submit(&io->bio, cl);\r\ncontinue_at(cl, write_dirty, system_wq);\r\n}\r\nstatic void read_dirty(struct cached_dev *dc)\r\n{\r\nunsigned delay = 0;\r\nstruct keybuf_key *w;\r\nstruct dirty_io *io;\r\nstruct closure cl;\r\nclosure_init_stack(&cl);\r\nwhile (!kthread_should_stop()) {\r\ntry_to_freeze();\r\nw = bch_keybuf_next(&dc->writeback_keys);\r\nif (!w)\r\nbreak;\r\nBUG_ON(ptr_stale(dc->disk.c, &w->key, 0));\r\nif (KEY_START(&w->key) != dc->last_read ||\r\njiffies_to_msecs(delay) > 50)\r\nwhile (!kthread_should_stop() && delay)\r\ndelay = schedule_timeout_interruptible(delay);\r\ndc->last_read = KEY_OFFSET(&w->key);\r\nio = kzalloc(sizeof(struct dirty_io) + sizeof(struct bio_vec)\r\n* DIV_ROUND_UP(KEY_SIZE(&w->key), PAGE_SECTORS),\r\nGFP_KERNEL);\r\nif (!io)\r\ngoto err;\r\nw->private = io;\r\nio->dc = dc;\r\ndirty_init(w);\r\nio->bio.bi_iter.bi_sector = PTR_OFFSET(&w->key, 0);\r\nio->bio.bi_bdev = PTR_CACHE(dc->disk.c,\r\n&w->key, 0)->bdev;\r\nio->bio.bi_rw = READ;\r\nio->bio.bi_end_io = read_dirty_endio;\r\nif (bio_alloc_pages(&io->bio, GFP_KERNEL))\r\ngoto err_free;\r\ntrace_bcache_writeback(&w->key);\r\ndown(&dc->in_flight);\r\nclosure_call(&io->cl, read_dirty_submit, NULL, &cl);\r\ndelay = writeback_delay(dc, KEY_SIZE(&w->key));\r\n}\r\nif (0) {\r\nerr_free:\r\nkfree(w->private);\r\nerr:\r\nbch_keybuf_del(&dc->writeback_keys, w);\r\n}\r\nclosure_sync(&cl);\r\n}\r\nvoid bcache_dev_sectors_dirty_add(struct cache_set *c, unsigned inode,\r\nuint64_t offset, int nr_sectors)\r\n{\r\nstruct bcache_device *d = c->devices[inode];\r\nunsigned stripe_offset, stripe, sectors_dirty;\r\nif (!d)\r\nreturn;\r\nstripe = offset_to_stripe(d, offset);\r\nstripe_offset = offset & (d->stripe_size - 1);\r\nwhile (nr_sectors) {\r\nint s = min_t(unsigned, abs(nr_sectors),\r\nd->stripe_size - stripe_offset);\r\nif (nr_sectors < 0)\r\ns = -s;\r\nif (stripe >= d->nr_stripes)\r\nreturn;\r\nsectors_dirty = atomic_add_return(s,\r\nd->stripe_sectors_dirty + stripe);\r\nif (sectors_dirty == d->stripe_size)\r\nset_bit(stripe, d->full_dirty_stripes);\r\nelse\r\nclear_bit(stripe, d->full_dirty_stripes);\r\nnr_sectors -= s;\r\nstripe_offset = 0;\r\nstripe++;\r\n}\r\n}\r\nstatic bool dirty_pred(struct keybuf *buf, struct bkey *k)\r\n{\r\nstruct cached_dev *dc = container_of(buf, struct cached_dev, writeback_keys);\r\nBUG_ON(KEY_INODE(k) != dc->disk.id);\r\nreturn KEY_DIRTY(k);\r\n}\r\nstatic void refill_full_stripes(struct cached_dev *dc)\r\n{\r\nstruct keybuf *buf = &dc->writeback_keys;\r\nunsigned start_stripe, stripe, next_stripe;\r\nbool wrapped = false;\r\nstripe = offset_to_stripe(&dc->disk, KEY_OFFSET(&buf->last_scanned));\r\nif (stripe >= dc->disk.nr_stripes)\r\nstripe = 0;\r\nstart_stripe = stripe;\r\nwhile (1) {\r\nstripe = find_next_bit(dc->disk.full_dirty_stripes,\r\ndc->disk.nr_stripes, stripe);\r\nif (stripe == dc->disk.nr_stripes)\r\ngoto next;\r\nnext_stripe = find_next_zero_bit(dc->disk.full_dirty_stripes,\r\ndc->disk.nr_stripes, stripe);\r\nbuf->last_scanned = KEY(dc->disk.id,\r\nstripe * dc->disk.stripe_size, 0);\r\nbch_refill_keybuf(dc->disk.c, buf,\r\n&KEY(dc->disk.id,\r\nnext_stripe * dc->disk.stripe_size, 0),\r\ndirty_pred);\r\nif (array_freelist_empty(&buf->freelist))\r\nreturn;\r\nstripe = next_stripe;\r\nnext:\r\nif (wrapped && stripe > start_stripe)\r\nreturn;\r\nif (stripe == dc->disk.nr_stripes) {\r\nstripe = 0;\r\nwrapped = true;\r\n}\r\n}\r\n}\r\nstatic bool refill_dirty(struct cached_dev *dc)\r\n{\r\nstruct keybuf *buf = &dc->writeback_keys;\r\nstruct bkey start = KEY(dc->disk.id, 0, 0);\r\nstruct bkey end = KEY(dc->disk.id, MAX_KEY_OFFSET, 0);\r\nstruct bkey start_pos;\r\nif (bkey_cmp(&buf->last_scanned, &start) < 0 ||\r\nbkey_cmp(&buf->last_scanned, &end) > 0)\r\nbuf->last_scanned = start;\r\nif (dc->partial_stripes_expensive) {\r\nrefill_full_stripes(dc);\r\nif (array_freelist_empty(&buf->freelist))\r\nreturn false;\r\n}\r\nstart_pos = buf->last_scanned;\r\nbch_refill_keybuf(dc->disk.c, buf, &end, dirty_pred);\r\nif (bkey_cmp(&buf->last_scanned, &end) < 0)\r\nreturn false;\r\nbuf->last_scanned = start;\r\nbch_refill_keybuf(dc->disk.c, buf, &start_pos, dirty_pred);\r\nreturn bkey_cmp(&buf->last_scanned, &start_pos) >= 0;\r\n}\r\nstatic int bch_writeback_thread(void *arg)\r\n{\r\nstruct cached_dev *dc = arg;\r\nbool searched_full_index;\r\nwhile (!kthread_should_stop()) {\r\ndown_write(&dc->writeback_lock);\r\nif (!atomic_read(&dc->has_dirty) ||\r\n(!test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags) &&\r\n!dc->writeback_running)) {\r\nup_write(&dc->writeback_lock);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (kthread_should_stop())\r\nreturn 0;\r\ntry_to_freeze();\r\nschedule();\r\ncontinue;\r\n}\r\nsearched_full_index = refill_dirty(dc);\r\nif (searched_full_index &&\r\nRB_EMPTY_ROOT(&dc->writeback_keys.keys)) {\r\natomic_set(&dc->has_dirty, 0);\r\ncached_dev_put(dc);\r\nSET_BDEV_STATE(&dc->sb, BDEV_STATE_CLEAN);\r\nbch_write_bdev_super(dc, NULL);\r\n}\r\nup_write(&dc->writeback_lock);\r\nbch_ratelimit_reset(&dc->writeback_rate);\r\nread_dirty(dc);\r\nif (searched_full_index) {\r\nunsigned delay = dc->writeback_delay * HZ;\r\nwhile (delay &&\r\n!kthread_should_stop() &&\r\n!test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags))\r\ndelay = schedule_timeout_interruptible(delay);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int sectors_dirty_init_fn(struct btree_op *_op, struct btree *b,\r\nstruct bkey *k)\r\n{\r\nstruct sectors_dirty_init *op = container_of(_op,\r\nstruct sectors_dirty_init, op);\r\nif (KEY_INODE(k) > op->inode)\r\nreturn MAP_DONE;\r\nif (KEY_DIRTY(k))\r\nbcache_dev_sectors_dirty_add(b->c, KEY_INODE(k),\r\nKEY_START(k), KEY_SIZE(k));\r\nreturn MAP_CONTINUE;\r\n}\r\nvoid bch_sectors_dirty_init(struct cached_dev *dc)\r\n{\r\nstruct sectors_dirty_init op;\r\nbch_btree_op_init(&op.op, -1);\r\nop.inode = dc->disk.id;\r\nbch_btree_map_keys(&op.op, dc->disk.c, &KEY(op.inode, 0, 0),\r\nsectors_dirty_init_fn, 0);\r\ndc->disk.sectors_dirty_last = bcache_dev_sectors_dirty(&dc->disk);\r\n}\r\nvoid bch_cached_dev_writeback_init(struct cached_dev *dc)\r\n{\r\nsema_init(&dc->in_flight, 64);\r\ninit_rwsem(&dc->writeback_lock);\r\nbch_keybuf_init(&dc->writeback_keys);\r\ndc->writeback_metadata = true;\r\ndc->writeback_running = true;\r\ndc->writeback_percent = 10;\r\ndc->writeback_delay = 30;\r\ndc->writeback_rate.rate = 1024;\r\ndc->writeback_rate_update_seconds = 5;\r\ndc->writeback_rate_d_term = 30;\r\ndc->writeback_rate_p_term_inverse = 6000;\r\nINIT_DELAYED_WORK(&dc->writeback_rate_update, update_writeback_rate);\r\n}\r\nint bch_cached_dev_writeback_start(struct cached_dev *dc)\r\n{\r\ndc->writeback_thread = kthread_create(bch_writeback_thread, dc,\r\n"bcache_writeback");\r\nif (IS_ERR(dc->writeback_thread))\r\nreturn PTR_ERR(dc->writeback_thread);\r\nschedule_delayed_work(&dc->writeback_rate_update,\r\ndc->writeback_rate_update_seconds * HZ);\r\nbch_writeback_queue(dc);\r\nreturn 0;\r\n}
