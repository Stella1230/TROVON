static void rpcrdma_bc_free_rqst(struct rpcrdma_xprt *r_xprt,\r\nstruct rpc_rqst *rqst)\r\n{\r\nstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\r\nstruct rpcrdma_req *req = rpcr_to_rdmar(rqst);\r\nspin_lock(&buf->rb_reqslock);\r\nlist_del(&req->rl_all);\r\nspin_unlock(&buf->rb_reqslock);\r\nrpcrdma_destroy_req(&r_xprt->rx_ia, req);\r\nkfree(rqst);\r\n}\r\nstatic int rpcrdma_bc_setup_rqst(struct rpcrdma_xprt *r_xprt,\r\nstruct rpc_rqst *rqst)\r\n{\r\nstruct rpcrdma_ia *ia = &r_xprt->rx_ia;\r\nstruct rpcrdma_regbuf *rb;\r\nstruct rpcrdma_req *req;\r\nstruct xdr_buf *buf;\r\nsize_t size;\r\nreq = rpcrdma_create_req(r_xprt);\r\nif (IS_ERR(req))\r\nreturn PTR_ERR(req);\r\nreq->rl_backchannel = true;\r\nsize = RPCRDMA_INLINE_WRITE_THRESHOLD(rqst);\r\nrb = rpcrdma_alloc_regbuf(ia, size, GFP_KERNEL);\r\nif (IS_ERR(rb))\r\ngoto out_fail;\r\nreq->rl_rdmabuf = rb;\r\nsize += RPCRDMA_INLINE_READ_THRESHOLD(rqst);\r\nrb = rpcrdma_alloc_regbuf(ia, size, GFP_KERNEL);\r\nif (IS_ERR(rb))\r\ngoto out_fail;\r\nrb->rg_owner = req;\r\nreq->rl_sendbuf = rb;\r\nrqst->rq_buffer = (void *)req->rl_sendbuf->rg_base;\r\nbuf = &rqst->rq_snd_buf;\r\nbuf->head[0].iov_base = rqst->rq_buffer;\r\nbuf->head[0].iov_len = 0;\r\nbuf->tail[0].iov_base = NULL;\r\nbuf->tail[0].iov_len = 0;\r\nbuf->page_len = 0;\r\nbuf->len = 0;\r\nbuf->buflen = size;\r\nreturn 0;\r\nout_fail:\r\nrpcrdma_bc_free_rqst(r_xprt, rqst);\r\nreturn -ENOMEM;\r\n}\r\nstatic int rpcrdma_bc_setup_reps(struct rpcrdma_xprt *r_xprt,\r\nunsigned int count)\r\n{\r\nstruct rpcrdma_rep *rep;\r\nint rc = 0;\r\nwhile (count--) {\r\nrep = rpcrdma_create_rep(r_xprt);\r\nif (IS_ERR(rep)) {\r\npr_err("RPC: %s: reply buffer alloc failed\n",\r\n__func__);\r\nrc = PTR_ERR(rep);\r\nbreak;\r\n}\r\nrpcrdma_recv_buffer_put(rep);\r\n}\r\nreturn rc;\r\n}\r\nint xprt_rdma_bc_setup(struct rpc_xprt *xprt, unsigned int reqs)\r\n{\r\nstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\r\nstruct rpcrdma_buffer *buffer = &r_xprt->rx_buf;\r\nstruct rpc_rqst *rqst;\r\nunsigned int i;\r\nint rc;\r\nif (reqs > RPCRDMA_BACKWARD_WRS >> 1)\r\ngoto out_err;\r\nfor (i = 0; i < (reqs << 1); i++) {\r\nrqst = kzalloc(sizeof(*rqst), GFP_KERNEL);\r\nif (!rqst) {\r\npr_err("RPC: %s: Failed to create bc rpc_rqst\n",\r\n__func__);\r\ngoto out_free;\r\n}\r\ndprintk("RPC: %s: new rqst %p\n", __func__, rqst);\r\nrqst->rq_xprt = &r_xprt->rx_xprt;\r\nINIT_LIST_HEAD(&rqst->rq_list);\r\nINIT_LIST_HEAD(&rqst->rq_bc_list);\r\nif (rpcrdma_bc_setup_rqst(r_xprt, rqst))\r\ngoto out_free;\r\nspin_lock_bh(&xprt->bc_pa_lock);\r\nlist_add(&rqst->rq_bc_pa_list, &xprt->bc_pa_list);\r\nspin_unlock_bh(&xprt->bc_pa_lock);\r\n}\r\nrc = rpcrdma_bc_setup_reps(r_xprt, reqs);\r\nif (rc)\r\ngoto out_free;\r\nrc = rpcrdma_ep_post_extra_recv(r_xprt, reqs);\r\nif (rc)\r\ngoto out_free;\r\nbuffer->rb_bc_srv_max_requests = reqs;\r\nrequest_module("svcrdma");\r\nreturn 0;\r\nout_free:\r\nxprt_rdma_bc_destroy(xprt, reqs);\r\nout_err:\r\npr_err("RPC: %s: setup backchannel transport failed\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nint xprt_rdma_bc_up(struct svc_serv *serv, struct net *net)\r\n{\r\nint ret;\r\nret = svc_create_xprt(serv, "rdma-bc", net, PF_INET, 0, 0);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nint rpcrdma_bc_marshal_reply(struct rpc_rqst *rqst)\r\n{\r\nstruct rpc_xprt *xprt = rqst->rq_xprt;\r\nstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\r\nstruct rpcrdma_req *req = rpcr_to_rdmar(rqst);\r\nstruct rpcrdma_msg *headerp;\r\nsize_t rpclen;\r\nheaderp = rdmab_to_msg(req->rl_rdmabuf);\r\nheaderp->rm_xid = rqst->rq_xid;\r\nheaderp->rm_vers = rpcrdma_version;\r\nheaderp->rm_credit =\r\ncpu_to_be32(r_xprt->rx_buf.rb_bc_srv_max_requests);\r\nheaderp->rm_type = rdma_msg;\r\nheaderp->rm_body.rm_chunks[0] = xdr_zero;\r\nheaderp->rm_body.rm_chunks[1] = xdr_zero;\r\nheaderp->rm_body.rm_chunks[2] = xdr_zero;\r\nrpclen = rqst->rq_svec[0].iov_len;\r\n#ifdef RPCRDMA_BACKCHANNEL_DEBUG\r\npr_info("RPC: %s: rpclen %zd headerp 0x%p lkey 0x%x\n",\r\n__func__, rpclen, headerp, rdmab_lkey(req->rl_rdmabuf));\r\npr_info("RPC: %s: RPC/RDMA: %*ph\n",\r\n__func__, (int)RPCRDMA_HDRLEN_MIN, headerp);\r\npr_info("RPC: %s: RPC: %*ph\n",\r\n__func__, (int)rpclen, rqst->rq_svec[0].iov_base);\r\n#endif\r\nreq->rl_send_iov[0].addr = rdmab_addr(req->rl_rdmabuf);\r\nreq->rl_send_iov[0].length = RPCRDMA_HDRLEN_MIN;\r\nreq->rl_send_iov[0].lkey = rdmab_lkey(req->rl_rdmabuf);\r\nreq->rl_send_iov[1].addr = rdmab_addr(req->rl_sendbuf);\r\nreq->rl_send_iov[1].length = rpclen;\r\nreq->rl_send_iov[1].lkey = rdmab_lkey(req->rl_sendbuf);\r\nreq->rl_niovs = 2;\r\nreturn 0;\r\n}\r\nvoid xprt_rdma_bc_destroy(struct rpc_xprt *xprt, unsigned int reqs)\r\n{\r\nstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\r\nstruct rpc_rqst *rqst, *tmp;\r\nspin_lock_bh(&xprt->bc_pa_lock);\r\nlist_for_each_entry_safe(rqst, tmp, &xprt->bc_pa_list, rq_bc_pa_list) {\r\nlist_del(&rqst->rq_bc_pa_list);\r\nspin_unlock_bh(&xprt->bc_pa_lock);\r\nrpcrdma_bc_free_rqst(r_xprt, rqst);\r\nspin_lock_bh(&xprt->bc_pa_lock);\r\n}\r\nspin_unlock_bh(&xprt->bc_pa_lock);\r\n}\r\nvoid xprt_rdma_bc_free_rqst(struct rpc_rqst *rqst)\r\n{\r\nstruct rpc_xprt *xprt = rqst->rq_xprt;\r\ndprintk("RPC: %s: freeing rqst %p (req %p)\n",\r\n__func__, rqst, rpcr_to_rdmar(rqst));\r\nsmp_mb__before_atomic();\r\nWARN_ON_ONCE(!test_bit(RPC_BC_PA_IN_USE, &rqst->rq_bc_pa_state));\r\nclear_bit(RPC_BC_PA_IN_USE, &rqst->rq_bc_pa_state);\r\nsmp_mb__after_atomic();\r\nspin_lock_bh(&xprt->bc_pa_lock);\r\nlist_add_tail(&rqst->rq_bc_pa_list, &xprt->bc_pa_list);\r\nspin_unlock_bh(&xprt->bc_pa_lock);\r\n}\r\nvoid rpcrdma_bc_receive_call(struct rpcrdma_xprt *r_xprt,\r\nstruct rpcrdma_rep *rep)\r\n{\r\nstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\r\nstruct rpcrdma_msg *headerp;\r\nstruct svc_serv *bc_serv;\r\nstruct rpcrdma_req *req;\r\nstruct rpc_rqst *rqst;\r\nstruct xdr_buf *buf;\r\nsize_t size;\r\n__be32 *p;\r\nheaderp = rdmab_to_msg(rep->rr_rdmabuf);\r\n#ifdef RPCRDMA_BACKCHANNEL_DEBUG\r\npr_info("RPC: %s: callback XID %08x, length=%u\n",\r\n__func__, be32_to_cpu(headerp->rm_xid), rep->rr_len);\r\npr_info("RPC: %s: %*ph\n", __func__, rep->rr_len, headerp);\r\n#endif\r\nif (rep->rr_len < RPCRDMA_HDRLEN_MIN + 24)\r\ngoto out_short;\r\np = (__be32 *)((unsigned char *)headerp + RPCRDMA_HDRLEN_MIN);\r\nsize = rep->rr_len - RPCRDMA_HDRLEN_MIN;\r\nspin_lock(&xprt->bc_pa_lock);\r\nif (list_empty(&xprt->bc_pa_list)) {\r\nspin_unlock(&xprt->bc_pa_lock);\r\ngoto out_overflow;\r\n}\r\nrqst = list_first_entry(&xprt->bc_pa_list,\r\nstruct rpc_rqst, rq_bc_pa_list);\r\nlist_del(&rqst->rq_bc_pa_list);\r\nspin_unlock(&xprt->bc_pa_lock);\r\ndprintk("RPC: %s: using rqst %p\n", __func__, rqst);\r\nrqst->rq_reply_bytes_recvd = 0;\r\nrqst->rq_bytes_sent = 0;\r\nrqst->rq_xid = headerp->rm_xid;\r\nrqst->rq_private_buf.len = size;\r\nset_bit(RPC_BC_PA_IN_USE, &rqst->rq_bc_pa_state);\r\nbuf = &rqst->rq_rcv_buf;\r\nmemset(buf, 0, sizeof(*buf));\r\nbuf->head[0].iov_base = p;\r\nbuf->head[0].iov_len = size;\r\nbuf->len = size;\r\nreq = rpcr_to_rdmar(rqst);\r\ndprintk("RPC: %s: attaching rep %p to req %p\n",\r\n__func__, rep, req);\r\nreq->rl_reply = rep;\r\nreq->rl_connect_cookie = 0;\r\nbc_serv = xprt->bc_serv;\r\nspin_lock(&bc_serv->sv_cb_lock);\r\nlist_add(&rqst->rq_bc_list, &bc_serv->sv_cb_list);\r\nspin_unlock(&bc_serv->sv_cb_lock);\r\nwake_up(&bc_serv->sv_cb_waitq);\r\nr_xprt->rx_stats.bcall_count++;\r\nreturn;\r\nout_overflow:\r\npr_warn("RPC/RDMA backchannel overflow\n");\r\nxprt_disconnect_done(xprt);\r\nreturn;\r\nout_short:\r\npr_warn("RPC/RDMA short backward direction call\n");\r\nif (rpcrdma_ep_post_recv(&r_xprt->rx_ia, &r_xprt->rx_ep, rep))\r\nxprt_disconnect_done(xprt);\r\nelse\r\npr_warn("RPC: %s: reposting rep %p\n",\r\n__func__, rep);\r\n}
