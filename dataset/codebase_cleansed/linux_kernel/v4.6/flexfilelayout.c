static struct pnfs_layout_hdr *\r\nff_layout_alloc_layout_hdr(struct inode *inode, gfp_t gfp_flags)\r\n{\r\nstruct nfs4_flexfile_layout *ffl;\r\nffl = kzalloc(sizeof(*ffl), gfp_flags);\r\nif (ffl) {\r\nINIT_LIST_HEAD(&ffl->error_list);\r\nINIT_LIST_HEAD(&ffl->mirrors);\r\nreturn &ffl->generic_hdr;\r\n} else\r\nreturn NULL;\r\n}\r\nstatic void\r\nff_layout_free_layout_hdr(struct pnfs_layout_hdr *lo)\r\n{\r\nstruct nfs4_ff_layout_ds_err *err, *n;\r\nlist_for_each_entry_safe(err, n, &FF_LAYOUT_FROM_HDR(lo)->error_list,\r\nlist) {\r\nlist_del(&err->list);\r\nkfree(err);\r\n}\r\nkfree(FF_LAYOUT_FROM_HDR(lo));\r\n}\r\nstatic int decode_stateid(struct xdr_stream *xdr, nfs4_stateid *stateid)\r\n{\r\n__be32 *p;\r\np = xdr_inline_decode(xdr, NFS4_STATEID_SIZE);\r\nif (unlikely(p == NULL))\r\nreturn -ENOBUFS;\r\nmemcpy(stateid, p, NFS4_STATEID_SIZE);\r\ndprintk("%s: stateid id= [%x%x%x%x]\n", __func__,\r\np[0], p[1], p[2], p[3]);\r\nreturn 0;\r\n}\r\nstatic int decode_deviceid(struct xdr_stream *xdr, struct nfs4_deviceid *devid)\r\n{\r\n__be32 *p;\r\np = xdr_inline_decode(xdr, NFS4_DEVICEID4_SIZE);\r\nif (unlikely(!p))\r\nreturn -ENOBUFS;\r\nmemcpy(devid, p, NFS4_DEVICEID4_SIZE);\r\nnfs4_print_deviceid(devid);\r\nreturn 0;\r\n}\r\nstatic int decode_nfs_fh(struct xdr_stream *xdr, struct nfs_fh *fh)\r\n{\r\n__be32 *p;\r\np = xdr_inline_decode(xdr, 4);\r\nif (unlikely(!p))\r\nreturn -ENOBUFS;\r\nfh->size = be32_to_cpup(p++);\r\nif (fh->size > sizeof(struct nfs_fh)) {\r\nprintk(KERN_ERR "NFS flexfiles: Too big fh received %d\n",\r\nfh->size);\r\nreturn -EOVERFLOW;\r\n}\r\np = xdr_inline_decode(xdr, fh->size);\r\nif (unlikely(!p))\r\nreturn -ENOBUFS;\r\nmemcpy(&fh->data, p, fh->size);\r\ndprintk("%s: fh len %d\n", __func__, fh->size);\r\nreturn 0;\r\n}\r\nstatic int\r\ndecode_name(struct xdr_stream *xdr, u32 *id)\r\n{\r\n__be32 *p;\r\nint len;\r\np = xdr_inline_decode(xdr, 4);\r\nif (unlikely(!p))\r\nreturn -ENOBUFS;\r\nlen = be32_to_cpup(p++);\r\nif (len < 0)\r\nreturn -EINVAL;\r\ndprintk("%s: len %u\n", __func__, len);\r\np = xdr_inline_decode(xdr, len);\r\nif (unlikely(!p))\r\nreturn -ENOBUFS;\r\nif (!nfs_map_string_to_numeric((char *)p, len, id))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic bool ff_mirror_match_fh(const struct nfs4_ff_layout_mirror *m1,\r\nconst struct nfs4_ff_layout_mirror *m2)\r\n{\r\nint i, j;\r\nif (m1->fh_versions_cnt != m2->fh_versions_cnt)\r\nreturn false;\r\nfor (i = 0; i < m1->fh_versions_cnt; i++) {\r\nbool found_fh = false;\r\nfor (j = 0; j < m2->fh_versions_cnt; j++) {\r\nif (nfs_compare_fh(&m1->fh_versions[i],\r\n&m2->fh_versions[j]) == 0) {\r\nfound_fh = true;\r\nbreak;\r\n}\r\n}\r\nif (!found_fh)\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic struct nfs4_ff_layout_mirror *\r\nff_layout_add_mirror(struct pnfs_layout_hdr *lo,\r\nstruct nfs4_ff_layout_mirror *mirror)\r\n{\r\nstruct nfs4_flexfile_layout *ff_layout = FF_LAYOUT_FROM_HDR(lo);\r\nstruct nfs4_ff_layout_mirror *pos;\r\nstruct inode *inode = lo->plh_inode;\r\nspin_lock(&inode->i_lock);\r\nlist_for_each_entry(pos, &ff_layout->mirrors, mirrors) {\r\nif (mirror->mirror_ds != pos->mirror_ds)\r\ncontinue;\r\nif (!ff_mirror_match_fh(mirror, pos))\r\ncontinue;\r\nif (atomic_inc_not_zero(&pos->ref)) {\r\nspin_unlock(&inode->i_lock);\r\nreturn pos;\r\n}\r\n}\r\nlist_add(&mirror->mirrors, &ff_layout->mirrors);\r\nmirror->layout = lo;\r\nspin_unlock(&inode->i_lock);\r\nreturn mirror;\r\n}\r\nstatic void\r\nff_layout_remove_mirror(struct nfs4_ff_layout_mirror *mirror)\r\n{\r\nstruct inode *inode;\r\nif (mirror->layout == NULL)\r\nreturn;\r\ninode = mirror->layout->plh_inode;\r\nspin_lock(&inode->i_lock);\r\nlist_del(&mirror->mirrors);\r\nspin_unlock(&inode->i_lock);\r\nmirror->layout = NULL;\r\n}\r\nstatic struct nfs4_ff_layout_mirror *ff_layout_alloc_mirror(gfp_t gfp_flags)\r\n{\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nmirror = kzalloc(sizeof(*mirror), gfp_flags);\r\nif (mirror != NULL) {\r\nspin_lock_init(&mirror->lock);\r\natomic_set(&mirror->ref, 1);\r\nINIT_LIST_HEAD(&mirror->mirrors);\r\n}\r\nreturn mirror;\r\n}\r\nstatic void ff_layout_free_mirror(struct nfs4_ff_layout_mirror *mirror)\r\n{\r\nff_layout_remove_mirror(mirror);\r\nkfree(mirror->fh_versions);\r\nif (mirror->cred)\r\nput_rpccred(mirror->cred);\r\nnfs4_ff_layout_put_deviceid(mirror->mirror_ds);\r\nkfree(mirror);\r\n}\r\nstatic void ff_layout_put_mirror(struct nfs4_ff_layout_mirror *mirror)\r\n{\r\nif (mirror != NULL && atomic_dec_and_test(&mirror->ref))\r\nff_layout_free_mirror(mirror);\r\n}\r\nstatic void ff_layout_free_mirror_array(struct nfs4_ff_layout_segment *fls)\r\n{\r\nint i;\r\nif (fls->mirror_array) {\r\nfor (i = 0; i < fls->mirror_array_cnt; i++) {\r\nff_layout_put_mirror(fls->mirror_array[i]);\r\n}\r\nkfree(fls->mirror_array);\r\nfls->mirror_array = NULL;\r\n}\r\n}\r\nstatic int ff_layout_check_layout(struct nfs4_layoutget_res *lgr)\r\n{\r\nint ret = 0;\r\ndprintk("--> %s\n", __func__);\r\nif (lgr->range.offset != 0 ||\r\nlgr->range.length != NFS4_MAX_UINT64) {\r\ndprintk("%s Only whole file layouts supported. Use MDS i/o\n",\r\n__func__);\r\nret = -EINVAL;\r\n}\r\ndprintk("--> %s returns %d\n", __func__, ret);\r\nreturn ret;\r\n}\r\nstatic void _ff_layout_free_lseg(struct nfs4_ff_layout_segment *fls)\r\n{\r\nif (fls) {\r\nff_layout_free_mirror_array(fls);\r\nkfree(fls);\r\n}\r\n}\r\nstatic bool\r\nff_lseg_range_is_after(const struct pnfs_layout_range *l1,\r\nconst struct pnfs_layout_range *l2)\r\n{\r\nu64 end1, end2;\r\nif (l1->iomode != l2->iomode)\r\nreturn l1->iomode != IOMODE_READ;\r\nend1 = pnfs_calc_offset_end(l1->offset, l1->length);\r\nend2 = pnfs_calc_offset_end(l2->offset, l2->length);\r\nif (end1 < l2->offset)\r\nreturn false;\r\nif (end2 < l1->offset)\r\nreturn true;\r\nreturn l2->offset <= l1->offset;\r\n}\r\nstatic bool\r\nff_lseg_merge(struct pnfs_layout_segment *new,\r\nstruct pnfs_layout_segment *old)\r\n{\r\nu64 new_end, old_end;\r\nif (new->pls_range.iomode != old->pls_range.iomode)\r\nreturn false;\r\nold_end = pnfs_calc_offset_end(old->pls_range.offset,\r\nold->pls_range.length);\r\nif (old_end < new->pls_range.offset)\r\nreturn false;\r\nnew_end = pnfs_calc_offset_end(new->pls_range.offset,\r\nnew->pls_range.length);\r\nif (new_end < old->pls_range.offset)\r\nreturn false;\r\nif (new_end < old_end)\r\nnew_end = old_end;\r\nif (new->pls_range.offset < old->pls_range.offset)\r\nnew->pls_range.offset = old->pls_range.offset;\r\nnew->pls_range.length = pnfs_calc_offset_length(new->pls_range.offset,\r\nnew_end);\r\nif (test_bit(NFS_LSEG_ROC, &old->pls_flags))\r\nset_bit(NFS_LSEG_ROC, &new->pls_flags);\r\nif (test_bit(NFS_LSEG_LAYOUTRETURN, &old->pls_flags))\r\nset_bit(NFS_LSEG_LAYOUTRETURN, &new->pls_flags);\r\nreturn true;\r\n}\r\nstatic void\r\nff_layout_add_lseg(struct pnfs_layout_hdr *lo,\r\nstruct pnfs_layout_segment *lseg,\r\nstruct list_head *free_me)\r\n{\r\npnfs_generic_layout_insert_lseg(lo, lseg,\r\nff_lseg_range_is_after,\r\nff_lseg_merge,\r\nfree_me);\r\n}\r\nstatic void ff_layout_sort_mirrors(struct nfs4_ff_layout_segment *fls)\r\n{\r\nint i, j;\r\nfor (i = 0; i < fls->mirror_array_cnt - 1; i++) {\r\nfor (j = i + 1; j < fls->mirror_array_cnt; j++)\r\nif (fls->mirror_array[i]->efficiency <\r\nfls->mirror_array[j]->efficiency)\r\nswap(fls->mirror_array[i],\r\nfls->mirror_array[j]);\r\n}\r\n}\r\nstatic void ff_layout_mark_devices_valid(struct nfs4_ff_layout_segment *fls)\r\n{\r\nstruct nfs4_deviceid_node *node;\r\nint i;\r\nif (!(fls->flags & FF_FLAGS_NO_IO_THRU_MDS))\r\nreturn;\r\nfor (i = 0; i < fls->mirror_array_cnt; i++) {\r\nnode = &fls->mirror_array[i]->mirror_ds->id_node;\r\nclear_bit(NFS_DEVICEID_UNAVAILABLE, &node->flags);\r\n}\r\n}\r\nstatic struct pnfs_layout_segment *\r\nff_layout_alloc_lseg(struct pnfs_layout_hdr *lh,\r\nstruct nfs4_layoutget_res *lgr,\r\ngfp_t gfp_flags)\r\n{\r\nstruct pnfs_layout_segment *ret;\r\nstruct nfs4_ff_layout_segment *fls = NULL;\r\nstruct xdr_stream stream;\r\nstruct xdr_buf buf;\r\nstruct page *scratch;\r\nu64 stripe_unit;\r\nu32 mirror_array_cnt;\r\n__be32 *p;\r\nint i, rc;\r\ndprintk("--> %s\n", __func__);\r\nscratch = alloc_page(gfp_flags);\r\nif (!scratch)\r\nreturn ERR_PTR(-ENOMEM);\r\nxdr_init_decode_pages(&stream, &buf, lgr->layoutp->pages,\r\nlgr->layoutp->len);\r\nxdr_set_scratch_buffer(&stream, page_address(scratch), PAGE_SIZE);\r\nrc = -EIO;\r\np = xdr_inline_decode(&stream, 8 + 4);\r\nif (!p)\r\ngoto out_err_free;\r\np = xdr_decode_hyper(p, &stripe_unit);\r\nmirror_array_cnt = be32_to_cpup(p++);\r\ndprintk("%s: stripe_unit=%llu mirror_array_cnt=%u\n", __func__,\r\nstripe_unit, mirror_array_cnt);\r\nif (mirror_array_cnt > NFS4_FLEXFILE_LAYOUT_MAX_MIRROR_CNT ||\r\nmirror_array_cnt == 0)\r\ngoto out_err_free;\r\nrc = -ENOMEM;\r\nfls = kzalloc(sizeof(*fls), gfp_flags);\r\nif (!fls)\r\ngoto out_err_free;\r\nfls->mirror_array_cnt = mirror_array_cnt;\r\nfls->stripe_unit = stripe_unit;\r\nfls->mirror_array = kcalloc(fls->mirror_array_cnt,\r\nsizeof(fls->mirror_array[0]), gfp_flags);\r\nif (fls->mirror_array == NULL)\r\ngoto out_err_free;\r\nfor (i = 0; i < fls->mirror_array_cnt; i++) {\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nstruct nfs4_deviceid devid;\r\nstruct nfs4_deviceid_node *idnode;\r\nu32 ds_count;\r\nu32 fh_count;\r\nint j;\r\nrc = -EIO;\r\np = xdr_inline_decode(&stream, 4);\r\nif (!p)\r\ngoto out_err_free;\r\nds_count = be32_to_cpup(p);\r\nif (ds_count != 1)\r\ngoto out_err_free;\r\nfls->mirror_array[i] = ff_layout_alloc_mirror(gfp_flags);\r\nif (fls->mirror_array[i] == NULL) {\r\nrc = -ENOMEM;\r\ngoto out_err_free;\r\n}\r\nfls->mirror_array[i]->ds_count = ds_count;\r\nrc = decode_deviceid(&stream, &devid);\r\nif (rc)\r\ngoto out_err_free;\r\nidnode = nfs4_find_get_deviceid(NFS_SERVER(lh->plh_inode),\r\n&devid, lh->plh_lc_cred,\r\ngfp_flags);\r\nif (idnode)\r\nfls->mirror_array[i]->mirror_ds =\r\nFF_LAYOUT_MIRROR_DS(idnode);\r\nelse\r\ngoto out_err_free;\r\nrc = -EIO;\r\np = xdr_inline_decode(&stream, 4);\r\nif (!p)\r\ngoto out_err_free;\r\nfls->mirror_array[i]->efficiency = be32_to_cpup(p);\r\nrc = decode_stateid(&stream, &fls->mirror_array[i]->stateid);\r\nif (rc)\r\ngoto out_err_free;\r\np = xdr_inline_decode(&stream, 4);\r\nif (!p)\r\ngoto out_err_free;\r\nfh_count = be32_to_cpup(p);\r\nfls->mirror_array[i]->fh_versions =\r\nkzalloc(fh_count * sizeof(struct nfs_fh),\r\ngfp_flags);\r\nif (fls->mirror_array[i]->fh_versions == NULL) {\r\nrc = -ENOMEM;\r\ngoto out_err_free;\r\n}\r\nfor (j = 0; j < fh_count; j++) {\r\nrc = decode_nfs_fh(&stream,\r\n&fls->mirror_array[i]->fh_versions[j]);\r\nif (rc)\r\ngoto out_err_free;\r\n}\r\nfls->mirror_array[i]->fh_versions_cnt = fh_count;\r\nrc = decode_name(&stream, &fls->mirror_array[i]->uid);\r\nif (rc)\r\ngoto out_err_free;\r\nrc = decode_name(&stream, &fls->mirror_array[i]->gid);\r\nif (rc)\r\ngoto out_err_free;\r\nmirror = ff_layout_add_mirror(lh, fls->mirror_array[i]);\r\nif (mirror != fls->mirror_array[i]) {\r\nff_layout_free_mirror(fls->mirror_array[i]);\r\nfls->mirror_array[i] = mirror;\r\n}\r\ndprintk("%s: uid %d gid %d\n", __func__,\r\nfls->mirror_array[i]->uid,\r\nfls->mirror_array[i]->gid);\r\n}\r\np = xdr_inline_decode(&stream, 4);\r\nif (!p)\r\ngoto out_sort_mirrors;\r\nfls->flags = be32_to_cpup(p);\r\np = xdr_inline_decode(&stream, 4);\r\nif (!p)\r\ngoto out_sort_mirrors;\r\nfor (i=0; i < fls->mirror_array_cnt; i++)\r\nfls->mirror_array[i]->report_interval = be32_to_cpup(p);\r\nout_sort_mirrors:\r\nff_layout_sort_mirrors(fls);\r\nrc = ff_layout_check_layout(lgr);\r\nif (rc)\r\ngoto out_err_free;\r\nff_layout_mark_devices_valid(fls);\r\nret = &fls->generic_hdr;\r\ndprintk("<-- %s (success)\n", __func__);\r\nout_free_page:\r\n__free_page(scratch);\r\nreturn ret;\r\nout_err_free:\r\n_ff_layout_free_lseg(fls);\r\nret = ERR_PTR(rc);\r\ndprintk("<-- %s (%d)\n", __func__, rc);\r\ngoto out_free_page;\r\n}\r\nstatic bool ff_layout_has_rw_segments(struct pnfs_layout_hdr *layout)\r\n{\r\nstruct pnfs_layout_segment *lseg;\r\nlist_for_each_entry(lseg, &layout->plh_segs, pls_list)\r\nif (lseg->pls_range.iomode == IOMODE_RW)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void\r\nff_layout_free_lseg(struct pnfs_layout_segment *lseg)\r\n{\r\nstruct nfs4_ff_layout_segment *fls = FF_LAYOUT_LSEG(lseg);\r\ndprintk("--> %s\n", __func__);\r\nif (lseg->pls_range.iomode == IOMODE_RW) {\r\nstruct nfs4_flexfile_layout *ffl;\r\nstruct inode *inode;\r\nffl = FF_LAYOUT_FROM_HDR(lseg->pls_layout);\r\ninode = ffl->generic_hdr.plh_inode;\r\nspin_lock(&inode->i_lock);\r\nif (!ff_layout_has_rw_segments(lseg->pls_layout)) {\r\nffl->commit_info.nbuckets = 0;\r\nkfree(ffl->commit_info.buckets);\r\nffl->commit_info.buckets = NULL;\r\n}\r\nspin_unlock(&inode->i_lock);\r\n}\r\n_ff_layout_free_lseg(fls);\r\n}\r\nstatic int\r\nff_layout_get_lseg_count(struct nfs4_ff_layout_segment *fls)\r\n{\r\nreturn 1;\r\n}\r\nstatic void\r\nnfs4_ff_start_busy_timer(struct nfs4_ff_busy_timer *timer, ktime_t now)\r\n{\r\nif (atomic_inc_return(&timer->n_ops) == 1) {\r\ntimer->start_time = now;\r\n}\r\n}\r\nstatic ktime_t\r\nnfs4_ff_end_busy_timer(struct nfs4_ff_busy_timer *timer, ktime_t now)\r\n{\r\nktime_t start;\r\nif (atomic_dec_return(&timer->n_ops) < 0)\r\nWARN_ON_ONCE(1);\r\nstart = timer->start_time;\r\ntimer->start_time = now;\r\nreturn ktime_sub(now, start);\r\n}\r\nstatic bool\r\nnfs4_ff_layoutstat_start_io(struct nfs4_ff_layout_mirror *mirror,\r\nstruct nfs4_ff_layoutstat *layoutstat,\r\nktime_t now)\r\n{\r\nstatic const ktime_t notime = {0};\r\ns64 report_interval = FF_LAYOUTSTATS_REPORT_INTERVAL;\r\nnfs4_ff_start_busy_timer(&layoutstat->busy_timer, now);\r\nif (ktime_equal(mirror->start_time, notime))\r\nmirror->start_time = now;\r\nif (ktime_equal(mirror->last_report_time, notime))\r\nmirror->last_report_time = now;\r\nif (mirror->report_interval != 0)\r\nreport_interval = (s64)mirror->report_interval * 1000LL;\r\nelse if (layoutstats_timer != 0)\r\nreport_interval = (s64)layoutstats_timer * 1000LL;\r\nif (ktime_to_ms(ktime_sub(now, mirror->last_report_time)) >=\r\nreport_interval) {\r\nmirror->last_report_time = now;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void\r\nnfs4_ff_layout_stat_io_update_requested(struct nfs4_ff_layoutstat *layoutstat,\r\n__u64 requested)\r\n{\r\nstruct nfs4_ff_io_stat *iostat = &layoutstat->io_stat;\r\niostat->ops_requested++;\r\niostat->bytes_requested += requested;\r\n}\r\nstatic void\r\nnfs4_ff_layout_stat_io_update_completed(struct nfs4_ff_layoutstat *layoutstat,\r\n__u64 requested,\r\n__u64 completed,\r\nktime_t time_completed,\r\nktime_t time_started)\r\n{\r\nstruct nfs4_ff_io_stat *iostat = &layoutstat->io_stat;\r\nktime_t completion_time = ktime_sub(time_completed, time_started);\r\nktime_t timer;\r\niostat->ops_completed++;\r\niostat->bytes_completed += completed;\r\niostat->bytes_not_delivered += requested - completed;\r\ntimer = nfs4_ff_end_busy_timer(&layoutstat->busy_timer, time_completed);\r\niostat->total_busy_time =\r\nktime_add(iostat->total_busy_time, timer);\r\niostat->aggregate_completion_time =\r\nktime_add(iostat->aggregate_completion_time,\r\ncompletion_time);\r\n}\r\nstatic void\r\nnfs4_ff_layout_stat_io_start_read(struct inode *inode,\r\nstruct nfs4_ff_layout_mirror *mirror,\r\n__u64 requested, ktime_t now)\r\n{\r\nbool report;\r\nspin_lock(&mirror->lock);\r\nreport = nfs4_ff_layoutstat_start_io(mirror, &mirror->read_stat, now);\r\nnfs4_ff_layout_stat_io_update_requested(&mirror->read_stat, requested);\r\nspin_unlock(&mirror->lock);\r\nif (report)\r\npnfs_report_layoutstat(inode, GFP_KERNEL);\r\n}\r\nstatic void\r\nnfs4_ff_layout_stat_io_end_read(struct rpc_task *task,\r\nstruct nfs4_ff_layout_mirror *mirror,\r\n__u64 requested,\r\n__u64 completed)\r\n{\r\nspin_lock(&mirror->lock);\r\nnfs4_ff_layout_stat_io_update_completed(&mirror->read_stat,\r\nrequested, completed,\r\nktime_get(), task->tk_start);\r\nspin_unlock(&mirror->lock);\r\n}\r\nstatic void\r\nnfs4_ff_layout_stat_io_start_write(struct inode *inode,\r\nstruct nfs4_ff_layout_mirror *mirror,\r\n__u64 requested, ktime_t now)\r\n{\r\nbool report;\r\nspin_lock(&mirror->lock);\r\nreport = nfs4_ff_layoutstat_start_io(mirror , &mirror->write_stat, now);\r\nnfs4_ff_layout_stat_io_update_requested(&mirror->write_stat, requested);\r\nspin_unlock(&mirror->lock);\r\nif (report)\r\npnfs_report_layoutstat(inode, GFP_NOIO);\r\n}\r\nstatic void\r\nnfs4_ff_layout_stat_io_end_write(struct rpc_task *task,\r\nstruct nfs4_ff_layout_mirror *mirror,\r\n__u64 requested,\r\n__u64 completed,\r\nenum nfs3_stable_how committed)\r\n{\r\nif (committed == NFS_UNSTABLE)\r\nrequested = completed = 0;\r\nspin_lock(&mirror->lock);\r\nnfs4_ff_layout_stat_io_update_completed(&mirror->write_stat,\r\nrequested, completed, ktime_get(), task->tk_start);\r\nspin_unlock(&mirror->lock);\r\n}\r\nstatic int\r\nff_layout_alloc_commit_info(struct pnfs_layout_segment *lseg,\r\nstruct nfs_commit_info *cinfo,\r\ngfp_t gfp_flags)\r\n{\r\nstruct nfs4_ff_layout_segment *fls = FF_LAYOUT_LSEG(lseg);\r\nstruct pnfs_commit_bucket *buckets;\r\nint size;\r\nif (cinfo->ds->nbuckets != 0) {\r\nreturn 0;\r\n}\r\nsize = ff_layout_get_lseg_count(fls) * FF_LAYOUT_MIRROR_COUNT(lseg);\r\nbuckets = kcalloc(size, sizeof(struct pnfs_commit_bucket),\r\ngfp_flags);\r\nif (!buckets)\r\nreturn -ENOMEM;\r\nelse {\r\nint i;\r\nspin_lock(cinfo->lock);\r\nif (cinfo->ds->nbuckets != 0)\r\nkfree(buckets);\r\nelse {\r\ncinfo->ds->buckets = buckets;\r\ncinfo->ds->nbuckets = size;\r\nfor (i = 0; i < size; i++) {\r\nINIT_LIST_HEAD(&buckets[i].written);\r\nINIT_LIST_HEAD(&buckets[i].committing);\r\nbuckets[i].direct_verf.committed =\r\nNFS_INVALID_STABLE_HOW;\r\n}\r\n}\r\nspin_unlock(cinfo->lock);\r\nreturn 0;\r\n}\r\n}\r\nstatic struct nfs4_pnfs_ds *\r\nff_layout_choose_best_ds_for_read(struct pnfs_layout_segment *lseg,\r\nint start_idx,\r\nint *best_idx)\r\n{\r\nstruct nfs4_ff_layout_segment *fls = FF_LAYOUT_LSEG(lseg);\r\nstruct nfs4_pnfs_ds *ds;\r\nint idx;\r\nfor (idx = start_idx; idx < fls->mirror_array_cnt; idx++) {\r\nds = nfs4_ff_layout_prepare_ds(lseg, idx, false);\r\nif (ds) {\r\n*best_idx = idx;\r\nreturn ds;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void\r\nff_layout_pg_init_read(struct nfs_pageio_descriptor *pgio,\r\nstruct nfs_page *req)\r\n{\r\nstruct nfs_pgio_mirror *pgm;\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nstruct nfs4_pnfs_ds *ds;\r\nint ds_idx;\r\nif (!pgio->pg_lseg) {\r\npgio->pg_lseg = pnfs_update_layout(pgio->pg_inode,\r\nreq->wb_context,\r\n0,\r\nNFS4_MAX_UINT64,\r\nIOMODE_READ,\r\nGFP_KERNEL);\r\nif (IS_ERR(pgio->pg_lseg)) {\r\npgio->pg_error = PTR_ERR(pgio->pg_lseg);\r\npgio->pg_lseg = NULL;\r\nreturn;\r\n}\r\n}\r\nif (pgio->pg_lseg == NULL)\r\ngoto out_mds;\r\nds = ff_layout_choose_best_ds_for_read(pgio->pg_lseg, 0, &ds_idx);\r\nif (!ds)\r\ngoto out_mds;\r\nmirror = FF_LAYOUT_COMP(pgio->pg_lseg, ds_idx);\r\npgio->pg_mirror_idx = ds_idx;\r\npgm = &pgio->pg_mirrors[0];\r\npgm->pg_bsize = mirror->mirror_ds->ds_versions[0].rsize;\r\nreturn;\r\nout_mds:\r\npnfs_put_lseg(pgio->pg_lseg);\r\npgio->pg_lseg = NULL;\r\nnfs_pageio_reset_read_mds(pgio);\r\n}\r\nstatic void\r\nff_layout_pg_init_write(struct nfs_pageio_descriptor *pgio,\r\nstruct nfs_page *req)\r\n{\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nstruct nfs_pgio_mirror *pgm;\r\nstruct nfs_commit_info cinfo;\r\nstruct nfs4_pnfs_ds *ds;\r\nint i;\r\nint status;\r\nif (!pgio->pg_lseg) {\r\npgio->pg_lseg = pnfs_update_layout(pgio->pg_inode,\r\nreq->wb_context,\r\n0,\r\nNFS4_MAX_UINT64,\r\nIOMODE_RW,\r\nGFP_NOFS);\r\nif (IS_ERR(pgio->pg_lseg)) {\r\npgio->pg_error = PTR_ERR(pgio->pg_lseg);\r\npgio->pg_lseg = NULL;\r\nreturn;\r\n}\r\n}\r\nif (pgio->pg_lseg == NULL)\r\ngoto out_mds;\r\nnfs_init_cinfo(&cinfo, pgio->pg_inode, pgio->pg_dreq);\r\nstatus = ff_layout_alloc_commit_info(pgio->pg_lseg, &cinfo, GFP_NOFS);\r\nif (status < 0)\r\ngoto out_mds;\r\nif (WARN_ON_ONCE(pgio->pg_mirror_count !=\r\nFF_LAYOUT_MIRROR_COUNT(pgio->pg_lseg)))\r\ngoto out_mds;\r\nfor (i = 0; i < pgio->pg_mirror_count; i++) {\r\nds = nfs4_ff_layout_prepare_ds(pgio->pg_lseg, i, true);\r\nif (!ds)\r\ngoto out_mds;\r\npgm = &pgio->pg_mirrors[i];\r\nmirror = FF_LAYOUT_COMP(pgio->pg_lseg, i);\r\npgm->pg_bsize = mirror->mirror_ds->ds_versions[0].wsize;\r\n}\r\nreturn;\r\nout_mds:\r\npnfs_put_lseg(pgio->pg_lseg);\r\npgio->pg_lseg = NULL;\r\nnfs_pageio_reset_write_mds(pgio);\r\n}\r\nstatic unsigned int\r\nff_layout_pg_get_mirror_count_write(struct nfs_pageio_descriptor *pgio,\r\nstruct nfs_page *req)\r\n{\r\nif (!pgio->pg_lseg) {\r\npgio->pg_lseg = pnfs_update_layout(pgio->pg_inode,\r\nreq->wb_context,\r\n0,\r\nNFS4_MAX_UINT64,\r\nIOMODE_RW,\r\nGFP_NOFS);\r\nif (IS_ERR(pgio->pg_lseg)) {\r\npgio->pg_error = PTR_ERR(pgio->pg_lseg);\r\npgio->pg_lseg = NULL;\r\ngoto out;\r\n}\r\n}\r\nif (pgio->pg_lseg)\r\nreturn FF_LAYOUT_MIRROR_COUNT(pgio->pg_lseg);\r\nnfs_pageio_reset_write_mds(pgio);\r\nout:\r\nreturn 1;\r\n}\r\nstatic void ff_layout_reset_write(struct nfs_pgio_header *hdr, bool retry_pnfs)\r\n{\r\nstruct rpc_task *task = &hdr->task;\r\npnfs_layoutcommit_inode(hdr->inode, false);\r\nif (retry_pnfs) {\r\ndprintk("%s Reset task %5u for i/o through pNFS "\r\n"(req %s/%llu, %u bytes @ offset %llu)\n", __func__,\r\nhdr->task.tk_pid,\r\nhdr->inode->i_sb->s_id,\r\n(unsigned long long)NFS_FILEID(hdr->inode),\r\nhdr->args.count,\r\n(unsigned long long)hdr->args.offset);\r\nhdr->completion_ops->reschedule_io(hdr);\r\nreturn;\r\n}\r\nif (!test_and_set_bit(NFS_IOHDR_REDO, &hdr->flags)) {\r\ndprintk("%s Reset task %5u for i/o through MDS "\r\n"(req %s/%llu, %u bytes @ offset %llu)\n", __func__,\r\nhdr->task.tk_pid,\r\nhdr->inode->i_sb->s_id,\r\n(unsigned long long)NFS_FILEID(hdr->inode),\r\nhdr->args.count,\r\n(unsigned long long)hdr->args.offset);\r\ntask->tk_status = pnfs_write_done_resend_to_mds(hdr);\r\n}\r\n}\r\nstatic void ff_layout_reset_read(struct nfs_pgio_header *hdr)\r\n{\r\nstruct rpc_task *task = &hdr->task;\r\npnfs_layoutcommit_inode(hdr->inode, false);\r\nif (!test_and_set_bit(NFS_IOHDR_REDO, &hdr->flags)) {\r\ndprintk("%s Reset task %5u for i/o through MDS "\r\n"(req %s/%llu, %u bytes @ offset %llu)\n", __func__,\r\nhdr->task.tk_pid,\r\nhdr->inode->i_sb->s_id,\r\n(unsigned long long)NFS_FILEID(hdr->inode),\r\nhdr->args.count,\r\n(unsigned long long)hdr->args.offset);\r\ntask->tk_status = pnfs_read_done_resend_to_mds(hdr);\r\n}\r\n}\r\nstatic int ff_layout_async_handle_error_v4(struct rpc_task *task,\r\nstruct nfs4_state *state,\r\nstruct nfs_client *clp,\r\nstruct pnfs_layout_segment *lseg,\r\nint idx)\r\n{\r\nstruct pnfs_layout_hdr *lo = lseg->pls_layout;\r\nstruct inode *inode = lo->plh_inode;\r\nstruct nfs_server *mds_server = NFS_SERVER(inode);\r\nstruct nfs4_deviceid_node *devid = FF_LAYOUT_DEVID_NODE(lseg, idx);\r\nstruct nfs_client *mds_client = mds_server->nfs_client;\r\nstruct nfs4_slot_table *tbl = &clp->cl_session->fc_slot_table;\r\nif (task->tk_status >= 0)\r\nreturn 0;\r\nswitch (task->tk_status) {\r\ncase -NFS4ERR_DELEG_REVOKED:\r\ncase -NFS4ERR_ADMIN_REVOKED:\r\ncase -NFS4ERR_BAD_STATEID:\r\nif (state == NULL)\r\nbreak;\r\nnfs_remove_bad_delegation(state->inode);\r\ncase -NFS4ERR_OPENMODE:\r\nif (state == NULL)\r\nbreak;\r\nif (nfs4_schedule_stateid_recovery(mds_server, state) < 0)\r\ngoto out_bad_stateid;\r\ngoto wait_on_recovery;\r\ncase -NFS4ERR_EXPIRED:\r\nif (state != NULL) {\r\nif (nfs4_schedule_stateid_recovery(mds_server, state) < 0)\r\ngoto out_bad_stateid;\r\n}\r\nnfs4_schedule_lease_recovery(mds_client);\r\ngoto wait_on_recovery;\r\ncase -NFS4ERR_BADSESSION:\r\ncase -NFS4ERR_BADSLOT:\r\ncase -NFS4ERR_BAD_HIGH_SLOT:\r\ncase -NFS4ERR_DEADSESSION:\r\ncase -NFS4ERR_CONN_NOT_BOUND_TO_SESSION:\r\ncase -NFS4ERR_SEQ_FALSE_RETRY:\r\ncase -NFS4ERR_SEQ_MISORDERED:\r\ndprintk("%s ERROR %d, Reset session. Exchangeid "\r\n"flags 0x%x\n", __func__, task->tk_status,\r\nclp->cl_exchange_flags);\r\nnfs4_schedule_session_recovery(clp->cl_session, task->tk_status);\r\nbreak;\r\ncase -NFS4ERR_DELAY:\r\ncase -NFS4ERR_GRACE:\r\nrpc_delay(task, FF_LAYOUT_POLL_RETRY_MAX);\r\nbreak;\r\ncase -NFS4ERR_RETRY_UNCACHED_REP:\r\nbreak;\r\ncase -NFS4ERR_PNFS_NO_LAYOUT:\r\ncase -ESTALE:\r\ncase -EBADHANDLE:\r\ncase -EISDIR:\r\ncase -NFS4ERR_FHEXPIRED:\r\ncase -NFS4ERR_WRONG_TYPE:\r\ndprintk("%s Invalid layout error %d\n", __func__,\r\ntask->tk_status);\r\npnfs_destroy_layout(NFS_I(inode));\r\nrpc_wake_up(&tbl->slot_tbl_waitq);\r\ngoto reset;\r\ncase -ECONNREFUSED:\r\ncase -EHOSTDOWN:\r\ncase -EHOSTUNREACH:\r\ncase -ENETUNREACH:\r\ncase -EIO:\r\ncase -ETIMEDOUT:\r\ncase -EPIPE:\r\ndprintk("%s DS connection error %d\n", __func__,\r\ntask->tk_status);\r\nnfs4_mark_deviceid_unavailable(devid);\r\nrpc_wake_up(&tbl->slot_tbl_waitq);\r\ndefault:\r\nif (ff_layout_no_fallback_to_mds(lseg) ||\r\nff_layout_has_available_ds(lseg))\r\nreturn -NFS4ERR_RESET_TO_PNFS;\r\nreset:\r\ndprintk("%s Retry through MDS. Error %d\n", __func__,\r\ntask->tk_status);\r\nreturn -NFS4ERR_RESET_TO_MDS;\r\n}\r\nout:\r\ntask->tk_status = 0;\r\nreturn -EAGAIN;\r\nout_bad_stateid:\r\ntask->tk_status = -EIO;\r\nreturn 0;\r\nwait_on_recovery:\r\nrpc_sleep_on(&mds_client->cl_rpcwaitq, task, NULL);\r\nif (test_bit(NFS4CLNT_MANAGER_RUNNING, &mds_client->cl_state) == 0)\r\nrpc_wake_up_queued_task(&mds_client->cl_rpcwaitq, task);\r\ngoto out;\r\n}\r\nstatic int ff_layout_async_handle_error_v3(struct rpc_task *task,\r\nstruct pnfs_layout_segment *lseg,\r\nint idx)\r\n{\r\nstruct nfs4_deviceid_node *devid = FF_LAYOUT_DEVID_NODE(lseg, idx);\r\nif (task->tk_status >= 0)\r\nreturn 0;\r\nswitch (task->tk_status) {\r\ncase -EACCES:\r\ncase -ESTALE:\r\ncase -EISDIR:\r\ncase -EBADHANDLE:\r\ncase -ELOOP:\r\ncase -ENOSPC:\r\nbreak;\r\ncase -EJUKEBOX:\r\nnfs_inc_stats(lseg->pls_layout->plh_inode, NFSIOS_DELAY);\r\ngoto out_retry;\r\ndefault:\r\ndprintk("%s DS connection error %d\n", __func__,\r\ntask->tk_status);\r\nnfs4_mark_deviceid_unavailable(devid);\r\n}\r\nreturn -NFS4ERR_RESET_TO_PNFS;\r\nout_retry:\r\ntask->tk_status = 0;\r\nrpc_restart_call_prepare(task);\r\nrpc_delay(task, NFS_JUKEBOX_RETRY_TIME);\r\nreturn -EAGAIN;\r\n}\r\nstatic int ff_layout_async_handle_error(struct rpc_task *task,\r\nstruct nfs4_state *state,\r\nstruct nfs_client *clp,\r\nstruct pnfs_layout_segment *lseg,\r\nint idx)\r\n{\r\nint vers = clp->cl_nfs_mod->rpc_vers->number;\r\nswitch (vers) {\r\ncase 3:\r\nreturn ff_layout_async_handle_error_v3(task, lseg, idx);\r\ncase 4:\r\nreturn ff_layout_async_handle_error_v4(task, state, clp,\r\nlseg, idx);\r\ndefault:\r\nWARN_ON_ONCE(1);\r\nreturn 0;\r\n}\r\n}\r\nstatic void ff_layout_io_track_ds_error(struct pnfs_layout_segment *lseg,\r\nint idx, u64 offset, u64 length,\r\nu32 status, int opnum, int error)\r\n{\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nint err;\r\nif (status == 0) {\r\nswitch (error) {\r\ncase -ETIMEDOUT:\r\ncase -EPFNOSUPPORT:\r\ncase -EPROTONOSUPPORT:\r\ncase -EOPNOTSUPP:\r\ncase -ECONNREFUSED:\r\ncase -ECONNRESET:\r\ncase -EHOSTDOWN:\r\ncase -EHOSTUNREACH:\r\ncase -ENETUNREACH:\r\ncase -EADDRINUSE:\r\ncase -ENOBUFS:\r\ncase -EPIPE:\r\ncase -EPERM:\r\nstatus = NFS4ERR_NXIO;\r\nbreak;\r\ncase -EACCES:\r\nstatus = NFS4ERR_ACCESS;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\n}\r\nswitch (status) {\r\ncase NFS4ERR_DELAY:\r\ncase NFS4ERR_GRACE:\r\nreturn;\r\ndefault:\r\nbreak;\r\n}\r\nmirror = FF_LAYOUT_COMP(lseg, idx);\r\nerr = ff_layout_track_ds_error(FF_LAYOUT_FROM_HDR(lseg->pls_layout),\r\nmirror, offset, length, status, opnum,\r\nGFP_NOIO);\r\npnfs_error_mark_layout_for_return(lseg->pls_layout->plh_inode, lseg);\r\ndprintk("%s: err %d op %d status %u\n", __func__, err, opnum, status);\r\n}\r\nstatic int ff_layout_read_done_cb(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nint err;\r\ntrace_nfs4_pnfs_read(hdr, task->tk_status);\r\nif (task->tk_status < 0)\r\nff_layout_io_track_ds_error(hdr->lseg, hdr->pgio_mirror_idx,\r\nhdr->args.offset, hdr->args.count,\r\nhdr->res.op_status, OP_READ,\r\ntask->tk_status);\r\nerr = ff_layout_async_handle_error(task, hdr->args.context->state,\r\nhdr->ds_clp, hdr->lseg,\r\nhdr->pgio_mirror_idx);\r\nswitch (err) {\r\ncase -NFS4ERR_RESET_TO_PNFS:\r\nif (ff_layout_choose_best_ds_for_read(hdr->lseg,\r\nhdr->pgio_mirror_idx + 1,\r\n&hdr->pgio_mirror_idx))\r\ngoto out_eagain;\r\nset_bit(NFS_LAYOUT_RETURN_REQUESTED,\r\n&hdr->lseg->pls_layout->plh_flags);\r\npnfs_read_resend_pnfs(hdr);\r\nreturn task->tk_status;\r\ncase -NFS4ERR_RESET_TO_MDS:\r\nff_layout_reset_read(hdr);\r\nreturn task->tk_status;\r\ncase -EAGAIN:\r\ngoto out_eagain;\r\n}\r\nreturn 0;\r\nout_eagain:\r\nrpc_restart_call_prepare(task);\r\nreturn -EAGAIN;\r\n}\r\nstatic bool\r\nff_layout_need_layoutcommit(struct pnfs_layout_segment *lseg)\r\n{\r\nreturn !(FF_LAYOUT_LSEG(lseg)->flags & FF_FLAGS_NO_LAYOUTCOMMIT);\r\n}\r\nstatic void\r\nff_layout_set_layoutcommit(struct nfs_pgio_header *hdr)\r\n{\r\nif (!ff_layout_need_layoutcommit(hdr->lseg))\r\nreturn;\r\npnfs_set_layoutcommit(hdr->inode, hdr->lseg,\r\nhdr->mds_offset + hdr->res.count);\r\ndprintk("%s inode %lu pls_end_pos %lu\n", __func__, hdr->inode->i_ino,\r\n(unsigned long) NFS_I(hdr->inode)->layout->plh_lwb);\r\n}\r\nstatic bool\r\nff_layout_reset_to_mds(struct pnfs_layout_segment *lseg, int idx)\r\n{\r\nstruct nfs4_deviceid_node *node = FF_LAYOUT_DEVID_NODE(lseg, idx);\r\nreturn ff_layout_test_devid_unavailable(node);\r\n}\r\nstatic void ff_layout_read_record_layoutstats_start(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nif (test_and_set_bit(NFS_IOHDR_STAT, &hdr->flags))\r\nreturn;\r\nnfs4_ff_layout_stat_io_start_read(hdr->inode,\r\nFF_LAYOUT_COMP(hdr->lseg, hdr->pgio_mirror_idx),\r\nhdr->args.count,\r\ntask->tk_start);\r\n}\r\nstatic void ff_layout_read_record_layoutstats_done(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nif (!test_and_clear_bit(NFS_IOHDR_STAT, &hdr->flags))\r\nreturn;\r\nnfs4_ff_layout_stat_io_end_read(task,\r\nFF_LAYOUT_COMP(hdr->lseg, hdr->pgio_mirror_idx),\r\nhdr->args.count,\r\nhdr->res.count);\r\n}\r\nstatic int ff_layout_read_prepare_common(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nif (unlikely(test_bit(NFS_CONTEXT_BAD, &hdr->args.context->flags))) {\r\nrpc_exit(task, -EIO);\r\nreturn -EIO;\r\n}\r\nif (ff_layout_reset_to_mds(hdr->lseg, hdr->pgio_mirror_idx)) {\r\ndprintk("%s task %u reset io to MDS\n", __func__, task->tk_pid);\r\nif (ff_layout_has_available_ds(hdr->lseg))\r\npnfs_read_resend_pnfs(hdr);\r\nelse\r\nff_layout_reset_read(hdr);\r\nrpc_exit(task, 0);\r\nreturn -EAGAIN;\r\n}\r\nhdr->pgio_done_cb = ff_layout_read_done_cb;\r\nff_layout_read_record_layoutstats_start(task, hdr);\r\nreturn 0;\r\n}\r\nstatic void ff_layout_read_prepare_v3(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nif (ff_layout_read_prepare_common(task, hdr))\r\nreturn;\r\nrpc_call_start(task);\r\n}\r\nstatic int ff_layout_setup_sequence(struct nfs_client *ds_clp,\r\nstruct nfs4_sequence_args *args,\r\nstruct nfs4_sequence_res *res,\r\nstruct rpc_task *task)\r\n{\r\nif (ds_clp->cl_session)\r\nreturn nfs41_setup_sequence(ds_clp->cl_session,\r\nargs,\r\nres,\r\ntask);\r\nreturn nfs40_setup_sequence(ds_clp->cl_slot_tbl,\r\nargs,\r\nres,\r\ntask);\r\n}\r\nstatic void ff_layout_read_prepare_v4(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nif (ff_layout_setup_sequence(hdr->ds_clp,\r\n&hdr->args.seq_args,\r\n&hdr->res.seq_res,\r\ntask))\r\nreturn;\r\nif (ff_layout_read_prepare_common(task, hdr))\r\nreturn;\r\nif (nfs4_set_rw_stateid(&hdr->args.stateid, hdr->args.context,\r\nhdr->args.lock_context, FMODE_READ) == -EIO)\r\nrpc_exit(task, -EIO);\r\n}\r\nstatic void ff_layout_read_call_done(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\ndprintk("--> %s task->tk_status %d\n", __func__, task->tk_status);\r\nif (test_bit(NFS_IOHDR_REDO, &hdr->flags) &&\r\ntask->tk_status == 0) {\r\nnfs4_sequence_done(task, &hdr->res.seq_res);\r\nreturn;\r\n}\r\nhdr->mds_ops->rpc_call_done(task, hdr);\r\n}\r\nstatic void ff_layout_read_count_stats(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nff_layout_read_record_layoutstats_done(task, hdr);\r\nrpc_count_iostats_metrics(task,\r\n&NFS_CLIENT(hdr->inode)->cl_metrics[NFSPROC4_CLNT_READ]);\r\n}\r\nstatic void ff_layout_read_release(void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nff_layout_read_record_layoutstats_done(&hdr->task, hdr);\r\npnfs_generic_rw_release(data);\r\n}\r\nstatic int ff_layout_write_done_cb(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nint err;\r\ntrace_nfs4_pnfs_write(hdr, task->tk_status);\r\nif (task->tk_status < 0)\r\nff_layout_io_track_ds_error(hdr->lseg, hdr->pgio_mirror_idx,\r\nhdr->args.offset, hdr->args.count,\r\nhdr->res.op_status, OP_WRITE,\r\ntask->tk_status);\r\nerr = ff_layout_async_handle_error(task, hdr->args.context->state,\r\nhdr->ds_clp, hdr->lseg,\r\nhdr->pgio_mirror_idx);\r\nswitch (err) {\r\ncase -NFS4ERR_RESET_TO_PNFS:\r\nff_layout_reset_write(hdr, true);\r\nreturn task->tk_status;\r\ncase -NFS4ERR_RESET_TO_MDS:\r\nff_layout_reset_write(hdr, false);\r\nreturn task->tk_status;\r\ncase -EAGAIN:\r\nreturn -EAGAIN;\r\n}\r\nif (hdr->res.verf->committed == NFS_FILE_SYNC ||\r\nhdr->res.verf->committed == NFS_DATA_SYNC)\r\nff_layout_set_layoutcommit(hdr);\r\nhdr->fattr.valid = 0;\r\nif (task->tk_status >= 0)\r\nnfs_writeback_update_inode(hdr);\r\nreturn 0;\r\n}\r\nstatic int ff_layout_commit_done_cb(struct rpc_task *task,\r\nstruct nfs_commit_data *data)\r\n{\r\nint err;\r\ntrace_nfs4_pnfs_commit_ds(data, task->tk_status);\r\nif (task->tk_status < 0)\r\nff_layout_io_track_ds_error(data->lseg, data->ds_commit_index,\r\ndata->args.offset, data->args.count,\r\ndata->res.op_status, OP_COMMIT,\r\ntask->tk_status);\r\nerr = ff_layout_async_handle_error(task, NULL, data->ds_clp,\r\ndata->lseg, data->ds_commit_index);\r\nswitch (err) {\r\ncase -NFS4ERR_RESET_TO_PNFS:\r\npnfs_generic_prepare_to_resend_writes(data);\r\nreturn -EAGAIN;\r\ncase -NFS4ERR_RESET_TO_MDS:\r\npnfs_generic_prepare_to_resend_writes(data);\r\nreturn -EAGAIN;\r\ncase -EAGAIN:\r\nrpc_restart_call_prepare(task);\r\nreturn -EAGAIN;\r\n}\r\nif (data->verf.committed == NFS_UNSTABLE\r\n&& ff_layout_need_layoutcommit(data->lseg))\r\npnfs_set_layoutcommit(data->inode, data->lseg, data->lwb);\r\nreturn 0;\r\n}\r\nstatic void ff_layout_write_record_layoutstats_start(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nif (test_and_set_bit(NFS_IOHDR_STAT, &hdr->flags))\r\nreturn;\r\nnfs4_ff_layout_stat_io_start_write(hdr->inode,\r\nFF_LAYOUT_COMP(hdr->lseg, hdr->pgio_mirror_idx),\r\nhdr->args.count,\r\ntask->tk_start);\r\n}\r\nstatic void ff_layout_write_record_layoutstats_done(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nif (!test_and_clear_bit(NFS_IOHDR_STAT, &hdr->flags))\r\nreturn;\r\nnfs4_ff_layout_stat_io_end_write(task,\r\nFF_LAYOUT_COMP(hdr->lseg, hdr->pgio_mirror_idx),\r\nhdr->args.count, hdr->res.count,\r\nhdr->res.verf->committed);\r\n}\r\nstatic int ff_layout_write_prepare_common(struct rpc_task *task,\r\nstruct nfs_pgio_header *hdr)\r\n{\r\nif (unlikely(test_bit(NFS_CONTEXT_BAD, &hdr->args.context->flags))) {\r\nrpc_exit(task, -EIO);\r\nreturn -EIO;\r\n}\r\nif (ff_layout_reset_to_mds(hdr->lseg, hdr->pgio_mirror_idx)) {\r\nbool retry_pnfs;\r\nretry_pnfs = ff_layout_has_available_ds(hdr->lseg);\r\ndprintk("%s task %u reset io to %s\n", __func__,\r\ntask->tk_pid, retry_pnfs ? "pNFS" : "MDS");\r\nff_layout_reset_write(hdr, retry_pnfs);\r\nrpc_exit(task, 0);\r\nreturn -EAGAIN;\r\n}\r\nff_layout_write_record_layoutstats_start(task, hdr);\r\nreturn 0;\r\n}\r\nstatic void ff_layout_write_prepare_v3(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nif (ff_layout_write_prepare_common(task, hdr))\r\nreturn;\r\nrpc_call_start(task);\r\n}\r\nstatic void ff_layout_write_prepare_v4(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nif (ff_layout_setup_sequence(hdr->ds_clp,\r\n&hdr->args.seq_args,\r\n&hdr->res.seq_res,\r\ntask))\r\nreturn;\r\nif (ff_layout_write_prepare_common(task, hdr))\r\nreturn;\r\nif (nfs4_set_rw_stateid(&hdr->args.stateid, hdr->args.context,\r\nhdr->args.lock_context, FMODE_WRITE) == -EIO)\r\nrpc_exit(task, -EIO);\r\n}\r\nstatic void ff_layout_write_call_done(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nif (test_bit(NFS_IOHDR_REDO, &hdr->flags) &&\r\ntask->tk_status == 0) {\r\nnfs4_sequence_done(task, &hdr->res.seq_res);\r\nreturn;\r\n}\r\nhdr->mds_ops->rpc_call_done(task, hdr);\r\n}\r\nstatic void ff_layout_write_count_stats(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nff_layout_write_record_layoutstats_done(task, hdr);\r\nrpc_count_iostats_metrics(task,\r\n&NFS_CLIENT(hdr->inode)->cl_metrics[NFSPROC4_CLNT_WRITE]);\r\n}\r\nstatic void ff_layout_write_release(void *data)\r\n{\r\nstruct nfs_pgio_header *hdr = data;\r\nff_layout_write_record_layoutstats_done(&hdr->task, hdr);\r\npnfs_generic_rw_release(data);\r\n}\r\nstatic void ff_layout_commit_record_layoutstats_start(struct rpc_task *task,\r\nstruct nfs_commit_data *cdata)\r\n{\r\nif (test_and_set_bit(NFS_IOHDR_STAT, &cdata->flags))\r\nreturn;\r\nnfs4_ff_layout_stat_io_start_write(cdata->inode,\r\nFF_LAYOUT_COMP(cdata->lseg, cdata->ds_commit_index),\r\n0, task->tk_start);\r\n}\r\nstatic void ff_layout_commit_record_layoutstats_done(struct rpc_task *task,\r\nstruct nfs_commit_data *cdata)\r\n{\r\nstruct nfs_page *req;\r\n__u64 count = 0;\r\nif (!test_and_clear_bit(NFS_IOHDR_STAT, &cdata->flags))\r\nreturn;\r\nif (task->tk_status == 0) {\r\nlist_for_each_entry(req, &cdata->pages, wb_list)\r\ncount += req->wb_bytes;\r\n}\r\nnfs4_ff_layout_stat_io_end_write(task,\r\nFF_LAYOUT_COMP(cdata->lseg, cdata->ds_commit_index),\r\ncount, count, NFS_FILE_SYNC);\r\n}\r\nstatic void ff_layout_commit_prepare_common(struct rpc_task *task,\r\nstruct nfs_commit_data *cdata)\r\n{\r\nff_layout_commit_record_layoutstats_start(task, cdata);\r\n}\r\nstatic void ff_layout_commit_prepare_v3(struct rpc_task *task, void *data)\r\n{\r\nff_layout_commit_prepare_common(task, data);\r\nrpc_call_start(task);\r\n}\r\nstatic void ff_layout_commit_prepare_v4(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_commit_data *wdata = data;\r\nif (ff_layout_setup_sequence(wdata->ds_clp,\r\n&wdata->args.seq_args,\r\n&wdata->res.seq_res,\r\ntask))\r\nreturn;\r\nff_layout_commit_prepare_common(task, data);\r\n}\r\nstatic void ff_layout_commit_done(struct rpc_task *task, void *data)\r\n{\r\npnfs_generic_write_commit_done(task, data);\r\n}\r\nstatic void ff_layout_commit_count_stats(struct rpc_task *task, void *data)\r\n{\r\nstruct nfs_commit_data *cdata = data;\r\nff_layout_commit_record_layoutstats_done(task, cdata);\r\nrpc_count_iostats_metrics(task,\r\n&NFS_CLIENT(cdata->inode)->cl_metrics[NFSPROC4_CLNT_COMMIT]);\r\n}\r\nstatic void ff_layout_commit_release(void *data)\r\n{\r\nstruct nfs_commit_data *cdata = data;\r\nff_layout_commit_record_layoutstats_done(&cdata->task, cdata);\r\npnfs_generic_commit_release(data);\r\n}\r\nstatic enum pnfs_try_status\r\nff_layout_read_pagelist(struct nfs_pgio_header *hdr)\r\n{\r\nstruct pnfs_layout_segment *lseg = hdr->lseg;\r\nstruct nfs4_pnfs_ds *ds;\r\nstruct rpc_clnt *ds_clnt;\r\nstruct rpc_cred *ds_cred;\r\nloff_t offset = hdr->args.offset;\r\nu32 idx = hdr->pgio_mirror_idx;\r\nint vers;\r\nstruct nfs_fh *fh;\r\ndprintk("--> %s ino %lu pgbase %u req %Zu@%llu\n",\r\n__func__, hdr->inode->i_ino,\r\nhdr->args.pgbase, (size_t)hdr->args.count, offset);\r\nds = nfs4_ff_layout_prepare_ds(lseg, idx, false);\r\nif (!ds)\r\ngoto out_failed;\r\nds_clnt = nfs4_ff_find_or_create_ds_client(lseg, idx, ds->ds_clp,\r\nhdr->inode);\r\nif (IS_ERR(ds_clnt))\r\ngoto out_failed;\r\nds_cred = ff_layout_get_ds_cred(lseg, idx, hdr->cred);\r\nif (IS_ERR(ds_cred))\r\ngoto out_failed;\r\nvers = nfs4_ff_layout_ds_version(lseg, idx);\r\ndprintk("%s USE DS: %s cl_count %d vers %d\n", __func__,\r\nds->ds_remotestr, atomic_read(&ds->ds_clp->cl_count), vers);\r\natomic_inc(&ds->ds_clp->cl_count);\r\nhdr->ds_clp = ds->ds_clp;\r\nfh = nfs4_ff_layout_select_ds_fh(lseg, idx);\r\nif (fh)\r\nhdr->args.fh = fh;\r\nhdr->args.offset = offset;\r\nhdr->mds_offset = offset;\r\nnfs_initiate_pgio(ds_clnt, hdr, ds_cred, ds->ds_clp->rpc_ops,\r\nvers == 3 ? &ff_layout_read_call_ops_v3 :\r\n&ff_layout_read_call_ops_v4,\r\n0, RPC_TASK_SOFTCONN);\r\nreturn PNFS_ATTEMPTED;\r\nout_failed:\r\nif (ff_layout_has_available_ds(lseg))\r\nreturn PNFS_TRY_AGAIN;\r\nreturn PNFS_NOT_ATTEMPTED;\r\n}\r\nstatic enum pnfs_try_status\r\nff_layout_write_pagelist(struct nfs_pgio_header *hdr, int sync)\r\n{\r\nstruct pnfs_layout_segment *lseg = hdr->lseg;\r\nstruct nfs4_pnfs_ds *ds;\r\nstruct rpc_clnt *ds_clnt;\r\nstruct rpc_cred *ds_cred;\r\nloff_t offset = hdr->args.offset;\r\nint vers;\r\nstruct nfs_fh *fh;\r\nint idx = hdr->pgio_mirror_idx;\r\nds = nfs4_ff_layout_prepare_ds(lseg, idx, true);\r\nif (!ds)\r\nreturn PNFS_NOT_ATTEMPTED;\r\nds_clnt = nfs4_ff_find_or_create_ds_client(lseg, idx, ds->ds_clp,\r\nhdr->inode);\r\nif (IS_ERR(ds_clnt))\r\nreturn PNFS_NOT_ATTEMPTED;\r\nds_cred = ff_layout_get_ds_cred(lseg, idx, hdr->cred);\r\nif (IS_ERR(ds_cred))\r\nreturn PNFS_NOT_ATTEMPTED;\r\nvers = nfs4_ff_layout_ds_version(lseg, idx);\r\ndprintk("%s ino %lu sync %d req %Zu@%llu DS: %s cl_count %d vers %d\n",\r\n__func__, hdr->inode->i_ino, sync, (size_t) hdr->args.count,\r\noffset, ds->ds_remotestr, atomic_read(&ds->ds_clp->cl_count),\r\nvers);\r\nhdr->pgio_done_cb = ff_layout_write_done_cb;\r\natomic_inc(&ds->ds_clp->cl_count);\r\nhdr->ds_clp = ds->ds_clp;\r\nhdr->ds_commit_idx = idx;\r\nfh = nfs4_ff_layout_select_ds_fh(lseg, idx);\r\nif (fh)\r\nhdr->args.fh = fh;\r\nhdr->args.offset = offset;\r\nnfs_initiate_pgio(ds_clnt, hdr, ds_cred, ds->ds_clp->rpc_ops,\r\nvers == 3 ? &ff_layout_write_call_ops_v3 :\r\n&ff_layout_write_call_ops_v4,\r\nsync, RPC_TASK_SOFTCONN);\r\nreturn PNFS_ATTEMPTED;\r\n}\r\nstatic u32 calc_ds_index_from_commit(struct pnfs_layout_segment *lseg, u32 i)\r\n{\r\nreturn i;\r\n}\r\nstatic struct nfs_fh *\r\nselect_ds_fh_from_commit(struct pnfs_layout_segment *lseg, u32 i)\r\n{\r\nstruct nfs4_ff_layout_segment *flseg = FF_LAYOUT_LSEG(lseg);\r\nreturn &flseg->mirror_array[i]->fh_versions[0];\r\n}\r\nstatic int ff_layout_initiate_commit(struct nfs_commit_data *data, int how)\r\n{\r\nstruct pnfs_layout_segment *lseg = data->lseg;\r\nstruct nfs4_pnfs_ds *ds;\r\nstruct rpc_clnt *ds_clnt;\r\nstruct rpc_cred *ds_cred;\r\nu32 idx;\r\nint vers;\r\nstruct nfs_fh *fh;\r\nidx = calc_ds_index_from_commit(lseg, data->ds_commit_index);\r\nds = nfs4_ff_layout_prepare_ds(lseg, idx, true);\r\nif (!ds)\r\ngoto out_err;\r\nds_clnt = nfs4_ff_find_or_create_ds_client(lseg, idx, ds->ds_clp,\r\ndata->inode);\r\nif (IS_ERR(ds_clnt))\r\ngoto out_err;\r\nds_cred = ff_layout_get_ds_cred(lseg, idx, data->cred);\r\nif (IS_ERR(ds_cred))\r\ngoto out_err;\r\nvers = nfs4_ff_layout_ds_version(lseg, idx);\r\ndprintk("%s ino %lu, how %d cl_count %d vers %d\n", __func__,\r\ndata->inode->i_ino, how, atomic_read(&ds->ds_clp->cl_count),\r\nvers);\r\ndata->commit_done_cb = ff_layout_commit_done_cb;\r\ndata->cred = ds_cred;\r\natomic_inc(&ds->ds_clp->cl_count);\r\ndata->ds_clp = ds->ds_clp;\r\nfh = select_ds_fh_from_commit(lseg, data->ds_commit_index);\r\nif (fh)\r\ndata->args.fh = fh;\r\nreturn nfs_initiate_commit(ds_clnt, data, ds->ds_clp->rpc_ops,\r\nvers == 3 ? &ff_layout_commit_call_ops_v3 :\r\n&ff_layout_commit_call_ops_v4,\r\nhow, RPC_TASK_SOFTCONN);\r\nout_err:\r\npnfs_generic_prepare_to_resend_writes(data);\r\npnfs_generic_commit_release(data);\r\nreturn -EAGAIN;\r\n}\r\nstatic int\r\nff_layout_commit_pagelist(struct inode *inode, struct list_head *mds_pages,\r\nint how, struct nfs_commit_info *cinfo)\r\n{\r\nreturn pnfs_generic_commit_pagelist(inode, mds_pages, how, cinfo,\r\nff_layout_initiate_commit);\r\n}\r\nstatic struct pnfs_ds_commit_info *\r\nff_layout_get_ds_info(struct inode *inode)\r\n{\r\nstruct pnfs_layout_hdr *layout = NFS_I(inode)->layout;\r\nif (layout == NULL)\r\nreturn NULL;\r\nreturn &FF_LAYOUT_FROM_HDR(layout)->commit_info;\r\n}\r\nstatic void\r\nff_layout_free_deviceid_node(struct nfs4_deviceid_node *d)\r\n{\r\nnfs4_ff_layout_free_deviceid(container_of(d, struct nfs4_ff_layout_ds,\r\nid_node));\r\n}\r\nstatic int ff_layout_encode_ioerr(struct nfs4_flexfile_layout *flo,\r\nstruct xdr_stream *xdr,\r\nconst struct nfs4_layoutreturn_args *args)\r\n{\r\nstruct pnfs_layout_hdr *hdr = &flo->generic_hdr;\r\n__be32 *start;\r\nint count = 0, ret = 0;\r\nstart = xdr_reserve_space(xdr, 4);\r\nif (unlikely(!start))\r\nreturn -E2BIG;\r\nspin_lock(&hdr->plh_inode->i_lock);\r\nret = ff_layout_encode_ds_ioerr(flo, xdr, &count, &args->range);\r\nspin_unlock(&hdr->plh_inode->i_lock);\r\n*start = cpu_to_be32(count);\r\nreturn ret;\r\n}\r\nstatic void ff_layout_encode_iostats(struct nfs4_flexfile_layout *flo,\r\nstruct xdr_stream *xdr,\r\nconst struct nfs4_layoutreturn_args *args)\r\n{\r\n__be32 *p;\r\np = xdr_reserve_space(xdr, 4);\r\nif (likely(p))\r\n*p = cpu_to_be32(0);\r\n}\r\nstatic struct nfs4_deviceid_node *\r\nff_layout_alloc_deviceid_node(struct nfs_server *server,\r\nstruct pnfs_device *pdev, gfp_t gfp_flags)\r\n{\r\nstruct nfs4_ff_layout_ds *dsaddr;\r\ndsaddr = nfs4_ff_alloc_deviceid_node(server, pdev, gfp_flags);\r\nif (!dsaddr)\r\nreturn NULL;\r\nreturn &dsaddr->id_node;\r\n}\r\nstatic void\r\nff_layout_encode_layoutreturn(struct pnfs_layout_hdr *lo,\r\nstruct xdr_stream *xdr,\r\nconst struct nfs4_layoutreturn_args *args)\r\n{\r\nstruct nfs4_flexfile_layout *flo = FF_LAYOUT_FROM_HDR(lo);\r\n__be32 *start;\r\ndprintk("%s: Begin\n", __func__);\r\nstart = xdr_reserve_space(xdr, 4);\r\nBUG_ON(!start);\r\nff_layout_encode_ioerr(flo, xdr, args);\r\nff_layout_encode_iostats(flo, xdr, args);\r\n*start = cpu_to_be32((xdr->p - start - 1) * 4);\r\ndprintk("%s: Return\n", __func__);\r\n}\r\nstatic int\r\nff_layout_ntop4(const struct sockaddr *sap, char *buf, const size_t buflen)\r\n{\r\nconst struct sockaddr_in *sin = (struct sockaddr_in *)sap;\r\nreturn snprintf(buf, buflen, "%pI4", &sin->sin_addr);\r\n}\r\nstatic size_t\r\nff_layout_ntop6_noscopeid(const struct sockaddr *sap, char *buf,\r\nconst int buflen)\r\n{\r\nconst struct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)sap;\r\nconst struct in6_addr *addr = &sin6->sin6_addr;\r\nif (ipv6_addr_any(addr))\r\nreturn snprintf(buf, buflen, "::");\r\nif (ipv6_addr_loopback(addr))\r\nreturn snprintf(buf, buflen, "::1");\r\nif (ipv6_addr_v4mapped(addr))\r\nreturn snprintf(buf, buflen, "::ffff:%pI4",\r\n&addr->s6_addr32[3]);\r\nreturn snprintf(buf, buflen, "%pI6c", addr);\r\n}\r\nstatic void\r\nff_layout_encode_netaddr(struct xdr_stream *xdr, struct nfs4_pnfs_ds_addr *da)\r\n{\r\nstruct sockaddr *sap = (struct sockaddr *)&da->da_addr;\r\nchar portbuf[RPCBIND_MAXUADDRPLEN];\r\nchar addrbuf[RPCBIND_MAXUADDRLEN];\r\nchar *netid;\r\nunsigned short port;\r\nint len, netid_len;\r\n__be32 *p;\r\nswitch (sap->sa_family) {\r\ncase AF_INET:\r\nif (ff_layout_ntop4(sap, addrbuf, sizeof(addrbuf)) == 0)\r\nreturn;\r\nport = ntohs(((struct sockaddr_in *)sap)->sin_port);\r\nnetid = "tcp";\r\nnetid_len = 3;\r\nbreak;\r\ncase AF_INET6:\r\nif (ff_layout_ntop6_noscopeid(sap, addrbuf, sizeof(addrbuf)) == 0)\r\nreturn;\r\nport = ntohs(((struct sockaddr_in6 *)sap)->sin6_port);\r\nnetid = "tcp6";\r\nnetid_len = 4;\r\nbreak;\r\ndefault:\r\nWARN_ON_ONCE(1);\r\nreturn;\r\n}\r\nsnprintf(portbuf, sizeof(portbuf), ".%u.%u", port >> 8, port & 0xff);\r\nlen = strlcat(addrbuf, portbuf, sizeof(addrbuf));\r\np = xdr_reserve_space(xdr, 4 + netid_len);\r\nxdr_encode_opaque(p, netid, netid_len);\r\np = xdr_reserve_space(xdr, 4 + len);\r\nxdr_encode_opaque(p, addrbuf, len);\r\n}\r\nstatic void\r\nff_layout_encode_nfstime(struct xdr_stream *xdr,\r\nktime_t t)\r\n{\r\nstruct timespec64 ts;\r\n__be32 *p;\r\np = xdr_reserve_space(xdr, 12);\r\nts = ktime_to_timespec64(t);\r\np = xdr_encode_hyper(p, ts.tv_sec);\r\n*p++ = cpu_to_be32(ts.tv_nsec);\r\n}\r\nstatic void\r\nff_layout_encode_io_latency(struct xdr_stream *xdr,\r\nstruct nfs4_ff_io_stat *stat)\r\n{\r\n__be32 *p;\r\np = xdr_reserve_space(xdr, 5 * 8);\r\np = xdr_encode_hyper(p, stat->ops_requested);\r\np = xdr_encode_hyper(p, stat->bytes_requested);\r\np = xdr_encode_hyper(p, stat->ops_completed);\r\np = xdr_encode_hyper(p, stat->bytes_completed);\r\np = xdr_encode_hyper(p, stat->bytes_not_delivered);\r\nff_layout_encode_nfstime(xdr, stat->total_busy_time);\r\nff_layout_encode_nfstime(xdr, stat->aggregate_completion_time);\r\n}\r\nstatic void\r\nff_layout_encode_layoutstats(struct xdr_stream *xdr,\r\nstruct nfs42_layoutstat_args *args,\r\nstruct nfs42_layoutstat_devinfo *devinfo)\r\n{\r\nstruct nfs4_ff_layout_mirror *mirror = devinfo->layout_private;\r\nstruct nfs4_pnfs_ds_addr *da;\r\nstruct nfs4_pnfs_ds *ds = mirror->mirror_ds->ds;\r\nstruct nfs_fh *fh = &mirror->fh_versions[0];\r\n__be32 *p, *start;\r\nda = list_first_entry(&ds->ds_addrs, struct nfs4_pnfs_ds_addr, da_node);\r\ndprintk("%s: DS %s: encoding address %s\n",\r\n__func__, ds->ds_remotestr, da->da_remotestr);\r\nstart = xdr_reserve_space(xdr, 4);\r\nff_layout_encode_netaddr(xdr, da);\r\np = xdr_reserve_space(xdr, 4 + fh->size);\r\nxdr_encode_opaque(p, fh->data, fh->size);\r\nspin_lock(&mirror->lock);\r\nff_layout_encode_io_latency(xdr, &mirror->read_stat.io_stat);\r\nff_layout_encode_io_latency(xdr, &mirror->write_stat.io_stat);\r\nspin_unlock(&mirror->lock);\r\nff_layout_encode_nfstime(xdr, ktime_sub(ktime_get(), mirror->start_time));\r\np = xdr_reserve_space(xdr, 4);\r\n*p = cpu_to_be32(false);\r\n*start = cpu_to_be32((xdr->p - start - 1) * 4);\r\n}\r\nstatic int\r\nff_layout_mirror_prepare_stats(struct nfs42_layoutstat_args *args,\r\nstruct pnfs_layout_hdr *lo,\r\nint dev_limit)\r\n{\r\nstruct nfs4_flexfile_layout *ff_layout = FF_LAYOUT_FROM_HDR(lo);\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nstruct nfs4_deviceid_node *dev;\r\nstruct nfs42_layoutstat_devinfo *devinfo;\r\nint i = 0;\r\nlist_for_each_entry(mirror, &ff_layout->mirrors, mirrors) {\r\nif (i >= dev_limit)\r\nbreak;\r\nif (!mirror->mirror_ds)\r\ncontinue;\r\nif (!atomic_inc_not_zero(&mirror->ref))\r\ncontinue;\r\ndev = &mirror->mirror_ds->id_node;\r\ndevinfo = &args->devinfo[i];\r\nmemcpy(&devinfo->dev_id, &dev->deviceid, NFS4_DEVICEID4_SIZE);\r\ndevinfo->offset = 0;\r\ndevinfo->length = NFS4_MAX_UINT64;\r\ndevinfo->read_count = mirror->read_stat.io_stat.ops_completed;\r\ndevinfo->read_bytes = mirror->read_stat.io_stat.bytes_completed;\r\ndevinfo->write_count = mirror->write_stat.io_stat.ops_completed;\r\ndevinfo->write_bytes = mirror->write_stat.io_stat.bytes_completed;\r\ndevinfo->layout_type = LAYOUT_FLEX_FILES;\r\ndevinfo->layoutstats_encode = ff_layout_encode_layoutstats;\r\ndevinfo->layout_private = mirror;\r\ni++;\r\n}\r\nreturn i;\r\n}\r\nstatic int\r\nff_layout_prepare_layoutstats(struct nfs42_layoutstat_args *args)\r\n{\r\nstruct nfs4_flexfile_layout *ff_layout;\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nint dev_count = 0;\r\nspin_lock(&args->inode->i_lock);\r\nff_layout = FF_LAYOUT_FROM_HDR(NFS_I(args->inode)->layout);\r\nlist_for_each_entry(mirror, &ff_layout->mirrors, mirrors) {\r\nif (atomic_read(&mirror->ref) != 0)\r\ndev_count ++;\r\n}\r\nspin_unlock(&args->inode->i_lock);\r\nif (dev_count > PNFS_LAYOUTSTATS_MAXDEV) {\r\ndprintk("%s: truncating devinfo to limit (%d:%d)\n",\r\n__func__, dev_count, PNFS_LAYOUTSTATS_MAXDEV);\r\ndev_count = PNFS_LAYOUTSTATS_MAXDEV;\r\n}\r\nargs->devinfo = kmalloc_array(dev_count, sizeof(*args->devinfo), GFP_NOIO);\r\nif (!args->devinfo)\r\nreturn -ENOMEM;\r\nspin_lock(&args->inode->i_lock);\r\nargs->num_dev = ff_layout_mirror_prepare_stats(args,\r\n&ff_layout->generic_hdr, dev_count);\r\nspin_unlock(&args->inode->i_lock);\r\nreturn 0;\r\n}\r\nstatic void\r\nff_layout_cleanup_layoutstats(struct nfs42_layoutstat_data *data)\r\n{\r\nstruct nfs4_ff_layout_mirror *mirror;\r\nint i;\r\nfor (i = 0; i < data->args.num_dev; i++) {\r\nmirror = data->args.devinfo[i].layout_private;\r\ndata->args.devinfo[i].layout_private = NULL;\r\nff_layout_put_mirror(mirror);\r\n}\r\n}\r\nstatic int __init nfs4flexfilelayout_init(void)\r\n{\r\nprintk(KERN_INFO "%s: NFSv4 Flexfile Layout Driver Registering...\n",\r\n__func__);\r\nreturn pnfs_register_layoutdriver(&flexfilelayout_type);\r\n}\r\nstatic void __exit nfs4flexfilelayout_exit(void)\r\n{\r\nprintk(KERN_INFO "%s: NFSv4 Flexfile Layout Driver Unregistering...\n",\r\n__func__);\r\npnfs_unregister_layoutdriver(&flexfilelayout_type);\r\n}
