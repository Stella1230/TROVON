static void crypto4xx_hw_init(struct crypto4xx_device *dev)\r\n{\r\nunion ce_ring_size ring_size;\r\nunion ce_ring_contol ring_ctrl;\r\nunion ce_part_ring_size part_ring_size;\r\nunion ce_io_threshold io_threshold;\r\nu32 rand_num;\r\nunion ce_pe_dma_cfg pe_dma_cfg;\r\nu32 device_ctrl;\r\nwritel(PPC4XX_BYTE_ORDER, dev->ce_base + CRYPTO4XX_BYTE_ORDER_CFG);\r\npe_dma_cfg.w = 0;\r\npe_dma_cfg.bf.bo_sgpd_en = 1;\r\npe_dma_cfg.bf.bo_data_en = 0;\r\npe_dma_cfg.bf.bo_sa_en = 1;\r\npe_dma_cfg.bf.bo_pd_en = 1;\r\npe_dma_cfg.bf.dynamic_sa_en = 1;\r\npe_dma_cfg.bf.reset_sg = 1;\r\npe_dma_cfg.bf.reset_pdr = 1;\r\npe_dma_cfg.bf.reset_pe = 1;\r\nwritel(pe_dma_cfg.w, dev->ce_base + CRYPTO4XX_PE_DMA_CFG);\r\npe_dma_cfg.bf.pe_mode = 0;\r\npe_dma_cfg.bf.reset_sg = 0;\r\npe_dma_cfg.bf.reset_pdr = 0;\r\npe_dma_cfg.bf.reset_pe = 0;\r\npe_dma_cfg.bf.bo_td_en = 0;\r\nwritel(pe_dma_cfg.w, dev->ce_base + CRYPTO4XX_PE_DMA_CFG);\r\nwritel(dev->pdr_pa, dev->ce_base + CRYPTO4XX_PDR_BASE);\r\nwritel(dev->pdr_pa, dev->ce_base + CRYPTO4XX_RDR_BASE);\r\nwritel(PPC4XX_PRNG_CTRL_AUTO_EN, dev->ce_base + CRYPTO4XX_PRNG_CTRL);\r\nget_random_bytes(&rand_num, sizeof(rand_num));\r\nwritel(rand_num, dev->ce_base + CRYPTO4XX_PRNG_SEED_L);\r\nget_random_bytes(&rand_num, sizeof(rand_num));\r\nwritel(rand_num, dev->ce_base + CRYPTO4XX_PRNG_SEED_H);\r\nring_size.w = 0;\r\nring_size.bf.ring_offset = PPC4XX_PD_SIZE;\r\nring_size.bf.ring_size = PPC4XX_NUM_PD;\r\nwritel(ring_size.w, dev->ce_base + CRYPTO4XX_RING_SIZE);\r\nring_ctrl.w = 0;\r\nwritel(ring_ctrl.w, dev->ce_base + CRYPTO4XX_RING_CTRL);\r\ndevice_ctrl = readl(dev->ce_base + CRYPTO4XX_DEVICE_CTRL);\r\ndevice_ctrl |= PPC4XX_DC_3DES_EN;\r\nwritel(device_ctrl, dev->ce_base + CRYPTO4XX_DEVICE_CTRL);\r\nwritel(dev->gdr_pa, dev->ce_base + CRYPTO4XX_GATH_RING_BASE);\r\nwritel(dev->sdr_pa, dev->ce_base + CRYPTO4XX_SCAT_RING_BASE);\r\npart_ring_size.w = 0;\r\npart_ring_size.bf.sdr_size = PPC4XX_SDR_SIZE;\r\npart_ring_size.bf.gdr_size = PPC4XX_GDR_SIZE;\r\nwritel(part_ring_size.w, dev->ce_base + CRYPTO4XX_PART_RING_SIZE);\r\nwritel(PPC4XX_SD_BUFFER_SIZE, dev->ce_base + CRYPTO4XX_PART_RING_CFG);\r\nio_threshold.w = 0;\r\nio_threshold.bf.output_threshold = PPC4XX_OUTPUT_THRESHOLD;\r\nio_threshold.bf.input_threshold = PPC4XX_INPUT_THRESHOLD;\r\nwritel(io_threshold.w, dev->ce_base + CRYPTO4XX_IO_THRESHOLD);\r\nwritel(0, dev->ce_base + CRYPTO4XX_PDR_BASE_UADDR);\r\nwritel(0, dev->ce_base + CRYPTO4XX_RDR_BASE_UADDR);\r\nwritel(0, dev->ce_base + CRYPTO4XX_PKT_SRC_UADDR);\r\nwritel(0, dev->ce_base + CRYPTO4XX_PKT_DEST_UADDR);\r\nwritel(0, dev->ce_base + CRYPTO4XX_SA_UADDR);\r\nwritel(0, dev->ce_base + CRYPTO4XX_GATH_RING_BASE_UADDR);\r\nwritel(0, dev->ce_base + CRYPTO4XX_SCAT_RING_BASE_UADDR);\r\npe_dma_cfg.bf.pe_mode = 1;\r\npe_dma_cfg.bf.reset_sg = 0;\r\npe_dma_cfg.bf.reset_pdr = 0;\r\npe_dma_cfg.bf.reset_pe = 0;\r\npe_dma_cfg.bf.bo_td_en = 0;\r\nwritel(pe_dma_cfg.w, dev->ce_base + CRYPTO4XX_PE_DMA_CFG);\r\nwritel(PPC4XX_INTERRUPT_CLR, dev->ce_base + CRYPTO4XX_INT_CLR);\r\nwritel(PPC4XX_INT_DESCR_CNT, dev->ce_base + CRYPTO4XX_INT_DESCR_CNT);\r\nwritel(PPC4XX_INT_DESCR_CNT, dev->ce_base + CRYPTO4XX_INT_DESCR_CNT);\r\nwritel(PPC4XX_INT_CFG, dev->ce_base + CRYPTO4XX_INT_CFG);\r\nwritel(PPC4XX_PD_DONE_INT, dev->ce_base + CRYPTO4XX_INT_EN);\r\n}\r\nint crypto4xx_alloc_sa(struct crypto4xx_ctx *ctx, u32 size)\r\n{\r\nctx->sa_in = dma_alloc_coherent(ctx->dev->core_dev->device, size * 4,\r\n&ctx->sa_in_dma_addr, GFP_ATOMIC);\r\nif (ctx->sa_in == NULL)\r\nreturn -ENOMEM;\r\nctx->sa_out = dma_alloc_coherent(ctx->dev->core_dev->device, size * 4,\r\n&ctx->sa_out_dma_addr, GFP_ATOMIC);\r\nif (ctx->sa_out == NULL) {\r\ndma_free_coherent(ctx->dev->core_dev->device,\r\nctx->sa_len * 4,\r\nctx->sa_in, ctx->sa_in_dma_addr);\r\nreturn -ENOMEM;\r\n}\r\nmemset(ctx->sa_in, 0, size * 4);\r\nmemset(ctx->sa_out, 0, size * 4);\r\nctx->sa_len = size;\r\nreturn 0;\r\n}\r\nvoid crypto4xx_free_sa(struct crypto4xx_ctx *ctx)\r\n{\r\nif (ctx->sa_in != NULL)\r\ndma_free_coherent(ctx->dev->core_dev->device, ctx->sa_len * 4,\r\nctx->sa_in, ctx->sa_in_dma_addr);\r\nif (ctx->sa_out != NULL)\r\ndma_free_coherent(ctx->dev->core_dev->device, ctx->sa_len * 4,\r\nctx->sa_out, ctx->sa_out_dma_addr);\r\nctx->sa_in_dma_addr = 0;\r\nctx->sa_out_dma_addr = 0;\r\nctx->sa_len = 0;\r\n}\r\nu32 crypto4xx_alloc_state_record(struct crypto4xx_ctx *ctx)\r\n{\r\nctx->state_record = dma_alloc_coherent(ctx->dev->core_dev->device,\r\nsizeof(struct sa_state_record),\r\n&ctx->state_record_dma_addr, GFP_ATOMIC);\r\nif (!ctx->state_record_dma_addr)\r\nreturn -ENOMEM;\r\nmemset(ctx->state_record, 0, sizeof(struct sa_state_record));\r\nreturn 0;\r\n}\r\nvoid crypto4xx_free_state_record(struct crypto4xx_ctx *ctx)\r\n{\r\nif (ctx->state_record != NULL)\r\ndma_free_coherent(ctx->dev->core_dev->device,\r\nsizeof(struct sa_state_record),\r\nctx->state_record,\r\nctx->state_record_dma_addr);\r\nctx->state_record_dma_addr = 0;\r\n}\r\nstatic u32 crypto4xx_build_pdr(struct crypto4xx_device *dev)\r\n{\r\nint i;\r\nstruct pd_uinfo *pd_uinfo;\r\ndev->pdr = dma_alloc_coherent(dev->core_dev->device,\r\nsizeof(struct ce_pd) * PPC4XX_NUM_PD,\r\n&dev->pdr_pa, GFP_ATOMIC);\r\nif (!dev->pdr)\r\nreturn -ENOMEM;\r\ndev->pdr_uinfo = kzalloc(sizeof(struct pd_uinfo) * PPC4XX_NUM_PD,\r\nGFP_KERNEL);\r\nif (!dev->pdr_uinfo) {\r\ndma_free_coherent(dev->core_dev->device,\r\nsizeof(struct ce_pd) * PPC4XX_NUM_PD,\r\ndev->pdr,\r\ndev->pdr_pa);\r\nreturn -ENOMEM;\r\n}\r\nmemset(dev->pdr, 0, sizeof(struct ce_pd) * PPC4XX_NUM_PD);\r\ndev->shadow_sa_pool = dma_alloc_coherent(dev->core_dev->device,\r\n256 * PPC4XX_NUM_PD,\r\n&dev->shadow_sa_pool_pa,\r\nGFP_ATOMIC);\r\nif (!dev->shadow_sa_pool)\r\nreturn -ENOMEM;\r\ndev->shadow_sr_pool = dma_alloc_coherent(dev->core_dev->device,\r\nsizeof(struct sa_state_record) * PPC4XX_NUM_PD,\r\n&dev->shadow_sr_pool_pa, GFP_ATOMIC);\r\nif (!dev->shadow_sr_pool)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < PPC4XX_NUM_PD; i++) {\r\npd_uinfo = (struct pd_uinfo *) (dev->pdr_uinfo +\r\nsizeof(struct pd_uinfo) * i);\r\npd_uinfo->sa_va = dev->shadow_sa_pool + 256 * i;\r\npd_uinfo->sa_pa = dev->shadow_sa_pool_pa + 256 * i;\r\npd_uinfo->sr_va = dev->shadow_sr_pool +\r\nsizeof(struct sa_state_record) * i;\r\npd_uinfo->sr_pa = dev->shadow_sr_pool_pa +\r\nsizeof(struct sa_state_record) * i;\r\n}\r\nreturn 0;\r\n}\r\nstatic void crypto4xx_destroy_pdr(struct crypto4xx_device *dev)\r\n{\r\nif (dev->pdr != NULL)\r\ndma_free_coherent(dev->core_dev->device,\r\nsizeof(struct ce_pd) * PPC4XX_NUM_PD,\r\ndev->pdr, dev->pdr_pa);\r\nif (dev->shadow_sa_pool)\r\ndma_free_coherent(dev->core_dev->device, 256 * PPC4XX_NUM_PD,\r\ndev->shadow_sa_pool, dev->shadow_sa_pool_pa);\r\nif (dev->shadow_sr_pool)\r\ndma_free_coherent(dev->core_dev->device,\r\nsizeof(struct sa_state_record) * PPC4XX_NUM_PD,\r\ndev->shadow_sr_pool, dev->shadow_sr_pool_pa);\r\nkfree(dev->pdr_uinfo);\r\n}\r\nstatic u32 crypto4xx_get_pd_from_pdr_nolock(struct crypto4xx_device *dev)\r\n{\r\nu32 retval;\r\nu32 tmp;\r\nretval = dev->pdr_head;\r\ntmp = (dev->pdr_head + 1) % PPC4XX_NUM_PD;\r\nif (tmp == dev->pdr_tail)\r\nreturn ERING_WAS_FULL;\r\ndev->pdr_head = tmp;\r\nreturn retval;\r\n}\r\nstatic u32 crypto4xx_put_pd_to_pdr(struct crypto4xx_device *dev, u32 idx)\r\n{\r\nstruct pd_uinfo *pd_uinfo;\r\nunsigned long flags;\r\npd_uinfo = (struct pd_uinfo *)(dev->pdr_uinfo +\r\nsizeof(struct pd_uinfo) * idx);\r\nspin_lock_irqsave(&dev->core_dev->lock, flags);\r\nif (dev->pdr_tail != PPC4XX_LAST_PD)\r\ndev->pdr_tail++;\r\nelse\r\ndev->pdr_tail = 0;\r\npd_uinfo->state = PD_ENTRY_FREE;\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn 0;\r\n}\r\nstatic struct ce_pd *crypto4xx_get_pdp(struct crypto4xx_device *dev,\r\ndma_addr_t *pd_dma, u32 idx)\r\n{\r\n*pd_dma = dev->pdr_pa + sizeof(struct ce_pd) * idx;\r\nreturn dev->pdr + sizeof(struct ce_pd) * idx;\r\n}\r\nstatic u32 crypto4xx_build_gdr(struct crypto4xx_device *dev)\r\n{\r\ndev->gdr = dma_alloc_coherent(dev->core_dev->device,\r\nsizeof(struct ce_gd) * PPC4XX_NUM_GD,\r\n&dev->gdr_pa, GFP_ATOMIC);\r\nif (!dev->gdr)\r\nreturn -ENOMEM;\r\nmemset(dev->gdr, 0, sizeof(struct ce_gd) * PPC4XX_NUM_GD);\r\nreturn 0;\r\n}\r\nstatic inline void crypto4xx_destroy_gdr(struct crypto4xx_device *dev)\r\n{\r\ndma_free_coherent(dev->core_dev->device,\r\nsizeof(struct ce_gd) * PPC4XX_NUM_GD,\r\ndev->gdr, dev->gdr_pa);\r\n}\r\nu32 crypto4xx_get_n_gd(struct crypto4xx_device *dev, int n)\r\n{\r\nu32 retval;\r\nu32 tmp;\r\nif (n >= PPC4XX_NUM_GD)\r\nreturn ERING_WAS_FULL;\r\nretval = dev->gdr_head;\r\ntmp = (dev->gdr_head + n) % PPC4XX_NUM_GD;\r\nif (dev->gdr_head > dev->gdr_tail) {\r\nif (tmp < dev->gdr_head && tmp >= dev->gdr_tail)\r\nreturn ERING_WAS_FULL;\r\n} else if (dev->gdr_head < dev->gdr_tail) {\r\nif (tmp < dev->gdr_head || tmp >= dev->gdr_tail)\r\nreturn ERING_WAS_FULL;\r\n}\r\ndev->gdr_head = tmp;\r\nreturn retval;\r\n}\r\nstatic u32 crypto4xx_put_gd_to_gdr(struct crypto4xx_device *dev)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->core_dev->lock, flags);\r\nif (dev->gdr_tail == dev->gdr_head) {\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn 0;\r\n}\r\nif (dev->gdr_tail != PPC4XX_LAST_GD)\r\ndev->gdr_tail++;\r\nelse\r\ndev->gdr_tail = 0;\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn 0;\r\n}\r\nstatic inline struct ce_gd *crypto4xx_get_gdp(struct crypto4xx_device *dev,\r\ndma_addr_t *gd_dma, u32 idx)\r\n{\r\n*gd_dma = dev->gdr_pa + sizeof(struct ce_gd) * idx;\r\nreturn (struct ce_gd *) (dev->gdr + sizeof(struct ce_gd) * idx);\r\n}\r\nstatic u32 crypto4xx_build_sdr(struct crypto4xx_device *dev)\r\n{\r\nint i;\r\nstruct ce_sd *sd_array;\r\ndev->sdr = dma_alloc_coherent(dev->core_dev->device,\r\nsizeof(struct ce_sd) * PPC4XX_NUM_SD,\r\n&dev->sdr_pa, GFP_ATOMIC);\r\nif (!dev->sdr)\r\nreturn -ENOMEM;\r\ndev->scatter_buffer_size = PPC4XX_SD_BUFFER_SIZE;\r\ndev->scatter_buffer_va =\r\ndma_alloc_coherent(dev->core_dev->device,\r\ndev->scatter_buffer_size * PPC4XX_NUM_SD,\r\n&dev->scatter_buffer_pa, GFP_ATOMIC);\r\nif (!dev->scatter_buffer_va) {\r\ndma_free_coherent(dev->core_dev->device,\r\nsizeof(struct ce_sd) * PPC4XX_NUM_SD,\r\ndev->sdr, dev->sdr_pa);\r\nreturn -ENOMEM;\r\n}\r\nsd_array = dev->sdr;\r\nfor (i = 0; i < PPC4XX_NUM_SD; i++) {\r\nsd_array[i].ptr = dev->scatter_buffer_pa +\r\ndev->scatter_buffer_size * i;\r\n}\r\nreturn 0;\r\n}\r\nstatic void crypto4xx_destroy_sdr(struct crypto4xx_device *dev)\r\n{\r\nif (dev->sdr != NULL)\r\ndma_free_coherent(dev->core_dev->device,\r\nsizeof(struct ce_sd) * PPC4XX_NUM_SD,\r\ndev->sdr, dev->sdr_pa);\r\nif (dev->scatter_buffer_va != NULL)\r\ndma_free_coherent(dev->core_dev->device,\r\ndev->scatter_buffer_size * PPC4XX_NUM_SD,\r\ndev->scatter_buffer_va,\r\ndev->scatter_buffer_pa);\r\n}\r\nstatic u32 crypto4xx_get_n_sd(struct crypto4xx_device *dev, int n)\r\n{\r\nu32 retval;\r\nu32 tmp;\r\nif (n >= PPC4XX_NUM_SD)\r\nreturn ERING_WAS_FULL;\r\nretval = dev->sdr_head;\r\ntmp = (dev->sdr_head + n) % PPC4XX_NUM_SD;\r\nif (dev->sdr_head > dev->gdr_tail) {\r\nif (tmp < dev->sdr_head && tmp >= dev->sdr_tail)\r\nreturn ERING_WAS_FULL;\r\n} else if (dev->sdr_head < dev->sdr_tail) {\r\nif (tmp < dev->sdr_head || tmp >= dev->sdr_tail)\r\nreturn ERING_WAS_FULL;\r\n}\r\ndev->sdr_head = tmp;\r\nreturn retval;\r\n}\r\nstatic u32 crypto4xx_put_sd_to_sdr(struct crypto4xx_device *dev)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->core_dev->lock, flags);\r\nif (dev->sdr_tail == dev->sdr_head) {\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn 0;\r\n}\r\nif (dev->sdr_tail != PPC4XX_LAST_SD)\r\ndev->sdr_tail++;\r\nelse\r\ndev->sdr_tail = 0;\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn 0;\r\n}\r\nstatic inline struct ce_sd *crypto4xx_get_sdp(struct crypto4xx_device *dev,\r\ndma_addr_t *sd_dma, u32 idx)\r\n{\r\n*sd_dma = dev->sdr_pa + sizeof(struct ce_sd) * idx;\r\nreturn (struct ce_sd *)(dev->sdr + sizeof(struct ce_sd) * idx);\r\n}\r\nstatic u32 crypto4xx_fill_one_page(struct crypto4xx_device *dev,\r\ndma_addr_t *addr, u32 *length,\r\nu32 *idx, u32 *offset, u32 *nbytes)\r\n{\r\nu32 len;\r\nif (*length > dev->scatter_buffer_size) {\r\nmemcpy(phys_to_virt(*addr),\r\ndev->scatter_buffer_va +\r\n*idx * dev->scatter_buffer_size + *offset,\r\ndev->scatter_buffer_size);\r\n*offset = 0;\r\n*length -= dev->scatter_buffer_size;\r\n*nbytes -= dev->scatter_buffer_size;\r\nif (*idx == PPC4XX_LAST_SD)\r\n*idx = 0;\r\nelse\r\n(*idx)++;\r\n*addr = *addr + dev->scatter_buffer_size;\r\nreturn 1;\r\n} else if (*length < dev->scatter_buffer_size) {\r\nmemcpy(phys_to_virt(*addr),\r\ndev->scatter_buffer_va +\r\n*idx * dev->scatter_buffer_size + *offset, *length);\r\nif ((*offset + *length) == dev->scatter_buffer_size) {\r\nif (*idx == PPC4XX_LAST_SD)\r\n*idx = 0;\r\nelse\r\n(*idx)++;\r\n*nbytes -= *length;\r\n*offset = 0;\r\n} else {\r\n*nbytes -= *length;\r\n*offset += *length;\r\n}\r\nreturn 0;\r\n} else {\r\nlen = (*nbytes <= dev->scatter_buffer_size) ?\r\n(*nbytes) : dev->scatter_buffer_size;\r\nmemcpy(phys_to_virt(*addr),\r\ndev->scatter_buffer_va +\r\n*idx * dev->scatter_buffer_size + *offset,\r\nlen);\r\n*offset = 0;\r\n*nbytes -= len;\r\nif (*idx == PPC4XX_LAST_SD)\r\n*idx = 0;\r\nelse\r\n(*idx)++;\r\nreturn 0;\r\n}\r\n}\r\nstatic void crypto4xx_copy_pkt_to_dst(struct crypto4xx_device *dev,\r\nstruct ce_pd *pd,\r\nstruct pd_uinfo *pd_uinfo,\r\nu32 nbytes,\r\nstruct scatterlist *dst)\r\n{\r\ndma_addr_t addr;\r\nu32 this_sd;\r\nu32 offset;\r\nu32 len;\r\nu32 i;\r\nu32 sg_len;\r\nstruct scatterlist *sg;\r\nthis_sd = pd_uinfo->first_sd;\r\noffset = 0;\r\ni = 0;\r\nwhile (nbytes) {\r\nsg = &dst[i];\r\nsg_len = sg->length;\r\naddr = dma_map_page(dev->core_dev->device, sg_page(sg),\r\nsg->offset, sg->length, DMA_TO_DEVICE);\r\nif (offset == 0) {\r\nlen = (nbytes <= sg->length) ? nbytes : sg->length;\r\nwhile (crypto4xx_fill_one_page(dev, &addr, &len,\r\n&this_sd, &offset, &nbytes))\r\n;\r\nif (!nbytes)\r\nreturn;\r\ni++;\r\n} else {\r\nlen = (nbytes <= (dev->scatter_buffer_size - offset)) ?\r\nnbytes : (dev->scatter_buffer_size - offset);\r\nlen = (sg->length < len) ? sg->length : len;\r\nwhile (crypto4xx_fill_one_page(dev, &addr, &len,\r\n&this_sd, &offset, &nbytes))\r\n;\r\nif (!nbytes)\r\nreturn;\r\nsg_len -= len;\r\nif (sg_len) {\r\naddr += len;\r\nwhile (crypto4xx_fill_one_page(dev, &addr,\r\n&sg_len, &this_sd, &offset, &nbytes))\r\n;\r\n}\r\ni++;\r\n}\r\n}\r\n}\r\nstatic u32 crypto4xx_copy_digest_to_dst(struct pd_uinfo *pd_uinfo,\r\nstruct crypto4xx_ctx *ctx)\r\n{\r\nstruct dynamic_sa_ctl *sa = (struct dynamic_sa_ctl *) ctx->sa_in;\r\nstruct sa_state_record *state_record =\r\n(struct sa_state_record *) pd_uinfo->sr_va;\r\nif (sa->sa_command_0.bf.hash_alg == SA_HASH_ALG_SHA1) {\r\nmemcpy((void *) pd_uinfo->dest_va, state_record->save_digest,\r\nSA_HASH_ALG_SHA1_DIGEST_SIZE);\r\n}\r\nreturn 0;\r\n}\r\nstatic void crypto4xx_ret_sg_desc(struct crypto4xx_device *dev,\r\nstruct pd_uinfo *pd_uinfo)\r\n{\r\nint i;\r\nif (pd_uinfo->num_gd) {\r\nfor (i = 0; i < pd_uinfo->num_gd; i++)\r\ncrypto4xx_put_gd_to_gdr(dev);\r\npd_uinfo->first_gd = 0xffffffff;\r\npd_uinfo->num_gd = 0;\r\n}\r\nif (pd_uinfo->num_sd) {\r\nfor (i = 0; i < pd_uinfo->num_sd; i++)\r\ncrypto4xx_put_sd_to_sdr(dev);\r\npd_uinfo->first_sd = 0xffffffff;\r\npd_uinfo->num_sd = 0;\r\n}\r\n}\r\nstatic u32 crypto4xx_ablkcipher_done(struct crypto4xx_device *dev,\r\nstruct pd_uinfo *pd_uinfo,\r\nstruct ce_pd *pd)\r\n{\r\nstruct crypto4xx_ctx *ctx;\r\nstruct ablkcipher_request *ablk_req;\r\nstruct scatterlist *dst;\r\ndma_addr_t addr;\r\nablk_req = ablkcipher_request_cast(pd_uinfo->async_req);\r\nctx = crypto_tfm_ctx(ablk_req->base.tfm);\r\nif (pd_uinfo->using_sd) {\r\ncrypto4xx_copy_pkt_to_dst(dev, pd, pd_uinfo, ablk_req->nbytes,\r\nablk_req->dst);\r\n} else {\r\ndst = pd_uinfo->dest_va;\r\naddr = dma_map_page(dev->core_dev->device, sg_page(dst),\r\ndst->offset, dst->length, DMA_FROM_DEVICE);\r\n}\r\ncrypto4xx_ret_sg_desc(dev, pd_uinfo);\r\nif (ablk_req->base.complete != NULL)\r\nablk_req->base.complete(&ablk_req->base, 0);\r\nreturn 0;\r\n}\r\nstatic u32 crypto4xx_ahash_done(struct crypto4xx_device *dev,\r\nstruct pd_uinfo *pd_uinfo)\r\n{\r\nstruct crypto4xx_ctx *ctx;\r\nstruct ahash_request *ahash_req;\r\nahash_req = ahash_request_cast(pd_uinfo->async_req);\r\nctx = crypto_tfm_ctx(ahash_req->base.tfm);\r\ncrypto4xx_copy_digest_to_dst(pd_uinfo,\r\ncrypto_tfm_ctx(ahash_req->base.tfm));\r\ncrypto4xx_ret_sg_desc(dev, pd_uinfo);\r\nif (ahash_req->base.complete != NULL)\r\nahash_req->base.complete(&ahash_req->base, 0);\r\nreturn 0;\r\n}\r\nstatic u32 crypto4xx_pd_done(struct crypto4xx_device *dev, u32 idx)\r\n{\r\nstruct ce_pd *pd;\r\nstruct pd_uinfo *pd_uinfo;\r\npd = dev->pdr + sizeof(struct ce_pd)*idx;\r\npd_uinfo = dev->pdr_uinfo + sizeof(struct pd_uinfo)*idx;\r\nif (crypto_tfm_alg_type(pd_uinfo->async_req->tfm) ==\r\nCRYPTO_ALG_TYPE_ABLKCIPHER)\r\nreturn crypto4xx_ablkcipher_done(dev, pd_uinfo, pd);\r\nelse\r\nreturn crypto4xx_ahash_done(dev, pd_uinfo);\r\n}\r\nvoid crypto4xx_memcpy_le(unsigned int *dst,\r\nconst unsigned char *buf,\r\nint len)\r\n{\r\nu8 *tmp;\r\nfor (; len >= 4; buf += 4, len -= 4)\r\n*dst++ = cpu_to_le32(*(unsigned int *) buf);\r\ntmp = (u8 *)dst;\r\nswitch (len) {\r\ncase 3:\r\n*tmp++ = 0;\r\n*tmp++ = *(buf+2);\r\n*tmp++ = *(buf+1);\r\n*tmp++ = *buf;\r\nbreak;\r\ncase 2:\r\n*tmp++ = 0;\r\n*tmp++ = 0;\r\n*tmp++ = *(buf+1);\r\n*tmp++ = *buf;\r\nbreak;\r\ncase 1:\r\n*tmp++ = 0;\r\n*tmp++ = 0;\r\n*tmp++ = 0;\r\n*tmp++ = *buf;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void crypto4xx_stop_all(struct crypto4xx_core_device *core_dev)\r\n{\r\ncrypto4xx_destroy_pdr(core_dev->dev);\r\ncrypto4xx_destroy_gdr(core_dev->dev);\r\ncrypto4xx_destroy_sdr(core_dev->dev);\r\niounmap(core_dev->dev->ce_base);\r\nkfree(core_dev->dev);\r\nkfree(core_dev);\r\n}\r\nvoid crypto4xx_return_pd(struct crypto4xx_device *dev,\r\nu32 pd_entry, struct ce_pd *pd,\r\nstruct pd_uinfo *pd_uinfo)\r\n{\r\ndev->pdr_head = pd_entry;\r\npd->pd_ctl.w = 0;\r\npd->pd_ctl_len.w = 0;\r\npd_uinfo->state = PD_ENTRY_FREE;\r\n}\r\nstatic u32 get_next_gd(u32 current)\r\n{\r\nif (current != PPC4XX_LAST_GD)\r\nreturn current + 1;\r\nelse\r\nreturn 0;\r\n}\r\nstatic u32 get_next_sd(u32 current)\r\n{\r\nif (current != PPC4XX_LAST_SD)\r\nreturn current + 1;\r\nelse\r\nreturn 0;\r\n}\r\nu32 crypto4xx_build_pd(struct crypto_async_request *req,\r\nstruct crypto4xx_ctx *ctx,\r\nstruct scatterlist *src,\r\nstruct scatterlist *dst,\r\nunsigned int datalen,\r\nvoid *iv, u32 iv_len)\r\n{\r\nstruct crypto4xx_device *dev = ctx->dev;\r\ndma_addr_t addr, pd_dma, sd_dma, gd_dma;\r\nstruct dynamic_sa_ctl *sa;\r\nstruct scatterlist *sg;\r\nstruct ce_gd *gd;\r\nstruct ce_pd *pd;\r\nu32 num_gd, num_sd;\r\nu32 fst_gd = 0xffffffff;\r\nu32 fst_sd = 0xffffffff;\r\nu32 pd_entry;\r\nunsigned long flags;\r\nstruct pd_uinfo *pd_uinfo = NULL;\r\nunsigned int nbytes = datalen, idx;\r\nunsigned int ivlen = 0;\r\nu32 gd_idx = 0;\r\nnum_gd = sg_nents_for_len(src, datalen);\r\nif ((int)num_gd < 0) {\r\ndev_err(dev->core_dev->device, "Invalid number of src SG.\n");\r\nreturn -EINVAL;\r\n}\r\nif (num_gd == 1)\r\nnum_gd = 0;\r\nif (sg_is_last(dst) || ctx->is_hash) {\r\nnum_sd = 0;\r\n} else {\r\nif (datalen > PPC4XX_SD_BUFFER_SIZE) {\r\nnum_sd = datalen / PPC4XX_SD_BUFFER_SIZE;\r\nif (datalen % PPC4XX_SD_BUFFER_SIZE)\r\nnum_sd++;\r\n} else {\r\nnum_sd = 1;\r\n}\r\n}\r\nspin_lock_irqsave(&dev->core_dev->lock, flags);\r\nif (num_gd) {\r\nfst_gd = crypto4xx_get_n_gd(dev, num_gd);\r\nif (fst_gd == ERING_WAS_FULL) {\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn -EAGAIN;\r\n}\r\n}\r\nif (num_sd) {\r\nfst_sd = crypto4xx_get_n_sd(dev, num_sd);\r\nif (fst_sd == ERING_WAS_FULL) {\r\nif (num_gd)\r\ndev->gdr_head = fst_gd;\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn -EAGAIN;\r\n}\r\n}\r\npd_entry = crypto4xx_get_pd_from_pdr_nolock(dev);\r\nif (pd_entry == ERING_WAS_FULL) {\r\nif (num_gd)\r\ndev->gdr_head = fst_gd;\r\nif (num_sd)\r\ndev->sdr_head = fst_sd;\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nspin_unlock_irqrestore(&dev->core_dev->lock, flags);\r\npd_uinfo = (struct pd_uinfo *)(dev->pdr_uinfo +\r\nsizeof(struct pd_uinfo) * pd_entry);\r\npd = crypto4xx_get_pdp(dev, &pd_dma, pd_entry);\r\npd_uinfo->async_req = req;\r\npd_uinfo->num_gd = num_gd;\r\npd_uinfo->num_sd = num_sd;\r\nif (iv_len || ctx->is_hash) {\r\nivlen = iv_len;\r\npd->sa = pd_uinfo->sa_pa;\r\nsa = (struct dynamic_sa_ctl *) pd_uinfo->sa_va;\r\nif (ctx->direction == DIR_INBOUND)\r\nmemcpy(sa, ctx->sa_in, ctx->sa_len * 4);\r\nelse\r\nmemcpy(sa, ctx->sa_out, ctx->sa_len * 4);\r\nmemcpy((void *) sa + ctx->offset_to_sr_ptr,\r\n&pd_uinfo->sr_pa, 4);\r\nif (iv_len)\r\ncrypto4xx_memcpy_le(pd_uinfo->sr_va, iv, iv_len);\r\n} else {\r\nif (ctx->direction == DIR_INBOUND) {\r\npd->sa = ctx->sa_in_dma_addr;\r\nsa = (struct dynamic_sa_ctl *) ctx->sa_in;\r\n} else {\r\npd->sa = ctx->sa_out_dma_addr;\r\nsa = (struct dynamic_sa_ctl *) ctx->sa_out;\r\n}\r\n}\r\npd->sa_len = ctx->sa_len;\r\nif (num_gd) {\r\ngd_idx = fst_gd;\r\npd_uinfo->first_gd = fst_gd;\r\npd_uinfo->num_gd = num_gd;\r\ngd = crypto4xx_get_gdp(dev, &gd_dma, gd_idx);\r\npd->src = gd_dma;\r\nsa->sa_command_0.bf.gather = 1;\r\nidx = 0;\r\nsrc = &src[0];\r\nwhile (nbytes) {\r\nsg = &src[idx];\r\naddr = dma_map_page(dev->core_dev->device, sg_page(sg),\r\nsg->offset, sg->length, DMA_TO_DEVICE);\r\ngd->ptr = addr;\r\ngd->ctl_len.len = sg->length;\r\ngd->ctl_len.done = 0;\r\ngd->ctl_len.ready = 1;\r\nif (sg->length >= nbytes)\r\nbreak;\r\nnbytes -= sg->length;\r\ngd_idx = get_next_gd(gd_idx);\r\ngd = crypto4xx_get_gdp(dev, &gd_dma, gd_idx);\r\nidx++;\r\n}\r\n} else {\r\npd->src = (u32)dma_map_page(dev->core_dev->device, sg_page(src),\r\nsrc->offset, src->length, DMA_TO_DEVICE);\r\nsa->sa_command_0.bf.gather = 0;\r\npd_uinfo->first_gd = 0xffffffff;\r\npd_uinfo->num_gd = 0;\r\n}\r\nif (ctx->is_hash || sg_is_last(dst)) {\r\npd_uinfo->using_sd = 0;\r\npd_uinfo->first_sd = 0xffffffff;\r\npd_uinfo->num_sd = 0;\r\npd_uinfo->dest_va = dst;\r\nsa->sa_command_0.bf.scatter = 0;\r\nif (ctx->is_hash)\r\npd->dest = virt_to_phys((void *)dst);\r\nelse\r\npd->dest = (u32)dma_map_page(dev->core_dev->device,\r\nsg_page(dst), dst->offset,\r\ndst->length, DMA_TO_DEVICE);\r\n} else {\r\nstruct ce_sd *sd = NULL;\r\nu32 sd_idx = fst_sd;\r\nnbytes = datalen;\r\nsa->sa_command_0.bf.scatter = 1;\r\npd_uinfo->using_sd = 1;\r\npd_uinfo->dest_va = dst;\r\npd_uinfo->first_sd = fst_sd;\r\npd_uinfo->num_sd = num_sd;\r\nsd = crypto4xx_get_sdp(dev, &sd_dma, sd_idx);\r\npd->dest = sd_dma;\r\nsd->ctl.done = 0;\r\nsd->ctl.rdy = 1;\r\nidx = 0;\r\nif (nbytes >= PPC4XX_SD_BUFFER_SIZE)\r\nnbytes -= PPC4XX_SD_BUFFER_SIZE;\r\nelse\r\nnbytes = 0;\r\nwhile (nbytes) {\r\nsd_idx = get_next_sd(sd_idx);\r\nsd = crypto4xx_get_sdp(dev, &sd_dma, sd_idx);\r\nsd->ctl.done = 0;\r\nsd->ctl.rdy = 1;\r\nif (nbytes >= PPC4XX_SD_BUFFER_SIZE)\r\nnbytes -= PPC4XX_SD_BUFFER_SIZE;\r\nelse\r\nnbytes = 0;\r\n}\r\n}\r\nsa->sa_command_1.bf.hash_crypto_offset = 0;\r\npd->pd_ctl.w = ctx->pd_ctl;\r\npd->pd_ctl_len.w = 0x00400000 | (ctx->bypass << 24) | datalen;\r\npd_uinfo->state = PD_ENTRY_INUSE;\r\nwmb();\r\nwritel(1, dev->ce_base + CRYPTO4XX_INT_DESCR_RD);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int crypto4xx_alg_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct crypto4xx_alg *amcc_alg = crypto_alg_to_crypto4xx_alg(alg);\r\nstruct crypto4xx_ctx *ctx = crypto_tfm_ctx(tfm);\r\nctx->dev = amcc_alg->dev;\r\nctx->sa_in = NULL;\r\nctx->sa_out = NULL;\r\nctx->sa_in_dma_addr = 0;\r\nctx->sa_out_dma_addr = 0;\r\nctx->sa_len = 0;\r\nswitch (alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {\r\ndefault:\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct crypto4xx_ctx);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct crypto4xx_ctx));\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic void crypto4xx_alg_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto4xx_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncrypto4xx_free_sa(ctx);\r\ncrypto4xx_free_state_record(ctx);\r\n}\r\nint crypto4xx_register_alg(struct crypto4xx_device *sec_dev,\r\nstruct crypto4xx_alg_common *crypto_alg,\r\nint array_size)\r\n{\r\nstruct crypto4xx_alg *alg;\r\nint i;\r\nint rc = 0;\r\nfor (i = 0; i < array_size; i++) {\r\nalg = kzalloc(sizeof(struct crypto4xx_alg), GFP_KERNEL);\r\nif (!alg)\r\nreturn -ENOMEM;\r\nalg->alg = crypto_alg[i];\r\nalg->dev = sec_dev;\r\nswitch (alg->alg.type) {\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nrc = crypto_register_ahash(&alg->alg.u.hash);\r\nbreak;\r\ndefault:\r\nrc = crypto_register_alg(&alg->alg.u.cipher);\r\nbreak;\r\n}\r\nif (rc) {\r\nlist_del(&alg->entry);\r\nkfree(alg);\r\n} else {\r\nlist_add_tail(&alg->entry, &sec_dev->alg_list);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void crypto4xx_unregister_alg(struct crypto4xx_device *sec_dev)\r\n{\r\nstruct crypto4xx_alg *alg, *tmp;\r\nlist_for_each_entry_safe(alg, tmp, &sec_dev->alg_list, entry) {\r\nlist_del(&alg->entry);\r\nswitch (alg->alg.type) {\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\ncrypto_unregister_ahash(&alg->alg.u.hash);\r\nbreak;\r\ndefault:\r\ncrypto_unregister_alg(&alg->alg.u.cipher);\r\n}\r\nkfree(alg);\r\n}\r\n}\r\nstatic void crypto4xx_bh_tasklet_cb(unsigned long data)\r\n{\r\nstruct device *dev = (struct device *)data;\r\nstruct crypto4xx_core_device *core_dev = dev_get_drvdata(dev);\r\nstruct pd_uinfo *pd_uinfo;\r\nstruct ce_pd *pd;\r\nu32 tail;\r\nwhile (core_dev->dev->pdr_head != core_dev->dev->pdr_tail) {\r\ntail = core_dev->dev->pdr_tail;\r\npd_uinfo = core_dev->dev->pdr_uinfo +\r\nsizeof(struct pd_uinfo)*tail;\r\npd = core_dev->dev->pdr + sizeof(struct ce_pd) * tail;\r\nif ((pd_uinfo->state == PD_ENTRY_INUSE) &&\r\npd->pd_ctl.bf.pe_done &&\r\n!pd->pd_ctl.bf.host_ready) {\r\npd->pd_ctl.bf.pe_done = 0;\r\ncrypto4xx_pd_done(core_dev->dev, tail);\r\ncrypto4xx_put_pd_to_pdr(core_dev->dev, tail);\r\npd_uinfo->state = PD_ENTRY_FREE;\r\n} else {\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic irqreturn_t crypto4xx_ce_interrupt_handler(int irq, void *data)\r\n{\r\nstruct device *dev = (struct device *)data;\r\nstruct crypto4xx_core_device *core_dev = dev_get_drvdata(dev);\r\nif (!core_dev->dev->ce_base)\r\nreturn 0;\r\nwritel(PPC4XX_INTERRUPT_CLR,\r\ncore_dev->dev->ce_base + CRYPTO4XX_INT_CLR);\r\ntasklet_schedule(&core_dev->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int crypto4xx_probe(struct platform_device *ofdev)\r\n{\r\nint rc;\r\nstruct resource res;\r\nstruct device *dev = &ofdev->dev;\r\nstruct crypto4xx_core_device *core_dev;\r\nrc = of_address_to_resource(ofdev->dev.of_node, 0, &res);\r\nif (rc)\r\nreturn -ENODEV;\r\nif (of_find_compatible_node(NULL, NULL, "amcc,ppc460ex-crypto")) {\r\nmtdcri(SDR0, PPC460EX_SDR0_SRST,\r\nmfdcri(SDR0, PPC460EX_SDR0_SRST) | PPC460EX_CE_RESET);\r\nmtdcri(SDR0, PPC460EX_SDR0_SRST,\r\nmfdcri(SDR0, PPC460EX_SDR0_SRST) & ~PPC460EX_CE_RESET);\r\n} else if (of_find_compatible_node(NULL, NULL,\r\n"amcc,ppc405ex-crypto")) {\r\nmtdcri(SDR0, PPC405EX_SDR0_SRST,\r\nmfdcri(SDR0, PPC405EX_SDR0_SRST) | PPC405EX_CE_RESET);\r\nmtdcri(SDR0, PPC405EX_SDR0_SRST,\r\nmfdcri(SDR0, PPC405EX_SDR0_SRST) & ~PPC405EX_CE_RESET);\r\n} else if (of_find_compatible_node(NULL, NULL,\r\n"amcc,ppc460sx-crypto")) {\r\nmtdcri(SDR0, PPC460SX_SDR0_SRST,\r\nmfdcri(SDR0, PPC460SX_SDR0_SRST) | PPC460SX_CE_RESET);\r\nmtdcri(SDR0, PPC460SX_SDR0_SRST,\r\nmfdcri(SDR0, PPC460SX_SDR0_SRST) & ~PPC460SX_CE_RESET);\r\n} else {\r\nprintk(KERN_ERR "Crypto Function Not supported!\n");\r\nreturn -EINVAL;\r\n}\r\ncore_dev = kzalloc(sizeof(struct crypto4xx_core_device), GFP_KERNEL);\r\nif (!core_dev)\r\nreturn -ENOMEM;\r\ndev_set_drvdata(dev, core_dev);\r\ncore_dev->ofdev = ofdev;\r\ncore_dev->dev = kzalloc(sizeof(struct crypto4xx_device), GFP_KERNEL);\r\nif (!core_dev->dev)\r\ngoto err_alloc_dev;\r\ncore_dev->dev->core_dev = core_dev;\r\ncore_dev->device = dev;\r\nspin_lock_init(&core_dev->lock);\r\nINIT_LIST_HEAD(&core_dev->dev->alg_list);\r\nrc = crypto4xx_build_pdr(core_dev->dev);\r\nif (rc)\r\ngoto err_build_pdr;\r\nrc = crypto4xx_build_gdr(core_dev->dev);\r\nif (rc)\r\ngoto err_build_gdr;\r\nrc = crypto4xx_build_sdr(core_dev->dev);\r\nif (rc)\r\ngoto err_build_sdr;\r\ntasklet_init(&core_dev->tasklet, crypto4xx_bh_tasklet_cb,\r\n(unsigned long) dev);\r\ncore_dev->irq = irq_of_parse_and_map(ofdev->dev.of_node, 0);\r\nrc = request_irq(core_dev->irq, crypto4xx_ce_interrupt_handler, 0,\r\ncore_dev->dev->name, dev);\r\nif (rc)\r\ngoto err_request_irq;\r\ncore_dev->dev->ce_base = of_iomap(ofdev->dev.of_node, 0);\r\nif (!core_dev->dev->ce_base) {\r\ndev_err(dev, "failed to of_iomap\n");\r\nrc = -ENOMEM;\r\ngoto err_iomap;\r\n}\r\ncrypto4xx_hw_init(core_dev->dev);\r\nrc = crypto4xx_register_alg(core_dev->dev, crypto4xx_alg,\r\nARRAY_SIZE(crypto4xx_alg));\r\nif (rc)\r\ngoto err_start_dev;\r\nreturn 0;\r\nerr_start_dev:\r\niounmap(core_dev->dev->ce_base);\r\nerr_iomap:\r\nfree_irq(core_dev->irq, dev);\r\nerr_request_irq:\r\nirq_dispose_mapping(core_dev->irq);\r\ntasklet_kill(&core_dev->tasklet);\r\ncrypto4xx_destroy_sdr(core_dev->dev);\r\nerr_build_sdr:\r\ncrypto4xx_destroy_gdr(core_dev->dev);\r\nerr_build_gdr:\r\ncrypto4xx_destroy_pdr(core_dev->dev);\r\nerr_build_pdr:\r\nkfree(core_dev->dev);\r\nerr_alloc_dev:\r\nkfree(core_dev);\r\nreturn rc;\r\n}\r\nstatic int crypto4xx_remove(struct platform_device *ofdev)\r\n{\r\nstruct device *dev = &ofdev->dev;\r\nstruct crypto4xx_core_device *core_dev = dev_get_drvdata(dev);\r\nfree_irq(core_dev->irq, dev);\r\nirq_dispose_mapping(core_dev->irq);\r\ntasklet_kill(&core_dev->tasklet);\r\ncrypto4xx_unregister_alg(core_dev->dev);\r\ncrypto4xx_stop_all(core_dev);\r\nreturn 0;\r\n}
