static struct bio *blk_bio_discard_split(struct request_queue *q,\r\nstruct bio *bio,\r\nstruct bio_set *bs,\r\nunsigned *nsegs)\r\n{\r\nunsigned int max_discard_sectors, granularity;\r\nint alignment;\r\nsector_t tmp;\r\nunsigned split_sectors;\r\n*nsegs = 1;\r\ngranularity = max(q->limits.discard_granularity >> 9, 1U);\r\nmax_discard_sectors = min(q->limits.max_discard_sectors, UINT_MAX >> 9);\r\nmax_discard_sectors -= max_discard_sectors % granularity;\r\nif (unlikely(!max_discard_sectors)) {\r\nreturn NULL;\r\n}\r\nif (bio_sectors(bio) <= max_discard_sectors)\r\nreturn NULL;\r\nsplit_sectors = max_discard_sectors;\r\nalignment = (q->limits.discard_alignment >> 9) % granularity;\r\ntmp = bio->bi_iter.bi_sector + split_sectors - alignment;\r\ntmp = sector_div(tmp, granularity);\r\nif (split_sectors > tmp)\r\nsplit_sectors -= tmp;\r\nreturn bio_split(bio, split_sectors, GFP_NOIO, bs);\r\n}\r\nstatic struct bio *blk_bio_write_same_split(struct request_queue *q,\r\nstruct bio *bio,\r\nstruct bio_set *bs,\r\nunsigned *nsegs)\r\n{\r\n*nsegs = 1;\r\nif (!q->limits.max_write_same_sectors)\r\nreturn NULL;\r\nif (bio_sectors(bio) <= q->limits.max_write_same_sectors)\r\nreturn NULL;\r\nreturn bio_split(bio, q->limits.max_write_same_sectors, GFP_NOIO, bs);\r\n}\r\nstatic inline unsigned get_max_io_size(struct request_queue *q,\r\nstruct bio *bio)\r\n{\r\nunsigned sectors = blk_max_size_offset(q, bio->bi_iter.bi_sector);\r\nunsigned mask = queue_logical_block_size(q) - 1;\r\nsectors &= ~(mask >> 9);\r\nreturn sectors;\r\n}\r\nstatic struct bio *blk_bio_segment_split(struct request_queue *q,\r\nstruct bio *bio,\r\nstruct bio_set *bs,\r\nunsigned *segs)\r\n{\r\nstruct bio_vec bv, bvprv, *bvprvp = NULL;\r\nstruct bvec_iter iter;\r\nunsigned seg_size = 0, nsegs = 0, sectors = 0;\r\nunsigned front_seg_size = bio->bi_seg_front_size;\r\nbool do_split = true;\r\nstruct bio *new = NULL;\r\nconst unsigned max_sectors = get_max_io_size(q, bio);\r\nbio_for_each_segment(bv, bio, iter) {\r\nif (bvprvp && bvec_gap_to_prev(q, bvprvp, bv.bv_offset))\r\ngoto split;\r\nif (sectors + (bv.bv_len >> 9) > max_sectors) {\r\nif (nsegs < queue_max_segments(q) &&\r\nsectors < max_sectors) {\r\nnsegs++;\r\nsectors = max_sectors;\r\n}\r\nif (sectors)\r\ngoto split;\r\n}\r\nif (bvprvp && blk_queue_cluster(q)) {\r\nif (seg_size + bv.bv_len > queue_max_segment_size(q))\r\ngoto new_segment;\r\nif (!BIOVEC_PHYS_MERGEABLE(bvprvp, &bv))\r\ngoto new_segment;\r\nif (!BIOVEC_SEG_BOUNDARY(q, bvprvp, &bv))\r\ngoto new_segment;\r\nseg_size += bv.bv_len;\r\nbvprv = bv;\r\nbvprvp = &bvprv;\r\nsectors += bv.bv_len >> 9;\r\nif (nsegs == 1 && seg_size > front_seg_size)\r\nfront_seg_size = seg_size;\r\ncontinue;\r\n}\r\nnew_segment:\r\nif (nsegs == queue_max_segments(q))\r\ngoto split;\r\nnsegs++;\r\nbvprv = bv;\r\nbvprvp = &bvprv;\r\nseg_size = bv.bv_len;\r\nsectors += bv.bv_len >> 9;\r\nif (nsegs == 1 && seg_size > front_seg_size)\r\nfront_seg_size = seg_size;\r\n}\r\ndo_split = false;\r\nsplit:\r\n*segs = nsegs;\r\nif (do_split) {\r\nnew = bio_split(bio, sectors, GFP_NOIO, bs);\r\nif (new)\r\nbio = new;\r\n}\r\nbio->bi_seg_front_size = front_seg_size;\r\nif (seg_size > bio->bi_seg_back_size)\r\nbio->bi_seg_back_size = seg_size;\r\nreturn do_split ? new : NULL;\r\n}\r\nvoid blk_queue_split(struct request_queue *q, struct bio **bio,\r\nstruct bio_set *bs)\r\n{\r\nstruct bio *split, *res;\r\nunsigned nsegs;\r\nif ((*bio)->bi_rw & REQ_DISCARD)\r\nsplit = blk_bio_discard_split(q, *bio, bs, &nsegs);\r\nelse if ((*bio)->bi_rw & REQ_WRITE_SAME)\r\nsplit = blk_bio_write_same_split(q, *bio, bs, &nsegs);\r\nelse\r\nsplit = blk_bio_segment_split(q, *bio, q->bio_split, &nsegs);\r\nres = split ? split : *bio;\r\nres->bi_phys_segments = nsegs;\r\nbio_set_flag(res, BIO_SEG_VALID);\r\nif (split) {\r\nsplit->bi_rw |= REQ_NOMERGE;\r\nbio_chain(split, *bio);\r\ntrace_block_split(q, split, (*bio)->bi_iter.bi_sector);\r\ngeneric_make_request(*bio);\r\n*bio = split;\r\n}\r\n}\r\nstatic unsigned int __blk_recalc_rq_segments(struct request_queue *q,\r\nstruct bio *bio,\r\nbool no_sg_merge)\r\n{\r\nstruct bio_vec bv, bvprv = { NULL };\r\nint cluster, prev = 0;\r\nunsigned int seg_size, nr_phys_segs;\r\nstruct bio *fbio, *bbio;\r\nstruct bvec_iter iter;\r\nif (!bio)\r\nreturn 0;\r\nif (bio->bi_rw & REQ_DISCARD)\r\nreturn 1;\r\nif (bio->bi_rw & REQ_WRITE_SAME)\r\nreturn 1;\r\nfbio = bio;\r\ncluster = blk_queue_cluster(q);\r\nseg_size = 0;\r\nnr_phys_segs = 0;\r\nfor_each_bio(bio) {\r\nbio_for_each_segment(bv, bio, iter) {\r\nif (no_sg_merge)\r\ngoto new_segment;\r\nif (prev && cluster) {\r\nif (seg_size + bv.bv_len\r\n> queue_max_segment_size(q))\r\ngoto new_segment;\r\nif (!BIOVEC_PHYS_MERGEABLE(&bvprv, &bv))\r\ngoto new_segment;\r\nif (!BIOVEC_SEG_BOUNDARY(q, &bvprv, &bv))\r\ngoto new_segment;\r\nseg_size += bv.bv_len;\r\nbvprv = bv;\r\ncontinue;\r\n}\r\nnew_segment:\r\nif (nr_phys_segs == 1 && seg_size >\r\nfbio->bi_seg_front_size)\r\nfbio->bi_seg_front_size = seg_size;\r\nnr_phys_segs++;\r\nbvprv = bv;\r\nprev = 1;\r\nseg_size = bv.bv_len;\r\n}\r\nbbio = bio;\r\n}\r\nif (nr_phys_segs == 1 && seg_size > fbio->bi_seg_front_size)\r\nfbio->bi_seg_front_size = seg_size;\r\nif (seg_size > bbio->bi_seg_back_size)\r\nbbio->bi_seg_back_size = seg_size;\r\nreturn nr_phys_segs;\r\n}\r\nvoid blk_recalc_rq_segments(struct request *rq)\r\n{\r\nbool no_sg_merge = !!test_bit(QUEUE_FLAG_NO_SG_MERGE,\r\n&rq->q->queue_flags);\r\nrq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio,\r\nno_sg_merge);\r\n}\r\nvoid blk_recount_segments(struct request_queue *q, struct bio *bio)\r\n{\r\nunsigned short seg_cnt;\r\nif (bio_flagged(bio, BIO_CLONED))\r\nseg_cnt = bio_segments(bio);\r\nelse\r\nseg_cnt = bio->bi_vcnt;\r\nif (test_bit(QUEUE_FLAG_NO_SG_MERGE, &q->queue_flags) &&\r\n(seg_cnt < queue_max_segments(q)))\r\nbio->bi_phys_segments = seg_cnt;\r\nelse {\r\nstruct bio *nxt = bio->bi_next;\r\nbio->bi_next = NULL;\r\nbio->bi_phys_segments = __blk_recalc_rq_segments(q, bio, false);\r\nbio->bi_next = nxt;\r\n}\r\nbio_set_flag(bio, BIO_SEG_VALID);\r\n}\r\nstatic int blk_phys_contig_segment(struct request_queue *q, struct bio *bio,\r\nstruct bio *nxt)\r\n{\r\nstruct bio_vec end_bv = { NULL }, nxt_bv;\r\nif (!blk_queue_cluster(q))\r\nreturn 0;\r\nif (bio->bi_seg_back_size + nxt->bi_seg_front_size >\r\nqueue_max_segment_size(q))\r\nreturn 0;\r\nif (!bio_has_data(bio))\r\nreturn 1;\r\nbio_get_last_bvec(bio, &end_bv);\r\nbio_get_first_bvec(nxt, &nxt_bv);\r\nif (!BIOVEC_PHYS_MERGEABLE(&end_bv, &nxt_bv))\r\nreturn 0;\r\nif (BIOVEC_SEG_BOUNDARY(q, &end_bv, &nxt_bv))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline void\r\n__blk_segment_map_sg(struct request_queue *q, struct bio_vec *bvec,\r\nstruct scatterlist *sglist, struct bio_vec *bvprv,\r\nstruct scatterlist **sg, int *nsegs, int *cluster)\r\n{\r\nint nbytes = bvec->bv_len;\r\nif (*sg && *cluster) {\r\nif ((*sg)->length + nbytes > queue_max_segment_size(q))\r\ngoto new_segment;\r\nif (!BIOVEC_PHYS_MERGEABLE(bvprv, bvec))\r\ngoto new_segment;\r\nif (!BIOVEC_SEG_BOUNDARY(q, bvprv, bvec))\r\ngoto new_segment;\r\n(*sg)->length += nbytes;\r\n} else {\r\nnew_segment:\r\nif (!*sg)\r\n*sg = sglist;\r\nelse {\r\nsg_unmark_end(*sg);\r\n*sg = sg_next(*sg);\r\n}\r\nsg_set_page(*sg, bvec->bv_page, nbytes, bvec->bv_offset);\r\n(*nsegs)++;\r\n}\r\n*bvprv = *bvec;\r\n}\r\nstatic int __blk_bios_map_sg(struct request_queue *q, struct bio *bio,\r\nstruct scatterlist *sglist,\r\nstruct scatterlist **sg)\r\n{\r\nstruct bio_vec bvec, bvprv = { NULL };\r\nstruct bvec_iter iter;\r\nint nsegs, cluster;\r\nnsegs = 0;\r\ncluster = blk_queue_cluster(q);\r\nif (bio->bi_rw & REQ_DISCARD) {\r\nif (bio->bi_vcnt)\r\ngoto single_segment;\r\nreturn 0;\r\n}\r\nif (bio->bi_rw & REQ_WRITE_SAME) {\r\nsingle_segment:\r\n*sg = sglist;\r\nbvec = bio_iovec(bio);\r\nsg_set_page(*sg, bvec.bv_page, bvec.bv_len, bvec.bv_offset);\r\nreturn 1;\r\n}\r\nfor_each_bio(bio)\r\nbio_for_each_segment(bvec, bio, iter)\r\n__blk_segment_map_sg(q, &bvec, sglist, &bvprv, sg,\r\n&nsegs, &cluster);\r\nreturn nsegs;\r\n}\r\nint blk_rq_map_sg(struct request_queue *q, struct request *rq,\r\nstruct scatterlist *sglist)\r\n{\r\nstruct scatterlist *sg = NULL;\r\nint nsegs = 0;\r\nif (rq->bio)\r\nnsegs = __blk_bios_map_sg(q, rq->bio, sglist, &sg);\r\nif (unlikely(rq->cmd_flags & REQ_COPY_USER) &&\r\n(blk_rq_bytes(rq) & q->dma_pad_mask)) {\r\nunsigned int pad_len =\r\n(q->dma_pad_mask & ~blk_rq_bytes(rq)) + 1;\r\nsg->length += pad_len;\r\nrq->extra_len += pad_len;\r\n}\r\nif (q->dma_drain_size && q->dma_drain_needed(rq)) {\r\nif (rq->cmd_flags & REQ_WRITE)\r\nmemset(q->dma_drain_buffer, 0, q->dma_drain_size);\r\nsg_unmark_end(sg);\r\nsg = sg_next(sg);\r\nsg_set_page(sg, virt_to_page(q->dma_drain_buffer),\r\nq->dma_drain_size,\r\n((unsigned long)q->dma_drain_buffer) &\r\n(PAGE_SIZE - 1));\r\nnsegs++;\r\nrq->extra_len += q->dma_drain_size;\r\n}\r\nif (sg)\r\nsg_mark_end(sg);\r\nWARN_ON(nsegs > rq->nr_phys_segments);\r\nreturn nsegs;\r\n}\r\nstatic inline int ll_new_hw_segment(struct request_queue *q,\r\nstruct request *req,\r\nstruct bio *bio)\r\n{\r\nint nr_phys_segs = bio_phys_segments(q, bio);\r\nif (req->nr_phys_segments + nr_phys_segs > queue_max_segments(q))\r\ngoto no_merge;\r\nif (blk_integrity_merge_bio(q, req, bio) == false)\r\ngoto no_merge;\r\nreq->nr_phys_segments += nr_phys_segs;\r\nreturn 1;\r\nno_merge:\r\nreq->cmd_flags |= REQ_NOMERGE;\r\nif (req == q->last_merge)\r\nq->last_merge = NULL;\r\nreturn 0;\r\n}\r\nint ll_back_merge_fn(struct request_queue *q, struct request *req,\r\nstruct bio *bio)\r\n{\r\nif (req_gap_back_merge(req, bio))\r\nreturn 0;\r\nif (blk_integrity_rq(req) &&\r\nintegrity_req_gap_back_merge(req, bio))\r\nreturn 0;\r\nif (blk_rq_sectors(req) + bio_sectors(bio) >\r\nblk_rq_get_max_sectors(req)) {\r\nreq->cmd_flags |= REQ_NOMERGE;\r\nif (req == q->last_merge)\r\nq->last_merge = NULL;\r\nreturn 0;\r\n}\r\nif (!bio_flagged(req->biotail, BIO_SEG_VALID))\r\nblk_recount_segments(q, req->biotail);\r\nif (!bio_flagged(bio, BIO_SEG_VALID))\r\nblk_recount_segments(q, bio);\r\nreturn ll_new_hw_segment(q, req, bio);\r\n}\r\nint ll_front_merge_fn(struct request_queue *q, struct request *req,\r\nstruct bio *bio)\r\n{\r\nif (req_gap_front_merge(req, bio))\r\nreturn 0;\r\nif (blk_integrity_rq(req) &&\r\nintegrity_req_gap_front_merge(req, bio))\r\nreturn 0;\r\nif (blk_rq_sectors(req) + bio_sectors(bio) >\r\nblk_rq_get_max_sectors(req)) {\r\nreq->cmd_flags |= REQ_NOMERGE;\r\nif (req == q->last_merge)\r\nq->last_merge = NULL;\r\nreturn 0;\r\n}\r\nif (!bio_flagged(bio, BIO_SEG_VALID))\r\nblk_recount_segments(q, bio);\r\nif (!bio_flagged(req->bio, BIO_SEG_VALID))\r\nblk_recount_segments(q, req->bio);\r\nreturn ll_new_hw_segment(q, req, bio);\r\n}\r\nstatic bool req_no_special_merge(struct request *req)\r\n{\r\nstruct request_queue *q = req->q;\r\nreturn !q->mq_ops && req->special;\r\n}\r\nstatic int ll_merge_requests_fn(struct request_queue *q, struct request *req,\r\nstruct request *next)\r\n{\r\nint total_phys_segments;\r\nunsigned int seg_size =\r\nreq->biotail->bi_seg_back_size + next->bio->bi_seg_front_size;\r\nif (req_no_special_merge(req) || req_no_special_merge(next))\r\nreturn 0;\r\nif (req_gap_back_merge(req, next->bio))\r\nreturn 0;\r\nif ((blk_rq_sectors(req) + blk_rq_sectors(next)) >\r\nblk_rq_get_max_sectors(req))\r\nreturn 0;\r\ntotal_phys_segments = req->nr_phys_segments + next->nr_phys_segments;\r\nif (blk_phys_contig_segment(q, req->biotail, next->bio)) {\r\nif (req->nr_phys_segments == 1)\r\nreq->bio->bi_seg_front_size = seg_size;\r\nif (next->nr_phys_segments == 1)\r\nnext->biotail->bi_seg_back_size = seg_size;\r\ntotal_phys_segments--;\r\n}\r\nif (total_phys_segments > queue_max_segments(q))\r\nreturn 0;\r\nif (blk_integrity_merge_rq(q, req, next) == false)\r\nreturn 0;\r\nreq->nr_phys_segments = total_phys_segments;\r\nreturn 1;\r\n}\r\nvoid blk_rq_set_mixed_merge(struct request *rq)\r\n{\r\nunsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;\r\nstruct bio *bio;\r\nif (rq->cmd_flags & REQ_MIXED_MERGE)\r\nreturn;\r\nfor (bio = rq->bio; bio; bio = bio->bi_next) {\r\nWARN_ON_ONCE((bio->bi_rw & REQ_FAILFAST_MASK) &&\r\n(bio->bi_rw & REQ_FAILFAST_MASK) != ff);\r\nbio->bi_rw |= ff;\r\n}\r\nrq->cmd_flags |= REQ_MIXED_MERGE;\r\n}\r\nstatic void blk_account_io_merge(struct request *req)\r\n{\r\nif (blk_do_io_stat(req)) {\r\nstruct hd_struct *part;\r\nint cpu;\r\ncpu = part_stat_lock();\r\npart = req->part;\r\npart_round_stats(cpu, part);\r\npart_dec_in_flight(part, rq_data_dir(req));\r\nhd_struct_put(part);\r\npart_stat_unlock();\r\n}\r\n}\r\nstatic int attempt_merge(struct request_queue *q, struct request *req,\r\nstruct request *next)\r\n{\r\nif (!rq_mergeable(req) || !rq_mergeable(next))\r\nreturn 0;\r\nif (!blk_check_merge_flags(req->cmd_flags, next->cmd_flags))\r\nreturn 0;\r\nif (blk_rq_pos(req) + blk_rq_sectors(req) != blk_rq_pos(next))\r\nreturn 0;\r\nif (rq_data_dir(req) != rq_data_dir(next)\r\n|| req->rq_disk != next->rq_disk\r\n|| req_no_special_merge(next))\r\nreturn 0;\r\nif (req->cmd_flags & REQ_WRITE_SAME &&\r\n!blk_write_same_mergeable(req->bio, next->bio))\r\nreturn 0;\r\nif (!ll_merge_requests_fn(q, req, next))\r\nreturn 0;\r\nif ((req->cmd_flags | next->cmd_flags) & REQ_MIXED_MERGE ||\r\n(req->cmd_flags & REQ_FAILFAST_MASK) !=\r\n(next->cmd_flags & REQ_FAILFAST_MASK)) {\r\nblk_rq_set_mixed_merge(req);\r\nblk_rq_set_mixed_merge(next);\r\n}\r\nif (time_after(req->start_time, next->start_time))\r\nreq->start_time = next->start_time;\r\nreq->biotail->bi_next = next->bio;\r\nreq->biotail = next->biotail;\r\nreq->__data_len += blk_rq_bytes(next);\r\nelv_merge_requests(q, req, next);\r\nblk_account_io_merge(next);\r\nreq->ioprio = ioprio_best(req->ioprio, next->ioprio);\r\nif (blk_rq_cpu_valid(next))\r\nreq->cpu = next->cpu;\r\nnext->bio = NULL;\r\n__blk_put_request(q, next);\r\nreturn 1;\r\n}\r\nint attempt_back_merge(struct request_queue *q, struct request *rq)\r\n{\r\nstruct request *next = elv_latter_request(q, rq);\r\nif (next)\r\nreturn attempt_merge(q, rq, next);\r\nreturn 0;\r\n}\r\nint attempt_front_merge(struct request_queue *q, struct request *rq)\r\n{\r\nstruct request *prev = elv_former_request(q, rq);\r\nif (prev)\r\nreturn attempt_merge(q, prev, rq);\r\nreturn 0;\r\n}\r\nint blk_attempt_req_merge(struct request_queue *q, struct request *rq,\r\nstruct request *next)\r\n{\r\nreturn attempt_merge(q, rq, next);\r\n}\r\nbool blk_rq_merge_ok(struct request *rq, struct bio *bio)\r\n{\r\nif (!rq_mergeable(rq) || !bio_mergeable(bio))\r\nreturn false;\r\nif (!blk_check_merge_flags(rq->cmd_flags, bio->bi_rw))\r\nreturn false;\r\nif (bio_data_dir(bio) != rq_data_dir(rq))\r\nreturn false;\r\nif (rq->rq_disk != bio->bi_bdev->bd_disk || req_no_special_merge(rq))\r\nreturn false;\r\nif (blk_integrity_merge_bio(rq->q, rq, bio) == false)\r\nreturn false;\r\nif (rq->cmd_flags & REQ_WRITE_SAME &&\r\n!blk_write_same_mergeable(rq->bio, bio))\r\nreturn false;\r\nreturn true;\r\n}\r\nint blk_try_merge(struct request *rq, struct bio *bio)\r\n{\r\nif (blk_rq_pos(rq) + blk_rq_sectors(rq) == bio->bi_iter.bi_sector)\r\nreturn ELEVATOR_BACK_MERGE;\r\nelse if (blk_rq_pos(rq) - bio_sectors(bio) == bio->bi_iter.bi_sector)\r\nreturn ELEVATOR_FRONT_MERGE;\r\nreturn ELEVATOR_NO_MERGE;\r\n}
