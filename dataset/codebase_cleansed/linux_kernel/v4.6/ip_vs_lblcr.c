static void ip_vs_dest_set_insert(struct ip_vs_dest_set *set,\r\nstruct ip_vs_dest *dest, bool check)\r\n{\r\nstruct ip_vs_dest_set_elem *e;\r\nif (check) {\r\nlist_for_each_entry(e, &set->list, list) {\r\nif (e->dest == dest)\r\nreturn;\r\n}\r\n}\r\ne = kmalloc(sizeof(*e), GFP_ATOMIC);\r\nif (e == NULL)\r\nreturn;\r\nip_vs_dest_hold(dest);\r\ne->dest = dest;\r\nlist_add_rcu(&e->list, &set->list);\r\natomic_inc(&set->size);\r\nset->lastmod = jiffies;\r\n}\r\nstatic void ip_vs_lblcr_elem_rcu_free(struct rcu_head *head)\r\n{\r\nstruct ip_vs_dest_set_elem *e;\r\ne = container_of(head, struct ip_vs_dest_set_elem, rcu_head);\r\nip_vs_dest_put_and_free(e->dest);\r\nkfree(e);\r\n}\r\nstatic void\r\nip_vs_dest_set_erase(struct ip_vs_dest_set *set, struct ip_vs_dest *dest)\r\n{\r\nstruct ip_vs_dest_set_elem *e;\r\nlist_for_each_entry(e, &set->list, list) {\r\nif (e->dest == dest) {\r\natomic_dec(&set->size);\r\nset->lastmod = jiffies;\r\nlist_del_rcu(&e->list);\r\ncall_rcu(&e->rcu_head, ip_vs_lblcr_elem_rcu_free);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void ip_vs_dest_set_eraseall(struct ip_vs_dest_set *set)\r\n{\r\nstruct ip_vs_dest_set_elem *e, *ep;\r\nlist_for_each_entry_safe(e, ep, &set->list, list) {\r\nlist_del_rcu(&e->list);\r\ncall_rcu(&e->rcu_head, ip_vs_lblcr_elem_rcu_free);\r\n}\r\n}\r\nstatic inline struct ip_vs_dest *ip_vs_dest_set_min(struct ip_vs_dest_set *set)\r\n{\r\nregister struct ip_vs_dest_set_elem *e;\r\nstruct ip_vs_dest *dest, *least;\r\nint loh, doh;\r\nlist_for_each_entry_rcu(e, &set->list, list) {\r\nleast = e->dest;\r\nif (least->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\nif ((atomic_read(&least->weight) > 0)\r\n&& (least->flags & IP_VS_DEST_F_AVAILABLE)) {\r\nloh = ip_vs_dest_conn_overhead(least);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry_continue_rcu(e, &set->list, list) {\r\ndest = e->dest;\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif (((__s64)loh * atomic_read(&dest->weight) >\r\n(__s64)doh * atomic_read(&least->weight))\r\n&& (dest->flags & IP_VS_DEST_F_AVAILABLE)) {\r\nleast = dest;\r\nloh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "%s(): server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\n__func__,\r\nIP_VS_DBG_ADDR(least->af, &least->addr),\r\nntohs(least->port),\r\natomic_read(&least->activeconns),\r\natomic_read(&least->refcnt),\r\natomic_read(&least->weight), loh);\r\nreturn least;\r\n}\r\nstatic inline struct ip_vs_dest *ip_vs_dest_set_max(struct ip_vs_dest_set *set)\r\n{\r\nregister struct ip_vs_dest_set_elem *e;\r\nstruct ip_vs_dest *dest, *most;\r\nint moh, doh;\r\nif (set == NULL)\r\nreturn NULL;\r\nlist_for_each_entry(e, &set->list, list) {\r\nmost = e->dest;\r\nif (atomic_read(&most->weight) > 0) {\r\nmoh = ip_vs_dest_conn_overhead(most);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry_continue(e, &set->list, list) {\r\ndest = e->dest;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif (((__s64)moh * atomic_read(&dest->weight) <\r\n(__s64)doh * atomic_read(&most->weight))\r\n&& (atomic_read(&dest->weight) > 0)) {\r\nmost = dest;\r\nmoh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "%s(): server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\n__func__,\r\nIP_VS_DBG_ADDR(most->af, &most->addr), ntohs(most->port),\r\natomic_read(&most->activeconns),\r\natomic_read(&most->refcnt),\r\natomic_read(&most->weight), moh);\r\nreturn most;\r\n}\r\nstatic inline void ip_vs_lblcr_free(struct ip_vs_lblcr_entry *en)\r\n{\r\nhlist_del_rcu(&en->list);\r\nip_vs_dest_set_eraseall(&en->set);\r\nkfree_rcu(en, rcu_head);\r\n}\r\nstatic inline unsigned int\r\nip_vs_lblcr_hashkey(int af, const union nf_inet_addr *addr)\r\n{\r\n__be32 addr_fold = addr->ip;\r\n#ifdef CONFIG_IP_VS_IPV6\r\nif (af == AF_INET6)\r\naddr_fold = addr->ip6[0]^addr->ip6[1]^\r\naddr->ip6[2]^addr->ip6[3];\r\n#endif\r\nreturn (ntohl(addr_fold)*2654435761UL) & IP_VS_LBLCR_TAB_MASK;\r\n}\r\nstatic void\r\nip_vs_lblcr_hash(struct ip_vs_lblcr_table *tbl, struct ip_vs_lblcr_entry *en)\r\n{\r\nunsigned int hash = ip_vs_lblcr_hashkey(en->af, &en->addr);\r\nhlist_add_head_rcu(&en->list, &tbl->bucket[hash]);\r\natomic_inc(&tbl->entries);\r\n}\r\nstatic inline struct ip_vs_lblcr_entry *\r\nip_vs_lblcr_get(int af, struct ip_vs_lblcr_table *tbl,\r\nconst union nf_inet_addr *addr)\r\n{\r\nunsigned int hash = ip_vs_lblcr_hashkey(af, addr);\r\nstruct ip_vs_lblcr_entry *en;\r\nhlist_for_each_entry_rcu(en, &tbl->bucket[hash], list)\r\nif (ip_vs_addr_equal(af, &en->addr, addr))\r\nreturn en;\r\nreturn NULL;\r\n}\r\nstatic inline struct ip_vs_lblcr_entry *\r\nip_vs_lblcr_new(struct ip_vs_lblcr_table *tbl, const union nf_inet_addr *daddr,\r\nu16 af, struct ip_vs_dest *dest)\r\n{\r\nstruct ip_vs_lblcr_entry *en;\r\nen = ip_vs_lblcr_get(af, tbl, daddr);\r\nif (!en) {\r\nen = kmalloc(sizeof(*en), GFP_ATOMIC);\r\nif (!en)\r\nreturn NULL;\r\nen->af = af;\r\nip_vs_addr_copy(af, &en->addr, daddr);\r\nen->lastuse = jiffies;\r\natomic_set(&(en->set.size), 0);\r\nINIT_LIST_HEAD(&en->set.list);\r\nip_vs_dest_set_insert(&en->set, dest, false);\r\nip_vs_lblcr_hash(tbl, en);\r\nreturn en;\r\n}\r\nip_vs_dest_set_insert(&en->set, dest, true);\r\nreturn en;\r\n}\r\nstatic void ip_vs_lblcr_flush(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\nint i;\r\nstruct ip_vs_lblcr_entry *en;\r\nstruct hlist_node *next;\r\nspin_lock_bh(&svc->sched_lock);\r\ntbl->dead = 1;\r\nfor (i = 0; i < IP_VS_LBLCR_TAB_SIZE; i++) {\r\nhlist_for_each_entry_safe(en, next, &tbl->bucket[i], list) {\r\nip_vs_lblcr_free(en);\r\n}\r\n}\r\nspin_unlock_bh(&svc->sched_lock);\r\n}\r\nstatic int sysctl_lblcr_expiration(struct ip_vs_service *svc)\r\n{\r\n#ifdef CONFIG_SYSCTL\r\nreturn svc->ipvs->sysctl_lblcr_expiration;\r\n#else\r\nreturn DEFAULT_EXPIRATION;\r\n#endif\r\n}\r\nstatic inline void ip_vs_lblcr_full_check(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\nunsigned long now = jiffies;\r\nint i, j;\r\nstruct ip_vs_lblcr_entry *en;\r\nstruct hlist_node *next;\r\nfor (i = 0, j = tbl->rover; i < IP_VS_LBLCR_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLCR_TAB_MASK;\r\nspin_lock(&svc->sched_lock);\r\nhlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {\r\nif (time_after(en->lastuse +\r\nsysctl_lblcr_expiration(svc), now))\r\ncontinue;\r\nip_vs_lblcr_free(en);\r\natomic_dec(&tbl->entries);\r\n}\r\nspin_unlock(&svc->sched_lock);\r\n}\r\ntbl->rover = j;\r\n}\r\nstatic void ip_vs_lblcr_check_expire(unsigned long data)\r\n{\r\nstruct ip_vs_service *svc = (struct ip_vs_service *) data;\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\nunsigned long now = jiffies;\r\nint goal;\r\nint i, j;\r\nstruct ip_vs_lblcr_entry *en;\r\nstruct hlist_node *next;\r\nif ((tbl->counter % COUNT_FOR_FULL_EXPIRATION) == 0) {\r\nip_vs_lblcr_full_check(svc);\r\ntbl->counter = 1;\r\ngoto out;\r\n}\r\nif (atomic_read(&tbl->entries) <= tbl->max_size) {\r\ntbl->counter++;\r\ngoto out;\r\n}\r\ngoal = (atomic_read(&tbl->entries) - tbl->max_size)*4/3;\r\nif (goal > tbl->max_size/2)\r\ngoal = tbl->max_size/2;\r\nfor (i = 0, j = tbl->rover; i < IP_VS_LBLCR_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLCR_TAB_MASK;\r\nspin_lock(&svc->sched_lock);\r\nhlist_for_each_entry_safe(en, next, &tbl->bucket[j], list) {\r\nif (time_before(now, en->lastuse+ENTRY_TIMEOUT))\r\ncontinue;\r\nip_vs_lblcr_free(en);\r\natomic_dec(&tbl->entries);\r\ngoal--;\r\n}\r\nspin_unlock(&svc->sched_lock);\r\nif (goal <= 0)\r\nbreak;\r\n}\r\ntbl->rover = j;\r\nout:\r\nmod_timer(&tbl->periodic_timer, jiffies+CHECK_EXPIRE_INTERVAL);\r\n}\r\nstatic int ip_vs_lblcr_init_svc(struct ip_vs_service *svc)\r\n{\r\nint i;\r\nstruct ip_vs_lblcr_table *tbl;\r\ntbl = kmalloc(sizeof(*tbl), GFP_KERNEL);\r\nif (tbl == NULL)\r\nreturn -ENOMEM;\r\nsvc->sched_data = tbl;\r\nIP_VS_DBG(6, "LBLCR hash table (memory=%Zdbytes) allocated for "\r\n"current service\n", sizeof(*tbl));\r\nfor (i = 0; i < IP_VS_LBLCR_TAB_SIZE; i++) {\r\nINIT_HLIST_HEAD(&tbl->bucket[i]);\r\n}\r\ntbl->max_size = IP_VS_LBLCR_TAB_SIZE*16;\r\ntbl->rover = 0;\r\ntbl->counter = 1;\r\ntbl->dead = 0;\r\nsetup_timer(&tbl->periodic_timer, ip_vs_lblcr_check_expire,\r\n(unsigned long)svc);\r\nmod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);\r\nreturn 0;\r\n}\r\nstatic void ip_vs_lblcr_done_svc(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\ndel_timer_sync(&tbl->periodic_timer);\r\nip_vs_lblcr_flush(svc);\r\nkfree_rcu(tbl, rcu_head);\r\nIP_VS_DBG(6, "LBLCR hash table (memory=%Zdbytes) released\n",\r\nsizeof(*tbl));\r\n}\r\nstatic inline struct ip_vs_dest *\r\n__ip_vs_lblcr_schedule(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_dest *dest, *least;\r\nint loh, doh;\r\nlist_for_each_entry_rcu(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\nif (atomic_read(&dest->weight) > 0) {\r\nleast = dest;\r\nloh = ip_vs_dest_conn_overhead(least);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry_continue_rcu(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif ((__s64)loh * atomic_read(&dest->weight) >\r\n(__s64)doh * atomic_read(&least->weight)) {\r\nleast = dest;\r\nloh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "LBLCR: server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\nIP_VS_DBG_ADDR(least->af, &least->addr),\r\nntohs(least->port),\r\natomic_read(&least->activeconns),\r\natomic_read(&least->refcnt),\r\natomic_read(&least->weight), loh);\r\nreturn least;\r\n}\r\nstatic inline int\r\nis_overloaded(struct ip_vs_dest *dest, struct ip_vs_service *svc)\r\n{\r\nif (atomic_read(&dest->activeconns) > atomic_read(&dest->weight)) {\r\nstruct ip_vs_dest *d;\r\nlist_for_each_entry_rcu(d, &svc->destinations, n_list) {\r\nif (atomic_read(&d->activeconns)*2\r\n< atomic_read(&d->weight)) {\r\nreturn 1;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic struct ip_vs_dest *\r\nip_vs_lblcr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb,\r\nstruct ip_vs_iphdr *iph)\r\n{\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\nstruct ip_vs_dest *dest;\r\nstruct ip_vs_lblcr_entry *en;\r\nIP_VS_DBG(6, "%s(): Scheduling...\n", __func__);\r\nen = ip_vs_lblcr_get(svc->af, tbl, &iph->daddr);\r\nif (en) {\r\nen->lastuse = jiffies;\r\ndest = ip_vs_dest_set_min(&en->set);\r\nif (atomic_read(&en->set.size) > 1 &&\r\ntime_after(jiffies, en->set.lastmod +\r\nsysctl_lblcr_expiration(svc))) {\r\nspin_lock_bh(&svc->sched_lock);\r\nif (atomic_read(&en->set.size) > 1) {\r\nstruct ip_vs_dest *m;\r\nm = ip_vs_dest_set_max(&en->set);\r\nif (m)\r\nip_vs_dest_set_erase(&en->set, m);\r\n}\r\nspin_unlock_bh(&svc->sched_lock);\r\n}\r\nif (dest && !is_overloaded(dest, svc))\r\ngoto out;\r\ndest = __ip_vs_lblcr_schedule(svc);\r\nif (!dest) {\r\nip_vs_scheduler_err(svc, "no destination available");\r\nreturn NULL;\r\n}\r\nspin_lock_bh(&svc->sched_lock);\r\nif (!tbl->dead)\r\nip_vs_dest_set_insert(&en->set, dest, true);\r\nspin_unlock_bh(&svc->sched_lock);\r\ngoto out;\r\n}\r\ndest = __ip_vs_lblcr_schedule(svc);\r\nif (!dest) {\r\nIP_VS_DBG(1, "no destination available\n");\r\nreturn NULL;\r\n}\r\nspin_lock_bh(&svc->sched_lock);\r\nif (!tbl->dead)\r\nip_vs_lblcr_new(tbl, &iph->daddr, svc->af, dest);\r\nspin_unlock_bh(&svc->sched_lock);\r\nout:\r\nIP_VS_DBG_BUF(6, "LBLCR: destination IP address %s --> server %s:%d\n",\r\nIP_VS_DBG_ADDR(svc->af, &iph->daddr),\r\nIP_VS_DBG_ADDR(dest->af, &dest->addr), ntohs(dest->port));\r\nreturn dest;\r\n}\r\nstatic int __net_init __ip_vs_lblcr_init(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nif (!ipvs)\r\nreturn -ENOENT;\r\nif (!net_eq(net, &init_net)) {\r\nipvs->lblcr_ctl_table = kmemdup(vs_vars_table,\r\nsizeof(vs_vars_table),\r\nGFP_KERNEL);\r\nif (ipvs->lblcr_ctl_table == NULL)\r\nreturn -ENOMEM;\r\nif (net->user_ns != &init_user_ns)\r\nipvs->lblcr_ctl_table[0].procname = NULL;\r\n} else\r\nipvs->lblcr_ctl_table = vs_vars_table;\r\nipvs->sysctl_lblcr_expiration = DEFAULT_EXPIRATION;\r\nipvs->lblcr_ctl_table[0].data = &ipvs->sysctl_lblcr_expiration;\r\nipvs->lblcr_ctl_header =\r\nregister_net_sysctl(net, "net/ipv4/vs", ipvs->lblcr_ctl_table);\r\nif (!ipvs->lblcr_ctl_header) {\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblcr_ctl_table);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __net_exit __ip_vs_lblcr_exit(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nunregister_net_sysctl_table(ipvs->lblcr_ctl_header);\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblcr_ctl_table);\r\n}\r\nstatic int __net_init __ip_vs_lblcr_init(struct net *net) { return 0; }\r\nstatic void __net_exit __ip_vs_lblcr_exit(struct net *net) { }\r\nstatic int __init ip_vs_lblcr_init(void)\r\n{\r\nint ret;\r\nret = register_pernet_subsys(&ip_vs_lblcr_ops);\r\nif (ret)\r\nreturn ret;\r\nret = register_ip_vs_scheduler(&ip_vs_lblcr_scheduler);\r\nif (ret)\r\nunregister_pernet_subsys(&ip_vs_lblcr_ops);\r\nreturn ret;\r\n}\r\nstatic void __exit ip_vs_lblcr_cleanup(void)\r\n{\r\nunregister_ip_vs_scheduler(&ip_vs_lblcr_scheduler);\r\nunregister_pernet_subsys(&ip_vs_lblcr_ops);\r\nrcu_barrier();\r\n}
