struct fld_cache *fld_cache_init(const char *name,\r\nint cache_size, int cache_threshold)\r\n{\r\nstruct fld_cache *cache;\r\nLASSERT(name);\r\nLASSERT(cache_threshold < cache_size);\r\ncache = kzalloc(sizeof(*cache), GFP_NOFS);\r\nif (!cache)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_LIST_HEAD(&cache->fci_entries_head);\r\nINIT_LIST_HEAD(&cache->fci_lru);\r\ncache->fci_cache_count = 0;\r\nrwlock_init(&cache->fci_lock);\r\nstrlcpy(cache->fci_name, name,\r\nsizeof(cache->fci_name));\r\ncache->fci_cache_size = cache_size;\r\ncache->fci_threshold = cache_threshold;\r\nmemset(&cache->fci_stat, 0, sizeof(cache->fci_stat));\r\nCDEBUG(D_INFO, "%s: FLD cache - Size: %d, Threshold: %d\n",\r\ncache->fci_name, cache_size, cache_threshold);\r\nreturn cache;\r\n}\r\nvoid fld_cache_fini(struct fld_cache *cache)\r\n{\r\n__u64 pct;\r\nLASSERT(cache);\r\nfld_cache_flush(cache);\r\nif (cache->fci_stat.fst_count > 0) {\r\npct = cache->fci_stat.fst_cache * 100;\r\ndo_div(pct, cache->fci_stat.fst_count);\r\n} else {\r\npct = 0;\r\n}\r\nCDEBUG(D_INFO, "FLD cache statistics (%s):\n", cache->fci_name);\r\nCDEBUG(D_INFO, " Total reqs: %llu\n", cache->fci_stat.fst_count);\r\nCDEBUG(D_INFO, " Cache reqs: %llu\n", cache->fci_stat.fst_cache);\r\nCDEBUG(D_INFO, " Cache hits: %llu%%\n", pct);\r\nkfree(cache);\r\n}\r\nstatic void fld_cache_entry_delete(struct fld_cache *cache,\r\nstruct fld_cache_entry *node)\r\n{\r\nlist_del(&node->fce_list);\r\nlist_del(&node->fce_lru);\r\ncache->fci_cache_count--;\r\nkfree(node);\r\n}\r\nstatic void fld_fix_new_list(struct fld_cache *cache)\r\n{\r\nstruct fld_cache_entry *f_curr;\r\nstruct fld_cache_entry *f_next;\r\nstruct lu_seq_range *c_range;\r\nstruct lu_seq_range *n_range;\r\nstruct list_head *head = &cache->fci_entries_head;\r\nrestart_fixup:\r\nlist_for_each_entry_safe(f_curr, f_next, head, fce_list) {\r\nc_range = &f_curr->fce_range;\r\nn_range = &f_next->fce_range;\r\nLASSERT(lu_seq_range_is_sane(c_range));\r\nif (&f_next->fce_list == head)\r\nbreak;\r\nif (c_range->lsr_flags != n_range->lsr_flags)\r\ncontinue;\r\nLASSERTF(c_range->lsr_start <= n_range->lsr_start,\r\n"cur lsr_start "DRANGE" next lsr_start "DRANGE"\n",\r\nPRANGE(c_range), PRANGE(n_range));\r\nif (c_range->lsr_end == n_range->lsr_start) {\r\nif (c_range->lsr_index != n_range->lsr_index)\r\ncontinue;\r\nn_range->lsr_start = c_range->lsr_start;\r\nfld_cache_entry_delete(cache, f_curr);\r\ncontinue;\r\n}\r\nif (n_range->lsr_start < c_range->lsr_end) {\r\nif (c_range->lsr_index == n_range->lsr_index) {\r\nn_range->lsr_start = c_range->lsr_start;\r\nn_range->lsr_end = max(c_range->lsr_end,\r\nn_range->lsr_end);\r\nfld_cache_entry_delete(cache, f_curr);\r\n} else {\r\nif (n_range->lsr_end <= c_range->lsr_end) {\r\n*n_range = *c_range;\r\nfld_cache_entry_delete(cache, f_curr);\r\n} else {\r\nn_range->lsr_start = c_range->lsr_end;\r\n}\r\n}\r\ngoto restart_fixup;\r\n}\r\nif (c_range->lsr_start == n_range->lsr_start &&\r\nc_range->lsr_end == n_range->lsr_end)\r\nfld_cache_entry_delete(cache, f_curr);\r\n}\r\n}\r\nstatic inline void fld_cache_entry_add(struct fld_cache *cache,\r\nstruct fld_cache_entry *f_new,\r\nstruct list_head *pos)\r\n{\r\nlist_add(&f_new->fce_list, pos);\r\nlist_add(&f_new->fce_lru, &cache->fci_lru);\r\ncache->fci_cache_count++;\r\nfld_fix_new_list(cache);\r\n}\r\nstatic int fld_cache_shrink(struct fld_cache *cache)\r\n{\r\nstruct fld_cache_entry *flde;\r\nstruct list_head *curr;\r\nint num = 0;\r\nif (cache->fci_cache_count < cache->fci_cache_size)\r\nreturn 0;\r\ncurr = cache->fci_lru.prev;\r\nwhile (cache->fci_cache_count + cache->fci_threshold >\r\ncache->fci_cache_size && curr != &cache->fci_lru) {\r\nflde = list_entry(curr, struct fld_cache_entry, fce_lru);\r\ncurr = curr->prev;\r\nfld_cache_entry_delete(cache, flde);\r\nnum++;\r\n}\r\nCDEBUG(D_INFO, "%s: FLD cache - Shrunk by %d entries\n",\r\ncache->fci_name, num);\r\nreturn 0;\r\n}\r\nvoid fld_cache_flush(struct fld_cache *cache)\r\n{\r\nwrite_lock(&cache->fci_lock);\r\ncache->fci_cache_size = 0;\r\nfld_cache_shrink(cache);\r\nwrite_unlock(&cache->fci_lock);\r\n}\r\nstatic void fld_cache_punch_hole(struct fld_cache *cache,\r\nstruct fld_cache_entry *f_curr,\r\nstruct fld_cache_entry *f_new)\r\n{\r\nconst struct lu_seq_range *range = &f_new->fce_range;\r\nconst u64 new_start = range->lsr_start;\r\nconst u64 new_end = range->lsr_end;\r\nstruct fld_cache_entry *fldt;\r\nfldt = kzalloc(sizeof(*fldt), GFP_ATOMIC);\r\nif (!fldt) {\r\nkfree(f_new);\r\nreturn;\r\n}\r\nfldt->fce_range.lsr_start = new_end;\r\nfldt->fce_range.lsr_end = f_curr->fce_range.lsr_end;\r\nfldt->fce_range.lsr_index = f_curr->fce_range.lsr_index;\r\nf_curr->fce_range.lsr_end = new_start;\r\nfld_cache_entry_add(cache, f_new, &f_curr->fce_list);\r\nfld_cache_entry_add(cache, fldt, &f_new->fce_list);\r\n}\r\nstatic void fld_cache_overlap_handle(struct fld_cache *cache,\r\nstruct fld_cache_entry *f_curr,\r\nstruct fld_cache_entry *f_new)\r\n{\r\nconst struct lu_seq_range *range = &f_new->fce_range;\r\nconst u64 new_start = range->lsr_start;\r\nconst u64 new_end = range->lsr_end;\r\nconst u32 mdt = range->lsr_index;\r\nif (f_curr->fce_range.lsr_index == mdt) {\r\nf_curr->fce_range.lsr_start = min(f_curr->fce_range.lsr_start,\r\nnew_start);\r\nf_curr->fce_range.lsr_end = max(f_curr->fce_range.lsr_end,\r\nnew_end);\r\nkfree(f_new);\r\nfld_fix_new_list(cache);\r\n} else if (new_start <= f_curr->fce_range.lsr_start &&\r\nf_curr->fce_range.lsr_end <= new_end) {\r\nf_curr->fce_range = *range;\r\nkfree(f_new);\r\nfld_fix_new_list(cache);\r\n} else if (f_curr->fce_range.lsr_start < new_start &&\r\nnew_end < f_curr->fce_range.lsr_end) {\r\nfld_cache_punch_hole(cache, f_curr, f_new);\r\n} else if (new_end <= f_curr->fce_range.lsr_end) {\r\nLASSERT(new_start <= f_curr->fce_range.lsr_start);\r\nf_curr->fce_range.lsr_start = new_end;\r\nfld_cache_entry_add(cache, f_new, f_curr->fce_list.prev);\r\n} else if (f_curr->fce_range.lsr_start <= new_start) {\r\nLASSERT(f_curr->fce_range.lsr_end <= new_end);\r\nf_curr->fce_range.lsr_end = new_start;\r\nfld_cache_entry_add(cache, f_new, &f_curr->fce_list);\r\n} else\r\nCERROR("NEW range ="DRANGE" curr = "DRANGE"\n",\r\nPRANGE(range), PRANGE(&f_curr->fce_range));\r\n}\r\nstruct fld_cache_entry\r\n*fld_cache_entry_create(const struct lu_seq_range *range)\r\n{\r\nstruct fld_cache_entry *f_new;\r\nLASSERT(lu_seq_range_is_sane(range));\r\nf_new = kzalloc(sizeof(*f_new), GFP_NOFS);\r\nif (!f_new)\r\nreturn ERR_PTR(-ENOMEM);\r\nf_new->fce_range = *range;\r\nreturn f_new;\r\n}\r\nstatic int fld_cache_insert_nolock(struct fld_cache *cache,\r\nstruct fld_cache_entry *f_new)\r\n{\r\nstruct fld_cache_entry *f_curr;\r\nstruct fld_cache_entry *n;\r\nstruct list_head *head;\r\nstruct list_head *prev = NULL;\r\nconst u64 new_start = f_new->fce_range.lsr_start;\r\nconst u64 new_end = f_new->fce_range.lsr_end;\r\n__u32 new_flags = f_new->fce_range.lsr_flags;\r\nif (!cache->fci_no_shrink)\r\nfld_cache_shrink(cache);\r\nhead = &cache->fci_entries_head;\r\nlist_for_each_entry_safe(f_curr, n, head, fce_list) {\r\nif (new_end < f_curr->fce_range.lsr_start ||\r\n(new_end == f_curr->fce_range.lsr_start &&\r\nnew_flags != f_curr->fce_range.lsr_flags))\r\nbreak;\r\nprev = &f_curr->fce_list;\r\nif (new_start < f_curr->fce_range.lsr_end &&\r\nnew_flags == f_curr->fce_range.lsr_flags) {\r\nfld_cache_overlap_handle(cache, f_curr, f_new);\r\ngoto out;\r\n}\r\n}\r\nif (!prev)\r\nprev = head;\r\nCDEBUG(D_INFO, "insert range "DRANGE"\n", PRANGE(&f_new->fce_range));\r\nfld_cache_entry_add(cache, f_new, prev);\r\nout:\r\nreturn 0;\r\n}\r\nint fld_cache_insert(struct fld_cache *cache,\r\nconst struct lu_seq_range *range)\r\n{\r\nstruct fld_cache_entry *flde;\r\nint rc;\r\nflde = fld_cache_entry_create(range);\r\nif (IS_ERR(flde))\r\nreturn PTR_ERR(flde);\r\nwrite_lock(&cache->fci_lock);\r\nrc = fld_cache_insert_nolock(cache, flde);\r\nwrite_unlock(&cache->fci_lock);\r\nif (rc)\r\nkfree(flde);\r\nreturn rc;\r\n}\r\nstruct fld_cache_entry\r\n*fld_cache_entry_lookup_nolock(struct fld_cache *cache,\r\nstruct lu_seq_range *range)\r\n{\r\nstruct fld_cache_entry *flde;\r\nstruct fld_cache_entry *got = NULL;\r\nstruct list_head *head;\r\nhead = &cache->fci_entries_head;\r\nlist_for_each_entry(flde, head, fce_list) {\r\nif (range->lsr_start == flde->fce_range.lsr_start ||\r\n(range->lsr_end == flde->fce_range.lsr_end &&\r\nrange->lsr_flags == flde->fce_range.lsr_flags)) {\r\ngot = flde;\r\nbreak;\r\n}\r\n}\r\nreturn got;\r\n}\r\nstruct fld_cache_entry\r\n*fld_cache_entry_lookup(struct fld_cache *cache, struct lu_seq_range *range)\r\n{\r\nstruct fld_cache_entry *got = NULL;\r\nread_lock(&cache->fci_lock);\r\ngot = fld_cache_entry_lookup_nolock(cache, range);\r\nread_unlock(&cache->fci_lock);\r\nreturn got;\r\n}\r\nint fld_cache_lookup(struct fld_cache *cache,\r\nconst u64 seq, struct lu_seq_range *range)\r\n{\r\nstruct fld_cache_entry *flde;\r\nstruct fld_cache_entry *prev = NULL;\r\nstruct list_head *head;\r\nread_lock(&cache->fci_lock);\r\nhead = &cache->fci_entries_head;\r\ncache->fci_stat.fst_count++;\r\nlist_for_each_entry(flde, head, fce_list) {\r\nif (flde->fce_range.lsr_start > seq) {\r\nif (prev)\r\n*range = prev->fce_range;\r\nbreak;\r\n}\r\nprev = flde;\r\nif (lu_seq_range_within(&flde->fce_range, seq)) {\r\n*range = flde->fce_range;\r\ncache->fci_stat.fst_cache++;\r\nread_unlock(&cache->fci_lock);\r\nreturn 0;\r\n}\r\n}\r\nread_unlock(&cache->fci_lock);\r\nreturn -ENOENT;\r\n}
