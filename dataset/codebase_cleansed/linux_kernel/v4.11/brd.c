static struct page *brd_lookup_page(struct brd_device *brd, sector_t sector)\r\n{\r\npgoff_t idx;\r\nstruct page *page;\r\nrcu_read_lock();\r\nidx = sector >> PAGE_SECTORS_SHIFT;\r\npage = radix_tree_lookup(&brd->brd_pages, idx);\r\nrcu_read_unlock();\r\nBUG_ON(page && page->index != idx);\r\nreturn page;\r\n}\r\nstatic struct page *brd_insert_page(struct brd_device *brd, sector_t sector)\r\n{\r\npgoff_t idx;\r\nstruct page *page;\r\ngfp_t gfp_flags;\r\npage = brd_lookup_page(brd, sector);\r\nif (page)\r\nreturn page;\r\ngfp_flags = GFP_NOIO | __GFP_ZERO;\r\n#ifndef CONFIG_BLK_DEV_RAM_DAX\r\ngfp_flags |= __GFP_HIGHMEM;\r\n#endif\r\npage = alloc_page(gfp_flags);\r\nif (!page)\r\nreturn NULL;\r\nif (radix_tree_preload(GFP_NOIO)) {\r\n__free_page(page);\r\nreturn NULL;\r\n}\r\nspin_lock(&brd->brd_lock);\r\nidx = sector >> PAGE_SECTORS_SHIFT;\r\npage->index = idx;\r\nif (radix_tree_insert(&brd->brd_pages, idx, page)) {\r\n__free_page(page);\r\npage = radix_tree_lookup(&brd->brd_pages, idx);\r\nBUG_ON(!page);\r\nBUG_ON(page->index != idx);\r\n}\r\nspin_unlock(&brd->brd_lock);\r\nradix_tree_preload_end();\r\nreturn page;\r\n}\r\nstatic void brd_free_page(struct brd_device *brd, sector_t sector)\r\n{\r\nstruct page *page;\r\npgoff_t idx;\r\nspin_lock(&brd->brd_lock);\r\nidx = sector >> PAGE_SECTORS_SHIFT;\r\npage = radix_tree_delete(&brd->brd_pages, idx);\r\nspin_unlock(&brd->brd_lock);\r\nif (page)\r\n__free_page(page);\r\n}\r\nstatic void brd_zero_page(struct brd_device *brd, sector_t sector)\r\n{\r\nstruct page *page;\r\npage = brd_lookup_page(brd, sector);\r\nif (page)\r\nclear_highpage(page);\r\n}\r\nstatic void brd_free_pages(struct brd_device *brd)\r\n{\r\nunsigned long pos = 0;\r\nstruct page *pages[FREE_BATCH];\r\nint nr_pages;\r\ndo {\r\nint i;\r\nnr_pages = radix_tree_gang_lookup(&brd->brd_pages,\r\n(void **)pages, pos, FREE_BATCH);\r\nfor (i = 0; i < nr_pages; i++) {\r\nvoid *ret;\r\nBUG_ON(pages[i]->index < pos);\r\npos = pages[i]->index;\r\nret = radix_tree_delete(&brd->brd_pages, pos);\r\nBUG_ON(!ret || ret != pages[i]);\r\n__free_page(pages[i]);\r\n}\r\npos++;\r\n} while (nr_pages == FREE_BATCH);\r\n}\r\nstatic int copy_to_brd_setup(struct brd_device *brd, sector_t sector, size_t n)\r\n{\r\nunsigned int offset = (sector & (PAGE_SECTORS-1)) << SECTOR_SHIFT;\r\nsize_t copy;\r\ncopy = min_t(size_t, n, PAGE_SIZE - offset);\r\nif (!brd_insert_page(brd, sector))\r\nreturn -ENOSPC;\r\nif (copy < n) {\r\nsector += copy >> SECTOR_SHIFT;\r\nif (!brd_insert_page(brd, sector))\r\nreturn -ENOSPC;\r\n}\r\nreturn 0;\r\n}\r\nstatic void discard_from_brd(struct brd_device *brd,\r\nsector_t sector, size_t n)\r\n{\r\nwhile (n >= PAGE_SIZE) {\r\nif (0)\r\nbrd_free_page(brd, sector);\r\nelse\r\nbrd_zero_page(brd, sector);\r\nsector += PAGE_SIZE >> SECTOR_SHIFT;\r\nn -= PAGE_SIZE;\r\n}\r\n}\r\nstatic void copy_to_brd(struct brd_device *brd, const void *src,\r\nsector_t sector, size_t n)\r\n{\r\nstruct page *page;\r\nvoid *dst;\r\nunsigned int offset = (sector & (PAGE_SECTORS-1)) << SECTOR_SHIFT;\r\nsize_t copy;\r\ncopy = min_t(size_t, n, PAGE_SIZE - offset);\r\npage = brd_lookup_page(brd, sector);\r\nBUG_ON(!page);\r\ndst = kmap_atomic(page);\r\nmemcpy(dst + offset, src, copy);\r\nkunmap_atomic(dst);\r\nif (copy < n) {\r\nsrc += copy;\r\nsector += copy >> SECTOR_SHIFT;\r\ncopy = n - copy;\r\npage = brd_lookup_page(brd, sector);\r\nBUG_ON(!page);\r\ndst = kmap_atomic(page);\r\nmemcpy(dst, src, copy);\r\nkunmap_atomic(dst);\r\n}\r\n}\r\nstatic void copy_from_brd(void *dst, struct brd_device *brd,\r\nsector_t sector, size_t n)\r\n{\r\nstruct page *page;\r\nvoid *src;\r\nunsigned int offset = (sector & (PAGE_SECTORS-1)) << SECTOR_SHIFT;\r\nsize_t copy;\r\ncopy = min_t(size_t, n, PAGE_SIZE - offset);\r\npage = brd_lookup_page(brd, sector);\r\nif (page) {\r\nsrc = kmap_atomic(page);\r\nmemcpy(dst, src + offset, copy);\r\nkunmap_atomic(src);\r\n} else\r\nmemset(dst, 0, copy);\r\nif (copy < n) {\r\ndst += copy;\r\nsector += copy >> SECTOR_SHIFT;\r\ncopy = n - copy;\r\npage = brd_lookup_page(brd, sector);\r\nif (page) {\r\nsrc = kmap_atomic(page);\r\nmemcpy(dst, src, copy);\r\nkunmap_atomic(src);\r\n} else\r\nmemset(dst, 0, copy);\r\n}\r\n}\r\nstatic int brd_do_bvec(struct brd_device *brd, struct page *page,\r\nunsigned int len, unsigned int off, bool is_write,\r\nsector_t sector)\r\n{\r\nvoid *mem;\r\nint err = 0;\r\nif (is_write) {\r\nerr = copy_to_brd_setup(brd, sector, len);\r\nif (err)\r\ngoto out;\r\n}\r\nmem = kmap_atomic(page);\r\nif (!is_write) {\r\ncopy_from_brd(mem + off, brd, sector, len);\r\nflush_dcache_page(page);\r\n} else {\r\nflush_dcache_page(page);\r\ncopy_to_brd(brd, mem + off, sector, len);\r\n}\r\nkunmap_atomic(mem);\r\nout:\r\nreturn err;\r\n}\r\nstatic blk_qc_t brd_make_request(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct block_device *bdev = bio->bi_bdev;\r\nstruct brd_device *brd = bdev->bd_disk->private_data;\r\nstruct bio_vec bvec;\r\nsector_t sector;\r\nstruct bvec_iter iter;\r\nsector = bio->bi_iter.bi_sector;\r\nif (bio_end_sector(bio) > get_capacity(bdev->bd_disk))\r\ngoto io_error;\r\nif (unlikely(bio_op(bio) == REQ_OP_DISCARD)) {\r\nif (sector & ((PAGE_SIZE >> SECTOR_SHIFT) - 1) ||\r\nbio->bi_iter.bi_size & ~PAGE_MASK)\r\ngoto io_error;\r\ndiscard_from_brd(brd, sector, bio->bi_iter.bi_size);\r\ngoto out;\r\n}\r\nbio_for_each_segment(bvec, bio, iter) {\r\nunsigned int len = bvec.bv_len;\r\nint err;\r\nerr = brd_do_bvec(brd, bvec.bv_page, len, bvec.bv_offset,\r\nop_is_write(bio_op(bio)), sector);\r\nif (err)\r\ngoto io_error;\r\nsector += len >> SECTOR_SHIFT;\r\n}\r\nout:\r\nbio_endio(bio);\r\nreturn BLK_QC_T_NONE;\r\nio_error:\r\nbio_io_error(bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic int brd_rw_page(struct block_device *bdev, sector_t sector,\r\nstruct page *page, bool is_write)\r\n{\r\nstruct brd_device *brd = bdev->bd_disk->private_data;\r\nint err = brd_do_bvec(brd, page, PAGE_SIZE, 0, is_write, sector);\r\npage_endio(page, is_write, err);\r\nreturn err;\r\n}\r\nstatic long brd_direct_access(struct block_device *bdev, sector_t sector,\r\nvoid **kaddr, pfn_t *pfn, long size)\r\n{\r\nstruct brd_device *brd = bdev->bd_disk->private_data;\r\nstruct page *page;\r\nif (!brd)\r\nreturn -ENODEV;\r\npage = brd_insert_page(brd, sector);\r\nif (!page)\r\nreturn -ENOSPC;\r\n*kaddr = page_address(page);\r\n*pfn = page_to_pfn_t(page);\r\nreturn PAGE_SIZE;\r\n}\r\nstatic int __init ramdisk_size(char *str)\r\n{\r\nrd_size = simple_strtol(str, NULL, 0);\r\nreturn 1;\r\n}\r\nstatic struct brd_device *brd_alloc(int i)\r\n{\r\nstruct brd_device *brd;\r\nstruct gendisk *disk;\r\nbrd = kzalloc(sizeof(*brd), GFP_KERNEL);\r\nif (!brd)\r\ngoto out;\r\nbrd->brd_number = i;\r\nspin_lock_init(&brd->brd_lock);\r\nINIT_RADIX_TREE(&brd->brd_pages, GFP_ATOMIC);\r\nbrd->brd_queue = blk_alloc_queue(GFP_KERNEL);\r\nif (!brd->brd_queue)\r\ngoto out_free_dev;\r\nblk_queue_make_request(brd->brd_queue, brd_make_request);\r\nblk_queue_max_hw_sectors(brd->brd_queue, 1024);\r\nblk_queue_bounce_limit(brd->brd_queue, BLK_BOUNCE_ANY);\r\nblk_queue_physical_block_size(brd->brd_queue, PAGE_SIZE);\r\nbrd->brd_queue->limits.discard_granularity = PAGE_SIZE;\r\nblk_queue_max_discard_sectors(brd->brd_queue, UINT_MAX);\r\nbrd->brd_queue->limits.discard_zeroes_data = 1;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, brd->brd_queue);\r\n#ifdef CONFIG_BLK_DEV_RAM_DAX\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DAX, brd->brd_queue);\r\n#endif\r\ndisk = brd->brd_disk = alloc_disk(max_part);\r\nif (!disk)\r\ngoto out_free_queue;\r\ndisk->major = RAMDISK_MAJOR;\r\ndisk->first_minor = i * max_part;\r\ndisk->fops = &brd_fops;\r\ndisk->private_data = brd;\r\ndisk->queue = brd->brd_queue;\r\ndisk->flags = GENHD_FL_EXT_DEVT;\r\nsprintf(disk->disk_name, "ram%d", i);\r\nset_capacity(disk, rd_size * 2);\r\nreturn brd;\r\nout_free_queue:\r\nblk_cleanup_queue(brd->brd_queue);\r\nout_free_dev:\r\nkfree(brd);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic void brd_free(struct brd_device *brd)\r\n{\r\nput_disk(brd->brd_disk);\r\nblk_cleanup_queue(brd->brd_queue);\r\nbrd_free_pages(brd);\r\nkfree(brd);\r\n}\r\nstatic struct brd_device *brd_init_one(int i, bool *new)\r\n{\r\nstruct brd_device *brd;\r\n*new = false;\r\nlist_for_each_entry(brd, &brd_devices, brd_list) {\r\nif (brd->brd_number == i)\r\ngoto out;\r\n}\r\nbrd = brd_alloc(i);\r\nif (brd) {\r\nadd_disk(brd->brd_disk);\r\nlist_add_tail(&brd->brd_list, &brd_devices);\r\n}\r\n*new = true;\r\nout:\r\nreturn brd;\r\n}\r\nstatic void brd_del_one(struct brd_device *brd)\r\n{\r\nlist_del(&brd->brd_list);\r\ndel_gendisk(brd->brd_disk);\r\nbrd_free(brd);\r\n}\r\nstatic struct kobject *brd_probe(dev_t dev, int *part, void *data)\r\n{\r\nstruct brd_device *brd;\r\nstruct kobject *kobj;\r\nbool new;\r\nmutex_lock(&brd_devices_mutex);\r\nbrd = brd_init_one(MINOR(dev) / max_part, &new);\r\nkobj = brd ? get_disk(brd->brd_disk) : NULL;\r\nmutex_unlock(&brd_devices_mutex);\r\nif (new)\r\n*part = 0;\r\nreturn kobj;\r\n}\r\nstatic int __init brd_init(void)\r\n{\r\nstruct brd_device *brd, *next;\r\nint i;\r\nif (register_blkdev(RAMDISK_MAJOR, "ramdisk"))\r\nreturn -EIO;\r\nif (unlikely(!max_part))\r\nmax_part = 1;\r\nfor (i = 0; i < rd_nr; i++) {\r\nbrd = brd_alloc(i);\r\nif (!brd)\r\ngoto out_free;\r\nlist_add_tail(&brd->brd_list, &brd_devices);\r\n}\r\nlist_for_each_entry(brd, &brd_devices, brd_list)\r\nadd_disk(brd->brd_disk);\r\nblk_register_region(MKDEV(RAMDISK_MAJOR, 0), 1UL << MINORBITS,\r\nTHIS_MODULE, brd_probe, NULL, NULL);\r\npr_info("brd: module loaded\n");\r\nreturn 0;\r\nout_free:\r\nlist_for_each_entry_safe(brd, next, &brd_devices, brd_list) {\r\nlist_del(&brd->brd_list);\r\nbrd_free(brd);\r\n}\r\nunregister_blkdev(RAMDISK_MAJOR, "ramdisk");\r\npr_info("brd: module NOT loaded !!!\n");\r\nreturn -ENOMEM;\r\n}\r\nstatic void __exit brd_exit(void)\r\n{\r\nstruct brd_device *brd, *next;\r\nlist_for_each_entry_safe(brd, next, &brd_devices, brd_list)\r\nbrd_del_one(brd);\r\nblk_unregister_region(MKDEV(RAMDISK_MAJOR, 0), 1UL << MINORBITS);\r\nunregister_blkdev(RAMDISK_MAJOR, "ramdisk");\r\npr_info("brd: module unloaded\n");\r\n}
