int __cfs_fail_check_set(u32 id, u32 value, int set)\r\n{\r\nstatic atomic_t cfs_fail_count = ATOMIC_INIT(0);\r\nLASSERT(!(id & CFS_FAIL_ONCE));\r\nif ((cfs_fail_loc & (CFS_FAILED | CFS_FAIL_ONCE)) ==\r\n(CFS_FAILED | CFS_FAIL_ONCE)) {\r\natomic_set(&cfs_fail_count, 0);\r\nreturn 0;\r\n}\r\nif (cfs_fail_loc & CFS_FAIL_RAND) {\r\nif (cfs_fail_val < 2 || cfs_rand() % cfs_fail_val > 0)\r\nreturn 0;\r\n}\r\nif (cfs_fail_loc & CFS_FAIL_SKIP) {\r\nif (atomic_inc_return(&cfs_fail_count) <= cfs_fail_val)\r\nreturn 0;\r\n}\r\nif (set == CFS_FAIL_LOC_VALUE) {\r\nif (cfs_fail_val != -1 && cfs_fail_val != value)\r\nreturn 0;\r\n}\r\nif (cfs_fail_loc & CFS_FAIL_SOME &&\r\n(!(cfs_fail_loc & CFS_FAIL_ONCE) || cfs_fail_val <= 1)) {\r\nint count = atomic_inc_return(&cfs_fail_count);\r\nif (count >= cfs_fail_val) {\r\nset_bit(CFS_FAIL_ONCE_BIT, &cfs_fail_loc);\r\natomic_set(&cfs_fail_count, 0);\r\nif (count > cfs_fail_val)\r\nreturn 0;\r\n}\r\n}\r\nif ((set == CFS_FAIL_LOC_ORSET) && (value & CFS_FAIL_ONCE))\r\nset_bit(CFS_FAIL_ONCE_BIT, &cfs_fail_loc);\r\nif (test_and_set_bit(CFS_FAILED_BIT, &cfs_fail_loc)) {\r\nif (cfs_fail_loc & CFS_FAIL_ONCE)\r\nreturn 0;\r\n}\r\nswitch (set) {\r\ncase CFS_FAIL_LOC_NOSET:\r\ncase CFS_FAIL_LOC_VALUE:\r\nbreak;\r\ncase CFS_FAIL_LOC_ORSET:\r\ncfs_fail_loc |= value & ~(CFS_FAILED | CFS_FAIL_ONCE);\r\nbreak;\r\ncase CFS_FAIL_LOC_RESET:\r\ncfs_fail_loc = value;\r\natomic_set(&cfs_fail_count, 0);\r\nbreak;\r\ndefault:\r\nLASSERTF(0, "called with bad set %u\n", set);\r\nbreak;\r\n}\r\nreturn 1;\r\n}\r\nint __cfs_fail_timeout_set(u32 id, u32 value, int ms, int set)\r\n{\r\nint ret;\r\nret = __cfs_fail_check_set(id, value, set);\r\nif (ret && likely(ms > 0)) {\r\nCERROR("cfs_fail_timeout id %x sleeping for %dms\n",\r\nid, ms);\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nschedule_timeout(cfs_time_seconds(ms) / 1000);\r\nCERROR("cfs_fail_timeout id %x awake\n", id);\r\n}\r\nreturn ret;\r\n}
