void nfp_net_filter_stats_timer(unsigned long data)\r\n{\r\nstruct nfp_net *nn = (void *)data;\r\nstruct nfp_stat_pair latest;\r\nspin_lock_bh(&nn->rx_filter_lock);\r\nif (nn->ctrl & NFP_NET_CFG_CTRL_BPF)\r\nmod_timer(&nn->rx_filter_stats_timer,\r\njiffies + NFP_NET_STAT_POLL_IVL);\r\nspin_unlock_bh(&nn->rx_filter_lock);\r\nlatest.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);\r\nlatest.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);\r\nif (latest.pkts != nn->rx_filter.pkts)\r\nnn->rx_filter_change = jiffies;\r\nnn->rx_filter = latest;\r\n}\r\nstatic void nfp_net_bpf_stats_reset(struct nfp_net *nn)\r\n{\r\nnn->rx_filter.pkts = nn_readq(nn, NFP_NET_CFG_STATS_APP1_FRAMES);\r\nnn->rx_filter.bytes = nn_readq(nn, NFP_NET_CFG_STATS_APP1_BYTES);\r\nnn->rx_filter_prev = nn->rx_filter;\r\nnn->rx_filter_change = jiffies;\r\n}\r\nstatic int\r\nnfp_net_bpf_stats_update(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)\r\n{\r\nstruct tc_action *a;\r\nLIST_HEAD(actions);\r\nu64 bytes, pkts;\r\npkts = nn->rx_filter.pkts - nn->rx_filter_prev.pkts;\r\nbytes = nn->rx_filter.bytes - nn->rx_filter_prev.bytes;\r\nbytes -= pkts * ETH_HLEN;\r\nnn->rx_filter_prev = nn->rx_filter;\r\npreempt_disable();\r\ntcf_exts_to_list(cls_bpf->exts, &actions);\r\nlist_for_each_entry(a, &actions, list)\r\ntcf_action_stats_update(a, bytes, pkts, nn->rx_filter_change);\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nstatic int\r\nnfp_net_bpf_get_act(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)\r\n{\r\nconst struct tc_action *a;\r\nLIST_HEAD(actions);\r\nif (!cls_bpf->exts)\r\nreturn NN_ACT_XDP;\r\nif (cls_bpf->exts_integrated) {\r\nif (tc_no_actions(cls_bpf->exts))\r\nreturn NN_ACT_DIRECT;\r\nreturn -ENOTSUPP;\r\n}\r\nif (!tc_single_action(cls_bpf->exts))\r\nreturn -ENOTSUPP;\r\ntcf_exts_to_list(cls_bpf->exts, &actions);\r\nlist_for_each_entry(a, &actions, list) {\r\nif (is_tcf_gact_shot(a))\r\nreturn NN_ACT_TC_DROP;\r\nif (is_tcf_mirred_egress_redirect(a) &&\r\ntcf_mirred_ifindex(a) == nn->netdev->ifindex)\r\nreturn NN_ACT_TC_REDIR;\r\n}\r\nreturn -ENOTSUPP;\r\n}\r\nstatic int\r\nnfp_net_bpf_offload_prepare(struct nfp_net *nn,\r\nstruct tc_cls_bpf_offload *cls_bpf,\r\nstruct nfp_bpf_result *res,\r\nvoid **code, dma_addr_t *dma_addr, u16 max_instr)\r\n{\r\nunsigned int code_sz = max_instr * sizeof(u64);\r\nenum nfp_bpf_action_type act;\r\nu16 start_off, done_off;\r\nunsigned int max_mtu;\r\nint ret;\r\nif (!IS_ENABLED(CONFIG_BPF_SYSCALL))\r\nreturn -ENOTSUPP;\r\nret = nfp_net_bpf_get_act(nn, cls_bpf);\r\nif (ret < 0)\r\nreturn ret;\r\nact = ret;\r\nmax_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;\r\nif (max_mtu < nn->netdev->mtu) {\r\nnn_info(nn, "BPF offload not supported with MTU larger than HW packet split boundary\n");\r\nreturn -ENOTSUPP;\r\n}\r\nstart_off = nn_readw(nn, NFP_NET_CFG_BPF_START);\r\ndone_off = nn_readw(nn, NFP_NET_CFG_BPF_DONE);\r\n*code = dma_zalloc_coherent(&nn->pdev->dev, code_sz, dma_addr,\r\nGFP_KERNEL);\r\nif (!*code)\r\nreturn -ENOMEM;\r\nret = nfp_bpf_jit(cls_bpf->prog, *code, act, start_off, done_off,\r\nmax_instr, res);\r\nif (ret)\r\ngoto out;\r\nreturn 0;\r\nout:\r\ndma_free_coherent(&nn->pdev->dev, code_sz, *code, *dma_addr);\r\nreturn ret;\r\n}\r\nstatic void\r\nnfp_net_bpf_load_and_start(struct nfp_net *nn, u32 tc_flags,\r\nvoid *code, dma_addr_t dma_addr,\r\nunsigned int code_sz, unsigned int n_instr,\r\nbool dense_mode)\r\n{\r\nu64 bpf_addr = dma_addr;\r\nint err;\r\nnn->bpf_offload_skip_sw = !!(tc_flags & TCA_CLS_FLAGS_SKIP_SW);\r\nif (dense_mode)\r\nbpf_addr |= NFP_NET_CFG_BPF_CFG_8CTX;\r\nnn_writew(nn, NFP_NET_CFG_BPF_SIZE, n_instr);\r\nnn_writeq(nn, NFP_NET_CFG_BPF_ADDR, bpf_addr);\r\nerr = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_BPF);\r\nif (err)\r\nnn_err(nn, "FW command error while loading BPF: %d\n", err);\r\nnn->ctrl |= NFP_NET_CFG_CTRL_BPF;\r\nnn_writel(nn, NFP_NET_CFG_CTRL, nn->ctrl);\r\nerr = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);\r\nif (err)\r\nnn_err(nn, "FW command error while enabling BPF: %d\n", err);\r\ndma_free_coherent(&nn->pdev->dev, code_sz, code, dma_addr);\r\nnfp_net_bpf_stats_reset(nn);\r\nmod_timer(&nn->rx_filter_stats_timer, jiffies + NFP_NET_STAT_POLL_IVL);\r\n}\r\nstatic int nfp_net_bpf_stop(struct nfp_net *nn)\r\n{\r\nif (!(nn->ctrl & NFP_NET_CFG_CTRL_BPF))\r\nreturn 0;\r\nspin_lock_bh(&nn->rx_filter_lock);\r\nnn->ctrl &= ~NFP_NET_CFG_CTRL_BPF;\r\nspin_unlock_bh(&nn->rx_filter_lock);\r\nnn_writel(nn, NFP_NET_CFG_CTRL, nn->ctrl);\r\ndel_timer_sync(&nn->rx_filter_stats_timer);\r\nnn->bpf_offload_skip_sw = 0;\r\nreturn nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);\r\n}\r\nint nfp_net_bpf_offload(struct nfp_net *nn, struct tc_cls_bpf_offload *cls_bpf)\r\n{\r\nstruct nfp_bpf_result res;\r\ndma_addr_t dma_addr;\r\nu16 max_instr;\r\nvoid *code;\r\nint err;\r\nmax_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);\r\nswitch (cls_bpf->command) {\r\ncase TC_CLSBPF_REPLACE:\r\nif (nn->bpf_offload_skip_sw)\r\nreturn -EBUSY;\r\nerr = nfp_net_bpf_offload_prepare(nn, cls_bpf, &res, &code,\r\n&dma_addr, max_instr);\r\nif (err)\r\nreturn err;\r\nnfp_net_bpf_stop(nn);\r\nnfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,\r\ndma_addr, max_instr * sizeof(u64),\r\nres.n_instr, res.dense_mode);\r\nreturn 0;\r\ncase TC_CLSBPF_ADD:\r\nif (nn->ctrl & NFP_NET_CFG_CTRL_BPF)\r\nreturn -EBUSY;\r\nerr = nfp_net_bpf_offload_prepare(nn, cls_bpf, &res, &code,\r\n&dma_addr, max_instr);\r\nif (err)\r\nreturn err;\r\nnfp_net_bpf_load_and_start(nn, cls_bpf->gen_flags, code,\r\ndma_addr, max_instr * sizeof(u64),\r\nres.n_instr, res.dense_mode);\r\nreturn 0;\r\ncase TC_CLSBPF_DESTROY:\r\nreturn nfp_net_bpf_stop(nn);\r\ncase TC_CLSBPF_STATS:\r\nreturn nfp_net_bpf_stats_update(nn, cls_bpf);\r\ndefault:\r\nreturn -ENOTSUPP;\r\n}\r\n}
