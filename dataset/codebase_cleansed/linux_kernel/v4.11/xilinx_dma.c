static inline u32 dma_read(struct xilinx_dma_chan *chan, u32 reg)\r\n{\r\nreturn ioread32(chan->xdev->regs + reg);\r\n}\r\nstatic inline void dma_write(struct xilinx_dma_chan *chan, u32 reg, u32 value)\r\n{\r\niowrite32(value, chan->xdev->regs + reg);\r\n}\r\nstatic inline void vdma_desc_write(struct xilinx_dma_chan *chan, u32 reg,\r\nu32 value)\r\n{\r\ndma_write(chan, chan->desc_offset + reg, value);\r\n}\r\nstatic inline u32 dma_ctrl_read(struct xilinx_dma_chan *chan, u32 reg)\r\n{\r\nreturn dma_read(chan, chan->ctrl_offset + reg);\r\n}\r\nstatic inline void dma_ctrl_write(struct xilinx_dma_chan *chan, u32 reg,\r\nu32 value)\r\n{\r\ndma_write(chan, chan->ctrl_offset + reg, value);\r\n}\r\nstatic inline void dma_ctrl_clr(struct xilinx_dma_chan *chan, u32 reg,\r\nu32 clr)\r\n{\r\ndma_ctrl_write(chan, reg, dma_ctrl_read(chan, reg) & ~clr);\r\n}\r\nstatic inline void dma_ctrl_set(struct xilinx_dma_chan *chan, u32 reg,\r\nu32 set)\r\n{\r\ndma_ctrl_write(chan, reg, dma_ctrl_read(chan, reg) | set);\r\n}\r\nstatic inline void vdma_desc_write_64(struct xilinx_dma_chan *chan, u32 reg,\r\nu32 value_lsb, u32 value_msb)\r\n{\r\nwritel(value_lsb, chan->xdev->regs + chan->desc_offset + reg);\r\nwritel(value_msb, chan->xdev->regs + chan->desc_offset + reg + 4);\r\n}\r\nstatic inline void dma_writeq(struct xilinx_dma_chan *chan, u32 reg, u64 value)\r\n{\r\nlo_hi_writeq(value, chan->xdev->regs + chan->ctrl_offset + reg);\r\n}\r\nstatic inline void xilinx_write(struct xilinx_dma_chan *chan, u32 reg,\r\ndma_addr_t addr)\r\n{\r\nif (chan->ext_addr)\r\ndma_writeq(chan, reg, addr);\r\nelse\r\ndma_ctrl_write(chan, reg, addr);\r\n}\r\nstatic inline void xilinx_axidma_buf(struct xilinx_dma_chan *chan,\r\nstruct xilinx_axidma_desc_hw *hw,\r\ndma_addr_t buf_addr, size_t sg_used,\r\nsize_t period_len)\r\n{\r\nif (chan->ext_addr) {\r\nhw->buf_addr = lower_32_bits(buf_addr + sg_used + period_len);\r\nhw->buf_addr_msb = upper_32_bits(buf_addr + sg_used +\r\nperiod_len);\r\n} else {\r\nhw->buf_addr = buf_addr + sg_used + period_len;\r\n}\r\n}\r\nstatic struct xilinx_vdma_tx_segment *\r\nxilinx_vdma_alloc_tx_segment(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_vdma_tx_segment *segment;\r\ndma_addr_t phys;\r\nsegment = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &phys);\r\nif (!segment)\r\nreturn NULL;\r\nsegment->phys = phys;\r\nreturn segment;\r\n}\r\nstatic struct xilinx_cdma_tx_segment *\r\nxilinx_cdma_alloc_tx_segment(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_cdma_tx_segment *segment;\r\ndma_addr_t phys;\r\nsegment = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &phys);\r\nif (!segment)\r\nreturn NULL;\r\nsegment->phys = phys;\r\nreturn segment;\r\n}\r\nstatic struct xilinx_axidma_tx_segment *\r\nxilinx_axidma_alloc_tx_segment(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_axidma_tx_segment *segment;\r\ndma_addr_t phys;\r\nsegment = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &phys);\r\nif (!segment)\r\nreturn NULL;\r\nsegment->phys = phys;\r\nreturn segment;\r\n}\r\nstatic void xilinx_dma_free_tx_segment(struct xilinx_dma_chan *chan,\r\nstruct xilinx_axidma_tx_segment *segment)\r\n{\r\ndma_pool_free(chan->desc_pool, segment, segment->phys);\r\n}\r\nstatic void xilinx_cdma_free_tx_segment(struct xilinx_dma_chan *chan,\r\nstruct xilinx_cdma_tx_segment *segment)\r\n{\r\ndma_pool_free(chan->desc_pool, segment, segment->phys);\r\n}\r\nstatic void xilinx_vdma_free_tx_segment(struct xilinx_dma_chan *chan,\r\nstruct xilinx_vdma_tx_segment *segment)\r\n{\r\ndma_pool_free(chan->desc_pool, segment, segment->phys);\r\n}\r\nstatic struct xilinx_dma_tx_descriptor *\r\nxilinx_dma_alloc_tx_descriptor(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_dma_tx_descriptor *desc;\r\ndesc = kzalloc(sizeof(*desc), GFP_KERNEL);\r\nif (!desc)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&desc->segments);\r\nreturn desc;\r\n}\r\nstatic void\r\nxilinx_dma_free_tx_descriptor(struct xilinx_dma_chan *chan,\r\nstruct xilinx_dma_tx_descriptor *desc)\r\n{\r\nstruct xilinx_vdma_tx_segment *segment, *next;\r\nstruct xilinx_cdma_tx_segment *cdma_segment, *cdma_next;\r\nstruct xilinx_axidma_tx_segment *axidma_segment, *axidma_next;\r\nif (!desc)\r\nreturn;\r\nif (chan->xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\r\nlist_for_each_entry_safe(segment, next, &desc->segments, node) {\r\nlist_del(&segment->node);\r\nxilinx_vdma_free_tx_segment(chan, segment);\r\n}\r\n} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\r\nlist_for_each_entry_safe(cdma_segment, cdma_next,\r\n&desc->segments, node) {\r\nlist_del(&cdma_segment->node);\r\nxilinx_cdma_free_tx_segment(chan, cdma_segment);\r\n}\r\n} else {\r\nlist_for_each_entry_safe(axidma_segment, axidma_next,\r\n&desc->segments, node) {\r\nlist_del(&axidma_segment->node);\r\nxilinx_dma_free_tx_segment(chan, axidma_segment);\r\n}\r\n}\r\nkfree(desc);\r\n}\r\nstatic void xilinx_dma_free_desc_list(struct xilinx_dma_chan *chan,\r\nstruct list_head *list)\r\n{\r\nstruct xilinx_dma_tx_descriptor *desc, *next;\r\nlist_for_each_entry_safe(desc, next, list, node) {\r\nlist_del(&desc->node);\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\n}\r\n}\r\nstatic void xilinx_dma_free_descriptors(struct xilinx_dma_chan *chan)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nxilinx_dma_free_desc_list(chan, &chan->pending_list);\r\nxilinx_dma_free_desc_list(chan, &chan->done_list);\r\nxilinx_dma_free_desc_list(chan, &chan->active_list);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void xilinx_dma_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\ndev_dbg(chan->dev, "Free all channel resources.\n");\r\nxilinx_dma_free_descriptors(chan);\r\nif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\r\nxilinx_dma_free_tx_segment(chan, chan->cyclic_seg_v);\r\nxilinx_dma_free_tx_segment(chan, chan->seg_v);\r\n}\r\ndma_pool_destroy(chan->desc_pool);\r\nchan->desc_pool = NULL;\r\n}\r\nstatic void xilinx_dma_chan_handle_cyclic(struct xilinx_dma_chan *chan,\r\nstruct xilinx_dma_tx_descriptor *desc,\r\nunsigned long *flags)\r\n{\r\ndma_async_tx_callback callback;\r\nvoid *callback_param;\r\ncallback = desc->async_tx.callback;\r\ncallback_param = desc->async_tx.callback_param;\r\nif (callback) {\r\nspin_unlock_irqrestore(&chan->lock, *flags);\r\ncallback(callback_param);\r\nspin_lock_irqsave(&chan->lock, *flags);\r\n}\r\n}\r\nstatic void xilinx_dma_chan_desc_cleanup(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_dma_tx_descriptor *desc, *next;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nlist_for_each_entry_safe(desc, next, &chan->done_list, node) {\r\nstruct dmaengine_desc_callback cb;\r\nif (desc->cyclic) {\r\nxilinx_dma_chan_handle_cyclic(chan, desc, &flags);\r\nbreak;\r\n}\r\nlist_del(&desc->node);\r\ndmaengine_desc_get_callback(&desc->async_tx, &cb);\r\nif (dmaengine_desc_callback_valid(&cb)) {\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\ndmaengine_desc_callback_invoke(&cb, NULL);\r\nspin_lock_irqsave(&chan->lock, flags);\r\n}\r\ndma_run_dependencies(&desc->async_tx);\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\n}\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void xilinx_dma_do_tasklet(unsigned long data)\r\n{\r\nstruct xilinx_dma_chan *chan = (struct xilinx_dma_chan *)data;\r\nxilinx_dma_chan_desc_cleanup(chan);\r\n}\r\nstatic int xilinx_dma_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nif (chan->desc_pool)\r\nreturn 0;\r\nif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\r\nchan->desc_pool = dma_pool_create("xilinx_dma_desc_pool",\r\nchan->dev,\r\nsizeof(struct xilinx_axidma_tx_segment),\r\n__alignof__(struct xilinx_axidma_tx_segment),\r\n0);\r\n} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\r\nchan->desc_pool = dma_pool_create("xilinx_cdma_desc_pool",\r\nchan->dev,\r\nsizeof(struct xilinx_cdma_tx_segment),\r\n__alignof__(struct xilinx_cdma_tx_segment),\r\n0);\r\n} else {\r\nchan->desc_pool = dma_pool_create("xilinx_vdma_desc_pool",\r\nchan->dev,\r\nsizeof(struct xilinx_vdma_tx_segment),\r\n__alignof__(struct xilinx_vdma_tx_segment),\r\n0);\r\n}\r\nif (!chan->desc_pool) {\r\ndev_err(chan->dev,\r\n"unable to allocate channel %d descriptor pool\n",\r\nchan->id);\r\nreturn -ENOMEM;\r\n}\r\nif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\r\nchan->seg_v = xilinx_axidma_alloc_tx_segment(chan);\r\nchan->cyclic_seg_v = xilinx_axidma_alloc_tx_segment(chan);\r\n}\r\ndma_cookie_init(dchan);\r\nif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\r\ndma_ctrl_set(chan, XILINX_DMA_REG_DMACR,\r\nXILINX_DMA_DMAXR_ALL_IRQ_MASK);\r\n}\r\nif ((chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) && chan->has_sg)\r\ndma_ctrl_set(chan, XILINX_DMA_REG_DMACR,\r\nXILINX_CDMA_CR_SGMODE);\r\nreturn 0;\r\n}\r\nstatic enum dma_status xilinx_dma_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nstruct xilinx_dma_tx_descriptor *desc;\r\nstruct xilinx_axidma_tx_segment *segment;\r\nstruct xilinx_axidma_desc_hw *hw;\r\nenum dma_status ret;\r\nunsigned long flags;\r\nu32 residue = 0;\r\nret = dma_cookie_status(dchan, cookie, txstate);\r\nif (ret == DMA_COMPLETE || !txstate)\r\nreturn ret;\r\nif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\r\nspin_lock_irqsave(&chan->lock, flags);\r\ndesc = list_last_entry(&chan->active_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\nif (chan->has_sg) {\r\nlist_for_each_entry(segment, &desc->segments, node) {\r\nhw = &segment->hw;\r\nresidue += (hw->control - hw->status) &\r\nXILINX_DMA_MAX_TRANS_LEN;\r\n}\r\n}\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nchan->residue = residue;\r\ndma_set_residue(txstate, chan->residue);\r\n}\r\nreturn ret;\r\n}\r\nstatic bool xilinx_dma_is_running(struct xilinx_dma_chan *chan)\r\n{\r\nreturn !(dma_ctrl_read(chan, XILINX_DMA_REG_DMASR) &\r\nXILINX_DMA_DMASR_HALTED) &&\r\n(dma_ctrl_read(chan, XILINX_DMA_REG_DMACR) &\r\nXILINX_DMA_DMACR_RUNSTOP);\r\n}\r\nstatic bool xilinx_dma_is_idle(struct xilinx_dma_chan *chan)\r\n{\r\nreturn dma_ctrl_read(chan, XILINX_DMA_REG_DMASR) &\r\nXILINX_DMA_DMASR_IDLE;\r\n}\r\nstatic void xilinx_dma_halt(struct xilinx_dma_chan *chan)\r\n{\r\nint err;\r\nu32 val;\r\ndma_ctrl_clr(chan, XILINX_DMA_REG_DMACR, XILINX_DMA_DMACR_RUNSTOP);\r\nerr = xilinx_dma_poll_timeout(chan, XILINX_DMA_REG_DMASR, val,\r\n(val & XILINX_DMA_DMASR_HALTED), 0,\r\nXILINX_DMA_LOOP_COUNT);\r\nif (err) {\r\ndev_err(chan->dev, "Cannot stop channel %p: %x\n",\r\nchan, dma_ctrl_read(chan, XILINX_DMA_REG_DMASR));\r\nchan->err = true;\r\n}\r\n}\r\nstatic void xilinx_dma_start(struct xilinx_dma_chan *chan)\r\n{\r\nint err;\r\nu32 val;\r\ndma_ctrl_set(chan, XILINX_DMA_REG_DMACR, XILINX_DMA_DMACR_RUNSTOP);\r\nerr = xilinx_dma_poll_timeout(chan, XILINX_DMA_REG_DMASR, val,\r\n!(val & XILINX_DMA_DMASR_HALTED), 0,\r\nXILINX_DMA_LOOP_COUNT);\r\nif (err) {\r\ndev_err(chan->dev, "Cannot start channel %p: %x\n",\r\nchan, dma_ctrl_read(chan, XILINX_DMA_REG_DMASR));\r\nchan->err = true;\r\n}\r\n}\r\nstatic void xilinx_vdma_start_transfer(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_vdma_config *config = &chan->config;\r\nstruct xilinx_dma_tx_descriptor *desc, *tail_desc;\r\nu32 reg;\r\nstruct xilinx_vdma_tx_segment *tail_segment;\r\nif (chan->err)\r\nreturn;\r\nif (list_empty(&chan->pending_list))\r\nreturn;\r\ndesc = list_first_entry(&chan->pending_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\ntail_desc = list_last_entry(&chan->pending_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\ntail_segment = list_last_entry(&tail_desc->segments,\r\nstruct xilinx_vdma_tx_segment, node);\r\nif (chan->has_sg && xilinx_dma_is_running(chan) &&\r\n!xilinx_dma_is_idle(chan)) {\r\ndev_dbg(chan->dev, "DMA controller still busy\n");\r\nreturn;\r\n}\r\nif (chan->has_sg)\r\ndma_ctrl_write(chan, XILINX_DMA_REG_CURDESC,\r\ndesc->async_tx.phys);\r\nreg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\r\nif (config->frm_cnt_en)\r\nreg |= XILINX_DMA_DMACR_FRAMECNT_EN;\r\nelse\r\nreg &= ~XILINX_DMA_DMACR_FRAMECNT_EN;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_FRMSTORE,\r\nchan->desc_pendingcount);\r\nif (chan->has_sg || !config->park)\r\nreg |= XILINX_DMA_DMACR_CIRC_EN;\r\nif (config->park)\r\nreg &= ~XILINX_DMA_DMACR_CIRC_EN;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\r\nif (config->park && (config->park_frm >= 0) &&\r\n(config->park_frm < chan->num_frms)) {\r\nif (chan->direction == DMA_MEM_TO_DEV)\r\ndma_write(chan, XILINX_DMA_REG_PARK_PTR,\r\nconfig->park_frm <<\r\nXILINX_DMA_PARK_PTR_RD_REF_SHIFT);\r\nelse\r\ndma_write(chan, XILINX_DMA_REG_PARK_PTR,\r\nconfig->park_frm <<\r\nXILINX_DMA_PARK_PTR_WR_REF_SHIFT);\r\n}\r\nxilinx_dma_start(chan);\r\nif (chan->err)\r\nreturn;\r\nif (chan->has_sg) {\r\ndma_ctrl_write(chan, XILINX_DMA_REG_TAILDESC,\r\ntail_segment->phys);\r\n} else {\r\nstruct xilinx_vdma_tx_segment *segment, *last = NULL;\r\nint i = 0;\r\nif (chan->desc_submitcount < chan->num_frms)\r\ni = chan->desc_submitcount;\r\nlist_for_each_entry(segment, &desc->segments, node) {\r\nif (chan->ext_addr)\r\nvdma_desc_write_64(chan,\r\nXILINX_VDMA_REG_START_ADDRESS_64(i++),\r\nsegment->hw.buf_addr,\r\nsegment->hw.buf_addr_msb);\r\nelse\r\nvdma_desc_write(chan,\r\nXILINX_VDMA_REG_START_ADDRESS(i++),\r\nsegment->hw.buf_addr);\r\nlast = segment;\r\n}\r\nif (!last)\r\nreturn;\r\nvdma_desc_write(chan, XILINX_DMA_REG_HSIZE, last->hw.hsize);\r\nvdma_desc_write(chan, XILINX_DMA_REG_FRMDLY_STRIDE,\r\nlast->hw.stride);\r\nvdma_desc_write(chan, XILINX_DMA_REG_VSIZE, last->hw.vsize);\r\n}\r\nif (!chan->has_sg) {\r\nlist_del(&desc->node);\r\nlist_add_tail(&desc->node, &chan->active_list);\r\nchan->desc_submitcount++;\r\nchan->desc_pendingcount--;\r\nif (chan->desc_submitcount == chan->num_frms)\r\nchan->desc_submitcount = 0;\r\n} else {\r\nlist_splice_tail_init(&chan->pending_list, &chan->active_list);\r\nchan->desc_pendingcount = 0;\r\n}\r\n}\r\nstatic void xilinx_cdma_start_transfer(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_dma_tx_descriptor *head_desc, *tail_desc;\r\nstruct xilinx_cdma_tx_segment *tail_segment;\r\nu32 ctrl_reg = dma_read(chan, XILINX_DMA_REG_DMACR);\r\nif (chan->err)\r\nreturn;\r\nif (list_empty(&chan->pending_list))\r\nreturn;\r\nhead_desc = list_first_entry(&chan->pending_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\ntail_desc = list_last_entry(&chan->pending_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\ntail_segment = list_last_entry(&tail_desc->segments,\r\nstruct xilinx_cdma_tx_segment, node);\r\nif (chan->desc_pendingcount <= XILINX_DMA_COALESCE_MAX) {\r\nctrl_reg &= ~XILINX_DMA_CR_COALESCE_MAX;\r\nctrl_reg |= chan->desc_pendingcount <<\r\nXILINX_DMA_CR_COALESCE_SHIFT;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMACR, ctrl_reg);\r\n}\r\nif (chan->has_sg) {\r\nxilinx_write(chan, XILINX_DMA_REG_CURDESC,\r\nhead_desc->async_tx.phys);\r\nxilinx_write(chan, XILINX_DMA_REG_TAILDESC,\r\ntail_segment->phys);\r\n} else {\r\nstruct xilinx_cdma_tx_segment *segment;\r\nstruct xilinx_cdma_desc_hw *hw;\r\nsegment = list_first_entry(&head_desc->segments,\r\nstruct xilinx_cdma_tx_segment,\r\nnode);\r\nhw = &segment->hw;\r\nxilinx_write(chan, XILINX_CDMA_REG_SRCADDR, hw->src_addr);\r\nxilinx_write(chan, XILINX_CDMA_REG_DSTADDR, hw->dest_addr);\r\ndma_ctrl_write(chan, XILINX_DMA_REG_BTT,\r\nhw->control & XILINX_DMA_MAX_TRANS_LEN);\r\n}\r\nlist_splice_tail_init(&chan->pending_list, &chan->active_list);\r\nchan->desc_pendingcount = 0;\r\n}\r\nstatic void xilinx_dma_start_transfer(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_dma_tx_descriptor *head_desc, *tail_desc;\r\nstruct xilinx_axidma_tx_segment *tail_segment, *old_head, *new_head;\r\nu32 reg;\r\nif (chan->err)\r\nreturn;\r\nif (list_empty(&chan->pending_list))\r\nreturn;\r\nif (chan->has_sg && xilinx_dma_is_running(chan) &&\r\n!xilinx_dma_is_idle(chan)) {\r\ndev_dbg(chan->dev, "DMA controller still busy\n");\r\nreturn;\r\n}\r\nhead_desc = list_first_entry(&chan->pending_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\ntail_desc = list_last_entry(&chan->pending_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\ntail_segment = list_last_entry(&tail_desc->segments,\r\nstruct xilinx_axidma_tx_segment, node);\r\nif (chan->has_sg && !chan->xdev->mcdma) {\r\nold_head = list_first_entry(&head_desc->segments,\r\nstruct xilinx_axidma_tx_segment, node);\r\nnew_head = chan->seg_v;\r\nnew_head->hw = old_head->hw;\r\nlist_replace_init(&old_head->node, &new_head->node);\r\nchan->seg_v = old_head;\r\ntail_segment->hw.next_desc = chan->seg_v->phys;\r\nhead_desc->async_tx.phys = new_head->phys;\r\n}\r\nreg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\r\nif (chan->desc_pendingcount <= XILINX_DMA_COALESCE_MAX) {\r\nreg &= ~XILINX_DMA_CR_COALESCE_MAX;\r\nreg |= chan->desc_pendingcount <<\r\nXILINX_DMA_CR_COALESCE_SHIFT;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\r\n}\r\nif (chan->has_sg && !chan->xdev->mcdma)\r\nxilinx_write(chan, XILINX_DMA_REG_CURDESC,\r\nhead_desc->async_tx.phys);\r\nif (chan->has_sg && chan->xdev->mcdma) {\r\nif (chan->direction == DMA_MEM_TO_DEV) {\r\ndma_ctrl_write(chan, XILINX_DMA_REG_CURDESC,\r\nhead_desc->async_tx.phys);\r\n} else {\r\nif (!chan->tdest) {\r\ndma_ctrl_write(chan, XILINX_DMA_REG_CURDESC,\r\nhead_desc->async_tx.phys);\r\n} else {\r\ndma_ctrl_write(chan,\r\nXILINX_DMA_MCRX_CDESC(chan->tdest),\r\nhead_desc->async_tx.phys);\r\n}\r\n}\r\n}\r\nxilinx_dma_start(chan);\r\nif (chan->err)\r\nreturn;\r\nif (chan->has_sg && !chan->xdev->mcdma) {\r\nif (chan->cyclic)\r\nxilinx_write(chan, XILINX_DMA_REG_TAILDESC,\r\nchan->cyclic_seg_v->phys);\r\nelse\r\nxilinx_write(chan, XILINX_DMA_REG_TAILDESC,\r\ntail_segment->phys);\r\n} else if (chan->has_sg && chan->xdev->mcdma) {\r\nif (chan->direction == DMA_MEM_TO_DEV) {\r\ndma_ctrl_write(chan, XILINX_DMA_REG_TAILDESC,\r\ntail_segment->phys);\r\n} else {\r\nif (!chan->tdest) {\r\ndma_ctrl_write(chan, XILINX_DMA_REG_TAILDESC,\r\ntail_segment->phys);\r\n} else {\r\ndma_ctrl_write(chan,\r\nXILINX_DMA_MCRX_TDESC(chan->tdest),\r\ntail_segment->phys);\r\n}\r\n}\r\n} else {\r\nstruct xilinx_axidma_tx_segment *segment;\r\nstruct xilinx_axidma_desc_hw *hw;\r\nsegment = list_first_entry(&head_desc->segments,\r\nstruct xilinx_axidma_tx_segment,\r\nnode);\r\nhw = &segment->hw;\r\nxilinx_write(chan, XILINX_DMA_REG_SRCDSTADDR, hw->buf_addr);\r\ndma_ctrl_write(chan, XILINX_DMA_REG_BTT,\r\nhw->control & XILINX_DMA_MAX_TRANS_LEN);\r\n}\r\nlist_splice_tail_init(&chan->pending_list, &chan->active_list);\r\nchan->desc_pendingcount = 0;\r\n}\r\nstatic void xilinx_dma_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nchan->start_transfer(chan);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void xilinx_dma_complete_descriptor(struct xilinx_dma_chan *chan)\r\n{\r\nstruct xilinx_dma_tx_descriptor *desc, *next;\r\nif (list_empty(&chan->active_list))\r\nreturn;\r\nlist_for_each_entry_safe(desc, next, &chan->active_list, node) {\r\nlist_del(&desc->node);\r\nif (!desc->cyclic)\r\ndma_cookie_complete(&desc->async_tx);\r\nlist_add_tail(&desc->node, &chan->done_list);\r\n}\r\n}\r\nstatic int xilinx_dma_reset(struct xilinx_dma_chan *chan)\r\n{\r\nint err;\r\nu32 tmp;\r\ndma_ctrl_set(chan, XILINX_DMA_REG_DMACR, XILINX_DMA_DMACR_RESET);\r\nerr = xilinx_dma_poll_timeout(chan, XILINX_DMA_REG_DMACR, tmp,\r\n!(tmp & XILINX_DMA_DMACR_RESET), 0,\r\nXILINX_DMA_LOOP_COUNT);\r\nif (err) {\r\ndev_err(chan->dev, "reset timeout, cr %x, sr %x\n",\r\ndma_ctrl_read(chan, XILINX_DMA_REG_DMACR),\r\ndma_ctrl_read(chan, XILINX_DMA_REG_DMASR));\r\nreturn -ETIMEDOUT;\r\n}\r\nchan->err = false;\r\nreturn err;\r\n}\r\nstatic int xilinx_dma_chan_reset(struct xilinx_dma_chan *chan)\r\n{\r\nint err;\r\nerr = xilinx_dma_reset(chan);\r\nif (err)\r\nreturn err;\r\ndma_ctrl_set(chan, XILINX_DMA_REG_DMACR,\r\nXILINX_DMA_DMAXR_ALL_IRQ_MASK);\r\nreturn 0;\r\n}\r\nstatic irqreturn_t xilinx_dma_irq_handler(int irq, void *data)\r\n{\r\nstruct xilinx_dma_chan *chan = data;\r\nu32 status;\r\nstatus = dma_ctrl_read(chan, XILINX_DMA_REG_DMASR);\r\nif (!(status & XILINX_DMA_DMAXR_ALL_IRQ_MASK))\r\nreturn IRQ_NONE;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMASR,\r\nstatus & XILINX_DMA_DMAXR_ALL_IRQ_MASK);\r\nif (status & XILINX_DMA_DMASR_ERR_IRQ) {\r\nu32 errors = status & XILINX_DMA_DMASR_ALL_ERR_MASK;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMASR,\r\nerrors & XILINX_DMA_DMASR_ERR_RECOVER_MASK);\r\nif (!chan->flush_on_fsync ||\r\n(errors & ~XILINX_DMA_DMASR_ERR_RECOVER_MASK)) {\r\ndev_err(chan->dev,\r\n"Channel %p has errors %x, cdr %x tdr %x\n",\r\nchan, errors,\r\ndma_ctrl_read(chan, XILINX_DMA_REG_CURDESC),\r\ndma_ctrl_read(chan, XILINX_DMA_REG_TAILDESC));\r\nchan->err = true;\r\n}\r\n}\r\nif (status & XILINX_DMA_DMASR_DLY_CNT_IRQ) {\r\ndev_dbg(chan->dev, "Inter-packet latency too long\n");\r\n}\r\nif (status & XILINX_DMA_DMASR_FRM_CNT_IRQ) {\r\nspin_lock(&chan->lock);\r\nxilinx_dma_complete_descriptor(chan);\r\nchan->start_transfer(chan);\r\nspin_unlock(&chan->lock);\r\n}\r\ntasklet_schedule(&chan->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void append_desc_queue(struct xilinx_dma_chan *chan,\r\nstruct xilinx_dma_tx_descriptor *desc)\r\n{\r\nstruct xilinx_vdma_tx_segment *tail_segment;\r\nstruct xilinx_dma_tx_descriptor *tail_desc;\r\nstruct xilinx_axidma_tx_segment *axidma_tail_segment;\r\nstruct xilinx_cdma_tx_segment *cdma_tail_segment;\r\nif (list_empty(&chan->pending_list))\r\ngoto append;\r\ntail_desc = list_last_entry(&chan->pending_list,\r\nstruct xilinx_dma_tx_descriptor, node);\r\nif (chan->xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\r\ntail_segment = list_last_entry(&tail_desc->segments,\r\nstruct xilinx_vdma_tx_segment,\r\nnode);\r\ntail_segment->hw.next_desc = (u32)desc->async_tx.phys;\r\n} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\r\ncdma_tail_segment = list_last_entry(&tail_desc->segments,\r\nstruct xilinx_cdma_tx_segment,\r\nnode);\r\ncdma_tail_segment->hw.next_desc = (u32)desc->async_tx.phys;\r\n} else {\r\naxidma_tail_segment = list_last_entry(&tail_desc->segments,\r\nstruct xilinx_axidma_tx_segment,\r\nnode);\r\naxidma_tail_segment->hw.next_desc = (u32)desc->async_tx.phys;\r\n}\r\nappend:\r\nlist_add_tail(&desc->node, &chan->pending_list);\r\nchan->desc_pendingcount++;\r\nif (chan->has_sg && (chan->xdev->dma_config->dmatype == XDMA_TYPE_VDMA)\r\n&& unlikely(chan->desc_pendingcount > chan->num_frms)) {\r\ndev_dbg(chan->dev, "desc pendingcount is too high\n");\r\nchan->desc_pendingcount = chan->num_frms;\r\n}\r\n}\r\nstatic dma_cookie_t xilinx_dma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct xilinx_dma_tx_descriptor *desc = to_dma_tx_descriptor(tx);\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nunsigned long flags;\r\nint err;\r\nif (chan->cyclic) {\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\nreturn -EBUSY;\r\n}\r\nif (chan->err) {\r\nerr = xilinx_dma_chan_reset(chan);\r\nif (err < 0)\r\nreturn err;\r\n}\r\nspin_lock_irqsave(&chan->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\nappend_desc_queue(chan, desc);\r\nif (desc->cyclic)\r\nchan->cyclic = true;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nxilinx_vdma_dma_prep_interleaved(struct dma_chan *dchan,\r\nstruct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nstruct xilinx_dma_tx_descriptor *desc;\r\nstruct xilinx_vdma_tx_segment *segment, *prev = NULL;\r\nstruct xilinx_vdma_desc_hw *hw;\r\nif (!is_slave_direction(xt->dir))\r\nreturn NULL;\r\nif (!xt->numf || !xt->sgl[0].size)\r\nreturn NULL;\r\nif (xt->frame_size != 1)\r\nreturn NULL;\r\ndesc = xilinx_dma_alloc_tx_descriptor(chan);\r\nif (!desc)\r\nreturn NULL;\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\r\ndesc->async_tx.tx_submit = xilinx_dma_tx_submit;\r\nasync_tx_ack(&desc->async_tx);\r\nsegment = xilinx_vdma_alloc_tx_segment(chan);\r\nif (!segment)\r\ngoto error;\r\nhw = &segment->hw;\r\nhw->vsize = xt->numf;\r\nhw->hsize = xt->sgl[0].size;\r\nhw->stride = (xt->sgl[0].icg + xt->sgl[0].size) <<\r\nXILINX_DMA_FRMDLY_STRIDE_STRIDE_SHIFT;\r\nhw->stride |= chan->config.frm_dly <<\r\nXILINX_DMA_FRMDLY_STRIDE_FRMDLY_SHIFT;\r\nif (xt->dir != DMA_MEM_TO_DEV) {\r\nif (chan->ext_addr) {\r\nhw->buf_addr = lower_32_bits(xt->dst_start);\r\nhw->buf_addr_msb = upper_32_bits(xt->dst_start);\r\n} else {\r\nhw->buf_addr = xt->dst_start;\r\n}\r\n} else {\r\nif (chan->ext_addr) {\r\nhw->buf_addr = lower_32_bits(xt->src_start);\r\nhw->buf_addr_msb = upper_32_bits(xt->src_start);\r\n} else {\r\nhw->buf_addr = xt->src_start;\r\n}\r\n}\r\nlist_add_tail(&segment->node, &desc->segments);\r\nprev = segment;\r\nsegment = list_first_entry(&desc->segments,\r\nstruct xilinx_vdma_tx_segment, node);\r\ndesc->async_tx.phys = segment->phys;\r\nreturn &desc->async_tx;\r\nerror:\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nxilinx_cdma_prep_memcpy(struct dma_chan *dchan, dma_addr_t dma_dst,\r\ndma_addr_t dma_src, size_t len, unsigned long flags)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nstruct xilinx_dma_tx_descriptor *desc;\r\nstruct xilinx_cdma_tx_segment *segment, *prev;\r\nstruct xilinx_cdma_desc_hw *hw;\r\nif (!len || len > XILINX_DMA_MAX_TRANS_LEN)\r\nreturn NULL;\r\ndesc = xilinx_dma_alloc_tx_descriptor(chan);\r\nif (!desc)\r\nreturn NULL;\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\r\ndesc->async_tx.tx_submit = xilinx_dma_tx_submit;\r\nsegment = xilinx_cdma_alloc_tx_segment(chan);\r\nif (!segment)\r\ngoto error;\r\nhw = &segment->hw;\r\nhw->control = len;\r\nhw->src_addr = dma_src;\r\nhw->dest_addr = dma_dst;\r\nif (chan->ext_addr) {\r\nhw->src_addr_msb = upper_32_bits(dma_src);\r\nhw->dest_addr_msb = upper_32_bits(dma_dst);\r\n}\r\nprev = list_last_entry(&desc->segments,\r\nstruct xilinx_cdma_tx_segment, node);\r\nprev->hw.next_desc = segment->phys;\r\nlist_add_tail(&segment->node, &desc->segments);\r\nprev = segment;\r\nsegment = list_first_entry(&desc->segments,\r\nstruct xilinx_cdma_tx_segment, node);\r\ndesc->async_tx.phys = segment->phys;\r\nprev->hw.next_desc = segment->phys;\r\nreturn &desc->async_tx;\r\nerror:\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *xilinx_dma_prep_slave_sg(\r\nstruct dma_chan *dchan, struct scatterlist *sgl, unsigned int sg_len,\r\nenum dma_transfer_direction direction, unsigned long flags,\r\nvoid *context)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nstruct xilinx_dma_tx_descriptor *desc;\r\nstruct xilinx_axidma_tx_segment *segment = NULL, *prev = NULL;\r\nu32 *app_w = (u32 *)context;\r\nstruct scatterlist *sg;\r\nsize_t copy;\r\nsize_t sg_used;\r\nunsigned int i;\r\nif (!is_slave_direction(direction))\r\nreturn NULL;\r\ndesc = xilinx_dma_alloc_tx_descriptor(chan);\r\nif (!desc)\r\nreturn NULL;\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\r\ndesc->async_tx.tx_submit = xilinx_dma_tx_submit;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nsg_used = 0;\r\nwhile (sg_used < sg_dma_len(sg)) {\r\nstruct xilinx_axidma_desc_hw *hw;\r\nsegment = xilinx_axidma_alloc_tx_segment(chan);\r\nif (!segment)\r\ngoto error;\r\ncopy = min_t(size_t, sg_dma_len(sg) - sg_used,\r\nXILINX_DMA_MAX_TRANS_LEN);\r\nhw = &segment->hw;\r\nxilinx_axidma_buf(chan, hw, sg_dma_address(sg),\r\nsg_used, 0);\r\nhw->control = copy;\r\nif (chan->direction == DMA_MEM_TO_DEV) {\r\nif (app_w)\r\nmemcpy(hw->app, app_w, sizeof(u32) *\r\nXILINX_DMA_NUM_APP_WORDS);\r\n}\r\nif (prev)\r\nprev->hw.next_desc = segment->phys;\r\nprev = segment;\r\nsg_used += copy;\r\nlist_add_tail(&segment->node, &desc->segments);\r\n}\r\n}\r\nsegment = list_first_entry(&desc->segments,\r\nstruct xilinx_axidma_tx_segment, node);\r\ndesc->async_tx.phys = segment->phys;\r\nprev->hw.next_desc = segment->phys;\r\nif (chan->direction == DMA_MEM_TO_DEV) {\r\nsegment->hw.control |= XILINX_DMA_BD_SOP;\r\nsegment = list_last_entry(&desc->segments,\r\nstruct xilinx_axidma_tx_segment,\r\nnode);\r\nsegment->hw.control |= XILINX_DMA_BD_EOP;\r\n}\r\nreturn &desc->async_tx;\r\nerror:\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *xilinx_dma_prep_dma_cyclic(\r\nstruct dma_chan *dchan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nstruct xilinx_dma_tx_descriptor *desc;\r\nstruct xilinx_axidma_tx_segment *segment, *head_segment, *prev = NULL;\r\nsize_t copy, sg_used;\r\nunsigned int num_periods;\r\nint i;\r\nu32 reg;\r\nif (!period_len)\r\nreturn NULL;\r\nnum_periods = buf_len / period_len;\r\nif (!num_periods)\r\nreturn NULL;\r\nif (!is_slave_direction(direction))\r\nreturn NULL;\r\ndesc = xilinx_dma_alloc_tx_descriptor(chan);\r\nif (!desc)\r\nreturn NULL;\r\nchan->direction = direction;\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\r\ndesc->async_tx.tx_submit = xilinx_dma_tx_submit;\r\nfor (i = 0; i < num_periods; ++i) {\r\nsg_used = 0;\r\nwhile (sg_used < period_len) {\r\nstruct xilinx_axidma_desc_hw *hw;\r\nsegment = xilinx_axidma_alloc_tx_segment(chan);\r\nif (!segment)\r\ngoto error;\r\ncopy = min_t(size_t, period_len - sg_used,\r\nXILINX_DMA_MAX_TRANS_LEN);\r\nhw = &segment->hw;\r\nxilinx_axidma_buf(chan, hw, buf_addr, sg_used,\r\nperiod_len * i);\r\nhw->control = copy;\r\nif (prev)\r\nprev->hw.next_desc = segment->phys;\r\nprev = segment;\r\nsg_used += copy;\r\nlist_add_tail(&segment->node, &desc->segments);\r\n}\r\n}\r\nhead_segment = list_first_entry(&desc->segments,\r\nstruct xilinx_axidma_tx_segment, node);\r\ndesc->async_tx.phys = head_segment->phys;\r\ndesc->cyclic = true;\r\nreg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\r\nreg |= XILINX_DMA_CR_CYCLIC_BD_EN_MASK;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\r\nsegment = list_last_entry(&desc->segments,\r\nstruct xilinx_axidma_tx_segment,\r\nnode);\r\nsegment->hw.next_desc = (u32) head_segment->phys;\r\nif (direction == DMA_MEM_TO_DEV) {\r\nhead_segment->hw.control |= XILINX_DMA_BD_SOP;\r\nsegment->hw.control |= XILINX_DMA_BD_EOP;\r\n}\r\nreturn &desc->async_tx;\r\nerror:\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nxilinx_dma_prep_interleaved(struct dma_chan *dchan,\r\nstruct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nstruct xilinx_dma_tx_descriptor *desc;\r\nstruct xilinx_axidma_tx_segment *segment;\r\nstruct xilinx_axidma_desc_hw *hw;\r\nif (!is_slave_direction(xt->dir))\r\nreturn NULL;\r\nif (!xt->numf || !xt->sgl[0].size)\r\nreturn NULL;\r\nif (xt->frame_size != 1)\r\nreturn NULL;\r\ndesc = xilinx_dma_alloc_tx_descriptor(chan);\r\nif (!desc)\r\nreturn NULL;\r\nchan->direction = xt->dir;\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\r\ndesc->async_tx.tx_submit = xilinx_dma_tx_submit;\r\nsegment = xilinx_axidma_alloc_tx_segment(chan);\r\nif (!segment)\r\ngoto error;\r\nhw = &segment->hw;\r\nif (xt->dir != DMA_MEM_TO_DEV)\r\nhw->buf_addr = xt->dst_start;\r\nelse\r\nhw->buf_addr = xt->src_start;\r\nhw->mcdma_control = chan->tdest & XILINX_DMA_BD_TDEST_MASK;\r\nhw->vsize_stride = (xt->numf << XILINX_DMA_BD_VSIZE_SHIFT) &\r\nXILINX_DMA_BD_VSIZE_MASK;\r\nhw->vsize_stride |= (xt->sgl[0].icg + xt->sgl[0].size) &\r\nXILINX_DMA_BD_STRIDE_MASK;\r\nhw->control = xt->sgl[0].size & XILINX_DMA_BD_HSIZE_MASK;\r\nlist_add_tail(&segment->node, &desc->segments);\r\nsegment = list_first_entry(&desc->segments,\r\nstruct xilinx_axidma_tx_segment, node);\r\ndesc->async_tx.phys = segment->phys;\r\nif (xt->dir == DMA_MEM_TO_DEV) {\r\nsegment->hw.control |= XILINX_DMA_BD_SOP;\r\nsegment = list_last_entry(&desc->segments,\r\nstruct xilinx_axidma_tx_segment,\r\nnode);\r\nsegment->hw.control |= XILINX_DMA_BD_EOP;\r\n}\r\nreturn &desc->async_tx;\r\nerror:\r\nxilinx_dma_free_tx_descriptor(chan, desc);\r\nreturn NULL;\r\n}\r\nstatic int xilinx_dma_terminate_all(struct dma_chan *dchan)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nu32 reg;\r\nif (chan->cyclic)\r\nxilinx_dma_chan_reset(chan);\r\nxilinx_dma_halt(chan);\r\nxilinx_dma_free_descriptors(chan);\r\nif (chan->cyclic) {\r\nreg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\r\nreg &= ~XILINX_DMA_CR_CYCLIC_BD_EN_MASK;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\r\nchan->cyclic = false;\r\n}\r\nreturn 0;\r\n}\r\nint xilinx_vdma_channel_set_config(struct dma_chan *dchan,\r\nstruct xilinx_vdma_config *cfg)\r\n{\r\nstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\r\nu32 dmacr;\r\nif (cfg->reset)\r\nreturn xilinx_dma_chan_reset(chan);\r\ndmacr = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\r\nchan->config.frm_dly = cfg->frm_dly;\r\nchan->config.park = cfg->park;\r\nchan->config.gen_lock = cfg->gen_lock;\r\nchan->config.master = cfg->master;\r\nif (cfg->gen_lock && chan->genlock) {\r\ndmacr |= XILINX_DMA_DMACR_GENLOCK_EN;\r\ndmacr |= cfg->master << XILINX_DMA_DMACR_MASTER_SHIFT;\r\n}\r\nchan->config.frm_cnt_en = cfg->frm_cnt_en;\r\nif (cfg->park)\r\nchan->config.park_frm = cfg->park_frm;\r\nelse\r\nchan->config.park_frm = -1;\r\nchan->config.coalesc = cfg->coalesc;\r\nchan->config.delay = cfg->delay;\r\nif (cfg->coalesc <= XILINX_DMA_DMACR_FRAME_COUNT_MAX) {\r\ndmacr |= cfg->coalesc << XILINX_DMA_DMACR_FRAME_COUNT_SHIFT;\r\nchan->config.coalesc = cfg->coalesc;\r\n}\r\nif (cfg->delay <= XILINX_DMA_DMACR_DELAY_MAX) {\r\ndmacr |= cfg->delay << XILINX_DMA_DMACR_DELAY_SHIFT;\r\nchan->config.delay = cfg->delay;\r\n}\r\ndmacr &= ~XILINX_DMA_DMACR_FSYNCSRC_MASK;\r\ndmacr |= cfg->ext_fsync << XILINX_DMA_DMACR_FSYNCSRC_SHIFT;\r\ndma_ctrl_write(chan, XILINX_DMA_REG_DMACR, dmacr);\r\nreturn 0;\r\n}\r\nstatic void xilinx_dma_chan_remove(struct xilinx_dma_chan *chan)\r\n{\r\ndma_ctrl_clr(chan, XILINX_DMA_REG_DMACR,\r\nXILINX_DMA_DMAXR_ALL_IRQ_MASK);\r\nif (chan->irq > 0)\r\nfree_irq(chan->irq, chan);\r\ntasklet_kill(&chan->tasklet);\r\nlist_del(&chan->common.device_node);\r\n}\r\nstatic int axidma_clk_init(struct platform_device *pdev, struct clk **axi_clk,\r\nstruct clk **tx_clk, struct clk **rx_clk,\r\nstruct clk **sg_clk, struct clk **tmp_clk)\r\n{\r\nint err;\r\n*tmp_clk = NULL;\r\n*axi_clk = devm_clk_get(&pdev->dev, "s_axi_lite_aclk");\r\nif (IS_ERR(*axi_clk)) {\r\nerr = PTR_ERR(*axi_clk);\r\ndev_err(&pdev->dev, "failed to get axi_aclk (%u)\n", err);\r\nreturn err;\r\n}\r\n*tx_clk = devm_clk_get(&pdev->dev, "m_axi_mm2s_aclk");\r\nif (IS_ERR(*tx_clk))\r\n*tx_clk = NULL;\r\n*rx_clk = devm_clk_get(&pdev->dev, "m_axi_s2mm_aclk");\r\nif (IS_ERR(*rx_clk))\r\n*rx_clk = NULL;\r\n*sg_clk = devm_clk_get(&pdev->dev, "m_axi_sg_aclk");\r\nif (IS_ERR(*sg_clk))\r\n*sg_clk = NULL;\r\nerr = clk_prepare_enable(*axi_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable axi_clk (%u)\n", err);\r\nreturn err;\r\n}\r\nerr = clk_prepare_enable(*tx_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable tx_clk (%u)\n", err);\r\ngoto err_disable_axiclk;\r\n}\r\nerr = clk_prepare_enable(*rx_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable rx_clk (%u)\n", err);\r\ngoto err_disable_txclk;\r\n}\r\nerr = clk_prepare_enable(*sg_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable sg_clk (%u)\n", err);\r\ngoto err_disable_rxclk;\r\n}\r\nreturn 0;\r\nerr_disable_rxclk:\r\nclk_disable_unprepare(*rx_clk);\r\nerr_disable_txclk:\r\nclk_disable_unprepare(*tx_clk);\r\nerr_disable_axiclk:\r\nclk_disable_unprepare(*axi_clk);\r\nreturn err;\r\n}\r\nstatic int axicdma_clk_init(struct platform_device *pdev, struct clk **axi_clk,\r\nstruct clk **dev_clk, struct clk **tmp_clk,\r\nstruct clk **tmp1_clk, struct clk **tmp2_clk)\r\n{\r\nint err;\r\n*tmp_clk = NULL;\r\n*tmp1_clk = NULL;\r\n*tmp2_clk = NULL;\r\n*axi_clk = devm_clk_get(&pdev->dev, "s_axi_lite_aclk");\r\nif (IS_ERR(*axi_clk)) {\r\nerr = PTR_ERR(*axi_clk);\r\ndev_err(&pdev->dev, "failed to get axi_clk (%u)\n", err);\r\nreturn err;\r\n}\r\n*dev_clk = devm_clk_get(&pdev->dev, "m_axi_aclk");\r\nif (IS_ERR(*dev_clk)) {\r\nerr = PTR_ERR(*dev_clk);\r\ndev_err(&pdev->dev, "failed to get dev_clk (%u)\n", err);\r\nreturn err;\r\n}\r\nerr = clk_prepare_enable(*axi_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable axi_clk (%u)\n", err);\r\nreturn err;\r\n}\r\nerr = clk_prepare_enable(*dev_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable dev_clk (%u)\n", err);\r\ngoto err_disable_axiclk;\r\n}\r\nreturn 0;\r\nerr_disable_axiclk:\r\nclk_disable_unprepare(*axi_clk);\r\nreturn err;\r\n}\r\nstatic int axivdma_clk_init(struct platform_device *pdev, struct clk **axi_clk,\r\nstruct clk **tx_clk, struct clk **txs_clk,\r\nstruct clk **rx_clk, struct clk **rxs_clk)\r\n{\r\nint err;\r\n*axi_clk = devm_clk_get(&pdev->dev, "s_axi_lite_aclk");\r\nif (IS_ERR(*axi_clk)) {\r\nerr = PTR_ERR(*axi_clk);\r\ndev_err(&pdev->dev, "failed to get axi_aclk (%u)\n", err);\r\nreturn err;\r\n}\r\n*tx_clk = devm_clk_get(&pdev->dev, "m_axi_mm2s_aclk");\r\nif (IS_ERR(*tx_clk))\r\n*tx_clk = NULL;\r\n*txs_clk = devm_clk_get(&pdev->dev, "m_axis_mm2s_aclk");\r\nif (IS_ERR(*txs_clk))\r\n*txs_clk = NULL;\r\n*rx_clk = devm_clk_get(&pdev->dev, "m_axi_s2mm_aclk");\r\nif (IS_ERR(*rx_clk))\r\n*rx_clk = NULL;\r\n*rxs_clk = devm_clk_get(&pdev->dev, "s_axis_s2mm_aclk");\r\nif (IS_ERR(*rxs_clk))\r\n*rxs_clk = NULL;\r\nerr = clk_prepare_enable(*axi_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable axi_clk (%u)\n", err);\r\nreturn err;\r\n}\r\nerr = clk_prepare_enable(*tx_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable tx_clk (%u)\n", err);\r\ngoto err_disable_axiclk;\r\n}\r\nerr = clk_prepare_enable(*txs_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable txs_clk (%u)\n", err);\r\ngoto err_disable_txclk;\r\n}\r\nerr = clk_prepare_enable(*rx_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable rx_clk (%u)\n", err);\r\ngoto err_disable_txsclk;\r\n}\r\nerr = clk_prepare_enable(*rxs_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable rxs_clk (%u)\n", err);\r\ngoto err_disable_rxclk;\r\n}\r\nreturn 0;\r\nerr_disable_rxclk:\r\nclk_disable_unprepare(*rx_clk);\r\nerr_disable_txsclk:\r\nclk_disable_unprepare(*txs_clk);\r\nerr_disable_txclk:\r\nclk_disable_unprepare(*tx_clk);\r\nerr_disable_axiclk:\r\nclk_disable_unprepare(*axi_clk);\r\nreturn err;\r\n}\r\nstatic void xdma_disable_allclks(struct xilinx_dma_device *xdev)\r\n{\r\nclk_disable_unprepare(xdev->rxs_clk);\r\nclk_disable_unprepare(xdev->rx_clk);\r\nclk_disable_unprepare(xdev->txs_clk);\r\nclk_disable_unprepare(xdev->tx_clk);\r\nclk_disable_unprepare(xdev->axi_clk);\r\n}\r\nstatic int xilinx_dma_chan_probe(struct xilinx_dma_device *xdev,\r\nstruct device_node *node, int chan_id)\r\n{\r\nstruct xilinx_dma_chan *chan;\r\nbool has_dre = false;\r\nu32 value, width;\r\nint err;\r\nchan = devm_kzalloc(xdev->dev, sizeof(*chan), GFP_KERNEL);\r\nif (!chan)\r\nreturn -ENOMEM;\r\nchan->dev = xdev->dev;\r\nchan->xdev = xdev;\r\nchan->has_sg = xdev->has_sg;\r\nchan->desc_pendingcount = 0x0;\r\nchan->ext_addr = xdev->ext_addr;\r\nspin_lock_init(&chan->lock);\r\nINIT_LIST_HEAD(&chan->pending_list);\r\nINIT_LIST_HEAD(&chan->done_list);\r\nINIT_LIST_HEAD(&chan->active_list);\r\nhas_dre = of_property_read_bool(node, "xlnx,include-dre");\r\nchan->genlock = of_property_read_bool(node, "xlnx,genlock-mode");\r\nerr = of_property_read_u32(node, "xlnx,datawidth", &value);\r\nif (err) {\r\ndev_err(xdev->dev, "missing xlnx,datawidth property\n");\r\nreturn err;\r\n}\r\nwidth = value >> 3;\r\nif (width > 8)\r\nhas_dre = false;\r\nif (!has_dre)\r\nxdev->common.copy_align = fls(width - 1);\r\nif (of_device_is_compatible(node, "xlnx,axi-vdma-mm2s-channel") ||\r\nof_device_is_compatible(node, "xlnx,axi-dma-mm2s-channel") ||\r\nof_device_is_compatible(node, "xlnx,axi-cdma-channel")) {\r\nchan->direction = DMA_MEM_TO_DEV;\r\nchan->id = chan_id;\r\nchan->tdest = chan_id;\r\nchan->ctrl_offset = XILINX_DMA_MM2S_CTRL_OFFSET;\r\nif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\r\nchan->desc_offset = XILINX_VDMA_MM2S_DESC_OFFSET;\r\nif (xdev->flush_on_fsync == XILINX_DMA_FLUSH_BOTH ||\r\nxdev->flush_on_fsync == XILINX_DMA_FLUSH_MM2S)\r\nchan->flush_on_fsync = true;\r\n}\r\n} else if (of_device_is_compatible(node,\r\n"xlnx,axi-vdma-s2mm-channel") ||\r\nof_device_is_compatible(node,\r\n"xlnx,axi-dma-s2mm-channel")) {\r\nchan->direction = DMA_DEV_TO_MEM;\r\nchan->id = chan_id;\r\nchan->tdest = chan_id - xdev->nr_channels;\r\nchan->ctrl_offset = XILINX_DMA_S2MM_CTRL_OFFSET;\r\nif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\r\nchan->desc_offset = XILINX_VDMA_S2MM_DESC_OFFSET;\r\nif (xdev->flush_on_fsync == XILINX_DMA_FLUSH_BOTH ||\r\nxdev->flush_on_fsync == XILINX_DMA_FLUSH_S2MM)\r\nchan->flush_on_fsync = true;\r\n}\r\n} else {\r\ndev_err(xdev->dev, "Invalid channel compatible node\n");\r\nreturn -EINVAL;\r\n}\r\nchan->irq = irq_of_parse_and_map(node, 0);\r\nerr = request_irq(chan->irq, xilinx_dma_irq_handler, IRQF_SHARED,\r\n"xilinx-dma-controller", chan);\r\nif (err) {\r\ndev_err(xdev->dev, "unable to request IRQ %d\n", chan->irq);\r\nreturn err;\r\n}\r\nif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA)\r\nchan->start_transfer = xilinx_dma_start_transfer;\r\nelse if (xdev->dma_config->dmatype == XDMA_TYPE_CDMA)\r\nchan->start_transfer = xilinx_cdma_start_transfer;\r\nelse\r\nchan->start_transfer = xilinx_vdma_start_transfer;\r\ntasklet_init(&chan->tasklet, xilinx_dma_do_tasklet,\r\n(unsigned long)chan);\r\nchan->common.device = &xdev->common;\r\nlist_add_tail(&chan->common.device_node, &xdev->common.channels);\r\nxdev->chan[chan->id] = chan;\r\nerr = xilinx_dma_chan_reset(chan);\r\nif (err < 0) {\r\ndev_err(xdev->dev, "Reset channel failed\n");\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int xilinx_dma_child_probe(struct xilinx_dma_device *xdev,\r\nstruct device_node *node) {\r\nint ret, i, nr_channels = 1;\r\nret = of_property_read_u32(node, "dma-channels", &nr_channels);\r\nif ((ret < 0) && xdev->mcdma)\r\ndev_warn(xdev->dev, "missing dma-channels property\n");\r\nfor (i = 0; i < nr_channels; i++)\r\nxilinx_dma_chan_probe(xdev, node, xdev->chan_id++);\r\nxdev->nr_channels += nr_channels;\r\nreturn 0;\r\n}\r\nstatic struct dma_chan *of_dma_xilinx_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct xilinx_dma_device *xdev = ofdma->of_dma_data;\r\nint chan_id = dma_spec->args[0];\r\nif (chan_id >= xdev->nr_channels || !xdev->chan[chan_id])\r\nreturn NULL;\r\nreturn dma_get_slave_channel(&xdev->chan[chan_id]->common);\r\n}\r\nstatic int xilinx_dma_probe(struct platform_device *pdev)\r\n{\r\nint (*clk_init)(struct platform_device *, struct clk **, struct clk **,\r\nstruct clk **, struct clk **, struct clk **)\r\n= axivdma_clk_init;\r\nstruct device_node *node = pdev->dev.of_node;\r\nstruct xilinx_dma_device *xdev;\r\nstruct device_node *child, *np = pdev->dev.of_node;\r\nstruct resource *io;\r\nu32 num_frames, addr_width;\r\nint i, err;\r\nxdev = devm_kzalloc(&pdev->dev, sizeof(*xdev), GFP_KERNEL);\r\nif (!xdev)\r\nreturn -ENOMEM;\r\nxdev->dev = &pdev->dev;\r\nif (np) {\r\nconst struct of_device_id *match;\r\nmatch = of_match_node(xilinx_dma_of_ids, np);\r\nif (match && match->data) {\r\nxdev->dma_config = match->data;\r\nclk_init = xdev->dma_config->clk_init;\r\n}\r\n}\r\nerr = clk_init(pdev, &xdev->axi_clk, &xdev->tx_clk, &xdev->txs_clk,\r\n&xdev->rx_clk, &xdev->rxs_clk);\r\nif (err)\r\nreturn err;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nxdev->regs = devm_ioremap_resource(&pdev->dev, io);\r\nif (IS_ERR(xdev->regs))\r\nreturn PTR_ERR(xdev->regs);\r\nxdev->has_sg = of_property_read_bool(node, "xlnx,include-sg");\r\nif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA)\r\nxdev->mcdma = of_property_read_bool(node, "xlnx,mcdma");\r\nif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\r\nerr = of_property_read_u32(node, "xlnx,num-fstores",\r\n&num_frames);\r\nif (err < 0) {\r\ndev_err(xdev->dev,\r\n"missing xlnx,num-fstores property\n");\r\nreturn err;\r\n}\r\nerr = of_property_read_u32(node, "xlnx,flush-fsync",\r\n&xdev->flush_on_fsync);\r\nif (err < 0)\r\ndev_warn(xdev->dev,\r\n"missing xlnx,flush-fsync property\n");\r\n}\r\nerr = of_property_read_u32(node, "xlnx,addrwidth", &addr_width);\r\nif (err < 0)\r\ndev_warn(xdev->dev, "missing xlnx,addrwidth property\n");\r\nif (addr_width > 32)\r\nxdev->ext_addr = true;\r\nelse\r\nxdev->ext_addr = false;\r\ndma_set_mask(xdev->dev, DMA_BIT_MASK(addr_width));\r\nxdev->common.dev = &pdev->dev;\r\nINIT_LIST_HEAD(&xdev->common.channels);\r\nif (!(xdev->dma_config->dmatype == XDMA_TYPE_CDMA)) {\r\ndma_cap_set(DMA_SLAVE, xdev->common.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);\r\n}\r\nxdev->common.device_alloc_chan_resources =\r\nxilinx_dma_alloc_chan_resources;\r\nxdev->common.device_free_chan_resources =\r\nxilinx_dma_free_chan_resources;\r\nxdev->common.device_terminate_all = xilinx_dma_terminate_all;\r\nxdev->common.device_tx_status = xilinx_dma_tx_status;\r\nxdev->common.device_issue_pending = xilinx_dma_issue_pending;\r\nif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\r\ndma_cap_set(DMA_CYCLIC, xdev->common.cap_mask);\r\nxdev->common.device_prep_slave_sg = xilinx_dma_prep_slave_sg;\r\nxdev->common.device_prep_dma_cyclic =\r\nxilinx_dma_prep_dma_cyclic;\r\nxdev->common.device_prep_interleaved_dma =\r\nxilinx_dma_prep_interleaved;\r\nxdev->common.residue_granularity =\r\nDMA_RESIDUE_GRANULARITY_SEGMENT;\r\n} else if (xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\r\ndma_cap_set(DMA_MEMCPY, xdev->common.cap_mask);\r\nxdev->common.device_prep_dma_memcpy = xilinx_cdma_prep_memcpy;\r\n} else {\r\nxdev->common.device_prep_interleaved_dma =\r\nxilinx_vdma_dma_prep_interleaved;\r\n}\r\nplatform_set_drvdata(pdev, xdev);\r\nfor_each_child_of_node(node, child) {\r\nerr = xilinx_dma_child_probe(xdev, child);\r\nif (err < 0)\r\ngoto disable_clks;\r\n}\r\nif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\r\nfor (i = 0; i < xdev->nr_channels; i++)\r\nif (xdev->chan[i])\r\nxdev->chan[i]->num_frms = num_frames;\r\n}\r\ndma_async_device_register(&xdev->common);\r\nerr = of_dma_controller_register(node, of_dma_xilinx_xlate,\r\nxdev);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "Unable to register DMA to DT\n");\r\ndma_async_device_unregister(&xdev->common);\r\ngoto error;\r\n}\r\ndev_info(&pdev->dev, "Xilinx AXI VDMA Engine Driver Probed!!\n");\r\nreturn 0;\r\ndisable_clks:\r\nxdma_disable_allclks(xdev);\r\nerror:\r\nfor (i = 0; i < xdev->nr_channels; i++)\r\nif (xdev->chan[i])\r\nxilinx_dma_chan_remove(xdev->chan[i]);\r\nreturn err;\r\n}\r\nstatic int xilinx_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct xilinx_dma_device *xdev = platform_get_drvdata(pdev);\r\nint i;\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&xdev->common);\r\nfor (i = 0; i < xdev->nr_channels; i++)\r\nif (xdev->chan[i])\r\nxilinx_dma_chan_remove(xdev->chan[i]);\r\nxdma_disable_allclks(xdev);\r\nreturn 0;\r\n}
