static inline struct hidma_dev *to_hidma_dev(struct dma_device *dmadev)\r\n{\r\nreturn container_of(dmadev, struct hidma_dev, ddev);\r\n}\r\nstatic inline\r\nstruct hidma_dev *to_hidma_dev_from_lldev(struct hidma_lldev **_lldevp)\r\n{\r\nreturn container_of(_lldevp, struct hidma_dev, lldev);\r\n}\r\nstatic inline struct hidma_chan *to_hidma_chan(struct dma_chan *dmach)\r\n{\r\nreturn container_of(dmach, struct hidma_chan, chan);\r\n}\r\nstatic inline\r\nstruct hidma_desc *to_hidma_desc(struct dma_async_tx_descriptor *t)\r\n{\r\nreturn container_of(t, struct hidma_desc, desc);\r\n}\r\nstatic void hidma_free(struct hidma_dev *dmadev)\r\n{\r\nINIT_LIST_HEAD(&dmadev->ddev.channels);\r\n}\r\nstatic void hidma_process_completed(struct hidma_chan *mchan)\r\n{\r\nstruct dma_device *ddev = mchan->chan.device;\r\nstruct hidma_dev *mdma = to_hidma_dev(ddev);\r\nstruct dma_async_tx_descriptor *desc;\r\ndma_cookie_t last_cookie;\r\nstruct hidma_desc *mdesc;\r\nstruct hidma_desc *next;\r\nunsigned long irqflags;\r\nstruct list_head list;\r\nINIT_LIST_HEAD(&list);\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nlist_splice_tail_init(&mchan->completed, &list);\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nlist_for_each_entry_safe(mdesc, next, &list, node) {\r\nenum dma_status llstat;\r\nstruct dmaengine_desc_callback cb;\r\nstruct dmaengine_result result;\r\ndesc = &mdesc->desc;\r\nlast_cookie = desc->cookie;\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\ndma_cookie_complete(desc);\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nllstat = hidma_ll_status(mdma->lldev, mdesc->tre_ch);\r\ndmaengine_desc_get_callback(desc, &cb);\r\ndma_run_dependencies(desc);\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nlist_move(&mdesc->node, &mchan->free);\r\nif (llstat == DMA_COMPLETE) {\r\nmchan->last_success = last_cookie;\r\nresult.result = DMA_TRANS_NOERROR;\r\n} else\r\nresult.result = DMA_TRANS_ABORTED;\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\ndmaengine_desc_callback_invoke(&cb, &result);\r\n}\r\n}\r\nstatic void hidma_callback(void *data)\r\n{\r\nstruct hidma_desc *mdesc = data;\r\nstruct hidma_chan *mchan = to_hidma_chan(mdesc->desc.chan);\r\nstruct dma_device *ddev = mchan->chan.device;\r\nstruct hidma_dev *dmadev = to_hidma_dev(ddev);\r\nunsigned long irqflags;\r\nbool queued = false;\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nif (mdesc->node.next) {\r\nlist_move_tail(&mdesc->node, &mchan->completed);\r\nqueued = true;\r\nmchan->running = list_first_entry(&mchan->active,\r\nstruct hidma_desc, node);\r\n}\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nhidma_process_completed(mchan);\r\nif (queued) {\r\npm_runtime_mark_last_busy(dmadev->ddev.dev);\r\npm_runtime_put_autosuspend(dmadev->ddev.dev);\r\n}\r\n}\r\nstatic int hidma_chan_init(struct hidma_dev *dmadev, u32 dma_sig)\r\n{\r\nstruct hidma_chan *mchan;\r\nstruct dma_device *ddev;\r\nmchan = devm_kzalloc(dmadev->ddev.dev, sizeof(*mchan), GFP_KERNEL);\r\nif (!mchan)\r\nreturn -ENOMEM;\r\nddev = &dmadev->ddev;\r\nmchan->dma_sig = dma_sig;\r\nmchan->dmadev = dmadev;\r\nmchan->chan.device = ddev;\r\ndma_cookie_init(&mchan->chan);\r\nINIT_LIST_HEAD(&mchan->free);\r\nINIT_LIST_HEAD(&mchan->prepared);\r\nINIT_LIST_HEAD(&mchan->active);\r\nINIT_LIST_HEAD(&mchan->completed);\r\nspin_lock_init(&mchan->lock);\r\nlist_add_tail(&mchan->chan.device_node, &ddev->channels);\r\ndmadev->ddev.chancnt++;\r\nreturn 0;\r\n}\r\nstatic void hidma_issue_task(unsigned long arg)\r\n{\r\nstruct hidma_dev *dmadev = (struct hidma_dev *)arg;\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\nhidma_ll_start(dmadev->lldev);\r\n}\r\nstatic void hidma_issue_pending(struct dma_chan *dmach)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(dmach);\r\nstruct hidma_dev *dmadev = mchan->dmadev;\r\nunsigned long flags;\r\nint status;\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nif (!mchan->running) {\r\nstruct hidma_desc *desc = list_first_entry(&mchan->active,\r\nstruct hidma_desc,\r\nnode);\r\nmchan->running = desc;\r\n}\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\nstatus = pm_runtime_get(dmadev->ddev.dev);\r\nif (status < 0)\r\ntasklet_schedule(&dmadev->task);\r\nelse\r\nhidma_ll_start(dmadev->lldev);\r\n}\r\nstatic inline bool hidma_txn_is_success(dma_cookie_t cookie,\r\ndma_cookie_t last_success, dma_cookie_t last_used)\r\n{\r\nif (last_success <= last_used) {\r\nif ((cookie <= last_success) || (cookie > last_used))\r\nreturn true;\r\n} else {\r\nif ((cookie <= last_success) && (cookie > last_used))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic enum dma_status hidma_tx_status(struct dma_chan *dmach,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(dmach);\r\nenum dma_status ret;\r\nret = dma_cookie_status(dmach, cookie, txstate);\r\nif (ret == DMA_COMPLETE) {\r\nbool is_success;\r\nis_success = hidma_txn_is_success(cookie, mchan->last_success,\r\ndmach->cookie);\r\nreturn is_success ? ret : DMA_ERROR;\r\n}\r\nif (mchan->paused && (ret == DMA_IN_PROGRESS)) {\r\nunsigned long flags;\r\ndma_cookie_t runcookie;\r\nspin_lock_irqsave(&mchan->lock, flags);\r\nif (mchan->running)\r\nruncookie = mchan->running->desc.cookie;\r\nelse\r\nruncookie = -EINVAL;\r\nif (runcookie == cookie)\r\nret = DMA_PAUSED;\r\nspin_unlock_irqrestore(&mchan->lock, flags);\r\n}\r\nreturn ret;\r\n}\r\nstatic dma_cookie_t hidma_tx_submit(struct dma_async_tx_descriptor *txd)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(txd->chan);\r\nstruct hidma_dev *dmadev = mchan->dmadev;\r\nstruct hidma_desc *mdesc;\r\nunsigned long irqflags;\r\ndma_cookie_t cookie;\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\nif (!hidma_ll_isenabled(dmadev->lldev)) {\r\npm_runtime_mark_last_busy(dmadev->ddev.dev);\r\npm_runtime_put_autosuspend(dmadev->ddev.dev);\r\nreturn -ENODEV;\r\n}\r\nmdesc = container_of(txd, struct hidma_desc, desc);\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nlist_move_tail(&mdesc->node, &mchan->active);\r\ncookie = dma_cookie_assign(txd);\r\nhidma_ll_queue_request(dmadev->lldev, mdesc->tre_ch);\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nreturn cookie;\r\n}\r\nstatic int hidma_alloc_chan_resources(struct dma_chan *dmach)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(dmach);\r\nstruct hidma_dev *dmadev = mchan->dmadev;\r\nstruct hidma_desc *mdesc, *tmp;\r\nunsigned long irqflags;\r\nLIST_HEAD(descs);\r\nunsigned int i;\r\nint rc = 0;\r\nif (mchan->allocated)\r\nreturn 0;\r\nfor (i = 0; i < dmadev->nr_descriptors; i++) {\r\nmdesc = kzalloc(sizeof(struct hidma_desc), GFP_NOWAIT);\r\nif (!mdesc) {\r\nrc = -ENOMEM;\r\nbreak;\r\n}\r\ndma_async_tx_descriptor_init(&mdesc->desc, dmach);\r\nmdesc->desc.tx_submit = hidma_tx_submit;\r\nrc = hidma_ll_request(dmadev->lldev, mchan->dma_sig,\r\n"DMA engine", hidma_callback, mdesc,\r\n&mdesc->tre_ch);\r\nif (rc) {\r\ndev_err(dmach->device->dev,\r\n"channel alloc failed at %u\n", i);\r\nkfree(mdesc);\r\nbreak;\r\n}\r\nlist_add_tail(&mdesc->node, &descs);\r\n}\r\nif (rc) {\r\nlist_for_each_entry_safe(mdesc, tmp, &descs, node) {\r\nhidma_ll_free(dmadev->lldev, mdesc->tre_ch);\r\nkfree(mdesc);\r\n}\r\nreturn rc;\r\n}\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nlist_splice_tail_init(&descs, &mchan->free);\r\nmchan->allocated = true;\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nreturn 1;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nhidma_prep_dma_memcpy(struct dma_chan *dmach, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(dmach);\r\nstruct hidma_desc *mdesc = NULL;\r\nstruct hidma_dev *mdma = mchan->dmadev;\r\nunsigned long irqflags;\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nif (!list_empty(&mchan->free)) {\r\nmdesc = list_first_entry(&mchan->free, struct hidma_desc, node);\r\nlist_del(&mdesc->node);\r\n}\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nif (!mdesc)\r\nreturn NULL;\r\nhidma_ll_set_transfer_params(mdma->lldev, mdesc->tre_ch,\r\nsrc, dest, len, flags);\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nlist_add_tail(&mdesc->node, &mchan->prepared);\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nreturn &mdesc->desc;\r\n}\r\nstatic int hidma_terminate_channel(struct dma_chan *chan)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(chan);\r\nstruct hidma_dev *dmadev = to_hidma_dev(mchan->chan.device);\r\nstruct hidma_desc *tmp, *mdesc;\r\nunsigned long irqflags;\r\nLIST_HEAD(list);\r\nint rc;\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\nhidma_process_completed(mchan);\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nmchan->last_success = 0;\r\nlist_splice_init(&mchan->active, &list);\r\nlist_splice_init(&mchan->prepared, &list);\r\nlist_splice_init(&mchan->completed, &list);\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\nrc = hidma_ll_disable(dmadev->lldev);\r\nif (rc) {\r\ndev_err(dmadev->ddev.dev, "channel did not pause\n");\r\ngoto out;\r\n}\r\nlist_for_each_entry_safe(mdesc, tmp, &list, node) {\r\nstruct dma_async_tx_descriptor *txd = &mdesc->desc;\r\ndma_descriptor_unmap(txd);\r\ndmaengine_desc_get_callback_invoke(txd, NULL);\r\ndma_run_dependencies(txd);\r\nlist_move(&mdesc->node, &mchan->free);\r\n}\r\nrc = hidma_ll_enable(dmadev->lldev);\r\nout:\r\npm_runtime_mark_last_busy(dmadev->ddev.dev);\r\npm_runtime_put_autosuspend(dmadev->ddev.dev);\r\nreturn rc;\r\n}\r\nstatic int hidma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(chan);\r\nstruct hidma_dev *dmadev = to_hidma_dev(mchan->chan.device);\r\nint rc;\r\nrc = hidma_terminate_channel(chan);\r\nif (rc)\r\nreturn rc;\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\nrc = hidma_ll_setup(dmadev->lldev);\r\npm_runtime_mark_last_busy(dmadev->ddev.dev);\r\npm_runtime_put_autosuspend(dmadev->ddev.dev);\r\nreturn rc;\r\n}\r\nstatic void hidma_free_chan_resources(struct dma_chan *dmach)\r\n{\r\nstruct hidma_chan *mchan = to_hidma_chan(dmach);\r\nstruct hidma_dev *mdma = mchan->dmadev;\r\nstruct hidma_desc *mdesc, *tmp;\r\nunsigned long irqflags;\r\nLIST_HEAD(descs);\r\nhidma_terminate_channel(dmach);\r\nspin_lock_irqsave(&mchan->lock, irqflags);\r\nlist_splice_tail_init(&mchan->free, &descs);\r\nlist_for_each_entry_safe(mdesc, tmp, &descs, node) {\r\nhidma_ll_free(mdma->lldev, mdesc->tre_ch);\r\nlist_del(&mdesc->node);\r\nkfree(mdesc);\r\n}\r\nmchan->allocated = 0;\r\nspin_unlock_irqrestore(&mchan->lock, irqflags);\r\n}\r\nstatic int hidma_pause(struct dma_chan *chan)\r\n{\r\nstruct hidma_chan *mchan;\r\nstruct hidma_dev *dmadev;\r\nmchan = to_hidma_chan(chan);\r\ndmadev = to_hidma_dev(mchan->chan.device);\r\nif (!mchan->paused) {\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\nif (hidma_ll_disable(dmadev->lldev))\r\ndev_warn(dmadev->ddev.dev, "channel did not stop\n");\r\nmchan->paused = true;\r\npm_runtime_mark_last_busy(dmadev->ddev.dev);\r\npm_runtime_put_autosuspend(dmadev->ddev.dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int hidma_resume(struct dma_chan *chan)\r\n{\r\nstruct hidma_chan *mchan;\r\nstruct hidma_dev *dmadev;\r\nint rc = 0;\r\nmchan = to_hidma_chan(chan);\r\ndmadev = to_hidma_dev(mchan->chan.device);\r\nif (mchan->paused) {\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\nrc = hidma_ll_enable(dmadev->lldev);\r\nif (!rc)\r\nmchan->paused = false;\r\nelse\r\ndev_err(dmadev->ddev.dev,\r\n"failed to resume the channel");\r\npm_runtime_mark_last_busy(dmadev->ddev.dev);\r\npm_runtime_put_autosuspend(dmadev->ddev.dev);\r\n}\r\nreturn rc;\r\n}\r\nstatic irqreturn_t hidma_chirq_handler(int chirq, void *arg)\r\n{\r\nstruct hidma_lldev *lldev = arg;\r\nreturn hidma_ll_inthandler(chirq, lldev);\r\n}\r\nstatic irqreturn_t hidma_chirq_handler_msi(int chirq, void *arg)\r\n{\r\nstruct hidma_lldev **lldevp = arg;\r\nstruct hidma_dev *dmadev = to_hidma_dev_from_lldev(lldevp);\r\nreturn hidma_ll_inthandler_msi(chirq, *lldevp,\r\n1 << (chirq - dmadev->msi_virqbase));\r\n}\r\nstatic ssize_t hidma_show_values(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct hidma_dev *mdev = platform_get_drvdata(pdev);\r\nbuf[0] = 0;\r\nif (strcmp(attr->attr.name, "chid") == 0)\r\nsprintf(buf, "%d\n", mdev->chidx);\r\nreturn strlen(buf);\r\n}\r\nstatic inline void hidma_sysfs_uninit(struct hidma_dev *dev)\r\n{\r\ndevice_remove_file(dev->ddev.dev, dev->chid_attrs);\r\n}\r\nstatic struct device_attribute*\r\nhidma_create_sysfs_entry(struct hidma_dev *dev, char *name, int mode)\r\n{\r\nstruct device_attribute *attrs;\r\nchar *name_copy;\r\nattrs = devm_kmalloc(dev->ddev.dev, sizeof(struct device_attribute),\r\nGFP_KERNEL);\r\nif (!attrs)\r\nreturn NULL;\r\nname_copy = devm_kstrdup(dev->ddev.dev, name, GFP_KERNEL);\r\nif (!name_copy)\r\nreturn NULL;\r\nattrs->attr.name = name_copy;\r\nattrs->attr.mode = mode;\r\nattrs->show = hidma_show_values;\r\nsysfs_attr_init(&attrs->attr);\r\nreturn attrs;\r\n}\r\nstatic int hidma_sysfs_init(struct hidma_dev *dev)\r\n{\r\ndev->chid_attrs = hidma_create_sysfs_entry(dev, "chid", S_IRUGO);\r\nif (!dev->chid_attrs)\r\nreturn -ENOMEM;\r\nreturn device_create_file(dev->ddev.dev, dev->chid_attrs);\r\n}\r\nstatic void hidma_write_msi_msg(struct msi_desc *desc, struct msi_msg *msg)\r\n{\r\nstruct device *dev = msi_desc_to_dev(desc);\r\nstruct hidma_dev *dmadev = dev_get_drvdata(dev);\r\nif (!desc->platform.msi_index) {\r\nwritel(msg->address_lo, dmadev->dev_evca + 0x118);\r\nwritel(msg->address_hi, dmadev->dev_evca + 0x11C);\r\nwritel(msg->data, dmadev->dev_evca + 0x120);\r\n}\r\n}\r\nstatic void hidma_free_msis(struct hidma_dev *dmadev)\r\n{\r\n#ifdef CONFIG_GENERIC_MSI_IRQ_DOMAIN\r\nstruct device *dev = dmadev->ddev.dev;\r\nstruct msi_desc *desc;\r\nfor_each_msi_entry(desc, dev)\r\ndevm_free_irq(dev, desc->irq, &dmadev->lldev);\r\nplatform_msi_domain_free_irqs(dev);\r\n#endif\r\n}\r\nstatic int hidma_request_msi(struct hidma_dev *dmadev,\r\nstruct platform_device *pdev)\r\n{\r\n#ifdef CONFIG_GENERIC_MSI_IRQ_DOMAIN\r\nint rc;\r\nstruct msi_desc *desc;\r\nstruct msi_desc *failed_desc = NULL;\r\nrc = platform_msi_domain_alloc_irqs(&pdev->dev, HIDMA_MSI_INTS,\r\nhidma_write_msi_msg);\r\nif (rc)\r\nreturn rc;\r\nfor_each_msi_entry(desc, &pdev->dev) {\r\nif (!desc->platform.msi_index)\r\ndmadev->msi_virqbase = desc->irq;\r\nrc = devm_request_irq(&pdev->dev, desc->irq,\r\nhidma_chirq_handler_msi,\r\n0, "qcom-hidma-msi",\r\n&dmadev->lldev);\r\nif (rc) {\r\nfailed_desc = desc;\r\nbreak;\r\n}\r\n}\r\nif (rc) {\r\nfor_each_msi_entry(desc, &pdev->dev) {\r\nif (desc == failed_desc)\r\nbreak;\r\ndevm_free_irq(&pdev->dev, desc->irq,\r\n&dmadev->lldev);\r\n}\r\n} else {\r\nhidma_ll_setup_irq(dmadev->lldev, true);\r\n}\r\nif (rc)\r\ndev_warn(&pdev->dev,\r\n"failed to request MSI irq, falling back to wired IRQ\n");\r\nreturn rc;\r\n#else\r\nreturn -EINVAL;\r\n#endif\r\n}\r\nstatic bool hidma_msi_capable(struct device *dev)\r\n{\r\nstruct acpi_device *adev = ACPI_COMPANION(dev);\r\nconst char *of_compat;\r\nint ret = -EINVAL;\r\nif (!adev || acpi_disabled) {\r\nret = device_property_read_string(dev, "compatible",\r\n&of_compat);\r\nif (ret)\r\nreturn false;\r\nret = strcmp(of_compat, "qcom,hidma-1.1");\r\n} else {\r\n#ifdef CONFIG_ACPI\r\nret = strcmp(acpi_device_hid(adev), "QCOM8062");\r\n#endif\r\n}\r\nreturn ret == 0;\r\n}\r\nstatic int hidma_probe(struct platform_device *pdev)\r\n{\r\nstruct hidma_dev *dmadev;\r\nstruct resource *trca_resource;\r\nstruct resource *evca_resource;\r\nint chirq;\r\nvoid __iomem *evca;\r\nvoid __iomem *trca;\r\nint rc;\r\nbool msi;\r\npm_runtime_set_autosuspend_delay(&pdev->dev, HIDMA_AUTOSUSPEND_TIMEOUT);\r\npm_runtime_use_autosuspend(&pdev->dev);\r\npm_runtime_set_active(&pdev->dev);\r\npm_runtime_enable(&pdev->dev);\r\ntrca_resource = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ntrca = devm_ioremap_resource(&pdev->dev, trca_resource);\r\nif (IS_ERR(trca)) {\r\nrc = -ENOMEM;\r\ngoto bailout;\r\n}\r\nevca_resource = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nevca = devm_ioremap_resource(&pdev->dev, evca_resource);\r\nif (IS_ERR(evca)) {\r\nrc = -ENOMEM;\r\ngoto bailout;\r\n}\r\nchirq = platform_get_irq(pdev, 0);\r\nif (chirq < 0) {\r\nrc = -ENODEV;\r\ngoto bailout;\r\n}\r\ndmadev = devm_kzalloc(&pdev->dev, sizeof(*dmadev), GFP_KERNEL);\r\nif (!dmadev) {\r\nrc = -ENOMEM;\r\ngoto bailout;\r\n}\r\nINIT_LIST_HEAD(&dmadev->ddev.channels);\r\nspin_lock_init(&dmadev->lock);\r\ndmadev->ddev.dev = &pdev->dev;\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\ndma_cap_set(DMA_MEMCPY, dmadev->ddev.cap_mask);\r\nif (WARN_ON(!pdev->dev.dma_mask)) {\r\nrc = -ENXIO;\r\ngoto dmafree;\r\n}\r\ndmadev->dev_evca = evca;\r\ndmadev->evca_resource = evca_resource;\r\ndmadev->dev_trca = trca;\r\ndmadev->trca_resource = trca_resource;\r\ndmadev->ddev.device_prep_dma_memcpy = hidma_prep_dma_memcpy;\r\ndmadev->ddev.device_alloc_chan_resources = hidma_alloc_chan_resources;\r\ndmadev->ddev.device_free_chan_resources = hidma_free_chan_resources;\r\ndmadev->ddev.device_tx_status = hidma_tx_status;\r\ndmadev->ddev.device_issue_pending = hidma_issue_pending;\r\ndmadev->ddev.device_pause = hidma_pause;\r\ndmadev->ddev.device_resume = hidma_resume;\r\ndmadev->ddev.device_terminate_all = hidma_terminate_all;\r\ndmadev->ddev.copy_align = 8;\r\nmsi = hidma_msi_capable(&pdev->dev);\r\ndevice_property_read_u32(&pdev->dev, "desc-count",\r\n&dmadev->nr_descriptors);\r\nif (!dmadev->nr_descriptors && nr_desc_prm)\r\ndmadev->nr_descriptors = nr_desc_prm;\r\nif (!dmadev->nr_descriptors)\r\ndmadev->nr_descriptors = HIDMA_NR_DEFAULT_DESC;\r\ndmadev->chidx = readl(dmadev->dev_trca + 0x28);\r\nrc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\r\nif (rc) {\r\ndev_warn(&pdev->dev, "unable to set coherent mask to 64");\r\nrc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\r\nif (rc)\r\ngoto dmafree;\r\n}\r\ndmadev->lldev = hidma_ll_init(dmadev->ddev.dev,\r\ndmadev->nr_descriptors, dmadev->dev_trca,\r\ndmadev->dev_evca, dmadev->chidx);\r\nif (!dmadev->lldev) {\r\nrc = -EPROBE_DEFER;\r\ngoto dmafree;\r\n}\r\nplatform_set_drvdata(pdev, dmadev);\r\nif (msi)\r\nrc = hidma_request_msi(dmadev, pdev);\r\nif (!msi || rc) {\r\nhidma_ll_setup_irq(dmadev->lldev, false);\r\nrc = devm_request_irq(&pdev->dev, chirq, hidma_chirq_handler,\r\n0, "qcom-hidma", dmadev->lldev);\r\nif (rc)\r\ngoto uninit;\r\n}\r\nINIT_LIST_HEAD(&dmadev->ddev.channels);\r\nrc = hidma_chan_init(dmadev, 0);\r\nif (rc)\r\ngoto uninit;\r\nrc = dma_async_device_register(&dmadev->ddev);\r\nif (rc)\r\ngoto uninit;\r\ndmadev->irq = chirq;\r\ntasklet_init(&dmadev->task, hidma_issue_task, (unsigned long)dmadev);\r\nhidma_debug_init(dmadev);\r\nhidma_sysfs_init(dmadev);\r\ndev_info(&pdev->dev, "HI-DMA engine driver registration complete\n");\r\npm_runtime_mark_last_busy(dmadev->ddev.dev);\r\npm_runtime_put_autosuspend(dmadev->ddev.dev);\r\nreturn 0;\r\nuninit:\r\nif (msi)\r\nhidma_free_msis(dmadev);\r\nhidma_debug_uninit(dmadev);\r\nhidma_ll_uninit(dmadev->lldev);\r\ndmafree:\r\nif (dmadev)\r\nhidma_free(dmadev);\r\nbailout:\r\npm_runtime_put_sync(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nreturn rc;\r\n}\r\nstatic int hidma_remove(struct platform_device *pdev)\r\n{\r\nstruct hidma_dev *dmadev = platform_get_drvdata(pdev);\r\npm_runtime_get_sync(dmadev->ddev.dev);\r\ndma_async_device_unregister(&dmadev->ddev);\r\nif (!dmadev->lldev->msi_support)\r\ndevm_free_irq(dmadev->ddev.dev, dmadev->irq, dmadev->lldev);\r\nelse\r\nhidma_free_msis(dmadev);\r\ntasklet_kill(&dmadev->task);\r\nhidma_sysfs_uninit(dmadev);\r\nhidma_debug_uninit(dmadev);\r\nhidma_ll_uninit(dmadev->lldev);\r\nhidma_free(dmadev);\r\ndev_info(&pdev->dev, "HI-DMA engine removed\n");\r\npm_runtime_put_sync_suspend(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nreturn 0;\r\n}
