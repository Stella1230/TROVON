static struct sock_reuseport *__reuseport_alloc(u16 max_socks)\r\n{\r\nsize_t size = sizeof(struct sock_reuseport) +\r\nsizeof(struct sock *) * max_socks;\r\nstruct sock_reuseport *reuse = kzalloc(size, GFP_ATOMIC);\r\nif (!reuse)\r\nreturn NULL;\r\nreuse->max_socks = max_socks;\r\nRCU_INIT_POINTER(reuse->prog, NULL);\r\nreturn reuse;\r\n}\r\nint reuseport_alloc(struct sock *sk)\r\n{\r\nstruct sock_reuseport *reuse;\r\nspin_lock_bh(&reuseport_lock);\r\nWARN_ONCE(rcu_dereference_protected(sk->sk_reuseport_cb,\r\nlockdep_is_held(&reuseport_lock)),\r\n"multiple allocations for the same socket");\r\nreuse = __reuseport_alloc(INIT_SOCKS);\r\nif (!reuse) {\r\nspin_unlock_bh(&reuseport_lock);\r\nreturn -ENOMEM;\r\n}\r\nreuse->socks[0] = sk;\r\nreuse->num_socks = 1;\r\nrcu_assign_pointer(sk->sk_reuseport_cb, reuse);\r\nspin_unlock_bh(&reuseport_lock);\r\nreturn 0;\r\n}\r\nstatic struct sock_reuseport *reuseport_grow(struct sock_reuseport *reuse)\r\n{\r\nstruct sock_reuseport *more_reuse;\r\nu32 more_socks_size, i;\r\nmore_socks_size = reuse->max_socks * 2U;\r\nif (more_socks_size > U16_MAX)\r\nreturn NULL;\r\nmore_reuse = __reuseport_alloc(more_socks_size);\r\nif (!more_reuse)\r\nreturn NULL;\r\nmore_reuse->max_socks = more_socks_size;\r\nmore_reuse->num_socks = reuse->num_socks;\r\nmore_reuse->prog = reuse->prog;\r\nmemcpy(more_reuse->socks, reuse->socks,\r\nreuse->num_socks * sizeof(struct sock *));\r\nfor (i = 0; i < reuse->num_socks; ++i)\r\nrcu_assign_pointer(reuse->socks[i]->sk_reuseport_cb,\r\nmore_reuse);\r\nkfree_rcu(reuse, rcu);\r\nreturn more_reuse;\r\n}\r\nint reuseport_add_sock(struct sock *sk, struct sock *sk2)\r\n{\r\nstruct sock_reuseport *reuse;\r\nif (!rcu_access_pointer(sk2->sk_reuseport_cb)) {\r\nint err = reuseport_alloc(sk2);\r\nif (err)\r\nreturn err;\r\n}\r\nspin_lock_bh(&reuseport_lock);\r\nreuse = rcu_dereference_protected(sk2->sk_reuseport_cb,\r\nlockdep_is_held(&reuseport_lock)),\r\nWARN_ONCE(rcu_dereference_protected(sk->sk_reuseport_cb,\r\nlockdep_is_held(&reuseport_lock)),\r\n"socket already in reuseport group");\r\nif (reuse->num_socks == reuse->max_socks) {\r\nreuse = reuseport_grow(reuse);\r\nif (!reuse) {\r\nspin_unlock_bh(&reuseport_lock);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreuse->socks[reuse->num_socks] = sk;\r\nsmp_wmb();\r\nreuse->num_socks++;\r\nrcu_assign_pointer(sk->sk_reuseport_cb, reuse);\r\nspin_unlock_bh(&reuseport_lock);\r\nreturn 0;\r\n}\r\nstatic void reuseport_free_rcu(struct rcu_head *head)\r\n{\r\nstruct sock_reuseport *reuse;\r\nreuse = container_of(head, struct sock_reuseport, rcu);\r\nif (reuse->prog)\r\nbpf_prog_destroy(reuse->prog);\r\nkfree(reuse);\r\n}\r\nvoid reuseport_detach_sock(struct sock *sk)\r\n{\r\nstruct sock_reuseport *reuse;\r\nint i;\r\nspin_lock_bh(&reuseport_lock);\r\nreuse = rcu_dereference_protected(sk->sk_reuseport_cb,\r\nlockdep_is_held(&reuseport_lock));\r\nrcu_assign_pointer(sk->sk_reuseport_cb, NULL);\r\nfor (i = 0; i < reuse->num_socks; i++) {\r\nif (reuse->socks[i] == sk) {\r\nreuse->socks[i] = reuse->socks[reuse->num_socks - 1];\r\nreuse->num_socks--;\r\nif (reuse->num_socks == 0)\r\ncall_rcu(&reuse->rcu, reuseport_free_rcu);\r\nbreak;\r\n}\r\n}\r\nspin_unlock_bh(&reuseport_lock);\r\n}\r\nstatic struct sock *run_bpf(struct sock_reuseport *reuse, u16 socks,\r\nstruct bpf_prog *prog, struct sk_buff *skb,\r\nint hdr_len)\r\n{\r\nstruct sk_buff *nskb = NULL;\r\nu32 index;\r\nif (skb_shared(skb)) {\r\nnskb = skb_clone(skb, GFP_ATOMIC);\r\nif (!nskb)\r\nreturn NULL;\r\nskb = nskb;\r\n}\r\nif (!pskb_pull(skb, hdr_len)) {\r\nkfree_skb(nskb);\r\nreturn NULL;\r\n}\r\nindex = bpf_prog_run_save_cb(prog, skb);\r\n__skb_push(skb, hdr_len);\r\nconsume_skb(nskb);\r\nif (index >= socks)\r\nreturn NULL;\r\nreturn reuse->socks[index];\r\n}\r\nstruct sock *reuseport_select_sock(struct sock *sk,\r\nu32 hash,\r\nstruct sk_buff *skb,\r\nint hdr_len)\r\n{\r\nstruct sock_reuseport *reuse;\r\nstruct bpf_prog *prog;\r\nstruct sock *sk2 = NULL;\r\nu16 socks;\r\nrcu_read_lock();\r\nreuse = rcu_dereference(sk->sk_reuseport_cb);\r\nif (!reuse)\r\ngoto out;\r\nprog = rcu_dereference(reuse->prog);\r\nsocks = READ_ONCE(reuse->num_socks);\r\nif (likely(socks)) {\r\nsmp_rmb();\r\nif (prog && skb)\r\nsk2 = run_bpf(reuse, socks, prog, skb, hdr_len);\r\nelse\r\nsk2 = reuse->socks[reciprocal_scale(hash, socks)];\r\n}\r\nout:\r\nrcu_read_unlock();\r\nreturn sk2;\r\n}\r\nstruct bpf_prog *\r\nreuseport_attach_prog(struct sock *sk, struct bpf_prog *prog)\r\n{\r\nstruct sock_reuseport *reuse;\r\nstruct bpf_prog *old_prog;\r\nspin_lock_bh(&reuseport_lock);\r\nreuse = rcu_dereference_protected(sk->sk_reuseport_cb,\r\nlockdep_is_held(&reuseport_lock));\r\nold_prog = rcu_dereference_protected(reuse->prog,\r\nlockdep_is_held(&reuseport_lock));\r\nrcu_assign_pointer(reuse->prog, prog);\r\nspin_unlock_bh(&reuseport_lock);\r\nreturn old_prog;\r\n}
