void tsc_verify_tsc_adjust(bool resume)\r\n{\r\nstruct tsc_adjust *adj = this_cpu_ptr(&tsc_adjust);\r\ns64 curval;\r\nif (!boot_cpu_has(X86_FEATURE_TSC_ADJUST))\r\nreturn;\r\nif (!resume && time_before(jiffies, adj->nextcheck))\r\nreturn;\r\nadj->nextcheck = jiffies + HZ;\r\nrdmsrl(MSR_IA32_TSC_ADJUST, curval);\r\nif (adj->adjusted == curval)\r\nreturn;\r\nwrmsrl(MSR_IA32_TSC_ADJUST, adj->adjusted);\r\nif (!adj->warned || resume) {\r\npr_warn(FW_BUG "TSC ADJUST differs: CPU%u %lld --> %lld. Restoring\n",\r\nsmp_processor_id(), adj->adjusted, curval);\r\nadj->warned = true;\r\n}\r\n}\r\nstatic void tsc_sanitize_first_cpu(struct tsc_adjust *cur, s64 bootval,\r\nunsigned int cpu, bool bootcpu)\r\n{\r\nif ((bootcpu && bootval != 0) || (!bootcpu && bootval < 0) ||\r\n(bootval > 0x7FFFFFFF)) {\r\npr_warn(FW_BUG "TSC ADJUST: CPU%u: %lld force to 0\n", cpu,\r\nbootval);\r\nwrmsrl(MSR_IA32_TSC_ADJUST, 0);\r\nbootval = 0;\r\n}\r\ncur->adjusted = bootval;\r\n}\r\nbool __init tsc_store_and_check_tsc_adjust(bool bootcpu)\r\n{\r\nstruct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);\r\ns64 bootval;\r\nif (!boot_cpu_has(X86_FEATURE_TSC_ADJUST))\r\nreturn false;\r\nrdmsrl(MSR_IA32_TSC_ADJUST, bootval);\r\ncur->bootval = bootval;\r\ncur->nextcheck = jiffies + HZ;\r\ntsc_sanitize_first_cpu(cur, bootval, smp_processor_id(), bootcpu);\r\nreturn false;\r\n}\r\nbool tsc_store_and_check_tsc_adjust(bool bootcpu)\r\n{\r\nstruct tsc_adjust *ref, *cur = this_cpu_ptr(&tsc_adjust);\r\nunsigned int refcpu, cpu = smp_processor_id();\r\nstruct cpumask *mask;\r\ns64 bootval;\r\nif (!boot_cpu_has(X86_FEATURE_TSC_ADJUST))\r\nreturn false;\r\nrdmsrl(MSR_IA32_TSC_ADJUST, bootval);\r\ncur->bootval = bootval;\r\ncur->nextcheck = jiffies + HZ;\r\ncur->warned = false;\r\nmask = topology_core_cpumask(cpu);\r\nrefcpu = mask ? cpumask_any_but(mask, cpu) : nr_cpu_ids;\r\nif (refcpu >= nr_cpu_ids) {\r\ntsc_sanitize_first_cpu(cur, bootval, smp_processor_id(),\r\nbootcpu);\r\nreturn false;\r\n}\r\nref = per_cpu_ptr(&tsc_adjust, refcpu);\r\nif (bootval != ref->bootval) {\r\npr_warn(FW_BUG "TSC ADJUST differs: Reference CPU%u: %lld CPU%u: %lld\n",\r\nrefcpu, ref->bootval, cpu, bootval);\r\n}\r\nif (bootval != ref->adjusted) {\r\npr_warn("TSC ADJUST synchronize: Reference CPU%u: %lld CPU%u: %lld\n",\r\nrefcpu, ref->adjusted, cpu, bootval);\r\ncur->adjusted = ref->adjusted;\r\nwrmsrl(MSR_IA32_TSC_ADJUST, ref->adjusted);\r\n}\r\nreturn true;\r\n}\r\nstatic cycles_t check_tsc_warp(unsigned int timeout)\r\n{\r\ncycles_t start, now, prev, end, cur_max_warp = 0;\r\nint i, cur_warps = 0;\r\nstart = rdtsc_ordered();\r\nend = start + (cycles_t) tsc_khz * timeout;\r\nnow = start;\r\nfor (i = 0; ; i++) {\r\narch_spin_lock(&sync_lock);\r\nprev = last_tsc;\r\nnow = rdtsc_ordered();\r\nlast_tsc = now;\r\narch_spin_unlock(&sync_lock);\r\nif (unlikely(!(i & 7))) {\r\nif (now > end || i > 10000000)\r\nbreak;\r\ncpu_relax();\r\ntouch_nmi_watchdog();\r\n}\r\nif (unlikely(prev > now)) {\r\narch_spin_lock(&sync_lock);\r\nmax_warp = max(max_warp, prev - now);\r\ncur_max_warp = max_warp;\r\nif (cur_warps != nr_warps)\r\nrandom_warps++;\r\nnr_warps++;\r\ncur_warps = nr_warps;\r\narch_spin_unlock(&sync_lock);\r\n}\r\n}\r\nWARN(!(now-start),\r\n"Warning: zero tsc calibration delta: %Ld [max: %Ld]\n",\r\nnow-start, end-start);\r\nreturn cur_max_warp;\r\n}\r\nstatic inline unsigned int loop_timeout(int cpu)\r\n{\r\nreturn (cpumask_weight(topology_core_cpumask(cpu)) > 1) ? 2 : 20;\r\n}\r\nvoid check_tsc_sync_source(int cpu)\r\n{\r\nint cpus = 2;\r\nif (unsynchronized_tsc())\r\nreturn;\r\nif (!boot_cpu_has(X86_FEATURE_TSC_ADJUST))\r\natomic_set(&test_runs, 1);\r\nelse\r\natomic_set(&test_runs, 3);\r\nretry:\r\nwhile (atomic_read(&start_count) != cpus - 1) {\r\nif (atomic_read(&skip_test) > 0) {\r\natomic_set(&skip_test, 0);\r\nreturn;\r\n}\r\ncpu_relax();\r\n}\r\natomic_inc(&start_count);\r\ncheck_tsc_warp(loop_timeout(cpu));\r\nwhile (atomic_read(&stop_count) != cpus-1)\r\ncpu_relax();\r\nif (!nr_warps) {\r\natomic_set(&test_runs, 0);\r\npr_debug("TSC synchronization [CPU#%d -> CPU#%d]: passed\n",\r\nsmp_processor_id(), cpu);\r\n} else if (atomic_dec_and_test(&test_runs) || random_warps) {\r\natomic_set(&test_runs, 0);\r\npr_warning("TSC synchronization [CPU#%d -> CPU#%d]:\n",\r\nsmp_processor_id(), cpu);\r\npr_warning("Measured %Ld cycles TSC warp between CPUs, "\r\n"turning off TSC clock.\n", max_warp);\r\nif (random_warps)\r\npr_warning("TSC warped randomly between CPUs\n");\r\nmark_tsc_unstable("check_tsc_sync_source failed");\r\n}\r\natomic_set(&start_count, 0);\r\nrandom_warps = 0;\r\nnr_warps = 0;\r\nmax_warp = 0;\r\nlast_tsc = 0;\r\natomic_inc(&stop_count);\r\nif (atomic_read(&test_runs) > 0)\r\ngoto retry;\r\n}\r\nvoid check_tsc_sync_target(void)\r\n{\r\nstruct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);\r\nunsigned int cpu = smp_processor_id();\r\ncycles_t cur_max_warp, gbl_max_warp;\r\nint cpus = 2;\r\nif (unsynchronized_tsc())\r\nreturn;\r\nif (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable) {\r\natomic_inc(&skip_test);\r\nreturn;\r\n}\r\nretry:\r\natomic_inc(&start_count);\r\nwhile (atomic_read(&start_count) != cpus)\r\ncpu_relax();\r\ncur_max_warp = check_tsc_warp(loop_timeout(cpu));\r\ngbl_max_warp = max_warp;\r\natomic_inc(&stop_count);\r\nwhile (atomic_read(&stop_count) != cpus)\r\ncpu_relax();\r\natomic_set(&stop_count, 0);\r\nif (!atomic_read(&test_runs))\r\nreturn;\r\nif (!cur_max_warp)\r\ncur_max_warp = -gbl_max_warp;\r\ncur->adjusted += cur_max_warp;\r\nif (cur->adjusted < 0)\r\ncur->adjusted = 0;\r\nif (cur->adjusted > 0x7FFFFFFF)\r\ncur->adjusted = 0x7FFFFFFF;\r\npr_warn("TSC ADJUST compensate: CPU%u observed %lld warp. Adjust: %lld\n",\r\ncpu, cur_max_warp, cur->adjusted);\r\nwrmsrl(MSR_IA32_TSC_ADJUST, cur->adjusted);\r\ngoto retry;\r\n}
