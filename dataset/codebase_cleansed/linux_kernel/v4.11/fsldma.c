static void set_sr(struct fsldma_chan *chan, u32 val)\r\n{\r\nDMA_OUT(chan, &chan->regs->sr, val, 32);\r\n}\r\nstatic u32 get_sr(struct fsldma_chan *chan)\r\n{\r\nreturn DMA_IN(chan, &chan->regs->sr, 32);\r\n}\r\nstatic void set_mr(struct fsldma_chan *chan, u32 val)\r\n{\r\nDMA_OUT(chan, &chan->regs->mr, val, 32);\r\n}\r\nstatic u32 get_mr(struct fsldma_chan *chan)\r\n{\r\nreturn DMA_IN(chan, &chan->regs->mr, 32);\r\n}\r\nstatic void set_cdar(struct fsldma_chan *chan, dma_addr_t addr)\r\n{\r\nDMA_OUT(chan, &chan->regs->cdar, addr | FSL_DMA_SNEN, 64);\r\n}\r\nstatic dma_addr_t get_cdar(struct fsldma_chan *chan)\r\n{\r\nreturn DMA_IN(chan, &chan->regs->cdar, 64) & ~FSL_DMA_SNEN;\r\n}\r\nstatic void set_bcr(struct fsldma_chan *chan, u32 val)\r\n{\r\nDMA_OUT(chan, &chan->regs->bcr, val, 32);\r\n}\r\nstatic u32 get_bcr(struct fsldma_chan *chan)\r\n{\r\nreturn DMA_IN(chan, &chan->regs->bcr, 32);\r\n}\r\nstatic void set_desc_cnt(struct fsldma_chan *chan,\r\nstruct fsl_dma_ld_hw *hw, u32 count)\r\n{\r\nhw->count = CPU_TO_DMA(chan, count, 32);\r\n}\r\nstatic void set_desc_src(struct fsldma_chan *chan,\r\nstruct fsl_dma_ld_hw *hw, dma_addr_t src)\r\n{\r\nu64 snoop_bits;\r\nsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX)\r\n? ((u64)FSL_DMA_SATR_SREADTYPE_SNOOP_READ << 32) : 0;\r\nhw->src_addr = CPU_TO_DMA(chan, snoop_bits | src, 64);\r\n}\r\nstatic void set_desc_dst(struct fsldma_chan *chan,\r\nstruct fsl_dma_ld_hw *hw, dma_addr_t dst)\r\n{\r\nu64 snoop_bits;\r\nsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX)\r\n? ((u64)FSL_DMA_DATR_DWRITETYPE_SNOOP_WRITE << 32) : 0;\r\nhw->dst_addr = CPU_TO_DMA(chan, snoop_bits | dst, 64);\r\n}\r\nstatic void set_desc_next(struct fsldma_chan *chan,\r\nstruct fsl_dma_ld_hw *hw, dma_addr_t next)\r\n{\r\nu64 snoop_bits;\r\nsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_83XX)\r\n? FSL_DMA_SNEN : 0;\r\nhw->next_ln_addr = CPU_TO_DMA(chan, snoop_bits | next, 64);\r\n}\r\nstatic void set_ld_eol(struct fsldma_chan *chan, struct fsl_desc_sw *desc)\r\n{\r\nu64 snoop_bits;\r\nsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_83XX)\r\n? FSL_DMA_SNEN : 0;\r\ndesc->hw.next_ln_addr = CPU_TO_DMA(chan,\r\nDMA_TO_CPU(chan, desc->hw.next_ln_addr, 64) | FSL_DMA_EOL\r\n| snoop_bits, 64);\r\n}\r\nstatic void dma_init(struct fsldma_chan *chan)\r\n{\r\nset_mr(chan, 0);\r\nswitch (chan->feature & FSL_DMA_IP_MASK) {\r\ncase FSL_DMA_IP_85XX:\r\nset_mr(chan, FSL_DMA_MR_BWC | FSL_DMA_MR_EIE\r\n| FSL_DMA_MR_EOLNIE);\r\nbreak;\r\ncase FSL_DMA_IP_83XX:\r\nset_mr(chan, FSL_DMA_MR_EOTIE | FSL_DMA_MR_PRC_RM);\r\nbreak;\r\n}\r\n}\r\nstatic int dma_is_idle(struct fsldma_chan *chan)\r\n{\r\nu32 sr = get_sr(chan);\r\nreturn (!(sr & FSL_DMA_SR_CB)) || (sr & FSL_DMA_SR_CH);\r\n}\r\nstatic void dma_start(struct fsldma_chan *chan)\r\n{\r\nu32 mode;\r\nmode = get_mr(chan);\r\nif (chan->feature & FSL_DMA_CHAN_PAUSE_EXT) {\r\nset_bcr(chan, 0);\r\nmode |= FSL_DMA_MR_EMP_EN;\r\n} else {\r\nmode &= ~FSL_DMA_MR_EMP_EN;\r\n}\r\nif (chan->feature & FSL_DMA_CHAN_START_EXT) {\r\nmode |= FSL_DMA_MR_EMS_EN;\r\n} else {\r\nmode &= ~FSL_DMA_MR_EMS_EN;\r\nmode |= FSL_DMA_MR_CS;\r\n}\r\nset_mr(chan, mode);\r\n}\r\nstatic void dma_halt(struct fsldma_chan *chan)\r\n{\r\nu32 mode;\r\nint i;\r\nmode = get_mr(chan);\r\nif ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX) {\r\nmode |= FSL_DMA_MR_CA;\r\nset_mr(chan, mode);\r\nmode &= ~FSL_DMA_MR_CA;\r\n}\r\nmode &= ~(FSL_DMA_MR_CS | FSL_DMA_MR_EMS_EN);\r\nset_mr(chan, mode);\r\nfor (i = 0; i < 100; i++) {\r\nif (dma_is_idle(chan))\r\nreturn;\r\nudelay(10);\r\n}\r\nif (!dma_is_idle(chan))\r\nchan_err(chan, "DMA halt timeout!\n");\r\n}\r\nstatic void fsl_chan_set_src_loop_size(struct fsldma_chan *chan, int size)\r\n{\r\nu32 mode;\r\nmode = get_mr(chan);\r\nswitch (size) {\r\ncase 0:\r\nmode &= ~FSL_DMA_MR_SAHE;\r\nbreak;\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nmode |= FSL_DMA_MR_SAHE | (__ilog2(size) << 14);\r\nbreak;\r\n}\r\nset_mr(chan, mode);\r\n}\r\nstatic void fsl_chan_set_dst_loop_size(struct fsldma_chan *chan, int size)\r\n{\r\nu32 mode;\r\nmode = get_mr(chan);\r\nswitch (size) {\r\ncase 0:\r\nmode &= ~FSL_DMA_MR_DAHE;\r\nbreak;\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nmode |= FSL_DMA_MR_DAHE | (__ilog2(size) << 16);\r\nbreak;\r\n}\r\nset_mr(chan, mode);\r\n}\r\nstatic void fsl_chan_set_request_count(struct fsldma_chan *chan, int size)\r\n{\r\nu32 mode;\r\nBUG_ON(size > 1024);\r\nmode = get_mr(chan);\r\nmode |= (__ilog2(size) << 24) & 0x0f000000;\r\nset_mr(chan, mode);\r\n}\r\nstatic void fsl_chan_toggle_ext_pause(struct fsldma_chan *chan, int enable)\r\n{\r\nif (enable)\r\nchan->feature |= FSL_DMA_CHAN_PAUSE_EXT;\r\nelse\r\nchan->feature &= ~FSL_DMA_CHAN_PAUSE_EXT;\r\n}\r\nstatic void fsl_chan_toggle_ext_start(struct fsldma_chan *chan, int enable)\r\n{\r\nif (enable)\r\nchan->feature |= FSL_DMA_CHAN_START_EXT;\r\nelse\r\nchan->feature &= ~FSL_DMA_CHAN_START_EXT;\r\n}\r\nint fsl_dma_external_start(struct dma_chan *dchan, int enable)\r\n{\r\nstruct fsldma_chan *chan;\r\nif (!dchan)\r\nreturn -EINVAL;\r\nchan = to_fsl_chan(dchan);\r\nfsl_chan_toggle_ext_start(chan, enable);\r\nreturn 0;\r\n}\r\nstatic void append_ld_queue(struct fsldma_chan *chan, struct fsl_desc_sw *desc)\r\n{\r\nstruct fsl_desc_sw *tail = to_fsl_desc(chan->ld_pending.prev);\r\nif (list_empty(&chan->ld_pending))\r\ngoto out_splice;\r\nset_desc_next(chan, &tail->hw, desc->async_tx.phys);\r\nout_splice:\r\nlist_splice_tail_init(&desc->tx_list, &chan->ld_pending);\r\n}\r\nstatic dma_cookie_t fsl_dma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct fsldma_chan *chan = to_fsl_chan(tx->chan);\r\nstruct fsl_desc_sw *desc = tx_to_fsl_desc(tx);\r\nstruct fsl_desc_sw *child;\r\ndma_cookie_t cookie = -EINVAL;\r\nspin_lock_bh(&chan->desc_lock);\r\n#ifdef CONFIG_PM\r\nif (unlikely(chan->pm_state != RUNNING)) {\r\nchan_dbg(chan, "cannot submit due to suspend\n");\r\nspin_unlock_bh(&chan->desc_lock);\r\nreturn -1;\r\n}\r\n#endif\r\nlist_for_each_entry(child, &desc->tx_list, node) {\r\ncookie = dma_cookie_assign(&child->async_tx);\r\n}\r\nappend_ld_queue(chan, desc);\r\nspin_unlock_bh(&chan->desc_lock);\r\nreturn cookie;\r\n}\r\nstatic void fsl_dma_free_descriptor(struct fsldma_chan *chan,\r\nstruct fsl_desc_sw *desc)\r\n{\r\nlist_del(&desc->node);\r\nchan_dbg(chan, "LD %p free\n", desc);\r\ndma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);\r\n}\r\nstatic struct fsl_desc_sw *fsl_dma_alloc_descriptor(struct fsldma_chan *chan)\r\n{\r\nstruct fsl_desc_sw *desc;\r\ndma_addr_t pdesc;\r\ndesc = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &pdesc);\r\nif (!desc) {\r\nchan_dbg(chan, "out of memory for link descriptor\n");\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\r\ndesc->async_tx.tx_submit = fsl_dma_tx_submit;\r\ndesc->async_tx.phys = pdesc;\r\nchan_dbg(chan, "LD %p allocated\n", desc);\r\nreturn desc;\r\n}\r\nstatic void fsldma_clean_completed_descriptor(struct fsldma_chan *chan)\r\n{\r\nstruct fsl_desc_sw *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, &chan->ld_completed, node)\r\nif (async_tx_test_ack(&desc->async_tx))\r\nfsl_dma_free_descriptor(chan, desc);\r\n}\r\nstatic dma_cookie_t fsldma_run_tx_complete_actions(struct fsldma_chan *chan,\r\nstruct fsl_desc_sw *desc, dma_cookie_t cookie)\r\n{\r\nstruct dma_async_tx_descriptor *txd = &desc->async_tx;\r\ndma_cookie_t ret = cookie;\r\nBUG_ON(txd->cookie < 0);\r\nif (txd->cookie > 0) {\r\nret = txd->cookie;\r\ndma_descriptor_unmap(txd);\r\ndmaengine_desc_get_callback_invoke(txd, NULL);\r\n}\r\ndma_run_dependencies(txd);\r\nreturn ret;\r\n}\r\nstatic void fsldma_clean_running_descriptor(struct fsldma_chan *chan,\r\nstruct fsl_desc_sw *desc)\r\n{\r\nlist_del(&desc->node);\r\nif (!async_tx_test_ack(&desc->async_tx)) {\r\nlist_add_tail(&desc->node, &chan->ld_completed);\r\nreturn;\r\n}\r\ndma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);\r\n}\r\nstatic void fsl_chan_xfer_ld_queue(struct fsldma_chan *chan)\r\n{\r\nstruct fsl_desc_sw *desc;\r\nif (list_empty(&chan->ld_pending)) {\r\nchan_dbg(chan, "no pending LDs\n");\r\nreturn;\r\n}\r\nif (!chan->idle) {\r\nchan_dbg(chan, "DMA controller still busy\n");\r\nreturn;\r\n}\r\nchan_dbg(chan, "idle, starting controller\n");\r\ndesc = list_first_entry(&chan->ld_pending, struct fsl_desc_sw, node);\r\nlist_splice_tail_init(&chan->ld_pending, &chan->ld_running);\r\nif ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX) {\r\nu32 mode;\r\nmode = get_mr(chan);\r\nmode &= ~FSL_DMA_MR_CS;\r\nset_mr(chan, mode);\r\n}\r\nset_cdar(chan, desc->async_tx.phys);\r\nget_cdar(chan);\r\ndma_start(chan);\r\nchan->idle = false;\r\n}\r\nstatic void fsldma_cleanup_descriptors(struct fsldma_chan *chan)\r\n{\r\nstruct fsl_desc_sw *desc, *_desc;\r\ndma_cookie_t cookie = 0;\r\ndma_addr_t curr_phys = get_cdar(chan);\r\nint seen_current = 0;\r\nfsldma_clean_completed_descriptor(chan);\r\nlist_for_each_entry_safe(desc, _desc, &chan->ld_running, node) {\r\nif (seen_current)\r\nbreak;\r\nif (desc->async_tx.phys == curr_phys) {\r\nseen_current = 1;\r\nif (!dma_is_idle(chan))\r\nbreak;\r\n}\r\ncookie = fsldma_run_tx_complete_actions(chan, desc, cookie);\r\nfsldma_clean_running_descriptor(chan, desc);\r\n}\r\nfsl_chan_xfer_ld_queue(chan);\r\nif (cookie > 0)\r\nchan->common.completed_cookie = cookie;\r\n}\r\nstatic int fsl_dma_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct fsldma_chan *chan = to_fsl_chan(dchan);\r\nif (chan->desc_pool)\r\nreturn 1;\r\nchan->desc_pool = dma_pool_create(chan->name, chan->dev,\r\nsizeof(struct fsl_desc_sw),\r\n__alignof__(struct fsl_desc_sw), 0);\r\nif (!chan->desc_pool) {\r\nchan_err(chan, "unable to allocate descriptor pool\n");\r\nreturn -ENOMEM;\r\n}\r\nreturn 1;\r\n}\r\nstatic void fsldma_free_desc_list(struct fsldma_chan *chan,\r\nstruct list_head *list)\r\n{\r\nstruct fsl_desc_sw *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, list, node)\r\nfsl_dma_free_descriptor(chan, desc);\r\n}\r\nstatic void fsldma_free_desc_list_reverse(struct fsldma_chan *chan,\r\nstruct list_head *list)\r\n{\r\nstruct fsl_desc_sw *desc, *_desc;\r\nlist_for_each_entry_safe_reverse(desc, _desc, list, node)\r\nfsl_dma_free_descriptor(chan, desc);\r\n}\r\nstatic void fsl_dma_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct fsldma_chan *chan = to_fsl_chan(dchan);\r\nchan_dbg(chan, "free all channel resources\n");\r\nspin_lock_bh(&chan->desc_lock);\r\nfsldma_cleanup_descriptors(chan);\r\nfsldma_free_desc_list(chan, &chan->ld_pending);\r\nfsldma_free_desc_list(chan, &chan->ld_running);\r\nfsldma_free_desc_list(chan, &chan->ld_completed);\r\nspin_unlock_bh(&chan->desc_lock);\r\ndma_pool_destroy(chan->desc_pool);\r\nchan->desc_pool = NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nfsl_dma_prep_memcpy(struct dma_chan *dchan,\r\ndma_addr_t dma_dst, dma_addr_t dma_src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct fsldma_chan *chan;\r\nstruct fsl_desc_sw *first = NULL, *prev = NULL, *new;\r\nsize_t copy;\r\nif (!dchan)\r\nreturn NULL;\r\nif (!len)\r\nreturn NULL;\r\nchan = to_fsl_chan(dchan);\r\ndo {\r\nnew = fsl_dma_alloc_descriptor(chan);\r\nif (!new) {\r\nchan_err(chan, "%s\n", msg_ld_oom);\r\ngoto fail;\r\n}\r\ncopy = min(len, (size_t)FSL_DMA_BCR_MAX_CNT);\r\nset_desc_cnt(chan, &new->hw, copy);\r\nset_desc_src(chan, &new->hw, dma_src);\r\nset_desc_dst(chan, &new->hw, dma_dst);\r\nif (!first)\r\nfirst = new;\r\nelse\r\nset_desc_next(chan, &prev->hw, new->async_tx.phys);\r\nnew->async_tx.cookie = 0;\r\nasync_tx_ack(&new->async_tx);\r\nprev = new;\r\nlen -= copy;\r\ndma_src += copy;\r\ndma_dst += copy;\r\nlist_add_tail(&new->node, &first->tx_list);\r\n} while (len);\r\nnew->async_tx.flags = flags;\r\nnew->async_tx.cookie = -EBUSY;\r\nset_ld_eol(chan, new);\r\nreturn &first->async_tx;\r\nfail:\r\nif (!first)\r\nreturn NULL;\r\nfsldma_free_desc_list_reverse(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *fsl_dma_prep_sg(struct dma_chan *dchan,\r\nstruct scatterlist *dst_sg, unsigned int dst_nents,\r\nstruct scatterlist *src_sg, unsigned int src_nents,\r\nunsigned long flags)\r\n{\r\nstruct fsl_desc_sw *first = NULL, *prev = NULL, *new = NULL;\r\nstruct fsldma_chan *chan = to_fsl_chan(dchan);\r\nsize_t dst_avail, src_avail;\r\ndma_addr_t dst, src;\r\nsize_t len;\r\nif (dst_nents == 0 || src_nents == 0)\r\nreturn NULL;\r\nif (dst_sg == NULL || src_sg == NULL)\r\nreturn NULL;\r\ndst_avail = sg_dma_len(dst_sg);\r\nsrc_avail = sg_dma_len(src_sg);\r\nwhile (true) {\r\nlen = min_t(size_t, src_avail, dst_avail);\r\nlen = min_t(size_t, len, FSL_DMA_BCR_MAX_CNT);\r\nif (len == 0)\r\ngoto fetch;\r\ndst = sg_dma_address(dst_sg) + sg_dma_len(dst_sg) - dst_avail;\r\nsrc = sg_dma_address(src_sg) + sg_dma_len(src_sg) - src_avail;\r\nnew = fsl_dma_alloc_descriptor(chan);\r\nif (!new) {\r\nchan_err(chan, "%s\n", msg_ld_oom);\r\ngoto fail;\r\n}\r\nset_desc_cnt(chan, &new->hw, len);\r\nset_desc_src(chan, &new->hw, src);\r\nset_desc_dst(chan, &new->hw, dst);\r\nif (!first)\r\nfirst = new;\r\nelse\r\nset_desc_next(chan, &prev->hw, new->async_tx.phys);\r\nnew->async_tx.cookie = 0;\r\nasync_tx_ack(&new->async_tx);\r\nprev = new;\r\nlist_add_tail(&new->node, &first->tx_list);\r\ndst_avail -= len;\r\nsrc_avail -= len;\r\nfetch:\r\nif (dst_avail == 0) {\r\nif (dst_nents == 0)\r\nbreak;\r\ndst_sg = sg_next(dst_sg);\r\nif (dst_sg == NULL)\r\nbreak;\r\ndst_nents--;\r\ndst_avail = sg_dma_len(dst_sg);\r\n}\r\nif (src_avail == 0) {\r\nif (src_nents == 0)\r\nbreak;\r\nsrc_sg = sg_next(src_sg);\r\nif (src_sg == NULL)\r\nbreak;\r\nsrc_nents--;\r\nsrc_avail = sg_dma_len(src_sg);\r\n}\r\n}\r\nnew->async_tx.flags = flags;\r\nnew->async_tx.cookie = -EBUSY;\r\nset_ld_eol(chan, new);\r\nreturn &first->async_tx;\r\nfail:\r\nif (!first)\r\nreturn NULL;\r\nfsldma_free_desc_list_reverse(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic int fsl_dma_device_terminate_all(struct dma_chan *dchan)\r\n{\r\nstruct fsldma_chan *chan;\r\nif (!dchan)\r\nreturn -EINVAL;\r\nchan = to_fsl_chan(dchan);\r\nspin_lock_bh(&chan->desc_lock);\r\ndma_halt(chan);\r\nfsldma_free_desc_list(chan, &chan->ld_pending);\r\nfsldma_free_desc_list(chan, &chan->ld_running);\r\nfsldma_free_desc_list(chan, &chan->ld_completed);\r\nchan->idle = true;\r\nspin_unlock_bh(&chan->desc_lock);\r\nreturn 0;\r\n}\r\nstatic int fsl_dma_device_config(struct dma_chan *dchan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct fsldma_chan *chan;\r\nint size;\r\nif (!dchan)\r\nreturn -EINVAL;\r\nchan = to_fsl_chan(dchan);\r\nif (!chan->set_request_count)\r\nreturn -ENXIO;\r\nif (config->direction == DMA_MEM_TO_DEV)\r\nsize = config->dst_addr_width * config->dst_maxburst;\r\nelse\r\nsize = config->src_addr_width * config->src_maxburst;\r\nchan->set_request_count(chan, size);\r\nreturn 0;\r\n}\r\nstatic void fsl_dma_memcpy_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct fsldma_chan *chan = to_fsl_chan(dchan);\r\nspin_lock_bh(&chan->desc_lock);\r\nfsl_chan_xfer_ld_queue(chan);\r\nspin_unlock_bh(&chan->desc_lock);\r\n}\r\nstatic enum dma_status fsl_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct fsldma_chan *chan = to_fsl_chan(dchan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(dchan, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nspin_lock_bh(&chan->desc_lock);\r\nfsldma_cleanup_descriptors(chan);\r\nspin_unlock_bh(&chan->desc_lock);\r\nreturn dma_cookie_status(dchan, cookie, txstate);\r\n}\r\nstatic irqreturn_t fsldma_chan_irq(int irq, void *data)\r\n{\r\nstruct fsldma_chan *chan = data;\r\nu32 stat;\r\nstat = get_sr(chan);\r\nset_sr(chan, stat);\r\nchan_dbg(chan, "irq: stat = 0x%x\n", stat);\r\nstat &= ~(FSL_DMA_SR_CB | FSL_DMA_SR_CH);\r\nif (!stat)\r\nreturn IRQ_NONE;\r\nif (stat & FSL_DMA_SR_TE)\r\nchan_err(chan, "Transfer Error!\n");\r\nif (stat & FSL_DMA_SR_PE) {\r\nchan_dbg(chan, "irq: Programming Error INT\n");\r\nstat &= ~FSL_DMA_SR_PE;\r\nif (get_bcr(chan) != 0)\r\nchan_err(chan, "Programming Error!\n");\r\n}\r\nif (stat & FSL_DMA_SR_EOCDI) {\r\nchan_dbg(chan, "irq: End-of-Chain link INT\n");\r\nstat &= ~FSL_DMA_SR_EOCDI;\r\n}\r\nif (stat & FSL_DMA_SR_EOLNI) {\r\nchan_dbg(chan, "irq: End-of-link INT\n");\r\nstat &= ~FSL_DMA_SR_EOLNI;\r\n}\r\nif (!dma_is_idle(chan))\r\nchan_err(chan, "irq: controller not idle!\n");\r\nif (stat)\r\nchan_err(chan, "irq: unhandled sr 0x%08x\n", stat);\r\ntasklet_schedule(&chan->tasklet);\r\nchan_dbg(chan, "irq: Exit\n");\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void dma_do_tasklet(unsigned long data)\r\n{\r\nstruct fsldma_chan *chan = (struct fsldma_chan *)data;\r\nchan_dbg(chan, "tasklet entry\n");\r\nspin_lock_bh(&chan->desc_lock);\r\nchan->idle = true;\r\nfsldma_cleanup_descriptors(chan);\r\nspin_unlock_bh(&chan->desc_lock);\r\nchan_dbg(chan, "tasklet exit\n");\r\n}\r\nstatic irqreturn_t fsldma_ctrl_irq(int irq, void *data)\r\n{\r\nstruct fsldma_device *fdev = data;\r\nstruct fsldma_chan *chan;\r\nunsigned int handled = 0;\r\nu32 gsr, mask;\r\nint i;\r\ngsr = (fdev->feature & FSL_DMA_BIG_ENDIAN) ? in_be32(fdev->regs)\r\n: in_le32(fdev->regs);\r\nmask = 0xff000000;\r\ndev_dbg(fdev->dev, "IRQ: gsr 0x%.8x\n", gsr);\r\nfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\r\nchan = fdev->chan[i];\r\nif (!chan)\r\ncontinue;\r\nif (gsr & mask) {\r\ndev_dbg(fdev->dev, "IRQ: chan %d\n", chan->id);\r\nfsldma_chan_irq(irq, chan);\r\nhandled++;\r\n}\r\ngsr &= ~mask;\r\nmask >>= 8;\r\n}\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic void fsldma_free_irqs(struct fsldma_device *fdev)\r\n{\r\nstruct fsldma_chan *chan;\r\nint i;\r\nif (fdev->irq) {\r\ndev_dbg(fdev->dev, "free per-controller IRQ\n");\r\nfree_irq(fdev->irq, fdev);\r\nreturn;\r\n}\r\nfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\r\nchan = fdev->chan[i];\r\nif (chan && chan->irq) {\r\nchan_dbg(chan, "free per-channel IRQ\n");\r\nfree_irq(chan->irq, chan);\r\n}\r\n}\r\n}\r\nstatic int fsldma_request_irqs(struct fsldma_device *fdev)\r\n{\r\nstruct fsldma_chan *chan;\r\nint ret;\r\nint i;\r\nif (fdev->irq) {\r\ndev_dbg(fdev->dev, "request per-controller IRQ\n");\r\nret = request_irq(fdev->irq, fsldma_ctrl_irq, IRQF_SHARED,\r\n"fsldma-controller", fdev);\r\nreturn ret;\r\n}\r\nfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\r\nchan = fdev->chan[i];\r\nif (!chan)\r\ncontinue;\r\nif (!chan->irq) {\r\nchan_err(chan, "interrupts property missing in device tree\n");\r\nret = -ENODEV;\r\ngoto out_unwind;\r\n}\r\nchan_dbg(chan, "request per-channel IRQ\n");\r\nret = request_irq(chan->irq, fsldma_chan_irq, IRQF_SHARED,\r\n"fsldma-chan", chan);\r\nif (ret) {\r\nchan_err(chan, "unable to request per-channel IRQ\n");\r\ngoto out_unwind;\r\n}\r\n}\r\nreturn 0;\r\nout_unwind:\r\nfor (; i >= 0; i--) {\r\nchan = fdev->chan[i];\r\nif (!chan)\r\ncontinue;\r\nif (!chan->irq)\r\ncontinue;\r\nfree_irq(chan->irq, chan);\r\n}\r\nreturn ret;\r\n}\r\nstatic int fsl_dma_chan_probe(struct fsldma_device *fdev,\r\nstruct device_node *node, u32 feature, const char *compatible)\r\n{\r\nstruct fsldma_chan *chan;\r\nstruct resource res;\r\nint err;\r\nchan = kzalloc(sizeof(*chan), GFP_KERNEL);\r\nif (!chan) {\r\nerr = -ENOMEM;\r\ngoto out_return;\r\n}\r\nchan->regs = of_iomap(node, 0);\r\nif (!chan->regs) {\r\ndev_err(fdev->dev, "unable to ioremap registers\n");\r\nerr = -ENOMEM;\r\ngoto out_free_chan;\r\n}\r\nerr = of_address_to_resource(node, 0, &res);\r\nif (err) {\r\ndev_err(fdev->dev, "unable to find 'reg' property\n");\r\ngoto out_iounmap_regs;\r\n}\r\nchan->feature = feature;\r\nif (!fdev->feature)\r\nfdev->feature = chan->feature;\r\nWARN_ON(fdev->feature != chan->feature);\r\nchan->dev = fdev->dev;\r\nchan->id = (res.start & 0xfff) < 0x300 ?\r\n((res.start - 0x100) & 0xfff) >> 7 :\r\n((res.start - 0x200) & 0xfff) >> 7;\r\nif (chan->id >= FSL_DMA_MAX_CHANS_PER_DEVICE) {\r\ndev_err(fdev->dev, "too many channels for device\n");\r\nerr = -EINVAL;\r\ngoto out_iounmap_regs;\r\n}\r\nfdev->chan[chan->id] = chan;\r\ntasklet_init(&chan->tasklet, dma_do_tasklet, (unsigned long)chan);\r\nsnprintf(chan->name, sizeof(chan->name), "chan%d", chan->id);\r\ndma_init(chan);\r\nset_cdar(chan, 0);\r\nswitch (chan->feature & FSL_DMA_IP_MASK) {\r\ncase FSL_DMA_IP_85XX:\r\nchan->toggle_ext_pause = fsl_chan_toggle_ext_pause;\r\ncase FSL_DMA_IP_83XX:\r\nchan->toggle_ext_start = fsl_chan_toggle_ext_start;\r\nchan->set_src_loop_size = fsl_chan_set_src_loop_size;\r\nchan->set_dst_loop_size = fsl_chan_set_dst_loop_size;\r\nchan->set_request_count = fsl_chan_set_request_count;\r\n}\r\nspin_lock_init(&chan->desc_lock);\r\nINIT_LIST_HEAD(&chan->ld_pending);\r\nINIT_LIST_HEAD(&chan->ld_running);\r\nINIT_LIST_HEAD(&chan->ld_completed);\r\nchan->idle = true;\r\n#ifdef CONFIG_PM\r\nchan->pm_state = RUNNING;\r\n#endif\r\nchan->common.device = &fdev->common;\r\ndma_cookie_init(&chan->common);\r\nchan->irq = irq_of_parse_and_map(node, 0);\r\nlist_add_tail(&chan->common.device_node, &fdev->common.channels);\r\ndev_info(fdev->dev, "#%d (%s), irq %d\n", chan->id, compatible,\r\nchan->irq ? chan->irq : fdev->irq);\r\nreturn 0;\r\nout_iounmap_regs:\r\niounmap(chan->regs);\r\nout_free_chan:\r\nkfree(chan);\r\nout_return:\r\nreturn err;\r\n}\r\nstatic void fsl_dma_chan_remove(struct fsldma_chan *chan)\r\n{\r\nirq_dispose_mapping(chan->irq);\r\nlist_del(&chan->common.device_node);\r\niounmap(chan->regs);\r\nkfree(chan);\r\n}\r\nstatic int fsldma_of_probe(struct platform_device *op)\r\n{\r\nstruct fsldma_device *fdev;\r\nstruct device_node *child;\r\nint err;\r\nfdev = kzalloc(sizeof(*fdev), GFP_KERNEL);\r\nif (!fdev) {\r\nerr = -ENOMEM;\r\ngoto out_return;\r\n}\r\nfdev->dev = &op->dev;\r\nINIT_LIST_HEAD(&fdev->common.channels);\r\nfdev->regs = of_iomap(op->dev.of_node, 0);\r\nif (!fdev->regs) {\r\ndev_err(&op->dev, "unable to ioremap registers\n");\r\nerr = -ENOMEM;\r\ngoto out_free;\r\n}\r\nfdev->irq = irq_of_parse_and_map(op->dev.of_node, 0);\r\ndma_cap_set(DMA_MEMCPY, fdev->common.cap_mask);\r\ndma_cap_set(DMA_SG, fdev->common.cap_mask);\r\ndma_cap_set(DMA_SLAVE, fdev->common.cap_mask);\r\nfdev->common.device_alloc_chan_resources = fsl_dma_alloc_chan_resources;\r\nfdev->common.device_free_chan_resources = fsl_dma_free_chan_resources;\r\nfdev->common.device_prep_dma_memcpy = fsl_dma_prep_memcpy;\r\nfdev->common.device_prep_dma_sg = fsl_dma_prep_sg;\r\nfdev->common.device_tx_status = fsl_tx_status;\r\nfdev->common.device_issue_pending = fsl_dma_memcpy_issue_pending;\r\nfdev->common.device_config = fsl_dma_device_config;\r\nfdev->common.device_terminate_all = fsl_dma_device_terminate_all;\r\nfdev->common.dev = &op->dev;\r\nfdev->common.src_addr_widths = FSL_DMA_BUSWIDTHS;\r\nfdev->common.dst_addr_widths = FSL_DMA_BUSWIDTHS;\r\nfdev->common.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nfdev->common.residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\r\ndma_set_mask(&(op->dev), DMA_BIT_MASK(36));\r\nplatform_set_drvdata(op, fdev);\r\nfor_each_child_of_node(op->dev.of_node, child) {\r\nif (of_device_is_compatible(child, "fsl,eloplus-dma-channel")) {\r\nfsl_dma_chan_probe(fdev, child,\r\nFSL_DMA_IP_85XX | FSL_DMA_BIG_ENDIAN,\r\n"fsl,eloplus-dma-channel");\r\n}\r\nif (of_device_is_compatible(child, "fsl,elo-dma-channel")) {\r\nfsl_dma_chan_probe(fdev, child,\r\nFSL_DMA_IP_83XX | FSL_DMA_LITTLE_ENDIAN,\r\n"fsl,elo-dma-channel");\r\n}\r\n}\r\nerr = fsldma_request_irqs(fdev);\r\nif (err) {\r\ndev_err(fdev->dev, "unable to request IRQs\n");\r\ngoto out_free_fdev;\r\n}\r\ndma_async_device_register(&fdev->common);\r\nreturn 0;\r\nout_free_fdev:\r\nirq_dispose_mapping(fdev->irq);\r\niounmap(fdev->regs);\r\nout_free:\r\nkfree(fdev);\r\nout_return:\r\nreturn err;\r\n}\r\nstatic int fsldma_of_remove(struct platform_device *op)\r\n{\r\nstruct fsldma_device *fdev;\r\nunsigned int i;\r\nfdev = platform_get_drvdata(op);\r\ndma_async_device_unregister(&fdev->common);\r\nfsldma_free_irqs(fdev);\r\nfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\r\nif (fdev->chan[i])\r\nfsl_dma_chan_remove(fdev->chan[i]);\r\n}\r\niounmap(fdev->regs);\r\nkfree(fdev);\r\nreturn 0;\r\n}\r\nstatic int fsldma_suspend_late(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct fsldma_device *fdev = platform_get_drvdata(pdev);\r\nstruct fsldma_chan *chan;\r\nint i;\r\nfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\r\nchan = fdev->chan[i];\r\nif (!chan)\r\ncontinue;\r\nspin_lock_bh(&chan->desc_lock);\r\nif (unlikely(!chan->idle))\r\ngoto out;\r\nchan->regs_save.mr = get_mr(chan);\r\nchan->pm_state = SUSPENDED;\r\nspin_unlock_bh(&chan->desc_lock);\r\n}\r\nreturn 0;\r\nout:\r\nfor (; i >= 0; i--) {\r\nchan = fdev->chan[i];\r\nif (!chan)\r\ncontinue;\r\nchan->pm_state = RUNNING;\r\nspin_unlock_bh(&chan->desc_lock);\r\n}\r\nreturn -EBUSY;\r\n}\r\nstatic int fsldma_resume_early(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct fsldma_device *fdev = platform_get_drvdata(pdev);\r\nstruct fsldma_chan *chan;\r\nu32 mode;\r\nint i;\r\nfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\r\nchan = fdev->chan[i];\r\nif (!chan)\r\ncontinue;\r\nspin_lock_bh(&chan->desc_lock);\r\nmode = chan->regs_save.mr\r\n& ~FSL_DMA_MR_CS & ~FSL_DMA_MR_CC & ~FSL_DMA_MR_CA;\r\nset_mr(chan, mode);\r\nchan->pm_state = RUNNING;\r\nspin_unlock_bh(&chan->desc_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic __init int fsldma_init(void)\r\n{\r\npr_info("Freescale Elo series DMA driver\n");\r\nreturn platform_driver_register(&fsldma_of_driver);\r\n}\r\nstatic void __exit fsldma_exit(void)\r\n{\r\nplatform_driver_unregister(&fsldma_of_driver);\r\n}
