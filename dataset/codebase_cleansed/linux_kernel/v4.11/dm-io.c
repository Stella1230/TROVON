struct dm_io_client *dm_io_client_create(void)\r\n{\r\nstruct dm_io_client *client;\r\nunsigned min_ios = dm_get_reserved_bio_based_ios();\r\nclient = kmalloc(sizeof(*client), GFP_KERNEL);\r\nif (!client)\r\nreturn ERR_PTR(-ENOMEM);\r\nclient->pool = mempool_create_slab_pool(min_ios, _dm_io_cache);\r\nif (!client->pool)\r\ngoto bad;\r\nclient->bios = bioset_create(min_ios, 0);\r\nif (!client->bios)\r\ngoto bad;\r\nreturn client;\r\nbad:\r\nmempool_destroy(client->pool);\r\nkfree(client);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvoid dm_io_client_destroy(struct dm_io_client *client)\r\n{\r\nmempool_destroy(client->pool);\r\nbioset_free(client->bios);\r\nkfree(client);\r\n}\r\nstatic void store_io_and_region_in_bio(struct bio *bio, struct io *io,\r\nunsigned region)\r\n{\r\nif (unlikely(!IS_ALIGNED((unsigned long)io, DM_IO_MAX_REGIONS))) {\r\nDMCRIT("Unaligned struct io pointer %p", io);\r\nBUG();\r\n}\r\nbio->bi_private = (void *)((unsigned long)io | region);\r\n}\r\nstatic void retrieve_io_and_region_from_bio(struct bio *bio, struct io **io,\r\nunsigned *region)\r\n{\r\nunsigned long val = (unsigned long)bio->bi_private;\r\n*io = (void *)(val & -(unsigned long)DM_IO_MAX_REGIONS);\r\n*region = val & (DM_IO_MAX_REGIONS - 1);\r\n}\r\nstatic void complete_io(struct io *io)\r\n{\r\nunsigned long error_bits = io->error_bits;\r\nio_notify_fn fn = io->callback;\r\nvoid *context = io->context;\r\nif (io->vma_invalidate_size)\r\ninvalidate_kernel_vmap_range(io->vma_invalidate_address,\r\nio->vma_invalidate_size);\r\nmempool_free(io, io->client->pool);\r\nfn(error_bits, context);\r\n}\r\nstatic void dec_count(struct io *io, unsigned int region, int error)\r\n{\r\nif (error)\r\nset_bit(region, &io->error_bits);\r\nif (atomic_dec_and_test(&io->count))\r\ncomplete_io(io);\r\n}\r\nstatic void endio(struct bio *bio)\r\n{\r\nstruct io *io;\r\nunsigned region;\r\nint error;\r\nif (bio->bi_error && bio_data_dir(bio) == READ)\r\nzero_fill_bio(bio);\r\nretrieve_io_and_region_from_bio(bio, &io, &region);\r\nerror = bio->bi_error;\r\nbio_put(bio);\r\ndec_count(io, region, error);\r\n}\r\nstatic void list_get_page(struct dpages *dp,\r\nstruct page **p, unsigned long *len, unsigned *offset)\r\n{\r\nunsigned o = dp->context_u;\r\nstruct page_list *pl = (struct page_list *) dp->context_ptr;\r\n*p = pl->page;\r\n*len = PAGE_SIZE - o;\r\n*offset = o;\r\n}\r\nstatic void list_next_page(struct dpages *dp)\r\n{\r\nstruct page_list *pl = (struct page_list *) dp->context_ptr;\r\ndp->context_ptr = pl->next;\r\ndp->context_u = 0;\r\n}\r\nstatic void list_dp_init(struct dpages *dp, struct page_list *pl, unsigned offset)\r\n{\r\ndp->get_page = list_get_page;\r\ndp->next_page = list_next_page;\r\ndp->context_u = offset;\r\ndp->context_ptr = pl;\r\n}\r\nstatic void bio_get_page(struct dpages *dp, struct page **p,\r\nunsigned long *len, unsigned *offset)\r\n{\r\nstruct bio_vec bvec = bvec_iter_bvec((struct bio_vec *)dp->context_ptr,\r\ndp->context_bi);\r\n*p = bvec.bv_page;\r\n*len = bvec.bv_len;\r\n*offset = bvec.bv_offset;\r\ndp->context_bi.bi_sector = (sector_t)bvec.bv_len;\r\n}\r\nstatic void bio_next_page(struct dpages *dp)\r\n{\r\nunsigned int len = (unsigned int)dp->context_bi.bi_sector;\r\nbvec_iter_advance((struct bio_vec *)dp->context_ptr,\r\n&dp->context_bi, len);\r\n}\r\nstatic void bio_dp_init(struct dpages *dp, struct bio *bio)\r\n{\r\ndp->get_page = bio_get_page;\r\ndp->next_page = bio_next_page;\r\ndp->context_ptr = bio->bi_io_vec;\r\ndp->context_bi = bio->bi_iter;\r\n}\r\nstatic void vm_get_page(struct dpages *dp,\r\nstruct page **p, unsigned long *len, unsigned *offset)\r\n{\r\n*p = vmalloc_to_page(dp->context_ptr);\r\n*offset = dp->context_u;\r\n*len = PAGE_SIZE - dp->context_u;\r\n}\r\nstatic void vm_next_page(struct dpages *dp)\r\n{\r\ndp->context_ptr += PAGE_SIZE - dp->context_u;\r\ndp->context_u = 0;\r\n}\r\nstatic void vm_dp_init(struct dpages *dp, void *data)\r\n{\r\ndp->get_page = vm_get_page;\r\ndp->next_page = vm_next_page;\r\ndp->context_u = offset_in_page(data);\r\ndp->context_ptr = data;\r\n}\r\nstatic void km_get_page(struct dpages *dp, struct page **p, unsigned long *len,\r\nunsigned *offset)\r\n{\r\n*p = virt_to_page(dp->context_ptr);\r\n*offset = dp->context_u;\r\n*len = PAGE_SIZE - dp->context_u;\r\n}\r\nstatic void km_next_page(struct dpages *dp)\r\n{\r\ndp->context_ptr += PAGE_SIZE - dp->context_u;\r\ndp->context_u = 0;\r\n}\r\nstatic void km_dp_init(struct dpages *dp, void *data)\r\n{\r\ndp->get_page = km_get_page;\r\ndp->next_page = km_next_page;\r\ndp->context_u = offset_in_page(data);\r\ndp->context_ptr = data;\r\n}\r\nstatic void do_region(int op, int op_flags, unsigned region,\r\nstruct dm_io_region *where, struct dpages *dp,\r\nstruct io *io)\r\n{\r\nstruct bio *bio;\r\nstruct page *page;\r\nunsigned long len;\r\nunsigned offset;\r\nunsigned num_bvecs;\r\nsector_t remaining = where->count;\r\nstruct request_queue *q = bdev_get_queue(where->bdev);\r\nunsigned short logical_block_size = queue_logical_block_size(q);\r\nsector_t num_sectors;\r\nunsigned int uninitialized_var(special_cmd_max_sectors);\r\nif (op == REQ_OP_DISCARD)\r\nspecial_cmd_max_sectors = q->limits.max_discard_sectors;\r\nelse if (op == REQ_OP_WRITE_SAME)\r\nspecial_cmd_max_sectors = q->limits.max_write_same_sectors;\r\nif ((op == REQ_OP_DISCARD || op == REQ_OP_WRITE_SAME) &&\r\nspecial_cmd_max_sectors == 0) {\r\ndec_count(io, region, -EOPNOTSUPP);\r\nreturn;\r\n}\r\ndo {\r\nif ((op == REQ_OP_DISCARD) || (op == REQ_OP_WRITE_SAME))\r\nnum_bvecs = 1;\r\nelse\r\nnum_bvecs = min_t(int, BIO_MAX_PAGES,\r\ndm_sector_div_up(remaining, (PAGE_SIZE >> SECTOR_SHIFT)));\r\nbio = bio_alloc_bioset(GFP_NOIO, num_bvecs, io->client->bios);\r\nbio->bi_iter.bi_sector = where->sector + (where->count - remaining);\r\nbio->bi_bdev = where->bdev;\r\nbio->bi_end_io = endio;\r\nbio_set_op_attrs(bio, op, op_flags);\r\nstore_io_and_region_in_bio(bio, io, region);\r\nif (op == REQ_OP_DISCARD) {\r\nnum_sectors = min_t(sector_t, special_cmd_max_sectors, remaining);\r\nbio->bi_iter.bi_size = num_sectors << SECTOR_SHIFT;\r\nremaining -= num_sectors;\r\n} else if (op == REQ_OP_WRITE_SAME) {\r\ndp->get_page(dp, &page, &len, &offset);\r\nbio_add_page(bio, page, logical_block_size, offset);\r\nnum_sectors = min_t(sector_t, special_cmd_max_sectors, remaining);\r\nbio->bi_iter.bi_size = num_sectors << SECTOR_SHIFT;\r\noffset = 0;\r\nremaining -= num_sectors;\r\ndp->next_page(dp);\r\n} else while (remaining) {\r\ndp->get_page(dp, &page, &len, &offset);\r\nlen = min(len, to_bytes(remaining));\r\nif (!bio_add_page(bio, page, len, offset))\r\nbreak;\r\noffset = 0;\r\nremaining -= to_sector(len);\r\ndp->next_page(dp);\r\n}\r\natomic_inc(&io->count);\r\nsubmit_bio(bio);\r\n} while (remaining);\r\n}\r\nstatic void dispatch_io(int op, int op_flags, unsigned int num_regions,\r\nstruct dm_io_region *where, struct dpages *dp,\r\nstruct io *io, int sync)\r\n{\r\nint i;\r\nstruct dpages old_pages = *dp;\r\nBUG_ON(num_regions > DM_IO_MAX_REGIONS);\r\nif (sync)\r\nop_flags |= REQ_SYNC;\r\nfor (i = 0; i < num_regions; i++) {\r\n*dp = old_pages;\r\nif (where[i].count || (op_flags & REQ_PREFLUSH))\r\ndo_region(op, op_flags, i, where + i, dp, io);\r\n}\r\ndec_count(io, 0, 0);\r\n}\r\nstatic void sync_io_complete(unsigned long error, void *context)\r\n{\r\nstruct sync_io *sio = context;\r\nsio->error_bits = error;\r\ncomplete(&sio->wait);\r\n}\r\nstatic int sync_io(struct dm_io_client *client, unsigned int num_regions,\r\nstruct dm_io_region *where, int op, int op_flags,\r\nstruct dpages *dp, unsigned long *error_bits)\r\n{\r\nstruct io *io;\r\nstruct sync_io sio;\r\nif (num_regions > 1 && !op_is_write(op)) {\r\nWARN_ON(1);\r\nreturn -EIO;\r\n}\r\ninit_completion(&sio.wait);\r\nio = mempool_alloc(client->pool, GFP_NOIO);\r\nio->error_bits = 0;\r\natomic_set(&io->count, 1);\r\nio->client = client;\r\nio->callback = sync_io_complete;\r\nio->context = &sio;\r\nio->vma_invalidate_address = dp->vma_invalidate_address;\r\nio->vma_invalidate_size = dp->vma_invalidate_size;\r\ndispatch_io(op, op_flags, num_regions, where, dp, io, 1);\r\nwait_for_completion_io(&sio.wait);\r\nif (error_bits)\r\n*error_bits = sio.error_bits;\r\nreturn sio.error_bits ? -EIO : 0;\r\n}\r\nstatic int async_io(struct dm_io_client *client, unsigned int num_regions,\r\nstruct dm_io_region *where, int op, int op_flags,\r\nstruct dpages *dp, io_notify_fn fn, void *context)\r\n{\r\nstruct io *io;\r\nif (num_regions > 1 && !op_is_write(op)) {\r\nWARN_ON(1);\r\nfn(1, context);\r\nreturn -EIO;\r\n}\r\nio = mempool_alloc(client->pool, GFP_NOIO);\r\nio->error_bits = 0;\r\natomic_set(&io->count, 1);\r\nio->client = client;\r\nio->callback = fn;\r\nio->context = context;\r\nio->vma_invalidate_address = dp->vma_invalidate_address;\r\nio->vma_invalidate_size = dp->vma_invalidate_size;\r\ndispatch_io(op, op_flags, num_regions, where, dp, io, 0);\r\nreturn 0;\r\n}\r\nstatic int dp_init(struct dm_io_request *io_req, struct dpages *dp,\r\nunsigned long size)\r\n{\r\ndp->vma_invalidate_address = NULL;\r\ndp->vma_invalidate_size = 0;\r\nswitch (io_req->mem.type) {\r\ncase DM_IO_PAGE_LIST:\r\nlist_dp_init(dp, io_req->mem.ptr.pl, io_req->mem.offset);\r\nbreak;\r\ncase DM_IO_BIO:\r\nbio_dp_init(dp, io_req->mem.ptr.bio);\r\nbreak;\r\ncase DM_IO_VMA:\r\nflush_kernel_vmap_range(io_req->mem.ptr.vma, size);\r\nif (io_req->bi_op == REQ_OP_READ) {\r\ndp->vma_invalidate_address = io_req->mem.ptr.vma;\r\ndp->vma_invalidate_size = size;\r\n}\r\nvm_dp_init(dp, io_req->mem.ptr.vma);\r\nbreak;\r\ncase DM_IO_KMEM:\r\nkm_dp_init(dp, io_req->mem.ptr.addr);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint dm_io(struct dm_io_request *io_req, unsigned num_regions,\r\nstruct dm_io_region *where, unsigned long *sync_error_bits)\r\n{\r\nint r;\r\nstruct dpages dp;\r\nr = dp_init(io_req, &dp, (unsigned long)where->count << SECTOR_SHIFT);\r\nif (r)\r\nreturn r;\r\nif (!io_req->notify.fn)\r\nreturn sync_io(io_req->client, num_regions, where,\r\nio_req->bi_op, io_req->bi_op_flags, &dp,\r\nsync_error_bits);\r\nreturn async_io(io_req->client, num_regions, where, io_req->bi_op,\r\nio_req->bi_op_flags, &dp, io_req->notify.fn,\r\nio_req->notify.context);\r\n}\r\nint __init dm_io_init(void)\r\n{\r\n_dm_io_cache = KMEM_CACHE(io, 0);\r\nif (!_dm_io_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid dm_io_exit(void)\r\n{\r\nkmem_cache_destroy(_dm_io_cache);\r\n_dm_io_cache = NULL;\r\n}
