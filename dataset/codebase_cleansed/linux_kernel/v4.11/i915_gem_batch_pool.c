void i915_gem_batch_pool_init(struct intel_engine_cs *engine,\r\nstruct i915_gem_batch_pool *pool)\r\n{\r\nint n;\r\npool->engine = engine;\r\nfor (n = 0; n < ARRAY_SIZE(pool->cache_list); n++)\r\nINIT_LIST_HEAD(&pool->cache_list[n]);\r\n}\r\nvoid i915_gem_batch_pool_fini(struct i915_gem_batch_pool *pool)\r\n{\r\nint n;\r\nlockdep_assert_held(&pool->engine->i915->drm.struct_mutex);\r\nfor (n = 0; n < ARRAY_SIZE(pool->cache_list); n++) {\r\nstruct drm_i915_gem_object *obj, *next;\r\nlist_for_each_entry_safe(obj, next,\r\n&pool->cache_list[n],\r\nbatch_pool_link)\r\n__i915_gem_object_release_unless_active(obj);\r\nINIT_LIST_HEAD(&pool->cache_list[n]);\r\n}\r\n}\r\nstruct drm_i915_gem_object *\r\ni915_gem_batch_pool_get(struct i915_gem_batch_pool *pool,\r\nsize_t size)\r\n{\r\nstruct drm_i915_gem_object *obj = NULL;\r\nstruct drm_i915_gem_object *tmp;\r\nstruct list_head *list;\r\nint n, ret;\r\nlockdep_assert_held(&pool->engine->i915->drm.struct_mutex);\r\nn = fls(size >> PAGE_SHIFT) - 1;\r\nif (n >= ARRAY_SIZE(pool->cache_list))\r\nn = ARRAY_SIZE(pool->cache_list) - 1;\r\nlist = &pool->cache_list[n];\r\nlist_for_each_entry(tmp, list, batch_pool_link) {\r\nif (i915_gem_object_is_active(tmp))\r\nbreak;\r\nGEM_BUG_ON(!reservation_object_test_signaled_rcu(tmp->resv,\r\ntrue));\r\nif (tmp->base.size >= size) {\r\nww_mutex_lock(&tmp->resv->lock, NULL);\r\nreservation_object_add_excl_fence(tmp->resv, NULL);\r\nww_mutex_unlock(&tmp->resv->lock);\r\nobj = tmp;\r\nbreak;\r\n}\r\n}\r\nif (obj == NULL) {\r\nobj = i915_gem_object_create_internal(pool->engine->i915, size);\r\nif (IS_ERR(obj))\r\nreturn obj;\r\n}\r\nret = i915_gem_object_pin_pages(obj);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nlist_move_tail(&obj->batch_pool_link, list);\r\nreturn obj;\r\n}
