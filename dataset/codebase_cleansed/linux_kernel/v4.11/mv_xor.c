static void mv_desc_init(struct mv_xor_desc_slot *desc,\r\ndma_addr_t addr, u32 byte_count,\r\nenum dma_ctrl_flags flags)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->status = XOR_DESC_DMA_OWNED;\r\nhw_desc->phy_next_desc = 0;\r\nhw_desc->desc_command = (flags & DMA_PREP_INTERRUPT) ?\r\nXOR_DESC_EOD_INT_EN : 0;\r\nhw_desc->phy_dest_addr = addr;\r\nhw_desc->byte_count = byte_count;\r\n}\r\nstatic void mv_xor_config_sg_ll_desc(struct mv_xor_desc_slot *desc,\r\ndma_addr_t dma_src, dma_addr_t dma_dst,\r\nu32 len, struct mv_xor_desc_slot *prev)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->status = XOR_DESC_DMA_OWNED;\r\nhw_desc->phy_next_desc = 0;\r\nhw_desc->desc_command = XOR_DESC_OPERATION_XOR | (0x1 << 0);\r\nhw_desc->phy_dest_addr = dma_dst;\r\nhw_desc->phy_src_addr[0] = dma_src;\r\nhw_desc->byte_count = len;\r\nif (prev) {\r\nstruct mv_xor_desc *hw_prev = prev->hw_desc;\r\nhw_prev->phy_next_desc = desc->async_tx.phys;\r\n}\r\n}\r\nstatic void mv_xor_desc_config_eod(struct mv_xor_desc_slot *desc)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->desc_command |= XOR_DESC_EOD_INT_EN;\r\n}\r\nstatic void mv_desc_set_mode(struct mv_xor_desc_slot *desc)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nswitch (desc->type) {\r\ncase DMA_XOR:\r\ncase DMA_INTERRUPT:\r\nhw_desc->desc_command |= XOR_DESC_OPERATION_XOR;\r\nbreak;\r\ncase DMA_MEMCPY:\r\nhw_desc->desc_command |= XOR_DESC_OPERATION_MEMCPY;\r\nbreak;\r\ndefault:\r\nBUG();\r\nreturn;\r\n}\r\n}\r\nstatic void mv_desc_set_next_desc(struct mv_xor_desc_slot *desc,\r\nu32 next_desc_addr)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nBUG_ON(hw_desc->phy_next_desc);\r\nhw_desc->phy_next_desc = next_desc_addr;\r\n}\r\nstatic void mv_desc_set_src_addr(struct mv_xor_desc_slot *desc,\r\nint index, dma_addr_t addr)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->phy_src_addr[mv_phy_src_idx(index)] = addr;\r\nif (desc->type == DMA_XOR)\r\nhw_desc->desc_command |= (1 << index);\r\n}\r\nstatic u32 mv_chan_get_current_desc(struct mv_xor_chan *chan)\r\n{\r\nreturn readl_relaxed(XOR_CURR_DESC(chan));\r\n}\r\nstatic void mv_chan_set_next_descriptor(struct mv_xor_chan *chan,\r\nu32 next_desc_addr)\r\n{\r\nwritel_relaxed(next_desc_addr, XOR_NEXT_DESC(chan));\r\n}\r\nstatic void mv_chan_unmask_interrupts(struct mv_xor_chan *chan)\r\n{\r\nu32 val = readl_relaxed(XOR_INTR_MASK(chan));\r\nval |= XOR_INTR_MASK_VALUE << (chan->idx * 16);\r\nwritel_relaxed(val, XOR_INTR_MASK(chan));\r\n}\r\nstatic u32 mv_chan_get_intr_cause(struct mv_xor_chan *chan)\r\n{\r\nu32 intr_cause = readl_relaxed(XOR_INTR_CAUSE(chan));\r\nintr_cause = (intr_cause >> (chan->idx * 16)) & 0xFFFF;\r\nreturn intr_cause;\r\n}\r\nstatic void mv_chan_clear_eoc_cause(struct mv_xor_chan *chan)\r\n{\r\nu32 val;\r\nval = XOR_INT_END_OF_DESC | XOR_INT_END_OF_CHAIN | XOR_INT_STOPPED;\r\nval = ~(val << (chan->idx * 16));\r\ndev_dbg(mv_chan_to_devp(chan), "%s, val 0x%08x\n", __func__, val);\r\nwritel_relaxed(val, XOR_INTR_CAUSE(chan));\r\n}\r\nstatic void mv_chan_clear_err_status(struct mv_xor_chan *chan)\r\n{\r\nu32 val = 0xFFFF0000 >> (chan->idx * 16);\r\nwritel_relaxed(val, XOR_INTR_CAUSE(chan));\r\n}\r\nstatic void mv_chan_set_mode(struct mv_xor_chan *chan,\r\nu32 op_mode)\r\n{\r\nu32 config = readl_relaxed(XOR_CONFIG(chan));\r\nconfig &= ~0x7;\r\nconfig |= op_mode;\r\n#if defined(__BIG_ENDIAN)\r\nconfig |= XOR_DESCRIPTOR_SWAP;\r\n#else\r\nconfig &= ~XOR_DESCRIPTOR_SWAP;\r\n#endif\r\nwritel_relaxed(config, XOR_CONFIG(chan));\r\n}\r\nstatic void mv_chan_activate(struct mv_xor_chan *chan)\r\n{\r\ndev_dbg(mv_chan_to_devp(chan), " activate chan.\n");\r\nwritel(BIT(0), XOR_ACTIVATION(chan));\r\n}\r\nstatic char mv_chan_is_busy(struct mv_xor_chan *chan)\r\n{\r\nu32 state = readl_relaxed(XOR_ACTIVATION(chan));\r\nstate = (state >> 4) & 0x3;\r\nreturn (state == 1) ? 1 : 0;\r\n}\r\nstatic void mv_chan_start_new_chain(struct mv_xor_chan *mv_chan,\r\nstruct mv_xor_desc_slot *sw_desc)\r\n{\r\ndev_dbg(mv_chan_to_devp(mv_chan), "%s %d: sw_desc %p\n",\r\n__func__, __LINE__, sw_desc);\r\nmv_chan_set_next_descriptor(mv_chan, sw_desc->async_tx.phys);\r\nmv_chan->pending++;\r\nmv_xor_issue_pending(&mv_chan->dmachan);\r\n}\r\nstatic dma_cookie_t\r\nmv_desc_run_tx_complete_actions(struct mv_xor_desc_slot *desc,\r\nstruct mv_xor_chan *mv_chan,\r\ndma_cookie_t cookie)\r\n{\r\nBUG_ON(desc->async_tx.cookie < 0);\r\nif (desc->async_tx.cookie > 0) {\r\ncookie = desc->async_tx.cookie;\r\ndma_descriptor_unmap(&desc->async_tx);\r\ndmaengine_desc_get_callback_invoke(&desc->async_tx, NULL);\r\n}\r\ndma_run_dependencies(&desc->async_tx);\r\nreturn cookie;\r\n}\r\nstatic int\r\nmv_chan_clean_completed_slots(struct mv_xor_chan *mv_chan)\r\n{\r\nstruct mv_xor_desc_slot *iter, *_iter;\r\ndev_dbg(mv_chan_to_devp(mv_chan), "%s %d\n", __func__, __LINE__);\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->completed_slots,\r\nnode) {\r\nif (async_tx_test_ack(&iter->async_tx)) {\r\nlist_move_tail(&iter->node, &mv_chan->free_slots);\r\nif (!list_empty(&iter->sg_tx_list)) {\r\nlist_splice_tail_init(&iter->sg_tx_list,\r\n&mv_chan->free_slots);\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nmv_desc_clean_slot(struct mv_xor_desc_slot *desc,\r\nstruct mv_xor_chan *mv_chan)\r\n{\r\ndev_dbg(mv_chan_to_devp(mv_chan), "%s %d: desc %p flags %d\n",\r\n__func__, __LINE__, desc, desc->async_tx.flags);\r\nif (!async_tx_test_ack(&desc->async_tx)) {\r\nlist_move_tail(&desc->node, &mv_chan->completed_slots);\r\nif (!list_empty(&desc->sg_tx_list)) {\r\nlist_splice_tail_init(&desc->sg_tx_list,\r\n&mv_chan->completed_slots);\r\n}\r\n} else {\r\nlist_move_tail(&desc->node, &mv_chan->free_slots);\r\nif (!list_empty(&desc->sg_tx_list)) {\r\nlist_splice_tail_init(&desc->sg_tx_list,\r\n&mv_chan->free_slots);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void mv_chan_slot_cleanup(struct mv_xor_chan *mv_chan)\r\n{\r\nstruct mv_xor_desc_slot *iter, *_iter;\r\ndma_cookie_t cookie = 0;\r\nint busy = mv_chan_is_busy(mv_chan);\r\nu32 current_desc = mv_chan_get_current_desc(mv_chan);\r\nint current_cleaned = 0;\r\nstruct mv_xor_desc *hw_desc;\r\ndev_dbg(mv_chan_to_devp(mv_chan), "%s %d\n", __func__, __LINE__);\r\ndev_dbg(mv_chan_to_devp(mv_chan), "current_desc %x\n", current_desc);\r\nmv_chan_clean_completed_slots(mv_chan);\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->chain,\r\nnode) {\r\nhw_desc = iter->hw_desc;\r\nif (hw_desc->status & XOR_DESC_SUCCESS) {\r\ncookie = mv_desc_run_tx_complete_actions(iter, mv_chan,\r\ncookie);\r\nmv_desc_clean_slot(iter, mv_chan);\r\nif (iter->async_tx.phys == current_desc) {\r\ncurrent_cleaned = 1;\r\nbreak;\r\n}\r\n} else {\r\nif (iter->async_tx.phys == current_desc) {\r\ncurrent_cleaned = 0;\r\nbreak;\r\n}\r\n}\r\n}\r\nif ((busy == 0) && !list_empty(&mv_chan->chain)) {\r\nif (current_cleaned) {\r\niter = list_entry(mv_chan->chain.next,\r\nstruct mv_xor_desc_slot,\r\nnode);\r\nmv_chan_start_new_chain(mv_chan, iter);\r\n} else {\r\nif (!list_is_last(&iter->node, &mv_chan->chain)) {\r\niter = list_entry(iter->node.next,\r\nstruct mv_xor_desc_slot,\r\nnode);\r\nmv_chan_start_new_chain(mv_chan, iter);\r\n} else {\r\ntasklet_schedule(&mv_chan->irq_tasklet);\r\n}\r\n}\r\n}\r\nif (cookie > 0)\r\nmv_chan->dmachan.completed_cookie = cookie;\r\n}\r\nstatic void mv_xor_tasklet(unsigned long data)\r\n{\r\nstruct mv_xor_chan *chan = (struct mv_xor_chan *) data;\r\nspin_lock_bh(&chan->lock);\r\nmv_chan_slot_cleanup(chan);\r\nspin_unlock_bh(&chan->lock);\r\n}\r\nstatic struct mv_xor_desc_slot *\r\nmv_chan_alloc_slot(struct mv_xor_chan *mv_chan)\r\n{\r\nstruct mv_xor_desc_slot *iter;\r\nspin_lock_bh(&mv_chan->lock);\r\nif (!list_empty(&mv_chan->free_slots)) {\r\niter = list_first_entry(&mv_chan->free_slots,\r\nstruct mv_xor_desc_slot,\r\nnode);\r\nlist_move_tail(&iter->node, &mv_chan->allocated_slots);\r\nspin_unlock_bh(&mv_chan->lock);\r\nasync_tx_ack(&iter->async_tx);\r\niter->async_tx.cookie = -EBUSY;\r\nreturn iter;\r\n}\r\nspin_unlock_bh(&mv_chan->lock);\r\ntasklet_schedule(&mv_chan->irq_tasklet);\r\nreturn NULL;\r\n}\r\nstatic dma_cookie_t\r\nmv_xor_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct mv_xor_desc_slot *sw_desc = to_mv_xor_slot(tx);\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(tx->chan);\r\nstruct mv_xor_desc_slot *old_chain_tail;\r\ndma_cookie_t cookie;\r\nint new_hw_chain = 1;\r\ndev_dbg(mv_chan_to_devp(mv_chan),\r\n"%s sw_desc %p: async_tx %p\n",\r\n__func__, sw_desc, &sw_desc->async_tx);\r\nspin_lock_bh(&mv_chan->lock);\r\ncookie = dma_cookie_assign(tx);\r\nif (list_empty(&mv_chan->chain))\r\nlist_move_tail(&sw_desc->node, &mv_chan->chain);\r\nelse {\r\nnew_hw_chain = 0;\r\nold_chain_tail = list_entry(mv_chan->chain.prev,\r\nstruct mv_xor_desc_slot,\r\nnode);\r\nlist_move_tail(&sw_desc->node, &mv_chan->chain);\r\ndev_dbg(mv_chan_to_devp(mv_chan), "Append to last desc %pa\n",\r\n&old_chain_tail->async_tx.phys);\r\nmv_desc_set_next_desc(old_chain_tail, sw_desc->async_tx.phys);\r\nif (!mv_chan_is_busy(mv_chan)) {\r\nu32 current_desc = mv_chan_get_current_desc(mv_chan);\r\nif (current_desc == old_chain_tail->async_tx.phys)\r\nnew_hw_chain = 1;\r\n}\r\n}\r\nif (new_hw_chain)\r\nmv_chan_start_new_chain(mv_chan, sw_desc);\r\nspin_unlock_bh(&mv_chan->lock);\r\nreturn cookie;\r\n}\r\nstatic int mv_xor_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nvoid *virt_desc;\r\ndma_addr_t dma_desc;\r\nint idx;\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *slot = NULL;\r\nint num_descs_in_pool = MV_XOR_POOL_SIZE/MV_XOR_SLOT_SIZE;\r\nidx = mv_chan->slots_allocated;\r\nwhile (idx < num_descs_in_pool) {\r\nslot = kzalloc(sizeof(*slot), GFP_KERNEL);\r\nif (!slot) {\r\ndev_info(mv_chan_to_devp(mv_chan),\r\n"channel only initialized %d descriptor slots",\r\nidx);\r\nbreak;\r\n}\r\nvirt_desc = mv_chan->dma_desc_pool_virt;\r\nslot->hw_desc = virt_desc + idx * MV_XOR_SLOT_SIZE;\r\ndma_async_tx_descriptor_init(&slot->async_tx, chan);\r\nslot->async_tx.tx_submit = mv_xor_tx_submit;\r\nINIT_LIST_HEAD(&slot->node);\r\nINIT_LIST_HEAD(&slot->sg_tx_list);\r\ndma_desc = mv_chan->dma_desc_pool;\r\nslot->async_tx.phys = dma_desc + idx * MV_XOR_SLOT_SIZE;\r\nslot->idx = idx++;\r\nspin_lock_bh(&mv_chan->lock);\r\nmv_chan->slots_allocated = idx;\r\nlist_add_tail(&slot->node, &mv_chan->free_slots);\r\nspin_unlock_bh(&mv_chan->lock);\r\n}\r\ndev_dbg(mv_chan_to_devp(mv_chan),\r\n"allocated %d descriptor slots\n",\r\nmv_chan->slots_allocated);\r\nreturn mv_chan->slots_allocated ? : -ENOMEM;\r\n}\r\nstatic int mv_xor_add_io_win(struct mv_xor_chan *mv_chan, u32 addr)\r\n{\r\nstruct mv_xor_device *xordev = mv_chan->xordev;\r\nvoid __iomem *base = mv_chan->mmr_high_base;\r\nu32 win_enable;\r\nu32 size;\r\nu8 target, attr;\r\nint ret;\r\nint i;\r\nif (xordev->xor_type == XOR_ARMADA_37XX)\r\nreturn 0;\r\nfor (i = 0; i < WINDOW_COUNT; i++) {\r\nif (addr >= xordev->win_start[i] &&\r\naddr <= xordev->win_end[i]) {\r\nreturn 0;\r\n}\r\n}\r\nret = mvebu_mbus_get_io_win_info(addr, &size, &target, &attr);\r\nif (ret < 0)\r\nreturn 0;\r\nsize -= 1;\r\naddr &= ~size;\r\nwin_enable = readl(base + WINDOW_BAR_ENABLE(0));\r\ni = ffs(~win_enable) - 1;\r\nif (i >= WINDOW_COUNT)\r\nreturn -ENOMEM;\r\nwritel((addr & 0xffff0000) | (attr << 8) | target,\r\nbase + WINDOW_BASE(i));\r\nwritel(size & 0xffff0000, base + WINDOW_SIZE(i));\r\nxordev->win_start[i] = addr;\r\nxordev->win_end[i] = addr + size;\r\nwin_enable |= (1 << i);\r\nwin_enable |= 3 << (16 + (2 * i));\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(0));\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(1));\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmv_xor_prep_dma_xor(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,\r\nunsigned int src_cnt, size_t len, unsigned long flags)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *sw_desc;\r\nint ret;\r\nif (unlikely(len < MV_XOR_MIN_BYTE_COUNT))\r\nreturn NULL;\r\nBUG_ON(len > MV_XOR_MAX_BYTE_COUNT);\r\ndev_dbg(mv_chan_to_devp(mv_chan),\r\n"%s src_cnt: %d len: %zu dest %pad flags: %ld\n",\r\n__func__, src_cnt, len, &dest, flags);\r\nret = mv_xor_add_io_win(mv_chan, dest);\r\nif (ret)\r\nreturn NULL;\r\nsw_desc = mv_chan_alloc_slot(mv_chan);\r\nif (sw_desc) {\r\nsw_desc->type = DMA_XOR;\r\nsw_desc->async_tx.flags = flags;\r\nmv_desc_init(sw_desc, dest, len, flags);\r\nif (mv_chan->op_in_desc == XOR_MODE_IN_DESC)\r\nmv_desc_set_mode(sw_desc);\r\nwhile (src_cnt--) {\r\nret = mv_xor_add_io_win(mv_chan, src[src_cnt]);\r\nif (ret)\r\nreturn NULL;\r\nmv_desc_set_src_addr(sw_desc, src_cnt, src[src_cnt]);\r\n}\r\n}\r\ndev_dbg(mv_chan_to_devp(mv_chan),\r\n"%s sw_desc %p async_tx %p \n",\r\n__func__, sw_desc, &sw_desc->async_tx);\r\nreturn sw_desc ? &sw_desc->async_tx : NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmv_xor_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nreturn mv_xor_prep_dma_xor(chan, dest, &src, 1, len, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmv_xor_prep_dma_interrupt(struct dma_chan *chan, unsigned long flags)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\ndma_addr_t src, dest;\r\nsize_t len;\r\nsrc = mv_chan->dummy_src_addr;\r\ndest = mv_chan->dummy_dst_addr;\r\nlen = MV_XOR_MIN_BYTE_COUNT;\r\nreturn mv_xor_prep_dma_xor(chan, dest, &src, 1, len, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmv_xor_prep_dma_sg(struct dma_chan *chan, struct scatterlist *dst_sg,\r\nunsigned int dst_sg_len, struct scatterlist *src_sg,\r\nunsigned int src_sg_len, unsigned long flags)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *new;\r\nstruct mv_xor_desc_slot *first = NULL;\r\nstruct mv_xor_desc_slot *prev = NULL;\r\nsize_t len, dst_avail, src_avail;\r\ndma_addr_t dma_dst, dma_src;\r\nint desc_cnt = 0;\r\nint ret;\r\ndev_dbg(mv_chan_to_devp(mv_chan),\r\n"%s dst_sg_len: %d src_sg_len: %d flags: %ld\n",\r\n__func__, dst_sg_len, src_sg_len, flags);\r\ndst_avail = sg_dma_len(dst_sg);\r\nsrc_avail = sg_dma_len(src_sg);\r\nwhile (true) {\r\ndesc_cnt++;\r\nnew = mv_chan_alloc_slot(mv_chan);\r\nif (!new) {\r\ndev_err(mv_chan_to_devp(mv_chan),\r\n"Out of descriptors (desc_cnt=%d)!\n",\r\ndesc_cnt);\r\ngoto err;\r\n}\r\nlen = min_t(size_t, src_avail, dst_avail);\r\nlen = min_t(size_t, len, MV_XOR_MAX_BYTE_COUNT);\r\nif (len == 0)\r\ngoto fetch;\r\nif (len < MV_XOR_MIN_BYTE_COUNT) {\r\ndev_err(mv_chan_to_devp(mv_chan),\r\n"Transfer size of %zu too small!\n", len);\r\ngoto err;\r\n}\r\ndma_dst = sg_dma_address(dst_sg) + sg_dma_len(dst_sg) -\r\ndst_avail;\r\ndma_src = sg_dma_address(src_sg) + sg_dma_len(src_sg) -\r\nsrc_avail;\r\nret = mv_xor_add_io_win(mv_chan, dma_dst);\r\nif (ret)\r\ngoto err;\r\nret = mv_xor_add_io_win(mv_chan, dma_src);\r\nif (ret)\r\ngoto err;\r\nmv_xor_config_sg_ll_desc(new, dma_src, dma_dst, len, prev);\r\nprev = new;\r\ndst_avail -= len;\r\nsrc_avail -= len;\r\nif (!first)\r\nfirst = new;\r\nelse\r\nlist_move_tail(&new->node, &first->sg_tx_list);\r\nfetch:\r\nif (dst_avail == 0) {\r\nif (dst_sg_len == 0)\r\nbreak;\r\ndst_sg = sg_next(dst_sg);\r\nif (dst_sg == NULL)\r\nbreak;\r\ndst_sg_len--;\r\ndst_avail = sg_dma_len(dst_sg);\r\n}\r\nif (src_avail == 0) {\r\nif (src_sg_len == 0)\r\nbreak;\r\nsrc_sg = sg_next(src_sg);\r\nif (src_sg == NULL)\r\nbreak;\r\nsrc_sg_len--;\r\nsrc_avail = sg_dma_len(src_sg);\r\n}\r\n}\r\nmv_xor_desc_config_eod(new);\r\nfirst->async_tx.flags = flags;\r\nreturn &first->async_tx;\r\nerr:\r\nspin_lock_bh(&mv_chan->lock);\r\nmv_desc_clean_slot(first, mv_chan);\r\nspin_unlock_bh(&mv_chan->lock);\r\nreturn NULL;\r\n}\r\nstatic void mv_xor_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *iter, *_iter;\r\nint in_use_descs = 0;\r\nspin_lock_bh(&mv_chan->lock);\r\nmv_chan_slot_cleanup(mv_chan);\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->chain,\r\nnode) {\r\nin_use_descs++;\r\nlist_move_tail(&iter->node, &mv_chan->free_slots);\r\n}\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->completed_slots,\r\nnode) {\r\nin_use_descs++;\r\nlist_move_tail(&iter->node, &mv_chan->free_slots);\r\n}\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->allocated_slots,\r\nnode) {\r\nin_use_descs++;\r\nlist_move_tail(&iter->node, &mv_chan->free_slots);\r\n}\r\nlist_for_each_entry_safe_reverse(\r\niter, _iter, &mv_chan->free_slots, node) {\r\nlist_del(&iter->node);\r\nkfree(iter);\r\nmv_chan->slots_allocated--;\r\n}\r\ndev_dbg(mv_chan_to_devp(mv_chan), "%s slots_allocated %d\n",\r\n__func__, mv_chan->slots_allocated);\r\nspin_unlock_bh(&mv_chan->lock);\r\nif (in_use_descs)\r\ndev_err(mv_chan_to_devp(mv_chan),\r\n"freeing %d in use descriptors!\n", in_use_descs);\r\n}\r\nstatic enum dma_status mv_xor_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nspin_lock_bh(&mv_chan->lock);\r\nmv_chan_slot_cleanup(mv_chan);\r\nspin_unlock_bh(&mv_chan->lock);\r\nreturn dma_cookie_status(chan, cookie, txstate);\r\n}\r\nstatic void mv_chan_dump_regs(struct mv_xor_chan *chan)\r\n{\r\nu32 val;\r\nval = readl_relaxed(XOR_CONFIG(chan));\r\ndev_err(mv_chan_to_devp(chan), "config 0x%08x\n", val);\r\nval = readl_relaxed(XOR_ACTIVATION(chan));\r\ndev_err(mv_chan_to_devp(chan), "activation 0x%08x\n", val);\r\nval = readl_relaxed(XOR_INTR_CAUSE(chan));\r\ndev_err(mv_chan_to_devp(chan), "intr cause 0x%08x\n", val);\r\nval = readl_relaxed(XOR_INTR_MASK(chan));\r\ndev_err(mv_chan_to_devp(chan), "intr mask 0x%08x\n", val);\r\nval = readl_relaxed(XOR_ERROR_CAUSE(chan));\r\ndev_err(mv_chan_to_devp(chan), "error cause 0x%08x\n", val);\r\nval = readl_relaxed(XOR_ERROR_ADDR(chan));\r\ndev_err(mv_chan_to_devp(chan), "error addr 0x%08x\n", val);\r\n}\r\nstatic void mv_chan_err_interrupt_handler(struct mv_xor_chan *chan,\r\nu32 intr_cause)\r\n{\r\nif (intr_cause & XOR_INT_ERR_DECODE) {\r\ndev_dbg(mv_chan_to_devp(chan), "ignoring address decode error\n");\r\nreturn;\r\n}\r\ndev_err(mv_chan_to_devp(chan), "error on chan %d. intr cause 0x%08x\n",\r\nchan->idx, intr_cause);\r\nmv_chan_dump_regs(chan);\r\nWARN_ON(1);\r\n}\r\nstatic irqreturn_t mv_xor_interrupt_handler(int irq, void *data)\r\n{\r\nstruct mv_xor_chan *chan = data;\r\nu32 intr_cause = mv_chan_get_intr_cause(chan);\r\ndev_dbg(mv_chan_to_devp(chan), "intr cause %x\n", intr_cause);\r\nif (intr_cause & XOR_INTR_ERRORS)\r\nmv_chan_err_interrupt_handler(chan, intr_cause);\r\ntasklet_schedule(&chan->irq_tasklet);\r\nmv_chan_clear_eoc_cause(chan);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void mv_xor_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nif (mv_chan->pending >= MV_XOR_THRESHOLD) {\r\nmv_chan->pending = 0;\r\nmv_chan_activate(mv_chan);\r\n}\r\n}\r\nstatic int mv_chan_memcpy_self_test(struct mv_xor_chan *mv_chan)\r\n{\r\nint i, ret;\r\nvoid *src, *dest;\r\ndma_addr_t src_dma, dest_dma;\r\nstruct dma_chan *dma_chan;\r\ndma_cookie_t cookie;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dmaengine_unmap_data *unmap;\r\nint err = 0;\r\nsrc = kmalloc(sizeof(u8) * PAGE_SIZE, GFP_KERNEL);\r\nif (!src)\r\nreturn -ENOMEM;\r\ndest = kzalloc(sizeof(u8) * PAGE_SIZE, GFP_KERNEL);\r\nif (!dest) {\r\nkfree(src);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < PAGE_SIZE; i++)\r\n((u8 *) src)[i] = (u8)i;\r\ndma_chan = &mv_chan->dmachan;\r\nif (mv_xor_alloc_chan_resources(dma_chan) < 1) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nunmap = dmaengine_get_unmap_data(dma_chan->device->dev, 2, GFP_KERNEL);\r\nif (!unmap) {\r\nerr = -ENOMEM;\r\ngoto free_resources;\r\n}\r\nsrc_dma = dma_map_page(dma_chan->device->dev, virt_to_page(src),\r\n(size_t)src & ~PAGE_MASK, PAGE_SIZE,\r\nDMA_TO_DEVICE);\r\nunmap->addr[0] = src_dma;\r\nret = dma_mapping_error(dma_chan->device->dev, src_dma);\r\nif (ret) {\r\nerr = -ENOMEM;\r\ngoto free_resources;\r\n}\r\nunmap->to_cnt = 1;\r\ndest_dma = dma_map_page(dma_chan->device->dev, virt_to_page(dest),\r\n(size_t)dest & ~PAGE_MASK, PAGE_SIZE,\r\nDMA_FROM_DEVICE);\r\nunmap->addr[1] = dest_dma;\r\nret = dma_mapping_error(dma_chan->device->dev, dest_dma);\r\nif (ret) {\r\nerr = -ENOMEM;\r\ngoto free_resources;\r\n}\r\nunmap->from_cnt = 1;\r\nunmap->len = PAGE_SIZE;\r\ntx = mv_xor_prep_dma_memcpy(dma_chan, dest_dma, src_dma,\r\nPAGE_SIZE, 0);\r\nif (!tx) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test cannot prepare operation, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ncookie = mv_xor_tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test submit error, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nmv_xor_issue_pending(dma_chan);\r\nasync_tx_ack(tx);\r\nmsleep(1);\r\nif (mv_xor_status(dma_chan, cookie, NULL) !=\r\nDMA_COMPLETE) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test copy timed out, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ndma_sync_single_for_cpu(dma_chan->device->dev, dest_dma,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nif (memcmp(src, dest, PAGE_SIZE)) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test copy failed compare, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nfree_resources:\r\ndmaengine_unmap_put(unmap);\r\nmv_xor_free_chan_resources(dma_chan);\r\nout:\r\nkfree(src);\r\nkfree(dest);\r\nreturn err;\r\n}\r\nstatic int\r\nmv_chan_xor_self_test(struct mv_xor_chan *mv_chan)\r\n{\r\nint i, src_idx, ret;\r\nstruct page *dest;\r\nstruct page *xor_srcs[MV_XOR_NUM_SRC_TEST];\r\ndma_addr_t dma_srcs[MV_XOR_NUM_SRC_TEST];\r\ndma_addr_t dest_dma;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dmaengine_unmap_data *unmap;\r\nstruct dma_chan *dma_chan;\r\ndma_cookie_t cookie;\r\nu8 cmp_byte = 0;\r\nu32 cmp_word;\r\nint err = 0;\r\nint src_count = MV_XOR_NUM_SRC_TEST;\r\nfor (src_idx = 0; src_idx < src_count; src_idx++) {\r\nxor_srcs[src_idx] = alloc_page(GFP_KERNEL);\r\nif (!xor_srcs[src_idx]) {\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\nreturn -ENOMEM;\r\n}\r\n}\r\ndest = alloc_page(GFP_KERNEL);\r\nif (!dest) {\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\nreturn -ENOMEM;\r\n}\r\nfor (src_idx = 0; src_idx < src_count; src_idx++) {\r\nu8 *ptr = page_address(xor_srcs[src_idx]);\r\nfor (i = 0; i < PAGE_SIZE; i++)\r\nptr[i] = (1 << src_idx);\r\n}\r\nfor (src_idx = 0; src_idx < src_count; src_idx++)\r\ncmp_byte ^= (u8) (1 << src_idx);\r\ncmp_word = (cmp_byte << 24) | (cmp_byte << 16) |\r\n(cmp_byte << 8) | cmp_byte;\r\nmemset(page_address(dest), 0, PAGE_SIZE);\r\ndma_chan = &mv_chan->dmachan;\r\nif (mv_xor_alloc_chan_resources(dma_chan) < 1) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nunmap = dmaengine_get_unmap_data(dma_chan->device->dev, src_count + 1,\r\nGFP_KERNEL);\r\nif (!unmap) {\r\nerr = -ENOMEM;\r\ngoto free_resources;\r\n}\r\nfor (i = 0; i < src_count; i++) {\r\nunmap->addr[i] = dma_map_page(dma_chan->device->dev, xor_srcs[i],\r\n0, PAGE_SIZE, DMA_TO_DEVICE);\r\ndma_srcs[i] = unmap->addr[i];\r\nret = dma_mapping_error(dma_chan->device->dev, unmap->addr[i]);\r\nif (ret) {\r\nerr = -ENOMEM;\r\ngoto free_resources;\r\n}\r\nunmap->to_cnt++;\r\n}\r\nunmap->addr[src_count] = dma_map_page(dma_chan->device->dev, dest, 0, PAGE_SIZE,\r\nDMA_FROM_DEVICE);\r\ndest_dma = unmap->addr[src_count];\r\nret = dma_mapping_error(dma_chan->device->dev, unmap->addr[src_count]);\r\nif (ret) {\r\nerr = -ENOMEM;\r\ngoto free_resources;\r\n}\r\nunmap->from_cnt = 1;\r\nunmap->len = PAGE_SIZE;\r\ntx = mv_xor_prep_dma_xor(dma_chan, dest_dma, dma_srcs,\r\nsrc_count, PAGE_SIZE, 0);\r\nif (!tx) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test cannot prepare operation, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ncookie = mv_xor_tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test submit error, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nmv_xor_issue_pending(dma_chan);\r\nasync_tx_ack(tx);\r\nmsleep(8);\r\nif (mv_xor_status(dma_chan, cookie, NULL) !=\r\nDMA_COMPLETE) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test xor timed out, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ndma_sync_single_for_cpu(dma_chan->device->dev, dest_dma,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nfor (i = 0; i < (PAGE_SIZE / sizeof(u32)); i++) {\r\nu32 *ptr = page_address(dest);\r\nif (ptr[i] != cmp_word) {\r\ndev_err(dma_chan->device->dev,\r\n"Self-test xor failed compare, disabling. index %d, data %x, expected %x\n",\r\ni, ptr[i], cmp_word);\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\n}\r\nfree_resources:\r\ndmaengine_unmap_put(unmap);\r\nmv_xor_free_chan_resources(dma_chan);\r\nout:\r\nsrc_idx = src_count;\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\n__free_page(dest);\r\nreturn err;\r\n}\r\nstatic int mv_xor_channel_remove(struct mv_xor_chan *mv_chan)\r\n{\r\nstruct dma_chan *chan, *_chan;\r\nstruct device *dev = mv_chan->dmadev.dev;\r\ndma_async_device_unregister(&mv_chan->dmadev);\r\ndma_free_coherent(dev, MV_XOR_POOL_SIZE,\r\nmv_chan->dma_desc_pool_virt, mv_chan->dma_desc_pool);\r\ndma_unmap_single(dev, mv_chan->dummy_src_addr,\r\nMV_XOR_MIN_BYTE_COUNT, DMA_FROM_DEVICE);\r\ndma_unmap_single(dev, mv_chan->dummy_dst_addr,\r\nMV_XOR_MIN_BYTE_COUNT, DMA_TO_DEVICE);\r\nlist_for_each_entry_safe(chan, _chan, &mv_chan->dmadev.channels,\r\ndevice_node) {\r\nlist_del(&chan->device_node);\r\n}\r\nfree_irq(mv_chan->irq, mv_chan);\r\nreturn 0;\r\n}\r\nstatic struct mv_xor_chan *\r\nmv_xor_channel_add(struct mv_xor_device *xordev,\r\nstruct platform_device *pdev,\r\nint idx, dma_cap_mask_t cap_mask, int irq)\r\n{\r\nint ret = 0;\r\nstruct mv_xor_chan *mv_chan;\r\nstruct dma_device *dma_dev;\r\nmv_chan = devm_kzalloc(&pdev->dev, sizeof(*mv_chan), GFP_KERNEL);\r\nif (!mv_chan)\r\nreturn ERR_PTR(-ENOMEM);\r\nmv_chan->idx = idx;\r\nmv_chan->irq = irq;\r\nif (xordev->xor_type == XOR_ORION)\r\nmv_chan->op_in_desc = XOR_MODE_IN_REG;\r\nelse\r\nmv_chan->op_in_desc = XOR_MODE_IN_DESC;\r\ndma_dev = &mv_chan->dmadev;\r\nmv_chan->xordev = xordev;\r\nmv_chan->dummy_src_addr = dma_map_single(dma_dev->dev,\r\nmv_chan->dummy_src, MV_XOR_MIN_BYTE_COUNT, DMA_FROM_DEVICE);\r\nmv_chan->dummy_dst_addr = dma_map_single(dma_dev->dev,\r\nmv_chan->dummy_dst, MV_XOR_MIN_BYTE_COUNT, DMA_TO_DEVICE);\r\nmv_chan->dma_desc_pool_virt =\r\ndma_alloc_wc(&pdev->dev, MV_XOR_POOL_SIZE, &mv_chan->dma_desc_pool,\r\nGFP_KERNEL);\r\nif (!mv_chan->dma_desc_pool_virt)\r\nreturn ERR_PTR(-ENOMEM);\r\ndma_dev->cap_mask = cap_mask;\r\nINIT_LIST_HEAD(&dma_dev->channels);\r\ndma_dev->device_alloc_chan_resources = mv_xor_alloc_chan_resources;\r\ndma_dev->device_free_chan_resources = mv_xor_free_chan_resources;\r\ndma_dev->device_tx_status = mv_xor_status;\r\ndma_dev->device_issue_pending = mv_xor_issue_pending;\r\ndma_dev->dev = &pdev->dev;\r\nif (dma_has_cap(DMA_INTERRUPT, dma_dev->cap_mask))\r\ndma_dev->device_prep_dma_interrupt = mv_xor_prep_dma_interrupt;\r\nif (dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask))\r\ndma_dev->device_prep_dma_memcpy = mv_xor_prep_dma_memcpy;\r\nif (dma_has_cap(DMA_SG, dma_dev->cap_mask))\r\ndma_dev->device_prep_dma_sg = mv_xor_prep_dma_sg;\r\nif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\r\ndma_dev->max_xor = 8;\r\ndma_dev->device_prep_dma_xor = mv_xor_prep_dma_xor;\r\n}\r\nmv_chan->mmr_base = xordev->xor_base;\r\nmv_chan->mmr_high_base = xordev->xor_high_base;\r\ntasklet_init(&mv_chan->irq_tasklet, mv_xor_tasklet, (unsigned long)\r\nmv_chan);\r\nmv_chan_clear_err_status(mv_chan);\r\nret = request_irq(mv_chan->irq, mv_xor_interrupt_handler,\r\n0, dev_name(&pdev->dev), mv_chan);\r\nif (ret)\r\ngoto err_free_dma;\r\nmv_chan_unmask_interrupts(mv_chan);\r\nif (mv_chan->op_in_desc == XOR_MODE_IN_DESC)\r\nmv_chan_set_mode(mv_chan, XOR_OPERATION_MODE_IN_DESC);\r\nelse\r\nmv_chan_set_mode(mv_chan, XOR_OPERATION_MODE_XOR);\r\nspin_lock_init(&mv_chan->lock);\r\nINIT_LIST_HEAD(&mv_chan->chain);\r\nINIT_LIST_HEAD(&mv_chan->completed_slots);\r\nINIT_LIST_HEAD(&mv_chan->free_slots);\r\nINIT_LIST_HEAD(&mv_chan->allocated_slots);\r\nmv_chan->dmachan.device = dma_dev;\r\ndma_cookie_init(&mv_chan->dmachan);\r\nlist_add_tail(&mv_chan->dmachan.device_node, &dma_dev->channels);\r\nif (dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask)) {\r\nret = mv_chan_memcpy_self_test(mv_chan);\r\ndev_dbg(&pdev->dev, "memcpy self test returned %d\n", ret);\r\nif (ret)\r\ngoto err_free_irq;\r\n}\r\nif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\r\nret = mv_chan_xor_self_test(mv_chan);\r\ndev_dbg(&pdev->dev, "xor self test returned %d\n", ret);\r\nif (ret)\r\ngoto err_free_irq;\r\n}\r\ndev_info(&pdev->dev, "Marvell XOR (%s): ( %s%s%s%s)\n",\r\nmv_chan->op_in_desc ? "Descriptor Mode" : "Registers Mode",\r\ndma_has_cap(DMA_XOR, dma_dev->cap_mask) ? "xor " : "",\r\ndma_has_cap(DMA_MEMCPY, dma_dev->cap_mask) ? "cpy " : "",\r\ndma_has_cap(DMA_SG, dma_dev->cap_mask) ? "sg " : "",\r\ndma_has_cap(DMA_INTERRUPT, dma_dev->cap_mask) ? "intr " : "");\r\ndma_async_device_register(dma_dev);\r\nreturn mv_chan;\r\nerr_free_irq:\r\nfree_irq(mv_chan->irq, mv_chan);\r\nerr_free_dma:\r\ndma_free_coherent(&pdev->dev, MV_XOR_POOL_SIZE,\r\nmv_chan->dma_desc_pool_virt, mv_chan->dma_desc_pool);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void\r\nmv_xor_conf_mbus_windows(struct mv_xor_device *xordev,\r\nconst struct mbus_dram_target_info *dram)\r\n{\r\nvoid __iomem *base = xordev->xor_high_base;\r\nu32 win_enable = 0;\r\nint i;\r\nfor (i = 0; i < 8; i++) {\r\nwritel(0, base + WINDOW_BASE(i));\r\nwritel(0, base + WINDOW_SIZE(i));\r\nif (i < 4)\r\nwritel(0, base + WINDOW_REMAP_HIGH(i));\r\n}\r\nfor (i = 0; i < dram->num_cs; i++) {\r\nconst struct mbus_dram_window *cs = dram->cs + i;\r\nwritel((cs->base & 0xffff0000) |\r\n(cs->mbus_attr << 8) |\r\ndram->mbus_dram_target_id, base + WINDOW_BASE(i));\r\nwritel((cs->size - 1) & 0xffff0000, base + WINDOW_SIZE(i));\r\nxordev->win_start[i] = cs->base;\r\nxordev->win_end[i] = cs->base + cs->size - 1;\r\nwin_enable |= (1 << i);\r\nwin_enable |= 3 << (16 + (2 * i));\r\n}\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(0));\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(1));\r\nwritel(0, base + WINDOW_OVERRIDE_CTRL(0));\r\nwritel(0, base + WINDOW_OVERRIDE_CTRL(1));\r\n}\r\nstatic void\r\nmv_xor_conf_mbus_windows_a3700(struct mv_xor_device *xordev)\r\n{\r\nvoid __iomem *base = xordev->xor_high_base;\r\nu32 win_enable = 0;\r\nint i;\r\nfor (i = 0; i < 8; i++) {\r\nwritel(0, base + WINDOW_BASE(i));\r\nwritel(0, base + WINDOW_SIZE(i));\r\nif (i < 4)\r\nwritel(0, base + WINDOW_REMAP_HIGH(i));\r\n}\r\nwritel(0xffff0000, base + WINDOW_SIZE(0));\r\nwin_enable |= 1;\r\nwin_enable |= 3 << 16;\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(0));\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(1));\r\nwritel(0, base + WINDOW_OVERRIDE_CTRL(0));\r\nwritel(0, base + WINDOW_OVERRIDE_CTRL(1));\r\n}\r\nstatic int mv_xor_suspend(struct platform_device *pdev, pm_message_t state)\r\n{\r\nstruct mv_xor_device *xordev = platform_get_drvdata(pdev);\r\nint i;\r\nfor (i = 0; i < MV_XOR_MAX_CHANNELS; i++) {\r\nstruct mv_xor_chan *mv_chan = xordev->channels[i];\r\nif (!mv_chan)\r\ncontinue;\r\nmv_chan->saved_config_reg =\r\nreadl_relaxed(XOR_CONFIG(mv_chan));\r\nmv_chan->saved_int_mask_reg =\r\nreadl_relaxed(XOR_INTR_MASK(mv_chan));\r\n}\r\nreturn 0;\r\n}\r\nstatic int mv_xor_resume(struct platform_device *dev)\r\n{\r\nstruct mv_xor_device *xordev = platform_get_drvdata(dev);\r\nconst struct mbus_dram_target_info *dram;\r\nint i;\r\nfor (i = 0; i < MV_XOR_MAX_CHANNELS; i++) {\r\nstruct mv_xor_chan *mv_chan = xordev->channels[i];\r\nif (!mv_chan)\r\ncontinue;\r\nwritel_relaxed(mv_chan->saved_config_reg,\r\nXOR_CONFIG(mv_chan));\r\nwritel_relaxed(mv_chan->saved_int_mask_reg,\r\nXOR_INTR_MASK(mv_chan));\r\n}\r\nif (xordev->xor_type == XOR_ARMADA_37XX) {\r\nmv_xor_conf_mbus_windows_a3700(xordev);\r\nreturn 0;\r\n}\r\ndram = mv_mbus_dram_info();\r\nif (dram)\r\nmv_xor_conf_mbus_windows(xordev, dram);\r\nreturn 0;\r\n}\r\nstatic int mv_xor_probe(struct platform_device *pdev)\r\n{\r\nconst struct mbus_dram_target_info *dram;\r\nstruct mv_xor_device *xordev;\r\nstruct mv_xor_platform_data *pdata = dev_get_platdata(&pdev->dev);\r\nstruct resource *res;\r\nunsigned int max_engines, max_channels;\r\nint i, ret;\r\ndev_notice(&pdev->dev, "Marvell shared XOR driver\n");\r\nxordev = devm_kzalloc(&pdev->dev, sizeof(*xordev), GFP_KERNEL);\r\nif (!xordev)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res)\r\nreturn -ENODEV;\r\nxordev->xor_base = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!xordev->xor_base)\r\nreturn -EBUSY;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (!res)\r\nreturn -ENODEV;\r\nxordev->xor_high_base = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!xordev->xor_high_base)\r\nreturn -EBUSY;\r\nplatform_set_drvdata(pdev, xordev);\r\nxordev->xor_type = XOR_ORION;\r\nif (pdev->dev.of_node) {\r\nconst struct of_device_id *of_id =\r\nof_match_device(mv_xor_dt_ids,\r\n&pdev->dev);\r\nxordev->xor_type = (uintptr_t)of_id->data;\r\n}\r\nif (xordev->xor_type == XOR_ARMADA_37XX) {\r\nmv_xor_conf_mbus_windows_a3700(xordev);\r\n} else {\r\ndram = mv_mbus_dram_info();\r\nif (dram)\r\nmv_xor_conf_mbus_windows(xordev, dram);\r\n}\r\nxordev->clk = clk_get(&pdev->dev, NULL);\r\nif (!IS_ERR(xordev->clk))\r\nclk_prepare_enable(xordev->clk);\r\nmax_engines = num_present_cpus();\r\nif (xordev->xor_type == XOR_ARMADA_37XX)\r\nmax_channels = num_present_cpus();\r\nelse\r\nmax_channels = min_t(unsigned int,\r\nMV_XOR_MAX_CHANNELS,\r\nDIV_ROUND_UP(num_present_cpus(), 2));\r\nif (mv_xor_engine_count >= max_engines)\r\nreturn 0;\r\nif (pdev->dev.of_node) {\r\nstruct device_node *np;\r\nint i = 0;\r\nfor_each_child_of_node(pdev->dev.of_node, np) {\r\nstruct mv_xor_chan *chan;\r\ndma_cap_mask_t cap_mask;\r\nint irq;\r\nif (i >= max_channels)\r\ncontinue;\r\ndma_cap_zero(cap_mask);\r\ndma_cap_set(DMA_MEMCPY, cap_mask);\r\ndma_cap_set(DMA_SG, cap_mask);\r\ndma_cap_set(DMA_XOR, cap_mask);\r\ndma_cap_set(DMA_INTERRUPT, cap_mask);\r\nirq = irq_of_parse_and_map(np, 0);\r\nif (!irq) {\r\nret = -ENODEV;\r\ngoto err_channel_add;\r\n}\r\nchan = mv_xor_channel_add(xordev, pdev, i,\r\ncap_mask, irq);\r\nif (IS_ERR(chan)) {\r\nret = PTR_ERR(chan);\r\nirq_dispose_mapping(irq);\r\ngoto err_channel_add;\r\n}\r\nxordev->channels[i] = chan;\r\ni++;\r\n}\r\n} else if (pdata && pdata->channels) {\r\nfor (i = 0; i < max_channels; i++) {\r\nstruct mv_xor_channel_data *cd;\r\nstruct mv_xor_chan *chan;\r\nint irq;\r\ncd = &pdata->channels[i];\r\nif (!cd) {\r\nret = -ENODEV;\r\ngoto err_channel_add;\r\n}\r\nirq = platform_get_irq(pdev, i);\r\nif (irq < 0) {\r\nret = irq;\r\ngoto err_channel_add;\r\n}\r\nchan = mv_xor_channel_add(xordev, pdev, i,\r\ncd->cap_mask, irq);\r\nif (IS_ERR(chan)) {\r\nret = PTR_ERR(chan);\r\ngoto err_channel_add;\r\n}\r\nxordev->channels[i] = chan;\r\n}\r\n}\r\nreturn 0;\r\nerr_channel_add:\r\nfor (i = 0; i < MV_XOR_MAX_CHANNELS; i++)\r\nif (xordev->channels[i]) {\r\nmv_xor_channel_remove(xordev->channels[i]);\r\nif (pdev->dev.of_node)\r\nirq_dispose_mapping(xordev->channels[i]->irq);\r\n}\r\nif (!IS_ERR(xordev->clk)) {\r\nclk_disable_unprepare(xordev->clk);\r\nclk_put(xordev->clk);\r\n}\r\nreturn ret;\r\n}
