static struct sk_buff *dequeue_func(struct codel_vars *vars, void *ctx)\r\n{\r\nstruct Qdisc *sch = ctx;\r\nstruct sk_buff *skb = __qdisc_dequeue_head(&sch->q);\r\nif (skb)\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\nprefetch(&skb->end);\r\nreturn skb;\r\n}\r\nstatic void drop_func(struct sk_buff *skb, void *ctx)\r\n{\r\nstruct Qdisc *sch = ctx;\r\nkfree_skb(skb);\r\nqdisc_qstats_drop(sch);\r\n}\r\nstatic struct sk_buff *codel_qdisc_dequeue(struct Qdisc *sch)\r\n{\r\nstruct codel_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nskb = codel_dequeue(sch, &sch->qstats.backlog, &q->params, &q->vars,\r\n&q->stats, qdisc_pkt_len, codel_get_enqueue_time,\r\ndrop_func, dequeue_func);\r\nif (q->stats.drop_count && sch->q.qlen) {\r\nqdisc_tree_reduce_backlog(sch, q->stats.drop_count, q->stats.drop_len);\r\nq->stats.drop_count = 0;\r\nq->stats.drop_len = 0;\r\n}\r\nif (skb)\r\nqdisc_bstats_update(sch, skb);\r\nreturn skb;\r\n}\r\nstatic int codel_qdisc_enqueue(struct sk_buff *skb, struct Qdisc *sch,\r\nstruct sk_buff **to_free)\r\n{\r\nstruct codel_sched_data *q;\r\nif (likely(qdisc_qlen(sch) < sch->limit)) {\r\ncodel_set_enqueue_time(skb);\r\nreturn qdisc_enqueue_tail(skb, sch);\r\n}\r\nq = qdisc_priv(sch);\r\nq->drop_overlimit++;\r\nreturn qdisc_drop(skb, sch, to_free);\r\n}\r\nstatic int codel_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct codel_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_CODEL_MAX + 1];\r\nunsigned int qlen, dropped = 0;\r\nint err;\r\nif (!opt)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_CODEL_MAX, opt, codel_policy);\r\nif (err < 0)\r\nreturn err;\r\nsch_tree_lock(sch);\r\nif (tb[TCA_CODEL_TARGET]) {\r\nu32 target = nla_get_u32(tb[TCA_CODEL_TARGET]);\r\nq->params.target = ((u64)target * NSEC_PER_USEC) >> CODEL_SHIFT;\r\n}\r\nif (tb[TCA_CODEL_CE_THRESHOLD]) {\r\nu64 val = nla_get_u32(tb[TCA_CODEL_CE_THRESHOLD]);\r\nq->params.ce_threshold = (val * NSEC_PER_USEC) >> CODEL_SHIFT;\r\n}\r\nif (tb[TCA_CODEL_INTERVAL]) {\r\nu32 interval = nla_get_u32(tb[TCA_CODEL_INTERVAL]);\r\nq->params.interval = ((u64)interval * NSEC_PER_USEC) >> CODEL_SHIFT;\r\n}\r\nif (tb[TCA_CODEL_LIMIT])\r\nsch->limit = nla_get_u32(tb[TCA_CODEL_LIMIT]);\r\nif (tb[TCA_CODEL_ECN])\r\nq->params.ecn = !!nla_get_u32(tb[TCA_CODEL_ECN]);\r\nqlen = sch->q.qlen;\r\nwhile (sch->q.qlen > sch->limit) {\r\nstruct sk_buff *skb = __qdisc_dequeue_head(&sch->q);\r\ndropped += qdisc_pkt_len(skb);\r\nqdisc_qstats_backlog_dec(sch, skb);\r\nrtnl_qdisc_drop(skb, sch);\r\n}\r\nqdisc_tree_reduce_backlog(sch, qlen - sch->q.qlen, dropped);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic int codel_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct codel_sched_data *q = qdisc_priv(sch);\r\nsch->limit = DEFAULT_CODEL_LIMIT;\r\ncodel_params_init(&q->params);\r\ncodel_vars_init(&q->vars);\r\ncodel_stats_init(&q->stats);\r\nq->params.mtu = psched_mtu(qdisc_dev(sch));\r\nif (opt) {\r\nint err = codel_change(sch, opt);\r\nif (err)\r\nreturn err;\r\n}\r\nif (sch->limit >= 1)\r\nsch->flags |= TCQ_F_CAN_BYPASS;\r\nelse\r\nsch->flags &= ~TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstatic int codel_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct codel_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *opts;\r\nopts = nla_nest_start(skb, TCA_OPTIONS);\r\nif (opts == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put_u32(skb, TCA_CODEL_TARGET,\r\ncodel_time_to_us(q->params.target)) ||\r\nnla_put_u32(skb, TCA_CODEL_LIMIT,\r\nsch->limit) ||\r\nnla_put_u32(skb, TCA_CODEL_INTERVAL,\r\ncodel_time_to_us(q->params.interval)) ||\r\nnla_put_u32(skb, TCA_CODEL_ECN,\r\nq->params.ecn))\r\ngoto nla_put_failure;\r\nif (q->params.ce_threshold != CODEL_DISABLED_THRESHOLD &&\r\nnla_put_u32(skb, TCA_CODEL_CE_THRESHOLD,\r\ncodel_time_to_us(q->params.ce_threshold)))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, opts);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, opts);\r\nreturn -1;\r\n}\r\nstatic int codel_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\r\n{\r\nconst struct codel_sched_data *q = qdisc_priv(sch);\r\nstruct tc_codel_xstats st = {\r\n.maxpacket = q->stats.maxpacket,\r\n.count = q->vars.count,\r\n.lastcount = q->vars.lastcount,\r\n.drop_overlimit = q->drop_overlimit,\r\n.ldelay = codel_time_to_us(q->vars.ldelay),\r\n.dropping = q->vars.dropping,\r\n.ecn_mark = q->stats.ecn_mark,\r\n.ce_mark = q->stats.ce_mark,\r\n};\r\nif (q->vars.dropping) {\r\ncodel_tdiff_t delta = q->vars.drop_next - codel_get_time();\r\nif (delta >= 0)\r\nst.drop_next = codel_time_to_us(delta);\r\nelse\r\nst.drop_next = -codel_time_to_us(-delta);\r\n}\r\nreturn gnet_stats_copy_app(d, &st, sizeof(st));\r\n}\r\nstatic void codel_reset(struct Qdisc *sch)\r\n{\r\nstruct codel_sched_data *q = qdisc_priv(sch);\r\nqdisc_reset_queue(sch);\r\ncodel_vars_init(&q->vars);\r\n}\r\nstatic int __init codel_module_init(void)\r\n{\r\nreturn register_qdisc(&codel_qdisc_ops);\r\n}\r\nstatic void __exit codel_module_exit(void)\r\n{\r\nunregister_qdisc(&codel_qdisc_ops);\r\n}
