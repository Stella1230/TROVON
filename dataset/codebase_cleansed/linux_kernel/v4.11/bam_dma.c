static inline struct bam_chan *to_bam_chan(struct dma_chan *common)\r\n{\r\nreturn container_of(common, struct bam_chan, vc.chan);\r\n}\r\nstatic inline void __iomem *bam_addr(struct bam_device *bdev, u32 pipe,\r\nenum bam_reg reg)\r\n{\r\nconst struct reg_offset_data r = bdev->layout[reg];\r\nreturn bdev->regs + r.base_offset +\r\nr.pipe_mult * pipe +\r\nr.evnt_mult * pipe +\r\nr.ee_mult * bdev->ee;\r\n}\r\nstatic void bam_reset_channel(struct bam_chan *bchan)\r\n{\r\nstruct bam_device *bdev = bchan->bdev;\r\nlockdep_assert_held(&bchan->vc.lock);\r\nwritel_relaxed(1, bam_addr(bdev, bchan->id, BAM_P_RST));\r\nwritel_relaxed(0, bam_addr(bdev, bchan->id, BAM_P_RST));\r\nwmb();\r\nbchan->initialized = 0;\r\n}\r\nstatic void bam_chan_init_hw(struct bam_chan *bchan,\r\nenum dma_transfer_direction dir)\r\n{\r\nstruct bam_device *bdev = bchan->bdev;\r\nu32 val;\r\nbam_reset_channel(bchan);\r\nwritel_relaxed(ALIGN(bchan->fifo_phys, sizeof(struct bam_desc_hw)),\r\nbam_addr(bdev, bchan->id, BAM_P_DESC_FIFO_ADDR));\r\nwritel_relaxed(BAM_FIFO_SIZE,\r\nbam_addr(bdev, bchan->id, BAM_P_FIFO_SIZES));\r\nwritel_relaxed(P_DEFAULT_IRQS_EN,\r\nbam_addr(bdev, bchan->id, BAM_P_IRQ_EN));\r\nval = readl_relaxed(bam_addr(bdev, 0, BAM_IRQ_SRCS_MSK_EE));\r\nval |= BIT(bchan->id);\r\nwritel_relaxed(val, bam_addr(bdev, 0, BAM_IRQ_SRCS_MSK_EE));\r\nwmb();\r\nval = P_EN | P_SYS_MODE;\r\nif (dir == DMA_DEV_TO_MEM)\r\nval |= P_DIRECTION;\r\nwritel_relaxed(val, bam_addr(bdev, bchan->id, BAM_P_CTRL));\r\nbchan->initialized = 1;\r\nbchan->head = 0;\r\nbchan->tail = 0;\r\n}\r\nstatic int bam_alloc_chan(struct dma_chan *chan)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nstruct bam_device *bdev = bchan->bdev;\r\nif (bchan->fifo_virt)\r\nreturn 0;\r\nbchan->fifo_virt = dma_alloc_wc(bdev->dev, BAM_DESC_FIFO_SIZE,\r\n&bchan->fifo_phys, GFP_KERNEL);\r\nif (!bchan->fifo_virt) {\r\ndev_err(bdev->dev, "Failed to allocate desc fifo\n");\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void bam_free_chan(struct dma_chan *chan)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nstruct bam_device *bdev = bchan->bdev;\r\nu32 val;\r\nunsigned long flags;\r\nint ret;\r\nret = pm_runtime_get_sync(bdev->dev);\r\nif (ret < 0)\r\nreturn;\r\nvchan_free_chan_resources(to_virt_chan(chan));\r\nif (bchan->curr_txd) {\r\ndev_err(bchan->bdev->dev, "Cannot free busy channel\n");\r\ngoto err;\r\n}\r\nspin_lock_irqsave(&bchan->vc.lock, flags);\r\nbam_reset_channel(bchan);\r\nspin_unlock_irqrestore(&bchan->vc.lock, flags);\r\ndma_free_wc(bdev->dev, BAM_DESC_FIFO_SIZE, bchan->fifo_virt,\r\nbchan->fifo_phys);\r\nbchan->fifo_virt = NULL;\r\nval = readl_relaxed(bam_addr(bdev, 0, BAM_IRQ_SRCS_MSK_EE));\r\nval &= ~BIT(bchan->id);\r\nwritel_relaxed(val, bam_addr(bdev, 0, BAM_IRQ_SRCS_MSK_EE));\r\nwritel_relaxed(0, bam_addr(bdev, bchan->id, BAM_P_IRQ_EN));\r\nerr:\r\npm_runtime_mark_last_busy(bdev->dev);\r\npm_runtime_put_autosuspend(bdev->dev);\r\n}\r\nstatic int bam_slave_config(struct dma_chan *chan,\r\nstruct dma_slave_config *cfg)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nunsigned long flag;\r\nspin_lock_irqsave(&bchan->vc.lock, flag);\r\nmemcpy(&bchan->slave, cfg, sizeof(*cfg));\r\nbchan->reconfigure = 1;\r\nspin_unlock_irqrestore(&bchan->vc.lock, flag);\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *bam_prep_slave_sg(struct dma_chan *chan,\r\nstruct scatterlist *sgl, unsigned int sg_len,\r\nenum dma_transfer_direction direction, unsigned long flags,\r\nvoid *context)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nstruct bam_device *bdev = bchan->bdev;\r\nstruct bam_async_desc *async_desc;\r\nstruct scatterlist *sg;\r\nu32 i;\r\nstruct bam_desc_hw *desc;\r\nunsigned int num_alloc = 0;\r\nif (!is_slave_direction(direction)) {\r\ndev_err(bdev->dev, "invalid dma direction\n");\r\nreturn NULL;\r\n}\r\nfor_each_sg(sgl, sg, sg_len, i)\r\nnum_alloc += DIV_ROUND_UP(sg_dma_len(sg), BAM_FIFO_SIZE);\r\nasync_desc = kzalloc(sizeof(*async_desc) +\r\n(num_alloc * sizeof(struct bam_desc_hw)), GFP_NOWAIT);\r\nif (!async_desc)\r\ngoto err_out;\r\nif (flags & DMA_PREP_FENCE)\r\nasync_desc->flags |= DESC_FLAG_NWD;\r\nif (flags & DMA_PREP_INTERRUPT)\r\nasync_desc->flags |= DESC_FLAG_EOT;\r\nelse\r\nasync_desc->flags |= DESC_FLAG_INT;\r\nasync_desc->num_desc = num_alloc;\r\nasync_desc->curr_desc = async_desc->desc;\r\nasync_desc->dir = direction;\r\ndesc = async_desc->desc;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nunsigned int remainder = sg_dma_len(sg);\r\nunsigned int curr_offset = 0;\r\ndo {\r\ndesc->addr = cpu_to_le32(sg_dma_address(sg) +\r\ncurr_offset);\r\nif (remainder > BAM_FIFO_SIZE) {\r\ndesc->size = cpu_to_le16(BAM_FIFO_SIZE);\r\nremainder -= BAM_FIFO_SIZE;\r\ncurr_offset += BAM_FIFO_SIZE;\r\n} else {\r\ndesc->size = cpu_to_le16(remainder);\r\nremainder = 0;\r\n}\r\nasync_desc->length += desc->size;\r\ndesc++;\r\n} while (remainder > 0);\r\n}\r\nreturn vchan_tx_prep(&bchan->vc, &async_desc->vd, flags);\r\nerr_out:\r\nkfree(async_desc);\r\nreturn NULL;\r\n}\r\nstatic int bam_dma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nunsigned long flag;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&bchan->vc.lock, flag);\r\nif (bchan->curr_txd) {\r\nlist_add(&bchan->curr_txd->vd.node, &bchan->vc.desc_issued);\r\nbchan->curr_txd = NULL;\r\n}\r\nvchan_get_all_descriptors(&bchan->vc, &head);\r\nspin_unlock_irqrestore(&bchan->vc.lock, flag);\r\nvchan_dma_desc_free_list(&bchan->vc, &head);\r\nreturn 0;\r\n}\r\nstatic int bam_pause(struct dma_chan *chan)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nstruct bam_device *bdev = bchan->bdev;\r\nunsigned long flag;\r\nint ret;\r\nret = pm_runtime_get_sync(bdev->dev);\r\nif (ret < 0)\r\nreturn ret;\r\nspin_lock_irqsave(&bchan->vc.lock, flag);\r\nwritel_relaxed(1, bam_addr(bdev, bchan->id, BAM_P_HALT));\r\nbchan->paused = 1;\r\nspin_unlock_irqrestore(&bchan->vc.lock, flag);\r\npm_runtime_mark_last_busy(bdev->dev);\r\npm_runtime_put_autosuspend(bdev->dev);\r\nreturn 0;\r\n}\r\nstatic int bam_resume(struct dma_chan *chan)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nstruct bam_device *bdev = bchan->bdev;\r\nunsigned long flag;\r\nint ret;\r\nret = pm_runtime_get_sync(bdev->dev);\r\nif (ret < 0)\r\nreturn ret;\r\nspin_lock_irqsave(&bchan->vc.lock, flag);\r\nwritel_relaxed(0, bam_addr(bdev, bchan->id, BAM_P_HALT));\r\nbchan->paused = 0;\r\nspin_unlock_irqrestore(&bchan->vc.lock, flag);\r\npm_runtime_mark_last_busy(bdev->dev);\r\npm_runtime_put_autosuspend(bdev->dev);\r\nreturn 0;\r\n}\r\nstatic u32 process_channel_irqs(struct bam_device *bdev)\r\n{\r\nu32 i, srcs, pipe_stts;\r\nunsigned long flags;\r\nstruct bam_async_desc *async_desc;\r\nsrcs = readl_relaxed(bam_addr(bdev, 0, BAM_IRQ_SRCS_EE));\r\nif (!(srcs & P_IRQ))\r\nreturn srcs;\r\nfor (i = 0; i < bdev->num_channels; i++) {\r\nstruct bam_chan *bchan = &bdev->channels[i];\r\nif (!(srcs & BIT(i)))\r\ncontinue;\r\npipe_stts = readl_relaxed(bam_addr(bdev, i, BAM_P_IRQ_STTS));\r\nwritel_relaxed(pipe_stts, bam_addr(bdev, i, BAM_P_IRQ_CLR));\r\nspin_lock_irqsave(&bchan->vc.lock, flags);\r\nasync_desc = bchan->curr_txd;\r\nif (async_desc) {\r\nasync_desc->num_desc -= async_desc->xfer_len;\r\nasync_desc->curr_desc += async_desc->xfer_len;\r\nbchan->curr_txd = NULL;\r\nbchan->head += async_desc->xfer_len;\r\nbchan->head %= MAX_DESCRIPTORS;\r\nif (!async_desc->num_desc)\r\nvchan_cookie_complete(&async_desc->vd);\r\nelse\r\nlist_add(&async_desc->vd.node,\r\n&bchan->vc.desc_issued);\r\n}\r\nspin_unlock_irqrestore(&bchan->vc.lock, flags);\r\n}\r\nreturn srcs;\r\n}\r\nstatic irqreturn_t bam_dma_irq(int irq, void *data)\r\n{\r\nstruct bam_device *bdev = data;\r\nu32 clr_mask = 0, srcs = 0;\r\nint ret;\r\nsrcs |= process_channel_irqs(bdev);\r\nif (srcs & P_IRQ)\r\ntasklet_schedule(&bdev->task);\r\nret = pm_runtime_get_sync(bdev->dev);\r\nif (ret < 0)\r\nreturn ret;\r\nif (srcs & BAM_IRQ) {\r\nclr_mask = readl_relaxed(bam_addr(bdev, 0, BAM_IRQ_STTS));\r\nmb();\r\nwritel_relaxed(clr_mask, bam_addr(bdev, 0, BAM_IRQ_CLR));\r\n}\r\npm_runtime_mark_last_busy(bdev->dev);\r\npm_runtime_put_autosuspend(bdev->dev);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic enum dma_status bam_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nstruct virt_dma_desc *vd;\r\nint ret;\r\nsize_t residue = 0;\r\nunsigned int i;\r\nunsigned long flags;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nif (!txstate)\r\nreturn bchan->paused ? DMA_PAUSED : ret;\r\nspin_lock_irqsave(&bchan->vc.lock, flags);\r\nvd = vchan_find_desc(&bchan->vc, cookie);\r\nif (vd)\r\nresidue = container_of(vd, struct bam_async_desc, vd)->length;\r\nelse if (bchan->curr_txd && bchan->curr_txd->vd.tx.cookie == cookie)\r\nfor (i = 0; i < bchan->curr_txd->num_desc; i++)\r\nresidue += bchan->curr_txd->curr_desc[i].size;\r\nspin_unlock_irqrestore(&bchan->vc.lock, flags);\r\ndma_set_residue(txstate, residue);\r\nif (ret == DMA_IN_PROGRESS && bchan->paused)\r\nret = DMA_PAUSED;\r\nreturn ret;\r\n}\r\nstatic void bam_apply_new_config(struct bam_chan *bchan,\r\nenum dma_transfer_direction dir)\r\n{\r\nstruct bam_device *bdev = bchan->bdev;\r\nu32 maxburst;\r\nif (dir == DMA_DEV_TO_MEM)\r\nmaxburst = bchan->slave.src_maxburst;\r\nelse\r\nmaxburst = bchan->slave.dst_maxburst;\r\nwritel_relaxed(maxburst, bam_addr(bdev, 0, BAM_DESC_CNT_TRSHLD));\r\nbchan->reconfigure = 0;\r\n}\r\nstatic void bam_start_dma(struct bam_chan *bchan)\r\n{\r\nstruct virt_dma_desc *vd = vchan_next_desc(&bchan->vc);\r\nstruct bam_device *bdev = bchan->bdev;\r\nstruct bam_async_desc *async_desc;\r\nstruct bam_desc_hw *desc;\r\nstruct bam_desc_hw *fifo = PTR_ALIGN(bchan->fifo_virt,\r\nsizeof(struct bam_desc_hw));\r\nint ret;\r\nlockdep_assert_held(&bchan->vc.lock);\r\nif (!vd)\r\nreturn;\r\nlist_del(&vd->node);\r\nasync_desc = container_of(vd, struct bam_async_desc, vd);\r\nbchan->curr_txd = async_desc;\r\nret = pm_runtime_get_sync(bdev->dev);\r\nif (ret < 0)\r\nreturn;\r\nif (!bchan->initialized)\r\nbam_chan_init_hw(bchan, async_desc->dir);\r\nif (bchan->reconfigure)\r\nbam_apply_new_config(bchan, async_desc->dir);\r\ndesc = bchan->curr_txd->curr_desc;\r\nif (async_desc->num_desc > MAX_DESCRIPTORS)\r\nasync_desc->xfer_len = MAX_DESCRIPTORS;\r\nelse\r\nasync_desc->xfer_len = async_desc->num_desc;\r\nif (async_desc->num_desc == async_desc->xfer_len)\r\ndesc[async_desc->xfer_len - 1].flags =\r\ncpu_to_le16(async_desc->flags);\r\nelse\r\ndesc[async_desc->xfer_len - 1].flags |=\r\ncpu_to_le16(DESC_FLAG_INT);\r\nif (bchan->tail + async_desc->xfer_len > MAX_DESCRIPTORS) {\r\nu32 partial = MAX_DESCRIPTORS - bchan->tail;\r\nmemcpy(&fifo[bchan->tail], desc,\r\npartial * sizeof(struct bam_desc_hw));\r\nmemcpy(fifo, &desc[partial], (async_desc->xfer_len - partial) *\r\nsizeof(struct bam_desc_hw));\r\n} else {\r\nmemcpy(&fifo[bchan->tail], desc,\r\nasync_desc->xfer_len * sizeof(struct bam_desc_hw));\r\n}\r\nbchan->tail += async_desc->xfer_len;\r\nbchan->tail %= MAX_DESCRIPTORS;\r\nwmb();\r\nwritel_relaxed(bchan->tail * sizeof(struct bam_desc_hw),\r\nbam_addr(bdev, bchan->id, BAM_P_EVNT_REG));\r\npm_runtime_mark_last_busy(bdev->dev);\r\npm_runtime_put_autosuspend(bdev->dev);\r\n}\r\nstatic void dma_tasklet(unsigned long data)\r\n{\r\nstruct bam_device *bdev = (struct bam_device *)data;\r\nstruct bam_chan *bchan;\r\nunsigned long flags;\r\nunsigned int i;\r\nfor (i = 0; i < bdev->num_channels; i++) {\r\nbchan = &bdev->channels[i];\r\nspin_lock_irqsave(&bchan->vc.lock, flags);\r\nif (!list_empty(&bchan->vc.desc_issued) && !bchan->curr_txd)\r\nbam_start_dma(bchan);\r\nspin_unlock_irqrestore(&bchan->vc.lock, flags);\r\n}\r\n}\r\nstatic void bam_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct bam_chan *bchan = to_bam_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&bchan->vc.lock, flags);\r\nif (vchan_issue_pending(&bchan->vc) && !bchan->curr_txd)\r\nbam_start_dma(bchan);\r\nspin_unlock_irqrestore(&bchan->vc.lock, flags);\r\n}\r\nstatic void bam_dma_free_desc(struct virt_dma_desc *vd)\r\n{\r\nstruct bam_async_desc *async_desc = container_of(vd,\r\nstruct bam_async_desc, vd);\r\nkfree(async_desc);\r\n}\r\nstatic struct dma_chan *bam_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *of)\r\n{\r\nstruct bam_device *bdev = container_of(of->of_dma_data,\r\nstruct bam_device, common);\r\nunsigned int request;\r\nif (dma_spec->args_count != 1)\r\nreturn NULL;\r\nrequest = dma_spec->args[0];\r\nif (request >= bdev->num_channels)\r\nreturn NULL;\r\nreturn dma_get_slave_channel(&(bdev->channels[request].vc.chan));\r\n}\r\nstatic int bam_init(struct bam_device *bdev)\r\n{\r\nu32 val;\r\nval = readl_relaxed(bam_addr(bdev, 0, BAM_REVISION)) >> NUM_EES_SHIFT;\r\nval &= NUM_EES_MASK;\r\nif (bdev->ee >= val)\r\nreturn -EINVAL;\r\nval = readl_relaxed(bam_addr(bdev, 0, BAM_NUM_PIPES));\r\nbdev->num_channels = val & BAM_NUM_PIPES_MASK;\r\nif (bdev->controlled_remotely)\r\nreturn 0;\r\nval = readl_relaxed(bam_addr(bdev, 0, BAM_CTRL));\r\nval |= BAM_SW_RST;\r\nwritel_relaxed(val, bam_addr(bdev, 0, BAM_CTRL));\r\nval &= ~BAM_SW_RST;\r\nwritel_relaxed(val, bam_addr(bdev, 0, BAM_CTRL));\r\nwmb();\r\nval |= BAM_EN;\r\nwritel_relaxed(val, bam_addr(bdev, 0, BAM_CTRL));\r\nwritel_relaxed(DEFAULT_CNT_THRSHLD,\r\nbam_addr(bdev, 0, BAM_DESC_CNT_TRSHLD));\r\nwritel_relaxed(BAM_CNFG_BITS_DEFAULT, bam_addr(bdev, 0, BAM_CNFG_BITS));\r\nwritel_relaxed(BAM_ERROR_EN | BAM_HRESP_ERR_EN,\r\nbam_addr(bdev, 0, BAM_IRQ_EN));\r\nwritel_relaxed(BAM_IRQ_MSK, bam_addr(bdev, 0, BAM_IRQ_SRCS_MSK_EE));\r\nreturn 0;\r\n}\r\nstatic void bam_channel_init(struct bam_device *bdev, struct bam_chan *bchan,\r\nu32 index)\r\n{\r\nbchan->id = index;\r\nbchan->bdev = bdev;\r\nvchan_init(&bchan->vc, &bdev->common);\r\nbchan->vc.desc_free = bam_dma_free_desc;\r\n}\r\nstatic int bam_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct bam_device *bdev;\r\nconst struct of_device_id *match;\r\nstruct resource *iores;\r\nint ret, i;\r\nbdev = devm_kzalloc(&pdev->dev, sizeof(*bdev), GFP_KERNEL);\r\nif (!bdev)\r\nreturn -ENOMEM;\r\nbdev->dev = &pdev->dev;\r\nmatch = of_match_node(bam_of_match, pdev->dev.of_node);\r\nif (!match) {\r\ndev_err(&pdev->dev, "Unsupported BAM module\n");\r\nreturn -ENODEV;\r\n}\r\nbdev->layout = match->data;\r\niores = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nbdev->regs = devm_ioremap_resource(&pdev->dev, iores);\r\nif (IS_ERR(bdev->regs))\r\nreturn PTR_ERR(bdev->regs);\r\nbdev->irq = platform_get_irq(pdev, 0);\r\nif (bdev->irq < 0)\r\nreturn bdev->irq;\r\nret = of_property_read_u32(pdev->dev.of_node, "qcom,ee", &bdev->ee);\r\nif (ret) {\r\ndev_err(bdev->dev, "Execution environment unspecified\n");\r\nreturn ret;\r\n}\r\nbdev->controlled_remotely = of_property_read_bool(pdev->dev.of_node,\r\n"qcom,controlled-remotely");\r\nbdev->bamclk = devm_clk_get(bdev->dev, "bam_clk");\r\nif (IS_ERR(bdev->bamclk))\r\nreturn PTR_ERR(bdev->bamclk);\r\nret = clk_prepare_enable(bdev->bamclk);\r\nif (ret) {\r\ndev_err(bdev->dev, "failed to prepare/enable clock\n");\r\nreturn ret;\r\n}\r\nret = bam_init(bdev);\r\nif (ret)\r\ngoto err_disable_clk;\r\ntasklet_init(&bdev->task, dma_tasklet, (unsigned long)bdev);\r\nbdev->channels = devm_kcalloc(bdev->dev, bdev->num_channels,\r\nsizeof(*bdev->channels), GFP_KERNEL);\r\nif (!bdev->channels) {\r\nret = -ENOMEM;\r\ngoto err_tasklet_kill;\r\n}\r\nINIT_LIST_HEAD(&bdev->common.channels);\r\nfor (i = 0; i < bdev->num_channels; i++)\r\nbam_channel_init(bdev, &bdev->channels[i], i);\r\nret = devm_request_irq(bdev->dev, bdev->irq, bam_dma_irq,\r\nIRQF_TRIGGER_HIGH, "bam_dma", bdev);\r\nif (ret)\r\ngoto err_bam_channel_exit;\r\nbdev->common.dev = bdev->dev;\r\nbdev->common.dev->dma_parms = &bdev->dma_parms;\r\nret = dma_set_max_seg_size(bdev->common.dev, BAM_FIFO_SIZE);\r\nif (ret) {\r\ndev_err(bdev->dev, "cannot set maximum segment size\n");\r\ngoto err_bam_channel_exit;\r\n}\r\nplatform_set_drvdata(pdev, bdev);\r\ndma_cap_zero(bdev->common.cap_mask);\r\ndma_cap_set(DMA_SLAVE, bdev->common.cap_mask);\r\nbdev->common.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nbdev->common.residue_granularity = DMA_RESIDUE_GRANULARITY_SEGMENT;\r\nbdev->common.src_addr_widths = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\nbdev->common.dst_addr_widths = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\nbdev->common.device_alloc_chan_resources = bam_alloc_chan;\r\nbdev->common.device_free_chan_resources = bam_free_chan;\r\nbdev->common.device_prep_slave_sg = bam_prep_slave_sg;\r\nbdev->common.device_config = bam_slave_config;\r\nbdev->common.device_pause = bam_pause;\r\nbdev->common.device_resume = bam_resume;\r\nbdev->common.device_terminate_all = bam_dma_terminate_all;\r\nbdev->common.device_issue_pending = bam_issue_pending;\r\nbdev->common.device_tx_status = bam_tx_status;\r\nbdev->common.dev = bdev->dev;\r\nret = dma_async_device_register(&bdev->common);\r\nif (ret) {\r\ndev_err(bdev->dev, "failed to register dma async device\n");\r\ngoto err_bam_channel_exit;\r\n}\r\nret = of_dma_controller_register(pdev->dev.of_node, bam_dma_xlate,\r\n&bdev->common);\r\nif (ret)\r\ngoto err_unregister_dma;\r\npm_runtime_irq_safe(&pdev->dev);\r\npm_runtime_set_autosuspend_delay(&pdev->dev, BAM_DMA_AUTOSUSPEND_DELAY);\r\npm_runtime_use_autosuspend(&pdev->dev);\r\npm_runtime_mark_last_busy(&pdev->dev);\r\npm_runtime_set_active(&pdev->dev);\r\npm_runtime_enable(&pdev->dev);\r\nreturn 0;\r\nerr_unregister_dma:\r\ndma_async_device_unregister(&bdev->common);\r\nerr_bam_channel_exit:\r\nfor (i = 0; i < bdev->num_channels; i++)\r\ntasklet_kill(&bdev->channels[i].vc.task);\r\nerr_tasklet_kill:\r\ntasklet_kill(&bdev->task);\r\nerr_disable_clk:\r\nclk_disable_unprepare(bdev->bamclk);\r\nreturn ret;\r\n}\r\nstatic int bam_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct bam_device *bdev = platform_get_drvdata(pdev);\r\nu32 i;\r\npm_runtime_force_suspend(&pdev->dev);\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&bdev->common);\r\nwritel_relaxed(0, bam_addr(bdev, 0, BAM_IRQ_SRCS_MSK_EE));\r\ndevm_free_irq(bdev->dev, bdev->irq, bdev);\r\nfor (i = 0; i < bdev->num_channels; i++) {\r\nbam_dma_terminate_all(&bdev->channels[i].vc.chan);\r\ntasklet_kill(&bdev->channels[i].vc.task);\r\nif (!bdev->channels[i].fifo_virt)\r\ncontinue;\r\ndma_free_wc(bdev->dev, BAM_DESC_FIFO_SIZE,\r\nbdev->channels[i].fifo_virt,\r\nbdev->channels[i].fifo_phys);\r\n}\r\ntasklet_kill(&bdev->task);\r\nclk_disable_unprepare(bdev->bamclk);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused bam_dma_runtime_suspend(struct device *dev)\r\n{\r\nstruct bam_device *bdev = dev_get_drvdata(dev);\r\nclk_disable(bdev->bamclk);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused bam_dma_runtime_resume(struct device *dev)\r\n{\r\nstruct bam_device *bdev = dev_get_drvdata(dev);\r\nint ret;\r\nret = clk_enable(bdev->bamclk);\r\nif (ret < 0) {\r\ndev_err(dev, "clk_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused bam_dma_suspend(struct device *dev)\r\n{\r\nstruct bam_device *bdev = dev_get_drvdata(dev);\r\npm_runtime_force_suspend(dev);\r\nclk_unprepare(bdev->bamclk);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused bam_dma_resume(struct device *dev)\r\n{\r\nstruct bam_device *bdev = dev_get_drvdata(dev);\r\nint ret;\r\nret = clk_prepare(bdev->bamclk);\r\nif (ret)\r\nreturn ret;\r\npm_runtime_force_resume(dev);\r\nreturn 0;\r\n}
