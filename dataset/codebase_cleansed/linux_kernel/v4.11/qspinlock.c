static inline __pure u32 encode_tail(int cpu, int idx)\r\n{\r\nu32 tail;\r\n#ifdef CONFIG_DEBUG_SPINLOCK\r\nBUG_ON(idx > 3);\r\n#endif\r\ntail = (cpu + 1) << _Q_TAIL_CPU_OFFSET;\r\ntail |= idx << _Q_TAIL_IDX_OFFSET;\r\nreturn tail;\r\n}\r\nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\r\n{\r\nstruct __qspinlock *l = (void *)lock;\r\nWRITE_ONCE(l->locked_pending, _Q_LOCKED_VAL);\r\n}\r\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\r\n{\r\nstruct __qspinlock *l = (void *)lock;\r\nreturn (u32)xchg_release(&l->tail,\r\ntail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;\r\n}\r\nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\r\n{\r\natomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);\r\n}\r\nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\r\n{\r\nu32 old, new, val = atomic_read(&lock->val);\r\nfor (;;) {\r\nnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\r\nold = atomic_cmpxchg_release(&lock->val, val, new);\r\nif (old == val)\r\nbreak;\r\nval = old;\r\n}\r\nreturn old;\r\n}\r\nstatic __always_inline void set_locked(struct qspinlock *lock)\r\n{\r\nstruct __qspinlock *l = (void *)lock;\r\nWRITE_ONCE(l->locked, _Q_LOCKED_VAL);\r\n}\r\nstatic __always_inline void __pv_init_node(struct mcs_spinlock *node) { }\r\nstatic __always_inline void __pv_wait_node(struct mcs_spinlock *node,\r\nstruct mcs_spinlock *prev) { }\r\nstatic __always_inline void __pv_kick_node(struct qspinlock *lock,\r\nstruct mcs_spinlock *node) { }\r\nstatic __always_inline u32 __pv_wait_head_or_lock(struct qspinlock *lock,\r\nstruct mcs_spinlock *node)\r\n{ return 0; }\r\nvoid queued_spin_unlock_wait(struct qspinlock *lock)\r\n{\r\nu32 val;\r\nfor (;;) {\r\nval = atomic_read(&lock->val);\r\nif (!val)\r\ngoto done;\r\nif (val & _Q_LOCKED_MASK)\r\nbreak;\r\ncpu_relax();\r\n}\r\nwhile (atomic_read(&lock->val) & _Q_LOCKED_MASK)\r\ncpu_relax();\r\ndone:\r\nsmp_acquire__after_ctrl_dep();\r\n}\r\nvoid queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)\r\n{\r\nstruct mcs_spinlock *prev, *next, *node;\r\nu32 new, old, tail;\r\nint idx;\r\nBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));\r\nif (pv_enabled())\r\ngoto queue;\r\nif (virt_spin_lock(lock))\r\nreturn;\r\nif (val == _Q_PENDING_VAL) {\r\nwhile ((val = atomic_read(&lock->val)) == _Q_PENDING_VAL)\r\ncpu_relax();\r\n}\r\nfor (;;) {\r\nif (val & ~_Q_LOCKED_MASK)\r\ngoto queue;\r\nnew = _Q_LOCKED_VAL;\r\nif (val == new)\r\nnew |= _Q_PENDING_VAL;\r\nold = atomic_cmpxchg_acquire(&lock->val, val, new);\r\nif (old == val)\r\nbreak;\r\nval = old;\r\n}\r\nif (new == _Q_LOCKED_VAL)\r\nreturn;\r\nsmp_cond_load_acquire(&lock->val.counter, !(VAL & _Q_LOCKED_MASK));\r\nclear_pending_set_locked(lock);\r\nreturn;\r\nqueue:\r\nnode = this_cpu_ptr(&mcs_nodes[0]);\r\nidx = node->count++;\r\ntail = encode_tail(smp_processor_id(), idx);\r\nnode += idx;\r\nnode->locked = 0;\r\nnode->next = NULL;\r\npv_init_node(node);\r\nif (queued_spin_trylock(lock))\r\ngoto release;\r\nold = xchg_tail(lock, tail);\r\nnext = NULL;\r\nif (old & _Q_TAIL_MASK) {\r\nprev = decode_tail(old);\r\nsmp_read_barrier_depends();\r\nWRITE_ONCE(prev->next, node);\r\npv_wait_node(node, prev);\r\narch_mcs_spin_lock_contended(&node->locked);\r\nnext = READ_ONCE(node->next);\r\nif (next)\r\nprefetchw(next);\r\n}\r\nif ((val = pv_wait_head_or_lock(lock, node)))\r\ngoto locked;\r\nval = smp_cond_load_acquire(&lock->val.counter, !(VAL & _Q_LOCKED_PENDING_MASK));\r\nlocked:\r\nfor (;;) {\r\nif ((val & _Q_TAIL_MASK) != tail) {\r\nset_locked(lock);\r\nbreak;\r\n}\r\nold = atomic_cmpxchg_relaxed(&lock->val, val, _Q_LOCKED_VAL);\r\nif (old == val)\r\ngoto release;\r\nval = old;\r\n}\r\nif (!next) {\r\nwhile (!(next = READ_ONCE(node->next)))\r\ncpu_relax();\r\n}\r\narch_mcs_spin_unlock_contended(&next->locked);\r\npv_kick_node(lock, next);\r\nrelease:\r\n__this_cpu_dec(mcs_nodes[0].count);\r\n}
