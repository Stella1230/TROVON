static u32 hw_index_to_key(unsigned long ind)\r\n{\r\nreturn (u32)(ind >> 24) | (ind << 8);\r\n}\r\nunsigned long key_to_hw_index(u32 key)\r\n{\r\nreturn (key << 24) | (key >> 8);\r\n}\r\nstatic int hns_roce_sw2hw_mpt(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_cmd_mailbox *mailbox,\r\nunsigned long mpt_index)\r\n{\r\nreturn hns_roce_cmd_mbox(hr_dev, mailbox->dma, 0, mpt_index, 0,\r\nHNS_ROCE_CMD_SW2HW_MPT,\r\nHNS_ROCE_CMD_TIMEOUT_MSECS);\r\n}\r\nint hns_roce_hw2sw_mpt(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_cmd_mailbox *mailbox,\r\nunsigned long mpt_index)\r\n{\r\nreturn hns_roce_cmd_mbox(hr_dev, 0, mailbox ? mailbox->dma : 0,\r\nmpt_index, !mailbox, HNS_ROCE_CMD_HW2SW_MPT,\r\nHNS_ROCE_CMD_TIMEOUT_MSECS);\r\n}\r\nstatic int hns_roce_buddy_alloc(struct hns_roce_buddy *buddy, int order,\r\nunsigned long *seg)\r\n{\r\nint o;\r\nu32 m;\r\nspin_lock(&buddy->lock);\r\nfor (o = order; o <= buddy->max_order; ++o) {\r\nif (buddy->num_free[o]) {\r\nm = 1 << (buddy->max_order - o);\r\n*seg = find_first_bit(buddy->bits[o], m);\r\nif (*seg < m)\r\ngoto found;\r\n}\r\n}\r\nspin_unlock(&buddy->lock);\r\nreturn -1;\r\nfound:\r\nclear_bit(*seg, buddy->bits[o]);\r\n--buddy->num_free[o];\r\nwhile (o > order) {\r\n--o;\r\n*seg <<= 1;\r\nset_bit(*seg ^ 1, buddy->bits[o]);\r\n++buddy->num_free[o];\r\n}\r\nspin_unlock(&buddy->lock);\r\n*seg <<= order;\r\nreturn 0;\r\n}\r\nstatic void hns_roce_buddy_free(struct hns_roce_buddy *buddy, unsigned long seg,\r\nint order)\r\n{\r\nseg >>= order;\r\nspin_lock(&buddy->lock);\r\nwhile (test_bit(seg ^ 1, buddy->bits[order])) {\r\nclear_bit(seg ^ 1, buddy->bits[order]);\r\n--buddy->num_free[order];\r\nseg >>= 1;\r\n++order;\r\n}\r\nset_bit(seg, buddy->bits[order]);\r\n++buddy->num_free[order];\r\nspin_unlock(&buddy->lock);\r\n}\r\nstatic int hns_roce_buddy_init(struct hns_roce_buddy *buddy, int max_order)\r\n{\r\nint i, s;\r\nbuddy->max_order = max_order;\r\nspin_lock_init(&buddy->lock);\r\nbuddy->bits = kzalloc((buddy->max_order + 1) * sizeof(long *),\r\nGFP_KERNEL);\r\nbuddy->num_free = kzalloc((buddy->max_order + 1) * sizeof(int *),\r\nGFP_KERNEL);\r\nif (!buddy->bits || !buddy->num_free)\r\ngoto err_out;\r\nfor (i = 0; i <= buddy->max_order; ++i) {\r\ns = BITS_TO_LONGS(1 << (buddy->max_order - i));\r\nbuddy->bits[i] = kcalloc(s, sizeof(long), GFP_KERNEL |\r\n__GFP_NOWARN);\r\nif (!buddy->bits[i]) {\r\nbuddy->bits[i] = vzalloc(s * sizeof(long));\r\nif (!buddy->bits[i])\r\ngoto err_out_free;\r\n}\r\n}\r\nset_bit(0, buddy->bits[buddy->max_order]);\r\nbuddy->num_free[buddy->max_order] = 1;\r\nreturn 0;\r\nerr_out_free:\r\nfor (i = 0; i <= buddy->max_order; ++i)\r\nkvfree(buddy->bits[i]);\r\nerr_out:\r\nkfree(buddy->bits);\r\nkfree(buddy->num_free);\r\nreturn -ENOMEM;\r\n}\r\nstatic void hns_roce_buddy_cleanup(struct hns_roce_buddy *buddy)\r\n{\r\nint i;\r\nfor (i = 0; i <= buddy->max_order; ++i)\r\nkvfree(buddy->bits[i]);\r\nkfree(buddy->bits);\r\nkfree(buddy->num_free);\r\n}\r\nstatic int hns_roce_alloc_mtt_range(struct hns_roce_dev *hr_dev, int order,\r\nunsigned long *seg)\r\n{\r\nstruct hns_roce_mr_table *mr_table = &hr_dev->mr_table;\r\nint ret = 0;\r\nret = hns_roce_buddy_alloc(&mr_table->mtt_buddy, order, seg);\r\nif (ret == -1)\r\nreturn -1;\r\nif (hns_roce_table_get_range(hr_dev, &mr_table->mtt_table, *seg,\r\n*seg + (1 << order) - 1)) {\r\nhns_roce_buddy_free(&mr_table->mtt_buddy, *seg, order);\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nint hns_roce_mtt_init(struct hns_roce_dev *hr_dev, int npages, int page_shift,\r\nstruct hns_roce_mtt *mtt)\r\n{\r\nint ret = 0;\r\nint i;\r\nif (!npages) {\r\nmtt->order = -1;\r\nmtt->page_shift = HNS_ROCE_HEM_PAGE_SHIFT;\r\nreturn 0;\r\n}\r\nmtt->page_shift = page_shift;\r\nfor (mtt->order = 0, i = HNS_ROCE_MTT_ENTRY_PER_SEG; i < npages;\r\ni <<= 1)\r\n++mtt->order;\r\nret = hns_roce_alloc_mtt_range(hr_dev, mtt->order, &mtt->first_seg);\r\nif (ret == -1)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid hns_roce_mtt_cleanup(struct hns_roce_dev *hr_dev, struct hns_roce_mtt *mtt)\r\n{\r\nstruct hns_roce_mr_table *mr_table = &hr_dev->mr_table;\r\nif (mtt->order < 0)\r\nreturn;\r\nhns_roce_buddy_free(&mr_table->mtt_buddy, mtt->first_seg, mtt->order);\r\nhns_roce_table_put_range(hr_dev, &mr_table->mtt_table, mtt->first_seg,\r\nmtt->first_seg + (1 << mtt->order) - 1);\r\n}\r\nstatic int hns_roce_mr_alloc(struct hns_roce_dev *hr_dev, u32 pd, u64 iova,\r\nu64 size, u32 access, int npages,\r\nstruct hns_roce_mr *mr)\r\n{\r\nunsigned long index = 0;\r\nint ret = 0;\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nret = hns_roce_bitmap_alloc(&hr_dev->mr_table.mtpt_bitmap, &index);\r\nif (ret == -1)\r\nreturn -ENOMEM;\r\nmr->iova = iova;\r\nmr->size = size;\r\nmr->pd = pd;\r\nmr->access = access;\r\nmr->enabled = 0;\r\nmr->key = hw_index_to_key(index);\r\nif (size == ~0ull) {\r\nmr->type = MR_TYPE_DMA;\r\nmr->pbl_buf = NULL;\r\nmr->pbl_dma_addr = 0;\r\n} else {\r\nmr->type = MR_TYPE_MR;\r\nmr->pbl_buf = dma_alloc_coherent(dev, npages * 8,\r\n&(mr->pbl_dma_addr),\r\nGFP_KERNEL);\r\nif (!mr->pbl_buf)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void hns_roce_mr_free(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_mr *mr)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nint npages = 0;\r\nint ret;\r\nif (mr->enabled) {\r\nret = hns_roce_hw2sw_mpt(hr_dev, NULL, key_to_hw_index(mr->key)\r\n& (hr_dev->caps.num_mtpts - 1));\r\nif (ret)\r\ndev_warn(dev, "HW2SW_MPT failed (%d)\n", ret);\r\n}\r\nif (mr->size != ~0ULL) {\r\nnpages = ib_umem_page_count(mr->umem);\r\ndma_free_coherent(dev, (unsigned int)(npages * 8), mr->pbl_buf,\r\nmr->pbl_dma_addr);\r\n}\r\nhns_roce_bitmap_free(&hr_dev->mr_table.mtpt_bitmap,\r\nkey_to_hw_index(mr->key), BITMAP_NO_RR);\r\n}\r\nstatic int hns_roce_mr_enable(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_mr *mr)\r\n{\r\nint ret;\r\nunsigned long mtpt_idx = key_to_hw_index(mr->key);\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nstruct hns_roce_cmd_mailbox *mailbox;\r\nstruct hns_roce_mr_table *mr_table = &hr_dev->mr_table;\r\nret = hns_roce_table_get(hr_dev, &mr_table->mtpt_table, mtpt_idx);\r\nif (ret)\r\nreturn ret;\r\nmailbox = hns_roce_alloc_cmd_mailbox(hr_dev);\r\nif (IS_ERR(mailbox)) {\r\nret = PTR_ERR(mailbox);\r\ngoto err_table;\r\n}\r\nret = hr_dev->hw->write_mtpt(mailbox->buf, mr, mtpt_idx);\r\nif (ret) {\r\ndev_err(dev, "Write mtpt fail!\n");\r\ngoto err_page;\r\n}\r\nret = hns_roce_sw2hw_mpt(hr_dev, mailbox,\r\nmtpt_idx & (hr_dev->caps.num_mtpts - 1));\r\nif (ret) {\r\ndev_err(dev, "SW2HW_MPT failed (%d)\n", ret);\r\ngoto err_page;\r\n}\r\nmr->enabled = 1;\r\nhns_roce_free_cmd_mailbox(hr_dev, mailbox);\r\nreturn 0;\r\nerr_page:\r\nhns_roce_free_cmd_mailbox(hr_dev, mailbox);\r\nerr_table:\r\nhns_roce_table_put(hr_dev, &mr_table->mtpt_table, mtpt_idx);\r\nreturn ret;\r\n}\r\nstatic int hns_roce_write_mtt_chunk(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_mtt *mtt, u32 start_index,\r\nu32 npages, u64 *page_list)\r\n{\r\nu32 i = 0;\r\n__le64 *mtts = NULL;\r\ndma_addr_t dma_handle;\r\nu32 s = start_index * sizeof(u64);\r\nif (start_index / (PAGE_SIZE / sizeof(u64)) !=\r\n(start_index + npages - 1) / (PAGE_SIZE / sizeof(u64)))\r\nreturn -EINVAL;\r\nif (start_index & (HNS_ROCE_MTT_ENTRY_PER_SEG - 1))\r\nreturn -EINVAL;\r\nmtts = hns_roce_table_find(&hr_dev->mr_table.mtt_table,\r\nmtt->first_seg + s / hr_dev->caps.mtt_entry_sz,\r\n&dma_handle);\r\nif (!mtts)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < npages; ++i)\r\nmtts[i] = (cpu_to_le64(page_list[i])) >> PAGE_ADDR_SHIFT;\r\nreturn 0;\r\n}\r\nstatic int hns_roce_write_mtt(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_mtt *mtt, u32 start_index,\r\nu32 npages, u64 *page_list)\r\n{\r\nint chunk;\r\nint ret;\r\nif (mtt->order < 0)\r\nreturn -EINVAL;\r\nwhile (npages > 0) {\r\nchunk = min_t(int, PAGE_SIZE / sizeof(u64), npages);\r\nret = hns_roce_write_mtt_chunk(hr_dev, mtt, start_index, chunk,\r\npage_list);\r\nif (ret)\r\nreturn ret;\r\nnpages -= chunk;\r\nstart_index += chunk;\r\npage_list += chunk;\r\n}\r\nreturn 0;\r\n}\r\nint hns_roce_buf_write_mtt(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_mtt *mtt, struct hns_roce_buf *buf)\r\n{\r\nu32 i = 0;\r\nint ret = 0;\r\nu64 *page_list = NULL;\r\npage_list = kmalloc_array(buf->npages, sizeof(*page_list), GFP_KERNEL);\r\nif (!page_list)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < buf->npages; ++i) {\r\nif (buf->nbufs == 1)\r\npage_list[i] = buf->direct.map + (i << buf->page_shift);\r\nelse\r\npage_list[i] = buf->page_list[i].map;\r\n}\r\nret = hns_roce_write_mtt(hr_dev, mtt, 0, buf->npages, page_list);\r\nkfree(page_list);\r\nreturn ret;\r\n}\r\nint hns_roce_init_mr_table(struct hns_roce_dev *hr_dev)\r\n{\r\nstruct hns_roce_mr_table *mr_table = &hr_dev->mr_table;\r\nint ret = 0;\r\nret = hns_roce_bitmap_init(&mr_table->mtpt_bitmap,\r\nhr_dev->caps.num_mtpts,\r\nhr_dev->caps.num_mtpts - 1,\r\nhr_dev->caps.reserved_mrws, 0);\r\nif (ret)\r\nreturn ret;\r\nret = hns_roce_buddy_init(&mr_table->mtt_buddy,\r\nilog2(hr_dev->caps.num_mtt_segs));\r\nif (ret)\r\ngoto err_buddy;\r\nreturn 0;\r\nerr_buddy:\r\nhns_roce_bitmap_cleanup(&mr_table->mtpt_bitmap);\r\nreturn ret;\r\n}\r\nvoid hns_roce_cleanup_mr_table(struct hns_roce_dev *hr_dev)\r\n{\r\nstruct hns_roce_mr_table *mr_table = &hr_dev->mr_table;\r\nhns_roce_buddy_cleanup(&mr_table->mtt_buddy);\r\nhns_roce_bitmap_cleanup(&mr_table->mtpt_bitmap);\r\n}\r\nstruct ib_mr *hns_roce_get_dma_mr(struct ib_pd *pd, int acc)\r\n{\r\nint ret = 0;\r\nstruct hns_roce_mr *mr = NULL;\r\nmr = kmalloc(sizeof(*mr), GFP_KERNEL);\r\nif (mr == NULL)\r\nreturn ERR_PTR(-ENOMEM);\r\nret = hns_roce_mr_alloc(to_hr_dev(pd->device), to_hr_pd(pd)->pdn, 0,\r\n~0ULL, acc, 0, mr);\r\nif (ret)\r\ngoto err_free;\r\nret = hns_roce_mr_enable(to_hr_dev(pd->device), mr);\r\nif (ret)\r\ngoto err_mr;\r\nmr->ibmr.rkey = mr->ibmr.lkey = mr->key;\r\nmr->umem = NULL;\r\nreturn &mr->ibmr;\r\nerr_mr:\r\nhns_roce_mr_free(to_hr_dev(pd->device), mr);\r\nerr_free:\r\nkfree(mr);\r\nreturn ERR_PTR(ret);\r\n}\r\nint hns_roce_ib_umem_write_mtt(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_mtt *mtt, struct ib_umem *umem)\r\n{\r\nstruct scatterlist *sg;\r\nint i, k, entry;\r\nint ret = 0;\r\nu64 *pages;\r\nu32 n;\r\nint len;\r\npages = (u64 *) __get_free_page(GFP_KERNEL);\r\nif (!pages)\r\nreturn -ENOMEM;\r\ni = n = 0;\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {\r\nlen = sg_dma_len(sg) >> mtt->page_shift;\r\nfor (k = 0; k < len; ++k) {\r\npages[i++] = sg_dma_address(sg) + umem->page_size * k;\r\nif (i == PAGE_SIZE / sizeof(u64)) {\r\nret = hns_roce_write_mtt(hr_dev, mtt, n, i,\r\npages);\r\nif (ret)\r\ngoto out;\r\nn += i;\r\ni = 0;\r\n}\r\n}\r\n}\r\nif (i)\r\nret = hns_roce_write_mtt(hr_dev, mtt, n, i, pages);\r\nout:\r\nfree_page((unsigned long) pages);\r\nreturn ret;\r\n}\r\nstatic int hns_roce_ib_umem_write_mr(struct hns_roce_mr *mr,\r\nstruct ib_umem *umem)\r\n{\r\nint i = 0;\r\nint entry;\r\nstruct scatterlist *sg;\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {\r\nmr->pbl_buf[i] = ((u64)sg_dma_address(sg)) >> 12;\r\ni++;\r\n}\r\nmb();\r\nreturn 0;\r\n}\r\nstruct ib_mr *hns_roce_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\r\nu64 virt_addr, int access_flags,\r\nstruct ib_udata *udata)\r\n{\r\nstruct hns_roce_dev *hr_dev = to_hr_dev(pd->device);\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nstruct hns_roce_mr *mr = NULL;\r\nint ret = 0;\r\nint n = 0;\r\nmr = kmalloc(sizeof(*mr), GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(-ENOMEM);\r\nmr->umem = ib_umem_get(pd->uobject->context, start, length,\r\naccess_flags, 0);\r\nif (IS_ERR(mr->umem)) {\r\nret = PTR_ERR(mr->umem);\r\ngoto err_free;\r\n}\r\nn = ib_umem_page_count(mr->umem);\r\nif (mr->umem->page_size != HNS_ROCE_HEM_PAGE_SIZE) {\r\ndev_err(dev, "Just support 4K page size but is 0x%x now!\n",\r\nmr->umem->page_size);\r\nret = -EINVAL;\r\ngoto err_umem;\r\n}\r\nif (n > HNS_ROCE_MAX_MTPT_PBL_NUM) {\r\ndev_err(dev, " MR len %lld err. MR is limited to 4G at most!\n",\r\nlength);\r\nret = -EINVAL;\r\ngoto err_umem;\r\n}\r\nret = hns_roce_mr_alloc(hr_dev, to_hr_pd(pd)->pdn, virt_addr, length,\r\naccess_flags, n, mr);\r\nif (ret)\r\ngoto err_umem;\r\nret = hns_roce_ib_umem_write_mr(mr, mr->umem);\r\nif (ret)\r\ngoto err_mr;\r\nret = hns_roce_mr_enable(hr_dev, mr);\r\nif (ret)\r\ngoto err_mr;\r\nmr->ibmr.rkey = mr->ibmr.lkey = mr->key;\r\nreturn &mr->ibmr;\r\nerr_mr:\r\nhns_roce_mr_free(hr_dev, mr);\r\nerr_umem:\r\nib_umem_release(mr->umem);\r\nerr_free:\r\nkfree(mr);\r\nreturn ERR_PTR(ret);\r\n}\r\nint hns_roce_dereg_mr(struct ib_mr *ibmr)\r\n{\r\nstruct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);\r\nstruct hns_roce_mr *mr = to_hr_mr(ibmr);\r\nint ret = 0;\r\nif (hr_dev->hw->dereg_mr) {\r\nret = hr_dev->hw->dereg_mr(hr_dev, mr);\r\n} else {\r\nhns_roce_mr_free(hr_dev, mr);\r\nif (mr->umem)\r\nib_umem_release(mr->umem);\r\nkfree(mr);\r\n}\r\nreturn ret;\r\n}
