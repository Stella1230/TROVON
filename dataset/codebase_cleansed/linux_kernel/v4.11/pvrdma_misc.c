int pvrdma_page_dir_init(struct pvrdma_dev *dev, struct pvrdma_page_dir *pdir,\r\nu64 npages, bool alloc_pages)\r\n{\r\nu64 i;\r\nif (npages > PVRDMA_PAGE_DIR_MAX_PAGES)\r\nreturn -EINVAL;\r\nmemset(pdir, 0, sizeof(*pdir));\r\npdir->dir = dma_alloc_coherent(&dev->pdev->dev, PAGE_SIZE,\r\n&pdir->dir_dma, GFP_KERNEL);\r\nif (!pdir->dir)\r\ngoto err;\r\npdir->ntables = PVRDMA_PAGE_DIR_TABLE(npages - 1) + 1;\r\npdir->tables = kcalloc(pdir->ntables, sizeof(*pdir->tables),\r\nGFP_KERNEL);\r\nif (!pdir->tables)\r\ngoto err;\r\nfor (i = 0; i < pdir->ntables; i++) {\r\npdir->tables[i] = dma_alloc_coherent(&dev->pdev->dev, PAGE_SIZE,\r\n(dma_addr_t *)&pdir->dir[i],\r\nGFP_KERNEL);\r\nif (!pdir->tables[i])\r\ngoto err;\r\n}\r\npdir->npages = npages;\r\nif (alloc_pages) {\r\npdir->pages = kcalloc(npages, sizeof(*pdir->pages),\r\nGFP_KERNEL);\r\nif (!pdir->pages)\r\ngoto err;\r\nfor (i = 0; i < pdir->npages; i++) {\r\ndma_addr_t page_dma;\r\npdir->pages[i] = dma_alloc_coherent(&dev->pdev->dev,\r\nPAGE_SIZE,\r\n&page_dma,\r\nGFP_KERNEL);\r\nif (!pdir->pages[i])\r\ngoto err;\r\npvrdma_page_dir_insert_dma(pdir, i, page_dma);\r\n}\r\n}\r\nreturn 0;\r\nerr:\r\npvrdma_page_dir_cleanup(dev, pdir);\r\nreturn -ENOMEM;\r\n}\r\nstatic u64 *pvrdma_page_dir_table(struct pvrdma_page_dir *pdir, u64 idx)\r\n{\r\nreturn pdir->tables[PVRDMA_PAGE_DIR_TABLE(idx)];\r\n}\r\ndma_addr_t pvrdma_page_dir_get_dma(struct pvrdma_page_dir *pdir, u64 idx)\r\n{\r\nreturn pvrdma_page_dir_table(pdir, idx)[PVRDMA_PAGE_DIR_PAGE(idx)];\r\n}\r\nstatic void pvrdma_page_dir_cleanup_pages(struct pvrdma_dev *dev,\r\nstruct pvrdma_page_dir *pdir)\r\n{\r\nif (pdir->pages) {\r\nu64 i;\r\nfor (i = 0; i < pdir->npages && pdir->pages[i]; i++) {\r\ndma_addr_t page_dma = pvrdma_page_dir_get_dma(pdir, i);\r\ndma_free_coherent(&dev->pdev->dev, PAGE_SIZE,\r\npdir->pages[i], page_dma);\r\n}\r\nkfree(pdir->pages);\r\n}\r\n}\r\nstatic void pvrdma_page_dir_cleanup_tables(struct pvrdma_dev *dev,\r\nstruct pvrdma_page_dir *pdir)\r\n{\r\nif (pdir->tables) {\r\nint i;\r\npvrdma_page_dir_cleanup_pages(dev, pdir);\r\nfor (i = 0; i < pdir->ntables; i++) {\r\nu64 *table = pdir->tables[i];\r\nif (table)\r\ndma_free_coherent(&dev->pdev->dev, PAGE_SIZE,\r\ntable, pdir->dir[i]);\r\n}\r\nkfree(pdir->tables);\r\n}\r\n}\r\nvoid pvrdma_page_dir_cleanup(struct pvrdma_dev *dev,\r\nstruct pvrdma_page_dir *pdir)\r\n{\r\nif (pdir->dir) {\r\npvrdma_page_dir_cleanup_tables(dev, pdir);\r\ndma_free_coherent(&dev->pdev->dev, PAGE_SIZE,\r\npdir->dir, pdir->dir_dma);\r\n}\r\n}\r\nint pvrdma_page_dir_insert_dma(struct pvrdma_page_dir *pdir, u64 idx,\r\ndma_addr_t daddr)\r\n{\r\nu64 *table;\r\nif (idx >= pdir->npages)\r\nreturn -EINVAL;\r\ntable = pvrdma_page_dir_table(pdir, idx);\r\ntable[PVRDMA_PAGE_DIR_PAGE(idx)] = daddr;\r\nreturn 0;\r\n}\r\nint pvrdma_page_dir_insert_umem(struct pvrdma_page_dir *pdir,\r\nstruct ib_umem *umem, u64 offset)\r\n{\r\nu64 i = offset;\r\nint j, entry;\r\nint ret = 0, len = 0;\r\nstruct scatterlist *sg;\r\nif (offset >= pdir->npages)\r\nreturn -EINVAL;\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {\r\nlen = sg_dma_len(sg) >> PAGE_SHIFT;\r\nfor (j = 0; j < len; j++) {\r\ndma_addr_t addr = sg_dma_address(sg) +\r\numem->page_size * j;\r\nret = pvrdma_page_dir_insert_dma(pdir, i, addr);\r\nif (ret)\r\ngoto exit;\r\ni++;\r\n}\r\n}\r\nexit:\r\nreturn ret;\r\n}\r\nint pvrdma_page_dir_insert_page_list(struct pvrdma_page_dir *pdir,\r\nu64 *page_list,\r\nint num_pages)\r\n{\r\nint i;\r\nint ret;\r\nif (num_pages > pdir->npages)\r\nreturn -EINVAL;\r\nfor (i = 0; i < num_pages; i++) {\r\nret = pvrdma_page_dir_insert_dma(pdir, i, page_list[i]);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nvoid pvrdma_qp_cap_to_ib(struct ib_qp_cap *dst, const struct pvrdma_qp_cap *src)\r\n{\r\ndst->max_send_wr = src->max_send_wr;\r\ndst->max_recv_wr = src->max_recv_wr;\r\ndst->max_send_sge = src->max_send_sge;\r\ndst->max_recv_sge = src->max_recv_sge;\r\ndst->max_inline_data = src->max_inline_data;\r\n}\r\nvoid ib_qp_cap_to_pvrdma(struct pvrdma_qp_cap *dst, const struct ib_qp_cap *src)\r\n{\r\ndst->max_send_wr = src->max_send_wr;\r\ndst->max_recv_wr = src->max_recv_wr;\r\ndst->max_send_sge = src->max_send_sge;\r\ndst->max_recv_sge = src->max_recv_sge;\r\ndst->max_inline_data = src->max_inline_data;\r\n}\r\nvoid pvrdma_gid_to_ib(union ib_gid *dst, const union pvrdma_gid *src)\r\n{\r\nBUILD_BUG_ON(sizeof(union pvrdma_gid) != sizeof(union ib_gid));\r\nmemcpy(dst, src, sizeof(*src));\r\n}\r\nvoid ib_gid_to_pvrdma(union pvrdma_gid *dst, const union ib_gid *src)\r\n{\r\nBUILD_BUG_ON(sizeof(union pvrdma_gid) != sizeof(union ib_gid));\r\nmemcpy(dst, src, sizeof(*src));\r\n}\r\nvoid pvrdma_global_route_to_ib(struct ib_global_route *dst,\r\nconst struct pvrdma_global_route *src)\r\n{\r\npvrdma_gid_to_ib(&dst->dgid, &src->dgid);\r\ndst->flow_label = src->flow_label;\r\ndst->sgid_index = src->sgid_index;\r\ndst->hop_limit = src->hop_limit;\r\ndst->traffic_class = src->traffic_class;\r\n}\r\nvoid ib_global_route_to_pvrdma(struct pvrdma_global_route *dst,\r\nconst struct ib_global_route *src)\r\n{\r\nib_gid_to_pvrdma(&dst->dgid, &src->dgid);\r\ndst->flow_label = src->flow_label;\r\ndst->sgid_index = src->sgid_index;\r\ndst->hop_limit = src->hop_limit;\r\ndst->traffic_class = src->traffic_class;\r\n}\r\nvoid pvrdma_ah_attr_to_ib(struct ib_ah_attr *dst,\r\nconst struct pvrdma_ah_attr *src)\r\n{\r\npvrdma_global_route_to_ib(&dst->grh, &src->grh);\r\ndst->dlid = src->dlid;\r\ndst->sl = src->sl;\r\ndst->src_path_bits = src->src_path_bits;\r\ndst->static_rate = src->static_rate;\r\ndst->ah_flags = src->ah_flags;\r\ndst->port_num = src->port_num;\r\nmemcpy(&dst->dmac, &src->dmac, sizeof(dst->dmac));\r\n}\r\nvoid ib_ah_attr_to_pvrdma(struct pvrdma_ah_attr *dst,\r\nconst struct ib_ah_attr *src)\r\n{\r\nib_global_route_to_pvrdma(&dst->grh, &src->grh);\r\ndst->dlid = src->dlid;\r\ndst->sl = src->sl;\r\ndst->src_path_bits = src->src_path_bits;\r\ndst->static_rate = src->static_rate;\r\ndst->ah_flags = src->ah_flags;\r\ndst->port_num = src->port_num;\r\nmemcpy(&dst->dmac, &src->dmac, sizeof(dst->dmac));\r\n}
