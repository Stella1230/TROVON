static inline struct omap_dmadev *to_omap_dma_dev(struct dma_device *d)\r\n{\r\nreturn container_of(d, struct omap_dmadev, ddev);\r\n}\r\nstatic inline struct omap_chan *to_omap_dma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct omap_chan, vc.chan);\r\n}\r\nstatic inline struct omap_desc *to_omap_dma_desc(struct dma_async_tx_descriptor *t)\r\n{\r\nreturn container_of(t, struct omap_desc, vd.tx);\r\n}\r\nstatic void omap_dma_desc_free(struct virt_dma_desc *vd)\r\n{\r\nstruct omap_desc *d = to_omap_dma_desc(&vd->tx);\r\nif (d->using_ll) {\r\nstruct omap_dmadev *od = to_omap_dma_dev(vd->tx.chan->device);\r\nint i;\r\nfor (i = 0; i < d->sglen; i++) {\r\nif (d->sg[i].t2_desc)\r\ndma_pool_free(od->desc_pool, d->sg[i].t2_desc,\r\nd->sg[i].t2_desc_paddr);\r\n}\r\n}\r\nkfree(d);\r\n}\r\nstatic void omap_dma_fill_type2_desc(struct omap_desc *d, int idx,\r\nenum dma_transfer_direction dir, bool last)\r\n{\r\nstruct omap_sg *sg = &d->sg[idx];\r\nstruct omap_type2_desc *t2_desc = sg->t2_desc;\r\nif (idx)\r\nd->sg[idx - 1].t2_desc->next_desc = sg->t2_desc_paddr;\r\nif (last)\r\nt2_desc->next_desc = 0xfffffffc;\r\nt2_desc->en = sg->en;\r\nt2_desc->addr = sg->addr;\r\nt2_desc->fn = sg->fn & 0xffff;\r\nt2_desc->cicr = d->cicr;\r\nif (!last)\r\nt2_desc->cicr &= ~CICR_BLOCK_IE;\r\nswitch (dir) {\r\ncase DMA_DEV_TO_MEM:\r\nt2_desc->cdei = sg->ei;\r\nt2_desc->csei = d->ei;\r\nt2_desc->cdfi = sg->fi;\r\nt2_desc->csfi = d->fi;\r\nt2_desc->en |= DESC_NXT_DV_REFRESH;\r\nt2_desc->en |= DESC_NXT_SV_REUSE;\r\nbreak;\r\ncase DMA_MEM_TO_DEV:\r\nt2_desc->cdei = d->ei;\r\nt2_desc->csei = sg->ei;\r\nt2_desc->cdfi = d->fi;\r\nt2_desc->csfi = sg->fi;\r\nt2_desc->en |= DESC_NXT_SV_REFRESH;\r\nt2_desc->en |= DESC_NXT_DV_REUSE;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nt2_desc->en |= DESC_NTYPE_TYPE2;\r\n}\r\nstatic void omap_dma_write(uint32_t val, unsigned type, void __iomem *addr)\r\n{\r\nswitch (type) {\r\ncase OMAP_DMA_REG_16BIT:\r\nwritew_relaxed(val, addr);\r\nbreak;\r\ncase OMAP_DMA_REG_2X16BIT:\r\nwritew_relaxed(val, addr);\r\nwritew_relaxed(val >> 16, addr + 2);\r\nbreak;\r\ncase OMAP_DMA_REG_32BIT:\r\nwritel_relaxed(val, addr);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\n}\r\n}\r\nstatic unsigned omap_dma_read(unsigned type, void __iomem *addr)\r\n{\r\nunsigned val;\r\nswitch (type) {\r\ncase OMAP_DMA_REG_16BIT:\r\nval = readw_relaxed(addr);\r\nbreak;\r\ncase OMAP_DMA_REG_2X16BIT:\r\nval = readw_relaxed(addr);\r\nval |= readw_relaxed(addr + 2) << 16;\r\nbreak;\r\ncase OMAP_DMA_REG_32BIT:\r\nval = readl_relaxed(addr);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nval = 0;\r\n}\r\nreturn val;\r\n}\r\nstatic void omap_dma_glbl_write(struct omap_dmadev *od, unsigned reg, unsigned val)\r\n{\r\nconst struct omap_dma_reg *r = od->reg_map + reg;\r\nWARN_ON(r->stride);\r\nomap_dma_write(val, r->type, od->base + r->offset);\r\n}\r\nstatic unsigned omap_dma_glbl_read(struct omap_dmadev *od, unsigned reg)\r\n{\r\nconst struct omap_dma_reg *r = od->reg_map + reg;\r\nWARN_ON(r->stride);\r\nreturn omap_dma_read(r->type, od->base + r->offset);\r\n}\r\nstatic void omap_dma_chan_write(struct omap_chan *c, unsigned reg, unsigned val)\r\n{\r\nconst struct omap_dma_reg *r = c->reg_map + reg;\r\nomap_dma_write(val, r->type, c->channel_base + r->offset);\r\n}\r\nstatic unsigned omap_dma_chan_read(struct omap_chan *c, unsigned reg)\r\n{\r\nconst struct omap_dma_reg *r = c->reg_map + reg;\r\nreturn omap_dma_read(r->type, c->channel_base + r->offset);\r\n}\r\nstatic void omap_dma_clear_csr(struct omap_chan *c)\r\n{\r\nif (dma_omap1())\r\nomap_dma_chan_read(c, CSR);\r\nelse\r\nomap_dma_chan_write(c, CSR, ~0);\r\n}\r\nstatic unsigned omap_dma_get_csr(struct omap_chan *c)\r\n{\r\nunsigned val = omap_dma_chan_read(c, CSR);\r\nif (!dma_omap1())\r\nomap_dma_chan_write(c, CSR, val);\r\nreturn val;\r\n}\r\nstatic void omap_dma_assign(struct omap_dmadev *od, struct omap_chan *c,\r\nunsigned lch)\r\n{\r\nc->channel_base = od->base + od->plat->channel_stride * lch;\r\nod->lch_map[lch] = c;\r\n}\r\nstatic void omap_dma_start(struct omap_chan *c, struct omap_desc *d)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\nuint16_t cicr = d->cicr;\r\nif (__dma_omap15xx(od->plat->dma_attr))\r\nomap_dma_chan_write(c, CPC, 0);\r\nelse\r\nomap_dma_chan_write(c, CDAC, 0);\r\nomap_dma_clear_csr(c);\r\nif (d->using_ll) {\r\nuint32_t cdp = CDP_TMODE_LLIST | CDP_NTYPE_TYPE2 | CDP_FAST;\r\nif (d->dir == DMA_DEV_TO_MEM)\r\ncdp |= (CDP_DST_VALID_RELOAD | CDP_SRC_VALID_REUSE);\r\nelse\r\ncdp |= (CDP_DST_VALID_REUSE | CDP_SRC_VALID_RELOAD);\r\nomap_dma_chan_write(c, CDP, cdp);\r\nomap_dma_chan_write(c, CNDP, d->sg[0].t2_desc_paddr);\r\nomap_dma_chan_write(c, CCDN, 0);\r\nomap_dma_chan_write(c, CCFN, 0xffff);\r\nomap_dma_chan_write(c, CCEN, 0xffffff);\r\ncicr &= ~CICR_BLOCK_IE;\r\n} else if (od->ll123_supported) {\r\nomap_dma_chan_write(c, CDP, 0);\r\n}\r\nomap_dma_chan_write(c, CICR, cicr);\r\nomap_dma_chan_write(c, CCR, d->ccr | CCR_ENABLE);\r\nc->running = true;\r\n}\r\nstatic void omap_dma_drain_chan(struct omap_chan *c)\r\n{\r\nint i;\r\nu32 val;\r\nfor (i = 0; ; i++) {\r\nval = omap_dma_chan_read(c, CCR);\r\nif (!(val & (CCR_RD_ACTIVE | CCR_WR_ACTIVE)))\r\nbreak;\r\nif (i > 100)\r\nbreak;\r\nudelay(5);\r\n}\r\nif (val & (CCR_RD_ACTIVE | CCR_WR_ACTIVE))\r\ndev_err(c->vc.chan.device->dev,\r\n"DMA drain did not complete on lch %d\n",\r\nc->dma_ch);\r\n}\r\nstatic int omap_dma_stop(struct omap_chan *c)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\nuint32_t val;\r\nomap_dma_chan_write(c, CICR, 0);\r\nomap_dma_clear_csr(c);\r\nval = omap_dma_chan_read(c, CCR);\r\nif (od->plat->errata & DMA_ERRATA_i541 && val & CCR_TRIGGER_SRC) {\r\nuint32_t sysconfig;\r\nsysconfig = omap_dma_glbl_read(od, OCP_SYSCONFIG);\r\nval = sysconfig & ~DMA_SYSCONFIG_MIDLEMODE_MASK;\r\nval |= DMA_SYSCONFIG_MIDLEMODE(DMA_IDLEMODE_NO_IDLE);\r\nomap_dma_glbl_write(od, OCP_SYSCONFIG, val);\r\nval = omap_dma_chan_read(c, CCR);\r\nval &= ~CCR_ENABLE;\r\nomap_dma_chan_write(c, CCR, val);\r\nif (!(c->ccr & CCR_BUFFERING_DISABLE))\r\nomap_dma_drain_chan(c);\r\nomap_dma_glbl_write(od, OCP_SYSCONFIG, sysconfig);\r\n} else {\r\nif (!(val & CCR_ENABLE))\r\nreturn -EINVAL;\r\nval &= ~CCR_ENABLE;\r\nomap_dma_chan_write(c, CCR, val);\r\nif (!(c->ccr & CCR_BUFFERING_DISABLE))\r\nomap_dma_drain_chan(c);\r\n}\r\nmb();\r\nif (!__dma_omap15xx(od->plat->dma_attr) && c->cyclic) {\r\nval = omap_dma_chan_read(c, CLNK_CTRL);\r\nif (dma_omap1())\r\nval |= 1 << 14;\r\nelse\r\nval &= ~CLNK_CTRL_ENABLE_LNK;\r\nomap_dma_chan_write(c, CLNK_CTRL, val);\r\n}\r\nc->running = false;\r\nreturn 0;\r\n}\r\nstatic void omap_dma_start_sg(struct omap_chan *c, struct omap_desc *d)\r\n{\r\nstruct omap_sg *sg = d->sg + c->sgidx;\r\nunsigned cxsa, cxei, cxfi;\r\nif (d->dir == DMA_DEV_TO_MEM || d->dir == DMA_MEM_TO_MEM) {\r\ncxsa = CDSA;\r\ncxei = CDEI;\r\ncxfi = CDFI;\r\n} else {\r\ncxsa = CSSA;\r\ncxei = CSEI;\r\ncxfi = CSFI;\r\n}\r\nomap_dma_chan_write(c, cxsa, sg->addr);\r\nomap_dma_chan_write(c, cxei, sg->ei);\r\nomap_dma_chan_write(c, cxfi, sg->fi);\r\nomap_dma_chan_write(c, CEN, sg->en);\r\nomap_dma_chan_write(c, CFN, sg->fn);\r\nomap_dma_start(c, d);\r\nc->sgidx++;\r\n}\r\nstatic void omap_dma_start_desc(struct omap_chan *c)\r\n{\r\nstruct virt_dma_desc *vd = vchan_next_desc(&c->vc);\r\nstruct omap_desc *d;\r\nunsigned cxsa, cxei, cxfi;\r\nif (!vd) {\r\nc->desc = NULL;\r\nreturn;\r\n}\r\nlist_del(&vd->node);\r\nc->desc = d = to_omap_dma_desc(&vd->tx);\r\nc->sgidx = 0;\r\nmb();\r\nomap_dma_chan_write(c, CCR, d->ccr);\r\nif (dma_omap1())\r\nomap_dma_chan_write(c, CCR2, d->ccr >> 16);\r\nif (d->dir == DMA_DEV_TO_MEM || d->dir == DMA_MEM_TO_MEM) {\r\ncxsa = CSSA;\r\ncxei = CSEI;\r\ncxfi = CSFI;\r\n} else {\r\ncxsa = CDSA;\r\ncxei = CDEI;\r\ncxfi = CDFI;\r\n}\r\nomap_dma_chan_write(c, cxsa, d->dev_addr);\r\nomap_dma_chan_write(c, cxei, d->ei);\r\nomap_dma_chan_write(c, cxfi, d->fi);\r\nomap_dma_chan_write(c, CSDP, d->csdp);\r\nomap_dma_chan_write(c, CLNK_CTRL, d->clnk_ctrl);\r\nomap_dma_start_sg(c, d);\r\n}\r\nstatic void omap_dma_callback(int ch, u16 status, void *data)\r\n{\r\nstruct omap_chan *c = data;\r\nstruct omap_desc *d;\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nd = c->desc;\r\nif (d) {\r\nif (c->cyclic) {\r\nvchan_cyclic_callback(&d->vd);\r\n} else if (d->using_ll || c->sgidx == d->sglen) {\r\nomap_dma_start_desc(c);\r\nvchan_cookie_complete(&d->vd);\r\n} else {\r\nomap_dma_start_sg(c, d);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nstatic irqreturn_t omap_dma_irq(int irq, void *devid)\r\n{\r\nstruct omap_dmadev *od = devid;\r\nunsigned status, channel;\r\nspin_lock(&od->irq_lock);\r\nstatus = omap_dma_glbl_read(od, IRQSTATUS_L1);\r\nstatus &= od->irq_enable_mask;\r\nif (status == 0) {\r\nspin_unlock(&od->irq_lock);\r\nreturn IRQ_NONE;\r\n}\r\nwhile ((channel = ffs(status)) != 0) {\r\nunsigned mask, csr;\r\nstruct omap_chan *c;\r\nchannel -= 1;\r\nmask = BIT(channel);\r\nstatus &= ~mask;\r\nc = od->lch_map[channel];\r\nif (c == NULL) {\r\ndev_err(od->ddev.dev, "invalid channel %u\n", channel);\r\ncontinue;\r\n}\r\ncsr = omap_dma_get_csr(c);\r\nomap_dma_glbl_write(od, IRQSTATUS_L1, mask);\r\nomap_dma_callback(channel, csr, c);\r\n}\r\nspin_unlock(&od->irq_lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int omap_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct device *dev = od->ddev.dev;\r\nint ret;\r\nif (od->legacy) {\r\nret = omap_request_dma(c->dma_sig, "DMA engine",\r\nomap_dma_callback, c, &c->dma_ch);\r\n} else {\r\nret = omap_request_dma(c->dma_sig, "DMA engine", NULL, NULL,\r\n&c->dma_ch);\r\n}\r\ndev_dbg(dev, "allocating channel %u for %u\n", c->dma_ch, c->dma_sig);\r\nif (ret >= 0) {\r\nomap_dma_assign(od, c, c->dma_ch);\r\nif (!od->legacy) {\r\nunsigned val;\r\nspin_lock_irq(&od->irq_lock);\r\nval = BIT(c->dma_ch);\r\nomap_dma_glbl_write(od, IRQSTATUS_L1, val);\r\nod->irq_enable_mask |= val;\r\nomap_dma_glbl_write(od, IRQENABLE_L1, od->irq_enable_mask);\r\nval = omap_dma_glbl_read(od, IRQENABLE_L0);\r\nval &= ~BIT(c->dma_ch);\r\nomap_dma_glbl_write(od, IRQENABLE_L0, val);\r\nspin_unlock_irq(&od->irq_lock);\r\n}\r\n}\r\nif (dma_omap1()) {\r\nif (__dma_omap16xx(od->plat->dma_attr)) {\r\nc->ccr = CCR_OMAP31_DISABLE;\r\nc->ccr |= c->dma_ch + 1;\r\n} else {\r\nc->ccr = c->dma_sig & 0x1f;\r\n}\r\n} else {\r\nc->ccr = c->dma_sig & 0x1f;\r\nc->ccr |= (c->dma_sig & ~0x1f) << 14;\r\n}\r\nif (od->plat->errata & DMA_ERRATA_IFRAME_BUFFERING)\r\nc->ccr |= CCR_BUFFERING_DISABLE;\r\nreturn ret;\r\n}\r\nstatic void omap_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nif (!od->legacy) {\r\nspin_lock_irq(&od->irq_lock);\r\nod->irq_enable_mask &= ~BIT(c->dma_ch);\r\nomap_dma_glbl_write(od, IRQENABLE_L1, od->irq_enable_mask);\r\nspin_unlock_irq(&od->irq_lock);\r\n}\r\nc->channel_base = NULL;\r\nod->lch_map[c->dma_ch] = NULL;\r\nvchan_free_chan_resources(&c->vc);\r\nomap_free_dma(c->dma_ch);\r\ndev_dbg(od->ddev.dev, "freeing channel %u used for %u\n", c->dma_ch,\r\nc->dma_sig);\r\nc->dma_sig = 0;\r\n}\r\nstatic size_t omap_dma_sg_size(struct omap_sg *sg)\r\n{\r\nreturn sg->en * sg->fn;\r\n}\r\nstatic size_t omap_dma_desc_size(struct omap_desc *d)\r\n{\r\nunsigned i;\r\nsize_t size;\r\nfor (size = i = 0; i < d->sglen; i++)\r\nsize += omap_dma_sg_size(&d->sg[i]);\r\nreturn size * es_bytes[d->es];\r\n}\r\nstatic size_t omap_dma_desc_size_pos(struct omap_desc *d, dma_addr_t addr)\r\n{\r\nunsigned i;\r\nsize_t size, es_size = es_bytes[d->es];\r\nfor (size = i = 0; i < d->sglen; i++) {\r\nsize_t this_size = omap_dma_sg_size(&d->sg[i]) * es_size;\r\nif (size)\r\nsize += this_size;\r\nelse if (addr >= d->sg[i].addr &&\r\naddr < d->sg[i].addr + this_size)\r\nsize += d->sg[i].addr + this_size - addr;\r\n}\r\nreturn size;\r\n}\r\nstatic uint32_t omap_dma_chan_read_3_3(struct omap_chan *c, unsigned reg)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\nuint32_t val;\r\nval = omap_dma_chan_read(c, reg);\r\nif (val == 0 && od->plat->errata & DMA_ERRATA_3_3)\r\nval = omap_dma_chan_read(c, reg);\r\nreturn val;\r\n}\r\nstatic dma_addr_t omap_dma_get_src_pos(struct omap_chan *c)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\ndma_addr_t addr, cdac;\r\nif (__dma_omap15xx(od->plat->dma_attr)) {\r\naddr = omap_dma_chan_read(c, CPC);\r\n} else {\r\naddr = omap_dma_chan_read_3_3(c, CSAC);\r\ncdac = omap_dma_chan_read_3_3(c, CDAC);\r\nif (cdac == 0)\r\naddr = omap_dma_chan_read(c, CSSA);\r\n}\r\nif (dma_omap1())\r\naddr |= omap_dma_chan_read(c, CSSA) & 0xffff0000;\r\nreturn addr;\r\n}\r\nstatic dma_addr_t omap_dma_get_dst_pos(struct omap_chan *c)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\ndma_addr_t addr;\r\nif (__dma_omap15xx(od->plat->dma_attr)) {\r\naddr = omap_dma_chan_read(c, CPC);\r\n} else {\r\naddr = omap_dma_chan_read_3_3(c, CDAC);\r\nif (addr == 0)\r\naddr = omap_dma_chan_read(c, CDSA);\r\n}\r\nif (dma_omap1())\r\naddr |= omap_dma_chan_read(c, CDSA) & 0xffff0000;\r\nreturn addr;\r\n}\r\nstatic enum dma_status omap_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct virt_dma_desc *vd;\r\nenum dma_status ret;\r\nunsigned long flags;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (!c->paused && c->running) {\r\nuint32_t ccr = omap_dma_chan_read(c, CCR);\r\nif (!(ccr & CCR_ENABLE))\r\nret = DMA_COMPLETE;\r\n}\r\nif (ret == DMA_COMPLETE || !txstate)\r\nreturn ret;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nvd = vchan_find_desc(&c->vc, cookie);\r\nif (vd) {\r\ntxstate->residue = omap_dma_desc_size(to_omap_dma_desc(&vd->tx));\r\n} else if (c->desc && c->desc->vd.tx.cookie == cookie) {\r\nstruct omap_desc *d = c->desc;\r\ndma_addr_t pos;\r\nif (d->dir == DMA_MEM_TO_DEV)\r\npos = omap_dma_get_src_pos(c);\r\nelse if (d->dir == DMA_DEV_TO_MEM || d->dir == DMA_MEM_TO_MEM)\r\npos = omap_dma_get_dst_pos(c);\r\nelse\r\npos = 0;\r\ntxstate->residue = omap_dma_desc_size_pos(d, pos);\r\n} else {\r\ntxstate->residue = 0;\r\n}\r\nif (ret == DMA_IN_PROGRESS && c->paused)\r\nret = DMA_PAUSED;\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nreturn ret;\r\n}\r\nstatic void omap_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (vchan_issue_pending(&c->vc) && !c->desc)\r\nomap_dma_start_desc(c);\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned sglen,\r\nenum dma_transfer_direction dir, unsigned long tx_flags, void *context)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nenum dma_slave_buswidth dev_width;\r\nstruct scatterlist *sgent;\r\nstruct omap_desc *d;\r\ndma_addr_t dev_addr;\r\nunsigned i, es, en, frame_bytes;\r\nbool ll_failed = false;\r\nu32 burst;\r\nu32 port_window, port_window_bytes;\r\nif (dir == DMA_DEV_TO_MEM) {\r\ndev_addr = c->cfg.src_addr;\r\ndev_width = c->cfg.src_addr_width;\r\nburst = c->cfg.src_maxburst;\r\nport_window = c->cfg.src_port_window_size;\r\n} else if (dir == DMA_MEM_TO_DEV) {\r\ndev_addr = c->cfg.dst_addr;\r\ndev_width = c->cfg.dst_addr_width;\r\nburst = c->cfg.dst_maxburst;\r\nport_window = c->cfg.dst_port_window_size;\r\n} else {\r\ndev_err(chan->device->dev, "%s: bad direction?\n", __func__);\r\nreturn NULL;\r\n}\r\nswitch (dev_width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nes = CSDP_DATA_TYPE_8;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nes = CSDP_DATA_TYPE_16;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nes = CSDP_DATA_TYPE_32;\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nif (port_window) {\r\nburst = port_window;\r\nport_window_bytes = port_window * es_bytes[es];\r\n}\r\nd = kzalloc(sizeof(*d) + sglen * sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\nd->dir = dir;\r\nd->dev_addr = dev_addr;\r\nd->es = es;\r\nd->ccr = c->ccr | CCR_SYNC_FRAME;\r\nif (dir == DMA_DEV_TO_MEM) {\r\nd->csdp = CSDP_DST_BURST_64 | CSDP_DST_PACKED;\r\nd->ccr |= CCR_DST_AMODE_POSTINC;\r\nif (port_window) {\r\nd->ccr |= CCR_SRC_AMODE_DBLIDX;\r\nif (port_window_bytes >= 64)\r\nd->csdp |= CSDP_SRC_BURST_64;\r\nelse if (port_window_bytes >= 32)\r\nd->csdp |= CSDP_SRC_BURST_32;\r\nelse if (port_window_bytes >= 16)\r\nd->csdp |= CSDP_SRC_BURST_16;\r\n} else {\r\nd->ccr |= CCR_SRC_AMODE_CONSTANT;\r\n}\r\n} else {\r\nd->csdp = CSDP_SRC_BURST_64 | CSDP_SRC_PACKED;\r\nd->ccr |= CCR_SRC_AMODE_POSTINC;\r\nif (port_window) {\r\nd->ccr |= CCR_DST_AMODE_DBLIDX;\r\nd->ei = 1;\r\nd->fi = -(port_window_bytes - 1);\r\nif (port_window_bytes >= 64)\r\nd->csdp |= CSDP_DST_BURST_64;\r\nelse if (port_window_bytes >= 32)\r\nd->csdp |= CSDP_DST_BURST_32;\r\nelse if (port_window_bytes >= 16)\r\nd->csdp |= CSDP_DST_BURST_16;\r\n} else {\r\nd->ccr |= CCR_DST_AMODE_CONSTANT;\r\n}\r\n}\r\nd->cicr = CICR_DROP_IE | CICR_BLOCK_IE;\r\nd->csdp |= es;\r\nif (dma_omap1()) {\r\nd->cicr |= CICR_TOUT_IE;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_TIPB;\r\nelse\r\nd->csdp |= CSDP_DST_PORT_TIPB | CSDP_SRC_PORT_EMIFF;\r\n} else {\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->ccr |= CCR_TRIGGER_SRC;\r\nd->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\r\nif (port_window)\r\nd->csdp |= CSDP_WRITE_LAST_NON_POSTED;\r\n}\r\nif (od->plat->errata & DMA_ERRATA_PARALLEL_CHANNELS)\r\nd->clnk_ctrl = c->dma_ch;\r\nen = burst;\r\nframe_bytes = es_bytes[es] * en;\r\nif (sglen >= 2)\r\nd->using_ll = od->ll123_supported;\r\nfor_each_sg(sgl, sgent, sglen, i) {\r\nstruct omap_sg *osg = &d->sg[i];\r\nosg->addr = sg_dma_address(sgent);\r\nosg->en = en;\r\nosg->fn = sg_dma_len(sgent) / frame_bytes;\r\nif (port_window && dir == DMA_DEV_TO_MEM) {\r\nosg->ei = 1;\r\nosg->fi = -(port_window_bytes - 1);\r\n}\r\nif (d->using_ll) {\r\nosg->t2_desc = dma_pool_alloc(od->desc_pool, GFP_ATOMIC,\r\n&osg->t2_desc_paddr);\r\nif (!osg->t2_desc) {\r\ndev_err(chan->device->dev,\r\n"t2_desc[%d] allocation failed\n", i);\r\nll_failed = true;\r\nd->using_ll = false;\r\ncontinue;\r\n}\r\nomap_dma_fill_type2_desc(d, i, dir, (i == sglen - 1));\r\n}\r\n}\r\nd->sglen = sglen;\r\nif (ll_failed) {\r\nfor (i = 0; i < d->sglen; i++) {\r\nstruct omap_sg *osg = &d->sg[i];\r\nif (osg->t2_desc) {\r\ndma_pool_free(od->desc_pool, osg->t2_desc,\r\nosg->t2_desc_paddr);\r\nosg->t2_desc = NULL;\r\n}\r\n}\r\n}\r\nreturn vchan_tx_prep(&c->vc, &d->vd, tx_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction dir, unsigned long flags)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nenum dma_slave_buswidth dev_width;\r\nstruct omap_desc *d;\r\ndma_addr_t dev_addr;\r\nunsigned es;\r\nu32 burst;\r\nif (dir == DMA_DEV_TO_MEM) {\r\ndev_addr = c->cfg.src_addr;\r\ndev_width = c->cfg.src_addr_width;\r\nburst = c->cfg.src_maxburst;\r\n} else if (dir == DMA_MEM_TO_DEV) {\r\ndev_addr = c->cfg.dst_addr;\r\ndev_width = c->cfg.dst_addr_width;\r\nburst = c->cfg.dst_maxburst;\r\n} else {\r\ndev_err(chan->device->dev, "%s: bad direction?\n", __func__);\r\nreturn NULL;\r\n}\r\nswitch (dev_width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nes = CSDP_DATA_TYPE_8;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nes = CSDP_DATA_TYPE_16;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nes = CSDP_DATA_TYPE_32;\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nd = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\nd->dir = dir;\r\nd->dev_addr = dev_addr;\r\nd->fi = burst;\r\nd->es = es;\r\nd->sg[0].addr = buf_addr;\r\nd->sg[0].en = period_len / es_bytes[es];\r\nd->sg[0].fn = buf_len / period_len;\r\nd->sglen = 1;\r\nd->ccr = c->ccr;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->ccr |= CCR_DST_AMODE_POSTINC | CCR_SRC_AMODE_CONSTANT;\r\nelse\r\nd->ccr |= CCR_DST_AMODE_CONSTANT | CCR_SRC_AMODE_POSTINC;\r\nd->cicr = CICR_DROP_IE;\r\nif (flags & DMA_PREP_INTERRUPT)\r\nd->cicr |= CICR_FRAME_IE;\r\nd->csdp = es;\r\nif (dma_omap1()) {\r\nd->cicr |= CICR_TOUT_IE;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_MPUI;\r\nelse\r\nd->csdp |= CSDP_DST_PORT_MPUI | CSDP_SRC_PORT_EMIFF;\r\n} else {\r\nif (burst)\r\nd->ccr |= CCR_SYNC_PACKET;\r\nelse\r\nd->ccr |= CCR_SYNC_ELEMENT;\r\nif (dir == DMA_DEV_TO_MEM) {\r\nd->ccr |= CCR_TRIGGER_SRC;\r\nd->csdp |= CSDP_DST_PACKED;\r\n} else {\r\nd->csdp |= CSDP_SRC_PACKED;\r\n}\r\nd->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\r\nd->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\r\n}\r\nif (__dma_omap15xx(od->plat->dma_attr))\r\nd->ccr |= CCR_AUTO_INIT | CCR_REPEAT;\r\nelse\r\nd->clnk_ctrl = c->dma_ch | CLNK_CTRL_ENABLE_LNK;\r\nc->cyclic = true;\r\nreturn vchan_tx_prep(&c->vc, &d->vd, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long tx_flags)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct omap_desc *d;\r\nuint8_t data_type;\r\nd = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\ndata_type = __ffs((src | dest | len));\r\nif (data_type > CSDP_DATA_TYPE_32)\r\ndata_type = CSDP_DATA_TYPE_32;\r\nd->dir = DMA_MEM_TO_MEM;\r\nd->dev_addr = src;\r\nd->fi = 0;\r\nd->es = data_type;\r\nd->sg[0].en = len / BIT(data_type);\r\nd->sg[0].fn = 1;\r\nd->sg[0].addr = dest;\r\nd->sglen = 1;\r\nd->ccr = c->ccr;\r\nd->ccr |= CCR_DST_AMODE_POSTINC | CCR_SRC_AMODE_POSTINC;\r\nd->cicr = CICR_DROP_IE | CICR_FRAME_IE;\r\nd->csdp = data_type;\r\nif (dma_omap1()) {\r\nd->cicr |= CICR_TOUT_IE;\r\nd->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_EMIFF;\r\n} else {\r\nd->csdp |= CSDP_DST_PACKED | CSDP_SRC_PACKED;\r\nd->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\r\nd->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\r\n}\r\nreturn vchan_tx_prep(&c->vc, &d->vd, tx_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_interleaved(\r\nstruct dma_chan *chan, struct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct omap_desc *d;\r\nstruct omap_sg *sg;\r\nuint8_t data_type;\r\nsize_t src_icg, dst_icg;\r\nif (is_slave_direction(xt->dir))\r\nreturn NULL;\r\nif (xt->frame_size != 1 || xt->numf == 0)\r\nreturn NULL;\r\nd = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\ndata_type = __ffs((xt->src_start | xt->dst_start | xt->sgl[0].size));\r\nif (data_type > CSDP_DATA_TYPE_32)\r\ndata_type = CSDP_DATA_TYPE_32;\r\nsg = &d->sg[0];\r\nd->dir = DMA_MEM_TO_MEM;\r\nd->dev_addr = xt->src_start;\r\nd->es = data_type;\r\nsg->en = xt->sgl[0].size / BIT(data_type);\r\nsg->fn = xt->numf;\r\nsg->addr = xt->dst_start;\r\nd->sglen = 1;\r\nd->ccr = c->ccr;\r\nsrc_icg = dmaengine_get_src_icg(xt, &xt->sgl[0]);\r\ndst_icg = dmaengine_get_dst_icg(xt, &xt->sgl[0]);\r\nif (src_icg) {\r\nd->ccr |= CCR_SRC_AMODE_DBLIDX;\r\nd->ei = 1;\r\nd->fi = src_icg;\r\n} else if (xt->src_inc) {\r\nd->ccr |= CCR_SRC_AMODE_POSTINC;\r\nd->fi = 0;\r\n} else {\r\ndev_err(chan->device->dev,\r\n"%s: SRC constant addressing is not supported\n",\r\n__func__);\r\nkfree(d);\r\nreturn NULL;\r\n}\r\nif (dst_icg) {\r\nd->ccr |= CCR_DST_AMODE_DBLIDX;\r\nsg->ei = 1;\r\nsg->fi = dst_icg;\r\n} else if (xt->dst_inc) {\r\nd->ccr |= CCR_DST_AMODE_POSTINC;\r\nsg->fi = 0;\r\n} else {\r\ndev_err(chan->device->dev,\r\n"%s: DST constant addressing is not supported\n",\r\n__func__);\r\nkfree(d);\r\nreturn NULL;\r\n}\r\nd->cicr = CICR_DROP_IE | CICR_FRAME_IE;\r\nd->csdp = data_type;\r\nif (dma_omap1()) {\r\nd->cicr |= CICR_TOUT_IE;\r\nd->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_EMIFF;\r\n} else {\r\nd->csdp |= CSDP_DST_PACKED | CSDP_SRC_PACKED;\r\nd->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\r\nd->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\r\n}\r\nreturn vchan_tx_prep(&c->vc, &d->vd, flags);\r\n}\r\nstatic int omap_dma_slave_config(struct dma_chan *chan, struct dma_slave_config *cfg)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nif (cfg->src_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES ||\r\ncfg->dst_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES)\r\nreturn -EINVAL;\r\nmemcpy(&c->cfg, cfg, sizeof(c->cfg));\r\nreturn 0;\r\n}\r\nstatic int omap_dma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (c->desc) {\r\nomap_dma_desc_free(&c->desc->vd);\r\nc->desc = NULL;\r\nif (!c->paused)\r\nomap_dma_stop(c);\r\n}\r\nc->cyclic = false;\r\nc->paused = false;\r\nvchan_get_all_descriptors(&c->vc, &head);\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nvchan_dma_desc_free_list(&c->vc, &head);\r\nreturn 0;\r\n}\r\nstatic void omap_dma_synchronize(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nvchan_synchronize(&c->vc);\r\n}\r\nstatic int omap_dma_pause(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nunsigned long flags;\r\nint ret = -EINVAL;\r\nbool can_pause = false;\r\nspin_lock_irqsave(&od->irq_lock, flags);\r\nif (!c->desc)\r\ngoto out;\r\nif (c->cyclic)\r\ncan_pause = true;\r\nelse if (c->desc->dir == DMA_DEV_TO_MEM)\r\ncan_pause = true;\r\nif (can_pause && !c->paused) {\r\nret = omap_dma_stop(c);\r\nif (!ret)\r\nc->paused = true;\r\n}\r\nout:\r\nspin_unlock_irqrestore(&od->irq_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int omap_dma_resume(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nunsigned long flags;\r\nint ret = -EINVAL;\r\nspin_lock_irqsave(&od->irq_lock, flags);\r\nif (c->paused && c->desc) {\r\nmb();\r\nomap_dma_chan_write(c, CLNK_CTRL, c->desc->clnk_ctrl);\r\nomap_dma_start(c, c->desc);\r\nc->paused = false;\r\nret = 0;\r\n}\r\nspin_unlock_irqrestore(&od->irq_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int omap_dma_chan_init(struct omap_dmadev *od)\r\n{\r\nstruct omap_chan *c;\r\nc = kzalloc(sizeof(*c), GFP_KERNEL);\r\nif (!c)\r\nreturn -ENOMEM;\r\nc->reg_map = od->reg_map;\r\nc->vc.desc_free = omap_dma_desc_free;\r\nvchan_init(&c->vc, &od->ddev);\r\nreturn 0;\r\n}\r\nstatic void omap_dma_free(struct omap_dmadev *od)\r\n{\r\nwhile (!list_empty(&od->ddev.channels)) {\r\nstruct omap_chan *c = list_first_entry(&od->ddev.channels,\r\nstruct omap_chan, vc.chan.device_node);\r\nlist_del(&c->vc.chan.device_node);\r\ntasklet_kill(&c->vc.task);\r\nkfree(c);\r\n}\r\n}\r\nstatic int omap_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct omap_dmadev *od;\r\nstruct resource *res;\r\nint rc, i, irq;\r\nu32 lch_count;\r\nod = devm_kzalloc(&pdev->dev, sizeof(*od), GFP_KERNEL);\r\nif (!od)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nod->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(od->base))\r\nreturn PTR_ERR(od->base);\r\nod->plat = omap_get_plat_info();\r\nif (!od->plat)\r\nreturn -EPROBE_DEFER;\r\nod->reg_map = od->plat->reg_map;\r\ndma_cap_set(DMA_SLAVE, od->ddev.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, od->ddev.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, od->ddev.cap_mask);\r\ndma_cap_set(DMA_INTERLEAVE, od->ddev.cap_mask);\r\nod->ddev.device_alloc_chan_resources = omap_dma_alloc_chan_resources;\r\nod->ddev.device_free_chan_resources = omap_dma_free_chan_resources;\r\nod->ddev.device_tx_status = omap_dma_tx_status;\r\nod->ddev.device_issue_pending = omap_dma_issue_pending;\r\nod->ddev.device_prep_slave_sg = omap_dma_prep_slave_sg;\r\nod->ddev.device_prep_dma_cyclic = omap_dma_prep_dma_cyclic;\r\nod->ddev.device_prep_dma_memcpy = omap_dma_prep_dma_memcpy;\r\nod->ddev.device_prep_interleaved_dma = omap_dma_prep_dma_interleaved;\r\nod->ddev.device_config = omap_dma_slave_config;\r\nod->ddev.device_pause = omap_dma_pause;\r\nod->ddev.device_resume = omap_dma_resume;\r\nod->ddev.device_terminate_all = omap_dma_terminate_all;\r\nod->ddev.device_synchronize = omap_dma_synchronize;\r\nod->ddev.src_addr_widths = OMAP_DMA_BUSWIDTHS;\r\nod->ddev.dst_addr_widths = OMAP_DMA_BUSWIDTHS;\r\nod->ddev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nod->ddev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\nod->ddev.dev = &pdev->dev;\r\nINIT_LIST_HEAD(&od->ddev.channels);\r\nspin_lock_init(&od->lock);\r\nspin_lock_init(&od->irq_lock);\r\nod->dma_requests = OMAP_SDMA_REQUESTS;\r\nif (pdev->dev.of_node && of_property_read_u32(pdev->dev.of_node,\r\n"dma-requests",\r\n&od->dma_requests)) {\r\ndev_info(&pdev->dev,\r\n"Missing dma-requests property, using %u.\n",\r\nOMAP_SDMA_REQUESTS);\r\n}\r\nif (!pdev->dev.of_node) {\r\nlch_count = od->plat->dma_attr->lch_count;\r\nif (unlikely(!lch_count))\r\nlch_count = OMAP_SDMA_CHANNELS;\r\n} else if (of_property_read_u32(pdev->dev.of_node, "dma-channels",\r\n&lch_count)) {\r\ndev_info(&pdev->dev,\r\n"Missing dma-channels property, using %u.\n",\r\nOMAP_SDMA_CHANNELS);\r\nlch_count = OMAP_SDMA_CHANNELS;\r\n}\r\nod->lch_map = devm_kcalloc(&pdev->dev, lch_count, sizeof(*od->lch_map),\r\nGFP_KERNEL);\r\nif (!od->lch_map)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < od->dma_requests; i++) {\r\nrc = omap_dma_chan_init(od);\r\nif (rc) {\r\nomap_dma_free(od);\r\nreturn rc;\r\n}\r\n}\r\nirq = platform_get_irq(pdev, 1);\r\nif (irq <= 0) {\r\ndev_info(&pdev->dev, "failed to get L1 IRQ: %d\n", irq);\r\nod->legacy = true;\r\n} else {\r\nod->irq_enable_mask = 0;\r\nomap_dma_glbl_write(od, IRQENABLE_L1, 0);\r\nrc = devm_request_irq(&pdev->dev, irq, omap_dma_irq,\r\nIRQF_SHARED, "omap-dma-engine", od);\r\nif (rc)\r\nreturn rc;\r\n}\r\nif (omap_dma_glbl_read(od, CAPS_0) & CAPS_0_SUPPORT_LL123)\r\nod->ll123_supported = true;\r\nod->ddev.filter.map = od->plat->slave_map;\r\nod->ddev.filter.mapcnt = od->plat->slavecnt;\r\nod->ddev.filter.fn = omap_dma_filter_fn;\r\nif (od->ll123_supported) {\r\nod->desc_pool = dma_pool_create(dev_name(&pdev->dev),\r\n&pdev->dev,\r\nsizeof(struct omap_type2_desc),\r\n4, 0);\r\nif (!od->desc_pool) {\r\ndev_err(&pdev->dev,\r\n"unable to allocate descriptor pool\n");\r\nod->ll123_supported = false;\r\n}\r\n}\r\nrc = dma_async_device_register(&od->ddev);\r\nif (rc) {\r\npr_warn("OMAP-DMA: failed to register slave DMA engine device: %d\n",\r\nrc);\r\nomap_dma_free(od);\r\nreturn rc;\r\n}\r\nplatform_set_drvdata(pdev, od);\r\nif (pdev->dev.of_node) {\r\nomap_dma_info.dma_cap = od->ddev.cap_mask;\r\nrc = of_dma_controller_register(pdev->dev.of_node,\r\nof_dma_simple_xlate, &omap_dma_info);\r\nif (rc) {\r\npr_warn("OMAP-DMA: failed to register DMA controller\n");\r\ndma_async_device_unregister(&od->ddev);\r\nomap_dma_free(od);\r\n}\r\n}\r\ndev_info(&pdev->dev, "OMAP DMA engine driver%s\n",\r\nod->ll123_supported ? " (LinkedList1/2/3 supported)" : "");\r\nreturn rc;\r\n}\r\nstatic int omap_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct omap_dmadev *od = platform_get_drvdata(pdev);\r\nint irq;\r\nif (pdev->dev.of_node)\r\nof_dma_controller_free(pdev->dev.of_node);\r\nirq = platform_get_irq(pdev, 1);\r\ndevm_free_irq(&pdev->dev, irq, od);\r\ndma_async_device_unregister(&od->ddev);\r\nif (!od->legacy) {\r\nomap_dma_glbl_write(od, IRQENABLE_L0, 0);\r\n}\r\nif (od->ll123_supported)\r\ndma_pool_destroy(od->desc_pool);\r\nomap_dma_free(od);\r\nreturn 0;\r\n}\r\nbool omap_dma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nif (chan->device->dev->driver == &omap_dma_driver.driver) {\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nunsigned req = *(unsigned *)param;\r\nif (req <= od->dma_requests) {\r\nc->dma_sig = req;\r\nreturn true;\r\n}\r\n}\r\nreturn false;\r\n}\r\nstatic int omap_dma_init(void)\r\n{\r\nreturn platform_driver_register(&omap_dma_driver);\r\n}\r\nstatic void __exit omap_dma_exit(void)\r\n{\r\nplatform_driver_unregister(&omap_dma_driver);\r\n}
