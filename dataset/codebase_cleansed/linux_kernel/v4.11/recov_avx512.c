static int raid6_has_avx512(void)\r\n{\r\nreturn boot_cpu_has(X86_FEATURE_AVX2) &&\r\nboot_cpu_has(X86_FEATURE_AVX) &&\r\nboot_cpu_has(X86_FEATURE_AVX512F) &&\r\nboot_cpu_has(X86_FEATURE_AVX512BW) &&\r\nboot_cpu_has(X86_FEATURE_AVX512VL) &&\r\nboot_cpu_has(X86_FEATURE_AVX512DQ);\r\n}\r\nstatic void raid6_2data_recov_avx512(int disks, size_t bytes, int faila,\r\nint failb, void **ptrs)\r\n{\r\nu8 *p, *q, *dp, *dq;\r\nconst u8 *pbmul;\r\nconst u8 *qmul;\r\nconst u8 x0f = 0x0f;\r\np = (u8 *)ptrs[disks-2];\r\nq = (u8 *)ptrs[disks-1];\r\ndp = (u8 *)ptrs[faila];\r\nptrs[faila] = (void *)raid6_empty_zero_page;\r\nptrs[disks-2] = dp;\r\ndq = (u8 *)ptrs[failb];\r\nptrs[failb] = (void *)raid6_empty_zero_page;\r\nptrs[disks-1] = dq;\r\nraid6_call.gen_syndrome(disks, bytes, ptrs);\r\nptrs[faila] = dp;\r\nptrs[failb] = dq;\r\nptrs[disks-2] = p;\r\nptrs[disks-1] = q;\r\npbmul = raid6_vgfmul[raid6_gfexi[failb-faila]];\r\nqmul = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila] ^\r\nraid6_gfexp[failb]]];\r\nkernel_fpu_begin();\r\nasm volatile("vpbroadcastb %0, %%zmm7" : : "m" (x0f));\r\nwhile (bytes) {\r\n#ifdef CONFIG_X86_64\r\nasm volatile("vmovdqa64 %0, %%zmm1\n\t"\r\n"vmovdqa64 %1, %%zmm9\n\t"\r\n"vmovdqa64 %2, %%zmm0\n\t"\r\n"vmovdqa64 %3, %%zmm8\n\t"\r\n"vpxorq %4, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %5, %%zmm9, %%zmm9\n\t"\r\n"vpxorq %6, %%zmm0, %%zmm0\n\t"\r\n"vpxorq %7, %%zmm8, %%zmm8"\r\n:\r\n: "m" (q[0]), "m" (q[64]), "m" (p[0]),\r\n"m" (p[64]), "m" (dq[0]), "m" (dq[64]),\r\n"m" (dp[0]), "m" (dp[64]));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm5"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm1, %%zmm3\n\t"\r\n"vpsraw $4, %%zmm9, %%zmm12\n\t"\r\n"vpandq %%zmm7, %%zmm1, %%zmm1\n\t"\r\n"vpandq %%zmm7, %%zmm9, %%zmm9\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm12, %%zmm12\n\t"\r\n"vpshufb %%zmm9, %%zmm4, %%zmm14\n\t"\r\n"vpshufb %%zmm1, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm12, %%zmm5, %%zmm15\n\t"\r\n"vpshufb %%zmm3, %%zmm5, %%zmm5\n\t"\r\n"vpxorq %%zmm14, %%zmm15, %%zmm15\n\t"\r\n"vpxorq %%zmm4, %%zmm5, %%zmm5"\r\n:\r\n: );\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1\n\t"\r\n"vpsraw $4, %%zmm0, %%zmm2\n\t"\r\n"vpsraw $4, %%zmm8, %%zmm6\n\t"\r\n"vpandq %%zmm7, %%zmm0, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm8, %%zmm14\n\t"\r\n"vpandq %%zmm7, %%zmm2, %%zmm2\n\t"\r\n"vpandq %%zmm7, %%zmm6, %%zmm6\n\t"\r\n"vpshufb %%zmm14, %%zmm4, %%zmm12\n\t"\r\n"vpshufb %%zmm3, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm6, %%zmm1, %%zmm13\n\t"\r\n"vpshufb %%zmm2, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm4, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm12, %%zmm13, %%zmm13"\r\n:\r\n: "m" (pbmul[0]), "m" (pbmul[16]));\r\nasm volatile("vpxorq %%zmm5, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm15, %%zmm13, %%zmm13"\r\n:\r\n: );\r\nasm volatile("vmovdqa64 %%zmm1, %0\n\t"\r\n"vmovdqa64 %%zmm13,%1\n\t"\r\n"vpxorq %%zmm1, %%zmm0, %%zmm0\n\t"\r\n"vpxorq %%zmm13, %%zmm8, %%zmm8"\r\n:\r\n: "m" (dq[0]), "m" (dq[64]));\r\nasm volatile("vmovdqa64 %%zmm0, %0\n\t"\r\n"vmovdqa64 %%zmm8, %1"\r\n:\r\n: "m" (dp[0]), "m" (dp[64]));\r\nbytes -= 128;\r\np += 128;\r\nq += 128;\r\ndp += 128;\r\ndq += 128;\r\n#else\r\nasm volatile("vmovdqa64 %0, %%zmm1\n\t"\r\n"vmovdqa64 %1, %%zmm0\n\t"\r\n"vpxorq %2, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %3, %%zmm0, %%zmm0"\r\n:\r\n: "m" (*q), "m" (*p), "m"(*dq), "m" (*dp));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm5"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm1, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm1, %%zmm1\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpshufb %%zmm1, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm3, %%zmm5, %%zmm5\n\t"\r\n"vpxorq %%zmm4, %%zmm5, %%zmm5"\r\n:\r\n: );\r\nasm volatile("vbroadcasti64x2 %0, %%zmm4\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1"\r\n:\r\n: "m" (pbmul[0]), "m" (pbmul[16]));\r\nasm volatile("vpsraw $4, %%zmm0, %%zmm2\n\t"\r\n"vpandq %%zmm7, %%zmm0, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm2, %%zmm2\n\t"\r\n"vpshufb %%zmm3, %%zmm4, %%zmm4\n\t"\r\n"vpshufb %%zmm2, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm4, %%zmm1, %%zmm1"\r\n:\r\n: );\r\nasm volatile("vpxorq %%zmm5, %%zmm1, %%zmm1\n\t"\r\n"vmovdqa64 %%zmm1, %0\n\t"\r\n:\r\n: "m" (dq[0]));\r\nasm volatile("vpxorq %%zmm1, %%zmm0, %%zmm0\n\t"\r\n"vmovdqa64 %%zmm0, %0"\r\n:\r\n: "m" (dp[0]));\r\nbytes -= 64;\r\np += 64;\r\nq += 64;\r\ndp += 64;\r\ndq += 64;\r\n#endif\r\n}\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_datap_recov_avx512(int disks, size_t bytes, int faila,\r\nvoid **ptrs)\r\n{\r\nu8 *p, *q, *dq;\r\nconst u8 *qmul;\r\nconst u8 x0f = 0x0f;\r\np = (u8 *)ptrs[disks-2];\r\nq = (u8 *)ptrs[disks-1];\r\ndq = (u8 *)ptrs[faila];\r\nptrs[faila] = (void *)raid6_empty_zero_page;\r\nptrs[disks-1] = dq;\r\nraid6_call.gen_syndrome(disks, bytes, ptrs);\r\nptrs[faila] = dq;\r\nptrs[disks-1] = q;\r\nqmul = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila]]];\r\nkernel_fpu_begin();\r\nasm volatile("vpbroadcastb %0, %%zmm7" : : "m" (x0f));\r\nwhile (bytes) {\r\n#ifdef CONFIG_X86_64\r\nasm volatile("vmovdqa64 %0, %%zmm3\n\t"\r\n"vmovdqa64 %1, %%zmm8\n\t"\r\n"vpxorq %2, %%zmm3, %%zmm3\n\t"\r\n"vpxorq %3, %%zmm8, %%zmm8"\r\n:\r\n: "m" (dq[0]), "m" (dq[64]), "m" (q[0]),\r\n"m" (q[64]));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm0\n\t"\r\n"vmovapd %%zmm0, %%zmm13\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1\n\t"\r\n"vmovapd %%zmm1, %%zmm14"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm3, %%zmm6\n\t"\r\n"vpsraw $4, %%zmm8, %%zmm12\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm8, %%zmm8\n\t"\r\n"vpandq %%zmm7, %%zmm6, %%zmm6\n\t"\r\n"vpandq %%zmm7, %%zmm12, %%zmm12\n\t"\r\n"vpshufb %%zmm3, %%zmm0, %%zmm0\n\t"\r\n"vpshufb %%zmm8, %%zmm13, %%zmm13\n\t"\r\n"vpshufb %%zmm6, %%zmm1, %%zmm1\n\t"\r\n"vpshufb %%zmm12, %%zmm14, %%zmm14\n\t"\r\n"vpxorq %%zmm0, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm13, %%zmm14, %%zmm14"\r\n:\r\n: );\r\nasm volatile("vmovdqa64 %0, %%zmm2\n\t"\r\n"vmovdqa64 %1, %%zmm12\n\t"\r\n"vpxorq %%zmm1, %%zmm2, %%zmm2\n\t"\r\n"vpxorq %%zmm14, %%zmm12, %%zmm12"\r\n:\r\n: "m" (p[0]), "m" (p[64]));\r\nasm volatile("vmovdqa64 %%zmm1, %0\n\t"\r\n"vmovdqa64 %%zmm14, %1\n\t"\r\n"vmovdqa64 %%zmm2, %2\n\t"\r\n"vmovdqa64 %%zmm12,%3"\r\n:\r\n: "m" (dq[0]), "m" (dq[64]), "m" (p[0]),\r\n"m" (p[64]));\r\nbytes -= 128;\r\np += 128;\r\nq += 128;\r\ndq += 128;\r\n#else\r\nasm volatile("vmovdqa64 %0, %%zmm3\n\t"\r\n"vpxorq %1, %%zmm3, %%zmm3"\r\n:\r\n: "m" (dq[0]), "m" (q[0]));\r\nasm volatile("vbroadcasti64x2 %0, %%zmm0\n\t"\r\n"vbroadcasti64x2 %1, %%zmm1"\r\n:\r\n: "m" (qmul[0]), "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %%zmm3, %%zmm6\n\t"\r\n"vpandq %%zmm7, %%zmm3, %%zmm3\n\t"\r\n"vpandq %%zmm7, %%zmm6, %%zmm6\n\t"\r\n"vpshufb %%zmm3, %%zmm0, %%zmm0\n\t"\r\n"vpshufb %%zmm6, %%zmm1, %%zmm1\n\t"\r\n"vpxorq %%zmm0, %%zmm1, %%zmm1"\r\n:\r\n: );\r\nasm volatile("vmovdqa64 %0, %%zmm2\n\t"\r\n"vpxorq %%zmm1, %%zmm2, %%zmm2"\r\n:\r\n: "m" (p[0]));\r\nasm volatile("vmovdqa64 %%zmm1, %0\n\t"\r\n"vmovdqa64 %%zmm2, %1"\r\n:\r\n: "m" (dq[0]), "m" (p[0]));\r\nbytes -= 64;\r\np += 64;\r\nq += 64;\r\ndq += 64;\r\n#endif\r\n}\r\nkernel_fpu_end();\r\n}
