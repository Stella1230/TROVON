static inline void __iomem *at_xdmac_chan_reg_base(struct at_xdmac *atxdmac, unsigned int chan_nb)\r\n{\r\nreturn atxdmac->regs + (AT_XDMAC_CHAN_REG_BASE + chan_nb * 0x40);\r\n}\r\nstatic inline struct at_xdmac_chan *to_at_xdmac_chan(struct dma_chan *dchan)\r\n{\r\nreturn container_of(dchan, struct at_xdmac_chan, chan);\r\n}\r\nstatic struct device *chan2dev(struct dma_chan *chan)\r\n{\r\nreturn &chan->dev->device;\r\n}\r\nstatic inline struct at_xdmac *to_at_xdmac(struct dma_device *ddev)\r\n{\r\nreturn container_of(ddev, struct at_xdmac, dma);\r\n}\r\nstatic inline struct at_xdmac_desc *txd_to_at_desc(struct dma_async_tx_descriptor *txd)\r\n{\r\nreturn container_of(txd, struct at_xdmac_desc, tx_dma_desc);\r\n}\r\nstatic inline int at_xdmac_chan_is_cyclic(struct at_xdmac_chan *atchan)\r\n{\r\nreturn test_bit(AT_XDMAC_CHAN_IS_CYCLIC, &atchan->status);\r\n}\r\nstatic inline int at_xdmac_chan_is_paused(struct at_xdmac_chan *atchan)\r\n{\r\nreturn test_bit(AT_XDMAC_CHAN_IS_PAUSED, &atchan->status);\r\n}\r\nstatic inline int at_xdmac_csize(u32 maxburst)\r\n{\r\nint csize;\r\ncsize = ffs(maxburst) - 1;\r\nif (csize > 4)\r\ncsize = -EINVAL;\r\nreturn csize;\r\n}\r\nstatic inline u8 at_xdmac_get_dwidth(u32 cfg)\r\n{\r\nreturn (cfg & AT_XDMAC_CC_DWIDTH_MASK) >> AT_XDMAC_CC_DWIDTH_OFFSET;\r\n}\r\nstatic bool at_xdmac_chan_is_enabled(struct at_xdmac_chan *atchan)\r\n{\r\nreturn at_xdmac_chan_read(atchan, AT_XDMAC_GS) & atchan->mask;\r\n}\r\nstatic void at_xdmac_off(struct at_xdmac *atxdmac)\r\n{\r\nat_xdmac_write(atxdmac, AT_XDMAC_GD, -1L);\r\nwhile (at_xdmac_read(atxdmac, AT_XDMAC_GS))\r\ncpu_relax();\r\nat_xdmac_write(atxdmac, AT_XDMAC_GID, -1L);\r\n}\r\nstatic void at_xdmac_start_xfer(struct at_xdmac_chan *atchan,\r\nstruct at_xdmac_desc *first)\r\n{\r\nstruct at_xdmac *atxdmac = to_at_xdmac(atchan->chan.device);\r\nu32 reg;\r\ndev_vdbg(chan2dev(&atchan->chan), "%s: desc 0x%p\n", __func__, first);\r\nif (at_xdmac_chan_is_enabled(atchan))\r\nreturn;\r\nfirst->active_xfer = true;\r\nreg = AT_XDMAC_CNDA_NDA(first->tx_dma_desc.phys)\r\n| AT_XDMAC_CNDA_NDAIF(atchan->memif);\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CNDA, reg);\r\nif (at_xdmac_chan_is_cyclic(atchan))\r\nreg = AT_XDMAC_CNDC_NDVIEW_NDV1;\r\nelse if (first->lld.mbr_ubc & AT_XDMAC_MBR_UBC_NDV3)\r\nreg = AT_XDMAC_CNDC_NDVIEW_NDV3;\r\nelse\r\nreg = AT_XDMAC_CNDC_NDVIEW_NDV2;\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CC, first->lld.mbr_cfg);\r\nreg |= AT_XDMAC_CNDC_NDDUP\r\n| AT_XDMAC_CNDC_NDSUP\r\n| AT_XDMAC_CNDC_NDE;\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CNDC, reg);\r\ndev_vdbg(chan2dev(&atchan->chan),\r\n"%s: CC=0x%08x CNDA=0x%08x, CNDC=0x%08x, CSA=0x%08x, CDA=0x%08x, CUBC=0x%08x\n",\r\n__func__, at_xdmac_chan_read(atchan, AT_XDMAC_CC),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CNDA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CNDC),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CSA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CDA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CUBC));\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CID, 0xffffffff);\r\nreg = AT_XDMAC_CIE_RBEIE | AT_XDMAC_CIE_WBEIE | AT_XDMAC_CIE_ROIE;\r\nif (at_xdmac_chan_is_cyclic(atchan))\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CIE,\r\nreg | AT_XDMAC_CIE_BIE);\r\nelse\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CIE,\r\nreg | AT_XDMAC_CIE_LIE);\r\nat_xdmac_write(atxdmac, AT_XDMAC_GIE, atchan->mask);\r\ndev_vdbg(chan2dev(&atchan->chan),\r\n"%s: enable channel (0x%08x)\n", __func__, atchan->mask);\r\nwmb();\r\nat_xdmac_write(atxdmac, AT_XDMAC_GE, atchan->mask);\r\ndev_vdbg(chan2dev(&atchan->chan),\r\n"%s: CC=0x%08x CNDA=0x%08x, CNDC=0x%08x, CSA=0x%08x, CDA=0x%08x, CUBC=0x%08x\n",\r\n__func__, at_xdmac_chan_read(atchan, AT_XDMAC_CC),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CNDA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CNDC),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CSA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CDA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CUBC));\r\n}\r\nstatic dma_cookie_t at_xdmac_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct at_xdmac_desc *desc = txd_to_at_desc(tx);\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nunsigned long irqflags;\r\nspin_lock_irqsave(&atchan->lock, irqflags);\r\ncookie = dma_cookie_assign(tx);\r\ndev_vdbg(chan2dev(tx->chan), "%s: atchan 0x%p, add desc 0x%p to xfers_list\n",\r\n__func__, atchan, desc);\r\nlist_add_tail(&desc->xfer_node, &atchan->xfers_list);\r\nif (list_is_singular(&atchan->xfers_list))\r\nat_xdmac_start_xfer(atchan, desc);\r\nspin_unlock_irqrestore(&atchan->lock, irqflags);\r\nreturn cookie;\r\n}\r\nstatic struct at_xdmac_desc *at_xdmac_alloc_desc(struct dma_chan *chan,\r\ngfp_t gfp_flags)\r\n{\r\nstruct at_xdmac_desc *desc;\r\nstruct at_xdmac *atxdmac = to_at_xdmac(chan->device);\r\ndma_addr_t phys;\r\ndesc = dma_pool_zalloc(atxdmac->at_xdmac_desc_pool, gfp_flags, &phys);\r\nif (desc) {\r\nINIT_LIST_HEAD(&desc->descs_list);\r\ndma_async_tx_descriptor_init(&desc->tx_dma_desc, chan);\r\ndesc->tx_dma_desc.tx_submit = at_xdmac_tx_submit;\r\ndesc->tx_dma_desc.phys = phys;\r\n}\r\nreturn desc;\r\n}\r\nstatic void at_xdmac_init_used_desc(struct at_xdmac_desc *desc)\r\n{\r\nmemset(&desc->lld, 0, sizeof(desc->lld));\r\nINIT_LIST_HEAD(&desc->descs_list);\r\ndesc->direction = DMA_TRANS_NONE;\r\ndesc->xfer_size = 0;\r\ndesc->active_xfer = false;\r\n}\r\nstatic struct at_xdmac_desc *at_xdmac_get_desc(struct at_xdmac_chan *atchan)\r\n{\r\nstruct at_xdmac_desc *desc;\r\nif (list_empty(&atchan->free_descs_list)) {\r\ndesc = at_xdmac_alloc_desc(&atchan->chan, GFP_NOWAIT);\r\n} else {\r\ndesc = list_first_entry(&atchan->free_descs_list,\r\nstruct at_xdmac_desc, desc_node);\r\nlist_del(&desc->desc_node);\r\nat_xdmac_init_used_desc(desc);\r\n}\r\nreturn desc;\r\n}\r\nstatic void at_xdmac_queue_desc(struct dma_chan *chan,\r\nstruct at_xdmac_desc *prev,\r\nstruct at_xdmac_desc *desc)\r\n{\r\nif (!prev || !desc)\r\nreturn;\r\nprev->lld.mbr_nda = desc->tx_dma_desc.phys;\r\nprev->lld.mbr_ubc |= AT_XDMAC_MBR_UBC_NDE;\r\ndev_dbg(chan2dev(chan), "%s: chain lld: prev=0x%p, mbr_nda=%pad\n",\r\n__func__, prev, &prev->lld.mbr_nda);\r\n}\r\nstatic inline void at_xdmac_increment_block_count(struct dma_chan *chan,\r\nstruct at_xdmac_desc *desc)\r\n{\r\nif (!desc)\r\nreturn;\r\ndesc->lld.mbr_bc++;\r\ndev_dbg(chan2dev(chan),\r\n"%s: incrementing the block count of the desc 0x%p\n",\r\n__func__, desc);\r\n}\r\nstatic struct dma_chan *at_xdmac_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *of_dma)\r\n{\r\nstruct at_xdmac *atxdmac = of_dma->of_dma_data;\r\nstruct at_xdmac_chan *atchan;\r\nstruct dma_chan *chan;\r\nstruct device *dev = atxdmac->dma.dev;\r\nif (dma_spec->args_count != 1) {\r\ndev_err(dev, "dma phandler args: bad number of args\n");\r\nreturn NULL;\r\n}\r\nchan = dma_get_any_slave_channel(&atxdmac->dma);\r\nif (!chan) {\r\ndev_err(dev, "can't get a dma channel\n");\r\nreturn NULL;\r\n}\r\natchan = to_at_xdmac_chan(chan);\r\natchan->memif = AT91_XDMAC_DT_GET_MEM_IF(dma_spec->args[0]);\r\natchan->perif = AT91_XDMAC_DT_GET_PER_IF(dma_spec->args[0]);\r\natchan->perid = AT91_XDMAC_DT_GET_PERID(dma_spec->args[0]);\r\ndev_dbg(dev, "chan dt cfg: memif=%u perif=%u perid=%u\n",\r\natchan->memif, atchan->perif, atchan->perid);\r\nreturn chan;\r\n}\r\nstatic int at_xdmac_compute_chan_conf(struct dma_chan *chan,\r\nenum dma_transfer_direction direction)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nint csize, dwidth;\r\nif (direction == DMA_DEV_TO_MEM) {\r\natchan->cfg =\r\nAT91_XDMAC_DT_PERID(atchan->perid)\r\n| AT_XDMAC_CC_DAM_INCREMENTED_AM\r\n| AT_XDMAC_CC_SAM_FIXED_AM\r\n| AT_XDMAC_CC_DIF(atchan->memif)\r\n| AT_XDMAC_CC_SIF(atchan->perif)\r\n| AT_XDMAC_CC_SWREQ_HWR_CONNECTED\r\n| AT_XDMAC_CC_DSYNC_PER2MEM\r\n| AT_XDMAC_CC_MBSIZE_SIXTEEN\r\n| AT_XDMAC_CC_TYPE_PER_TRAN;\r\ncsize = ffs(atchan->sconfig.src_maxburst) - 1;\r\nif (csize < 0) {\r\ndev_err(chan2dev(chan), "invalid src maxburst value\n");\r\nreturn -EINVAL;\r\n}\r\natchan->cfg |= AT_XDMAC_CC_CSIZE(csize);\r\ndwidth = ffs(atchan->sconfig.src_addr_width) - 1;\r\nif (dwidth < 0) {\r\ndev_err(chan2dev(chan), "invalid src addr width value\n");\r\nreturn -EINVAL;\r\n}\r\natchan->cfg |= AT_XDMAC_CC_DWIDTH(dwidth);\r\n} else if (direction == DMA_MEM_TO_DEV) {\r\natchan->cfg =\r\nAT91_XDMAC_DT_PERID(atchan->perid)\r\n| AT_XDMAC_CC_DAM_FIXED_AM\r\n| AT_XDMAC_CC_SAM_INCREMENTED_AM\r\n| AT_XDMAC_CC_DIF(atchan->perif)\r\n| AT_XDMAC_CC_SIF(atchan->memif)\r\n| AT_XDMAC_CC_SWREQ_HWR_CONNECTED\r\n| AT_XDMAC_CC_DSYNC_MEM2PER\r\n| AT_XDMAC_CC_MBSIZE_SIXTEEN\r\n| AT_XDMAC_CC_TYPE_PER_TRAN;\r\ncsize = ffs(atchan->sconfig.dst_maxburst) - 1;\r\nif (csize < 0) {\r\ndev_err(chan2dev(chan), "invalid src maxburst value\n");\r\nreturn -EINVAL;\r\n}\r\natchan->cfg |= AT_XDMAC_CC_CSIZE(csize);\r\ndwidth = ffs(atchan->sconfig.dst_addr_width) - 1;\r\nif (dwidth < 0) {\r\ndev_err(chan2dev(chan), "invalid dst addr width value\n");\r\nreturn -EINVAL;\r\n}\r\natchan->cfg |= AT_XDMAC_CC_DWIDTH(dwidth);\r\n}\r\ndev_dbg(chan2dev(chan), "%s: cfg=0x%08x\n", __func__, atchan->cfg);\r\nreturn 0;\r\n}\r\nstatic int at_xdmac_check_slave_config(struct dma_slave_config *sconfig)\r\n{\r\nif ((sconfig->src_maxburst > AT_XDMAC_MAX_CSIZE)\r\n|| (sconfig->dst_maxburst > AT_XDMAC_MAX_CSIZE))\r\nreturn -EINVAL;\r\nif ((sconfig->src_addr_width > AT_XDMAC_MAX_DWIDTH)\r\n|| (sconfig->dst_addr_width > AT_XDMAC_MAX_DWIDTH))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int at_xdmac_set_slave_config(struct dma_chan *chan,\r\nstruct dma_slave_config *sconfig)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nif (at_xdmac_check_slave_config(sconfig)) {\r\ndev_err(chan2dev(chan), "invalid slave configuration\n");\r\nreturn -EINVAL;\r\n}\r\nmemcpy(&atchan->sconfig, sconfig, sizeof(atchan->sconfig));\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nat_xdmac_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac_desc *first = NULL, *prev = NULL;\r\nstruct scatterlist *sg;\r\nint i;\r\nunsigned int xfer_size = 0;\r\nunsigned long irqflags;\r\nstruct dma_async_tx_descriptor *ret = NULL;\r\nif (!sgl)\r\nreturn NULL;\r\nif (!is_slave_direction(direction)) {\r\ndev_err(chan2dev(chan), "invalid DMA direction\n");\r\nreturn NULL;\r\n}\r\ndev_dbg(chan2dev(chan), "%s: sg_len=%d, dir=%s, flags=0x%lx\n",\r\n__func__, sg_len,\r\ndirection == DMA_MEM_TO_DEV ? "to device" : "from device",\r\nflags);\r\nspin_lock_irqsave(&atchan->lock, irqflags);\r\nif (at_xdmac_compute_chan_conf(chan, direction))\r\ngoto spin_unlock;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct at_xdmac_desc *desc = NULL;\r\nu32 len, mem, dwidth, fixed_dwidth;\r\nlen = sg_dma_len(sg);\r\nmem = sg_dma_address(sg);\r\nif (unlikely(!len)) {\r\ndev_err(chan2dev(chan), "sg data length is zero\n");\r\ngoto spin_unlock;\r\n}\r\ndev_dbg(chan2dev(chan), "%s: * sg%d len=%u, mem=0x%08x\n",\r\n__func__, i, len, mem);\r\ndesc = at_xdmac_get_desc(atchan);\r\nif (!desc) {\r\ndev_err(chan2dev(chan), "can't get descriptor\n");\r\nif (first)\r\nlist_splice_init(&first->descs_list, &atchan->free_descs_list);\r\ngoto spin_unlock;\r\n}\r\nif (direction == DMA_DEV_TO_MEM) {\r\ndesc->lld.mbr_sa = atchan->sconfig.src_addr;\r\ndesc->lld.mbr_da = mem;\r\n} else {\r\ndesc->lld.mbr_sa = mem;\r\ndesc->lld.mbr_da = atchan->sconfig.dst_addr;\r\n}\r\ndwidth = at_xdmac_get_dwidth(atchan->cfg);\r\nfixed_dwidth = IS_ALIGNED(len, 1 << dwidth)\r\n? dwidth\r\n: AT_XDMAC_CC_DWIDTH_BYTE;\r\ndesc->lld.mbr_ubc = AT_XDMAC_MBR_UBC_NDV2\r\n| AT_XDMAC_MBR_UBC_NDEN\r\n| AT_XDMAC_MBR_UBC_NSEN\r\n| (len >> fixed_dwidth);\r\ndesc->lld.mbr_cfg = (atchan->cfg & ~AT_XDMAC_CC_DWIDTH_MASK) |\r\nAT_XDMAC_CC_DWIDTH(fixed_dwidth);\r\ndev_dbg(chan2dev(chan),\r\n"%s: lld: mbr_sa=%pad, mbr_da=%pad, mbr_ubc=0x%08x\n",\r\n__func__, &desc->lld.mbr_sa, &desc->lld.mbr_da, desc->lld.mbr_ubc);\r\nif (prev)\r\nat_xdmac_queue_desc(chan, prev, desc);\r\nprev = desc;\r\nif (!first)\r\nfirst = desc;\r\ndev_dbg(chan2dev(chan), "%s: add desc 0x%p to descs_list 0x%p\n",\r\n__func__, desc, first);\r\nlist_add_tail(&desc->desc_node, &first->descs_list);\r\nxfer_size += len;\r\n}\r\nfirst->tx_dma_desc.flags = flags;\r\nfirst->xfer_size = xfer_size;\r\nfirst->direction = direction;\r\nret = &first->tx_dma_desc;\r\nspin_unlock:\r\nspin_unlock_irqrestore(&atchan->lock, irqflags);\r\nreturn ret;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nat_xdmac_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr,\r\nsize_t buf_len, size_t period_len,\r\nenum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac_desc *first = NULL, *prev = NULL;\r\nunsigned int periods = buf_len / period_len;\r\nint i;\r\nunsigned long irqflags;\r\ndev_dbg(chan2dev(chan), "%s: buf_addr=%pad, buf_len=%zd, period_len=%zd, dir=%s, flags=0x%lx\n",\r\n__func__, &buf_addr, buf_len, period_len,\r\ndirection == DMA_MEM_TO_DEV ? "mem2per" : "per2mem", flags);\r\nif (!is_slave_direction(direction)) {\r\ndev_err(chan2dev(chan), "invalid DMA direction\n");\r\nreturn NULL;\r\n}\r\nif (test_and_set_bit(AT_XDMAC_CHAN_IS_CYCLIC, &atchan->status)) {\r\ndev_err(chan2dev(chan), "channel currently used\n");\r\nreturn NULL;\r\n}\r\nif (at_xdmac_compute_chan_conf(chan, direction))\r\nreturn NULL;\r\nfor (i = 0; i < periods; i++) {\r\nstruct at_xdmac_desc *desc = NULL;\r\nspin_lock_irqsave(&atchan->lock, irqflags);\r\ndesc = at_xdmac_get_desc(atchan);\r\nif (!desc) {\r\ndev_err(chan2dev(chan), "can't get descriptor\n");\r\nif (first)\r\nlist_splice_init(&first->descs_list, &atchan->free_descs_list);\r\nspin_unlock_irqrestore(&atchan->lock, irqflags);\r\nreturn NULL;\r\n}\r\nspin_unlock_irqrestore(&atchan->lock, irqflags);\r\ndev_dbg(chan2dev(chan),\r\n"%s: desc=0x%p, tx_dma_desc.phys=%pad\n",\r\n__func__, desc, &desc->tx_dma_desc.phys);\r\nif (direction == DMA_DEV_TO_MEM) {\r\ndesc->lld.mbr_sa = atchan->sconfig.src_addr;\r\ndesc->lld.mbr_da = buf_addr + i * period_len;\r\n} else {\r\ndesc->lld.mbr_sa = buf_addr + i * period_len;\r\ndesc->lld.mbr_da = atchan->sconfig.dst_addr;\r\n}\r\ndesc->lld.mbr_cfg = atchan->cfg;\r\ndesc->lld.mbr_ubc = AT_XDMAC_MBR_UBC_NDV1\r\n| AT_XDMAC_MBR_UBC_NDEN\r\n| AT_XDMAC_MBR_UBC_NSEN\r\n| period_len >> at_xdmac_get_dwidth(desc->lld.mbr_cfg);\r\ndev_dbg(chan2dev(chan),\r\n"%s: lld: mbr_sa=%pad, mbr_da=%pad, mbr_ubc=0x%08x\n",\r\n__func__, &desc->lld.mbr_sa, &desc->lld.mbr_da, desc->lld.mbr_ubc);\r\nif (prev)\r\nat_xdmac_queue_desc(chan, prev, desc);\r\nprev = desc;\r\nif (!first)\r\nfirst = desc;\r\ndev_dbg(chan2dev(chan), "%s: add desc 0x%p to descs_list 0x%p\n",\r\n__func__, desc, first);\r\nlist_add_tail(&desc->desc_node, &first->descs_list);\r\n}\r\nat_xdmac_queue_desc(chan, prev, first);\r\nfirst->tx_dma_desc.flags = flags;\r\nfirst->xfer_size = buf_len;\r\nfirst->direction = direction;\r\nreturn &first->tx_dma_desc;\r\n}\r\nstatic inline u32 at_xdmac_align_width(struct dma_chan *chan, dma_addr_t addr)\r\n{\r\nu32 width;\r\nif (!(addr & 7)) {\r\nwidth = AT_XDMAC_CC_DWIDTH_DWORD;\r\ndev_dbg(chan2dev(chan), "%s: dwidth: double word\n", __func__);\r\n} else if (!(addr & 3)) {\r\nwidth = AT_XDMAC_CC_DWIDTH_WORD;\r\ndev_dbg(chan2dev(chan), "%s: dwidth: word\n", __func__);\r\n} else if (!(addr & 1)) {\r\nwidth = AT_XDMAC_CC_DWIDTH_HALFWORD;\r\ndev_dbg(chan2dev(chan), "%s: dwidth: half word\n", __func__);\r\n} else {\r\nwidth = AT_XDMAC_CC_DWIDTH_BYTE;\r\ndev_dbg(chan2dev(chan), "%s: dwidth: byte\n", __func__);\r\n}\r\nreturn width;\r\n}\r\nstatic struct at_xdmac_desc *\r\nat_xdmac_interleaved_queue_desc(struct dma_chan *chan,\r\nstruct at_xdmac_chan *atchan,\r\nstruct at_xdmac_desc *prev,\r\ndma_addr_t src, dma_addr_t dst,\r\nstruct dma_interleaved_template *xt,\r\nstruct data_chunk *chunk)\r\n{\r\nstruct at_xdmac_desc *desc;\r\nu32 dwidth;\r\nunsigned long flags;\r\nsize_t ublen;\r\nu32 chan_cc = AT_XDMAC_CC_PERID(0x3f)\r\n| AT_XDMAC_CC_DIF(0)\r\n| AT_XDMAC_CC_SIF(0)\r\n| AT_XDMAC_CC_MBSIZE_SIXTEEN\r\n| AT_XDMAC_CC_TYPE_MEM_TRAN;\r\ndwidth = at_xdmac_align_width(chan, src | dst | chunk->size);\r\nif (chunk->size >= (AT_XDMAC_MBR_UBC_UBLEN_MAX << dwidth)) {\r\ndev_dbg(chan2dev(chan),\r\n"%s: chunk too big (%d, max size %lu)...\n",\r\n__func__, chunk->size,\r\nAT_XDMAC_MBR_UBC_UBLEN_MAX << dwidth);\r\nreturn NULL;\r\n}\r\nif (prev)\r\ndev_dbg(chan2dev(chan),\r\n"Adding items at the end of desc 0x%p\n", prev);\r\nif (xt->src_inc) {\r\nif (xt->src_sgl)\r\nchan_cc |= AT_XDMAC_CC_SAM_UBS_AM;\r\nelse\r\nchan_cc |= AT_XDMAC_CC_SAM_INCREMENTED_AM;\r\n}\r\nif (xt->dst_inc) {\r\nif (xt->dst_sgl)\r\nchan_cc |= AT_XDMAC_CC_DAM_UBS_AM;\r\nelse\r\nchan_cc |= AT_XDMAC_CC_DAM_INCREMENTED_AM;\r\n}\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndesc = at_xdmac_get_desc(atchan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nif (!desc) {\r\ndev_err(chan2dev(chan), "can't get descriptor\n");\r\nreturn NULL;\r\n}\r\nchan_cc |= AT_XDMAC_CC_DWIDTH(dwidth);\r\nublen = chunk->size >> dwidth;\r\ndesc->lld.mbr_sa = src;\r\ndesc->lld.mbr_da = dst;\r\ndesc->lld.mbr_sus = dmaengine_get_src_icg(xt, chunk);\r\ndesc->lld.mbr_dus = dmaengine_get_dst_icg(xt, chunk);\r\ndesc->lld.mbr_ubc = AT_XDMAC_MBR_UBC_NDV3\r\n| AT_XDMAC_MBR_UBC_NDEN\r\n| AT_XDMAC_MBR_UBC_NSEN\r\n| ublen;\r\ndesc->lld.mbr_cfg = chan_cc;\r\ndev_dbg(chan2dev(chan),\r\n"%s: lld: mbr_sa=%pad, mbr_da=%pad, mbr_ubc=0x%08x, mbr_cfg=0x%08x\n",\r\n__func__, &desc->lld.mbr_sa, &desc->lld.mbr_da,\r\ndesc->lld.mbr_ubc, desc->lld.mbr_cfg);\r\nif (prev)\r\nat_xdmac_queue_desc(chan, prev, desc);\r\nreturn desc;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nat_xdmac_prep_interleaved(struct dma_chan *chan,\r\nstruct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac_desc *prev = NULL, *first = NULL;\r\ndma_addr_t dst_addr, src_addr;\r\nsize_t src_skip = 0, dst_skip = 0, len = 0;\r\nstruct data_chunk *chunk;\r\nint i;\r\nif (!xt || !xt->numf || (xt->dir != DMA_MEM_TO_MEM))\r\nreturn NULL;\r\nif ((xt->numf > 1) && (xt->frame_size > 1))\r\nreturn NULL;\r\ndev_dbg(chan2dev(chan), "%s: src=%pad, dest=%pad, numf=%d, frame_size=%d, flags=0x%lx\n",\r\n__func__, &xt->src_start, &xt->dst_start, xt->numf,\r\nxt->frame_size, flags);\r\nsrc_addr = xt->src_start;\r\ndst_addr = xt->dst_start;\r\nif (xt->numf > 1) {\r\nfirst = at_xdmac_interleaved_queue_desc(chan, atchan,\r\nNULL,\r\nsrc_addr, dst_addr,\r\nxt, xt->sgl);\r\nfor (i = 0; i < xt->numf - 1; i++)\r\nat_xdmac_increment_block_count(chan, first);\r\ndev_dbg(chan2dev(chan), "%s: add desc 0x%p to descs_list 0x%p\n",\r\n__func__, first, first);\r\nlist_add_tail(&first->desc_node, &first->descs_list);\r\n} else {\r\nfor (i = 0; i < xt->frame_size; i++) {\r\nsize_t src_icg = 0, dst_icg = 0;\r\nstruct at_xdmac_desc *desc;\r\nchunk = xt->sgl + i;\r\ndst_icg = dmaengine_get_dst_icg(xt, chunk);\r\nsrc_icg = dmaengine_get_src_icg(xt, chunk);\r\nsrc_skip = chunk->size + src_icg;\r\ndst_skip = chunk->size + dst_icg;\r\ndev_dbg(chan2dev(chan),\r\n"%s: chunk size=%d, src icg=%d, dst icg=%d\n",\r\n__func__, chunk->size, src_icg, dst_icg);\r\ndesc = at_xdmac_interleaved_queue_desc(chan, atchan,\r\nprev,\r\nsrc_addr, dst_addr,\r\nxt, chunk);\r\nif (!desc) {\r\nlist_splice_init(&first->descs_list,\r\n&atchan->free_descs_list);\r\nreturn NULL;\r\n}\r\nif (!first)\r\nfirst = desc;\r\ndev_dbg(chan2dev(chan), "%s: add desc 0x%p to descs_list 0x%p\n",\r\n__func__, desc, first);\r\nlist_add_tail(&desc->desc_node, &first->descs_list);\r\nif (xt->src_sgl)\r\nsrc_addr += src_skip;\r\nif (xt->dst_sgl)\r\ndst_addr += dst_skip;\r\nlen += chunk->size;\r\nprev = desc;\r\n}\r\n}\r\nfirst->tx_dma_desc.cookie = -EBUSY;\r\nfirst->tx_dma_desc.flags = flags;\r\nfirst->xfer_size = len;\r\nreturn &first->tx_dma_desc;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nat_xdmac_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac_desc *first = NULL, *prev = NULL;\r\nsize_t remaining_size = len, xfer_size = 0, ublen;\r\ndma_addr_t src_addr = src, dst_addr = dest;\r\nu32 dwidth;\r\nu32 chan_cc = AT_XDMAC_CC_PERID(0x3f)\r\n| AT_XDMAC_CC_DAM_INCREMENTED_AM\r\n| AT_XDMAC_CC_SAM_INCREMENTED_AM\r\n| AT_XDMAC_CC_DIF(0)\r\n| AT_XDMAC_CC_SIF(0)\r\n| AT_XDMAC_CC_MBSIZE_SIXTEEN\r\n| AT_XDMAC_CC_TYPE_MEM_TRAN;\r\nunsigned long irqflags;\r\ndev_dbg(chan2dev(chan), "%s: src=%pad, dest=%pad, len=%zd, flags=0x%lx\n",\r\n__func__, &src, &dest, len, flags);\r\nif (unlikely(!len))\r\nreturn NULL;\r\ndwidth = at_xdmac_align_width(chan, src_addr | dst_addr);\r\nwhile (remaining_size) {\r\nstruct at_xdmac_desc *desc = NULL;\r\ndev_dbg(chan2dev(chan), "%s: remaining_size=%zu\n", __func__, remaining_size);\r\nspin_lock_irqsave(&atchan->lock, irqflags);\r\ndesc = at_xdmac_get_desc(atchan);\r\nspin_unlock_irqrestore(&atchan->lock, irqflags);\r\nif (!desc) {\r\ndev_err(chan2dev(chan), "can't get descriptor\n");\r\nif (first)\r\nlist_splice_init(&first->descs_list, &atchan->free_descs_list);\r\nreturn NULL;\r\n}\r\nsrc_addr += xfer_size;\r\ndst_addr += xfer_size;\r\nif (remaining_size >= AT_XDMAC_MBR_UBC_UBLEN_MAX << dwidth)\r\nxfer_size = AT_XDMAC_MBR_UBC_UBLEN_MAX << dwidth;\r\nelse\r\nxfer_size = remaining_size;\r\ndev_dbg(chan2dev(chan), "%s: xfer_size=%zu\n", __func__, xfer_size);\r\ndwidth = at_xdmac_align_width(chan,\r\nsrc_addr | dst_addr | xfer_size);\r\nchan_cc &= ~AT_XDMAC_CC_DWIDTH_MASK;\r\nchan_cc |= AT_XDMAC_CC_DWIDTH(dwidth);\r\nublen = xfer_size >> dwidth;\r\nremaining_size -= xfer_size;\r\ndesc->lld.mbr_sa = src_addr;\r\ndesc->lld.mbr_da = dst_addr;\r\ndesc->lld.mbr_ubc = AT_XDMAC_MBR_UBC_NDV2\r\n| AT_XDMAC_MBR_UBC_NDEN\r\n| AT_XDMAC_MBR_UBC_NSEN\r\n| ublen;\r\ndesc->lld.mbr_cfg = chan_cc;\r\ndev_dbg(chan2dev(chan),\r\n"%s: lld: mbr_sa=%pad, mbr_da=%pad, mbr_ubc=0x%08x, mbr_cfg=0x%08x\n",\r\n__func__, &desc->lld.mbr_sa, &desc->lld.mbr_da, desc->lld.mbr_ubc, desc->lld.mbr_cfg);\r\nif (prev)\r\nat_xdmac_queue_desc(chan, prev, desc);\r\nprev = desc;\r\nif (!first)\r\nfirst = desc;\r\ndev_dbg(chan2dev(chan), "%s: add desc 0x%p to descs_list 0x%p\n",\r\n__func__, desc, first);\r\nlist_add_tail(&desc->desc_node, &first->descs_list);\r\n}\r\nfirst->tx_dma_desc.flags = flags;\r\nfirst->xfer_size = len;\r\nreturn &first->tx_dma_desc;\r\n}\r\nstatic struct at_xdmac_desc *at_xdmac_memset_create_desc(struct dma_chan *chan,\r\nstruct at_xdmac_chan *atchan,\r\ndma_addr_t dst_addr,\r\nsize_t len,\r\nint value)\r\n{\r\nstruct at_xdmac_desc *desc;\r\nunsigned long flags;\r\nsize_t ublen;\r\nu32 dwidth;\r\nu32 chan_cc = AT_XDMAC_CC_PERID(0x3f)\r\n| AT_XDMAC_CC_DAM_UBS_AM\r\n| AT_XDMAC_CC_SAM_INCREMENTED_AM\r\n| AT_XDMAC_CC_DIF(0)\r\n| AT_XDMAC_CC_SIF(0)\r\n| AT_XDMAC_CC_MBSIZE_SIXTEEN\r\n| AT_XDMAC_CC_MEMSET_HW_MODE\r\n| AT_XDMAC_CC_TYPE_MEM_TRAN;\r\ndwidth = at_xdmac_align_width(chan, dst_addr);\r\nif (len >= (AT_XDMAC_MBR_UBC_UBLEN_MAX << dwidth)) {\r\ndev_err(chan2dev(chan),\r\n"%s: Transfer too large, aborting...\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndesc = at_xdmac_get_desc(atchan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nif (!desc) {\r\ndev_err(chan2dev(chan), "can't get descriptor\n");\r\nreturn NULL;\r\n}\r\nchan_cc |= AT_XDMAC_CC_DWIDTH(dwidth);\r\nublen = len >> dwidth;\r\ndesc->lld.mbr_da = dst_addr;\r\ndesc->lld.mbr_ds = value;\r\ndesc->lld.mbr_ubc = AT_XDMAC_MBR_UBC_NDV3\r\n| AT_XDMAC_MBR_UBC_NDEN\r\n| AT_XDMAC_MBR_UBC_NSEN\r\n| ublen;\r\ndesc->lld.mbr_cfg = chan_cc;\r\ndev_dbg(chan2dev(chan),\r\n"%s: lld: mbr_da=%pad, mbr_ds=0x%08x, mbr_ubc=0x%08x, mbr_cfg=0x%08x\n",\r\n__func__, &desc->lld.mbr_da, desc->lld.mbr_ds, desc->lld.mbr_ubc,\r\ndesc->lld.mbr_cfg);\r\nreturn desc;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nat_xdmac_prep_dma_memset(struct dma_chan *chan, dma_addr_t dest, int value,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac_desc *desc;\r\ndev_dbg(chan2dev(chan), "%s: dest=%pad, len=%d, pattern=0x%x, flags=0x%lx\n",\r\n__func__, &dest, len, value, flags);\r\nif (unlikely(!len))\r\nreturn NULL;\r\ndesc = at_xdmac_memset_create_desc(chan, atchan, dest, len, value);\r\nlist_add_tail(&desc->desc_node, &desc->descs_list);\r\ndesc->tx_dma_desc.cookie = -EBUSY;\r\ndesc->tx_dma_desc.flags = flags;\r\ndesc->xfer_size = len;\r\nreturn &desc->tx_dma_desc;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nat_xdmac_prep_dma_memset_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, int value,\r\nunsigned long flags)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac_desc *desc, *pdesc = NULL,\r\n*ppdesc = NULL, *first = NULL;\r\nstruct scatterlist *sg, *psg = NULL, *ppsg = NULL;\r\nsize_t stride = 0, pstride = 0, len = 0;\r\nint i;\r\nif (!sgl)\r\nreturn NULL;\r\ndev_dbg(chan2dev(chan), "%s: sg_len=%d, value=0x%x, flags=0x%lx\n",\r\n__func__, sg_len, value, flags);\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndev_dbg(chan2dev(chan), "%s: dest=%pad, len=%d, pattern=0x%x, flags=0x%lx\n",\r\n__func__, &sg_dma_address(sg), sg_dma_len(sg),\r\nvalue, flags);\r\ndesc = at_xdmac_memset_create_desc(chan, atchan,\r\nsg_dma_address(sg),\r\nsg_dma_len(sg),\r\nvalue);\r\nif (!desc && first)\r\nlist_splice_init(&first->descs_list,\r\n&atchan->free_descs_list);\r\nif (!first)\r\nfirst = desc;\r\npstride = stride;\r\nif (psg)\r\nstride = sg_dma_address(sg) -\r\n(sg_dma_address(psg) + sg_dma_len(psg));\r\nif (ppdesc && pdesc) {\r\nif ((stride == pstride) &&\r\n(sg_dma_len(ppsg) == sg_dma_len(psg))) {\r\ndev_dbg(chan2dev(chan),\r\n"%s: desc 0x%p can be merged with desc 0x%p\n",\r\n__func__, pdesc, ppdesc);\r\nat_xdmac_increment_block_count(chan, ppdesc);\r\nppdesc->lld.mbr_dus = stride;\r\nlist_add_tail(&pdesc->desc_node,\r\n&atchan->free_descs_list);\r\npdesc = ppdesc;\r\n} else if (pstride ||\r\nsg_dma_address(sg) < sg_dma_address(psg)) {\r\nat_xdmac_queue_desc(chan, ppdesc, pdesc);\r\nlist_add_tail(&desc->desc_node,\r\n&first->descs_list);\r\ndev_dbg(chan2dev(chan),\r\n"%s: add desc 0x%p to descs_list 0x%p\n",\r\n__func__, desc, first);\r\n}\r\n}\r\nif ((i == (sg_len - 1)) &&\r\nsg_dma_len(psg) == sg_dma_len(sg)) {\r\ndev_dbg(chan2dev(chan),\r\n"%s: desc 0x%p can be merged with desc 0x%p\n",\r\n__func__, desc, pdesc);\r\nat_xdmac_increment_block_count(chan, pdesc);\r\npdesc->lld.mbr_dus = stride;\r\nlist_add_tail(&desc->desc_node,\r\n&atchan->free_descs_list);\r\n}\r\nppdesc = pdesc;\r\npdesc = desc;\r\nppsg = psg;\r\npsg = sg;\r\nlen += sg_dma_len(sg);\r\n}\r\nfirst->tx_dma_desc.cookie = -EBUSY;\r\nfirst->tx_dma_desc.flags = flags;\r\nfirst->xfer_size = len;\r\nreturn &first->tx_dma_desc;\r\n}\r\nstatic enum dma_status\r\nat_xdmac_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac *atxdmac = to_at_xdmac(atchan->chan.device);\r\nstruct at_xdmac_desc *desc, *_desc;\r\nstruct list_head *descs_list;\r\nenum dma_status ret;\r\nint residue, retry;\r\nu32 cur_nda, check_nda, cur_ubc, mask, value;\r\nu8 dwidth = 0;\r\nunsigned long flags;\r\nbool initd;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nif (!txstate)\r\nreturn ret;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndesc = list_first_entry(&atchan->xfers_list, struct at_xdmac_desc, xfer_node);\r\nif (!desc->active_xfer) {\r\ndma_set_residue(txstate, desc->xfer_size);\r\ngoto spin_unlock;\r\n}\r\nresidue = desc->xfer_size;\r\nmask = AT_XDMAC_CC_TYPE | AT_XDMAC_CC_DSYNC;\r\nvalue = AT_XDMAC_CC_TYPE_PER_TRAN | AT_XDMAC_CC_DSYNC_PER2MEM;\r\nif ((desc->lld.mbr_cfg & mask) == value) {\r\nat_xdmac_write(atxdmac, AT_XDMAC_GSWF, atchan->mask);\r\nwhile (!(at_xdmac_chan_read(atchan, AT_XDMAC_CIS) & AT_XDMAC_CIS_FIS))\r\ncpu_relax();\r\n}\r\nfor (retry = 0; retry < AT_XDMAC_RESIDUE_MAX_RETRIES; retry++) {\r\ncheck_nda = at_xdmac_chan_read(atchan, AT_XDMAC_CNDA) & 0xfffffffc;\r\nrmb();\r\ninitd = !!(at_xdmac_chan_read(atchan, AT_XDMAC_CC) & AT_XDMAC_CC_INITD);\r\nrmb();\r\ncur_ubc = at_xdmac_chan_read(atchan, AT_XDMAC_CUBC);\r\nrmb();\r\ncur_nda = at_xdmac_chan_read(atchan, AT_XDMAC_CNDA) & 0xfffffffc;\r\nrmb();\r\nif ((check_nda == cur_nda) && initd)\r\nbreak;\r\n}\r\nif (unlikely(retry >= AT_XDMAC_RESIDUE_MAX_RETRIES)) {\r\nret = DMA_ERROR;\r\ngoto spin_unlock;\r\n}\r\nif ((desc->lld.mbr_cfg & mask) == value) {\r\nat_xdmac_write(atxdmac, AT_XDMAC_GSWF, atchan->mask);\r\nwhile (!(at_xdmac_chan_read(atchan, AT_XDMAC_CIS) & AT_XDMAC_CIS_FIS))\r\ncpu_relax();\r\n}\r\ndescs_list = &desc->descs_list;\r\nlist_for_each_entry_safe(desc, _desc, descs_list, desc_node) {\r\ndwidth = at_xdmac_get_dwidth(desc->lld.mbr_cfg);\r\nresidue -= (desc->lld.mbr_ubc & 0xffffff) << dwidth;\r\nif ((desc->lld.mbr_nda & 0xfffffffc) == cur_nda)\r\nbreak;\r\n}\r\nresidue += cur_ubc << dwidth;\r\ndma_set_residue(txstate, residue);\r\ndev_dbg(chan2dev(chan),\r\n"%s: desc=0x%p, tx_dma_desc.phys=%pad, tx_status=%d, cookie=%d, residue=%d\n",\r\n__func__, desc, &desc->tx_dma_desc.phys, ret, cookie, residue);\r\nspin_unlock:\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void at_xdmac_remove_xfer(struct at_xdmac_chan *atchan,\r\nstruct at_xdmac_desc *desc)\r\n{\r\ndev_dbg(chan2dev(&atchan->chan), "%s: desc 0x%p\n", __func__, desc);\r\nlist_del(&desc->xfer_node);\r\nlist_splice_init(&desc->descs_list, &atchan->free_descs_list);\r\n}\r\nstatic void at_xdmac_advance_work(struct at_xdmac_chan *atchan)\r\n{\r\nstruct at_xdmac_desc *desc;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nif (!at_xdmac_chan_is_enabled(atchan) && !list_empty(&atchan->xfers_list)) {\r\ndesc = list_first_entry(&atchan->xfers_list,\r\nstruct at_xdmac_desc,\r\nxfer_node);\r\ndev_vdbg(chan2dev(&atchan->chan), "%s: desc 0x%p\n", __func__, desc);\r\nif (!desc->active_xfer)\r\nat_xdmac_start_xfer(atchan, desc);\r\n}\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n}\r\nstatic void at_xdmac_handle_cyclic(struct at_xdmac_chan *atchan)\r\n{\r\nstruct at_xdmac_desc *desc;\r\nstruct dma_async_tx_descriptor *txd;\r\ndesc = list_first_entry(&atchan->xfers_list, struct at_xdmac_desc, xfer_node);\r\ntxd = &desc->tx_dma_desc;\r\nif (txd->flags & DMA_PREP_INTERRUPT)\r\ndmaengine_desc_get_callback_invoke(txd, NULL);\r\n}\r\nstatic void at_xdmac_tasklet(unsigned long data)\r\n{\r\nstruct at_xdmac_chan *atchan = (struct at_xdmac_chan *)data;\r\nstruct at_xdmac_desc *desc;\r\nu32 error_mask;\r\ndev_dbg(chan2dev(&atchan->chan), "%s: status=0x%08lx\n",\r\n__func__, atchan->status);\r\nerror_mask = AT_XDMAC_CIS_RBEIS\r\n| AT_XDMAC_CIS_WBEIS\r\n| AT_XDMAC_CIS_ROIS;\r\nif (at_xdmac_chan_is_cyclic(atchan)) {\r\nat_xdmac_handle_cyclic(atchan);\r\n} else if ((atchan->status & AT_XDMAC_CIS_LIS)\r\n|| (atchan->status & error_mask)) {\r\nstruct dma_async_tx_descriptor *txd;\r\nif (atchan->status & AT_XDMAC_CIS_RBEIS)\r\ndev_err(chan2dev(&atchan->chan), "read bus error!!!");\r\nif (atchan->status & AT_XDMAC_CIS_WBEIS)\r\ndev_err(chan2dev(&atchan->chan), "write bus error!!!");\r\nif (atchan->status & AT_XDMAC_CIS_ROIS)\r\ndev_err(chan2dev(&atchan->chan), "request overflow error!!!");\r\nspin_lock_bh(&atchan->lock);\r\ndesc = list_first_entry(&atchan->xfers_list,\r\nstruct at_xdmac_desc,\r\nxfer_node);\r\ndev_vdbg(chan2dev(&atchan->chan), "%s: desc 0x%p\n", __func__, desc);\r\nBUG_ON(!desc->active_xfer);\r\ntxd = &desc->tx_dma_desc;\r\nat_xdmac_remove_xfer(atchan, desc);\r\nspin_unlock_bh(&atchan->lock);\r\nif (!at_xdmac_chan_is_cyclic(atchan)) {\r\ndma_cookie_complete(txd);\r\nif (txd->flags & DMA_PREP_INTERRUPT)\r\ndmaengine_desc_get_callback_invoke(txd, NULL);\r\n}\r\ndma_run_dependencies(txd);\r\nat_xdmac_advance_work(atchan);\r\n}\r\n}\r\nstatic irqreturn_t at_xdmac_interrupt(int irq, void *dev_id)\r\n{\r\nstruct at_xdmac *atxdmac = (struct at_xdmac *)dev_id;\r\nstruct at_xdmac_chan *atchan;\r\nu32 imr, status, pending;\r\nu32 chan_imr, chan_status;\r\nint i, ret = IRQ_NONE;\r\ndo {\r\nimr = at_xdmac_read(atxdmac, AT_XDMAC_GIM);\r\nstatus = at_xdmac_read(atxdmac, AT_XDMAC_GIS);\r\npending = status & imr;\r\ndev_vdbg(atxdmac->dma.dev,\r\n"%s: status=0x%08x, imr=0x%08x, pending=0x%08x\n",\r\n__func__, status, imr, pending);\r\nif (!pending)\r\nbreak;\r\nfor (i = 0; i < atxdmac->dma.chancnt; i++) {\r\nif (!((1 << i) & pending))\r\ncontinue;\r\natchan = &atxdmac->chan[i];\r\nchan_imr = at_xdmac_chan_read(atchan, AT_XDMAC_CIM);\r\nchan_status = at_xdmac_chan_read(atchan, AT_XDMAC_CIS);\r\natchan->status = chan_status & chan_imr;\r\ndev_vdbg(atxdmac->dma.dev,\r\n"%s: chan%d: imr=0x%x, status=0x%x\n",\r\n__func__, i, chan_imr, chan_status);\r\ndev_vdbg(chan2dev(&atchan->chan),\r\n"%s: CC=0x%08x CNDA=0x%08x, CNDC=0x%08x, CSA=0x%08x, CDA=0x%08x, CUBC=0x%08x\n",\r\n__func__,\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CC),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CNDA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CNDC),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CSA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CDA),\r\nat_xdmac_chan_read(atchan, AT_XDMAC_CUBC));\r\nif (atchan->status & (AT_XDMAC_CIS_RBEIS | AT_XDMAC_CIS_WBEIS))\r\nat_xdmac_write(atxdmac, AT_XDMAC_GD, atchan->mask);\r\ntasklet_schedule(&atchan->tasklet);\r\nret = IRQ_HANDLED;\r\n}\r\n} while (pending);\r\nreturn ret;\r\n}\r\nstatic void at_xdmac_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\ndev_dbg(chan2dev(&atchan->chan), "%s\n", __func__);\r\nif (!at_xdmac_chan_is_cyclic(atchan))\r\nat_xdmac_advance_work(atchan);\r\nreturn;\r\n}\r\nstatic int at_xdmac_device_config(struct dma_chan *chan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nint ret;\r\nunsigned long flags;\r\ndev_dbg(chan2dev(chan), "%s\n", __func__);\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nret = at_xdmac_set_slave_config(chan, config);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn ret;\r\n}\r\nstatic int at_xdmac_device_pause(struct dma_chan *chan)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac *atxdmac = to_at_xdmac(atchan->chan.device);\r\nunsigned long flags;\r\ndev_dbg(chan2dev(chan), "%s\n", __func__);\r\nif (test_and_set_bit(AT_XDMAC_CHAN_IS_PAUSED, &atchan->status))\r\nreturn 0;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nat_xdmac_write(atxdmac, AT_XDMAC_GRWS, atchan->mask);\r\nwhile (at_xdmac_chan_read(atchan, AT_XDMAC_CC)\r\n& (AT_XDMAC_CC_WRIP | AT_XDMAC_CC_RDIP))\r\ncpu_relax();\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int at_xdmac_device_resume(struct dma_chan *chan)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac *atxdmac = to_at_xdmac(atchan->chan.device);\r\nunsigned long flags;\r\ndev_dbg(chan2dev(chan), "%s\n", __func__);\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nif (!at_xdmac_chan_is_paused(atchan)) {\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn 0;\r\n}\r\nat_xdmac_write(atxdmac, AT_XDMAC_GRWR, atchan->mask);\r\nclear_bit(AT_XDMAC_CHAN_IS_PAUSED, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int at_xdmac_device_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct at_xdmac_desc *desc, *_desc;\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac *atxdmac = to_at_xdmac(atchan->chan.device);\r\nunsigned long flags;\r\ndev_dbg(chan2dev(chan), "%s\n", __func__);\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nat_xdmac_write(atxdmac, AT_XDMAC_GD, atchan->mask);\r\nwhile (at_xdmac_read(atxdmac, AT_XDMAC_GS) & atchan->mask)\r\ncpu_relax();\r\nlist_for_each_entry_safe(desc, _desc, &atchan->xfers_list, xfer_node)\r\nat_xdmac_remove_xfer(atchan, desc);\r\nclear_bit(AT_XDMAC_CHAN_IS_PAUSED, &atchan->status);\r\nclear_bit(AT_XDMAC_CHAN_IS_CYCLIC, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int at_xdmac_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac_desc *desc;\r\nint i;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nif (at_xdmac_chan_is_enabled(atchan)) {\r\ndev_err(chan2dev(chan),\r\n"can't allocate channel resources (channel enabled)\n");\r\ni = -EIO;\r\ngoto spin_unlock;\r\n}\r\nif (!list_empty(&atchan->free_descs_list)) {\r\ndev_err(chan2dev(chan),\r\n"can't allocate channel resources (channel not free from a previous use)\n");\r\ni = -EIO;\r\ngoto spin_unlock;\r\n}\r\nfor (i = 0; i < init_nr_desc_per_channel; i++) {\r\ndesc = at_xdmac_alloc_desc(chan, GFP_ATOMIC);\r\nif (!desc) {\r\ndev_warn(chan2dev(chan),\r\n"only %d descriptors have been allocated\n", i);\r\nbreak;\r\n}\r\nlist_add_tail(&desc->desc_node, &atchan->free_descs_list);\r\n}\r\ndma_cookie_init(chan);\r\ndev_dbg(chan2dev(chan), "%s: allocated %d descriptors\n", __func__, i);\r\nspin_unlock:\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn i;\r\n}\r\nstatic void at_xdmac_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nstruct at_xdmac *atxdmac = to_at_xdmac(chan->device);\r\nstruct at_xdmac_desc *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, &atchan->free_descs_list, desc_node) {\r\ndev_dbg(chan2dev(chan), "%s: freeing descriptor %p\n", __func__, desc);\r\nlist_del(&desc->desc_node);\r\ndma_pool_free(atxdmac->at_xdmac_desc_pool, desc, desc->tx_dma_desc.phys);\r\n}\r\nreturn;\r\n}\r\nstatic int atmel_xdmac_prepare(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_xdmac *atxdmac = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nlist_for_each_entry_safe(chan, _chan, &atxdmac->dma.channels, device_node) {\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\nif (at_xdmac_chan_is_enabled(atchan) && !at_xdmac_chan_is_cyclic(atchan))\r\nreturn -EAGAIN;\r\n}\r\nreturn 0;\r\n}\r\nstatic int atmel_xdmac_suspend(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_xdmac *atxdmac = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nlist_for_each_entry_safe(chan, _chan, &atxdmac->dma.channels, device_node) {\r\nstruct at_xdmac_chan *atchan = to_at_xdmac_chan(chan);\r\natchan->save_cc = at_xdmac_chan_read(atchan, AT_XDMAC_CC);\r\nif (at_xdmac_chan_is_cyclic(atchan)) {\r\nif (!at_xdmac_chan_is_paused(atchan))\r\nat_xdmac_device_pause(chan);\r\natchan->save_cim = at_xdmac_chan_read(atchan, AT_XDMAC_CIM);\r\natchan->save_cnda = at_xdmac_chan_read(atchan, AT_XDMAC_CNDA);\r\natchan->save_cndc = at_xdmac_chan_read(atchan, AT_XDMAC_CNDC);\r\n}\r\n}\r\natxdmac->save_gim = at_xdmac_read(atxdmac, AT_XDMAC_GIM);\r\nat_xdmac_off(atxdmac);\r\nclk_disable_unprepare(atxdmac->clk);\r\nreturn 0;\r\n}\r\nstatic int atmel_xdmac_resume(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_xdmac *atxdmac = platform_get_drvdata(pdev);\r\nstruct at_xdmac_chan *atchan;\r\nstruct dma_chan *chan, *_chan;\r\nint i;\r\nclk_prepare_enable(atxdmac->clk);\r\nfor (i = 0; i < atxdmac->dma.chancnt; i++) {\r\natchan = &atxdmac->chan[i];\r\nwhile (at_xdmac_chan_read(atchan, AT_XDMAC_CIS))\r\ncpu_relax();\r\n}\r\nat_xdmac_write(atxdmac, AT_XDMAC_GIE, atxdmac->save_gim);\r\nlist_for_each_entry_safe(chan, _chan, &atxdmac->dma.channels, device_node) {\r\natchan = to_at_xdmac_chan(chan);\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CC, atchan->save_cc);\r\nif (at_xdmac_chan_is_cyclic(atchan)) {\r\nif (at_xdmac_chan_is_paused(atchan))\r\nat_xdmac_device_resume(chan);\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CNDA, atchan->save_cnda);\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CNDC, atchan->save_cndc);\r\nat_xdmac_chan_write(atchan, AT_XDMAC_CIE, atchan->save_cim);\r\nwmb();\r\nat_xdmac_write(atxdmac, AT_XDMAC_GE, atchan->mask);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int at_xdmac_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *res;\r\nstruct at_xdmac *atxdmac;\r\nint irq, size, nr_channels, i, ret;\r\nvoid __iomem *base;\r\nu32 reg;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res)\r\nreturn -EINVAL;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0)\r\nreturn irq;\r\nbase = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(base))\r\nreturn PTR_ERR(base);\r\nreg = readl_relaxed(base + AT_XDMAC_GTYPE);\r\nnr_channels = AT_XDMAC_NB_CH(reg);\r\nif (nr_channels > AT_XDMAC_MAX_CHAN) {\r\ndev_err(&pdev->dev, "invalid number of channels (%u)\n",\r\nnr_channels);\r\nreturn -EINVAL;\r\n}\r\nsize = sizeof(*atxdmac);\r\nsize += nr_channels * sizeof(struct at_xdmac_chan);\r\natxdmac = devm_kzalloc(&pdev->dev, size, GFP_KERNEL);\r\nif (!atxdmac) {\r\ndev_err(&pdev->dev, "can't allocate at_xdmac structure\n");\r\nreturn -ENOMEM;\r\n}\r\natxdmac->regs = base;\r\natxdmac->irq = irq;\r\natxdmac->clk = devm_clk_get(&pdev->dev, "dma_clk");\r\nif (IS_ERR(atxdmac->clk)) {\r\ndev_err(&pdev->dev, "can't get dma_clk\n");\r\nreturn PTR_ERR(atxdmac->clk);\r\n}\r\nret = request_irq(atxdmac->irq, at_xdmac_interrupt, 0, "at_xdmac", atxdmac);\r\nif (ret) {\r\ndev_err(&pdev->dev, "can't request irq\n");\r\nreturn ret;\r\n}\r\nret = clk_prepare_enable(atxdmac->clk);\r\nif (ret) {\r\ndev_err(&pdev->dev, "can't prepare or enable clock\n");\r\ngoto err_free_irq;\r\n}\r\natxdmac->at_xdmac_desc_pool =\r\ndmam_pool_create(dev_name(&pdev->dev), &pdev->dev,\r\nsizeof(struct at_xdmac_desc), 4, 0);\r\nif (!atxdmac->at_xdmac_desc_pool) {\r\ndev_err(&pdev->dev, "no memory for descriptors dma pool\n");\r\nret = -ENOMEM;\r\ngoto err_clk_disable;\r\n}\r\ndma_cap_set(DMA_CYCLIC, atxdmac->dma.cap_mask);\r\ndma_cap_set(DMA_INTERLEAVE, atxdmac->dma.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, atxdmac->dma.cap_mask);\r\ndma_cap_set(DMA_MEMSET, atxdmac->dma.cap_mask);\r\ndma_cap_set(DMA_MEMSET_SG, atxdmac->dma.cap_mask);\r\ndma_cap_set(DMA_SLAVE, atxdmac->dma.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, atxdmac->dma.cap_mask);\r\natxdmac->dma.dev = &pdev->dev;\r\natxdmac->dma.device_alloc_chan_resources = at_xdmac_alloc_chan_resources;\r\natxdmac->dma.device_free_chan_resources = at_xdmac_free_chan_resources;\r\natxdmac->dma.device_tx_status = at_xdmac_tx_status;\r\natxdmac->dma.device_issue_pending = at_xdmac_issue_pending;\r\natxdmac->dma.device_prep_dma_cyclic = at_xdmac_prep_dma_cyclic;\r\natxdmac->dma.device_prep_interleaved_dma = at_xdmac_prep_interleaved;\r\natxdmac->dma.device_prep_dma_memcpy = at_xdmac_prep_dma_memcpy;\r\natxdmac->dma.device_prep_dma_memset = at_xdmac_prep_dma_memset;\r\natxdmac->dma.device_prep_dma_memset_sg = at_xdmac_prep_dma_memset_sg;\r\natxdmac->dma.device_prep_slave_sg = at_xdmac_prep_slave_sg;\r\natxdmac->dma.device_config = at_xdmac_device_config;\r\natxdmac->dma.device_pause = at_xdmac_device_pause;\r\natxdmac->dma.device_resume = at_xdmac_device_resume;\r\natxdmac->dma.device_terminate_all = at_xdmac_device_terminate_all;\r\natxdmac->dma.src_addr_widths = AT_XDMAC_DMA_BUSWIDTHS;\r\natxdmac->dma.dst_addr_widths = AT_XDMAC_DMA_BUSWIDTHS;\r\natxdmac->dma.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\natxdmac->dma.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\nat_xdmac_off(atxdmac);\r\nINIT_LIST_HEAD(&atxdmac->dma.channels);\r\nfor (i = 0; i < nr_channels; i++) {\r\nstruct at_xdmac_chan *atchan = &atxdmac->chan[i];\r\natchan->chan.device = &atxdmac->dma;\r\nlist_add_tail(&atchan->chan.device_node,\r\n&atxdmac->dma.channels);\r\natchan->ch_regs = at_xdmac_chan_reg_base(atxdmac, i);\r\natchan->mask = 1 << i;\r\nspin_lock_init(&atchan->lock);\r\nINIT_LIST_HEAD(&atchan->xfers_list);\r\nINIT_LIST_HEAD(&atchan->free_descs_list);\r\ntasklet_init(&atchan->tasklet, at_xdmac_tasklet,\r\n(unsigned long)atchan);\r\nwhile (at_xdmac_chan_read(atchan, AT_XDMAC_CIS))\r\ncpu_relax();\r\n}\r\nplatform_set_drvdata(pdev, atxdmac);\r\nret = dma_async_device_register(&atxdmac->dma);\r\nif (ret) {\r\ndev_err(&pdev->dev, "fail to register DMA engine device\n");\r\ngoto err_clk_disable;\r\n}\r\nret = of_dma_controller_register(pdev->dev.of_node,\r\nat_xdmac_xlate, atxdmac);\r\nif (ret) {\r\ndev_err(&pdev->dev, "could not register of dma controller\n");\r\ngoto err_dma_unregister;\r\n}\r\ndev_info(&pdev->dev, "%d channels, mapped at 0x%p\n",\r\nnr_channels, atxdmac->regs);\r\nreturn 0;\r\nerr_dma_unregister:\r\ndma_async_device_unregister(&atxdmac->dma);\r\nerr_clk_disable:\r\nclk_disable_unprepare(atxdmac->clk);\r\nerr_free_irq:\r\nfree_irq(atxdmac->irq, atxdmac);\r\nreturn ret;\r\n}\r\nstatic int at_xdmac_remove(struct platform_device *pdev)\r\n{\r\nstruct at_xdmac *atxdmac = (struct at_xdmac *)platform_get_drvdata(pdev);\r\nint i;\r\nat_xdmac_off(atxdmac);\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&atxdmac->dma);\r\nclk_disable_unprepare(atxdmac->clk);\r\nfree_irq(atxdmac->irq, atxdmac);\r\nfor (i = 0; i < atxdmac->dma.chancnt; i++) {\r\nstruct at_xdmac_chan *atchan = &atxdmac->chan[i];\r\ntasklet_kill(&atchan->tasklet);\r\nat_xdmac_free_chan_resources(&atchan->chan);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init at_xdmac_init(void)\r\n{\r\nreturn platform_driver_probe(&at_xdmac_driver, at_xdmac_probe);\r\n}
