static inline void hsu_chan_disable(struct hsu_dma_chan *hsuc)\r\n{\r\nhsu_chan_writel(hsuc, HSU_CH_CR, 0);\r\n}\r\nstatic inline void hsu_chan_enable(struct hsu_dma_chan *hsuc)\r\n{\r\nu32 cr = HSU_CH_CR_CHA;\r\nif (hsuc->direction == DMA_MEM_TO_DEV)\r\ncr &= ~HSU_CH_CR_CHD;\r\nelse if (hsuc->direction == DMA_DEV_TO_MEM)\r\ncr |= HSU_CH_CR_CHD;\r\nhsu_chan_writel(hsuc, HSU_CH_CR, cr);\r\n}\r\nstatic void hsu_dma_chan_start(struct hsu_dma_chan *hsuc)\r\n{\r\nstruct dma_slave_config *config = &hsuc->config;\r\nstruct hsu_dma_desc *desc = hsuc->desc;\r\nu32 bsr = 0, mtsr = 0;\r\nu32 dcr = HSU_CH_DCR_CHSOE | HSU_CH_DCR_CHEI;\r\nunsigned int i, count;\r\nif (hsuc->direction == DMA_MEM_TO_DEV) {\r\nbsr = config->dst_maxburst;\r\nmtsr = config->src_addr_width;\r\n} else if (hsuc->direction == DMA_DEV_TO_MEM) {\r\nbsr = config->src_maxburst;\r\nmtsr = config->dst_addr_width;\r\n}\r\nhsu_chan_disable(hsuc);\r\nhsu_chan_writel(hsuc, HSU_CH_DCR, 0);\r\nhsu_chan_writel(hsuc, HSU_CH_BSR, bsr);\r\nhsu_chan_writel(hsuc, HSU_CH_MTSR, mtsr);\r\ncount = desc->nents - desc->active;\r\nfor (i = 0; i < count && i < HSU_DMA_CHAN_NR_DESC; i++) {\r\nhsu_chan_writel(hsuc, HSU_CH_DxSAR(i), desc->sg[i].addr);\r\nhsu_chan_writel(hsuc, HSU_CH_DxTSR(i), desc->sg[i].len);\r\ndcr |= HSU_CH_DCR_DESCA(i);\r\ndcr |= HSU_CH_DCR_CHTOI(i);\r\ndesc->active++;\r\n}\r\ndcr |= HSU_CH_DCR_CHSOD(count - 1);\r\ndcr |= HSU_CH_DCR_CHDI(count - 1);\r\nhsu_chan_writel(hsuc, HSU_CH_DCR, dcr);\r\nhsu_chan_enable(hsuc);\r\n}\r\nstatic void hsu_dma_stop_channel(struct hsu_dma_chan *hsuc)\r\n{\r\nhsu_chan_disable(hsuc);\r\nhsu_chan_writel(hsuc, HSU_CH_DCR, 0);\r\n}\r\nstatic void hsu_dma_start_channel(struct hsu_dma_chan *hsuc)\r\n{\r\nhsu_dma_chan_start(hsuc);\r\n}\r\nstatic void hsu_dma_start_transfer(struct hsu_dma_chan *hsuc)\r\n{\r\nstruct virt_dma_desc *vdesc;\r\nvdesc = vchan_next_desc(&hsuc->vchan);\r\nif (!vdesc) {\r\nhsuc->desc = NULL;\r\nreturn;\r\n}\r\nlist_del(&vdesc->node);\r\nhsuc->desc = to_hsu_dma_desc(vdesc);\r\nhsu_dma_start_channel(hsuc);\r\n}\r\nint hsu_dma_get_status(struct hsu_dma_chip *chip, unsigned short nr,\r\nu32 *status)\r\n{\r\nstruct hsu_dma_chan *hsuc;\r\nunsigned long flags;\r\nu32 sr;\r\nif (nr >= chip->hsu->nr_channels)\r\nreturn -EINVAL;\r\nhsuc = &chip->hsu->chan[nr];\r\nspin_lock_irqsave(&hsuc->vchan.lock, flags);\r\nsr = hsu_chan_readl(hsuc, HSU_CH_SR);\r\nspin_unlock_irqrestore(&hsuc->vchan.lock, flags);\r\nsr &= ~(HSU_CH_SR_DESCE_ANY | HSU_CH_SR_CDESC_ANY);\r\nif (!sr)\r\nreturn -EIO;\r\nif (sr & HSU_CH_SR_DESCTO_ANY)\r\nudelay(2);\r\nsr &= ~HSU_CH_SR_DESCTO_ANY;\r\n*status = sr;\r\nreturn sr ? 0 : 1;\r\n}\r\nint hsu_dma_do_irq(struct hsu_dma_chip *chip, unsigned short nr, u32 status)\r\n{\r\nstruct hsu_dma_chan *hsuc;\r\nstruct hsu_dma_desc *desc;\r\nunsigned long flags;\r\nif (nr >= chip->hsu->nr_channels)\r\nreturn 0;\r\nhsuc = &chip->hsu->chan[nr];\r\nspin_lock_irqsave(&hsuc->vchan.lock, flags);\r\ndesc = hsuc->desc;\r\nif (desc) {\r\nif (status & HSU_CH_SR_CHE) {\r\ndesc->status = DMA_ERROR;\r\n} else if (desc->active < desc->nents) {\r\nhsu_dma_start_channel(hsuc);\r\n} else {\r\nvchan_cookie_complete(&desc->vdesc);\r\ndesc->status = DMA_COMPLETE;\r\nhsu_dma_start_transfer(hsuc);\r\n}\r\n}\r\nspin_unlock_irqrestore(&hsuc->vchan.lock, flags);\r\nreturn 1;\r\n}\r\nstatic struct hsu_dma_desc *hsu_dma_alloc_desc(unsigned int nents)\r\n{\r\nstruct hsu_dma_desc *desc;\r\ndesc = kzalloc(sizeof(*desc), GFP_NOWAIT);\r\nif (!desc)\r\nreturn NULL;\r\ndesc->sg = kcalloc(nents, sizeof(*desc->sg), GFP_NOWAIT);\r\nif (!desc->sg) {\r\nkfree(desc);\r\nreturn NULL;\r\n}\r\nreturn desc;\r\n}\r\nstatic void hsu_dma_desc_free(struct virt_dma_desc *vdesc)\r\n{\r\nstruct hsu_dma_desc *desc = to_hsu_dma_desc(vdesc);\r\nkfree(desc->sg);\r\nkfree(desc);\r\n}\r\nstatic struct dma_async_tx_descriptor *hsu_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct hsu_dma_chan *hsuc = to_hsu_dma_chan(chan);\r\nstruct hsu_dma_desc *desc;\r\nstruct scatterlist *sg;\r\nunsigned int i;\r\ndesc = hsu_dma_alloc_desc(sg_len);\r\nif (!desc)\r\nreturn NULL;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndesc->sg[i].addr = sg_dma_address(sg);\r\ndesc->sg[i].len = sg_dma_len(sg);\r\ndesc->length += sg_dma_len(sg);\r\n}\r\ndesc->nents = sg_len;\r\ndesc->direction = direction;\r\ndesc->status = DMA_IN_PROGRESS;\r\nreturn vchan_tx_prep(&hsuc->vchan, &desc->vdesc, flags);\r\n}\r\nstatic void hsu_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct hsu_dma_chan *hsuc = to_hsu_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&hsuc->vchan.lock, flags);\r\nif (vchan_issue_pending(&hsuc->vchan) && !hsuc->desc)\r\nhsu_dma_start_transfer(hsuc);\r\nspin_unlock_irqrestore(&hsuc->vchan.lock, flags);\r\n}\r\nstatic size_t hsu_dma_active_desc_size(struct hsu_dma_chan *hsuc)\r\n{\r\nstruct hsu_dma_desc *desc = hsuc->desc;\r\nsize_t bytes = 0;\r\nint i;\r\nfor (i = desc->active; i < desc->nents; i++)\r\nbytes += desc->sg[i].len;\r\ni = HSU_DMA_CHAN_NR_DESC - 1;\r\ndo {\r\nbytes += hsu_chan_readl(hsuc, HSU_CH_DxTSR(i));\r\n} while (--i >= 0);\r\nreturn bytes;\r\n}\r\nstatic enum dma_status hsu_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *state)\r\n{\r\nstruct hsu_dma_chan *hsuc = to_hsu_dma_chan(chan);\r\nstruct virt_dma_desc *vdesc;\r\nenum dma_status status;\r\nsize_t bytes;\r\nunsigned long flags;\r\nstatus = dma_cookie_status(chan, cookie, state);\r\nif (status == DMA_COMPLETE)\r\nreturn status;\r\nspin_lock_irqsave(&hsuc->vchan.lock, flags);\r\nvdesc = vchan_find_desc(&hsuc->vchan, cookie);\r\nif (hsuc->desc && cookie == hsuc->desc->vdesc.tx.cookie) {\r\nbytes = hsu_dma_active_desc_size(hsuc);\r\ndma_set_residue(state, bytes);\r\nstatus = hsuc->desc->status;\r\n} else if (vdesc) {\r\nbytes = to_hsu_dma_desc(vdesc)->length;\r\ndma_set_residue(state, bytes);\r\n}\r\nspin_unlock_irqrestore(&hsuc->vchan.lock, flags);\r\nreturn status;\r\n}\r\nstatic int hsu_dma_slave_config(struct dma_chan *chan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct hsu_dma_chan *hsuc = to_hsu_dma_chan(chan);\r\nif (!is_slave_direction(config->direction))\r\nreturn -EINVAL;\r\nmemcpy(&hsuc->config, config, sizeof(hsuc->config));\r\nreturn 0;\r\n}\r\nstatic int hsu_dma_pause(struct dma_chan *chan)\r\n{\r\nstruct hsu_dma_chan *hsuc = to_hsu_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&hsuc->vchan.lock, flags);\r\nif (hsuc->desc && hsuc->desc->status == DMA_IN_PROGRESS) {\r\nhsu_chan_disable(hsuc);\r\nhsuc->desc->status = DMA_PAUSED;\r\n}\r\nspin_unlock_irqrestore(&hsuc->vchan.lock, flags);\r\nreturn 0;\r\n}\r\nstatic int hsu_dma_resume(struct dma_chan *chan)\r\n{\r\nstruct hsu_dma_chan *hsuc = to_hsu_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&hsuc->vchan.lock, flags);\r\nif (hsuc->desc && hsuc->desc->status == DMA_PAUSED) {\r\nhsuc->desc->status = DMA_IN_PROGRESS;\r\nhsu_chan_enable(hsuc);\r\n}\r\nspin_unlock_irqrestore(&hsuc->vchan.lock, flags);\r\nreturn 0;\r\n}\r\nstatic int hsu_dma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct hsu_dma_chan *hsuc = to_hsu_dma_chan(chan);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&hsuc->vchan.lock, flags);\r\nhsu_dma_stop_channel(hsuc);\r\nif (hsuc->desc) {\r\nhsu_dma_desc_free(&hsuc->desc->vdesc);\r\nhsuc->desc = NULL;\r\n}\r\nvchan_get_all_descriptors(&hsuc->vchan, &head);\r\nspin_unlock_irqrestore(&hsuc->vchan.lock, flags);\r\nvchan_dma_desc_free_list(&hsuc->vchan, &head);\r\nreturn 0;\r\n}\r\nstatic void hsu_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nvchan_free_chan_resources(to_virt_chan(chan));\r\n}\r\nint hsu_dma_probe(struct hsu_dma_chip *chip)\r\n{\r\nstruct hsu_dma *hsu;\r\nvoid __iomem *addr = chip->regs + chip->offset;\r\nunsigned short i;\r\nint ret;\r\nhsu = devm_kzalloc(chip->dev, sizeof(*hsu), GFP_KERNEL);\r\nif (!hsu)\r\nreturn -ENOMEM;\r\nchip->hsu = hsu;\r\nhsu->nr_channels = (chip->length - chip->offset) / HSU_DMA_CHAN_LENGTH;\r\nhsu->chan = devm_kcalloc(chip->dev, hsu->nr_channels,\r\nsizeof(*hsu->chan), GFP_KERNEL);\r\nif (!hsu->chan)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&hsu->dma.channels);\r\nfor (i = 0; i < hsu->nr_channels; i++) {\r\nstruct hsu_dma_chan *hsuc = &hsu->chan[i];\r\nhsuc->vchan.desc_free = hsu_dma_desc_free;\r\nvchan_init(&hsuc->vchan, &hsu->dma);\r\nhsuc->direction = (i & 0x1) ? DMA_DEV_TO_MEM : DMA_MEM_TO_DEV;\r\nhsuc->reg = addr + i * HSU_DMA_CHAN_LENGTH;\r\n}\r\ndma_cap_set(DMA_SLAVE, hsu->dma.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, hsu->dma.cap_mask);\r\nhsu->dma.device_free_chan_resources = hsu_dma_free_chan_resources;\r\nhsu->dma.device_prep_slave_sg = hsu_dma_prep_slave_sg;\r\nhsu->dma.device_issue_pending = hsu_dma_issue_pending;\r\nhsu->dma.device_tx_status = hsu_dma_tx_status;\r\nhsu->dma.device_config = hsu_dma_slave_config;\r\nhsu->dma.device_pause = hsu_dma_pause;\r\nhsu->dma.device_resume = hsu_dma_resume;\r\nhsu->dma.device_terminate_all = hsu_dma_terminate_all;\r\nhsu->dma.src_addr_widths = HSU_DMA_BUSWIDTHS;\r\nhsu->dma.dst_addr_widths = HSU_DMA_BUSWIDTHS;\r\nhsu->dma.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nhsu->dma.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\nhsu->dma.dev = chip->dev;\r\ndma_set_max_seg_size(hsu->dma.dev, HSU_CH_DxTSR_MASK);\r\nret = dma_async_device_register(&hsu->dma);\r\nif (ret)\r\nreturn ret;\r\ndev_info(chip->dev, "Found HSU DMA, %d channels\n", hsu->nr_channels);\r\nreturn 0;\r\n}\r\nint hsu_dma_remove(struct hsu_dma_chip *chip)\r\n{\r\nstruct hsu_dma *hsu = chip->hsu;\r\nunsigned short i;\r\ndma_async_device_unregister(&hsu->dma);\r\nfor (i = 0; i < hsu->nr_channels; i++) {\r\nstruct hsu_dma_chan *hsuc = &hsu->chan[i];\r\ntasklet_kill(&hsuc->vchan.task);\r\n}\r\nreturn 0;\r\n}
