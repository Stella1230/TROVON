static int __init ion_dummy_init(void)\r\n{\r\nint i, err;\r\nidev = ion_device_create(NULL);\r\nif (IS_ERR(idev))\r\nreturn PTR_ERR(idev);\r\nheaps = kcalloc(dummy_ion_pdata.nr, sizeof(struct ion_heap *),\r\nGFP_KERNEL);\r\nif (!heaps)\r\nreturn -ENOMEM;\r\ncarveout_ptr = alloc_pages_exact(\r\ndummy_heaps[ION_HEAP_TYPE_CARVEOUT].size,\r\nGFP_KERNEL);\r\nif (carveout_ptr)\r\ndummy_heaps[ION_HEAP_TYPE_CARVEOUT].base =\r\nvirt_to_phys(carveout_ptr);\r\nelse\r\npr_err("ion_dummy: Could not allocate carveout\n");\r\nchunk_ptr = alloc_pages_exact(\r\ndummy_heaps[ION_HEAP_TYPE_CHUNK].size,\r\nGFP_KERNEL);\r\nif (chunk_ptr)\r\ndummy_heaps[ION_HEAP_TYPE_CHUNK].base = virt_to_phys(chunk_ptr);\r\nelse\r\npr_err("ion_dummy: Could not allocate chunk\n");\r\nfor (i = 0; i < dummy_ion_pdata.nr; i++) {\r\nstruct ion_platform_heap *heap_data = &dummy_ion_pdata.heaps[i];\r\nif (heap_data->type == ION_HEAP_TYPE_CARVEOUT &&\r\n!heap_data->base)\r\ncontinue;\r\nif (heap_data->type == ION_HEAP_TYPE_CHUNK && !heap_data->base)\r\ncontinue;\r\nheaps[i] = ion_heap_create(heap_data);\r\nif (IS_ERR_OR_NULL(heaps[i])) {\r\nerr = PTR_ERR(heaps[i]);\r\ngoto err;\r\n}\r\nion_device_add_heap(idev, heaps[i]);\r\n}\r\nreturn 0;\r\nerr:\r\nfor (i = 0; i < dummy_ion_pdata.nr; ++i)\r\nion_heap_destroy(heaps[i]);\r\nkfree(heaps);\r\nif (carveout_ptr) {\r\nfree_pages_exact(carveout_ptr,\r\ndummy_heaps[ION_HEAP_TYPE_CARVEOUT].size);\r\ncarveout_ptr = NULL;\r\n}\r\nif (chunk_ptr) {\r\nfree_pages_exact(chunk_ptr,\r\ndummy_heaps[ION_HEAP_TYPE_CHUNK].size);\r\nchunk_ptr = NULL;\r\n}\r\nreturn err;\r\n}\r\nstatic void __exit ion_dummy_exit(void)\r\n{\r\nint i;\r\nion_device_destroy(idev);\r\nfor (i = 0; i < dummy_ion_pdata.nr; i++)\r\nion_heap_destroy(heaps[i]);\r\nkfree(heaps);\r\nif (carveout_ptr) {\r\nfree_pages_exact(carveout_ptr,\r\ndummy_heaps[ION_HEAP_TYPE_CARVEOUT].size);\r\ncarveout_ptr = NULL;\r\n}\r\nif (chunk_ptr) {\r\nfree_pages_exact(chunk_ptr,\r\ndummy_heaps[ION_HEAP_TYPE_CHUNK].size);\r\nchunk_ptr = NULL;\r\n}\r\n}
