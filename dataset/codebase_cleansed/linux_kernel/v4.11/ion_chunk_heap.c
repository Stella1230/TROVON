static int ion_chunk_heap_allocate(struct ion_heap *heap,\r\nstruct ion_buffer *buffer,\r\nunsigned long size, unsigned long align,\r\nunsigned long flags)\r\n{\r\nstruct ion_chunk_heap *chunk_heap =\r\ncontainer_of(heap, struct ion_chunk_heap, heap);\r\nstruct sg_table *table;\r\nstruct scatterlist *sg;\r\nint ret, i;\r\nunsigned long num_chunks;\r\nunsigned long allocated_size;\r\nif (align > chunk_heap->chunk_size)\r\nreturn -EINVAL;\r\nallocated_size = ALIGN(size, chunk_heap->chunk_size);\r\nnum_chunks = allocated_size / chunk_heap->chunk_size;\r\nif (allocated_size > chunk_heap->size - chunk_heap->allocated)\r\nreturn -ENOMEM;\r\ntable = kmalloc(sizeof(*table), GFP_KERNEL);\r\nif (!table)\r\nreturn -ENOMEM;\r\nret = sg_alloc_table(table, num_chunks, GFP_KERNEL);\r\nif (ret) {\r\nkfree(table);\r\nreturn ret;\r\n}\r\nsg = table->sgl;\r\nfor (i = 0; i < num_chunks; i++) {\r\nunsigned long paddr = gen_pool_alloc(chunk_heap->pool,\r\nchunk_heap->chunk_size);\r\nif (!paddr)\r\ngoto err;\r\nsg_set_page(sg, pfn_to_page(PFN_DOWN(paddr)),\r\nchunk_heap->chunk_size, 0);\r\nsg = sg_next(sg);\r\n}\r\nbuffer->sg_table = table;\r\nchunk_heap->allocated += allocated_size;\r\nreturn 0;\r\nerr:\r\nsg = table->sgl;\r\nfor (i -= 1; i >= 0; i--) {\r\ngen_pool_free(chunk_heap->pool, page_to_phys(sg_page(sg)),\r\nsg->length);\r\nsg = sg_next(sg);\r\n}\r\nsg_free_table(table);\r\nkfree(table);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ion_chunk_heap_free(struct ion_buffer *buffer)\r\n{\r\nstruct ion_heap *heap = buffer->heap;\r\nstruct ion_chunk_heap *chunk_heap =\r\ncontainer_of(heap, struct ion_chunk_heap, heap);\r\nstruct sg_table *table = buffer->sg_table;\r\nstruct scatterlist *sg;\r\nint i;\r\nunsigned long allocated_size;\r\nallocated_size = ALIGN(buffer->size, chunk_heap->chunk_size);\r\nion_heap_buffer_zero(buffer);\r\nif (ion_buffer_cached(buffer))\r\ndma_sync_sg_for_device(NULL, table->sgl, table->nents,\r\nDMA_BIDIRECTIONAL);\r\nfor_each_sg(table->sgl, sg, table->nents, i) {\r\ngen_pool_free(chunk_heap->pool, page_to_phys(sg_page(sg)),\r\nsg->length);\r\n}\r\nchunk_heap->allocated -= allocated_size;\r\nsg_free_table(table);\r\nkfree(table);\r\n}\r\nstruct ion_heap *ion_chunk_heap_create(struct ion_platform_heap *heap_data)\r\n{\r\nstruct ion_chunk_heap *chunk_heap;\r\nint ret;\r\nstruct page *page;\r\nsize_t size;\r\npage = pfn_to_page(PFN_DOWN(heap_data->base));\r\nsize = heap_data->size;\r\nion_pages_sync_for_device(NULL, page, size, DMA_BIDIRECTIONAL);\r\nret = ion_heap_pages_zero(page, size, pgprot_writecombine(PAGE_KERNEL));\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nchunk_heap = kzalloc(sizeof(*chunk_heap), GFP_KERNEL);\r\nif (!chunk_heap)\r\nreturn ERR_PTR(-ENOMEM);\r\nchunk_heap->chunk_size = (unsigned long)heap_data->priv;\r\nchunk_heap->pool = gen_pool_create(get_order(chunk_heap->chunk_size) +\r\nPAGE_SHIFT, -1);\r\nif (!chunk_heap->pool) {\r\nret = -ENOMEM;\r\ngoto error_gen_pool_create;\r\n}\r\nchunk_heap->base = heap_data->base;\r\nchunk_heap->size = heap_data->size;\r\nchunk_heap->allocated = 0;\r\ngen_pool_add(chunk_heap->pool, chunk_heap->base, heap_data->size, -1);\r\nchunk_heap->heap.ops = &chunk_heap_ops;\r\nchunk_heap->heap.type = ION_HEAP_TYPE_CHUNK;\r\nchunk_heap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;\r\npr_debug("%s: base %lu size %zu align %ld\n", __func__,\r\nchunk_heap->base, heap_data->size, heap_data->align);\r\nreturn &chunk_heap->heap;\r\nerror_gen_pool_create:\r\nkfree(chunk_heap);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid ion_chunk_heap_destroy(struct ion_heap *heap)\r\n{\r\nstruct ion_chunk_heap *chunk_heap =\r\ncontainer_of(heap, struct ion_chunk_heap, heap);\r\ngen_pool_destroy(chunk_heap->pool);\r\nkfree(chunk_heap);\r\nchunk_heap = NULL;\r\n}
