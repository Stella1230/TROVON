static void shdma_chan_xfer_ld_queue(struct shdma_chan *schan)\r\n{\r\nstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\nstruct shdma_desc *sdesc;\r\nif (ops->channel_busy(schan))\r\nreturn;\r\nlist_for_each_entry(sdesc, &schan->ld_queue, node)\r\nif (sdesc->mark == DESC_SUBMITTED) {\r\nops->start_xfer(schan, sdesc);\r\nbreak;\r\n}\r\n}\r\nstatic dma_cookie_t shdma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct shdma_desc *chunk, *c, *desc =\r\ncontainer_of(tx, struct shdma_desc, async_tx);\r\nstruct shdma_chan *schan = to_shdma_chan(tx->chan);\r\ndma_async_tx_callback callback = tx->callback;\r\ndma_cookie_t cookie;\r\nbool power_up;\r\nspin_lock_irq(&schan->chan_lock);\r\npower_up = list_empty(&schan->ld_queue);\r\ncookie = dma_cookie_assign(tx);\r\nlist_for_each_entry_safe(chunk, c, desc->node.prev, node) {\r\nif (chunk != desc && (chunk->mark == DESC_IDLE ||\r\nchunk->async_tx.cookie > 0 ||\r\nchunk->async_tx.cookie == -EBUSY ||\r\n&chunk->node == &schan->ld_free))\r\nbreak;\r\nchunk->mark = DESC_SUBMITTED;\r\nif (chunk->chunks == 1) {\r\nchunk->async_tx.callback = callback;\r\nchunk->async_tx.callback_param = tx->callback_param;\r\n} else {\r\nchunk->async_tx.callback = NULL;\r\n}\r\nchunk->cookie = cookie;\r\nlist_move_tail(&chunk->node, &schan->ld_queue);\r\ndev_dbg(schan->dev, "submit #%d@%p on %d\n",\r\ntx->cookie, &chunk->async_tx, schan->id);\r\n}\r\nif (power_up) {\r\nint ret;\r\nschan->pm_state = SHDMA_PM_BUSY;\r\nret = pm_runtime_get(schan->dev);\r\nspin_unlock_irq(&schan->chan_lock);\r\nif (ret < 0)\r\ndev_err(schan->dev, "%s(): GET = %d\n", __func__, ret);\r\npm_runtime_barrier(schan->dev);\r\nspin_lock_irq(&schan->chan_lock);\r\nif (schan->pm_state != SHDMA_PM_ESTABLISHED) {\r\nstruct shdma_dev *sdev =\r\nto_shdma_dev(schan->dma_chan.device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\ndev_dbg(schan->dev, "Bring up channel %d\n",\r\nschan->id);\r\nops->setup_xfer(schan, schan->slave_id);\r\nif (schan->pm_state == SHDMA_PM_PENDING)\r\nshdma_chan_xfer_ld_queue(schan);\r\nschan->pm_state = SHDMA_PM_ESTABLISHED;\r\n}\r\n} else {\r\nschan->pm_state = SHDMA_PM_PENDING;\r\n}\r\nspin_unlock_irq(&schan->chan_lock);\r\nreturn cookie;\r\n}\r\nstatic struct shdma_desc *shdma_get_desc(struct shdma_chan *schan)\r\n{\r\nstruct shdma_desc *sdesc;\r\nlist_for_each_entry(sdesc, &schan->ld_free, node)\r\nif (sdesc->mark != DESC_PREPARED) {\r\nBUG_ON(sdesc->mark != DESC_IDLE);\r\nlist_del(&sdesc->node);\r\nreturn sdesc;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int shdma_setup_slave(struct shdma_chan *schan, dma_addr_t slave_addr)\r\n{\r\nstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\nint ret, match;\r\nif (schan->dev->of_node) {\r\nmatch = schan->hw_req;\r\nret = ops->set_slave(schan, match, slave_addr, true);\r\nif (ret < 0)\r\nreturn ret;\r\n} else {\r\nmatch = schan->real_slave_id;\r\n}\r\nif (schan->real_slave_id < 0 || schan->real_slave_id >= slave_num)\r\nreturn -EINVAL;\r\nif (test_and_set_bit(schan->real_slave_id, shdma_slave_used))\r\nreturn -EBUSY;\r\nret = ops->set_slave(schan, match, slave_addr, false);\r\nif (ret < 0) {\r\nclear_bit(schan->real_slave_id, shdma_slave_used);\r\nreturn ret;\r\n}\r\nschan->slave_id = schan->real_slave_id;\r\nreturn 0;\r\n}\r\nstatic int shdma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\nstruct shdma_desc *desc;\r\nstruct shdma_slave *slave = chan->private;\r\nint ret, i;\r\nif (slave) {\r\nschan->real_slave_id = slave->slave_id;\r\nret = shdma_setup_slave(schan, 0);\r\nif (ret < 0)\r\ngoto esetslave;\r\n} else {\r\nschan->slave_id = -EINVAL;\r\n}\r\nschan->desc = kcalloc(NR_DESCS_PER_CHANNEL,\r\nsdev->desc_size, GFP_KERNEL);\r\nif (!schan->desc) {\r\nret = -ENOMEM;\r\ngoto edescalloc;\r\n}\r\nschan->desc_num = NR_DESCS_PER_CHANNEL;\r\nfor (i = 0; i < NR_DESCS_PER_CHANNEL; i++) {\r\ndesc = ops->embedded_desc(schan->desc, i);\r\ndma_async_tx_descriptor_init(&desc->async_tx,\r\n&schan->dma_chan);\r\ndesc->async_tx.tx_submit = shdma_tx_submit;\r\ndesc->mark = DESC_IDLE;\r\nlist_add(&desc->node, &schan->ld_free);\r\n}\r\nreturn NR_DESCS_PER_CHANNEL;\r\nedescalloc:\r\nif (slave)\r\nesetslave:\r\nclear_bit(slave->slave_id, shdma_slave_used);\r\nchan->private = NULL;\r\nreturn ret;\r\n}\r\nbool shdma_chan_filter(struct dma_chan *chan, void *arg)\r\n{\r\nstruct shdma_chan *schan;\r\nstruct shdma_dev *sdev;\r\nint slave_id = (long)arg;\r\nint ret;\r\nif (chan->device->device_alloc_chan_resources !=\r\nshdma_alloc_chan_resources)\r\nreturn false;\r\nschan = to_shdma_chan(chan);\r\nsdev = to_shdma_dev(chan->device);\r\nif (schan->dev->of_node) {\r\nret = sdev->ops->set_slave(schan, slave_id, 0, true);\r\nif (ret < 0)\r\nreturn false;\r\nschan->real_slave_id = schan->slave_id;\r\nreturn true;\r\n}\r\nif (slave_id < 0) {\r\ndev_warn(sdev->dma_dev.dev, "invalid slave ID passed to dma_request_slave\n");\r\nreturn true;\r\n}\r\nif (slave_id >= slave_num)\r\nreturn false;\r\nret = sdev->ops->set_slave(schan, slave_id, 0, true);\r\nif (ret < 0)\r\nreturn false;\r\nschan->real_slave_id = slave_id;\r\nreturn true;\r\n}\r\nstatic dma_async_tx_callback __ld_cleanup(struct shdma_chan *schan, bool all)\r\n{\r\nstruct shdma_desc *desc, *_desc;\r\nbool head_acked = false;\r\ndma_cookie_t cookie = 0;\r\ndma_async_tx_callback callback = NULL;\r\nstruct dmaengine_desc_callback cb;\r\nunsigned long flags;\r\nLIST_HEAD(cyclic_list);\r\nmemset(&cb, 0, sizeof(cb));\r\nspin_lock_irqsave(&schan->chan_lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &schan->ld_queue, node) {\r\nstruct dma_async_tx_descriptor *tx = &desc->async_tx;\r\nBUG_ON(tx->cookie > 0 && tx->cookie != desc->cookie);\r\nBUG_ON(desc->mark != DESC_SUBMITTED &&\r\ndesc->mark != DESC_COMPLETED &&\r\ndesc->mark != DESC_WAITING);\r\nif (!all && desc->mark == DESC_SUBMITTED &&\r\ndesc->cookie != cookie)\r\nbreak;\r\nif (tx->cookie > 0)\r\ncookie = tx->cookie;\r\nif (desc->mark == DESC_COMPLETED && desc->chunks == 1) {\r\nif (schan->dma_chan.completed_cookie != desc->cookie - 1)\r\ndev_dbg(schan->dev,\r\n"Completing cookie %d, expected %d\n",\r\ndesc->cookie,\r\nschan->dma_chan.completed_cookie + 1);\r\nschan->dma_chan.completed_cookie = desc->cookie;\r\n}\r\nif (desc->mark == DESC_COMPLETED && tx->callback) {\r\ndesc->mark = DESC_WAITING;\r\ndmaengine_desc_get_callback(tx, &cb);\r\ncallback = tx->callback;\r\ndev_dbg(schan->dev, "descriptor #%d@%p on %d callback\n",\r\ntx->cookie, tx, schan->id);\r\nBUG_ON(desc->chunks != 1);\r\nbreak;\r\n}\r\nif (tx->cookie > 0 || tx->cookie == -EBUSY) {\r\nif (desc->mark == DESC_COMPLETED) {\r\nBUG_ON(tx->cookie < 0);\r\ndesc->mark = DESC_WAITING;\r\n}\r\nhead_acked = async_tx_test_ack(tx);\r\n} else {\r\nswitch (desc->mark) {\r\ncase DESC_COMPLETED:\r\ndesc->mark = DESC_WAITING;\r\ncase DESC_WAITING:\r\nif (head_acked)\r\nasync_tx_ack(&desc->async_tx);\r\n}\r\n}\r\ndev_dbg(schan->dev, "descriptor %p #%d completed.\n",\r\ntx, tx->cookie);\r\nif (((desc->mark == DESC_COMPLETED ||\r\ndesc->mark == DESC_WAITING) &&\r\nasync_tx_test_ack(&desc->async_tx)) || all) {\r\nif (all || !desc->cyclic) {\r\ndesc->mark = DESC_IDLE;\r\nlist_move(&desc->node, &schan->ld_free);\r\n} else {\r\ndesc->mark = DESC_SUBMITTED;\r\nlist_move_tail(&desc->node, &cyclic_list);\r\n}\r\nif (list_empty(&schan->ld_queue)) {\r\ndev_dbg(schan->dev, "Bring down channel %d\n", schan->id);\r\npm_runtime_put(schan->dev);\r\nschan->pm_state = SHDMA_PM_ESTABLISHED;\r\n} else if (schan->pm_state == SHDMA_PM_PENDING) {\r\nshdma_chan_xfer_ld_queue(schan);\r\n}\r\n}\r\n}\r\nif (all && !callback)\r\nschan->dma_chan.completed_cookie = schan->dma_chan.cookie;\r\nlist_splice_tail(&cyclic_list, &schan->ld_queue);\r\nspin_unlock_irqrestore(&schan->chan_lock, flags);\r\ndmaengine_desc_callback_invoke(&cb, NULL);\r\nreturn callback;\r\n}\r\nstatic void shdma_chan_ld_cleanup(struct shdma_chan *schan, bool all)\r\n{\r\nwhile (__ld_cleanup(schan, all))\r\n;\r\n}\r\nstatic void shdma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nstruct shdma_dev *sdev = to_shdma_dev(chan->device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\nLIST_HEAD(list);\r\nspin_lock_irq(&schan->chan_lock);\r\nops->halt_channel(schan);\r\nspin_unlock_irq(&schan->chan_lock);\r\nif (!list_empty(&schan->ld_queue))\r\nshdma_chan_ld_cleanup(schan, true);\r\nif (schan->slave_id >= 0) {\r\nclear_bit(schan->slave_id, shdma_slave_used);\r\nchan->private = NULL;\r\n}\r\nschan->real_slave_id = 0;\r\nspin_lock_irq(&schan->chan_lock);\r\nlist_splice_init(&schan->ld_free, &list);\r\nschan->desc_num = 0;\r\nspin_unlock_irq(&schan->chan_lock);\r\nkfree(schan->desc);\r\n}\r\nstatic struct shdma_desc *shdma_add_desc(struct shdma_chan *schan,\r\nunsigned long flags, dma_addr_t *dst, dma_addr_t *src, size_t *len,\r\nstruct shdma_desc **first, enum dma_transfer_direction direction)\r\n{\r\nstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\nstruct shdma_desc *new;\r\nsize_t copy_size = *len;\r\nif (!copy_size)\r\nreturn NULL;\r\nnew = shdma_get_desc(schan);\r\nif (!new) {\r\ndev_err(schan->dev, "No free link descriptor available\n");\r\nreturn NULL;\r\n}\r\nops->desc_setup(schan, new, *src, *dst, &copy_size);\r\nif (!*first) {\r\nnew->async_tx.cookie = -EBUSY;\r\n*first = new;\r\n} else {\r\nnew->async_tx.cookie = -EINVAL;\r\n}\r\ndev_dbg(schan->dev,\r\n"chaining (%zu/%zu)@%pad -> %pad with %p, cookie %d\n",\r\ncopy_size, *len, src, dst, &new->async_tx,\r\nnew->async_tx.cookie);\r\nnew->mark = DESC_PREPARED;\r\nnew->async_tx.flags = flags;\r\nnew->direction = direction;\r\nnew->partial = 0;\r\n*len -= copy_size;\r\nif (direction == DMA_MEM_TO_MEM || direction == DMA_MEM_TO_DEV)\r\n*src += copy_size;\r\nif (direction == DMA_MEM_TO_MEM || direction == DMA_DEV_TO_MEM)\r\n*dst += copy_size;\r\nreturn new;\r\n}\r\nstatic struct dma_async_tx_descriptor *shdma_prep_sg(struct shdma_chan *schan,\r\nstruct scatterlist *sgl, unsigned int sg_len, dma_addr_t *addr,\r\nenum dma_transfer_direction direction, unsigned long flags, bool cyclic)\r\n{\r\nstruct scatterlist *sg;\r\nstruct shdma_desc *first = NULL, *new = NULL ;\r\nLIST_HEAD(tx_list);\r\nint chunks = 0;\r\nunsigned long irq_flags;\r\nint i;\r\nfor_each_sg(sgl, sg, sg_len, i)\r\nchunks += DIV_ROUND_UP(sg_dma_len(sg), schan->max_xfer_len);\r\nspin_lock_irqsave(&schan->chan_lock, irq_flags);\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndma_addr_t sg_addr = sg_dma_address(sg);\r\nsize_t len = sg_dma_len(sg);\r\nif (!len)\r\ngoto err_get_desc;\r\ndo {\r\ndev_dbg(schan->dev, "Add SG #%d@%p[%zu], dma %pad\n",\r\ni, sg, len, &sg_addr);\r\nif (direction == DMA_DEV_TO_MEM)\r\nnew = shdma_add_desc(schan, flags,\r\n&sg_addr, addr, &len, &first,\r\ndirection);\r\nelse\r\nnew = shdma_add_desc(schan, flags,\r\naddr, &sg_addr, &len, &first,\r\ndirection);\r\nif (!new)\r\ngoto err_get_desc;\r\nnew->cyclic = cyclic;\r\nif (cyclic)\r\nnew->chunks = 1;\r\nelse\r\nnew->chunks = chunks--;\r\nlist_add_tail(&new->node, &tx_list);\r\n} while (len);\r\n}\r\nif (new != first)\r\nnew->async_tx.cookie = -ENOSPC;\r\nlist_splice_tail(&tx_list, &schan->ld_free);\r\nspin_unlock_irqrestore(&schan->chan_lock, irq_flags);\r\nreturn &first->async_tx;\r\nerr_get_desc:\r\nlist_for_each_entry(new, &tx_list, node)\r\nnew->mark = DESC_IDLE;\r\nlist_splice(&tx_list, &schan->ld_free);\r\nspin_unlock_irqrestore(&schan->chan_lock, irq_flags);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *shdma_prep_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dma_dest, dma_addr_t dma_src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nstruct scatterlist sg;\r\nif (!chan || !len)\r\nreturn NULL;\r\nBUG_ON(!schan->desc_num);\r\nsg_init_table(&sg, 1);\r\nsg_set_page(&sg, pfn_to_page(PFN_DOWN(dma_src)), len,\r\noffset_in_page(dma_src));\r\nsg_dma_address(&sg) = dma_src;\r\nsg_dma_len(&sg) = len;\r\nreturn shdma_prep_sg(schan, &sg, 1, &dma_dest, DMA_MEM_TO_MEM,\r\nflags, false);\r\n}\r\nstatic struct dma_async_tx_descriptor *shdma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned int sg_len,\r\nenum dma_transfer_direction direction, unsigned long flags, void *context)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\nint slave_id = schan->slave_id;\r\ndma_addr_t slave_addr;\r\nif (!chan)\r\nreturn NULL;\r\nBUG_ON(!schan->desc_num);\r\nif (slave_id < 0 || !sg_len) {\r\ndev_warn(schan->dev, "%s: bad parameter: len=%d, id=%d\n",\r\n__func__, sg_len, slave_id);\r\nreturn NULL;\r\n}\r\nslave_addr = ops->slave_addr(schan);\r\nreturn shdma_prep_sg(schan, sgl, sg_len, &slave_addr,\r\ndirection, flags, false);\r\n}\r\nstatic struct dma_async_tx_descriptor *shdma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nstruct shdma_dev *sdev = to_shdma_dev(schan->dma_chan.device);\r\nstruct dma_async_tx_descriptor *desc;\r\nconst struct shdma_ops *ops = sdev->ops;\r\nunsigned int sg_len = buf_len / period_len;\r\nint slave_id = schan->slave_id;\r\ndma_addr_t slave_addr;\r\nstruct scatterlist *sgl;\r\nint i;\r\nif (!chan)\r\nreturn NULL;\r\nBUG_ON(!schan->desc_num);\r\nif (sg_len > SHDMA_MAX_SG_LEN) {\r\ndev_err(schan->dev, "sg length %d exceds limit %d",\r\nsg_len, SHDMA_MAX_SG_LEN);\r\nreturn NULL;\r\n}\r\nif (slave_id < 0 || (buf_len < period_len)) {\r\ndev_warn(schan->dev,\r\n"%s: bad parameter: buf_len=%zu, period_len=%zu, id=%d\n",\r\n__func__, buf_len, period_len, slave_id);\r\nreturn NULL;\r\n}\r\nslave_addr = ops->slave_addr(schan);\r\nsgl = kcalloc(sg_len, sizeof(*sgl), GFP_KERNEL);\r\nif (!sgl)\r\nreturn NULL;\r\nsg_init_table(sgl, sg_len);\r\nfor (i = 0; i < sg_len; i++) {\r\ndma_addr_t src = buf_addr + (period_len * i);\r\nsg_set_page(&sgl[i], pfn_to_page(PFN_DOWN(src)), period_len,\r\noffset_in_page(src));\r\nsg_dma_address(&sgl[i]) = src;\r\nsg_dma_len(&sgl[i]) = period_len;\r\n}\r\ndesc = shdma_prep_sg(schan, sgl, sg_len, &slave_addr,\r\ndirection, flags, true);\r\nkfree(sgl);\r\nreturn desc;\r\n}\r\nstatic int shdma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nstruct shdma_dev *sdev = to_shdma_dev(chan->device);\r\nconst struct shdma_ops *ops = sdev->ops;\r\nunsigned long flags;\r\nspin_lock_irqsave(&schan->chan_lock, flags);\r\nops->halt_channel(schan);\r\nif (ops->get_partial && !list_empty(&schan->ld_queue)) {\r\nstruct shdma_desc *desc = list_first_entry(&schan->ld_queue,\r\nstruct shdma_desc, node);\r\ndesc->partial = ops->get_partial(schan, desc);\r\n}\r\nspin_unlock_irqrestore(&schan->chan_lock, flags);\r\nshdma_chan_ld_cleanup(schan, true);\r\nreturn 0;\r\n}\r\nstatic int shdma_config(struct dma_chan *chan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nif (!config)\r\nreturn -EINVAL;\r\nif (WARN_ON_ONCE(config->slave_id &&\r\nconfig->slave_id != schan->real_slave_id))\r\nschan->real_slave_id = config->slave_id;\r\nreturn shdma_setup_slave(schan,\r\nconfig->direction == DMA_DEV_TO_MEM ?\r\nconfig->src_addr : config->dst_addr);\r\n}\r\nstatic void shdma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nspin_lock_irq(&schan->chan_lock);\r\nif (schan->pm_state == SHDMA_PM_ESTABLISHED)\r\nshdma_chan_xfer_ld_queue(schan);\r\nelse\r\nschan->pm_state = SHDMA_PM_PENDING;\r\nspin_unlock_irq(&schan->chan_lock);\r\n}\r\nstatic enum dma_status shdma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct shdma_chan *schan = to_shdma_chan(chan);\r\nenum dma_status status;\r\nunsigned long flags;\r\nshdma_chan_ld_cleanup(schan, false);\r\nspin_lock_irqsave(&schan->chan_lock, flags);\r\nstatus = dma_cookie_status(chan, cookie, txstate);\r\nif (status != DMA_COMPLETE) {\r\nstruct shdma_desc *sdesc;\r\nstatus = DMA_ERROR;\r\nlist_for_each_entry(sdesc, &schan->ld_queue, node)\r\nif (sdesc->cookie == cookie) {\r\nstatus = DMA_IN_PROGRESS;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&schan->chan_lock, flags);\r\nreturn status;\r\n}\r\nbool shdma_reset(struct shdma_dev *sdev)\r\n{\r\nconst struct shdma_ops *ops = sdev->ops;\r\nstruct shdma_chan *schan;\r\nunsigned int handled = 0;\r\nint i;\r\nshdma_for_each_chan(schan, sdev, i) {\r\nstruct shdma_desc *sdesc;\r\nLIST_HEAD(dl);\r\nif (!schan)\r\ncontinue;\r\nspin_lock(&schan->chan_lock);\r\nops->halt_channel(schan);\r\nlist_splice_init(&schan->ld_queue, &dl);\r\nif (!list_empty(&dl)) {\r\ndev_dbg(schan->dev, "Bring down channel %d\n", schan->id);\r\npm_runtime_put(schan->dev);\r\n}\r\nschan->pm_state = SHDMA_PM_ESTABLISHED;\r\nspin_unlock(&schan->chan_lock);\r\nlist_for_each_entry(sdesc, &dl, node) {\r\nstruct dma_async_tx_descriptor *tx = &sdesc->async_tx;\r\nsdesc->mark = DESC_IDLE;\r\ndmaengine_desc_get_callback_invoke(tx, NULL);\r\n}\r\nspin_lock(&schan->chan_lock);\r\nlist_splice(&dl, &schan->ld_free);\r\nspin_unlock(&schan->chan_lock);\r\nhandled++;\r\n}\r\nreturn !!handled;\r\n}\r\nstatic irqreturn_t chan_irq(int irq, void *dev)\r\n{\r\nstruct shdma_chan *schan = dev;\r\nconst struct shdma_ops *ops =\r\nto_shdma_dev(schan->dma_chan.device)->ops;\r\nirqreturn_t ret;\r\nspin_lock(&schan->chan_lock);\r\nret = ops->chan_irq(schan, irq) ? IRQ_WAKE_THREAD : IRQ_NONE;\r\nspin_unlock(&schan->chan_lock);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t chan_irqt(int irq, void *dev)\r\n{\r\nstruct shdma_chan *schan = dev;\r\nconst struct shdma_ops *ops =\r\nto_shdma_dev(schan->dma_chan.device)->ops;\r\nstruct shdma_desc *sdesc;\r\nspin_lock_irq(&schan->chan_lock);\r\nlist_for_each_entry(sdesc, &schan->ld_queue, node) {\r\nif (sdesc->mark == DESC_SUBMITTED &&\r\nops->desc_completed(schan, sdesc)) {\r\ndev_dbg(schan->dev, "done #%d@%p\n",\r\nsdesc->async_tx.cookie, &sdesc->async_tx);\r\nsdesc->mark = DESC_COMPLETED;\r\nbreak;\r\n}\r\n}\r\nshdma_chan_xfer_ld_queue(schan);\r\nspin_unlock_irq(&schan->chan_lock);\r\nshdma_chan_ld_cleanup(schan, false);\r\nreturn IRQ_HANDLED;\r\n}\r\nint shdma_request_irq(struct shdma_chan *schan, int irq,\r\nunsigned long flags, const char *name)\r\n{\r\nint ret = devm_request_threaded_irq(schan->dev, irq, chan_irq,\r\nchan_irqt, flags, name, schan);\r\nschan->irq = ret < 0 ? ret : irq;\r\nreturn ret;\r\n}\r\nvoid shdma_chan_probe(struct shdma_dev *sdev,\r\nstruct shdma_chan *schan, int id)\r\n{\r\nschan->pm_state = SHDMA_PM_ESTABLISHED;\r\nschan->dma_chan.device = &sdev->dma_dev;\r\ndma_cookie_init(&schan->dma_chan);\r\nschan->dev = sdev->dma_dev.dev;\r\nschan->id = id;\r\nif (!schan->max_xfer_len)\r\nschan->max_xfer_len = PAGE_SIZE;\r\nspin_lock_init(&schan->chan_lock);\r\nINIT_LIST_HEAD(&schan->ld_queue);\r\nINIT_LIST_HEAD(&schan->ld_free);\r\nlist_add_tail(&schan->dma_chan.device_node,\r\n&sdev->dma_dev.channels);\r\nsdev->schan[id] = schan;\r\n}\r\nvoid shdma_chan_remove(struct shdma_chan *schan)\r\n{\r\nlist_del(&schan->dma_chan.device_node);\r\n}\r\nint shdma_init(struct device *dev, struct shdma_dev *sdev,\r\nint chan_num)\r\n{\r\nstruct dma_device *dma_dev = &sdev->dma_dev;\r\nif (!sdev->ops ||\r\n!sdev->desc_size ||\r\n!sdev->ops->embedded_desc ||\r\n!sdev->ops->start_xfer ||\r\n!sdev->ops->setup_xfer ||\r\n!sdev->ops->set_slave ||\r\n!sdev->ops->desc_setup ||\r\n!sdev->ops->slave_addr ||\r\n!sdev->ops->channel_busy ||\r\n!sdev->ops->halt_channel ||\r\n!sdev->ops->desc_completed)\r\nreturn -EINVAL;\r\nsdev->schan = kcalloc(chan_num, sizeof(*sdev->schan), GFP_KERNEL);\r\nif (!sdev->schan)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&dma_dev->channels);\r\ndma_dev->device_alloc_chan_resources\r\n= shdma_alloc_chan_resources;\r\ndma_dev->device_free_chan_resources = shdma_free_chan_resources;\r\ndma_dev->device_prep_dma_memcpy = shdma_prep_memcpy;\r\ndma_dev->device_tx_status = shdma_tx_status;\r\ndma_dev->device_issue_pending = shdma_issue_pending;\r\ndma_dev->device_prep_slave_sg = shdma_prep_slave_sg;\r\ndma_dev->device_prep_dma_cyclic = shdma_prep_dma_cyclic;\r\ndma_dev->device_config = shdma_config;\r\ndma_dev->device_terminate_all = shdma_terminate_all;\r\ndma_dev->dev = dev;\r\nreturn 0;\r\n}\r\nvoid shdma_cleanup(struct shdma_dev *sdev)\r\n{\r\nkfree(sdev->schan);\r\n}\r\nstatic int __init shdma_enter(void)\r\n{\r\nshdma_slave_used = kzalloc(DIV_ROUND_UP(slave_num, BITS_PER_LONG) *\r\nsizeof(long), GFP_KERNEL);\r\nif (!shdma_slave_used)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void __exit shdma_exit(void)\r\n{\r\nkfree(shdma_slave_used);\r\n}
