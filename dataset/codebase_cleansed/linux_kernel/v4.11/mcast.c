void rvt_driver_mcast_init(struct rvt_dev_info *rdi)\r\n{\r\nspin_lock_init(&rdi->n_mcast_grps_lock);\r\n}\r\nstatic struct rvt_mcast_qp *rvt_mcast_qp_alloc(struct rvt_qp *qp)\r\n{\r\nstruct rvt_mcast_qp *mqp;\r\nmqp = kmalloc(sizeof(*mqp), GFP_KERNEL);\r\nif (!mqp)\r\ngoto bail;\r\nmqp->qp = qp;\r\nrvt_get_qp(qp);\r\nbail:\r\nreturn mqp;\r\n}\r\nstatic void rvt_mcast_qp_free(struct rvt_mcast_qp *mqp)\r\n{\r\nstruct rvt_qp *qp = mqp->qp;\r\nrvt_put_qp(qp);\r\nkfree(mqp);\r\n}\r\nstatic struct rvt_mcast *rvt_mcast_alloc(union ib_gid *mgid)\r\n{\r\nstruct rvt_mcast *mcast;\r\nmcast = kzalloc(sizeof(*mcast), GFP_KERNEL);\r\nif (!mcast)\r\ngoto bail;\r\nmcast->mgid = *mgid;\r\nINIT_LIST_HEAD(&mcast->qp_list);\r\ninit_waitqueue_head(&mcast->wait);\r\natomic_set(&mcast->refcount, 0);\r\nbail:\r\nreturn mcast;\r\n}\r\nstatic void rvt_mcast_free(struct rvt_mcast *mcast)\r\n{\r\nstruct rvt_mcast_qp *p, *tmp;\r\nlist_for_each_entry_safe(p, tmp, &mcast->qp_list, list)\r\nrvt_mcast_qp_free(p);\r\nkfree(mcast);\r\n}\r\nstruct rvt_mcast *rvt_mcast_find(struct rvt_ibport *ibp, union ib_gid *mgid)\r\n{\r\nstruct rb_node *n;\r\nunsigned long flags;\r\nstruct rvt_mcast *found = NULL;\r\nspin_lock_irqsave(&ibp->lock, flags);\r\nn = ibp->mcast_tree.rb_node;\r\nwhile (n) {\r\nint ret;\r\nstruct rvt_mcast *mcast;\r\nmcast = rb_entry(n, struct rvt_mcast, rb_node);\r\nret = memcmp(mgid->raw, mcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0) {\r\nn = n->rb_left;\r\n} else if (ret > 0) {\r\nn = n->rb_right;\r\n} else {\r\natomic_inc(&mcast->refcount);\r\nfound = mcast;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&ibp->lock, flags);\r\nreturn found;\r\n}\r\nstatic int rvt_mcast_add(struct rvt_dev_info *rdi, struct rvt_ibport *ibp,\r\nstruct rvt_mcast *mcast, struct rvt_mcast_qp *mqp)\r\n{\r\nstruct rb_node **n = &ibp->mcast_tree.rb_node;\r\nstruct rb_node *pn = NULL;\r\nint ret;\r\nspin_lock_irq(&ibp->lock);\r\nwhile (*n) {\r\nstruct rvt_mcast *tmcast;\r\nstruct rvt_mcast_qp *p;\r\npn = *n;\r\ntmcast = rb_entry(pn, struct rvt_mcast, rb_node);\r\nret = memcmp(mcast->mgid.raw, tmcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0) {\r\nn = &pn->rb_left;\r\ncontinue;\r\n}\r\nif (ret > 0) {\r\nn = &pn->rb_right;\r\ncontinue;\r\n}\r\nlist_for_each_entry_rcu(p, &tmcast->qp_list, list) {\r\nif (p->qp == mqp->qp) {\r\nret = ESRCH;\r\ngoto bail;\r\n}\r\n}\r\nif (tmcast->n_attached ==\r\nrdi->dparms.props.max_mcast_qp_attach) {\r\nret = ENOMEM;\r\ngoto bail;\r\n}\r\ntmcast->n_attached++;\r\nlist_add_tail_rcu(&mqp->list, &tmcast->qp_list);\r\nret = EEXIST;\r\ngoto bail;\r\n}\r\nspin_lock(&rdi->n_mcast_grps_lock);\r\nif (rdi->n_mcast_grps_allocated == rdi->dparms.props.max_mcast_grp) {\r\nspin_unlock(&rdi->n_mcast_grps_lock);\r\nret = ENOMEM;\r\ngoto bail;\r\n}\r\nrdi->n_mcast_grps_allocated++;\r\nspin_unlock(&rdi->n_mcast_grps_lock);\r\nmcast->n_attached++;\r\nlist_add_tail_rcu(&mqp->list, &mcast->qp_list);\r\natomic_inc(&mcast->refcount);\r\nrb_link_node(&mcast->rb_node, pn, n);\r\nrb_insert_color(&mcast->rb_node, &ibp->mcast_tree);\r\nret = 0;\r\nbail:\r\nspin_unlock_irq(&ibp->lock);\r\nreturn ret;\r\n}\r\nint rvt_attach_mcast(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\r\n{\r\nstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\r\nstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\r\nstruct rvt_ibport *ibp = rdi->ports[qp->port_num - 1];\r\nstruct rvt_mcast *mcast;\r\nstruct rvt_mcast_qp *mqp;\r\nint ret = -ENOMEM;\r\nif (ibqp->qp_num <= 1 || qp->state == IB_QPS_RESET)\r\nreturn -EINVAL;\r\nmcast = rvt_mcast_alloc(gid);\r\nif (!mcast)\r\nreturn -ENOMEM;\r\nmqp = rvt_mcast_qp_alloc(qp);\r\nif (!mqp)\r\ngoto bail_mcast;\r\nswitch (rvt_mcast_add(rdi, ibp, mcast, mqp)) {\r\ncase ESRCH:\r\nret = 0;\r\ngoto bail_mqp;\r\ncase EEXIST:\r\nret = 0;\r\ngoto bail_mcast;\r\ncase ENOMEM:\r\nret = -ENOMEM;\r\ngoto bail_mqp;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\nbail_mqp:\r\nrvt_mcast_qp_free(mqp);\r\nbail_mcast:\r\nrvt_mcast_free(mcast);\r\nreturn ret;\r\n}\r\nint rvt_detach_mcast(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\r\n{\r\nstruct rvt_qp *qp = ibqp_to_rvtqp(ibqp);\r\nstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\r\nstruct rvt_ibport *ibp = rdi->ports[qp->port_num - 1];\r\nstruct rvt_mcast *mcast = NULL;\r\nstruct rvt_mcast_qp *p, *tmp, *delp = NULL;\r\nstruct rb_node *n;\r\nint last = 0;\r\nint ret = 0;\r\nif (ibqp->qp_num <= 1 || qp->state == IB_QPS_RESET)\r\nreturn -EINVAL;\r\nspin_lock_irq(&ibp->lock);\r\nn = ibp->mcast_tree.rb_node;\r\nwhile (1) {\r\nif (!n) {\r\nspin_unlock_irq(&ibp->lock);\r\nreturn -EINVAL;\r\n}\r\nmcast = rb_entry(n, struct rvt_mcast, rb_node);\r\nret = memcmp(gid->raw, mcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0)\r\nn = n->rb_left;\r\nelse if (ret > 0)\r\nn = n->rb_right;\r\nelse\r\nbreak;\r\n}\r\nlist_for_each_entry_safe(p, tmp, &mcast->qp_list, list) {\r\nif (p->qp != qp)\r\ncontinue;\r\nlist_del_rcu(&p->list);\r\nmcast->n_attached--;\r\ndelp = p;\r\nif (list_empty(&mcast->qp_list)) {\r\nrb_erase(&mcast->rb_node, &ibp->mcast_tree);\r\nlast = 1;\r\n}\r\nbreak;\r\n}\r\nspin_unlock_irq(&ibp->lock);\r\nif (!delp)\r\nreturn -EINVAL;\r\nwait_event(mcast->wait, atomic_read(&mcast->refcount) <= 1);\r\nrvt_mcast_qp_free(delp);\r\nif (last) {\r\natomic_dec(&mcast->refcount);\r\nwait_event(mcast->wait, !atomic_read(&mcast->refcount));\r\nrvt_mcast_free(mcast);\r\nspin_lock_irq(&rdi->n_mcast_grps_lock);\r\nrdi->n_mcast_grps_allocated--;\r\nspin_unlock_irq(&rdi->n_mcast_grps_lock);\r\n}\r\nreturn 0;\r\n}\r\nint rvt_mcast_tree_empty(struct rvt_dev_info *rdi)\r\n{\r\nint i;\r\nint in_use = 0;\r\nfor (i = 0; i < rdi->dparms.nports; i++)\r\nif (rdi->ports[i]->mcast_tree.rb_node)\r\nin_use++;\r\nreturn in_use;\r\n}
