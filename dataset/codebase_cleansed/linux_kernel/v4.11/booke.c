void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nprintk("pc: %08lx msr: %08llx\n", vcpu->arch.pc, vcpu->arch.shared->msr);\r\nprintk("lr: %08lx ctr: %08lx\n", vcpu->arch.lr, vcpu->arch.ctr);\r\nprintk("srr0: %08llx srr1: %08llx\n", vcpu->arch.shared->srr0,\r\nvcpu->arch.shared->srr1);\r\nprintk("exceptions: %08lx\n", vcpu->arch.pending_exceptions);\r\nfor (i = 0; i < 32; i += 4) {\r\nprintk("gpr%02d: %08lx %08lx %08lx %08lx\n", i,\r\nkvmppc_get_gpr(vcpu, i),\r\nkvmppc_get_gpr(vcpu, i+1),\r\nkvmppc_get_gpr(vcpu, i+2),\r\nkvmppc_get_gpr(vcpu, i+3));\r\n}\r\n}\r\nvoid kvmppc_vcpu_disable_spe(struct kvm_vcpu *vcpu)\r\n{\r\npreempt_disable();\r\nenable_kernel_spe();\r\nkvmppc_save_guest_spe(vcpu);\r\ndisable_kernel_spe();\r\nvcpu->arch.shadow_msr &= ~MSR_SPE;\r\npreempt_enable();\r\n}\r\nstatic void kvmppc_vcpu_enable_spe(struct kvm_vcpu *vcpu)\r\n{\r\npreempt_disable();\r\nenable_kernel_spe();\r\nkvmppc_load_guest_spe(vcpu);\r\ndisable_kernel_spe();\r\nvcpu->arch.shadow_msr |= MSR_SPE;\r\npreempt_enable();\r\n}\r\nstatic void kvmppc_vcpu_sync_spe(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.shared->msr & MSR_SPE) {\r\nif (!(vcpu->arch.shadow_msr & MSR_SPE))\r\nkvmppc_vcpu_enable_spe(vcpu);\r\n} else if (vcpu->arch.shadow_msr & MSR_SPE) {\r\nkvmppc_vcpu_disable_spe(vcpu);\r\n}\r\n}\r\nstatic void kvmppc_vcpu_sync_spe(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic inline void kvmppc_load_guest_fp(struct kvm_vcpu *vcpu)\r\n{\r\n#ifdef CONFIG_PPC_FPU\r\nif (!(current->thread.regs->msr & MSR_FP)) {\r\nenable_kernel_fp();\r\nload_fp_state(&vcpu->arch.fp);\r\ndisable_kernel_fp();\r\ncurrent->thread.fp_save_area = &vcpu->arch.fp;\r\ncurrent->thread.regs->msr |= MSR_FP;\r\n}\r\n#endif\r\n}\r\nstatic inline void kvmppc_save_guest_fp(struct kvm_vcpu *vcpu)\r\n{\r\n#ifdef CONFIG_PPC_FPU\r\nif (current->thread.regs->msr & MSR_FP)\r\ngiveup_fpu(current);\r\ncurrent->thread.fp_save_area = NULL;\r\n#endif\r\n}\r\nstatic void kvmppc_vcpu_sync_fpu(struct kvm_vcpu *vcpu)\r\n{\r\n#if defined(CONFIG_PPC_FPU) && !defined(CONFIG_KVM_BOOKE_HV)\r\nvcpu->arch.shadow_msr &= ~MSR_FP;\r\nvcpu->arch.shadow_msr |= vcpu->arch.shared->msr & MSR_FP;\r\n#endif\r\n}\r\nstatic inline void kvmppc_load_guest_altivec(struct kvm_vcpu *vcpu)\r\n{\r\n#ifdef CONFIG_ALTIVEC\r\nif (cpu_has_feature(CPU_FTR_ALTIVEC)) {\r\nif (!(current->thread.regs->msr & MSR_VEC)) {\r\nenable_kernel_altivec();\r\nload_vr_state(&vcpu->arch.vr);\r\ndisable_kernel_altivec();\r\ncurrent->thread.vr_save_area = &vcpu->arch.vr;\r\ncurrent->thread.regs->msr |= MSR_VEC;\r\n}\r\n}\r\n#endif\r\n}\r\nstatic inline void kvmppc_save_guest_altivec(struct kvm_vcpu *vcpu)\r\n{\r\n#ifdef CONFIG_ALTIVEC\r\nif (cpu_has_feature(CPU_FTR_ALTIVEC)) {\r\nif (current->thread.regs->msr & MSR_VEC)\r\ngiveup_altivec(current);\r\ncurrent->thread.vr_save_area = NULL;\r\n}\r\n#endif\r\n}\r\nstatic void kvmppc_vcpu_sync_debug(struct kvm_vcpu *vcpu)\r\n{\r\n#ifndef CONFIG_KVM_BOOKE_HV\r\nvcpu->arch.shadow_msr &= ~MSR_DE;\r\nvcpu->arch.shadow_msr |= vcpu->arch.shared->msr & MSR_DE;\r\n#endif\r\nif (vcpu->guest_debug) {\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\nvcpu->arch.shared->msr |= MSR_DE;\r\n#else\r\nvcpu->arch.shadow_msr |= MSR_DE;\r\nvcpu->arch.shared->msr &= ~MSR_DE;\r\n#endif\r\n}\r\n}\r\nvoid kvmppc_set_msr(struct kvm_vcpu *vcpu, u32 new_msr)\r\n{\r\nu32 old_msr = vcpu->arch.shared->msr;\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\nnew_msr |= MSR_GS;\r\n#endif\r\nvcpu->arch.shared->msr = new_msr;\r\nkvmppc_mmu_msr_notify(vcpu, old_msr);\r\nkvmppc_vcpu_sync_spe(vcpu);\r\nkvmppc_vcpu_sync_fpu(vcpu);\r\nkvmppc_vcpu_sync_debug(vcpu);\r\n}\r\nstatic void kvmppc_booke_queue_irqprio(struct kvm_vcpu *vcpu,\r\nunsigned int priority)\r\n{\r\ntrace_kvm_booke_queue_irqprio(vcpu, priority);\r\nset_bit(priority, &vcpu->arch.pending_exceptions);\r\n}\r\nvoid kvmppc_core_queue_dtlb_miss(struct kvm_vcpu *vcpu,\r\nulong dear_flags, ulong esr_flags)\r\n{\r\nvcpu->arch.queued_dear = dear_flags;\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_DTLB_MISS);\r\n}\r\nvoid kvmppc_core_queue_data_storage(struct kvm_vcpu *vcpu,\r\nulong dear_flags, ulong esr_flags)\r\n{\r\nvcpu->arch.queued_dear = dear_flags;\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_DATA_STORAGE);\r\n}\r\nvoid kvmppc_core_queue_itlb_miss(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_ITLB_MISS);\r\n}\r\nvoid kvmppc_core_queue_inst_storage(struct kvm_vcpu *vcpu, ulong esr_flags)\r\n{\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_INST_STORAGE);\r\n}\r\nstatic void kvmppc_core_queue_alignment(struct kvm_vcpu *vcpu, ulong dear_flags,\r\nulong esr_flags)\r\n{\r\nvcpu->arch.queued_dear = dear_flags;\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_ALIGNMENT);\r\n}\r\nvoid kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong esr_flags)\r\n{\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_PROGRAM);\r\n}\r\nvoid kvmppc_core_queue_dec(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_DECREMENTER);\r\n}\r\nint kvmppc_core_pending_dec(struct kvm_vcpu *vcpu)\r\n{\r\nreturn test_bit(BOOKE_IRQPRIO_DECREMENTER, &vcpu->arch.pending_exceptions);\r\n}\r\nvoid kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu)\r\n{\r\nclear_bit(BOOKE_IRQPRIO_DECREMENTER, &vcpu->arch.pending_exceptions);\r\n}\r\nvoid kvmppc_core_queue_external(struct kvm_vcpu *vcpu,\r\nstruct kvm_interrupt *irq)\r\n{\r\nunsigned int prio = BOOKE_IRQPRIO_EXTERNAL;\r\nif (irq->irq == KVM_INTERRUPT_SET_LEVEL)\r\nprio = BOOKE_IRQPRIO_EXTERNAL_LEVEL;\r\nkvmppc_booke_queue_irqprio(vcpu, prio);\r\n}\r\nvoid kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu)\r\n{\r\nclear_bit(BOOKE_IRQPRIO_EXTERNAL, &vcpu->arch.pending_exceptions);\r\nclear_bit(BOOKE_IRQPRIO_EXTERNAL_LEVEL, &vcpu->arch.pending_exceptions);\r\n}\r\nstatic void kvmppc_core_queue_watchdog(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_WATCHDOG);\r\n}\r\nstatic void kvmppc_core_dequeue_watchdog(struct kvm_vcpu *vcpu)\r\n{\r\nclear_bit(BOOKE_IRQPRIO_WATCHDOG, &vcpu->arch.pending_exceptions);\r\n}\r\nvoid kvmppc_core_queue_debug(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_DEBUG);\r\n}\r\nvoid kvmppc_core_dequeue_debug(struct kvm_vcpu *vcpu)\r\n{\r\nclear_bit(BOOKE_IRQPRIO_DEBUG, &vcpu->arch.pending_exceptions);\r\n}\r\nstatic void set_guest_srr(struct kvm_vcpu *vcpu, unsigned long srr0, u32 srr1)\r\n{\r\nkvmppc_set_srr0(vcpu, srr0);\r\nkvmppc_set_srr1(vcpu, srr1);\r\n}\r\nstatic void set_guest_csrr(struct kvm_vcpu *vcpu, unsigned long srr0, u32 srr1)\r\n{\r\nvcpu->arch.csrr0 = srr0;\r\nvcpu->arch.csrr1 = srr1;\r\n}\r\nstatic void set_guest_dsrr(struct kvm_vcpu *vcpu, unsigned long srr0, u32 srr1)\r\n{\r\nif (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC)) {\r\nvcpu->arch.dsrr0 = srr0;\r\nvcpu->arch.dsrr1 = srr1;\r\n} else {\r\nset_guest_csrr(vcpu, srr0, srr1);\r\n}\r\n}\r\nstatic void set_guest_mcsrr(struct kvm_vcpu *vcpu, unsigned long srr0, u32 srr1)\r\n{\r\nvcpu->arch.mcsrr0 = srr0;\r\nvcpu->arch.mcsrr1 = srr1;\r\n}\r\nstatic int kvmppc_booke_irqprio_deliver(struct kvm_vcpu *vcpu,\r\nunsigned int priority)\r\n{\r\nint allowed = 0;\r\nulong msr_mask = 0;\r\nbool update_esr = false, update_dear = false, update_epr = false;\r\nulong crit_raw = vcpu->arch.shared->critical;\r\nulong crit_r1 = kvmppc_get_gpr(vcpu, 1);\r\nbool crit;\r\nbool keep_irq = false;\r\nenum int_class int_class;\r\nulong new_msr = vcpu->arch.shared->msr;\r\nif (!(vcpu->arch.shared->msr & MSR_SF)) {\r\ncrit_raw &= 0xffffffff;\r\ncrit_r1 &= 0xffffffff;\r\n}\r\ncrit = (crit_raw == crit_r1);\r\ncrit = crit && !(vcpu->arch.shared->msr & MSR_PR);\r\nif (priority == BOOKE_IRQPRIO_EXTERNAL_LEVEL) {\r\npriority = BOOKE_IRQPRIO_EXTERNAL;\r\nkeep_irq = true;\r\n}\r\nif ((priority == BOOKE_IRQPRIO_EXTERNAL) && vcpu->arch.epr_flags)\r\nupdate_epr = true;\r\nswitch (priority) {\r\ncase BOOKE_IRQPRIO_DTLB_MISS:\r\ncase BOOKE_IRQPRIO_DATA_STORAGE:\r\ncase BOOKE_IRQPRIO_ALIGNMENT:\r\nupdate_dear = true;\r\ncase BOOKE_IRQPRIO_INST_STORAGE:\r\ncase BOOKE_IRQPRIO_PROGRAM:\r\nupdate_esr = true;\r\ncase BOOKE_IRQPRIO_ITLB_MISS:\r\ncase BOOKE_IRQPRIO_SYSCALL:\r\ncase BOOKE_IRQPRIO_FP_UNAVAIL:\r\n#ifdef CONFIG_SPE_POSSIBLE\r\ncase BOOKE_IRQPRIO_SPE_UNAVAIL:\r\ncase BOOKE_IRQPRIO_SPE_FP_DATA:\r\ncase BOOKE_IRQPRIO_SPE_FP_ROUND:\r\n#endif\r\n#ifdef CONFIG_ALTIVEC\r\ncase BOOKE_IRQPRIO_ALTIVEC_UNAVAIL:\r\ncase BOOKE_IRQPRIO_ALTIVEC_ASSIST:\r\n#endif\r\ncase BOOKE_IRQPRIO_AP_UNAVAIL:\r\nallowed = 1;\r\nmsr_mask = MSR_CE | MSR_ME | MSR_DE;\r\nint_class = INT_CLASS_NONCRIT;\r\nbreak;\r\ncase BOOKE_IRQPRIO_WATCHDOG:\r\ncase BOOKE_IRQPRIO_CRITICAL:\r\ncase BOOKE_IRQPRIO_DBELL_CRIT:\r\nallowed = vcpu->arch.shared->msr & MSR_CE;\r\nallowed = allowed && !crit;\r\nmsr_mask = MSR_ME;\r\nint_class = INT_CLASS_CRIT;\r\nbreak;\r\ncase BOOKE_IRQPRIO_MACHINE_CHECK:\r\nallowed = vcpu->arch.shared->msr & MSR_ME;\r\nallowed = allowed && !crit;\r\nint_class = INT_CLASS_MC;\r\nbreak;\r\ncase BOOKE_IRQPRIO_DECREMENTER:\r\ncase BOOKE_IRQPRIO_FIT:\r\nkeep_irq = true;\r\ncase BOOKE_IRQPRIO_EXTERNAL:\r\ncase BOOKE_IRQPRIO_DBELL:\r\nallowed = vcpu->arch.shared->msr & MSR_EE;\r\nallowed = allowed && !crit;\r\nmsr_mask = MSR_CE | MSR_ME | MSR_DE;\r\nint_class = INT_CLASS_NONCRIT;\r\nbreak;\r\ncase BOOKE_IRQPRIO_DEBUG:\r\nallowed = vcpu->arch.shared->msr & MSR_DE;\r\nallowed = allowed && !crit;\r\nmsr_mask = MSR_ME;\r\nif (cpu_has_feature(CPU_FTR_DEBUG_LVL_EXC))\r\nint_class = INT_CLASS_DBG;\r\nelse\r\nint_class = INT_CLASS_CRIT;\r\nbreak;\r\n}\r\nif (allowed) {\r\nswitch (int_class) {\r\ncase INT_CLASS_NONCRIT:\r\nset_guest_srr(vcpu, vcpu->arch.pc,\r\nvcpu->arch.shared->msr);\r\nbreak;\r\ncase INT_CLASS_CRIT:\r\nset_guest_csrr(vcpu, vcpu->arch.pc,\r\nvcpu->arch.shared->msr);\r\nbreak;\r\ncase INT_CLASS_DBG:\r\nset_guest_dsrr(vcpu, vcpu->arch.pc,\r\nvcpu->arch.shared->msr);\r\nbreak;\r\ncase INT_CLASS_MC:\r\nset_guest_mcsrr(vcpu, vcpu->arch.pc,\r\nvcpu->arch.shared->msr);\r\nbreak;\r\n}\r\nvcpu->arch.pc = vcpu->arch.ivpr | vcpu->arch.ivor[priority];\r\nif (update_esr == true)\r\nkvmppc_set_esr(vcpu, vcpu->arch.queued_esr);\r\nif (update_dear == true)\r\nkvmppc_set_dar(vcpu, vcpu->arch.queued_dear);\r\nif (update_epr == true) {\r\nif (vcpu->arch.epr_flags & KVMPPC_EPR_USER)\r\nkvm_make_request(KVM_REQ_EPR_EXIT, vcpu);\r\nelse if (vcpu->arch.epr_flags & KVMPPC_EPR_KERNEL) {\r\nBUG_ON(vcpu->arch.irq_type != KVMPPC_IRQ_MPIC);\r\nkvmppc_mpic_set_epr(vcpu);\r\n}\r\n}\r\nnew_msr &= msr_mask;\r\n#if defined(CONFIG_64BIT)\r\nif (vcpu->arch.epcr & SPRN_EPCR_ICM)\r\nnew_msr |= MSR_CM;\r\n#endif\r\nkvmppc_set_msr(vcpu, new_msr);\r\nif (!keep_irq)\r\nclear_bit(priority, &vcpu->arch.pending_exceptions);\r\n}\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\nif (vcpu->arch.pending_exceptions & BOOKE_IRQMASK_EE)\r\nkvmppc_set_pending_interrupt(vcpu, INT_CLASS_NONCRIT);\r\nif (vcpu->arch.pending_exceptions & BOOKE_IRQMASK_CE)\r\nkvmppc_set_pending_interrupt(vcpu, INT_CLASS_CRIT);\r\nif (vcpu->arch.pending_exceptions & BOOKE_IRQPRIO_MACHINE_CHECK)\r\nkvmppc_set_pending_interrupt(vcpu, INT_CLASS_MC);\r\n#endif\r\nreturn allowed;\r\n}\r\nstatic unsigned long watchdog_next_timeout(struct kvm_vcpu *vcpu)\r\n{\r\nu64 tb, wdt_tb, wdt_ticks = 0;\r\nu64 nr_jiffies = 0;\r\nu32 period = TCR_GET_WP(vcpu->arch.tcr);\r\nwdt_tb = 1ULL << (63 - period);\r\ntb = get_tb();\r\nif (tb & wdt_tb)\r\nwdt_ticks = wdt_tb;\r\nwdt_ticks += wdt_tb - (tb & (wdt_tb - 1));\r\nnr_jiffies = wdt_ticks;\r\nif (do_div(nr_jiffies, tb_ticks_per_jiffy))\r\nnr_jiffies++;\r\nreturn min_t(unsigned long long, nr_jiffies, NEXT_TIMER_MAX_DELTA);\r\n}\r\nstatic void arm_next_watchdog(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long nr_jiffies;\r\nunsigned long flags;\r\nif ((vcpu->arch.tsr & (TSR_ENW | TSR_WIS)) != (TSR_ENW | TSR_WIS))\r\nclear_bit(KVM_REQ_WATCHDOG, &vcpu->requests);\r\nspin_lock_irqsave(&vcpu->arch.wdt_lock, flags);\r\nnr_jiffies = watchdog_next_timeout(vcpu);\r\nif (nr_jiffies < NEXT_TIMER_MAX_DELTA)\r\nmod_timer(&vcpu->arch.wdt_timer, jiffies + nr_jiffies);\r\nelse\r\ndel_timer(&vcpu->arch.wdt_timer);\r\nspin_unlock_irqrestore(&vcpu->arch.wdt_lock, flags);\r\n}\r\nvoid kvmppc_watchdog_func(unsigned long data)\r\n{\r\nstruct kvm_vcpu *vcpu = (struct kvm_vcpu *)data;\r\nu32 tsr, new_tsr;\r\nint final;\r\ndo {\r\nnew_tsr = tsr = vcpu->arch.tsr;\r\nfinal = 0;\r\nif (tsr & TSR_ENW) {\r\nif (tsr & TSR_WIS)\r\nfinal = 1;\r\nelse\r\nnew_tsr = tsr | TSR_WIS;\r\n} else {\r\nnew_tsr = tsr | TSR_ENW;\r\n}\r\n} while (cmpxchg(&vcpu->arch.tsr, tsr, new_tsr) != tsr);\r\nif (new_tsr & TSR_WIS) {\r\nsmp_wmb();\r\nkvm_make_request(KVM_REQ_PENDING_TIMER, vcpu);\r\nkvm_vcpu_kick(vcpu);\r\n}\r\nif (final && (vcpu->arch.tcr & TCR_WRC_MASK) &&\r\nvcpu->arch.watchdog_enabled) {\r\nsmp_wmb();\r\nkvm_make_request(KVM_REQ_WATCHDOG, vcpu);\r\nkvm_vcpu_kick(vcpu);\r\n}\r\nif (!final)\r\narm_next_watchdog(vcpu);\r\n}\r\nstatic void update_timer_ints(struct kvm_vcpu *vcpu)\r\n{\r\nif ((vcpu->arch.tcr & TCR_DIE) && (vcpu->arch.tsr & TSR_DIS))\r\nkvmppc_core_queue_dec(vcpu);\r\nelse\r\nkvmppc_core_dequeue_dec(vcpu);\r\nif ((vcpu->arch.tcr & TCR_WIE) && (vcpu->arch.tsr & TSR_WIS))\r\nkvmppc_core_queue_watchdog(vcpu);\r\nelse\r\nkvmppc_core_dequeue_watchdog(vcpu);\r\n}\r\nstatic void kvmppc_core_check_exceptions(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long *pending = &vcpu->arch.pending_exceptions;\r\nunsigned int priority;\r\npriority = __ffs(*pending);\r\nwhile (priority < BOOKE_IRQPRIO_MAX) {\r\nif (kvmppc_booke_irqprio_deliver(vcpu, priority))\r\nbreak;\r\npriority = find_next_bit(pending,\r\nBITS_PER_BYTE * sizeof(*pending),\r\npriority + 1);\r\n}\r\nvcpu->arch.shared->int_pending = !!*pending;\r\n}\r\nint kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu)\r\n{\r\nint r = 0;\r\nWARN_ON_ONCE(!irqs_disabled());\r\nkvmppc_core_check_exceptions(vcpu);\r\nif (vcpu->requests) {\r\nreturn 1;\r\n}\r\nif (vcpu->arch.shared->msr & MSR_WE) {\r\nlocal_irq_enable();\r\nkvm_vcpu_block(vcpu);\r\nclear_bit(KVM_REQ_UNHALT, &vcpu->requests);\r\nhard_irq_disable();\r\nkvmppc_set_exit_type(vcpu, EMULATED_MTMSRWE_EXITS);\r\nr = 1;\r\n};\r\nreturn r;\r\n}\r\nint kvmppc_core_check_requests(struct kvm_vcpu *vcpu)\r\n{\r\nint r = 1;\r\nif (kvm_check_request(KVM_REQ_PENDING_TIMER, vcpu))\r\nupdate_timer_ints(vcpu);\r\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\r\nif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))\r\nkvmppc_core_flush_tlb(vcpu);\r\n#endif\r\nif (kvm_check_request(KVM_REQ_WATCHDOG, vcpu)) {\r\nvcpu->run->exit_reason = KVM_EXIT_WATCHDOG;\r\nr = 0;\r\n}\r\nif (kvm_check_request(KVM_REQ_EPR_EXIT, vcpu)) {\r\nvcpu->run->epr.epr = 0;\r\nvcpu->arch.epr_needed = true;\r\nvcpu->run->exit_reason = KVM_EXIT_EPR;\r\nr = 0;\r\n}\r\nreturn r;\r\n}\r\nint kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)\r\n{\r\nint ret, s;\r\nstruct debug_reg debug;\r\nif (!vcpu->arch.sane) {\r\nkvm_run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn -EINVAL;\r\n}\r\ns = kvmppc_prepare_to_enter(vcpu);\r\nif (s <= 0) {\r\nret = s;\r\ngoto out;\r\n}\r\n#ifdef CONFIG_PPC_FPU\r\nenable_kernel_fp();\r\nkvmppc_load_guest_fp(vcpu);\r\n#endif\r\n#ifdef CONFIG_ALTIVEC\r\nif (cpu_has_feature(CPU_FTR_ALTIVEC))\r\nenable_kernel_altivec();\r\nkvmppc_load_guest_altivec(vcpu);\r\n#endif\r\ndebug = vcpu->arch.dbg_reg;\r\nswitch_booke_debug_regs(&debug);\r\ndebug = current->thread.debug;\r\ncurrent->thread.debug = vcpu->arch.dbg_reg;\r\nvcpu->arch.pgdir = current->mm->pgd;\r\nkvmppc_fix_ee_before_entry();\r\nret = __kvmppc_vcpu_run(kvm_run, vcpu);\r\nswitch_booke_debug_regs(&debug);\r\ncurrent->thread.debug = debug;\r\n#ifdef CONFIG_PPC_FPU\r\nkvmppc_save_guest_fp(vcpu);\r\n#endif\r\n#ifdef CONFIG_ALTIVEC\r\nkvmppc_save_guest_altivec(vcpu);\r\n#endif\r\nout:\r\nvcpu->mode = OUTSIDE_GUEST_MODE;\r\nreturn ret;\r\n}\r\nstatic int emulation_exit(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er;\r\ner = kvmppc_emulate_instruction(run, vcpu);\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nkvmppc_account_exit_stat(vcpu, EMULATED_INST_EXITS);\r\nreturn RESUME_GUEST_NV;\r\ncase EMULATE_AGAIN:\r\nreturn RESUME_GUEST;\r\ncase EMULATE_FAIL:\r\nprintk(KERN_CRIT "%s: emulation at %lx failed (%08x)\n",\r\n__func__, vcpu->arch.pc, vcpu->arch.last_inst);\r\nrun->hw.hardware_exit_reason = ~0ULL << 32;\r\nrun->hw.hardware_exit_reason |= vcpu->arch.last_inst;\r\nkvmppc_core_queue_program(vcpu, ESR_PIL);\r\nreturn RESUME_HOST;\r\ncase EMULATE_EXIT_USER:\r\nreturn RESUME_HOST;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic int kvmppc_handle_debug(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct debug_reg *dbg_reg = &(vcpu->arch.dbg_reg);\r\nu32 dbsr = vcpu->arch.dbsr;\r\nif (vcpu->guest_debug == 0) {\r\nif (dbsr & DBSR_IDE) {\r\ndbsr &= ~DBSR_IDE;\r\nif (!dbsr)\r\nreturn RESUME_GUEST;\r\n}\r\nif (dbsr && (vcpu->arch.shared->msr & MSR_DE) &&\r\n(vcpu->arch.dbg_reg.dbcr0 & DBCR0_IDM))\r\nkvmppc_core_queue_debug(vcpu);\r\nif ((dbsr & DBSR_TIE) && !(vcpu->arch.shared->msr & MSR_DE))\r\nkvmppc_core_queue_program(vcpu, ESR_PTR);\r\nreturn RESUME_GUEST;\r\n}\r\nvcpu->arch.dbsr = 0;\r\nrun->debug.arch.status = 0;\r\nrun->debug.arch.address = vcpu->arch.pc;\r\nif (dbsr & (DBSR_IAC1 | DBSR_IAC2 | DBSR_IAC3 | DBSR_IAC4)) {\r\nrun->debug.arch.status |= KVMPPC_DEBUG_BREAKPOINT;\r\n} else {\r\nif (dbsr & (DBSR_DAC1W | DBSR_DAC2W))\r\nrun->debug.arch.status |= KVMPPC_DEBUG_WATCH_WRITE;\r\nelse if (dbsr & (DBSR_DAC1R | DBSR_DAC2R))\r\nrun->debug.arch.status |= KVMPPC_DEBUG_WATCH_READ;\r\nif (dbsr & (DBSR_DAC1R | DBSR_DAC1W))\r\nrun->debug.arch.address = dbg_reg->dac1;\r\nelse if (dbsr & (DBSR_DAC2R | DBSR_DAC2W))\r\nrun->debug.arch.address = dbg_reg->dac2;\r\n}\r\nreturn RESUME_HOST;\r\n}\r\nstatic void kvmppc_fill_pt_regs(struct pt_regs *regs)\r\n{\r\nulong r1, ip, msr, lr;\r\nasm("mr %0, 1" : "=r"(r1));\r\nasm("mflr %0" : "=r"(lr));\r\nasm("mfmsr %0" : "=r"(msr));\r\nasm("bl 1f; 1: mflr %0" : "=r"(ip));\r\nmemset(regs, 0, sizeof(*regs));\r\nregs->gpr[1] = r1;\r\nregs->nip = ip;\r\nregs->msr = msr;\r\nregs->link = lr;\r\n}\r\nstatic void kvmppc_restart_interrupt(struct kvm_vcpu *vcpu,\r\nunsigned int exit_nr)\r\n{\r\nstruct pt_regs regs;\r\nswitch (exit_nr) {\r\ncase BOOKE_INTERRUPT_EXTERNAL:\r\nkvmppc_fill_pt_regs(&regs);\r\ndo_IRQ(&regs);\r\nbreak;\r\ncase BOOKE_INTERRUPT_DECREMENTER:\r\nkvmppc_fill_pt_regs(&regs);\r\ntimer_interrupt(&regs);\r\nbreak;\r\n#if defined(CONFIG_PPC_DOORBELL)\r\ncase BOOKE_INTERRUPT_DOORBELL:\r\nkvmppc_fill_pt_regs(&regs);\r\ndoorbell_exception(&regs);\r\nbreak;\r\n#endif\r\ncase BOOKE_INTERRUPT_MACHINE_CHECK:\r\nbreak;\r\ncase BOOKE_INTERRUPT_PERFORMANCE_MONITOR:\r\nkvmppc_fill_pt_regs(&regs);\r\nperformance_monitor_exception(&regs);\r\nbreak;\r\ncase BOOKE_INTERRUPT_WATCHDOG:\r\nkvmppc_fill_pt_regs(&regs);\r\n#ifdef CONFIG_BOOKE_WDT\r\nWatchdogException(&regs);\r\n#else\r\nunknown_exception(&regs);\r\n#endif\r\nbreak;\r\ncase BOOKE_INTERRUPT_CRITICAL:\r\nkvmppc_fill_pt_regs(&regs);\r\nunknown_exception(&regs);\r\nbreak;\r\ncase BOOKE_INTERRUPT_DEBUG:\r\nvcpu->arch.dbsr = mfspr(SPRN_DBSR);\r\nkvmppc_clear_dbsr();\r\nbreak;\r\n}\r\n}\r\nstatic int kvmppc_resume_inst_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nenum emulation_result emulated, u32 last_inst)\r\n{\r\nswitch (emulated) {\r\ncase EMULATE_AGAIN:\r\nreturn RESUME_GUEST;\r\ncase EMULATE_FAIL:\r\npr_debug("%s: load instruction from guest address %lx failed\n",\r\n__func__, vcpu->arch.pc);\r\nrun->hw.hardware_exit_reason = ~0ULL << 32;\r\nrun->hw.hardware_exit_reason |= last_inst;\r\nkvmppc_core_queue_program(vcpu, ESR_PIL);\r\nreturn RESUME_HOST;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nint kvmppc_handle_exit(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int exit_nr)\r\n{\r\nint r = RESUME_HOST;\r\nint s;\r\nint idx;\r\nu32 last_inst = KVM_INST_FETCH_FAILED;\r\nenum emulation_result emulated = EMULATE_DONE;\r\nkvmppc_update_timing_stats(vcpu);\r\nkvmppc_restart_interrupt(vcpu, exit_nr);\r\nswitch (exit_nr) {\r\ncase BOOKE_INTERRUPT_DATA_STORAGE:\r\ncase BOOKE_INTERRUPT_DTLB_MISS:\r\ncase BOOKE_INTERRUPT_HV_PRIV:\r\nemulated = kvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst);\r\nbreak;\r\ncase BOOKE_INTERRUPT_PROGRAM:\r\nif (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\r\nemulated = kvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\ntrace_kvm_exit(exit_nr, vcpu);\r\nguest_exit_irqoff();\r\nlocal_irq_enable();\r\nrun->exit_reason = KVM_EXIT_UNKNOWN;\r\nrun->ready_for_interrupt_injection = 1;\r\nif (emulated != EMULATE_DONE) {\r\nr = kvmppc_resume_inst_load(run, vcpu, emulated, last_inst);\r\ngoto out;\r\n}\r\nswitch (exit_nr) {\r\ncase BOOKE_INTERRUPT_MACHINE_CHECK:\r\nprintk("MACHINE CHECK: %lx\n", mfspr(SPRN_MCSR));\r\nkvmppc_dump_vcpu(vcpu);\r\nrun->hw.hardware_exit_reason = ~1ULL << 32;\r\nrun->hw.hardware_exit_reason |= mfspr(SPRN_MCSR);\r\nr = RESUME_HOST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_EXTERNAL:\r\nkvmppc_account_exit(vcpu, EXT_INTR_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_DECREMENTER:\r\nkvmppc_account_exit(vcpu, DEC_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_WATCHDOG:\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_DOORBELL:\r\nkvmppc_account_exit(vcpu, DBELL_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_GUEST_DBELL_CRIT:\r\nkvmppc_account_exit(vcpu, GDBELL_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_GUEST_DBELL:\r\nkvmppc_account_exit(vcpu, GDBELL_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_PERFORMANCE_MONITOR:\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_HV_PRIV:\r\nr = emulation_exit(run, vcpu);\r\nbreak;\r\ncase BOOKE_INTERRUPT_PROGRAM:\r\nif ((vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP) &&\r\n(last_inst == KVMPPC_INST_SW_BREAKPOINT)) {\r\nr = kvmppc_handle_debug(run, vcpu);\r\nrun->exit_reason = KVM_EXIT_DEBUG;\r\nkvmppc_account_exit(vcpu, DEBUG_EXITS);\r\nbreak;\r\n}\r\nif (vcpu->arch.shared->msr & (MSR_PR | MSR_GS)) {\r\nkvmppc_core_queue_program(vcpu, vcpu->arch.fault_esr);\r\nr = RESUME_GUEST;\r\nkvmppc_account_exit(vcpu, USR_PR_INST);\r\nbreak;\r\n}\r\nr = emulation_exit(run, vcpu);\r\nbreak;\r\ncase BOOKE_INTERRUPT_FP_UNAVAIL:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_FP_UNAVAIL);\r\nkvmppc_account_exit(vcpu, FP_UNAVAIL);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#ifdef CONFIG_SPE\r\ncase BOOKE_INTERRUPT_SPE_UNAVAIL: {\r\nif (vcpu->arch.shared->msr & MSR_SPE)\r\nkvmppc_vcpu_enable_spe(vcpu);\r\nelse\r\nkvmppc_booke_queue_irqprio(vcpu,\r\nBOOKE_IRQPRIO_SPE_UNAVAIL);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\ncase BOOKE_INTERRUPT_SPE_FP_DATA:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_SPE_FP_DATA);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_SPE_FP_ROUND:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_SPE_FP_ROUND);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#elif defined(CONFIG_SPE_POSSIBLE)\r\ncase BOOKE_INTERRUPT_SPE_UNAVAIL:\r\nkvmppc_core_queue_program(vcpu, ESR_PUO | ESR_SPV);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_SPE_FP_DATA:\r\ncase BOOKE_INTERRUPT_SPE_FP_ROUND:\r\nprintk(KERN_CRIT "%s: unexpected SPE interrupt %u at %08lx\n",\r\n__func__, exit_nr, vcpu->arch.pc);\r\nrun->hw.hardware_exit_reason = exit_nr;\r\nr = RESUME_HOST;\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_ALTIVEC\r\ncase BOOKE_INTERRUPT_ALTIVEC_UNAVAIL:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_ALTIVEC_UNAVAIL);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_ALTIVEC_ASSIST:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_ALTIVEC_ASSIST);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#endif\r\ncase BOOKE_INTERRUPT_DATA_STORAGE:\r\nkvmppc_core_queue_data_storage(vcpu, vcpu->arch.fault_dear,\r\nvcpu->arch.fault_esr);\r\nkvmppc_account_exit(vcpu, DSI_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_INST_STORAGE:\r\nkvmppc_core_queue_inst_storage(vcpu, vcpu->arch.fault_esr);\r\nkvmppc_account_exit(vcpu, ISI_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_ALIGNMENT:\r\nkvmppc_core_queue_alignment(vcpu, vcpu->arch.fault_dear,\r\nvcpu->arch.fault_esr);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\ncase BOOKE_INTERRUPT_HV_SYSCALL:\r\nif (!(vcpu->arch.shared->msr & MSR_PR)) {\r\nkvmppc_set_gpr(vcpu, 3, kvmppc_kvm_pv(vcpu));\r\n} else {\r\nkvmppc_core_queue_program(vcpu, ESR_PPR);\r\n}\r\nr = RESUME_GUEST;\r\nbreak;\r\n#else\r\ncase BOOKE_INTERRUPT_SYSCALL:\r\nif (!(vcpu->arch.shared->msr & MSR_PR) &&\r\n(((u32)kvmppc_get_gpr(vcpu, 0)) == KVM_SC_MAGIC_R0)) {\r\nkvmppc_set_gpr(vcpu, 3, kvmppc_kvm_pv(vcpu));\r\nr = RESUME_GUEST;\r\n} else {\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_SYSCALL);\r\n}\r\nkvmppc_account_exit(vcpu, SYSCALL_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#endif\r\ncase BOOKE_INTERRUPT_DTLB_MISS: {\r\nunsigned long eaddr = vcpu->arch.fault_dear;\r\nint gtlb_index;\r\ngpa_t gpaddr;\r\ngfn_t gfn;\r\n#ifdef CONFIG_KVM_E500V2\r\nif (!(vcpu->arch.shared->msr & MSR_PR) &&\r\n(eaddr & PAGE_MASK) == vcpu->arch.magic_page_ea) {\r\nkvmppc_map_magic(vcpu);\r\nkvmppc_account_exit(vcpu, DTLB_VIRT_MISS_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n#endif\r\ngtlb_index = kvmppc_mmu_dtlb_index(vcpu, eaddr);\r\nif (gtlb_index < 0) {\r\nkvmppc_core_queue_dtlb_miss(vcpu,\r\nvcpu->arch.fault_dear,\r\nvcpu->arch.fault_esr);\r\nkvmppc_mmu_dtlb_miss(vcpu);\r\nkvmppc_account_exit(vcpu, DTLB_REAL_MISS_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\nidx = srcu_read_lock(&vcpu->kvm->srcu);\r\ngpaddr = kvmppc_mmu_xlate(vcpu, gtlb_index, eaddr);\r\ngfn = gpaddr >> PAGE_SHIFT;\r\nif (kvm_is_visible_gfn(vcpu->kvm, gfn)) {\r\nkvmppc_mmu_map(vcpu, eaddr, gpaddr, gtlb_index);\r\nkvmppc_account_exit(vcpu, DTLB_VIRT_MISS_EXITS);\r\nr = RESUME_GUEST;\r\n} else {\r\nvcpu->arch.paddr_accessed = gpaddr;\r\nvcpu->arch.vaddr_accessed = eaddr;\r\nr = kvmppc_emulate_mmio(run, vcpu);\r\nkvmppc_account_exit(vcpu, MMIO_EXITS);\r\n}\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nbreak;\r\n}\r\ncase BOOKE_INTERRUPT_ITLB_MISS: {\r\nunsigned long eaddr = vcpu->arch.pc;\r\ngpa_t gpaddr;\r\ngfn_t gfn;\r\nint gtlb_index;\r\nr = RESUME_GUEST;\r\ngtlb_index = kvmppc_mmu_itlb_index(vcpu, eaddr);\r\nif (gtlb_index < 0) {\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_ITLB_MISS);\r\nkvmppc_mmu_itlb_miss(vcpu);\r\nkvmppc_account_exit(vcpu, ITLB_REAL_MISS_EXITS);\r\nbreak;\r\n}\r\nkvmppc_account_exit(vcpu, ITLB_VIRT_MISS_EXITS);\r\nidx = srcu_read_lock(&vcpu->kvm->srcu);\r\ngpaddr = kvmppc_mmu_xlate(vcpu, gtlb_index, eaddr);\r\ngfn = gpaddr >> PAGE_SHIFT;\r\nif (kvm_is_visible_gfn(vcpu->kvm, gfn)) {\r\nkvmppc_mmu_map(vcpu, eaddr, gpaddr, gtlb_index);\r\n} else {\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_MACHINE_CHECK);\r\n}\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nbreak;\r\n}\r\ncase BOOKE_INTERRUPT_DEBUG: {\r\nr = kvmppc_handle_debug(run, vcpu);\r\nif (r == RESUME_HOST)\r\nrun->exit_reason = KVM_EXIT_DEBUG;\r\nkvmppc_account_exit(vcpu, DEBUG_EXITS);\r\nbreak;\r\n}\r\ndefault:\r\nprintk(KERN_EMERG "exit_nr %d\n", exit_nr);\r\nBUG();\r\n}\r\nout:\r\nif (!(r & RESUME_HOST)) {\r\ns = kvmppc_prepare_to_enter(vcpu);\r\nif (s <= 0)\r\nr = (s << 2) | RESUME_HOST | (r & RESUME_FLAG_NV);\r\nelse {\r\nkvmppc_fix_ee_before_entry();\r\nkvmppc_load_guest_fp(vcpu);\r\nkvmppc_load_guest_altivec(vcpu);\r\n}\r\n}\r\nreturn r;\r\n}\r\nstatic void kvmppc_set_tsr(struct kvm_vcpu *vcpu, u32 new_tsr)\r\n{\r\nu32 old_tsr = vcpu->arch.tsr;\r\nvcpu->arch.tsr = new_tsr;\r\nif ((old_tsr ^ vcpu->arch.tsr) & (TSR_ENW | TSR_WIS))\r\narm_next_watchdog(vcpu);\r\nupdate_timer_ints(vcpu);\r\n}\r\nint kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nint r;\r\nvcpu->arch.pc = 0;\r\nvcpu->arch.shared->pir = vcpu->vcpu_id;\r\nkvmppc_set_gpr(vcpu, 1, (16<<20) - 8);\r\nkvmppc_set_msr(vcpu, 0);\r\n#ifndef CONFIG_KVM_BOOKE_HV\r\nvcpu->arch.shadow_msr = MSR_USER | MSR_IS | MSR_DS;\r\nvcpu->arch.shadow_pid = 1;\r\nvcpu->arch.shared->msr = 0;\r\n#endif\r\nvcpu->arch.ivpr = 0x55550000;\r\nfor (i = 0; i < BOOKE_IRQPRIO_MAX; i++)\r\nvcpu->arch.ivor[i] = 0x7700 | i * 4;\r\nkvmppc_init_timing_stats(vcpu);\r\nr = kvmppc_core_vcpu_setup(vcpu);\r\nkvmppc_sanity_check(vcpu);\r\nreturn r;\r\n}\r\nint kvmppc_subarch_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nspin_lock_init(&vcpu->arch.wdt_lock);\r\nsetup_timer(&vcpu->arch.wdt_timer, kvmppc_watchdog_func,\r\n(unsigned long)vcpu);\r\nmtspr(SPRN_DBSR, DBSR_MRR);\r\nreturn 0;\r\n}\r\nvoid kvmppc_subarch_vcpu_uninit(struct kvm_vcpu *vcpu)\r\n{\r\ndel_timer_sync(&vcpu->arch.wdt_timer);\r\n}\r\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nregs->pc = vcpu->arch.pc;\r\nregs->cr = kvmppc_get_cr(vcpu);\r\nregs->ctr = vcpu->arch.ctr;\r\nregs->lr = vcpu->arch.lr;\r\nregs->xer = kvmppc_get_xer(vcpu);\r\nregs->msr = vcpu->arch.shared->msr;\r\nregs->srr0 = kvmppc_get_srr0(vcpu);\r\nregs->srr1 = kvmppc_get_srr1(vcpu);\r\nregs->pid = vcpu->arch.pid;\r\nregs->sprg0 = kvmppc_get_sprg0(vcpu);\r\nregs->sprg1 = kvmppc_get_sprg1(vcpu);\r\nregs->sprg2 = kvmppc_get_sprg2(vcpu);\r\nregs->sprg3 = kvmppc_get_sprg3(vcpu);\r\nregs->sprg4 = kvmppc_get_sprg4(vcpu);\r\nregs->sprg5 = kvmppc_get_sprg5(vcpu);\r\nregs->sprg6 = kvmppc_get_sprg6(vcpu);\r\nregs->sprg7 = kvmppc_get_sprg7(vcpu);\r\nfor (i = 0; i < ARRAY_SIZE(regs->gpr); i++)\r\nregs->gpr[i] = kvmppc_get_gpr(vcpu, i);\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nvcpu->arch.pc = regs->pc;\r\nkvmppc_set_cr(vcpu, regs->cr);\r\nvcpu->arch.ctr = regs->ctr;\r\nvcpu->arch.lr = regs->lr;\r\nkvmppc_set_xer(vcpu, regs->xer);\r\nkvmppc_set_msr(vcpu, regs->msr);\r\nkvmppc_set_srr0(vcpu, regs->srr0);\r\nkvmppc_set_srr1(vcpu, regs->srr1);\r\nkvmppc_set_pid(vcpu, regs->pid);\r\nkvmppc_set_sprg0(vcpu, regs->sprg0);\r\nkvmppc_set_sprg1(vcpu, regs->sprg1);\r\nkvmppc_set_sprg2(vcpu, regs->sprg2);\r\nkvmppc_set_sprg3(vcpu, regs->sprg3);\r\nkvmppc_set_sprg4(vcpu, regs->sprg4);\r\nkvmppc_set_sprg5(vcpu, regs->sprg5);\r\nkvmppc_set_sprg6(vcpu, regs->sprg6);\r\nkvmppc_set_sprg7(vcpu, regs->sprg7);\r\nfor (i = 0; i < ARRAY_SIZE(regs->gpr); i++)\r\nkvmppc_set_gpr(vcpu, i, regs->gpr[i]);\r\nreturn 0;\r\n}\r\nstatic void get_sregs_base(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nu64 tb = get_tb();\r\nsregs->u.e.features |= KVM_SREGS_E_BASE;\r\nsregs->u.e.csrr0 = vcpu->arch.csrr0;\r\nsregs->u.e.csrr1 = vcpu->arch.csrr1;\r\nsregs->u.e.mcsr = vcpu->arch.mcsr;\r\nsregs->u.e.esr = kvmppc_get_esr(vcpu);\r\nsregs->u.e.dear = kvmppc_get_dar(vcpu);\r\nsregs->u.e.tsr = vcpu->arch.tsr;\r\nsregs->u.e.tcr = vcpu->arch.tcr;\r\nsregs->u.e.dec = kvmppc_get_dec(vcpu, tb);\r\nsregs->u.e.tb = tb;\r\nsregs->u.e.vrsave = vcpu->arch.vrsave;\r\n}\r\nstatic int set_sregs_base(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nif (!(sregs->u.e.features & KVM_SREGS_E_BASE))\r\nreturn 0;\r\nvcpu->arch.csrr0 = sregs->u.e.csrr0;\r\nvcpu->arch.csrr1 = sregs->u.e.csrr1;\r\nvcpu->arch.mcsr = sregs->u.e.mcsr;\r\nkvmppc_set_esr(vcpu, sregs->u.e.esr);\r\nkvmppc_set_dar(vcpu, sregs->u.e.dear);\r\nvcpu->arch.vrsave = sregs->u.e.vrsave;\r\nkvmppc_set_tcr(vcpu, sregs->u.e.tcr);\r\nif (sregs->u.e.update_special & KVM_SREGS_E_UPDATE_DEC) {\r\nvcpu->arch.dec = sregs->u.e.dec;\r\nkvmppc_emulate_dec(vcpu);\r\n}\r\nif (sregs->u.e.update_special & KVM_SREGS_E_UPDATE_TSR)\r\nkvmppc_set_tsr(vcpu, sregs->u.e.tsr);\r\nreturn 0;\r\n}\r\nstatic void get_sregs_arch206(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nsregs->u.e.features |= KVM_SREGS_E_ARCH206;\r\nsregs->u.e.pir = vcpu->vcpu_id;\r\nsregs->u.e.mcsrr0 = vcpu->arch.mcsrr0;\r\nsregs->u.e.mcsrr1 = vcpu->arch.mcsrr1;\r\nsregs->u.e.decar = vcpu->arch.decar;\r\nsregs->u.e.ivpr = vcpu->arch.ivpr;\r\n}\r\nstatic int set_sregs_arch206(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nif (!(sregs->u.e.features & KVM_SREGS_E_ARCH206))\r\nreturn 0;\r\nif (sregs->u.e.pir != vcpu->vcpu_id)\r\nreturn -EINVAL;\r\nvcpu->arch.mcsrr0 = sregs->u.e.mcsrr0;\r\nvcpu->arch.mcsrr1 = sregs->u.e.mcsrr1;\r\nvcpu->arch.decar = sregs->u.e.decar;\r\nvcpu->arch.ivpr = sregs->u.e.ivpr;\r\nreturn 0;\r\n}\r\nint kvmppc_get_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nsregs->u.e.features |= KVM_SREGS_E_IVOR;\r\nsregs->u.e.ivor_low[0] = vcpu->arch.ivor[BOOKE_IRQPRIO_CRITICAL];\r\nsregs->u.e.ivor_low[1] = vcpu->arch.ivor[BOOKE_IRQPRIO_MACHINE_CHECK];\r\nsregs->u.e.ivor_low[2] = vcpu->arch.ivor[BOOKE_IRQPRIO_DATA_STORAGE];\r\nsregs->u.e.ivor_low[3] = vcpu->arch.ivor[BOOKE_IRQPRIO_INST_STORAGE];\r\nsregs->u.e.ivor_low[4] = vcpu->arch.ivor[BOOKE_IRQPRIO_EXTERNAL];\r\nsregs->u.e.ivor_low[5] = vcpu->arch.ivor[BOOKE_IRQPRIO_ALIGNMENT];\r\nsregs->u.e.ivor_low[6] = vcpu->arch.ivor[BOOKE_IRQPRIO_PROGRAM];\r\nsregs->u.e.ivor_low[7] = vcpu->arch.ivor[BOOKE_IRQPRIO_FP_UNAVAIL];\r\nsregs->u.e.ivor_low[8] = vcpu->arch.ivor[BOOKE_IRQPRIO_SYSCALL];\r\nsregs->u.e.ivor_low[9] = vcpu->arch.ivor[BOOKE_IRQPRIO_AP_UNAVAIL];\r\nsregs->u.e.ivor_low[10] = vcpu->arch.ivor[BOOKE_IRQPRIO_DECREMENTER];\r\nsregs->u.e.ivor_low[11] = vcpu->arch.ivor[BOOKE_IRQPRIO_FIT];\r\nsregs->u.e.ivor_low[12] = vcpu->arch.ivor[BOOKE_IRQPRIO_WATCHDOG];\r\nsregs->u.e.ivor_low[13] = vcpu->arch.ivor[BOOKE_IRQPRIO_DTLB_MISS];\r\nsregs->u.e.ivor_low[14] = vcpu->arch.ivor[BOOKE_IRQPRIO_ITLB_MISS];\r\nsregs->u.e.ivor_low[15] = vcpu->arch.ivor[BOOKE_IRQPRIO_DEBUG];\r\nreturn 0;\r\n}\r\nint kvmppc_set_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nif (!(sregs->u.e.features & KVM_SREGS_E_IVOR))\r\nreturn 0;\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_CRITICAL] = sregs->u.e.ivor_low[0];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_MACHINE_CHECK] = sregs->u.e.ivor_low[1];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DATA_STORAGE] = sregs->u.e.ivor_low[2];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_INST_STORAGE] = sregs->u.e.ivor_low[3];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_EXTERNAL] = sregs->u.e.ivor_low[4];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_ALIGNMENT] = sregs->u.e.ivor_low[5];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_PROGRAM] = sregs->u.e.ivor_low[6];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_FP_UNAVAIL] = sregs->u.e.ivor_low[7];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_SYSCALL] = sregs->u.e.ivor_low[8];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_AP_UNAVAIL] = sregs->u.e.ivor_low[9];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DECREMENTER] = sregs->u.e.ivor_low[10];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_FIT] = sregs->u.e.ivor_low[11];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_WATCHDOG] = sregs->u.e.ivor_low[12];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DTLB_MISS] = sregs->u.e.ivor_low[13];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_ITLB_MISS] = sregs->u.e.ivor_low[14];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DEBUG] = sregs->u.e.ivor_low[15];\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nsregs->pvr = vcpu->arch.pvr;\r\nget_sregs_base(vcpu, sregs);\r\nget_sregs_arch206(vcpu, sregs);\r\nreturn vcpu->kvm->arch.kvm_ops->get_sregs(vcpu, sregs);\r\n}\r\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nint ret;\r\nif (vcpu->arch.pvr != sregs->pvr)\r\nreturn -EINVAL;\r\nret = set_sregs_base(vcpu, sregs);\r\nif (ret < 0)\r\nreturn ret;\r\nret = set_sregs_arch206(vcpu, sregs);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn vcpu->kvm->arch.kvm_ops->set_sregs(vcpu, sregs);\r\n}\r\nint kvmppc_get_one_reg(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nswitch (id) {\r\ncase KVM_REG_PPC_IAC1:\r\n*val = get_reg_val(id, vcpu->arch.dbg_reg.iac1);\r\nbreak;\r\ncase KVM_REG_PPC_IAC2:\r\n*val = get_reg_val(id, vcpu->arch.dbg_reg.iac2);\r\nbreak;\r\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\r\ncase KVM_REG_PPC_IAC3:\r\n*val = get_reg_val(id, vcpu->arch.dbg_reg.iac3);\r\nbreak;\r\ncase KVM_REG_PPC_IAC4:\r\n*val = get_reg_val(id, vcpu->arch.dbg_reg.iac4);\r\nbreak;\r\n#endif\r\ncase KVM_REG_PPC_DAC1:\r\n*val = get_reg_val(id, vcpu->arch.dbg_reg.dac1);\r\nbreak;\r\ncase KVM_REG_PPC_DAC2:\r\n*val = get_reg_val(id, vcpu->arch.dbg_reg.dac2);\r\nbreak;\r\ncase KVM_REG_PPC_EPR: {\r\nu32 epr = kvmppc_get_epr(vcpu);\r\n*val = get_reg_val(id, epr);\r\nbreak;\r\n}\r\n#if defined(CONFIG_64BIT)\r\ncase KVM_REG_PPC_EPCR:\r\n*val = get_reg_val(id, vcpu->arch.epcr);\r\nbreak;\r\n#endif\r\ncase KVM_REG_PPC_TCR:\r\n*val = get_reg_val(id, vcpu->arch.tcr);\r\nbreak;\r\ncase KVM_REG_PPC_TSR:\r\n*val = get_reg_val(id, vcpu->arch.tsr);\r\nbreak;\r\ncase KVM_REG_PPC_DEBUG_INST:\r\n*val = get_reg_val(id, KVMPPC_INST_SW_BREAKPOINT);\r\nbreak;\r\ncase KVM_REG_PPC_VRSAVE:\r\n*val = get_reg_val(id, vcpu->arch.vrsave);\r\nbreak;\r\ndefault:\r\nr = vcpu->kvm->arch.kvm_ops->get_one_reg(vcpu, id, val);\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nint kvmppc_set_one_reg(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nswitch (id) {\r\ncase KVM_REG_PPC_IAC1:\r\nvcpu->arch.dbg_reg.iac1 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_IAC2:\r\nvcpu->arch.dbg_reg.iac2 = set_reg_val(id, *val);\r\nbreak;\r\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\r\ncase KVM_REG_PPC_IAC3:\r\nvcpu->arch.dbg_reg.iac3 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_IAC4:\r\nvcpu->arch.dbg_reg.iac4 = set_reg_val(id, *val);\r\nbreak;\r\n#endif\r\ncase KVM_REG_PPC_DAC1:\r\nvcpu->arch.dbg_reg.dac1 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_DAC2:\r\nvcpu->arch.dbg_reg.dac2 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_EPR: {\r\nu32 new_epr = set_reg_val(id, *val);\r\nkvmppc_set_epr(vcpu, new_epr);\r\nbreak;\r\n}\r\n#if defined(CONFIG_64BIT)\r\ncase KVM_REG_PPC_EPCR: {\r\nu32 new_epcr = set_reg_val(id, *val);\r\nkvmppc_set_epcr(vcpu, new_epcr);\r\nbreak;\r\n}\r\n#endif\r\ncase KVM_REG_PPC_OR_TSR: {\r\nu32 tsr_bits = set_reg_val(id, *val);\r\nkvmppc_set_tsr_bits(vcpu, tsr_bits);\r\nbreak;\r\n}\r\ncase KVM_REG_PPC_CLEAR_TSR: {\r\nu32 tsr_bits = set_reg_val(id, *val);\r\nkvmppc_clr_tsr_bits(vcpu, tsr_bits);\r\nbreak;\r\n}\r\ncase KVM_REG_PPC_TSR: {\r\nu32 tsr = set_reg_val(id, *val);\r\nkvmppc_set_tsr(vcpu, tsr);\r\nbreak;\r\n}\r\ncase KVM_REG_PPC_TCR: {\r\nu32 tcr = set_reg_val(id, *val);\r\nkvmppc_set_tcr(vcpu, tcr);\r\nbreak;\r\n}\r\ncase KVM_REG_PPC_VRSAVE:\r\nvcpu->arch.vrsave = set_reg_val(id, *val);\r\nbreak;\r\ndefault:\r\nr = vcpu->kvm->arch.kvm_ops->set_one_reg(vcpu, id, val);\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\r\nstruct kvm_translation *tr)\r\n{\r\nint r;\r\nr = kvmppc_core_vcpu_translate(vcpu, tr);\r\nreturn r;\r\n}\r\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nvoid kvmppc_core_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,\r\nstruct kvm_memory_slot *dont)\r\n{\r\n}\r\nint kvmppc_core_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,\r\nunsigned long npages)\r\n{\r\nreturn 0;\r\n}\r\nint kvmppc_core_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot,\r\nconst struct kvm_userspace_memory_region *mem)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_commit_memory_region(struct kvm *kvm,\r\nconst struct kvm_userspace_memory_region *mem,\r\nconst struct kvm_memory_slot *old,\r\nconst struct kvm_memory_slot *new)\r\n{\r\n}\r\nvoid kvmppc_core_flush_memslot(struct kvm *kvm, struct kvm_memory_slot *memslot)\r\n{\r\n}\r\nvoid kvmppc_set_epcr(struct kvm_vcpu *vcpu, u32 new_epcr)\r\n{\r\n#if defined(CONFIG_64BIT)\r\nvcpu->arch.epcr = new_epcr;\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\nvcpu->arch.shadow_epcr &= ~SPRN_EPCR_GICM;\r\nif (vcpu->arch.epcr & SPRN_EPCR_ICM)\r\nvcpu->arch.shadow_epcr |= SPRN_EPCR_GICM;\r\n#endif\r\n#endif\r\n}\r\nvoid kvmppc_set_tcr(struct kvm_vcpu *vcpu, u32 new_tcr)\r\n{\r\nvcpu->arch.tcr = new_tcr;\r\narm_next_watchdog(vcpu);\r\nupdate_timer_ints(vcpu);\r\n}\r\nvoid kvmppc_set_tsr_bits(struct kvm_vcpu *vcpu, u32 tsr_bits)\r\n{\r\nset_bits(tsr_bits, &vcpu->arch.tsr);\r\nsmp_wmb();\r\nkvm_make_request(KVM_REQ_PENDING_TIMER, vcpu);\r\nkvm_vcpu_kick(vcpu);\r\n}\r\nvoid kvmppc_clr_tsr_bits(struct kvm_vcpu *vcpu, u32 tsr_bits)\r\n{\r\nclear_bits(tsr_bits, &vcpu->arch.tsr);\r\nif (tsr_bits & (TSR_ENW | TSR_WIS))\r\narm_next_watchdog(vcpu);\r\nupdate_timer_ints(vcpu);\r\n}\r\nvoid kvmppc_decrementer_func(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.tcr & TCR_ARE) {\r\nvcpu->arch.dec = vcpu->arch.decar;\r\nkvmppc_emulate_dec(vcpu);\r\n}\r\nkvmppc_set_tsr_bits(vcpu, TSR_DIS);\r\n}\r\nstatic int kvmppc_booke_add_breakpoint(struct debug_reg *dbg_reg,\r\nuint64_t addr, int index)\r\n{\r\nswitch (index) {\r\ncase 0:\r\ndbg_reg->dbcr0 |= DBCR0_IAC1;\r\ndbg_reg->iac1 = addr;\r\nbreak;\r\ncase 1:\r\ndbg_reg->dbcr0 |= DBCR0_IAC2;\r\ndbg_reg->iac2 = addr;\r\nbreak;\r\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\r\ncase 2:\r\ndbg_reg->dbcr0 |= DBCR0_IAC3;\r\ndbg_reg->iac3 = addr;\r\nbreak;\r\ncase 3:\r\ndbg_reg->dbcr0 |= DBCR0_IAC4;\r\ndbg_reg->iac4 = addr;\r\nbreak;\r\n#endif\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ndbg_reg->dbcr0 |= DBCR0_IDM;\r\nreturn 0;\r\n}\r\nstatic int kvmppc_booke_add_watchpoint(struct debug_reg *dbg_reg, uint64_t addr,\r\nint type, int index)\r\n{\r\nswitch (index) {\r\ncase 0:\r\nif (type & KVMPPC_DEBUG_WATCH_READ)\r\ndbg_reg->dbcr0 |= DBCR0_DAC1R;\r\nif (type & KVMPPC_DEBUG_WATCH_WRITE)\r\ndbg_reg->dbcr0 |= DBCR0_DAC1W;\r\ndbg_reg->dac1 = addr;\r\nbreak;\r\ncase 1:\r\nif (type & KVMPPC_DEBUG_WATCH_READ)\r\ndbg_reg->dbcr0 |= DBCR0_DAC2R;\r\nif (type & KVMPPC_DEBUG_WATCH_WRITE)\r\ndbg_reg->dbcr0 |= DBCR0_DAC2W;\r\ndbg_reg->dac2 = addr;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ndbg_reg->dbcr0 |= DBCR0_IDM;\r\nreturn 0;\r\n}\r\nvoid kvm_guest_protect_msr(struct kvm_vcpu *vcpu, ulong prot_bitmap, bool set)\r\n{\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\nBUG_ON(prot_bitmap & ~(MSRP_UCLEP | MSRP_DEP | MSRP_PMMP));\r\nif (set) {\r\nif (prot_bitmap & MSR_UCLE)\r\nvcpu->arch.shadow_msrp |= MSRP_UCLEP;\r\nif (prot_bitmap & MSR_DE)\r\nvcpu->arch.shadow_msrp |= MSRP_DEP;\r\nif (prot_bitmap & MSR_PMM)\r\nvcpu->arch.shadow_msrp |= MSRP_PMMP;\r\n} else {\r\nif (prot_bitmap & MSR_UCLE)\r\nvcpu->arch.shadow_msrp &= ~MSRP_UCLEP;\r\nif (prot_bitmap & MSR_DE)\r\nvcpu->arch.shadow_msrp &= ~MSRP_DEP;\r\nif (prot_bitmap & MSR_PMM)\r\nvcpu->arch.shadow_msrp &= ~MSRP_PMMP;\r\n}\r\n#endif\r\n}\r\nint kvmppc_xlate(struct kvm_vcpu *vcpu, ulong eaddr, enum xlate_instdata xlid,\r\nenum xlate_readwrite xlrw, struct kvmppc_pte *pte)\r\n{\r\nint gtlb_index;\r\ngpa_t gpaddr;\r\n#ifdef CONFIG_KVM_E500V2\r\nif (!(vcpu->arch.shared->msr & MSR_PR) &&\r\n(eaddr & PAGE_MASK) == vcpu->arch.magic_page_ea) {\r\npte->eaddr = eaddr;\r\npte->raddr = (vcpu->arch.magic_page_pa & PAGE_MASK) |\r\n(eaddr & ~PAGE_MASK);\r\npte->vpage = eaddr >> PAGE_SHIFT;\r\npte->may_read = true;\r\npte->may_write = true;\r\npte->may_execute = true;\r\nreturn 0;\r\n}\r\n#endif\r\nswitch (xlid) {\r\ncase XLATE_INST:\r\ngtlb_index = kvmppc_mmu_itlb_index(vcpu, eaddr);\r\nbreak;\r\ncase XLATE_DATA:\r\ngtlb_index = kvmppc_mmu_dtlb_index(vcpu, eaddr);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nif (gtlb_index < 0)\r\nreturn -ENOENT;\r\ngpaddr = kvmppc_mmu_xlate(vcpu, gtlb_index, eaddr);\r\npte->eaddr = eaddr;\r\npte->raddr = (gpaddr & PAGE_MASK) | (eaddr & ~PAGE_MASK);\r\npte->vpage = eaddr >> PAGE_SHIFT;\r\npte->may_read = true;\r\npte->may_write = true;\r\npte->may_execute = true;\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\r\nstruct kvm_guest_debug *dbg)\r\n{\r\nstruct debug_reg *dbg_reg;\r\nint n, b = 0, w = 0;\r\nif (!(dbg->control & KVM_GUESTDBG_ENABLE)) {\r\nvcpu->arch.dbg_reg.dbcr0 = 0;\r\nvcpu->guest_debug = 0;\r\nkvm_guest_protect_msr(vcpu, MSR_DE, false);\r\nreturn 0;\r\n}\r\nkvm_guest_protect_msr(vcpu, MSR_DE, true);\r\nvcpu->guest_debug = dbg->control;\r\nvcpu->arch.dbg_reg.dbcr0 = 0;\r\nif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\r\nvcpu->arch.dbg_reg.dbcr0 |= DBCR0_IDM | DBCR0_IC;\r\ndbg_reg = &(vcpu->arch.dbg_reg);\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\ndbg_reg->dbcr1 = 0;\r\ndbg_reg->dbcr2 = 0;\r\n#else\r\ndbg_reg->dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US | DBCR1_IAC3US |\r\nDBCR1_IAC4US;\r\ndbg_reg->dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;\r\n#endif\r\nif (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP))\r\nreturn 0;\r\nfor (n = 0; n < (KVMPPC_BOOKE_IAC_NUM + KVMPPC_BOOKE_DAC_NUM); n++) {\r\nuint64_t addr = dbg->arch.bp[n].addr;\r\nuint32_t type = dbg->arch.bp[n].type;\r\nif (type == KVMPPC_DEBUG_NONE)\r\ncontinue;\r\nif (type & ~(KVMPPC_DEBUG_WATCH_READ |\r\nKVMPPC_DEBUG_WATCH_WRITE |\r\nKVMPPC_DEBUG_BREAKPOINT))\r\nreturn -EINVAL;\r\nif (type & KVMPPC_DEBUG_BREAKPOINT) {\r\nif (kvmppc_booke_add_breakpoint(dbg_reg, addr, b++))\r\nreturn -EINVAL;\r\n} else {\r\nif (kvmppc_booke_add_watchpoint(dbg_reg, addr,\r\ntype, w++))\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid kvmppc_booke_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nvcpu->cpu = smp_processor_id();\r\ncurrent->thread.kvm_vcpu = vcpu;\r\n}\r\nvoid kvmppc_booke_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\ncurrent->thread.kvm_vcpu = NULL;\r\nvcpu->cpu = -1;\r\nkvmppc_clear_dbsr();\r\n}\r\nvoid kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->mmu_destroy(vcpu);\r\n}\r\nint kvmppc_core_init_vm(struct kvm *kvm)\r\n{\r\nreturn kvm->arch.kvm_ops->init_vm(kvm);\r\n}\r\nstruct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm, unsigned int id)\r\n{\r\nreturn kvm->arch.kvm_ops->vcpu_create(kvm, id);\r\n}\r\nvoid kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->vcpu_free(vcpu);\r\n}\r\nvoid kvmppc_core_destroy_vm(struct kvm *kvm)\r\n{\r\nkvm->arch.kvm_ops->destroy_vm(kvm);\r\n}\r\nvoid kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->vcpu_load(vcpu, cpu);\r\n}\r\nvoid kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->vcpu_put(vcpu);\r\n}\r\nint __init kvmppc_booke_init(void)\r\n{\r\n#ifndef CONFIG_KVM_BOOKE_HV\r\nunsigned long ivor[16];\r\nunsigned long *handler = kvmppc_booke_handler_addr;\r\nunsigned long max_ivor = 0;\r\nunsigned long handler_len;\r\nint i;\r\nkvmppc_booke_handlers = __get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nVCPU_SIZE_ORDER);\r\nif (!kvmppc_booke_handlers)\r\nreturn -ENOMEM;\r\nivor[0] = mfspr(SPRN_IVOR0);\r\nivor[1] = mfspr(SPRN_IVOR1);\r\nivor[2] = mfspr(SPRN_IVOR2);\r\nivor[3] = mfspr(SPRN_IVOR3);\r\nivor[4] = mfspr(SPRN_IVOR4);\r\nivor[5] = mfspr(SPRN_IVOR5);\r\nivor[6] = mfspr(SPRN_IVOR6);\r\nivor[7] = mfspr(SPRN_IVOR7);\r\nivor[8] = mfspr(SPRN_IVOR8);\r\nivor[9] = mfspr(SPRN_IVOR9);\r\nivor[10] = mfspr(SPRN_IVOR10);\r\nivor[11] = mfspr(SPRN_IVOR11);\r\nivor[12] = mfspr(SPRN_IVOR12);\r\nivor[13] = mfspr(SPRN_IVOR13);\r\nivor[14] = mfspr(SPRN_IVOR14);\r\nivor[15] = mfspr(SPRN_IVOR15);\r\nfor (i = 0; i < 16; i++) {\r\nif (ivor[i] > max_ivor)\r\nmax_ivor = i;\r\nhandler_len = handler[i + 1] - handler[i];\r\nmemcpy((void *)kvmppc_booke_handlers + ivor[i],\r\n(void *)handler[i], handler_len);\r\n}\r\nhandler_len = handler[max_ivor + 1] - handler[max_ivor];\r\nflush_icache_range(kvmppc_booke_handlers, kvmppc_booke_handlers +\r\nivor[max_ivor] + handler_len);\r\n#endif\r\nreturn 0;\r\n}\r\nvoid __exit kvmppc_booke_exit(void)\r\n{\r\nfree_pages(kvmppc_booke_handlers, VCPU_SIZE_ORDER);\r\nkvm_exit();\r\n}
