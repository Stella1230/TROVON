void ldlm_handle_bl_callback(struct ldlm_namespace *ns,\r\nstruct ldlm_lock_desc *ld, struct ldlm_lock *lock)\r\n{\r\nint do_ast;\r\nLDLM_DEBUG(lock, "client blocking AST callback handler");\r\nlock_res_and_lock(lock);\r\nldlm_set_cbpending(lock);\r\nif (ldlm_is_cancel_on_block(lock))\r\nldlm_set_cancel(lock);\r\ndo_ast = !lock->l_readers && !lock->l_writers;\r\nunlock_res_and_lock(lock);\r\nif (do_ast) {\r\nCDEBUG(D_DLMTRACE,\r\n"Lock %p already unused, calling callback (%p)\n", lock,\r\nlock->l_blocking_ast);\r\nif (lock->l_blocking_ast)\r\nlock->l_blocking_ast(lock, ld, lock->l_ast_data,\r\nLDLM_CB_BLOCKING);\r\n} else {\r\nCDEBUG(D_DLMTRACE,\r\n"Lock %p is referenced, will be cancelled later\n",\r\nlock);\r\n}\r\nLDLM_DEBUG(lock, "client blocking callback handler END");\r\nLDLM_LOCK_RELEASE(lock);\r\n}\r\nstatic void ldlm_handle_cp_callback(struct ptlrpc_request *req,\r\nstruct ldlm_namespace *ns,\r\nstruct ldlm_request *dlm_req,\r\nstruct ldlm_lock *lock)\r\n{\r\nint lvb_len;\r\nLIST_HEAD(ast_list);\r\nint rc = 0;\r\nLDLM_DEBUG(lock, "client completion callback handler START");\r\nif (OBD_FAIL_CHECK(OBD_FAIL_LDLM_CANCEL_BL_CB_RACE)) {\r\nint to = cfs_time_seconds(1);\r\nwhile (to > 0) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nschedule_timeout(to);\r\nif (lock->l_granted_mode == lock->l_req_mode ||\r\nldlm_is_destroyed(lock))\r\nbreak;\r\n}\r\n}\r\nlvb_len = req_capsule_get_size(&req->rq_pill, &RMF_DLM_LVB, RCL_CLIENT);\r\nif (lvb_len < 0) {\r\nLDLM_ERROR(lock, "Fail to get lvb_len, rc = %d", lvb_len);\r\nrc = lvb_len;\r\ngoto out;\r\n} else if (lvb_len > 0) {\r\nif (lock->l_lvb_len > 0) {\r\nLASSERT(lock->l_lvb_data);\r\nif (unlikely(lock->l_lvb_len < lvb_len)) {\r\nLDLM_ERROR(lock, "Replied LVB is larger than expectation, expected = %d, replied = %d",\r\nlock->l_lvb_len, lvb_len);\r\nrc = -EINVAL;\r\ngoto out;\r\n}\r\n} else if (ldlm_has_layout(lock)) {\r\nvoid *lvb_data;\r\nlvb_data = kzalloc(lvb_len, GFP_NOFS);\r\nif (!lvb_data) {\r\nLDLM_ERROR(lock, "No memory: %d.\n", lvb_len);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nlock_res_and_lock(lock);\r\nLASSERT(!lock->l_lvb_data);\r\nlock->l_lvb_type = LVB_T_LAYOUT;\r\nlock->l_lvb_data = lvb_data;\r\nlock->l_lvb_len = lvb_len;\r\nunlock_res_and_lock(lock);\r\n}\r\n}\r\nlock_res_and_lock(lock);\r\nif (ldlm_is_destroyed(lock) ||\r\nlock->l_granted_mode == lock->l_req_mode) {\r\nunlock_res_and_lock(lock);\r\nLDLM_DEBUG(lock, "Double grant race happened");\r\nrc = 0;\r\ngoto out;\r\n}\r\nif (dlm_req->lock_desc.l_granted_mode != lock->l_req_mode) {\r\nlock->l_req_mode = dlm_req->lock_desc.l_granted_mode;\r\nLDLM_DEBUG(lock, "completion AST, new lock mode");\r\n}\r\nif (lock->l_resource->lr_type != LDLM_PLAIN) {\r\nldlm_convert_policy_to_local(req->rq_export,\r\ndlm_req->lock_desc.l_resource.lr_type,\r\n&dlm_req->lock_desc.l_policy_data,\r\n&lock->l_policy_data);\r\nLDLM_DEBUG(lock, "completion AST, new policy data");\r\n}\r\nldlm_resource_unlink_lock(lock);\r\nif (memcmp(&dlm_req->lock_desc.l_resource.lr_name,\r\n&lock->l_resource->lr_name,\r\nsizeof(lock->l_resource->lr_name)) != 0) {\r\nunlock_res_and_lock(lock);\r\nrc = ldlm_lock_change_resource(ns, lock,\r\n&dlm_req->lock_desc.l_resource.lr_name);\r\nif (rc < 0) {\r\nLDLM_ERROR(lock, "Failed to allocate resource");\r\ngoto out;\r\n}\r\nLDLM_DEBUG(lock, "completion AST, new resource");\r\nCERROR("change resource!\n");\r\nlock_res_and_lock(lock);\r\n}\r\nif (dlm_req->lock_flags & LDLM_FL_AST_SENT) {\r\nldlm_lock_remove_from_lru(lock);\r\nlock->l_flags |= LDLM_FL_CBPENDING | LDLM_FL_BL_AST;\r\nLDLM_DEBUG(lock, "completion AST includes blocking AST");\r\n}\r\nif (lock->l_lvb_len > 0) {\r\nrc = ldlm_fill_lvb(lock, &req->rq_pill, RCL_CLIENT,\r\nlock->l_lvb_data, lvb_len);\r\nif (rc < 0) {\r\nunlock_res_and_lock(lock);\r\ngoto out;\r\n}\r\n}\r\nldlm_grant_lock(lock, &ast_list);\r\nunlock_res_and_lock(lock);\r\nLDLM_DEBUG(lock, "callback handler finished, about to run_ast_work");\r\nOBD_FAIL_TIMEOUT(OBD_FAIL_OSC_CP_ENQ_RACE, 2);\r\nldlm_run_ast_work(ns, &ast_list, LDLM_WORK_CP_AST);\r\nLDLM_DEBUG_NOLOCK("client completion callback handler END (lock %p)",\r\nlock);\r\ngoto out;\r\nout:\r\nif (rc < 0) {\r\nlock_res_and_lock(lock);\r\nldlm_set_failed(lock);\r\nunlock_res_and_lock(lock);\r\nwake_up(&lock->l_waitq);\r\n}\r\nLDLM_LOCK_RELEASE(lock);\r\n}\r\nstatic void ldlm_handle_gl_callback(struct ptlrpc_request *req,\r\nstruct ldlm_namespace *ns,\r\nstruct ldlm_request *dlm_req,\r\nstruct ldlm_lock *lock)\r\n{\r\nint rc = -ENOSYS;\r\nLDLM_DEBUG(lock, "client glimpse AST callback handler");\r\nif (lock->l_glimpse_ast)\r\nrc = lock->l_glimpse_ast(lock, req);\r\nif (req->rq_repmsg) {\r\nptlrpc_reply(req);\r\n} else {\r\nreq->rq_status = rc;\r\nptlrpc_error(req);\r\n}\r\nlock_res_and_lock(lock);\r\nif (lock->l_granted_mode == LCK_PW &&\r\n!lock->l_readers && !lock->l_writers &&\r\ncfs_time_after(cfs_time_current(),\r\ncfs_time_add(lock->l_last_used,\r\ncfs_time_seconds(10)))) {\r\nunlock_res_and_lock(lock);\r\nif (ldlm_bl_to_thread_lock(ns, NULL, lock))\r\nldlm_handle_bl_callback(ns, NULL, lock);\r\nreturn;\r\n}\r\nunlock_res_and_lock(lock);\r\nLDLM_LOCK_RELEASE(lock);\r\n}\r\nstatic int ldlm_callback_reply(struct ptlrpc_request *req, int rc)\r\n{\r\nif (req->rq_no_reply)\r\nreturn 0;\r\nreq->rq_status = rc;\r\nif (!req->rq_packed_final) {\r\nrc = lustre_pack_reply(req, 1, NULL, NULL);\r\nif (rc)\r\nreturn rc;\r\n}\r\nreturn ptlrpc_reply(req);\r\n}\r\nstatic int __ldlm_bl_to_thread(struct ldlm_bl_work_item *blwi,\r\nenum ldlm_cancel_flags cancel_flags)\r\n{\r\nstruct ldlm_bl_pool *blp = ldlm_state->ldlm_bl_pool;\r\nspin_lock(&blp->blp_lock);\r\nif (blwi->blwi_lock && ldlm_is_discard_data(blwi->blwi_lock)) {\r\nlist_add_tail(&blwi->blwi_entry, &blp->blp_prio_list);\r\n} else {\r\nlist_add_tail(&blwi->blwi_entry, &blp->blp_list);\r\n}\r\nspin_unlock(&blp->blp_lock);\r\nwake_up(&blp->blp_waitq);\r\nif (!(cancel_flags & LCF_ASYNC))\r\nwait_for_completion(&blwi->blwi_comp);\r\nreturn 0;\r\n}\r\nstatic inline void init_blwi(struct ldlm_bl_work_item *blwi,\r\nstruct ldlm_namespace *ns,\r\nstruct ldlm_lock_desc *ld,\r\nstruct list_head *cancels, int count,\r\nstruct ldlm_lock *lock,\r\nenum ldlm_cancel_flags cancel_flags)\r\n{\r\ninit_completion(&blwi->blwi_comp);\r\nINIT_LIST_HEAD(&blwi->blwi_head);\r\nif (memory_pressure_get())\r\nblwi->blwi_mem_pressure = 1;\r\nblwi->blwi_ns = ns;\r\nblwi->blwi_flags = cancel_flags;\r\nif (ld)\r\nblwi->blwi_ld = *ld;\r\nif (count) {\r\nlist_add(&blwi->blwi_head, cancels);\r\nlist_del_init(cancels);\r\nblwi->blwi_count = count;\r\n} else {\r\nblwi->blwi_lock = lock;\r\n}\r\n}\r\nstatic int ldlm_bl_to_thread(struct ldlm_namespace *ns,\r\nstruct ldlm_lock_desc *ld,\r\nstruct ldlm_lock *lock,\r\nstruct list_head *cancels, int count,\r\nenum ldlm_cancel_flags cancel_flags)\r\n{\r\nif (cancels && count == 0)\r\nreturn 0;\r\nif (cancel_flags & LCF_ASYNC) {\r\nstruct ldlm_bl_work_item *blwi;\r\nblwi = kzalloc(sizeof(*blwi), GFP_NOFS);\r\nif (!blwi)\r\nreturn -ENOMEM;\r\ninit_blwi(blwi, ns, ld, cancels, count, lock, cancel_flags);\r\nreturn __ldlm_bl_to_thread(blwi, cancel_flags);\r\n} else {\r\nstruct ldlm_bl_work_item blwi;\r\nmemset(&blwi, 0, sizeof(blwi));\r\ninit_blwi(&blwi, ns, ld, cancels, count, lock, cancel_flags);\r\nreturn __ldlm_bl_to_thread(&blwi, cancel_flags);\r\n}\r\n}\r\nint ldlm_bl_to_thread_lock(struct ldlm_namespace *ns, struct ldlm_lock_desc *ld,\r\nstruct ldlm_lock *lock)\r\n{\r\nreturn ldlm_bl_to_thread(ns, ld, lock, NULL, 0, LCF_ASYNC);\r\n}\r\nint ldlm_bl_to_thread_list(struct ldlm_namespace *ns, struct ldlm_lock_desc *ld,\r\nstruct list_head *cancels, int count,\r\nenum ldlm_cancel_flags cancel_flags)\r\n{\r\nreturn ldlm_bl_to_thread(ns, ld, NULL, cancels, count, cancel_flags);\r\n}\r\nstatic int ldlm_handle_setinfo(struct ptlrpc_request *req)\r\n{\r\nstruct obd_device *obd = req->rq_export->exp_obd;\r\nchar *key;\r\nvoid *val;\r\nint keylen, vallen;\r\nint rc = -ENOSYS;\r\nDEBUG_REQ(D_HSM, req, "%s: handle setinfo\n", obd->obd_name);\r\nreq_capsule_set(&req->rq_pill, &RQF_OBD_SET_INFO);\r\nkey = req_capsule_client_get(&req->rq_pill, &RMF_SETINFO_KEY);\r\nif (!key) {\r\nDEBUG_REQ(D_IOCTL, req, "no set_info key");\r\nreturn -EFAULT;\r\n}\r\nkeylen = req_capsule_get_size(&req->rq_pill, &RMF_SETINFO_KEY,\r\nRCL_CLIENT);\r\nval = req_capsule_client_get(&req->rq_pill, &RMF_SETINFO_VAL);\r\nif (!val) {\r\nDEBUG_REQ(D_IOCTL, req, "no set_info val");\r\nreturn -EFAULT;\r\n}\r\nvallen = req_capsule_get_size(&req->rq_pill, &RMF_SETINFO_VAL,\r\nRCL_CLIENT);\r\nif (KEY_IS(KEY_HSM_COPYTOOL_SEND))\r\nrc = obd_set_info_async(req->rq_svc_thread->t_env,\r\nreq->rq_export,\r\nsizeof(KEY_HSM_COPYTOOL_SEND),\r\nKEY_HSM_COPYTOOL_SEND,\r\nvallen, val, NULL);\r\nelse\r\nDEBUG_REQ(D_WARNING, req, "ignoring unknown key %s", key);\r\nreturn rc;\r\n}\r\nstatic inline void ldlm_callback_errmsg(struct ptlrpc_request *req,\r\nconst char *msg, int rc,\r\nconst struct lustre_handle *handle)\r\n{\r\nDEBUG_REQ((req->rq_no_reply || rc) ? D_WARNING : D_DLMTRACE, req,\r\n"%s: [nid %s] [rc %d] [lock %#llx]",\r\nmsg, libcfs_id2str(req->rq_peer), rc,\r\nhandle ? handle->cookie : 0);\r\nif (req->rq_no_reply)\r\nCWARN("No reply was sent, maybe cause bug 21636.\n");\r\nelse if (rc)\r\nCWARN("Send reply failed, maybe cause bug 21636.\n");\r\n}\r\nstatic int ldlm_callback_handler(struct ptlrpc_request *req)\r\n{\r\nstruct ldlm_namespace *ns;\r\nstruct ldlm_request *dlm_req;\r\nstruct ldlm_lock *lock;\r\nint rc;\r\nif (lustre_msg_get_opc(req->rq_reqmsg) == SEC_CTX_FINI)\r\nreturn 0;\r\nreq_capsule_init(&req->rq_pill, req, RCL_SERVER);\r\nif (!req->rq_export) {\r\nrc = ldlm_callback_reply(req, -ENOTCONN);\r\nldlm_callback_errmsg(req, "Operate on unconnected server",\r\nrc, NULL);\r\nreturn 0;\r\n}\r\nLASSERT(req->rq_export->exp_obd);\r\nswitch (lustre_msg_get_opc(req->rq_reqmsg)) {\r\ncase LDLM_BL_CALLBACK:\r\nif (OBD_FAIL_CHECK(OBD_FAIL_LDLM_BL_CALLBACK_NET)) {\r\nif (cfs_fail_err)\r\nldlm_callback_reply(req, -(int)cfs_fail_err);\r\nreturn 0;\r\n}\r\nbreak;\r\ncase LDLM_CP_CALLBACK:\r\nif (OBD_FAIL_CHECK(OBD_FAIL_LDLM_CP_CALLBACK_NET))\r\nreturn 0;\r\nbreak;\r\ncase LDLM_GL_CALLBACK:\r\nif (OBD_FAIL_CHECK(OBD_FAIL_LDLM_GL_CALLBACK_NET))\r\nreturn 0;\r\nbreak;\r\ncase LDLM_SET_INFO:\r\nrc = ldlm_handle_setinfo(req);\r\nldlm_callback_reply(req, rc);\r\nreturn 0;\r\ndefault:\r\nCERROR("unknown opcode %u\n",\r\nlustre_msg_get_opc(req->rq_reqmsg));\r\nldlm_callback_reply(req, -EPROTO);\r\nreturn 0;\r\n}\r\nns = req->rq_export->exp_obd->obd_namespace;\r\nLASSERT(ns);\r\nreq_capsule_set(&req->rq_pill, &RQF_LDLM_CALLBACK);\r\ndlm_req = req_capsule_client_get(&req->rq_pill, &RMF_DLM_REQ);\r\nif (!dlm_req) {\r\nrc = ldlm_callback_reply(req, -EPROTO);\r\nldlm_callback_errmsg(req, "Operate without parameter", rc,\r\nNULL);\r\nreturn 0;\r\n}\r\nif (OBD_FAIL_CHECK(OBD_FAIL_LDLM_CANCEL_BL_CB_RACE) &&\r\nlustre_msg_get_opc(req->rq_reqmsg) == LDLM_BL_CALLBACK) {\r\nrc = ldlm_cli_cancel(&dlm_req->lock_handle[0], 0);\r\nif (rc < 0)\r\nCERROR("ldlm_cli_cancel: %d\n", rc);\r\n}\r\nlock = ldlm_handle2lock_long(&dlm_req->lock_handle[0], 0);\r\nif (!lock) {\r\nCDEBUG(D_DLMTRACE, "callback on lock %#llx - lock disappeared\n",\r\ndlm_req->lock_handle[0].cookie);\r\nrc = ldlm_callback_reply(req, -EINVAL);\r\nldlm_callback_errmsg(req, "Operate with invalid parameter", rc,\r\n&dlm_req->lock_handle[0]);\r\nreturn 0;\r\n}\r\nif (ldlm_is_fail_loc(lock) &&\r\nlustre_msg_get_opc(req->rq_reqmsg) == LDLM_BL_CALLBACK)\r\nOBD_RACE(OBD_FAIL_LDLM_CP_BL_RACE);\r\nlock_res_and_lock(lock);\r\nlock->l_flags |= ldlm_flags_from_wire(dlm_req->lock_flags &\r\nLDLM_FL_AST_MASK);\r\nif (lustre_msg_get_opc(req->rq_reqmsg) == LDLM_BL_CALLBACK) {\r\nif ((ldlm_is_canceling(lock) && ldlm_is_bl_done(lock)) ||\r\nldlm_is_failed(lock)) {\r\nLDLM_DEBUG(lock,\r\n"callback on lock %#llx - lock disappeared",\r\ndlm_req->lock_handle[0].cookie);\r\nunlock_res_and_lock(lock);\r\nLDLM_LOCK_RELEASE(lock);\r\nrc = ldlm_callback_reply(req, -EINVAL);\r\nldlm_callback_errmsg(req, "Operate on stale lock", rc,\r\n&dlm_req->lock_handle[0]);\r\nreturn 0;\r\n}\r\nldlm_lock_remove_from_lru(lock);\r\nldlm_set_bl_ast(lock);\r\n}\r\nunlock_res_and_lock(lock);\r\nswitch (lustre_msg_get_opc(req->rq_reqmsg)) {\r\ncase LDLM_BL_CALLBACK:\r\nCDEBUG(D_INODE, "blocking ast\n");\r\nreq_capsule_extend(&req->rq_pill, &RQF_LDLM_BL_CALLBACK);\r\nif (!ldlm_is_cancel_on_block(lock)) {\r\nrc = ldlm_callback_reply(req, 0);\r\nif (req->rq_no_reply || rc)\r\nldlm_callback_errmsg(req, "Normal process", rc,\r\n&dlm_req->lock_handle[0]);\r\n}\r\nif (ldlm_bl_to_thread_lock(ns, &dlm_req->lock_desc, lock))\r\nldlm_handle_bl_callback(ns, &dlm_req->lock_desc, lock);\r\nbreak;\r\ncase LDLM_CP_CALLBACK:\r\nCDEBUG(D_INODE, "completion ast\n");\r\nreq_capsule_extend(&req->rq_pill, &RQF_LDLM_CP_CALLBACK);\r\nldlm_callback_reply(req, 0);\r\nldlm_handle_cp_callback(req, ns, dlm_req, lock);\r\nbreak;\r\ncase LDLM_GL_CALLBACK:\r\nCDEBUG(D_INODE, "glimpse ast\n");\r\nreq_capsule_extend(&req->rq_pill, &RQF_LDLM_GL_CALLBACK);\r\nldlm_handle_gl_callback(req, ns, dlm_req, lock);\r\nbreak;\r\ndefault:\r\nLBUG();\r\n}\r\nreturn 0;\r\n}\r\nstatic struct ldlm_bl_work_item *ldlm_bl_get_work(struct ldlm_bl_pool *blp)\r\n{\r\nstruct ldlm_bl_work_item *blwi = NULL;\r\nstatic unsigned int num_bl;\r\nspin_lock(&blp->blp_lock);\r\nif (!list_empty(&blp->blp_list) &&\r\n(list_empty(&blp->blp_prio_list) || num_bl == 0))\r\nblwi = list_entry(blp->blp_list.next,\r\nstruct ldlm_bl_work_item, blwi_entry);\r\nelse\r\nif (!list_empty(&blp->blp_prio_list))\r\nblwi = list_entry(blp->blp_prio_list.next,\r\nstruct ldlm_bl_work_item,\r\nblwi_entry);\r\nif (blwi) {\r\nif (++num_bl >= atomic_read(&blp->blp_num_threads))\r\nnum_bl = 0;\r\nlist_del(&blwi->blwi_entry);\r\n}\r\nspin_unlock(&blp->blp_lock);\r\nreturn blwi;\r\n}\r\nstatic int ldlm_bl_thread_start(struct ldlm_bl_pool *blp)\r\n{\r\nstruct ldlm_bl_thread_data bltd = { .bltd_blp = blp };\r\nstruct task_struct *task;\r\ninit_completion(&bltd.bltd_comp);\r\nbltd.bltd_num = atomic_read(&blp->blp_num_threads);\r\nsnprintf(bltd.bltd_name, sizeof(bltd.bltd_name),\r\n"ldlm_bl_%02d", bltd.bltd_num);\r\ntask = kthread_run(ldlm_bl_thread_main, &bltd, "%s", bltd.bltd_name);\r\nif (IS_ERR(task)) {\r\nCERROR("cannot start LDLM thread ldlm_bl_%02d: rc %ld\n",\r\natomic_read(&blp->blp_num_threads), PTR_ERR(task));\r\nreturn PTR_ERR(task);\r\n}\r\nwait_for_completion(&bltd.bltd_comp);\r\nreturn 0;\r\n}\r\nstatic int ldlm_bl_thread_main(void *arg)\r\n{\r\nstruct ldlm_bl_pool *blp;\r\n{\r\nstruct ldlm_bl_thread_data *bltd = arg;\r\nblp = bltd->bltd_blp;\r\natomic_inc(&blp->blp_num_threads);\r\natomic_inc(&blp->blp_busy_threads);\r\ncomplete(&bltd->bltd_comp);\r\n}\r\nwhile (1) {\r\nstruct l_wait_info lwi = { 0 };\r\nstruct ldlm_bl_work_item *blwi = NULL;\r\nint busy;\r\nblwi = ldlm_bl_get_work(blp);\r\nif (!blwi) {\r\natomic_dec(&blp->blp_busy_threads);\r\nl_wait_event_exclusive(blp->blp_waitq,\r\n(blwi = ldlm_bl_get_work(blp)),\r\n&lwi);\r\nbusy = atomic_inc_return(&blp->blp_busy_threads);\r\n} else {\r\nbusy = atomic_read(&blp->blp_busy_threads);\r\n}\r\nif (!blwi->blwi_ns)\r\nbreak;\r\nif (unlikely(busy < blp->blp_max_threads &&\r\nbusy >= atomic_read(&blp->blp_num_threads) &&\r\n!blwi->blwi_mem_pressure))\r\nldlm_bl_thread_start(blp);\r\nif (blwi->blwi_mem_pressure)\r\nmemory_pressure_set();\r\nif (blwi->blwi_count) {\r\nint count;\r\ncount = ldlm_cli_cancel_list_local(&blwi->blwi_head,\r\nblwi->blwi_count,\r\nLCF_BL_AST);\r\nldlm_cli_cancel_list(&blwi->blwi_head, count, NULL,\r\nblwi->blwi_flags);\r\n} else {\r\nldlm_handle_bl_callback(blwi->blwi_ns, &blwi->blwi_ld,\r\nblwi->blwi_lock);\r\n}\r\nif (blwi->blwi_mem_pressure)\r\nmemory_pressure_clr();\r\nif (blwi->blwi_flags & LCF_ASYNC)\r\nkfree(blwi);\r\nelse\r\ncomplete(&blwi->blwi_comp);\r\n}\r\natomic_dec(&blp->blp_busy_threads);\r\natomic_dec(&blp->blp_num_threads);\r\ncomplete(&blp->blp_comp);\r\nreturn 0;\r\n}\r\nint ldlm_get_ref(void)\r\n{\r\nint rc = 0;\r\nmutex_lock(&ldlm_ref_mutex);\r\nif (++ldlm_refcount == 1) {\r\nrc = ldlm_setup();\r\nif (rc)\r\nldlm_refcount--;\r\n}\r\nmutex_unlock(&ldlm_ref_mutex);\r\nreturn rc;\r\n}\r\nvoid ldlm_put_ref(void)\r\n{\r\nmutex_lock(&ldlm_ref_mutex);\r\nif (ldlm_refcount == 1) {\r\nint rc = ldlm_cleanup();\r\nif (rc)\r\nCERROR("ldlm_cleanup failed: %d\n", rc);\r\nelse\r\nldlm_refcount--;\r\n} else {\r\nldlm_refcount--;\r\n}\r\nmutex_unlock(&ldlm_ref_mutex);\r\n}\r\nstatic ssize_t cancel_unused_locks_before_replay_show(struct kobject *kobj,\r\nstruct attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%d\n", ldlm_cancel_unused_locks_before_replay);\r\n}\r\nstatic ssize_t cancel_unused_locks_before_replay_store(struct kobject *kobj,\r\nstruct attribute *attr,\r\nconst char *buffer,\r\nsize_t count)\r\n{\r\nint rc;\r\nunsigned long val;\r\nrc = kstrtoul(buffer, 10, &val);\r\nif (rc)\r\nreturn rc;\r\nldlm_cancel_unused_locks_before_replay = val;\r\nreturn count;\r\n}\r\nstatic int ldlm_setup(void)\r\n{\r\nstatic struct ptlrpc_service_conf conf;\r\nstruct ldlm_bl_pool *blp = NULL;\r\nint rc = 0;\r\nint i;\r\nif (ldlm_state)\r\nreturn -EALREADY;\r\nldlm_state = kzalloc(sizeof(*ldlm_state), GFP_NOFS);\r\nif (!ldlm_state)\r\nreturn -ENOMEM;\r\nldlm_kobj = kobject_create_and_add("ldlm", lustre_kobj);\r\nif (!ldlm_kobj) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nrc = sysfs_create_group(ldlm_kobj, &ldlm_attr_group);\r\nif (rc)\r\ngoto out;\r\nldlm_ns_kset = kset_create_and_add("namespaces", NULL, ldlm_kobj);\r\nif (!ldlm_ns_kset) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nldlm_svc_kset = kset_create_and_add("services", NULL, ldlm_kobj);\r\nif (!ldlm_svc_kset) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nrc = ldlm_debugfs_setup();\r\nif (rc != 0)\r\ngoto out;\r\nmemset(&conf, 0, sizeof(conf));\r\nconf = (typeof(conf)) {\r\n.psc_name = "ldlm_cbd",\r\n.psc_watchdog_factor = 2,\r\n.psc_buf = {\r\n.bc_nbufs = LDLM_CLIENT_NBUFS,\r\n.bc_buf_size = LDLM_BUFSIZE,\r\n.bc_req_max_size = LDLM_MAXREQSIZE,\r\n.bc_rep_max_size = LDLM_MAXREPSIZE,\r\n.bc_req_portal = LDLM_CB_REQUEST_PORTAL,\r\n.bc_rep_portal = LDLM_CB_REPLY_PORTAL,\r\n},\r\n.psc_thr = {\r\n.tc_thr_name = "ldlm_cb",\r\n.tc_thr_factor = LDLM_THR_FACTOR,\r\n.tc_nthrs_init = LDLM_NTHRS_INIT,\r\n.tc_nthrs_base = LDLM_NTHRS_BASE,\r\n.tc_nthrs_max = LDLM_NTHRS_MAX,\r\n.tc_nthrs_user = ldlm_num_threads,\r\n.tc_cpu_affinity = 1,\r\n.tc_ctx_tags = LCT_MD_THREAD | LCT_DT_THREAD,\r\n},\r\n.psc_cpt = {\r\n.cc_pattern = ldlm_cpts,\r\n},\r\n.psc_ops = {\r\n.so_req_handler = ldlm_callback_handler,\r\n},\r\n};\r\nldlm_state->ldlm_cb_service =\r\nptlrpc_register_service(&conf, ldlm_svc_kset,\r\nldlm_svc_debugfs_dir);\r\nif (IS_ERR(ldlm_state->ldlm_cb_service)) {\r\nCERROR("failed to start service\n");\r\nrc = PTR_ERR(ldlm_state->ldlm_cb_service);\r\nldlm_state->ldlm_cb_service = NULL;\r\ngoto out;\r\n}\r\nblp = kzalloc(sizeof(*blp), GFP_NOFS);\r\nif (!blp) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nldlm_state->ldlm_bl_pool = blp;\r\nspin_lock_init(&blp->blp_lock);\r\nINIT_LIST_HEAD(&blp->blp_list);\r\nINIT_LIST_HEAD(&blp->blp_prio_list);\r\ninit_waitqueue_head(&blp->blp_waitq);\r\natomic_set(&blp->blp_num_threads, 0);\r\natomic_set(&blp->blp_busy_threads, 0);\r\nif (ldlm_num_threads == 0) {\r\nblp->blp_min_threads = LDLM_NTHRS_INIT;\r\nblp->blp_max_threads = LDLM_NTHRS_MAX;\r\n} else {\r\nblp->blp_min_threads = min_t(int, LDLM_NTHRS_MAX,\r\nmax_t(int, LDLM_NTHRS_INIT,\r\nldlm_num_threads));\r\nblp->blp_max_threads = blp->blp_min_threads;\r\n}\r\nfor (i = 0; i < blp->blp_min_threads; i++) {\r\nrc = ldlm_bl_thread_start(blp);\r\nif (rc < 0)\r\ngoto out;\r\n}\r\nrc = ldlm_pools_init();\r\nif (rc) {\r\nCERROR("Failed to initialize LDLM pools: %d\n", rc);\r\ngoto out;\r\n}\r\nreturn 0;\r\nout:\r\nldlm_cleanup();\r\nreturn rc;\r\n}\r\nstatic int ldlm_cleanup(void)\r\n{\r\nif (!list_empty(ldlm_namespace_list(LDLM_NAMESPACE_SERVER)) ||\r\n!list_empty(ldlm_namespace_list(LDLM_NAMESPACE_CLIENT))) {\r\nCERROR("ldlm still has namespaces; clean these up first.\n");\r\nldlm_dump_all_namespaces(LDLM_NAMESPACE_SERVER, D_DLMTRACE);\r\nldlm_dump_all_namespaces(LDLM_NAMESPACE_CLIENT, D_DLMTRACE);\r\nreturn -EBUSY;\r\n}\r\nldlm_pools_fini();\r\nif (ldlm_state->ldlm_bl_pool) {\r\nstruct ldlm_bl_pool *blp = ldlm_state->ldlm_bl_pool;\r\nwhile (atomic_read(&blp->blp_num_threads) > 0) {\r\nstruct ldlm_bl_work_item blwi = { .blwi_ns = NULL };\r\ninit_completion(&blp->blp_comp);\r\nspin_lock(&blp->blp_lock);\r\nlist_add_tail(&blwi.blwi_entry, &blp->blp_list);\r\nwake_up(&blp->blp_waitq);\r\nspin_unlock(&blp->blp_lock);\r\nwait_for_completion(&blp->blp_comp);\r\n}\r\nkfree(blp);\r\n}\r\nif (ldlm_state->ldlm_cb_service)\r\nptlrpc_unregister_service(ldlm_state->ldlm_cb_service);\r\nif (ldlm_ns_kset)\r\nkset_unregister(ldlm_ns_kset);\r\nif (ldlm_svc_kset)\r\nkset_unregister(ldlm_svc_kset);\r\nif (ldlm_kobj)\r\nkobject_put(ldlm_kobj);\r\nldlm_debugfs_cleanup();\r\nkfree(ldlm_state);\r\nldlm_state = NULL;\r\nreturn 0;\r\n}\r\nint ldlm_init(void)\r\n{\r\nmutex_init(&ldlm_ref_mutex);\r\nmutex_init(ldlm_namespace_lock(LDLM_NAMESPACE_SERVER));\r\nmutex_init(ldlm_namespace_lock(LDLM_NAMESPACE_CLIENT));\r\nldlm_resource_slab = kmem_cache_create("ldlm_resources",\r\nsizeof(struct ldlm_resource), 0,\r\nSLAB_HWCACHE_ALIGN, NULL);\r\nif (!ldlm_resource_slab)\r\nreturn -ENOMEM;\r\nldlm_lock_slab = kmem_cache_create("ldlm_locks",\r\nsizeof(struct ldlm_lock), 0,\r\nSLAB_HWCACHE_ALIGN |\r\nSLAB_DESTROY_BY_RCU, NULL);\r\nif (!ldlm_lock_slab) {\r\nkmem_cache_destroy(ldlm_resource_slab);\r\nreturn -ENOMEM;\r\n}\r\nldlm_interval_slab = kmem_cache_create("interval_node",\r\nsizeof(struct ldlm_interval),\r\n0, SLAB_HWCACHE_ALIGN, NULL);\r\nif (!ldlm_interval_slab) {\r\nkmem_cache_destroy(ldlm_resource_slab);\r\nkmem_cache_destroy(ldlm_lock_slab);\r\nreturn -ENOMEM;\r\n}\r\n#if LUSTRE_TRACKS_LOCK_EXP_REFS\r\nclass_export_dump_hook = ldlm_dump_export_locks;\r\n#endif\r\nreturn 0;\r\n}\r\nvoid ldlm_exit(void)\r\n{\r\nif (ldlm_refcount)\r\nCERROR("ldlm_refcount is %d in ldlm_exit!\n", ldlm_refcount);\r\nkmem_cache_destroy(ldlm_resource_slab);\r\nsynchronize_rcu();\r\nkmem_cache_destroy(ldlm_lock_slab);\r\nkmem_cache_destroy(ldlm_interval_slab);\r\n}
