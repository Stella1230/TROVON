static void est_fetch_counters(struct net_rate_estimator *e,\r\nstruct gnet_stats_basic_packed *b)\r\n{\r\nif (e->stats_lock)\r\nspin_lock(e->stats_lock);\r\n__gnet_stats_copy_basic(e->running, b, e->cpu_bstats, e->bstats);\r\nif (e->stats_lock)\r\nspin_unlock(e->stats_lock);\r\n}\r\nstatic void est_timer(unsigned long arg)\r\n{\r\nstruct net_rate_estimator *est = (struct net_rate_estimator *)arg;\r\nstruct gnet_stats_basic_packed b;\r\nu64 rate, brate;\r\nest_fetch_counters(est, &b);\r\nbrate = (b.bytes - est->last_bytes) << (8 - est->ewma_log);\r\nbrate -= (est->avbps >> est->ewma_log);\r\nrate = (u64)(b.packets - est->last_packets) << (8 - est->ewma_log);\r\nrate -= (est->avpps >> est->ewma_log);\r\nwrite_seqcount_begin(&est->seq);\r\nest->avbps += brate;\r\nest->avpps += rate;\r\nwrite_seqcount_end(&est->seq);\r\nest->last_bytes = b.bytes;\r\nest->last_packets = b.packets;\r\nest->next_jiffies += ((HZ/4) << est->intvl_log);\r\nif (unlikely(time_after_eq(jiffies, est->next_jiffies))) {\r\nest->next_jiffies = jiffies + 1;\r\n}\r\nmod_timer(&est->timer, est->next_jiffies);\r\n}\r\nint gen_new_estimator(struct gnet_stats_basic_packed *bstats,\r\nstruct gnet_stats_basic_cpu __percpu *cpu_bstats,\r\nstruct net_rate_estimator __rcu **rate_est,\r\nspinlock_t *stats_lock,\r\nseqcount_t *running,\r\nstruct nlattr *opt)\r\n{\r\nstruct gnet_estimator *parm = nla_data(opt);\r\nstruct net_rate_estimator *old, *est;\r\nstruct gnet_stats_basic_packed b;\r\nint intvl_log;\r\nif (nla_len(opt) < sizeof(*parm))\r\nreturn -EINVAL;\r\nif (parm->interval < -2 || parm->interval > 3)\r\nreturn -EINVAL;\r\nest = kzalloc(sizeof(*est), GFP_KERNEL);\r\nif (!est)\r\nreturn -ENOBUFS;\r\nseqcount_init(&est->seq);\r\nintvl_log = parm->interval + 2;\r\nest->bstats = bstats;\r\nest->stats_lock = stats_lock;\r\nest->running = running;\r\nest->ewma_log = parm->ewma_log;\r\nest->intvl_log = intvl_log;\r\nest->cpu_bstats = cpu_bstats;\r\nest_fetch_counters(est, &b);\r\nest->last_bytes = b.bytes;\r\nest->last_packets = b.packets;\r\nold = rcu_dereference_protected(*rate_est, 1);\r\nif (old) {\r\ndel_timer_sync(&old->timer);\r\nest->avbps = old->avbps;\r\nest->avpps = old->avpps;\r\n}\r\nest->next_jiffies = jiffies + ((HZ/4) << intvl_log);\r\nsetup_timer(&est->timer, est_timer, (unsigned long)est);\r\nmod_timer(&est->timer, est->next_jiffies);\r\nrcu_assign_pointer(*rate_est, est);\r\nif (old)\r\nkfree_rcu(old, rcu);\r\nreturn 0;\r\n}\r\nvoid gen_kill_estimator(struct net_rate_estimator __rcu **rate_est)\r\n{\r\nstruct net_rate_estimator *est;\r\nest = xchg((__force struct net_rate_estimator **)rate_est, NULL);\r\nif (est) {\r\ndel_timer_sync(&est->timer);\r\nkfree_rcu(est, rcu);\r\n}\r\n}\r\nint gen_replace_estimator(struct gnet_stats_basic_packed *bstats,\r\nstruct gnet_stats_basic_cpu __percpu *cpu_bstats,\r\nstruct net_rate_estimator __rcu **rate_est,\r\nspinlock_t *stats_lock,\r\nseqcount_t *running, struct nlattr *opt)\r\n{\r\nreturn gen_new_estimator(bstats, cpu_bstats, rate_est,\r\nstats_lock, running, opt);\r\n}\r\nbool gen_estimator_active(struct net_rate_estimator __rcu **rate_est)\r\n{\r\nreturn !!rcu_access_pointer(*rate_est);\r\n}\r\nbool gen_estimator_read(struct net_rate_estimator __rcu **rate_est,\r\nstruct gnet_stats_rate_est64 *sample)\r\n{\r\nstruct net_rate_estimator *est;\r\nunsigned seq;\r\nrcu_read_lock();\r\nest = rcu_dereference(*rate_est);\r\nif (!est) {\r\nrcu_read_unlock();\r\nreturn false;\r\n}\r\ndo {\r\nseq = read_seqcount_begin(&est->seq);\r\nsample->bps = est->avbps >> 8;\r\nsample->pps = est->avpps >> 8;\r\n} while (read_seqcount_retry(&est->seq, seq));\r\nrcu_read_unlock();\r\nreturn true;\r\n}
