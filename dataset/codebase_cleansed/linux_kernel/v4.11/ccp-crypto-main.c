static inline bool ccp_crypto_success(int err)\r\n{\r\nif (err && (err != -EINPROGRESS) && (err != -EBUSY))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic struct ccp_crypto_cmd *ccp_crypto_cmd_complete(\r\nstruct ccp_crypto_cmd *crypto_cmd, struct ccp_crypto_cmd **backlog)\r\n{\r\nstruct ccp_crypto_cmd *held = NULL, *tmp;\r\nunsigned long flags;\r\n*backlog = NULL;\r\nspin_lock_irqsave(&req_queue_lock, flags);\r\ntmp = crypto_cmd;\r\nlist_for_each_entry_continue(tmp, &req_queue.cmds, entry) {\r\nif (crypto_cmd->tfm != tmp->tfm)\r\ncontinue;\r\nheld = tmp;\r\nbreak;\r\n}\r\nif (req_queue.backlog != &req_queue.cmds) {\r\nif (req_queue.backlog == &crypto_cmd->entry)\r\nreq_queue.backlog = crypto_cmd->entry.next;\r\n*backlog = container_of(req_queue.backlog,\r\nstruct ccp_crypto_cmd, entry);\r\nreq_queue.backlog = req_queue.backlog->next;\r\nif (req_queue.backlog == &crypto_cmd->entry)\r\nreq_queue.backlog = crypto_cmd->entry.next;\r\n}\r\nreq_queue.cmd_count--;\r\nlist_del(&crypto_cmd->entry);\r\nspin_unlock_irqrestore(&req_queue_lock, flags);\r\nreturn held;\r\n}\r\nstatic void ccp_crypto_complete(void *data, int err)\r\n{\r\nstruct ccp_crypto_cmd *crypto_cmd = data;\r\nstruct ccp_crypto_cmd *held, *next, *backlog;\r\nstruct crypto_async_request *req = crypto_cmd->req;\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(req->tfm);\r\nint ret;\r\nif (err == -EINPROGRESS) {\r\nif (crypto_cmd->ret == -EBUSY) {\r\ncrypto_cmd->ret = -EINPROGRESS;\r\nreq->complete(req, -EINPROGRESS);\r\n}\r\nreturn;\r\n}\r\nheld = ccp_crypto_cmd_complete(crypto_cmd, &backlog);\r\nif (backlog) {\r\nbacklog->ret = -EINPROGRESS;\r\nbacklog->req->complete(backlog->req, -EINPROGRESS);\r\n}\r\nif (crypto_cmd->ret == -EBUSY)\r\nreq->complete(req, -EINPROGRESS);\r\nret = err;\r\nif (ctx->complete)\r\nret = ctx->complete(req, ret);\r\nreq->complete(req, ret);\r\nwhile (held) {\r\nheld->cmd->flags |= CCP_CMD_MAY_BACKLOG;\r\nret = ccp_enqueue_cmd(held->cmd);\r\nif (ccp_crypto_success(ret))\r\nbreak;\r\nctx = crypto_tfm_ctx(held->req->tfm);\r\nif (ctx->complete)\r\nret = ctx->complete(held->req, ret);\r\nheld->req->complete(held->req, ret);\r\nnext = ccp_crypto_cmd_complete(held, &backlog);\r\nif (backlog) {\r\nbacklog->ret = -EINPROGRESS;\r\nbacklog->req->complete(backlog->req, -EINPROGRESS);\r\n}\r\nkfree(held);\r\nheld = next;\r\n}\r\nkfree(crypto_cmd);\r\n}\r\nstatic int ccp_crypto_enqueue_cmd(struct ccp_crypto_cmd *crypto_cmd)\r\n{\r\nstruct ccp_crypto_cmd *active = NULL, *tmp;\r\nunsigned long flags;\r\nbool free_cmd = true;\r\nint ret;\r\nspin_lock_irqsave(&req_queue_lock, flags);\r\nif (req_queue.cmd_count >= CCP_CRYPTO_MAX_QLEN) {\r\nret = -EBUSY;\r\nif (!(crypto_cmd->cmd->flags & CCP_CMD_MAY_BACKLOG))\r\ngoto e_lock;\r\n}\r\nlist_for_each_entry(tmp, &req_queue.cmds, entry) {\r\nif (crypto_cmd->tfm != tmp->tfm)\r\ncontinue;\r\nactive = tmp;\r\nbreak;\r\n}\r\nret = -EINPROGRESS;\r\nif (!active) {\r\nret = ccp_enqueue_cmd(crypto_cmd->cmd);\r\nif (!ccp_crypto_success(ret))\r\ngoto e_lock;\r\nif ((ret == -EBUSY) &&\r\n!(crypto_cmd->cmd->flags & CCP_CMD_MAY_BACKLOG))\r\ngoto e_lock;\r\n}\r\nif (req_queue.cmd_count >= CCP_CRYPTO_MAX_QLEN) {\r\nret = -EBUSY;\r\nif (req_queue.backlog == &req_queue.cmds)\r\nreq_queue.backlog = &crypto_cmd->entry;\r\n}\r\ncrypto_cmd->ret = ret;\r\nreq_queue.cmd_count++;\r\nlist_add_tail(&crypto_cmd->entry, &req_queue.cmds);\r\nfree_cmd = false;\r\ne_lock:\r\nspin_unlock_irqrestore(&req_queue_lock, flags);\r\nif (free_cmd)\r\nkfree(crypto_cmd);\r\nreturn ret;\r\n}\r\nint ccp_crypto_enqueue_request(struct crypto_async_request *req,\r\nstruct ccp_cmd *cmd)\r\n{\r\nstruct ccp_crypto_cmd *crypto_cmd;\r\ngfp_t gfp;\r\ngfp = req->flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL : GFP_ATOMIC;\r\ncrypto_cmd = kzalloc(sizeof(*crypto_cmd), gfp);\r\nif (!crypto_cmd)\r\nreturn -ENOMEM;\r\ncrypto_cmd->cmd = cmd;\r\ncrypto_cmd->req = req;\r\ncrypto_cmd->tfm = req->tfm;\r\ncmd->callback = ccp_crypto_complete;\r\ncmd->data = crypto_cmd;\r\nif (req->flags & CRYPTO_TFM_REQ_MAY_BACKLOG)\r\ncmd->flags |= CCP_CMD_MAY_BACKLOG;\r\nelse\r\ncmd->flags &= ~CCP_CMD_MAY_BACKLOG;\r\nreturn ccp_crypto_enqueue_cmd(crypto_cmd);\r\n}\r\nstruct scatterlist *ccp_crypto_sg_table_add(struct sg_table *table,\r\nstruct scatterlist *sg_add)\r\n{\r\nstruct scatterlist *sg, *sg_last = NULL;\r\nfor (sg = table->sgl; sg; sg = sg_next(sg))\r\nif (!sg_page(sg))\r\nbreak;\r\nif (WARN_ON(!sg))\r\nreturn NULL;\r\nfor (; sg && sg_add; sg = sg_next(sg), sg_add = sg_next(sg_add)) {\r\nsg_set_page(sg, sg_page(sg_add), sg_add->length,\r\nsg_add->offset);\r\nsg_last = sg;\r\n}\r\nif (WARN_ON(sg_add))\r\nreturn NULL;\r\nreturn sg_last;\r\n}\r\nstatic int ccp_register_algs(void)\r\n{\r\nint ret;\r\nif (!aes_disable) {\r\nret = ccp_register_aes_algs(&cipher_algs);\r\nif (ret)\r\nreturn ret;\r\nret = ccp_register_aes_cmac_algs(&hash_algs);\r\nif (ret)\r\nreturn ret;\r\nret = ccp_register_aes_xts_algs(&cipher_algs);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (!sha_disable) {\r\nret = ccp_register_sha_algs(&hash_algs);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ccp_unregister_algs(void)\r\n{\r\nstruct ccp_crypto_ahash_alg *ahash_alg, *ahash_tmp;\r\nstruct ccp_crypto_ablkcipher_alg *ablk_alg, *ablk_tmp;\r\nlist_for_each_entry_safe(ahash_alg, ahash_tmp, &hash_algs, entry) {\r\ncrypto_unregister_ahash(&ahash_alg->alg);\r\nlist_del(&ahash_alg->entry);\r\nkfree(ahash_alg);\r\n}\r\nlist_for_each_entry_safe(ablk_alg, ablk_tmp, &cipher_algs, entry) {\r\ncrypto_unregister_alg(&ablk_alg->alg);\r\nlist_del(&ablk_alg->entry);\r\nkfree(ablk_alg);\r\n}\r\n}\r\nstatic int ccp_crypto_init(void)\r\n{\r\nint ret;\r\nret = ccp_present();\r\nif (ret)\r\nreturn ret;\r\nspin_lock_init(&req_queue_lock);\r\nINIT_LIST_HEAD(&req_queue.cmds);\r\nreq_queue.backlog = &req_queue.cmds;\r\nreq_queue.cmd_count = 0;\r\nret = ccp_register_algs();\r\nif (ret)\r\nccp_unregister_algs();\r\nreturn ret;\r\n}\r\nstatic void ccp_crypto_exit(void)\r\n{\r\nccp_unregister_algs();\r\n}
