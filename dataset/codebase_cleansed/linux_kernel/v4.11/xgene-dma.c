static bool is_pq_enabled(struct xgene_dma *pdma)\r\n{\r\nu32 val;\r\nval = ioread32(pdma->csr_efuse + XGENE_SOC_JTAG1_SHADOW);\r\nreturn !(val & XGENE_DMA_PQ_DISABLE_MASK);\r\n}\r\nstatic u64 xgene_dma_encode_len(size_t len)\r\n{\r\nreturn (len < XGENE_DMA_MAX_BYTE_CNT) ?\r\n((u64)len << XGENE_DMA_DESC_BUFLEN_POS) :\r\nXGENE_DMA_16K_BUFFER_LEN_CODE;\r\n}\r\nstatic u8 xgene_dma_encode_xor_flyby(u32 src_cnt)\r\n{\r\nstatic u8 flyby_type[] = {\r\nFLYBY_2SRC_XOR,\r\nFLYBY_2SRC_XOR,\r\nFLYBY_2SRC_XOR,\r\nFLYBY_3SRC_XOR,\r\nFLYBY_4SRC_XOR,\r\nFLYBY_5SRC_XOR\r\n};\r\nreturn flyby_type[src_cnt];\r\n}\r\nstatic void xgene_dma_set_src_buffer(__le64 *ext8, size_t *len,\r\ndma_addr_t *paddr)\r\n{\r\nsize_t nbytes = (*len < XGENE_DMA_MAX_BYTE_CNT) ?\r\n*len : XGENE_DMA_MAX_BYTE_CNT;\r\n*ext8 |= cpu_to_le64(*paddr);\r\n*ext8 |= cpu_to_le64(xgene_dma_encode_len(nbytes));\r\n*len -= nbytes;\r\n*paddr += nbytes;\r\n}\r\nstatic void xgene_dma_invalidate_buffer(__le64 *ext8)\r\n{\r\n*ext8 |= cpu_to_le64(XGENE_DMA_INVALID_LEN_CODE);\r\n}\r\nstatic __le64 *xgene_dma_lookup_ext8(struct xgene_dma_desc_hw *desc, int idx)\r\n{\r\nswitch (idx) {\r\ncase 0:\r\nreturn &desc->m1;\r\ncase 1:\r\nreturn &desc->m0;\r\ncase 2:\r\nreturn &desc->m3;\r\ncase 3:\r\nreturn &desc->m2;\r\ndefault:\r\npr_err("Invalid dma descriptor index\n");\r\n}\r\nreturn NULL;\r\n}\r\nstatic void xgene_dma_init_desc(struct xgene_dma_desc_hw *desc,\r\nu16 dst_ring_num)\r\n{\r\ndesc->m0 |= cpu_to_le64(XGENE_DMA_DESC_IN_BIT);\r\ndesc->m0 |= cpu_to_le64((u64)XGENE_DMA_RING_OWNER_DMA <<\r\nXGENE_DMA_DESC_RTYPE_POS);\r\ndesc->m1 |= cpu_to_le64(XGENE_DMA_DESC_C_BIT);\r\ndesc->m3 |= cpu_to_le64((u64)dst_ring_num <<\r\nXGENE_DMA_DESC_HOENQ_NUM_POS);\r\n}\r\nstatic void xgene_dma_prep_cpy_desc(struct xgene_dma_chan *chan,\r\nstruct xgene_dma_desc_sw *desc_sw,\r\ndma_addr_t dst, dma_addr_t src,\r\nsize_t len)\r\n{\r\nstruct xgene_dma_desc_hw *desc1, *desc2;\r\nint i;\r\ndesc1 = &desc_sw->desc1;\r\nxgene_dma_init_desc(desc1, chan->tx_ring.dst_ring_num);\r\ndesc1->m2 |= cpu_to_le64(XGENE_DMA_DESC_DR_BIT);\r\ndesc1->m3 |= cpu_to_le64(dst);\r\nxgene_dma_set_src_buffer(&desc1->m1, &len, &src);\r\nif (!len)\r\nreturn;\r\ndesc2 = &desc_sw->desc2;\r\ndesc1->m0 |= cpu_to_le64(XGENE_DMA_DESC_NV_BIT);\r\nfor (i = 0; i < 4 && len; i++)\r\nxgene_dma_set_src_buffer(xgene_dma_lookup_ext8(desc2, i),\r\n&len, &src);\r\nfor (; i < 4; i++)\r\nxgene_dma_invalidate_buffer(xgene_dma_lookup_ext8(desc2, i));\r\ndesc_sw->flags |= XGENE_DMA_FLAG_64B_DESC;\r\n}\r\nstatic void xgene_dma_prep_xor_desc(struct xgene_dma_chan *chan,\r\nstruct xgene_dma_desc_sw *desc_sw,\r\ndma_addr_t *dst, dma_addr_t *src,\r\nu32 src_cnt, size_t *nbytes,\r\nconst u8 *scf)\r\n{\r\nstruct xgene_dma_desc_hw *desc1, *desc2;\r\nsize_t len = *nbytes;\r\nint i;\r\ndesc1 = &desc_sw->desc1;\r\ndesc2 = &desc_sw->desc2;\r\nxgene_dma_init_desc(desc1, chan->tx_ring.dst_ring_num);\r\ndesc1->m2 |= cpu_to_le64(XGENE_DMA_DESC_DR_BIT);\r\ndesc1->m3 |= cpu_to_le64(*dst);\r\ndesc1->m0 |= cpu_to_le64(XGENE_DMA_DESC_NV_BIT);\r\ndesc1->m2 |= cpu_to_le64(xgene_dma_encode_xor_flyby(src_cnt));\r\nfor (i = 0; i < src_cnt; i++) {\r\nlen = *nbytes;\r\nxgene_dma_set_src_buffer((i == 0) ? &desc1->m1 :\r\nxgene_dma_lookup_ext8(desc2, i - 1),\r\n&len, &src[i]);\r\ndesc1->m2 |= cpu_to_le64((scf[i] << ((i + 1) * 8)));\r\n}\r\n*nbytes = len;\r\n*dst += XGENE_DMA_MAX_BYTE_CNT;\r\ndesc_sw->flags |= XGENE_DMA_FLAG_64B_DESC;\r\n}\r\nstatic dma_cookie_t xgene_dma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct xgene_dma_desc_sw *desc;\r\nstruct xgene_dma_chan *chan;\r\ndma_cookie_t cookie;\r\nif (unlikely(!tx))\r\nreturn -EINVAL;\r\nchan = to_dma_chan(tx->chan);\r\ndesc = to_dma_desc_sw(tx);\r\nspin_lock_bh(&chan->lock);\r\ncookie = dma_cookie_assign(tx);\r\nlist_splice_tail_init(&desc->tx_list, &chan->ld_pending);\r\nspin_unlock_bh(&chan->lock);\r\nreturn cookie;\r\n}\r\nstatic void xgene_dma_clean_descriptor(struct xgene_dma_chan *chan,\r\nstruct xgene_dma_desc_sw *desc)\r\n{\r\nlist_del(&desc->node);\r\nchan_dbg(chan, "LD %p free\n", desc);\r\ndma_pool_free(chan->desc_pool, desc, desc->tx.phys);\r\n}\r\nstatic struct xgene_dma_desc_sw *xgene_dma_alloc_descriptor(\r\nstruct xgene_dma_chan *chan)\r\n{\r\nstruct xgene_dma_desc_sw *desc;\r\ndma_addr_t phys;\r\ndesc = dma_pool_zalloc(chan->desc_pool, GFP_NOWAIT, &phys);\r\nif (!desc) {\r\nchan_err(chan, "Failed to allocate LDs\n");\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndesc->tx.phys = phys;\r\ndesc->tx.tx_submit = xgene_dma_tx_submit;\r\ndma_async_tx_descriptor_init(&desc->tx, &chan->dma_chan);\r\nchan_dbg(chan, "LD %p allocated\n", desc);\r\nreturn desc;\r\n}\r\nstatic void xgene_dma_clean_completed_descriptor(struct xgene_dma_chan *chan)\r\n{\r\nstruct xgene_dma_desc_sw *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, &chan->ld_completed, node) {\r\nif (async_tx_test_ack(&desc->tx))\r\nxgene_dma_clean_descriptor(chan, desc);\r\n}\r\n}\r\nstatic void xgene_dma_run_tx_complete_actions(struct xgene_dma_chan *chan,\r\nstruct xgene_dma_desc_sw *desc)\r\n{\r\nstruct dma_async_tx_descriptor *tx = &desc->tx;\r\nif (tx->cookie == 0)\r\nreturn;\r\ndma_cookie_complete(tx);\r\ndma_descriptor_unmap(tx);\r\ndmaengine_desc_get_callback_invoke(tx, NULL);\r\ndma_run_dependencies(tx);\r\n}\r\nstatic void xgene_dma_clean_running_descriptor(struct xgene_dma_chan *chan,\r\nstruct xgene_dma_desc_sw *desc)\r\n{\r\nlist_del(&desc->node);\r\nif (!async_tx_test_ack(&desc->tx)) {\r\nlist_add_tail(&desc->node, &chan->ld_completed);\r\nreturn;\r\n}\r\nchan_dbg(chan, "LD %p free\n", desc);\r\ndma_pool_free(chan->desc_pool, desc, desc->tx.phys);\r\n}\r\nstatic void xgene_chan_xfer_request(struct xgene_dma_chan *chan,\r\nstruct xgene_dma_desc_sw *desc_sw)\r\n{\r\nstruct xgene_dma_ring *ring = &chan->tx_ring;\r\nstruct xgene_dma_desc_hw *desc_hw;\r\ndesc_hw = &ring->desc_hw[ring->head];\r\nif (++ring->head == ring->slots)\r\nring->head = 0;\r\nmemcpy(desc_hw, &desc_sw->desc1, sizeof(*desc_hw));\r\nif (desc_sw->flags & XGENE_DMA_FLAG_64B_DESC) {\r\ndesc_hw = &ring->desc_hw[ring->head];\r\nif (++ring->head == ring->slots)\r\nring->head = 0;\r\nmemcpy(desc_hw, &desc_sw->desc2, sizeof(*desc_hw));\r\n}\r\nchan->pending += ((desc_sw->flags &\r\nXGENE_DMA_FLAG_64B_DESC) ? 2 : 1);\r\niowrite32((desc_sw->flags & XGENE_DMA_FLAG_64B_DESC) ?\r\n2 : 1, ring->cmd);\r\n}\r\nstatic void xgene_chan_xfer_ld_pending(struct xgene_dma_chan *chan)\r\n{\r\nstruct xgene_dma_desc_sw *desc_sw, *_desc_sw;\r\nif (list_empty(&chan->ld_pending)) {\r\nchan_dbg(chan, "No pending LDs\n");\r\nreturn;\r\n}\r\nlist_for_each_entry_safe(desc_sw, _desc_sw, &chan->ld_pending, node) {\r\nif (chan->pending >= chan->max_outstanding)\r\nreturn;\r\nxgene_chan_xfer_request(chan, desc_sw);\r\nlist_move_tail(&desc_sw->node, &chan->ld_running);\r\n}\r\n}\r\nstatic void xgene_dma_cleanup_descriptors(struct xgene_dma_chan *chan)\r\n{\r\nstruct xgene_dma_ring *ring = &chan->rx_ring;\r\nstruct xgene_dma_desc_sw *desc_sw, *_desc_sw;\r\nstruct xgene_dma_desc_hw *desc_hw;\r\nstruct list_head ld_completed;\r\nu8 status;\r\nINIT_LIST_HEAD(&ld_completed);\r\nspin_lock_bh(&chan->lock);\r\nxgene_dma_clean_completed_descriptor(chan);\r\nlist_for_each_entry_safe(desc_sw, _desc_sw, &chan->ld_running, node) {\r\ndesc_hw = &ring->desc_hw[ring->head];\r\nif (unlikely(le64_to_cpu(desc_hw->m0) ==\r\nXGENE_DMA_DESC_EMPTY_SIGNATURE))\r\nbreak;\r\nif (++ring->head == ring->slots)\r\nring->head = 0;\r\nstatus = XGENE_DMA_DESC_STATUS(\r\nXGENE_DMA_DESC_ELERR_RD(le64_to_cpu(\r\ndesc_hw->m0)),\r\nXGENE_DMA_DESC_LERR_RD(le64_to_cpu(\r\ndesc_hw->m0)));\r\nif (status) {\r\nchan_err(chan, "%s\n", xgene_dma_desc_err[status]);\r\nXGENE_DMA_DESC_DUMP(&desc_sw->desc1,\r\n"X-Gene DMA TX DESC1: ");\r\nif (desc_sw->flags & XGENE_DMA_FLAG_64B_DESC)\r\nXGENE_DMA_DESC_DUMP(&desc_sw->desc2,\r\n"X-Gene DMA TX DESC2: ");\r\nXGENE_DMA_DESC_DUMP(desc_hw,\r\n"X-Gene DMA RX ERR DESC: ");\r\n}\r\niowrite32(-1, ring->cmd);\r\ndesc_hw->m0 = cpu_to_le64(XGENE_DMA_DESC_EMPTY_SIGNATURE);\r\nchan->pending -= ((desc_sw->flags &\r\nXGENE_DMA_FLAG_64B_DESC) ? 2 : 1);\r\nlist_move_tail(&desc_sw->node, &ld_completed);\r\n}\r\nxgene_chan_xfer_ld_pending(chan);\r\nspin_unlock_bh(&chan->lock);\r\nlist_for_each_entry_safe(desc_sw, _desc_sw, &ld_completed, node) {\r\nxgene_dma_run_tx_complete_actions(chan, desc_sw);\r\nxgene_dma_clean_running_descriptor(chan, desc_sw);\r\n}\r\n}\r\nstatic int xgene_dma_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct xgene_dma_chan *chan = to_dma_chan(dchan);\r\nif (chan->desc_pool)\r\nreturn 1;\r\nchan->desc_pool = dma_pool_create(chan->name, chan->dev,\r\nsizeof(struct xgene_dma_desc_sw),\r\n0, 0);\r\nif (!chan->desc_pool) {\r\nchan_err(chan, "Failed to allocate descriptor pool\n");\r\nreturn -ENOMEM;\r\n}\r\nchan_dbg(chan, "Allocate descripto pool\n");\r\nreturn 1;\r\n}\r\nstatic void xgene_dma_free_desc_list(struct xgene_dma_chan *chan,\r\nstruct list_head *list)\r\n{\r\nstruct xgene_dma_desc_sw *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, list, node)\r\nxgene_dma_clean_descriptor(chan, desc);\r\n}\r\nstatic void xgene_dma_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct xgene_dma_chan *chan = to_dma_chan(dchan);\r\nchan_dbg(chan, "Free all resources\n");\r\nif (!chan->desc_pool)\r\nreturn;\r\nxgene_dma_cleanup_descriptors(chan);\r\nspin_lock_bh(&chan->lock);\r\nxgene_dma_free_desc_list(chan, &chan->ld_pending);\r\nxgene_dma_free_desc_list(chan, &chan->ld_running);\r\nxgene_dma_free_desc_list(chan, &chan->ld_completed);\r\nspin_unlock_bh(&chan->lock);\r\ndma_pool_destroy(chan->desc_pool);\r\nchan->desc_pool = NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *xgene_dma_prep_sg(\r\nstruct dma_chan *dchan, struct scatterlist *dst_sg,\r\nu32 dst_nents, struct scatterlist *src_sg,\r\nu32 src_nents, unsigned long flags)\r\n{\r\nstruct xgene_dma_desc_sw *first = NULL, *new = NULL;\r\nstruct xgene_dma_chan *chan;\r\nsize_t dst_avail, src_avail;\r\ndma_addr_t dst, src;\r\nsize_t len;\r\nif (unlikely(!dchan))\r\nreturn NULL;\r\nif (unlikely(!dst_nents || !src_nents))\r\nreturn NULL;\r\nif (unlikely(!dst_sg || !src_sg))\r\nreturn NULL;\r\nchan = to_dma_chan(dchan);\r\ndst_avail = sg_dma_len(dst_sg);\r\nsrc_avail = sg_dma_len(src_sg);\r\ndst_nents--;\r\nsrc_nents--;\r\nwhile (true) {\r\nlen = min_t(size_t, src_avail, dst_avail);\r\nlen = min_t(size_t, len, XGENE_DMA_MAX_64B_DESC_BYTE_CNT);\r\nif (len == 0)\r\ngoto fetch;\r\ndst = sg_dma_address(dst_sg) + sg_dma_len(dst_sg) - dst_avail;\r\nsrc = sg_dma_address(src_sg) + sg_dma_len(src_sg) - src_avail;\r\nnew = xgene_dma_alloc_descriptor(chan);\r\nif (!new)\r\ngoto fail;\r\nxgene_dma_prep_cpy_desc(chan, new, dst, src, len);\r\nif (!first)\r\nfirst = new;\r\nnew->tx.cookie = 0;\r\nasync_tx_ack(&new->tx);\r\ndst_avail -= len;\r\nsrc_avail -= len;\r\nlist_add_tail(&new->node, &first->tx_list);\r\nfetch:\r\nif (dst_avail == 0) {\r\nif (dst_nents == 0)\r\nbreak;\r\ndst_sg = sg_next(dst_sg);\r\nif (!dst_sg)\r\nbreak;\r\ndst_nents--;\r\ndst_avail = sg_dma_len(dst_sg);\r\n}\r\nif (src_avail == 0) {\r\nif (src_nents == 0)\r\nbreak;\r\nsrc_sg = sg_next(src_sg);\r\nif (!src_sg)\r\nbreak;\r\nsrc_nents--;\r\nsrc_avail = sg_dma_len(src_sg);\r\n}\r\n}\r\nif (!new)\r\nreturn NULL;\r\nnew->tx.flags = flags;\r\nnew->tx.cookie = -EBUSY;\r\nlist_splice(&first->tx_list, &new->tx_list);\r\nreturn &new->tx;\r\nfail:\r\nif (!first)\r\nreturn NULL;\r\nxgene_dma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *xgene_dma_prep_xor(\r\nstruct dma_chan *dchan, dma_addr_t dst, dma_addr_t *src,\r\nu32 src_cnt, size_t len, unsigned long flags)\r\n{\r\nstruct xgene_dma_desc_sw *first = NULL, *new;\r\nstruct xgene_dma_chan *chan;\r\nstatic u8 multi[XGENE_DMA_MAX_XOR_SRC] = {\r\n0x01, 0x01, 0x01, 0x01, 0x01};\r\nif (unlikely(!dchan || !len))\r\nreturn NULL;\r\nchan = to_dma_chan(dchan);\r\ndo {\r\nnew = xgene_dma_alloc_descriptor(chan);\r\nif (!new)\r\ngoto fail;\r\nxgene_dma_prep_xor_desc(chan, new, &dst, src,\r\nsrc_cnt, &len, multi);\r\nif (!first)\r\nfirst = new;\r\nnew->tx.cookie = 0;\r\nasync_tx_ack(&new->tx);\r\nlist_add_tail(&new->node, &first->tx_list);\r\n} while (len);\r\nnew->tx.flags = flags;\r\nnew->tx.cookie = -EBUSY;\r\nlist_splice(&first->tx_list, &new->tx_list);\r\nreturn &new->tx;\r\nfail:\r\nif (!first)\r\nreturn NULL;\r\nxgene_dma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *xgene_dma_prep_pq(\r\nstruct dma_chan *dchan, dma_addr_t *dst, dma_addr_t *src,\r\nu32 src_cnt, const u8 *scf, size_t len, unsigned long flags)\r\n{\r\nstruct xgene_dma_desc_sw *first = NULL, *new;\r\nstruct xgene_dma_chan *chan;\r\nsize_t _len = len;\r\ndma_addr_t _src[XGENE_DMA_MAX_XOR_SRC];\r\nstatic u8 multi[XGENE_DMA_MAX_XOR_SRC] = {0x01, 0x01, 0x01, 0x01, 0x01};\r\nif (unlikely(!dchan || !len))\r\nreturn NULL;\r\nchan = to_dma_chan(dchan);\r\nmemcpy(_src, src, sizeof(*src) * src_cnt);\r\nif (flags & DMA_PREP_PQ_DISABLE_P)\r\nlen = 0;\r\nif (flags & DMA_PREP_PQ_DISABLE_Q)\r\n_len = 0;\r\ndo {\r\nnew = xgene_dma_alloc_descriptor(chan);\r\nif (!new)\r\ngoto fail;\r\nif (!first)\r\nfirst = new;\r\nnew->tx.cookie = 0;\r\nasync_tx_ack(&new->tx);\r\nlist_add_tail(&new->node, &first->tx_list);\r\nif (len) {\r\nxgene_dma_prep_xor_desc(chan, new, &dst[0], src,\r\nsrc_cnt, &len, multi);\r\ncontinue;\r\n}\r\nif (_len) {\r\nxgene_dma_prep_xor_desc(chan, new, &dst[1], _src,\r\nsrc_cnt, &_len, scf);\r\n}\r\n} while (len || _len);\r\nnew->tx.flags = flags;\r\nnew->tx.cookie = -EBUSY;\r\nlist_splice(&first->tx_list, &new->tx_list);\r\nreturn &new->tx;\r\nfail:\r\nif (!first)\r\nreturn NULL;\r\nxgene_dma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic void xgene_dma_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct xgene_dma_chan *chan = to_dma_chan(dchan);\r\nspin_lock_bh(&chan->lock);\r\nxgene_chan_xfer_ld_pending(chan);\r\nspin_unlock_bh(&chan->lock);\r\n}\r\nstatic enum dma_status xgene_dma_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nreturn dma_cookie_status(dchan, cookie, txstate);\r\n}\r\nstatic void xgene_dma_tasklet_cb(unsigned long data)\r\n{\r\nstruct xgene_dma_chan *chan = (struct xgene_dma_chan *)data;\r\nxgene_dma_cleanup_descriptors(chan);\r\nenable_irq(chan->rx_irq);\r\n}\r\nstatic irqreturn_t xgene_dma_chan_ring_isr(int irq, void *id)\r\n{\r\nstruct xgene_dma_chan *chan = (struct xgene_dma_chan *)id;\r\nBUG_ON(!chan);\r\ndisable_irq_nosync(chan->rx_irq);\r\ntasklet_schedule(&chan->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t xgene_dma_err_isr(int irq, void *id)\r\n{\r\nstruct xgene_dma *pdma = (struct xgene_dma *)id;\r\nunsigned long int_mask;\r\nu32 val, i;\r\nval = ioread32(pdma->csr_dma + XGENE_DMA_INT);\r\niowrite32(val, pdma->csr_dma + XGENE_DMA_INT);\r\nint_mask = val >> XGENE_DMA_INT_MASK_SHIFT;\r\nfor_each_set_bit(i, &int_mask, ARRAY_SIZE(xgene_dma_err))\r\ndev_err(pdma->dev,\r\n"Interrupt status 0x%08X %s\n", val, xgene_dma_err[i]);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void xgene_dma_wr_ring_state(struct xgene_dma_ring *ring)\r\n{\r\nint i;\r\niowrite32(ring->num, ring->pdma->csr_ring + XGENE_DMA_RING_STATE);\r\nfor (i = 0; i < XGENE_DMA_RING_NUM_CONFIG; i++)\r\niowrite32(ring->state[i], ring->pdma->csr_ring +\r\nXGENE_DMA_RING_STATE_WR_BASE + (i * 4));\r\n}\r\nstatic void xgene_dma_clr_ring_state(struct xgene_dma_ring *ring)\r\n{\r\nmemset(ring->state, 0, sizeof(u32) * XGENE_DMA_RING_NUM_CONFIG);\r\nxgene_dma_wr_ring_state(ring);\r\n}\r\nstatic void xgene_dma_setup_ring(struct xgene_dma_ring *ring)\r\n{\r\nvoid *ring_cfg = ring->state;\r\nu64 addr = ring->desc_paddr;\r\nu32 i, val;\r\nring->slots = ring->size / XGENE_DMA_RING_WQ_DESC_SIZE;\r\nxgene_dma_clr_ring_state(ring);\r\nXGENE_DMA_RING_TYPE_SET(ring_cfg, XGENE_DMA_RING_TYPE_REGULAR);\r\nif (ring->owner == XGENE_DMA_RING_OWNER_DMA) {\r\nXGENE_DMA_RING_RECOMBBUF_SET(ring_cfg);\r\nXGENE_DMA_RING_RECOMTIMEOUTL_SET(ring_cfg);\r\nXGENE_DMA_RING_RECOMTIMEOUTH_SET(ring_cfg);\r\n}\r\nXGENE_DMA_RING_SELTHRSH_SET(ring_cfg);\r\nXGENE_DMA_RING_ACCEPTLERR_SET(ring_cfg);\r\nXGENE_DMA_RING_COHERENT_SET(ring_cfg);\r\nXGENE_DMA_RING_ADDRL_SET(ring_cfg, addr);\r\nXGENE_DMA_RING_ADDRH_SET(ring_cfg, addr);\r\nXGENE_DMA_RING_SIZE_SET(ring_cfg, ring->cfgsize);\r\nxgene_dma_wr_ring_state(ring);\r\niowrite32(XGENE_DMA_RING_ID_SETUP(ring->id),\r\nring->pdma->csr_ring + XGENE_DMA_RING_ID);\r\niowrite32(XGENE_DMA_RING_ID_BUF_SETUP(ring->num),\r\nring->pdma->csr_ring + XGENE_DMA_RING_ID_BUF);\r\nif (ring->owner != XGENE_DMA_RING_OWNER_CPU)\r\nreturn;\r\nfor (i = 0; i < ring->slots; i++) {\r\nstruct xgene_dma_desc_hw *desc;\r\ndesc = &ring->desc_hw[i];\r\ndesc->m0 = cpu_to_le64(XGENE_DMA_DESC_EMPTY_SIGNATURE);\r\n}\r\nval = ioread32(ring->pdma->csr_ring + XGENE_DMA_RING_NE_INT_MODE);\r\nXGENE_DMA_RING_NE_INT_MODE_SET(val, ring->buf_num);\r\niowrite32(val, ring->pdma->csr_ring + XGENE_DMA_RING_NE_INT_MODE);\r\n}\r\nstatic void xgene_dma_clear_ring(struct xgene_dma_ring *ring)\r\n{\r\nu32 ring_id, val;\r\nif (ring->owner == XGENE_DMA_RING_OWNER_CPU) {\r\nval = ioread32(ring->pdma->csr_ring +\r\nXGENE_DMA_RING_NE_INT_MODE);\r\nXGENE_DMA_RING_NE_INT_MODE_RESET(val, ring->buf_num);\r\niowrite32(val, ring->pdma->csr_ring +\r\nXGENE_DMA_RING_NE_INT_MODE);\r\n}\r\nring_id = XGENE_DMA_RING_ID_SETUP(ring->id);\r\niowrite32(ring_id, ring->pdma->csr_ring + XGENE_DMA_RING_ID);\r\niowrite32(0, ring->pdma->csr_ring + XGENE_DMA_RING_ID_BUF);\r\nxgene_dma_clr_ring_state(ring);\r\n}\r\nstatic void xgene_dma_set_ring_cmd(struct xgene_dma_ring *ring)\r\n{\r\nring->cmd_base = ring->pdma->csr_ring_cmd +\r\nXGENE_DMA_RING_CMD_BASE_OFFSET((ring->num -\r\nXGENE_DMA_RING_NUM));\r\nring->cmd = ring->cmd_base + XGENE_DMA_RING_CMD_OFFSET;\r\n}\r\nstatic int xgene_dma_get_ring_size(struct xgene_dma_chan *chan,\r\nenum xgene_dma_ring_cfgsize cfgsize)\r\n{\r\nint size;\r\nswitch (cfgsize) {\r\ncase XGENE_DMA_RING_CFG_SIZE_512B:\r\nsize = 0x200;\r\nbreak;\r\ncase XGENE_DMA_RING_CFG_SIZE_2KB:\r\nsize = 0x800;\r\nbreak;\r\ncase XGENE_DMA_RING_CFG_SIZE_16KB:\r\nsize = 0x4000;\r\nbreak;\r\ncase XGENE_DMA_RING_CFG_SIZE_64KB:\r\nsize = 0x10000;\r\nbreak;\r\ncase XGENE_DMA_RING_CFG_SIZE_512KB:\r\nsize = 0x80000;\r\nbreak;\r\ndefault:\r\nchan_err(chan, "Unsupported cfg ring size %d\n", cfgsize);\r\nreturn -EINVAL;\r\n}\r\nreturn size;\r\n}\r\nstatic void xgene_dma_delete_ring_one(struct xgene_dma_ring *ring)\r\n{\r\nxgene_dma_clear_ring(ring);\r\nif (ring->desc_vaddr) {\r\ndma_free_coherent(ring->pdma->dev, ring->size,\r\nring->desc_vaddr, ring->desc_paddr);\r\nring->desc_vaddr = NULL;\r\n}\r\n}\r\nstatic void xgene_dma_delete_chan_rings(struct xgene_dma_chan *chan)\r\n{\r\nxgene_dma_delete_ring_one(&chan->rx_ring);\r\nxgene_dma_delete_ring_one(&chan->tx_ring);\r\n}\r\nstatic int xgene_dma_create_ring_one(struct xgene_dma_chan *chan,\r\nstruct xgene_dma_ring *ring,\r\nenum xgene_dma_ring_cfgsize cfgsize)\r\n{\r\nint ret;\r\nring->pdma = chan->pdma;\r\nring->cfgsize = cfgsize;\r\nring->num = chan->pdma->ring_num++;\r\nring->id = XGENE_DMA_RING_ID_GET(ring->owner, ring->buf_num);\r\nret = xgene_dma_get_ring_size(chan, cfgsize);\r\nif (ret <= 0)\r\nreturn ret;\r\nring->size = ret;\r\nring->desc_vaddr = dma_zalloc_coherent(chan->dev, ring->size,\r\n&ring->desc_paddr, GFP_KERNEL);\r\nif (!ring->desc_vaddr) {\r\nchan_err(chan, "Failed to allocate ring desc\n");\r\nreturn -ENOMEM;\r\n}\r\nxgene_dma_set_ring_cmd(ring);\r\nxgene_dma_setup_ring(ring);\r\nreturn 0;\r\n}\r\nstatic int xgene_dma_create_chan_rings(struct xgene_dma_chan *chan)\r\n{\r\nstruct xgene_dma_ring *rx_ring = &chan->rx_ring;\r\nstruct xgene_dma_ring *tx_ring = &chan->tx_ring;\r\nint ret;\r\nrx_ring->owner = XGENE_DMA_RING_OWNER_CPU;\r\nrx_ring->buf_num = XGENE_DMA_CPU_BUFNUM + chan->id;\r\nret = xgene_dma_create_ring_one(chan, rx_ring,\r\nXGENE_DMA_RING_CFG_SIZE_64KB);\r\nif (ret)\r\nreturn ret;\r\nchan_dbg(chan, "Rx ring id 0x%X num %d desc 0x%p\n",\r\nrx_ring->id, rx_ring->num, rx_ring->desc_vaddr);\r\ntx_ring->owner = XGENE_DMA_RING_OWNER_DMA;\r\ntx_ring->buf_num = XGENE_DMA_BUFNUM + chan->id;\r\nret = xgene_dma_create_ring_one(chan, tx_ring,\r\nXGENE_DMA_RING_CFG_SIZE_64KB);\r\nif (ret) {\r\nxgene_dma_delete_ring_one(rx_ring);\r\nreturn ret;\r\n}\r\ntx_ring->dst_ring_num = XGENE_DMA_RING_DST_ID(rx_ring->num);\r\nchan_dbg(chan,\r\n"Tx ring id 0x%X num %d desc 0x%p\n",\r\ntx_ring->id, tx_ring->num, tx_ring->desc_vaddr);\r\nchan->max_outstanding = tx_ring->slots;\r\nreturn ret;\r\n}\r\nstatic int xgene_dma_init_rings(struct xgene_dma *pdma)\r\n{\r\nint ret, i, j;\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\r\nret = xgene_dma_create_chan_rings(&pdma->chan[i]);\r\nif (ret) {\r\nfor (j = 0; j < i; j++)\r\nxgene_dma_delete_chan_rings(&pdma->chan[j]);\r\nreturn ret;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void xgene_dma_enable(struct xgene_dma *pdma)\r\n{\r\nu32 val;\r\nval = ioread32(pdma->csr_dma + XGENE_DMA_GCR);\r\nXGENE_DMA_CH_SETUP(val);\r\nXGENE_DMA_ENABLE(val);\r\niowrite32(val, pdma->csr_dma + XGENE_DMA_GCR);\r\n}\r\nstatic void xgene_dma_disable(struct xgene_dma *pdma)\r\n{\r\nu32 val;\r\nval = ioread32(pdma->csr_dma + XGENE_DMA_GCR);\r\nXGENE_DMA_DISABLE(val);\r\niowrite32(val, pdma->csr_dma + XGENE_DMA_GCR);\r\n}\r\nstatic void xgene_dma_mask_interrupts(struct xgene_dma *pdma)\r\n{\r\niowrite32(XGENE_DMA_INT_ALL_MASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT0_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_MASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT1_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_MASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT2_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_MASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT3_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_MASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT4_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_MASK, pdma->csr_dma + XGENE_DMA_INT_MASK);\r\n}\r\nstatic void xgene_dma_unmask_interrupts(struct xgene_dma *pdma)\r\n{\r\niowrite32(XGENE_DMA_INT_ALL_UNMASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT0_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_UNMASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT1_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_UNMASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT2_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_UNMASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT3_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_UNMASK,\r\npdma->csr_dma + XGENE_DMA_RING_INT4_MASK);\r\niowrite32(XGENE_DMA_INT_ALL_UNMASK,\r\npdma->csr_dma + XGENE_DMA_INT_MASK);\r\n}\r\nstatic void xgene_dma_init_hw(struct xgene_dma *pdma)\r\n{\r\nu32 val;\r\niowrite32(XGENE_DMA_ASSOC_RING_MNGR1,\r\npdma->csr_dma + XGENE_DMA_CFG_RING_WQ_ASSOC);\r\nif (is_pq_enabled(pdma))\r\niowrite32(XGENE_DMA_RAID6_MULTI_CTRL(0x1D),\r\npdma->csr_dma + XGENE_DMA_RAID6_CONT);\r\nelse\r\ndev_info(pdma->dev, "PQ is disabled in HW\n");\r\nxgene_dma_enable(pdma);\r\nxgene_dma_unmask_interrupts(pdma);\r\nval = ioread32(pdma->csr_dma + XGENE_DMA_IPBRR);\r\ndev_info(pdma->dev,\r\n"X-Gene DMA v%d.%02d.%02d driver registered %d channels",\r\nXGENE_DMA_REV_NO_RD(val), XGENE_DMA_BUS_ID_RD(val),\r\nXGENE_DMA_DEV_ID_RD(val), XGENE_DMA_MAX_CHANNEL);\r\n}\r\nstatic int xgene_dma_init_ring_mngr(struct xgene_dma *pdma)\r\n{\r\nif (ioread32(pdma->csr_ring + XGENE_DMA_RING_CLKEN) &&\r\n(!ioread32(pdma->csr_ring + XGENE_DMA_RING_SRST)))\r\nreturn 0;\r\niowrite32(0x3, pdma->csr_ring + XGENE_DMA_RING_CLKEN);\r\niowrite32(0x0, pdma->csr_ring + XGENE_DMA_RING_SRST);\r\niowrite32(0x0, pdma->csr_ring + XGENE_DMA_RING_MEM_RAM_SHUTDOWN);\r\nioread32(pdma->csr_ring + XGENE_DMA_RING_MEM_RAM_SHUTDOWN);\r\nusleep_range(1000, 1100);\r\nif (ioread32(pdma->csr_ring + XGENE_DMA_RING_BLK_MEM_RDY)\r\n!= XGENE_DMA_RING_BLK_MEM_RDY_VAL) {\r\ndev_err(pdma->dev,\r\n"Failed to release ring mngr memory from shutdown\n");\r\nreturn -ENODEV;\r\n}\r\niowrite32(XGENE_DMA_RING_THRESLD0_SET1_VAL,\r\npdma->csr_ring + XGENE_DMA_RING_THRESLD0_SET1);\r\niowrite32(XGENE_DMA_RING_THRESLD1_SET1_VAL,\r\npdma->csr_ring + XGENE_DMA_RING_THRESLD1_SET1);\r\niowrite32(XGENE_DMA_RING_HYSTERESIS_VAL,\r\npdma->csr_ring + XGENE_DMA_RING_HYSTERESIS);\r\niowrite32(XGENE_DMA_RING_ENABLE,\r\npdma->csr_ring + XGENE_DMA_RING_CONFIG);\r\nreturn 0;\r\n}\r\nstatic int xgene_dma_init_mem(struct xgene_dma *pdma)\r\n{\r\nint ret;\r\nret = xgene_dma_init_ring_mngr(pdma);\r\nif (ret)\r\nreturn ret;\r\niowrite32(0x0, pdma->csr_dma + XGENE_DMA_MEM_RAM_SHUTDOWN);\r\nioread32(pdma->csr_dma + XGENE_DMA_MEM_RAM_SHUTDOWN);\r\nusleep_range(1000, 1100);\r\nif (ioread32(pdma->csr_dma + XGENE_DMA_BLK_MEM_RDY)\r\n!= XGENE_DMA_BLK_MEM_RDY_VAL) {\r\ndev_err(pdma->dev,\r\n"Failed to release DMA memory from shutdown\n");\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic int xgene_dma_request_irqs(struct xgene_dma *pdma)\r\n{\r\nstruct xgene_dma_chan *chan;\r\nint ret, i, j;\r\nret = devm_request_irq(pdma->dev, pdma->err_irq, xgene_dma_err_isr,\r\n0, "dma_error", pdma);\r\nif (ret) {\r\ndev_err(pdma->dev,\r\n"Failed to register error IRQ %d\n", pdma->err_irq);\r\nreturn ret;\r\n}\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\r\nchan = &pdma->chan[i];\r\nirq_set_status_flags(chan->rx_irq, IRQ_DISABLE_UNLAZY);\r\nret = devm_request_irq(chan->dev, chan->rx_irq,\r\nxgene_dma_chan_ring_isr,\r\n0, chan->name, chan);\r\nif (ret) {\r\nchan_err(chan, "Failed to register Rx IRQ %d\n",\r\nchan->rx_irq);\r\ndevm_free_irq(pdma->dev, pdma->err_irq, pdma);\r\nfor (j = 0; j < i; j++) {\r\nchan = &pdma->chan[i];\r\nirq_clear_status_flags(chan->rx_irq, IRQ_DISABLE_UNLAZY);\r\ndevm_free_irq(chan->dev, chan->rx_irq, chan);\r\n}\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void xgene_dma_free_irqs(struct xgene_dma *pdma)\r\n{\r\nstruct xgene_dma_chan *chan;\r\nint i;\r\ndevm_free_irq(pdma->dev, pdma->err_irq, pdma);\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\r\nchan = &pdma->chan[i];\r\nirq_clear_status_flags(chan->rx_irq, IRQ_DISABLE_UNLAZY);\r\ndevm_free_irq(chan->dev, chan->rx_irq, chan);\r\n}\r\n}\r\nstatic void xgene_dma_set_caps(struct xgene_dma_chan *chan,\r\nstruct dma_device *dma_dev)\r\n{\r\ndma_cap_zero(dma_dev->cap_mask);\r\ndma_cap_set(DMA_SG, dma_dev->cap_mask);\r\nif ((chan->id == XGENE_DMA_PQ_CHANNEL) &&\r\nis_pq_enabled(chan->pdma)) {\r\ndma_cap_set(DMA_PQ, dma_dev->cap_mask);\r\ndma_cap_set(DMA_XOR, dma_dev->cap_mask);\r\n} else if ((chan->id == XGENE_DMA_XOR_CHANNEL) &&\r\n!is_pq_enabled(chan->pdma)) {\r\ndma_cap_set(DMA_XOR, dma_dev->cap_mask);\r\n}\r\ndma_dev->dev = chan->dev;\r\ndma_dev->device_alloc_chan_resources = xgene_dma_alloc_chan_resources;\r\ndma_dev->device_free_chan_resources = xgene_dma_free_chan_resources;\r\ndma_dev->device_issue_pending = xgene_dma_issue_pending;\r\ndma_dev->device_tx_status = xgene_dma_tx_status;\r\ndma_dev->device_prep_dma_sg = xgene_dma_prep_sg;\r\nif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\r\ndma_dev->device_prep_dma_xor = xgene_dma_prep_xor;\r\ndma_dev->max_xor = XGENE_DMA_MAX_XOR_SRC;\r\ndma_dev->xor_align = DMAENGINE_ALIGN_64_BYTES;\r\n}\r\nif (dma_has_cap(DMA_PQ, dma_dev->cap_mask)) {\r\ndma_dev->device_prep_dma_pq = xgene_dma_prep_pq;\r\ndma_dev->max_pq = XGENE_DMA_MAX_XOR_SRC;\r\ndma_dev->pq_align = DMAENGINE_ALIGN_64_BYTES;\r\n}\r\n}\r\nstatic int xgene_dma_async_register(struct xgene_dma *pdma, int id)\r\n{\r\nstruct xgene_dma_chan *chan = &pdma->chan[id];\r\nstruct dma_device *dma_dev = &pdma->dma_dev[id];\r\nint ret;\r\nchan->dma_chan.device = dma_dev;\r\nspin_lock_init(&chan->lock);\r\nINIT_LIST_HEAD(&chan->ld_pending);\r\nINIT_LIST_HEAD(&chan->ld_running);\r\nINIT_LIST_HEAD(&chan->ld_completed);\r\ntasklet_init(&chan->tasklet, xgene_dma_tasklet_cb,\r\n(unsigned long)chan);\r\nchan->pending = 0;\r\nchan->desc_pool = NULL;\r\ndma_cookie_init(&chan->dma_chan);\r\nxgene_dma_set_caps(chan, dma_dev);\r\nINIT_LIST_HEAD(&dma_dev->channels);\r\nlist_add_tail(&chan->dma_chan.device_node, &dma_dev->channels);\r\nret = dma_async_device_register(dma_dev);\r\nif (ret) {\r\nchan_err(chan, "Failed to register async device %d", ret);\r\ntasklet_kill(&chan->tasklet);\r\nreturn ret;\r\n}\r\ndev_info(pdma->dev,\r\n"%s: CAPABILITY ( %s%s%s)\n", dma_chan_name(&chan->dma_chan),\r\ndma_has_cap(DMA_SG, dma_dev->cap_mask) ? "SGCPY " : "",\r\ndma_has_cap(DMA_XOR, dma_dev->cap_mask) ? "XOR " : "",\r\ndma_has_cap(DMA_PQ, dma_dev->cap_mask) ? "PQ " : "");\r\nreturn 0;\r\n}\r\nstatic int xgene_dma_init_async(struct xgene_dma *pdma)\r\n{\r\nint ret, i, j;\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL ; i++) {\r\nret = xgene_dma_async_register(pdma, i);\r\nif (ret) {\r\nfor (j = 0; j < i; j++) {\r\ndma_async_device_unregister(&pdma->dma_dev[j]);\r\ntasklet_kill(&pdma->chan[j].tasklet);\r\n}\r\nreturn ret;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void xgene_dma_async_unregister(struct xgene_dma *pdma)\r\n{\r\nint i;\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++)\r\ndma_async_device_unregister(&pdma->dma_dev[i]);\r\n}\r\nstatic void xgene_dma_init_channels(struct xgene_dma *pdma)\r\n{\r\nstruct xgene_dma_chan *chan;\r\nint i;\r\npdma->ring_num = XGENE_DMA_RING_NUM;\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\r\nchan = &pdma->chan[i];\r\nchan->dev = pdma->dev;\r\nchan->pdma = pdma;\r\nchan->id = i;\r\nsnprintf(chan->name, sizeof(chan->name), "dmachan%d", chan->id);\r\n}\r\n}\r\nstatic int xgene_dma_get_resources(struct platform_device *pdev,\r\nstruct xgene_dma *pdma)\r\n{\r\nstruct resource *res;\r\nint irq, i;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res) {\r\ndev_err(&pdev->dev, "Failed to get csr region\n");\r\nreturn -ENXIO;\r\n}\r\npdma->csr_dma = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!pdma->csr_dma) {\r\ndev_err(&pdev->dev, "Failed to ioremap csr region");\r\nreturn -ENOMEM;\r\n}\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (!res) {\r\ndev_err(&pdev->dev, "Failed to get ring csr region\n");\r\nreturn -ENXIO;\r\n}\r\npdma->csr_ring = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!pdma->csr_ring) {\r\ndev_err(&pdev->dev, "Failed to ioremap ring csr region");\r\nreturn -ENOMEM;\r\n}\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 2);\r\nif (!res) {\r\ndev_err(&pdev->dev, "Failed to get ring cmd csr region\n");\r\nreturn -ENXIO;\r\n}\r\npdma->csr_ring_cmd = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!pdma->csr_ring_cmd) {\r\ndev_err(&pdev->dev, "Failed to ioremap ring cmd csr region");\r\nreturn -ENOMEM;\r\n}\r\npdma->csr_ring_cmd += XGENE_DMA_RING_CMD_SM_OFFSET;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 3);\r\nif (!res) {\r\ndev_err(&pdev->dev, "Failed to get efuse csr region\n");\r\nreturn -ENXIO;\r\n}\r\npdma->csr_efuse = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!pdma->csr_efuse) {\r\ndev_err(&pdev->dev, "Failed to ioremap efuse csr region");\r\nreturn -ENOMEM;\r\n}\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq <= 0) {\r\ndev_err(&pdev->dev, "Failed to get Error IRQ\n");\r\nreturn -ENXIO;\r\n}\r\npdma->err_irq = irq;\r\nfor (i = 1; i <= XGENE_DMA_MAX_CHANNEL; i++) {\r\nirq = platform_get_irq(pdev, i);\r\nif (irq <= 0) {\r\ndev_err(&pdev->dev, "Failed to get Rx IRQ\n");\r\nreturn -ENXIO;\r\n}\r\npdma->chan[i - 1].rx_irq = irq;\r\n}\r\nreturn 0;\r\n}\r\nstatic int xgene_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct xgene_dma *pdma;\r\nint ret, i;\r\npdma = devm_kzalloc(&pdev->dev, sizeof(*pdma), GFP_KERNEL);\r\nif (!pdma)\r\nreturn -ENOMEM;\r\npdma->dev = &pdev->dev;\r\nplatform_set_drvdata(pdev, pdma);\r\nret = xgene_dma_get_resources(pdev, pdma);\r\nif (ret)\r\nreturn ret;\r\npdma->clk = devm_clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(pdma->clk) && !ACPI_COMPANION(&pdev->dev)) {\r\ndev_err(&pdev->dev, "Failed to get clk\n");\r\nreturn PTR_ERR(pdma->clk);\r\n}\r\nif (!IS_ERR(pdma->clk)) {\r\nret = clk_prepare_enable(pdma->clk);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Failed to enable clk %d\n", ret);\r\nreturn ret;\r\n}\r\n}\r\nret = xgene_dma_init_mem(pdma);\r\nif (ret)\r\ngoto err_clk_enable;\r\nret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(42));\r\nif (ret) {\r\ndev_err(&pdev->dev, "No usable DMA configuration\n");\r\ngoto err_dma_mask;\r\n}\r\nxgene_dma_init_channels(pdma);\r\nret = xgene_dma_init_rings(pdma);\r\nif (ret)\r\ngoto err_clk_enable;\r\nret = xgene_dma_request_irqs(pdma);\r\nif (ret)\r\ngoto err_request_irq;\r\nxgene_dma_init_hw(pdma);\r\nret = xgene_dma_init_async(pdma);\r\nif (ret)\r\ngoto err_async_init;\r\nreturn 0;\r\nerr_async_init:\r\nxgene_dma_free_irqs(pdma);\r\nerr_request_irq:\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++)\r\nxgene_dma_delete_chan_rings(&pdma->chan[i]);\r\nerr_dma_mask:\r\nerr_clk_enable:\r\nif (!IS_ERR(pdma->clk))\r\nclk_disable_unprepare(pdma->clk);\r\nreturn ret;\r\n}\r\nstatic int xgene_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct xgene_dma *pdma = platform_get_drvdata(pdev);\r\nstruct xgene_dma_chan *chan;\r\nint i;\r\nxgene_dma_async_unregister(pdma);\r\nxgene_dma_mask_interrupts(pdma);\r\nxgene_dma_disable(pdma);\r\nxgene_dma_free_irqs(pdma);\r\nfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\r\nchan = &pdma->chan[i];\r\ntasklet_kill(&chan->tasklet);\r\nxgene_dma_delete_chan_rings(chan);\r\n}\r\nif (!IS_ERR(pdma->clk))\r\nclk_disable_unprepare(pdma->clk);\r\nreturn 0;\r\n}
