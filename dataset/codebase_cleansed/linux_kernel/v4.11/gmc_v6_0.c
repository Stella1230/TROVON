static void gmc_v6_0_mc_stop(struct amdgpu_device *adev,\r\nstruct amdgpu_mode_mc_save *save)\r\n{\r\nu32 blackout;\r\nif (adev->mode_info.num_crtc)\r\namdgpu_display_stop_mc_access(adev, save);\r\ngmc_v6_0_wait_for_idle((void *)adev);\r\nblackout = RREG32(mmMC_SHARED_BLACKOUT_CNTL);\r\nif (REG_GET_FIELD(blackout, MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE) != 1) {\r\nWREG32(mmBIF_FB_EN, 0);\r\nblackout = REG_SET_FIELD(blackout,\r\nMC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE, 0);\r\nWREG32(mmMC_SHARED_BLACKOUT_CNTL, blackout | 1);\r\n}\r\nudelay(100);\r\n}\r\nstatic void gmc_v6_0_mc_resume(struct amdgpu_device *adev,\r\nstruct amdgpu_mode_mc_save *save)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(mmMC_SHARED_BLACKOUT_CNTL);\r\ntmp = REG_SET_FIELD(tmp, MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE, 0);\r\nWREG32(mmMC_SHARED_BLACKOUT_CNTL, tmp);\r\ntmp = REG_SET_FIELD(0, BIF_FB_EN, FB_READ_EN, 1);\r\ntmp = REG_SET_FIELD(tmp, BIF_FB_EN, FB_WRITE_EN, 1);\r\nWREG32(mmBIF_FB_EN, tmp);\r\nif (adev->mode_info.num_crtc)\r\namdgpu_display_resume_mc_access(adev, save);\r\n}\r\nstatic int gmc_v6_0_init_microcode(struct amdgpu_device *adev)\r\n{\r\nconst char *chip_name;\r\nchar fw_name[30];\r\nint err;\r\nbool is_58_fw = false;\r\nDRM_DEBUG("\n");\r\nswitch (adev->asic_type) {\r\ncase CHIP_TAHITI:\r\nchip_name = "tahiti";\r\nbreak;\r\ncase CHIP_PITCAIRN:\r\nchip_name = "pitcairn";\r\nbreak;\r\ncase CHIP_VERDE:\r\nchip_name = "verde";\r\nbreak;\r\ncase CHIP_OLAND:\r\nchip_name = "oland";\r\nbreak;\r\ncase CHIP_HAINAN:\r\nchip_name = "hainan";\r\nbreak;\r\ndefault: BUG();\r\n}\r\nif (((RREG32(mmMC_SEQ_MISC0) & 0xff000000) >> 24) == 0x58)\r\nis_58_fw = true;\r\nif (is_58_fw)\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/si58_mc.bin");\r\nelse\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc.bin", chip_name);\r\nerr = request_firmware(&adev->mc.fw, fw_name, adev->dev);\r\nif (err)\r\ngoto out;\r\nerr = amdgpu_ucode_validate(adev->mc.fw);\r\nout:\r\nif (err) {\r\ndev_err(adev->dev,\r\n"si_mc: Failed to load firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(adev->mc.fw);\r\nadev->mc.fw = NULL;\r\n}\r\nreturn err;\r\n}\r\nstatic int gmc_v6_0_mc_load_microcode(struct amdgpu_device *adev)\r\n{\r\nconst __le32 *new_fw_data = NULL;\r\nu32 running;\r\nconst __le32 *new_io_mc_regs = NULL;\r\nint i, regs_size, ucode_size;\r\nconst struct mc_firmware_header_v1_0 *hdr;\r\nif (!adev->mc.fw)\r\nreturn -EINVAL;\r\nhdr = (const struct mc_firmware_header_v1_0 *)adev->mc.fw->data;\r\namdgpu_ucode_print_mc_hdr(&hdr->header);\r\nadev->mc.fw_version = le32_to_cpu(hdr->header.ucode_version);\r\nregs_size = le32_to_cpu(hdr->io_debug_size_bytes) / (4 * 2);\r\nnew_io_mc_regs = (const __le32 *)\r\n(adev->mc.fw->data + le32_to_cpu(hdr->io_debug_array_offset_bytes));\r\nucode_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\r\nnew_fw_data = (const __le32 *)\r\n(adev->mc.fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\nrunning = RREG32(mmMC_SEQ_SUP_CNTL) & MC_SEQ_SUP_CNTL__RUN_MASK;\r\nif (running == 0) {\r\nWREG32(mmMC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(mmMC_SEQ_SUP_CNTL, 0x00000010);\r\nfor (i = 0; i < regs_size; i++) {\r\nWREG32(mmMC_SEQ_IO_DEBUG_INDEX, le32_to_cpup(new_io_mc_regs++));\r\nWREG32(mmMC_SEQ_IO_DEBUG_DATA, le32_to_cpup(new_io_mc_regs++));\r\n}\r\nfor (i = 0; i < ucode_size; i++) {\r\nWREG32(mmMC_SEQ_SUP_PGM, le32_to_cpup(new_fw_data++));\r\n}\r\nWREG32(mmMC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(mmMC_SEQ_SUP_CNTL, 0x00000004);\r\nWREG32(mmMC_SEQ_SUP_CNTL, 0x00000001);\r\nfor (i = 0; i < adev->usec_timeout; i++) {\r\nif (RREG32(mmMC_SEQ_TRAIN_WAKEUP_CNTL) & MC_SEQ_TRAIN_WAKEUP_CNTL__TRAIN_DONE_D0_MASK)\r\nbreak;\r\nudelay(1);\r\n}\r\nfor (i = 0; i < adev->usec_timeout; i++) {\r\nif (RREG32(mmMC_SEQ_TRAIN_WAKEUP_CNTL) & MC_SEQ_TRAIN_WAKEUP_CNTL__TRAIN_DONE_D1_MASK)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void gmc_v6_0_vram_gtt_location(struct amdgpu_device *adev,\r\nstruct amdgpu_mc *mc)\r\n{\r\nif (mc->mc_vram_size > 0xFFC0000000ULL) {\r\ndev_warn(adev->dev, "limiting VRAM\n");\r\nmc->real_vram_size = 0xFFC0000000ULL;\r\nmc->mc_vram_size = 0xFFC0000000ULL;\r\n}\r\namdgpu_vram_location(adev, &adev->mc, 0);\r\nadev->mc.gtt_base_align = 0;\r\namdgpu_gtt_location(adev, mc);\r\n}\r\nstatic void gmc_v6_0_mc_program(struct amdgpu_device *adev)\r\n{\r\nstruct amdgpu_mode_mc_save save;\r\nu32 tmp;\r\nint i, j;\r\nfor (i = 0, j = 0; i < 32; i++, j += 0x6) {\r\nWREG32((0xb05 + j), 0x00000000);\r\nWREG32((0xb06 + j), 0x00000000);\r\nWREG32((0xb07 + j), 0x00000000);\r\nWREG32((0xb08 + j), 0x00000000);\r\nWREG32((0xb09 + j), 0x00000000);\r\n}\r\nWREG32(mmHDP_REG_COHERENCY_FLUSH_CNTL, 0);\r\nif (adev->mode_info.num_crtc)\r\namdgpu_display_set_vga_render_state(adev, false);\r\ngmc_v6_0_mc_stop(adev, &save);\r\nif (gmc_v6_0_wait_for_idle((void *)adev)) {\r\ndev_warn(adev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nWREG32(mmVGA_HDP_CONTROL, VGA_HDP_CONTROL__VGA_MEMORY_DISABLE_MASK);\r\nWREG32(mmMC_VM_SYSTEM_APERTURE_LOW_ADDR,\r\nadev->mc.vram_start >> 12);\r\nWREG32(mmMC_VM_SYSTEM_APERTURE_HIGH_ADDR,\r\nadev->mc.vram_end >> 12);\r\nWREG32(mmMC_VM_SYSTEM_APERTURE_DEFAULT_ADDR,\r\nadev->vram_scratch.gpu_addr >> 12);\r\ntmp = ((adev->mc.vram_end >> 24) & 0xFFFF) << 16;\r\ntmp |= ((adev->mc.vram_start >> 24) & 0xFFFF);\r\nWREG32(mmMC_VM_FB_LOCATION, tmp);\r\nWREG32(mmHDP_NONSURFACE_BASE, (adev->mc.vram_start >> 8));\r\nWREG32(mmHDP_NONSURFACE_INFO, (2 << 7) | (1 << 30));\r\nWREG32(mmHDP_NONSURFACE_SIZE, 0x3FFFFFFF);\r\nWREG32(mmMC_VM_AGP_BASE, 0);\r\nWREG32(mmMC_VM_AGP_TOP, 0x0FFFFFFF);\r\nWREG32(mmMC_VM_AGP_BOT, 0x0FFFFFFF);\r\nif (gmc_v6_0_wait_for_idle((void *)adev)) {\r\ndev_warn(adev->dev, "Wait for MC idle timedout !\n");\r\n}\r\ngmc_v6_0_mc_resume(adev, &save);\r\n}\r\nstatic int gmc_v6_0_mc_init(struct amdgpu_device *adev)\r\n{\r\nu32 tmp;\r\nint chansize, numchan;\r\ntmp = RREG32(mmMC_ARB_RAMCFG);\r\nif (tmp & (1 << 11)) {\r\nchansize = 16;\r\n} else if (tmp & MC_ARB_RAMCFG__CHANSIZE_MASK) {\r\nchansize = 64;\r\n} else {\r\nchansize = 32;\r\n}\r\ntmp = RREG32(mmMC_SHARED_CHMAP);\r\nswitch ((tmp & MC_SHARED_CHMAP__NOOFCHAN_MASK) >> MC_SHARED_CHMAP__NOOFCHAN__SHIFT) {\r\ncase 0:\r\ndefault:\r\nnumchan = 1;\r\nbreak;\r\ncase 1:\r\nnumchan = 2;\r\nbreak;\r\ncase 2:\r\nnumchan = 4;\r\nbreak;\r\ncase 3:\r\nnumchan = 8;\r\nbreak;\r\ncase 4:\r\nnumchan = 3;\r\nbreak;\r\ncase 5:\r\nnumchan = 6;\r\nbreak;\r\ncase 6:\r\nnumchan = 10;\r\nbreak;\r\ncase 7:\r\nnumchan = 12;\r\nbreak;\r\ncase 8:\r\nnumchan = 16;\r\nbreak;\r\n}\r\nadev->mc.vram_width = numchan * chansize;\r\nadev->mc.aper_base = pci_resource_start(adev->pdev, 0);\r\nadev->mc.aper_size = pci_resource_len(adev->pdev, 0);\r\nadev->mc.mc_vram_size = RREG32(mmCONFIG_MEMSIZE) * 1024ULL * 1024ULL;\r\nadev->mc.real_vram_size = RREG32(mmCONFIG_MEMSIZE) * 1024ULL * 1024ULL;\r\nadev->mc.visible_vram_size = adev->mc.aper_size;\r\nif (amdgpu_gart_size == -1)\r\nadev->mc.gtt_size = max((1024ULL << 20), adev->mc.mc_vram_size);\r\nelse\r\nadev->mc.gtt_size = (uint64_t)amdgpu_gart_size << 20;\r\ngmc_v6_0_vram_gtt_location(adev, &adev->mc);\r\nreturn 0;\r\n}\r\nstatic void gmc_v6_0_gart_flush_gpu_tlb(struct amdgpu_device *adev,\r\nuint32_t vmid)\r\n{\r\nWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 0);\r\nWREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);\r\n}\r\nstatic int gmc_v6_0_gart_set_pte_pde(struct amdgpu_device *adev,\r\nvoid *cpu_pt_addr,\r\nuint32_t gpu_page_idx,\r\nuint64_t addr,\r\nuint32_t flags)\r\n{\r\nvoid __iomem *ptr = (void *)cpu_pt_addr;\r\nuint64_t value;\r\nvalue = addr & 0xFFFFFFFFFFFFF000ULL;\r\nvalue |= flags;\r\nwriteq(value, ptr + (gpu_page_idx * 8));\r\nreturn 0;\r\n}\r\nstatic void gmc_v6_0_set_fault_enable_default(struct amdgpu_device *adev,\r\nbool value)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(mmVM_CONTEXT1_CNTL);\r\ntmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\r\nRANGE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\r\ntmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\r\nDUMMY_PAGE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\r\ntmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\r\nPDE0_PROTECTION_FAULT_ENABLE_DEFAULT, value);\r\ntmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\r\nVALID_PROTECTION_FAULT_ENABLE_DEFAULT, value);\r\ntmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\r\nREAD_PROTECTION_FAULT_ENABLE_DEFAULT, value);\r\ntmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\r\nWRITE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\r\nWREG32(mmVM_CONTEXT1_CNTL, tmp);\r\n}\r\nstatic int gmc_v6_0_gart_enable(struct amdgpu_device *adev)\r\n{\r\nint r, i;\r\nif (adev->gart.robj == NULL) {\r\ndev_err(adev->dev, "No VRAM object for PCIE GART.\n");\r\nreturn -EINVAL;\r\n}\r\nr = amdgpu_gart_table_vram_pin(adev);\r\nif (r)\r\nreturn r;\r\nWREG32(mmMC_VM_MX_L1_TLB_CNTL,\r\n(0xA << 7) |\r\nMC_VM_MX_L1_TLB_CNTL__ENABLE_L1_TLB_MASK |\r\nMC_VM_MX_L1_TLB_CNTL__ENABLE_L1_FRAGMENT_PROCESSING_MASK |\r\nMC_VM_MX_L1_TLB_CNTL__SYSTEM_ACCESS_MODE_MASK |\r\nMC_VM_MX_L1_TLB_CNTL__ENABLE_ADVANCED_DRIVER_MODEL_MASK |\r\n(0UL << MC_VM_MX_L1_TLB_CNTL__SYSTEM_APERTURE_UNMAPPED_ACCESS__SHIFT));\r\nWREG32(mmVM_L2_CNTL,\r\nVM_L2_CNTL__ENABLE_L2_CACHE_MASK |\r\nVM_L2_CNTL__ENABLE_L2_FRAGMENT_PROCESSING_MASK |\r\nVM_L2_CNTL__ENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE_MASK |\r\nVM_L2_CNTL__ENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE_MASK |\r\n(7UL << VM_L2_CNTL__EFFECTIVE_L2_QUEUE_SIZE__SHIFT) |\r\n(1UL << VM_L2_CNTL__CONTEXT1_IDENTITY_ACCESS_MODE__SHIFT));\r\nWREG32(mmVM_L2_CNTL2,\r\nVM_L2_CNTL2__INVALIDATE_ALL_L1_TLBS_MASK |\r\nVM_L2_CNTL2__INVALIDATE_L2_CACHE_MASK);\r\nWREG32(mmVM_L2_CNTL3,\r\nVM_L2_CNTL3__L2_CACHE_BIGK_ASSOCIATIVITY_MASK |\r\n(4UL << VM_L2_CNTL3__BANK_SELECT__SHIFT) |\r\n(4UL << VM_L2_CNTL3__L2_CACHE_BIGK_FRAGMENT_SIZE__SHIFT));\r\nWREG32(mmVM_CONTEXT0_PAGE_TABLE_START_ADDR, adev->mc.gtt_start >> 12);\r\nWREG32(mmVM_CONTEXT0_PAGE_TABLE_END_ADDR, adev->mc.gtt_end >> 12);\r\nWREG32(mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR, adev->gart.table_addr >> 12);\r\nWREG32(mmVM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(adev->dummy_page.addr >> 12));\r\nWREG32(mmVM_CONTEXT0_CNTL2, 0);\r\nWREG32(mmVM_CONTEXT0_CNTL,\r\nVM_CONTEXT0_CNTL__ENABLE_CONTEXT_MASK |\r\n(0UL << VM_CONTEXT0_CNTL__PAGE_TABLE_DEPTH__SHIFT) |\r\nVM_CONTEXT0_CNTL__RANGE_PROTECTION_FAULT_ENABLE_DEFAULT_MASK);\r\nWREG32(0x575, 0);\r\nWREG32(0x576, 0);\r\nWREG32(0x577, 0);\r\nWREG32(mmVM_CONTEXT1_PAGE_TABLE_START_ADDR, 0);\r\nWREG32(mmVM_CONTEXT1_PAGE_TABLE_END_ADDR, adev->vm_manager.max_pfn - 1);\r\nfor (i = 1; i < 16; i++) {\r\nif (i < 8)\r\nWREG32(mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR + i,\r\nadev->gart.table_addr >> 12);\r\nelse\r\nWREG32(mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + i - 8,\r\nadev->gart.table_addr >> 12);\r\n}\r\nWREG32(mmVM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(adev->dummy_page.addr >> 12));\r\nWREG32(mmVM_CONTEXT1_CNTL2, 4);\r\nWREG32(mmVM_CONTEXT1_CNTL,\r\nVM_CONTEXT1_CNTL__ENABLE_CONTEXT_MASK |\r\n(1UL << VM_CONTEXT1_CNTL__PAGE_TABLE_DEPTH__SHIFT) |\r\n((amdgpu_vm_block_size - 9) << VM_CONTEXT1_CNTL__PAGE_TABLE_BLOCK_SIZE__SHIFT));\r\nif (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_ALWAYS)\r\ngmc_v6_0_set_fault_enable_default(adev, false);\r\nelse\r\ngmc_v6_0_set_fault_enable_default(adev, true);\r\ngmc_v6_0_gart_flush_gpu_tlb(adev, 0);\r\ndev_info(adev->dev, "PCIE GART of %uM enabled (table at 0x%016llX).\n",\r\n(unsigned)(adev->mc.gtt_size >> 20),\r\n(unsigned long long)adev->gart.table_addr);\r\nadev->gart.ready = true;\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_gart_init(struct amdgpu_device *adev)\r\n{\r\nint r;\r\nif (adev->gart.robj) {\r\ndev_warn(adev->dev, "gmc_v6_0 PCIE GART already initialized\n");\r\nreturn 0;\r\n}\r\nr = amdgpu_gart_init(adev);\r\nif (r)\r\nreturn r;\r\nadev->gart.table_size = adev->gart.num_gpu_pages * 8;\r\nreturn amdgpu_gart_table_vram_alloc(adev);\r\n}\r\nstatic void gmc_v6_0_gart_disable(struct amdgpu_device *adev)\r\n{\r\nWREG32(mmVM_CONTEXT0_CNTL, 0);\r\nWREG32(mmVM_CONTEXT1_CNTL, 0);\r\nWREG32(mmMC_VM_MX_L1_TLB_CNTL,\r\nMC_VM_MX_L1_TLB_CNTL__SYSTEM_ACCESS_MODE_MASK |\r\n(0UL << MC_VM_MX_L1_TLB_CNTL__SYSTEM_APERTURE_UNMAPPED_ACCESS__SHIFT));\r\nWREG32(mmVM_L2_CNTL,\r\nVM_L2_CNTL__ENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE_MASK |\r\nVM_L2_CNTL__ENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE_MASK |\r\n(7UL << VM_L2_CNTL__EFFECTIVE_L2_QUEUE_SIZE__SHIFT) |\r\n(1UL << VM_L2_CNTL__CONTEXT1_IDENTITY_ACCESS_MODE__SHIFT));\r\nWREG32(mmVM_L2_CNTL2, 0);\r\nWREG32(mmVM_L2_CNTL3,\r\nVM_L2_CNTL3__L2_CACHE_BIGK_ASSOCIATIVITY_MASK |\r\n(0UL << VM_L2_CNTL3__L2_CACHE_BIGK_FRAGMENT_SIZE__SHIFT));\r\namdgpu_gart_table_vram_unpin(adev);\r\n}\r\nstatic void gmc_v6_0_gart_fini(struct amdgpu_device *adev)\r\n{\r\namdgpu_gart_table_vram_free(adev);\r\namdgpu_gart_fini(adev);\r\n}\r\nstatic int gmc_v6_0_vm_init(struct amdgpu_device *adev)\r\n{\r\nadev->vm_manager.num_ids = AMDGPU_NUM_OF_VMIDS;\r\namdgpu_vm_manager_init(adev);\r\nif (adev->flags & AMD_IS_APU) {\r\nu64 tmp = RREG32(mmMC_VM_FB_OFFSET);\r\ntmp <<= 22;\r\nadev->vm_manager.vram_base_offset = tmp;\r\n} else\r\nadev->vm_manager.vram_base_offset = 0;\r\nreturn 0;\r\n}\r\nstatic void gmc_v6_0_vm_fini(struct amdgpu_device *adev)\r\n{\r\n}\r\nstatic void gmc_v6_0_vm_decode_fault(struct amdgpu_device *adev,\r\nu32 status, u32 addr, u32 mc_client)\r\n{\r\nu32 mc_id;\r\nu32 vmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);\r\nu32 protections = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\r\nPROTECTIONS);\r\nchar block[5] = { mc_client >> 24, (mc_client >> 16) & 0xff,\r\n(mc_client >> 8) & 0xff, mc_client & 0xff, 0 };\r\nmc_id = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\r\nMEMORY_CLIENT_ID);\r\ndev_err(adev->dev, "VM fault (0x%02x, vmid %d) at page %u, %s from '%s' (0x%08x) (%d)\n",\r\nprotections, vmid, addr,\r\nREG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\r\nMEMORY_CLIENT_RW) ?\r\n"write" : "read", block, mc_client, mc_id);\r\n}\r\nstatic int gmc_v6_0_convert_vram_type(int mc_seq_vram_type)\r\n{\r\nswitch (mc_seq_vram_type) {\r\ncase MC_SEQ_MISC0__MT__GDDR1:\r\nreturn AMDGPU_VRAM_TYPE_GDDR1;\r\ncase MC_SEQ_MISC0__MT__DDR2:\r\nreturn AMDGPU_VRAM_TYPE_DDR2;\r\ncase MC_SEQ_MISC0__MT__GDDR3:\r\nreturn AMDGPU_VRAM_TYPE_GDDR3;\r\ncase MC_SEQ_MISC0__MT__GDDR4:\r\nreturn AMDGPU_VRAM_TYPE_GDDR4;\r\ncase MC_SEQ_MISC0__MT__GDDR5:\r\nreturn AMDGPU_VRAM_TYPE_GDDR5;\r\ncase MC_SEQ_MISC0__MT__DDR3:\r\nreturn AMDGPU_VRAM_TYPE_DDR3;\r\ndefault:\r\nreturn AMDGPU_VRAM_TYPE_UNKNOWN;\r\n}\r\n}\r\nstatic int gmc_v6_0_early_init(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\ngmc_v6_0_set_gart_funcs(adev);\r\ngmc_v6_0_set_irq_funcs(adev);\r\nif (adev->flags & AMD_IS_APU) {\r\nadev->mc.vram_type = AMDGPU_VRAM_TYPE_UNKNOWN;\r\n} else {\r\nu32 tmp = RREG32(mmMC_SEQ_MISC0);\r\ntmp &= MC_SEQ_MISC0__MT__MASK;\r\nadev->mc.vram_type = gmc_v6_0_convert_vram_type(tmp);\r\n}\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_late_init(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nif (amdgpu_vm_fault_stop != AMDGPU_VM_FAULT_STOP_ALWAYS)\r\nreturn amdgpu_irq_get(adev, &adev->mc.vm_fault, 0);\r\nelse\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_sw_init(void *handle)\r\n{\r\nint r;\r\nint dma_bits;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nr = amdgpu_irq_add_id(adev, 146, &adev->mc.vm_fault);\r\nif (r)\r\nreturn r;\r\nr = amdgpu_irq_add_id(adev, 147, &adev->mc.vm_fault);\r\nif (r)\r\nreturn r;\r\nadev->vm_manager.max_pfn = amdgpu_vm_size << 18;\r\nadev->mc.mc_mask = 0xffffffffffULL;\r\nadev->need_dma32 = false;\r\ndma_bits = adev->need_dma32 ? 32 : 40;\r\nr = pci_set_dma_mask(adev->pdev, DMA_BIT_MASK(dma_bits));\r\nif (r) {\r\nadev->need_dma32 = true;\r\ndma_bits = 32;\r\ndev_warn(adev->dev, "amdgpu: No suitable DMA available.\n");\r\n}\r\nr = pci_set_consistent_dma_mask(adev->pdev, DMA_BIT_MASK(dma_bits));\r\nif (r) {\r\npci_set_consistent_dma_mask(adev->pdev, DMA_BIT_MASK(32));\r\ndev_warn(adev->dev, "amdgpu: No coherent DMA available.\n");\r\n}\r\nr = gmc_v6_0_init_microcode(adev);\r\nif (r) {\r\ndev_err(adev->dev, "Failed to load mc firmware!\n");\r\nreturn r;\r\n}\r\nr = gmc_v6_0_mc_init(adev);\r\nif (r)\r\nreturn r;\r\nr = amdgpu_bo_init(adev);\r\nif (r)\r\nreturn r;\r\nr = gmc_v6_0_gart_init(adev);\r\nif (r)\r\nreturn r;\r\nif (!adev->vm_manager.enabled) {\r\nr = gmc_v6_0_vm_init(adev);\r\nif (r) {\r\ndev_err(adev->dev, "vm manager initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nadev->vm_manager.enabled = true;\r\n}\r\nreturn r;\r\n}\r\nstatic int gmc_v6_0_sw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nif (adev->vm_manager.enabled) {\r\ngmc_v6_0_vm_fini(adev);\r\nadev->vm_manager.enabled = false;\r\n}\r\ngmc_v6_0_gart_fini(adev);\r\namdgpu_gem_force_release(adev);\r\namdgpu_bo_fini(adev);\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_hw_init(void *handle)\r\n{\r\nint r;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\ngmc_v6_0_mc_program(adev);\r\nif (!(adev->flags & AMD_IS_APU)) {\r\nr = gmc_v6_0_mc_load_microcode(adev);\r\nif (r) {\r\ndev_err(adev->dev, "Failed to load MC firmware!\n");\r\nreturn r;\r\n}\r\n}\r\nr = gmc_v6_0_gart_enable(adev);\r\nif (r)\r\nreturn r;\r\nreturn r;\r\n}\r\nstatic int gmc_v6_0_hw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\namdgpu_irq_put(adev, &adev->mc.vm_fault, 0);\r\ngmc_v6_0_gart_disable(adev);\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_suspend(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nif (adev->vm_manager.enabled) {\r\ngmc_v6_0_vm_fini(adev);\r\nadev->vm_manager.enabled = false;\r\n}\r\ngmc_v6_0_hw_fini(adev);\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_resume(void *handle)\r\n{\r\nint r;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nr = gmc_v6_0_hw_init(adev);\r\nif (r)\r\nreturn r;\r\nif (!adev->vm_manager.enabled) {\r\nr = gmc_v6_0_vm_init(adev);\r\nif (r) {\r\ndev_err(adev->dev, "vm manager initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nadev->vm_manager.enabled = true;\r\n}\r\nreturn r;\r\n}\r\nstatic bool gmc_v6_0_is_idle(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nu32 tmp = RREG32(mmSRBM_STATUS);\r\nif (tmp & (SRBM_STATUS__MCB_BUSY_MASK | SRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\r\nSRBM_STATUS__MCC_BUSY_MASK | SRBM_STATUS__MCD_BUSY_MASK | SRBM_STATUS__VMC_BUSY_MASK))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int gmc_v6_0_wait_for_idle(void *handle)\r\n{\r\nunsigned i;\r\nu32 tmp;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nfor (i = 0; i < adev->usec_timeout; i++) {\r\ntmp = RREG32(mmSRBM_STATUS) & (SRBM_STATUS__MCB_BUSY_MASK |\r\nSRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\r\nSRBM_STATUS__MCC_BUSY_MASK |\r\nSRBM_STATUS__MCD_BUSY_MASK |\r\nSRBM_STATUS__VMC_BUSY_MASK);\r\nif (!tmp)\r\nreturn 0;\r\nudelay(1);\r\n}\r\nreturn -ETIMEDOUT;\r\n}\r\nstatic int gmc_v6_0_soft_reset(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nstruct amdgpu_mode_mc_save save;\r\nu32 srbm_soft_reset = 0;\r\nu32 tmp = RREG32(mmSRBM_STATUS);\r\nif (tmp & SRBM_STATUS__VMC_BUSY_MASK)\r\nsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset,\r\nSRBM_SOFT_RESET, SOFT_RESET_VMC, 1);\r\nif (tmp & (SRBM_STATUS__MCB_BUSY_MASK | SRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\r\nSRBM_STATUS__MCC_BUSY_MASK | SRBM_STATUS__MCD_BUSY_MASK)) {\r\nif (!(adev->flags & AMD_IS_APU))\r\nsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset,\r\nSRBM_SOFT_RESET, SOFT_RESET_MC, 1);\r\n}\r\nif (srbm_soft_reset) {\r\ngmc_v6_0_mc_stop(adev, &save);\r\nif (gmc_v6_0_wait_for_idle(adev)) {\r\ndev_warn(adev->dev, "Wait for GMC idle timed out !\n");\r\n}\r\ntmp = RREG32(mmSRBM_SOFT_RESET);\r\ntmp |= srbm_soft_reset;\r\ndev_info(adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(mmSRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(mmSRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~srbm_soft_reset;\r\nWREG32(mmSRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(mmSRBM_SOFT_RESET);\r\nudelay(50);\r\ngmc_v6_0_mc_resume(adev, &save);\r\nudelay(50);\r\n}\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_vm_fault_interrupt_state(struct amdgpu_device *adev,\r\nstruct amdgpu_irq_src *src,\r\nunsigned type,\r\nenum amdgpu_interrupt_state state)\r\n{\r\nu32 tmp;\r\nu32 bits = (VM_CONTEXT1_CNTL__RANGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\r\nVM_CONTEXT1_CNTL__DUMMY_PAGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\r\nVM_CONTEXT1_CNTL__PDE0_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\r\nVM_CONTEXT1_CNTL__VALID_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\r\nVM_CONTEXT1_CNTL__READ_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\r\nVM_CONTEXT1_CNTL__WRITE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK);\r\nswitch (state) {\r\ncase AMDGPU_IRQ_STATE_DISABLE:\r\ntmp = RREG32(mmVM_CONTEXT0_CNTL);\r\ntmp &= ~bits;\r\nWREG32(mmVM_CONTEXT0_CNTL, tmp);\r\ntmp = RREG32(mmVM_CONTEXT1_CNTL);\r\ntmp &= ~bits;\r\nWREG32(mmVM_CONTEXT1_CNTL, tmp);\r\nbreak;\r\ncase AMDGPU_IRQ_STATE_ENABLE:\r\ntmp = RREG32(mmVM_CONTEXT0_CNTL);\r\ntmp |= bits;\r\nWREG32(mmVM_CONTEXT0_CNTL, tmp);\r\ntmp = RREG32(mmVM_CONTEXT1_CNTL);\r\ntmp |= bits;\r\nWREG32(mmVM_CONTEXT1_CNTL, tmp);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_process_interrupt(struct amdgpu_device *adev,\r\nstruct amdgpu_irq_src *source,\r\nstruct amdgpu_iv_entry *entry)\r\n{\r\nu32 addr, status;\r\naddr = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_ADDR);\r\nstatus = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_STATUS);\r\nWREG32_P(mmVM_CONTEXT1_CNTL2, 1, ~1);\r\nif (!addr && !status)\r\nreturn 0;\r\nif (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_FIRST)\r\ngmc_v6_0_set_fault_enable_default(adev, false);\r\nif (printk_ratelimit()) {\r\ndev_err(adev->dev, "GPU fault detected: %d 0x%08x\n",\r\nentry->src_id, entry->src_data);\r\ndev_err(adev->dev, " VM_CONTEXT1_PROTECTION_FAULT_ADDR 0x%08X\n",\r\naddr);\r\ndev_err(adev->dev, " VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",\r\nstatus);\r\ngmc_v6_0_vm_decode_fault(adev, status, addr, 0);\r\n}\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_set_clockgating_state(void *handle,\r\nenum amd_clockgating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic int gmc_v6_0_set_powergating_state(void *handle,\r\nenum amd_powergating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic void gmc_v6_0_set_gart_funcs(struct amdgpu_device *adev)\r\n{\r\nif (adev->gart.gart_funcs == NULL)\r\nadev->gart.gart_funcs = &gmc_v6_0_gart_funcs;\r\n}\r\nstatic void gmc_v6_0_set_irq_funcs(struct amdgpu_device *adev)\r\n{\r\nadev->mc.vm_fault.num_types = 1;\r\nadev->mc.vm_fault.funcs = &gmc_v6_0_irq_funcs;\r\n}
