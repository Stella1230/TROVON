static bool mid_spi_dma_chan_filter(struct dma_chan *chan, void *param)\r\n{\r\nstruct dw_dma_slave *s = param;\r\nif (s->dma_dev != chan->device->dev)\r\nreturn false;\r\nchan->private = s;\r\nreturn true;\r\n}\r\nstatic int mid_spi_dma_init(struct dw_spi *dws)\r\n{\r\nstruct pci_dev *dma_dev;\r\nstruct dw_dma_slave *tx = dws->dma_tx;\r\nstruct dw_dma_slave *rx = dws->dma_rx;\r\ndma_cap_mask_t mask;\r\ndma_dev = pci_get_device(PCI_VENDOR_ID_INTEL, 0x0827, NULL);\r\nif (!dma_dev)\r\nreturn -ENODEV;\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\nrx->dma_dev = &dma_dev->dev;\r\ndws->rxchan = dma_request_channel(mask, mid_spi_dma_chan_filter, rx);\r\nif (!dws->rxchan)\r\ngoto err_exit;\r\ndws->master->dma_rx = dws->rxchan;\r\ntx->dma_dev = &dma_dev->dev;\r\ndws->txchan = dma_request_channel(mask, mid_spi_dma_chan_filter, tx);\r\nif (!dws->txchan)\r\ngoto free_rxchan;\r\ndws->master->dma_tx = dws->txchan;\r\ndws->dma_inited = 1;\r\nreturn 0;\r\nfree_rxchan:\r\ndma_release_channel(dws->rxchan);\r\nerr_exit:\r\nreturn -EBUSY;\r\n}\r\nstatic void mid_spi_dma_exit(struct dw_spi *dws)\r\n{\r\nif (!dws->dma_inited)\r\nreturn;\r\ndmaengine_terminate_sync(dws->txchan);\r\ndma_release_channel(dws->txchan);\r\ndmaengine_terminate_sync(dws->rxchan);\r\ndma_release_channel(dws->rxchan);\r\n}\r\nstatic irqreturn_t dma_transfer(struct dw_spi *dws)\r\n{\r\nu16 irq_status = dw_readl(dws, DW_SPI_ISR);\r\nif (!irq_status)\r\nreturn IRQ_NONE;\r\ndw_readl(dws, DW_SPI_ICR);\r\nspi_reset_chip(dws);\r\ndev_err(&dws->master->dev, "%s: FIFO overrun/underrun\n", __func__);\r\ndws->master->cur_msg->status = -EIO;\r\nspi_finalize_current_transfer(dws->master);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic bool mid_spi_can_dma(struct spi_master *master, struct spi_device *spi,\r\nstruct spi_transfer *xfer)\r\n{\r\nstruct dw_spi *dws = spi_master_get_devdata(master);\r\nif (!dws->dma_inited)\r\nreturn false;\r\nreturn xfer->len > dws->fifo_len;\r\n}\r\nstatic enum dma_slave_buswidth convert_dma_width(u32 dma_width) {\r\nif (dma_width == 1)\r\nreturn DMA_SLAVE_BUSWIDTH_1_BYTE;\r\nelse if (dma_width == 2)\r\nreturn DMA_SLAVE_BUSWIDTH_2_BYTES;\r\nreturn DMA_SLAVE_BUSWIDTH_UNDEFINED;\r\n}\r\nstatic void dw_spi_dma_tx_done(void *arg)\r\n{\r\nstruct dw_spi *dws = arg;\r\nclear_bit(TX_BUSY, &dws->dma_chan_busy);\r\nif (test_bit(RX_BUSY, &dws->dma_chan_busy))\r\nreturn;\r\nspi_finalize_current_transfer(dws->master);\r\n}\r\nstatic struct dma_async_tx_descriptor *dw_spi_dma_prepare_tx(struct dw_spi *dws,\r\nstruct spi_transfer *xfer)\r\n{\r\nstruct dma_slave_config txconf;\r\nstruct dma_async_tx_descriptor *txdesc;\r\nif (!xfer->tx_buf)\r\nreturn NULL;\r\ntxconf.direction = DMA_MEM_TO_DEV;\r\ntxconf.dst_addr = dws->dma_addr;\r\ntxconf.dst_maxburst = 16;\r\ntxconf.src_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\ntxconf.dst_addr_width = convert_dma_width(dws->dma_width);\r\ntxconf.device_fc = false;\r\ndmaengine_slave_config(dws->txchan, &txconf);\r\ntxdesc = dmaengine_prep_slave_sg(dws->txchan,\r\nxfer->tx_sg.sgl,\r\nxfer->tx_sg.nents,\r\nDMA_MEM_TO_DEV,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!txdesc)\r\nreturn NULL;\r\ntxdesc->callback = dw_spi_dma_tx_done;\r\ntxdesc->callback_param = dws;\r\nreturn txdesc;\r\n}\r\nstatic void dw_spi_dma_rx_done(void *arg)\r\n{\r\nstruct dw_spi *dws = arg;\r\nclear_bit(RX_BUSY, &dws->dma_chan_busy);\r\nif (test_bit(TX_BUSY, &dws->dma_chan_busy))\r\nreturn;\r\nspi_finalize_current_transfer(dws->master);\r\n}\r\nstatic struct dma_async_tx_descriptor *dw_spi_dma_prepare_rx(struct dw_spi *dws,\r\nstruct spi_transfer *xfer)\r\n{\r\nstruct dma_slave_config rxconf;\r\nstruct dma_async_tx_descriptor *rxdesc;\r\nif (!xfer->rx_buf)\r\nreturn NULL;\r\nrxconf.direction = DMA_DEV_TO_MEM;\r\nrxconf.src_addr = dws->dma_addr;\r\nrxconf.src_maxburst = 16;\r\nrxconf.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\nrxconf.src_addr_width = convert_dma_width(dws->dma_width);\r\nrxconf.device_fc = false;\r\ndmaengine_slave_config(dws->rxchan, &rxconf);\r\nrxdesc = dmaengine_prep_slave_sg(dws->rxchan,\r\nxfer->rx_sg.sgl,\r\nxfer->rx_sg.nents,\r\nDMA_DEV_TO_MEM,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!rxdesc)\r\nreturn NULL;\r\nrxdesc->callback = dw_spi_dma_rx_done;\r\nrxdesc->callback_param = dws;\r\nreturn rxdesc;\r\n}\r\nstatic int mid_spi_dma_setup(struct dw_spi *dws, struct spi_transfer *xfer)\r\n{\r\nu16 dma_ctrl = 0;\r\ndw_writel(dws, DW_SPI_DMARDLR, 0xf);\r\ndw_writel(dws, DW_SPI_DMATDLR, 0x10);\r\nif (xfer->tx_buf)\r\ndma_ctrl |= SPI_DMA_TDMAE;\r\nif (xfer->rx_buf)\r\ndma_ctrl |= SPI_DMA_RDMAE;\r\ndw_writel(dws, DW_SPI_DMACR, dma_ctrl);\r\nspi_umask_intr(dws, SPI_INT_TXOI | SPI_INT_RXUI | SPI_INT_RXOI);\r\ndws->transfer_handler = dma_transfer;\r\nreturn 0;\r\n}\r\nstatic int mid_spi_dma_transfer(struct dw_spi *dws, struct spi_transfer *xfer)\r\n{\r\nstruct dma_async_tx_descriptor *txdesc, *rxdesc;\r\ntxdesc = dw_spi_dma_prepare_tx(dws, xfer);\r\nrxdesc = dw_spi_dma_prepare_rx(dws, xfer);\r\nif (rxdesc) {\r\nset_bit(RX_BUSY, &dws->dma_chan_busy);\r\ndmaengine_submit(rxdesc);\r\ndma_async_issue_pending(dws->rxchan);\r\n}\r\nif (txdesc) {\r\nset_bit(TX_BUSY, &dws->dma_chan_busy);\r\ndmaengine_submit(txdesc);\r\ndma_async_issue_pending(dws->txchan);\r\n}\r\nreturn 0;\r\n}\r\nstatic void mid_spi_dma_stop(struct dw_spi *dws)\r\n{\r\nif (test_bit(TX_BUSY, &dws->dma_chan_busy)) {\r\ndmaengine_terminate_sync(dws->txchan);\r\nclear_bit(TX_BUSY, &dws->dma_chan_busy);\r\n}\r\nif (test_bit(RX_BUSY, &dws->dma_chan_busy)) {\r\ndmaengine_terminate_sync(dws->rxchan);\r\nclear_bit(RX_BUSY, &dws->dma_chan_busy);\r\n}\r\n}\r\nint dw_spi_mid_init(struct dw_spi *dws)\r\n{\r\nvoid __iomem *clk_reg;\r\nu32 clk_cdiv;\r\nclk_reg = ioremap_nocache(MRST_CLK_SPI_REG, 16);\r\nif (!clk_reg)\r\nreturn -ENOMEM;\r\nclk_cdiv = readl(clk_reg + dws->bus_num * sizeof(u32));\r\nclk_cdiv &= CLK_SPI_CDIV_MASK;\r\nclk_cdiv >>= CLK_SPI_CDIV_OFFSET;\r\ndws->max_freq = MRST_SPI_CLK_BASE / (clk_cdiv + 1);\r\niounmap(clk_reg);\r\n#ifdef CONFIG_SPI_DW_MID_DMA\r\ndws->dma_tx = &mid_dma_tx;\r\ndws->dma_rx = &mid_dma_rx;\r\ndws->dma_ops = &mid_dma_ops;\r\n#endif\r\nreturn 0;\r\n}
