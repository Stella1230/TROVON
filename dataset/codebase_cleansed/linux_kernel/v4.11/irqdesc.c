static int __init irq_affinity_setup(char *str)\r\n{\r\nzalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);\r\ncpulist_parse(str, irq_default_affinity);\r\ncpumask_set_cpu(smp_processor_id(), irq_default_affinity);\r\nreturn 1;\r\n}\r\nstatic void __init init_irq_default_affinity(void)\r\n{\r\n#ifdef CONFIG_CPUMASK_OFFSTACK\r\nif (!irq_default_affinity)\r\nzalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);\r\n#endif\r\nif (cpumask_empty(irq_default_affinity))\r\ncpumask_setall(irq_default_affinity);\r\n}\r\nstatic void __init init_irq_default_affinity(void)\r\n{\r\n}\r\nstatic int alloc_masks(struct irq_desc *desc, gfp_t gfp, int node)\r\n{\r\nif (!zalloc_cpumask_var_node(&desc->irq_common_data.affinity,\r\ngfp, node))\r\nreturn -ENOMEM;\r\n#ifdef CONFIG_GENERIC_PENDING_IRQ\r\nif (!zalloc_cpumask_var_node(&desc->pending_mask, gfp, node)) {\r\nfree_cpumask_var(desc->irq_common_data.affinity);\r\nreturn -ENOMEM;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void desc_smp_init(struct irq_desc *desc, int node,\r\nconst struct cpumask *affinity)\r\n{\r\nif (!affinity)\r\naffinity = irq_default_affinity;\r\ncpumask_copy(desc->irq_common_data.affinity, affinity);\r\n#ifdef CONFIG_GENERIC_PENDING_IRQ\r\ncpumask_clear(desc->pending_mask);\r\n#endif\r\n#ifdef CONFIG_NUMA\r\ndesc->irq_common_data.node = node;\r\n#endif\r\n}\r\nstatic inline int\r\nalloc_masks(struct irq_desc *desc, gfp_t gfp, int node) { return 0; }\r\nstatic inline void\r\ndesc_smp_init(struct irq_desc *desc, int node, const struct cpumask *affinity) { }\r\nstatic void desc_set_defaults(unsigned int irq, struct irq_desc *desc, int node,\r\nconst struct cpumask *affinity, struct module *owner)\r\n{\r\nint cpu;\r\ndesc->irq_common_data.handler_data = NULL;\r\ndesc->irq_common_data.msi_desc = NULL;\r\ndesc->irq_data.common = &desc->irq_common_data;\r\ndesc->irq_data.irq = irq;\r\ndesc->irq_data.chip = &no_irq_chip;\r\ndesc->irq_data.chip_data = NULL;\r\nirq_settings_clr_and_set(desc, ~0, _IRQ_DEFAULT_INIT_FLAGS);\r\nirqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);\r\ndesc->handle_irq = handle_bad_irq;\r\ndesc->depth = 1;\r\ndesc->irq_count = 0;\r\ndesc->irqs_unhandled = 0;\r\ndesc->name = NULL;\r\ndesc->owner = owner;\r\nfor_each_possible_cpu(cpu)\r\n*per_cpu_ptr(desc->kstat_irqs, cpu) = 0;\r\ndesc_smp_init(desc, node, affinity);\r\n}\r\nstatic ssize_t per_cpu_count_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\r\nint cpu, irq = desc->irq_data.irq;\r\nssize_t ret = 0;\r\nchar *p = "";\r\nfor_each_possible_cpu(cpu) {\r\nunsigned int c = kstat_irqs_cpu(irq, cpu);\r\nret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s%u", p, c);\r\np = ",";\r\n}\r\nret += scnprintf(buf + ret, PAGE_SIZE - ret, "\n");\r\nreturn ret;\r\n}\r\nstatic ssize_t chip_name_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\r\nssize_t ret = 0;\r\nraw_spin_lock_irq(&desc->lock);\r\nif (desc->irq_data.chip && desc->irq_data.chip->name) {\r\nret = scnprintf(buf, PAGE_SIZE, "%s\n",\r\ndesc->irq_data.chip->name);\r\n}\r\nraw_spin_unlock_irq(&desc->lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t hwirq_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\r\nssize_t ret = 0;\r\nraw_spin_lock_irq(&desc->lock);\r\nif (desc->irq_data.domain)\r\nret = sprintf(buf, "%d\n", (int)desc->irq_data.hwirq);\r\nraw_spin_unlock_irq(&desc->lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t type_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\r\nssize_t ret = 0;\r\nraw_spin_lock_irq(&desc->lock);\r\nret = sprintf(buf, "%s\n",\r\nirqd_is_level_type(&desc->irq_data) ? "level" : "edge");\r\nraw_spin_unlock_irq(&desc->lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t name_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\r\nssize_t ret = 0;\r\nraw_spin_lock_irq(&desc->lock);\r\nif (desc->name)\r\nret = scnprintf(buf, PAGE_SIZE, "%s\n", desc->name);\r\nraw_spin_unlock_irq(&desc->lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t actions_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\r\nstruct irqaction *action;\r\nssize_t ret = 0;\r\nchar *p = "";\r\nraw_spin_lock_irq(&desc->lock);\r\nfor (action = desc->action; action != NULL; action = action->next) {\r\nret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s%s",\r\np, action->name);\r\np = ",";\r\n}\r\nraw_spin_unlock_irq(&desc->lock);\r\nif (ret)\r\nret += scnprintf(buf + ret, PAGE_SIZE - ret, "\n");\r\nreturn ret;\r\n}\r\nstatic void irq_sysfs_add(int irq, struct irq_desc *desc)\r\n{\r\nif (irq_kobj_base) {\r\nif (kobject_add(&desc->kobj, irq_kobj_base, "%d", irq))\r\npr_warn("Failed to add kobject for irq %d\n", irq);\r\n}\r\n}\r\nstatic int __init irq_sysfs_init(void)\r\n{\r\nstruct irq_desc *desc;\r\nint irq;\r\nirq_lock_sparse();\r\nirq_kobj_base = kobject_create_and_add("irq", kernel_kobj);\r\nif (!irq_kobj_base) {\r\nirq_unlock_sparse();\r\nreturn -ENOMEM;\r\n}\r\nfor_each_irq_desc(irq, desc)\r\nirq_sysfs_add(irq, desc);\r\nirq_unlock_sparse();\r\nreturn 0;\r\n}\r\nstatic void irq_sysfs_add(int irq, struct irq_desc *desc) {}\r\nstatic void irq_insert_desc(unsigned int irq, struct irq_desc *desc)\r\n{\r\nradix_tree_insert(&irq_desc_tree, irq, desc);\r\n}\r\nstruct irq_desc *irq_to_desc(unsigned int irq)\r\n{\r\nreturn radix_tree_lookup(&irq_desc_tree, irq);\r\n}\r\nstatic void delete_irq_desc(unsigned int irq)\r\n{\r\nradix_tree_delete(&irq_desc_tree, irq);\r\n}\r\nstatic void free_masks(struct irq_desc *desc)\r\n{\r\n#ifdef CONFIG_GENERIC_PENDING_IRQ\r\nfree_cpumask_var(desc->pending_mask);\r\n#endif\r\nfree_cpumask_var(desc->irq_common_data.affinity);\r\n}\r\nstatic inline void free_masks(struct irq_desc *desc) { }\r\nvoid irq_lock_sparse(void)\r\n{\r\nmutex_lock(&sparse_irq_lock);\r\n}\r\nvoid irq_unlock_sparse(void)\r\n{\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nstatic struct irq_desc *alloc_desc(int irq, int node, unsigned int flags,\r\nconst struct cpumask *affinity,\r\nstruct module *owner)\r\n{\r\nstruct irq_desc *desc;\r\ngfp_t gfp = GFP_KERNEL;\r\ndesc = kzalloc_node(sizeof(*desc), gfp, node);\r\nif (!desc)\r\nreturn NULL;\r\ndesc->kstat_irqs = alloc_percpu(unsigned int);\r\nif (!desc->kstat_irqs)\r\ngoto err_desc;\r\nif (alloc_masks(desc, gfp, node))\r\ngoto err_kstat;\r\nraw_spin_lock_init(&desc->lock);\r\nlockdep_set_class(&desc->lock, &irq_desc_lock_class);\r\ninit_rcu_head(&desc->rcu);\r\ndesc_set_defaults(irq, desc, node, affinity, owner);\r\nirqd_set(&desc->irq_data, flags);\r\nkobject_init(&desc->kobj, &irq_kobj_type);\r\nreturn desc;\r\nerr_kstat:\r\nfree_percpu(desc->kstat_irqs);\r\nerr_desc:\r\nkfree(desc);\r\nreturn NULL;\r\n}\r\nstatic void irq_kobj_release(struct kobject *kobj)\r\n{\r\nstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\r\nfree_masks(desc);\r\nfree_percpu(desc->kstat_irqs);\r\nkfree(desc);\r\n}\r\nstatic void delayed_free_desc(struct rcu_head *rhp)\r\n{\r\nstruct irq_desc *desc = container_of(rhp, struct irq_desc, rcu);\r\nkobject_put(&desc->kobj);\r\n}\r\nstatic void free_desc(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nunregister_irq_proc(irq, desc);\r\nmutex_lock(&sparse_irq_lock);\r\nkobject_del(&desc->kobj);\r\ndelete_irq_desc(irq);\r\nmutex_unlock(&sparse_irq_lock);\r\ncall_rcu(&desc->rcu, delayed_free_desc);\r\n}\r\nstatic int alloc_descs(unsigned int start, unsigned int cnt, int node,\r\nconst struct cpumask *affinity, struct module *owner)\r\n{\r\nconst struct cpumask *mask = NULL;\r\nstruct irq_desc *desc;\r\nunsigned int flags;\r\nint i;\r\nif (affinity) {\r\nfor (i = 0, mask = affinity; i < cnt; i++, mask++) {\r\nif (cpumask_empty(mask))\r\nreturn -EINVAL;\r\n}\r\n}\r\nflags = affinity ? IRQD_AFFINITY_MANAGED : 0;\r\nmask = NULL;\r\nfor (i = 0; i < cnt; i++) {\r\nif (affinity) {\r\nnode = cpu_to_node(cpumask_first(affinity));\r\nmask = affinity;\r\naffinity++;\r\n}\r\ndesc = alloc_desc(start + i, node, flags, mask, owner);\r\nif (!desc)\r\ngoto err;\r\nmutex_lock(&sparse_irq_lock);\r\nirq_insert_desc(start + i, desc);\r\nirq_sysfs_add(start + i, desc);\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nreturn start;\r\nerr:\r\nfor (i--; i >= 0; i--)\r\nfree_desc(start + i);\r\nmutex_lock(&sparse_irq_lock);\r\nbitmap_clear(allocated_irqs, start, cnt);\r\nmutex_unlock(&sparse_irq_lock);\r\nreturn -ENOMEM;\r\n}\r\nstatic int irq_expand_nr_irqs(unsigned int nr)\r\n{\r\nif (nr > IRQ_BITMAP_BITS)\r\nreturn -ENOMEM;\r\nnr_irqs = nr;\r\nreturn 0;\r\n}\r\nint __init early_irq_init(void)\r\n{\r\nint i, initcnt, node = first_online_node;\r\nstruct irq_desc *desc;\r\ninit_irq_default_affinity();\r\ninitcnt = arch_probe_nr_irqs();\r\nprintk(KERN_INFO "NR_IRQS:%d nr_irqs:%d %d\n", NR_IRQS, nr_irqs, initcnt);\r\nif (WARN_ON(nr_irqs > IRQ_BITMAP_BITS))\r\nnr_irqs = IRQ_BITMAP_BITS;\r\nif (WARN_ON(initcnt > IRQ_BITMAP_BITS))\r\ninitcnt = IRQ_BITMAP_BITS;\r\nif (initcnt > nr_irqs)\r\nnr_irqs = initcnt;\r\nfor (i = 0; i < initcnt; i++) {\r\ndesc = alloc_desc(i, node, 0, NULL, NULL);\r\nset_bit(i, allocated_irqs);\r\nirq_insert_desc(i, desc);\r\n}\r\nreturn arch_early_irq_init();\r\n}\r\nint __init early_irq_init(void)\r\n{\r\nint count, i, node = first_online_node;\r\nstruct irq_desc *desc;\r\ninit_irq_default_affinity();\r\nprintk(KERN_INFO "NR_IRQS:%d\n", NR_IRQS);\r\ndesc = irq_desc;\r\ncount = ARRAY_SIZE(irq_desc);\r\nfor (i = 0; i < count; i++) {\r\ndesc[i].kstat_irqs = alloc_percpu(unsigned int);\r\nalloc_masks(&desc[i], GFP_KERNEL, node);\r\nraw_spin_lock_init(&desc[i].lock);\r\nlockdep_set_class(&desc[i].lock, &irq_desc_lock_class);\r\ndesc_set_defaults(i, &desc[i], node, NULL, NULL);\r\n}\r\nreturn arch_early_irq_init();\r\n}\r\nstruct irq_desc *irq_to_desc(unsigned int irq)\r\n{\r\nreturn (irq < NR_IRQS) ? irq_desc + irq : NULL;\r\n}\r\nstatic void free_desc(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&desc->lock, flags);\r\ndesc_set_defaults(irq, desc, irq_desc_get_node(desc), NULL, NULL);\r\nraw_spin_unlock_irqrestore(&desc->lock, flags);\r\n}\r\nstatic inline int alloc_descs(unsigned int start, unsigned int cnt, int node,\r\nconst struct cpumask *affinity,\r\nstruct module *owner)\r\n{\r\nu32 i;\r\nfor (i = 0; i < cnt; i++) {\r\nstruct irq_desc *desc = irq_to_desc(start + i);\r\ndesc->owner = owner;\r\n}\r\nreturn start;\r\n}\r\nstatic int irq_expand_nr_irqs(unsigned int nr)\r\n{\r\nreturn -ENOMEM;\r\n}\r\nvoid irq_mark_irq(unsigned int irq)\r\n{\r\nmutex_lock(&sparse_irq_lock);\r\nbitmap_set(allocated_irqs, irq, 1);\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nvoid irq_init_desc(unsigned int irq)\r\n{\r\nfree_desc(irq);\r\n}\r\nint generic_handle_irq(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nif (!desc)\r\nreturn -EINVAL;\r\ngeneric_handle_irq_desc(desc);\r\nreturn 0;\r\n}\r\nint __handle_domain_irq(struct irq_domain *domain, unsigned int hwirq,\r\nbool lookup, struct pt_regs *regs)\r\n{\r\nstruct pt_regs *old_regs = set_irq_regs(regs);\r\nunsigned int irq = hwirq;\r\nint ret = 0;\r\nirq_enter();\r\n#ifdef CONFIG_IRQ_DOMAIN\r\nif (lookup)\r\nirq = irq_find_mapping(domain, hwirq);\r\n#endif\r\nif (unlikely(!irq || irq >= nr_irqs)) {\r\nack_bad_irq(irq);\r\nret = -EINVAL;\r\n} else {\r\ngeneric_handle_irq(irq);\r\n}\r\nirq_exit();\r\nset_irq_regs(old_regs);\r\nreturn ret;\r\n}\r\nvoid irq_free_descs(unsigned int from, unsigned int cnt)\r\n{\r\nint i;\r\nif (from >= nr_irqs || (from + cnt) > nr_irqs)\r\nreturn;\r\nfor (i = 0; i < cnt; i++)\r\nfree_desc(from + i);\r\nmutex_lock(&sparse_irq_lock);\r\nbitmap_clear(allocated_irqs, from, cnt);\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nint __ref\r\n__irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,\r\nstruct module *owner, const struct cpumask *affinity)\r\n{\r\nint start, ret;\r\nif (!cnt)\r\nreturn -EINVAL;\r\nif (irq >= 0) {\r\nif (from > irq)\r\nreturn -EINVAL;\r\nfrom = irq;\r\n} else {\r\nfrom = arch_dynirq_lower_bound(from);\r\n}\r\nmutex_lock(&sparse_irq_lock);\r\nstart = bitmap_find_next_zero_area(allocated_irqs, IRQ_BITMAP_BITS,\r\nfrom, cnt, 0);\r\nret = -EEXIST;\r\nif (irq >=0 && start != irq)\r\ngoto err;\r\nif (start + cnt > nr_irqs) {\r\nret = irq_expand_nr_irqs(start + cnt);\r\nif (ret)\r\ngoto err;\r\n}\r\nbitmap_set(allocated_irqs, start, cnt);\r\nmutex_unlock(&sparse_irq_lock);\r\nreturn alloc_descs(start, cnt, node, affinity, owner);\r\nerr:\r\nmutex_unlock(&sparse_irq_lock);\r\nreturn ret;\r\n}\r\nunsigned int irq_alloc_hwirqs(int cnt, int node)\r\n{\r\nint i, irq = __irq_alloc_descs(-1, 0, cnt, node, NULL, NULL);\r\nif (irq < 0)\r\nreturn 0;\r\nfor (i = irq; cnt > 0; i++, cnt--) {\r\nif (arch_setup_hwirq(i, node))\r\ngoto err;\r\nirq_clear_status_flags(i, _IRQ_NOREQUEST);\r\n}\r\nreturn irq;\r\nerr:\r\nfor (i--; i >= irq; i--) {\r\nirq_set_status_flags(i, _IRQ_NOREQUEST | _IRQ_NOPROBE);\r\narch_teardown_hwirq(i);\r\n}\r\nirq_free_descs(irq, cnt);\r\nreturn 0;\r\n}\r\nvoid irq_free_hwirqs(unsigned int from, int cnt)\r\n{\r\nint i, j;\r\nfor (i = from, j = cnt; j > 0; i++, j--) {\r\nirq_set_status_flags(i, _IRQ_NOREQUEST | _IRQ_NOPROBE);\r\narch_teardown_hwirq(i);\r\n}\r\nirq_free_descs(from, cnt);\r\n}\r\nunsigned int irq_get_next_irq(unsigned int offset)\r\n{\r\nreturn find_next_bit(allocated_irqs, nr_irqs, offset);\r\n}\r\nstruct irq_desc *\r\n__irq_get_desc_lock(unsigned int irq, unsigned long *flags, bool bus,\r\nunsigned int check)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nif (desc) {\r\nif (check & _IRQ_DESC_CHECK) {\r\nif ((check & _IRQ_DESC_PERCPU) &&\r\n!irq_settings_is_per_cpu_devid(desc))\r\nreturn NULL;\r\nif (!(check & _IRQ_DESC_PERCPU) &&\r\nirq_settings_is_per_cpu_devid(desc))\r\nreturn NULL;\r\n}\r\nif (bus)\r\nchip_bus_lock(desc);\r\nraw_spin_lock_irqsave(&desc->lock, *flags);\r\n}\r\nreturn desc;\r\n}\r\nvoid __irq_put_desc_unlock(struct irq_desc *desc, unsigned long flags, bool bus)\r\n{\r\nraw_spin_unlock_irqrestore(&desc->lock, flags);\r\nif (bus)\r\nchip_bus_sync_unlock(desc);\r\n}\r\nint irq_set_percpu_devid_partition(unsigned int irq,\r\nconst struct cpumask *affinity)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nif (!desc)\r\nreturn -EINVAL;\r\nif (desc->percpu_enabled)\r\nreturn -EINVAL;\r\ndesc->percpu_enabled = kzalloc(sizeof(*desc->percpu_enabled), GFP_KERNEL);\r\nif (!desc->percpu_enabled)\r\nreturn -ENOMEM;\r\nif (affinity)\r\ndesc->percpu_affinity = affinity;\r\nelse\r\ndesc->percpu_affinity = cpu_possible_mask;\r\nirq_set_percpu_devid_flags(irq);\r\nreturn 0;\r\n}\r\nint irq_set_percpu_devid(unsigned int irq)\r\n{\r\nreturn irq_set_percpu_devid_partition(irq, NULL);\r\n}\r\nint irq_get_percpu_devid_partition(unsigned int irq, struct cpumask *affinity)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nif (!desc || !desc->percpu_enabled)\r\nreturn -EINVAL;\r\nif (affinity)\r\ncpumask_copy(affinity, desc->percpu_affinity);\r\nreturn 0;\r\n}\r\nvoid kstat_incr_irq_this_cpu(unsigned int irq)\r\n{\r\nkstat_incr_irqs_this_cpu(irq_to_desc(irq));\r\n}\r\nunsigned int kstat_irqs_cpu(unsigned int irq, int cpu)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nreturn desc && desc->kstat_irqs ?\r\n*per_cpu_ptr(desc->kstat_irqs, cpu) : 0;\r\n}\r\nunsigned int kstat_irqs(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nint cpu;\r\nunsigned int sum = 0;\r\nif (!desc || !desc->kstat_irqs)\r\nreturn 0;\r\nfor_each_possible_cpu(cpu)\r\nsum += *per_cpu_ptr(desc->kstat_irqs, cpu);\r\nreturn sum;\r\n}\r\nunsigned int kstat_irqs_usr(unsigned int irq)\r\n{\r\nunsigned int sum;\r\nirq_lock_sparse();\r\nsum = kstat_irqs(irq);\r\nirq_unlock_sparse();\r\nreturn sum;\r\n}
