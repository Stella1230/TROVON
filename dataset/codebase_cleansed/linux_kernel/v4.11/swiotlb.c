static int __init\r\nsetup_io_tlb_npages(char *str)\r\n{\r\nif (isdigit(*str)) {\r\nio_tlb_nslabs = simple_strtoul(str, &str, 0);\r\nio_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n}\r\nif (*str == ',')\r\n++str;\r\nif (!strcmp(str, "force")) {\r\nswiotlb_force = SWIOTLB_FORCE;\r\n} else if (!strcmp(str, "noforce")) {\r\nswiotlb_force = SWIOTLB_NO_FORCE;\r\nio_tlb_nslabs = 1;\r\n}\r\nreturn 0;\r\n}\r\nunsigned long swiotlb_nr_tbl(void)\r\n{\r\nreturn io_tlb_nslabs;\r\n}\r\nunsigned int swiotlb_max_segment(void)\r\n{\r\nreturn max_segment;\r\n}\r\nvoid swiotlb_set_max_segment(unsigned int val)\r\n{\r\nif (swiotlb_force == SWIOTLB_FORCE)\r\nmax_segment = 1;\r\nelse\r\nmax_segment = rounddown(val, PAGE_SIZE);\r\n}\r\nunsigned long swiotlb_size_or_default(void)\r\n{\r\nunsigned long size;\r\nsize = io_tlb_nslabs << IO_TLB_SHIFT;\r\nreturn size ? size : (IO_TLB_DEFAULT_SIZE);\r\n}\r\nstatic dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,\r\nvolatile void *address)\r\n{\r\nreturn phys_to_dma(hwdev, virt_to_phys(address));\r\n}\r\nvoid swiotlb_print_info(void)\r\n{\r\nunsigned long bytes = io_tlb_nslabs << IO_TLB_SHIFT;\r\nunsigned char *vstart, *vend;\r\nif (no_iotlb_memory) {\r\npr_warn("software IO TLB: No low mem\n");\r\nreturn;\r\n}\r\nvstart = phys_to_virt(io_tlb_start);\r\nvend = phys_to_virt(io_tlb_end);\r\nprintk(KERN_INFO "software IO TLB [mem %#010llx-%#010llx] (%luMB) mapped at [%p-%p]\n",\r\n(unsigned long long)io_tlb_start,\r\n(unsigned long long)io_tlb_end,\r\nbytes >> 20, vstart, vend - 1);\r\n}\r\nint __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)\r\n{\r\nvoid *v_overflow_buffer;\r\nunsigned long i, bytes;\r\nbytes = nslabs << IO_TLB_SHIFT;\r\nio_tlb_nslabs = nslabs;\r\nio_tlb_start = __pa(tlb);\r\nio_tlb_end = io_tlb_start + bytes;\r\nv_overflow_buffer = memblock_virt_alloc_low_nopanic(\r\nPAGE_ALIGN(io_tlb_overflow),\r\nPAGE_SIZE);\r\nif (!v_overflow_buffer)\r\nreturn -ENOMEM;\r\nio_tlb_overflow_buffer = __pa(v_overflow_buffer);\r\nio_tlb_list = memblock_virt_alloc(\r\nPAGE_ALIGN(io_tlb_nslabs * sizeof(int)),\r\nPAGE_SIZE);\r\nio_tlb_orig_addr = memblock_virt_alloc(\r\nPAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)),\r\nPAGE_SIZE);\r\nfor (i = 0; i < io_tlb_nslabs; i++) {\r\nio_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);\r\nio_tlb_orig_addr[i] = INVALID_PHYS_ADDR;\r\n}\r\nio_tlb_index = 0;\r\nif (verbose)\r\nswiotlb_print_info();\r\nswiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);\r\nreturn 0;\r\n}\r\nvoid __init\r\nswiotlb_init(int verbose)\r\n{\r\nsize_t default_size = IO_TLB_DEFAULT_SIZE;\r\nunsigned char *vstart;\r\nunsigned long bytes;\r\nif (!io_tlb_nslabs) {\r\nio_tlb_nslabs = (default_size >> IO_TLB_SHIFT);\r\nio_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n}\r\nbytes = io_tlb_nslabs << IO_TLB_SHIFT;\r\nvstart = memblock_virt_alloc_low_nopanic(PAGE_ALIGN(bytes), PAGE_SIZE);\r\nif (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, verbose))\r\nreturn;\r\nif (io_tlb_start)\r\nmemblock_free_early(io_tlb_start,\r\nPAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT));\r\npr_warn("Cannot allocate SWIOTLB buffer");\r\nno_iotlb_memory = true;\r\n}\r\nint\r\nswiotlb_late_init_with_default_size(size_t default_size)\r\n{\r\nunsigned long bytes, req_nslabs = io_tlb_nslabs;\r\nunsigned char *vstart = NULL;\r\nunsigned int order;\r\nint rc = 0;\r\nif (!io_tlb_nslabs) {\r\nio_tlb_nslabs = (default_size >> IO_TLB_SHIFT);\r\nio_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n}\r\norder = get_order(io_tlb_nslabs << IO_TLB_SHIFT);\r\nio_tlb_nslabs = SLABS_PER_PAGE << order;\r\nbytes = io_tlb_nslabs << IO_TLB_SHIFT;\r\nwhile ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {\r\nvstart = (void *)__get_free_pages(GFP_DMA | __GFP_NOWARN,\r\norder);\r\nif (vstart)\r\nbreak;\r\norder--;\r\n}\r\nif (!vstart) {\r\nio_tlb_nslabs = req_nslabs;\r\nreturn -ENOMEM;\r\n}\r\nif (order != get_order(bytes)) {\r\nprintk(KERN_WARNING "Warning: only able to allocate %ld MB "\r\n"for software IO TLB\n", (PAGE_SIZE << order) >> 20);\r\nio_tlb_nslabs = SLABS_PER_PAGE << order;\r\n}\r\nrc = swiotlb_late_init_with_tbl(vstart, io_tlb_nslabs);\r\nif (rc)\r\nfree_pages((unsigned long)vstart, order);\r\nreturn rc;\r\n}\r\nint\r\nswiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)\r\n{\r\nunsigned long i, bytes;\r\nunsigned char *v_overflow_buffer;\r\nbytes = nslabs << IO_TLB_SHIFT;\r\nio_tlb_nslabs = nslabs;\r\nio_tlb_start = virt_to_phys(tlb);\r\nio_tlb_end = io_tlb_start + bytes;\r\nmemset(tlb, 0, bytes);\r\nv_overflow_buffer = (void *)__get_free_pages(GFP_DMA,\r\nget_order(io_tlb_overflow));\r\nif (!v_overflow_buffer)\r\ngoto cleanup2;\r\nio_tlb_overflow_buffer = virt_to_phys(v_overflow_buffer);\r\nio_tlb_list = (unsigned int *)__get_free_pages(GFP_KERNEL,\r\nget_order(io_tlb_nslabs * sizeof(int)));\r\nif (!io_tlb_list)\r\ngoto cleanup3;\r\nio_tlb_orig_addr = (phys_addr_t *)\r\n__get_free_pages(GFP_KERNEL,\r\nget_order(io_tlb_nslabs *\r\nsizeof(phys_addr_t)));\r\nif (!io_tlb_orig_addr)\r\ngoto cleanup4;\r\nfor (i = 0; i < io_tlb_nslabs; i++) {\r\nio_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);\r\nio_tlb_orig_addr[i] = INVALID_PHYS_ADDR;\r\n}\r\nio_tlb_index = 0;\r\nswiotlb_print_info();\r\nlate_alloc = 1;\r\nswiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);\r\nreturn 0;\r\ncleanup4:\r\nfree_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *\r\nsizeof(int)));\r\nio_tlb_list = NULL;\r\ncleanup3:\r\nfree_pages((unsigned long)v_overflow_buffer,\r\nget_order(io_tlb_overflow));\r\nio_tlb_overflow_buffer = 0;\r\ncleanup2:\r\nio_tlb_end = 0;\r\nio_tlb_start = 0;\r\nio_tlb_nslabs = 0;\r\nmax_segment = 0;\r\nreturn -ENOMEM;\r\n}\r\nvoid __init swiotlb_free(void)\r\n{\r\nif (!io_tlb_orig_addr)\r\nreturn;\r\nif (late_alloc) {\r\nfree_pages((unsigned long)phys_to_virt(io_tlb_overflow_buffer),\r\nget_order(io_tlb_overflow));\r\nfree_pages((unsigned long)io_tlb_orig_addr,\r\nget_order(io_tlb_nslabs * sizeof(phys_addr_t)));\r\nfree_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *\r\nsizeof(int)));\r\nfree_pages((unsigned long)phys_to_virt(io_tlb_start),\r\nget_order(io_tlb_nslabs << IO_TLB_SHIFT));\r\n} else {\r\nmemblock_free_late(io_tlb_overflow_buffer,\r\nPAGE_ALIGN(io_tlb_overflow));\r\nmemblock_free_late(__pa(io_tlb_orig_addr),\r\nPAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)));\r\nmemblock_free_late(__pa(io_tlb_list),\r\nPAGE_ALIGN(io_tlb_nslabs * sizeof(int)));\r\nmemblock_free_late(io_tlb_start,\r\nPAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT));\r\n}\r\nio_tlb_nslabs = 0;\r\nmax_segment = 0;\r\n}\r\nint is_swiotlb_buffer(phys_addr_t paddr)\r\n{\r\nreturn paddr >= io_tlb_start && paddr < io_tlb_end;\r\n}\r\nstatic void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nunsigned long pfn = PFN_DOWN(orig_addr);\r\nunsigned char *vaddr = phys_to_virt(tlb_addr);\r\nif (PageHighMem(pfn_to_page(pfn))) {\r\nunsigned int offset = orig_addr & ~PAGE_MASK;\r\nchar *buffer;\r\nunsigned int sz = 0;\r\nunsigned long flags;\r\nwhile (size) {\r\nsz = min_t(size_t, PAGE_SIZE - offset, size);\r\nlocal_irq_save(flags);\r\nbuffer = kmap_atomic(pfn_to_page(pfn));\r\nif (dir == DMA_TO_DEVICE)\r\nmemcpy(vaddr, buffer + offset, sz);\r\nelse\r\nmemcpy(buffer + offset, vaddr, sz);\r\nkunmap_atomic(buffer);\r\nlocal_irq_restore(flags);\r\nsize -= sz;\r\npfn++;\r\nvaddr += sz;\r\noffset = 0;\r\n}\r\n} else if (dir == DMA_TO_DEVICE) {\r\nmemcpy(vaddr, phys_to_virt(orig_addr), size);\r\n} else {\r\nmemcpy(phys_to_virt(orig_addr), vaddr, size);\r\n}\r\n}\r\nphys_addr_t swiotlb_tbl_map_single(struct device *hwdev,\r\ndma_addr_t tbl_dma_addr,\r\nphys_addr_t orig_addr, size_t size,\r\nenum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nunsigned long flags;\r\nphys_addr_t tlb_addr;\r\nunsigned int nslots, stride, index, wrap;\r\nint i;\r\nunsigned long mask;\r\nunsigned long offset_slots;\r\nunsigned long max_slots;\r\nif (no_iotlb_memory)\r\npanic("Can not allocate SWIOTLB buffer earlier and can't now provide you with the DMA bounce buffer");\r\nmask = dma_get_seg_boundary(hwdev);\r\ntbl_dma_addr &= mask;\r\noffset_slots = ALIGN(tbl_dma_addr, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;\r\nmax_slots = mask + 1\r\n? ALIGN(mask + 1, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT\r\n: 1UL << (BITS_PER_LONG - IO_TLB_SHIFT);\r\nnslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;\r\nif (size >= PAGE_SIZE)\r\nstride = (1 << (PAGE_SHIFT - IO_TLB_SHIFT));\r\nelse\r\nstride = 1;\r\nBUG_ON(!nslots);\r\nspin_lock_irqsave(&io_tlb_lock, flags);\r\nindex = ALIGN(io_tlb_index, stride);\r\nif (index >= io_tlb_nslabs)\r\nindex = 0;\r\nwrap = index;\r\ndo {\r\nwhile (iommu_is_span_boundary(index, nslots, offset_slots,\r\nmax_slots)) {\r\nindex += stride;\r\nif (index >= io_tlb_nslabs)\r\nindex = 0;\r\nif (index == wrap)\r\ngoto not_found;\r\n}\r\nif (io_tlb_list[index] >= nslots) {\r\nint count = 0;\r\nfor (i = index; i < (int) (index + nslots); i++)\r\nio_tlb_list[i] = 0;\r\nfor (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE - 1) && io_tlb_list[i]; i--)\r\nio_tlb_list[i] = ++count;\r\ntlb_addr = io_tlb_start + (index << IO_TLB_SHIFT);\r\nio_tlb_index = ((index + nslots) < io_tlb_nslabs\r\n? (index + nslots) : 0);\r\ngoto found;\r\n}\r\nindex += stride;\r\nif (index >= io_tlb_nslabs)\r\nindex = 0;\r\n} while (index != wrap);\r\nnot_found:\r\nspin_unlock_irqrestore(&io_tlb_lock, flags);\r\nif (printk_ratelimit())\r\ndev_warn(hwdev, "swiotlb buffer is full (sz: %zd bytes)\n", size);\r\nreturn SWIOTLB_MAP_ERROR;\r\nfound:\r\nspin_unlock_irqrestore(&io_tlb_lock, flags);\r\nfor (i = 0; i < nslots; i++)\r\nio_tlb_orig_addr[index+i] = orig_addr + (i << IO_TLB_SHIFT);\r\nif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&\r\n(dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))\r\nswiotlb_bounce(orig_addr, tlb_addr, size, DMA_TO_DEVICE);\r\nreturn tlb_addr;\r\n}\r\nstatic phys_addr_t\r\nmap_single(struct device *hwdev, phys_addr_t phys, size_t size,\r\nenum dma_data_direction dir, unsigned long attrs)\r\n{\r\ndma_addr_t start_dma_addr;\r\nif (swiotlb_force == SWIOTLB_NO_FORCE) {\r\ndev_warn_ratelimited(hwdev, "Cannot do DMA to address %pa\n",\r\n&phys);\r\nreturn SWIOTLB_MAP_ERROR;\r\n}\r\nstart_dma_addr = phys_to_dma(hwdev, io_tlb_start);\r\nreturn swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size,\r\ndir, attrs);\r\n}\r\nvoid swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nunsigned long flags;\r\nint i, count, nslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;\r\nint index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;\r\nphys_addr_t orig_addr = io_tlb_orig_addr[index];\r\nif (orig_addr != INVALID_PHYS_ADDR &&\r\n!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&\r\n((dir == DMA_FROM_DEVICE) || (dir == DMA_BIDIRECTIONAL)))\r\nswiotlb_bounce(orig_addr, tlb_addr, size, DMA_FROM_DEVICE);\r\nspin_lock_irqsave(&io_tlb_lock, flags);\r\n{\r\ncount = ((index + nslots) < ALIGN(index + 1, IO_TLB_SEGSIZE) ?\r\nio_tlb_list[index + nslots] : 0);\r\nfor (i = index + nslots - 1; i >= index; i--) {\r\nio_tlb_list[i] = ++count;\r\nio_tlb_orig_addr[i] = INVALID_PHYS_ADDR;\r\n}\r\nfor (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE -1) && io_tlb_list[i]; i--)\r\nio_tlb_list[i] = ++count;\r\n}\r\nspin_unlock_irqrestore(&io_tlb_lock, flags);\r\n}\r\nvoid swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nint index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;\r\nphys_addr_t orig_addr = io_tlb_orig_addr[index];\r\nif (orig_addr == INVALID_PHYS_ADDR)\r\nreturn;\r\norig_addr += (unsigned long)tlb_addr & ((1 << IO_TLB_SHIFT) - 1);\r\nswitch (target) {\r\ncase SYNC_FOR_CPU:\r\nif (likely(dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))\r\nswiotlb_bounce(orig_addr, tlb_addr,\r\nsize, DMA_FROM_DEVICE);\r\nelse\r\nBUG_ON(dir != DMA_TO_DEVICE);\r\nbreak;\r\ncase SYNC_FOR_DEVICE:\r\nif (likely(dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))\r\nswiotlb_bounce(orig_addr, tlb_addr,\r\nsize, DMA_TO_DEVICE);\r\nelse\r\nBUG_ON(dir != DMA_FROM_DEVICE);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nvoid *\r\nswiotlb_alloc_coherent(struct device *hwdev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t flags)\r\n{\r\ndma_addr_t dev_addr;\r\nvoid *ret;\r\nint order = get_order(size);\r\nu64 dma_mask = DMA_BIT_MASK(32);\r\nif (hwdev && hwdev->coherent_dma_mask)\r\ndma_mask = hwdev->coherent_dma_mask;\r\nret = (void *)__get_free_pages(flags, order);\r\nif (ret) {\r\ndev_addr = swiotlb_virt_to_bus(hwdev, ret);\r\nif (dev_addr + size - 1 > dma_mask) {\r\nfree_pages((unsigned long) ret, order);\r\nret = NULL;\r\n}\r\n}\r\nif (!ret) {\r\nphys_addr_t paddr = map_single(hwdev, 0, size,\r\nDMA_FROM_DEVICE, 0);\r\nif (paddr == SWIOTLB_MAP_ERROR)\r\ngoto err_warn;\r\nret = phys_to_virt(paddr);\r\ndev_addr = phys_to_dma(hwdev, paddr);\r\nif (dev_addr + size - 1 > dma_mask) {\r\nprintk("hwdev DMA mask = 0x%016Lx, dev_addr = 0x%016Lx\n",\r\n(unsigned long long)dma_mask,\r\n(unsigned long long)dev_addr);\r\nswiotlb_tbl_unmap_single(hwdev, paddr,\r\nsize, DMA_TO_DEVICE,\r\nDMA_ATTR_SKIP_CPU_SYNC);\r\ngoto err_warn;\r\n}\r\n}\r\n*dma_handle = dev_addr;\r\nmemset(ret, 0, size);\r\nreturn ret;\r\nerr_warn:\r\npr_warn("swiotlb: coherent allocation failed for device %s size=%zu\n",\r\ndev_name(hwdev), size);\r\ndump_stack();\r\nreturn NULL;\r\n}\r\nvoid\r\nswiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,\r\ndma_addr_t dev_addr)\r\n{\r\nphys_addr_t paddr = dma_to_phys(hwdev, dev_addr);\r\nWARN_ON(irqs_disabled());\r\nif (!is_swiotlb_buffer(paddr))\r\nfree_pages((unsigned long)vaddr, get_order(size));\r\nelse\r\nswiotlb_tbl_unmap_single(hwdev, paddr, size, DMA_TO_DEVICE,\r\nDMA_ATTR_SKIP_CPU_SYNC);\r\n}\r\nstatic void\r\nswiotlb_full(struct device *dev, size_t size, enum dma_data_direction dir,\r\nint do_panic)\r\n{\r\nif (swiotlb_force == SWIOTLB_NO_FORCE)\r\nreturn;\r\ndev_err_ratelimited(dev, "DMA: Out of SW-IOMMU space for %zu bytes\n",\r\nsize);\r\nif (size <= io_tlb_overflow || !do_panic)\r\nreturn;\r\nif (dir == DMA_BIDIRECTIONAL)\r\npanic("DMA: Random memory could be DMA accessed\n");\r\nif (dir == DMA_FROM_DEVICE)\r\npanic("DMA: Random memory could be DMA written\n");\r\nif (dir == DMA_TO_DEVICE)\r\npanic("DMA: Random memory could be DMA read\n");\r\n}\r\ndma_addr_t swiotlb_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nphys_addr_t map, phys = page_to_phys(page) + offset;\r\ndma_addr_t dev_addr = phys_to_dma(dev, phys);\r\nBUG_ON(dir == DMA_NONE);\r\nif (dma_capable(dev, dev_addr, size) && swiotlb_force != SWIOTLB_FORCE)\r\nreturn dev_addr;\r\ntrace_swiotlb_bounced(dev, dev_addr, size, swiotlb_force);\r\nmap = map_single(dev, phys, size, dir, attrs);\r\nif (map == SWIOTLB_MAP_ERROR) {\r\nswiotlb_full(dev, size, dir, 1);\r\nreturn phys_to_dma(dev, io_tlb_overflow_buffer);\r\n}\r\ndev_addr = phys_to_dma(dev, map);\r\nif (dma_capable(dev, dev_addr, size))\r\nreturn dev_addr;\r\nattrs |= DMA_ATTR_SKIP_CPU_SYNC;\r\nswiotlb_tbl_unmap_single(dev, map, size, dir, attrs);\r\nreturn phys_to_dma(dev, io_tlb_overflow_buffer);\r\n}\r\nstatic void unmap_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nphys_addr_t paddr = dma_to_phys(hwdev, dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nif (is_swiotlb_buffer(paddr)) {\r\nswiotlb_tbl_unmap_single(hwdev, paddr, size, dir, attrs);\r\nreturn;\r\n}\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nunmap_single(hwdev, dev_addr, size, dir, attrs);\r\n}\r\nstatic void\r\nswiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nphys_addr_t paddr = dma_to_phys(hwdev, dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nif (is_swiotlb_buffer(paddr)) {\r\nswiotlb_tbl_sync_single(hwdev, paddr, size, dir, target);\r\nreturn;\r\n}\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid\r\nswiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nswiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nswiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,\r\nenum dma_data_direction dir, unsigned long attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i) {\r\nphys_addr_t paddr = sg_phys(sg);\r\ndma_addr_t dev_addr = phys_to_dma(hwdev, paddr);\r\nif (swiotlb_force == SWIOTLB_FORCE ||\r\n!dma_capable(hwdev, dev_addr, sg->length)) {\r\nphys_addr_t map = map_single(hwdev, sg_phys(sg),\r\nsg->length, dir, attrs);\r\nif (map == SWIOTLB_MAP_ERROR) {\r\nswiotlb_full(hwdev, sg->length, dir, 0);\r\nattrs |= DMA_ATTR_SKIP_CPU_SYNC;\r\nswiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,\r\nattrs);\r\nsg_dma_len(sgl) = 0;\r\nreturn 0;\r\n}\r\nsg->dma_address = phys_to_dma(hwdev, map);\r\n} else\r\nsg->dma_address = dev_addr;\r\nsg_dma_len(sg) = sg->length;\r\n}\r\nreturn nelems;\r\n}\r\nvoid\r\nswiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i)\r\nunmap_single(hwdev, sg->dma_address, sg_dma_len(sg), dir,\r\nattrs);\r\n}\r\nstatic void\r\nswiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, nelems, i)\r\nswiotlb_sync_single(hwdev, sg->dma_address,\r\nsg_dma_len(sg), dir, target);\r\n}\r\nvoid\r\nswiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nswiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nswiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)\r\n{\r\nreturn (dma_addr == phys_to_dma(hwdev, io_tlb_overflow_buffer));\r\n}\r\nint\r\nswiotlb_dma_supported(struct device *hwdev, u64 mask)\r\n{\r\nreturn phys_to_dma(hwdev, io_tlb_end - 1) <= mask;\r\n}
