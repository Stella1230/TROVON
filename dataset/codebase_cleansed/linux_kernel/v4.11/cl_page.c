static void cl_page_get_trust(struct cl_page *page)\r\n{\r\nLASSERT(atomic_read(&page->cp_ref) > 0);\r\natomic_inc(&page->cp_ref);\r\n}\r\nstatic const struct cl_page_slice *\r\ncl_page_at_trusted(const struct cl_page *page,\r\nconst struct lu_device_type *dtype)\r\n{\r\nconst struct cl_page_slice *slice;\r\nlist_for_each_entry(slice, &page->cp_layers, cpl_linkage) {\r\nif (slice->cpl_obj->co_lu.lo_dev->ld_type == dtype)\r\nreturn slice;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void cl_page_free(const struct lu_env *env, struct cl_page *page)\r\n{\r\nstruct cl_object *obj = page->cp_obj;\r\nPASSERT(env, page, list_empty(&page->cp_batch));\r\nPASSERT(env, page, !page->cp_owner);\r\nPASSERT(env, page, page->cp_state == CPS_FREEING);\r\nwhile (!list_empty(&page->cp_layers)) {\r\nstruct cl_page_slice *slice;\r\nslice = list_entry(page->cp_layers.next,\r\nstruct cl_page_slice, cpl_linkage);\r\nlist_del_init(page->cp_layers.next);\r\nif (unlikely(slice->cpl_ops->cpo_fini))\r\nslice->cpl_ops->cpo_fini(env, slice);\r\n}\r\nlu_object_ref_del_at(&obj->co_lu, &page->cp_obj_ref, "cl_page", page);\r\ncl_object_put(env, obj);\r\nlu_ref_fini(&page->cp_reference);\r\nkfree(page);\r\n}\r\nstatic inline void cl_page_state_set_trust(struct cl_page *page,\r\nenum cl_page_state state)\r\n{\r\n*(enum cl_page_state *)&page->cp_state = state;\r\n}\r\nstruct cl_page *cl_page_alloc(const struct lu_env *env,\r\nstruct cl_object *o, pgoff_t ind,\r\nstruct page *vmpage,\r\nenum cl_page_type type)\r\n{\r\nstruct cl_page *page;\r\nstruct lu_object_header *head;\r\npage = kzalloc(cl_object_header(o)->coh_page_bufsize, GFP_NOFS);\r\nif (page) {\r\nint result = 0;\r\natomic_set(&page->cp_ref, 1);\r\npage->cp_obj = o;\r\ncl_object_get(o);\r\nlu_object_ref_add_at(&o->co_lu, &page->cp_obj_ref, "cl_page",\r\npage);\r\npage->cp_vmpage = vmpage;\r\ncl_page_state_set_trust(page, CPS_CACHED);\r\npage->cp_type = type;\r\nINIT_LIST_HEAD(&page->cp_layers);\r\nINIT_LIST_HEAD(&page->cp_batch);\r\nlu_ref_init(&page->cp_reference);\r\nhead = o->co_lu.lo_header;\r\nlist_for_each_entry(o, &head->loh_layers, co_lu.lo_linkage) {\r\nif (o->co_ops->coo_page_init) {\r\nresult = o->co_ops->coo_page_init(env, o, page,\r\nind);\r\nif (result != 0) {\r\ncl_page_delete0(env, page);\r\ncl_page_free(env, page);\r\npage = ERR_PTR(result);\r\nbreak;\r\n}\r\n}\r\n}\r\n} else {\r\npage = ERR_PTR(-ENOMEM);\r\n}\r\nreturn page;\r\n}\r\nstruct cl_page *cl_page_find(const struct lu_env *env,\r\nstruct cl_object *o,\r\npgoff_t idx, struct page *vmpage,\r\nenum cl_page_type type)\r\n{\r\nstruct cl_page *page = NULL;\r\nstruct cl_object_header *hdr;\r\nLASSERT(type == CPT_CACHEABLE || type == CPT_TRANSIENT);\r\nmight_sleep();\r\nhdr = cl_object_header(o);\r\nCDEBUG(D_PAGE, "%lu@"DFID" %p %lx %d\n",\r\nidx, PFID(&hdr->coh_lu.loh_fid), vmpage, vmpage->private, type);\r\nif (type == CPT_CACHEABLE) {\r\nKLASSERT(PageLocked(vmpage));\r\npage = cl_vmpage_page(vmpage, o);\r\nif (page)\r\nreturn page;\r\n}\r\npage = cl_page_alloc(env, o, idx, vmpage, type);\r\nreturn page;\r\n}\r\nstatic inline int cl_page_invariant(const struct cl_page *pg)\r\n{\r\nreturn cl_page_in_use_noref(pg);\r\n}\r\nstatic void cl_page_state_set0(const struct lu_env *env,\r\nstruct cl_page *page, enum cl_page_state state)\r\n{\r\nenum cl_page_state old;\r\nstatic const int allowed_transitions[CPS_NR][CPS_NR] = {\r\n[CPS_CACHED] = {\r\n[CPS_CACHED] = 0,\r\n[CPS_OWNED] = 1,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 1,\r\n[CPS_FREEING] = 1,\r\n},\r\n[CPS_OWNED] = {\r\n[CPS_CACHED] = 1,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 1,\r\n[CPS_PAGEOUT] = 1,\r\n[CPS_FREEING] = 1,\r\n},\r\n[CPS_PAGEIN] = {\r\n[CPS_CACHED] = 1,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 0,\r\n[CPS_FREEING] = 0,\r\n},\r\n[CPS_PAGEOUT] = {\r\n[CPS_CACHED] = 1,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 0,\r\n[CPS_FREEING] = 0,\r\n},\r\n[CPS_FREEING] = {\r\n[CPS_CACHED] = 0,\r\n[CPS_OWNED] = 0,\r\n[CPS_PAGEIN] = 0,\r\n[CPS_PAGEOUT] = 0,\r\n[CPS_FREEING] = 0,\r\n}\r\n};\r\nold = page->cp_state;\r\nPASSERT(env, page, allowed_transitions[old][state]);\r\nCL_PAGE_HEADER(D_TRACE, env, page, "%d -> %d\n", old, state);\r\nPASSERT(env, page, page->cp_state == old);\r\nPASSERT(env, page, equi(state == CPS_OWNED, page->cp_owner));\r\ncl_page_state_set_trust(page, state);\r\n}\r\nstatic void cl_page_state_set(const struct lu_env *env,\r\nstruct cl_page *page, enum cl_page_state state)\r\n{\r\ncl_page_state_set0(env, page, state);\r\n}\r\nvoid cl_page_get(struct cl_page *page)\r\n{\r\ncl_page_get_trust(page);\r\n}\r\nvoid cl_page_put(const struct lu_env *env, struct cl_page *page)\r\n{\r\nCL_PAGE_HEADER(D_TRACE, env, page, "%d\n",\r\natomic_read(&page->cp_ref));\r\nif (atomic_dec_and_test(&page->cp_ref)) {\r\nLASSERT(page->cp_state == CPS_FREEING);\r\nLASSERT(atomic_read(&page->cp_ref) == 0);\r\nPASSERT(env, page, !page->cp_owner);\r\nPASSERT(env, page, list_empty(&page->cp_batch));\r\ncl_page_free(env, page);\r\n}\r\n}\r\nstruct cl_page *cl_vmpage_page(struct page *vmpage, struct cl_object *obj)\r\n{\r\nstruct cl_page *page;\r\nKLASSERT(PageLocked(vmpage));\r\npage = (struct cl_page *)vmpage->private;\r\nif (page) {\r\ncl_page_get_trust(page);\r\nLASSERT(page->cp_type == CPT_CACHEABLE);\r\n}\r\nreturn page;\r\n}\r\nconst struct cl_page_slice *cl_page_at(const struct cl_page *page,\r\nconst struct lu_device_type *dtype)\r\n{\r\nreturn cl_page_at_trusted(page, dtype);\r\n}\r\nstatic int cl_page_invoke(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *page, ptrdiff_t op)\r\n{\r\nPINVRNT(env, page, cl_object_same(page->cp_obj, io->ci_obj));\r\nreturn CL_PAGE_INVOKE(env, page, op,\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *),\r\nio);\r\n}\r\nstatic void cl_page_invoid(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *page, ptrdiff_t op)\r\n{\r\nPINVRNT(env, page, cl_object_same(page->cp_obj, io->ci_obj));\r\nCL_PAGE_INVOID(env, page, op,\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *), io);\r\n}\r\nstatic void cl_page_owner_clear(struct cl_page *page)\r\n{\r\nif (page->cp_owner) {\r\nLASSERT(page->cp_owner->ci_owned_nr > 0);\r\npage->cp_owner->ci_owned_nr--;\r\npage->cp_owner = NULL;\r\n}\r\n}\r\nstatic void cl_page_owner_set(struct cl_page *page)\r\n{\r\npage->cp_owner->ci_owned_nr++;\r\n}\r\nvoid cl_page_disown0(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nenum cl_page_state state;\r\nstate = pg->cp_state;\r\nPINVRNT(env, pg, state == CPS_OWNED || state == CPS_FREEING);\r\nPINVRNT(env, pg, cl_page_invariant(pg) || state == CPS_FREEING);\r\ncl_page_owner_clear(pg);\r\nif (state == CPS_OWNED)\r\ncl_page_state_set(env, pg, CPS_CACHED);\r\nCL_PAGE_INVOID_REVERSE(env, pg, CL_PAGE_OP(cpo_disown),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *),\r\nio);\r\n}\r\nint cl_page_is_owned(const struct cl_page *pg, const struct cl_io *io)\r\n{\r\nstruct cl_io *top = cl_io_top((struct cl_io *)io);\r\nLINVRNT(cl_object_same(pg->cp_obj, io->ci_obj));\r\nreturn pg->cp_state == CPS_OWNED && pg->cp_owner == top;\r\n}\r\nstatic int cl_page_own0(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg, int nonblock)\r\n{\r\nint result;\r\nPINVRNT(env, pg, !cl_page_is_owned(pg, io));\r\nio = cl_io_top(io);\r\nif (pg->cp_state == CPS_FREEING) {\r\nresult = -ENOENT;\r\n} else {\r\nresult = CL_PAGE_INVOKE(env, pg, CL_PAGE_OP(cpo_own),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *,\r\nstruct cl_io *, int),\r\nio, nonblock);\r\nif (result == 0) {\r\nPASSERT(env, pg, !pg->cp_owner);\r\npg->cp_owner = cl_io_top(io);\r\ncl_page_owner_set(pg);\r\nif (pg->cp_state != CPS_FREEING) {\r\ncl_page_state_set(env, pg, CPS_OWNED);\r\n} else {\r\ncl_page_disown0(env, io, pg);\r\nresult = -ENOENT;\r\n}\r\n}\r\n}\r\nPINVRNT(env, pg, ergo(result == 0, cl_page_invariant(pg)));\r\nreturn result;\r\n}\r\nint cl_page_own(const struct lu_env *env, struct cl_io *io, struct cl_page *pg)\r\n{\r\nreturn cl_page_own0(env, io, pg, 0);\r\n}\r\nint cl_page_own_try(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg)\r\n{\r\nreturn cl_page_own0(env, io, pg, 1);\r\n}\r\nvoid cl_page_assume(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_object_same(pg->cp_obj, io->ci_obj));\r\nio = cl_io_top(io);\r\ncl_page_invoid(env, io, pg, CL_PAGE_OP(cpo_assume));\r\nPASSERT(env, pg, !pg->cp_owner);\r\npg->cp_owner = cl_io_top(io);\r\ncl_page_owner_set(pg);\r\ncl_page_state_set(env, pg, CPS_OWNED);\r\n}\r\nvoid cl_page_unassume(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nio = cl_io_top(io);\r\ncl_page_owner_clear(pg);\r\ncl_page_state_set(env, pg, CPS_CACHED);\r\nCL_PAGE_INVOID_REVERSE(env, pg, CL_PAGE_OP(cpo_unassume),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, struct cl_io *),\r\nio);\r\n}\r\nvoid cl_page_disown(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io) ||\r\npg->cp_state == CPS_FREEING);\r\nio = cl_io_top(io);\r\ncl_page_disown0(env, io, pg);\r\n}\r\nvoid cl_page_discard(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\ncl_page_invoid(env, io, pg, CL_PAGE_OP(cpo_discard));\r\n}\r\nstatic void cl_page_delete0(const struct lu_env *env, struct cl_page *pg)\r\n{\r\nPASSERT(env, pg, pg->cp_state != CPS_FREEING);\r\ncl_page_owner_clear(pg);\r\ncl_page_state_set0(env, pg, CPS_FREEING);\r\nCL_PAGE_INVOID_REVERSE(env, pg, CL_PAGE_OP(cpo_delete),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *));\r\n}\r\nvoid cl_page_delete(const struct lu_env *env, struct cl_page *pg)\r\n{\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\ncl_page_delete0(env, pg);\r\n}\r\nvoid cl_page_export(const struct lu_env *env, struct cl_page *pg, int uptodate)\r\n{\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nCL_PAGE_INVOID(env, pg, CL_PAGE_OP(cpo_export),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, int), uptodate);\r\n}\r\nint cl_page_is_vmlocked(const struct lu_env *env, const struct cl_page *pg)\r\n{\r\nint result;\r\nconst struct cl_page_slice *slice;\r\nslice = container_of(pg->cp_layers.next,\r\nconst struct cl_page_slice, cpl_linkage);\r\nPASSERT(env, pg, slice->cpl_ops->cpo_is_vmlocked);\r\nresult = slice->cpl_ops->cpo_is_vmlocked(env, slice);\r\nPASSERT(env, pg, result == -EBUSY || result == -ENODATA);\r\nreturn result == -EBUSY;\r\n}\r\nstatic enum cl_page_state cl_req_type_state(enum cl_req_type crt)\r\n{\r\nreturn crt == CRT_WRITE ? CPS_PAGEOUT : CPS_PAGEIN;\r\n}\r\nstatic void cl_page_io_start(const struct lu_env *env,\r\nstruct cl_page *pg, enum cl_req_type crt)\r\n{\r\ncl_page_owner_clear(pg);\r\ncl_page_state_set(env, pg, cl_req_type_state(crt));\r\n}\r\nint cl_page_prep(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg, enum cl_req_type crt)\r\n{\r\nint result;\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nPINVRNT(env, pg, crt < CRT_NR);\r\nif (crt >= CRT_NR)\r\nreturn -EINVAL;\r\nresult = cl_page_invoke(env, io, pg, CL_PAGE_OP(io[crt].cpo_prep));\r\nif (result == 0)\r\ncl_page_io_start(env, pg, crt);\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", crt, result);\r\nreturn result;\r\n}\r\nvoid cl_page_completion(const struct lu_env *env,\r\nstruct cl_page *pg, enum cl_req_type crt, int ioret)\r\n{\r\nstruct cl_sync_io *anchor = pg->cp_sync_io;\r\nPASSERT(env, pg, crt < CRT_NR);\r\nPASSERT(env, pg, pg->cp_state == cl_req_type_state(crt));\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", crt, ioret);\r\ncl_page_state_set(env, pg, CPS_CACHED);\r\nif (crt >= CRT_NR)\r\nreturn;\r\nCL_PAGE_INVOID_REVERSE(env, pg, CL_PAGE_OP(io[crt].cpo_completion),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, int), ioret);\r\nif (anchor) {\r\nLASSERT(pg->cp_sync_io == anchor);\r\npg->cp_sync_io = NULL;\r\ncl_sync_io_note(env, anchor, ioret);\r\n}\r\n}\r\nint cl_page_make_ready(const struct lu_env *env, struct cl_page *pg,\r\nenum cl_req_type crt)\r\n{\r\nint result;\r\nPINVRNT(env, pg, crt < CRT_NR);\r\nif (crt >= CRT_NR)\r\nreturn -EINVAL;\r\nresult = CL_PAGE_INVOKE(env, pg, CL_PAGE_OP(io[crt].cpo_make_ready),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *));\r\nif (result == 0) {\r\nPASSERT(env, pg, pg->cp_state == CPS_CACHED);\r\ncl_page_io_start(env, pg, crt);\r\n}\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", crt, result);\r\nreturn result;\r\n}\r\nint cl_page_flush(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *pg)\r\n{\r\nint result;\r\nPINVRNT(env, pg, cl_page_is_owned(pg, io));\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nresult = cl_page_invoke(env, io, pg, CL_PAGE_OP(cpo_flush));\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d\n", result);\r\nreturn result;\r\n}\r\nvoid cl_page_clip(const struct lu_env *env, struct cl_page *pg,\r\nint from, int to)\r\n{\r\nPINVRNT(env, pg, cl_page_invariant(pg));\r\nCL_PAGE_HEADER(D_TRACE, env, pg, "%d %d\n", from, to);\r\nCL_PAGE_INVOID(env, pg, CL_PAGE_OP(cpo_clip),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *, int, int),\r\nfrom, to);\r\n}\r\nvoid cl_page_header_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer, const struct cl_page *pg)\r\n{\r\n(*printer)(env, cookie,\r\n"page@%p[%d %p %d %d %p]\n",\r\npg, atomic_read(&pg->cp_ref), pg->cp_obj,\r\npg->cp_state, pg->cp_type,\r\npg->cp_owner);\r\n}\r\nvoid cl_page_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer, const struct cl_page *pg)\r\n{\r\ncl_page_header_print(env, cookie, printer, pg);\r\nCL_PAGE_INVOKE(env, (struct cl_page *)pg, CL_PAGE_OP(cpo_print),\r\n(const struct lu_env *env,\r\nconst struct cl_page_slice *slice,\r\nvoid *cookie, lu_printer_t p), cookie, printer);\r\n(*printer)(env, cookie, "end page@%p\n", pg);\r\n}\r\nint cl_page_cancel(const struct lu_env *env, struct cl_page *page)\r\n{\r\nreturn CL_PAGE_INVOKE(env, page, CL_PAGE_OP(cpo_cancel),\r\n(const struct lu_env *,\r\nconst struct cl_page_slice *));\r\n}\r\nloff_t cl_offset(const struct cl_object *obj, pgoff_t idx)\r\n{\r\nreturn (loff_t)idx << PAGE_SHIFT;\r\n}\r\npgoff_t cl_index(const struct cl_object *obj, loff_t offset)\r\n{\r\nreturn offset >> PAGE_SHIFT;\r\n}\r\nsize_t cl_page_size(const struct cl_object *obj)\r\n{\r\nreturn 1UL << PAGE_SHIFT;\r\n}\r\nvoid cl_page_slice_add(struct cl_page *page, struct cl_page_slice *slice,\r\nstruct cl_object *obj, pgoff_t index,\r\nconst struct cl_page_operations *ops)\r\n{\r\nlist_add_tail(&slice->cpl_linkage, &page->cp_layers);\r\nslice->cpl_obj = obj;\r\nslice->cpl_index = index;\r\nslice->cpl_ops = ops;\r\nslice->cpl_page = page;\r\n}\r\nstruct cl_client_cache *cl_cache_init(unsigned long lru_page_max)\r\n{\r\nstruct cl_client_cache *cache = NULL;\r\ncache = kzalloc(sizeof(*cache), GFP_KERNEL);\r\nif (!cache)\r\nreturn NULL;\r\natomic_set(&cache->ccc_users, 1);\r\ncache->ccc_lru_max = lru_page_max;\r\natomic_long_set(&cache->ccc_lru_left, lru_page_max);\r\nspin_lock_init(&cache->ccc_lru_lock);\r\nINIT_LIST_HEAD(&cache->ccc_lru);\r\natomic_long_set(&cache->ccc_unstable_nr, 0);\r\ninit_waitqueue_head(&cache->ccc_unstable_waitq);\r\nreturn cache;\r\n}\r\nvoid cl_cache_incref(struct cl_client_cache *cache)\r\n{\r\natomic_inc(&cache->ccc_users);\r\n}\r\nvoid cl_cache_decref(struct cl_client_cache *cache)\r\n{\r\nif (atomic_dec_and_test(&cache->ccc_users))\r\nkfree(cache);\r\n}
