static inline struct acpi_processor_performance *to_perf_data(struct acpi_cpufreq_data *data)\r\n{\r\nreturn per_cpu_ptr(acpi_perf_data, data->acpi_perf_cpu);\r\n}\r\nstatic bool boost_state(unsigned int cpu)\r\n{\r\nu32 lo, hi;\r\nu64 msr;\r\nswitch (boot_cpu_data.x86_vendor) {\r\ncase X86_VENDOR_INTEL:\r\nrdmsr_on_cpu(cpu, MSR_IA32_MISC_ENABLE, &lo, &hi);\r\nmsr = lo | ((u64)hi << 32);\r\nreturn !(msr & MSR_IA32_MISC_ENABLE_TURBO_DISABLE);\r\ncase X86_VENDOR_AMD:\r\nrdmsr_on_cpu(cpu, MSR_K7_HWCR, &lo, &hi);\r\nmsr = lo | ((u64)hi << 32);\r\nreturn !(msr & MSR_K7_HWCR_CPB_DIS);\r\n}\r\nreturn false;\r\n}\r\nstatic int boost_set_msr(bool enable)\r\n{\r\nu32 msr_addr;\r\nu64 msr_mask, val;\r\nswitch (boot_cpu_data.x86_vendor) {\r\ncase X86_VENDOR_INTEL:\r\nmsr_addr = MSR_IA32_MISC_ENABLE;\r\nmsr_mask = MSR_IA32_MISC_ENABLE_TURBO_DISABLE;\r\nbreak;\r\ncase X86_VENDOR_AMD:\r\nmsr_addr = MSR_K7_HWCR;\r\nmsr_mask = MSR_K7_HWCR_CPB_DIS;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nrdmsrl(msr_addr, val);\r\nif (enable)\r\nval &= ~msr_mask;\r\nelse\r\nval |= msr_mask;\r\nwrmsrl(msr_addr, val);\r\nreturn 0;\r\n}\r\nstatic void boost_set_msr_each(void *p_en)\r\n{\r\nbool enable = (bool) p_en;\r\nboost_set_msr(enable);\r\n}\r\nstatic int set_boost(int val)\r\n{\r\nget_online_cpus();\r\non_each_cpu(boost_set_msr_each, (void *)(long)val, 1);\r\nput_online_cpus();\r\npr_debug("Core Boosting %sabled.\n", val ? "en" : "dis");\r\nreturn 0;\r\n}\r\nstatic ssize_t show_freqdomain_cpus(struct cpufreq_policy *policy, char *buf)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\nif (unlikely(!data))\r\nreturn -ENODEV;\r\nreturn cpufreq_show_cpus(data->freqdomain_cpus, buf);\r\n}\r\nstatic ssize_t store_cpb(struct cpufreq_policy *policy, const char *buf,\r\nsize_t count)\r\n{\r\nint ret;\r\nunsigned int val = 0;\r\nif (!acpi_cpufreq_driver.set_boost)\r\nreturn -EINVAL;\r\nret = kstrtouint(buf, 10, &val);\r\nif (ret || val > 1)\r\nreturn -EINVAL;\r\nset_boost(val);\r\nreturn count;\r\n}\r\nstatic ssize_t show_cpb(struct cpufreq_policy *policy, char *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", acpi_cpufreq_driver.boost_enabled);\r\n}\r\nstatic int check_est_cpu(unsigned int cpuid)\r\n{\r\nstruct cpuinfo_x86 *cpu = &cpu_data(cpuid);\r\nreturn cpu_has(cpu, X86_FEATURE_EST);\r\n}\r\nstatic int check_amd_hwpstate_cpu(unsigned int cpuid)\r\n{\r\nstruct cpuinfo_x86 *cpu = &cpu_data(cpuid);\r\nreturn cpu_has(cpu, X86_FEATURE_HW_PSTATE);\r\n}\r\nstatic unsigned extract_io(struct cpufreq_policy *policy, u32 value)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\nstruct acpi_processor_performance *perf;\r\nint i;\r\nperf = to_perf_data(data);\r\nfor (i = 0; i < perf->state_count; i++) {\r\nif (value == perf->states[i].status)\r\nreturn policy->freq_table[i].frequency;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned extract_msr(struct cpufreq_policy *policy, u32 msr)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\nstruct cpufreq_frequency_table *pos;\r\nstruct acpi_processor_performance *perf;\r\nif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)\r\nmsr &= AMD_MSR_RANGE;\r\nelse\r\nmsr &= INTEL_MSR_RANGE;\r\nperf = to_perf_data(data);\r\ncpufreq_for_each_entry(pos, policy->freq_table)\r\nif (msr == perf->states[pos->driver_data].status)\r\nreturn pos->frequency;\r\nreturn policy->freq_table[0].frequency;\r\n}\r\nstatic unsigned extract_freq(struct cpufreq_policy *policy, u32 val)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\nswitch (data->cpu_feature) {\r\ncase SYSTEM_INTEL_MSR_CAPABLE:\r\ncase SYSTEM_AMD_MSR_CAPABLE:\r\nreturn extract_msr(policy, val);\r\ncase SYSTEM_IO_CAPABLE:\r\nreturn extract_io(policy, val);\r\ndefault:\r\nreturn 0;\r\n}\r\n}\r\nstatic u32 cpu_freq_read_intel(struct acpi_pct_register *not_used)\r\n{\r\nu32 val, dummy;\r\nrdmsr(MSR_IA32_PERF_CTL, val, dummy);\r\nreturn val;\r\n}\r\nstatic void cpu_freq_write_intel(struct acpi_pct_register *not_used, u32 val)\r\n{\r\nu32 lo, hi;\r\nrdmsr(MSR_IA32_PERF_CTL, lo, hi);\r\nlo = (lo & ~INTEL_MSR_RANGE) | (val & INTEL_MSR_RANGE);\r\nwrmsr(MSR_IA32_PERF_CTL, lo, hi);\r\n}\r\nstatic u32 cpu_freq_read_amd(struct acpi_pct_register *not_used)\r\n{\r\nu32 val, dummy;\r\nrdmsr(MSR_AMD_PERF_CTL, val, dummy);\r\nreturn val;\r\n}\r\nstatic void cpu_freq_write_amd(struct acpi_pct_register *not_used, u32 val)\r\n{\r\nwrmsr(MSR_AMD_PERF_CTL, val, 0);\r\n}\r\nstatic u32 cpu_freq_read_io(struct acpi_pct_register *reg)\r\n{\r\nu32 val;\r\nacpi_os_read_port(reg->address, &val, reg->bit_width);\r\nreturn val;\r\n}\r\nstatic void cpu_freq_write_io(struct acpi_pct_register *reg, u32 val)\r\n{\r\nacpi_os_write_port(reg->address, val, reg->bit_width);\r\n}\r\nstatic void do_drv_read(void *_cmd)\r\n{\r\nstruct drv_cmd *cmd = _cmd;\r\ncmd->val = cmd->func.read(cmd->reg);\r\n}\r\nstatic u32 drv_read(struct acpi_cpufreq_data *data, const struct cpumask *mask)\r\n{\r\nstruct acpi_processor_performance *perf = to_perf_data(data);\r\nstruct drv_cmd cmd = {\r\n.reg = &perf->control_register,\r\n.func.read = data->cpu_freq_read,\r\n};\r\nint err;\r\nerr = smp_call_function_any(mask, do_drv_read, &cmd, 1);\r\nWARN_ON_ONCE(err);\r\nreturn cmd.val;\r\n}\r\nstatic void do_drv_write(void *_cmd)\r\n{\r\nstruct drv_cmd *cmd = _cmd;\r\ncmd->func.write(cmd->reg, cmd->val);\r\n}\r\nstatic void drv_write(struct acpi_cpufreq_data *data,\r\nconst struct cpumask *mask, u32 val)\r\n{\r\nstruct acpi_processor_performance *perf = to_perf_data(data);\r\nstruct drv_cmd cmd = {\r\n.reg = &perf->control_register,\r\n.val = val,\r\n.func.write = data->cpu_freq_write,\r\n};\r\nint this_cpu;\r\nthis_cpu = get_cpu();\r\nif (cpumask_test_cpu(this_cpu, mask))\r\ndo_drv_write(&cmd);\r\nsmp_call_function_many(mask, do_drv_write, &cmd, 1);\r\nput_cpu();\r\n}\r\nstatic u32 get_cur_val(const struct cpumask *mask, struct acpi_cpufreq_data *data)\r\n{\r\nu32 val;\r\nif (unlikely(cpumask_empty(mask)))\r\nreturn 0;\r\nval = drv_read(data, mask);\r\npr_debug("get_cur_val = %u\n", val);\r\nreturn val;\r\n}\r\nstatic unsigned int get_cur_freq_on_cpu(unsigned int cpu)\r\n{\r\nstruct acpi_cpufreq_data *data;\r\nstruct cpufreq_policy *policy;\r\nunsigned int freq;\r\nunsigned int cached_freq;\r\npr_debug("get_cur_freq_on_cpu (%d)\n", cpu);\r\npolicy = cpufreq_cpu_get_raw(cpu);\r\nif (unlikely(!policy))\r\nreturn 0;\r\ndata = policy->driver_data;\r\nif (unlikely(!data || !policy->freq_table))\r\nreturn 0;\r\ncached_freq = policy->freq_table[to_perf_data(data)->state].frequency;\r\nfreq = extract_freq(policy, get_cur_val(cpumask_of(cpu), data));\r\nif (freq != cached_freq) {\r\ndata->resume = 1;\r\n}\r\npr_debug("cur freq = %u\n", freq);\r\nreturn freq;\r\n}\r\nstatic unsigned int check_freqs(struct cpufreq_policy *policy,\r\nconst struct cpumask *mask, unsigned int freq)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\nunsigned int cur_freq;\r\nunsigned int i;\r\nfor (i = 0; i < 100; i++) {\r\ncur_freq = extract_freq(policy, get_cur_val(mask, data));\r\nif (cur_freq == freq)\r\nreturn 1;\r\nudelay(10);\r\n}\r\nreturn 0;\r\n}\r\nstatic int acpi_cpufreq_target(struct cpufreq_policy *policy,\r\nunsigned int index)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\nstruct acpi_processor_performance *perf;\r\nconst struct cpumask *mask;\r\nunsigned int next_perf_state = 0;\r\nint result = 0;\r\nif (unlikely(!data)) {\r\nreturn -ENODEV;\r\n}\r\nperf = to_perf_data(data);\r\nnext_perf_state = policy->freq_table[index].driver_data;\r\nif (perf->state == next_perf_state) {\r\nif (unlikely(data->resume)) {\r\npr_debug("Called after resume, resetting to P%d\n",\r\nnext_perf_state);\r\ndata->resume = 0;\r\n} else {\r\npr_debug("Already at target state (P%d)\n",\r\nnext_perf_state);\r\nreturn 0;\r\n}\r\n}\r\nmask = policy->shared_type == CPUFREQ_SHARED_TYPE_ANY ?\r\ncpumask_of(policy->cpu) : policy->cpus;\r\ndrv_write(data, mask, perf->states[next_perf_state].control);\r\nif (acpi_pstate_strict) {\r\nif (!check_freqs(policy, mask,\r\npolicy->freq_table[index].frequency)) {\r\npr_debug("acpi_cpufreq_target failed (%d)\n",\r\npolicy->cpu);\r\nresult = -EAGAIN;\r\n}\r\n}\r\nif (!result)\r\nperf->state = next_perf_state;\r\nreturn result;\r\n}\r\nunsigned int acpi_cpufreq_fast_switch(struct cpufreq_policy *policy,\r\nunsigned int target_freq)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\nstruct acpi_processor_performance *perf;\r\nstruct cpufreq_frequency_table *entry;\r\nunsigned int next_perf_state, next_freq, index;\r\nif (policy->cached_target_freq == target_freq)\r\nindex = policy->cached_resolved_idx;\r\nelse\r\nindex = cpufreq_table_find_index_dl(policy, target_freq);\r\nentry = &policy->freq_table[index];\r\nnext_freq = entry->frequency;\r\nnext_perf_state = entry->driver_data;\r\nperf = to_perf_data(data);\r\nif (perf->state == next_perf_state) {\r\nif (unlikely(data->resume))\r\ndata->resume = 0;\r\nelse\r\nreturn next_freq;\r\n}\r\ndata->cpu_freq_write(&perf->control_register,\r\nperf->states[next_perf_state].control);\r\nperf->state = next_perf_state;\r\nreturn next_freq;\r\n}\r\nstatic unsigned long\r\nacpi_cpufreq_guess_freq(struct acpi_cpufreq_data *data, unsigned int cpu)\r\n{\r\nstruct acpi_processor_performance *perf;\r\nperf = to_perf_data(data);\r\nif (cpu_khz) {\r\nunsigned int i;\r\nunsigned long freq;\r\nunsigned long freqn = perf->states[0].core_frequency * 1000;\r\nfor (i = 0; i < (perf->state_count-1); i++) {\r\nfreq = freqn;\r\nfreqn = perf->states[i+1].core_frequency * 1000;\r\nif ((2 * cpu_khz) > (freqn + freq)) {\r\nperf->state = i;\r\nreturn freq;\r\n}\r\n}\r\nperf->state = perf->state_count-1;\r\nreturn freqn;\r\n} else {\r\nperf->state = 0;\r\nreturn perf->states[0].core_frequency * 1000;\r\n}\r\n}\r\nstatic void free_acpi_perf_data(void)\r\n{\r\nunsigned int i;\r\nfor_each_possible_cpu(i)\r\nfree_cpumask_var(per_cpu_ptr(acpi_perf_data, i)\r\n->shared_cpu_map);\r\nfree_percpu(acpi_perf_data);\r\n}\r\nstatic int cpufreq_boost_online(unsigned int cpu)\r\n{\r\nreturn boost_set_msr(acpi_cpufreq_driver.boost_enabled);\r\n}\r\nstatic int cpufreq_boost_down_prep(unsigned int cpu)\r\n{\r\nreturn boost_set_msr(1);\r\n}\r\nstatic int __init acpi_cpufreq_early_init(void)\r\n{\r\nunsigned int i;\r\npr_debug("acpi_cpufreq_early_init\n");\r\nacpi_perf_data = alloc_percpu(struct acpi_processor_performance);\r\nif (!acpi_perf_data) {\r\npr_debug("Memory allocation error for acpi_perf_data.\n");\r\nreturn -ENOMEM;\r\n}\r\nfor_each_possible_cpu(i) {\r\nif (!zalloc_cpumask_var_node(\r\n&per_cpu_ptr(acpi_perf_data, i)->shared_cpu_map,\r\nGFP_KERNEL, cpu_to_node(i))) {\r\nfree_acpi_perf_data();\r\nreturn -ENOMEM;\r\n}\r\n}\r\nacpi_processor_preregister_performance(acpi_perf_data);\r\nreturn 0;\r\n}\r\nstatic int sw_any_bug_found(const struct dmi_system_id *d)\r\n{\r\nbios_with_sw_any_bug = 1;\r\nreturn 0;\r\n}\r\nstatic int acpi_cpufreq_blacklist(struct cpuinfo_x86 *c)\r\n{\r\nif (c->x86_vendor == X86_VENDOR_INTEL) {\r\nif ((c->x86 == 15) &&\r\n(c->x86_model == 6) &&\r\n(c->x86_mask == 8)) {\r\npr_info("Intel(R) Xeon(R) 7100 Errata AL30, processors may lock up on frequency changes: disabling acpi-cpufreq\n");\r\nreturn -ENODEV;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int acpi_cpufreq_cpu_init(struct cpufreq_policy *policy)\r\n{\r\nunsigned int i;\r\nunsigned int valid_states = 0;\r\nunsigned int cpu = policy->cpu;\r\nstruct acpi_cpufreq_data *data;\r\nunsigned int result = 0;\r\nstruct cpuinfo_x86 *c = &cpu_data(policy->cpu);\r\nstruct acpi_processor_performance *perf;\r\nstruct cpufreq_frequency_table *freq_table;\r\n#ifdef CONFIG_SMP\r\nstatic int blacklisted;\r\n#endif\r\npr_debug("acpi_cpufreq_cpu_init\n");\r\n#ifdef CONFIG_SMP\r\nif (blacklisted)\r\nreturn blacklisted;\r\nblacklisted = acpi_cpufreq_blacklist(c);\r\nif (blacklisted)\r\nreturn blacklisted;\r\n#endif\r\ndata = kzalloc(sizeof(*data), GFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\nif (!zalloc_cpumask_var(&data->freqdomain_cpus, GFP_KERNEL)) {\r\nresult = -ENOMEM;\r\ngoto err_free;\r\n}\r\nperf = per_cpu_ptr(acpi_perf_data, cpu);\r\ndata->acpi_perf_cpu = cpu;\r\npolicy->driver_data = data;\r\nif (cpu_has(c, X86_FEATURE_CONSTANT_TSC))\r\nacpi_cpufreq_driver.flags |= CPUFREQ_CONST_LOOPS;\r\nresult = acpi_processor_register_performance(perf, cpu);\r\nif (result)\r\ngoto err_free_mask;\r\npolicy->shared_type = perf->shared_type;\r\nif (policy->shared_type == CPUFREQ_SHARED_TYPE_ALL ||\r\npolicy->shared_type == CPUFREQ_SHARED_TYPE_ANY) {\r\ncpumask_copy(policy->cpus, perf->shared_cpu_map);\r\n}\r\ncpumask_copy(data->freqdomain_cpus, perf->shared_cpu_map);\r\n#ifdef CONFIG_SMP\r\ndmi_check_system(sw_any_bug_dmi_table);\r\nif (bios_with_sw_any_bug && !policy_is_shared(policy)) {\r\npolicy->shared_type = CPUFREQ_SHARED_TYPE_ALL;\r\ncpumask_copy(policy->cpus, topology_core_cpumask(cpu));\r\n}\r\nif (check_amd_hwpstate_cpu(cpu) && !acpi_pstate_strict) {\r\ncpumask_clear(policy->cpus);\r\ncpumask_set_cpu(cpu, policy->cpus);\r\ncpumask_copy(data->freqdomain_cpus,\r\ntopology_sibling_cpumask(cpu));\r\npolicy->shared_type = CPUFREQ_SHARED_TYPE_HW;\r\npr_info_once("overriding BIOS provided _PSD data\n");\r\n}\r\n#endif\r\nif (perf->state_count <= 1) {\r\npr_debug("No P-States\n");\r\nresult = -ENODEV;\r\ngoto err_unreg;\r\n}\r\nif (perf->control_register.space_id != perf->status_register.space_id) {\r\nresult = -ENODEV;\r\ngoto err_unreg;\r\n}\r\nswitch (perf->control_register.space_id) {\r\ncase ACPI_ADR_SPACE_SYSTEM_IO:\r\nif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD &&\r\nboot_cpu_data.x86 == 0xf) {\r\npr_debug("AMD K8 systems must use native drivers.\n");\r\nresult = -ENODEV;\r\ngoto err_unreg;\r\n}\r\npr_debug("SYSTEM IO addr space\n");\r\ndata->cpu_feature = SYSTEM_IO_CAPABLE;\r\ndata->cpu_freq_read = cpu_freq_read_io;\r\ndata->cpu_freq_write = cpu_freq_write_io;\r\nbreak;\r\ncase ACPI_ADR_SPACE_FIXED_HARDWARE:\r\npr_debug("HARDWARE addr space\n");\r\nif (check_est_cpu(cpu)) {\r\ndata->cpu_feature = SYSTEM_INTEL_MSR_CAPABLE;\r\ndata->cpu_freq_read = cpu_freq_read_intel;\r\ndata->cpu_freq_write = cpu_freq_write_intel;\r\nbreak;\r\n}\r\nif (check_amd_hwpstate_cpu(cpu)) {\r\ndata->cpu_feature = SYSTEM_AMD_MSR_CAPABLE;\r\ndata->cpu_freq_read = cpu_freq_read_amd;\r\ndata->cpu_freq_write = cpu_freq_write_amd;\r\nbreak;\r\n}\r\nresult = -ENODEV;\r\ngoto err_unreg;\r\ndefault:\r\npr_debug("Unknown addr space %d\n",\r\n(u32) (perf->control_register.space_id));\r\nresult = -ENODEV;\r\ngoto err_unreg;\r\n}\r\nfreq_table = kzalloc(sizeof(*freq_table) *\r\n(perf->state_count+1), GFP_KERNEL);\r\nif (!freq_table) {\r\nresult = -ENOMEM;\r\ngoto err_unreg;\r\n}\r\npolicy->cpuinfo.transition_latency = 0;\r\nfor (i = 0; i < perf->state_count; i++) {\r\nif ((perf->states[i].transition_latency * 1000) >\r\npolicy->cpuinfo.transition_latency)\r\npolicy->cpuinfo.transition_latency =\r\nperf->states[i].transition_latency * 1000;\r\n}\r\nif (perf->control_register.space_id == ACPI_ADR_SPACE_FIXED_HARDWARE &&\r\npolicy->cpuinfo.transition_latency > 20 * 1000) {\r\npolicy->cpuinfo.transition_latency = 20 * 1000;\r\npr_info_once("P-state transition latency capped at 20 uS\n");\r\n}\r\nfor (i = 0; i < perf->state_count; i++) {\r\nif (i > 0 && perf->states[i].core_frequency >=\r\nfreq_table[valid_states-1].frequency / 1000)\r\ncontinue;\r\nfreq_table[valid_states].driver_data = i;\r\nfreq_table[valid_states].frequency =\r\nperf->states[i].core_frequency * 1000;\r\nvalid_states++;\r\n}\r\nfreq_table[valid_states].frequency = CPUFREQ_TABLE_END;\r\nperf->state = 0;\r\nresult = cpufreq_table_validate_and_show(policy, freq_table);\r\nif (result)\r\ngoto err_freqfree;\r\nif (perf->states[0].core_frequency * 1000 != policy->cpuinfo.max_freq)\r\npr_warn(FW_WARN "P-state 0 is not max freq\n");\r\nswitch (perf->control_register.space_id) {\r\ncase ACPI_ADR_SPACE_SYSTEM_IO:\r\npolicy->cur = acpi_cpufreq_guess_freq(data, policy->cpu);\r\nbreak;\r\ncase ACPI_ADR_SPACE_FIXED_HARDWARE:\r\nacpi_cpufreq_driver.get = get_cur_freq_on_cpu;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nacpi_processor_notify_smm(THIS_MODULE);\r\npr_debug("CPU%u - ACPI performance management activated.\n", cpu);\r\nfor (i = 0; i < perf->state_count; i++)\r\npr_debug(" %cP%d: %d MHz, %d mW, %d uS\n",\r\n(i == perf->state ? '*' : ' '), i,\r\n(u32) perf->states[i].core_frequency,\r\n(u32) perf->states[i].power,\r\n(u32) perf->states[i].transition_latency);\r\ndata->resume = 1;\r\npolicy->fast_switch_possible = !acpi_pstate_strict &&\r\n!(policy_is_shared(policy) && policy->shared_type != CPUFREQ_SHARED_TYPE_ANY);\r\nreturn result;\r\nerr_freqfree:\r\nkfree(freq_table);\r\nerr_unreg:\r\nacpi_processor_unregister_performance(cpu);\r\nerr_free_mask:\r\nfree_cpumask_var(data->freqdomain_cpus);\r\nerr_free:\r\nkfree(data);\r\npolicy->driver_data = NULL;\r\nreturn result;\r\n}\r\nstatic int acpi_cpufreq_cpu_exit(struct cpufreq_policy *policy)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\npr_debug("acpi_cpufreq_cpu_exit\n");\r\npolicy->fast_switch_possible = false;\r\npolicy->driver_data = NULL;\r\nacpi_processor_unregister_performance(data->acpi_perf_cpu);\r\nfree_cpumask_var(data->freqdomain_cpus);\r\nkfree(policy->freq_table);\r\nkfree(data);\r\nreturn 0;\r\n}\r\nstatic int acpi_cpufreq_resume(struct cpufreq_policy *policy)\r\n{\r\nstruct acpi_cpufreq_data *data = policy->driver_data;\r\npr_debug("acpi_cpufreq_resume\n");\r\ndata->resume = 1;\r\nreturn 0;\r\n}\r\nstatic void __init acpi_cpufreq_boost_init(void)\r\n{\r\nint ret;\r\nif (!(boot_cpu_has(X86_FEATURE_CPB) || boot_cpu_has(X86_FEATURE_IDA)))\r\nreturn;\r\nacpi_cpufreq_driver.set_boost = set_boost;\r\nacpi_cpufreq_driver.boost_enabled = boost_state(0);\r\nret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, "cpufreq/acpi:online",\r\ncpufreq_boost_online, cpufreq_boost_down_prep);\r\nif (ret < 0) {\r\npr_err("acpi_cpufreq: failed to register hotplug callbacks\n");\r\nreturn;\r\n}\r\nacpi_cpufreq_online = ret;\r\n}\r\nstatic void acpi_cpufreq_boost_exit(void)\r\n{\r\nif (acpi_cpufreq_online > 0)\r\ncpuhp_remove_state_nocalls(acpi_cpufreq_online);\r\n}\r\nstatic int __init acpi_cpufreq_init(void)\r\n{\r\nint ret;\r\nif (acpi_disabled)\r\nreturn -ENODEV;\r\nif (cpufreq_get_current_driver())\r\nreturn -EEXIST;\r\npr_debug("acpi_cpufreq_init\n");\r\nret = acpi_cpufreq_early_init();\r\nif (ret)\r\nreturn ret;\r\n#ifdef CONFIG_X86_ACPI_CPUFREQ_CPB\r\nif (!check_amd_hwpstate_cpu(0)) {\r\nstruct freq_attr **attr;\r\npr_debug("CPB unsupported, do not expose it\n");\r\nfor (attr = acpi_cpufreq_attr; *attr; attr++)\r\nif (*attr == &cpb) {\r\n*attr = NULL;\r\nbreak;\r\n}\r\n}\r\n#endif\r\nacpi_cpufreq_boost_init();\r\nret = cpufreq_register_driver(&acpi_cpufreq_driver);\r\nif (ret) {\r\nfree_acpi_perf_data();\r\nacpi_cpufreq_boost_exit();\r\n}\r\nreturn ret;\r\n}\r\nstatic void __exit acpi_cpufreq_exit(void)\r\n{\r\npr_debug("acpi_cpufreq_exit\n");\r\nacpi_cpufreq_boost_exit();\r\ncpufreq_unregister_driver(&acpi_cpufreq_driver);\r\nfree_acpi_perf_data();\r\n}
