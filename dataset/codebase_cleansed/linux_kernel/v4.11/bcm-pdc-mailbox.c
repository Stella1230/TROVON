static ssize_t pdc_debugfs_read(struct file *filp, char __user *ubuf,\r\nsize_t count, loff_t *offp)\r\n{\r\nstruct pdc_state *pdcs;\r\nchar *buf;\r\nssize_t ret, out_offset, out_count;\r\nout_count = 512;\r\nbuf = kmalloc(out_count, GFP_KERNEL);\r\nif (!buf)\r\nreturn -ENOMEM;\r\npdcs = filp->private_data;\r\nout_offset = 0;\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"SPU %u stats:\n", pdcs->pdc_idx);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"PDC requests....................%u\n",\r\npdcs->pdc_requests);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"PDC responses...................%u\n",\r\npdcs->pdc_replies);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"Tx not done.....................%u\n",\r\npdcs->last_tx_not_done);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"Tx ring full....................%u\n",\r\npdcs->tx_ring_full);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"Rx ring full....................%u\n",\r\npdcs->rx_ring_full);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"Tx desc write fail. Ring full...%u\n",\r\npdcs->txnobuf);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"Rx desc write fail. Ring full...%u\n",\r\npdcs->rxnobuf);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"Receive overflow................%u\n",\r\npdcs->rx_oflow);\r\nout_offset += snprintf(buf + out_offset, out_count - out_offset,\r\n"Num frags in rx ring............%u\n",\r\nNRXDACTIVE(pdcs->rxin, pdcs->last_rx_curr,\r\npdcs->nrxpost));\r\nif (out_offset > out_count)\r\nout_offset = out_count;\r\nret = simple_read_from_buffer(ubuf, count, offp, buf, out_offset);\r\nkfree(buf);\r\nreturn ret;\r\n}\r\nstatic void pdc_setup_debugfs(struct pdc_state *pdcs)\r\n{\r\nchar spu_stats_name[16];\r\nif (!debugfs_initialized())\r\nreturn;\r\nsnprintf(spu_stats_name, 16, "pdc%d_stats", pdcs->pdc_idx);\r\nif (!debugfs_dir)\r\ndebugfs_dir = debugfs_create_dir(KBUILD_MODNAME, NULL);\r\npdcs->debugfs_stats = debugfs_create_file(spu_stats_name, 0400,\r\ndebugfs_dir, pdcs,\r\n&pdc_debugfs_stats);\r\n}\r\nstatic void pdc_free_debugfs(void)\r\n{\r\ndebugfs_remove_recursive(debugfs_dir);\r\ndebugfs_dir = NULL;\r\n}\r\nstatic inline void\r\npdc_build_rxd(struct pdc_state *pdcs, dma_addr_t dma_addr,\r\nu32 buf_len, u32 flags)\r\n{\r\nstruct device *dev = &pdcs->pdev->dev;\r\nstruct dma64dd *rxd = &pdcs->rxd_64[pdcs->rxout];\r\ndev_dbg(dev,\r\n"Writing rx descriptor for PDC %u at index %u with length %u. flags %#x\n",\r\npdcs->pdc_idx, pdcs->rxout, buf_len, flags);\r\nrxd->addrlow = cpu_to_le32(lower_32_bits(dma_addr));\r\nrxd->addrhigh = cpu_to_le32(upper_32_bits(dma_addr));\r\nrxd->ctrl1 = cpu_to_le32(flags);\r\nrxd->ctrl2 = cpu_to_le32(buf_len);\r\npdcs->rxout = NEXTRXD(pdcs->rxout, pdcs->nrxpost);\r\n}\r\nstatic inline void\r\npdc_build_txd(struct pdc_state *pdcs, dma_addr_t dma_addr, u32 buf_len,\r\nu32 flags)\r\n{\r\nstruct device *dev = &pdcs->pdev->dev;\r\nstruct dma64dd *txd = &pdcs->txd_64[pdcs->txout];\r\ndev_dbg(dev,\r\n"Writing tx descriptor for PDC %u at index %u with length %u, flags %#x\n",\r\npdcs->pdc_idx, pdcs->txout, buf_len, flags);\r\ntxd->addrlow = cpu_to_le32(lower_32_bits(dma_addr));\r\ntxd->addrhigh = cpu_to_le32(upper_32_bits(dma_addr));\r\ntxd->ctrl1 = cpu_to_le32(flags);\r\ntxd->ctrl2 = cpu_to_le32(buf_len);\r\npdcs->txout = NEXTTXD(pdcs->txout, pdcs->ntxpost);\r\n}\r\nstatic int\r\npdc_receive_one(struct pdc_state *pdcs)\r\n{\r\nstruct device *dev = &pdcs->pdev->dev;\r\nstruct mbox_controller *mbc;\r\nstruct mbox_chan *chan;\r\nstruct brcm_message mssg;\r\nu32 len, rx_status;\r\nu32 num_frags;\r\nu8 *resp_hdr;\r\nu32 frags_rdy;\r\nu32 rx_idx;\r\ndma_addr_t resp_hdr_daddr;\r\nstruct pdc_rx_ctx *rx_ctx;\r\nmbc = &pdcs->mbc;\r\nchan = &mbc->chans[0];\r\nmssg.type = BRCM_MESSAGE_SPU;\r\nfrags_rdy = NRXDACTIVE(pdcs->rxin, pdcs->last_rx_curr, pdcs->nrxpost);\r\nif ((frags_rdy == 0) ||\r\n(frags_rdy < pdcs->rx_ctx[pdcs->rxin].rxin_numd))\r\nreturn -EAGAIN;\r\nnum_frags = pdcs->txin_numd[pdcs->txin];\r\nWARN_ON(num_frags == 0);\r\ndma_unmap_sg(dev, pdcs->src_sg[pdcs->txin],\r\nsg_nents(pdcs->src_sg[pdcs->txin]), DMA_TO_DEVICE);\r\npdcs->txin = (pdcs->txin + num_frags) & pdcs->ntxpost;\r\ndev_dbg(dev, "PDC %u reclaimed %d tx descriptors",\r\npdcs->pdc_idx, num_frags);\r\nrx_idx = pdcs->rxin;\r\nrx_ctx = &pdcs->rx_ctx[rx_idx];\r\nnum_frags = rx_ctx->rxin_numd;\r\nmssg.ctx = rx_ctx->rxp_ctx;\r\nrx_ctx->rxp_ctx = NULL;\r\nresp_hdr = rx_ctx->resp_hdr;\r\nresp_hdr_daddr = rx_ctx->resp_hdr_daddr;\r\ndma_unmap_sg(dev, rx_ctx->dst_sg, sg_nents(rx_ctx->dst_sg),\r\nDMA_FROM_DEVICE);\r\npdcs->rxin = (pdcs->rxin + num_frags) & pdcs->nrxpost;\r\ndev_dbg(dev, "PDC %u reclaimed %d rx descriptors",\r\npdcs->pdc_idx, num_frags);\r\ndev_dbg(dev,\r\n"PDC %u txin %u, txout %u, rxin %u, rxout %u, last_rx_curr %u\n",\r\npdcs->pdc_idx, pdcs->txin, pdcs->txout, pdcs->rxin,\r\npdcs->rxout, pdcs->last_rx_curr);\r\nif (pdcs->pdc_resp_hdr_len == PDC_SPUM_RESP_HDR_LEN) {\r\nrx_status = *((u32 *)resp_hdr);\r\nlen = rx_status & RX_STATUS_LEN;\r\ndev_dbg(dev,\r\n"SPU response length %u bytes", len);\r\nif (unlikely(((rx_status & RX_STATUS_OVERFLOW) || (!len)))) {\r\nif (rx_status & RX_STATUS_OVERFLOW) {\r\ndev_err_ratelimited(dev,\r\n"crypto receive overflow");\r\npdcs->rx_oflow++;\r\n} else {\r\ndev_info_ratelimited(dev, "crypto rx len = 0");\r\n}\r\nreturn -EIO;\r\n}\r\n}\r\ndma_pool_free(pdcs->rx_buf_pool, resp_hdr, resp_hdr_daddr);\r\nmbox_chan_received_data(chan, &mssg);\r\npdcs->pdc_replies++;\r\nreturn PDC_SUCCESS;\r\n}\r\nstatic int\r\npdc_receive(struct pdc_state *pdcs)\r\n{\r\nint rx_status;\r\npdcs->last_rx_curr =\r\n(ioread32(&pdcs->rxregs_64->status0) &\r\nCRYPTO_D64_RS0_CD_MASK) / RING_ENTRY_SIZE;\r\ndo {\r\nrx_status = pdc_receive_one(pdcs);\r\n} while (rx_status == PDC_SUCCESS);\r\nreturn 0;\r\n}\r\nstatic int pdc_tx_list_sg_add(struct pdc_state *pdcs, struct scatterlist *sg)\r\n{\r\nu32 flags = 0;\r\nu32 eot;\r\nu32 tx_avail;\r\nu32 num_desc;\r\nu32 desc_w = 0;\r\nu32 bufcnt;\r\ndma_addr_t databufptr;\r\nnum_desc = (u32)sg_nents(sg);\r\ntx_avail = pdcs->ntxpost - NTXDACTIVE(pdcs->txin, pdcs->txout,\r\npdcs->ntxpost);\r\nif (unlikely(num_desc > tx_avail)) {\r\npdcs->txnobuf++;\r\nreturn -ENOSPC;\r\n}\r\nif (pdcs->tx_msg_start == pdcs->txout) {\r\npdcs->txin_numd[pdcs->tx_msg_start] = 0;\r\npdcs->src_sg[pdcs->txout] = sg;\r\nflags = D64_CTRL1_SOF;\r\n}\r\nwhile (sg) {\r\nif (unlikely(pdcs->txout == (pdcs->ntxd - 1)))\r\neot = D64_CTRL1_EOT;\r\nelse\r\neot = 0;\r\nbufcnt = sg_dma_len(sg);\r\ndatabufptr = sg_dma_address(sg);\r\nwhile (bufcnt > PDC_DMA_BUF_MAX) {\r\npdc_build_txd(pdcs, databufptr, PDC_DMA_BUF_MAX,\r\nflags | eot);\r\ndesc_w++;\r\nbufcnt -= PDC_DMA_BUF_MAX;\r\ndatabufptr += PDC_DMA_BUF_MAX;\r\nif (unlikely(pdcs->txout == (pdcs->ntxd - 1)))\r\neot = D64_CTRL1_EOT;\r\nelse\r\neot = 0;\r\n}\r\nsg = sg_next(sg);\r\nif (!sg)\r\nflags |= (D64_CTRL1_EOF | D64_CTRL1_IOC);\r\npdc_build_txd(pdcs, databufptr, bufcnt, flags | eot);\r\ndesc_w++;\r\nflags &= ~D64_CTRL1_SOF;\r\n}\r\npdcs->txin_numd[pdcs->tx_msg_start] += desc_w;\r\nreturn PDC_SUCCESS;\r\n}\r\nstatic int pdc_tx_list_final(struct pdc_state *pdcs)\r\n{\r\nwmb();\r\niowrite32(pdcs->rxout << 4, &pdcs->rxregs_64->ptr);\r\niowrite32(pdcs->txout << 4, &pdcs->txregs_64->ptr);\r\npdcs->pdc_requests++;\r\nreturn PDC_SUCCESS;\r\n}\r\nstatic int pdc_rx_list_init(struct pdc_state *pdcs, struct scatterlist *dst_sg,\r\nvoid *ctx)\r\n{\r\nu32 flags = 0;\r\nu32 rx_avail;\r\nu32 rx_pkt_cnt = 1;\r\ndma_addr_t daddr;\r\nvoid *vaddr;\r\nstruct pdc_rx_ctx *rx_ctx;\r\nrx_avail = pdcs->nrxpost - NRXDACTIVE(pdcs->rxin, pdcs->rxout,\r\npdcs->nrxpost);\r\nif (unlikely(rx_pkt_cnt > rx_avail)) {\r\npdcs->rxnobuf++;\r\nreturn -ENOSPC;\r\n}\r\nvaddr = dma_pool_zalloc(pdcs->rx_buf_pool, GFP_ATOMIC, &daddr);\r\nif (unlikely(!vaddr))\r\nreturn -ENOMEM;\r\npdcs->rx_msg_start = pdcs->rxout;\r\npdcs->tx_msg_start = pdcs->txout;\r\nflags = D64_CTRL1_SOF;\r\npdcs->rx_ctx[pdcs->rx_msg_start].rxin_numd = 1;\r\nif (unlikely(pdcs->rxout == (pdcs->nrxd - 1)))\r\nflags |= D64_CTRL1_EOT;\r\nrx_ctx = &pdcs->rx_ctx[pdcs->rxout];\r\nrx_ctx->rxp_ctx = ctx;\r\nrx_ctx->dst_sg = dst_sg;\r\nrx_ctx->resp_hdr = vaddr;\r\nrx_ctx->resp_hdr_daddr = daddr;\r\npdc_build_rxd(pdcs, daddr, pdcs->pdc_resp_hdr_len, flags);\r\nreturn PDC_SUCCESS;\r\n}\r\nstatic int pdc_rx_list_sg_add(struct pdc_state *pdcs, struct scatterlist *sg)\r\n{\r\nu32 flags = 0;\r\nu32 rx_avail;\r\nu32 num_desc;\r\nu32 desc_w = 0;\r\nu32 bufcnt;\r\ndma_addr_t databufptr;\r\nnum_desc = (u32)sg_nents(sg);\r\nrx_avail = pdcs->nrxpost - NRXDACTIVE(pdcs->rxin, pdcs->rxout,\r\npdcs->nrxpost);\r\nif (unlikely(num_desc > rx_avail)) {\r\npdcs->rxnobuf++;\r\nreturn -ENOSPC;\r\n}\r\nwhile (sg) {\r\nif (unlikely(pdcs->rxout == (pdcs->nrxd - 1)))\r\nflags = D64_CTRL1_EOT;\r\nelse\r\nflags = 0;\r\nbufcnt = sg_dma_len(sg);\r\ndatabufptr = sg_dma_address(sg);\r\nwhile (bufcnt > PDC_DMA_BUF_MAX) {\r\npdc_build_rxd(pdcs, databufptr, PDC_DMA_BUF_MAX, flags);\r\ndesc_w++;\r\nbufcnt -= PDC_DMA_BUF_MAX;\r\ndatabufptr += PDC_DMA_BUF_MAX;\r\nif (unlikely(pdcs->rxout == (pdcs->nrxd - 1)))\r\nflags = D64_CTRL1_EOT;\r\nelse\r\nflags = 0;\r\n}\r\npdc_build_rxd(pdcs, databufptr, bufcnt, flags);\r\ndesc_w++;\r\nsg = sg_next(sg);\r\n}\r\npdcs->rx_ctx[pdcs->rx_msg_start].rxin_numd += desc_w;\r\nreturn PDC_SUCCESS;\r\n}\r\nstatic irqreturn_t pdc_irq_handler(int irq, void *data)\r\n{\r\nstruct device *dev = (struct device *)data;\r\nstruct pdc_state *pdcs = dev_get_drvdata(dev);\r\nu32 intstatus = ioread32(pdcs->pdc_reg_vbase + PDC_INTSTATUS_OFFSET);\r\nif (unlikely(intstatus == 0))\r\nreturn IRQ_NONE;\r\niowrite32(0, pdcs->pdc_reg_vbase + PDC_INTMASK_OFFSET);\r\niowrite32(intstatus, pdcs->pdc_reg_vbase + PDC_INTSTATUS_OFFSET);\r\ntasklet_schedule(&pdcs->rx_tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void pdc_tasklet_cb(unsigned long data)\r\n{\r\nstruct pdc_state *pdcs = (struct pdc_state *)data;\r\npdc_receive(pdcs);\r\niowrite32(PDC_INTMASK, pdcs->pdc_reg_vbase + PDC_INTMASK_OFFSET);\r\n}\r\nstatic int pdc_ring_init(struct pdc_state *pdcs, int ringset)\r\n{\r\nint i;\r\nint err = PDC_SUCCESS;\r\nstruct dma64 *dma_reg;\r\nstruct device *dev = &pdcs->pdev->dev;\r\nstruct pdc_ring_alloc tx;\r\nstruct pdc_ring_alloc rx;\r\ntx.vbase = dma_pool_zalloc(pdcs->ring_pool, GFP_KERNEL, &tx.dmabase);\r\nif (unlikely(!tx.vbase)) {\r\nerr = -ENOMEM;\r\ngoto done;\r\n}\r\nrx.vbase = dma_pool_zalloc(pdcs->ring_pool, GFP_KERNEL, &rx.dmabase);\r\nif (unlikely(!rx.vbase)) {\r\nerr = -ENOMEM;\r\ngoto fail_dealloc;\r\n}\r\ndev_dbg(dev, " - base DMA addr of tx ring %pad", &tx.dmabase);\r\ndev_dbg(dev, " - base virtual addr of tx ring %p", tx.vbase);\r\ndev_dbg(dev, " - base DMA addr of rx ring %pad", &rx.dmabase);\r\ndev_dbg(dev, " - base virtual addr of rx ring %p", rx.vbase);\r\nmemcpy(&pdcs->tx_ring_alloc, &tx, sizeof(tx));\r\nmemcpy(&pdcs->rx_ring_alloc, &rx, sizeof(rx));\r\npdcs->rxin = 0;\r\npdcs->rx_msg_start = 0;\r\npdcs->last_rx_curr = 0;\r\npdcs->rxout = 0;\r\npdcs->txin = 0;\r\npdcs->tx_msg_start = 0;\r\npdcs->txout = 0;\r\npdcs->txd_64 = (struct dma64dd *)pdcs->tx_ring_alloc.vbase;\r\npdcs->rxd_64 = (struct dma64dd *)pdcs->rx_ring_alloc.vbase;\r\ndma_reg = &pdcs->regs->dmaregs[ringset];\r\niowrite32(PDC_TX_CTL, &dma_reg->dmaxmt.control);\r\niowrite32((PDC_RX_CTL + (pdcs->rx_status_len << 1)),\r\n&dma_reg->dmarcv.control);\r\niowrite32(0, &dma_reg->dmaxmt.ptr);\r\niowrite32(0, &dma_reg->dmarcv.ptr);\r\niowrite32(lower_32_bits(pdcs->tx_ring_alloc.dmabase),\r\n&dma_reg->dmaxmt.addrlow);\r\niowrite32(upper_32_bits(pdcs->tx_ring_alloc.dmabase),\r\n&dma_reg->dmaxmt.addrhigh);\r\niowrite32(lower_32_bits(pdcs->rx_ring_alloc.dmabase),\r\n&dma_reg->dmarcv.addrlow);\r\niowrite32(upper_32_bits(pdcs->rx_ring_alloc.dmabase),\r\n&dma_reg->dmarcv.addrhigh);\r\niowrite32(PDC_TX_CTL | PDC_TX_ENABLE, &dma_reg->dmaxmt.control);\r\niowrite32((PDC_RX_CTL | PDC_RX_ENABLE | (pdcs->rx_status_len << 1)),\r\n&dma_reg->dmarcv.control);\r\nfor (i = 0; i < PDC_RING_ENTRIES; i++) {\r\nif (i != pdcs->ntxpost) {\r\niowrite32(D64_CTRL1_SOF | D64_CTRL1_EOF,\r\n&pdcs->txd_64[i].ctrl1);\r\n} else {\r\niowrite32(D64_CTRL1_SOF | D64_CTRL1_EOF |\r\nD64_CTRL1_EOT, &pdcs->txd_64[i].ctrl1);\r\n}\r\nif (i != pdcs->nrxpost) {\r\niowrite32(D64_CTRL1_SOF,\r\n&pdcs->rxd_64[i].ctrl1);\r\n} else {\r\niowrite32(D64_CTRL1_SOF | D64_CTRL1_EOT,\r\n&pdcs->rxd_64[i].ctrl1);\r\n}\r\n}\r\nreturn PDC_SUCCESS;\r\nfail_dealloc:\r\ndma_pool_free(pdcs->ring_pool, tx.vbase, tx.dmabase);\r\ndone:\r\nreturn err;\r\n}\r\nstatic void pdc_ring_free(struct pdc_state *pdcs)\r\n{\r\nif (pdcs->tx_ring_alloc.vbase) {\r\ndma_pool_free(pdcs->ring_pool, pdcs->tx_ring_alloc.vbase,\r\npdcs->tx_ring_alloc.dmabase);\r\npdcs->tx_ring_alloc.vbase = NULL;\r\n}\r\nif (pdcs->rx_ring_alloc.vbase) {\r\ndma_pool_free(pdcs->ring_pool, pdcs->rx_ring_alloc.vbase,\r\npdcs->rx_ring_alloc.dmabase);\r\npdcs->rx_ring_alloc.vbase = NULL;\r\n}\r\n}\r\nstatic u32 pdc_desc_count(struct scatterlist *sg)\r\n{\r\nu32 cnt = 0;\r\nwhile (sg) {\r\ncnt += ((sg->length / PDC_DMA_BUF_MAX) + 1);\r\nsg = sg_next(sg);\r\n}\r\nreturn cnt;\r\n}\r\nstatic bool pdc_rings_full(struct pdc_state *pdcs, int tx_cnt, int rx_cnt)\r\n{\r\nu32 rx_avail;\r\nu32 tx_avail;\r\nbool full = false;\r\nrx_avail = pdcs->nrxpost - NRXDACTIVE(pdcs->rxin, pdcs->rxout,\r\npdcs->nrxpost);\r\nif (unlikely(rx_cnt > rx_avail)) {\r\npdcs->rx_ring_full++;\r\nfull = true;\r\n}\r\nif (likely(!full)) {\r\ntx_avail = pdcs->ntxpost - NTXDACTIVE(pdcs->txin, pdcs->txout,\r\npdcs->ntxpost);\r\nif (unlikely(tx_cnt > tx_avail)) {\r\npdcs->tx_ring_full++;\r\nfull = true;\r\n}\r\n}\r\nreturn full;\r\n}\r\nstatic bool pdc_last_tx_done(struct mbox_chan *chan)\r\n{\r\nstruct pdc_state *pdcs = chan->con_priv;\r\nbool ret;\r\nif (unlikely(pdc_rings_full(pdcs, PDC_RING_SPACE_MIN,\r\nPDC_RING_SPACE_MIN))) {\r\npdcs->last_tx_not_done++;\r\nret = false;\r\n} else {\r\nret = true;\r\n}\r\nreturn ret;\r\n}\r\nstatic int pdc_send_data(struct mbox_chan *chan, void *data)\r\n{\r\nstruct pdc_state *pdcs = chan->con_priv;\r\nstruct device *dev = &pdcs->pdev->dev;\r\nstruct brcm_message *mssg = data;\r\nint err = PDC_SUCCESS;\r\nint src_nent;\r\nint dst_nent;\r\nint nent;\r\nu32 tx_desc_req;\r\nu32 rx_desc_req;\r\nif (unlikely(mssg->type != BRCM_MESSAGE_SPU))\r\nreturn -ENOTSUPP;\r\nsrc_nent = sg_nents(mssg->spu.src);\r\nif (likely(src_nent)) {\r\nnent = dma_map_sg(dev, mssg->spu.src, src_nent, DMA_TO_DEVICE);\r\nif (unlikely(nent == 0))\r\nreturn -EIO;\r\n}\r\ndst_nent = sg_nents(mssg->spu.dst);\r\nif (likely(dst_nent)) {\r\nnent = dma_map_sg(dev, mssg->spu.dst, dst_nent,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(nent == 0)) {\r\ndma_unmap_sg(dev, mssg->spu.src, src_nent,\r\nDMA_TO_DEVICE);\r\nreturn -EIO;\r\n}\r\n}\r\ntx_desc_req = pdc_desc_count(mssg->spu.src);\r\nrx_desc_req = pdc_desc_count(mssg->spu.dst);\r\nif (unlikely(pdc_rings_full(pdcs, tx_desc_req, rx_desc_req + 1)))\r\nreturn -ENOSPC;\r\nerr = pdc_rx_list_init(pdcs, mssg->spu.dst, mssg->ctx);\r\nerr |= pdc_rx_list_sg_add(pdcs, mssg->spu.dst);\r\nerr |= pdc_tx_list_sg_add(pdcs, mssg->spu.src);\r\nerr |= pdc_tx_list_final(pdcs);\r\nif (unlikely(err))\r\ndev_err(&pdcs->pdev->dev,\r\n"%s failed with error %d", __func__, err);\r\nreturn err;\r\n}\r\nstatic int pdc_startup(struct mbox_chan *chan)\r\n{\r\nreturn pdc_ring_init(chan->con_priv, PDC_RINGSET);\r\n}\r\nstatic void pdc_shutdown(struct mbox_chan *chan)\r\n{\r\nstruct pdc_state *pdcs = chan->con_priv;\r\nif (!pdcs)\r\nreturn;\r\ndev_dbg(&pdcs->pdev->dev,\r\n"Shutdown mailbox channel for PDC %u", pdcs->pdc_idx);\r\npdc_ring_free(pdcs);\r\n}\r\nstatic\r\nvoid pdc_hw_init(struct pdc_state *pdcs)\r\n{\r\nstruct platform_device *pdev;\r\nstruct device *dev;\r\nstruct dma64 *dma_reg;\r\nint ringset = PDC_RINGSET;\r\npdev = pdcs->pdev;\r\ndev = &pdev->dev;\r\ndev_dbg(dev, "PDC %u initial values:", pdcs->pdc_idx);\r\ndev_dbg(dev, "state structure: %p",\r\npdcs);\r\ndev_dbg(dev, " - base virtual addr of hw regs %p",\r\npdcs->pdc_reg_vbase);\r\npdcs->regs = (struct pdc_regs *)pdcs->pdc_reg_vbase;\r\npdcs->txregs_64 = (struct dma64_regs *)\r\n(((u8 *)pdcs->pdc_reg_vbase) +\r\nPDC_TXREGS_OFFSET + (sizeof(struct dma64) * ringset));\r\npdcs->rxregs_64 = (struct dma64_regs *)\r\n(((u8 *)pdcs->pdc_reg_vbase) +\r\nPDC_RXREGS_OFFSET + (sizeof(struct dma64) * ringset));\r\npdcs->ntxd = PDC_RING_ENTRIES;\r\npdcs->nrxd = PDC_RING_ENTRIES;\r\npdcs->ntxpost = PDC_RING_ENTRIES - 1;\r\npdcs->nrxpost = PDC_RING_ENTRIES - 1;\r\niowrite32(0, &pdcs->regs->intmask);\r\ndma_reg = &pdcs->regs->dmaregs[ringset];\r\niowrite32(PDC_TX_CTL, &dma_reg->dmaxmt.control);\r\niowrite32(PDC_RX_CTL + (pdcs->rx_status_len << 1),\r\n&dma_reg->dmarcv.control);\r\niowrite32(0, &dma_reg->dmaxmt.ptr);\r\niowrite32(0, &dma_reg->dmarcv.ptr);\r\nif (pdcs->pdc_resp_hdr_len == PDC_SPU2_RESP_HDR_LEN)\r\niowrite32(PDC_CKSUM_CTRL,\r\npdcs->pdc_reg_vbase + PDC_CKSUM_CTRL_OFFSET);\r\n}\r\nstatic void pdc_hw_disable(struct pdc_state *pdcs)\r\n{\r\nstruct dma64 *dma_reg;\r\ndma_reg = &pdcs->regs->dmaregs[PDC_RINGSET];\r\niowrite32(PDC_TX_CTL, &dma_reg->dmaxmt.control);\r\niowrite32(PDC_RX_CTL + (pdcs->rx_status_len << 1),\r\n&dma_reg->dmarcv.control);\r\n}\r\nstatic int pdc_rx_buf_pool_create(struct pdc_state *pdcs)\r\n{\r\nstruct platform_device *pdev;\r\nstruct device *dev;\r\npdev = pdcs->pdev;\r\ndev = &pdev->dev;\r\npdcs->pdc_resp_hdr_len = pdcs->rx_status_len;\r\nif (pdcs->use_bcm_hdr)\r\npdcs->pdc_resp_hdr_len += BCM_HDR_LEN;\r\npdcs->rx_buf_pool = dma_pool_create("pdc rx bufs", dev,\r\npdcs->pdc_resp_hdr_len,\r\nRX_BUF_ALIGN, 0);\r\nif (!pdcs->rx_buf_pool)\r\nreturn -ENOMEM;\r\nreturn PDC_SUCCESS;\r\n}\r\nstatic int pdc_interrupts_init(struct pdc_state *pdcs)\r\n{\r\nstruct platform_device *pdev = pdcs->pdev;\r\nstruct device *dev = &pdev->dev;\r\nstruct device_node *dn = pdev->dev.of_node;\r\nint err;\r\niowrite32(PDC_INTMASK, pdcs->pdc_reg_vbase + PDC_INTMASK_OFFSET);\r\niowrite32(PDC_LAZY_INT, pdcs->pdc_reg_vbase + PDC_RCVLAZY0_OFFSET);\r\npdcs->pdc_irq = irq_of_parse_and_map(dn, 0);\r\ndev_dbg(dev, "pdc device %s irq %u for pdcs %p",\r\ndev_name(dev), pdcs->pdc_irq, pdcs);\r\nerr = devm_request_irq(dev, pdcs->pdc_irq, pdc_irq_handler, 0,\r\ndev_name(dev), dev);\r\nif (err) {\r\ndev_err(dev, "IRQ %u request failed with err %d\n",\r\npdcs->pdc_irq, err);\r\nreturn err;\r\n}\r\nreturn PDC_SUCCESS;\r\n}\r\nstatic int pdc_mb_init(struct pdc_state *pdcs)\r\n{\r\nstruct device *dev = &pdcs->pdev->dev;\r\nstruct mbox_controller *mbc;\r\nint chan_index;\r\nint err;\r\nmbc = &pdcs->mbc;\r\nmbc->dev = dev;\r\nmbc->ops = &pdc_mbox_chan_ops;\r\nmbc->num_chans = 1;\r\nmbc->chans = devm_kcalloc(dev, mbc->num_chans, sizeof(*mbc->chans),\r\nGFP_KERNEL);\r\nif (!mbc->chans)\r\nreturn -ENOMEM;\r\nmbc->txdone_irq = false;\r\nmbc->txdone_poll = true;\r\nmbc->txpoll_period = 1;\r\nfor (chan_index = 0; chan_index < mbc->num_chans; chan_index++)\r\nmbc->chans[chan_index].con_priv = pdcs;\r\nerr = mbox_controller_register(mbc);\r\nif (err) {\r\ndev_crit(dev,\r\n"Failed to register PDC mailbox controller. Error %d.",\r\nerr);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pdc_dt_read(struct platform_device *pdev, struct pdc_state *pdcs)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct device_node *dn = pdev->dev.of_node;\r\nint err;\r\nerr = of_property_read_u32(dn, "brcm,rx-status-len",\r\n&pdcs->rx_status_len);\r\nif (err < 0)\r\ndev_err(dev,\r\n"%s failed to get DMA receive status length from device tree",\r\n__func__);\r\npdcs->use_bcm_hdr = of_property_read_bool(dn, "brcm,use-bcm-hdr");\r\nreturn 0;\r\n}\r\nstatic int pdc_probe(struct platform_device *pdev)\r\n{\r\nint err = 0;\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *pdc_regs;\r\nstruct pdc_state *pdcs;\r\npdcs = devm_kzalloc(dev, sizeof(*pdcs), GFP_KERNEL);\r\nif (!pdcs) {\r\nerr = -ENOMEM;\r\ngoto cleanup;\r\n}\r\npdcs->pdev = pdev;\r\nplatform_set_drvdata(pdev, pdcs);\r\npdcs->pdc_idx = pdcg.num_spu;\r\npdcg.num_spu++;\r\nerr = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));\r\nif (err) {\r\ndev_warn(dev, "PDC device cannot perform DMA. Error %d.", err);\r\ngoto cleanup;\r\n}\r\npdcs->ring_pool = dma_pool_create("pdc rings", dev, PDC_RING_SIZE,\r\nRING_ALIGN, 0);\r\nif (!pdcs->ring_pool) {\r\nerr = -ENOMEM;\r\ngoto cleanup;\r\n}\r\nerr = pdc_dt_read(pdev, pdcs);\r\nif (err)\r\ngoto cleanup_ring_pool;\r\npdc_regs = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!pdc_regs) {\r\nerr = -ENODEV;\r\ngoto cleanup_ring_pool;\r\n}\r\ndev_dbg(dev, "PDC register region res.start = %pa, res.end = %pa",\r\n&pdc_regs->start, &pdc_regs->end);\r\npdcs->pdc_reg_vbase = devm_ioremap_resource(&pdev->dev, pdc_regs);\r\nif (IS_ERR(pdcs->pdc_reg_vbase)) {\r\nerr = PTR_ERR(pdcs->pdc_reg_vbase);\r\ndev_err(&pdev->dev, "Failed to map registers: %d\n", err);\r\ngoto cleanup_ring_pool;\r\n}\r\nerr = pdc_rx_buf_pool_create(pdcs);\r\nif (err)\r\ngoto cleanup_ring_pool;\r\npdc_hw_init(pdcs);\r\ntasklet_init(&pdcs->rx_tasklet, pdc_tasklet_cb, (unsigned long)pdcs);\r\nerr = pdc_interrupts_init(pdcs);\r\nif (err)\r\ngoto cleanup_buf_pool;\r\nerr = pdc_mb_init(pdcs);\r\nif (err)\r\ngoto cleanup_buf_pool;\r\npdcs->debugfs_stats = NULL;\r\npdc_setup_debugfs(pdcs);\r\ndev_dbg(dev, "pdc_probe() successful");\r\nreturn PDC_SUCCESS;\r\ncleanup_buf_pool:\r\ntasklet_kill(&pdcs->rx_tasklet);\r\ndma_pool_destroy(pdcs->rx_buf_pool);\r\ncleanup_ring_pool:\r\ndma_pool_destroy(pdcs->ring_pool);\r\ncleanup:\r\nreturn err;\r\n}\r\nstatic int pdc_remove(struct platform_device *pdev)\r\n{\r\nstruct pdc_state *pdcs = platform_get_drvdata(pdev);\r\npdc_free_debugfs();\r\ntasklet_kill(&pdcs->rx_tasklet);\r\npdc_hw_disable(pdcs);\r\nmbox_controller_unregister(&pdcs->mbc);\r\ndma_pool_destroy(pdcs->rx_buf_pool);\r\ndma_pool_destroy(pdcs->ring_pool);\r\nreturn 0;\r\n}
