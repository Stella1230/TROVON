static inline void rk_table_flush(struct rk_iommu_domain *dom, dma_addr_t dma,\r\nunsigned int count)\r\n{\r\nsize_t size = count * sizeof(u32);\r\ndma_sync_single_for_device(&dom->pdev->dev, dma, size, DMA_TO_DEVICE);\r\n}\r\nstatic struct rk_iommu_domain *to_rk_domain(struct iommu_domain *dom)\r\n{\r\nreturn container_of(dom, struct rk_iommu_domain, domain);\r\n}\r\nstatic inline phys_addr_t rk_dte_pt_address(u32 dte)\r\n{\r\nreturn (phys_addr_t)dte & RK_DTE_PT_ADDRESS_MASK;\r\n}\r\nstatic inline bool rk_dte_is_pt_valid(u32 dte)\r\n{\r\nreturn dte & RK_DTE_PT_VALID;\r\n}\r\nstatic inline u32 rk_mk_dte(dma_addr_t pt_dma)\r\n{\r\nreturn (pt_dma & RK_DTE_PT_ADDRESS_MASK) | RK_DTE_PT_VALID;\r\n}\r\nstatic inline phys_addr_t rk_pte_page_address(u32 pte)\r\n{\r\nreturn (phys_addr_t)pte & RK_PTE_PAGE_ADDRESS_MASK;\r\n}\r\nstatic inline bool rk_pte_is_page_valid(u32 pte)\r\n{\r\nreturn pte & RK_PTE_PAGE_VALID;\r\n}\r\nstatic u32 rk_mk_pte(phys_addr_t page, int prot)\r\n{\r\nu32 flags = 0;\r\nflags |= (prot & IOMMU_READ) ? RK_PTE_PAGE_READABLE : 0;\r\nflags |= (prot & IOMMU_WRITE) ? RK_PTE_PAGE_WRITABLE : 0;\r\npage &= RK_PTE_PAGE_ADDRESS_MASK;\r\nreturn page | flags | RK_PTE_PAGE_VALID;\r\n}\r\nstatic u32 rk_mk_pte_invalid(u32 pte)\r\n{\r\nreturn pte & ~RK_PTE_PAGE_VALID;\r\n}\r\nstatic u32 rk_iova_dte_index(dma_addr_t iova)\r\n{\r\nreturn (u32)(iova & RK_IOVA_DTE_MASK) >> RK_IOVA_DTE_SHIFT;\r\n}\r\nstatic u32 rk_iova_pte_index(dma_addr_t iova)\r\n{\r\nreturn (u32)(iova & RK_IOVA_PTE_MASK) >> RK_IOVA_PTE_SHIFT;\r\n}\r\nstatic u32 rk_iova_page_offset(dma_addr_t iova)\r\n{\r\nreturn (u32)(iova & RK_IOVA_PAGE_MASK) >> RK_IOVA_PAGE_SHIFT;\r\n}\r\nstatic u32 rk_iommu_read(void __iomem *base, u32 offset)\r\n{\r\nreturn readl(base + offset);\r\n}\r\nstatic void rk_iommu_write(void __iomem *base, u32 offset, u32 value)\r\n{\r\nwritel(value, base + offset);\r\n}\r\nstatic void rk_iommu_command(struct rk_iommu *iommu, u32 command)\r\n{\r\nint i;\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\nwritel(command, iommu->bases[i] + RK_MMU_COMMAND);\r\n}\r\nstatic void rk_iommu_base_command(void __iomem *base, u32 command)\r\n{\r\nwritel(command, base + RK_MMU_COMMAND);\r\n}\r\nstatic void rk_iommu_zap_lines(struct rk_iommu *iommu, dma_addr_t iova,\r\nsize_t size)\r\n{\r\nint i;\r\ndma_addr_t iova_end = iova + size;\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\nfor (; iova < iova_end; iova += SPAGE_SIZE)\r\nrk_iommu_write(iommu->bases[i], RK_MMU_ZAP_ONE_LINE, iova);\r\n}\r\nstatic bool rk_iommu_is_stall_active(struct rk_iommu *iommu)\r\n{\r\nbool active = true;\r\nint i;\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\nactive &= !!(rk_iommu_read(iommu->bases[i], RK_MMU_STATUS) &\r\nRK_MMU_STATUS_STALL_ACTIVE);\r\nreturn active;\r\n}\r\nstatic bool rk_iommu_is_paging_enabled(struct rk_iommu *iommu)\r\n{\r\nbool enable = true;\r\nint i;\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\nenable &= !!(rk_iommu_read(iommu->bases[i], RK_MMU_STATUS) &\r\nRK_MMU_STATUS_PAGING_ENABLED);\r\nreturn enable;\r\n}\r\nstatic int rk_iommu_enable_stall(struct rk_iommu *iommu)\r\n{\r\nint ret, i;\r\nif (rk_iommu_is_stall_active(iommu))\r\nreturn 0;\r\nif (!rk_iommu_is_paging_enabled(iommu))\r\nreturn 0;\r\nrk_iommu_command(iommu, RK_MMU_CMD_ENABLE_STALL);\r\nret = rk_wait_for(rk_iommu_is_stall_active(iommu), 1);\r\nif (ret)\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\ndev_err(iommu->dev, "Enable stall request timed out, status: %#08x\n",\r\nrk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\r\nreturn ret;\r\n}\r\nstatic int rk_iommu_disable_stall(struct rk_iommu *iommu)\r\n{\r\nint ret, i;\r\nif (!rk_iommu_is_stall_active(iommu))\r\nreturn 0;\r\nrk_iommu_command(iommu, RK_MMU_CMD_DISABLE_STALL);\r\nret = rk_wait_for(!rk_iommu_is_stall_active(iommu), 1);\r\nif (ret)\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\ndev_err(iommu->dev, "Disable stall request timed out, status: %#08x\n",\r\nrk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\r\nreturn ret;\r\n}\r\nstatic int rk_iommu_enable_paging(struct rk_iommu *iommu)\r\n{\r\nint ret, i;\r\nif (rk_iommu_is_paging_enabled(iommu))\r\nreturn 0;\r\nrk_iommu_command(iommu, RK_MMU_CMD_ENABLE_PAGING);\r\nret = rk_wait_for(rk_iommu_is_paging_enabled(iommu), 1);\r\nif (ret)\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\ndev_err(iommu->dev, "Enable paging request timed out, status: %#08x\n",\r\nrk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\r\nreturn ret;\r\n}\r\nstatic int rk_iommu_disable_paging(struct rk_iommu *iommu)\r\n{\r\nint ret, i;\r\nif (!rk_iommu_is_paging_enabled(iommu))\r\nreturn 0;\r\nrk_iommu_command(iommu, RK_MMU_CMD_DISABLE_PAGING);\r\nret = rk_wait_for(!rk_iommu_is_paging_enabled(iommu), 1);\r\nif (ret)\r\nfor (i = 0; i < iommu->num_mmu; i++)\r\ndev_err(iommu->dev, "Disable paging request timed out, status: %#08x\n",\r\nrk_iommu_read(iommu->bases[i], RK_MMU_STATUS));\r\nreturn ret;\r\n}\r\nstatic int rk_iommu_force_reset(struct rk_iommu *iommu)\r\n{\r\nint ret, i;\r\nu32 dte_addr;\r\nfor (i = 0; i < iommu->num_mmu; i++) {\r\nrk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR, DTE_ADDR_DUMMY);\r\ndte_addr = rk_iommu_read(iommu->bases[i], RK_MMU_DTE_ADDR);\r\nif (dte_addr != (DTE_ADDR_DUMMY & RK_DTE_PT_ADDRESS_MASK)) {\r\ndev_err(iommu->dev, "Error during raw reset. MMU_DTE_ADDR is not functioning\n");\r\nreturn -EFAULT;\r\n}\r\n}\r\nrk_iommu_command(iommu, RK_MMU_CMD_FORCE_RESET);\r\nfor (i = 0; i < iommu->num_mmu; i++) {\r\nret = rk_wait_for(rk_iommu_read(iommu->bases[i], RK_MMU_DTE_ADDR) == 0x00000000,\r\nFORCE_RESET_TIMEOUT);\r\nif (ret) {\r\ndev_err(iommu->dev, "FORCE_RESET command timed out\n");\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void log_iova(struct rk_iommu *iommu, int index, dma_addr_t iova)\r\n{\r\nvoid __iomem *base = iommu->bases[index];\r\nu32 dte_index, pte_index, page_offset;\r\nu32 mmu_dte_addr;\r\nphys_addr_t mmu_dte_addr_phys, dte_addr_phys;\r\nu32 *dte_addr;\r\nu32 dte;\r\nphys_addr_t pte_addr_phys = 0;\r\nu32 *pte_addr = NULL;\r\nu32 pte = 0;\r\nphys_addr_t page_addr_phys = 0;\r\nu32 page_flags = 0;\r\ndte_index = rk_iova_dte_index(iova);\r\npte_index = rk_iova_pte_index(iova);\r\npage_offset = rk_iova_page_offset(iova);\r\nmmu_dte_addr = rk_iommu_read(base, RK_MMU_DTE_ADDR);\r\nmmu_dte_addr_phys = (phys_addr_t)mmu_dte_addr;\r\ndte_addr_phys = mmu_dte_addr_phys + (4 * dte_index);\r\ndte_addr = phys_to_virt(dte_addr_phys);\r\ndte = *dte_addr;\r\nif (!rk_dte_is_pt_valid(dte))\r\ngoto print_it;\r\npte_addr_phys = rk_dte_pt_address(dte) + (pte_index * 4);\r\npte_addr = phys_to_virt(pte_addr_phys);\r\npte = *pte_addr;\r\nif (!rk_pte_is_page_valid(pte))\r\ngoto print_it;\r\npage_addr_phys = rk_pte_page_address(pte) + page_offset;\r\npage_flags = pte & RK_PTE_PAGE_FLAGS_MASK;\r\nprint_it:\r\ndev_err(iommu->dev, "iova = %pad: dte_index: %#03x pte_index: %#03x page_offset: %#03x\n",\r\n&iova, dte_index, pte_index, page_offset);\r\ndev_err(iommu->dev, "mmu_dte_addr: %pa dte@%pa: %#08x valid: %u pte@%pa: %#08x valid: %u page@%pa flags: %#03x\n",\r\n&mmu_dte_addr_phys, &dte_addr_phys, dte,\r\nrk_dte_is_pt_valid(dte), &pte_addr_phys, pte,\r\nrk_pte_is_page_valid(pte), &page_addr_phys, page_flags);\r\n}\r\nstatic irqreturn_t rk_iommu_irq(int irq, void *dev_id)\r\n{\r\nstruct rk_iommu *iommu = dev_id;\r\nu32 status;\r\nu32 int_status;\r\ndma_addr_t iova;\r\nirqreturn_t ret = IRQ_NONE;\r\nint i;\r\nfor (i = 0; i < iommu->num_mmu; i++) {\r\nint_status = rk_iommu_read(iommu->bases[i], RK_MMU_INT_STATUS);\r\nif (int_status == 0)\r\ncontinue;\r\nret = IRQ_HANDLED;\r\niova = rk_iommu_read(iommu->bases[i], RK_MMU_PAGE_FAULT_ADDR);\r\nif (int_status & RK_MMU_IRQ_PAGE_FAULT) {\r\nint flags;\r\nstatus = rk_iommu_read(iommu->bases[i], RK_MMU_STATUS);\r\nflags = (status & RK_MMU_STATUS_PAGE_FAULT_IS_WRITE) ?\r\nIOMMU_FAULT_WRITE : IOMMU_FAULT_READ;\r\ndev_err(iommu->dev, "Page fault at %pad of type %s\n",\r\n&iova,\r\n(flags == IOMMU_FAULT_WRITE) ? "write" : "read");\r\nlog_iova(iommu, i, iova);\r\nif (iommu->domain)\r\nreport_iommu_fault(iommu->domain, iommu->dev, iova,\r\nflags);\r\nelse\r\ndev_err(iommu->dev, "Page fault while iommu not attached to domain?\n");\r\nrk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_ZAP_CACHE);\r\nrk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_PAGE_FAULT_DONE);\r\n}\r\nif (int_status & RK_MMU_IRQ_BUS_ERROR)\r\ndev_err(iommu->dev, "BUS_ERROR occurred at %pad\n", &iova);\r\nif (int_status & ~RK_MMU_IRQ_MASK)\r\ndev_err(iommu->dev, "unexpected int_status: %#08x\n",\r\nint_status);\r\nrk_iommu_write(iommu->bases[i], RK_MMU_INT_CLEAR, int_status);\r\n}\r\nreturn ret;\r\n}\r\nstatic phys_addr_t rk_iommu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\r\nunsigned long flags;\r\nphys_addr_t pt_phys, phys = 0;\r\nu32 dte, pte;\r\nu32 *page_table;\r\nspin_lock_irqsave(&rk_domain->dt_lock, flags);\r\ndte = rk_domain->dt[rk_iova_dte_index(iova)];\r\nif (!rk_dte_is_pt_valid(dte))\r\ngoto out;\r\npt_phys = rk_dte_pt_address(dte);\r\npage_table = (u32 *)phys_to_virt(pt_phys);\r\npte = page_table[rk_iova_pte_index(iova)];\r\nif (!rk_pte_is_page_valid(pte))\r\ngoto out;\r\nphys = rk_pte_page_address(pte) + rk_iova_page_offset(iova);\r\nout:\r\nspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\r\nreturn phys;\r\n}\r\nstatic void rk_iommu_zap_iova(struct rk_iommu_domain *rk_domain,\r\ndma_addr_t iova, size_t size)\r\n{\r\nstruct list_head *pos;\r\nunsigned long flags;\r\nspin_lock_irqsave(&rk_domain->iommus_lock, flags);\r\nlist_for_each(pos, &rk_domain->iommus) {\r\nstruct rk_iommu *iommu;\r\niommu = list_entry(pos, struct rk_iommu, node);\r\nrk_iommu_zap_lines(iommu, iova, size);\r\n}\r\nspin_unlock_irqrestore(&rk_domain->iommus_lock, flags);\r\n}\r\nstatic void rk_iommu_zap_iova_first_last(struct rk_iommu_domain *rk_domain,\r\ndma_addr_t iova, size_t size)\r\n{\r\nrk_iommu_zap_iova(rk_domain, iova, SPAGE_SIZE);\r\nif (size > SPAGE_SIZE)\r\nrk_iommu_zap_iova(rk_domain, iova + size - SPAGE_SIZE,\r\nSPAGE_SIZE);\r\n}\r\nstatic u32 *rk_dte_get_page_table(struct rk_iommu_domain *rk_domain,\r\ndma_addr_t iova)\r\n{\r\nstruct device *dev = &rk_domain->pdev->dev;\r\nu32 *page_table, *dte_addr;\r\nu32 dte_index, dte;\r\nphys_addr_t pt_phys;\r\ndma_addr_t pt_dma;\r\nassert_spin_locked(&rk_domain->dt_lock);\r\ndte_index = rk_iova_dte_index(iova);\r\ndte_addr = &rk_domain->dt[dte_index];\r\ndte = *dte_addr;\r\nif (rk_dte_is_pt_valid(dte))\r\ngoto done;\r\npage_table = (u32 *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);\r\nif (!page_table)\r\nreturn ERR_PTR(-ENOMEM);\r\npt_dma = dma_map_single(dev, page_table, SPAGE_SIZE, DMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, pt_dma)) {\r\ndev_err(dev, "DMA mapping error while allocating page table\n");\r\nfree_page((unsigned long)page_table);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\ndte = rk_mk_dte(pt_dma);\r\n*dte_addr = dte;\r\nrk_table_flush(rk_domain, pt_dma, NUM_PT_ENTRIES);\r\nrk_table_flush(rk_domain,\r\nrk_domain->dt_dma + dte_index * sizeof(u32), 1);\r\ndone:\r\npt_phys = rk_dte_pt_address(dte);\r\nreturn (u32 *)phys_to_virt(pt_phys);\r\n}\r\nstatic size_t rk_iommu_unmap_iova(struct rk_iommu_domain *rk_domain,\r\nu32 *pte_addr, dma_addr_t pte_dma,\r\nsize_t size)\r\n{\r\nunsigned int pte_count;\r\nunsigned int pte_total = size / SPAGE_SIZE;\r\nassert_spin_locked(&rk_domain->dt_lock);\r\nfor (pte_count = 0; pte_count < pte_total; pte_count++) {\r\nu32 pte = pte_addr[pte_count];\r\nif (!rk_pte_is_page_valid(pte))\r\nbreak;\r\npte_addr[pte_count] = rk_mk_pte_invalid(pte);\r\n}\r\nrk_table_flush(rk_domain, pte_dma, pte_count);\r\nreturn pte_count * SPAGE_SIZE;\r\n}\r\nstatic int rk_iommu_map_iova(struct rk_iommu_domain *rk_domain, u32 *pte_addr,\r\ndma_addr_t pte_dma, dma_addr_t iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nunsigned int pte_count;\r\nunsigned int pte_total = size / SPAGE_SIZE;\r\nphys_addr_t page_phys;\r\nassert_spin_locked(&rk_domain->dt_lock);\r\nfor (pte_count = 0; pte_count < pte_total; pte_count++) {\r\nu32 pte = pte_addr[pte_count];\r\nif (rk_pte_is_page_valid(pte))\r\ngoto unwind;\r\npte_addr[pte_count] = rk_mk_pte(paddr, prot);\r\npaddr += SPAGE_SIZE;\r\n}\r\nrk_table_flush(rk_domain, pte_dma, pte_total);\r\nrk_iommu_zap_iova_first_last(rk_domain, iova, size);\r\nreturn 0;\r\nunwind:\r\nrk_iommu_unmap_iova(rk_domain, pte_addr, pte_dma,\r\npte_count * SPAGE_SIZE);\r\niova += pte_count * SPAGE_SIZE;\r\npage_phys = rk_pte_page_address(pte_addr[pte_count]);\r\npr_err("iova: %pad already mapped to %pa cannot remap to phys: %pa prot: %#x\n",\r\n&iova, &page_phys, &paddr, prot);\r\nreturn -EADDRINUSE;\r\n}\r\nstatic int rk_iommu_map(struct iommu_domain *domain, unsigned long _iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\r\nunsigned long flags;\r\ndma_addr_t pte_dma, iova = (dma_addr_t)_iova;\r\nu32 *page_table, *pte_addr;\r\nu32 dte_index, pte_index;\r\nint ret;\r\nspin_lock_irqsave(&rk_domain->dt_lock, flags);\r\npage_table = rk_dte_get_page_table(rk_domain, iova);\r\nif (IS_ERR(page_table)) {\r\nspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\r\nreturn PTR_ERR(page_table);\r\n}\r\ndte_index = rk_domain->dt[rk_iova_dte_index(iova)];\r\npte_index = rk_iova_pte_index(iova);\r\npte_addr = &page_table[pte_index];\r\npte_dma = rk_dte_pt_address(dte_index) + pte_index * sizeof(u32);\r\nret = rk_iommu_map_iova(rk_domain, pte_addr, pte_dma, iova,\r\npaddr, size, prot);\r\nspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\r\nreturn ret;\r\n}\r\nstatic size_t rk_iommu_unmap(struct iommu_domain *domain, unsigned long _iova,\r\nsize_t size)\r\n{\r\nstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\r\nunsigned long flags;\r\ndma_addr_t pte_dma, iova = (dma_addr_t)_iova;\r\nphys_addr_t pt_phys;\r\nu32 dte;\r\nu32 *pte_addr;\r\nsize_t unmap_size;\r\nspin_lock_irqsave(&rk_domain->dt_lock, flags);\r\ndte = rk_domain->dt[rk_iova_dte_index(iova)];\r\nif (!rk_dte_is_pt_valid(dte)) {\r\nspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\r\nreturn 0;\r\n}\r\npt_phys = rk_dte_pt_address(dte);\r\npte_addr = (u32 *)phys_to_virt(pt_phys) + rk_iova_pte_index(iova);\r\npte_dma = pt_phys + rk_iova_pte_index(iova) * sizeof(u32);\r\nunmap_size = rk_iommu_unmap_iova(rk_domain, pte_addr, pte_dma, size);\r\nspin_unlock_irqrestore(&rk_domain->dt_lock, flags);\r\nrk_iommu_zap_iova(rk_domain, iova, unmap_size);\r\nreturn unmap_size;\r\n}\r\nstatic struct rk_iommu *rk_iommu_from_dev(struct device *dev)\r\n{\r\nstruct iommu_group *group;\r\nstruct device *iommu_dev;\r\nstruct rk_iommu *rk_iommu;\r\ngroup = iommu_group_get(dev);\r\nif (!group)\r\nreturn NULL;\r\niommu_dev = iommu_group_get_iommudata(group);\r\nrk_iommu = dev_get_drvdata(iommu_dev);\r\niommu_group_put(group);\r\nreturn rk_iommu;\r\n}\r\nstatic int rk_iommu_attach_device(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct rk_iommu *iommu;\r\nstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\r\nunsigned long flags;\r\nint ret, i;\r\niommu = rk_iommu_from_dev(dev);\r\nif (!iommu)\r\nreturn 0;\r\nret = rk_iommu_enable_stall(iommu);\r\nif (ret)\r\nreturn ret;\r\nret = rk_iommu_force_reset(iommu);\r\nif (ret)\r\nreturn ret;\r\niommu->domain = domain;\r\nret = devm_request_irq(iommu->dev, iommu->irq, rk_iommu_irq,\r\nIRQF_SHARED, dev_name(dev), iommu);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < iommu->num_mmu; i++) {\r\nrk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR,\r\nrk_domain->dt_dma);\r\nrk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_ZAP_CACHE);\r\nrk_iommu_write(iommu->bases[i], RK_MMU_INT_MASK, RK_MMU_IRQ_MASK);\r\n}\r\nret = rk_iommu_enable_paging(iommu);\r\nif (ret)\r\nreturn ret;\r\nspin_lock_irqsave(&rk_domain->iommus_lock, flags);\r\nlist_add_tail(&iommu->node, &rk_domain->iommus);\r\nspin_unlock_irqrestore(&rk_domain->iommus_lock, flags);\r\ndev_dbg(dev, "Attached to iommu domain\n");\r\nrk_iommu_disable_stall(iommu);\r\nreturn 0;\r\n}\r\nstatic void rk_iommu_detach_device(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct rk_iommu *iommu;\r\nstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\r\nunsigned long flags;\r\nint i;\r\niommu = rk_iommu_from_dev(dev);\r\nif (!iommu)\r\nreturn;\r\nspin_lock_irqsave(&rk_domain->iommus_lock, flags);\r\nlist_del_init(&iommu->node);\r\nspin_unlock_irqrestore(&rk_domain->iommus_lock, flags);\r\nrk_iommu_enable_stall(iommu);\r\nrk_iommu_disable_paging(iommu);\r\nfor (i = 0; i < iommu->num_mmu; i++) {\r\nrk_iommu_write(iommu->bases[i], RK_MMU_INT_MASK, 0);\r\nrk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR, 0);\r\n}\r\nrk_iommu_disable_stall(iommu);\r\ndevm_free_irq(iommu->dev, iommu->irq, iommu);\r\niommu->domain = NULL;\r\ndev_dbg(dev, "Detached from iommu domain\n");\r\n}\r\nstatic struct iommu_domain *rk_iommu_domain_alloc(unsigned type)\r\n{\r\nstruct rk_iommu_domain *rk_domain;\r\nstruct platform_device *pdev;\r\nstruct device *iommu_dev;\r\nif (type != IOMMU_DOMAIN_UNMANAGED && type != IOMMU_DOMAIN_DMA)\r\nreturn NULL;\r\npdev = platform_device_register_simple("rk_iommu_domain",\r\nPLATFORM_DEVID_AUTO, NULL, 0);\r\nif (IS_ERR(pdev))\r\nreturn NULL;\r\nrk_domain = devm_kzalloc(&pdev->dev, sizeof(*rk_domain), GFP_KERNEL);\r\nif (!rk_domain)\r\ngoto err_unreg_pdev;\r\nrk_domain->pdev = pdev;\r\nif (type == IOMMU_DOMAIN_DMA &&\r\niommu_get_dma_cookie(&rk_domain->domain))\r\ngoto err_unreg_pdev;\r\nrk_domain->dt = (u32 *)get_zeroed_page(GFP_KERNEL | GFP_DMA32);\r\nif (!rk_domain->dt)\r\ngoto err_put_cookie;\r\niommu_dev = &pdev->dev;\r\nrk_domain->dt_dma = dma_map_single(iommu_dev, rk_domain->dt,\r\nSPAGE_SIZE, DMA_TO_DEVICE);\r\nif (dma_mapping_error(iommu_dev, rk_domain->dt_dma)) {\r\ndev_err(iommu_dev, "DMA map error for DT\n");\r\ngoto err_free_dt;\r\n}\r\nrk_table_flush(rk_domain, rk_domain->dt_dma, NUM_DT_ENTRIES);\r\nspin_lock_init(&rk_domain->iommus_lock);\r\nspin_lock_init(&rk_domain->dt_lock);\r\nINIT_LIST_HEAD(&rk_domain->iommus);\r\nrk_domain->domain.geometry.aperture_start = 0;\r\nrk_domain->domain.geometry.aperture_end = DMA_BIT_MASK(32);\r\nrk_domain->domain.geometry.force_aperture = true;\r\nreturn &rk_domain->domain;\r\nerr_free_dt:\r\nfree_page((unsigned long)rk_domain->dt);\r\nerr_put_cookie:\r\nif (type == IOMMU_DOMAIN_DMA)\r\niommu_put_dma_cookie(&rk_domain->domain);\r\nerr_unreg_pdev:\r\nplatform_device_unregister(pdev);\r\nreturn NULL;\r\n}\r\nstatic void rk_iommu_domain_free(struct iommu_domain *domain)\r\n{\r\nstruct rk_iommu_domain *rk_domain = to_rk_domain(domain);\r\nint i;\r\nWARN_ON(!list_empty(&rk_domain->iommus));\r\nfor (i = 0; i < NUM_DT_ENTRIES; i++) {\r\nu32 dte = rk_domain->dt[i];\r\nif (rk_dte_is_pt_valid(dte)) {\r\nphys_addr_t pt_phys = rk_dte_pt_address(dte);\r\nu32 *page_table = phys_to_virt(pt_phys);\r\ndma_unmap_single(&rk_domain->pdev->dev, pt_phys,\r\nSPAGE_SIZE, DMA_TO_DEVICE);\r\nfree_page((unsigned long)page_table);\r\n}\r\n}\r\ndma_unmap_single(&rk_domain->pdev->dev, rk_domain->dt_dma,\r\nSPAGE_SIZE, DMA_TO_DEVICE);\r\nfree_page((unsigned long)rk_domain->dt);\r\nif (domain->type == IOMMU_DOMAIN_DMA)\r\niommu_put_dma_cookie(&rk_domain->domain);\r\nplatform_device_unregister(rk_domain->pdev);\r\n}\r\nstatic bool rk_iommu_is_dev_iommu_master(struct device *dev)\r\n{\r\nstruct device_node *np = dev->of_node;\r\nint ret;\r\nret = of_count_phandle_with_args(np, "iommus", "#iommu-cells");\r\nreturn (ret > 0);\r\n}\r\nstatic int rk_iommu_group_set_iommudata(struct iommu_group *group,\r\nstruct device *dev)\r\n{\r\nstruct device_node *np = dev->of_node;\r\nstruct platform_device *pd;\r\nint ret;\r\nstruct of_phandle_args args;\r\nret = of_parse_phandle_with_args(np, "iommus", "#iommu-cells", 0,\r\n&args);\r\nif (ret) {\r\ndev_err(dev, "of_parse_phandle_with_args(%s) => %d\n",\r\nnp->full_name, ret);\r\nreturn ret;\r\n}\r\nif (args.args_count != 0) {\r\ndev_err(dev, "incorrect number of iommu params found for %s (found %d, expected 0)\n",\r\nargs.np->full_name, args.args_count);\r\nreturn -EINVAL;\r\n}\r\npd = of_find_device_by_node(args.np);\r\nof_node_put(args.np);\r\nif (!pd) {\r\ndev_err(dev, "iommu %s not found\n", args.np->full_name);\r\nreturn -EPROBE_DEFER;\r\n}\r\niommu_group_set_iommudata(group, &pd->dev, NULL);\r\nreturn 0;\r\n}\r\nstatic int rk_iommu_add_device(struct device *dev)\r\n{\r\nstruct iommu_group *group;\r\nint ret;\r\nif (!rk_iommu_is_dev_iommu_master(dev))\r\nreturn -ENODEV;\r\ngroup = iommu_group_get(dev);\r\nif (!group) {\r\ngroup = iommu_group_alloc();\r\nif (IS_ERR(group)) {\r\ndev_err(dev, "Failed to allocate IOMMU group\n");\r\nreturn PTR_ERR(group);\r\n}\r\n}\r\nret = iommu_group_add_device(group, dev);\r\nif (ret)\r\ngoto err_put_group;\r\nret = rk_iommu_group_set_iommudata(group, dev);\r\nif (ret)\r\ngoto err_remove_device;\r\niommu_group_put(group);\r\nreturn 0;\r\nerr_remove_device:\r\niommu_group_remove_device(dev);\r\nerr_put_group:\r\niommu_group_put(group);\r\nreturn ret;\r\n}\r\nstatic void rk_iommu_remove_device(struct device *dev)\r\n{\r\nif (!rk_iommu_is_dev_iommu_master(dev))\r\nreturn;\r\niommu_group_remove_device(dev);\r\n}\r\nstatic int rk_iommu_domain_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\ndev->dma_parms = devm_kzalloc(dev, sizeof(*dev->dma_parms), GFP_KERNEL);\r\nif (!dev->dma_parms)\r\nreturn -ENOMEM;\r\narch_setup_dma_ops(dev, 0, DMA_BIT_MASK(32), NULL, false);\r\ndma_set_max_seg_size(dev, DMA_BIT_MASK(32));\r\ndma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(32));\r\nreturn 0;\r\n}\r\nstatic int rk_iommu_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct rk_iommu *iommu;\r\nstruct resource *res;\r\nint num_res = pdev->num_resources;\r\nint i;\r\niommu = devm_kzalloc(dev, sizeof(*iommu), GFP_KERNEL);\r\nif (!iommu)\r\nreturn -ENOMEM;\r\nplatform_set_drvdata(pdev, iommu);\r\niommu->dev = dev;\r\niommu->num_mmu = 0;\r\niommu->bases = devm_kzalloc(dev, sizeof(*iommu->bases) * num_res,\r\nGFP_KERNEL);\r\nif (!iommu->bases)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < num_res; i++) {\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, i);\r\nif (!res)\r\ncontinue;\r\niommu->bases[i] = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(iommu->bases[i]))\r\ncontinue;\r\niommu->num_mmu++;\r\n}\r\nif (iommu->num_mmu == 0)\r\nreturn PTR_ERR(iommu->bases[0]);\r\niommu->irq = platform_get_irq(pdev, 0);\r\nif (iommu->irq < 0) {\r\ndev_err(dev, "Failed to get IRQ, %d\n", iommu->irq);\r\nreturn -ENXIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rk_iommu_remove(struct platform_device *pdev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int __init rk_iommu_init(void)\r\n{\r\nstruct device_node *np;\r\nint ret;\r\nnp = of_find_matching_node(NULL, rk_iommu_dt_ids);\r\nif (!np)\r\nreturn 0;\r\nof_node_put(np);\r\nret = bus_set_iommu(&platform_bus_type, &rk_iommu_ops);\r\nif (ret)\r\nreturn ret;\r\nret = platform_driver_register(&rk_iommu_domain_driver);\r\nif (ret)\r\nreturn ret;\r\nret = platform_driver_register(&rk_iommu_driver);\r\nif (ret)\r\nplatform_driver_unregister(&rk_iommu_domain_driver);\r\nreturn ret;\r\n}\r\nstatic void __exit rk_iommu_exit(void)\r\n{\r\nplatform_driver_unregister(&rk_iommu_driver);\r\nplatform_driver_unregister(&rk_iommu_domain_driver);\r\n}
