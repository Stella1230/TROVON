static void si_init_golden_registers(struct radeon_device *rdev)\r\n{\r\nswitch (rdev->family) {\r\ncase CHIP_TAHITI:\r\nradeon_program_register_sequence(rdev,\r\ntahiti_golden_registers,\r\n(const u32)ARRAY_SIZE(tahiti_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\ntahiti_golden_rlc_registers,\r\n(const u32)ARRAY_SIZE(tahiti_golden_rlc_registers));\r\nradeon_program_register_sequence(rdev,\r\ntahiti_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(tahiti_mgcg_cgcg_init));\r\nradeon_program_register_sequence(rdev,\r\ntahiti_golden_registers2,\r\n(const u32)ARRAY_SIZE(tahiti_golden_registers2));\r\nbreak;\r\ncase CHIP_PITCAIRN:\r\nradeon_program_register_sequence(rdev,\r\npitcairn_golden_registers,\r\n(const u32)ARRAY_SIZE(pitcairn_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\npitcairn_golden_rlc_registers,\r\n(const u32)ARRAY_SIZE(pitcairn_golden_rlc_registers));\r\nradeon_program_register_sequence(rdev,\r\npitcairn_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(pitcairn_mgcg_cgcg_init));\r\nbreak;\r\ncase CHIP_VERDE:\r\nradeon_program_register_sequence(rdev,\r\nverde_golden_registers,\r\n(const u32)ARRAY_SIZE(verde_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\nverde_golden_rlc_registers,\r\n(const u32)ARRAY_SIZE(verde_golden_rlc_registers));\r\nradeon_program_register_sequence(rdev,\r\nverde_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(verde_mgcg_cgcg_init));\r\nradeon_program_register_sequence(rdev,\r\nverde_pg_init,\r\n(const u32)ARRAY_SIZE(verde_pg_init));\r\nbreak;\r\ncase CHIP_OLAND:\r\nradeon_program_register_sequence(rdev,\r\noland_golden_registers,\r\n(const u32)ARRAY_SIZE(oland_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\noland_golden_rlc_registers,\r\n(const u32)ARRAY_SIZE(oland_golden_rlc_registers));\r\nradeon_program_register_sequence(rdev,\r\noland_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(oland_mgcg_cgcg_init));\r\nbreak;\r\ncase CHIP_HAINAN:\r\nradeon_program_register_sequence(rdev,\r\nhainan_golden_registers,\r\n(const u32)ARRAY_SIZE(hainan_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\nhainan_golden_registers2,\r\n(const u32)ARRAY_SIZE(hainan_golden_registers2));\r\nradeon_program_register_sequence(rdev,\r\nhainan_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(hainan_mgcg_cgcg_init));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nint si_get_allowed_info_register(struct radeon_device *rdev,\r\nu32 reg, u32 *val)\r\n{\r\nswitch (reg) {\r\ncase GRBM_STATUS:\r\ncase GRBM_STATUS2:\r\ncase GRBM_STATUS_SE0:\r\ncase GRBM_STATUS_SE1:\r\ncase SRBM_STATUS:\r\ncase SRBM_STATUS2:\r\ncase (DMA_STATUS_REG + DMA0_REGISTER_OFFSET):\r\ncase (DMA_STATUS_REG + DMA1_REGISTER_OFFSET):\r\ncase UVD_STATUS:\r\n*val = RREG32(reg);\r\nreturn 0;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nu32 si_get_xclk(struct radeon_device *rdev)\r\n{\r\nu32 reference_clock = rdev->clock.spll.reference_freq;\r\nu32 tmp;\r\ntmp = RREG32(CG_CLKPIN_CNTL_2);\r\nif (tmp & MUX_TCLK_TO_XCLK)\r\nreturn TCLK;\r\ntmp = RREG32(CG_CLKPIN_CNTL);\r\nif (tmp & XTALIN_DIVIDE)\r\nreturn reference_clock / 4;\r\nreturn reference_clock;\r\n}\r\nint si_get_temp(struct radeon_device *rdev)\r\n{\r\nu32 temp;\r\nint actual_temp = 0;\r\ntemp = (RREG32(CG_MULT_THERMAL_STATUS) & CTF_TEMP_MASK) >>\r\nCTF_TEMP_SHIFT;\r\nif (temp & 0x200)\r\nactual_temp = 255;\r\nelse\r\nactual_temp = temp & 0x1ff;\r\nactual_temp = (actual_temp * 1000);\r\nreturn actual_temp;\r\n}\r\nint si_mc_load_microcode(struct radeon_device *rdev)\r\n{\r\nconst __be32 *fw_data = NULL;\r\nconst __le32 *new_fw_data = NULL;\r\nu32 running;\r\nu32 *io_mc_regs = NULL;\r\nconst __le32 *new_io_mc_regs = NULL;\r\nint i, regs_size, ucode_size;\r\nif (!rdev->mc_fw)\r\nreturn -EINVAL;\r\nif (rdev->new_fw) {\r\nconst struct mc_firmware_header_v1_0 *hdr =\r\n(const struct mc_firmware_header_v1_0 *)rdev->mc_fw->data;\r\nradeon_ucode_print_mc_hdr(&hdr->header);\r\nregs_size = le32_to_cpu(hdr->io_debug_size_bytes) / (4 * 2);\r\nnew_io_mc_regs = (const __le32 *)\r\n(rdev->mc_fw->data + le32_to_cpu(hdr->io_debug_array_offset_bytes));\r\nucode_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\r\nnew_fw_data = (const __le32 *)\r\n(rdev->mc_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\n} else {\r\nucode_size = rdev->mc_fw->size / 4;\r\nswitch (rdev->family) {\r\ncase CHIP_TAHITI:\r\nio_mc_regs = (u32 *)&tahiti_io_mc_regs;\r\nregs_size = TAHITI_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_PITCAIRN:\r\nio_mc_regs = (u32 *)&pitcairn_io_mc_regs;\r\nregs_size = TAHITI_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_VERDE:\r\ndefault:\r\nio_mc_regs = (u32 *)&verde_io_mc_regs;\r\nregs_size = TAHITI_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_OLAND:\r\nio_mc_regs = (u32 *)&oland_io_mc_regs;\r\nregs_size = TAHITI_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_HAINAN:\r\nio_mc_regs = (u32 *)&hainan_io_mc_regs;\r\nregs_size = TAHITI_IO_MC_REGS_SIZE;\r\nbreak;\r\n}\r\nfw_data = (const __be32 *)rdev->mc_fw->data;\r\n}\r\nrunning = RREG32(MC_SEQ_SUP_CNTL) & RUN_MASK;\r\nif (running == 0) {\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000010);\r\nfor (i = 0; i < regs_size; i++) {\r\nif (rdev->new_fw) {\r\nWREG32(MC_SEQ_IO_DEBUG_INDEX, le32_to_cpup(new_io_mc_regs++));\r\nWREG32(MC_SEQ_IO_DEBUG_DATA, le32_to_cpup(new_io_mc_regs++));\r\n} else {\r\nWREG32(MC_SEQ_IO_DEBUG_INDEX, io_mc_regs[(i << 1)]);\r\nWREG32(MC_SEQ_IO_DEBUG_DATA, io_mc_regs[(i << 1) + 1]);\r\n}\r\n}\r\nfor (i = 0; i < ucode_size; i++) {\r\nif (rdev->new_fw)\r\nWREG32(MC_SEQ_SUP_PGM, le32_to_cpup(new_fw_data++));\r\nelse\r\nWREG32(MC_SEQ_SUP_PGM, be32_to_cpup(fw_data++));\r\n}\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000004);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000001);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(MC_SEQ_TRAIN_WAKEUP_CNTL) & TRAIN_DONE_D0)\r\nbreak;\r\nudelay(1);\r\n}\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(MC_SEQ_TRAIN_WAKEUP_CNTL) & TRAIN_DONE_D1)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int si_init_microcode(struct radeon_device *rdev)\r\n{\r\nconst char *chip_name;\r\nconst char *new_chip_name;\r\nsize_t pfp_req_size, me_req_size, ce_req_size, rlc_req_size, mc_req_size;\r\nsize_t smc_req_size, mc2_req_size;\r\nchar fw_name[30];\r\nint err;\r\nint new_fw = 0;\r\nbool new_smc = false;\r\nbool si58_fw = false;\r\nbool banks2_fw = false;\r\nDRM_DEBUG("\n");\r\nswitch (rdev->family) {\r\ncase CHIP_TAHITI:\r\nchip_name = "TAHITI";\r\nnew_chip_name = "tahiti";\r\npfp_req_size = SI_PFP_UCODE_SIZE * 4;\r\nme_req_size = SI_PM4_UCODE_SIZE * 4;\r\nce_req_size = SI_CE_UCODE_SIZE * 4;\r\nrlc_req_size = SI_RLC_UCODE_SIZE * 4;\r\nmc_req_size = SI_MC_UCODE_SIZE * 4;\r\nmc2_req_size = TAHITI_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(TAHITI_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_PITCAIRN:\r\nchip_name = "PITCAIRN";\r\nif ((rdev->pdev->revision == 0x81) &&\r\n((rdev->pdev->device == 0x6810) ||\r\n(rdev->pdev->device == 0x6811)))\r\nnew_smc = true;\r\nnew_chip_name = "pitcairn";\r\npfp_req_size = SI_PFP_UCODE_SIZE * 4;\r\nme_req_size = SI_PM4_UCODE_SIZE * 4;\r\nce_req_size = SI_CE_UCODE_SIZE * 4;\r\nrlc_req_size = SI_RLC_UCODE_SIZE * 4;\r\nmc_req_size = SI_MC_UCODE_SIZE * 4;\r\nmc2_req_size = PITCAIRN_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(PITCAIRN_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_VERDE:\r\nchip_name = "VERDE";\r\nif (((rdev->pdev->device == 0x6820) &&\r\n((rdev->pdev->revision == 0x81) ||\r\n(rdev->pdev->revision == 0x83))) ||\r\n((rdev->pdev->device == 0x6821) &&\r\n((rdev->pdev->revision == 0x83) ||\r\n(rdev->pdev->revision == 0x87))) ||\r\n((rdev->pdev->revision == 0x87) &&\r\n((rdev->pdev->device == 0x6823) ||\r\n(rdev->pdev->device == 0x682b))))\r\nnew_smc = true;\r\nnew_chip_name = "verde";\r\npfp_req_size = SI_PFP_UCODE_SIZE * 4;\r\nme_req_size = SI_PM4_UCODE_SIZE * 4;\r\nce_req_size = SI_CE_UCODE_SIZE * 4;\r\nrlc_req_size = SI_RLC_UCODE_SIZE * 4;\r\nmc_req_size = SI_MC_UCODE_SIZE * 4;\r\nmc2_req_size = VERDE_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(VERDE_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_OLAND:\r\nchip_name = "OLAND";\r\nif (((rdev->pdev->revision == 0x81) &&\r\n((rdev->pdev->device == 0x6600) ||\r\n(rdev->pdev->device == 0x6604) ||\r\n(rdev->pdev->device == 0x6605) ||\r\n(rdev->pdev->device == 0x6610))) ||\r\n((rdev->pdev->revision == 0x83) &&\r\n(rdev->pdev->device == 0x6610)))\r\nnew_smc = true;\r\nnew_chip_name = "oland";\r\npfp_req_size = SI_PFP_UCODE_SIZE * 4;\r\nme_req_size = SI_PM4_UCODE_SIZE * 4;\r\nce_req_size = SI_CE_UCODE_SIZE * 4;\r\nrlc_req_size = SI_RLC_UCODE_SIZE * 4;\r\nmc_req_size = mc2_req_size = OLAND_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(OLAND_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_HAINAN:\r\nchip_name = "HAINAN";\r\nif (((rdev->pdev->revision == 0x81) &&\r\n(rdev->pdev->device == 0x6660)) ||\r\n((rdev->pdev->revision == 0x83) &&\r\n((rdev->pdev->device == 0x6660) ||\r\n(rdev->pdev->device == 0x6663) ||\r\n(rdev->pdev->device == 0x6665) ||\r\n(rdev->pdev->device == 0x6667))))\r\nnew_smc = true;\r\nelse if ((rdev->pdev->revision == 0xc3) &&\r\n(rdev->pdev->device == 0x6665))\r\nbanks2_fw = true;\r\nnew_chip_name = "hainan";\r\npfp_req_size = SI_PFP_UCODE_SIZE * 4;\r\nme_req_size = SI_PM4_UCODE_SIZE * 4;\r\nce_req_size = SI_CE_UCODE_SIZE * 4;\r\nrlc_req_size = SI_RLC_UCODE_SIZE * 4;\r\nmc_req_size = mc2_req_size = OLAND_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(HAINAN_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ndefault: BUG();\r\n}\r\nif (((RREG32(MC_SEQ_MISC0) & 0xff000000) >> 24) == 0x58)\r\nsi58_fw = true;\r\nDRM_INFO("Loading %s Microcode\n", new_chip_name);\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_pfp.bin", new_chip_name);\r\nerr = request_firmware(&rdev->pfp_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_pfp.bin", chip_name);\r\nerr = request_firmware(&rdev->pfp_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->pfp_fw->size != pfp_req_size) {\r\nprintk(KERN_ERR\r\n"si_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->pfp_fw->size, fw_name);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->pfp_fw);\r\nif (err) {\r\nprintk(KERN_ERR\r\n"si_cp: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_me.bin", new_chip_name);\r\nerr = request_firmware(&rdev->me_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_me.bin", chip_name);\r\nerr = request_firmware(&rdev->me_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->me_fw->size != me_req_size) {\r\nprintk(KERN_ERR\r\n"si_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->me_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->me_fw);\r\nif (err) {\r\nprintk(KERN_ERR\r\n"si_cp: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_ce.bin", new_chip_name);\r\nerr = request_firmware(&rdev->ce_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_ce.bin", chip_name);\r\nerr = request_firmware(&rdev->ce_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->ce_fw->size != ce_req_size) {\r\nprintk(KERN_ERR\r\n"si_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->ce_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->ce_fw);\r\nif (err) {\r\nprintk(KERN_ERR\r\n"si_cp: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_rlc.bin", new_chip_name);\r\nerr = request_firmware(&rdev->rlc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_rlc.bin", chip_name);\r\nerr = request_firmware(&rdev->rlc_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->rlc_fw->size != rlc_req_size) {\r\nprintk(KERN_ERR\r\n"si_rlc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->rlc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->rlc_fw);\r\nif (err) {\r\nprintk(KERN_ERR\r\n"si_cp: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nif (si58_fw)\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/si58_mc.bin");\r\nelse\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc.bin", new_chip_name);\r\nerr = request_firmware(&rdev->mc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc2.bin", chip_name);\r\nerr = request_firmware(&rdev->mc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc.bin", chip_name);\r\nerr = request_firmware(&rdev->mc_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\n}\r\nif ((rdev->mc_fw->size != mc_req_size) &&\r\n(rdev->mc_fw->size != mc2_req_size)) {\r\nprintk(KERN_ERR\r\n"si_mc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->mc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\nDRM_INFO("%s: %zu bytes\n", fw_name, rdev->mc_fw->size);\r\n} else {\r\nerr = radeon_ucode_validate(rdev->mc_fw);\r\nif (err) {\r\nprintk(KERN_ERR\r\n"si_cp: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nif (banks2_fw)\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/banks_k_2_smc.bin");\r\nelse if (new_smc)\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_k_smc.bin", new_chip_name);\r\nelse\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_smc.bin", new_chip_name);\r\nerr = request_firmware(&rdev->smc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_smc.bin", chip_name);\r\nerr = request_firmware(&rdev->smc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nprintk(KERN_ERR\r\n"smc: error loading firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(rdev->smc_fw);\r\nrdev->smc_fw = NULL;\r\nerr = 0;\r\n} else if (rdev->smc_fw->size != smc_req_size) {\r\nprintk(KERN_ERR\r\n"si_smc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->smc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->smc_fw);\r\nif (err) {\r\nprintk(KERN_ERR\r\n"si_cp: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nif (new_fw == 0) {\r\nrdev->new_fw = false;\r\n} else if (new_fw < 6) {\r\nprintk(KERN_ERR "si_fw: mixing new and old firmware!\n");\r\nerr = -EINVAL;\r\n} else {\r\nrdev->new_fw = true;\r\n}\r\nout:\r\nif (err) {\r\nif (err != -EINVAL)\r\nprintk(KERN_ERR\r\n"si_cp: Failed to load firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(rdev->pfp_fw);\r\nrdev->pfp_fw = NULL;\r\nrelease_firmware(rdev->me_fw);\r\nrdev->me_fw = NULL;\r\nrelease_firmware(rdev->ce_fw);\r\nrdev->ce_fw = NULL;\r\nrelease_firmware(rdev->rlc_fw);\r\nrdev->rlc_fw = NULL;\r\nrelease_firmware(rdev->mc_fw);\r\nrdev->mc_fw = NULL;\r\nrelease_firmware(rdev->smc_fw);\r\nrdev->smc_fw = NULL;\r\n}\r\nreturn err;\r\n}\r\nstatic u32 dce6_line_buffer_adjust(struct radeon_device *rdev,\r\nstruct radeon_crtc *radeon_crtc,\r\nstruct drm_display_mode *mode,\r\nstruct drm_display_mode *other_mode)\r\n{\r\nu32 tmp, buffer_alloc, i;\r\nu32 pipe_offset = radeon_crtc->crtc_id * 0x20;\r\nif (radeon_crtc->base.enabled && mode) {\r\nif (other_mode) {\r\ntmp = 0;\r\nbuffer_alloc = 1;\r\n} else {\r\ntmp = 2;\r\nbuffer_alloc = 2;\r\n}\r\n} else {\r\ntmp = 0;\r\nbuffer_alloc = 0;\r\n}\r\nWREG32(DC_LB_MEMORY_SPLIT + radeon_crtc->crtc_offset,\r\nDC_LB_MEMORY_CONFIG(tmp));\r\nWREG32(PIPE0_DMIF_BUFFER_CONTROL + pipe_offset,\r\nDMIF_BUFFERS_ALLOCATED(buffer_alloc));\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(PIPE0_DMIF_BUFFER_CONTROL + pipe_offset) &\r\nDMIF_BUFFERS_ALLOCATED_COMPLETED)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (radeon_crtc->base.enabled && mode) {\r\nswitch (tmp) {\r\ncase 0:\r\ndefault:\r\nreturn 4096 * 2;\r\ncase 2:\r\nreturn 8192 * 2;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic u32 si_get_number_of_dram_channels(struct radeon_device *rdev)\r\n{\r\nu32 tmp = RREG32(MC_SHARED_CHMAP);\r\nswitch ((tmp & NOOFCHAN_MASK) >> NOOFCHAN_SHIFT) {\r\ncase 0:\r\ndefault:\r\nreturn 1;\r\ncase 1:\r\nreturn 2;\r\ncase 2:\r\nreturn 4;\r\ncase 3:\r\nreturn 8;\r\ncase 4:\r\nreturn 3;\r\ncase 5:\r\nreturn 6;\r\ncase 6:\r\nreturn 10;\r\ncase 7:\r\nreturn 12;\r\ncase 8:\r\nreturn 16;\r\n}\r\n}\r\nstatic u32 dce6_dram_bandwidth(struct dce6_wm_params *wm)\r\n{\r\nfixed20_12 dram_efficiency;\r\nfixed20_12 yclk, dram_channels, bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nyclk.full = dfixed_const(wm->yclk);\r\nyclk.full = dfixed_div(yclk, a);\r\ndram_channels.full = dfixed_const(wm->dram_channels * 4);\r\na.full = dfixed_const(10);\r\ndram_efficiency.full = dfixed_const(7);\r\ndram_efficiency.full = dfixed_div(dram_efficiency, a);\r\nbandwidth.full = dfixed_mul(dram_channels, yclk);\r\nbandwidth.full = dfixed_mul(bandwidth, dram_efficiency);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce6_dram_bandwidth_for_display(struct dce6_wm_params *wm)\r\n{\r\nfixed20_12 disp_dram_allocation;\r\nfixed20_12 yclk, dram_channels, bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nyclk.full = dfixed_const(wm->yclk);\r\nyclk.full = dfixed_div(yclk, a);\r\ndram_channels.full = dfixed_const(wm->dram_channels * 4);\r\na.full = dfixed_const(10);\r\ndisp_dram_allocation.full = dfixed_const(3);\r\ndisp_dram_allocation.full = dfixed_div(disp_dram_allocation, a);\r\nbandwidth.full = dfixed_mul(dram_channels, yclk);\r\nbandwidth.full = dfixed_mul(bandwidth, disp_dram_allocation);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce6_data_return_bandwidth(struct dce6_wm_params *wm)\r\n{\r\nfixed20_12 return_efficiency;\r\nfixed20_12 sclk, bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nsclk.full = dfixed_const(wm->sclk);\r\nsclk.full = dfixed_div(sclk, a);\r\na.full = dfixed_const(10);\r\nreturn_efficiency.full = dfixed_const(8);\r\nreturn_efficiency.full = dfixed_div(return_efficiency, a);\r\na.full = dfixed_const(32);\r\nbandwidth.full = dfixed_mul(a, sclk);\r\nbandwidth.full = dfixed_mul(bandwidth, return_efficiency);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce6_get_dmif_bytes_per_request(struct dce6_wm_params *wm)\r\n{\r\nreturn 32;\r\n}\r\nstatic u32 dce6_dmif_request_bandwidth(struct dce6_wm_params *wm)\r\n{\r\nfixed20_12 disp_clk_request_efficiency;\r\nfixed20_12 disp_clk, sclk, bandwidth;\r\nfixed20_12 a, b1, b2;\r\nu32 min_bandwidth;\r\na.full = dfixed_const(1000);\r\ndisp_clk.full = dfixed_const(wm->disp_clk);\r\ndisp_clk.full = dfixed_div(disp_clk, a);\r\na.full = dfixed_const(dce6_get_dmif_bytes_per_request(wm) / 2);\r\nb1.full = dfixed_mul(a, disp_clk);\r\na.full = dfixed_const(1000);\r\nsclk.full = dfixed_const(wm->sclk);\r\nsclk.full = dfixed_div(sclk, a);\r\na.full = dfixed_const(dce6_get_dmif_bytes_per_request(wm));\r\nb2.full = dfixed_mul(a, sclk);\r\na.full = dfixed_const(10);\r\ndisp_clk_request_efficiency.full = dfixed_const(8);\r\ndisp_clk_request_efficiency.full = dfixed_div(disp_clk_request_efficiency, a);\r\nmin_bandwidth = min(dfixed_trunc(b1), dfixed_trunc(b2));\r\na.full = dfixed_const(min_bandwidth);\r\nbandwidth.full = dfixed_mul(a, disp_clk_request_efficiency);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce6_available_bandwidth(struct dce6_wm_params *wm)\r\n{\r\nu32 dram_bandwidth = dce6_dram_bandwidth(wm);\r\nu32 data_return_bandwidth = dce6_data_return_bandwidth(wm);\r\nu32 dmif_req_bandwidth = dce6_dmif_request_bandwidth(wm);\r\nreturn min(dram_bandwidth, min(data_return_bandwidth, dmif_req_bandwidth));\r\n}\r\nstatic u32 dce6_average_bandwidth(struct dce6_wm_params *wm)\r\n{\r\nfixed20_12 bpp;\r\nfixed20_12 line_time;\r\nfixed20_12 src_width;\r\nfixed20_12 bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nline_time.full = dfixed_const(wm->active_time + wm->blank_time);\r\nline_time.full = dfixed_div(line_time, a);\r\nbpp.full = dfixed_const(wm->bytes_per_pixel);\r\nsrc_width.full = dfixed_const(wm->src_width);\r\nbandwidth.full = dfixed_mul(src_width, bpp);\r\nbandwidth.full = dfixed_mul(bandwidth, wm->vsc);\r\nbandwidth.full = dfixed_div(bandwidth, line_time);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce6_latency_watermark(struct dce6_wm_params *wm)\r\n{\r\nu32 mc_latency = 2000;\r\nu32 available_bandwidth = dce6_available_bandwidth(wm);\r\nu32 worst_chunk_return_time = (512 * 8 * 1000) / available_bandwidth;\r\nu32 cursor_line_pair_return_time = (128 * 4 * 1000) / available_bandwidth;\r\nu32 dc_latency = 40000000 / wm->disp_clk;\r\nu32 other_heads_data_return_time = ((wm->num_heads + 1) * worst_chunk_return_time) +\r\n(wm->num_heads * cursor_line_pair_return_time);\r\nu32 latency = mc_latency + other_heads_data_return_time + dc_latency;\r\nu32 max_src_lines_per_dst_line, lb_fill_bw, line_fill_time;\r\nu32 tmp, dmif_size = 12288;\r\nfixed20_12 a, b, c;\r\nif (wm->num_heads == 0)\r\nreturn 0;\r\na.full = dfixed_const(2);\r\nb.full = dfixed_const(1);\r\nif ((wm->vsc.full > a.full) ||\r\n((wm->vsc.full > b.full) && (wm->vtaps >= 3)) ||\r\n(wm->vtaps >= 5) ||\r\n((wm->vsc.full >= a.full) && wm->interlaced))\r\nmax_src_lines_per_dst_line = 4;\r\nelse\r\nmax_src_lines_per_dst_line = 2;\r\na.full = dfixed_const(available_bandwidth);\r\nb.full = dfixed_const(wm->num_heads);\r\na.full = dfixed_div(a, b);\r\nb.full = dfixed_const(mc_latency + 512);\r\nc.full = dfixed_const(wm->disp_clk);\r\nb.full = dfixed_div(b, c);\r\nc.full = dfixed_const(dmif_size);\r\nb.full = dfixed_div(c, b);\r\ntmp = min(dfixed_trunc(a), dfixed_trunc(b));\r\nb.full = dfixed_const(1000);\r\nc.full = dfixed_const(wm->disp_clk);\r\nb.full = dfixed_div(c, b);\r\nc.full = dfixed_const(wm->bytes_per_pixel);\r\nb.full = dfixed_mul(b, c);\r\nlb_fill_bw = min(tmp, dfixed_trunc(b));\r\na.full = dfixed_const(max_src_lines_per_dst_line * wm->src_width * wm->bytes_per_pixel);\r\nb.full = dfixed_const(1000);\r\nc.full = dfixed_const(lb_fill_bw);\r\nb.full = dfixed_div(c, b);\r\na.full = dfixed_div(a, b);\r\nline_fill_time = dfixed_trunc(a);\r\nif (line_fill_time < wm->active_time)\r\nreturn latency;\r\nelse\r\nreturn latency + (line_fill_time - wm->active_time);\r\n}\r\nstatic bool dce6_average_bandwidth_vs_dram_bandwidth_for_display(struct dce6_wm_params *wm)\r\n{\r\nif (dce6_average_bandwidth(wm) <=\r\n(dce6_dram_bandwidth_for_display(wm) / wm->num_heads))\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic bool dce6_average_bandwidth_vs_available_bandwidth(struct dce6_wm_params *wm)\r\n{\r\nif (dce6_average_bandwidth(wm) <=\r\n(dce6_available_bandwidth(wm) / wm->num_heads))\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic bool dce6_check_latency_hiding(struct dce6_wm_params *wm)\r\n{\r\nu32 lb_partitions = wm->lb_size / wm->src_width;\r\nu32 line_time = wm->active_time + wm->blank_time;\r\nu32 latency_tolerant_lines;\r\nu32 latency_hiding;\r\nfixed20_12 a;\r\na.full = dfixed_const(1);\r\nif (wm->vsc.full > a.full)\r\nlatency_tolerant_lines = 1;\r\nelse {\r\nif (lb_partitions <= (wm->vtaps + 1))\r\nlatency_tolerant_lines = 1;\r\nelse\r\nlatency_tolerant_lines = 2;\r\n}\r\nlatency_hiding = (latency_tolerant_lines * line_time + wm->blank_time);\r\nif (dce6_latency_watermark(wm) <= latency_hiding)\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic void dce6_program_watermarks(struct radeon_device *rdev,\r\nstruct radeon_crtc *radeon_crtc,\r\nu32 lb_size, u32 num_heads)\r\n{\r\nstruct drm_display_mode *mode = &radeon_crtc->base.mode;\r\nstruct dce6_wm_params wm_low, wm_high;\r\nu32 dram_channels;\r\nu32 pixel_period;\r\nu32 line_time = 0;\r\nu32 latency_watermark_a = 0, latency_watermark_b = 0;\r\nu32 priority_a_mark = 0, priority_b_mark = 0;\r\nu32 priority_a_cnt = PRIORITY_OFF;\r\nu32 priority_b_cnt = PRIORITY_OFF;\r\nu32 tmp, arb_control3;\r\nfixed20_12 a, b, c;\r\nif (radeon_crtc->base.enabled && num_heads && mode) {\r\npixel_period = 1000000 / (u32)mode->clock;\r\nline_time = min((u32)mode->crtc_htotal * pixel_period, (u32)65535);\r\npriority_a_cnt = 0;\r\npriority_b_cnt = 0;\r\nif (rdev->family == CHIP_ARUBA)\r\ndram_channels = evergreen_get_number_of_dram_channels(rdev);\r\nelse\r\ndram_channels = si_get_number_of_dram_channels(rdev);\r\nif ((rdev->pm.pm_method == PM_METHOD_DPM) && rdev->pm.dpm_enabled) {\r\nwm_high.yclk =\r\nradeon_dpm_get_mclk(rdev, false) * 10;\r\nwm_high.sclk =\r\nradeon_dpm_get_sclk(rdev, false) * 10;\r\n} else {\r\nwm_high.yclk = rdev->pm.current_mclk * 10;\r\nwm_high.sclk = rdev->pm.current_sclk * 10;\r\n}\r\nwm_high.disp_clk = mode->clock;\r\nwm_high.src_width = mode->crtc_hdisplay;\r\nwm_high.active_time = mode->crtc_hdisplay * pixel_period;\r\nwm_high.blank_time = line_time - wm_high.active_time;\r\nwm_high.interlaced = false;\r\nif (mode->flags & DRM_MODE_FLAG_INTERLACE)\r\nwm_high.interlaced = true;\r\nwm_high.vsc = radeon_crtc->vsc;\r\nwm_high.vtaps = 1;\r\nif (radeon_crtc->rmx_type != RMX_OFF)\r\nwm_high.vtaps = 2;\r\nwm_high.bytes_per_pixel = 4;\r\nwm_high.lb_size = lb_size;\r\nwm_high.dram_channels = dram_channels;\r\nwm_high.num_heads = num_heads;\r\nif ((rdev->pm.pm_method == PM_METHOD_DPM) && rdev->pm.dpm_enabled) {\r\nwm_low.yclk =\r\nradeon_dpm_get_mclk(rdev, true) * 10;\r\nwm_low.sclk =\r\nradeon_dpm_get_sclk(rdev, true) * 10;\r\n} else {\r\nwm_low.yclk = rdev->pm.current_mclk * 10;\r\nwm_low.sclk = rdev->pm.current_sclk * 10;\r\n}\r\nwm_low.disp_clk = mode->clock;\r\nwm_low.src_width = mode->crtc_hdisplay;\r\nwm_low.active_time = mode->crtc_hdisplay * pixel_period;\r\nwm_low.blank_time = line_time - wm_low.active_time;\r\nwm_low.interlaced = false;\r\nif (mode->flags & DRM_MODE_FLAG_INTERLACE)\r\nwm_low.interlaced = true;\r\nwm_low.vsc = radeon_crtc->vsc;\r\nwm_low.vtaps = 1;\r\nif (radeon_crtc->rmx_type != RMX_OFF)\r\nwm_low.vtaps = 2;\r\nwm_low.bytes_per_pixel = 4;\r\nwm_low.lb_size = lb_size;\r\nwm_low.dram_channels = dram_channels;\r\nwm_low.num_heads = num_heads;\r\nlatency_watermark_a = min(dce6_latency_watermark(&wm_high), (u32)65535);\r\nlatency_watermark_b = min(dce6_latency_watermark(&wm_low), (u32)65535);\r\nif (!dce6_average_bandwidth_vs_dram_bandwidth_for_display(&wm_high) ||\r\n!dce6_average_bandwidth_vs_available_bandwidth(&wm_high) ||\r\n!dce6_check_latency_hiding(&wm_high) ||\r\n(rdev->disp_priority == 2)) {\r\nDRM_DEBUG_KMS("force priority to high\n");\r\npriority_a_cnt |= PRIORITY_ALWAYS_ON;\r\npriority_b_cnt |= PRIORITY_ALWAYS_ON;\r\n}\r\nif (!dce6_average_bandwidth_vs_dram_bandwidth_for_display(&wm_low) ||\r\n!dce6_average_bandwidth_vs_available_bandwidth(&wm_low) ||\r\n!dce6_check_latency_hiding(&wm_low) ||\r\n(rdev->disp_priority == 2)) {\r\nDRM_DEBUG_KMS("force priority to high\n");\r\npriority_a_cnt |= PRIORITY_ALWAYS_ON;\r\npriority_b_cnt |= PRIORITY_ALWAYS_ON;\r\n}\r\na.full = dfixed_const(1000);\r\nb.full = dfixed_const(mode->clock);\r\nb.full = dfixed_div(b, a);\r\nc.full = dfixed_const(latency_watermark_a);\r\nc.full = dfixed_mul(c, b);\r\nc.full = dfixed_mul(c, radeon_crtc->hsc);\r\nc.full = dfixed_div(c, a);\r\na.full = dfixed_const(16);\r\nc.full = dfixed_div(c, a);\r\npriority_a_mark = dfixed_trunc(c);\r\npriority_a_cnt |= priority_a_mark & PRIORITY_MARK_MASK;\r\na.full = dfixed_const(1000);\r\nb.full = dfixed_const(mode->clock);\r\nb.full = dfixed_div(b, a);\r\nc.full = dfixed_const(latency_watermark_b);\r\nc.full = dfixed_mul(c, b);\r\nc.full = dfixed_mul(c, radeon_crtc->hsc);\r\nc.full = dfixed_div(c, a);\r\na.full = dfixed_const(16);\r\nc.full = dfixed_div(c, a);\r\npriority_b_mark = dfixed_trunc(c);\r\npriority_b_cnt |= priority_b_mark & PRIORITY_MARK_MASK;\r\nradeon_crtc->lb_vblank_lead_lines = DIV_ROUND_UP(lb_size, mode->crtc_hdisplay);\r\n}\r\narb_control3 = RREG32(DPG_PIPE_ARBITRATION_CONTROL3 + radeon_crtc->crtc_offset);\r\ntmp = arb_control3;\r\ntmp &= ~LATENCY_WATERMARK_MASK(3);\r\ntmp |= LATENCY_WATERMARK_MASK(1);\r\nWREG32(DPG_PIPE_ARBITRATION_CONTROL3 + radeon_crtc->crtc_offset, tmp);\r\nWREG32(DPG_PIPE_LATENCY_CONTROL + radeon_crtc->crtc_offset,\r\n(LATENCY_LOW_WATERMARK(latency_watermark_a) |\r\nLATENCY_HIGH_WATERMARK(line_time)));\r\ntmp = RREG32(DPG_PIPE_ARBITRATION_CONTROL3 + radeon_crtc->crtc_offset);\r\ntmp &= ~LATENCY_WATERMARK_MASK(3);\r\ntmp |= LATENCY_WATERMARK_MASK(2);\r\nWREG32(DPG_PIPE_ARBITRATION_CONTROL3 + radeon_crtc->crtc_offset, tmp);\r\nWREG32(DPG_PIPE_LATENCY_CONTROL + radeon_crtc->crtc_offset,\r\n(LATENCY_LOW_WATERMARK(latency_watermark_b) |\r\nLATENCY_HIGH_WATERMARK(line_time)));\r\nWREG32(DPG_PIPE_ARBITRATION_CONTROL3 + radeon_crtc->crtc_offset, arb_control3);\r\nWREG32(PRIORITY_A_CNT + radeon_crtc->crtc_offset, priority_a_cnt);\r\nWREG32(PRIORITY_B_CNT + radeon_crtc->crtc_offset, priority_b_cnt);\r\nradeon_crtc->line_time = line_time;\r\nradeon_crtc->wm_high = latency_watermark_a;\r\nradeon_crtc->wm_low = latency_watermark_b;\r\n}\r\nvoid dce6_bandwidth_update(struct radeon_device *rdev)\r\n{\r\nstruct drm_display_mode *mode0 = NULL;\r\nstruct drm_display_mode *mode1 = NULL;\r\nu32 num_heads = 0, lb_size;\r\nint i;\r\nif (!rdev->mode_info.mode_config_initialized)\r\nreturn;\r\nradeon_update_display_priority(rdev);\r\nfor (i = 0; i < rdev->num_crtc; i++) {\r\nif (rdev->mode_info.crtcs[i]->base.enabled)\r\nnum_heads++;\r\n}\r\nfor (i = 0; i < rdev->num_crtc; i += 2) {\r\nmode0 = &rdev->mode_info.crtcs[i]->base.mode;\r\nmode1 = &rdev->mode_info.crtcs[i+1]->base.mode;\r\nlb_size = dce6_line_buffer_adjust(rdev, rdev->mode_info.crtcs[i], mode0, mode1);\r\ndce6_program_watermarks(rdev, rdev->mode_info.crtcs[i], lb_size, num_heads);\r\nlb_size = dce6_line_buffer_adjust(rdev, rdev->mode_info.crtcs[i+1], mode1, mode0);\r\ndce6_program_watermarks(rdev, rdev->mode_info.crtcs[i+1], lb_size, num_heads);\r\n}\r\n}\r\nstatic void si_tiling_mode_table_init(struct radeon_device *rdev)\r\n{\r\nu32 *tile = rdev->config.si.tile_mode_array;\r\nconst u32 num_tile_mode_states =\r\nARRAY_SIZE(rdev->config.si.tile_mode_array);\r\nu32 reg_offset, split_equal_to_row_size;\r\nswitch (rdev->config.si.mem_row_size_in_kb) {\r\ncase 1:\r\nsplit_equal_to_row_size = ADDR_SURF_TILE_SPLIT_1KB;\r\nbreak;\r\ncase 2:\r\ndefault:\r\nsplit_equal_to_row_size = ADDR_SURF_TILE_SPLIT_2KB;\r\nbreak;\r\ncase 4:\r\nsplit_equal_to_row_size = ADDR_SURF_TILE_SPLIT_4KB;\r\nbreak;\r\n}\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\ntile[reg_offset] = 0;\r\nswitch(rdev->family) {\r\ncase CHIP_TAHITI:\r\ncase CHIP_PITCAIRN:\r\ntile[0] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[1] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[2] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[3] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[4] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[5] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[6] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\ntile[7] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[8] = (ARRAY_MODE(ARRAY_LINEAR_ALIGNED) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[9] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[10] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[11] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[12] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\ntile[13] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[14] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\ntile[15] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\ntile[16] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\ntile[17] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\ntile[21] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[22] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[23] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[24] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[25] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_1KB) |\r\nNUM_BANKS(ADDR_SURF_8_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\nWREG32(GB_TILE_MODE0 + (reg_offset * 4), tile[reg_offset]);\r\nbreak;\r\ncase CHIP_VERDE:\r\ncase CHIP_OLAND:\r\ncase CHIP_HAINAN:\r\ntile[0] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[1] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[2] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[3] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[4] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[5] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[6] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[7] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[8] = (ARRAY_MODE(ARRAY_LINEAR_ALIGNED) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[9] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[10] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[11] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[12] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[13] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[14] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[15] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[16] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[17] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[21] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[22] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4));\r\ntile[23] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[24] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B) |\r\nNUM_BANKS(ADDR_SURF_16_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2));\r\ntile[25] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_1KB) |\r\nNUM_BANKS(ADDR_SURF_8_BANK) |\r\nBANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1));\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\nWREG32(GB_TILE_MODE0 + (reg_offset * 4), tile[reg_offset]);\r\nbreak;\r\ndefault:\r\nDRM_ERROR("unknown asic: 0x%x\n", rdev->family);\r\n}\r\n}\r\nstatic void si_select_se_sh(struct radeon_device *rdev,\r\nu32 se_num, u32 sh_num)\r\n{\r\nu32 data = INSTANCE_BROADCAST_WRITES;\r\nif ((se_num == 0xffffffff) && (sh_num == 0xffffffff))\r\ndata |= SH_BROADCAST_WRITES | SE_BROADCAST_WRITES;\r\nelse if (se_num == 0xffffffff)\r\ndata |= SE_BROADCAST_WRITES | SH_INDEX(sh_num);\r\nelse if (sh_num == 0xffffffff)\r\ndata |= SH_BROADCAST_WRITES | SE_INDEX(se_num);\r\nelse\r\ndata |= SH_INDEX(sh_num) | SE_INDEX(se_num);\r\nWREG32(GRBM_GFX_INDEX, data);\r\n}\r\nstatic u32 si_create_bitmask(u32 bit_width)\r\n{\r\nu32 i, mask = 0;\r\nfor (i = 0; i < bit_width; i++) {\r\nmask <<= 1;\r\nmask |= 1;\r\n}\r\nreturn mask;\r\n}\r\nstatic u32 si_get_cu_enabled(struct radeon_device *rdev, u32 cu_per_sh)\r\n{\r\nu32 data, mask;\r\ndata = RREG32(CC_GC_SHADER_ARRAY_CONFIG);\r\nif (data & 1)\r\ndata &= INACTIVE_CUS_MASK;\r\nelse\r\ndata = 0;\r\ndata |= RREG32(GC_USER_SHADER_ARRAY_CONFIG);\r\ndata >>= INACTIVE_CUS_SHIFT;\r\nmask = si_create_bitmask(cu_per_sh);\r\nreturn ~data & mask;\r\n}\r\nstatic void si_setup_spi(struct radeon_device *rdev,\r\nu32 se_num, u32 sh_per_se,\r\nu32 cu_per_sh)\r\n{\r\nint i, j, k;\r\nu32 data, mask, active_cu;\r\nfor (i = 0; i < se_num; i++) {\r\nfor (j = 0; j < sh_per_se; j++) {\r\nsi_select_se_sh(rdev, i, j);\r\ndata = RREG32(SPI_STATIC_THREAD_MGMT_3);\r\nactive_cu = si_get_cu_enabled(rdev, cu_per_sh);\r\nmask = 1;\r\nfor (k = 0; k < 16; k++) {\r\nmask <<= k;\r\nif (active_cu & mask) {\r\ndata &= ~mask;\r\nWREG32(SPI_STATIC_THREAD_MGMT_3, data);\r\nbreak;\r\n}\r\n}\r\n}\r\n}\r\nsi_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\n}\r\nstatic u32 si_get_rb_disabled(struct radeon_device *rdev,\r\nu32 max_rb_num_per_se,\r\nu32 sh_per_se)\r\n{\r\nu32 data, mask;\r\ndata = RREG32(CC_RB_BACKEND_DISABLE);\r\nif (data & 1)\r\ndata &= BACKEND_DISABLE_MASK;\r\nelse\r\ndata = 0;\r\ndata |= RREG32(GC_USER_RB_BACKEND_DISABLE);\r\ndata >>= BACKEND_DISABLE_SHIFT;\r\nmask = si_create_bitmask(max_rb_num_per_se / sh_per_se);\r\nreturn data & mask;\r\n}\r\nstatic void si_setup_rb(struct radeon_device *rdev,\r\nu32 se_num, u32 sh_per_se,\r\nu32 max_rb_num_per_se)\r\n{\r\nint i, j;\r\nu32 data, mask;\r\nu32 disabled_rbs = 0;\r\nu32 enabled_rbs = 0;\r\nfor (i = 0; i < se_num; i++) {\r\nfor (j = 0; j < sh_per_se; j++) {\r\nsi_select_se_sh(rdev, i, j);\r\ndata = si_get_rb_disabled(rdev, max_rb_num_per_se, sh_per_se);\r\ndisabled_rbs |= data << ((i * sh_per_se + j) * TAHITI_RB_BITMAP_WIDTH_PER_SH);\r\n}\r\n}\r\nsi_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nmask = 1;\r\nfor (i = 0; i < max_rb_num_per_se * se_num; i++) {\r\nif (!(disabled_rbs & mask))\r\nenabled_rbs |= mask;\r\nmask <<= 1;\r\n}\r\nrdev->config.si.backend_enable_mask = enabled_rbs;\r\nfor (i = 0; i < se_num; i++) {\r\nsi_select_se_sh(rdev, i, 0xffffffff);\r\ndata = 0;\r\nfor (j = 0; j < sh_per_se; j++) {\r\nswitch (enabled_rbs & 3) {\r\ncase 1:\r\ndata |= (RASTER_CONFIG_RB_MAP_0 << (i * sh_per_se + j) * 2);\r\nbreak;\r\ncase 2:\r\ndata |= (RASTER_CONFIG_RB_MAP_3 << (i * sh_per_se + j) * 2);\r\nbreak;\r\ncase 3:\r\ndefault:\r\ndata |= (RASTER_CONFIG_RB_MAP_2 << (i * sh_per_se + j) * 2);\r\nbreak;\r\n}\r\nenabled_rbs >>= 2;\r\n}\r\nWREG32(PA_SC_RASTER_CONFIG, data);\r\n}\r\nsi_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\n}\r\nstatic void si_gpu_init(struct radeon_device *rdev)\r\n{\r\nu32 gb_addr_config = 0;\r\nu32 mc_shared_chmap, mc_arb_ramcfg;\r\nu32 sx_debug_1;\r\nu32 hdp_host_path_cntl;\r\nu32 tmp;\r\nint i, j;\r\nswitch (rdev->family) {\r\ncase CHIP_TAHITI:\r\nrdev->config.si.max_shader_engines = 2;\r\nrdev->config.si.max_tile_pipes = 12;\r\nrdev->config.si.max_cu_per_sh = 8;\r\nrdev->config.si.max_sh_per_se = 2;\r\nrdev->config.si.max_backends_per_se = 4;\r\nrdev->config.si.max_texture_channel_caches = 12;\r\nrdev->config.si.max_gprs = 256;\r\nrdev->config.si.max_gs_threads = 32;\r\nrdev->config.si.max_hw_contexts = 8;\r\nrdev->config.si.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.si.sc_prim_fifo_size_backend = 0x100;\r\nrdev->config.si.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.si.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = TAHITI_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_PITCAIRN:\r\nrdev->config.si.max_shader_engines = 2;\r\nrdev->config.si.max_tile_pipes = 8;\r\nrdev->config.si.max_cu_per_sh = 5;\r\nrdev->config.si.max_sh_per_se = 2;\r\nrdev->config.si.max_backends_per_se = 4;\r\nrdev->config.si.max_texture_channel_caches = 8;\r\nrdev->config.si.max_gprs = 256;\r\nrdev->config.si.max_gs_threads = 32;\r\nrdev->config.si.max_hw_contexts = 8;\r\nrdev->config.si.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.si.sc_prim_fifo_size_backend = 0x100;\r\nrdev->config.si.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.si.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = TAHITI_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_VERDE:\r\ndefault:\r\nrdev->config.si.max_shader_engines = 1;\r\nrdev->config.si.max_tile_pipes = 4;\r\nrdev->config.si.max_cu_per_sh = 5;\r\nrdev->config.si.max_sh_per_se = 2;\r\nrdev->config.si.max_backends_per_se = 4;\r\nrdev->config.si.max_texture_channel_caches = 4;\r\nrdev->config.si.max_gprs = 256;\r\nrdev->config.si.max_gs_threads = 32;\r\nrdev->config.si.max_hw_contexts = 8;\r\nrdev->config.si.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.si.sc_prim_fifo_size_backend = 0x40;\r\nrdev->config.si.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.si.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = VERDE_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_OLAND:\r\nrdev->config.si.max_shader_engines = 1;\r\nrdev->config.si.max_tile_pipes = 4;\r\nrdev->config.si.max_cu_per_sh = 6;\r\nrdev->config.si.max_sh_per_se = 1;\r\nrdev->config.si.max_backends_per_se = 2;\r\nrdev->config.si.max_texture_channel_caches = 4;\r\nrdev->config.si.max_gprs = 256;\r\nrdev->config.si.max_gs_threads = 16;\r\nrdev->config.si.max_hw_contexts = 8;\r\nrdev->config.si.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.si.sc_prim_fifo_size_backend = 0x40;\r\nrdev->config.si.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.si.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = VERDE_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_HAINAN:\r\nrdev->config.si.max_shader_engines = 1;\r\nrdev->config.si.max_tile_pipes = 4;\r\nrdev->config.si.max_cu_per_sh = 5;\r\nrdev->config.si.max_sh_per_se = 1;\r\nrdev->config.si.max_backends_per_se = 1;\r\nrdev->config.si.max_texture_channel_caches = 2;\r\nrdev->config.si.max_gprs = 256;\r\nrdev->config.si.max_gs_threads = 16;\r\nrdev->config.si.max_hw_contexts = 8;\r\nrdev->config.si.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.si.sc_prim_fifo_size_backend = 0x40;\r\nrdev->config.si.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.si.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = HAINAN_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\n}\r\nfor (i = 0, j = 0; i < 32; i++, j += 0x18) {\r\nWREG32((0x2c14 + j), 0x00000000);\r\nWREG32((0x2c18 + j), 0x00000000);\r\nWREG32((0x2c1c + j), 0x00000000);\r\nWREG32((0x2c20 + j), 0x00000000);\r\nWREG32((0x2c24 + j), 0x00000000);\r\n}\r\nWREG32(GRBM_CNTL, GRBM_READ_TIMEOUT(0xff));\r\nWREG32(SRBM_INT_CNTL, 1);\r\nWREG32(SRBM_INT_ACK, 1);\r\nevergreen_fix_pci_max_read_req_size(rdev);\r\nWREG32(BIF_FB_EN, FB_READ_EN | FB_WRITE_EN);\r\nmc_shared_chmap = RREG32(MC_SHARED_CHMAP);\r\nmc_arb_ramcfg = RREG32(MC_ARB_RAMCFG);\r\nrdev->config.si.num_tile_pipes = rdev->config.si.max_tile_pipes;\r\nrdev->config.si.mem_max_burst_length_bytes = 256;\r\ntmp = (mc_arb_ramcfg & NOOFCOLS_MASK) >> NOOFCOLS_SHIFT;\r\nrdev->config.si.mem_row_size_in_kb = (4 * (1 << (8 + tmp))) / 1024;\r\nif (rdev->config.si.mem_row_size_in_kb > 4)\r\nrdev->config.si.mem_row_size_in_kb = 4;\r\nrdev->config.si.shader_engine_tile_size = 32;\r\nrdev->config.si.num_gpus = 1;\r\nrdev->config.si.multi_gpu_tile_size = 64;\r\ngb_addr_config &= ~ROW_SIZE_MASK;\r\nswitch (rdev->config.si.mem_row_size_in_kb) {\r\ncase 1:\r\ndefault:\r\ngb_addr_config |= ROW_SIZE(0);\r\nbreak;\r\ncase 2:\r\ngb_addr_config |= ROW_SIZE(1);\r\nbreak;\r\ncase 4:\r\ngb_addr_config |= ROW_SIZE(2);\r\nbreak;\r\n}\r\nrdev->config.si.tile_config = 0;\r\nswitch (rdev->config.si.num_tile_pipes) {\r\ncase 1:\r\nrdev->config.si.tile_config |= (0 << 0);\r\nbreak;\r\ncase 2:\r\nrdev->config.si.tile_config |= (1 << 0);\r\nbreak;\r\ncase 4:\r\nrdev->config.si.tile_config |= (2 << 0);\r\nbreak;\r\ncase 8:\r\ndefault:\r\nrdev->config.si.tile_config |= (3 << 0);\r\nbreak;\r\n}\r\nswitch ((mc_arb_ramcfg & NOOFBANK_MASK) >> NOOFBANK_SHIFT) {\r\ncase 0:\r\nrdev->config.si.tile_config |= 0 << 4;\r\nbreak;\r\ncase 1:\r\nrdev->config.si.tile_config |= 1 << 4;\r\nbreak;\r\ncase 2:\r\ndefault:\r\nrdev->config.si.tile_config |= 2 << 4;\r\nbreak;\r\n}\r\nrdev->config.si.tile_config |=\r\n((gb_addr_config & PIPE_INTERLEAVE_SIZE_MASK) >> PIPE_INTERLEAVE_SIZE_SHIFT) << 8;\r\nrdev->config.si.tile_config |=\r\n((gb_addr_config & ROW_SIZE_MASK) >> ROW_SIZE_SHIFT) << 12;\r\nWREG32(GB_ADDR_CONFIG, gb_addr_config);\r\nWREG32(DMIF_ADDR_CONFIG, gb_addr_config);\r\nWREG32(DMIF_ADDR_CALC, gb_addr_config);\r\nWREG32(HDP_ADDR_CONFIG, gb_addr_config);\r\nWREG32(DMA_TILING_CONFIG + DMA0_REGISTER_OFFSET, gb_addr_config);\r\nWREG32(DMA_TILING_CONFIG + DMA1_REGISTER_OFFSET, gb_addr_config);\r\nif (rdev->has_uvd) {\r\nWREG32(UVD_UDEC_ADDR_CONFIG, gb_addr_config);\r\nWREG32(UVD_UDEC_DB_ADDR_CONFIG, gb_addr_config);\r\nWREG32(UVD_UDEC_DBW_ADDR_CONFIG, gb_addr_config);\r\n}\r\nsi_tiling_mode_table_init(rdev);\r\nsi_setup_rb(rdev, rdev->config.si.max_shader_engines,\r\nrdev->config.si.max_sh_per_se,\r\nrdev->config.si.max_backends_per_se);\r\nsi_setup_spi(rdev, rdev->config.si.max_shader_engines,\r\nrdev->config.si.max_sh_per_se,\r\nrdev->config.si.max_cu_per_sh);\r\nrdev->config.si.active_cus = 0;\r\nfor (i = 0; i < rdev->config.si.max_shader_engines; i++) {\r\nfor (j = 0; j < rdev->config.si.max_sh_per_se; j++) {\r\nrdev->config.si.active_cus +=\r\nhweight32(si_get_cu_active_bitmap(rdev, i, j));\r\n}\r\n}\r\nWREG32(CP_QUEUE_THRESHOLDS, (ROQ_IB1_START(0x16) |\r\nROQ_IB2_START(0x2b)));\r\nWREG32(CP_MEQ_THRESHOLDS, MEQ1_START(0x30) | MEQ2_START(0x60));\r\nsx_debug_1 = RREG32(SX_DEBUG_1);\r\nWREG32(SX_DEBUG_1, sx_debug_1);\r\nWREG32(SPI_CONFIG_CNTL_1, VTX_DONE_DELAY(4));\r\nWREG32(PA_SC_FIFO_SIZE, (SC_FRONTEND_PRIM_FIFO_SIZE(rdev->config.si.sc_prim_fifo_size_frontend) |\r\nSC_BACKEND_PRIM_FIFO_SIZE(rdev->config.si.sc_prim_fifo_size_backend) |\r\nSC_HIZ_TILE_FIFO_SIZE(rdev->config.si.sc_hiz_tile_fifo_size) |\r\nSC_EARLYZ_TILE_FIFO_SIZE(rdev->config.si.sc_earlyz_tile_fifo_size)));\r\nWREG32(VGT_NUM_INSTANCES, 1);\r\nWREG32(CP_PERFMON_CNTL, 0);\r\nWREG32(SQ_CONFIG, 0);\r\nWREG32(PA_SC_FORCE_EOV_MAX_CNTS, (FORCE_EOV_MAX_CLK_CNT(4095) |\r\nFORCE_EOV_MAX_REZ_CNT(255)));\r\nWREG32(VGT_CACHE_INVALIDATION, CACHE_INVALIDATION(VC_AND_TC) |\r\nAUTO_INVLD_EN(ES_AND_GS_AUTO));\r\nWREG32(VGT_GS_VERTEX_REUSE, 16);\r\nWREG32(PA_SC_LINE_STIPPLE_STATE, 0);\r\nWREG32(CB_PERFCOUNTER0_SELECT0, 0);\r\nWREG32(CB_PERFCOUNTER0_SELECT1, 0);\r\nWREG32(CB_PERFCOUNTER1_SELECT0, 0);\r\nWREG32(CB_PERFCOUNTER1_SELECT1, 0);\r\nWREG32(CB_PERFCOUNTER2_SELECT0, 0);\r\nWREG32(CB_PERFCOUNTER2_SELECT1, 0);\r\nWREG32(CB_PERFCOUNTER3_SELECT0, 0);\r\nWREG32(CB_PERFCOUNTER3_SELECT1, 0);\r\ntmp = RREG32(HDP_MISC_CNTL);\r\ntmp |= HDP_FLUSH_INVALIDATE_CACHE;\r\nWREG32(HDP_MISC_CNTL, tmp);\r\nhdp_host_path_cntl = RREG32(HDP_HOST_PATH_CNTL);\r\nWREG32(HDP_HOST_PATH_CNTL, hdp_host_path_cntl);\r\nWREG32(PA_CL_ENHANCE, CLIP_VTX_REORDER_ENA | NUM_CLIP_SEQ(3));\r\nudelay(50);\r\n}\r\nstatic void si_scratch_init(struct radeon_device *rdev)\r\n{\r\nint i;\r\nrdev->scratch.num_reg = 7;\r\nrdev->scratch.reg_base = SCRATCH_REG0;\r\nfor (i = 0; i < rdev->scratch.num_reg; i++) {\r\nrdev->scratch.free[i] = true;\r\nrdev->scratch.reg[i] = rdev->scratch.reg_base + (i * 4);\r\n}\r\n}\r\nvoid si_fence_ring_emit(struct radeon_device *rdev,\r\nstruct radeon_fence *fence)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[fence->ring];\r\nu64 addr = rdev->fence_drv[fence->ring].gpu_addr;\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_CONFIG_REG, 1));\r\nradeon_ring_write(ring, (CP_COHER_CNTL2 - PACKET3_SET_CONFIG_REG_START) >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, PACKET3(PACKET3_SURFACE_SYNC, 3));\r\nradeon_ring_write(ring, PACKET3_TCL1_ACTION_ENA |\r\nPACKET3_TC_ACTION_ENA |\r\nPACKET3_SH_KCACHE_ACTION_ENA |\r\nPACKET3_SH_ICACHE_ACTION_ENA);\r\nradeon_ring_write(ring, 0xFFFFFFFF);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 10);\r\nradeon_ring_write(ring, PACKET3(PACKET3_EVENT_WRITE_EOP, 4));\r\nradeon_ring_write(ring, EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) | EVENT_INDEX(5));\r\nradeon_ring_write(ring, lower_32_bits(addr));\r\nradeon_ring_write(ring, (upper_32_bits(addr) & 0xff) | DATA_SEL(1) | INT_SEL(2));\r\nradeon_ring_write(ring, fence->seq);\r\nradeon_ring_write(ring, 0);\r\n}\r\nvoid si_ring_ib_execute(struct radeon_device *rdev, struct radeon_ib *ib)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[ib->ring];\r\nunsigned vm_id = ib->vm ? ib->vm->ids[ib->ring].id : 0;\r\nu32 header;\r\nif (ib->is_const_ib) {\r\nradeon_ring_write(ring, PACKET3(PACKET3_SWITCH_BUFFER, 0));\r\nradeon_ring_write(ring, 0);\r\nheader = PACKET3(PACKET3_INDIRECT_BUFFER_CONST, 2);\r\n} else {\r\nu32 next_rptr;\r\nif (ring->rptr_save_reg) {\r\nnext_rptr = ring->wptr + 3 + 4 + 8;\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_CONFIG_REG, 1));\r\nradeon_ring_write(ring, ((ring->rptr_save_reg -\r\nPACKET3_SET_CONFIG_REG_START) >> 2));\r\nradeon_ring_write(ring, next_rptr);\r\n} else if (rdev->wb.enabled) {\r\nnext_rptr = ring->wptr + 5 + 4 + 8;\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (1 << 8));\r\nradeon_ring_write(ring, ring->next_rptr_gpu_addr & 0xfffffffc);\r\nradeon_ring_write(ring, upper_32_bits(ring->next_rptr_gpu_addr));\r\nradeon_ring_write(ring, next_rptr);\r\n}\r\nheader = PACKET3(PACKET3_INDIRECT_BUFFER, 2);\r\n}\r\nradeon_ring_write(ring, header);\r\nradeon_ring_write(ring,\r\n#ifdef __BIG_ENDIAN\r\n(2 << 0) |\r\n#endif\r\n(ib->gpu_addr & 0xFFFFFFFC));\r\nradeon_ring_write(ring, upper_32_bits(ib->gpu_addr) & 0xFFFF);\r\nradeon_ring_write(ring, ib->length_dw | (vm_id << 24));\r\nif (!ib->is_const_ib) {\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_CONFIG_REG, 1));\r\nradeon_ring_write(ring, (CP_COHER_CNTL2 - PACKET3_SET_CONFIG_REG_START) >> 2);\r\nradeon_ring_write(ring, vm_id);\r\nradeon_ring_write(ring, PACKET3(PACKET3_SURFACE_SYNC, 3));\r\nradeon_ring_write(ring, PACKET3_TCL1_ACTION_ENA |\r\nPACKET3_TC_ACTION_ENA |\r\nPACKET3_SH_KCACHE_ACTION_ENA |\r\nPACKET3_SH_ICACHE_ACTION_ENA);\r\nradeon_ring_write(ring, 0xFFFFFFFF);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 10);\r\n}\r\n}\r\nstatic void si_cp_enable(struct radeon_device *rdev, bool enable)\r\n{\r\nif (enable)\r\nWREG32(CP_ME_CNTL, 0);\r\nelse {\r\nif (rdev->asic->copy.copy_ring_index == RADEON_RING_TYPE_GFX_INDEX)\r\nradeon_ttm_set_active_vram_size(rdev, rdev->mc.visible_vram_size);\r\nWREG32(CP_ME_CNTL, (CP_ME_HALT | CP_PFP_HALT | CP_CE_HALT));\r\nWREG32(SCRATCH_UMSK, 0);\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP1_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP2_INDEX].ready = false;\r\n}\r\nudelay(50);\r\n}\r\nstatic int si_cp_load_microcode(struct radeon_device *rdev)\r\n{\r\nint i;\r\nif (!rdev->me_fw || !rdev->pfp_fw || !rdev->ce_fw)\r\nreturn -EINVAL;\r\nsi_cp_enable(rdev, false);\r\nif (rdev->new_fw) {\r\nconst struct gfx_firmware_header_v1_0 *pfp_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->pfp_fw->data;\r\nconst struct gfx_firmware_header_v1_0 *ce_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->ce_fw->data;\r\nconst struct gfx_firmware_header_v1_0 *me_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->me_fw->data;\r\nconst __le32 *fw_data;\r\nu32 fw_size;\r\nradeon_ucode_print_gfx_hdr(&pfp_hdr->header);\r\nradeon_ucode_print_gfx_hdr(&ce_hdr->header);\r\nradeon_ucode_print_gfx_hdr(&me_hdr->header);\r\nfw_data = (const __le32 *)\r\n(rdev->pfp_fw->data + le32_to_cpu(pfp_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(pfp_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_PFP_UCODE_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfw_data = (const __le32 *)\r\n(rdev->ce_fw->data + le32_to_cpu(ce_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(ce_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_CE_UCODE_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nfw_data = (const __be32 *)\r\n(rdev->me_fw->data + le32_to_cpu(me_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(me_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_ME_RAM_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\n} else {\r\nconst __be32 *fw_data;\r\nfw_data = (const __be32 *)rdev->pfp_fw->data;\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfor (i = 0; i < SI_PFP_UCODE_SIZE; i++)\r\nWREG32(CP_PFP_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfw_data = (const __be32 *)rdev->ce_fw->data;\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nfor (i = 0; i < SI_CE_UCODE_SIZE; i++)\r\nWREG32(CP_CE_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nfw_data = (const __be32 *)rdev->me_fw->data;\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\nfor (i = 0; i < SI_PM4_UCODE_SIZE; i++)\r\nWREG32(CP_ME_RAM_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\n}\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\nWREG32(CP_ME_RAM_RADDR, 0);\r\nreturn 0;\r\n}\r\nstatic int si_cp_start(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nint r, i;\r\nr = radeon_ring_lock(rdev, ring, 7 + 4);\r\nif (r) {\r\nDRM_ERROR("radeon: cp failed to lock ring (%d).\n", r);\r\nreturn r;\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_ME_INITIALIZE, 5));\r\nradeon_ring_write(ring, 0x1);\r\nradeon_ring_write(ring, 0x0);\r\nradeon_ring_write(ring, rdev->config.si.max_hw_contexts - 1);\r\nradeon_ring_write(ring, PACKET3_ME_INITIALIZE_DEVICE_ID(1));\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_BASE, 2));\r\nradeon_ring_write(ring, PACKET3_BASE_INDEX(CE_PARTITION_BASE));\r\nradeon_ring_write(ring, 0xc000);\r\nradeon_ring_write(ring, 0xe000);\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\nsi_cp_enable(rdev, true);\r\nr = radeon_ring_lock(rdev, ring, si_default_size + 10);\r\nif (r) {\r\nDRM_ERROR("radeon: cp failed to lock ring (%d).\n", r);\r\nreturn r;\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nradeon_ring_write(ring, PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);\r\nfor (i = 0; i < si_default_size; i++)\r\nradeon_ring_write(ring, si_default_state[i]);\r\nradeon_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nradeon_ring_write(ring, PACKET3_PREAMBLE_END_CLEAR_STATE);\r\nradeon_ring_write(ring, PACKET3(PACKET3_CLEAR_STATE, 0));\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_CONTEXT_REG, 2));\r\nradeon_ring_write(ring, 0x00000316);\r\nradeon_ring_write(ring, 0x0000000e);\r\nradeon_ring_write(ring, 0x00000010);\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\nfor (i = RADEON_RING_TYPE_GFX_INDEX; i <= CAYMAN_RING_TYPE_CP2_INDEX; ++i) {\r\nring = &rdev->ring[i];\r\nr = radeon_ring_lock(rdev, ring, 2);\r\nradeon_ring_write(ring, PACKET3_COMPUTE(PACKET3_CLEAR_STATE, 0));\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\n}\r\nreturn 0;\r\n}\r\nstatic void si_cp_fini(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nsi_cp_enable(rdev, false);\r\nring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nradeon_ring_fini(rdev, ring);\r\nradeon_scratch_free(rdev, ring->rptr_save_reg);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nradeon_ring_fini(rdev, ring);\r\nradeon_scratch_free(rdev, ring->rptr_save_reg);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nradeon_ring_fini(rdev, ring);\r\nradeon_scratch_free(rdev, ring->rptr_save_reg);\r\n}\r\nstatic int si_cp_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nu32 tmp;\r\nu32 rb_bufsz;\r\nint r;\r\nsi_enable_gui_idle_interrupt(rdev, false);\r\nWREG32(CP_SEM_WAIT_TIMER, 0x0);\r\nWREG32(CP_SEM_INCOMPLETE_TIMER_CNTL, 0x0);\r\nWREG32(CP_RB_WPTR_DELAY, 0);\r\nWREG32(CP_DEBUG, 0);\r\nWREG32(SCRATCH_ADDR, ((rdev->wb.gpu_addr + RADEON_WB_SCRATCH_OFFSET) >> 8) & 0xFFFFFFFF);\r\nring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nrb_bufsz = order_base_2(ring->ring_size / 8);\r\ntmp = (order_base_2(RADEON_GPU_PAGE_SIZE/8) << 8) | rb_bufsz;\r\n#ifdef __BIG_ENDIAN\r\ntmp |= BUF_SWAP_32BIT;\r\n#endif\r\nWREG32(CP_RB0_CNTL, tmp);\r\nWREG32(CP_RB0_CNTL, tmp | RB_RPTR_WR_ENA);\r\nring->wptr = 0;\r\nWREG32(CP_RB0_WPTR, ring->wptr);\r\nWREG32(CP_RB0_RPTR_ADDR, (rdev->wb.gpu_addr + RADEON_WB_CP_RPTR_OFFSET) & 0xFFFFFFFC);\r\nWREG32(CP_RB0_RPTR_ADDR_HI, upper_32_bits(rdev->wb.gpu_addr + RADEON_WB_CP_RPTR_OFFSET) & 0xFF);\r\nif (rdev->wb.enabled)\r\nWREG32(SCRATCH_UMSK, 0xff);\r\nelse {\r\ntmp |= RB_NO_UPDATE;\r\nWREG32(SCRATCH_UMSK, 0);\r\n}\r\nmdelay(1);\r\nWREG32(CP_RB0_CNTL, tmp);\r\nWREG32(CP_RB0_BASE, ring->gpu_addr >> 8);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nrb_bufsz = order_base_2(ring->ring_size / 8);\r\ntmp = (order_base_2(RADEON_GPU_PAGE_SIZE/8) << 8) | rb_bufsz;\r\n#ifdef __BIG_ENDIAN\r\ntmp |= BUF_SWAP_32BIT;\r\n#endif\r\nWREG32(CP_RB1_CNTL, tmp);\r\nWREG32(CP_RB1_CNTL, tmp | RB_RPTR_WR_ENA);\r\nring->wptr = 0;\r\nWREG32(CP_RB1_WPTR, ring->wptr);\r\nWREG32(CP_RB1_RPTR_ADDR, (rdev->wb.gpu_addr + RADEON_WB_CP1_RPTR_OFFSET) & 0xFFFFFFFC);\r\nWREG32(CP_RB1_RPTR_ADDR_HI, upper_32_bits(rdev->wb.gpu_addr + RADEON_WB_CP1_RPTR_OFFSET) & 0xFF);\r\nmdelay(1);\r\nWREG32(CP_RB1_CNTL, tmp);\r\nWREG32(CP_RB1_BASE, ring->gpu_addr >> 8);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nrb_bufsz = order_base_2(ring->ring_size / 8);\r\ntmp = (order_base_2(RADEON_GPU_PAGE_SIZE/8) << 8) | rb_bufsz;\r\n#ifdef __BIG_ENDIAN\r\ntmp |= BUF_SWAP_32BIT;\r\n#endif\r\nWREG32(CP_RB2_CNTL, tmp);\r\nWREG32(CP_RB2_CNTL, tmp | RB_RPTR_WR_ENA);\r\nring->wptr = 0;\r\nWREG32(CP_RB2_WPTR, ring->wptr);\r\nWREG32(CP_RB2_RPTR_ADDR, (rdev->wb.gpu_addr + RADEON_WB_CP2_RPTR_OFFSET) & 0xFFFFFFFC);\r\nWREG32(CP_RB2_RPTR_ADDR_HI, upper_32_bits(rdev->wb.gpu_addr + RADEON_WB_CP2_RPTR_OFFSET) & 0xFF);\r\nmdelay(1);\r\nWREG32(CP_RB2_CNTL, tmp);\r\nWREG32(CP_RB2_BASE, ring->gpu_addr >> 8);\r\nsi_cp_start(rdev);\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = true;\r\nrdev->ring[CAYMAN_RING_TYPE_CP1_INDEX].ready = true;\r\nrdev->ring[CAYMAN_RING_TYPE_CP2_INDEX].ready = true;\r\nr = radeon_ring_test(rdev, RADEON_RING_TYPE_GFX_INDEX, &rdev->ring[RADEON_RING_TYPE_GFX_INDEX]);\r\nif (r) {\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP1_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP2_INDEX].ready = false;\r\nreturn r;\r\n}\r\nr = radeon_ring_test(rdev, CAYMAN_RING_TYPE_CP1_INDEX, &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX]);\r\nif (r) {\r\nrdev->ring[CAYMAN_RING_TYPE_CP1_INDEX].ready = false;\r\n}\r\nr = radeon_ring_test(rdev, CAYMAN_RING_TYPE_CP2_INDEX, &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX]);\r\nif (r) {\r\nrdev->ring[CAYMAN_RING_TYPE_CP2_INDEX].ready = false;\r\n}\r\nsi_enable_gui_idle_interrupt(rdev, true);\r\nif (rdev->asic->copy.copy_ring_index == RADEON_RING_TYPE_GFX_INDEX)\r\nradeon_ttm_set_active_vram_size(rdev, rdev->mc.real_vram_size);\r\nreturn 0;\r\n}\r\nu32 si_gpu_check_soft_reset(struct radeon_device *rdev)\r\n{\r\nu32 reset_mask = 0;\r\nu32 tmp;\r\ntmp = RREG32(GRBM_STATUS);\r\nif (tmp & (PA_BUSY | SC_BUSY |\r\nBCI_BUSY | SX_BUSY |\r\nTA_BUSY | VGT_BUSY |\r\nDB_BUSY | CB_BUSY |\r\nGDS_BUSY | SPI_BUSY |\r\nIA_BUSY | IA_BUSY_NO_DMA))\r\nreset_mask |= RADEON_RESET_GFX;\r\nif (tmp & (CF_RQ_PENDING | PF_RQ_PENDING |\r\nCP_BUSY | CP_COHERENCY_BUSY))\r\nreset_mask |= RADEON_RESET_CP;\r\nif (tmp & GRBM_EE_BUSY)\r\nreset_mask |= RADEON_RESET_GRBM | RADEON_RESET_GFX | RADEON_RESET_CP;\r\ntmp = RREG32(GRBM_STATUS2);\r\nif (tmp & (RLC_RQ_PENDING | RLC_BUSY))\r\nreset_mask |= RADEON_RESET_RLC;\r\ntmp = RREG32(DMA_STATUS_REG + DMA0_REGISTER_OFFSET);\r\nif (!(tmp & DMA_IDLE))\r\nreset_mask |= RADEON_RESET_DMA;\r\ntmp = RREG32(DMA_STATUS_REG + DMA1_REGISTER_OFFSET);\r\nif (!(tmp & DMA_IDLE))\r\nreset_mask |= RADEON_RESET_DMA1;\r\ntmp = RREG32(SRBM_STATUS2);\r\nif (tmp & DMA_BUSY)\r\nreset_mask |= RADEON_RESET_DMA;\r\nif (tmp & DMA1_BUSY)\r\nreset_mask |= RADEON_RESET_DMA1;\r\ntmp = RREG32(SRBM_STATUS);\r\nif (tmp & IH_BUSY)\r\nreset_mask |= RADEON_RESET_IH;\r\nif (tmp & SEM_BUSY)\r\nreset_mask |= RADEON_RESET_SEM;\r\nif (tmp & GRBM_RQ_PENDING)\r\nreset_mask |= RADEON_RESET_GRBM;\r\nif (tmp & VMC_BUSY)\r\nreset_mask |= RADEON_RESET_VMC;\r\nif (tmp & (MCB_BUSY | MCB_NON_DISPLAY_BUSY |\r\nMCC_BUSY | MCD_BUSY))\r\nreset_mask |= RADEON_RESET_MC;\r\nif (evergreen_is_display_hung(rdev))\r\nreset_mask |= RADEON_RESET_DISPLAY;\r\ntmp = RREG32(VM_L2_STATUS);\r\nif (tmp & L2_BUSY)\r\nreset_mask |= RADEON_RESET_VMC;\r\nif (reset_mask & RADEON_RESET_MC) {\r\nDRM_DEBUG("MC busy: 0x%08X, clearing.\n", reset_mask);\r\nreset_mask &= ~RADEON_RESET_MC;\r\n}\r\nreturn reset_mask;\r\n}\r\nstatic void si_gpu_soft_reset(struct radeon_device *rdev, u32 reset_mask)\r\n{\r\nstruct evergreen_mc_save save;\r\nu32 grbm_soft_reset = 0, srbm_soft_reset = 0;\r\nu32 tmp;\r\nif (reset_mask == 0)\r\nreturn;\r\ndev_info(rdev->dev, "GPU softreset: 0x%08X\n", reset_mask);\r\nevergreen_print_gpu_status_regs(rdev);\r\ndev_info(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_ADDR 0x%08X\n",\r\nRREG32(VM_CONTEXT1_PROTECTION_FAULT_ADDR));\r\ndev_info(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",\r\nRREG32(VM_CONTEXT1_PROTECTION_FAULT_STATUS));\r\nsi_fini_pg(rdev);\r\nsi_fini_cg(rdev);\r\nsi_rlc_stop(rdev);\r\nWREG32(CP_ME_CNTL, CP_ME_HALT | CP_PFP_HALT | CP_CE_HALT);\r\nif (reset_mask & RADEON_RESET_DMA) {\r\ntmp = RREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET);\r\ntmp &= ~DMA_RB_ENABLE;\r\nWREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET, tmp);\r\n}\r\nif (reset_mask & RADEON_RESET_DMA1) {\r\ntmp = RREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET);\r\ntmp &= ~DMA_RB_ENABLE;\r\nWREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET, tmp);\r\n}\r\nudelay(50);\r\nevergreen_mc_stop(rdev, &save);\r\nif (evergreen_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nif (reset_mask & (RADEON_RESET_GFX | RADEON_RESET_COMPUTE | RADEON_RESET_CP)) {\r\ngrbm_soft_reset = SOFT_RESET_CB |\r\nSOFT_RESET_DB |\r\nSOFT_RESET_GDS |\r\nSOFT_RESET_PA |\r\nSOFT_RESET_SC |\r\nSOFT_RESET_BCI |\r\nSOFT_RESET_SPI |\r\nSOFT_RESET_SX |\r\nSOFT_RESET_TC |\r\nSOFT_RESET_TA |\r\nSOFT_RESET_VGT |\r\nSOFT_RESET_IA;\r\n}\r\nif (reset_mask & RADEON_RESET_CP) {\r\ngrbm_soft_reset |= SOFT_RESET_CP | SOFT_RESET_VGT;\r\nsrbm_soft_reset |= SOFT_RESET_GRBM;\r\n}\r\nif (reset_mask & RADEON_RESET_DMA)\r\nsrbm_soft_reset |= SOFT_RESET_DMA;\r\nif (reset_mask & RADEON_RESET_DMA1)\r\nsrbm_soft_reset |= SOFT_RESET_DMA1;\r\nif (reset_mask & RADEON_RESET_DISPLAY)\r\nsrbm_soft_reset |= SOFT_RESET_DC;\r\nif (reset_mask & RADEON_RESET_RLC)\r\ngrbm_soft_reset |= SOFT_RESET_RLC;\r\nif (reset_mask & RADEON_RESET_SEM)\r\nsrbm_soft_reset |= SOFT_RESET_SEM;\r\nif (reset_mask & RADEON_RESET_IH)\r\nsrbm_soft_reset |= SOFT_RESET_IH;\r\nif (reset_mask & RADEON_RESET_GRBM)\r\nsrbm_soft_reset |= SOFT_RESET_GRBM;\r\nif (reset_mask & RADEON_RESET_VMC)\r\nsrbm_soft_reset |= SOFT_RESET_VMC;\r\nif (reset_mask & RADEON_RESET_MC)\r\nsrbm_soft_reset |= SOFT_RESET_MC;\r\nif (grbm_soft_reset) {\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\ntmp |= grbm_soft_reset;\r\ndev_info(rdev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~grbm_soft_reset;\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\n}\r\nif (srbm_soft_reset) {\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\ntmp |= srbm_soft_reset;\r\ndev_info(rdev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~srbm_soft_reset;\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\n}\r\nudelay(50);\r\nevergreen_mc_resume(rdev, &save);\r\nudelay(50);\r\nevergreen_print_gpu_status_regs(rdev);\r\n}\r\nstatic void si_set_clk_bypass_mode(struct radeon_device *rdev)\r\n{\r\nu32 tmp, i;\r\ntmp = RREG32(CG_SPLL_FUNC_CNTL);\r\ntmp |= SPLL_BYPASS_EN;\r\nWREG32(CG_SPLL_FUNC_CNTL, tmp);\r\ntmp = RREG32(CG_SPLL_FUNC_CNTL_2);\r\ntmp |= SPLL_CTLREQ_CHG;\r\nWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(SPLL_STATUS) & SPLL_CHG_STATUS)\r\nbreak;\r\nudelay(1);\r\n}\r\ntmp = RREG32(CG_SPLL_FUNC_CNTL_2);\r\ntmp &= ~(SPLL_CTLREQ_CHG | SCLK_MUX_UPDATE);\r\nWREG32(CG_SPLL_FUNC_CNTL_2, tmp);\r\ntmp = RREG32(MPLL_CNTL_MODE);\r\ntmp &= ~MPLL_MCLK_SEL;\r\nWREG32(MPLL_CNTL_MODE, tmp);\r\n}\r\nstatic void si_spll_powerdown(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(SPLL_CNTL_MODE);\r\ntmp |= SPLL_SW_DIR_CONTROL;\r\nWREG32(SPLL_CNTL_MODE, tmp);\r\ntmp = RREG32(CG_SPLL_FUNC_CNTL);\r\ntmp |= SPLL_RESET;\r\nWREG32(CG_SPLL_FUNC_CNTL, tmp);\r\ntmp = RREG32(CG_SPLL_FUNC_CNTL);\r\ntmp |= SPLL_SLEEP;\r\nWREG32(CG_SPLL_FUNC_CNTL, tmp);\r\ntmp = RREG32(SPLL_CNTL_MODE);\r\ntmp &= ~SPLL_SW_DIR_CONTROL;\r\nWREG32(SPLL_CNTL_MODE, tmp);\r\n}\r\nstatic void si_gpu_pci_config_reset(struct radeon_device *rdev)\r\n{\r\nstruct evergreen_mc_save save;\r\nu32 tmp, i;\r\ndev_info(rdev->dev, "GPU pci config reset\n");\r\nsi_fini_pg(rdev);\r\nsi_fini_cg(rdev);\r\nWREG32(CP_ME_CNTL, CP_ME_HALT | CP_PFP_HALT | CP_CE_HALT);\r\ntmp = RREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET);\r\ntmp &= ~DMA_RB_ENABLE;\r\nWREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET, tmp);\r\ntmp = RREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET);\r\ntmp &= ~DMA_RB_ENABLE;\r\nWREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET, tmp);\r\nsi_rlc_stop(rdev);\r\nudelay(50);\r\nevergreen_mc_stop(rdev, &save);\r\nif (evergreen_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timed out !\n");\r\n}\r\nsi_set_clk_bypass_mode(rdev);\r\nsi_spll_powerdown(rdev);\r\npci_clear_master(rdev->pdev);\r\nradeon_pci_config_reset(rdev);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(CONFIG_MEMSIZE) != 0xffffffff)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nint si_asic_reset(struct radeon_device *rdev, bool hard)\r\n{\r\nu32 reset_mask;\r\nif (hard) {\r\nsi_gpu_pci_config_reset(rdev);\r\nreturn 0;\r\n}\r\nreset_mask = si_gpu_check_soft_reset(rdev);\r\nif (reset_mask)\r\nr600_set_bios_scratch_engine_hung(rdev, true);\r\nsi_gpu_soft_reset(rdev, reset_mask);\r\nreset_mask = si_gpu_check_soft_reset(rdev);\r\nif (reset_mask && radeon_hard_reset)\r\nsi_gpu_pci_config_reset(rdev);\r\nreset_mask = si_gpu_check_soft_reset(rdev);\r\nif (!reset_mask)\r\nr600_set_bios_scratch_engine_hung(rdev, false);\r\nreturn 0;\r\n}\r\nbool si_gfx_is_lockup(struct radeon_device *rdev, struct radeon_ring *ring)\r\n{\r\nu32 reset_mask = si_gpu_check_soft_reset(rdev);\r\nif (!(reset_mask & (RADEON_RESET_GFX |\r\nRADEON_RESET_COMPUTE |\r\nRADEON_RESET_CP))) {\r\nradeon_ring_lockup_update(rdev, ring);\r\nreturn false;\r\n}\r\nreturn radeon_ring_test_lockup(rdev, ring);\r\n}\r\nstatic void si_mc_program(struct radeon_device *rdev)\r\n{\r\nstruct evergreen_mc_save save;\r\nu32 tmp;\r\nint i, j;\r\nfor (i = 0, j = 0; i < 32; i++, j += 0x18) {\r\nWREG32((0x2c14 + j), 0x00000000);\r\nWREG32((0x2c18 + j), 0x00000000);\r\nWREG32((0x2c1c + j), 0x00000000);\r\nWREG32((0x2c20 + j), 0x00000000);\r\nWREG32((0x2c24 + j), 0x00000000);\r\n}\r\nWREG32(HDP_REG_COHERENCY_FLUSH_CNTL, 0);\r\nevergreen_mc_stop(rdev, &save);\r\nif (radeon_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nif (!ASIC_IS_NODCE(rdev))\r\nWREG32(VGA_HDP_CONTROL, VGA_MEMORY_DISABLE);\r\nWREG32(MC_VM_SYSTEM_APERTURE_LOW_ADDR,\r\nrdev->mc.vram_start >> 12);\r\nWREG32(MC_VM_SYSTEM_APERTURE_HIGH_ADDR,\r\nrdev->mc.vram_end >> 12);\r\nWREG32(MC_VM_SYSTEM_APERTURE_DEFAULT_ADDR,\r\nrdev->vram_scratch.gpu_addr >> 12);\r\ntmp = ((rdev->mc.vram_end >> 24) & 0xFFFF) << 16;\r\ntmp |= ((rdev->mc.vram_start >> 24) & 0xFFFF);\r\nWREG32(MC_VM_FB_LOCATION, tmp);\r\nWREG32(HDP_NONSURFACE_BASE, (rdev->mc.vram_start >> 8));\r\nWREG32(HDP_NONSURFACE_INFO, (2 << 7) | (1 << 30));\r\nWREG32(HDP_NONSURFACE_SIZE, 0x3FFFFFFF);\r\nWREG32(MC_VM_AGP_BASE, 0);\r\nWREG32(MC_VM_AGP_TOP, 0x0FFFFFFF);\r\nWREG32(MC_VM_AGP_BOT, 0x0FFFFFFF);\r\nif (radeon_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nevergreen_mc_resume(rdev, &save);\r\nif (!ASIC_IS_NODCE(rdev)) {\r\nrv515_vga_render_disable(rdev);\r\n}\r\n}\r\nvoid si_vram_gtt_location(struct radeon_device *rdev,\r\nstruct radeon_mc *mc)\r\n{\r\nif (mc->mc_vram_size > 0xFFC0000000ULL) {\r\ndev_warn(rdev->dev, "limiting VRAM\n");\r\nmc->real_vram_size = 0xFFC0000000ULL;\r\nmc->mc_vram_size = 0xFFC0000000ULL;\r\n}\r\nradeon_vram_location(rdev, &rdev->mc, 0);\r\nrdev->mc.gtt_base_align = 0;\r\nradeon_gtt_location(rdev, mc);\r\n}\r\nstatic int si_mc_init(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\nint chansize, numchan;\r\nrdev->mc.vram_is_ddr = true;\r\ntmp = RREG32(MC_ARB_RAMCFG);\r\nif (tmp & CHANSIZE_OVERRIDE) {\r\nchansize = 16;\r\n} else if (tmp & CHANSIZE_MASK) {\r\nchansize = 64;\r\n} else {\r\nchansize = 32;\r\n}\r\ntmp = RREG32(MC_SHARED_CHMAP);\r\nswitch ((tmp & NOOFCHAN_MASK) >> NOOFCHAN_SHIFT) {\r\ncase 0:\r\ndefault:\r\nnumchan = 1;\r\nbreak;\r\ncase 1:\r\nnumchan = 2;\r\nbreak;\r\ncase 2:\r\nnumchan = 4;\r\nbreak;\r\ncase 3:\r\nnumchan = 8;\r\nbreak;\r\ncase 4:\r\nnumchan = 3;\r\nbreak;\r\ncase 5:\r\nnumchan = 6;\r\nbreak;\r\ncase 6:\r\nnumchan = 10;\r\nbreak;\r\ncase 7:\r\nnumchan = 12;\r\nbreak;\r\ncase 8:\r\nnumchan = 16;\r\nbreak;\r\n}\r\nrdev->mc.vram_width = numchan * chansize;\r\nrdev->mc.aper_base = pci_resource_start(rdev->pdev, 0);\r\nrdev->mc.aper_size = pci_resource_len(rdev->pdev, 0);\r\ntmp = RREG32(CONFIG_MEMSIZE);\r\nif (tmp & 0xffff0000) {\r\nDRM_INFO("Probable bad vram size: 0x%08x\n", tmp);\r\nif (tmp & 0xffff)\r\ntmp &= 0xffff;\r\n}\r\nrdev->mc.mc_vram_size = tmp * 1024ULL * 1024ULL;\r\nrdev->mc.real_vram_size = rdev->mc.mc_vram_size;\r\nrdev->mc.visible_vram_size = rdev->mc.aper_size;\r\nsi_vram_gtt_location(rdev, &rdev->mc);\r\nradeon_update_bandwidth_info(rdev);\r\nreturn 0;\r\n}\r\nvoid si_pcie_gart_tlb_flush(struct radeon_device *rdev)\r\n{\r\nWREG32(HDP_MEM_COHERENCY_FLUSH_CNTL, 0x1);\r\nWREG32(VM_INVALIDATE_REQUEST, 1);\r\n}\r\nstatic int si_pcie_gart_enable(struct radeon_device *rdev)\r\n{\r\nint r, i;\r\nif (rdev->gart.robj == NULL) {\r\ndev_err(rdev->dev, "No VRAM object for PCIE GART.\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_gart_table_vram_pin(rdev);\r\nif (r)\r\nreturn r;\r\nWREG32(MC_VM_MX_L1_TLB_CNTL,\r\n(0xA << 7) |\r\nENABLE_L1_TLB |\r\nENABLE_L1_FRAGMENT_PROCESSING |\r\nSYSTEM_ACCESS_MODE_NOT_IN_SYS |\r\nENABLE_ADVANCED_DRIVER_MODEL |\r\nSYSTEM_APERTURE_UNMAPPED_ACCESS_PASS_THRU);\r\nWREG32(VM_L2_CNTL, ENABLE_L2_CACHE |\r\nENABLE_L2_FRAGMENT_PROCESSING |\r\nENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE |\r\nENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE |\r\nEFFECTIVE_L2_QUEUE_SIZE(7) |\r\nCONTEXT1_IDENTITY_ACCESS_MODE(1));\r\nWREG32(VM_L2_CNTL2, INVALIDATE_ALL_L1_TLBS | INVALIDATE_L2_CACHE);\r\nWREG32(VM_L2_CNTL3, L2_CACHE_BIGK_ASSOCIATIVITY |\r\nBANK_SELECT(4) |\r\nL2_CACHE_BIGK_FRAGMENT_SIZE(4));\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_START_ADDR, rdev->mc.gtt_start >> 12);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_END_ADDR, rdev->mc.gtt_end >> 12);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR, rdev->gart.table_addr >> 12);\r\nWREG32(VM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(rdev->dummy_page.addr >> 12));\r\nWREG32(VM_CONTEXT0_CNTL2, 0);\r\nWREG32(VM_CONTEXT0_CNTL, (ENABLE_CONTEXT | PAGE_TABLE_DEPTH(0) |\r\nRANGE_PROTECTION_FAULT_ENABLE_DEFAULT));\r\nWREG32(0x15D4, 0);\r\nWREG32(0x15D8, 0);\r\nWREG32(0x15DC, 0);\r\nWREG32(VM_CONTEXT1_PAGE_TABLE_START_ADDR, 0);\r\nWREG32(VM_CONTEXT1_PAGE_TABLE_END_ADDR, rdev->vm_manager.max_pfn - 1);\r\nfor (i = 1; i < 16; i++) {\r\nif (i < 8)\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (i << 2),\r\nrdev->vm_manager.saved_table_addr[i]);\r\nelse\r\nWREG32(VM_CONTEXT8_PAGE_TABLE_BASE_ADDR + ((i - 8) << 2),\r\nrdev->vm_manager.saved_table_addr[i]);\r\n}\r\nWREG32(VM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(rdev->dummy_page.addr >> 12));\r\nWREG32(VM_CONTEXT1_CNTL2, 4);\r\nWREG32(VM_CONTEXT1_CNTL, ENABLE_CONTEXT | PAGE_TABLE_DEPTH(1) |\r\nPAGE_TABLE_BLOCK_SIZE(radeon_vm_block_size - 9) |\r\nRANGE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nRANGE_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nDUMMY_PAGE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nDUMMY_PAGE_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nPDE0_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nPDE0_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nVALID_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nVALID_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nREAD_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nREAD_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nWRITE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nWRITE_PROTECTION_FAULT_ENABLE_DEFAULT);\r\nsi_pcie_gart_tlb_flush(rdev);\r\nDRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",\r\n(unsigned)(rdev->mc.gtt_size >> 20),\r\n(unsigned long long)rdev->gart.table_addr);\r\nrdev->gart.ready = true;\r\nreturn 0;\r\n}\r\nstatic void si_pcie_gart_disable(struct radeon_device *rdev)\r\n{\r\nunsigned i;\r\nfor (i = 1; i < 16; ++i) {\r\nuint32_t reg;\r\nif (i < 8)\r\nreg = VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (i << 2);\r\nelse\r\nreg = VM_CONTEXT8_PAGE_TABLE_BASE_ADDR + ((i - 8) << 2);\r\nrdev->vm_manager.saved_table_addr[i] = RREG32(reg);\r\n}\r\nWREG32(VM_CONTEXT0_CNTL, 0);\r\nWREG32(VM_CONTEXT1_CNTL, 0);\r\nWREG32(MC_VM_MX_L1_TLB_CNTL, SYSTEM_ACCESS_MODE_NOT_IN_SYS |\r\nSYSTEM_APERTURE_UNMAPPED_ACCESS_PASS_THRU);\r\nWREG32(VM_L2_CNTL, ENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE |\r\nENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE |\r\nEFFECTIVE_L2_QUEUE_SIZE(7) |\r\nCONTEXT1_IDENTITY_ACCESS_MODE(1));\r\nWREG32(VM_L2_CNTL2, 0);\r\nWREG32(VM_L2_CNTL3, L2_CACHE_BIGK_ASSOCIATIVITY |\r\nL2_CACHE_BIGK_FRAGMENT_SIZE(0));\r\nradeon_gart_table_vram_unpin(rdev);\r\n}\r\nstatic void si_pcie_gart_fini(struct radeon_device *rdev)\r\n{\r\nsi_pcie_gart_disable(rdev);\r\nradeon_gart_table_vram_free(rdev);\r\nradeon_gart_fini(rdev);\r\n}\r\nstatic bool si_vm_reg_valid(u32 reg)\r\n{\r\nif (reg >= 0x28000)\r\nreturn true;\r\nif (reg >= 0xB000 && reg < 0xC000)\r\nreturn true;\r\nswitch (reg) {\r\ncase GRBM_GFX_INDEX:\r\ncase CP_STRMOUT_CNTL:\r\ncase VGT_VTX_VECT_EJECT_REG:\r\ncase VGT_CACHE_INVALIDATION:\r\ncase VGT_ESGS_RING_SIZE:\r\ncase VGT_GSVS_RING_SIZE:\r\ncase VGT_GS_VERTEX_REUSE:\r\ncase VGT_PRIMITIVE_TYPE:\r\ncase VGT_INDEX_TYPE:\r\ncase VGT_NUM_INDICES:\r\ncase VGT_NUM_INSTANCES:\r\ncase VGT_TF_RING_SIZE:\r\ncase VGT_HS_OFFCHIP_PARAM:\r\ncase VGT_TF_MEMORY_BASE:\r\ncase PA_CL_ENHANCE:\r\ncase PA_SU_LINE_STIPPLE_VALUE:\r\ncase PA_SC_LINE_STIPPLE_STATE:\r\ncase PA_SC_ENHANCE:\r\ncase SQC_CACHES:\r\ncase SPI_STATIC_THREAD_MGMT_1:\r\ncase SPI_STATIC_THREAD_MGMT_2:\r\ncase SPI_STATIC_THREAD_MGMT_3:\r\ncase SPI_PS_MAX_WAVE_ID:\r\ncase SPI_CONFIG_CNTL:\r\ncase SPI_CONFIG_CNTL_1:\r\ncase TA_CNTL_AUX:\r\ncase TA_CS_BC_BASE_ADDR:\r\nreturn true;\r\ndefault:\r\nDRM_ERROR("Invalid register 0x%x in CS\n", reg);\r\nreturn false;\r\n}\r\n}\r\nstatic int si_vm_packet3_ce_check(struct radeon_device *rdev,\r\nu32 *ib, struct radeon_cs_packet *pkt)\r\n{\r\nswitch (pkt->opcode) {\r\ncase PACKET3_NOP:\r\ncase PACKET3_SET_BASE:\r\ncase PACKET3_SET_CE_DE_COUNTERS:\r\ncase PACKET3_LOAD_CONST_RAM:\r\ncase PACKET3_WRITE_CONST_RAM:\r\ncase PACKET3_WRITE_CONST_RAM_OFFSET:\r\ncase PACKET3_DUMP_CONST_RAM:\r\ncase PACKET3_INCREMENT_CE_COUNTER:\r\ncase PACKET3_WAIT_ON_DE_COUNTER:\r\ncase PACKET3_CE_WRITE:\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Invalid CE packet3: 0x%x\n", pkt->opcode);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int si_vm_packet3_cp_dma_check(u32 *ib, u32 idx)\r\n{\r\nu32 start_reg, reg, i;\r\nu32 command = ib[idx + 4];\r\nu32 info = ib[idx + 1];\r\nu32 idx_value = ib[idx];\r\nif (command & PACKET3_CP_DMA_CMD_SAS) {\r\nif (((info & 0x60000000) >> 29) == 0) {\r\nstart_reg = idx_value << 2;\r\nif (command & PACKET3_CP_DMA_CMD_SAIC) {\r\nreg = start_reg;\r\nif (!si_vm_reg_valid(reg)) {\r\nDRM_ERROR("CP DMA Bad SRC register\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nfor (i = 0; i < (command & 0x1fffff); i++) {\r\nreg = start_reg + (4 * i);\r\nif (!si_vm_reg_valid(reg)) {\r\nDRM_ERROR("CP DMA Bad SRC register\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\n}\r\n}\r\n}\r\nif (command & PACKET3_CP_DMA_CMD_DAS) {\r\nif (((info & 0x00300000) >> 20) == 0) {\r\nstart_reg = ib[idx + 2];\r\nif (command & PACKET3_CP_DMA_CMD_DAIC) {\r\nreg = start_reg;\r\nif (!si_vm_reg_valid(reg)) {\r\nDRM_ERROR("CP DMA Bad DST register\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nfor (i = 0; i < (command & 0x1fffff); i++) {\r\nreg = start_reg + (4 * i);\r\nif (!si_vm_reg_valid(reg)) {\r\nDRM_ERROR("CP DMA Bad DST register\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int si_vm_packet3_gfx_check(struct radeon_device *rdev,\r\nu32 *ib, struct radeon_cs_packet *pkt)\r\n{\r\nint r;\r\nu32 idx = pkt->idx + 1;\r\nu32 idx_value = ib[idx];\r\nu32 start_reg, end_reg, reg, i;\r\nswitch (pkt->opcode) {\r\ncase PACKET3_NOP:\r\ncase PACKET3_SET_BASE:\r\ncase PACKET3_CLEAR_STATE:\r\ncase PACKET3_INDEX_BUFFER_SIZE:\r\ncase PACKET3_DISPATCH_DIRECT:\r\ncase PACKET3_DISPATCH_INDIRECT:\r\ncase PACKET3_ALLOC_GDS:\r\ncase PACKET3_WRITE_GDS_RAM:\r\ncase PACKET3_ATOMIC_GDS:\r\ncase PACKET3_ATOMIC:\r\ncase PACKET3_OCCLUSION_QUERY:\r\ncase PACKET3_SET_PREDICATION:\r\ncase PACKET3_COND_EXEC:\r\ncase PACKET3_PRED_EXEC:\r\ncase PACKET3_DRAW_INDIRECT:\r\ncase PACKET3_DRAW_INDEX_INDIRECT:\r\ncase PACKET3_INDEX_BASE:\r\ncase PACKET3_DRAW_INDEX_2:\r\ncase PACKET3_CONTEXT_CONTROL:\r\ncase PACKET3_INDEX_TYPE:\r\ncase PACKET3_DRAW_INDIRECT_MULTI:\r\ncase PACKET3_DRAW_INDEX_AUTO:\r\ncase PACKET3_DRAW_INDEX_IMMD:\r\ncase PACKET3_NUM_INSTANCES:\r\ncase PACKET3_DRAW_INDEX_MULTI_AUTO:\r\ncase PACKET3_STRMOUT_BUFFER_UPDATE:\r\ncase PACKET3_DRAW_INDEX_OFFSET_2:\r\ncase PACKET3_DRAW_INDEX_MULTI_ELEMENT:\r\ncase PACKET3_DRAW_INDEX_INDIRECT_MULTI:\r\ncase PACKET3_MPEG_INDEX:\r\ncase PACKET3_WAIT_REG_MEM:\r\ncase PACKET3_MEM_WRITE:\r\ncase PACKET3_PFP_SYNC_ME:\r\ncase PACKET3_SURFACE_SYNC:\r\ncase PACKET3_EVENT_WRITE:\r\ncase PACKET3_EVENT_WRITE_EOP:\r\ncase PACKET3_EVENT_WRITE_EOS:\r\ncase PACKET3_SET_CONTEXT_REG:\r\ncase PACKET3_SET_CONTEXT_REG_INDIRECT:\r\ncase PACKET3_SET_SH_REG:\r\ncase PACKET3_SET_SH_REG_OFFSET:\r\ncase PACKET3_INCREMENT_DE_COUNTER:\r\ncase PACKET3_WAIT_ON_CE_COUNTER:\r\ncase PACKET3_WAIT_ON_AVAIL_BUFFER:\r\ncase PACKET3_ME_WRITE:\r\nbreak;\r\ncase PACKET3_COPY_DATA:\r\nif ((idx_value & 0xf00) == 0) {\r\nreg = ib[idx + 3] * 4;\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET3_WRITE_DATA:\r\nif ((idx_value & 0xf00) == 0) {\r\nstart_reg = ib[idx + 1] * 4;\r\nif (idx_value & 0x10000) {\r\nif (!si_vm_reg_valid(start_reg))\r\nreturn -EINVAL;\r\n} else {\r\nfor (i = 0; i < (pkt->count - 2); i++) {\r\nreg = start_reg + (4 * i);\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\n}\r\n}\r\nbreak;\r\ncase PACKET3_COND_WRITE:\r\nif (idx_value & 0x100) {\r\nreg = ib[idx + 5] * 4;\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET3_COPY_DW:\r\nif (idx_value & 0x2) {\r\nreg = ib[idx + 3] * 4;\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET3_SET_CONFIG_REG:\r\nstart_reg = (idx_value << 2) + PACKET3_SET_CONFIG_REG_START;\r\nend_reg = 4 * pkt->count + start_reg - 4;\r\nif ((start_reg < PACKET3_SET_CONFIG_REG_START) ||\r\n(start_reg >= PACKET3_SET_CONFIG_REG_END) ||\r\n(end_reg >= PACKET3_SET_CONFIG_REG_END)) {\r\nDRM_ERROR("bad PACKET3_SET_CONFIG_REG\n");\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < pkt->count; i++) {\r\nreg = start_reg + (4 * i);\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET3_CP_DMA:\r\nr = si_vm_packet3_cp_dma_check(ib, idx);\r\nif (r)\r\nreturn r;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Invalid GFX packet3: 0x%x\n", pkt->opcode);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int si_vm_packet3_compute_check(struct radeon_device *rdev,\r\nu32 *ib, struct radeon_cs_packet *pkt)\r\n{\r\nint r;\r\nu32 idx = pkt->idx + 1;\r\nu32 idx_value = ib[idx];\r\nu32 start_reg, reg, i;\r\nswitch (pkt->opcode) {\r\ncase PACKET3_NOP:\r\ncase PACKET3_SET_BASE:\r\ncase PACKET3_CLEAR_STATE:\r\ncase PACKET3_DISPATCH_DIRECT:\r\ncase PACKET3_DISPATCH_INDIRECT:\r\ncase PACKET3_ALLOC_GDS:\r\ncase PACKET3_WRITE_GDS_RAM:\r\ncase PACKET3_ATOMIC_GDS:\r\ncase PACKET3_ATOMIC:\r\ncase PACKET3_OCCLUSION_QUERY:\r\ncase PACKET3_SET_PREDICATION:\r\ncase PACKET3_COND_EXEC:\r\ncase PACKET3_PRED_EXEC:\r\ncase PACKET3_CONTEXT_CONTROL:\r\ncase PACKET3_STRMOUT_BUFFER_UPDATE:\r\ncase PACKET3_WAIT_REG_MEM:\r\ncase PACKET3_MEM_WRITE:\r\ncase PACKET3_PFP_SYNC_ME:\r\ncase PACKET3_SURFACE_SYNC:\r\ncase PACKET3_EVENT_WRITE:\r\ncase PACKET3_EVENT_WRITE_EOP:\r\ncase PACKET3_EVENT_WRITE_EOS:\r\ncase PACKET3_SET_CONTEXT_REG:\r\ncase PACKET3_SET_CONTEXT_REG_INDIRECT:\r\ncase PACKET3_SET_SH_REG:\r\ncase PACKET3_SET_SH_REG_OFFSET:\r\ncase PACKET3_INCREMENT_DE_COUNTER:\r\ncase PACKET3_WAIT_ON_CE_COUNTER:\r\ncase PACKET3_WAIT_ON_AVAIL_BUFFER:\r\ncase PACKET3_ME_WRITE:\r\nbreak;\r\ncase PACKET3_COPY_DATA:\r\nif ((idx_value & 0xf00) == 0) {\r\nreg = ib[idx + 3] * 4;\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET3_WRITE_DATA:\r\nif ((idx_value & 0xf00) == 0) {\r\nstart_reg = ib[idx + 1] * 4;\r\nif (idx_value & 0x10000) {\r\nif (!si_vm_reg_valid(start_reg))\r\nreturn -EINVAL;\r\n} else {\r\nfor (i = 0; i < (pkt->count - 2); i++) {\r\nreg = start_reg + (4 * i);\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\n}\r\n}\r\nbreak;\r\ncase PACKET3_COND_WRITE:\r\nif (idx_value & 0x100) {\r\nreg = ib[idx + 5] * 4;\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET3_COPY_DW:\r\nif (idx_value & 0x2) {\r\nreg = ib[idx + 3] * 4;\r\nif (!si_vm_reg_valid(reg))\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET3_CP_DMA:\r\nr = si_vm_packet3_cp_dma_check(ib, idx);\r\nif (r)\r\nreturn r;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Invalid Compute packet3: 0x%x\n", pkt->opcode);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint si_ib_parse(struct radeon_device *rdev, struct radeon_ib *ib)\r\n{\r\nint ret = 0;\r\nu32 idx = 0, i;\r\nstruct radeon_cs_packet pkt;\r\ndo {\r\npkt.idx = idx;\r\npkt.type = RADEON_CP_PACKET_GET_TYPE(ib->ptr[idx]);\r\npkt.count = RADEON_CP_PACKET_GET_COUNT(ib->ptr[idx]);\r\npkt.one_reg_wr = 0;\r\nswitch (pkt.type) {\r\ncase RADEON_PACKET_TYPE0:\r\ndev_err(rdev->dev, "Packet0 not allowed!\n");\r\nret = -EINVAL;\r\nbreak;\r\ncase RADEON_PACKET_TYPE2:\r\nidx += 1;\r\nbreak;\r\ncase RADEON_PACKET_TYPE3:\r\npkt.opcode = RADEON_CP_PACKET3_GET_OPCODE(ib->ptr[idx]);\r\nif (ib->is_const_ib)\r\nret = si_vm_packet3_ce_check(rdev, ib->ptr, &pkt);\r\nelse {\r\nswitch (ib->ring) {\r\ncase RADEON_RING_TYPE_GFX_INDEX:\r\nret = si_vm_packet3_gfx_check(rdev, ib->ptr, &pkt);\r\nbreak;\r\ncase CAYMAN_RING_TYPE_CP1_INDEX:\r\ncase CAYMAN_RING_TYPE_CP2_INDEX:\r\nret = si_vm_packet3_compute_check(rdev, ib->ptr, &pkt);\r\nbreak;\r\ndefault:\r\ndev_err(rdev->dev, "Non-PM4 ring %d !\n", ib->ring);\r\nret = -EINVAL;\r\nbreak;\r\n}\r\n}\r\nidx += pkt.count + 2;\r\nbreak;\r\ndefault:\r\ndev_err(rdev->dev, "Unknown packet type %d !\n", pkt.type);\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nif (ret) {\r\nfor (i = 0; i < ib->length_dw; i++) {\r\nif (i == idx)\r\nprintk("\t0x%08x <---\n", ib->ptr[i]);\r\nelse\r\nprintk("\t0x%08x\n", ib->ptr[i]);\r\n}\r\nbreak;\r\n}\r\n} while (idx < ib->length_dw);\r\nreturn ret;\r\n}\r\nint si_vm_init(struct radeon_device *rdev)\r\n{\r\nrdev->vm_manager.nvm = 16;\r\nrdev->vm_manager.vram_base_offset = 0;\r\nreturn 0;\r\n}\r\nvoid si_vm_fini(struct radeon_device *rdev)\r\n{\r\n}\r\nstatic void si_vm_decode_fault(struct radeon_device *rdev,\r\nu32 status, u32 addr)\r\n{\r\nu32 mc_id = (status & MEMORY_CLIENT_ID_MASK) >> MEMORY_CLIENT_ID_SHIFT;\r\nu32 vmid = (status & FAULT_VMID_MASK) >> FAULT_VMID_SHIFT;\r\nu32 protections = (status & PROTECTIONS_MASK) >> PROTECTIONS_SHIFT;\r\nchar *block;\r\nif (rdev->family == CHIP_TAHITI) {\r\nswitch (mc_id) {\r\ncase 160:\r\ncase 144:\r\ncase 96:\r\ncase 80:\r\ncase 224:\r\ncase 208:\r\ncase 32:\r\ncase 16:\r\nblock = "CB";\r\nbreak;\r\ncase 161:\r\ncase 145:\r\ncase 97:\r\ncase 81:\r\ncase 225:\r\ncase 209:\r\ncase 33:\r\ncase 17:\r\nblock = "CB_FMASK";\r\nbreak;\r\ncase 162:\r\ncase 146:\r\ncase 98:\r\ncase 82:\r\ncase 226:\r\ncase 210:\r\ncase 34:\r\ncase 18:\r\nblock = "CB_CMASK";\r\nbreak;\r\ncase 163:\r\ncase 147:\r\ncase 99:\r\ncase 83:\r\ncase 227:\r\ncase 211:\r\ncase 35:\r\ncase 19:\r\nblock = "CB_IMMED";\r\nbreak;\r\ncase 164:\r\ncase 148:\r\ncase 100:\r\ncase 84:\r\ncase 228:\r\ncase 212:\r\ncase 36:\r\ncase 20:\r\nblock = "DB";\r\nbreak;\r\ncase 165:\r\ncase 149:\r\ncase 101:\r\ncase 85:\r\ncase 229:\r\ncase 213:\r\ncase 37:\r\ncase 21:\r\nblock = "DB_HTILE";\r\nbreak;\r\ncase 167:\r\ncase 151:\r\ncase 103:\r\ncase 87:\r\ncase 231:\r\ncase 215:\r\ncase 39:\r\ncase 23:\r\nblock = "DB_STEN";\r\nbreak;\r\ncase 72:\r\ncase 68:\r\ncase 64:\r\ncase 8:\r\ncase 4:\r\ncase 0:\r\ncase 136:\r\ncase 132:\r\ncase 128:\r\ncase 200:\r\ncase 196:\r\ncase 192:\r\nblock = "TC";\r\nbreak;\r\ncase 112:\r\ncase 48:\r\nblock = "CP";\r\nbreak;\r\ncase 49:\r\ncase 177:\r\ncase 50:\r\ncase 178:\r\nblock = "SH";\r\nbreak;\r\ncase 53:\r\ncase 190:\r\nblock = "VGT";\r\nbreak;\r\ncase 117:\r\nblock = "IH";\r\nbreak;\r\ncase 51:\r\ncase 115:\r\nblock = "RLC";\r\nbreak;\r\ncase 119:\r\ncase 183:\r\nblock = "DMA0";\r\nbreak;\r\ncase 61:\r\nblock = "DMA1";\r\nbreak;\r\ncase 248:\r\ncase 120:\r\nblock = "HDP";\r\nbreak;\r\ndefault:\r\nblock = "unknown";\r\nbreak;\r\n}\r\n} else {\r\nswitch (mc_id) {\r\ncase 32:\r\ncase 16:\r\ncase 96:\r\ncase 80:\r\ncase 160:\r\ncase 144:\r\ncase 224:\r\ncase 208:\r\nblock = "CB";\r\nbreak;\r\ncase 33:\r\ncase 17:\r\ncase 97:\r\ncase 81:\r\ncase 161:\r\ncase 145:\r\ncase 225:\r\ncase 209:\r\nblock = "CB_FMASK";\r\nbreak;\r\ncase 34:\r\ncase 18:\r\ncase 98:\r\ncase 82:\r\ncase 162:\r\ncase 146:\r\ncase 226:\r\ncase 210:\r\nblock = "CB_CMASK";\r\nbreak;\r\ncase 35:\r\ncase 19:\r\ncase 99:\r\ncase 83:\r\ncase 163:\r\ncase 147:\r\ncase 227:\r\ncase 211:\r\nblock = "CB_IMMED";\r\nbreak;\r\ncase 36:\r\ncase 20:\r\ncase 100:\r\ncase 84:\r\ncase 164:\r\ncase 148:\r\ncase 228:\r\ncase 212:\r\nblock = "DB";\r\nbreak;\r\ncase 37:\r\ncase 21:\r\ncase 101:\r\ncase 85:\r\ncase 165:\r\ncase 149:\r\ncase 229:\r\ncase 213:\r\nblock = "DB_HTILE";\r\nbreak;\r\ncase 39:\r\ncase 23:\r\ncase 103:\r\ncase 87:\r\ncase 167:\r\ncase 151:\r\ncase 231:\r\ncase 215:\r\nblock = "DB_STEN";\r\nbreak;\r\ncase 72:\r\ncase 68:\r\ncase 8:\r\ncase 4:\r\ncase 136:\r\ncase 132:\r\ncase 200:\r\ncase 196:\r\nblock = "TC";\r\nbreak;\r\ncase 112:\r\ncase 48:\r\nblock = "CP";\r\nbreak;\r\ncase 49:\r\ncase 177:\r\ncase 50:\r\ncase 178:\r\nblock = "SH";\r\nbreak;\r\ncase 53:\r\nblock = "VGT";\r\nbreak;\r\ncase 117:\r\nblock = "IH";\r\nbreak;\r\ncase 51:\r\ncase 115:\r\nblock = "RLC";\r\nbreak;\r\ncase 119:\r\ncase 183:\r\nblock = "DMA0";\r\nbreak;\r\ncase 61:\r\nblock = "DMA1";\r\nbreak;\r\ncase 248:\r\ncase 120:\r\nblock = "HDP";\r\nbreak;\r\ndefault:\r\nblock = "unknown";\r\nbreak;\r\n}\r\n}\r\nprintk("VM fault (0x%02x, vmid %d) at page %u, %s from %s (%d)\n",\r\nprotections, vmid, addr,\r\n(status & MEMORY_CLIENT_RW_MASK) ? "write" : "read",\r\nblock, mc_id);\r\n}\r\nvoid si_vm_flush(struct radeon_device *rdev, struct radeon_ring *ring,\r\nunsigned vm_id, uint64_t pd_addr)\r\n{\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(1) |\r\nWRITE_DATA_DST_SEL(0)));\r\nif (vm_id < 8) {\r\nradeon_ring_write(ring,\r\n(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (vm_id << 2)) >> 2);\r\n} else {\r\nradeon_ring_write(ring,\r\n(VM_CONTEXT8_PAGE_TABLE_BASE_ADDR + ((vm_id - 8) << 2)) >> 2);\r\n}\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, pd_addr >> 12);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(1) |\r\nWRITE_DATA_DST_SEL(0)));\r\nradeon_ring_write(ring, HDP_MEM_COHERENCY_FLUSH_CNTL >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0x1);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(1) |\r\nWRITE_DATA_DST_SEL(0)));\r\nradeon_ring_write(ring, VM_INVALIDATE_REQUEST >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 1 << vm_id);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WAIT_REG_MEM, 5));\r\nradeon_ring_write(ring, (WAIT_REG_MEM_FUNCTION(0) |\r\nWAIT_REG_MEM_ENGINE(0)));\r\nradeon_ring_write(ring, VM_INVALIDATE_REQUEST >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0x20);\r\nradeon_ring_write(ring, PACKET3(PACKET3_PFP_SYNC_ME, 0));\r\nradeon_ring_write(ring, 0x0);\r\n}\r\nstatic void si_wait_for_rlc_serdes(struct radeon_device *rdev)\r\n{\r\nint i;\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(RLC_SERDES_MASTER_BUSY_0) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(RLC_SERDES_MASTER_BUSY_1) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nstatic void si_enable_gui_idle_interrupt(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 tmp = RREG32(CP_INT_CNTL_RING0);\r\nu32 mask;\r\nint i;\r\nif (enable)\r\ntmp |= (CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\nelse\r\ntmp &= ~(CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\nWREG32(CP_INT_CNTL_RING0, tmp);\r\nif (!enable) {\r\ntmp = RREG32(DB_DEPTH_INFO);\r\nmask = RLC_BUSY_STATUS | GFX_POWER_STATUS | GFX_CLOCK_STATUS | GFX_LS_STATUS;\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif ((RREG32(RLC_STAT) & mask) == (GFX_CLOCK_STATUS | GFX_POWER_STATUS))\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\n}\r\nstatic void si_set_uvd_dcm(struct radeon_device *rdev,\r\nbool sw_mode)\r\n{\r\nu32 tmp, tmp2;\r\ntmp = RREG32(UVD_CGC_CTRL);\r\ntmp &= ~(CLK_OD_MASK | CG_DT_MASK);\r\ntmp |= DCM | CG_DT(1) | CLK_OD(4);\r\nif (sw_mode) {\r\ntmp &= ~0x7ffff800;\r\ntmp2 = DYN_OR_EN | DYN_RR_EN | G_DIV_ID(7);\r\n} else {\r\ntmp |= 0x7ffff800;\r\ntmp2 = 0;\r\n}\r\nWREG32(UVD_CGC_CTRL, tmp);\r\nWREG32_UVD_CTX(UVD_CGC_CTRL2, tmp2);\r\n}\r\nvoid si_init_uvd_internal_cg(struct radeon_device *rdev)\r\n{\r\nbool hw_mode = true;\r\nif (hw_mode) {\r\nsi_set_uvd_dcm(rdev, false);\r\n} else {\r\nu32 tmp = RREG32(UVD_CGC_CTRL);\r\ntmp &= ~DCM;\r\nWREG32(UVD_CGC_CTRL, tmp);\r\n}\r\n}\r\nstatic u32 si_halt_rlc(struct radeon_device *rdev)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_CNTL);\r\nif (data & RLC_ENABLE) {\r\ndata &= ~RLC_ENABLE;\r\nWREG32(RLC_CNTL, data);\r\nsi_wait_for_rlc_serdes(rdev);\r\n}\r\nreturn orig;\r\n}\r\nstatic void si_update_rlc(struct radeon_device *rdev, u32 rlc)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(RLC_CNTL);\r\nif (tmp != rlc)\r\nWREG32(RLC_CNTL, rlc);\r\n}\r\nstatic void si_enable_dma_pg(struct radeon_device *rdev, bool enable)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(DMA_PG);\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_SDMA))\r\ndata |= PG_CNTL_ENABLE;\r\nelse\r\ndata &= ~PG_CNTL_ENABLE;\r\nif (orig != data)\r\nWREG32(DMA_PG, data);\r\n}\r\nstatic void si_init_dma_pg(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\nWREG32(DMA_PGFSM_WRITE, 0x00002000);\r\nWREG32(DMA_PGFSM_CONFIG, 0x100010ff);\r\nfor (tmp = 0; tmp < 5; tmp++)\r\nWREG32(DMA_PGFSM_WRITE, 0);\r\n}\r\nstatic void si_enable_gfx_cgpg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 tmp;\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_GFX_PG)) {\r\ntmp = RLC_PUD(0x10) | RLC_PDD(0x10) | RLC_TTPD(0x10) | RLC_MSD(0x10);\r\nWREG32(RLC_TTOP_D, tmp);\r\ntmp = RREG32(RLC_PG_CNTL);\r\ntmp |= GFX_PG_ENABLE;\r\nWREG32(RLC_PG_CNTL, tmp);\r\ntmp = RREG32(RLC_AUTO_PG_CTRL);\r\ntmp |= AUTO_PG_EN;\r\nWREG32(RLC_AUTO_PG_CTRL, tmp);\r\n} else {\r\ntmp = RREG32(RLC_AUTO_PG_CTRL);\r\ntmp &= ~AUTO_PG_EN;\r\nWREG32(RLC_AUTO_PG_CTRL, tmp);\r\ntmp = RREG32(DB_RENDER_CONTROL);\r\n}\r\n}\r\nstatic void si_init_gfx_cgpg(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\nWREG32(RLC_SAVE_AND_RESTORE_BASE, rdev->rlc.save_restore_gpu_addr >> 8);\r\ntmp = RREG32(RLC_PG_CNTL);\r\ntmp |= GFX_PG_SRC;\r\nWREG32(RLC_PG_CNTL, tmp);\r\nWREG32(RLC_CLEAR_STATE_RESTORE_BASE, rdev->rlc.clear_state_gpu_addr >> 8);\r\ntmp = RREG32(RLC_AUTO_PG_CTRL);\r\ntmp &= ~GRBM_REG_SGIT_MASK;\r\ntmp |= GRBM_REG_SGIT(0x700);\r\ntmp &= ~PG_AFTER_GRBM_REG_ST_MASK;\r\nWREG32(RLC_AUTO_PG_CTRL, tmp);\r\n}\r\nstatic u32 si_get_cu_active_bitmap(struct radeon_device *rdev, u32 se, u32 sh)\r\n{\r\nu32 mask = 0, tmp, tmp1;\r\nint i;\r\nsi_select_se_sh(rdev, se, sh);\r\ntmp = RREG32(CC_GC_SHADER_ARRAY_CONFIG);\r\ntmp1 = RREG32(GC_USER_SHADER_ARRAY_CONFIG);\r\nsi_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\ntmp &= 0xffff0000;\r\ntmp |= tmp1;\r\ntmp >>= 16;\r\nfor (i = 0; i < rdev->config.si.max_cu_per_sh; i ++) {\r\nmask <<= 1;\r\nmask |= 1;\r\n}\r\nreturn (~tmp) & mask;\r\n}\r\nstatic void si_init_ao_cu_mask(struct radeon_device *rdev)\r\n{\r\nu32 i, j, k, active_cu_number = 0;\r\nu32 mask, counter, cu_bitmap;\r\nu32 tmp = 0;\r\nfor (i = 0; i < rdev->config.si.max_shader_engines; i++) {\r\nfor (j = 0; j < rdev->config.si.max_sh_per_se; j++) {\r\nmask = 1;\r\ncu_bitmap = 0;\r\ncounter = 0;\r\nfor (k = 0; k < rdev->config.si.max_cu_per_sh; k++) {\r\nif (si_get_cu_active_bitmap(rdev, i, j) & mask) {\r\nif (counter < 2)\r\ncu_bitmap |= mask;\r\ncounter++;\r\n}\r\nmask <<= 1;\r\n}\r\nactive_cu_number += counter;\r\ntmp |= (cu_bitmap << (i * 16 + j * 8));\r\n}\r\n}\r\nWREG32(RLC_PG_AO_CU_MASK, tmp);\r\ntmp = RREG32(RLC_MAX_PG_CU);\r\ntmp &= ~MAX_PU_CU_MASK;\r\ntmp |= MAX_PU_CU(active_cu_number);\r\nWREG32(RLC_MAX_PG_CU, tmp);\r\n}\r\nstatic void si_enable_cgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 data, orig, tmp;\r\norig = data = RREG32(RLC_CGCG_CGLS_CTRL);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_CGCG)) {\r\nsi_enable_gui_idle_interrupt(rdev, true);\r\nWREG32(RLC_GCPM_GENERAL_3, 0x00000080);\r\ntmp = si_halt_rlc(rdev);\r\nWREG32(RLC_SERDES_WR_MASTER_MASK_0, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_MASTER_MASK_1, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_CTRL, 0x00b000ff);\r\nsi_wait_for_rlc_serdes(rdev);\r\nsi_update_rlc(rdev, tmp);\r\nWREG32(RLC_SERDES_WR_CTRL, 0x007000ff);\r\ndata |= CGCG_EN | CGLS_EN;\r\n} else {\r\nsi_enable_gui_idle_interrupt(rdev, false);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\ndata &= ~(CGCG_EN | CGLS_EN);\r\n}\r\nif (orig != data)\r\nWREG32(RLC_CGCG_CGLS_CTRL, data);\r\n}\r\nstatic void si_enable_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 data, orig, tmp = 0;\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_MGCG)) {\r\norig = data = RREG32(CGTS_SM_CTRL_REG);\r\ndata = 0x96940200;\r\nif (orig != data)\r\nWREG32(CGTS_SM_CTRL_REG, data);\r\nif (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_CP_LS) {\r\norig = data = RREG32(CP_MEM_SLP_CNTL);\r\ndata |= CP_MEM_LS_EN;\r\nif (orig != data)\r\nWREG32(CP_MEM_SLP_CNTL, data);\r\n}\r\norig = data = RREG32(RLC_CGTT_MGCG_OVERRIDE);\r\ndata &= 0xffffffc0;\r\nif (orig != data)\r\nWREG32(RLC_CGTT_MGCG_OVERRIDE, data);\r\ntmp = si_halt_rlc(rdev);\r\nWREG32(RLC_SERDES_WR_MASTER_MASK_0, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_MASTER_MASK_1, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_CTRL, 0x00d000ff);\r\nsi_update_rlc(rdev, tmp);\r\n} else {\r\norig = data = RREG32(RLC_CGTT_MGCG_OVERRIDE);\r\ndata |= 0x00000003;\r\nif (orig != data)\r\nWREG32(RLC_CGTT_MGCG_OVERRIDE, data);\r\ndata = RREG32(CP_MEM_SLP_CNTL);\r\nif (data & CP_MEM_LS_EN) {\r\ndata &= ~CP_MEM_LS_EN;\r\nWREG32(CP_MEM_SLP_CNTL, data);\r\n}\r\norig = data = RREG32(CGTS_SM_CTRL_REG);\r\ndata |= LS_OVERRIDE | OVERRIDE;\r\nif (orig != data)\r\nWREG32(CGTS_SM_CTRL_REG, data);\r\ntmp = si_halt_rlc(rdev);\r\nWREG32(RLC_SERDES_WR_MASTER_MASK_0, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_MASTER_MASK_1, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_CTRL, 0x00e000ff);\r\nsi_update_rlc(rdev, tmp);\r\n}\r\n}\r\nstatic void si_enable_uvd_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data, tmp;\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_UVD_MGCG)) {\r\ntmp = RREG32_UVD_CTX(UVD_CGC_MEM_CTRL);\r\ntmp |= 0x3fff;\r\nWREG32_UVD_CTX(UVD_CGC_MEM_CTRL, tmp);\r\norig = data = RREG32(UVD_CGC_CTRL);\r\ndata |= DCM;\r\nif (orig != data)\r\nWREG32(UVD_CGC_CTRL, data);\r\nWREG32_SMC(SMC_CG_IND_START + CG_CGTT_LOCAL_0, 0);\r\nWREG32_SMC(SMC_CG_IND_START + CG_CGTT_LOCAL_1, 0);\r\n} else {\r\ntmp = RREG32_UVD_CTX(UVD_CGC_MEM_CTRL);\r\ntmp &= ~0x3fff;\r\nWREG32_UVD_CTX(UVD_CGC_MEM_CTRL, tmp);\r\norig = data = RREG32(UVD_CGC_CTRL);\r\ndata &= ~DCM;\r\nif (orig != data)\r\nWREG32(UVD_CGC_CTRL, data);\r\nWREG32_SMC(SMC_CG_IND_START + CG_CGTT_LOCAL_0, 0xffffffff);\r\nWREG32_SMC(SMC_CG_IND_START + CG_CGTT_LOCAL_1, 0xffffffff);\r\n}\r\n}\r\nstatic void si_enable_mc_ls(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nint i;\r\nu32 orig, data;\r\nfor (i = 0; i < ARRAY_SIZE(mc_cg_registers); i++) {\r\norig = data = RREG32(mc_cg_registers[i]);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_MC_LS))\r\ndata |= MC_LS_ENABLE;\r\nelse\r\ndata &= ~MC_LS_ENABLE;\r\nif (data != orig)\r\nWREG32(mc_cg_registers[i], data);\r\n}\r\n}\r\nstatic void si_enable_mc_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nint i;\r\nu32 orig, data;\r\nfor (i = 0; i < ARRAY_SIZE(mc_cg_registers); i++) {\r\norig = data = RREG32(mc_cg_registers[i]);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_MC_MGCG))\r\ndata |= MC_CG_ENABLE;\r\nelse\r\ndata &= ~MC_CG_ENABLE;\r\nif (data != orig)\r\nWREG32(mc_cg_registers[i], data);\r\n}\r\n}\r\nstatic void si_enable_dma_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data, offset;\r\nint i;\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_SDMA_MGCG)) {\r\nfor (i = 0; i < 2; i++) {\r\nif (i == 0)\r\noffset = DMA0_REGISTER_OFFSET;\r\nelse\r\noffset = DMA1_REGISTER_OFFSET;\r\norig = data = RREG32(DMA_POWER_CNTL + offset);\r\ndata &= ~MEM_POWER_OVERRIDE;\r\nif (data != orig)\r\nWREG32(DMA_POWER_CNTL + offset, data);\r\nWREG32(DMA_CLK_CTRL + offset, 0x00000100);\r\n}\r\n} else {\r\nfor (i = 0; i < 2; i++) {\r\nif (i == 0)\r\noffset = DMA0_REGISTER_OFFSET;\r\nelse\r\noffset = DMA1_REGISTER_OFFSET;\r\norig = data = RREG32(DMA_POWER_CNTL + offset);\r\ndata |= MEM_POWER_OVERRIDE;\r\nif (data != orig)\r\nWREG32(DMA_POWER_CNTL + offset, data);\r\norig = data = RREG32(DMA_CLK_CTRL + offset);\r\ndata = 0xff000000;\r\nif (data != orig)\r\nWREG32(DMA_CLK_CTRL + offset, data);\r\n}\r\n}\r\n}\r\nstatic void si_enable_bif_mgls(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\norig = data = RREG32_PCIE(PCIE_CNTL2);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_BIF_LS))\r\ndata |= SLV_MEM_LS_EN | MST_MEM_LS_EN |\r\nREPLAY_MEM_LS_EN | SLV_MEM_AGGRESSIVE_LS_EN;\r\nelse\r\ndata &= ~(SLV_MEM_LS_EN | MST_MEM_LS_EN |\r\nREPLAY_MEM_LS_EN | SLV_MEM_AGGRESSIVE_LS_EN);\r\nif (orig != data)\r\nWREG32_PCIE(PCIE_CNTL2, data);\r\n}\r\nstatic void si_enable_hdp_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\norig = data = RREG32(HDP_HOST_PATH_CNTL);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_HDP_MGCG))\r\ndata &= ~CLOCK_GATING_DIS;\r\nelse\r\ndata |= CLOCK_GATING_DIS;\r\nif (orig != data)\r\nWREG32(HDP_HOST_PATH_CNTL, data);\r\n}\r\nstatic void si_enable_hdp_ls(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\norig = data = RREG32(HDP_MEM_POWER_LS);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_HDP_LS))\r\ndata |= HDP_LS_ENABLE;\r\nelse\r\ndata &= ~HDP_LS_ENABLE;\r\nif (orig != data)\r\nWREG32(HDP_MEM_POWER_LS, data);\r\n}\r\nstatic void si_update_cg(struct radeon_device *rdev,\r\nu32 block, bool enable)\r\n{\r\nif (block & RADEON_CG_BLOCK_GFX) {\r\nsi_enable_gui_idle_interrupt(rdev, false);\r\nif (enable) {\r\nsi_enable_mgcg(rdev, true);\r\nsi_enable_cgcg(rdev, true);\r\n} else {\r\nsi_enable_cgcg(rdev, false);\r\nsi_enable_mgcg(rdev, false);\r\n}\r\nsi_enable_gui_idle_interrupt(rdev, true);\r\n}\r\nif (block & RADEON_CG_BLOCK_MC) {\r\nsi_enable_mc_mgcg(rdev, enable);\r\nsi_enable_mc_ls(rdev, enable);\r\n}\r\nif (block & RADEON_CG_BLOCK_SDMA) {\r\nsi_enable_dma_mgcg(rdev, enable);\r\n}\r\nif (block & RADEON_CG_BLOCK_BIF) {\r\nsi_enable_bif_mgls(rdev, enable);\r\n}\r\nif (block & RADEON_CG_BLOCK_UVD) {\r\nif (rdev->has_uvd) {\r\nsi_enable_uvd_mgcg(rdev, enable);\r\n}\r\n}\r\nif (block & RADEON_CG_BLOCK_HDP) {\r\nsi_enable_hdp_mgcg(rdev, enable);\r\nsi_enable_hdp_ls(rdev, enable);\r\n}\r\n}\r\nstatic void si_init_cg(struct radeon_device *rdev)\r\n{\r\nsi_update_cg(rdev, (RADEON_CG_BLOCK_GFX |\r\nRADEON_CG_BLOCK_MC |\r\nRADEON_CG_BLOCK_SDMA |\r\nRADEON_CG_BLOCK_BIF |\r\nRADEON_CG_BLOCK_HDP), true);\r\nif (rdev->has_uvd) {\r\nsi_update_cg(rdev, RADEON_CG_BLOCK_UVD, true);\r\nsi_init_uvd_internal_cg(rdev);\r\n}\r\n}\r\nstatic void si_fini_cg(struct radeon_device *rdev)\r\n{\r\nif (rdev->has_uvd) {\r\nsi_update_cg(rdev, RADEON_CG_BLOCK_UVD, false);\r\n}\r\nsi_update_cg(rdev, (RADEON_CG_BLOCK_GFX |\r\nRADEON_CG_BLOCK_MC |\r\nRADEON_CG_BLOCK_SDMA |\r\nRADEON_CG_BLOCK_BIF |\r\nRADEON_CG_BLOCK_HDP), false);\r\n}\r\nu32 si_get_csb_size(struct radeon_device *rdev)\r\n{\r\nu32 count = 0;\r\nconst struct cs_section_def *sect = NULL;\r\nconst struct cs_extent_def *ext = NULL;\r\nif (rdev->rlc.cs_data == NULL)\r\nreturn 0;\r\ncount += 2;\r\ncount += 3;\r\nfor (sect = rdev->rlc.cs_data; sect->section != NULL; ++sect) {\r\nfor (ext = sect->section; ext->extent != NULL; ++ext) {\r\nif (sect->id == SECT_CONTEXT)\r\ncount += 2 + ext->reg_count;\r\nelse\r\nreturn 0;\r\n}\r\n}\r\ncount += 3;\r\ncount += 2;\r\ncount += 2;\r\nreturn count;\r\n}\r\nvoid si_get_csb_buffer(struct radeon_device *rdev, volatile u32 *buffer)\r\n{\r\nu32 count = 0, i;\r\nconst struct cs_section_def *sect = NULL;\r\nconst struct cs_extent_def *ext = NULL;\r\nif (rdev->rlc.cs_data == NULL)\r\nreturn;\r\nif (buffer == NULL)\r\nreturn;\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nbuffer[count++] = cpu_to_le32(PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_CONTEXT_CONTROL, 1));\r\nbuffer[count++] = cpu_to_le32(0x80000000);\r\nbuffer[count++] = cpu_to_le32(0x80000000);\r\nfor (sect = rdev->rlc.cs_data; sect->section != NULL; ++sect) {\r\nfor (ext = sect->section; ext->extent != NULL; ++ext) {\r\nif (sect->id == SECT_CONTEXT) {\r\nbuffer[count++] =\r\ncpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, ext->reg_count));\r\nbuffer[count++] = cpu_to_le32(ext->reg_index - 0xa000);\r\nfor (i = 0; i < ext->reg_count; i++)\r\nbuffer[count++] = cpu_to_le32(ext->extent[i]);\r\n} else {\r\nreturn;\r\n}\r\n}\r\n}\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, 1));\r\nbuffer[count++] = cpu_to_le32(PA_SC_RASTER_CONFIG - PACKET3_SET_CONTEXT_REG_START);\r\nswitch (rdev->family) {\r\ncase CHIP_TAHITI:\r\ncase CHIP_PITCAIRN:\r\nbuffer[count++] = cpu_to_le32(0x2a00126a);\r\nbreak;\r\ncase CHIP_VERDE:\r\nbuffer[count++] = cpu_to_le32(0x0000124a);\r\nbreak;\r\ncase CHIP_OLAND:\r\nbuffer[count++] = cpu_to_le32(0x00000082);\r\nbreak;\r\ncase CHIP_HAINAN:\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbreak;\r\ndefault:\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbreak;\r\n}\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nbuffer[count++] = cpu_to_le32(PACKET3_PREAMBLE_END_CLEAR_STATE);\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_CLEAR_STATE, 0));\r\nbuffer[count++] = cpu_to_le32(0);\r\n}\r\nstatic void si_init_pg(struct radeon_device *rdev)\r\n{\r\nif (rdev->pg_flags) {\r\nif (rdev->pg_flags & RADEON_PG_SUPPORT_SDMA) {\r\nsi_init_dma_pg(rdev);\r\n}\r\nsi_init_ao_cu_mask(rdev);\r\nif (rdev->pg_flags & RADEON_PG_SUPPORT_GFX_PG) {\r\nsi_init_gfx_cgpg(rdev);\r\n} else {\r\nWREG32(RLC_SAVE_AND_RESTORE_BASE, rdev->rlc.save_restore_gpu_addr >> 8);\r\nWREG32(RLC_CLEAR_STATE_RESTORE_BASE, rdev->rlc.clear_state_gpu_addr >> 8);\r\n}\r\nsi_enable_dma_pg(rdev, true);\r\nsi_enable_gfx_cgpg(rdev, true);\r\n} else {\r\nWREG32(RLC_SAVE_AND_RESTORE_BASE, rdev->rlc.save_restore_gpu_addr >> 8);\r\nWREG32(RLC_CLEAR_STATE_RESTORE_BASE, rdev->rlc.clear_state_gpu_addr >> 8);\r\n}\r\n}\r\nstatic void si_fini_pg(struct radeon_device *rdev)\r\n{\r\nif (rdev->pg_flags) {\r\nsi_enable_dma_pg(rdev, false);\r\nsi_enable_gfx_cgpg(rdev, false);\r\n}\r\n}\r\nvoid si_rlc_reset(struct radeon_device *rdev)\r\n{\r\nu32 tmp = RREG32(GRBM_SOFT_RESET);\r\ntmp |= SOFT_RESET_RLC;\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\nudelay(50);\r\ntmp &= ~SOFT_RESET_RLC;\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\nudelay(50);\r\n}\r\nstatic void si_rlc_stop(struct radeon_device *rdev)\r\n{\r\nWREG32(RLC_CNTL, 0);\r\nsi_enable_gui_idle_interrupt(rdev, false);\r\nsi_wait_for_rlc_serdes(rdev);\r\n}\r\nstatic void si_rlc_start(struct radeon_device *rdev)\r\n{\r\nWREG32(RLC_CNTL, RLC_ENABLE);\r\nsi_enable_gui_idle_interrupt(rdev, true);\r\nudelay(50);\r\n}\r\nstatic bool si_lbpw_supported(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(MC_SEQ_MISC0);\r\nif ((tmp & 0xF0000000) == 0xB0000000)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void si_enable_lbpw(struct radeon_device *rdev, bool enable)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(RLC_LB_CNTL);\r\nif (enable)\r\ntmp |= LOAD_BALANCE_ENABLE;\r\nelse\r\ntmp &= ~LOAD_BALANCE_ENABLE;\r\nWREG32(RLC_LB_CNTL, tmp);\r\nif (!enable) {\r\nsi_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nWREG32(SPI_LB_CU_MASK, 0x00ff);\r\n}\r\n}\r\nstatic int si_rlc_resume(struct radeon_device *rdev)\r\n{\r\nu32 i;\r\nif (!rdev->rlc_fw)\r\nreturn -EINVAL;\r\nsi_rlc_stop(rdev);\r\nsi_rlc_reset(rdev);\r\nsi_init_pg(rdev);\r\nsi_init_cg(rdev);\r\nWREG32(RLC_RL_BASE, 0);\r\nWREG32(RLC_RL_SIZE, 0);\r\nWREG32(RLC_LB_CNTL, 0);\r\nWREG32(RLC_LB_CNTR_MAX, 0xffffffff);\r\nWREG32(RLC_LB_CNTR_INIT, 0);\r\nWREG32(RLC_LB_INIT_CU_MASK, 0xffffffff);\r\nWREG32(RLC_MC_CNTL, 0);\r\nWREG32(RLC_UCODE_CNTL, 0);\r\nif (rdev->new_fw) {\r\nconst struct rlc_firmware_header_v1_0 *hdr =\r\n(const struct rlc_firmware_header_v1_0 *)rdev->rlc_fw->data;\r\nu32 fw_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\r\nconst __le32 *fw_data = (const __le32 *)\r\n(rdev->rlc_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\nradeon_ucode_print_rlc_hdr(&hdr->header);\r\nfor (i = 0; i < fw_size; i++) {\r\nWREG32(RLC_UCODE_ADDR, i);\r\nWREG32(RLC_UCODE_DATA, le32_to_cpup(fw_data++));\r\n}\r\n} else {\r\nconst __be32 *fw_data =\r\n(const __be32 *)rdev->rlc_fw->data;\r\nfor (i = 0; i < SI_RLC_UCODE_SIZE; i++) {\r\nWREG32(RLC_UCODE_ADDR, i);\r\nWREG32(RLC_UCODE_DATA, be32_to_cpup(fw_data++));\r\n}\r\n}\r\nWREG32(RLC_UCODE_ADDR, 0);\r\nsi_enable_lbpw(rdev, si_lbpw_supported(rdev));\r\nsi_rlc_start(rdev);\r\nreturn 0;\r\n}\r\nstatic void si_enable_interrupts(struct radeon_device *rdev)\r\n{\r\nu32 ih_cntl = RREG32(IH_CNTL);\r\nu32 ih_rb_cntl = RREG32(IH_RB_CNTL);\r\nih_cntl |= ENABLE_INTR;\r\nih_rb_cntl |= IH_RB_ENABLE;\r\nWREG32(IH_CNTL, ih_cntl);\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nrdev->ih.enabled = true;\r\n}\r\nstatic void si_disable_interrupts(struct radeon_device *rdev)\r\n{\r\nu32 ih_rb_cntl = RREG32(IH_RB_CNTL);\r\nu32 ih_cntl = RREG32(IH_CNTL);\r\nih_rb_cntl &= ~IH_RB_ENABLE;\r\nih_cntl &= ~ENABLE_INTR;\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nWREG32(IH_CNTL, ih_cntl);\r\nWREG32(IH_RB_RPTR, 0);\r\nWREG32(IH_RB_WPTR, 0);\r\nrdev->ih.enabled = false;\r\nrdev->ih.rptr = 0;\r\n}\r\nstatic void si_disable_interrupt_state(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(CP_INT_CNTL_RING0) &\r\n(CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\nWREG32(CP_INT_CNTL_RING0, tmp);\r\nWREG32(CP_INT_CNTL_RING1, 0);\r\nWREG32(CP_INT_CNTL_RING2, 0);\r\ntmp = RREG32(DMA_CNTL + DMA0_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\nWREG32(DMA_CNTL + DMA0_REGISTER_OFFSET, tmp);\r\ntmp = RREG32(DMA_CNTL + DMA1_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\nWREG32(DMA_CNTL + DMA1_REGISTER_OFFSET, tmp);\r\nWREG32(GRBM_INT_CNTL, 0);\r\nWREG32(SRBM_INT_CNTL, 0);\r\nif (rdev->num_crtc >= 2) {\r\nWREG32(INT_MASK + EVERGREEN_CRTC0_REGISTER_OFFSET, 0);\r\nWREG32(INT_MASK + EVERGREEN_CRTC1_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(INT_MASK + EVERGREEN_CRTC2_REGISTER_OFFSET, 0);\r\nWREG32(INT_MASK + EVERGREEN_CRTC3_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(INT_MASK + EVERGREEN_CRTC4_REGISTER_OFFSET, 0);\r\nWREG32(INT_MASK + EVERGREEN_CRTC5_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 2) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC0_REGISTER_OFFSET, 0);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC1_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC2_REGISTER_OFFSET, 0);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC3_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC4_REGISTER_OFFSET, 0);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC5_REGISTER_OFFSET, 0);\r\n}\r\nif (!ASIC_IS_NODCE(rdev)) {\r\nWREG32(DAC_AUTODETECT_INT_CONTROL, 0);\r\ntmp = RREG32(DC_HPD1_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD1_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD2_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD2_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD3_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD3_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD4_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD4_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD5_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD5_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD6_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD6_INT_CONTROL, tmp);\r\n}\r\n}\r\nstatic int si_irq_init(struct radeon_device *rdev)\r\n{\r\nint ret = 0;\r\nint rb_bufsz;\r\nu32 interrupt_cntl, ih_cntl, ih_rb_cntl;\r\nret = r600_ih_ring_alloc(rdev);\r\nif (ret)\r\nreturn ret;\r\nsi_disable_interrupts(rdev);\r\nret = si_rlc_resume(rdev);\r\nif (ret) {\r\nr600_ih_ring_fini(rdev);\r\nreturn ret;\r\n}\r\nWREG32(INTERRUPT_CNTL2, rdev->ih.gpu_addr >> 8);\r\ninterrupt_cntl = RREG32(INTERRUPT_CNTL);\r\ninterrupt_cntl &= ~IH_DUMMY_RD_OVERRIDE;\r\ninterrupt_cntl &= ~IH_REQ_NONSNOOP_EN;\r\nWREG32(INTERRUPT_CNTL, interrupt_cntl);\r\nWREG32(IH_RB_BASE, rdev->ih.gpu_addr >> 8);\r\nrb_bufsz = order_base_2(rdev->ih.ring_size / 4);\r\nih_rb_cntl = (IH_WPTR_OVERFLOW_ENABLE |\r\nIH_WPTR_OVERFLOW_CLEAR |\r\n(rb_bufsz << 1));\r\nif (rdev->wb.enabled)\r\nih_rb_cntl |= IH_WPTR_WRITEBACK_ENABLE;\r\nWREG32(IH_RB_WPTR_ADDR_LO, (rdev->wb.gpu_addr + R600_WB_IH_WPTR_OFFSET) & 0xFFFFFFFC);\r\nWREG32(IH_RB_WPTR_ADDR_HI, upper_32_bits(rdev->wb.gpu_addr + R600_WB_IH_WPTR_OFFSET) & 0xFF);\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nWREG32(IH_RB_RPTR, 0);\r\nWREG32(IH_RB_WPTR, 0);\r\nih_cntl = MC_WRREQ_CREDIT(0x10) | MC_WR_CLEAN_CNT(0x10) | MC_VMID(0);\r\nif (rdev->msi_enabled)\r\nih_cntl |= RPTR_REARM;\r\nWREG32(IH_CNTL, ih_cntl);\r\nsi_disable_interrupt_state(rdev);\r\npci_set_master(rdev->pdev);\r\nsi_enable_interrupts(rdev);\r\nreturn ret;\r\n}\r\nint si_irq_set(struct radeon_device *rdev)\r\n{\r\nu32 cp_int_cntl;\r\nu32 cp_int_cntl1 = 0, cp_int_cntl2 = 0;\r\nu32 crtc1 = 0, crtc2 = 0, crtc3 = 0, crtc4 = 0, crtc5 = 0, crtc6 = 0;\r\nu32 hpd1 = 0, hpd2 = 0, hpd3 = 0, hpd4 = 0, hpd5 = 0, hpd6 = 0;\r\nu32 grbm_int_cntl = 0;\r\nu32 dma_cntl, dma_cntl1;\r\nu32 thermal_int = 0;\r\nif (!rdev->irq.installed) {\r\nWARN(1, "Can't enable IRQ/MSI because no handler is installed\n");\r\nreturn -EINVAL;\r\n}\r\nif (!rdev->ih.enabled) {\r\nsi_disable_interrupts(rdev);\r\nsi_disable_interrupt_state(rdev);\r\nreturn 0;\r\n}\r\ncp_int_cntl = RREG32(CP_INT_CNTL_RING0) &\r\n(CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\nif (!ASIC_IS_NODCE(rdev)) {\r\nhpd1 = RREG32(DC_HPD1_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd2 = RREG32(DC_HPD2_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd3 = RREG32(DC_HPD3_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd4 = RREG32(DC_HPD4_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd5 = RREG32(DC_HPD5_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd6 = RREG32(DC_HPD6_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\n}\r\ndma_cntl = RREG32(DMA_CNTL + DMA0_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\ndma_cntl1 = RREG32(DMA_CNTL + DMA1_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\nthermal_int = RREG32(CG_THERMAL_INT) &\r\n~(THERM_INT_MASK_HIGH | THERM_INT_MASK_LOW);\r\nif (atomic_read(&rdev->irq.ring_int[RADEON_RING_TYPE_GFX_INDEX])) {\r\nDRM_DEBUG("si_irq_set: sw int gfx\n");\r\ncp_int_cntl |= TIME_STAMP_INT_ENABLE;\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[CAYMAN_RING_TYPE_CP1_INDEX])) {\r\nDRM_DEBUG("si_irq_set: sw int cp1\n");\r\ncp_int_cntl1 |= TIME_STAMP_INT_ENABLE;\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[CAYMAN_RING_TYPE_CP2_INDEX])) {\r\nDRM_DEBUG("si_irq_set: sw int cp2\n");\r\ncp_int_cntl2 |= TIME_STAMP_INT_ENABLE;\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[R600_RING_TYPE_DMA_INDEX])) {\r\nDRM_DEBUG("si_irq_set: sw int dma\n");\r\ndma_cntl |= TRAP_ENABLE;\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[CAYMAN_RING_TYPE_DMA1_INDEX])) {\r\nDRM_DEBUG("si_irq_set: sw int dma1\n");\r\ndma_cntl1 |= TRAP_ENABLE;\r\n}\r\nif (rdev->irq.crtc_vblank_int[0] ||\r\natomic_read(&rdev->irq.pflip[0])) {\r\nDRM_DEBUG("si_irq_set: vblank 0\n");\r\ncrtc1 |= VBLANK_INT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[1] ||\r\natomic_read(&rdev->irq.pflip[1])) {\r\nDRM_DEBUG("si_irq_set: vblank 1\n");\r\ncrtc2 |= VBLANK_INT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[2] ||\r\natomic_read(&rdev->irq.pflip[2])) {\r\nDRM_DEBUG("si_irq_set: vblank 2\n");\r\ncrtc3 |= VBLANK_INT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[3] ||\r\natomic_read(&rdev->irq.pflip[3])) {\r\nDRM_DEBUG("si_irq_set: vblank 3\n");\r\ncrtc4 |= VBLANK_INT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[4] ||\r\natomic_read(&rdev->irq.pflip[4])) {\r\nDRM_DEBUG("si_irq_set: vblank 4\n");\r\ncrtc5 |= VBLANK_INT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[5] ||\r\natomic_read(&rdev->irq.pflip[5])) {\r\nDRM_DEBUG("si_irq_set: vblank 5\n");\r\ncrtc6 |= VBLANK_INT_MASK;\r\n}\r\nif (rdev->irq.hpd[0]) {\r\nDRM_DEBUG("si_irq_set: hpd 1\n");\r\nhpd1 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[1]) {\r\nDRM_DEBUG("si_irq_set: hpd 2\n");\r\nhpd2 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[2]) {\r\nDRM_DEBUG("si_irq_set: hpd 3\n");\r\nhpd3 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[3]) {\r\nDRM_DEBUG("si_irq_set: hpd 4\n");\r\nhpd4 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[4]) {\r\nDRM_DEBUG("si_irq_set: hpd 5\n");\r\nhpd5 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[5]) {\r\nDRM_DEBUG("si_irq_set: hpd 6\n");\r\nhpd6 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nWREG32(CP_INT_CNTL_RING0, cp_int_cntl);\r\nWREG32(CP_INT_CNTL_RING1, cp_int_cntl1);\r\nWREG32(CP_INT_CNTL_RING2, cp_int_cntl2);\r\nWREG32(DMA_CNTL + DMA0_REGISTER_OFFSET, dma_cntl);\r\nWREG32(DMA_CNTL + DMA1_REGISTER_OFFSET, dma_cntl1);\r\nWREG32(GRBM_INT_CNTL, grbm_int_cntl);\r\nif (rdev->irq.dpm_thermal) {\r\nDRM_DEBUG("dpm thermal\n");\r\nthermal_int |= THERM_INT_MASK_HIGH | THERM_INT_MASK_LOW;\r\n}\r\nif (rdev->num_crtc >= 2) {\r\nWREG32(INT_MASK + EVERGREEN_CRTC0_REGISTER_OFFSET, crtc1);\r\nWREG32(INT_MASK + EVERGREEN_CRTC1_REGISTER_OFFSET, crtc2);\r\n}\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(INT_MASK + EVERGREEN_CRTC2_REGISTER_OFFSET, crtc3);\r\nWREG32(INT_MASK + EVERGREEN_CRTC3_REGISTER_OFFSET, crtc4);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(INT_MASK + EVERGREEN_CRTC4_REGISTER_OFFSET, crtc5);\r\nWREG32(INT_MASK + EVERGREEN_CRTC5_REGISTER_OFFSET, crtc6);\r\n}\r\nif (rdev->num_crtc >= 2) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC0_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC1_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\n}\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC2_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC3_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC4_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC5_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\n}\r\nif (!ASIC_IS_NODCE(rdev)) {\r\nWREG32(DC_HPD1_INT_CONTROL, hpd1);\r\nWREG32(DC_HPD2_INT_CONTROL, hpd2);\r\nWREG32(DC_HPD3_INT_CONTROL, hpd3);\r\nWREG32(DC_HPD4_INT_CONTROL, hpd4);\r\nWREG32(DC_HPD5_INT_CONTROL, hpd5);\r\nWREG32(DC_HPD6_INT_CONTROL, hpd6);\r\n}\r\nWREG32(CG_THERMAL_INT, thermal_int);\r\nRREG32(SRBM_STATUS);\r\nreturn 0;\r\n}\r\nstatic inline void si_irq_ack(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\nif (ASIC_IS_NODCE(rdev))\r\nreturn;\r\nrdev->irq.stat_regs.evergreen.disp_int = RREG32(DISP_INTERRUPT_STATUS);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont = RREG32(DISP_INTERRUPT_STATUS_CONTINUE);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont2 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE2);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont3 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE3);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont4 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE4);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont5 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE5);\r\nrdev->irq.stat_regs.evergreen.d1grph_int = RREG32(GRPH_INT_STATUS + EVERGREEN_CRTC0_REGISTER_OFFSET);\r\nrdev->irq.stat_regs.evergreen.d2grph_int = RREG32(GRPH_INT_STATUS + EVERGREEN_CRTC1_REGISTER_OFFSET);\r\nif (rdev->num_crtc >= 4) {\r\nrdev->irq.stat_regs.evergreen.d3grph_int = RREG32(GRPH_INT_STATUS + EVERGREEN_CRTC2_REGISTER_OFFSET);\r\nrdev->irq.stat_regs.evergreen.d4grph_int = RREG32(GRPH_INT_STATUS + EVERGREEN_CRTC3_REGISTER_OFFSET);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nrdev->irq.stat_regs.evergreen.d5grph_int = RREG32(GRPH_INT_STATUS + EVERGREEN_CRTC4_REGISTER_OFFSET);\r\nrdev->irq.stat_regs.evergreen.d6grph_int = RREG32(GRPH_INT_STATUS + EVERGREEN_CRTC5_REGISTER_OFFSET);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.d1grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC0_REGISTER_OFFSET, GRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.evergreen.d2grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC1_REGISTER_OFFSET, GRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.evergreen.disp_int & LB_D1_VBLANK_INTERRUPT)\r\nWREG32(VBLANK_STATUS + EVERGREEN_CRTC0_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int & LB_D1_VLINE_INTERRUPT)\r\nWREG32(VLINE_STATUS + EVERGREEN_CRTC0_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont & LB_D2_VBLANK_INTERRUPT)\r\nWREG32(VBLANK_STATUS + EVERGREEN_CRTC1_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont & LB_D2_VLINE_INTERRUPT)\r\nWREG32(VLINE_STATUS + EVERGREEN_CRTC1_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->num_crtc >= 4) {\r\nif (rdev->irq.stat_regs.evergreen.d3grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC2_REGISTER_OFFSET, GRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.evergreen.d4grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC3_REGISTER_OFFSET, GRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont2 & LB_D3_VBLANK_INTERRUPT)\r\nWREG32(VBLANK_STATUS + EVERGREEN_CRTC2_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont2 & LB_D3_VLINE_INTERRUPT)\r\nWREG32(VLINE_STATUS + EVERGREEN_CRTC2_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont3 & LB_D4_VBLANK_INTERRUPT)\r\nWREG32(VBLANK_STATUS + EVERGREEN_CRTC3_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont3 & LB_D4_VLINE_INTERRUPT)\r\nWREG32(VLINE_STATUS + EVERGREEN_CRTC3_REGISTER_OFFSET, VLINE_ACK);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nif (rdev->irq.stat_regs.evergreen.d5grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC4_REGISTER_OFFSET, GRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.evergreen.d6grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC5_REGISTER_OFFSET, GRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont4 & LB_D5_VBLANK_INTERRUPT)\r\nWREG32(VBLANK_STATUS + EVERGREEN_CRTC4_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont4 & LB_D5_VLINE_INTERRUPT)\r\nWREG32(VLINE_STATUS + EVERGREEN_CRTC4_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont5 & LB_D6_VBLANK_INTERRUPT)\r\nWREG32(VBLANK_STATUS + EVERGREEN_CRTC5_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont5 & LB_D6_VLINE_INTERRUPT)\r\nWREG32(VLINE_STATUS + EVERGREEN_CRTC5_REGISTER_OFFSET, VLINE_ACK);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int & DC_HPD1_INTERRUPT) {\r\ntmp = RREG32(DC_HPD1_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD1_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont & DC_HPD2_INTERRUPT) {\r\ntmp = RREG32(DC_HPD2_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD2_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont2 & DC_HPD3_INTERRUPT) {\r\ntmp = RREG32(DC_HPD3_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD3_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont3 & DC_HPD4_INTERRUPT) {\r\ntmp = RREG32(DC_HPD4_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD4_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont4 & DC_HPD5_INTERRUPT) {\r\ntmp = RREG32(DC_HPD5_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD5_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont5 & DC_HPD6_INTERRUPT) {\r\ntmp = RREG32(DC_HPD5_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD6_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int & DC_HPD1_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD1_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD1_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont & DC_HPD2_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD2_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD2_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont2 & DC_HPD3_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD3_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD3_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont3 & DC_HPD4_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD4_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD4_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont4 & DC_HPD5_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD5_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD5_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.evergreen.disp_int_cont5 & DC_HPD6_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD5_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD6_INT_CONTROL, tmp);\r\n}\r\n}\r\nstatic void si_irq_disable(struct radeon_device *rdev)\r\n{\r\nsi_disable_interrupts(rdev);\r\nmdelay(1);\r\nsi_irq_ack(rdev);\r\nsi_disable_interrupt_state(rdev);\r\n}\r\nstatic void si_irq_suspend(struct radeon_device *rdev)\r\n{\r\nsi_irq_disable(rdev);\r\nsi_rlc_stop(rdev);\r\n}\r\nstatic void si_irq_fini(struct radeon_device *rdev)\r\n{\r\nsi_irq_suspend(rdev);\r\nr600_ih_ring_fini(rdev);\r\n}\r\nstatic inline u32 si_get_ih_wptr(struct radeon_device *rdev)\r\n{\r\nu32 wptr, tmp;\r\nif (rdev->wb.enabled)\r\nwptr = le32_to_cpu(rdev->wb.wb[R600_WB_IH_WPTR_OFFSET/4]);\r\nelse\r\nwptr = RREG32(IH_RB_WPTR);\r\nif (wptr & RB_OVERFLOW) {\r\nwptr &= ~RB_OVERFLOW;\r\ndev_warn(rdev->dev, "IH ring buffer overflow (0x%08X, 0x%08X, 0x%08X)\n",\r\nwptr, rdev->ih.rptr, (wptr + 16) & rdev->ih.ptr_mask);\r\nrdev->ih.rptr = (wptr + 16) & rdev->ih.ptr_mask;\r\ntmp = RREG32(IH_RB_CNTL);\r\ntmp |= IH_WPTR_OVERFLOW_CLEAR;\r\nWREG32(IH_RB_CNTL, tmp);\r\n}\r\nreturn (wptr & rdev->ih.ptr_mask);\r\n}\r\nint si_irq_process(struct radeon_device *rdev)\r\n{\r\nu32 wptr;\r\nu32 rptr;\r\nu32 src_id, src_data, ring_id;\r\nu32 ring_index;\r\nbool queue_hotplug = false;\r\nbool queue_dp = false;\r\nbool queue_thermal = false;\r\nu32 status, addr;\r\nif (!rdev->ih.enabled || rdev->shutdown)\r\nreturn IRQ_NONE;\r\nwptr = si_get_ih_wptr(rdev);\r\nrestart_ih:\r\nif (atomic_xchg(&rdev->ih.lock, 1))\r\nreturn IRQ_NONE;\r\nrptr = rdev->ih.rptr;\r\nDRM_DEBUG("si_irq_process start: rptr %d, wptr %d\n", rptr, wptr);\r\nrmb();\r\nsi_irq_ack(rdev);\r\nwhile (rptr != wptr) {\r\nring_index = rptr / 4;\r\nsrc_id = le32_to_cpu(rdev->ih.ring[ring_index]) & 0xff;\r\nsrc_data = le32_to_cpu(rdev->ih.ring[ring_index + 1]) & 0xfffffff;\r\nring_id = le32_to_cpu(rdev->ih.ring[ring_index + 2]) & 0xff;\r\nswitch (src_id) {\r\ncase 1:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int & LB_D1_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[0]) {\r\ndrm_handle_vblank(rdev->ddev, 0);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[0]))\r\nradeon_crtc_handle_vblank(rdev, 0);\r\nrdev->irq.stat_regs.evergreen.disp_int &= ~LB_D1_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D1 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int & LB_D1_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int &= ~LB_D1_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D1 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 2:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont & LB_D2_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[1]) {\r\ndrm_handle_vblank(rdev->ddev, 1);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[1]))\r\nradeon_crtc_handle_vblank(rdev, 1);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont &= ~LB_D2_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D2 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont & LB_D2_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont &= ~LB_D2_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D2 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 3:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont2 & LB_D3_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[2]) {\r\ndrm_handle_vblank(rdev->ddev, 2);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[2]))\r\nradeon_crtc_handle_vblank(rdev, 2);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont2 &= ~LB_D3_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D3 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont2 & LB_D3_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont2 &= ~LB_D3_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D3 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 4:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont3 & LB_D4_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[3]) {\r\ndrm_handle_vblank(rdev->ddev, 3);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[3]))\r\nradeon_crtc_handle_vblank(rdev, 3);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont3 &= ~LB_D4_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D4 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont3 & LB_D4_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont3 &= ~LB_D4_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D4 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 5:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont4 & LB_D5_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[4]) {\r\ndrm_handle_vblank(rdev->ddev, 4);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[4]))\r\nradeon_crtc_handle_vblank(rdev, 4);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont4 &= ~LB_D5_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D5 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont4 & LB_D5_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont4 &= ~LB_D5_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D5 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 6:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont5 & LB_D6_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[5]) {\r\ndrm_handle_vblank(rdev->ddev, 5);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[5]))\r\nradeon_crtc_handle_vblank(rdev, 5);\r\nrdev->irq.stat_regs.evergreen.disp_int_cont5 &= ~LB_D6_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D6 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont5 & LB_D6_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont5 &= ~LB_D6_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D6 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 8:\r\ncase 10:\r\ncase 12:\r\ncase 14:\r\ncase 16:\r\ncase 18:\r\nDRM_DEBUG("IH: D%d flip\n", ((src_id - 8) >> 1) + 1);\r\nif (radeon_use_pflipirq > 0)\r\nradeon_crtc_handle_flip(rdev, (src_id - 8) >> 1);\r\nbreak;\r\ncase 42:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int & DC_HPD1_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int &= ~DC_HPD1_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD1\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont & DC_HPD2_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont &= ~DC_HPD2_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD2\n");\r\nbreak;\r\ncase 2:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont2 & DC_HPD3_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont2 &= ~DC_HPD3_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD3\n");\r\nbreak;\r\ncase 3:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont3 & DC_HPD4_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont3 &= ~DC_HPD4_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD4\n");\r\nbreak;\r\ncase 4:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont4 & DC_HPD5_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont4 &= ~DC_HPD5_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD5\n");\r\nbreak;\r\ncase 5:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont5 & DC_HPD6_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont5 &= ~DC_HPD6_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD6\n");\r\nbreak;\r\ncase 6:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int & DC_HPD1_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int &= ~DC_HPD1_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 1\n");\r\nbreak;\r\ncase 7:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont & DC_HPD2_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont &= ~DC_HPD2_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 2\n");\r\nbreak;\r\ncase 8:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont2 & DC_HPD3_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont2 &= ~DC_HPD3_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 3\n");\r\nbreak;\r\ncase 9:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont3 & DC_HPD4_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont3 &= ~DC_HPD4_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 4\n");\r\nbreak;\r\ncase 10:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont4 & DC_HPD5_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont4 &= ~DC_HPD5_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 5\n");\r\nbreak;\r\ncase 11:\r\nif (!(rdev->irq.stat_regs.evergreen.disp_int_cont5 & DC_HPD6_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.evergreen.disp_int_cont5 &= ~DC_HPD6_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 6\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 96:\r\nDRM_ERROR("SRBM_READ_ERROR: 0x%x\n", RREG32(SRBM_READ_ERROR));\r\nWREG32(SRBM_INT_ACK, 0x1);\r\nbreak;\r\ncase 124:\r\nDRM_DEBUG("IH: UVD int: 0x%08x\n", src_data);\r\nradeon_fence_process(rdev, R600_RING_TYPE_UVD_INDEX);\r\nbreak;\r\ncase 146:\r\ncase 147:\r\naddr = RREG32(VM_CONTEXT1_PROTECTION_FAULT_ADDR);\r\nstatus = RREG32(VM_CONTEXT1_PROTECTION_FAULT_STATUS);\r\nWREG32_P(VM_CONTEXT1_CNTL2, 1, ~1);\r\nif (addr == 0x0 && status == 0x0)\r\nbreak;\r\ndev_err(rdev->dev, "GPU fault detected: %d 0x%08x\n", src_id, src_data);\r\ndev_err(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_ADDR 0x%08X\n",\r\naddr);\r\ndev_err(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",\r\nstatus);\r\nsi_vm_decode_fault(rdev, status, addr);\r\nbreak;\r\ncase 176:\r\nradeon_fence_process(rdev, RADEON_RING_TYPE_GFX_INDEX);\r\nbreak;\r\ncase 177:\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_CP1_INDEX);\r\nbreak;\r\ncase 178:\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_CP2_INDEX);\r\nbreak;\r\ncase 181:\r\nDRM_DEBUG("IH: CP EOP\n");\r\nswitch (ring_id) {\r\ncase 0:\r\nradeon_fence_process(rdev, RADEON_RING_TYPE_GFX_INDEX);\r\nbreak;\r\ncase 1:\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_CP1_INDEX);\r\nbreak;\r\ncase 2:\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_CP2_INDEX);\r\nbreak;\r\n}\r\nbreak;\r\ncase 224:\r\nDRM_DEBUG("IH: DMA trap\n");\r\nradeon_fence_process(rdev, R600_RING_TYPE_DMA_INDEX);\r\nbreak;\r\ncase 230:\r\nDRM_DEBUG("IH: thermal low to high\n");\r\nrdev->pm.dpm.thermal.high_to_low = false;\r\nqueue_thermal = true;\r\nbreak;\r\ncase 231:\r\nDRM_DEBUG("IH: thermal high to low\n");\r\nrdev->pm.dpm.thermal.high_to_low = true;\r\nqueue_thermal = true;\r\nbreak;\r\ncase 233:\r\nDRM_DEBUG("IH: GUI idle\n");\r\nbreak;\r\ncase 244:\r\nDRM_DEBUG("IH: DMA1 trap\n");\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_DMA1_INDEX);\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nrptr += 16;\r\nrptr &= rdev->ih.ptr_mask;\r\nWREG32(IH_RB_RPTR, rptr);\r\n}\r\nif (queue_dp)\r\nschedule_work(&rdev->dp_work);\r\nif (queue_hotplug)\r\nschedule_delayed_work(&rdev->hotplug_work, 0);\r\nif (queue_thermal && rdev->pm.dpm_enabled)\r\nschedule_work(&rdev->pm.dpm.thermal.work);\r\nrdev->ih.rptr = rptr;\r\natomic_set(&rdev->ih.lock, 0);\r\nwptr = si_get_ih_wptr(rdev);\r\nif (wptr != rptr)\r\ngoto restart_ih;\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void si_uvd_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_uvd)\r\nreturn;\r\nr = radeon_uvd_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed UVD (%d) init.\n", r);\r\nrdev->has_uvd = 0;\r\nreturn;\r\n}\r\nrdev->ring[R600_RING_TYPE_UVD_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[R600_RING_TYPE_UVD_INDEX], 4096);\r\n}\r\nstatic void si_uvd_start(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_uvd)\r\nreturn;\r\nr = uvd_v2_2_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed UVD resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, R600_RING_TYPE_UVD_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD fences (%d).\n", r);\r\ngoto error;\r\n}\r\nreturn;\r\nerror:\r\nrdev->ring[R600_RING_TYPE_UVD_INDEX].ring_size = 0;\r\n}\r\nstatic void si_uvd_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nif (!rdev->has_uvd || !rdev->ring[R600_RING_TYPE_UVD_INDEX].ring_size)\r\nreturn;\r\nring = &rdev->ring[R600_RING_TYPE_UVD_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, PACKET0(UVD_NO_OP, 0));\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD ring (%d).\n", r);\r\nreturn;\r\n}\r\nr = uvd_v1_0_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD (%d).\n", r);\r\nreturn;\r\n}\r\n}\r\nstatic void si_vce_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_vce)\r\nreturn;\r\nr = radeon_vce_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE (%d) init.\n", r);\r\nrdev->has_vce = 0;\r\nreturn;\r\n}\r\nrdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[TN_RING_TYPE_VCE1_INDEX], 4096);\r\nrdev->ring[TN_RING_TYPE_VCE2_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[TN_RING_TYPE_VCE2_INDEX], 4096);\r\n}\r\nstatic void si_vce_start(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_vce)\r\nreturn;\r\nr = radeon_vce_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = vce_v1_0_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, TN_RING_TYPE_VCE1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 fences (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, TN_RING_TYPE_VCE2_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE2 fences (%d).\n", r);\r\ngoto error;\r\n}\r\nreturn;\r\nerror:\r\nrdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_size = 0;\r\nrdev->ring[TN_RING_TYPE_VCE2_INDEX].ring_size = 0;\r\n}\r\nstatic void si_vce_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nif (!rdev->has_vce || !rdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_size)\r\nreturn;\r\nring = &rdev->ring[TN_RING_TYPE_VCE1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, VCE_CMD_NO_OP);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 ring (%d).\n", r);\r\nreturn;\r\n}\r\nring = &rdev->ring[TN_RING_TYPE_VCE2_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, VCE_CMD_NO_OP);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 ring (%d).\n", r);\r\nreturn;\r\n}\r\nr = vce_v1_0_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE (%d).\n", r);\r\nreturn;\r\n}\r\n}\r\nstatic int si_startup(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nsi_pcie_gen3_enable(rdev);\r\nsi_program_aspm(rdev);\r\nr = r600_vram_scratch_init(rdev);\r\nif (r)\r\nreturn r;\r\nsi_mc_program(rdev);\r\nif (!rdev->pm.dpm_enabled) {\r\nr = si_mc_load_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load MC firmware!\n");\r\nreturn r;\r\n}\r\n}\r\nr = si_pcie_gart_enable(rdev);\r\nif (r)\r\nreturn r;\r\nsi_gpu_init(rdev);\r\nif (rdev->family == CHIP_VERDE) {\r\nrdev->rlc.reg_list = verde_rlc_save_restore_register_list;\r\nrdev->rlc.reg_list_size =\r\n(u32)ARRAY_SIZE(verde_rlc_save_restore_register_list);\r\n}\r\nrdev->rlc.cs_data = si_cs_data;\r\nr = sumo_rlc_init(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to init rlc BOs!\n");\r\nreturn r;\r\n}\r\nr = radeon_wb_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = radeon_fence_driver_start_ring(rdev, RADEON_RING_TYPE_GFX_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_CP1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_CP2_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, R600_RING_TYPE_DMA_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing DMA fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_DMA1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing DMA fences (%d).\n", r);\r\nreturn r;\r\n}\r\nsi_uvd_start(rdev);\r\nsi_vce_start(rdev);\r\nif (!rdev->irq.installed) {\r\nr = radeon_irq_kms_init(rdev);\r\nif (r)\r\nreturn r;\r\n}\r\nr = si_irq_init(rdev);\r\nif (r) {\r\nDRM_ERROR("radeon: IH init failed (%d).\n", r);\r\nradeon_irq_kms_fini(rdev);\r\nreturn r;\r\n}\r\nsi_irq_set(rdev);\r\nring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, RADEON_WB_CP_RPTR_OFFSET,\r\nRADEON_CP_PACKET2);\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, RADEON_WB_CP1_RPTR_OFFSET,\r\nRADEON_CP_PACKET2);\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, RADEON_WB_CP2_RPTR_OFFSET,\r\nRADEON_CP_PACKET2);\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, R600_WB_DMA_RPTR_OFFSET,\r\nDMA_PACKET(DMA_PACKET_NOP, 0, 0, 0, 0));\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, CAYMAN_WB_DMA1_RPTR_OFFSET,\r\nDMA_PACKET(DMA_PACKET_NOP, 0, 0, 0, 0));\r\nif (r)\r\nreturn r;\r\nr = si_cp_load_microcode(rdev);\r\nif (r)\r\nreturn r;\r\nr = si_cp_resume(rdev);\r\nif (r)\r\nreturn r;\r\nr = cayman_dma_resume(rdev);\r\nif (r)\r\nreturn r;\r\nsi_uvd_resume(rdev);\r\nsi_vce_resume(rdev);\r\nr = radeon_ib_pool_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "IB initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_vm_manager_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "vm manager initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_audio_init(rdev);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nint si_resume(struct radeon_device *rdev)\r\n{\r\nint r;\r\natom_asic_init(rdev->mode_info.atom_context);\r\nsi_init_golden_registers(rdev);\r\nif (rdev->pm.pm_method == PM_METHOD_DPM)\r\nradeon_pm_resume(rdev);\r\nrdev->accel_working = true;\r\nr = si_startup(rdev);\r\nif (r) {\r\nDRM_ERROR("si startup failed on resume\n");\r\nrdev->accel_working = false;\r\nreturn r;\r\n}\r\nreturn r;\r\n}\r\nint si_suspend(struct radeon_device *rdev)\r\n{\r\nradeon_pm_suspend(rdev);\r\nradeon_audio_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\nsi_cp_enable(rdev, false);\r\ncayman_dma_stop(rdev);\r\nif (rdev->has_uvd) {\r\nuvd_v1_0_fini(rdev);\r\nradeon_uvd_suspend(rdev);\r\n}\r\nif (rdev->has_vce)\r\nradeon_vce_suspend(rdev);\r\nsi_fini_pg(rdev);\r\nsi_fini_cg(rdev);\r\nsi_irq_suspend(rdev);\r\nradeon_wb_disable(rdev);\r\nsi_pcie_gart_disable(rdev);\r\nreturn 0;\r\n}\r\nint si_init(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nint r;\r\nif (!radeon_get_bios(rdev)) {\r\nif (ASIC_IS_AVIVO(rdev))\r\nreturn -EINVAL;\r\n}\r\nif (!rdev->is_atom_bios) {\r\ndev_err(rdev->dev, "Expecting atombios for cayman GPU\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_atombios_init(rdev);\r\nif (r)\r\nreturn r;\r\nif (!radeon_card_posted(rdev)) {\r\nif (!rdev->bios) {\r\ndev_err(rdev->dev, "Card not posted and no BIOS - ignoring\n");\r\nreturn -EINVAL;\r\n}\r\nDRM_INFO("GPU not posted. posting now...\n");\r\natom_asic_init(rdev->mode_info.atom_context);\r\n}\r\nsi_init_golden_registers(rdev);\r\nsi_scratch_init(rdev);\r\nradeon_surface_init(rdev);\r\nradeon_get_clock_info(rdev->ddev);\r\nr = radeon_fence_driver_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = si_mc_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = radeon_bo_init(rdev);\r\nif (r)\r\nreturn r;\r\nif (!rdev->me_fw || !rdev->pfp_fw || !rdev->ce_fw ||\r\n!rdev->rlc_fw || !rdev->mc_fw) {\r\nr = si_init_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load firmware!\n");\r\nreturn r;\r\n}\r\n}\r\nradeon_pm_init(rdev);\r\nring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 1024 * 1024);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 1024 * 1024);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 1024 * 1024);\r\nring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 64 * 1024);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 64 * 1024);\r\nsi_uvd_init(rdev);\r\nsi_vce_init(rdev);\r\nrdev->ih.ring_obj = NULL;\r\nr600_ih_ring_init(rdev, 64 * 1024);\r\nr = r600_pcie_gart_init(rdev);\r\nif (r)\r\nreturn r;\r\nrdev->accel_working = true;\r\nr = si_startup(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "disabling GPU acceleration\n");\r\nsi_cp_fini(rdev);\r\ncayman_dma_fini(rdev);\r\nsi_irq_fini(rdev);\r\nsumo_rlc_fini(rdev);\r\nradeon_wb_fini(rdev);\r\nradeon_ib_pool_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\nradeon_irq_kms_fini(rdev);\r\nsi_pcie_gart_fini(rdev);\r\nrdev->accel_working = false;\r\n}\r\nif (!rdev->mc_fw) {\r\nDRM_ERROR("radeon: MC ucode required for NI+.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid si_fini(struct radeon_device *rdev)\r\n{\r\nradeon_pm_fini(rdev);\r\nsi_cp_fini(rdev);\r\ncayman_dma_fini(rdev);\r\nsi_fini_pg(rdev);\r\nsi_fini_cg(rdev);\r\nsi_irq_fini(rdev);\r\nsumo_rlc_fini(rdev);\r\nradeon_wb_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\nradeon_ib_pool_fini(rdev);\r\nradeon_irq_kms_fini(rdev);\r\nif (rdev->has_uvd) {\r\nuvd_v1_0_fini(rdev);\r\nradeon_uvd_fini(rdev);\r\n}\r\nif (rdev->has_vce)\r\nradeon_vce_fini(rdev);\r\nsi_pcie_gart_fini(rdev);\r\nr600_vram_scratch_fini(rdev);\r\nradeon_gem_fini(rdev);\r\nradeon_fence_driver_fini(rdev);\r\nradeon_bo_fini(rdev);\r\nradeon_atombios_fini(rdev);\r\nkfree(rdev->bios);\r\nrdev->bios = NULL;\r\n}\r\nuint64_t si_get_gpu_clock_counter(struct radeon_device *rdev)\r\n{\r\nuint64_t clock;\r\nmutex_lock(&rdev->gpu_clock_mutex);\r\nWREG32(RLC_CAPTURE_GPU_CLOCK_COUNT, 1);\r\nclock = (uint64_t)RREG32(RLC_GPU_CLOCK_COUNT_LSB) |\r\n((uint64_t)RREG32(RLC_GPU_CLOCK_COUNT_MSB) << 32ULL);\r\nmutex_unlock(&rdev->gpu_clock_mutex);\r\nreturn clock;\r\n}\r\nint si_set_uvd_clocks(struct radeon_device *rdev, u32 vclk, u32 dclk)\r\n{\r\nunsigned fb_div = 0, vclk_div = 0, dclk_div = 0;\r\nint r;\r\nWREG32_P(CG_UPLL_FUNC_CNTL_2,\r\nVCLK_SRC_SEL(1) | DCLK_SRC_SEL(1),\r\n~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\r\nWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_BYPASS_EN_MASK, ~UPLL_BYPASS_EN_MASK);\r\nif (!vclk || !dclk) {\r\nreturn 0;\r\n}\r\nr = radeon_uvd_calc_upll_dividers(rdev, vclk, dclk, 125000, 250000,\r\n16384, 0x03FFFFFF, 0, 128, 5,\r\n&fb_div, &vclk_div, &dclk_div);\r\nif (r)\r\nreturn r;\r\nWREG32_P(CG_UPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\r\nWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_VCO_MODE_MASK, ~UPLL_VCO_MODE_MASK);\r\nWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_SLEEP_MASK);\r\nWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\r\nmdelay(1);\r\nr = radeon_uvd_send_upll_ctlreq(rdev, CG_UPLL_FUNC_CNTL);\r\nif (r)\r\nreturn r;\r\nWREG32_P(CG_UPLL_FUNC_CNTL, UPLL_RESET_MASK, ~UPLL_RESET_MASK);\r\nWREG32_P(CG_UPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\r\nWREG32_P(CG_UPLL_FUNC_CNTL_3, UPLL_FB_DIV(fb_div), ~UPLL_FB_DIV_MASK);\r\nWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_REF_DIV_MASK);\r\nif (fb_div < 307200)\r\nWREG32_P(CG_UPLL_FUNC_CNTL_4, 0, ~UPLL_SPARE_ISPARE9);\r\nelse\r\nWREG32_P(CG_UPLL_FUNC_CNTL_4, UPLL_SPARE_ISPARE9, ~UPLL_SPARE_ISPARE9);\r\nWREG32_P(CG_UPLL_FUNC_CNTL_2,\r\nUPLL_PDIV_A(vclk_div) | UPLL_PDIV_B(dclk_div),\r\n~(UPLL_PDIV_A_MASK | UPLL_PDIV_B_MASK));\r\nmdelay(15);\r\nWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_RESET_MASK);\r\nmdelay(15);\r\nWREG32_P(CG_UPLL_FUNC_CNTL, 0, ~UPLL_BYPASS_EN_MASK);\r\nr = radeon_uvd_send_upll_ctlreq(rdev, CG_UPLL_FUNC_CNTL);\r\nif (r)\r\nreturn r;\r\nWREG32_P(CG_UPLL_FUNC_CNTL_2,\r\nVCLK_SRC_SEL(2) | DCLK_SRC_SEL(2),\r\n~(VCLK_SRC_SEL_MASK | DCLK_SRC_SEL_MASK));\r\nmdelay(100);\r\nreturn 0;\r\n}\r\nstatic void si_pcie_gen3_enable(struct radeon_device *rdev)\r\n{\r\nstruct pci_dev *root = rdev->pdev->bus->self;\r\nint bridge_pos, gpu_pos;\r\nu32 speed_cntl, mask, current_data_rate;\r\nint ret, i;\r\nu16 tmp16;\r\nif (pci_is_root_bus(rdev->pdev->bus))\r\nreturn;\r\nif (radeon_pcie_gen2 == 0)\r\nreturn;\r\nif (rdev->flags & RADEON_IS_IGP)\r\nreturn;\r\nif (!(rdev->flags & RADEON_IS_PCIE))\r\nreturn;\r\nret = drm_pcie_get_speed_cap_mask(rdev->ddev, &mask);\r\nif (ret != 0)\r\nreturn;\r\nif (!(mask & (DRM_PCIE_SPEED_50 | DRM_PCIE_SPEED_80)))\r\nreturn;\r\nspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\r\ncurrent_data_rate = (speed_cntl & LC_CURRENT_DATA_RATE_MASK) >>\r\nLC_CURRENT_DATA_RATE_SHIFT;\r\nif (mask & DRM_PCIE_SPEED_80) {\r\nif (current_data_rate == 2) {\r\nDRM_INFO("PCIE gen 3 link speeds already enabled\n");\r\nreturn;\r\n}\r\nDRM_INFO("enabling PCIE gen 3 link speeds, disable with radeon.pcie_gen2=0\n");\r\n} else if (mask & DRM_PCIE_SPEED_50) {\r\nif (current_data_rate == 1) {\r\nDRM_INFO("PCIE gen 2 link speeds already enabled\n");\r\nreturn;\r\n}\r\nDRM_INFO("enabling PCIE gen 2 link speeds, disable with radeon.pcie_gen2=0\n");\r\n}\r\nbridge_pos = pci_pcie_cap(root);\r\nif (!bridge_pos)\r\nreturn;\r\ngpu_pos = pci_pcie_cap(rdev->pdev);\r\nif (!gpu_pos)\r\nreturn;\r\nif (mask & DRM_PCIE_SPEED_80) {\r\nif (current_data_rate != 2) {\r\nu16 bridge_cfg, gpu_cfg;\r\nu16 bridge_cfg2, gpu_cfg2;\r\nu32 max_lw, current_lw, tmp;\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL, &bridge_cfg);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, &gpu_cfg);\r\ntmp16 = bridge_cfg | PCI_EXP_LNKCTL_HAWD;\r\npci_write_config_word(root, bridge_pos + PCI_EXP_LNKCTL, tmp16);\r\ntmp16 = gpu_cfg | PCI_EXP_LNKCTL_HAWD;\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, tmp16);\r\ntmp = RREG32_PCIE(PCIE_LC_STATUS1);\r\nmax_lw = (tmp & LC_DETECTED_LINK_WIDTH_MASK) >> LC_DETECTED_LINK_WIDTH_SHIFT;\r\ncurrent_lw = (tmp & LC_OPERATING_LINK_WIDTH_MASK) >> LC_OPERATING_LINK_WIDTH_SHIFT;\r\nif (current_lw < max_lw) {\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\r\nif (tmp & LC_RENEGOTIATION_SUPPORT) {\r\ntmp &= ~(LC_LINK_WIDTH_MASK | LC_UPCONFIGURE_DIS);\r\ntmp |= (max_lw << LC_LINK_WIDTH_SHIFT);\r\ntmp |= LC_UPCONFIGURE_SUPPORT | LC_RENEGOTIATE_EN | LC_RECONFIG_NOW;\r\nWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, tmp);\r\n}\r\n}\r\nfor (i = 0; i < 10; i++) {\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_DEVSTA, &tmp16);\r\nif (tmp16 & PCI_EXP_DEVSTA_TRPND)\r\nbreak;\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL, &bridge_cfg);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, &gpu_cfg);\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL2, &bridge_cfg2);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, &gpu_cfg2);\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\r\ntmp |= LC_SET_QUIESCE;\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\r\ntmp |= LC_REDO_EQ;\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\r\nmdelay(100);\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL, &tmp16);\r\ntmp16 &= ~PCI_EXP_LNKCTL_HAWD;\r\ntmp16 |= (bridge_cfg & PCI_EXP_LNKCTL_HAWD);\r\npci_write_config_word(root, bridge_pos + PCI_EXP_LNKCTL, tmp16);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, &tmp16);\r\ntmp16 &= ~PCI_EXP_LNKCTL_HAWD;\r\ntmp16 |= (gpu_cfg & PCI_EXP_LNKCTL_HAWD);\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, tmp16);\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL2, &tmp16);\r\ntmp16 &= ~((1 << 4) | (7 << 9));\r\ntmp16 |= (bridge_cfg2 & ((1 << 4) | (7 << 9)));\r\npci_write_config_word(root, bridge_pos + PCI_EXP_LNKCTL2, tmp16);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, &tmp16);\r\ntmp16 &= ~((1 << 4) | (7 << 9));\r\ntmp16 |= (gpu_cfg2 & ((1 << 4) | (7 << 9)));\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, tmp16);\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\r\ntmp &= ~LC_SET_QUIESCE;\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\r\n}\r\n}\r\n}\r\nspeed_cntl |= LC_FORCE_EN_SW_SPEED_CHANGE | LC_FORCE_DIS_HW_SPEED_CHANGE;\r\nspeed_cntl &= ~LC_FORCE_DIS_SW_SPEED_CHANGE;\r\nWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, &tmp16);\r\ntmp16 &= ~0xf;\r\nif (mask & DRM_PCIE_SPEED_80)\r\ntmp16 |= 3;\r\nelse if (mask & DRM_PCIE_SPEED_50)\r\ntmp16 |= 2;\r\nelse\r\ntmp16 |= 1;\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, tmp16);\r\nspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\r\nspeed_cntl |= LC_INITIATE_LINK_SPEED_CHANGE;\r\nWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\r\nif ((speed_cntl & LC_INITIATE_LINK_SPEED_CHANGE) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nstatic void si_program_aspm(struct radeon_device *rdev)\r\n{\r\nu32 data, orig;\r\nbool disable_l0s = false, disable_l1 = false, disable_plloff_in_l1 = false;\r\nbool disable_clkreq = false;\r\nif (radeon_aspm == 0)\r\nreturn;\r\nif (!(rdev->flags & RADEON_IS_PCIE))\r\nreturn;\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\r\ndata &= ~LC_XMIT_N_FTS_MASK;\r\ndata |= LC_XMIT_N_FTS(0x24) | LC_XMIT_N_FTS_OVERRIDE_EN;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL, data);\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL3);\r\ndata |= LC_GO_TO_RECOVERY;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL3, data);\r\norig = data = RREG32_PCIE(PCIE_P_CNTL);\r\ndata |= P_IGNORE_EDB_ERR;\r\nif (orig != data)\r\nWREG32_PCIE(PCIE_P_CNTL, data);\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\r\ndata &= ~(LC_L0S_INACTIVITY_MASK | LC_L1_INACTIVITY_MASK);\r\ndata |= LC_PMI_TO_L1_DIS;\r\nif (!disable_l0s)\r\ndata |= LC_L0S_INACTIVITY(7);\r\nif (!disable_l1) {\r\ndata |= LC_L1_INACTIVITY(7);\r\ndata &= ~LC_PMI_TO_L1_DIS;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\r\nif (!disable_plloff_in_l1) {\r\nbool clk_req_support;\r\norig = data = RREG32_PIF_PHY0(PB0_PIF_PWRDOWN_0);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\r\nif (orig != data)\r\nWREG32_PIF_PHY0(PB0_PIF_PWRDOWN_0, data);\r\norig = data = RREG32_PIF_PHY0(PB0_PIF_PWRDOWN_1);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\r\nif (orig != data)\r\nWREG32_PIF_PHY0(PB0_PIF_PWRDOWN_1, data);\r\norig = data = RREG32_PIF_PHY1(PB1_PIF_PWRDOWN_0);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\r\nif (orig != data)\r\nWREG32_PIF_PHY1(PB1_PIF_PWRDOWN_0, data);\r\norig = data = RREG32_PIF_PHY1(PB1_PIF_PWRDOWN_1);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\r\nif (orig != data)\r\nWREG32_PIF_PHY1(PB1_PIF_PWRDOWN_1, data);\r\nif ((rdev->family != CHIP_OLAND) && (rdev->family != CHIP_HAINAN)) {\r\norig = data = RREG32_PIF_PHY0(PB0_PIF_PWRDOWN_0);\r\ndata &= ~PLL_RAMP_UP_TIME_0_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY0(PB0_PIF_PWRDOWN_0, data);\r\norig = data = RREG32_PIF_PHY0(PB0_PIF_PWRDOWN_1);\r\ndata &= ~PLL_RAMP_UP_TIME_1_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY0(PB0_PIF_PWRDOWN_1, data);\r\norig = data = RREG32_PIF_PHY0(PB0_PIF_PWRDOWN_2);\r\ndata &= ~PLL_RAMP_UP_TIME_2_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY0(PB0_PIF_PWRDOWN_2, data);\r\norig = data = RREG32_PIF_PHY0(PB0_PIF_PWRDOWN_3);\r\ndata &= ~PLL_RAMP_UP_TIME_3_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY0(PB0_PIF_PWRDOWN_3, data);\r\norig = data = RREG32_PIF_PHY1(PB1_PIF_PWRDOWN_0);\r\ndata &= ~PLL_RAMP_UP_TIME_0_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY1(PB1_PIF_PWRDOWN_0, data);\r\norig = data = RREG32_PIF_PHY1(PB1_PIF_PWRDOWN_1);\r\ndata &= ~PLL_RAMP_UP_TIME_1_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY1(PB1_PIF_PWRDOWN_1, data);\r\norig = data = RREG32_PIF_PHY1(PB1_PIF_PWRDOWN_2);\r\ndata &= ~PLL_RAMP_UP_TIME_2_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY1(PB1_PIF_PWRDOWN_2, data);\r\norig = data = RREG32_PIF_PHY1(PB1_PIF_PWRDOWN_3);\r\ndata &= ~PLL_RAMP_UP_TIME_3_MASK;\r\nif (orig != data)\r\nWREG32_PIF_PHY1(PB1_PIF_PWRDOWN_3, data);\r\n}\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\r\ndata &= ~LC_DYN_LANES_PWR_STATE_MASK;\r\ndata |= LC_DYN_LANES_PWR_STATE(3);\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, data);\r\norig = data = RREG32_PIF_PHY0(PB0_PIF_CNTL);\r\ndata &= ~LS2_EXIT_TIME_MASK;\r\nif ((rdev->family == CHIP_OLAND) || (rdev->family == CHIP_HAINAN))\r\ndata |= LS2_EXIT_TIME(5);\r\nif (orig != data)\r\nWREG32_PIF_PHY0(PB0_PIF_CNTL, data);\r\norig = data = RREG32_PIF_PHY1(PB1_PIF_CNTL);\r\ndata &= ~LS2_EXIT_TIME_MASK;\r\nif ((rdev->family == CHIP_OLAND) || (rdev->family == CHIP_HAINAN))\r\ndata |= LS2_EXIT_TIME(5);\r\nif (orig != data)\r\nWREG32_PIF_PHY1(PB1_PIF_CNTL, data);\r\nif (!disable_clkreq &&\r\n!pci_is_root_bus(rdev->pdev->bus)) {\r\nstruct pci_dev *root = rdev->pdev->bus->self;\r\nu32 lnkcap;\r\nclk_req_support = false;\r\npcie_capability_read_dword(root, PCI_EXP_LNKCAP, &lnkcap);\r\nif (lnkcap & PCI_EXP_LNKCAP_CLKPM)\r\nclk_req_support = true;\r\n} else {\r\nclk_req_support = false;\r\n}\r\nif (clk_req_support) {\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL2);\r\ndata |= LC_ALLOW_PDWN_IN_L1 | LC_ALLOW_PDWN_IN_L23;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL2, data);\r\norig = data = RREG32(THM_CLK_CNTL);\r\ndata &= ~(CMON_CLK_SEL_MASK | TMON_CLK_SEL_MASK);\r\ndata |= CMON_CLK_SEL(1) | TMON_CLK_SEL(1);\r\nif (orig != data)\r\nWREG32(THM_CLK_CNTL, data);\r\norig = data = RREG32(MISC_CLK_CNTL);\r\ndata &= ~(DEEP_SLEEP_CLK_SEL_MASK | ZCLK_SEL_MASK);\r\ndata |= DEEP_SLEEP_CLK_SEL(1) | ZCLK_SEL(1);\r\nif (orig != data)\r\nWREG32(MISC_CLK_CNTL, data);\r\norig = data = RREG32(CG_CLKPIN_CNTL);\r\ndata &= ~BCLK_AS_XCLK;\r\nif (orig != data)\r\nWREG32(CG_CLKPIN_CNTL, data);\r\norig = data = RREG32(CG_CLKPIN_CNTL_2);\r\ndata &= ~FORCE_BIF_REFCLK_EN;\r\nif (orig != data)\r\nWREG32(CG_CLKPIN_CNTL_2, data);\r\norig = data = RREG32(MPLL_BYPASSCLK_SEL);\r\ndata &= ~MPLL_CLKOUT_SEL_MASK;\r\ndata |= MPLL_CLKOUT_SEL(4);\r\nif (orig != data)\r\nWREG32(MPLL_BYPASSCLK_SEL, data);\r\norig = data = RREG32(SPLL_CNTL_MODE);\r\ndata &= ~SPLL_REFCLK_SEL_MASK;\r\nif (orig != data)\r\nWREG32(SPLL_CNTL_MODE, data);\r\n}\r\n}\r\n} else {\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\r\n}\r\norig = data = RREG32_PCIE(PCIE_CNTL2);\r\ndata |= SLV_MEM_LS_EN | MST_MEM_LS_EN | REPLAY_MEM_LS_EN;\r\nif (orig != data)\r\nWREG32_PCIE(PCIE_CNTL2, data);\r\nif (!disable_l0s) {\r\ndata = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\r\nif((data & LC_N_FTS_MASK) == LC_N_FTS_MASK) {\r\ndata = RREG32_PCIE(PCIE_LC_STATUS1);\r\nif ((data & LC_REVERSE_XMIT) && (data & LC_REVERSE_RCVR)) {\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\r\ndata &= ~LC_L0S_INACTIVITY_MASK;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\r\n}\r\n}\r\n}\r\n}\r\nstatic int si_vce_send_vcepll_ctlreq(struct radeon_device *rdev)\r\n{\r\nunsigned i;\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\r\nmdelay(10);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\r\nfor (i = 0; i < 100; ++i) {\r\nuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\r\nif ((RREG32_SMC(CG_VCEPLL_FUNC_CNTL) & mask) == mask)\r\nbreak;\r\nmdelay(10);\r\n}\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~UPLL_CTLREQ_MASK);\r\nif (i == 100) {\r\nDRM_ERROR("Timeout setting UVD clocks!\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nreturn 0;\r\n}\r\nint si_set_vce_clocks(struct radeon_device *rdev, u32 evclk, u32 ecclk)\r\n{\r\nunsigned fb_div = 0, evclk_div = 0, ecclk_div = 0;\r\nint r;\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\r\nEVCLK_SRC_SEL(1) | ECCLK_SRC_SEL(1),\r\n~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_BYPASS_EN_MASK,\r\n~VCEPLL_BYPASS_EN_MASK);\r\nif (!evclk || !ecclk) {\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\r\n~VCEPLL_SLEEP_MASK);\r\nreturn 0;\r\n}\r\nr = radeon_uvd_calc_upll_dividers(rdev, evclk, ecclk, 125000, 250000,\r\n16384, 0x03FFFFFF, 0, 128, 5,\r\n&fb_div, &evclk_div, &ecclk_div);\r\nif (r)\r\nreturn r;\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_5, 0, ~RESET_ANTI_MUX_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_VCO_MODE_MASK,\r\n~VCEPLL_VCO_MODE_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_SLEEP_MASK,\r\n~VCEPLL_SLEEP_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_SLEEP_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\r\nmdelay(1);\r\nr = si_vce_send_vcepll_ctlreq(rdev);\r\nif (r)\r\nreturn r;\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, VCEPLL_RESET_MASK, ~VCEPLL_RESET_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_SPREAD_SPECTRUM, 0, ~SSEN_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_3, VCEPLL_FB_DIV(fb_div), ~VCEPLL_FB_DIV_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_REF_DIV_MASK);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\r\nVCEPLL_PDIV_A(evclk_div) | VCEPLL_PDIV_B(ecclk_div),\r\n~(VCEPLL_PDIV_A_MASK | VCEPLL_PDIV_B_MASK));\r\nmdelay(15);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_RESET_MASK);\r\nmdelay(15);\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL, 0, ~VCEPLL_BYPASS_EN_MASK);\r\nr = si_vce_send_vcepll_ctlreq(rdev);\r\nif (r)\r\nreturn r;\r\nWREG32_SMC_P(CG_VCEPLL_FUNC_CNTL_2,\r\nEVCLK_SRC_SEL(16) | ECCLK_SRC_SEL(16),\r\n~(EVCLK_SRC_SEL_MASK | ECCLK_SRC_SEL_MASK));\r\nmdelay(100);\r\nreturn 0;\r\n}
