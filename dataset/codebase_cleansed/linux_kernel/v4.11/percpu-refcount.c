static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)\r\n{\r\nreturn (unsigned long __percpu *)\r\n(ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC_DEAD);\r\n}\r\nint percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,\r\nunsigned int flags, gfp_t gfp)\r\n{\r\nsize_t align = max_t(size_t, 1 << __PERCPU_REF_FLAG_BITS,\r\n__alignof__(unsigned long));\r\nunsigned long start_count = 0;\r\nref->percpu_count_ptr = (unsigned long)\r\n__alloc_percpu_gfp(sizeof(unsigned long), align, gfp);\r\nif (!ref->percpu_count_ptr)\r\nreturn -ENOMEM;\r\nref->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;\r\nif (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD))\r\nref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;\r\nelse\r\nstart_count += PERCPU_COUNT_BIAS;\r\nif (flags & PERCPU_REF_INIT_DEAD)\r\nref->percpu_count_ptr |= __PERCPU_REF_DEAD;\r\nelse\r\nstart_count++;\r\natomic_long_set(&ref->count, start_count);\r\nref->release = release;\r\nref->confirm_switch = NULL;\r\nreturn 0;\r\n}\r\nvoid percpu_ref_exit(struct percpu_ref *ref)\r\n{\r\nunsigned long __percpu *percpu_count = percpu_count_ptr(ref);\r\nif (percpu_count) {\r\nWARN_ON_ONCE(ref->confirm_switch);\r\nfree_percpu(percpu_count);\r\nref->percpu_count_ptr = __PERCPU_REF_ATOMIC_DEAD;\r\n}\r\n}\r\nstatic void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)\r\n{\r\nstruct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);\r\nref->confirm_switch(ref);\r\nref->confirm_switch = NULL;\r\nwake_up_all(&percpu_ref_switch_waitq);\r\npercpu_ref_put(ref);\r\n}\r\nstatic void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)\r\n{\r\nstruct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);\r\nunsigned long __percpu *percpu_count = percpu_count_ptr(ref);\r\nunsigned long count = 0;\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\ncount += *per_cpu_ptr(percpu_count, cpu);\r\npr_debug("global %ld percpu %ld",\r\natomic_long_read(&ref->count), (long)count);\r\natomic_long_add((long)count - PERCPU_COUNT_BIAS, &ref->count);\r\nWARN_ONCE(atomic_long_read(&ref->count) <= 0,\r\n"percpu ref (%pf) <= 0 (%ld) after switching to atomic",\r\nref->release, atomic_long_read(&ref->count));\r\npercpu_ref_call_confirm_rcu(rcu);\r\n}\r\nstatic void percpu_ref_noop_confirm_switch(struct percpu_ref *ref)\r\n{\r\n}\r\nstatic void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,\r\npercpu_ref_func_t *confirm_switch)\r\n{\r\nif (ref->percpu_count_ptr & __PERCPU_REF_ATOMIC) {\r\nif (confirm_switch)\r\nconfirm_switch(ref);\r\nreturn;\r\n}\r\nref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;\r\nref->confirm_switch = confirm_switch ?: percpu_ref_noop_confirm_switch;\r\npercpu_ref_get(ref);\r\ncall_rcu_sched(&ref->rcu, percpu_ref_switch_to_atomic_rcu);\r\n}\r\nstatic void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)\r\n{\r\nunsigned long __percpu *percpu_count = percpu_count_ptr(ref);\r\nint cpu;\r\nBUG_ON(!percpu_count);\r\nif (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))\r\nreturn;\r\natomic_long_add(PERCPU_COUNT_BIAS, &ref->count);\r\nfor_each_possible_cpu(cpu)\r\n*per_cpu_ptr(percpu_count, cpu) = 0;\r\nsmp_store_release(&ref->percpu_count_ptr,\r\nref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);\r\n}\r\nstatic void __percpu_ref_switch_mode(struct percpu_ref *ref,\r\npercpu_ref_func_t *confirm_switch)\r\n{\r\nlockdep_assert_held(&percpu_ref_switch_lock);\r\nwait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,\r\npercpu_ref_switch_lock);\r\nif (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))\r\n__percpu_ref_switch_to_atomic(ref, confirm_switch);\r\nelse\r\n__percpu_ref_switch_to_percpu(ref);\r\n}\r\nvoid percpu_ref_switch_to_atomic(struct percpu_ref *ref,\r\npercpu_ref_func_t *confirm_switch)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&percpu_ref_switch_lock, flags);\r\nref->force_atomic = true;\r\n__percpu_ref_switch_mode(ref, confirm_switch);\r\nspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\r\n}\r\nvoid percpu_ref_switch_to_percpu(struct percpu_ref *ref)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&percpu_ref_switch_lock, flags);\r\nref->force_atomic = false;\r\n__percpu_ref_switch_mode(ref, NULL);\r\nspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\r\n}\r\nvoid percpu_ref_kill_and_confirm(struct percpu_ref *ref,\r\npercpu_ref_func_t *confirm_kill)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&percpu_ref_switch_lock, flags);\r\nWARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,\r\n"%s called more than once on %pf!", __func__, ref->release);\r\nref->percpu_count_ptr |= __PERCPU_REF_DEAD;\r\n__percpu_ref_switch_mode(ref, confirm_kill);\r\npercpu_ref_put(ref);\r\nspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\r\n}\r\nvoid percpu_ref_reinit(struct percpu_ref *ref)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&percpu_ref_switch_lock, flags);\r\nWARN_ON_ONCE(!percpu_ref_is_zero(ref));\r\nref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;\r\npercpu_ref_get(ref);\r\n__percpu_ref_switch_mode(ref, NULL);\r\nspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\r\n}
