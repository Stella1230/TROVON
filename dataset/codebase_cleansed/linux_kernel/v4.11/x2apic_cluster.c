static int x2apic_acpi_madt_oem_check(char *oem_id, char *oem_table_id)\r\n{\r\nreturn x2apic_enabled();\r\n}\r\nstatic inline u32 x2apic_cluster(int cpu)\r\n{\r\nreturn per_cpu(x86_cpu_to_logical_apicid, cpu) >> 16;\r\n}\r\nstatic void x2apic_send_IPI(int cpu, int vector)\r\n{\r\nu32 dest = per_cpu(x86_cpu_to_logical_apicid, cpu);\r\nx2apic_wrmsr_fence();\r\n__x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);\r\n}\r\nstatic void\r\n__x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)\r\n{\r\nstruct cpumask *cpus_in_cluster_ptr;\r\nstruct cpumask *ipi_mask_ptr;\r\nunsigned int cpu, this_cpu;\r\nunsigned long flags;\r\nu32 dest;\r\nx2apic_wrmsr_fence();\r\nlocal_irq_save(flags);\r\nthis_cpu = smp_processor_id();\r\nipi_mask_ptr = this_cpu_cpumask_var_ptr(ipi_mask);\r\ncpumask_copy(ipi_mask_ptr, mask);\r\nfor_each_cpu(cpu, ipi_mask_ptr) {\r\nunsigned long i;\r\ncpus_in_cluster_ptr = per_cpu(cpus_in_cluster, cpu);\r\ndest = 0;\r\nfor_each_cpu_and(i, ipi_mask_ptr, cpus_in_cluster_ptr) {\r\nif (apic_dest == APIC_DEST_ALLINC || i != this_cpu)\r\ndest |= per_cpu(x86_cpu_to_logical_apicid, i);\r\n}\r\nif (!dest)\r\ncontinue;\r\n__x2apic_send_IPI_dest(dest, vector, apic->dest_logical);\r\ncpumask_andnot(ipi_mask_ptr, ipi_mask_ptr, cpus_in_cluster_ptr);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void x2apic_send_IPI_mask(const struct cpumask *mask, int vector)\r\n{\r\n__x2apic_send_IPI_mask(mask, vector, APIC_DEST_ALLINC);\r\n}\r\nstatic void\r\nx2apic_send_IPI_mask_allbutself(const struct cpumask *mask, int vector)\r\n{\r\n__x2apic_send_IPI_mask(mask, vector, APIC_DEST_ALLBUT);\r\n}\r\nstatic void x2apic_send_IPI_allbutself(int vector)\r\n{\r\n__x2apic_send_IPI_mask(cpu_online_mask, vector, APIC_DEST_ALLBUT);\r\n}\r\nstatic void x2apic_send_IPI_all(int vector)\r\n{\r\n__x2apic_send_IPI_mask(cpu_online_mask, vector, APIC_DEST_ALLINC);\r\n}\r\nstatic int\r\nx2apic_cpu_mask_to_apicid_and(const struct cpumask *cpumask,\r\nconst struct cpumask *andmask,\r\nunsigned int *apicid)\r\n{\r\nu32 dest = 0;\r\nu16 cluster;\r\nint i;\r\nfor_each_cpu_and(i, cpumask, andmask) {\r\nif (!cpumask_test_cpu(i, cpu_online_mask))\r\ncontinue;\r\ndest = per_cpu(x86_cpu_to_logical_apicid, i);\r\ncluster = x2apic_cluster(i);\r\nbreak;\r\n}\r\nif (!dest)\r\nreturn -EINVAL;\r\nfor_each_cpu_and(i, cpumask, andmask) {\r\nif (!cpumask_test_cpu(i, cpu_online_mask))\r\ncontinue;\r\nif (cluster != x2apic_cluster(i))\r\ncontinue;\r\ndest |= per_cpu(x86_cpu_to_logical_apicid, i);\r\n}\r\n*apicid = dest;\r\nreturn 0;\r\n}\r\nstatic void init_x2apic_ldr(void)\r\n{\r\nunsigned int this_cpu = smp_processor_id();\r\nunsigned int cpu;\r\nper_cpu(x86_cpu_to_logical_apicid, this_cpu) = apic_read(APIC_LDR);\r\ncpumask_set_cpu(this_cpu, per_cpu(cpus_in_cluster, this_cpu));\r\nfor_each_online_cpu(cpu) {\r\nif (x2apic_cluster(this_cpu) != x2apic_cluster(cpu))\r\ncontinue;\r\ncpumask_set_cpu(this_cpu, per_cpu(cpus_in_cluster, cpu));\r\ncpumask_set_cpu(cpu, per_cpu(cpus_in_cluster, this_cpu));\r\n}\r\n}\r\nstatic int x2apic_prepare_cpu(unsigned int cpu)\r\n{\r\nif (!zalloc_cpumask_var(&per_cpu(cpus_in_cluster, cpu), GFP_KERNEL))\r\nreturn -ENOMEM;\r\nif (!zalloc_cpumask_var(&per_cpu(ipi_mask, cpu), GFP_KERNEL)) {\r\nfree_cpumask_var(per_cpu(cpus_in_cluster, cpu));\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic int x2apic_dead_cpu(unsigned int this_cpu)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu) {\r\nif (x2apic_cluster(this_cpu) != x2apic_cluster(cpu))\r\ncontinue;\r\ncpumask_clear_cpu(this_cpu, per_cpu(cpus_in_cluster, cpu));\r\ncpumask_clear_cpu(cpu, per_cpu(cpus_in_cluster, this_cpu));\r\n}\r\nfree_cpumask_var(per_cpu(cpus_in_cluster, this_cpu));\r\nfree_cpumask_var(per_cpu(ipi_mask, this_cpu));\r\nreturn 0;\r\n}\r\nstatic int x2apic_cluster_probe(void)\r\n{\r\nint cpu = smp_processor_id();\r\nint ret;\r\nif (!x2apic_mode)\r\nreturn 0;\r\nret = cpuhp_setup_state(CPUHP_X2APIC_PREPARE, "x86/x2apic:prepare",\r\nx2apic_prepare_cpu, x2apic_dead_cpu);\r\nif (ret < 0) {\r\npr_err("Failed to register X2APIC_PREPARE\n");\r\nreturn 0;\r\n}\r\ncpumask_set_cpu(cpu, per_cpu(cpus_in_cluster, cpu));\r\nreturn 1;\r\n}\r\nstatic const struct cpumask *x2apic_cluster_target_cpus(void)\r\n{\r\nreturn cpu_all_mask;\r\n}\r\nstatic void cluster_vector_allocation_domain(int cpu, struct cpumask *retmask,\r\nconst struct cpumask *mask)\r\n{\r\nif (mask == x2apic_cluster_target_cpus())\r\ncpumask_copy(retmask, cpumask_of(cpu));\r\nelse\r\ncpumask_and(retmask, mask, per_cpu(cpus_in_cluster, cpu));\r\n}
