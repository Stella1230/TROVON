static int hidma_is_chan_enabled(int state)\r\n{\r\nswitch (state) {\r\ncase HIDMA_CH_ENABLED:\r\ncase HIDMA_CH_RUNNING:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nvoid hidma_ll_free(struct hidma_lldev *lldev, u32 tre_ch)\r\n{\r\nstruct hidma_tre *tre;\r\nif (tre_ch >= lldev->nr_tres) {\r\ndev_err(lldev->dev, "invalid TRE number in free:%d", tre_ch);\r\nreturn;\r\n}\r\ntre = &lldev->trepool[tre_ch];\r\nif (atomic_read(&tre->allocated) != true) {\r\ndev_err(lldev->dev, "trying to free an unused TRE:%d", tre_ch);\r\nreturn;\r\n}\r\natomic_set(&tre->allocated, 0);\r\n}\r\nint hidma_ll_request(struct hidma_lldev *lldev, u32 sig, const char *dev_name,\r\nvoid (*callback)(void *data), void *data, u32 *tre_ch)\r\n{\r\nunsigned int i;\r\nstruct hidma_tre *tre;\r\nu32 *tre_local;\r\nif (!tre_ch || !lldev)\r\nreturn -EINVAL;\r\nfor (i = 0; i < lldev->nr_tres - 1; i++) {\r\nif (atomic_add_unless(&lldev->trepool[i].allocated, 1, 1))\r\nbreak;\r\n}\r\nif (i == (lldev->nr_tres - 1))\r\nreturn -ENOMEM;\r\ntre = &lldev->trepool[i];\r\ntre->dma_sig = sig;\r\ntre->dev_name = dev_name;\r\ntre->callback = callback;\r\ntre->data = data;\r\ntre->idx = i;\r\ntre->status = 0;\r\ntre->queued = 0;\r\ntre->err_code = 0;\r\ntre->err_info = 0;\r\ntre->lldev = lldev;\r\ntre_local = &tre->tre_local[0];\r\ntre_local[HIDMA_TRE_CFG_IDX] = HIDMA_TRE_MEMCPY;\r\ntre_local[HIDMA_TRE_CFG_IDX] |= (lldev->chidx & 0xFF) << 8;\r\ntre_local[HIDMA_TRE_CFG_IDX] |= BIT(16);\r\n*tre_ch = i;\r\nif (callback)\r\ncallback(data);\r\nreturn 0;\r\n}\r\nstatic void hidma_ll_tre_complete(unsigned long arg)\r\n{\r\nstruct hidma_lldev *lldev = (struct hidma_lldev *)arg;\r\nstruct hidma_tre *tre;\r\nwhile (kfifo_out(&lldev->handoff_fifo, &tre, 1)) {\r\nif (tre->callback)\r\ntre->callback(tre->data);\r\n}\r\n}\r\nstatic int hidma_post_completed(struct hidma_lldev *lldev, u8 err_info,\r\nu8 err_code)\r\n{\r\nstruct hidma_tre *tre;\r\nunsigned long flags;\r\nu32 tre_iterator;\r\nspin_lock_irqsave(&lldev->lock, flags);\r\ntre_iterator = lldev->tre_processed_off;\r\ntre = lldev->pending_tre_list[tre_iterator / HIDMA_TRE_SIZE];\r\nif (!tre) {\r\nspin_unlock_irqrestore(&lldev->lock, flags);\r\ndev_warn(lldev->dev, "tre_index [%d] and tre out of sync\n",\r\ntre_iterator / HIDMA_TRE_SIZE);\r\nreturn -EINVAL;\r\n}\r\nlldev->pending_tre_list[tre->tre_index] = NULL;\r\nif (atomic_dec_return(&lldev->pending_tre_count) < 0) {\r\ndev_warn(lldev->dev, "tre count mismatch on completion");\r\natomic_set(&lldev->pending_tre_count, 0);\r\n}\r\nHIDMA_INCREMENT_ITERATOR(tre_iterator, HIDMA_TRE_SIZE,\r\nlldev->tre_ring_size);\r\nlldev->tre_processed_off = tre_iterator;\r\nspin_unlock_irqrestore(&lldev->lock, flags);\r\ntre->err_info = err_info;\r\ntre->err_code = err_code;\r\ntre->queued = 0;\r\nkfifo_put(&lldev->handoff_fifo, tre);\r\ntasklet_schedule(&lldev->task);\r\nreturn 0;\r\n}\r\nstatic int hidma_handle_tre_completion(struct hidma_lldev *lldev)\r\n{\r\nu32 evre_ring_size = lldev->evre_ring_size;\r\nu32 err_info, err_code, evre_write_off;\r\nu32 evre_iterator;\r\nu32 num_completed = 0;\r\nevre_write_off = readl_relaxed(lldev->evca + HIDMA_EVCA_WRITE_PTR_REG);\r\nevre_iterator = lldev->evre_processed_off;\r\nif ((evre_write_off > evre_ring_size) ||\r\n(evre_write_off % HIDMA_EVRE_SIZE)) {\r\ndev_err(lldev->dev, "HW reports invalid EVRE write offset\n");\r\nreturn 0;\r\n}\r\nwhile ((evre_iterator != evre_write_off)) {\r\nu32 *current_evre = lldev->evre_ring + evre_iterator;\r\nu32 cfg;\r\ncfg = current_evre[HIDMA_EVRE_CFG_IDX];\r\nerr_info = cfg >> HIDMA_EVRE_ERRINFO_BIT_POS;\r\nerr_info &= HIDMA_EVRE_ERRINFO_MASK;\r\nerr_code =\r\n(cfg >> HIDMA_EVRE_CODE_BIT_POS) & HIDMA_EVRE_CODE_MASK;\r\nif (hidma_post_completed(lldev, err_info, err_code))\r\nbreak;\r\nHIDMA_INCREMENT_ITERATOR(evre_iterator, HIDMA_EVRE_SIZE,\r\nevre_ring_size);\r\nevre_write_off =\r\nreadl_relaxed(lldev->evca + HIDMA_EVCA_WRITE_PTR_REG);\r\nnum_completed++;\r\nif (!hidma_ll_isenabled(lldev))\r\nbreak;\r\n}\r\nif (num_completed) {\r\nu32 evre_read_off = (lldev->evre_processed_off +\r\nHIDMA_EVRE_SIZE * num_completed);\r\nevre_read_off = evre_read_off % evre_ring_size;\r\nwritel(evre_read_off, lldev->evca + HIDMA_EVCA_DOORBELL_REG);\r\nlldev->evre_processed_off = evre_read_off;\r\n}\r\nreturn num_completed;\r\n}\r\nvoid hidma_cleanup_pending_tre(struct hidma_lldev *lldev, u8 err_info,\r\nu8 err_code)\r\n{\r\nwhile (atomic_read(&lldev->pending_tre_count)) {\r\nif (hidma_post_completed(lldev, err_info, err_code))\r\nbreak;\r\n}\r\n}\r\nstatic int hidma_ll_reset(struct hidma_lldev *lldev)\r\n{\r\nu32 val;\r\nint ret;\r\nval = readl(lldev->trca + HIDMA_TRCA_CTRLSTS_REG);\r\nval &= ~(HIDMA_CH_CONTROL_MASK << 16);\r\nval |= HIDMA_CH_RESET << 16;\r\nwritel(val, lldev->trca + HIDMA_TRCA_CTRLSTS_REG);\r\nret = readl_poll_timeout(lldev->trca + HIDMA_TRCA_CTRLSTS_REG, val,\r\nHIDMA_CH_STATE(val) == HIDMA_CH_DISABLED,\r\n1000, 10000);\r\nif (ret) {\r\ndev_err(lldev->dev, "transfer channel did not reset\n");\r\nreturn ret;\r\n}\r\nval = readl(lldev->evca + HIDMA_EVCA_CTRLSTS_REG);\r\nval &= ~(HIDMA_CH_CONTROL_MASK << 16);\r\nval |= HIDMA_CH_RESET << 16;\r\nwritel(val, lldev->evca + HIDMA_EVCA_CTRLSTS_REG);\r\nret = readl_poll_timeout(lldev->evca + HIDMA_EVCA_CTRLSTS_REG, val,\r\nHIDMA_CH_STATE(val) == HIDMA_CH_DISABLED,\r\n1000, 10000);\r\nif (ret)\r\nreturn ret;\r\nlldev->trch_state = HIDMA_CH_DISABLED;\r\nlldev->evch_state = HIDMA_CH_DISABLED;\r\nreturn 0;\r\n}\r\nstatic void hidma_ll_int_handler_internal(struct hidma_lldev *lldev, int cause)\r\n{\r\nif (cause & HIDMA_ERR_INT_MASK) {\r\ndev_err(lldev->dev, "error 0x%x, disabling...\n",\r\ncause);\r\nwritel(cause, lldev->evca + HIDMA_EVCA_IRQ_CLR_REG);\r\nhidma_ll_disable(lldev);\r\nhidma_cleanup_pending_tre(lldev, 0xFF,\r\nHIDMA_EVRE_STATUS_ERROR);\r\nreturn;\r\n}\r\nhidma_handle_tre_completion(lldev);\r\nwritel_relaxed(cause, lldev->evca + HIDMA_EVCA_IRQ_CLR_REG);\r\n}\r\nirqreturn_t hidma_ll_inthandler(int chirq, void *arg)\r\n{\r\nstruct hidma_lldev *lldev = arg;\r\nu32 status;\r\nu32 enable;\r\nu32 cause;\r\nstatus = readl_relaxed(lldev->evca + HIDMA_EVCA_IRQ_STAT_REG);\r\nenable = readl_relaxed(lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\ncause = status & enable;\r\nwhile (cause) {\r\nhidma_ll_int_handler_internal(lldev, cause);\r\nstatus = readl_relaxed(lldev->evca + HIDMA_EVCA_IRQ_STAT_REG);\r\nenable = readl_relaxed(lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\ncause = status & enable;\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nirqreturn_t hidma_ll_inthandler_msi(int chirq, void *arg, int cause)\r\n{\r\nstruct hidma_lldev *lldev = arg;\r\nhidma_ll_int_handler_internal(lldev, cause);\r\nreturn IRQ_HANDLED;\r\n}\r\nint hidma_ll_enable(struct hidma_lldev *lldev)\r\n{\r\nu32 val;\r\nint ret;\r\nval = readl(lldev->evca + HIDMA_EVCA_CTRLSTS_REG);\r\nval &= ~(HIDMA_CH_CONTROL_MASK << 16);\r\nval |= HIDMA_CH_ENABLE << 16;\r\nwritel(val, lldev->evca + HIDMA_EVCA_CTRLSTS_REG);\r\nret = readl_poll_timeout(lldev->evca + HIDMA_EVCA_CTRLSTS_REG, val,\r\nhidma_is_chan_enabled(HIDMA_CH_STATE(val)),\r\n1000, 10000);\r\nif (ret) {\r\ndev_err(lldev->dev, "event channel did not get enabled\n");\r\nreturn ret;\r\n}\r\nval = readl(lldev->trca + HIDMA_TRCA_CTRLSTS_REG);\r\nval &= ~(HIDMA_CH_CONTROL_MASK << 16);\r\nval |= HIDMA_CH_ENABLE << 16;\r\nwritel(val, lldev->trca + HIDMA_TRCA_CTRLSTS_REG);\r\nret = readl_poll_timeout(lldev->trca + HIDMA_TRCA_CTRLSTS_REG, val,\r\nhidma_is_chan_enabled(HIDMA_CH_STATE(val)),\r\n1000, 10000);\r\nif (ret) {\r\ndev_err(lldev->dev, "transfer channel did not get enabled\n");\r\nreturn ret;\r\n}\r\nlldev->trch_state = HIDMA_CH_ENABLED;\r\nlldev->evch_state = HIDMA_CH_ENABLED;\r\nreturn 0;\r\n}\r\nvoid hidma_ll_start(struct hidma_lldev *lldev)\r\n{\r\nunsigned long irqflags;\r\nspin_lock_irqsave(&lldev->lock, irqflags);\r\nwritel(lldev->tre_write_offset, lldev->trca + HIDMA_TRCA_DOORBELL_REG);\r\nspin_unlock_irqrestore(&lldev->lock, irqflags);\r\n}\r\nbool hidma_ll_isenabled(struct hidma_lldev *lldev)\r\n{\r\nu32 val;\r\nval = readl(lldev->trca + HIDMA_TRCA_CTRLSTS_REG);\r\nlldev->trch_state = HIDMA_CH_STATE(val);\r\nval = readl(lldev->evca + HIDMA_EVCA_CTRLSTS_REG);\r\nlldev->evch_state = HIDMA_CH_STATE(val);\r\nif (hidma_is_chan_enabled(lldev->trch_state) &&\r\nhidma_is_chan_enabled(lldev->evch_state))\r\nreturn true;\r\nreturn false;\r\n}\r\nvoid hidma_ll_queue_request(struct hidma_lldev *lldev, u32 tre_ch)\r\n{\r\nstruct hidma_tre *tre;\r\nunsigned long flags;\r\ntre = &lldev->trepool[tre_ch];\r\nspin_lock_irqsave(&lldev->lock, flags);\r\ntre->tre_index = lldev->tre_write_offset / HIDMA_TRE_SIZE;\r\nlldev->pending_tre_list[tre->tre_index] = tre;\r\nmemcpy(lldev->tre_ring + lldev->tre_write_offset,\r\n&tre->tre_local[0], HIDMA_TRE_SIZE);\r\ntre->err_code = 0;\r\ntre->err_info = 0;\r\ntre->queued = 1;\r\natomic_inc(&lldev->pending_tre_count);\r\nlldev->tre_write_offset = (lldev->tre_write_offset + HIDMA_TRE_SIZE)\r\n% lldev->tre_ring_size;\r\nspin_unlock_irqrestore(&lldev->lock, flags);\r\n}\r\nint hidma_ll_disable(struct hidma_lldev *lldev)\r\n{\r\nu32 val;\r\nint ret;\r\nif (!hidma_ll_isenabled(lldev))\r\nreturn 0;\r\nval = readl(lldev->trca + HIDMA_TRCA_CTRLSTS_REG);\r\nval &= ~(HIDMA_CH_CONTROL_MASK << 16);\r\nval |= HIDMA_CH_SUSPEND << 16;\r\nwritel(val, lldev->trca + HIDMA_TRCA_CTRLSTS_REG);\r\nret = readl_poll_timeout(lldev->trca + HIDMA_TRCA_CTRLSTS_REG, val,\r\nHIDMA_CH_STATE(val) == HIDMA_CH_SUSPENDED,\r\n1000, 10000);\r\nif (ret)\r\nreturn ret;\r\nval = readl(lldev->evca + HIDMA_EVCA_CTRLSTS_REG);\r\nval &= ~(HIDMA_CH_CONTROL_MASK << 16);\r\nval |= HIDMA_CH_SUSPEND << 16;\r\nwritel(val, lldev->evca + HIDMA_EVCA_CTRLSTS_REG);\r\nret = readl_poll_timeout(lldev->evca + HIDMA_EVCA_CTRLSTS_REG, val,\r\nHIDMA_CH_STATE(val) == HIDMA_CH_SUSPENDED,\r\n1000, 10000);\r\nif (ret)\r\nreturn ret;\r\nlldev->trch_state = HIDMA_CH_SUSPENDED;\r\nlldev->evch_state = HIDMA_CH_SUSPENDED;\r\nreturn 0;\r\n}\r\nvoid hidma_ll_set_transfer_params(struct hidma_lldev *lldev, u32 tre_ch,\r\ndma_addr_t src, dma_addr_t dest, u32 len,\r\nu32 flags)\r\n{\r\nstruct hidma_tre *tre;\r\nu32 *tre_local;\r\nif (tre_ch >= lldev->nr_tres) {\r\ndev_err(lldev->dev, "invalid TRE number in transfer params:%d",\r\ntre_ch);\r\nreturn;\r\n}\r\ntre = &lldev->trepool[tre_ch];\r\nif (atomic_read(&tre->allocated) != true) {\r\ndev_err(lldev->dev, "trying to set params on an unused TRE:%d",\r\ntre_ch);\r\nreturn;\r\n}\r\ntre_local = &tre->tre_local[0];\r\ntre_local[HIDMA_TRE_LEN_IDX] = len;\r\ntre_local[HIDMA_TRE_SRC_LOW_IDX] = lower_32_bits(src);\r\ntre_local[HIDMA_TRE_SRC_HI_IDX] = upper_32_bits(src);\r\ntre_local[HIDMA_TRE_DEST_LOW_IDX] = lower_32_bits(dest);\r\ntre_local[HIDMA_TRE_DEST_HI_IDX] = upper_32_bits(dest);\r\ntre->int_flags = flags;\r\n}\r\nint hidma_ll_setup(struct hidma_lldev *lldev)\r\n{\r\nint rc;\r\nu64 addr;\r\nu32 val;\r\nu32 nr_tres = lldev->nr_tres;\r\natomic_set(&lldev->pending_tre_count, 0);\r\nlldev->tre_processed_off = 0;\r\nlldev->evre_processed_off = 0;\r\nlldev->tre_write_offset = 0;\r\nwritel(0, lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\nval = readl(lldev->evca + HIDMA_EVCA_IRQ_STAT_REG);\r\nwritel(val, lldev->evca + HIDMA_EVCA_IRQ_CLR_REG);\r\nrc = hidma_ll_reset(lldev);\r\nif (rc)\r\nreturn rc;\r\nval = readl(lldev->evca + HIDMA_EVCA_IRQ_STAT_REG);\r\nwritel(val, lldev->evca + HIDMA_EVCA_IRQ_CLR_REG);\r\nwritel(0, lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\naddr = lldev->tre_dma;\r\nwritel(lower_32_bits(addr), lldev->trca + HIDMA_TRCA_RING_LOW_REG);\r\nwritel(upper_32_bits(addr), lldev->trca + HIDMA_TRCA_RING_HIGH_REG);\r\nwritel(lldev->tre_ring_size, lldev->trca + HIDMA_TRCA_RING_LEN_REG);\r\naddr = lldev->evre_dma;\r\nwritel(lower_32_bits(addr), lldev->evca + HIDMA_EVCA_RING_LOW_REG);\r\nwritel(upper_32_bits(addr), lldev->evca + HIDMA_EVCA_RING_HIGH_REG);\r\nwritel(HIDMA_EVRE_SIZE * nr_tres,\r\nlldev->evca + HIDMA_EVCA_RING_LEN_REG);\r\nhidma_ll_setup_irq(lldev, lldev->msi_support);\r\nrc = hidma_ll_enable(lldev);\r\nif (rc)\r\nreturn rc;\r\nreturn rc;\r\n}\r\nvoid hidma_ll_setup_irq(struct hidma_lldev *lldev, bool msi)\r\n{\r\nu32 val;\r\nlldev->msi_support = msi;\r\nwritel(0, lldev->evca + HIDMA_EVCA_IRQ_CLR_REG);\r\nwritel(0, lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\nval = readl(lldev->evca + HIDMA_EVCA_INTCTRL_REG);\r\nval &= ~0xF;\r\nif (!lldev->msi_support)\r\nval = val | 0x1;\r\nwritel(val, lldev->evca + HIDMA_EVCA_INTCTRL_REG);\r\nwritel(ENABLE_IRQS, lldev->evca + HIDMA_EVCA_IRQ_CLR_REG);\r\nwritel(ENABLE_IRQS, lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\n}\r\nstruct hidma_lldev *hidma_ll_init(struct device *dev, u32 nr_tres,\r\nvoid __iomem *trca, void __iomem *evca,\r\nu8 chidx)\r\n{\r\nu32 required_bytes;\r\nstruct hidma_lldev *lldev;\r\nint rc;\r\nsize_t sz;\r\nif (!trca || !evca || !dev || !nr_tres)\r\nreturn NULL;\r\nif (nr_tres < 4)\r\nreturn NULL;\r\nnr_tres += 1;\r\nlldev = devm_kzalloc(dev, sizeof(struct hidma_lldev), GFP_KERNEL);\r\nif (!lldev)\r\nreturn NULL;\r\nlldev->evca = evca;\r\nlldev->trca = trca;\r\nlldev->dev = dev;\r\nsz = sizeof(struct hidma_tre);\r\nlldev->trepool = devm_kcalloc(lldev->dev, nr_tres, sz, GFP_KERNEL);\r\nif (!lldev->trepool)\r\nreturn NULL;\r\nrequired_bytes = sizeof(lldev->pending_tre_list[0]);\r\nlldev->pending_tre_list = devm_kcalloc(dev, nr_tres, required_bytes,\r\nGFP_KERNEL);\r\nif (!lldev->pending_tre_list)\r\nreturn NULL;\r\nsz = (HIDMA_TRE_SIZE + 1) * nr_tres;\r\nlldev->tre_ring = dmam_alloc_coherent(dev, sz, &lldev->tre_dma,\r\nGFP_KERNEL);\r\nif (!lldev->tre_ring)\r\nreturn NULL;\r\nmemset(lldev->tre_ring, 0, (HIDMA_TRE_SIZE + 1) * nr_tres);\r\nlldev->tre_ring_size = HIDMA_TRE_SIZE * nr_tres;\r\nlldev->nr_tres = nr_tres;\r\nif (!IS_ALIGNED(lldev->tre_dma, HIDMA_TRE_SIZE)) {\r\nu8 tre_ring_shift;\r\ntre_ring_shift = lldev->tre_dma % HIDMA_TRE_SIZE;\r\ntre_ring_shift = HIDMA_TRE_SIZE - tre_ring_shift;\r\nlldev->tre_dma += tre_ring_shift;\r\nlldev->tre_ring += tre_ring_shift;\r\n}\r\nsz = (HIDMA_EVRE_SIZE + 1) * nr_tres;\r\nlldev->evre_ring = dmam_alloc_coherent(dev, sz, &lldev->evre_dma,\r\nGFP_KERNEL);\r\nif (!lldev->evre_ring)\r\nreturn NULL;\r\nmemset(lldev->evre_ring, 0, (HIDMA_EVRE_SIZE + 1) * nr_tres);\r\nlldev->evre_ring_size = HIDMA_EVRE_SIZE * nr_tres;\r\nif (!IS_ALIGNED(lldev->evre_dma, HIDMA_EVRE_SIZE)) {\r\nu8 evre_ring_shift;\r\nevre_ring_shift = lldev->evre_dma % HIDMA_EVRE_SIZE;\r\nevre_ring_shift = HIDMA_EVRE_SIZE - evre_ring_shift;\r\nlldev->evre_dma += evre_ring_shift;\r\nlldev->evre_ring += evre_ring_shift;\r\n}\r\nlldev->nr_tres = nr_tres;\r\nlldev->chidx = chidx;\r\nsz = nr_tres * sizeof(struct hidma_tre *);\r\nrc = kfifo_alloc(&lldev->handoff_fifo, sz, GFP_KERNEL);\r\nif (rc)\r\nreturn NULL;\r\nrc = hidma_ll_setup(lldev);\r\nif (rc)\r\nreturn NULL;\r\nspin_lock_init(&lldev->lock);\r\ntasklet_init(&lldev->task, hidma_ll_tre_complete, (unsigned long)lldev);\r\nlldev->initialized = 1;\r\nwritel(ENABLE_IRQS, lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\nreturn lldev;\r\n}\r\nint hidma_ll_uninit(struct hidma_lldev *lldev)\r\n{\r\nu32 required_bytes;\r\nint rc = 0;\r\nu32 val;\r\nif (!lldev)\r\nreturn -ENODEV;\r\nif (!lldev->initialized)\r\nreturn 0;\r\nlldev->initialized = 0;\r\nrequired_bytes = sizeof(struct hidma_tre) * lldev->nr_tres;\r\ntasklet_kill(&lldev->task);\r\nmemset(lldev->trepool, 0, required_bytes);\r\nlldev->trepool = NULL;\r\natomic_set(&lldev->pending_tre_count, 0);\r\nlldev->tre_write_offset = 0;\r\nrc = hidma_ll_reset(lldev);\r\nval = readl(lldev->evca + HIDMA_EVCA_IRQ_STAT_REG);\r\nwritel(val, lldev->evca + HIDMA_EVCA_IRQ_CLR_REG);\r\nwritel(0, lldev->evca + HIDMA_EVCA_IRQ_EN_REG);\r\nreturn rc;\r\n}\r\nenum dma_status hidma_ll_status(struct hidma_lldev *lldev, u32 tre_ch)\r\n{\r\nenum dma_status ret = DMA_ERROR;\r\nstruct hidma_tre *tre;\r\nunsigned long flags;\r\nu8 err_code;\r\nspin_lock_irqsave(&lldev->lock, flags);\r\ntre = &lldev->trepool[tre_ch];\r\nerr_code = tre->err_code;\r\nif (err_code & HIDMA_EVRE_STATUS_COMPLETE)\r\nret = DMA_COMPLETE;\r\nelse if (err_code & HIDMA_EVRE_STATUS_ERROR)\r\nret = DMA_ERROR;\r\nelse\r\nret = DMA_IN_PROGRESS;\r\nspin_unlock_irqrestore(&lldev->lock, flags);\r\nreturn ret;\r\n}
