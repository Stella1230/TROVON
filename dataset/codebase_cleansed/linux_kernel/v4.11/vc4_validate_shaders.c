static uint32_t\r\nwaddr_to_live_reg_index(uint32_t waddr, bool is_b)\r\n{\r\nif (waddr < 32) {\r\nif (is_b)\r\nreturn 32 + waddr;\r\nelse\r\nreturn waddr;\r\n} else if (waddr <= QPU_W_ACC3) {\r\nreturn 64 + waddr - QPU_W_ACC0;\r\n} else {\r\nreturn ~0;\r\n}\r\n}\r\nstatic uint32_t\r\nraddr_add_a_to_live_reg_index(uint64_t inst)\r\n{\r\nuint32_t sig = QPU_GET_FIELD(inst, QPU_SIG);\r\nuint32_t add_a = QPU_GET_FIELD(inst, QPU_ADD_A);\r\nuint32_t raddr_a = QPU_GET_FIELD(inst, QPU_RADDR_A);\r\nuint32_t raddr_b = QPU_GET_FIELD(inst, QPU_RADDR_B);\r\nif (add_a == QPU_MUX_A)\r\nreturn raddr_a;\r\nelse if (add_a == QPU_MUX_B && sig != QPU_SIG_SMALL_IMM)\r\nreturn 32 + raddr_b;\r\nelse if (add_a <= QPU_MUX_R3)\r\nreturn 64 + add_a;\r\nelse\r\nreturn ~0;\r\n}\r\nstatic bool\r\nlive_reg_is_upper_half(uint32_t lri)\r\n{\r\nreturn (lri >= 16 && lri < 32) ||\r\n(lri >= 32 + 16 && lri < 32 + 32);\r\n}\r\nstatic bool\r\nis_tmu_submit(uint32_t waddr)\r\n{\r\nreturn (waddr == QPU_W_TMU0_S ||\r\nwaddr == QPU_W_TMU1_S);\r\n}\r\nstatic bool\r\nis_tmu_write(uint32_t waddr)\r\n{\r\nreturn (waddr >= QPU_W_TMU0_S &&\r\nwaddr <= QPU_W_TMU1_B);\r\n}\r\nstatic bool\r\nrecord_texture_sample(struct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state,\r\nint tmu)\r\n{\r\nuint32_t s = validated_shader->num_texture_samples;\r\nint i;\r\nstruct vc4_texture_sample_info *temp_samples;\r\ntemp_samples = krealloc(validated_shader->texture_samples,\r\n(s + 1) * sizeof(*temp_samples),\r\nGFP_KERNEL);\r\nif (!temp_samples)\r\nreturn false;\r\nmemcpy(&temp_samples[s],\r\n&validation_state->tmu_setup[tmu],\r\nsizeof(*temp_samples));\r\nvalidated_shader->num_texture_samples = s + 1;\r\nvalidated_shader->texture_samples = temp_samples;\r\nfor (i = 0; i < 4; i++)\r\nvalidation_state->tmu_setup[tmu].p_offset[i] = ~0;\r\nreturn true;\r\n}\r\nstatic bool\r\ncheck_tmu_write(struct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state,\r\nbool is_mul)\r\n{\r\nuint64_t inst = validation_state->shader[validation_state->ip];\r\nuint32_t waddr = (is_mul ?\r\nQPU_GET_FIELD(inst, QPU_WADDR_MUL) :\r\nQPU_GET_FIELD(inst, QPU_WADDR_ADD));\r\nuint32_t raddr_a = QPU_GET_FIELD(inst, QPU_RADDR_A);\r\nuint32_t raddr_b = QPU_GET_FIELD(inst, QPU_RADDR_B);\r\nint tmu = waddr > QPU_W_TMU0_B;\r\nbool submit = is_tmu_submit(waddr);\r\nbool is_direct = submit && validation_state->tmu_write_count[tmu] == 0;\r\nuint32_t sig = QPU_GET_FIELD(inst, QPU_SIG);\r\nif (is_direct) {\r\nuint32_t add_b = QPU_GET_FIELD(inst, QPU_ADD_B);\r\nuint32_t clamp_reg, clamp_offset;\r\nif (sig == QPU_SIG_SMALL_IMM) {\r\nDRM_ERROR("direct TMU read used small immediate\n");\r\nreturn false;\r\n}\r\nif (is_mul ||\r\nQPU_GET_FIELD(inst, QPU_OP_ADD) != QPU_A_ADD) {\r\nDRM_ERROR("direct TMU load wasn't an add\n");\r\nreturn false;\r\n}\r\nclamp_reg = raddr_add_a_to_live_reg_index(inst);\r\nif (clamp_reg == ~0) {\r\nDRM_ERROR("direct TMU load wasn't clamped\n");\r\nreturn false;\r\n}\r\nclamp_offset = validation_state->live_min_clamp_offsets[clamp_reg];\r\nif (clamp_offset == ~0) {\r\nDRM_ERROR("direct TMU load wasn't clamped\n");\r\nreturn false;\r\n}\r\nvalidation_state->tmu_setup[tmu].p_offset[1] =\r\nclamp_offset;\r\nif (!(add_b == QPU_MUX_A && raddr_a == QPU_R_UNIF) &&\r\n!(add_b == QPU_MUX_B && raddr_b == QPU_R_UNIF)) {\r\nDRM_ERROR("direct TMU load didn't add to a uniform\n");\r\nreturn false;\r\n}\r\nvalidation_state->tmu_setup[tmu].is_direct = true;\r\n} else {\r\nif (raddr_a == QPU_R_UNIF || (sig != QPU_SIG_SMALL_IMM &&\r\nraddr_b == QPU_R_UNIF)) {\r\nDRM_ERROR("uniform read in the same instruction as "\r\n"texture setup.\n");\r\nreturn false;\r\n}\r\n}\r\nif (validation_state->tmu_write_count[tmu] >= 4) {\r\nDRM_ERROR("TMU%d got too many parameters before dispatch\n",\r\ntmu);\r\nreturn false;\r\n}\r\nvalidation_state->tmu_setup[tmu].p_offset[validation_state->tmu_write_count[tmu]] =\r\nvalidated_shader->uniforms_size;\r\nvalidation_state->tmu_write_count[tmu]++;\r\nif (!is_direct) {\r\nif (validation_state->needs_uniform_address_update) {\r\nDRM_ERROR("Texturing with undefined uniform address\n");\r\nreturn false;\r\n}\r\nvalidated_shader->uniforms_size += 4;\r\n}\r\nif (submit) {\r\nif (!record_texture_sample(validated_shader,\r\nvalidation_state, tmu)) {\r\nreturn false;\r\n}\r\nvalidation_state->tmu_write_count[tmu] = 0;\r\n}\r\nreturn true;\r\n}\r\nstatic bool require_uniform_address_uniform(struct vc4_validated_shader_info *validated_shader)\r\n{\r\nuint32_t o = validated_shader->num_uniform_addr_offsets;\r\nuint32_t num_uniforms = validated_shader->uniforms_size / 4;\r\nvalidated_shader->uniform_addr_offsets =\r\nkrealloc(validated_shader->uniform_addr_offsets,\r\n(o + 1) *\r\nsizeof(*validated_shader->uniform_addr_offsets),\r\nGFP_KERNEL);\r\nif (!validated_shader->uniform_addr_offsets)\r\nreturn false;\r\nvalidated_shader->uniform_addr_offsets[o] = num_uniforms;\r\nvalidated_shader->num_uniform_addr_offsets++;\r\nreturn true;\r\n}\r\nstatic bool\r\nvalidate_uniform_address_write(struct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state,\r\nbool is_mul)\r\n{\r\nuint64_t inst = validation_state->shader[validation_state->ip];\r\nu32 add_b = QPU_GET_FIELD(inst, QPU_ADD_B);\r\nu32 raddr_a = QPU_GET_FIELD(inst, QPU_RADDR_A);\r\nu32 raddr_b = QPU_GET_FIELD(inst, QPU_RADDR_B);\r\nu32 add_lri = raddr_add_a_to_live_reg_index(inst);\r\nu32 expected_offset = validated_shader->uniforms_size + 4;\r\nswitch (QPU_GET_FIELD(inst, QPU_SIG)) {\r\ncase QPU_SIG_NONE:\r\ncase QPU_SIG_SCOREBOARD_UNLOCK:\r\ncase QPU_SIG_COLOR_LOAD:\r\ncase QPU_SIG_LOAD_TMU0:\r\ncase QPU_SIG_LOAD_TMU1:\r\nbreak;\r\ndefault:\r\nDRM_ERROR("uniforms address change must be "\r\n"normal math\n");\r\nreturn false;\r\n}\r\nif (is_mul || QPU_GET_FIELD(inst, QPU_OP_ADD) != QPU_A_ADD) {\r\nDRM_ERROR("Uniform address reset must be an ADD.\n");\r\nreturn false;\r\n}\r\nif (QPU_GET_FIELD(inst, QPU_COND_ADD) != QPU_COND_ALWAYS) {\r\nDRM_ERROR("Uniform address reset must be unconditional.\n");\r\nreturn false;\r\n}\r\nif (QPU_GET_FIELD(inst, QPU_PACK) != QPU_PACK_A_NOP &&\r\n!(inst & QPU_PM)) {\r\nDRM_ERROR("No packing allowed on uniforms reset\n");\r\nreturn false;\r\n}\r\nif (add_lri == -1) {\r\nDRM_ERROR("First argument of uniform address write must be "\r\n"an immediate value.\n");\r\nreturn false;\r\n}\r\nif (validation_state->live_immediates[add_lri] != expected_offset) {\r\nDRM_ERROR("Resetting uniforms with offset %db instead of %db\n",\r\nvalidation_state->live_immediates[add_lri],\r\nexpected_offset);\r\nreturn false;\r\n}\r\nif (!(add_b == QPU_MUX_A && raddr_a == QPU_R_UNIF) &&\r\n!(add_b == QPU_MUX_B && raddr_b == QPU_R_UNIF)) {\r\nDRM_ERROR("Second argument of uniform address write must be "\r\n"a uniform.\n");\r\nreturn false;\r\n}\r\nvalidation_state->needs_uniform_address_update = false;\r\nvalidation_state->needs_uniform_address_for_loop = false;\r\nreturn require_uniform_address_uniform(validated_shader);\r\n}\r\nstatic bool\r\ncheck_reg_write(struct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state,\r\nbool is_mul)\r\n{\r\nuint64_t inst = validation_state->shader[validation_state->ip];\r\nuint32_t waddr = (is_mul ?\r\nQPU_GET_FIELD(inst, QPU_WADDR_MUL) :\r\nQPU_GET_FIELD(inst, QPU_WADDR_ADD));\r\nuint32_t sig = QPU_GET_FIELD(inst, QPU_SIG);\r\nbool ws = inst & QPU_WS;\r\nbool is_b = is_mul ^ ws;\r\nu32 lri = waddr_to_live_reg_index(waddr, is_b);\r\nif (lri != -1) {\r\nuint32_t cond_add = QPU_GET_FIELD(inst, QPU_COND_ADD);\r\nuint32_t cond_mul = QPU_GET_FIELD(inst, QPU_COND_MUL);\r\nif (sig == QPU_SIG_LOAD_IMM &&\r\nQPU_GET_FIELD(inst, QPU_PACK) == QPU_PACK_A_NOP &&\r\n((is_mul && cond_mul == QPU_COND_ALWAYS) ||\r\n(!is_mul && cond_add == QPU_COND_ALWAYS))) {\r\nvalidation_state->live_immediates[lri] =\r\nQPU_GET_FIELD(inst, QPU_LOAD_IMM);\r\n} else {\r\nvalidation_state->live_immediates[lri] = ~0;\r\n}\r\nif (live_reg_is_upper_half(lri))\r\nvalidation_state->all_registers_used = true;\r\n}\r\nswitch (waddr) {\r\ncase QPU_W_UNIFORMS_ADDRESS:\r\nif (is_b) {\r\nDRM_ERROR("relative uniforms address change "\r\n"unsupported\n");\r\nreturn false;\r\n}\r\nreturn validate_uniform_address_write(validated_shader,\r\nvalidation_state,\r\nis_mul);\r\ncase QPU_W_TLB_COLOR_MS:\r\ncase QPU_W_TLB_COLOR_ALL:\r\ncase QPU_W_TLB_Z:\r\nreturn true;\r\ncase QPU_W_TMU0_S:\r\ncase QPU_W_TMU0_T:\r\ncase QPU_W_TMU0_R:\r\ncase QPU_W_TMU0_B:\r\ncase QPU_W_TMU1_S:\r\ncase QPU_W_TMU1_T:\r\ncase QPU_W_TMU1_R:\r\ncase QPU_W_TMU1_B:\r\nreturn check_tmu_write(validated_shader, validation_state,\r\nis_mul);\r\ncase QPU_W_HOST_INT:\r\ncase QPU_W_TMU_NOSWAP:\r\ncase QPU_W_TLB_ALPHA_MASK:\r\ncase QPU_W_MUTEX_RELEASE:\r\nDRM_ERROR("Unsupported waddr %d\n", waddr);\r\nreturn false;\r\ncase QPU_W_VPM_ADDR:\r\nDRM_ERROR("General VPM DMA unsupported\n");\r\nreturn false;\r\ncase QPU_W_VPM:\r\ncase QPU_W_VPMVCD_SETUP:\r\nreturn true;\r\ncase QPU_W_TLB_STENCIL_SETUP:\r\nreturn true;\r\n}\r\nreturn true;\r\n}\r\nstatic void\r\ntrack_live_clamps(struct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state)\r\n{\r\nuint64_t inst = validation_state->shader[validation_state->ip];\r\nuint32_t op_add = QPU_GET_FIELD(inst, QPU_OP_ADD);\r\nuint32_t waddr_add = QPU_GET_FIELD(inst, QPU_WADDR_ADD);\r\nuint32_t waddr_mul = QPU_GET_FIELD(inst, QPU_WADDR_MUL);\r\nuint32_t cond_add = QPU_GET_FIELD(inst, QPU_COND_ADD);\r\nuint32_t add_a = QPU_GET_FIELD(inst, QPU_ADD_A);\r\nuint32_t add_b = QPU_GET_FIELD(inst, QPU_ADD_B);\r\nuint32_t raddr_a = QPU_GET_FIELD(inst, QPU_RADDR_A);\r\nuint32_t raddr_b = QPU_GET_FIELD(inst, QPU_RADDR_B);\r\nuint32_t sig = QPU_GET_FIELD(inst, QPU_SIG);\r\nbool ws = inst & QPU_WS;\r\nuint32_t lri_add_a, lri_add, lri_mul;\r\nbool add_a_is_min_0;\r\nlri_add_a = raddr_add_a_to_live_reg_index(inst);\r\nadd_a_is_min_0 = (lri_add_a != ~0 &&\r\nvalidation_state->live_max_clamp_regs[lri_add_a]);\r\nlri_add = waddr_to_live_reg_index(waddr_add, ws);\r\nlri_mul = waddr_to_live_reg_index(waddr_mul, !ws);\r\nif (lri_mul != ~0) {\r\nvalidation_state->live_max_clamp_regs[lri_mul] = false;\r\nvalidation_state->live_min_clamp_offsets[lri_mul] = ~0;\r\n}\r\nif (lri_add != ~0) {\r\nvalidation_state->live_max_clamp_regs[lri_add] = false;\r\nvalidation_state->live_min_clamp_offsets[lri_add] = ~0;\r\n} else {\r\nreturn;\r\n}\r\nif (cond_add != QPU_COND_ALWAYS)\r\nreturn;\r\nif (op_add == QPU_A_MAX) {\r\nif (sig != QPU_SIG_SMALL_IMM || raddr_b != 0 ||\r\n(add_a != QPU_MUX_B && add_b != QPU_MUX_B)) {\r\nreturn;\r\n}\r\nvalidation_state->live_max_clamp_regs[lri_add] = true;\r\n} else if (op_add == QPU_A_MIN) {\r\nif (!add_a_is_min_0)\r\nreturn;\r\nif (!(add_b == QPU_MUX_A && raddr_a == QPU_R_UNIF) &&\r\n!(add_b == QPU_MUX_B && raddr_b == QPU_R_UNIF &&\r\nsig != QPU_SIG_SMALL_IMM)) {\r\nreturn;\r\n}\r\nvalidation_state->live_min_clamp_offsets[lri_add] =\r\nvalidated_shader->uniforms_size;\r\n}\r\n}\r\nstatic bool\r\ncheck_instruction_writes(struct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state)\r\n{\r\nuint64_t inst = validation_state->shader[validation_state->ip];\r\nuint32_t waddr_add = QPU_GET_FIELD(inst, QPU_WADDR_ADD);\r\nuint32_t waddr_mul = QPU_GET_FIELD(inst, QPU_WADDR_MUL);\r\nbool ok;\r\nif (is_tmu_write(waddr_add) && is_tmu_write(waddr_mul)) {\r\nDRM_ERROR("ADD and MUL both set up textures\n");\r\nreturn false;\r\n}\r\nok = (check_reg_write(validated_shader, validation_state, false) &&\r\ncheck_reg_write(validated_shader, validation_state, true));\r\ntrack_live_clamps(validated_shader, validation_state);\r\nreturn ok;\r\n}\r\nstatic bool\r\ncheck_branch(uint64_t inst,\r\nstruct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state,\r\nint ip)\r\n{\r\nint32_t branch_imm = QPU_GET_FIELD(inst, QPU_BRANCH_TARGET);\r\nuint32_t waddr_add = QPU_GET_FIELD(inst, QPU_WADDR_ADD);\r\nuint32_t waddr_mul = QPU_GET_FIELD(inst, QPU_WADDR_MUL);\r\nif ((int)branch_imm < 0)\r\nvalidation_state->needs_uniform_address_for_loop = true;\r\nif (waddr_add != QPU_W_NOP || waddr_mul != QPU_W_NOP) {\r\nDRM_ERROR("branch instruction at %d wrote a register.\n",\r\nvalidation_state->ip);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic bool\r\ncheck_instruction_reads(struct vc4_validated_shader_info *validated_shader,\r\nstruct vc4_shader_validation_state *validation_state)\r\n{\r\nuint64_t inst = validation_state->shader[validation_state->ip];\r\nuint32_t raddr_a = QPU_GET_FIELD(inst, QPU_RADDR_A);\r\nuint32_t raddr_b = QPU_GET_FIELD(inst, QPU_RADDR_B);\r\nuint32_t sig = QPU_GET_FIELD(inst, QPU_SIG);\r\nif (raddr_a == QPU_R_UNIF ||\r\n(raddr_b == QPU_R_UNIF && sig != QPU_SIG_SMALL_IMM)) {\r\nvalidated_shader->uniforms_size += 4;\r\nif (validation_state->needs_uniform_address_update) {\r\nDRM_ERROR("Uniform read with undefined uniform "\r\n"address\n");\r\nreturn false;\r\n}\r\n}\r\nif ((raddr_a >= 16 && raddr_a < 32) ||\r\n(raddr_b >= 16 && raddr_b < 32 && sig != QPU_SIG_SMALL_IMM)) {\r\nvalidation_state->all_registers_used = true;\r\n}\r\nreturn true;\r\n}\r\nstatic bool\r\nvc4_validate_branches(struct vc4_shader_validation_state *validation_state)\r\n{\r\nuint32_t max_branch_target = 0;\r\nint ip;\r\nint last_branch = -2;\r\nfor (ip = 0; ip < validation_state->max_ip; ip++) {\r\nuint64_t inst = validation_state->shader[ip];\r\nint32_t branch_imm = QPU_GET_FIELD(inst, QPU_BRANCH_TARGET);\r\nuint32_t sig = QPU_GET_FIELD(inst, QPU_SIG);\r\nuint32_t after_delay_ip = ip + 4;\r\nuint32_t branch_target_ip;\r\nif (sig == QPU_SIG_PROG_END) {\r\nvalidation_state->max_ip = ip + 3;\r\ncontinue;\r\n}\r\nif (sig != QPU_SIG_BRANCH)\r\ncontinue;\r\nif (ip - last_branch < 4) {\r\nDRM_ERROR("Branch at %d during delay slots\n", ip);\r\nreturn false;\r\n}\r\nlast_branch = ip;\r\nif (inst & QPU_BRANCH_REG) {\r\nDRM_ERROR("branching from register relative "\r\n"not supported\n");\r\nreturn false;\r\n}\r\nif (!(inst & QPU_BRANCH_REL)) {\r\nDRM_ERROR("relative branching required\n");\r\nreturn false;\r\n}\r\nif (branch_imm % sizeof(inst) != 0) {\r\nDRM_ERROR("branch target not aligned\n");\r\nreturn false;\r\n}\r\nbranch_target_ip = after_delay_ip + (branch_imm >> 3);\r\nif (branch_target_ip >= validation_state->max_ip) {\r\nDRM_ERROR("Branch at %d outside of shader (ip %d/%d)\n",\r\nip, branch_target_ip,\r\nvalidation_state->max_ip);\r\nreturn false;\r\n}\r\nset_bit(branch_target_ip, validation_state->branch_targets);\r\nif (after_delay_ip >= validation_state->max_ip) {\r\nDRM_ERROR("Branch at %d continues past shader end "\r\n"(%d/%d)\n",\r\nip, after_delay_ip, validation_state->max_ip);\r\nreturn false;\r\n}\r\nset_bit(after_delay_ip, validation_state->branch_targets);\r\nmax_branch_target = max(max_branch_target, after_delay_ip);\r\n}\r\nif (max_branch_target > validation_state->max_ip - 3) {\r\nDRM_ERROR("Branch landed after QPU_SIG_PROG_END");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic void\r\nreset_validation_state(struct vc4_shader_validation_state *validation_state)\r\n{\r\nint i;\r\nfor (i = 0; i < 8; i++)\r\nvalidation_state->tmu_setup[i / 4].p_offset[i % 4] = ~0;\r\nfor (i = 0; i < LIVE_REG_COUNT; i++) {\r\nvalidation_state->live_min_clamp_offsets[i] = ~0;\r\nvalidation_state->live_max_clamp_regs[i] = false;\r\nvalidation_state->live_immediates[i] = ~0;\r\n}\r\n}\r\nstatic bool\r\ntexturing_in_progress(struct vc4_shader_validation_state *validation_state)\r\n{\r\nreturn (validation_state->tmu_write_count[0] != 0 ||\r\nvalidation_state->tmu_write_count[1] != 0);\r\n}\r\nstatic bool\r\nvc4_handle_branch_target(struct vc4_shader_validation_state *validation_state)\r\n{\r\nuint32_t ip = validation_state->ip;\r\nif (!test_bit(ip, validation_state->branch_targets))\r\nreturn true;\r\nif (texturing_in_progress(validation_state)) {\r\nDRM_ERROR("Branch target landed during TMU setup\n");\r\nreturn false;\r\n}\r\nreset_validation_state(validation_state);\r\nvalidation_state->needs_uniform_address_update = true;\r\nreturn true;\r\n}\r\nstruct vc4_validated_shader_info *\r\nvc4_validate_shader(struct drm_gem_cma_object *shader_obj)\r\n{\r\nbool found_shader_end = false;\r\nint shader_end_ip = 0;\r\nuint32_t last_thread_switch_ip = -3;\r\nuint32_t ip;\r\nstruct vc4_validated_shader_info *validated_shader = NULL;\r\nstruct vc4_shader_validation_state validation_state;\r\nmemset(&validation_state, 0, sizeof(validation_state));\r\nvalidation_state.shader = shader_obj->vaddr;\r\nvalidation_state.max_ip = shader_obj->base.size / sizeof(uint64_t);\r\nreset_validation_state(&validation_state);\r\nvalidation_state.branch_targets =\r\nkcalloc(BITS_TO_LONGS(validation_state.max_ip),\r\nsizeof(unsigned long), GFP_KERNEL);\r\nif (!validation_state.branch_targets)\r\ngoto fail;\r\nvalidated_shader = kcalloc(1, sizeof(*validated_shader), GFP_KERNEL);\r\nif (!validated_shader)\r\ngoto fail;\r\nif (!vc4_validate_branches(&validation_state))\r\ngoto fail;\r\nfor (ip = 0; ip < validation_state.max_ip; ip++) {\r\nuint64_t inst = validation_state.shader[ip];\r\nuint32_t sig = QPU_GET_FIELD(inst, QPU_SIG);\r\nvalidation_state.ip = ip;\r\nif (!vc4_handle_branch_target(&validation_state))\r\ngoto fail;\r\nif (ip == last_thread_switch_ip + 3) {\r\nint i;\r\nfor (i = 64; i < LIVE_REG_COUNT; i++) {\r\nvalidation_state.live_min_clamp_offsets[i] = ~0;\r\nvalidation_state.live_max_clamp_regs[i] = false;\r\nvalidation_state.live_immediates[i] = ~0;\r\n}\r\n}\r\nswitch (sig) {\r\ncase QPU_SIG_NONE:\r\ncase QPU_SIG_WAIT_FOR_SCOREBOARD:\r\ncase QPU_SIG_SCOREBOARD_UNLOCK:\r\ncase QPU_SIG_COLOR_LOAD:\r\ncase QPU_SIG_LOAD_TMU0:\r\ncase QPU_SIG_LOAD_TMU1:\r\ncase QPU_SIG_PROG_END:\r\ncase QPU_SIG_SMALL_IMM:\r\ncase QPU_SIG_THREAD_SWITCH:\r\ncase QPU_SIG_LAST_THREAD_SWITCH:\r\nif (!check_instruction_writes(validated_shader,\r\n&validation_state)) {\r\nDRM_ERROR("Bad write at ip %d\n", ip);\r\ngoto fail;\r\n}\r\nif (!check_instruction_reads(validated_shader,\r\n&validation_state))\r\ngoto fail;\r\nif (sig == QPU_SIG_PROG_END) {\r\nfound_shader_end = true;\r\nshader_end_ip = ip;\r\n}\r\nif (sig == QPU_SIG_THREAD_SWITCH ||\r\nsig == QPU_SIG_LAST_THREAD_SWITCH) {\r\nvalidated_shader->is_threaded = true;\r\nif (ip < last_thread_switch_ip + 3) {\r\nDRM_ERROR("Thread switch too soon after "\r\n"last switch at ip %d\n", ip);\r\ngoto fail;\r\n}\r\nlast_thread_switch_ip = ip;\r\n}\r\nbreak;\r\ncase QPU_SIG_LOAD_IMM:\r\nif (!check_instruction_writes(validated_shader,\r\n&validation_state)) {\r\nDRM_ERROR("Bad LOAD_IMM write at ip %d\n", ip);\r\ngoto fail;\r\n}\r\nbreak;\r\ncase QPU_SIG_BRANCH:\r\nif (!check_branch(inst, validated_shader,\r\n&validation_state, ip))\r\ngoto fail;\r\nif (ip < last_thread_switch_ip + 3) {\r\nDRM_ERROR("Branch in thread switch at ip %d",\r\nip);\r\ngoto fail;\r\n}\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unsupported QPU signal %d at "\r\n"instruction %d\n", sig, ip);\r\ngoto fail;\r\n}\r\nif (found_shader_end && ip == shader_end_ip + 2)\r\nbreak;\r\n}\r\nif (ip == validation_state.max_ip) {\r\nDRM_ERROR("shader failed to terminate before "\r\n"shader BO end at %zd\n",\r\nshader_obj->base.size);\r\ngoto fail;\r\n}\r\nif (validated_shader->is_threaded &&\r\nvalidation_state.all_registers_used) {\r\nDRM_ERROR("Shader uses threading, but uses the upper "\r\n"half of the registers, too\n");\r\ngoto fail;\r\n}\r\nif (validation_state.needs_uniform_address_for_loop) {\r\nif (!require_uniform_address_uniform(validated_shader))\r\ngoto fail;\r\nvalidated_shader->uniforms_size += 4;\r\n}\r\nvalidated_shader->uniforms_src_size =\r\n(validated_shader->uniforms_size +\r\n4 * validated_shader->num_texture_samples);\r\nkfree(validation_state.branch_targets);\r\nreturn validated_shader;\r\nfail:\r\nkfree(validation_state.branch_targets);\r\nif (validated_shader) {\r\nkfree(validated_shader->texture_samples);\r\nkfree(validated_shader);\r\n}\r\nreturn NULL;\r\n}
