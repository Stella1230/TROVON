static int __glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,\r\nstruct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nvoid *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = 128 / 8;\r\nunsigned int nbytes, i, func_bytes;\r\nbool fpu_enabled = false;\r\nint err;\r\nerr = blkcipher_walk_virt(desc, walk);\r\nwhile ((nbytes = walk->nbytes)) {\r\nu8 *wsrc = walk->src.virt.addr;\r\nu8 *wdst = walk->dst.virt.addr;\r\nfpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,\r\ndesc, fpu_enabled, nbytes);\r\nfor (i = 0; i < gctx->num_funcs; i++) {\r\nfunc_bytes = bsize * gctx->funcs[i].num_blocks;\r\nif (nbytes >= func_bytes) {\r\ndo {\r\ngctx->funcs[i].fn_u.ecb(ctx, wdst,\r\nwsrc);\r\nwsrc += func_bytes;\r\nwdst += func_bytes;\r\nnbytes -= func_bytes;\r\n} while (nbytes >= func_bytes);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nerr = blkcipher_walk_done(desc, walk, nbytes);\r\n}\r\nglue_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nint glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,\r\nstruct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn __glue_ecb_crypt_128bit(gctx, desc, &walk);\r\n}\r\nstatic unsigned int __glue_cbc_encrypt_128bit(const common_glue_func_t fn,\r\nstruct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nvoid *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = 128 / 8;\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = (u128 *)walk->src.virt.addr;\r\nu128 *dst = (u128 *)walk->dst.virt.addr;\r\nu128 *iv = (u128 *)walk->iv;\r\ndo {\r\nu128_xor(dst, src, iv);\r\nfn(ctx, (u8 *)dst, (u8 *)dst);\r\niv = dst;\r\nsrc += 1;\r\ndst += 1;\r\nnbytes -= bsize;\r\n} while (nbytes >= bsize);\r\n*(u128 *)walk->iv = *iv;\r\nreturn nbytes;\r\n}\r\nint glue_cbc_encrypt_128bit(const common_glue_func_t fn,\r\nstruct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nwhile ((nbytes = walk.nbytes)) {\r\nnbytes = __glue_cbc_encrypt_128bit(fn, desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nreturn err;\r\n}\r\nstatic unsigned int\r\n__glue_cbc_decrypt_128bit(const struct common_glue_ctx *gctx,\r\nstruct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nvoid *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = 128 / 8;\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = (u128 *)walk->src.virt.addr;\r\nu128 *dst = (u128 *)walk->dst.virt.addr;\r\nu128 last_iv;\r\nunsigned int num_blocks, func_bytes;\r\nunsigned int i;\r\nsrc += nbytes / bsize - 1;\r\ndst += nbytes / bsize - 1;\r\nlast_iv = *src;\r\nfor (i = 0; i < gctx->num_funcs; i++) {\r\nnum_blocks = gctx->funcs[i].num_blocks;\r\nfunc_bytes = bsize * num_blocks;\r\nif (nbytes >= func_bytes) {\r\ndo {\r\nnbytes -= func_bytes - bsize;\r\nsrc -= num_blocks - 1;\r\ndst -= num_blocks - 1;\r\ngctx->funcs[i].fn_u.cbc(ctx, dst, src);\r\nnbytes -= bsize;\r\nif (nbytes < bsize)\r\ngoto done;\r\nu128_xor(dst, dst, src - 1);\r\nsrc -= 1;\r\ndst -= 1;\r\n} while (nbytes >= func_bytes);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nu128_xor(dst, dst, (u128 *)walk->iv);\r\n*(u128 *)walk->iv = last_iv;\r\nreturn nbytes;\r\n}\r\nint glue_cbc_decrypt_128bit(const struct common_glue_ctx *gctx,\r\nstruct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = 128 / 8;\r\nbool fpu_enabled = false;\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nwhile ((nbytes = walk.nbytes)) {\r\nfpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,\r\ndesc, fpu_enabled, nbytes);\r\nnbytes = __glue_cbc_decrypt_128bit(gctx, desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nglue_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nstatic void glue_ctr_crypt_final_128bit(const common_glue_ctr_func_t fn_ctr,\r\nstruct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nvoid *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nu8 *src = (u8 *)walk->src.virt.addr;\r\nu8 *dst = (u8 *)walk->dst.virt.addr;\r\nunsigned int nbytes = walk->nbytes;\r\nle128 ctrblk;\r\nu128 tmp;\r\nbe128_to_le128(&ctrblk, (be128 *)walk->iv);\r\nmemcpy(&tmp, src, nbytes);\r\nfn_ctr(ctx, &tmp, &tmp, &ctrblk);\r\nmemcpy(dst, &tmp, nbytes);\r\nle128_to_be128((be128 *)walk->iv, &ctrblk);\r\n}\r\nstatic unsigned int __glue_ctr_crypt_128bit(const struct common_glue_ctx *gctx,\r\nstruct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nconst unsigned int bsize = 128 / 8;\r\nvoid *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = (u128 *)walk->src.virt.addr;\r\nu128 *dst = (u128 *)walk->dst.virt.addr;\r\nle128 ctrblk;\r\nunsigned int num_blocks, func_bytes;\r\nunsigned int i;\r\nbe128_to_le128(&ctrblk, (be128 *)walk->iv);\r\nfor (i = 0; i < gctx->num_funcs; i++) {\r\nnum_blocks = gctx->funcs[i].num_blocks;\r\nfunc_bytes = bsize * num_blocks;\r\nif (nbytes >= func_bytes) {\r\ndo {\r\ngctx->funcs[i].fn_u.ctr(ctx, dst, src, &ctrblk);\r\nsrc += num_blocks;\r\ndst += num_blocks;\r\nnbytes -= func_bytes;\r\n} while (nbytes >= func_bytes);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nle128_to_be128((be128 *)walk->iv, &ctrblk);\r\nreturn nbytes;\r\n}\r\nint glue_ctr_crypt_128bit(const struct common_glue_ctx *gctx,\r\nstruct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = 128 / 8;\r\nbool fpu_enabled = false;\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt_block(desc, &walk, bsize);\r\nwhile ((nbytes = walk.nbytes) >= bsize) {\r\nfpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,\r\ndesc, fpu_enabled, nbytes);\r\nnbytes = __glue_ctr_crypt_128bit(gctx, desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nglue_fpu_end(fpu_enabled);\r\nif (walk.nbytes) {\r\nglue_ctr_crypt_final_128bit(\r\ngctx->funcs[gctx->num_funcs - 1].fn_u.ctr, desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, 0);\r\n}\r\nreturn err;\r\n}\r\nstatic unsigned int __glue_xts_crypt_128bit(const struct common_glue_ctx *gctx,\r\nvoid *ctx,\r\nstruct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nconst unsigned int bsize = 128 / 8;\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = (u128 *)walk->src.virt.addr;\r\nu128 *dst = (u128 *)walk->dst.virt.addr;\r\nunsigned int num_blocks, func_bytes;\r\nunsigned int i;\r\nfor (i = 0; i < gctx->num_funcs; i++) {\r\nnum_blocks = gctx->funcs[i].num_blocks;\r\nfunc_bytes = bsize * num_blocks;\r\nif (nbytes >= func_bytes) {\r\ndo {\r\ngctx->funcs[i].fn_u.xts(ctx, dst, src,\r\n(le128 *)walk->iv);\r\nsrc += num_blocks;\r\ndst += num_blocks;\r\nnbytes -= func_bytes;\r\n} while (nbytes >= func_bytes);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nreturn nbytes;\r\n}\r\nstatic unsigned int __glue_xts_req_128bit(const struct common_glue_ctx *gctx,\r\nvoid *ctx,\r\nstruct skcipher_walk *walk)\r\n{\r\nconst unsigned int bsize = 128 / 8;\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = walk->src.virt.addr;\r\nu128 *dst = walk->dst.virt.addr;\r\nunsigned int num_blocks, func_bytes;\r\nunsigned int i;\r\nfor (i = 0; i < gctx->num_funcs; i++) {\r\nnum_blocks = gctx->funcs[i].num_blocks;\r\nfunc_bytes = bsize * num_blocks;\r\nif (nbytes >= func_bytes) {\r\ndo {\r\ngctx->funcs[i].fn_u.xts(ctx, dst, src,\r\nwalk->iv);\r\nsrc += num_blocks;\r\ndst += num_blocks;\r\nnbytes -= func_bytes;\r\n} while (nbytes >= func_bytes);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nreturn nbytes;\r\n}\r\nint glue_xts_crypt_128bit(const struct common_glue_ctx *gctx,\r\nstruct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes,\r\nvoid (*tweak_fn)(void *ctx, u8 *dst, const u8 *src),\r\nvoid *tweak_ctx, void *crypt_ctx)\r\n{\r\nconst unsigned int bsize = 128 / 8;\r\nbool fpu_enabled = false;\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nnbytes = walk.nbytes;\r\nif (!nbytes)\r\nreturn err;\r\nfpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,\r\ndesc, fpu_enabled,\r\nnbytes < bsize ? bsize : nbytes);\r\ntweak_fn(tweak_ctx, walk.iv, walk.iv);\r\nwhile (nbytes) {\r\nnbytes = __glue_xts_crypt_128bit(gctx, crypt_ctx, desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\nnbytes = walk.nbytes;\r\n}\r\nglue_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nint glue_xts_req_128bit(const struct common_glue_ctx *gctx,\r\nstruct skcipher_request *req,\r\ncommon_glue_func_t tweak_fn, void *tweak_ctx,\r\nvoid *crypt_ctx)\r\n{\r\nconst unsigned int bsize = 128 / 8;\r\nstruct skcipher_walk walk;\r\nbool fpu_enabled = false;\r\nunsigned int nbytes;\r\nint err;\r\nerr = skcipher_walk_virt(&walk, req, false);\r\nnbytes = walk.nbytes;\r\nif (!nbytes)\r\nreturn err;\r\nfpu_enabled = glue_skwalk_fpu_begin(bsize, gctx->fpu_blocks_limit,\r\n&walk, fpu_enabled,\r\nnbytes < bsize ? bsize : nbytes);\r\ntweak_fn(tweak_ctx, walk.iv, walk.iv);\r\nwhile (nbytes) {\r\nnbytes = __glue_xts_req_128bit(gctx, crypt_ctx, &walk);\r\nerr = skcipher_walk_done(&walk, nbytes);\r\nnbytes = walk.nbytes;\r\n}\r\nglue_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nvoid glue_xts_crypt_128bit_one(void *ctx, u128 *dst, const u128 *src, le128 *iv,\r\ncommon_glue_func_t fn)\r\n{\r\nle128 ivblk = *iv;\r\nle128_gf128mul_x_ble(iv, &ivblk);\r\nu128_xor(dst, src, (u128 *)&ivblk);\r\nfn(ctx, (u8 *)dst, (u8 *)dst);\r\nu128_xor(dst, dst, (u128 *)&ivblk);\r\n}
