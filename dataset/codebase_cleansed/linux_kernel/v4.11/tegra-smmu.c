static struct tegra_smmu_as *to_smmu_as(struct iommu_domain *dom)\r\n{\r\nreturn container_of(dom, struct tegra_smmu_as, domain);\r\n}\r\nstatic inline void smmu_writel(struct tegra_smmu *smmu, u32 value,\r\nunsigned long offset)\r\n{\r\nwritel(value, smmu->regs + offset);\r\n}\r\nstatic inline u32 smmu_readl(struct tegra_smmu *smmu, unsigned long offset)\r\n{\r\nreturn readl(smmu->regs + offset);\r\n}\r\nstatic unsigned int iova_pd_index(unsigned long iova)\r\n{\r\nreturn (iova >> SMMU_PDE_SHIFT) & (SMMU_NUM_PDE - 1);\r\n}\r\nstatic unsigned int iova_pt_index(unsigned long iova)\r\n{\r\nreturn (iova >> SMMU_PTE_SHIFT) & (SMMU_NUM_PTE - 1);\r\n}\r\nstatic bool smmu_dma_addr_valid(struct tegra_smmu *smmu, dma_addr_t addr)\r\n{\r\naddr >>= 12;\r\nreturn (addr & smmu->pfn_mask) == addr;\r\n}\r\nstatic dma_addr_t smmu_pde_to_dma(u32 pde)\r\n{\r\nreturn pde << 12;\r\n}\r\nstatic void smmu_flush_ptc_all(struct tegra_smmu *smmu)\r\n{\r\nsmmu_writel(smmu, SMMU_PTC_FLUSH_TYPE_ALL, SMMU_PTC_FLUSH);\r\n}\r\nstatic inline void smmu_flush_ptc(struct tegra_smmu *smmu, dma_addr_t dma,\r\nunsigned long offset)\r\n{\r\nu32 value;\r\noffset &= ~(smmu->mc->soc->atom_size - 1);\r\nif (smmu->mc->soc->num_address_bits > 32) {\r\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\r\nvalue = (dma >> 32) & SMMU_PTC_FLUSH_HI_MASK;\r\n#else\r\nvalue = 0;\r\n#endif\r\nsmmu_writel(smmu, value, SMMU_PTC_FLUSH_HI);\r\n}\r\nvalue = (dma + offset) | SMMU_PTC_FLUSH_TYPE_ADR;\r\nsmmu_writel(smmu, value, SMMU_PTC_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb(struct tegra_smmu *smmu)\r\n{\r\nsmmu_writel(smmu, SMMU_TLB_FLUSH_VA_MATCH_ALL, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb_asid(struct tegra_smmu *smmu,\r\nunsigned long asid)\r\n{\r\nu32 value;\r\nvalue = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |\r\nSMMU_TLB_FLUSH_VA_MATCH_ALL;\r\nsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb_section(struct tegra_smmu *smmu,\r\nunsigned long asid,\r\nunsigned long iova)\r\n{\r\nu32 value;\r\nvalue = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |\r\nSMMU_TLB_FLUSH_VA_SECTION(iova);\r\nsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb_group(struct tegra_smmu *smmu,\r\nunsigned long asid,\r\nunsigned long iova)\r\n{\r\nu32 value;\r\nvalue = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |\r\nSMMU_TLB_FLUSH_VA_GROUP(iova);\r\nsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush(struct tegra_smmu *smmu)\r\n{\r\nsmmu_readl(smmu, SMMU_CONFIG);\r\n}\r\nstatic int tegra_smmu_alloc_asid(struct tegra_smmu *smmu, unsigned int *idp)\r\n{\r\nunsigned long id;\r\nmutex_lock(&smmu->lock);\r\nid = find_first_zero_bit(smmu->asids, smmu->soc->num_asids);\r\nif (id >= smmu->soc->num_asids) {\r\nmutex_unlock(&smmu->lock);\r\nreturn -ENOSPC;\r\n}\r\nset_bit(id, smmu->asids);\r\n*idp = id;\r\nmutex_unlock(&smmu->lock);\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_free_asid(struct tegra_smmu *smmu, unsigned int id)\r\n{\r\nmutex_lock(&smmu->lock);\r\nclear_bit(id, smmu->asids);\r\nmutex_unlock(&smmu->lock);\r\n}\r\nstatic bool tegra_smmu_capable(enum iommu_cap cap)\r\n{\r\nreturn false;\r\n}\r\nstatic struct iommu_domain *tegra_smmu_domain_alloc(unsigned type)\r\n{\r\nstruct tegra_smmu_as *as;\r\nif (type != IOMMU_DOMAIN_UNMANAGED)\r\nreturn NULL;\r\nas = kzalloc(sizeof(*as), GFP_KERNEL);\r\nif (!as)\r\nreturn NULL;\r\nas->attr = SMMU_PD_READABLE | SMMU_PD_WRITABLE | SMMU_PD_NONSECURE;\r\nas->pd = alloc_page(GFP_KERNEL | __GFP_DMA | __GFP_ZERO);\r\nif (!as->pd) {\r\nkfree(as);\r\nreturn NULL;\r\n}\r\nas->count = kcalloc(SMMU_NUM_PDE, sizeof(u32), GFP_KERNEL);\r\nif (!as->count) {\r\n__free_page(as->pd);\r\nkfree(as);\r\nreturn NULL;\r\n}\r\nas->pts = kcalloc(SMMU_NUM_PDE, sizeof(*as->pts), GFP_KERNEL);\r\nif (!as->pts) {\r\nkfree(as->count);\r\n__free_page(as->pd);\r\nkfree(as);\r\nreturn NULL;\r\n}\r\nas->domain.geometry.aperture_start = 0;\r\nas->domain.geometry.aperture_end = 0xffffffff;\r\nas->domain.geometry.force_aperture = true;\r\nreturn &as->domain;\r\n}\r\nstatic void tegra_smmu_domain_free(struct iommu_domain *domain)\r\n{\r\nstruct tegra_smmu_as *as = to_smmu_as(domain);\r\nkfree(as);\r\n}\r\nstatic const struct tegra_smmu_swgroup *\r\ntegra_smmu_find_swgroup(struct tegra_smmu *smmu, unsigned int swgroup)\r\n{\r\nconst struct tegra_smmu_swgroup *group = NULL;\r\nunsigned int i;\r\nfor (i = 0; i < smmu->soc->num_swgroups; i++) {\r\nif (smmu->soc->swgroups[i].swgroup == swgroup) {\r\ngroup = &smmu->soc->swgroups[i];\r\nbreak;\r\n}\r\n}\r\nreturn group;\r\n}\r\nstatic void tegra_smmu_enable(struct tegra_smmu *smmu, unsigned int swgroup,\r\nunsigned int asid)\r\n{\r\nconst struct tegra_smmu_swgroup *group;\r\nunsigned int i;\r\nu32 value;\r\nfor (i = 0; i < smmu->soc->num_clients; i++) {\r\nconst struct tegra_mc_client *client = &smmu->soc->clients[i];\r\nif (client->swgroup != swgroup)\r\ncontinue;\r\nvalue = smmu_readl(smmu, client->smmu.reg);\r\nvalue |= BIT(client->smmu.bit);\r\nsmmu_writel(smmu, value, client->smmu.reg);\r\n}\r\ngroup = tegra_smmu_find_swgroup(smmu, swgroup);\r\nif (group) {\r\nvalue = smmu_readl(smmu, group->reg);\r\nvalue &= ~SMMU_ASID_MASK;\r\nvalue |= SMMU_ASID_VALUE(asid);\r\nvalue |= SMMU_ASID_ENABLE;\r\nsmmu_writel(smmu, value, group->reg);\r\n}\r\n}\r\nstatic void tegra_smmu_disable(struct tegra_smmu *smmu, unsigned int swgroup,\r\nunsigned int asid)\r\n{\r\nconst struct tegra_smmu_swgroup *group;\r\nunsigned int i;\r\nu32 value;\r\ngroup = tegra_smmu_find_swgroup(smmu, swgroup);\r\nif (group) {\r\nvalue = smmu_readl(smmu, group->reg);\r\nvalue &= ~SMMU_ASID_MASK;\r\nvalue |= SMMU_ASID_VALUE(asid);\r\nvalue &= ~SMMU_ASID_ENABLE;\r\nsmmu_writel(smmu, value, group->reg);\r\n}\r\nfor (i = 0; i < smmu->soc->num_clients; i++) {\r\nconst struct tegra_mc_client *client = &smmu->soc->clients[i];\r\nif (client->swgroup != swgroup)\r\ncontinue;\r\nvalue = smmu_readl(smmu, client->smmu.reg);\r\nvalue &= ~BIT(client->smmu.bit);\r\nsmmu_writel(smmu, value, client->smmu.reg);\r\n}\r\n}\r\nstatic int tegra_smmu_as_prepare(struct tegra_smmu *smmu,\r\nstruct tegra_smmu_as *as)\r\n{\r\nu32 value;\r\nint err;\r\nif (as->use_count > 0) {\r\nas->use_count++;\r\nreturn 0;\r\n}\r\nas->pd_dma = dma_map_page(smmu->dev, as->pd, 0, SMMU_SIZE_PD,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(smmu->dev, as->pd_dma))\r\nreturn -ENOMEM;\r\nif (!smmu_dma_addr_valid(smmu, as->pd_dma)) {\r\nerr = -ENOMEM;\r\ngoto err_unmap;\r\n}\r\nerr = tegra_smmu_alloc_asid(smmu, &as->id);\r\nif (err < 0)\r\ngoto err_unmap;\r\nsmmu_flush_ptc(smmu, as->pd_dma, 0);\r\nsmmu_flush_tlb_asid(smmu, as->id);\r\nsmmu_writel(smmu, as->id & 0x7f, SMMU_PTB_ASID);\r\nvalue = SMMU_PTB_DATA_VALUE(as->pd_dma, as->attr);\r\nsmmu_writel(smmu, value, SMMU_PTB_DATA);\r\nsmmu_flush(smmu);\r\nas->smmu = smmu;\r\nas->use_count++;\r\nreturn 0;\r\nerr_unmap:\r\ndma_unmap_page(smmu->dev, as->pd_dma, SMMU_SIZE_PD, DMA_TO_DEVICE);\r\nreturn err;\r\n}\r\nstatic void tegra_smmu_as_unprepare(struct tegra_smmu *smmu,\r\nstruct tegra_smmu_as *as)\r\n{\r\nif (--as->use_count > 0)\r\nreturn;\r\ntegra_smmu_free_asid(smmu, as->id);\r\ndma_unmap_page(smmu->dev, as->pd_dma, SMMU_SIZE_PD, DMA_TO_DEVICE);\r\nas->smmu = NULL;\r\n}\r\nstatic int tegra_smmu_attach_dev(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct tegra_smmu *smmu = dev->archdata.iommu;\r\nstruct tegra_smmu_as *as = to_smmu_as(domain);\r\nstruct device_node *np = dev->of_node;\r\nstruct of_phandle_args args;\r\nunsigned int index = 0;\r\nint err = 0;\r\nwhile (!of_parse_phandle_with_args(np, "iommus", "#iommu-cells", index,\r\n&args)) {\r\nunsigned int swgroup = args.args[0];\r\nif (args.np != smmu->dev->of_node) {\r\nof_node_put(args.np);\r\ncontinue;\r\n}\r\nof_node_put(args.np);\r\nerr = tegra_smmu_as_prepare(smmu, as);\r\nif (err < 0)\r\nreturn err;\r\ntegra_smmu_enable(smmu, swgroup, as->id);\r\nindex++;\r\n}\r\nif (index == 0)\r\nreturn -ENODEV;\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_detach_dev(struct iommu_domain *domain, struct device *dev)\r\n{\r\nstruct tegra_smmu_as *as = to_smmu_as(domain);\r\nstruct device_node *np = dev->of_node;\r\nstruct tegra_smmu *smmu = as->smmu;\r\nstruct of_phandle_args args;\r\nunsigned int index = 0;\r\nwhile (!of_parse_phandle_with_args(np, "iommus", "#iommu-cells", index,\r\n&args)) {\r\nunsigned int swgroup = args.args[0];\r\nif (args.np != smmu->dev->of_node) {\r\nof_node_put(args.np);\r\ncontinue;\r\n}\r\nof_node_put(args.np);\r\ntegra_smmu_disable(smmu, swgroup, as->id);\r\ntegra_smmu_as_unprepare(smmu, as);\r\nindex++;\r\n}\r\n}\r\nstatic void tegra_smmu_set_pde(struct tegra_smmu_as *as, unsigned long iova,\r\nu32 value)\r\n{\r\nunsigned int pd_index = iova_pd_index(iova);\r\nstruct tegra_smmu *smmu = as->smmu;\r\nu32 *pd = page_address(as->pd);\r\nunsigned long offset = pd_index * sizeof(*pd);\r\npd[pd_index] = value;\r\ndma_sync_single_range_for_device(smmu->dev, as->pd_dma, offset,\r\nsizeof(*pd), DMA_TO_DEVICE);\r\nsmmu_flush_ptc(smmu, as->pd_dma, offset);\r\nsmmu_flush_tlb_section(smmu, as->id, iova);\r\nsmmu_flush(smmu);\r\n}\r\nstatic u32 *tegra_smmu_pte_offset(struct page *pt_page, unsigned long iova)\r\n{\r\nu32 *pt = page_address(pt_page);\r\nreturn pt + iova_pt_index(iova);\r\n}\r\nstatic u32 *tegra_smmu_pte_lookup(struct tegra_smmu_as *as, unsigned long iova,\r\ndma_addr_t *dmap)\r\n{\r\nunsigned int pd_index = iova_pd_index(iova);\r\nstruct page *pt_page;\r\nu32 *pd;\r\npt_page = as->pts[pd_index];\r\nif (!pt_page)\r\nreturn NULL;\r\npd = page_address(as->pd);\r\n*dmap = smmu_pde_to_dma(pd[pd_index]);\r\nreturn tegra_smmu_pte_offset(pt_page, iova);\r\n}\r\nstatic u32 *as_get_pte(struct tegra_smmu_as *as, dma_addr_t iova,\r\ndma_addr_t *dmap)\r\n{\r\nunsigned int pde = iova_pd_index(iova);\r\nstruct tegra_smmu *smmu = as->smmu;\r\nif (!as->pts[pde]) {\r\nstruct page *page;\r\ndma_addr_t dma;\r\npage = alloc_page(GFP_KERNEL | __GFP_DMA | __GFP_ZERO);\r\nif (!page)\r\nreturn NULL;\r\ndma = dma_map_page(smmu->dev, page, 0, SMMU_SIZE_PT,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(smmu->dev, dma)) {\r\n__free_page(page);\r\nreturn NULL;\r\n}\r\nif (!smmu_dma_addr_valid(smmu, dma)) {\r\ndma_unmap_page(smmu->dev, dma, SMMU_SIZE_PT,\r\nDMA_TO_DEVICE);\r\n__free_page(page);\r\nreturn NULL;\r\n}\r\nas->pts[pde] = page;\r\ntegra_smmu_set_pde(as, iova, SMMU_MK_PDE(dma, SMMU_PDE_ATTR |\r\nSMMU_PDE_NEXT));\r\n*dmap = dma;\r\n} else {\r\nu32 *pd = page_address(as->pd);\r\n*dmap = smmu_pde_to_dma(pd[pde]);\r\n}\r\nreturn tegra_smmu_pte_offset(as->pts[pde], iova);\r\n}\r\nstatic void tegra_smmu_pte_get_use(struct tegra_smmu_as *as, unsigned long iova)\r\n{\r\nunsigned int pd_index = iova_pd_index(iova);\r\nas->count[pd_index]++;\r\n}\r\nstatic void tegra_smmu_pte_put_use(struct tegra_smmu_as *as, unsigned long iova)\r\n{\r\nunsigned int pde = iova_pd_index(iova);\r\nstruct page *page = as->pts[pde];\r\nif (--as->count[pde] == 0) {\r\nstruct tegra_smmu *smmu = as->smmu;\r\nu32 *pd = page_address(as->pd);\r\ndma_addr_t pte_dma = smmu_pde_to_dma(pd[pde]);\r\ntegra_smmu_set_pde(as, iova, 0);\r\ndma_unmap_page(smmu->dev, pte_dma, SMMU_SIZE_PT, DMA_TO_DEVICE);\r\n__free_page(page);\r\nas->pts[pde] = NULL;\r\n}\r\n}\r\nstatic void tegra_smmu_set_pte(struct tegra_smmu_as *as, unsigned long iova,\r\nu32 *pte, dma_addr_t pte_dma, u32 val)\r\n{\r\nstruct tegra_smmu *smmu = as->smmu;\r\nunsigned long offset = offset_in_page(pte);\r\n*pte = val;\r\ndma_sync_single_range_for_device(smmu->dev, pte_dma, offset,\r\n4, DMA_TO_DEVICE);\r\nsmmu_flush_ptc(smmu, pte_dma, offset);\r\nsmmu_flush_tlb_group(smmu, as->id, iova);\r\nsmmu_flush(smmu);\r\n}\r\nstatic int tegra_smmu_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct tegra_smmu_as *as = to_smmu_as(domain);\r\ndma_addr_t pte_dma;\r\nu32 *pte;\r\npte = as_get_pte(as, iova, &pte_dma);\r\nif (!pte)\r\nreturn -ENOMEM;\r\nif (*pte == 0)\r\ntegra_smmu_pte_get_use(as, iova);\r\ntegra_smmu_set_pte(as, iova, pte, pte_dma,\r\n__phys_to_pfn(paddr) | SMMU_PTE_ATTR);\r\nreturn 0;\r\n}\r\nstatic size_t tegra_smmu_unmap(struct iommu_domain *domain, unsigned long iova,\r\nsize_t size)\r\n{\r\nstruct tegra_smmu_as *as = to_smmu_as(domain);\r\ndma_addr_t pte_dma;\r\nu32 *pte;\r\npte = tegra_smmu_pte_lookup(as, iova, &pte_dma);\r\nif (!pte || !*pte)\r\nreturn 0;\r\ntegra_smmu_set_pte(as, iova, pte, pte_dma, 0);\r\ntegra_smmu_pte_put_use(as, iova);\r\nreturn size;\r\n}\r\nstatic phys_addr_t tegra_smmu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct tegra_smmu_as *as = to_smmu_as(domain);\r\nunsigned long pfn;\r\ndma_addr_t pte_dma;\r\nu32 *pte;\r\npte = tegra_smmu_pte_lookup(as, iova, &pte_dma);\r\nif (!pte || !*pte)\r\nreturn 0;\r\npfn = *pte & as->smmu->pfn_mask;\r\nreturn PFN_PHYS(pfn);\r\n}\r\nstatic struct tegra_smmu *tegra_smmu_find(struct device_node *np)\r\n{\r\nstruct platform_device *pdev;\r\nstruct tegra_mc *mc;\r\npdev = of_find_device_by_node(np);\r\nif (!pdev)\r\nreturn NULL;\r\nmc = platform_get_drvdata(pdev);\r\nif (!mc)\r\nreturn NULL;\r\nreturn mc->smmu;\r\n}\r\nstatic int tegra_smmu_add_device(struct device *dev)\r\n{\r\nstruct device_node *np = dev->of_node;\r\nstruct of_phandle_args args;\r\nunsigned int index = 0;\r\nwhile (of_parse_phandle_with_args(np, "iommus", "#iommu-cells", index,\r\n&args) == 0) {\r\nstruct tegra_smmu *smmu;\r\nsmmu = tegra_smmu_find(args.np);\r\nif (smmu) {\r\ndev->archdata.iommu = smmu;\r\nbreak;\r\n}\r\nindex++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_remove_device(struct device *dev)\r\n{\r\ndev->archdata.iommu = NULL;\r\n}\r\nstatic void tegra_smmu_ahb_enable(void)\r\n{\r\nstatic const struct of_device_id ahb_match[] = {\r\n{ .compatible = "nvidia,tegra30-ahb", },\r\n{ }\r\n};\r\nstruct device_node *ahb;\r\nahb = of_find_matching_node(NULL, ahb_match);\r\nif (ahb) {\r\ntegra_ahb_enable_smmu(ahb);\r\nof_node_put(ahb);\r\n}\r\n}\r\nstatic int tegra_smmu_swgroups_show(struct seq_file *s, void *data)\r\n{\r\nstruct tegra_smmu *smmu = s->private;\r\nunsigned int i;\r\nu32 value;\r\nseq_printf(s, "swgroup enabled ASID\n");\r\nseq_printf(s, "------------------------\n");\r\nfor (i = 0; i < smmu->soc->num_swgroups; i++) {\r\nconst struct tegra_smmu_swgroup *group = &smmu->soc->swgroups[i];\r\nconst char *status;\r\nunsigned int asid;\r\nvalue = smmu_readl(smmu, group->reg);\r\nif (value & SMMU_ASID_ENABLE)\r\nstatus = "yes";\r\nelse\r\nstatus = "no";\r\nasid = value & SMMU_ASID_MASK;\r\nseq_printf(s, "%-9s %-7s %#04x\n", group->name, status,\r\nasid);\r\n}\r\nreturn 0;\r\n}\r\nstatic int tegra_smmu_swgroups_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, tegra_smmu_swgroups_show, inode->i_private);\r\n}\r\nstatic int tegra_smmu_clients_show(struct seq_file *s, void *data)\r\n{\r\nstruct tegra_smmu *smmu = s->private;\r\nunsigned int i;\r\nu32 value;\r\nseq_printf(s, "client enabled\n");\r\nseq_printf(s, "--------------------\n");\r\nfor (i = 0; i < smmu->soc->num_clients; i++) {\r\nconst struct tegra_mc_client *client = &smmu->soc->clients[i];\r\nconst char *status;\r\nvalue = smmu_readl(smmu, client->smmu.reg);\r\nif (value & BIT(client->smmu.bit))\r\nstatus = "yes";\r\nelse\r\nstatus = "no";\r\nseq_printf(s, "%-12s %s\n", client->name, status);\r\n}\r\nreturn 0;\r\n}\r\nstatic int tegra_smmu_clients_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, tegra_smmu_clients_show, inode->i_private);\r\n}\r\nstatic void tegra_smmu_debugfs_init(struct tegra_smmu *smmu)\r\n{\r\nsmmu->debugfs = debugfs_create_dir("smmu", NULL);\r\nif (!smmu->debugfs)\r\nreturn;\r\ndebugfs_create_file("swgroups", S_IRUGO, smmu->debugfs, smmu,\r\n&tegra_smmu_swgroups_fops);\r\ndebugfs_create_file("clients", S_IRUGO, smmu->debugfs, smmu,\r\n&tegra_smmu_clients_fops);\r\n}\r\nstatic void tegra_smmu_debugfs_exit(struct tegra_smmu *smmu)\r\n{\r\ndebugfs_remove_recursive(smmu->debugfs);\r\n}\r\nstruct tegra_smmu *tegra_smmu_probe(struct device *dev,\r\nconst struct tegra_smmu_soc *soc,\r\nstruct tegra_mc *mc)\r\n{\r\nstruct tegra_smmu *smmu;\r\nsize_t size;\r\nu32 value;\r\nint err;\r\nif (!soc)\r\nreturn NULL;\r\nsmmu = devm_kzalloc(dev, sizeof(*smmu), GFP_KERNEL);\r\nif (!smmu)\r\nreturn ERR_PTR(-ENOMEM);\r\nmc->smmu = smmu;\r\nsize = BITS_TO_LONGS(soc->num_asids) * sizeof(long);\r\nsmmu->asids = devm_kzalloc(dev, size, GFP_KERNEL);\r\nif (!smmu->asids)\r\nreturn ERR_PTR(-ENOMEM);\r\nmutex_init(&smmu->lock);\r\nsmmu->regs = mc->regs;\r\nsmmu->soc = soc;\r\nsmmu->dev = dev;\r\nsmmu->mc = mc;\r\nsmmu->pfn_mask = BIT_MASK(mc->soc->num_address_bits - PAGE_SHIFT) - 1;\r\ndev_dbg(dev, "address bits: %u, PFN mask: %#lx\n",\r\nmc->soc->num_address_bits, smmu->pfn_mask);\r\nsmmu->tlb_mask = (smmu->soc->num_tlb_lines << 1) - 1;\r\ndev_dbg(dev, "TLB lines: %u, mask: %#lx\n", smmu->soc->num_tlb_lines,\r\nsmmu->tlb_mask);\r\nvalue = SMMU_PTC_CONFIG_ENABLE | SMMU_PTC_CONFIG_INDEX_MAP(0x3f);\r\nif (soc->supports_request_limit)\r\nvalue |= SMMU_PTC_CONFIG_REQ_LIMIT(8);\r\nsmmu_writel(smmu, value, SMMU_PTC_CONFIG);\r\nvalue = SMMU_TLB_CONFIG_HIT_UNDER_MISS |\r\nSMMU_TLB_CONFIG_ACTIVE_LINES(smmu);\r\nif (soc->supports_round_robin_arbitration)\r\nvalue |= SMMU_TLB_CONFIG_ROUND_ROBIN_ARBITRATION;\r\nsmmu_writel(smmu, value, SMMU_TLB_CONFIG);\r\nsmmu_flush_ptc_all(smmu);\r\nsmmu_flush_tlb(smmu);\r\nsmmu_writel(smmu, SMMU_CONFIG_ENABLE, SMMU_CONFIG);\r\nsmmu_flush(smmu);\r\ntegra_smmu_ahb_enable();\r\nerr = bus_set_iommu(&platform_bus_type, &tegra_smmu_ops);\r\nif (err < 0)\r\nreturn ERR_PTR(err);\r\nif (IS_ENABLED(CONFIG_DEBUG_FS))\r\ntegra_smmu_debugfs_init(smmu);\r\nreturn smmu;\r\n}\r\nvoid tegra_smmu_remove(struct tegra_smmu *smmu)\r\n{\r\nif (IS_ENABLED(CONFIG_DEBUG_FS))\r\ntegra_smmu_debugfs_exit(smmu);\r\n}
