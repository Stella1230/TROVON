static bool moving_pred(struct keybuf *buf, struct bkey *k)\r\n{\r\nstruct cache_set *c = container_of(buf, struct cache_set,\r\nmoving_gc_keys);\r\nunsigned i;\r\nfor (i = 0; i < KEY_PTRS(k); i++)\r\nif (ptr_available(c, k, i) &&\r\nGC_MOVE(PTR_BUCKET(c, k, i)))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void moving_io_destructor(struct closure *cl)\r\n{\r\nstruct moving_io *io = container_of(cl, struct moving_io, cl);\r\nkfree(io);\r\n}\r\nstatic void write_moving_finish(struct closure *cl)\r\n{\r\nstruct moving_io *io = container_of(cl, struct moving_io, cl);\r\nstruct bio *bio = &io->bio.bio;\r\nbio_free_pages(bio);\r\nif (io->op.replace_collision)\r\ntrace_bcache_gc_copy_collision(&io->w->key);\r\nbch_keybuf_del(&io->op.c->moving_gc_keys, io->w);\r\nup(&io->op.c->moving_in_flight);\r\nclosure_return_with_destructor(cl, moving_io_destructor);\r\n}\r\nstatic void read_moving_endio(struct bio *bio)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nstruct moving_io *io = container_of(bio->bi_private,\r\nstruct moving_io, cl);\r\nif (bio->bi_error)\r\nio->op.error = bio->bi_error;\r\nelse if (!KEY_DIRTY(&b->key) &&\r\nptr_stale(io->op.c, &b->key, 0)) {\r\nio->op.error = -EINTR;\r\n}\r\nbch_bbio_endio(io->op.c, bio, bio->bi_error, "reading data to move");\r\n}\r\nstatic void moving_init(struct moving_io *io)\r\n{\r\nstruct bio *bio = &io->bio.bio;\r\nbio_init(bio, bio->bi_inline_vecs,\r\nDIV_ROUND_UP(KEY_SIZE(&io->w->key), PAGE_SECTORS));\r\nbio_get(bio);\r\nbio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));\r\nbio->bi_iter.bi_size = KEY_SIZE(&io->w->key) << 9;\r\nbio->bi_private = &io->cl;\r\nbch_bio_map(bio, NULL);\r\n}\r\nstatic void write_moving(struct closure *cl)\r\n{\r\nstruct moving_io *io = container_of(cl, struct moving_io, cl);\r\nstruct data_insert_op *op = &io->op;\r\nif (!op->error) {\r\nmoving_init(io);\r\nio->bio.bio.bi_iter.bi_sector = KEY_START(&io->w->key);\r\nop->write_prio = 1;\r\nop->bio = &io->bio.bio;\r\nop->writeback = KEY_DIRTY(&io->w->key);\r\nop->csum = KEY_CSUM(&io->w->key);\r\nbkey_copy(&op->replace_key, &io->w->key);\r\nop->replace = true;\r\nclosure_call(&op->cl, bch_data_insert, NULL, cl);\r\n}\r\ncontinue_at(cl, write_moving_finish, op->wq);\r\n}\r\nstatic void read_moving_submit(struct closure *cl)\r\n{\r\nstruct moving_io *io = container_of(cl, struct moving_io, cl);\r\nstruct bio *bio = &io->bio.bio;\r\nbch_submit_bbio(bio, io->op.c, &io->w->key, 0);\r\ncontinue_at(cl, write_moving, io->op.wq);\r\n}\r\nstatic void read_moving(struct cache_set *c)\r\n{\r\nstruct keybuf_key *w;\r\nstruct moving_io *io;\r\nstruct bio *bio;\r\nstruct closure cl;\r\nclosure_init_stack(&cl);\r\nwhile (!test_bit(CACHE_SET_STOPPING, &c->flags)) {\r\nw = bch_keybuf_next_rescan(c, &c->moving_gc_keys,\r\n&MAX_KEY, moving_pred);\r\nif (!w)\r\nbreak;\r\nif (ptr_stale(c, &w->key, 0)) {\r\nbch_keybuf_del(&c->moving_gc_keys, w);\r\ncontinue;\r\n}\r\nio = kzalloc(sizeof(struct moving_io) + sizeof(struct bio_vec)\r\n* DIV_ROUND_UP(KEY_SIZE(&w->key), PAGE_SECTORS),\r\nGFP_KERNEL);\r\nif (!io)\r\ngoto err;\r\nw->private = io;\r\nio->w = w;\r\nio->op.inode = KEY_INODE(&w->key);\r\nio->op.c = c;\r\nio->op.wq = c->moving_gc_wq;\r\nmoving_init(io);\r\nbio = &io->bio.bio;\r\nbio_set_op_attrs(bio, REQ_OP_READ, 0);\r\nbio->bi_end_io = read_moving_endio;\r\nif (bio_alloc_pages(bio, GFP_KERNEL))\r\ngoto err;\r\ntrace_bcache_gc_copy(&w->key);\r\ndown(&c->moving_in_flight);\r\nclosure_call(&io->cl, read_moving_submit, NULL, &cl);\r\n}\r\nif (0) {\r\nerr: if (!IS_ERR_OR_NULL(w->private))\r\nkfree(w->private);\r\nbch_keybuf_del(&c->moving_gc_keys, w);\r\n}\r\nclosure_sync(&cl);\r\n}\r\nstatic bool bucket_cmp(struct bucket *l, struct bucket *r)\r\n{\r\nreturn GC_SECTORS_USED(l) < GC_SECTORS_USED(r);\r\n}\r\nstatic unsigned bucket_heap_top(struct cache *ca)\r\n{\r\nstruct bucket *b;\r\nreturn (b = heap_peek(&ca->heap)) ? GC_SECTORS_USED(b) : 0;\r\n}\r\nvoid bch_moving_gc(struct cache_set *c)\r\n{\r\nstruct cache *ca;\r\nstruct bucket *b;\r\nunsigned i;\r\nif (!c->copy_gc_enabled)\r\nreturn;\r\nmutex_lock(&c->bucket_lock);\r\nfor_each_cache(ca, c, i) {\r\nunsigned sectors_to_move = 0;\r\nunsigned reserve_sectors = ca->sb.bucket_size *\r\nfifo_used(&ca->free[RESERVE_MOVINGGC]);\r\nca->heap.used = 0;\r\nfor_each_bucket(b, ca) {\r\nif (GC_MARK(b) == GC_MARK_METADATA ||\r\n!GC_SECTORS_USED(b) ||\r\nGC_SECTORS_USED(b) == ca->sb.bucket_size ||\r\natomic_read(&b->pin))\r\ncontinue;\r\nif (!heap_full(&ca->heap)) {\r\nsectors_to_move += GC_SECTORS_USED(b);\r\nheap_add(&ca->heap, b, bucket_cmp);\r\n} else if (bucket_cmp(b, heap_peek(&ca->heap))) {\r\nsectors_to_move -= bucket_heap_top(ca);\r\nsectors_to_move += GC_SECTORS_USED(b);\r\nca->heap.data[0] = b;\r\nheap_sift(&ca->heap, 0, bucket_cmp);\r\n}\r\n}\r\nwhile (sectors_to_move > reserve_sectors) {\r\nheap_pop(&ca->heap, b, bucket_cmp);\r\nsectors_to_move -= GC_SECTORS_USED(b);\r\n}\r\nwhile (heap_pop(&ca->heap, b, bucket_cmp))\r\nSET_GC_MOVE(b, 1);\r\n}\r\nmutex_unlock(&c->bucket_lock);\r\nc->moving_gc_keys.last_scanned = ZERO_KEY;\r\nread_moving(c);\r\n}\r\nvoid bch_moving_init_cache_set(struct cache_set *c)\r\n{\r\nbch_keybuf_init(&c->moving_gc_keys);\r\nsema_init(&c->moving_in_flight, 64);\r\n}
