static void eq_set_cons_index(struct hns_roce_eq *eq, int req_not)\r\n{\r\nroce_raw_write((eq->cons_index & CONS_INDEX_MASK) |\r\n(req_not << eq->log_entries), eq->doorbell);\r\nmb();\r\n}\r\nstatic struct hns_roce_aeqe *get_aeqe(struct hns_roce_eq *eq, u32 entry)\r\n{\r\nunsigned long off = (entry & (eq->entries - 1)) *\r\nHNS_ROCE_AEQ_ENTRY_SIZE;\r\nreturn (struct hns_roce_aeqe *)((u8 *)\r\n(eq->buf_list[off / HNS_ROCE_BA_SIZE].buf) +\r\noff % HNS_ROCE_BA_SIZE);\r\n}\r\nstatic struct hns_roce_aeqe *next_aeqe_sw(struct hns_roce_eq *eq)\r\n{\r\nstruct hns_roce_aeqe *aeqe = get_aeqe(eq, eq->cons_index);\r\nreturn (roce_get_bit(aeqe->asyn, HNS_ROCE_AEQE_U32_4_OWNER_S) ^\r\n!!(eq->cons_index & eq->entries)) ? aeqe : NULL;\r\n}\r\nstatic void hns_roce_wq_catas_err_handle(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_aeqe *aeqe, int qpn)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\ndev_warn(dev, "Local Work Queue Catastrophic Error.\n");\r\nswitch (roce_get_field(aeqe->asyn, HNS_ROCE_AEQE_U32_4_EVENT_SUB_TYPE_M,\r\nHNS_ROCE_AEQE_U32_4_EVENT_SUB_TYPE_S)) {\r\ncase HNS_ROCE_LWQCE_QPC_ERROR:\r\ndev_warn(dev, "QP %d, QPC error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LWQCE_MTU_ERROR:\r\ndev_warn(dev, "QP %d, MTU error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LWQCE_WQE_BA_ADDR_ERROR:\r\ndev_warn(dev, "QP %d, WQE BA addr error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LWQCE_WQE_ADDR_ERROR:\r\ndev_warn(dev, "QP %d, WQE addr error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LWQCE_SQ_WQE_SHIFT_ERROR:\r\ndev_warn(dev, "QP %d, WQE shift error\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LWQCE_SL_ERROR:\r\ndev_warn(dev, "QP %d, SL error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LWQCE_PORT_ERROR:\r\ndev_warn(dev, "QP %d, port error.\n", qpn);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void hns_roce_local_wq_access_err_handle(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_aeqe *aeqe,\r\nint qpn)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\ndev_warn(dev, "Local Access Violation Work Queue Error.\n");\r\nswitch (roce_get_field(aeqe->asyn, HNS_ROCE_AEQE_U32_4_EVENT_SUB_TYPE_M,\r\nHNS_ROCE_AEQE_U32_4_EVENT_SUB_TYPE_S)) {\r\ncase HNS_ROCE_LAVWQE_R_KEY_VIOLATION:\r\ndev_warn(dev, "QP %d, R_key violation.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LAVWQE_LENGTH_ERROR:\r\ndev_warn(dev, "QP %d, length error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LAVWQE_VA_ERROR:\r\ndev_warn(dev, "QP %d, VA error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LAVWQE_PD_ERROR:\r\ndev_err(dev, "QP %d, PD error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LAVWQE_RW_ACC_ERROR:\r\ndev_warn(dev, "QP %d, rw acc error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LAVWQE_KEY_STATE_ERROR:\r\ndev_warn(dev, "QP %d, key state error.\n", qpn);\r\nbreak;\r\ncase HNS_ROCE_LAVWQE_MR_OPERATION_ERROR:\r\ndev_warn(dev, "QP %d, MR operation error.\n", qpn);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void hns_roce_qp_err_handle(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_aeqe *aeqe,\r\nint event_type)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nint phy_port;\r\nint qpn;\r\nqpn = roce_get_field(aeqe->event.qp_event.qp,\r\nHNS_ROCE_AEQE_EVENT_QP_EVENT_QP_QPN_M,\r\nHNS_ROCE_AEQE_EVENT_QP_EVENT_QP_QPN_S);\r\nphy_port = roce_get_field(aeqe->event.qp_event.qp,\r\nHNS_ROCE_AEQE_EVENT_QP_EVENT_PORT_NUM_M,\r\nHNS_ROCE_AEQE_EVENT_QP_EVENT_PORT_NUM_S);\r\nif (qpn <= 1)\r\nqpn = HNS_ROCE_MAX_PORTS * qpn + phy_port;\r\nswitch (event_type) {\r\ncase HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:\r\ndev_warn(dev, "Invalid Req Local Work Queue Error.\n"\r\n"QP %d, phy_port %d.\n", qpn, phy_port);\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:\r\nhns_roce_wq_catas_err_handle(hr_dev, aeqe, qpn);\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:\r\nhns_roce_local_wq_access_err_handle(hr_dev, aeqe, qpn);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nhns_roce_qp_event(hr_dev, qpn, event_type);\r\n}\r\nstatic void hns_roce_cq_err_handle(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_aeqe *aeqe,\r\nint event_type)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nu32 cqn;\r\ncqn = le32_to_cpu(roce_get_field(aeqe->event.cq_event.cq,\r\nHNS_ROCE_AEQE_EVENT_CQ_EVENT_CQ_CQN_M,\r\nHNS_ROCE_AEQE_EVENT_CQ_EVENT_CQ_CQN_S));\r\nswitch (event_type) {\r\ncase HNS_ROCE_EVENT_TYPE_CQ_ACCESS_ERROR:\r\ndev_warn(dev, "CQ 0x%x access err.\n", cqn);\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_CQ_OVERFLOW:\r\ndev_warn(dev, "CQ 0x%x overflow\n", cqn);\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_CQ_ID_INVALID:\r\ndev_warn(dev, "CQ 0x%x ID invalid.\n", cqn);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nhns_roce_cq_event(hr_dev, cqn, event_type);\r\n}\r\nstatic void hns_roce_db_overflow_handle(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_aeqe *aeqe)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nswitch (roce_get_field(aeqe->asyn, HNS_ROCE_AEQE_U32_4_EVENT_SUB_TYPE_M,\r\nHNS_ROCE_AEQE_U32_4_EVENT_SUB_TYPE_S)) {\r\ncase HNS_ROCE_DB_SUBTYPE_SDB_OVF:\r\ndev_warn(dev, "SDB overflow.\n");\r\nbreak;\r\ncase HNS_ROCE_DB_SUBTYPE_SDB_ALM_OVF:\r\ndev_warn(dev, "SDB almost overflow.\n");\r\nbreak;\r\ncase HNS_ROCE_DB_SUBTYPE_SDB_ALM_EMP:\r\ndev_warn(dev, "SDB almost empty.\n");\r\nbreak;\r\ncase HNS_ROCE_DB_SUBTYPE_ODB_OVF:\r\ndev_warn(dev, "ODB overflow.\n");\r\nbreak;\r\ncase HNS_ROCE_DB_SUBTYPE_ODB_ALM_OVF:\r\ndev_warn(dev, "ODB almost overflow.\n");\r\nbreak;\r\ncase HNS_ROCE_DB_SUBTYPE_ODB_ALM_EMP:\r\ndev_warn(dev, "SDB almost empty.\n");\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic int hns_roce_aeq_int(struct hns_roce_dev *hr_dev, struct hns_roce_eq *eq)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nstruct hns_roce_aeqe *aeqe;\r\nint aeqes_found = 0;\r\nint event_type;\r\nwhile ((aeqe = next_aeqe_sw(eq))) {\r\ndev_dbg(dev, "aeqe = %p, aeqe->asyn.event_type = 0x%lx\n", aeqe,\r\nroce_get_field(aeqe->asyn,\r\nHNS_ROCE_AEQE_U32_4_EVENT_TYPE_M,\r\nHNS_ROCE_AEQE_U32_4_EVENT_TYPE_S));\r\nrmb();\r\nevent_type = roce_get_field(aeqe->asyn,\r\nHNS_ROCE_AEQE_U32_4_EVENT_TYPE_M,\r\nHNS_ROCE_AEQE_U32_4_EVENT_TYPE_S);\r\nswitch (event_type) {\r\ncase HNS_ROCE_EVENT_TYPE_PATH_MIG:\r\ndev_warn(dev, "PATH MIG not supported\n");\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_COMM_EST:\r\ndev_warn(dev, "COMMUNICATION established\n");\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_SQ_DRAINED:\r\ndev_warn(dev, "SQ DRAINED not supported\n");\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_PATH_MIG_FAILED:\r\ndev_warn(dev, "PATH MIG failed\n");\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_INV_REQ_LOCAL_WQ_ERROR:\r\ncase HNS_ROCE_EVENT_TYPE_WQ_CATAS_ERROR:\r\ncase HNS_ROCE_EVENT_TYPE_LOCAL_WQ_ACCESS_ERROR:\r\nhns_roce_qp_err_handle(hr_dev, aeqe, event_type);\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_SRQ_LIMIT_REACH:\r\ncase HNS_ROCE_EVENT_TYPE_SRQ_CATAS_ERROR:\r\ncase HNS_ROCE_EVENT_TYPE_SRQ_LAST_WQE_REACH:\r\ndev_warn(dev, "SRQ not support!\n");\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_CQ_ACCESS_ERROR:\r\ncase HNS_ROCE_EVENT_TYPE_CQ_OVERFLOW:\r\ncase HNS_ROCE_EVENT_TYPE_CQ_ID_INVALID:\r\nhns_roce_cq_err_handle(hr_dev, aeqe, event_type);\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_PORT_CHANGE:\r\ndev_warn(dev, "port change.\n");\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_MB:\r\nhns_roce_cmd_event(hr_dev,\r\nle16_to_cpu(aeqe->event.cmd.token),\r\naeqe->event.cmd.status,\r\nle64_to_cpu(aeqe->event.cmd.out_param\r\n));\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_DB_OVERFLOW:\r\nhns_roce_db_overflow_handle(hr_dev, aeqe);\r\nbreak;\r\ncase HNS_ROCE_EVENT_TYPE_CEQ_OVERFLOW:\r\ndev_warn(dev, "CEQ 0x%lx overflow.\n",\r\nroce_get_field(aeqe->event.ce_event.ceqe,\r\nHNS_ROCE_AEQE_EVENT_CE_EVENT_CEQE_CEQN_M,\r\nHNS_ROCE_AEQE_EVENT_CE_EVENT_CEQE_CEQN_S));\r\nbreak;\r\ndefault:\r\ndev_warn(dev, "Unhandled event %d on EQ %d at index %u\n",\r\nevent_type, eq->eqn, eq->cons_index);\r\nbreak;\r\n};\r\neq->cons_index++;\r\naeqes_found = 1;\r\nif (eq->cons_index > 2 * hr_dev->caps.aeqe_depth - 1) {\r\ndev_warn(dev, "cons_index overflow, set back to zero\n"\r\n);\r\neq->cons_index = 0;\r\n}\r\n}\r\neq_set_cons_index(eq, 0);\r\nreturn aeqes_found;\r\n}\r\nstatic struct hns_roce_ceqe *get_ceqe(struct hns_roce_eq *eq, u32 entry)\r\n{\r\nunsigned long off = (entry & (eq->entries - 1)) *\r\nHNS_ROCE_CEQ_ENTRY_SIZE;\r\nreturn (struct hns_roce_ceqe *)((u8 *)\r\n(eq->buf_list[off / HNS_ROCE_BA_SIZE].buf) +\r\noff % HNS_ROCE_BA_SIZE);\r\n}\r\nstatic struct hns_roce_ceqe *next_ceqe_sw(struct hns_roce_eq *eq)\r\n{\r\nstruct hns_roce_ceqe *ceqe = get_ceqe(eq, eq->cons_index);\r\nreturn (!!(roce_get_bit(ceqe->ceqe.comp,\r\nHNS_ROCE_CEQE_CEQE_COMP_OWNER_S))) ^\r\n(!!(eq->cons_index & eq->entries)) ? ceqe : NULL;\r\n}\r\nstatic int hns_roce_ceq_int(struct hns_roce_dev *hr_dev, struct hns_roce_eq *eq)\r\n{\r\nstruct hns_roce_ceqe *ceqe;\r\nint ceqes_found = 0;\r\nu32 cqn;\r\nwhile ((ceqe = next_ceqe_sw(eq))) {\r\nrmb();\r\ncqn = roce_get_field(ceqe->ceqe.comp,\r\nHNS_ROCE_CEQE_CEQE_COMP_CQN_M,\r\nHNS_ROCE_CEQE_CEQE_COMP_CQN_S);\r\nhns_roce_cq_completion(hr_dev, cqn);\r\n++eq->cons_index;\r\nceqes_found = 1;\r\nif (eq->cons_index > 2 * hr_dev->caps.ceqe_depth[eq->eqn] - 1) {\r\ndev_warn(&eq->hr_dev->pdev->dev,\r\n"cons_index overflow, set back to zero\n");\r\neq->cons_index = 0;\r\n}\r\n}\r\neq_set_cons_index(eq, 0);\r\nreturn ceqes_found;\r\n}\r\nstatic int hns_roce_aeq_ovf_int(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_eq *eq)\r\n{\r\nstruct device *dev = &eq->hr_dev->pdev->dev;\r\nint eqovf_found = 0;\r\nu32 caepaemask_val;\r\nu32 cealmovf_val;\r\nu32 caepaest_val;\r\nu32 aeshift_val;\r\nu32 ceshift_val;\r\nu32 cemask_val;\r\nint i = 0;\r\naeshift_val = roce_read(hr_dev, ROCEE_CAEP_AEQC_AEQE_SHIFT_REG);\r\nif (roce_get_bit(aeshift_val,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQ_ALM_OVF_INT_ST_S) == 1) {\r\ndev_warn(dev, "AEQ overflow!\n");\r\ncaepaemask_val = roce_read(hr_dev, ROCEE_CAEP_AE_MASK_REG);\r\nroce_set_bit(caepaemask_val,\r\nROCEE_CAEP_AE_MASK_CAEP_AEQ_ALM_OVF_MASK_S,\r\nHNS_ROCE_INT_MASK_ENABLE);\r\nroce_write(hr_dev, ROCEE_CAEP_AE_MASK_REG, caepaemask_val);\r\ncaepaest_val = roce_read(hr_dev, ROCEE_CAEP_AE_ST_REG);\r\nroce_set_bit(caepaest_val,\r\nROCEE_CAEP_AE_ST_CAEP_AEQ_ALM_OVF_S, 1);\r\nroce_write(hr_dev, ROCEE_CAEP_AE_ST_REG, caepaest_val);\r\ncaepaemask_val = roce_read(hr_dev, ROCEE_CAEP_AE_MASK_REG);\r\nroce_set_bit(caepaemask_val,\r\nROCEE_CAEP_AE_MASK_CAEP_AEQ_ALM_OVF_MASK_S,\r\nHNS_ROCE_INT_MASK_DISABLE);\r\nroce_write(hr_dev, ROCEE_CAEP_AE_MASK_REG, caepaemask_val);\r\n}\r\nfor (i = 0; i < hr_dev->caps.num_comp_vectors; i++) {\r\nceshift_val = roce_read(hr_dev, ROCEE_CAEP_CEQC_SHIFT_0_REG +\r\ni * CEQ_REG_OFFSET);\r\nif (roce_get_bit(ceshift_val,\r\nROCEE_CAEP_CEQC_SHIFT_CAEP_CEQ_ALM_OVF_INT_ST_S) == 1) {\r\ndev_warn(dev, "CEQ[%d] almost overflow!\n", i);\r\neqovf_found++;\r\ncemask_val = roce_read(hr_dev,\r\nROCEE_CAEP_CE_IRQ_MASK_0_REG +\r\ni * CEQ_REG_OFFSET);\r\nroce_set_bit(cemask_val,\r\nROCEE_CAEP_CE_IRQ_MASK_CAEP_CEQ_ALM_OVF_MASK_S,\r\nHNS_ROCE_INT_MASK_ENABLE);\r\nroce_write(hr_dev, ROCEE_CAEP_CE_IRQ_MASK_0_REG +\r\ni * CEQ_REG_OFFSET, cemask_val);\r\ncealmovf_val = roce_read(hr_dev,\r\nROCEE_CAEP_CEQ_ALM_OVF_0_REG +\r\ni * CEQ_REG_OFFSET);\r\nroce_set_bit(cealmovf_val,\r\nROCEE_CAEP_CEQ_ALM_OVF_CAEP_CEQ_ALM_OVF_S,\r\n1);\r\nroce_write(hr_dev, ROCEE_CAEP_CEQ_ALM_OVF_0_REG +\r\ni * CEQ_REG_OFFSET, cealmovf_val);\r\ncemask_val = roce_read(hr_dev,\r\nROCEE_CAEP_CE_IRQ_MASK_0_REG +\r\ni * CEQ_REG_OFFSET);\r\nroce_set_bit(cemask_val,\r\nROCEE_CAEP_CE_IRQ_MASK_CAEP_CEQ_ALM_OVF_MASK_S,\r\nHNS_ROCE_INT_MASK_DISABLE);\r\nroce_write(hr_dev, ROCEE_CAEP_CE_IRQ_MASK_0_REG +\r\ni * CEQ_REG_OFFSET, cemask_val);\r\n}\r\n}\r\ndev_warn(dev, "ECC UCERR ALARM: 0x%x, 0x%x, 0x%x\n",\r\nroce_read(hr_dev, ROCEE_ECC_UCERR_ALM0_REG),\r\nroce_read(hr_dev, ROCEE_ECC_UCERR_ALM1_REG),\r\nroce_read(hr_dev, ROCEE_ECC_UCERR_ALM2_REG));\r\ndev_warn(dev, "ECC CERR ALARM: 0x%x, 0x%x, 0x%x\n",\r\nroce_read(hr_dev, ROCEE_ECC_CERR_ALM0_REG),\r\nroce_read(hr_dev, ROCEE_ECC_CERR_ALM1_REG),\r\nroce_read(hr_dev, ROCEE_ECC_CERR_ALM2_REG));\r\nreturn eqovf_found;\r\n}\r\nstatic int hns_roce_eq_int(struct hns_roce_dev *hr_dev, struct hns_roce_eq *eq)\r\n{\r\nint eqes_found = 0;\r\nif (likely(eq->type_flag == HNS_ROCE_CEQ))\r\neqes_found = hns_roce_ceq_int(hr_dev, eq);\r\nelse if (likely(eq->type_flag == HNS_ROCE_AEQ))\r\neqes_found = hns_roce_aeq_int(hr_dev, eq);\r\nelse\r\neqes_found = hns_roce_aeq_ovf_int(hr_dev, eq);\r\nreturn eqes_found;\r\n}\r\nstatic irqreturn_t hns_roce_msi_x_interrupt(int irq, void *eq_ptr)\r\n{\r\nint int_work = 0;\r\nstruct hns_roce_eq *eq = eq_ptr;\r\nstruct hns_roce_dev *hr_dev = eq->hr_dev;\r\nint_work = hns_roce_eq_int(hr_dev, eq);\r\nreturn IRQ_RETVAL(int_work);\r\n}\r\nstatic void hns_roce_enable_eq(struct hns_roce_dev *hr_dev, int eq_num,\r\nint enable_flag)\r\n{\r\nvoid __iomem *eqc = hr_dev->eq_table.eqc_base[eq_num];\r\nu32 val;\r\nval = readl(eqc);\r\nif (enable_flag)\r\nroce_set_field(val,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_STATE_M,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_STATE_S,\r\nHNS_ROCE_EQ_STAT_VALID);\r\nelse\r\nroce_set_field(val,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_STATE_M,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_STATE_S,\r\nHNS_ROCE_EQ_STAT_INVALID);\r\nwritel(val, eqc);\r\n}\r\nstatic int hns_roce_create_eq(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_eq *eq)\r\n{\r\nvoid __iomem *eqc = hr_dev->eq_table.eqc_base[eq->eqn];\r\nstruct device *dev = &hr_dev->pdev->dev;\r\ndma_addr_t tmp_dma_addr;\r\nu32 eqconsindx_val = 0;\r\nu32 eqcuridx_val = 0;\r\nu32 eqshift_val = 0;\r\nint num_bas = 0;\r\nint ret;\r\nint i;\r\nnum_bas = (PAGE_ALIGN(eq->entries * eq->eqe_size) +\r\nHNS_ROCE_BA_SIZE - 1) / HNS_ROCE_BA_SIZE;\r\nif ((eq->entries * eq->eqe_size) > HNS_ROCE_BA_SIZE) {\r\ndev_err(dev, "[error]eq buf %d gt ba size(%d) need bas=%d\n",\r\n(eq->entries * eq->eqe_size), HNS_ROCE_BA_SIZE,\r\nnum_bas);\r\nreturn -EINVAL;\r\n}\r\neq->buf_list = kcalloc(num_bas, sizeof(*eq->buf_list), GFP_KERNEL);\r\nif (!eq->buf_list)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < num_bas; ++i) {\r\neq->buf_list[i].buf = dma_alloc_coherent(dev, HNS_ROCE_BA_SIZE,\r\n&tmp_dma_addr,\r\nGFP_KERNEL);\r\nif (!eq->buf_list[i].buf) {\r\nret = -ENOMEM;\r\ngoto err_out_free_pages;\r\n}\r\neq->buf_list[i].map = tmp_dma_addr;\r\nmemset(eq->buf_list[i].buf, 0, HNS_ROCE_BA_SIZE);\r\n}\r\neq->cons_index = 0;\r\nroce_set_field(eqshift_val,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_STATE_M,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_STATE_S,\r\nHNS_ROCE_EQ_STAT_INVALID);\r\nroce_set_field(eqshift_val,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_AEQE_SHIFT_M,\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_CAEP_AEQC_AEQE_SHIFT_S,\r\neq->log_entries);\r\nwritel(eqshift_val, eqc);\r\nwritel((u32)(eq->buf_list[0].map >> 12), (u8 *)eqc + 4);\r\nroce_set_field(eqcuridx_val, ROCEE_CAEP_AEQE_CUR_IDX_CAEP_AEQ_BT_H_M,\r\nROCEE_CAEP_AEQE_CUR_IDX_CAEP_AEQ_BT_H_S,\r\neq->buf_list[0].map >> 44);\r\nroce_set_field(eqcuridx_val,\r\nROCEE_CAEP_AEQE_CUR_IDX_CAEP_AEQE_CUR_IDX_M,\r\nROCEE_CAEP_AEQE_CUR_IDX_CAEP_AEQE_CUR_IDX_S, 0);\r\nwritel(eqcuridx_val, (u8 *)eqc + 8);\r\nroce_set_field(eqconsindx_val,\r\nROCEE_CAEP_AEQE_CONS_IDX_CAEP_AEQE_CONS_IDX_M,\r\nROCEE_CAEP_AEQE_CONS_IDX_CAEP_AEQE_CONS_IDX_S, 0);\r\nwritel(eqconsindx_val, (u8 *)eqc + 0xc);\r\nreturn 0;\r\nerr_out_free_pages:\r\nfor (i = i - 1; i >= 0; i--)\r\ndma_free_coherent(dev, HNS_ROCE_BA_SIZE, eq->buf_list[i].buf,\r\neq->buf_list[i].map);\r\nkfree(eq->buf_list);\r\nreturn ret;\r\n}\r\nstatic void hns_roce_free_eq(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_eq *eq)\r\n{\r\nint i = 0;\r\nint npages = (PAGE_ALIGN(eq->eqe_size * eq->entries) +\r\nHNS_ROCE_BA_SIZE - 1) / HNS_ROCE_BA_SIZE;\r\nif (!eq->buf_list)\r\nreturn;\r\nfor (i = 0; i < npages; ++i)\r\ndma_free_coherent(&hr_dev->pdev->dev, HNS_ROCE_BA_SIZE,\r\neq->buf_list[i].buf, eq->buf_list[i].map);\r\nkfree(eq->buf_list);\r\n}\r\nstatic void hns_roce_int_mask_en(struct hns_roce_dev *hr_dev)\r\n{\r\nint i = 0;\r\nu32 aemask_val;\r\nint masken = 0;\r\naemask_val = roce_read(hr_dev, ROCEE_CAEP_AE_MASK_REG);\r\nroce_set_bit(aemask_val, ROCEE_CAEP_AE_MASK_CAEP_AEQ_ALM_OVF_MASK_S,\r\nmasken);\r\nroce_set_bit(aemask_val, ROCEE_CAEP_AE_MASK_CAEP_AE_IRQ_MASK_S, masken);\r\nroce_write(hr_dev, ROCEE_CAEP_AE_MASK_REG, aemask_val);\r\nfor (i = 0; i < hr_dev->caps.num_comp_vectors; i++) {\r\nroce_write(hr_dev, ROCEE_CAEP_CE_IRQ_MASK_0_REG +\r\ni * CEQ_REG_OFFSET, masken);\r\n}\r\n}\r\nstatic void hns_roce_ce_int_default_cfg(struct hns_roce_dev *hr_dev)\r\n{\r\nroce_write(hr_dev, ROCEE_CAEP_CE_INTERVAL_CFG_REG,\r\nHNS_ROCE_CEQ_DEFAULT_INTERVAL);\r\nroce_write(hr_dev, ROCEE_CAEP_CE_BURST_NUM_CFG_REG,\r\nHNS_ROCE_CEQ_DEFAULT_BURST_NUM);\r\n}\r\nint hns_roce_init_eq_table(struct hns_roce_dev *hr_dev)\r\n{\r\nstruct hns_roce_eq_table *eq_table = &hr_dev->eq_table;\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nstruct hns_roce_eq *eq = NULL;\r\nint eq_num = 0;\r\nint ret = 0;\r\nint i = 0;\r\nint j = 0;\r\neq_num = hr_dev->caps.num_comp_vectors + hr_dev->caps.num_aeq_vectors;\r\neq_table->eq = kcalloc(eq_num, sizeof(*eq_table->eq), GFP_KERNEL);\r\nif (!eq_table->eq)\r\nreturn -ENOMEM;\r\neq_table->eqc_base = kcalloc(eq_num, sizeof(*eq_table->eqc_base),\r\nGFP_KERNEL);\r\nif (!eq_table->eqc_base) {\r\nret = -ENOMEM;\r\ngoto err_eqc_base_alloc_fail;\r\n}\r\nfor (i = 0; i < eq_num; i++) {\r\neq = &eq_table->eq[i];\r\neq->hr_dev = hr_dev;\r\neq->eqn = i;\r\neq->irq = hr_dev->irq[i];\r\neq->log_page_size = PAGE_SHIFT;\r\nif (i < hr_dev->caps.num_comp_vectors) {\r\neq_table->eqc_base[i] = hr_dev->reg_base +\r\nROCEE_CAEP_CEQC_SHIFT_0_REG +\r\nHNS_ROCE_CEQC_REG_OFFSET * i;\r\neq->type_flag = HNS_ROCE_CEQ;\r\neq->doorbell = hr_dev->reg_base +\r\nROCEE_CAEP_CEQC_CONS_IDX_0_REG +\r\nHNS_ROCE_CEQC_REG_OFFSET * i;\r\neq->entries = hr_dev->caps.ceqe_depth[i];\r\neq->log_entries = ilog2(eq->entries);\r\neq->eqe_size = sizeof(struct hns_roce_ceqe);\r\n} else {\r\neq_table->eqc_base[i] = hr_dev->reg_base +\r\nROCEE_CAEP_AEQC_AEQE_SHIFT_REG;\r\neq->type_flag = HNS_ROCE_AEQ;\r\neq->doorbell = hr_dev->reg_base +\r\nROCEE_CAEP_AEQE_CONS_IDX_REG;\r\neq->entries = hr_dev->caps.aeqe_depth;\r\neq->log_entries = ilog2(eq->entries);\r\neq->eqe_size = sizeof(struct hns_roce_aeqe);\r\n}\r\n}\r\nhns_roce_int_mask_en(hr_dev);\r\nhns_roce_ce_int_default_cfg(hr_dev);\r\nfor (i = 0; i < eq_num; i++) {\r\nret = hns_roce_create_eq(hr_dev, &eq_table->eq[i]);\r\nif (ret) {\r\ndev_err(dev, "eq create failed\n");\r\ngoto err_create_eq_fail;\r\n}\r\n}\r\nfor (j = 0; j < eq_num; j++) {\r\nret = request_irq(eq_table->eq[j].irq, hns_roce_msi_x_interrupt,\r\n0, hr_dev->irq_names[j], eq_table->eq + j);\r\nif (ret) {\r\ndev_err(dev, "request irq error!\n");\r\ngoto err_request_irq_fail;\r\n}\r\n}\r\nfor (i = 0; i < eq_num; i++)\r\nhns_roce_enable_eq(hr_dev, i, EQ_ENABLE);\r\nreturn 0;\r\nerr_request_irq_fail:\r\nfor (j = j - 1; j >= 0; j--)\r\nfree_irq(eq_table->eq[j].irq, eq_table->eq + j);\r\nerr_create_eq_fail:\r\nfor (i = i - 1; i >= 0; i--)\r\nhns_roce_free_eq(hr_dev, &eq_table->eq[i]);\r\nkfree(eq_table->eqc_base);\r\nerr_eqc_base_alloc_fail:\r\nkfree(eq_table->eq);\r\nreturn ret;\r\n}\r\nvoid hns_roce_cleanup_eq_table(struct hns_roce_dev *hr_dev)\r\n{\r\nint i;\r\nint eq_num;\r\nstruct hns_roce_eq_table *eq_table = &hr_dev->eq_table;\r\neq_num = hr_dev->caps.num_comp_vectors + hr_dev->caps.num_aeq_vectors;\r\nfor (i = 0; i < eq_num; i++) {\r\nhns_roce_enable_eq(hr_dev, i, EQ_DISABLE);\r\nfree_irq(eq_table->eq[i].irq, eq_table->eq + i);\r\nhns_roce_free_eq(hr_dev, &eq_table->eq[i]);\r\n}\r\nkfree(eq_table->eqc_base);\r\nkfree(eq_table->eq);\r\n}
