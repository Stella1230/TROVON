static void mthca_free_icm_pages(struct mthca_dev *dev, struct mthca_icm_chunk *chunk)\r\n{\r\nint i;\r\nif (chunk->nsg > 0)\r\npci_unmap_sg(dev->pdev, chunk->mem, chunk->npages,\r\nPCI_DMA_BIDIRECTIONAL);\r\nfor (i = 0; i < chunk->npages; ++i)\r\n__free_pages(sg_page(&chunk->mem[i]),\r\nget_order(chunk->mem[i].length));\r\n}\r\nstatic void mthca_free_icm_coherent(struct mthca_dev *dev, struct mthca_icm_chunk *chunk)\r\n{\r\nint i;\r\nfor (i = 0; i < chunk->npages; ++i) {\r\ndma_free_coherent(&dev->pdev->dev, chunk->mem[i].length,\r\nlowmem_page_address(sg_page(&chunk->mem[i])),\r\nsg_dma_address(&chunk->mem[i]));\r\n}\r\n}\r\nvoid mthca_free_icm(struct mthca_dev *dev, struct mthca_icm *icm, int coherent)\r\n{\r\nstruct mthca_icm_chunk *chunk, *tmp;\r\nif (!icm)\r\nreturn;\r\nlist_for_each_entry_safe(chunk, tmp, &icm->chunk_list, list) {\r\nif (coherent)\r\nmthca_free_icm_coherent(dev, chunk);\r\nelse\r\nmthca_free_icm_pages(dev, chunk);\r\nkfree(chunk);\r\n}\r\nkfree(icm);\r\n}\r\nstatic int mthca_alloc_icm_pages(struct scatterlist *mem, int order, gfp_t gfp_mask)\r\n{\r\nstruct page *page;\r\npage = alloc_pages(gfp_mask | __GFP_ZERO, order);\r\nif (!page)\r\nreturn -ENOMEM;\r\nsg_set_page(mem, page, PAGE_SIZE << order, 0);\r\nreturn 0;\r\n}\r\nstatic int mthca_alloc_icm_coherent(struct device *dev, struct scatterlist *mem,\r\nint order, gfp_t gfp_mask)\r\n{\r\nvoid *buf = dma_alloc_coherent(dev, PAGE_SIZE << order, &sg_dma_address(mem),\r\ngfp_mask);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nsg_set_buf(mem, buf, PAGE_SIZE << order);\r\nBUG_ON(mem->offset);\r\nsg_dma_len(mem) = PAGE_SIZE << order;\r\nreturn 0;\r\n}\r\nstruct mthca_icm *mthca_alloc_icm(struct mthca_dev *dev, int npages,\r\ngfp_t gfp_mask, int coherent)\r\n{\r\nstruct mthca_icm *icm;\r\nstruct mthca_icm_chunk *chunk = NULL;\r\nint cur_order;\r\nint ret;\r\nBUG_ON(coherent && (gfp_mask & __GFP_HIGHMEM));\r\nicm = kmalloc(sizeof *icm, gfp_mask & ~(__GFP_HIGHMEM | __GFP_NOWARN));\r\nif (!icm)\r\nreturn icm;\r\nicm->refcount = 0;\r\nINIT_LIST_HEAD(&icm->chunk_list);\r\ncur_order = get_order(MTHCA_ICM_ALLOC_SIZE);\r\nwhile (npages > 0) {\r\nif (!chunk) {\r\nchunk = kmalloc(sizeof *chunk,\r\ngfp_mask & ~(__GFP_HIGHMEM | __GFP_NOWARN));\r\nif (!chunk)\r\ngoto fail;\r\nsg_init_table(chunk->mem, MTHCA_ICM_CHUNK_LEN);\r\nchunk->npages = 0;\r\nchunk->nsg = 0;\r\nlist_add_tail(&chunk->list, &icm->chunk_list);\r\n}\r\nwhile (1 << cur_order > npages)\r\n--cur_order;\r\nif (coherent)\r\nret = mthca_alloc_icm_coherent(&dev->pdev->dev,\r\n&chunk->mem[chunk->npages],\r\ncur_order, gfp_mask);\r\nelse\r\nret = mthca_alloc_icm_pages(&chunk->mem[chunk->npages],\r\ncur_order, gfp_mask);\r\nif (!ret) {\r\n++chunk->npages;\r\nif (coherent)\r\n++chunk->nsg;\r\nelse if (chunk->npages == MTHCA_ICM_CHUNK_LEN) {\r\nchunk->nsg = pci_map_sg(dev->pdev, chunk->mem,\r\nchunk->npages,\r\nPCI_DMA_BIDIRECTIONAL);\r\nif (chunk->nsg <= 0)\r\ngoto fail;\r\n}\r\nif (chunk->npages == MTHCA_ICM_CHUNK_LEN)\r\nchunk = NULL;\r\nnpages -= 1 << cur_order;\r\n} else {\r\n--cur_order;\r\nif (cur_order < 0)\r\ngoto fail;\r\n}\r\n}\r\nif (!coherent && chunk) {\r\nchunk->nsg = pci_map_sg(dev->pdev, chunk->mem,\r\nchunk->npages,\r\nPCI_DMA_BIDIRECTIONAL);\r\nif (chunk->nsg <= 0)\r\ngoto fail;\r\n}\r\nreturn icm;\r\nfail:\r\nmthca_free_icm(dev, icm, coherent);\r\nreturn NULL;\r\n}\r\nint mthca_table_get(struct mthca_dev *dev, struct mthca_icm_table *table, int obj)\r\n{\r\nint i = (obj & (table->num_obj - 1)) * table->obj_size / MTHCA_TABLE_CHUNK_SIZE;\r\nint ret = 0;\r\nmutex_lock(&table->mutex);\r\nif (table->icm[i]) {\r\n++table->icm[i]->refcount;\r\ngoto out;\r\n}\r\ntable->icm[i] = mthca_alloc_icm(dev, MTHCA_TABLE_CHUNK_SIZE >> PAGE_SHIFT,\r\n(table->lowmem ? GFP_KERNEL : GFP_HIGHUSER) |\r\n__GFP_NOWARN, table->coherent);\r\nif (!table->icm[i]) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (mthca_MAP_ICM(dev, table->icm[i],\r\ntable->virt + i * MTHCA_TABLE_CHUNK_SIZE)) {\r\nmthca_free_icm(dev, table->icm[i], table->coherent);\r\ntable->icm[i] = NULL;\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\n++table->icm[i]->refcount;\r\nout:\r\nmutex_unlock(&table->mutex);\r\nreturn ret;\r\n}\r\nvoid mthca_table_put(struct mthca_dev *dev, struct mthca_icm_table *table, int obj)\r\n{\r\nint i;\r\nif (!mthca_is_memfree(dev))\r\nreturn;\r\ni = (obj & (table->num_obj - 1)) * table->obj_size / MTHCA_TABLE_CHUNK_SIZE;\r\nmutex_lock(&table->mutex);\r\nif (--table->icm[i]->refcount == 0) {\r\nmthca_UNMAP_ICM(dev, table->virt + i * MTHCA_TABLE_CHUNK_SIZE,\r\nMTHCA_TABLE_CHUNK_SIZE / MTHCA_ICM_PAGE_SIZE);\r\nmthca_free_icm(dev, table->icm[i], table->coherent);\r\ntable->icm[i] = NULL;\r\n}\r\nmutex_unlock(&table->mutex);\r\n}\r\nvoid *mthca_table_find(struct mthca_icm_table *table, int obj, dma_addr_t *dma_handle)\r\n{\r\nint idx, offset, dma_offset, i;\r\nstruct mthca_icm_chunk *chunk;\r\nstruct mthca_icm *icm;\r\nstruct page *page = NULL;\r\nif (!table->lowmem)\r\nreturn NULL;\r\nmutex_lock(&table->mutex);\r\nidx = (obj & (table->num_obj - 1)) * table->obj_size;\r\nicm = table->icm[idx / MTHCA_TABLE_CHUNK_SIZE];\r\ndma_offset = offset = idx % MTHCA_TABLE_CHUNK_SIZE;\r\nif (!icm)\r\ngoto out;\r\nlist_for_each_entry(chunk, &icm->chunk_list, list) {\r\nfor (i = 0; i < chunk->npages; ++i) {\r\nif (dma_handle && dma_offset >= 0) {\r\nif (sg_dma_len(&chunk->mem[i]) > dma_offset)\r\n*dma_handle = sg_dma_address(&chunk->mem[i]) +\r\ndma_offset;\r\ndma_offset -= sg_dma_len(&chunk->mem[i]);\r\n}\r\nif (chunk->mem[i].length > offset) {\r\npage = sg_page(&chunk->mem[i]);\r\ngoto out;\r\n}\r\noffset -= chunk->mem[i].length;\r\n}\r\n}\r\nout:\r\nmutex_unlock(&table->mutex);\r\nreturn page ? lowmem_page_address(page) + offset : NULL;\r\n}\r\nint mthca_table_get_range(struct mthca_dev *dev, struct mthca_icm_table *table,\r\nint start, int end)\r\n{\r\nint inc = MTHCA_TABLE_CHUNK_SIZE / table->obj_size;\r\nint i, err;\r\nfor (i = start; i <= end; i += inc) {\r\nerr = mthca_table_get(dev, table, i);\r\nif (err)\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nwhile (i > start) {\r\ni -= inc;\r\nmthca_table_put(dev, table, i);\r\n}\r\nreturn err;\r\n}\r\nvoid mthca_table_put_range(struct mthca_dev *dev, struct mthca_icm_table *table,\r\nint start, int end)\r\n{\r\nint i;\r\nif (!mthca_is_memfree(dev))\r\nreturn;\r\nfor (i = start; i <= end; i += MTHCA_TABLE_CHUNK_SIZE / table->obj_size)\r\nmthca_table_put(dev, table, i);\r\n}\r\nstruct mthca_icm_table *mthca_alloc_icm_table(struct mthca_dev *dev,\r\nu64 virt, int obj_size,\r\nint nobj, int reserved,\r\nint use_lowmem, int use_coherent)\r\n{\r\nstruct mthca_icm_table *table;\r\nint obj_per_chunk;\r\nint num_icm;\r\nunsigned chunk_size;\r\nint i;\r\nobj_per_chunk = MTHCA_TABLE_CHUNK_SIZE / obj_size;\r\nnum_icm = DIV_ROUND_UP(nobj, obj_per_chunk);\r\ntable = kmalloc(sizeof *table + num_icm * sizeof *table->icm, GFP_KERNEL);\r\nif (!table)\r\nreturn NULL;\r\ntable->virt = virt;\r\ntable->num_icm = num_icm;\r\ntable->num_obj = nobj;\r\ntable->obj_size = obj_size;\r\ntable->lowmem = use_lowmem;\r\ntable->coherent = use_coherent;\r\nmutex_init(&table->mutex);\r\nfor (i = 0; i < num_icm; ++i)\r\ntable->icm[i] = NULL;\r\nfor (i = 0; i * MTHCA_TABLE_CHUNK_SIZE < reserved * obj_size; ++i) {\r\nchunk_size = MTHCA_TABLE_CHUNK_SIZE;\r\nif ((i + 1) * MTHCA_TABLE_CHUNK_SIZE > nobj * obj_size)\r\nchunk_size = nobj * obj_size - i * MTHCA_TABLE_CHUNK_SIZE;\r\ntable->icm[i] = mthca_alloc_icm(dev, chunk_size >> PAGE_SHIFT,\r\n(use_lowmem ? GFP_KERNEL : GFP_HIGHUSER) |\r\n__GFP_NOWARN, use_coherent);\r\nif (!table->icm[i])\r\ngoto err;\r\nif (mthca_MAP_ICM(dev, table->icm[i],\r\nvirt + i * MTHCA_TABLE_CHUNK_SIZE)) {\r\nmthca_free_icm(dev, table->icm[i], table->coherent);\r\ntable->icm[i] = NULL;\r\ngoto err;\r\n}\r\n++table->icm[i]->refcount;\r\n}\r\nreturn table;\r\nerr:\r\nfor (i = 0; i < num_icm; ++i)\r\nif (table->icm[i]) {\r\nmthca_UNMAP_ICM(dev, virt + i * MTHCA_TABLE_CHUNK_SIZE,\r\nMTHCA_TABLE_CHUNK_SIZE / MTHCA_ICM_PAGE_SIZE);\r\nmthca_free_icm(dev, table->icm[i], table->coherent);\r\n}\r\nkfree(table);\r\nreturn NULL;\r\n}\r\nvoid mthca_free_icm_table(struct mthca_dev *dev, struct mthca_icm_table *table)\r\n{\r\nint i;\r\nfor (i = 0; i < table->num_icm; ++i)\r\nif (table->icm[i]) {\r\nmthca_UNMAP_ICM(dev,\r\ntable->virt + i * MTHCA_TABLE_CHUNK_SIZE,\r\nMTHCA_TABLE_CHUNK_SIZE / MTHCA_ICM_PAGE_SIZE);\r\nmthca_free_icm(dev, table->icm[i], table->coherent);\r\n}\r\nkfree(table);\r\n}\r\nstatic u64 mthca_uarc_virt(struct mthca_dev *dev, struct mthca_uar *uar, int page)\r\n{\r\nreturn dev->uar_table.uarc_base +\r\nuar->index * dev->uar_table.uarc_size +\r\npage * MTHCA_ICM_PAGE_SIZE;\r\n}\r\nint mthca_map_user_db(struct mthca_dev *dev, struct mthca_uar *uar,\r\nstruct mthca_user_db_table *db_tab, int index, u64 uaddr)\r\n{\r\nstruct page *pages[1];\r\nint ret = 0;\r\nint i;\r\nif (!mthca_is_memfree(dev))\r\nreturn 0;\r\nif (index < 0 || index > dev->uar_table.uarc_size / 8)\r\nreturn -EINVAL;\r\nmutex_lock(&db_tab->mutex);\r\ni = index / MTHCA_DB_REC_PER_PAGE;\r\nif ((db_tab->page[i].refcount >= MTHCA_DB_REC_PER_PAGE) ||\r\n(db_tab->page[i].uvirt && db_tab->page[i].uvirt != uaddr) ||\r\n(uaddr & 4095)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (db_tab->page[i].refcount) {\r\n++db_tab->page[i].refcount;\r\ngoto out;\r\n}\r\nret = get_user_pages(uaddr & PAGE_MASK, 1, FOLL_WRITE, pages, NULL);\r\nif (ret < 0)\r\ngoto out;\r\nsg_set_page(&db_tab->page[i].mem, pages[0], MTHCA_ICM_PAGE_SIZE,\r\nuaddr & ~PAGE_MASK);\r\nret = pci_map_sg(dev->pdev, &db_tab->page[i].mem, 1, PCI_DMA_TODEVICE);\r\nif (ret < 0) {\r\nput_page(pages[0]);\r\ngoto out;\r\n}\r\nret = mthca_MAP_ICM_page(dev, sg_dma_address(&db_tab->page[i].mem),\r\nmthca_uarc_virt(dev, uar, i));\r\nif (ret) {\r\npci_unmap_sg(dev->pdev, &db_tab->page[i].mem, 1, PCI_DMA_TODEVICE);\r\nput_page(sg_page(&db_tab->page[i].mem));\r\ngoto out;\r\n}\r\ndb_tab->page[i].uvirt = uaddr;\r\ndb_tab->page[i].refcount = 1;\r\nout:\r\nmutex_unlock(&db_tab->mutex);\r\nreturn ret;\r\n}\r\nvoid mthca_unmap_user_db(struct mthca_dev *dev, struct mthca_uar *uar,\r\nstruct mthca_user_db_table *db_tab, int index)\r\n{\r\nif (!mthca_is_memfree(dev))\r\nreturn;\r\nmutex_lock(&db_tab->mutex);\r\n--db_tab->page[index / MTHCA_DB_REC_PER_PAGE].refcount;\r\nmutex_unlock(&db_tab->mutex);\r\n}\r\nstruct mthca_user_db_table *mthca_init_user_db_tab(struct mthca_dev *dev)\r\n{\r\nstruct mthca_user_db_table *db_tab;\r\nint npages;\r\nint i;\r\nif (!mthca_is_memfree(dev))\r\nreturn NULL;\r\nnpages = dev->uar_table.uarc_size / MTHCA_ICM_PAGE_SIZE;\r\ndb_tab = kmalloc(sizeof *db_tab + npages * sizeof *db_tab->page, GFP_KERNEL);\r\nif (!db_tab)\r\nreturn ERR_PTR(-ENOMEM);\r\nmutex_init(&db_tab->mutex);\r\nfor (i = 0; i < npages; ++i) {\r\ndb_tab->page[i].refcount = 0;\r\ndb_tab->page[i].uvirt = 0;\r\nsg_init_table(&db_tab->page[i].mem, 1);\r\n}\r\nreturn db_tab;\r\n}\r\nvoid mthca_cleanup_user_db_tab(struct mthca_dev *dev, struct mthca_uar *uar,\r\nstruct mthca_user_db_table *db_tab)\r\n{\r\nint i;\r\nif (!mthca_is_memfree(dev))\r\nreturn;\r\nfor (i = 0; i < dev->uar_table.uarc_size / MTHCA_ICM_PAGE_SIZE; ++i) {\r\nif (db_tab->page[i].uvirt) {\r\nmthca_UNMAP_ICM(dev, mthca_uarc_virt(dev, uar, i), 1);\r\npci_unmap_sg(dev->pdev, &db_tab->page[i].mem, 1, PCI_DMA_TODEVICE);\r\nput_page(sg_page(&db_tab->page[i].mem));\r\n}\r\n}\r\nkfree(db_tab);\r\n}\r\nint mthca_alloc_db(struct mthca_dev *dev, enum mthca_db_type type,\r\nu32 qn, __be32 **db)\r\n{\r\nint group;\r\nint start, end, dir;\r\nint i, j;\r\nstruct mthca_db_page *page;\r\nint ret = 0;\r\nmutex_lock(&dev->db_tab->mutex);\r\nswitch (type) {\r\ncase MTHCA_DB_TYPE_CQ_ARM:\r\ncase MTHCA_DB_TYPE_SQ:\r\ngroup = 0;\r\nstart = 0;\r\nend = dev->db_tab->max_group1;\r\ndir = 1;\r\nbreak;\r\ncase MTHCA_DB_TYPE_CQ_SET_CI:\r\ncase MTHCA_DB_TYPE_RQ:\r\ncase MTHCA_DB_TYPE_SRQ:\r\ngroup = 1;\r\nstart = dev->db_tab->npages - 1;\r\nend = dev->db_tab->min_group2;\r\ndir = -1;\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nfor (i = start; i != end; i += dir)\r\nif (dev->db_tab->page[i].db_rec &&\r\n!bitmap_full(dev->db_tab->page[i].used,\r\nMTHCA_DB_REC_PER_PAGE)) {\r\npage = dev->db_tab->page + i;\r\ngoto found;\r\n}\r\nfor (i = start; i != end; i += dir)\r\nif (!dev->db_tab->page[i].db_rec) {\r\npage = dev->db_tab->page + i;\r\ngoto alloc;\r\n}\r\nif (dev->db_tab->max_group1 >= dev->db_tab->min_group2 - 1) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (group == 0)\r\n++dev->db_tab->max_group1;\r\nelse\r\n--dev->db_tab->min_group2;\r\npage = dev->db_tab->page + end;\r\nalloc:\r\npage->db_rec = dma_alloc_coherent(&dev->pdev->dev, MTHCA_ICM_PAGE_SIZE,\r\n&page->mapping, GFP_KERNEL);\r\nif (!page->db_rec) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nmemset(page->db_rec, 0, MTHCA_ICM_PAGE_SIZE);\r\nret = mthca_MAP_ICM_page(dev, page->mapping,\r\nmthca_uarc_virt(dev, &dev->driver_uar, i));\r\nif (ret) {\r\ndma_free_coherent(&dev->pdev->dev, MTHCA_ICM_PAGE_SIZE,\r\npage->db_rec, page->mapping);\r\ngoto out;\r\n}\r\nbitmap_zero(page->used, MTHCA_DB_REC_PER_PAGE);\r\nfound:\r\nj = find_first_zero_bit(page->used, MTHCA_DB_REC_PER_PAGE);\r\nset_bit(j, page->used);\r\nif (group == 1)\r\nj = MTHCA_DB_REC_PER_PAGE - 1 - j;\r\nret = i * MTHCA_DB_REC_PER_PAGE + j;\r\npage->db_rec[j] = cpu_to_be64((qn << 8) | (type << 5));\r\n*db = (__be32 *) &page->db_rec[j];\r\nout:\r\nmutex_unlock(&dev->db_tab->mutex);\r\nreturn ret;\r\n}\r\nvoid mthca_free_db(struct mthca_dev *dev, int type, int db_index)\r\n{\r\nint i, j;\r\nstruct mthca_db_page *page;\r\ni = db_index / MTHCA_DB_REC_PER_PAGE;\r\nj = db_index % MTHCA_DB_REC_PER_PAGE;\r\npage = dev->db_tab->page + i;\r\nmutex_lock(&dev->db_tab->mutex);\r\npage->db_rec[j] = 0;\r\nif (i >= dev->db_tab->min_group2)\r\nj = MTHCA_DB_REC_PER_PAGE - 1 - j;\r\nclear_bit(j, page->used);\r\nif (bitmap_empty(page->used, MTHCA_DB_REC_PER_PAGE) &&\r\ni >= dev->db_tab->max_group1 - 1) {\r\nmthca_UNMAP_ICM(dev, mthca_uarc_virt(dev, &dev->driver_uar, i), 1);\r\ndma_free_coherent(&dev->pdev->dev, MTHCA_ICM_PAGE_SIZE,\r\npage->db_rec, page->mapping);\r\npage->db_rec = NULL;\r\nif (i == dev->db_tab->max_group1) {\r\n--dev->db_tab->max_group1;\r\n}\r\nif (i == dev->db_tab->min_group2)\r\n++dev->db_tab->min_group2;\r\n}\r\nmutex_unlock(&dev->db_tab->mutex);\r\n}\r\nint mthca_init_db_tab(struct mthca_dev *dev)\r\n{\r\nint i;\r\nif (!mthca_is_memfree(dev))\r\nreturn 0;\r\ndev->db_tab = kmalloc(sizeof *dev->db_tab, GFP_KERNEL);\r\nif (!dev->db_tab)\r\nreturn -ENOMEM;\r\nmutex_init(&dev->db_tab->mutex);\r\ndev->db_tab->npages = dev->uar_table.uarc_size / MTHCA_ICM_PAGE_SIZE;\r\ndev->db_tab->max_group1 = 0;\r\ndev->db_tab->min_group2 = dev->db_tab->npages - 1;\r\ndev->db_tab->page = kmalloc(dev->db_tab->npages *\r\nsizeof *dev->db_tab->page,\r\nGFP_KERNEL);\r\nif (!dev->db_tab->page) {\r\nkfree(dev->db_tab);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < dev->db_tab->npages; ++i)\r\ndev->db_tab->page[i].db_rec = NULL;\r\nreturn 0;\r\n}\r\nvoid mthca_cleanup_db_tab(struct mthca_dev *dev)\r\n{\r\nint i;\r\nif (!mthca_is_memfree(dev))\r\nreturn;\r\nfor (i = 0; i < dev->db_tab->npages; ++i) {\r\nif (!dev->db_tab->page[i].db_rec)\r\ncontinue;\r\nif (!bitmap_empty(dev->db_tab->page[i].used, MTHCA_DB_REC_PER_PAGE))\r\nmthca_warn(dev, "Kernel UARC page %d not empty\n", i);\r\nmthca_UNMAP_ICM(dev, mthca_uarc_virt(dev, &dev->driver_uar, i), 1);\r\ndma_free_coherent(&dev->pdev->dev, MTHCA_ICM_PAGE_SIZE,\r\ndev->db_tab->page[i].db_rec,\r\ndev->db_tab->page[i].mapping);\r\n}\r\nkfree(dev->db_tab->page);\r\nkfree(dev->db_tab);\r\n}
