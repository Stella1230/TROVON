static inline u32 omap_aes_read(struct omap_aes_dev *dd, u32 offset)\r\n{\r\nreturn __raw_readl(dd->io_base + offset);\r\n}\r\nstatic inline void omap_aes_write(struct omap_aes_dev *dd, u32 offset,\r\nu32 value)\r\n{\r\n__raw_writel(value, dd->io_base + offset);\r\n}\r\nstatic inline void omap_aes_write_mask(struct omap_aes_dev *dd, u32 offset,\r\nu32 value, u32 mask)\r\n{\r\nu32 val;\r\nval = omap_aes_read(dd, offset);\r\nval &= ~mask;\r\nval |= value;\r\nomap_aes_write(dd, offset, val);\r\n}\r\nstatic void omap_aes_write_n(struct omap_aes_dev *dd, u32 offset,\r\nu32 *value, int count)\r\n{\r\nfor (; count--; value++, offset += 4)\r\nomap_aes_write(dd, offset, *value);\r\n}\r\nstatic int omap_aes_hw_init(struct omap_aes_dev *dd)\r\n{\r\nint err;\r\nif (!(dd->flags & FLAGS_INIT)) {\r\ndd->flags |= FLAGS_INIT;\r\ndd->err = 0;\r\n}\r\nerr = pm_runtime_get_sync(dd->dev);\r\nif (err < 0) {\r\ndev_err(dd->dev, "failed to get sync: %d\n", err);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_aes_write_ctrl(struct omap_aes_dev *dd)\r\n{\r\nunsigned int key32;\r\nint i, err;\r\nu32 val;\r\nerr = omap_aes_hw_init(dd);\r\nif (err)\r\nreturn err;\r\nkey32 = dd->ctx->keylen / sizeof(u32);\r\nfor (i = 0; i < key32; i++) {\r\nomap_aes_write(dd, AES_REG_KEY(dd, i),\r\n__le32_to_cpu(dd->ctx->key[i]));\r\n}\r\nif ((dd->flags & (FLAGS_CBC | FLAGS_CTR)) && dd->req->info)\r\nomap_aes_write_n(dd, AES_REG_IV(dd, 0), dd->req->info, 4);\r\nval = FLD_VAL(((dd->ctx->keylen >> 3) - 1), 4, 3);\r\nif (dd->flags & FLAGS_CBC)\r\nval |= AES_REG_CTRL_CBC;\r\nif (dd->flags & FLAGS_CTR)\r\nval |= AES_REG_CTRL_CTR | AES_REG_CTRL_CTR_WIDTH_128;\r\nif (dd->flags & FLAGS_ENCRYPT)\r\nval |= AES_REG_CTRL_DIRECTION;\r\nomap_aes_write_mask(dd, AES_REG_CTRL(dd), val, AES_REG_CTRL_MASK);\r\nreturn 0;\r\n}\r\nstatic void omap_aes_dma_trigger_omap2(struct omap_aes_dev *dd, int length)\r\n{\r\nu32 mask, val;\r\nval = dd->pdata->dma_start;\r\nif (dd->dma_lch_out != NULL)\r\nval |= dd->pdata->dma_enable_out;\r\nif (dd->dma_lch_in != NULL)\r\nval |= dd->pdata->dma_enable_in;\r\nmask = dd->pdata->dma_enable_out | dd->pdata->dma_enable_in |\r\ndd->pdata->dma_start;\r\nomap_aes_write_mask(dd, AES_REG_MASK(dd), val, mask);\r\n}\r\nstatic void omap_aes_dma_trigger_omap4(struct omap_aes_dev *dd, int length)\r\n{\r\nomap_aes_write(dd, AES_REG_LENGTH_N(0), length);\r\nomap_aes_write(dd, AES_REG_LENGTH_N(1), 0);\r\nomap_aes_dma_trigger_omap2(dd, length);\r\n}\r\nstatic void omap_aes_dma_stop(struct omap_aes_dev *dd)\r\n{\r\nu32 mask;\r\nmask = dd->pdata->dma_enable_out | dd->pdata->dma_enable_in |\r\ndd->pdata->dma_start;\r\nomap_aes_write_mask(dd, AES_REG_MASK(dd), 0, mask);\r\n}\r\nstatic struct omap_aes_dev *omap_aes_find_dev(struct omap_aes_ctx *ctx)\r\n{\r\nstruct omap_aes_dev *dd;\r\nspin_lock_bh(&list_lock);\r\ndd = list_first_entry(&dev_list, struct omap_aes_dev, list);\r\nlist_move_tail(&dd->list, &dev_list);\r\nctx->dd = dd;\r\nspin_unlock_bh(&list_lock);\r\nreturn dd;\r\n}\r\nstatic void omap_aes_dma_out_callback(void *data)\r\n{\r\nstruct omap_aes_dev *dd = data;\r\ntasklet_schedule(&dd->done_task);\r\n}\r\nstatic int omap_aes_dma_init(struct omap_aes_dev *dd)\r\n{\r\nint err;\r\ndd->dma_lch_out = NULL;\r\ndd->dma_lch_in = NULL;\r\ndd->dma_lch_in = dma_request_chan(dd->dev, "rx");\r\nif (IS_ERR(dd->dma_lch_in)) {\r\ndev_err(dd->dev, "Unable to request in DMA channel\n");\r\nreturn PTR_ERR(dd->dma_lch_in);\r\n}\r\ndd->dma_lch_out = dma_request_chan(dd->dev, "tx");\r\nif (IS_ERR(dd->dma_lch_out)) {\r\ndev_err(dd->dev, "Unable to request out DMA channel\n");\r\nerr = PTR_ERR(dd->dma_lch_out);\r\ngoto err_dma_out;\r\n}\r\nreturn 0;\r\nerr_dma_out:\r\ndma_release_channel(dd->dma_lch_in);\r\nreturn err;\r\n}\r\nstatic void omap_aes_dma_cleanup(struct omap_aes_dev *dd)\r\n{\r\nif (dd->pio_only)\r\nreturn;\r\ndma_release_channel(dd->dma_lch_out);\r\ndma_release_channel(dd->dma_lch_in);\r\n}\r\nstatic void sg_copy_buf(void *buf, struct scatterlist *sg,\r\nunsigned int start, unsigned int nbytes, int out)\r\n{\r\nstruct scatter_walk walk;\r\nif (!nbytes)\r\nreturn;\r\nscatterwalk_start(&walk, sg);\r\nscatterwalk_advance(&walk, start);\r\nscatterwalk_copychunks(buf, &walk, nbytes, out);\r\nscatterwalk_done(&walk, out, 0);\r\n}\r\nstatic int omap_aes_crypt_dma(struct crypto_tfm *tfm,\r\nstruct scatterlist *in_sg, struct scatterlist *out_sg,\r\nint in_sg_len, int out_sg_len)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct omap_aes_dev *dd = ctx->dd;\r\nstruct dma_async_tx_descriptor *tx_in, *tx_out;\r\nstruct dma_slave_config cfg;\r\nint ret;\r\nif (dd->pio_only) {\r\nscatterwalk_start(&dd->in_walk, dd->in_sg);\r\nscatterwalk_start(&dd->out_walk, dd->out_sg);\r\nomap_aes_write(dd, AES_REG_IRQ_ENABLE(dd), 0x2);\r\nreturn 0;\r\n}\r\ndma_sync_sg_for_device(dd->dev, dd->in_sg, in_sg_len, DMA_TO_DEVICE);\r\nmemset(&cfg, 0, sizeof(cfg));\r\ncfg.src_addr = dd->phys_base + AES_REG_DATA_N(dd, 0);\r\ncfg.dst_addr = dd->phys_base + AES_REG_DATA_N(dd, 0);\r\ncfg.src_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\ncfg.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\ncfg.src_maxburst = DST_MAXBURST;\r\ncfg.dst_maxburst = DST_MAXBURST;\r\nret = dmaengine_slave_config(dd->dma_lch_in, &cfg);\r\nif (ret) {\r\ndev_err(dd->dev, "can't configure IN dmaengine slave: %d\n",\r\nret);\r\nreturn ret;\r\n}\r\ntx_in = dmaengine_prep_slave_sg(dd->dma_lch_in, in_sg, in_sg_len,\r\nDMA_MEM_TO_DEV,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!tx_in) {\r\ndev_err(dd->dev, "IN prep_slave_sg() failed\n");\r\nreturn -EINVAL;\r\n}\r\ntx_in->callback_param = dd;\r\nret = dmaengine_slave_config(dd->dma_lch_out, &cfg);\r\nif (ret) {\r\ndev_err(dd->dev, "can't configure OUT dmaengine slave: %d\n",\r\nret);\r\nreturn ret;\r\n}\r\ntx_out = dmaengine_prep_slave_sg(dd->dma_lch_out, out_sg, out_sg_len,\r\nDMA_DEV_TO_MEM,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!tx_out) {\r\ndev_err(dd->dev, "OUT prep_slave_sg() failed\n");\r\nreturn -EINVAL;\r\n}\r\ntx_out->callback = omap_aes_dma_out_callback;\r\ntx_out->callback_param = dd;\r\ndmaengine_submit(tx_in);\r\ndmaengine_submit(tx_out);\r\ndma_async_issue_pending(dd->dma_lch_in);\r\ndma_async_issue_pending(dd->dma_lch_out);\r\ndd->pdata->trigger(dd, dd->total);\r\nreturn 0;\r\n}\r\nstatic int omap_aes_crypt_dma_start(struct omap_aes_dev *dd)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(\r\ncrypto_ablkcipher_reqtfm(dd->req));\r\nint err;\r\npr_debug("total: %d\n", dd->total);\r\nif (!dd->pio_only) {\r\nerr = dma_map_sg(dd->dev, dd->in_sg, dd->in_sg_len,\r\nDMA_TO_DEVICE);\r\nif (!err) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\nreturn -EINVAL;\r\n}\r\nerr = dma_map_sg(dd->dev, dd->out_sg, dd->out_sg_len,\r\nDMA_FROM_DEVICE);\r\nif (!err) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nerr = omap_aes_crypt_dma(tfm, dd->in_sg, dd->out_sg, dd->in_sg_len,\r\ndd->out_sg_len);\r\nif (err && !dd->pio_only) {\r\ndma_unmap_sg(dd->dev, dd->in_sg, dd->in_sg_len, DMA_TO_DEVICE);\r\ndma_unmap_sg(dd->dev, dd->out_sg, dd->out_sg_len,\r\nDMA_FROM_DEVICE);\r\n}\r\nreturn err;\r\n}\r\nstatic void omap_aes_finish_req(struct omap_aes_dev *dd, int err)\r\n{\r\nstruct ablkcipher_request *req = dd->req;\r\npr_debug("err: %d\n", err);\r\ncrypto_finalize_cipher_request(dd->engine, req, err);\r\npm_runtime_mark_last_busy(dd->dev);\r\npm_runtime_put_autosuspend(dd->dev);\r\n}\r\nstatic int omap_aes_crypt_dma_stop(struct omap_aes_dev *dd)\r\n{\r\npr_debug("total: %d\n", dd->total);\r\nomap_aes_dma_stop(dd);\r\nreturn 0;\r\n}\r\nstatic int omap_aes_check_aligned(struct scatterlist *sg, int total)\r\n{\r\nint len = 0;\r\nif (!IS_ALIGNED(total, AES_BLOCK_SIZE))\r\nreturn -EINVAL;\r\nwhile (sg) {\r\nif (!IS_ALIGNED(sg->offset, 4))\r\nreturn -1;\r\nif (!IS_ALIGNED(sg->length, AES_BLOCK_SIZE))\r\nreturn -1;\r\nlen += sg->length;\r\nsg = sg_next(sg);\r\n}\r\nif (len != total)\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic int omap_aes_copy_sgs(struct omap_aes_dev *dd)\r\n{\r\nvoid *buf_in, *buf_out;\r\nint pages, total;\r\ntotal = ALIGN(dd->total, AES_BLOCK_SIZE);\r\npages = get_order(total);\r\nbuf_in = (void *)__get_free_pages(GFP_ATOMIC, pages);\r\nbuf_out = (void *)__get_free_pages(GFP_ATOMIC, pages);\r\nif (!buf_in || !buf_out) {\r\npr_err("Couldn't allocated pages for unaligned cases.\n");\r\nreturn -1;\r\n}\r\ndd->orig_out = dd->out_sg;\r\nsg_copy_buf(buf_in, dd->in_sg, 0, dd->total, 0);\r\nsg_init_table(&dd->in_sgl, 1);\r\nsg_set_buf(&dd->in_sgl, buf_in, total);\r\ndd->in_sg = &dd->in_sgl;\r\ndd->in_sg_len = 1;\r\nsg_init_table(&dd->out_sgl, 1);\r\nsg_set_buf(&dd->out_sgl, buf_out, total);\r\ndd->out_sg = &dd->out_sgl;\r\ndd->out_sg_len = 1;\r\nreturn 0;\r\n}\r\nstatic int omap_aes_handle_queue(struct omap_aes_dev *dd,\r\nstruct ablkcipher_request *req)\r\n{\r\nif (req)\r\nreturn crypto_transfer_cipher_request_to_engine(dd->engine, req);\r\nreturn 0;\r\n}\r\nstatic int omap_aes_prepare_req(struct crypto_engine *engine,\r\nstruct ablkcipher_request *req)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nstruct omap_aes_dev *dd = ctx->dd;\r\nstruct omap_aes_reqctx *rctx;\r\nif (!dd)\r\nreturn -ENODEV;\r\ndd->req = req;\r\ndd->total = req->nbytes;\r\ndd->total_save = req->nbytes;\r\ndd->in_sg = req->src;\r\ndd->out_sg = req->dst;\r\ndd->in_sg_len = sg_nents_for_len(dd->in_sg, dd->total);\r\nif (dd->in_sg_len < 0)\r\nreturn dd->in_sg_len;\r\ndd->out_sg_len = sg_nents_for_len(dd->out_sg, dd->total);\r\nif (dd->out_sg_len < 0)\r\nreturn dd->out_sg_len;\r\nif (omap_aes_check_aligned(dd->in_sg, dd->total) ||\r\nomap_aes_check_aligned(dd->out_sg, dd->total)) {\r\nif (omap_aes_copy_sgs(dd))\r\npr_err("Failed to copy SGs for unaligned cases\n");\r\ndd->sgs_copied = 1;\r\n} else {\r\ndd->sgs_copied = 0;\r\n}\r\nrctx = ablkcipher_request_ctx(req);\r\nctx = crypto_ablkcipher_ctx(crypto_ablkcipher_reqtfm(req));\r\nrctx->mode &= FLAGS_MODE_MASK;\r\ndd->flags = (dd->flags & ~FLAGS_MODE_MASK) | rctx->mode;\r\ndd->ctx = ctx;\r\nctx->dd = dd;\r\nreturn omap_aes_write_ctrl(dd);\r\n}\r\nstatic int omap_aes_crypt_req(struct crypto_engine *engine,\r\nstruct ablkcipher_request *req)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nstruct omap_aes_dev *dd = ctx->dd;\r\nif (!dd)\r\nreturn -ENODEV;\r\nreturn omap_aes_crypt_dma_start(dd);\r\n}\r\nstatic void omap_aes_done_task(unsigned long data)\r\n{\r\nstruct omap_aes_dev *dd = (struct omap_aes_dev *)data;\r\nvoid *buf_in, *buf_out;\r\nint pages, len;\r\npr_debug("enter done_task\n");\r\nif (!dd->pio_only) {\r\ndma_sync_sg_for_device(dd->dev, dd->out_sg, dd->out_sg_len,\r\nDMA_FROM_DEVICE);\r\ndma_unmap_sg(dd->dev, dd->in_sg, dd->in_sg_len, DMA_TO_DEVICE);\r\ndma_unmap_sg(dd->dev, dd->out_sg, dd->out_sg_len,\r\nDMA_FROM_DEVICE);\r\nomap_aes_crypt_dma_stop(dd);\r\n}\r\nif (dd->sgs_copied) {\r\nbuf_in = sg_virt(&dd->in_sgl);\r\nbuf_out = sg_virt(&dd->out_sgl);\r\nsg_copy_buf(buf_out, dd->orig_out, 0, dd->total_save, 1);\r\nlen = ALIGN(dd->total_save, AES_BLOCK_SIZE);\r\npages = get_order(len);\r\nfree_pages((unsigned long)buf_in, pages);\r\nfree_pages((unsigned long)buf_out, pages);\r\n}\r\nomap_aes_finish_req(dd, 0);\r\npr_debug("exit\n");\r\n}\r\nstatic int omap_aes_crypt(struct ablkcipher_request *req, unsigned long mode)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nstruct omap_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct omap_aes_dev *dd;\r\nint ret;\r\npr_debug("nbytes: %d, enc: %d, cbc: %d\n", req->nbytes,\r\n!!(mode & FLAGS_ENCRYPT),\r\n!!(mode & FLAGS_CBC));\r\nif (req->nbytes < 200) {\r\nSKCIPHER_REQUEST_ON_STACK(subreq, ctx->fallback);\r\nskcipher_request_set_tfm(subreq, ctx->fallback);\r\nskcipher_request_set_callback(subreq, req->base.flags, NULL,\r\nNULL);\r\nskcipher_request_set_crypt(subreq, req->src, req->dst,\r\nreq->nbytes, req->info);\r\nif (mode & FLAGS_ENCRYPT)\r\nret = crypto_skcipher_encrypt(subreq);\r\nelse\r\nret = crypto_skcipher_decrypt(subreq);\r\nskcipher_request_zero(subreq);\r\nreturn ret;\r\n}\r\ndd = omap_aes_find_dev(ctx);\r\nif (!dd)\r\nreturn -ENODEV;\r\nrctx->mode = mode;\r\nreturn omap_aes_handle_queue(dd, req);\r\n}\r\nstatic int omap_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nint ret;\r\nif (keylen != AES_KEYSIZE_128 && keylen != AES_KEYSIZE_192 &&\r\nkeylen != AES_KEYSIZE_256)\r\nreturn -EINVAL;\r\npr_debug("enter, keylen: %d\n", keylen);\r\nmemcpy(ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\ncrypto_skcipher_clear_flags(ctx->fallback, CRYPTO_TFM_REQ_MASK);\r\ncrypto_skcipher_set_flags(ctx->fallback, tfm->base.crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nret = crypto_skcipher_setkey(ctx->fallback, key, keylen);\r\nif (!ret)\r\nreturn 0;\r\nreturn 0;\r\n}\r\nstatic int omap_aes_ecb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_ENCRYPT);\r\n}\r\nstatic int omap_aes_ecb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, 0);\r\n}\r\nstatic int omap_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_CBC);\r\n}\r\nstatic int omap_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_CBC);\r\n}\r\nstatic int omap_aes_ctr_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_CTR);\r\n}\r\nstatic int omap_aes_ctr_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_CTR);\r\n}\r\nstatic int omap_aes_cra_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = crypto_tfm_alg_name(tfm);\r\nconst u32 flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK;\r\nstruct omap_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct crypto_skcipher *blk;\r\nblk = crypto_alloc_skcipher(name, 0, flags);\r\nif (IS_ERR(blk))\r\nreturn PTR_ERR(blk);\r\nctx->fallback = blk;\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct omap_aes_reqctx);\r\nreturn 0;\r\n}\r\nstatic void omap_aes_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->fallback)\r\ncrypto_free_skcipher(ctx->fallback);\r\nctx->fallback = NULL;\r\n}\r\nstatic irqreturn_t omap_aes_irq(int irq, void *dev_id)\r\n{\r\nstruct omap_aes_dev *dd = dev_id;\r\nu32 status, i;\r\nu32 *src, *dst;\r\nstatus = omap_aes_read(dd, AES_REG_IRQ_STATUS(dd));\r\nif (status & AES_REG_IRQ_DATA_IN) {\r\nomap_aes_write(dd, AES_REG_IRQ_ENABLE(dd), 0x0);\r\nBUG_ON(!dd->in_sg);\r\nBUG_ON(_calc_walked(in) > dd->in_sg->length);\r\nsrc = sg_virt(dd->in_sg) + _calc_walked(in);\r\nfor (i = 0; i < AES_BLOCK_WORDS; i++) {\r\nomap_aes_write(dd, AES_REG_DATA_N(dd, i), *src);\r\nscatterwalk_advance(&dd->in_walk, 4);\r\nif (dd->in_sg->length == _calc_walked(in)) {\r\ndd->in_sg = sg_next(dd->in_sg);\r\nif (dd->in_sg) {\r\nscatterwalk_start(&dd->in_walk,\r\ndd->in_sg);\r\nsrc = sg_virt(dd->in_sg) +\r\n_calc_walked(in);\r\n}\r\n} else {\r\nsrc++;\r\n}\r\n}\r\nstatus &= ~AES_REG_IRQ_DATA_IN;\r\nomap_aes_write(dd, AES_REG_IRQ_STATUS(dd), status);\r\nomap_aes_write(dd, AES_REG_IRQ_ENABLE(dd), 0x4);\r\n} else if (status & AES_REG_IRQ_DATA_OUT) {\r\nomap_aes_write(dd, AES_REG_IRQ_ENABLE(dd), 0x0);\r\nBUG_ON(!dd->out_sg);\r\nBUG_ON(_calc_walked(out) > dd->out_sg->length);\r\ndst = sg_virt(dd->out_sg) + _calc_walked(out);\r\nfor (i = 0; i < AES_BLOCK_WORDS; i++) {\r\n*dst = omap_aes_read(dd, AES_REG_DATA_N(dd, i));\r\nscatterwalk_advance(&dd->out_walk, 4);\r\nif (dd->out_sg->length == _calc_walked(out)) {\r\ndd->out_sg = sg_next(dd->out_sg);\r\nif (dd->out_sg) {\r\nscatterwalk_start(&dd->out_walk,\r\ndd->out_sg);\r\ndst = sg_virt(dd->out_sg) +\r\n_calc_walked(out);\r\n}\r\n} else {\r\ndst++;\r\n}\r\n}\r\ndd->total -= min_t(size_t, AES_BLOCK_SIZE, dd->total);\r\nstatus &= ~AES_REG_IRQ_DATA_OUT;\r\nomap_aes_write(dd, AES_REG_IRQ_STATUS(dd), status);\r\nif (!dd->total)\r\ntasklet_schedule(&dd->done_task);\r\nelse\r\nomap_aes_write(dd, AES_REG_IRQ_ENABLE(dd), 0x2);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int omap_aes_get_res_of(struct omap_aes_dev *dd,\r\nstruct device *dev, struct resource *res)\r\n{\r\nstruct device_node *node = dev->of_node;\r\nconst struct of_device_id *match;\r\nint err = 0;\r\nmatch = of_match_device(of_match_ptr(omap_aes_of_match), dev);\r\nif (!match) {\r\ndev_err(dev, "no compatible OF match\n");\r\nerr = -EINVAL;\r\ngoto err;\r\n}\r\nerr = of_address_to_resource(node, 0, res);\r\nif (err < 0) {\r\ndev_err(dev, "can't translate OF node address\n");\r\nerr = -EINVAL;\r\ngoto err;\r\n}\r\ndd->pdata = match->data;\r\nerr:\r\nreturn err;\r\n}\r\nstatic int omap_aes_get_res_of(struct omap_aes_dev *dd,\r\nstruct device *dev, struct resource *res)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int omap_aes_get_res_pdev(struct omap_aes_dev *dd,\r\nstruct platform_device *pdev, struct resource *res)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *r;\r\nint err = 0;\r\nr = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!r) {\r\ndev_err(dev, "no MEM resource info\n");\r\nerr = -ENODEV;\r\ngoto err;\r\n}\r\nmemcpy(res, r, sizeof(*res));\r\ndd->pdata = &omap_aes_pdata_omap2;\r\nerr:\r\nreturn err;\r\n}\r\nstatic int omap_aes_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct omap_aes_dev *dd;\r\nstruct crypto_alg *algp;\r\nstruct resource res;\r\nint err = -ENOMEM, i, j, irq = -1;\r\nu32 reg;\r\ndd = devm_kzalloc(dev, sizeof(struct omap_aes_dev), GFP_KERNEL);\r\nif (dd == NULL) {\r\ndev_err(dev, "unable to alloc data struct.\n");\r\ngoto err_data;\r\n}\r\ndd->dev = dev;\r\nplatform_set_drvdata(pdev, dd);\r\nerr = (dev->of_node) ? omap_aes_get_res_of(dd, dev, &res) :\r\nomap_aes_get_res_pdev(dd, pdev, &res);\r\nif (err)\r\ngoto err_res;\r\ndd->io_base = devm_ioremap_resource(dev, &res);\r\nif (IS_ERR(dd->io_base)) {\r\nerr = PTR_ERR(dd->io_base);\r\ngoto err_res;\r\n}\r\ndd->phys_base = res.start;\r\npm_runtime_use_autosuspend(dev);\r\npm_runtime_set_autosuspend_delay(dev, DEFAULT_AUTOSUSPEND_DELAY);\r\npm_runtime_enable(dev);\r\nerr = pm_runtime_get_sync(dev);\r\nif (err < 0) {\r\ndev_err(dev, "%s: failed to get_sync(%d)\n",\r\n__func__, err);\r\ngoto err_res;\r\n}\r\nomap_aes_dma_stop(dd);\r\nreg = omap_aes_read(dd, AES_REG_REV(dd));\r\npm_runtime_put_sync(dev);\r\ndev_info(dev, "OMAP AES hw accel rev: %u.%u\n",\r\n(reg & dd->pdata->major_mask) >> dd->pdata->major_shift,\r\n(reg & dd->pdata->minor_mask) >> dd->pdata->minor_shift);\r\ntasklet_init(&dd->done_task, omap_aes_done_task, (unsigned long)dd);\r\nerr = omap_aes_dma_init(dd);\r\nif (err == -EPROBE_DEFER) {\r\ngoto err_irq;\r\n} else if (err && AES_REG_IRQ_STATUS(dd) && AES_REG_IRQ_ENABLE(dd)) {\r\ndd->pio_only = 1;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0) {\r\ndev_err(dev, "can't get IRQ resource\n");\r\ngoto err_irq;\r\n}\r\nerr = devm_request_irq(dev, irq, omap_aes_irq, 0,\r\ndev_name(dev), dd);\r\nif (err) {\r\ndev_err(dev, "Unable to grab omap-aes IRQ\n");\r\ngoto err_irq;\r\n}\r\n}\r\nINIT_LIST_HEAD(&dd->list);\r\nspin_lock(&list_lock);\r\nlist_add_tail(&dd->list, &dev_list);\r\nspin_unlock(&list_lock);\r\ndd->engine = crypto_engine_alloc_init(dev, 1);\r\nif (!dd->engine) {\r\nerr = -ENOMEM;\r\ngoto err_engine;\r\n}\r\ndd->engine->prepare_cipher_request = omap_aes_prepare_req;\r\ndd->engine->cipher_one_request = omap_aes_crypt_req;\r\nerr = crypto_engine_start(dd->engine);\r\nif (err)\r\ngoto err_engine;\r\nfor (i = 0; i < dd->pdata->algs_info_size; i++) {\r\nif (!dd->pdata->algs_info[i].registered) {\r\nfor (j = 0; j < dd->pdata->algs_info[i].size; j++) {\r\nalgp = &dd->pdata->algs_info[i].algs_list[j];\r\npr_debug("reg alg: %s\n", algp->cra_name);\r\nINIT_LIST_HEAD(&algp->cra_list);\r\nerr = crypto_register_alg(algp);\r\nif (err)\r\ngoto err_algs;\r\ndd->pdata->algs_info[i].registered++;\r\n}\r\n}\r\n}\r\nreturn 0;\r\nerr_algs:\r\nfor (i = dd->pdata->algs_info_size - 1; i >= 0; i--)\r\nfor (j = dd->pdata->algs_info[i].registered - 1; j >= 0; j--)\r\ncrypto_unregister_alg(\r\n&dd->pdata->algs_info[i].algs_list[j]);\r\nerr_engine:\r\nif (dd->engine)\r\ncrypto_engine_exit(dd->engine);\r\nomap_aes_dma_cleanup(dd);\r\nerr_irq:\r\ntasklet_kill(&dd->done_task);\r\npm_runtime_disable(dev);\r\nerr_res:\r\ndd = NULL;\r\nerr_data:\r\ndev_err(dev, "initialization failed.\n");\r\nreturn err;\r\n}\r\nstatic int omap_aes_remove(struct platform_device *pdev)\r\n{\r\nstruct omap_aes_dev *dd = platform_get_drvdata(pdev);\r\nint i, j;\r\nif (!dd)\r\nreturn -ENODEV;\r\nspin_lock(&list_lock);\r\nlist_del(&dd->list);\r\nspin_unlock(&list_lock);\r\nfor (i = dd->pdata->algs_info_size - 1; i >= 0; i--)\r\nfor (j = dd->pdata->algs_info[i].registered - 1; j >= 0; j--)\r\ncrypto_unregister_alg(\r\n&dd->pdata->algs_info[i].algs_list[j]);\r\ncrypto_engine_exit(dd->engine);\r\ntasklet_kill(&dd->done_task);\r\nomap_aes_dma_cleanup(dd);\r\npm_runtime_disable(dd->dev);\r\ndd = NULL;\r\nreturn 0;\r\n}\r\nstatic int omap_aes_suspend(struct device *dev)\r\n{\r\npm_runtime_put_sync(dev);\r\nreturn 0;\r\n}\r\nstatic int omap_aes_resume(struct device *dev)\r\n{\r\npm_runtime_get_sync(dev);\r\nreturn 0;\r\n}
