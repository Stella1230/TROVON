int amdgpu_sa_bo_manager_init(struct amdgpu_device *adev,\r\nstruct amdgpu_sa_manager *sa_manager,\r\nunsigned size, u32 align, u32 domain)\r\n{\r\nint i, r;\r\ninit_waitqueue_head(&sa_manager->wq);\r\nsa_manager->bo = NULL;\r\nsa_manager->size = size;\r\nsa_manager->domain = domain;\r\nsa_manager->align = align;\r\nsa_manager->hole = &sa_manager->olist;\r\nINIT_LIST_HEAD(&sa_manager->olist);\r\nfor (i = 0; i < AMDGPU_SA_NUM_FENCE_LISTS; ++i)\r\nINIT_LIST_HEAD(&sa_manager->flist[i]);\r\nr = amdgpu_bo_create(adev, size, align, true, domain,\r\n0, NULL, NULL, &sa_manager->bo);\r\nif (r) {\r\ndev_err(adev->dev, "(%d) failed to allocate bo for manager\n", r);\r\nreturn r;\r\n}\r\nreturn r;\r\n}\r\nvoid amdgpu_sa_bo_manager_fini(struct amdgpu_device *adev,\r\nstruct amdgpu_sa_manager *sa_manager)\r\n{\r\nstruct amdgpu_sa_bo *sa_bo, *tmp;\r\nif (!list_empty(&sa_manager->olist)) {\r\nsa_manager->hole = &sa_manager->olist,\r\namdgpu_sa_bo_try_free(sa_manager);\r\nif (!list_empty(&sa_manager->olist)) {\r\ndev_err(adev->dev, "sa_manager is not empty, clearing anyway\n");\r\n}\r\n}\r\nlist_for_each_entry_safe(sa_bo, tmp, &sa_manager->olist, olist) {\r\namdgpu_sa_bo_remove_locked(sa_bo);\r\n}\r\namdgpu_bo_unref(&sa_manager->bo);\r\nsa_manager->size = 0;\r\n}\r\nint amdgpu_sa_bo_manager_start(struct amdgpu_device *adev,\r\nstruct amdgpu_sa_manager *sa_manager)\r\n{\r\nint r;\r\nif (sa_manager->bo == NULL) {\r\ndev_err(adev->dev, "no bo for sa manager\n");\r\nreturn -EINVAL;\r\n}\r\nr = amdgpu_bo_reserve(sa_manager->bo, false);\r\nif (r) {\r\ndev_err(adev->dev, "(%d) failed to reserve manager bo\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_pin(sa_manager->bo, sa_manager->domain, &sa_manager->gpu_addr);\r\nif (r) {\r\namdgpu_bo_unreserve(sa_manager->bo);\r\ndev_err(adev->dev, "(%d) failed to pin manager bo\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_kmap(sa_manager->bo, &sa_manager->cpu_ptr);\r\nmemset(sa_manager->cpu_ptr, 0, sa_manager->size);\r\namdgpu_bo_unreserve(sa_manager->bo);\r\nreturn r;\r\n}\r\nint amdgpu_sa_bo_manager_suspend(struct amdgpu_device *adev,\r\nstruct amdgpu_sa_manager *sa_manager)\r\n{\r\nint r;\r\nif (sa_manager->bo == NULL) {\r\ndev_err(adev->dev, "no bo for sa manager\n");\r\nreturn -EINVAL;\r\n}\r\nr = amdgpu_bo_reserve(sa_manager->bo, false);\r\nif (!r) {\r\namdgpu_bo_kunmap(sa_manager->bo);\r\namdgpu_bo_unpin(sa_manager->bo);\r\namdgpu_bo_unreserve(sa_manager->bo);\r\n}\r\nreturn r;\r\n}\r\nstatic void amdgpu_sa_bo_remove_locked(struct amdgpu_sa_bo *sa_bo)\r\n{\r\nstruct amdgpu_sa_manager *sa_manager = sa_bo->manager;\r\nif (sa_manager->hole == &sa_bo->olist) {\r\nsa_manager->hole = sa_bo->olist.prev;\r\n}\r\nlist_del_init(&sa_bo->olist);\r\nlist_del_init(&sa_bo->flist);\r\ndma_fence_put(sa_bo->fence);\r\nkfree(sa_bo);\r\n}\r\nstatic void amdgpu_sa_bo_try_free(struct amdgpu_sa_manager *sa_manager)\r\n{\r\nstruct amdgpu_sa_bo *sa_bo, *tmp;\r\nif (sa_manager->hole->next == &sa_manager->olist)\r\nreturn;\r\nsa_bo = list_entry(sa_manager->hole->next, struct amdgpu_sa_bo, olist);\r\nlist_for_each_entry_safe_from(sa_bo, tmp, &sa_manager->olist, olist) {\r\nif (sa_bo->fence == NULL ||\r\n!dma_fence_is_signaled(sa_bo->fence)) {\r\nreturn;\r\n}\r\namdgpu_sa_bo_remove_locked(sa_bo);\r\n}\r\n}\r\nstatic inline unsigned amdgpu_sa_bo_hole_soffset(struct amdgpu_sa_manager *sa_manager)\r\n{\r\nstruct list_head *hole = sa_manager->hole;\r\nif (hole != &sa_manager->olist) {\r\nreturn list_entry(hole, struct amdgpu_sa_bo, olist)->eoffset;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline unsigned amdgpu_sa_bo_hole_eoffset(struct amdgpu_sa_manager *sa_manager)\r\n{\r\nstruct list_head *hole = sa_manager->hole;\r\nif (hole->next != &sa_manager->olist) {\r\nreturn list_entry(hole->next, struct amdgpu_sa_bo, olist)->soffset;\r\n}\r\nreturn sa_manager->size;\r\n}\r\nstatic bool amdgpu_sa_bo_try_alloc(struct amdgpu_sa_manager *sa_manager,\r\nstruct amdgpu_sa_bo *sa_bo,\r\nunsigned size, unsigned align)\r\n{\r\nunsigned soffset, eoffset, wasted;\r\nsoffset = amdgpu_sa_bo_hole_soffset(sa_manager);\r\neoffset = amdgpu_sa_bo_hole_eoffset(sa_manager);\r\nwasted = (align - (soffset % align)) % align;\r\nif ((eoffset - soffset) >= (size + wasted)) {\r\nsoffset += wasted;\r\nsa_bo->manager = sa_manager;\r\nsa_bo->soffset = soffset;\r\nsa_bo->eoffset = soffset + size;\r\nlist_add(&sa_bo->olist, sa_manager->hole);\r\nINIT_LIST_HEAD(&sa_bo->flist);\r\nsa_manager->hole = &sa_bo->olist;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool amdgpu_sa_event(struct amdgpu_sa_manager *sa_manager,\r\nunsigned size, unsigned align)\r\n{\r\nunsigned soffset, eoffset, wasted;\r\nint i;\r\nfor (i = 0; i < AMDGPU_SA_NUM_FENCE_LISTS; ++i)\r\nif (!list_empty(&sa_manager->flist[i]))\r\nreturn true;\r\nsoffset = amdgpu_sa_bo_hole_soffset(sa_manager);\r\neoffset = amdgpu_sa_bo_hole_eoffset(sa_manager);\r\nwasted = (align - (soffset % align)) % align;\r\nif ((eoffset - soffset) >= (size + wasted)) {\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool amdgpu_sa_bo_next_hole(struct amdgpu_sa_manager *sa_manager,\r\nstruct dma_fence **fences,\r\nunsigned *tries)\r\n{\r\nstruct amdgpu_sa_bo *best_bo = NULL;\r\nunsigned i, soffset, best, tmp;\r\nif (sa_manager->hole->next == &sa_manager->olist) {\r\nsa_manager->hole = &sa_manager->olist;\r\nreturn true;\r\n}\r\nsoffset = amdgpu_sa_bo_hole_soffset(sa_manager);\r\nbest = sa_manager->size * 2;\r\nfor (i = 0; i < AMDGPU_SA_NUM_FENCE_LISTS; ++i) {\r\nstruct amdgpu_sa_bo *sa_bo;\r\nif (list_empty(&sa_manager->flist[i]))\r\ncontinue;\r\nsa_bo = list_first_entry(&sa_manager->flist[i],\r\nstruct amdgpu_sa_bo, flist);\r\nif (!dma_fence_is_signaled(sa_bo->fence)) {\r\nfences[i] = sa_bo->fence;\r\ncontinue;\r\n}\r\nif (tries[i] > 2) {\r\ncontinue;\r\n}\r\ntmp = sa_bo->soffset;\r\nif (tmp < soffset) {\r\ntmp += sa_manager->size;\r\n}\r\ntmp -= soffset;\r\nif (tmp < best) {\r\nbest = tmp;\r\nbest_bo = sa_bo;\r\n}\r\n}\r\nif (best_bo) {\r\nuint32_t idx = best_bo->fence->context;\r\nidx %= AMDGPU_SA_NUM_FENCE_LISTS;\r\n++tries[idx];\r\nsa_manager->hole = best_bo->olist.prev;\r\namdgpu_sa_bo_remove_locked(best_bo);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nint amdgpu_sa_bo_new(struct amdgpu_sa_manager *sa_manager,\r\nstruct amdgpu_sa_bo **sa_bo,\r\nunsigned size, unsigned align)\r\n{\r\nstruct dma_fence *fences[AMDGPU_SA_NUM_FENCE_LISTS];\r\nunsigned tries[AMDGPU_SA_NUM_FENCE_LISTS];\r\nunsigned count;\r\nint i, r;\r\nsigned long t;\r\nif (WARN_ON_ONCE(align > sa_manager->align))\r\nreturn -EINVAL;\r\nif (WARN_ON_ONCE(size > sa_manager->size))\r\nreturn -EINVAL;\r\n*sa_bo = kmalloc(sizeof(struct amdgpu_sa_bo), GFP_KERNEL);\r\nif (!(*sa_bo))\r\nreturn -ENOMEM;\r\n(*sa_bo)->manager = sa_manager;\r\n(*sa_bo)->fence = NULL;\r\nINIT_LIST_HEAD(&(*sa_bo)->olist);\r\nINIT_LIST_HEAD(&(*sa_bo)->flist);\r\nspin_lock(&sa_manager->wq.lock);\r\ndo {\r\nfor (i = 0; i < AMDGPU_SA_NUM_FENCE_LISTS; ++i) {\r\nfences[i] = NULL;\r\ntries[i] = 0;\r\n}\r\ndo {\r\namdgpu_sa_bo_try_free(sa_manager);\r\nif (amdgpu_sa_bo_try_alloc(sa_manager, *sa_bo,\r\nsize, align)) {\r\nspin_unlock(&sa_manager->wq.lock);\r\nreturn 0;\r\n}\r\n} while (amdgpu_sa_bo_next_hole(sa_manager, fences, tries));\r\nfor (i = 0, count = 0; i < AMDGPU_SA_NUM_FENCE_LISTS; ++i)\r\nif (fences[i])\r\nfences[count++] = dma_fence_get(fences[i]);\r\nif (count) {\r\nspin_unlock(&sa_manager->wq.lock);\r\nt = dma_fence_wait_any_timeout(fences, count, false,\r\nMAX_SCHEDULE_TIMEOUT,\r\nNULL);\r\nfor (i = 0; i < count; ++i)\r\ndma_fence_put(fences[i]);\r\nr = (t > 0) ? 0 : t;\r\nspin_lock(&sa_manager->wq.lock);\r\n} else {\r\nr = wait_event_interruptible_locked(\r\nsa_manager->wq,\r\namdgpu_sa_event(sa_manager, size, align)\r\n);\r\n}\r\n} while (!r);\r\nspin_unlock(&sa_manager->wq.lock);\r\nkfree(*sa_bo);\r\n*sa_bo = NULL;\r\nreturn r;\r\n}\r\nvoid amdgpu_sa_bo_free(struct amdgpu_device *adev, struct amdgpu_sa_bo **sa_bo,\r\nstruct dma_fence *fence)\r\n{\r\nstruct amdgpu_sa_manager *sa_manager;\r\nif (sa_bo == NULL || *sa_bo == NULL) {\r\nreturn;\r\n}\r\nsa_manager = (*sa_bo)->manager;\r\nspin_lock(&sa_manager->wq.lock);\r\nif (fence && !dma_fence_is_signaled(fence)) {\r\nuint32_t idx;\r\n(*sa_bo)->fence = dma_fence_get(fence);\r\nidx = fence->context % AMDGPU_SA_NUM_FENCE_LISTS;\r\nlist_add_tail(&(*sa_bo)->flist, &sa_manager->flist[idx]);\r\n} else {\r\namdgpu_sa_bo_remove_locked(*sa_bo);\r\n}\r\nwake_up_all_locked(&sa_manager->wq);\r\nspin_unlock(&sa_manager->wq.lock);\r\n*sa_bo = NULL;\r\n}\r\nvoid amdgpu_sa_bo_dump_debug_info(struct amdgpu_sa_manager *sa_manager,\r\nstruct seq_file *m)\r\n{\r\nstruct amdgpu_sa_bo *i;\r\nspin_lock(&sa_manager->wq.lock);\r\nlist_for_each_entry(i, &sa_manager->olist, olist) {\r\nuint64_t soffset = i->soffset + sa_manager->gpu_addr;\r\nuint64_t eoffset = i->eoffset + sa_manager->gpu_addr;\r\nif (&i->olist == sa_manager->hole) {\r\nseq_printf(m, ">");\r\n} else {\r\nseq_printf(m, " ");\r\n}\r\nseq_printf(m, "[0x%010llx 0x%010llx] size %8lld",\r\nsoffset, eoffset, eoffset - soffset);\r\nif (i->fence)\r\nseq_printf(m, " protected by 0x%08x on context %llu",\r\ni->fence->seqno, i->fence->context);\r\nseq_printf(m, "\n");\r\n}\r\nspin_unlock(&sa_manager->wq.lock);\r\n}
