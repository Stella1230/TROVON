static int early_page_poison_param(char *buf)\r\n{\r\nif (!buf)\r\nreturn -EINVAL;\r\nreturn strtobool(buf, &want_page_poisoning);\r\n}\r\nbool page_poisoning_enabled(void)\r\n{\r\nreturn __page_poisoning_enabled;\r\n}\r\nstatic bool need_page_poisoning(void)\r\n{\r\nreturn want_page_poisoning;\r\n}\r\nstatic void init_page_poisoning(void)\r\n{\r\nif (!IS_ENABLED(CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC)) {\r\nif (!want_page_poisoning && !debug_pagealloc_enabled())\r\nreturn;\r\n} else {\r\nif (!want_page_poisoning)\r\nreturn;\r\n}\r\n__page_poisoning_enabled = true;\r\n}\r\nstatic inline void set_page_poison(struct page *page)\r\n{\r\nstruct page_ext *page_ext;\r\npage_ext = lookup_page_ext(page);\r\nif (unlikely(!page_ext))\r\nreturn;\r\n__set_bit(PAGE_EXT_DEBUG_POISON, &page_ext->flags);\r\n}\r\nstatic inline void clear_page_poison(struct page *page)\r\n{\r\nstruct page_ext *page_ext;\r\npage_ext = lookup_page_ext(page);\r\nif (unlikely(!page_ext))\r\nreturn;\r\n__clear_bit(PAGE_EXT_DEBUG_POISON, &page_ext->flags);\r\n}\r\nbool page_is_poisoned(struct page *page)\r\n{\r\nstruct page_ext *page_ext;\r\npage_ext = lookup_page_ext(page);\r\nif (unlikely(!page_ext))\r\nreturn false;\r\nreturn test_bit(PAGE_EXT_DEBUG_POISON, &page_ext->flags);\r\n}\r\nstatic void poison_page(struct page *page)\r\n{\r\nvoid *addr = kmap_atomic(page);\r\nset_page_poison(page);\r\nmemset(addr, PAGE_POISON, PAGE_SIZE);\r\nkunmap_atomic(addr);\r\n}\r\nstatic void poison_pages(struct page *page, int n)\r\n{\r\nint i;\r\nfor (i = 0; i < n; i++)\r\npoison_page(page + i);\r\n}\r\nstatic bool single_bit_flip(unsigned char a, unsigned char b)\r\n{\r\nunsigned char error = a ^ b;\r\nreturn error && !(error & (error - 1));\r\n}\r\nstatic void check_poison_mem(unsigned char *mem, size_t bytes)\r\n{\r\nstatic DEFINE_RATELIMIT_STATE(ratelimit, 5 * HZ, 10);\r\nunsigned char *start;\r\nunsigned char *end;\r\nif (IS_ENABLED(CONFIG_PAGE_POISONING_NO_SANITY))\r\nreturn;\r\nstart = memchr_inv(mem, PAGE_POISON, bytes);\r\nif (!start)\r\nreturn;\r\nfor (end = mem + bytes - 1; end > start; end--) {\r\nif (*end != PAGE_POISON)\r\nbreak;\r\n}\r\nif (!__ratelimit(&ratelimit))\r\nreturn;\r\nelse if (start == end && single_bit_flip(*start, PAGE_POISON))\r\npr_err("pagealloc: single bit error\n");\r\nelse\r\npr_err("pagealloc: memory corruption\n");\r\nprint_hex_dump(KERN_ERR, "", DUMP_PREFIX_ADDRESS, 16, 1, start,\r\nend - start + 1, 1);\r\ndump_stack();\r\n}\r\nstatic void unpoison_page(struct page *page)\r\n{\r\nvoid *addr;\r\nif (!page_is_poisoned(page))\r\nreturn;\r\naddr = kmap_atomic(page);\r\ncheck_poison_mem(addr, PAGE_SIZE);\r\nclear_page_poison(page);\r\nkunmap_atomic(addr);\r\n}\r\nstatic void unpoison_pages(struct page *page, int n)\r\n{\r\nint i;\r\nfor (i = 0; i < n; i++)\r\nunpoison_page(page + i);\r\n}\r\nvoid kernel_poison_pages(struct page *page, int numpages, int enable)\r\n{\r\nif (!page_poisoning_enabled())\r\nreturn;\r\nif (enable)\r\nunpoison_pages(page, numpages);\r\nelse\r\npoison_pages(page, numpages);\r\n}\r\nvoid __kernel_map_pages(struct page *page, int numpages, int enable)\r\n{\r\n}
