static inline u64 _ioread64(void __iomem *mmio)\r\n{\r\nu64 low, high;\r\nlow = ioread32(mmio);\r\nhigh = ioread32(mmio + sizeof(u32));\r\nreturn low | (high << 32);\r\n}\r\nstatic inline void _iowrite64(u64 val, void __iomem *mmio)\r\n{\r\niowrite32(val, mmio);\r\niowrite32(val >> 32, mmio + sizeof(u32));\r\n}\r\nstatic inline int pdev_is_atom(struct pci_dev *pdev)\r\n{\r\nswitch (pdev->device) {\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_BWD:\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int pdev_is_xeon(struct pci_dev *pdev)\r\n{\r\nswitch (pdev->device) {\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_BDX:\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int pdev_is_skx_xeon(struct pci_dev *pdev)\r\n{\r\nif (pdev->device == PCI_DEVICE_ID_INTEL_NTB_B2B_SKX)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline void ndev_reset_unsafe_flags(struct intel_ntb_dev *ndev)\r\n{\r\nndev->unsafe_flags = 0;\r\nndev->unsafe_flags_ignore = 0;\r\nif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP)\r\nif (!ntb_topo_is_b2b(ndev->ntb.topo))\r\nndev->unsafe_flags |= NTB_UNSAFE_DB;\r\nif (ndev->hwerr_flags & NTB_HWERR_SB01BASE_LOCKUP) {\r\nndev->unsafe_flags |= NTB_UNSAFE_DB;\r\nndev->unsafe_flags |= NTB_UNSAFE_SPAD;\r\n}\r\n}\r\nstatic inline int ndev_is_unsafe(struct intel_ntb_dev *ndev,\r\nunsigned long flag)\r\n{\r\nreturn !!(flag & ndev->unsafe_flags & ~ndev->unsafe_flags_ignore);\r\n}\r\nstatic inline int ndev_ignore_unsafe(struct intel_ntb_dev *ndev,\r\nunsigned long flag)\r\n{\r\nflag &= ndev->unsafe_flags;\r\nndev->unsafe_flags_ignore |= flag;\r\nreturn !!flag;\r\n}\r\nstatic int ndev_mw_to_bar(struct intel_ntb_dev *ndev, int idx)\r\n{\r\nif (idx < 0 || idx >= ndev->mw_count)\r\nreturn -EINVAL;\r\nreturn ndev->reg->mw_bar[idx];\r\n}\r\nstatic inline int ndev_db_addr(struct intel_ntb_dev *ndev,\r\nphys_addr_t *db_addr, resource_size_t *db_size,\r\nphys_addr_t reg_addr, unsigned long reg)\r\n{\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\r\npr_warn_once("%s: NTB unsafe doorbell access", __func__);\r\nif (db_addr) {\r\n*db_addr = reg_addr + reg;\r\ndev_dbg(ndev_dev(ndev), "Peer db addr %llx\n", *db_addr);\r\n}\r\nif (db_size) {\r\n*db_size = ndev->reg->db_size;\r\ndev_dbg(ndev_dev(ndev), "Peer db size %llx\n", *db_size);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline u64 ndev_db_read(struct intel_ntb_dev *ndev,\r\nvoid __iomem *mmio)\r\n{\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\r\npr_warn_once("%s: NTB unsafe doorbell access", __func__);\r\nreturn ndev->reg->db_ioread(mmio);\r\n}\r\nstatic inline int ndev_db_write(struct intel_ntb_dev *ndev, u64 db_bits,\r\nvoid __iomem *mmio)\r\n{\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\r\npr_warn_once("%s: NTB unsafe doorbell access", __func__);\r\nif (db_bits & ~ndev->db_valid_mask)\r\nreturn -EINVAL;\r\nndev->reg->db_iowrite(db_bits, mmio);\r\nreturn 0;\r\n}\r\nstatic inline int ndev_db_set_mask(struct intel_ntb_dev *ndev, u64 db_bits,\r\nvoid __iomem *mmio)\r\n{\r\nunsigned long irqflags;\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\r\npr_warn_once("%s: NTB unsafe doorbell access", __func__);\r\nif (db_bits & ~ndev->db_valid_mask)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&ndev->db_mask_lock, irqflags);\r\n{\r\nndev->db_mask |= db_bits;\r\nndev->reg->db_iowrite(ndev->db_mask, mmio);\r\n}\r\nspin_unlock_irqrestore(&ndev->db_mask_lock, irqflags);\r\nreturn 0;\r\n}\r\nstatic inline int ndev_db_clear_mask(struct intel_ntb_dev *ndev, u64 db_bits,\r\nvoid __iomem *mmio)\r\n{\r\nunsigned long irqflags;\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\r\npr_warn_once("%s: NTB unsafe doorbell access", __func__);\r\nif (db_bits & ~ndev->db_valid_mask)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&ndev->db_mask_lock, irqflags);\r\n{\r\nndev->db_mask &= ~db_bits;\r\nndev->reg->db_iowrite(ndev->db_mask, mmio);\r\n}\r\nspin_unlock_irqrestore(&ndev->db_mask_lock, irqflags);\r\nreturn 0;\r\n}\r\nstatic inline int ndev_vec_mask(struct intel_ntb_dev *ndev, int db_vector)\r\n{\r\nu64 shift, mask;\r\nshift = ndev->db_vec_shift;\r\nmask = BIT_ULL(shift) - 1;\r\nreturn mask << (shift * db_vector);\r\n}\r\nstatic inline int ndev_spad_addr(struct intel_ntb_dev *ndev, int idx,\r\nphys_addr_t *spad_addr, phys_addr_t reg_addr,\r\nunsigned long reg)\r\n{\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_SPAD))\r\npr_warn_once("%s: NTB unsafe scratchpad access", __func__);\r\nif (idx < 0 || idx >= ndev->spad_count)\r\nreturn -EINVAL;\r\nif (spad_addr) {\r\n*spad_addr = reg_addr + reg + (idx << 2);\r\ndev_dbg(ndev_dev(ndev), "Peer spad addr %llx\n", *spad_addr);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline u32 ndev_spad_read(struct intel_ntb_dev *ndev, int idx,\r\nvoid __iomem *mmio)\r\n{\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_SPAD))\r\npr_warn_once("%s: NTB unsafe scratchpad access", __func__);\r\nif (idx < 0 || idx >= ndev->spad_count)\r\nreturn 0;\r\nreturn ioread32(mmio + (idx << 2));\r\n}\r\nstatic inline int ndev_spad_write(struct intel_ntb_dev *ndev, int idx, u32 val,\r\nvoid __iomem *mmio)\r\n{\r\nif (ndev_is_unsafe(ndev, NTB_UNSAFE_SPAD))\r\npr_warn_once("%s: NTB unsafe scratchpad access", __func__);\r\nif (idx < 0 || idx >= ndev->spad_count)\r\nreturn -EINVAL;\r\niowrite32(val, mmio + (idx << 2));\r\nreturn 0;\r\n}\r\nstatic irqreturn_t ndev_interrupt(struct intel_ntb_dev *ndev, int vec)\r\n{\r\nu64 vec_mask;\r\nvec_mask = ndev_vec_mask(ndev, vec);\r\nif ((ndev->hwerr_flags & NTB_HWERR_MSIX_VECTOR32_BAD) && (vec == 31))\r\nvec_mask |= ndev->db_link_mask;\r\ndev_dbg(ndev_dev(ndev), "vec %d vec_mask %llx\n", vec, vec_mask);\r\nndev->last_ts = jiffies;\r\nif (vec_mask & ndev->db_link_mask) {\r\nif (ndev->reg->poll_link(ndev))\r\nntb_link_event(&ndev->ntb);\r\n}\r\nif (vec_mask & ndev->db_valid_mask)\r\nntb_db_event(&ndev->ntb, vec);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ndev_vec_isr(int irq, void *dev)\r\n{\r\nstruct intel_ntb_vec *nvec = dev;\r\ndev_dbg(ndev_dev(nvec->ndev), "irq: %d nvec->num: %d\n",\r\nirq, nvec->num);\r\nreturn ndev_interrupt(nvec->ndev, nvec->num);\r\n}\r\nstatic irqreturn_t ndev_irq_isr(int irq, void *dev)\r\n{\r\nstruct intel_ntb_dev *ndev = dev;\r\nreturn ndev_interrupt(ndev, irq - ndev_pdev(ndev)->irq);\r\n}\r\nstatic int ndev_init_isr(struct intel_ntb_dev *ndev,\r\nint msix_min, int msix_max,\r\nint msix_shift, int total_shift)\r\n{\r\nstruct pci_dev *pdev;\r\nint rc, i, msix_count, node;\r\npdev = ndev_pdev(ndev);\r\nnode = dev_to_node(&pdev->dev);\r\nndev->db_mask = ndev->db_valid_mask;\r\nndev->reg->db_iowrite(ndev->db_mask,\r\nndev->self_mmio +\r\nndev->self_reg->db_mask);\r\nndev->vec = kzalloc_node(msix_max * sizeof(*ndev->vec),\r\nGFP_KERNEL, node);\r\nif (!ndev->vec)\r\ngoto err_msix_vec_alloc;\r\nndev->msix = kzalloc_node(msix_max * sizeof(*ndev->msix),\r\nGFP_KERNEL, node);\r\nif (!ndev->msix)\r\ngoto err_msix_alloc;\r\nfor (i = 0; i < msix_max; ++i)\r\nndev->msix[i].entry = i;\r\nmsix_count = pci_enable_msix_range(pdev, ndev->msix,\r\nmsix_min, msix_max);\r\nif (msix_count < 0)\r\ngoto err_msix_enable;\r\nfor (i = 0; i < msix_count; ++i) {\r\nndev->vec[i].ndev = ndev;\r\nndev->vec[i].num = i;\r\nrc = request_irq(ndev->msix[i].vector, ndev_vec_isr, 0,\r\n"ndev_vec_isr", &ndev->vec[i]);\r\nif (rc)\r\ngoto err_msix_request;\r\n}\r\ndev_dbg(ndev_dev(ndev), "Using %d msix interrupts\n", msix_count);\r\nndev->db_vec_count = msix_count;\r\nndev->db_vec_shift = msix_shift;\r\nreturn 0;\r\nerr_msix_request:\r\nwhile (i-- > 0)\r\nfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\r\npci_disable_msix(pdev);\r\nerr_msix_enable:\r\nkfree(ndev->msix);\r\nerr_msix_alloc:\r\nkfree(ndev->vec);\r\nerr_msix_vec_alloc:\r\nndev->msix = NULL;\r\nndev->vec = NULL;\r\nrc = pci_enable_msi(pdev);\r\nif (rc)\r\ngoto err_msi_enable;\r\nrc = request_irq(pdev->irq, ndev_irq_isr, 0,\r\n"ndev_irq_isr", ndev);\r\nif (rc)\r\ngoto err_msi_request;\r\ndev_dbg(ndev_dev(ndev), "Using msi interrupts\n");\r\nndev->db_vec_count = 1;\r\nndev->db_vec_shift = total_shift;\r\nreturn 0;\r\nerr_msi_request:\r\npci_disable_msi(pdev);\r\nerr_msi_enable:\r\npci_intx(pdev, 1);\r\nrc = request_irq(pdev->irq, ndev_irq_isr, IRQF_SHARED,\r\n"ndev_irq_isr", ndev);\r\nif (rc)\r\ngoto err_intx_request;\r\ndev_dbg(ndev_dev(ndev), "Using intx interrupts\n");\r\nndev->db_vec_count = 1;\r\nndev->db_vec_shift = total_shift;\r\nreturn 0;\r\nerr_intx_request:\r\nreturn rc;\r\n}\r\nstatic void ndev_deinit_isr(struct intel_ntb_dev *ndev)\r\n{\r\nstruct pci_dev *pdev;\r\nint i;\r\npdev = ndev_pdev(ndev);\r\nndev->db_mask = ndev->db_valid_mask;\r\nndev->reg->db_iowrite(ndev->db_mask,\r\nndev->self_mmio +\r\nndev->self_reg->db_mask);\r\nif (ndev->msix) {\r\ni = ndev->db_vec_count;\r\nwhile (i--)\r\nfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\r\npci_disable_msix(pdev);\r\nkfree(ndev->msix);\r\nkfree(ndev->vec);\r\n} else {\r\nfree_irq(pdev->irq, ndev);\r\nif (pci_dev_msi_enabled(pdev))\r\npci_disable_msi(pdev);\r\n}\r\n}\r\nstatic ssize_t ndev_ntb3_debugfs_read(struct file *filp, char __user *ubuf,\r\nsize_t count, loff_t *offp)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nvoid __iomem *mmio;\r\nchar *buf;\r\nsize_t buf_size;\r\nssize_t ret, off;\r\nunion { u64 v64; u32 v32; u16 v16; } u;\r\nndev = filp->private_data;\r\nmmio = ndev->self_mmio;\r\nbuf_size = min(count, 0x800ul);\r\nbuf = kmalloc(buf_size, GFP_KERNEL);\r\nif (!buf)\r\nreturn -ENOMEM;\r\noff = 0;\r\noff += scnprintf(buf + off, buf_size - off,\r\n"NTB Device Information:\n");\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Connection Topology -\t%s\n",\r\nntb_topo_string(ndev->ntb.topo));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"NTB CTL -\t\t%#06x\n", ndev->ntb_ctl);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LNK STA -\t\t%#06x\n", ndev->lnk_sta);\r\nif (!ndev->reg->link_is_up(ndev))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Status -\t\tDown\n");\r\nelse {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Status -\t\tUp\n");\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Speed -\t\tPCI-E Gen %u\n",\r\nNTB_LNK_STA_SPEED(ndev->lnk_sta));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Width -\t\tx%u\n",\r\nNTB_LNK_STA_WIDTH(ndev->lnk_sta));\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Memory Window Count -\t%u\n", ndev->mw_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Scratchpad Count -\t%u\n", ndev->spad_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Count -\t%u\n", ndev->db_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Vector Count -\t%u\n", ndev->db_vec_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Vector Shift -\t%u\n", ndev->db_vec_shift);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Valid Mask -\t%#llx\n", ndev->db_valid_mask);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Link Mask -\t%#llx\n", ndev->db_link_mask);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Mask Cached -\t%#llx\n", ndev->db_mask);\r\nu.v64 = ndev_db_read(ndev, mmio + ndev->self_reg->db_mask);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Mask -\t\t%#llx\n", u.v64);\r\nu.v64 = ndev_db_read(ndev, mmio + ndev->self_reg->db_bell);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Bell -\t\t%#llx\n", u.v64);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Incoming XLAT:\n");\r\nu.v64 = ioread64(mmio + SKX_IMBAR1XBASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"IMBAR1XBASE -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_IMBAR2XBASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"IMBAR2XBASE -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_IMBAR1XLMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"IMBAR1XLMT -\t\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_IMBAR2XLMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"IMBAR2XLMT -\t\t\t%#018llx\n", u.v64);\r\nif (ntb_topo_is_b2b(ndev->ntb.topo)) {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Outgoing B2B XLAT:\n");\r\nu.v64 = ioread64(mmio + SKX_EMBAR1XBASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"EMBAR1XBASE -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_EMBAR2XBASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"EMBAR2XBASE -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_EMBAR1XLMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"EMBAR1XLMT -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_EMBAR2XLMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"EMBAR2XLMT -\t\t%#018llx\n", u.v64);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Secondary BAR:\n");\r\nu.v64 = ioread64(mmio + SKX_EMBAR0_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"EMBAR0 -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_EMBAR1_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"EMBAR1 -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + SKX_EMBAR2_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"EMBAR2 -\t\t%#018llx\n", u.v64);\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Statistics:\n");\r\nu.v16 = ioread16(mmio + SKX_USMEMMISS_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Upstream Memory Miss -\t%u\n", u.v16);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Hardware Errors:\n");\r\nif (!pci_read_config_word(ndev->ntb.pdev,\r\nSKX_DEVSTS_OFFSET, &u.v16))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"DEVSTS -\t\t%#06x\n", u.v16);\r\nif (!pci_read_config_word(ndev->ntb.pdev,\r\nSKX_LINK_STATUS_OFFSET, &u.v16))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LNKSTS -\t\t%#06x\n", u.v16);\r\nif (!pci_read_config_dword(ndev->ntb.pdev,\r\nSKX_UNCERRSTS_OFFSET, &u.v32))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"UNCERRSTS -\t\t%#06x\n", u.v32);\r\nif (!pci_read_config_dword(ndev->ntb.pdev,\r\nSKX_CORERRSTS_OFFSET, &u.v32))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"CORERRSTS -\t\t%#06x\n", u.v32);\r\nret = simple_read_from_buffer(ubuf, count, offp, buf, off);\r\nkfree(buf);\r\nreturn ret;\r\n}\r\nstatic ssize_t ndev_ntb_debugfs_read(struct file *filp, char __user *ubuf,\r\nsize_t count, loff_t *offp)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nstruct pci_dev *pdev;\r\nvoid __iomem *mmio;\r\nchar *buf;\r\nsize_t buf_size;\r\nssize_t ret, off;\r\nunion { u64 v64; u32 v32; u16 v16; u8 v8; } u;\r\nndev = filp->private_data;\r\npdev = ndev_pdev(ndev);\r\nmmio = ndev->self_mmio;\r\nbuf_size = min(count, 0x800ul);\r\nbuf = kmalloc(buf_size, GFP_KERNEL);\r\nif (!buf)\r\nreturn -ENOMEM;\r\noff = 0;\r\noff += scnprintf(buf + off, buf_size - off,\r\n"NTB Device Information:\n");\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Connection Topology -\t%s\n",\r\nntb_topo_string(ndev->ntb.topo));\r\nif (ndev->b2b_idx != UINT_MAX) {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B MW Idx -\t\t%u\n", ndev->b2b_idx);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B Offset -\t\t%#lx\n", ndev->b2b_off);\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"BAR4 Split -\t\t%s\n",\r\nndev->bar4_split ? "yes" : "no");\r\noff += scnprintf(buf + off, buf_size - off,\r\n"NTB CTL -\t\t%#06x\n", ndev->ntb_ctl);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LNK STA -\t\t%#06x\n", ndev->lnk_sta);\r\nif (!ndev->reg->link_is_up(ndev)) {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Status -\t\tDown\n");\r\n} else {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Status -\t\tUp\n");\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Speed -\t\tPCI-E Gen %u\n",\r\nNTB_LNK_STA_SPEED(ndev->lnk_sta));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Width -\t\tx%u\n",\r\nNTB_LNK_STA_WIDTH(ndev->lnk_sta));\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Memory Window Count -\t%u\n", ndev->mw_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Scratchpad Count -\t%u\n", ndev->spad_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Count -\t%u\n", ndev->db_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Vector Count -\t%u\n", ndev->db_vec_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Vector Shift -\t%u\n", ndev->db_vec_shift);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Valid Mask -\t%#llx\n", ndev->db_valid_mask);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Link Mask -\t%#llx\n", ndev->db_link_mask);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Mask Cached -\t%#llx\n", ndev->db_mask);\r\nu.v64 = ndev_db_read(ndev, mmio + ndev->self_reg->db_mask);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Mask -\t\t%#llx\n", u.v64);\r\nu.v64 = ndev_db_read(ndev, mmio + ndev->self_reg->db_bell);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Bell -\t\t%#llx\n", u.v64);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Window Size:\n");\r\npci_read_config_byte(pdev, XEON_PBAR23SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"PBAR23SZ %hhu\n", u.v8);\r\nif (!ndev->bar4_split) {\r\npci_read_config_byte(pdev, XEON_PBAR45SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"PBAR45SZ %hhu\n", u.v8);\r\n} else {\r\npci_read_config_byte(pdev, XEON_PBAR4SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"PBAR4SZ %hhu\n", u.v8);\r\npci_read_config_byte(pdev, XEON_PBAR5SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"PBAR5SZ %hhu\n", u.v8);\r\n}\r\npci_read_config_byte(pdev, XEON_SBAR23SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR23SZ %hhu\n", u.v8);\r\nif (!ndev->bar4_split) {\r\npci_read_config_byte(pdev, XEON_SBAR45SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR45SZ %hhu\n", u.v8);\r\n} else {\r\npci_read_config_byte(pdev, XEON_SBAR4SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR4SZ %hhu\n", u.v8);\r\npci_read_config_byte(pdev, XEON_SBAR5SZ_OFFSET, &u.v8);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR5SZ %hhu\n", u.v8);\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Incoming XLAT:\n");\r\nu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 2));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"XLAT23 -\t\t%#018llx\n", u.v64);\r\nif (ndev->bar4_split) {\r\nu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 4));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"XLAT4 -\t\t\t%#06x\n", u.v32);\r\nu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 5));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"XLAT5 -\t\t\t%#06x\n", u.v32);\r\n} else {\r\nu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 4));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"XLAT45 -\t\t%#018llx\n", u.v64);\r\n}\r\nu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 2));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LMT23 -\t\t\t%#018llx\n", u.v64);\r\nif (ndev->bar4_split) {\r\nu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 4));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LMT4 -\t\t\t%#06x\n", u.v32);\r\nu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 5));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LMT5 -\t\t\t%#06x\n", u.v32);\r\n} else {\r\nu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 4));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LMT45 -\t\t\t%#018llx\n", u.v64);\r\n}\r\nif (pdev_is_xeon(pdev)) {\r\nif (ntb_topo_is_b2b(ndev->ntb.topo)) {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Outgoing B2B XLAT:\n");\r\nu.v64 = ioread64(mmio + XEON_PBAR23XLAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B XLAT23 -\t\t%#018llx\n", u.v64);\r\nif (ndev->bar4_split) {\r\nu.v32 = ioread32(mmio + XEON_PBAR4XLAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B XLAT4 -\t\t%#06x\n",\r\nu.v32);\r\nu.v32 = ioread32(mmio + XEON_PBAR5XLAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B XLAT5 -\t\t%#06x\n",\r\nu.v32);\r\n} else {\r\nu.v64 = ioread64(mmio + XEON_PBAR45XLAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B XLAT45 -\t\t%#018llx\n",\r\nu.v64);\r\n}\r\nu.v64 = ioread64(mmio + XEON_PBAR23LMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B LMT23 -\t\t%#018llx\n", u.v64);\r\nif (ndev->bar4_split) {\r\nu.v32 = ioread32(mmio + XEON_PBAR4LMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B LMT4 -\t\t%#06x\n",\r\nu.v32);\r\nu.v32 = ioread32(mmio + XEON_PBAR5LMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B LMT5 -\t\t%#06x\n",\r\nu.v32);\r\n} else {\r\nu.v64 = ioread64(mmio + XEON_PBAR45LMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"B2B LMT45 -\t\t%#018llx\n",\r\nu.v64);\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Secondary BAR:\n");\r\nu.v64 = ioread64(mmio + XEON_SBAR0BASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR01 -\t\t%#018llx\n", u.v64);\r\nu.v64 = ioread64(mmio + XEON_SBAR23BASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR23 -\t\t%#018llx\n", u.v64);\r\nif (ndev->bar4_split) {\r\nu.v32 = ioread32(mmio + XEON_SBAR4BASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR4 -\t\t\t%#06x\n", u.v32);\r\nu.v32 = ioread32(mmio + XEON_SBAR5BASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR5 -\t\t\t%#06x\n", u.v32);\r\n} else {\r\nu.v64 = ioread64(mmio + XEON_SBAR45BASE_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"SBAR45 -\t\t%#018llx\n",\r\nu.v64);\r\n}\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nXEON NTB Statistics:\n");\r\nu.v16 = ioread16(mmio + XEON_USMEMMISS_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Upstream Memory Miss -\t%u\n", u.v16);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nXEON NTB Hardware Errors:\n");\r\nif (!pci_read_config_word(pdev,\r\nXEON_DEVSTS_OFFSET, &u.v16))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"DEVSTS -\t\t%#06x\n", u.v16);\r\nif (!pci_read_config_word(pdev,\r\nXEON_LINK_STATUS_OFFSET, &u.v16))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LNKSTS -\t\t%#06x\n", u.v16);\r\nif (!pci_read_config_dword(pdev,\r\nXEON_UNCERRSTS_OFFSET, &u.v32))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"UNCERRSTS -\t\t%#06x\n", u.v32);\r\nif (!pci_read_config_dword(pdev,\r\nXEON_CORERRSTS_OFFSET, &u.v32))\r\noff += scnprintf(buf + off, buf_size - off,\r\n"CORERRSTS -\t\t%#06x\n", u.v32);\r\n}\r\nret = simple_read_from_buffer(ubuf, count, offp, buf, off);\r\nkfree(buf);\r\nreturn ret;\r\n}\r\nstatic ssize_t ndev_debugfs_read(struct file *filp, char __user *ubuf,\r\nsize_t count, loff_t *offp)\r\n{\r\nstruct intel_ntb_dev *ndev = filp->private_data;\r\nif (pdev_is_xeon(ndev->ntb.pdev) ||\r\npdev_is_atom(ndev->ntb.pdev))\r\nreturn ndev_ntb_debugfs_read(filp, ubuf, count, offp);\r\nelse if (pdev_is_skx_xeon(ndev->ntb.pdev))\r\nreturn ndev_ntb3_debugfs_read(filp, ubuf, count, offp);\r\nreturn -ENXIO;\r\n}\r\nstatic void ndev_init_debugfs(struct intel_ntb_dev *ndev)\r\n{\r\nif (!debugfs_dir) {\r\nndev->debugfs_dir = NULL;\r\nndev->debugfs_info = NULL;\r\n} else {\r\nndev->debugfs_dir =\r\ndebugfs_create_dir(ndev_name(ndev), debugfs_dir);\r\nif (!ndev->debugfs_dir)\r\nndev->debugfs_info = NULL;\r\nelse\r\nndev->debugfs_info =\r\ndebugfs_create_file("info", S_IRUSR,\r\nndev->debugfs_dir, ndev,\r\n&intel_ntb_debugfs_info);\r\n}\r\n}\r\nstatic void ndev_deinit_debugfs(struct intel_ntb_dev *ndev)\r\n{\r\ndebugfs_remove_recursive(ndev->debugfs_dir);\r\n}\r\nstatic int intel_ntb_mw_count(struct ntb_dev *ntb)\r\n{\r\nreturn ntb_ndev(ntb)->mw_count;\r\n}\r\nstatic int intel_ntb_mw_get_range(struct ntb_dev *ntb, int idx,\r\nphys_addr_t *base,\r\nresource_size_t *size,\r\nresource_size_t *align,\r\nresource_size_t *align_size)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nint bar;\r\nif (idx >= ndev->b2b_idx && !ndev->b2b_off)\r\nidx += 1;\r\nbar = ndev_mw_to_bar(ndev, idx);\r\nif (bar < 0)\r\nreturn bar;\r\nif (base)\r\n*base = pci_resource_start(ndev->ntb.pdev, bar) +\r\n(idx == ndev->b2b_idx ? ndev->b2b_off : 0);\r\nif (size)\r\n*size = pci_resource_len(ndev->ntb.pdev, bar) -\r\n(idx == ndev->b2b_idx ? ndev->b2b_off : 0);\r\nif (align)\r\n*align = pci_resource_len(ndev->ntb.pdev, bar);\r\nif (align_size)\r\n*align_size = 1;\r\nreturn 0;\r\n}\r\nstatic int intel_ntb_mw_set_trans(struct ntb_dev *ntb, int idx,\r\ndma_addr_t addr, resource_size_t size)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nunsigned long base_reg, xlat_reg, limit_reg;\r\nresource_size_t bar_size, mw_size;\r\nvoid __iomem *mmio;\r\nu64 base, limit, reg_val;\r\nint bar;\r\nif (idx >= ndev->b2b_idx && !ndev->b2b_off)\r\nidx += 1;\r\nbar = ndev_mw_to_bar(ndev, idx);\r\nif (bar < 0)\r\nreturn bar;\r\nbar_size = pci_resource_len(ndev->ntb.pdev, bar);\r\nif (idx == ndev->b2b_idx)\r\nmw_size = bar_size - ndev->b2b_off;\r\nelse\r\nmw_size = bar_size;\r\nif (addr & (bar_size - 1))\r\nreturn -EINVAL;\r\nif (size > mw_size)\r\nreturn -EINVAL;\r\nmmio = ndev->self_mmio;\r\nbase_reg = bar0_off(ndev->xlat_reg->bar0_base, bar);\r\nxlat_reg = bar2_off(ndev->xlat_reg->bar2_xlat, bar);\r\nlimit_reg = bar2_off(ndev->xlat_reg->bar2_limit, bar);\r\nif (bar < 4 || !ndev->bar4_split) {\r\nbase = ioread64(mmio + base_reg) & NTB_BAR_MASK_64;\r\nif (limit_reg && size != mw_size)\r\nlimit = base + size;\r\nelse\r\nlimit = 0;\r\niowrite64(addr, mmio + xlat_reg);\r\nreg_val = ioread64(mmio + xlat_reg);\r\nif (reg_val != addr) {\r\niowrite64(0, mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\niowrite64(limit, mmio + limit_reg);\r\nreg_val = ioread64(mmio + limit_reg);\r\nif (reg_val != limit) {\r\niowrite64(base, mmio + limit_reg);\r\niowrite64(0, mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\n} else {\r\nif (addr & (~0ull << 32))\r\nreturn -EINVAL;\r\nif ((addr + size) & (~0ull << 32))\r\nreturn -EINVAL;\r\nbase = ioread32(mmio + base_reg) & NTB_BAR_MASK_32;\r\nif (limit_reg && size != mw_size)\r\nlimit = base + size;\r\nelse\r\nlimit = 0;\r\niowrite32(addr, mmio + xlat_reg);\r\nreg_val = ioread32(mmio + xlat_reg);\r\nif (reg_val != addr) {\r\niowrite32(0, mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\niowrite32(limit, mmio + limit_reg);\r\nreg_val = ioread32(mmio + limit_reg);\r\nif (reg_val != limit) {\r\niowrite32(base, mmio + limit_reg);\r\niowrite32(0, mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int intel_ntb_link_is_up(struct ntb_dev *ntb,\r\nenum ntb_speed *speed,\r\nenum ntb_width *width)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nif (ndev->reg->link_is_up(ndev)) {\r\nif (speed)\r\n*speed = NTB_LNK_STA_SPEED(ndev->lnk_sta);\r\nif (width)\r\n*width = NTB_LNK_STA_WIDTH(ndev->lnk_sta);\r\nreturn 1;\r\n} else {\r\nif (speed)\r\n*speed = NTB_SPEED_NONE;\r\nif (width)\r\n*width = NTB_WIDTH_NONE;\r\nreturn 0;\r\n}\r\n}\r\nstatic int intel_ntb_link_enable(struct ntb_dev *ntb,\r\nenum ntb_speed max_speed,\r\nenum ntb_width max_width)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nu32 ntb_ctl;\r\nndev = container_of(ntb, struct intel_ntb_dev, ntb);\r\nif (ndev->ntb.topo == NTB_TOPO_SEC)\r\nreturn -EINVAL;\r\ndev_dbg(ndev_dev(ndev),\r\n"Enabling link with max_speed %d max_width %d\n",\r\nmax_speed, max_width);\r\nif (max_speed != NTB_SPEED_AUTO)\r\ndev_dbg(ndev_dev(ndev), "ignoring max_speed %d\n", max_speed);\r\nif (max_width != NTB_WIDTH_AUTO)\r\ndev_dbg(ndev_dev(ndev), "ignoring max_width %d\n", max_width);\r\nntb_ctl = ioread32(ndev->self_mmio + ndev->reg->ntb_ctl);\r\nntb_ctl &= ~(NTB_CTL_DISABLE | NTB_CTL_CFG_LOCK);\r\nntb_ctl |= NTB_CTL_P2S_BAR2_SNOOP | NTB_CTL_S2P_BAR2_SNOOP;\r\nntb_ctl |= NTB_CTL_P2S_BAR4_SNOOP | NTB_CTL_S2P_BAR4_SNOOP;\r\nif (ndev->bar4_split)\r\nntb_ctl |= NTB_CTL_P2S_BAR5_SNOOP | NTB_CTL_S2P_BAR5_SNOOP;\r\niowrite32(ntb_ctl, ndev->self_mmio + ndev->reg->ntb_ctl);\r\nreturn 0;\r\n}\r\nstatic int intel_ntb_link_disable(struct ntb_dev *ntb)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nu32 ntb_cntl;\r\nndev = container_of(ntb, struct intel_ntb_dev, ntb);\r\nif (ndev->ntb.topo == NTB_TOPO_SEC)\r\nreturn -EINVAL;\r\ndev_dbg(ndev_dev(ndev), "Disabling link\n");\r\nntb_cntl = ioread32(ndev->self_mmio + ndev->reg->ntb_ctl);\r\nntb_cntl &= ~(NTB_CTL_P2S_BAR2_SNOOP | NTB_CTL_S2P_BAR2_SNOOP);\r\nntb_cntl &= ~(NTB_CTL_P2S_BAR4_SNOOP | NTB_CTL_S2P_BAR4_SNOOP);\r\nif (ndev->bar4_split)\r\nntb_cntl &= ~(NTB_CTL_P2S_BAR5_SNOOP | NTB_CTL_S2P_BAR5_SNOOP);\r\nntb_cntl |= NTB_CTL_DISABLE | NTB_CTL_CFG_LOCK;\r\niowrite32(ntb_cntl, ndev->self_mmio + ndev->reg->ntb_ctl);\r\nreturn 0;\r\n}\r\nstatic int intel_ntb_db_is_unsafe(struct ntb_dev *ntb)\r\n{\r\nreturn ndev_ignore_unsafe(ntb_ndev(ntb), NTB_UNSAFE_DB);\r\n}\r\nstatic u64 intel_ntb_db_valid_mask(struct ntb_dev *ntb)\r\n{\r\nreturn ntb_ndev(ntb)->db_valid_mask;\r\n}\r\nstatic int intel_ntb_db_vector_count(struct ntb_dev *ntb)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nndev = container_of(ntb, struct intel_ntb_dev, ntb);\r\nreturn ndev->db_vec_count;\r\n}\r\nstatic u64 intel_ntb_db_vector_mask(struct ntb_dev *ntb, int db_vector)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nif (db_vector < 0 || db_vector > ndev->db_vec_count)\r\nreturn 0;\r\nreturn ndev->db_valid_mask & ndev_vec_mask(ndev, db_vector);\r\n}\r\nstatic u64 intel_ntb_db_read(struct ntb_dev *ntb)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_read(ndev,\r\nndev->self_mmio +\r\nndev->self_reg->db_bell);\r\n}\r\nstatic int intel_ntb_db_clear(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_write(ndev, db_bits,\r\nndev->self_mmio +\r\nndev->self_reg->db_bell);\r\n}\r\nstatic int intel_ntb_db_set_mask(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_set_mask(ndev, db_bits,\r\nndev->self_mmio +\r\nndev->self_reg->db_mask);\r\n}\r\nstatic int intel_ntb_db_clear_mask(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_clear_mask(ndev, db_bits,\r\nndev->self_mmio +\r\nndev->self_reg->db_mask);\r\n}\r\nstatic int intel_ntb_peer_db_addr(struct ntb_dev *ntb,\r\nphys_addr_t *db_addr,\r\nresource_size_t *db_size)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_addr(ndev, db_addr, db_size, ndev->peer_addr,\r\nndev->peer_reg->db_bell);\r\n}\r\nstatic int intel_ntb_peer_db_set(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_write(ndev, db_bits,\r\nndev->peer_mmio +\r\nndev->peer_reg->db_bell);\r\n}\r\nstatic int intel_ntb_spad_is_unsafe(struct ntb_dev *ntb)\r\n{\r\nreturn ndev_ignore_unsafe(ntb_ndev(ntb), NTB_UNSAFE_SPAD);\r\n}\r\nstatic int intel_ntb_spad_count(struct ntb_dev *ntb)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nndev = container_of(ntb, struct intel_ntb_dev, ntb);\r\nreturn ndev->spad_count;\r\n}\r\nstatic u32 intel_ntb_spad_read(struct ntb_dev *ntb, int idx)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_spad_read(ndev, idx,\r\nndev->self_mmio +\r\nndev->self_reg->spad);\r\n}\r\nstatic int intel_ntb_spad_write(struct ntb_dev *ntb,\r\nint idx, u32 val)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_spad_write(ndev, idx, val,\r\nndev->self_mmio +\r\nndev->self_reg->spad);\r\n}\r\nstatic int intel_ntb_peer_spad_addr(struct ntb_dev *ntb, int idx,\r\nphys_addr_t *spad_addr)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_spad_addr(ndev, idx, spad_addr, ndev->peer_addr,\r\nndev->peer_reg->spad);\r\n}\r\nstatic u32 intel_ntb_peer_spad_read(struct ntb_dev *ntb, int idx)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_spad_read(ndev, idx,\r\nndev->peer_mmio +\r\nndev->peer_reg->spad);\r\n}\r\nstatic int intel_ntb_peer_spad_write(struct ntb_dev *ntb,\r\nint idx, u32 val)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_spad_write(ndev, idx, val,\r\nndev->peer_mmio +\r\nndev->peer_reg->spad);\r\n}\r\nstatic u64 atom_db_ioread(void __iomem *mmio)\r\n{\r\nreturn ioread64(mmio);\r\n}\r\nstatic void atom_db_iowrite(u64 bits, void __iomem *mmio)\r\n{\r\niowrite64(bits, mmio);\r\n}\r\nstatic int atom_poll_link(struct intel_ntb_dev *ndev)\r\n{\r\nu32 ntb_ctl;\r\nntb_ctl = ioread32(ndev->self_mmio + ATOM_NTBCNTL_OFFSET);\r\nif (ntb_ctl == ndev->ntb_ctl)\r\nreturn 0;\r\nndev->ntb_ctl = ntb_ctl;\r\nndev->lnk_sta = ioread32(ndev->self_mmio + ATOM_LINK_STATUS_OFFSET);\r\nreturn 1;\r\n}\r\nstatic int atom_link_is_up(struct intel_ntb_dev *ndev)\r\n{\r\nreturn ATOM_NTB_CTL_ACTIVE(ndev->ntb_ctl);\r\n}\r\nstatic int atom_link_is_err(struct intel_ntb_dev *ndev)\r\n{\r\nif (ioread32(ndev->self_mmio + ATOM_LTSSMSTATEJMP_OFFSET)\r\n& ATOM_LTSSMSTATEJMP_FORCEDETECT)\r\nreturn 1;\r\nif (ioread32(ndev->self_mmio + ATOM_IBSTERRRCRVSTS0_OFFSET)\r\n& ATOM_IBIST_ERR_OFLOW)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline enum ntb_topo atom_ppd_topo(struct intel_ntb_dev *ndev, u32 ppd)\r\n{\r\nswitch (ppd & ATOM_PPD_TOPO_MASK) {\r\ncase ATOM_PPD_TOPO_B2B_USD:\r\ndev_dbg(ndev_dev(ndev), "PPD %d B2B USD\n", ppd);\r\nreturn NTB_TOPO_B2B_USD;\r\ncase ATOM_PPD_TOPO_B2B_DSD:\r\ndev_dbg(ndev_dev(ndev), "PPD %d B2B DSD\n", ppd);\r\nreturn NTB_TOPO_B2B_DSD;\r\ncase ATOM_PPD_TOPO_PRI_USD:\r\ncase ATOM_PPD_TOPO_PRI_DSD:\r\ncase ATOM_PPD_TOPO_SEC_USD:\r\ncase ATOM_PPD_TOPO_SEC_DSD:\r\ndev_dbg(ndev_dev(ndev), "PPD %d non B2B disabled\n", ppd);\r\nreturn NTB_TOPO_NONE;\r\n}\r\ndev_dbg(ndev_dev(ndev), "PPD %d invalid\n", ppd);\r\nreturn NTB_TOPO_NONE;\r\n}\r\nstatic void atom_link_hb(struct work_struct *work)\r\n{\r\nstruct intel_ntb_dev *ndev = hb_ndev(work);\r\nunsigned long poll_ts;\r\nvoid __iomem *mmio;\r\nu32 status32;\r\npoll_ts = ndev->last_ts + ATOM_LINK_HB_TIMEOUT;\r\nif (time_after(poll_ts, jiffies) && atom_link_is_up(ndev)) {\r\nschedule_delayed_work(&ndev->hb_timer, poll_ts - jiffies);\r\nreturn;\r\n}\r\nif (atom_poll_link(ndev))\r\nntb_link_event(&ndev->ntb);\r\nif (atom_link_is_up(ndev) || !atom_link_is_err(ndev)) {\r\nschedule_delayed_work(&ndev->hb_timer, ATOM_LINK_HB_TIMEOUT);\r\nreturn;\r\n}\r\nmmio = ndev->self_mmio;\r\niowrite8(0xe0, mmio + ATOM_MODPHY_PCSREG6);\r\niowrite8(0x40, mmio + ATOM_MODPHY_PCSREG4);\r\niowrite8(0x60, mmio + ATOM_MODPHY_PCSREG4);\r\niowrite8(0x60, mmio + ATOM_MODPHY_PCSREG6);\r\nmsleep(100);\r\nstatus32 = ioread32(mmio + ATOM_ERRCORSTS_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "ERRCORSTS = %x\n", status32);\r\nstatus32 &= PCI_ERR_COR_REP_ROLL;\r\niowrite32(status32, mmio + ATOM_ERRCORSTS_OFFSET);\r\nstatus32 = ioread32(mmio + ATOM_LTSSMERRSTS0_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "LTSSMERRSTS0 = %x\n", status32);\r\nstatus32 |= ATOM_LTSSMERRSTS0_UNEXPECTEDEI;\r\niowrite32(status32, mmio + ATOM_LTSSMERRSTS0_OFFSET);\r\nstatus32 = ioread32(mmio + ATOM_DESKEWSTS_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "DESKEWSTS = %x\n", status32);\r\nstatus32 |= ATOM_DESKEWSTS_DBERR;\r\niowrite32(status32, mmio + ATOM_DESKEWSTS_OFFSET);\r\nstatus32 = ioread32(mmio + ATOM_IBSTERRRCRVSTS0_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "IBSTERRRCRVSTS0 = %x\n", status32);\r\nstatus32 &= ATOM_IBIST_ERR_OFLOW;\r\niowrite32(status32, mmio + ATOM_IBSTERRRCRVSTS0_OFFSET);\r\nstatus32 = ioread32(mmio + ATOM_LTSSMSTATEJMP_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "LTSSMSTATEJMP = %x\n", status32);\r\nstatus32 &= ~ATOM_LTSSMSTATEJMP_FORCEDETECT;\r\niowrite32(status32, mmio + ATOM_LTSSMSTATEJMP_OFFSET);\r\nschedule_delayed_work(&ndev->hb_timer, ATOM_LINK_RECOVERY_TIME\r\n+ prandom_u32() % ATOM_LINK_RECOVERY_TIME);\r\n}\r\nstatic int atom_init_isr(struct intel_ntb_dev *ndev)\r\n{\r\nint rc;\r\nrc = ndev_init_isr(ndev, 1, ATOM_DB_MSIX_VECTOR_COUNT,\r\nATOM_DB_MSIX_VECTOR_SHIFT, ATOM_DB_TOTAL_SHIFT);\r\nif (rc)\r\nreturn rc;\r\nndev->last_ts = jiffies;\r\nINIT_DELAYED_WORK(&ndev->hb_timer, atom_link_hb);\r\nschedule_delayed_work(&ndev->hb_timer, ATOM_LINK_HB_TIMEOUT);\r\nreturn 0;\r\n}\r\nstatic void atom_deinit_isr(struct intel_ntb_dev *ndev)\r\n{\r\ncancel_delayed_work_sync(&ndev->hb_timer);\r\nndev_deinit_isr(ndev);\r\n}\r\nstatic int atom_init_ntb(struct intel_ntb_dev *ndev)\r\n{\r\nndev->mw_count = ATOM_MW_COUNT;\r\nndev->spad_count = ATOM_SPAD_COUNT;\r\nndev->db_count = ATOM_DB_COUNT;\r\nswitch (ndev->ntb.topo) {\r\ncase NTB_TOPO_B2B_USD:\r\ncase NTB_TOPO_B2B_DSD:\r\nndev->self_reg = &atom_pri_reg;\r\nndev->peer_reg = &atom_b2b_reg;\r\nndev->xlat_reg = &atom_sec_xlat;\r\niowrite16(PCI_COMMAND_MEMORY | PCI_COMMAND_MASTER,\r\nndev->self_mmio + ATOM_SPCICMD_OFFSET);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\r\nreturn 0;\r\n}\r\nstatic int atom_init_dev(struct intel_ntb_dev *ndev)\r\n{\r\nu32 ppd;\r\nint rc;\r\nrc = pci_read_config_dword(ndev->ntb.pdev, ATOM_PPD_OFFSET, &ppd);\r\nif (rc)\r\nreturn -EIO;\r\nndev->ntb.topo = atom_ppd_topo(ndev, ppd);\r\nif (ndev->ntb.topo == NTB_TOPO_NONE)\r\nreturn -EINVAL;\r\nrc = atom_init_ntb(ndev);\r\nif (rc)\r\nreturn rc;\r\nrc = atom_init_isr(ndev);\r\nif (rc)\r\nreturn rc;\r\nif (ndev->ntb.topo != NTB_TOPO_SEC) {\r\nrc = pci_write_config_dword(ndev->ntb.pdev, ATOM_PPD_OFFSET,\r\nppd | ATOM_PPD_INIT_LINK);\r\nif (rc)\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic void atom_deinit_dev(struct intel_ntb_dev *ndev)\r\n{\r\natom_deinit_isr(ndev);\r\n}\r\nstatic int skx_poll_link(struct intel_ntb_dev *ndev)\r\n{\r\nu16 reg_val;\r\nint rc;\r\nndev->reg->db_iowrite(ndev->db_link_mask,\r\nndev->self_mmio +\r\nndev->self_reg->db_clear);\r\nrc = pci_read_config_word(ndev->ntb.pdev,\r\nSKX_LINK_STATUS_OFFSET, &reg_val);\r\nif (rc)\r\nreturn 0;\r\nif (reg_val == ndev->lnk_sta)\r\nreturn 0;\r\nndev->lnk_sta = reg_val;\r\nreturn 1;\r\n}\r\nstatic u64 skx_db_ioread(void __iomem *mmio)\r\n{\r\nreturn ioread64(mmio);\r\n}\r\nstatic void skx_db_iowrite(u64 bits, void __iomem *mmio)\r\n{\r\niowrite64(bits, mmio);\r\n}\r\nstatic int skx_init_isr(struct intel_ntb_dev *ndev)\r\n{\r\nint i;\r\nfor (i = 0; i < SKX_DB_MSIX_VECTOR_COUNT; i++)\r\niowrite8(i, ndev->self_mmio + SKX_INTVEC_OFFSET + i);\r\nif (ndev->hwerr_flags & NTB_HWERR_MSIX_VECTOR32_BAD) {\r\niowrite8(SKX_DB_MSIX_VECTOR_COUNT - 2,\r\nndev->self_mmio + SKX_INTVEC_OFFSET +\r\n(SKX_DB_MSIX_VECTOR_COUNT - 1));\r\n}\r\nreturn ndev_init_isr(ndev, SKX_DB_MSIX_VECTOR_COUNT,\r\nSKX_DB_MSIX_VECTOR_COUNT,\r\nSKX_DB_MSIX_VECTOR_SHIFT,\r\nSKX_DB_TOTAL_SHIFT);\r\n}\r\nstatic int skx_setup_b2b_mw(struct intel_ntb_dev *ndev,\r\nconst struct intel_b2b_addr *addr,\r\nconst struct intel_b2b_addr *peer_addr)\r\n{\r\nstruct pci_dev *pdev;\r\nvoid __iomem *mmio;\r\nresource_size_t bar_size;\r\nphys_addr_t bar_addr;\r\nint b2b_bar;\r\nu8 bar_sz;\r\npdev = ndev_pdev(ndev);\r\nmmio = ndev->self_mmio;\r\nif (ndev->b2b_idx == UINT_MAX) {\r\ndev_dbg(ndev_dev(ndev), "not using b2b mw\n");\r\nb2b_bar = 0;\r\nndev->b2b_off = 0;\r\n} else {\r\nb2b_bar = ndev_mw_to_bar(ndev, ndev->b2b_idx);\r\nif (b2b_bar < 0)\r\nreturn -EIO;\r\ndev_dbg(ndev_dev(ndev), "using b2b mw bar %d\n", b2b_bar);\r\nbar_size = pci_resource_len(ndev->ntb.pdev, b2b_bar);\r\ndev_dbg(ndev_dev(ndev), "b2b bar size %#llx\n", bar_size);\r\nif (b2b_mw_share && ((bar_size >> 1) >= XEON_B2B_MIN_SIZE)) {\r\ndev_dbg(ndev_dev(ndev),\r\n"b2b using first half of bar\n");\r\nndev->b2b_off = bar_size >> 1;\r\n} else if (bar_size >= XEON_B2B_MIN_SIZE) {\r\ndev_dbg(ndev_dev(ndev),\r\n"b2b using whole bar\n");\r\nndev->b2b_off = 0;\r\n--ndev->mw_count;\r\n} else {\r\ndev_dbg(ndev_dev(ndev),\r\n"b2b bar size is too small\n");\r\nreturn -EIO;\r\n}\r\n}\r\npci_read_config_byte(pdev, SKX_IMBAR1SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "IMBAR1SZ %#x\n", bar_sz);\r\nif (b2b_bar == 1) {\r\nif (ndev->b2b_off)\r\nbar_sz -= 1;\r\nelse\r\nbar_sz = 0;\r\n}\r\npci_write_config_byte(pdev, SKX_EMBAR1SZ_OFFSET, bar_sz);\r\npci_read_config_byte(pdev, SKX_EMBAR1SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "EMBAR1SZ %#x\n", bar_sz);\r\npci_read_config_byte(pdev, SKX_IMBAR2SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "IMBAR2SZ %#x\n", bar_sz);\r\nif (b2b_bar == 2) {\r\nif (ndev->b2b_off)\r\nbar_sz -= 1;\r\nelse\r\nbar_sz = 0;\r\n}\r\npci_write_config_byte(pdev, SKX_EMBAR2SZ_OFFSET, bar_sz);\r\npci_read_config_byte(pdev, SKX_EMBAR2SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "EMBAR2SZ %#x\n", bar_sz);\r\nif (b2b_bar == 0)\r\nbar_addr = addr->bar0_addr;\r\nelse if (b2b_bar == 1)\r\nbar_addr = addr->bar2_addr64;\r\nelse if (b2b_bar == 2)\r\nbar_addr = addr->bar4_addr64;\r\nelse\r\nreturn -EIO;\r\nbar_addr = addr->bar2_addr64 + (b2b_bar == 1 ? ndev->b2b_off : 0);\r\niowrite64(bar_addr, mmio + SKX_IMBAR1XLMT_OFFSET);\r\nbar_addr = ioread64(mmio + SKX_IMBAR1XLMT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "IMBAR1XLMT %#018llx\n", bar_addr);\r\nbar_addr = addr->bar4_addr64 + (b2b_bar == 2 ? ndev->b2b_off : 0);\r\niowrite64(bar_addr, mmio + SKX_IMBAR2XLMT_OFFSET);\r\nbar_addr = ioread64(mmio + SKX_IMBAR2XLMT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "IMBAR2XLMT %#018llx\n", bar_addr);\r\niowrite64(0, mmio + SKX_IMBAR1XBASE_OFFSET);\r\niowrite64(0, mmio + SKX_IMBAR2XBASE_OFFSET);\r\nndev->peer_mmio = ndev->self_mmio;\r\nreturn 0;\r\n}\r\nstatic int skx_init_ntb(struct intel_ntb_dev *ndev)\r\n{\r\nint rc;\r\nndev->mw_count = XEON_MW_COUNT;\r\nndev->spad_count = SKX_SPAD_COUNT;\r\nndev->db_count = SKX_DB_COUNT;\r\nndev->db_link_mask = SKX_DB_LINK_BIT;\r\nif (ndev->hwerr_flags & NTB_HWERR_MSIX_VECTOR32_BAD)\r\nndev->db_link_mask |= BIT_ULL(31);\r\nswitch (ndev->ntb.topo) {\r\ncase NTB_TOPO_B2B_USD:\r\ncase NTB_TOPO_B2B_DSD:\r\nndev->self_reg = &skx_pri_reg;\r\nndev->peer_reg = &skx_b2b_reg;\r\nndev->xlat_reg = &skx_sec_xlat;\r\nif (ndev->ntb.topo == NTB_TOPO_B2B_USD) {\r\nrc = skx_setup_b2b_mw(ndev,\r\n&xeon_b2b_dsd_addr,\r\n&xeon_b2b_usd_addr);\r\n} else {\r\nrc = skx_setup_b2b_mw(ndev,\r\n&xeon_b2b_usd_addr,\r\n&xeon_b2b_dsd_addr);\r\n}\r\nif (rc)\r\nreturn rc;\r\niowrite16(PCI_COMMAND_MEMORY | PCI_COMMAND_MASTER,\r\nndev->self_mmio + SKX_SPCICMD_OFFSET);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\r\nndev->reg->db_iowrite(ndev->db_valid_mask,\r\nndev->self_mmio +\r\nndev->self_reg->db_mask);\r\nreturn 0;\r\n}\r\nstatic int skx_init_dev(struct intel_ntb_dev *ndev)\r\n{\r\nstruct pci_dev *pdev;\r\nu8 ppd;\r\nint rc;\r\npdev = ndev_pdev(ndev);\r\nndev->reg = &skx_reg;\r\nrc = pci_read_config_byte(pdev, XEON_PPD_OFFSET, &ppd);\r\nif (rc)\r\nreturn -EIO;\r\nndev->ntb.topo = xeon_ppd_topo(ndev, ppd);\r\ndev_dbg(ndev_dev(ndev), "ppd %#x topo %s\n", ppd,\r\nntb_topo_string(ndev->ntb.topo));\r\nif (ndev->ntb.topo == NTB_TOPO_NONE)\r\nreturn -EINVAL;\r\nif (pdev_is_skx_xeon(pdev))\r\nndev->hwerr_flags |= NTB_HWERR_MSIX_VECTOR32_BAD;\r\nrc = skx_init_ntb(ndev);\r\nif (rc)\r\nreturn rc;\r\nreturn skx_init_isr(ndev);\r\n}\r\nstatic int intel_ntb3_link_enable(struct ntb_dev *ntb,\r\nenum ntb_speed max_speed,\r\nenum ntb_width max_width)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nu32 ntb_ctl;\r\nndev = container_of(ntb, struct intel_ntb_dev, ntb);\r\ndev_dbg(ndev_dev(ndev),\r\n"Enabling link with max_speed %d max_width %d\n",\r\nmax_speed, max_width);\r\nif (max_speed != NTB_SPEED_AUTO)\r\ndev_dbg(ndev_dev(ndev), "ignoring max_speed %d\n", max_speed);\r\nif (max_width != NTB_WIDTH_AUTO)\r\ndev_dbg(ndev_dev(ndev), "ignoring max_width %d\n", max_width);\r\nntb_ctl = ioread32(ndev->self_mmio + ndev->reg->ntb_ctl);\r\nntb_ctl &= ~(NTB_CTL_DISABLE | NTB_CTL_CFG_LOCK);\r\nntb_ctl |= NTB_CTL_P2S_BAR2_SNOOP | NTB_CTL_S2P_BAR2_SNOOP;\r\nntb_ctl |= NTB_CTL_P2S_BAR4_SNOOP | NTB_CTL_S2P_BAR4_SNOOP;\r\niowrite32(ntb_ctl, ndev->self_mmio + ndev->reg->ntb_ctl);\r\nreturn 0;\r\n}\r\nstatic int intel_ntb3_mw_set_trans(struct ntb_dev *ntb, int idx,\r\ndma_addr_t addr, resource_size_t size)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nunsigned long xlat_reg, limit_reg;\r\nresource_size_t bar_size, mw_size;\r\nvoid __iomem *mmio;\r\nu64 base, limit, reg_val;\r\nint bar;\r\nif (idx >= ndev->b2b_idx && !ndev->b2b_off)\r\nidx += 1;\r\nbar = ndev_mw_to_bar(ndev, idx);\r\nif (bar < 0)\r\nreturn bar;\r\nbar_size = pci_resource_len(ndev->ntb.pdev, bar);\r\nif (idx == ndev->b2b_idx)\r\nmw_size = bar_size - ndev->b2b_off;\r\nelse\r\nmw_size = bar_size;\r\nif (addr & (bar_size - 1))\r\nreturn -EINVAL;\r\nif (size > mw_size)\r\nreturn -EINVAL;\r\nmmio = ndev->self_mmio;\r\nxlat_reg = ndev->xlat_reg->bar2_xlat + (idx * 0x10);\r\nlimit_reg = ndev->xlat_reg->bar2_limit + (idx * 0x10);\r\nbase = pci_resource_start(ndev->ntb.pdev, bar);\r\nif (limit_reg && size != mw_size)\r\nlimit = base + size;\r\nelse\r\nlimit = base + mw_size;\r\niowrite64(addr, mmio + xlat_reg);\r\nreg_val = ioread64(mmio + xlat_reg);\r\nif (reg_val != addr) {\r\niowrite64(0, mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\ndev_dbg(ndev_dev(ndev), "BAR %d IMBARXBASE: %#Lx\n", bar, reg_val);\r\niowrite64(limit, mmio + limit_reg);\r\nreg_val = ioread64(mmio + limit_reg);\r\nif (reg_val != limit) {\r\niowrite64(base, mmio + limit_reg);\r\niowrite64(0, mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\ndev_dbg(ndev_dev(ndev), "BAR %d IMBARXLMT: %#Lx\n", bar, reg_val);\r\nlimit_reg = ndev->xlat_reg->bar2_limit + (idx * 0x10) + 0x4000;\r\nbase = ioread64(mmio + SKX_EMBAR1_OFFSET + (8 * idx));\r\nbase &= ~0xf;\r\nif (limit_reg && size != mw_size)\r\nlimit = base + size;\r\nelse\r\nlimit = base + mw_size;\r\niowrite64(limit, mmio + limit_reg);\r\nreg_val = ioread64(mmio + limit_reg);\r\nif (reg_val != limit) {\r\niowrite64(base, mmio + limit_reg);\r\niowrite64(0, mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\ndev_dbg(ndev_dev(ndev), "BAR %d EMBARXLMT: %#Lx\n", bar, reg_val);\r\nreturn 0;\r\n}\r\nstatic int intel_ntb3_peer_db_set(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nint bit;\r\nif (db_bits & ~ndev->db_valid_mask)\r\nreturn -EINVAL;\r\nwhile (db_bits) {\r\nbit = __ffs(db_bits);\r\niowrite32(1, ndev->peer_mmio +\r\nndev->peer_reg->db_bell + (bit * 4));\r\ndb_bits &= db_bits - 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic u64 intel_ntb3_db_read(struct ntb_dev *ntb)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_read(ndev,\r\nndev->self_mmio +\r\nndev->self_reg->db_clear);\r\n}\r\nstatic int intel_ntb3_db_clear(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\r\nreturn ndev_db_write(ndev, db_bits,\r\nndev->self_mmio +\r\nndev->self_reg->db_clear);\r\n}\r\nstatic u64 xeon_db_ioread(void __iomem *mmio)\r\n{\r\nreturn (u64)ioread16(mmio);\r\n}\r\nstatic void xeon_db_iowrite(u64 bits, void __iomem *mmio)\r\n{\r\niowrite16((u16)bits, mmio);\r\n}\r\nstatic int xeon_poll_link(struct intel_ntb_dev *ndev)\r\n{\r\nu16 reg_val;\r\nint rc;\r\nndev->reg->db_iowrite(ndev->db_link_mask,\r\nndev->self_mmio +\r\nndev->self_reg->db_bell);\r\nrc = pci_read_config_word(ndev->ntb.pdev,\r\nXEON_LINK_STATUS_OFFSET, &reg_val);\r\nif (rc)\r\nreturn 0;\r\nif (reg_val == ndev->lnk_sta)\r\nreturn 0;\r\nndev->lnk_sta = reg_val;\r\nreturn 1;\r\n}\r\nstatic int xeon_link_is_up(struct intel_ntb_dev *ndev)\r\n{\r\nif (ndev->ntb.topo == NTB_TOPO_SEC)\r\nreturn 1;\r\nreturn NTB_LNK_STA_ACTIVE(ndev->lnk_sta);\r\n}\r\nstatic inline enum ntb_topo xeon_ppd_topo(struct intel_ntb_dev *ndev, u8 ppd)\r\n{\r\nswitch (ppd & XEON_PPD_TOPO_MASK) {\r\ncase XEON_PPD_TOPO_B2B_USD:\r\nreturn NTB_TOPO_B2B_USD;\r\ncase XEON_PPD_TOPO_B2B_DSD:\r\nreturn NTB_TOPO_B2B_DSD;\r\ncase XEON_PPD_TOPO_PRI_USD:\r\ncase XEON_PPD_TOPO_PRI_DSD:\r\nreturn NTB_TOPO_PRI;\r\ncase XEON_PPD_TOPO_SEC_USD:\r\ncase XEON_PPD_TOPO_SEC_DSD:\r\nreturn NTB_TOPO_SEC;\r\n}\r\nreturn NTB_TOPO_NONE;\r\n}\r\nstatic inline int xeon_ppd_bar4_split(struct intel_ntb_dev *ndev, u8 ppd)\r\n{\r\nif (ppd & XEON_PPD_SPLIT_BAR_MASK) {\r\ndev_dbg(ndev_dev(ndev), "PPD %d split bar\n", ppd);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int xeon_init_isr(struct intel_ntb_dev *ndev)\r\n{\r\nreturn ndev_init_isr(ndev, XEON_DB_MSIX_VECTOR_COUNT,\r\nXEON_DB_MSIX_VECTOR_COUNT,\r\nXEON_DB_MSIX_VECTOR_SHIFT,\r\nXEON_DB_TOTAL_SHIFT);\r\n}\r\nstatic void xeon_deinit_isr(struct intel_ntb_dev *ndev)\r\n{\r\nndev_deinit_isr(ndev);\r\n}\r\nstatic int xeon_setup_b2b_mw(struct intel_ntb_dev *ndev,\r\nconst struct intel_b2b_addr *addr,\r\nconst struct intel_b2b_addr *peer_addr)\r\n{\r\nstruct pci_dev *pdev;\r\nvoid __iomem *mmio;\r\nresource_size_t bar_size;\r\nphys_addr_t bar_addr;\r\nint b2b_bar;\r\nu8 bar_sz;\r\npdev = ndev_pdev(ndev);\r\nmmio = ndev->self_mmio;\r\nif (ndev->b2b_idx == UINT_MAX) {\r\ndev_dbg(ndev_dev(ndev), "not using b2b mw\n");\r\nb2b_bar = 0;\r\nndev->b2b_off = 0;\r\n} else {\r\nb2b_bar = ndev_mw_to_bar(ndev, ndev->b2b_idx);\r\nif (b2b_bar < 0)\r\nreturn -EIO;\r\ndev_dbg(ndev_dev(ndev), "using b2b mw bar %d\n", b2b_bar);\r\nbar_size = pci_resource_len(ndev->ntb.pdev, b2b_bar);\r\ndev_dbg(ndev_dev(ndev), "b2b bar size %#llx\n", bar_size);\r\nif (b2b_mw_share && XEON_B2B_MIN_SIZE <= bar_size >> 1) {\r\ndev_dbg(ndev_dev(ndev),\r\n"b2b using first half of bar\n");\r\nndev->b2b_off = bar_size >> 1;\r\n} else if (XEON_B2B_MIN_SIZE <= bar_size) {\r\ndev_dbg(ndev_dev(ndev),\r\n"b2b using whole bar\n");\r\nndev->b2b_off = 0;\r\n--ndev->mw_count;\r\n} else {\r\ndev_dbg(ndev_dev(ndev),\r\n"b2b bar size is too small\n");\r\nreturn -EIO;\r\n}\r\n}\r\npci_read_config_byte(pdev, XEON_PBAR23SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "PBAR23SZ %#x\n", bar_sz);\r\nif (b2b_bar == 2) {\r\nif (ndev->b2b_off)\r\nbar_sz -= 1;\r\nelse\r\nbar_sz = 0;\r\n}\r\npci_write_config_byte(pdev, XEON_SBAR23SZ_OFFSET, bar_sz);\r\npci_read_config_byte(pdev, XEON_SBAR23SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "SBAR23SZ %#x\n", bar_sz);\r\nif (!ndev->bar4_split) {\r\npci_read_config_byte(pdev, XEON_PBAR45SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "PBAR45SZ %#x\n", bar_sz);\r\nif (b2b_bar == 4) {\r\nif (ndev->b2b_off)\r\nbar_sz -= 1;\r\nelse\r\nbar_sz = 0;\r\n}\r\npci_write_config_byte(pdev, XEON_SBAR45SZ_OFFSET, bar_sz);\r\npci_read_config_byte(pdev, XEON_SBAR45SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "SBAR45SZ %#x\n", bar_sz);\r\n} else {\r\npci_read_config_byte(pdev, XEON_PBAR4SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "PBAR4SZ %#x\n", bar_sz);\r\nif (b2b_bar == 4) {\r\nif (ndev->b2b_off)\r\nbar_sz -= 1;\r\nelse\r\nbar_sz = 0;\r\n}\r\npci_write_config_byte(pdev, XEON_SBAR4SZ_OFFSET, bar_sz);\r\npci_read_config_byte(pdev, XEON_SBAR4SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "SBAR4SZ %#x\n", bar_sz);\r\npci_read_config_byte(pdev, XEON_PBAR5SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "PBAR5SZ %#x\n", bar_sz);\r\nif (b2b_bar == 5) {\r\nif (ndev->b2b_off)\r\nbar_sz -= 1;\r\nelse\r\nbar_sz = 0;\r\n}\r\npci_write_config_byte(pdev, XEON_SBAR5SZ_OFFSET, bar_sz);\r\npci_read_config_byte(pdev, XEON_SBAR5SZ_OFFSET, &bar_sz);\r\ndev_dbg(ndev_dev(ndev), "SBAR5SZ %#x\n", bar_sz);\r\n}\r\nif (b2b_bar == 0)\r\nbar_addr = addr->bar0_addr;\r\nelse if (b2b_bar == 2)\r\nbar_addr = addr->bar2_addr64;\r\nelse if (b2b_bar == 4 && !ndev->bar4_split)\r\nbar_addr = addr->bar4_addr64;\r\nelse if (b2b_bar == 4)\r\nbar_addr = addr->bar4_addr32;\r\nelse if (b2b_bar == 5)\r\nbar_addr = addr->bar5_addr32;\r\nelse\r\nreturn -EIO;\r\ndev_dbg(ndev_dev(ndev), "SBAR01 %#018llx\n", bar_addr);\r\niowrite64(bar_addr, mmio + XEON_SBAR0BASE_OFFSET);\r\nbar_addr = addr->bar2_addr64 + (b2b_bar == 2 ? ndev->b2b_off : 0);\r\niowrite64(bar_addr, mmio + XEON_SBAR23BASE_OFFSET);\r\nbar_addr = ioread64(mmio + XEON_SBAR23BASE_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR23 %#018llx\n", bar_addr);\r\nif (!ndev->bar4_split) {\r\nbar_addr = addr->bar4_addr64 +\r\n(b2b_bar == 4 ? ndev->b2b_off : 0);\r\niowrite64(bar_addr, mmio + XEON_SBAR45BASE_OFFSET);\r\nbar_addr = ioread64(mmio + XEON_SBAR45BASE_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR45 %#018llx\n", bar_addr);\r\n} else {\r\nbar_addr = addr->bar4_addr32 +\r\n(b2b_bar == 4 ? ndev->b2b_off : 0);\r\niowrite32(bar_addr, mmio + XEON_SBAR4BASE_OFFSET);\r\nbar_addr = ioread32(mmio + XEON_SBAR4BASE_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR4 %#010llx\n", bar_addr);\r\nbar_addr = addr->bar5_addr32 +\r\n(b2b_bar == 5 ? ndev->b2b_off : 0);\r\niowrite32(bar_addr, mmio + XEON_SBAR5BASE_OFFSET);\r\nbar_addr = ioread32(mmio + XEON_SBAR5BASE_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR5 %#010llx\n", bar_addr);\r\n}\r\nbar_addr = addr->bar2_addr64 + (b2b_bar == 2 ? ndev->b2b_off : 0);\r\niowrite64(bar_addr, mmio + XEON_SBAR23LMT_OFFSET);\r\nbar_addr = ioread64(mmio + XEON_SBAR23LMT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR23LMT %#018llx\n", bar_addr);\r\nif (!ndev->bar4_split) {\r\nbar_addr = addr->bar4_addr64 +\r\n(b2b_bar == 4 ? ndev->b2b_off : 0);\r\niowrite64(bar_addr, mmio + XEON_SBAR45LMT_OFFSET);\r\nbar_addr = ioread64(mmio + XEON_SBAR45LMT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR45LMT %#018llx\n", bar_addr);\r\n} else {\r\nbar_addr = addr->bar4_addr32 +\r\n(b2b_bar == 4 ? ndev->b2b_off : 0);\r\niowrite32(bar_addr, mmio + XEON_SBAR4LMT_OFFSET);\r\nbar_addr = ioread32(mmio + XEON_SBAR4LMT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR4LMT %#010llx\n", bar_addr);\r\nbar_addr = addr->bar5_addr32 +\r\n(b2b_bar == 5 ? ndev->b2b_off : 0);\r\niowrite32(bar_addr, mmio + XEON_SBAR5LMT_OFFSET);\r\nbar_addr = ioread32(mmio + XEON_SBAR5LMT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "SBAR5LMT %#05llx\n", bar_addr);\r\n}\r\niowrite64(0, mmio + XEON_SBAR23XLAT_OFFSET);\r\nif (!ndev->bar4_split) {\r\niowrite64(0, mmio + XEON_SBAR45XLAT_OFFSET);\r\n} else {\r\niowrite32(0, mmio + XEON_SBAR4XLAT_OFFSET);\r\niowrite32(0, mmio + XEON_SBAR5XLAT_OFFSET);\r\n}\r\niowrite64(0, mmio + XEON_PBAR23LMT_OFFSET);\r\nif (!ndev->bar4_split) {\r\niowrite64(0, mmio + XEON_PBAR45LMT_OFFSET);\r\n} else {\r\niowrite32(0, mmio + XEON_PBAR4LMT_OFFSET);\r\niowrite32(0, mmio + XEON_PBAR5LMT_OFFSET);\r\n}\r\nbar_addr = peer_addr->bar2_addr64;\r\niowrite64(bar_addr, mmio + XEON_PBAR23XLAT_OFFSET);\r\nbar_addr = ioread64(mmio + XEON_PBAR23XLAT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "PBAR23XLAT %#018llx\n", bar_addr);\r\nif (!ndev->bar4_split) {\r\nbar_addr = peer_addr->bar4_addr64;\r\niowrite64(bar_addr, mmio + XEON_PBAR45XLAT_OFFSET);\r\nbar_addr = ioread64(mmio + XEON_PBAR45XLAT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "PBAR45XLAT %#018llx\n", bar_addr);\r\n} else {\r\nbar_addr = peer_addr->bar4_addr32;\r\niowrite32(bar_addr, mmio + XEON_PBAR4XLAT_OFFSET);\r\nbar_addr = ioread32(mmio + XEON_PBAR4XLAT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "PBAR4XLAT %#010llx\n", bar_addr);\r\nbar_addr = peer_addr->bar5_addr32;\r\niowrite32(bar_addr, mmio + XEON_PBAR5XLAT_OFFSET);\r\nbar_addr = ioread32(mmio + XEON_PBAR5XLAT_OFFSET);\r\ndev_dbg(ndev_dev(ndev), "PBAR5XLAT %#010llx\n", bar_addr);\r\n}\r\nif (b2b_bar == 0)\r\nbar_addr = peer_addr->bar0_addr;\r\nelse if (b2b_bar == 2)\r\nbar_addr = peer_addr->bar2_addr64;\r\nelse if (b2b_bar == 4 && !ndev->bar4_split)\r\nbar_addr = peer_addr->bar4_addr64;\r\nelse if (b2b_bar == 4)\r\nbar_addr = peer_addr->bar4_addr32;\r\nelse if (b2b_bar == 5)\r\nbar_addr = peer_addr->bar5_addr32;\r\nelse\r\nreturn -EIO;\r\ndev_dbg(ndev_dev(ndev), "B2BXLAT %#018llx\n", bar_addr);\r\niowrite32(bar_addr, mmio + XEON_B2B_XLAT_OFFSETL);\r\niowrite32(bar_addr >> 32, mmio + XEON_B2B_XLAT_OFFSETU);\r\nif (b2b_bar) {\r\nndev->peer_mmio = pci_iomap(pdev, b2b_bar,\r\nXEON_B2B_MIN_SIZE);\r\nif (!ndev->peer_mmio)\r\nreturn -EIO;\r\nndev->peer_addr = pci_resource_start(pdev, b2b_bar);\r\n}\r\nreturn 0;\r\n}\r\nstatic int xeon_init_ntb(struct intel_ntb_dev *ndev)\r\n{\r\nint rc;\r\nu32 ntb_ctl;\r\nif (ndev->bar4_split)\r\nndev->mw_count = HSX_SPLIT_BAR_MW_COUNT;\r\nelse\r\nndev->mw_count = XEON_MW_COUNT;\r\nndev->spad_count = XEON_SPAD_COUNT;\r\nndev->db_count = XEON_DB_COUNT;\r\nndev->db_link_mask = XEON_DB_LINK_BIT;\r\nswitch (ndev->ntb.topo) {\r\ncase NTB_TOPO_PRI:\r\nif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP) {\r\ndev_err(ndev_dev(ndev), "NTB Primary config disabled\n");\r\nreturn -EINVAL;\r\n}\r\nntb_ctl = ioread32(ndev->self_mmio + ndev->reg->ntb_ctl);\r\nntb_ctl &= ~NTB_CTL_DISABLE;\r\niowrite32(ntb_ctl, ndev->self_mmio + ndev->reg->ntb_ctl);\r\nndev->spad_count >>= 1;\r\nndev->self_reg = &xeon_pri_reg;\r\nndev->peer_reg = &xeon_sec_reg;\r\nndev->xlat_reg = &xeon_sec_xlat;\r\nbreak;\r\ncase NTB_TOPO_SEC:\r\nif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP) {\r\ndev_err(ndev_dev(ndev), "NTB Secondary config disabled\n");\r\nreturn -EINVAL;\r\n}\r\nndev->spad_count >>= 1;\r\nndev->self_reg = &xeon_sec_reg;\r\nndev->peer_reg = &xeon_pri_reg;\r\nndev->xlat_reg = &xeon_pri_xlat;\r\nbreak;\r\ncase NTB_TOPO_B2B_USD:\r\ncase NTB_TOPO_B2B_DSD:\r\nndev->self_reg = &xeon_pri_reg;\r\nndev->peer_reg = &xeon_b2b_reg;\r\nndev->xlat_reg = &xeon_sec_xlat;\r\nif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP) {\r\nndev->peer_reg = &xeon_pri_reg;\r\nif (b2b_mw_idx < 0)\r\nndev->b2b_idx = b2b_mw_idx + ndev->mw_count;\r\nelse\r\nndev->b2b_idx = b2b_mw_idx;\r\nif (ndev->b2b_idx >= ndev->mw_count) {\r\ndev_dbg(ndev_dev(ndev),\r\n"b2b_mw_idx %d invalid for mw_count %u\n",\r\nb2b_mw_idx, ndev->mw_count);\r\nreturn -EINVAL;\r\n}\r\ndev_dbg(ndev_dev(ndev),\r\n"setting up b2b mw idx %d means %d\n",\r\nb2b_mw_idx, ndev->b2b_idx);\r\n} else if (ndev->hwerr_flags & NTB_HWERR_B2BDOORBELL_BIT14) {\r\ndev_warn(ndev_dev(ndev), "Reduce doorbell count by 1\n");\r\nndev->db_count -= 1;\r\n}\r\nif (ndev->ntb.topo == NTB_TOPO_B2B_USD) {\r\nrc = xeon_setup_b2b_mw(ndev,\r\n&xeon_b2b_dsd_addr,\r\n&xeon_b2b_usd_addr);\r\n} else {\r\nrc = xeon_setup_b2b_mw(ndev,\r\n&xeon_b2b_usd_addr,\r\n&xeon_b2b_dsd_addr);\r\n}\r\nif (rc)\r\nreturn rc;\r\niowrite16(PCI_COMMAND_MEMORY | PCI_COMMAND_MASTER,\r\nndev->self_mmio + XEON_SPCICMD_OFFSET);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\r\nndev->reg->db_iowrite(ndev->db_valid_mask,\r\nndev->self_mmio +\r\nndev->self_reg->db_mask);\r\nreturn 0;\r\n}\r\nstatic int xeon_init_dev(struct intel_ntb_dev *ndev)\r\n{\r\nstruct pci_dev *pdev;\r\nu8 ppd;\r\nint rc, mem;\r\npdev = ndev_pdev(ndev);\r\nswitch (pdev->device) {\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_BDX:\r\nndev->hwerr_flags |= NTB_HWERR_SDOORBELL_LOCKUP;\r\nbreak;\r\n}\r\nswitch (pdev->device) {\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_BDX:\r\nndev->hwerr_flags |= NTB_HWERR_SB01BASE_LOCKUP;\r\nbreak;\r\n}\r\nswitch (pdev->device) {\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_JSF:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_SNB:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_IVT:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_HSX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_SS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_PS_BDX:\r\ncase PCI_DEVICE_ID_INTEL_NTB_B2B_BDX:\r\nndev->hwerr_flags |= NTB_HWERR_B2BDOORBELL_BIT14;\r\nbreak;\r\n}\r\nndev->reg = &xeon_reg;\r\nrc = pci_read_config_byte(pdev, XEON_PPD_OFFSET, &ppd);\r\nif (rc)\r\nreturn -EIO;\r\nndev->ntb.topo = xeon_ppd_topo(ndev, ppd);\r\ndev_dbg(ndev_dev(ndev), "ppd %#x topo %s\n", ppd,\r\nntb_topo_string(ndev->ntb.topo));\r\nif (ndev->ntb.topo == NTB_TOPO_NONE)\r\nreturn -EINVAL;\r\nif (ndev->ntb.topo != NTB_TOPO_SEC) {\r\nndev->bar4_split = xeon_ppd_bar4_split(ndev, ppd);\r\ndev_dbg(ndev_dev(ndev), "ppd %#x bar4_split %d\n",\r\nppd, ndev->bar4_split);\r\n} else {\r\nmem = pci_select_bars(pdev, IORESOURCE_MEM);\r\nndev->bar4_split = hweight32(mem) ==\r\nHSX_SPLIT_BAR_MW_COUNT + 1;\r\ndev_dbg(ndev_dev(ndev), "mem %#x bar4_split %d\n",\r\nmem, ndev->bar4_split);\r\n}\r\nrc = xeon_init_ntb(ndev);\r\nif (rc)\r\nreturn rc;\r\nreturn xeon_init_isr(ndev);\r\n}\r\nstatic void xeon_deinit_dev(struct intel_ntb_dev *ndev)\r\n{\r\nxeon_deinit_isr(ndev);\r\n}\r\nstatic int intel_ntb_init_pci(struct intel_ntb_dev *ndev, struct pci_dev *pdev)\r\n{\r\nint rc;\r\npci_set_drvdata(pdev, ndev);\r\nrc = pci_enable_device(pdev);\r\nif (rc)\r\ngoto err_pci_enable;\r\nrc = pci_request_regions(pdev, NTB_NAME);\r\nif (rc)\r\ngoto err_pci_regions;\r\npci_set_master(pdev);\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (rc) {\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc)\r\ngoto err_dma_mask;\r\ndev_warn(ndev_dev(ndev), "Cannot DMA highmem\n");\r\n}\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (rc) {\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc)\r\ngoto err_dma_mask;\r\ndev_warn(ndev_dev(ndev), "Cannot DMA consistent highmem\n");\r\n}\r\nndev->self_mmio = pci_iomap(pdev, 0, 0);\r\nif (!ndev->self_mmio) {\r\nrc = -EIO;\r\ngoto err_mmio;\r\n}\r\nndev->peer_mmio = ndev->self_mmio;\r\nndev->peer_addr = pci_resource_start(pdev, 0);\r\nreturn 0;\r\nerr_mmio:\r\nerr_dma_mask:\r\npci_clear_master(pdev);\r\npci_release_regions(pdev);\r\nerr_pci_regions:\r\npci_disable_device(pdev);\r\nerr_pci_enable:\r\npci_set_drvdata(pdev, NULL);\r\nreturn rc;\r\n}\r\nstatic void intel_ntb_deinit_pci(struct intel_ntb_dev *ndev)\r\n{\r\nstruct pci_dev *pdev = ndev_pdev(ndev);\r\nif (ndev->peer_mmio && ndev->peer_mmio != ndev->self_mmio)\r\npci_iounmap(pdev, ndev->peer_mmio);\r\npci_iounmap(pdev, ndev->self_mmio);\r\npci_clear_master(pdev);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\n}\r\nstatic inline void ndev_init_struct(struct intel_ntb_dev *ndev,\r\nstruct pci_dev *pdev)\r\n{\r\nndev->ntb.pdev = pdev;\r\nndev->ntb.topo = NTB_TOPO_NONE;\r\nndev->ntb.ops = &intel_ntb_ops;\r\nndev->b2b_off = 0;\r\nndev->b2b_idx = UINT_MAX;\r\nndev->bar4_split = 0;\r\nndev->mw_count = 0;\r\nndev->spad_count = 0;\r\nndev->db_count = 0;\r\nndev->db_vec_count = 0;\r\nndev->db_vec_shift = 0;\r\nndev->ntb_ctl = 0;\r\nndev->lnk_sta = 0;\r\nndev->db_valid_mask = 0;\r\nndev->db_link_mask = 0;\r\nndev->db_mask = 0;\r\nspin_lock_init(&ndev->db_mask_lock);\r\n}\r\nstatic int intel_ntb_pci_probe(struct pci_dev *pdev,\r\nconst struct pci_device_id *id)\r\n{\r\nstruct intel_ntb_dev *ndev;\r\nint rc, node;\r\nnode = dev_to_node(&pdev->dev);\r\nif (pdev_is_atom(pdev)) {\r\nndev = kzalloc_node(sizeof(*ndev), GFP_KERNEL, node);\r\nif (!ndev) {\r\nrc = -ENOMEM;\r\ngoto err_ndev;\r\n}\r\nndev_init_struct(ndev, pdev);\r\nrc = intel_ntb_init_pci(ndev, pdev);\r\nif (rc)\r\ngoto err_init_pci;\r\nrc = atom_init_dev(ndev);\r\nif (rc)\r\ngoto err_init_dev;\r\n} else if (pdev_is_xeon(pdev)) {\r\nndev = kzalloc_node(sizeof(*ndev), GFP_KERNEL, node);\r\nif (!ndev) {\r\nrc = -ENOMEM;\r\ngoto err_ndev;\r\n}\r\nndev_init_struct(ndev, pdev);\r\nrc = intel_ntb_init_pci(ndev, pdev);\r\nif (rc)\r\ngoto err_init_pci;\r\nrc = xeon_init_dev(ndev);\r\nif (rc)\r\ngoto err_init_dev;\r\n} else if (pdev_is_skx_xeon(pdev)) {\r\nndev = kzalloc_node(sizeof(*ndev), GFP_KERNEL, node);\r\nif (!ndev) {\r\nrc = -ENOMEM;\r\ngoto err_ndev;\r\n}\r\nndev_init_struct(ndev, pdev);\r\nndev->ntb.ops = &intel_ntb3_ops;\r\nrc = intel_ntb_init_pci(ndev, pdev);\r\nif (rc)\r\ngoto err_init_pci;\r\nrc = skx_init_dev(ndev);\r\nif (rc)\r\ngoto err_init_dev;\r\n} else {\r\nrc = -EINVAL;\r\ngoto err_ndev;\r\n}\r\nndev_reset_unsafe_flags(ndev);\r\nndev->reg->poll_link(ndev);\r\nndev_init_debugfs(ndev);\r\nrc = ntb_register_device(&ndev->ntb);\r\nif (rc)\r\ngoto err_register;\r\ndev_info(&pdev->dev, "NTB device registered.\n");\r\nreturn 0;\r\nerr_register:\r\nndev_deinit_debugfs(ndev);\r\nif (pdev_is_atom(pdev))\r\natom_deinit_dev(ndev);\r\nelse if (pdev_is_xeon(pdev) || pdev_is_skx_xeon(pdev))\r\nxeon_deinit_dev(ndev);\r\nerr_init_dev:\r\nintel_ntb_deinit_pci(ndev);\r\nerr_init_pci:\r\nkfree(ndev);\r\nerr_ndev:\r\nreturn rc;\r\n}\r\nstatic void intel_ntb_pci_remove(struct pci_dev *pdev)\r\n{\r\nstruct intel_ntb_dev *ndev = pci_get_drvdata(pdev);\r\nntb_unregister_device(&ndev->ntb);\r\nndev_deinit_debugfs(ndev);\r\nif (pdev_is_atom(pdev))\r\natom_deinit_dev(ndev);\r\nelse if (pdev_is_xeon(pdev) || pdev_is_skx_xeon(pdev))\r\nxeon_deinit_dev(ndev);\r\nintel_ntb_deinit_pci(ndev);\r\nkfree(ndev);\r\n}\r\nstatic int __init intel_ntb_pci_driver_init(void)\r\n{\r\npr_info("%s %s\n", NTB_DESC, NTB_VER);\r\nif (debugfs_initialized())\r\ndebugfs_dir = debugfs_create_dir(KBUILD_MODNAME, NULL);\r\nreturn pci_register_driver(&intel_ntb_pci_driver);\r\n}\r\nstatic void __exit intel_ntb_pci_driver_exit(void)\r\n{\r\npci_unregister_driver(&intel_ntb_pci_driver);\r\ndebugfs_remove_recursive(debugfs_dir);\r\n}
