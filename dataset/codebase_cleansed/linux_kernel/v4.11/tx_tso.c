static inline void prefetch_ptr(struct efx_tx_queue *tx_queue)\r\n{\r\nunsigned int insert_ptr = efx_tx_queue_get_insert_index(tx_queue);\r\nchar *ptr;\r\nptr = (char *) (tx_queue->buffer + insert_ptr);\r\nprefetch(ptr);\r\nprefetch(ptr + 0x80);\r\nptr = (char *) (((efx_qword_t *)tx_queue->txd.buf.addr) + insert_ptr);\r\nprefetch(ptr);\r\nprefetch(ptr + 0x80);\r\n}\r\nstatic void efx_tx_queue_insert(struct efx_tx_queue *tx_queue,\r\ndma_addr_t dma_addr, unsigned int len,\r\nstruct efx_tx_buffer **final_buffer)\r\n{\r\nstruct efx_tx_buffer *buffer;\r\nunsigned int dma_len;\r\nEFX_WARN_ON_ONCE_PARANOID(len <= 0);\r\nwhile (1) {\r\nbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\r\n++tx_queue->insert_count;\r\nEFX_WARN_ON_ONCE_PARANOID(tx_queue->insert_count -\r\ntx_queue->read_count >=\r\ntx_queue->efx->txq_entries);\r\nbuffer->dma_addr = dma_addr;\r\ndma_len = tx_queue->efx->type->tx_limit_len(tx_queue,\r\ndma_addr, len);\r\nif (dma_len >= len)\r\nbreak;\r\nbuffer->len = dma_len;\r\nbuffer->flags = EFX_TX_BUF_CONT;\r\ndma_addr += dma_len;\r\nlen -= dma_len;\r\n}\r\nEFX_WARN_ON_ONCE_PARANOID(!len);\r\nbuffer->len = len;\r\n*final_buffer = buffer;\r\n}\r\nstatic __be16 efx_tso_check_protocol(struct sk_buff *skb)\r\n{\r\n__be16 protocol = skb->protocol;\r\nEFX_WARN_ON_ONCE_PARANOID(((struct ethhdr *)skb->data)->h_proto !=\r\nprotocol);\r\nif (protocol == htons(ETH_P_8021Q)) {\r\nstruct vlan_ethhdr *veh = (struct vlan_ethhdr *)skb->data;\r\nprotocol = veh->h_vlan_encapsulated_proto;\r\n}\r\nif (protocol == htons(ETH_P_IP)) {\r\nEFX_WARN_ON_ONCE_PARANOID(ip_hdr(skb)->protocol != IPPROTO_TCP);\r\n} else {\r\nEFX_WARN_ON_ONCE_PARANOID(protocol != htons(ETH_P_IPV6));\r\nEFX_WARN_ON_ONCE_PARANOID(ipv6_hdr(skb)->nexthdr != NEXTHDR_TCP);\r\n}\r\nEFX_WARN_ON_ONCE_PARANOID((PTR_DIFF(tcp_hdr(skb), skb->data) +\r\n(tcp_hdr(skb)->doff << 2u)) >\r\nskb_headlen(skb));\r\nreturn protocol;\r\n}\r\nstatic int tso_start(struct tso_state *st, struct efx_nic *efx,\r\nstruct efx_tx_queue *tx_queue,\r\nconst struct sk_buff *skb)\r\n{\r\nstruct device *dma_dev = &efx->pci_dev->dev;\r\nunsigned int header_len, in_len;\r\ndma_addr_t dma_addr;\r\nst->ip_off = skb_network_header(skb) - skb->data;\r\nst->tcp_off = skb_transport_header(skb) - skb->data;\r\nheader_len = st->tcp_off + (tcp_hdr(skb)->doff << 2u);\r\nin_len = skb_headlen(skb) - header_len;\r\nst->header_len = header_len;\r\nst->in_len = in_len;\r\nif (st->protocol == htons(ETH_P_IP)) {\r\nst->ip_base_len = st->header_len - st->ip_off;\r\nst->ipv4_id = ntohs(ip_hdr(skb)->id);\r\n} else {\r\nst->ip_base_len = st->header_len - st->tcp_off;\r\nst->ipv4_id = 0;\r\n}\r\nst->seqnum = ntohl(tcp_hdr(skb)->seq);\r\nEFX_WARN_ON_ONCE_PARANOID(tcp_hdr(skb)->urg);\r\nEFX_WARN_ON_ONCE_PARANOID(tcp_hdr(skb)->syn);\r\nEFX_WARN_ON_ONCE_PARANOID(tcp_hdr(skb)->rst);\r\nst->out_len = skb->len - header_len;\r\ndma_addr = dma_map_single(dma_dev, skb->data,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\nst->header_dma_addr = dma_addr;\r\nst->header_unmap_len = skb_headlen(skb);\r\nst->dma_addr = dma_addr + header_len;\r\nst->unmap_len = 0;\r\nreturn unlikely(dma_mapping_error(dma_dev, dma_addr)) ? -ENOMEM : 0;\r\n}\r\nstatic int tso_get_fragment(struct tso_state *st, struct efx_nic *efx,\r\nskb_frag_t *frag)\r\n{\r\nst->unmap_addr = skb_frag_dma_map(&efx->pci_dev->dev, frag, 0,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\nif (likely(!dma_mapping_error(&efx->pci_dev->dev, st->unmap_addr))) {\r\nst->unmap_len = skb_frag_size(frag);\r\nst->in_len = skb_frag_size(frag);\r\nst->dma_addr = st->unmap_addr;\r\nreturn 0;\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void tso_fill_packet_with_fragment(struct efx_tx_queue *tx_queue,\r\nconst struct sk_buff *skb,\r\nstruct tso_state *st)\r\n{\r\nstruct efx_tx_buffer *buffer;\r\nint n;\r\nif (st->in_len == 0)\r\nreturn;\r\nif (st->packet_space == 0)\r\nreturn;\r\nEFX_WARN_ON_ONCE_PARANOID(st->in_len <= 0);\r\nEFX_WARN_ON_ONCE_PARANOID(st->packet_space <= 0);\r\nn = min(st->in_len, st->packet_space);\r\nst->packet_space -= n;\r\nst->out_len -= n;\r\nst->in_len -= n;\r\nefx_tx_queue_insert(tx_queue, st->dma_addr, n, &buffer);\r\nif (st->out_len == 0) {\r\nbuffer->skb = skb;\r\nbuffer->flags = EFX_TX_BUF_SKB;\r\n} else if (st->packet_space != 0) {\r\nbuffer->flags = EFX_TX_BUF_CONT;\r\n}\r\nif (st->in_len == 0) {\r\nbuffer->unmap_len = st->unmap_len;\r\nbuffer->dma_offset = buffer->unmap_len - buffer->len;\r\nst->unmap_len = 0;\r\n}\r\nst->dma_addr += n;\r\n}\r\nstatic int tso_start_new_packet(struct efx_tx_queue *tx_queue,\r\nconst struct sk_buff *skb,\r\nstruct tso_state *st)\r\n{\r\nstruct efx_tx_buffer *buffer =\r\nefx_tx_queue_get_insert_buffer(tx_queue);\r\nbool is_last = st->out_len <= skb_shinfo(skb)->gso_size;\r\nu8 tcp_flags_mask, tcp_flags;\r\nif (!is_last) {\r\nst->packet_space = skb_shinfo(skb)->gso_size;\r\ntcp_flags_mask = 0x09;\r\n} else {\r\nst->packet_space = st->out_len;\r\ntcp_flags_mask = 0x00;\r\n}\r\nif (WARN_ON(!st->header_unmap_len))\r\nreturn -EINVAL;\r\ntcp_flags = ((u8 *)tcp_hdr(skb))[TCP_FLAGS_OFFSET] & ~tcp_flags_mask;\r\nbuffer->flags = EFX_TX_BUF_OPTION;\r\nbuffer->len = 0;\r\nbuffer->unmap_len = 0;\r\nEFX_POPULATE_QWORD_5(buffer->option,\r\nESF_DZ_TX_DESC_IS_OPT, 1,\r\nESF_DZ_TX_OPTION_TYPE,\r\nESE_DZ_TX_OPTION_DESC_TSO,\r\nESF_DZ_TX_TSO_TCP_FLAGS, tcp_flags,\r\nESF_DZ_TX_TSO_IP_ID, st->ipv4_id,\r\nESF_DZ_TX_TSO_TCP_SEQNO, st->seqnum);\r\n++tx_queue->insert_count;\r\nbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\r\nbuffer->dma_addr = st->header_dma_addr;\r\nbuffer->len = st->header_len;\r\nif (is_last) {\r\nbuffer->flags = EFX_TX_BUF_CONT | EFX_TX_BUF_MAP_SINGLE;\r\nbuffer->unmap_len = st->header_unmap_len;\r\nbuffer->dma_offset = 0;\r\nst->header_unmap_len = 0;\r\n} else {\r\nbuffer->flags = EFX_TX_BUF_CONT;\r\nbuffer->unmap_len = 0;\r\n}\r\n++tx_queue->insert_count;\r\nst->seqnum += skb_shinfo(skb)->gso_size;\r\n++st->ipv4_id;\r\nreturn 0;\r\n}\r\nint efx_enqueue_skb_tso(struct efx_tx_queue *tx_queue,\r\nstruct sk_buff *skb,\r\nbool *data_mapped)\r\n{\r\nstruct efx_nic *efx = tx_queue->efx;\r\nint frag_i, rc;\r\nstruct tso_state state;\r\nif (tx_queue->tso_version != 1)\r\nreturn -EINVAL;\r\nprefetch(skb->data);\r\nstate.protocol = efx_tso_check_protocol(skb);\r\nEFX_WARN_ON_ONCE_PARANOID(tx_queue->write_count != tx_queue->insert_count);\r\nrc = tso_start(&state, efx, tx_queue, skb);\r\nif (rc)\r\ngoto fail;\r\nif (likely(state.in_len == 0)) {\r\nEFX_WARN_ON_ONCE_PARANOID(skb_shinfo(skb)->nr_frags < 1);\r\nfrag_i = 0;\r\nrc = tso_get_fragment(&state, efx,\r\nskb_shinfo(skb)->frags + frag_i);\r\nif (rc)\r\ngoto fail;\r\n} else {\r\nfrag_i = -1;\r\n}\r\nrc = tso_start_new_packet(tx_queue, skb, &state);\r\nif (rc)\r\ngoto fail;\r\nprefetch_ptr(tx_queue);\r\nwhile (1) {\r\ntso_fill_packet_with_fragment(tx_queue, skb, &state);\r\nif (state.in_len == 0) {\r\nif (++frag_i >= skb_shinfo(skb)->nr_frags)\r\nbreak;\r\nrc = tso_get_fragment(&state, efx,\r\nskb_shinfo(skb)->frags + frag_i);\r\nif (rc)\r\ngoto fail;\r\n}\r\nif (state.packet_space == 0) {\r\nrc = tso_start_new_packet(tx_queue, skb, &state);\r\nif (rc)\r\ngoto fail;\r\n}\r\n}\r\n*data_mapped = true;\r\nreturn 0;\r\nfail:\r\nif (rc == -ENOMEM)\r\nnetif_err(efx, tx_err, efx->net_dev,\r\n"Out of memory for TSO headers, or DMA mapping error\n");\r\nelse\r\nnetif_err(efx, tx_err, efx->net_dev, "TSO failed, rc = %d\n", rc);\r\nif (state.unmap_len) {\r\ndma_unmap_page(&efx->pci_dev->dev, state.unmap_addr,\r\nstate.unmap_len, DMA_TO_DEVICE);\r\n}\r\nif (state.header_unmap_len)\r\ndma_unmap_single(&efx->pci_dev->dev, state.header_dma_addr,\r\nstate.header_unmap_len, DMA_TO_DEVICE);\r\nreturn rc;\r\n}
