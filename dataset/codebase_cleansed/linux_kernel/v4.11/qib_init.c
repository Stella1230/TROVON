void qib_set_ctxtcnt(struct qib_devdata *dd)\r\n{\r\nif (!qib_cfgctxts) {\r\ndd->cfgctxts = dd->first_user_ctxt + num_online_cpus();\r\nif (dd->cfgctxts > dd->ctxtcnt)\r\ndd->cfgctxts = dd->ctxtcnt;\r\n} else if (qib_cfgctxts < dd->num_pports)\r\ndd->cfgctxts = dd->ctxtcnt;\r\nelse if (qib_cfgctxts <= dd->ctxtcnt)\r\ndd->cfgctxts = qib_cfgctxts;\r\nelse\r\ndd->cfgctxts = dd->ctxtcnt;\r\ndd->freectxts = (dd->first_user_ctxt > dd->cfgctxts) ? 0 :\r\ndd->cfgctxts - dd->first_user_ctxt;\r\n}\r\nint qib_create_ctxts(struct qib_devdata *dd)\r\n{\r\nunsigned i;\r\nint local_node_id = pcibus_to_node(dd->pcidev->bus);\r\nif (local_node_id < 0)\r\nlocal_node_id = numa_node_id();\r\ndd->assigned_node_id = local_node_id;\r\ndd->rcd = kcalloc(dd->ctxtcnt, sizeof(*dd->rcd), GFP_KERNEL);\r\nif (!dd->rcd)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < dd->first_user_ctxt; ++i) {\r\nstruct qib_pportdata *ppd;\r\nstruct qib_ctxtdata *rcd;\r\nif (dd->skip_kctxt_mask & (1 << i))\r\ncontinue;\r\nppd = dd->pport + (i % dd->num_pports);\r\nrcd = qib_create_ctxtdata(ppd, i, dd->assigned_node_id);\r\nif (!rcd) {\r\nqib_dev_err(dd,\r\n"Unable to allocate ctxtdata for Kernel ctxt, failing\n");\r\nkfree(dd->rcd);\r\ndd->rcd = NULL;\r\nreturn -ENOMEM;\r\n}\r\nrcd->pkeys[0] = QIB_DEFAULT_P_KEY;\r\nrcd->seq_cnt = 1;\r\n}\r\nreturn 0;\r\n}\r\nstruct qib_ctxtdata *qib_create_ctxtdata(struct qib_pportdata *ppd, u32 ctxt,\r\nint node_id)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nstruct qib_ctxtdata *rcd;\r\nrcd = kzalloc_node(sizeof(*rcd), GFP_KERNEL, node_id);\r\nif (rcd) {\r\nINIT_LIST_HEAD(&rcd->qp_wait_list);\r\nrcd->node_id = node_id;\r\nrcd->ppd = ppd;\r\nrcd->dd = dd;\r\nrcd->cnt = 1;\r\nrcd->ctxt = ctxt;\r\ndd->rcd[ctxt] = rcd;\r\n#ifdef CONFIG_DEBUG_FS\r\nif (ctxt < dd->first_user_ctxt) {\r\nrcd->opstats = kzalloc_node(sizeof(*rcd->opstats),\r\nGFP_KERNEL, node_id);\r\nif (!rcd->opstats) {\r\nkfree(rcd);\r\nqib_dev_err(dd,\r\n"Unable to allocate per ctxt stats buffer\n");\r\nreturn NULL;\r\n}\r\n}\r\n#endif\r\ndd->f_init_ctxt(rcd);\r\nrcd->rcvegrbuf_size = 0x8000;\r\nrcd->rcvegrbufs_perchunk =\r\nrcd->rcvegrbuf_size / dd->rcvegrbufsize;\r\nrcd->rcvegrbuf_chunks = (rcd->rcvegrcnt +\r\nrcd->rcvegrbufs_perchunk - 1) /\r\nrcd->rcvegrbufs_perchunk;\r\nBUG_ON(!is_power_of_2(rcd->rcvegrbufs_perchunk));\r\nrcd->rcvegrbufs_perchunk_shift =\r\nilog2(rcd->rcvegrbufs_perchunk);\r\n}\r\nreturn rcd;\r\n}\r\nint qib_init_pportdata(struct qib_pportdata *ppd, struct qib_devdata *dd,\r\nu8 hw_pidx, u8 port)\r\n{\r\nint size;\r\nppd->dd = dd;\r\nppd->hw_pidx = hw_pidx;\r\nppd->port = port;\r\nspin_lock_init(&ppd->sdma_lock);\r\nspin_lock_init(&ppd->lflags_lock);\r\nspin_lock_init(&ppd->cc_shadow_lock);\r\ninit_waitqueue_head(&ppd->state_wait);\r\ninit_timer(&ppd->symerr_clear_timer);\r\nppd->symerr_clear_timer.function = qib_clear_symerror_on_linkup;\r\nppd->symerr_clear_timer.data = (unsigned long)ppd;\r\nppd->qib_wq = NULL;\r\nppd->ibport_data.pmastats =\r\nalloc_percpu(struct qib_pma_counters);\r\nif (!ppd->ibport_data.pmastats)\r\nreturn -ENOMEM;\r\nppd->ibport_data.rvp.rc_acks = alloc_percpu(u64);\r\nppd->ibport_data.rvp.rc_qacks = alloc_percpu(u64);\r\nppd->ibport_data.rvp.rc_delayed_comp = alloc_percpu(u64);\r\nif (!(ppd->ibport_data.rvp.rc_acks) ||\r\n!(ppd->ibport_data.rvp.rc_qacks) ||\r\n!(ppd->ibport_data.rvp.rc_delayed_comp))\r\nreturn -ENOMEM;\r\nif (qib_cc_table_size < IB_CCT_MIN_ENTRIES)\r\ngoto bail;\r\nppd->cc_supported_table_entries = min(max_t(int, qib_cc_table_size,\r\nIB_CCT_MIN_ENTRIES), IB_CCT_ENTRIES*IB_CC_TABLE_CAP_DEFAULT);\r\nppd->cc_max_table_entries =\r\nppd->cc_supported_table_entries/IB_CCT_ENTRIES;\r\nsize = IB_CC_TABLE_CAP_DEFAULT * sizeof(struct ib_cc_table_entry)\r\n* IB_CCT_ENTRIES;\r\nppd->ccti_entries = kzalloc(size, GFP_KERNEL);\r\nif (!ppd->ccti_entries)\r\ngoto bail;\r\nsize = IB_CC_CCS_ENTRIES * sizeof(struct ib_cc_congestion_entry);\r\nppd->congestion_entries = kzalloc(size, GFP_KERNEL);\r\nif (!ppd->congestion_entries)\r\ngoto bail_1;\r\nsize = sizeof(struct cc_table_shadow);\r\nppd->ccti_entries_shadow = kzalloc(size, GFP_KERNEL);\r\nif (!ppd->ccti_entries_shadow)\r\ngoto bail_2;\r\nsize = sizeof(struct ib_cc_congestion_setting_attr);\r\nppd->congestion_entries_shadow = kzalloc(size, GFP_KERNEL);\r\nif (!ppd->congestion_entries_shadow)\r\ngoto bail_3;\r\nreturn 0;\r\nbail_3:\r\nkfree(ppd->ccti_entries_shadow);\r\nppd->ccti_entries_shadow = NULL;\r\nbail_2:\r\nkfree(ppd->congestion_entries);\r\nppd->congestion_entries = NULL;\r\nbail_1:\r\nkfree(ppd->ccti_entries);\r\nppd->ccti_entries = NULL;\r\nbail:\r\nif (!qib_cc_table_size)\r\nreturn 0;\r\nif (qib_cc_table_size < IB_CCT_MIN_ENTRIES) {\r\nqib_cc_table_size = 0;\r\nqib_dev_err(dd,\r\n"Congestion Control table size %d less than minimum %d for port %d\n",\r\nqib_cc_table_size, IB_CCT_MIN_ENTRIES, port);\r\n}\r\nqib_dev_err(dd, "Congestion Control Agent disabled for port %d\n",\r\nport);\r\nreturn 0;\r\n}\r\nstatic int init_pioavailregs(struct qib_devdata *dd)\r\n{\r\nint ret, pidx;\r\nu64 *status_page;\r\ndd->pioavailregs_dma = dma_alloc_coherent(\r\n&dd->pcidev->dev, PAGE_SIZE, &dd->pioavailregs_phys,\r\nGFP_KERNEL);\r\nif (!dd->pioavailregs_dma) {\r\nqib_dev_err(dd,\r\n"failed to allocate PIOavail reg area in memory\n");\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\nstatus_page = (u64 *)\r\n((char *) dd->pioavailregs_dma +\r\n((2 * L1_CACHE_BYTES +\r\ndd->pioavregs * sizeof(u64)) & ~L1_CACHE_BYTES));\r\ndd->devstatusp = status_page;\r\n*status_page++ = 0;\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\ndd->pport[pidx].statusp = status_page;\r\n*status_page++ = 0;\r\n}\r\ndd->freezemsg = (char *) status_page;\r\n*dd->freezemsg = 0;\r\nret = (char *) status_page - (char *) dd->pioavailregs_dma;\r\ndd->freezelen = PAGE_SIZE - ret;\r\nret = 0;\r\ndone:\r\nreturn ret;\r\n}\r\nstatic void init_shadow_tids(struct qib_devdata *dd)\r\n{\r\nstruct page **pages;\r\ndma_addr_t *addrs;\r\npages = vzalloc(dd->cfgctxts * dd->rcvtidcnt * sizeof(struct page *));\r\nif (!pages)\r\ngoto bail;\r\naddrs = vzalloc(dd->cfgctxts * dd->rcvtidcnt * sizeof(dma_addr_t));\r\nif (!addrs)\r\ngoto bail_free;\r\ndd->pageshadow = pages;\r\ndd->physshadow = addrs;\r\nreturn;\r\nbail_free:\r\nvfree(pages);\r\nbail:\r\ndd->pageshadow = NULL;\r\n}\r\nstatic int loadtime_init(struct qib_devdata *dd)\r\n{\r\nint ret = 0;\r\nif (((dd->revision >> QLOGIC_IB_R_SOFTWARE_SHIFT) &\r\nQLOGIC_IB_R_SOFTWARE_MASK) != QIB_CHIP_SWVERSION) {\r\nqib_dev_err(dd,\r\n"Driver only handles version %d, chip swversion is %d (%llx), failng\n",\r\nQIB_CHIP_SWVERSION,\r\n(int)(dd->revision >>\r\nQLOGIC_IB_R_SOFTWARE_SHIFT) &\r\nQLOGIC_IB_R_SOFTWARE_MASK,\r\n(unsigned long long) dd->revision);\r\nret = -ENOSYS;\r\ngoto done;\r\n}\r\nif (dd->revision & QLOGIC_IB_R_EMULATOR_MASK)\r\nqib_devinfo(dd->pcidev, "%s", dd->boardversion);\r\nspin_lock_init(&dd->pioavail_lock);\r\nspin_lock_init(&dd->sendctrl_lock);\r\nspin_lock_init(&dd->uctxt_lock);\r\nspin_lock_init(&dd->qib_diag_trans_lock);\r\nspin_lock_init(&dd->eep_st_lock);\r\nmutex_init(&dd->eep_lock);\r\nif (qib_mini_init)\r\ngoto done;\r\nret = init_pioavailregs(dd);\r\ninit_shadow_tids(dd);\r\nqib_get_eeprom_info(dd);\r\ninit_timer(&dd->intrchk_timer);\r\ndd->intrchk_timer.function = verify_interrupt;\r\ndd->intrchk_timer.data = (unsigned long) dd;\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int init_after_reset(struct qib_devdata *dd)\r\n{\r\nint i;\r\nfor (i = 0; i < dd->num_pports; ++i) {\r\ndd->f_rcvctrl(dd->pport + i, QIB_RCVCTRL_CTXT_DIS |\r\nQIB_RCVCTRL_INTRAVAIL_DIS |\r\nQIB_RCVCTRL_TAILUPD_DIS, -1);\r\ndd->f_sendctrl(dd->pport + i, QIB_SENDCTRL_SEND_DIS |\r\nQIB_SENDCTRL_AVAIL_DIS);\r\n}\r\nreturn 0;\r\n}\r\nstatic void enable_chip(struct qib_devdata *dd)\r\n{\r\nu64 rcvmask;\r\nint i;\r\nfor (i = 0; i < dd->num_pports; ++i)\r\ndd->f_sendctrl(dd->pport + i, QIB_SENDCTRL_SEND_ENB |\r\nQIB_SENDCTRL_AVAIL_ENB);\r\nrcvmask = QIB_RCVCTRL_CTXT_ENB | QIB_RCVCTRL_INTRAVAIL_ENB;\r\nrcvmask |= (dd->flags & QIB_NODMA_RTAIL) ?\r\nQIB_RCVCTRL_TAILUPD_DIS : QIB_RCVCTRL_TAILUPD_ENB;\r\nfor (i = 0; dd->rcd && i < dd->first_user_ctxt; ++i) {\r\nstruct qib_ctxtdata *rcd = dd->rcd[i];\r\nif (rcd)\r\ndd->f_rcvctrl(rcd->ppd, rcvmask, i);\r\n}\r\n}\r\nstatic void verify_interrupt(unsigned long opaque)\r\n{\r\nstruct qib_devdata *dd = (struct qib_devdata *) opaque;\r\nu64 int_counter;\r\nif (!dd)\r\nreturn;\r\nint_counter = qib_int_counter(dd) - dd->z_int_counter;\r\nif (int_counter == 0) {\r\nif (!dd->f_intr_fallback(dd))\r\ndev_err(&dd->pcidev->dev,\r\n"No interrupts detected, not usable.\n");\r\nelse\r\nmod_timer(&dd->intrchk_timer, jiffies + HZ/2);\r\n}\r\n}\r\nstatic void init_piobuf_state(struct qib_devdata *dd)\r\n{\r\nint i, pidx;\r\nu32 uctxts;\r\ndd->f_sendctrl(dd->pport, QIB_SENDCTRL_DISARM_ALL);\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx)\r\ndd->f_sendctrl(dd->pport + pidx, QIB_SENDCTRL_FLUSH);\r\nuctxts = dd->cfgctxts - dd->first_user_ctxt;\r\ndd->ctxts_extrabuf = dd->pbufsctxt ?\r\ndd->lastctxt_piobuf - (dd->pbufsctxt * uctxts) : 0;\r\nfor (i = 0; i < dd->pioavregs; i++) {\r\n__le64 tmp;\r\ntmp = dd->pioavailregs_dma[i];\r\ndd->pioavailshadow[i] = le64_to_cpu(tmp);\r\n}\r\nwhile (i < ARRAY_SIZE(dd->pioavailshadow))\r\ndd->pioavailshadow[i++] = 0;\r\nqib_chg_pioavailkernel(dd, 0, dd->piobcnt2k + dd->piobcnt4k,\r\nTXCHK_CHG_TYPE_KERN, NULL);\r\ndd->f_initvl15_bufs(dd);\r\n}\r\nstatic int qib_create_workqueues(struct qib_devdata *dd)\r\n{\r\nint pidx;\r\nstruct qib_pportdata *ppd;\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\nif (!ppd->qib_wq) {\r\nchar wq_name[8];\r\nsnprintf(wq_name, sizeof(wq_name), "qib%d_%d",\r\ndd->unit, pidx);\r\nppd->qib_wq = alloc_ordered_workqueue(wq_name,\r\nWQ_MEM_RECLAIM);\r\nif (!ppd->qib_wq)\r\ngoto wq_error;\r\n}\r\n}\r\nreturn 0;\r\nwq_error:\r\npr_err("create_singlethread_workqueue failed for port %d\n",\r\npidx + 1);\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\nif (ppd->qib_wq) {\r\ndestroy_workqueue(ppd->qib_wq);\r\nppd->qib_wq = NULL;\r\n}\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void qib_free_pportdata(struct qib_pportdata *ppd)\r\n{\r\nfree_percpu(ppd->ibport_data.pmastats);\r\nfree_percpu(ppd->ibport_data.rvp.rc_acks);\r\nfree_percpu(ppd->ibport_data.rvp.rc_qacks);\r\nfree_percpu(ppd->ibport_data.rvp.rc_delayed_comp);\r\nppd->ibport_data.pmastats = NULL;\r\n}\r\nint qib_init(struct qib_devdata *dd, int reinit)\r\n{\r\nint ret = 0, pidx, lastfail = 0;\r\nu32 portok = 0;\r\nunsigned i;\r\nstruct qib_ctxtdata *rcd;\r\nstruct qib_pportdata *ppd;\r\nunsigned long flags;\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\nspin_lock_irqsave(&ppd->lflags_lock, flags);\r\nppd->lflags &= ~(QIBL_LINKACTIVE | QIBL_LINKARMED |\r\nQIBL_LINKDOWN | QIBL_LINKINIT |\r\nQIBL_LINKV);\r\nspin_unlock_irqrestore(&ppd->lflags_lock, flags);\r\n}\r\nif (reinit)\r\nret = init_after_reset(dd);\r\nelse\r\nret = loadtime_init(dd);\r\nif (ret)\r\ngoto done;\r\nif (qib_mini_init)\r\nreturn 0;\r\nret = dd->f_late_initreg(dd);\r\nif (ret)\r\ngoto done;\r\nfor (i = 0; dd->rcd && i < dd->first_user_ctxt; ++i) {\r\nrcd = dd->rcd[i];\r\nif (!rcd)\r\ncontinue;\r\nlastfail = qib_create_rcvhdrq(dd, rcd);\r\nif (!lastfail)\r\nlastfail = qib_setup_eagerbufs(rcd);\r\nif (lastfail) {\r\nqib_dev_err(dd,\r\n"failed to allocate kernel ctxt's rcvhdrq and/or egr bufs\n");\r\ncontinue;\r\n}\r\n}\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nint mtu;\r\nif (lastfail)\r\nret = lastfail;\r\nppd = dd->pport + pidx;\r\nmtu = ib_mtu_enum_to_int(qib_ibmtu);\r\nif (mtu == -1) {\r\nmtu = QIB_DEFAULT_MTU;\r\nqib_ibmtu = 0;\r\n}\r\nppd->init_ibmaxlen = min(mtu > 2048 ?\r\ndd->piosize4k : dd->piosize2k,\r\ndd->rcvegrbufsize +\r\n(dd->rcvhdrentsize << 2));\r\nppd->ibmaxlen = ppd->init_ibmaxlen;\r\nqib_set_mtu(ppd, mtu);\r\nspin_lock_irqsave(&ppd->lflags_lock, flags);\r\nppd->lflags |= QIBL_IB_LINK_DISABLED;\r\nspin_unlock_irqrestore(&ppd->lflags_lock, flags);\r\nlastfail = dd->f_bringup_serdes(ppd);\r\nif (lastfail) {\r\nqib_devinfo(dd->pcidev,\r\n"Failed to bringup IB port %u\n", ppd->port);\r\nlastfail = -ENETDOWN;\r\ncontinue;\r\n}\r\nportok++;\r\n}\r\nif (!portok) {\r\nif (!ret && lastfail)\r\nret = lastfail;\r\nelse if (!ret)\r\nret = -ENETDOWN;\r\n}\r\nenable_chip(dd);\r\ninit_piobuf_state(dd);\r\ndone:\r\nif (!ret) {\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\n*ppd->statusp |= QIB_STATUS_CHIP_PRESENT |\r\nQIB_STATUS_INITTED;\r\nif (!ppd->link_speed_enabled)\r\ncontinue;\r\nif (dd->flags & QIB_HAS_SEND_DMA)\r\nret = qib_setup_sdma(ppd);\r\ninit_timer(&ppd->hol_timer);\r\nppd->hol_timer.function = qib_hol_event;\r\nppd->hol_timer.data = (unsigned long)ppd;\r\nppd->hol_state = QIB_HOL_UP;\r\n}\r\ndd->f_set_intr_state(dd, 1);\r\nmod_timer(&dd->intrchk_timer, jiffies + HZ/2);\r\nmod_timer(&dd->stats_timer, jiffies + HZ * ACTIVITY_TIMER);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline struct qib_devdata *__qib_lookup(int unit)\r\n{\r\nreturn idr_find(&qib_unit_table, unit);\r\n}\r\nstruct qib_devdata *qib_lookup(int unit)\r\n{\r\nstruct qib_devdata *dd;\r\nunsigned long flags;\r\nspin_lock_irqsave(&qib_devs_lock, flags);\r\ndd = __qib_lookup(unit);\r\nspin_unlock_irqrestore(&qib_devs_lock, flags);\r\nreturn dd;\r\n}\r\nstatic void qib_stop_timers(struct qib_devdata *dd)\r\n{\r\nstruct qib_pportdata *ppd;\r\nint pidx;\r\nif (dd->stats_timer.data) {\r\ndel_timer_sync(&dd->stats_timer);\r\ndd->stats_timer.data = 0;\r\n}\r\nif (dd->intrchk_timer.data) {\r\ndel_timer_sync(&dd->intrchk_timer);\r\ndd->intrchk_timer.data = 0;\r\n}\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\nif (ppd->hol_timer.data)\r\ndel_timer_sync(&ppd->hol_timer);\r\nif (ppd->led_override_timer.data) {\r\ndel_timer_sync(&ppd->led_override_timer);\r\natomic_set(&ppd->led_override_timer_active, 0);\r\n}\r\nif (ppd->symerr_clear_timer.data)\r\ndel_timer_sync(&ppd->symerr_clear_timer);\r\n}\r\n}\r\nstatic void qib_shutdown_device(struct qib_devdata *dd)\r\n{\r\nstruct qib_pportdata *ppd;\r\nunsigned pidx;\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\nspin_lock_irq(&ppd->lflags_lock);\r\nppd->lflags &= ~(QIBL_LINKDOWN | QIBL_LINKINIT |\r\nQIBL_LINKARMED | QIBL_LINKACTIVE |\r\nQIBL_LINKV);\r\nspin_unlock_irq(&ppd->lflags_lock);\r\n*ppd->statusp &= ~(QIB_STATUS_IB_CONF | QIB_STATUS_IB_READY);\r\n}\r\ndd->flags &= ~QIB_INITTED;\r\ndd->f_set_intr_state(dd, 0);\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\ndd->f_rcvctrl(ppd, QIB_RCVCTRL_TAILUPD_DIS |\r\nQIB_RCVCTRL_CTXT_DIS |\r\nQIB_RCVCTRL_INTRAVAIL_DIS |\r\nQIB_RCVCTRL_PKEY_ENB, -1);\r\ndd->f_sendctrl(ppd, QIB_SENDCTRL_CLEAR);\r\n}\r\nudelay(20);\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nppd = dd->pport + pidx;\r\ndd->f_setextled(ppd, 0);\r\nif (dd->flags & QIB_HAS_SEND_DMA)\r\nqib_teardown_sdma(ppd);\r\ndd->f_sendctrl(ppd, QIB_SENDCTRL_AVAIL_DIS |\r\nQIB_SENDCTRL_SEND_DIS);\r\ndd->f_quiet_serdes(ppd);\r\nif (ppd->qib_wq) {\r\ndestroy_workqueue(ppd->qib_wq);\r\nppd->qib_wq = NULL;\r\n}\r\nqib_free_pportdata(ppd);\r\n}\r\n}\r\nvoid qib_free_ctxtdata(struct qib_devdata *dd, struct qib_ctxtdata *rcd)\r\n{\r\nif (!rcd)\r\nreturn;\r\nif (rcd->rcvhdrq) {\r\ndma_free_coherent(&dd->pcidev->dev, rcd->rcvhdrq_size,\r\nrcd->rcvhdrq, rcd->rcvhdrq_phys);\r\nrcd->rcvhdrq = NULL;\r\nif (rcd->rcvhdrtail_kvaddr) {\r\ndma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\r\nrcd->rcvhdrtail_kvaddr,\r\nrcd->rcvhdrqtailaddr_phys);\r\nrcd->rcvhdrtail_kvaddr = NULL;\r\n}\r\n}\r\nif (rcd->rcvegrbuf) {\r\nunsigned e;\r\nfor (e = 0; e < rcd->rcvegrbuf_chunks; e++) {\r\nvoid *base = rcd->rcvegrbuf[e];\r\nsize_t size = rcd->rcvegrbuf_size;\r\ndma_free_coherent(&dd->pcidev->dev, size,\r\nbase, rcd->rcvegrbuf_phys[e]);\r\n}\r\nkfree(rcd->rcvegrbuf);\r\nrcd->rcvegrbuf = NULL;\r\nkfree(rcd->rcvegrbuf_phys);\r\nrcd->rcvegrbuf_phys = NULL;\r\nrcd->rcvegrbuf_chunks = 0;\r\n}\r\nkfree(rcd->tid_pg_list);\r\nvfree(rcd->user_event_mask);\r\nvfree(rcd->subctxt_uregbase);\r\nvfree(rcd->subctxt_rcvegrbuf);\r\nvfree(rcd->subctxt_rcvhdr_base);\r\n#ifdef CONFIG_DEBUG_FS\r\nkfree(rcd->opstats);\r\nrcd->opstats = NULL;\r\n#endif\r\nkfree(rcd);\r\n}\r\nstatic void qib_verify_pioperf(struct qib_devdata *dd)\r\n{\r\nu32 pbnum, cnt, lcnt;\r\nu32 __iomem *piobuf;\r\nu32 *addr;\r\nu64 msecs, emsecs;\r\npiobuf = dd->f_getsendbuf(dd->pport, 0ULL, &pbnum);\r\nif (!piobuf) {\r\nqib_devinfo(dd->pcidev,\r\n"No PIObufs for checking perf, skipping\n");\r\nreturn;\r\n}\r\ncnt = 1024;\r\naddr = vmalloc(cnt);\r\nif (!addr)\r\ngoto done;\r\npreempt_disable();\r\nmsecs = 1 + jiffies_to_msecs(jiffies);\r\nfor (lcnt = 0; lcnt < 10000U; lcnt++) {\r\nif (jiffies_to_msecs(jiffies) >= msecs)\r\nbreak;\r\nudelay(1);\r\n}\r\ndd->f_set_armlaunch(dd, 0);\r\nwriteq(0, piobuf);\r\nqib_flush_wc();\r\nmsecs = jiffies_to_msecs(jiffies);\r\nfor (emsecs = lcnt = 0; emsecs <= 5UL; lcnt++) {\r\nqib_pio_copy(piobuf + 64, addr, cnt >> 2);\r\nemsecs = jiffies_to_msecs(jiffies) - msecs;\r\n}\r\nif (lcnt < (emsecs * 1024U))\r\nqib_dev_err(dd,\r\n"Performance problem: bandwidth to PIO buffers is only %u MiB/sec\n",\r\nlcnt / (u32) emsecs);\r\npreempt_enable();\r\nvfree(addr);\r\ndone:\r\ndd->f_sendctrl(dd->pport, QIB_SENDCTRL_DISARM_BUF(pbnum));\r\nqib_sendbuf_done(dd, pbnum);\r\ndd->f_set_armlaunch(dd, 1);\r\n}\r\nvoid qib_free_devdata(struct qib_devdata *dd)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&qib_devs_lock, flags);\r\nidr_remove(&qib_unit_table, dd->unit);\r\nlist_del(&dd->list);\r\nspin_unlock_irqrestore(&qib_devs_lock, flags);\r\n#ifdef CONFIG_DEBUG_FS\r\nqib_dbg_ibdev_exit(&dd->verbs_dev);\r\n#endif\r\nfree_percpu(dd->int_counter);\r\nrvt_dealloc_device(&dd->verbs_dev.rdi);\r\n}\r\nu64 qib_int_counter(struct qib_devdata *dd)\r\n{\r\nint cpu;\r\nu64 int_counter = 0;\r\nfor_each_possible_cpu(cpu)\r\nint_counter += *per_cpu_ptr(dd->int_counter, cpu);\r\nreturn int_counter;\r\n}\r\nu64 qib_sps_ints(void)\r\n{\r\nunsigned long flags;\r\nstruct qib_devdata *dd;\r\nu64 sps_ints = 0;\r\nspin_lock_irqsave(&qib_devs_lock, flags);\r\nlist_for_each_entry(dd, &qib_dev_list, list) {\r\nsps_ints += qib_int_counter(dd);\r\n}\r\nspin_unlock_irqrestore(&qib_devs_lock, flags);\r\nreturn sps_ints;\r\n}\r\nstruct qib_devdata *qib_alloc_devdata(struct pci_dev *pdev, size_t extra)\r\n{\r\nunsigned long flags;\r\nstruct qib_devdata *dd;\r\nint ret, nports;\r\nnports = extra / sizeof(struct qib_pportdata);\r\ndd = (struct qib_devdata *)rvt_alloc_device(sizeof(*dd) + extra,\r\nnports);\r\nif (!dd)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_LIST_HEAD(&dd->list);\r\nidr_preload(GFP_KERNEL);\r\nspin_lock_irqsave(&qib_devs_lock, flags);\r\nret = idr_alloc(&qib_unit_table, dd, 0, 0, GFP_NOWAIT);\r\nif (ret >= 0) {\r\ndd->unit = ret;\r\nlist_add(&dd->list, &qib_dev_list);\r\n}\r\nspin_unlock_irqrestore(&qib_devs_lock, flags);\r\nidr_preload_end();\r\nif (ret < 0) {\r\nqib_early_err(&pdev->dev,\r\n"Could not allocate unit ID: error %d\n", -ret);\r\ngoto bail;\r\n}\r\ndd->int_counter = alloc_percpu(u64);\r\nif (!dd->int_counter) {\r\nret = -ENOMEM;\r\nqib_early_err(&pdev->dev,\r\n"Could not allocate per-cpu int_counter\n");\r\ngoto bail;\r\n}\r\nif (!qib_cpulist_count) {\r\nu32 count = num_online_cpus();\r\nqib_cpulist = kzalloc(BITS_TO_LONGS(count) *\r\nsizeof(long), GFP_KERNEL);\r\nif (qib_cpulist)\r\nqib_cpulist_count = count;\r\n}\r\n#ifdef CONFIG_DEBUG_FS\r\nqib_dbg_ibdev_init(&dd->verbs_dev);\r\n#endif\r\nreturn dd;\r\nbail:\r\nif (!list_empty(&dd->list))\r\nlist_del_init(&dd->list);\r\nrvt_dealloc_device(&dd->verbs_dev.rdi);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid qib_disable_after_error(struct qib_devdata *dd)\r\n{\r\nif (dd->flags & QIB_INITTED) {\r\nu32 pidx;\r\ndd->flags &= ~QIB_INITTED;\r\nif (dd->pport)\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nstruct qib_pportdata *ppd;\r\nppd = dd->pport + pidx;\r\nif (dd->flags & QIB_PRESENT) {\r\nqib_set_linkstate(ppd,\r\nQIB_IB_LINKDOWN_DISABLE);\r\ndd->f_setextled(ppd, 0);\r\n}\r\n*ppd->statusp &= ~QIB_STATUS_IB_READY;\r\n}\r\n}\r\nif (dd->devstatusp)\r\n*dd->devstatusp |= QIB_STATUS_HWERROR;\r\n}\r\nstatic int qib_notify_dca_device(struct device *device, void *data)\r\n{\r\nstruct qib_devdata *dd = dev_get_drvdata(device);\r\nunsigned long event = *(unsigned long *)data;\r\nreturn dd->f_notify_dca(dd, event);\r\n}\r\nstatic int qib_notify_dca(struct notifier_block *nb, unsigned long event,\r\nvoid *p)\r\n{\r\nint rval;\r\nrval = driver_for_each_device(&qib_driver.driver, NULL,\r\n&event, qib_notify_dca_device);\r\nreturn rval ? NOTIFY_BAD : NOTIFY_DONE;\r\n}\r\nstatic int __init qib_ib_init(void)\r\n{\r\nint ret;\r\nret = qib_dev_init();\r\nif (ret)\r\ngoto bail;\r\nidr_init(&qib_unit_table);\r\n#ifdef CONFIG_INFINIBAND_QIB_DCA\r\ndca_register_notify(&dca_notifier);\r\n#endif\r\n#ifdef CONFIG_DEBUG_FS\r\nqib_dbg_init();\r\n#endif\r\nret = pci_register_driver(&qib_driver);\r\nif (ret < 0) {\r\npr_err("Unable to register driver: error %d\n", -ret);\r\ngoto bail_dev;\r\n}\r\nif (qib_init_qibfs())\r\npr_err("Unable to register ipathfs\n");\r\ngoto bail;\r\nbail_dev:\r\n#ifdef CONFIG_INFINIBAND_QIB_DCA\r\ndca_unregister_notify(&dca_notifier);\r\n#endif\r\n#ifdef CONFIG_DEBUG_FS\r\nqib_dbg_exit();\r\n#endif\r\nidr_destroy(&qib_unit_table);\r\nqib_dev_cleanup();\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void __exit qib_ib_cleanup(void)\r\n{\r\nint ret;\r\nret = qib_exit_qibfs();\r\nif (ret)\r\npr_err(\r\n"Unable to cleanup counter filesystem: error %d\n",\r\n-ret);\r\n#ifdef CONFIG_INFINIBAND_QIB_DCA\r\ndca_unregister_notify(&dca_notifier);\r\n#endif\r\npci_unregister_driver(&qib_driver);\r\n#ifdef CONFIG_DEBUG_FS\r\nqib_dbg_exit();\r\n#endif\r\nqib_cpulist_count = 0;\r\nkfree(qib_cpulist);\r\nidr_destroy(&qib_unit_table);\r\nqib_dev_cleanup();\r\n}\r\nstatic void cleanup_device_data(struct qib_devdata *dd)\r\n{\r\nint ctxt;\r\nint pidx;\r\nstruct qib_ctxtdata **tmp;\r\nunsigned long flags;\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx) {\r\nif (dd->pport[pidx].statusp)\r\n*dd->pport[pidx].statusp &= ~QIB_STATUS_CHIP_PRESENT;\r\nspin_lock(&dd->pport[pidx].cc_shadow_lock);\r\nkfree(dd->pport[pidx].congestion_entries);\r\ndd->pport[pidx].congestion_entries = NULL;\r\nkfree(dd->pport[pidx].ccti_entries);\r\ndd->pport[pidx].ccti_entries = NULL;\r\nkfree(dd->pport[pidx].ccti_entries_shadow);\r\ndd->pport[pidx].ccti_entries_shadow = NULL;\r\nkfree(dd->pport[pidx].congestion_entries_shadow);\r\ndd->pport[pidx].congestion_entries_shadow = NULL;\r\nspin_unlock(&dd->pport[pidx].cc_shadow_lock);\r\n}\r\nqib_disable_wc(dd);\r\nif (dd->pioavailregs_dma) {\r\ndma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\r\n(void *) dd->pioavailregs_dma,\r\ndd->pioavailregs_phys);\r\ndd->pioavailregs_dma = NULL;\r\n}\r\nif (dd->pageshadow) {\r\nstruct page **tmpp = dd->pageshadow;\r\ndma_addr_t *tmpd = dd->physshadow;\r\nint i;\r\nfor (ctxt = 0; ctxt < dd->cfgctxts; ctxt++) {\r\nint ctxt_tidbase = ctxt * dd->rcvtidcnt;\r\nint maxtid = ctxt_tidbase + dd->rcvtidcnt;\r\nfor (i = ctxt_tidbase; i < maxtid; i++) {\r\nif (!tmpp[i])\r\ncontinue;\r\npci_unmap_page(dd->pcidev, tmpd[i],\r\nPAGE_SIZE, PCI_DMA_FROMDEVICE);\r\nqib_release_user_pages(&tmpp[i], 1);\r\ntmpp[i] = NULL;\r\n}\r\n}\r\ndd->pageshadow = NULL;\r\nvfree(tmpp);\r\ndd->physshadow = NULL;\r\nvfree(tmpd);\r\n}\r\nspin_lock_irqsave(&dd->uctxt_lock, flags);\r\ntmp = dd->rcd;\r\ndd->rcd = NULL;\r\nspin_unlock_irqrestore(&dd->uctxt_lock, flags);\r\nfor (ctxt = 0; tmp && ctxt < dd->ctxtcnt; ctxt++) {\r\nstruct qib_ctxtdata *rcd = tmp[ctxt];\r\ntmp[ctxt] = NULL;\r\nqib_free_ctxtdata(dd, rcd);\r\n}\r\nkfree(tmp);\r\nkfree(dd->boardname);\r\n}\r\nstatic void qib_postinit_cleanup(struct qib_devdata *dd)\r\n{\r\nif (dd->f_cleanup)\r\ndd->f_cleanup(dd);\r\nqib_pcie_ddcleanup(dd);\r\ncleanup_device_data(dd);\r\nqib_free_devdata(dd);\r\n}\r\nstatic int qib_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nint ret, j, pidx, initfail;\r\nstruct qib_devdata *dd = NULL;\r\nret = qib_pcie_init(pdev, ent);\r\nif (ret)\r\ngoto bail;\r\nswitch (ent->device) {\r\ncase PCI_DEVICE_ID_QLOGIC_IB_6120:\r\n#ifdef CONFIG_PCI_MSI\r\ndd = qib_init_iba6120_funcs(pdev, ent);\r\n#else\r\nqib_early_err(&pdev->dev,\r\n"Intel PCIE device 0x%x cannot work if CONFIG_PCI_MSI is not enabled\n",\r\nent->device);\r\ndd = ERR_PTR(-ENODEV);\r\n#endif\r\nbreak;\r\ncase PCI_DEVICE_ID_QLOGIC_IB_7220:\r\ndd = qib_init_iba7220_funcs(pdev, ent);\r\nbreak;\r\ncase PCI_DEVICE_ID_QLOGIC_IB_7322:\r\ndd = qib_init_iba7322_funcs(pdev, ent);\r\nbreak;\r\ndefault:\r\nqib_early_err(&pdev->dev,\r\n"Failing on unknown Intel deviceid 0x%x\n",\r\nent->device);\r\nret = -ENODEV;\r\n}\r\nif (IS_ERR(dd))\r\nret = PTR_ERR(dd);\r\nif (ret)\r\ngoto bail;\r\nret = qib_create_workqueues(dd);\r\nif (ret)\r\ngoto bail;\r\ninitfail = qib_init(dd, 0);\r\nret = qib_register_ib_device(dd);\r\nif (!qib_mini_init && !initfail && !ret)\r\ndd->flags |= QIB_INITTED;\r\nj = qib_device_create(dd);\r\nif (j)\r\nqib_dev_err(dd, "Failed to create /dev devices: %d\n", -j);\r\nj = qibfs_add(dd);\r\nif (j)\r\nqib_dev_err(dd, "Failed filesystem setup for counters: %d\n",\r\n-j);\r\nif (qib_mini_init || initfail || ret) {\r\nqib_stop_timers(dd);\r\nflush_workqueue(ib_wq);\r\nfor (pidx = 0; pidx < dd->num_pports; ++pidx)\r\ndd->f_quiet_serdes(dd->pport + pidx);\r\nif (qib_mini_init)\r\ngoto bail;\r\nif (!j) {\r\n(void) qibfs_remove(dd);\r\nqib_device_remove(dd);\r\n}\r\nif (!ret)\r\nqib_unregister_ib_device(dd);\r\nqib_postinit_cleanup(dd);\r\nif (initfail)\r\nret = initfail;\r\ngoto bail;\r\n}\r\nret = qib_enable_wc(dd);\r\nif (ret) {\r\nqib_dev_err(dd,\r\n"Write combining not enabled (err %d): performance may be poor\n",\r\n-ret);\r\nret = 0;\r\n}\r\nqib_verify_pioperf(dd);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void qib_remove_one(struct pci_dev *pdev)\r\n{\r\nstruct qib_devdata *dd = pci_get_drvdata(pdev);\r\nint ret;\r\nqib_unregister_ib_device(dd);\r\nif (!qib_mini_init)\r\nqib_shutdown_device(dd);\r\nqib_stop_timers(dd);\r\nflush_workqueue(ib_wq);\r\nret = qibfs_remove(dd);\r\nif (ret)\r\nqib_dev_err(dd, "Failed counters filesystem cleanup: %d\n",\r\n-ret);\r\nqib_device_remove(dd);\r\nqib_postinit_cleanup(dd);\r\n}\r\nint qib_create_rcvhdrq(struct qib_devdata *dd, struct qib_ctxtdata *rcd)\r\n{\r\nunsigned amt;\r\nint old_node_id;\r\nif (!rcd->rcvhdrq) {\r\ndma_addr_t phys_hdrqtail;\r\ngfp_t gfp_flags;\r\namt = ALIGN(dd->rcvhdrcnt * dd->rcvhdrentsize *\r\nsizeof(u32), PAGE_SIZE);\r\ngfp_flags = (rcd->ctxt >= dd->first_user_ctxt) ?\r\nGFP_USER : GFP_KERNEL;\r\nold_node_id = dev_to_node(&dd->pcidev->dev);\r\nset_dev_node(&dd->pcidev->dev, rcd->node_id);\r\nrcd->rcvhdrq = dma_alloc_coherent(\r\n&dd->pcidev->dev, amt, &rcd->rcvhdrq_phys,\r\ngfp_flags | __GFP_COMP);\r\nset_dev_node(&dd->pcidev->dev, old_node_id);\r\nif (!rcd->rcvhdrq) {\r\nqib_dev_err(dd,\r\n"attempt to allocate %d bytes for ctxt %u rcvhdrq failed\n",\r\namt, rcd->ctxt);\r\ngoto bail;\r\n}\r\nif (rcd->ctxt >= dd->first_user_ctxt) {\r\nrcd->user_event_mask = vmalloc_user(PAGE_SIZE);\r\nif (!rcd->user_event_mask)\r\ngoto bail_free_hdrq;\r\n}\r\nif (!(dd->flags & QIB_NODMA_RTAIL)) {\r\nset_dev_node(&dd->pcidev->dev, rcd->node_id);\r\nrcd->rcvhdrtail_kvaddr = dma_alloc_coherent(\r\n&dd->pcidev->dev, PAGE_SIZE, &phys_hdrqtail,\r\ngfp_flags);\r\nset_dev_node(&dd->pcidev->dev, old_node_id);\r\nif (!rcd->rcvhdrtail_kvaddr)\r\ngoto bail_free;\r\nrcd->rcvhdrqtailaddr_phys = phys_hdrqtail;\r\n}\r\nrcd->rcvhdrq_size = amt;\r\n}\r\nmemset(rcd->rcvhdrq, 0, rcd->rcvhdrq_size);\r\nif (rcd->rcvhdrtail_kvaddr)\r\nmemset(rcd->rcvhdrtail_kvaddr, 0, PAGE_SIZE);\r\nreturn 0;\r\nbail_free:\r\nqib_dev_err(dd,\r\n"attempt to allocate 1 page for ctxt %u rcvhdrqtailaddr failed\n",\r\nrcd->ctxt);\r\nvfree(rcd->user_event_mask);\r\nrcd->user_event_mask = NULL;\r\nbail_free_hdrq:\r\ndma_free_coherent(&dd->pcidev->dev, amt, rcd->rcvhdrq,\r\nrcd->rcvhdrq_phys);\r\nrcd->rcvhdrq = NULL;\r\nbail:\r\nreturn -ENOMEM;\r\n}\r\nint qib_setup_eagerbufs(struct qib_ctxtdata *rcd)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nunsigned e, egrcnt, egrperchunk, chunk, egrsize, egroff;\r\nsize_t size;\r\ngfp_t gfp_flags;\r\nint old_node_id;\r\ngfp_flags = __GFP_RECLAIM | __GFP_IO | __GFP_COMP;\r\negrcnt = rcd->rcvegrcnt;\r\negroff = rcd->rcvegr_tid_base;\r\negrsize = dd->rcvegrbufsize;\r\nchunk = rcd->rcvegrbuf_chunks;\r\negrperchunk = rcd->rcvegrbufs_perchunk;\r\nsize = rcd->rcvegrbuf_size;\r\nif (!rcd->rcvegrbuf) {\r\nrcd->rcvegrbuf =\r\nkzalloc_node(chunk * sizeof(rcd->rcvegrbuf[0]),\r\nGFP_KERNEL, rcd->node_id);\r\nif (!rcd->rcvegrbuf)\r\ngoto bail;\r\n}\r\nif (!rcd->rcvegrbuf_phys) {\r\nrcd->rcvegrbuf_phys =\r\nkmalloc_node(chunk * sizeof(rcd->rcvegrbuf_phys[0]),\r\nGFP_KERNEL, rcd->node_id);\r\nif (!rcd->rcvegrbuf_phys)\r\ngoto bail_rcvegrbuf;\r\n}\r\nfor (e = 0; e < rcd->rcvegrbuf_chunks; e++) {\r\nif (rcd->rcvegrbuf[e])\r\ncontinue;\r\nold_node_id = dev_to_node(&dd->pcidev->dev);\r\nset_dev_node(&dd->pcidev->dev, rcd->node_id);\r\nrcd->rcvegrbuf[e] =\r\ndma_alloc_coherent(&dd->pcidev->dev, size,\r\n&rcd->rcvegrbuf_phys[e],\r\ngfp_flags);\r\nset_dev_node(&dd->pcidev->dev, old_node_id);\r\nif (!rcd->rcvegrbuf[e])\r\ngoto bail_rcvegrbuf_phys;\r\n}\r\nrcd->rcvegr_phys = rcd->rcvegrbuf_phys[0];\r\nfor (e = chunk = 0; chunk < rcd->rcvegrbuf_chunks; chunk++) {\r\ndma_addr_t pa = rcd->rcvegrbuf_phys[chunk];\r\nunsigned i;\r\nmemset(rcd->rcvegrbuf[chunk], 0, size);\r\nfor (i = 0; e < egrcnt && i < egrperchunk; e++, i++) {\r\ndd->f_put_tid(dd, e + egroff +\r\n(u64 __iomem *)\r\n((char __iomem *)\r\ndd->kregbase +\r\ndd->rcvegrbase),\r\nRCVHQ_RCV_TYPE_EAGER, pa);\r\npa += egrsize;\r\n}\r\ncond_resched();\r\n}\r\nreturn 0;\r\nbail_rcvegrbuf_phys:\r\nfor (e = 0; e < rcd->rcvegrbuf_chunks && rcd->rcvegrbuf[e]; e++)\r\ndma_free_coherent(&dd->pcidev->dev, size,\r\nrcd->rcvegrbuf[e], rcd->rcvegrbuf_phys[e]);\r\nkfree(rcd->rcvegrbuf_phys);\r\nrcd->rcvegrbuf_phys = NULL;\r\nbail_rcvegrbuf:\r\nkfree(rcd->rcvegrbuf);\r\nrcd->rcvegrbuf = NULL;\r\nbail:\r\nreturn -ENOMEM;\r\n}\r\nint init_chip_wc_pat(struct qib_devdata *dd, u32 vl15buflen)\r\n{\r\nu64 __iomem *qib_kregbase = NULL;\r\nvoid __iomem *qib_piobase = NULL;\r\nu64 __iomem *qib_userbase = NULL;\r\nu64 qib_kreglen;\r\nu64 qib_pio2koffset = dd->piobufbase & 0xffffffff;\r\nu64 qib_pio4koffset = dd->piobufbase >> 32;\r\nu64 qib_pio2klen = dd->piobcnt2k * dd->palign;\r\nu64 qib_pio4klen = dd->piobcnt4k * dd->align4k;\r\nu64 qib_physaddr = dd->physaddr;\r\nu64 qib_piolen;\r\nu64 qib_userlen = 0;\r\niounmap(dd->kregbase);\r\ndd->kregbase = NULL;\r\nif (dd->piobcnt4k == 0) {\r\nqib_kreglen = qib_pio2koffset;\r\nqib_piolen = qib_pio2klen;\r\n} else if (qib_pio2koffset < qib_pio4koffset) {\r\nqib_kreglen = qib_pio2koffset;\r\nqib_piolen = qib_pio4koffset + qib_pio4klen - qib_kreglen;\r\n} else {\r\nqib_kreglen = qib_pio4koffset;\r\nqib_piolen = qib_pio2koffset + qib_pio2klen - qib_kreglen;\r\n}\r\nqib_piolen += vl15buflen;\r\nif (dd->uregbase > qib_kreglen)\r\nqib_userlen = dd->ureg_align * dd->cfgctxts;\r\nqib_kregbase = ioremap_nocache(qib_physaddr, qib_kreglen);\r\nif (!qib_kregbase)\r\ngoto bail;\r\nqib_piobase = ioremap_wc(qib_physaddr + qib_kreglen, qib_piolen);\r\nif (!qib_piobase)\r\ngoto bail_kregbase;\r\nif (qib_userlen) {\r\nqib_userbase = ioremap_nocache(qib_physaddr + dd->uregbase,\r\nqib_userlen);\r\nif (!qib_userbase)\r\ngoto bail_piobase;\r\n}\r\ndd->kregbase = qib_kregbase;\r\ndd->kregend = (u64 __iomem *)\r\n((char __iomem *) qib_kregbase + qib_kreglen);\r\ndd->piobase = qib_piobase;\r\ndd->pio2kbase = (void __iomem *)\r\n(((char __iomem *) dd->piobase) +\r\nqib_pio2koffset - qib_kreglen);\r\nif (dd->piobcnt4k)\r\ndd->pio4kbase = (void __iomem *)\r\n(((char __iomem *) dd->piobase) +\r\nqib_pio4koffset - qib_kreglen);\r\nif (qib_userlen)\r\ndd->userbase = qib_userbase;\r\nreturn 0;\r\nbail_piobase:\r\niounmap(qib_piobase);\r\nbail_kregbase:\r\niounmap(qib_kregbase);\r\nbail:\r\nreturn -ENOMEM;\r\n}
