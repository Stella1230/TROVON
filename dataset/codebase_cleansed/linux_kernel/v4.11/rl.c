int mlx5_create_scheduling_element_cmd(struct mlx5_core_dev *dev, u8 hierarchy,\r\nvoid *ctx, u32 *element_id)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(create_scheduling_element_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(create_scheduling_element_in)] = {0};\r\nvoid *schedc;\r\nint err;\r\nschedc = MLX5_ADDR_OF(create_scheduling_element_in, in,\r\nscheduling_context);\r\nMLX5_SET(create_scheduling_element_in, in, opcode,\r\nMLX5_CMD_OP_CREATE_SCHEDULING_ELEMENT);\r\nMLX5_SET(create_scheduling_element_in, in, scheduling_hierarchy,\r\nhierarchy);\r\nmemcpy(schedc, ctx, MLX5_ST_SZ_BYTES(scheduling_context));\r\nerr = mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\nif (err)\r\nreturn err;\r\n*element_id = MLX5_GET(create_scheduling_element_out, out,\r\nscheduling_element_id);\r\nreturn 0;\r\n}\r\nint mlx5_modify_scheduling_element_cmd(struct mlx5_core_dev *dev, u8 hierarchy,\r\nvoid *ctx, u32 element_id,\r\nu32 modify_bitmask)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(modify_scheduling_element_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(modify_scheduling_element_in)] = {0};\r\nvoid *schedc;\r\nschedc = MLX5_ADDR_OF(modify_scheduling_element_in, in,\r\nscheduling_context);\r\nMLX5_SET(modify_scheduling_element_in, in, opcode,\r\nMLX5_CMD_OP_MODIFY_SCHEDULING_ELEMENT);\r\nMLX5_SET(modify_scheduling_element_in, in, scheduling_element_id,\r\nelement_id);\r\nMLX5_SET(modify_scheduling_element_in, in, modify_bitmask,\r\nmodify_bitmask);\r\nMLX5_SET(modify_scheduling_element_in, in, scheduling_hierarchy,\r\nhierarchy);\r\nmemcpy(schedc, ctx, MLX5_ST_SZ_BYTES(scheduling_context));\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nint mlx5_destroy_scheduling_element_cmd(struct mlx5_core_dev *dev, u8 hierarchy,\r\nu32 element_id)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(destroy_scheduling_element_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(destroy_scheduling_element_in)] = {0};\r\nMLX5_SET(destroy_scheduling_element_in, in, opcode,\r\nMLX5_CMD_OP_DESTROY_SCHEDULING_ELEMENT);\r\nMLX5_SET(destroy_scheduling_element_in, in, scheduling_element_id,\r\nelement_id);\r\nMLX5_SET(destroy_scheduling_element_in, in, scheduling_hierarchy,\r\nhierarchy);\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nstatic struct mlx5_rl_entry *find_rl_entry(struct mlx5_rl_table *table,\r\nu32 rate)\r\n{\r\nstruct mlx5_rl_entry *ret_entry = NULL;\r\nbool empty_found = false;\r\nint i;\r\nfor (i = 0; i < table->max_size; i++) {\r\nif (table->rl_entry[i].rate == rate)\r\nreturn &table->rl_entry[i];\r\nif (!empty_found && !table->rl_entry[i].rate) {\r\nempty_found = true;\r\nret_entry = &table->rl_entry[i];\r\n}\r\n}\r\nreturn ret_entry;\r\n}\r\nstatic int mlx5_set_rate_limit_cmd(struct mlx5_core_dev *dev,\r\nu32 rate, u16 index)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(set_rate_limit_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(set_rate_limit_out)] = {0};\r\nMLX5_SET(set_rate_limit_in, in, opcode,\r\nMLX5_CMD_OP_SET_RATE_LIMIT);\r\nMLX5_SET(set_rate_limit_in, in, rate_limit_index, index);\r\nMLX5_SET(set_rate_limit_in, in, rate_limit, rate);\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nbool mlx5_rl_is_in_range(struct mlx5_core_dev *dev, u32 rate)\r\n{\r\nstruct mlx5_rl_table *table = &dev->priv.rl_table;\r\nreturn (rate <= table->max_rate && rate >= table->min_rate);\r\n}\r\nint mlx5_rl_add_rate(struct mlx5_core_dev *dev, u32 rate, u16 *index)\r\n{\r\nstruct mlx5_rl_table *table = &dev->priv.rl_table;\r\nstruct mlx5_rl_entry *entry;\r\nint err = 0;\r\nmutex_lock(&table->rl_lock);\r\nif (!rate || !mlx5_rl_is_in_range(dev, rate)) {\r\nmlx5_core_err(dev, "Invalid rate: %u, should be %u to %u\n",\r\nrate, table->min_rate, table->max_rate);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nentry = find_rl_entry(table, rate);\r\nif (!entry) {\r\nmlx5_core_err(dev, "Max number of %u rates reached\n",\r\ntable->max_size);\r\nerr = -ENOSPC;\r\ngoto out;\r\n}\r\nif (entry->refcount) {\r\nentry->refcount++;\r\n} else {\r\nerr = mlx5_set_rate_limit_cmd(dev, rate, entry->index);\r\nif (err) {\r\nmlx5_core_err(dev, "Failed configuring rate: %u (%d)\n",\r\nrate, err);\r\ngoto out;\r\n}\r\nentry->rate = rate;\r\nentry->refcount = 1;\r\n}\r\n*index = entry->index;\r\nout:\r\nmutex_unlock(&table->rl_lock);\r\nreturn err;\r\n}\r\nvoid mlx5_rl_remove_rate(struct mlx5_core_dev *dev, u32 rate)\r\n{\r\nstruct mlx5_rl_table *table = &dev->priv.rl_table;\r\nstruct mlx5_rl_entry *entry = NULL;\r\nif (rate == 0)\r\nreturn;\r\nmutex_lock(&table->rl_lock);\r\nentry = find_rl_entry(table, rate);\r\nif (!entry || !entry->refcount) {\r\nmlx5_core_warn(dev, "Rate %u is not configured\n", rate);\r\ngoto out;\r\n}\r\nentry->refcount--;\r\nif (!entry->refcount) {\r\nmlx5_set_rate_limit_cmd(dev, 0, entry->index);\r\nentry->rate = 0;\r\n}\r\nout:\r\nmutex_unlock(&table->rl_lock);\r\n}\r\nint mlx5_init_rl_table(struct mlx5_core_dev *dev)\r\n{\r\nstruct mlx5_rl_table *table = &dev->priv.rl_table;\r\nint i;\r\nmutex_init(&table->rl_lock);\r\nif (!MLX5_CAP_GEN(dev, qos) || !MLX5_CAP_QOS(dev, packet_pacing)) {\r\ntable->max_size = 0;\r\nreturn 0;\r\n}\r\ntable->max_size = MLX5_CAP_QOS(dev, packet_pacing_rate_table_size) - 1;\r\ntable->max_rate = MLX5_CAP_QOS(dev, packet_pacing_max_rate);\r\ntable->min_rate = MLX5_CAP_QOS(dev, packet_pacing_min_rate);\r\ntable->rl_entry = kcalloc(table->max_size, sizeof(struct mlx5_rl_entry),\r\nGFP_KERNEL);\r\nif (!table->rl_entry)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < table->max_size; i++)\r\ntable->rl_entry[i].index = i + 1;\r\nmlx5_core_info(dev, "Rate limit: %u rates are supported, range: %uMbps to %uMbps\n",\r\ntable->max_size,\r\ntable->min_rate >> 10,\r\ntable->max_rate >> 10);\r\nreturn 0;\r\n}\r\nvoid mlx5_cleanup_rl_table(struct mlx5_core_dev *dev)\r\n{\r\nstruct mlx5_rl_table *table = &dev->priv.rl_table;\r\nint i;\r\nfor (i = 0; i < table->max_size; i++)\r\nif (table->rl_entry[i].rate)\r\nmlx5_set_rate_limit_cmd(dev, 0,\r\ntable->rl_entry[i].index);\r\nkfree(dev->priv.rl_table.rl_entry);\r\n}
