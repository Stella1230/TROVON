int amd_sched_fence_slab_init(void)\r\n{\r\nsched_fence_slab = kmem_cache_create(\r\n"amd_sched_fence", sizeof(struct amd_sched_fence), 0,\r\nSLAB_HWCACHE_ALIGN, NULL);\r\nif (!sched_fence_slab)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid amd_sched_fence_slab_fini(void)\r\n{\r\nrcu_barrier();\r\nkmem_cache_destroy(sched_fence_slab);\r\n}\r\nstruct amd_sched_fence *amd_sched_fence_create(struct amd_sched_entity *entity,\r\nvoid *owner)\r\n{\r\nstruct amd_sched_fence *fence = NULL;\r\nunsigned seq;\r\nfence = kmem_cache_zalloc(sched_fence_slab, GFP_KERNEL);\r\nif (fence == NULL)\r\nreturn NULL;\r\nfence->owner = owner;\r\nfence->sched = entity->sched;\r\nspin_lock_init(&fence->lock);\r\nseq = atomic_inc_return(&entity->fence_seq);\r\ndma_fence_init(&fence->scheduled, &amd_sched_fence_ops_scheduled,\r\n&fence->lock, entity->fence_context, seq);\r\ndma_fence_init(&fence->finished, &amd_sched_fence_ops_finished,\r\n&fence->lock, entity->fence_context + 1, seq);\r\nreturn fence;\r\n}\r\nvoid amd_sched_fence_scheduled(struct amd_sched_fence *fence)\r\n{\r\nint ret = dma_fence_signal(&fence->scheduled);\r\nif (!ret)\r\nDMA_FENCE_TRACE(&fence->scheduled,\r\n"signaled from irq context\n");\r\nelse\r\nDMA_FENCE_TRACE(&fence->scheduled,\r\n"was already signaled\n");\r\n}\r\nvoid amd_sched_fence_finished(struct amd_sched_fence *fence)\r\n{\r\nint ret = dma_fence_signal(&fence->finished);\r\nif (!ret)\r\nDMA_FENCE_TRACE(&fence->finished,\r\n"signaled from irq context\n");\r\nelse\r\nDMA_FENCE_TRACE(&fence->finished,\r\n"was already signaled\n");\r\n}\r\nstatic const char *amd_sched_fence_get_driver_name(struct dma_fence *fence)\r\n{\r\nreturn "amd_sched";\r\n}\r\nstatic const char *amd_sched_fence_get_timeline_name(struct dma_fence *f)\r\n{\r\nstruct amd_sched_fence *fence = to_amd_sched_fence(f);\r\nreturn (const char *)fence->sched->name;\r\n}\r\nstatic bool amd_sched_fence_enable_signaling(struct dma_fence *f)\r\n{\r\nreturn true;\r\n}\r\nstatic void amd_sched_fence_free(struct rcu_head *rcu)\r\n{\r\nstruct dma_fence *f = container_of(rcu, struct dma_fence, rcu);\r\nstruct amd_sched_fence *fence = to_amd_sched_fence(f);\r\ndma_fence_put(fence->parent);\r\nkmem_cache_free(sched_fence_slab, fence);\r\n}\r\nstatic void amd_sched_fence_release_scheduled(struct dma_fence *f)\r\n{\r\nstruct amd_sched_fence *fence = to_amd_sched_fence(f);\r\ncall_rcu(&fence->finished.rcu, amd_sched_fence_free);\r\n}\r\nstatic void amd_sched_fence_release_finished(struct dma_fence *f)\r\n{\r\nstruct amd_sched_fence *fence = to_amd_sched_fence(f);\r\ndma_fence_put(&fence->scheduled);\r\n}
