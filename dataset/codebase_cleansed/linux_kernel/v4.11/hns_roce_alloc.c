int hns_roce_bitmap_alloc(struct hns_roce_bitmap *bitmap, unsigned long *obj)\r\n{\r\nint ret = 0;\r\nspin_lock(&bitmap->lock);\r\n*obj = find_next_zero_bit(bitmap->table, bitmap->max, bitmap->last);\r\nif (*obj >= bitmap->max) {\r\nbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\r\n& bitmap->mask;\r\n*obj = find_first_zero_bit(bitmap->table, bitmap->max);\r\n}\r\nif (*obj < bitmap->max) {\r\nset_bit(*obj, bitmap->table);\r\nbitmap->last = (*obj + 1);\r\nif (bitmap->last == bitmap->max)\r\nbitmap->last = 0;\r\n*obj |= bitmap->top;\r\n} else {\r\nret = -1;\r\n}\r\nspin_unlock(&bitmap->lock);\r\nreturn ret;\r\n}\r\nvoid hns_roce_bitmap_free(struct hns_roce_bitmap *bitmap, unsigned long obj,\r\nint rr)\r\n{\r\nhns_roce_bitmap_free_range(bitmap, obj, 1, rr);\r\n}\r\nint hns_roce_bitmap_alloc_range(struct hns_roce_bitmap *bitmap, int cnt,\r\nint align, unsigned long *obj)\r\n{\r\nint ret = 0;\r\nint i;\r\nif (likely(cnt == 1 && align == 1))\r\nreturn hns_roce_bitmap_alloc(bitmap, obj);\r\nspin_lock(&bitmap->lock);\r\n*obj = bitmap_find_next_zero_area(bitmap->table, bitmap->max,\r\nbitmap->last, cnt, align - 1);\r\nif (*obj >= bitmap->max) {\r\nbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\r\n& bitmap->mask;\r\n*obj = bitmap_find_next_zero_area(bitmap->table, bitmap->max, 0,\r\ncnt, align - 1);\r\n}\r\nif (*obj < bitmap->max) {\r\nfor (i = 0; i < cnt; i++)\r\nset_bit(*obj + i, bitmap->table);\r\nif (*obj == bitmap->last) {\r\nbitmap->last = (*obj + cnt);\r\nif (bitmap->last >= bitmap->max)\r\nbitmap->last = 0;\r\n}\r\n*obj |= bitmap->top;\r\n} else {\r\nret = -1;\r\n}\r\nspin_unlock(&bitmap->lock);\r\nreturn ret;\r\n}\r\nvoid hns_roce_bitmap_free_range(struct hns_roce_bitmap *bitmap,\r\nunsigned long obj, int cnt,\r\nint rr)\r\n{\r\nint i;\r\nobj &= bitmap->max + bitmap->reserved_top - 1;\r\nspin_lock(&bitmap->lock);\r\nfor (i = 0; i < cnt; i++)\r\nclear_bit(obj + i, bitmap->table);\r\nif (!rr)\r\nbitmap->last = min(bitmap->last, obj);\r\nbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\r\n& bitmap->mask;\r\nspin_unlock(&bitmap->lock);\r\n}\r\nint hns_roce_bitmap_init(struct hns_roce_bitmap *bitmap, u32 num, u32 mask,\r\nu32 reserved_bot, u32 reserved_top)\r\n{\r\nu32 i;\r\nif (num != roundup_pow_of_two(num))\r\nreturn -EINVAL;\r\nbitmap->last = 0;\r\nbitmap->top = 0;\r\nbitmap->max = num - reserved_top;\r\nbitmap->mask = mask;\r\nbitmap->reserved_top = reserved_top;\r\nspin_lock_init(&bitmap->lock);\r\nbitmap->table = kcalloc(BITS_TO_LONGS(bitmap->max), sizeof(long),\r\nGFP_KERNEL);\r\nif (!bitmap->table)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < reserved_bot; ++i)\r\nset_bit(i, bitmap->table);\r\nreturn 0;\r\n}\r\nvoid hns_roce_bitmap_cleanup(struct hns_roce_bitmap *bitmap)\r\n{\r\nkfree(bitmap->table);\r\n}\r\nvoid hns_roce_buf_free(struct hns_roce_dev *hr_dev, u32 size,\r\nstruct hns_roce_buf *buf)\r\n{\r\nint i;\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nu32 bits_per_long = BITS_PER_LONG;\r\nif (buf->nbufs == 1) {\r\ndma_free_coherent(dev, size, buf->direct.buf, buf->direct.map);\r\n} else {\r\nif (bits_per_long == 64)\r\nvunmap(buf->direct.buf);\r\nfor (i = 0; i < buf->nbufs; ++i)\r\nif (buf->page_list[i].buf)\r\ndma_free_coherent(&hr_dev->pdev->dev, PAGE_SIZE,\r\nbuf->page_list[i].buf,\r\nbuf->page_list[i].map);\r\nkfree(buf->page_list);\r\n}\r\n}\r\nint hns_roce_buf_alloc(struct hns_roce_dev *hr_dev, u32 size, u32 max_direct,\r\nstruct hns_roce_buf *buf)\r\n{\r\nint i = 0;\r\ndma_addr_t t;\r\nstruct page **pages;\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nu32 bits_per_long = BITS_PER_LONG;\r\nif (size <= max_direct) {\r\nbuf->nbufs = 1;\r\nbuf->npages = 1 << get_order(size);\r\nbuf->page_shift = PAGE_SHIFT;\r\nbuf->direct.buf = dma_alloc_coherent(dev, size, &t, GFP_KERNEL);\r\nif (!buf->direct.buf)\r\nreturn -ENOMEM;\r\nbuf->direct.map = t;\r\nwhile (t & ((1 << buf->page_shift) - 1)) {\r\n--buf->page_shift;\r\nbuf->npages *= 2;\r\n}\r\nmemset(buf->direct.buf, 0, size);\r\n} else {\r\nbuf->nbufs = (size + PAGE_SIZE - 1) / PAGE_SIZE;\r\nbuf->npages = buf->nbufs;\r\nbuf->page_shift = PAGE_SHIFT;\r\nbuf->page_list = kcalloc(buf->nbufs, sizeof(*buf->page_list),\r\nGFP_KERNEL);\r\nif (!buf->page_list)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < buf->nbufs; ++i) {\r\nbuf->page_list[i].buf = dma_alloc_coherent(dev,\r\nPAGE_SIZE, &t,\r\nGFP_KERNEL);\r\nif (!buf->page_list[i].buf)\r\ngoto err_free;\r\nbuf->page_list[i].map = t;\r\nmemset(buf->page_list[i].buf, 0, PAGE_SIZE);\r\n}\r\nif (bits_per_long == 64) {\r\npages = kmalloc_array(buf->nbufs, sizeof(*pages),\r\nGFP_KERNEL);\r\nif (!pages)\r\ngoto err_free;\r\nfor (i = 0; i < buf->nbufs; ++i)\r\npages[i] = virt_to_page(buf->page_list[i].buf);\r\nbuf->direct.buf = vmap(pages, buf->nbufs, VM_MAP,\r\nPAGE_KERNEL);\r\nkfree(pages);\r\nif (!buf->direct.buf)\r\ngoto err_free;\r\n}\r\n}\r\nreturn 0;\r\nerr_free:\r\nhns_roce_buf_free(hr_dev, size, buf);\r\nreturn -ENOMEM;\r\n}\r\nvoid hns_roce_cleanup_bitmap(struct hns_roce_dev *hr_dev)\r\n{\r\nhns_roce_cleanup_qp_table(hr_dev);\r\nhns_roce_cleanup_cq_table(hr_dev);\r\nhns_roce_cleanup_mr_table(hr_dev);\r\nhns_roce_cleanup_pd_table(hr_dev);\r\nhns_roce_cleanup_uar_table(hr_dev);\r\n}
