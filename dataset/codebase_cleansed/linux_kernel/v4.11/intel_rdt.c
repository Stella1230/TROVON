static int cbm_idx(struct rdt_resource *r, int closid)\r\n{\r\nreturn closid * r->cbm_idx_multi + r->cbm_idx_offset;\r\n}\r\nstatic inline bool cache_alloc_hsw_probe(void)\r\n{\r\nif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&\r\nboot_cpu_data.x86 == 6 &&\r\nboot_cpu_data.x86_model == INTEL_FAM6_HASWELL_X) {\r\nstruct rdt_resource *r = &rdt_resources_all[RDT_RESOURCE_L3];\r\nu32 l, h, max_cbm = BIT_MASK(20) - 1;\r\nif (wrmsr_safe(IA32_L3_CBM_BASE, max_cbm, 0))\r\nreturn false;\r\nrdmsr(IA32_L3_CBM_BASE, l, h);\r\nif (l != max_cbm)\r\nreturn false;\r\nr->num_closid = 4;\r\nr->cbm_len = 20;\r\nr->max_cbm = max_cbm;\r\nr->min_cbm_bits = 2;\r\nr->capable = true;\r\nr->enabled = true;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void rdt_get_config(int idx, struct rdt_resource *r)\r\n{\r\nunion cpuid_0x10_1_eax eax;\r\nunion cpuid_0x10_1_edx edx;\r\nu32 ebx, ecx;\r\ncpuid_count(0x00000010, idx, &eax.full, &ebx, &ecx, &edx.full);\r\nr->num_closid = edx.split.cos_max + 1;\r\nr->cbm_len = eax.split.cbm_len + 1;\r\nr->max_cbm = BIT_MASK(eax.split.cbm_len + 1) - 1;\r\nr->capable = true;\r\nr->enabled = true;\r\n}\r\nstatic void rdt_get_cdp_l3_config(int type)\r\n{\r\nstruct rdt_resource *r_l3 = &rdt_resources_all[RDT_RESOURCE_L3];\r\nstruct rdt_resource *r = &rdt_resources_all[type];\r\nr->num_closid = r_l3->num_closid / 2;\r\nr->cbm_len = r_l3->cbm_len;\r\nr->max_cbm = r_l3->max_cbm;\r\nr->capable = true;\r\nr->enabled = false;\r\n}\r\nstatic inline bool get_rdt_resources(void)\r\n{\r\nbool ret = false;\r\nif (cache_alloc_hsw_probe())\r\nreturn true;\r\nif (!boot_cpu_has(X86_FEATURE_RDT_A))\r\nreturn false;\r\nif (boot_cpu_has(X86_FEATURE_CAT_L3)) {\r\nrdt_get_config(1, &rdt_resources_all[RDT_RESOURCE_L3]);\r\nif (boot_cpu_has(X86_FEATURE_CDP_L3)) {\r\nrdt_get_cdp_l3_config(RDT_RESOURCE_L3DATA);\r\nrdt_get_cdp_l3_config(RDT_RESOURCE_L3CODE);\r\n}\r\nret = true;\r\n}\r\nif (boot_cpu_has(X86_FEATURE_CAT_L2)) {\r\nrdt_get_config(2, &rdt_resources_all[RDT_RESOURCE_L2]);\r\nret = true;\r\n}\r\nreturn ret;\r\n}\r\nstatic int get_cache_id(int cpu, int level)\r\n{\r\nstruct cpu_cacheinfo *ci = get_cpu_cacheinfo(cpu);\r\nint i;\r\nfor (i = 0; i < ci->num_leaves; i++) {\r\nif (ci->info_list[i].level == level)\r\nreturn ci->info_list[i].id;\r\n}\r\nreturn -1;\r\n}\r\nvoid rdt_cbm_update(void *arg)\r\n{\r\nstruct msr_param *m = (struct msr_param *)arg;\r\nstruct rdt_resource *r = m->res;\r\nint i, cpu = smp_processor_id();\r\nstruct rdt_domain *d;\r\nlist_for_each_entry(d, &r->domains, list) {\r\nif (cpumask_test_cpu(cpu, &d->cpu_mask))\r\ngoto found;\r\n}\r\npr_info_once("cpu %d not found in any domain for resource %s\n",\r\ncpu, r->name);\r\nreturn;\r\nfound:\r\nfor (i = m->low; i < m->high; i++) {\r\nint idx = cbm_idx(r, i);\r\nwrmsrl(r->msr_base + idx, d->cbm[i]);\r\n}\r\n}\r\nstatic struct rdt_domain *rdt_find_domain(struct rdt_resource *r, int id,\r\nstruct list_head **pos)\r\n{\r\nstruct rdt_domain *d;\r\nstruct list_head *l;\r\nif (id < 0)\r\nreturn ERR_PTR(id);\r\nlist_for_each(l, &r->domains) {\r\nd = list_entry(l, struct rdt_domain, list);\r\nif (id == d->id)\r\nreturn d;\r\nif (id < d->id)\r\nbreak;\r\n}\r\nif (pos)\r\n*pos = l;\r\nreturn NULL;\r\n}\r\nstatic void domain_add_cpu(int cpu, struct rdt_resource *r)\r\n{\r\nint i, id = get_cache_id(cpu, r->cache_level);\r\nstruct list_head *add_pos = NULL;\r\nstruct rdt_domain *d;\r\nd = rdt_find_domain(r, id, &add_pos);\r\nif (IS_ERR(d)) {\r\npr_warn("Could't find cache id for cpu %d\n", cpu);\r\nreturn;\r\n}\r\nif (d) {\r\ncpumask_set_cpu(cpu, &d->cpu_mask);\r\nreturn;\r\n}\r\nd = kzalloc_node(sizeof(*d), GFP_KERNEL, cpu_to_node(cpu));\r\nif (!d)\r\nreturn;\r\nd->id = id;\r\nd->cbm = kmalloc_array(r->num_closid, sizeof(*d->cbm), GFP_KERNEL);\r\nif (!d->cbm) {\r\nkfree(d);\r\nreturn;\r\n}\r\nfor (i = 0; i < r->num_closid; i++) {\r\nint idx = cbm_idx(r, i);\r\nd->cbm[i] = r->max_cbm;\r\nwrmsrl(r->msr_base + idx, d->cbm[i]);\r\n}\r\ncpumask_set_cpu(cpu, &d->cpu_mask);\r\nlist_add_tail(&d->list, add_pos);\r\nr->num_domains++;\r\n}\r\nstatic void domain_remove_cpu(int cpu, struct rdt_resource *r)\r\n{\r\nint id = get_cache_id(cpu, r->cache_level);\r\nstruct rdt_domain *d;\r\nd = rdt_find_domain(r, id, NULL);\r\nif (IS_ERR_OR_NULL(d)) {\r\npr_warn("Could't find cache id for cpu %d\n", cpu);\r\nreturn;\r\n}\r\ncpumask_clear_cpu(cpu, &d->cpu_mask);\r\nif (cpumask_empty(&d->cpu_mask)) {\r\nr->num_domains--;\r\nkfree(d->cbm);\r\nlist_del(&d->list);\r\nkfree(d);\r\n}\r\n}\r\nstatic void clear_closid(int cpu)\r\n{\r\nstruct intel_pqr_state *state = this_cpu_ptr(&pqr_state);\r\nper_cpu(cpu_closid, cpu) = 0;\r\nstate->closid = 0;\r\nwrmsr(MSR_IA32_PQR_ASSOC, state->rmid, 0);\r\n}\r\nstatic int intel_rdt_online_cpu(unsigned int cpu)\r\n{\r\nstruct rdt_resource *r;\r\nmutex_lock(&rdtgroup_mutex);\r\nfor_each_capable_rdt_resource(r)\r\ndomain_add_cpu(cpu, r);\r\ncpumask_set_cpu(cpu, &rdtgroup_default.cpu_mask);\r\nclear_closid(cpu);\r\nmutex_unlock(&rdtgroup_mutex);\r\nreturn 0;\r\n}\r\nstatic int intel_rdt_offline_cpu(unsigned int cpu)\r\n{\r\nstruct rdtgroup *rdtgrp;\r\nstruct rdt_resource *r;\r\nmutex_lock(&rdtgroup_mutex);\r\nfor_each_capable_rdt_resource(r)\r\ndomain_remove_cpu(cpu, r);\r\nlist_for_each_entry(rdtgrp, &rdt_all_groups, rdtgroup_list) {\r\nif (cpumask_test_and_clear_cpu(cpu, &rdtgrp->cpu_mask))\r\nbreak;\r\n}\r\nclear_closid(cpu);\r\nmutex_unlock(&rdtgroup_mutex);\r\nreturn 0;\r\n}\r\nstatic int __init intel_rdt_late_init(void)\r\n{\r\nstruct rdt_resource *r;\r\nint state, ret;\r\nif (!get_rdt_resources())\r\nreturn -ENODEV;\r\nstate = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN,\r\n"x86/rdt/cat:online:",\r\nintel_rdt_online_cpu, intel_rdt_offline_cpu);\r\nif (state < 0)\r\nreturn state;\r\nret = rdtgroup_init();\r\nif (ret) {\r\ncpuhp_remove_state(state);\r\nreturn ret;\r\n}\r\nfor_each_capable_rdt_resource(r)\r\npr_info("Intel RDT %s allocation detected\n", r->name);\r\nreturn 0;\r\n}
