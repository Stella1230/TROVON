static void vc4_bo_stats_dump(struct vc4_dev *vc4)\r\n{\r\nDRM_INFO("num bos allocated: %d\n",\r\nvc4->bo_stats.num_allocated);\r\nDRM_INFO("size bos allocated: %dkb\n",\r\nvc4->bo_stats.size_allocated / 1024);\r\nDRM_INFO("num bos used: %d\n",\r\nvc4->bo_stats.num_allocated - vc4->bo_stats.num_cached);\r\nDRM_INFO("size bos used: %dkb\n",\r\n(vc4->bo_stats.size_allocated -\r\nvc4->bo_stats.size_cached) / 1024);\r\nDRM_INFO("num bos cached: %d\n",\r\nvc4->bo_stats.num_cached);\r\nDRM_INFO("size bos cached: %dkb\n",\r\nvc4->bo_stats.size_cached / 1024);\r\n}\r\nint vc4_bo_stats_debugfs(struct seq_file *m, void *unused)\r\n{\r\nstruct drm_info_node *node = (struct drm_info_node *)m->private;\r\nstruct drm_device *dev = node->minor->dev;\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nstruct vc4_bo_stats stats;\r\nmutex_lock(&vc4->bo_lock);\r\nstats = vc4->bo_stats;\r\nmutex_unlock(&vc4->bo_lock);\r\nseq_printf(m, "num bos allocated: %d\n",\r\nstats.num_allocated);\r\nseq_printf(m, "size bos allocated: %dkb\n",\r\nstats.size_allocated / 1024);\r\nseq_printf(m, "num bos used: %d\n",\r\nstats.num_allocated - stats.num_cached);\r\nseq_printf(m, "size bos used: %dkb\n",\r\n(stats.size_allocated - stats.size_cached) / 1024);\r\nseq_printf(m, "num bos cached: %d\n",\r\nstats.num_cached);\r\nseq_printf(m, "size bos cached: %dkb\n",\r\nstats.size_cached / 1024);\r\nreturn 0;\r\n}\r\nstatic uint32_t bo_page_index(size_t size)\r\n{\r\nreturn (size / PAGE_SIZE) - 1;\r\n}\r\nstatic void vc4_bo_destroy(struct vc4_bo *bo)\r\n{\r\nstruct drm_gem_object *obj = &bo->base.base;\r\nstruct vc4_dev *vc4 = to_vc4_dev(obj->dev);\r\nif (bo->validated_shader) {\r\nkfree(bo->validated_shader->texture_samples);\r\nkfree(bo->validated_shader);\r\nbo->validated_shader = NULL;\r\n}\r\nvc4->bo_stats.num_allocated--;\r\nvc4->bo_stats.size_allocated -= obj->size;\r\ndrm_gem_cma_free_object(obj);\r\n}\r\nstatic void vc4_bo_remove_from_cache(struct vc4_bo *bo)\r\n{\r\nstruct drm_gem_object *obj = &bo->base.base;\r\nstruct vc4_dev *vc4 = to_vc4_dev(obj->dev);\r\nvc4->bo_stats.num_cached--;\r\nvc4->bo_stats.size_cached -= obj->size;\r\nlist_del(&bo->unref_head);\r\nlist_del(&bo->size_head);\r\n}\r\nstatic struct list_head *vc4_get_cache_list_for_size(struct drm_device *dev,\r\nsize_t size)\r\n{\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nuint32_t page_index = bo_page_index(size);\r\nif (vc4->bo_cache.size_list_size <= page_index) {\r\nuint32_t new_size = max(vc4->bo_cache.size_list_size * 2,\r\npage_index + 1);\r\nstruct list_head *new_list;\r\nuint32_t i;\r\nnew_list = kmalloc_array(new_size, sizeof(struct list_head),\r\nGFP_KERNEL);\r\nif (!new_list)\r\nreturn NULL;\r\nfor (i = 0; i < vc4->bo_cache.size_list_size; i++) {\r\nstruct list_head *old_list =\r\n&vc4->bo_cache.size_list[i];\r\nif (list_empty(old_list))\r\nINIT_LIST_HEAD(&new_list[i]);\r\nelse\r\nlist_replace(old_list, &new_list[i]);\r\n}\r\nfor (i = vc4->bo_cache.size_list_size; i < new_size; i++)\r\nINIT_LIST_HEAD(&new_list[i]);\r\nkfree(vc4->bo_cache.size_list);\r\nvc4->bo_cache.size_list = new_list;\r\nvc4->bo_cache.size_list_size = new_size;\r\n}\r\nreturn &vc4->bo_cache.size_list[page_index];\r\n}\r\nstatic void vc4_bo_cache_purge(struct drm_device *dev)\r\n{\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nmutex_lock(&vc4->bo_lock);\r\nwhile (!list_empty(&vc4->bo_cache.time_list)) {\r\nstruct vc4_bo *bo = list_last_entry(&vc4->bo_cache.time_list,\r\nstruct vc4_bo, unref_head);\r\nvc4_bo_remove_from_cache(bo);\r\nvc4_bo_destroy(bo);\r\n}\r\nmutex_unlock(&vc4->bo_lock);\r\n}\r\nstatic struct vc4_bo *vc4_bo_get_from_cache(struct drm_device *dev,\r\nuint32_t size)\r\n{\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nuint32_t page_index = bo_page_index(size);\r\nstruct vc4_bo *bo = NULL;\r\nsize = roundup(size, PAGE_SIZE);\r\nmutex_lock(&vc4->bo_lock);\r\nif (page_index >= vc4->bo_cache.size_list_size)\r\ngoto out;\r\nif (list_empty(&vc4->bo_cache.size_list[page_index]))\r\ngoto out;\r\nbo = list_first_entry(&vc4->bo_cache.size_list[page_index],\r\nstruct vc4_bo, size_head);\r\nvc4_bo_remove_from_cache(bo);\r\nkref_init(&bo->base.base.refcount);\r\nout:\r\nmutex_unlock(&vc4->bo_lock);\r\nreturn bo;\r\n}\r\nstruct drm_gem_object *vc4_create_object(struct drm_device *dev, size_t size)\r\n{\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nstruct vc4_bo *bo;\r\nbo = kzalloc(sizeof(*bo), GFP_KERNEL);\r\nif (!bo)\r\nreturn ERR_PTR(-ENOMEM);\r\nmutex_lock(&vc4->bo_lock);\r\nvc4->bo_stats.num_allocated++;\r\nvc4->bo_stats.size_allocated += size;\r\nmutex_unlock(&vc4->bo_lock);\r\nreturn &bo->base.base;\r\n}\r\nstruct vc4_bo *vc4_bo_create(struct drm_device *dev, size_t unaligned_size,\r\nbool from_cache)\r\n{\r\nsize_t size = roundup(unaligned_size, PAGE_SIZE);\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nstruct drm_gem_cma_object *cma_obj;\r\nif (size == 0)\r\nreturn ERR_PTR(-EINVAL);\r\nif (from_cache) {\r\nstruct vc4_bo *bo = vc4_bo_get_from_cache(dev, size);\r\nif (bo)\r\nreturn bo;\r\n}\r\ncma_obj = drm_gem_cma_create(dev, size);\r\nif (IS_ERR(cma_obj)) {\r\nvc4_bo_cache_purge(dev);\r\ncma_obj = drm_gem_cma_create(dev, size);\r\nif (IS_ERR(cma_obj)) {\r\nDRM_ERROR("Failed to allocate from CMA:\n");\r\nvc4_bo_stats_dump(vc4);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\n}\r\nreturn to_vc4_bo(&cma_obj->base);\r\n}\r\nint vc4_dumb_create(struct drm_file *file_priv,\r\nstruct drm_device *dev,\r\nstruct drm_mode_create_dumb *args)\r\n{\r\nint min_pitch = DIV_ROUND_UP(args->width * args->bpp, 8);\r\nstruct vc4_bo *bo = NULL;\r\nint ret;\r\nif (args->pitch < min_pitch)\r\nargs->pitch = min_pitch;\r\nif (args->size < args->pitch * args->height)\r\nargs->size = args->pitch * args->height;\r\nbo = vc4_bo_create(dev, args->size, false);\r\nif (IS_ERR(bo))\r\nreturn PTR_ERR(bo);\r\nret = drm_gem_handle_create(file_priv, &bo->base.base, &args->handle);\r\ndrm_gem_object_unreference_unlocked(&bo->base.base);\r\nreturn ret;\r\n}\r\nstatic void vc4_bo_cache_free_old(struct drm_device *dev)\r\n{\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nunsigned long expire_time = jiffies - msecs_to_jiffies(1000);\r\nwhile (!list_empty(&vc4->bo_cache.time_list)) {\r\nstruct vc4_bo *bo = list_last_entry(&vc4->bo_cache.time_list,\r\nstruct vc4_bo, unref_head);\r\nif (time_before(expire_time, bo->free_time)) {\r\nmod_timer(&vc4->bo_cache.time_timer,\r\nround_jiffies_up(jiffies +\r\nmsecs_to_jiffies(1000)));\r\nreturn;\r\n}\r\nvc4_bo_remove_from_cache(bo);\r\nvc4_bo_destroy(bo);\r\n}\r\n}\r\nvoid vc4_free_object(struct drm_gem_object *gem_bo)\r\n{\r\nstruct drm_device *dev = gem_bo->dev;\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nstruct vc4_bo *bo = to_vc4_bo(gem_bo);\r\nstruct list_head *cache_list;\r\nmutex_lock(&vc4->bo_lock);\r\nif (gem_bo->import_attach) {\r\nvc4_bo_destroy(bo);\r\ngoto out;\r\n}\r\nif (gem_bo->name) {\r\nvc4_bo_destroy(bo);\r\ngoto out;\r\n}\r\ncache_list = vc4_get_cache_list_for_size(dev, gem_bo->size);\r\nif (!cache_list) {\r\nvc4_bo_destroy(bo);\r\ngoto out;\r\n}\r\nif (bo->validated_shader) {\r\nkfree(bo->validated_shader->texture_samples);\r\nkfree(bo->validated_shader);\r\nbo->validated_shader = NULL;\r\n}\r\nbo->free_time = jiffies;\r\nlist_add(&bo->size_head, cache_list);\r\nlist_add(&bo->unref_head, &vc4->bo_cache.time_list);\r\nvc4->bo_stats.num_cached++;\r\nvc4->bo_stats.size_cached += gem_bo->size;\r\nvc4_bo_cache_free_old(dev);\r\nout:\r\nmutex_unlock(&vc4->bo_lock);\r\n}\r\nstatic void vc4_bo_cache_time_work(struct work_struct *work)\r\n{\r\nstruct vc4_dev *vc4 =\r\ncontainer_of(work, struct vc4_dev, bo_cache.time_work);\r\nstruct drm_device *dev = vc4->dev;\r\nmutex_lock(&vc4->bo_lock);\r\nvc4_bo_cache_free_old(dev);\r\nmutex_unlock(&vc4->bo_lock);\r\n}\r\nstatic void vc4_bo_cache_time_timer(unsigned long data)\r\n{\r\nstruct drm_device *dev = (struct drm_device *)data;\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nschedule_work(&vc4->bo_cache.time_work);\r\n}\r\nstruct dma_buf *\r\nvc4_prime_export(struct drm_device *dev, struct drm_gem_object *obj, int flags)\r\n{\r\nstruct vc4_bo *bo = to_vc4_bo(obj);\r\nif (bo->validated_shader) {\r\nDRM_ERROR("Attempting to export shader BO\n");\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nreturn drm_gem_prime_export(dev, obj, flags);\r\n}\r\nint vc4_mmap(struct file *filp, struct vm_area_struct *vma)\r\n{\r\nstruct drm_gem_object *gem_obj;\r\nstruct vc4_bo *bo;\r\nint ret;\r\nret = drm_gem_mmap(filp, vma);\r\nif (ret)\r\nreturn ret;\r\ngem_obj = vma->vm_private_data;\r\nbo = to_vc4_bo(gem_obj);\r\nif (bo->validated_shader && (vma->vm_flags & VM_WRITE)) {\r\nDRM_ERROR("mmaping of shader BOs for writing not allowed.\n");\r\nreturn -EINVAL;\r\n}\r\nvma->vm_flags &= ~VM_PFNMAP;\r\nvma->vm_pgoff = 0;\r\nret = dma_mmap_wc(bo->base.base.dev->dev, vma, bo->base.vaddr,\r\nbo->base.paddr, vma->vm_end - vma->vm_start);\r\nif (ret)\r\ndrm_gem_vm_close(vma);\r\nreturn ret;\r\n}\r\nint vc4_prime_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\r\n{\r\nstruct vc4_bo *bo = to_vc4_bo(obj);\r\nif (bo->validated_shader && (vma->vm_flags & VM_WRITE)) {\r\nDRM_ERROR("mmaping of shader BOs for writing not allowed.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn drm_gem_cma_prime_mmap(obj, vma);\r\n}\r\nvoid *vc4_prime_vmap(struct drm_gem_object *obj)\r\n{\r\nstruct vc4_bo *bo = to_vc4_bo(obj);\r\nif (bo->validated_shader) {\r\nDRM_ERROR("mmaping of shader BOs not allowed.\n");\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nreturn drm_gem_cma_prime_vmap(obj);\r\n}\r\nint vc4_create_bo_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_vc4_create_bo *args = data;\r\nstruct vc4_bo *bo = NULL;\r\nint ret;\r\nbo = vc4_bo_create(dev, args->size, false);\r\nif (IS_ERR(bo))\r\nreturn PTR_ERR(bo);\r\nret = drm_gem_handle_create(file_priv, &bo->base.base, &args->handle);\r\ndrm_gem_object_unreference_unlocked(&bo->base.base);\r\nreturn ret;\r\n}\r\nint vc4_mmap_bo_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_vc4_mmap_bo *args = data;\r\nstruct drm_gem_object *gem_obj;\r\ngem_obj = drm_gem_object_lookup(file_priv, args->handle);\r\nif (!gem_obj) {\r\nDRM_ERROR("Failed to look up GEM BO %d\n", args->handle);\r\nreturn -EINVAL;\r\n}\r\nargs->offset = drm_vma_node_offset_addr(&gem_obj->vma_node);\r\ndrm_gem_object_unreference_unlocked(gem_obj);\r\nreturn 0;\r\n}\r\nint\r\nvc4_create_shader_bo_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_vc4_create_shader_bo *args = data;\r\nstruct vc4_bo *bo = NULL;\r\nint ret;\r\nif (args->size == 0)\r\nreturn -EINVAL;\r\nif (args->size % sizeof(u64) != 0)\r\nreturn -EINVAL;\r\nif (args->flags != 0) {\r\nDRM_INFO("Unknown flags set: 0x%08x\n", args->flags);\r\nreturn -EINVAL;\r\n}\r\nif (args->pad != 0) {\r\nDRM_INFO("Pad set: 0x%08x\n", args->pad);\r\nreturn -EINVAL;\r\n}\r\nbo = vc4_bo_create(dev, args->size, true);\r\nif (IS_ERR(bo))\r\nreturn PTR_ERR(bo);\r\nif (copy_from_user(bo->base.vaddr,\r\n(void __user *)(uintptr_t)args->data,\r\nargs->size)) {\r\nret = -EFAULT;\r\ngoto fail;\r\n}\r\nmemset(bo->base.vaddr + args->size, 0,\r\nbo->base.base.size - args->size);\r\nbo->validated_shader = vc4_validate_shader(&bo->base);\r\nif (!bo->validated_shader) {\r\nret = -EINVAL;\r\ngoto fail;\r\n}\r\nret = drm_gem_handle_create(file_priv, &bo->base.base, &args->handle);\r\nfail:\r\ndrm_gem_object_unreference_unlocked(&bo->base.base);\r\nreturn ret;\r\n}\r\nvoid vc4_bo_cache_init(struct drm_device *dev)\r\n{\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\nmutex_init(&vc4->bo_lock);\r\nINIT_LIST_HEAD(&vc4->bo_cache.time_list);\r\nINIT_WORK(&vc4->bo_cache.time_work, vc4_bo_cache_time_work);\r\nsetup_timer(&vc4->bo_cache.time_timer,\r\nvc4_bo_cache_time_timer,\r\n(unsigned long)dev);\r\n}\r\nvoid vc4_bo_cache_destroy(struct drm_device *dev)\r\n{\r\nstruct vc4_dev *vc4 = to_vc4_dev(dev);\r\ndel_timer(&vc4->bo_cache.time_timer);\r\ncancel_work_sync(&vc4->bo_cache.time_work);\r\nvc4_bo_cache_purge(dev);\r\nif (vc4->bo_stats.num_allocated) {\r\nDRM_ERROR("Destroying BO cache while BOs still allocated:\n");\r\nvc4_bo_stats_dump(vc4);\r\n}\r\n}
