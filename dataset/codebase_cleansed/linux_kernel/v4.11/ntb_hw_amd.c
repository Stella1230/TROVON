static int ndev_mw_to_bar(struct amd_ntb_dev *ndev, int idx)\r\n{\r\nif (idx < 0 || idx > ndev->mw_count)\r\nreturn -EINVAL;\r\nreturn 1 << idx;\r\n}\r\nstatic int amd_ntb_mw_count(struct ntb_dev *ntb)\r\n{\r\nreturn ntb_ndev(ntb)->mw_count;\r\n}\r\nstatic int amd_ntb_mw_get_range(struct ntb_dev *ntb, int idx,\r\nphys_addr_t *base,\r\nresource_size_t *size,\r\nresource_size_t *align,\r\nresource_size_t *align_size)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nint bar;\r\nbar = ndev_mw_to_bar(ndev, idx);\r\nif (bar < 0)\r\nreturn bar;\r\nif (base)\r\n*base = pci_resource_start(ndev->ntb.pdev, bar);\r\nif (size)\r\n*size = pci_resource_len(ndev->ntb.pdev, bar);\r\nif (align)\r\n*align = SZ_4K;\r\nif (align_size)\r\n*align_size = 1;\r\nreturn 0;\r\n}\r\nstatic int amd_ntb_mw_set_trans(struct ntb_dev *ntb, int idx,\r\ndma_addr_t addr, resource_size_t size)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nunsigned long xlat_reg, limit_reg = 0;\r\nresource_size_t mw_size;\r\nvoid __iomem *mmio, *peer_mmio;\r\nu64 base_addr, limit, reg_val;\r\nint bar;\r\nbar = ndev_mw_to_bar(ndev, idx);\r\nif (bar < 0)\r\nreturn bar;\r\nmw_size = pci_resource_len(ndev->ntb.pdev, bar);\r\nif (size > mw_size)\r\nreturn -EINVAL;\r\nmmio = ndev->self_mmio;\r\npeer_mmio = ndev->peer_mmio;\r\nbase_addr = pci_resource_start(ndev->ntb.pdev, bar);\r\nif (bar != 1) {\r\nxlat_reg = AMD_BAR23XLAT_OFFSET + ((bar - 2) << 2);\r\nlimit_reg = AMD_BAR23LMT_OFFSET + ((bar - 2) << 2);\r\nlimit = size;\r\nwrite64(addr, peer_mmio + xlat_reg);\r\nreg_val = read64(peer_mmio + xlat_reg);\r\nif (reg_val != addr) {\r\nwrite64(0, peer_mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\nwrite64(limit, mmio + limit_reg);\r\nreg_val = read64(mmio + limit_reg);\r\nif (reg_val != limit) {\r\nwrite64(base_addr, mmio + limit_reg);\r\nwrite64(0, peer_mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\n} else {\r\nxlat_reg = AMD_BAR1XLAT_OFFSET;\r\nlimit_reg = AMD_BAR1LMT_OFFSET;\r\nlimit = size;\r\nwrite64(addr, peer_mmio + xlat_reg);\r\nreg_val = read64(peer_mmio + xlat_reg);\r\nif (reg_val != addr) {\r\nwrite64(0, peer_mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\nwritel(limit, mmio + limit_reg);\r\nreg_val = readl(mmio + limit_reg);\r\nif (reg_val != limit) {\r\nwritel(base_addr, mmio + limit_reg);\r\nwritel(0, peer_mmio + xlat_reg);\r\nreturn -EIO;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int amd_link_is_up(struct amd_ntb_dev *ndev)\r\n{\r\nif (!ndev->peer_sta)\r\nreturn NTB_LNK_STA_ACTIVE(ndev->cntl_sta);\r\nif (ndev->peer_sta & AMD_LINK_UP_EVENT) {\r\nndev->peer_sta = 0;\r\nreturn 1;\r\n}\r\nif (ndev->peer_sta & AMD_PEER_RESET_EVENT)\r\nndev->peer_sta &= ~AMD_PEER_RESET_EVENT;\r\nelse if (ndev->peer_sta & (AMD_PEER_D0_EVENT | AMD_LINK_DOWN_EVENT))\r\nndev->peer_sta = 0;\r\nreturn 0;\r\n}\r\nstatic int amd_ntb_link_is_up(struct ntb_dev *ntb,\r\nenum ntb_speed *speed,\r\nenum ntb_width *width)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nint ret = 0;\r\nif (amd_link_is_up(ndev)) {\r\nif (speed)\r\n*speed = NTB_LNK_STA_SPEED(ndev->lnk_sta);\r\nif (width)\r\n*width = NTB_LNK_STA_WIDTH(ndev->lnk_sta);\r\ndev_dbg(ndev_dev(ndev), "link is up.\n");\r\nret = 1;\r\n} else {\r\nif (speed)\r\n*speed = NTB_SPEED_NONE;\r\nif (width)\r\n*width = NTB_WIDTH_NONE;\r\ndev_dbg(ndev_dev(ndev), "link is down.\n");\r\n}\r\nreturn ret;\r\n}\r\nstatic int amd_ntb_link_enable(struct ntb_dev *ntb,\r\nenum ntb_speed max_speed,\r\nenum ntb_width max_width)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 ntb_ctl;\r\nndev->int_mask &= ~AMD_EVENT_INTMASK;\r\nwritel(ndev->int_mask, mmio + AMD_INTMASK_OFFSET);\r\nif (ndev->ntb.topo == NTB_TOPO_SEC)\r\nreturn -EINVAL;\r\ndev_dbg(ndev_dev(ndev), "Enabling Link.\n");\r\nntb_ctl = readl(mmio + AMD_CNTL_OFFSET);\r\nntb_ctl |= (PMM_REG_CTL | SMM_REG_CTL);\r\nwritel(ntb_ctl, mmio + AMD_CNTL_OFFSET);\r\nreturn 0;\r\n}\r\nstatic int amd_ntb_link_disable(struct ntb_dev *ntb)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 ntb_ctl;\r\nndev->int_mask |= AMD_EVENT_INTMASK;\r\nwritel(ndev->int_mask, mmio + AMD_INTMASK_OFFSET);\r\nif (ndev->ntb.topo == NTB_TOPO_SEC)\r\nreturn -EINVAL;\r\ndev_dbg(ndev_dev(ndev), "Enabling Link.\n");\r\nntb_ctl = readl(mmio + AMD_CNTL_OFFSET);\r\nntb_ctl &= ~(PMM_REG_CTL | SMM_REG_CTL);\r\nwritel(ntb_ctl, mmio + AMD_CNTL_OFFSET);\r\nreturn 0;\r\n}\r\nstatic u64 amd_ntb_db_valid_mask(struct ntb_dev *ntb)\r\n{\r\nreturn ntb_ndev(ntb)->db_valid_mask;\r\n}\r\nstatic int amd_ntb_db_vector_count(struct ntb_dev *ntb)\r\n{\r\nreturn ntb_ndev(ntb)->db_count;\r\n}\r\nstatic u64 amd_ntb_db_vector_mask(struct ntb_dev *ntb, int db_vector)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nif (db_vector < 0 || db_vector > ndev->db_count)\r\nreturn 0;\r\nreturn ntb_ndev(ntb)->db_valid_mask & (1 << db_vector);\r\n}\r\nstatic u64 amd_ntb_db_read(struct ntb_dev *ntb)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nreturn (u64)readw(mmio + AMD_DBSTAT_OFFSET);\r\n}\r\nstatic int amd_ntb_db_clear(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nwritew((u16)db_bits, mmio + AMD_DBSTAT_OFFSET);\r\nreturn 0;\r\n}\r\nstatic int amd_ntb_db_set_mask(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nunsigned long flags;\r\nif (db_bits & ~ndev->db_valid_mask)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&ndev->db_mask_lock, flags);\r\nndev->db_mask |= db_bits;\r\nwritew((u16)ndev->db_mask, mmio + AMD_DBMASK_OFFSET);\r\nspin_unlock_irqrestore(&ndev->db_mask_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int amd_ntb_db_clear_mask(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nunsigned long flags;\r\nif (db_bits & ~ndev->db_valid_mask)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&ndev->db_mask_lock, flags);\r\nndev->db_mask &= ~db_bits;\r\nwritew((u16)ndev->db_mask, mmio + AMD_DBMASK_OFFSET);\r\nspin_unlock_irqrestore(&ndev->db_mask_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int amd_ntb_peer_db_set(struct ntb_dev *ntb, u64 db_bits)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nwritew((u16)db_bits, mmio + AMD_DBREQ_OFFSET);\r\nreturn 0;\r\n}\r\nstatic int amd_ntb_spad_count(struct ntb_dev *ntb)\r\n{\r\nreturn ntb_ndev(ntb)->spad_count;\r\n}\r\nstatic u32 amd_ntb_spad_read(struct ntb_dev *ntb, int idx)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 offset;\r\nif (idx < 0 || idx >= ndev->spad_count)\r\nreturn 0;\r\noffset = ndev->self_spad + (idx << 2);\r\nreturn readl(mmio + AMD_SPAD_OFFSET + offset);\r\n}\r\nstatic int amd_ntb_spad_write(struct ntb_dev *ntb,\r\nint idx, u32 val)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 offset;\r\nif (idx < 0 || idx >= ndev->spad_count)\r\nreturn -EINVAL;\r\noffset = ndev->self_spad + (idx << 2);\r\nwritel(val, mmio + AMD_SPAD_OFFSET + offset);\r\nreturn 0;\r\n}\r\nstatic u32 amd_ntb_peer_spad_read(struct ntb_dev *ntb, int idx)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 offset;\r\nif (idx < 0 || idx >= ndev->spad_count)\r\nreturn -EINVAL;\r\noffset = ndev->peer_spad + (idx << 2);\r\nreturn readl(mmio + AMD_SPAD_OFFSET + offset);\r\n}\r\nstatic int amd_ntb_peer_spad_write(struct ntb_dev *ntb,\r\nint idx, u32 val)\r\n{\r\nstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 offset;\r\nif (idx < 0 || idx >= ndev->spad_count)\r\nreturn -EINVAL;\r\noffset = ndev->peer_spad + (idx << 2);\r\nwritel(val, mmio + AMD_SPAD_OFFSET + offset);\r\nreturn 0;\r\n}\r\nstatic void amd_ack_smu(struct amd_ntb_dev *ndev, u32 bit)\r\n{\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nint reg;\r\nreg = readl(mmio + AMD_SMUACK_OFFSET);\r\nreg |= bit;\r\nwritel(reg, mmio + AMD_SMUACK_OFFSET);\r\nndev->peer_sta |= bit;\r\n}\r\nstatic void amd_handle_event(struct amd_ntb_dev *ndev, int vec)\r\n{\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 status;\r\nstatus = readl(mmio + AMD_INTSTAT_OFFSET);\r\nif (!(status & AMD_EVENT_INTMASK))\r\nreturn;\r\ndev_dbg(ndev_dev(ndev), "status = 0x%x and vec = %d\n", status, vec);\r\nstatus &= AMD_EVENT_INTMASK;\r\nswitch (status) {\r\ncase AMD_PEER_FLUSH_EVENT:\r\ndev_info(ndev_dev(ndev), "Flush is done.\n");\r\nbreak;\r\ncase AMD_PEER_RESET_EVENT:\r\namd_ack_smu(ndev, AMD_PEER_RESET_EVENT);\r\nntb_link_event(&ndev->ntb);\r\nschedule_delayed_work(&ndev->hb_timer, AMD_LINK_HB_TIMEOUT);\r\nbreak;\r\ncase AMD_PEER_D3_EVENT:\r\ncase AMD_PEER_PMETO_EVENT:\r\ncase AMD_LINK_UP_EVENT:\r\ncase AMD_LINK_DOWN_EVENT:\r\namd_ack_smu(ndev, status);\r\nntb_link_event(&ndev->ntb);\r\nbreak;\r\ncase AMD_PEER_D0_EVENT:\r\nmmio = ndev->peer_mmio;\r\nstatus = readl(mmio + AMD_PMESTAT_OFFSET);\r\nif (status & 0x1)\r\ndev_info(ndev_dev(ndev), "Wakeup is done.\n");\r\namd_ack_smu(ndev, AMD_PEER_D0_EVENT);\r\nschedule_delayed_work(&ndev->hb_timer,\r\nAMD_LINK_HB_TIMEOUT);\r\nbreak;\r\ndefault:\r\ndev_info(ndev_dev(ndev), "event status = 0x%x.\n", status);\r\nbreak;\r\n}\r\n}\r\nstatic irqreturn_t ndev_interrupt(struct amd_ntb_dev *ndev, int vec)\r\n{\r\ndev_dbg(ndev_dev(ndev), "vec %d\n", vec);\r\nif (vec > (AMD_DB_CNT - 1) || (ndev->msix_vec_count == 1))\r\namd_handle_event(ndev, vec);\r\nif (vec < AMD_DB_CNT)\r\nntb_db_event(&ndev->ntb, vec);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ndev_vec_isr(int irq, void *dev)\r\n{\r\nstruct amd_ntb_vec *nvec = dev;\r\nreturn ndev_interrupt(nvec->ndev, nvec->num);\r\n}\r\nstatic irqreturn_t ndev_irq_isr(int irq, void *dev)\r\n{\r\nstruct amd_ntb_dev *ndev = dev;\r\nreturn ndev_interrupt(ndev, irq - ndev_pdev(ndev)->irq);\r\n}\r\nstatic int ndev_init_isr(struct amd_ntb_dev *ndev,\r\nint msix_min, int msix_max)\r\n{\r\nstruct pci_dev *pdev;\r\nint rc, i, msix_count, node;\r\npdev = ndev_pdev(ndev);\r\nnode = dev_to_node(&pdev->dev);\r\nndev->db_mask = ndev->db_valid_mask;\r\nndev->vec = kzalloc_node(msix_max * sizeof(*ndev->vec),\r\nGFP_KERNEL, node);\r\nif (!ndev->vec)\r\ngoto err_msix_vec_alloc;\r\nndev->msix = kzalloc_node(msix_max * sizeof(*ndev->msix),\r\nGFP_KERNEL, node);\r\nif (!ndev->msix)\r\ngoto err_msix_alloc;\r\nfor (i = 0; i < msix_max; ++i)\r\nndev->msix[i].entry = i;\r\nmsix_count = pci_enable_msix_range(pdev, ndev->msix,\r\nmsix_min, msix_max);\r\nif (msix_count < 0)\r\ngoto err_msix_enable;\r\nif (msix_count < msix_min) {\r\npci_disable_msix(pdev);\r\ngoto err_msix_enable;\r\n}\r\nfor (i = 0; i < msix_count; ++i) {\r\nndev->vec[i].ndev = ndev;\r\nndev->vec[i].num = i;\r\nrc = request_irq(ndev->msix[i].vector, ndev_vec_isr, 0,\r\n"ndev_vec_isr", &ndev->vec[i]);\r\nif (rc)\r\ngoto err_msix_request;\r\n}\r\ndev_dbg(ndev_dev(ndev), "Using msix interrupts\n");\r\nndev->db_count = msix_min;\r\nndev->msix_vec_count = msix_max;\r\nreturn 0;\r\nerr_msix_request:\r\nwhile (i-- > 0)\r\nfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\r\npci_disable_msix(pdev);\r\nerr_msix_enable:\r\nkfree(ndev->msix);\r\nerr_msix_alloc:\r\nkfree(ndev->vec);\r\nerr_msix_vec_alloc:\r\nndev->msix = NULL;\r\nndev->vec = NULL;\r\nrc = pci_enable_msi(pdev);\r\nif (rc)\r\ngoto err_msi_enable;\r\nrc = request_irq(pdev->irq, ndev_irq_isr, 0,\r\n"ndev_irq_isr", ndev);\r\nif (rc)\r\ngoto err_msi_request;\r\ndev_dbg(ndev_dev(ndev), "Using msi interrupts\n");\r\nndev->db_count = 1;\r\nndev->msix_vec_count = 1;\r\nreturn 0;\r\nerr_msi_request:\r\npci_disable_msi(pdev);\r\nerr_msi_enable:\r\npci_intx(pdev, 1);\r\nrc = request_irq(pdev->irq, ndev_irq_isr, IRQF_SHARED,\r\n"ndev_irq_isr", ndev);\r\nif (rc)\r\ngoto err_intx_request;\r\ndev_dbg(ndev_dev(ndev), "Using intx interrupts\n");\r\nndev->db_count = 1;\r\nndev->msix_vec_count = 1;\r\nreturn 0;\r\nerr_intx_request:\r\nreturn rc;\r\n}\r\nstatic void ndev_deinit_isr(struct amd_ntb_dev *ndev)\r\n{\r\nstruct pci_dev *pdev;\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nint i;\r\npdev = ndev_pdev(ndev);\r\nndev->db_mask = ndev->db_valid_mask;\r\nwritel(ndev->db_mask, mmio + AMD_DBMASK_OFFSET);\r\nif (ndev->msix) {\r\ni = ndev->msix_vec_count;\r\nwhile (i--)\r\nfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\r\npci_disable_msix(pdev);\r\nkfree(ndev->msix);\r\nkfree(ndev->vec);\r\n} else {\r\nfree_irq(pdev->irq, ndev);\r\nif (pci_dev_msi_enabled(pdev))\r\npci_disable_msi(pdev);\r\nelse\r\npci_intx(pdev, 0);\r\n}\r\n}\r\nstatic ssize_t ndev_debugfs_read(struct file *filp, char __user *ubuf,\r\nsize_t count, loff_t *offp)\r\n{\r\nstruct amd_ntb_dev *ndev;\r\nvoid __iomem *mmio;\r\nchar *buf;\r\nsize_t buf_size;\r\nssize_t ret, off;\r\nunion { u64 v64; u32 v32; u16 v16; } u;\r\nndev = filp->private_data;\r\nmmio = ndev->self_mmio;\r\nbuf_size = min(count, 0x800ul);\r\nbuf = kmalloc(buf_size, GFP_KERNEL);\r\nif (!buf)\r\nreturn -ENOMEM;\r\noff = 0;\r\noff += scnprintf(buf + off, buf_size - off,\r\n"NTB Device Information:\n");\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Connection Topology -\t%s\n",\r\nntb_topo_string(ndev->ntb.topo));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LNK STA -\t\t%#06x\n", ndev->lnk_sta);\r\nif (!amd_link_is_up(ndev)) {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Status -\t\tDown\n");\r\n} else {\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Status -\t\tUp\n");\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Speed -\t\tPCI-E Gen %u\n",\r\nNTB_LNK_STA_SPEED(ndev->lnk_sta));\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Link Width -\t\tx%u\n",\r\nNTB_LNK_STA_WIDTH(ndev->lnk_sta));\r\n}\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Memory Window Count -\t%u\n", ndev->mw_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Scratchpad Count -\t%u\n", ndev->spad_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Count -\t%u\n", ndev->db_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"MSIX Vector Count -\t%u\n", ndev->msix_vec_count);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Valid Mask -\t%#llx\n", ndev->db_valid_mask);\r\nu.v32 = readl(ndev->self_mmio + AMD_DBMASK_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Mask -\t\t\t%#06x\n", u.v32);\r\nu.v32 = readl(mmio + AMD_DBSTAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"Doorbell Bell -\t\t\t%#06x\n", u.v32);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"\nNTB Incoming XLAT:\n");\r\nu.v64 = read64(mmio + AMD_BAR1XLAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"XLAT1 -\t\t%#018llx\n", u.v64);\r\nu.v64 = read64(ndev->self_mmio + AMD_BAR23XLAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"XLAT23 -\t\t%#018llx\n", u.v64);\r\nu.v64 = read64(ndev->self_mmio + AMD_BAR45XLAT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"XLAT45 -\t\t%#018llx\n", u.v64);\r\nu.v32 = readl(mmio + AMD_BAR1LMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LMT1 -\t\t\t%#06x\n", u.v32);\r\nu.v64 = read64(ndev->self_mmio + AMD_BAR23LMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LMT23 -\t\t\t%#018llx\n", u.v64);\r\nu.v64 = read64(ndev->self_mmio + AMD_BAR45LMT_OFFSET);\r\noff += scnprintf(buf + off, buf_size - off,\r\n"LMT45 -\t\t\t%#018llx\n", u.v64);\r\nret = simple_read_from_buffer(ubuf, count, offp, buf, off);\r\nkfree(buf);\r\nreturn ret;\r\n}\r\nstatic void ndev_init_debugfs(struct amd_ntb_dev *ndev)\r\n{\r\nif (!debugfs_dir) {\r\nndev->debugfs_dir = NULL;\r\nndev->debugfs_info = NULL;\r\n} else {\r\nndev->debugfs_dir =\r\ndebugfs_create_dir(ndev_name(ndev), debugfs_dir);\r\nif (!ndev->debugfs_dir)\r\nndev->debugfs_info = NULL;\r\nelse\r\nndev->debugfs_info =\r\ndebugfs_create_file("info", S_IRUSR,\r\nndev->debugfs_dir, ndev,\r\n&amd_ntb_debugfs_info);\r\n}\r\n}\r\nstatic void ndev_deinit_debugfs(struct amd_ntb_dev *ndev)\r\n{\r\ndebugfs_remove_recursive(ndev->debugfs_dir);\r\n}\r\nstatic inline void ndev_init_struct(struct amd_ntb_dev *ndev,\r\nstruct pci_dev *pdev)\r\n{\r\nndev->ntb.pdev = pdev;\r\nndev->ntb.topo = NTB_TOPO_NONE;\r\nndev->ntb.ops = &amd_ntb_ops;\r\nndev->int_mask = AMD_EVENT_INTMASK;\r\nspin_lock_init(&ndev->db_mask_lock);\r\n}\r\nstatic int amd_poll_link(struct amd_ntb_dev *ndev)\r\n{\r\nvoid __iomem *mmio = ndev->peer_mmio;\r\nu32 reg, stat;\r\nint rc;\r\nreg = readl(mmio + AMD_SIDEINFO_OFFSET);\r\nreg &= NTB_LIN_STA_ACTIVE_BIT;\r\ndev_dbg(ndev_dev(ndev), "%s: reg_val = 0x%x.\n", __func__, reg);\r\nif (reg == ndev->cntl_sta)\r\nreturn 0;\r\nndev->cntl_sta = reg;\r\nrc = pci_read_config_dword(ndev->ntb.pdev,\r\nAMD_LINK_STATUS_OFFSET, &stat);\r\nif (rc)\r\nreturn 0;\r\nndev->lnk_sta = stat;\r\nreturn 1;\r\n}\r\nstatic void amd_link_hb(struct work_struct *work)\r\n{\r\nstruct amd_ntb_dev *ndev = hb_ndev(work);\r\nif (amd_poll_link(ndev))\r\nntb_link_event(&ndev->ntb);\r\nif (!amd_link_is_up(ndev))\r\nschedule_delayed_work(&ndev->hb_timer, AMD_LINK_HB_TIMEOUT);\r\n}\r\nstatic int amd_init_isr(struct amd_ntb_dev *ndev)\r\n{\r\nreturn ndev_init_isr(ndev, AMD_DB_CNT, AMD_MSIX_VECTOR_CNT);\r\n}\r\nstatic void amd_init_side_info(struct amd_ntb_dev *ndev)\r\n{\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nunsigned int reg;\r\nreg = readl(mmio + AMD_SIDEINFO_OFFSET);\r\nif (!(reg & AMD_SIDE_READY)) {\r\nreg |= AMD_SIDE_READY;\r\nwritel(reg, mmio + AMD_SIDEINFO_OFFSET);\r\n}\r\n}\r\nstatic void amd_deinit_side_info(struct amd_ntb_dev *ndev)\r\n{\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nunsigned int reg;\r\nreg = readl(mmio + AMD_SIDEINFO_OFFSET);\r\nif (reg & AMD_SIDE_READY) {\r\nreg &= ~AMD_SIDE_READY;\r\nwritel(reg, mmio + AMD_SIDEINFO_OFFSET);\r\nreadl(mmio + AMD_SIDEINFO_OFFSET);\r\n}\r\n}\r\nstatic int amd_init_ntb(struct amd_ntb_dev *ndev)\r\n{\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nndev->mw_count = AMD_MW_CNT;\r\nndev->spad_count = AMD_SPADS_CNT;\r\nndev->db_count = AMD_DB_CNT;\r\nswitch (ndev->ntb.topo) {\r\ncase NTB_TOPO_PRI:\r\ncase NTB_TOPO_SEC:\r\nndev->spad_count >>= 1;\r\nif (ndev->ntb.topo == NTB_TOPO_PRI) {\r\nndev->self_spad = 0;\r\nndev->peer_spad = 0x20;\r\n} else {\r\nndev->self_spad = 0x20;\r\nndev->peer_spad = 0;\r\n}\r\nINIT_DELAYED_WORK(&ndev->hb_timer, amd_link_hb);\r\nschedule_delayed_work(&ndev->hb_timer, AMD_LINK_HB_TIMEOUT);\r\nbreak;\r\ndefault:\r\ndev_err(ndev_dev(ndev), "AMD NTB does not support B2B mode.\n");\r\nreturn -EINVAL;\r\n}\r\nndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\r\nwritel(ndev->int_mask, mmio + AMD_INTMASK_OFFSET);\r\nreturn 0;\r\n}\r\nstatic enum ntb_topo amd_get_topo(struct amd_ntb_dev *ndev)\r\n{\r\nvoid __iomem *mmio = ndev->self_mmio;\r\nu32 info;\r\ninfo = readl(mmio + AMD_SIDEINFO_OFFSET);\r\nif (info & AMD_SIDE_MASK)\r\nreturn NTB_TOPO_SEC;\r\nelse\r\nreturn NTB_TOPO_PRI;\r\n}\r\nstatic int amd_init_dev(struct amd_ntb_dev *ndev)\r\n{\r\nstruct pci_dev *pdev;\r\nint rc = 0;\r\npdev = ndev_pdev(ndev);\r\nndev->ntb.topo = amd_get_topo(ndev);\r\ndev_dbg(ndev_dev(ndev), "AMD NTB topo is %s\n",\r\nntb_topo_string(ndev->ntb.topo));\r\nrc = amd_init_ntb(ndev);\r\nif (rc)\r\nreturn rc;\r\nrc = amd_init_isr(ndev);\r\nif (rc) {\r\ndev_err(ndev_dev(ndev), "fail to init isr.\n");\r\nreturn rc;\r\n}\r\nndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\r\nreturn 0;\r\n}\r\nstatic void amd_deinit_dev(struct amd_ntb_dev *ndev)\r\n{\r\ncancel_delayed_work_sync(&ndev->hb_timer);\r\nndev_deinit_isr(ndev);\r\n}\r\nstatic int amd_ntb_init_pci(struct amd_ntb_dev *ndev,\r\nstruct pci_dev *pdev)\r\n{\r\nint rc;\r\npci_set_drvdata(pdev, ndev);\r\nrc = pci_enable_device(pdev);\r\nif (rc)\r\ngoto err_pci_enable;\r\nrc = pci_request_regions(pdev, NTB_NAME);\r\nif (rc)\r\ngoto err_pci_regions;\r\npci_set_master(pdev);\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (rc) {\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc)\r\ngoto err_dma_mask;\r\ndev_warn(ndev_dev(ndev), "Cannot DMA highmem\n");\r\n}\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (rc) {\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc)\r\ngoto err_dma_mask;\r\ndev_warn(ndev_dev(ndev), "Cannot DMA consistent highmem\n");\r\n}\r\nndev->self_mmio = pci_iomap(pdev, 0, 0);\r\nif (!ndev->self_mmio) {\r\nrc = -EIO;\r\ngoto err_dma_mask;\r\n}\r\nndev->peer_mmio = ndev->self_mmio + AMD_PEER_OFFSET;\r\nreturn 0;\r\nerr_dma_mask:\r\npci_clear_master(pdev);\r\nerr_pci_regions:\r\npci_disable_device(pdev);\r\nerr_pci_enable:\r\npci_set_drvdata(pdev, NULL);\r\nreturn rc;\r\n}\r\nstatic void amd_ntb_deinit_pci(struct amd_ntb_dev *ndev)\r\n{\r\nstruct pci_dev *pdev = ndev_pdev(ndev);\r\npci_iounmap(pdev, ndev->self_mmio);\r\npci_clear_master(pdev);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\n}\r\nstatic int amd_ntb_pci_probe(struct pci_dev *pdev,\r\nconst struct pci_device_id *id)\r\n{\r\nstruct amd_ntb_dev *ndev;\r\nint rc, node;\r\nnode = dev_to_node(&pdev->dev);\r\nndev = kzalloc_node(sizeof(*ndev), GFP_KERNEL, node);\r\nif (!ndev) {\r\nrc = -ENOMEM;\r\ngoto err_ndev;\r\n}\r\nndev_init_struct(ndev, pdev);\r\nrc = amd_ntb_init_pci(ndev, pdev);\r\nif (rc)\r\ngoto err_init_pci;\r\nrc = amd_init_dev(ndev);\r\nif (rc)\r\ngoto err_init_dev;\r\namd_init_side_info(ndev);\r\namd_poll_link(ndev);\r\nndev_init_debugfs(ndev);\r\nrc = ntb_register_device(&ndev->ntb);\r\nif (rc)\r\ngoto err_register;\r\ndev_info(&pdev->dev, "NTB device registered.\n");\r\nreturn 0;\r\nerr_register:\r\nndev_deinit_debugfs(ndev);\r\namd_deinit_dev(ndev);\r\nerr_init_dev:\r\namd_ntb_deinit_pci(ndev);\r\nerr_init_pci:\r\nkfree(ndev);\r\nerr_ndev:\r\nreturn rc;\r\n}\r\nstatic void amd_ntb_pci_remove(struct pci_dev *pdev)\r\n{\r\nstruct amd_ntb_dev *ndev = pci_get_drvdata(pdev);\r\nntb_unregister_device(&ndev->ntb);\r\nndev_deinit_debugfs(ndev);\r\namd_deinit_side_info(ndev);\r\namd_deinit_dev(ndev);\r\namd_ntb_deinit_pci(ndev);\r\nkfree(ndev);\r\n}\r\nstatic int __init amd_ntb_pci_driver_init(void)\r\n{\r\npr_info("%s %s\n", NTB_DESC, NTB_VER);\r\nif (debugfs_initialized())\r\ndebugfs_dir = debugfs_create_dir(KBUILD_MODNAME, NULL);\r\nreturn pci_register_driver(&amd_ntb_pci_driver);\r\n}\r\nstatic void __exit amd_ntb_pci_driver_exit(void)\r\n{\r\npci_unregister_driver(&amd_ntb_pci_driver);\r\ndebugfs_remove_recursive(debugfs_dir);\r\n}
