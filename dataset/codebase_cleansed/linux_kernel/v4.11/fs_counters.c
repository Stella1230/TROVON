static void mlx5_fc_stats_insert(struct rb_root *root, struct mlx5_fc *counter)\r\n{\r\nstruct rb_node **new = &root->rb_node;\r\nstruct rb_node *parent = NULL;\r\nwhile (*new) {\r\nstruct mlx5_fc *this = rb_entry(*new, struct mlx5_fc, node);\r\nint result = counter->id - this->id;\r\nparent = *new;\r\nif (result < 0)\r\nnew = &((*new)->rb_left);\r\nelse\r\nnew = &((*new)->rb_right);\r\n}\r\nrb_link_node(&counter->node, parent, new);\r\nrb_insert_color(&counter->node, root);\r\n}\r\nstatic struct rb_node *mlx5_fc_stats_query(struct mlx5_core_dev *dev,\r\nstruct mlx5_fc *first,\r\nu16 last_id)\r\n{\r\nstruct mlx5_cmd_fc_bulk *b;\r\nstruct rb_node *node = NULL;\r\nu16 afirst_id;\r\nint num;\r\nint err;\r\nint max_bulk = 1 << MLX5_CAP_GEN(dev, log_max_flow_counter_bulk);\r\nafirst_id = first->id & ~0x3;\r\nnum = ALIGN(last_id - afirst_id + 1, 4);\r\nif (num > max_bulk) {\r\nnum = max_bulk;\r\nlast_id = afirst_id + num - 1;\r\n}\r\nb = mlx5_cmd_fc_bulk_alloc(dev, afirst_id, num);\r\nif (!b) {\r\nmlx5_core_err(dev, "Error allocating resources for bulk query\n");\r\nreturn NULL;\r\n}\r\nerr = mlx5_cmd_fc_bulk_query(dev, b);\r\nif (err) {\r\nmlx5_core_err(dev, "Error doing bulk query: %d\n", err);\r\ngoto out;\r\n}\r\nfor (node = &first->node; node; node = rb_next(node)) {\r\nstruct mlx5_fc *counter = rb_entry(node, struct mlx5_fc, node);\r\nstruct mlx5_fc_cache *c = &counter->cache;\r\nu64 packets;\r\nu64 bytes;\r\nif (counter->id > last_id)\r\nbreak;\r\nmlx5_cmd_fc_bulk_get(dev, b,\r\ncounter->id, &packets, &bytes);\r\nif (c->packets == packets)\r\ncontinue;\r\nc->packets = packets;\r\nc->bytes = bytes;\r\nc->lastuse = jiffies;\r\n}\r\nout:\r\nmlx5_cmd_fc_bulk_free(b);\r\nreturn node;\r\n}\r\nstatic void mlx5_fc_stats_work(struct work_struct *work)\r\n{\r\nstruct mlx5_core_dev *dev = container_of(work, struct mlx5_core_dev,\r\npriv.fc_stats.work.work);\r\nstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\r\nunsigned long now = jiffies;\r\nstruct mlx5_fc *counter = NULL;\r\nstruct mlx5_fc *last = NULL;\r\nstruct rb_node *node;\r\nLIST_HEAD(tmplist);\r\nspin_lock(&fc_stats->addlist_lock);\r\nlist_splice_tail_init(&fc_stats->addlist, &tmplist);\r\nif (!list_empty(&tmplist) || !RB_EMPTY_ROOT(&fc_stats->counters))\r\nqueue_delayed_work(fc_stats->wq, &fc_stats->work, MLX5_FC_STATS_PERIOD);\r\nspin_unlock(&fc_stats->addlist_lock);\r\nlist_for_each_entry(counter, &tmplist, list)\r\nmlx5_fc_stats_insert(&fc_stats->counters, counter);\r\nnode = rb_first(&fc_stats->counters);\r\nwhile (node) {\r\ncounter = rb_entry(node, struct mlx5_fc, node);\r\nnode = rb_next(node);\r\nif (counter->deleted) {\r\nrb_erase(&counter->node, &fc_stats->counters);\r\nmlx5_cmd_fc_free(dev, counter->id);\r\nkfree(counter);\r\ncontinue;\r\n}\r\nlast = counter;\r\n}\r\nif (time_before(now, fc_stats->next_query) || !last)\r\nreturn;\r\nnode = rb_first(&fc_stats->counters);\r\nwhile (node) {\r\ncounter = rb_entry(node, struct mlx5_fc, node);\r\nnode = mlx5_fc_stats_query(dev, counter, last->id);\r\n}\r\nfc_stats->next_query = now + MLX5_FC_STATS_PERIOD;\r\n}\r\nstruct mlx5_fc *mlx5_fc_create(struct mlx5_core_dev *dev, bool aging)\r\n{\r\nstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\r\nstruct mlx5_fc *counter;\r\nint err;\r\ncounter = kzalloc(sizeof(*counter), GFP_KERNEL);\r\nif (!counter)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = mlx5_cmd_fc_alloc(dev, &counter->id);\r\nif (err)\r\ngoto err_out;\r\nif (aging) {\r\ncounter->cache.lastuse = jiffies;\r\ncounter->aging = true;\r\nspin_lock(&fc_stats->addlist_lock);\r\nlist_add(&counter->list, &fc_stats->addlist);\r\nspin_unlock(&fc_stats->addlist_lock);\r\nmod_delayed_work(fc_stats->wq, &fc_stats->work, 0);\r\n}\r\nreturn counter;\r\nerr_out:\r\nkfree(counter);\r\nreturn ERR_PTR(err);\r\n}\r\nvoid mlx5_fc_destroy(struct mlx5_core_dev *dev, struct mlx5_fc *counter)\r\n{\r\nstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\r\nif (!counter)\r\nreturn;\r\nif (counter->aging) {\r\ncounter->deleted = true;\r\nmod_delayed_work(fc_stats->wq, &fc_stats->work, 0);\r\nreturn;\r\n}\r\nmlx5_cmd_fc_free(dev, counter->id);\r\nkfree(counter);\r\n}\r\nint mlx5_init_fc_stats(struct mlx5_core_dev *dev)\r\n{\r\nstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\r\nfc_stats->counters = RB_ROOT;\r\nINIT_LIST_HEAD(&fc_stats->addlist);\r\nspin_lock_init(&fc_stats->addlist_lock);\r\nfc_stats->wq = create_singlethread_workqueue("mlx5_fc");\r\nif (!fc_stats->wq)\r\nreturn -ENOMEM;\r\nINIT_DELAYED_WORK(&fc_stats->work, mlx5_fc_stats_work);\r\nreturn 0;\r\n}\r\nvoid mlx5_cleanup_fc_stats(struct mlx5_core_dev *dev)\r\n{\r\nstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\r\nstruct mlx5_fc *counter;\r\nstruct mlx5_fc *tmp;\r\nstruct rb_node *node;\r\ncancel_delayed_work_sync(&dev->priv.fc_stats.work);\r\ndestroy_workqueue(dev->priv.fc_stats.wq);\r\ndev->priv.fc_stats.wq = NULL;\r\nlist_for_each_entry_safe(counter, tmp, &fc_stats->addlist, list) {\r\nlist_del(&counter->list);\r\nmlx5_cmd_fc_free(dev, counter->id);\r\nkfree(counter);\r\n}\r\nnode = rb_first(&fc_stats->counters);\r\nwhile (node) {\r\ncounter = rb_entry(node, struct mlx5_fc, node);\r\nnode = rb_next(node);\r\nrb_erase(&counter->node, &fc_stats->counters);\r\nmlx5_cmd_fc_free(dev, counter->id);\r\nkfree(counter);\r\n}\r\n}\r\nvoid mlx5_fc_query_cached(struct mlx5_fc *counter,\r\nu64 *bytes, u64 *packets, u64 *lastuse)\r\n{\r\nstruct mlx5_fc_cache c;\r\nc = counter->cache;\r\n*bytes = c.bytes - counter->lastbytes;\r\n*packets = c.packets - counter->lastpackets;\r\n*lastuse = c.lastuse;\r\ncounter->lastbytes = c.bytes;\r\ncounter->lastpackets = c.packets;\r\n}
