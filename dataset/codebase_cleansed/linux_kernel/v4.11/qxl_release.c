static const char *qxl_get_driver_name(struct dma_fence *fence)\r\n{\r\nreturn "qxl";\r\n}\r\nstatic const char *qxl_get_timeline_name(struct dma_fence *fence)\r\n{\r\nreturn "release";\r\n}\r\nstatic bool qxl_nop_signaling(struct dma_fence *fence)\r\n{\r\nreturn true;\r\n}\r\nstatic long qxl_fence_wait(struct dma_fence *fence, bool intr,\r\nsigned long timeout)\r\n{\r\nstruct qxl_device *qdev;\r\nstruct qxl_release *release;\r\nint count = 0, sc = 0;\r\nbool have_drawable_releases;\r\nunsigned long cur, end = jiffies + timeout;\r\nqdev = container_of(fence->lock, struct qxl_device, release_lock);\r\nrelease = container_of(fence, struct qxl_release, base);\r\nhave_drawable_releases = release->type == QXL_RELEASE_DRAWABLE;\r\nretry:\r\nsc++;\r\nif (dma_fence_is_signaled(fence))\r\ngoto signaled;\r\nqxl_io_notify_oom(qdev);\r\nfor (count = 0; count < 11; count++) {\r\nif (!qxl_queue_garbage_collect(qdev, true))\r\nbreak;\r\nif (dma_fence_is_signaled(fence))\r\ngoto signaled;\r\n}\r\nif (dma_fence_is_signaled(fence))\r\ngoto signaled;\r\nif (have_drawable_releases || sc < 4) {\r\nif (sc > 2)\r\nusleep_range(500, 1000);\r\nif (time_after(jiffies, end))\r\nreturn 0;\r\nif (have_drawable_releases && sc > 300) {\r\nDMA_FENCE_WARN(fence, "failed to wait on release %llu "\r\n"after spincount %d\n",\r\nfence->context & ~0xf0000000, sc);\r\ngoto signaled;\r\n}\r\ngoto retry;\r\n}\r\nsignaled:\r\ncur = jiffies;\r\nif (time_after(cur, end))\r\nreturn 0;\r\nreturn end - cur;\r\n}\r\nstatic int\r\nqxl_release_alloc(struct qxl_device *qdev, int type,\r\nstruct qxl_release **ret)\r\n{\r\nstruct qxl_release *release;\r\nint handle;\r\nsize_t size = sizeof(*release);\r\nrelease = kmalloc(size, GFP_KERNEL);\r\nif (!release) {\r\nDRM_ERROR("Out of memory\n");\r\nreturn -ENOMEM;\r\n}\r\nrelease->base.ops = NULL;\r\nrelease->type = type;\r\nrelease->release_offset = 0;\r\nrelease->surface_release_id = 0;\r\nINIT_LIST_HEAD(&release->bos);\r\nidr_preload(GFP_KERNEL);\r\nspin_lock(&qdev->release_idr_lock);\r\nhandle = idr_alloc(&qdev->release_idr, release, 1, 0, GFP_NOWAIT);\r\nrelease->base.seqno = ++qdev->release_seqno;\r\nspin_unlock(&qdev->release_idr_lock);\r\nidr_preload_end();\r\nif (handle < 0) {\r\nkfree(release);\r\n*ret = NULL;\r\nreturn handle;\r\n}\r\n*ret = release;\r\nQXL_INFO(qdev, "allocated release %d\n", handle);\r\nrelease->id = handle;\r\nreturn handle;\r\n}\r\nstatic void\r\nqxl_release_free_list(struct qxl_release *release)\r\n{\r\nwhile (!list_empty(&release->bos)) {\r\nstruct qxl_bo_list *entry;\r\nstruct qxl_bo *bo;\r\nentry = container_of(release->bos.next,\r\nstruct qxl_bo_list, tv.head);\r\nbo = to_qxl_bo(entry->tv.bo);\r\nqxl_bo_unref(&bo);\r\nlist_del(&entry->tv.head);\r\nkfree(entry);\r\n}\r\n}\r\nvoid\r\nqxl_release_free(struct qxl_device *qdev,\r\nstruct qxl_release *release)\r\n{\r\nQXL_INFO(qdev, "release %d, type %d\n", release->id,\r\nrelease->type);\r\nif (release->surface_release_id)\r\nqxl_surface_id_dealloc(qdev, release->surface_release_id);\r\nspin_lock(&qdev->release_idr_lock);\r\nidr_remove(&qdev->release_idr, release->id);\r\nspin_unlock(&qdev->release_idr_lock);\r\nif (release->base.ops) {\r\nWARN_ON(list_empty(&release->bos));\r\nqxl_release_free_list(release);\r\ndma_fence_signal(&release->base);\r\ndma_fence_put(&release->base);\r\n} else {\r\nqxl_release_free_list(release);\r\nkfree(release);\r\n}\r\n}\r\nstatic int qxl_release_bo_alloc(struct qxl_device *qdev,\r\nstruct qxl_bo **bo)\r\n{\r\nreturn qxl_bo_create(qdev, PAGE_SIZE, false, true,\r\nQXL_GEM_DOMAIN_VRAM, NULL, bo);\r\n}\r\nint qxl_release_list_add(struct qxl_release *release, struct qxl_bo *bo)\r\n{\r\nstruct qxl_bo_list *entry;\r\nlist_for_each_entry(entry, &release->bos, tv.head) {\r\nif (entry->tv.bo == &bo->tbo)\r\nreturn 0;\r\n}\r\nentry = kmalloc(sizeof(struct qxl_bo_list), GFP_KERNEL);\r\nif (!entry)\r\nreturn -ENOMEM;\r\nqxl_bo_ref(bo);\r\nentry->tv.bo = &bo->tbo;\r\nentry->tv.shared = false;\r\nlist_add_tail(&entry->tv.head, &release->bos);\r\nreturn 0;\r\n}\r\nstatic int qxl_release_validate_bo(struct qxl_bo *bo)\r\n{\r\nint ret;\r\nif (!bo->pin_count) {\r\nqxl_ttm_placement_from_domain(bo, bo->type, false);\r\nret = ttm_bo_validate(&bo->tbo, &bo->placement,\r\ntrue, false);\r\nif (ret)\r\nreturn ret;\r\n}\r\nret = reservation_object_reserve_shared(bo->tbo.resv);\r\nif (ret)\r\nreturn ret;\r\nret = qxl_bo_check_id(bo->gem_base.dev->dev_private, bo);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nint qxl_release_reserve_list(struct qxl_release *release, bool no_intr)\r\n{\r\nint ret;\r\nstruct qxl_bo_list *entry;\r\nif (list_is_singular(&release->bos))\r\nreturn 0;\r\nret = ttm_eu_reserve_buffers(&release->ticket, &release->bos,\r\n!no_intr, NULL);\r\nif (ret)\r\nreturn ret;\r\nlist_for_each_entry(entry, &release->bos, tv.head) {\r\nstruct qxl_bo *bo = to_qxl_bo(entry->tv.bo);\r\nret = qxl_release_validate_bo(bo);\r\nif (ret) {\r\nttm_eu_backoff_reservation(&release->ticket, &release->bos);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid qxl_release_backoff_reserve_list(struct qxl_release *release)\r\n{\r\nif (list_is_singular(&release->bos))\r\nreturn;\r\nttm_eu_backoff_reservation(&release->ticket, &release->bos);\r\n}\r\nint qxl_alloc_surface_release_reserved(struct qxl_device *qdev,\r\nenum qxl_surface_cmd_type surface_cmd_type,\r\nstruct qxl_release *create_rel,\r\nstruct qxl_release **release)\r\n{\r\nif (surface_cmd_type == QXL_SURFACE_CMD_DESTROY && create_rel) {\r\nint idr_ret;\r\nstruct qxl_bo_list *entry = list_first_entry(&create_rel->bos, struct qxl_bo_list, tv.head);\r\nstruct qxl_bo *bo;\r\nunion qxl_release_info *info;\r\nidr_ret = qxl_release_alloc(qdev, QXL_RELEASE_SURFACE_CMD, release);\r\nif (idr_ret < 0)\r\nreturn idr_ret;\r\nbo = to_qxl_bo(entry->tv.bo);\r\n(*release)->release_offset = create_rel->release_offset + 64;\r\nqxl_release_list_add(*release, bo);\r\ninfo = qxl_release_map(qdev, *release);\r\ninfo->id = idr_ret;\r\nqxl_release_unmap(qdev, *release, info);\r\nreturn 0;\r\n}\r\nreturn qxl_alloc_release_reserved(qdev, sizeof(struct qxl_surface_cmd),\r\nQXL_RELEASE_SURFACE_CMD, release, NULL);\r\n}\r\nint qxl_alloc_release_reserved(struct qxl_device *qdev, unsigned long size,\r\nint type, struct qxl_release **release,\r\nstruct qxl_bo **rbo)\r\n{\r\nstruct qxl_bo *bo;\r\nint idr_ret;\r\nint ret = 0;\r\nunion qxl_release_info *info;\r\nint cur_idx;\r\nif (type == QXL_RELEASE_DRAWABLE)\r\ncur_idx = 0;\r\nelse if (type == QXL_RELEASE_SURFACE_CMD)\r\ncur_idx = 1;\r\nelse if (type == QXL_RELEASE_CURSOR_CMD)\r\ncur_idx = 2;\r\nelse {\r\nDRM_ERROR("got illegal type: %d\n", type);\r\nreturn -EINVAL;\r\n}\r\nidr_ret = qxl_release_alloc(qdev, type, release);\r\nif (idr_ret < 0) {\r\nif (rbo)\r\n*rbo = NULL;\r\nreturn idr_ret;\r\n}\r\nmutex_lock(&qdev->release_mutex);\r\nif (qdev->current_release_bo_offset[cur_idx] + 1 >= releases_per_bo[cur_idx]) {\r\nqxl_bo_unref(&qdev->current_release_bo[cur_idx]);\r\nqdev->current_release_bo_offset[cur_idx] = 0;\r\nqdev->current_release_bo[cur_idx] = NULL;\r\n}\r\nif (!qdev->current_release_bo[cur_idx]) {\r\nret = qxl_release_bo_alloc(qdev, &qdev->current_release_bo[cur_idx]);\r\nif (ret) {\r\nmutex_unlock(&qdev->release_mutex);\r\nqxl_release_free(qdev, *release);\r\nreturn ret;\r\n}\r\n}\r\nbo = qxl_bo_ref(qdev->current_release_bo[cur_idx]);\r\n(*release)->release_offset = qdev->current_release_bo_offset[cur_idx] * release_size_per_bo[cur_idx];\r\nqdev->current_release_bo_offset[cur_idx]++;\r\nif (rbo)\r\n*rbo = bo;\r\nmutex_unlock(&qdev->release_mutex);\r\nret = qxl_release_list_add(*release, bo);\r\nqxl_bo_unref(&bo);\r\nif (ret) {\r\nqxl_release_free(qdev, *release);\r\nreturn ret;\r\n}\r\ninfo = qxl_release_map(qdev, *release);\r\ninfo->id = idr_ret;\r\nqxl_release_unmap(qdev, *release, info);\r\nreturn ret;\r\n}\r\nstruct qxl_release *qxl_release_from_id_locked(struct qxl_device *qdev,\r\nuint64_t id)\r\n{\r\nstruct qxl_release *release;\r\nspin_lock(&qdev->release_idr_lock);\r\nrelease = idr_find(&qdev->release_idr, id);\r\nspin_unlock(&qdev->release_idr_lock);\r\nif (!release) {\r\nDRM_ERROR("failed to find id in release_idr\n");\r\nreturn NULL;\r\n}\r\nreturn release;\r\n}\r\nunion qxl_release_info *qxl_release_map(struct qxl_device *qdev,\r\nstruct qxl_release *release)\r\n{\r\nvoid *ptr;\r\nunion qxl_release_info *info;\r\nstruct qxl_bo_list *entry = list_first_entry(&release->bos, struct qxl_bo_list, tv.head);\r\nstruct qxl_bo *bo = to_qxl_bo(entry->tv.bo);\r\nptr = qxl_bo_kmap_atomic_page(qdev, bo, release->release_offset & PAGE_SIZE);\r\nif (!ptr)\r\nreturn NULL;\r\ninfo = ptr + (release->release_offset & ~PAGE_SIZE);\r\nreturn info;\r\n}\r\nvoid qxl_release_unmap(struct qxl_device *qdev,\r\nstruct qxl_release *release,\r\nunion qxl_release_info *info)\r\n{\r\nstruct qxl_bo_list *entry = list_first_entry(&release->bos, struct qxl_bo_list, tv.head);\r\nstruct qxl_bo *bo = to_qxl_bo(entry->tv.bo);\r\nvoid *ptr;\r\nptr = ((void *)info) - (release->release_offset & ~PAGE_SIZE);\r\nqxl_bo_kunmap_atomic_page(qdev, bo, ptr);\r\n}\r\nvoid qxl_release_fence_buffer_objects(struct qxl_release *release)\r\n{\r\nstruct ttm_buffer_object *bo;\r\nstruct ttm_bo_global *glob;\r\nstruct ttm_bo_device *bdev;\r\nstruct ttm_bo_driver *driver;\r\nstruct qxl_bo *qbo;\r\nstruct ttm_validate_buffer *entry;\r\nstruct qxl_device *qdev;\r\nif (list_is_singular(&release->bos) || list_empty(&release->bos))\r\nreturn;\r\nbo = list_first_entry(&release->bos, struct ttm_validate_buffer, head)->bo;\r\nbdev = bo->bdev;\r\nqdev = container_of(bdev, struct qxl_device, mman.bdev);\r\ndma_fence_init(&release->base, &qxl_fence_ops, &qdev->release_lock,\r\nrelease->id | 0xf0000000, release->base.seqno);\r\ntrace_dma_fence_emit(&release->base);\r\ndriver = bdev->driver;\r\nglob = bo->glob;\r\nspin_lock(&glob->lru_lock);\r\nlist_for_each_entry(entry, &release->bos, head) {\r\nbo = entry->bo;\r\nqbo = to_qxl_bo(bo);\r\nreservation_object_add_shared_fence(bo->resv, &release->base);\r\nttm_bo_add_to_lru(bo);\r\n__ttm_bo_unreserve(bo);\r\n}\r\nspin_unlock(&glob->lru_lock);\r\nww_acquire_fini(&release->ticket);\r\n}
