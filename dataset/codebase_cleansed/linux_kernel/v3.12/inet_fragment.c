static void inet_frag_secret_rebuild(unsigned long dummy)\r\n{\r\nstruct inet_frags *f = (struct inet_frags *)dummy;\r\nunsigned long now = jiffies;\r\nint i;\r\nwrite_lock(&f->lock);\r\nget_random_bytes(&f->rnd, sizeof(u32));\r\nfor (i = 0; i < INETFRAGS_HASHSZ; i++) {\r\nstruct inet_frag_bucket *hb;\r\nstruct inet_frag_queue *q;\r\nstruct hlist_node *n;\r\nhb = &f->hash[i];\r\nhlist_for_each_entry_safe(q, n, &hb->chain, list) {\r\nunsigned int hval = f->hashfn(q);\r\nif (hval != i) {\r\nstruct inet_frag_bucket *hb_dest;\r\nhlist_del(&q->list);\r\nhb_dest = &f->hash[hval];\r\nhlist_add_head(&q->list, &hb_dest->chain);\r\n}\r\n}\r\n}\r\nwrite_unlock(&f->lock);\r\nmod_timer(&f->secret_timer, now + f->secret_interval);\r\n}\r\nvoid inet_frags_init(struct inet_frags *f)\r\n{\r\nint i;\r\nfor (i = 0; i < INETFRAGS_HASHSZ; i++) {\r\nstruct inet_frag_bucket *hb = &f->hash[i];\r\nspin_lock_init(&hb->chain_lock);\r\nINIT_HLIST_HEAD(&hb->chain);\r\n}\r\nrwlock_init(&f->lock);\r\nf->rnd = (u32) ((totalram_pages ^ (totalram_pages >> 7)) ^\r\n(jiffies ^ (jiffies >> 6)));\r\nsetup_timer(&f->secret_timer, inet_frag_secret_rebuild,\r\n(unsigned long)f);\r\nf->secret_timer.expires = jiffies + f->secret_interval;\r\nadd_timer(&f->secret_timer);\r\n}\r\nvoid inet_frags_init_net(struct netns_frags *nf)\r\n{\r\nnf->nqueues = 0;\r\ninit_frag_mem_limit(nf);\r\nINIT_LIST_HEAD(&nf->lru_list);\r\nspin_lock_init(&nf->lru_lock);\r\n}\r\nvoid inet_frags_fini(struct inet_frags *f)\r\n{\r\ndel_timer(&f->secret_timer);\r\n}\r\nvoid inet_frags_exit_net(struct netns_frags *nf, struct inet_frags *f)\r\n{\r\nnf->low_thresh = 0;\r\nlocal_bh_disable();\r\ninet_frag_evictor(nf, f, true);\r\nlocal_bh_enable();\r\npercpu_counter_destroy(&nf->mem);\r\n}\r\nstatic inline void fq_unlink(struct inet_frag_queue *fq, struct inet_frags *f)\r\n{\r\nstruct inet_frag_bucket *hb;\r\nunsigned int hash;\r\nread_lock(&f->lock);\r\nhash = f->hashfn(fq);\r\nhb = &f->hash[hash];\r\nspin_lock(&hb->chain_lock);\r\nhlist_del(&fq->list);\r\nspin_unlock(&hb->chain_lock);\r\nread_unlock(&f->lock);\r\ninet_frag_lru_del(fq);\r\n}\r\nvoid inet_frag_kill(struct inet_frag_queue *fq, struct inet_frags *f)\r\n{\r\nif (del_timer(&fq->timer))\r\natomic_dec(&fq->refcnt);\r\nif (!(fq->last_in & INET_FRAG_COMPLETE)) {\r\nfq_unlink(fq, f);\r\natomic_dec(&fq->refcnt);\r\nfq->last_in |= INET_FRAG_COMPLETE;\r\n}\r\n}\r\nstatic inline void frag_kfree_skb(struct netns_frags *nf, struct inet_frags *f,\r\nstruct sk_buff *skb)\r\n{\r\nif (f->skb_free)\r\nf->skb_free(skb);\r\nkfree_skb(skb);\r\n}\r\nvoid inet_frag_destroy(struct inet_frag_queue *q, struct inet_frags *f,\r\nint *work)\r\n{\r\nstruct sk_buff *fp;\r\nstruct netns_frags *nf;\r\nunsigned int sum, sum_truesize = 0;\r\nWARN_ON(!(q->last_in & INET_FRAG_COMPLETE));\r\nWARN_ON(del_timer(&q->timer) != 0);\r\nfp = q->fragments;\r\nnf = q->net;\r\nwhile (fp) {\r\nstruct sk_buff *xp = fp->next;\r\nsum_truesize += fp->truesize;\r\nfrag_kfree_skb(nf, f, fp);\r\nfp = xp;\r\n}\r\nsum = sum_truesize + f->qsize;\r\nif (work)\r\n*work -= sum;\r\nsub_frag_mem_limit(q, sum);\r\nif (f->destructor)\r\nf->destructor(q);\r\nkfree(q);\r\n}\r\nint inet_frag_evictor(struct netns_frags *nf, struct inet_frags *f, bool force)\r\n{\r\nstruct inet_frag_queue *q;\r\nint work, evicted = 0;\r\nif (!force) {\r\nif (frag_mem_limit(nf) <= nf->high_thresh)\r\nreturn 0;\r\n}\r\nwork = frag_mem_limit(nf) - nf->low_thresh;\r\nwhile (work > 0) {\r\nspin_lock(&nf->lru_lock);\r\nif (list_empty(&nf->lru_list)) {\r\nspin_unlock(&nf->lru_lock);\r\nbreak;\r\n}\r\nq = list_first_entry(&nf->lru_list,\r\nstruct inet_frag_queue, lru_list);\r\natomic_inc(&q->refcnt);\r\nlist_del_init(&q->lru_list);\r\nspin_unlock(&nf->lru_lock);\r\nspin_lock(&q->lock);\r\nif (!(q->last_in & INET_FRAG_COMPLETE))\r\ninet_frag_kill(q, f);\r\nspin_unlock(&q->lock);\r\nif (atomic_dec_and_test(&q->refcnt))\r\ninet_frag_destroy(q, f, &work);\r\nevicted++;\r\n}\r\nreturn evicted;\r\n}\r\nstatic struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\r\nstruct inet_frag_queue *qp_in, struct inet_frags *f,\r\nvoid *arg)\r\n{\r\nstruct inet_frag_bucket *hb;\r\nstruct inet_frag_queue *qp;\r\nunsigned int hash;\r\nread_lock(&f->lock);\r\nhash = f->hashfn(qp_in);\r\nhb = &f->hash[hash];\r\nspin_lock(&hb->chain_lock);\r\n#ifdef CONFIG_SMP\r\nhlist_for_each_entry(qp, &hb->chain, list) {\r\nif (qp->net == nf && f->match(qp, arg)) {\r\natomic_inc(&qp->refcnt);\r\nspin_unlock(&hb->chain_lock);\r\nread_unlock(&f->lock);\r\nqp_in->last_in |= INET_FRAG_COMPLETE;\r\ninet_frag_put(qp_in, f);\r\nreturn qp;\r\n}\r\n}\r\n#endif\r\nqp = qp_in;\r\nif (!mod_timer(&qp->timer, jiffies + nf->timeout))\r\natomic_inc(&qp->refcnt);\r\natomic_inc(&qp->refcnt);\r\nhlist_add_head(&qp->list, &hb->chain);\r\nspin_unlock(&hb->chain_lock);\r\nread_unlock(&f->lock);\r\ninet_frag_lru_add(nf, qp);\r\nreturn qp;\r\n}\r\nstatic struct inet_frag_queue *inet_frag_alloc(struct netns_frags *nf,\r\nstruct inet_frags *f, void *arg)\r\n{\r\nstruct inet_frag_queue *q;\r\nq = kzalloc(f->qsize, GFP_ATOMIC);\r\nif (q == NULL)\r\nreturn NULL;\r\nq->net = nf;\r\nf->constructor(q, arg);\r\nadd_frag_mem_limit(q, f->qsize);\r\nsetup_timer(&q->timer, f->frag_expire, (unsigned long)q);\r\nspin_lock_init(&q->lock);\r\natomic_set(&q->refcnt, 1);\r\nINIT_LIST_HEAD(&q->lru_list);\r\nreturn q;\r\n}\r\nstatic struct inet_frag_queue *inet_frag_create(struct netns_frags *nf,\r\nstruct inet_frags *f, void *arg)\r\n{\r\nstruct inet_frag_queue *q;\r\nq = inet_frag_alloc(nf, f, arg);\r\nif (q == NULL)\r\nreturn NULL;\r\nreturn inet_frag_intern(nf, q, f, arg);\r\n}\r\nstruct inet_frag_queue *inet_frag_find(struct netns_frags *nf,\r\nstruct inet_frags *f, void *key, unsigned int hash)\r\n__releases(&f->lock\r\nvoid inet_frag_maybe_warn_overflow(struct inet_frag_queue *q,\r\nconst char *prefix)\r\n{\r\nstatic const char msg[] = "inet_frag_find: Fragment hash bucket"\r\n" list length grew over limit " __stringify(INETFRAGS_MAXDEPTH)\r\n". Dropping fragment.\n";\r\nif (PTR_ERR(q) == -ENOBUFS)\r\nLIMIT_NETDEBUG(KERN_WARNING "%s%s", prefix, msg);\r\n}
