int radeon_gart_table_ram_alloc(struct radeon_device *rdev)\r\n{\r\nvoid *ptr;\r\nptr = pci_alloc_consistent(rdev->pdev, rdev->gart.table_size,\r\n&rdev->gart.table_addr);\r\nif (ptr == NULL) {\r\nreturn -ENOMEM;\r\n}\r\n#ifdef CONFIG_X86\r\nif (rdev->family == CHIP_RS400 || rdev->family == CHIP_RS480 ||\r\nrdev->family == CHIP_RS690 || rdev->family == CHIP_RS740) {\r\nset_memory_uc((unsigned long)ptr,\r\nrdev->gart.table_size >> PAGE_SHIFT);\r\n}\r\n#endif\r\nrdev->gart.ptr = ptr;\r\nmemset((void *)rdev->gart.ptr, 0, rdev->gart.table_size);\r\nreturn 0;\r\n}\r\nvoid radeon_gart_table_ram_free(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.ptr == NULL) {\r\nreturn;\r\n}\r\n#ifdef CONFIG_X86\r\nif (rdev->family == CHIP_RS400 || rdev->family == CHIP_RS480 ||\r\nrdev->family == CHIP_RS690 || rdev->family == CHIP_RS740) {\r\nset_memory_wb((unsigned long)rdev->gart.ptr,\r\nrdev->gart.table_size >> PAGE_SHIFT);\r\n}\r\n#endif\r\npci_free_consistent(rdev->pdev, rdev->gart.table_size,\r\n(void *)rdev->gart.ptr,\r\nrdev->gart.table_addr);\r\nrdev->gart.ptr = NULL;\r\nrdev->gart.table_addr = 0;\r\n}\r\nint radeon_gart_table_vram_alloc(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (rdev->gart.robj == NULL) {\r\nr = radeon_bo_create(rdev, rdev->gart.table_size,\r\nPAGE_SIZE, true, RADEON_GEM_DOMAIN_VRAM,\r\nNULL, &rdev->gart.robj);\r\nif (r) {\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint radeon_gart_table_vram_pin(struct radeon_device *rdev)\r\n{\r\nuint64_t gpu_addr;\r\nint r;\r\nr = radeon_bo_reserve(rdev->gart.robj, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nr = radeon_bo_pin(rdev->gart.robj,\r\nRADEON_GEM_DOMAIN_VRAM, &gpu_addr);\r\nif (r) {\r\nradeon_bo_unreserve(rdev->gart.robj);\r\nreturn r;\r\n}\r\nr = radeon_bo_kmap(rdev->gart.robj, &rdev->gart.ptr);\r\nif (r)\r\nradeon_bo_unpin(rdev->gart.robj);\r\nradeon_bo_unreserve(rdev->gart.robj);\r\nrdev->gart.table_addr = gpu_addr;\r\nreturn r;\r\n}\r\nvoid radeon_gart_table_vram_unpin(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (rdev->gart.robj == NULL) {\r\nreturn;\r\n}\r\nr = radeon_bo_reserve(rdev->gart.robj, false);\r\nif (likely(r == 0)) {\r\nradeon_bo_kunmap(rdev->gart.robj);\r\nradeon_bo_unpin(rdev->gart.robj);\r\nradeon_bo_unreserve(rdev->gart.robj);\r\nrdev->gart.ptr = NULL;\r\n}\r\n}\r\nvoid radeon_gart_table_vram_free(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.robj == NULL) {\r\nreturn;\r\n}\r\nradeon_bo_unref(&rdev->gart.robj);\r\n}\r\nvoid radeon_gart_unbind(struct radeon_device *rdev, unsigned offset,\r\nint pages)\r\n{\r\nunsigned t;\r\nunsigned p;\r\nint i, j;\r\nu64 page_base;\r\nif (!rdev->gart.ready) {\r\nWARN(1, "trying to unbind memory from uninitialized GART !\n");\r\nreturn;\r\n}\r\nt = offset / RADEON_GPU_PAGE_SIZE;\r\np = t / (PAGE_SIZE / RADEON_GPU_PAGE_SIZE);\r\nfor (i = 0; i < pages; i++, p++) {\r\nif (rdev->gart.pages[p]) {\r\nrdev->gart.pages[p] = NULL;\r\nrdev->gart.pages_addr[p] = rdev->dummy_page.addr;\r\npage_base = rdev->gart.pages_addr[p];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nif (rdev->gart.ptr) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\n}\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\n}\r\nint radeon_gart_bind(struct radeon_device *rdev, unsigned offset,\r\nint pages, struct page **pagelist, dma_addr_t *dma_addr)\r\n{\r\nunsigned t;\r\nunsigned p;\r\nuint64_t page_base;\r\nint i, j;\r\nif (!rdev->gart.ready) {\r\nWARN(1, "trying to bind memory to uninitialized GART !\n");\r\nreturn -EINVAL;\r\n}\r\nt = offset / RADEON_GPU_PAGE_SIZE;\r\np = t / (PAGE_SIZE / RADEON_GPU_PAGE_SIZE);\r\nfor (i = 0; i < pages; i++, p++) {\r\nrdev->gart.pages_addr[p] = dma_addr[i];\r\nrdev->gart.pages[p] = pagelist[i];\r\nif (rdev->gart.ptr) {\r\npage_base = rdev->gart.pages_addr[p];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\nreturn 0;\r\n}\r\nvoid radeon_gart_restore(struct radeon_device *rdev)\r\n{\r\nint i, j, t;\r\nu64 page_base;\r\nif (!rdev->gart.ptr) {\r\nreturn;\r\n}\r\nfor (i = 0, t = 0; i < rdev->gart.num_cpu_pages; i++) {\r\npage_base = rdev->gart.pages_addr[i];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\n}\r\nint radeon_gart_init(struct radeon_device *rdev)\r\n{\r\nint r, i;\r\nif (rdev->gart.pages) {\r\nreturn 0;\r\n}\r\nif (PAGE_SIZE < RADEON_GPU_PAGE_SIZE) {\r\nDRM_ERROR("Page size is smaller than GPU page size!\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_dummy_page_init(rdev);\r\nif (r)\r\nreturn r;\r\nrdev->gart.num_cpu_pages = rdev->mc.gtt_size / PAGE_SIZE;\r\nrdev->gart.num_gpu_pages = rdev->mc.gtt_size / RADEON_GPU_PAGE_SIZE;\r\nDRM_INFO("GART: num cpu pages %u, num gpu pages %u\n",\r\nrdev->gart.num_cpu_pages, rdev->gart.num_gpu_pages);\r\nrdev->gart.pages = vzalloc(sizeof(void *) * rdev->gart.num_cpu_pages);\r\nif (rdev->gart.pages == NULL) {\r\nradeon_gart_fini(rdev);\r\nreturn -ENOMEM;\r\n}\r\nrdev->gart.pages_addr = vzalloc(sizeof(dma_addr_t) *\r\nrdev->gart.num_cpu_pages);\r\nif (rdev->gart.pages_addr == NULL) {\r\nradeon_gart_fini(rdev);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < rdev->gart.num_cpu_pages; i++) {\r\nrdev->gart.pages_addr[i] = rdev->dummy_page.addr;\r\n}\r\nreturn 0;\r\n}\r\nvoid radeon_gart_fini(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.pages && rdev->gart.pages_addr && rdev->gart.ready) {\r\nradeon_gart_unbind(rdev, 0, rdev->gart.num_cpu_pages);\r\n}\r\nrdev->gart.ready = false;\r\nvfree(rdev->gart.pages);\r\nvfree(rdev->gart.pages_addr);\r\nrdev->gart.pages = NULL;\r\nrdev->gart.pages_addr = NULL;\r\nradeon_dummy_page_fini(rdev);\r\n}\r\nstatic unsigned radeon_vm_num_pdes(struct radeon_device *rdev)\r\n{\r\nreturn rdev->vm_manager.max_pfn >> RADEON_VM_BLOCK_SIZE;\r\n}\r\nstatic unsigned radeon_vm_directory_size(struct radeon_device *rdev)\r\n{\r\nreturn RADEON_GPU_PAGE_ALIGN(radeon_vm_num_pdes(rdev) * 8);\r\n}\r\nint radeon_vm_manager_init(struct radeon_device *rdev)\r\n{\r\nstruct radeon_vm *vm;\r\nstruct radeon_bo_va *bo_va;\r\nint r;\r\nunsigned size;\r\nif (!rdev->vm_manager.enabled) {\r\nsize = radeon_vm_directory_size(rdev);\r\nsize += rdev->vm_manager.max_pfn * 8;\r\nsize *= 2;\r\nr = radeon_sa_bo_manager_init(rdev, &rdev->vm_manager.sa_manager,\r\nRADEON_GPU_PAGE_ALIGN(size),\r\nRADEON_VM_PTB_ALIGN_SIZE,\r\nRADEON_GEM_DOMAIN_VRAM);\r\nif (r) {\r\ndev_err(rdev->dev, "failed to allocate vm bo (%dKB)\n",\r\n(rdev->vm_manager.max_pfn * 8) >> 10);\r\nreturn r;\r\n}\r\nr = radeon_asic_vm_init(rdev);\r\nif (r)\r\nreturn r;\r\nrdev->vm_manager.enabled = true;\r\nr = radeon_sa_bo_manager_start(rdev, &rdev->vm_manager.sa_manager);\r\nif (r)\r\nreturn r;\r\n}\r\nlist_for_each_entry(vm, &rdev->vm_manager.lru_vm, list) {\r\nif (vm->page_directory == NULL)\r\ncontinue;\r\nlist_for_each_entry(bo_va, &vm->va, vm_list) {\r\nbo_va->valid = false;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void radeon_vm_free_pt(struct radeon_device *rdev,\r\nstruct radeon_vm *vm)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nint i;\r\nif (!vm->page_directory)\r\nreturn;\r\nlist_del_init(&vm->list);\r\nradeon_sa_bo_free(rdev, &vm->page_directory, vm->fence);\r\nlist_for_each_entry(bo_va, &vm->va, vm_list) {\r\nbo_va->valid = false;\r\n}\r\nif (vm->page_tables == NULL)\r\nreturn;\r\nfor (i = 0; i < radeon_vm_num_pdes(rdev); i++)\r\nradeon_sa_bo_free(rdev, &vm->page_tables[i], vm->fence);\r\nkfree(vm->page_tables);\r\n}\r\nvoid radeon_vm_manager_fini(struct radeon_device *rdev)\r\n{\r\nstruct radeon_vm *vm, *tmp;\r\nint i;\r\nif (!rdev->vm_manager.enabled)\r\nreturn;\r\nmutex_lock(&rdev->vm_manager.lock);\r\nlist_for_each_entry_safe(vm, tmp, &rdev->vm_manager.lru_vm, list) {\r\nmutex_lock(&vm->mutex);\r\nradeon_vm_free_pt(rdev, vm);\r\nmutex_unlock(&vm->mutex);\r\n}\r\nfor (i = 0; i < RADEON_NUM_VM; ++i) {\r\nradeon_fence_unref(&rdev->vm_manager.active[i]);\r\n}\r\nradeon_asic_vm_fini(rdev);\r\nmutex_unlock(&rdev->vm_manager.lock);\r\nradeon_sa_bo_manager_suspend(rdev, &rdev->vm_manager.sa_manager);\r\nradeon_sa_bo_manager_fini(rdev, &rdev->vm_manager.sa_manager);\r\nrdev->vm_manager.enabled = false;\r\n}\r\nstatic int radeon_vm_evict(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nstruct radeon_vm *vm_evict;\r\nif (list_empty(&rdev->vm_manager.lru_vm))\r\nreturn -ENOMEM;\r\nvm_evict = list_first_entry(&rdev->vm_manager.lru_vm,\r\nstruct radeon_vm, list);\r\nif (vm_evict == vm)\r\nreturn -ENOMEM;\r\nmutex_lock(&vm_evict->mutex);\r\nradeon_vm_free_pt(rdev, vm_evict);\r\nmutex_unlock(&vm_evict->mutex);\r\nreturn 0;\r\n}\r\nint radeon_vm_alloc_pt(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nunsigned pd_size, pts_size;\r\nu64 *pd_addr;\r\nint r;\r\nif (vm == NULL) {\r\nreturn -EINVAL;\r\n}\r\nif (vm->page_directory != NULL) {\r\nreturn 0;\r\n}\r\nretry:\r\npd_size = radeon_vm_directory_size(rdev);\r\nr = radeon_sa_bo_new(rdev, &rdev->vm_manager.sa_manager,\r\n&vm->page_directory, pd_size,\r\nRADEON_VM_PTB_ALIGN_SIZE, false);\r\nif (r == -ENOMEM) {\r\nr = radeon_vm_evict(rdev, vm);\r\nif (r)\r\nreturn r;\r\ngoto retry;\r\n} else if (r) {\r\nreturn r;\r\n}\r\nvm->pd_gpu_addr = radeon_sa_bo_gpu_addr(vm->page_directory);\r\npd_addr = radeon_sa_bo_cpu_addr(vm->page_directory);\r\nmemset(pd_addr, 0, pd_size);\r\npts_size = radeon_vm_num_pdes(rdev) * sizeof(struct radeon_sa_bo *);\r\nvm->page_tables = kzalloc(pts_size, GFP_KERNEL);\r\nif (vm->page_tables == NULL) {\r\nDRM_ERROR("Cannot allocate memory for page table array\n");\r\nradeon_sa_bo_free(rdev, &vm->page_directory, vm->fence);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nvoid radeon_vm_add_to_lru(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nlist_del_init(&vm->list);\r\nlist_add_tail(&vm->list, &rdev->vm_manager.lru_vm);\r\n}\r\nstruct radeon_fence *radeon_vm_grab_id(struct radeon_device *rdev,\r\nstruct radeon_vm *vm, int ring)\r\n{\r\nstruct radeon_fence *best[RADEON_NUM_RINGS] = {};\r\nunsigned choices[2] = {};\r\nunsigned i;\r\nif (vm->fence && vm->fence == rdev->vm_manager.active[vm->id])\r\nreturn NULL;\r\nradeon_fence_unref(&vm->last_flush);\r\nfor (i = 1; i < rdev->vm_manager.nvm; ++i) {\r\nstruct radeon_fence *fence = rdev->vm_manager.active[i];\r\nif (fence == NULL) {\r\nvm->id = i;\r\nreturn NULL;\r\n}\r\nif (radeon_fence_is_earlier(fence, best[fence->ring])) {\r\nbest[fence->ring] = fence;\r\nchoices[fence->ring == ring ? 0 : 1] = i;\r\n}\r\n}\r\nfor (i = 0; i < 2; ++i) {\r\nif (choices[i]) {\r\nvm->id = choices[i];\r\nreturn rdev->vm_manager.active[choices[i]];\r\n}\r\n}\r\nBUG();\r\nreturn NULL;\r\n}\r\nvoid radeon_vm_fence(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_fence *fence)\r\n{\r\nradeon_fence_unref(&rdev->vm_manager.active[vm->id]);\r\nrdev->vm_manager.active[vm->id] = radeon_fence_ref(fence);\r\nradeon_fence_unref(&vm->fence);\r\nvm->fence = radeon_fence_ref(fence);\r\n}\r\nstruct radeon_bo_va *radeon_vm_bo_find(struct radeon_vm *vm,\r\nstruct radeon_bo *bo)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nlist_for_each_entry(bo_va, &bo->va, bo_list) {\r\nif (bo_va->vm == vm) {\r\nreturn bo_va;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstruct radeon_bo_va *radeon_vm_bo_add(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_bo *bo)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nbo_va = kzalloc(sizeof(struct radeon_bo_va), GFP_KERNEL);\r\nif (bo_va == NULL) {\r\nreturn NULL;\r\n}\r\nbo_va->vm = vm;\r\nbo_va->bo = bo;\r\nbo_va->soffset = 0;\r\nbo_va->eoffset = 0;\r\nbo_va->flags = 0;\r\nbo_va->valid = false;\r\nbo_va->ref_count = 1;\r\nINIT_LIST_HEAD(&bo_va->bo_list);\r\nINIT_LIST_HEAD(&bo_va->vm_list);\r\nmutex_lock(&vm->mutex);\r\nlist_add(&bo_va->vm_list, &vm->va);\r\nlist_add_tail(&bo_va->bo_list, &bo->va);\r\nmutex_unlock(&vm->mutex);\r\nreturn bo_va;\r\n}\r\nint radeon_vm_bo_set_addr(struct radeon_device *rdev,\r\nstruct radeon_bo_va *bo_va,\r\nuint64_t soffset,\r\nuint32_t flags)\r\n{\r\nuint64_t size = radeon_bo_size(bo_va->bo);\r\nuint64_t eoffset, last_offset = 0;\r\nstruct radeon_vm *vm = bo_va->vm;\r\nstruct radeon_bo_va *tmp;\r\nstruct list_head *head;\r\nunsigned last_pfn;\r\nif (soffset) {\r\neoffset = soffset + size;\r\nif (soffset >= eoffset) {\r\nreturn -EINVAL;\r\n}\r\nlast_pfn = eoffset / RADEON_GPU_PAGE_SIZE;\r\nif (last_pfn > rdev->vm_manager.max_pfn) {\r\ndev_err(rdev->dev, "va above limit (0x%08X > 0x%08X)\n",\r\nlast_pfn, rdev->vm_manager.max_pfn);\r\nreturn -EINVAL;\r\n}\r\n} else {\r\neoffset = last_pfn = 0;\r\n}\r\nmutex_lock(&vm->mutex);\r\nhead = &vm->va;\r\nlast_offset = 0;\r\nlist_for_each_entry(tmp, &vm->va, vm_list) {\r\nif (bo_va == tmp) {\r\ncontinue;\r\n}\r\nif (soffset >= last_offset && eoffset <= tmp->soffset) {\r\nbreak;\r\n}\r\nif (eoffset > tmp->soffset && soffset < tmp->eoffset) {\r\ndev_err(rdev->dev, "bo %p va 0x%08X conflict with (bo %p 0x%08X 0x%08X)\n",\r\nbo_va->bo, (unsigned)bo_va->soffset, tmp->bo,\r\n(unsigned)tmp->soffset, (unsigned)tmp->eoffset);\r\nmutex_unlock(&vm->mutex);\r\nreturn -EINVAL;\r\n}\r\nlast_offset = tmp->eoffset;\r\nhead = &tmp->vm_list;\r\n}\r\nbo_va->soffset = soffset;\r\nbo_va->eoffset = eoffset;\r\nbo_va->flags = flags;\r\nbo_va->valid = false;\r\nlist_move(&bo_va->vm_list, head);\r\nmutex_unlock(&vm->mutex);\r\nreturn 0;\r\n}\r\nuint64_t radeon_vm_map_gart(struct radeon_device *rdev, uint64_t addr)\r\n{\r\nuint64_t result;\r\nresult = rdev->gart.pages_addr[addr >> PAGE_SHIFT];\r\nresult |= addr & (~PAGE_MASK);\r\nreturn result;\r\n}\r\nstatic int radeon_vm_update_pdes(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_ib *ib,\r\nuint64_t start, uint64_t end)\r\n{\r\nstatic const uint32_t incr = RADEON_VM_PTE_COUNT * 8;\r\nuint64_t last_pde = ~0, last_pt = ~0;\r\nunsigned count = 0;\r\nuint64_t pt_idx;\r\nint r;\r\nstart = (start / RADEON_GPU_PAGE_SIZE) >> RADEON_VM_BLOCK_SIZE;\r\nend = (end / RADEON_GPU_PAGE_SIZE) >> RADEON_VM_BLOCK_SIZE;\r\nfor (pt_idx = start; pt_idx <= end; ++pt_idx) {\r\nuint64_t pde, pt;\r\nif (vm->page_tables[pt_idx])\r\ncontinue;\r\nretry:\r\nr = radeon_sa_bo_new(rdev, &rdev->vm_manager.sa_manager,\r\n&vm->page_tables[pt_idx],\r\nRADEON_VM_PTE_COUNT * 8,\r\nRADEON_GPU_PAGE_SIZE, false);\r\nif (r == -ENOMEM) {\r\nr = radeon_vm_evict(rdev, vm);\r\nif (r)\r\nreturn r;\r\ngoto retry;\r\n} else if (r) {\r\nreturn r;\r\n}\r\npde = vm->pd_gpu_addr + pt_idx * 8;\r\npt = radeon_sa_bo_gpu_addr(vm->page_tables[pt_idx]);\r\nif (((last_pde + 8 * count) != pde) ||\r\n((last_pt + incr * count) != pt)) {\r\nif (count) {\r\nradeon_asic_vm_set_page(rdev, ib, last_pde,\r\nlast_pt, count, incr,\r\nRADEON_VM_PAGE_VALID);\r\n}\r\ncount = 1;\r\nlast_pde = pde;\r\nlast_pt = pt;\r\n} else {\r\n++count;\r\n}\r\n}\r\nif (count) {\r\nradeon_asic_vm_set_page(rdev, ib, last_pde, last_pt, count,\r\nincr, RADEON_VM_PAGE_VALID);\r\n}\r\nreturn 0;\r\n}\r\nstatic void radeon_vm_update_ptes(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_ib *ib,\r\nuint64_t start, uint64_t end,\r\nuint64_t dst, uint32_t flags)\r\n{\r\nstatic const uint64_t mask = RADEON_VM_PTE_COUNT - 1;\r\nuint64_t last_pte = ~0, last_dst = ~0;\r\nunsigned count = 0;\r\nuint64_t addr;\r\nstart = start / RADEON_GPU_PAGE_SIZE;\r\nend = end / RADEON_GPU_PAGE_SIZE;\r\nfor (addr = start; addr < end; ) {\r\nuint64_t pt_idx = addr >> RADEON_VM_BLOCK_SIZE;\r\nunsigned nptes;\r\nuint64_t pte;\r\nif ((addr & ~mask) == (end & ~mask))\r\nnptes = end - addr;\r\nelse\r\nnptes = RADEON_VM_PTE_COUNT - (addr & mask);\r\npte = radeon_sa_bo_gpu_addr(vm->page_tables[pt_idx]);\r\npte += (addr & mask) * 8;\r\nif ((last_pte + 8 * count) != pte) {\r\nif (count) {\r\nradeon_asic_vm_set_page(rdev, ib, last_pte,\r\nlast_dst, count,\r\nRADEON_GPU_PAGE_SIZE,\r\nflags);\r\n}\r\ncount = nptes;\r\nlast_pte = pte;\r\nlast_dst = dst;\r\n} else {\r\ncount += nptes;\r\n}\r\naddr += nptes;\r\ndst += nptes * RADEON_GPU_PAGE_SIZE;\r\n}\r\nif (count) {\r\nradeon_asic_vm_set_page(rdev, ib, last_pte,\r\nlast_dst, count,\r\nRADEON_GPU_PAGE_SIZE, flags);\r\n}\r\n}\r\nint radeon_vm_bo_update_pte(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_bo *bo,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nunsigned ridx = rdev->asic->vm.pt_ring_index;\r\nstruct radeon_ib ib;\r\nstruct radeon_bo_va *bo_va;\r\nunsigned nptes, npdes, ndw;\r\nuint64_t addr;\r\nint r;\r\nif (vm->page_directory == NULL)\r\nreturn 0;\r\nbo_va = radeon_vm_bo_find(vm, bo);\r\nif (bo_va == NULL) {\r\ndev_err(rdev->dev, "bo %p not in vm %p\n", bo, vm);\r\nreturn -EINVAL;\r\n}\r\nif (!bo_va->soffset) {\r\ndev_err(rdev->dev, "bo %p don't has a mapping in vm %p\n",\r\nbo, vm);\r\nreturn -EINVAL;\r\n}\r\nif ((bo_va->valid && mem) || (!bo_va->valid && mem == NULL))\r\nreturn 0;\r\nbo_va->flags &= ~RADEON_VM_PAGE_VALID;\r\nbo_va->flags &= ~RADEON_VM_PAGE_SYSTEM;\r\nif (mem) {\r\naddr = mem->start << PAGE_SHIFT;\r\nif (mem->mem_type != TTM_PL_SYSTEM) {\r\nbo_va->flags |= RADEON_VM_PAGE_VALID;\r\nbo_va->valid = true;\r\n}\r\nif (mem->mem_type == TTM_PL_TT) {\r\nbo_va->flags |= RADEON_VM_PAGE_SYSTEM;\r\n} else {\r\naddr += rdev->vm_manager.vram_base_offset;\r\n}\r\n} else {\r\naddr = 0;\r\nbo_va->valid = false;\r\n}\r\nnptes = radeon_bo_ngpu_pages(bo);\r\nnpdes = (nptes >> RADEON_VM_BLOCK_SIZE) + 2;\r\nndw = 64;\r\nif (RADEON_VM_BLOCK_SIZE > 11)\r\nndw += (nptes >> 11) * 4;\r\nelse\r\nndw += (nptes >> RADEON_VM_BLOCK_SIZE) * 4;\r\nndw += nptes * 2;\r\nndw += (npdes >> 11) * 4;\r\nndw += npdes * 2;\r\nif (ndw > 0xfffff)\r\nreturn -ENOMEM;\r\nr = radeon_ib_get(rdev, ridx, &ib, NULL, ndw * 4);\r\nib.length_dw = 0;\r\nr = radeon_vm_update_pdes(rdev, vm, &ib, bo_va->soffset, bo_va->eoffset);\r\nif (r) {\r\nradeon_ib_free(rdev, &ib);\r\nreturn r;\r\n}\r\nradeon_vm_update_ptes(rdev, vm, &ib, bo_va->soffset, bo_va->eoffset,\r\naddr, bo_va->flags);\r\nradeon_ib_sync_to(&ib, vm->fence);\r\nr = radeon_ib_schedule(rdev, &ib, NULL);\r\nif (r) {\r\nradeon_ib_free(rdev, &ib);\r\nreturn r;\r\n}\r\nradeon_fence_unref(&vm->fence);\r\nvm->fence = radeon_fence_ref(ib.fence);\r\nradeon_ib_free(rdev, &ib);\r\nradeon_fence_unref(&vm->last_flush);\r\nreturn 0;\r\n}\r\nint radeon_vm_bo_rmv(struct radeon_device *rdev,\r\nstruct radeon_bo_va *bo_va)\r\n{\r\nint r = 0;\r\nmutex_lock(&rdev->vm_manager.lock);\r\nmutex_lock(&bo_va->vm->mutex);\r\nif (bo_va->soffset) {\r\nr = radeon_vm_bo_update_pte(rdev, bo_va->vm, bo_va->bo, NULL);\r\n}\r\nmutex_unlock(&rdev->vm_manager.lock);\r\nlist_del(&bo_va->vm_list);\r\nmutex_unlock(&bo_va->vm->mutex);\r\nlist_del(&bo_va->bo_list);\r\nkfree(bo_va);\r\nreturn r;\r\n}\r\nvoid radeon_vm_bo_invalidate(struct radeon_device *rdev,\r\nstruct radeon_bo *bo)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nlist_for_each_entry(bo_va, &bo->va, bo_list) {\r\nbo_va->valid = false;\r\n}\r\n}\r\nvoid radeon_vm_init(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nvm->id = 0;\r\nvm->fence = NULL;\r\nmutex_init(&vm->mutex);\r\nINIT_LIST_HEAD(&vm->list);\r\nINIT_LIST_HEAD(&vm->va);\r\n}\r\nvoid radeon_vm_fini(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nstruct radeon_bo_va *bo_va, *tmp;\r\nint r;\r\nmutex_lock(&rdev->vm_manager.lock);\r\nmutex_lock(&vm->mutex);\r\nradeon_vm_free_pt(rdev, vm);\r\nmutex_unlock(&rdev->vm_manager.lock);\r\nif (!list_empty(&vm->va)) {\r\ndev_err(rdev->dev, "still active bo inside vm\n");\r\n}\r\nlist_for_each_entry_safe(bo_va, tmp, &vm->va, vm_list) {\r\nlist_del_init(&bo_va->vm_list);\r\nr = radeon_bo_reserve(bo_va->bo, false);\r\nif (!r) {\r\nlist_del_init(&bo_va->bo_list);\r\nradeon_bo_unreserve(bo_va->bo);\r\nkfree(bo_va);\r\n}\r\n}\r\nradeon_fence_unref(&vm->fence);\r\nradeon_fence_unref(&vm->last_flush);\r\nmutex_unlock(&vm->mutex);\r\n}
