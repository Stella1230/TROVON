static inline unsigned long tsb_hash(unsigned long vaddr, unsigned long hash_shift, unsigned long nentries)\r\n{\r\nvaddr >>= hash_shift;\r\nreturn vaddr & (nentries - 1);\r\n}\r\nstatic inline int tag_compare(unsigned long tag, unsigned long vaddr)\r\n{\r\nreturn (tag == (vaddr >> 22));\r\n}\r\nvoid flush_tsb_kernel_range(unsigned long start, unsigned long end)\r\n{\r\nunsigned long v;\r\nfor (v = start; v < end; v += PAGE_SIZE) {\r\nunsigned long hash = tsb_hash(v, PAGE_SHIFT,\r\nKERNEL_TSB_NENTRIES);\r\nstruct tsb *ent = &swapper_tsb[hash];\r\nif (tag_compare(ent->tag, v))\r\nent->tag = (1UL << TSB_TAG_INVALID_BIT);\r\n}\r\n}\r\nstatic void __flush_tsb_one_entry(unsigned long tsb, unsigned long v,\r\nunsigned long hash_shift,\r\nunsigned long nentries)\r\n{\r\nunsigned long tag, ent, hash;\r\nv &= ~0x1UL;\r\nhash = tsb_hash(v, hash_shift, nentries);\r\nent = tsb + (hash * sizeof(struct tsb));\r\ntag = (v >> 22UL);\r\ntsb_flush(ent, tag);\r\n}\r\nstatic void __flush_tsb_one(struct tlb_batch *tb, unsigned long hash_shift,\r\nunsigned long tsb, unsigned long nentries)\r\n{\r\nunsigned long i;\r\nfor (i = 0; i < tb->tlb_nr; i++)\r\n__flush_tsb_one_entry(tsb, tb->vaddrs[i], hash_shift, nentries);\r\n}\r\nvoid flush_tsb_user(struct tlb_batch *tb)\r\n{\r\nstruct mm_struct *mm = tb->mm;\r\nunsigned long nentries, base, flags;\r\nspin_lock_irqsave(&mm->context.lock, flags);\r\nbase = (unsigned long) mm->context.tsb_block[MM_TSB_BASE].tsb;\r\nnentries = mm->context.tsb_block[MM_TSB_BASE].tsb_nentries;\r\nif (tlb_type == cheetah_plus || tlb_type == hypervisor)\r\nbase = __pa(base);\r\n__flush_tsb_one(tb, PAGE_SHIFT, base, nentries);\r\n#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)\r\nif (mm->context.tsb_block[MM_TSB_HUGE].tsb) {\r\nbase = (unsigned long) mm->context.tsb_block[MM_TSB_HUGE].tsb;\r\nnentries = mm->context.tsb_block[MM_TSB_HUGE].tsb_nentries;\r\nif (tlb_type == cheetah_plus || tlb_type == hypervisor)\r\nbase = __pa(base);\r\n__flush_tsb_one(tb, HPAGE_SHIFT, base, nentries);\r\n}\r\n#endif\r\nspin_unlock_irqrestore(&mm->context.lock, flags);\r\n}\r\nvoid flush_tsb_user_page(struct mm_struct *mm, unsigned long vaddr)\r\n{\r\nunsigned long nentries, base, flags;\r\nspin_lock_irqsave(&mm->context.lock, flags);\r\nbase = (unsigned long) mm->context.tsb_block[MM_TSB_BASE].tsb;\r\nnentries = mm->context.tsb_block[MM_TSB_BASE].tsb_nentries;\r\nif (tlb_type == cheetah_plus || tlb_type == hypervisor)\r\nbase = __pa(base);\r\n__flush_tsb_one_entry(base, vaddr, PAGE_SHIFT, nentries);\r\n#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)\r\nif (mm->context.tsb_block[MM_TSB_HUGE].tsb) {\r\nbase = (unsigned long) mm->context.tsb_block[MM_TSB_HUGE].tsb;\r\nnentries = mm->context.tsb_block[MM_TSB_HUGE].tsb_nentries;\r\nif (tlb_type == cheetah_plus || tlb_type == hypervisor)\r\nbase = __pa(base);\r\n__flush_tsb_one_entry(base, vaddr, HPAGE_SHIFT, nentries);\r\n}\r\n#endif\r\nspin_unlock_irqrestore(&mm->context.lock, flags);\r\n}\r\nstatic void setup_tsb_params(struct mm_struct *mm, unsigned long tsb_idx, unsigned long tsb_bytes)\r\n{\r\nunsigned long tsb_reg, base, tsb_paddr;\r\nunsigned long page_sz, tte;\r\nmm->context.tsb_block[tsb_idx].tsb_nentries =\r\ntsb_bytes / sizeof(struct tsb);\r\nbase = TSBMAP_BASE;\r\ntte = pgprot_val(PAGE_KERNEL_LOCKED);\r\ntsb_paddr = __pa(mm->context.tsb_block[tsb_idx].tsb);\r\nBUG_ON(tsb_paddr & (tsb_bytes - 1UL));\r\nswitch (tsb_bytes) {\r\ncase 8192 << 0:\r\ntsb_reg = 0x0UL;\r\n#ifdef DCACHE_ALIASING_POSSIBLE\r\nbase += (tsb_paddr & 8192);\r\n#endif\r\npage_sz = 8192;\r\nbreak;\r\ncase 8192 << 1:\r\ntsb_reg = 0x1UL;\r\npage_sz = 64 * 1024;\r\nbreak;\r\ncase 8192 << 2:\r\ntsb_reg = 0x2UL;\r\npage_sz = 64 * 1024;\r\nbreak;\r\ncase 8192 << 3:\r\ntsb_reg = 0x3UL;\r\npage_sz = 64 * 1024;\r\nbreak;\r\ncase 8192 << 4:\r\ntsb_reg = 0x4UL;\r\npage_sz = 512 * 1024;\r\nbreak;\r\ncase 8192 << 5:\r\ntsb_reg = 0x5UL;\r\npage_sz = 512 * 1024;\r\nbreak;\r\ncase 8192 << 6:\r\ntsb_reg = 0x6UL;\r\npage_sz = 512 * 1024;\r\nbreak;\r\ncase 8192 << 7:\r\ntsb_reg = 0x7UL;\r\npage_sz = 4 * 1024 * 1024;\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "TSB[%s:%d]: Impossible TSB size %lu, killing process.\n",\r\ncurrent->comm, current->pid, tsb_bytes);\r\ndo_exit(SIGSEGV);\r\n}\r\ntte |= pte_sz_bits(page_sz);\r\nif (tlb_type == cheetah_plus || tlb_type == hypervisor) {\r\ntsb_reg |= tsb_paddr;\r\nmm->context.tsb_block[tsb_idx].tsb_reg_val = tsb_reg;\r\nmm->context.tsb_block[tsb_idx].tsb_map_vaddr = 0;\r\nmm->context.tsb_block[tsb_idx].tsb_map_pte = 0;\r\n} else {\r\ntsb_reg |= base;\r\ntsb_reg |= (tsb_paddr & (page_sz - 1UL));\r\ntte |= (tsb_paddr & ~(page_sz - 1UL));\r\nmm->context.tsb_block[tsb_idx].tsb_reg_val = tsb_reg;\r\nmm->context.tsb_block[tsb_idx].tsb_map_vaddr = base;\r\nmm->context.tsb_block[tsb_idx].tsb_map_pte = tte;\r\n}\r\nif (tlb_type == hypervisor) {\r\nstruct hv_tsb_descr *hp = &mm->context.tsb_descr[tsb_idx];\r\nswitch (tsb_idx) {\r\ncase MM_TSB_BASE:\r\nhp->pgsz_idx = HV_PGSZ_IDX_BASE;\r\nbreak;\r\n#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)\r\ncase MM_TSB_HUGE:\r\nhp->pgsz_idx = HV_PGSZ_IDX_HUGE;\r\nbreak;\r\n#endif\r\ndefault:\r\nBUG();\r\n}\r\nhp->assoc = 1;\r\nhp->num_ttes = tsb_bytes / 16;\r\nhp->ctx_idx = 0;\r\nswitch (tsb_idx) {\r\ncase MM_TSB_BASE:\r\nhp->pgsz_mask = HV_PGSZ_MASK_BASE;\r\nbreak;\r\n#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)\r\ncase MM_TSB_HUGE:\r\nhp->pgsz_mask = HV_PGSZ_MASK_HUGE;\r\nbreak;\r\n#endif\r\ndefault:\r\nBUG();\r\n}\r\nhp->tsb_base = tsb_paddr;\r\nhp->resv = 0;\r\n}\r\n}\r\nvoid __init pgtable_cache_init(void)\r\n{\r\nunsigned long i;\r\npgtable_cache = kmem_cache_create("pgtable_cache",\r\nPAGE_SIZE, PAGE_SIZE,\r\n0,\r\n_clear_page);\r\nif (!pgtable_cache) {\r\nprom_printf("pgtable_cache_init(): Could not create!\n");\r\nprom_halt();\r\n}\r\nfor (i = 0; i < 8; i++) {\r\nunsigned long size = 8192 << i;\r\nconst char *name = tsb_cache_names[i];\r\ntsb_caches[i] = kmem_cache_create(name,\r\nsize, size,\r\n0, NULL);\r\nif (!tsb_caches[i]) {\r\nprom_printf("Could not create %s cache\n", name);\r\nprom_halt();\r\n}\r\n}\r\n}\r\nstatic unsigned long tsb_size_to_rss_limit(unsigned long new_size)\r\n{\r\nunsigned long num_ents = (new_size / sizeof(struct tsb));\r\nif (sysctl_tsb_ratio < 0)\r\nreturn num_ents - (num_ents >> -sysctl_tsb_ratio);\r\nelse\r\nreturn num_ents + (num_ents >> sysctl_tsb_ratio);\r\n}\r\nvoid tsb_grow(struct mm_struct *mm, unsigned long tsb_index, unsigned long rss)\r\n{\r\nunsigned long max_tsb_size = 1 * 1024 * 1024;\r\nunsigned long new_size, old_size, flags;\r\nstruct tsb *old_tsb, *new_tsb;\r\nunsigned long new_cache_index, old_cache_index;\r\nunsigned long new_rss_limit;\r\ngfp_t gfp_flags;\r\nif (max_tsb_size > (PAGE_SIZE << MAX_ORDER))\r\nmax_tsb_size = (PAGE_SIZE << MAX_ORDER);\r\nnew_cache_index = 0;\r\nfor (new_size = 8192; new_size < max_tsb_size; new_size <<= 1UL) {\r\nnew_rss_limit = tsb_size_to_rss_limit(new_size);\r\nif (new_rss_limit > rss)\r\nbreak;\r\nnew_cache_index++;\r\n}\r\nif (new_size == max_tsb_size)\r\nnew_rss_limit = ~0UL;\r\nretry_tsb_alloc:\r\ngfp_flags = GFP_KERNEL;\r\nif (new_size > (PAGE_SIZE * 2))\r\ngfp_flags |= __GFP_NOWARN | __GFP_NORETRY;\r\nnew_tsb = kmem_cache_alloc_node(tsb_caches[new_cache_index],\r\ngfp_flags, numa_node_id());\r\nif (unlikely(!new_tsb)) {\r\nif (mm->context.tsb_block[tsb_index].tsb == NULL &&\r\nnew_cache_index > 0) {\r\nnew_cache_index = 0;\r\nnew_size = 8192;\r\nnew_rss_limit = ~0UL;\r\ngoto retry_tsb_alloc;\r\n}\r\nif (mm->context.tsb_block[tsb_index].tsb != NULL)\r\nmm->context.tsb_block[tsb_index].tsb_rss_limit = ~0UL;\r\nreturn;\r\n}\r\ntsb_init(new_tsb, new_size);\r\nspin_lock_irqsave(&mm->context.lock, flags);\r\nold_tsb = mm->context.tsb_block[tsb_index].tsb;\r\nold_cache_index =\r\n(mm->context.tsb_block[tsb_index].tsb_reg_val & 0x7UL);\r\nold_size = (mm->context.tsb_block[tsb_index].tsb_nentries *\r\nsizeof(struct tsb));\r\nif (unlikely(old_tsb &&\r\n(rss < mm->context.tsb_block[tsb_index].tsb_rss_limit))) {\r\nspin_unlock_irqrestore(&mm->context.lock, flags);\r\nkmem_cache_free(tsb_caches[new_cache_index], new_tsb);\r\nreturn;\r\n}\r\nmm->context.tsb_block[tsb_index].tsb_rss_limit = new_rss_limit;\r\nif (old_tsb) {\r\nextern void copy_tsb(unsigned long old_tsb_base,\r\nunsigned long old_tsb_size,\r\nunsigned long new_tsb_base,\r\nunsigned long new_tsb_size);\r\nunsigned long old_tsb_base = (unsigned long) old_tsb;\r\nunsigned long new_tsb_base = (unsigned long) new_tsb;\r\nif (tlb_type == cheetah_plus || tlb_type == hypervisor) {\r\nold_tsb_base = __pa(old_tsb_base);\r\nnew_tsb_base = __pa(new_tsb_base);\r\n}\r\ncopy_tsb(old_tsb_base, old_size, new_tsb_base, new_size);\r\n}\r\nmm->context.tsb_block[tsb_index].tsb = new_tsb;\r\nsetup_tsb_params(mm, tsb_index, new_size);\r\nspin_unlock_irqrestore(&mm->context.lock, flags);\r\nif (old_tsb) {\r\ntsb_context_switch(mm);\r\npreempt_disable();\r\nsmp_tsb_sync(mm);\r\npreempt_enable();\r\nkmem_cache_free(tsb_caches[old_cache_index], old_tsb);\r\n}\r\n}\r\nint init_new_context(struct task_struct *tsk, struct mm_struct *mm)\r\n{\r\n#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)\r\nunsigned long huge_pte_count;\r\n#endif\r\nunsigned int i;\r\nspin_lock_init(&mm->context.lock);\r\nmm->context.sparc64_ctx_val = 0UL;\r\n#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)\r\nhuge_pte_count = mm->context.huge_pte_count;\r\nmm->context.huge_pte_count = 0;\r\n#endif\r\nmm->context.pgtable_page = NULL;\r\nfor (i = 0; i < MM_NUM_TSBS; i++)\r\nmm->context.tsb_block[i].tsb = NULL;\r\ntsb_grow(mm, MM_TSB_BASE, get_mm_rss(mm));\r\n#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)\r\nif (unlikely(huge_pte_count))\r\ntsb_grow(mm, MM_TSB_HUGE, huge_pte_count);\r\n#endif\r\nif (unlikely(!mm->context.tsb_block[MM_TSB_BASE].tsb))\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void tsb_destroy_one(struct tsb_config *tp)\r\n{\r\nunsigned long cache_index;\r\nif (!tp->tsb)\r\nreturn;\r\ncache_index = tp->tsb_reg_val & 0x7UL;\r\nkmem_cache_free(tsb_caches[cache_index], tp->tsb);\r\ntp->tsb = NULL;\r\ntp->tsb_reg_val = 0UL;\r\n}\r\nvoid destroy_context(struct mm_struct *mm)\r\n{\r\nunsigned long flags, i;\r\nstruct page *page;\r\nfor (i = 0; i < MM_NUM_TSBS; i++)\r\ntsb_destroy_one(&mm->context.tsb_block[i]);\r\npage = mm->context.pgtable_page;\r\nif (page && put_page_testzero(page)) {\r\npgtable_page_dtor(page);\r\nfree_hot_cold_page(page, 0);\r\n}\r\nspin_lock_irqsave(&ctx_alloc_lock, flags);\r\nif (CTX_VALID(mm->context)) {\r\nunsigned long nr = CTX_NRBITS(mm->context);\r\nmmu_context_bmap[nr>>6] &= ~(1UL << (nr & 63));\r\n}\r\nspin_unlock_irqrestore(&ctx_alloc_lock, flags);\r\n}
