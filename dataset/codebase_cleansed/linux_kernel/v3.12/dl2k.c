static void dl2k_enable_int(struct netdev_private *np)\r\n{\r\nvoid __iomem *ioaddr = np->ioaddr;\r\ndw16(IntEnable, DEFAULT_INTR);\r\n}\r\nstatic int\r\nrio_probe1 (struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct net_device *dev;\r\nstruct netdev_private *np;\r\nstatic int card_idx;\r\nint chip_idx = ent->driver_data;\r\nint err, irq;\r\nvoid __iomem *ioaddr;\r\nstatic int version_printed;\r\nvoid *ring_space;\r\ndma_addr_t ring_dma;\r\nif (!version_printed++)\r\nprintk ("%s", version);\r\nerr = pci_enable_device (pdev);\r\nif (err)\r\nreturn err;\r\nirq = pdev->irq;\r\nerr = pci_request_regions (pdev, "dl2k");\r\nif (err)\r\ngoto err_out_disable;\r\npci_set_master (pdev);\r\nerr = -ENOMEM;\r\ndev = alloc_etherdev (sizeof (*np));\r\nif (!dev)\r\ngoto err_out_res;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nnp = netdev_priv(dev);\r\nioaddr = pci_iomap(pdev, 0, 0);\r\nif (!ioaddr)\r\ngoto err_out_dev;\r\nnp->eeprom_addr = ioaddr;\r\n#ifdef MEM_MAPPING\r\nioaddr = pci_iomap(pdev, 1, 0);\r\nif (!ioaddr)\r\ngoto err_out_iounmap;\r\n#endif\r\nnp->ioaddr = ioaddr;\r\nnp->chip_id = chip_idx;\r\nnp->pdev = pdev;\r\nspin_lock_init (&np->tx_lock);\r\nspin_lock_init (&np->rx_lock);\r\nnp->an_enable = 1;\r\nnp->tx_coalesce = 1;\r\nif (card_idx < MAX_UNITS) {\r\nif (media[card_idx] != NULL) {\r\nnp->an_enable = 0;\r\nif (strcmp (media[card_idx], "auto") == 0 ||\r\nstrcmp (media[card_idx], "autosense") == 0 ||\r\nstrcmp (media[card_idx], "0") == 0 ) {\r\nnp->an_enable = 2;\r\n} else if (strcmp (media[card_idx], "100mbps_fd") == 0 ||\r\nstrcmp (media[card_idx], "4") == 0) {\r\nnp->speed = 100;\r\nnp->full_duplex = 1;\r\n} else if (strcmp (media[card_idx], "100mbps_hd") == 0 ||\r\nstrcmp (media[card_idx], "3") == 0) {\r\nnp->speed = 100;\r\nnp->full_duplex = 0;\r\n} else if (strcmp (media[card_idx], "10mbps_fd") == 0 ||\r\nstrcmp (media[card_idx], "2") == 0) {\r\nnp->speed = 10;\r\nnp->full_duplex = 1;\r\n} else if (strcmp (media[card_idx], "10mbps_hd") == 0 ||\r\nstrcmp (media[card_idx], "1") == 0) {\r\nnp->speed = 10;\r\nnp->full_duplex = 0;\r\n} else if (strcmp (media[card_idx], "1000mbps_fd") == 0 ||\r\nstrcmp (media[card_idx], "6") == 0) {\r\nnp->speed=1000;\r\nnp->full_duplex=1;\r\n} else if (strcmp (media[card_idx], "1000mbps_hd") == 0 ||\r\nstrcmp (media[card_idx], "5") == 0) {\r\nnp->speed = 1000;\r\nnp->full_duplex = 0;\r\n} else {\r\nnp->an_enable = 1;\r\n}\r\n}\r\nif (jumbo[card_idx] != 0) {\r\nnp->jumbo = 1;\r\ndev->mtu = MAX_JUMBO;\r\n} else {\r\nnp->jumbo = 0;\r\nif (mtu[card_idx] > 0 && mtu[card_idx] < PACKET_SIZE)\r\ndev->mtu = mtu[card_idx];\r\n}\r\nnp->vlan = (vlan[card_idx] > 0 && vlan[card_idx] < 4096) ?\r\nvlan[card_idx] : 0;\r\nif (rx_coalesce > 0 && rx_timeout > 0) {\r\nnp->rx_coalesce = rx_coalesce;\r\nnp->rx_timeout = rx_timeout;\r\nnp->coalesce = 1;\r\n}\r\nnp->tx_flow = (tx_flow == 0) ? 0 : 1;\r\nnp->rx_flow = (rx_flow == 0) ? 0 : 1;\r\nif (tx_coalesce < 1)\r\ntx_coalesce = 1;\r\nelse if (tx_coalesce > TX_RING_SIZE-1)\r\ntx_coalesce = TX_RING_SIZE - 1;\r\n}\r\ndev->netdev_ops = &netdev_ops;\r\ndev->watchdog_timeo = TX_TIMEOUT;\r\nSET_ETHTOOL_OPS(dev, &ethtool_ops);\r\n#if 0\r\ndev->features = NETIF_F_IP_CSUM;\r\n#endif\r\npci_set_drvdata (pdev, dev);\r\nring_space = pci_alloc_consistent (pdev, TX_TOTAL_SIZE, &ring_dma);\r\nif (!ring_space)\r\ngoto err_out_iounmap;\r\nnp->tx_ring = ring_space;\r\nnp->tx_ring_dma = ring_dma;\r\nring_space = pci_alloc_consistent (pdev, RX_TOTAL_SIZE, &ring_dma);\r\nif (!ring_space)\r\ngoto err_out_unmap_tx;\r\nnp->rx_ring = ring_space;\r\nnp->rx_ring_dma = ring_dma;\r\nparse_eeprom (dev);\r\nerr = find_miiphy (dev);\r\nif (err)\r\ngoto err_out_unmap_rx;\r\nnp->phy_media = (dr16(ASICCtrl) & PhyMedia) ? 1 : 0;\r\nnp->link_status = 0;\r\nif (np->phy_media) {\r\nif (np->an_enable == 2) {\r\nnp->an_enable = 1;\r\n}\r\nmii_set_media_pcs (dev);\r\n} else {\r\nif (np->speed == 1000)\r\nnp->an_enable = 1;\r\nmii_set_media (dev);\r\n}\r\nerr = register_netdev (dev);\r\nif (err)\r\ngoto err_out_unmap_rx;\r\ncard_idx++;\r\nprintk (KERN_INFO "%s: %s, %pM, IRQ %d\n",\r\ndev->name, np->name, dev->dev_addr, irq);\r\nif (tx_coalesce > 1)\r\nprintk(KERN_INFO "tx_coalesce:\t%d packets\n",\r\ntx_coalesce);\r\nif (np->coalesce)\r\nprintk(KERN_INFO\r\n"rx_coalesce:\t%d packets\n"\r\n"rx_timeout: \t%d ns\n",\r\nnp->rx_coalesce, np->rx_timeout*640);\r\nif (np->vlan)\r\nprintk(KERN_INFO "vlan(id):\t%d\n", np->vlan);\r\nreturn 0;\r\nerr_out_unmap_rx:\r\npci_free_consistent (pdev, RX_TOTAL_SIZE, np->rx_ring, np->rx_ring_dma);\r\nerr_out_unmap_tx:\r\npci_free_consistent (pdev, TX_TOTAL_SIZE, np->tx_ring, np->tx_ring_dma);\r\nerr_out_iounmap:\r\n#ifdef MEM_MAPPING\r\npci_iounmap(pdev, np->ioaddr);\r\n#endif\r\npci_iounmap(pdev, np->eeprom_addr);\r\nerr_out_dev:\r\nfree_netdev (dev);\r\nerr_out_res:\r\npci_release_regions (pdev);\r\nerr_out_disable:\r\npci_disable_device (pdev);\r\nreturn err;\r\n}\r\nstatic int\r\nfind_miiphy (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint i, phy_found = 0;\r\nnp = netdev_priv(dev);\r\nnp->phy_addr = 1;\r\nfor (i = 31; i >= 0; i--) {\r\nint mii_status = mii_read (dev, i, 1);\r\nif (mii_status != 0xffff && mii_status != 0x0000) {\r\nnp->phy_addr = i;\r\nphy_found++;\r\n}\r\n}\r\nif (!phy_found) {\r\nprintk (KERN_ERR "%s: No MII PHY found!\n", dev->name);\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nparse_eeprom (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nint i, j;\r\nu8 sromdata[256];\r\nu8 *psib;\r\nu32 crc;\r\nPSROM_t psrom = (PSROM_t) sromdata;\r\nint cid, next;\r\nfor (i = 0; i < 128; i++)\r\n((__le16 *) sromdata)[i] = cpu_to_le16(read_eeprom(np, i));\r\nif (np->pdev->vendor == PCI_VENDOR_ID_DLINK) {\r\ncrc = ~ether_crc_le (256 - 4, sromdata);\r\nif (psrom->crc != cpu_to_le32(crc)) {\r\nprintk (KERN_ERR "%s: EEPROM data CRC error.\n",\r\ndev->name);\r\nreturn -1;\r\n}\r\n}\r\nfor (i = 0; i < 6; i++)\r\ndev->dev_addr[i] = psrom->mac_addr[i];\r\nif (np->pdev->vendor != PCI_VENDOR_ID_DLINK) {\r\nreturn 0;\r\n}\r\ni = 0x30;\r\npsib = (u8 *) sromdata;\r\ndo {\r\ncid = psib[i++];\r\nnext = psib[i++];\r\nif ((cid == 0 && next == 0) || (cid == 0xff && next == 0xff)) {\r\nprintk (KERN_ERR "Cell data error\n");\r\nreturn -1;\r\n}\r\nswitch (cid) {\r\ncase 0:\r\nbreak;\r\ncase 1:\r\nreturn 0;\r\ncase 2:\r\nnp->duplex_polarity = psib[i];\r\ndw8(PhyCtrl, dr8(PhyCtrl) | psib[i]);\r\nbreak;\r\ncase 3:\r\nnp->wake_polarity = psib[i];\r\nbreak;\r\ncase 9:\r\nj = (next - i > 255) ? 255 : next - i;\r\nmemcpy (np->name, &(psib[i]), j);\r\nbreak;\r\ncase 4:\r\ncase 5:\r\ncase 6:\r\ncase 7:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -1;\r\n}\r\ni = next;\r\n} while (1);\r\nreturn 0;\r\n}\r\nstatic int\r\nrio_open (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nconst int irq = np->pdev->irq;\r\nint i;\r\nu16 macctrl;\r\ni = request_irq(irq, rio_interrupt, IRQF_SHARED, dev->name, dev);\r\nif (i)\r\nreturn i;\r\ndw16(ASICCtrl + 2,\r\nGlobalReset | DMAReset | FIFOReset | NetworkReset | HostReset);\r\nmdelay(10);\r\ndw32(DebugCtrl, dr32(DebugCtrl) | 0x0230);\r\nif (np->jumbo != 0)\r\ndw16(MaxFrameSize, MAX_JUMBO+14);\r\nalloc_list (dev);\r\nfor (i = 0; i < 6; i++)\r\ndw8(StationAddr0 + i, dev->dev_addr[i]);\r\nset_multicast (dev);\r\nif (np->coalesce) {\r\ndw32(RxDMAIntCtrl, np->rx_coalesce | np->rx_timeout << 16);\r\n}\r\ndw8(RxDMAPollPeriod, 0x20);\r\ndw8(TxDMAPollPeriod, 0xff);\r\ndw8(RxDMABurstThresh, 0x30);\r\ndw8(RxDMAUrgentThresh, 0x30);\r\ndw32(RmonStatMask, 0x0007ffff);\r\nclear_stats (dev);\r\nif (np->vlan) {\r\ndw32(RxDMAIntCtrl, dr32(RxDMAIntCtrl) | 0x7 << 10);\r\ndw16(VLANId, np->vlan);\r\ndw32(VLANTag, 0x8100 << 16 | np->vlan);\r\ndw32(MACCtrl, dr32(MACCtrl) | AutoVLANuntagging);\r\n}\r\ninit_timer (&np->timer);\r\nnp->timer.expires = jiffies + 1*HZ;\r\nnp->timer.data = (unsigned long) dev;\r\nnp->timer.function = rio_timer;\r\nadd_timer (&np->timer);\r\ndw32(MACCtrl, dr32(MACCtrl) | StatsEnable | RxEnable | TxEnable);\r\nmacctrl = 0;\r\nmacctrl |= (np->vlan) ? AutoVLANuntagging : 0;\r\nmacctrl |= (np->full_duplex) ? DuplexSelect : 0;\r\nmacctrl |= (np->tx_flow) ? TxFlowControlEnable : 0;\r\nmacctrl |= (np->rx_flow) ? RxFlowControlEnable : 0;\r\ndw16(MACCtrl, macctrl);\r\nnetif_start_queue (dev);\r\ndl2k_enable_int(np);\r\nreturn 0;\r\n}\r\nstatic void\r\nrio_timer (unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *)data;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nunsigned int entry;\r\nint next_tick = 1*HZ;\r\nunsigned long flags;\r\nspin_lock_irqsave(&np->rx_lock, flags);\r\nif (np->cur_rx - np->old_rx >= RX_RING_SIZE) {\r\nprintk(KERN_INFO "Try to recover rx ring exhausted...\n");\r\nfor (; np->cur_rx - np->old_rx > 0; np->old_rx++) {\r\nstruct sk_buff *skb;\r\nentry = np->old_rx % RX_RING_SIZE;\r\nif (np->rx_skbuff[entry] == NULL) {\r\nskb = netdev_alloc_skb_ip_align(dev,\r\nnp->rx_buf_sz);\r\nif (skb == NULL) {\r\nnp->rx_ring[entry].fraginfo = 0;\r\nprintk (KERN_INFO\r\n"%s: Still unable to re-allocate Rx skbuff.#%d\n",\r\ndev->name, entry);\r\nbreak;\r\n}\r\nnp->rx_skbuff[entry] = skb;\r\nnp->rx_ring[entry].fraginfo =\r\ncpu_to_le64 (pci_map_single\r\n(np->pdev, skb->data, np->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE));\r\n}\r\nnp->rx_ring[entry].fraginfo |=\r\ncpu_to_le64((u64)np->rx_buf_sz << 48);\r\nnp->rx_ring[entry].status = 0;\r\n}\r\n}\r\nspin_unlock_irqrestore (&np->rx_lock, flags);\r\nnp->timer.expires = jiffies + next_tick;\r\nadd_timer(&np->timer);\r\n}\r\nstatic void\r\nrio_tx_timeout (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nprintk (KERN_INFO "%s: Tx timed out (%4.4x), is buffer full?\n",\r\ndev->name, dr32(TxStatus));\r\nrio_free_tx(dev, 0);\r\ndev->if_port = 0;\r\ndev->trans_start = jiffies;\r\n}\r\nstatic void\r\nalloc_list (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nint i;\r\nnp->cur_rx = np->cur_tx = 0;\r\nnp->old_rx = np->old_tx = 0;\r\nnp->rx_buf_sz = (dev->mtu <= 1500 ? PACKET_SIZE : dev->mtu + 32);\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nnp->tx_skbuff[i] = NULL;\r\nnp->tx_ring[i].status = cpu_to_le64 (TFDDone);\r\nnp->tx_ring[i].next_desc = cpu_to_le64 (np->tx_ring_dma +\r\n((i+1)%TX_RING_SIZE) *\r\nsizeof (struct netdev_desc));\r\n}\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nnp->rx_ring[i].next_desc = cpu_to_le64 (np->rx_ring_dma +\r\n((i + 1) % RX_RING_SIZE) *\r\nsizeof (struct netdev_desc));\r\nnp->rx_ring[i].status = 0;\r\nnp->rx_ring[i].fraginfo = 0;\r\nnp->rx_skbuff[i] = NULL;\r\n}\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nstruct sk_buff *skb;\r\nskb = netdev_alloc_skb_ip_align(dev, np->rx_buf_sz);\r\nnp->rx_skbuff[i] = skb;\r\nif (skb == NULL)\r\nbreak;\r\nnp->rx_ring[i].fraginfo =\r\ncpu_to_le64 ( pci_map_single (\r\nnp->pdev, skb->data, np->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE));\r\nnp->rx_ring[i].fraginfo |= cpu_to_le64((u64)np->rx_buf_sz << 48);\r\n}\r\ndw32(RFDListPtr0, np->rx_ring_dma);\r\ndw32(RFDListPtr1, 0);\r\n}\r\nstatic netdev_tx_t\r\nstart_xmit (struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nstruct netdev_desc *txdesc;\r\nunsigned entry;\r\nu64 tfc_vlan_tag = 0;\r\nif (np->link_status == 0) {\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nentry = np->cur_tx % TX_RING_SIZE;\r\nnp->tx_skbuff[entry] = skb;\r\ntxdesc = &np->tx_ring[entry];\r\n#if 0\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\ntxdesc->status |=\r\ncpu_to_le64 (TCPChecksumEnable | UDPChecksumEnable |\r\nIPChecksumEnable);\r\n}\r\n#endif\r\nif (np->vlan) {\r\ntfc_vlan_tag = VLANTagInsert |\r\n((u64)np->vlan << 32) |\r\n((u64)skb->priority << 45);\r\n}\r\ntxdesc->fraginfo = cpu_to_le64 (pci_map_single (np->pdev, skb->data,\r\nskb->len,\r\nPCI_DMA_TODEVICE));\r\ntxdesc->fraginfo |= cpu_to_le64((u64)skb->len << 48);\r\nif (entry % np->tx_coalesce == 0 || np->speed == 10)\r\ntxdesc->status = cpu_to_le64 (entry | tfc_vlan_tag |\r\nWordAlignDisable |\r\nTxDMAIndicate |\r\n(1 << FragCountShift));\r\nelse\r\ntxdesc->status = cpu_to_le64 (entry | tfc_vlan_tag |\r\nWordAlignDisable |\r\n(1 << FragCountShift));\r\ndw32(DMACtrl, dr32(DMACtrl) | 0x00001000);\r\ndw32(CountDown, 10000);\r\nnp->cur_tx = (np->cur_tx + 1) % TX_RING_SIZE;\r\nif ((np->cur_tx - np->old_tx + TX_RING_SIZE) % TX_RING_SIZE\r\n< TX_QUEUE_LEN - 1 && np->speed != 10) {\r\n} else if (!netif_queue_stopped(dev)) {\r\nnetif_stop_queue (dev);\r\n}\r\nif (!dr32(TFDListPtr0)) {\r\ndw32(TFDListPtr0, np->tx_ring_dma +\r\nentry * sizeof (struct netdev_desc));\r\ndw32(TFDListPtr1, 0);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic irqreturn_t\r\nrio_interrupt (int irq, void *dev_instance)\r\n{\r\nstruct net_device *dev = dev_instance;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nunsigned int_status;\r\nint cnt = max_intrloop;\r\nint handled = 0;\r\nwhile (1) {\r\nint_status = dr16(IntStatus);\r\ndw16(IntStatus, int_status);\r\nint_status &= DEFAULT_INTR;\r\nif (int_status == 0 || --cnt < 0)\r\nbreak;\r\nhandled = 1;\r\nif (int_status & RxDMAComplete)\r\nreceive_packet (dev);\r\nif ((int_status & (TxDMAComplete|IntRequested))) {\r\nint tx_status;\r\ntx_status = dr32(TxStatus);\r\nif (tx_status & 0x01)\r\ntx_error (dev, tx_status);\r\nrio_free_tx (dev, 1);\r\n}\r\nif (int_status &\r\n(HostError | LinkEvent | UpdateStats))\r\nrio_error (dev, int_status);\r\n}\r\nif (np->cur_tx != np->old_tx)\r\ndw32(CountDown, 100);\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic inline dma_addr_t desc_to_dma(struct netdev_desc *desc)\r\n{\r\nreturn le64_to_cpu(desc->fraginfo) & DMA_BIT_MASK(48);\r\n}\r\nstatic void\r\nrio_free_tx (struct net_device *dev, int irq)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint entry = np->old_tx % TX_RING_SIZE;\r\nint tx_use = 0;\r\nunsigned long flag = 0;\r\nif (irq)\r\nspin_lock(&np->tx_lock);\r\nelse\r\nspin_lock_irqsave(&np->tx_lock, flag);\r\nwhile (entry != np->cur_tx) {\r\nstruct sk_buff *skb;\r\nif (!(np->tx_ring[entry].status & cpu_to_le64(TFDDone)))\r\nbreak;\r\nskb = np->tx_skbuff[entry];\r\npci_unmap_single (np->pdev,\r\ndesc_to_dma(&np->tx_ring[entry]),\r\nskb->len, PCI_DMA_TODEVICE);\r\nif (irq)\r\ndev_kfree_skb_irq (skb);\r\nelse\r\ndev_kfree_skb (skb);\r\nnp->tx_skbuff[entry] = NULL;\r\nentry = (entry + 1) % TX_RING_SIZE;\r\ntx_use++;\r\n}\r\nif (irq)\r\nspin_unlock(&np->tx_lock);\r\nelse\r\nspin_unlock_irqrestore(&np->tx_lock, flag);\r\nnp->old_tx = entry;\r\nif (netif_queue_stopped(dev) &&\r\n((np->cur_tx - np->old_tx + TX_RING_SIZE) % TX_RING_SIZE\r\n< TX_QUEUE_LEN - 1 || np->speed == 10)) {\r\nnetif_wake_queue (dev);\r\n}\r\n}\r\nstatic void\r\ntx_error (struct net_device *dev, int tx_status)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nint frame_id;\r\nint i;\r\nframe_id = (tx_status & 0xffff0000);\r\nprintk (KERN_ERR "%s: Transmit error, TxStatus %4.4x, FrameId %d.\n",\r\ndev->name, tx_status, frame_id);\r\nnp->stats.tx_errors++;\r\nif (tx_status & 0x10) {\r\nnp->stats.tx_fifo_errors++;\r\ndw16(TxStartThresh, dr16(TxStartThresh) + 0x10);\r\ndw16(ASICCtrl + 2,\r\nTxReset | DMAReset | FIFOReset | NetworkReset);\r\nfor (i = 50; i > 0; i--) {\r\nif (!(dr16(ASICCtrl + 2) & ResetBusy))\r\nbreak;\r\nmdelay (1);\r\n}\r\nrio_free_tx (dev, 1);\r\ndw32(TFDListPtr0, np->tx_ring_dma +\r\nnp->old_tx * sizeof (struct netdev_desc));\r\ndw32(TFDListPtr1, 0);\r\n}\r\nif (tx_status & 0x04) {\r\nnp->stats.tx_fifo_errors++;\r\ndw16(ASICCtrl + 2, TxReset | FIFOReset);\r\nfor (i = 50; i > 0; i--) {\r\nif (!(dr16(ASICCtrl + 2) & ResetBusy))\r\nbreak;\r\nmdelay (1);\r\n}\r\n}\r\n#ifdef ETHER_STATS\r\nif (tx_status & 0x08)\r\nnp->stats.collisions16++;\r\n#else\r\nif (tx_status & 0x08)\r\nnp->stats.collisions++;\r\n#endif\r\ndw32(MACCtrl, dr16(MACCtrl) | TxEnable);\r\n}\r\nstatic int\r\nreceive_packet (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint entry = np->cur_rx % RX_RING_SIZE;\r\nint cnt = 30;\r\nwhile (1) {\r\nstruct netdev_desc *desc = &np->rx_ring[entry];\r\nint pkt_len;\r\nu64 frame_status;\r\nif (!(desc->status & cpu_to_le64(RFDDone)) ||\r\n!(desc->status & cpu_to_le64(FrameStart)) ||\r\n!(desc->status & cpu_to_le64(FrameEnd)))\r\nbreak;\r\nframe_status = le64_to_cpu(desc->status);\r\npkt_len = frame_status & 0xffff;\r\nif (--cnt < 0)\r\nbreak;\r\nif (frame_status & RFS_Errors) {\r\nnp->stats.rx_errors++;\r\nif (frame_status & (RxRuntFrame | RxLengthError))\r\nnp->stats.rx_length_errors++;\r\nif (frame_status & RxFCSError)\r\nnp->stats.rx_crc_errors++;\r\nif (frame_status & RxAlignmentError && np->speed != 1000)\r\nnp->stats.rx_frame_errors++;\r\nif (frame_status & RxFIFOOverrun)\r\nnp->stats.rx_fifo_errors++;\r\n} else {\r\nstruct sk_buff *skb;\r\nif (pkt_len > copy_thresh) {\r\npci_unmap_single (np->pdev,\r\ndesc_to_dma(desc),\r\nnp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\nskb_put (skb = np->rx_skbuff[entry], pkt_len);\r\nnp->rx_skbuff[entry] = NULL;\r\n} else if ((skb = netdev_alloc_skb_ip_align(dev, pkt_len))) {\r\npci_dma_sync_single_for_cpu(np->pdev,\r\ndesc_to_dma(desc),\r\nnp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\nskb_copy_to_linear_data (skb,\r\nnp->rx_skbuff[entry]->data,\r\npkt_len);\r\nskb_put (skb, pkt_len);\r\npci_dma_sync_single_for_device(np->pdev,\r\ndesc_to_dma(desc),\r\nnp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\n}\r\nskb->protocol = eth_type_trans (skb, dev);\r\n#if 0\r\nif (np->pdev->pci_rev_id >= 0x0c &&\r\n!(frame_status & (TCPError | UDPError | IPError))) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n}\r\n#endif\r\nnetif_rx (skb);\r\n}\r\nentry = (entry + 1) % RX_RING_SIZE;\r\n}\r\nspin_lock(&np->rx_lock);\r\nnp->cur_rx = entry;\r\nentry = np->old_rx;\r\nwhile (entry != np->cur_rx) {\r\nstruct sk_buff *skb;\r\nif (np->rx_skbuff[entry] == NULL) {\r\nskb = netdev_alloc_skb_ip_align(dev, np->rx_buf_sz);\r\nif (skb == NULL) {\r\nnp->rx_ring[entry].fraginfo = 0;\r\nprintk (KERN_INFO\r\n"%s: receive_packet: "\r\n"Unable to re-allocate Rx skbuff.#%d\n",\r\ndev->name, entry);\r\nbreak;\r\n}\r\nnp->rx_skbuff[entry] = skb;\r\nnp->rx_ring[entry].fraginfo =\r\ncpu_to_le64 (pci_map_single\r\n(np->pdev, skb->data, np->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE));\r\n}\r\nnp->rx_ring[entry].fraginfo |=\r\ncpu_to_le64((u64)np->rx_buf_sz << 48);\r\nnp->rx_ring[entry].status = 0;\r\nentry = (entry + 1) % RX_RING_SIZE;\r\n}\r\nnp->old_rx = entry;\r\nspin_unlock(&np->rx_lock);\r\nreturn 0;\r\n}\r\nstatic void\r\nrio_error (struct net_device *dev, int int_status)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nu16 macctrl;\r\nif (int_status & LinkEvent) {\r\nif (mii_wait_link (dev, 10) == 0) {\r\nprintk (KERN_INFO "%s: Link up\n", dev->name);\r\nif (np->phy_media)\r\nmii_get_media_pcs (dev);\r\nelse\r\nmii_get_media (dev);\r\nif (np->speed == 1000)\r\nnp->tx_coalesce = tx_coalesce;\r\nelse\r\nnp->tx_coalesce = 1;\r\nmacctrl = 0;\r\nmacctrl |= (np->vlan) ? AutoVLANuntagging : 0;\r\nmacctrl |= (np->full_duplex) ? DuplexSelect : 0;\r\nmacctrl |= (np->tx_flow) ?\r\nTxFlowControlEnable : 0;\r\nmacctrl |= (np->rx_flow) ?\r\nRxFlowControlEnable : 0;\r\ndw16(MACCtrl, macctrl);\r\nnp->link_status = 1;\r\nnetif_carrier_on(dev);\r\n} else {\r\nprintk (KERN_INFO "%s: Link off\n", dev->name);\r\nnp->link_status = 0;\r\nnetif_carrier_off(dev);\r\n}\r\n}\r\nif (int_status & UpdateStats) {\r\nget_stats (dev);\r\n}\r\nif (int_status & HostError) {\r\nprintk (KERN_ERR "%s: HostError! IntStatus %4.4x.\n",\r\ndev->name, int_status);\r\ndw16(ASICCtrl + 2, GlobalReset | HostReset);\r\nmdelay (500);\r\n}\r\n}\r\nstatic struct net_device_stats *\r\nget_stats (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\n#ifdef MEM_MAPPING\r\nint i;\r\n#endif\r\nunsigned int stat_reg;\r\nnp->stats.rx_packets += dr32(FramesRcvOk);\r\nnp->stats.tx_packets += dr32(FramesXmtOk);\r\nnp->stats.rx_bytes += dr32(OctetRcvOk);\r\nnp->stats.tx_bytes += dr32(OctetXmtOk);\r\nnp->stats.multicast = dr32(McstFramesRcvdOk);\r\nnp->stats.collisions += dr32(SingleColFrames)\r\n+ dr32(MultiColFrames);\r\nstat_reg = dr16(FramesAbortXSColls);\r\nnp->stats.tx_aborted_errors += stat_reg;\r\nnp->stats.tx_errors += stat_reg;\r\nstat_reg = dr16(CarrierSenseErrors);\r\nnp->stats.tx_carrier_errors += stat_reg;\r\nnp->stats.tx_errors += stat_reg;\r\ndr32(McstOctetXmtOk);\r\ndr16(BcstFramesXmtdOk);\r\ndr32(McstFramesXmtdOk);\r\ndr16(BcstFramesRcvdOk);\r\ndr16(MacControlFramesRcvd);\r\ndr16(FrameTooLongErrors);\r\ndr16(InRangeLengthErrors);\r\ndr16(FramesCheckSeqErrors);\r\ndr16(FramesLostRxErrors);\r\ndr32(McstOctetXmtOk);\r\ndr32(BcstOctetXmtOk);\r\ndr32(McstFramesXmtdOk);\r\ndr32(FramesWDeferredXmt);\r\ndr32(LateCollisions);\r\ndr16(BcstFramesXmtdOk);\r\ndr16(MacControlFramesXmtd);\r\ndr16(FramesWEXDeferal);\r\n#ifdef MEM_MAPPING\r\nfor (i = 0x100; i <= 0x150; i += 4)\r\ndr32(i);\r\n#endif\r\ndr16(TxJumboFrames);\r\ndr16(RxJumboFrames);\r\ndr16(TCPCheckSumErrors);\r\ndr16(UDPCheckSumErrors);\r\ndr16(IPCheckSumErrors);\r\nreturn &np->stats;\r\n}\r\nstatic int\r\nclear_stats (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\n#ifdef MEM_MAPPING\r\nint i;\r\n#endif\r\ndr32(FramesRcvOk);\r\ndr32(FramesXmtOk);\r\ndr32(OctetRcvOk);\r\ndr32(OctetXmtOk);\r\ndr32(McstFramesRcvdOk);\r\ndr32(SingleColFrames);\r\ndr32(MultiColFrames);\r\ndr32(LateCollisions);\r\ndr16(FrameTooLongErrors);\r\ndr16(InRangeLengthErrors);\r\ndr16(FramesCheckSeqErrors);\r\ndr16(FramesLostRxErrors);\r\ndr16(FramesAbortXSColls);\r\ndr16(CarrierSenseErrors);\r\ndr32(McstOctetXmtOk);\r\ndr16(BcstFramesXmtdOk);\r\ndr32(McstFramesXmtdOk);\r\ndr16(BcstFramesRcvdOk);\r\ndr16(MacControlFramesRcvd);\r\ndr32(McstOctetXmtOk);\r\ndr32(BcstOctetXmtOk);\r\ndr32(McstFramesXmtdOk);\r\ndr32(FramesWDeferredXmt);\r\ndr16(BcstFramesXmtdOk);\r\ndr16(MacControlFramesXmtd);\r\ndr16(FramesWEXDeferal);\r\n#ifdef MEM_MAPPING\r\nfor (i = 0x100; i <= 0x150; i += 4)\r\ndr32(i);\r\n#endif\r\ndr16(TxJumboFrames);\r\ndr16(RxJumboFrames);\r\ndr16(TCPCheckSumErrors);\r\ndr16(UDPCheckSumErrors);\r\ndr16(IPCheckSumErrors);\r\nreturn 0;\r\n}\r\nstatic int\r\nchange_mtu (struct net_device *dev, int new_mtu)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint max = (np->jumbo) ? MAX_JUMBO : 1536;\r\nif ((new_mtu < 68) || (new_mtu > max)) {\r\nreturn -EINVAL;\r\n}\r\ndev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic void\r\nset_multicast (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nu32 hash_table[2];\r\nu16 rx_mode = 0;\r\nhash_table[0] = hash_table[1] = 0;\r\nhash_table[1] |= 0x02000000;\r\nif (dev->flags & IFF_PROMISC) {\r\nrx_mode = ReceiveAllFrames;\r\n} else if ((dev->flags & IFF_ALLMULTI) ||\r\n(netdev_mc_count(dev) > multicast_filter_limit)) {\r\nrx_mode = ReceiveBroadcast | ReceiveMulticast | ReceiveUnicast;\r\n} else if (!netdev_mc_empty(dev)) {\r\nstruct netdev_hw_addr *ha;\r\nrx_mode =\r\nReceiveBroadcast | ReceiveMulticastHash | ReceiveUnicast;\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nint bit, index = 0;\r\nint crc = ether_crc_le(ETH_ALEN, ha->addr);\r\nfor (bit = 0; bit < 6; bit++)\r\nif (crc & (1 << (31 - bit)))\r\nindex |= (1 << bit);\r\nhash_table[index / 32] |= (1 << (index % 32));\r\n}\r\n} else {\r\nrx_mode = ReceiveBroadcast | ReceiveUnicast;\r\n}\r\nif (np->vlan) {\r\nrx_mode |= ReceiveVLANMatch;\r\n}\r\ndw32(HashTable0, hash_table[0]);\r\ndw32(HashTable1, hash_table[1]);\r\ndw16(ReceiveMode, rx_mode);\r\n}\r\nstatic void rio_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nstrlcpy(info->driver, "dl2k", sizeof(info->driver));\r\nstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\r\nstrlcpy(info->bus_info, pci_name(np->pdev), sizeof(info->bus_info));\r\n}\r\nstatic int rio_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nif (np->phy_media) {\r\ncmd->supported = SUPPORTED_Autoneg | SUPPORTED_FIBRE;\r\ncmd->advertising= ADVERTISED_Autoneg | ADVERTISED_FIBRE;\r\ncmd->port = PORT_FIBRE;\r\ncmd->transceiver = XCVR_INTERNAL;\r\n} else {\r\ncmd->supported = SUPPORTED_10baseT_Half |\r\nSUPPORTED_10baseT_Full | SUPPORTED_100baseT_Half\r\n| SUPPORTED_100baseT_Full | SUPPORTED_1000baseT_Full |\r\nSUPPORTED_Autoneg | SUPPORTED_MII;\r\ncmd->advertising = ADVERTISED_10baseT_Half |\r\nADVERTISED_10baseT_Full | ADVERTISED_100baseT_Half |\r\nADVERTISED_100baseT_Full | ADVERTISED_1000baseT_Full|\r\nADVERTISED_Autoneg | ADVERTISED_MII;\r\ncmd->port = PORT_MII;\r\ncmd->transceiver = XCVR_INTERNAL;\r\n}\r\nif ( np->link_status ) {\r\nethtool_cmd_speed_set(cmd, np->speed);\r\ncmd->duplex = np->full_duplex ? DUPLEX_FULL : DUPLEX_HALF;\r\n} else {\r\nethtool_cmd_speed_set(cmd, -1);\r\ncmd->duplex = -1;\r\n}\r\nif ( np->an_enable)\r\ncmd->autoneg = AUTONEG_ENABLE;\r\nelse\r\ncmd->autoneg = AUTONEG_DISABLE;\r\ncmd->phy_address = np->phy_addr;\r\nreturn 0;\r\n}\r\nstatic int rio_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nnetif_carrier_off(dev);\r\nif (cmd->autoneg == AUTONEG_ENABLE) {\r\nif (np->an_enable)\r\nreturn 0;\r\nelse {\r\nnp->an_enable = 1;\r\nmii_set_media(dev);\r\nreturn 0;\r\n}\r\n} else {\r\nnp->an_enable = 0;\r\nif (np->speed == 1000) {\r\nethtool_cmd_speed_set(cmd, SPEED_100);\r\ncmd->duplex = DUPLEX_FULL;\r\nprintk("Warning!! Can't disable Auto negotiation in 1000Mbps, change to Manual 100Mbps, Full duplex.\n");\r\n}\r\nswitch (ethtool_cmd_speed(cmd)) {\r\ncase SPEED_10:\r\nnp->speed = 10;\r\nnp->full_duplex = (cmd->duplex == DUPLEX_FULL);\r\nbreak;\r\ncase SPEED_100:\r\nnp->speed = 100;\r\nnp->full_duplex = (cmd->duplex == DUPLEX_FULL);\r\nbreak;\r\ncase SPEED_1000:\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nmii_set_media(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic u32 rio_get_link(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nreturn np->link_status;\r\n}\r\nstatic int\r\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nint phy_addr;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nstruct mii_ioctl_data *miidata = if_mii(rq);\r\nphy_addr = np->phy_addr;\r\nswitch (cmd) {\r\ncase SIOCGMIIPHY:\r\nmiidata->phy_id = phy_addr;\r\nbreak;\r\ncase SIOCGMIIREG:\r\nmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);\r\nbreak;\r\ncase SIOCSMIIREG:\r\nif (!capable(CAP_NET_ADMIN))\r\nreturn -EPERM;\r\nmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);\r\nbreak;\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\nreturn 0;\r\n}\r\nstatic int read_eeprom(struct netdev_private *np, int eep_addr)\r\n{\r\nvoid __iomem *ioaddr = np->eeprom_addr;\r\nint i = 1000;\r\ndw16(EepromCtrl, EEP_READ | (eep_addr & 0xff));\r\nwhile (i-- > 0) {\r\nif (!(dr16(EepromCtrl) & EEP_BUSY))\r\nreturn dr16(EepromData);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nmii_sendbit (struct net_device *dev, u32 data)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\ndata = ((data) ? MII_DATA1 : 0) | (dr8(PhyCtrl) & 0xf8) | MII_WRITE;\r\ndw8(PhyCtrl, data);\r\nmii_delay ();\r\ndw8(PhyCtrl, data | MII_CLK);\r\nmii_delay ();\r\n}\r\nstatic int\r\nmii_getbit (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nu8 data;\r\ndata = (dr8(PhyCtrl) & 0xf8) | MII_READ;\r\ndw8(PhyCtrl, data);\r\nmii_delay ();\r\ndw8(PhyCtrl, data | MII_CLK);\r\nmii_delay ();\r\nreturn (dr8(PhyCtrl) >> 1) & 1;\r\n}\r\nstatic void\r\nmii_send_bits (struct net_device *dev, u32 data, int len)\r\n{\r\nint i;\r\nfor (i = len - 1; i >= 0; i--) {\r\nmii_sendbit (dev, data & (1 << i));\r\n}\r\n}\r\nstatic int\r\nmii_read (struct net_device *dev, int phy_addr, int reg_num)\r\n{\r\nu32 cmd;\r\nint i;\r\nu32 retval = 0;\r\nmii_send_bits (dev, 0xffffffff, 32);\r\ncmd = (0x06 << 10 | phy_addr << 5 | reg_num);\r\nmii_send_bits (dev, cmd, 14);\r\nif (mii_getbit (dev))\r\ngoto err_out;\r\nfor (i = 0; i < 16; i++) {\r\nretval |= mii_getbit (dev);\r\nretval <<= 1;\r\n}\r\nmii_getbit (dev);\r\nreturn (retval >> 1) & 0xffff;\r\nerr_out:\r\nreturn 0;\r\n}\r\nstatic int\r\nmii_write (struct net_device *dev, int phy_addr, int reg_num, u16 data)\r\n{\r\nu32 cmd;\r\nmii_send_bits (dev, 0xffffffff, 32);\r\ncmd = (0x5002 << 16) | (phy_addr << 23) | (reg_num << 18) | data;\r\nmii_send_bits (dev, cmd, 32);\r\nmii_getbit (dev);\r\nreturn 0;\r\n}\r\nstatic int\r\nmii_wait_link (struct net_device *dev, int wait)\r\n{\r\n__u16 bmsr;\r\nint phy_addr;\r\nstruct netdev_private *np;\r\nnp = netdev_priv(dev);\r\nphy_addr = np->phy_addr;\r\ndo {\r\nbmsr = mii_read (dev, phy_addr, MII_BMSR);\r\nif (bmsr & BMSR_LSTATUS)\r\nreturn 0;\r\nmdelay (1);\r\n} while (--wait > 0);\r\nreturn -1;\r\n}\r\nstatic int\r\nmii_get_media (struct net_device *dev)\r\n{\r\n__u16 negotiate;\r\n__u16 bmsr;\r\n__u16 mscr;\r\n__u16 mssr;\r\nint phy_addr;\r\nstruct netdev_private *np;\r\nnp = netdev_priv(dev);\r\nphy_addr = np->phy_addr;\r\nbmsr = mii_read (dev, phy_addr, MII_BMSR);\r\nif (np->an_enable) {\r\nif (!(bmsr & BMSR_ANEGCOMPLETE)) {\r\nreturn -1;\r\n}\r\nnegotiate = mii_read (dev, phy_addr, MII_ADVERTISE) &\r\nmii_read (dev, phy_addr, MII_LPA);\r\nmscr = mii_read (dev, phy_addr, MII_CTRL1000);\r\nmssr = mii_read (dev, phy_addr, MII_STAT1000);\r\nif (mscr & ADVERTISE_1000FULL && mssr & LPA_1000FULL) {\r\nnp->speed = 1000;\r\nnp->full_duplex = 1;\r\nprintk (KERN_INFO "Auto 1000 Mbps, Full duplex\n");\r\n} else if (mscr & ADVERTISE_1000HALF && mssr & LPA_1000HALF) {\r\nnp->speed = 1000;\r\nnp->full_duplex = 0;\r\nprintk (KERN_INFO "Auto 1000 Mbps, Half duplex\n");\r\n} else if (negotiate & ADVERTISE_100FULL) {\r\nnp->speed = 100;\r\nnp->full_duplex = 1;\r\nprintk (KERN_INFO "Auto 100 Mbps, Full duplex\n");\r\n} else if (negotiate & ADVERTISE_100HALF) {\r\nnp->speed = 100;\r\nnp->full_duplex = 0;\r\nprintk (KERN_INFO "Auto 100 Mbps, Half duplex\n");\r\n} else if (negotiate & ADVERTISE_10FULL) {\r\nnp->speed = 10;\r\nnp->full_duplex = 1;\r\nprintk (KERN_INFO "Auto 10 Mbps, Full duplex\n");\r\n} else if (negotiate & ADVERTISE_10HALF) {\r\nnp->speed = 10;\r\nnp->full_duplex = 0;\r\nprintk (KERN_INFO "Auto 10 Mbps, Half duplex\n");\r\n}\r\nif (negotiate & ADVERTISE_PAUSE_CAP) {\r\nnp->tx_flow &= 1;\r\nnp->rx_flow &= 1;\r\n} else if (negotiate & ADVERTISE_PAUSE_ASYM) {\r\nnp->tx_flow = 0;\r\nnp->rx_flow &= 1;\r\n}\r\n} else {\r\n__u16 bmcr = mii_read (dev, phy_addr, MII_BMCR);\r\nswitch (bmcr & (BMCR_SPEED100 | BMCR_SPEED1000)) {\r\ncase BMCR_SPEED1000:\r\nprintk (KERN_INFO "Operating at 1000 Mbps, ");\r\nbreak;\r\ncase BMCR_SPEED100:\r\nprintk (KERN_INFO "Operating at 100 Mbps, ");\r\nbreak;\r\ncase 0:\r\nprintk (KERN_INFO "Operating at 10 Mbps, ");\r\n}\r\nif (bmcr & BMCR_FULLDPLX) {\r\nprintk (KERN_CONT "Full duplex\n");\r\n} else {\r\nprintk (KERN_CONT "Half duplex\n");\r\n}\r\n}\r\nif (np->tx_flow)\r\nprintk(KERN_INFO "Enable Tx Flow Control\n");\r\nelse\r\nprintk(KERN_INFO "Disable Tx Flow Control\n");\r\nif (np->rx_flow)\r\nprintk(KERN_INFO "Enable Rx Flow Control\n");\r\nelse\r\nprintk(KERN_INFO "Disable Rx Flow Control\n");\r\nreturn 0;\r\n}\r\nstatic int\r\nmii_set_media (struct net_device *dev)\r\n{\r\n__u16 pscr;\r\n__u16 bmcr;\r\n__u16 bmsr;\r\n__u16 anar;\r\nint phy_addr;\r\nstruct netdev_private *np;\r\nnp = netdev_priv(dev);\r\nphy_addr = np->phy_addr;\r\nif (np->an_enable) {\r\nbmsr = mii_read (dev, phy_addr, MII_BMSR);\r\nanar = mii_read (dev, phy_addr, MII_ADVERTISE) &\r\n~(ADVERTISE_100FULL | ADVERTISE_10FULL |\r\nADVERTISE_100HALF | ADVERTISE_10HALF |\r\nADVERTISE_100BASE4);\r\nif (bmsr & BMSR_100FULL)\r\nanar |= ADVERTISE_100FULL;\r\nif (bmsr & BMSR_100HALF)\r\nanar |= ADVERTISE_100HALF;\r\nif (bmsr & BMSR_100BASE4)\r\nanar |= ADVERTISE_100BASE4;\r\nif (bmsr & BMSR_10FULL)\r\nanar |= ADVERTISE_10FULL;\r\nif (bmsr & BMSR_10HALF)\r\nanar |= ADVERTISE_10HALF;\r\nanar |= ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\r\nmii_write (dev, phy_addr, MII_ADVERTISE, anar);\r\npscr = mii_read (dev, phy_addr, MII_PHY_SCR);\r\npscr |= 3 << 5;\r\nmii_write (dev, phy_addr, MII_PHY_SCR, pscr);\r\nmii_write (dev, phy_addr, MII_BMCR, BMCR_RESET);\r\nbmcr = BMCR_ANENABLE | BMCR_ANRESTART | BMCR_RESET;\r\nmii_write (dev, phy_addr, MII_BMCR, bmcr);\r\nmdelay(1);\r\n} else {\r\npscr = mii_read (dev, phy_addr, MII_PHY_SCR);\r\npscr &= ~(3 << 5);\r\nmii_write (dev, phy_addr, MII_PHY_SCR, pscr);\r\nbmcr = mii_read (dev, phy_addr, MII_BMCR);\r\nbmcr |= BMCR_RESET;\r\nmii_write (dev, phy_addr, MII_BMCR, bmcr);\r\nbmcr = 0x1940;\r\nmii_write (dev, phy_addr, MII_BMCR, bmcr);\r\nmdelay (100);\r\nmii_write (dev, phy_addr, MII_ADVERTISE, 0);\r\nbmcr = BMCR_PDOWN;\r\nif (np->speed == 100) {\r\nbmcr |= BMCR_SPEED100;\r\nprintk (KERN_INFO "Manual 100 Mbps, ");\r\n} else if (np->speed == 10) {\r\nprintk (KERN_INFO "Manual 10 Mbps, ");\r\n}\r\nif (np->full_duplex) {\r\nbmcr |= BMCR_FULLDPLX;\r\nprintk (KERN_CONT "Full duplex\n");\r\n} else {\r\nprintk (KERN_CONT "Half duplex\n");\r\n}\r\n#if 0\r\nmscr = mii_read (dev, phy_addr, MII_CTRL1000);\r\nmscr |= MII_MSCR_CFG_ENABLE;\r\nmscr &= ~MII_MSCR_CFG_VALUE = 0;\r\n#endif\r\nmii_write (dev, phy_addr, MII_BMCR, bmcr);\r\nmdelay(10);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nmii_get_media_pcs (struct net_device *dev)\r\n{\r\n__u16 negotiate;\r\n__u16 bmsr;\r\nint phy_addr;\r\nstruct netdev_private *np;\r\nnp = netdev_priv(dev);\r\nphy_addr = np->phy_addr;\r\nbmsr = mii_read (dev, phy_addr, PCS_BMSR);\r\nif (np->an_enable) {\r\nif (!(bmsr & BMSR_ANEGCOMPLETE)) {\r\nreturn -1;\r\n}\r\nnegotiate = mii_read (dev, phy_addr, PCS_ANAR) &\r\nmii_read (dev, phy_addr, PCS_ANLPAR);\r\nnp->speed = 1000;\r\nif (negotiate & PCS_ANAR_FULL_DUPLEX) {\r\nprintk (KERN_INFO "Auto 1000 Mbps, Full duplex\n");\r\nnp->full_duplex = 1;\r\n} else {\r\nprintk (KERN_INFO "Auto 1000 Mbps, half duplex\n");\r\nnp->full_duplex = 0;\r\n}\r\nif (negotiate & PCS_ANAR_PAUSE) {\r\nnp->tx_flow &= 1;\r\nnp->rx_flow &= 1;\r\n} else if (negotiate & PCS_ANAR_ASYMMETRIC) {\r\nnp->tx_flow = 0;\r\nnp->rx_flow &= 1;\r\n}\r\n} else {\r\n__u16 bmcr = mii_read (dev, phy_addr, PCS_BMCR);\r\nprintk (KERN_INFO "Operating at 1000 Mbps, ");\r\nif (bmcr & BMCR_FULLDPLX) {\r\nprintk (KERN_CONT "Full duplex\n");\r\n} else {\r\nprintk (KERN_CONT "Half duplex\n");\r\n}\r\n}\r\nif (np->tx_flow)\r\nprintk(KERN_INFO "Enable Tx Flow Control\n");\r\nelse\r\nprintk(KERN_INFO "Disable Tx Flow Control\n");\r\nif (np->rx_flow)\r\nprintk(KERN_INFO "Enable Rx Flow Control\n");\r\nelse\r\nprintk(KERN_INFO "Disable Rx Flow Control\n");\r\nreturn 0;\r\n}\r\nstatic int\r\nmii_set_media_pcs (struct net_device *dev)\r\n{\r\n__u16 bmcr;\r\n__u16 esr;\r\n__u16 anar;\r\nint phy_addr;\r\nstruct netdev_private *np;\r\nnp = netdev_priv(dev);\r\nphy_addr = np->phy_addr;\r\nif (np->an_enable) {\r\nesr = mii_read (dev, phy_addr, PCS_ESR);\r\nanar = mii_read (dev, phy_addr, MII_ADVERTISE) &\r\n~PCS_ANAR_HALF_DUPLEX &\r\n~PCS_ANAR_FULL_DUPLEX;\r\nif (esr & (MII_ESR_1000BT_HD | MII_ESR_1000BX_HD))\r\nanar |= PCS_ANAR_HALF_DUPLEX;\r\nif (esr & (MII_ESR_1000BT_FD | MII_ESR_1000BX_FD))\r\nanar |= PCS_ANAR_FULL_DUPLEX;\r\nanar |= PCS_ANAR_PAUSE | PCS_ANAR_ASYMMETRIC;\r\nmii_write (dev, phy_addr, MII_ADVERTISE, anar);\r\nmii_write (dev, phy_addr, MII_BMCR, BMCR_RESET);\r\nbmcr = BMCR_ANENABLE | BMCR_ANRESTART | BMCR_RESET;\r\nmii_write (dev, phy_addr, MII_BMCR, bmcr);\r\nmdelay(1);\r\n} else {\r\nbmcr = BMCR_RESET;\r\nmii_write (dev, phy_addr, MII_BMCR, bmcr);\r\nmdelay(10);\r\nif (np->full_duplex) {\r\nbmcr = BMCR_FULLDPLX;\r\nprintk (KERN_INFO "Manual full duplex\n");\r\n} else {\r\nbmcr = 0;\r\nprintk (KERN_INFO "Manual half duplex\n");\r\n}\r\nmii_write (dev, phy_addr, MII_BMCR, bmcr);\r\nmdelay(10);\r\nmii_write (dev, phy_addr, MII_ADVERTISE, 0);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nrio_close (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->ioaddr;\r\nstruct pci_dev *pdev = np->pdev;\r\nstruct sk_buff *skb;\r\nint i;\r\nnetif_stop_queue (dev);\r\ndw16(IntEnable, 0);\r\ndw32(MACCtrl, TxDisable | RxDisable | StatsDisable);\r\nfree_irq(pdev->irq, dev);\r\ndel_timer_sync (&np->timer);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nskb = np->rx_skbuff[i];\r\nif (skb) {\r\npci_unmap_single(pdev, desc_to_dma(&np->rx_ring[i]),\r\nskb->len, PCI_DMA_FROMDEVICE);\r\ndev_kfree_skb (skb);\r\nnp->rx_skbuff[i] = NULL;\r\n}\r\nnp->rx_ring[i].status = 0;\r\nnp->rx_ring[i].fraginfo = 0;\r\n}\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nskb = np->tx_skbuff[i];\r\nif (skb) {\r\npci_unmap_single(pdev, desc_to_dma(&np->tx_ring[i]),\r\nskb->len, PCI_DMA_TODEVICE);\r\ndev_kfree_skb (skb);\r\nnp->tx_skbuff[i] = NULL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nrio_remove1 (struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata (pdev);\r\nif (dev) {\r\nstruct netdev_private *np = netdev_priv(dev);\r\nunregister_netdev (dev);\r\npci_free_consistent (pdev, RX_TOTAL_SIZE, np->rx_ring,\r\nnp->rx_ring_dma);\r\npci_free_consistent (pdev, TX_TOTAL_SIZE, np->tx_ring,\r\nnp->tx_ring_dma);\r\n#ifdef MEM_MAPPING\r\npci_iounmap(pdev, np->ioaddr);\r\n#endif\r\npci_iounmap(pdev, np->eeprom_addr);\r\nfree_netdev (dev);\r\npci_release_regions (pdev);\r\npci_disable_device (pdev);\r\n}\r\npci_set_drvdata (pdev, NULL);\r\n}
