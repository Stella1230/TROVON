int\r\nftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,\r\nunsigned long frame_pointer)\r\n{\r\nunsigned long long calltime;\r\nint index;\r\nif (!current->ret_stack)\r\nreturn -EBUSY;\r\nsmp_rmb();\r\nif (current->curr_ret_stack == FTRACE_RETFUNC_DEPTH - 1) {\r\natomic_inc(&current->trace_overrun);\r\nreturn -EBUSY;\r\n}\r\ncalltime = trace_clock_local();\r\nindex = ++current->curr_ret_stack;\r\nbarrier();\r\ncurrent->ret_stack[index].ret = ret;\r\ncurrent->ret_stack[index].func = func;\r\ncurrent->ret_stack[index].calltime = calltime;\r\ncurrent->ret_stack[index].subtime = 0;\r\ncurrent->ret_stack[index].fp = frame_pointer;\r\n*depth = index;\r\nreturn 0;\r\n}\r\nstatic void\r\nftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret,\r\nunsigned long frame_pointer)\r\n{\r\nint index;\r\nindex = current->curr_ret_stack;\r\nif (unlikely(index < 0)) {\r\nftrace_graph_stop();\r\nWARN_ON(1);\r\n*ret = (unsigned long)panic;\r\nreturn;\r\n}\r\n#if defined(CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST) && !defined(CC_USING_FENTRY)\r\nif (unlikely(current->ret_stack[index].fp != frame_pointer)) {\r\nftrace_graph_stop();\r\nWARN(1, "Bad frame pointer: expected %lx, received %lx\n"\r\n" from func %ps return to %lx\n",\r\ncurrent->ret_stack[index].fp,\r\nframe_pointer,\r\n(void *)current->ret_stack[index].func,\r\ncurrent->ret_stack[index].ret);\r\n*ret = (unsigned long)panic;\r\nreturn;\r\n}\r\n#endif\r\n*ret = current->ret_stack[index].ret;\r\ntrace->func = current->ret_stack[index].func;\r\ntrace->calltime = current->ret_stack[index].calltime;\r\ntrace->overrun = atomic_read(&current->trace_overrun);\r\ntrace->depth = index;\r\n}\r\nunsigned long ftrace_return_to_handler(unsigned long frame_pointer)\r\n{\r\nstruct ftrace_graph_ret trace;\r\nunsigned long ret;\r\nftrace_pop_return_trace(&trace, &ret, frame_pointer);\r\ntrace.rettime = trace_clock_local();\r\nbarrier();\r\ncurrent->curr_ret_stack--;\r\nftrace_graph_return(&trace);\r\nif (unlikely(!ret)) {\r\nftrace_graph_stop();\r\nWARN_ON(1);\r\nret = (unsigned long)panic;\r\n}\r\nreturn ret;\r\n}\r\nint __trace_graph_entry(struct trace_array *tr,\r\nstruct ftrace_graph_ent *trace,\r\nunsigned long flags,\r\nint pc)\r\n{\r\nstruct ftrace_event_call *call = &event_funcgraph_entry;\r\nstruct ring_buffer_event *event;\r\nstruct ring_buffer *buffer = tr->trace_buffer.buffer;\r\nstruct ftrace_graph_ent_entry *entry;\r\nif (unlikely(__this_cpu_read(ftrace_cpu_disabled)))\r\nreturn 0;\r\nevent = trace_buffer_lock_reserve(buffer, TRACE_GRAPH_ENT,\r\nsizeof(*entry), flags, pc);\r\nif (!event)\r\nreturn 0;\r\nentry = ring_buffer_event_data(event);\r\nentry->graph_ent = *trace;\r\nif (!filter_current_check_discard(buffer, call, entry, event))\r\n__buffer_unlock_commit(buffer, event);\r\nreturn 1;\r\n}\r\nstatic inline int ftrace_graph_ignore_irqs(void)\r\n{\r\nif (!ftrace_graph_skip_irqs || trace_recursion_test(TRACE_IRQ_BIT))\r\nreturn 0;\r\nreturn in_irq();\r\n}\r\nint trace_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nstruct trace_array *tr = graph_array;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nlong disabled;\r\nint ret;\r\nint cpu;\r\nint pc;\r\nif (!ftrace_trace_task(current))\r\nreturn 0;\r\nif ((!(trace->depth || ftrace_graph_addr(trace->func)) ||\r\nftrace_graph_ignore_irqs()) ||\r\n(max_depth && trace->depth >= max_depth))\r\nreturn 0;\r\nlocal_irq_save(flags);\r\ncpu = raw_smp_processor_id();\r\ndata = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\ndisabled = atomic_inc_return(&data->disabled);\r\nif (likely(disabled == 1)) {\r\npc = preempt_count();\r\nret = __trace_graph_entry(tr, trace, flags, pc);\r\n} else {\r\nret = 0;\r\n}\r\natomic_dec(&data->disabled);\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nint trace_graph_thresh_entry(struct ftrace_graph_ent *trace)\r\n{\r\nif (tracing_thresh)\r\nreturn 1;\r\nelse\r\nreturn trace_graph_entry(trace);\r\n}\r\nstatic void\r\n__trace_graph_function(struct trace_array *tr,\r\nunsigned long ip, unsigned long flags, int pc)\r\n{\r\nu64 time = trace_clock_local();\r\nstruct ftrace_graph_ent ent = {\r\n.func = ip,\r\n.depth = 0,\r\n};\r\nstruct ftrace_graph_ret ret = {\r\n.func = ip,\r\n.depth = 0,\r\n.calltime = time,\r\n.rettime = time,\r\n};\r\n__trace_graph_entry(tr, &ent, flags, pc);\r\n__trace_graph_return(tr, &ret, flags, pc);\r\n}\r\nvoid\r\ntrace_graph_function(struct trace_array *tr,\r\nunsigned long ip, unsigned long parent_ip,\r\nunsigned long flags, int pc)\r\n{\r\n__trace_graph_function(tr, ip, flags, pc);\r\n}\r\nvoid __trace_graph_return(struct trace_array *tr,\r\nstruct ftrace_graph_ret *trace,\r\nunsigned long flags,\r\nint pc)\r\n{\r\nstruct ftrace_event_call *call = &event_funcgraph_exit;\r\nstruct ring_buffer_event *event;\r\nstruct ring_buffer *buffer = tr->trace_buffer.buffer;\r\nstruct ftrace_graph_ret_entry *entry;\r\nif (unlikely(__this_cpu_read(ftrace_cpu_disabled)))\r\nreturn;\r\nevent = trace_buffer_lock_reserve(buffer, TRACE_GRAPH_RET,\r\nsizeof(*entry), flags, pc);\r\nif (!event)\r\nreturn;\r\nentry = ring_buffer_event_data(event);\r\nentry->ret = *trace;\r\nif (!filter_current_check_discard(buffer, call, entry, event))\r\n__buffer_unlock_commit(buffer, event);\r\n}\r\nvoid trace_graph_return(struct ftrace_graph_ret *trace)\r\n{\r\nstruct trace_array *tr = graph_array;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nlong disabled;\r\nint cpu;\r\nint pc;\r\nlocal_irq_save(flags);\r\ncpu = raw_smp_processor_id();\r\ndata = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\ndisabled = atomic_inc_return(&data->disabled);\r\nif (likely(disabled == 1)) {\r\npc = preempt_count();\r\n__trace_graph_return(tr, trace, flags, pc);\r\n}\r\natomic_dec(&data->disabled);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid set_graph_array(struct trace_array *tr)\r\n{\r\ngraph_array = tr;\r\nsmp_mb();\r\n}\r\nvoid trace_graph_thresh_return(struct ftrace_graph_ret *trace)\r\n{\r\nif (tracing_thresh &&\r\n(trace->rettime - trace->calltime < tracing_thresh))\r\nreturn;\r\nelse\r\ntrace_graph_return(trace);\r\n}\r\nstatic int graph_trace_init(struct trace_array *tr)\r\n{\r\nint ret;\r\nset_graph_array(tr);\r\nif (tracing_thresh)\r\nret = register_ftrace_graph(&trace_graph_thresh_return,\r\n&trace_graph_thresh_entry);\r\nelse\r\nret = register_ftrace_graph(&trace_graph_return,\r\n&trace_graph_entry);\r\nif (ret)\r\nreturn ret;\r\ntracing_start_cmdline_record();\r\nreturn 0;\r\n}\r\nstatic void graph_trace_reset(struct trace_array *tr)\r\n{\r\ntracing_stop_cmdline_record();\r\nunregister_ftrace_graph();\r\n}\r\nstatic enum print_line_t\r\nprint_graph_cpu(struct trace_seq *s, int cpu)\r\n{\r\nint ret;\r\nret = trace_seq_printf(s, " %*d) ", max_bytes_for_cpu, cpu);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_proc(struct trace_seq *s, pid_t pid)\r\n{\r\nchar comm[TASK_COMM_LEN];\r\nchar pid_str[11];\r\nint spaces = 0;\r\nint ret;\r\nint len;\r\nint i;\r\ntrace_find_cmdline(pid, comm);\r\ncomm[7] = '\0';\r\nsprintf(pid_str, "%d", pid);\r\nlen = strlen(comm) + strlen(pid_str) + 1;\r\nif (len < TRACE_GRAPH_PROCINFO_LENGTH)\r\nspaces = TRACE_GRAPH_PROCINFO_LENGTH - len;\r\nfor (i = 0; i < spaces / 2; i++) {\r\nret = trace_seq_putc(s, ' ');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nret = trace_seq_printf(s, "%s-%s", comm, pid_str);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nfor (i = 0; i < spaces - (spaces / 2); i++) {\r\nret = trace_seq_putc(s, ' ');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_lat_fmt(struct trace_seq *s, struct trace_entry *entry)\r\n{\r\nif (!trace_seq_putc(s, ' '))\r\nreturn 0;\r\nreturn trace_print_lat_fmt(s, entry);\r\n}\r\nstatic enum print_line_t\r\nverif_pid(struct trace_seq *s, pid_t pid, int cpu, struct fgraph_data *data)\r\n{\r\npid_t prev_pid;\r\npid_t *last_pid;\r\nint ret;\r\nif (!data)\r\nreturn TRACE_TYPE_HANDLED;\r\nlast_pid = &(per_cpu_ptr(data->cpu_data, cpu)->last_pid);\r\nif (*last_pid == pid)\r\nreturn TRACE_TYPE_HANDLED;\r\nprev_pid = *last_pid;\r\n*last_pid = pid;\r\nif (prev_pid == -1)\r\nreturn TRACE_TYPE_HANDLED;\r\nret = trace_seq_puts(s,\r\n" ------------------------------------------\n");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = print_graph_cpu(s, cpu);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = print_graph_proc(s, prev_pid);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = trace_seq_puts(s, " => ");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = print_graph_proc(s, pid);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = trace_seq_puts(s,\r\n"\n ------------------------------------------\n\n");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic struct ftrace_graph_ret_entry *\r\nget_return_for_leaf(struct trace_iterator *iter,\r\nstruct ftrace_graph_ent_entry *curr)\r\n{\r\nstruct fgraph_data *data = iter->private;\r\nstruct ring_buffer_iter *ring_iter = NULL;\r\nstruct ring_buffer_event *event;\r\nstruct ftrace_graph_ret_entry *next;\r\nif (data && data->failed) {\r\ncurr = &data->ent;\r\nnext = &data->ret;\r\n} else {\r\nring_iter = trace_buffer_iter(iter, iter->cpu);\r\nif (ring_iter)\r\nevent = ring_buffer_iter_peek(ring_iter, NULL);\r\nelse {\r\nring_buffer_consume(iter->trace_buffer->buffer, iter->cpu,\r\nNULL, NULL);\r\nevent = ring_buffer_peek(iter->trace_buffer->buffer, iter->cpu,\r\nNULL, NULL);\r\n}\r\nif (!event)\r\nreturn NULL;\r\nnext = ring_buffer_event_data(event);\r\nif (data) {\r\ndata->ent = *curr;\r\nif (next->ent.type == TRACE_GRAPH_RET)\r\ndata->ret = *next;\r\nelse\r\ndata->ret.ent.type = next->ent.type;\r\n}\r\n}\r\nif (next->ent.type != TRACE_GRAPH_RET)\r\nreturn NULL;\r\nif (curr->ent.pid != next->ent.pid ||\r\ncurr->graph_ent.func != next->ret.func)\r\nreturn NULL;\r\nif (ring_iter)\r\nring_buffer_read(ring_iter, NULL);\r\nreturn next;\r\n}\r\nstatic int print_graph_abs_time(u64 t, struct trace_seq *s)\r\n{\r\nunsigned long usecs_rem;\r\nusecs_rem = do_div(t, NSEC_PER_SEC);\r\nusecs_rem /= 1000;\r\nreturn trace_seq_printf(s, "%5lu.%06lu | ",\r\n(unsigned long)t, usecs_rem);\r\n}\r\nstatic enum print_line_t\r\nprint_graph_irq(struct trace_iterator *iter, unsigned long addr,\r\nenum trace_type type, int cpu, pid_t pid, u32 flags)\r\n{\r\nint ret;\r\nstruct trace_seq *s = &iter->seq;\r\nif (addr < (unsigned long)__irqentry_text_start ||\r\naddr >= (unsigned long)__irqentry_text_end)\r\nreturn TRACE_TYPE_UNHANDLED;\r\nif (trace_flags & TRACE_ITER_CONTEXT_INFO) {\r\nif (flags & TRACE_GRAPH_PRINT_ABS_TIME) {\r\nret = print_graph_abs_time(iter->ts, s);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (flags & TRACE_GRAPH_PRINT_CPU) {\r\nret = print_graph_cpu(s, cpu);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (flags & TRACE_GRAPH_PRINT_PROC) {\r\nret = print_graph_proc(s, pid);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = trace_seq_puts(s, " | ");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\n}\r\nret = print_graph_duration(DURATION_FILL_START, s, flags);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\nif (type == TRACE_GRAPH_ENT)\r\nret = trace_seq_puts(s, "==========>");\r\nelse\r\nret = trace_seq_puts(s, "<==========");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = print_graph_duration(DURATION_FILL_END, s, flags);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\nret = trace_seq_putc(s, '\n');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nenum print_line_t\r\ntrace_print_graph_duration(unsigned long long duration, struct trace_seq *s)\r\n{\r\nunsigned long nsecs_rem = do_div(duration, 1000);\r\nchar msecs_str[21];\r\nchar nsecs_str[5];\r\nint ret, len;\r\nint i;\r\nsprintf(msecs_str, "%lu", (unsigned long) duration);\r\nret = trace_seq_printf(s, "%s", msecs_str);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nlen = strlen(msecs_str);\r\nif (len < 7) {\r\nsize_t slen = min_t(size_t, sizeof(nsecs_str), 8UL - len);\r\nsnprintf(nsecs_str, slen, "%03lu", nsecs_rem);\r\nret = trace_seq_printf(s, ".%s", nsecs_str);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nlen += strlen(nsecs_str);\r\n}\r\nret = trace_seq_puts(s, " us ");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nfor (i = len; i < 7; i++) {\r\nret = trace_seq_putc(s, ' ');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_duration(unsigned long long duration, struct trace_seq *s,\r\nu32 flags)\r\n{\r\nint ret = -1;\r\nif (!(flags & TRACE_GRAPH_PRINT_DURATION) ||\r\n!(trace_flags & TRACE_ITER_CONTEXT_INFO))\r\nreturn TRACE_TYPE_HANDLED;\r\nswitch (duration) {\r\ncase DURATION_FILL_FULL:\r\nret = trace_seq_puts(s, " | ");\r\nreturn ret ? TRACE_TYPE_HANDLED : TRACE_TYPE_PARTIAL_LINE;\r\ncase DURATION_FILL_START:\r\nret = trace_seq_puts(s, " ");\r\nreturn ret ? TRACE_TYPE_HANDLED : TRACE_TYPE_PARTIAL_LINE;\r\ncase DURATION_FILL_END:\r\nret = trace_seq_puts(s, " |");\r\nreturn ret ? TRACE_TYPE_HANDLED : TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (flags & TRACE_GRAPH_PRINT_OVERHEAD) {\r\nif (duration > 100000ULL)\r\nret = trace_seq_puts(s, "! ");\r\nelse if (duration > 10000ULL)\r\nret = trace_seq_puts(s, "+ ");\r\n}\r\nif (ret == -1)\r\nret = trace_seq_puts(s, " ");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = trace_print_graph_duration(duration, s);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\nret = trace_seq_puts(s, "| ");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_entry_leaf(struct trace_iterator *iter,\r\nstruct ftrace_graph_ent_entry *entry,\r\nstruct ftrace_graph_ret_entry *ret_entry,\r\nstruct trace_seq *s, u32 flags)\r\n{\r\nstruct fgraph_data *data = iter->private;\r\nstruct ftrace_graph_ret *graph_ret;\r\nstruct ftrace_graph_ent *call;\r\nunsigned long long duration;\r\nint ret;\r\nint i;\r\ngraph_ret = &ret_entry->ret;\r\ncall = &entry->graph_ent;\r\nduration = graph_ret->rettime - graph_ret->calltime;\r\nif (data) {\r\nstruct fgraph_cpu_data *cpu_data;\r\nint cpu = iter->cpu;\r\ncpu_data = per_cpu_ptr(data->cpu_data, cpu);\r\ncpu_data->depth = call->depth - 1;\r\nif (call->depth < FTRACE_RETFUNC_DEPTH)\r\ncpu_data->enter_funcs[call->depth] = 0;\r\n}\r\nret = print_graph_duration(duration, s, flags);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nfor (i = 0; i < call->depth * TRACE_GRAPH_INDENT; i++) {\r\nret = trace_seq_putc(s, ' ');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nret = trace_seq_printf(s, "%ps();\n", (void *)call->func);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_entry_nested(struct trace_iterator *iter,\r\nstruct ftrace_graph_ent_entry *entry,\r\nstruct trace_seq *s, int cpu, u32 flags)\r\n{\r\nstruct ftrace_graph_ent *call = &entry->graph_ent;\r\nstruct fgraph_data *data = iter->private;\r\nint ret;\r\nint i;\r\nif (data) {\r\nstruct fgraph_cpu_data *cpu_data;\r\nint cpu = iter->cpu;\r\ncpu_data = per_cpu_ptr(data->cpu_data, cpu);\r\ncpu_data->depth = call->depth;\r\nif (call->depth < FTRACE_RETFUNC_DEPTH)\r\ncpu_data->enter_funcs[call->depth] = call->func;\r\n}\r\nret = print_graph_duration(DURATION_FILL_FULL, s, flags);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\nfor (i = 0; i < call->depth * TRACE_GRAPH_INDENT; i++) {\r\nret = trace_seq_putc(s, ' ');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nret = trace_seq_printf(s, "%ps() {\n", (void *)call->func);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_NO_CONSUME;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_prologue(struct trace_iterator *iter, struct trace_seq *s,\r\nint type, unsigned long addr, u32 flags)\r\n{\r\nstruct fgraph_data *data = iter->private;\r\nstruct trace_entry *ent = iter->ent;\r\nint cpu = iter->cpu;\r\nint ret;\r\nif (verif_pid(s, ent->pid, cpu, data) == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nif (type) {\r\nret = print_graph_irq(iter, addr, type, cpu, ent->pid, flags);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (!(trace_flags & TRACE_ITER_CONTEXT_INFO))\r\nreturn 0;\r\nif (flags & TRACE_GRAPH_PRINT_ABS_TIME) {\r\nret = print_graph_abs_time(iter->ts, s);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (flags & TRACE_GRAPH_PRINT_CPU) {\r\nret = print_graph_cpu(s, cpu);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (flags & TRACE_GRAPH_PRINT_PROC) {\r\nret = print_graph_proc(s, ent->pid);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = trace_seq_puts(s, " | ");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (trace_flags & TRACE_ITER_LATENCY_FMT) {\r\nret = print_graph_lat_fmt(s, ent);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\ncheck_irq_entry(struct trace_iterator *iter, u32 flags,\r\nunsigned long addr, int depth)\r\n{\r\nint cpu = iter->cpu;\r\nint *depth_irq;\r\nstruct fgraph_data *data = iter->private;\r\nif ((flags & TRACE_GRAPH_PRINT_IRQS) ||\r\n(!data))\r\nreturn 0;\r\ndepth_irq = &(per_cpu_ptr(data->cpu_data, cpu)->depth_irq);\r\nif (*depth_irq >= 0)\r\nreturn 1;\r\nif ((addr < (unsigned long)__irqentry_text_start) ||\r\n(addr >= (unsigned long)__irqentry_text_end))\r\nreturn 0;\r\n*depth_irq = depth;\r\nreturn 1;\r\n}\r\nstatic int\r\ncheck_irq_return(struct trace_iterator *iter, u32 flags, int depth)\r\n{\r\nint cpu = iter->cpu;\r\nint *depth_irq;\r\nstruct fgraph_data *data = iter->private;\r\nif ((flags & TRACE_GRAPH_PRINT_IRQS) ||\r\n(!data))\r\nreturn 0;\r\ndepth_irq = &(per_cpu_ptr(data->cpu_data, cpu)->depth_irq);\r\nif (*depth_irq == -1)\r\nreturn 0;\r\nif (*depth_irq >= depth) {\r\n*depth_irq = -1;\r\nreturn 1;\r\n}\r\nreturn 1;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_entry(struct ftrace_graph_ent_entry *field, struct trace_seq *s,\r\nstruct trace_iterator *iter, u32 flags)\r\n{\r\nstruct fgraph_data *data = iter->private;\r\nstruct ftrace_graph_ent *call = &field->graph_ent;\r\nstruct ftrace_graph_ret_entry *leaf_ret;\r\nstatic enum print_line_t ret;\r\nint cpu = iter->cpu;\r\nif (check_irq_entry(iter, flags, call->func, call->depth))\r\nreturn TRACE_TYPE_HANDLED;\r\nif (print_graph_prologue(iter, s, TRACE_GRAPH_ENT, call->func, flags))\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nleaf_ret = get_return_for_leaf(iter, field);\r\nif (leaf_ret)\r\nret = print_graph_entry_leaf(iter, field, leaf_ret, s, flags);\r\nelse\r\nret = print_graph_entry_nested(iter, field, s, cpu, flags);\r\nif (data) {\r\nif (s->full) {\r\ndata->failed = 1;\r\ndata->cpu = cpu;\r\n} else\r\ndata->failed = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_return(struct ftrace_graph_ret *trace, struct trace_seq *s,\r\nstruct trace_entry *ent, struct trace_iterator *iter,\r\nu32 flags)\r\n{\r\nunsigned long long duration = trace->rettime - trace->calltime;\r\nstruct fgraph_data *data = iter->private;\r\npid_t pid = ent->pid;\r\nint cpu = iter->cpu;\r\nint func_match = 1;\r\nint ret;\r\nint i;\r\nif (check_irq_return(iter, flags, trace->depth))\r\nreturn TRACE_TYPE_HANDLED;\r\nif (data) {\r\nstruct fgraph_cpu_data *cpu_data;\r\nint cpu = iter->cpu;\r\ncpu_data = per_cpu_ptr(data->cpu_data, cpu);\r\ncpu_data->depth = trace->depth - 1;\r\nif (trace->depth < FTRACE_RETFUNC_DEPTH) {\r\nif (cpu_data->enter_funcs[trace->depth] != trace->func)\r\nfunc_match = 0;\r\ncpu_data->enter_funcs[trace->depth] = 0;\r\n}\r\n}\r\nif (print_graph_prologue(iter, s, 0, 0, flags))\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = print_graph_duration(duration, s, flags);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nfor (i = 0; i < trace->depth * TRACE_GRAPH_INDENT; i++) {\r\nret = trace_seq_putc(s, ' ');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (func_match) {\r\nret = trace_seq_puts(s, "}\n");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n} else {\r\nret = trace_seq_printf(s, "} /* %ps */\n", (void *)trace->func);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nif (flags & TRACE_GRAPH_PRINT_OVERRUN) {\r\nret = trace_seq_printf(s, " (Overruns: %lu)\n",\r\ntrace->overrun);\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nret = print_graph_irq(iter, trace->func, TRACE_GRAPH_RET,\r\ncpu, pid, flags);\r\nif (ret == TRACE_TYPE_PARTIAL_LINE)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_comment(struct trace_seq *s, struct trace_entry *ent,\r\nstruct trace_iterator *iter, u32 flags)\r\n{\r\nunsigned long sym_flags = (trace_flags & TRACE_ITER_SYM_MASK);\r\nstruct fgraph_data *data = iter->private;\r\nstruct trace_event *event;\r\nint depth = 0;\r\nint ret;\r\nint i;\r\nif (data)\r\ndepth = per_cpu_ptr(data->cpu_data, iter->cpu)->depth;\r\nif (print_graph_prologue(iter, s, 0, 0, flags))\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nret = print_graph_duration(DURATION_FILL_FULL, s, flags);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\nif (depth > 0)\r\nfor (i = 0; i < (depth + 1) * TRACE_GRAPH_INDENT; i++) {\r\nret = trace_seq_putc(s, ' ');\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\n}\r\nret = trace_seq_puts(s, "/* ");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nswitch (iter->ent->type) {\r\ncase TRACE_BPRINT:\r\nret = trace_print_bprintk_msg_only(iter);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\nbreak;\r\ncase TRACE_PRINT:\r\nret = trace_print_printk_msg_only(iter);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\nbreak;\r\ndefault:\r\nevent = ftrace_find_event(ent->type);\r\nif (!event)\r\nreturn TRACE_TYPE_UNHANDLED;\r\nret = event->funcs->trace(iter, sym_flags, event);\r\nif (ret != TRACE_TYPE_HANDLED)\r\nreturn ret;\r\n}\r\nif (s->buffer[s->len - 1] == '\n') {\r\ns->buffer[s->len - 1] = '\0';\r\ns->len--;\r\n}\r\nret = trace_seq_puts(s, " */\n");\r\nif (!ret)\r\nreturn TRACE_TYPE_PARTIAL_LINE;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nenum print_line_t\r\nprint_graph_function_flags(struct trace_iterator *iter, u32 flags)\r\n{\r\nstruct ftrace_graph_ent_entry *field;\r\nstruct fgraph_data *data = iter->private;\r\nstruct trace_entry *entry = iter->ent;\r\nstruct trace_seq *s = &iter->seq;\r\nint cpu = iter->cpu;\r\nint ret;\r\nif (data && per_cpu_ptr(data->cpu_data, cpu)->ignore) {\r\nper_cpu_ptr(data->cpu_data, cpu)->ignore = 0;\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nif (data && data->failed) {\r\nfield = &data->ent;\r\niter->cpu = data->cpu;\r\nret = print_graph_entry(field, s, iter, flags);\r\nif (ret == TRACE_TYPE_HANDLED && iter->cpu != cpu) {\r\nper_cpu_ptr(data->cpu_data, iter->cpu)->ignore = 1;\r\nret = TRACE_TYPE_NO_CONSUME;\r\n}\r\niter->cpu = cpu;\r\nreturn ret;\r\n}\r\nswitch (entry->type) {\r\ncase TRACE_GRAPH_ENT: {\r\nstruct ftrace_graph_ent_entry saved;\r\ntrace_assign_type(field, entry);\r\nsaved = *field;\r\nreturn print_graph_entry(&saved, s, iter, flags);\r\n}\r\ncase TRACE_GRAPH_RET: {\r\nstruct ftrace_graph_ret_entry *field;\r\ntrace_assign_type(field, entry);\r\nreturn print_graph_return(&field->ret, s, entry, iter, flags);\r\n}\r\ncase TRACE_STACK:\r\ncase TRACE_FN:\r\nreturn TRACE_TYPE_UNHANDLED;\r\ndefault:\r\nreturn print_graph_comment(s, entry, iter, flags);\r\n}\r\nreturn TRACE_TYPE_HANDLED;\r\n}\r\nstatic enum print_line_t\r\nprint_graph_function(struct trace_iterator *iter)\r\n{\r\nreturn print_graph_function_flags(iter, tracer_flags.val);\r\n}\r\nstatic enum print_line_t\r\nprint_graph_function_event(struct trace_iterator *iter, int flags,\r\nstruct trace_event *event)\r\n{\r\nreturn print_graph_function(iter);\r\n}\r\nstatic void print_lat_header(struct seq_file *s, u32 flags)\r\n{\r\nstatic const char spaces[] = " "\r\n" "\r\n" ";\r\nint size = 0;\r\nif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\r\nsize += 16;\r\nif (flags & TRACE_GRAPH_PRINT_CPU)\r\nsize += 4;\r\nif (flags & TRACE_GRAPH_PRINT_PROC)\r\nsize += 17;\r\nseq_printf(s, "#%.*s _-----=> irqs-off \n", size, spaces);\r\nseq_printf(s, "#%.*s / _----=> need-resched \n", size, spaces);\r\nseq_printf(s, "#%.*s| / _---=> hardirq/softirq \n", size, spaces);\r\nseq_printf(s, "#%.*s|| / _--=> preempt-depth \n", size, spaces);\r\nseq_printf(s, "#%.*s||| / \n", size, spaces);\r\n}\r\nstatic void __print_graph_headers_flags(struct seq_file *s, u32 flags)\r\n{\r\nint lat = trace_flags & TRACE_ITER_LATENCY_FMT;\r\nif (lat)\r\nprint_lat_header(s, flags);\r\nseq_printf(s, "#");\r\nif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\r\nseq_printf(s, " TIME ");\r\nif (flags & TRACE_GRAPH_PRINT_CPU)\r\nseq_printf(s, " CPU");\r\nif (flags & TRACE_GRAPH_PRINT_PROC)\r\nseq_printf(s, " TASK/PID ");\r\nif (lat)\r\nseq_printf(s, "||||");\r\nif (flags & TRACE_GRAPH_PRINT_DURATION)\r\nseq_printf(s, " DURATION ");\r\nseq_printf(s, " FUNCTION CALLS\n");\r\nseq_printf(s, "#");\r\nif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\r\nseq_printf(s, " | ");\r\nif (flags & TRACE_GRAPH_PRINT_CPU)\r\nseq_printf(s, " | ");\r\nif (flags & TRACE_GRAPH_PRINT_PROC)\r\nseq_printf(s, " | | ");\r\nif (lat)\r\nseq_printf(s, "||||");\r\nif (flags & TRACE_GRAPH_PRINT_DURATION)\r\nseq_printf(s, " | | ");\r\nseq_printf(s, " | | | |\n");\r\n}\r\nvoid print_graph_headers(struct seq_file *s)\r\n{\r\nprint_graph_headers_flags(s, tracer_flags.val);\r\n}\r\nvoid print_graph_headers_flags(struct seq_file *s, u32 flags)\r\n{\r\nstruct trace_iterator *iter = s->private;\r\nif (!(trace_flags & TRACE_ITER_CONTEXT_INFO))\r\nreturn;\r\nif (trace_flags & TRACE_ITER_LATENCY_FMT) {\r\nif (trace_empty(iter))\r\nreturn;\r\nprint_trace_header(s, iter);\r\n}\r\n__print_graph_headers_flags(s, flags);\r\n}\r\nvoid graph_trace_open(struct trace_iterator *iter)\r\n{\r\nstruct fgraph_data *data;\r\nint cpu;\r\niter->private = NULL;\r\ndata = kzalloc(sizeof(*data), GFP_KERNEL);\r\nif (!data)\r\ngoto out_err;\r\ndata->cpu_data = alloc_percpu(struct fgraph_cpu_data);\r\nif (!data->cpu_data)\r\ngoto out_err_free;\r\nfor_each_possible_cpu(cpu) {\r\npid_t *pid = &(per_cpu_ptr(data->cpu_data, cpu)->last_pid);\r\nint *depth = &(per_cpu_ptr(data->cpu_data, cpu)->depth);\r\nint *ignore = &(per_cpu_ptr(data->cpu_data, cpu)->ignore);\r\nint *depth_irq = &(per_cpu_ptr(data->cpu_data, cpu)->depth_irq);\r\n*pid = -1;\r\n*depth = 0;\r\n*ignore = 0;\r\n*depth_irq = -1;\r\n}\r\niter->private = data;\r\nreturn;\r\nout_err_free:\r\nkfree(data);\r\nout_err:\r\npr_warning("function graph tracer: not enough memory\n");\r\n}\r\nvoid graph_trace_close(struct trace_iterator *iter)\r\n{\r\nstruct fgraph_data *data = iter->private;\r\nif (data) {\r\nfree_percpu(data->cpu_data);\r\nkfree(data);\r\n}\r\n}\r\nstatic int func_graph_set_flag(u32 old_flags, u32 bit, int set)\r\n{\r\nif (bit == TRACE_GRAPH_PRINT_IRQS)\r\nftrace_graph_skip_irqs = !set;\r\nreturn 0;\r\n}\r\nstatic ssize_t\r\ngraph_depth_write(struct file *filp, const char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nunsigned long val;\r\nint ret;\r\nret = kstrtoul_from_user(ubuf, cnt, 10, &val);\r\nif (ret)\r\nreturn ret;\r\nmax_depth = val;\r\n*ppos += cnt;\r\nreturn cnt;\r\n}\r\nstatic ssize_t\r\ngraph_depth_read(struct file *filp, char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nchar buf[15];\r\nint n;\r\nn = sprintf(buf, "%d\n", max_depth);\r\nreturn simple_read_from_buffer(ubuf, cnt, ppos, buf, n);\r\n}\r\nstatic __init int init_graph_debugfs(void)\r\n{\r\nstruct dentry *d_tracer;\r\nd_tracer = tracing_init_dentry();\r\nif (!d_tracer)\r\nreturn 0;\r\ntrace_create_file("max_graph_depth", 0644, d_tracer,\r\nNULL, &graph_depth_fops);\r\nreturn 0;\r\n}\r\nstatic __init int init_graph_trace(void)\r\n{\r\nmax_bytes_for_cpu = snprintf(NULL, 0, "%d", nr_cpu_ids - 1);\r\nif (!register_ftrace_event(&graph_trace_entry_event)) {\r\npr_warning("Warning: could not register graph trace events\n");\r\nreturn 1;\r\n}\r\nif (!register_ftrace_event(&graph_trace_ret_event)) {\r\npr_warning("Warning: could not register graph trace events\n");\r\nreturn 1;\r\n}\r\nreturn register_tracer(&graph_trace);\r\n}
