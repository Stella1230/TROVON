static inline struct omap_dmadev *to_omap_dma_dev(struct dma_device *d)\r\n{\r\nreturn container_of(d, struct omap_dmadev, ddev);\r\n}\r\nstatic inline struct omap_chan *to_omap_dma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct omap_chan, vc.chan);\r\n}\r\nstatic inline struct omap_desc *to_omap_dma_desc(struct dma_async_tx_descriptor *t)\r\n{\r\nreturn container_of(t, struct omap_desc, vd.tx);\r\n}\r\nstatic void omap_dma_desc_free(struct virt_dma_desc *vd)\r\n{\r\nkfree(container_of(vd, struct omap_desc, vd));\r\n}\r\nstatic void omap_dma_start_sg(struct omap_chan *c, struct omap_desc *d,\r\nunsigned idx)\r\n{\r\nstruct omap_sg *sg = d->sg + idx;\r\nif (d->dir == DMA_DEV_TO_MEM)\r\nomap_set_dma_dest_params(c->dma_ch, OMAP_DMA_PORT_EMIFF,\r\nOMAP_DMA_AMODE_POST_INC, sg->addr, 0, 0);\r\nelse\r\nomap_set_dma_src_params(c->dma_ch, OMAP_DMA_PORT_EMIFF,\r\nOMAP_DMA_AMODE_POST_INC, sg->addr, 0, 0);\r\nomap_set_dma_transfer_params(c->dma_ch, d->es, sg->en, sg->fn,\r\nd->sync_mode, c->dma_sig, d->sync_type);\r\nomap_start_dma(c->dma_ch);\r\n}\r\nstatic void omap_dma_start_desc(struct omap_chan *c)\r\n{\r\nstruct virt_dma_desc *vd = vchan_next_desc(&c->vc);\r\nstruct omap_desc *d;\r\nif (!vd) {\r\nc->desc = NULL;\r\nreturn;\r\n}\r\nlist_del(&vd->node);\r\nc->desc = d = to_omap_dma_desc(&vd->tx);\r\nc->sgidx = 0;\r\nif (d->dir == DMA_DEV_TO_MEM)\r\nomap_set_dma_src_params(c->dma_ch, d->periph_port,\r\nOMAP_DMA_AMODE_CONSTANT, d->dev_addr, 0, d->fi);\r\nelse\r\nomap_set_dma_dest_params(c->dma_ch, d->periph_port,\r\nOMAP_DMA_AMODE_CONSTANT, d->dev_addr, 0, d->fi);\r\nomap_dma_start_sg(c, d, 0);\r\n}\r\nstatic void omap_dma_callback(int ch, u16 status, void *data)\r\n{\r\nstruct omap_chan *c = data;\r\nstruct omap_desc *d;\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nd = c->desc;\r\nif (d) {\r\nif (!c->cyclic) {\r\nif (++c->sgidx < d->sglen) {\r\nomap_dma_start_sg(c, d, c->sgidx);\r\n} else {\r\nomap_dma_start_desc(c);\r\nvchan_cookie_complete(&d->vd);\r\n}\r\n} else {\r\nvchan_cyclic_callback(&d->vd);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nstatic void omap_dma_sched(unsigned long data)\r\n{\r\nstruct omap_dmadev *d = (struct omap_dmadev *)data;\r\nLIST_HEAD(head);\r\nspin_lock_irq(&d->lock);\r\nlist_splice_tail_init(&d->pending, &head);\r\nspin_unlock_irq(&d->lock);\r\nwhile (!list_empty(&head)) {\r\nstruct omap_chan *c = list_first_entry(&head,\r\nstruct omap_chan, node);\r\nspin_lock_irq(&c->vc.lock);\r\nlist_del_init(&c->node);\r\nomap_dma_start_desc(c);\r\nspin_unlock_irq(&c->vc.lock);\r\n}\r\n}\r\nstatic int omap_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\ndev_info(c->vc.chan.device->dev, "allocating channel for %u\n", c->dma_sig);\r\nreturn omap_request_dma(c->dma_sig, "DMA engine",\r\nomap_dma_callback, c, &c->dma_ch);\r\n}\r\nstatic void omap_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nvchan_free_chan_resources(&c->vc);\r\nomap_free_dma(c->dma_ch);\r\ndev_info(c->vc.chan.device->dev, "freeing channel for %u\n", c->dma_sig);\r\n}\r\nstatic size_t omap_dma_sg_size(struct omap_sg *sg)\r\n{\r\nreturn sg->en * sg->fn;\r\n}\r\nstatic size_t omap_dma_desc_size(struct omap_desc *d)\r\n{\r\nunsigned i;\r\nsize_t size;\r\nfor (size = i = 0; i < d->sglen; i++)\r\nsize += omap_dma_sg_size(&d->sg[i]);\r\nreturn size * es_bytes[d->es];\r\n}\r\nstatic size_t omap_dma_desc_size_pos(struct omap_desc *d, dma_addr_t addr)\r\n{\r\nunsigned i;\r\nsize_t size, es_size = es_bytes[d->es];\r\nfor (size = i = 0; i < d->sglen; i++) {\r\nsize_t this_size = omap_dma_sg_size(&d->sg[i]) * es_size;\r\nif (size)\r\nsize += this_size;\r\nelse if (addr >= d->sg[i].addr &&\r\naddr < d->sg[i].addr + this_size)\r\nsize += d->sg[i].addr + this_size - addr;\r\n}\r\nreturn size;\r\n}\r\nstatic enum dma_status omap_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct virt_dma_desc *vd;\r\nenum dma_status ret;\r\nunsigned long flags;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_SUCCESS || !txstate)\r\nreturn ret;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nvd = vchan_find_desc(&c->vc, cookie);\r\nif (vd) {\r\ntxstate->residue = omap_dma_desc_size(to_omap_dma_desc(&vd->tx));\r\n} else if (c->desc && c->desc->vd.tx.cookie == cookie) {\r\nstruct omap_desc *d = c->desc;\r\ndma_addr_t pos;\r\nif (d->dir == DMA_MEM_TO_DEV)\r\npos = omap_get_dma_src_pos(c->dma_ch);\r\nelse if (d->dir == DMA_DEV_TO_MEM)\r\npos = omap_get_dma_dst_pos(c->dma_ch);\r\nelse\r\npos = 0;\r\ntxstate->residue = omap_dma_desc_size_pos(d, pos);\r\n} else {\r\ntxstate->residue = 0;\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nreturn ret;\r\n}\r\nstatic void omap_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (vchan_issue_pending(&c->vc) && !c->desc) {\r\nif (!c->cyclic) {\r\nstruct omap_dmadev *d = to_omap_dma_dev(chan->device);\r\nspin_lock(&d->lock);\r\nif (list_empty(&c->node))\r\nlist_add_tail(&c->node, &d->pending);\r\nspin_unlock(&d->lock);\r\ntasklet_schedule(&d->task);\r\n} else {\r\nomap_dma_start_desc(c);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned sglen,\r\nenum dma_transfer_direction dir, unsigned long tx_flags, void *context)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nenum dma_slave_buswidth dev_width;\r\nstruct scatterlist *sgent;\r\nstruct omap_desc *d;\r\ndma_addr_t dev_addr;\r\nunsigned i, j = 0, es, en, frame_bytes, sync_type;\r\nu32 burst;\r\nif (dir == DMA_DEV_TO_MEM) {\r\ndev_addr = c->cfg.src_addr;\r\ndev_width = c->cfg.src_addr_width;\r\nburst = c->cfg.src_maxburst;\r\nsync_type = OMAP_DMA_SRC_SYNC;\r\n} else if (dir == DMA_MEM_TO_DEV) {\r\ndev_addr = c->cfg.dst_addr;\r\ndev_width = c->cfg.dst_addr_width;\r\nburst = c->cfg.dst_maxburst;\r\nsync_type = OMAP_DMA_DST_SYNC;\r\n} else {\r\ndev_err(chan->device->dev, "%s: bad direction?\n", __func__);\r\nreturn NULL;\r\n}\r\nswitch (dev_width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nes = OMAP_DMA_DATA_TYPE_S8;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nes = OMAP_DMA_DATA_TYPE_S16;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nes = OMAP_DMA_DATA_TYPE_S32;\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nd = kzalloc(sizeof(*d) + sglen * sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\nd->dir = dir;\r\nd->dev_addr = dev_addr;\r\nd->es = es;\r\nd->sync_mode = OMAP_DMA_SYNC_FRAME;\r\nd->sync_type = sync_type;\r\nd->periph_port = OMAP_DMA_PORT_TIPB;\r\nen = burst;\r\nframe_bytes = es_bytes[es] * en;\r\nfor_each_sg(sgl, sgent, sglen, i) {\r\nd->sg[j].addr = sg_dma_address(sgent);\r\nd->sg[j].en = en;\r\nd->sg[j].fn = sg_dma_len(sgent) / frame_bytes;\r\nj++;\r\n}\r\nd->sglen = j;\r\nreturn vchan_tx_prep(&c->vc, &d->vd, tx_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction dir, unsigned long flags,\r\nvoid *context)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nenum dma_slave_buswidth dev_width;\r\nstruct omap_desc *d;\r\ndma_addr_t dev_addr;\r\nunsigned es, sync_type;\r\nu32 burst;\r\nif (dir == DMA_DEV_TO_MEM) {\r\ndev_addr = c->cfg.src_addr;\r\ndev_width = c->cfg.src_addr_width;\r\nburst = c->cfg.src_maxburst;\r\nsync_type = OMAP_DMA_SRC_SYNC;\r\n} else if (dir == DMA_MEM_TO_DEV) {\r\ndev_addr = c->cfg.dst_addr;\r\ndev_width = c->cfg.dst_addr_width;\r\nburst = c->cfg.dst_maxburst;\r\nsync_type = OMAP_DMA_DST_SYNC;\r\n} else {\r\ndev_err(chan->device->dev, "%s: bad direction?\n", __func__);\r\nreturn NULL;\r\n}\r\nswitch (dev_width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nes = OMAP_DMA_DATA_TYPE_S8;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nes = OMAP_DMA_DATA_TYPE_S16;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nes = OMAP_DMA_DATA_TYPE_S32;\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nd = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\nd->dir = dir;\r\nd->dev_addr = dev_addr;\r\nd->fi = burst;\r\nd->es = es;\r\nif (burst)\r\nd->sync_mode = OMAP_DMA_SYNC_PACKET;\r\nelse\r\nd->sync_mode = OMAP_DMA_SYNC_ELEMENT;\r\nd->sync_type = sync_type;\r\nd->periph_port = OMAP_DMA_PORT_MPUI;\r\nd->sg[0].addr = buf_addr;\r\nd->sg[0].en = period_len / es_bytes[es];\r\nd->sg[0].fn = buf_len / period_len;\r\nd->sglen = 1;\r\nif (!c->cyclic) {\r\nc->cyclic = true;\r\nomap_dma_link_lch(c->dma_ch, c->dma_ch);\r\nif (flags & DMA_PREP_INTERRUPT)\r\nomap_enable_dma_irq(c->dma_ch, OMAP_DMA_FRAME_IRQ);\r\nomap_disable_dma_irq(c->dma_ch, OMAP_DMA_BLOCK_IRQ);\r\n}\r\nif (dma_omap2plus()) {\r\nomap_set_dma_src_burst_mode(c->dma_ch, OMAP_DMA_DATA_BURST_16);\r\nomap_set_dma_dest_burst_mode(c->dma_ch, OMAP_DMA_DATA_BURST_16);\r\n}\r\nreturn vchan_tx_prep(&c->vc, &d->vd, flags);\r\n}\r\nstatic int omap_dma_slave_config(struct omap_chan *c, struct dma_slave_config *cfg)\r\n{\r\nif (cfg->src_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES ||\r\ncfg->dst_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES)\r\nreturn -EINVAL;\r\nmemcpy(&c->cfg, cfg, sizeof(c->cfg));\r\nreturn 0;\r\n}\r\nstatic int omap_dma_terminate_all(struct omap_chan *c)\r\n{\r\nstruct omap_dmadev *d = to_omap_dma_dev(c->vc.chan.device);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nspin_lock(&d->lock);\r\nlist_del_init(&c->node);\r\nspin_unlock(&d->lock);\r\nif (c->desc) {\r\nc->desc = NULL;\r\nif (!c->paused)\r\nomap_stop_dma(c->dma_ch);\r\n}\r\nif (c->cyclic) {\r\nc->cyclic = false;\r\nc->paused = false;\r\nomap_dma_unlink_lch(c->dma_ch, c->dma_ch);\r\n}\r\nvchan_get_all_descriptors(&c->vc, &head);\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nvchan_dma_desc_free_list(&c->vc, &head);\r\nreturn 0;\r\n}\r\nstatic int omap_dma_pause(struct omap_chan *c)\r\n{\r\nif (!c->cyclic)\r\nreturn -EINVAL;\r\nif (!c->paused) {\r\nomap_stop_dma(c->dma_ch);\r\nc->paused = true;\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_dma_resume(struct omap_chan *c)\r\n{\r\nif (!c->cyclic)\r\nreturn -EINVAL;\r\nif (c->paused) {\r\nomap_start_dma(c->dma_ch);\r\nc->paused = false;\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_dma_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nint ret;\r\nswitch (cmd) {\r\ncase DMA_SLAVE_CONFIG:\r\nret = omap_dma_slave_config(c, (struct dma_slave_config *)arg);\r\nbreak;\r\ncase DMA_TERMINATE_ALL:\r\nret = omap_dma_terminate_all(c);\r\nbreak;\r\ncase DMA_PAUSE:\r\nret = omap_dma_pause(c);\r\nbreak;\r\ncase DMA_RESUME:\r\nret = omap_dma_resume(c);\r\nbreak;\r\ndefault:\r\nret = -ENXIO;\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int omap_dma_chan_init(struct omap_dmadev *od, int dma_sig)\r\n{\r\nstruct omap_chan *c;\r\nc = kzalloc(sizeof(*c), GFP_KERNEL);\r\nif (!c)\r\nreturn -ENOMEM;\r\nc->dma_sig = dma_sig;\r\nc->vc.desc_free = omap_dma_desc_free;\r\nvchan_init(&c->vc, &od->ddev);\r\nINIT_LIST_HEAD(&c->node);\r\nod->ddev.chancnt++;\r\nreturn 0;\r\n}\r\nstatic void omap_dma_free(struct omap_dmadev *od)\r\n{\r\ntasklet_kill(&od->task);\r\nwhile (!list_empty(&od->ddev.channels)) {\r\nstruct omap_chan *c = list_first_entry(&od->ddev.channels,\r\nstruct omap_chan, vc.chan.device_node);\r\nlist_del(&c->vc.chan.device_node);\r\ntasklet_kill(&c->vc.task);\r\nkfree(c);\r\n}\r\nkfree(od);\r\n}\r\nstatic int omap_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct omap_dmadev *od;\r\nint rc, i;\r\nod = kzalloc(sizeof(*od), GFP_KERNEL);\r\nif (!od)\r\nreturn -ENOMEM;\r\ndma_cap_set(DMA_SLAVE, od->ddev.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, od->ddev.cap_mask);\r\nod->ddev.device_alloc_chan_resources = omap_dma_alloc_chan_resources;\r\nod->ddev.device_free_chan_resources = omap_dma_free_chan_resources;\r\nod->ddev.device_tx_status = omap_dma_tx_status;\r\nod->ddev.device_issue_pending = omap_dma_issue_pending;\r\nod->ddev.device_prep_slave_sg = omap_dma_prep_slave_sg;\r\nod->ddev.device_prep_dma_cyclic = omap_dma_prep_dma_cyclic;\r\nod->ddev.device_control = omap_dma_control;\r\nod->ddev.dev = &pdev->dev;\r\nINIT_LIST_HEAD(&od->ddev.channels);\r\nINIT_LIST_HEAD(&od->pending);\r\nspin_lock_init(&od->lock);\r\ntasklet_init(&od->task, omap_dma_sched, (unsigned long)od);\r\nfor (i = 0; i < 127; i++) {\r\nrc = omap_dma_chan_init(od, i);\r\nif (rc) {\r\nomap_dma_free(od);\r\nreturn rc;\r\n}\r\n}\r\nrc = dma_async_device_register(&od->ddev);\r\nif (rc) {\r\npr_warn("OMAP-DMA: failed to register slave DMA engine device: %d\n",\r\nrc);\r\nomap_dma_free(od);\r\nreturn rc;\r\n}\r\nplatform_set_drvdata(pdev, od);\r\nif (pdev->dev.of_node) {\r\nomap_dma_info.dma_cap = od->ddev.cap_mask;\r\nrc = of_dma_controller_register(pdev->dev.of_node,\r\nof_dma_simple_xlate, &omap_dma_info);\r\nif (rc) {\r\npr_warn("OMAP-DMA: failed to register DMA controller\n");\r\ndma_async_device_unregister(&od->ddev);\r\nomap_dma_free(od);\r\n}\r\n}\r\ndev_info(&pdev->dev, "OMAP DMA engine driver\n");\r\nreturn rc;\r\n}\r\nstatic int omap_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct omap_dmadev *od = platform_get_drvdata(pdev);\r\nif (pdev->dev.of_node)\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&od->ddev);\r\nomap_dma_free(od);\r\nreturn 0;\r\n}\r\nbool omap_dma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nif (chan->device->dev->driver == &omap_dma_driver.driver) {\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nunsigned req = *(unsigned *)param;\r\nreturn req == c->dma_sig;\r\n}\r\nreturn false;\r\n}\r\nstatic int omap_dma_init(void)\r\n{\r\nreturn platform_driver_register(&omap_dma_driver);\r\n}\r\nstatic void __exit omap_dma_exit(void)\r\n{\r\nplatform_driver_unregister(&omap_dma_driver);\r\n}
