static inline bool is_obj_pfmemalloc(void *objp)\r\n{\r\nreturn (unsigned long)objp & SLAB_OBJ_PFMEMALLOC;\r\n}\r\nstatic inline void set_obj_pfmemalloc(void **objp)\r\n{\r\n*objp = (void *)((unsigned long)*objp | SLAB_OBJ_PFMEMALLOC);\r\nreturn;\r\n}\r\nstatic inline void clear_obj_pfmemalloc(void **objp)\r\n{\r\n*objp = (void *)((unsigned long)*objp & ~SLAB_OBJ_PFMEMALLOC);\r\n}\r\nstatic void kmem_cache_node_init(struct kmem_cache_node *parent)\r\n{\r\nINIT_LIST_HEAD(&parent->slabs_full);\r\nINIT_LIST_HEAD(&parent->slabs_partial);\r\nINIT_LIST_HEAD(&parent->slabs_free);\r\nparent->shared = NULL;\r\nparent->alien = NULL;\r\nparent->colour_next = 0;\r\nspin_lock_init(&parent->list_lock);\r\nparent->free_objects = 0;\r\nparent->free_touched = 0;\r\n}\r\nstatic int obj_offset(struct kmem_cache *cachep)\r\n{\r\nreturn cachep->obj_offset;\r\n}\r\nstatic unsigned long long *dbg_redzone1(struct kmem_cache *cachep, void *objp)\r\n{\r\nBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\r\nreturn (unsigned long long*) (objp + obj_offset(cachep) -\r\nsizeof(unsigned long long));\r\n}\r\nstatic unsigned long long *dbg_redzone2(struct kmem_cache *cachep, void *objp)\r\n{\r\nBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\r\nif (cachep->flags & SLAB_STORE_USER)\r\nreturn (unsigned long long *)(objp + cachep->size -\r\nsizeof(unsigned long long) -\r\nREDZONE_ALIGN);\r\nreturn (unsigned long long *) (objp + cachep->size -\r\nsizeof(unsigned long long));\r\n}\r\nstatic void **dbg_userword(struct kmem_cache *cachep, void *objp)\r\n{\r\nBUG_ON(!(cachep->flags & SLAB_STORE_USER));\r\nreturn (void **)(objp + cachep->size - BYTES_PER_WORD);\r\n}\r\nstatic inline struct kmem_cache *virt_to_cache(const void *obj)\r\n{\r\nstruct page *page = virt_to_head_page(obj);\r\nreturn page->slab_cache;\r\n}\r\nstatic inline struct slab *virt_to_slab(const void *obj)\r\n{\r\nstruct page *page = virt_to_head_page(obj);\r\nVM_BUG_ON(!PageSlab(page));\r\nreturn page->slab_page;\r\n}\r\nstatic inline void *index_to_obj(struct kmem_cache *cache, struct slab *slab,\r\nunsigned int idx)\r\n{\r\nreturn slab->s_mem + cache->size * idx;\r\n}\r\nstatic inline unsigned int obj_to_index(const struct kmem_cache *cache,\r\nconst struct slab *slab, void *obj)\r\n{\r\nu32 offset = (obj - slab->s_mem);\r\nreturn reciprocal_divide(offset, cache->reciprocal_buffer_size);\r\n}\r\nstatic void slab_set_lock_classes(struct kmem_cache *cachep,\r\nstruct lock_class_key *l3_key, struct lock_class_key *alc_key,\r\nint q)\r\n{\r\nstruct array_cache **alc;\r\nstruct kmem_cache_node *n;\r\nint r;\r\nn = cachep->node[q];\r\nif (!n)\r\nreturn;\r\nlockdep_set_class(&n->list_lock, l3_key);\r\nalc = n->alien;\r\nif (!alc || (unsigned long)alc == BAD_ALIEN_MAGIC)\r\nreturn;\r\nfor_each_node(r) {\r\nif (alc[r])\r\nlockdep_set_class(&alc[r]->lock, alc_key);\r\n}\r\n}\r\nstatic void slab_set_debugobj_lock_classes_node(struct kmem_cache *cachep, int node)\r\n{\r\nslab_set_lock_classes(cachep, &debugobj_l3_key, &debugobj_alc_key, node);\r\n}\r\nstatic void slab_set_debugobj_lock_classes(struct kmem_cache *cachep)\r\n{\r\nint node;\r\nfor_each_online_node(node)\r\nslab_set_debugobj_lock_classes_node(cachep, node);\r\n}\r\nstatic void init_node_lock_keys(int q)\r\n{\r\nint i;\r\nif (slab_state < UP)\r\nreturn;\r\nfor (i = 1; i <= KMALLOC_SHIFT_HIGH; i++) {\r\nstruct kmem_cache_node *n;\r\nstruct kmem_cache *cache = kmalloc_caches[i];\r\nif (!cache)\r\ncontinue;\r\nn = cache->node[q];\r\nif (!n || OFF_SLAB(cache))\r\ncontinue;\r\nslab_set_lock_classes(cache, &on_slab_l3_key,\r\n&on_slab_alc_key, q);\r\n}\r\n}\r\nstatic void on_slab_lock_classes_node(struct kmem_cache *cachep, int q)\r\n{\r\nif (!cachep->node[q])\r\nreturn;\r\nslab_set_lock_classes(cachep, &on_slab_l3_key,\r\n&on_slab_alc_key, q);\r\n}\r\nstatic inline void on_slab_lock_classes(struct kmem_cache *cachep)\r\n{\r\nint node;\r\nVM_BUG_ON(OFF_SLAB(cachep));\r\nfor_each_node(node)\r\non_slab_lock_classes_node(cachep, node);\r\n}\r\nstatic inline void init_lock_keys(void)\r\n{\r\nint node;\r\nfor_each_node(node)\r\ninit_node_lock_keys(node);\r\n}\r\nstatic void init_node_lock_keys(int q)\r\n{\r\n}\r\nstatic inline void init_lock_keys(void)\r\n{\r\n}\r\nstatic inline void on_slab_lock_classes(struct kmem_cache *cachep)\r\n{\r\n}\r\nstatic inline void on_slab_lock_classes_node(struct kmem_cache *cachep, int node)\r\n{\r\n}\r\nstatic void slab_set_debugobj_lock_classes_node(struct kmem_cache *cachep, int node)\r\n{\r\n}\r\nstatic void slab_set_debugobj_lock_classes(struct kmem_cache *cachep)\r\n{\r\n}\r\nstatic inline struct array_cache *cpu_cache_get(struct kmem_cache *cachep)\r\n{\r\nreturn cachep->array[smp_processor_id()];\r\n}\r\nstatic size_t slab_mgmt_size(size_t nr_objs, size_t align)\r\n{\r\nreturn ALIGN(sizeof(struct slab)+nr_objs*sizeof(kmem_bufctl_t), align);\r\n}\r\nstatic void cache_estimate(unsigned long gfporder, size_t buffer_size,\r\nsize_t align, int flags, size_t *left_over,\r\nunsigned int *num)\r\n{\r\nint nr_objs;\r\nsize_t mgmt_size;\r\nsize_t slab_size = PAGE_SIZE << gfporder;\r\nif (flags & CFLGS_OFF_SLAB) {\r\nmgmt_size = 0;\r\nnr_objs = slab_size / buffer_size;\r\nif (nr_objs > SLAB_LIMIT)\r\nnr_objs = SLAB_LIMIT;\r\n} else {\r\nnr_objs = (slab_size - sizeof(struct slab)) /\r\n(buffer_size + sizeof(kmem_bufctl_t));\r\nif (slab_mgmt_size(nr_objs, align) + nr_objs*buffer_size\r\n> slab_size)\r\nnr_objs--;\r\nif (nr_objs > SLAB_LIMIT)\r\nnr_objs = SLAB_LIMIT;\r\nmgmt_size = slab_mgmt_size(nr_objs, align);\r\n}\r\n*num = nr_objs;\r\n*left_over = slab_size - nr_objs*buffer_size - mgmt_size;\r\n}\r\nstatic void __slab_error(const char *function, struct kmem_cache *cachep,\r\nchar *msg)\r\n{\r\nprintk(KERN_ERR "slab error in %s(): cache `%s': %s\n",\r\nfunction, cachep->name, msg);\r\ndump_stack();\r\nadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\r\n}\r\nstatic int __init noaliencache_setup(char *s)\r\n{\r\nuse_alien_caches = 0;\r\nreturn 1;\r\n}\r\nstatic int __init slab_max_order_setup(char *str)\r\n{\r\nget_option(&str, &slab_max_order);\r\nslab_max_order = slab_max_order < 0 ? 0 :\r\nmin(slab_max_order, MAX_ORDER - 1);\r\nslab_max_order_set = true;\r\nreturn 1;\r\n}\r\nstatic void init_reap_node(int cpu)\r\n{\r\nint node;\r\nnode = next_node(cpu_to_mem(cpu), node_online_map);\r\nif (node == MAX_NUMNODES)\r\nnode = first_node(node_online_map);\r\nper_cpu(slab_reap_node, cpu) = node;\r\n}\r\nstatic void next_reap_node(void)\r\n{\r\nint node = __this_cpu_read(slab_reap_node);\r\nnode = next_node(node, node_online_map);\r\nif (unlikely(node >= MAX_NUMNODES))\r\nnode = first_node(node_online_map);\r\n__this_cpu_write(slab_reap_node, node);\r\n}\r\nstatic void start_cpu_timer(int cpu)\r\n{\r\nstruct delayed_work *reap_work = &per_cpu(slab_reap_work, cpu);\r\nif (keventd_up() && reap_work->work.func == NULL) {\r\ninit_reap_node(cpu);\r\nINIT_DEFERRABLE_WORK(reap_work, cache_reap);\r\nschedule_delayed_work_on(cpu, reap_work,\r\n__round_jiffies_relative(HZ, cpu));\r\n}\r\n}\r\nstatic struct array_cache *alloc_arraycache(int node, int entries,\r\nint batchcount, gfp_t gfp)\r\n{\r\nint memsize = sizeof(void *) * entries + sizeof(struct array_cache);\r\nstruct array_cache *nc = NULL;\r\nnc = kmalloc_node(memsize, gfp, node);\r\nkmemleak_no_scan(nc);\r\nif (nc) {\r\nnc->avail = 0;\r\nnc->limit = entries;\r\nnc->batchcount = batchcount;\r\nnc->touched = 0;\r\nspin_lock_init(&nc->lock);\r\n}\r\nreturn nc;\r\n}\r\nstatic inline bool is_slab_pfmemalloc(struct slab *slabp)\r\n{\r\nstruct page *page = virt_to_page(slabp->s_mem);\r\nreturn PageSlabPfmemalloc(page);\r\n}\r\nstatic void recheck_pfmemalloc_active(struct kmem_cache *cachep,\r\nstruct array_cache *ac)\r\n{\r\nstruct kmem_cache_node *n = cachep->node[numa_mem_id()];\r\nstruct slab *slabp;\r\nunsigned long flags;\r\nif (!pfmemalloc_active)\r\nreturn;\r\nspin_lock_irqsave(&n->list_lock, flags);\r\nlist_for_each_entry(slabp, &n->slabs_full, list)\r\nif (is_slab_pfmemalloc(slabp))\r\ngoto out;\r\nlist_for_each_entry(slabp, &n->slabs_partial, list)\r\nif (is_slab_pfmemalloc(slabp))\r\ngoto out;\r\nlist_for_each_entry(slabp, &n->slabs_free, list)\r\nif (is_slab_pfmemalloc(slabp))\r\ngoto out;\r\npfmemalloc_active = false;\r\nout:\r\nspin_unlock_irqrestore(&n->list_lock, flags);\r\n}\r\nstatic void *__ac_get_obj(struct kmem_cache *cachep, struct array_cache *ac,\r\ngfp_t flags, bool force_refill)\r\n{\r\nint i;\r\nvoid *objp = ac->entry[--ac->avail];\r\nif (unlikely(is_obj_pfmemalloc(objp))) {\r\nstruct kmem_cache_node *n;\r\nif (gfp_pfmemalloc_allowed(flags)) {\r\nclear_obj_pfmemalloc(&objp);\r\nreturn objp;\r\n}\r\nfor (i = 0; i < ac->avail; i++) {\r\nif (!is_obj_pfmemalloc(ac->entry[i])) {\r\nobjp = ac->entry[i];\r\nac->entry[i] = ac->entry[ac->avail];\r\nac->entry[ac->avail] = objp;\r\nreturn objp;\r\n}\r\n}\r\nn = cachep->node[numa_mem_id()];\r\nif (!list_empty(&n->slabs_free) && force_refill) {\r\nstruct slab *slabp = virt_to_slab(objp);\r\nClearPageSlabPfmemalloc(virt_to_head_page(slabp->s_mem));\r\nclear_obj_pfmemalloc(&objp);\r\nrecheck_pfmemalloc_active(cachep, ac);\r\nreturn objp;\r\n}\r\nac->avail++;\r\nobjp = NULL;\r\n}\r\nreturn objp;\r\n}\r\nstatic inline void *ac_get_obj(struct kmem_cache *cachep,\r\nstruct array_cache *ac, gfp_t flags, bool force_refill)\r\n{\r\nvoid *objp;\r\nif (unlikely(sk_memalloc_socks()))\r\nobjp = __ac_get_obj(cachep, ac, flags, force_refill);\r\nelse\r\nobjp = ac->entry[--ac->avail];\r\nreturn objp;\r\n}\r\nstatic void *__ac_put_obj(struct kmem_cache *cachep, struct array_cache *ac,\r\nvoid *objp)\r\n{\r\nif (unlikely(pfmemalloc_active)) {\r\nstruct page *page = virt_to_head_page(objp);\r\nif (PageSlabPfmemalloc(page))\r\nset_obj_pfmemalloc(&objp);\r\n}\r\nreturn objp;\r\n}\r\nstatic inline void ac_put_obj(struct kmem_cache *cachep, struct array_cache *ac,\r\nvoid *objp)\r\n{\r\nif (unlikely(sk_memalloc_socks()))\r\nobjp = __ac_put_obj(cachep, ac, objp);\r\nac->entry[ac->avail++] = objp;\r\n}\r\nstatic int transfer_objects(struct array_cache *to,\r\nstruct array_cache *from, unsigned int max)\r\n{\r\nint nr = min3(from->avail, max, to->limit - to->avail);\r\nif (!nr)\r\nreturn 0;\r\nmemcpy(to->entry + to->avail, from->entry + from->avail -nr,\r\nsizeof(void *) *nr);\r\nfrom->avail -= nr;\r\nto->avail += nr;\r\nreturn nr;\r\n}\r\nstatic inline struct array_cache **alloc_alien_cache(int node, int limit, gfp_t gfp)\r\n{\r\nreturn (struct array_cache **)BAD_ALIEN_MAGIC;\r\n}\r\nstatic inline void free_alien_cache(struct array_cache **ac_ptr)\r\n{\r\n}\r\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void *alternate_node_alloc(struct kmem_cache *cachep,\r\ngfp_t flags)\r\n{\r\nreturn NULL;\r\n}\r\nstatic inline void *____cache_alloc_node(struct kmem_cache *cachep,\r\ngfp_t flags, int nodeid)\r\n{\r\nreturn NULL;\r\n}\r\nstatic struct array_cache **alloc_alien_cache(int node, int limit, gfp_t gfp)\r\n{\r\nstruct array_cache **ac_ptr;\r\nint memsize = sizeof(void *) * nr_node_ids;\r\nint i;\r\nif (limit > 1)\r\nlimit = 12;\r\nac_ptr = kzalloc_node(memsize, gfp, node);\r\nif (ac_ptr) {\r\nfor_each_node(i) {\r\nif (i == node || !node_online(i))\r\ncontinue;\r\nac_ptr[i] = alloc_arraycache(node, limit, 0xbaadf00d, gfp);\r\nif (!ac_ptr[i]) {\r\nfor (i--; i >= 0; i--)\r\nkfree(ac_ptr[i]);\r\nkfree(ac_ptr);\r\nreturn NULL;\r\n}\r\n}\r\n}\r\nreturn ac_ptr;\r\n}\r\nstatic void free_alien_cache(struct array_cache **ac_ptr)\r\n{\r\nint i;\r\nif (!ac_ptr)\r\nreturn;\r\nfor_each_node(i)\r\nkfree(ac_ptr[i]);\r\nkfree(ac_ptr);\r\n}\r\nstatic void __drain_alien_cache(struct kmem_cache *cachep,\r\nstruct array_cache *ac, int node)\r\n{\r\nstruct kmem_cache_node *n = cachep->node[node];\r\nif (ac->avail) {\r\nspin_lock(&n->list_lock);\r\nif (n->shared)\r\ntransfer_objects(n->shared, ac, ac->limit);\r\nfree_block(cachep, ac->entry, ac->avail, node);\r\nac->avail = 0;\r\nspin_unlock(&n->list_lock);\r\n}\r\n}\r\nstatic void reap_alien(struct kmem_cache *cachep, struct kmem_cache_node *n)\r\n{\r\nint node = __this_cpu_read(slab_reap_node);\r\nif (n->alien) {\r\nstruct array_cache *ac = n->alien[node];\r\nif (ac && ac->avail && spin_trylock_irq(&ac->lock)) {\r\n__drain_alien_cache(cachep, ac, node);\r\nspin_unlock_irq(&ac->lock);\r\n}\r\n}\r\n}\r\nstatic void drain_alien_cache(struct kmem_cache *cachep,\r\nstruct array_cache **alien)\r\n{\r\nint i = 0;\r\nstruct array_cache *ac;\r\nunsigned long flags;\r\nfor_each_online_node(i) {\r\nac = alien[i];\r\nif (ac) {\r\nspin_lock_irqsave(&ac->lock, flags);\r\n__drain_alien_cache(cachep, ac, i);\r\nspin_unlock_irqrestore(&ac->lock, flags);\r\n}\r\n}\r\n}\r\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\r\n{\r\nstruct slab *slabp = virt_to_slab(objp);\r\nint nodeid = slabp->nodeid;\r\nstruct kmem_cache_node *n;\r\nstruct array_cache *alien = NULL;\r\nint node;\r\nnode = numa_mem_id();\r\nif (likely(slabp->nodeid == node))\r\nreturn 0;\r\nn = cachep->node[node];\r\nSTATS_INC_NODEFREES(cachep);\r\nif (n->alien && n->alien[nodeid]) {\r\nalien = n->alien[nodeid];\r\nspin_lock(&alien->lock);\r\nif (unlikely(alien->avail == alien->limit)) {\r\nSTATS_INC_ACOVERFLOW(cachep);\r\n__drain_alien_cache(cachep, alien, nodeid);\r\n}\r\nac_put_obj(cachep, alien, objp);\r\nspin_unlock(&alien->lock);\r\n} else {\r\nspin_lock(&(cachep->node[nodeid])->list_lock);\r\nfree_block(cachep, &objp, 1, nodeid);\r\nspin_unlock(&(cachep->node[nodeid])->list_lock);\r\n}\r\nreturn 1;\r\n}\r\nstatic int init_cache_node_node(int node)\r\n{\r\nstruct kmem_cache *cachep;\r\nstruct kmem_cache_node *n;\r\nconst int memsize = sizeof(struct kmem_cache_node);\r\nlist_for_each_entry(cachep, &slab_caches, list) {\r\nif (!cachep->node[node]) {\r\nn = kmalloc_node(memsize, GFP_KERNEL, node);\r\nif (!n)\r\nreturn -ENOMEM;\r\nkmem_cache_node_init(n);\r\nn->next_reap = jiffies + REAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\ncachep->node[node] = n;\r\n}\r\nspin_lock_irq(&cachep->node[node]->list_lock);\r\ncachep->node[node]->free_limit =\r\n(1 + nr_cpus_node(node)) *\r\ncachep->batchcount + cachep->num;\r\nspin_unlock_irq(&cachep->node[node]->list_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int slabs_tofree(struct kmem_cache *cachep,\r\nstruct kmem_cache_node *n)\r\n{\r\nreturn (n->free_objects + cachep->num - 1) / cachep->num;\r\n}\r\nstatic void cpuup_canceled(long cpu)\r\n{\r\nstruct kmem_cache *cachep;\r\nstruct kmem_cache_node *n = NULL;\r\nint node = cpu_to_mem(cpu);\r\nconst struct cpumask *mask = cpumask_of_node(node);\r\nlist_for_each_entry(cachep, &slab_caches, list) {\r\nstruct array_cache *nc;\r\nstruct array_cache *shared;\r\nstruct array_cache **alien;\r\nnc = cachep->array[cpu];\r\ncachep->array[cpu] = NULL;\r\nn = cachep->node[node];\r\nif (!n)\r\ngoto free_array_cache;\r\nspin_lock_irq(&n->list_lock);\r\nn->free_limit -= cachep->batchcount;\r\nif (nc)\r\nfree_block(cachep, nc->entry, nc->avail, node);\r\nif (!cpumask_empty(mask)) {\r\nspin_unlock_irq(&n->list_lock);\r\ngoto free_array_cache;\r\n}\r\nshared = n->shared;\r\nif (shared) {\r\nfree_block(cachep, shared->entry,\r\nshared->avail, node);\r\nn->shared = NULL;\r\n}\r\nalien = n->alien;\r\nn->alien = NULL;\r\nspin_unlock_irq(&n->list_lock);\r\nkfree(shared);\r\nif (alien) {\r\ndrain_alien_cache(cachep, alien);\r\nfree_alien_cache(alien);\r\n}\r\nfree_array_cache:\r\nkfree(nc);\r\n}\r\nlist_for_each_entry(cachep, &slab_caches, list) {\r\nn = cachep->node[node];\r\nif (!n)\r\ncontinue;\r\ndrain_freelist(cachep, n, slabs_tofree(cachep, n));\r\n}\r\n}\r\nstatic int cpuup_prepare(long cpu)\r\n{\r\nstruct kmem_cache *cachep;\r\nstruct kmem_cache_node *n = NULL;\r\nint node = cpu_to_mem(cpu);\r\nint err;\r\nerr = init_cache_node_node(node);\r\nif (err < 0)\r\ngoto bad;\r\nlist_for_each_entry(cachep, &slab_caches, list) {\r\nstruct array_cache *nc;\r\nstruct array_cache *shared = NULL;\r\nstruct array_cache **alien = NULL;\r\nnc = alloc_arraycache(node, cachep->limit,\r\ncachep->batchcount, GFP_KERNEL);\r\nif (!nc)\r\ngoto bad;\r\nif (cachep->shared) {\r\nshared = alloc_arraycache(node,\r\ncachep->shared * cachep->batchcount,\r\n0xbaadf00d, GFP_KERNEL);\r\nif (!shared) {\r\nkfree(nc);\r\ngoto bad;\r\n}\r\n}\r\nif (use_alien_caches) {\r\nalien = alloc_alien_cache(node, cachep->limit, GFP_KERNEL);\r\nif (!alien) {\r\nkfree(shared);\r\nkfree(nc);\r\ngoto bad;\r\n}\r\n}\r\ncachep->array[cpu] = nc;\r\nn = cachep->node[node];\r\nBUG_ON(!n);\r\nspin_lock_irq(&n->list_lock);\r\nif (!n->shared) {\r\nn->shared = shared;\r\nshared = NULL;\r\n}\r\n#ifdef CONFIG_NUMA\r\nif (!n->alien) {\r\nn->alien = alien;\r\nalien = NULL;\r\n}\r\n#endif\r\nspin_unlock_irq(&n->list_lock);\r\nkfree(shared);\r\nfree_alien_cache(alien);\r\nif (cachep->flags & SLAB_DEBUG_OBJECTS)\r\nslab_set_debugobj_lock_classes_node(cachep, node);\r\nelse if (!OFF_SLAB(cachep) &&\r\n!(cachep->flags & SLAB_DESTROY_BY_RCU))\r\non_slab_lock_classes_node(cachep, node);\r\n}\r\ninit_node_lock_keys(node);\r\nreturn 0;\r\nbad:\r\ncpuup_canceled(cpu);\r\nreturn -ENOMEM;\r\n}\r\nstatic int cpuup_callback(struct notifier_block *nfb,\r\nunsigned long action, void *hcpu)\r\n{\r\nlong cpu = (long)hcpu;\r\nint err = 0;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\nmutex_lock(&slab_mutex);\r\nerr = cpuup_prepare(cpu);\r\nmutex_unlock(&slab_mutex);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_ONLINE_FROZEN:\r\nstart_cpu_timer(cpu);\r\nbreak;\r\n#ifdef CONFIG_HOTPLUG_CPU\r\ncase CPU_DOWN_PREPARE:\r\ncase CPU_DOWN_PREPARE_FROZEN:\r\ncancel_delayed_work_sync(&per_cpu(slab_reap_work, cpu));\r\nper_cpu(slab_reap_work, cpu).work.func = NULL;\r\nbreak;\r\ncase CPU_DOWN_FAILED:\r\ncase CPU_DOWN_FAILED_FROZEN:\r\nstart_cpu_timer(cpu);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\n#endif\r\ncase CPU_UP_CANCELED:\r\ncase CPU_UP_CANCELED_FROZEN:\r\nmutex_lock(&slab_mutex);\r\ncpuup_canceled(cpu);\r\nmutex_unlock(&slab_mutex);\r\nbreak;\r\n}\r\nreturn notifier_from_errno(err);\r\n}\r\nstatic int __meminit drain_cache_node_node(int node)\r\n{\r\nstruct kmem_cache *cachep;\r\nint ret = 0;\r\nlist_for_each_entry(cachep, &slab_caches, list) {\r\nstruct kmem_cache_node *n;\r\nn = cachep->node[node];\r\nif (!n)\r\ncontinue;\r\ndrain_freelist(cachep, n, slabs_tofree(cachep, n));\r\nif (!list_empty(&n->slabs_full) ||\r\n!list_empty(&n->slabs_partial)) {\r\nret = -EBUSY;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int __meminit slab_memory_callback(struct notifier_block *self,\r\nunsigned long action, void *arg)\r\n{\r\nstruct memory_notify *mnb = arg;\r\nint ret = 0;\r\nint nid;\r\nnid = mnb->status_change_nid;\r\nif (nid < 0)\r\ngoto out;\r\nswitch (action) {\r\ncase MEM_GOING_ONLINE:\r\nmutex_lock(&slab_mutex);\r\nret = init_cache_node_node(nid);\r\nmutex_unlock(&slab_mutex);\r\nbreak;\r\ncase MEM_GOING_OFFLINE:\r\nmutex_lock(&slab_mutex);\r\nret = drain_cache_node_node(nid);\r\nmutex_unlock(&slab_mutex);\r\nbreak;\r\ncase MEM_ONLINE:\r\ncase MEM_OFFLINE:\r\ncase MEM_CANCEL_ONLINE:\r\ncase MEM_CANCEL_OFFLINE:\r\nbreak;\r\n}\r\nout:\r\nreturn notifier_from_errno(ret);\r\n}\r\nstatic void __init init_list(struct kmem_cache *cachep, struct kmem_cache_node *list,\r\nint nodeid)\r\n{\r\nstruct kmem_cache_node *ptr;\r\nptr = kmalloc_node(sizeof(struct kmem_cache_node), GFP_NOWAIT, nodeid);\r\nBUG_ON(!ptr);\r\nmemcpy(ptr, list, sizeof(struct kmem_cache_node));\r\nspin_lock_init(&ptr->list_lock);\r\nMAKE_ALL_LISTS(cachep, ptr, nodeid);\r\ncachep->node[nodeid] = ptr;\r\n}\r\nstatic void __init set_up_node(struct kmem_cache *cachep, int index)\r\n{\r\nint node;\r\nfor_each_online_node(node) {\r\ncachep->node[node] = &init_kmem_cache_node[index + node];\r\ncachep->node[node]->next_reap = jiffies +\r\nREAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\n}\r\n}\r\nstatic void setup_node_pointer(struct kmem_cache *cachep)\r\n{\r\ncachep->node = (struct kmem_cache_node **)&cachep->array[nr_cpu_ids];\r\n}\r\nvoid __init kmem_cache_init(void)\r\n{\r\nint i;\r\nkmem_cache = &kmem_cache_boot;\r\nsetup_node_pointer(kmem_cache);\r\nif (num_possible_nodes() == 1)\r\nuse_alien_caches = 0;\r\nfor (i = 0; i < NUM_INIT_LISTS; i++)\r\nkmem_cache_node_init(&init_kmem_cache_node[i]);\r\nset_up_node(kmem_cache, CACHE_CACHE);\r\nif (!slab_max_order_set && totalram_pages > (32 << 20) >> PAGE_SHIFT)\r\nslab_max_order = SLAB_MAX_ORDER_HI;\r\ncreate_boot_cache(kmem_cache, "kmem_cache",\r\noffsetof(struct kmem_cache, array[nr_cpu_ids]) +\r\nnr_node_ids * sizeof(struct kmem_cache_node *),\r\nSLAB_HWCACHE_ALIGN);\r\nlist_add(&kmem_cache->list, &slab_caches);\r\nkmalloc_caches[INDEX_AC] = create_kmalloc_cache("kmalloc-ac",\r\nkmalloc_size(INDEX_AC), ARCH_KMALLOC_FLAGS);\r\nif (INDEX_AC != INDEX_NODE)\r\nkmalloc_caches[INDEX_NODE] =\r\ncreate_kmalloc_cache("kmalloc-node",\r\nkmalloc_size(INDEX_NODE), ARCH_KMALLOC_FLAGS);\r\nslab_early_init = 0;\r\n{\r\nstruct array_cache *ptr;\r\nptr = kmalloc(sizeof(struct arraycache_init), GFP_NOWAIT);\r\nmemcpy(ptr, cpu_cache_get(kmem_cache),\r\nsizeof(struct arraycache_init));\r\nspin_lock_init(&ptr->lock);\r\nkmem_cache->array[smp_processor_id()] = ptr;\r\nptr = kmalloc(sizeof(struct arraycache_init), GFP_NOWAIT);\r\nBUG_ON(cpu_cache_get(kmalloc_caches[INDEX_AC])\r\n!= &initarray_generic.cache);\r\nmemcpy(ptr, cpu_cache_get(kmalloc_caches[INDEX_AC]),\r\nsizeof(struct arraycache_init));\r\nspin_lock_init(&ptr->lock);\r\nkmalloc_caches[INDEX_AC]->array[smp_processor_id()] = ptr;\r\n}\r\n{\r\nint nid;\r\nfor_each_online_node(nid) {\r\ninit_list(kmem_cache, &init_kmem_cache_node[CACHE_CACHE + nid], nid);\r\ninit_list(kmalloc_caches[INDEX_AC],\r\n&init_kmem_cache_node[SIZE_AC + nid], nid);\r\nif (INDEX_AC != INDEX_NODE) {\r\ninit_list(kmalloc_caches[INDEX_NODE],\r\n&init_kmem_cache_node[SIZE_NODE + nid], nid);\r\n}\r\n}\r\n}\r\ncreate_kmalloc_caches(ARCH_KMALLOC_FLAGS);\r\n}\r\nvoid __init kmem_cache_init_late(void)\r\n{\r\nstruct kmem_cache *cachep;\r\nslab_state = UP;\r\nmutex_lock(&slab_mutex);\r\nlist_for_each_entry(cachep, &slab_caches, list)\r\nif (enable_cpucache(cachep, GFP_NOWAIT))\r\nBUG();\r\nmutex_unlock(&slab_mutex);\r\ninit_lock_keys();\r\nslab_state = FULL;\r\nregister_cpu_notifier(&cpucache_notifier);\r\n#ifdef CONFIG_NUMA\r\nhotplug_memory_notifier(slab_memory_callback, SLAB_CALLBACK_PRI);\r\n#endif\r\n}\r\nstatic int __init cpucache_init(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nstart_cpu_timer(cpu);\r\nslab_state = FULL;\r\nreturn 0;\r\n}\r\nstatic noinline void\r\nslab_out_of_memory(struct kmem_cache *cachep, gfp_t gfpflags, int nodeid)\r\n{\r\nstruct kmem_cache_node *n;\r\nstruct slab *slabp;\r\nunsigned long flags;\r\nint node;\r\nprintk(KERN_WARNING\r\n"SLAB: Unable to allocate memory on node %d (gfp=0x%x)\n",\r\nnodeid, gfpflags);\r\nprintk(KERN_WARNING " cache: %s, object size: %d, order: %d\n",\r\ncachep->name, cachep->size, cachep->gfporder);\r\nfor_each_online_node(node) {\r\nunsigned long active_objs = 0, num_objs = 0, free_objects = 0;\r\nunsigned long active_slabs = 0, num_slabs = 0;\r\nn = cachep->node[node];\r\nif (!n)\r\ncontinue;\r\nspin_lock_irqsave(&n->list_lock, flags);\r\nlist_for_each_entry(slabp, &n->slabs_full, list) {\r\nactive_objs += cachep->num;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &n->slabs_partial, list) {\r\nactive_objs += slabp->inuse;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &n->slabs_free, list)\r\nnum_slabs++;\r\nfree_objects += n->free_objects;\r\nspin_unlock_irqrestore(&n->list_lock, flags);\r\nnum_slabs += active_slabs;\r\nnum_objs = num_slabs * cachep->num;\r\nprintk(KERN_WARNING\r\n" node %d: slabs: %ld/%ld, objs: %ld/%ld, free: %ld\n",\r\nnode, active_slabs, num_slabs, active_objs, num_objs,\r\nfree_objects);\r\n}\r\n}\r\nstatic void *kmem_getpages(struct kmem_cache *cachep, gfp_t flags, int nodeid)\r\n{\r\nstruct page *page;\r\nint nr_pages;\r\nint i;\r\n#ifndef CONFIG_MMU\r\nflags |= __GFP_COMP;\r\n#endif\r\nflags |= cachep->allocflags;\r\nif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\r\nflags |= __GFP_RECLAIMABLE;\r\npage = alloc_pages_exact_node(nodeid, flags | __GFP_NOTRACK, cachep->gfporder);\r\nif (!page) {\r\nif (!(flags & __GFP_NOWARN) && printk_ratelimit())\r\nslab_out_of_memory(cachep, flags, nodeid);\r\nreturn NULL;\r\n}\r\nif (unlikely(page->pfmemalloc))\r\npfmemalloc_active = true;\r\nnr_pages = (1 << cachep->gfporder);\r\nif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\r\nadd_zone_page_state(page_zone(page),\r\nNR_SLAB_RECLAIMABLE, nr_pages);\r\nelse\r\nadd_zone_page_state(page_zone(page),\r\nNR_SLAB_UNRECLAIMABLE, nr_pages);\r\nfor (i = 0; i < nr_pages; i++) {\r\n__SetPageSlab(page + i);\r\nif (page->pfmemalloc)\r\nSetPageSlabPfmemalloc(page + i);\r\n}\r\nmemcg_bind_pages(cachep, cachep->gfporder);\r\nif (kmemcheck_enabled && !(cachep->flags & SLAB_NOTRACK)) {\r\nkmemcheck_alloc_shadow(page, cachep->gfporder, flags, nodeid);\r\nif (cachep->ctor)\r\nkmemcheck_mark_uninitialized_pages(page, nr_pages);\r\nelse\r\nkmemcheck_mark_unallocated_pages(page, nr_pages);\r\n}\r\nreturn page_address(page);\r\n}\r\nstatic void kmem_freepages(struct kmem_cache *cachep, void *addr)\r\n{\r\nunsigned long i = (1 << cachep->gfporder);\r\nstruct page *page = virt_to_page(addr);\r\nconst unsigned long nr_freed = i;\r\nkmemcheck_free_shadow(page, cachep->gfporder);\r\nif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\r\nsub_zone_page_state(page_zone(page),\r\nNR_SLAB_RECLAIMABLE, nr_freed);\r\nelse\r\nsub_zone_page_state(page_zone(page),\r\nNR_SLAB_UNRECLAIMABLE, nr_freed);\r\nwhile (i--) {\r\nBUG_ON(!PageSlab(page));\r\n__ClearPageSlabPfmemalloc(page);\r\n__ClearPageSlab(page);\r\npage++;\r\n}\r\nmemcg_release_pages(cachep, cachep->gfporder);\r\nif (current->reclaim_state)\r\ncurrent->reclaim_state->reclaimed_slab += nr_freed;\r\nfree_memcg_kmem_pages((unsigned long)addr, cachep->gfporder);\r\n}\r\nstatic void kmem_rcu_free(struct rcu_head *head)\r\n{\r\nstruct slab_rcu *slab_rcu = (struct slab_rcu *)head;\r\nstruct kmem_cache *cachep = slab_rcu->cachep;\r\nkmem_freepages(cachep, slab_rcu->addr);\r\nif (OFF_SLAB(cachep))\r\nkmem_cache_free(cachep->slabp_cache, slab_rcu);\r\n}\r\nstatic void store_stackinfo(struct kmem_cache *cachep, unsigned long *addr,\r\nunsigned long caller)\r\n{\r\nint size = cachep->object_size;\r\naddr = (unsigned long *)&((char *)addr)[obj_offset(cachep)];\r\nif (size < 5 * sizeof(unsigned long))\r\nreturn;\r\n*addr++ = 0x12345678;\r\n*addr++ = caller;\r\n*addr++ = smp_processor_id();\r\nsize -= 3 * sizeof(unsigned long);\r\n{\r\nunsigned long *sptr = &caller;\r\nunsigned long svalue;\r\nwhile (!kstack_end(sptr)) {\r\nsvalue = *sptr++;\r\nif (kernel_text_address(svalue)) {\r\n*addr++ = svalue;\r\nsize -= sizeof(unsigned long);\r\nif (size <= sizeof(unsigned long))\r\nbreak;\r\n}\r\n}\r\n}\r\n*addr++ = 0x87654321;\r\n}\r\nstatic void poison_obj(struct kmem_cache *cachep, void *addr, unsigned char val)\r\n{\r\nint size = cachep->object_size;\r\naddr = &((char *)addr)[obj_offset(cachep)];\r\nmemset(addr, val, size);\r\n*(unsigned char *)(addr + size - 1) = POISON_END;\r\n}\r\nstatic void dump_line(char *data, int offset, int limit)\r\n{\r\nint i;\r\nunsigned char error = 0;\r\nint bad_count = 0;\r\nprintk(KERN_ERR "%03x: ", offset);\r\nfor (i = 0; i < limit; i++) {\r\nif (data[offset + i] != POISON_FREE) {\r\nerror = data[offset + i];\r\nbad_count++;\r\n}\r\n}\r\nprint_hex_dump(KERN_CONT, "", 0, 16, 1,\r\n&data[offset], limit, 1);\r\nif (bad_count == 1) {\r\nerror ^= POISON_FREE;\r\nif (!(error & (error - 1))) {\r\nprintk(KERN_ERR "Single bit error detected. Probably "\r\n"bad RAM.\n");\r\n#ifdef CONFIG_X86\r\nprintk(KERN_ERR "Run memtest86+ or a similar memory "\r\n"test tool.\n");\r\n#else\r\nprintk(KERN_ERR "Run a memory test tool.\n");\r\n#endif\r\n}\r\n}\r\n}\r\nstatic void print_objinfo(struct kmem_cache *cachep, void *objp, int lines)\r\n{\r\nint i, size;\r\nchar *realobj;\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nprintk(KERN_ERR "Redzone: 0x%llx/0x%llx.\n",\r\n*dbg_redzone1(cachep, objp),\r\n*dbg_redzone2(cachep, objp));\r\n}\r\nif (cachep->flags & SLAB_STORE_USER) {\r\nprintk(KERN_ERR "Last user: [<%p>](%pSR)\n",\r\n*dbg_userword(cachep, objp),\r\n*dbg_userword(cachep, objp));\r\n}\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nsize = cachep->object_size;\r\nfor (i = 0; i < size && lines; i += 16, lines--) {\r\nint limit;\r\nlimit = 16;\r\nif (i + limit > size)\r\nlimit = size - i;\r\ndump_line(realobj, i, limit);\r\n}\r\n}\r\nstatic void check_poison_obj(struct kmem_cache *cachep, void *objp)\r\n{\r\nchar *realobj;\r\nint size, i;\r\nint lines = 0;\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nsize = cachep->object_size;\r\nfor (i = 0; i < size; i++) {\r\nchar exp = POISON_FREE;\r\nif (i == size - 1)\r\nexp = POISON_END;\r\nif (realobj[i] != exp) {\r\nint limit;\r\nif (lines == 0) {\r\nprintk(KERN_ERR\r\n"Slab corruption (%s): %s start=%p, len=%d\n",\r\nprint_tainted(), cachep->name, realobj, size);\r\nprint_objinfo(cachep, objp, 0);\r\n}\r\ni = (i / 16) * 16;\r\nlimit = 16;\r\nif (i + limit > size)\r\nlimit = size - i;\r\ndump_line(realobj, i, limit);\r\ni += 16;\r\nlines++;\r\nif (lines > 5)\r\nbreak;\r\n}\r\n}\r\nif (lines != 0) {\r\nstruct slab *slabp = virt_to_slab(objp);\r\nunsigned int objnr;\r\nobjnr = obj_to_index(cachep, slabp, objp);\r\nif (objnr) {\r\nobjp = index_to_obj(cachep, slabp, objnr - 1);\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nprintk(KERN_ERR "Prev obj: start=%p, len=%d\n",\r\nrealobj, size);\r\nprint_objinfo(cachep, objp, 2);\r\n}\r\nif (objnr + 1 < cachep->num) {\r\nobjp = index_to_obj(cachep, slabp, objnr + 1);\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nprintk(KERN_ERR "Next obj: start=%p, len=%d\n",\r\nrealobj, size);\r\nprint_objinfo(cachep, objp, 2);\r\n}\r\n}\r\n}\r\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\nint i;\r\nfor (i = 0; i < cachep->num; i++) {\r\nvoid *objp = index_to_obj(cachep, slabp, i);\r\nif (cachep->flags & SLAB_POISON) {\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif (cachep->size % PAGE_SIZE == 0 &&\r\nOFF_SLAB(cachep))\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->size / PAGE_SIZE, 1);\r\nelse\r\ncheck_poison_obj(cachep, objp);\r\n#else\r\ncheck_poison_obj(cachep, objp);\r\n#endif\r\n}\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "start of a freed object "\r\n"was overwritten");\r\nif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "end of a freed object "\r\n"was overwritten");\r\n}\r\n}\r\n}\r\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\n}\r\nstatic void slab_destroy(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\nvoid *addr = slabp->s_mem - slabp->colouroff;\r\nslab_destroy_debugcheck(cachep, slabp);\r\nif (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU)) {\r\nstruct slab_rcu *slab_rcu;\r\nslab_rcu = (struct slab_rcu *)slabp;\r\nslab_rcu->cachep = cachep;\r\nslab_rcu->addr = addr;\r\ncall_rcu(&slab_rcu->head, kmem_rcu_free);\r\n} else {\r\nkmem_freepages(cachep, addr);\r\nif (OFF_SLAB(cachep))\r\nkmem_cache_free(cachep->slabp_cache, slabp);\r\n}\r\n}\r\nstatic size_t calculate_slab_order(struct kmem_cache *cachep,\r\nsize_t size, size_t align, unsigned long flags)\r\n{\r\nunsigned long offslab_limit;\r\nsize_t left_over = 0;\r\nint gfporder;\r\nfor (gfporder = 0; gfporder <= KMALLOC_MAX_ORDER; gfporder++) {\r\nunsigned int num;\r\nsize_t remainder;\r\ncache_estimate(gfporder, size, align, flags, &remainder, &num);\r\nif (!num)\r\ncontinue;\r\nif (flags & CFLGS_OFF_SLAB) {\r\noffslab_limit = size - sizeof(struct slab);\r\noffslab_limit /= sizeof(kmem_bufctl_t);\r\nif (num > offslab_limit)\r\nbreak;\r\n}\r\ncachep->num = num;\r\ncachep->gfporder = gfporder;\r\nleft_over = remainder;\r\nif (flags & SLAB_RECLAIM_ACCOUNT)\r\nbreak;\r\nif (gfporder >= slab_max_order)\r\nbreak;\r\nif (left_over * 8 <= (PAGE_SIZE << gfporder))\r\nbreak;\r\n}\r\nreturn left_over;\r\n}\r\nstatic int __init_refok setup_cpu_cache(struct kmem_cache *cachep, gfp_t gfp)\r\n{\r\nif (slab_state >= FULL)\r\nreturn enable_cpucache(cachep, gfp);\r\nif (slab_state == DOWN) {\r\ncachep->array[smp_processor_id()] = &initarray_generic.cache;\r\nslab_state = PARTIAL;\r\n} else if (slab_state == PARTIAL) {\r\ncachep->array[smp_processor_id()] = &initarray_generic.cache;\r\nset_up_node(cachep, SIZE_AC);\r\nif (INDEX_AC == INDEX_NODE)\r\nslab_state = PARTIAL_NODE;\r\nelse\r\nslab_state = PARTIAL_ARRAYCACHE;\r\n} else {\r\ncachep->array[smp_processor_id()] =\r\nkmalloc(sizeof(struct arraycache_init), gfp);\r\nif (slab_state == PARTIAL_ARRAYCACHE) {\r\nset_up_node(cachep, SIZE_NODE);\r\nslab_state = PARTIAL_NODE;\r\n} else {\r\nint node;\r\nfor_each_online_node(node) {\r\ncachep->node[node] =\r\nkmalloc_node(sizeof(struct kmem_cache_node),\r\ngfp, node);\r\nBUG_ON(!cachep->node[node]);\r\nkmem_cache_node_init(cachep->node[node]);\r\n}\r\n}\r\n}\r\ncachep->node[numa_mem_id()]->next_reap =\r\njiffies + REAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\ncpu_cache_get(cachep)->avail = 0;\r\ncpu_cache_get(cachep)->limit = BOOT_CPUCACHE_ENTRIES;\r\ncpu_cache_get(cachep)->batchcount = 1;\r\ncpu_cache_get(cachep)->touched = 0;\r\ncachep->batchcount = 1;\r\ncachep->limit = BOOT_CPUCACHE_ENTRIES;\r\nreturn 0;\r\n}\r\nint\r\n__kmem_cache_create (struct kmem_cache *cachep, unsigned long flags)\r\n{\r\nsize_t left_over, slab_size, ralign;\r\ngfp_t gfp;\r\nint err;\r\nsize_t size = cachep->size;\r\n#if DEBUG\r\n#if FORCED_DEBUG\r\nif (size < 4096 || fls(size - 1) == fls(size-1 + REDZONE_ALIGN +\r\n2 * sizeof(unsigned long long)))\r\nflags |= SLAB_RED_ZONE | SLAB_STORE_USER;\r\nif (!(flags & SLAB_DESTROY_BY_RCU))\r\nflags |= SLAB_POISON;\r\n#endif\r\nif (flags & SLAB_DESTROY_BY_RCU)\r\nBUG_ON(flags & SLAB_POISON);\r\n#endif\r\nif (size & (BYTES_PER_WORD - 1)) {\r\nsize += (BYTES_PER_WORD - 1);\r\nsize &= ~(BYTES_PER_WORD - 1);\r\n}\r\nif (flags & SLAB_STORE_USER)\r\nralign = BYTES_PER_WORD;\r\nif (flags & SLAB_RED_ZONE) {\r\nralign = REDZONE_ALIGN;\r\nsize += REDZONE_ALIGN - 1;\r\nsize &= ~(REDZONE_ALIGN - 1);\r\n}\r\nif (ralign < cachep->align) {\r\nralign = cachep->align;\r\n}\r\nif (ralign > __alignof__(unsigned long long))\r\nflags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\r\ncachep->align = ralign;\r\nif (slab_is_available())\r\ngfp = GFP_KERNEL;\r\nelse\r\ngfp = GFP_NOWAIT;\r\nsetup_node_pointer(cachep);\r\n#if DEBUG\r\nif (flags & SLAB_RED_ZONE) {\r\ncachep->obj_offset += sizeof(unsigned long long);\r\nsize += 2 * sizeof(unsigned long long);\r\n}\r\nif (flags & SLAB_STORE_USER) {\r\nif (flags & SLAB_RED_ZONE)\r\nsize += REDZONE_ALIGN;\r\nelse\r\nsize += BYTES_PER_WORD;\r\n}\r\n#if FORCED_DEBUG && defined(CONFIG_DEBUG_PAGEALLOC)\r\nif (size >= kmalloc_size(INDEX_NODE + 1)\r\n&& cachep->object_size > cache_line_size()\r\n&& ALIGN(size, cachep->align) < PAGE_SIZE) {\r\ncachep->obj_offset += PAGE_SIZE - ALIGN(size, cachep->align);\r\nsize = PAGE_SIZE;\r\n}\r\n#endif\r\n#endif\r\nif ((size >= (PAGE_SIZE >> 3)) && !slab_early_init &&\r\n!(flags & SLAB_NOLEAKTRACE))\r\nflags |= CFLGS_OFF_SLAB;\r\nsize = ALIGN(size, cachep->align);\r\nleft_over = calculate_slab_order(cachep, size, cachep->align, flags);\r\nif (!cachep->num)\r\nreturn -E2BIG;\r\nslab_size = ALIGN(cachep->num * sizeof(kmem_bufctl_t)\r\n+ sizeof(struct slab), cachep->align);\r\nif (flags & CFLGS_OFF_SLAB && left_over >= slab_size) {\r\nflags &= ~CFLGS_OFF_SLAB;\r\nleft_over -= slab_size;\r\n}\r\nif (flags & CFLGS_OFF_SLAB) {\r\nslab_size =\r\ncachep->num * sizeof(kmem_bufctl_t) + sizeof(struct slab);\r\n#ifdef CONFIG_PAGE_POISONING\r\nif (size % PAGE_SIZE == 0 && flags & SLAB_POISON)\r\nflags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\r\n#endif\r\n}\r\ncachep->colour_off = cache_line_size();\r\nif (cachep->colour_off < cachep->align)\r\ncachep->colour_off = cachep->align;\r\ncachep->colour = left_over / cachep->colour_off;\r\ncachep->slab_size = slab_size;\r\ncachep->flags = flags;\r\ncachep->allocflags = 0;\r\nif (CONFIG_ZONE_DMA_FLAG && (flags & SLAB_CACHE_DMA))\r\ncachep->allocflags |= GFP_DMA;\r\ncachep->size = size;\r\ncachep->reciprocal_buffer_size = reciprocal_value(size);\r\nif (flags & CFLGS_OFF_SLAB) {\r\ncachep->slabp_cache = kmalloc_slab(slab_size, 0u);\r\nBUG_ON(ZERO_OR_NULL_PTR(cachep->slabp_cache));\r\n}\r\nerr = setup_cpu_cache(cachep, gfp);\r\nif (err) {\r\n__kmem_cache_shutdown(cachep);\r\nreturn err;\r\n}\r\nif (flags & SLAB_DEBUG_OBJECTS) {\r\nWARN_ON_ONCE(flags & SLAB_DESTROY_BY_RCU);\r\nslab_set_debugobj_lock_classes(cachep);\r\n} else if (!OFF_SLAB(cachep) && !(flags & SLAB_DESTROY_BY_RCU))\r\non_slab_lock_classes(cachep);\r\nreturn 0;\r\n}\r\nstatic void check_irq_off(void)\r\n{\r\nBUG_ON(!irqs_disabled());\r\n}\r\nstatic void check_irq_on(void)\r\n{\r\nBUG_ON(irqs_disabled());\r\n}\r\nstatic void check_spinlock_acquired(struct kmem_cache *cachep)\r\n{\r\n#ifdef CONFIG_SMP\r\ncheck_irq_off();\r\nassert_spin_locked(&cachep->node[numa_mem_id()]->list_lock);\r\n#endif\r\n}\r\nstatic void check_spinlock_acquired_node(struct kmem_cache *cachep, int node)\r\n{\r\n#ifdef CONFIG_SMP\r\ncheck_irq_off();\r\nassert_spin_locked(&cachep->node[node]->list_lock);\r\n#endif\r\n}\r\nstatic void do_drain(void *arg)\r\n{\r\nstruct kmem_cache *cachep = arg;\r\nstruct array_cache *ac;\r\nint node = numa_mem_id();\r\ncheck_irq_off();\r\nac = cpu_cache_get(cachep);\r\nspin_lock(&cachep->node[node]->list_lock);\r\nfree_block(cachep, ac->entry, ac->avail, node);\r\nspin_unlock(&cachep->node[node]->list_lock);\r\nac->avail = 0;\r\n}\r\nstatic void drain_cpu_caches(struct kmem_cache *cachep)\r\n{\r\nstruct kmem_cache_node *n;\r\nint node;\r\non_each_cpu(do_drain, cachep, 1);\r\ncheck_irq_on();\r\nfor_each_online_node(node) {\r\nn = cachep->node[node];\r\nif (n && n->alien)\r\ndrain_alien_cache(cachep, n->alien);\r\n}\r\nfor_each_online_node(node) {\r\nn = cachep->node[node];\r\nif (n)\r\ndrain_array(cachep, n, n->shared, 1, node);\r\n}\r\n}\r\nstatic int drain_freelist(struct kmem_cache *cache,\r\nstruct kmem_cache_node *n, int tofree)\r\n{\r\nstruct list_head *p;\r\nint nr_freed;\r\nstruct slab *slabp;\r\nnr_freed = 0;\r\nwhile (nr_freed < tofree && !list_empty(&n->slabs_free)) {\r\nspin_lock_irq(&n->list_lock);\r\np = n->slabs_free.prev;\r\nif (p == &n->slabs_free) {\r\nspin_unlock_irq(&n->list_lock);\r\ngoto out;\r\n}\r\nslabp = list_entry(p, struct slab, list);\r\n#if DEBUG\r\nBUG_ON(slabp->inuse);\r\n#endif\r\nlist_del(&slabp->list);\r\nn->free_objects -= cache->num;\r\nspin_unlock_irq(&n->list_lock);\r\nslab_destroy(cache, slabp);\r\nnr_freed++;\r\n}\r\nout:\r\nreturn nr_freed;\r\n}\r\nstatic int __cache_shrink(struct kmem_cache *cachep)\r\n{\r\nint ret = 0, i = 0;\r\nstruct kmem_cache_node *n;\r\ndrain_cpu_caches(cachep);\r\ncheck_irq_on();\r\nfor_each_online_node(i) {\r\nn = cachep->node[i];\r\nif (!n)\r\ncontinue;\r\ndrain_freelist(cachep, n, slabs_tofree(cachep, n));\r\nret += !list_empty(&n->slabs_full) ||\r\n!list_empty(&n->slabs_partial);\r\n}\r\nreturn (ret ? 1 : 0);\r\n}\r\nint kmem_cache_shrink(struct kmem_cache *cachep)\r\n{\r\nint ret;\r\nBUG_ON(!cachep || in_interrupt());\r\nget_online_cpus();\r\nmutex_lock(&slab_mutex);\r\nret = __cache_shrink(cachep);\r\nmutex_unlock(&slab_mutex);\r\nput_online_cpus();\r\nreturn ret;\r\n}\r\nint __kmem_cache_shutdown(struct kmem_cache *cachep)\r\n{\r\nint i;\r\nstruct kmem_cache_node *n;\r\nint rc = __cache_shrink(cachep);\r\nif (rc)\r\nreturn rc;\r\nfor_each_online_cpu(i)\r\nkfree(cachep->array[i]);\r\nfor_each_online_node(i) {\r\nn = cachep->node[i];\r\nif (n) {\r\nkfree(n->shared);\r\nfree_alien_cache(n->alien);\r\nkfree(n);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic struct slab *alloc_slabmgmt(struct kmem_cache *cachep, void *objp,\r\nint colour_off, gfp_t local_flags,\r\nint nodeid)\r\n{\r\nstruct slab *slabp;\r\nif (OFF_SLAB(cachep)) {\r\nslabp = kmem_cache_alloc_node(cachep->slabp_cache,\r\nlocal_flags, nodeid);\r\nkmemleak_scan_area(&slabp->list, sizeof(struct list_head),\r\nlocal_flags);\r\nif (!slabp)\r\nreturn NULL;\r\n} else {\r\nslabp = objp + colour_off;\r\ncolour_off += cachep->slab_size;\r\n}\r\nslabp->inuse = 0;\r\nslabp->colouroff = colour_off;\r\nslabp->s_mem = objp + colour_off;\r\nslabp->nodeid = nodeid;\r\nslabp->free = 0;\r\nreturn slabp;\r\n}\r\nstatic inline kmem_bufctl_t *slab_bufctl(struct slab *slabp)\r\n{\r\nreturn (kmem_bufctl_t *) (slabp + 1);\r\n}\r\nstatic void cache_init_objs(struct kmem_cache *cachep,\r\nstruct slab *slabp)\r\n{\r\nint i;\r\nfor (i = 0; i < cachep->num; i++) {\r\nvoid *objp = index_to_obj(cachep, slabp, i);\r\n#if DEBUG\r\nif (cachep->flags & SLAB_POISON)\r\npoison_obj(cachep, objp, POISON_FREE);\r\nif (cachep->flags & SLAB_STORE_USER)\r\n*dbg_userword(cachep, objp) = NULL;\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\n*dbg_redzone1(cachep, objp) = RED_INACTIVE;\r\n*dbg_redzone2(cachep, objp) = RED_INACTIVE;\r\n}\r\nif (cachep->ctor && !(cachep->flags & SLAB_POISON))\r\ncachep->ctor(objp + obj_offset(cachep));\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "constructor overwrote the"\r\n" end of an object");\r\nif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "constructor overwrote the"\r\n" start of an object");\r\n}\r\nif ((cachep->size % PAGE_SIZE) == 0 &&\r\nOFF_SLAB(cachep) && cachep->flags & SLAB_POISON)\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->size / PAGE_SIZE, 0);\r\n#else\r\nif (cachep->ctor)\r\ncachep->ctor(objp);\r\n#endif\r\nslab_bufctl(slabp)[i] = i + 1;\r\n}\r\nslab_bufctl(slabp)[i - 1] = BUFCTL_END;\r\n}\r\nstatic void kmem_flagcheck(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nif (CONFIG_ZONE_DMA_FLAG) {\r\nif (flags & GFP_DMA)\r\nBUG_ON(!(cachep->allocflags & GFP_DMA));\r\nelse\r\nBUG_ON(cachep->allocflags & GFP_DMA);\r\n}\r\n}\r\nstatic void *slab_get_obj(struct kmem_cache *cachep, struct slab *slabp,\r\nint nodeid)\r\n{\r\nvoid *objp = index_to_obj(cachep, slabp, slabp->free);\r\nkmem_bufctl_t next;\r\nslabp->inuse++;\r\nnext = slab_bufctl(slabp)[slabp->free];\r\n#if DEBUG\r\nslab_bufctl(slabp)[slabp->free] = BUFCTL_FREE;\r\nWARN_ON(slabp->nodeid != nodeid);\r\n#endif\r\nslabp->free = next;\r\nreturn objp;\r\n}\r\nstatic void slab_put_obj(struct kmem_cache *cachep, struct slab *slabp,\r\nvoid *objp, int nodeid)\r\n{\r\nunsigned int objnr = obj_to_index(cachep, slabp, objp);\r\n#if DEBUG\r\nWARN_ON(slabp->nodeid != nodeid);\r\nif (slab_bufctl(slabp)[objnr] + 1 <= SLAB_LIMIT + 1) {\r\nprintk(KERN_ERR "slab: double free detected in cache "\r\n"'%s', objp %p\n", cachep->name, objp);\r\nBUG();\r\n}\r\n#endif\r\nslab_bufctl(slabp)[objnr] = slabp->free;\r\nslabp->free = objnr;\r\nslabp->inuse--;\r\n}\r\nstatic void slab_map_pages(struct kmem_cache *cache, struct slab *slab,\r\nvoid *addr)\r\n{\r\nint nr_pages;\r\nstruct page *page;\r\npage = virt_to_page(addr);\r\nnr_pages = 1;\r\nif (likely(!PageCompound(page)))\r\nnr_pages <<= cache->gfporder;\r\ndo {\r\npage->slab_cache = cache;\r\npage->slab_page = slab;\r\npage++;\r\n} while (--nr_pages);\r\n}\r\nstatic int cache_grow(struct kmem_cache *cachep,\r\ngfp_t flags, int nodeid, void *objp)\r\n{\r\nstruct slab *slabp;\r\nsize_t offset;\r\ngfp_t local_flags;\r\nstruct kmem_cache_node *n;\r\nBUG_ON(flags & GFP_SLAB_BUG_MASK);\r\nlocal_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);\r\ncheck_irq_off();\r\nn = cachep->node[nodeid];\r\nspin_lock(&n->list_lock);\r\noffset = n->colour_next;\r\nn->colour_next++;\r\nif (n->colour_next >= cachep->colour)\r\nn->colour_next = 0;\r\nspin_unlock(&n->list_lock);\r\noffset *= cachep->colour_off;\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_enable();\r\nkmem_flagcheck(cachep, flags);\r\nif (!objp)\r\nobjp = kmem_getpages(cachep, local_flags, nodeid);\r\nif (!objp)\r\ngoto failed;\r\nslabp = alloc_slabmgmt(cachep, objp, offset,\r\nlocal_flags & ~GFP_CONSTRAINT_MASK, nodeid);\r\nif (!slabp)\r\ngoto opps1;\r\nslab_map_pages(cachep, slabp, objp);\r\ncache_init_objs(cachep, slabp);\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_disable();\r\ncheck_irq_off();\r\nspin_lock(&n->list_lock);\r\nlist_add_tail(&slabp->list, &(n->slabs_free));\r\nSTATS_INC_GROWN(cachep);\r\nn->free_objects += cachep->num;\r\nspin_unlock(&n->list_lock);\r\nreturn 1;\r\nopps1:\r\nkmem_freepages(cachep, objp);\r\nfailed:\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_disable();\r\nreturn 0;\r\n}\r\nstatic void kfree_debugcheck(const void *objp)\r\n{\r\nif (!virt_addr_valid(objp)) {\r\nprintk(KERN_ERR "kfree_debugcheck: out of range ptr %lxh.\n",\r\n(unsigned long)objp);\r\nBUG();\r\n}\r\n}\r\nstatic inline void verify_redzone_free(struct kmem_cache *cache, void *obj)\r\n{\r\nunsigned long long redzone1, redzone2;\r\nredzone1 = *dbg_redzone1(cache, obj);\r\nredzone2 = *dbg_redzone2(cache, obj);\r\nif (redzone1 == RED_ACTIVE && redzone2 == RED_ACTIVE)\r\nreturn;\r\nif (redzone1 == RED_INACTIVE && redzone2 == RED_INACTIVE)\r\nslab_error(cache, "double free detected");\r\nelse\r\nslab_error(cache, "memory outside object was overwritten");\r\nprintk(KERN_ERR "%p: redzone 1:0x%llx, redzone 2:0x%llx.\n",\r\nobj, redzone1, redzone2);\r\n}\r\nstatic void *cache_free_debugcheck(struct kmem_cache *cachep, void *objp,\r\nunsigned long caller)\r\n{\r\nstruct page *page;\r\nunsigned int objnr;\r\nstruct slab *slabp;\r\nBUG_ON(virt_to_cache(objp) != cachep);\r\nobjp -= obj_offset(cachep);\r\nkfree_debugcheck(objp);\r\npage = virt_to_head_page(objp);\r\nslabp = page->slab_page;\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nverify_redzone_free(cachep, objp);\r\n*dbg_redzone1(cachep, objp) = RED_INACTIVE;\r\n*dbg_redzone2(cachep, objp) = RED_INACTIVE;\r\n}\r\nif (cachep->flags & SLAB_STORE_USER)\r\n*dbg_userword(cachep, objp) = (void *)caller;\r\nobjnr = obj_to_index(cachep, slabp, objp);\r\nBUG_ON(objnr >= cachep->num);\r\nBUG_ON(objp != index_to_obj(cachep, slabp, objnr));\r\n#ifdef CONFIG_DEBUG_SLAB_LEAK\r\nslab_bufctl(slabp)[objnr] = BUFCTL_FREE;\r\n#endif\r\nif (cachep->flags & SLAB_POISON) {\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif ((cachep->size % PAGE_SIZE)==0 && OFF_SLAB(cachep)) {\r\nstore_stackinfo(cachep, objp, caller);\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->size / PAGE_SIZE, 0);\r\n} else {\r\npoison_obj(cachep, objp, POISON_FREE);\r\n}\r\n#else\r\npoison_obj(cachep, objp, POISON_FREE);\r\n#endif\r\n}\r\nreturn objp;\r\n}\r\nstatic void check_slabp(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\nkmem_bufctl_t i;\r\nint entries = 0;\r\nfor (i = slabp->free; i != BUFCTL_END; i = slab_bufctl(slabp)[i]) {\r\nentries++;\r\nif (entries > cachep->num || i >= cachep->num)\r\ngoto bad;\r\n}\r\nif (entries != cachep->num - slabp->inuse) {\r\nbad:\r\nprintk(KERN_ERR "slab: Internal list corruption detected in "\r\n"cache '%s'(%d), slabp %p(%d). Tainted(%s). Hexdump:\n",\r\ncachep->name, cachep->num, slabp, slabp->inuse,\r\nprint_tainted());\r\nprint_hex_dump(KERN_ERR, "", DUMP_PREFIX_OFFSET, 16, 1, slabp,\r\nsizeof(*slabp) + cachep->num * sizeof(kmem_bufctl_t),\r\n1);\r\nBUG();\r\n}\r\n}\r\nstatic void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags,\r\nbool force_refill)\r\n{\r\nint batchcount;\r\nstruct kmem_cache_node *n;\r\nstruct array_cache *ac;\r\nint node;\r\ncheck_irq_off();\r\nnode = numa_mem_id();\r\nif (unlikely(force_refill))\r\ngoto force_grow;\r\nretry:\r\nac = cpu_cache_get(cachep);\r\nbatchcount = ac->batchcount;\r\nif (!ac->touched && batchcount > BATCHREFILL_LIMIT) {\r\nbatchcount = BATCHREFILL_LIMIT;\r\n}\r\nn = cachep->node[node];\r\nBUG_ON(ac->avail > 0 || !n);\r\nspin_lock(&n->list_lock);\r\nif (n->shared && transfer_objects(ac, n->shared, batchcount)) {\r\nn->shared->touched = 1;\r\ngoto alloc_done;\r\n}\r\nwhile (batchcount > 0) {\r\nstruct list_head *entry;\r\nstruct slab *slabp;\r\nentry = n->slabs_partial.next;\r\nif (entry == &n->slabs_partial) {\r\nn->free_touched = 1;\r\nentry = n->slabs_free.next;\r\nif (entry == &n->slabs_free)\r\ngoto must_grow;\r\n}\r\nslabp = list_entry(entry, struct slab, list);\r\ncheck_slabp(cachep, slabp);\r\ncheck_spinlock_acquired(cachep);\r\nBUG_ON(slabp->inuse >= cachep->num);\r\nwhile (slabp->inuse < cachep->num && batchcount--) {\r\nSTATS_INC_ALLOCED(cachep);\r\nSTATS_INC_ACTIVE(cachep);\r\nSTATS_SET_HIGH(cachep);\r\nac_put_obj(cachep, ac, slab_get_obj(cachep, slabp,\r\nnode));\r\n}\r\ncheck_slabp(cachep, slabp);\r\nlist_del(&slabp->list);\r\nif (slabp->free == BUFCTL_END)\r\nlist_add(&slabp->list, &n->slabs_full);\r\nelse\r\nlist_add(&slabp->list, &n->slabs_partial);\r\n}\r\nmust_grow:\r\nn->free_objects -= ac->avail;\r\nalloc_done:\r\nspin_unlock(&n->list_lock);\r\nif (unlikely(!ac->avail)) {\r\nint x;\r\nforce_grow:\r\nx = cache_grow(cachep, flags | GFP_THISNODE, node, NULL);\r\nac = cpu_cache_get(cachep);\r\nnode = numa_mem_id();\r\nif (!x && (ac->avail == 0 || force_refill))\r\nreturn NULL;\r\nif (!ac->avail)\r\ngoto retry;\r\n}\r\nac->touched = 1;\r\nreturn ac_get_obj(cachep, ac, flags, force_refill);\r\n}\r\nstatic inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,\r\ngfp_t flags)\r\n{\r\nmight_sleep_if(flags & __GFP_WAIT);\r\n#if DEBUG\r\nkmem_flagcheck(cachep, flags);\r\n#endif\r\n}\r\nstatic void *cache_alloc_debugcheck_after(struct kmem_cache *cachep,\r\ngfp_t flags, void *objp, unsigned long caller)\r\n{\r\nif (!objp)\r\nreturn objp;\r\nif (cachep->flags & SLAB_POISON) {\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif ((cachep->size % PAGE_SIZE) == 0 && OFF_SLAB(cachep))\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->size / PAGE_SIZE, 1);\r\nelse\r\ncheck_poison_obj(cachep, objp);\r\n#else\r\ncheck_poison_obj(cachep, objp);\r\n#endif\r\npoison_obj(cachep, objp, POISON_INUSE);\r\n}\r\nif (cachep->flags & SLAB_STORE_USER)\r\n*dbg_userword(cachep, objp) = (void *)caller;\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nif (*dbg_redzone1(cachep, objp) != RED_INACTIVE ||\r\n*dbg_redzone2(cachep, objp) != RED_INACTIVE) {\r\nslab_error(cachep, "double free, or memory outside"\r\n" object was overwritten");\r\nprintk(KERN_ERR\r\n"%p: redzone 1:0x%llx, redzone 2:0x%llx\n",\r\nobjp, *dbg_redzone1(cachep, objp),\r\n*dbg_redzone2(cachep, objp));\r\n}\r\n*dbg_redzone1(cachep, objp) = RED_ACTIVE;\r\n*dbg_redzone2(cachep, objp) = RED_ACTIVE;\r\n}\r\n#ifdef CONFIG_DEBUG_SLAB_LEAK\r\n{\r\nstruct slab *slabp;\r\nunsigned objnr;\r\nslabp = virt_to_head_page(objp)->slab_page;\r\nobjnr = (unsigned)(objp - slabp->s_mem) / cachep->size;\r\nslab_bufctl(slabp)[objnr] = BUFCTL_ACTIVE;\r\n}\r\n#endif\r\nobjp += obj_offset(cachep);\r\nif (cachep->ctor && cachep->flags & SLAB_POISON)\r\ncachep->ctor(objp);\r\nif (ARCH_SLAB_MINALIGN &&\r\n((unsigned long)objp & (ARCH_SLAB_MINALIGN-1))) {\r\nprintk(KERN_ERR "0x%p: not aligned to ARCH_SLAB_MINALIGN=%d\n",\r\nobjp, (int)ARCH_SLAB_MINALIGN);\r\n}\r\nreturn objp;\r\n}\r\nstatic bool slab_should_failslab(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nif (cachep == kmem_cache)\r\nreturn false;\r\nreturn should_failslab(cachep->object_size, flags, cachep->flags);\r\n}\r\nstatic inline void *____cache_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nvoid *objp;\r\nstruct array_cache *ac;\r\nbool force_refill = false;\r\ncheck_irq_off();\r\nac = cpu_cache_get(cachep);\r\nif (likely(ac->avail)) {\r\nac->touched = 1;\r\nobjp = ac_get_obj(cachep, ac, flags, false);\r\nif (objp) {\r\nSTATS_INC_ALLOCHIT(cachep);\r\ngoto out;\r\n}\r\nforce_refill = true;\r\n}\r\nSTATS_INC_ALLOCMISS(cachep);\r\nobjp = cache_alloc_refill(cachep, flags, force_refill);\r\nac = cpu_cache_get(cachep);\r\nout:\r\nif (objp)\r\nkmemleak_erase(&ac->entry[ac->avail]);\r\nreturn objp;\r\n}\r\nstatic void *alternate_node_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nint nid_alloc, nid_here;\r\nif (in_interrupt() || (flags & __GFP_THISNODE))\r\nreturn NULL;\r\nnid_alloc = nid_here = numa_mem_id();\r\nif (cpuset_do_slab_mem_spread() && (cachep->flags & SLAB_MEM_SPREAD))\r\nnid_alloc = cpuset_slab_spread_node();\r\nelse if (current->mempolicy)\r\nnid_alloc = slab_node();\r\nif (nid_alloc != nid_here)\r\nreturn ____cache_alloc_node(cachep, flags, nid_alloc);\r\nreturn NULL;\r\n}\r\nstatic void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)\r\n{\r\nstruct zonelist *zonelist;\r\ngfp_t local_flags;\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nenum zone_type high_zoneidx = gfp_zone(flags);\r\nvoid *obj = NULL;\r\nint nid;\r\nunsigned int cpuset_mems_cookie;\r\nif (flags & __GFP_THISNODE)\r\nreturn NULL;\r\nlocal_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);\r\nretry_cpuset:\r\ncpuset_mems_cookie = get_mems_allowed();\r\nzonelist = node_zonelist(slab_node(), flags);\r\nretry:\r\nfor_each_zone_zonelist(zone, z, zonelist, high_zoneidx) {\r\nnid = zone_to_nid(zone);\r\nif (cpuset_zone_allowed_hardwall(zone, flags) &&\r\ncache->node[nid] &&\r\ncache->node[nid]->free_objects) {\r\nobj = ____cache_alloc_node(cache,\r\nflags | GFP_THISNODE, nid);\r\nif (obj)\r\nbreak;\r\n}\r\n}\r\nif (!obj) {\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_enable();\r\nkmem_flagcheck(cache, flags);\r\nobj = kmem_getpages(cache, local_flags, numa_mem_id());\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_disable();\r\nif (obj) {\r\nnid = page_to_nid(virt_to_page(obj));\r\nif (cache_grow(cache, flags, nid, obj)) {\r\nobj = ____cache_alloc_node(cache,\r\nflags | GFP_THISNODE, nid);\r\nif (!obj)\r\ngoto retry;\r\n} else {\r\nobj = NULL;\r\n}\r\n}\r\n}\r\nif (unlikely(!put_mems_allowed(cpuset_mems_cookie) && !obj))\r\ngoto retry_cpuset;\r\nreturn obj;\r\n}\r\nstatic void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,\r\nint nodeid)\r\n{\r\nstruct list_head *entry;\r\nstruct slab *slabp;\r\nstruct kmem_cache_node *n;\r\nvoid *obj;\r\nint x;\r\nVM_BUG_ON(nodeid > num_online_nodes());\r\nn = cachep->node[nodeid];\r\nBUG_ON(!n);\r\nretry:\r\ncheck_irq_off();\r\nspin_lock(&n->list_lock);\r\nentry = n->slabs_partial.next;\r\nif (entry == &n->slabs_partial) {\r\nn->free_touched = 1;\r\nentry = n->slabs_free.next;\r\nif (entry == &n->slabs_free)\r\ngoto must_grow;\r\n}\r\nslabp = list_entry(entry, struct slab, list);\r\ncheck_spinlock_acquired_node(cachep, nodeid);\r\ncheck_slabp(cachep, slabp);\r\nSTATS_INC_NODEALLOCS(cachep);\r\nSTATS_INC_ACTIVE(cachep);\r\nSTATS_SET_HIGH(cachep);\r\nBUG_ON(slabp->inuse == cachep->num);\r\nobj = slab_get_obj(cachep, slabp, nodeid);\r\ncheck_slabp(cachep, slabp);\r\nn->free_objects--;\r\nlist_del(&slabp->list);\r\nif (slabp->free == BUFCTL_END)\r\nlist_add(&slabp->list, &n->slabs_full);\r\nelse\r\nlist_add(&slabp->list, &n->slabs_partial);\r\nspin_unlock(&n->list_lock);\r\ngoto done;\r\nmust_grow:\r\nspin_unlock(&n->list_lock);\r\nx = cache_grow(cachep, flags | GFP_THISNODE, nodeid, NULL);\r\nif (x)\r\ngoto retry;\r\nreturn fallback_alloc(cachep, flags);\r\ndone:\r\nreturn obj;\r\n}\r\nstatic __always_inline void *\r\nslab_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,\r\nunsigned long caller)\r\n{\r\nunsigned long save_flags;\r\nvoid *ptr;\r\nint slab_node = numa_mem_id();\r\nflags &= gfp_allowed_mask;\r\nlockdep_trace_alloc(flags);\r\nif (slab_should_failslab(cachep, flags))\r\nreturn NULL;\r\ncachep = memcg_kmem_get_cache(cachep, flags);\r\ncache_alloc_debugcheck_before(cachep, flags);\r\nlocal_irq_save(save_flags);\r\nif (nodeid == NUMA_NO_NODE)\r\nnodeid = slab_node;\r\nif (unlikely(!cachep->node[nodeid])) {\r\nptr = fallback_alloc(cachep, flags);\r\ngoto out;\r\n}\r\nif (nodeid == slab_node) {\r\nptr = ____cache_alloc(cachep, flags);\r\nif (ptr)\r\ngoto out;\r\n}\r\nptr = ____cache_alloc_node(cachep, flags, nodeid);\r\nout:\r\nlocal_irq_restore(save_flags);\r\nptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller);\r\nkmemleak_alloc_recursive(ptr, cachep->object_size, 1, cachep->flags,\r\nflags);\r\nif (likely(ptr))\r\nkmemcheck_slab_alloc(cachep, flags, ptr, cachep->object_size);\r\nif (unlikely((flags & __GFP_ZERO) && ptr))\r\nmemset(ptr, 0, cachep->object_size);\r\nreturn ptr;\r\n}\r\nstatic __always_inline void *\r\n__do_cache_alloc(struct kmem_cache *cache, gfp_t flags)\r\n{\r\nvoid *objp;\r\nif (unlikely(current->flags & (PF_SPREAD_SLAB | PF_MEMPOLICY))) {\r\nobjp = alternate_node_alloc(cache, flags);\r\nif (objp)\r\ngoto out;\r\n}\r\nobjp = ____cache_alloc(cache, flags);\r\nif (!objp)\r\nobjp = ____cache_alloc_node(cache, flags, numa_mem_id());\r\nout:\r\nreturn objp;\r\n}\r\nstatic __always_inline void *\r\n__do_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nreturn ____cache_alloc(cachep, flags);\r\n}\r\nstatic __always_inline void *\r\nslab_alloc(struct kmem_cache *cachep, gfp_t flags, unsigned long caller)\r\n{\r\nunsigned long save_flags;\r\nvoid *objp;\r\nflags &= gfp_allowed_mask;\r\nlockdep_trace_alloc(flags);\r\nif (slab_should_failslab(cachep, flags))\r\nreturn NULL;\r\ncachep = memcg_kmem_get_cache(cachep, flags);\r\ncache_alloc_debugcheck_before(cachep, flags);\r\nlocal_irq_save(save_flags);\r\nobjp = __do_cache_alloc(cachep, flags);\r\nlocal_irq_restore(save_flags);\r\nobjp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);\r\nkmemleak_alloc_recursive(objp, cachep->object_size, 1, cachep->flags,\r\nflags);\r\nprefetchw(objp);\r\nif (likely(objp))\r\nkmemcheck_slab_alloc(cachep, flags, objp, cachep->object_size);\r\nif (unlikely((flags & __GFP_ZERO) && objp))\r\nmemset(objp, 0, cachep->object_size);\r\nreturn objp;\r\n}\r\nstatic void free_block(struct kmem_cache *cachep, void **objpp, int nr_objects,\r\nint node)\r\n{\r\nint i;\r\nstruct kmem_cache_node *n;\r\nfor (i = 0; i < nr_objects; i++) {\r\nvoid *objp;\r\nstruct slab *slabp;\r\nclear_obj_pfmemalloc(&objpp[i]);\r\nobjp = objpp[i];\r\nslabp = virt_to_slab(objp);\r\nn = cachep->node[node];\r\nlist_del(&slabp->list);\r\ncheck_spinlock_acquired_node(cachep, node);\r\ncheck_slabp(cachep, slabp);\r\nslab_put_obj(cachep, slabp, objp, node);\r\nSTATS_DEC_ACTIVE(cachep);\r\nn->free_objects++;\r\ncheck_slabp(cachep, slabp);\r\nif (slabp->inuse == 0) {\r\nif (n->free_objects > n->free_limit) {\r\nn->free_objects -= cachep->num;\r\nslab_destroy(cachep, slabp);\r\n} else {\r\nlist_add(&slabp->list, &n->slabs_free);\r\n}\r\n} else {\r\nlist_add_tail(&slabp->list, &n->slabs_partial);\r\n}\r\n}\r\n}\r\nstatic void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)\r\n{\r\nint batchcount;\r\nstruct kmem_cache_node *n;\r\nint node = numa_mem_id();\r\nbatchcount = ac->batchcount;\r\n#if DEBUG\r\nBUG_ON(!batchcount || batchcount > ac->avail);\r\n#endif\r\ncheck_irq_off();\r\nn = cachep->node[node];\r\nspin_lock(&n->list_lock);\r\nif (n->shared) {\r\nstruct array_cache *shared_array = n->shared;\r\nint max = shared_array->limit - shared_array->avail;\r\nif (max) {\r\nif (batchcount > max)\r\nbatchcount = max;\r\nmemcpy(&(shared_array->entry[shared_array->avail]),\r\nac->entry, sizeof(void *) * batchcount);\r\nshared_array->avail += batchcount;\r\ngoto free_done;\r\n}\r\n}\r\nfree_block(cachep, ac->entry, batchcount, node);\r\nfree_done:\r\n#if STATS\r\n{\r\nint i = 0;\r\nstruct list_head *p;\r\np = n->slabs_free.next;\r\nwhile (p != &(n->slabs_free)) {\r\nstruct slab *slabp;\r\nslabp = list_entry(p, struct slab, list);\r\nBUG_ON(slabp->inuse);\r\ni++;\r\np = p->next;\r\n}\r\nSTATS_SET_FREEABLE(cachep, i);\r\n}\r\n#endif\r\nspin_unlock(&n->list_lock);\r\nac->avail -= batchcount;\r\nmemmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);\r\n}\r\nstatic inline void __cache_free(struct kmem_cache *cachep, void *objp,\r\nunsigned long caller)\r\n{\r\nstruct array_cache *ac = cpu_cache_get(cachep);\r\ncheck_irq_off();\r\nkmemleak_free_recursive(objp, cachep->flags);\r\nobjp = cache_free_debugcheck(cachep, objp, caller);\r\nkmemcheck_slab_free(cachep, objp, cachep->object_size);\r\nif (nr_online_nodes > 1 && cache_free_alien(cachep, objp))\r\nreturn;\r\nif (likely(ac->avail < ac->limit)) {\r\nSTATS_INC_FREEHIT(cachep);\r\n} else {\r\nSTATS_INC_FREEMISS(cachep);\r\ncache_flusharray(cachep, ac);\r\n}\r\nac_put_obj(cachep, ac, objp);\r\n}\r\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nvoid *ret = slab_alloc(cachep, flags, _RET_IP_);\r\ntrace_kmem_cache_alloc(_RET_IP_, ret,\r\ncachep->object_size, cachep->size, flags);\r\nreturn ret;\r\n}\r\nvoid *\r\nkmem_cache_alloc_trace(struct kmem_cache *cachep, gfp_t flags, size_t size)\r\n{\r\nvoid *ret;\r\nret = slab_alloc(cachep, flags, _RET_IP_);\r\ntrace_kmalloc(_RET_IP_, ret,\r\nsize, cachep->size, flags);\r\nreturn ret;\r\n}\r\nvoid *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)\r\n{\r\nvoid *ret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);\r\ntrace_kmem_cache_alloc_node(_RET_IP_, ret,\r\ncachep->object_size, cachep->size,\r\nflags, nodeid);\r\nreturn ret;\r\n}\r\nvoid *kmem_cache_alloc_node_trace(struct kmem_cache *cachep,\r\ngfp_t flags,\r\nint nodeid,\r\nsize_t size)\r\n{\r\nvoid *ret;\r\nret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);\r\ntrace_kmalloc_node(_RET_IP_, ret,\r\nsize, cachep->size,\r\nflags, nodeid);\r\nreturn ret;\r\n}\r\nstatic __always_inline void *\r\n__do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)\r\n{\r\nstruct kmem_cache *cachep;\r\ncachep = kmalloc_slab(size, flags);\r\nif (unlikely(ZERO_OR_NULL_PTR(cachep)))\r\nreturn cachep;\r\nreturn kmem_cache_alloc_node_trace(cachep, flags, node, size);\r\n}\r\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\r\n{\r\nreturn __do_kmalloc_node(size, flags, node, _RET_IP_);\r\n}\r\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t flags,\r\nint node, unsigned long caller)\r\n{\r\nreturn __do_kmalloc_node(size, flags, node, caller);\r\n}\r\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\r\n{\r\nreturn __do_kmalloc_node(size, flags, node, 0);\r\n}\r\nstatic __always_inline void *__do_kmalloc(size_t size, gfp_t flags,\r\nunsigned long caller)\r\n{\r\nstruct kmem_cache *cachep;\r\nvoid *ret;\r\ncachep = kmalloc_slab(size, flags);\r\nif (unlikely(ZERO_OR_NULL_PTR(cachep)))\r\nreturn cachep;\r\nret = slab_alloc(cachep, flags, caller);\r\ntrace_kmalloc(caller, ret,\r\nsize, cachep->size, flags);\r\nreturn ret;\r\n}\r\nvoid *__kmalloc(size_t size, gfp_t flags)\r\n{\r\nreturn __do_kmalloc(size, flags, _RET_IP_);\r\n}\r\nvoid *__kmalloc_track_caller(size_t size, gfp_t flags, unsigned long caller)\r\n{\r\nreturn __do_kmalloc(size, flags, caller);\r\n}\r\nvoid *__kmalloc(size_t size, gfp_t flags)\r\n{\r\nreturn __do_kmalloc(size, flags, 0);\r\n}\r\nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\r\n{\r\nunsigned long flags;\r\ncachep = cache_from_obj(cachep, objp);\r\nif (!cachep)\r\nreturn;\r\nlocal_irq_save(flags);\r\ndebug_check_no_locks_freed(objp, cachep->object_size);\r\nif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\r\ndebug_check_no_obj_freed(objp, cachep->object_size);\r\n__cache_free(cachep, objp, _RET_IP_);\r\nlocal_irq_restore(flags);\r\ntrace_kmem_cache_free(_RET_IP_, objp);\r\n}\r\nvoid kfree(const void *objp)\r\n{\r\nstruct kmem_cache *c;\r\nunsigned long flags;\r\ntrace_kfree(_RET_IP_, objp);\r\nif (unlikely(ZERO_OR_NULL_PTR(objp)))\r\nreturn;\r\nlocal_irq_save(flags);\r\nkfree_debugcheck(objp);\r\nc = virt_to_cache(objp);\r\ndebug_check_no_locks_freed(objp, c->object_size);\r\ndebug_check_no_obj_freed(objp, c->object_size);\r\n__cache_free(c, (void *)objp, _RET_IP_);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int alloc_kmemlist(struct kmem_cache *cachep, gfp_t gfp)\r\n{\r\nint node;\r\nstruct kmem_cache_node *n;\r\nstruct array_cache *new_shared;\r\nstruct array_cache **new_alien = NULL;\r\nfor_each_online_node(node) {\r\nif (use_alien_caches) {\r\nnew_alien = alloc_alien_cache(node, cachep->limit, gfp);\r\nif (!new_alien)\r\ngoto fail;\r\n}\r\nnew_shared = NULL;\r\nif (cachep->shared) {\r\nnew_shared = alloc_arraycache(node,\r\ncachep->shared*cachep->batchcount,\r\n0xbaadf00d, gfp);\r\nif (!new_shared) {\r\nfree_alien_cache(new_alien);\r\ngoto fail;\r\n}\r\n}\r\nn = cachep->node[node];\r\nif (n) {\r\nstruct array_cache *shared = n->shared;\r\nspin_lock_irq(&n->list_lock);\r\nif (shared)\r\nfree_block(cachep, shared->entry,\r\nshared->avail, node);\r\nn->shared = new_shared;\r\nif (!n->alien) {\r\nn->alien = new_alien;\r\nnew_alien = NULL;\r\n}\r\nn->free_limit = (1 + nr_cpus_node(node)) *\r\ncachep->batchcount + cachep->num;\r\nspin_unlock_irq(&n->list_lock);\r\nkfree(shared);\r\nfree_alien_cache(new_alien);\r\ncontinue;\r\n}\r\nn = kmalloc_node(sizeof(struct kmem_cache_node), gfp, node);\r\nif (!n) {\r\nfree_alien_cache(new_alien);\r\nkfree(new_shared);\r\ngoto fail;\r\n}\r\nkmem_cache_node_init(n);\r\nn->next_reap = jiffies + REAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\nn->shared = new_shared;\r\nn->alien = new_alien;\r\nn->free_limit = (1 + nr_cpus_node(node)) *\r\ncachep->batchcount + cachep->num;\r\ncachep->node[node] = n;\r\n}\r\nreturn 0;\r\nfail:\r\nif (!cachep->list.next) {\r\nnode--;\r\nwhile (node >= 0) {\r\nif (cachep->node[node]) {\r\nn = cachep->node[node];\r\nkfree(n->shared);\r\nfree_alien_cache(n->alien);\r\nkfree(n);\r\ncachep->node[node] = NULL;\r\n}\r\nnode--;\r\n}\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void do_ccupdate_local(void *info)\r\n{\r\nstruct ccupdate_struct *new = info;\r\nstruct array_cache *old;\r\ncheck_irq_off();\r\nold = cpu_cache_get(new->cachep);\r\nnew->cachep->array[smp_processor_id()] = new->new[smp_processor_id()];\r\nnew->new[smp_processor_id()] = old;\r\n}\r\nstatic int __do_tune_cpucache(struct kmem_cache *cachep, int limit,\r\nint batchcount, int shared, gfp_t gfp)\r\n{\r\nstruct ccupdate_struct *new;\r\nint i;\r\nnew = kzalloc(sizeof(*new) + nr_cpu_ids * sizeof(struct array_cache *),\r\ngfp);\r\nif (!new)\r\nreturn -ENOMEM;\r\nfor_each_online_cpu(i) {\r\nnew->new[i] = alloc_arraycache(cpu_to_mem(i), limit,\r\nbatchcount, gfp);\r\nif (!new->new[i]) {\r\nfor (i--; i >= 0; i--)\r\nkfree(new->new[i]);\r\nkfree(new);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nnew->cachep = cachep;\r\non_each_cpu(do_ccupdate_local, (void *)new, 1);\r\ncheck_irq_on();\r\ncachep->batchcount = batchcount;\r\ncachep->limit = limit;\r\ncachep->shared = shared;\r\nfor_each_online_cpu(i) {\r\nstruct array_cache *ccold = new->new[i];\r\nif (!ccold)\r\ncontinue;\r\nspin_lock_irq(&cachep->node[cpu_to_mem(i)]->list_lock);\r\nfree_block(cachep, ccold->entry, ccold->avail, cpu_to_mem(i));\r\nspin_unlock_irq(&cachep->node[cpu_to_mem(i)]->list_lock);\r\nkfree(ccold);\r\n}\r\nkfree(new);\r\nreturn alloc_kmemlist(cachep, gfp);\r\n}\r\nstatic int do_tune_cpucache(struct kmem_cache *cachep, int limit,\r\nint batchcount, int shared, gfp_t gfp)\r\n{\r\nint ret;\r\nstruct kmem_cache *c = NULL;\r\nint i = 0;\r\nret = __do_tune_cpucache(cachep, limit, batchcount, shared, gfp);\r\nif (slab_state < FULL)\r\nreturn ret;\r\nif ((ret < 0) || !is_root_cache(cachep))\r\nreturn ret;\r\nVM_BUG_ON(!mutex_is_locked(&slab_mutex));\r\nfor_each_memcg_cache_index(i) {\r\nc = cache_from_memcg(cachep, i);\r\nif (c)\r\n__do_tune_cpucache(c, limit, batchcount, shared, gfp);\r\n}\r\nreturn ret;\r\n}\r\nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp)\r\n{\r\nint err;\r\nint limit = 0;\r\nint shared = 0;\r\nint batchcount = 0;\r\nif (!is_root_cache(cachep)) {\r\nstruct kmem_cache *root = memcg_root_cache(cachep);\r\nlimit = root->limit;\r\nshared = root->shared;\r\nbatchcount = root->batchcount;\r\n}\r\nif (limit && shared && batchcount)\r\ngoto skip_setup;\r\nif (cachep->size > 131072)\r\nlimit = 1;\r\nelse if (cachep->size > PAGE_SIZE)\r\nlimit = 8;\r\nelse if (cachep->size > 1024)\r\nlimit = 24;\r\nelse if (cachep->size > 256)\r\nlimit = 54;\r\nelse\r\nlimit = 120;\r\nshared = 0;\r\nif (cachep->size <= PAGE_SIZE && num_possible_cpus() > 1)\r\nshared = 8;\r\n#if DEBUG\r\nif (limit > 32)\r\nlimit = 32;\r\n#endif\r\nbatchcount = (limit + 1) / 2;\r\nskip_setup:\r\nerr = do_tune_cpucache(cachep, limit, batchcount, shared, gfp);\r\nif (err)\r\nprintk(KERN_ERR "enable_cpucache failed for %s, error %d.\n",\r\ncachep->name, -err);\r\nreturn err;\r\n}\r\nstatic void drain_array(struct kmem_cache *cachep, struct kmem_cache_node *n,\r\nstruct array_cache *ac, int force, int node)\r\n{\r\nint tofree;\r\nif (!ac || !ac->avail)\r\nreturn;\r\nif (ac->touched && !force) {\r\nac->touched = 0;\r\n} else {\r\nspin_lock_irq(&n->list_lock);\r\nif (ac->avail) {\r\ntofree = force ? ac->avail : (ac->limit + 4) / 5;\r\nif (tofree > ac->avail)\r\ntofree = (ac->avail + 1) / 2;\r\nfree_block(cachep, ac->entry, tofree, node);\r\nac->avail -= tofree;\r\nmemmove(ac->entry, &(ac->entry[tofree]),\r\nsizeof(void *) * ac->avail);\r\n}\r\nspin_unlock_irq(&n->list_lock);\r\n}\r\n}\r\nstatic void cache_reap(struct work_struct *w)\r\n{\r\nstruct kmem_cache *searchp;\r\nstruct kmem_cache_node *n;\r\nint node = numa_mem_id();\r\nstruct delayed_work *work = to_delayed_work(w);\r\nif (!mutex_trylock(&slab_mutex))\r\ngoto out;\r\nlist_for_each_entry(searchp, &slab_caches, list) {\r\ncheck_irq_on();\r\nn = searchp->node[node];\r\nreap_alien(searchp, n);\r\ndrain_array(searchp, n, cpu_cache_get(searchp), 0, node);\r\nif (time_after(n->next_reap, jiffies))\r\ngoto next;\r\nn->next_reap = jiffies + REAPTIMEOUT_LIST3;\r\ndrain_array(searchp, n, n->shared, 0, node);\r\nif (n->free_touched)\r\nn->free_touched = 0;\r\nelse {\r\nint freed;\r\nfreed = drain_freelist(searchp, n, (n->free_limit +\r\n5 * searchp->num - 1) / (5 * searchp->num));\r\nSTATS_ADD_REAPED(searchp, freed);\r\n}\r\nnext:\r\ncond_resched();\r\n}\r\ncheck_irq_on();\r\nmutex_unlock(&slab_mutex);\r\nnext_reap_node();\r\nout:\r\nschedule_delayed_work(work, round_jiffies_relative(REAPTIMEOUT_CPUC));\r\n}\r\nvoid get_slabinfo(struct kmem_cache *cachep, struct slabinfo *sinfo)\r\n{\r\nstruct slab *slabp;\r\nunsigned long active_objs;\r\nunsigned long num_objs;\r\nunsigned long active_slabs = 0;\r\nunsigned long num_slabs, free_objects = 0, shared_avail = 0;\r\nconst char *name;\r\nchar *error = NULL;\r\nint node;\r\nstruct kmem_cache_node *n;\r\nactive_objs = 0;\r\nnum_slabs = 0;\r\nfor_each_online_node(node) {\r\nn = cachep->node[node];\r\nif (!n)\r\ncontinue;\r\ncheck_irq_on();\r\nspin_lock_irq(&n->list_lock);\r\nlist_for_each_entry(slabp, &n->slabs_full, list) {\r\nif (slabp->inuse != cachep->num && !error)\r\nerror = "slabs_full accounting error";\r\nactive_objs += cachep->num;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &n->slabs_partial, list) {\r\nif (slabp->inuse == cachep->num && !error)\r\nerror = "slabs_partial inuse accounting error";\r\nif (!slabp->inuse && !error)\r\nerror = "slabs_partial/inuse accounting error";\r\nactive_objs += slabp->inuse;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &n->slabs_free, list) {\r\nif (slabp->inuse && !error)\r\nerror = "slabs_free/inuse accounting error";\r\nnum_slabs++;\r\n}\r\nfree_objects += n->free_objects;\r\nif (n->shared)\r\nshared_avail += n->shared->avail;\r\nspin_unlock_irq(&n->list_lock);\r\n}\r\nnum_slabs += active_slabs;\r\nnum_objs = num_slabs * cachep->num;\r\nif (num_objs - active_objs != free_objects && !error)\r\nerror = "free_objects accounting error";\r\nname = cachep->name;\r\nif (error)\r\nprintk(KERN_ERR "slab: cache %s error: %s\n", name, error);\r\nsinfo->active_objs = active_objs;\r\nsinfo->num_objs = num_objs;\r\nsinfo->active_slabs = active_slabs;\r\nsinfo->num_slabs = num_slabs;\r\nsinfo->shared_avail = shared_avail;\r\nsinfo->limit = cachep->limit;\r\nsinfo->batchcount = cachep->batchcount;\r\nsinfo->shared = cachep->shared;\r\nsinfo->objects_per_slab = cachep->num;\r\nsinfo->cache_order = cachep->gfporder;\r\n}\r\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *cachep)\r\n{\r\n#if STATS\r\n{\r\nunsigned long high = cachep->high_mark;\r\nunsigned long allocs = cachep->num_allocations;\r\nunsigned long grown = cachep->grown;\r\nunsigned long reaped = cachep->reaped;\r\nunsigned long errors = cachep->errors;\r\nunsigned long max_freeable = cachep->max_freeable;\r\nunsigned long node_allocs = cachep->node_allocs;\r\nunsigned long node_frees = cachep->node_frees;\r\nunsigned long overflows = cachep->node_overflow;\r\nseq_printf(m, " : globalstat %7lu %6lu %5lu %4lu "\r\n"%4lu %4lu %4lu %4lu %4lu",\r\nallocs, high, grown,\r\nreaped, errors, max_freeable, node_allocs,\r\nnode_frees, overflows);\r\n}\r\n{\r\nunsigned long allochit = atomic_read(&cachep->allochit);\r\nunsigned long allocmiss = atomic_read(&cachep->allocmiss);\r\nunsigned long freehit = atomic_read(&cachep->freehit);\r\nunsigned long freemiss = atomic_read(&cachep->freemiss);\r\nseq_printf(m, " : cpustat %6lu %6lu %6lu %6lu",\r\nallochit, allocmiss, freehit, freemiss);\r\n}\r\n#endif\r\n}\r\nssize_t slabinfo_write(struct file *file, const char __user *buffer,\r\nsize_t count, loff_t *ppos)\r\n{\r\nchar kbuf[MAX_SLABINFO_WRITE + 1], *tmp;\r\nint limit, batchcount, shared, res;\r\nstruct kmem_cache *cachep;\r\nif (count > MAX_SLABINFO_WRITE)\r\nreturn -EINVAL;\r\nif (copy_from_user(&kbuf, buffer, count))\r\nreturn -EFAULT;\r\nkbuf[MAX_SLABINFO_WRITE] = '\0';\r\ntmp = strchr(kbuf, ' ');\r\nif (!tmp)\r\nreturn -EINVAL;\r\n*tmp = '\0';\r\ntmp++;\r\nif (sscanf(tmp, " %d %d %d", &limit, &batchcount, &shared) != 3)\r\nreturn -EINVAL;\r\nmutex_lock(&slab_mutex);\r\nres = -EINVAL;\r\nlist_for_each_entry(cachep, &slab_caches, list) {\r\nif (!strcmp(cachep->name, kbuf)) {\r\nif (limit < 1 || batchcount < 1 ||\r\nbatchcount > limit || shared < 0) {\r\nres = 0;\r\n} else {\r\nres = do_tune_cpucache(cachep, limit,\r\nbatchcount, shared,\r\nGFP_KERNEL);\r\n}\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&slab_mutex);\r\nif (res >= 0)\r\nres = count;\r\nreturn res;\r\n}\r\nstatic void *leaks_start(struct seq_file *m, loff_t *pos)\r\n{\r\nmutex_lock(&slab_mutex);\r\nreturn seq_list_start(&slab_caches, *pos);\r\n}\r\nstatic inline int add_caller(unsigned long *n, unsigned long v)\r\n{\r\nunsigned long *p;\r\nint l;\r\nif (!v)\r\nreturn 1;\r\nl = n[1];\r\np = n + 2;\r\nwhile (l) {\r\nint i = l/2;\r\nunsigned long *q = p + 2 * i;\r\nif (*q == v) {\r\nq[1]++;\r\nreturn 1;\r\n}\r\nif (*q > v) {\r\nl = i;\r\n} else {\r\np = q + 2;\r\nl -= i + 1;\r\n}\r\n}\r\nif (++n[1] == n[0])\r\nreturn 0;\r\nmemmove(p + 2, p, n[1] * 2 * sizeof(unsigned long) - ((void *)p - (void *)n));\r\np[0] = v;\r\np[1] = 1;\r\nreturn 1;\r\n}\r\nstatic void handle_slab(unsigned long *n, struct kmem_cache *c, struct slab *s)\r\n{\r\nvoid *p;\r\nint i;\r\nif (n[0] == n[1])\r\nreturn;\r\nfor (i = 0, p = s->s_mem; i < c->num; i++, p += c->size) {\r\nif (slab_bufctl(s)[i] != BUFCTL_ACTIVE)\r\ncontinue;\r\nif (!add_caller(n, (unsigned long)*dbg_userword(c, p)))\r\nreturn;\r\n}\r\n}\r\nstatic void show_symbol(struct seq_file *m, unsigned long address)\r\n{\r\n#ifdef CONFIG_KALLSYMS\r\nunsigned long offset, size;\r\nchar modname[MODULE_NAME_LEN], name[KSYM_NAME_LEN];\r\nif (lookup_symbol_attrs(address, &size, &offset, modname, name) == 0) {\r\nseq_printf(m, "%s+%#lx/%#lx", name, offset, size);\r\nif (modname[0])\r\nseq_printf(m, " [%s]", modname);\r\nreturn;\r\n}\r\n#endif\r\nseq_printf(m, "%p", (void *)address);\r\n}\r\nstatic int leaks_show(struct seq_file *m, void *p)\r\n{\r\nstruct kmem_cache *cachep = list_entry(p, struct kmem_cache, list);\r\nstruct slab *slabp;\r\nstruct kmem_cache_node *n;\r\nconst char *name;\r\nunsigned long *x = m->private;\r\nint node;\r\nint i;\r\nif (!(cachep->flags & SLAB_STORE_USER))\r\nreturn 0;\r\nif (!(cachep->flags & SLAB_RED_ZONE))\r\nreturn 0;\r\nx[1] = 0;\r\nfor_each_online_node(node) {\r\nn = cachep->node[node];\r\nif (!n)\r\ncontinue;\r\ncheck_irq_on();\r\nspin_lock_irq(&n->list_lock);\r\nlist_for_each_entry(slabp, &n->slabs_full, list)\r\nhandle_slab(x, cachep, slabp);\r\nlist_for_each_entry(slabp, &n->slabs_partial, list)\r\nhandle_slab(x, cachep, slabp);\r\nspin_unlock_irq(&n->list_lock);\r\n}\r\nname = cachep->name;\r\nif (x[0] == x[1]) {\r\nmutex_unlock(&slab_mutex);\r\nm->private = kzalloc(x[0] * 4 * sizeof(unsigned long), GFP_KERNEL);\r\nif (!m->private) {\r\nm->private = x;\r\nmutex_lock(&slab_mutex);\r\nreturn -ENOMEM;\r\n}\r\n*(unsigned long *)m->private = x[0] * 2;\r\nkfree(x);\r\nmutex_lock(&slab_mutex);\r\nm->count = m->size;\r\nreturn 0;\r\n}\r\nfor (i = 0; i < x[1]; i++) {\r\nseq_printf(m, "%s: %lu ", name, x[2*i+3]);\r\nshow_symbol(m, x[2*i+2]);\r\nseq_putc(m, '\n');\r\n}\r\nreturn 0;\r\n}\r\nstatic int slabstats_open(struct inode *inode, struct file *file)\r\n{\r\nunsigned long *n = kzalloc(PAGE_SIZE, GFP_KERNEL);\r\nint ret = -ENOMEM;\r\nif (n) {\r\nret = seq_open(file, &slabstats_op);\r\nif (!ret) {\r\nstruct seq_file *m = file->private_data;\r\n*n = PAGE_SIZE / (2 * sizeof(unsigned long));\r\nm->private = n;\r\nn = NULL;\r\n}\r\nkfree(n);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __init slab_proc_init(void)\r\n{\r\n#ifdef CONFIG_DEBUG_SLAB_LEAK\r\nproc_create("slab_allocators", 0, NULL, &proc_slabstats_operations);\r\n#endif\r\nreturn 0;\r\n}\r\nsize_t ksize(const void *objp)\r\n{\r\nBUG_ON(!objp);\r\nif (unlikely(objp == ZERO_SIZE_PTR))\r\nreturn 0;\r\nreturn virt_to_cache(objp)->object_size;\r\n}
