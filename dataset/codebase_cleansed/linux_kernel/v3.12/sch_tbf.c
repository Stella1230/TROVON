static int tbf_segment(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *segs, *nskb;\r\nnetdev_features_t features = netif_skb_features(skb);\r\nint ret, nb;\r\nsegs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);\r\nif (IS_ERR_OR_NULL(segs))\r\nreturn qdisc_reshape_fail(skb, sch);\r\nnb = 0;\r\nwhile (segs) {\r\nnskb = segs->next;\r\nsegs->next = NULL;\r\nif (likely(segs->len <= q->max_size)) {\r\nqdisc_skb_cb(segs)->pkt_len = segs->len;\r\nret = qdisc_enqueue(segs, q->qdisc);\r\n} else {\r\nret = qdisc_reshape_fail(skb, sch);\r\n}\r\nif (ret != NET_XMIT_SUCCESS) {\r\nif (net_xmit_drop_count(ret))\r\nsch->qstats.drops++;\r\n} else {\r\nnb++;\r\n}\r\nsegs = nskb;\r\n}\r\nsch->q.qlen += nb;\r\nif (nb > 1)\r\nqdisc_tree_decrease_qlen(sch, 1 - nb);\r\nconsume_skb(skb);\r\nreturn nb > 0 ? NET_XMIT_SUCCESS : NET_XMIT_DROP;\r\n}\r\nstatic int tbf_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nint ret;\r\nif (qdisc_pkt_len(skb) > q->max_size) {\r\nif (skb_is_gso(skb))\r\nreturn tbf_segment(skb, sch);\r\nreturn qdisc_reshape_fail(skb, sch);\r\n}\r\nret = qdisc_enqueue(skb, q->qdisc);\r\nif (ret != NET_XMIT_SUCCESS) {\r\nif (net_xmit_drop_count(ret))\r\nsch->qstats.drops++;\r\nreturn ret;\r\n}\r\nsch->q.qlen++;\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic unsigned int tbf_drop(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nunsigned int len = 0;\r\nif (q->qdisc->ops->drop && (len = q->qdisc->ops->drop(q->qdisc)) != 0) {\r\nsch->q.qlen--;\r\nsch->qstats.drops++;\r\n}\r\nreturn len;\r\n}\r\nstatic struct sk_buff *tbf_dequeue(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nskb = q->qdisc->ops->peek(q->qdisc);\r\nif (skb) {\r\ns64 now;\r\ns64 toks;\r\ns64 ptoks = 0;\r\nunsigned int len = qdisc_pkt_len(skb);\r\nnow = ktime_to_ns(ktime_get());\r\ntoks = min_t(s64, now - q->t_c, q->buffer);\r\nif (q->peak_present) {\r\nptoks = toks + q->ptokens;\r\nif (ptoks > q->mtu)\r\nptoks = q->mtu;\r\nptoks -= (s64) psched_l2t_ns(&q->peak, len);\r\n}\r\ntoks += q->tokens;\r\nif (toks > q->buffer)\r\ntoks = q->buffer;\r\ntoks -= (s64) psched_l2t_ns(&q->rate, len);\r\nif ((toks|ptoks) >= 0) {\r\nskb = qdisc_dequeue_peeked(q->qdisc);\r\nif (unlikely(!skb))\r\nreturn NULL;\r\nq->t_c = now;\r\nq->tokens = toks;\r\nq->ptokens = ptoks;\r\nsch->q.qlen--;\r\nqdisc_unthrottled(sch);\r\nqdisc_bstats_update(sch, skb);\r\nreturn skb;\r\n}\r\nqdisc_watchdog_schedule_ns(&q->watchdog,\r\nnow + max_t(long, -toks, -ptoks));\r\nsch->qstats.overlimits++;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void tbf_reset(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nqdisc_reset(q->qdisc);\r\nsch->q.qlen = 0;\r\nq->t_c = ktime_to_ns(ktime_get());\r\nq->tokens = q->buffer;\r\nq->ptokens = q->mtu;\r\nqdisc_watchdog_cancel(&q->watchdog);\r\n}\r\nstatic int tbf_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nint err;\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_TBF_PTAB + 1];\r\nstruct tc_tbf_qopt *qopt;\r\nstruct qdisc_rate_table *rtab = NULL;\r\nstruct qdisc_rate_table *ptab = NULL;\r\nstruct Qdisc *child = NULL;\r\nint max_size, n;\r\nerr = nla_parse_nested(tb, TCA_TBF_PTAB, opt, tbf_policy);\r\nif (err < 0)\r\nreturn err;\r\nerr = -EINVAL;\r\nif (tb[TCA_TBF_PARMS] == NULL)\r\ngoto done;\r\nqopt = nla_data(tb[TCA_TBF_PARMS]);\r\nrtab = qdisc_get_rtab(&qopt->rate, tb[TCA_TBF_RTAB]);\r\nif (rtab == NULL)\r\ngoto done;\r\nif (qopt->peakrate.rate) {\r\nif (qopt->peakrate.rate > qopt->rate.rate)\r\nptab = qdisc_get_rtab(&qopt->peakrate, tb[TCA_TBF_PTAB]);\r\nif (ptab == NULL)\r\ngoto done;\r\n}\r\nfor (n = 0; n < 256; n++)\r\nif (rtab->data[n] > qopt->buffer)\r\nbreak;\r\nmax_size = (n << qopt->rate.cell_log) - 1;\r\nif (ptab) {\r\nint size;\r\nfor (n = 0; n < 256; n++)\r\nif (ptab->data[n] > qopt->mtu)\r\nbreak;\r\nsize = (n << qopt->peakrate.cell_log) - 1;\r\nif (size < max_size)\r\nmax_size = size;\r\n}\r\nif (max_size < 0)\r\ngoto done;\r\nif (q->qdisc != &noop_qdisc) {\r\nerr = fifo_set_limit(q->qdisc, qopt->limit);\r\nif (err)\r\ngoto done;\r\n} else if (qopt->limit > 0) {\r\nchild = fifo_create_dflt(sch, &bfifo_qdisc_ops, qopt->limit);\r\nif (IS_ERR(child)) {\r\nerr = PTR_ERR(child);\r\ngoto done;\r\n}\r\n}\r\nsch_tree_lock(sch);\r\nif (child) {\r\nqdisc_tree_decrease_qlen(q->qdisc, q->qdisc->q.qlen);\r\nqdisc_destroy(q->qdisc);\r\nq->qdisc = child;\r\n}\r\nq->limit = qopt->limit;\r\nq->mtu = PSCHED_TICKS2NS(qopt->mtu);\r\nq->max_size = max_size;\r\nq->buffer = PSCHED_TICKS2NS(qopt->buffer);\r\nq->tokens = q->buffer;\r\nq->ptokens = q->mtu;\r\npsched_ratecfg_precompute(&q->rate, &rtab->rate);\r\nif (ptab) {\r\npsched_ratecfg_precompute(&q->peak, &ptab->rate);\r\nq->peak_present = true;\r\n} else {\r\nq->peak_present = false;\r\n}\r\nsch_tree_unlock(sch);\r\nerr = 0;\r\ndone:\r\nif (rtab)\r\nqdisc_put_rtab(rtab);\r\nif (ptab)\r\nqdisc_put_rtab(ptab);\r\nreturn err;\r\n}\r\nstatic int tbf_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nq->t_c = ktime_to_ns(ktime_get());\r\nqdisc_watchdog_init(&q->watchdog, sch);\r\nq->qdisc = &noop_qdisc;\r\nreturn tbf_change(sch, opt);\r\n}\r\nstatic void tbf_destroy(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\nqdisc_destroy(q->qdisc);\r\n}\r\nstatic int tbf_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *nest;\r\nstruct tc_tbf_qopt opt;\r\nsch->qstats.backlog = q->qdisc->qstats.backlog;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nopt.limit = q->limit;\r\npsched_ratecfg_getrate(&opt.rate, &q->rate);\r\nif (q->peak_present)\r\npsched_ratecfg_getrate(&opt.peakrate, &q->peak);\r\nelse\r\nmemset(&opt.peakrate, 0, sizeof(opt.peakrate));\r\nopt.mtu = PSCHED_NS2TICKS(q->mtu);\r\nopt.buffer = PSCHED_NS2TICKS(q->buffer);\r\nif (nla_put(skb, TCA_TBF_PARMS, sizeof(opt), &opt))\r\ngoto nla_put_failure;\r\nnla_nest_end(skb, nest);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic int tbf_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\ntcm->tcm_handle |= TC_H_MIN(1);\r\ntcm->tcm_info = q->qdisc->handle;\r\nreturn 0;\r\n}\r\nstatic int tbf_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\r\nstruct Qdisc **old)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nif (new == NULL)\r\nnew = &noop_qdisc;\r\nsch_tree_lock(sch);\r\n*old = q->qdisc;\r\nq->qdisc = new;\r\nqdisc_tree_decrease_qlen(*old, (*old)->q.qlen);\r\nqdisc_reset(*old);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct Qdisc *tbf_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nreturn q->qdisc;\r\n}\r\nstatic unsigned long tbf_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 1;\r\n}\r\nstatic void tbf_put(struct Qdisc *sch, unsigned long arg)\r\n{\r\n}\r\nstatic void tbf_walk(struct Qdisc *sch, struct qdisc_walker *walker)\r\n{\r\nif (!walker->stop) {\r\nif (walker->count >= walker->skip)\r\nif (walker->fn(sch, 1, walker) < 0) {\r\nwalker->stop = 1;\r\nreturn;\r\n}\r\nwalker->count++;\r\n}\r\n}\r\nstatic int __init tbf_module_init(void)\r\n{\r\nreturn register_qdisc(&tbf_qdisc_ops);\r\n}\r\nstatic void __exit tbf_module_exit(void)\r\n{\r\nunregister_qdisc(&tbf_qdisc_ops);\r\n}
