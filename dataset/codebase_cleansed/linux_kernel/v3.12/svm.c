static inline void mark_all_dirty(struct vmcb *vmcb)\r\n{\r\nvmcb->control.clean = 0;\r\n}\r\nstatic inline void mark_all_clean(struct vmcb *vmcb)\r\n{\r\nvmcb->control.clean = ((1 << VMCB_DIRTY_MAX) - 1)\r\n& ~VMCB_ALWAYS_DIRTY_MASK;\r\n}\r\nstatic inline void mark_dirty(struct vmcb *vmcb, int bit)\r\n{\r\nvmcb->control.clean &= ~(1 << bit);\r\n}\r\nstatic inline struct vcpu_svm *to_svm(struct kvm_vcpu *vcpu)\r\n{\r\nreturn container_of(vcpu, struct vcpu_svm, vcpu);\r\n}\r\nstatic void recalc_intercepts(struct vcpu_svm *svm)\r\n{\r\nstruct vmcb_control_area *c, *h;\r\nstruct nested_state *g;\r\nmark_dirty(svm->vmcb, VMCB_INTERCEPTS);\r\nif (!is_guest_mode(&svm->vcpu))\r\nreturn;\r\nc = &svm->vmcb->control;\r\nh = &svm->nested.hsave->control;\r\ng = &svm->nested;\r\nc->intercept_cr = h->intercept_cr | g->intercept_cr;\r\nc->intercept_dr = h->intercept_dr | g->intercept_dr;\r\nc->intercept_exceptions = h->intercept_exceptions | g->intercept_exceptions;\r\nc->intercept = h->intercept | g->intercept;\r\n}\r\nstatic inline struct vmcb *get_host_vmcb(struct vcpu_svm *svm)\r\n{\r\nif (is_guest_mode(&svm->vcpu))\r\nreturn svm->nested.hsave;\r\nelse\r\nreturn svm->vmcb;\r\n}\r\nstatic inline void set_cr_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept_cr |= (1U << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline void clr_cr_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept_cr &= ~(1U << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline bool is_cr_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nreturn vmcb->control.intercept_cr & (1U << bit);\r\n}\r\nstatic inline void set_dr_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept_dr |= (1U << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline void clr_dr_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept_dr &= ~(1U << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline void set_exception_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept_exceptions |= (1U << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline void clr_exception_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept_exceptions &= ~(1U << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline void set_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept |= (1ULL << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline void clr_intercept(struct vcpu_svm *svm, int bit)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(svm);\r\nvmcb->control.intercept &= ~(1ULL << bit);\r\nrecalc_intercepts(svm);\r\n}\r\nstatic inline void enable_gif(struct vcpu_svm *svm)\r\n{\r\nsvm->vcpu.arch.hflags |= HF_GIF_MASK;\r\n}\r\nstatic inline void disable_gif(struct vcpu_svm *svm)\r\n{\r\nsvm->vcpu.arch.hflags &= ~HF_GIF_MASK;\r\n}\r\nstatic inline bool gif_set(struct vcpu_svm *svm)\r\n{\r\nreturn !!(svm->vcpu.arch.hflags & HF_GIF_MASK);\r\n}\r\nstatic u32 svm_msrpm_offset(u32 msr)\r\n{\r\nu32 offset;\r\nint i;\r\nfor (i = 0; i < NUM_MSR_MAPS; i++) {\r\nif (msr < msrpm_ranges[i] ||\r\nmsr >= msrpm_ranges[i] + MSRS_IN_RANGE)\r\ncontinue;\r\noffset = (msr - msrpm_ranges[i]) / 4;\r\noffset += (i * MSRS_RANGE_SIZE);\r\nreturn offset / 4;\r\n}\r\nreturn MSR_INVALID;\r\n}\r\nstatic inline void clgi(void)\r\n{\r\nasm volatile (__ex(SVM_CLGI));\r\n}\r\nstatic inline void stgi(void)\r\n{\r\nasm volatile (__ex(SVM_STGI));\r\n}\r\nstatic inline void invlpga(unsigned long addr, u32 asid)\r\n{\r\nasm volatile (__ex(SVM_INVLPGA) : : "a"(addr), "c"(asid));\r\n}\r\nstatic int get_npt_level(void)\r\n{\r\n#ifdef CONFIG_X86_64\r\nreturn PT64_ROOT_LEVEL;\r\n#else\r\nreturn PT32E_ROOT_LEVEL;\r\n#endif\r\n}\r\nstatic void svm_set_efer(struct kvm_vcpu *vcpu, u64 efer)\r\n{\r\nvcpu->arch.efer = efer;\r\nif (!npt_enabled && !(efer & EFER_LMA))\r\nefer &= ~EFER_LME;\r\nto_svm(vcpu)->vmcb->save.efer = efer | EFER_SVME;\r\nmark_dirty(to_svm(vcpu)->vmcb, VMCB_CR);\r\n}\r\nstatic int is_external_interrupt(u32 info)\r\n{\r\ninfo &= SVM_EVTINJ_TYPE_MASK | SVM_EVTINJ_VALID;\r\nreturn info == (SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR);\r\n}\r\nstatic u32 svm_get_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu32 ret = 0;\r\nif (svm->vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK)\r\nret |= KVM_X86_SHADOW_INT_STI | KVM_X86_SHADOW_INT_MOV_SS;\r\nreturn ret & mask;\r\n}\r\nstatic void svm_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (mask == 0)\r\nsvm->vmcb->control.int_state &= ~SVM_INTERRUPT_SHADOW_MASK;\r\nelse\r\nsvm->vmcb->control.int_state |= SVM_INTERRUPT_SHADOW_MASK;\r\n}\r\nstatic void skip_emulated_instruction(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (svm->vmcb->control.next_rip != 0)\r\nsvm->next_rip = svm->vmcb->control.next_rip;\r\nif (!svm->next_rip) {\r\nif (emulate_instruction(vcpu, EMULTYPE_SKIP) !=\r\nEMULATE_DONE)\r\nprintk(KERN_DEBUG "%s: NOP\n", __func__);\r\nreturn;\r\n}\r\nif (svm->next_rip - kvm_rip_read(vcpu) > MAX_INST_SIZE)\r\nprintk(KERN_ERR "%s: ip 0x%lx next 0x%llx\n",\r\n__func__, kvm_rip_read(vcpu), svm->next_rip);\r\nkvm_rip_write(vcpu, svm->next_rip);\r\nsvm_set_interrupt_shadow(vcpu, 0);\r\n}\r\nstatic void svm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr,\r\nbool has_error_code, u32 error_code,\r\nbool reinject)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (!reinject &&\r\nnested_svm_check_exception(svm, nr, has_error_code, error_code))\r\nreturn;\r\nif (nr == BP_VECTOR && !static_cpu_has(X86_FEATURE_NRIPS)) {\r\nunsigned long rip, old_rip = kvm_rip_read(&svm->vcpu);\r\nskip_emulated_instruction(&svm->vcpu);\r\nrip = kvm_rip_read(&svm->vcpu);\r\nsvm->int3_rip = rip + svm->vmcb->save.cs.base;\r\nsvm->int3_injected = rip - old_rip;\r\n}\r\nsvm->vmcb->control.event_inj = nr\r\n| SVM_EVTINJ_VALID\r\n| (has_error_code ? SVM_EVTINJ_VALID_ERR : 0)\r\n| SVM_EVTINJ_TYPE_EXEPT;\r\nsvm->vmcb->control.event_inj_err = error_code;\r\n}\r\nstatic void svm_init_erratum_383(void)\r\n{\r\nu32 low, high;\r\nint err;\r\nu64 val;\r\nif (!static_cpu_has_bug(X86_BUG_AMD_TLB_MMATCH))\r\nreturn;\r\nval = native_read_msr_safe(MSR_AMD64_DC_CFG, &err);\r\nif (err)\r\nreturn;\r\nval |= (1ULL << 47);\r\nlow = lower_32_bits(val);\r\nhigh = upper_32_bits(val);\r\nnative_write_msr_safe(MSR_AMD64_DC_CFG, low, high);\r\nerratum_383_found = true;\r\n}\r\nstatic void svm_init_osvw(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->arch.osvw.length = (osvw_len >= 3) ? (osvw_len) : 3;\r\nvcpu->arch.osvw.status = osvw_status & ~(6ULL);\r\nif (osvw_len == 0 && boot_cpu_data.x86 == 0x10)\r\nvcpu->arch.osvw.status |= 1;\r\n}\r\nstatic int has_svm(void)\r\n{\r\nconst char *msg;\r\nif (!cpu_has_svm(&msg)) {\r\nprintk(KERN_INFO "has_svm: %s\n", msg);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void svm_hardware_disable(void *garbage)\r\n{\r\nif (static_cpu_has(X86_FEATURE_TSCRATEMSR))\r\nwrmsrl(MSR_AMD64_TSC_RATIO, TSC_RATIO_DEFAULT);\r\ncpu_svm_disable();\r\namd_pmu_disable_virt();\r\n}\r\nstatic int svm_hardware_enable(void *garbage)\r\n{\r\nstruct svm_cpu_data *sd;\r\nuint64_t efer;\r\nstruct desc_ptr gdt_descr;\r\nstruct desc_struct *gdt;\r\nint me = raw_smp_processor_id();\r\nrdmsrl(MSR_EFER, efer);\r\nif (efer & EFER_SVME)\r\nreturn -EBUSY;\r\nif (!has_svm()) {\r\npr_err("%s: err EOPNOTSUPP on %d\n", __func__, me);\r\nreturn -EINVAL;\r\n}\r\nsd = per_cpu(svm_data, me);\r\nif (!sd) {\r\npr_err("%s: svm_data is NULL on %d\n", __func__, me);\r\nreturn -EINVAL;\r\n}\r\nsd->asid_generation = 1;\r\nsd->max_asid = cpuid_ebx(SVM_CPUID_FUNC) - 1;\r\nsd->next_asid = sd->max_asid + 1;\r\nnative_store_gdt(&gdt_descr);\r\ngdt = (struct desc_struct *)gdt_descr.address;\r\nsd->tss_desc = (struct kvm_ldttss_desc *)(gdt + GDT_ENTRY_TSS);\r\nwrmsrl(MSR_EFER, efer | EFER_SVME);\r\nwrmsrl(MSR_VM_HSAVE_PA, page_to_pfn(sd->save_area) << PAGE_SHIFT);\r\nif (static_cpu_has(X86_FEATURE_TSCRATEMSR)) {\r\nwrmsrl(MSR_AMD64_TSC_RATIO, TSC_RATIO_DEFAULT);\r\n__get_cpu_var(current_tsc_ratio) = TSC_RATIO_DEFAULT;\r\n}\r\nif (cpu_has(&boot_cpu_data, X86_FEATURE_OSVW)) {\r\nuint64_t len, status = 0;\r\nint err;\r\nlen = native_read_msr_safe(MSR_AMD64_OSVW_ID_LENGTH, &err);\r\nif (!err)\r\nstatus = native_read_msr_safe(MSR_AMD64_OSVW_STATUS,\r\n&err);\r\nif (err)\r\nosvw_status = osvw_len = 0;\r\nelse {\r\nif (len < osvw_len)\r\nosvw_len = len;\r\nosvw_status |= status;\r\nosvw_status &= (1ULL << osvw_len) - 1;\r\n}\r\n} else\r\nosvw_status = osvw_len = 0;\r\nsvm_init_erratum_383();\r\namd_pmu_enable_virt();\r\nreturn 0;\r\n}\r\nstatic void svm_cpu_uninit(int cpu)\r\n{\r\nstruct svm_cpu_data *sd = per_cpu(svm_data, raw_smp_processor_id());\r\nif (!sd)\r\nreturn;\r\nper_cpu(svm_data, raw_smp_processor_id()) = NULL;\r\n__free_page(sd->save_area);\r\nkfree(sd);\r\n}\r\nstatic int svm_cpu_init(int cpu)\r\n{\r\nstruct svm_cpu_data *sd;\r\nint r;\r\nsd = kzalloc(sizeof(struct svm_cpu_data), GFP_KERNEL);\r\nif (!sd)\r\nreturn -ENOMEM;\r\nsd->cpu = cpu;\r\nsd->save_area = alloc_page(GFP_KERNEL);\r\nr = -ENOMEM;\r\nif (!sd->save_area)\r\ngoto err_1;\r\nper_cpu(svm_data, cpu) = sd;\r\nreturn 0;\r\nerr_1:\r\nkfree(sd);\r\nreturn r;\r\n}\r\nstatic bool valid_msr_intercept(u32 index)\r\n{\r\nint i;\r\nfor (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++)\r\nif (direct_access_msrs[i].index == index)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void set_msr_interception(u32 *msrpm, unsigned msr,\r\nint read, int write)\r\n{\r\nu8 bit_read, bit_write;\r\nunsigned long tmp;\r\nu32 offset;\r\nWARN_ON(!valid_msr_intercept(msr));\r\noffset = svm_msrpm_offset(msr);\r\nbit_read = 2 * (msr & 0x0f);\r\nbit_write = 2 * (msr & 0x0f) + 1;\r\ntmp = msrpm[offset];\r\nBUG_ON(offset == MSR_INVALID);\r\nread ? clear_bit(bit_read, &tmp) : set_bit(bit_read, &tmp);\r\nwrite ? clear_bit(bit_write, &tmp) : set_bit(bit_write, &tmp);\r\nmsrpm[offset] = tmp;\r\n}\r\nstatic void svm_vcpu_init_msrpm(u32 *msrpm)\r\n{\r\nint i;\r\nmemset(msrpm, 0xff, PAGE_SIZE * (1 << MSRPM_ALLOC_ORDER));\r\nfor (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++) {\r\nif (!direct_access_msrs[i].always)\r\ncontinue;\r\nset_msr_interception(msrpm, direct_access_msrs[i].index, 1, 1);\r\n}\r\n}\r\nstatic void add_msr_offset(u32 offset)\r\n{\r\nint i;\r\nfor (i = 0; i < MSRPM_OFFSETS; ++i) {\r\nif (msrpm_offsets[i] == offset)\r\nreturn;\r\nif (msrpm_offsets[i] != MSR_INVALID)\r\ncontinue;\r\nmsrpm_offsets[i] = offset;\r\nreturn;\r\n}\r\nBUG();\r\n}\r\nstatic void init_msrpm_offsets(void)\r\n{\r\nint i;\r\nmemset(msrpm_offsets, 0xff, sizeof(msrpm_offsets));\r\nfor (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++) {\r\nu32 offset;\r\noffset = svm_msrpm_offset(direct_access_msrs[i].index);\r\nBUG_ON(offset == MSR_INVALID);\r\nadd_msr_offset(offset);\r\n}\r\n}\r\nstatic void svm_enable_lbrv(struct vcpu_svm *svm)\r\n{\r\nu32 *msrpm = svm->msrpm;\r\nsvm->vmcb->control.lbr_ctl = 1;\r\nset_msr_interception(msrpm, MSR_IA32_LASTBRANCHFROMIP, 1, 1);\r\nset_msr_interception(msrpm, MSR_IA32_LASTBRANCHTOIP, 1, 1);\r\nset_msr_interception(msrpm, MSR_IA32_LASTINTFROMIP, 1, 1);\r\nset_msr_interception(msrpm, MSR_IA32_LASTINTTOIP, 1, 1);\r\n}\r\nstatic void svm_disable_lbrv(struct vcpu_svm *svm)\r\n{\r\nu32 *msrpm = svm->msrpm;\r\nsvm->vmcb->control.lbr_ctl = 0;\r\nset_msr_interception(msrpm, MSR_IA32_LASTBRANCHFROMIP, 0, 0);\r\nset_msr_interception(msrpm, MSR_IA32_LASTBRANCHTOIP, 0, 0);\r\nset_msr_interception(msrpm, MSR_IA32_LASTINTFROMIP, 0, 0);\r\nset_msr_interception(msrpm, MSR_IA32_LASTINTTOIP, 0, 0);\r\n}\r\nstatic __init int svm_hardware_setup(void)\r\n{\r\nint cpu;\r\nstruct page *iopm_pages;\r\nvoid *iopm_va;\r\nint r;\r\niopm_pages = alloc_pages(GFP_KERNEL, IOPM_ALLOC_ORDER);\r\nif (!iopm_pages)\r\nreturn -ENOMEM;\r\niopm_va = page_address(iopm_pages);\r\nmemset(iopm_va, 0xff, PAGE_SIZE * (1 << IOPM_ALLOC_ORDER));\r\niopm_base = page_to_pfn(iopm_pages) << PAGE_SHIFT;\r\ninit_msrpm_offsets();\r\nif (boot_cpu_has(X86_FEATURE_NX))\r\nkvm_enable_efer_bits(EFER_NX);\r\nif (boot_cpu_has(X86_FEATURE_FXSR_OPT))\r\nkvm_enable_efer_bits(EFER_FFXSR);\r\nif (boot_cpu_has(X86_FEATURE_TSCRATEMSR)) {\r\nu64 max;\r\nkvm_has_tsc_control = true;\r\nmax = min(0x7fffffffULL, __scale_tsc(tsc_khz, TSC_RATIO_MAX));\r\nkvm_max_guest_tsc_khz = max;\r\n}\r\nif (nested) {\r\nprintk(KERN_INFO "kvm: Nested Virtualization enabled\n");\r\nkvm_enable_efer_bits(EFER_SVME | EFER_LMSLE);\r\n}\r\nfor_each_possible_cpu(cpu) {\r\nr = svm_cpu_init(cpu);\r\nif (r)\r\ngoto err;\r\n}\r\nif (!boot_cpu_has(X86_FEATURE_NPT))\r\nnpt_enabled = false;\r\nif (npt_enabled && !npt) {\r\nprintk(KERN_INFO "kvm: Nested Paging disabled\n");\r\nnpt_enabled = false;\r\n}\r\nif (npt_enabled) {\r\nprintk(KERN_INFO "kvm: Nested Paging enabled\n");\r\nkvm_enable_tdp();\r\n} else\r\nkvm_disable_tdp();\r\nreturn 0;\r\nerr:\r\n__free_pages(iopm_pages, IOPM_ALLOC_ORDER);\r\niopm_base = 0;\r\nreturn r;\r\n}\r\nstatic __exit void svm_hardware_unsetup(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\nsvm_cpu_uninit(cpu);\r\n__free_pages(pfn_to_page(iopm_base >> PAGE_SHIFT), IOPM_ALLOC_ORDER);\r\niopm_base = 0;\r\n}\r\nstatic void init_seg(struct vmcb_seg *seg)\r\n{\r\nseg->selector = 0;\r\nseg->attrib = SVM_SELECTOR_P_MASK | SVM_SELECTOR_S_MASK |\r\nSVM_SELECTOR_WRITE_MASK;\r\nseg->limit = 0xffff;\r\nseg->base = 0;\r\n}\r\nstatic void init_sys_seg(struct vmcb_seg *seg, uint32_t type)\r\n{\r\nseg->selector = 0;\r\nseg->attrib = SVM_SELECTOR_P_MASK | type;\r\nseg->limit = 0xffff;\r\nseg->base = 0;\r\n}\r\nstatic u64 __scale_tsc(u64 ratio, u64 tsc)\r\n{\r\nu64 mult, frac, _tsc;\r\nmult = ratio >> 32;\r\nfrac = ratio & ((1ULL << 32) - 1);\r\n_tsc = tsc;\r\n_tsc *= mult;\r\n_tsc += (tsc >> 32) * frac;\r\n_tsc += ((tsc & ((1ULL << 32) - 1)) * frac) >> 32;\r\nreturn _tsc;\r\n}\r\nstatic u64 svm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu64 _tsc = tsc;\r\nif (svm->tsc_ratio != TSC_RATIO_DEFAULT)\r\n_tsc = __scale_tsc(svm->tsc_ratio, tsc);\r\nreturn _tsc;\r\n}\r\nstatic void svm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu64 ratio;\r\nu64 khz;\r\nif (!scale) {\r\nsvm->tsc_ratio = TSC_RATIO_DEFAULT;\r\nreturn;\r\n}\r\nif (!boot_cpu_has(X86_FEATURE_TSCRATEMSR)) {\r\nif (user_tsc_khz > tsc_khz) {\r\nvcpu->arch.tsc_catchup = 1;\r\nvcpu->arch.tsc_always_catchup = 1;\r\n} else\r\nWARN(1, "user requested TSC rate below hardware speed\n");\r\nreturn;\r\n}\r\nkhz = user_tsc_khz;\r\nratio = khz << 32;\r\ndo_div(ratio, tsc_khz);\r\nif (ratio == 0 || ratio & TSC_RATIO_RSVD) {\r\nWARN_ONCE(1, "Invalid TSC ratio - virtual-tsc-khz=%u\n",\r\nuser_tsc_khz);\r\nreturn;\r\n}\r\nsvm->tsc_ratio = ratio;\r\n}\r\nstatic u64 svm_read_tsc_offset(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nreturn svm->vmcb->control.tsc_offset;\r\n}\r\nstatic void svm_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu64 g_tsc_offset = 0;\r\nif (is_guest_mode(vcpu)) {\r\ng_tsc_offset = svm->vmcb->control.tsc_offset -\r\nsvm->nested.hsave->control.tsc_offset;\r\nsvm->nested.hsave->control.tsc_offset = offset;\r\n} else\r\ntrace_kvm_write_tsc_offset(vcpu->vcpu_id,\r\nsvm->vmcb->control.tsc_offset,\r\noffset);\r\nsvm->vmcb->control.tsc_offset = offset + g_tsc_offset;\r\nmark_dirty(svm->vmcb, VMCB_INTERCEPTS);\r\n}\r\nstatic void svm_adjust_tsc_offset(struct kvm_vcpu *vcpu, s64 adjustment, bool host)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nWARN_ON(adjustment < 0);\r\nif (host)\r\nadjustment = svm_scale_tsc(vcpu, adjustment);\r\nsvm->vmcb->control.tsc_offset += adjustment;\r\nif (is_guest_mode(vcpu))\r\nsvm->nested.hsave->control.tsc_offset += adjustment;\r\nelse\r\ntrace_kvm_write_tsc_offset(vcpu->vcpu_id,\r\nsvm->vmcb->control.tsc_offset - adjustment,\r\nsvm->vmcb->control.tsc_offset);\r\nmark_dirty(svm->vmcb, VMCB_INTERCEPTS);\r\n}\r\nstatic u64 svm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)\r\n{\r\nu64 tsc;\r\ntsc = svm_scale_tsc(vcpu, native_read_tsc());\r\nreturn target_tsc - tsc;\r\n}\r\nstatic void init_vmcb(struct vcpu_svm *svm)\r\n{\r\nstruct vmcb_control_area *control = &svm->vmcb->control;\r\nstruct vmcb_save_area *save = &svm->vmcb->save;\r\nsvm->vcpu.fpu_active = 1;\r\nsvm->vcpu.arch.hflags = 0;\r\nset_cr_intercept(svm, INTERCEPT_CR0_READ);\r\nset_cr_intercept(svm, INTERCEPT_CR3_READ);\r\nset_cr_intercept(svm, INTERCEPT_CR4_READ);\r\nset_cr_intercept(svm, INTERCEPT_CR0_WRITE);\r\nset_cr_intercept(svm, INTERCEPT_CR3_WRITE);\r\nset_cr_intercept(svm, INTERCEPT_CR4_WRITE);\r\nset_cr_intercept(svm, INTERCEPT_CR8_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR0_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR1_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR2_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR3_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR4_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR5_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR6_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR7_READ);\r\nset_dr_intercept(svm, INTERCEPT_DR0_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR1_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR2_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR3_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR4_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR5_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR6_WRITE);\r\nset_dr_intercept(svm, INTERCEPT_DR7_WRITE);\r\nset_exception_intercept(svm, PF_VECTOR);\r\nset_exception_intercept(svm, UD_VECTOR);\r\nset_exception_intercept(svm, MC_VECTOR);\r\nset_intercept(svm, INTERCEPT_INTR);\r\nset_intercept(svm, INTERCEPT_NMI);\r\nset_intercept(svm, INTERCEPT_SMI);\r\nset_intercept(svm, INTERCEPT_SELECTIVE_CR0);\r\nset_intercept(svm, INTERCEPT_RDPMC);\r\nset_intercept(svm, INTERCEPT_CPUID);\r\nset_intercept(svm, INTERCEPT_INVD);\r\nset_intercept(svm, INTERCEPT_HLT);\r\nset_intercept(svm, INTERCEPT_INVLPG);\r\nset_intercept(svm, INTERCEPT_INVLPGA);\r\nset_intercept(svm, INTERCEPT_IOIO_PROT);\r\nset_intercept(svm, INTERCEPT_MSR_PROT);\r\nset_intercept(svm, INTERCEPT_TASK_SWITCH);\r\nset_intercept(svm, INTERCEPT_SHUTDOWN);\r\nset_intercept(svm, INTERCEPT_VMRUN);\r\nset_intercept(svm, INTERCEPT_VMMCALL);\r\nset_intercept(svm, INTERCEPT_VMLOAD);\r\nset_intercept(svm, INTERCEPT_VMSAVE);\r\nset_intercept(svm, INTERCEPT_STGI);\r\nset_intercept(svm, INTERCEPT_CLGI);\r\nset_intercept(svm, INTERCEPT_SKINIT);\r\nset_intercept(svm, INTERCEPT_WBINVD);\r\nset_intercept(svm, INTERCEPT_MONITOR);\r\nset_intercept(svm, INTERCEPT_MWAIT);\r\nset_intercept(svm, INTERCEPT_XSETBV);\r\ncontrol->iopm_base_pa = iopm_base;\r\ncontrol->msrpm_base_pa = __pa(svm->msrpm);\r\ncontrol->int_ctl = V_INTR_MASKING_MASK;\r\ninit_seg(&save->es);\r\ninit_seg(&save->ss);\r\ninit_seg(&save->ds);\r\ninit_seg(&save->fs);\r\ninit_seg(&save->gs);\r\nsave->cs.selector = 0xf000;\r\nsave->cs.base = 0xffff0000;\r\nsave->cs.attrib = SVM_SELECTOR_READ_MASK | SVM_SELECTOR_P_MASK |\r\nSVM_SELECTOR_S_MASK | SVM_SELECTOR_CODE_MASK;\r\nsave->cs.limit = 0xffff;\r\nsave->gdtr.limit = 0xffff;\r\nsave->idtr.limit = 0xffff;\r\ninit_sys_seg(&save->ldtr, SEG_TYPE_LDT);\r\ninit_sys_seg(&save->tr, SEG_TYPE_BUSY_TSS16);\r\nsvm_set_efer(&svm->vcpu, 0);\r\nsave->dr6 = 0xffff0ff0;\r\nkvm_set_rflags(&svm->vcpu, 2);\r\nsave->rip = 0x0000fff0;\r\nsvm->vcpu.arch.regs[VCPU_REGS_RIP] = save->rip;\r\nsvm->vcpu.arch.cr0 = 0;\r\n(void)kvm_set_cr0(&svm->vcpu, X86_CR0_NW | X86_CR0_CD | X86_CR0_ET);\r\nsave->cr4 = X86_CR4_PAE;\r\nif (npt_enabled) {\r\ncontrol->nested_ctl = 1;\r\nclr_intercept(svm, INTERCEPT_INVLPG);\r\nclr_exception_intercept(svm, PF_VECTOR);\r\nclr_cr_intercept(svm, INTERCEPT_CR3_READ);\r\nclr_cr_intercept(svm, INTERCEPT_CR3_WRITE);\r\nsave->g_pat = 0x0007040600070406ULL;\r\nsave->cr3 = 0;\r\nsave->cr4 = 0;\r\n}\r\nsvm->asid_generation = 0;\r\nsvm->nested.vmcb = 0;\r\nsvm->vcpu.arch.hflags = 0;\r\nif (boot_cpu_has(X86_FEATURE_PAUSEFILTER)) {\r\ncontrol->pause_filter_count = 3000;\r\nset_intercept(svm, INTERCEPT_PAUSE);\r\n}\r\nmark_all_dirty(svm->vmcb);\r\nenable_gif(svm);\r\n}\r\nstatic void svm_vcpu_reset(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu32 dummy;\r\nu32 eax = 1;\r\ninit_vmcb(svm);\r\nkvm_cpuid(vcpu, &eax, &dummy, &dummy, &dummy);\r\nkvm_register_write(vcpu, VCPU_REGS_RDX, eax);\r\n}\r\nstatic struct kvm_vcpu *svm_create_vcpu(struct kvm *kvm, unsigned int id)\r\n{\r\nstruct vcpu_svm *svm;\r\nstruct page *page;\r\nstruct page *msrpm_pages;\r\nstruct page *hsave_page;\r\nstruct page *nested_msrpm_pages;\r\nint err;\r\nsvm = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\r\nif (!svm) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nsvm->tsc_ratio = TSC_RATIO_DEFAULT;\r\nerr = kvm_vcpu_init(&svm->vcpu, kvm, id);\r\nif (err)\r\ngoto free_svm;\r\nerr = -ENOMEM;\r\npage = alloc_page(GFP_KERNEL);\r\nif (!page)\r\ngoto uninit;\r\nmsrpm_pages = alloc_pages(GFP_KERNEL, MSRPM_ALLOC_ORDER);\r\nif (!msrpm_pages)\r\ngoto free_page1;\r\nnested_msrpm_pages = alloc_pages(GFP_KERNEL, MSRPM_ALLOC_ORDER);\r\nif (!nested_msrpm_pages)\r\ngoto free_page2;\r\nhsave_page = alloc_page(GFP_KERNEL);\r\nif (!hsave_page)\r\ngoto free_page3;\r\nsvm->nested.hsave = page_address(hsave_page);\r\nsvm->msrpm = page_address(msrpm_pages);\r\nsvm_vcpu_init_msrpm(svm->msrpm);\r\nsvm->nested.msrpm = page_address(nested_msrpm_pages);\r\nsvm_vcpu_init_msrpm(svm->nested.msrpm);\r\nsvm->vmcb = page_address(page);\r\nclear_page(svm->vmcb);\r\nsvm->vmcb_pa = page_to_pfn(page) << PAGE_SHIFT;\r\nsvm->asid_generation = 0;\r\ninit_vmcb(svm);\r\nsvm->vcpu.arch.apic_base = 0xfee00000 | MSR_IA32_APICBASE_ENABLE;\r\nif (kvm_vcpu_is_bsp(&svm->vcpu))\r\nsvm->vcpu.arch.apic_base |= MSR_IA32_APICBASE_BSP;\r\nsvm_init_osvw(&svm->vcpu);\r\nreturn &svm->vcpu;\r\nfree_page3:\r\n__free_pages(nested_msrpm_pages, MSRPM_ALLOC_ORDER);\r\nfree_page2:\r\n__free_pages(msrpm_pages, MSRPM_ALLOC_ORDER);\r\nfree_page1:\r\n__free_page(page);\r\nuninit:\r\nkvm_vcpu_uninit(&svm->vcpu);\r\nfree_svm:\r\nkmem_cache_free(kvm_vcpu_cache, svm);\r\nout:\r\nreturn ERR_PTR(err);\r\n}\r\nstatic void svm_free_vcpu(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\n__free_page(pfn_to_page(svm->vmcb_pa >> PAGE_SHIFT));\r\n__free_pages(virt_to_page(svm->msrpm), MSRPM_ALLOC_ORDER);\r\n__free_page(virt_to_page(svm->nested.hsave));\r\n__free_pages(virt_to_page(svm->nested.msrpm), MSRPM_ALLOC_ORDER);\r\nkvm_vcpu_uninit(vcpu);\r\nkmem_cache_free(kvm_vcpu_cache, svm);\r\n}\r\nstatic void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nint i;\r\nif (unlikely(cpu != vcpu->cpu)) {\r\nsvm->asid_generation = 0;\r\nmark_all_dirty(svm->vmcb);\r\n}\r\n#ifdef CONFIG_X86_64\r\nrdmsrl(MSR_GS_BASE, to_svm(vcpu)->host.gs_base);\r\n#endif\r\nsavesegment(fs, svm->host.fs);\r\nsavesegment(gs, svm->host.gs);\r\nsvm->host.ldt = kvm_read_ldt();\r\nfor (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)\r\nrdmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);\r\nif (static_cpu_has(X86_FEATURE_TSCRATEMSR) &&\r\nsvm->tsc_ratio != __get_cpu_var(current_tsc_ratio)) {\r\n__get_cpu_var(current_tsc_ratio) = svm->tsc_ratio;\r\nwrmsrl(MSR_AMD64_TSC_RATIO, svm->tsc_ratio);\r\n}\r\n}\r\nstatic void svm_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nint i;\r\n++vcpu->stat.host_state_reload;\r\nkvm_load_ldt(svm->host.ldt);\r\n#ifdef CONFIG_X86_64\r\nloadsegment(fs, svm->host.fs);\r\nwrmsrl(MSR_KERNEL_GS_BASE, current->thread.gs);\r\nload_gs_index(svm->host.gs);\r\n#else\r\n#ifdef CONFIG_X86_32_LAZY_GS\r\nloadsegment(gs, svm->host.gs);\r\n#endif\r\n#endif\r\nfor (i = 0; i < NR_HOST_SAVE_USER_MSRS; i++)\r\nwrmsrl(host_save_user_msrs[i], svm->host_user_msrs[i]);\r\n}\r\nstatic void svm_update_cpl(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nint cpl;\r\nif (!is_protmode(vcpu))\r\ncpl = 0;\r\nelse if (svm->vmcb->save.rflags & X86_EFLAGS_VM)\r\ncpl = 3;\r\nelse\r\ncpl = svm->vmcb->save.cs.selector & 0x3;\r\nsvm->vmcb->save.cpl = cpl;\r\n}\r\nstatic unsigned long svm_get_rflags(struct kvm_vcpu *vcpu)\r\n{\r\nreturn to_svm(vcpu)->vmcb->save.rflags;\r\n}\r\nstatic void svm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\r\n{\r\nunsigned long old_rflags = to_svm(vcpu)->vmcb->save.rflags;\r\nto_svm(vcpu)->vmcb->save.rflags = rflags;\r\nif ((old_rflags ^ rflags) & X86_EFLAGS_VM)\r\nsvm_update_cpl(vcpu);\r\n}\r\nstatic void svm_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)\r\n{\r\nswitch (reg) {\r\ncase VCPU_EXREG_PDPTR:\r\nBUG_ON(!npt_enabled);\r\nload_pdptrs(vcpu, vcpu->arch.walk_mmu, kvm_read_cr3(vcpu));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic void svm_set_vintr(struct vcpu_svm *svm)\r\n{\r\nset_intercept(svm, INTERCEPT_VINTR);\r\n}\r\nstatic void svm_clear_vintr(struct vcpu_svm *svm)\r\n{\r\nclr_intercept(svm, INTERCEPT_VINTR);\r\n}\r\nstatic struct vmcb_seg *svm_seg(struct kvm_vcpu *vcpu, int seg)\r\n{\r\nstruct vmcb_save_area *save = &to_svm(vcpu)->vmcb->save;\r\nswitch (seg) {\r\ncase VCPU_SREG_CS: return &save->cs;\r\ncase VCPU_SREG_DS: return &save->ds;\r\ncase VCPU_SREG_ES: return &save->es;\r\ncase VCPU_SREG_FS: return &save->fs;\r\ncase VCPU_SREG_GS: return &save->gs;\r\ncase VCPU_SREG_SS: return &save->ss;\r\ncase VCPU_SREG_TR: return &save->tr;\r\ncase VCPU_SREG_LDTR: return &save->ldtr;\r\n}\r\nBUG();\r\nreturn NULL;\r\n}\r\nstatic u64 svm_get_segment_base(struct kvm_vcpu *vcpu, int seg)\r\n{\r\nstruct vmcb_seg *s = svm_seg(vcpu, seg);\r\nreturn s->base;\r\n}\r\nstatic void svm_get_segment(struct kvm_vcpu *vcpu,\r\nstruct kvm_segment *var, int seg)\r\n{\r\nstruct vmcb_seg *s = svm_seg(vcpu, seg);\r\nvar->base = s->base;\r\nvar->limit = s->limit;\r\nvar->selector = s->selector;\r\nvar->type = s->attrib & SVM_SELECTOR_TYPE_MASK;\r\nvar->s = (s->attrib >> SVM_SELECTOR_S_SHIFT) & 1;\r\nvar->dpl = (s->attrib >> SVM_SELECTOR_DPL_SHIFT) & 3;\r\nvar->present = (s->attrib >> SVM_SELECTOR_P_SHIFT) & 1;\r\nvar->avl = (s->attrib >> SVM_SELECTOR_AVL_SHIFT) & 1;\r\nvar->l = (s->attrib >> SVM_SELECTOR_L_SHIFT) & 1;\r\nvar->db = (s->attrib >> SVM_SELECTOR_DB_SHIFT) & 1;\r\nvar->g = (s->attrib >> SVM_SELECTOR_G_SHIFT) & 1;\r\nvar->unusable = !var->present || (var->type == 0);\r\nswitch (seg) {\r\ncase VCPU_SREG_CS:\r\nvar->g = s->limit > 0xfffff;\r\nbreak;\r\ncase VCPU_SREG_TR:\r\nvar->type |= 0x2;\r\nbreak;\r\ncase VCPU_SREG_DS:\r\ncase VCPU_SREG_ES:\r\ncase VCPU_SREG_FS:\r\ncase VCPU_SREG_GS:\r\nif (!var->unusable)\r\nvar->type |= 0x1;\r\nbreak;\r\ncase VCPU_SREG_SS:\r\nif (var->unusable)\r\nvar->db = 0;\r\nbreak;\r\n}\r\n}\r\nstatic int svm_get_cpl(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vmcb_save_area *save = &to_svm(vcpu)->vmcb->save;\r\nreturn save->cpl;\r\n}\r\nstatic void svm_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\ndt->size = svm->vmcb->save.idtr.limit;\r\ndt->address = svm->vmcb->save.idtr.base;\r\n}\r\nstatic void svm_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->save.idtr.limit = dt->size;\r\nsvm->vmcb->save.idtr.base = dt->address ;\r\nmark_dirty(svm->vmcb, VMCB_DT);\r\n}\r\nstatic void svm_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\ndt->size = svm->vmcb->save.gdtr.limit;\r\ndt->address = svm->vmcb->save.gdtr.base;\r\n}\r\nstatic void svm_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->save.gdtr.limit = dt->size;\r\nsvm->vmcb->save.gdtr.base = dt->address ;\r\nmark_dirty(svm->vmcb, VMCB_DT);\r\n}\r\nstatic void svm_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic void svm_decache_cr3(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic void svm_decache_cr4_guest_bits(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic void update_cr0_intercept(struct vcpu_svm *svm)\r\n{\r\nulong gcr0 = svm->vcpu.arch.cr0;\r\nu64 *hcr0 = &svm->vmcb->save.cr0;\r\nif (!svm->vcpu.fpu_active)\r\n*hcr0 |= SVM_CR0_SELECTIVE_MASK;\r\nelse\r\n*hcr0 = (*hcr0 & ~SVM_CR0_SELECTIVE_MASK)\r\n| (gcr0 & SVM_CR0_SELECTIVE_MASK);\r\nmark_dirty(svm->vmcb, VMCB_CR);\r\nif (gcr0 == *hcr0 && svm->vcpu.fpu_active) {\r\nclr_cr_intercept(svm, INTERCEPT_CR0_READ);\r\nclr_cr_intercept(svm, INTERCEPT_CR0_WRITE);\r\n} else {\r\nset_cr_intercept(svm, INTERCEPT_CR0_READ);\r\nset_cr_intercept(svm, INTERCEPT_CR0_WRITE);\r\n}\r\n}\r\nstatic void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\n#ifdef CONFIG_X86_64\r\nif (vcpu->arch.efer & EFER_LME) {\r\nif (!is_paging(vcpu) && (cr0 & X86_CR0_PG)) {\r\nvcpu->arch.efer |= EFER_LMA;\r\nsvm->vmcb->save.efer |= EFER_LMA | EFER_LME;\r\n}\r\nif (is_paging(vcpu) && !(cr0 & X86_CR0_PG)) {\r\nvcpu->arch.efer &= ~EFER_LMA;\r\nsvm->vmcb->save.efer &= ~(EFER_LMA | EFER_LME);\r\n}\r\n}\r\n#endif\r\nvcpu->arch.cr0 = cr0;\r\nif (!npt_enabled)\r\ncr0 |= X86_CR0_PG | X86_CR0_WP;\r\nif (!vcpu->fpu_active)\r\ncr0 |= X86_CR0_TS;\r\ncr0 &= ~(X86_CR0_CD | X86_CR0_NW);\r\nsvm->vmcb->save.cr0 = cr0;\r\nmark_dirty(svm->vmcb, VMCB_CR);\r\nupdate_cr0_intercept(svm);\r\n}\r\nstatic int svm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\r\n{\r\nunsigned long host_cr4_mce = read_cr4() & X86_CR4_MCE;\r\nunsigned long old_cr4 = to_svm(vcpu)->vmcb->save.cr4;\r\nif (cr4 & X86_CR4_VMXE)\r\nreturn 1;\r\nif (npt_enabled && ((old_cr4 ^ cr4) & X86_CR4_PGE))\r\nsvm_flush_tlb(vcpu);\r\nvcpu->arch.cr4 = cr4;\r\nif (!npt_enabled)\r\ncr4 |= X86_CR4_PAE;\r\ncr4 |= host_cr4_mce;\r\nto_svm(vcpu)->vmcb->save.cr4 = cr4;\r\nmark_dirty(to_svm(vcpu)->vmcb, VMCB_CR);\r\nreturn 0;\r\n}\r\nstatic void svm_set_segment(struct kvm_vcpu *vcpu,\r\nstruct kvm_segment *var, int seg)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nstruct vmcb_seg *s = svm_seg(vcpu, seg);\r\ns->base = var->base;\r\ns->limit = var->limit;\r\ns->selector = var->selector;\r\nif (var->unusable)\r\ns->attrib = 0;\r\nelse {\r\ns->attrib = (var->type & SVM_SELECTOR_TYPE_MASK);\r\ns->attrib |= (var->s & 1) << SVM_SELECTOR_S_SHIFT;\r\ns->attrib |= (var->dpl & 3) << SVM_SELECTOR_DPL_SHIFT;\r\ns->attrib |= (var->present & 1) << SVM_SELECTOR_P_SHIFT;\r\ns->attrib |= (var->avl & 1) << SVM_SELECTOR_AVL_SHIFT;\r\ns->attrib |= (var->l & 1) << SVM_SELECTOR_L_SHIFT;\r\ns->attrib |= (var->db & 1) << SVM_SELECTOR_DB_SHIFT;\r\ns->attrib |= (var->g & 1) << SVM_SELECTOR_G_SHIFT;\r\n}\r\nif (seg == VCPU_SREG_CS)\r\nsvm_update_cpl(vcpu);\r\nmark_dirty(svm->vmcb, VMCB_SEG);\r\n}\r\nstatic void update_db_bp_intercept(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nclr_exception_intercept(svm, DB_VECTOR);\r\nclr_exception_intercept(svm, BP_VECTOR);\r\nif (svm->nmi_singlestep)\r\nset_exception_intercept(svm, DB_VECTOR);\r\nif (vcpu->guest_debug & KVM_GUESTDBG_ENABLE) {\r\nif (vcpu->guest_debug &\r\n(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\r\nset_exception_intercept(svm, DB_VECTOR);\r\nif (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\r\nset_exception_intercept(svm, BP_VECTOR);\r\n} else\r\nvcpu->guest_debug = 0;\r\n}\r\nstatic void new_asid(struct vcpu_svm *svm, struct svm_cpu_data *sd)\r\n{\r\nif (sd->next_asid > sd->max_asid) {\r\n++sd->asid_generation;\r\nsd->next_asid = 1;\r\nsvm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ALL_ASID;\r\n}\r\nsvm->asid_generation = sd->asid_generation;\r\nsvm->vmcb->control.asid = sd->next_asid++;\r\nmark_dirty(svm->vmcb, VMCB_ASID);\r\n}\r\nstatic void svm_set_dr7(struct kvm_vcpu *vcpu, unsigned long value)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->save.dr7 = value;\r\nmark_dirty(svm->vmcb, VMCB_DR);\r\n}\r\nstatic int pf_interception(struct vcpu_svm *svm)\r\n{\r\nu64 fault_address = svm->vmcb->control.exit_info_2;\r\nu32 error_code;\r\nint r = 1;\r\nswitch (svm->apf_reason) {\r\ndefault:\r\nerror_code = svm->vmcb->control.exit_info_1;\r\ntrace_kvm_page_fault(fault_address, error_code);\r\nif (!npt_enabled && kvm_event_needs_reinjection(&svm->vcpu))\r\nkvm_mmu_unprotect_page_virt(&svm->vcpu, fault_address);\r\nr = kvm_mmu_page_fault(&svm->vcpu, fault_address, error_code,\r\nsvm->vmcb->control.insn_bytes,\r\nsvm->vmcb->control.insn_len);\r\nbreak;\r\ncase KVM_PV_REASON_PAGE_NOT_PRESENT:\r\nsvm->apf_reason = 0;\r\nlocal_irq_disable();\r\nkvm_async_pf_task_wait(fault_address);\r\nlocal_irq_enable();\r\nbreak;\r\ncase KVM_PV_REASON_PAGE_READY:\r\nsvm->apf_reason = 0;\r\nlocal_irq_disable();\r\nkvm_async_pf_task_wake(fault_address);\r\nlocal_irq_enable();\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic int db_interception(struct vcpu_svm *svm)\r\n{\r\nstruct kvm_run *kvm_run = svm->vcpu.run;\r\nif (!(svm->vcpu.guest_debug &\r\n(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP)) &&\r\n!svm->nmi_singlestep) {\r\nkvm_queue_exception(&svm->vcpu, DB_VECTOR);\r\nreturn 1;\r\n}\r\nif (svm->nmi_singlestep) {\r\nsvm->nmi_singlestep = false;\r\nif (!(svm->vcpu.guest_debug & KVM_GUESTDBG_SINGLESTEP))\r\nsvm->vmcb->save.rflags &=\r\n~(X86_EFLAGS_TF | X86_EFLAGS_RF);\r\nupdate_db_bp_intercept(&svm->vcpu);\r\n}\r\nif (svm->vcpu.guest_debug &\r\n(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP)) {\r\nkvm_run->exit_reason = KVM_EXIT_DEBUG;\r\nkvm_run->debug.arch.pc =\r\nsvm->vmcb->save.cs.base + svm->vmcb->save.rip;\r\nkvm_run->debug.arch.exception = DB_VECTOR;\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int bp_interception(struct vcpu_svm *svm)\r\n{\r\nstruct kvm_run *kvm_run = svm->vcpu.run;\r\nkvm_run->exit_reason = KVM_EXIT_DEBUG;\r\nkvm_run->debug.arch.pc = svm->vmcb->save.cs.base + svm->vmcb->save.rip;\r\nkvm_run->debug.arch.exception = BP_VECTOR;\r\nreturn 0;\r\n}\r\nstatic int ud_interception(struct vcpu_svm *svm)\r\n{\r\nint er;\r\ner = emulate_instruction(&svm->vcpu, EMULTYPE_TRAP_UD);\r\nif (er != EMULATE_DONE)\r\nkvm_queue_exception(&svm->vcpu, UD_VECTOR);\r\nreturn 1;\r\n}\r\nstatic void svm_fpu_activate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nclr_exception_intercept(svm, NM_VECTOR);\r\nsvm->vcpu.fpu_active = 1;\r\nupdate_cr0_intercept(svm);\r\n}\r\nstatic int nm_interception(struct vcpu_svm *svm)\r\n{\r\nsvm_fpu_activate(&svm->vcpu);\r\nreturn 1;\r\n}\r\nstatic bool is_erratum_383(void)\r\n{\r\nint err, i;\r\nu64 value;\r\nif (!erratum_383_found)\r\nreturn false;\r\nvalue = native_read_msr_safe(MSR_IA32_MC0_STATUS, &err);\r\nif (err)\r\nreturn false;\r\nvalue &= ~(1ULL << 62);\r\nif (value != 0xb600000000010015ULL)\r\nreturn false;\r\nfor (i = 0; i < 6; ++i)\r\nnative_write_msr_safe(MSR_IA32_MCx_STATUS(i), 0, 0);\r\nvalue = native_read_msr_safe(MSR_IA32_MCG_STATUS, &err);\r\nif (!err) {\r\nu32 low, high;\r\nvalue &= ~(1ULL << 2);\r\nlow = lower_32_bits(value);\r\nhigh = upper_32_bits(value);\r\nnative_write_msr_safe(MSR_IA32_MCG_STATUS, low, high);\r\n}\r\n__flush_tlb_all();\r\nreturn true;\r\n}\r\nstatic void svm_handle_mce(struct vcpu_svm *svm)\r\n{\r\nif (is_erratum_383()) {\r\npr_err("KVM: Guest triggered AMD Erratum 383\n");\r\nkvm_make_request(KVM_REQ_TRIPLE_FAULT, &svm->vcpu);\r\nreturn;\r\n}\r\nasm volatile (\r\n"int $0x12\n");\r\nreturn;\r\n}\r\nstatic int mc_interception(struct vcpu_svm *svm)\r\n{\r\nreturn 1;\r\n}\r\nstatic int shutdown_interception(struct vcpu_svm *svm)\r\n{\r\nstruct kvm_run *kvm_run = svm->vcpu.run;\r\nclear_page(svm->vmcb);\r\ninit_vmcb(svm);\r\nkvm_run->exit_reason = KVM_EXIT_SHUTDOWN;\r\nreturn 0;\r\n}\r\nstatic int io_interception(struct vcpu_svm *svm)\r\n{\r\nstruct kvm_vcpu *vcpu = &svm->vcpu;\r\nu32 io_info = svm->vmcb->control.exit_info_1;\r\nint size, in, string;\r\nunsigned port;\r\n++svm->vcpu.stat.io_exits;\r\nstring = (io_info & SVM_IOIO_STR_MASK) != 0;\r\nin = (io_info & SVM_IOIO_TYPE_MASK) != 0;\r\nif (string || in)\r\nreturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\r\nport = io_info >> 16;\r\nsize = (io_info & SVM_IOIO_SIZE_MASK) >> SVM_IOIO_SIZE_SHIFT;\r\nsvm->next_rip = svm->vmcb->control.exit_info_2;\r\nskip_emulated_instruction(&svm->vcpu);\r\nreturn kvm_fast_pio_out(vcpu, size, port);\r\n}\r\nstatic int nmi_interception(struct vcpu_svm *svm)\r\n{\r\nreturn 1;\r\n}\r\nstatic int intr_interception(struct vcpu_svm *svm)\r\n{\r\n++svm->vcpu.stat.irq_exits;\r\nreturn 1;\r\n}\r\nstatic int nop_on_interception(struct vcpu_svm *svm)\r\n{\r\nreturn 1;\r\n}\r\nstatic int halt_interception(struct vcpu_svm *svm)\r\n{\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 1;\r\nskip_emulated_instruction(&svm->vcpu);\r\nreturn kvm_emulate_halt(&svm->vcpu);\r\n}\r\nstatic int vmmcall_interception(struct vcpu_svm *svm)\r\n{\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 3;\r\nskip_emulated_instruction(&svm->vcpu);\r\nkvm_emulate_hypercall(&svm->vcpu);\r\nreturn 1;\r\n}\r\nstatic unsigned long nested_svm_get_tdp_cr3(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nreturn svm->nested.nested_cr3;\r\n}\r\nstatic u64 nested_svm_get_tdp_pdptr(struct kvm_vcpu *vcpu, int index)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu64 cr3 = svm->nested.nested_cr3;\r\nu64 pdpte;\r\nint ret;\r\nret = kvm_read_guest_page(vcpu->kvm, gpa_to_gfn(cr3), &pdpte,\r\noffset_in_page(cr3) + index * 8, 8);\r\nif (ret)\r\nreturn 0;\r\nreturn pdpte;\r\n}\r\nstatic void nested_svm_set_tdp_cr3(struct kvm_vcpu *vcpu,\r\nunsigned long root)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->control.nested_cr3 = root;\r\nmark_dirty(svm->vmcb, VMCB_NPT);\r\nsvm_flush_tlb(vcpu);\r\n}\r\nstatic void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,\r\nstruct x86_exception *fault)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->control.exit_code = SVM_EXIT_NPF;\r\nsvm->vmcb->control.exit_code_hi = 0;\r\nsvm->vmcb->control.exit_info_1 = fault->error_code;\r\nsvm->vmcb->control.exit_info_2 = fault->address;\r\nnested_svm_vmexit(svm);\r\n}\r\nstatic int nested_svm_init_mmu_context(struct kvm_vcpu *vcpu)\r\n{\r\nint r;\r\nr = kvm_init_shadow_mmu(vcpu, &vcpu->arch.mmu);\r\nvcpu->arch.mmu.set_cr3 = nested_svm_set_tdp_cr3;\r\nvcpu->arch.mmu.get_cr3 = nested_svm_get_tdp_cr3;\r\nvcpu->arch.mmu.get_pdptr = nested_svm_get_tdp_pdptr;\r\nvcpu->arch.mmu.inject_page_fault = nested_svm_inject_npf_exit;\r\nvcpu->arch.mmu.shadow_root_level = get_npt_level();\r\nvcpu->arch.walk_mmu = &vcpu->arch.nested_mmu;\r\nreturn r;\r\n}\r\nstatic void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->arch.walk_mmu = &vcpu->arch.mmu;\r\n}\r\nstatic int nested_svm_check_permissions(struct vcpu_svm *svm)\r\n{\r\nif (!(svm->vcpu.arch.efer & EFER_SVME)\r\n|| !is_paging(&svm->vcpu)) {\r\nkvm_queue_exception(&svm->vcpu, UD_VECTOR);\r\nreturn 1;\r\n}\r\nif (svm->vmcb->save.cpl) {\r\nkvm_inject_gp(&svm->vcpu, 0);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int nested_svm_check_exception(struct vcpu_svm *svm, unsigned nr,\r\nbool has_error_code, u32 error_code)\r\n{\r\nint vmexit;\r\nif (!is_guest_mode(&svm->vcpu))\r\nreturn 0;\r\nsvm->vmcb->control.exit_code = SVM_EXIT_EXCP_BASE + nr;\r\nsvm->vmcb->control.exit_code_hi = 0;\r\nsvm->vmcb->control.exit_info_1 = error_code;\r\nsvm->vmcb->control.exit_info_2 = svm->vcpu.arch.cr2;\r\nvmexit = nested_svm_intercept(svm);\r\nif (vmexit == NESTED_EXIT_DONE)\r\nsvm->nested.exit_required = true;\r\nreturn vmexit;\r\n}\r\nstatic inline bool nested_svm_intr(struct vcpu_svm *svm)\r\n{\r\nif (!is_guest_mode(&svm->vcpu))\r\nreturn true;\r\nif (!(svm->vcpu.arch.hflags & HF_VINTR_MASK))\r\nreturn true;\r\nif (!(svm->vcpu.arch.hflags & HF_HIF_MASK))\r\nreturn false;\r\nif (svm->nested.exit_required)\r\nreturn false;\r\nsvm->vmcb->control.exit_code = SVM_EXIT_INTR;\r\nsvm->vmcb->control.exit_info_1 = 0;\r\nsvm->vmcb->control.exit_info_2 = 0;\r\nif (svm->nested.intercept & 1ULL) {\r\nsvm->nested.exit_required = true;\r\ntrace_kvm_nested_intr_vmexit(svm->vmcb->save.rip);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic inline bool nested_svm_nmi(struct vcpu_svm *svm)\r\n{\r\nif (!is_guest_mode(&svm->vcpu))\r\nreturn true;\r\nif (!(svm->nested.intercept & (1ULL << INTERCEPT_NMI)))\r\nreturn true;\r\nsvm->vmcb->control.exit_code = SVM_EXIT_NMI;\r\nsvm->nested.exit_required = true;\r\nreturn false;\r\n}\r\nstatic void *nested_svm_map(struct vcpu_svm *svm, u64 gpa, struct page **_page)\r\n{\r\nstruct page *page;\r\nmight_sleep();\r\npage = gfn_to_page(svm->vcpu.kvm, gpa >> PAGE_SHIFT);\r\nif (is_error_page(page))\r\ngoto error;\r\n*_page = page;\r\nreturn kmap(page);\r\nerror:\r\nkvm_inject_gp(&svm->vcpu, 0);\r\nreturn NULL;\r\n}\r\nstatic void nested_svm_unmap(struct page *page)\r\n{\r\nkunmap(page);\r\nkvm_release_page_dirty(page);\r\n}\r\nstatic int nested_svm_intercept_ioio(struct vcpu_svm *svm)\r\n{\r\nunsigned port;\r\nu8 val, bit;\r\nu64 gpa;\r\nif (!(svm->nested.intercept & (1ULL << INTERCEPT_IOIO_PROT)))\r\nreturn NESTED_EXIT_HOST;\r\nport = svm->vmcb->control.exit_info_1 >> 16;\r\ngpa = svm->nested.vmcb_iopm + (port / 8);\r\nbit = port % 8;\r\nval = 0;\r\nif (kvm_read_guest(svm->vcpu.kvm, gpa, &val, 1))\r\nval &= (1 << bit);\r\nreturn val ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;\r\n}\r\nstatic int nested_svm_exit_handled_msr(struct vcpu_svm *svm)\r\n{\r\nu32 offset, msr, value;\r\nint write, mask;\r\nif (!(svm->nested.intercept & (1ULL << INTERCEPT_MSR_PROT)))\r\nreturn NESTED_EXIT_HOST;\r\nmsr = svm->vcpu.arch.regs[VCPU_REGS_RCX];\r\noffset = svm_msrpm_offset(msr);\r\nwrite = svm->vmcb->control.exit_info_1 & 1;\r\nmask = 1 << ((2 * (msr & 0xf)) + write);\r\nif (offset == MSR_INVALID)\r\nreturn NESTED_EXIT_DONE;\r\noffset *= 4;\r\nif (kvm_read_guest(svm->vcpu.kvm, svm->nested.vmcb_msrpm + offset, &value, 4))\r\nreturn NESTED_EXIT_DONE;\r\nreturn (value & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;\r\n}\r\nstatic int nested_svm_exit_special(struct vcpu_svm *svm)\r\n{\r\nu32 exit_code = svm->vmcb->control.exit_code;\r\nswitch (exit_code) {\r\ncase SVM_EXIT_INTR:\r\ncase SVM_EXIT_NMI:\r\ncase SVM_EXIT_EXCP_BASE + MC_VECTOR:\r\nreturn NESTED_EXIT_HOST;\r\ncase SVM_EXIT_NPF:\r\nif (npt_enabled)\r\nreturn NESTED_EXIT_HOST;\r\nbreak;\r\ncase SVM_EXIT_EXCP_BASE + PF_VECTOR:\r\nif (!npt_enabled && svm->apf_reason == 0)\r\nreturn NESTED_EXIT_HOST;\r\nbreak;\r\ncase SVM_EXIT_EXCP_BASE + NM_VECTOR:\r\nnm_interception(svm);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NESTED_EXIT_CONTINUE;\r\n}\r\nstatic int nested_svm_intercept(struct vcpu_svm *svm)\r\n{\r\nu32 exit_code = svm->vmcb->control.exit_code;\r\nint vmexit = NESTED_EXIT_HOST;\r\nswitch (exit_code) {\r\ncase SVM_EXIT_MSR:\r\nvmexit = nested_svm_exit_handled_msr(svm);\r\nbreak;\r\ncase SVM_EXIT_IOIO:\r\nvmexit = nested_svm_intercept_ioio(svm);\r\nbreak;\r\ncase SVM_EXIT_READ_CR0 ... SVM_EXIT_WRITE_CR8: {\r\nu32 bit = 1U << (exit_code - SVM_EXIT_READ_CR0);\r\nif (svm->nested.intercept_cr & bit)\r\nvmexit = NESTED_EXIT_DONE;\r\nbreak;\r\n}\r\ncase SVM_EXIT_READ_DR0 ... SVM_EXIT_WRITE_DR7: {\r\nu32 bit = 1U << (exit_code - SVM_EXIT_READ_DR0);\r\nif (svm->nested.intercept_dr & bit)\r\nvmexit = NESTED_EXIT_DONE;\r\nbreak;\r\n}\r\ncase SVM_EXIT_EXCP_BASE ... SVM_EXIT_EXCP_BASE + 0x1f: {\r\nu32 excp_bits = 1 << (exit_code - SVM_EXIT_EXCP_BASE);\r\nif (svm->nested.intercept_exceptions & excp_bits)\r\nvmexit = NESTED_EXIT_DONE;\r\nelse if ((exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR) &&\r\nsvm->apf_reason != 0)\r\nvmexit = NESTED_EXIT_DONE;\r\nbreak;\r\n}\r\ncase SVM_EXIT_ERR: {\r\nvmexit = NESTED_EXIT_DONE;\r\nbreak;\r\n}\r\ndefault: {\r\nu64 exit_bits = 1ULL << (exit_code - SVM_EXIT_INTR);\r\nif (svm->nested.intercept & exit_bits)\r\nvmexit = NESTED_EXIT_DONE;\r\n}\r\n}\r\nreturn vmexit;\r\n}\r\nstatic int nested_svm_exit_handled(struct vcpu_svm *svm)\r\n{\r\nint vmexit;\r\nvmexit = nested_svm_intercept(svm);\r\nif (vmexit == NESTED_EXIT_DONE)\r\nnested_svm_vmexit(svm);\r\nreturn vmexit;\r\n}\r\nstatic inline void copy_vmcb_control_area(struct vmcb *dst_vmcb, struct vmcb *from_vmcb)\r\n{\r\nstruct vmcb_control_area *dst = &dst_vmcb->control;\r\nstruct vmcb_control_area *from = &from_vmcb->control;\r\ndst->intercept_cr = from->intercept_cr;\r\ndst->intercept_dr = from->intercept_dr;\r\ndst->intercept_exceptions = from->intercept_exceptions;\r\ndst->intercept = from->intercept;\r\ndst->iopm_base_pa = from->iopm_base_pa;\r\ndst->msrpm_base_pa = from->msrpm_base_pa;\r\ndst->tsc_offset = from->tsc_offset;\r\ndst->asid = from->asid;\r\ndst->tlb_ctl = from->tlb_ctl;\r\ndst->int_ctl = from->int_ctl;\r\ndst->int_vector = from->int_vector;\r\ndst->int_state = from->int_state;\r\ndst->exit_code = from->exit_code;\r\ndst->exit_code_hi = from->exit_code_hi;\r\ndst->exit_info_1 = from->exit_info_1;\r\ndst->exit_info_2 = from->exit_info_2;\r\ndst->exit_int_info = from->exit_int_info;\r\ndst->exit_int_info_err = from->exit_int_info_err;\r\ndst->nested_ctl = from->nested_ctl;\r\ndst->event_inj = from->event_inj;\r\ndst->event_inj_err = from->event_inj_err;\r\ndst->nested_cr3 = from->nested_cr3;\r\ndst->lbr_ctl = from->lbr_ctl;\r\n}\r\nstatic int nested_svm_vmexit(struct vcpu_svm *svm)\r\n{\r\nstruct vmcb *nested_vmcb;\r\nstruct vmcb *hsave = svm->nested.hsave;\r\nstruct vmcb *vmcb = svm->vmcb;\r\nstruct page *page;\r\ntrace_kvm_nested_vmexit_inject(vmcb->control.exit_code,\r\nvmcb->control.exit_info_1,\r\nvmcb->control.exit_info_2,\r\nvmcb->control.exit_int_info,\r\nvmcb->control.exit_int_info_err,\r\nKVM_ISA_SVM);\r\nnested_vmcb = nested_svm_map(svm, svm->nested.vmcb, &page);\r\nif (!nested_vmcb)\r\nreturn 1;\r\nleave_guest_mode(&svm->vcpu);\r\nsvm->nested.vmcb = 0;\r\ndisable_gif(svm);\r\nnested_vmcb->save.es = vmcb->save.es;\r\nnested_vmcb->save.cs = vmcb->save.cs;\r\nnested_vmcb->save.ss = vmcb->save.ss;\r\nnested_vmcb->save.ds = vmcb->save.ds;\r\nnested_vmcb->save.gdtr = vmcb->save.gdtr;\r\nnested_vmcb->save.idtr = vmcb->save.idtr;\r\nnested_vmcb->save.efer = svm->vcpu.arch.efer;\r\nnested_vmcb->save.cr0 = kvm_read_cr0(&svm->vcpu);\r\nnested_vmcb->save.cr3 = kvm_read_cr3(&svm->vcpu);\r\nnested_vmcb->save.cr2 = vmcb->save.cr2;\r\nnested_vmcb->save.cr4 = svm->vcpu.arch.cr4;\r\nnested_vmcb->save.rflags = kvm_get_rflags(&svm->vcpu);\r\nnested_vmcb->save.rip = vmcb->save.rip;\r\nnested_vmcb->save.rsp = vmcb->save.rsp;\r\nnested_vmcb->save.rax = vmcb->save.rax;\r\nnested_vmcb->save.dr7 = vmcb->save.dr7;\r\nnested_vmcb->save.dr6 = vmcb->save.dr6;\r\nnested_vmcb->save.cpl = vmcb->save.cpl;\r\nnested_vmcb->control.int_ctl = vmcb->control.int_ctl;\r\nnested_vmcb->control.int_vector = vmcb->control.int_vector;\r\nnested_vmcb->control.int_state = vmcb->control.int_state;\r\nnested_vmcb->control.exit_code = vmcb->control.exit_code;\r\nnested_vmcb->control.exit_code_hi = vmcb->control.exit_code_hi;\r\nnested_vmcb->control.exit_info_1 = vmcb->control.exit_info_1;\r\nnested_vmcb->control.exit_info_2 = vmcb->control.exit_info_2;\r\nnested_vmcb->control.exit_int_info = vmcb->control.exit_int_info;\r\nnested_vmcb->control.exit_int_info_err = vmcb->control.exit_int_info_err;\r\nnested_vmcb->control.next_rip = vmcb->control.next_rip;\r\nif (vmcb->control.event_inj & SVM_EVTINJ_VALID) {\r\nstruct vmcb_control_area *nc = &nested_vmcb->control;\r\nnc->exit_int_info = vmcb->control.event_inj;\r\nnc->exit_int_info_err = vmcb->control.event_inj_err;\r\n}\r\nnested_vmcb->control.tlb_ctl = 0;\r\nnested_vmcb->control.event_inj = 0;\r\nnested_vmcb->control.event_inj_err = 0;\r\nif (!(svm->vcpu.arch.hflags & HF_VINTR_MASK))\r\nnested_vmcb->control.int_ctl &= ~V_INTR_MASKING_MASK;\r\ncopy_vmcb_control_area(vmcb, hsave);\r\nkvm_clear_exception_queue(&svm->vcpu);\r\nkvm_clear_interrupt_queue(&svm->vcpu);\r\nsvm->nested.nested_cr3 = 0;\r\nsvm->vmcb->save.es = hsave->save.es;\r\nsvm->vmcb->save.cs = hsave->save.cs;\r\nsvm->vmcb->save.ss = hsave->save.ss;\r\nsvm->vmcb->save.ds = hsave->save.ds;\r\nsvm->vmcb->save.gdtr = hsave->save.gdtr;\r\nsvm->vmcb->save.idtr = hsave->save.idtr;\r\nkvm_set_rflags(&svm->vcpu, hsave->save.rflags);\r\nsvm_set_efer(&svm->vcpu, hsave->save.efer);\r\nsvm_set_cr0(&svm->vcpu, hsave->save.cr0 | X86_CR0_PE);\r\nsvm_set_cr4(&svm->vcpu, hsave->save.cr4);\r\nif (npt_enabled) {\r\nsvm->vmcb->save.cr3 = hsave->save.cr3;\r\nsvm->vcpu.arch.cr3 = hsave->save.cr3;\r\n} else {\r\n(void)kvm_set_cr3(&svm->vcpu, hsave->save.cr3);\r\n}\r\nkvm_register_write(&svm->vcpu, VCPU_REGS_RAX, hsave->save.rax);\r\nkvm_register_write(&svm->vcpu, VCPU_REGS_RSP, hsave->save.rsp);\r\nkvm_register_write(&svm->vcpu, VCPU_REGS_RIP, hsave->save.rip);\r\nsvm->vmcb->save.dr7 = 0;\r\nsvm->vmcb->save.cpl = 0;\r\nsvm->vmcb->control.exit_int_info = 0;\r\nmark_all_dirty(svm->vmcb);\r\nnested_svm_unmap(page);\r\nnested_svm_uninit_mmu_context(&svm->vcpu);\r\nkvm_mmu_reset_context(&svm->vcpu);\r\nkvm_mmu_load(&svm->vcpu);\r\nreturn 0;\r\n}\r\nstatic bool nested_svm_vmrun_msrpm(struct vcpu_svm *svm)\r\n{\r\nint i;\r\nif (!(svm->nested.intercept & (1ULL << INTERCEPT_MSR_PROT)))\r\nreturn true;\r\nfor (i = 0; i < MSRPM_OFFSETS; i++) {\r\nu32 value, p;\r\nu64 offset;\r\nif (msrpm_offsets[i] == 0xffffffff)\r\nbreak;\r\np = msrpm_offsets[i];\r\noffset = svm->nested.vmcb_msrpm + (p * 4);\r\nif (kvm_read_guest(svm->vcpu.kvm, offset, &value, 4))\r\nreturn false;\r\nsvm->nested.msrpm[p] = svm->msrpm[p] | value;\r\n}\r\nsvm->vmcb->control.msrpm_base_pa = __pa(svm->nested.msrpm);\r\nreturn true;\r\n}\r\nstatic bool nested_vmcb_checks(struct vmcb *vmcb)\r\n{\r\nif ((vmcb->control.intercept & (1ULL << INTERCEPT_VMRUN)) == 0)\r\nreturn false;\r\nif (vmcb->control.asid == 0)\r\nreturn false;\r\nif (vmcb->control.nested_ctl && !npt_enabled)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool nested_svm_vmrun(struct vcpu_svm *svm)\r\n{\r\nstruct vmcb *nested_vmcb;\r\nstruct vmcb *hsave = svm->nested.hsave;\r\nstruct vmcb *vmcb = svm->vmcb;\r\nstruct page *page;\r\nu64 vmcb_gpa;\r\nvmcb_gpa = svm->vmcb->save.rax;\r\nnested_vmcb = nested_svm_map(svm, svm->vmcb->save.rax, &page);\r\nif (!nested_vmcb)\r\nreturn false;\r\nif (!nested_vmcb_checks(nested_vmcb)) {\r\nnested_vmcb->control.exit_code = SVM_EXIT_ERR;\r\nnested_vmcb->control.exit_code_hi = 0;\r\nnested_vmcb->control.exit_info_1 = 0;\r\nnested_vmcb->control.exit_info_2 = 0;\r\nnested_svm_unmap(page);\r\nreturn false;\r\n}\r\ntrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb_gpa,\r\nnested_vmcb->save.rip,\r\nnested_vmcb->control.int_ctl,\r\nnested_vmcb->control.event_inj,\r\nnested_vmcb->control.nested_ctl);\r\ntrace_kvm_nested_intercepts(nested_vmcb->control.intercept_cr & 0xffff,\r\nnested_vmcb->control.intercept_cr >> 16,\r\nnested_vmcb->control.intercept_exceptions,\r\nnested_vmcb->control.intercept);\r\nkvm_clear_exception_queue(&svm->vcpu);\r\nkvm_clear_interrupt_queue(&svm->vcpu);\r\nhsave->save.es = vmcb->save.es;\r\nhsave->save.cs = vmcb->save.cs;\r\nhsave->save.ss = vmcb->save.ss;\r\nhsave->save.ds = vmcb->save.ds;\r\nhsave->save.gdtr = vmcb->save.gdtr;\r\nhsave->save.idtr = vmcb->save.idtr;\r\nhsave->save.efer = svm->vcpu.arch.efer;\r\nhsave->save.cr0 = kvm_read_cr0(&svm->vcpu);\r\nhsave->save.cr4 = svm->vcpu.arch.cr4;\r\nhsave->save.rflags = kvm_get_rflags(&svm->vcpu);\r\nhsave->save.rip = kvm_rip_read(&svm->vcpu);\r\nhsave->save.rsp = vmcb->save.rsp;\r\nhsave->save.rax = vmcb->save.rax;\r\nif (npt_enabled)\r\nhsave->save.cr3 = vmcb->save.cr3;\r\nelse\r\nhsave->save.cr3 = kvm_read_cr3(&svm->vcpu);\r\ncopy_vmcb_control_area(hsave, vmcb);\r\nif (kvm_get_rflags(&svm->vcpu) & X86_EFLAGS_IF)\r\nsvm->vcpu.arch.hflags |= HF_HIF_MASK;\r\nelse\r\nsvm->vcpu.arch.hflags &= ~HF_HIF_MASK;\r\nif (nested_vmcb->control.nested_ctl) {\r\nkvm_mmu_unload(&svm->vcpu);\r\nsvm->nested.nested_cr3 = nested_vmcb->control.nested_cr3;\r\nnested_svm_init_mmu_context(&svm->vcpu);\r\n}\r\nsvm->vmcb->save.es = nested_vmcb->save.es;\r\nsvm->vmcb->save.cs = nested_vmcb->save.cs;\r\nsvm->vmcb->save.ss = nested_vmcb->save.ss;\r\nsvm->vmcb->save.ds = nested_vmcb->save.ds;\r\nsvm->vmcb->save.gdtr = nested_vmcb->save.gdtr;\r\nsvm->vmcb->save.idtr = nested_vmcb->save.idtr;\r\nkvm_set_rflags(&svm->vcpu, nested_vmcb->save.rflags);\r\nsvm_set_efer(&svm->vcpu, nested_vmcb->save.efer);\r\nsvm_set_cr0(&svm->vcpu, nested_vmcb->save.cr0);\r\nsvm_set_cr4(&svm->vcpu, nested_vmcb->save.cr4);\r\nif (npt_enabled) {\r\nsvm->vmcb->save.cr3 = nested_vmcb->save.cr3;\r\nsvm->vcpu.arch.cr3 = nested_vmcb->save.cr3;\r\n} else\r\n(void)kvm_set_cr3(&svm->vcpu, nested_vmcb->save.cr3);\r\nkvm_mmu_reset_context(&svm->vcpu);\r\nsvm->vmcb->save.cr2 = svm->vcpu.arch.cr2 = nested_vmcb->save.cr2;\r\nkvm_register_write(&svm->vcpu, VCPU_REGS_RAX, nested_vmcb->save.rax);\r\nkvm_register_write(&svm->vcpu, VCPU_REGS_RSP, nested_vmcb->save.rsp);\r\nkvm_register_write(&svm->vcpu, VCPU_REGS_RIP, nested_vmcb->save.rip);\r\nsvm->vmcb->save.rax = nested_vmcb->save.rax;\r\nsvm->vmcb->save.rsp = nested_vmcb->save.rsp;\r\nsvm->vmcb->save.rip = nested_vmcb->save.rip;\r\nsvm->vmcb->save.dr7 = nested_vmcb->save.dr7;\r\nsvm->vmcb->save.dr6 = nested_vmcb->save.dr6;\r\nsvm->vmcb->save.cpl = nested_vmcb->save.cpl;\r\nsvm->nested.vmcb_msrpm = nested_vmcb->control.msrpm_base_pa & ~0x0fffULL;\r\nsvm->nested.vmcb_iopm = nested_vmcb->control.iopm_base_pa & ~0x0fffULL;\r\nsvm->nested.intercept_cr = nested_vmcb->control.intercept_cr;\r\nsvm->nested.intercept_dr = nested_vmcb->control.intercept_dr;\r\nsvm->nested.intercept_exceptions = nested_vmcb->control.intercept_exceptions;\r\nsvm->nested.intercept = nested_vmcb->control.intercept;\r\nsvm_flush_tlb(&svm->vcpu);\r\nsvm->vmcb->control.int_ctl = nested_vmcb->control.int_ctl | V_INTR_MASKING_MASK;\r\nif (nested_vmcb->control.int_ctl & V_INTR_MASKING_MASK)\r\nsvm->vcpu.arch.hflags |= HF_VINTR_MASK;\r\nelse\r\nsvm->vcpu.arch.hflags &= ~HF_VINTR_MASK;\r\nif (svm->vcpu.arch.hflags & HF_VINTR_MASK) {\r\nclr_cr_intercept(svm, INTERCEPT_CR8_READ);\r\nclr_cr_intercept(svm, INTERCEPT_CR8_WRITE);\r\n}\r\nclr_intercept(svm, INTERCEPT_VMMCALL);\r\nsvm->vmcb->control.lbr_ctl = nested_vmcb->control.lbr_ctl;\r\nsvm->vmcb->control.int_vector = nested_vmcb->control.int_vector;\r\nsvm->vmcb->control.int_state = nested_vmcb->control.int_state;\r\nsvm->vmcb->control.tsc_offset += nested_vmcb->control.tsc_offset;\r\nsvm->vmcb->control.event_inj = nested_vmcb->control.event_inj;\r\nsvm->vmcb->control.event_inj_err = nested_vmcb->control.event_inj_err;\r\nnested_svm_unmap(page);\r\nenter_guest_mode(&svm->vcpu);\r\nrecalc_intercepts(svm);\r\nsvm->nested.vmcb = vmcb_gpa;\r\nenable_gif(svm);\r\nmark_all_dirty(svm->vmcb);\r\nreturn true;\r\n}\r\nstatic void nested_svm_vmloadsave(struct vmcb *from_vmcb, struct vmcb *to_vmcb)\r\n{\r\nto_vmcb->save.fs = from_vmcb->save.fs;\r\nto_vmcb->save.gs = from_vmcb->save.gs;\r\nto_vmcb->save.tr = from_vmcb->save.tr;\r\nto_vmcb->save.ldtr = from_vmcb->save.ldtr;\r\nto_vmcb->save.kernel_gs_base = from_vmcb->save.kernel_gs_base;\r\nto_vmcb->save.star = from_vmcb->save.star;\r\nto_vmcb->save.lstar = from_vmcb->save.lstar;\r\nto_vmcb->save.cstar = from_vmcb->save.cstar;\r\nto_vmcb->save.sfmask = from_vmcb->save.sfmask;\r\nto_vmcb->save.sysenter_cs = from_vmcb->save.sysenter_cs;\r\nto_vmcb->save.sysenter_esp = from_vmcb->save.sysenter_esp;\r\nto_vmcb->save.sysenter_eip = from_vmcb->save.sysenter_eip;\r\n}\r\nstatic int vmload_interception(struct vcpu_svm *svm)\r\n{\r\nstruct vmcb *nested_vmcb;\r\nstruct page *page;\r\nif (nested_svm_check_permissions(svm))\r\nreturn 1;\r\nnested_vmcb = nested_svm_map(svm, svm->vmcb->save.rax, &page);\r\nif (!nested_vmcb)\r\nreturn 1;\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 3;\r\nskip_emulated_instruction(&svm->vcpu);\r\nnested_svm_vmloadsave(nested_vmcb, svm->vmcb);\r\nnested_svm_unmap(page);\r\nreturn 1;\r\n}\r\nstatic int vmsave_interception(struct vcpu_svm *svm)\r\n{\r\nstruct vmcb *nested_vmcb;\r\nstruct page *page;\r\nif (nested_svm_check_permissions(svm))\r\nreturn 1;\r\nnested_vmcb = nested_svm_map(svm, svm->vmcb->save.rax, &page);\r\nif (!nested_vmcb)\r\nreturn 1;\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 3;\r\nskip_emulated_instruction(&svm->vcpu);\r\nnested_svm_vmloadsave(svm->vmcb, nested_vmcb);\r\nnested_svm_unmap(page);\r\nreturn 1;\r\n}\r\nstatic int vmrun_interception(struct vcpu_svm *svm)\r\n{\r\nif (nested_svm_check_permissions(svm))\r\nreturn 1;\r\nkvm_rip_write(&svm->vcpu, kvm_rip_read(&svm->vcpu) + 3);\r\nif (!nested_svm_vmrun(svm))\r\nreturn 1;\r\nif (!nested_svm_vmrun_msrpm(svm))\r\ngoto failed;\r\nreturn 1;\r\nfailed:\r\nsvm->vmcb->control.exit_code = SVM_EXIT_ERR;\r\nsvm->vmcb->control.exit_code_hi = 0;\r\nsvm->vmcb->control.exit_info_1 = 0;\r\nsvm->vmcb->control.exit_info_2 = 0;\r\nnested_svm_vmexit(svm);\r\nreturn 1;\r\n}\r\nstatic int stgi_interception(struct vcpu_svm *svm)\r\n{\r\nif (nested_svm_check_permissions(svm))\r\nreturn 1;\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 3;\r\nskip_emulated_instruction(&svm->vcpu);\r\nkvm_make_request(KVM_REQ_EVENT, &svm->vcpu);\r\nenable_gif(svm);\r\nreturn 1;\r\n}\r\nstatic int clgi_interception(struct vcpu_svm *svm)\r\n{\r\nif (nested_svm_check_permissions(svm))\r\nreturn 1;\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 3;\r\nskip_emulated_instruction(&svm->vcpu);\r\ndisable_gif(svm);\r\nsvm_clear_vintr(svm);\r\nsvm->vmcb->control.int_ctl &= ~V_IRQ_MASK;\r\nmark_dirty(svm->vmcb, VMCB_INTR);\r\nreturn 1;\r\n}\r\nstatic int invlpga_interception(struct vcpu_svm *svm)\r\n{\r\nstruct kvm_vcpu *vcpu = &svm->vcpu;\r\ntrace_kvm_invlpga(svm->vmcb->save.rip, vcpu->arch.regs[VCPU_REGS_RCX],\r\nvcpu->arch.regs[VCPU_REGS_RAX]);\r\nkvm_mmu_invlpg(vcpu, vcpu->arch.regs[VCPU_REGS_RAX]);\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 3;\r\nskip_emulated_instruction(&svm->vcpu);\r\nreturn 1;\r\n}\r\nstatic int skinit_interception(struct vcpu_svm *svm)\r\n{\r\ntrace_kvm_skinit(svm->vmcb->save.rip, svm->vcpu.arch.regs[VCPU_REGS_RAX]);\r\nkvm_queue_exception(&svm->vcpu, UD_VECTOR);\r\nreturn 1;\r\n}\r\nstatic int xsetbv_interception(struct vcpu_svm *svm)\r\n{\r\nu64 new_bv = kvm_read_edx_eax(&svm->vcpu);\r\nu32 index = kvm_register_read(&svm->vcpu, VCPU_REGS_RCX);\r\nif (kvm_set_xcr(&svm->vcpu, index, new_bv) == 0) {\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 3;\r\nskip_emulated_instruction(&svm->vcpu);\r\n}\r\nreturn 1;\r\n}\r\nstatic int invalid_op_interception(struct vcpu_svm *svm)\r\n{\r\nkvm_queue_exception(&svm->vcpu, UD_VECTOR);\r\nreturn 1;\r\n}\r\nstatic int task_switch_interception(struct vcpu_svm *svm)\r\n{\r\nu16 tss_selector;\r\nint reason;\r\nint int_type = svm->vmcb->control.exit_int_info &\r\nSVM_EXITINTINFO_TYPE_MASK;\r\nint int_vec = svm->vmcb->control.exit_int_info & SVM_EVTINJ_VEC_MASK;\r\nuint32_t type =\r\nsvm->vmcb->control.exit_int_info & SVM_EXITINTINFO_TYPE_MASK;\r\nuint32_t idt_v =\r\nsvm->vmcb->control.exit_int_info & SVM_EXITINTINFO_VALID;\r\nbool has_error_code = false;\r\nu32 error_code = 0;\r\ntss_selector = (u16)svm->vmcb->control.exit_info_1;\r\nif (svm->vmcb->control.exit_info_2 &\r\n(1ULL << SVM_EXITINFOSHIFT_TS_REASON_IRET))\r\nreason = TASK_SWITCH_IRET;\r\nelse if (svm->vmcb->control.exit_info_2 &\r\n(1ULL << SVM_EXITINFOSHIFT_TS_REASON_JMP))\r\nreason = TASK_SWITCH_JMP;\r\nelse if (idt_v)\r\nreason = TASK_SWITCH_GATE;\r\nelse\r\nreason = TASK_SWITCH_CALL;\r\nif (reason == TASK_SWITCH_GATE) {\r\nswitch (type) {\r\ncase SVM_EXITINTINFO_TYPE_NMI:\r\nsvm->vcpu.arch.nmi_injected = false;\r\nbreak;\r\ncase SVM_EXITINTINFO_TYPE_EXEPT:\r\nif (svm->vmcb->control.exit_info_2 &\r\n(1ULL << SVM_EXITINFOSHIFT_TS_HAS_ERROR_CODE)) {\r\nhas_error_code = true;\r\nerror_code =\r\n(u32)svm->vmcb->control.exit_info_2;\r\n}\r\nkvm_clear_exception_queue(&svm->vcpu);\r\nbreak;\r\ncase SVM_EXITINTINFO_TYPE_INTR:\r\nkvm_clear_interrupt_queue(&svm->vcpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nif (reason != TASK_SWITCH_GATE ||\r\nint_type == SVM_EXITINTINFO_TYPE_SOFT ||\r\n(int_type == SVM_EXITINTINFO_TYPE_EXEPT &&\r\n(int_vec == OF_VECTOR || int_vec == BP_VECTOR)))\r\nskip_emulated_instruction(&svm->vcpu);\r\nif (int_type != SVM_EXITINTINFO_TYPE_SOFT)\r\nint_vec = -1;\r\nif (kvm_task_switch(&svm->vcpu, tss_selector, int_vec, reason,\r\nhas_error_code, error_code) == EMULATE_FAIL) {\r\nsvm->vcpu.run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nsvm->vcpu.run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\r\nsvm->vcpu.run->internal.ndata = 0;\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int cpuid_interception(struct vcpu_svm *svm)\r\n{\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 2;\r\nkvm_emulate_cpuid(&svm->vcpu);\r\nreturn 1;\r\n}\r\nstatic int iret_interception(struct vcpu_svm *svm)\r\n{\r\n++svm->vcpu.stat.nmi_window_exits;\r\nclr_intercept(svm, INTERCEPT_IRET);\r\nsvm->vcpu.arch.hflags |= HF_IRET_MASK;\r\nsvm->nmi_iret_rip = kvm_rip_read(&svm->vcpu);\r\nreturn 1;\r\n}\r\nstatic int invlpg_interception(struct vcpu_svm *svm)\r\n{\r\nif (!static_cpu_has(X86_FEATURE_DECODEASSISTS))\r\nreturn emulate_instruction(&svm->vcpu, 0) == EMULATE_DONE;\r\nkvm_mmu_invlpg(&svm->vcpu, svm->vmcb->control.exit_info_1);\r\nskip_emulated_instruction(&svm->vcpu);\r\nreturn 1;\r\n}\r\nstatic int emulate_on_interception(struct vcpu_svm *svm)\r\n{\r\nreturn emulate_instruction(&svm->vcpu, 0) == EMULATE_DONE;\r\n}\r\nstatic int rdpmc_interception(struct vcpu_svm *svm)\r\n{\r\nint err;\r\nif (!static_cpu_has(X86_FEATURE_NRIPS))\r\nreturn emulate_on_interception(svm);\r\nerr = kvm_rdpmc(&svm->vcpu);\r\nkvm_complete_insn_gp(&svm->vcpu, err);\r\nreturn 1;\r\n}\r\nbool check_selective_cr0_intercepted(struct vcpu_svm *svm, unsigned long val)\r\n{\r\nunsigned long cr0 = svm->vcpu.arch.cr0;\r\nbool ret = false;\r\nu64 intercept;\r\nintercept = svm->nested.intercept;\r\nif (!is_guest_mode(&svm->vcpu) ||\r\n(!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0))))\r\nreturn false;\r\ncr0 &= ~SVM_CR0_SELECTIVE_MASK;\r\nval &= ~SVM_CR0_SELECTIVE_MASK;\r\nif (cr0 ^ val) {\r\nsvm->vmcb->control.exit_code = SVM_EXIT_CR0_SEL_WRITE;\r\nret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);\r\n}\r\nreturn ret;\r\n}\r\nstatic int cr_interception(struct vcpu_svm *svm)\r\n{\r\nint reg, cr;\r\nunsigned long val;\r\nint err;\r\nif (!static_cpu_has(X86_FEATURE_DECODEASSISTS))\r\nreturn emulate_on_interception(svm);\r\nif (unlikely((svm->vmcb->control.exit_info_1 & CR_VALID) == 0))\r\nreturn emulate_on_interception(svm);\r\nreg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;\r\ncr = svm->vmcb->control.exit_code - SVM_EXIT_READ_CR0;\r\nerr = 0;\r\nif (cr >= 16) {\r\ncr -= 16;\r\nval = kvm_register_read(&svm->vcpu, reg);\r\nswitch (cr) {\r\ncase 0:\r\nif (!check_selective_cr0_intercepted(svm, val))\r\nerr = kvm_set_cr0(&svm->vcpu, val);\r\nelse\r\nreturn 1;\r\nbreak;\r\ncase 3:\r\nerr = kvm_set_cr3(&svm->vcpu, val);\r\nbreak;\r\ncase 4:\r\nerr = kvm_set_cr4(&svm->vcpu, val);\r\nbreak;\r\ncase 8:\r\nerr = kvm_set_cr8(&svm->vcpu, val);\r\nbreak;\r\ndefault:\r\nWARN(1, "unhandled write to CR%d", cr);\r\nkvm_queue_exception(&svm->vcpu, UD_VECTOR);\r\nreturn 1;\r\n}\r\n} else {\r\nswitch (cr) {\r\ncase 0:\r\nval = kvm_read_cr0(&svm->vcpu);\r\nbreak;\r\ncase 2:\r\nval = svm->vcpu.arch.cr2;\r\nbreak;\r\ncase 3:\r\nval = kvm_read_cr3(&svm->vcpu);\r\nbreak;\r\ncase 4:\r\nval = kvm_read_cr4(&svm->vcpu);\r\nbreak;\r\ncase 8:\r\nval = kvm_get_cr8(&svm->vcpu);\r\nbreak;\r\ndefault:\r\nWARN(1, "unhandled read from CR%d", cr);\r\nkvm_queue_exception(&svm->vcpu, UD_VECTOR);\r\nreturn 1;\r\n}\r\nkvm_register_write(&svm->vcpu, reg, val);\r\n}\r\nkvm_complete_insn_gp(&svm->vcpu, err);\r\nreturn 1;\r\n}\r\nstatic int dr_interception(struct vcpu_svm *svm)\r\n{\r\nint reg, dr;\r\nunsigned long val;\r\nint err;\r\nif (!boot_cpu_has(X86_FEATURE_DECODEASSISTS))\r\nreturn emulate_on_interception(svm);\r\nreg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;\r\ndr = svm->vmcb->control.exit_code - SVM_EXIT_READ_DR0;\r\nif (dr >= 16) {\r\nval = kvm_register_read(&svm->vcpu, reg);\r\nkvm_set_dr(&svm->vcpu, dr - 16, val);\r\n} else {\r\nerr = kvm_get_dr(&svm->vcpu, dr, &val);\r\nif (!err)\r\nkvm_register_write(&svm->vcpu, reg, val);\r\n}\r\nskip_emulated_instruction(&svm->vcpu);\r\nreturn 1;\r\n}\r\nstatic int cr8_write_interception(struct vcpu_svm *svm)\r\n{\r\nstruct kvm_run *kvm_run = svm->vcpu.run;\r\nint r;\r\nu8 cr8_prev = kvm_get_cr8(&svm->vcpu);\r\nr = cr_interception(svm);\r\nif (irqchip_in_kernel(svm->vcpu.kvm)) {\r\nclr_cr_intercept(svm, INTERCEPT_CR8_WRITE);\r\nreturn r;\r\n}\r\nif (cr8_prev <= kvm_get_cr8(&svm->vcpu))\r\nreturn r;\r\nkvm_run->exit_reason = KVM_EXIT_SET_TPR;\r\nreturn 0;\r\n}\r\nu64 svm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)\r\n{\r\nstruct vmcb *vmcb = get_host_vmcb(to_svm(vcpu));\r\nreturn vmcb->control.tsc_offset +\r\nsvm_scale_tsc(vcpu, host_tsc);\r\n}\r\nstatic int svm_get_msr(struct kvm_vcpu *vcpu, unsigned ecx, u64 *data)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nswitch (ecx) {\r\ncase MSR_IA32_TSC: {\r\n*data = svm->vmcb->control.tsc_offset +\r\nsvm_scale_tsc(vcpu, native_read_tsc());\r\nbreak;\r\n}\r\ncase MSR_STAR:\r\n*data = svm->vmcb->save.star;\r\nbreak;\r\n#ifdef CONFIG_X86_64\r\ncase MSR_LSTAR:\r\n*data = svm->vmcb->save.lstar;\r\nbreak;\r\ncase MSR_CSTAR:\r\n*data = svm->vmcb->save.cstar;\r\nbreak;\r\ncase MSR_KERNEL_GS_BASE:\r\n*data = svm->vmcb->save.kernel_gs_base;\r\nbreak;\r\ncase MSR_SYSCALL_MASK:\r\n*data = svm->vmcb->save.sfmask;\r\nbreak;\r\n#endif\r\ncase MSR_IA32_SYSENTER_CS:\r\n*data = svm->vmcb->save.sysenter_cs;\r\nbreak;\r\ncase MSR_IA32_SYSENTER_EIP:\r\n*data = svm->sysenter_eip;\r\nbreak;\r\ncase MSR_IA32_SYSENTER_ESP:\r\n*data = svm->sysenter_esp;\r\nbreak;\r\ncase MSR_IA32_DEBUGCTLMSR:\r\n*data = svm->vmcb->save.dbgctl;\r\nbreak;\r\ncase MSR_IA32_LASTBRANCHFROMIP:\r\n*data = svm->vmcb->save.br_from;\r\nbreak;\r\ncase MSR_IA32_LASTBRANCHTOIP:\r\n*data = svm->vmcb->save.br_to;\r\nbreak;\r\ncase MSR_IA32_LASTINTFROMIP:\r\n*data = svm->vmcb->save.last_excp_from;\r\nbreak;\r\ncase MSR_IA32_LASTINTTOIP:\r\n*data = svm->vmcb->save.last_excp_to;\r\nbreak;\r\ncase MSR_VM_HSAVE_PA:\r\n*data = svm->nested.hsave_msr;\r\nbreak;\r\ncase MSR_VM_CR:\r\n*data = svm->nested.vm_cr_msr;\r\nbreak;\r\ncase MSR_IA32_UCODE_REV:\r\n*data = 0x01000065;\r\nbreak;\r\ndefault:\r\nreturn kvm_get_msr_common(vcpu, ecx, data);\r\n}\r\nreturn 0;\r\n}\r\nstatic int rdmsr_interception(struct vcpu_svm *svm)\r\n{\r\nu32 ecx = svm->vcpu.arch.regs[VCPU_REGS_RCX];\r\nu64 data;\r\nif (svm_get_msr(&svm->vcpu, ecx, &data)) {\r\ntrace_kvm_msr_read_ex(ecx);\r\nkvm_inject_gp(&svm->vcpu, 0);\r\n} else {\r\ntrace_kvm_msr_read(ecx, data);\r\nsvm->vcpu.arch.regs[VCPU_REGS_RAX] = data & 0xffffffff;\r\nsvm->vcpu.arch.regs[VCPU_REGS_RDX] = data >> 32;\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 2;\r\nskip_emulated_instruction(&svm->vcpu);\r\n}\r\nreturn 1;\r\n}\r\nstatic int svm_set_vm_cr(struct kvm_vcpu *vcpu, u64 data)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nint svm_dis, chg_mask;\r\nif (data & ~SVM_VM_CR_VALID_MASK)\r\nreturn 1;\r\nchg_mask = SVM_VM_CR_VALID_MASK;\r\nif (svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK)\r\nchg_mask &= ~(SVM_VM_CR_SVM_LOCK_MASK | SVM_VM_CR_SVM_DIS_MASK);\r\nsvm->nested.vm_cr_msr &= ~chg_mask;\r\nsvm->nested.vm_cr_msr |= (data & chg_mask);\r\nsvm_dis = svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK;\r\nif (svm_dis && (vcpu->arch.efer & EFER_SVME))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu32 ecx = msr->index;\r\nu64 data = msr->data;\r\nswitch (ecx) {\r\ncase MSR_IA32_TSC:\r\nkvm_write_tsc(vcpu, msr);\r\nbreak;\r\ncase MSR_STAR:\r\nsvm->vmcb->save.star = data;\r\nbreak;\r\n#ifdef CONFIG_X86_64\r\ncase MSR_LSTAR:\r\nsvm->vmcb->save.lstar = data;\r\nbreak;\r\ncase MSR_CSTAR:\r\nsvm->vmcb->save.cstar = data;\r\nbreak;\r\ncase MSR_KERNEL_GS_BASE:\r\nsvm->vmcb->save.kernel_gs_base = data;\r\nbreak;\r\ncase MSR_SYSCALL_MASK:\r\nsvm->vmcb->save.sfmask = data;\r\nbreak;\r\n#endif\r\ncase MSR_IA32_SYSENTER_CS:\r\nsvm->vmcb->save.sysenter_cs = data;\r\nbreak;\r\ncase MSR_IA32_SYSENTER_EIP:\r\nsvm->sysenter_eip = data;\r\nsvm->vmcb->save.sysenter_eip = data;\r\nbreak;\r\ncase MSR_IA32_SYSENTER_ESP:\r\nsvm->sysenter_esp = data;\r\nsvm->vmcb->save.sysenter_esp = data;\r\nbreak;\r\ncase MSR_IA32_DEBUGCTLMSR:\r\nif (!boot_cpu_has(X86_FEATURE_LBRV)) {\r\nvcpu_unimpl(vcpu, "%s: MSR_IA32_DEBUGCTL 0x%llx, nop\n",\r\n__func__, data);\r\nbreak;\r\n}\r\nif (data & DEBUGCTL_RESERVED_BITS)\r\nreturn 1;\r\nsvm->vmcb->save.dbgctl = data;\r\nmark_dirty(svm->vmcb, VMCB_LBR);\r\nif (data & (1ULL<<0))\r\nsvm_enable_lbrv(svm);\r\nelse\r\nsvm_disable_lbrv(svm);\r\nbreak;\r\ncase MSR_VM_HSAVE_PA:\r\nsvm->nested.hsave_msr = data;\r\nbreak;\r\ncase MSR_VM_CR:\r\nreturn svm_set_vm_cr(vcpu, data);\r\ncase MSR_VM_IGNNE:\r\nvcpu_unimpl(vcpu, "unimplemented wrmsr: 0x%x data 0x%llx\n", ecx, data);\r\nbreak;\r\ndefault:\r\nreturn kvm_set_msr_common(vcpu, msr);\r\n}\r\nreturn 0;\r\n}\r\nstatic int wrmsr_interception(struct vcpu_svm *svm)\r\n{\r\nstruct msr_data msr;\r\nu32 ecx = svm->vcpu.arch.regs[VCPU_REGS_RCX];\r\nu64 data = (svm->vcpu.arch.regs[VCPU_REGS_RAX] & -1u)\r\n| ((u64)(svm->vcpu.arch.regs[VCPU_REGS_RDX] & -1u) << 32);\r\nmsr.data = data;\r\nmsr.index = ecx;\r\nmsr.host_initiated = false;\r\nsvm->next_rip = kvm_rip_read(&svm->vcpu) + 2;\r\nif (svm_set_msr(&svm->vcpu, &msr)) {\r\ntrace_kvm_msr_write_ex(ecx, data);\r\nkvm_inject_gp(&svm->vcpu, 0);\r\n} else {\r\ntrace_kvm_msr_write(ecx, data);\r\nskip_emulated_instruction(&svm->vcpu);\r\n}\r\nreturn 1;\r\n}\r\nstatic int msr_interception(struct vcpu_svm *svm)\r\n{\r\nif (svm->vmcb->control.exit_info_1)\r\nreturn wrmsr_interception(svm);\r\nelse\r\nreturn rdmsr_interception(svm);\r\n}\r\nstatic int interrupt_window_interception(struct vcpu_svm *svm)\r\n{\r\nstruct kvm_run *kvm_run = svm->vcpu.run;\r\nkvm_make_request(KVM_REQ_EVENT, &svm->vcpu);\r\nsvm_clear_vintr(svm);\r\nsvm->vmcb->control.int_ctl &= ~V_IRQ_MASK;\r\nmark_dirty(svm->vmcb, VMCB_INTR);\r\n++svm->vcpu.stat.irq_window_exits;\r\nif (!irqchip_in_kernel(svm->vcpu.kvm) &&\r\nkvm_run->request_interrupt_window &&\r\n!kvm_cpu_has_interrupt(&svm->vcpu)) {\r\nkvm_run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int pause_interception(struct vcpu_svm *svm)\r\n{\r\nkvm_vcpu_on_spin(&(svm->vcpu));\r\nreturn 1;\r\n}\r\nstatic void dump_vmcb(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nstruct vmcb_control_area *control = &svm->vmcb->control;\r\nstruct vmcb_save_area *save = &svm->vmcb->save;\r\npr_err("VMCB Control Area:\n");\r\npr_err("%-20s%04x\n", "cr_read:", control->intercept_cr & 0xffff);\r\npr_err("%-20s%04x\n", "cr_write:", control->intercept_cr >> 16);\r\npr_err("%-20s%04x\n", "dr_read:", control->intercept_dr & 0xffff);\r\npr_err("%-20s%04x\n", "dr_write:", control->intercept_dr >> 16);\r\npr_err("%-20s%08x\n", "exceptions:", control->intercept_exceptions);\r\npr_err("%-20s%016llx\n", "intercepts:", control->intercept);\r\npr_err("%-20s%d\n", "pause filter count:", control->pause_filter_count);\r\npr_err("%-20s%016llx\n", "iopm_base_pa:", control->iopm_base_pa);\r\npr_err("%-20s%016llx\n", "msrpm_base_pa:", control->msrpm_base_pa);\r\npr_err("%-20s%016llx\n", "tsc_offset:", control->tsc_offset);\r\npr_err("%-20s%d\n", "asid:", control->asid);\r\npr_err("%-20s%d\n", "tlb_ctl:", control->tlb_ctl);\r\npr_err("%-20s%08x\n", "int_ctl:", control->int_ctl);\r\npr_err("%-20s%08x\n", "int_vector:", control->int_vector);\r\npr_err("%-20s%08x\n", "int_state:", control->int_state);\r\npr_err("%-20s%08x\n", "exit_code:", control->exit_code);\r\npr_err("%-20s%016llx\n", "exit_info1:", control->exit_info_1);\r\npr_err("%-20s%016llx\n", "exit_info2:", control->exit_info_2);\r\npr_err("%-20s%08x\n", "exit_int_info:", control->exit_int_info);\r\npr_err("%-20s%08x\n", "exit_int_info_err:", control->exit_int_info_err);\r\npr_err("%-20s%lld\n", "nested_ctl:", control->nested_ctl);\r\npr_err("%-20s%016llx\n", "nested_cr3:", control->nested_cr3);\r\npr_err("%-20s%08x\n", "event_inj:", control->event_inj);\r\npr_err("%-20s%08x\n", "event_inj_err:", control->event_inj_err);\r\npr_err("%-20s%lld\n", "lbr_ctl:", control->lbr_ctl);\r\npr_err("%-20s%016llx\n", "next_rip:", control->next_rip);\r\npr_err("VMCB State Save Area:\n");\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"es:",\r\nsave->es.selector, save->es.attrib,\r\nsave->es.limit, save->es.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"cs:",\r\nsave->cs.selector, save->cs.attrib,\r\nsave->cs.limit, save->cs.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"ss:",\r\nsave->ss.selector, save->ss.attrib,\r\nsave->ss.limit, save->ss.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"ds:",\r\nsave->ds.selector, save->ds.attrib,\r\nsave->ds.limit, save->ds.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"fs:",\r\nsave->fs.selector, save->fs.attrib,\r\nsave->fs.limit, save->fs.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"gs:",\r\nsave->gs.selector, save->gs.attrib,\r\nsave->gs.limit, save->gs.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"gdtr:",\r\nsave->gdtr.selector, save->gdtr.attrib,\r\nsave->gdtr.limit, save->gdtr.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"ldtr:",\r\nsave->ldtr.selector, save->ldtr.attrib,\r\nsave->ldtr.limit, save->ldtr.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"idtr:",\r\nsave->idtr.selector, save->idtr.attrib,\r\nsave->idtr.limit, save->idtr.base);\r\npr_err("%-5s s: %04x a: %04x l: %08x b: %016llx\n",\r\n"tr:",\r\nsave->tr.selector, save->tr.attrib,\r\nsave->tr.limit, save->tr.base);\r\npr_err("cpl: %d efer: %016llx\n",\r\nsave->cpl, save->efer);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"cr0:", save->cr0, "cr2:", save->cr2);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"cr3:", save->cr3, "cr4:", save->cr4);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"dr6:", save->dr6, "dr7:", save->dr7);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"rip:", save->rip, "rflags:", save->rflags);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"rsp:", save->rsp, "rax:", save->rax);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"star:", save->star, "lstar:", save->lstar);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"cstar:", save->cstar, "sfmask:", save->sfmask);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"kernel_gs_base:", save->kernel_gs_base,\r\n"sysenter_cs:", save->sysenter_cs);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"sysenter_esp:", save->sysenter_esp,\r\n"sysenter_eip:", save->sysenter_eip);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"gpat:", save->g_pat, "dbgctl:", save->dbgctl);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"br_from:", save->br_from, "br_to:", save->br_to);\r\npr_err("%-15s %016llx %-13s %016llx\n",\r\n"excp_from:", save->last_excp_from,\r\n"excp_to:", save->last_excp_to);\r\n}\r\nstatic void svm_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)\r\n{\r\nstruct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;\r\n*info1 = control->exit_info_1;\r\n*info2 = control->exit_info_2;\r\n}\r\nstatic int handle_exit(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nstruct kvm_run *kvm_run = vcpu->run;\r\nu32 exit_code = svm->vmcb->control.exit_code;\r\nif (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))\r\nvcpu->arch.cr0 = svm->vmcb->save.cr0;\r\nif (npt_enabled)\r\nvcpu->arch.cr3 = svm->vmcb->save.cr3;\r\nif (unlikely(svm->nested.exit_required)) {\r\nnested_svm_vmexit(svm);\r\nsvm->nested.exit_required = false;\r\nreturn 1;\r\n}\r\nif (is_guest_mode(vcpu)) {\r\nint vmexit;\r\ntrace_kvm_nested_vmexit(svm->vmcb->save.rip, exit_code,\r\nsvm->vmcb->control.exit_info_1,\r\nsvm->vmcb->control.exit_info_2,\r\nsvm->vmcb->control.exit_int_info,\r\nsvm->vmcb->control.exit_int_info_err,\r\nKVM_ISA_SVM);\r\nvmexit = nested_svm_exit_special(svm);\r\nif (vmexit == NESTED_EXIT_CONTINUE)\r\nvmexit = nested_svm_exit_handled(svm);\r\nif (vmexit == NESTED_EXIT_DONE)\r\nreturn 1;\r\n}\r\nsvm_complete_interrupts(svm);\r\nif (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {\r\nkvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;\r\nkvm_run->fail_entry.hardware_entry_failure_reason\r\n= svm->vmcb->control.exit_code;\r\npr_err("KVM: FAILED VMRUN WITH VMCB:\n");\r\ndump_vmcb(vcpu);\r\nreturn 0;\r\n}\r\nif (is_external_interrupt(svm->vmcb->control.exit_int_info) &&\r\nexit_code != SVM_EXIT_EXCP_BASE + PF_VECTOR &&\r\nexit_code != SVM_EXIT_NPF && exit_code != SVM_EXIT_TASK_SWITCH &&\r\nexit_code != SVM_EXIT_INTR && exit_code != SVM_EXIT_NMI)\r\nprintk(KERN_ERR "%s: unexpected exit_int_info 0x%x "\r\n"exit_code 0x%x\n",\r\n__func__, svm->vmcb->control.exit_int_info,\r\nexit_code);\r\nif (exit_code >= ARRAY_SIZE(svm_exit_handlers)\r\n|| !svm_exit_handlers[exit_code]) {\r\nkvm_run->exit_reason = KVM_EXIT_UNKNOWN;\r\nkvm_run->hw.hardware_exit_reason = exit_code;\r\nreturn 0;\r\n}\r\nreturn svm_exit_handlers[exit_code](svm);\r\n}\r\nstatic void reload_tss(struct kvm_vcpu *vcpu)\r\n{\r\nint cpu = raw_smp_processor_id();\r\nstruct svm_cpu_data *sd = per_cpu(svm_data, cpu);\r\nsd->tss_desc->type = 9;\r\nload_TR_desc();\r\n}\r\nstatic void pre_svm_run(struct vcpu_svm *svm)\r\n{\r\nint cpu = raw_smp_processor_id();\r\nstruct svm_cpu_data *sd = per_cpu(svm_data, cpu);\r\nif (svm->asid_generation != sd->asid_generation)\r\nnew_asid(svm, sd);\r\n}\r\nstatic void svm_inject_nmi(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;\r\nvcpu->arch.hflags |= HF_NMI_MASK;\r\nset_intercept(svm, INTERCEPT_IRET);\r\n++vcpu->stat.nmi_injections;\r\n}\r\nstatic inline void svm_inject_irq(struct vcpu_svm *svm, int irq)\r\n{\r\nstruct vmcb_control_area *control;\r\ncontrol = &svm->vmcb->control;\r\ncontrol->int_vector = irq;\r\ncontrol->int_ctl &= ~V_INTR_PRIO_MASK;\r\ncontrol->int_ctl |= V_IRQ_MASK |\r\n(( 0xf) << V_INTR_PRIO_SHIFT);\r\nmark_dirty(svm->vmcb, VMCB_INTR);\r\n}\r\nstatic void svm_set_irq(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nBUG_ON(!(gif_set(svm)));\r\ntrace_kvm_inj_virq(vcpu->arch.interrupt.nr);\r\n++vcpu->stat.irq_injections;\r\nsvm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |\r\nSVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_INTR;\r\n}\r\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (is_guest_mode(vcpu) && (vcpu->arch.hflags & HF_VINTR_MASK))\r\nreturn;\r\nif (irr == -1)\r\nreturn;\r\nif (tpr >= irr)\r\nset_cr_intercept(svm, INTERCEPT_CR8_WRITE);\r\n}\r\nstatic void svm_set_virtual_x2apic_mode(struct kvm_vcpu *vcpu, bool set)\r\n{\r\nreturn;\r\n}\r\nstatic int svm_vm_has_apicv(struct kvm *kvm)\r\n{\r\nreturn 0;\r\n}\r\nstatic void svm_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)\r\n{\r\nreturn;\r\n}\r\nstatic void svm_hwapic_isr_update(struct kvm *kvm, int isr)\r\n{\r\nreturn;\r\n}\r\nstatic void svm_sync_pir_to_irr(struct kvm_vcpu *vcpu)\r\n{\r\nreturn;\r\n}\r\nstatic int svm_nmi_allowed(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nstruct vmcb *vmcb = svm->vmcb;\r\nint ret;\r\nret = !(vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK) &&\r\n!(svm->vcpu.arch.hflags & HF_NMI_MASK);\r\nret = ret && gif_set(svm) && nested_svm_nmi(svm);\r\nreturn ret;\r\n}\r\nstatic bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nreturn !!(svm->vcpu.arch.hflags & HF_NMI_MASK);\r\n}\r\nstatic void svm_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (masked) {\r\nsvm->vcpu.arch.hflags |= HF_NMI_MASK;\r\nset_intercept(svm, INTERCEPT_IRET);\r\n} else {\r\nsvm->vcpu.arch.hflags &= ~HF_NMI_MASK;\r\nclr_intercept(svm, INTERCEPT_IRET);\r\n}\r\n}\r\nstatic int svm_interrupt_allowed(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nstruct vmcb *vmcb = svm->vmcb;\r\nint ret;\r\nif (!gif_set(svm) ||\r\n(vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK))\r\nreturn 0;\r\nret = !!(kvm_get_rflags(vcpu) & X86_EFLAGS_IF);\r\nif (is_guest_mode(vcpu))\r\nreturn ret && !(svm->vcpu.arch.hflags & HF_VINTR_MASK);\r\nreturn ret;\r\n}\r\nstatic int enable_irq_window(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (gif_set(svm) && nested_svm_intr(svm)) {\r\nsvm_set_vintr(svm);\r\nsvm_inject_irq(svm, 0x0);\r\n}\r\nreturn 0;\r\n}\r\nstatic int enable_nmi_window(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif ((svm->vcpu.arch.hflags & (HF_NMI_MASK | HF_IRET_MASK))\r\n== HF_NMI_MASK)\r\nreturn 0;\r\nsvm->nmi_singlestep = true;\r\nsvm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);\r\nupdate_db_bp_intercept(vcpu);\r\nreturn 0;\r\n}\r\nstatic int svm_set_tss_addr(struct kvm *kvm, unsigned int addr)\r\n{\r\nreturn 0;\r\n}\r\nstatic void svm_flush_tlb(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (static_cpu_has(X86_FEATURE_FLUSHBYASID))\r\nsvm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;\r\nelse\r\nsvm->asid_generation--;\r\n}\r\nstatic void svm_prepare_guest_switch(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nif (is_guest_mode(vcpu) && (vcpu->arch.hflags & HF_VINTR_MASK))\r\nreturn;\r\nif (!is_cr_intercept(svm, INTERCEPT_CR8_WRITE)) {\r\nint cr8 = svm->vmcb->control.int_ctl & V_TPR_MASK;\r\nkvm_set_cr8(vcpu, cr8);\r\n}\r\n}\r\nstatic inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nu64 cr8;\r\nif (is_guest_mode(vcpu) && (vcpu->arch.hflags & HF_VINTR_MASK))\r\nreturn;\r\ncr8 = kvm_get_cr8(vcpu);\r\nsvm->vmcb->control.int_ctl &= ~V_TPR_MASK;\r\nsvm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;\r\n}\r\nstatic void svm_complete_interrupts(struct vcpu_svm *svm)\r\n{\r\nu8 vector;\r\nint type;\r\nu32 exitintinfo = svm->vmcb->control.exit_int_info;\r\nunsigned int3_injected = svm->int3_injected;\r\nsvm->int3_injected = 0;\r\nif ((svm->vcpu.arch.hflags & HF_IRET_MASK)\r\n&& kvm_rip_read(&svm->vcpu) != svm->nmi_iret_rip) {\r\nsvm->vcpu.arch.hflags &= ~(HF_NMI_MASK | HF_IRET_MASK);\r\nkvm_make_request(KVM_REQ_EVENT, &svm->vcpu);\r\n}\r\nsvm->vcpu.arch.nmi_injected = false;\r\nkvm_clear_exception_queue(&svm->vcpu);\r\nkvm_clear_interrupt_queue(&svm->vcpu);\r\nif (!(exitintinfo & SVM_EXITINTINFO_VALID))\r\nreturn;\r\nkvm_make_request(KVM_REQ_EVENT, &svm->vcpu);\r\nvector = exitintinfo & SVM_EXITINTINFO_VEC_MASK;\r\ntype = exitintinfo & SVM_EXITINTINFO_TYPE_MASK;\r\nswitch (type) {\r\ncase SVM_EXITINTINFO_TYPE_NMI:\r\nsvm->vcpu.arch.nmi_injected = true;\r\nbreak;\r\ncase SVM_EXITINTINFO_TYPE_EXEPT:\r\nif (kvm_exception_is_soft(vector)) {\r\nif (vector == BP_VECTOR && int3_injected &&\r\nkvm_is_linear_rip(&svm->vcpu, svm->int3_rip))\r\nkvm_rip_write(&svm->vcpu,\r\nkvm_rip_read(&svm->vcpu) -\r\nint3_injected);\r\nbreak;\r\n}\r\nif (exitintinfo & SVM_EXITINTINFO_VALID_ERR) {\r\nu32 err = svm->vmcb->control.exit_int_info_err;\r\nkvm_requeue_exception_e(&svm->vcpu, vector, err);\r\n} else\r\nkvm_requeue_exception(&svm->vcpu, vector);\r\nbreak;\r\ncase SVM_EXITINTINFO_TYPE_INTR:\r\nkvm_queue_interrupt(&svm->vcpu, vector, false);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void svm_cancel_injection(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nstruct vmcb_control_area *control = &svm->vmcb->control;\r\ncontrol->exit_int_info = control->event_inj;\r\ncontrol->exit_int_info_err = control->event_inj_err;\r\ncontrol->event_inj = 0;\r\nsvm_complete_interrupts(svm);\r\n}\r\nstatic void svm_vcpu_run(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];\r\nsvm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];\r\nsvm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];\r\nif (unlikely(svm->nested.exit_required))\r\nreturn;\r\npre_svm_run(svm);\r\nsync_lapic_to_cr8(vcpu);\r\nsvm->vmcb->save.cr2 = vcpu->arch.cr2;\r\nclgi();\r\nlocal_irq_enable();\r\nasm volatile (\r\n"push %%" _ASM_BP "; \n\t"\r\n"mov %c[rbx](%[svm]), %%" _ASM_BX " \n\t"\r\n"mov %c[rcx](%[svm]), %%" _ASM_CX " \n\t"\r\n"mov %c[rdx](%[svm]), %%" _ASM_DX " \n\t"\r\n"mov %c[rsi](%[svm]), %%" _ASM_SI " \n\t"\r\n"mov %c[rdi](%[svm]), %%" _ASM_DI " \n\t"\r\n"mov %c[rbp](%[svm]), %%" _ASM_BP " \n\t"\r\n#ifdef CONFIG_X86_64\r\n"mov %c[r8](%[svm]), %%r8 \n\t"\r\n"mov %c[r9](%[svm]), %%r9 \n\t"\r\n"mov %c[r10](%[svm]), %%r10 \n\t"\r\n"mov %c[r11](%[svm]), %%r11 \n\t"\r\n"mov %c[r12](%[svm]), %%r12 \n\t"\r\n"mov %c[r13](%[svm]), %%r13 \n\t"\r\n"mov %c[r14](%[svm]), %%r14 \n\t"\r\n"mov %c[r15](%[svm]), %%r15 \n\t"\r\n#endif\r\n"push %%" _ASM_AX " \n\t"\r\n"mov %c[vmcb](%[svm]), %%" _ASM_AX " \n\t"\r\n__ex(SVM_VMLOAD) "\n\t"\r\n__ex(SVM_VMRUN) "\n\t"\r\n__ex(SVM_VMSAVE) "\n\t"\r\n"pop %%" _ASM_AX " \n\t"\r\n"mov %%" _ASM_BX ", %c[rbx](%[svm]) \n\t"\r\n"mov %%" _ASM_CX ", %c[rcx](%[svm]) \n\t"\r\n"mov %%" _ASM_DX ", %c[rdx](%[svm]) \n\t"\r\n"mov %%" _ASM_SI ", %c[rsi](%[svm]) \n\t"\r\n"mov %%" _ASM_DI ", %c[rdi](%[svm]) \n\t"\r\n"mov %%" _ASM_BP ", %c[rbp](%[svm]) \n\t"\r\n#ifdef CONFIG_X86_64\r\n"mov %%r8, %c[r8](%[svm]) \n\t"\r\n"mov %%r9, %c[r9](%[svm]) \n\t"\r\n"mov %%r10, %c[r10](%[svm]) \n\t"\r\n"mov %%r11, %c[r11](%[svm]) \n\t"\r\n"mov %%r12, %c[r12](%[svm]) \n\t"\r\n"mov %%r13, %c[r13](%[svm]) \n\t"\r\n"mov %%r14, %c[r14](%[svm]) \n\t"\r\n"mov %%r15, %c[r15](%[svm]) \n\t"\r\n#endif\r\n"pop %%" _ASM_BP\r\n:\r\n: [svm]"a"(svm),\r\n[vmcb]"i"(offsetof(struct vcpu_svm, vmcb_pa)),\r\n[rbx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBX])),\r\n[rcx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RCX])),\r\n[rdx]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDX])),\r\n[rsi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RSI])),\r\n[rdi]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDI])),\r\n[rbp]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBP]))\r\n#ifdef CONFIG_X86_64\r\n, [r8]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R8])),\r\n[r9]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R9])),\r\n[r10]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R10])),\r\n[r11]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R11])),\r\n[r12]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R12])),\r\n[r13]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R13])),\r\n[r14]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R14])),\r\n[r15]"i"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R15]))\r\n#endif\r\n: "cc", "memory"\r\n#ifdef CONFIG_X86_64\r\n, "rbx", "rcx", "rdx", "rsi", "rdi"\r\n, "r8", "r9", "r10", "r11" , "r12", "r13", "r14", "r15"\r\n#else\r\n, "ebx", "ecx", "edx", "esi", "edi"\r\n#endif\r\n);\r\n#ifdef CONFIG_X86_64\r\nwrmsrl(MSR_GS_BASE, svm->host.gs_base);\r\n#else\r\nloadsegment(fs, svm->host.fs);\r\n#ifndef CONFIG_X86_32_LAZY_GS\r\nloadsegment(gs, svm->host.gs);\r\n#endif\r\n#endif\r\nreload_tss(vcpu);\r\nlocal_irq_disable();\r\nvcpu->arch.cr2 = svm->vmcb->save.cr2;\r\nvcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;\r\nvcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;\r\nvcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;\r\ntrace_kvm_exit(svm->vmcb->control.exit_code, vcpu, KVM_ISA_SVM);\r\nif (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))\r\nkvm_before_handle_nmi(&svm->vcpu);\r\nstgi();\r\nif (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))\r\nkvm_after_handle_nmi(&svm->vcpu);\r\nsync_cr8_to_lapic(vcpu);\r\nsvm->next_rip = 0;\r\nsvm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;\r\nif (svm->vmcb->control.exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR)\r\nsvm->apf_reason = kvm_read_and_reset_pf_reason();\r\nif (npt_enabled) {\r\nvcpu->arch.regs_avail &= ~(1 << VCPU_EXREG_PDPTR);\r\nvcpu->arch.regs_dirty &= ~(1 << VCPU_EXREG_PDPTR);\r\n}\r\nif (unlikely(svm->vmcb->control.exit_code ==\r\nSVM_EXIT_EXCP_BASE + MC_VECTOR))\r\nsvm_handle_mce(svm);\r\nmark_all_clean(svm->vmcb);\r\n}\r\nstatic void svm_set_cr3(struct kvm_vcpu *vcpu, unsigned long root)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->save.cr3 = root;\r\nmark_dirty(svm->vmcb, VMCB_CR);\r\nsvm_flush_tlb(vcpu);\r\n}\r\nstatic void set_tdp_cr3(struct kvm_vcpu *vcpu, unsigned long root)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nsvm->vmcb->control.nested_cr3 = root;\r\nmark_dirty(svm->vmcb, VMCB_NPT);\r\nsvm->vmcb->save.cr3 = kvm_read_cr3(vcpu);\r\nmark_dirty(svm->vmcb, VMCB_CR);\r\nsvm_flush_tlb(vcpu);\r\n}\r\nstatic int is_disabled(void)\r\n{\r\nu64 vm_cr;\r\nrdmsrl(MSR_VM_CR, vm_cr);\r\nif (vm_cr & (1 << SVM_VM_CR_SVM_DISABLE))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void\r\nsvm_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)\r\n{\r\nhypercall[0] = 0x0f;\r\nhypercall[1] = 0x01;\r\nhypercall[2] = 0xd9;\r\n}\r\nstatic void svm_check_processor_compat(void *rtn)\r\n{\r\n*(int *)rtn = 0;\r\n}\r\nstatic bool svm_cpu_has_accelerated_tpr(void)\r\n{\r\nreturn false;\r\n}\r\nstatic u64 svm_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)\r\n{\r\nreturn 0;\r\n}\r\nstatic void svm_cpuid_update(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic void svm_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)\r\n{\r\nswitch (func) {\r\ncase 0x80000001:\r\nif (nested)\r\nentry->ecx |= (1 << 2);\r\nbreak;\r\ncase 0x8000000A:\r\nentry->eax = 1;\r\nentry->ebx = 8;\r\nentry->ecx = 0;\r\nentry->edx = 0;\r\nif (boot_cpu_has(X86_FEATURE_NRIPS))\r\nentry->edx |= SVM_FEATURE_NRIP;\r\nif (npt_enabled)\r\nentry->edx |= SVM_FEATURE_NPT;\r\nbreak;\r\n}\r\n}\r\nstatic int svm_get_lpage_level(void)\r\n{\r\nreturn PT_PDPE_LEVEL;\r\n}\r\nstatic bool svm_rdtscp_supported(void)\r\n{\r\nreturn false;\r\n}\r\nstatic bool svm_invpcid_supported(void)\r\n{\r\nreturn false;\r\n}\r\nstatic bool svm_has_wbinvd_exit(void)\r\n{\r\nreturn true;\r\n}\r\nstatic void svm_fpu_deactivate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nset_exception_intercept(svm, NM_VECTOR);\r\nupdate_cr0_intercept(svm);\r\n}\r\nstatic int svm_check_intercept(struct kvm_vcpu *vcpu,\r\nstruct x86_instruction_info *info,\r\nenum x86_intercept_stage stage)\r\n{\r\nstruct vcpu_svm *svm = to_svm(vcpu);\r\nint vmexit, ret = X86EMUL_CONTINUE;\r\nstruct __x86_intercept icpt_info;\r\nstruct vmcb *vmcb = svm->vmcb;\r\nif (info->intercept >= ARRAY_SIZE(x86_intercept_map))\r\ngoto out;\r\nicpt_info = x86_intercept_map[info->intercept];\r\nif (stage != icpt_info.stage)\r\ngoto out;\r\nswitch (icpt_info.exit_code) {\r\ncase SVM_EXIT_READ_CR0:\r\nif (info->intercept == x86_intercept_cr_read)\r\nicpt_info.exit_code += info->modrm_reg;\r\nbreak;\r\ncase SVM_EXIT_WRITE_CR0: {\r\nunsigned long cr0, val;\r\nu64 intercept;\r\nif (info->intercept == x86_intercept_cr_write)\r\nicpt_info.exit_code += info->modrm_reg;\r\nif (icpt_info.exit_code != SVM_EXIT_WRITE_CR0)\r\nbreak;\r\nintercept = svm->nested.intercept;\r\nif (!(intercept & (1ULL << INTERCEPT_SELECTIVE_CR0)))\r\nbreak;\r\ncr0 = vcpu->arch.cr0 & ~SVM_CR0_SELECTIVE_MASK;\r\nval = info->src_val & ~SVM_CR0_SELECTIVE_MASK;\r\nif (info->intercept == x86_intercept_lmsw) {\r\ncr0 &= 0xfUL;\r\nval &= 0xfUL;\r\nif (cr0 & X86_CR0_PE)\r\nval |= X86_CR0_PE;\r\n}\r\nif (cr0 ^ val)\r\nicpt_info.exit_code = SVM_EXIT_CR0_SEL_WRITE;\r\nbreak;\r\n}\r\ncase SVM_EXIT_READ_DR0:\r\ncase SVM_EXIT_WRITE_DR0:\r\nicpt_info.exit_code += info->modrm_reg;\r\nbreak;\r\ncase SVM_EXIT_MSR:\r\nif (info->intercept == x86_intercept_wrmsr)\r\nvmcb->control.exit_info_1 = 1;\r\nelse\r\nvmcb->control.exit_info_1 = 0;\r\nbreak;\r\ncase SVM_EXIT_PAUSE:\r\nif (info->rep_prefix != REPE_PREFIX)\r\ngoto out;\r\ncase SVM_EXIT_IOIO: {\r\nu64 exit_info;\r\nu32 bytes;\r\nexit_info = (vcpu->arch.regs[VCPU_REGS_RDX] & 0xffff) << 16;\r\nif (info->intercept == x86_intercept_in ||\r\ninfo->intercept == x86_intercept_ins) {\r\nexit_info |= SVM_IOIO_TYPE_MASK;\r\nbytes = info->src_bytes;\r\n} else {\r\nbytes = info->dst_bytes;\r\n}\r\nif (info->intercept == x86_intercept_outs ||\r\ninfo->intercept == x86_intercept_ins)\r\nexit_info |= SVM_IOIO_STR_MASK;\r\nif (info->rep_prefix)\r\nexit_info |= SVM_IOIO_REP_MASK;\r\nbytes = min(bytes, 4u);\r\nexit_info |= bytes << SVM_IOIO_SIZE_SHIFT;\r\nexit_info |= (u32)info->ad_bytes << (SVM_IOIO_ASIZE_SHIFT - 1);\r\nvmcb->control.exit_info_1 = exit_info;\r\nvmcb->control.exit_info_2 = info->next_rip;\r\nbreak;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\nvmcb->control.next_rip = info->next_rip;\r\nvmcb->control.exit_code = icpt_info.exit_code;\r\nvmexit = nested_svm_exit_handled(svm);\r\nret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED\r\n: X86EMUL_CONTINUE;\r\nout:\r\nreturn ret;\r\n}\r\nstatic void svm_handle_external_intr(struct kvm_vcpu *vcpu)\r\n{\r\nlocal_irq_enable();\r\n}\r\nstatic int __init svm_init(void)\r\n{\r\nreturn kvm_init(&svm_x86_ops, sizeof(struct vcpu_svm),\r\n__alignof__(struct vcpu_svm), THIS_MODULE);\r\n}\r\nstatic void __exit svm_exit(void)\r\n{\r\nkvm_exit();\r\n}
