static u32 bcbuf_acks(struct sk_buff *buf)\r\n{\r\nreturn (u32)(unsigned long)TIPC_SKB_CB(buf)->handle;\r\n}\r\nstatic void bcbuf_set_acks(struct sk_buff *buf, u32 acks)\r\n{\r\nTIPC_SKB_CB(buf)->handle = (void *)(unsigned long)acks;\r\n}\r\nstatic void bcbuf_decr_acks(struct sk_buff *buf)\r\n{\r\nbcbuf_set_acks(buf, bcbuf_acks(buf) - 1);\r\n}\r\nvoid tipc_bclink_add_node(u32 addr)\r\n{\r\nspin_lock_bh(&bc_lock);\r\ntipc_nmap_add(&bclink->bcast_nodes, addr);\r\nspin_unlock_bh(&bc_lock);\r\n}\r\nvoid tipc_bclink_remove_node(u32 addr)\r\n{\r\nspin_lock_bh(&bc_lock);\r\ntipc_nmap_remove(&bclink->bcast_nodes, addr);\r\nspin_unlock_bh(&bc_lock);\r\n}\r\nstatic void bclink_set_last_sent(void)\r\n{\r\nif (bcl->next_out)\r\nbcl->fsm_msg_cnt = mod(buf_seqno(bcl->next_out) - 1);\r\nelse\r\nbcl->fsm_msg_cnt = mod(bcl->next_out_no - 1);\r\n}\r\nu32 tipc_bclink_get_last_sent(void)\r\n{\r\nreturn bcl->fsm_msg_cnt;\r\n}\r\nstatic void bclink_update_last_sent(struct tipc_node *node, u32 seqno)\r\n{\r\nnode->bclink.last_sent = less_eq(node->bclink.last_sent, seqno) ?\r\nseqno : node->bclink.last_sent;\r\n}\r\nstruct tipc_node *tipc_bclink_retransmit_to(void)\r\n{\r\nreturn bclink->retransmit_to;\r\n}\r\nstatic void bclink_retransmit_pkt(u32 after, u32 to)\r\n{\r\nstruct sk_buff *buf;\r\nbuf = bcl->first_out;\r\nwhile (buf && less_eq(buf_seqno(buf), after))\r\nbuf = buf->next;\r\ntipc_link_retransmit(bcl, buf, mod(to - after));\r\n}\r\nvoid tipc_bclink_acknowledge(struct tipc_node *n_ptr, u32 acked)\r\n{\r\nstruct sk_buff *crs;\r\nstruct sk_buff *next;\r\nunsigned int released = 0;\r\nspin_lock_bh(&bc_lock);\r\ncrs = bcl->first_out;\r\nif (!crs)\r\ngoto exit;\r\nif (acked == INVALID_LINK_SEQ) {\r\nif (bclink->bcast_nodes.count)\r\nacked = bcl->fsm_msg_cnt;\r\nelse\r\nacked = bcl->next_out_no;\r\n} else {\r\nif (less(acked, buf_seqno(crs)) ||\r\nless(bcl->fsm_msg_cnt, acked) ||\r\nless_eq(acked, n_ptr->bclink.acked))\r\ngoto exit;\r\n}\r\nwhile (crs && less_eq(buf_seqno(crs), n_ptr->bclink.acked))\r\ncrs = crs->next;\r\nwhile (crs && less_eq(buf_seqno(crs), acked)) {\r\nnext = crs->next;\r\nif (crs != bcl->next_out)\r\nbcbuf_decr_acks(crs);\r\nelse {\r\nbcbuf_set_acks(crs, 0);\r\nbcl->next_out = next;\r\nbclink_set_last_sent();\r\n}\r\nif (bcbuf_acks(crs) == 0) {\r\nbcl->first_out = next;\r\nbcl->out_queue_size--;\r\nkfree_skb(crs);\r\nreleased = 1;\r\n}\r\ncrs = next;\r\n}\r\nn_ptr->bclink.acked = acked;\r\nif (unlikely(bcl->next_out)) {\r\ntipc_link_push_queue(bcl);\r\nbclink_set_last_sent();\r\n}\r\nif (unlikely(released && !list_empty(&bcl->waiting_ports)))\r\ntipc_link_wakeup_ports(bcl, 0);\r\nexit:\r\nspin_unlock_bh(&bc_lock);\r\n}\r\nvoid tipc_bclink_update_link_state(struct tipc_node *n_ptr, u32 last_sent)\r\n{\r\nstruct sk_buff *buf;\r\nif (less_eq(last_sent, n_ptr->bclink.last_in))\r\nreturn;\r\nbclink_update_last_sent(n_ptr, last_sent);\r\nif (n_ptr->bclink.last_sent == n_ptr->bclink.last_in)\r\nreturn;\r\nif ((++n_ptr->bclink.oos_state) == 1) {\r\nif (n_ptr->bclink.deferred_size < (TIPC_MIN_LINK_WIN / 2))\r\nreturn;\r\nn_ptr->bclink.oos_state++;\r\n}\r\nif (n_ptr->bclink.oos_state & 0x1)\r\nreturn;\r\nbuf = tipc_buf_acquire(INT_H_SIZE);\r\nif (buf) {\r\nstruct tipc_msg *msg = buf_msg(buf);\r\ntipc_msg_init(msg, BCAST_PROTOCOL, STATE_MSG,\r\nINT_H_SIZE, n_ptr->addr);\r\nmsg_set_non_seq(msg, 1);\r\nmsg_set_mc_netid(msg, tipc_net_id);\r\nmsg_set_bcast_ack(msg, n_ptr->bclink.last_in);\r\nmsg_set_bcgap_after(msg, n_ptr->bclink.last_in);\r\nmsg_set_bcgap_to(msg, n_ptr->bclink.deferred_head\r\n? buf_seqno(n_ptr->bclink.deferred_head) - 1\r\n: n_ptr->bclink.last_sent);\r\nspin_lock_bh(&bc_lock);\r\ntipc_bearer_send(&bcbearer->bearer, buf, NULL);\r\nbcl->stats.sent_nacks++;\r\nspin_unlock_bh(&bc_lock);\r\nkfree_skb(buf);\r\nn_ptr->bclink.oos_state++;\r\n}\r\n}\r\nstatic void bclink_peek_nack(struct tipc_msg *msg)\r\n{\r\nstruct tipc_node *n_ptr = tipc_node_find(msg_destnode(msg));\r\nif (unlikely(!n_ptr))\r\nreturn;\r\ntipc_node_lock(n_ptr);\r\nif (n_ptr->bclink.recv_permitted &&\r\n(n_ptr->bclink.last_in != n_ptr->bclink.last_sent) &&\r\n(n_ptr->bclink.last_in == msg_bcgap_after(msg)))\r\nn_ptr->bclink.oos_state = 2;\r\ntipc_node_unlock(n_ptr);\r\n}\r\nint tipc_bclink_send_msg(struct sk_buff *buf)\r\n{\r\nint res;\r\nspin_lock_bh(&bc_lock);\r\nif (!bclink->bcast_nodes.count) {\r\nres = msg_data_sz(buf_msg(buf));\r\nkfree_skb(buf);\r\ngoto exit;\r\n}\r\nres = tipc_link_send_buf(bcl, buf);\r\nif (likely(res >= 0)) {\r\nbclink_set_last_sent();\r\nbcl->stats.queue_sz_counts++;\r\nbcl->stats.accu_queue_sz += bcl->out_queue_size;\r\n}\r\nexit:\r\nspin_unlock_bh(&bc_lock);\r\nreturn res;\r\n}\r\nstatic void bclink_accept_pkt(struct tipc_node *node, u32 seqno)\r\n{\r\nbclink_update_last_sent(node, seqno);\r\nnode->bclink.last_in = seqno;\r\nnode->bclink.oos_state = 0;\r\nbcl->stats.recv_info++;\r\nif (((seqno - tipc_own_addr) % TIPC_MIN_LINK_WIN) == 0) {\r\ntipc_link_send_proto_msg(\r\nnode->active_links[node->addr & 1],\r\nSTATE_MSG, 0, 0, 0, 0, 0);\r\nbcl->stats.sent_acks++;\r\n}\r\n}\r\nvoid tipc_bclink_recv_pkt(struct sk_buff *buf)\r\n{\r\nstruct tipc_msg *msg = buf_msg(buf);\r\nstruct tipc_node *node;\r\nu32 next_in;\r\nu32 seqno;\r\nint deferred;\r\nif (msg_mc_netid(msg) != tipc_net_id)\r\ngoto exit;\r\nnode = tipc_node_find(msg_prevnode(msg));\r\nif (unlikely(!node))\r\ngoto exit;\r\ntipc_node_lock(node);\r\nif (unlikely(!node->bclink.recv_permitted))\r\ngoto unlock;\r\nif (unlikely(msg_user(msg) == BCAST_PROTOCOL)) {\r\nif (msg_type(msg) != STATE_MSG)\r\ngoto unlock;\r\nif (msg_destnode(msg) == tipc_own_addr) {\r\ntipc_bclink_acknowledge(node, msg_bcast_ack(msg));\r\ntipc_node_unlock(node);\r\nspin_lock_bh(&bc_lock);\r\nbcl->stats.recv_nacks++;\r\nbclink->retransmit_to = node;\r\nbclink_retransmit_pkt(msg_bcgap_after(msg),\r\nmsg_bcgap_to(msg));\r\nspin_unlock_bh(&bc_lock);\r\n} else {\r\ntipc_node_unlock(node);\r\nbclink_peek_nack(msg);\r\n}\r\ngoto exit;\r\n}\r\nseqno = msg_seqno(msg);\r\nnext_in = mod(node->bclink.last_in + 1);\r\nif (likely(seqno == next_in)) {\r\nreceive:\r\nif (likely(msg_isdata(msg))) {\r\nspin_lock_bh(&bc_lock);\r\nbclink_accept_pkt(node, seqno);\r\nspin_unlock_bh(&bc_lock);\r\ntipc_node_unlock(node);\r\nif (likely(msg_mcast(msg)))\r\ntipc_port_recv_mcast(buf, NULL);\r\nelse\r\nkfree_skb(buf);\r\n} else if (msg_user(msg) == MSG_BUNDLER) {\r\nspin_lock_bh(&bc_lock);\r\nbclink_accept_pkt(node, seqno);\r\nbcl->stats.recv_bundles++;\r\nbcl->stats.recv_bundled += msg_msgcnt(msg);\r\nspin_unlock_bh(&bc_lock);\r\ntipc_node_unlock(node);\r\ntipc_link_recv_bundle(buf);\r\n} else if (msg_user(msg) == MSG_FRAGMENTER) {\r\nint ret = tipc_link_recv_fragment(&node->bclink.defragm,\r\n&buf, &msg);\r\nif (ret < 0)\r\ngoto unlock;\r\nspin_lock_bh(&bc_lock);\r\nbclink_accept_pkt(node, seqno);\r\nbcl->stats.recv_fragments++;\r\nif (ret > 0)\r\nbcl->stats.recv_fragmented++;\r\nspin_unlock_bh(&bc_lock);\r\ntipc_node_unlock(node);\r\ntipc_net_route_msg(buf);\r\n} else if (msg_user(msg) == NAME_DISTRIBUTOR) {\r\nspin_lock_bh(&bc_lock);\r\nbclink_accept_pkt(node, seqno);\r\nspin_unlock_bh(&bc_lock);\r\ntipc_node_unlock(node);\r\ntipc_named_recv(buf);\r\n} else {\r\nspin_lock_bh(&bc_lock);\r\nbclink_accept_pkt(node, seqno);\r\nspin_unlock_bh(&bc_lock);\r\ntipc_node_unlock(node);\r\nkfree_skb(buf);\r\n}\r\nbuf = NULL;\r\ntipc_node_lock(node);\r\nif (unlikely(!tipc_node_is_up(node)))\r\ngoto unlock;\r\nif (node->bclink.last_in == node->bclink.last_sent)\r\ngoto unlock;\r\nif (!node->bclink.deferred_head) {\r\nnode->bclink.oos_state = 1;\r\ngoto unlock;\r\n}\r\nmsg = buf_msg(node->bclink.deferred_head);\r\nseqno = msg_seqno(msg);\r\nnext_in = mod(next_in + 1);\r\nif (seqno != next_in)\r\ngoto unlock;\r\nbuf = node->bclink.deferred_head;\r\nnode->bclink.deferred_head = buf->next;\r\nnode->bclink.deferred_size--;\r\ngoto receive;\r\n}\r\nif (less(next_in, seqno)) {\r\ndeferred = tipc_link_defer_pkt(&node->bclink.deferred_head,\r\n&node->bclink.deferred_tail,\r\nbuf);\r\nnode->bclink.deferred_size += deferred;\r\nbclink_update_last_sent(node, seqno);\r\nbuf = NULL;\r\n} else\r\ndeferred = 0;\r\nspin_lock_bh(&bc_lock);\r\nif (deferred)\r\nbcl->stats.deferred_recv++;\r\nelse\r\nbcl->stats.duplicates++;\r\nspin_unlock_bh(&bc_lock);\r\nunlock:\r\ntipc_node_unlock(node);\r\nexit:\r\nkfree_skb(buf);\r\n}\r\nu32 tipc_bclink_acks_missing(struct tipc_node *n_ptr)\r\n{\r\nreturn (n_ptr->bclink.recv_permitted &&\r\n(tipc_bclink_get_last_sent() != n_ptr->bclink.acked));\r\n}\r\nstatic int tipc_bcbearer_send(struct sk_buff *buf, struct tipc_bearer *unused1,\r\nstruct tipc_media_addr *unused2)\r\n{\r\nint bp_index;\r\nif (likely(!msg_non_seq(buf_msg(buf)))) {\r\nstruct tipc_msg *msg;\r\nbcbuf_set_acks(buf, bclink->bcast_nodes.count);\r\nmsg = buf_msg(buf);\r\nmsg_set_non_seq(msg, 1);\r\nmsg_set_mc_netid(msg, tipc_net_id);\r\nbcl->stats.sent_info++;\r\nif (WARN_ON(!bclink->bcast_nodes.count)) {\r\ndump_stack();\r\nreturn 0;\r\n}\r\n}\r\nbcbearer->remains = bclink->bcast_nodes;\r\nfor (bp_index = 0; bp_index < MAX_BEARERS; bp_index++) {\r\nstruct tipc_bearer *p = bcbearer->bpairs[bp_index].primary;\r\nstruct tipc_bearer *s = bcbearer->bpairs[bp_index].secondary;\r\nstruct tipc_bearer *b = p;\r\nstruct sk_buff *tbuf;\r\nif (!p)\r\nbreak;\r\nif (tipc_bearer_blocked(p)) {\r\nif (!s || tipc_bearer_blocked(s))\r\ncontinue;\r\nb = s;\r\n}\r\ntipc_nmap_diff(&bcbearer->remains, &b->nodes,\r\n&bcbearer->remains_new);\r\nif (bcbearer->remains_new.count == bcbearer->remains.count)\r\ncontinue;\r\nif (bp_index == 0) {\r\ntipc_bearer_send(b, buf, &b->bcast_addr);\r\n} else {\r\ntbuf = pskb_copy(buf, GFP_ATOMIC);\r\nif (!tbuf)\r\nbreak;\r\ntipc_bearer_send(b, tbuf, &b->bcast_addr);\r\nkfree_skb(tbuf);\r\n}\r\nif (s) {\r\nbcbearer->bpairs[bp_index].primary = s;\r\nbcbearer->bpairs[bp_index].secondary = p;\r\n}\r\nif (bcbearer->remains_new.count == 0)\r\nbreak;\r\nbcbearer->remains = bcbearer->remains_new;\r\n}\r\nreturn 0;\r\n}\r\nvoid tipc_bcbearer_sort(void)\r\n{\r\nstruct tipc_bcbearer_pair *bp_temp = bcbearer->bpairs_temp;\r\nstruct tipc_bcbearer_pair *bp_curr;\r\nint b_index;\r\nint pri;\r\nspin_lock_bh(&bc_lock);\r\nmemset(bp_temp, 0, sizeof(bcbearer->bpairs_temp));\r\nfor (b_index = 0; b_index < MAX_BEARERS; b_index++) {\r\nstruct tipc_bearer *b = &tipc_bearers[b_index];\r\nif (!b->active || !b->nodes.count)\r\ncontinue;\r\nif (!bp_temp[b->priority].primary)\r\nbp_temp[b->priority].primary = b;\r\nelse\r\nbp_temp[b->priority].secondary = b;\r\n}\r\nbp_curr = bcbearer->bpairs;\r\nmemset(bcbearer->bpairs, 0, sizeof(bcbearer->bpairs));\r\nfor (pri = TIPC_MAX_LINK_PRI; pri >= 0; pri--) {\r\nif (!bp_temp[pri].primary)\r\ncontinue;\r\nbp_curr->primary = bp_temp[pri].primary;\r\nif (bp_temp[pri].secondary) {\r\nif (tipc_nmap_equal(&bp_temp[pri].primary->nodes,\r\n&bp_temp[pri].secondary->nodes)) {\r\nbp_curr->secondary = bp_temp[pri].secondary;\r\n} else {\r\nbp_curr++;\r\nbp_curr->primary = bp_temp[pri].secondary;\r\n}\r\n}\r\nbp_curr++;\r\n}\r\nspin_unlock_bh(&bc_lock);\r\n}\r\nint tipc_bclink_stats(char *buf, const u32 buf_size)\r\n{\r\nint ret;\r\nstruct tipc_stats *s;\r\nif (!bcl)\r\nreturn 0;\r\nspin_lock_bh(&bc_lock);\r\ns = &bcl->stats;\r\nret = tipc_snprintf(buf, buf_size, "Link <%s>\n"\r\n" Window:%u packets\n",\r\nbcl->name, bcl->queue_limit[0]);\r\nret += tipc_snprintf(buf + ret, buf_size - ret,\r\n" RX packets:%u fragments:%u/%u bundles:%u/%u\n",\r\ns->recv_info, s->recv_fragments,\r\ns->recv_fragmented, s->recv_bundles,\r\ns->recv_bundled);\r\nret += tipc_snprintf(buf + ret, buf_size - ret,\r\n" TX packets:%u fragments:%u/%u bundles:%u/%u\n",\r\ns->sent_info, s->sent_fragments,\r\ns->sent_fragmented, s->sent_bundles,\r\ns->sent_bundled);\r\nret += tipc_snprintf(buf + ret, buf_size - ret,\r\n" RX naks:%u defs:%u dups:%u\n",\r\ns->recv_nacks, s->deferred_recv, s->duplicates);\r\nret += tipc_snprintf(buf + ret, buf_size - ret,\r\n" TX naks:%u acks:%u dups:%u\n",\r\ns->sent_nacks, s->sent_acks, s->retransmitted);\r\nret += tipc_snprintf(buf + ret, buf_size - ret,\r\n" Congestion link:%u Send queue max:%u avg:%u\n",\r\ns->link_congs, s->max_queue_sz,\r\ns->queue_sz_counts ?\r\n(s->accu_queue_sz / s->queue_sz_counts) : 0);\r\nspin_unlock_bh(&bc_lock);\r\nreturn ret;\r\n}\r\nint tipc_bclink_reset_stats(void)\r\n{\r\nif (!bcl)\r\nreturn -ENOPROTOOPT;\r\nspin_lock_bh(&bc_lock);\r\nmemset(&bcl->stats, 0, sizeof(bcl->stats));\r\nspin_unlock_bh(&bc_lock);\r\nreturn 0;\r\n}\r\nint tipc_bclink_set_queue_limits(u32 limit)\r\n{\r\nif (!bcl)\r\nreturn -ENOPROTOOPT;\r\nif ((limit < TIPC_MIN_LINK_WIN) || (limit > TIPC_MAX_LINK_WIN))\r\nreturn -EINVAL;\r\nspin_lock_bh(&bc_lock);\r\ntipc_link_set_queue_limits(bcl, limit);\r\nspin_unlock_bh(&bc_lock);\r\nreturn 0;\r\n}\r\nvoid tipc_bclink_init(void)\r\n{\r\nbcbearer->bearer.media = &bcbearer->media;\r\nbcbearer->media.send_msg = tipc_bcbearer_send;\r\nsprintf(bcbearer->media.name, "tipc-broadcast");\r\nINIT_LIST_HEAD(&bcl->waiting_ports);\r\nbcl->next_out_no = 1;\r\nspin_lock_init(&bclink->node.lock);\r\nbcl->owner = &bclink->node;\r\nbcl->max_pkt = MAX_PKT_DEFAULT_MCAST;\r\ntipc_link_set_queue_limits(bcl, BCLINK_WIN_DEFAULT);\r\nspin_lock_init(&bcbearer->bearer.lock);\r\nbcl->b_ptr = &bcbearer->bearer;\r\nbcl->state = WORKING_WORKING;\r\nstrlcpy(bcl->name, tipc_bclink_name, TIPC_MAX_LINK_NAME);\r\n}\r\nvoid tipc_bclink_stop(void)\r\n{\r\nspin_lock_bh(&bc_lock);\r\ntipc_link_stop(bcl);\r\nspin_unlock_bh(&bc_lock);\r\nmemset(bclink, 0, sizeof(*bclink));\r\nmemset(bcbearer, 0, sizeof(*bcbearer));\r\n}\r\nvoid tipc_nmap_add(struct tipc_node_map *nm_ptr, u32 node)\r\n{\r\nint n = tipc_node(node);\r\nint w = n / WSIZE;\r\nu32 mask = (1 << (n % WSIZE));\r\nif ((nm_ptr->map[w] & mask) == 0) {\r\nnm_ptr->count++;\r\nnm_ptr->map[w] |= mask;\r\n}\r\n}\r\nvoid tipc_nmap_remove(struct tipc_node_map *nm_ptr, u32 node)\r\n{\r\nint n = tipc_node(node);\r\nint w = n / WSIZE;\r\nu32 mask = (1 << (n % WSIZE));\r\nif ((nm_ptr->map[w] & mask) != 0) {\r\nnm_ptr->map[w] &= ~mask;\r\nnm_ptr->count--;\r\n}\r\n}\r\nstatic void tipc_nmap_diff(struct tipc_node_map *nm_a,\r\nstruct tipc_node_map *nm_b,\r\nstruct tipc_node_map *nm_diff)\r\n{\r\nint stop = ARRAY_SIZE(nm_a->map);\r\nint w;\r\nint b;\r\nu32 map;\r\nmemset(nm_diff, 0, sizeof(*nm_diff));\r\nfor (w = 0; w < stop; w++) {\r\nmap = nm_a->map[w] ^ (nm_a->map[w] & nm_b->map[w]);\r\nnm_diff->map[w] = map;\r\nif (map != 0) {\r\nfor (b = 0 ; b < WSIZE; b++) {\r\nif (map & (1 << b))\r\nnm_diff->count++;\r\n}\r\n}\r\n}\r\n}\r\nvoid tipc_port_list_add(struct tipc_port_list *pl_ptr, u32 port)\r\n{\r\nstruct tipc_port_list *item = pl_ptr;\r\nint i;\r\nint item_sz = PLSIZE;\r\nint cnt = pl_ptr->count;\r\nfor (; ; cnt -= item_sz, item = item->next) {\r\nif (cnt < PLSIZE)\r\nitem_sz = cnt;\r\nfor (i = 0; i < item_sz; i++)\r\nif (item->ports[i] == port)\r\nreturn;\r\nif (i < PLSIZE) {\r\nitem->ports[i] = port;\r\npl_ptr->count++;\r\nreturn;\r\n}\r\nif (!item->next) {\r\nitem->next = kmalloc(sizeof(*item), GFP_ATOMIC);\r\nif (!item->next) {\r\npr_warn("Incomplete multicast delivery, no memory\n");\r\nreturn;\r\n}\r\nitem->next->next = NULL;\r\n}\r\n}\r\n}\r\nvoid tipc_port_list_free(struct tipc_port_list *pl_ptr)\r\n{\r\nstruct tipc_port_list *item;\r\nstruct tipc_port_list *next;\r\nfor (item = pl_ptr->next; item; item = next) {\r\nnext = item->next;\r\nkfree(item);\r\n}\r\n}
