static bool moving_pred(struct keybuf *buf, struct bkey *k)\r\n{\r\nstruct cache_set *c = container_of(buf, struct cache_set,\r\nmoving_gc_keys);\r\nunsigned i;\r\nfor (i = 0; i < KEY_PTRS(k); i++) {\r\nstruct cache *ca = PTR_CACHE(c, k, i);\r\nstruct bucket *g = PTR_BUCKET(c, k, i);\r\nif (GC_SECTORS_USED(g) < ca->gc_move_threshold)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void moving_io_destructor(struct closure *cl)\r\n{\r\nstruct moving_io *io = container_of(cl, struct moving_io, s.cl);\r\nkfree(io);\r\n}\r\nstatic void write_moving_finish(struct closure *cl)\r\n{\r\nstruct moving_io *io = container_of(cl, struct moving_io, s.cl);\r\nstruct bio *bio = &io->bio.bio;\r\nstruct bio_vec *bv;\r\nint i;\r\nbio_for_each_segment_all(bv, bio, i)\r\n__free_page(bv->bv_page);\r\nif (io->s.op.insert_collision)\r\ntrace_bcache_gc_copy_collision(&io->w->key);\r\nbch_keybuf_del(&io->s.op.c->moving_gc_keys, io->w);\r\natomic_dec_bug(&io->s.op.c->in_flight);\r\nclosure_wake_up(&io->s.op.c->moving_gc_wait);\r\nclosure_return_with_destructor(cl, moving_io_destructor);\r\n}\r\nstatic void read_moving_endio(struct bio *bio, int error)\r\n{\r\nstruct moving_io *io = container_of(bio->bi_private,\r\nstruct moving_io, s.cl);\r\nif (error)\r\nio->s.error = error;\r\nbch_bbio_endio(io->s.op.c, bio, error, "reading data to move");\r\n}\r\nstatic void moving_init(struct moving_io *io)\r\n{\r\nstruct bio *bio = &io->bio.bio;\r\nbio_init(bio);\r\nbio_get(bio);\r\nbio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));\r\nbio->bi_size = KEY_SIZE(&io->w->key) << 9;\r\nbio->bi_max_vecs = DIV_ROUND_UP(KEY_SIZE(&io->w->key),\r\nPAGE_SECTORS);\r\nbio->bi_private = &io->s.cl;\r\nbio->bi_io_vec = bio->bi_inline_vecs;\r\nbch_bio_map(bio, NULL);\r\n}\r\nstatic void write_moving(struct closure *cl)\r\n{\r\nstruct search *s = container_of(cl, struct search, cl);\r\nstruct moving_io *io = container_of(s, struct moving_io, s);\r\nif (!s->error) {\r\nmoving_init(io);\r\nio->bio.bio.bi_sector = KEY_START(&io->w->key);\r\ns->op.lock = -1;\r\ns->op.write_prio = 1;\r\ns->op.cache_bio = &io->bio.bio;\r\ns->writeback = KEY_DIRTY(&io->w->key);\r\ns->op.csum = KEY_CSUM(&io->w->key);\r\ns->op.type = BTREE_REPLACE;\r\nbkey_copy(&s->op.replace, &io->w->key);\r\nclosure_init(&s->op.cl, cl);\r\nbch_insert_data(&s->op.cl);\r\n}\r\ncontinue_at(cl, write_moving_finish, NULL);\r\n}\r\nstatic void read_moving_submit(struct closure *cl)\r\n{\r\nstruct search *s = container_of(cl, struct search, cl);\r\nstruct moving_io *io = container_of(s, struct moving_io, s);\r\nstruct bio *bio = &io->bio.bio;\r\nbch_submit_bbio(bio, s->op.c, &io->w->key, 0);\r\ncontinue_at(cl, write_moving, bch_gc_wq);\r\n}\r\nstatic void read_moving(struct closure *cl)\r\n{\r\nstruct cache_set *c = container_of(cl, struct cache_set, moving_gc);\r\nstruct keybuf_key *w;\r\nstruct moving_io *io;\r\nstruct bio *bio;\r\nwhile (!test_bit(CACHE_SET_STOPPING, &c->flags)) {\r\nw = bch_keybuf_next_rescan(c, &c->moving_gc_keys,\r\n&MAX_KEY, moving_pred);\r\nif (!w)\r\nbreak;\r\nio = kzalloc(sizeof(struct moving_io) + sizeof(struct bio_vec)\r\n* DIV_ROUND_UP(KEY_SIZE(&w->key), PAGE_SECTORS),\r\nGFP_KERNEL);\r\nif (!io)\r\ngoto err;\r\nw->private = io;\r\nio->w = w;\r\nio->s.op.inode = KEY_INODE(&w->key);\r\nio->s.op.c = c;\r\nmoving_init(io);\r\nbio = &io->bio.bio;\r\nbio->bi_rw = READ;\r\nbio->bi_end_io = read_moving_endio;\r\nif (bio_alloc_pages(bio, GFP_KERNEL))\r\ngoto err;\r\ntrace_bcache_gc_copy(&w->key);\r\nclosure_call(&io->s.cl, read_moving_submit, NULL, &c->gc.cl);\r\nif (atomic_inc_return(&c->in_flight) >= 64) {\r\nclosure_wait_event(&c->moving_gc_wait, cl,\r\natomic_read(&c->in_flight) < 64);\r\ncontinue_at(cl, read_moving, bch_gc_wq);\r\n}\r\n}\r\nif (0) {\r\nerr: if (!IS_ERR_OR_NULL(w->private))\r\nkfree(w->private);\r\nbch_keybuf_del(&c->moving_gc_keys, w);\r\n}\r\nclosure_return(cl);\r\n}\r\nstatic bool bucket_cmp(struct bucket *l, struct bucket *r)\r\n{\r\nreturn GC_SECTORS_USED(l) < GC_SECTORS_USED(r);\r\n}\r\nstatic unsigned bucket_heap_top(struct cache *ca)\r\n{\r\nreturn GC_SECTORS_USED(heap_peek(&ca->heap));\r\n}\r\nvoid bch_moving_gc(struct closure *cl)\r\n{\r\nstruct cache_set *c = container_of(cl, struct cache_set, gc.cl);\r\nstruct cache *ca;\r\nstruct bucket *b;\r\nunsigned i;\r\nif (!c->copy_gc_enabled)\r\nclosure_return(cl);\r\nmutex_lock(&c->bucket_lock);\r\nfor_each_cache(ca, c, i) {\r\nunsigned sectors_to_move = 0;\r\nunsigned reserve_sectors = ca->sb.bucket_size *\r\nmin(fifo_used(&ca->free), ca->free.size / 2);\r\nca->heap.used = 0;\r\nfor_each_bucket(b, ca) {\r\nif (!GC_SECTORS_USED(b))\r\ncontinue;\r\nif (!heap_full(&ca->heap)) {\r\nsectors_to_move += GC_SECTORS_USED(b);\r\nheap_add(&ca->heap, b, bucket_cmp);\r\n} else if (bucket_cmp(b, heap_peek(&ca->heap))) {\r\nsectors_to_move -= bucket_heap_top(ca);\r\nsectors_to_move += GC_SECTORS_USED(b);\r\nca->heap.data[0] = b;\r\nheap_sift(&ca->heap, 0, bucket_cmp);\r\n}\r\n}\r\nwhile (sectors_to_move > reserve_sectors) {\r\nheap_pop(&ca->heap, b, bucket_cmp);\r\nsectors_to_move -= GC_SECTORS_USED(b);\r\n}\r\nca->gc_move_threshold = bucket_heap_top(ca);\r\npr_debug("threshold %u", ca->gc_move_threshold);\r\n}\r\nmutex_unlock(&c->bucket_lock);\r\nc->moving_gc_keys.last_scanned = ZERO_KEY;\r\nclosure_init(&c->moving_gc, cl);\r\nread_moving(&c->moving_gc);\r\nclosure_return(cl);\r\n}\r\nvoid bch_moving_init_cache_set(struct cache_set *c)\r\n{\r\nbch_keybuf_init(&c->moving_gc_keys);\r\n}
