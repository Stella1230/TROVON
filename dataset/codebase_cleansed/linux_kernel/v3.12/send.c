void rds_send_reset(struct rds_connection *conn)\r\n{\r\nstruct rds_message *rm, *tmp;\r\nunsigned long flags;\r\nif (conn->c_xmit_rm) {\r\nrm = conn->c_xmit_rm;\r\nconn->c_xmit_rm = NULL;\r\nrds_message_unmapped(rm);\r\nrds_message_put(rm);\r\n}\r\nconn->c_xmit_sg = 0;\r\nconn->c_xmit_hdr_off = 0;\r\nconn->c_xmit_data_off = 0;\r\nconn->c_xmit_atomic_sent = 0;\r\nconn->c_xmit_rdma_sent = 0;\r\nconn->c_xmit_data_sent = 0;\r\nconn->c_map_queued = 0;\r\nconn->c_unacked_packets = rds_sysctl_max_unacked_packets;\r\nconn->c_unacked_bytes = rds_sysctl_max_unacked_bytes;\r\nspin_lock_irqsave(&conn->c_lock, flags);\r\nlist_for_each_entry_safe(rm, tmp, &conn->c_retrans, m_conn_item) {\r\nset_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\r\nset_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags);\r\n}\r\nlist_splice_init(&conn->c_retrans, &conn->c_send_queue);\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\n}\r\nstatic int acquire_in_xmit(struct rds_connection *conn)\r\n{\r\nreturn test_and_set_bit(RDS_IN_XMIT, &conn->c_flags) == 0;\r\n}\r\nstatic void release_in_xmit(struct rds_connection *conn)\r\n{\r\nclear_bit(RDS_IN_XMIT, &conn->c_flags);\r\nsmp_mb__after_clear_bit();\r\nif (waitqueue_active(&conn->c_waitq))\r\nwake_up_all(&conn->c_waitq);\r\n}\r\nint rds_send_xmit(struct rds_connection *conn)\r\n{\r\nstruct rds_message *rm;\r\nunsigned long flags;\r\nunsigned int tmp;\r\nstruct scatterlist *sg;\r\nint ret = 0;\r\nLIST_HEAD(to_be_dropped);\r\nrestart:\r\nif (!acquire_in_xmit(conn)) {\r\nrds_stats_inc(s_send_lock_contention);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (!rds_conn_up(conn)) {\r\nrelease_in_xmit(conn);\r\nret = 0;\r\ngoto out;\r\n}\r\nif (conn->c_trans->xmit_prepare)\r\nconn->c_trans->xmit_prepare(conn);\r\nwhile (1) {\r\nrm = conn->c_xmit_rm;\r\nif (!rm && test_and_clear_bit(0, &conn->c_map_queued)) {\r\nrm = rds_cong_update_alloc(conn);\r\nif (IS_ERR(rm)) {\r\nret = PTR_ERR(rm);\r\nbreak;\r\n}\r\nrm->data.op_active = 1;\r\nconn->c_xmit_rm = rm;\r\n}\r\nif (!rm) {\r\nunsigned int len;\r\nspin_lock_irqsave(&conn->c_lock, flags);\r\nif (!list_empty(&conn->c_send_queue)) {\r\nrm = list_entry(conn->c_send_queue.next,\r\nstruct rds_message,\r\nm_conn_item);\r\nrds_message_addref(rm);\r\nlist_move_tail(&rm->m_conn_item, &conn->c_retrans);\r\n}\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\nif (!rm)\r\nbreak;\r\nif (rm->rdma.op_active &&\r\ntest_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags)) {\r\nspin_lock_irqsave(&conn->c_lock, flags);\r\nif (test_and_clear_bit(RDS_MSG_ON_CONN, &rm->m_flags))\r\nlist_move(&rm->m_conn_item, &to_be_dropped);\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\ncontinue;\r\n}\r\nlen = ntohl(rm->m_inc.i_hdr.h_len);\r\nif (conn->c_unacked_packets == 0 ||\r\nconn->c_unacked_bytes < len) {\r\n__set_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\r\nconn->c_unacked_packets = rds_sysctl_max_unacked_packets;\r\nconn->c_unacked_bytes = rds_sysctl_max_unacked_bytes;\r\nrds_stats_inc(s_send_ack_required);\r\n} else {\r\nconn->c_unacked_bytes -= len;\r\nconn->c_unacked_packets--;\r\n}\r\nconn->c_xmit_rm = rm;\r\n}\r\nif (rm->rdma.op_active && !conn->c_xmit_rdma_sent) {\r\nrm->m_final_op = &rm->rdma;\r\nret = conn->c_trans->xmit_rdma(conn, &rm->rdma);\r\nif (ret)\r\nbreak;\r\nconn->c_xmit_rdma_sent = 1;\r\nset_bit(RDS_MSG_MAPPED, &rm->m_flags);\r\n}\r\nif (rm->atomic.op_active && !conn->c_xmit_atomic_sent) {\r\nrm->m_final_op = &rm->atomic;\r\nret = conn->c_trans->xmit_atomic(conn, &rm->atomic);\r\nif (ret)\r\nbreak;\r\nconn->c_xmit_atomic_sent = 1;\r\nset_bit(RDS_MSG_MAPPED, &rm->m_flags);\r\n}\r\nif (rm->data.op_nents == 0) {\r\nint ops_present;\r\nint all_ops_are_silent = 1;\r\nops_present = (rm->atomic.op_active || rm->rdma.op_active);\r\nif (rm->atomic.op_active && !rm->atomic.op_silent)\r\nall_ops_are_silent = 0;\r\nif (rm->rdma.op_active && !rm->rdma.op_silent)\r\nall_ops_are_silent = 0;\r\nif (ops_present && all_ops_are_silent\r\n&& !rm->m_rdma_cookie)\r\nrm->data.op_active = 0;\r\n}\r\nif (rm->data.op_active && !conn->c_xmit_data_sent) {\r\nrm->m_final_op = &rm->data;\r\nret = conn->c_trans->xmit(conn, rm,\r\nconn->c_xmit_hdr_off,\r\nconn->c_xmit_sg,\r\nconn->c_xmit_data_off);\r\nif (ret <= 0)\r\nbreak;\r\nif (conn->c_xmit_hdr_off < sizeof(struct rds_header)) {\r\ntmp = min_t(int, ret,\r\nsizeof(struct rds_header) -\r\nconn->c_xmit_hdr_off);\r\nconn->c_xmit_hdr_off += tmp;\r\nret -= tmp;\r\n}\r\nsg = &rm->data.op_sg[conn->c_xmit_sg];\r\nwhile (ret) {\r\ntmp = min_t(int, ret, sg->length -\r\nconn->c_xmit_data_off);\r\nconn->c_xmit_data_off += tmp;\r\nret -= tmp;\r\nif (conn->c_xmit_data_off == sg->length) {\r\nconn->c_xmit_data_off = 0;\r\nsg++;\r\nconn->c_xmit_sg++;\r\nBUG_ON(ret != 0 &&\r\nconn->c_xmit_sg == rm->data.op_nents);\r\n}\r\n}\r\nif (conn->c_xmit_hdr_off == sizeof(struct rds_header) &&\r\n(conn->c_xmit_sg == rm->data.op_nents))\r\nconn->c_xmit_data_sent = 1;\r\n}\r\nif (!rm->data.op_active || conn->c_xmit_data_sent) {\r\nconn->c_xmit_rm = NULL;\r\nconn->c_xmit_sg = 0;\r\nconn->c_xmit_hdr_off = 0;\r\nconn->c_xmit_data_off = 0;\r\nconn->c_xmit_rdma_sent = 0;\r\nconn->c_xmit_atomic_sent = 0;\r\nconn->c_xmit_data_sent = 0;\r\nrds_message_put(rm);\r\n}\r\n}\r\nif (conn->c_trans->xmit_complete)\r\nconn->c_trans->xmit_complete(conn);\r\nrelease_in_xmit(conn);\r\nif (!list_empty(&to_be_dropped)) {\r\nlist_for_each_entry(rm, &to_be_dropped, m_conn_item)\r\nrds_message_put(rm);\r\nrds_send_remove_from_sock(&to_be_dropped, RDS_RDMA_DROPPED);\r\n}\r\nif (ret == 0) {\r\nsmp_mb();\r\nif (!list_empty(&conn->c_send_queue)) {\r\nrds_stats_inc(s_send_lock_queue_raced);\r\ngoto restart;\r\n}\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic void rds_send_sndbuf_remove(struct rds_sock *rs, struct rds_message *rm)\r\n{\r\nu32 len = be32_to_cpu(rm->m_inc.i_hdr.h_len);\r\nassert_spin_locked(&rs->rs_lock);\r\nBUG_ON(rs->rs_snd_bytes < len);\r\nrs->rs_snd_bytes -= len;\r\nif (rs->rs_snd_bytes == 0)\r\nrds_stats_inc(s_send_queue_empty);\r\n}\r\nstatic inline int rds_send_is_acked(struct rds_message *rm, u64 ack,\r\nis_acked_func is_acked)\r\n{\r\nif (is_acked)\r\nreturn is_acked(rm, ack);\r\nreturn be64_to_cpu(rm->m_inc.i_hdr.h_sequence) <= ack;\r\n}\r\nvoid rds_rdma_send_complete(struct rds_message *rm, int status)\r\n{\r\nstruct rds_sock *rs = NULL;\r\nstruct rm_rdma_op *ro;\r\nstruct rds_notifier *notifier;\r\nunsigned long flags;\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nro = &rm->rdma;\r\nif (test_bit(RDS_MSG_ON_SOCK, &rm->m_flags) &&\r\nro->op_active && ro->op_notify && ro->op_notifier) {\r\nnotifier = ro->op_notifier;\r\nrs = rm->m_rs;\r\nsock_hold(rds_rs_to_sk(rs));\r\nnotifier->n_status = status;\r\nspin_lock(&rs->rs_lock);\r\nlist_add_tail(&notifier->n_list, &rs->rs_notify_queue);\r\nspin_unlock(&rs->rs_lock);\r\nro->op_notifier = NULL;\r\n}\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\n}\r\nvoid rds_atomic_send_complete(struct rds_message *rm, int status)\r\n{\r\nstruct rds_sock *rs = NULL;\r\nstruct rm_atomic_op *ao;\r\nstruct rds_notifier *notifier;\r\nunsigned long flags;\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nao = &rm->atomic;\r\nif (test_bit(RDS_MSG_ON_SOCK, &rm->m_flags)\r\n&& ao->op_active && ao->op_notify && ao->op_notifier) {\r\nnotifier = ao->op_notifier;\r\nrs = rm->m_rs;\r\nsock_hold(rds_rs_to_sk(rs));\r\nnotifier->n_status = status;\r\nspin_lock(&rs->rs_lock);\r\nlist_add_tail(&notifier->n_list, &rs->rs_notify_queue);\r\nspin_unlock(&rs->rs_lock);\r\nao->op_notifier = NULL;\r\n}\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\n}\r\nstatic inline void\r\n__rds_send_complete(struct rds_sock *rs, struct rds_message *rm, int status)\r\n{\r\nstruct rm_rdma_op *ro;\r\nstruct rm_atomic_op *ao;\r\nro = &rm->rdma;\r\nif (ro->op_active && ro->op_notify && ro->op_notifier) {\r\nro->op_notifier->n_status = status;\r\nlist_add_tail(&ro->op_notifier->n_list, &rs->rs_notify_queue);\r\nro->op_notifier = NULL;\r\n}\r\nao = &rm->atomic;\r\nif (ao->op_active && ao->op_notify && ao->op_notifier) {\r\nao->op_notifier->n_status = status;\r\nlist_add_tail(&ao->op_notifier->n_list, &rs->rs_notify_queue);\r\nao->op_notifier = NULL;\r\n}\r\n}\r\nstruct rds_message *rds_send_get_message(struct rds_connection *conn,\r\nstruct rm_rdma_op *op)\r\n{\r\nstruct rds_message *rm, *tmp, *found = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&conn->c_lock, flags);\r\nlist_for_each_entry_safe(rm, tmp, &conn->c_retrans, m_conn_item) {\r\nif (&rm->rdma == op) {\r\natomic_inc(&rm->m_refcount);\r\nfound = rm;\r\ngoto out;\r\n}\r\n}\r\nlist_for_each_entry_safe(rm, tmp, &conn->c_send_queue, m_conn_item) {\r\nif (&rm->rdma == op) {\r\natomic_inc(&rm->m_refcount);\r\nfound = rm;\r\nbreak;\r\n}\r\n}\r\nout:\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\nreturn found;\r\n}\r\nstatic void rds_send_remove_from_sock(struct list_head *messages, int status)\r\n{\r\nunsigned long flags;\r\nstruct rds_sock *rs = NULL;\r\nstruct rds_message *rm;\r\nwhile (!list_empty(messages)) {\r\nint was_on_sock = 0;\r\nrm = list_entry(messages->next, struct rds_message,\r\nm_conn_item);\r\nlist_del_init(&rm->m_conn_item);\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nif (!test_bit(RDS_MSG_ON_SOCK, &rm->m_flags))\r\ngoto unlock_and_drop;\r\nif (rs != rm->m_rs) {\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\nrs = rm->m_rs;\r\nsock_hold(rds_rs_to_sk(rs));\r\n}\r\nspin_lock(&rs->rs_lock);\r\nif (test_and_clear_bit(RDS_MSG_ON_SOCK, &rm->m_flags)) {\r\nstruct rm_rdma_op *ro = &rm->rdma;\r\nstruct rds_notifier *notifier;\r\nlist_del_init(&rm->m_sock_item);\r\nrds_send_sndbuf_remove(rs, rm);\r\nif (ro->op_active && ro->op_notifier &&\r\n(ro->op_notify || (ro->op_recverr && status))) {\r\nnotifier = ro->op_notifier;\r\nlist_add_tail(&notifier->n_list,\r\n&rs->rs_notify_queue);\r\nif (!notifier->n_status)\r\nnotifier->n_status = status;\r\nrm->rdma.op_notifier = NULL;\r\n}\r\nwas_on_sock = 1;\r\nrm->m_rs = NULL;\r\n}\r\nspin_unlock(&rs->rs_lock);\r\nunlock_and_drop:\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nrds_message_put(rm);\r\nif (was_on_sock)\r\nrds_message_put(rm);\r\n}\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\n}\r\nvoid rds_send_drop_acked(struct rds_connection *conn, u64 ack,\r\nis_acked_func is_acked)\r\n{\r\nstruct rds_message *rm, *tmp;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\nspin_lock_irqsave(&conn->c_lock, flags);\r\nlist_for_each_entry_safe(rm, tmp, &conn->c_retrans, m_conn_item) {\r\nif (!rds_send_is_acked(rm, ack, is_acked))\r\nbreak;\r\nlist_move(&rm->m_conn_item, &list);\r\nclear_bit(RDS_MSG_ON_CONN, &rm->m_flags);\r\n}\r\nif (!list_empty(&list))\r\nsmp_mb__after_clear_bit();\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\nrds_send_remove_from_sock(&list, RDS_RDMA_SUCCESS);\r\n}\r\nvoid rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in *dest)\r\n{\r\nstruct rds_message *rm, *tmp;\r\nstruct rds_connection *conn;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\nspin_lock_irqsave(&rs->rs_lock, flags);\r\nlist_for_each_entry_safe(rm, tmp, &rs->rs_send_queue, m_sock_item) {\r\nif (dest && (dest->sin_addr.s_addr != rm->m_daddr ||\r\ndest->sin_port != rm->m_inc.i_hdr.h_dport))\r\ncontinue;\r\nlist_move(&rm->m_sock_item, &list);\r\nrds_send_sndbuf_remove(rs, rm);\r\nclear_bit(RDS_MSG_ON_SOCK, &rm->m_flags);\r\n}\r\nsmp_mb__after_clear_bit();\r\nspin_unlock_irqrestore(&rs->rs_lock, flags);\r\nif (list_empty(&list))\r\nreturn;\r\nlist_for_each_entry(rm, &list, m_sock_item) {\r\nconn = rm->m_inc.i_conn;\r\nspin_lock_irqsave(&conn->c_lock, flags);\r\nif (!test_and_clear_bit(RDS_MSG_ON_CONN, &rm->m_flags)) {\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\ncontinue;\r\n}\r\nlist_del_init(&rm->m_conn_item);\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nspin_lock(&rs->rs_lock);\r\n__rds_send_complete(rs, rm, RDS_RDMA_CANCELED);\r\nspin_unlock(&rs->rs_lock);\r\nrm->m_rs = NULL;\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nrds_message_put(rm);\r\n}\r\nrds_wake_sk_sleep(rs);\r\nwhile (!list_empty(&list)) {\r\nrm = list_entry(list.next, struct rds_message, m_sock_item);\r\nlist_del_init(&rm->m_sock_item);\r\nrds_message_wait(rm);\r\nrds_message_put(rm);\r\n}\r\n}\r\nstatic int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,\r\nstruct rds_message *rm, __be16 sport,\r\n__be16 dport, int *queued)\r\n{\r\nunsigned long flags;\r\nu32 len;\r\nif (*queued)\r\ngoto out;\r\nlen = be32_to_cpu(rm->m_inc.i_hdr.h_len);\r\nspin_lock_irqsave(&rs->rs_lock, flags);\r\nif (rs->rs_snd_bytes < rds_sk_sndbuf(rs)) {\r\nrs->rs_snd_bytes += len;\r\nif (rs->rs_snd_bytes >= rds_sk_sndbuf(rs) / 2)\r\n__set_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\r\nlist_add_tail(&rm->m_sock_item, &rs->rs_send_queue);\r\nset_bit(RDS_MSG_ON_SOCK, &rm->m_flags);\r\nrds_message_addref(rm);\r\nrm->m_rs = rs;\r\nrds_message_populate_header(&rm->m_inc.i_hdr, sport, dport, 0);\r\nrm->m_inc.i_conn = conn;\r\nrds_message_addref(rm);\r\nspin_lock(&conn->c_lock);\r\nrm->m_inc.i_hdr.h_sequence = cpu_to_be64(conn->c_next_tx_seq++);\r\nlist_add_tail(&rm->m_conn_item, &conn->c_send_queue);\r\nset_bit(RDS_MSG_ON_CONN, &rm->m_flags);\r\nspin_unlock(&conn->c_lock);\r\nrdsdebug("queued msg %p len %d, rs %p bytes %d seq %llu\n",\r\nrm, len, rs, rs->rs_snd_bytes,\r\n(unsigned long long)be64_to_cpu(rm->m_inc.i_hdr.h_sequence));\r\n*queued = 1;\r\n}\r\nspin_unlock_irqrestore(&rs->rs_lock, flags);\r\nout:\r\nreturn *queued;\r\n}\r\nstatic int rds_rm_size(struct msghdr *msg, int data_len)\r\n{\r\nstruct cmsghdr *cmsg;\r\nint size = 0;\r\nint cmsg_groups = 0;\r\nint retval;\r\nfor (cmsg = CMSG_FIRSTHDR(msg); cmsg; cmsg = CMSG_NXTHDR(msg, cmsg)) {\r\nif (!CMSG_OK(msg, cmsg))\r\nreturn -EINVAL;\r\nif (cmsg->cmsg_level != SOL_RDS)\r\ncontinue;\r\nswitch (cmsg->cmsg_type) {\r\ncase RDS_CMSG_RDMA_ARGS:\r\ncmsg_groups |= 1;\r\nretval = rds_rdma_extra_size(CMSG_DATA(cmsg));\r\nif (retval < 0)\r\nreturn retval;\r\nsize += retval;\r\nbreak;\r\ncase RDS_CMSG_RDMA_DEST:\r\ncase RDS_CMSG_RDMA_MAP:\r\ncmsg_groups |= 2;\r\nbreak;\r\ncase RDS_CMSG_ATOMIC_CSWP:\r\ncase RDS_CMSG_ATOMIC_FADD:\r\ncase RDS_CMSG_MASKED_ATOMIC_CSWP:\r\ncase RDS_CMSG_MASKED_ATOMIC_FADD:\r\ncmsg_groups |= 1;\r\nsize += sizeof(struct scatterlist);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nsize += ceil(data_len, PAGE_SIZE) * sizeof(struct scatterlist);\r\nif (cmsg_groups == 3)\r\nreturn -EINVAL;\r\nreturn size;\r\n}\r\nstatic int rds_cmsg_send(struct rds_sock *rs, struct rds_message *rm,\r\nstruct msghdr *msg, int *allocated_mr)\r\n{\r\nstruct cmsghdr *cmsg;\r\nint ret = 0;\r\nfor (cmsg = CMSG_FIRSTHDR(msg); cmsg; cmsg = CMSG_NXTHDR(msg, cmsg)) {\r\nif (!CMSG_OK(msg, cmsg))\r\nreturn -EINVAL;\r\nif (cmsg->cmsg_level != SOL_RDS)\r\ncontinue;\r\nswitch (cmsg->cmsg_type) {\r\ncase RDS_CMSG_RDMA_ARGS:\r\nret = rds_cmsg_rdma_args(rs, rm, cmsg);\r\nbreak;\r\ncase RDS_CMSG_RDMA_DEST:\r\nret = rds_cmsg_rdma_dest(rs, rm, cmsg);\r\nbreak;\r\ncase RDS_CMSG_RDMA_MAP:\r\nret = rds_cmsg_rdma_map(rs, rm, cmsg);\r\nif (!ret)\r\n*allocated_mr = 1;\r\nbreak;\r\ncase RDS_CMSG_ATOMIC_CSWP:\r\ncase RDS_CMSG_ATOMIC_FADD:\r\ncase RDS_CMSG_MASKED_ATOMIC_CSWP:\r\ncase RDS_CMSG_MASKED_ATOMIC_FADD:\r\nret = rds_cmsg_atomic(rs, rm, cmsg);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (ret)\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nint rds_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\r\nsize_t payload_len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct rds_sock *rs = rds_sk_to_rs(sk);\r\nstruct sockaddr_in *usin = (struct sockaddr_in *)msg->msg_name;\r\n__be32 daddr;\r\n__be16 dport;\r\nstruct rds_message *rm = NULL;\r\nstruct rds_connection *conn;\r\nint ret = 0;\r\nint queued = 0, allocated_mr = 0;\r\nint nonblock = msg->msg_flags & MSG_DONTWAIT;\r\nlong timeo = sock_sndtimeo(sk, nonblock);\r\nif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\r\nret = -EOPNOTSUPP;\r\ngoto out;\r\n}\r\nif (msg->msg_namelen) {\r\nif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\ndaddr = usin->sin_addr.s_addr;\r\ndport = usin->sin_port;\r\n} else {\r\nlock_sock(sk);\r\ndaddr = rs->rs_conn_addr;\r\ndport = rs->rs_conn_port;\r\nrelease_sock(sk);\r\n}\r\nif (daddr == 0 || rs->rs_bound_addr == 0) {\r\nret = -ENOTCONN;\r\ngoto out;\r\n}\r\nret = rds_rm_size(msg, payload_len);\r\nif (ret < 0)\r\ngoto out;\r\nrm = rds_message_alloc(ret, GFP_KERNEL);\r\nif (!rm) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (payload_len) {\r\nrm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\r\nif (!rm->data.op_sg) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = rds_message_copy_from_user(rm, msg->msg_iov, payload_len);\r\nif (ret)\r\ngoto out;\r\n}\r\nrm->data.op_active = 1;\r\nrm->m_daddr = daddr;\r\nif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\r\nconn = rs->rs_conn;\r\nelse {\r\nconn = rds_conn_create_outgoing(rs->rs_bound_addr, daddr,\r\nrs->rs_transport,\r\nsock->sk->sk_allocation);\r\nif (IS_ERR(conn)) {\r\nret = PTR_ERR(conn);\r\ngoto out;\r\n}\r\nrs->rs_conn = conn;\r\n}\r\nret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\r\nif (ret)\r\ngoto out;\r\nif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\r\nprintk_ratelimited(KERN_NOTICE "rdma_op %p conn xmit_rdma %p\n",\r\n&rm->rdma, conn->c_trans->xmit_rdma);\r\nret = -EOPNOTSUPP;\r\ngoto out;\r\n}\r\nif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\r\nprintk_ratelimited(KERN_NOTICE "atomic_op %p conn xmit_atomic %p\n",\r\n&rm->atomic, conn->c_trans->xmit_atomic);\r\nret = -EOPNOTSUPP;\r\ngoto out;\r\n}\r\nrds_conn_connect_if_down(conn);\r\nret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\r\nif (ret) {\r\nrs->rs_seen_congestion = 1;\r\ngoto out;\r\n}\r\nwhile (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,\r\ndport, &queued)) {\r\nrds_stats_inc(s_send_queue_full);\r\nif (payload_len > rds_sk_sndbuf(rs)) {\r\nret = -EMSGSIZE;\r\ngoto out;\r\n}\r\nif (nonblock) {\r\nret = -EAGAIN;\r\ngoto out;\r\n}\r\ntimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\r\nrds_send_queue_rm(rs, conn, rm,\r\nrs->rs_bound_port,\r\ndport,\r\n&queued),\r\ntimeo);\r\nrdsdebug("sendmsg woke queued %d timeo %ld\n", queued, timeo);\r\nif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\r\ncontinue;\r\nret = timeo;\r\nif (ret == 0)\r\nret = -ETIMEDOUT;\r\ngoto out;\r\n}\r\nrds_stats_inc(s_send_queued);\r\nif (!test_bit(RDS_LL_SEND_FULL, &conn->c_flags))\r\nrds_send_xmit(conn);\r\nrds_message_put(rm);\r\nreturn payload_len;\r\nout:\r\nif (allocated_mr)\r\nrds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\r\nif (rm)\r\nrds_message_put(rm);\r\nreturn ret;\r\n}\r\nint\r\nrds_send_pong(struct rds_connection *conn, __be16 dport)\r\n{\r\nstruct rds_message *rm;\r\nunsigned long flags;\r\nint ret = 0;\r\nrm = rds_message_alloc(0, GFP_ATOMIC);\r\nif (!rm) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nrm->m_daddr = conn->c_faddr;\r\nrm->data.op_active = 1;\r\nrds_conn_connect_if_down(conn);\r\nret = rds_cong_wait(conn->c_fcong, dport, 1, NULL);\r\nif (ret)\r\ngoto out;\r\nspin_lock_irqsave(&conn->c_lock, flags);\r\nlist_add_tail(&rm->m_conn_item, &conn->c_send_queue);\r\nset_bit(RDS_MSG_ON_CONN, &rm->m_flags);\r\nrds_message_addref(rm);\r\nrm->m_inc.i_conn = conn;\r\nrds_message_populate_header(&rm->m_inc.i_hdr, 0, dport,\r\nconn->c_next_tx_seq);\r\nconn->c_next_tx_seq++;\r\nspin_unlock_irqrestore(&conn->c_lock, flags);\r\nrds_stats_inc(s_send_queued);\r\nrds_stats_inc(s_send_pong);\r\nif (!test_bit(RDS_LL_SEND_FULL, &conn->c_flags))\r\nqueue_delayed_work(rds_wq, &conn->c_send_w, 0);\r\nrds_message_put(rm);\r\nreturn 0;\r\nout:\r\nif (rm)\r\nrds_message_put(rm);\r\nreturn ret;\r\n}
