static void uv_noop(struct irq_data *data) { }\r\nstatic void uv_ack_apic(struct irq_data *data)\r\n{\r\nack_APIC_irq();\r\n}\r\nstatic int uv_set_irq_2_mmr_info(int irq, unsigned long offset, unsigned blade)\r\n{\r\nstruct rb_node **link = &uv_irq_root.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct uv_irq_2_mmr_pnode *n;\r\nstruct uv_irq_2_mmr_pnode *e;\r\nunsigned long irqflags;\r\nn = kmalloc_node(sizeof(struct uv_irq_2_mmr_pnode), GFP_KERNEL,\r\nuv_blade_to_memory_nid(blade));\r\nif (!n)\r\nreturn -ENOMEM;\r\nn->irq = irq;\r\nn->offset = offset;\r\nn->pnode = uv_blade_to_pnode(blade);\r\nspin_lock_irqsave(&uv_irq_lock, irqflags);\r\nwhile (*link) {\r\nparent = *link;\r\ne = rb_entry(parent, struct uv_irq_2_mmr_pnode, list);\r\nif (unlikely(irq == e->irq)) {\r\ne->pnode = uv_blade_to_pnode(blade);\r\ne->offset = offset;\r\nspin_unlock_irqrestore(&uv_irq_lock, irqflags);\r\nkfree(n);\r\nreturn 0;\r\n}\r\nif (irq < e->irq)\r\nlink = &(*link)->rb_left;\r\nelse\r\nlink = &(*link)->rb_right;\r\n}\r\nrb_link_node(&n->list, parent, link);\r\nrb_insert_color(&n->list, &uv_irq_root);\r\nspin_unlock_irqrestore(&uv_irq_lock, irqflags);\r\nreturn 0;\r\n}\r\nint uv_irq_2_mmr_info(int irq, unsigned long *offset, int *pnode)\r\n{\r\nstruct uv_irq_2_mmr_pnode *e;\r\nstruct rb_node *n;\r\nunsigned long irqflags;\r\nspin_lock_irqsave(&uv_irq_lock, irqflags);\r\nn = uv_irq_root.rb_node;\r\nwhile (n) {\r\ne = rb_entry(n, struct uv_irq_2_mmr_pnode, list);\r\nif (e->irq == irq) {\r\n*offset = e->offset;\r\n*pnode = e->pnode;\r\nspin_unlock_irqrestore(&uv_irq_lock, irqflags);\r\nreturn 0;\r\n}\r\nif (irq < e->irq)\r\nn = n->rb_left;\r\nelse\r\nn = n->rb_right;\r\n}\r\nspin_unlock_irqrestore(&uv_irq_lock, irqflags);\r\nreturn -1;\r\n}\r\nstatic int\r\narch_enable_uv_irq(char *irq_name, unsigned int irq, int cpu, int mmr_blade,\r\nunsigned long mmr_offset, int limit)\r\n{\r\nconst struct cpumask *eligible_cpu = cpumask_of(cpu);\r\nstruct irq_cfg *cfg = irq_get_chip_data(irq);\r\nunsigned long mmr_value;\r\nstruct uv_IO_APIC_route_entry *entry;\r\nint mmr_pnode, err;\r\nunsigned int dest;\r\nBUILD_BUG_ON(sizeof(struct uv_IO_APIC_route_entry) !=\r\nsizeof(unsigned long));\r\nerr = assign_irq_vector(irq, cfg, eligible_cpu);\r\nif (err != 0)\r\nreturn err;\r\nerr = apic->cpu_mask_to_apicid_and(eligible_cpu, eligible_cpu, &dest);\r\nif (err != 0)\r\nreturn err;\r\nif (limit == UV_AFFINITY_CPU)\r\nirq_set_status_flags(irq, IRQ_NO_BALANCING);\r\nelse\r\nirq_set_status_flags(irq, IRQ_MOVE_PCNTXT);\r\nirq_set_chip_and_handler_name(irq, &uv_irq_chip, handle_percpu_irq,\r\nirq_name);\r\nmmr_value = 0;\r\nentry = (struct uv_IO_APIC_route_entry *)&mmr_value;\r\nentry->vector = cfg->vector;\r\nentry->delivery_mode = apic->irq_delivery_mode;\r\nentry->dest_mode = apic->irq_dest_mode;\r\nentry->polarity = 0;\r\nentry->trigger = 0;\r\nentry->mask = 0;\r\nentry->dest = dest;\r\nmmr_pnode = uv_blade_to_pnode(mmr_blade);\r\nuv_write_global_mmr64(mmr_pnode, mmr_offset, mmr_value);\r\nif (cfg->move_in_progress)\r\nsend_cleanup_vector(cfg);\r\nreturn irq;\r\n}\r\nstatic void arch_disable_uv_irq(int mmr_pnode, unsigned long mmr_offset)\r\n{\r\nunsigned long mmr_value;\r\nstruct uv_IO_APIC_route_entry *entry;\r\nBUILD_BUG_ON(sizeof(struct uv_IO_APIC_route_entry) !=\r\nsizeof(unsigned long));\r\nmmr_value = 0;\r\nentry = (struct uv_IO_APIC_route_entry *)&mmr_value;\r\nentry->mask = 1;\r\nuv_write_global_mmr64(mmr_pnode, mmr_offset, mmr_value);\r\n}\r\nstatic int\r\nuv_set_irq_affinity(struct irq_data *data, const struct cpumask *mask,\r\nbool force)\r\n{\r\nstruct irq_cfg *cfg = data->chip_data;\r\nunsigned int dest;\r\nunsigned long mmr_value, mmr_offset;\r\nstruct uv_IO_APIC_route_entry *entry;\r\nint mmr_pnode;\r\nif (__ioapic_set_affinity(data, mask, &dest))\r\nreturn -1;\r\nmmr_value = 0;\r\nentry = (struct uv_IO_APIC_route_entry *)&mmr_value;\r\nentry->vector = cfg->vector;\r\nentry->delivery_mode = apic->irq_delivery_mode;\r\nentry->dest_mode = apic->irq_dest_mode;\r\nentry->polarity = 0;\r\nentry->trigger = 0;\r\nentry->mask = 0;\r\nentry->dest = dest;\r\nif (uv_irq_2_mmr_info(data->irq, &mmr_offset, &mmr_pnode))\r\nreturn -1;\r\nuv_write_global_mmr64(mmr_pnode, mmr_offset, mmr_value);\r\nif (cfg->move_in_progress)\r\nsend_cleanup_vector(cfg);\r\nreturn IRQ_SET_MASK_OK_NOCOPY;\r\n}\r\nint uv_setup_irq(char *irq_name, int cpu, int mmr_blade,\r\nunsigned long mmr_offset, int limit)\r\n{\r\nint irq, ret;\r\nirq = create_irq_nr(NR_IRQS_LEGACY, uv_blade_to_memory_nid(mmr_blade));\r\nif (irq <= 0)\r\nreturn -EBUSY;\r\nret = arch_enable_uv_irq(irq_name, irq, cpu, mmr_blade, mmr_offset,\r\nlimit);\r\nif (ret == irq)\r\nuv_set_irq_2_mmr_info(irq, mmr_offset, mmr_blade);\r\nelse\r\ndestroy_irq(irq);\r\nreturn ret;\r\n}\r\nvoid uv_teardown_irq(unsigned int irq)\r\n{\r\nstruct uv_irq_2_mmr_pnode *e;\r\nstruct rb_node *n;\r\nunsigned long irqflags;\r\nspin_lock_irqsave(&uv_irq_lock, irqflags);\r\nn = uv_irq_root.rb_node;\r\nwhile (n) {\r\ne = rb_entry(n, struct uv_irq_2_mmr_pnode, list);\r\nif (e->irq == irq) {\r\narch_disable_uv_irq(e->pnode, e->offset);\r\nrb_erase(n, &uv_irq_root);\r\nkfree(e);\r\nbreak;\r\n}\r\nif (irq < e->irq)\r\nn = n->rb_left;\r\nelse\r\nn = n->rb_right;\r\n}\r\nspin_unlock_irqrestore(&uv_irq_lock, irqflags);\r\ndestroy_irq(irq);\r\n}
