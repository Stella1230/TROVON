static struct ipath_mcast_qp *ipath_mcast_qp_alloc(struct ipath_qp *qp)\r\n{\r\nstruct ipath_mcast_qp *mqp;\r\nmqp = kmalloc(sizeof *mqp, GFP_KERNEL);\r\nif (!mqp)\r\ngoto bail;\r\nmqp->qp = qp;\r\natomic_inc(&qp->refcount);\r\nbail:\r\nreturn mqp;\r\n}\r\nstatic void ipath_mcast_qp_free(struct ipath_mcast_qp *mqp)\r\n{\r\nstruct ipath_qp *qp = mqp->qp;\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\nkfree(mqp);\r\n}\r\nstatic struct ipath_mcast *ipath_mcast_alloc(union ib_gid *mgid)\r\n{\r\nstruct ipath_mcast *mcast;\r\nmcast = kmalloc(sizeof *mcast, GFP_KERNEL);\r\nif (!mcast)\r\ngoto bail;\r\nmcast->mgid = *mgid;\r\nINIT_LIST_HEAD(&mcast->qp_list);\r\ninit_waitqueue_head(&mcast->wait);\r\natomic_set(&mcast->refcount, 0);\r\nmcast->n_attached = 0;\r\nbail:\r\nreturn mcast;\r\n}\r\nstatic void ipath_mcast_free(struct ipath_mcast *mcast)\r\n{\r\nstruct ipath_mcast_qp *p, *tmp;\r\nlist_for_each_entry_safe(p, tmp, &mcast->qp_list, list)\r\nipath_mcast_qp_free(p);\r\nkfree(mcast);\r\n}\r\nstruct ipath_mcast *ipath_mcast_find(union ib_gid *mgid)\r\n{\r\nstruct rb_node *n;\r\nunsigned long flags;\r\nstruct ipath_mcast *mcast;\r\nspin_lock_irqsave(&mcast_lock, flags);\r\nn = mcast_tree.rb_node;\r\nwhile (n) {\r\nint ret;\r\nmcast = rb_entry(n, struct ipath_mcast, rb_node);\r\nret = memcmp(mgid->raw, mcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0)\r\nn = n->rb_left;\r\nelse if (ret > 0)\r\nn = n->rb_right;\r\nelse {\r\natomic_inc(&mcast->refcount);\r\nspin_unlock_irqrestore(&mcast_lock, flags);\r\ngoto bail;\r\n}\r\n}\r\nspin_unlock_irqrestore(&mcast_lock, flags);\r\nmcast = NULL;\r\nbail:\r\nreturn mcast;\r\n}\r\nstatic int ipath_mcast_add(struct ipath_ibdev *dev,\r\nstruct ipath_mcast *mcast,\r\nstruct ipath_mcast_qp *mqp)\r\n{\r\nstruct rb_node **n = &mcast_tree.rb_node;\r\nstruct rb_node *pn = NULL;\r\nint ret;\r\nspin_lock_irq(&mcast_lock);\r\nwhile (*n) {\r\nstruct ipath_mcast *tmcast;\r\nstruct ipath_mcast_qp *p;\r\npn = *n;\r\ntmcast = rb_entry(pn, struct ipath_mcast, rb_node);\r\nret = memcmp(mcast->mgid.raw, tmcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0) {\r\nn = &pn->rb_left;\r\ncontinue;\r\n}\r\nif (ret > 0) {\r\nn = &pn->rb_right;\r\ncontinue;\r\n}\r\nlist_for_each_entry_rcu(p, &tmcast->qp_list, list) {\r\nif (p->qp == mqp->qp) {\r\nret = ESRCH;\r\ngoto bail;\r\n}\r\n}\r\nif (tmcast->n_attached == ib_ipath_max_mcast_qp_attached) {\r\nret = ENOMEM;\r\ngoto bail;\r\n}\r\ntmcast->n_attached++;\r\nlist_add_tail_rcu(&mqp->list, &tmcast->qp_list);\r\nret = EEXIST;\r\ngoto bail;\r\n}\r\nspin_lock(&dev->n_mcast_grps_lock);\r\nif (dev->n_mcast_grps_allocated == ib_ipath_max_mcast_grps) {\r\nspin_unlock(&dev->n_mcast_grps_lock);\r\nret = ENOMEM;\r\ngoto bail;\r\n}\r\ndev->n_mcast_grps_allocated++;\r\nspin_unlock(&dev->n_mcast_grps_lock);\r\nmcast->n_attached++;\r\nlist_add_tail_rcu(&mqp->list, &mcast->qp_list);\r\natomic_inc(&mcast->refcount);\r\nrb_link_node(&mcast->rb_node, pn, n);\r\nrb_insert_color(&mcast->rb_node, &mcast_tree);\r\nret = 0;\r\nbail:\r\nspin_unlock_irq(&mcast_lock);\r\nreturn ret;\r\n}\r\nint ipath_multicast_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\r\n{\r\nstruct ipath_qp *qp = to_iqp(ibqp);\r\nstruct ipath_ibdev *dev = to_idev(ibqp->device);\r\nstruct ipath_mcast *mcast;\r\nstruct ipath_mcast_qp *mqp;\r\nint ret;\r\nmcast = ipath_mcast_alloc(gid);\r\nif (mcast == NULL) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nmqp = ipath_mcast_qp_alloc(qp);\r\nif (mqp == NULL) {\r\nipath_mcast_free(mcast);\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nswitch (ipath_mcast_add(dev, mcast, mqp)) {\r\ncase ESRCH:\r\nipath_mcast_qp_free(mqp);\r\nipath_mcast_free(mcast);\r\nret = -EINVAL;\r\ngoto bail;\r\ncase EEXIST:\r\nipath_mcast_free(mcast);\r\nbreak;\r\ncase ENOMEM:\r\nipath_mcast_qp_free(mqp);\r\nipath_mcast_free(mcast);\r\nret = -ENOMEM;\r\ngoto bail;\r\ndefault:\r\nbreak;\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_multicast_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\r\n{\r\nstruct ipath_qp *qp = to_iqp(ibqp);\r\nstruct ipath_ibdev *dev = to_idev(ibqp->device);\r\nstruct ipath_mcast *mcast = NULL;\r\nstruct ipath_mcast_qp *p, *tmp;\r\nstruct rb_node *n;\r\nint last = 0;\r\nint ret;\r\nspin_lock_irq(&mcast_lock);\r\nn = mcast_tree.rb_node;\r\nwhile (1) {\r\nif (n == NULL) {\r\nspin_unlock_irq(&mcast_lock);\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nmcast = rb_entry(n, struct ipath_mcast, rb_node);\r\nret = memcmp(gid->raw, mcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0)\r\nn = n->rb_left;\r\nelse if (ret > 0)\r\nn = n->rb_right;\r\nelse\r\nbreak;\r\n}\r\nlist_for_each_entry_safe(p, tmp, &mcast->qp_list, list) {\r\nif (p->qp != qp)\r\ncontinue;\r\nlist_del_rcu(&p->list);\r\nmcast->n_attached--;\r\nif (list_empty(&mcast->qp_list)) {\r\nrb_erase(&mcast->rb_node, &mcast_tree);\r\nlast = 1;\r\n}\r\nbreak;\r\n}\r\nspin_unlock_irq(&mcast_lock);\r\nif (p) {\r\nwait_event(mcast->wait, atomic_read(&mcast->refcount) <= 1);\r\nipath_mcast_qp_free(p);\r\n}\r\nif (last) {\r\natomic_dec(&mcast->refcount);\r\nwait_event(mcast->wait, !atomic_read(&mcast->refcount));\r\nipath_mcast_free(mcast);\r\nspin_lock_irq(&dev->n_mcast_grps_lock);\r\ndev->n_mcast_grps_allocated--;\r\nspin_unlock_irq(&dev->n_mcast_grps_lock);\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_mcast_tree_empty(void)\r\n{\r\nreturn mcast_tree.rb_node == NULL;\r\n}
