static struct at_desc *atc_first_active(struct at_dma_chan *atchan)\r\n{\r\nreturn list_first_entry(&atchan->active_list,\r\nstruct at_desc, desc_node);\r\n}\r\nstatic struct at_desc *atc_first_queued(struct at_dma_chan *atchan)\r\n{\r\nreturn list_first_entry(&atchan->queue,\r\nstruct at_desc, desc_node);\r\n}\r\nstatic struct at_desc *atc_alloc_descriptor(struct dma_chan *chan,\r\ngfp_t gfp_flags)\r\n{\r\nstruct at_desc *desc = NULL;\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\ndma_addr_t phys;\r\ndesc = dma_pool_alloc(atdma->dma_desc_pool, gfp_flags, &phys);\r\nif (desc) {\r\nmemset(desc, 0, sizeof(struct at_desc));\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->txd, chan);\r\ndesc->txd.flags = DMA_CTRL_ACK;\r\ndesc->txd.tx_submit = atc_tx_submit;\r\ndesc->txd.phys = phys;\r\n}\r\nreturn desc;\r\n}\r\nstatic struct at_desc *atc_desc_get(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *desc, *_desc;\r\nstruct at_desc *ret = NULL;\r\nunsigned long flags;\r\nunsigned int i = 0;\r\nLIST_HEAD(tmp_list);\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &atchan->free_list, desc_node) {\r\ni++;\r\nif (async_tx_test_ack(&desc->txd)) {\r\nlist_del(&desc->desc_node);\r\nret = desc;\r\nbreak;\r\n}\r\ndev_dbg(chan2dev(&atchan->chan_common),\r\n"desc %p not ACKed\n", desc);\r\n}\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"scanned %u descriptors on freelist\n", i);\r\nif (!ret) {\r\nret = atc_alloc_descriptor(&atchan->chan_common, GFP_ATOMIC);\r\nif (ret) {\r\nspin_lock_irqsave(&atchan->lock, flags);\r\natchan->descs_allocated++;\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n} else {\r\ndev_err(chan2dev(&atchan->chan_common),\r\n"not enough descriptors available\n");\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void atc_desc_put(struct at_dma_chan *atchan, struct at_desc *desc)\r\n{\r\nif (desc) {\r\nstruct at_desc *child;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"moving child desc %p to freelist\n",\r\nchild);\r\nlist_splice_init(&desc->tx_list, &atchan->free_list);\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"moving desc %p to freelist\n", desc);\r\nlist_add(&desc->desc_node, &atchan->free_list);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n}\r\n}\r\nstatic void atc_desc_chain(struct at_desc **first, struct at_desc **prev,\r\nstruct at_desc *desc)\r\n{\r\nif (!(*first)) {\r\n*first = desc;\r\n} else {\r\n(*prev)->lli.dscr = desc->txd.phys;\r\nlist_add_tail(&desc->desc_node,\r\n&(*first)->tx_list);\r\n}\r\n*prev = desc;\r\n}\r\nstatic void atc_dostart(struct at_dma_chan *atchan, struct at_desc *first)\r\n{\r\nstruct at_dma *atdma = to_at_dma(atchan->chan_common.device);\r\nif (atc_chan_is_enabled(atchan)) {\r\ndev_err(chan2dev(&atchan->chan_common),\r\n"BUG: Attempted to start non-idle channel\n");\r\ndev_err(chan2dev(&atchan->chan_common),\r\n" channel: s0x%x d0x%x ctrl0x%x:0x%x l0x%x\n",\r\nchannel_readl(atchan, SADDR),\r\nchannel_readl(atchan, DADDR),\r\nchannel_readl(atchan, CTRLA),\r\nchannel_readl(atchan, CTRLB),\r\nchannel_readl(atchan, DSCR));\r\nreturn;\r\n}\r\nvdbg_dump_regs(atchan);\r\nchannel_writel(atchan, SADDR, 0);\r\nchannel_writel(atchan, DADDR, 0);\r\nchannel_writel(atchan, CTRLA, 0);\r\nchannel_writel(atchan, CTRLB, 0);\r\nchannel_writel(atchan, DSCR, first->txd.phys);\r\ndma_writel(atdma, CHER, atchan->mask);\r\nvdbg_dump_regs(atchan);\r\n}\r\nstatic struct at_desc *atc_get_current_descriptors(struct at_dma_chan *atchan,\r\nu32 dscr_addr)\r\n{\r\nstruct at_desc *desc, *_desc, *child, *desc_cur = NULL;\r\nlist_for_each_entry_safe(desc, _desc, &atchan->active_list, desc_node) {\r\nif (desc->lli.dscr == dscr_addr) {\r\ndesc_cur = desc;\r\nbreak;\r\n}\r\nlist_for_each_entry(child, &desc->tx_list, desc_node) {\r\nif (child->lli.dscr == dscr_addr) {\r\ndesc_cur = child;\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn desc_cur;\r\n}\r\nstatic int atc_get_bytes_left(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nint chan_id = atchan->chan_common.chan_id;\r\nstruct at_desc *desc_first = atc_first_active(atchan);\r\nstruct at_desc *desc_cur;\r\nint ret = 0, count = 0;\r\nif (atchan->remain_desc == 0)\r\natchan->remain_desc = desc_first->len;\r\nif (unlikely(test_bit(ATC_IS_BTC, &atchan->status))) {\r\nclear_bit(ATC_IS_BTC, &atchan->status);\r\ndesc_cur = atc_get_current_descriptors(atchan,\r\nchannel_readl(atchan, DSCR));\r\nif (!desc_cur) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\natchan->remain_desc -= (desc_cur->lli.ctrla & ATC_BTSIZE_MAX)\r\n<< (desc_first->tx_width);\r\nif (atchan->remain_desc < 0) {\r\nret = -EINVAL;\r\ngoto out;\r\n} else {\r\nret = atchan->remain_desc;\r\n}\r\n} else {\r\ncount = (channel_readl(atchan, CTRLA) & ATC_BTSIZE_MAX)\r\n<< (desc_first->tx_width);\r\nret = atchan->remain_desc - count;\r\n}\r\nif (!(dma_readl(atdma, CHSR) & AT_DMA_EMPT(chan_id)))\r\natc_issue_pending(chan);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void\r\natc_chain_complete(struct at_dma_chan *atchan, struct at_desc *desc)\r\n{\r\nstruct dma_async_tx_descriptor *txd = &desc->txd;\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"descriptor %u complete\n", txd->cookie);\r\nif (!atc_chan_is_cyclic(atchan))\r\ndma_cookie_complete(txd);\r\nlist_splice_init(&desc->tx_list, &atchan->free_list);\r\nlist_move(&desc->desc_node, &atchan->free_list);\r\nif (!atchan->chan_common.private) {\r\nstruct device *parent = chan2parent(&atchan->chan_common);\r\nif (!(txd->flags & DMA_COMPL_SKIP_DEST_UNMAP)) {\r\nif (txd->flags & DMA_COMPL_DEST_UNMAP_SINGLE)\r\ndma_unmap_single(parent,\r\ndesc->lli.daddr,\r\ndesc->len, DMA_FROM_DEVICE);\r\nelse\r\ndma_unmap_page(parent,\r\ndesc->lli.daddr,\r\ndesc->len, DMA_FROM_DEVICE);\r\n}\r\nif (!(txd->flags & DMA_COMPL_SKIP_SRC_UNMAP)) {\r\nif (txd->flags & DMA_COMPL_SRC_UNMAP_SINGLE)\r\ndma_unmap_single(parent,\r\ndesc->lli.saddr,\r\ndesc->len, DMA_TO_DEVICE);\r\nelse\r\ndma_unmap_page(parent,\r\ndesc->lli.saddr,\r\ndesc->len, DMA_TO_DEVICE);\r\n}\r\n}\r\nif (!atc_chan_is_cyclic(atchan)) {\r\ndma_async_tx_callback callback = txd->callback;\r\nvoid *param = txd->callback_param;\r\nif (callback)\r\ncallback(param);\r\n}\r\ndma_run_dependencies(txd);\r\n}\r\nstatic void atc_complete_all(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\ndev_vdbg(chan2dev(&atchan->chan_common), "complete all\n");\r\nif (!list_empty(&atchan->queue))\r\natc_dostart(atchan, atc_first_queued(atchan));\r\nlist_splice_init(&atchan->active_list, &list);\r\nlist_splice_init(&atchan->queue, &atchan->active_list);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\natc_chain_complete(atchan, desc);\r\n}\r\nstatic void atc_advance_work(struct at_dma_chan *atchan)\r\n{\r\ndev_vdbg(chan2dev(&atchan->chan_common), "advance_work\n");\r\nif (atc_chan_is_enabled(atchan))\r\nreturn;\r\nif (list_empty(&atchan->active_list) ||\r\nlist_is_singular(&atchan->active_list)) {\r\natc_complete_all(atchan);\r\n} else {\r\natc_chain_complete(atchan, atc_first_active(atchan));\r\natc_dostart(atchan, atc_first_active(atchan));\r\n}\r\n}\r\nstatic void atc_handle_error(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *bad_desc;\r\nstruct at_desc *child;\r\nbad_desc = atc_first_active(atchan);\r\nlist_del_init(&bad_desc->desc_node);\r\nlist_splice_init(&atchan->queue, atchan->active_list.prev);\r\nif (!list_empty(&atchan->active_list))\r\natc_dostart(atchan, atc_first_active(atchan));\r\ndev_crit(chan2dev(&atchan->chan_common),\r\n"Bad descriptor submitted for DMA!\n");\r\ndev_crit(chan2dev(&atchan->chan_common),\r\n" cookie: %d\n", bad_desc->txd.cookie);\r\natc_dump_lli(atchan, &bad_desc->lli);\r\nlist_for_each_entry(child, &bad_desc->tx_list, desc_node)\r\natc_dump_lli(atchan, &child->lli);\r\natc_chain_complete(atchan, bad_desc);\r\n}\r\nstatic void atc_handle_cyclic(struct at_dma_chan *atchan)\r\n{\r\nstruct at_desc *first = atc_first_active(atchan);\r\nstruct dma_async_tx_descriptor *txd = &first->txd;\r\ndma_async_tx_callback callback = txd->callback;\r\nvoid *param = txd->callback_param;\r\ndev_vdbg(chan2dev(&atchan->chan_common),\r\n"new cyclic period llp 0x%08x\n",\r\nchannel_readl(atchan, DSCR));\r\nif (callback)\r\ncallback(param);\r\n}\r\nstatic void atc_tasklet(unsigned long data)\r\n{\r\nstruct at_dma_chan *atchan = (struct at_dma_chan *)data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nif (test_and_clear_bit(ATC_IS_ERROR, &atchan->status))\r\natc_handle_error(atchan);\r\nelse if (atc_chan_is_cyclic(atchan))\r\natc_handle_cyclic(atchan);\r\nelse\r\natc_advance_work(atchan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n}\r\nstatic irqreturn_t at_dma_interrupt(int irq, void *dev_id)\r\n{\r\nstruct at_dma *atdma = (struct at_dma *)dev_id;\r\nstruct at_dma_chan *atchan;\r\nint i;\r\nu32 status, pending, imr;\r\nint ret = IRQ_NONE;\r\ndo {\r\nimr = dma_readl(atdma, EBCIMR);\r\nstatus = dma_readl(atdma, EBCISR);\r\npending = status & imr;\r\nif (!pending)\r\nbreak;\r\ndev_vdbg(atdma->dma_common.dev,\r\n"interrupt: status = 0x%08x, 0x%08x, 0x%08x\n",\r\nstatus, imr, pending);\r\nfor (i = 0; i < atdma->dma_common.chancnt; i++) {\r\natchan = &atdma->chan[i];\r\nif (pending & (AT_DMA_BTC(i) | AT_DMA_ERR(i))) {\r\nif (pending & AT_DMA_ERR(i)) {\r\ndma_writel(atdma, CHDR,\r\nAT_DMA_RES(i) | atchan->mask);\r\nset_bit(ATC_IS_ERROR, &atchan->status);\r\n}\r\nif (pending & AT_DMA_BTC(i))\r\nset_bit(ATC_IS_BTC, &atchan->status);\r\ntasklet_schedule(&atchan->tasklet);\r\nret = IRQ_HANDLED;\r\n}\r\n}\r\n} while (pending);\r\nreturn ret;\r\n}\r\nstatic dma_cookie_t atc_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct at_desc *desc = txd_to_at_desc(tx);\r\nstruct at_dma_chan *atchan = to_at_dma_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nunsigned long flags;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\nif (list_empty(&atchan->active_list)) {\r\ndev_vdbg(chan2dev(tx->chan), "tx_submit: started %u\n",\r\ndesc->txd.cookie);\r\natc_dostart(atchan, desc);\r\nlist_add_tail(&desc->desc_node, &atchan->active_list);\r\n} else {\r\ndev_vdbg(chan2dev(tx->chan), "tx_submit: queued %u\n",\r\ndesc->txd.cookie);\r\nlist_add_tail(&desc->desc_node, &atchan->queue);\r\n}\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_desc *desc = NULL;\r\nstruct at_desc *first = NULL;\r\nstruct at_desc *prev = NULL;\r\nsize_t xfer_count;\r\nsize_t offset;\r\nunsigned int src_width;\r\nunsigned int dst_width;\r\nu32 ctrla;\r\nu32 ctrlb;\r\ndev_vdbg(chan2dev(chan), "prep_dma_memcpy: d0x%x s0x%x l0x%zx f0x%lx\n",\r\ndest, src, len, flags);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan), "prep_dma_memcpy: length is zero!\n");\r\nreturn NULL;\r\n}\r\nctrlb = ATC_DEFAULT_CTRLB | ATC_IEN\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_DST_ADDR_MODE_INCR\r\n| ATC_FC_MEM2MEM;\r\nif (!((src | dest | len) & 3)) {\r\nctrla = ATC_SRC_WIDTH_WORD | ATC_DST_WIDTH_WORD;\r\nsrc_width = dst_width = 2;\r\n} else if (!((src | dest | len) & 1)) {\r\nctrla = ATC_SRC_WIDTH_HALFWORD | ATC_DST_WIDTH_HALFWORD;\r\nsrc_width = dst_width = 1;\r\n} else {\r\nctrla = ATC_SRC_WIDTH_BYTE | ATC_DST_WIDTH_BYTE;\r\nsrc_width = dst_width = 0;\r\n}\r\nfor (offset = 0; offset < len; offset += xfer_count << src_width) {\r\nxfer_count = min_t(size_t, (len - offset) >> src_width,\r\nATC_BTSIZE_MAX);\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\ndesc->lli.saddr = src + offset;\r\ndesc->lli.daddr = dest + offset;\r\ndesc->lli.ctrla = ctrla | xfer_count;\r\ndesc->lli.ctrlb = ctrlb;\r\ndesc->txd.cookie = 0;\r\natc_desc_chain(&first, &prev, desc);\r\n}\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->len = len;\r\nfirst->tx_width = src_width;\r\nset_desc_eol(desc);\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nerr_desc_get:\r\natc_desc_put(atchan, first);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma_slave *atslave = chan->private;\r\nstruct dma_slave_config *sconfig = &atchan->dma_sconfig;\r\nstruct at_desc *first = NULL;\r\nstruct at_desc *prev = NULL;\r\nu32 ctrla;\r\nu32 ctrlb;\r\ndma_addr_t reg;\r\nunsigned int reg_width;\r\nunsigned int mem_width;\r\nunsigned int i;\r\nstruct scatterlist *sg;\r\nsize_t total_len = 0;\r\ndev_vdbg(chan2dev(chan), "prep_slave_sg (%d): %s f0x%lx\n",\r\nsg_len,\r\ndirection == DMA_MEM_TO_DEV ? "TO DEVICE" : "FROM DEVICE",\r\nflags);\r\nif (unlikely(!atslave || !sg_len)) {\r\ndev_dbg(chan2dev(chan), "prep_slave_sg: sg length is zero!\n");\r\nreturn NULL;\r\n}\r\nctrla = ATC_SCSIZE(sconfig->src_maxburst)\r\n| ATC_DCSIZE(sconfig->dst_maxburst);\r\nctrlb = ATC_IEN;\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\nreg_width = convert_buswidth(sconfig->dst_addr_width);\r\nctrla |= ATC_DST_WIDTH(reg_width);\r\nctrlb |= ATC_DST_ADDR_MODE_FIXED\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_FC_MEM2PER\r\n| ATC_SIF(atchan->mem_if) | ATC_DIF(atchan->per_if);\r\nreg = sconfig->dst_addr;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct at_desc *desc;\r\nu32 len;\r\nu32 mem;\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\nmem = sg_dma_address(sg);\r\nlen = sg_dma_len(sg);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan),\r\n"prep_slave_sg: sg(%d) data length is zero\n", i);\r\ngoto err;\r\n}\r\nmem_width = 2;\r\nif (unlikely(mem & 3 || len & 3))\r\nmem_width = 0;\r\ndesc->lli.saddr = mem;\r\ndesc->lli.daddr = reg;\r\ndesc->lli.ctrla = ctrla\r\n| ATC_SRC_WIDTH(mem_width)\r\n| len >> mem_width;\r\ndesc->lli.ctrlb = ctrlb;\r\natc_desc_chain(&first, &prev, desc);\r\ntotal_len += len;\r\n}\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\nreg_width = convert_buswidth(sconfig->src_addr_width);\r\nctrla |= ATC_SRC_WIDTH(reg_width);\r\nctrlb |= ATC_DST_ADDR_MODE_INCR\r\n| ATC_SRC_ADDR_MODE_FIXED\r\n| ATC_FC_PER2MEM\r\n| ATC_SIF(atchan->per_if) | ATC_DIF(atchan->mem_if);\r\nreg = sconfig->src_addr;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct at_desc *desc;\r\nu32 len;\r\nu32 mem;\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\nmem = sg_dma_address(sg);\r\nlen = sg_dma_len(sg);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan),\r\n"prep_slave_sg: sg(%d) data length is zero\n", i);\r\ngoto err;\r\n}\r\nmem_width = 2;\r\nif (unlikely(mem & 3 || len & 3))\r\nmem_width = 0;\r\ndesc->lli.saddr = reg;\r\ndesc->lli.daddr = mem;\r\ndesc->lli.ctrla = ctrla\r\n| ATC_DST_WIDTH(mem_width)\r\n| len >> reg_width;\r\ndesc->lli.ctrlb = ctrlb;\r\natc_desc_chain(&first, &prev, desc);\r\ntotal_len += len;\r\n}\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nset_desc_eol(prev);\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->len = total_len;\r\nfirst->tx_width = reg_width;\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nerr_desc_get:\r\ndev_err(chan2dev(chan), "not enough descriptors available\n");\r\nerr:\r\natc_desc_put(atchan, first);\r\nreturn NULL;\r\n}\r\nstatic int\r\natc_dma_cyclic_check_values(unsigned int reg_width, dma_addr_t buf_addr,\r\nsize_t period_len)\r\n{\r\nif (period_len > (ATC_BTSIZE_MAX << reg_width))\r\ngoto err_out;\r\nif (unlikely(period_len & ((1 << reg_width) - 1)))\r\ngoto err_out;\r\nif (unlikely(buf_addr & ((1 << reg_width) - 1)))\r\ngoto err_out;\r\nreturn 0;\r\nerr_out:\r\nreturn -EINVAL;\r\n}\r\nstatic int\r\natc_dma_cyclic_fill_desc(struct dma_chan *chan, struct at_desc *desc,\r\nunsigned int period_index, dma_addr_t buf_addr,\r\nunsigned int reg_width, size_t period_len,\r\nenum dma_transfer_direction direction)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct dma_slave_config *sconfig = &atchan->dma_sconfig;\r\nu32 ctrla;\r\nctrla = ATC_SCSIZE(sconfig->src_maxburst)\r\n| ATC_DCSIZE(sconfig->dst_maxburst)\r\n| ATC_DST_WIDTH(reg_width)\r\n| ATC_SRC_WIDTH(reg_width)\r\n| period_len >> reg_width;\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\ndesc->lli.saddr = buf_addr + (period_len * period_index);\r\ndesc->lli.daddr = sconfig->dst_addr;\r\ndesc->lli.ctrla = ctrla;\r\ndesc->lli.ctrlb = ATC_DST_ADDR_MODE_FIXED\r\n| ATC_SRC_ADDR_MODE_INCR\r\n| ATC_FC_MEM2PER\r\n| ATC_SIF(atchan->mem_if)\r\n| ATC_DIF(atchan->per_if);\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\ndesc->lli.saddr = sconfig->src_addr;\r\ndesc->lli.daddr = buf_addr + (period_len * period_index);\r\ndesc->lli.ctrla = ctrla;\r\ndesc->lli.ctrlb = ATC_DST_ADDR_MODE_INCR\r\n| ATC_SRC_ADDR_MODE_FIXED\r\n| ATC_FC_PER2MEM\r\n| ATC_SIF(atchan->per_if)\r\n| ATC_DIF(atchan->mem_if);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\natc_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma_slave *atslave = chan->private;\r\nstruct dma_slave_config *sconfig = &atchan->dma_sconfig;\r\nstruct at_desc *first = NULL;\r\nstruct at_desc *prev = NULL;\r\nunsigned long was_cyclic;\r\nunsigned int reg_width;\r\nunsigned int periods = buf_len / period_len;\r\nunsigned int i;\r\ndev_vdbg(chan2dev(chan), "prep_dma_cyclic: %s buf@0x%08x - %d (%d/%d)\n",\r\ndirection == DMA_MEM_TO_DEV ? "TO DEVICE" : "FROM DEVICE",\r\nbuf_addr,\r\nperiods, buf_len, period_len);\r\nif (unlikely(!atslave || !buf_len || !period_len)) {\r\ndev_dbg(chan2dev(chan), "prep_dma_cyclic: length is zero!\n");\r\nreturn NULL;\r\n}\r\nwas_cyclic = test_and_set_bit(ATC_IS_CYCLIC, &atchan->status);\r\nif (was_cyclic) {\r\ndev_dbg(chan2dev(chan), "prep_dma_cyclic: channel in use!\n");\r\nreturn NULL;\r\n}\r\nif (unlikely(!is_slave_direction(direction)))\r\ngoto err_out;\r\nif (sconfig->direction == DMA_MEM_TO_DEV)\r\nreg_width = convert_buswidth(sconfig->dst_addr_width);\r\nelse\r\nreg_width = convert_buswidth(sconfig->src_addr_width);\r\nif (atc_dma_cyclic_check_values(reg_width, buf_addr, period_len))\r\ngoto err_out;\r\nfor (i = 0; i < periods; i++) {\r\nstruct at_desc *desc;\r\ndesc = atc_desc_get(atchan);\r\nif (!desc)\r\ngoto err_desc_get;\r\nif (atc_dma_cyclic_fill_desc(chan, desc, i, buf_addr,\r\nreg_width, period_len, direction))\r\ngoto err_desc_get;\r\natc_desc_chain(&first, &prev, desc);\r\n}\r\nprev->lli.dscr = first->txd.phys;\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->len = buf_len;\r\nfirst->tx_width = reg_width;\r\nreturn &first->txd;\r\nerr_desc_get:\r\ndev_err(chan2dev(chan), "not enough descriptors available\n");\r\natc_desc_put(atchan, first);\r\nerr_out:\r\nclear_bit(ATC_IS_CYCLIC, &atchan->status);\r\nreturn NULL;\r\n}\r\nstatic int set_runtime_config(struct dma_chan *chan,\r\nstruct dma_slave_config *sconfig)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nif (!chan->private)\r\nreturn -EINVAL;\r\nmemcpy(&atchan->dma_sconfig, sconfig, sizeof(*sconfig));\r\nconvert_burst(&atchan->dma_sconfig.src_maxburst);\r\nconvert_burst(&atchan->dma_sconfig.dst_maxburst);\r\nreturn 0;\r\n}\r\nstatic int atc_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nint chan_id = atchan->chan_common.chan_id;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\ndev_vdbg(chan2dev(chan), "atc_control (%d)\n", cmd);\r\nif (cmd == DMA_PAUSE) {\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndma_writel(atdma, CHER, AT_DMA_SUSP(chan_id));\r\nset_bit(ATC_IS_PAUSED, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n} else if (cmd == DMA_RESUME) {\r\nif (!atc_chan_is_paused(atchan))\r\nreturn 0;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndma_writel(atdma, CHDR, AT_DMA_RES(chan_id));\r\nclear_bit(ATC_IS_PAUSED, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n} else if (cmd == DMA_TERMINATE_ALL) {\r\nstruct at_desc *desc, *_desc;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\ndma_writel(atdma, CHDR, AT_DMA_RES(chan_id) | atchan->mask);\r\nwhile (dma_readl(atdma, CHSR) & atchan->mask)\r\ncpu_relax();\r\nlist_splice_init(&atchan->queue, &list);\r\nlist_splice_init(&atchan->active_list, &list);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\natc_chain_complete(atchan, desc);\r\nclear_bit(ATC_IS_PAUSED, &atchan->status);\r\nclear_bit(ATC_IS_CYCLIC, &atchan->status);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n} else if (cmd == DMA_SLAVE_CONFIG) {\r\nreturn set_runtime_config(chan, (struct dma_slave_config *)arg);\r\n} else {\r\nreturn -ENXIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic enum dma_status\r\natc_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nunsigned long flags;\r\nenum dma_status ret;\r\nint bytes = 0;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_SUCCESS)\r\nreturn ret;\r\nif (!txstate)\r\nreturn DMA_ERROR;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\nbytes = atc_get_bytes_left(chan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nif (unlikely(bytes < 0)) {\r\ndev_vdbg(chan2dev(chan), "get residual bytes error\n");\r\nreturn DMA_ERROR;\r\n} else {\r\ndma_set_residue(txstate, bytes);\r\n}\r\ndev_vdbg(chan2dev(chan), "tx_status %d: cookie = %d residue = %d\n",\r\nret, cookie, bytes);\r\nreturn ret;\r\n}\r\nstatic void atc_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nunsigned long flags;\r\ndev_vdbg(chan2dev(chan), "issue_pending\n");\r\nif (atc_chan_is_cyclic(atchan))\r\nreturn;\r\nspin_lock_irqsave(&atchan->lock, flags);\r\natc_advance_work(atchan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\n}\r\nstatic int atc_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nstruct at_desc *desc;\r\nstruct at_dma_slave *atslave;\r\nunsigned long flags;\r\nint i;\r\nu32 cfg;\r\nLIST_HEAD(tmp_list);\r\ndev_vdbg(chan2dev(chan), "alloc_chan_resources\n");\r\nif (atc_chan_is_enabled(atchan)) {\r\ndev_dbg(chan2dev(chan), "DMA channel not idle ?\n");\r\nreturn -EIO;\r\n}\r\ncfg = ATC_DEFAULT_CFG;\r\natslave = chan->private;\r\nif (atslave) {\r\nBUG_ON(!atslave->dma_dev || atslave->dma_dev != atdma->dma_common.dev);\r\nif (atslave->cfg)\r\ncfg = atslave->cfg;\r\n}\r\nif (!list_empty(&atchan->free_list))\r\nreturn atchan->descs_allocated;\r\nfor (i = 0; i < init_nr_desc_per_channel; i++) {\r\ndesc = atc_alloc_descriptor(chan, GFP_KERNEL);\r\nif (!desc) {\r\ndev_err(atdma->dma_common.dev,\r\n"Only %d initial descriptors\n", i);\r\nbreak;\r\n}\r\nlist_add_tail(&desc->desc_node, &tmp_list);\r\n}\r\nspin_lock_irqsave(&atchan->lock, flags);\r\natchan->descs_allocated = i;\r\natchan->remain_desc = 0;\r\nlist_splice(&tmp_list, &atchan->free_list);\r\ndma_cookie_init(chan);\r\nspin_unlock_irqrestore(&atchan->lock, flags);\r\nchannel_writel(atchan, CFG, cfg);\r\ndev_dbg(chan2dev(chan),\r\n"alloc_chan_resources: allocated %d descriptors\n",\r\natchan->descs_allocated);\r\nreturn atchan->descs_allocated;\r\n}\r\nstatic void atc_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nstruct at_dma *atdma = to_at_dma(chan->device);\r\nstruct at_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\ndev_dbg(chan2dev(chan), "free_chan_resources: (descs allocated=%u)\n",\r\natchan->descs_allocated);\r\nBUG_ON(!list_empty(&atchan->active_list));\r\nBUG_ON(!list_empty(&atchan->queue));\r\nBUG_ON(atc_chan_is_enabled(atchan));\r\nlist_for_each_entry_safe(desc, _desc, &atchan->free_list, desc_node) {\r\ndev_vdbg(chan2dev(chan), " freeing descriptor %p\n", desc);\r\nlist_del(&desc->desc_node);\r\ndma_pool_free(atdma->dma_desc_pool, desc, desc->txd.phys);\r\n}\r\nlist_splice_init(&atchan->free_list, &list);\r\natchan->descs_allocated = 0;\r\natchan->status = 0;\r\natchan->remain_desc = 0;\r\ndev_vdbg(chan2dev(chan), "free_chan_resources: done\n");\r\n}\r\nstatic bool at_dma_filter(struct dma_chan *chan, void *slave)\r\n{\r\nstruct at_dma_slave *atslave = slave;\r\nif (atslave->dma_dev == chan->device->dev) {\r\nchan->private = atslave;\r\nreturn true;\r\n} else {\r\nreturn false;\r\n}\r\n}\r\nstatic struct dma_chan *at_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *of_dma)\r\n{\r\nstruct dma_chan *chan;\r\nstruct at_dma_chan *atchan;\r\nstruct at_dma_slave *atslave;\r\ndma_cap_mask_t mask;\r\nunsigned int per_id;\r\nstruct platform_device *dmac_pdev;\r\nif (dma_spec->args_count != 2)\r\nreturn NULL;\r\ndmac_pdev = of_find_device_by_node(dma_spec->np);\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\natslave = devm_kzalloc(&dmac_pdev->dev, sizeof(*atslave), GFP_KERNEL);\r\nif (!atslave)\r\nreturn NULL;\r\natslave->cfg = ATC_DST_H2SEL_HW | ATC_SRC_H2SEL_HW;\r\nper_id = dma_spec->args[1] & AT91_DMA_CFG_PER_ID_MASK;\r\natslave->cfg |= ATC_DST_PER_MSB(per_id) | ATC_DST_PER(per_id)\r\n| ATC_SRC_PER_MSB(per_id) | ATC_SRC_PER(per_id);\r\nswitch (dma_spec->args[1] & AT91_DMA_CFG_FIFOCFG_MASK) {\r\ncase AT91_DMA_CFG_FIFOCFG_ALAP:\r\natslave->cfg |= ATC_FIFOCFG_LARGESTBURST;\r\nbreak;\r\ncase AT91_DMA_CFG_FIFOCFG_ASAP:\r\natslave->cfg |= ATC_FIFOCFG_ENOUGHSPACE;\r\nbreak;\r\ncase AT91_DMA_CFG_FIFOCFG_HALF:\r\ndefault:\r\natslave->cfg |= ATC_FIFOCFG_HALFFIFO;\r\n}\r\natslave->dma_dev = &dmac_pdev->dev;\r\nchan = dma_request_channel(mask, at_dma_filter, atslave);\r\nif (!chan)\r\nreturn NULL;\r\natchan = to_at_dma_chan(chan);\r\natchan->per_if = dma_spec->args[0] & 0xff;\r\natchan->mem_if = (dma_spec->args[0] >> 16) & 0xff;\r\nreturn chan;\r\n}\r\nstatic struct dma_chan *at_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *of_dma)\r\n{\r\nreturn NULL;\r\n}\r\nstatic inline const struct at_dma_platform_data * __init at_dma_get_driver_data(\r\nstruct platform_device *pdev)\r\n{\r\nif (pdev->dev.of_node) {\r\nconst struct of_device_id *match;\r\nmatch = of_match_node(atmel_dma_dt_ids, pdev->dev.of_node);\r\nif (match == NULL)\r\nreturn NULL;\r\nreturn match->data;\r\n}\r\nreturn (struct at_dma_platform_data *)\r\nplatform_get_device_id(pdev)->driver_data;\r\n}\r\nstatic void at_dma_off(struct at_dma *atdma)\r\n{\r\ndma_writel(atdma, EN, 0);\r\ndma_writel(atdma, EBCIDR, -1L);\r\nwhile (dma_readl(atdma, CHSR) & atdma->all_chan_mask)\r\ncpu_relax();\r\n}\r\nstatic int __init at_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *io;\r\nstruct at_dma *atdma;\r\nsize_t size;\r\nint irq;\r\nint err;\r\nint i;\r\nconst struct at_dma_platform_data *plat_dat;\r\ndma_cap_set(DMA_MEMCPY, at91sam9rl_config.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, at91sam9g45_config.cap_mask);\r\ndma_cap_set(DMA_SLAVE, at91sam9g45_config.cap_mask);\r\nplat_dat = at_dma_get_driver_data(pdev);\r\nif (!plat_dat)\r\nreturn -ENODEV;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!io)\r\nreturn -EINVAL;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0)\r\nreturn irq;\r\nsize = sizeof(struct at_dma);\r\nsize += plat_dat->nr_channels * sizeof(struct at_dma_chan);\r\natdma = kzalloc(size, GFP_KERNEL);\r\nif (!atdma)\r\nreturn -ENOMEM;\r\natdma->dma_common.cap_mask = plat_dat->cap_mask;\r\natdma->all_chan_mask = (1 << plat_dat->nr_channels) - 1;\r\nsize = resource_size(io);\r\nif (!request_mem_region(io->start, size, pdev->dev.driver->name)) {\r\nerr = -EBUSY;\r\ngoto err_kfree;\r\n}\r\natdma->regs = ioremap(io->start, size);\r\nif (!atdma->regs) {\r\nerr = -ENOMEM;\r\ngoto err_release_r;\r\n}\r\natdma->clk = clk_get(&pdev->dev, "dma_clk");\r\nif (IS_ERR(atdma->clk)) {\r\nerr = PTR_ERR(atdma->clk);\r\ngoto err_clk;\r\n}\r\nerr = clk_prepare_enable(atdma->clk);\r\nif (err)\r\ngoto err_clk_prepare;\r\nat_dma_off(atdma);\r\nerr = request_irq(irq, at_dma_interrupt, 0, "at_hdmac", atdma);\r\nif (err)\r\ngoto err_irq;\r\nplatform_set_drvdata(pdev, atdma);\r\natdma->dma_desc_pool = dma_pool_create("at_hdmac_desc_pool",\r\n&pdev->dev, sizeof(struct at_desc),\r\n4 , 0);\r\nif (!atdma->dma_desc_pool) {\r\ndev_err(&pdev->dev, "No memory for descriptors dma pool\n");\r\nerr = -ENOMEM;\r\ngoto err_pool_create;\r\n}\r\nwhile (dma_readl(atdma, EBCISR))\r\ncpu_relax();\r\nINIT_LIST_HEAD(&atdma->dma_common.channels);\r\nfor (i = 0; i < plat_dat->nr_channels; i++) {\r\nstruct at_dma_chan *atchan = &atdma->chan[i];\r\natchan->mem_if = AT_DMA_MEM_IF;\r\natchan->per_if = AT_DMA_PER_IF;\r\natchan->chan_common.device = &atdma->dma_common;\r\ndma_cookie_init(&atchan->chan_common);\r\nlist_add_tail(&atchan->chan_common.device_node,\r\n&atdma->dma_common.channels);\r\natchan->ch_regs = atdma->regs + ch_regs(i);\r\nspin_lock_init(&atchan->lock);\r\natchan->mask = 1 << i;\r\nINIT_LIST_HEAD(&atchan->active_list);\r\nINIT_LIST_HEAD(&atchan->queue);\r\nINIT_LIST_HEAD(&atchan->free_list);\r\ntasklet_init(&atchan->tasklet, atc_tasklet,\r\n(unsigned long)atchan);\r\natc_enable_chan_irq(atdma, i);\r\n}\r\natdma->dma_common.device_alloc_chan_resources = atc_alloc_chan_resources;\r\natdma->dma_common.device_free_chan_resources = atc_free_chan_resources;\r\natdma->dma_common.device_tx_status = atc_tx_status;\r\natdma->dma_common.device_issue_pending = atc_issue_pending;\r\natdma->dma_common.dev = &pdev->dev;\r\nif (dma_has_cap(DMA_MEMCPY, atdma->dma_common.cap_mask))\r\natdma->dma_common.device_prep_dma_memcpy = atc_prep_dma_memcpy;\r\nif (dma_has_cap(DMA_SLAVE, atdma->dma_common.cap_mask)) {\r\natdma->dma_common.device_prep_slave_sg = atc_prep_slave_sg;\r\ndma_cap_set(DMA_CYCLIC, atdma->dma_common.cap_mask);\r\natdma->dma_common.device_prep_dma_cyclic = atc_prep_dma_cyclic;\r\natdma->dma_common.device_control = atc_control;\r\n}\r\ndma_writel(atdma, EN, AT_DMA_ENABLE);\r\ndev_info(&pdev->dev, "Atmel AHB DMA Controller ( %s%s), %d channels\n",\r\ndma_has_cap(DMA_MEMCPY, atdma->dma_common.cap_mask) ? "cpy " : "",\r\ndma_has_cap(DMA_SLAVE, atdma->dma_common.cap_mask) ? "slave " : "",\r\nplat_dat->nr_channels);\r\ndma_async_device_register(&atdma->dma_common);\r\nif (pdev->dev.of_node) {\r\nerr = of_dma_controller_register(pdev->dev.of_node,\r\nat_dma_xlate, atdma);\r\nif (err) {\r\ndev_err(&pdev->dev, "could not register of_dma_controller\n");\r\ngoto err_of_dma_controller_register;\r\n}\r\n}\r\nreturn 0;\r\nerr_of_dma_controller_register:\r\ndma_async_device_unregister(&atdma->dma_common);\r\ndma_pool_destroy(atdma->dma_desc_pool);\r\nerr_pool_create:\r\nfree_irq(platform_get_irq(pdev, 0), atdma);\r\nerr_irq:\r\nclk_disable_unprepare(atdma->clk);\r\nerr_clk_prepare:\r\nclk_put(atdma->clk);\r\nerr_clk:\r\niounmap(atdma->regs);\r\natdma->regs = NULL;\r\nerr_release_r:\r\nrelease_mem_region(io->start, size);\r\nerr_kfree:\r\nkfree(atdma);\r\nreturn err;\r\n}\r\nstatic int at_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nstruct resource *io;\r\nat_dma_off(atdma);\r\ndma_async_device_unregister(&atdma->dma_common);\r\ndma_pool_destroy(atdma->dma_desc_pool);\r\nfree_irq(platform_get_irq(pdev, 0), atdma);\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\natc_disable_chan_irq(atdma, chan->chan_id);\r\ntasklet_disable(&atchan->tasklet);\r\ntasklet_kill(&atchan->tasklet);\r\nlist_del(&chan->device_node);\r\n}\r\nclk_disable_unprepare(atdma->clk);\r\nclk_put(atdma->clk);\r\niounmap(atdma->regs);\r\natdma->regs = NULL;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nrelease_mem_region(io->start, resource_size(io));\r\nkfree(atdma);\r\nreturn 0;\r\n}\r\nstatic void at_dma_shutdown(struct platform_device *pdev)\r\n{\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nat_dma_off(platform_get_drvdata(pdev));\r\nclk_disable_unprepare(atdma->clk);\r\n}\r\nstatic int at_dma_prepare(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nif (atc_chan_is_enabled(atchan) && !atc_chan_is_cyclic(atchan))\r\nreturn -EAGAIN;\r\n}\r\nreturn 0;\r\n}\r\nstatic void atc_suspend_cyclic(struct at_dma_chan *atchan)\r\n{\r\nstruct dma_chan *chan = &atchan->chan_common;\r\nif (!atc_chan_is_paused(atchan)) {\r\ndev_warn(chan2dev(chan),\r\n"cyclic channel not paused, should be done by channel user\n");\r\natc_control(chan, DMA_PAUSE, 0);\r\n}\r\natchan->save_dscr = channel_readl(atchan, DSCR);\r\nvdbg_dump_regs(atchan);\r\n}\r\nstatic int at_dma_suspend_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nif (atc_chan_is_cyclic(atchan))\r\natc_suspend_cyclic(atchan);\r\natchan->save_cfg = channel_readl(atchan, CFG);\r\n}\r\natdma->save_imr = dma_readl(atdma, EBCIMR);\r\nat_dma_off(atdma);\r\nclk_disable_unprepare(atdma->clk);\r\nreturn 0;\r\n}\r\nstatic void atc_resume_cyclic(struct at_dma_chan *atchan)\r\n{\r\nstruct at_dma *atdma = to_at_dma(atchan->chan_common.device);\r\nchannel_writel(atchan, SADDR, 0);\r\nchannel_writel(atchan, DADDR, 0);\r\nchannel_writel(atchan, CTRLA, 0);\r\nchannel_writel(atchan, CTRLB, 0);\r\nchannel_writel(atchan, DSCR, atchan->save_dscr);\r\ndma_writel(atdma, CHER, atchan->mask);\r\nvdbg_dump_regs(atchan);\r\n}\r\nstatic int at_dma_resume_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct at_dma *atdma = platform_get_drvdata(pdev);\r\nstruct dma_chan *chan, *_chan;\r\nclk_prepare_enable(atdma->clk);\r\ndma_writel(atdma, EN, AT_DMA_ENABLE);\r\nwhile (dma_readl(atdma, EBCISR))\r\ncpu_relax();\r\ndma_writel(atdma, EBCIER, atdma->save_imr);\r\nlist_for_each_entry_safe(chan, _chan, &atdma->dma_common.channels,\r\ndevice_node) {\r\nstruct at_dma_chan *atchan = to_at_dma_chan(chan);\r\nchannel_writel(atchan, CFG, atchan->save_cfg);\r\nif (atc_chan_is_cyclic(atchan))\r\natc_resume_cyclic(atchan);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init at_dma_init(void)\r\n{\r\nreturn platform_driver_probe(&at_dma_driver, at_dma_probe);\r\n}\r\nstatic void __exit at_dma_exit(void)\r\n{\r\nplatform_driver_unregister(&at_dma_driver);\r\n}
