static inline u32 dcp_chan_reg(u32 reg, int chan)\r\n{\r\nreturn reg + (chan) * 0x40;\r\n}\r\nstatic inline void dcp_write(struct dcp_dev *dev, u32 data, u32 reg)\r\n{\r\nwritel(data, dev->dcp_regs_base + reg);\r\n}\r\nstatic inline void dcp_set(struct dcp_dev *dev, u32 data, u32 reg)\r\n{\r\nwritel(data, dev->dcp_regs_base + (reg | 0x04));\r\n}\r\nstatic inline void dcp_clear(struct dcp_dev *dev, u32 data, u32 reg)\r\n{\r\nwritel(data, dev->dcp_regs_base + (reg | 0x08));\r\n}\r\nstatic inline void dcp_toggle(struct dcp_dev *dev, u32 data, u32 reg)\r\n{\r\nwritel(data, dev->dcp_regs_base + (reg | 0x0C));\r\n}\r\nstatic inline unsigned int dcp_read(struct dcp_dev *dev, u32 reg)\r\n{\r\nreturn readl(dev->dcp_regs_base + reg);\r\n}\r\nstatic void dcp_dma_unmap(struct dcp_dev *dev, struct dcp_hw_packet *pkt)\r\n{\r\ndma_unmap_page(dev->dev, pkt->src, pkt->size, DMA_TO_DEVICE);\r\ndma_unmap_page(dev->dev, pkt->dst, pkt->size, DMA_FROM_DEVICE);\r\ndev_dbg(dev->dev, "unmap packet %x", (unsigned int) pkt);\r\n}\r\nstatic int dcp_dma_map(struct dcp_dev *dev,\r\nstruct ablkcipher_walk *walk, struct dcp_hw_packet *pkt)\r\n{\r\ndev_dbg(dev->dev, "map packet %x", (unsigned int) pkt);\r\npkt->size = walk->nbytes - (walk->nbytes % 16);\r\npkt->src = dma_map_page(dev->dev, walk->src.page, walk->src.offset,\r\npkt->size, DMA_TO_DEVICE);\r\nif (pkt->src == 0) {\r\ndev_err(dev->dev, "Unable to map src");\r\nreturn -ENOMEM;\r\n}\r\npkt->dst = dma_map_page(dev->dev, walk->dst.page, walk->dst.offset,\r\npkt->size, DMA_FROM_DEVICE);\r\nif (pkt->dst == 0) {\r\ndev_err(dev->dev, "Unable to map dst");\r\ndma_unmap_page(dev->dev, pkt->src, pkt->size, DMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void dcp_op_one(struct dcp_dev *dev, struct dcp_hw_packet *pkt,\r\nuint8_t last)\r\n{\r\nstruct dcp_op *ctx = dev->ctx;\r\npkt->pkt1 = ctx->pkt1;\r\npkt->pkt2 = ctx->pkt2;\r\npkt->payload = (u32) dev->payload_base_dma;\r\npkt->stat = 0;\r\nif (ctx->flags & DCP_CBC_INIT) {\r\npkt->pkt1 |= DCP_PKT_CIPHER_INIT;\r\nctx->flags &= ~DCP_CBC_INIT;\r\n}\r\nmod_timer(&dev->watchdog, jiffies + msecs_to_jiffies(500));\r\npkt->pkt1 |= DCP_PKT_IRQ;\r\nif (!last)\r\npkt->pkt1 |= DCP_PKT_CHAIN;\r\ndev->pkt_produced++;\r\ndcp_write(dev, 1,\r\ndcp_chan_reg(DCP_REG_CHAN_SEMA, USED_CHANNEL));\r\n}\r\nstatic void dcp_op_proceed(struct dcp_dev *dev)\r\n{\r\nstruct dcp_op *ctx = dev->ctx;\r\nstruct dcp_hw_packet *pkt;\r\nwhile (ctx->walk.nbytes) {\r\nint err = 0;\r\npkt = dev->hw_pkg[dev->pkt_produced % DCP_MAX_PKG];\r\nerr = dcp_dma_map(dev, &ctx->walk, pkt);\r\nif (err) {\r\ndev->ctx->stat |= err;\r\nmod_timer(&dev->watchdog,\r\njiffies + msecs_to_jiffies(500));\r\nbreak;\r\n}\r\nerr = ctx->walk.nbytes - pkt->size;\r\nablkcipher_walk_done(dev->ctx->req, &dev->ctx->walk, err);\r\ndcp_op_one(dev, pkt, ctx->walk.nbytes == 0);\r\nif (dev->pkt_produced - dev->pkt_consumed == DCP_MAX_PKG)\r\nbreak;\r\n}\r\nclear_bit(DCP_FLAG_PRODUCING, &dev->flags);\r\n}\r\nstatic void dcp_op_start(struct dcp_dev *dev, uint8_t use_walk)\r\n{\r\nstruct dcp_op *ctx = dev->ctx;\r\nif (ctx->flags & DCP_NEW_KEY) {\r\nmemcpy(dev->payload_base, ctx->key, ctx->keylen);\r\nctx->flags &= ~DCP_NEW_KEY;\r\n}\r\nctx->pkt1 = 0;\r\nctx->pkt1 |= DCP_PKT_CIPHER_ENABLE;\r\nctx->pkt1 |= DCP_PKT_DECR_SEM;\r\nif (ctx->flags & DCP_OTP_KEY)\r\nctx->pkt1 |= DCP_PKT_OTP_KEY;\r\nelse\r\nctx->pkt1 |= DCP_PKT_PAYLOAD_KEY;\r\nif (ctx->flags & DCP_ENC)\r\nctx->pkt1 |= DCP_PKG_CIPHER_ENCRYPT;\r\nctx->pkt2 = 0;\r\nif (ctx->flags & DCP_CBC)\r\nctx->pkt2 |= DCP_PKT_MODE_CBC;\r\ndev->pkt_produced = 0;\r\ndev->pkt_consumed = 0;\r\nctx->stat = 0;\r\ndcp_clear(dev, -1, dcp_chan_reg(DCP_REG_CHAN_STAT, USED_CHANNEL));\r\ndcp_write(dev, (u32) dev->hw_phys_pkg,\r\ndcp_chan_reg(DCP_REG_CHAN_PTR, USED_CHANNEL));\r\nset_bit(DCP_FLAG_PRODUCING, &dev->flags);\r\nif (use_walk) {\r\nablkcipher_walk_init(&ctx->walk, ctx->req->dst,\r\nctx->req->src, ctx->req->nbytes);\r\nablkcipher_walk_phys(ctx->req, &ctx->walk);\r\ndcp_op_proceed(dev);\r\n} else {\r\ndcp_op_one(dev, dev->hw_pkg[0], 1);\r\nclear_bit(DCP_FLAG_PRODUCING, &dev->flags);\r\n}\r\n}\r\nstatic void dcp_done_task(unsigned long data)\r\n{\r\nstruct dcp_dev *dev = (struct dcp_dev *)data;\r\nstruct dcp_hw_packet *last_packet;\r\nint fin;\r\nfin = 0;\r\nfor (last_packet = dev->hw_pkg[(dev->pkt_consumed) % DCP_MAX_PKG];\r\nlast_packet->stat == 1;\r\nlast_packet =\r\ndev->hw_pkg[++(dev->pkt_consumed) % DCP_MAX_PKG]) {\r\ndcp_dma_unmap(dev, last_packet);\r\nlast_packet->stat = 0;\r\nfin++;\r\n}\r\nif (fin == 0)\r\nreturn;\r\ndev_dbg(dev->dev,\r\n"Packet(s) done with status %x; finished: %d, produced:%d, complete consumed: %d",\r\ndev->ctx->stat, fin, dev->pkt_produced, dev->pkt_consumed);\r\nlast_packet = dev->hw_pkg[(dev->pkt_consumed - 1) % DCP_MAX_PKG];\r\nif (!dev->ctx->stat && last_packet->pkt1 & DCP_PKT_CHAIN) {\r\nif (!test_and_set_bit(DCP_FLAG_PRODUCING, &dev->flags))\r\ndcp_op_proceed(dev);\r\nreturn;\r\n}\r\nwhile (unlikely(dev->pkt_consumed < dev->pkt_produced)) {\r\ndcp_dma_unmap(dev,\r\ndev->hw_pkg[dev->pkt_consumed++ % DCP_MAX_PKG]);\r\n}\r\nif (dev->ctx->flags & DCP_OTP_KEY) {\r\nclear_bit(DCP_FLAG_BUSY, &dev->flags);\r\nreturn;\r\n}\r\nablkcipher_walk_complete(&dev->ctx->walk);\r\ndev->ctx->req->base.complete(&dev->ctx->req->base,\r\ndev->ctx->stat);\r\ndev->ctx->req = NULL;\r\ntasklet_schedule(&dev->queue_task);\r\n}\r\nstatic void dcp_watchdog(unsigned long data)\r\n{\r\nstruct dcp_dev *dev = (struct dcp_dev *)data;\r\ndev->ctx->stat |= dcp_read(dev,\r\ndcp_chan_reg(DCP_REG_CHAN_STAT, USED_CHANNEL));\r\ndev_err(dev->dev, "Timeout, Channel status: %x", dev->ctx->stat);\r\nif (!dev->ctx->stat)\r\ndev->ctx->stat = -ETIMEDOUT;\r\ndcp_done_task(data);\r\n}\r\nstatic irqreturn_t dcp_common_irq(int irq, void *context)\r\n{\r\nu32 msk;\r\nstruct dcp_dev *dev = (struct dcp_dev *) context;\r\ndel_timer(&dev->watchdog);\r\nmsk = DCP_STAT_IRQ(dcp_read(dev, DCP_REG_STAT));\r\ndcp_clear(dev, msk, DCP_REG_STAT);\r\nif (msk == 0)\r\nreturn IRQ_NONE;\r\ndev->ctx->stat |= dcp_read(dev,\r\ndcp_chan_reg(DCP_REG_CHAN_STAT, USED_CHANNEL));\r\nif (msk & DCP_STAT_CHAN_1)\r\ntasklet_schedule(&dev->done_task);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t dcp_vmi_irq(int irq, void *context)\r\n{\r\nreturn dcp_common_irq(irq, context);\r\n}\r\nstatic irqreturn_t dcp_irq(int irq, void *context)\r\n{\r\nreturn dcp_common_irq(irq, context);\r\n}\r\nstatic void dcp_crypt(struct dcp_dev *dev, struct dcp_op *ctx)\r\n{\r\ndev->ctx = ctx;\r\nif ((ctx->flags & DCP_CBC) && ctx->req->info) {\r\nctx->flags |= DCP_CBC_INIT;\r\nmemcpy(dev->payload_base + AES_KEYSIZE_128,\r\nctx->req->info, AES_KEYSIZE_128);\r\n}\r\ndcp_op_start(dev, 1);\r\n}\r\nstatic void dcp_queue_task(unsigned long data)\r\n{\r\nstruct dcp_dev *dev = (struct dcp_dev *) data;\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct crypto_ablkcipher *tfm;\r\nstruct dcp_op *ctx;\r\nstruct dcp_dev_req_ctx *rctx;\r\nstruct ablkcipher_request *req;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->queue_lock, flags);\r\nbacklog = crypto_get_backlog(&dev->queue);\r\nasync_req = crypto_dequeue_request(&dev->queue);\r\nspin_unlock_irqrestore(&dev->queue_lock, flags);\r\nif (!async_req)\r\ngoto ret_nothing_done;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ablkcipher_request_cast(async_req);\r\ntfm = crypto_ablkcipher_reqtfm(req);\r\nrctx = ablkcipher_request_ctx(req);\r\nctx = crypto_ablkcipher_ctx(tfm);\r\nif (!req->src || !req->dst)\r\ngoto ret_nothing_done;\r\nctx->flags |= rctx->mode;\r\nctx->req = req;\r\ndcp_crypt(dev, ctx);\r\nreturn;\r\nret_nothing_done:\r\nclear_bit(DCP_FLAG_BUSY, &dev->flags);\r\n}\r\nstatic int dcp_cra_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = tfm->__crt_alg->cra_name;\r\nstruct dcp_op *ctx = crypto_tfm_ctx(tfm);\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct dcp_dev_req_ctx);\r\nctx->fallback = crypto_alloc_ablkcipher(name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ctx->fallback)) {\r\ndev_err(global_dev->dev, "Error allocating fallback algo %s\n",\r\nname);\r\nreturn PTR_ERR(ctx->fallback);\r\n}\r\nreturn 0;\r\n}\r\nstatic void dcp_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct dcp_op *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->fallback)\r\ncrypto_free_ablkcipher(ctx->fallback);\r\nctx->fallback = NULL;\r\n}\r\nstatic int dcp_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int len)\r\n{\r\nstruct dcp_op *ctx = crypto_ablkcipher_ctx(tfm);\r\nunsigned int ret = 0;\r\nctx->keylen = len;\r\nctx->flags = 0;\r\nif (len == AES_KEYSIZE_128) {\r\nif (memcmp(ctx->key, key, AES_KEYSIZE_128)) {\r\nmemcpy(ctx->key, key, len);\r\nctx->flags |= DCP_NEW_KEY;\r\n}\r\nreturn 0;\r\n}\r\nctx->fallback->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nctx->fallback->base.crt_flags |=\r\n(tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);\r\nret = crypto_ablkcipher_setkey(ctx->fallback, key, len);\r\nif (ret) {\r\nstruct crypto_tfm *tfm_aux = crypto_ablkcipher_tfm(tfm);\r\ntfm_aux->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm_aux->crt_flags |=\r\n(ctx->fallback->base.crt_flags & CRYPTO_TFM_RES_MASK);\r\n}\r\nreturn ret;\r\n}\r\nstatic int dcp_aes_cbc_crypt(struct ablkcipher_request *req, int mode)\r\n{\r\nstruct dcp_dev_req_ctx *rctx = ablkcipher_request_ctx(req);\r\nstruct dcp_dev *dev = global_dev;\r\nunsigned long flags;\r\nint err = 0;\r\nif (!IS_ALIGNED(req->nbytes, AES_BLOCK_SIZE))\r\nreturn -EINVAL;\r\nrctx->mode = mode;\r\nspin_lock_irqsave(&dev->queue_lock, flags);\r\nerr = ablkcipher_enqueue_request(&dev->queue, req);\r\nspin_unlock_irqrestore(&dev->queue_lock, flags);\r\nflags = test_and_set_bit(DCP_FLAG_BUSY, &dev->flags);\r\nif (!(flags & DCP_FLAG_BUSY))\r\ntasklet_schedule(&dev->queue_task);\r\nreturn err;\r\n}\r\nstatic int dcp_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct dcp_op *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nint err = 0;\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_encrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn dcp_aes_cbc_crypt(req, DCP_AES | DCP_ENC | DCP_CBC);\r\n}\r\nstatic int dcp_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct dcp_op *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nint err = 0;\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_decrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn dcp_aes_cbc_crypt(req, DCP_AES | DCP_DEC | DCP_CBC);\r\n}\r\nstatic int dcp_bootstream_open(struct inode *inode, struct file *file)\r\n{\r\nfile->private_data = container_of((file->private_data),\r\nstruct dcp_dev, dcp_bootstream_misc);\r\nreturn 0;\r\n}\r\nstatic long dcp_bootstream_ioctl(struct file *file,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nstruct dcp_dev *dev = (struct dcp_dev *) file->private_data;\r\nvoid __user *argp = (void __user *)arg;\r\nint ret;\r\nif (dev == NULL)\r\nreturn -EBADF;\r\nif (cmd != DBS_ENC && cmd != DBS_DEC)\r\nreturn -EINVAL;\r\nif (copy_from_user(dev->payload_base, argp, 16))\r\nreturn -EFAULT;\r\nif (test_and_set_bit(DCP_FLAG_BUSY, &dev->flags))\r\nreturn -EAGAIN;\r\ndev->ctx = kzalloc(sizeof(struct dcp_op), GFP_KERNEL);\r\nif (!dev->ctx) {\r\ndev_err(dev->dev,\r\n"cannot allocate context for OTP crypto");\r\nclear_bit(DCP_FLAG_BUSY, &dev->flags);\r\nreturn -ENOMEM;\r\n}\r\ndev->ctx->flags = DCP_AES | DCP_ECB | DCP_OTP_KEY | DCP_CBC_INIT;\r\ndev->ctx->flags |= (cmd == DBS_ENC) ? DCP_ENC : DCP_DEC;\r\ndev->hw_pkg[0]->src = dev->payload_base_dma;\r\ndev->hw_pkg[0]->dst = dev->payload_base_dma;\r\ndev->hw_pkg[0]->size = 16;\r\ndcp_op_start(dev, 0);\r\nwhile (test_bit(DCP_FLAG_BUSY, &dev->flags))\r\ncpu_relax();\r\nret = dev->ctx->stat;\r\nif (!ret && copy_to_user(argp, dev->payload_base, 16))\r\nret = -EFAULT;\r\nkfree(dev->ctx);\r\nreturn ret;\r\n}\r\nstatic int dcp_probe(struct platform_device *pdev)\r\n{\r\nstruct dcp_dev *dev = NULL;\r\nstruct resource *r;\r\nint i, ret, j;\r\ndev = devm_kzalloc(&pdev->dev, sizeof(*dev), GFP_KERNEL);\r\nif (!dev)\r\nreturn -ENOMEM;\r\nglobal_dev = dev;\r\ndev->dev = &pdev->dev;\r\nplatform_set_drvdata(pdev, dev);\r\nr = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!r) {\r\ndev_err(&pdev->dev, "failed to get IORESOURCE_MEM\n");\r\nreturn -ENXIO;\r\n}\r\ndev->dcp_regs_base = devm_ioremap(&pdev->dev, r->start,\r\nresource_size(r));\r\ndcp_set(dev, DCP_CTRL_SFRST, DCP_REG_CTRL);\r\nudelay(10);\r\ndcp_clear(dev, DCP_CTRL_SFRST | DCP_CTRL_CLKGATE, DCP_REG_CTRL);\r\ndcp_write(dev, DCP_CTRL_GATHER_RES_WRITE |\r\nDCP_CTRL_ENABLE_CONTEXT_CACHE | DCP_CTRL_CH_IRQ_E_1,\r\nDCP_REG_CTRL);\r\ndcp_write(dev, DCP_CHAN_CTRL_ENABLE_1, DCP_REG_CHAN_CTRL);\r\nfor (i = 0; i < 4; i++)\r\ndcp_clear(dev, -1, dcp_chan_reg(DCP_REG_CHAN_STAT, i));\r\ndcp_clear(dev, -1, DCP_REG_STAT);\r\nr = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!r) {\r\ndev_err(&pdev->dev, "can't get IRQ resource (0)\n");\r\nreturn -EIO;\r\n}\r\ndev->dcp_vmi_irq = r->start;\r\nret = request_irq(dev->dcp_vmi_irq, dcp_vmi_irq, 0, "dcp", dev);\r\nif (ret != 0) {\r\ndev_err(&pdev->dev, "can't request_irq (0)\n");\r\nreturn -EIO;\r\n}\r\nr = platform_get_resource(pdev, IORESOURCE_IRQ, 1);\r\nif (!r) {\r\ndev_err(&pdev->dev, "can't get IRQ resource (1)\n");\r\nret = -EIO;\r\ngoto err_free_irq0;\r\n}\r\ndev->dcp_irq = r->start;\r\nret = request_irq(dev->dcp_irq, dcp_irq, 0, "dcp", dev);\r\nif (ret != 0) {\r\ndev_err(&pdev->dev, "can't request_irq (1)\n");\r\nret = -EIO;\r\ngoto err_free_irq0;\r\n}\r\ndev->hw_pkg[0] = dma_alloc_coherent(&pdev->dev,\r\nDCP_MAX_PKG * sizeof(struct dcp_hw_packet),\r\n&dev->hw_phys_pkg,\r\nGFP_KERNEL);\r\nif (!dev->hw_pkg[0]) {\r\ndev_err(&pdev->dev, "Could not allocate hw descriptors\n");\r\nret = -ENOMEM;\r\ngoto err_free_irq1;\r\n}\r\nfor (i = 1; i < DCP_MAX_PKG; i++) {\r\ndev->hw_pkg[i - 1]->next = dev->hw_phys_pkg\r\n+ i * sizeof(struct dcp_hw_packet);\r\ndev->hw_pkg[i] = dev->hw_pkg[i - 1] + 1;\r\n}\r\ndev->hw_pkg[i - 1]->next = dev->hw_phys_pkg;\r\ndev->payload_base = dma_alloc_coherent(&pdev->dev, 2 * AES_KEYSIZE_128,\r\n&dev->payload_base_dma, GFP_KERNEL);\r\nif (!dev->payload_base) {\r\ndev_err(&pdev->dev, "Could not allocate memory for key\n");\r\nret = -ENOMEM;\r\ngoto err_free_hw_packet;\r\n}\r\ntasklet_init(&dev->queue_task, dcp_queue_task,\r\n(unsigned long) dev);\r\ntasklet_init(&dev->done_task, dcp_done_task,\r\n(unsigned long) dev);\r\nspin_lock_init(&dev->queue_lock);\r\ncrypto_init_queue(&dev->queue, 10);\r\ninit_timer(&dev->watchdog);\r\ndev->watchdog.function = &dcp_watchdog;\r\ndev->watchdog.data = (unsigned long)dev;\r\ndev->dcp_bootstream_misc.minor = MISC_DYNAMIC_MINOR,\r\ndev->dcp_bootstream_misc.name = "dcpboot",\r\ndev->dcp_bootstream_misc.fops = &dcp_bootstream_fops,\r\nret = misc_register(&dev->dcp_bootstream_misc);\r\nif (ret != 0) {\r\ndev_err(dev->dev, "Unable to register misc device\n");\r\ngoto err_free_key_iv;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(algs); i++) {\r\nalgs[i].cra_priority = 300;\r\nalgs[i].cra_ctxsize = sizeof(struct dcp_op);\r\nalgs[i].cra_module = THIS_MODULE;\r\nalgs[i].cra_init = dcp_cra_init;\r\nalgs[i].cra_exit = dcp_cra_exit;\r\nif (crypto_register_alg(&algs[i])) {\r\ndev_err(&pdev->dev, "register algorithm failed\n");\r\nret = -ENOMEM;\r\ngoto err_unregister;\r\n}\r\n}\r\ndev_notice(&pdev->dev, "DCP crypto enabled.!\n");\r\nreturn 0;\r\nerr_unregister:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_alg(&algs[j]);\r\nerr_free_key_iv:\r\ndma_free_coherent(&pdev->dev, 2 * AES_KEYSIZE_128, dev->payload_base,\r\ndev->payload_base_dma);\r\nerr_free_hw_packet:\r\ndma_free_coherent(&pdev->dev, DCP_MAX_PKG *\r\nsizeof(struct dcp_hw_packet), dev->hw_pkg[0],\r\ndev->hw_phys_pkg);\r\nerr_free_irq1:\r\nfree_irq(dev->dcp_irq, dev);\r\nerr_free_irq0:\r\nfree_irq(dev->dcp_vmi_irq, dev);\r\nreturn ret;\r\n}\r\nstatic int dcp_remove(struct platform_device *pdev)\r\n{\r\nstruct dcp_dev *dev;\r\nint j;\r\ndev = platform_get_drvdata(pdev);\r\ndma_free_coherent(&pdev->dev,\r\nDCP_MAX_PKG * sizeof(struct dcp_hw_packet),\r\ndev->hw_pkg[0], dev->hw_phys_pkg);\r\ndma_free_coherent(&pdev->dev, 2 * AES_KEYSIZE_128, dev->payload_base,\r\ndev->payload_base_dma);\r\nfree_irq(dev->dcp_irq, dev);\r\nfree_irq(dev->dcp_vmi_irq, dev);\r\ntasklet_kill(&dev->done_task);\r\ntasklet_kill(&dev->queue_task);\r\nfor (j = 0; j < ARRAY_SIZE(algs); j++)\r\ncrypto_unregister_alg(&algs[j]);\r\nmisc_deregister(&dev->dcp_bootstream_misc);\r\nreturn 0;\r\n}
