static void flow_cache_new_hashrnd(unsigned long arg)\r\n{\r\nstruct flow_cache *fc = (void *) arg;\r\nint i;\r\nfor_each_possible_cpu(i)\r\nper_cpu_ptr(fc->percpu, i)->hash_rnd_recalc = 1;\r\nfc->rnd_timer.expires = jiffies + FLOW_HASH_RND_PERIOD;\r\nadd_timer(&fc->rnd_timer);\r\n}\r\nstatic int flow_entry_valid(struct flow_cache_entry *fle)\r\n{\r\nif (atomic_read(&flow_cache_genid) != fle->genid)\r\nreturn 0;\r\nif (fle->object && !fle->object->ops->check(fle->object))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void flow_entry_kill(struct flow_cache_entry *fle)\r\n{\r\nif (fle->object)\r\nfle->object->ops->delete(fle->object);\r\nkmem_cache_free(flow_cachep, fle);\r\n}\r\nstatic void flow_cache_gc_task(struct work_struct *work)\r\n{\r\nstruct list_head gc_list;\r\nstruct flow_cache_entry *fce, *n;\r\nINIT_LIST_HEAD(&gc_list);\r\nspin_lock_bh(&flow_cache_gc_lock);\r\nlist_splice_tail_init(&flow_cache_gc_list, &gc_list);\r\nspin_unlock_bh(&flow_cache_gc_lock);\r\nlist_for_each_entry_safe(fce, n, &gc_list, u.gc_list)\r\nflow_entry_kill(fce);\r\n}\r\nstatic void flow_cache_queue_garbage(struct flow_cache_percpu *fcp,\r\nint deleted, struct list_head *gc_list)\r\n{\r\nif (deleted) {\r\nfcp->hash_count -= deleted;\r\nspin_lock_bh(&flow_cache_gc_lock);\r\nlist_splice_tail(gc_list, &flow_cache_gc_list);\r\nspin_unlock_bh(&flow_cache_gc_lock);\r\nschedule_work(&flow_cache_gc_work);\r\n}\r\n}\r\nstatic void __flow_cache_shrink(struct flow_cache *fc,\r\nstruct flow_cache_percpu *fcp,\r\nint shrink_to)\r\n{\r\nstruct flow_cache_entry *fle;\r\nstruct hlist_node *tmp;\r\nLIST_HEAD(gc_list);\r\nint i, deleted = 0;\r\nfor (i = 0; i < flow_cache_hash_size(fc); i++) {\r\nint saved = 0;\r\nhlist_for_each_entry_safe(fle, tmp,\r\n&fcp->hash_table[i], u.hlist) {\r\nif (saved < shrink_to &&\r\nflow_entry_valid(fle)) {\r\nsaved++;\r\n} else {\r\ndeleted++;\r\nhlist_del(&fle->u.hlist);\r\nlist_add_tail(&fle->u.gc_list, &gc_list);\r\n}\r\n}\r\n}\r\nflow_cache_queue_garbage(fcp, deleted, &gc_list);\r\n}\r\nstatic void flow_cache_shrink(struct flow_cache *fc,\r\nstruct flow_cache_percpu *fcp)\r\n{\r\nint shrink_to = fc->low_watermark / flow_cache_hash_size(fc);\r\n__flow_cache_shrink(fc, fcp, shrink_to);\r\n}\r\nstatic void flow_new_hash_rnd(struct flow_cache *fc,\r\nstruct flow_cache_percpu *fcp)\r\n{\r\nget_random_bytes(&fcp->hash_rnd, sizeof(u32));\r\nfcp->hash_rnd_recalc = 0;\r\n__flow_cache_shrink(fc, fcp, 0);\r\n}\r\nstatic u32 flow_hash_code(struct flow_cache *fc,\r\nstruct flow_cache_percpu *fcp,\r\nconst struct flowi *key,\r\nsize_t keysize)\r\n{\r\nconst u32 *k = (const u32 *) key;\r\nconst u32 length = keysize * sizeof(flow_compare_t) / sizeof(u32);\r\nreturn jhash2(k, length, fcp->hash_rnd)\r\n& (flow_cache_hash_size(fc) - 1);\r\n}\r\nstatic int flow_key_compare(const struct flowi *key1, const struct flowi *key2,\r\nsize_t keysize)\r\n{\r\nconst flow_compare_t *k1, *k1_lim, *k2;\r\nk1 = (const flow_compare_t *) key1;\r\nk1_lim = k1 + keysize;\r\nk2 = (const flow_compare_t *) key2;\r\ndo {\r\nif (*k1++ != *k2++)\r\nreturn 1;\r\n} while (k1 < k1_lim);\r\nreturn 0;\r\n}\r\nstruct flow_cache_object *\r\nflow_cache_lookup(struct net *net, const struct flowi *key, u16 family, u8 dir,\r\nflow_resolve_t resolver, void *ctx)\r\n{\r\nstruct flow_cache *fc = &flow_cache_global;\r\nstruct flow_cache_percpu *fcp;\r\nstruct flow_cache_entry *fle, *tfle;\r\nstruct flow_cache_object *flo;\r\nsize_t keysize;\r\nunsigned int hash;\r\nlocal_bh_disable();\r\nfcp = this_cpu_ptr(fc->percpu);\r\nfle = NULL;\r\nflo = NULL;\r\nkeysize = flow_key_size(family);\r\nif (!keysize)\r\ngoto nocache;\r\nif (!fcp->hash_table)\r\ngoto nocache;\r\nif (fcp->hash_rnd_recalc)\r\nflow_new_hash_rnd(fc, fcp);\r\nhash = flow_hash_code(fc, fcp, key, keysize);\r\nhlist_for_each_entry(tfle, &fcp->hash_table[hash], u.hlist) {\r\nif (tfle->net == net &&\r\ntfle->family == family &&\r\ntfle->dir == dir &&\r\nflow_key_compare(key, &tfle->key, keysize) == 0) {\r\nfle = tfle;\r\nbreak;\r\n}\r\n}\r\nif (unlikely(!fle)) {\r\nif (fcp->hash_count > fc->high_watermark)\r\nflow_cache_shrink(fc, fcp);\r\nfle = kmem_cache_alloc(flow_cachep, GFP_ATOMIC);\r\nif (fle) {\r\nfle->net = net;\r\nfle->family = family;\r\nfle->dir = dir;\r\nmemcpy(&fle->key, key, keysize * sizeof(flow_compare_t));\r\nfle->object = NULL;\r\nhlist_add_head(&fle->u.hlist, &fcp->hash_table[hash]);\r\nfcp->hash_count++;\r\n}\r\n} else if (likely(fle->genid == atomic_read(&flow_cache_genid))) {\r\nflo = fle->object;\r\nif (!flo)\r\ngoto ret_object;\r\nflo = flo->ops->get(flo);\r\nif (flo)\r\ngoto ret_object;\r\n} else if (fle->object) {\r\nflo = fle->object;\r\nflo->ops->delete(flo);\r\nfle->object = NULL;\r\n}\r\nnocache:\r\nflo = NULL;\r\nif (fle) {\r\nflo = fle->object;\r\nfle->object = NULL;\r\n}\r\nflo = resolver(net, key, family, dir, flo, ctx);\r\nif (fle) {\r\nfle->genid = atomic_read(&flow_cache_genid);\r\nif (!IS_ERR(flo))\r\nfle->object = flo;\r\nelse\r\nfle->genid--;\r\n} else {\r\nif (!IS_ERR_OR_NULL(flo))\r\nflo->ops->delete(flo);\r\n}\r\nret_object:\r\nlocal_bh_enable();\r\nreturn flo;\r\n}\r\nstatic void flow_cache_flush_tasklet(unsigned long data)\r\n{\r\nstruct flow_flush_info *info = (void *)data;\r\nstruct flow_cache *fc = info->cache;\r\nstruct flow_cache_percpu *fcp;\r\nstruct flow_cache_entry *fle;\r\nstruct hlist_node *tmp;\r\nLIST_HEAD(gc_list);\r\nint i, deleted = 0;\r\nfcp = this_cpu_ptr(fc->percpu);\r\nfor (i = 0; i < flow_cache_hash_size(fc); i++) {\r\nhlist_for_each_entry_safe(fle, tmp,\r\n&fcp->hash_table[i], u.hlist) {\r\nif (flow_entry_valid(fle))\r\ncontinue;\r\ndeleted++;\r\nhlist_del(&fle->u.hlist);\r\nlist_add_tail(&fle->u.gc_list, &gc_list);\r\n}\r\n}\r\nflow_cache_queue_garbage(fcp, deleted, &gc_list);\r\nif (atomic_dec_and_test(&info->cpuleft))\r\ncomplete(&info->completion);\r\n}\r\nstatic int flow_cache_percpu_empty(struct flow_cache *fc, int cpu)\r\n{\r\nstruct flow_cache_percpu *fcp;\r\nint i;\r\nfcp = per_cpu_ptr(fc->percpu, cpu);\r\nfor (i = 0; i < flow_cache_hash_size(fc); i++)\r\nif (!hlist_empty(&fcp->hash_table[i]))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void flow_cache_flush_per_cpu(void *data)\r\n{\r\nstruct flow_flush_info *info = data;\r\nstruct tasklet_struct *tasklet;\r\ntasklet = &this_cpu_ptr(info->cache->percpu)->flush_tasklet;\r\ntasklet->data = (unsigned long)info;\r\ntasklet_schedule(tasklet);\r\n}\r\nvoid flow_cache_flush(void)\r\n{\r\nstruct flow_flush_info info;\r\nstatic DEFINE_MUTEX(flow_flush_sem);\r\ncpumask_var_t mask;\r\nint i, self;\r\nif (!alloc_cpumask_var(&mask, GFP_KERNEL))\r\nreturn;\r\ncpumask_clear(mask);\r\nget_online_cpus();\r\nmutex_lock(&flow_flush_sem);\r\ninfo.cache = &flow_cache_global;\r\nfor_each_online_cpu(i)\r\nif (!flow_cache_percpu_empty(info.cache, i))\r\ncpumask_set_cpu(i, mask);\r\natomic_set(&info.cpuleft, cpumask_weight(mask));\r\nif (atomic_read(&info.cpuleft) == 0)\r\ngoto done;\r\ninit_completion(&info.completion);\r\nlocal_bh_disable();\r\nself = cpumask_test_and_clear_cpu(smp_processor_id(), mask);\r\non_each_cpu_mask(mask, flow_cache_flush_per_cpu, &info, 0);\r\nif (self)\r\nflow_cache_flush_tasklet((unsigned long)&info);\r\nlocal_bh_enable();\r\nwait_for_completion(&info.completion);\r\ndone:\r\nmutex_unlock(&flow_flush_sem);\r\nput_online_cpus();\r\nfree_cpumask_var(mask);\r\n}\r\nstatic void flow_cache_flush_task(struct work_struct *work)\r\n{\r\nflow_cache_flush();\r\n}\r\nvoid flow_cache_flush_deferred(void)\r\n{\r\nschedule_work(&flow_cache_flush_work);\r\n}\r\nstatic int flow_cache_cpu_prepare(struct flow_cache *fc, int cpu)\r\n{\r\nstruct flow_cache_percpu *fcp = per_cpu_ptr(fc->percpu, cpu);\r\nsize_t sz = sizeof(struct hlist_head) * flow_cache_hash_size(fc);\r\nif (!fcp->hash_table) {\r\nfcp->hash_table = kzalloc_node(sz, GFP_KERNEL, cpu_to_node(cpu));\r\nif (!fcp->hash_table) {\r\npr_err("NET: failed to allocate flow cache sz %zu\n", sz);\r\nreturn -ENOMEM;\r\n}\r\nfcp->hash_rnd_recalc = 1;\r\nfcp->hash_count = 0;\r\ntasklet_init(&fcp->flush_tasklet, flow_cache_flush_tasklet, 0);\r\n}\r\nreturn 0;\r\n}\r\nstatic int flow_cache_cpu(struct notifier_block *nfb,\r\nunsigned long action,\r\nvoid *hcpu)\r\n{\r\nstruct flow_cache *fc = container_of(nfb, struct flow_cache, hotcpu_notifier);\r\nint res, cpu = (unsigned long) hcpu;\r\nstruct flow_cache_percpu *fcp = per_cpu_ptr(fc->percpu, cpu);\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\nres = flow_cache_cpu_prepare(fc, cpu);\r\nif (res)\r\nreturn notifier_from_errno(res);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\n__flow_cache_shrink(fc, fcp, 0);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int __init flow_cache_init(struct flow_cache *fc)\r\n{\r\nint i;\r\nfc->hash_shift = 10;\r\nfc->low_watermark = 2 * flow_cache_hash_size(fc);\r\nfc->high_watermark = 4 * flow_cache_hash_size(fc);\r\nfc->percpu = alloc_percpu(struct flow_cache_percpu);\r\nif (!fc->percpu)\r\nreturn -ENOMEM;\r\nfor_each_online_cpu(i) {\r\nif (flow_cache_cpu_prepare(fc, i))\r\ngoto err;\r\n}\r\nfc->hotcpu_notifier = (struct notifier_block){\r\n.notifier_call = flow_cache_cpu,\r\n};\r\nregister_hotcpu_notifier(&fc->hotcpu_notifier);\r\nsetup_timer(&fc->rnd_timer, flow_cache_new_hashrnd,\r\n(unsigned long) fc);\r\nfc->rnd_timer.expires = jiffies + FLOW_HASH_RND_PERIOD;\r\nadd_timer(&fc->rnd_timer);\r\nreturn 0;\r\nerr:\r\nfor_each_possible_cpu(i) {\r\nstruct flow_cache_percpu *fcp = per_cpu_ptr(fc->percpu, i);\r\nkfree(fcp->hash_table);\r\nfcp->hash_table = NULL;\r\n}\r\nfree_percpu(fc->percpu);\r\nfc->percpu = NULL;\r\nreturn -ENOMEM;\r\n}\r\nstatic int __init flow_cache_init_global(void)\r\n{\r\nflow_cachep = kmem_cache_create("flow_cache",\r\nsizeof(struct flow_cache_entry),\r\n0, SLAB_PANIC, NULL);\r\nreturn flow_cache_init(&flow_cache_global);\r\n}
