static int raid6_have_sse2(void)\r\n{\r\nreturn boot_cpu_has(X86_FEATURE_MMX) &&\r\nboot_cpu_has(X86_FEATURE_FXSR) &&\r\nboot_cpu_has(X86_FEATURE_XMM) &&\r\nboot_cpu_has(X86_FEATURE_XMM2);\r\n}\r\nstatic void raid6_sse21_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("movdqa %0,%%xmm0" : : "m" (raid6_sse_constants.x1d[0]));\r\nasm volatile("pxor %xmm5,%xmm5");\r\nfor ( d = 0 ; d < bytes ; d += 16 ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("movdqa %0,%%xmm2" : : "m" (dptr[z0][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0-1][d]));\r\nasm volatile("movdqa %xmm2,%xmm4");\r\nasm volatile("movdqa %0,%%xmm6" : : "m" (dptr[z0-1][d]));\r\nfor ( z = z0-2 ; z >= 0 ; z-- ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("pcmpgtb %xmm4,%xmm5");\r\nasm volatile("paddb %xmm4,%xmm4");\r\nasm volatile("pand %xmm0,%xmm5");\r\nasm volatile("pxor %xmm5,%xmm4");\r\nasm volatile("pxor %xmm5,%xmm5");\r\nasm volatile("pxor %xmm6,%xmm2");\r\nasm volatile("pxor %xmm6,%xmm4");\r\nasm volatile("movdqa %0,%%xmm6" : : "m" (dptr[z][d]));\r\n}\r\nasm volatile("pcmpgtb %xmm4,%xmm5");\r\nasm volatile("paddb %xmm4,%xmm4");\r\nasm volatile("pand %xmm0,%xmm5");\r\nasm volatile("pxor %xmm5,%xmm4");\r\nasm volatile("pxor %xmm5,%xmm5");\r\nasm volatile("pxor %xmm6,%xmm2");\r\nasm volatile("pxor %xmm6,%xmm4");\r\nasm volatile("movntdq %%xmm2,%0" : "=m" (p[d]));\r\nasm volatile("pxor %xmm2,%xmm2");\r\nasm volatile("movntdq %%xmm4,%0" : "=m" (q[d]));\r\nasm volatile("pxor %xmm4,%xmm4");\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_sse22_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("movdqa %0,%%xmm0" : : "m" (raid6_sse_constants.x1d[0]));\r\nasm volatile("pxor %xmm5,%xmm5");\r\nasm volatile("pxor %xmm7,%xmm7");\r\nfor ( d = 0 ; d < bytes ; d += 32 ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("movdqa %0,%%xmm2" : : "m" (dptr[z0][d]));\r\nasm volatile("movdqa %0,%%xmm3" : : "m" (dptr[z0][d+16]));\r\nasm volatile("movdqa %xmm2,%xmm4");\r\nasm volatile("movdqa %xmm3,%xmm6");\r\nfor ( z = z0-1 ; z >= 0 ; z-- ) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("pcmpgtb %xmm4,%xmm5");\r\nasm volatile("pcmpgtb %xmm6,%xmm7");\r\nasm volatile("paddb %xmm4,%xmm4");\r\nasm volatile("paddb %xmm6,%xmm6");\r\nasm volatile("pand %xmm0,%xmm5");\r\nasm volatile("pand %xmm0,%xmm7");\r\nasm volatile("pxor %xmm5,%xmm4");\r\nasm volatile("pxor %xmm7,%xmm6");\r\nasm volatile("movdqa %0,%%xmm5" : : "m" (dptr[z][d]));\r\nasm volatile("movdqa %0,%%xmm7" : : "m" (dptr[z][d+16]));\r\nasm volatile("pxor %xmm5,%xmm2");\r\nasm volatile("pxor %xmm7,%xmm3");\r\nasm volatile("pxor %xmm5,%xmm4");\r\nasm volatile("pxor %xmm7,%xmm6");\r\nasm volatile("pxor %xmm5,%xmm5");\r\nasm volatile("pxor %xmm7,%xmm7");\r\n}\r\nasm volatile("movntdq %%xmm2,%0" : "=m" (p[d]));\r\nasm volatile("movntdq %%xmm3,%0" : "=m" (p[d+16]));\r\nasm volatile("movntdq %%xmm4,%0" : "=m" (q[d]));\r\nasm volatile("movntdq %%xmm6,%0" : "=m" (q[d+16]));\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_sse24_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("movdqa %0,%%xmm0" :: "m" (raid6_sse_constants.x1d[0]));\r\nasm volatile("pxor %xmm2,%xmm2");\r\nasm volatile("pxor %xmm3,%xmm3");\r\nasm volatile("pxor %xmm4,%xmm4");\r\nasm volatile("pxor %xmm5,%xmm5");\r\nasm volatile("pxor %xmm6,%xmm6");\r\nasm volatile("pxor %xmm7,%xmm7");\r\nasm volatile("pxor %xmm10,%xmm10");\r\nasm volatile("pxor %xmm11,%xmm11");\r\nasm volatile("pxor %xmm12,%xmm12");\r\nasm volatile("pxor %xmm13,%xmm13");\r\nasm volatile("pxor %xmm14,%xmm14");\r\nasm volatile("pxor %xmm15,%xmm15");\r\nfor ( d = 0 ; d < bytes ; d += 64 ) {\r\nfor ( z = z0 ; z >= 0 ; z-- ) {\r\nasm volatile("prefetchnta %0" :: "m" (dptr[z][d]));\r\nasm volatile("prefetchnta %0" :: "m" (dptr[z][d+32]));\r\nasm volatile("pcmpgtb %xmm4,%xmm5");\r\nasm volatile("pcmpgtb %xmm6,%xmm7");\r\nasm volatile("pcmpgtb %xmm12,%xmm13");\r\nasm volatile("pcmpgtb %xmm14,%xmm15");\r\nasm volatile("paddb %xmm4,%xmm4");\r\nasm volatile("paddb %xmm6,%xmm6");\r\nasm volatile("paddb %xmm12,%xmm12");\r\nasm volatile("paddb %xmm14,%xmm14");\r\nasm volatile("pand %xmm0,%xmm5");\r\nasm volatile("pand %xmm0,%xmm7");\r\nasm volatile("pand %xmm0,%xmm13");\r\nasm volatile("pand %xmm0,%xmm15");\r\nasm volatile("pxor %xmm5,%xmm4");\r\nasm volatile("pxor %xmm7,%xmm6");\r\nasm volatile("pxor %xmm13,%xmm12");\r\nasm volatile("pxor %xmm15,%xmm14");\r\nasm volatile("movdqa %0,%%xmm5" :: "m" (dptr[z][d]));\r\nasm volatile("movdqa %0,%%xmm7" :: "m" (dptr[z][d+16]));\r\nasm volatile("movdqa %0,%%xmm13" :: "m" (dptr[z][d+32]));\r\nasm volatile("movdqa %0,%%xmm15" :: "m" (dptr[z][d+48]));\r\nasm volatile("pxor %xmm5,%xmm2");\r\nasm volatile("pxor %xmm7,%xmm3");\r\nasm volatile("pxor %xmm13,%xmm10");\r\nasm volatile("pxor %xmm15,%xmm11");\r\nasm volatile("pxor %xmm5,%xmm4");\r\nasm volatile("pxor %xmm7,%xmm6");\r\nasm volatile("pxor %xmm13,%xmm12");\r\nasm volatile("pxor %xmm15,%xmm14");\r\nasm volatile("pxor %xmm5,%xmm5");\r\nasm volatile("pxor %xmm7,%xmm7");\r\nasm volatile("pxor %xmm13,%xmm13");\r\nasm volatile("pxor %xmm15,%xmm15");\r\n}\r\nasm volatile("movntdq %%xmm2,%0" : "=m" (p[d]));\r\nasm volatile("pxor %xmm2,%xmm2");\r\nasm volatile("movntdq %%xmm3,%0" : "=m" (p[d+16]));\r\nasm volatile("pxor %xmm3,%xmm3");\r\nasm volatile("movntdq %%xmm10,%0" : "=m" (p[d+32]));\r\nasm volatile("pxor %xmm10,%xmm10");\r\nasm volatile("movntdq %%xmm11,%0" : "=m" (p[d+48]));\r\nasm volatile("pxor %xmm11,%xmm11");\r\nasm volatile("movntdq %%xmm4,%0" : "=m" (q[d]));\r\nasm volatile("pxor %xmm4,%xmm4");\r\nasm volatile("movntdq %%xmm6,%0" : "=m" (q[d+16]));\r\nasm volatile("pxor %xmm6,%xmm6");\r\nasm volatile("movntdq %%xmm12,%0" : "=m" (q[d+32]));\r\nasm volatile("pxor %xmm12,%xmm12");\r\nasm volatile("movntdq %%xmm14,%0" : "=m" (q[d+48]));\r\nasm volatile("pxor %xmm14,%xmm14");\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nkernel_fpu_end();\r\n}
