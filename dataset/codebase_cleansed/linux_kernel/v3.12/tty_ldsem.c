static inline long ldsem_atomic_update(long delta, struct ld_semaphore *sem)\r\n{\r\nreturn atomic_long_add_return(delta, (atomic_long_t *)&sem->count);\r\n}\r\nstatic inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)\r\n{\r\nlong tmp = *old;\r\n*old = atomic_long_cmpxchg(&sem->count, *old, new);\r\nreturn *old == tmp;\r\n}\r\nvoid __init_ldsem(struct ld_semaphore *sem, const char *name,\r\nstruct lock_class_key *key)\r\n{\r\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\r\ndebug_check_no_locks_freed((void *)sem, sizeof(*sem));\r\nlockdep_init_map(&sem->dep_map, name, key, 0);\r\n#endif\r\nsem->count = LDSEM_UNLOCKED;\r\nsem->wait_readers = 0;\r\nraw_spin_lock_init(&sem->wait_lock);\r\nINIT_LIST_HEAD(&sem->read_wait);\r\nINIT_LIST_HEAD(&sem->write_wait);\r\n}\r\nstatic void __ldsem_wake_readers(struct ld_semaphore *sem)\r\n{\r\nstruct ldsem_waiter *waiter, *next;\r\nstruct task_struct *tsk;\r\nlong adjust, count;\r\nadjust = sem->wait_readers * (LDSEM_ACTIVE_BIAS - LDSEM_WAIT_BIAS);\r\ncount = ldsem_atomic_update(adjust, sem);\r\ndo {\r\nif (count > 0)\r\nbreak;\r\nif (ldsem_cmpxchg(&count, count - adjust, sem))\r\nreturn;\r\n} while (1);\r\nlist_for_each_entry_safe(waiter, next, &sem->read_wait, list) {\r\ntsk = waiter->task;\r\nsmp_mb();\r\nwaiter->task = NULL;\r\nwake_up_process(tsk);\r\nput_task_struct(tsk);\r\n}\r\nINIT_LIST_HEAD(&sem->read_wait);\r\nsem->wait_readers = 0;\r\n}\r\nstatic inline int writer_trylock(struct ld_semaphore *sem)\r\n{\r\nlong count = ldsem_atomic_update(LDSEM_ACTIVE_BIAS, sem);\r\ndo {\r\nif ((count & LDSEM_ACTIVE_MASK) == LDSEM_ACTIVE_BIAS)\r\nreturn 1;\r\nif (ldsem_cmpxchg(&count, count - LDSEM_ACTIVE_BIAS, sem))\r\nreturn 0;\r\n} while (1);\r\n}\r\nstatic void __ldsem_wake_writer(struct ld_semaphore *sem)\r\n{\r\nstruct ldsem_waiter *waiter;\r\nwaiter = list_entry(sem->write_wait.next, struct ldsem_waiter, list);\r\nwake_up_process(waiter->task);\r\n}\r\nstatic void __ldsem_wake(struct ld_semaphore *sem)\r\n{\r\nif (!list_empty(&sem->write_wait))\r\n__ldsem_wake_writer(sem);\r\nelse if (!list_empty(&sem->read_wait))\r\n__ldsem_wake_readers(sem);\r\n}\r\nstatic void ldsem_wake(struct ld_semaphore *sem)\r\n{\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&sem->wait_lock, flags);\r\n__ldsem_wake(sem);\r\nraw_spin_unlock_irqrestore(&sem->wait_lock, flags);\r\n}\r\nstatic struct ld_semaphore __sched *\r\ndown_read_failed(struct ld_semaphore *sem, long count, long timeout)\r\n{\r\nstruct ldsem_waiter waiter;\r\nstruct task_struct *tsk = current;\r\nlong adjust = -LDSEM_ACTIVE_BIAS + LDSEM_WAIT_BIAS;\r\nraw_spin_lock_irq(&sem->wait_lock);\r\ndo {\r\nif (ldsem_cmpxchg(&count, count + adjust, sem))\r\nbreak;\r\nif (count > 0) {\r\nraw_spin_unlock_irq(&sem->wait_lock);\r\nreturn sem;\r\n}\r\n} while (1);\r\nlist_add_tail(&waiter.list, &sem->read_wait);\r\nsem->wait_readers++;\r\nwaiter.task = tsk;\r\nget_task_struct(tsk);\r\nif ((count & LDSEM_ACTIVE_MASK) == 0)\r\n__ldsem_wake(sem);\r\nraw_spin_unlock_irq(&sem->wait_lock);\r\nfor (;;) {\r\nset_task_state(tsk, TASK_UNINTERRUPTIBLE);\r\nif (!waiter.task)\r\nbreak;\r\nif (!timeout)\r\nbreak;\r\ntimeout = schedule_timeout(timeout);\r\n}\r\n__set_task_state(tsk, TASK_RUNNING);\r\nif (!timeout) {\r\nraw_spin_lock_irq(&sem->wait_lock);\r\nif (waiter.task) {\r\nldsem_atomic_update(-LDSEM_WAIT_BIAS, sem);\r\nlist_del(&waiter.list);\r\nraw_spin_unlock_irq(&sem->wait_lock);\r\nput_task_struct(waiter.task);\r\nreturn NULL;\r\n}\r\nraw_spin_unlock_irq(&sem->wait_lock);\r\n}\r\nreturn sem;\r\n}\r\nstatic struct ld_semaphore __sched *\r\ndown_write_failed(struct ld_semaphore *sem, long count, long timeout)\r\n{\r\nstruct ldsem_waiter waiter;\r\nstruct task_struct *tsk = current;\r\nlong adjust = -LDSEM_ACTIVE_BIAS;\r\nint locked = 0;\r\nraw_spin_lock_irq(&sem->wait_lock);\r\ndo {\r\nif (ldsem_cmpxchg(&count, count + adjust, sem))\r\nbreak;\r\nif ((count & LDSEM_ACTIVE_MASK) == LDSEM_ACTIVE_BIAS) {\r\nraw_spin_unlock_irq(&sem->wait_lock);\r\nreturn sem;\r\n}\r\n} while (1);\r\nlist_add_tail(&waiter.list, &sem->write_wait);\r\nwaiter.task = tsk;\r\nset_task_state(tsk, TASK_UNINTERRUPTIBLE);\r\nfor (;;) {\r\nif (!timeout)\r\nbreak;\r\nraw_spin_unlock_irq(&sem->wait_lock);\r\ntimeout = schedule_timeout(timeout);\r\nraw_spin_lock_irq(&sem->wait_lock);\r\nset_task_state(tsk, TASK_UNINTERRUPTIBLE);\r\nif ((locked = writer_trylock(sem)))\r\nbreak;\r\n}\r\nif (!locked)\r\nldsem_atomic_update(-LDSEM_WAIT_BIAS, sem);\r\nlist_del(&waiter.list);\r\nraw_spin_unlock_irq(&sem->wait_lock);\r\n__set_task_state(tsk, TASK_RUNNING);\r\nif (!locked)\r\nreturn NULL;\r\nreturn sem;\r\n}\r\nstatic inline int __ldsem_down_read_nested(struct ld_semaphore *sem,\r\nint subclass, long timeout)\r\n{\r\nlong count;\r\nlockdep_acquire_read(sem, subclass, 0, _RET_IP_);\r\ncount = ldsem_atomic_update(LDSEM_READ_BIAS, sem);\r\nif (count <= 0) {\r\nlock_stat(sem, contended);\r\nif (!down_read_failed(sem, count, timeout)) {\r\nlockdep_release(sem, 1, _RET_IP_);\r\nreturn 0;\r\n}\r\n}\r\nlock_stat(sem, acquired);\r\nreturn 1;\r\n}\r\nstatic inline int __ldsem_down_write_nested(struct ld_semaphore *sem,\r\nint subclass, long timeout)\r\n{\r\nlong count;\r\nlockdep_acquire(sem, subclass, 0, _RET_IP_);\r\ncount = ldsem_atomic_update(LDSEM_WRITE_BIAS, sem);\r\nif ((count & LDSEM_ACTIVE_MASK) != LDSEM_ACTIVE_BIAS) {\r\nlock_stat(sem, contended);\r\nif (!down_write_failed(sem, count, timeout)) {\r\nlockdep_release(sem, 1, _RET_IP_);\r\nreturn 0;\r\n}\r\n}\r\nlock_stat(sem, acquired);\r\nreturn 1;\r\n}\r\nint __sched ldsem_down_read(struct ld_semaphore *sem, long timeout)\r\n{\r\nmight_sleep();\r\nreturn __ldsem_down_read_nested(sem, 0, timeout);\r\n}\r\nint ldsem_down_read_trylock(struct ld_semaphore *sem)\r\n{\r\nlong count = sem->count;\r\nwhile (count >= 0) {\r\nif (ldsem_cmpxchg(&count, count + LDSEM_READ_BIAS, sem)) {\r\nlockdep_acquire_read(sem, 0, 1, _RET_IP_);\r\nlock_stat(sem, acquired);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint __sched ldsem_down_write(struct ld_semaphore *sem, long timeout)\r\n{\r\nmight_sleep();\r\nreturn __ldsem_down_write_nested(sem, 0, timeout);\r\n}\r\nint ldsem_down_write_trylock(struct ld_semaphore *sem)\r\n{\r\nlong count = sem->count;\r\nwhile ((count & LDSEM_ACTIVE_MASK) == 0) {\r\nif (ldsem_cmpxchg(&count, count + LDSEM_WRITE_BIAS, sem)) {\r\nlockdep_acquire(sem, 0, 1, _RET_IP_);\r\nlock_stat(sem, acquired);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid ldsem_up_read(struct ld_semaphore *sem)\r\n{\r\nlong count;\r\nlockdep_release(sem, 1, _RET_IP_);\r\ncount = ldsem_atomic_update(-LDSEM_READ_BIAS, sem);\r\nif (count < 0 && (count & LDSEM_ACTIVE_MASK) == 0)\r\nldsem_wake(sem);\r\n}\r\nvoid ldsem_up_write(struct ld_semaphore *sem)\r\n{\r\nlong count;\r\nlockdep_release(sem, 1, _RET_IP_);\r\ncount = ldsem_atomic_update(-LDSEM_WRITE_BIAS, sem);\r\nif (count < 0)\r\nldsem_wake(sem);\r\n}\r\nint ldsem_down_read_nested(struct ld_semaphore *sem, int subclass, long timeout)\r\n{\r\nmight_sleep();\r\nreturn __ldsem_down_read_nested(sem, subclass, timeout);\r\n}\r\nint ldsem_down_write_nested(struct ld_semaphore *sem, int subclass,\r\nlong timeout)\r\n{\r\nmight_sleep();\r\nreturn __ldsem_down_write_nested(sem, subclass, timeout);\r\n}
