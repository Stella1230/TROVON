static int __init async_tx_init(void)\r\n{\r\nasync_dmaengine_get();\r\nprintk(KERN_INFO "async_tx: api initialized (async)\n");\r\nreturn 0;\r\n}\r\nstatic void __exit async_tx_exit(void)\r\n{\r\nasync_dmaengine_put();\r\n}\r\nstruct dma_chan *\r\n__async_tx_find_channel(struct async_submit_ctl *submit,\r\nenum dma_transaction_type tx_type)\r\n{\r\nstruct dma_async_tx_descriptor *depend_tx = submit->depend_tx;\r\nif (depend_tx &&\r\ndma_has_cap(tx_type, depend_tx->chan->device->cap_mask))\r\nreturn depend_tx->chan;\r\nreturn async_dma_find_channel(tx_type);\r\n}\r\nstatic void\r\nasync_tx_channel_switch(struct dma_async_tx_descriptor *depend_tx,\r\nstruct dma_async_tx_descriptor *tx)\r\n{\r\nstruct dma_chan *chan = depend_tx->chan;\r\nstruct dma_device *device = chan->device;\r\nstruct dma_async_tx_descriptor *intr_tx = (void *) ~0;\r\ntxd_lock(depend_tx);\r\nif (txd_parent(depend_tx) && depend_tx->chan == tx->chan) {\r\ntxd_chain(depend_tx, tx);\r\nintr_tx = NULL;\r\n}\r\ntxd_unlock(depend_tx);\r\nif (!intr_tx) {\r\ndevice->device_issue_pending(chan);\r\nreturn;\r\n}\r\nif (dma_has_cap(DMA_INTERRUPT, device->cap_mask))\r\nintr_tx = device->device_prep_dma_interrupt(chan, 0);\r\nelse\r\nintr_tx = NULL;\r\nif (intr_tx) {\r\nintr_tx->callback = NULL;\r\nintr_tx->callback_param = NULL;\r\ntxd_chain(intr_tx, tx);\r\ntxd_lock(depend_tx);\r\nif (txd_parent(depend_tx)) {\r\ntxd_chain(depend_tx, intr_tx);\r\nasync_tx_ack(intr_tx);\r\nintr_tx = NULL;\r\n}\r\ntxd_unlock(depend_tx);\r\nif (intr_tx) {\r\ntxd_clear_parent(intr_tx);\r\nintr_tx->tx_submit(intr_tx);\r\nasync_tx_ack(intr_tx);\r\n}\r\ndevice->device_issue_pending(chan);\r\n} else {\r\nif (dma_wait_for_async_tx(depend_tx) != DMA_SUCCESS)\r\npanic("%s: DMA error waiting for depend_tx\n",\r\n__func__);\r\ntx->tx_submit(tx);\r\n}\r\n}\r\nvoid\r\nasync_tx_submit(struct dma_chan *chan, struct dma_async_tx_descriptor *tx,\r\nstruct async_submit_ctl *submit)\r\n{\r\nstruct dma_async_tx_descriptor *depend_tx = submit->depend_tx;\r\ntx->callback = submit->cb_fn;\r\ntx->callback_param = submit->cb_param;\r\nif (depend_tx) {\r\nenum submit_disposition s;\r\nBUG_ON(async_tx_test_ack(depend_tx) || txd_next(depend_tx) ||\r\ntxd_parent(tx));\r\ntxd_lock(depend_tx);\r\nif (txd_parent(depend_tx)) {\r\nif (depend_tx->chan == chan) {\r\ntxd_chain(depend_tx, tx);\r\ns = ASYNC_TX_SUBMITTED;\r\n} else\r\ns = ASYNC_TX_CHANNEL_SWITCH;\r\n} else {\r\nif (depend_tx->chan == chan)\r\ns = ASYNC_TX_DIRECT_SUBMIT;\r\nelse\r\ns = ASYNC_TX_CHANNEL_SWITCH;\r\n}\r\ntxd_unlock(depend_tx);\r\nswitch (s) {\r\ncase ASYNC_TX_SUBMITTED:\r\nbreak;\r\ncase ASYNC_TX_CHANNEL_SWITCH:\r\nasync_tx_channel_switch(depend_tx, tx);\r\nbreak;\r\ncase ASYNC_TX_DIRECT_SUBMIT:\r\ntxd_clear_parent(tx);\r\ntx->tx_submit(tx);\r\nbreak;\r\n}\r\n} else {\r\ntxd_clear_parent(tx);\r\ntx->tx_submit(tx);\r\n}\r\nif (submit->flags & ASYNC_TX_ACK)\r\nasync_tx_ack(tx);\r\nif (depend_tx)\r\nasync_tx_ack(depend_tx);\r\n}\r\nstruct dma_async_tx_descriptor *\r\nasync_trigger_callback(struct async_submit_ctl *submit)\r\n{\r\nstruct dma_chan *chan;\r\nstruct dma_device *device;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dma_async_tx_descriptor *depend_tx = submit->depend_tx;\r\nif (depend_tx) {\r\nchan = depend_tx->chan;\r\ndevice = chan->device;\r\nif (device && !dma_has_cap(DMA_INTERRUPT, device->cap_mask))\r\ndevice = NULL;\r\ntx = device ? device->device_prep_dma_interrupt(chan, 0) : NULL;\r\n} else\r\ntx = NULL;\r\nif (tx) {\r\npr_debug("%s: (async)\n", __func__);\r\nasync_tx_submit(chan, tx, submit);\r\n} else {\r\npr_debug("%s: (sync)\n", __func__);\r\nasync_tx_quiesce(&submit->depend_tx);\r\nasync_tx_sync_epilog(submit);\r\n}\r\nreturn tx;\r\n}\r\nvoid async_tx_quiesce(struct dma_async_tx_descriptor **tx)\r\n{\r\nif (*tx) {\r\nBUG_ON(async_tx_test_ack(*tx));\r\nif (dma_wait_for_async_tx(*tx) != DMA_SUCCESS)\r\npanic("%s: DMA error waiting for transaction\n",\r\n__func__);\r\nasync_tx_ack(*tx);\r\n*tx = NULL;\r\n}\r\n}
