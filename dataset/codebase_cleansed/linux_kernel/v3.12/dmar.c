static void __init dmar_register_drhd_unit(struct dmar_drhd_unit *drhd)\r\n{\r\nif (drhd->include_all)\r\nlist_add_tail(&drhd->list, &dmar_drhd_units);\r\nelse\r\nlist_add(&drhd->list, &dmar_drhd_units);\r\n}\r\nstatic int __init dmar_parse_one_dev_scope(struct acpi_dmar_device_scope *scope,\r\nstruct pci_dev **dev, u16 segment)\r\n{\r\nstruct pci_bus *bus;\r\nstruct pci_dev *pdev = NULL;\r\nstruct acpi_dmar_pci_path *path;\r\nint count;\r\nbus = pci_find_bus(segment, scope->bus);\r\npath = (struct acpi_dmar_pci_path *)(scope + 1);\r\ncount = (scope->length - sizeof(struct acpi_dmar_device_scope))\r\n/ sizeof(struct acpi_dmar_pci_path);\r\nwhile (count) {\r\nif (pdev)\r\npci_dev_put(pdev);\r\nif (!bus) {\r\npr_warn("Device scope bus [%d] not found\n", scope->bus);\r\nbreak;\r\n}\r\npdev = pci_get_slot(bus, PCI_DEVFN(path->dev, path->fn));\r\nif (!pdev) {\r\nbreak;\r\n}\r\npath ++;\r\ncount --;\r\nbus = pdev->subordinate;\r\n}\r\nif (!pdev) {\r\npr_warn("Device scope device [%04x:%02x:%02x.%02x] not found\n",\r\nsegment, scope->bus, path->dev, path->fn);\r\n*dev = NULL;\r\nreturn 0;\r\n}\r\nif ((scope->entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT && \\r\npdev->subordinate) || (scope->entry_type == \\r\nACPI_DMAR_SCOPE_TYPE_BRIDGE && !pdev->subordinate)) {\r\npci_dev_put(pdev);\r\npr_warn("Device scope type does not match for %s\n",\r\npci_name(pdev));\r\nreturn -EINVAL;\r\n}\r\n*dev = pdev;\r\nreturn 0;\r\n}\r\nint __init dmar_parse_dev_scope(void *start, void *end, int *cnt,\r\nstruct pci_dev ***devices, u16 segment)\r\n{\r\nstruct acpi_dmar_device_scope *scope;\r\nvoid * tmp = start;\r\nint index;\r\nint ret;\r\n*cnt = 0;\r\nwhile (start < end) {\r\nscope = start;\r\nif (scope->entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT ||\r\nscope->entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE)\r\n(*cnt)++;\r\nelse if (scope->entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC &&\r\nscope->entry_type != ACPI_DMAR_SCOPE_TYPE_HPET) {\r\npr_warn("Unsupported device scope\n");\r\n}\r\nstart += scope->length;\r\n}\r\nif (*cnt == 0)\r\nreturn 0;\r\n*devices = kcalloc(*cnt, sizeof(struct pci_dev *), GFP_KERNEL);\r\nif (!*devices)\r\nreturn -ENOMEM;\r\nstart = tmp;\r\nindex = 0;\r\nwhile (start < end) {\r\nscope = start;\r\nif (scope->entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT ||\r\nscope->entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE) {\r\nret = dmar_parse_one_dev_scope(scope,\r\n&(*devices)[index], segment);\r\nif (ret) {\r\nkfree(*devices);\r\nreturn ret;\r\n}\r\nindex ++;\r\n}\r\nstart += scope->length;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init\r\ndmar_parse_one_drhd(struct acpi_dmar_header *header)\r\n{\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nstruct dmar_drhd_unit *dmaru;\r\nint ret = 0;\r\ndrhd = (struct acpi_dmar_hardware_unit *)header;\r\ndmaru = kzalloc(sizeof(*dmaru), GFP_KERNEL);\r\nif (!dmaru)\r\nreturn -ENOMEM;\r\ndmaru->hdr = header;\r\ndmaru->reg_base_addr = drhd->address;\r\ndmaru->segment = drhd->segment;\r\ndmaru->include_all = drhd->flags & 0x1;\r\nret = alloc_iommu(dmaru);\r\nif (ret) {\r\nkfree(dmaru);\r\nreturn ret;\r\n}\r\ndmar_register_drhd_unit(dmaru);\r\nreturn 0;\r\n}\r\nstatic int __init dmar_parse_dev(struct dmar_drhd_unit *dmaru)\r\n{\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nint ret = 0;\r\ndrhd = (struct acpi_dmar_hardware_unit *) dmaru->hdr;\r\nif (dmaru->include_all)\r\nreturn 0;\r\nret = dmar_parse_dev_scope((void *)(drhd + 1),\r\n((void *)drhd) + drhd->header.length,\r\n&dmaru->devices_cnt, &dmaru->devices,\r\ndrhd->segment);\r\nif (ret) {\r\nlist_del(&dmaru->list);\r\nkfree(dmaru);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __init\r\ndmar_parse_one_rhsa(struct acpi_dmar_header *header)\r\n{\r\nstruct acpi_dmar_rhsa *rhsa;\r\nstruct dmar_drhd_unit *drhd;\r\nrhsa = (struct acpi_dmar_rhsa *)header;\r\nfor_each_drhd_unit(drhd) {\r\nif (drhd->reg_base_addr == rhsa->base_address) {\r\nint node = acpi_map_pxm_to_node(rhsa->proximity_domain);\r\nif (!node_online(node))\r\nnode = -1;\r\ndrhd->iommu->node = node;\r\nreturn 0;\r\n}\r\n}\r\nWARN_TAINT(\r\n1, TAINT_FIRMWARE_WORKAROUND,\r\n"Your BIOS is broken; RHSA refers to non-existent DMAR unit at %llx\n"\r\n"BIOS vendor: %s; Ver: %s; Product Version: %s\n",\r\ndrhd->reg_base_addr,\r\ndmi_get_system_info(DMI_BIOS_VENDOR),\r\ndmi_get_system_info(DMI_BIOS_VERSION),\r\ndmi_get_system_info(DMI_PRODUCT_VERSION));\r\nreturn 0;\r\n}\r\nstatic void __init\r\ndmar_table_print_dmar_entry(struct acpi_dmar_header *header)\r\n{\r\nstruct acpi_dmar_hardware_unit *drhd;\r\nstruct acpi_dmar_reserved_memory *rmrr;\r\nstruct acpi_dmar_atsr *atsr;\r\nstruct acpi_dmar_rhsa *rhsa;\r\nswitch (header->type) {\r\ncase ACPI_DMAR_TYPE_HARDWARE_UNIT:\r\ndrhd = container_of(header, struct acpi_dmar_hardware_unit,\r\nheader);\r\npr_info("DRHD base: %#016Lx flags: %#x\n",\r\n(unsigned long long)drhd->address, drhd->flags);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_RESERVED_MEMORY:\r\nrmrr = container_of(header, struct acpi_dmar_reserved_memory,\r\nheader);\r\npr_info("RMRR base: %#016Lx end: %#016Lx\n",\r\n(unsigned long long)rmrr->base_address,\r\n(unsigned long long)rmrr->end_address);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_ATSR:\r\natsr = container_of(header, struct acpi_dmar_atsr, header);\r\npr_info("ATSR flags: %#x\n", atsr->flags);\r\nbreak;\r\ncase ACPI_DMAR_HARDWARE_AFFINITY:\r\nrhsa = container_of(header, struct acpi_dmar_rhsa, header);\r\npr_info("RHSA base: %#016Lx proximity domain: %#x\n",\r\n(unsigned long long)rhsa->base_address,\r\nrhsa->proximity_domain);\r\nbreak;\r\n}\r\n}\r\nstatic int __init dmar_table_detect(void)\r\n{\r\nacpi_status status = AE_OK;\r\nstatus = acpi_get_table_with_size(ACPI_SIG_DMAR, 0,\r\n(struct acpi_table_header **)&dmar_tbl,\r\n&dmar_tbl_size);\r\nif (ACPI_SUCCESS(status) && !dmar_tbl) {\r\npr_warn("Unable to map DMAR\n");\r\nstatus = AE_NOT_FOUND;\r\n}\r\nreturn (ACPI_SUCCESS(status) ? 1 : 0);\r\n}\r\nstatic int __init\r\nparse_dmar_table(void)\r\n{\r\nstruct acpi_table_dmar *dmar;\r\nstruct acpi_dmar_header *entry_header;\r\nint ret = 0;\r\nint drhd_count = 0;\r\ndmar_table_detect();\r\ndmar_tbl = tboot_get_dmar_table(dmar_tbl);\r\ndmar = (struct acpi_table_dmar *)dmar_tbl;\r\nif (!dmar)\r\nreturn -ENODEV;\r\nif (dmar->width < PAGE_SHIFT - 1) {\r\npr_warn("Invalid DMAR haw\n");\r\nreturn -EINVAL;\r\n}\r\npr_info("Host address width %d\n", dmar->width + 1);\r\nentry_header = (struct acpi_dmar_header *)(dmar + 1);\r\nwhile (((unsigned long)entry_header) <\r\n(((unsigned long)dmar) + dmar_tbl->length)) {\r\nif (entry_header->length == 0) {\r\npr_warn("Invalid 0-length structure\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\ndmar_table_print_dmar_entry(entry_header);\r\nswitch (entry_header->type) {\r\ncase ACPI_DMAR_TYPE_HARDWARE_UNIT:\r\ndrhd_count++;\r\nret = dmar_parse_one_drhd(entry_header);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_RESERVED_MEMORY:\r\nret = dmar_parse_one_rmrr(entry_header);\r\nbreak;\r\ncase ACPI_DMAR_TYPE_ATSR:\r\nret = dmar_parse_one_atsr(entry_header);\r\nbreak;\r\ncase ACPI_DMAR_HARDWARE_AFFINITY:\r\n#ifdef CONFIG_ACPI_NUMA\r\nret = dmar_parse_one_rhsa(entry_header);\r\n#endif\r\nbreak;\r\ndefault:\r\npr_warn("Unknown DMAR structure type %d\n",\r\nentry_header->type);\r\nret = 0;\r\nbreak;\r\n}\r\nif (ret)\r\nbreak;\r\nentry_header = ((void *)entry_header + entry_header->length);\r\n}\r\nif (drhd_count == 0)\r\npr_warn(FW_BUG "No DRHD structure found in DMAR table\n");\r\nreturn ret;\r\n}\r\nstatic int dmar_pci_device_match(struct pci_dev *devices[], int cnt,\r\nstruct pci_dev *dev)\r\n{\r\nint index;\r\nwhile (dev) {\r\nfor (index = 0; index < cnt; index++)\r\nif (dev == devices[index])\r\nreturn 1;\r\ndev = dev->bus->self;\r\n}\r\nreturn 0;\r\n}\r\nstruct dmar_drhd_unit *\r\ndmar_find_matched_drhd_unit(struct pci_dev *dev)\r\n{\r\nstruct dmar_drhd_unit *dmaru = NULL;\r\nstruct acpi_dmar_hardware_unit *drhd;\r\ndev = pci_physfn(dev);\r\nlist_for_each_entry(dmaru, &dmar_drhd_units, list) {\r\ndrhd = container_of(dmaru->hdr,\r\nstruct acpi_dmar_hardware_unit,\r\nheader);\r\nif (dmaru->include_all &&\r\ndrhd->segment == pci_domain_nr(dev->bus))\r\nreturn dmaru;\r\nif (dmar_pci_device_match(dmaru->devices,\r\ndmaru->devices_cnt, dev))\r\nreturn dmaru;\r\n}\r\nreturn NULL;\r\n}\r\nint __init dmar_dev_scope_init(void)\r\n{\r\nstatic int dmar_dev_scope_initialized;\r\nstruct dmar_drhd_unit *drhd, *drhd_n;\r\nint ret = -ENODEV;\r\nif (dmar_dev_scope_initialized)\r\nreturn dmar_dev_scope_initialized;\r\nif (list_empty(&dmar_drhd_units))\r\ngoto fail;\r\nlist_for_each_entry_safe(drhd, drhd_n, &dmar_drhd_units, list) {\r\nret = dmar_parse_dev(drhd);\r\nif (ret)\r\ngoto fail;\r\n}\r\nret = dmar_parse_rmrr_atsr_dev();\r\nif (ret)\r\ngoto fail;\r\ndmar_dev_scope_initialized = 1;\r\nreturn 0;\r\nfail:\r\ndmar_dev_scope_initialized = ret;\r\nreturn ret;\r\n}\r\nint __init dmar_table_init(void)\r\n{\r\nstatic int dmar_table_initialized;\r\nint ret;\r\nif (dmar_table_initialized)\r\nreturn 0;\r\ndmar_table_initialized = 1;\r\nret = parse_dmar_table();\r\nif (ret) {\r\nif (ret != -ENODEV)\r\npr_info("parse DMAR table failure.\n");\r\nreturn ret;\r\n}\r\nif (list_empty(&dmar_drhd_units)) {\r\npr_info("No DMAR devices found\n");\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic void warn_invalid_dmar(u64 addr, const char *message)\r\n{\r\nWARN_TAINT_ONCE(\r\n1, TAINT_FIRMWARE_WORKAROUND,\r\n"Your BIOS is broken; DMAR reported at address %llx%s!\n"\r\n"BIOS vendor: %s; Ver: %s; Product Version: %s\n",\r\naddr, message,\r\ndmi_get_system_info(DMI_BIOS_VENDOR),\r\ndmi_get_system_info(DMI_BIOS_VERSION),\r\ndmi_get_system_info(DMI_PRODUCT_VERSION));\r\n}\r\nint __init check_zero_address(void)\r\n{\r\nstruct acpi_table_dmar *dmar;\r\nstruct acpi_dmar_header *entry_header;\r\nstruct acpi_dmar_hardware_unit *drhd;\r\ndmar = (struct acpi_table_dmar *)dmar_tbl;\r\nentry_header = (struct acpi_dmar_header *)(dmar + 1);\r\nwhile (((unsigned long)entry_header) <\r\n(((unsigned long)dmar) + dmar_tbl->length)) {\r\nif (entry_header->length == 0) {\r\npr_warn("Invalid 0-length structure\n");\r\nreturn 0;\r\n}\r\nif (entry_header->type == ACPI_DMAR_TYPE_HARDWARE_UNIT) {\r\nvoid __iomem *addr;\r\nu64 cap, ecap;\r\ndrhd = (void *)entry_header;\r\nif (!drhd->address) {\r\nwarn_invalid_dmar(0, "");\r\ngoto failed;\r\n}\r\naddr = early_ioremap(drhd->address, VTD_PAGE_SIZE);\r\nif (!addr ) {\r\nprintk("IOMMU: can't validate: %llx\n", drhd->address);\r\ngoto failed;\r\n}\r\ncap = dmar_readq(addr + DMAR_CAP_REG);\r\necap = dmar_readq(addr + DMAR_ECAP_REG);\r\nearly_iounmap(addr, VTD_PAGE_SIZE);\r\nif (cap == (uint64_t)-1 && ecap == (uint64_t)-1) {\r\nwarn_invalid_dmar(drhd->address,\r\n" returns all ones");\r\ngoto failed;\r\n}\r\n}\r\nentry_header = ((void *)entry_header + entry_header->length);\r\n}\r\nreturn 1;\r\nfailed:\r\nreturn 0;\r\n}\r\nint __init detect_intel_iommu(void)\r\n{\r\nint ret;\r\nret = dmar_table_detect();\r\nif (ret)\r\nret = check_zero_address();\r\n{\r\nstruct acpi_table_dmar *dmar;\r\ndmar = (struct acpi_table_dmar *) dmar_tbl;\r\nif (ret && irq_remapping_enabled && cpu_has_x2apic &&\r\ndmar->flags & 0x1)\r\npr_info("Queued invalidation will be enabled to support x2apic and Intr-remapping.\n");\r\nif (ret && !no_iommu && !iommu_detected && !dmar_disabled) {\r\niommu_detected = 1;\r\npci_request_acs();\r\n}\r\n#ifdef CONFIG_X86\r\nif (ret)\r\nx86_init.iommu.iommu_init = intel_iommu_init;\r\n#endif\r\n}\r\nearly_acpi_os_unmap_memory(dmar_tbl, dmar_tbl_size);\r\ndmar_tbl = NULL;\r\nreturn ret ? 1 : -ENODEV;\r\n}\r\nstatic void unmap_iommu(struct intel_iommu *iommu)\r\n{\r\niounmap(iommu->reg);\r\nrelease_mem_region(iommu->reg_phys, iommu->reg_size);\r\n}\r\nstatic int map_iommu(struct intel_iommu *iommu, u64 phys_addr)\r\n{\r\nint map_size, err=0;\r\niommu->reg_phys = phys_addr;\r\niommu->reg_size = VTD_PAGE_SIZE;\r\nif (!request_mem_region(iommu->reg_phys, iommu->reg_size, iommu->name)) {\r\npr_err("IOMMU: can't reserve memory\n");\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\niommu->reg = ioremap(iommu->reg_phys, iommu->reg_size);\r\nif (!iommu->reg) {\r\npr_err("IOMMU: can't map the region\n");\r\nerr = -ENOMEM;\r\ngoto release;\r\n}\r\niommu->cap = dmar_readq(iommu->reg + DMAR_CAP_REG);\r\niommu->ecap = dmar_readq(iommu->reg + DMAR_ECAP_REG);\r\nif (iommu->cap == (uint64_t)-1 && iommu->ecap == (uint64_t)-1) {\r\nerr = -EINVAL;\r\nwarn_invalid_dmar(phys_addr, " returns all ones");\r\ngoto unmap;\r\n}\r\nmap_size = max_t(int, ecap_max_iotlb_offset(iommu->ecap),\r\ncap_max_fault_reg_offset(iommu->cap));\r\nmap_size = VTD_PAGE_ALIGN(map_size);\r\nif (map_size > iommu->reg_size) {\r\niounmap(iommu->reg);\r\nrelease_mem_region(iommu->reg_phys, iommu->reg_size);\r\niommu->reg_size = map_size;\r\nif (!request_mem_region(iommu->reg_phys, iommu->reg_size,\r\niommu->name)) {\r\npr_err("IOMMU: can't reserve memory\n");\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\niommu->reg = ioremap(iommu->reg_phys, iommu->reg_size);\r\nif (!iommu->reg) {\r\npr_err("IOMMU: can't map the region\n");\r\nerr = -ENOMEM;\r\ngoto release;\r\n}\r\n}\r\nerr = 0;\r\ngoto out;\r\nunmap:\r\niounmap(iommu->reg);\r\nrelease:\r\nrelease_mem_region(iommu->reg_phys, iommu->reg_size);\r\nout:\r\nreturn err;\r\n}\r\nint alloc_iommu(struct dmar_drhd_unit *drhd)\r\n{\r\nstruct intel_iommu *iommu;\r\nu32 ver, sts;\r\nstatic int iommu_allocated = 0;\r\nint agaw = 0;\r\nint msagaw = 0;\r\nint err;\r\nif (!drhd->reg_base_addr) {\r\nwarn_invalid_dmar(0, "");\r\nreturn -EINVAL;\r\n}\r\niommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\r\nif (!iommu)\r\nreturn -ENOMEM;\r\niommu->seq_id = iommu_allocated++;\r\nsprintf (iommu->name, "dmar%d", iommu->seq_id);\r\nerr = map_iommu(iommu, drhd->reg_base_addr);\r\nif (err) {\r\npr_err("IOMMU: failed to map %s\n", iommu->name);\r\ngoto error;\r\n}\r\nerr = -EINVAL;\r\nagaw = iommu_calculate_agaw(iommu);\r\nif (agaw < 0) {\r\npr_err("Cannot get a valid agaw for iommu (seq_id = %d)\n",\r\niommu->seq_id);\r\ngoto err_unmap;\r\n}\r\nmsagaw = iommu_calculate_max_sagaw(iommu);\r\nif (msagaw < 0) {\r\npr_err("Cannot get a valid max agaw for iommu (seq_id = %d)\n",\r\niommu->seq_id);\r\ngoto err_unmap;\r\n}\r\niommu->agaw = agaw;\r\niommu->msagaw = msagaw;\r\niommu->node = -1;\r\nver = readl(iommu->reg + DMAR_VER_REG);\r\npr_info("IOMMU %d: reg_base_addr %llx ver %d:%d cap %llx ecap %llx\n",\r\niommu->seq_id,\r\n(unsigned long long)drhd->reg_base_addr,\r\nDMAR_VER_MAJOR(ver), DMAR_VER_MINOR(ver),\r\n(unsigned long long)iommu->cap,\r\n(unsigned long long)iommu->ecap);\r\nsts = readl(iommu->reg + DMAR_GSTS_REG);\r\nif (sts & DMA_GSTS_IRES)\r\niommu->gcmd |= DMA_GCMD_IRE;\r\nif (sts & DMA_GSTS_TES)\r\niommu->gcmd |= DMA_GCMD_TE;\r\nif (sts & DMA_GSTS_QIES)\r\niommu->gcmd |= DMA_GCMD_QIE;\r\nraw_spin_lock_init(&iommu->register_lock);\r\ndrhd->iommu = iommu;\r\nreturn 0;\r\nerr_unmap:\r\nunmap_iommu(iommu);\r\nerror:\r\nkfree(iommu);\r\nreturn err;\r\n}\r\nvoid free_iommu(struct intel_iommu *iommu)\r\n{\r\nif (!iommu)\r\nreturn;\r\nfree_dmar_iommu(iommu);\r\nif (iommu->reg)\r\nunmap_iommu(iommu);\r\nkfree(iommu);\r\n}\r\nstatic inline void reclaim_free_desc(struct q_inval *qi)\r\n{\r\nwhile (qi->desc_status[qi->free_tail] == QI_DONE ||\r\nqi->desc_status[qi->free_tail] == QI_ABORT) {\r\nqi->desc_status[qi->free_tail] = QI_FREE;\r\nqi->free_tail = (qi->free_tail + 1) % QI_LENGTH;\r\nqi->free_cnt++;\r\n}\r\n}\r\nstatic int qi_check_fault(struct intel_iommu *iommu, int index)\r\n{\r\nu32 fault;\r\nint head, tail;\r\nstruct q_inval *qi = iommu->qi;\r\nint wait_index = (index + 1) % QI_LENGTH;\r\nif (qi->desc_status[wait_index] == QI_ABORT)\r\nreturn -EAGAIN;\r\nfault = readl(iommu->reg + DMAR_FSTS_REG);\r\nif (fault & DMA_FSTS_IQE) {\r\nhead = readl(iommu->reg + DMAR_IQH_REG);\r\nif ((head >> DMAR_IQ_SHIFT) == index) {\r\npr_err("VT-d detected invalid descriptor: "\r\n"low=%llx, high=%llx\n",\r\n(unsigned long long)qi->desc[index].low,\r\n(unsigned long long)qi->desc[index].high);\r\nmemcpy(&qi->desc[index], &qi->desc[wait_index],\r\nsizeof(struct qi_desc));\r\n__iommu_flush_cache(iommu, &qi->desc[index],\r\nsizeof(struct qi_desc));\r\nwritel(DMA_FSTS_IQE, iommu->reg + DMAR_FSTS_REG);\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (fault & DMA_FSTS_ITE) {\r\nhead = readl(iommu->reg + DMAR_IQH_REG);\r\nhead = ((head >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;\r\nhead |= 1;\r\ntail = readl(iommu->reg + DMAR_IQT_REG);\r\ntail = ((tail >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;\r\nwritel(DMA_FSTS_ITE, iommu->reg + DMAR_FSTS_REG);\r\ndo {\r\nif (qi->desc_status[head] == QI_IN_USE)\r\nqi->desc_status[head] = QI_ABORT;\r\nhead = (head - 2 + QI_LENGTH) % QI_LENGTH;\r\n} while (head != tail);\r\nif (qi->desc_status[wait_index] == QI_ABORT)\r\nreturn -EAGAIN;\r\n}\r\nif (fault & DMA_FSTS_ICE)\r\nwritel(DMA_FSTS_ICE, iommu->reg + DMAR_FSTS_REG);\r\nreturn 0;\r\n}\r\nint qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu)\r\n{\r\nint rc;\r\nstruct q_inval *qi = iommu->qi;\r\nstruct qi_desc *hw, wait_desc;\r\nint wait_index, index;\r\nunsigned long flags;\r\nif (!qi)\r\nreturn 0;\r\nhw = qi->desc;\r\nrestart:\r\nrc = 0;\r\nraw_spin_lock_irqsave(&qi->q_lock, flags);\r\nwhile (qi->free_cnt < 3) {\r\nraw_spin_unlock_irqrestore(&qi->q_lock, flags);\r\ncpu_relax();\r\nraw_spin_lock_irqsave(&qi->q_lock, flags);\r\n}\r\nindex = qi->free_head;\r\nwait_index = (index + 1) % QI_LENGTH;\r\nqi->desc_status[index] = qi->desc_status[wait_index] = QI_IN_USE;\r\nhw[index] = *desc;\r\nwait_desc.low = QI_IWD_STATUS_DATA(QI_DONE) |\r\nQI_IWD_STATUS_WRITE | QI_IWD_TYPE;\r\nwait_desc.high = virt_to_phys(&qi->desc_status[wait_index]);\r\nhw[wait_index] = wait_desc;\r\n__iommu_flush_cache(iommu, &hw[index], sizeof(struct qi_desc));\r\n__iommu_flush_cache(iommu, &hw[wait_index], sizeof(struct qi_desc));\r\nqi->free_head = (qi->free_head + 2) % QI_LENGTH;\r\nqi->free_cnt -= 2;\r\nwritel(qi->free_head << DMAR_IQ_SHIFT, iommu->reg + DMAR_IQT_REG);\r\nwhile (qi->desc_status[wait_index] != QI_DONE) {\r\nrc = qi_check_fault(iommu, index);\r\nif (rc)\r\nbreak;\r\nraw_spin_unlock(&qi->q_lock);\r\ncpu_relax();\r\nraw_spin_lock(&qi->q_lock);\r\n}\r\nqi->desc_status[index] = QI_DONE;\r\nreclaim_free_desc(qi);\r\nraw_spin_unlock_irqrestore(&qi->q_lock, flags);\r\nif (rc == -EAGAIN)\r\ngoto restart;\r\nreturn rc;\r\n}\r\nvoid qi_global_iec(struct intel_iommu *iommu)\r\n{\r\nstruct qi_desc desc;\r\ndesc.low = QI_IEC_TYPE;\r\ndesc.high = 0;\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid qi_flush_context(struct intel_iommu *iommu, u16 did, u16 sid, u8 fm,\r\nu64 type)\r\n{\r\nstruct qi_desc desc;\r\ndesc.low = QI_CC_FM(fm) | QI_CC_SID(sid) | QI_CC_DID(did)\r\n| QI_CC_GRAN(type) | QI_CC_TYPE;\r\ndesc.high = 0;\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,\r\nunsigned int size_order, u64 type)\r\n{\r\nu8 dw = 0, dr = 0;\r\nstruct qi_desc desc;\r\nint ih = 0;\r\nif (cap_write_drain(iommu->cap))\r\ndw = 1;\r\nif (cap_read_drain(iommu->cap))\r\ndr = 1;\r\ndesc.low = QI_IOTLB_DID(did) | QI_IOTLB_DR(dr) | QI_IOTLB_DW(dw)\r\n| QI_IOTLB_GRAN(type) | QI_IOTLB_TYPE;\r\ndesc.high = QI_IOTLB_ADDR(addr) | QI_IOTLB_IH(ih)\r\n| QI_IOTLB_AM(size_order);\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 qdep,\r\nu64 addr, unsigned mask)\r\n{\r\nstruct qi_desc desc;\r\nif (mask) {\r\nBUG_ON(addr & ((1 << (VTD_PAGE_SHIFT + mask)) - 1));\r\naddr |= (1 << (VTD_PAGE_SHIFT + mask - 1)) - 1;\r\ndesc.high = QI_DEV_IOTLB_ADDR(addr) | QI_DEV_IOTLB_SIZE;\r\n} else\r\ndesc.high = QI_DEV_IOTLB_ADDR(addr);\r\nif (qdep >= QI_DEV_IOTLB_MAX_INVS)\r\nqdep = 0;\r\ndesc.low = QI_DEV_IOTLB_SID(sid) | QI_DEV_IOTLB_QDEP(qdep) |\r\nQI_DIOTLB_TYPE;\r\nqi_submit_sync(&desc, iommu);\r\n}\r\nvoid dmar_disable_qi(struct intel_iommu *iommu)\r\n{\r\nunsigned long flags;\r\nu32 sts;\r\ncycles_t start_time = get_cycles();\r\nif (!ecap_qis(iommu->ecap))\r\nreturn;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flags);\r\nsts = dmar_readq(iommu->reg + DMAR_GSTS_REG);\r\nif (!(sts & DMA_GSTS_QIES))\r\ngoto end;\r\nwhile ((readl(iommu->reg + DMAR_IQT_REG) !=\r\nreadl(iommu->reg + DMAR_IQH_REG)) &&\r\n(DMAR_OPERATION_TIMEOUT > (get_cycles() - start_time)))\r\ncpu_relax();\r\niommu->gcmd &= ~DMA_GCMD_QIE;\r\nwritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\r\nIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, readl,\r\n!(sts & DMA_GSTS_QIES), sts);\r\nend:\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flags);\r\n}\r\nstatic void __dmar_enable_qi(struct intel_iommu *iommu)\r\n{\r\nu32 sts;\r\nunsigned long flags;\r\nstruct q_inval *qi = iommu->qi;\r\nqi->free_head = qi->free_tail = 0;\r\nqi->free_cnt = QI_LENGTH;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flags);\r\nwritel(0, iommu->reg + DMAR_IQT_REG);\r\ndmar_writeq(iommu->reg + DMAR_IQA_REG, virt_to_phys(qi->desc));\r\niommu->gcmd |= DMA_GCMD_QIE;\r\nwritel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);\r\nIOMMU_WAIT_OP(iommu, DMAR_GSTS_REG, readl, (sts & DMA_GSTS_QIES), sts);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flags);\r\n}\r\nint dmar_enable_qi(struct intel_iommu *iommu)\r\n{\r\nstruct q_inval *qi;\r\nstruct page *desc_page;\r\nif (!ecap_qis(iommu->ecap))\r\nreturn -ENOENT;\r\nif (iommu->qi)\r\nreturn 0;\r\niommu->qi = kmalloc(sizeof(*qi), GFP_ATOMIC);\r\nif (!iommu->qi)\r\nreturn -ENOMEM;\r\nqi = iommu->qi;\r\ndesc_page = alloc_pages_node(iommu->node, GFP_ATOMIC | __GFP_ZERO, 0);\r\nif (!desc_page) {\r\nkfree(qi);\r\niommu->qi = 0;\r\nreturn -ENOMEM;\r\n}\r\nqi->desc = page_address(desc_page);\r\nqi->desc_status = kzalloc(QI_LENGTH * sizeof(int), GFP_ATOMIC);\r\nif (!qi->desc_status) {\r\nfree_page((unsigned long) qi->desc);\r\nkfree(qi);\r\niommu->qi = 0;\r\nreturn -ENOMEM;\r\n}\r\nqi->free_head = qi->free_tail = 0;\r\nqi->free_cnt = QI_LENGTH;\r\nraw_spin_lock_init(&qi->q_lock);\r\n__dmar_enable_qi(iommu);\r\nreturn 0;\r\n}\r\nconst char *dmar_get_fault_reason(u8 fault_reason, int *fault_type)\r\n{\r\nif (fault_reason >= 0x20 && (fault_reason - 0x20 <\r\nARRAY_SIZE(irq_remap_fault_reasons))) {\r\n*fault_type = INTR_REMAP;\r\nreturn irq_remap_fault_reasons[fault_reason - 0x20];\r\n} else if (fault_reason < ARRAY_SIZE(dma_remap_fault_reasons)) {\r\n*fault_type = DMA_REMAP;\r\nreturn dma_remap_fault_reasons[fault_reason];\r\n} else {\r\n*fault_type = UNKNOWN;\r\nreturn "Unknown";\r\n}\r\n}\r\nvoid dmar_msi_unmask(struct irq_data *data)\r\n{\r\nstruct intel_iommu *iommu = irq_data_get_irq_handler_data(data);\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nwritel(0, iommu->reg + DMAR_FECTL_REG);\r\nreadl(iommu->reg + DMAR_FECTL_REG);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nvoid dmar_msi_mask(struct irq_data *data)\r\n{\r\nunsigned long flag;\r\nstruct intel_iommu *iommu = irq_data_get_irq_handler_data(data);\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nwritel(DMA_FECTL_IM, iommu->reg + DMAR_FECTL_REG);\r\nreadl(iommu->reg + DMAR_FECTL_REG);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nvoid dmar_msi_write(int irq, struct msi_msg *msg)\r\n{\r\nstruct intel_iommu *iommu = irq_get_handler_data(irq);\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nwritel(msg->data, iommu->reg + DMAR_FEDATA_REG);\r\nwritel(msg->address_lo, iommu->reg + DMAR_FEADDR_REG);\r\nwritel(msg->address_hi, iommu->reg + DMAR_FEUADDR_REG);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nvoid dmar_msi_read(int irq, struct msi_msg *msg)\r\n{\r\nstruct intel_iommu *iommu = irq_get_handler_data(irq);\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nmsg->data = readl(iommu->reg + DMAR_FEDATA_REG);\r\nmsg->address_lo = readl(iommu->reg + DMAR_FEADDR_REG);\r\nmsg->address_hi = readl(iommu->reg + DMAR_FEUADDR_REG);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\n}\r\nstatic int dmar_fault_do_one(struct intel_iommu *iommu, int type,\r\nu8 fault_reason, u16 source_id, unsigned long long addr)\r\n{\r\nconst char *reason;\r\nint fault_type;\r\nreason = dmar_get_fault_reason(fault_reason, &fault_type);\r\nif (fault_type == INTR_REMAP)\r\npr_err("INTR-REMAP: Request device [[%02x:%02x.%d] "\r\n"fault index %llx\n"\r\n"INTR-REMAP:[fault reason %02d] %s\n",\r\n(source_id >> 8), PCI_SLOT(source_id & 0xFF),\r\nPCI_FUNC(source_id & 0xFF), addr >> 48,\r\nfault_reason, reason);\r\nelse\r\npr_err("DMAR:[%s] Request device [%02x:%02x.%d] "\r\n"fault addr %llx \n"\r\n"DMAR:[fault reason %02d] %s\n",\r\n(type ? "DMA Read" : "DMA Write"),\r\n(source_id >> 8), PCI_SLOT(source_id & 0xFF),\r\nPCI_FUNC(source_id & 0xFF), addr, fault_reason, reason);\r\nreturn 0;\r\n}\r\nirqreturn_t dmar_fault(int irq, void *dev_id)\r\n{\r\nstruct intel_iommu *iommu = dev_id;\r\nint reg, fault_index;\r\nu32 fault_status;\r\nunsigned long flag;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\nfault_status = readl(iommu->reg + DMAR_FSTS_REG);\r\nif (fault_status)\r\npr_err("DRHD: handling fault status reg %x\n", fault_status);\r\nif (!(fault_status & DMA_FSTS_PPF))\r\ngoto unlock_exit;\r\nfault_index = dma_fsts_fault_record_index(fault_status);\r\nreg = cap_fault_reg_offset(iommu->cap);\r\nwhile (1) {\r\nu8 fault_reason;\r\nu16 source_id;\r\nu64 guest_addr;\r\nint type;\r\nu32 data;\r\ndata = readl(iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN + 12);\r\nif (!(data & DMA_FRCD_F))\r\nbreak;\r\nfault_reason = dma_frcd_fault_reason(data);\r\ntype = dma_frcd_type(data);\r\ndata = readl(iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN + 8);\r\nsource_id = dma_frcd_source_id(data);\r\nguest_addr = dmar_readq(iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN);\r\nguest_addr = dma_frcd_page_addr(guest_addr);\r\nwritel(DMA_FRCD_F, iommu->reg + reg +\r\nfault_index * PRIMARY_FAULT_REG_LEN + 12);\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\ndmar_fault_do_one(iommu, type, fault_reason,\r\nsource_id, guest_addr);\r\nfault_index++;\r\nif (fault_index >= cap_num_fault_regs(iommu->cap))\r\nfault_index = 0;\r\nraw_spin_lock_irqsave(&iommu->register_lock, flag);\r\n}\r\nwritel(DMA_FSTS_PFO | DMA_FSTS_PPF, iommu->reg + DMAR_FSTS_REG);\r\nunlock_exit:\r\nraw_spin_unlock_irqrestore(&iommu->register_lock, flag);\r\nreturn IRQ_HANDLED;\r\n}\r\nint dmar_set_interrupt(struct intel_iommu *iommu)\r\n{\r\nint irq, ret;\r\nif (iommu->irq)\r\nreturn 0;\r\nirq = create_irq();\r\nif (!irq) {\r\npr_err("IOMMU: no free vectors\n");\r\nreturn -EINVAL;\r\n}\r\nirq_set_handler_data(irq, iommu);\r\niommu->irq = irq;\r\nret = arch_setup_dmar_msi(irq);\r\nif (ret) {\r\nirq_set_handler_data(irq, NULL);\r\niommu->irq = 0;\r\ndestroy_irq(irq);\r\nreturn ret;\r\n}\r\nret = request_irq(irq, dmar_fault, IRQF_NO_THREAD, iommu->name, iommu);\r\nif (ret)\r\npr_err("IOMMU: can't request irq\n");\r\nreturn ret;\r\n}\r\nint __init enable_drhd_fault_handling(void)\r\n{\r\nstruct dmar_drhd_unit *drhd;\r\nfor_each_drhd_unit(drhd) {\r\nint ret;\r\nstruct intel_iommu *iommu = drhd->iommu;\r\nu32 fault_status;\r\nret = dmar_set_interrupt(iommu);\r\nif (ret) {\r\npr_err("DRHD %Lx: failed to enable fault, interrupt, ret %d\n",\r\n(unsigned long long)drhd->reg_base_addr, ret);\r\nreturn -1;\r\n}\r\ndmar_fault(iommu->irq, iommu);\r\nfault_status = readl(iommu->reg + DMAR_FSTS_REG);\r\nwritel(fault_status, iommu->reg + DMAR_FSTS_REG);\r\n}\r\nreturn 0;\r\n}\r\nint dmar_reenable_qi(struct intel_iommu *iommu)\r\n{\r\nif (!ecap_qis(iommu->ecap))\r\nreturn -ENOENT;\r\nif (!iommu->qi)\r\nreturn -ENOENT;\r\ndmar_disable_qi(iommu);\r\n__dmar_enable_qi(iommu);\r\nreturn 0;\r\n}\r\nint __init dmar_ir_support(void)\r\n{\r\nstruct acpi_table_dmar *dmar;\r\ndmar = (struct acpi_table_dmar *)dmar_tbl;\r\nif (!dmar)\r\nreturn 0;\r\nreturn dmar->flags & 0x1;\r\n}
