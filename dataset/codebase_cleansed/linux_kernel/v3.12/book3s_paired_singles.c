static inline void kvmppc_sync_qpr(struct kvm_vcpu *vcpu, int rt)\r\n{\r\nkvm_cvt_df(&vcpu->arch.fpr[rt], &vcpu->arch.qpr[rt]);\r\n}\r\nstatic void kvmppc_inject_pf(struct kvm_vcpu *vcpu, ulong eaddr, bool is_store)\r\n{\r\nu64 dsisr;\r\nstruct kvm_vcpu_arch_shared *shared = vcpu->arch.shared;\r\nshared->msr = kvmppc_set_field(shared->msr, 33, 36, 0);\r\nshared->msr = kvmppc_set_field(shared->msr, 42, 47, 0);\r\nshared->dar = eaddr;\r\ndsisr = kvmppc_set_field(0, 33, 33, 1);\r\nif (is_store)\r\nshared->dsisr = kvmppc_set_field(dsisr, 38, 38, 1);\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_DATA_STORAGE);\r\n}\r\nstatic int kvmppc_emulate_fpr_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nint rs, ulong addr, int ls_type)\r\n{\r\nint emulated = EMULATE_FAIL;\r\nint r;\r\nchar tmp[8];\r\nint len = sizeof(u32);\r\nif (ls_type == FPU_LS_DOUBLE)\r\nlen = sizeof(u64);\r\nr = kvmppc_ld(vcpu, &addr, len, tmp, true);\r\nvcpu->arch.paddr_accessed = addr;\r\nif (r < 0) {\r\nkvmppc_inject_pf(vcpu, addr, false);\r\ngoto done_load;\r\n} else if (r == EMULATE_DO_MMIO) {\r\nemulated = kvmppc_handle_load(run, vcpu, KVM_MMIO_REG_FPR | rs,\r\nlen, 1);\r\ngoto done_load;\r\n}\r\nemulated = EMULATE_DONE;\r\nswitch (ls_type) {\r\ncase FPU_LS_SINGLE:\r\nkvm_cvt_fd((u32*)tmp, &vcpu->arch.fpr[rs]);\r\nvcpu->arch.qpr[rs] = *((u32*)tmp);\r\nbreak;\r\ncase FPU_LS_DOUBLE:\r\nvcpu->arch.fpr[rs] = *((u64*)tmp);\r\nbreak;\r\n}\r\ndprintk(KERN_INFO "KVM: FPR_LD [0x%llx] at 0x%lx (%d)\n", *(u64*)tmp,\r\naddr, len);\r\ndone_load:\r\nreturn emulated;\r\n}\r\nstatic int kvmppc_emulate_fpr_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nint rs, ulong addr, int ls_type)\r\n{\r\nint emulated = EMULATE_FAIL;\r\nint r;\r\nchar tmp[8];\r\nu64 val;\r\nint len;\r\nswitch (ls_type) {\r\ncase FPU_LS_SINGLE:\r\nkvm_cvt_df(&vcpu->arch.fpr[rs], (u32*)tmp);\r\nval = *((u32*)tmp);\r\nlen = sizeof(u32);\r\nbreak;\r\ncase FPU_LS_SINGLE_LOW:\r\n*((u32*)tmp) = vcpu->arch.fpr[rs];\r\nval = vcpu->arch.fpr[rs] & 0xffffffff;\r\nlen = sizeof(u32);\r\nbreak;\r\ncase FPU_LS_DOUBLE:\r\n*((u64*)tmp) = vcpu->arch.fpr[rs];\r\nval = vcpu->arch.fpr[rs];\r\nlen = sizeof(u64);\r\nbreak;\r\ndefault:\r\nval = 0;\r\nlen = 0;\r\n}\r\nr = kvmppc_st(vcpu, &addr, len, tmp, true);\r\nvcpu->arch.paddr_accessed = addr;\r\nif (r < 0) {\r\nkvmppc_inject_pf(vcpu, addr, true);\r\n} else if (r == EMULATE_DO_MMIO) {\r\nemulated = kvmppc_handle_store(run, vcpu, val, len, 1);\r\n} else {\r\nemulated = EMULATE_DONE;\r\n}\r\ndprintk(KERN_INFO "KVM: FPR_ST [0x%llx] at 0x%lx (%d)\n",\r\nval, addr, len);\r\nreturn emulated;\r\n}\r\nstatic int kvmppc_emulate_psq_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nint rs, ulong addr, bool w, int i)\r\n{\r\nint emulated = EMULATE_FAIL;\r\nint r;\r\nfloat one = 1.0;\r\nu32 tmp[2];\r\nif (w) {\r\nr = kvmppc_ld(vcpu, &addr, sizeof(u32), tmp, true);\r\nmemcpy(&tmp[1], &one, sizeof(u32));\r\n} else {\r\nr = kvmppc_ld(vcpu, &addr, sizeof(u32) * 2, tmp, true);\r\n}\r\nvcpu->arch.paddr_accessed = addr;\r\nif (r < 0) {\r\nkvmppc_inject_pf(vcpu, addr, false);\r\ngoto done_load;\r\n} else if ((r == EMULATE_DO_MMIO) && w) {\r\nemulated = kvmppc_handle_load(run, vcpu, KVM_MMIO_REG_FPR | rs,\r\n4, 1);\r\nvcpu->arch.qpr[rs] = tmp[1];\r\ngoto done_load;\r\n} else if (r == EMULATE_DO_MMIO) {\r\nemulated = kvmppc_handle_load(run, vcpu, KVM_MMIO_REG_FQPR | rs,\r\n8, 1);\r\ngoto done_load;\r\n}\r\nemulated = EMULATE_DONE;\r\nkvm_cvt_fd(&tmp[0], &vcpu->arch.fpr[rs]);\r\nvcpu->arch.qpr[rs] = tmp[1];\r\ndprintk(KERN_INFO "KVM: PSQ_LD [0x%x, 0x%x] at 0x%lx (%d)\n", tmp[0],\r\ntmp[1], addr, w ? 4 : 8);\r\ndone_load:\r\nreturn emulated;\r\n}\r\nstatic int kvmppc_emulate_psq_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nint rs, ulong addr, bool w, int i)\r\n{\r\nint emulated = EMULATE_FAIL;\r\nint r;\r\nu32 tmp[2];\r\nint len = w ? sizeof(u32) : sizeof(u64);\r\nkvm_cvt_df(&vcpu->arch.fpr[rs], &tmp[0]);\r\ntmp[1] = vcpu->arch.qpr[rs];\r\nr = kvmppc_st(vcpu, &addr, len, tmp, true);\r\nvcpu->arch.paddr_accessed = addr;\r\nif (r < 0) {\r\nkvmppc_inject_pf(vcpu, addr, true);\r\n} else if ((r == EMULATE_DO_MMIO) && w) {\r\nemulated = kvmppc_handle_store(run, vcpu, tmp[0], 4, 1);\r\n} else if (r == EMULATE_DO_MMIO) {\r\nu64 val = ((u64)tmp[0] << 32) | tmp[1];\r\nemulated = kvmppc_handle_store(run, vcpu, val, 8, 1);\r\n} else {\r\nemulated = EMULATE_DONE;\r\n}\r\ndprintk(KERN_INFO "KVM: PSQ_ST [0x%x, 0x%x] at 0x%lx (%d)\n",\r\ntmp[0], tmp[1], addr, len);\r\nreturn emulated;\r\n}\r\nstatic inline u32 inst_get_field(u32 inst, int msb, int lsb)\r\n{\r\nreturn kvmppc_get_field(inst, msb + 32, lsb + 32);\r\n}\r\nstatic inline u32 inst_set_field(u32 inst, int msb, int lsb, int value)\r\n{\r\nreturn kvmppc_set_field(inst, msb + 32, lsb + 32, value);\r\n}\r\nbool kvmppc_inst_is_paired_single(struct kvm_vcpu *vcpu, u32 inst)\r\n{\r\nif (!(vcpu->arch.hflags & BOOK3S_HFLAG_PAIRED_SINGLE))\r\nreturn false;\r\nswitch (get_op(inst)) {\r\ncase OP_PSQ_L:\r\ncase OP_PSQ_LU:\r\ncase OP_PSQ_ST:\r\ncase OP_PSQ_STU:\r\ncase OP_LFS:\r\ncase OP_LFSU:\r\ncase OP_LFD:\r\ncase OP_LFDU:\r\ncase OP_STFS:\r\ncase OP_STFSU:\r\ncase OP_STFD:\r\ncase OP_STFDU:\r\nreturn true;\r\ncase 4:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_4X_PS_CMPU0:\r\ncase OP_4X_PSQ_LX:\r\ncase OP_4X_PS_CMPO0:\r\ncase OP_4X_PSQ_LUX:\r\ncase OP_4X_PS_NEG:\r\ncase OP_4X_PS_CMPU1:\r\ncase OP_4X_PS_MR:\r\ncase OP_4X_PS_CMPO1:\r\ncase OP_4X_PS_NABS:\r\ncase OP_4X_PS_ABS:\r\ncase OP_4X_PS_MERGE00:\r\ncase OP_4X_PS_MERGE01:\r\ncase OP_4X_PS_MERGE10:\r\ncase OP_4X_PS_MERGE11:\r\nreturn true;\r\n}\r\nswitch (inst_get_field(inst, 25, 30)) {\r\ncase OP_4XW_PSQ_STX:\r\ncase OP_4XW_PSQ_STUX:\r\nreturn true;\r\n}\r\nswitch (inst_get_field(inst, 26, 30)) {\r\ncase OP_4A_PS_SUM1:\r\ncase OP_4A_PS_SUM0:\r\ncase OP_4A_PS_MULS0:\r\ncase OP_4A_PS_MULS1:\r\ncase OP_4A_PS_MADDS0:\r\ncase OP_4A_PS_MADDS1:\r\ncase OP_4A_PS_DIV:\r\ncase OP_4A_PS_SUB:\r\ncase OP_4A_PS_ADD:\r\ncase OP_4A_PS_SEL:\r\ncase OP_4A_PS_RES:\r\ncase OP_4A_PS_MUL:\r\ncase OP_4A_PS_RSQRTE:\r\ncase OP_4A_PS_MSUB:\r\ncase OP_4A_PS_MADD:\r\ncase OP_4A_PS_NMSUB:\r\ncase OP_4A_PS_NMADD:\r\nreturn true;\r\n}\r\nbreak;\r\ncase 59:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_59_FADDS:\r\ncase OP_59_FSUBS:\r\ncase OP_59_FDIVS:\r\ncase OP_59_FRES:\r\ncase OP_59_FRSQRTES:\r\nreturn true;\r\n}\r\nswitch (inst_get_field(inst, 26, 30)) {\r\ncase OP_59_FMULS:\r\ncase OP_59_FMSUBS:\r\ncase OP_59_FMADDS:\r\ncase OP_59_FNMSUBS:\r\ncase OP_59_FNMADDS:\r\nreturn true;\r\n}\r\nbreak;\r\ncase 63:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_63_MTFSB0:\r\ncase OP_63_MTFSB1:\r\ncase OP_63_MTFSF:\r\ncase OP_63_MTFSFI:\r\ncase OP_63_MCRFS:\r\ncase OP_63_MFFS:\r\ncase OP_63_FCMPU:\r\ncase OP_63_FCMPO:\r\ncase OP_63_FNEG:\r\ncase OP_63_FMR:\r\ncase OP_63_FABS:\r\ncase OP_63_FRSP:\r\ncase OP_63_FDIV:\r\ncase OP_63_FADD:\r\ncase OP_63_FSUB:\r\ncase OP_63_FCTIW:\r\ncase OP_63_FCTIWZ:\r\ncase OP_63_FRSQRTE:\r\ncase OP_63_FCPSGN:\r\nreturn true;\r\n}\r\nswitch (inst_get_field(inst, 26, 30)) {\r\ncase OP_63_FMUL:\r\ncase OP_63_FSEL:\r\ncase OP_63_FMSUB:\r\ncase OP_63_FMADD:\r\ncase OP_63_FNMSUB:\r\ncase OP_63_FNMADD:\r\nreturn true;\r\n}\r\nbreak;\r\ncase 31:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_31_LFSX:\r\ncase OP_31_LFSUX:\r\ncase OP_31_LFDX:\r\ncase OP_31_LFDUX:\r\ncase OP_31_STFSX:\r\ncase OP_31_STFSUX:\r\ncase OP_31_STFX:\r\ncase OP_31_STFUX:\r\ncase OP_31_STFIWX:\r\nreturn true;\r\n}\r\nbreak;\r\n}\r\nreturn false;\r\n}\r\nstatic int get_d_signext(u32 inst)\r\n{\r\nint d = inst & 0x8ff;\r\nif (d & 0x800)\r\nreturn -(d & 0x7ff);\r\nreturn (d & 0x7ff);\r\n}\r\nstatic int kvmppc_ps_three_in(struct kvm_vcpu *vcpu, bool rc,\r\nint reg_out, int reg_in1, int reg_in2,\r\nint reg_in3, int scalar,\r\nvoid (*func)(u64 *fpscr,\r\nu32 *dst, u32 *src1,\r\nu32 *src2, u32 *src3))\r\n{\r\nu32 *qpr = vcpu->arch.qpr;\r\nu64 *fpr = vcpu->arch.fpr;\r\nu32 ps0_out;\r\nu32 ps0_in1, ps0_in2, ps0_in3;\r\nu32 ps1_in1, ps1_in2, ps1_in3;\r\nWARN_ON(rc);\r\nkvm_cvt_df(&fpr[reg_in1], &ps0_in1);\r\nkvm_cvt_df(&fpr[reg_in2], &ps0_in2);\r\nkvm_cvt_df(&fpr[reg_in3], &ps0_in3);\r\nif (scalar & SCALAR_LOW)\r\nps0_in2 = qpr[reg_in2];\r\nfunc(&vcpu->arch.fpscr, &ps0_out, &ps0_in1, &ps0_in2, &ps0_in3);\r\ndprintk(KERN_INFO "PS3 ps0 -> f(0x%x, 0x%x, 0x%x) = 0x%x\n",\r\nps0_in1, ps0_in2, ps0_in3, ps0_out);\r\nif (!(scalar & SCALAR_NO_PS0))\r\nkvm_cvt_fd(&ps0_out, &fpr[reg_out]);\r\nps1_in1 = qpr[reg_in1];\r\nps1_in2 = qpr[reg_in2];\r\nps1_in3 = qpr[reg_in3];\r\nif (scalar & SCALAR_HIGH)\r\nps1_in2 = ps0_in2;\r\nif (!(scalar & SCALAR_NO_PS1))\r\nfunc(&vcpu->arch.fpscr, &qpr[reg_out], &ps1_in1, &ps1_in2, &ps1_in3);\r\ndprintk(KERN_INFO "PS3 ps1 -> f(0x%x, 0x%x, 0x%x) = 0x%x\n",\r\nps1_in1, ps1_in2, ps1_in3, qpr[reg_out]);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_ps_two_in(struct kvm_vcpu *vcpu, bool rc,\r\nint reg_out, int reg_in1, int reg_in2,\r\nint scalar,\r\nvoid (*func)(u64 *fpscr,\r\nu32 *dst, u32 *src1,\r\nu32 *src2))\r\n{\r\nu32 *qpr = vcpu->arch.qpr;\r\nu64 *fpr = vcpu->arch.fpr;\r\nu32 ps0_out;\r\nu32 ps0_in1, ps0_in2;\r\nu32 ps1_out;\r\nu32 ps1_in1, ps1_in2;\r\nWARN_ON(rc);\r\nkvm_cvt_df(&fpr[reg_in1], &ps0_in1);\r\nif (scalar & SCALAR_LOW)\r\nps0_in2 = qpr[reg_in2];\r\nelse\r\nkvm_cvt_df(&fpr[reg_in2], &ps0_in2);\r\nfunc(&vcpu->arch.fpscr, &ps0_out, &ps0_in1, &ps0_in2);\r\nif (!(scalar & SCALAR_NO_PS0)) {\r\ndprintk(KERN_INFO "PS2 ps0 -> f(0x%x, 0x%x) = 0x%x\n",\r\nps0_in1, ps0_in2, ps0_out);\r\nkvm_cvt_fd(&ps0_out, &fpr[reg_out]);\r\n}\r\nps1_in1 = qpr[reg_in1];\r\nps1_in2 = qpr[reg_in2];\r\nif (scalar & SCALAR_HIGH)\r\nps1_in2 = ps0_in2;\r\nfunc(&vcpu->arch.fpscr, &ps1_out, &ps1_in1, &ps1_in2);\r\nif (!(scalar & SCALAR_NO_PS1)) {\r\nqpr[reg_out] = ps1_out;\r\ndprintk(KERN_INFO "PS2 ps1 -> f(0x%x, 0x%x) = 0x%x\n",\r\nps1_in1, ps1_in2, qpr[reg_out]);\r\n}\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_ps_one_in(struct kvm_vcpu *vcpu, bool rc,\r\nint reg_out, int reg_in,\r\nvoid (*func)(u64 *t,\r\nu32 *dst, u32 *src1))\r\n{\r\nu32 *qpr = vcpu->arch.qpr;\r\nu64 *fpr = vcpu->arch.fpr;\r\nu32 ps0_out, ps0_in;\r\nu32 ps1_in;\r\nWARN_ON(rc);\r\nkvm_cvt_df(&fpr[reg_in], &ps0_in);\r\nfunc(&vcpu->arch.fpscr, &ps0_out, &ps0_in);\r\ndprintk(KERN_INFO "PS1 ps0 -> f(0x%x) = 0x%x\n",\r\nps0_in, ps0_out);\r\nkvm_cvt_fd(&ps0_out, &fpr[reg_out]);\r\nps1_in = qpr[reg_in];\r\nfunc(&vcpu->arch.fpscr, &qpr[reg_out], &ps1_in);\r\ndprintk(KERN_INFO "PS1 ps1 -> f(0x%x) = 0x%x\n",\r\nps1_in, qpr[reg_out]);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_emulate_paired_single(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nu32 inst = kvmppc_get_last_inst(vcpu);\r\nenum emulation_result emulated = EMULATE_DONE;\r\nint ax_rd = inst_get_field(inst, 6, 10);\r\nint ax_ra = inst_get_field(inst, 11, 15);\r\nint ax_rb = inst_get_field(inst, 16, 20);\r\nint ax_rc = inst_get_field(inst, 21, 25);\r\nshort full_d = inst_get_field(inst, 16, 31);\r\nu64 *fpr_d = &vcpu->arch.fpr[ax_rd];\r\nu64 *fpr_a = &vcpu->arch.fpr[ax_ra];\r\nu64 *fpr_b = &vcpu->arch.fpr[ax_rb];\r\nu64 *fpr_c = &vcpu->arch.fpr[ax_rc];\r\nbool rcomp = (inst & 1) ? true : false;\r\nu32 cr = kvmppc_get_cr(vcpu);\r\n#ifdef DEBUG\r\nint i;\r\n#endif\r\nif (!kvmppc_inst_is_paired_single(vcpu, inst))\r\nreturn EMULATE_FAIL;\r\nif (!(vcpu->arch.shared->msr & MSR_FP)) {\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_FP_UNAVAIL);\r\nreturn EMULATE_AGAIN;\r\n}\r\nkvmppc_giveup_ext(vcpu, MSR_FP);\r\npreempt_disable();\r\nenable_kernel_fp();\r\n#ifdef DEBUG\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.fpr); i++) {\r\nu32 f;\r\nkvm_cvt_df(&vcpu->arch.fpr[i], &f);\r\ndprintk(KERN_INFO "FPR[%d] = 0x%x / 0x%llx QPR[%d] = 0x%x\n",\r\ni, f, vcpu->arch.fpr[i], i, vcpu->arch.qpr[i]);\r\n}\r\n#endif\r\nswitch (get_op(inst)) {\r\ncase OP_PSQ_L:\r\n{\r\nulong addr = ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0;\r\nbool w = inst_get_field(inst, 16, 16) ? true : false;\r\nint i = inst_get_field(inst, 17, 19);\r\naddr += get_d_signext(inst);\r\nemulated = kvmppc_emulate_psq_load(run, vcpu, ax_rd, addr, w, i);\r\nbreak;\r\n}\r\ncase OP_PSQ_LU:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra);\r\nbool w = inst_get_field(inst, 16, 16) ? true : false;\r\nint i = inst_get_field(inst, 17, 19);\r\naddr += get_d_signext(inst);\r\nemulated = kvmppc_emulate_psq_load(run, vcpu, ax_rd, addr, w, i);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_PSQ_ST:\r\n{\r\nulong addr = ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0;\r\nbool w = inst_get_field(inst, 16, 16) ? true : false;\r\nint i = inst_get_field(inst, 17, 19);\r\naddr += get_d_signext(inst);\r\nemulated = kvmppc_emulate_psq_store(run, vcpu, ax_rd, addr, w, i);\r\nbreak;\r\n}\r\ncase OP_PSQ_STU:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra);\r\nbool w = inst_get_field(inst, 16, 16) ? true : false;\r\nint i = inst_get_field(inst, 17, 19);\r\naddr += get_d_signext(inst);\r\nemulated = kvmppc_emulate_psq_store(run, vcpu, ax_rd, addr, w, i);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase 4:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_4X_PS_CMPU0:\r\nemulated = EMULATE_FAIL;\r\nbreak;\r\ncase OP_4X_PSQ_LX:\r\n{\r\nulong addr = ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0;\r\nbool w = inst_get_field(inst, 21, 21) ? true : false;\r\nint i = inst_get_field(inst, 22, 24);\r\naddr += kvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_psq_load(run, vcpu, ax_rd, addr, w, i);\r\nbreak;\r\n}\r\ncase OP_4X_PS_CMPO0:\r\nemulated = EMULATE_FAIL;\r\nbreak;\r\ncase OP_4X_PSQ_LUX:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra);\r\nbool w = inst_get_field(inst, 21, 21) ? true : false;\r\nint i = inst_get_field(inst, 22, 24);\r\naddr += kvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_psq_load(run, vcpu, ax_rd, addr, w, i);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_4X_PS_NEG:\r\nvcpu->arch.fpr[ax_rd] = vcpu->arch.fpr[ax_rb];\r\nvcpu->arch.fpr[ax_rd] ^= 0x8000000000000000ULL;\r\nvcpu->arch.qpr[ax_rd] = vcpu->arch.qpr[ax_rb];\r\nvcpu->arch.qpr[ax_rd] ^= 0x80000000;\r\nbreak;\r\ncase OP_4X_PS_CMPU1:\r\nemulated = EMULATE_FAIL;\r\nbreak;\r\ncase OP_4X_PS_MR:\r\nWARN_ON(rcomp);\r\nvcpu->arch.fpr[ax_rd] = vcpu->arch.fpr[ax_rb];\r\nvcpu->arch.qpr[ax_rd] = vcpu->arch.qpr[ax_rb];\r\nbreak;\r\ncase OP_4X_PS_CMPO1:\r\nemulated = EMULATE_FAIL;\r\nbreak;\r\ncase OP_4X_PS_NABS:\r\nWARN_ON(rcomp);\r\nvcpu->arch.fpr[ax_rd] = vcpu->arch.fpr[ax_rb];\r\nvcpu->arch.fpr[ax_rd] |= 0x8000000000000000ULL;\r\nvcpu->arch.qpr[ax_rd] = vcpu->arch.qpr[ax_rb];\r\nvcpu->arch.qpr[ax_rd] |= 0x80000000;\r\nbreak;\r\ncase OP_4X_PS_ABS:\r\nWARN_ON(rcomp);\r\nvcpu->arch.fpr[ax_rd] = vcpu->arch.fpr[ax_rb];\r\nvcpu->arch.fpr[ax_rd] &= ~0x8000000000000000ULL;\r\nvcpu->arch.qpr[ax_rd] = vcpu->arch.qpr[ax_rb];\r\nvcpu->arch.qpr[ax_rd] &= ~0x80000000;\r\nbreak;\r\ncase OP_4X_PS_MERGE00:\r\nWARN_ON(rcomp);\r\nvcpu->arch.fpr[ax_rd] = vcpu->arch.fpr[ax_ra];\r\nkvm_cvt_df(&vcpu->arch.fpr[ax_rb],\r\n&vcpu->arch.qpr[ax_rd]);\r\nbreak;\r\ncase OP_4X_PS_MERGE01:\r\nWARN_ON(rcomp);\r\nvcpu->arch.fpr[ax_rd] = vcpu->arch.fpr[ax_ra];\r\nvcpu->arch.qpr[ax_rd] = vcpu->arch.qpr[ax_rb];\r\nbreak;\r\ncase OP_4X_PS_MERGE10:\r\nWARN_ON(rcomp);\r\nkvm_cvt_fd(&vcpu->arch.qpr[ax_ra],\r\n&vcpu->arch.fpr[ax_rd]);\r\nkvm_cvt_df(&vcpu->arch.fpr[ax_rb],\r\n&vcpu->arch.qpr[ax_rd]);\r\nbreak;\r\ncase OP_4X_PS_MERGE11:\r\nWARN_ON(rcomp);\r\nkvm_cvt_fd(&vcpu->arch.qpr[ax_ra],\r\n&vcpu->arch.fpr[ax_rd]);\r\nvcpu->arch.qpr[ax_rd] = vcpu->arch.qpr[ax_rb];\r\nbreak;\r\n}\r\nswitch (inst_get_field(inst, 25, 30)) {\r\ncase OP_4XW_PSQ_STX:\r\n{\r\nulong addr = ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0;\r\nbool w = inst_get_field(inst, 21, 21) ? true : false;\r\nint i = inst_get_field(inst, 22, 24);\r\naddr += kvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_psq_store(run, vcpu, ax_rd, addr, w, i);\r\nbreak;\r\n}\r\ncase OP_4XW_PSQ_STUX:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra);\r\nbool w = inst_get_field(inst, 21, 21) ? true : false;\r\nint i = inst_get_field(inst, 22, 24);\r\naddr += kvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_psq_store(run, vcpu, ax_rd, addr, w, i);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\n}\r\nswitch (inst_get_field(inst, 26, 30)) {\r\ncase OP_4A_PS_SUM1:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_rb, ax_ra, SCALAR_NO_PS0 | SCALAR_HIGH, fps_fadds);\r\nvcpu->arch.fpr[ax_rd] = vcpu->arch.fpr[ax_rc];\r\nbreak;\r\ncase OP_4A_PS_SUM0:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rb, SCALAR_NO_PS1 | SCALAR_LOW, fps_fadds);\r\nvcpu->arch.qpr[ax_rd] = vcpu->arch.qpr[ax_rc];\r\nbreak;\r\ncase OP_4A_PS_MULS0:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, SCALAR_HIGH, fps_fmuls);\r\nbreak;\r\ncase OP_4A_PS_MULS1:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, SCALAR_LOW, fps_fmuls);\r\nbreak;\r\ncase OP_4A_PS_MADDS0:\r\nemulated = kvmppc_ps_three_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, ax_rb, SCALAR_HIGH, fps_fmadds);\r\nbreak;\r\ncase OP_4A_PS_MADDS1:\r\nemulated = kvmppc_ps_three_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, ax_rb, SCALAR_LOW, fps_fmadds);\r\nbreak;\r\ncase OP_4A_PS_DIV:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rb, SCALAR_NONE, fps_fdivs);\r\nbreak;\r\ncase OP_4A_PS_SUB:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rb, SCALAR_NONE, fps_fsubs);\r\nbreak;\r\ncase OP_4A_PS_ADD:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rb, SCALAR_NONE, fps_fadds);\r\nbreak;\r\ncase OP_4A_PS_SEL:\r\nemulated = kvmppc_ps_three_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, ax_rb, SCALAR_NONE, fps_fsel);\r\nbreak;\r\ncase OP_4A_PS_RES:\r\nemulated = kvmppc_ps_one_in(vcpu, rcomp, ax_rd,\r\nax_rb, fps_fres);\r\nbreak;\r\ncase OP_4A_PS_MUL:\r\nemulated = kvmppc_ps_two_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, SCALAR_NONE, fps_fmuls);\r\nbreak;\r\ncase OP_4A_PS_RSQRTE:\r\nemulated = kvmppc_ps_one_in(vcpu, rcomp, ax_rd,\r\nax_rb, fps_frsqrte);\r\nbreak;\r\ncase OP_4A_PS_MSUB:\r\nemulated = kvmppc_ps_three_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, ax_rb, SCALAR_NONE, fps_fmsubs);\r\nbreak;\r\ncase OP_4A_PS_MADD:\r\nemulated = kvmppc_ps_three_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, ax_rb, SCALAR_NONE, fps_fmadds);\r\nbreak;\r\ncase OP_4A_PS_NMSUB:\r\nemulated = kvmppc_ps_three_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, ax_rb, SCALAR_NONE, fps_fnmsubs);\r\nbreak;\r\ncase OP_4A_PS_NMADD:\r\nemulated = kvmppc_ps_three_in(vcpu, rcomp, ax_rd,\r\nax_ra, ax_rc, ax_rb, SCALAR_NONE, fps_fnmadds);\r\nbreak;\r\n}\r\nbreak;\r\ncase OP_LFS:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) + full_d;\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd, addr,\r\nFPU_LS_SINGLE);\r\nbreak;\r\n}\r\ncase OP_LFSU:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) + full_d;\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd, addr,\r\nFPU_LS_SINGLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_LFD:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) + full_d;\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd, addr,\r\nFPU_LS_DOUBLE);\r\nbreak;\r\n}\r\ncase OP_LFDU:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) + full_d;\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd, addr,\r\nFPU_LS_DOUBLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_STFS:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) + full_d;\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd, addr,\r\nFPU_LS_SINGLE);\r\nbreak;\r\n}\r\ncase OP_STFSU:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) + full_d;\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd, addr,\r\nFPU_LS_SINGLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_STFD:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) + full_d;\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd, addr,\r\nFPU_LS_DOUBLE);\r\nbreak;\r\n}\r\ncase OP_STFDU:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) + full_d;\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd, addr,\r\nFPU_LS_DOUBLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase 31:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_31_LFSX:\r\n{\r\nulong addr = ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0;\r\naddr += kvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd,\r\naddr, FPU_LS_SINGLE);\r\nbreak;\r\n}\r\ncase OP_31_LFSUX:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd,\r\naddr, FPU_LS_SINGLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_31_LFDX:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd,\r\naddr, FPU_LS_DOUBLE);\r\nbreak;\r\n}\r\ncase OP_31_LFDUX:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_load(run, vcpu, ax_rd,\r\naddr, FPU_LS_DOUBLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_31_STFSX:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd,\r\naddr, FPU_LS_SINGLE);\r\nbreak;\r\n}\r\ncase OP_31_STFSUX:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd,\r\naddr, FPU_LS_SINGLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_31_STFX:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd,\r\naddr, FPU_LS_DOUBLE);\r\nbreak;\r\n}\r\ncase OP_31_STFUX:\r\n{\r\nulong addr = kvmppc_get_gpr(vcpu, ax_ra) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd,\r\naddr, FPU_LS_DOUBLE);\r\nif (emulated == EMULATE_DONE)\r\nkvmppc_set_gpr(vcpu, ax_ra, addr);\r\nbreak;\r\n}\r\ncase OP_31_STFIWX:\r\n{\r\nulong addr = (ax_ra ? kvmppc_get_gpr(vcpu, ax_ra) : 0) +\r\nkvmppc_get_gpr(vcpu, ax_rb);\r\nemulated = kvmppc_emulate_fpr_store(run, vcpu, ax_rd,\r\naddr,\r\nFPU_LS_SINGLE_LOW);\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nbreak;\r\ncase 59:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_59_FADDS:\r\nfpd_fadds(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FSUBS:\r\nfpd_fsubs(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FDIVS:\r\nfpd_fdivs(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FRES:\r\nfpd_fres(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FRSQRTES:\r\nfpd_frsqrtes(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\n}\r\nswitch (inst_get_field(inst, 26, 30)) {\r\ncase OP_59_FMULS:\r\nfpd_fmuls(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FMSUBS:\r\nfpd_fmsubs(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FMADDS:\r\nfpd_fmadds(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FNMSUBS:\r\nfpd_fnmsubs(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_59_FNMADDS:\r\nfpd_fnmadds(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\n}\r\nbreak;\r\ncase 63:\r\nswitch (inst_get_field(inst, 21, 30)) {\r\ncase OP_63_MTFSB0:\r\ncase OP_63_MTFSB1:\r\ncase OP_63_MCRFS:\r\ncase OP_63_MTFSFI:\r\nbreak;\r\ncase OP_63_MFFS:\r\n*fpr_d = vcpu->arch.fpscr;\r\nbreak;\r\ncase OP_63_MTFSF:\r\nvcpu->arch.fpscr = *fpr_b;\r\nbreak;\r\ncase OP_63_FCMPU:\r\n{\r\nu32 tmp_cr;\r\nu32 cr0_mask = 0xf0000000;\r\nu32 cr_shift = inst_get_field(inst, 6, 8) * 4;\r\nfpd_fcmpu(&vcpu->arch.fpscr, &tmp_cr, fpr_a, fpr_b);\r\ncr &= ~(cr0_mask >> cr_shift);\r\ncr |= (cr & cr0_mask) >> cr_shift;\r\nbreak;\r\n}\r\ncase OP_63_FCMPO:\r\n{\r\nu32 tmp_cr;\r\nu32 cr0_mask = 0xf0000000;\r\nu32 cr_shift = inst_get_field(inst, 6, 8) * 4;\r\nfpd_fcmpo(&vcpu->arch.fpscr, &tmp_cr, fpr_a, fpr_b);\r\ncr &= ~(cr0_mask >> cr_shift);\r\ncr |= (cr & cr0_mask) >> cr_shift;\r\nbreak;\r\n}\r\ncase OP_63_FNEG:\r\nfpd_fneg(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nbreak;\r\ncase OP_63_FMR:\r\n*fpr_d = *fpr_b;\r\nbreak;\r\ncase OP_63_FABS:\r\nfpd_fabs(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nbreak;\r\ncase OP_63_FCPSGN:\r\nfpd_fcpsgn(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_b);\r\nbreak;\r\ncase OP_63_FDIV:\r\nfpd_fdiv(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_b);\r\nbreak;\r\ncase OP_63_FADD:\r\nfpd_fadd(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_b);\r\nbreak;\r\ncase OP_63_FSUB:\r\nfpd_fsub(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_b);\r\nbreak;\r\ncase OP_63_FCTIW:\r\nfpd_fctiw(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nbreak;\r\ncase OP_63_FCTIWZ:\r\nfpd_fctiwz(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nbreak;\r\ncase OP_63_FRSP:\r\nfpd_frsp(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nkvmppc_sync_qpr(vcpu, ax_rd);\r\nbreak;\r\ncase OP_63_FRSQRTE:\r\n{\r\ndouble one = 1.0f;\r\nfpd_fsqrt(&vcpu->arch.fpscr, &cr, fpr_d, fpr_b);\r\nfpd_fdiv(&vcpu->arch.fpscr, &cr, fpr_d, (u64*)&one, fpr_d);\r\nbreak;\r\n}\r\n}\r\nswitch (inst_get_field(inst, 26, 30)) {\r\ncase OP_63_FMUL:\r\nfpd_fmul(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c);\r\nbreak;\r\ncase OP_63_FSEL:\r\nfpd_fsel(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nbreak;\r\ncase OP_63_FMSUB:\r\nfpd_fmsub(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nbreak;\r\ncase OP_63_FMADD:\r\nfpd_fmadd(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nbreak;\r\ncase OP_63_FNMSUB:\r\nfpd_fnmsub(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nbreak;\r\ncase OP_63_FNMADD:\r\nfpd_fnmadd(&vcpu->arch.fpscr, &cr, fpr_d, fpr_a, fpr_c, fpr_b);\r\nbreak;\r\n}\r\nbreak;\r\n}\r\n#ifdef DEBUG\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.fpr); i++) {\r\nu32 f;\r\nkvm_cvt_df(&vcpu->arch.fpr[i], &f);\r\ndprintk(KERN_INFO "FPR[%d] = 0x%x\n", i, f);\r\n}\r\n#endif\r\nif (rcomp)\r\nkvmppc_set_cr(vcpu, cr);\r\npreempt_enable();\r\nreturn emulated;\r\n}
