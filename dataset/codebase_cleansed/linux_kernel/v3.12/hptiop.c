static int iop_wait_ready_itl(struct hptiop_hba *hba, u32 millisec)\r\n{\r\nu32 req = 0;\r\nint i;\r\nfor (i = 0; i < millisec; i++) {\r\nreq = readl(&hba->u.itl.iop->inbound_queue);\r\nif (req != IOPMU_QUEUE_EMPTY)\r\nbreak;\r\nmsleep(1);\r\n}\r\nif (req != IOPMU_QUEUE_EMPTY) {\r\nwritel(req, &hba->u.itl.iop->outbound_queue);\r\nreadl(&hba->u.itl.iop->outbound_intstatus);\r\nreturn 0;\r\n}\r\nreturn -1;\r\n}\r\nstatic int iop_wait_ready_mv(struct hptiop_hba *hba, u32 millisec)\r\n{\r\nreturn iop_send_sync_msg(hba, IOPMU_INBOUND_MSG0_NOP, millisec);\r\n}\r\nstatic int iop_wait_ready_mvfrey(struct hptiop_hba *hba, u32 millisec)\r\n{\r\nreturn iop_send_sync_msg(hba, IOPMU_INBOUND_MSG0_NOP, millisec);\r\n}\r\nstatic void hptiop_request_callback_itl(struct hptiop_hba *hba, u32 tag)\r\n{\r\nif (tag & IOPMU_QUEUE_ADDR_HOST_BIT)\r\nhptiop_host_request_callback_itl(hba,\r\ntag & ~IOPMU_QUEUE_ADDR_HOST_BIT);\r\nelse\r\nhptiop_iop_request_callback_itl(hba, tag);\r\n}\r\nstatic void hptiop_drain_outbound_queue_itl(struct hptiop_hba *hba)\r\n{\r\nu32 req;\r\nwhile ((req = readl(&hba->u.itl.iop->outbound_queue)) !=\r\nIOPMU_QUEUE_EMPTY) {\r\nif (req & IOPMU_QUEUE_MASK_HOST_BITS)\r\nhptiop_request_callback_itl(hba, req);\r\nelse {\r\nstruct hpt_iop_request_header __iomem * p;\r\np = (struct hpt_iop_request_header __iomem *)\r\n((char __iomem *)hba->u.itl.iop + req);\r\nif (readl(&p->flags) & IOP_REQUEST_FLAG_SYNC_REQUEST) {\r\nif (readl(&p->context))\r\nhptiop_request_callback_itl(hba, req);\r\nelse\r\nwritel(1, &p->context);\r\n}\r\nelse\r\nhptiop_request_callback_itl(hba, req);\r\n}\r\n}\r\n}\r\nstatic int iop_intr_itl(struct hptiop_hba *hba)\r\n{\r\nstruct hpt_iopmu_itl __iomem *iop = hba->u.itl.iop;\r\nvoid __iomem *plx = hba->u.itl.plx;\r\nu32 status;\r\nint ret = 0;\r\nif (plx && readl(plx + 0x11C5C) & 0xf)\r\nwritel(1, plx + 0x11C60);\r\nstatus = readl(&iop->outbound_intstatus);\r\nif (status & IOPMU_OUTBOUND_INT_MSG0) {\r\nu32 msg = readl(&iop->outbound_msgaddr0);\r\ndprintk("received outbound msg %x\n", msg);\r\nwritel(IOPMU_OUTBOUND_INT_MSG0, &iop->outbound_intstatus);\r\nhptiop_message_callback(hba, msg);\r\nret = 1;\r\n}\r\nif (status & IOPMU_OUTBOUND_INT_POSTQUEUE) {\r\nhptiop_drain_outbound_queue_itl(hba);\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic u64 mv_outbound_read(struct hpt_iopmu_mv __iomem *mu)\r\n{\r\nu32 outbound_tail = readl(&mu->outbound_tail);\r\nu32 outbound_head = readl(&mu->outbound_head);\r\nif (outbound_tail != outbound_head) {\r\nu64 p;\r\nmemcpy_fromio(&p, &mu->outbound_q[mu->outbound_tail], 8);\r\noutbound_tail++;\r\nif (outbound_tail == MVIOP_QUEUE_LEN)\r\noutbound_tail = 0;\r\nwritel(outbound_tail, &mu->outbound_tail);\r\nreturn p;\r\n} else\r\nreturn 0;\r\n}\r\nstatic void mv_inbound_write(u64 p, struct hptiop_hba *hba)\r\n{\r\nu32 inbound_head = readl(&hba->u.mv.mu->inbound_head);\r\nu32 head = inbound_head + 1;\r\nif (head == MVIOP_QUEUE_LEN)\r\nhead = 0;\r\nmemcpy_toio(&hba->u.mv.mu->inbound_q[inbound_head], &p, 8);\r\nwritel(head, &hba->u.mv.mu->inbound_head);\r\nwritel(MVIOP_MU_INBOUND_INT_POSTQUEUE,\r\n&hba->u.mv.regs->inbound_doorbell);\r\n}\r\nstatic void hptiop_request_callback_mv(struct hptiop_hba *hba, u64 tag)\r\n{\r\nu32 req_type = (tag >> 5) & 0x7;\r\nstruct hpt_iop_request_scsi_command *req;\r\ndprintk("hptiop_request_callback_mv: tag=%llx\n", tag);\r\nBUG_ON((tag & MVIOP_MU_QUEUE_REQUEST_RETURN_CONTEXT) == 0);\r\nswitch (req_type) {\r\ncase IOP_REQUEST_TYPE_GET_CONFIG:\r\ncase IOP_REQUEST_TYPE_SET_CONFIG:\r\nhba->msg_done = 1;\r\nbreak;\r\ncase IOP_REQUEST_TYPE_SCSI_COMMAND:\r\nreq = hba->reqs[tag >> 8].req_virt;\r\nif (likely(tag & MVIOP_MU_QUEUE_REQUEST_RESULT_BIT))\r\nreq->header.result = cpu_to_le32(IOP_RESULT_SUCCESS);\r\nhptiop_finish_scsi_req(hba, tag>>8, req);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic int iop_intr_mv(struct hptiop_hba *hba)\r\n{\r\nu32 status;\r\nint ret = 0;\r\nstatus = readl(&hba->u.mv.regs->outbound_doorbell);\r\nwritel(~status, &hba->u.mv.regs->outbound_doorbell);\r\nif (status & MVIOP_MU_OUTBOUND_INT_MSG) {\r\nu32 msg;\r\nmsg = readl(&hba->u.mv.mu->outbound_msg);\r\ndprintk("received outbound msg %x\n", msg);\r\nhptiop_message_callback(hba, msg);\r\nret = 1;\r\n}\r\nif (status & MVIOP_MU_OUTBOUND_INT_POSTQUEUE) {\r\nu64 tag;\r\nwhile ((tag = mv_outbound_read(hba->u.mv.mu)))\r\nhptiop_request_callback_mv(hba, tag);\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic void hptiop_request_callback_mvfrey(struct hptiop_hba *hba, u32 _tag)\r\n{\r\nu32 req_type = _tag & 0xf;\r\nstruct hpt_iop_request_scsi_command *req;\r\nswitch (req_type) {\r\ncase IOP_REQUEST_TYPE_GET_CONFIG:\r\ncase IOP_REQUEST_TYPE_SET_CONFIG:\r\nhba->msg_done = 1;\r\nbreak;\r\ncase IOP_REQUEST_TYPE_SCSI_COMMAND:\r\nreq = hba->reqs[(_tag >> 4) & 0xff].req_virt;\r\nif (likely(_tag & IOPMU_QUEUE_REQUEST_RESULT_BIT))\r\nreq->header.result = IOP_RESULT_SUCCESS;\r\nhptiop_finish_scsi_req(hba, (_tag >> 4) & 0xff, req);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic int iop_intr_mvfrey(struct hptiop_hba *hba)\r\n{\r\nu32 _tag, status, cptr, cur_rptr;\r\nint ret = 0;\r\nif (hba->initialized)\r\nwritel(0, &(hba->u.mvfrey.mu->pcie_f0_int_enable));\r\nstatus = readl(&(hba->u.mvfrey.mu->f0_doorbell));\r\nif (status) {\r\nwritel(status, &(hba->u.mvfrey.mu->f0_doorbell));\r\nif (status & CPU_TO_F0_DRBL_MSG_BIT) {\r\nu32 msg = readl(&(hba->u.mvfrey.mu->cpu_to_f0_msg_a));\r\ndprintk("received outbound msg %x\n", msg);\r\nhptiop_message_callback(hba, msg);\r\n}\r\nret = 1;\r\n}\r\nstatus = readl(&(hba->u.mvfrey.mu->isr_cause));\r\nif (status) {\r\nwritel(status, &(hba->u.mvfrey.mu->isr_cause));\r\ndo {\r\ncptr = *hba->u.mvfrey.outlist_cptr & 0xff;\r\ncur_rptr = hba->u.mvfrey.outlist_rptr;\r\nwhile (cur_rptr != cptr) {\r\ncur_rptr++;\r\nif (cur_rptr == hba->u.mvfrey.list_count)\r\ncur_rptr = 0;\r\n_tag = hba->u.mvfrey.outlist[cur_rptr].val;\r\nBUG_ON(!(_tag & IOPMU_QUEUE_MASK_HOST_BITS));\r\nhptiop_request_callback_mvfrey(hba, _tag);\r\nret = 1;\r\n}\r\nhba->u.mvfrey.outlist_rptr = cur_rptr;\r\n} while (cptr != (*hba->u.mvfrey.outlist_cptr & 0xff));\r\n}\r\nif (hba->initialized)\r\nwritel(0x1010, &(hba->u.mvfrey.mu->pcie_f0_int_enable));\r\nreturn ret;\r\n}\r\nstatic int iop_send_sync_request_itl(struct hptiop_hba *hba,\r\nvoid __iomem *_req, u32 millisec)\r\n{\r\nstruct hpt_iop_request_header __iomem *req = _req;\r\nu32 i;\r\nwritel(readl(&req->flags) | IOP_REQUEST_FLAG_SYNC_REQUEST, &req->flags);\r\nwritel(0, &req->context);\r\nwritel((unsigned long)req - (unsigned long)hba->u.itl.iop,\r\n&hba->u.itl.iop->inbound_queue);\r\nreadl(&hba->u.itl.iop->outbound_intstatus);\r\nfor (i = 0; i < millisec; i++) {\r\niop_intr_itl(hba);\r\nif (readl(&req->context))\r\nreturn 0;\r\nmsleep(1);\r\n}\r\nreturn -1;\r\n}\r\nstatic int iop_send_sync_request_mv(struct hptiop_hba *hba,\r\nu32 size_bits, u32 millisec)\r\n{\r\nstruct hpt_iop_request_header *reqhdr = hba->u.mv.internal_req;\r\nu32 i;\r\nhba->msg_done = 0;\r\nreqhdr->flags |= cpu_to_le32(IOP_REQUEST_FLAG_SYNC_REQUEST);\r\nmv_inbound_write(hba->u.mv.internal_req_phy |\r\nMVIOP_MU_QUEUE_ADDR_HOST_BIT | size_bits, hba);\r\nfor (i = 0; i < millisec; i++) {\r\niop_intr_mv(hba);\r\nif (hba->msg_done)\r\nreturn 0;\r\nmsleep(1);\r\n}\r\nreturn -1;\r\n}\r\nstatic int iop_send_sync_request_mvfrey(struct hptiop_hba *hba,\r\nu32 size_bits, u32 millisec)\r\n{\r\nstruct hpt_iop_request_header *reqhdr =\r\nhba->u.mvfrey.internal_req.req_virt;\r\nu32 i;\r\nhba->msg_done = 0;\r\nreqhdr->flags |= cpu_to_le32(IOP_REQUEST_FLAG_SYNC_REQUEST);\r\nhba->ops->post_req(hba, &(hba->u.mvfrey.internal_req));\r\nfor (i = 0; i < millisec; i++) {\r\niop_intr_mvfrey(hba);\r\nif (hba->msg_done)\r\nbreak;\r\nmsleep(1);\r\n}\r\nreturn hba->msg_done ? 0 : -1;\r\n}\r\nstatic void hptiop_post_msg_itl(struct hptiop_hba *hba, u32 msg)\r\n{\r\nwritel(msg, &hba->u.itl.iop->inbound_msgaddr0);\r\nreadl(&hba->u.itl.iop->outbound_intstatus);\r\n}\r\nstatic void hptiop_post_msg_mv(struct hptiop_hba *hba, u32 msg)\r\n{\r\nwritel(msg, &hba->u.mv.mu->inbound_msg);\r\nwritel(MVIOP_MU_INBOUND_INT_MSG, &hba->u.mv.regs->inbound_doorbell);\r\nreadl(&hba->u.mv.regs->inbound_doorbell);\r\n}\r\nstatic void hptiop_post_msg_mvfrey(struct hptiop_hba *hba, u32 msg)\r\n{\r\nwritel(msg, &(hba->u.mvfrey.mu->f0_to_cpu_msg_a));\r\nreadl(&(hba->u.mvfrey.mu->f0_to_cpu_msg_a));\r\n}\r\nstatic int iop_send_sync_msg(struct hptiop_hba *hba, u32 msg, u32 millisec)\r\n{\r\nu32 i;\r\nhba->msg_done = 0;\r\nhba->ops->disable_intr(hba);\r\nhba->ops->post_msg(hba, msg);\r\nfor (i = 0; i < millisec; i++) {\r\nspin_lock_irq(hba->host->host_lock);\r\nhba->ops->iop_intr(hba);\r\nspin_unlock_irq(hba->host->host_lock);\r\nif (hba->msg_done)\r\nbreak;\r\nmsleep(1);\r\n}\r\nhba->ops->enable_intr(hba);\r\nreturn hba->msg_done? 0 : -1;\r\n}\r\nstatic int iop_get_config_itl(struct hptiop_hba *hba,\r\nstruct hpt_iop_request_get_config *config)\r\n{\r\nu32 req32;\r\nstruct hpt_iop_request_get_config __iomem *req;\r\nreq32 = readl(&hba->u.itl.iop->inbound_queue);\r\nif (req32 == IOPMU_QUEUE_EMPTY)\r\nreturn -1;\r\nreq = (struct hpt_iop_request_get_config __iomem *)\r\n((unsigned long)hba->u.itl.iop + req32);\r\nwritel(0, &req->header.flags);\r\nwritel(IOP_REQUEST_TYPE_GET_CONFIG, &req->header.type);\r\nwritel(sizeof(struct hpt_iop_request_get_config), &req->header.size);\r\nwritel(IOP_RESULT_PENDING, &req->header.result);\r\nif (iop_send_sync_request_itl(hba, req, 20000)) {\r\ndprintk("Get config send cmd failed\n");\r\nreturn -1;\r\n}\r\nmemcpy_fromio(config, req, sizeof(*config));\r\nwritel(req32, &hba->u.itl.iop->outbound_queue);\r\nreturn 0;\r\n}\r\nstatic int iop_get_config_mv(struct hptiop_hba *hba,\r\nstruct hpt_iop_request_get_config *config)\r\n{\r\nstruct hpt_iop_request_get_config *req = hba->u.mv.internal_req;\r\nreq->header.flags = cpu_to_le32(IOP_REQUEST_FLAG_OUTPUT_CONTEXT);\r\nreq->header.type = cpu_to_le32(IOP_REQUEST_TYPE_GET_CONFIG);\r\nreq->header.size =\r\ncpu_to_le32(sizeof(struct hpt_iop_request_get_config));\r\nreq->header.result = cpu_to_le32(IOP_RESULT_PENDING);\r\nreq->header.context = cpu_to_le32(IOP_REQUEST_TYPE_GET_CONFIG<<5);\r\nreq->header.context_hi32 = 0;\r\nif (iop_send_sync_request_mv(hba, 0, 20000)) {\r\ndprintk("Get config send cmd failed\n");\r\nreturn -1;\r\n}\r\nmemcpy(config, req, sizeof(struct hpt_iop_request_get_config));\r\nreturn 0;\r\n}\r\nstatic int iop_get_config_mvfrey(struct hptiop_hba *hba,\r\nstruct hpt_iop_request_get_config *config)\r\n{\r\nstruct hpt_iop_request_get_config *info = hba->u.mvfrey.config;\r\nif (info->header.size != sizeof(struct hpt_iop_request_get_config) ||\r\ninfo->header.type != IOP_REQUEST_TYPE_GET_CONFIG)\r\nreturn -1;\r\nconfig->interface_version = info->interface_version;\r\nconfig->firmware_version = info->firmware_version;\r\nconfig->max_requests = info->max_requests;\r\nconfig->request_size = info->request_size;\r\nconfig->max_sg_count = info->max_sg_count;\r\nconfig->data_transfer_length = info->data_transfer_length;\r\nconfig->alignment_mask = info->alignment_mask;\r\nconfig->max_devices = info->max_devices;\r\nconfig->sdram_size = info->sdram_size;\r\nreturn 0;\r\n}\r\nstatic int iop_set_config_itl(struct hptiop_hba *hba,\r\nstruct hpt_iop_request_set_config *config)\r\n{\r\nu32 req32;\r\nstruct hpt_iop_request_set_config __iomem *req;\r\nreq32 = readl(&hba->u.itl.iop->inbound_queue);\r\nif (req32 == IOPMU_QUEUE_EMPTY)\r\nreturn -1;\r\nreq = (struct hpt_iop_request_set_config __iomem *)\r\n((unsigned long)hba->u.itl.iop + req32);\r\nmemcpy_toio((u8 __iomem *)req + sizeof(struct hpt_iop_request_header),\r\n(u8 *)config + sizeof(struct hpt_iop_request_header),\r\nsizeof(struct hpt_iop_request_set_config) -\r\nsizeof(struct hpt_iop_request_header));\r\nwritel(0, &req->header.flags);\r\nwritel(IOP_REQUEST_TYPE_SET_CONFIG, &req->header.type);\r\nwritel(sizeof(struct hpt_iop_request_set_config), &req->header.size);\r\nwritel(IOP_RESULT_PENDING, &req->header.result);\r\nif (iop_send_sync_request_itl(hba, req, 20000)) {\r\ndprintk("Set config send cmd failed\n");\r\nreturn -1;\r\n}\r\nwritel(req32, &hba->u.itl.iop->outbound_queue);\r\nreturn 0;\r\n}\r\nstatic int iop_set_config_mv(struct hptiop_hba *hba,\r\nstruct hpt_iop_request_set_config *config)\r\n{\r\nstruct hpt_iop_request_set_config *req = hba->u.mv.internal_req;\r\nmemcpy(req, config, sizeof(struct hpt_iop_request_set_config));\r\nreq->header.flags = cpu_to_le32(IOP_REQUEST_FLAG_OUTPUT_CONTEXT);\r\nreq->header.type = cpu_to_le32(IOP_REQUEST_TYPE_SET_CONFIG);\r\nreq->header.size =\r\ncpu_to_le32(sizeof(struct hpt_iop_request_set_config));\r\nreq->header.result = cpu_to_le32(IOP_RESULT_PENDING);\r\nreq->header.context = cpu_to_le32(IOP_REQUEST_TYPE_SET_CONFIG<<5);\r\nreq->header.context_hi32 = 0;\r\nif (iop_send_sync_request_mv(hba, 0, 20000)) {\r\ndprintk("Set config send cmd failed\n");\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int iop_set_config_mvfrey(struct hptiop_hba *hba,\r\nstruct hpt_iop_request_set_config *config)\r\n{\r\nstruct hpt_iop_request_set_config *req =\r\nhba->u.mvfrey.internal_req.req_virt;\r\nmemcpy(req, config, sizeof(struct hpt_iop_request_set_config));\r\nreq->header.flags = cpu_to_le32(IOP_REQUEST_FLAG_OUTPUT_CONTEXT);\r\nreq->header.type = cpu_to_le32(IOP_REQUEST_TYPE_SET_CONFIG);\r\nreq->header.size =\r\ncpu_to_le32(sizeof(struct hpt_iop_request_set_config));\r\nreq->header.result = cpu_to_le32(IOP_RESULT_PENDING);\r\nreq->header.context = cpu_to_le32(IOP_REQUEST_TYPE_SET_CONFIG<<5);\r\nreq->header.context_hi32 = 0;\r\nif (iop_send_sync_request_mvfrey(hba, 0, 20000)) {\r\ndprintk("Set config send cmd failed\n");\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void hptiop_enable_intr_itl(struct hptiop_hba *hba)\r\n{\r\nwritel(~(IOPMU_OUTBOUND_INT_POSTQUEUE | IOPMU_OUTBOUND_INT_MSG0),\r\n&hba->u.itl.iop->outbound_intmask);\r\n}\r\nstatic void hptiop_enable_intr_mv(struct hptiop_hba *hba)\r\n{\r\nwritel(MVIOP_MU_OUTBOUND_INT_POSTQUEUE | MVIOP_MU_OUTBOUND_INT_MSG,\r\n&hba->u.mv.regs->outbound_intmask);\r\n}\r\nstatic void hptiop_enable_intr_mvfrey(struct hptiop_hba *hba)\r\n{\r\nwritel(CPU_TO_F0_DRBL_MSG_BIT, &(hba->u.mvfrey.mu->f0_doorbell_enable));\r\nwritel(0x1, &(hba->u.mvfrey.mu->isr_enable));\r\nwritel(0x1010, &(hba->u.mvfrey.mu->pcie_f0_int_enable));\r\n}\r\nstatic int hptiop_initialize_iop(struct hptiop_hba *hba)\r\n{\r\nhba->ops->enable_intr(hba);\r\nhba->initialized = 1;\r\nif (iop_send_sync_msg(hba,\r\nIOPMU_INBOUND_MSG0_START_BACKGROUND_TASK, 5000)) {\r\nprintk(KERN_ERR "scsi%d: fail to start background task\n",\r\nhba->host->host_no);\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __iomem *hptiop_map_pci_bar(struct hptiop_hba *hba, int index)\r\n{\r\nu32 mem_base_phy, length;\r\nvoid __iomem *mem_base_virt;\r\nstruct pci_dev *pcidev = hba->pcidev;\r\nif (!(pci_resource_flags(pcidev, index) & IORESOURCE_MEM)) {\r\nprintk(KERN_ERR "scsi%d: pci resource invalid\n",\r\nhba->host->host_no);\r\nreturn NULL;\r\n}\r\nmem_base_phy = pci_resource_start(pcidev, index);\r\nlength = pci_resource_len(pcidev, index);\r\nmem_base_virt = ioremap(mem_base_phy, length);\r\nif (!mem_base_virt) {\r\nprintk(KERN_ERR "scsi%d: Fail to ioremap memory space\n",\r\nhba->host->host_no);\r\nreturn NULL;\r\n}\r\nreturn mem_base_virt;\r\n}\r\nstatic int hptiop_map_pci_bar_itl(struct hptiop_hba *hba)\r\n{\r\nstruct pci_dev *pcidev = hba->pcidev;\r\nhba->u.itl.iop = hptiop_map_pci_bar(hba, 0);\r\nif (hba->u.itl.iop == NULL)\r\nreturn -1;\r\nif ((pcidev->device & 0xff00) == 0x4400) {\r\nhba->u.itl.plx = hba->u.itl.iop;\r\nhba->u.itl.iop = hptiop_map_pci_bar(hba, 2);\r\nif (hba->u.itl.iop == NULL) {\r\niounmap(hba->u.itl.plx);\r\nreturn -1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void hptiop_unmap_pci_bar_itl(struct hptiop_hba *hba)\r\n{\r\nif (hba->u.itl.plx)\r\niounmap(hba->u.itl.plx);\r\niounmap(hba->u.itl.iop);\r\n}\r\nstatic int hptiop_map_pci_bar_mv(struct hptiop_hba *hba)\r\n{\r\nhba->u.mv.regs = hptiop_map_pci_bar(hba, 0);\r\nif (hba->u.mv.regs == NULL)\r\nreturn -1;\r\nhba->u.mv.mu = hptiop_map_pci_bar(hba, 2);\r\nif (hba->u.mv.mu == NULL) {\r\niounmap(hba->u.mv.regs);\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int hptiop_map_pci_bar_mvfrey(struct hptiop_hba *hba)\r\n{\r\nhba->u.mvfrey.config = hptiop_map_pci_bar(hba, 0);\r\nif (hba->u.mvfrey.config == NULL)\r\nreturn -1;\r\nhba->u.mvfrey.mu = hptiop_map_pci_bar(hba, 2);\r\nif (hba->u.mvfrey.mu == NULL) {\r\niounmap(hba->u.mvfrey.config);\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void hptiop_unmap_pci_bar_mv(struct hptiop_hba *hba)\r\n{\r\niounmap(hba->u.mv.regs);\r\niounmap(hba->u.mv.mu);\r\n}\r\nstatic void hptiop_unmap_pci_bar_mvfrey(struct hptiop_hba *hba)\r\n{\r\niounmap(hba->u.mvfrey.config);\r\niounmap(hba->u.mvfrey.mu);\r\n}\r\nstatic void hptiop_message_callback(struct hptiop_hba *hba, u32 msg)\r\n{\r\ndprintk("iop message 0x%x\n", msg);\r\nif (msg == IOPMU_INBOUND_MSG0_NOP ||\r\nmsg == IOPMU_INBOUND_MSG0_RESET_COMM)\r\nhba->msg_done = 1;\r\nif (!hba->initialized)\r\nreturn;\r\nif (msg == IOPMU_INBOUND_MSG0_RESET) {\r\natomic_set(&hba->resetting, 0);\r\nwake_up(&hba->reset_wq);\r\n}\r\nelse if (msg <= IOPMU_INBOUND_MSG0_MAX)\r\nhba->msg_done = 1;\r\n}\r\nstatic struct hptiop_request *get_req(struct hptiop_hba *hba)\r\n{\r\nstruct hptiop_request *ret;\r\ndprintk("get_req : req=%p\n", hba->req_list);\r\nret = hba->req_list;\r\nif (ret)\r\nhba->req_list = ret->next;\r\nreturn ret;\r\n}\r\nstatic void free_req(struct hptiop_hba *hba, struct hptiop_request *req)\r\n{\r\ndprintk("free_req(%d, %p)\n", req->index, req);\r\nreq->next = hba->req_list;\r\nhba->req_list = req;\r\n}\r\nstatic void hptiop_finish_scsi_req(struct hptiop_hba *hba, u32 tag,\r\nstruct hpt_iop_request_scsi_command *req)\r\n{\r\nstruct scsi_cmnd *scp;\r\ndprintk("hptiop_finish_scsi_req: req=%p, type=%d, "\r\n"result=%d, context=0x%x tag=%d\n",\r\nreq, req->header.type, req->header.result,\r\nreq->header.context, tag);\r\nBUG_ON(!req->header.result);\r\nBUG_ON(req->header.type != cpu_to_le32(IOP_REQUEST_TYPE_SCSI_COMMAND));\r\nscp = hba->reqs[tag].scp;\r\nif (HPT_SCP(scp)->mapped)\r\nscsi_dma_unmap(scp);\r\nswitch (le32_to_cpu(req->header.result)) {\r\ncase IOP_RESULT_SUCCESS:\r\nscsi_set_resid(scp,\r\nscsi_bufflen(scp) - le32_to_cpu(req->dataxfer_length));\r\nscp->result = (DID_OK<<16);\r\nbreak;\r\ncase IOP_RESULT_BAD_TARGET:\r\nscp->result = (DID_BAD_TARGET<<16);\r\nbreak;\r\ncase IOP_RESULT_BUSY:\r\nscp->result = (DID_BUS_BUSY<<16);\r\nbreak;\r\ncase IOP_RESULT_RESET:\r\nscp->result = (DID_RESET<<16);\r\nbreak;\r\ncase IOP_RESULT_FAIL:\r\nscp->result = (DID_ERROR<<16);\r\nbreak;\r\ncase IOP_RESULT_INVALID_REQUEST:\r\nscp->result = (DID_ABORT<<16);\r\nbreak;\r\ncase IOP_RESULT_CHECK_CONDITION:\r\nscsi_set_resid(scp,\r\nscsi_bufflen(scp) - le32_to_cpu(req->dataxfer_length));\r\nscp->result = SAM_STAT_CHECK_CONDITION;\r\nmemcpy(scp->sense_buffer, &req->sg_list,\r\nmin_t(size_t, SCSI_SENSE_BUFFERSIZE,\r\nle32_to_cpu(req->dataxfer_length)));\r\ngoto skip_resid;\r\nbreak;\r\ndefault:\r\nscp->result = DRIVER_INVALID << 24 | DID_ABORT << 16;\r\nbreak;\r\n}\r\nscsi_set_resid(scp,\r\nscsi_bufflen(scp) - le32_to_cpu(req->dataxfer_length));\r\nskip_resid:\r\ndprintk("scsi_done(%p)\n", scp);\r\nscp->scsi_done(scp);\r\nfree_req(hba, &hba->reqs[tag]);\r\n}\r\nstatic void hptiop_host_request_callback_itl(struct hptiop_hba *hba, u32 _tag)\r\n{\r\nstruct hpt_iop_request_scsi_command *req;\r\nu32 tag;\r\nif (hba->iopintf_v2) {\r\ntag = _tag & ~IOPMU_QUEUE_REQUEST_RESULT_BIT;\r\nreq = hba->reqs[tag].req_virt;\r\nif (likely(_tag & IOPMU_QUEUE_REQUEST_RESULT_BIT))\r\nreq->header.result = cpu_to_le32(IOP_RESULT_SUCCESS);\r\n} else {\r\ntag = _tag;\r\nreq = hba->reqs[tag].req_virt;\r\n}\r\nhptiop_finish_scsi_req(hba, tag, req);\r\n}\r\nvoid hptiop_iop_request_callback_itl(struct hptiop_hba *hba, u32 tag)\r\n{\r\nstruct hpt_iop_request_header __iomem *req;\r\nstruct hpt_iop_request_ioctl_command __iomem *p;\r\nstruct hpt_ioctl_k *arg;\r\nreq = (struct hpt_iop_request_header __iomem *)\r\n((unsigned long)hba->u.itl.iop + tag);\r\ndprintk("hptiop_iop_request_callback_itl: req=%p, type=%d, "\r\n"result=%d, context=0x%x tag=%d\n",\r\nreq, readl(&req->type), readl(&req->result),\r\nreadl(&req->context), tag);\r\nBUG_ON(!readl(&req->result));\r\nBUG_ON(readl(&req->type) != IOP_REQUEST_TYPE_IOCTL_COMMAND);\r\np = (struct hpt_iop_request_ioctl_command __iomem *)req;\r\narg = (struct hpt_ioctl_k *)(unsigned long)\r\n(readl(&req->context) |\r\n((u64)readl(&req->context_hi32)<<32));\r\nif (readl(&req->result) == IOP_RESULT_SUCCESS) {\r\narg->result = HPT_IOCTL_RESULT_OK;\r\nif (arg->outbuf_size)\r\nmemcpy_fromio(arg->outbuf,\r\n&p->buf[(readl(&p->inbuf_size) + 3)& ~3],\r\narg->outbuf_size);\r\nif (arg->bytes_returned)\r\n*arg->bytes_returned = arg->outbuf_size;\r\n}\r\nelse\r\narg->result = HPT_IOCTL_RESULT_FAILED;\r\narg->done(arg);\r\nwritel(tag, &hba->u.itl.iop->outbound_queue);\r\n}\r\nstatic irqreturn_t hptiop_intr(int irq, void *dev_id)\r\n{\r\nstruct hptiop_hba *hba = dev_id;\r\nint handled;\r\nunsigned long flags;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhandled = hba->ops->iop_intr(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn handled;\r\n}\r\nstatic int hptiop_buildsgl(struct scsi_cmnd *scp, struct hpt_iopsg *psg)\r\n{\r\nstruct Scsi_Host *host = scp->device->host;\r\nstruct hptiop_hba *hba = (struct hptiop_hba *)host->hostdata;\r\nstruct scatterlist *sg;\r\nint idx, nseg;\r\nnseg = scsi_dma_map(scp);\r\nBUG_ON(nseg < 0);\r\nif (!nseg)\r\nreturn 0;\r\nHPT_SCP(scp)->sgcnt = nseg;\r\nHPT_SCP(scp)->mapped = 1;\r\nBUG_ON(HPT_SCP(scp)->sgcnt > hba->max_sg_descriptors);\r\nscsi_for_each_sg(scp, sg, HPT_SCP(scp)->sgcnt, idx) {\r\npsg[idx].pci_address = cpu_to_le64(sg_dma_address(sg)) |\r\nhba->ops->host_phy_flag;\r\npsg[idx].size = cpu_to_le32(sg_dma_len(sg));\r\npsg[idx].eot = (idx == HPT_SCP(scp)->sgcnt - 1) ?\r\ncpu_to_le32(1) : 0;\r\n}\r\nreturn HPT_SCP(scp)->sgcnt;\r\n}\r\nstatic void hptiop_post_req_itl(struct hptiop_hba *hba,\r\nstruct hptiop_request *_req)\r\n{\r\nstruct hpt_iop_request_header *reqhdr = _req->req_virt;\r\nreqhdr->context = cpu_to_le32(IOPMU_QUEUE_ADDR_HOST_BIT |\r\n(u32)_req->index);\r\nreqhdr->context_hi32 = 0;\r\nif (hba->iopintf_v2) {\r\nu32 size, size_bits;\r\nsize = le32_to_cpu(reqhdr->size);\r\nif (size < 256)\r\nsize_bits = IOPMU_QUEUE_REQUEST_SIZE_BIT;\r\nelse if (size < 512)\r\nsize_bits = IOPMU_QUEUE_ADDR_HOST_BIT;\r\nelse\r\nsize_bits = IOPMU_QUEUE_REQUEST_SIZE_BIT |\r\nIOPMU_QUEUE_ADDR_HOST_BIT;\r\nwritel(_req->req_shifted_phy | size_bits,\r\n&hba->u.itl.iop->inbound_queue);\r\n} else\r\nwritel(_req->req_shifted_phy | IOPMU_QUEUE_ADDR_HOST_BIT,\r\n&hba->u.itl.iop->inbound_queue);\r\n}\r\nstatic void hptiop_post_req_mv(struct hptiop_hba *hba,\r\nstruct hptiop_request *_req)\r\n{\r\nstruct hpt_iop_request_header *reqhdr = _req->req_virt;\r\nu32 size, size_bit;\r\nreqhdr->context = cpu_to_le32(_req->index<<8 |\r\nIOP_REQUEST_TYPE_SCSI_COMMAND<<5);\r\nreqhdr->context_hi32 = 0;\r\nsize = le32_to_cpu(reqhdr->size);\r\nif (size <= 256)\r\nsize_bit = 0;\r\nelse if (size <= 256*2)\r\nsize_bit = 1;\r\nelse if (size <= 256*3)\r\nsize_bit = 2;\r\nelse\r\nsize_bit = 3;\r\nmv_inbound_write((_req->req_shifted_phy << 5) |\r\nMVIOP_MU_QUEUE_ADDR_HOST_BIT | size_bit, hba);\r\n}\r\nstatic void hptiop_post_req_mvfrey(struct hptiop_hba *hba,\r\nstruct hptiop_request *_req)\r\n{\r\nstruct hpt_iop_request_header *reqhdr = _req->req_virt;\r\nu32 index;\r\nreqhdr->flags |= cpu_to_le32(IOP_REQUEST_FLAG_OUTPUT_CONTEXT |\r\nIOP_REQUEST_FLAG_ADDR_BITS |\r\n((_req->req_shifted_phy >> 11) & 0xffff0000));\r\nreqhdr->context = cpu_to_le32(IOPMU_QUEUE_ADDR_HOST_BIT |\r\n(_req->index << 4) | reqhdr->type);\r\nreqhdr->context_hi32 = cpu_to_le32((_req->req_shifted_phy << 5) &\r\n0xffffffff);\r\nhba->u.mvfrey.inlist_wptr++;\r\nindex = hba->u.mvfrey.inlist_wptr & 0x3fff;\r\nif (index == hba->u.mvfrey.list_count) {\r\nindex = 0;\r\nhba->u.mvfrey.inlist_wptr &= ~0x3fff;\r\nhba->u.mvfrey.inlist_wptr ^= CL_POINTER_TOGGLE;\r\n}\r\nhba->u.mvfrey.inlist[index].addr =\r\n(dma_addr_t)_req->req_shifted_phy << 5;\r\nhba->u.mvfrey.inlist[index].intrfc_len = (reqhdr->size + 3) / 4;\r\nwritel(hba->u.mvfrey.inlist_wptr,\r\n&(hba->u.mvfrey.mu->inbound_write_ptr));\r\nreadl(&(hba->u.mvfrey.mu->inbound_write_ptr));\r\n}\r\nstatic int hptiop_reset_comm_itl(struct hptiop_hba *hba)\r\n{\r\nreturn 0;\r\n}\r\nstatic int hptiop_reset_comm_mv(struct hptiop_hba *hba)\r\n{\r\nreturn 0;\r\n}\r\nstatic int hptiop_reset_comm_mvfrey(struct hptiop_hba *hba)\r\n{\r\nu32 list_count = hba->u.mvfrey.list_count;\r\nif (iop_send_sync_msg(hba, IOPMU_INBOUND_MSG0_RESET_COMM, 3000))\r\nreturn -1;\r\nmsleep(100);\r\nwritel(cpu_to_le32(hba->u.mvfrey.inlist_phy & 0xffffffff),\r\n&(hba->u.mvfrey.mu->inbound_base));\r\nwritel(cpu_to_le32((hba->u.mvfrey.inlist_phy >> 16) >> 16),\r\n&(hba->u.mvfrey.mu->inbound_base_high));\r\nwritel(cpu_to_le32(hba->u.mvfrey.outlist_phy & 0xffffffff),\r\n&(hba->u.mvfrey.mu->outbound_base));\r\nwritel(cpu_to_le32((hba->u.mvfrey.outlist_phy >> 16) >> 16),\r\n&(hba->u.mvfrey.mu->outbound_base_high));\r\nwritel(cpu_to_le32(hba->u.mvfrey.outlist_cptr_phy & 0xffffffff),\r\n&(hba->u.mvfrey.mu->outbound_shadow_base));\r\nwritel(cpu_to_le32((hba->u.mvfrey.outlist_cptr_phy >> 16) >> 16),\r\n&(hba->u.mvfrey.mu->outbound_shadow_base_high));\r\nhba->u.mvfrey.inlist_wptr = (list_count - 1) | CL_POINTER_TOGGLE;\r\n*hba->u.mvfrey.outlist_cptr = (list_count - 1) | CL_POINTER_TOGGLE;\r\nhba->u.mvfrey.outlist_rptr = list_count - 1;\r\nreturn 0;\r\n}\r\nstatic int hptiop_queuecommand_lck(struct scsi_cmnd *scp,\r\nvoid (*done)(struct scsi_cmnd *))\r\n{\r\nstruct Scsi_Host *host = scp->device->host;\r\nstruct hptiop_hba *hba = (struct hptiop_hba *)host->hostdata;\r\nstruct hpt_iop_request_scsi_command *req;\r\nint sg_count = 0;\r\nstruct hptiop_request *_req;\r\nBUG_ON(!done);\r\nscp->scsi_done = done;\r\n_req = get_req(hba);\r\nif (_req == NULL) {\r\ndprintk("hptiop_queuecmd : no free req\n");\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\n_req->scp = scp;\r\ndprintk("hptiop_queuecmd(scp=%p) %d/%d/%d/%d cdb=(%08x-%08x-%08x-%08x) "\r\n"req_index=%d, req=%p\n",\r\nscp,\r\nhost->host_no, scp->device->channel,\r\nscp->device->id, scp->device->lun,\r\ncpu_to_be32(((u32 *)scp->cmnd)[0]),\r\ncpu_to_be32(((u32 *)scp->cmnd)[1]),\r\ncpu_to_be32(((u32 *)scp->cmnd)[2]),\r\ncpu_to_be32(((u32 *)scp->cmnd)[3]),\r\n_req->index, _req->req_virt);\r\nscp->result = 0;\r\nif (scp->device->channel || scp->device->lun ||\r\nscp->device->id > hba->max_devices) {\r\nscp->result = DID_BAD_TARGET << 16;\r\nfree_req(hba, _req);\r\ngoto cmd_done;\r\n}\r\nreq = _req->req_virt;\r\nsg_count = hptiop_buildsgl(scp, req->sg_list);\r\nif (!sg_count)\r\nHPT_SCP(scp)->mapped = 0;\r\nreq->header.flags = cpu_to_le32(IOP_REQUEST_FLAG_OUTPUT_CONTEXT);\r\nreq->header.type = cpu_to_le32(IOP_REQUEST_TYPE_SCSI_COMMAND);\r\nreq->header.result = cpu_to_le32(IOP_RESULT_PENDING);\r\nreq->dataxfer_length = cpu_to_le32(scsi_bufflen(scp));\r\nreq->channel = scp->device->channel;\r\nreq->target = scp->device->id;\r\nreq->lun = scp->device->lun;\r\nreq->header.size = cpu_to_le32(\r\nsizeof(struct hpt_iop_request_scsi_command)\r\n- sizeof(struct hpt_iopsg)\r\n+ sg_count * sizeof(struct hpt_iopsg));\r\nmemcpy(req->cdb, scp->cmnd, sizeof(req->cdb));\r\nhba->ops->post_req(hba, _req);\r\nreturn 0;\r\ncmd_done:\r\ndprintk("scsi_done(scp=%p)\n", scp);\r\nscp->scsi_done(scp);\r\nreturn 0;\r\n}\r\nstatic int hptiop_reset_hba(struct hptiop_hba *hba)\r\n{\r\nif (atomic_xchg(&hba->resetting, 1) == 0) {\r\natomic_inc(&hba->reset_count);\r\nhba->ops->post_msg(hba, IOPMU_INBOUND_MSG0_RESET);\r\n}\r\nwait_event_timeout(hba->reset_wq,\r\natomic_read(&hba->resetting) == 0, 60 * HZ);\r\nif (atomic_read(&hba->resetting)) {\r\nprintk(KERN_ERR "scsi%d: reset failed\n", hba->host->host_no);\r\nreturn -1;\r\n}\r\nif (iop_send_sync_msg(hba,\r\nIOPMU_INBOUND_MSG0_START_BACKGROUND_TASK, 5000)) {\r\ndprintk("scsi%d: fail to start background task\n",\r\nhba->host->host_no);\r\n}\r\nreturn 0;\r\n}\r\nstatic int hptiop_reset(struct scsi_cmnd *scp)\r\n{\r\nstruct Scsi_Host * host = scp->device->host;\r\nstruct hptiop_hba * hba = (struct hptiop_hba *)host->hostdata;\r\nprintk(KERN_WARNING "hptiop_reset(%d/%d/%d) scp=%p\n",\r\nscp->device->host->host_no, scp->device->channel,\r\nscp->device->id, scp);\r\nreturn hptiop_reset_hba(hba)? FAILED : SUCCESS;\r\n}\r\nstatic int hptiop_adjust_disk_queue_depth(struct scsi_device *sdev,\r\nint queue_depth, int reason)\r\n{\r\nstruct hptiop_hba *hba = (struct hptiop_hba *)sdev->host->hostdata;\r\nif (reason != SCSI_QDEPTH_DEFAULT)\r\nreturn -EOPNOTSUPP;\r\nif (queue_depth > hba->max_requests)\r\nqueue_depth = hba->max_requests;\r\nscsi_adjust_queue_depth(sdev, MSG_ORDERED_TAG, queue_depth);\r\nreturn queue_depth;\r\n}\r\nstatic ssize_t hptiop_show_version(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nreturn snprintf(buf, PAGE_SIZE, "%s\n", driver_ver);\r\n}\r\nstatic ssize_t hptiop_show_fw_version(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *host = class_to_shost(dev);\r\nstruct hptiop_hba *hba = (struct hptiop_hba *)host->hostdata;\r\nreturn snprintf(buf, PAGE_SIZE, "%d.%d.%d.%d\n",\r\nhba->firmware_version >> 24,\r\n(hba->firmware_version >> 16) & 0xff,\r\n(hba->firmware_version >> 8) & 0xff,\r\nhba->firmware_version & 0xff);\r\n}\r\nstatic int hptiop_internal_memalloc_itl(struct hptiop_hba *hba)\r\n{\r\nreturn 0;\r\n}\r\nstatic int hptiop_internal_memalloc_mv(struct hptiop_hba *hba)\r\n{\r\nhba->u.mv.internal_req = dma_alloc_coherent(&hba->pcidev->dev,\r\n0x800, &hba->u.mv.internal_req_phy, GFP_KERNEL);\r\nif (hba->u.mv.internal_req)\r\nreturn 0;\r\nelse\r\nreturn -1;\r\n}\r\nstatic int hptiop_internal_memalloc_mvfrey(struct hptiop_hba *hba)\r\n{\r\nu32 list_count = readl(&hba->u.mvfrey.mu->inbound_conf_ctl);\r\nchar *p;\r\ndma_addr_t phy;\r\nBUG_ON(hba->max_request_size == 0);\r\nif (list_count == 0) {\r\nBUG_ON(1);\r\nreturn -1;\r\n}\r\nlist_count >>= 16;\r\nhba->u.mvfrey.list_count = list_count;\r\nhba->u.mvfrey.internal_mem_size = 0x800 +\r\nlist_count * sizeof(struct mvfrey_inlist_entry) +\r\nlist_count * sizeof(struct mvfrey_outlist_entry) +\r\nsizeof(int);\r\np = dma_alloc_coherent(&hba->pcidev->dev,\r\nhba->u.mvfrey.internal_mem_size, &phy, GFP_KERNEL);\r\nif (!p)\r\nreturn -1;\r\nhba->u.mvfrey.internal_req.req_virt = p;\r\nhba->u.mvfrey.internal_req.req_shifted_phy = phy >> 5;\r\nhba->u.mvfrey.internal_req.scp = NULL;\r\nhba->u.mvfrey.internal_req.next = NULL;\r\np += 0x800;\r\nphy += 0x800;\r\nhba->u.mvfrey.inlist = (struct mvfrey_inlist_entry *)p;\r\nhba->u.mvfrey.inlist_phy = phy;\r\np += list_count * sizeof(struct mvfrey_inlist_entry);\r\nphy += list_count * sizeof(struct mvfrey_inlist_entry);\r\nhba->u.mvfrey.outlist = (struct mvfrey_outlist_entry *)p;\r\nhba->u.mvfrey.outlist_phy = phy;\r\np += list_count * sizeof(struct mvfrey_outlist_entry);\r\nphy += list_count * sizeof(struct mvfrey_outlist_entry);\r\nhba->u.mvfrey.outlist_cptr = (__le32 *)p;\r\nhba->u.mvfrey.outlist_cptr_phy = phy;\r\nreturn 0;\r\n}\r\nstatic int hptiop_internal_memfree_itl(struct hptiop_hba *hba)\r\n{\r\nreturn 0;\r\n}\r\nstatic int hptiop_internal_memfree_mv(struct hptiop_hba *hba)\r\n{\r\nif (hba->u.mv.internal_req) {\r\ndma_free_coherent(&hba->pcidev->dev, 0x800,\r\nhba->u.mv.internal_req, hba->u.mv.internal_req_phy);\r\nreturn 0;\r\n} else\r\nreturn -1;\r\n}\r\nstatic int hptiop_internal_memfree_mvfrey(struct hptiop_hba *hba)\r\n{\r\nif (hba->u.mvfrey.internal_req.req_virt) {\r\ndma_free_coherent(&hba->pcidev->dev,\r\nhba->u.mvfrey.internal_mem_size,\r\nhba->u.mvfrey.internal_req.req_virt,\r\n(dma_addr_t)\r\nhba->u.mvfrey.internal_req.req_shifted_phy << 5);\r\nreturn 0;\r\n} else\r\nreturn -1;\r\n}\r\nstatic int hptiop_probe(struct pci_dev *pcidev, const struct pci_device_id *id)\r\n{\r\nstruct Scsi_Host *host = NULL;\r\nstruct hptiop_hba *hba;\r\nstruct hptiop_adapter_ops *iop_ops;\r\nstruct hpt_iop_request_get_config iop_config;\r\nstruct hpt_iop_request_set_config set_config;\r\ndma_addr_t start_phy;\r\nvoid *start_virt;\r\nu32 offset, i, req_size;\r\ndprintk("hptiop_probe(%p)\n", pcidev);\r\nif (pci_enable_device(pcidev)) {\r\nprintk(KERN_ERR "hptiop: fail to enable pci device\n");\r\nreturn -ENODEV;\r\n}\r\nprintk(KERN_INFO "adapter at PCI %d:%d:%d, IRQ %d\n",\r\npcidev->bus->number, pcidev->devfn >> 3, pcidev->devfn & 7,\r\npcidev->irq);\r\npci_set_master(pcidev);\r\niop_ops = (struct hptiop_adapter_ops *)id->driver_data;\r\nif (pci_set_dma_mask(pcidev, DMA_BIT_MASK(iop_ops->hw_dma_bit_mask))) {\r\nif (pci_set_dma_mask(pcidev, DMA_BIT_MASK(32))) {\r\nprintk(KERN_ERR "hptiop: fail to set dma_mask\n");\r\ngoto disable_pci_device;\r\n}\r\n}\r\nif (pci_request_regions(pcidev, driver_name)) {\r\nprintk(KERN_ERR "hptiop: pci_request_regions failed\n");\r\ngoto disable_pci_device;\r\n}\r\nhost = scsi_host_alloc(&driver_template, sizeof(struct hptiop_hba));\r\nif (!host) {\r\nprintk(KERN_ERR "hptiop: fail to alloc scsi host\n");\r\ngoto free_pci_regions;\r\n}\r\nhba = (struct hptiop_hba *)host->hostdata;\r\nhba->ops = iop_ops;\r\nhba->pcidev = pcidev;\r\nhba->host = host;\r\nhba->initialized = 0;\r\nhba->iopintf_v2 = 0;\r\natomic_set(&hba->resetting, 0);\r\natomic_set(&hba->reset_count, 0);\r\ninit_waitqueue_head(&hba->reset_wq);\r\ninit_waitqueue_head(&hba->ioctl_wq);\r\nhost->max_lun = 1;\r\nhost->max_channel = 0;\r\nhost->io_port = 0;\r\nhost->n_io_port = 0;\r\nhost->irq = pcidev->irq;\r\nif (hba->ops->map_pci_bar(hba))\r\ngoto free_scsi_host;\r\nif (hba->ops->iop_wait_ready(hba, 20000)) {\r\nprintk(KERN_ERR "scsi%d: firmware not ready\n",\r\nhba->host->host_no);\r\ngoto unmap_pci_bar;\r\n}\r\nif (hba->ops->family == MV_BASED_IOP) {\r\nif (hba->ops->internal_memalloc(hba)) {\r\nprintk(KERN_ERR "scsi%d: internal_memalloc failed\n",\r\nhba->host->host_no);\r\ngoto unmap_pci_bar;\r\n}\r\n}\r\nif (hba->ops->get_config(hba, &iop_config)) {\r\nprintk(KERN_ERR "scsi%d: get config failed\n",\r\nhba->host->host_no);\r\ngoto unmap_pci_bar;\r\n}\r\nhba->max_requests = min(le32_to_cpu(iop_config.max_requests),\r\nHPTIOP_MAX_REQUESTS);\r\nhba->max_devices = le32_to_cpu(iop_config.max_devices);\r\nhba->max_request_size = le32_to_cpu(iop_config.request_size);\r\nhba->max_sg_descriptors = le32_to_cpu(iop_config.max_sg_count);\r\nhba->firmware_version = le32_to_cpu(iop_config.firmware_version);\r\nhba->interface_version = le32_to_cpu(iop_config.interface_version);\r\nhba->sdram_size = le32_to_cpu(iop_config.sdram_size);\r\nif (hba->ops->family == MVFREY_BASED_IOP) {\r\nif (hba->ops->internal_memalloc(hba)) {\r\nprintk(KERN_ERR "scsi%d: internal_memalloc failed\n",\r\nhba->host->host_no);\r\ngoto unmap_pci_bar;\r\n}\r\nif (hba->ops->reset_comm(hba)) {\r\nprintk(KERN_ERR "scsi%d: reset comm failed\n",\r\nhba->host->host_no);\r\ngoto unmap_pci_bar;\r\n}\r\n}\r\nif (hba->firmware_version > 0x01020000 ||\r\nhba->interface_version > 0x01020000)\r\nhba->iopintf_v2 = 1;\r\nhost->max_sectors = le32_to_cpu(iop_config.data_transfer_length) >> 9;\r\nhost->max_id = le32_to_cpu(iop_config.max_devices);\r\nhost->sg_tablesize = le32_to_cpu(iop_config.max_sg_count);\r\nhost->can_queue = le32_to_cpu(iop_config.max_requests);\r\nhost->cmd_per_lun = le32_to_cpu(iop_config.max_requests);\r\nhost->max_cmd_len = 16;\r\nreq_size = sizeof(struct hpt_iop_request_scsi_command)\r\n+ sizeof(struct hpt_iopsg) * (hba->max_sg_descriptors - 1);\r\nif ((req_size & 0x1f) != 0)\r\nreq_size = (req_size + 0x1f) & ~0x1f;\r\nmemset(&set_config, 0, sizeof(struct hpt_iop_request_set_config));\r\nset_config.iop_id = cpu_to_le32(host->host_no);\r\nset_config.vbus_id = cpu_to_le16(host->host_no);\r\nset_config.max_host_request_size = cpu_to_le16(req_size);\r\nif (hba->ops->set_config(hba, &set_config)) {\r\nprintk(KERN_ERR "scsi%d: set config failed\n",\r\nhba->host->host_no);\r\ngoto unmap_pci_bar;\r\n}\r\npci_set_drvdata(pcidev, host);\r\nif (request_irq(pcidev->irq, hptiop_intr, IRQF_SHARED,\r\ndriver_name, hba)) {\r\nprintk(KERN_ERR "scsi%d: request irq %d failed\n",\r\nhba->host->host_no, pcidev->irq);\r\ngoto unmap_pci_bar;\r\n}\r\ndprintk("req_size=%d, max_requests=%d\n", req_size, hba->max_requests);\r\nhba->req_size = req_size;\r\nstart_virt = dma_alloc_coherent(&pcidev->dev,\r\nhba->req_size*hba->max_requests + 0x20,\r\n&start_phy, GFP_KERNEL);\r\nif (!start_virt) {\r\nprintk(KERN_ERR "scsi%d: fail to alloc request mem\n",\r\nhba->host->host_no);\r\ngoto free_request_irq;\r\n}\r\nhba->dma_coherent = start_virt;\r\nhba->dma_coherent_handle = start_phy;\r\nif ((start_phy & 0x1f) != 0) {\r\noffset = ((start_phy + 0x1f) & ~0x1f) - start_phy;\r\nstart_phy += offset;\r\nstart_virt += offset;\r\n}\r\nhba->req_list = NULL;\r\nfor (i = 0; i < hba->max_requests; i++) {\r\nhba->reqs[i].next = NULL;\r\nhba->reqs[i].req_virt = start_virt;\r\nhba->reqs[i].req_shifted_phy = start_phy >> 5;\r\nhba->reqs[i].index = i;\r\nfree_req(hba, &hba->reqs[i]);\r\nstart_virt = (char *)start_virt + hba->req_size;\r\nstart_phy = start_phy + hba->req_size;\r\n}\r\nif (hptiop_initialize_iop(hba))\r\ngoto free_request_mem;\r\nif (scsi_add_host(host, &pcidev->dev)) {\r\nprintk(KERN_ERR "scsi%d: scsi_add_host failed\n",\r\nhba->host->host_no);\r\ngoto free_request_mem;\r\n}\r\nscsi_scan_host(host);\r\ndprintk("scsi%d: hptiop_probe successfully\n", hba->host->host_no);\r\nreturn 0;\r\nfree_request_mem:\r\ndma_free_coherent(&hba->pcidev->dev,\r\nhba->req_size * hba->max_requests + 0x20,\r\nhba->dma_coherent, hba->dma_coherent_handle);\r\nfree_request_irq:\r\nfree_irq(hba->pcidev->irq, hba);\r\nunmap_pci_bar:\r\nhba->ops->internal_memfree(hba);\r\nhba->ops->unmap_pci_bar(hba);\r\nfree_scsi_host:\r\nscsi_host_put(host);\r\nfree_pci_regions:\r\npci_release_regions(pcidev);\r\ndisable_pci_device:\r\npci_disable_device(pcidev);\r\ndprintk("scsi%d: hptiop_probe fail\n", host ? host->host_no : 0);\r\nreturn -ENODEV;\r\n}\r\nstatic void hptiop_shutdown(struct pci_dev *pcidev)\r\n{\r\nstruct Scsi_Host *host = pci_get_drvdata(pcidev);\r\nstruct hptiop_hba *hba = (struct hptiop_hba *)host->hostdata;\r\ndprintk("hptiop_shutdown(%p)\n", hba);\r\nif (iop_send_sync_msg(hba, IOPMU_INBOUND_MSG0_SHUTDOWN, 60000))\r\nprintk(KERN_ERR "scsi%d: shutdown the iop timeout\n",\r\nhba->host->host_no);\r\nhba->ops->disable_intr(hba);\r\n}\r\nstatic void hptiop_disable_intr_itl(struct hptiop_hba *hba)\r\n{\r\nu32 int_mask;\r\nint_mask = readl(&hba->u.itl.iop->outbound_intmask);\r\nwritel(int_mask |\r\nIOPMU_OUTBOUND_INT_MSG0 | IOPMU_OUTBOUND_INT_POSTQUEUE,\r\n&hba->u.itl.iop->outbound_intmask);\r\nreadl(&hba->u.itl.iop->outbound_intmask);\r\n}\r\nstatic void hptiop_disable_intr_mv(struct hptiop_hba *hba)\r\n{\r\nwritel(0, &hba->u.mv.regs->outbound_intmask);\r\nreadl(&hba->u.mv.regs->outbound_intmask);\r\n}\r\nstatic void hptiop_disable_intr_mvfrey(struct hptiop_hba *hba)\r\n{\r\nwritel(0, &(hba->u.mvfrey.mu->f0_doorbell_enable));\r\nreadl(&(hba->u.mvfrey.mu->f0_doorbell_enable));\r\nwritel(0, &(hba->u.mvfrey.mu->isr_enable));\r\nreadl(&(hba->u.mvfrey.mu->isr_enable));\r\nwritel(0, &(hba->u.mvfrey.mu->pcie_f0_int_enable));\r\nreadl(&(hba->u.mvfrey.mu->pcie_f0_int_enable));\r\n}\r\nstatic void hptiop_remove(struct pci_dev *pcidev)\r\n{\r\nstruct Scsi_Host *host = pci_get_drvdata(pcidev);\r\nstruct hptiop_hba *hba = (struct hptiop_hba *)host->hostdata;\r\ndprintk("scsi%d: hptiop_remove\n", hba->host->host_no);\r\nscsi_remove_host(host);\r\nhptiop_shutdown(pcidev);\r\nfree_irq(hba->pcidev->irq, hba);\r\ndma_free_coherent(&hba->pcidev->dev,\r\nhba->req_size * hba->max_requests + 0x20,\r\nhba->dma_coherent,\r\nhba->dma_coherent_handle);\r\nhba->ops->internal_memfree(hba);\r\nhba->ops->unmap_pci_bar(hba);\r\npci_release_regions(hba->pcidev);\r\npci_set_drvdata(hba->pcidev, NULL);\r\npci_disable_device(hba->pcidev);\r\nscsi_host_put(host);\r\n}\r\nstatic int __init hptiop_module_init(void)\r\n{\r\nprintk(KERN_INFO "%s %s\n", driver_name_long, driver_ver);\r\nreturn pci_register_driver(&hptiop_pci_driver);\r\n}\r\nstatic void __exit hptiop_module_exit(void)\r\n{\r\npci_unregister_driver(&hptiop_pci_driver);\r\n}
