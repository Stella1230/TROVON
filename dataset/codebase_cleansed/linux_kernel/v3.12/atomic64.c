static inline raw_spinlock_t *lock_addr(const atomic64_t *v)\r\n{\r\nunsigned long addr = (unsigned long) v;\r\naddr >>= L1_CACHE_SHIFT;\r\naddr ^= (addr >> 8) ^ (addr >> 16);\r\nreturn &atomic64_lock[addr & (NR_LOCKS - 1)].lock;\r\n}\r\nlong long atomic64_read(const atomic64_t *v)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nlong long val;\r\nraw_spin_lock_irqsave(lock, flags);\r\nval = v->counter;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\nreturn val;\r\n}\r\nvoid atomic64_set(atomic64_t *v, long long i)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nraw_spin_lock_irqsave(lock, flags);\r\nv->counter = i;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\n}\r\nvoid atomic64_add(long long a, atomic64_t *v)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nraw_spin_lock_irqsave(lock, flags);\r\nv->counter += a;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\n}\r\nlong long atomic64_add_return(long long a, atomic64_t *v)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nlong long val;\r\nraw_spin_lock_irqsave(lock, flags);\r\nval = v->counter += a;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\nreturn val;\r\n}\r\nvoid atomic64_sub(long long a, atomic64_t *v)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nraw_spin_lock_irqsave(lock, flags);\r\nv->counter -= a;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\n}\r\nlong long atomic64_sub_return(long long a, atomic64_t *v)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nlong long val;\r\nraw_spin_lock_irqsave(lock, flags);\r\nval = v->counter -= a;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\nreturn val;\r\n}\r\nlong long atomic64_dec_if_positive(atomic64_t *v)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nlong long val;\r\nraw_spin_lock_irqsave(lock, flags);\r\nval = v->counter - 1;\r\nif (val >= 0)\r\nv->counter = val;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\nreturn val;\r\n}\r\nlong long atomic64_cmpxchg(atomic64_t *v, long long o, long long n)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nlong long val;\r\nraw_spin_lock_irqsave(lock, flags);\r\nval = v->counter;\r\nif (val == o)\r\nv->counter = n;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\nreturn val;\r\n}\r\nlong long atomic64_xchg(atomic64_t *v, long long new)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nlong long val;\r\nraw_spin_lock_irqsave(lock, flags);\r\nval = v->counter;\r\nv->counter = new;\r\nraw_spin_unlock_irqrestore(lock, flags);\r\nreturn val;\r\n}\r\nint atomic64_add_unless(atomic64_t *v, long long a, long long u)\r\n{\r\nunsigned long flags;\r\nraw_spinlock_t *lock = lock_addr(v);\r\nint ret = 0;\r\nraw_spin_lock_irqsave(lock, flags);\r\nif (v->counter != u) {\r\nv->counter += a;\r\nret = 1;\r\n}\r\nraw_spin_unlock_irqrestore(lock, flags);\r\nreturn ret;\r\n}
