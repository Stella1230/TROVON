static void __unhash_process(struct task_struct *p, bool group_dead)\r\n{\r\nnr_threads--;\r\ndetach_pid(p, PIDTYPE_PID);\r\nif (group_dead) {\r\ndetach_pid(p, PIDTYPE_PGID);\r\ndetach_pid(p, PIDTYPE_SID);\r\nlist_del_rcu(&p->tasks);\r\nlist_del_init(&p->sibling);\r\n__this_cpu_dec(process_counts);\r\n}\r\nlist_del_rcu(&p->thread_group);\r\n}\r\nstatic void __exit_signal(struct task_struct *tsk)\r\n{\r\nstruct signal_struct *sig = tsk->signal;\r\nbool group_dead = thread_group_leader(tsk);\r\nstruct sighand_struct *sighand;\r\nstruct tty_struct *uninitialized_var(tty);\r\ncputime_t utime, stime;\r\nsighand = rcu_dereference_check(tsk->sighand,\r\nlockdep_tasklist_lock_is_held());\r\nspin_lock(&sighand->siglock);\r\nposix_cpu_timers_exit(tsk);\r\nif (group_dead) {\r\nposix_cpu_timers_exit_group(tsk);\r\ntty = sig->tty;\r\nsig->tty = NULL;\r\n} else {\r\nif (unlikely(has_group_leader_pid(tsk)))\r\nposix_cpu_timers_exit_group(tsk);\r\nif (sig->notify_count > 0 && !--sig->notify_count)\r\nwake_up_process(sig->group_exit_task);\r\nif (tsk == sig->curr_target)\r\nsig->curr_target = next_thread(tsk);\r\ntask_cputime(tsk, &utime, &stime);\r\nsig->utime += utime;\r\nsig->stime += stime;\r\nsig->gtime += task_gtime(tsk);\r\nsig->min_flt += tsk->min_flt;\r\nsig->maj_flt += tsk->maj_flt;\r\nsig->nvcsw += tsk->nvcsw;\r\nsig->nivcsw += tsk->nivcsw;\r\nsig->inblock += task_io_get_inblock(tsk);\r\nsig->oublock += task_io_get_oublock(tsk);\r\ntask_io_accounting_add(&sig->ioac, &tsk->ioac);\r\nsig->sum_sched_runtime += tsk->se.sum_exec_runtime;\r\n}\r\nsig->nr_threads--;\r\n__unhash_process(tsk, group_dead);\r\nflush_sigqueue(&tsk->pending);\r\ntsk->sighand = NULL;\r\nspin_unlock(&sighand->siglock);\r\n__cleanup_sighand(sighand);\r\nclear_tsk_thread_flag(tsk,TIF_SIGPENDING);\r\nif (group_dead) {\r\nflush_sigqueue(&sig->shared_pending);\r\ntty_kref_put(tty);\r\n}\r\n}\r\nstatic void delayed_put_task_struct(struct rcu_head *rhp)\r\n{\r\nstruct task_struct *tsk = container_of(rhp, struct task_struct, rcu);\r\nperf_event_delayed_put(tsk);\r\ntrace_sched_process_free(tsk);\r\nput_task_struct(tsk);\r\n}\r\nvoid release_task(struct task_struct * p)\r\n{\r\nstruct task_struct *leader;\r\nint zap_leader;\r\nrepeat:\r\nrcu_read_lock();\r\natomic_dec(&__task_cred(p)->user->processes);\r\nrcu_read_unlock();\r\nproc_flush_task(p);\r\nwrite_lock_irq(&tasklist_lock);\r\nptrace_release_task(p);\r\n__exit_signal(p);\r\nzap_leader = 0;\r\nleader = p->group_leader;\r\nif (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {\r\nzap_leader = do_notify_parent(leader, leader->exit_signal);\r\nif (zap_leader)\r\nleader->exit_state = EXIT_DEAD;\r\n}\r\nwrite_unlock_irq(&tasklist_lock);\r\nrelease_thread(p);\r\ncall_rcu(&p->rcu, delayed_put_task_struct);\r\np = leader;\r\nif (unlikely(zap_leader))\r\ngoto repeat;\r\n}\r\nstruct pid *session_of_pgrp(struct pid *pgrp)\r\n{\r\nstruct task_struct *p;\r\nstruct pid *sid = NULL;\r\np = pid_task(pgrp, PIDTYPE_PGID);\r\nif (p == NULL)\r\np = pid_task(pgrp, PIDTYPE_PID);\r\nif (p != NULL)\r\nsid = task_session(p);\r\nreturn sid;\r\n}\r\nstatic int will_become_orphaned_pgrp(struct pid *pgrp, struct task_struct *ignored_task)\r\n{\r\nstruct task_struct *p;\r\ndo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\r\nif ((p == ignored_task) ||\r\n(p->exit_state && thread_group_empty(p)) ||\r\nis_global_init(p->real_parent))\r\ncontinue;\r\nif (task_pgrp(p->real_parent) != pgrp &&\r\ntask_session(p->real_parent) == task_session(p))\r\nreturn 0;\r\n} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\r\nreturn 1;\r\n}\r\nint is_current_pgrp_orphaned(void)\r\n{\r\nint retval;\r\nread_lock(&tasklist_lock);\r\nretval = will_become_orphaned_pgrp(task_pgrp(current), NULL);\r\nread_unlock(&tasklist_lock);\r\nreturn retval;\r\n}\r\nstatic bool has_stopped_jobs(struct pid *pgrp)\r\n{\r\nstruct task_struct *p;\r\ndo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\r\nif (p->signal->flags & SIGNAL_STOP_STOPPED)\r\nreturn true;\r\n} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\r\nreturn false;\r\n}\r\nstatic void\r\nkill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)\r\n{\r\nstruct pid *pgrp = task_pgrp(tsk);\r\nstruct task_struct *ignored_task = tsk;\r\nif (!parent)\r\nparent = tsk->real_parent;\r\nelse\r\nignored_task = NULL;\r\nif (task_pgrp(parent) != pgrp &&\r\ntask_session(parent) == task_session(tsk) &&\r\nwill_become_orphaned_pgrp(pgrp, ignored_task) &&\r\nhas_stopped_jobs(pgrp)) {\r\n__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);\r\n__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);\r\n}\r\n}\r\nint allow_signal(int sig)\r\n{\r\nif (!valid_signal(sig) || sig < 1)\r\nreturn -EINVAL;\r\nspin_lock_irq(&current->sighand->siglock);\r\nsigdelset(&current->blocked, sig);\r\ncurrent->sighand->action[(sig)-1].sa.sa_handler = (void __user *)2;\r\nrecalc_sigpending();\r\nspin_unlock_irq(&current->sighand->siglock);\r\nreturn 0;\r\n}\r\nint disallow_signal(int sig)\r\n{\r\nif (!valid_signal(sig) || sig < 1)\r\nreturn -EINVAL;\r\nspin_lock_irq(&current->sighand->siglock);\r\ncurrent->sighand->action[(sig)-1].sa.sa_handler = SIG_IGN;\r\nrecalc_sigpending();\r\nspin_unlock_irq(&current->sighand->siglock);\r\nreturn 0;\r\n}\r\nvoid mm_update_next_owner(struct mm_struct *mm)\r\n{\r\nstruct task_struct *c, *g, *p = current;\r\nretry:\r\nif (mm->owner != p)\r\nreturn;\r\nif (atomic_read(&mm->mm_users) <= 1) {\r\nmm->owner = NULL;\r\nreturn;\r\n}\r\nread_lock(&tasklist_lock);\r\nlist_for_each_entry(c, &p->children, sibling) {\r\nif (c->mm == mm)\r\ngoto assign_new_owner;\r\n}\r\nlist_for_each_entry(c, &p->real_parent->children, sibling) {\r\nif (c->mm == mm)\r\ngoto assign_new_owner;\r\n}\r\ndo_each_thread(g, c) {\r\nif (c->mm == mm)\r\ngoto assign_new_owner;\r\n} while_each_thread(g, c);\r\nread_unlock(&tasklist_lock);\r\nmm->owner = NULL;\r\nreturn;\r\nassign_new_owner:\r\nBUG_ON(c == p);\r\nget_task_struct(c);\r\ntask_lock(c);\r\nread_unlock(&tasklist_lock);\r\nif (c->mm != mm) {\r\ntask_unlock(c);\r\nput_task_struct(c);\r\ngoto retry;\r\n}\r\nmm->owner = c;\r\ntask_unlock(c);\r\nput_task_struct(c);\r\n}\r\nstatic void exit_mm(struct task_struct * tsk)\r\n{\r\nstruct mm_struct *mm = tsk->mm;\r\nstruct core_state *core_state;\r\nmm_release(tsk, mm);\r\nif (!mm)\r\nreturn;\r\nsync_mm_rss(mm);\r\ndown_read(&mm->mmap_sem);\r\ncore_state = mm->core_state;\r\nif (core_state) {\r\nstruct core_thread self;\r\nup_read(&mm->mmap_sem);\r\nself.task = tsk;\r\nself.next = xchg(&core_state->dumper.next, &self);\r\nif (atomic_dec_and_test(&core_state->nr_threads))\r\ncomplete(&core_state->startup);\r\nfor (;;) {\r\nset_task_state(tsk, TASK_UNINTERRUPTIBLE);\r\nif (!self.task)\r\nbreak;\r\nfreezable_schedule();\r\n}\r\n__set_task_state(tsk, TASK_RUNNING);\r\ndown_read(&mm->mmap_sem);\r\n}\r\natomic_inc(&mm->mm_count);\r\nBUG_ON(mm != tsk->active_mm);\r\ntask_lock(tsk);\r\ntsk->mm = NULL;\r\nup_read(&mm->mmap_sem);\r\nenter_lazy_tlb(mm, current);\r\ntask_unlock(tsk);\r\nmm_update_next_owner(mm);\r\nmmput(mm);\r\n}\r\nstatic struct task_struct *find_new_reaper(struct task_struct *father)\r\n__releases(&tasklist_lock\r\nvoid reparent_leader(struct task_struct *father, struct task_struct *p,\r\nstruct list_head *dead)\r\n{\r\nlist_move_tail(&p->sibling, &p->real_parent->children);\r\nif (p->exit_state == EXIT_DEAD)\r\nreturn;\r\nif (same_thread_group(p->real_parent, father))\r\nreturn;\r\np->exit_signal = SIGCHLD;\r\nif (!p->ptrace &&\r\np->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {\r\nif (do_notify_parent(p, p->exit_signal)) {\r\np->exit_state = EXIT_DEAD;\r\nlist_move_tail(&p->sibling, dead);\r\n}\r\n}\r\nkill_orphaned_pgrp(p, father);\r\n}\r\nstatic void forget_original_parent(struct task_struct *father)\r\n{\r\nstruct task_struct *p, *n, *reaper;\r\nLIST_HEAD(dead_children);\r\nwrite_lock_irq(&tasklist_lock);\r\nexit_ptrace(father);\r\nreaper = find_new_reaper(father);\r\nlist_for_each_entry_safe(p, n, &father->children, sibling) {\r\nstruct task_struct *t = p;\r\ndo {\r\nt->real_parent = reaper;\r\nif (t->parent == father) {\r\nBUG_ON(t->ptrace);\r\nt->parent = t->real_parent;\r\n}\r\nif (t->pdeath_signal)\r\ngroup_send_sig_info(t->pdeath_signal,\r\nSEND_SIG_NOINFO, t);\r\n} while_each_thread(p, t);\r\nreparent_leader(father, p, &dead_children);\r\n}\r\nvoid exit_notify(struct task_struct *tsk, int group_dead)\r\n{\r\nbool autoreap;\r\nforget_original_parent(tsk);\r\nwrite_lock_irq(&tasklist_lock);\r\nif (group_dead)\r\nkill_orphaned_pgrp(tsk->group_leader, NULL);\r\nif (unlikely(tsk->ptrace)) {\r\nint sig = thread_group_leader(tsk) &&\r\nthread_group_empty(tsk) &&\r\n!ptrace_reparented(tsk) ?\r\ntsk->exit_signal : SIGCHLD;\r\nautoreap = do_notify_parent(tsk, sig);\r\n} else if (thread_group_leader(tsk)) {\r\nautoreap = thread_group_empty(tsk) &&\r\ndo_notify_parent(tsk, tsk->exit_signal);\r\n} else {\r\nautoreap = true;\r\n}\r\ntsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;\r\nif (unlikely(tsk->signal->notify_count < 0))\r\nwake_up_process(tsk->signal->group_exit_task);\r\nwrite_unlock_irq(&tasklist_lock);\r\nif (autoreap)\r\nrelease_task(tsk);\r\n}\r\nstatic void check_stack_usage(void)\r\n{\r\nstatic DEFINE_SPINLOCK(low_water_lock);\r\nstatic int lowest_to_date = THREAD_SIZE;\r\nunsigned long free;\r\nfree = stack_not_used(current);\r\nif (free >= lowest_to_date)\r\nreturn;\r\nspin_lock(&low_water_lock);\r\nif (free < lowest_to_date) {\r\nprintk(KERN_WARNING "%s (%d) used greatest stack depth: "\r\n"%lu bytes left\n",\r\ncurrent->comm, task_pid_nr(current), free);\r\nlowest_to_date = free;\r\n}\r\nspin_unlock(&low_water_lock);\r\n}\r\nstatic inline void check_stack_usage(void) {}\r\nvoid do_exit(long code)\r\n{\r\nstruct task_struct *tsk = current;\r\nint group_dead;\r\nprofile_task_exit(tsk);\r\nWARN_ON(blk_needs_flush_plug(tsk));\r\nif (unlikely(in_interrupt()))\r\npanic("Aiee, killing interrupt handler!");\r\nif (unlikely(!tsk->pid))\r\npanic("Attempted to kill the idle task!");\r\nset_fs(USER_DS);\r\nptrace_event(PTRACE_EVENT_EXIT, code);\r\nvalidate_creds_for_do_exit(tsk);\r\nif (unlikely(tsk->flags & PF_EXITING)) {\r\nprintk(KERN_ALERT\r\n"Fixing recursive fault but reboot is needed!\n");\r\ntsk->flags |= PF_EXITPIDONE;\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nschedule();\r\n}\r\nexit_signals(tsk);\r\nsmp_mb();\r\nraw_spin_unlock_wait(&tsk->pi_lock);\r\nif (unlikely(in_atomic()))\r\nprintk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",\r\ncurrent->comm, task_pid_nr(current),\r\npreempt_count());\r\nacct_update_integrals(tsk);\r\nif (tsk->mm)\r\nsync_mm_rss(tsk->mm);\r\ngroup_dead = atomic_dec_and_test(&tsk->signal->live);\r\nif (group_dead) {\r\nhrtimer_cancel(&tsk->signal->real_timer);\r\nexit_itimers(tsk->signal);\r\nif (tsk->mm)\r\nsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\r\n}\r\nacct_collect(code, group_dead);\r\nif (group_dead)\r\ntty_audit_exit();\r\naudit_free(tsk);\r\ntsk->exit_code = code;\r\ntaskstats_exit(tsk, group_dead);\r\nexit_mm(tsk);\r\nif (group_dead)\r\nacct_process();\r\ntrace_sched_process_exit(tsk);\r\nexit_sem(tsk);\r\nexit_shm(tsk);\r\nexit_files(tsk);\r\nexit_fs(tsk);\r\nexit_task_namespaces(tsk);\r\nexit_task_work(tsk);\r\ncheck_stack_usage();\r\nexit_thread();\r\nperf_event_exit_task(tsk);\r\ncgroup_exit(tsk, 1);\r\nif (group_dead)\r\ndisassociate_ctty(1);\r\nmodule_put(task_thread_info(tsk)->exec_domain->module);\r\nproc_exit_connector(tsk);\r\nflush_ptrace_hw_breakpoint(tsk);\r\nexit_notify(tsk, group_dead);\r\n#ifdef CONFIG_NUMA\r\ntask_lock(tsk);\r\nmpol_put(tsk->mempolicy);\r\ntsk->mempolicy = NULL;\r\ntask_unlock(tsk);\r\n#endif\r\n#ifdef CONFIG_FUTEX\r\nif (unlikely(current->pi_state_cache))\r\nkfree(current->pi_state_cache);\r\n#endif\r\ndebug_check_no_locks_held();\r\ntsk->flags |= PF_EXITPIDONE;\r\nif (tsk->io_context)\r\nexit_io_context(tsk);\r\nif (tsk->splice_pipe)\r\nfree_pipe_info(tsk->splice_pipe);\r\nif (tsk->task_frag.page)\r\nput_page(tsk->task_frag.page);\r\nvalidate_creds_for_do_exit(tsk);\r\npreempt_disable();\r\nif (tsk->nr_dirtied)\r\n__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);\r\nexit_rcu();\r\nsmp_mb();\r\nraw_spin_unlock_wait(&tsk->pi_lock);\r\ntsk->state = TASK_DEAD;\r\ntsk->flags |= PF_NOFREEZE;\r\nschedule();\r\nBUG();\r\nfor (;;)\r\ncpu_relax();\r\n}\r\nvoid complete_and_exit(struct completion *comp, long code)\r\n{\r\nif (comp)\r\ncomplete(comp);\r\ndo_exit(code);\r\n}\r\nvoid\r\ndo_group_exit(int exit_code)\r\n{\r\nstruct signal_struct *sig = current->signal;\r\nBUG_ON(exit_code & 0x80);\r\nif (signal_group_exit(sig))\r\nexit_code = sig->group_exit_code;\r\nelse if (!thread_group_empty(current)) {\r\nstruct sighand_struct *const sighand = current->sighand;\r\nspin_lock_irq(&sighand->siglock);\r\nif (signal_group_exit(sig))\r\nexit_code = sig->group_exit_code;\r\nelse {\r\nsig->group_exit_code = exit_code;\r\nsig->flags = SIGNAL_GROUP_EXIT;\r\nzap_other_threads(current);\r\n}\r\nspin_unlock_irq(&sighand->siglock);\r\n}\r\ndo_exit(exit_code);\r\n}\r\nstatic inline\r\nstruct pid *task_pid_type(struct task_struct *task, enum pid_type type)\r\n{\r\nif (type != PIDTYPE_PID)\r\ntask = task->group_leader;\r\nreturn task->pids[type].pid;\r\n}\r\nstatic int eligible_pid(struct wait_opts *wo, struct task_struct *p)\r\n{\r\nreturn wo->wo_type == PIDTYPE_MAX ||\r\ntask_pid_type(p, wo->wo_type) == wo->wo_pid;\r\n}\r\nstatic int eligible_child(struct wait_opts *wo, struct task_struct *p)\r\n{\r\nif (!eligible_pid(wo, p))\r\nreturn 0;\r\nif (((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))\r\n&& !(wo->wo_flags & __WALL))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,\r\npid_t pid, uid_t uid, int why, int status)\r\n{\r\nstruct siginfo __user *infop;\r\nint retval = wo->wo_rusage\r\n? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;\r\nput_task_struct(p);\r\ninfop = wo->wo_info;\r\nif (infop) {\r\nif (!retval)\r\nretval = put_user(SIGCHLD, &infop->si_signo);\r\nif (!retval)\r\nretval = put_user(0, &infop->si_errno);\r\nif (!retval)\r\nretval = put_user((short)why, &infop->si_code);\r\nif (!retval)\r\nretval = put_user(pid, &infop->si_pid);\r\nif (!retval)\r\nretval = put_user(uid, &infop->si_uid);\r\nif (!retval)\r\nretval = put_user(status, &infop->si_status);\r\n}\r\nif (!retval)\r\nretval = pid;\r\nreturn retval;\r\n}\r\nstatic int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)\r\n{\r\nunsigned long state;\r\nint retval, status, traced;\r\npid_t pid = task_pid_vnr(p);\r\nuid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));\r\nstruct siginfo __user *infop;\r\nif (!likely(wo->wo_flags & WEXITED))\r\nreturn 0;\r\nif (unlikely(wo->wo_flags & WNOWAIT)) {\r\nint exit_code = p->exit_code;\r\nint why;\r\nget_task_struct(p);\r\nread_unlock(&tasklist_lock);\r\nif ((exit_code & 0x7f) == 0) {\r\nwhy = CLD_EXITED;\r\nstatus = exit_code >> 8;\r\n} else {\r\nwhy = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;\r\nstatus = exit_code & 0x7f;\r\n}\r\nreturn wait_noreap_copyout(wo, p, pid, uid, why, status);\r\n}\r\nstate = xchg(&p->exit_state, EXIT_DEAD);\r\nif (state != EXIT_ZOMBIE) {\r\nBUG_ON(state != EXIT_DEAD);\r\nreturn 0;\r\n}\r\ntraced = ptrace_reparented(p);\r\nif (likely(!traced) && thread_group_leader(p)) {\r\nstruct signal_struct *psig;\r\nstruct signal_struct *sig;\r\nunsigned long maxrss;\r\ncputime_t tgutime, tgstime;\r\nthread_group_cputime_adjusted(p, &tgutime, &tgstime);\r\nspin_lock_irq(&p->real_parent->sighand->siglock);\r\npsig = p->real_parent->signal;\r\nsig = p->signal;\r\npsig->cutime += tgutime + sig->cutime;\r\npsig->cstime += tgstime + sig->cstime;\r\npsig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;\r\npsig->cmin_flt +=\r\np->min_flt + sig->min_flt + sig->cmin_flt;\r\npsig->cmaj_flt +=\r\np->maj_flt + sig->maj_flt + sig->cmaj_flt;\r\npsig->cnvcsw +=\r\np->nvcsw + sig->nvcsw + sig->cnvcsw;\r\npsig->cnivcsw +=\r\np->nivcsw + sig->nivcsw + sig->cnivcsw;\r\npsig->cinblock +=\r\ntask_io_get_inblock(p) +\r\nsig->inblock + sig->cinblock;\r\npsig->coublock +=\r\ntask_io_get_oublock(p) +\r\nsig->oublock + sig->coublock;\r\nmaxrss = max(sig->maxrss, sig->cmaxrss);\r\nif (psig->cmaxrss < maxrss)\r\npsig->cmaxrss = maxrss;\r\ntask_io_accounting_add(&psig->ioac, &p->ioac);\r\ntask_io_accounting_add(&psig->ioac, &sig->ioac);\r\nspin_unlock_irq(&p->real_parent->sighand->siglock);\r\n}\r\nread_unlock(&tasklist_lock);\r\nretval = wo->wo_rusage\r\n? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;\r\nstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)\r\n? p->signal->group_exit_code : p->exit_code;\r\nif (!retval && wo->wo_stat)\r\nretval = put_user(status, wo->wo_stat);\r\ninfop = wo->wo_info;\r\nif (!retval && infop)\r\nretval = put_user(SIGCHLD, &infop->si_signo);\r\nif (!retval && infop)\r\nretval = put_user(0, &infop->si_errno);\r\nif (!retval && infop) {\r\nint why;\r\nif ((status & 0x7f) == 0) {\r\nwhy = CLD_EXITED;\r\nstatus >>= 8;\r\n} else {\r\nwhy = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;\r\nstatus &= 0x7f;\r\n}\r\nretval = put_user((short)why, &infop->si_code);\r\nif (!retval)\r\nretval = put_user(status, &infop->si_status);\r\n}\r\nif (!retval && infop)\r\nretval = put_user(pid, &infop->si_pid);\r\nif (!retval && infop)\r\nretval = put_user(uid, &infop->si_uid);\r\nif (!retval)\r\nretval = pid;\r\nif (traced) {\r\nwrite_lock_irq(&tasklist_lock);\r\nptrace_unlink(p);\r\nif (thread_group_leader(p) &&\r\n!do_notify_parent(p, p->exit_signal)) {\r\np->exit_state = EXIT_ZOMBIE;\r\np = NULL;\r\n}\r\nwrite_unlock_irq(&tasklist_lock);\r\n}\r\nif (p != NULL)\r\nrelease_task(p);\r\nreturn retval;\r\n}\r\nstatic int *task_stopped_code(struct task_struct *p, bool ptrace)\r\n{\r\nif (ptrace) {\r\nif (task_is_stopped_or_traced(p) &&\r\n!(p->jobctl & JOBCTL_LISTENING))\r\nreturn &p->exit_code;\r\n} else {\r\nif (p->signal->flags & SIGNAL_STOP_STOPPED)\r\nreturn &p->signal->group_exit_code;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int wait_task_stopped(struct wait_opts *wo,\r\nint ptrace, struct task_struct *p)\r\n{\r\nstruct siginfo __user *infop;\r\nint retval, exit_code, *p_code, why;\r\nuid_t uid = 0;\r\npid_t pid;\r\nif (!ptrace && !(wo->wo_flags & WUNTRACED))\r\nreturn 0;\r\nif (!task_stopped_code(p, ptrace))\r\nreturn 0;\r\nexit_code = 0;\r\nspin_lock_irq(&p->sighand->siglock);\r\np_code = task_stopped_code(p, ptrace);\r\nif (unlikely(!p_code))\r\ngoto unlock_sig;\r\nexit_code = *p_code;\r\nif (!exit_code)\r\ngoto unlock_sig;\r\nif (!unlikely(wo->wo_flags & WNOWAIT))\r\n*p_code = 0;\r\nuid = from_kuid_munged(current_user_ns(), task_uid(p));\r\nunlock_sig:\r\nspin_unlock_irq(&p->sighand->siglock);\r\nif (!exit_code)\r\nreturn 0;\r\nget_task_struct(p);\r\npid = task_pid_vnr(p);\r\nwhy = ptrace ? CLD_TRAPPED : CLD_STOPPED;\r\nread_unlock(&tasklist_lock);\r\nif (unlikely(wo->wo_flags & WNOWAIT))\r\nreturn wait_noreap_copyout(wo, p, pid, uid, why, exit_code);\r\nretval = wo->wo_rusage\r\n? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;\r\nif (!retval && wo->wo_stat)\r\nretval = put_user((exit_code << 8) | 0x7f, wo->wo_stat);\r\ninfop = wo->wo_info;\r\nif (!retval && infop)\r\nretval = put_user(SIGCHLD, &infop->si_signo);\r\nif (!retval && infop)\r\nretval = put_user(0, &infop->si_errno);\r\nif (!retval && infop)\r\nretval = put_user((short)why, &infop->si_code);\r\nif (!retval && infop)\r\nretval = put_user(exit_code, &infop->si_status);\r\nif (!retval && infop)\r\nretval = put_user(pid, &infop->si_pid);\r\nif (!retval && infop)\r\nretval = put_user(uid, &infop->si_uid);\r\nif (!retval)\r\nretval = pid;\r\nput_task_struct(p);\r\nBUG_ON(!retval);\r\nreturn retval;\r\n}\r\nstatic int wait_task_continued(struct wait_opts *wo, struct task_struct *p)\r\n{\r\nint retval;\r\npid_t pid;\r\nuid_t uid;\r\nif (!unlikely(wo->wo_flags & WCONTINUED))\r\nreturn 0;\r\nif (!(p->signal->flags & SIGNAL_STOP_CONTINUED))\r\nreturn 0;\r\nspin_lock_irq(&p->sighand->siglock);\r\nif (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {\r\nspin_unlock_irq(&p->sighand->siglock);\r\nreturn 0;\r\n}\r\nif (!unlikely(wo->wo_flags & WNOWAIT))\r\np->signal->flags &= ~SIGNAL_STOP_CONTINUED;\r\nuid = from_kuid_munged(current_user_ns(), task_uid(p));\r\nspin_unlock_irq(&p->sighand->siglock);\r\npid = task_pid_vnr(p);\r\nget_task_struct(p);\r\nread_unlock(&tasklist_lock);\r\nif (!wo->wo_info) {\r\nretval = wo->wo_rusage\r\n? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;\r\nput_task_struct(p);\r\nif (!retval && wo->wo_stat)\r\nretval = put_user(0xffff, wo->wo_stat);\r\nif (!retval)\r\nretval = pid;\r\n} else {\r\nretval = wait_noreap_copyout(wo, p, pid, uid,\r\nCLD_CONTINUED, SIGCONT);\r\nBUG_ON(retval == 0);\r\n}\r\nreturn retval;\r\n}\r\nstatic int wait_consider_task(struct wait_opts *wo, int ptrace,\r\nstruct task_struct *p)\r\n{\r\nint ret = eligible_child(wo, p);\r\nif (!ret)\r\nreturn ret;\r\nret = security_task_wait(p);\r\nif (unlikely(ret < 0)) {\r\nif (wo->notask_error)\r\nwo->notask_error = ret;\r\nreturn 0;\r\n}\r\nif (unlikely(p->exit_state == EXIT_DEAD)) {\r\nif (likely(!ptrace) && unlikely(ptrace_reparented(p)))\r\nwo->notask_error = 0;\r\nreturn 0;\r\n}\r\nif (p->exit_state == EXIT_ZOMBIE) {\r\nif (likely(!ptrace) && unlikely(p->ptrace)) {\r\nwo->notask_error = 0;\r\nreturn 0;\r\n}\r\nif (!delay_group_leader(p))\r\nreturn wait_task_zombie(wo, p);\r\nif (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))\r\nwo->notask_error = 0;\r\n} else {\r\nif (likely(!ptrace) && p->ptrace && !ptrace_reparented(p))\r\nreturn 0;\r\nwo->notask_error = 0;\r\n}\r\nret = wait_task_stopped(wo, ptrace, p);\r\nif (ret)\r\nreturn ret;\r\nreturn wait_task_continued(wo, p);\r\n}\r\nstatic int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)\r\n{\r\nstruct task_struct *p;\r\nlist_for_each_entry(p, &tsk->children, sibling) {\r\nint ret = wait_consider_task(wo, 0, p);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)\r\n{\r\nstruct task_struct *p;\r\nlist_for_each_entry(p, &tsk->ptraced, ptrace_entry) {\r\nint ret = wait_consider_task(wo, 1, p);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int child_wait_callback(wait_queue_t *wait, unsigned mode,\r\nint sync, void *key)\r\n{\r\nstruct wait_opts *wo = container_of(wait, struct wait_opts,\r\nchild_wait);\r\nstruct task_struct *p = key;\r\nif (!eligible_pid(wo, p))\r\nreturn 0;\r\nif ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)\r\nreturn 0;\r\nreturn default_wake_function(wait, mode, sync, key);\r\n}\r\nvoid __wake_up_parent(struct task_struct *p, struct task_struct *parent)\r\n{\r\n__wake_up_sync_key(&parent->signal->wait_chldexit,\r\nTASK_INTERRUPTIBLE, 1, p);\r\n}\r\nstatic long do_wait(struct wait_opts *wo)\r\n{\r\nstruct task_struct *tsk;\r\nint retval;\r\ntrace_sched_process_wait(wo->wo_pid);\r\ninit_waitqueue_func_entry(&wo->child_wait, child_wait_callback);\r\nwo->child_wait.private = current;\r\nadd_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\r\nrepeat:\r\nwo->notask_error = -ECHILD;\r\nif ((wo->wo_type < PIDTYPE_MAX) &&\r\n(!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))\r\ngoto notask;\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nread_lock(&tasklist_lock);\r\ntsk = current;\r\ndo {\r\nretval = do_wait_thread(wo, tsk);\r\nif (retval)\r\ngoto end;\r\nretval = ptrace_do_wait(wo, tsk);\r\nif (retval)\r\ngoto end;\r\nif (wo->wo_flags & __WNOTHREAD)\r\nbreak;\r\n} while_each_thread(current, tsk);
