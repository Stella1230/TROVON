static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)\r\n{\r\nstruct rt_bandwidth *rt_b =\r\ncontainer_of(timer, struct rt_bandwidth, rt_period_timer);\r\nktime_t now;\r\nint overrun;\r\nint idle = 0;\r\nfor (;;) {\r\nnow = hrtimer_cb_get_time(timer);\r\noverrun = hrtimer_forward(timer, now, rt_b->rt_period);\r\nif (!overrun)\r\nbreak;\r\nidle = do_sched_rt_period_timer(rt_b, overrun);\r\n}\r\nreturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\r\n}\r\nvoid init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)\r\n{\r\nrt_b->rt_period = ns_to_ktime(period);\r\nrt_b->rt_runtime = runtime;\r\nraw_spin_lock_init(&rt_b->rt_runtime_lock);\r\nhrtimer_init(&rt_b->rt_period_timer,\r\nCLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nrt_b->rt_period_timer.function = sched_rt_period_timer;\r\n}\r\nstatic void start_rt_bandwidth(struct rt_bandwidth *rt_b)\r\n{\r\nif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)\r\nreturn;\r\nif (hrtimer_active(&rt_b->rt_period_timer))\r\nreturn;\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nstart_bandwidth_timer(&rt_b->rt_period_timer, rt_b->rt_period);\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\n}\r\nvoid init_rt_rq(struct rt_rq *rt_rq, struct rq *rq)\r\n{\r\nstruct rt_prio_array *array;\r\nint i;\r\narray = &rt_rq->active;\r\nfor (i = 0; i < MAX_RT_PRIO; i++) {\r\nINIT_LIST_HEAD(array->queue + i);\r\n__clear_bit(i, array->bitmap);\r\n}\r\n__set_bit(MAX_RT_PRIO, array->bitmap);\r\n#if defined CONFIG_SMP\r\nrt_rq->highest_prio.curr = MAX_RT_PRIO;\r\nrt_rq->highest_prio.next = MAX_RT_PRIO;\r\nrt_rq->rt_nr_migratory = 0;\r\nrt_rq->overloaded = 0;\r\nplist_head_init(&rt_rq->pushable_tasks);\r\n#endif\r\nrt_rq->rt_time = 0;\r\nrt_rq->rt_throttled = 0;\r\nrt_rq->rt_runtime = 0;\r\nraw_spin_lock_init(&rt_rq->rt_runtime_lock);\r\n}\r\nstatic void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)\r\n{\r\nhrtimer_cancel(&rt_b->rt_period_timer);\r\n}\r\nstatic inline struct task_struct *rt_task_of(struct sched_rt_entity *rt_se)\r\n{\r\n#ifdef CONFIG_SCHED_DEBUG\r\nWARN_ON_ONCE(!rt_entity_is_task(rt_se));\r\n#endif\r\nreturn container_of(rt_se, struct task_struct, rt);\r\n}\r\nstatic inline struct rq *rq_of_rt_rq(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rq;\r\n}\r\nstatic inline struct rt_rq *rt_rq_of_se(struct sched_rt_entity *rt_se)\r\n{\r\nreturn rt_se->rt_rq;\r\n}\r\nvoid free_rt_sched_group(struct task_group *tg)\r\n{\r\nint i;\r\nif (tg->rt_se)\r\ndestroy_rt_bandwidth(&tg->rt_bandwidth);\r\nfor_each_possible_cpu(i) {\r\nif (tg->rt_rq)\r\nkfree(tg->rt_rq[i]);\r\nif (tg->rt_se)\r\nkfree(tg->rt_se[i]);\r\n}\r\nkfree(tg->rt_rq);\r\nkfree(tg->rt_se);\r\n}\r\nvoid init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\r\nstruct sched_rt_entity *rt_se, int cpu,\r\nstruct sched_rt_entity *parent)\r\n{\r\nstruct rq *rq = cpu_rq(cpu);\r\nrt_rq->highest_prio.curr = MAX_RT_PRIO;\r\nrt_rq->rt_nr_boosted = 0;\r\nrt_rq->rq = rq;\r\nrt_rq->tg = tg;\r\ntg->rt_rq[cpu] = rt_rq;\r\ntg->rt_se[cpu] = rt_se;\r\nif (!rt_se)\r\nreturn;\r\nif (!parent)\r\nrt_se->rt_rq = &rq->rt;\r\nelse\r\nrt_se->rt_rq = parent->my_q;\r\nrt_se->my_q = rt_rq;\r\nrt_se->parent = parent;\r\nINIT_LIST_HEAD(&rt_se->run_list);\r\n}\r\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\r\n{\r\nstruct rt_rq *rt_rq;\r\nstruct sched_rt_entity *rt_se;\r\nint i;\r\ntg->rt_rq = kzalloc(sizeof(rt_rq) * nr_cpu_ids, GFP_KERNEL);\r\nif (!tg->rt_rq)\r\ngoto err;\r\ntg->rt_se = kzalloc(sizeof(rt_se) * nr_cpu_ids, GFP_KERNEL);\r\nif (!tg->rt_se)\r\ngoto err;\r\ninit_rt_bandwidth(&tg->rt_bandwidth,\r\nktime_to_ns(def_rt_bandwidth.rt_period), 0);\r\nfor_each_possible_cpu(i) {\r\nrt_rq = kzalloc_node(sizeof(struct rt_rq),\r\nGFP_KERNEL, cpu_to_node(i));\r\nif (!rt_rq)\r\ngoto err;\r\nrt_se = kzalloc_node(sizeof(struct sched_rt_entity),\r\nGFP_KERNEL, cpu_to_node(i));\r\nif (!rt_se)\r\ngoto err_free_rq;\r\ninit_rt_rq(rt_rq, cpu_rq(i));\r\nrt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;\r\ninit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);\r\n}\r\nreturn 1;\r\nerr_free_rq:\r\nkfree(rt_rq);\r\nerr:\r\nreturn 0;\r\n}\r\nstatic inline struct task_struct *rt_task_of(struct sched_rt_entity *rt_se)\r\n{\r\nreturn container_of(rt_se, struct task_struct, rt);\r\n}\r\nstatic inline struct rq *rq_of_rt_rq(struct rt_rq *rt_rq)\r\n{\r\nreturn container_of(rt_rq, struct rq, rt);\r\n}\r\nstatic inline struct rt_rq *rt_rq_of_se(struct sched_rt_entity *rt_se)\r\n{\r\nstruct task_struct *p = rt_task_of(rt_se);\r\nstruct rq *rq = task_rq(p);\r\nreturn &rq->rt;\r\n}\r\nvoid free_rt_sched_group(struct task_group *tg) { }\r\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\r\n{\r\nreturn 1;\r\n}\r\nstatic inline int rt_overloaded(struct rq *rq)\r\n{\r\nreturn atomic_read(&rq->rd->rto_count);\r\n}\r\nstatic inline void rt_set_overload(struct rq *rq)\r\n{\r\nif (!rq->online)\r\nreturn;\r\ncpumask_set_cpu(rq->cpu, rq->rd->rto_mask);\r\nwmb();\r\natomic_inc(&rq->rd->rto_count);\r\n}\r\nstatic inline void rt_clear_overload(struct rq *rq)\r\n{\r\nif (!rq->online)\r\nreturn;\r\natomic_dec(&rq->rd->rto_count);\r\ncpumask_clear_cpu(rq->cpu, rq->rd->rto_mask);\r\n}\r\nstatic void update_rt_migration(struct rt_rq *rt_rq)\r\n{\r\nif (rt_rq->rt_nr_migratory && rt_rq->rt_nr_total > 1) {\r\nif (!rt_rq->overloaded) {\r\nrt_set_overload(rq_of_rt_rq(rt_rq));\r\nrt_rq->overloaded = 1;\r\n}\r\n} else if (rt_rq->overloaded) {\r\nrt_clear_overload(rq_of_rt_rq(rt_rq));\r\nrt_rq->overloaded = 0;\r\n}\r\n}\r\nstatic void inc_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nstruct task_struct *p;\r\nif (!rt_entity_is_task(rt_se))\r\nreturn;\r\np = rt_task_of(rt_se);\r\nrt_rq = &rq_of_rt_rq(rt_rq)->rt;\r\nrt_rq->rt_nr_total++;\r\nif (p->nr_cpus_allowed > 1)\r\nrt_rq->rt_nr_migratory++;\r\nupdate_rt_migration(rt_rq);\r\n}\r\nstatic void dec_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nstruct task_struct *p;\r\nif (!rt_entity_is_task(rt_se))\r\nreturn;\r\np = rt_task_of(rt_se);\r\nrt_rq = &rq_of_rt_rq(rt_rq)->rt;\r\nrt_rq->rt_nr_total--;\r\nif (p->nr_cpus_allowed > 1)\r\nrt_rq->rt_nr_migratory--;\r\nupdate_rt_migration(rt_rq);\r\n}\r\nstatic inline int has_pushable_tasks(struct rq *rq)\r\n{\r\nreturn !plist_head_empty(&rq->rt.pushable_tasks);\r\n}\r\nstatic void enqueue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\nplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);\r\nplist_node_init(&p->pushable_tasks, p->prio);\r\nplist_add(&p->pushable_tasks, &rq->rt.pushable_tasks);\r\nif (p->prio < rq->rt.highest_prio.next)\r\nrq->rt.highest_prio.next = p->prio;\r\n}\r\nstatic void dequeue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\nplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);\r\nif (has_pushable_tasks(rq)) {\r\np = plist_first_entry(&rq->rt.pushable_tasks,\r\nstruct task_struct, pushable_tasks);\r\nrq->rt.highest_prio.next = p->prio;\r\n} else\r\nrq->rt.highest_prio.next = MAX_RT_PRIO;\r\n}\r\nstatic inline void enqueue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\n}\r\nstatic inline void dequeue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\n}\r\nstatic inline\r\nvoid inc_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\n}\r\nstatic inline\r\nvoid dec_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\n}\r\nstatic inline int on_rt_rq(struct sched_rt_entity *rt_se)\r\n{\r\nreturn !list_empty(&rt_se->run_list);\r\n}\r\nstatic inline u64 sched_rt_runtime(struct rt_rq *rt_rq)\r\n{\r\nif (!rt_rq->tg)\r\nreturn RUNTIME_INF;\r\nreturn rt_rq->rt_runtime;\r\n}\r\nstatic inline u64 sched_rt_period(struct rt_rq *rt_rq)\r\n{\r\nreturn ktime_to_ns(rt_rq->tg->rt_bandwidth.rt_period);\r\n}\r\nstatic inline struct task_group *next_task_group(struct task_group *tg)\r\n{\r\ndo {\r\ntg = list_entry_rcu(tg->list.next,\r\ntypeof(struct task_group), list);\r\n} while (&tg->list != &task_groups && task_group_is_autogroup(tg));\r\nif (&tg->list == &task_groups)\r\ntg = NULL;\r\nreturn tg;\r\n}\r\nstatic inline struct rt_rq *group_rt_rq(struct sched_rt_entity *rt_se)\r\n{\r\nreturn rt_se->my_q;\r\n}\r\nstatic void sched_rt_rq_enqueue(struct rt_rq *rt_rq)\r\n{\r\nstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;\r\nstruct sched_rt_entity *rt_se;\r\nint cpu = cpu_of(rq_of_rt_rq(rt_rq));\r\nrt_se = rt_rq->tg->rt_se[cpu];\r\nif (rt_rq->rt_nr_running) {\r\nif (rt_se && !on_rt_rq(rt_se))\r\nenqueue_rt_entity(rt_se, false);\r\nif (rt_rq->highest_prio.curr < curr->prio)\r\nresched_task(curr);\r\n}\r\n}\r\nstatic void sched_rt_rq_dequeue(struct rt_rq *rt_rq)\r\n{\r\nstruct sched_rt_entity *rt_se;\r\nint cpu = cpu_of(rq_of_rt_rq(rt_rq));\r\nrt_se = rt_rq->tg->rt_se[cpu];\r\nif (rt_se && on_rt_rq(rt_se))\r\ndequeue_rt_entity(rt_se);\r\n}\r\nstatic inline int rt_rq_throttled(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;\r\n}\r\nstatic int rt_se_boosted(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rt_rq *rt_rq = group_rt_rq(rt_se);\r\nstruct task_struct *p;\r\nif (rt_rq)\r\nreturn !!rt_rq->rt_nr_boosted;\r\np = rt_task_of(rt_se);\r\nreturn p->prio != p->normal_prio;\r\n}\r\nstatic inline const struct cpumask *sched_rt_period_mask(void)\r\n{\r\nreturn this_rq()->rd->span;\r\n}\r\nstatic inline const struct cpumask *sched_rt_period_mask(void)\r\n{\r\nreturn cpu_online_mask;\r\n}\r\nstatic inline\r\nstruct rt_rq *sched_rt_period_rt_rq(struct rt_bandwidth *rt_b, int cpu)\r\n{\r\nreturn container_of(rt_b, struct task_group, rt_bandwidth)->rt_rq[cpu];\r\n}\r\nstatic inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)\r\n{\r\nreturn &rt_rq->tg->rt_bandwidth;\r\n}\r\nstatic inline u64 sched_rt_runtime(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rt_runtime;\r\n}\r\nstatic inline u64 sched_rt_period(struct rt_rq *rt_rq)\r\n{\r\nreturn ktime_to_ns(def_rt_bandwidth.rt_period);\r\n}\r\nstatic inline struct rt_rq *group_rt_rq(struct sched_rt_entity *rt_se)\r\n{\r\nreturn NULL;\r\n}\r\nstatic inline void sched_rt_rq_enqueue(struct rt_rq *rt_rq)\r\n{\r\nif (rt_rq->rt_nr_running)\r\nresched_task(rq_of_rt_rq(rt_rq)->curr);\r\n}\r\nstatic inline void sched_rt_rq_dequeue(struct rt_rq *rt_rq)\r\n{\r\n}\r\nstatic inline int rt_rq_throttled(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rt_throttled;\r\n}\r\nstatic inline const struct cpumask *sched_rt_period_mask(void)\r\n{\r\nreturn cpu_online_mask;\r\n}\r\nstatic inline\r\nstruct rt_rq *sched_rt_period_rt_rq(struct rt_bandwidth *rt_b, int cpu)\r\n{\r\nreturn &cpu_rq(cpu)->rt;\r\n}\r\nstatic inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)\r\n{\r\nreturn &def_rt_bandwidth;\r\n}\r\nstatic int do_balance_runtime(struct rt_rq *rt_rq)\r\n{\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\nstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;\r\nint i, weight, more = 0;\r\nu64 rt_period;\r\nweight = cpumask_weight(rd->span);\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nrt_period = ktime_to_ns(rt_b->rt_period);\r\nfor_each_cpu(i, rd->span) {\r\nstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);\r\ns64 diff;\r\nif (iter == rt_rq)\r\ncontinue;\r\nraw_spin_lock(&iter->rt_runtime_lock);\r\nif (iter->rt_runtime == RUNTIME_INF)\r\ngoto next;\r\ndiff = iter->rt_runtime - iter->rt_time;\r\nif (diff > 0) {\r\ndiff = div_u64((u64)diff, weight);\r\nif (rt_rq->rt_runtime + diff > rt_period)\r\ndiff = rt_period - rt_rq->rt_runtime;\r\niter->rt_runtime -= diff;\r\nrt_rq->rt_runtime += diff;\r\nmore = 1;\r\nif (rt_rq->rt_runtime == rt_period) {\r\nraw_spin_unlock(&iter->rt_runtime_lock);\r\nbreak;\r\n}\r\n}\r\nnext:\r\nraw_spin_unlock(&iter->rt_runtime_lock);\r\n}\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\nreturn more;\r\n}\r\nstatic void __disable_runtime(struct rq *rq)\r\n{\r\nstruct root_domain *rd = rq->rd;\r\nrt_rq_iter_t iter;\r\nstruct rt_rq *rt_rq;\r\nif (unlikely(!scheduler_running))\r\nreturn;\r\nfor_each_rt_rq(rt_rq, iter, rq) {\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\ns64 want;\r\nint i;\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nif (rt_rq->rt_runtime == RUNTIME_INF ||\r\nrt_rq->rt_runtime == rt_b->rt_runtime)\r\ngoto balanced;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\nwant = rt_b->rt_runtime - rt_rq->rt_runtime;\r\nfor_each_cpu(i, rd->span) {\r\nstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);\r\ns64 diff;\r\nif (iter == rt_rq || iter->rt_runtime == RUNTIME_INF)\r\ncontinue;\r\nraw_spin_lock(&iter->rt_runtime_lock);\r\nif (want > 0) {\r\ndiff = min_t(s64, iter->rt_runtime, want);\r\niter->rt_runtime -= diff;\r\nwant -= diff;\r\n} else {\r\niter->rt_runtime -= want;\r\nwant -= want;\r\n}\r\nraw_spin_unlock(&iter->rt_runtime_lock);\r\nif (!want)\r\nbreak;\r\n}\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nBUG_ON(want);\r\nbalanced:\r\nrt_rq->rt_runtime = RUNTIME_INF;\r\nrt_rq->rt_throttled = 0;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\n}\r\n}\r\nstatic void __enable_runtime(struct rq *rq)\r\n{\r\nrt_rq_iter_t iter;\r\nstruct rt_rq *rt_rq;\r\nif (unlikely(!scheduler_running))\r\nreturn;\r\nfor_each_rt_rq(rt_rq, iter, rq) {\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nrt_rq->rt_runtime = rt_b->rt_runtime;\r\nrt_rq->rt_time = 0;\r\nrt_rq->rt_throttled = 0;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\n}\r\n}\r\nstatic int balance_runtime(struct rt_rq *rt_rq)\r\n{\r\nint more = 0;\r\nif (!sched_feat(RT_RUNTIME_SHARE))\r\nreturn more;\r\nif (rt_rq->rt_time > rt_rq->rt_runtime) {\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\nmore = do_balance_runtime(rt_rq);\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\n}\r\nreturn more;\r\n}\r\nstatic inline int balance_runtime(struct rt_rq *rt_rq)\r\n{\r\nreturn 0;\r\n}\r\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)\r\n{\r\nint i, idle = 1, throttled = 0;\r\nconst struct cpumask *span;\r\nspan = sched_rt_period_mask();\r\n#ifdef CONFIG_RT_GROUP_SCHED\r\nif (rt_b == &root_task_group.rt_bandwidth)\r\nspan = cpu_online_mask;\r\n#endif\r\nfor_each_cpu(i, span) {\r\nint enqueue = 0;\r\nstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nraw_spin_lock(&rq->lock);\r\nif (rt_rq->rt_time) {\r\nu64 runtime;\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nif (rt_rq->rt_throttled)\r\nbalance_runtime(rt_rq);\r\nruntime = rt_rq->rt_runtime;\r\nrt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);\r\nif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {\r\nrt_rq->rt_throttled = 0;\r\nenqueue = 1;\r\nif (rt_rq->rt_nr_running && rq->curr == rq->idle)\r\nrq->skip_clock_update = -1;\r\n}\r\nif (rt_rq->rt_time || rt_rq->rt_nr_running)\r\nidle = 0;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\n} else if (rt_rq->rt_nr_running) {\r\nidle = 0;\r\nif (!rt_rq_throttled(rt_rq))\r\nenqueue = 1;\r\n}\r\nif (rt_rq->rt_throttled)\r\nthrottled = 1;\r\nif (enqueue)\r\nsched_rt_rq_enqueue(rt_rq);\r\nraw_spin_unlock(&rq->lock);\r\n}\r\nif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))\r\nreturn 1;\r\nreturn idle;\r\n}\r\nstatic inline int rt_se_prio(struct sched_rt_entity *rt_se)\r\n{\r\n#ifdef CONFIG_RT_GROUP_SCHED\r\nstruct rt_rq *rt_rq = group_rt_rq(rt_se);\r\nif (rt_rq)\r\nreturn rt_rq->highest_prio.curr;\r\n#endif\r\nreturn rt_task_of(rt_se)->prio;\r\n}\r\nstatic int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)\r\n{\r\nu64 runtime = sched_rt_runtime(rt_rq);\r\nif (rt_rq->rt_throttled)\r\nreturn rt_rq_throttled(rt_rq);\r\nif (runtime >= sched_rt_period(rt_rq))\r\nreturn 0;\r\nbalance_runtime(rt_rq);\r\nruntime = sched_rt_runtime(rt_rq);\r\nif (runtime == RUNTIME_INF)\r\nreturn 0;\r\nif (rt_rq->rt_time > runtime) {\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\nif (likely(rt_b->rt_runtime)) {\r\nstatic bool once = false;\r\nrt_rq->rt_throttled = 1;\r\nif (!once) {\r\nonce = true;\r\nprintk_sched("sched: RT throttling activated\n");\r\n}\r\n} else {\r\nrt_rq->rt_time = 0;\r\n}\r\nif (rt_rq_throttled(rt_rq)) {\r\nsched_rt_rq_dequeue(rt_rq);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void update_curr_rt(struct rq *rq)\r\n{\r\nstruct task_struct *curr = rq->curr;\r\nstruct sched_rt_entity *rt_se = &curr->rt;\r\nstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\r\nu64 delta_exec;\r\nif (curr->sched_class != &rt_sched_class)\r\nreturn;\r\ndelta_exec = rq_clock_task(rq) - curr->se.exec_start;\r\nif (unlikely((s64)delta_exec <= 0))\r\nreturn;\r\nschedstat_set(curr->se.statistics.exec_max,\r\nmax(curr->se.statistics.exec_max, delta_exec));\r\ncurr->se.sum_exec_runtime += delta_exec;\r\naccount_group_exec_runtime(curr, delta_exec);\r\ncurr->se.exec_start = rq_clock_task(rq);\r\ncpuacct_charge(curr, delta_exec);\r\nsched_rt_avg_update(rq, delta_exec);\r\nif (!rt_bandwidth_enabled())\r\nreturn;\r\nfor_each_sched_rt_entity(rt_se) {\r\nrt_rq = rt_rq_of_se(rt_se);\r\nif (sched_rt_runtime(rt_rq) != RUNTIME_INF) {\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nrt_rq->rt_time += delta_exec;\r\nif (sched_rt_runtime_exceeded(rt_rq))\r\nresched_task(curr);\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\n}\r\n}\r\n}\r\nstatic void\r\ninc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)\r\n{\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nif (rq->online && prio < prev_prio)\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, prio);\r\n}\r\nstatic void\r\ndec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)\r\n{\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nif (rq->online && rt_rq->highest_prio.curr != prev_prio)\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, rt_rq->highest_prio.curr);\r\n}\r\nstatic inline\r\nvoid inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}\r\nstatic inline\r\nvoid dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}\r\nstatic void\r\ninc_rt_prio(struct rt_rq *rt_rq, int prio)\r\n{\r\nint prev_prio = rt_rq->highest_prio.curr;\r\nif (prio < prev_prio)\r\nrt_rq->highest_prio.curr = prio;\r\ninc_rt_prio_smp(rt_rq, prio, prev_prio);\r\n}\r\nstatic void\r\ndec_rt_prio(struct rt_rq *rt_rq, int prio)\r\n{\r\nint prev_prio = rt_rq->highest_prio.curr;\r\nif (rt_rq->rt_nr_running) {\r\nWARN_ON(prio < prev_prio);\r\nif (prio == prev_prio) {\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nrt_rq->highest_prio.curr =\r\nsched_find_first_bit(array->bitmap);\r\n}\r\n} else\r\nrt_rq->highest_prio.curr = MAX_RT_PRIO;\r\ndec_rt_prio_smp(rt_rq, prio, prev_prio);\r\n}\r\nstatic inline void inc_rt_prio(struct rt_rq *rt_rq, int prio) {}\r\nstatic inline void dec_rt_prio(struct rt_rq *rt_rq, int prio) {}\r\nstatic void\r\ninc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nif (rt_se_boosted(rt_se))\r\nrt_rq->rt_nr_boosted++;\r\nif (rt_rq->tg)\r\nstart_rt_bandwidth(&rt_rq->tg->rt_bandwidth);\r\n}\r\nstatic void\r\ndec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nif (rt_se_boosted(rt_se))\r\nrt_rq->rt_nr_boosted--;\r\nWARN_ON(!rt_rq->rt_nr_running && rt_rq->rt_nr_boosted);\r\n}\r\nstatic void\r\ninc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nstart_rt_bandwidth(&def_rt_bandwidth);\r\n}\r\nstatic inline\r\nvoid dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq) {}\r\nstatic inline\r\nvoid inc_rt_tasks(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nint prio = rt_se_prio(rt_se);\r\nWARN_ON(!rt_prio(prio));\r\nrt_rq->rt_nr_running++;\r\ninc_rt_prio(rt_rq, prio);\r\ninc_rt_migration(rt_se, rt_rq);\r\ninc_rt_group(rt_se, rt_rq);\r\n}\r\nstatic inline\r\nvoid dec_rt_tasks(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nWARN_ON(!rt_prio(rt_se_prio(rt_se)));\r\nWARN_ON(!rt_rq->rt_nr_running);\r\nrt_rq->rt_nr_running--;\r\ndec_rt_prio(rt_rq, rt_se_prio(rt_se));\r\ndec_rt_migration(rt_se, rt_rq);\r\ndec_rt_group(rt_se, rt_rq);\r\n}\r\nstatic void __enqueue_rt_entity(struct sched_rt_entity *rt_se, bool head)\r\n{\r\nstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nstruct rt_rq *group_rq = group_rt_rq(rt_se);\r\nstruct list_head *queue = array->queue + rt_se_prio(rt_se);\r\nif (group_rq && (rt_rq_throttled(group_rq) || !group_rq->rt_nr_running))\r\nreturn;\r\nif (head)\r\nlist_add(&rt_se->run_list, queue);\r\nelse\r\nlist_add_tail(&rt_se->run_list, queue);\r\n__set_bit(rt_se_prio(rt_se), array->bitmap);\r\ninc_rt_tasks(rt_se, rt_rq);\r\n}\r\nstatic void __dequeue_rt_entity(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nlist_del_init(&rt_se->run_list);\r\nif (list_empty(array->queue + rt_se_prio(rt_se)))\r\n__clear_bit(rt_se_prio(rt_se), array->bitmap);\r\ndec_rt_tasks(rt_se, rt_rq);\r\n}\r\nstatic void dequeue_rt_stack(struct sched_rt_entity *rt_se)\r\n{\r\nstruct sched_rt_entity *back = NULL;\r\nfor_each_sched_rt_entity(rt_se) {\r\nrt_se->back = back;\r\nback = rt_se;\r\n}\r\nfor (rt_se = back; rt_se; rt_se = rt_se->back) {\r\nif (on_rt_rq(rt_se))\r\n__dequeue_rt_entity(rt_se);\r\n}\r\n}\r\nstatic void enqueue_rt_entity(struct sched_rt_entity *rt_se, bool head)\r\n{\r\ndequeue_rt_stack(rt_se);\r\nfor_each_sched_rt_entity(rt_se)\r\n__enqueue_rt_entity(rt_se, head);\r\n}\r\nstatic void dequeue_rt_entity(struct sched_rt_entity *rt_se)\r\n{\r\ndequeue_rt_stack(rt_se);\r\nfor_each_sched_rt_entity(rt_se) {\r\nstruct rt_rq *rt_rq = group_rt_rq(rt_se);\r\nif (rt_rq && rt_rq->rt_nr_running)\r\n__enqueue_rt_entity(rt_se, false);\r\n}\r\n}\r\nstatic void\r\nenqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nif (flags & ENQUEUE_WAKEUP)\r\nrt_se->timeout = 0;\r\nenqueue_rt_entity(rt_se, flags & ENQUEUE_HEAD);\r\nif (!task_current(rq, p) && p->nr_cpus_allowed > 1)\r\nenqueue_pushable_task(rq, p);\r\ninc_nr_running(rq);\r\n}\r\nstatic void dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nupdate_curr_rt(rq);\r\ndequeue_rt_entity(rt_se);\r\ndequeue_pushable_task(rq, p);\r\ndec_nr_running(rq);\r\n}\r\nstatic void\r\nrequeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)\r\n{\r\nif (on_rt_rq(rt_se)) {\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nstruct list_head *queue = array->queue + rt_se_prio(rt_se);\r\nif (head)\r\nlist_move(&rt_se->run_list, queue);\r\nelse\r\nlist_move_tail(&rt_se->run_list, queue);\r\n}\r\n}\r\nstatic void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nstruct rt_rq *rt_rq;\r\nfor_each_sched_rt_entity(rt_se) {\r\nrt_rq = rt_rq_of_se(rt_se);\r\nrequeue_rt_entity(rt_rq, rt_se, head);\r\n}\r\n}\r\nstatic void yield_task_rt(struct rq *rq)\r\n{\r\nrequeue_task_rt(rq, rq->curr, 0);\r\n}\r\nstatic int\r\nselect_task_rq_rt(struct task_struct *p, int sd_flag, int flags)\r\n{\r\nstruct task_struct *curr;\r\nstruct rq *rq;\r\nint cpu;\r\ncpu = task_cpu(p);\r\nif (p->nr_cpus_allowed == 1)\r\ngoto out;\r\nif (sd_flag != SD_BALANCE_WAKE && sd_flag != SD_BALANCE_FORK)\r\ngoto out;\r\nrq = cpu_rq(cpu);\r\nrcu_read_lock();\r\ncurr = ACCESS_ONCE(rq->curr);\r\nif (curr && unlikely(rt_task(curr)) &&\r\n(curr->nr_cpus_allowed < 2 ||\r\ncurr->prio <= p->prio) &&\r\n(p->nr_cpus_allowed > 1)) {\r\nint target = find_lowest_rq(p);\r\nif (target != -1)\r\ncpu = target;\r\n}\r\nrcu_read_unlock();\r\nout:\r\nreturn cpu;\r\n}\r\nstatic void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)\r\n{\r\nif (rq->curr->nr_cpus_allowed == 1)\r\nreturn;\r\nif (p->nr_cpus_allowed != 1\r\n&& cpupri_find(&rq->rd->cpupri, p, NULL))\r\nreturn;\r\nif (!cpupri_find(&rq->rd->cpupri, rq->curr, NULL))\r\nreturn;\r\nrequeue_task_rt(rq, p, 1);\r\nresched_task(rq->curr);\r\n}\r\nstatic void check_preempt_curr_rt(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nif (p->prio < rq->curr->prio) {\r\nresched_task(rq->curr);\r\nreturn;\r\n}\r\n#ifdef CONFIG_SMP\r\nif (p->prio == rq->curr->prio && !test_tsk_need_resched(rq->curr))\r\ncheck_preempt_equal_prio(rq, p);\r\n#endif\r\n}\r\nstatic struct sched_rt_entity *pick_next_rt_entity(struct rq *rq,\r\nstruct rt_rq *rt_rq)\r\n{\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nstruct sched_rt_entity *next = NULL;\r\nstruct list_head *queue;\r\nint idx;\r\nidx = sched_find_first_bit(array->bitmap);\r\nBUG_ON(idx >= MAX_RT_PRIO);\r\nqueue = array->queue + idx;\r\nnext = list_entry(queue->next, struct sched_rt_entity, run_list);\r\nreturn next;\r\n}\r\nstatic struct task_struct *_pick_next_task_rt(struct rq *rq)\r\n{\r\nstruct sched_rt_entity *rt_se;\r\nstruct task_struct *p;\r\nstruct rt_rq *rt_rq;\r\nrt_rq = &rq->rt;\r\nif (!rt_rq->rt_nr_running)\r\nreturn NULL;\r\nif (rt_rq_throttled(rt_rq))\r\nreturn NULL;\r\ndo {\r\nrt_se = pick_next_rt_entity(rq, rt_rq);\r\nBUG_ON(!rt_se);\r\nrt_rq = group_rt_rq(rt_se);\r\n} while (rt_rq);\r\np = rt_task_of(rt_se);\r\np->se.exec_start = rq_clock_task(rq);\r\nreturn p;\r\n}\r\nstatic struct task_struct *pick_next_task_rt(struct rq *rq)\r\n{\r\nstruct task_struct *p = _pick_next_task_rt(rq);\r\nif (p)\r\ndequeue_pushable_task(rq, p);\r\n#ifdef CONFIG_SMP\r\nrq->post_schedule = has_pushable_tasks(rq);\r\n#endif\r\nreturn p;\r\n}\r\nstatic void put_prev_task_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nupdate_curr_rt(rq);\r\nif (on_rt_rq(&p->rt) && p->nr_cpus_allowed > 1)\r\nenqueue_pushable_task(rq, p);\r\n}\r\nstatic int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)\r\n{\r\nif (!task_running(rq, p) &&\r\ncpumask_test_cpu(cpu, tsk_cpus_allowed(p)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic struct task_struct *pick_highest_pushable_task(struct rq *rq, int cpu)\r\n{\r\nstruct plist_head *head = &rq->rt.pushable_tasks;\r\nstruct task_struct *p;\r\nif (!has_pushable_tasks(rq))\r\nreturn NULL;\r\nplist_for_each_entry(p, head, pushable_tasks) {\r\nif (pick_rt_task(rq, p, cpu))\r\nreturn p;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int find_lowest_rq(struct task_struct *task)\r\n{\r\nstruct sched_domain *sd;\r\nstruct cpumask *lowest_mask = __get_cpu_var(local_cpu_mask);\r\nint this_cpu = smp_processor_id();\r\nint cpu = task_cpu(task);\r\nif (unlikely(!lowest_mask))\r\nreturn -1;\r\nif (task->nr_cpus_allowed == 1)\r\nreturn -1;\r\nif (!cpupri_find(&task_rq(task)->rd->cpupri, task, lowest_mask))\r\nreturn -1;\r\nif (cpumask_test_cpu(cpu, lowest_mask))\r\nreturn cpu;\r\nif (!cpumask_test_cpu(this_cpu, lowest_mask))\r\nthis_cpu = -1;\r\nrcu_read_lock();\r\nfor_each_domain(cpu, sd) {\r\nif (sd->flags & SD_WAKE_AFFINE) {\r\nint best_cpu;\r\nif (this_cpu != -1 &&\r\ncpumask_test_cpu(this_cpu, sched_domain_span(sd))) {\r\nrcu_read_unlock();\r\nreturn this_cpu;\r\n}\r\nbest_cpu = cpumask_first_and(lowest_mask,\r\nsched_domain_span(sd));\r\nif (best_cpu < nr_cpu_ids) {\r\nrcu_read_unlock();\r\nreturn best_cpu;\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\nif (this_cpu != -1)\r\nreturn this_cpu;\r\ncpu = cpumask_any(lowest_mask);\r\nif (cpu < nr_cpu_ids)\r\nreturn cpu;\r\nreturn -1;\r\n}\r\nstatic struct rq *find_lock_lowest_rq(struct task_struct *task, struct rq *rq)\r\n{\r\nstruct rq *lowest_rq = NULL;\r\nint tries;\r\nint cpu;\r\nfor (tries = 0; tries < RT_MAX_TRIES; tries++) {\r\ncpu = find_lowest_rq(task);\r\nif ((cpu == -1) || (cpu == rq->cpu))\r\nbreak;\r\nlowest_rq = cpu_rq(cpu);\r\nif (double_lock_balance(rq, lowest_rq)) {\r\nif (unlikely(task_rq(task) != rq ||\r\n!cpumask_test_cpu(lowest_rq->cpu,\r\ntsk_cpus_allowed(task)) ||\r\ntask_running(rq, task) ||\r\n!task->on_rq)) {\r\ndouble_unlock_balance(rq, lowest_rq);\r\nlowest_rq = NULL;\r\nbreak;\r\n}\r\n}\r\nif (lowest_rq->rt.highest_prio.curr > task->prio)\r\nbreak;\r\ndouble_unlock_balance(rq, lowest_rq);\r\nlowest_rq = NULL;\r\n}\r\nreturn lowest_rq;\r\n}\r\nstatic struct task_struct *pick_next_pushable_task(struct rq *rq)\r\n{\r\nstruct task_struct *p;\r\nif (!has_pushable_tasks(rq))\r\nreturn NULL;\r\np = plist_first_entry(&rq->rt.pushable_tasks,\r\nstruct task_struct, pushable_tasks);\r\nBUG_ON(rq->cpu != task_cpu(p));\r\nBUG_ON(task_current(rq, p));\r\nBUG_ON(p->nr_cpus_allowed <= 1);\r\nBUG_ON(!p->on_rq);\r\nBUG_ON(!rt_task(p));\r\nreturn p;\r\n}\r\nstatic int push_rt_task(struct rq *rq)\r\n{\r\nstruct task_struct *next_task;\r\nstruct rq *lowest_rq;\r\nint ret = 0;\r\nif (!rq->rt.overloaded)\r\nreturn 0;\r\nnext_task = pick_next_pushable_task(rq);\r\nif (!next_task)\r\nreturn 0;\r\nretry:\r\nif (unlikely(next_task == rq->curr)) {\r\nWARN_ON(1);\r\nreturn 0;\r\n}\r\nif (unlikely(next_task->prio < rq->curr->prio)) {\r\nresched_task(rq->curr);\r\nreturn 0;\r\n}\r\nget_task_struct(next_task);\r\nlowest_rq = find_lock_lowest_rq(next_task, rq);\r\nif (!lowest_rq) {\r\nstruct task_struct *task;\r\ntask = pick_next_pushable_task(rq);\r\nif (task_cpu(next_task) == rq->cpu && task == next_task) {\r\ngoto out;\r\n}\r\nif (!task)\r\ngoto out;\r\nput_task_struct(next_task);\r\nnext_task = task;\r\ngoto retry;\r\n}\r\ndeactivate_task(rq, next_task, 0);\r\nset_task_cpu(next_task, lowest_rq->cpu);\r\nactivate_task(lowest_rq, next_task, 0);\r\nret = 1;\r\nresched_task(lowest_rq->curr);\r\ndouble_unlock_balance(rq, lowest_rq);\r\nout:\r\nput_task_struct(next_task);\r\nreturn ret;\r\n}\r\nstatic void push_rt_tasks(struct rq *rq)\r\n{\r\nwhile (push_rt_task(rq))\r\n;\r\n}\r\nstatic int pull_rt_task(struct rq *this_rq)\r\n{\r\nint this_cpu = this_rq->cpu, ret = 0, cpu;\r\nstruct task_struct *p;\r\nstruct rq *src_rq;\r\nif (likely(!rt_overloaded(this_rq)))\r\nreturn 0;\r\nfor_each_cpu(cpu, this_rq->rd->rto_mask) {\r\nif (this_cpu == cpu)\r\ncontinue;\r\nsrc_rq = cpu_rq(cpu);\r\nif (src_rq->rt.highest_prio.next >=\r\nthis_rq->rt.highest_prio.curr)\r\ncontinue;\r\ndouble_lock_balance(this_rq, src_rq);\r\np = pick_highest_pushable_task(src_rq, this_cpu);\r\nif (p && (p->prio < this_rq->rt.highest_prio.curr)) {\r\nWARN_ON(p == src_rq->curr);\r\nWARN_ON(!p->on_rq);\r\nif (p->prio < src_rq->curr->prio)\r\ngoto skip;\r\nret = 1;\r\ndeactivate_task(src_rq, p, 0);\r\nset_task_cpu(p, this_cpu);\r\nactivate_task(this_rq, p, 0);\r\n}\r\nskip:\r\ndouble_unlock_balance(this_rq, src_rq);\r\n}\r\nreturn ret;\r\n}\r\nstatic void pre_schedule_rt(struct rq *rq, struct task_struct *prev)\r\n{\r\nif (rq->rt.highest_prio.curr > prev->prio)\r\npull_rt_task(rq);\r\n}\r\nstatic void post_schedule_rt(struct rq *rq)\r\n{\r\npush_rt_tasks(rq);\r\n}\r\nstatic void task_woken_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nif (!task_running(rq, p) &&\r\n!test_tsk_need_resched(rq->curr) &&\r\nhas_pushable_tasks(rq) &&\r\np->nr_cpus_allowed > 1 &&\r\nrt_task(rq->curr) &&\r\n(rq->curr->nr_cpus_allowed < 2 ||\r\nrq->curr->prio <= p->prio))\r\npush_rt_tasks(rq);\r\n}\r\nstatic void set_cpus_allowed_rt(struct task_struct *p,\r\nconst struct cpumask *new_mask)\r\n{\r\nstruct rq *rq;\r\nint weight;\r\nBUG_ON(!rt_task(p));\r\nif (!p->on_rq)\r\nreturn;\r\nweight = cpumask_weight(new_mask);\r\nif ((p->nr_cpus_allowed > 1) == (weight > 1))\r\nreturn;\r\nrq = task_rq(p);\r\nif (weight <= 1) {\r\nif (!task_current(rq, p))\r\ndequeue_pushable_task(rq, p);\r\nBUG_ON(!rq->rt.rt_nr_migratory);\r\nrq->rt.rt_nr_migratory--;\r\n} else {\r\nif (!task_current(rq, p))\r\nenqueue_pushable_task(rq, p);\r\nrq->rt.rt_nr_migratory++;\r\n}\r\nupdate_rt_migration(&rq->rt);\r\n}\r\nstatic void rq_online_rt(struct rq *rq)\r\n{\r\nif (rq->rt.overloaded)\r\nrt_set_overload(rq);\r\n__enable_runtime(rq);\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, rq->rt.highest_prio.curr);\r\n}\r\nstatic void rq_offline_rt(struct rq *rq)\r\n{\r\nif (rq->rt.overloaded)\r\nrt_clear_overload(rq);\r\n__disable_runtime(rq);\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, CPUPRI_INVALID);\r\n}\r\nstatic void switched_from_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nif (!p->on_rq || rq->rt.rt_nr_running)\r\nreturn;\r\nif (pull_rt_task(rq))\r\nresched_task(rq->curr);\r\n}\r\nvoid init_sched_rt_class(void)\r\n{\r\nunsigned int i;\r\nfor_each_possible_cpu(i) {\r\nzalloc_cpumask_var_node(&per_cpu(local_cpu_mask, i),\r\nGFP_KERNEL, cpu_to_node(i));\r\n}\r\n}\r\nstatic void switched_to_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nint check_resched = 1;\r\nif (p->on_rq && rq->curr != p) {\r\n#ifdef CONFIG_SMP\r\nif (rq->rt.overloaded && push_rt_task(rq) &&\r\nrq != task_rq(p))\r\ncheck_resched = 0;\r\n#endif\r\nif (check_resched && p->prio < rq->curr->prio)\r\nresched_task(rq->curr);\r\n}\r\n}\r\nstatic void\r\nprio_changed_rt(struct rq *rq, struct task_struct *p, int oldprio)\r\n{\r\nif (!p->on_rq)\r\nreturn;\r\nif (rq->curr == p) {\r\n#ifdef CONFIG_SMP\r\nif (oldprio < p->prio)\r\npull_rt_task(rq);\r\nif (p->prio > rq->rt.highest_prio.curr && rq->curr == p)\r\nresched_task(p);\r\n#else\r\nif (oldprio < p->prio)\r\nresched_task(p);\r\n#endif\r\n} else {\r\nif (p->prio < rq->curr->prio)\r\nresched_task(rq->curr);\r\n}\r\n}\r\nstatic void watchdog(struct rq *rq, struct task_struct *p)\r\n{\r\nunsigned long soft, hard;\r\nsoft = task_rlimit(p, RLIMIT_RTTIME);\r\nhard = task_rlimit_max(p, RLIMIT_RTTIME);\r\nif (soft != RLIM_INFINITY) {\r\nunsigned long next;\r\nif (p->rt.watchdog_stamp != jiffies) {\r\np->rt.timeout++;\r\np->rt.watchdog_stamp = jiffies;\r\n}\r\nnext = DIV_ROUND_UP(min(soft, hard), USEC_PER_SEC/HZ);\r\nif (p->rt.timeout > next)\r\np->cputime_expires.sched_exp = p->se.sum_exec_runtime;\r\n}\r\n}\r\nstatic void task_tick_rt(struct rq *rq, struct task_struct *p, int queued)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nupdate_curr_rt(rq);\r\nwatchdog(rq, p);\r\nif (p->policy != SCHED_RR)\r\nreturn;\r\nif (--p->rt.time_slice)\r\nreturn;\r\np->rt.time_slice = sched_rr_timeslice;\r\nfor_each_sched_rt_entity(rt_se) {\r\nif (rt_se->run_list.prev != rt_se->run_list.next) {\r\nrequeue_task_rt(rq, p, 0);\r\nset_tsk_need_resched(p);\r\nreturn;\r\n}\r\n}\r\n}\r\nstatic void set_curr_task_rt(struct rq *rq)\r\n{\r\nstruct task_struct *p = rq->curr;\r\np->se.exec_start = rq_clock_task(rq);\r\ndequeue_pushable_task(rq, p);\r\n}\r\nstatic unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)\r\n{\r\nif (task->policy == SCHED_RR)\r\nreturn sched_rr_timeslice;\r\nelse\r\nreturn 0;\r\n}\r\nvoid print_rt_stats(struct seq_file *m, int cpu)\r\n{\r\nrt_rq_iter_t iter;\r\nstruct rt_rq *rt_rq;\r\nrcu_read_lock();\r\nfor_each_rt_rq(rt_rq, iter, cpu_rq(cpu))\r\nprint_rt_rq(m, cpu, rt_rq);\r\nrcu_read_unlock();\r\n}
