static bool tegra_apb_dma_init(void)\r\n{\r\ndma_cap_mask_t mask;\r\nmutex_lock(&tegra_apb_dma_lock);\r\nif (tegra_apb_dma_chan)\r\ngoto skip_init;\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\ntegra_apb_dma_chan = dma_request_channel(mask, NULL, NULL);\r\nif (!tegra_apb_dma_chan) {\r\npr_debug("%s: can not allocate dma channel\n", __func__);\r\ngoto err_dma_alloc;\r\n}\r\ntegra_apb_bb = dma_alloc_coherent(NULL, sizeof(u32),\r\n&tegra_apb_bb_phys, GFP_KERNEL);\r\nif (!tegra_apb_bb) {\r\npr_err("%s: can not allocate bounce buffer\n", __func__);\r\ngoto err_buff_alloc;\r\n}\r\ndma_sconfig.src_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndma_sconfig.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndma_sconfig.src_maxburst = 1;\r\ndma_sconfig.dst_maxburst = 1;\r\nskip_init:\r\nmutex_unlock(&tegra_apb_dma_lock);\r\nreturn true;\r\nerr_buff_alloc:\r\ndma_release_channel(tegra_apb_dma_chan);\r\ntegra_apb_dma_chan = NULL;\r\nerr_dma_alloc:\r\nmutex_unlock(&tegra_apb_dma_lock);\r\nreturn false;\r\n}\r\nstatic void apb_dma_complete(void *args)\r\n{\r\ncomplete(&tegra_apb_wait);\r\n}\r\nstatic int do_dma_transfer(unsigned long apb_add,\r\nenum dma_transfer_direction dir)\r\n{\r\nstruct dma_async_tx_descriptor *dma_desc;\r\nint ret;\r\nif (dir == DMA_DEV_TO_MEM)\r\ndma_sconfig.src_addr = apb_add;\r\nelse\r\ndma_sconfig.dst_addr = apb_add;\r\nret = dmaengine_slave_config(tegra_apb_dma_chan, &dma_sconfig);\r\nif (ret)\r\nreturn ret;\r\ndma_desc = dmaengine_prep_slave_single(tegra_apb_dma_chan,\r\ntegra_apb_bb_phys, sizeof(u32), dir,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!dma_desc)\r\nreturn -EINVAL;\r\ndma_desc->callback = apb_dma_complete;\r\ndma_desc->callback_param = NULL;\r\nINIT_COMPLETION(tegra_apb_wait);\r\ndmaengine_submit(dma_desc);\r\ndma_async_issue_pending(tegra_apb_dma_chan);\r\nret = wait_for_completion_timeout(&tegra_apb_wait,\r\nmsecs_to_jiffies(50));\r\nif (WARN(ret == 0, "apb read dma timed out")) {\r\ndmaengine_terminate_all(tegra_apb_dma_chan);\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic u32 tegra_apb_readl_using_dma(unsigned long offset)\r\n{\r\nint ret;\r\nif (!tegra_apb_dma_chan && !tegra_apb_dma_init())\r\nreturn tegra_apb_readl_direct(offset);\r\nmutex_lock(&tegra_apb_dma_lock);\r\nret = do_dma_transfer(offset, DMA_DEV_TO_MEM);\r\nif (ret < 0) {\r\npr_err("error in reading offset 0x%08lx using dma\n", offset);\r\n*(u32 *)tegra_apb_bb = 0;\r\n}\r\nmutex_unlock(&tegra_apb_dma_lock);\r\nreturn *((u32 *)tegra_apb_bb);\r\n}\r\nstatic void tegra_apb_writel_using_dma(u32 value, unsigned long offset)\r\n{\r\nint ret;\r\nif (!tegra_apb_dma_chan && !tegra_apb_dma_init()) {\r\ntegra_apb_writel_direct(value, offset);\r\nreturn;\r\n}\r\nmutex_lock(&tegra_apb_dma_lock);\r\n*((u32 *)tegra_apb_bb) = value;\r\nret = do_dma_transfer(offset, DMA_MEM_TO_DEV);\r\nif (ret < 0)\r\npr_err("error in writing offset 0x%08lx using dma\n", offset);\r\nmutex_unlock(&tegra_apb_dma_lock);\r\n}\r\nstatic u32 tegra_apb_readl_direct(unsigned long offset)\r\n{\r\nreturn readl(IO_ADDRESS(offset));\r\n}\r\nstatic void tegra_apb_writel_direct(u32 value, unsigned long offset)\r\n{\r\nwritel(value, IO_ADDRESS(offset));\r\n}\r\nvoid tegra_apb_io_init(void)\r\n{\r\nif (of_machine_is_compatible("nvidia,tegra20") ||\r\n!of_have_populated_dt()) {\r\napbio_read = tegra_apb_readl_using_dma;\r\napbio_write = tegra_apb_writel_using_dma;\r\n} else {\r\napbio_read = tegra_apb_readl_direct;\r\napbio_write = tegra_apb_writel_direct;\r\n}\r\n}\r\nu32 tegra_apb_readl(unsigned long offset)\r\n{\r\nreturn apbio_read(offset);\r\n}\r\nvoid tegra_apb_writel(u32 value, unsigned long offset)\r\n{\r\napbio_write(value, offset);\r\n}
