static unsigned next_power(unsigned n, unsigned min)\r\n{\r\nreturn roundup_pow_of_two(max(n, min));\r\n}\r\nstatic unsigned long *alloc_bitset(unsigned nr_entries)\r\n{\r\nsize_t s = sizeof(unsigned long) * dm_div_up(nr_entries, BITS_PER_LONG);\r\nreturn vzalloc(s);\r\n}\r\nstatic void free_bitset(unsigned long *bits)\r\n{\r\nvfree(bits);\r\n}\r\nstatic void iot_init(struct io_tracker *t,\r\nint sequential_threshold, int random_threshold)\r\n{\r\nt->pattern = PATTERN_RANDOM;\r\nt->nr_seq_samples = 0;\r\nt->nr_rand_samples = 0;\r\nt->last_end_oblock = 0;\r\nt->thresholds[PATTERN_RANDOM] = random_threshold;\r\nt->thresholds[PATTERN_SEQUENTIAL] = sequential_threshold;\r\n}\r\nstatic enum io_pattern iot_pattern(struct io_tracker *t)\r\n{\r\nreturn t->pattern;\r\n}\r\nstatic void iot_update_stats(struct io_tracker *t, struct bio *bio)\r\n{\r\nif (bio->bi_sector == from_oblock(t->last_end_oblock) + 1)\r\nt->nr_seq_samples++;\r\nelse {\r\nif (t->nr_seq_samples) {\r\nt->nr_seq_samples = 0;\r\nt->nr_rand_samples = 0;\r\n}\r\nt->nr_rand_samples++;\r\n}\r\nt->last_end_oblock = to_oblock(bio->bi_sector + bio_sectors(bio) - 1);\r\n}\r\nstatic void iot_check_for_pattern_switch(struct io_tracker *t)\r\n{\r\nswitch (t->pattern) {\r\ncase PATTERN_SEQUENTIAL:\r\nif (t->nr_rand_samples >= t->thresholds[PATTERN_RANDOM]) {\r\nt->pattern = PATTERN_RANDOM;\r\nt->nr_seq_samples = t->nr_rand_samples = 0;\r\n}\r\nbreak;\r\ncase PATTERN_RANDOM:\r\nif (t->nr_seq_samples >= t->thresholds[PATTERN_SEQUENTIAL]) {\r\nt->pattern = PATTERN_SEQUENTIAL;\r\nt->nr_seq_samples = t->nr_rand_samples = 0;\r\n}\r\nbreak;\r\n}\r\n}\r\nstatic void iot_examine_bio(struct io_tracker *t, struct bio *bio)\r\n{\r\niot_update_stats(t, bio);\r\niot_check_for_pattern_switch(t);\r\n}\r\nstatic void queue_init(struct queue *q)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < NR_QUEUE_LEVELS; i++)\r\nINIT_LIST_HEAD(q->qs + i);\r\n}\r\nstatic void queue_push(struct queue *q, unsigned level, struct list_head *elt)\r\n{\r\nlist_add_tail(elt, q->qs + level);\r\n}\r\nstatic void queue_remove(struct list_head *elt)\r\n{\r\nlist_del(elt);\r\n}\r\nstatic void queue_shift_down(struct queue *q)\r\n{\r\nunsigned level;\r\nfor (level = 1; level < NR_QUEUE_LEVELS; level++)\r\nlist_splice_init(q->qs + level, q->qs + level - 1);\r\n}\r\nstatic struct list_head *queue_pop(struct queue *q)\r\n{\r\nunsigned level;\r\nstruct list_head *r;\r\nfor (level = 0; level < NR_QUEUE_LEVELS; level++)\r\nif (!list_empty(q->qs + level)) {\r\nr = q->qs[level].next;\r\nlist_del(r);\r\nif (level == 0 && list_empty(q->qs))\r\nqueue_shift_down(q);\r\nreturn r;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct list_head *list_pop(struct list_head *lh)\r\n{\r\nstruct list_head *r = lh->next;\r\nBUG_ON(!r);\r\nlist_del_init(r);\r\nreturn r;\r\n}\r\nstatic void takeout_queue(struct list_head *lh, struct queue *q)\r\n{\r\nunsigned level;\r\nfor (level = 0; level < NR_QUEUE_LEVELS; level++)\r\nlist_splice(q->qs + level, lh);\r\n}\r\nstatic void free_entries(struct mq_policy *mq)\r\n{\r\nstruct entry *e, *tmp;\r\ntakeout_queue(&mq->free, &mq->pre_cache);\r\ntakeout_queue(&mq->free, &mq->cache);\r\nlist_for_each_entry_safe(e, tmp, &mq->free, list)\r\nkmem_cache_free(mq_entry_cache, e);\r\n}\r\nstatic int alloc_entries(struct mq_policy *mq, unsigned elts)\r\n{\r\nunsigned u = mq->nr_entries;\r\nINIT_LIST_HEAD(&mq->free);\r\nmq->nr_entries_allocated = 0;\r\nwhile (u--) {\r\nstruct entry *e = kmem_cache_zalloc(mq_entry_cache, GFP_KERNEL);\r\nif (!e) {\r\nfree_entries(mq);\r\nreturn -ENOMEM;\r\n}\r\nlist_add(&e->list, &mq->free);\r\n}\r\nreturn 0;\r\n}\r\nstatic void hash_insert(struct mq_policy *mq, struct entry *e)\r\n{\r\nunsigned h = hash_64(from_oblock(e->oblock), mq->hash_bits);\r\nhlist_add_head(&e->hlist, mq->table + h);\r\n}\r\nstatic struct entry *hash_lookup(struct mq_policy *mq, dm_oblock_t oblock)\r\n{\r\nunsigned h = hash_64(from_oblock(oblock), mq->hash_bits);\r\nstruct hlist_head *bucket = mq->table + h;\r\nstruct entry *e;\r\nhlist_for_each_entry(e, bucket, hlist)\r\nif (e->oblock == oblock) {\r\nhlist_del(&e->hlist);\r\nhlist_add_head(&e->hlist, bucket);\r\nreturn e;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void hash_remove(struct entry *e)\r\n{\r\nhlist_del(&e->hlist);\r\n}\r\nstatic struct entry *alloc_entry(struct mq_policy *mq)\r\n{\r\nstruct entry *e;\r\nif (mq->nr_entries_allocated >= mq->nr_entries) {\r\nBUG_ON(!list_empty(&mq->free));\r\nreturn NULL;\r\n}\r\ne = list_entry(list_pop(&mq->free), struct entry, list);\r\nINIT_LIST_HEAD(&e->list);\r\nINIT_HLIST_NODE(&e->hlist);\r\nmq->nr_entries_allocated++;\r\nreturn e;\r\n}\r\nstatic void alloc_cblock(struct mq_policy *mq, dm_cblock_t cblock)\r\n{\r\nBUG_ON(from_cblock(cblock) > from_cblock(mq->cache_size));\r\nBUG_ON(test_bit(from_cblock(cblock), mq->allocation_bitset));\r\nset_bit(from_cblock(cblock), mq->allocation_bitset);\r\nmq->nr_cblocks_allocated++;\r\n}\r\nstatic void free_cblock(struct mq_policy *mq, dm_cblock_t cblock)\r\n{\r\nBUG_ON(from_cblock(cblock) > from_cblock(mq->cache_size));\r\nBUG_ON(!test_bit(from_cblock(cblock), mq->allocation_bitset));\r\nclear_bit(from_cblock(cblock), mq->allocation_bitset);\r\nmq->nr_cblocks_allocated--;\r\n}\r\nstatic bool any_free_cblocks(struct mq_policy *mq)\r\n{\r\nreturn mq->nr_cblocks_allocated < from_cblock(mq->cache_size);\r\n}\r\nstatic int __find_free_cblock(struct mq_policy *mq, unsigned begin, unsigned end,\r\ndm_cblock_t *result, unsigned *last_word)\r\n{\r\nint r = -ENOSPC;\r\nunsigned w;\r\nfor (w = begin; w < end; w++) {\r\nif (mq->allocation_bitset[w] != ~0UL) {\r\n*last_word = w;\r\n*result = to_cblock((w * BITS_PER_LONG) + ffz(mq->allocation_bitset[w]));\r\nif (from_cblock(*result) < from_cblock(mq->cache_size))\r\nr = 0;\r\nbreak;\r\n}\r\n}\r\nreturn r;\r\n}\r\nstatic int find_free_cblock(struct mq_policy *mq, dm_cblock_t *result)\r\n{\r\nint r;\r\nif (!any_free_cblocks(mq))\r\nreturn -ENOSPC;\r\nr = __find_free_cblock(mq, mq->find_free_last_word, mq->find_free_nr_words, result, &mq->find_free_last_word);\r\nif (r == -ENOSPC && mq->find_free_last_word)\r\nr = __find_free_cblock(mq, 0, mq->find_free_last_word, result, &mq->find_free_last_word);\r\nreturn r;\r\n}\r\nstatic unsigned queue_level(struct entry *e)\r\n{\r\nreturn min((unsigned) ilog2(e->hit_count), NR_QUEUE_LEVELS - 1u);\r\n}\r\nstatic void push(struct mq_policy *mq, struct entry *e)\r\n{\r\ne->tick = mq->tick;\r\nhash_insert(mq, e);\r\nif (e->in_cache) {\r\nalloc_cblock(mq, e->cblock);\r\nqueue_push(&mq->cache, queue_level(e), &e->list);\r\n} else\r\nqueue_push(&mq->pre_cache, queue_level(e), &e->list);\r\n}\r\nstatic void del(struct mq_policy *mq, struct entry *e)\r\n{\r\nqueue_remove(&e->list);\r\nhash_remove(e);\r\nif (e->in_cache)\r\nfree_cblock(mq, e->cblock);\r\n}\r\nstatic struct entry *pop(struct mq_policy *mq, struct queue *q)\r\n{\r\nstruct entry *e = container_of(queue_pop(q), struct entry, list);\r\nif (e) {\r\nhash_remove(e);\r\nif (e->in_cache)\r\nfree_cblock(mq, e->cblock);\r\n}\r\nreturn e;\r\n}\r\nstatic bool updated_this_tick(struct mq_policy *mq, struct entry *e)\r\n{\r\nreturn mq->tick == e->tick;\r\n}\r\nstatic void check_generation(struct mq_policy *mq)\r\n{\r\nunsigned total = 0, nr = 0, count = 0, level;\r\nstruct list_head *head;\r\nstruct entry *e;\r\nif ((mq->hit_count >= mq->generation_period) &&\r\n(mq->nr_cblocks_allocated == from_cblock(mq->cache_size))) {\r\nmq->hit_count = 0;\r\nmq->generation++;\r\nfor (level = 0; level < NR_QUEUE_LEVELS && count < MAX_TO_AVERAGE; level++) {\r\nhead = mq->cache.qs + level;\r\nlist_for_each_entry(e, head, list) {\r\nnr++;\r\ntotal += e->hit_count;\r\nif (++count >= MAX_TO_AVERAGE)\r\nbreak;\r\n}\r\n}\r\nmq->promote_threshold = nr ? total / nr : 1;\r\nif (mq->promote_threshold * nr < total)\r\nmq->promote_threshold++;\r\n}\r\n}\r\nstatic void requeue_and_update_tick(struct mq_policy *mq, struct entry *e)\r\n{\r\nif (updated_this_tick(mq, e))\r\nreturn;\r\ne->hit_count++;\r\nmq->hit_count++;\r\ncheck_generation(mq);\r\ne->generation = mq->generation;\r\ndel(mq, e);\r\npush(mq, e);\r\n}\r\nstatic dm_cblock_t demote_cblock(struct mq_policy *mq, dm_oblock_t *oblock)\r\n{\r\ndm_cblock_t result;\r\nstruct entry *demoted = pop(mq, &mq->cache);\r\nBUG_ON(!demoted);\r\nresult = demoted->cblock;\r\n*oblock = demoted->oblock;\r\ndemoted->in_cache = false;\r\ndemoted->hit_count = 1;\r\npush(mq, demoted);\r\nreturn result;\r\n}\r\nstatic unsigned adjusted_promote_threshold(struct mq_policy *mq,\r\nbool discarded_oblock, int data_dir)\r\n{\r\nif (discarded_oblock && any_free_cblocks(mq) && data_dir == WRITE)\r\nreturn DISCARDED_PROMOTE_THRESHOLD;\r\nreturn data_dir == READ ?\r\n(mq->promote_threshold + READ_PROMOTE_THRESHOLD) :\r\n(mq->promote_threshold + WRITE_PROMOTE_THRESHOLD);\r\n}\r\nstatic bool should_promote(struct mq_policy *mq, struct entry *e,\r\nbool discarded_oblock, int data_dir)\r\n{\r\nreturn e->hit_count >=\r\nadjusted_promote_threshold(mq, discarded_oblock, data_dir);\r\n}\r\nstatic int cache_entry_found(struct mq_policy *mq,\r\nstruct entry *e,\r\nstruct policy_result *result)\r\n{\r\nrequeue_and_update_tick(mq, e);\r\nif (e->in_cache) {\r\nresult->op = POLICY_HIT;\r\nresult->cblock = e->cblock;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pre_cache_to_cache(struct mq_policy *mq, struct entry *e,\r\nstruct policy_result *result)\r\n{\r\ndm_cblock_t cblock;\r\nif (find_free_cblock(mq, &cblock) == -ENOSPC) {\r\nresult->op = POLICY_REPLACE;\r\ncblock = demote_cblock(mq, &result->old_oblock);\r\n} else\r\nresult->op = POLICY_NEW;\r\nresult->cblock = e->cblock = cblock;\r\ndel(mq, e);\r\ne->in_cache = true;\r\npush(mq, e);\r\nreturn 0;\r\n}\r\nstatic int pre_cache_entry_found(struct mq_policy *mq, struct entry *e,\r\nbool can_migrate, bool discarded_oblock,\r\nint data_dir, struct policy_result *result)\r\n{\r\nint r = 0;\r\nbool updated = updated_this_tick(mq, e);\r\nrequeue_and_update_tick(mq, e);\r\nif ((!discarded_oblock && updated) ||\r\n!should_promote(mq, e, discarded_oblock, data_dir))\r\nresult->op = POLICY_MISS;\r\nelse if (!can_migrate)\r\nr = -EWOULDBLOCK;\r\nelse\r\nr = pre_cache_to_cache(mq, e, result);\r\nreturn r;\r\n}\r\nstatic void insert_in_pre_cache(struct mq_policy *mq,\r\ndm_oblock_t oblock)\r\n{\r\nstruct entry *e = alloc_entry(mq);\r\nif (!e)\r\ne = pop(mq, &mq->pre_cache);\r\nif (unlikely(!e)) {\r\nDMWARN("couldn't pop from pre cache");\r\nreturn;\r\n}\r\ne->in_cache = false;\r\ne->oblock = oblock;\r\ne->hit_count = 1;\r\ne->generation = mq->generation;\r\npush(mq, e);\r\n}\r\nstatic void insert_in_cache(struct mq_policy *mq, dm_oblock_t oblock,\r\nstruct policy_result *result)\r\n{\r\nstruct entry *e;\r\ndm_cblock_t cblock;\r\nif (find_free_cblock(mq, &cblock) == -ENOSPC) {\r\nresult->op = POLICY_MISS;\r\ninsert_in_pre_cache(mq, oblock);\r\nreturn;\r\n}\r\ne = alloc_entry(mq);\r\nif (unlikely(!e)) {\r\nresult->op = POLICY_MISS;\r\nreturn;\r\n}\r\ne->oblock = oblock;\r\ne->cblock = cblock;\r\ne->in_cache = true;\r\ne->hit_count = 1;\r\ne->generation = mq->generation;\r\npush(mq, e);\r\nresult->op = POLICY_NEW;\r\nresult->cblock = e->cblock;\r\n}\r\nstatic int no_entry_found(struct mq_policy *mq, dm_oblock_t oblock,\r\nbool can_migrate, bool discarded_oblock,\r\nint data_dir, struct policy_result *result)\r\n{\r\nif (adjusted_promote_threshold(mq, discarded_oblock, data_dir) == 1) {\r\nif (can_migrate)\r\ninsert_in_cache(mq, oblock, result);\r\nelse\r\nreturn -EWOULDBLOCK;\r\n} else {\r\ninsert_in_pre_cache(mq, oblock);\r\nresult->op = POLICY_MISS;\r\n}\r\nreturn 0;\r\n}\r\nstatic int map(struct mq_policy *mq, dm_oblock_t oblock,\r\nbool can_migrate, bool discarded_oblock,\r\nint data_dir, struct policy_result *result)\r\n{\r\nint r = 0;\r\nstruct entry *e = hash_lookup(mq, oblock);\r\nif (e && e->in_cache)\r\nr = cache_entry_found(mq, e, result);\r\nelse if (iot_pattern(&mq->tracker) == PATTERN_SEQUENTIAL)\r\nresult->op = POLICY_MISS;\r\nelse if (e)\r\nr = pre_cache_entry_found(mq, e, can_migrate, discarded_oblock,\r\ndata_dir, result);\r\nelse\r\nr = no_entry_found(mq, oblock, can_migrate, discarded_oblock,\r\ndata_dir, result);\r\nif (r == -EWOULDBLOCK)\r\nresult->op = POLICY_MISS;\r\nreturn r;\r\n}\r\nstatic struct mq_policy *to_mq_policy(struct dm_cache_policy *p)\r\n{\r\nreturn container_of(p, struct mq_policy, policy);\r\n}\r\nstatic void mq_destroy(struct dm_cache_policy *p)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nfree_bitset(mq->allocation_bitset);\r\nkfree(mq->table);\r\nfree_entries(mq);\r\nkfree(mq);\r\n}\r\nstatic void copy_tick(struct mq_policy *mq)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&mq->tick_lock, flags);\r\nmq->tick = mq->tick_protected;\r\nspin_unlock_irqrestore(&mq->tick_lock, flags);\r\n}\r\nstatic int mq_map(struct dm_cache_policy *p, dm_oblock_t oblock,\r\nbool can_block, bool can_migrate, bool discarded_oblock,\r\nstruct bio *bio, struct policy_result *result)\r\n{\r\nint r;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nresult->op = POLICY_MISS;\r\nif (can_block)\r\nmutex_lock(&mq->lock);\r\nelse if (!mutex_trylock(&mq->lock))\r\nreturn -EWOULDBLOCK;\r\ncopy_tick(mq);\r\niot_examine_bio(&mq->tracker, bio);\r\nr = map(mq, oblock, can_migrate, discarded_oblock,\r\nbio_data_dir(bio), result);\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic int mq_lookup(struct dm_cache_policy *p, dm_oblock_t oblock, dm_cblock_t *cblock)\r\n{\r\nint r;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nstruct entry *e;\r\nif (!mutex_trylock(&mq->lock))\r\nreturn -EWOULDBLOCK;\r\ne = hash_lookup(mq, oblock);\r\nif (e && e->in_cache) {\r\n*cblock = e->cblock;\r\nr = 0;\r\n} else\r\nr = -ENOENT;\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic int mq_load_mapping(struct dm_cache_policy *p,\r\ndm_oblock_t oblock, dm_cblock_t cblock,\r\nuint32_t hint, bool hint_valid)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nstruct entry *e;\r\ne = alloc_entry(mq);\r\nif (!e)\r\nreturn -ENOMEM;\r\ne->cblock = cblock;\r\ne->oblock = oblock;\r\ne->in_cache = true;\r\ne->hit_count = hint_valid ? hint : 1;\r\ne->generation = mq->generation;\r\npush(mq, e);\r\nreturn 0;\r\n}\r\nstatic int mq_walk_mappings(struct dm_cache_policy *p, policy_walk_fn fn,\r\nvoid *context)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nint r = 0;\r\nstruct entry *e;\r\nunsigned level;\r\nmutex_lock(&mq->lock);\r\nfor (level = 0; level < NR_QUEUE_LEVELS; level++)\r\nlist_for_each_entry(e, &mq->cache.qs[level], list) {\r\nr = fn(context, e->cblock, e->oblock, e->hit_count);\r\nif (r)\r\ngoto out;\r\n}\r\nout:\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic void mq_remove_mapping(struct dm_cache_policy *p, dm_oblock_t oblock)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nstruct entry *e;\r\nmutex_lock(&mq->lock);\r\ne = hash_lookup(mq, oblock);\r\nBUG_ON(!e || !e->in_cache);\r\ndel(mq, e);\r\ne->in_cache = false;\r\npush(mq, e);\r\nmutex_unlock(&mq->lock);\r\n}\r\nstatic void force_mapping(struct mq_policy *mq,\r\ndm_oblock_t current_oblock, dm_oblock_t new_oblock)\r\n{\r\nstruct entry *e = hash_lookup(mq, current_oblock);\r\nBUG_ON(!e || !e->in_cache);\r\ndel(mq, e);\r\ne->oblock = new_oblock;\r\npush(mq, e);\r\n}\r\nstatic void mq_force_mapping(struct dm_cache_policy *p,\r\ndm_oblock_t current_oblock, dm_oblock_t new_oblock)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\nforce_mapping(mq, current_oblock, new_oblock);\r\nmutex_unlock(&mq->lock);\r\n}\r\nstatic dm_cblock_t mq_residency(struct dm_cache_policy *p)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nreturn to_cblock(mq->nr_cblocks_allocated);\r\n}\r\nstatic void mq_tick(struct dm_cache_policy *p)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mq->tick_lock, flags);\r\nmq->tick_protected++;\r\nspin_unlock_irqrestore(&mq->tick_lock, flags);\r\n}\r\nstatic int mq_set_config_value(struct dm_cache_policy *p,\r\nconst char *key, const char *value)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nenum io_pattern pattern;\r\nunsigned long tmp;\r\nif (!strcasecmp(key, "random_threshold"))\r\npattern = PATTERN_RANDOM;\r\nelse if (!strcasecmp(key, "sequential_threshold"))\r\npattern = PATTERN_SEQUENTIAL;\r\nelse\r\nreturn -EINVAL;\r\nif (kstrtoul(value, 10, &tmp))\r\nreturn -EINVAL;\r\nmq->tracker.thresholds[pattern] = tmp;\r\nreturn 0;\r\n}\r\nstatic int mq_emit_config_values(struct dm_cache_policy *p, char *result, unsigned maxlen)\r\n{\r\nssize_t sz = 0;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nDMEMIT("4 random_threshold %u sequential_threshold %u",\r\nmq->tracker.thresholds[PATTERN_RANDOM],\r\nmq->tracker.thresholds[PATTERN_SEQUENTIAL]);\r\nreturn 0;\r\n}\r\nstatic void init_policy_functions(struct mq_policy *mq)\r\n{\r\nmq->policy.destroy = mq_destroy;\r\nmq->policy.map = mq_map;\r\nmq->policy.lookup = mq_lookup;\r\nmq->policy.load_mapping = mq_load_mapping;\r\nmq->policy.walk_mappings = mq_walk_mappings;\r\nmq->policy.remove_mapping = mq_remove_mapping;\r\nmq->policy.writeback_work = NULL;\r\nmq->policy.force_mapping = mq_force_mapping;\r\nmq->policy.residency = mq_residency;\r\nmq->policy.tick = mq_tick;\r\nmq->policy.emit_config_values = mq_emit_config_values;\r\nmq->policy.set_config_value = mq_set_config_value;\r\n}\r\nstatic struct dm_cache_policy *mq_create(dm_cblock_t cache_size,\r\nsector_t origin_size,\r\nsector_t cache_block_size)\r\n{\r\nint r;\r\nstruct mq_policy *mq = kzalloc(sizeof(*mq), GFP_KERNEL);\r\nif (!mq)\r\nreturn NULL;\r\ninit_policy_functions(mq);\r\niot_init(&mq->tracker, SEQUENTIAL_THRESHOLD_DEFAULT, RANDOM_THRESHOLD_DEFAULT);\r\nmq->cache_size = cache_size;\r\nmq->tick_protected = 0;\r\nmq->tick = 0;\r\nmq->hit_count = 0;\r\nmq->generation = 0;\r\nmq->promote_threshold = 0;\r\nmutex_init(&mq->lock);\r\nspin_lock_init(&mq->tick_lock);\r\nmq->find_free_nr_words = dm_div_up(from_cblock(mq->cache_size), BITS_PER_LONG);\r\nmq->find_free_last_word = 0;\r\nqueue_init(&mq->pre_cache);\r\nqueue_init(&mq->cache);\r\nmq->generation_period = max((unsigned) from_cblock(cache_size), 1024U);\r\nmq->nr_entries = 2 * from_cblock(cache_size);\r\nr = alloc_entries(mq, mq->nr_entries);\r\nif (r)\r\ngoto bad_cache_alloc;\r\nmq->nr_entries_allocated = 0;\r\nmq->nr_cblocks_allocated = 0;\r\nmq->nr_buckets = next_power(from_cblock(cache_size) / 2, 16);\r\nmq->hash_bits = ffs(mq->nr_buckets) - 1;\r\nmq->table = kzalloc(sizeof(*mq->table) * mq->nr_buckets, GFP_KERNEL);\r\nif (!mq->table)\r\ngoto bad_alloc_table;\r\nmq->allocation_bitset = alloc_bitset(from_cblock(cache_size));\r\nif (!mq->allocation_bitset)\r\ngoto bad_alloc_bitset;\r\nreturn &mq->policy;\r\nbad_alloc_bitset:\r\nkfree(mq->table);\r\nbad_alloc_table:\r\nfree_entries(mq);\r\nbad_cache_alloc:\r\nkfree(mq);\r\nreturn NULL;\r\n}\r\nstatic int __init mq_init(void)\r\n{\r\nint r;\r\nmq_entry_cache = kmem_cache_create("dm_mq_policy_cache_entry",\r\nsizeof(struct entry),\r\n__alignof__(struct entry),\r\n0, NULL);\r\nif (!mq_entry_cache)\r\ngoto bad;\r\nr = dm_cache_policy_register(&mq_policy_type);\r\nif (r) {\r\nDMERR("register failed %d", r);\r\ngoto bad_register_mq;\r\n}\r\nr = dm_cache_policy_register(&default_policy_type);\r\nif (!r) {\r\nDMINFO("version %u.%u.%u loaded",\r\nmq_policy_type.version[0],\r\nmq_policy_type.version[1],\r\nmq_policy_type.version[2]);\r\nreturn 0;\r\n}\r\nDMERR("register failed (as default) %d", r);\r\ndm_cache_policy_unregister(&mq_policy_type);\r\nbad_register_mq:\r\nkmem_cache_destroy(mq_entry_cache);\r\nbad:\r\nreturn -ENOMEM;\r\n}\r\nstatic void __exit mq_exit(void)\r\n{\r\ndm_cache_policy_unregister(&mq_policy_type);\r\ndm_cache_policy_unregister(&default_policy_type);\r\nkmem_cache_destroy(mq_entry_cache);\r\n}
