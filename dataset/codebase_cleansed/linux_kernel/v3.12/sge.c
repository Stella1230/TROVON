static inline dma_addr_t get_buf_addr(const struct rx_sw_desc *sdesc)\r\n{\r\nreturn sdesc->dma_addr & ~(dma_addr_t)(RX_LARGE_BUF | RX_UNMAPPED_BUF);\r\n}\r\nstatic inline bool is_buf_mapped(const struct rx_sw_desc *sdesc)\r\n{\r\nreturn !(sdesc->dma_addr & RX_UNMAPPED_BUF);\r\n}\r\nstatic inline int need_skb_unmap(void)\r\n{\r\n#ifdef CONFIG_NEED_DMA_MAP_STATE\r\nreturn 1;\r\n#else\r\nreturn 0;\r\n#endif\r\n}\r\nstatic inline unsigned int txq_avail(const struct sge_txq *tq)\r\n{\r\nreturn tq->size - 1 - tq->in_use;\r\n}\r\nstatic inline unsigned int fl_cap(const struct sge_fl *fl)\r\n{\r\nreturn fl->size - FL_PER_EQ_UNIT;\r\n}\r\nstatic inline bool fl_starving(const struct sge_fl *fl)\r\n{\r\nreturn fl->avail - fl->pend_cred <= FL_STARVE_THRES;\r\n}\r\nstatic int map_skb(struct device *dev, const struct sk_buff *skb,\r\ndma_addr_t *addr)\r\n{\r\nconst skb_frag_t *fp, *end;\r\nconst struct skb_shared_info *si;\r\n*addr = dma_map_single(dev, skb->data, skb_headlen(skb), DMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, *addr))\r\ngoto out_err;\r\nsi = skb_shinfo(skb);\r\nend = &si->frags[si->nr_frags];\r\nfor (fp = si->frags; fp < end; fp++) {\r\n*++addr = skb_frag_dma_map(dev, fp, 0, skb_frag_size(fp),\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, *addr))\r\ngoto unwind;\r\n}\r\nreturn 0;\r\nunwind:\r\nwhile (fp-- > si->frags)\r\ndma_unmap_page(dev, *--addr, skb_frag_size(fp), DMA_TO_DEVICE);\r\ndma_unmap_single(dev, addr[-1], skb_headlen(skb), DMA_TO_DEVICE);\r\nout_err:\r\nreturn -ENOMEM;\r\n}\r\nstatic void unmap_sgl(struct device *dev, const struct sk_buff *skb,\r\nconst struct ulptx_sgl *sgl, const struct sge_txq *tq)\r\n{\r\nconst struct ulptx_sge_pair *p;\r\nunsigned int nfrags = skb_shinfo(skb)->nr_frags;\r\nif (likely(skb_headlen(skb)))\r\ndma_unmap_single(dev, be64_to_cpu(sgl->addr0),\r\nbe32_to_cpu(sgl->len0), DMA_TO_DEVICE);\r\nelse {\r\ndma_unmap_page(dev, be64_to_cpu(sgl->addr0),\r\nbe32_to_cpu(sgl->len0), DMA_TO_DEVICE);\r\nnfrags--;\r\n}\r\nfor (p = sgl->sge; nfrags >= 2; nfrags -= 2) {\r\nif (likely((u8 *)(p + 1) <= (u8 *)tq->stat)) {\r\nunmap:\r\ndma_unmap_page(dev, be64_to_cpu(p->addr[0]),\r\nbe32_to_cpu(p->len[0]), DMA_TO_DEVICE);\r\ndma_unmap_page(dev, be64_to_cpu(p->addr[1]),\r\nbe32_to_cpu(p->len[1]), DMA_TO_DEVICE);\r\np++;\r\n} else if ((u8 *)p == (u8 *)tq->stat) {\r\np = (const struct ulptx_sge_pair *)tq->desc;\r\ngoto unmap;\r\n} else if ((u8 *)p + 8 == (u8 *)tq->stat) {\r\nconst __be64 *addr = (const __be64 *)tq->desc;\r\ndma_unmap_page(dev, be64_to_cpu(addr[0]),\r\nbe32_to_cpu(p->len[0]), DMA_TO_DEVICE);\r\ndma_unmap_page(dev, be64_to_cpu(addr[1]),\r\nbe32_to_cpu(p->len[1]), DMA_TO_DEVICE);\r\np = (const struct ulptx_sge_pair *)&addr[2];\r\n} else {\r\nconst __be64 *addr = (const __be64 *)tq->desc;\r\ndma_unmap_page(dev, be64_to_cpu(p->addr[0]),\r\nbe32_to_cpu(p->len[0]), DMA_TO_DEVICE);\r\ndma_unmap_page(dev, be64_to_cpu(addr[0]),\r\nbe32_to_cpu(p->len[1]), DMA_TO_DEVICE);\r\np = (const struct ulptx_sge_pair *)&addr[1];\r\n}\r\n}\r\nif (nfrags) {\r\n__be64 addr;\r\nif ((u8 *)p == (u8 *)tq->stat)\r\np = (const struct ulptx_sge_pair *)tq->desc;\r\naddr = ((u8 *)p + 16 <= (u8 *)tq->stat\r\n? p->addr[0]\r\n: *(const __be64 *)tq->desc);\r\ndma_unmap_page(dev, be64_to_cpu(addr), be32_to_cpu(p->len[0]),\r\nDMA_TO_DEVICE);\r\n}\r\n}\r\nstatic void free_tx_desc(struct adapter *adapter, struct sge_txq *tq,\r\nunsigned int n, bool unmap)\r\n{\r\nstruct tx_sw_desc *sdesc;\r\nunsigned int cidx = tq->cidx;\r\nstruct device *dev = adapter->pdev_dev;\r\nconst int need_unmap = need_skb_unmap() && unmap;\r\nsdesc = &tq->sdesc[cidx];\r\nwhile (n--) {\r\nif (sdesc->skb) {\r\nif (need_unmap)\r\nunmap_sgl(dev, sdesc->skb, sdesc->sgl, tq);\r\nkfree_skb(sdesc->skb);\r\nsdesc->skb = NULL;\r\n}\r\nsdesc++;\r\nif (++cidx == tq->size) {\r\ncidx = 0;\r\nsdesc = tq->sdesc;\r\n}\r\n}\r\ntq->cidx = cidx;\r\n}\r\nstatic inline int reclaimable(const struct sge_txq *tq)\r\n{\r\nint hw_cidx = be16_to_cpu(tq->stat->cidx);\r\nint reclaimable = hw_cidx - tq->cidx;\r\nif (reclaimable < 0)\r\nreclaimable += tq->size;\r\nreturn reclaimable;\r\n}\r\nstatic inline void reclaim_completed_tx(struct adapter *adapter,\r\nstruct sge_txq *tq,\r\nbool unmap)\r\n{\r\nint avail = reclaimable(tq);\r\nif (avail) {\r\nif (avail > MAX_TX_RECLAIM)\r\navail = MAX_TX_RECLAIM;\r\nfree_tx_desc(adapter, tq, avail, unmap);\r\ntq->in_use -= avail;\r\n}\r\n}\r\nstatic inline int get_buf_size(const struct rx_sw_desc *sdesc)\r\n{\r\nreturn FL_PG_ORDER > 0 && (sdesc->dma_addr & RX_LARGE_BUF)\r\n? (PAGE_SIZE << FL_PG_ORDER)\r\n: PAGE_SIZE;\r\n}\r\nstatic void free_rx_bufs(struct adapter *adapter, struct sge_fl *fl, int n)\r\n{\r\nwhile (n--) {\r\nstruct rx_sw_desc *sdesc = &fl->sdesc[fl->cidx];\r\nif (is_buf_mapped(sdesc))\r\ndma_unmap_page(adapter->pdev_dev, get_buf_addr(sdesc),\r\nget_buf_size(sdesc), PCI_DMA_FROMDEVICE);\r\nput_page(sdesc->page);\r\nsdesc->page = NULL;\r\nif (++fl->cidx == fl->size)\r\nfl->cidx = 0;\r\nfl->avail--;\r\n}\r\n}\r\nstatic void unmap_rx_buf(struct adapter *adapter, struct sge_fl *fl)\r\n{\r\nstruct rx_sw_desc *sdesc = &fl->sdesc[fl->cidx];\r\nif (is_buf_mapped(sdesc))\r\ndma_unmap_page(adapter->pdev_dev, get_buf_addr(sdesc),\r\nget_buf_size(sdesc), PCI_DMA_FROMDEVICE);\r\nsdesc->page = NULL;\r\nif (++fl->cidx == fl->size)\r\nfl->cidx = 0;\r\nfl->avail--;\r\n}\r\nstatic inline void ring_fl_db(struct adapter *adapter, struct sge_fl *fl)\r\n{\r\nu32 val;\r\nif (fl->pend_cred >= FL_PER_EQ_UNIT) {\r\nval = PIDX(fl->pend_cred / FL_PER_EQ_UNIT);\r\nif (!is_t4(adapter->chip))\r\nval |= DBTYPE(1);\r\nwmb();\r\nt4_write_reg(adapter, T4VF_SGE_BASE_ADDR + SGE_VF_KDOORBELL,\r\nDBPRIO(1) |\r\nQID(fl->cntxt_id) | val);\r\nfl->pend_cred %= FL_PER_EQ_UNIT;\r\n}\r\n}\r\nstatic inline void set_rx_sw_desc(struct rx_sw_desc *sdesc, struct page *page,\r\ndma_addr_t dma_addr)\r\n{\r\nsdesc->page = page;\r\nsdesc->dma_addr = dma_addr;\r\n}\r\nstatic inline void poison_buf(struct page *page, size_t sz)\r\n{\r\n#if POISON_BUF_VAL >= 0\r\nmemset(page_address(page), POISON_BUF_VAL, sz);\r\n#endif\r\n}\r\nstatic unsigned int refill_fl(struct adapter *adapter, struct sge_fl *fl,\r\nint n, gfp_t gfp)\r\n{\r\nstruct page *page;\r\ndma_addr_t dma_addr;\r\nunsigned int cred = fl->avail;\r\n__be64 *d = &fl->desc[fl->pidx];\r\nstruct rx_sw_desc *sdesc = &fl->sdesc[fl->pidx];\r\nBUG_ON(fl->avail + n > fl->size - FL_PER_EQ_UNIT);\r\nif (FL_PG_ORDER == 0)\r\ngoto alloc_small_pages;\r\nwhile (n) {\r\npage = alloc_pages(gfp | __GFP_COMP | __GFP_NOWARN,\r\nFL_PG_ORDER);\r\nif (unlikely(!page)) {\r\nfl->large_alloc_failed++;\r\nbreak;\r\n}\r\npoison_buf(page, PAGE_SIZE << FL_PG_ORDER);\r\ndma_addr = dma_map_page(adapter->pdev_dev, page, 0,\r\nPAGE_SIZE << FL_PG_ORDER,\r\nPCI_DMA_FROMDEVICE);\r\nif (unlikely(dma_mapping_error(adapter->pdev_dev, dma_addr))) {\r\n__free_pages(page, FL_PG_ORDER);\r\ngoto out;\r\n}\r\ndma_addr |= RX_LARGE_BUF;\r\n*d++ = cpu_to_be64(dma_addr);\r\nset_rx_sw_desc(sdesc, page, dma_addr);\r\nsdesc++;\r\nfl->avail++;\r\nif (++fl->pidx == fl->size) {\r\nfl->pidx = 0;\r\nsdesc = fl->sdesc;\r\nd = fl->desc;\r\n}\r\nn--;\r\n}\r\nalloc_small_pages:\r\nwhile (n--) {\r\npage = __skb_alloc_page(gfp | __GFP_NOWARN, NULL);\r\nif (unlikely(!page)) {\r\nfl->alloc_failed++;\r\nbreak;\r\n}\r\npoison_buf(page, PAGE_SIZE);\r\ndma_addr = dma_map_page(adapter->pdev_dev, page, 0, PAGE_SIZE,\r\nPCI_DMA_FROMDEVICE);\r\nif (unlikely(dma_mapping_error(adapter->pdev_dev, dma_addr))) {\r\nput_page(page);\r\nbreak;\r\n}\r\n*d++ = cpu_to_be64(dma_addr);\r\nset_rx_sw_desc(sdesc, page, dma_addr);\r\nsdesc++;\r\nfl->avail++;\r\nif (++fl->pidx == fl->size) {\r\nfl->pidx = 0;\r\nsdesc = fl->sdesc;\r\nd = fl->desc;\r\n}\r\n}\r\nout:\r\ncred = fl->avail - cred;\r\nfl->pend_cred += cred;\r\nring_fl_db(adapter, fl);\r\nif (unlikely(fl_starving(fl))) {\r\nsmp_wmb();\r\nset_bit(fl->cntxt_id, adapter->sge.starving_fl);\r\n}\r\nreturn cred;\r\n}\r\nstatic inline void __refill_fl(struct adapter *adapter, struct sge_fl *fl)\r\n{\r\nrefill_fl(adapter, fl,\r\nmin((unsigned int)MAX_RX_REFILL, fl_cap(fl) - fl->avail),\r\nGFP_ATOMIC);\r\n}\r\nstatic void *alloc_ring(struct device *dev, size_t nelem, size_t hwsize,\r\nsize_t swsize, dma_addr_t *busaddrp, void *swringp,\r\nsize_t stat_size)\r\n{\r\nsize_t hwlen = nelem * hwsize + stat_size;\r\nvoid *hwring = dma_alloc_coherent(dev, hwlen, busaddrp, GFP_KERNEL);\r\nif (!hwring)\r\nreturn NULL;\r\nBUG_ON((swsize != 0) != (swringp != NULL));\r\nif (swsize) {\r\nvoid *swring = kcalloc(nelem, swsize, GFP_KERNEL);\r\nif (!swring) {\r\ndma_free_coherent(dev, hwlen, hwring, *busaddrp);\r\nreturn NULL;\r\n}\r\n*(void **)swringp = swring;\r\n}\r\nmemset(hwring, 0, hwlen);\r\nreturn hwring;\r\n}\r\nstatic inline unsigned int sgl_len(unsigned int n)\r\n{\r\nn--;\r\nreturn (3 * n) / 2 + (n & 1) + 2;\r\n}\r\nstatic inline unsigned int flits_to_desc(unsigned int flits)\r\n{\r\nBUG_ON(flits > SGE_MAX_WR_LEN / sizeof(__be64));\r\nreturn DIV_ROUND_UP(flits, TXD_PER_EQ_UNIT);\r\n}\r\nstatic inline int is_eth_imm(const struct sk_buff *skb)\r\n{\r\nreturn false;\r\n}\r\nstatic inline unsigned int calc_tx_flits(const struct sk_buff *skb)\r\n{\r\nunsigned int flits;\r\nif (is_eth_imm(skb))\r\nreturn DIV_ROUND_UP(skb->len + sizeof(struct cpl_tx_pkt),\r\nsizeof(__be64));\r\nflits = sgl_len(skb_shinfo(skb)->nr_frags + 1);\r\nif (skb_shinfo(skb)->gso_size)\r\nflits += (sizeof(struct fw_eth_tx_pkt_vm_wr) +\r\nsizeof(struct cpl_tx_pkt_lso_core) +\r\nsizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);\r\nelse\r\nflits += (sizeof(struct fw_eth_tx_pkt_vm_wr) +\r\nsizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);\r\nreturn flits;\r\n}\r\nstatic void write_sgl(const struct sk_buff *skb, struct sge_txq *tq,\r\nstruct ulptx_sgl *sgl, u64 *end, unsigned int start,\r\nconst dma_addr_t *addr)\r\n{\r\nunsigned int i, len;\r\nstruct ulptx_sge_pair *to;\r\nconst struct skb_shared_info *si = skb_shinfo(skb);\r\nunsigned int nfrags = si->nr_frags;\r\nstruct ulptx_sge_pair buf[MAX_SKB_FRAGS / 2 + 1];\r\nlen = skb_headlen(skb) - start;\r\nif (likely(len)) {\r\nsgl->len0 = htonl(len);\r\nsgl->addr0 = cpu_to_be64(addr[0] + start);\r\nnfrags++;\r\n} else {\r\nsgl->len0 = htonl(skb_frag_size(&si->frags[0]));\r\nsgl->addr0 = cpu_to_be64(addr[1]);\r\n}\r\nsgl->cmd_nsge = htonl(ULPTX_CMD(ULP_TX_SC_DSGL) |\r\nULPTX_NSGE(nfrags));\r\nif (likely(--nfrags == 0))\r\nreturn;\r\nto = (u8 *)end > (u8 *)tq->stat ? buf : sgl->sge;\r\nfor (i = (nfrags != si->nr_frags); nfrags >= 2; nfrags -= 2, to++) {\r\nto->len[0] = cpu_to_be32(skb_frag_size(&si->frags[i]));\r\nto->len[1] = cpu_to_be32(skb_frag_size(&si->frags[++i]));\r\nto->addr[0] = cpu_to_be64(addr[i]);\r\nto->addr[1] = cpu_to_be64(addr[++i]);\r\n}\r\nif (nfrags) {\r\nto->len[0] = cpu_to_be32(skb_frag_size(&si->frags[i]));\r\nto->len[1] = cpu_to_be32(0);\r\nto->addr[0] = cpu_to_be64(addr[i + 1]);\r\n}\r\nif (unlikely((u8 *)end > (u8 *)tq->stat)) {\r\nunsigned int part0 = (u8 *)tq->stat - (u8 *)sgl->sge, part1;\r\nif (likely(part0))\r\nmemcpy(sgl->sge, buf, part0);\r\npart1 = (u8 *)end - (u8 *)tq->stat;\r\nmemcpy(tq->desc, (u8 *)buf + part0, part1);\r\nend = (void *)tq->desc + part1;\r\n}\r\nif ((uintptr_t)end & 8)\r\n*end = 0;\r\n}\r\nstatic inline void ring_tx_db(struct adapter *adapter, struct sge_txq *tq,\r\nint n)\r\n{\r\nWARN_ON((QID(tq->cntxt_id) | PIDX(n)) & DBPRIO(1));\r\nwmb();\r\nt4_write_reg(adapter, T4VF_SGE_BASE_ADDR + SGE_VF_KDOORBELL,\r\nQID(tq->cntxt_id) | PIDX(n));\r\n}\r\nstatic void inline_tx_skb(const struct sk_buff *skb, const struct sge_txq *tq,\r\nvoid *pos)\r\n{\r\nu64 *p;\r\nint left = (void *)tq->stat - pos;\r\nif (likely(skb->len <= left)) {\r\nif (likely(!skb->data_len))\r\nskb_copy_from_linear_data(skb, pos, skb->len);\r\nelse\r\nskb_copy_bits(skb, 0, pos, skb->len);\r\npos += skb->len;\r\n} else {\r\nskb_copy_bits(skb, 0, pos, left);\r\nskb_copy_bits(skb, left, tq->desc, skb->len - left);\r\npos = (void *)tq->desc + (skb->len - left);\r\n}\r\np = PTR_ALIGN(pos, 8);\r\nif ((uintptr_t)p & 8)\r\n*p = 0;\r\n}\r\nstatic u64 hwcsum(const struct sk_buff *skb)\r\n{\r\nint csum_type;\r\nconst struct iphdr *iph = ip_hdr(skb);\r\nif (iph->version == 4) {\r\nif (iph->protocol == IPPROTO_TCP)\r\ncsum_type = TX_CSUM_TCPIP;\r\nelse if (iph->protocol == IPPROTO_UDP)\r\ncsum_type = TX_CSUM_UDPIP;\r\nelse {\r\nnocsum:\r\nreturn TXPKT_L4CSUM_DIS;\r\n}\r\n} else {\r\nconst struct ipv6hdr *ip6h = (const struct ipv6hdr *)iph;\r\nif (ip6h->nexthdr == IPPROTO_TCP)\r\ncsum_type = TX_CSUM_TCPIP6;\r\nelse if (ip6h->nexthdr == IPPROTO_UDP)\r\ncsum_type = TX_CSUM_UDPIP6;\r\nelse\r\ngoto nocsum;\r\n}\r\nif (likely(csum_type >= TX_CSUM_TCPIP))\r\nreturn TXPKT_CSUM_TYPE(csum_type) |\r\nTXPKT_IPHDR_LEN(skb_network_header_len(skb)) |\r\nTXPKT_ETHHDR_LEN(skb_network_offset(skb) - ETH_HLEN);\r\nelse {\r\nint start = skb_transport_offset(skb);\r\nreturn TXPKT_CSUM_TYPE(csum_type) |\r\nTXPKT_CSUM_START(start) |\r\nTXPKT_CSUM_LOC(start + skb->csum_offset);\r\n}\r\n}\r\nstatic void txq_stop(struct sge_eth_txq *txq)\r\n{\r\nnetif_tx_stop_queue(txq->txq);\r\ntxq->q.stops++;\r\n}\r\nstatic inline void txq_advance(struct sge_txq *tq, unsigned int n)\r\n{\r\ntq->in_use += n;\r\ntq->pidx += n;\r\nif (tq->pidx >= tq->size)\r\ntq->pidx -= tq->size;\r\n}\r\nint t4vf_eth_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nu32 wr_mid;\r\nu64 cntrl, *end;\r\nint qidx, credits;\r\nunsigned int flits, ndesc;\r\nstruct adapter *adapter;\r\nstruct sge_eth_txq *txq;\r\nconst struct port_info *pi;\r\nstruct fw_eth_tx_pkt_vm_wr *wr;\r\nstruct cpl_tx_pkt_core *cpl;\r\nconst struct skb_shared_info *ssi;\r\ndma_addr_t addr[MAX_SKB_FRAGS + 1];\r\nconst size_t fw_hdr_copy_len = (sizeof(wr->ethmacdst) +\r\nsizeof(wr->ethmacsrc) +\r\nsizeof(wr->ethtype) +\r\nsizeof(wr->vlantci));\r\nif (unlikely(skb->len < fw_hdr_copy_len))\r\ngoto out_free;\r\npi = netdev_priv(dev);\r\nadapter = pi->adapter;\r\nqidx = skb_get_queue_mapping(skb);\r\nBUG_ON(qidx >= pi->nqsets);\r\ntxq = &adapter->sge.ethtxq[pi->first_qset + qidx];\r\nreclaim_completed_tx(adapter, &txq->q, true);\r\nflits = calc_tx_flits(skb);\r\nndesc = flits_to_desc(flits);\r\ncredits = txq_avail(&txq->q) - ndesc;\r\nif (unlikely(credits < 0)) {\r\ntxq_stop(txq);\r\ndev_err(adapter->pdev_dev,\r\n"%s: TX ring %u full while queue awake!\n",\r\ndev->name, qidx);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nif (!is_eth_imm(skb) &&\r\nunlikely(map_skb(adapter->pdev_dev, skb, addr) < 0)) {\r\ntxq->mapping_err++;\r\ngoto out_free;\r\n}\r\nwr_mid = FW_WR_LEN16(DIV_ROUND_UP(flits, 2));\r\nif (unlikely(credits < ETHTXQ_STOP_THRES)) {\r\ntxq_stop(txq);\r\nwr_mid |= FW_WR_EQUEQ | FW_WR_EQUIQ;\r\n}\r\nBUG_ON(DIV_ROUND_UP(ETHTXQ_MAX_HDR, TXD_PER_EQ_UNIT) > 1);\r\nwr = (void *)&txq->q.desc[txq->q.pidx];\r\nwr->equiq_to_len16 = cpu_to_be32(wr_mid);\r\nwr->r3[0] = cpu_to_be64(0);\r\nwr->r3[1] = cpu_to_be64(0);\r\nskb_copy_from_linear_data(skb, (void *)wr->ethmacdst, fw_hdr_copy_len);\r\nend = (u64 *)wr + flits;\r\nssi = skb_shinfo(skb);\r\nif (ssi->gso_size) {\r\nstruct cpl_tx_pkt_lso_core *lso = (void *)(wr + 1);\r\nbool v6 = (ssi->gso_type & SKB_GSO_TCPV6) != 0;\r\nint l3hdr_len = skb_network_header_len(skb);\r\nint eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;\r\nwr->op_immdlen =\r\ncpu_to_be32(FW_WR_OP(FW_ETH_TX_PKT_VM_WR) |\r\nFW_WR_IMMDLEN(sizeof(*lso) +\r\nsizeof(*cpl)));\r\nlso->lso_ctrl =\r\ncpu_to_be32(LSO_OPCODE(CPL_TX_PKT_LSO) |\r\nLSO_FIRST_SLICE |\r\nLSO_LAST_SLICE |\r\nLSO_IPV6(v6) |\r\nLSO_ETHHDR_LEN(eth_xtra_len/4) |\r\nLSO_IPHDR_LEN(l3hdr_len/4) |\r\nLSO_TCPHDR_LEN(tcp_hdr(skb)->doff));\r\nlso->ipid_ofst = cpu_to_be16(0);\r\nlso->mss = cpu_to_be16(ssi->gso_size);\r\nlso->seqno_offset = cpu_to_be32(0);\r\nlso->len = cpu_to_be32(skb->len);\r\ncpl = (void *)(lso + 1);\r\ncntrl = (TXPKT_CSUM_TYPE(v6 ? TX_CSUM_TCPIP6 : TX_CSUM_TCPIP) |\r\nTXPKT_IPHDR_LEN(l3hdr_len) |\r\nTXPKT_ETHHDR_LEN(eth_xtra_len));\r\ntxq->tso++;\r\ntxq->tx_cso += ssi->gso_segs;\r\n} else {\r\nint len;\r\nlen = is_eth_imm(skb) ? skb->len + sizeof(*cpl) : sizeof(*cpl);\r\nwr->op_immdlen =\r\ncpu_to_be32(FW_WR_OP(FW_ETH_TX_PKT_VM_WR) |\r\nFW_WR_IMMDLEN(len));\r\ncpl = (void *)(wr + 1);\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\ncntrl = hwcsum(skb) | TXPKT_IPCSUM_DIS;\r\ntxq->tx_cso++;\r\n} else\r\ncntrl = TXPKT_L4CSUM_DIS | TXPKT_IPCSUM_DIS;\r\n}\r\nif (vlan_tx_tag_present(skb)) {\r\ntxq->vlan_ins++;\r\ncntrl |= TXPKT_VLAN_VLD | TXPKT_VLAN(vlan_tx_tag_get(skb));\r\n}\r\ncpl->ctrl0 = cpu_to_be32(TXPKT_OPCODE(CPL_TX_PKT_XT) |\r\nTXPKT_INTF(pi->port_id) |\r\nTXPKT_PF(0));\r\ncpl->pack = cpu_to_be16(0);\r\ncpl->len = cpu_to_be16(skb->len);\r\ncpl->ctrl1 = cpu_to_be64(cntrl);\r\n#ifdef T4_TRACE\r\nT4_TRACE5(adapter->tb[txq->q.cntxt_id & 7],\r\n"eth_xmit: ndesc %u, credits %u, pidx %u, len %u, frags %u",\r\nndesc, credits, txq->q.pidx, skb->len, ssi->nr_frags);\r\n#endif\r\nif (is_eth_imm(skb)) {\r\ninline_tx_skb(skb, &txq->q, cpl + 1);\r\ndev_kfree_skb(skb);\r\n} else {\r\nstruct ulptx_sgl *sgl = (struct ulptx_sgl *)(cpl + 1);\r\nstruct sge_txq *tq = &txq->q;\r\nint last_desc;\r\nif (unlikely((void *)sgl == (void *)tq->stat)) {\r\nsgl = (void *)tq->desc;\r\nend = ((void *)tq->desc + ((void *)end - (void *)tq->stat));\r\n}\r\nwrite_sgl(skb, tq, sgl, end, 0, addr);\r\nskb_orphan(skb);\r\nlast_desc = tq->pidx + ndesc - 1;\r\nif (last_desc >= tq->size)\r\nlast_desc -= tq->size;\r\ntq->sdesc[last_desc].skb = skb;\r\ntq->sdesc[last_desc].sgl = sgl;\r\n}\r\ntxq_advance(&txq->q, ndesc);\r\ndev->trans_start = jiffies;\r\nring_tx_db(adapter, &txq->q, ndesc);\r\nreturn NETDEV_TX_OK;\r\nout_free:\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic inline void copy_frags(struct sk_buff *skb,\r\nconst struct pkt_gl *gl,\r\nunsigned int offset)\r\n{\r\nint i;\r\n__skb_fill_page_desc(skb, 0, gl->frags[0].page,\r\ngl->frags[0].offset + offset,\r\ngl->frags[0].size - offset);\r\nskb_shinfo(skb)->nr_frags = gl->nfrags;\r\nfor (i = 1; i < gl->nfrags; i++)\r\n__skb_fill_page_desc(skb, i, gl->frags[i].page,\r\ngl->frags[i].offset,\r\ngl->frags[i].size);\r\nget_page(gl->frags[gl->nfrags - 1].page);\r\n}\r\nstruct sk_buff *t4vf_pktgl_to_skb(const struct pkt_gl *gl,\r\nunsigned int skb_len, unsigned int pull_len)\r\n{\r\nstruct sk_buff *skb;\r\nif (gl->tot_len <= RX_COPY_THRES) {\r\nskb = alloc_skb(gl->tot_len, GFP_ATOMIC);\r\nif (unlikely(!skb))\r\ngoto out;\r\n__skb_put(skb, gl->tot_len);\r\nskb_copy_to_linear_data(skb, gl->va, gl->tot_len);\r\n} else {\r\nskb = alloc_skb(skb_len, GFP_ATOMIC);\r\nif (unlikely(!skb))\r\ngoto out;\r\n__skb_put(skb, pull_len);\r\nskb_copy_to_linear_data(skb, gl->va, pull_len);\r\ncopy_frags(skb, gl, pull_len);\r\nskb->len = gl->tot_len;\r\nskb->data_len = skb->len - pull_len;\r\nskb->truesize += skb->data_len;\r\n}\r\nout:\r\nreturn skb;\r\n}\r\nvoid t4vf_pktgl_free(const struct pkt_gl *gl)\r\n{\r\nint frag;\r\nfrag = gl->nfrags - 1;\r\nwhile (frag--)\r\nput_page(gl->frags[frag].page);\r\n}\r\nstatic void do_gro(struct sge_eth_rxq *rxq, const struct pkt_gl *gl,\r\nconst struct cpl_rx_pkt *pkt)\r\n{\r\nint ret;\r\nstruct sk_buff *skb;\r\nskb = napi_get_frags(&rxq->rspq.napi);\r\nif (unlikely(!skb)) {\r\nt4vf_pktgl_free(gl);\r\nrxq->stats.rx_drops++;\r\nreturn;\r\n}\r\ncopy_frags(skb, gl, PKTSHIFT);\r\nskb->len = gl->tot_len - PKTSHIFT;\r\nskb->data_len = skb->len;\r\nskb->truesize += skb->data_len;\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb_record_rx_queue(skb, rxq->rspq.idx);\r\nif (pkt->vlan_ex) {\r\n__vlan_hwaccel_put_tag(skb, cpu_to_be16(ETH_P_8021Q),\r\nbe16_to_cpu(pkt->vlan));\r\nrxq->stats.vlan_ex++;\r\n}\r\nret = napi_gro_frags(&rxq->rspq.napi);\r\nif (ret == GRO_HELD)\r\nrxq->stats.lro_pkts++;\r\nelse if (ret == GRO_MERGED || ret == GRO_MERGED_FREE)\r\nrxq->stats.lro_merged++;\r\nrxq->stats.pkts++;\r\nrxq->stats.rx_cso++;\r\n}\r\nint t4vf_ethrx_handler(struct sge_rspq *rspq, const __be64 *rsp,\r\nconst struct pkt_gl *gl)\r\n{\r\nstruct sk_buff *skb;\r\nconst struct cpl_rx_pkt *pkt = (void *)rsp;\r\nbool csum_ok = pkt->csum_calc && !pkt->err_vec;\r\nstruct sge_eth_rxq *rxq = container_of(rspq, struct sge_eth_rxq, rspq);\r\nif ((pkt->l2info & cpu_to_be32(RXF_TCP)) &&\r\n(rspq->netdev->features & NETIF_F_GRO) && csum_ok &&\r\n!pkt->ip_frag) {\r\ndo_gro(rxq, gl, pkt);\r\nreturn 0;\r\n}\r\nskb = t4vf_pktgl_to_skb(gl, RX_SKB_LEN, RX_PULL_LEN);\r\nif (unlikely(!skb)) {\r\nt4vf_pktgl_free(gl);\r\nrxq->stats.rx_drops++;\r\nreturn 0;\r\n}\r\n__skb_pull(skb, PKTSHIFT);\r\nskb->protocol = eth_type_trans(skb, rspq->netdev);\r\nskb_record_rx_queue(skb, rspq->idx);\r\nrxq->stats.pkts++;\r\nif (csum_ok && (rspq->netdev->features & NETIF_F_RXCSUM) &&\r\n!pkt->err_vec && (be32_to_cpu(pkt->l2info) & (RXF_UDP|RXF_TCP))) {\r\nif (!pkt->ip_frag)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nelse {\r\n__sum16 c = (__force __sum16)pkt->csum;\r\nskb->csum = csum_unfold(c);\r\nskb->ip_summed = CHECKSUM_COMPLETE;\r\n}\r\nrxq->stats.rx_cso++;\r\n} else\r\nskb_checksum_none_assert(skb);\r\nif (pkt->vlan_ex) {\r\nrxq->stats.vlan_ex++;\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), be16_to_cpu(pkt->vlan));\r\n}\r\nnetif_receive_skb(skb);\r\nreturn 0;\r\n}\r\nstatic inline bool is_new_response(const struct rsp_ctrl *rc,\r\nconst struct sge_rspq *rspq)\r\n{\r\nreturn RSPD_GEN(rc->type_gen) == rspq->gen;\r\n}\r\nstatic void restore_rx_bufs(const struct pkt_gl *gl, struct sge_fl *fl,\r\nint frags)\r\n{\r\nstruct rx_sw_desc *sdesc;\r\nwhile (frags--) {\r\nif (fl->cidx == 0)\r\nfl->cidx = fl->size - 1;\r\nelse\r\nfl->cidx--;\r\nsdesc = &fl->sdesc[fl->cidx];\r\nsdesc->page = gl->frags[frags].page;\r\nsdesc->dma_addr |= RX_UNMAPPED_BUF;\r\nfl->avail++;\r\n}\r\n}\r\nstatic inline void rspq_next(struct sge_rspq *rspq)\r\n{\r\nrspq->cur_desc = (void *)rspq->cur_desc + rspq->iqe_len;\r\nif (unlikely(++rspq->cidx == rspq->size)) {\r\nrspq->cidx = 0;\r\nrspq->gen ^= 1;\r\nrspq->cur_desc = rspq->desc;\r\n}\r\n}\r\nint process_responses(struct sge_rspq *rspq, int budget)\r\n{\r\nstruct sge_eth_rxq *rxq = container_of(rspq, struct sge_eth_rxq, rspq);\r\nint budget_left = budget;\r\nwhile (likely(budget_left)) {\r\nint ret, rsp_type;\r\nconst struct rsp_ctrl *rc;\r\nrc = (void *)rspq->cur_desc + (rspq->iqe_len - sizeof(*rc));\r\nif (!is_new_response(rc, rspq))\r\nbreak;\r\nrmb();\r\nrsp_type = RSPD_TYPE(rc->type_gen);\r\nif (likely(rsp_type == RSP_TYPE_FLBUF)) {\r\nstruct page_frag *fp;\r\nstruct pkt_gl gl;\r\nconst struct rx_sw_desc *sdesc;\r\nu32 bufsz, frag;\r\nu32 len = be32_to_cpu(rc->pldbuflen_qid);\r\nif (len & RSPD_NEWBUF) {\r\nif (likely(rspq->offset > 0)) {\r\nfree_rx_bufs(rspq->adapter, &rxq->fl,\r\n1);\r\nrspq->offset = 0;\r\n}\r\nlen = RSPD_LEN(len);\r\n}\r\ngl.tot_len = len;\r\nfor (frag = 0, fp = gl.frags; ; frag++, fp++) {\r\nBUG_ON(frag >= MAX_SKB_FRAGS);\r\nBUG_ON(rxq->fl.avail == 0);\r\nsdesc = &rxq->fl.sdesc[rxq->fl.cidx];\r\nbufsz = get_buf_size(sdesc);\r\nfp->page = sdesc->page;\r\nfp->offset = rspq->offset;\r\nfp->size = min(bufsz, len);\r\nlen -= fp->size;\r\nif (!len)\r\nbreak;\r\nunmap_rx_buf(rspq->adapter, &rxq->fl);\r\n}\r\ngl.nfrags = frag+1;\r\ndma_sync_single_for_cpu(rspq->adapter->pdev_dev,\r\nget_buf_addr(sdesc),\r\nfp->size, DMA_FROM_DEVICE);\r\ngl.va = (page_address(gl.frags[0].page) +\r\ngl.frags[0].offset);\r\nprefetch(gl.va);\r\nret = rspq->handler(rspq, rspq->cur_desc, &gl);\r\nif (likely(ret == 0))\r\nrspq->offset += ALIGN(fp->size, FL_ALIGN);\r\nelse\r\nrestore_rx_bufs(&gl, &rxq->fl, frag);\r\n} else if (likely(rsp_type == RSP_TYPE_CPL)) {\r\nret = rspq->handler(rspq, rspq->cur_desc, NULL);\r\n} else {\r\nWARN_ON(rsp_type > RSP_TYPE_CPL);\r\nret = 0;\r\n}\r\nif (unlikely(ret)) {\r\nconst int NOMEM_TIMER_IDX = SGE_NTIMERS-1;\r\nrspq->next_intr_params =\r\nQINTR_TIMER_IDX(NOMEM_TIMER_IDX);\r\nbreak;\r\n}\r\nrspq_next(rspq);\r\nbudget_left--;\r\n}\r\nif (rspq->offset >= 0 &&\r\nrxq->fl.size - rxq->fl.avail >= 2*FL_PER_EQ_UNIT)\r\n__refill_fl(rspq->adapter, &rxq->fl);\r\nreturn budget - budget_left;\r\n}\r\nstatic int napi_rx_handler(struct napi_struct *napi, int budget)\r\n{\r\nunsigned int intr_params;\r\nstruct sge_rspq *rspq = container_of(napi, struct sge_rspq, napi);\r\nint work_done = process_responses(rspq, budget);\r\nif (likely(work_done < budget)) {\r\nnapi_complete(napi);\r\nintr_params = rspq->next_intr_params;\r\nrspq->next_intr_params = rspq->intr_params;\r\n} else\r\nintr_params = QINTR_TIMER_IDX(SGE_TIMER_UPD_CIDX);\r\nif (unlikely(work_done == 0))\r\nrspq->unhandled_irqs++;\r\nt4_write_reg(rspq->adapter,\r\nT4VF_SGE_BASE_ADDR + SGE_VF_GTS,\r\nCIDXINC(work_done) |\r\nINGRESSQID((u32)rspq->cntxt_id) |\r\nSEINTARM(intr_params));\r\nreturn work_done;\r\n}\r\nirqreturn_t t4vf_sge_intr_msix(int irq, void *cookie)\r\n{\r\nstruct sge_rspq *rspq = cookie;\r\nnapi_schedule(&rspq->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic unsigned int process_intrq(struct adapter *adapter)\r\n{\r\nstruct sge *s = &adapter->sge;\r\nstruct sge_rspq *intrq = &s->intrq;\r\nunsigned int work_done;\r\nspin_lock(&adapter->sge.intrq_lock);\r\nfor (work_done = 0; ; work_done++) {\r\nconst struct rsp_ctrl *rc;\r\nunsigned int qid, iq_idx;\r\nstruct sge_rspq *rspq;\r\nrc = (void *)intrq->cur_desc + (intrq->iqe_len - sizeof(*rc));\r\nif (!is_new_response(rc, intrq))\r\nbreak;\r\nrmb();\r\nif (unlikely(RSPD_TYPE(rc->type_gen) != RSP_TYPE_INTR)) {\r\ndev_err(adapter->pdev_dev,\r\n"Unexpected INTRQ response type %d\n",\r\nRSPD_TYPE(rc->type_gen));\r\ncontinue;\r\n}\r\nqid = RSPD_QID(be32_to_cpu(rc->pldbuflen_qid));\r\niq_idx = IQ_IDX(s, qid);\r\nif (unlikely(iq_idx >= MAX_INGQ)) {\r\ndev_err(adapter->pdev_dev,\r\n"Ingress QID %d out of range\n", qid);\r\ncontinue;\r\n}\r\nrspq = s->ingr_map[iq_idx];\r\nif (unlikely(rspq == NULL)) {\r\ndev_err(adapter->pdev_dev,\r\n"Ingress QID %d RSPQ=NULL\n", qid);\r\ncontinue;\r\n}\r\nif (unlikely(rspq->abs_id != qid)) {\r\ndev_err(adapter->pdev_dev,\r\n"Ingress QID %d refers to RSPQ %d\n",\r\nqid, rspq->abs_id);\r\ncontinue;\r\n}\r\nnapi_schedule(&rspq->napi);\r\nrspq_next(intrq);\r\n}\r\nt4_write_reg(adapter, T4VF_SGE_BASE_ADDR + SGE_VF_GTS,\r\nCIDXINC(work_done) |\r\nINGRESSQID(intrq->cntxt_id) |\r\nSEINTARM(intrq->intr_params));\r\nspin_unlock(&adapter->sge.intrq_lock);\r\nreturn work_done;\r\n}\r\nirqreturn_t t4vf_intr_msi(int irq, void *cookie)\r\n{\r\nstruct adapter *adapter = cookie;\r\nprocess_intrq(adapter);\r\nreturn IRQ_HANDLED;\r\n}\r\nirq_handler_t t4vf_intr_handler(struct adapter *adapter)\r\n{\r\nBUG_ON((adapter->flags & (USING_MSIX|USING_MSI)) == 0);\r\nif (adapter->flags & USING_MSIX)\r\nreturn t4vf_sge_intr_msix;\r\nelse\r\nreturn t4vf_intr_msi;\r\n}\r\nstatic void sge_rx_timer_cb(unsigned long data)\r\n{\r\nstruct adapter *adapter = (struct adapter *)data;\r\nstruct sge *s = &adapter->sge;\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(s->starving_fl); i++) {\r\nunsigned long m;\r\nfor (m = s->starving_fl[i]; m; m &= m - 1) {\r\nunsigned int id = __ffs(m) + i * BITS_PER_LONG;\r\nstruct sge_fl *fl = s->egr_map[id];\r\nclear_bit(id, s->starving_fl);\r\nsmp_mb__after_clear_bit();\r\nif (fl_starving(fl)) {\r\nstruct sge_eth_rxq *rxq;\r\nrxq = container_of(fl, struct sge_eth_rxq, fl);\r\nif (napi_reschedule(&rxq->rspq.napi))\r\nfl->starving++;\r\nelse\r\nset_bit(id, s->starving_fl);\r\n}\r\n}\r\n}\r\nmod_timer(&s->rx_timer, jiffies + RX_QCHECK_PERIOD);\r\n}\r\nstatic void sge_tx_timer_cb(unsigned long data)\r\n{\r\nstruct adapter *adapter = (struct adapter *)data;\r\nstruct sge *s = &adapter->sge;\r\nunsigned int i, budget;\r\nbudget = MAX_TIMER_TX_RECLAIM;\r\ni = s->ethtxq_rover;\r\ndo {\r\nstruct sge_eth_txq *txq = &s->ethtxq[i];\r\nif (reclaimable(&txq->q) && __netif_tx_trylock(txq->txq)) {\r\nint avail = reclaimable(&txq->q);\r\nif (avail > budget)\r\navail = budget;\r\nfree_tx_desc(adapter, &txq->q, avail, true);\r\ntxq->q.in_use -= avail;\r\n__netif_tx_unlock(txq->txq);\r\nbudget -= avail;\r\nif (!budget)\r\nbreak;\r\n}\r\ni++;\r\nif (i >= s->ethqsets)\r\ni = 0;\r\n} while (i != s->ethtxq_rover);\r\ns->ethtxq_rover = i;\r\nmod_timer(&s->tx_timer, jiffies + (budget ? TX_QCHECK_PERIOD : 2));\r\n}\r\nint t4vf_sge_alloc_rxq(struct adapter *adapter, struct sge_rspq *rspq,\r\nbool iqasynch, struct net_device *dev,\r\nint intr_dest,\r\nstruct sge_fl *fl, rspq_handler_t hnd)\r\n{\r\nstruct port_info *pi = netdev_priv(dev);\r\nstruct fw_iq_cmd cmd, rpl;\r\nint ret, iqandst, flsz = 0;\r\nif ((adapter->flags & USING_MSI) && rspq != &adapter->sge.intrq) {\r\niqandst = SGE_INTRDST_IQ;\r\nintr_dest = adapter->sge.intrq.abs_id;\r\n} else\r\niqandst = SGE_INTRDST_PCI;\r\nrspq->size = roundup(rspq->size, 16);\r\nrspq->desc = alloc_ring(adapter->pdev_dev, rspq->size, rspq->iqe_len,\r\n0, &rspq->phys_addr, NULL, 0);\r\nif (!rspq->desc)\r\nreturn -ENOMEM;\r\nmemset(&cmd, 0, sizeof(cmd));\r\ncmd.op_to_vfn = cpu_to_be32(FW_CMD_OP(FW_IQ_CMD) |\r\nFW_CMD_REQUEST |\r\nFW_CMD_WRITE |\r\nFW_CMD_EXEC);\r\ncmd.alloc_to_len16 = cpu_to_be32(FW_IQ_CMD_ALLOC |\r\nFW_IQ_CMD_IQSTART(1) |\r\nFW_LEN16(cmd));\r\ncmd.type_to_iqandstindex =\r\ncpu_to_be32(FW_IQ_CMD_TYPE(FW_IQ_TYPE_FL_INT_CAP) |\r\nFW_IQ_CMD_IQASYNCH(iqasynch) |\r\nFW_IQ_CMD_VIID(pi->viid) |\r\nFW_IQ_CMD_IQANDST(iqandst) |\r\nFW_IQ_CMD_IQANUS(1) |\r\nFW_IQ_CMD_IQANUD(SGE_UPDATEDEL_INTR) |\r\nFW_IQ_CMD_IQANDSTINDEX(intr_dest));\r\ncmd.iqdroprss_to_iqesize =\r\ncpu_to_be16(FW_IQ_CMD_IQPCIECH(pi->port_id) |\r\nFW_IQ_CMD_IQGTSMODE |\r\nFW_IQ_CMD_IQINTCNTTHRESH(rspq->pktcnt_idx) |\r\nFW_IQ_CMD_IQESIZE(ilog2(rspq->iqe_len) - 4));\r\ncmd.iqsize = cpu_to_be16(rspq->size);\r\ncmd.iqaddr = cpu_to_be64(rspq->phys_addr);\r\nif (fl) {\r\nfl->size = roundup(fl->size, FL_PER_EQ_UNIT);\r\nfl->desc = alloc_ring(adapter->pdev_dev, fl->size,\r\nsizeof(__be64), sizeof(struct rx_sw_desc),\r\n&fl->addr, &fl->sdesc, STAT_LEN);\r\nif (!fl->desc) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nflsz = (fl->size / FL_PER_EQ_UNIT +\r\nSTAT_LEN / EQ_UNIT);\r\ncmd.iqns_to_fl0congen =\r\ncpu_to_be32(\r\nFW_IQ_CMD_FL0HOSTFCMODE(SGE_HOSTFCMODE_NONE) |\r\nFW_IQ_CMD_FL0PACKEN(1) |\r\nFW_IQ_CMD_FL0PADEN(1));\r\ncmd.fl0dcaen_to_fl0cidxfthresh =\r\ncpu_to_be16(\r\nFW_IQ_CMD_FL0FBMIN(SGE_FETCHBURSTMIN_64B) |\r\nFW_IQ_CMD_FL0FBMAX(SGE_FETCHBURSTMAX_512B));\r\ncmd.fl0size = cpu_to_be16(flsz);\r\ncmd.fl0addr = cpu_to_be64(fl->addr);\r\n}\r\nret = t4vf_wr_mbox(adapter, &cmd, sizeof(cmd), &rpl);\r\nif (ret)\r\ngoto err;\r\nnetif_napi_add(dev, &rspq->napi, napi_rx_handler, 64);\r\nrspq->cur_desc = rspq->desc;\r\nrspq->cidx = 0;\r\nrspq->gen = 1;\r\nrspq->next_intr_params = rspq->intr_params;\r\nrspq->cntxt_id = be16_to_cpu(rpl.iqid);\r\nrspq->abs_id = be16_to_cpu(rpl.physiqid);\r\nrspq->size--;\r\nrspq->adapter = adapter;\r\nrspq->netdev = dev;\r\nrspq->handler = hnd;\r\nrspq->offset = fl ? 0 : -1;\r\nif (fl) {\r\nfl->cntxt_id = be16_to_cpu(rpl.fl0id);\r\nfl->avail = 0;\r\nfl->pend_cred = 0;\r\nfl->pidx = 0;\r\nfl->cidx = 0;\r\nfl->alloc_failed = 0;\r\nfl->large_alloc_failed = 0;\r\nfl->starving = 0;\r\nrefill_fl(adapter, fl, fl_cap(fl), GFP_KERNEL);\r\n}\r\nreturn 0;\r\nerr:\r\nif (rspq->desc) {\r\ndma_free_coherent(adapter->pdev_dev, rspq->size * rspq->iqe_len,\r\nrspq->desc, rspq->phys_addr);\r\nrspq->desc = NULL;\r\n}\r\nif (fl && fl->desc) {\r\nkfree(fl->sdesc);\r\nfl->sdesc = NULL;\r\ndma_free_coherent(adapter->pdev_dev, flsz * EQ_UNIT,\r\nfl->desc, fl->addr);\r\nfl->desc = NULL;\r\n}\r\nreturn ret;\r\n}\r\nint t4vf_sge_alloc_eth_txq(struct adapter *adapter, struct sge_eth_txq *txq,\r\nstruct net_device *dev, struct netdev_queue *devq,\r\nunsigned int iqid)\r\n{\r\nint ret, nentries;\r\nstruct fw_eq_eth_cmd cmd, rpl;\r\nstruct port_info *pi = netdev_priv(dev);\r\nnentries = txq->q.size + STAT_LEN / sizeof(struct tx_desc);\r\ntxq->q.desc = alloc_ring(adapter->pdev_dev, txq->q.size,\r\nsizeof(struct tx_desc),\r\nsizeof(struct tx_sw_desc),\r\n&txq->q.phys_addr, &txq->q.sdesc, STAT_LEN);\r\nif (!txq->q.desc)\r\nreturn -ENOMEM;\r\nmemset(&cmd, 0, sizeof(cmd));\r\ncmd.op_to_vfn = cpu_to_be32(FW_CMD_OP(FW_EQ_ETH_CMD) |\r\nFW_CMD_REQUEST |\r\nFW_CMD_WRITE |\r\nFW_CMD_EXEC);\r\ncmd.alloc_to_len16 = cpu_to_be32(FW_EQ_ETH_CMD_ALLOC |\r\nFW_EQ_ETH_CMD_EQSTART |\r\nFW_LEN16(cmd));\r\ncmd.viid_pkd = cpu_to_be32(FW_EQ_ETH_CMD_VIID(pi->viid));\r\ncmd.fetchszm_to_iqid =\r\ncpu_to_be32(FW_EQ_ETH_CMD_HOSTFCMODE(SGE_HOSTFCMODE_STPG) |\r\nFW_EQ_ETH_CMD_PCIECHN(pi->port_id) |\r\nFW_EQ_ETH_CMD_IQID(iqid));\r\ncmd.dcaen_to_eqsize =\r\ncpu_to_be32(FW_EQ_ETH_CMD_FBMIN(SGE_FETCHBURSTMIN_64B) |\r\nFW_EQ_ETH_CMD_FBMAX(SGE_FETCHBURSTMAX_512B) |\r\nFW_EQ_ETH_CMD_CIDXFTHRESH(SGE_CIDXFLUSHTHRESH_32) |\r\nFW_EQ_ETH_CMD_EQSIZE(nentries));\r\ncmd.eqaddr = cpu_to_be64(txq->q.phys_addr);\r\nret = t4vf_wr_mbox(adapter, &cmd, sizeof(cmd), &rpl);\r\nif (ret) {\r\nkfree(txq->q.sdesc);\r\ntxq->q.sdesc = NULL;\r\ndma_free_coherent(adapter->pdev_dev,\r\nnentries * sizeof(struct tx_desc),\r\ntxq->q.desc, txq->q.phys_addr);\r\ntxq->q.desc = NULL;\r\nreturn ret;\r\n}\r\ntxq->q.in_use = 0;\r\ntxq->q.cidx = 0;\r\ntxq->q.pidx = 0;\r\ntxq->q.stat = (void *)&txq->q.desc[txq->q.size];\r\ntxq->q.cntxt_id = FW_EQ_ETH_CMD_EQID_GET(be32_to_cpu(rpl.eqid_pkd));\r\ntxq->q.abs_id =\r\nFW_EQ_ETH_CMD_PHYSEQID_GET(be32_to_cpu(rpl.physeqid_pkd));\r\ntxq->txq = devq;\r\ntxq->tso = 0;\r\ntxq->tx_cso = 0;\r\ntxq->vlan_ins = 0;\r\ntxq->q.stops = 0;\r\ntxq->q.restarts = 0;\r\ntxq->mapping_err = 0;\r\nreturn 0;\r\n}\r\nstatic void free_txq(struct adapter *adapter, struct sge_txq *tq)\r\n{\r\ndma_free_coherent(adapter->pdev_dev,\r\ntq->size * sizeof(*tq->desc) + STAT_LEN,\r\ntq->desc, tq->phys_addr);\r\ntq->cntxt_id = 0;\r\ntq->sdesc = NULL;\r\ntq->desc = NULL;\r\n}\r\nstatic void free_rspq_fl(struct adapter *adapter, struct sge_rspq *rspq,\r\nstruct sge_fl *fl)\r\n{\r\nunsigned int flid = fl ? fl->cntxt_id : 0xffff;\r\nt4vf_iq_free(adapter, FW_IQ_TYPE_FL_INT_CAP,\r\nrspq->cntxt_id, flid, 0xffff);\r\ndma_free_coherent(adapter->pdev_dev, (rspq->size + 1) * rspq->iqe_len,\r\nrspq->desc, rspq->phys_addr);\r\nnetif_napi_del(&rspq->napi);\r\nrspq->netdev = NULL;\r\nrspq->cntxt_id = 0;\r\nrspq->abs_id = 0;\r\nrspq->desc = NULL;\r\nif (fl) {\r\nfree_rx_bufs(adapter, fl, fl->avail);\r\ndma_free_coherent(adapter->pdev_dev,\r\nfl->size * sizeof(*fl->desc) + STAT_LEN,\r\nfl->desc, fl->addr);\r\nkfree(fl->sdesc);\r\nfl->sdesc = NULL;\r\nfl->cntxt_id = 0;\r\nfl->desc = NULL;\r\n}\r\n}\r\nvoid t4vf_free_sge_resources(struct adapter *adapter)\r\n{\r\nstruct sge *s = &adapter->sge;\r\nstruct sge_eth_rxq *rxq = s->ethrxq;\r\nstruct sge_eth_txq *txq = s->ethtxq;\r\nstruct sge_rspq *evtq = &s->fw_evtq;\r\nstruct sge_rspq *intrq = &s->intrq;\r\nint qs;\r\nfor (qs = 0; qs < adapter->sge.ethqsets; qs++, rxq++, txq++) {\r\nif (rxq->rspq.desc)\r\nfree_rspq_fl(adapter, &rxq->rspq, &rxq->fl);\r\nif (txq->q.desc) {\r\nt4vf_eth_eq_free(adapter, txq->q.cntxt_id);\r\nfree_tx_desc(adapter, &txq->q, txq->q.in_use, true);\r\nkfree(txq->q.sdesc);\r\nfree_txq(adapter, &txq->q);\r\n}\r\n}\r\nif (evtq->desc)\r\nfree_rspq_fl(adapter, evtq, NULL);\r\nif (intrq->desc)\r\nfree_rspq_fl(adapter, intrq, NULL);\r\n}\r\nvoid t4vf_sge_start(struct adapter *adapter)\r\n{\r\nadapter->sge.ethtxq_rover = 0;\r\nmod_timer(&adapter->sge.rx_timer, jiffies + RX_QCHECK_PERIOD);\r\nmod_timer(&adapter->sge.tx_timer, jiffies + TX_QCHECK_PERIOD);\r\n}\r\nvoid t4vf_sge_stop(struct adapter *adapter)\r\n{\r\nstruct sge *s = &adapter->sge;\r\nif (s->rx_timer.function)\r\ndel_timer_sync(&s->rx_timer);\r\nif (s->tx_timer.function)\r\ndel_timer_sync(&s->tx_timer);\r\n}\r\nint t4vf_sge_init(struct adapter *adapter)\r\n{\r\nstruct sge_params *sge_params = &adapter->params.sge;\r\nu32 fl0 = sge_params->sge_fl_buffer_size[0];\r\nu32 fl1 = sge_params->sge_fl_buffer_size[1];\r\nstruct sge *s = &adapter->sge;\r\nif (fl0 != PAGE_SIZE || (fl1 != 0 && fl1 <= fl0)) {\r\ndev_err(adapter->pdev_dev, "bad SGE FL buffer sizes [%d, %d]\n",\r\nfl0, fl1);\r\nreturn -EINVAL;\r\n}\r\nif ((sge_params->sge_control & RXPKTCPLMODE_MASK) == 0) {\r\ndev_err(adapter->pdev_dev, "bad SGE CPL MODE\n");\r\nreturn -EINVAL;\r\n}\r\nif (fl1)\r\nFL_PG_ORDER = ilog2(fl1) - PAGE_SHIFT;\r\nSTAT_LEN = ((sge_params->sge_control & EGRSTATUSPAGESIZE_MASK)\r\n? 128 : 64);\r\nPKTSHIFT = PKTSHIFT_GET(sge_params->sge_control);\r\nFL_ALIGN = 1 << (INGPADBOUNDARY_GET(sge_params->sge_control) +\r\nSGE_INGPADBOUNDARY_SHIFT);\r\nsetup_timer(&s->rx_timer, sge_rx_timer_cb, (unsigned long)adapter);\r\nsetup_timer(&s->tx_timer, sge_tx_timer_cb, (unsigned long)adapter);\r\nspin_lock_init(&s->intrq_lock);\r\nreturn 0;\r\n}
