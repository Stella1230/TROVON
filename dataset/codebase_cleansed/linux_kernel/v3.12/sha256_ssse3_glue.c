static int sha256_ssse3_init(struct shash_desc *desc)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nsctx->state[0] = SHA256_H0;\r\nsctx->state[1] = SHA256_H1;\r\nsctx->state[2] = SHA256_H2;\r\nsctx->state[3] = SHA256_H3;\r\nsctx->state[4] = SHA256_H4;\r\nsctx->state[5] = SHA256_H5;\r\nsctx->state[6] = SHA256_H6;\r\nsctx->state[7] = SHA256_H7;\r\nsctx->count = 0;\r\nreturn 0;\r\n}\r\nstatic int __sha256_ssse3_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, unsigned int partial)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nunsigned int done = 0;\r\nsctx->count += len;\r\nif (partial) {\r\ndone = SHA256_BLOCK_SIZE - partial;\r\nmemcpy(sctx->buf + partial, data, done);\r\nsha256_transform_asm(sctx->buf, sctx->state, 1);\r\n}\r\nif (len - done >= SHA256_BLOCK_SIZE) {\r\nconst unsigned int rounds = (len - done) / SHA256_BLOCK_SIZE;\r\nsha256_transform_asm(data + done, sctx->state, (u64) rounds);\r\ndone += rounds * SHA256_BLOCK_SIZE;\r\n}\r\nmemcpy(sctx->buf, data + done, len - done);\r\nreturn 0;\r\n}\r\nstatic int sha256_ssse3_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nunsigned int partial = sctx->count % SHA256_BLOCK_SIZE;\r\nint res;\r\nif (partial + len < SHA256_BLOCK_SIZE) {\r\nsctx->count += len;\r\nmemcpy(sctx->buf + partial, data, len);\r\nreturn 0;\r\n}\r\nif (!irq_fpu_usable()) {\r\nres = crypto_sha256_update(desc, data, len);\r\n} else {\r\nkernel_fpu_begin();\r\nres = __sha256_ssse3_update(desc, data, len, partial);\r\nkernel_fpu_end();\r\n}\r\nreturn res;\r\n}\r\nstatic int sha256_ssse3_final(struct shash_desc *desc, u8 *out)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nunsigned int i, index, padlen;\r\n__be32 *dst = (__be32 *)out;\r\n__be64 bits;\r\nstatic const u8 padding[SHA256_BLOCK_SIZE] = { 0x80, };\r\nbits = cpu_to_be64(sctx->count << 3);\r\nindex = sctx->count % SHA256_BLOCK_SIZE;\r\npadlen = (index < 56) ? (56 - index) : ((SHA256_BLOCK_SIZE+56)-index);\r\nif (!irq_fpu_usable()) {\r\ncrypto_sha256_update(desc, padding, padlen);\r\ncrypto_sha256_update(desc, (const u8 *)&bits, sizeof(bits));\r\n} else {\r\nkernel_fpu_begin();\r\nif (padlen <= 56) {\r\nsctx->count += padlen;\r\nmemcpy(sctx->buf + index, padding, padlen);\r\n} else {\r\n__sha256_ssse3_update(desc, padding, padlen, index);\r\n}\r\n__sha256_ssse3_update(desc, (const u8 *)&bits,\r\nsizeof(bits), 56);\r\nkernel_fpu_end();\r\n}\r\nfor (i = 0; i < 8; i++)\r\ndst[i] = cpu_to_be32(sctx->state[i]);\r\nmemset(sctx, 0, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha256_ssse3_export(struct shash_desc *desc, void *out)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nmemcpy(out, sctx, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha256_ssse3_import(struct shash_desc *desc, const void *in)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nmemcpy(sctx, in, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha224_ssse3_init(struct shash_desc *desc)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nsctx->state[0] = SHA224_H0;\r\nsctx->state[1] = SHA224_H1;\r\nsctx->state[2] = SHA224_H2;\r\nsctx->state[3] = SHA224_H3;\r\nsctx->state[4] = SHA224_H4;\r\nsctx->state[5] = SHA224_H5;\r\nsctx->state[6] = SHA224_H6;\r\nsctx->state[7] = SHA224_H7;\r\nsctx->count = 0;\r\nreturn 0;\r\n}\r\nstatic int sha224_ssse3_final(struct shash_desc *desc, u8 *hash)\r\n{\r\nu8 D[SHA256_DIGEST_SIZE];\r\nsha256_ssse3_final(desc, D);\r\nmemcpy(hash, D, SHA224_DIGEST_SIZE);\r\nmemset(D, 0, SHA256_DIGEST_SIZE);\r\nreturn 0;\r\n}\r\nstatic bool __init avx_usable(void)\r\n{\r\nu64 xcr0;\r\nif (!cpu_has_avx || !cpu_has_osxsave)\r\nreturn false;\r\nxcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\r\nif ((xcr0 & (XSTATE_SSE | XSTATE_YMM)) != (XSTATE_SSE | XSTATE_YMM)) {\r\npr_info("AVX detected but unusable.\n");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int __init sha256_ssse3_mod_init(void)\r\n{\r\nif (cpu_has_ssse3)\r\nsha256_transform_asm = sha256_transform_ssse3;\r\n#ifdef CONFIG_AS_AVX\r\nif (avx_usable()) {\r\n#ifdef CONFIG_AS_AVX2\r\nif (boot_cpu_has(X86_FEATURE_AVX2))\r\nsha256_transform_asm = sha256_transform_rorx;\r\nelse\r\n#endif\r\nsha256_transform_asm = sha256_transform_avx;\r\n}\r\n#endif\r\nif (sha256_transform_asm) {\r\n#ifdef CONFIG_AS_AVX\r\nif (sha256_transform_asm == sha256_transform_avx)\r\npr_info("Using AVX optimized SHA-256 implementation\n");\r\n#ifdef CONFIG_AS_AVX2\r\nelse if (sha256_transform_asm == sha256_transform_rorx)\r\npr_info("Using AVX2 optimized SHA-256 implementation\n");\r\n#endif\r\nelse\r\n#endif\r\npr_info("Using SSSE3 optimized SHA-256 implementation\n");\r\nreturn crypto_register_shashes(algs, ARRAY_SIZE(algs));\r\n}\r\npr_info("Neither AVX nor SSSE3 is available/usable.\n");\r\nreturn -ENODEV;\r\n}\r\nstatic void __exit sha256_ssse3_mod_fini(void)\r\n{\r\ncrypto_unregister_shashes(algs, ARRAY_SIZE(algs));\r\n}
