struct bio *bio_alloc_drbd(gfp_t gfp_mask)\r\n{\r\nstruct bio *bio;\r\nif (!drbd_md_io_bio_set)\r\nreturn bio_alloc(gfp_mask, 1);\r\nbio = bio_alloc_bioset(gfp_mask, 1, drbd_md_io_bio_set);\r\nif (!bio)\r\nreturn NULL;\r\nreturn bio;\r\n}\r\nint _get_ldev_if_state(struct drbd_conf *mdev, enum drbd_disk_state mins)\r\n{\r\nint io_allowed;\r\natomic_inc(&mdev->local_cnt);\r\nio_allowed = (mdev->state.disk >= mins);\r\nif (!io_allowed) {\r\nif (atomic_dec_and_test(&mdev->local_cnt))\r\nwake_up(&mdev->misc_wait);\r\n}\r\nreturn io_allowed;\r\n}\r\nvoid tl_release(struct drbd_tconn *tconn, unsigned int barrier_nr,\r\nunsigned int set_size)\r\n{\r\nstruct drbd_request *r;\r\nstruct drbd_request *req = NULL;\r\nint expect_epoch = 0;\r\nint expect_size = 0;\r\nspin_lock_irq(&tconn->req_lock);\r\nlist_for_each_entry(r, &tconn->transfer_log, tl_requests) {\r\nconst unsigned s = r->rq_state;\r\nif (!req) {\r\nif (!(s & RQ_WRITE))\r\ncontinue;\r\nif (!(s & RQ_NET_MASK))\r\ncontinue;\r\nif (s & RQ_NET_DONE)\r\ncontinue;\r\nreq = r;\r\nexpect_epoch = req->epoch;\r\nexpect_size ++;\r\n} else {\r\nif (r->epoch != expect_epoch)\r\nbreak;\r\nif (!(s & RQ_WRITE))\r\ncontinue;\r\nexpect_size++;\r\n}\r\n}\r\nif (req == NULL) {\r\nconn_err(tconn, "BAD! BarrierAck #%u received, but no epoch in tl!?\n",\r\nbarrier_nr);\r\ngoto bail;\r\n}\r\nif (expect_epoch != barrier_nr) {\r\nconn_err(tconn, "BAD! BarrierAck #%u received, expected #%u!\n",\r\nbarrier_nr, expect_epoch);\r\ngoto bail;\r\n}\r\nif (expect_size != set_size) {\r\nconn_err(tconn, "BAD! BarrierAck #%u received with n_writes=%u, expected n_writes=%u!\n",\r\nbarrier_nr, set_size, expect_size);\r\ngoto bail;\r\n}\r\nlist_for_each_entry(req, &tconn->transfer_log, tl_requests)\r\nif (req->epoch == expect_epoch)\r\nbreak;\r\nlist_for_each_entry_safe_from(req, r, &tconn->transfer_log, tl_requests) {\r\nif (req->epoch != expect_epoch)\r\nbreak;\r\n_req_mod(req, BARRIER_ACKED);\r\n}\r\nspin_unlock_irq(&tconn->req_lock);\r\nreturn;\r\nbail:\r\nspin_unlock_irq(&tconn->req_lock);\r\nconn_request_state(tconn, NS(conn, C_PROTOCOL_ERROR), CS_HARD);\r\n}\r\nvoid _tl_restart(struct drbd_tconn *tconn, enum drbd_req_event what)\r\n{\r\nstruct drbd_request *req, *r;\r\nlist_for_each_entry_safe(req, r, &tconn->transfer_log, tl_requests)\r\n_req_mod(req, what);\r\n}\r\nvoid tl_restart(struct drbd_tconn *tconn, enum drbd_req_event what)\r\n{\r\nspin_lock_irq(&tconn->req_lock);\r\n_tl_restart(tconn, what);\r\nspin_unlock_irq(&tconn->req_lock);\r\n}\r\nvoid tl_clear(struct drbd_tconn *tconn)\r\n{\r\ntl_restart(tconn, CONNECTION_LOST_WHILE_PENDING);\r\n}\r\nvoid tl_abort_disk_io(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_tconn *tconn = mdev->tconn;\r\nstruct drbd_request *req, *r;\r\nspin_lock_irq(&tconn->req_lock);\r\nlist_for_each_entry_safe(req, r, &tconn->transfer_log, tl_requests) {\r\nif (!(req->rq_state & RQ_LOCAL_PENDING))\r\ncontinue;\r\nif (req->w.mdev != mdev)\r\ncontinue;\r\n_req_mod(req, ABORT_DISK_IO);\r\n}\r\nspin_unlock_irq(&tconn->req_lock);\r\n}\r\nstatic int drbd_thread_setup(void *arg)\r\n{\r\nstruct drbd_thread *thi = (struct drbd_thread *) arg;\r\nstruct drbd_tconn *tconn = thi->tconn;\r\nunsigned long flags;\r\nint retval;\r\nsnprintf(current->comm, sizeof(current->comm), "drbd_%c_%s",\r\nthi->name[0], thi->tconn->name);\r\nrestart:\r\nretval = thi->function(thi);\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nif (thi->t_state == RESTARTING) {\r\nconn_info(tconn, "Restarting %s thread\n", thi->name);\r\nthi->t_state = RUNNING;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\ngoto restart;\r\n}\r\nthi->task = NULL;\r\nthi->t_state = NONE;\r\nsmp_mb();\r\ncomplete_all(&thi->stop);\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nconn_info(tconn, "Terminating %s\n", current->comm);\r\nkref_put(&tconn->kref, &conn_destroy);\r\nmodule_put(THIS_MODULE);\r\nreturn retval;\r\n}\r\nstatic void drbd_thread_init(struct drbd_tconn *tconn, struct drbd_thread *thi,\r\nint (*func) (struct drbd_thread *), char *name)\r\n{\r\nspin_lock_init(&thi->t_lock);\r\nthi->task = NULL;\r\nthi->t_state = NONE;\r\nthi->function = func;\r\nthi->tconn = tconn;\r\nstrncpy(thi->name, name, ARRAY_SIZE(thi->name));\r\n}\r\nint drbd_thread_start(struct drbd_thread *thi)\r\n{\r\nstruct drbd_tconn *tconn = thi->tconn;\r\nstruct task_struct *nt;\r\nunsigned long flags;\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nswitch (thi->t_state) {\r\ncase NONE:\r\nconn_info(tconn, "Starting %s thread (from %s [%d])\n",\r\nthi->name, current->comm, current->pid);\r\nif (!try_module_get(THIS_MODULE)) {\r\nconn_err(tconn, "Failed to get module reference in drbd_thread_start\n");\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nreturn false;\r\n}\r\nkref_get(&thi->tconn->kref);\r\ninit_completion(&thi->stop);\r\nthi->reset_cpu_mask = 1;\r\nthi->t_state = RUNNING;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nflush_signals(current);\r\nnt = kthread_create(drbd_thread_setup, (void *) thi,\r\n"drbd_%c_%s", thi->name[0], thi->tconn->name);\r\nif (IS_ERR(nt)) {\r\nconn_err(tconn, "Couldn't start thread\n");\r\nkref_put(&tconn->kref, &conn_destroy);\r\nmodule_put(THIS_MODULE);\r\nreturn false;\r\n}\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nthi->task = nt;\r\nthi->t_state = RUNNING;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nwake_up_process(nt);\r\nbreak;\r\ncase EXITING:\r\nthi->t_state = RESTARTING;\r\nconn_info(tconn, "Restarting %s thread (from %s [%d])\n",\r\nthi->name, current->comm, current->pid);\r\ncase RUNNING:\r\ncase RESTARTING:\r\ndefault:\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nbreak;\r\n}\r\nreturn true;\r\n}\r\nvoid _drbd_thread_stop(struct drbd_thread *thi, int restart, int wait)\r\n{\r\nunsigned long flags;\r\nenum drbd_thread_state ns = restart ? RESTARTING : EXITING;\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nif (thi->t_state == NONE) {\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nif (restart)\r\ndrbd_thread_start(thi);\r\nreturn;\r\n}\r\nif (thi->t_state != ns) {\r\nif (thi->task == NULL) {\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nreturn;\r\n}\r\nthi->t_state = ns;\r\nsmp_mb();\r\ninit_completion(&thi->stop);\r\nif (thi->task != current)\r\nforce_sig(DRBD_SIGKILL, thi->task);\r\n}\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nif (wait)\r\nwait_for_completion(&thi->stop);\r\n}\r\nstatic struct drbd_thread *drbd_task_to_thread(struct drbd_tconn *tconn, struct task_struct *task)\r\n{\r\nstruct drbd_thread *thi =\r\ntask == tconn->receiver.task ? &tconn->receiver :\r\ntask == tconn->asender.task ? &tconn->asender :\r\ntask == tconn->worker.task ? &tconn->worker : NULL;\r\nreturn thi;\r\n}\r\nchar *drbd_task_to_thread_name(struct drbd_tconn *tconn, struct task_struct *task)\r\n{\r\nstruct drbd_thread *thi = drbd_task_to_thread(tconn, task);\r\nreturn thi ? thi->name : task->comm;\r\n}\r\nint conn_lowest_minor(struct drbd_tconn *tconn)\r\n{\r\nstruct drbd_conf *mdev;\r\nint vnr = 0, m;\r\nrcu_read_lock();\r\nmdev = idr_get_next(&tconn->volumes, &vnr);\r\nm = mdev ? mdev_to_minor(mdev) : -1;\r\nrcu_read_unlock();\r\nreturn m;\r\n}\r\nvoid drbd_calc_cpu_mask(struct drbd_tconn *tconn)\r\n{\r\nint ord, cpu;\r\nif (cpumask_weight(tconn->cpu_mask))\r\nreturn;\r\nord = conn_lowest_minor(tconn) % cpumask_weight(cpu_online_mask);\r\nfor_each_online_cpu(cpu) {\r\nif (ord-- == 0) {\r\ncpumask_set_cpu(cpu, tconn->cpu_mask);\r\nreturn;\r\n}\r\n}\r\ncpumask_setall(tconn->cpu_mask);\r\n}\r\nvoid drbd_thread_current_set_cpu(struct drbd_thread *thi)\r\n{\r\nstruct task_struct *p = current;\r\nif (!thi->reset_cpu_mask)\r\nreturn;\r\nthi->reset_cpu_mask = 0;\r\nset_cpus_allowed_ptr(p, thi->tconn->cpu_mask);\r\n}\r\nunsigned int drbd_header_size(struct drbd_tconn *tconn)\r\n{\r\nif (tconn->agreed_pro_version >= 100) {\r\nBUILD_BUG_ON(!IS_ALIGNED(sizeof(struct p_header100), 8));\r\nreturn sizeof(struct p_header100);\r\n} else {\r\nBUILD_BUG_ON(sizeof(struct p_header80) !=\r\nsizeof(struct p_header95));\r\nBUILD_BUG_ON(!IS_ALIGNED(sizeof(struct p_header80), 8));\r\nreturn sizeof(struct p_header80);\r\n}\r\n}\r\nstatic unsigned int prepare_header80(struct p_header80 *h, enum drbd_packet cmd, int size)\r\n{\r\nh->magic = cpu_to_be32(DRBD_MAGIC);\r\nh->command = cpu_to_be16(cmd);\r\nh->length = cpu_to_be16(size);\r\nreturn sizeof(struct p_header80);\r\n}\r\nstatic unsigned int prepare_header95(struct p_header95 *h, enum drbd_packet cmd, int size)\r\n{\r\nh->magic = cpu_to_be16(DRBD_MAGIC_BIG);\r\nh->command = cpu_to_be16(cmd);\r\nh->length = cpu_to_be32(size);\r\nreturn sizeof(struct p_header95);\r\n}\r\nstatic unsigned int prepare_header100(struct p_header100 *h, enum drbd_packet cmd,\r\nint size, int vnr)\r\n{\r\nh->magic = cpu_to_be32(DRBD_MAGIC_100);\r\nh->volume = cpu_to_be16(vnr);\r\nh->command = cpu_to_be16(cmd);\r\nh->length = cpu_to_be32(size);\r\nh->pad = 0;\r\nreturn sizeof(struct p_header100);\r\n}\r\nstatic unsigned int prepare_header(struct drbd_tconn *tconn, int vnr,\r\nvoid *buffer, enum drbd_packet cmd, int size)\r\n{\r\nif (tconn->agreed_pro_version >= 100)\r\nreturn prepare_header100(buffer, cmd, size, vnr);\r\nelse if (tconn->agreed_pro_version >= 95 &&\r\nsize > DRBD_MAX_SIZE_H80_PACKET)\r\nreturn prepare_header95(buffer, cmd, size);\r\nelse\r\nreturn prepare_header80(buffer, cmd, size);\r\n}\r\nstatic void *__conn_prepare_command(struct drbd_tconn *tconn,\r\nstruct drbd_socket *sock)\r\n{\r\nif (!sock->socket)\r\nreturn NULL;\r\nreturn sock->sbuf + drbd_header_size(tconn);\r\n}\r\nvoid *conn_prepare_command(struct drbd_tconn *tconn, struct drbd_socket *sock)\r\n{\r\nvoid *p;\r\nmutex_lock(&sock->mutex);\r\np = __conn_prepare_command(tconn, sock);\r\nif (!p)\r\nmutex_unlock(&sock->mutex);\r\nreturn p;\r\n}\r\nvoid *drbd_prepare_command(struct drbd_conf *mdev, struct drbd_socket *sock)\r\n{\r\nreturn conn_prepare_command(mdev->tconn, sock);\r\n}\r\nstatic int __send_command(struct drbd_tconn *tconn, int vnr,\r\nstruct drbd_socket *sock, enum drbd_packet cmd,\r\nunsigned int header_size, void *data,\r\nunsigned int size)\r\n{\r\nint msg_flags;\r\nint err;\r\nmsg_flags = data ? MSG_MORE : 0;\r\nheader_size += prepare_header(tconn, vnr, sock->sbuf, cmd,\r\nheader_size + size);\r\nerr = drbd_send_all(tconn, sock->socket, sock->sbuf, header_size,\r\nmsg_flags);\r\nif (data && !err)\r\nerr = drbd_send_all(tconn, sock->socket, data, size, 0);\r\nreturn err;\r\n}\r\nstatic int __conn_send_command(struct drbd_tconn *tconn, struct drbd_socket *sock,\r\nenum drbd_packet cmd, unsigned int header_size,\r\nvoid *data, unsigned int size)\r\n{\r\nreturn __send_command(tconn, 0, sock, cmd, header_size, data, size);\r\n}\r\nint conn_send_command(struct drbd_tconn *tconn, struct drbd_socket *sock,\r\nenum drbd_packet cmd, unsigned int header_size,\r\nvoid *data, unsigned int size)\r\n{\r\nint err;\r\nerr = __conn_send_command(tconn, sock, cmd, header_size, data, size);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_command(struct drbd_conf *mdev, struct drbd_socket *sock,\r\nenum drbd_packet cmd, unsigned int header_size,\r\nvoid *data, unsigned int size)\r\n{\r\nint err;\r\nerr = __send_command(mdev->tconn, mdev->vnr, sock, cmd, header_size,\r\ndata, size);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_ping(struct drbd_tconn *tconn)\r\n{\r\nstruct drbd_socket *sock;\r\nsock = &tconn->meta;\r\nif (!conn_prepare_command(tconn, sock))\r\nreturn -EIO;\r\nreturn conn_send_command(tconn, sock, P_PING, 0, NULL, 0);\r\n}\r\nint drbd_send_ping_ack(struct drbd_tconn *tconn)\r\n{\r\nstruct drbd_socket *sock;\r\nsock = &tconn->meta;\r\nif (!conn_prepare_command(tconn, sock))\r\nreturn -EIO;\r\nreturn conn_send_command(tconn, sock, P_PING_ACK, 0, NULL, 0);\r\n}\r\nint drbd_send_sync_param(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_rs_param_95 *p;\r\nint size;\r\nconst int apv = mdev->tconn->agreed_pro_version;\r\nenum drbd_packet cmd;\r\nstruct net_conf *nc;\r\nstruct disk_conf *dc;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\nrcu_read_lock();\r\nnc = rcu_dereference(mdev->tconn->net_conf);\r\nsize = apv <= 87 ? sizeof(struct p_rs_param)\r\n: apv == 88 ? sizeof(struct p_rs_param)\r\n+ strlen(nc->verify_alg) + 1\r\n: apv <= 94 ? sizeof(struct p_rs_param_89)\r\n: sizeof(struct p_rs_param_95);\r\ncmd = apv >= 89 ? P_SYNC_PARAM89 : P_SYNC_PARAM;\r\nmemset(p->verify_alg, 0, 2 * SHARED_SECRET_MAX);\r\nif (get_ldev(mdev)) {\r\ndc = rcu_dereference(mdev->ldev->disk_conf);\r\np->resync_rate = cpu_to_be32(dc->resync_rate);\r\np->c_plan_ahead = cpu_to_be32(dc->c_plan_ahead);\r\np->c_delay_target = cpu_to_be32(dc->c_delay_target);\r\np->c_fill_target = cpu_to_be32(dc->c_fill_target);\r\np->c_max_rate = cpu_to_be32(dc->c_max_rate);\r\nput_ldev(mdev);\r\n} else {\r\np->resync_rate = cpu_to_be32(DRBD_RESYNC_RATE_DEF);\r\np->c_plan_ahead = cpu_to_be32(DRBD_C_PLAN_AHEAD_DEF);\r\np->c_delay_target = cpu_to_be32(DRBD_C_DELAY_TARGET_DEF);\r\np->c_fill_target = cpu_to_be32(DRBD_C_FILL_TARGET_DEF);\r\np->c_max_rate = cpu_to_be32(DRBD_C_MAX_RATE_DEF);\r\n}\r\nif (apv >= 88)\r\nstrcpy(p->verify_alg, nc->verify_alg);\r\nif (apv >= 89)\r\nstrcpy(p->csums_alg, nc->csums_alg);\r\nrcu_read_unlock();\r\nreturn drbd_send_command(mdev, sock, cmd, size, NULL, 0);\r\n}\r\nint __drbd_send_protocol(struct drbd_tconn *tconn, enum drbd_packet cmd)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_protocol *p;\r\nstruct net_conf *nc;\r\nint size, cf;\r\nsock = &tconn->data;\r\np = __conn_prepare_command(tconn, sock);\r\nif (!p)\r\nreturn -EIO;\r\nrcu_read_lock();\r\nnc = rcu_dereference(tconn->net_conf);\r\nif (nc->tentative && tconn->agreed_pro_version < 92) {\r\nrcu_read_unlock();\r\nmutex_unlock(&sock->mutex);\r\nconn_err(tconn, "--dry-run is not supported by peer");\r\nreturn -EOPNOTSUPP;\r\n}\r\nsize = sizeof(*p);\r\nif (tconn->agreed_pro_version >= 87)\r\nsize += strlen(nc->integrity_alg) + 1;\r\np->protocol = cpu_to_be32(nc->wire_protocol);\r\np->after_sb_0p = cpu_to_be32(nc->after_sb_0p);\r\np->after_sb_1p = cpu_to_be32(nc->after_sb_1p);\r\np->after_sb_2p = cpu_to_be32(nc->after_sb_2p);\r\np->two_primaries = cpu_to_be32(nc->two_primaries);\r\ncf = 0;\r\nif (nc->discard_my_data)\r\ncf |= CF_DISCARD_MY_DATA;\r\nif (nc->tentative)\r\ncf |= CF_DRY_RUN;\r\np->conn_flags = cpu_to_be32(cf);\r\nif (tconn->agreed_pro_version >= 87)\r\nstrcpy(p->integrity_alg, nc->integrity_alg);\r\nrcu_read_unlock();\r\nreturn __conn_send_command(tconn, sock, cmd, size, NULL, 0);\r\n}\r\nint drbd_send_protocol(struct drbd_tconn *tconn)\r\n{\r\nint err;\r\nmutex_lock(&tconn->data.mutex);\r\nerr = __drbd_send_protocol(tconn, P_PROTOCOL);\r\nmutex_unlock(&tconn->data.mutex);\r\nreturn err;\r\n}\r\nint _drbd_send_uuids(struct drbd_conf *mdev, u64 uuid_flags)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_uuids *p;\r\nint i;\r\nif (!get_ldev_if_state(mdev, D_NEGOTIATING))\r\nreturn 0;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p) {\r\nput_ldev(mdev);\r\nreturn -EIO;\r\n}\r\nspin_lock_irq(&mdev->ldev->md.uuid_lock);\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\np->uuid[i] = cpu_to_be64(mdev->ldev->md.uuid[i]);\r\nspin_unlock_irq(&mdev->ldev->md.uuid_lock);\r\nmdev->comm_bm_set = drbd_bm_total_weight(mdev);\r\np->uuid[UI_SIZE] = cpu_to_be64(mdev->comm_bm_set);\r\nrcu_read_lock();\r\nuuid_flags |= rcu_dereference(mdev->tconn->net_conf)->discard_my_data ? 1 : 0;\r\nrcu_read_unlock();\r\nuuid_flags |= test_bit(CRASHED_PRIMARY, &mdev->flags) ? 2 : 0;\r\nuuid_flags |= mdev->new_state_tmp.disk == D_INCONSISTENT ? 4 : 0;\r\np->uuid[UI_FLAGS] = cpu_to_be64(uuid_flags);\r\nput_ldev(mdev);\r\nreturn drbd_send_command(mdev, sock, P_UUIDS, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_uuids(struct drbd_conf *mdev)\r\n{\r\nreturn _drbd_send_uuids(mdev, 0);\r\n}\r\nint drbd_send_uuids_skip_initial_sync(struct drbd_conf *mdev)\r\n{\r\nreturn _drbd_send_uuids(mdev, 8);\r\n}\r\nvoid drbd_print_uuids(struct drbd_conf *mdev, const char *text)\r\n{\r\nif (get_ldev_if_state(mdev, D_NEGOTIATING)) {\r\nu64 *uuid = mdev->ldev->md.uuid;\r\ndev_info(DEV, "%s %016llX:%016llX:%016llX:%016llX\n",\r\ntext,\r\n(unsigned long long)uuid[UI_CURRENT],\r\n(unsigned long long)uuid[UI_BITMAP],\r\n(unsigned long long)uuid[UI_HISTORY_START],\r\n(unsigned long long)uuid[UI_HISTORY_END]);\r\nput_ldev(mdev);\r\n} else {\r\ndev_info(DEV, "%s effective data uuid: %016llX\n",\r\ntext,\r\n(unsigned long long)mdev->ed_uuid);\r\n}\r\n}\r\nvoid drbd_gen_and_send_sync_uuid(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_rs_uuid *p;\r\nu64 uuid;\r\nD_ASSERT(mdev->state.disk == D_UP_TO_DATE);\r\nuuid = mdev->ldev->md.uuid[UI_BITMAP];\r\nif (uuid && uuid != UUID_JUST_CREATED)\r\nuuid = uuid + UUID_NEW_BM_OFFSET;\r\nelse\r\nget_random_bytes(&uuid, sizeof(u64));\r\ndrbd_uuid_set(mdev, UI_BITMAP, uuid);\r\ndrbd_print_uuids(mdev, "updated sync UUID");\r\ndrbd_md_sync(mdev);\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (p) {\r\np->uuid = cpu_to_be64(uuid);\r\ndrbd_send_command(mdev, sock, P_SYNC_UUID, sizeof(*p), NULL, 0);\r\n}\r\n}\r\nint drbd_send_sizes(struct drbd_conf *mdev, int trigger_reply, enum dds_flags flags)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_sizes *p;\r\nsector_t d_size, u_size;\r\nint q_order_type;\r\nunsigned int max_bio_size;\r\nif (get_ldev_if_state(mdev, D_NEGOTIATING)) {\r\nD_ASSERT(mdev->ldev->backing_bdev);\r\nd_size = drbd_get_max_capacity(mdev->ldev);\r\nrcu_read_lock();\r\nu_size = rcu_dereference(mdev->ldev->disk_conf)->disk_size;\r\nrcu_read_unlock();\r\nq_order_type = drbd_queue_order_type(mdev);\r\nmax_bio_size = queue_max_hw_sectors(mdev->ldev->backing_bdev->bd_disk->queue) << 9;\r\nmax_bio_size = min(max_bio_size, DRBD_MAX_BIO_SIZE);\r\nput_ldev(mdev);\r\n} else {\r\nd_size = 0;\r\nu_size = 0;\r\nq_order_type = QUEUE_ORDERED_NONE;\r\nmax_bio_size = DRBD_MAX_BIO_SIZE;\r\n}\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\nif (mdev->tconn->agreed_pro_version <= 94)\r\nmax_bio_size = min(max_bio_size, DRBD_MAX_SIZE_H80_PACKET);\r\nelse if (mdev->tconn->agreed_pro_version < 100)\r\nmax_bio_size = min(max_bio_size, DRBD_MAX_BIO_SIZE_P95);\r\np->d_size = cpu_to_be64(d_size);\r\np->u_size = cpu_to_be64(u_size);\r\np->c_size = cpu_to_be64(trigger_reply ? 0 : drbd_get_capacity(mdev->this_bdev));\r\np->max_bio_size = cpu_to_be32(max_bio_size);\r\np->queue_order_type = cpu_to_be16(q_order_type);\r\np->dds_flags = cpu_to_be16(flags);\r\nreturn drbd_send_command(mdev, sock, P_SIZES, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_current_state(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_state *p;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->state = cpu_to_be32(mdev->state.i);\r\nreturn drbd_send_command(mdev, sock, P_STATE, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_state(struct drbd_conf *mdev, union drbd_state state)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_state *p;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->state = cpu_to_be32(state.i);\r\nreturn drbd_send_command(mdev, sock, P_STATE, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_state_req(struct drbd_conf *mdev, union drbd_state mask, union drbd_state val)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_req_state *p;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->mask = cpu_to_be32(mask.i);\r\np->val = cpu_to_be32(val.i);\r\nreturn drbd_send_command(mdev, sock, P_STATE_CHG_REQ, sizeof(*p), NULL, 0);\r\n}\r\nint conn_send_state_req(struct drbd_tconn *tconn, union drbd_state mask, union drbd_state val)\r\n{\r\nenum drbd_packet cmd;\r\nstruct drbd_socket *sock;\r\nstruct p_req_state *p;\r\ncmd = tconn->agreed_pro_version < 100 ? P_STATE_CHG_REQ : P_CONN_ST_CHG_REQ;\r\nsock = &tconn->data;\r\np = conn_prepare_command(tconn, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->mask = cpu_to_be32(mask.i);\r\np->val = cpu_to_be32(val.i);\r\nreturn conn_send_command(tconn, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\nvoid drbd_send_sr_reply(struct drbd_conf *mdev, enum drbd_state_rv retcode)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_req_state_reply *p;\r\nsock = &mdev->tconn->meta;\r\np = drbd_prepare_command(mdev, sock);\r\nif (p) {\r\np->retcode = cpu_to_be32(retcode);\r\ndrbd_send_command(mdev, sock, P_STATE_CHG_REPLY, sizeof(*p), NULL, 0);\r\n}\r\n}\r\nvoid conn_send_sr_reply(struct drbd_tconn *tconn, enum drbd_state_rv retcode)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_req_state_reply *p;\r\nenum drbd_packet cmd = tconn->agreed_pro_version < 100 ? P_STATE_CHG_REPLY : P_CONN_ST_CHG_REPLY;\r\nsock = &tconn->meta;\r\np = conn_prepare_command(tconn, sock);\r\nif (p) {\r\np->retcode = cpu_to_be32(retcode);\r\nconn_send_command(tconn, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\n}\r\nstatic void dcbp_set_code(struct p_compressed_bm *p, enum drbd_bitmap_code code)\r\n{\r\nBUG_ON(code & ~0xf);\r\np->encoding = (p->encoding & ~0xf) | code;\r\n}\r\nstatic void dcbp_set_start(struct p_compressed_bm *p, int set)\r\n{\r\np->encoding = (p->encoding & ~0x80) | (set ? 0x80 : 0);\r\n}\r\nstatic void dcbp_set_pad_bits(struct p_compressed_bm *p, int n)\r\n{\r\nBUG_ON(n & ~0x7);\r\np->encoding = (p->encoding & (~0x7 << 4)) | (n << 4);\r\n}\r\nint fill_bitmap_rle_bits(struct drbd_conf *mdev,\r\nstruct p_compressed_bm *p,\r\nunsigned int size,\r\nstruct bm_xfer_ctx *c)\r\n{\r\nstruct bitstream bs;\r\nunsigned long plain_bits;\r\nunsigned long tmp;\r\nunsigned long rl;\r\nunsigned len;\r\nunsigned toggle;\r\nint bits, use_rle;\r\nrcu_read_lock();\r\nuse_rle = rcu_dereference(mdev->tconn->net_conf)->use_rle;\r\nrcu_read_unlock();\r\nif (!use_rle || mdev->tconn->agreed_pro_version < 90)\r\nreturn 0;\r\nif (c->bit_offset >= c->bm_bits)\r\nreturn 0;\r\nbitstream_init(&bs, p->code, size, 0);\r\nmemset(p->code, 0, size);\r\nplain_bits = 0;\r\ntoggle = 2;\r\ndo {\r\ntmp = (toggle == 0) ? _drbd_bm_find_next_zero(mdev, c->bit_offset)\r\n: _drbd_bm_find_next(mdev, c->bit_offset);\r\nif (tmp == -1UL)\r\ntmp = c->bm_bits;\r\nrl = tmp - c->bit_offset;\r\nif (toggle == 2) {\r\nif (rl == 0) {\r\ndcbp_set_start(p, 1);\r\ntoggle = !toggle;\r\ncontinue;\r\n}\r\ndcbp_set_start(p, 0);\r\n}\r\nif (rl == 0) {\r\ndev_err(DEV, "unexpected zero runlength while encoding bitmap "\r\n"t:%u bo:%lu\n", toggle, c->bit_offset);\r\nreturn -1;\r\n}\r\nbits = vli_encode_bits(&bs, rl);\r\nif (bits == -ENOBUFS)\r\nbreak;\r\nif (bits <= 0) {\r\ndev_err(DEV, "error while encoding bitmap: %d\n", bits);\r\nreturn 0;\r\n}\r\ntoggle = !toggle;\r\nplain_bits += rl;\r\nc->bit_offset = tmp;\r\n} while (c->bit_offset < c->bm_bits);\r\nlen = bs.cur.b - p->code + !!bs.cur.bit;\r\nif (plain_bits < (len << 3)) {\r\nc->bit_offset -= plain_bits;\r\nbm_xfer_ctx_bit_to_word_offset(c);\r\nc->bit_offset = c->word_offset * BITS_PER_LONG;\r\nreturn 0;\r\n}\r\nbm_xfer_ctx_bit_to_word_offset(c);\r\ndcbp_set_pad_bits(p, (8 - bs.cur.bit) & 0x7);\r\nreturn len;\r\n}\r\nstatic int\r\nsend_bitmap_rle_or_plain(struct drbd_conf *mdev, struct bm_xfer_ctx *c)\r\n{\r\nstruct drbd_socket *sock = &mdev->tconn->data;\r\nunsigned int header_size = drbd_header_size(mdev->tconn);\r\nstruct p_compressed_bm *p = sock->sbuf + header_size;\r\nint len, err;\r\nlen = fill_bitmap_rle_bits(mdev, p,\r\nDRBD_SOCKET_BUFFER_SIZE - header_size - sizeof(*p), c);\r\nif (len < 0)\r\nreturn -EIO;\r\nif (len) {\r\ndcbp_set_code(p, RLE_VLI_Bits);\r\nerr = __send_command(mdev->tconn, mdev->vnr, sock,\r\nP_COMPRESSED_BITMAP, sizeof(*p) + len,\r\nNULL, 0);\r\nc->packets[0]++;\r\nc->bytes[0] += header_size + sizeof(*p) + len;\r\nif (c->bit_offset >= c->bm_bits)\r\nlen = 0;\r\n} else {\r\nunsigned int data_size;\r\nunsigned long num_words;\r\nunsigned long *p = sock->sbuf + header_size;\r\ndata_size = DRBD_SOCKET_BUFFER_SIZE - header_size;\r\nnum_words = min_t(size_t, data_size / sizeof(*p),\r\nc->bm_words - c->word_offset);\r\nlen = num_words * sizeof(*p);\r\nif (len)\r\ndrbd_bm_get_lel(mdev, c->word_offset, num_words, p);\r\nerr = __send_command(mdev->tconn, mdev->vnr, sock, P_BITMAP, len, NULL, 0);\r\nc->word_offset += num_words;\r\nc->bit_offset = c->word_offset * BITS_PER_LONG;\r\nc->packets[1]++;\r\nc->bytes[1] += header_size + len;\r\nif (c->bit_offset > c->bm_bits)\r\nc->bit_offset = c->bm_bits;\r\n}\r\nif (!err) {\r\nif (len == 0) {\r\nINFO_bm_xfer_stats(mdev, "send", c);\r\nreturn 0;\r\n} else\r\nreturn 1;\r\n}\r\nreturn -EIO;\r\n}\r\nstatic int _drbd_send_bitmap(struct drbd_conf *mdev)\r\n{\r\nstruct bm_xfer_ctx c;\r\nint err;\r\nif (!expect(mdev->bitmap))\r\nreturn false;\r\nif (get_ldev(mdev)) {\r\nif (drbd_md_test_flag(mdev->ldev, MDF_FULL_SYNC)) {\r\ndev_info(DEV, "Writing the whole bitmap, MDF_FullSync was set.\n");\r\ndrbd_bm_set_all(mdev);\r\nif (drbd_bm_write(mdev)) {\r\ndev_err(DEV, "Failed to write bitmap to disk!\n");\r\n} else {\r\ndrbd_md_clear_flag(mdev, MDF_FULL_SYNC);\r\ndrbd_md_sync(mdev);\r\n}\r\n}\r\nput_ldev(mdev);\r\n}\r\nc = (struct bm_xfer_ctx) {\r\n.bm_bits = drbd_bm_bits(mdev),\r\n.bm_words = drbd_bm_words(mdev),\r\n};\r\ndo {\r\nerr = send_bitmap_rle_or_plain(mdev, &c);\r\n} while (err > 0);\r\nreturn err == 0;\r\n}\r\nint drbd_send_bitmap(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_socket *sock = &mdev->tconn->data;\r\nint err = -1;\r\nmutex_lock(&sock->mutex);\r\nif (sock->socket)\r\nerr = !_drbd_send_bitmap(mdev);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nvoid drbd_send_b_ack(struct drbd_tconn *tconn, u32 barrier_nr, u32 set_size)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_barrier_ack *p;\r\nif (tconn->cstate < C_WF_REPORT_PARAMS)\r\nreturn;\r\nsock = &tconn->meta;\r\np = conn_prepare_command(tconn, sock);\r\nif (!p)\r\nreturn;\r\np->barrier = barrier_nr;\r\np->set_size = cpu_to_be32(set_size);\r\nconn_send_command(tconn, sock, P_BARRIER_ACK, sizeof(*p), NULL, 0);\r\n}\r\nstatic int _drbd_send_ack(struct drbd_conf *mdev, enum drbd_packet cmd,\r\nu64 sector, u32 blksize, u64 block_id)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_ack *p;\r\nif (mdev->state.conn < C_CONNECTED)\r\nreturn -EIO;\r\nsock = &mdev->tconn->meta;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = sector;\r\np->block_id = block_id;\r\np->blksize = blksize;\r\np->seq_num = cpu_to_be32(atomic_inc_return(&mdev->packet_seq));\r\nreturn drbd_send_command(mdev, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\nvoid drbd_send_ack_dp(struct drbd_conf *mdev, enum drbd_packet cmd,\r\nstruct p_data *dp, int data_size)\r\n{\r\nif (mdev->tconn->peer_integrity_tfm)\r\ndata_size -= crypto_hash_digestsize(mdev->tconn->peer_integrity_tfm);\r\n_drbd_send_ack(mdev, cmd, dp->sector, cpu_to_be32(data_size),\r\ndp->block_id);\r\n}\r\nvoid drbd_send_ack_rp(struct drbd_conf *mdev, enum drbd_packet cmd,\r\nstruct p_block_req *rp)\r\n{\r\n_drbd_send_ack(mdev, cmd, rp->sector, rp->blksize, rp->block_id);\r\n}\r\nint drbd_send_ack(struct drbd_conf *mdev, enum drbd_packet cmd,\r\nstruct drbd_peer_request *peer_req)\r\n{\r\nreturn _drbd_send_ack(mdev, cmd,\r\ncpu_to_be64(peer_req->i.sector),\r\ncpu_to_be32(peer_req->i.size),\r\npeer_req->block_id);\r\n}\r\nint drbd_send_ack_ex(struct drbd_conf *mdev, enum drbd_packet cmd,\r\nsector_t sector, int blksize, u64 block_id)\r\n{\r\nreturn _drbd_send_ack(mdev, cmd,\r\ncpu_to_be64(sector),\r\ncpu_to_be32(blksize),\r\ncpu_to_be64(block_id));\r\n}\r\nint drbd_send_drequest(struct drbd_conf *mdev, int cmd,\r\nsector_t sector, int size, u64 block_id)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_req *p;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(sector);\r\np->block_id = block_id;\r\np->blksize = cpu_to_be32(size);\r\nreturn drbd_send_command(mdev, sock, cmd, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send_drequest_csum(struct drbd_conf *mdev, sector_t sector, int size,\r\nvoid *digest, int digest_size, enum drbd_packet cmd)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_req *p;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(sector);\r\np->block_id = ID_SYNCER ;\r\np->blksize = cpu_to_be32(size);\r\nreturn drbd_send_command(mdev, sock, cmd, sizeof(*p),\r\ndigest, digest_size);\r\n}\r\nint drbd_send_ov_request(struct drbd_conf *mdev, sector_t sector, int size)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_req *p;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(sector);\r\np->block_id = ID_SYNCER ;\r\np->blksize = cpu_to_be32(size);\r\nreturn drbd_send_command(mdev, sock, P_OV_REQUEST, sizeof(*p), NULL, 0);\r\n}\r\nstatic int we_should_drop_the_connection(struct drbd_tconn *tconn, struct socket *sock)\r\n{\r\nint drop_it;\r\ndrop_it = tconn->meta.socket == sock\r\n|| !tconn->asender.task\r\n|| get_t_state(&tconn->asender) != RUNNING\r\n|| tconn->cstate < C_WF_REPORT_PARAMS;\r\nif (drop_it)\r\nreturn true;\r\ndrop_it = !--tconn->ko_count;\r\nif (!drop_it) {\r\nconn_err(tconn, "[%s/%d] sock_sendmsg time expired, ko = %u\n",\r\ncurrent->comm, current->pid, tconn->ko_count);\r\nrequest_ping(tconn);\r\n}\r\nreturn drop_it; ;\r\n}\r\nstatic void drbd_update_congested(struct drbd_tconn *tconn)\r\n{\r\nstruct sock *sk = tconn->data.socket->sk;\r\nif (sk->sk_wmem_queued > sk->sk_sndbuf * 4 / 5)\r\nset_bit(NET_CONGESTED, &tconn->flags);\r\n}\r\nstatic int _drbd_no_send_page(struct drbd_conf *mdev, struct page *page,\r\nint offset, size_t size, unsigned msg_flags)\r\n{\r\nstruct socket *socket;\r\nvoid *addr;\r\nint err;\r\nsocket = mdev->tconn->data.socket;\r\naddr = kmap(page) + offset;\r\nerr = drbd_send_all(mdev->tconn, socket, addr, size, msg_flags);\r\nkunmap(page);\r\nif (!err)\r\nmdev->send_cnt += size >> 9;\r\nreturn err;\r\n}\r\nstatic int _drbd_send_page(struct drbd_conf *mdev, struct page *page,\r\nint offset, size_t size, unsigned msg_flags)\r\n{\r\nstruct socket *socket = mdev->tconn->data.socket;\r\nmm_segment_t oldfs = get_fs();\r\nint len = size;\r\nint err = -EIO;\r\nif (disable_sendpage || (page_count(page) < 1) || PageSlab(page))\r\nreturn _drbd_no_send_page(mdev, page, offset, size, msg_flags);\r\nmsg_flags |= MSG_NOSIGNAL;\r\ndrbd_update_congested(mdev->tconn);\r\nset_fs(KERNEL_DS);\r\ndo {\r\nint sent;\r\nsent = socket->ops->sendpage(socket, page, offset, len, msg_flags);\r\nif (sent <= 0) {\r\nif (sent == -EAGAIN) {\r\nif (we_should_drop_the_connection(mdev->tconn, socket))\r\nbreak;\r\ncontinue;\r\n}\r\ndev_warn(DEV, "%s: size=%d len=%d sent=%d\n",\r\n__func__, (int)size, len, sent);\r\nif (sent < 0)\r\nerr = sent;\r\nbreak;\r\n}\r\nlen -= sent;\r\noffset += sent;\r\n} while (len > 0 );\r\nset_fs(oldfs);\r\nclear_bit(NET_CONGESTED, &mdev->tconn->flags);\r\nif (len == 0) {\r\nerr = 0;\r\nmdev->send_cnt += size >> 9;\r\n}\r\nreturn err;\r\n}\r\nstatic int _drbd_send_bio(struct drbd_conf *mdev, struct bio *bio)\r\n{\r\nstruct bio_vec *bvec;\r\nint i;\r\nbio_for_each_segment(bvec, bio, i) {\r\nint err;\r\nerr = _drbd_no_send_page(mdev, bvec->bv_page,\r\nbvec->bv_offset, bvec->bv_len,\r\ni == bio->bi_vcnt - 1 ? 0 : MSG_MORE);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int _drbd_send_zc_bio(struct drbd_conf *mdev, struct bio *bio)\r\n{\r\nstruct bio_vec *bvec;\r\nint i;\r\nbio_for_each_segment(bvec, bio, i) {\r\nint err;\r\nerr = _drbd_send_page(mdev, bvec->bv_page,\r\nbvec->bv_offset, bvec->bv_len,\r\ni == bio->bi_vcnt - 1 ? 0 : MSG_MORE);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int _drbd_send_zc_ee(struct drbd_conf *mdev,\r\nstruct drbd_peer_request *peer_req)\r\n{\r\nstruct page *page = peer_req->pages;\r\nunsigned len = peer_req->i.size;\r\nint err;\r\npage_chain_for_each(page) {\r\nunsigned l = min_t(unsigned, len, PAGE_SIZE);\r\nerr = _drbd_send_page(mdev, page, 0, l,\r\npage_chain_next(page) ? MSG_MORE : 0);\r\nif (err)\r\nreturn err;\r\nlen -= l;\r\n}\r\nreturn 0;\r\n}\r\nstatic u32 bio_flags_to_wire(struct drbd_conf *mdev, unsigned long bi_rw)\r\n{\r\nif (mdev->tconn->agreed_pro_version >= 95)\r\nreturn (bi_rw & REQ_SYNC ? DP_RW_SYNC : 0) |\r\n(bi_rw & REQ_FUA ? DP_FUA : 0) |\r\n(bi_rw & REQ_FLUSH ? DP_FLUSH : 0) |\r\n(bi_rw & REQ_DISCARD ? DP_DISCARD : 0);\r\nelse\r\nreturn bi_rw & REQ_SYNC ? DP_RW_SYNC : 0;\r\n}\r\nint drbd_send_dblock(struct drbd_conf *mdev, struct drbd_request *req)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_data *p;\r\nunsigned int dp_flags = 0;\r\nint dgs;\r\nint err;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\ndgs = mdev->tconn->integrity_tfm ? crypto_hash_digestsize(mdev->tconn->integrity_tfm) : 0;\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(req->i.sector);\r\np->block_id = (unsigned long)req;\r\np->seq_num = cpu_to_be32(atomic_inc_return(&mdev->packet_seq));\r\ndp_flags = bio_flags_to_wire(mdev, req->master_bio->bi_rw);\r\nif (mdev->state.conn >= C_SYNC_SOURCE &&\r\nmdev->state.conn <= C_PAUSED_SYNC_T)\r\ndp_flags |= DP_MAY_SET_IN_SYNC;\r\nif (mdev->tconn->agreed_pro_version >= 100) {\r\nif (req->rq_state & RQ_EXP_RECEIVE_ACK)\r\ndp_flags |= DP_SEND_RECEIVE_ACK;\r\nif (req->rq_state & RQ_EXP_WRITE_ACK)\r\ndp_flags |= DP_SEND_WRITE_ACK;\r\n}\r\np->dp_flags = cpu_to_be32(dp_flags);\r\nif (dgs)\r\ndrbd_csum_bio(mdev, mdev->tconn->integrity_tfm, req->master_bio, p + 1);\r\nerr = __send_command(mdev->tconn, mdev->vnr, sock, P_DATA, sizeof(*p) + dgs, NULL, req->i.size);\r\nif (!err) {\r\nif (!(req->rq_state & (RQ_EXP_RECEIVE_ACK | RQ_EXP_WRITE_ACK)) || dgs)\r\nerr = _drbd_send_bio(mdev, req->master_bio);\r\nelse\r\nerr = _drbd_send_zc_bio(mdev, req->master_bio);\r\nif (dgs > 0 && dgs <= 64) {\r\nunsigned char digest[64];\r\ndrbd_csum_bio(mdev, mdev->tconn->integrity_tfm, req->master_bio, digest);\r\nif (memcmp(p + 1, digest, dgs)) {\r\ndev_warn(DEV,\r\n"Digest mismatch, buffer modified by upper layers during write: %llus +%u\n",\r\n(unsigned long long)req->i.sector, req->i.size);\r\n}\r\n}\r\n}\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_block(struct drbd_conf *mdev, enum drbd_packet cmd,\r\nstruct drbd_peer_request *peer_req)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_data *p;\r\nint err;\r\nint dgs;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\ndgs = mdev->tconn->integrity_tfm ? crypto_hash_digestsize(mdev->tconn->integrity_tfm) : 0;\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(peer_req->i.sector);\r\np->block_id = peer_req->block_id;\r\np->seq_num = 0;\r\np->dp_flags = 0;\r\nif (dgs)\r\ndrbd_csum_ee(mdev, mdev->tconn->integrity_tfm, peer_req, p + 1);\r\nerr = __send_command(mdev->tconn, mdev->vnr, sock, cmd, sizeof(*p) + dgs, NULL, peer_req->i.size);\r\nif (!err)\r\nerr = _drbd_send_zc_ee(mdev, peer_req);\r\nmutex_unlock(&sock->mutex);\r\nreturn err;\r\n}\r\nint drbd_send_out_of_sync(struct drbd_conf *mdev, struct drbd_request *req)\r\n{\r\nstruct drbd_socket *sock;\r\nstruct p_block_desc *p;\r\nsock = &mdev->tconn->data;\r\np = drbd_prepare_command(mdev, sock);\r\nif (!p)\r\nreturn -EIO;\r\np->sector = cpu_to_be64(req->i.sector);\r\np->blksize = cpu_to_be32(req->i.size);\r\nreturn drbd_send_command(mdev, sock, P_OUT_OF_SYNC, sizeof(*p), NULL, 0);\r\n}\r\nint drbd_send(struct drbd_tconn *tconn, struct socket *sock,\r\nvoid *buf, size_t size, unsigned msg_flags)\r\n{\r\nstruct kvec iov;\r\nstruct msghdr msg;\r\nint rv, sent = 0;\r\nif (!sock)\r\nreturn -EBADR;\r\niov.iov_base = buf;\r\niov.iov_len = size;\r\nmsg.msg_name = NULL;\r\nmsg.msg_namelen = 0;\r\nmsg.msg_control = NULL;\r\nmsg.msg_controllen = 0;\r\nmsg.msg_flags = msg_flags | MSG_NOSIGNAL;\r\nif (sock == tconn->data.socket) {\r\nrcu_read_lock();\r\ntconn->ko_count = rcu_dereference(tconn->net_conf)->ko_count;\r\nrcu_read_unlock();\r\ndrbd_update_congested(tconn);\r\n}\r\ndo {\r\nrv = kernel_sendmsg(sock, &msg, &iov, 1, size);\r\nif (rv == -EAGAIN) {\r\nif (we_should_drop_the_connection(tconn, sock))\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\nif (rv == -EINTR) {\r\nflush_signals(current);\r\nrv = 0;\r\n}\r\nif (rv < 0)\r\nbreak;\r\nsent += rv;\r\niov.iov_base += rv;\r\niov.iov_len -= rv;\r\n} while (sent < size);\r\nif (sock == tconn->data.socket)\r\nclear_bit(NET_CONGESTED, &tconn->flags);\r\nif (rv <= 0) {\r\nif (rv != -EAGAIN) {\r\nconn_err(tconn, "%s_sendmsg returned %d\n",\r\nsock == tconn->meta.socket ? "msock" : "sock",\r\nrv);\r\nconn_request_state(tconn, NS(conn, C_BROKEN_PIPE), CS_HARD);\r\n} else\r\nconn_request_state(tconn, NS(conn, C_TIMEOUT), CS_HARD);\r\n}\r\nreturn sent;\r\n}\r\nint drbd_send_all(struct drbd_tconn *tconn, struct socket *sock, void *buffer,\r\nsize_t size, unsigned msg_flags)\r\n{\r\nint err;\r\nerr = drbd_send(tconn, sock, buffer, size, msg_flags);\r\nif (err < 0)\r\nreturn err;\r\nif (err != size)\r\nreturn -EIO;\r\nreturn 0;\r\n}\r\nstatic int drbd_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nstruct drbd_conf *mdev = bdev->bd_disk->private_data;\r\nunsigned long flags;\r\nint rv = 0;\r\nmutex_lock(&drbd_main_mutex);\r\nspin_lock_irqsave(&mdev->tconn->req_lock, flags);\r\nif (mdev->state.role != R_PRIMARY) {\r\nif (mode & FMODE_WRITE)\r\nrv = -EROFS;\r\nelse if (!allow_oos)\r\nrv = -EMEDIUMTYPE;\r\n}\r\nif (!rv)\r\nmdev->open_cnt++;\r\nspin_unlock_irqrestore(&mdev->tconn->req_lock, flags);\r\nmutex_unlock(&drbd_main_mutex);\r\nreturn rv;\r\n}\r\nstatic void drbd_release(struct gendisk *gd, fmode_t mode)\r\n{\r\nstruct drbd_conf *mdev = gd->private_data;\r\nmutex_lock(&drbd_main_mutex);\r\nmdev->open_cnt--;\r\nmutex_unlock(&drbd_main_mutex);\r\n}\r\nstatic void drbd_set_defaults(struct drbd_conf *mdev)\r\n{\r\nmdev->state = (union drbd_dev_state) {\r\n{ .role = R_SECONDARY,\r\n.peer = R_UNKNOWN,\r\n.conn = C_STANDALONE,\r\n.disk = D_DISKLESS,\r\n.pdsk = D_UNKNOWN,\r\n} };\r\n}\r\nvoid drbd_init_set_defaults(struct drbd_conf *mdev)\r\n{\r\ndrbd_set_defaults(mdev);\r\natomic_set(&mdev->ap_bio_cnt, 0);\r\natomic_set(&mdev->ap_pending_cnt, 0);\r\natomic_set(&mdev->rs_pending_cnt, 0);\r\natomic_set(&mdev->unacked_cnt, 0);\r\natomic_set(&mdev->local_cnt, 0);\r\natomic_set(&mdev->pp_in_use_by_net, 0);\r\natomic_set(&mdev->rs_sect_in, 0);\r\natomic_set(&mdev->rs_sect_ev, 0);\r\natomic_set(&mdev->ap_in_flight, 0);\r\natomic_set(&mdev->md_io_in_use, 0);\r\nmutex_init(&mdev->own_state_mutex);\r\nmdev->state_mutex = &mdev->own_state_mutex;\r\nspin_lock_init(&mdev->al_lock);\r\nspin_lock_init(&mdev->peer_seq_lock);\r\nINIT_LIST_HEAD(&mdev->active_ee);\r\nINIT_LIST_HEAD(&mdev->sync_ee);\r\nINIT_LIST_HEAD(&mdev->done_ee);\r\nINIT_LIST_HEAD(&mdev->read_ee);\r\nINIT_LIST_HEAD(&mdev->net_ee);\r\nINIT_LIST_HEAD(&mdev->resync_reads);\r\nINIT_LIST_HEAD(&mdev->resync_work.list);\r\nINIT_LIST_HEAD(&mdev->unplug_work.list);\r\nINIT_LIST_HEAD(&mdev->go_diskless.list);\r\nINIT_LIST_HEAD(&mdev->md_sync_work.list);\r\nINIT_LIST_HEAD(&mdev->start_resync_work.list);\r\nINIT_LIST_HEAD(&mdev->bm_io_work.w.list);\r\nmdev->resync_work.cb = w_resync_timer;\r\nmdev->unplug_work.cb = w_send_write_hint;\r\nmdev->go_diskless.cb = w_go_diskless;\r\nmdev->md_sync_work.cb = w_md_sync;\r\nmdev->bm_io_work.w.cb = w_bitmap_io;\r\nmdev->start_resync_work.cb = w_start_resync;\r\nmdev->resync_work.mdev = mdev;\r\nmdev->unplug_work.mdev = mdev;\r\nmdev->go_diskless.mdev = mdev;\r\nmdev->md_sync_work.mdev = mdev;\r\nmdev->bm_io_work.w.mdev = mdev;\r\nmdev->start_resync_work.mdev = mdev;\r\ninit_timer(&mdev->resync_timer);\r\ninit_timer(&mdev->md_sync_timer);\r\ninit_timer(&mdev->start_resync_timer);\r\ninit_timer(&mdev->request_timer);\r\nmdev->resync_timer.function = resync_timer_fn;\r\nmdev->resync_timer.data = (unsigned long) mdev;\r\nmdev->md_sync_timer.function = md_sync_timer_fn;\r\nmdev->md_sync_timer.data = (unsigned long) mdev;\r\nmdev->start_resync_timer.function = start_resync_timer_fn;\r\nmdev->start_resync_timer.data = (unsigned long) mdev;\r\nmdev->request_timer.function = request_timer_fn;\r\nmdev->request_timer.data = (unsigned long) mdev;\r\ninit_waitqueue_head(&mdev->misc_wait);\r\ninit_waitqueue_head(&mdev->state_wait);\r\ninit_waitqueue_head(&mdev->ee_wait);\r\ninit_waitqueue_head(&mdev->al_wait);\r\ninit_waitqueue_head(&mdev->seq_wait);\r\nmdev->resync_wenr = LC_FREE;\r\nmdev->peer_max_bio_size = DRBD_MAX_BIO_SIZE_SAFE;\r\nmdev->local_max_bio_size = DRBD_MAX_BIO_SIZE_SAFE;\r\n}\r\nvoid drbd_mdev_cleanup(struct drbd_conf *mdev)\r\n{\r\nint i;\r\nif (mdev->tconn->receiver.t_state != NONE)\r\ndev_err(DEV, "ASSERT FAILED: receiver t_state == %d expected 0.\n",\r\nmdev->tconn->receiver.t_state);\r\nmdev->al_writ_cnt =\r\nmdev->bm_writ_cnt =\r\nmdev->read_cnt =\r\nmdev->recv_cnt =\r\nmdev->send_cnt =\r\nmdev->writ_cnt =\r\nmdev->p_size =\r\nmdev->rs_start =\r\nmdev->rs_total =\r\nmdev->rs_failed = 0;\r\nmdev->rs_last_events = 0;\r\nmdev->rs_last_sect_ev = 0;\r\nfor (i = 0; i < DRBD_SYNC_MARKS; i++) {\r\nmdev->rs_mark_left[i] = 0;\r\nmdev->rs_mark_time[i] = 0;\r\n}\r\nD_ASSERT(mdev->tconn->net_conf == NULL);\r\ndrbd_set_my_capacity(mdev, 0);\r\nif (mdev->bitmap) {\r\ndrbd_bm_resize(mdev, 0, 1);\r\ndrbd_bm_cleanup(mdev);\r\n}\r\ndrbd_free_bc(mdev->ldev);\r\nmdev->ldev = NULL;\r\nclear_bit(AL_SUSPENDED, &mdev->flags);\r\nD_ASSERT(list_empty(&mdev->active_ee));\r\nD_ASSERT(list_empty(&mdev->sync_ee));\r\nD_ASSERT(list_empty(&mdev->done_ee));\r\nD_ASSERT(list_empty(&mdev->read_ee));\r\nD_ASSERT(list_empty(&mdev->net_ee));\r\nD_ASSERT(list_empty(&mdev->resync_reads));\r\nD_ASSERT(list_empty(&mdev->tconn->sender_work.q));\r\nD_ASSERT(list_empty(&mdev->resync_work.list));\r\nD_ASSERT(list_empty(&mdev->unplug_work.list));\r\nD_ASSERT(list_empty(&mdev->go_diskless.list));\r\ndrbd_set_defaults(mdev);\r\n}\r\nstatic void drbd_destroy_mempools(void)\r\n{\r\nstruct page *page;\r\nwhile (drbd_pp_pool) {\r\npage = drbd_pp_pool;\r\ndrbd_pp_pool = (struct page *)page_private(page);\r\n__free_page(page);\r\ndrbd_pp_vacant--;\r\n}\r\nif (drbd_md_io_bio_set)\r\nbioset_free(drbd_md_io_bio_set);\r\nif (drbd_md_io_page_pool)\r\nmempool_destroy(drbd_md_io_page_pool);\r\nif (drbd_ee_mempool)\r\nmempool_destroy(drbd_ee_mempool);\r\nif (drbd_request_mempool)\r\nmempool_destroy(drbd_request_mempool);\r\nif (drbd_ee_cache)\r\nkmem_cache_destroy(drbd_ee_cache);\r\nif (drbd_request_cache)\r\nkmem_cache_destroy(drbd_request_cache);\r\nif (drbd_bm_ext_cache)\r\nkmem_cache_destroy(drbd_bm_ext_cache);\r\nif (drbd_al_ext_cache)\r\nkmem_cache_destroy(drbd_al_ext_cache);\r\ndrbd_md_io_bio_set = NULL;\r\ndrbd_md_io_page_pool = NULL;\r\ndrbd_ee_mempool = NULL;\r\ndrbd_request_mempool = NULL;\r\ndrbd_ee_cache = NULL;\r\ndrbd_request_cache = NULL;\r\ndrbd_bm_ext_cache = NULL;\r\ndrbd_al_ext_cache = NULL;\r\nreturn;\r\n}\r\nstatic int drbd_create_mempools(void)\r\n{\r\nstruct page *page;\r\nconst int number = (DRBD_MAX_BIO_SIZE/PAGE_SIZE) * minor_count;\r\nint i;\r\ndrbd_request_mempool = NULL;\r\ndrbd_ee_cache = NULL;\r\ndrbd_request_cache = NULL;\r\ndrbd_bm_ext_cache = NULL;\r\ndrbd_al_ext_cache = NULL;\r\ndrbd_pp_pool = NULL;\r\ndrbd_md_io_page_pool = NULL;\r\ndrbd_md_io_bio_set = NULL;\r\ndrbd_request_cache = kmem_cache_create(\r\n"drbd_req", sizeof(struct drbd_request), 0, 0, NULL);\r\nif (drbd_request_cache == NULL)\r\ngoto Enomem;\r\ndrbd_ee_cache = kmem_cache_create(\r\n"drbd_ee", sizeof(struct drbd_peer_request), 0, 0, NULL);\r\nif (drbd_ee_cache == NULL)\r\ngoto Enomem;\r\ndrbd_bm_ext_cache = kmem_cache_create(\r\n"drbd_bm", sizeof(struct bm_extent), 0, 0, NULL);\r\nif (drbd_bm_ext_cache == NULL)\r\ngoto Enomem;\r\ndrbd_al_ext_cache = kmem_cache_create(\r\n"drbd_al", sizeof(struct lc_element), 0, 0, NULL);\r\nif (drbd_al_ext_cache == NULL)\r\ngoto Enomem;\r\ndrbd_md_io_bio_set = bioset_create(DRBD_MIN_POOL_PAGES, 0);\r\nif (drbd_md_io_bio_set == NULL)\r\ngoto Enomem;\r\ndrbd_md_io_page_pool = mempool_create_page_pool(DRBD_MIN_POOL_PAGES, 0);\r\nif (drbd_md_io_page_pool == NULL)\r\ngoto Enomem;\r\ndrbd_request_mempool = mempool_create(number,\r\nmempool_alloc_slab, mempool_free_slab, drbd_request_cache);\r\nif (drbd_request_mempool == NULL)\r\ngoto Enomem;\r\ndrbd_ee_mempool = mempool_create(number,\r\nmempool_alloc_slab, mempool_free_slab, drbd_ee_cache);\r\nif (drbd_ee_mempool == NULL)\r\ngoto Enomem;\r\nspin_lock_init(&drbd_pp_lock);\r\nfor (i = 0; i < number; i++) {\r\npage = alloc_page(GFP_HIGHUSER);\r\nif (!page)\r\ngoto Enomem;\r\nset_page_private(page, (unsigned long)drbd_pp_pool);\r\ndrbd_pp_pool = page;\r\n}\r\ndrbd_pp_vacant = number;\r\nreturn 0;\r\nEnomem:\r\ndrbd_destroy_mempools();\r\nreturn -ENOMEM;\r\n}\r\nstatic int drbd_notify_sys(struct notifier_block *this, unsigned long code,\r\nvoid *unused)\r\n{\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic void drbd_release_all_peer_reqs(struct drbd_conf *mdev)\r\n{\r\nint rr;\r\nrr = drbd_free_peer_reqs(mdev, &mdev->active_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in active list found!\n", rr);\r\nrr = drbd_free_peer_reqs(mdev, &mdev->sync_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in sync list found!\n", rr);\r\nrr = drbd_free_peer_reqs(mdev, &mdev->read_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in read list found!\n", rr);\r\nrr = drbd_free_peer_reqs(mdev, &mdev->done_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in done list found!\n", rr);\r\nrr = drbd_free_peer_reqs(mdev, &mdev->net_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in net list found!\n", rr);\r\n}\r\nvoid drbd_minor_destroy(struct kref *kref)\r\n{\r\nstruct drbd_conf *mdev = container_of(kref, struct drbd_conf, kref);\r\nstruct drbd_tconn *tconn = mdev->tconn;\r\ndel_timer_sync(&mdev->request_timer);\r\nD_ASSERT(mdev->open_cnt == 0);\r\nif (mdev->this_bdev)\r\nbdput(mdev->this_bdev);\r\ndrbd_free_bc(mdev->ldev);\r\nmdev->ldev = NULL;\r\ndrbd_release_all_peer_reqs(mdev);\r\nlc_destroy(mdev->act_log);\r\nlc_destroy(mdev->resync);\r\nkfree(mdev->p_uuid);\r\nif (mdev->bitmap)\r\ndrbd_bm_cleanup(mdev);\r\n__free_page(mdev->md_io_page);\r\nput_disk(mdev->vdisk);\r\nblk_cleanup_queue(mdev->rq_queue);\r\nkfree(mdev->rs_plan_s);\r\nkfree(mdev);\r\nkref_put(&tconn->kref, &conn_destroy);\r\n}\r\nstatic void do_retry(struct work_struct *ws)\r\n{\r\nstruct retry_worker *retry = container_of(ws, struct retry_worker, worker);\r\nLIST_HEAD(writes);\r\nstruct drbd_request *req, *tmp;\r\nspin_lock_irq(&retry->lock);\r\nlist_splice_init(&retry->writes, &writes);\r\nspin_unlock_irq(&retry->lock);\r\nlist_for_each_entry_safe(req, tmp, &writes, tl_requests) {\r\nstruct drbd_conf *mdev = req->w.mdev;\r\nstruct bio *bio = req->master_bio;\r\nunsigned long start_time = req->start_time;\r\nbool expected;\r\nexpected =\r\nexpect(atomic_read(&req->completion_ref) == 0) &&\r\nexpect(req->rq_state & RQ_POSTPONED) &&\r\nexpect((req->rq_state & RQ_LOCAL_PENDING) == 0 ||\r\n(req->rq_state & RQ_LOCAL_ABORTED) != 0);\r\nif (!expected)\r\ndev_err(DEV, "req=%p completion_ref=%d rq_state=%x\n",\r\nreq, atomic_read(&req->completion_ref),\r\nreq->rq_state);\r\nkref_put(&req->kref, drbd_req_destroy);\r\ninc_ap_bio(mdev);\r\n__drbd_make_request(mdev, bio, start_time);\r\n}\r\n}\r\nvoid drbd_restart_request(struct drbd_request *req)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&retry.lock, flags);\r\nlist_move_tail(&req->tl_requests, &retry.writes);\r\nspin_unlock_irqrestore(&retry.lock, flags);\r\ndec_ap_bio(req->w.mdev);\r\nqueue_work(retry.wq, &retry.worker);\r\n}\r\nstatic void drbd_cleanup(void)\r\n{\r\nunsigned int i;\r\nstruct drbd_conf *mdev;\r\nstruct drbd_tconn *tconn, *tmp;\r\nunregister_reboot_notifier(&drbd_notifier);\r\nif (drbd_proc)\r\nremove_proc_entry("drbd", NULL);\r\nif (retry.wq)\r\ndestroy_workqueue(retry.wq);\r\ndrbd_genl_unregister();\r\nidr_for_each_entry(&minors, mdev, i) {\r\nidr_remove(&minors, mdev_to_minor(mdev));\r\nidr_remove(&mdev->tconn->volumes, mdev->vnr);\r\ndestroy_workqueue(mdev->submit.wq);\r\ndel_gendisk(mdev->vdisk);\r\nkref_put(&mdev->kref, &drbd_minor_destroy);\r\n}\r\nlist_for_each_entry_safe(tconn, tmp, &drbd_tconns, all_tconn) {\r\nlist_del(&tconn->all_tconn);\r\nkref_put(&tconn->kref, &conn_destroy);\r\n}\r\ndrbd_destroy_mempools();\r\nunregister_blkdev(DRBD_MAJOR, "drbd");\r\nidr_destroy(&minors);\r\nprintk(KERN_INFO "drbd: module cleanup done.\n");\r\n}\r\nstatic int drbd_congested(void *congested_data, int bdi_bits)\r\n{\r\nstruct drbd_conf *mdev = congested_data;\r\nstruct request_queue *q;\r\nchar reason = '-';\r\nint r = 0;\r\nif (!may_inc_ap_bio(mdev)) {\r\nr = bdi_bits;\r\nreason = 'd';\r\ngoto out;\r\n}\r\nif (test_bit(CALLBACK_PENDING, &mdev->tconn->flags)) {\r\nr |= (1 << BDI_async_congested);\r\nif (!get_ldev_if_state(mdev, D_UP_TO_DATE))\r\nr |= (1 << BDI_sync_congested);\r\nelse\r\nput_ldev(mdev);\r\nr &= bdi_bits;\r\nreason = 'c';\r\ngoto out;\r\n}\r\nif (get_ldev(mdev)) {\r\nq = bdev_get_queue(mdev->ldev->backing_bdev);\r\nr = bdi_congested(&q->backing_dev_info, bdi_bits);\r\nput_ldev(mdev);\r\nif (r)\r\nreason = 'b';\r\n}\r\nif (bdi_bits & (1 << BDI_async_congested) && test_bit(NET_CONGESTED, &mdev->tconn->flags)) {\r\nr |= (1 << BDI_async_congested);\r\nreason = reason == 'b' ? 'a' : 'n';\r\n}\r\nout:\r\nmdev->congestion_reason = reason;\r\nreturn r;\r\n}\r\nstatic void drbd_init_workqueue(struct drbd_work_queue* wq)\r\n{\r\nspin_lock_init(&wq->q_lock);\r\nINIT_LIST_HEAD(&wq->q);\r\ninit_waitqueue_head(&wq->q_wait);\r\n}\r\nstruct drbd_tconn *conn_get_by_name(const char *name)\r\n{\r\nstruct drbd_tconn *tconn;\r\nif (!name || !name[0])\r\nreturn NULL;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(tconn, &drbd_tconns, all_tconn) {\r\nif (!strcmp(tconn->name, name)) {\r\nkref_get(&tconn->kref);\r\ngoto found;\r\n}\r\n}\r\ntconn = NULL;\r\nfound:\r\nrcu_read_unlock();\r\nreturn tconn;\r\n}\r\nstruct drbd_tconn *conn_get_by_addrs(void *my_addr, int my_addr_len,\r\nvoid *peer_addr, int peer_addr_len)\r\n{\r\nstruct drbd_tconn *tconn;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(tconn, &drbd_tconns, all_tconn) {\r\nif (tconn->my_addr_len == my_addr_len &&\r\ntconn->peer_addr_len == peer_addr_len &&\r\n!memcmp(&tconn->my_addr, my_addr, my_addr_len) &&\r\n!memcmp(&tconn->peer_addr, peer_addr, peer_addr_len)) {\r\nkref_get(&tconn->kref);\r\ngoto found;\r\n}\r\n}\r\ntconn = NULL;\r\nfound:\r\nrcu_read_unlock();\r\nreturn tconn;\r\n}\r\nstatic int drbd_alloc_socket(struct drbd_socket *socket)\r\n{\r\nsocket->rbuf = (void *) __get_free_page(GFP_KERNEL);\r\nif (!socket->rbuf)\r\nreturn -ENOMEM;\r\nsocket->sbuf = (void *) __get_free_page(GFP_KERNEL);\r\nif (!socket->sbuf)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void drbd_free_socket(struct drbd_socket *socket)\r\n{\r\nfree_page((unsigned long) socket->sbuf);\r\nfree_page((unsigned long) socket->rbuf);\r\n}\r\nvoid conn_free_crypto(struct drbd_tconn *tconn)\r\n{\r\ndrbd_free_sock(tconn);\r\ncrypto_free_hash(tconn->csums_tfm);\r\ncrypto_free_hash(tconn->verify_tfm);\r\ncrypto_free_hash(tconn->cram_hmac_tfm);\r\ncrypto_free_hash(tconn->integrity_tfm);\r\ncrypto_free_hash(tconn->peer_integrity_tfm);\r\nkfree(tconn->int_dig_in);\r\nkfree(tconn->int_dig_vv);\r\ntconn->csums_tfm = NULL;\r\ntconn->verify_tfm = NULL;\r\ntconn->cram_hmac_tfm = NULL;\r\ntconn->integrity_tfm = NULL;\r\ntconn->peer_integrity_tfm = NULL;\r\ntconn->int_dig_in = NULL;\r\ntconn->int_dig_vv = NULL;\r\n}\r\nint set_resource_options(struct drbd_tconn *tconn, struct res_opts *res_opts)\r\n{\r\ncpumask_var_t new_cpu_mask;\r\nint err;\r\nif (!zalloc_cpumask_var(&new_cpu_mask, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nif (nr_cpu_ids > 1 && res_opts->cpu_mask[0] != 0) {\r\nerr = bitmap_parse(res_opts->cpu_mask, 32,\r\ncpumask_bits(new_cpu_mask), nr_cpu_ids);\r\nif (err) {\r\nconn_warn(tconn, "bitmap_parse() failed with %d\n", err);\r\ngoto fail;\r\n}\r\n}\r\ntconn->res_opts = *res_opts;\r\nif (!cpumask_equal(tconn->cpu_mask, new_cpu_mask)) {\r\ncpumask_copy(tconn->cpu_mask, new_cpu_mask);\r\ndrbd_calc_cpu_mask(tconn);\r\ntconn->receiver.reset_cpu_mask = 1;\r\ntconn->asender.reset_cpu_mask = 1;\r\ntconn->worker.reset_cpu_mask = 1;\r\n}\r\nerr = 0;\r\nfail:\r\nfree_cpumask_var(new_cpu_mask);\r\nreturn err;\r\n}\r\nstruct drbd_tconn *conn_create(const char *name, struct res_opts *res_opts)\r\n{\r\nstruct drbd_tconn *tconn;\r\ntconn = kzalloc(sizeof(struct drbd_tconn), GFP_KERNEL);\r\nif (!tconn)\r\nreturn NULL;\r\ntconn->name = kstrdup(name, GFP_KERNEL);\r\nif (!tconn->name)\r\ngoto fail;\r\nif (drbd_alloc_socket(&tconn->data))\r\ngoto fail;\r\nif (drbd_alloc_socket(&tconn->meta))\r\ngoto fail;\r\nif (!zalloc_cpumask_var(&tconn->cpu_mask, GFP_KERNEL))\r\ngoto fail;\r\nif (set_resource_options(tconn, res_opts))\r\ngoto fail;\r\ntconn->current_epoch = kzalloc(sizeof(struct drbd_epoch), GFP_KERNEL);\r\nif (!tconn->current_epoch)\r\ngoto fail;\r\nINIT_LIST_HEAD(&tconn->transfer_log);\r\nINIT_LIST_HEAD(&tconn->current_epoch->list);\r\ntconn->epochs = 1;\r\nspin_lock_init(&tconn->epoch_lock);\r\ntconn->write_ordering = WO_bdev_flush;\r\ntconn->send.seen_any_write_yet = false;\r\ntconn->send.current_epoch_nr = 0;\r\ntconn->send.current_epoch_writes = 0;\r\ntconn->cstate = C_STANDALONE;\r\nmutex_init(&tconn->cstate_mutex);\r\nspin_lock_init(&tconn->req_lock);\r\nmutex_init(&tconn->conf_update);\r\ninit_waitqueue_head(&tconn->ping_wait);\r\nidr_init(&tconn->volumes);\r\ndrbd_init_workqueue(&tconn->sender_work);\r\nmutex_init(&tconn->data.mutex);\r\nmutex_init(&tconn->meta.mutex);\r\ndrbd_thread_init(tconn, &tconn->receiver, drbdd_init, "receiver");\r\ndrbd_thread_init(tconn, &tconn->worker, drbd_worker, "worker");\r\ndrbd_thread_init(tconn, &tconn->asender, drbd_asender, "asender");\r\nkref_init(&tconn->kref);\r\nlist_add_tail_rcu(&tconn->all_tconn, &drbd_tconns);\r\nreturn tconn;\r\nfail:\r\nkfree(tconn->current_epoch);\r\nfree_cpumask_var(tconn->cpu_mask);\r\ndrbd_free_socket(&tconn->meta);\r\ndrbd_free_socket(&tconn->data);\r\nkfree(tconn->name);\r\nkfree(tconn);\r\nreturn NULL;\r\n}\r\nvoid conn_destroy(struct kref *kref)\r\n{\r\nstruct drbd_tconn *tconn = container_of(kref, struct drbd_tconn, kref);\r\nif (atomic_read(&tconn->current_epoch->epoch_size) != 0)\r\nconn_err(tconn, "epoch_size:%d\n", atomic_read(&tconn->current_epoch->epoch_size));\r\nkfree(tconn->current_epoch);\r\nidr_destroy(&tconn->volumes);\r\nfree_cpumask_var(tconn->cpu_mask);\r\ndrbd_free_socket(&tconn->meta);\r\ndrbd_free_socket(&tconn->data);\r\nkfree(tconn->name);\r\nkfree(tconn->int_dig_in);\r\nkfree(tconn->int_dig_vv);\r\nkfree(tconn);\r\n}\r\nint init_submitter(struct drbd_conf *mdev)\r\n{\r\nmdev->submit.wq = alloc_workqueue("drbd%u_submit",\r\nWQ_UNBOUND | WQ_MEM_RECLAIM, 1, mdev->minor);\r\nif (!mdev->submit.wq)\r\nreturn -ENOMEM;\r\nINIT_WORK(&mdev->submit.worker, do_submit);\r\nspin_lock_init(&mdev->submit.lock);\r\nINIT_LIST_HEAD(&mdev->submit.writes);\r\nreturn 0;\r\n}\r\nenum drbd_ret_code conn_new_minor(struct drbd_tconn *tconn, unsigned int minor, int vnr)\r\n{\r\nstruct drbd_conf *mdev;\r\nstruct gendisk *disk;\r\nstruct request_queue *q;\r\nint vnr_got = vnr;\r\nint minor_got = minor;\r\nenum drbd_ret_code err = ERR_NOMEM;\r\nmdev = minor_to_mdev(minor);\r\nif (mdev)\r\nreturn ERR_MINOR_EXISTS;\r\nmdev = kzalloc(sizeof(struct drbd_conf), GFP_KERNEL);\r\nif (!mdev)\r\nreturn ERR_NOMEM;\r\nkref_get(&tconn->kref);\r\nmdev->tconn = tconn;\r\nmdev->minor = minor;\r\nmdev->vnr = vnr;\r\ndrbd_init_set_defaults(mdev);\r\nq = blk_alloc_queue(GFP_KERNEL);\r\nif (!q)\r\ngoto out_no_q;\r\nmdev->rq_queue = q;\r\nq->queuedata = mdev;\r\ndisk = alloc_disk(1);\r\nif (!disk)\r\ngoto out_no_disk;\r\nmdev->vdisk = disk;\r\nset_disk_ro(disk, true);\r\ndisk->queue = q;\r\ndisk->major = DRBD_MAJOR;\r\ndisk->first_minor = minor;\r\ndisk->fops = &drbd_ops;\r\nsprintf(disk->disk_name, "drbd%d", minor);\r\ndisk->private_data = mdev;\r\nmdev->this_bdev = bdget(MKDEV(DRBD_MAJOR, minor));\r\nmdev->this_bdev->bd_contains = mdev->this_bdev;\r\nq->backing_dev_info.congested_fn = drbd_congested;\r\nq->backing_dev_info.congested_data = mdev;\r\nblk_queue_make_request(q, drbd_make_request);\r\nblk_queue_flush(q, REQ_FLUSH | REQ_FUA);\r\nblk_queue_max_hw_sectors(q, DRBD_MAX_BIO_SIZE_SAFE >> 8);\r\nblk_queue_bounce_limit(q, BLK_BOUNCE_ANY);\r\nblk_queue_merge_bvec(q, drbd_merge_bvec);\r\nq->queue_lock = &mdev->tconn->req_lock;\r\nmdev->md_io_page = alloc_page(GFP_KERNEL);\r\nif (!mdev->md_io_page)\r\ngoto out_no_io_page;\r\nif (drbd_bm_init(mdev))\r\ngoto out_no_bitmap;\r\nmdev->read_requests = RB_ROOT;\r\nmdev->write_requests = RB_ROOT;\r\nminor_got = idr_alloc(&minors, mdev, minor, minor + 1, GFP_KERNEL);\r\nif (minor_got < 0) {\r\nif (minor_got == -ENOSPC) {\r\nerr = ERR_MINOR_EXISTS;\r\ndrbd_msg_put_info("requested minor exists already");\r\n}\r\ngoto out_no_minor_idr;\r\n}\r\nvnr_got = idr_alloc(&tconn->volumes, mdev, vnr, vnr + 1, GFP_KERNEL);\r\nif (vnr_got < 0) {\r\nif (vnr_got == -ENOSPC) {\r\nerr = ERR_INVALID_REQUEST;\r\ndrbd_msg_put_info("requested volume exists already");\r\n}\r\ngoto out_idr_remove_minor;\r\n}\r\nif (init_submitter(mdev)) {\r\nerr = ERR_NOMEM;\r\ndrbd_msg_put_info("unable to create submit workqueue");\r\ngoto out_idr_remove_vol;\r\n}\r\nadd_disk(disk);\r\nkref_init(&mdev->kref);\r\nmdev->state.conn = tconn->cstate;\r\nif (mdev->state.conn == C_WF_REPORT_PARAMS)\r\ndrbd_connected(mdev);\r\nreturn NO_ERROR;\r\nout_idr_remove_vol:\r\nidr_remove(&tconn->volumes, vnr_got);\r\nout_idr_remove_minor:\r\nidr_remove(&minors, minor_got);\r\nsynchronize_rcu();\r\nout_no_minor_idr:\r\ndrbd_bm_cleanup(mdev);\r\nout_no_bitmap:\r\n__free_page(mdev->md_io_page);\r\nout_no_io_page:\r\nput_disk(disk);\r\nout_no_disk:\r\nblk_cleanup_queue(q);\r\nout_no_q:\r\nkfree(mdev);\r\nkref_put(&tconn->kref, &conn_destroy);\r\nreturn err;\r\n}\r\nint __init drbd_init(void)\r\n{\r\nint err;\r\nif (minor_count < DRBD_MINOR_COUNT_MIN || minor_count > DRBD_MINOR_COUNT_MAX) {\r\nprintk(KERN_ERR\r\n"drbd: invalid minor_count (%d)\n", minor_count);\r\n#ifdef MODULE\r\nreturn -EINVAL;\r\n#else\r\nminor_count = DRBD_MINOR_COUNT_DEF;\r\n#endif\r\n}\r\nerr = register_blkdev(DRBD_MAJOR, "drbd");\r\nif (err) {\r\nprintk(KERN_ERR\r\n"drbd: unable to register block device major %d\n",\r\nDRBD_MAJOR);\r\nreturn err;\r\n}\r\nerr = drbd_genl_register();\r\nif (err) {\r\nprintk(KERN_ERR "drbd: unable to register generic netlink family\n");\r\ngoto fail;\r\n}\r\nregister_reboot_notifier(&drbd_notifier);\r\ninit_waitqueue_head(&drbd_pp_wait);\r\ndrbd_proc = NULL;\r\nidr_init(&minors);\r\nerr = drbd_create_mempools();\r\nif (err)\r\ngoto fail;\r\nerr = -ENOMEM;\r\ndrbd_proc = proc_create_data("drbd", S_IFREG | S_IRUGO , NULL, &drbd_proc_fops, NULL);\r\nif (!drbd_proc) {\r\nprintk(KERN_ERR "drbd: unable to register proc file\n");\r\ngoto fail;\r\n}\r\nrwlock_init(&global_state_lock);\r\nINIT_LIST_HEAD(&drbd_tconns);\r\nretry.wq = create_singlethread_workqueue("drbd-reissue");\r\nif (!retry.wq) {\r\nprintk(KERN_ERR "drbd: unable to create retry workqueue\n");\r\ngoto fail;\r\n}\r\nINIT_WORK(&retry.worker, do_retry);\r\nspin_lock_init(&retry.lock);\r\nINIT_LIST_HEAD(&retry.writes);\r\nprintk(KERN_INFO "drbd: initialized. "\r\n"Version: " REL_VERSION " (api:%d/proto:%d-%d)\n",\r\nAPI_VERSION, PRO_VERSION_MIN, PRO_VERSION_MAX);\r\nprintk(KERN_INFO "drbd: %s\n", drbd_buildtag());\r\nprintk(KERN_INFO "drbd: registered as block device major %d\n",\r\nDRBD_MAJOR);\r\nreturn 0;\r\nfail:\r\ndrbd_cleanup();\r\nif (err == -ENOMEM)\r\nprintk(KERN_ERR "drbd: ran out of memory\n");\r\nelse\r\nprintk(KERN_ERR "drbd: initialization failure\n");\r\nreturn err;\r\n}\r\nvoid drbd_free_bc(struct drbd_backing_dev *ldev)\r\n{\r\nif (ldev == NULL)\r\nreturn;\r\nblkdev_put(ldev->backing_bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nblkdev_put(ldev->md_bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nkfree(ldev->disk_conf);\r\nkfree(ldev);\r\n}\r\nvoid drbd_free_sock(struct drbd_tconn *tconn)\r\n{\r\nif (tconn->data.socket) {\r\nmutex_lock(&tconn->data.mutex);\r\nkernel_sock_shutdown(tconn->data.socket, SHUT_RDWR);\r\nsock_release(tconn->data.socket);\r\ntconn->data.socket = NULL;\r\nmutex_unlock(&tconn->data.mutex);\r\n}\r\nif (tconn->meta.socket) {\r\nmutex_lock(&tconn->meta.mutex);\r\nkernel_sock_shutdown(tconn->meta.socket, SHUT_RDWR);\r\nsock_release(tconn->meta.socket);\r\ntconn->meta.socket = NULL;\r\nmutex_unlock(&tconn->meta.mutex);\r\n}\r\n}\r\nvoid conn_md_sync(struct drbd_tconn *tconn)\r\n{\r\nstruct drbd_conf *mdev;\r\nint vnr;\r\nrcu_read_lock();\r\nidr_for_each_entry(&tconn->volumes, mdev, vnr) {\r\nkref_get(&mdev->kref);\r\nrcu_read_unlock();\r\ndrbd_md_sync(mdev);\r\nkref_put(&mdev->kref, &drbd_minor_destroy);\r\nrcu_read_lock();\r\n}\r\nrcu_read_unlock();\r\n}\r\nvoid drbd_md_write(struct drbd_conf *mdev, void *b)\r\n{\r\nstruct meta_data_on_disk *buffer = b;\r\nsector_t sector;\r\nint i;\r\nmemset(buffer, 0, sizeof(*buffer));\r\nbuffer->la_size_sect = cpu_to_be64(drbd_get_capacity(mdev->this_bdev));\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\nbuffer->uuid[i] = cpu_to_be64(mdev->ldev->md.uuid[i]);\r\nbuffer->flags = cpu_to_be32(mdev->ldev->md.flags);\r\nbuffer->magic = cpu_to_be32(DRBD_MD_MAGIC_84_UNCLEAN);\r\nbuffer->md_size_sect = cpu_to_be32(mdev->ldev->md.md_size_sect);\r\nbuffer->al_offset = cpu_to_be32(mdev->ldev->md.al_offset);\r\nbuffer->al_nr_extents = cpu_to_be32(mdev->act_log->nr_elements);\r\nbuffer->bm_bytes_per_bit = cpu_to_be32(BM_BLOCK_SIZE);\r\nbuffer->device_uuid = cpu_to_be64(mdev->ldev->md.device_uuid);\r\nbuffer->bm_offset = cpu_to_be32(mdev->ldev->md.bm_offset);\r\nbuffer->la_peer_max_bio_size = cpu_to_be32(mdev->peer_max_bio_size);\r\nbuffer->al_stripes = cpu_to_be32(mdev->ldev->md.al_stripes);\r\nbuffer->al_stripe_size_4k = cpu_to_be32(mdev->ldev->md.al_stripe_size_4k);\r\nD_ASSERT(drbd_md_ss(mdev->ldev) == mdev->ldev->md.md_offset);\r\nsector = mdev->ldev->md.md_offset;\r\nif (drbd_md_sync_page_io(mdev, mdev->ldev, sector, WRITE)) {\r\ndev_err(DEV, "meta data update failed!\n");\r\ndrbd_chk_io_error(mdev, 1, DRBD_META_IO_ERROR);\r\n}\r\n}\r\nvoid drbd_md_sync(struct drbd_conf *mdev)\r\n{\r\nstruct meta_data_on_disk *buffer;\r\nBUILD_BUG_ON(UI_SIZE != 4);\r\nBUILD_BUG_ON(sizeof(struct meta_data_on_disk) != 4096);\r\ndel_timer(&mdev->md_sync_timer);\r\nif (!test_and_clear_bit(MD_DIRTY, &mdev->flags))\r\nreturn;\r\nif (!get_ldev_if_state(mdev, D_FAILED))\r\nreturn;\r\nbuffer = drbd_md_get_buffer(mdev);\r\nif (!buffer)\r\ngoto out;\r\ndrbd_md_write(mdev, buffer);\r\nmdev->ldev->md.la_size_sect = drbd_get_capacity(mdev->this_bdev);\r\ndrbd_md_put_buffer(mdev);\r\nout:\r\nput_ldev(mdev);\r\n}\r\nstatic int check_activity_log_stripe_size(struct drbd_conf *mdev,\r\nstruct meta_data_on_disk *on_disk,\r\nstruct drbd_md *in_core)\r\n{\r\nu32 al_stripes = be32_to_cpu(on_disk->al_stripes);\r\nu32 al_stripe_size_4k = be32_to_cpu(on_disk->al_stripe_size_4k);\r\nu64 al_size_4k;\r\nif (al_stripes == 0 && al_stripe_size_4k == 0) {\r\nal_stripes = 1;\r\nal_stripe_size_4k = MD_32kB_SECT/8;\r\n}\r\nif (al_stripes == 0 || al_stripe_size_4k == 0)\r\ngoto err;\r\nal_size_4k = (u64)al_stripes * al_stripe_size_4k;\r\nif (al_size_4k > (16 * 1024 * 1024/4))\r\ngoto err;\r\nif (al_size_4k < MD_32kB_SECT/8)\r\ngoto err;\r\nin_core->al_stripe_size_4k = al_stripe_size_4k;\r\nin_core->al_stripes = al_stripes;\r\nin_core->al_size_4k = al_size_4k;\r\nreturn 0;\r\nerr:\r\ndev_err(DEV, "invalid activity log striping: al_stripes=%u, al_stripe_size_4k=%u\n",\r\nal_stripes, al_stripe_size_4k);\r\nreturn -EINVAL;\r\n}\r\nstatic int check_offsets_and_sizes(struct drbd_conf *mdev, struct drbd_backing_dev *bdev)\r\n{\r\nsector_t capacity = drbd_get_capacity(bdev->md_bdev);\r\nstruct drbd_md *in_core = &bdev->md;\r\ns32 on_disk_al_sect;\r\ns32 on_disk_bm_sect;\r\nif (in_core->al_offset < 0) {\r\nif (in_core->bm_offset > in_core->al_offset)\r\ngoto err;\r\non_disk_al_sect = -in_core->al_offset;\r\non_disk_bm_sect = in_core->al_offset - in_core->bm_offset;\r\n} else {\r\nif (in_core->al_offset != MD_4kB_SECT)\r\ngoto err;\r\nif (in_core->bm_offset < in_core->al_offset + in_core->al_size_4k * MD_4kB_SECT)\r\ngoto err;\r\non_disk_al_sect = in_core->bm_offset - MD_4kB_SECT;\r\non_disk_bm_sect = in_core->md_size_sect - in_core->bm_offset;\r\n}\r\nif (in_core->meta_dev_idx >= 0) {\r\nif (in_core->md_size_sect != MD_128MB_SECT\r\n|| in_core->al_offset != MD_4kB_SECT\r\n|| in_core->bm_offset != MD_4kB_SECT + MD_32kB_SECT\r\n|| in_core->al_stripes != 1\r\n|| in_core->al_stripe_size_4k != MD_32kB_SECT/8)\r\ngoto err;\r\n}\r\nif (capacity < in_core->md_size_sect)\r\ngoto err;\r\nif (capacity - in_core->md_size_sect < drbd_md_first_sector(bdev))\r\ngoto err;\r\nif ((on_disk_al_sect & 7) || (on_disk_al_sect < MD_32kB_SECT))\r\ngoto err;\r\nif (on_disk_al_sect != in_core->al_size_4k * MD_4kB_SECT)\r\ngoto err;\r\nif (in_core->bm_offset & 7)\r\ngoto err;\r\nif (on_disk_bm_sect < (in_core->la_size_sect+7)/MD_4kB_SECT/8/512)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\ndev_err(DEV, "meta data offsets don't make sense: idx=%d "\r\n"al_s=%u, al_sz4k=%u, al_offset=%d, bm_offset=%d, "\r\n"md_size_sect=%u, la_size=%llu, md_capacity=%llu\n",\r\nin_core->meta_dev_idx,\r\nin_core->al_stripes, in_core->al_stripe_size_4k,\r\nin_core->al_offset, in_core->bm_offset, in_core->md_size_sect,\r\n(unsigned long long)in_core->la_size_sect,\r\n(unsigned long long)capacity);\r\nreturn -EINVAL;\r\n}\r\nint drbd_md_read(struct drbd_conf *mdev, struct drbd_backing_dev *bdev)\r\n{\r\nstruct meta_data_on_disk *buffer;\r\nu32 magic, flags;\r\nint i, rv = NO_ERROR;\r\nif (mdev->state.disk != D_DISKLESS)\r\nreturn ERR_DISK_CONFIGURED;\r\nbuffer = drbd_md_get_buffer(mdev);\r\nif (!buffer)\r\nreturn ERR_NOMEM;\r\nbdev->md.meta_dev_idx = bdev->disk_conf->meta_dev_idx;\r\nbdev->md.md_offset = drbd_md_ss(bdev);\r\nif (drbd_md_sync_page_io(mdev, bdev, bdev->md.md_offset, READ)) {\r\ndev_err(DEV, "Error while reading metadata.\n");\r\nrv = ERR_IO_MD_DISK;\r\ngoto err;\r\n}\r\nmagic = be32_to_cpu(buffer->magic);\r\nflags = be32_to_cpu(buffer->flags);\r\nif (magic == DRBD_MD_MAGIC_84_UNCLEAN ||\r\n(magic == DRBD_MD_MAGIC_08 && !(flags & MDF_AL_CLEAN))) {\r\ndev_err(DEV, "Found unclean meta data. Did you \"drbdadm apply-al\"?\n");\r\nrv = ERR_MD_UNCLEAN;\r\ngoto err;\r\n}\r\nrv = ERR_MD_INVALID;\r\nif (magic != DRBD_MD_MAGIC_08) {\r\nif (magic == DRBD_MD_MAGIC_07)\r\ndev_err(DEV, "Found old (0.7) meta data magic. Did you \"drbdadm create-md\"?\n");\r\nelse\r\ndev_err(DEV, "Meta data magic not found. Did you \"drbdadm create-md\"?\n");\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->bm_bytes_per_bit) != BM_BLOCK_SIZE) {\r\ndev_err(DEV, "unexpected bm_bytes_per_bit: %u (expected %u)\n",\r\nbe32_to_cpu(buffer->bm_bytes_per_bit), BM_BLOCK_SIZE);\r\ngoto err;\r\n}\r\nbdev->md.la_size_sect = be64_to_cpu(buffer->la_size_sect);\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\nbdev->md.uuid[i] = be64_to_cpu(buffer->uuid[i]);\r\nbdev->md.flags = be32_to_cpu(buffer->flags);\r\nbdev->md.device_uuid = be64_to_cpu(buffer->device_uuid);\r\nbdev->md.md_size_sect = be32_to_cpu(buffer->md_size_sect);\r\nbdev->md.al_offset = be32_to_cpu(buffer->al_offset);\r\nbdev->md.bm_offset = be32_to_cpu(buffer->bm_offset);\r\nif (check_activity_log_stripe_size(mdev, buffer, &bdev->md))\r\ngoto err;\r\nif (check_offsets_and_sizes(mdev, bdev))\r\ngoto err;\r\nif (be32_to_cpu(buffer->bm_offset) != bdev->md.bm_offset) {\r\ndev_err(DEV, "unexpected bm_offset: %d (expected %d)\n",\r\nbe32_to_cpu(buffer->bm_offset), bdev->md.bm_offset);\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->md_size_sect) != bdev->md.md_size_sect) {\r\ndev_err(DEV, "unexpected md_size: %u (expected %u)\n",\r\nbe32_to_cpu(buffer->md_size_sect), bdev->md.md_size_sect);\r\ngoto err;\r\n}\r\nrv = NO_ERROR;\r\nspin_lock_irq(&mdev->tconn->req_lock);\r\nif (mdev->state.conn < C_CONNECTED) {\r\nunsigned int peer;\r\npeer = be32_to_cpu(buffer->la_peer_max_bio_size);\r\npeer = max(peer, DRBD_MAX_BIO_SIZE_SAFE);\r\nmdev->peer_max_bio_size = peer;\r\n}\r\nspin_unlock_irq(&mdev->tconn->req_lock);\r\nerr:\r\ndrbd_md_put_buffer(mdev);\r\nreturn rv;\r\n}\r\nvoid drbd_md_mark_dirty_(struct drbd_conf *mdev, unsigned int line, const char *func)\r\n{\r\nif (!test_and_set_bit(MD_DIRTY, &mdev->flags)) {\r\nmod_timer(&mdev->md_sync_timer, jiffies + HZ);\r\nmdev->last_md_mark_dirty.line = line;\r\nmdev->last_md_mark_dirty.func = func;\r\n}\r\n}\r\nvoid drbd_md_mark_dirty(struct drbd_conf *mdev)\r\n{\r\nif (!test_and_set_bit(MD_DIRTY, &mdev->flags))\r\nmod_timer(&mdev->md_sync_timer, jiffies + 5*HZ);\r\n}\r\nvoid drbd_uuid_move_history(struct drbd_conf *mdev) __must_hold(local)\r\n{\r\nint i;\r\nfor (i = UI_HISTORY_START; i < UI_HISTORY_END; i++)\r\nmdev->ldev->md.uuid[i+1] = mdev->ldev->md.uuid[i];\r\n}\r\nvoid __drbd_uuid_set(struct drbd_conf *mdev, int idx, u64 val) __must_hold(local)\r\n{\r\nif (idx == UI_CURRENT) {\r\nif (mdev->state.role == R_PRIMARY)\r\nval |= 1;\r\nelse\r\nval &= ~((u64)1);\r\ndrbd_set_ed_uuid(mdev, val);\r\n}\r\nmdev->ldev->md.uuid[idx] = val;\r\ndrbd_md_mark_dirty(mdev);\r\n}\r\nvoid _drbd_uuid_set(struct drbd_conf *mdev, int idx, u64 val) __must_hold(local)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&mdev->ldev->md.uuid_lock, flags);\r\n__drbd_uuid_set(mdev, idx, val);\r\nspin_unlock_irqrestore(&mdev->ldev->md.uuid_lock, flags);\r\n}\r\nvoid drbd_uuid_set(struct drbd_conf *mdev, int idx, u64 val) __must_hold(local)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&mdev->ldev->md.uuid_lock, flags);\r\nif (mdev->ldev->md.uuid[idx]) {\r\ndrbd_uuid_move_history(mdev);\r\nmdev->ldev->md.uuid[UI_HISTORY_START] = mdev->ldev->md.uuid[idx];\r\n}\r\n__drbd_uuid_set(mdev, idx, val);\r\nspin_unlock_irqrestore(&mdev->ldev->md.uuid_lock, flags);\r\n}\r\nvoid drbd_uuid_new_current(struct drbd_conf *mdev) __must_hold(local)\r\n{\r\nu64 val;\r\nunsigned long long bm_uuid;\r\nget_random_bytes(&val, sizeof(u64));\r\nspin_lock_irq(&mdev->ldev->md.uuid_lock);\r\nbm_uuid = mdev->ldev->md.uuid[UI_BITMAP];\r\nif (bm_uuid)\r\ndev_warn(DEV, "bm UUID was already set: %llX\n", bm_uuid);\r\nmdev->ldev->md.uuid[UI_BITMAP] = mdev->ldev->md.uuid[UI_CURRENT];\r\n__drbd_uuid_set(mdev, UI_CURRENT, val);\r\nspin_unlock_irq(&mdev->ldev->md.uuid_lock);\r\ndrbd_print_uuids(mdev, "new current UUID");\r\ndrbd_md_sync(mdev);\r\n}\r\nvoid drbd_uuid_set_bm(struct drbd_conf *mdev, u64 val) __must_hold(local)\r\n{\r\nunsigned long flags;\r\nif (mdev->ldev->md.uuid[UI_BITMAP] == 0 && val == 0)\r\nreturn;\r\nspin_lock_irqsave(&mdev->ldev->md.uuid_lock, flags);\r\nif (val == 0) {\r\ndrbd_uuid_move_history(mdev);\r\nmdev->ldev->md.uuid[UI_HISTORY_START] = mdev->ldev->md.uuid[UI_BITMAP];\r\nmdev->ldev->md.uuid[UI_BITMAP] = 0;\r\n} else {\r\nunsigned long long bm_uuid = mdev->ldev->md.uuid[UI_BITMAP];\r\nif (bm_uuid)\r\ndev_warn(DEV, "bm UUID was already set: %llX\n", bm_uuid);\r\nmdev->ldev->md.uuid[UI_BITMAP] = val & ~((u64)1);\r\n}\r\nspin_unlock_irqrestore(&mdev->ldev->md.uuid_lock, flags);\r\ndrbd_md_mark_dirty(mdev);\r\n}\r\nint drbd_bmio_set_n_write(struct drbd_conf *mdev)\r\n{\r\nint rv = -EIO;\r\nif (get_ldev_if_state(mdev, D_ATTACHING)) {\r\ndrbd_md_set_flag(mdev, MDF_FULL_SYNC);\r\ndrbd_md_sync(mdev);\r\ndrbd_bm_set_all(mdev);\r\nrv = drbd_bm_write(mdev);\r\nif (!rv) {\r\ndrbd_md_clear_flag(mdev, MDF_FULL_SYNC);\r\ndrbd_md_sync(mdev);\r\n}\r\nput_ldev(mdev);\r\n}\r\nreturn rv;\r\n}\r\nint drbd_bmio_clear_n_write(struct drbd_conf *mdev)\r\n{\r\nint rv = -EIO;\r\ndrbd_resume_al(mdev);\r\nif (get_ldev_if_state(mdev, D_ATTACHING)) {\r\ndrbd_bm_clear_all(mdev);\r\nrv = drbd_bm_write(mdev);\r\nput_ldev(mdev);\r\n}\r\nreturn rv;\r\n}\r\nstatic int w_bitmap_io(struct drbd_work *w, int unused)\r\n{\r\nstruct bm_io_work *work = container_of(w, struct bm_io_work, w);\r\nstruct drbd_conf *mdev = w->mdev;\r\nint rv = -EIO;\r\nD_ASSERT(atomic_read(&mdev->ap_bio_cnt) == 0);\r\nif (get_ldev(mdev)) {\r\ndrbd_bm_lock(mdev, work->why, work->flags);\r\nrv = work->io_fn(mdev);\r\ndrbd_bm_unlock(mdev);\r\nput_ldev(mdev);\r\n}\r\nclear_bit_unlock(BITMAP_IO, &mdev->flags);\r\nwake_up(&mdev->misc_wait);\r\nif (work->done)\r\nwork->done(mdev, rv);\r\nclear_bit(BITMAP_IO_QUEUED, &mdev->flags);\r\nwork->why = NULL;\r\nwork->flags = 0;\r\nreturn 0;\r\n}\r\nvoid drbd_ldev_destroy(struct drbd_conf *mdev)\r\n{\r\nlc_destroy(mdev->resync);\r\nmdev->resync = NULL;\r\nlc_destroy(mdev->act_log);\r\nmdev->act_log = NULL;\r\n__no_warn(local,\r\ndrbd_free_bc(mdev->ldev);\r\nmdev->ldev = NULL;);\r\nclear_bit(GO_DISKLESS, &mdev->flags);\r\n}\r\nstatic int w_go_diskless(struct drbd_work *w, int unused)\r\n{\r\nstruct drbd_conf *mdev = w->mdev;\r\nD_ASSERT(mdev->state.disk == D_FAILED);\r\nif (mdev->bitmap && mdev->ldev) {\r\nif (drbd_bitmap_io_from_worker(mdev, drbd_bm_write,\r\n"detach", BM_LOCKED_TEST_ALLOWED)) {\r\nif (test_bit(WAS_READ_ERROR, &mdev->flags)) {\r\ndrbd_md_set_flag(mdev, MDF_FULL_SYNC);\r\ndrbd_md_sync(mdev);\r\n}\r\n}\r\n}\r\ndrbd_force_state(mdev, NS(disk, D_DISKLESS));\r\nreturn 0;\r\n}\r\nvoid drbd_queue_bitmap_io(struct drbd_conf *mdev,\r\nint (*io_fn)(struct drbd_conf *),\r\nvoid (*done)(struct drbd_conf *, int),\r\nchar *why, enum bm_flag flags)\r\n{\r\nD_ASSERT(current == mdev->tconn->worker.task);\r\nD_ASSERT(!test_bit(BITMAP_IO_QUEUED, &mdev->flags));\r\nD_ASSERT(!test_bit(BITMAP_IO, &mdev->flags));\r\nD_ASSERT(list_empty(&mdev->bm_io_work.w.list));\r\nif (mdev->bm_io_work.why)\r\ndev_err(DEV, "FIXME going to queue '%s' but '%s' still pending?\n",\r\nwhy, mdev->bm_io_work.why);\r\nmdev->bm_io_work.io_fn = io_fn;\r\nmdev->bm_io_work.done = done;\r\nmdev->bm_io_work.why = why;\r\nmdev->bm_io_work.flags = flags;\r\nspin_lock_irq(&mdev->tconn->req_lock);\r\nset_bit(BITMAP_IO, &mdev->flags);\r\nif (atomic_read(&mdev->ap_bio_cnt) == 0) {\r\nif (!test_and_set_bit(BITMAP_IO_QUEUED, &mdev->flags))\r\ndrbd_queue_work(&mdev->tconn->sender_work, &mdev->bm_io_work.w);\r\n}\r\nspin_unlock_irq(&mdev->tconn->req_lock);\r\n}\r\nint drbd_bitmap_io(struct drbd_conf *mdev, int (*io_fn)(struct drbd_conf *),\r\nchar *why, enum bm_flag flags)\r\n{\r\nint rv;\r\nD_ASSERT(current != mdev->tconn->worker.task);\r\nif ((flags & BM_LOCKED_SET_ALLOWED) == 0)\r\ndrbd_suspend_io(mdev);\r\ndrbd_bm_lock(mdev, why, flags);\r\nrv = io_fn(mdev);\r\ndrbd_bm_unlock(mdev);\r\nif ((flags & BM_LOCKED_SET_ALLOWED) == 0)\r\ndrbd_resume_io(mdev);\r\nreturn rv;\r\n}\r\nvoid drbd_md_set_flag(struct drbd_conf *mdev, int flag) __must_hold(local)\r\n{\r\nif ((mdev->ldev->md.flags & flag) != flag) {\r\ndrbd_md_mark_dirty(mdev);\r\nmdev->ldev->md.flags |= flag;\r\n}\r\n}\r\nvoid drbd_md_clear_flag(struct drbd_conf *mdev, int flag) __must_hold(local)\r\n{\r\nif ((mdev->ldev->md.flags & flag) != 0) {\r\ndrbd_md_mark_dirty(mdev);\r\nmdev->ldev->md.flags &= ~flag;\r\n}\r\n}\r\nint drbd_md_test_flag(struct drbd_backing_dev *bdev, int flag)\r\n{\r\nreturn (bdev->md.flags & flag) != 0;\r\n}\r\nstatic void md_sync_timer_fn(unsigned long data)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *) data;\r\nif (list_empty(&mdev->md_sync_work.list))\r\ndrbd_queue_work_front(&mdev->tconn->sender_work, &mdev->md_sync_work);\r\n}\r\nstatic int w_md_sync(struct drbd_work *w, int unused)\r\n{\r\nstruct drbd_conf *mdev = w->mdev;\r\ndev_warn(DEV, "md_sync_timer expired! Worker calls drbd_md_sync().\n");\r\n#ifdef DEBUG\r\ndev_warn(DEV, "last md_mark_dirty: %s:%u\n",\r\nmdev->last_md_mark_dirty.func, mdev->last_md_mark_dirty.line);\r\n#endif\r\ndrbd_md_sync(mdev);\r\nreturn 0;\r\n}\r\nconst char *cmdname(enum drbd_packet cmd)\r\n{\r\nstatic const char *cmdnames[] = {\r\n[P_DATA] = "Data",\r\n[P_DATA_REPLY] = "DataReply",\r\n[P_RS_DATA_REPLY] = "RSDataReply",\r\n[P_BARRIER] = "Barrier",\r\n[P_BITMAP] = "ReportBitMap",\r\n[P_BECOME_SYNC_TARGET] = "BecomeSyncTarget",\r\n[P_BECOME_SYNC_SOURCE] = "BecomeSyncSource",\r\n[P_UNPLUG_REMOTE] = "UnplugRemote",\r\n[P_DATA_REQUEST] = "DataRequest",\r\n[P_RS_DATA_REQUEST] = "RSDataRequest",\r\n[P_SYNC_PARAM] = "SyncParam",\r\n[P_SYNC_PARAM89] = "SyncParam89",\r\n[P_PROTOCOL] = "ReportProtocol",\r\n[P_UUIDS] = "ReportUUIDs",\r\n[P_SIZES] = "ReportSizes",\r\n[P_STATE] = "ReportState",\r\n[P_SYNC_UUID] = "ReportSyncUUID",\r\n[P_AUTH_CHALLENGE] = "AuthChallenge",\r\n[P_AUTH_RESPONSE] = "AuthResponse",\r\n[P_PING] = "Ping",\r\n[P_PING_ACK] = "PingAck",\r\n[P_RECV_ACK] = "RecvAck",\r\n[P_WRITE_ACK] = "WriteAck",\r\n[P_RS_WRITE_ACK] = "RSWriteAck",\r\n[P_SUPERSEDED] = "Superseded",\r\n[P_NEG_ACK] = "NegAck",\r\n[P_NEG_DREPLY] = "NegDReply",\r\n[P_NEG_RS_DREPLY] = "NegRSDReply",\r\n[P_BARRIER_ACK] = "BarrierAck",\r\n[P_STATE_CHG_REQ] = "StateChgRequest",\r\n[P_STATE_CHG_REPLY] = "StateChgReply",\r\n[P_OV_REQUEST] = "OVRequest",\r\n[P_OV_REPLY] = "OVReply",\r\n[P_OV_RESULT] = "OVResult",\r\n[P_CSUM_RS_REQUEST] = "CsumRSRequest",\r\n[P_RS_IS_IN_SYNC] = "CsumRSIsInSync",\r\n[P_COMPRESSED_BITMAP] = "CBitmap",\r\n[P_DELAY_PROBE] = "DelayProbe",\r\n[P_OUT_OF_SYNC] = "OutOfSync",\r\n[P_RETRY_WRITE] = "RetryWrite",\r\n[P_RS_CANCEL] = "RSCancel",\r\n[P_CONN_ST_CHG_REQ] = "conn_st_chg_req",\r\n[P_CONN_ST_CHG_REPLY] = "conn_st_chg_reply",\r\n[P_RETRY_WRITE] = "retry_write",\r\n[P_PROTOCOL_UPDATE] = "protocol_update",\r\n};\r\nif (cmd == P_INITIAL_META)\r\nreturn "InitialMeta";\r\nif (cmd == P_INITIAL_DATA)\r\nreturn "InitialData";\r\nif (cmd == P_CONNECTION_FEATURES)\r\nreturn "ConnectionFeatures";\r\nif (cmd >= ARRAY_SIZE(cmdnames))\r\nreturn "Unknown";\r\nreturn cmdnames[cmd];\r\n}\r\nint drbd_wait_misc(struct drbd_conf *mdev, struct drbd_interval *i)\r\n{\r\nstruct net_conf *nc;\r\nDEFINE_WAIT(wait);\r\nlong timeout;\r\nrcu_read_lock();\r\nnc = rcu_dereference(mdev->tconn->net_conf);\r\nif (!nc) {\r\nrcu_read_unlock();\r\nreturn -ETIMEDOUT;\r\n}\r\ntimeout = nc->ko_count ? nc->timeout * HZ / 10 * nc->ko_count : MAX_SCHEDULE_TIMEOUT;\r\nrcu_read_unlock();\r\ni->waiting = true;\r\nprepare_to_wait(&mdev->misc_wait, &wait, TASK_INTERRUPTIBLE);\r\nspin_unlock_irq(&mdev->tconn->req_lock);\r\ntimeout = schedule_timeout(timeout);\r\nfinish_wait(&mdev->misc_wait, &wait);\r\nspin_lock_irq(&mdev->tconn->req_lock);\r\nif (!timeout || mdev->state.conn < C_CONNECTED)\r\nreturn -ETIMEDOUT;\r\nif (signal_pending(current))\r\nreturn -ERESTARTSYS;\r\nreturn 0;\r\n}\r\nstatic unsigned long\r\n_drbd_fault_random(struct fault_random_state *rsp)\r\n{\r\nlong refresh;\r\nif (!rsp->count--) {\r\nget_random_bytes(&refresh, sizeof(refresh));\r\nrsp->state += refresh;\r\nrsp->count = FAULT_RANDOM_REFRESH;\r\n}\r\nrsp->state = rsp->state * FAULT_RANDOM_MULT + FAULT_RANDOM_ADD;\r\nreturn swahw32(rsp->state);\r\n}\r\nstatic char *\r\n_drbd_fault_str(unsigned int type) {\r\nstatic char *_faults[] = {\r\n[DRBD_FAULT_MD_WR] = "Meta-data write",\r\n[DRBD_FAULT_MD_RD] = "Meta-data read",\r\n[DRBD_FAULT_RS_WR] = "Resync write",\r\n[DRBD_FAULT_RS_RD] = "Resync read",\r\n[DRBD_FAULT_DT_WR] = "Data write",\r\n[DRBD_FAULT_DT_RD] = "Data read",\r\n[DRBD_FAULT_DT_RA] = "Data read ahead",\r\n[DRBD_FAULT_BM_ALLOC] = "BM allocation",\r\n[DRBD_FAULT_AL_EE] = "EE allocation",\r\n[DRBD_FAULT_RECEIVE] = "receive data corruption",\r\n};\r\nreturn (type < DRBD_FAULT_MAX) ? _faults[type] : "**Unknown**";\r\n}\r\nunsigned int\r\n_drbd_insert_fault(struct drbd_conf *mdev, unsigned int type)\r\n{\r\nstatic struct fault_random_state rrs = {0, 0};\r\nunsigned int ret = (\r\n(fault_devs == 0 ||\r\n((1 << mdev_to_minor(mdev)) & fault_devs) != 0) &&\r\n(((_drbd_fault_random(&rrs) % 100) + 1) <= fault_rate));\r\nif (ret) {\r\nfault_count++;\r\nif (__ratelimit(&drbd_ratelimit_state))\r\ndev_warn(DEV, "***Simulating %s failure\n",\r\n_drbd_fault_str(type));\r\n}\r\nreturn ret;\r\n}\r\nconst char *drbd_buildtag(void)\r\n{\r\nstatic char buildtag[38] = "\0uilt-in";\r\nif (buildtag[0] == 0) {\r\n#ifdef MODULE\r\nsprintf(buildtag, "srcversion: %-24s", THIS_MODULE->srcversion);\r\n#else\r\nbuildtag[0] = 'b';\r\n#endif\r\n}\r\nreturn buildtag;\r\n}
