int mlx4_en_setup_tc(struct net_device *dev, u8 up)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nint i;\r\nunsigned int offset = 0;\r\nif (up && up != MLX4_EN_NUM_UP)\r\nreturn -EINVAL;\r\nnetdev_set_num_tc(dev, up);\r\nfor (i = 0; i < up; i++) {\r\nnetdev_set_tc_queue(dev, i, priv->num_tx_rings_p_up, offset);\r\noffset += priv->num_tx_rings_p_up;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mlx4_en_low_latency_recv(struct napi_struct *napi)\r\n{\r\nstruct mlx4_en_cq *cq = container_of(napi, struct mlx4_en_cq, napi);\r\nstruct net_device *dev = cq->dev;\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_rx_ring *rx_ring = &priv->rx_ring[cq->ring];\r\nint done;\r\nif (!priv->port_up)\r\nreturn LL_FLUSH_FAILED;\r\nif (!mlx4_en_cq_lock_poll(cq))\r\nreturn LL_FLUSH_BUSY;\r\ndone = mlx4_en_process_rx_cq(dev, cq, 4);\r\nif (likely(done))\r\nrx_ring->cleaned += done;\r\nelse\r\nrx_ring->misses++;\r\nmlx4_en_cq_unlock_poll(cq);\r\nreturn done;\r\n}\r\nstatic void mlx4_en_filter_work(struct work_struct *work)\r\n{\r\nstruct mlx4_en_filter *filter = container_of(work,\r\nstruct mlx4_en_filter,\r\nwork);\r\nstruct mlx4_en_priv *priv = filter->priv;\r\nstruct mlx4_spec_list spec_tcp = {\r\n.id = MLX4_NET_TRANS_RULE_ID_TCP,\r\n{\r\n.tcp_udp = {\r\n.dst_port = filter->dst_port,\r\n.dst_port_msk = (__force __be16)-1,\r\n.src_port = filter->src_port,\r\n.src_port_msk = (__force __be16)-1,\r\n},\r\n},\r\n};\r\nstruct mlx4_spec_list spec_ip = {\r\n.id = MLX4_NET_TRANS_RULE_ID_IPV4,\r\n{\r\n.ipv4 = {\r\n.dst_ip = filter->dst_ip,\r\n.dst_ip_msk = (__force __be32)-1,\r\n.src_ip = filter->src_ip,\r\n.src_ip_msk = (__force __be32)-1,\r\n},\r\n},\r\n};\r\nstruct mlx4_spec_list spec_eth = {\r\n.id = MLX4_NET_TRANS_RULE_ID_ETH,\r\n};\r\nstruct mlx4_net_trans_rule rule = {\r\n.list = LIST_HEAD_INIT(rule.list),\r\n.queue_mode = MLX4_NET_TRANS_Q_LIFO,\r\n.exclusive = 1,\r\n.allow_loopback = 1,\r\n.promisc_mode = MLX4_FS_REGULAR,\r\n.port = priv->port,\r\n.priority = MLX4_DOMAIN_RFS,\r\n};\r\nint rc;\r\n__be64 mac_mask = cpu_to_be64(MLX4_MAC_MASK << 16);\r\nlist_add_tail(&spec_eth.list, &rule.list);\r\nlist_add_tail(&spec_ip.list, &rule.list);\r\nlist_add_tail(&spec_tcp.list, &rule.list);\r\nrule.qpn = priv->rss_map.qps[filter->rxq_index].qpn;\r\nmemcpy(spec_eth.eth.dst_mac, priv->dev->dev_addr, ETH_ALEN);\r\nmemcpy(spec_eth.eth.dst_mac_msk, &mac_mask, ETH_ALEN);\r\nfilter->activated = 0;\r\nif (filter->reg_id) {\r\nrc = mlx4_flow_detach(priv->mdev->dev, filter->reg_id);\r\nif (rc && rc != -ENOENT)\r\nen_err(priv, "Error detaching flow. rc = %d\n", rc);\r\n}\r\nrc = mlx4_flow_attach(priv->mdev->dev, &rule, &filter->reg_id);\r\nif (rc)\r\nen_err(priv, "Error attaching flow. err = %d\n", rc);\r\nmlx4_en_filter_rfs_expire(priv);\r\nfilter->activated = 1;\r\n}\r\nstatic inline struct hlist_head *\r\nfilter_hash_bucket(struct mlx4_en_priv *priv, __be32 src_ip, __be32 dst_ip,\r\n__be16 src_port, __be16 dst_port)\r\n{\r\nunsigned long l;\r\nint bucket_idx;\r\nl = (__force unsigned long)src_port |\r\n((__force unsigned long)dst_port << 2);\r\nl ^= (__force unsigned long)(src_ip ^ dst_ip);\r\nbucket_idx = hash_long(l, MLX4_EN_FILTER_HASH_SHIFT);\r\nreturn &priv->filter_hash[bucket_idx];\r\n}\r\nstatic struct mlx4_en_filter *\r\nmlx4_en_filter_alloc(struct mlx4_en_priv *priv, int rxq_index, __be32 src_ip,\r\n__be32 dst_ip, __be16 src_port, __be16 dst_port,\r\nu32 flow_id)\r\n{\r\nstruct mlx4_en_filter *filter = NULL;\r\nfilter = kzalloc(sizeof(struct mlx4_en_filter), GFP_ATOMIC);\r\nif (!filter)\r\nreturn NULL;\r\nfilter->priv = priv;\r\nfilter->rxq_index = rxq_index;\r\nINIT_WORK(&filter->work, mlx4_en_filter_work);\r\nfilter->src_ip = src_ip;\r\nfilter->dst_ip = dst_ip;\r\nfilter->src_port = src_port;\r\nfilter->dst_port = dst_port;\r\nfilter->flow_id = flow_id;\r\nfilter->id = priv->last_filter_id++ % RPS_NO_FILTER;\r\nlist_add_tail(&filter->next, &priv->filters);\r\nhlist_add_head(&filter->filter_chain,\r\nfilter_hash_bucket(priv, src_ip, dst_ip, src_port,\r\ndst_port));\r\nreturn filter;\r\n}\r\nstatic void mlx4_en_filter_free(struct mlx4_en_filter *filter)\r\n{\r\nstruct mlx4_en_priv *priv = filter->priv;\r\nint rc;\r\nlist_del(&filter->next);\r\nrc = mlx4_flow_detach(priv->mdev->dev, filter->reg_id);\r\nif (rc && rc != -ENOENT)\r\nen_err(priv, "Error detaching flow. rc = %d\n", rc);\r\nkfree(filter);\r\n}\r\nstatic inline struct mlx4_en_filter *\r\nmlx4_en_filter_find(struct mlx4_en_priv *priv, __be32 src_ip, __be32 dst_ip,\r\n__be16 src_port, __be16 dst_port)\r\n{\r\nstruct mlx4_en_filter *filter;\r\nstruct mlx4_en_filter *ret = NULL;\r\nhlist_for_each_entry(filter,\r\nfilter_hash_bucket(priv, src_ip, dst_ip,\r\nsrc_port, dst_port),\r\nfilter_chain) {\r\nif (filter->src_ip == src_ip &&\r\nfilter->dst_ip == dst_ip &&\r\nfilter->src_port == src_port &&\r\nfilter->dst_port == dst_port) {\r\nret = filter;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nmlx4_en_filter_rfs(struct net_device *net_dev, const struct sk_buff *skb,\r\nu16 rxq_index, u32 flow_id)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(net_dev);\r\nstruct mlx4_en_filter *filter;\r\nconst struct iphdr *ip;\r\nconst __be16 *ports;\r\n__be32 src_ip;\r\n__be32 dst_ip;\r\n__be16 src_port;\r\n__be16 dst_port;\r\nint nhoff = skb_network_offset(skb);\r\nint ret = 0;\r\nif (skb->protocol != htons(ETH_P_IP))\r\nreturn -EPROTONOSUPPORT;\r\nip = (const struct iphdr *)(skb->data + nhoff);\r\nif (ip_is_fragment(ip))\r\nreturn -EPROTONOSUPPORT;\r\nports = (const __be16 *)(skb->data + nhoff + 4 * ip->ihl);\r\nsrc_ip = ip->saddr;\r\ndst_ip = ip->daddr;\r\nsrc_port = ports[0];\r\ndst_port = ports[1];\r\nif (ip->protocol != IPPROTO_TCP)\r\nreturn -EPROTONOSUPPORT;\r\nspin_lock_bh(&priv->filters_lock);\r\nfilter = mlx4_en_filter_find(priv, src_ip, dst_ip, src_port, dst_port);\r\nif (filter) {\r\nif (filter->rxq_index == rxq_index)\r\ngoto out;\r\nfilter->rxq_index = rxq_index;\r\n} else {\r\nfilter = mlx4_en_filter_alloc(priv, rxq_index,\r\nsrc_ip, dst_ip,\r\nsrc_port, dst_port, flow_id);\r\nif (!filter) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\n}\r\nqueue_work(priv->mdev->workqueue, &filter->work);\r\nout:\r\nret = filter->id;\r\nerr:\r\nspin_unlock_bh(&priv->filters_lock);\r\nreturn ret;\r\n}\r\nvoid mlx4_en_cleanup_filters(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *rx_ring)\r\n{\r\nstruct mlx4_en_filter *filter, *tmp;\r\nLIST_HEAD(del_list);\r\nspin_lock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &priv->filters, next) {\r\nlist_move(&filter->next, &del_list);\r\nhlist_del(&filter->filter_chain);\r\n}\r\nspin_unlock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &del_list, next) {\r\ncancel_work_sync(&filter->work);\r\nmlx4_en_filter_free(filter);\r\n}\r\n}\r\nstatic void mlx4_en_filter_rfs_expire(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_filter *filter = NULL, *tmp, *last_filter = NULL;\r\nLIST_HEAD(del_list);\r\nint i = 0;\r\nspin_lock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &priv->filters, next) {\r\nif (i > MLX4_EN_FILTER_EXPIRY_QUOTA)\r\nbreak;\r\nif (filter->activated &&\r\n!work_pending(&filter->work) &&\r\nrps_may_expire_flow(priv->dev,\r\nfilter->rxq_index, filter->flow_id,\r\nfilter->id)) {\r\nlist_move(&filter->next, &del_list);\r\nhlist_del(&filter->filter_chain);\r\n} else\r\nlast_filter = filter;\r\ni++;\r\n}\r\nif (last_filter && (&last_filter->next != priv->filters.next))\r\nlist_move(&priv->filters, &last_filter->next);\r\nspin_unlock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &del_list, next)\r\nmlx4_en_filter_free(filter);\r\n}\r\nstatic int mlx4_en_vlan_rx_add_vid(struct net_device *dev,\r\n__be16 proto, u16 vid)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err;\r\nint idx;\r\nen_dbg(HW, priv, "adding VLAN:%d\n", vid);\r\nset_bit(vid, priv->active_vlans);\r\nmutex_lock(&mdev->state_lock);\r\nif (mdev->device_up && priv->port_up) {\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed configuring VLAN filter\n");\r\n}\r\nif (mlx4_register_vlan(mdev->dev, priv->port, vid, &idx))\r\nen_dbg(HW, priv, "failed adding vlan %d\n", vid);\r\nmutex_unlock(&mdev->state_lock);\r\nreturn 0;\r\n}\r\nstatic int mlx4_en_vlan_rx_kill_vid(struct net_device *dev,\r\n__be16 proto, u16 vid)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err;\r\nint idx;\r\nen_dbg(HW, priv, "Killing VID:%d\n", vid);\r\nclear_bit(vid, priv->active_vlans);\r\nmutex_lock(&mdev->state_lock);\r\nif (!mlx4_find_cached_vlan(mdev->dev, priv->port, vid, &idx))\r\nmlx4_unregister_vlan(mdev->dev, priv->port, idx);\r\nelse\r\nen_dbg(HW, priv, "could not find vid %d in cache\n", vid);\r\nif (mdev->device_up && priv->port_up) {\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed configuring VLAN filter\n");\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\nreturn 0;\r\n}\r\nstatic void mlx4_en_u64_to_mac(unsigned char dst_mac[ETH_ALEN + 2], u64 src_mac)\r\n{\r\nint i;\r\nfor (i = ETH_ALEN - 1; i >= 0; --i) {\r\ndst_mac[i] = src_mac & 0xff;\r\nsrc_mac >>= 8;\r\n}\r\nmemset(&dst_mac[ETH_ALEN], 0, 2);\r\n}\r\nstatic int mlx4_en_uc_steer_add(struct mlx4_en_priv *priv,\r\nunsigned char *mac, int *qpn, u64 *reg_id)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_dev *dev = mdev->dev;\r\nint err;\r\nswitch (dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_B0: {\r\nstruct mlx4_qp qp;\r\nu8 gid[16] = {0};\r\nqp.qpn = *qpn;\r\nmemcpy(&gid[10], mac, ETH_ALEN);\r\ngid[5] = priv->port;\r\nerr = mlx4_unicast_attach(dev, &qp, gid, 0, MLX4_PROT_ETH);\r\nbreak;\r\n}\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED: {\r\nstruct mlx4_spec_list spec_eth = { {NULL} };\r\n__be64 mac_mask = cpu_to_be64(MLX4_MAC_MASK << 16);\r\nstruct mlx4_net_trans_rule rule = {\r\n.queue_mode = MLX4_NET_TRANS_Q_FIFO,\r\n.exclusive = 0,\r\n.allow_loopback = 1,\r\n.promisc_mode = MLX4_FS_REGULAR,\r\n.priority = MLX4_DOMAIN_NIC,\r\n};\r\nrule.port = priv->port;\r\nrule.qpn = *qpn;\r\nINIT_LIST_HEAD(&rule.list);\r\nspec_eth.id = MLX4_NET_TRANS_RULE_ID_ETH;\r\nmemcpy(spec_eth.eth.dst_mac, mac, ETH_ALEN);\r\nmemcpy(spec_eth.eth.dst_mac_msk, &mac_mask, ETH_ALEN);\r\nlist_add_tail(&spec_eth.list, &rule.list);\r\nerr = mlx4_flow_attach(dev, &rule, reg_id);\r\nbreak;\r\n}\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (err)\r\nen_warn(priv, "Failed Attaching Unicast\n");\r\nreturn err;\r\n}\r\nstatic void mlx4_en_uc_steer_release(struct mlx4_en_priv *priv,\r\nunsigned char *mac, int qpn, u64 reg_id)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_dev *dev = mdev->dev;\r\nswitch (dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_B0: {\r\nstruct mlx4_qp qp;\r\nu8 gid[16] = {0};\r\nqp.qpn = qpn;\r\nmemcpy(&gid[10], mac, ETH_ALEN);\r\ngid[5] = priv->port;\r\nmlx4_unicast_detach(dev, &qp, gid, MLX4_PROT_ETH);\r\nbreak;\r\n}\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED: {\r\nmlx4_flow_detach(dev, reg_id);\r\nbreak;\r\n}\r\ndefault:\r\nen_err(priv, "Invalid steering mode.\n");\r\n}\r\n}\r\nstatic int mlx4_en_get_qp(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_dev *dev = mdev->dev;\r\nstruct mlx4_mac_entry *entry;\r\nint index = 0;\r\nint err = 0;\r\nu64 reg_id;\r\nint *qpn = &priv->base_qpn;\r\nu64 mac = mlx4_en_mac_to_u64(priv->dev->dev_addr);\r\nen_dbg(DRV, priv, "Registering MAC: %pM for adding\n",\r\npriv->dev->dev_addr);\r\nindex = mlx4_register_mac(dev, priv->port, mac);\r\nif (index < 0) {\r\nerr = index;\r\nen_err(priv, "Failed adding MAC: %pM\n",\r\npriv->dev->dev_addr);\r\nreturn err;\r\n}\r\nif (dev->caps.steering_mode == MLX4_STEERING_MODE_A0) {\r\nint base_qpn = mlx4_get_base_qpn(dev, priv->port);\r\n*qpn = base_qpn + index;\r\nreturn 0;\r\n}\r\nerr = mlx4_qp_reserve_range(dev, 1, 1, qpn);\r\nen_dbg(DRV, priv, "Reserved qp %d\n", *qpn);\r\nif (err) {\r\nen_err(priv, "Failed to reserve qp for mac registration\n");\r\ngoto qp_err;\r\n}\r\nerr = mlx4_en_uc_steer_add(priv, priv->dev->dev_addr, qpn, &reg_id);\r\nif (err)\r\ngoto steer_err;\r\nentry = kmalloc(sizeof(*entry), GFP_KERNEL);\r\nif (!entry) {\r\nerr = -ENOMEM;\r\ngoto alloc_err;\r\n}\r\nmemcpy(entry->mac, priv->dev->dev_addr, sizeof(entry->mac));\r\nentry->reg_id = reg_id;\r\nhlist_add_head_rcu(&entry->hlist,\r\n&priv->mac_hash[entry->mac[MLX4_EN_MAC_HASH_IDX]]);\r\nreturn 0;\r\nalloc_err:\r\nmlx4_en_uc_steer_release(priv, priv->dev->dev_addr, *qpn, reg_id);\r\nsteer_err:\r\nmlx4_qp_release_range(dev, *qpn, 1);\r\nqp_err:\r\nmlx4_unregister_mac(dev, priv->port, mac);\r\nreturn err;\r\n}\r\nstatic void mlx4_en_put_qp(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_dev *dev = mdev->dev;\r\nint qpn = priv->base_qpn;\r\nu64 mac;\r\nif (dev->caps.steering_mode == MLX4_STEERING_MODE_A0) {\r\nmac = mlx4_en_mac_to_u64(priv->dev->dev_addr);\r\nen_dbg(DRV, priv, "Registering MAC: %pM for deleting\n",\r\npriv->dev->dev_addr);\r\nmlx4_unregister_mac(dev, priv->port, mac);\r\n} else {\r\nstruct mlx4_mac_entry *entry;\r\nstruct hlist_node *tmp;\r\nstruct hlist_head *bucket;\r\nunsigned int i;\r\nfor (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i) {\r\nbucket = &priv->mac_hash[i];\r\nhlist_for_each_entry_safe(entry, tmp, bucket, hlist) {\r\nmac = mlx4_en_mac_to_u64(entry->mac);\r\nen_dbg(DRV, priv, "Registering MAC: %pM for deleting\n",\r\nentry->mac);\r\nmlx4_en_uc_steer_release(priv, entry->mac,\r\nqpn, entry->reg_id);\r\nmlx4_unregister_mac(dev, priv->port, mac);\r\nhlist_del_rcu(&entry->hlist);\r\nkfree_rcu(entry, rcu);\r\n}\r\n}\r\nen_dbg(DRV, priv, "Releasing qp: port %d, qpn %d\n",\r\npriv->port, qpn);\r\nmlx4_qp_release_range(dev, qpn, 1);\r\npriv->flags &= ~MLX4_EN_FLAG_FORCE_PROMISC;\r\n}\r\n}\r\nstatic int mlx4_en_replace_mac(struct mlx4_en_priv *priv, int qpn,\r\nunsigned char *new_mac, unsigned char *prev_mac)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_dev *dev = mdev->dev;\r\nint err = 0;\r\nu64 new_mac_u64 = mlx4_en_mac_to_u64(new_mac);\r\nif (dev->caps.steering_mode != MLX4_STEERING_MODE_A0) {\r\nstruct hlist_head *bucket;\r\nunsigned int mac_hash;\r\nstruct mlx4_mac_entry *entry;\r\nstruct hlist_node *tmp;\r\nu64 prev_mac_u64 = mlx4_en_mac_to_u64(prev_mac);\r\nbucket = &priv->mac_hash[prev_mac[MLX4_EN_MAC_HASH_IDX]];\r\nhlist_for_each_entry_safe(entry, tmp, bucket, hlist) {\r\nif (ether_addr_equal_64bits(entry->mac, prev_mac)) {\r\nmlx4_en_uc_steer_release(priv, entry->mac,\r\nqpn, entry->reg_id);\r\nmlx4_unregister_mac(dev, priv->port,\r\nprev_mac_u64);\r\nhlist_del_rcu(&entry->hlist);\r\nsynchronize_rcu();\r\nmemcpy(entry->mac, new_mac, ETH_ALEN);\r\nentry->reg_id = 0;\r\nmac_hash = new_mac[MLX4_EN_MAC_HASH_IDX];\r\nhlist_add_head_rcu(&entry->hlist,\r\n&priv->mac_hash[mac_hash]);\r\nmlx4_register_mac(dev, priv->port, new_mac_u64);\r\nerr = mlx4_en_uc_steer_add(priv, new_mac,\r\n&qpn,\r\n&entry->reg_id);\r\nreturn err;\r\n}\r\n}\r\nreturn -EINVAL;\r\n}\r\nreturn __mlx4_replace_mac(dev, priv->port, qpn, new_mac_u64);\r\n}\r\nu64 mlx4_en_mac_to_u64(u8 *addr)\r\n{\r\nu64 mac = 0;\r\nint i;\r\nfor (i = 0; i < ETH_ALEN; i++) {\r\nmac <<= 8;\r\nmac |= addr[i];\r\n}\r\nreturn mac;\r\n}\r\nstatic int mlx4_en_do_set_mac(struct mlx4_en_priv *priv)\r\n{\r\nint err = 0;\r\nif (priv->port_up) {\r\nerr = mlx4_en_replace_mac(priv, priv->base_qpn,\r\npriv->dev->dev_addr, priv->prev_mac);\r\nif (err)\r\nen_err(priv, "Failed changing HW MAC address\n");\r\nmemcpy(priv->prev_mac, priv->dev->dev_addr,\r\nsizeof(priv->prev_mac));\r\n} else\r\nen_dbg(HW, priv, "Port is down while registering mac, exiting...\n");\r\nreturn err;\r\n}\r\nstatic int mlx4_en_set_mac(struct net_device *dev, void *addr)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct sockaddr *saddr = addr;\r\nint err;\r\nif (!is_valid_ether_addr(saddr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, saddr->sa_data, ETH_ALEN);\r\nmutex_lock(&mdev->state_lock);\r\nerr = mlx4_en_do_set_mac(priv);\r\nmutex_unlock(&mdev->state_lock);\r\nreturn err;\r\n}\r\nstatic void mlx4_en_clear_list(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_mc_list *tmp, *mc_to_del;\r\nlist_for_each_entry_safe(mc_to_del, tmp, &priv->mc_list, list) {\r\nlist_del(&mc_to_del->list);\r\nkfree(mc_to_del);\r\n}\r\n}\r\nstatic void mlx4_en_cache_mclist(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct netdev_hw_addr *ha;\r\nstruct mlx4_en_mc_list *tmp;\r\nmlx4_en_clear_list(dev);\r\nnetdev_for_each_mc_addr(ha, dev) {\r\ntmp = kzalloc(sizeof(struct mlx4_en_mc_list), GFP_ATOMIC);\r\nif (!tmp) {\r\nmlx4_en_clear_list(dev);\r\nreturn;\r\n}\r\nmemcpy(tmp->addr, ha->addr, ETH_ALEN);\r\nlist_add_tail(&tmp->list, &priv->mc_list);\r\n}\r\n}\r\nstatic void update_mclist_flags(struct mlx4_en_priv *priv,\r\nstruct list_head *dst,\r\nstruct list_head *src)\r\n{\r\nstruct mlx4_en_mc_list *dst_tmp, *src_tmp, *new_mc;\r\nbool found;\r\nlist_for_each_entry(dst_tmp, dst, list) {\r\nfound = false;\r\nlist_for_each_entry(src_tmp, src, list) {\r\nif (!memcmp(dst_tmp->addr, src_tmp->addr, ETH_ALEN)) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found)\r\ndst_tmp->action = MCLIST_REM;\r\n}\r\nlist_for_each_entry(src_tmp, src, list) {\r\nfound = false;\r\nlist_for_each_entry(dst_tmp, dst, list) {\r\nif (!memcmp(dst_tmp->addr, src_tmp->addr, ETH_ALEN)) {\r\ndst_tmp->action = MCLIST_NONE;\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nnew_mc = kmemdup(src_tmp,\r\nsizeof(struct mlx4_en_mc_list),\r\nGFP_KERNEL);\r\nif (!new_mc)\r\nreturn;\r\nnew_mc->action = MCLIST_ADD;\r\nlist_add_tail(&new_mc->list, dst);\r\n}\r\n}\r\n}\r\nstatic void mlx4_en_set_rx_mode(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nif (!priv->port_up)\r\nreturn;\r\nqueue_work(priv->mdev->workqueue, &priv->rx_mode_task);\r\n}\r\nstatic void mlx4_en_set_promisc_mode(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_dev *mdev)\r\n{\r\nint err = 0;\r\nif (!(priv->flags & MLX4_EN_FLAG_PROMISC)) {\r\nif (netif_msg_rx_status(priv))\r\nen_warn(priv, "Entering promiscuous mode\n");\r\npriv->flags |= MLX4_EN_FLAG_PROMISC;\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_add(mdev->dev,\r\npriv->port,\r\npriv->base_qpn,\r\nMLX4_FS_ALL_DEFAULT);\r\nif (err)\r\nen_err(priv, "Failed enabling promiscuous mode\n");\r\npriv->flags |= MLX4_EN_FLAG_MC_PROMISC;\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_unicast_promisc_add(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed enabling unicast promiscuous mode\n");\r\nif (!(priv->flags & MLX4_EN_FLAG_MC_PROMISC)) {\r\nerr = mlx4_multicast_promisc_add(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed enabling multicast promiscuous mode\n");\r\npriv->flags |= MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nerr = mlx4_SET_PORT_qpn_calc(mdev->dev,\r\npriv->port,\r\npriv->base_qpn,\r\n1);\r\nif (err)\r\nen_err(priv, "Failed enabling promiscuous mode\n");\r\nbreak;\r\n}\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_DISABLE);\r\nif (err)\r\nen_err(priv, "Failed disabling multicast filter\n");\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed disabling VLAN filter\n");\r\n}\r\n}\r\nstatic void mlx4_en_clear_promisc_mode(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_dev *mdev)\r\n{\r\nint err = 0;\r\nif (netif_msg_rx_status(priv))\r\nen_warn(priv, "Leaving promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_PROMISC;\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_ALL_DEFAULT);\r\nif (err)\r\nen_err(priv, "Failed disabling promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_unicast_promisc_remove(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed disabling unicast promiscuous mode\n");\r\nif (priv->flags & MLX4_EN_FLAG_MC_PROMISC) {\r\nerr = mlx4_multicast_promisc_remove(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed disabling multicast promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nerr = mlx4_SET_PORT_qpn_calc(mdev->dev,\r\npriv->port,\r\npriv->base_qpn, 0);\r\nif (err)\r\nen_err(priv, "Failed disabling promiscuous mode\n");\r\nbreak;\r\n}\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed enabling VLAN filter\n");\r\n}\r\nstatic void mlx4_en_do_multicast(struct mlx4_en_priv *priv,\r\nstruct net_device *dev,\r\nstruct mlx4_en_dev *mdev)\r\n{\r\nstruct mlx4_en_mc_list *mclist, *tmp;\r\nu64 mcast_addr = 0;\r\nu8 mc_list[16] = {0};\r\nint err = 0;\r\nif (dev->flags & IFF_ALLMULTI) {\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_DISABLE);\r\nif (err)\r\nen_err(priv, "Failed disabling multicast filter\n");\r\nif (!(priv->flags & MLX4_EN_FLAG_MC_PROMISC)) {\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_add(mdev->dev,\r\npriv->port,\r\npriv->base_qpn,\r\nMLX4_FS_MC_DEFAULT);\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_multicast_promisc_add(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nbreak;\r\n}\r\nif (err)\r\nen_err(priv, "Failed entering multicast promisc mode\n");\r\npriv->flags |= MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\n} else {\r\nif (priv->flags & MLX4_EN_FLAG_MC_PROMISC) {\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_MC_DEFAULT);\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_multicast_promisc_remove(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nbreak;\r\n}\r\nif (err)\r\nen_err(priv, "Failed disabling multicast promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_DISABLE);\r\nif (err)\r\nen_err(priv, "Failed disabling multicast filter\n");\r\nmlx4_SET_MCAST_FLTR(mdev->dev, priv->port, ETH_BCAST,\r\n1, MLX4_MCAST_CONFIG);\r\nnetif_addr_lock_bh(dev);\r\nmlx4_en_cache_mclist(dev);\r\nnetif_addr_unlock_bh(dev);\r\nlist_for_each_entry(mclist, &priv->mc_list, list) {\r\nmcast_addr = mlx4_en_mac_to_u64(mclist->addr);\r\nmlx4_SET_MCAST_FLTR(mdev->dev, priv->port,\r\nmcast_addr, 0, MLX4_MCAST_CONFIG);\r\n}\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_ENABLE);\r\nif (err)\r\nen_err(priv, "Failed enabling multicast filter\n");\r\nupdate_mclist_flags(priv, &priv->curr_list, &priv->mc_list);\r\nlist_for_each_entry_safe(mclist, tmp, &priv->curr_list, list) {\r\nif (mclist->action == MCLIST_REM) {\r\nmemcpy(&mc_list[10], mclist->addr, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nerr = mlx4_multicast_detach(mdev->dev,\r\n&priv->rss_map.indir_qp,\r\nmc_list,\r\nMLX4_PROT_ETH,\r\nmclist->reg_id);\r\nif (err)\r\nen_err(priv, "Fail to detach multicast address\n");\r\nlist_del(&mclist->list);\r\nkfree(mclist);\r\n} else if (mclist->action == MCLIST_ADD) {\r\nmemcpy(&mc_list[10], mclist->addr, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nerr = mlx4_multicast_attach(mdev->dev,\r\n&priv->rss_map.indir_qp,\r\nmc_list,\r\npriv->port, 0,\r\nMLX4_PROT_ETH,\r\n&mclist->reg_id);\r\nif (err)\r\nen_err(priv, "Fail to attach multicast address\n");\r\n}\r\n}\r\n}\r\n}\r\nstatic void mlx4_en_do_uc_filter(struct mlx4_en_priv *priv,\r\nstruct net_device *dev,\r\nstruct mlx4_en_dev *mdev)\r\n{\r\nstruct netdev_hw_addr *ha;\r\nstruct mlx4_mac_entry *entry;\r\nstruct hlist_node *tmp;\r\nbool found;\r\nu64 mac;\r\nint err = 0;\r\nstruct hlist_head *bucket;\r\nunsigned int i;\r\nint removed = 0;\r\nu32 prev_flags;\r\nfor (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i) {\r\nbucket = &priv->mac_hash[i];\r\nhlist_for_each_entry_safe(entry, tmp, bucket, hlist) {\r\nfound = false;\r\nnetdev_for_each_uc_addr(ha, dev) {\r\nif (ether_addr_equal_64bits(entry->mac,\r\nha->addr)) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (ether_addr_equal_64bits(entry->mac, dev->dev_addr))\r\nfound = true;\r\nif (!found) {\r\nmac = mlx4_en_mac_to_u64(entry->mac);\r\nmlx4_en_uc_steer_release(priv, entry->mac,\r\npriv->base_qpn,\r\nentry->reg_id);\r\nmlx4_unregister_mac(mdev->dev, priv->port, mac);\r\nhlist_del_rcu(&entry->hlist);\r\nkfree_rcu(entry, rcu);\r\nen_dbg(DRV, priv, "Removed MAC %pM on port:%d\n",\r\nentry->mac, priv->port);\r\n++removed;\r\n}\r\n}\r\n}\r\nif ((priv->flags & MLX4_EN_FLAG_FORCE_PROMISC) && 0 == removed)\r\nreturn;\r\nprev_flags = priv->flags;\r\npriv->flags &= ~MLX4_EN_FLAG_FORCE_PROMISC;\r\nnetdev_for_each_uc_addr(ha, dev) {\r\nfound = false;\r\nbucket = &priv->mac_hash[ha->addr[MLX4_EN_MAC_HASH_IDX]];\r\nhlist_for_each_entry(entry, bucket, hlist) {\r\nif (ether_addr_equal_64bits(entry->mac, ha->addr)) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nentry = kmalloc(sizeof(*entry), GFP_KERNEL);\r\nif (!entry) {\r\nen_err(priv, "Failed adding MAC %pM on port:%d (out of memory)\n",\r\nha->addr, priv->port);\r\npriv->flags |= MLX4_EN_FLAG_FORCE_PROMISC;\r\nbreak;\r\n}\r\nmac = mlx4_en_mac_to_u64(ha->addr);\r\nmemcpy(entry->mac, ha->addr, ETH_ALEN);\r\nerr = mlx4_register_mac(mdev->dev, priv->port, mac);\r\nif (err < 0) {\r\nen_err(priv, "Failed registering MAC %pM on port %d: %d\n",\r\nha->addr, priv->port, err);\r\nkfree(entry);\r\npriv->flags |= MLX4_EN_FLAG_FORCE_PROMISC;\r\nbreak;\r\n}\r\nerr = mlx4_en_uc_steer_add(priv, ha->addr,\r\n&priv->base_qpn,\r\n&entry->reg_id);\r\nif (err) {\r\nen_err(priv, "Failed adding MAC %pM on port %d: %d\n",\r\nha->addr, priv->port, err);\r\nmlx4_unregister_mac(mdev->dev, priv->port, mac);\r\nkfree(entry);\r\npriv->flags |= MLX4_EN_FLAG_FORCE_PROMISC;\r\nbreak;\r\n} else {\r\nunsigned int mac_hash;\r\nen_dbg(DRV, priv, "Added MAC %pM on port:%d\n",\r\nha->addr, priv->port);\r\nmac_hash = ha->addr[MLX4_EN_MAC_HASH_IDX];\r\nbucket = &priv->mac_hash[mac_hash];\r\nhlist_add_head_rcu(&entry->hlist, bucket);\r\n}\r\n}\r\n}\r\nif (priv->flags & MLX4_EN_FLAG_FORCE_PROMISC) {\r\nen_warn(priv, "Forcing promiscuous mode on port:%d\n",\r\npriv->port);\r\n} else if (prev_flags & MLX4_EN_FLAG_FORCE_PROMISC) {\r\nen_warn(priv, "Stop forcing promiscuous mode on port:%d\n",\r\npriv->port);\r\n}\r\n}\r\nstatic void mlx4_en_do_set_rx_mode(struct work_struct *work)\r\n{\r\nstruct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,\r\nrx_mode_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct net_device *dev = priv->dev;\r\nmutex_lock(&mdev->state_lock);\r\nif (!mdev->device_up) {\r\nen_dbg(HW, priv, "Card is not up, ignoring rx mode change.\n");\r\ngoto out;\r\n}\r\nif (!priv->port_up) {\r\nen_dbg(HW, priv, "Port is down, ignoring rx mode change.\n");\r\ngoto out;\r\n}\r\nif (!netif_carrier_ok(dev)) {\r\nif (!mlx4_en_QUERY_PORT(mdev, priv->port)) {\r\nif (priv->port_state.link_state) {\r\npriv->last_link_state = MLX4_DEV_EVENT_PORT_UP;\r\nnetif_carrier_on(dev);\r\nen_dbg(LINK, priv, "Link Up\n");\r\n}\r\n}\r\n}\r\nif (dev->priv_flags & IFF_UNICAST_FLT)\r\nmlx4_en_do_uc_filter(priv, dev, mdev);\r\nif ((dev->flags & IFF_PROMISC) ||\r\n(priv->flags & MLX4_EN_FLAG_FORCE_PROMISC)) {\r\nmlx4_en_set_promisc_mode(priv, mdev);\r\ngoto out;\r\n}\r\nif (priv->flags & MLX4_EN_FLAG_PROMISC)\r\nmlx4_en_clear_promisc_mode(priv, mdev);\r\nmlx4_en_do_multicast(priv, dev, mdev);\r\nout:\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_netpoll(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_cq *cq;\r\nunsigned long flags;\r\nint i;\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\ncq = &priv->rx_cq[i];\r\nspin_lock_irqsave(&cq->lock, flags);\r\nnapi_synchronize(&cq->napi);\r\nmlx4_en_process_rx_cq(dev, cq, 0);\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\n}\r\n}\r\nstatic void mlx4_en_tx_timeout(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint i;\r\nif (netif_msg_timer(priv))\r\nen_warn(priv, "Tx timeout called on port:%d\n", priv->port);\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\nif (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, i)))\r\ncontinue;\r\nen_warn(priv, "TX timeout on queue: %d, QP: 0x%x, CQ: 0x%x, Cons: 0x%x, Prod: 0x%x\n",\r\ni, priv->tx_ring[i].qpn, priv->tx_ring[i].cqn,\r\npriv->tx_ring[i].cons, priv->tx_ring[i].prod);\r\n}\r\npriv->port_stats.tx_timeout++;\r\nen_dbg(DRV, priv, "Scheduling watchdog\n");\r\nqueue_work(mdev->workqueue, &priv->watchdog_task);\r\n}\r\nstatic struct net_device_stats *mlx4_en_get_stats(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nspin_lock_bh(&priv->stats_lock);\r\nmemcpy(&priv->ret_stats, &priv->stats, sizeof(priv->stats));\r\nspin_unlock_bh(&priv->stats_lock);\r\nreturn &priv->ret_stats;\r\n}\r\nstatic void mlx4_en_set_default_moderation(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_cq *cq;\r\nint i;\r\npriv->rx_frames = MLX4_EN_RX_COAL_TARGET;\r\npriv->rx_usecs = MLX4_EN_RX_COAL_TIME;\r\npriv->tx_frames = MLX4_EN_TX_COAL_PKTS;\r\npriv->tx_usecs = MLX4_EN_TX_COAL_TIME;\r\nen_dbg(INTR, priv, "Default coalesing params for mtu:%d - rx_frames:%d rx_usecs:%d\n",\r\npriv->dev->mtu, priv->rx_frames, priv->rx_usecs);\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\ncq = &priv->rx_cq[i];\r\ncq->moder_cnt = priv->rx_frames;\r\ncq->moder_time = priv->rx_usecs;\r\npriv->last_moder_time[i] = MLX4_EN_AUTO_CONF;\r\npriv->last_moder_packets[i] = 0;\r\npriv->last_moder_bytes[i] = 0;\r\n}\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\ncq = &priv->tx_cq[i];\r\ncq->moder_cnt = priv->tx_frames;\r\ncq->moder_time = priv->tx_usecs;\r\n}\r\npriv->pkt_rate_low = MLX4_EN_RX_RATE_LOW;\r\npriv->rx_usecs_low = MLX4_EN_RX_COAL_TIME_LOW;\r\npriv->pkt_rate_high = MLX4_EN_RX_RATE_HIGH;\r\npriv->rx_usecs_high = MLX4_EN_RX_COAL_TIME_HIGH;\r\npriv->sample_interval = MLX4_EN_SAMPLE_INTERVAL;\r\npriv->adaptive_rx_coal = 1;\r\npriv->last_moder_jiffies = 0;\r\npriv->last_moder_tx_packets = 0;\r\n}\r\nstatic void mlx4_en_auto_moderation(struct mlx4_en_priv *priv)\r\n{\r\nunsigned long period = (unsigned long) (jiffies - priv->last_moder_jiffies);\r\nstruct mlx4_en_cq *cq;\r\nunsigned long packets;\r\nunsigned long rate;\r\nunsigned long avg_pkt_size;\r\nunsigned long rx_packets;\r\nunsigned long rx_bytes;\r\nunsigned long rx_pkt_diff;\r\nint moder_time;\r\nint ring, err;\r\nif (!priv->adaptive_rx_coal || period < priv->sample_interval * HZ)\r\nreturn;\r\nfor (ring = 0; ring < priv->rx_ring_num; ring++) {\r\nspin_lock_bh(&priv->stats_lock);\r\nrx_packets = priv->rx_ring[ring].packets;\r\nrx_bytes = priv->rx_ring[ring].bytes;\r\nspin_unlock_bh(&priv->stats_lock);\r\nrx_pkt_diff = ((unsigned long) (rx_packets -\r\npriv->last_moder_packets[ring]));\r\npackets = rx_pkt_diff;\r\nrate = packets * HZ / period;\r\navg_pkt_size = packets ? ((unsigned long) (rx_bytes -\r\npriv->last_moder_bytes[ring])) / packets : 0;\r\nif (rate > (MLX4_EN_RX_RATE_THRESH / priv->rx_ring_num) &&\r\navg_pkt_size > MLX4_EN_AVG_PKT_SMALL) {\r\nif (rate < priv->pkt_rate_low)\r\nmoder_time = priv->rx_usecs_low;\r\nelse if (rate > priv->pkt_rate_high)\r\nmoder_time = priv->rx_usecs_high;\r\nelse\r\nmoder_time = (rate - priv->pkt_rate_low) *\r\n(priv->rx_usecs_high - priv->rx_usecs_low) /\r\n(priv->pkt_rate_high - priv->pkt_rate_low) +\r\npriv->rx_usecs_low;\r\n} else {\r\nmoder_time = priv->rx_usecs_low;\r\n}\r\nif (moder_time != priv->last_moder_time[ring]) {\r\npriv->last_moder_time[ring] = moder_time;\r\ncq = &priv->rx_cq[ring];\r\ncq->moder_time = moder_time;\r\ncq->moder_cnt = priv->rx_frames;\r\nerr = mlx4_en_set_cq_moder(priv, cq);\r\nif (err)\r\nen_err(priv, "Failed modifying moderation for cq:%d\n",\r\nring);\r\n}\r\npriv->last_moder_packets[ring] = rx_packets;\r\npriv->last_moder_bytes[ring] = rx_bytes;\r\n}\r\npriv->last_moder_jiffies = jiffies;\r\n}\r\nstatic void mlx4_en_do_get_stats(struct work_struct *work)\r\n{\r\nstruct delayed_work *delay = to_delayed_work(work);\r\nstruct mlx4_en_priv *priv = container_of(delay, struct mlx4_en_priv,\r\nstats_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err;\r\nmutex_lock(&mdev->state_lock);\r\nif (mdev->device_up) {\r\nif (priv->port_up) {\r\nerr = mlx4_en_DUMP_ETH_STATS(mdev, priv->port, 0);\r\nif (err)\r\nen_dbg(HW, priv, "Could not update stats\n");\r\nmlx4_en_auto_moderation(priv);\r\n}\r\nqueue_delayed_work(mdev->workqueue, &priv->stats_task, STATS_DELAY);\r\n}\r\nif (mdev->mac_removed[MLX4_MAX_PORTS + 1 - priv->port]) {\r\nmlx4_en_do_set_mac(priv);\r\nmdev->mac_removed[MLX4_MAX_PORTS + 1 - priv->port] = 0;\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_service_task(struct work_struct *work)\r\n{\r\nstruct delayed_work *delay = to_delayed_work(work);\r\nstruct mlx4_en_priv *priv = container_of(delay, struct mlx4_en_priv,\r\nservice_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nmutex_lock(&mdev->state_lock);\r\nif (mdev->device_up) {\r\nif (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_TS)\r\nmlx4_en_ptp_overflow_check(mdev);\r\nqueue_delayed_work(mdev->workqueue, &priv->service_task,\r\nSERVICE_TASK_DELAY);\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_linkstate(struct work_struct *work)\r\n{\r\nstruct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,\r\nlinkstate_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint linkstate = priv->link_state;\r\nmutex_lock(&mdev->state_lock);\r\nif (priv->last_link_state != linkstate) {\r\nif (linkstate == MLX4_DEV_EVENT_PORT_DOWN) {\r\nen_info(priv, "Link Down\n");\r\nnetif_carrier_off(priv->dev);\r\n} else {\r\nen_info(priv, "Link Up\n");\r\nnetif_carrier_on(priv->dev);\r\n}\r\n}\r\npriv->last_link_state = linkstate;\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nint mlx4_en_start_port(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_en_cq *cq;\r\nstruct mlx4_en_tx_ring *tx_ring;\r\nint rx_index = 0;\r\nint tx_index = 0;\r\nint err = 0;\r\nint i;\r\nint j;\r\nu8 mc_list[16] = {0};\r\nif (priv->port_up) {\r\nen_dbg(DRV, priv, "start port called while port already up\n");\r\nreturn 0;\r\n}\r\nINIT_LIST_HEAD(&priv->mc_list);\r\nINIT_LIST_HEAD(&priv->curr_list);\r\nINIT_LIST_HEAD(&priv->ethtool_list);\r\nmemset(&priv->ethtool_rules[0], 0,\r\nsizeof(struct ethtool_flow_id) * MAX_NUM_OF_FS_RULES);\r\ndev->mtu = min(dev->mtu, priv->max_mtu);\r\nmlx4_en_calc_rx_buf(dev);\r\nen_dbg(DRV, priv, "Rx buf size:%d\n", priv->rx_skb_size);\r\nerr = mlx4_en_activate_rx_rings(priv);\r\nif (err) {\r\nen_err(priv, "Failed to activate RX rings\n");\r\nreturn err;\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\ncq = &priv->rx_cq[i];\r\nmlx4_en_cq_init_lock(cq);\r\nerr = mlx4_en_activate_cq(priv, cq, i);\r\nif (err) {\r\nen_err(priv, "Failed activating Rx CQ\n");\r\ngoto cq_err;\r\n}\r\nfor (j = 0; j < cq->size; j++)\r\ncq->buf[j].owner_sr_opcode = MLX4_CQE_OWNER_MASK;\r\nerr = mlx4_en_set_cq_moder(priv, cq);\r\nif (err) {\r\nen_err(priv, "Failed setting cq moderation parameters");\r\nmlx4_en_deactivate_cq(priv, cq);\r\ngoto cq_err;\r\n}\r\nmlx4_en_arm_cq(priv, cq);\r\npriv->rx_ring[i].cqn = cq->mcq.cqn;\r\n++rx_index;\r\n}\r\nen_dbg(DRV, priv, "Getting qp number for port %d\n", priv->port);\r\nerr = mlx4_en_get_qp(priv);\r\nif (err) {\r\nen_err(priv, "Failed getting eth qp\n");\r\ngoto cq_err;\r\n}\r\nmdev->mac_removed[priv->port] = 0;\r\nerr = mlx4_en_config_rss_steer(priv);\r\nif (err) {\r\nen_err(priv, "Failed configuring rss steering\n");\r\ngoto mac_err;\r\n}\r\nerr = mlx4_en_create_drop_qp(priv);\r\nif (err)\r\ngoto rss_err;\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\ncq = &priv->tx_cq[i];\r\nerr = mlx4_en_activate_cq(priv, cq, i);\r\nif (err) {\r\nen_err(priv, "Failed allocating Tx CQ\n");\r\ngoto tx_err;\r\n}\r\nerr = mlx4_en_set_cq_moder(priv, cq);\r\nif (err) {\r\nen_err(priv, "Failed setting cq moderation parameters");\r\nmlx4_en_deactivate_cq(priv, cq);\r\ngoto tx_err;\r\n}\r\nen_dbg(DRV, priv, "Resetting index of collapsed CQ:%d to -1\n", i);\r\ncq->buf->wqe_index = cpu_to_be16(0xffff);\r\ntx_ring = &priv->tx_ring[i];\r\nerr = mlx4_en_activate_tx_ring(priv, tx_ring, cq->mcq.cqn,\r\ni / priv->num_tx_rings_p_up);\r\nif (err) {\r\nen_err(priv, "Failed allocating Tx ring\n");\r\nmlx4_en_deactivate_cq(priv, cq);\r\ngoto tx_err;\r\n}\r\ntx_ring->tx_queue = netdev_get_tx_queue(dev, i);\r\nmlx4_en_arm_cq(priv, cq);\r\nfor (j = 0; j < tx_ring->buf_size; j += STAMP_STRIDE)\r\n*((u32 *) (tx_ring->buf + j)) = 0xffffffff;\r\n++tx_index;\r\n}\r\nerr = mlx4_SET_PORT_general(mdev->dev, priv->port,\r\npriv->rx_skb_size + ETH_FCS_LEN,\r\npriv->prof->tx_pause,\r\npriv->prof->tx_ppp,\r\npriv->prof->rx_pause,\r\npriv->prof->rx_ppp);\r\nif (err) {\r\nen_err(priv, "Failed setting port general configurations for port %d, with error %d\n",\r\npriv->port, err);\r\ngoto tx_err;\r\n}\r\nerr = mlx4_SET_PORT_qpn_calc(mdev->dev, priv->port, priv->base_qpn, 0);\r\nif (err) {\r\nen_err(priv, "Failed setting default qp numbers\n");\r\ngoto tx_err;\r\n}\r\nen_dbg(HW, priv, "Initializing port\n");\r\nerr = mlx4_INIT_PORT(mdev->dev, priv->port);\r\nif (err) {\r\nen_err(priv, "Failed Initializing port\n");\r\ngoto tx_err;\r\n}\r\nmemset(&mc_list[10], 0xff, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nif (mlx4_multicast_attach(mdev->dev, &priv->rss_map.indir_qp, mc_list,\r\npriv->port, 0, MLX4_PROT_ETH,\r\n&priv->broadcast_id))\r\nmlx4_warn(mdev, "Failed Attaching Broadcast\n");\r\npriv->flags &= ~(MLX4_EN_FLAG_PROMISC | MLX4_EN_FLAG_MC_PROMISC);\r\nqueue_work(mdev->workqueue, &priv->rx_mode_task);\r\nmlx4_set_stats_bitmap(mdev->dev, &priv->stats_bitmap);\r\npriv->port_up = true;\r\nnetif_tx_start_all_queues(dev);\r\nnetif_device_attach(dev);\r\nreturn 0;\r\ntx_err:\r\nwhile (tx_index--) {\r\nmlx4_en_deactivate_tx_ring(priv, &priv->tx_ring[tx_index]);\r\nmlx4_en_deactivate_cq(priv, &priv->tx_cq[tx_index]);\r\n}\r\nmlx4_en_destroy_drop_qp(priv);\r\nrss_err:\r\nmlx4_en_release_rss_steer(priv);\r\nmac_err:\r\nmlx4_en_put_qp(priv);\r\ncq_err:\r\nwhile (rx_index--)\r\nmlx4_en_deactivate_cq(priv, &priv->rx_cq[rx_index]);\r\nfor (i = 0; i < priv->rx_ring_num; i++)\r\nmlx4_en_deactivate_rx_ring(priv, &priv->rx_ring[i]);\r\nreturn err;\r\n}\r\nvoid mlx4_en_stop_port(struct net_device *dev, int detach)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_en_mc_list *mclist, *tmp;\r\nstruct ethtool_flow_id *flow, *tmp_flow;\r\nint i;\r\nu8 mc_list[16] = {0};\r\nif (!priv->port_up) {\r\nen_dbg(DRV, priv, "stop port called while port already down\n");\r\nreturn;\r\n}\r\nmlx4_CLOSE_PORT(mdev->dev, priv->port);\r\nnetif_tx_lock_bh(dev);\r\nif (detach)\r\nnetif_device_detach(dev);\r\nnetif_tx_stop_all_queues(dev);\r\nnetif_tx_unlock_bh(dev);\r\nnetif_tx_disable(dev);\r\npriv->port_up = false;\r\nif (mdev->dev->caps.steering_mode ==\r\nMLX4_STEERING_MODE_DEVICE_MANAGED) {\r\npriv->flags &= ~(MLX4_EN_FLAG_PROMISC |\r\nMLX4_EN_FLAG_MC_PROMISC);\r\nmlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_ALL_DEFAULT);\r\nmlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_MC_DEFAULT);\r\n} else if (priv->flags & MLX4_EN_FLAG_PROMISC) {\r\npriv->flags &= ~MLX4_EN_FLAG_PROMISC;\r\nmlx4_unicast_promisc_remove(mdev->dev, priv->base_qpn,\r\npriv->port);\r\nif (priv->flags & MLX4_EN_FLAG_MC_PROMISC) {\r\nmlx4_multicast_promisc_remove(mdev->dev, priv->base_qpn,\r\npriv->port);\r\npriv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\n}\r\nmemset(&mc_list[10], 0xff, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nmlx4_multicast_detach(mdev->dev, &priv->rss_map.indir_qp, mc_list,\r\nMLX4_PROT_ETH, priv->broadcast_id);\r\nlist_for_each_entry(mclist, &priv->curr_list, list) {\r\nmemcpy(&mc_list[10], mclist->addr, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nmlx4_multicast_detach(mdev->dev, &priv->rss_map.indir_qp,\r\nmc_list, MLX4_PROT_ETH, mclist->reg_id);\r\n}\r\nmlx4_en_clear_list(dev);\r\nlist_for_each_entry_safe(mclist, tmp, &priv->curr_list, list) {\r\nlist_del(&mclist->list);\r\nkfree(mclist);\r\n}\r\nmlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0, 1, MLX4_MCAST_CONFIG);\r\nif (mdev->dev->caps.steering_mode ==\r\nMLX4_STEERING_MODE_DEVICE_MANAGED) {\r\nASSERT_RTNL();\r\nlist_for_each_entry_safe(flow, tmp_flow,\r\n&priv->ethtool_list, list) {\r\nmlx4_flow_detach(mdev->dev, flow->id);\r\nlist_del(&flow->list);\r\n}\r\n}\r\nmlx4_en_destroy_drop_qp(priv);\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\nmlx4_en_deactivate_tx_ring(priv, &priv->tx_ring[i]);\r\nmlx4_en_deactivate_cq(priv, &priv->tx_cq[i]);\r\n}\r\nmsleep(10);\r\nfor (i = 0; i < priv->tx_ring_num; i++)\r\nmlx4_en_free_tx_buf(dev, &priv->tx_ring[i]);\r\nmlx4_en_release_rss_steer(priv);\r\nmlx4_en_put_qp(priv);\r\nif (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAGS2_REASSIGN_MAC_EN))\r\nmdev->mac_removed[priv->port] = 1;\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nstruct mlx4_en_cq *cq = &priv->rx_cq[i];\r\nlocal_bh_disable();\r\nwhile (!mlx4_en_cq_lock_napi(cq)) {\r\npr_info("CQ %d locked\n", i);\r\nmdelay(1);\r\n}\r\nlocal_bh_enable();\r\nwhile (test_bit(NAPI_STATE_SCHED, &cq->napi.state))\r\nmsleep(1);\r\nmlx4_en_deactivate_rx_ring(priv, &priv->rx_ring[i]);\r\nmlx4_en_deactivate_cq(priv, cq);\r\n}\r\n}\r\nstatic void mlx4_en_restart(struct work_struct *work)\r\n{\r\nstruct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,\r\nwatchdog_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct net_device *dev = priv->dev;\r\nen_dbg(DRV, priv, "Watchdog task called for port %d\n", priv->port);\r\nmutex_lock(&mdev->state_lock);\r\nif (priv->port_up) {\r\nmlx4_en_stop_port(dev, 1);\r\nif (mlx4_en_start_port(dev))\r\nen_err(priv, "Failed restarting port %d\n", priv->port);\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_clear_stats(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint i;\r\nif (mlx4_en_DUMP_ETH_STATS(mdev, priv->port, 1))\r\nen_dbg(HW, priv, "Failed dumping statistics\n");\r\nmemset(&priv->stats, 0, sizeof(priv->stats));\r\nmemset(&priv->pstats, 0, sizeof(priv->pstats));\r\nmemset(&priv->pkstats, 0, sizeof(priv->pkstats));\r\nmemset(&priv->port_stats, 0, sizeof(priv->port_stats));\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\npriv->tx_ring[i].bytes = 0;\r\npriv->tx_ring[i].packets = 0;\r\npriv->tx_ring[i].tx_csum = 0;\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\npriv->rx_ring[i].bytes = 0;\r\npriv->rx_ring[i].packets = 0;\r\npriv->rx_ring[i].csum_ok = 0;\r\npriv->rx_ring[i].csum_none = 0;\r\n}\r\n}\r\nstatic int mlx4_en_open(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err = 0;\r\nmutex_lock(&mdev->state_lock);\r\nif (!mdev->device_up) {\r\nen_err(priv, "Cannot open - device down/disabled\n");\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\nmlx4_en_clear_stats(dev);\r\nerr = mlx4_en_start_port(dev);\r\nif (err)\r\nen_err(priv, "Failed starting port:%d\n", priv->port);\r\nout:\r\nmutex_unlock(&mdev->state_lock);\r\nreturn err;\r\n}\r\nstatic int mlx4_en_close(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nen_dbg(IFDOWN, priv, "Close port called\n");\r\nmutex_lock(&mdev->state_lock);\r\nmlx4_en_stop_port(dev, 0);\r\nnetif_carrier_off(dev);\r\nmutex_unlock(&mdev->state_lock);\r\nreturn 0;\r\n}\r\nvoid mlx4_en_free_resources(struct mlx4_en_priv *priv)\r\n{\r\nint i;\r\n#ifdef CONFIG_RFS_ACCEL\r\nfree_irq_cpu_rmap(priv->dev->rx_cpu_rmap);\r\npriv->dev->rx_cpu_rmap = NULL;\r\n#endif\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\nif (priv->tx_ring[i].tx_info)\r\nmlx4_en_destroy_tx_ring(priv, &priv->tx_ring[i]);\r\nif (priv->tx_cq[i].buf)\r\nmlx4_en_destroy_cq(priv, &priv->tx_cq[i]);\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nif (priv->rx_ring[i].rx_info)\r\nmlx4_en_destroy_rx_ring(priv, &priv->rx_ring[i],\r\npriv->prof->rx_ring_size, priv->stride);\r\nif (priv->rx_cq[i].buf)\r\nmlx4_en_destroy_cq(priv, &priv->rx_cq[i]);\r\n}\r\nif (priv->base_tx_qpn) {\r\nmlx4_qp_release_range(priv->mdev->dev, priv->base_tx_qpn, priv->tx_ring_num);\r\npriv->base_tx_qpn = 0;\r\n}\r\n}\r\nint mlx4_en_alloc_resources(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_port_profile *prof = priv->prof;\r\nint i;\r\nint err;\r\nerr = mlx4_qp_reserve_range(priv->mdev->dev, priv->tx_ring_num, 256, &priv->base_tx_qpn);\r\nif (err) {\r\nen_err(priv, "failed reserving range for TX rings\n");\r\nreturn err;\r\n}\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\nif (mlx4_en_create_cq(priv, &priv->tx_cq[i],\r\nprof->tx_ring_size, i, TX))\r\ngoto err;\r\nif (mlx4_en_create_tx_ring(priv, &priv->tx_ring[i], priv->base_tx_qpn + i,\r\nprof->tx_ring_size, TXBB_SIZE))\r\ngoto err;\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nif (mlx4_en_create_cq(priv, &priv->rx_cq[i],\r\nprof->rx_ring_size, i, RX))\r\ngoto err;\r\nif (mlx4_en_create_rx_ring(priv, &priv->rx_ring[i],\r\nprof->rx_ring_size, priv->stride))\r\ngoto err;\r\n}\r\n#ifdef CONFIG_RFS_ACCEL\r\nif (priv->mdev->dev->caps.comp_pool) {\r\npriv->dev->rx_cpu_rmap = alloc_irq_cpu_rmap(priv->mdev->dev->caps.comp_pool);\r\nif (!priv->dev->rx_cpu_rmap)\r\ngoto err;\r\n}\r\n#endif\r\nreturn 0;\r\nerr:\r\nen_err(priv, "Failed to allocate NIC resources\n");\r\nreturn -ENOMEM;\r\n}\r\nvoid mlx4_en_destroy_netdev(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nen_dbg(DRV, priv, "Destroying netdev on port:%d\n", priv->port);\r\nif (priv->registered)\r\nunregister_netdev(dev);\r\nif (priv->allocated)\r\nmlx4_free_hwq_res(mdev->dev, &priv->res, MLX4_EN_PAGE_SIZE);\r\ncancel_delayed_work(&priv->stats_task);\r\ncancel_delayed_work(&priv->service_task);\r\nflush_workqueue(mdev->workqueue);\r\nmutex_lock(&mdev->state_lock);\r\nmdev->pndev[priv->port] = NULL;\r\nmutex_unlock(&mdev->state_lock);\r\nmlx4_en_free_resources(priv);\r\nkfree(priv->tx_ring);\r\nkfree(priv->tx_cq);\r\nfree_netdev(dev);\r\n}\r\nstatic int mlx4_en_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err = 0;\r\nen_dbg(DRV, priv, "Change MTU called - current:%d new:%d\n",\r\ndev->mtu, new_mtu);\r\nif ((new_mtu < MLX4_EN_MIN_MTU) || (new_mtu > priv->max_mtu)) {\r\nen_err(priv, "Bad MTU size:%d.\n", new_mtu);\r\nreturn -EPERM;\r\n}\r\ndev->mtu = new_mtu;\r\nif (netif_running(dev)) {\r\nmutex_lock(&mdev->state_lock);\r\nif (!mdev->device_up) {\r\nen_dbg(DRV, priv, "Change MTU called with card down!?\n");\r\n} else {\r\nmlx4_en_stop_port(dev, 1);\r\nerr = mlx4_en_start_port(dev);\r\nif (err) {\r\nen_err(priv, "Failed restarting port:%d\n",\r\npriv->port);\r\nqueue_work(mdev->workqueue, &priv->watchdog_task);\r\n}\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mlx4_en_hwtstamp_ioctl(struct net_device *dev, struct ifreq *ifr)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct hwtstamp_config config;\r\nif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\r\nreturn -EFAULT;\r\nif (config.flags)\r\nreturn -EINVAL;\r\nif (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_TS))\r\nreturn -EINVAL;\r\nswitch (config.tx_type) {\r\ncase HWTSTAMP_TX_OFF:\r\ncase HWTSTAMP_TX_ON:\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nswitch (config.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\nbreak;\r\ncase HWTSTAMP_FILTER_ALL:\r\ncase HWTSTAMP_FILTER_SOME:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\r\nconfig.rx_filter = HWTSTAMP_FILTER_ALL;\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nif (mlx4_en_timestamp_config(dev, config.tx_type, config.rx_filter)) {\r\nconfig.tx_type = HWTSTAMP_TX_OFF;\r\nconfig.rx_filter = HWTSTAMP_FILTER_NONE;\r\n}\r\nreturn copy_to_user(ifr->ifr_data, &config,\r\nsizeof(config)) ? -EFAULT : 0;\r\n}\r\nstatic int mlx4_en_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nswitch (cmd) {\r\ncase SIOCSHWTSTAMP:\r\nreturn mlx4_en_hwtstamp_ioctl(dev, ifr);\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic int mlx4_en_set_features(struct net_device *netdev,\r\nnetdev_features_t features)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(netdev);\r\nif (features & NETIF_F_LOOPBACK)\r\npriv->ctrl_flags |= cpu_to_be32(MLX4_WQE_CTRL_FORCE_LOOPBACK);\r\nelse\r\npriv->ctrl_flags &=\r\ncpu_to_be32(~MLX4_WQE_CTRL_FORCE_LOOPBACK);\r\nmlx4_en_update_loopback_state(netdev, features);\r\nreturn 0;\r\n}\r\nstatic int mlx4_en_set_vf_mac(struct net_device *dev, int queue, u8 *mac)\r\n{\r\nstruct mlx4_en_priv *en_priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = en_priv->mdev;\r\nu64 mac_u64 = mlx4_en_mac_to_u64(mac);\r\nif (!is_valid_ether_addr(mac))\r\nreturn -EINVAL;\r\nreturn mlx4_set_vf_mac(mdev->dev, en_priv->port, queue, mac_u64);\r\n}\r\nstatic int mlx4_en_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos)\r\n{\r\nstruct mlx4_en_priv *en_priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = en_priv->mdev;\r\nreturn mlx4_set_vf_vlan(mdev->dev, en_priv->port, vf, vlan, qos);\r\n}\r\nstatic int mlx4_en_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)\r\n{\r\nstruct mlx4_en_priv *en_priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = en_priv->mdev;\r\nreturn mlx4_set_vf_spoofchk(mdev->dev, en_priv->port, vf, setting);\r\n}\r\nstatic int mlx4_en_get_vf_config(struct net_device *dev, int vf, struct ifla_vf_info *ivf)\r\n{\r\nstruct mlx4_en_priv *en_priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = en_priv->mdev;\r\nreturn mlx4_get_vf_config(mdev->dev, en_priv->port, vf, ivf);\r\n}\r\nstatic int mlx4_en_set_vf_link_state(struct net_device *dev, int vf, int link_state)\r\n{\r\nstruct mlx4_en_priv *en_priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = en_priv->mdev;\r\nreturn mlx4_set_vf_link_state(mdev->dev, en_priv->port, vf, link_state);\r\n}\r\nint mlx4_en_init_netdev(struct mlx4_en_dev *mdev, int port,\r\nstruct mlx4_en_port_profile *prof)\r\n{\r\nstruct net_device *dev;\r\nstruct mlx4_en_priv *priv;\r\nint i;\r\nint err;\r\nu64 mac_u64;\r\ndev = alloc_etherdev_mqs(sizeof(struct mlx4_en_priv),\r\nMAX_TX_RINGS, MAX_RX_RINGS);\r\nif (dev == NULL)\r\nreturn -ENOMEM;\r\nnetif_set_real_num_tx_queues(dev, prof->tx_ring_num);\r\nnetif_set_real_num_rx_queues(dev, prof->rx_ring_num);\r\nSET_NETDEV_DEV(dev, &mdev->dev->pdev->dev);\r\ndev->dev_id = port - 1;\r\npriv = netdev_priv(dev);\r\nmemset(priv, 0, sizeof(struct mlx4_en_priv));\r\npriv->dev = dev;\r\npriv->mdev = mdev;\r\npriv->ddev = &mdev->pdev->dev;\r\npriv->prof = prof;\r\npriv->port = port;\r\npriv->port_up = false;\r\npriv->flags = prof->flags;\r\npriv->ctrl_flags = cpu_to_be32(MLX4_WQE_CTRL_CQ_UPDATE |\r\nMLX4_WQE_CTRL_SOLICITED);\r\npriv->num_tx_rings_p_up = mdev->profile.num_tx_rings_p_up;\r\npriv->tx_ring_num = prof->tx_ring_num;\r\npriv->tx_ring = kzalloc(sizeof(struct mlx4_en_tx_ring) * MAX_TX_RINGS,\r\nGFP_KERNEL);\r\nif (!priv->tx_ring) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\npriv->tx_cq = kzalloc(sizeof(struct mlx4_en_cq) * MAX_TX_RINGS,\r\nGFP_KERNEL);\r\nif (!priv->tx_cq) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\npriv->rx_ring_num = prof->rx_ring_num;\r\npriv->cqe_factor = (mdev->dev->caps.cqe_size == 64) ? 1 : 0;\r\npriv->mac_index = -1;\r\npriv->msg_enable = MLX4_EN_MSG_LEVEL;\r\nspin_lock_init(&priv->stats_lock);\r\nINIT_WORK(&priv->rx_mode_task, mlx4_en_do_set_rx_mode);\r\nINIT_WORK(&priv->watchdog_task, mlx4_en_restart);\r\nINIT_WORK(&priv->linkstate_task, mlx4_en_linkstate);\r\nINIT_DELAYED_WORK(&priv->stats_task, mlx4_en_do_get_stats);\r\nINIT_DELAYED_WORK(&priv->service_task, mlx4_en_service_task);\r\n#ifdef CONFIG_MLX4_EN_DCB\r\nif (!mlx4_is_slave(priv->mdev->dev)) {\r\nif (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_SET_ETH_SCHED) {\r\ndev->dcbnl_ops = &mlx4_en_dcbnl_ops;\r\n} else {\r\nen_info(priv, "enabling only PFC DCB ops\n");\r\ndev->dcbnl_ops = &mlx4_en_dcbnl_pfc_ops;\r\n}\r\n}\r\n#endif\r\nfor (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i)\r\nINIT_HLIST_HEAD(&priv->mac_hash[i]);\r\npriv->max_mtu = mdev->dev->caps.eth_mtu_cap[priv->port];\r\ndev->addr_len = ETH_ALEN;\r\nmlx4_en_u64_to_mac(dev->dev_addr, mdev->dev->caps.def_mac[priv->port]);\r\nif (!is_valid_ether_addr(dev->dev_addr)) {\r\nif (mlx4_is_slave(priv->mdev->dev)) {\r\neth_hw_addr_random(dev);\r\nen_warn(priv, "Assigned random MAC address %pM\n", dev->dev_addr);\r\nmac_u64 = mlx4_en_mac_to_u64(dev->dev_addr);\r\nmdev->dev->caps.def_mac[priv->port] = mac_u64;\r\n} else {\r\nen_err(priv, "Port: %d, invalid mac burned: %pM, quiting\n",\r\npriv->port, dev->dev_addr);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nmemcpy(priv->prev_mac, dev->dev_addr, sizeof(priv->prev_mac));\r\npriv->stride = roundup_pow_of_two(sizeof(struct mlx4_en_rx_desc) +\r\nDS_SIZE * MLX4_EN_MAX_RX_FRAGS);\r\nerr = mlx4_en_alloc_resources(priv);\r\nif (err)\r\ngoto out;\r\n#ifdef CONFIG_RFS_ACCEL\r\nINIT_LIST_HEAD(&priv->filters);\r\nspin_lock_init(&priv->filters_lock);\r\n#endif\r\npriv->hwtstamp_config.flags = 0;\r\npriv->hwtstamp_config.tx_type = HWTSTAMP_TX_OFF;\r\npriv->hwtstamp_config.rx_filter = HWTSTAMP_FILTER_NONE;\r\nerr = mlx4_alloc_hwq_res(mdev->dev, &priv->res,\r\nMLX4_EN_PAGE_SIZE, MLX4_EN_PAGE_SIZE);\r\nif (err) {\r\nen_err(priv, "Failed to allocate page for rx qps\n");\r\ngoto out;\r\n}\r\npriv->allocated = 1;\r\nif (mlx4_is_master(priv->mdev->dev))\r\ndev->netdev_ops = &mlx4_netdev_ops_master;\r\nelse\r\ndev->netdev_ops = &mlx4_netdev_ops;\r\ndev->watchdog_timeo = MLX4_EN_WATCHDOG_TIMEOUT;\r\nnetif_set_real_num_tx_queues(dev, priv->tx_ring_num);\r\nnetif_set_real_num_rx_queues(dev, priv->rx_ring_num);\r\nSET_ETHTOOL_OPS(dev, &mlx4_en_ethtool_ops);\r\ndev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\r\nif (mdev->LSO_support)\r\ndev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;\r\ndev->vlan_features = dev->hw_features;\r\ndev->hw_features |= NETIF_F_RXCSUM | NETIF_F_RXHASH;\r\ndev->features = dev->hw_features | NETIF_F_HIGHDMA |\r\nNETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |\r\nNETIF_F_HW_VLAN_CTAG_FILTER;\r\ndev->hw_features |= NETIF_F_LOOPBACK;\r\nif (mdev->dev->caps.steering_mode ==\r\nMLX4_STEERING_MODE_DEVICE_MANAGED)\r\ndev->hw_features |= NETIF_F_NTUPLE;\r\nif (mdev->dev->caps.steering_mode != MLX4_STEERING_MODE_A0)\r\ndev->priv_flags |= IFF_UNICAST_FLT;\r\nmdev->pndev[port] = dev;\r\nnetif_carrier_off(dev);\r\nmlx4_en_set_default_moderation(priv);\r\nerr = register_netdev(dev);\r\nif (err) {\r\nen_err(priv, "Netdev registration failed for port %d\n", port);\r\ngoto out;\r\n}\r\npriv->registered = 1;\r\nen_warn(priv, "Using %d TX rings\n", prof->tx_ring_num);\r\nen_warn(priv, "Using %d RX rings\n", prof->rx_ring_num);\r\nmlx4_en_update_loopback_state(priv->dev, priv->dev->features);\r\nmlx4_en_calc_rx_buf(dev);\r\nerr = mlx4_SET_PORT_general(mdev->dev, priv->port,\r\npriv->rx_skb_size + ETH_FCS_LEN,\r\nprof->tx_pause, prof->tx_ppp,\r\nprof->rx_pause, prof->rx_ppp);\r\nif (err) {\r\nen_err(priv, "Failed setting port general configurations "\r\n"for port %d, with error %d\n", priv->port, err);\r\ngoto out;\r\n}\r\nen_warn(priv, "Initializing port\n");\r\nerr = mlx4_INIT_PORT(mdev->dev, priv->port);\r\nif (err) {\r\nen_err(priv, "Failed Initializing port\n");\r\ngoto out;\r\n}\r\nqueue_delayed_work(mdev->workqueue, &priv->stats_task, STATS_DELAY);\r\nif (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_TS)\r\nqueue_delayed_work(mdev->workqueue, &priv->service_task,\r\nSERVICE_TASK_DELAY);\r\nreturn 0;\r\nout:\r\nmlx4_en_destroy_netdev(dev);\r\nreturn err;\r\n}
