int amdgpu_uvd_sw_init(struct amdgpu_device *adev)\r\n{\r\nunsigned long bo_size;\r\nconst char *fw_name;\r\nconst struct common_firmware_header *hdr;\r\nunsigned version_major, version_minor, family_id;\r\nint i, r;\r\nINIT_DELAYED_WORK(&adev->uvd.idle_work, amdgpu_uvd_idle_work_handler);\r\nswitch (adev->asic_type) {\r\n#ifdef CONFIG_DRM_AMDGPU_CIK\r\ncase CHIP_BONAIRE:\r\nfw_name = FIRMWARE_BONAIRE;\r\nbreak;\r\ncase CHIP_KABINI:\r\nfw_name = FIRMWARE_KABINI;\r\nbreak;\r\ncase CHIP_KAVERI:\r\nfw_name = FIRMWARE_KAVERI;\r\nbreak;\r\ncase CHIP_HAWAII:\r\nfw_name = FIRMWARE_HAWAII;\r\nbreak;\r\ncase CHIP_MULLINS:\r\nfw_name = FIRMWARE_MULLINS;\r\nbreak;\r\n#endif\r\ncase CHIP_TONGA:\r\nfw_name = FIRMWARE_TONGA;\r\nbreak;\r\ncase CHIP_FIJI:\r\nfw_name = FIRMWARE_FIJI;\r\nbreak;\r\ncase CHIP_CARRIZO:\r\nfw_name = FIRMWARE_CARRIZO;\r\nbreak;\r\ncase CHIP_STONEY:\r\nfw_name = FIRMWARE_STONEY;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nr = request_firmware(&adev->uvd.fw, fw_name, adev->dev);\r\nif (r) {\r\ndev_err(adev->dev, "amdgpu_uvd: Can't load firmware \"%s\"\n",\r\nfw_name);\r\nreturn r;\r\n}\r\nr = amdgpu_ucode_validate(adev->uvd.fw);\r\nif (r) {\r\ndev_err(adev->dev, "amdgpu_uvd: Can't validate firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(adev->uvd.fw);\r\nadev->uvd.fw = NULL;\r\nreturn r;\r\n}\r\nhdr = (const struct common_firmware_header *)adev->uvd.fw->data;\r\nfamily_id = le32_to_cpu(hdr->ucode_version) & 0xff;\r\nversion_major = (le32_to_cpu(hdr->ucode_version) >> 24) & 0xff;\r\nversion_minor = (le32_to_cpu(hdr->ucode_version) >> 8) & 0xff;\r\nDRM_INFO("Found UVD firmware Version: %hu.%hu Family ID: %hu\n",\r\nversion_major, version_minor, family_id);\r\nbo_size = AMDGPU_GPU_PAGE_ALIGN(le32_to_cpu(hdr->ucode_size_bytes) + 8)\r\n+ AMDGPU_UVD_STACK_SIZE + AMDGPU_UVD_HEAP_SIZE;\r\nr = amdgpu_bo_create(adev, bo_size, PAGE_SIZE, true,\r\nAMDGPU_GEM_DOMAIN_VRAM,\r\nAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED,\r\nNULL, NULL, &adev->uvd.vcpu_bo);\r\nif (r) {\r\ndev_err(adev->dev, "(%d) failed to allocate UVD bo\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_reserve(adev->uvd.vcpu_bo, false);\r\nif (r) {\r\namdgpu_bo_unref(&adev->uvd.vcpu_bo);\r\ndev_err(adev->dev, "(%d) failed to reserve UVD bo\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_pin(adev->uvd.vcpu_bo, AMDGPU_GEM_DOMAIN_VRAM,\r\n&adev->uvd.gpu_addr);\r\nif (r) {\r\namdgpu_bo_unreserve(adev->uvd.vcpu_bo);\r\namdgpu_bo_unref(&adev->uvd.vcpu_bo);\r\ndev_err(adev->dev, "(%d) UVD bo pin failed\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_kmap(adev->uvd.vcpu_bo, &adev->uvd.cpu_addr);\r\nif (r) {\r\ndev_err(adev->dev, "(%d) UVD map failed\n", r);\r\nreturn r;\r\n}\r\namdgpu_bo_unreserve(adev->uvd.vcpu_bo);\r\nfor (i = 0; i < AMDGPU_MAX_UVD_HANDLES; ++i) {\r\natomic_set(&adev->uvd.handles[i], 0);\r\nadev->uvd.filp[i] = NULL;\r\n}\r\nif (!amdgpu_ip_block_version_cmp(adev, AMD_IP_BLOCK_TYPE_UVD, 5, 0))\r\nadev->uvd.address_64_bit = true;\r\nreturn 0;\r\n}\r\nint amdgpu_uvd_sw_fini(struct amdgpu_device *adev)\r\n{\r\nint r;\r\nif (adev->uvd.vcpu_bo == NULL)\r\nreturn 0;\r\nr = amdgpu_bo_reserve(adev->uvd.vcpu_bo, false);\r\nif (!r) {\r\namdgpu_bo_kunmap(adev->uvd.vcpu_bo);\r\namdgpu_bo_unpin(adev->uvd.vcpu_bo);\r\namdgpu_bo_unreserve(adev->uvd.vcpu_bo);\r\n}\r\namdgpu_bo_unref(&adev->uvd.vcpu_bo);\r\namdgpu_ring_fini(&adev->uvd.ring);\r\nrelease_firmware(adev->uvd.fw);\r\nreturn 0;\r\n}\r\nint amdgpu_uvd_suspend(struct amdgpu_device *adev)\r\n{\r\nstruct amdgpu_ring *ring = &adev->uvd.ring;\r\nint i, r;\r\nif (adev->uvd.vcpu_bo == NULL)\r\nreturn 0;\r\nfor (i = 0; i < AMDGPU_MAX_UVD_HANDLES; ++i) {\r\nuint32_t handle = atomic_read(&adev->uvd.handles[i]);\r\nif (handle != 0) {\r\nstruct fence *fence;\r\namdgpu_uvd_note_usage(adev);\r\nr = amdgpu_uvd_get_destroy_msg(ring, handle, &fence);\r\nif (r) {\r\nDRM_ERROR("Error destroying UVD (%d)!\n", r);\r\ncontinue;\r\n}\r\nfence_wait(fence, false);\r\nfence_put(fence);\r\nadev->uvd.filp[i] = NULL;\r\natomic_set(&adev->uvd.handles[i], 0);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint amdgpu_uvd_resume(struct amdgpu_device *adev)\r\n{\r\nunsigned size;\r\nvoid *ptr;\r\nconst struct common_firmware_header *hdr;\r\nunsigned offset;\r\nif (adev->uvd.vcpu_bo == NULL)\r\nreturn -EINVAL;\r\nhdr = (const struct common_firmware_header *)adev->uvd.fw->data;\r\noffset = le32_to_cpu(hdr->ucode_array_offset_bytes);\r\nmemcpy(adev->uvd.cpu_addr, (adev->uvd.fw->data) + offset,\r\n(adev->uvd.fw->size) - offset);\r\nsize = amdgpu_bo_size(adev->uvd.vcpu_bo);\r\nsize -= le32_to_cpu(hdr->ucode_size_bytes);\r\nptr = adev->uvd.cpu_addr;\r\nptr += le32_to_cpu(hdr->ucode_size_bytes);\r\nmemset(ptr, 0, size);\r\nreturn 0;\r\n}\r\nvoid amdgpu_uvd_free_handles(struct amdgpu_device *adev, struct drm_file *filp)\r\n{\r\nstruct amdgpu_ring *ring = &adev->uvd.ring;\r\nint i, r;\r\nfor (i = 0; i < AMDGPU_MAX_UVD_HANDLES; ++i) {\r\nuint32_t handle = atomic_read(&adev->uvd.handles[i]);\r\nif (handle != 0 && adev->uvd.filp[i] == filp) {\r\nstruct fence *fence;\r\namdgpu_uvd_note_usage(adev);\r\nr = amdgpu_uvd_get_destroy_msg(ring, handle, &fence);\r\nif (r) {\r\nDRM_ERROR("Error destroying UVD (%d)!\n", r);\r\ncontinue;\r\n}\r\nfence_wait(fence, false);\r\nfence_put(fence);\r\nadev->uvd.filp[i] = NULL;\r\natomic_set(&adev->uvd.handles[i], 0);\r\n}\r\n}\r\n}\r\nstatic void amdgpu_uvd_force_into_uvd_segment(struct amdgpu_bo *rbo)\r\n{\r\nint i;\r\nfor (i = 0; i < rbo->placement.num_placement; ++i) {\r\nrbo->placements[i].fpfn = 0 >> PAGE_SHIFT;\r\nrbo->placements[i].lpfn = (256 * 1024 * 1024) >> PAGE_SHIFT;\r\n}\r\n}\r\nstatic int amdgpu_uvd_cs_pass1(struct amdgpu_uvd_cs_ctx *ctx)\r\n{\r\nstruct amdgpu_bo_va_mapping *mapping;\r\nstruct amdgpu_bo *bo;\r\nuint32_t cmd, lo, hi;\r\nuint64_t addr;\r\nint r = 0;\r\nlo = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, ctx->data0);\r\nhi = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, ctx->data1);\r\naddr = ((uint64_t)lo) | (((uint64_t)hi) << 32);\r\nmapping = amdgpu_cs_find_mapping(ctx->parser, addr, &bo);\r\nif (mapping == NULL) {\r\nDRM_ERROR("Can't find BO for addr 0x%08Lx\n", addr);\r\nreturn -EINVAL;\r\n}\r\nif (!ctx->parser->adev->uvd.address_64_bit) {\r\ncmd = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, ctx->idx) >> 1;\r\nif (cmd == 0x0 || cmd == 0x3) {\r\nuint32_t domain = AMDGPU_GEM_DOMAIN_VRAM;\r\namdgpu_ttm_placement_from_domain(bo, domain);\r\n}\r\namdgpu_uvd_force_into_uvd_segment(bo);\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\n}\r\nreturn r;\r\n}\r\nstatic int amdgpu_uvd_cs_msg_decode(uint32_t *msg, unsigned buf_sizes[])\r\n{\r\nunsigned stream_type = msg[4];\r\nunsigned width = msg[6];\r\nunsigned height = msg[7];\r\nunsigned dpb_size = msg[9];\r\nunsigned pitch = msg[28];\r\nunsigned level = msg[57];\r\nunsigned width_in_mb = width / 16;\r\nunsigned height_in_mb = ALIGN(height / 16, 2);\r\nunsigned fs_in_mb = width_in_mb * height_in_mb;\r\nunsigned image_size, tmp, min_dpb_size, num_dpb_buffer;\r\nunsigned min_ctx_size = 0;\r\nimage_size = width * height;\r\nimage_size += image_size / 2;\r\nimage_size = ALIGN(image_size, 1024);\r\nswitch (stream_type) {\r\ncase 0:\r\ncase 7:\r\nswitch(level) {\r\ncase 30:\r\nnum_dpb_buffer = 8100 / fs_in_mb;\r\nbreak;\r\ncase 31:\r\nnum_dpb_buffer = 18000 / fs_in_mb;\r\nbreak;\r\ncase 32:\r\nnum_dpb_buffer = 20480 / fs_in_mb;\r\nbreak;\r\ncase 41:\r\nnum_dpb_buffer = 32768 / fs_in_mb;\r\nbreak;\r\ncase 42:\r\nnum_dpb_buffer = 34816 / fs_in_mb;\r\nbreak;\r\ncase 50:\r\nnum_dpb_buffer = 110400 / fs_in_mb;\r\nbreak;\r\ncase 51:\r\nnum_dpb_buffer = 184320 / fs_in_mb;\r\nbreak;\r\ndefault:\r\nnum_dpb_buffer = 184320 / fs_in_mb;\r\nbreak;\r\n}\r\nnum_dpb_buffer++;\r\nif (num_dpb_buffer > 17)\r\nnum_dpb_buffer = 17;\r\nmin_dpb_size = image_size * num_dpb_buffer;\r\nmin_dpb_size += width_in_mb * height_in_mb * num_dpb_buffer * 192;\r\nmin_dpb_size += width_in_mb * height_in_mb * 32;\r\nbreak;\r\ncase 1:\r\nmin_dpb_size = image_size * 3;\r\nmin_dpb_size += width_in_mb * height_in_mb * 128;\r\nmin_dpb_size += width_in_mb * 64;\r\nmin_dpb_size += width_in_mb * 128;\r\ntmp = max(width_in_mb, height_in_mb);\r\nmin_dpb_size += ALIGN(tmp * 7 * 16, 64);\r\nbreak;\r\ncase 3:\r\nmin_dpb_size = image_size * 3;\r\nbreak;\r\ncase 4:\r\nmin_dpb_size = image_size * 3;\r\nmin_dpb_size += width_in_mb * height_in_mb * 64;\r\nmin_dpb_size += ALIGN(width_in_mb * height_in_mb * 32, 64);\r\nbreak;\r\ncase 16:\r\nimage_size = (ALIGN(width, 16) * ALIGN(height, 16) * 3) / 2;\r\nimage_size = ALIGN(image_size, 256);\r\nnum_dpb_buffer = (le32_to_cpu(msg[59]) & 0xff) + 2;\r\nmin_dpb_size = image_size * num_dpb_buffer;\r\nmin_ctx_size = ((width + 255) / 16) * ((height + 255) / 16)\r\n* 16 * num_dpb_buffer + 52 * 1024;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("UVD codec not handled %d!\n", stream_type);\r\nreturn -EINVAL;\r\n}\r\nif (width > pitch) {\r\nDRM_ERROR("Invalid UVD decoding target pitch!\n");\r\nreturn -EINVAL;\r\n}\r\nif (dpb_size < min_dpb_size) {\r\nDRM_ERROR("Invalid dpb_size in UVD message (%d / %d)!\n",\r\ndpb_size, min_dpb_size);\r\nreturn -EINVAL;\r\n}\r\nbuf_sizes[0x1] = dpb_size;\r\nbuf_sizes[0x2] = image_size;\r\nbuf_sizes[0x4] = min_ctx_size;\r\nreturn 0;\r\n}\r\nstatic int amdgpu_uvd_cs_msg(struct amdgpu_uvd_cs_ctx *ctx,\r\nstruct amdgpu_bo *bo, unsigned offset)\r\n{\r\nstruct amdgpu_device *adev = ctx->parser->adev;\r\nint32_t *msg, msg_type, handle;\r\nvoid *ptr;\r\nlong r;\r\nint i;\r\nif (offset & 0x3F) {\r\nDRM_ERROR("UVD messages must be 64 byte aligned!\n");\r\nreturn -EINVAL;\r\n}\r\nr = reservation_object_wait_timeout_rcu(bo->tbo.resv, true, false,\r\nMAX_SCHEDULE_TIMEOUT);\r\nif (r < 0) {\r\nDRM_ERROR("Failed waiting for UVD message (%ld)!\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_kmap(bo, &ptr);\r\nif (r) {\r\nDRM_ERROR("Failed mapping the UVD message (%ld)!\n", r);\r\nreturn r;\r\n}\r\nmsg = ptr + offset;\r\nmsg_type = msg[1];\r\nhandle = msg[2];\r\nif (handle == 0) {\r\nDRM_ERROR("Invalid UVD handle!\n");\r\nreturn -EINVAL;\r\n}\r\nswitch (msg_type) {\r\ncase 0:\r\namdgpu_bo_kunmap(bo);\r\nfor (i = 0; i < AMDGPU_MAX_UVD_HANDLES; ++i) {\r\nif (atomic_read(&adev->uvd.handles[i]) == handle) {\r\nDRM_ERROR("Handle 0x%x already in use!\n", handle);\r\nreturn -EINVAL;\r\n}\r\nif (!atomic_cmpxchg(&adev->uvd.handles[i], 0, handle)) {\r\nadev->uvd.filp[i] = ctx->parser->filp;\r\nreturn 0;\r\n}\r\n}\r\nDRM_ERROR("No more free UVD handles!\n");\r\nreturn -EINVAL;\r\ncase 1:\r\nr = amdgpu_uvd_cs_msg_decode(msg, ctx->buf_sizes);\r\namdgpu_bo_kunmap(bo);\r\nif (r)\r\nreturn r;\r\nfor (i = 0; i < AMDGPU_MAX_UVD_HANDLES; ++i) {\r\nif (atomic_read(&adev->uvd.handles[i]) == handle) {\r\nif (adev->uvd.filp[i] != ctx->parser->filp) {\r\nDRM_ERROR("UVD handle collision detected!\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\n}\r\nDRM_ERROR("Invalid UVD handle 0x%x!\n", handle);\r\nreturn -ENOENT;\r\ncase 2:\r\nfor (i = 0; i < AMDGPU_MAX_UVD_HANDLES; ++i)\r\natomic_cmpxchg(&adev->uvd.handles[i], handle, 0);\r\namdgpu_bo_kunmap(bo);\r\nreturn 0;\r\ndefault:\r\nDRM_ERROR("Illegal UVD message type (%d)!\n", msg_type);\r\nreturn -EINVAL;\r\n}\r\nBUG();\r\nreturn -EINVAL;\r\n}\r\nstatic int amdgpu_uvd_cs_pass2(struct amdgpu_uvd_cs_ctx *ctx)\r\n{\r\nstruct amdgpu_bo_va_mapping *mapping;\r\nstruct amdgpu_bo *bo;\r\nstruct amdgpu_ib *ib;\r\nuint32_t cmd, lo, hi;\r\nuint64_t start, end;\r\nuint64_t addr;\r\nint r;\r\nlo = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, ctx->data0);\r\nhi = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, ctx->data1);\r\naddr = ((uint64_t)lo) | (((uint64_t)hi) << 32);\r\nmapping = amdgpu_cs_find_mapping(ctx->parser, addr, &bo);\r\nif (mapping == NULL)\r\nreturn -EINVAL;\r\nstart = amdgpu_bo_gpu_offset(bo);\r\nend = (mapping->it.last + 1 - mapping->it.start);\r\nend = end * AMDGPU_GPU_PAGE_SIZE + start;\r\naddr -= ((uint64_t)mapping->it.start) * AMDGPU_GPU_PAGE_SIZE;\r\nstart += addr;\r\nib = &ctx->parser->ibs[ctx->ib_idx];\r\nib->ptr[ctx->data0] = start & 0xFFFFFFFF;\r\nib->ptr[ctx->data1] = start >> 32;\r\ncmd = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, ctx->idx) >> 1;\r\nif (cmd < 0x4) {\r\nif ((end - start) < ctx->buf_sizes[cmd]) {\r\nDRM_ERROR("buffer (%d) to small (%d / %d)!\n", cmd,\r\n(unsigned)(end - start),\r\nctx->buf_sizes[cmd]);\r\nreturn -EINVAL;\r\n}\r\n} else if (cmd == 0x206) {\r\nif ((end - start) < ctx->buf_sizes[4]) {\r\nDRM_ERROR("buffer (%d) to small (%d / %d)!\n", cmd,\r\n(unsigned)(end - start),\r\nctx->buf_sizes[4]);\r\nreturn -EINVAL;\r\n}\r\n} else if ((cmd != 0x100) && (cmd != 0x204)) {\r\nDRM_ERROR("invalid UVD command %X!\n", cmd);\r\nreturn -EINVAL;\r\n}\r\nif (!ctx->parser->adev->uvd.address_64_bit) {\r\nif ((start >> 28) != ((end - 1) >> 28)) {\r\nDRM_ERROR("reloc %LX-%LX crossing 256MB boundary!\n",\r\nstart, end);\r\nreturn -EINVAL;\r\n}\r\nif ((cmd == 0 || cmd == 0x3) &&\r\n(start >> 28) != (ctx->parser->adev->uvd.gpu_addr >> 28)) {\r\nDRM_ERROR("msg/fb buffer %LX-%LX out of 256MB segment!\n",\r\nstart, end);\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (cmd == 0) {\r\nctx->has_msg_cmd = true;\r\nr = amdgpu_uvd_cs_msg(ctx, bo, addr);\r\nif (r)\r\nreturn r;\r\n} else if (!ctx->has_msg_cmd) {\r\nDRM_ERROR("Message needed before other commands are send!\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int amdgpu_uvd_cs_reg(struct amdgpu_uvd_cs_ctx *ctx,\r\nint (*cb)(struct amdgpu_uvd_cs_ctx *ctx))\r\n{\r\nstruct amdgpu_ib *ib = &ctx->parser->ibs[ctx->ib_idx];\r\nint i, r;\r\nctx->idx++;\r\nfor (i = 0; i <= ctx->count; ++i) {\r\nunsigned reg = ctx->reg + i;\r\nif (ctx->idx >= ib->length_dw) {\r\nDRM_ERROR("Register command after end of CS!\n");\r\nreturn -EINVAL;\r\n}\r\nswitch (reg) {\r\ncase mmUVD_GPCOM_VCPU_DATA0:\r\nctx->data0 = ctx->idx;\r\nbreak;\r\ncase mmUVD_GPCOM_VCPU_DATA1:\r\nctx->data1 = ctx->idx;\r\nbreak;\r\ncase mmUVD_GPCOM_VCPU_CMD:\r\nr = cb(ctx);\r\nif (r)\r\nreturn r;\r\nbreak;\r\ncase mmUVD_ENGINE_CNTL:\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Invalid reg 0x%X!\n", reg);\r\nreturn -EINVAL;\r\n}\r\nctx->idx++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int amdgpu_uvd_cs_packets(struct amdgpu_uvd_cs_ctx *ctx,\r\nint (*cb)(struct amdgpu_uvd_cs_ctx *ctx))\r\n{\r\nstruct amdgpu_ib *ib = &ctx->parser->ibs[ctx->ib_idx];\r\nint r;\r\nfor (ctx->idx = 0 ; ctx->idx < ib->length_dw; ) {\r\nuint32_t cmd = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, ctx->idx);\r\nunsigned type = CP_PACKET_GET_TYPE(cmd);\r\nswitch (type) {\r\ncase PACKET_TYPE0:\r\nctx->reg = CP_PACKET0_GET_REG(cmd);\r\nctx->count = CP_PACKET_GET_COUNT(cmd);\r\nr = amdgpu_uvd_cs_reg(ctx, cb);\r\nif (r)\r\nreturn r;\r\nbreak;\r\ncase PACKET_TYPE2:\r\n++ctx->idx;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unknown packet type %d !\n", type);\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser, uint32_t ib_idx)\r\n{\r\nstruct amdgpu_uvd_cs_ctx ctx = {};\r\nunsigned buf_sizes[] = {\r\n[0x00000000] = 2048,\r\n[0x00000001] = 0xFFFFFFFF,\r\n[0x00000002] = 0xFFFFFFFF,\r\n[0x00000003] = 2048,\r\n[0x00000004] = 0xFFFFFFFF,\r\n};\r\nstruct amdgpu_ib *ib = &parser->ibs[ib_idx];\r\nint r;\r\nif (ib->length_dw % 16) {\r\nDRM_ERROR("UVD IB length (%d) not 16 dwords aligned!\n",\r\nib->length_dw);\r\nreturn -EINVAL;\r\n}\r\nctx.parser = parser;\r\nctx.buf_sizes = buf_sizes;\r\nctx.ib_idx = ib_idx;\r\nr = amdgpu_uvd_cs_packets(&ctx, amdgpu_uvd_cs_pass1);\r\nif (r)\r\nreturn r;\r\nr = amdgpu_uvd_cs_packets(&ctx, amdgpu_uvd_cs_pass2);\r\nif (r)\r\nreturn r;\r\nif (!ctx.has_msg_cmd) {\r\nDRM_ERROR("UVD-IBs need a msg command!\n");\r\nreturn -EINVAL;\r\n}\r\namdgpu_uvd_note_usage(ctx.parser->adev);\r\nreturn 0;\r\n}\r\nstatic int amdgpu_uvd_free_job(\r\nstruct amdgpu_job *job)\r\n{\r\namdgpu_ib_free(job->adev, job->ibs);\r\nkfree(job->ibs);\r\nreturn 0;\r\n}\r\nstatic int amdgpu_uvd_send_msg(struct amdgpu_ring *ring,\r\nstruct amdgpu_bo *bo,\r\nstruct fence **fence)\r\n{\r\nstruct ttm_validate_buffer tv;\r\nstruct ww_acquire_ctx ticket;\r\nstruct list_head head;\r\nstruct amdgpu_ib *ib = NULL;\r\nstruct fence *f = NULL;\r\nstruct amdgpu_device *adev = ring->adev;\r\nuint64_t addr;\r\nint i, r;\r\nmemset(&tv, 0, sizeof(tv));\r\ntv.bo = &bo->tbo;\r\nINIT_LIST_HEAD(&head);\r\nlist_add(&tv.head, &head);\r\nr = ttm_eu_reserve_buffers(&ticket, &head, true, NULL);\r\nif (r)\r\nreturn r;\r\nif (!bo->adev->uvd.address_64_bit) {\r\namdgpu_ttm_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_VRAM);\r\namdgpu_uvd_force_into_uvd_segment(bo);\r\n}\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);\r\nif (r)\r\ngoto err;\r\nib = kzalloc(sizeof(struct amdgpu_ib), GFP_KERNEL);\r\nif (!ib) {\r\nr = -ENOMEM;\r\ngoto err;\r\n}\r\nr = amdgpu_ib_get(ring, NULL, 64, ib);\r\nif (r)\r\ngoto err1;\r\naddr = amdgpu_bo_gpu_offset(bo);\r\nib->ptr[0] = PACKET0(mmUVD_GPCOM_VCPU_DATA0, 0);\r\nib->ptr[1] = addr;\r\nib->ptr[2] = PACKET0(mmUVD_GPCOM_VCPU_DATA1, 0);\r\nib->ptr[3] = addr >> 32;\r\nib->ptr[4] = PACKET0(mmUVD_GPCOM_VCPU_CMD, 0);\r\nib->ptr[5] = 0;\r\nfor (i = 6; i < 16; ++i)\r\nib->ptr[i] = PACKET2(0);\r\nib->length_dw = 16;\r\nr = amdgpu_sched_ib_submit_kernel_helper(adev, ring, ib, 1,\r\n&amdgpu_uvd_free_job,\r\nAMDGPU_FENCE_OWNER_UNDEFINED,\r\n&f);\r\nif (r)\r\ngoto err2;\r\nttm_eu_fence_buffer_objects(&ticket, &head, f);\r\nif (fence)\r\n*fence = fence_get(f);\r\namdgpu_bo_unref(&bo);\r\nfence_put(f);\r\nif (amdgpu_enable_scheduler)\r\nreturn 0;\r\namdgpu_ib_free(ring->adev, ib);\r\nkfree(ib);\r\nreturn 0;\r\nerr2:\r\namdgpu_ib_free(ring->adev, ib);\r\nerr1:\r\nkfree(ib);\r\nerr:\r\nttm_eu_backoff_reservation(&ticket, &head);\r\nreturn r;\r\n}\r\nint amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,\r\nstruct fence **fence)\r\n{\r\nstruct amdgpu_device *adev = ring->adev;\r\nstruct amdgpu_bo *bo;\r\nuint32_t *msg;\r\nint r, i;\r\nr = amdgpu_bo_create(adev, 1024, PAGE_SIZE, true,\r\nAMDGPU_GEM_DOMAIN_VRAM,\r\nAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED,\r\nNULL, NULL, &bo);\r\nif (r)\r\nreturn r;\r\nr = amdgpu_bo_reserve(bo, false);\r\nif (r) {\r\namdgpu_bo_unref(&bo);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_kmap(bo, (void **)&msg);\r\nif (r) {\r\namdgpu_bo_unreserve(bo);\r\namdgpu_bo_unref(&bo);\r\nreturn r;\r\n}\r\nmsg[0] = cpu_to_le32(0x00000de4);\r\nmsg[1] = cpu_to_le32(0x00000000);\r\nmsg[2] = cpu_to_le32(handle);\r\nmsg[3] = cpu_to_le32(0x00000000);\r\nmsg[4] = cpu_to_le32(0x00000000);\r\nmsg[5] = cpu_to_le32(0x00000000);\r\nmsg[6] = cpu_to_le32(0x00000000);\r\nmsg[7] = cpu_to_le32(0x00000780);\r\nmsg[8] = cpu_to_le32(0x00000440);\r\nmsg[9] = cpu_to_le32(0x00000000);\r\nmsg[10] = cpu_to_le32(0x01b37000);\r\nfor (i = 11; i < 1024; ++i)\r\nmsg[i] = cpu_to_le32(0x0);\r\namdgpu_bo_kunmap(bo);\r\namdgpu_bo_unreserve(bo);\r\nreturn amdgpu_uvd_send_msg(ring, bo, fence);\r\n}\r\nint amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,\r\nstruct fence **fence)\r\n{\r\nstruct amdgpu_device *adev = ring->adev;\r\nstruct amdgpu_bo *bo;\r\nuint32_t *msg;\r\nint r, i;\r\nr = amdgpu_bo_create(adev, 1024, PAGE_SIZE, true,\r\nAMDGPU_GEM_DOMAIN_VRAM,\r\nAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED,\r\nNULL, NULL, &bo);\r\nif (r)\r\nreturn r;\r\nr = amdgpu_bo_reserve(bo, false);\r\nif (r) {\r\namdgpu_bo_unref(&bo);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_kmap(bo, (void **)&msg);\r\nif (r) {\r\namdgpu_bo_unreserve(bo);\r\namdgpu_bo_unref(&bo);\r\nreturn r;\r\n}\r\nmsg[0] = cpu_to_le32(0x00000de4);\r\nmsg[1] = cpu_to_le32(0x00000002);\r\nmsg[2] = cpu_to_le32(handle);\r\nmsg[3] = cpu_to_le32(0x00000000);\r\nfor (i = 4; i < 1024; ++i)\r\nmsg[i] = cpu_to_le32(0x0);\r\namdgpu_bo_kunmap(bo);\r\namdgpu_bo_unreserve(bo);\r\nreturn amdgpu_uvd_send_msg(ring, bo, fence);\r\n}\r\nstatic void amdgpu_uvd_idle_work_handler(struct work_struct *work)\r\n{\r\nstruct amdgpu_device *adev =\r\ncontainer_of(work, struct amdgpu_device, uvd.idle_work.work);\r\nunsigned i, fences, handles = 0;\r\nfences = amdgpu_fence_count_emitted(&adev->uvd.ring);\r\nfor (i = 0; i < AMDGPU_MAX_UVD_HANDLES; ++i)\r\nif (atomic_read(&adev->uvd.handles[i]))\r\n++handles;\r\nif (fences == 0 && handles == 0) {\r\nif (adev->pm.dpm_enabled) {\r\namdgpu_dpm_enable_uvd(adev, false);\r\n} else {\r\namdgpu_asic_set_uvd_clocks(adev, 0, 0);\r\n}\r\n} else {\r\nschedule_delayed_work(&adev->uvd.idle_work,\r\nmsecs_to_jiffies(UVD_IDLE_TIMEOUT_MS));\r\n}\r\n}\r\nstatic void amdgpu_uvd_note_usage(struct amdgpu_device *adev)\r\n{\r\nbool set_clocks = !cancel_delayed_work_sync(&adev->uvd.idle_work);\r\nset_clocks &= schedule_delayed_work(&adev->uvd.idle_work,\r\nmsecs_to_jiffies(UVD_IDLE_TIMEOUT_MS));\r\nif (set_clocks) {\r\nif (adev->pm.dpm_enabled) {\r\namdgpu_dpm_enable_uvd(adev, true);\r\n} else {\r\namdgpu_asic_set_uvd_clocks(adev, 53300, 40000);\r\n}\r\n}\r\n}
