static inline struct nilfs_dat_info *NILFS_DAT_I(struct inode *dat)\r\n{\r\nreturn (struct nilfs_dat_info *)NILFS_MDT(dat);\r\n}\r\nstatic int nilfs_dat_prepare_entry(struct inode *dat,\r\nstruct nilfs_palloc_req *req, int create)\r\n{\r\nreturn nilfs_palloc_get_entry_block(dat, req->pr_entry_nr,\r\ncreate, &req->pr_entry_bh);\r\n}\r\nstatic void nilfs_dat_commit_entry(struct inode *dat,\r\nstruct nilfs_palloc_req *req)\r\n{\r\nmark_buffer_dirty(req->pr_entry_bh);\r\nnilfs_mdt_mark_dirty(dat);\r\nbrelse(req->pr_entry_bh);\r\n}\r\nstatic void nilfs_dat_abort_entry(struct inode *dat,\r\nstruct nilfs_palloc_req *req)\r\n{\r\nbrelse(req->pr_entry_bh);\r\n}\r\nint nilfs_dat_prepare_alloc(struct inode *dat, struct nilfs_palloc_req *req)\r\n{\r\nint ret;\r\nret = nilfs_palloc_prepare_alloc_entry(dat, req);\r\nif (ret < 0)\r\nreturn ret;\r\nret = nilfs_dat_prepare_entry(dat, req, 1);\r\nif (ret < 0)\r\nnilfs_palloc_abort_alloc_entry(dat, req);\r\nreturn ret;\r\n}\r\nvoid nilfs_dat_commit_alloc(struct inode *dat, struct nilfs_palloc_req *req)\r\n{\r\nstruct nilfs_dat_entry *entry;\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(req->pr_entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\r\nreq->pr_entry_bh, kaddr);\r\nentry->de_start = cpu_to_le64(NILFS_CNO_MIN);\r\nentry->de_end = cpu_to_le64(NILFS_CNO_MAX);\r\nentry->de_blocknr = cpu_to_le64(0);\r\nkunmap_atomic(kaddr);\r\nnilfs_palloc_commit_alloc_entry(dat, req);\r\nnilfs_dat_commit_entry(dat, req);\r\n}\r\nvoid nilfs_dat_abort_alloc(struct inode *dat, struct nilfs_palloc_req *req)\r\n{\r\nnilfs_dat_abort_entry(dat, req);\r\nnilfs_palloc_abort_alloc_entry(dat, req);\r\n}\r\nstatic void nilfs_dat_commit_free(struct inode *dat,\r\nstruct nilfs_palloc_req *req)\r\n{\r\nstruct nilfs_dat_entry *entry;\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(req->pr_entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\r\nreq->pr_entry_bh, kaddr);\r\nentry->de_start = cpu_to_le64(NILFS_CNO_MIN);\r\nentry->de_end = cpu_to_le64(NILFS_CNO_MIN);\r\nentry->de_blocknr = cpu_to_le64(0);\r\nkunmap_atomic(kaddr);\r\nnilfs_dat_commit_entry(dat, req);\r\nnilfs_palloc_commit_free_entry(dat, req);\r\n}\r\nint nilfs_dat_prepare_start(struct inode *dat, struct nilfs_palloc_req *req)\r\n{\r\nint ret;\r\nret = nilfs_dat_prepare_entry(dat, req, 0);\r\nWARN_ON(ret == -ENOENT);\r\nreturn ret;\r\n}\r\nvoid nilfs_dat_commit_start(struct inode *dat, struct nilfs_palloc_req *req,\r\nsector_t blocknr)\r\n{\r\nstruct nilfs_dat_entry *entry;\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(req->pr_entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\r\nreq->pr_entry_bh, kaddr);\r\nentry->de_start = cpu_to_le64(nilfs_mdt_cno(dat));\r\nentry->de_blocknr = cpu_to_le64(blocknr);\r\nkunmap_atomic(kaddr);\r\nnilfs_dat_commit_entry(dat, req);\r\n}\r\nint nilfs_dat_prepare_end(struct inode *dat, struct nilfs_palloc_req *req)\r\n{\r\nstruct nilfs_dat_entry *entry;\r\nsector_t blocknr;\r\nvoid *kaddr;\r\nint ret;\r\nret = nilfs_dat_prepare_entry(dat, req, 0);\r\nif (ret < 0) {\r\nWARN_ON(ret == -ENOENT);\r\nreturn ret;\r\n}\r\nkaddr = kmap_atomic(req->pr_entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\r\nreq->pr_entry_bh, kaddr);\r\nblocknr = le64_to_cpu(entry->de_blocknr);\r\nkunmap_atomic(kaddr);\r\nif (blocknr == 0) {\r\nret = nilfs_palloc_prepare_free_entry(dat, req);\r\nif (ret < 0) {\r\nnilfs_dat_abort_entry(dat, req);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid nilfs_dat_commit_end(struct inode *dat, struct nilfs_palloc_req *req,\r\nint dead)\r\n{\r\nstruct nilfs_dat_entry *entry;\r\n__u64 start, end;\r\nsector_t blocknr;\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(req->pr_entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\r\nreq->pr_entry_bh, kaddr);\r\nend = start = le64_to_cpu(entry->de_start);\r\nif (!dead) {\r\nend = nilfs_mdt_cno(dat);\r\nWARN_ON(start > end);\r\n}\r\nentry->de_end = cpu_to_le64(end);\r\nblocknr = le64_to_cpu(entry->de_blocknr);\r\nkunmap_atomic(kaddr);\r\nif (blocknr == 0)\r\nnilfs_dat_commit_free(dat, req);\r\nelse\r\nnilfs_dat_commit_entry(dat, req);\r\n}\r\nvoid nilfs_dat_abort_end(struct inode *dat, struct nilfs_palloc_req *req)\r\n{\r\nstruct nilfs_dat_entry *entry;\r\n__u64 start;\r\nsector_t blocknr;\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(req->pr_entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\r\nreq->pr_entry_bh, kaddr);\r\nstart = le64_to_cpu(entry->de_start);\r\nblocknr = le64_to_cpu(entry->de_blocknr);\r\nkunmap_atomic(kaddr);\r\nif (start == nilfs_mdt_cno(dat) && blocknr == 0)\r\nnilfs_palloc_abort_free_entry(dat, req);\r\nnilfs_dat_abort_entry(dat, req);\r\n}\r\nint nilfs_dat_prepare_update(struct inode *dat,\r\nstruct nilfs_palloc_req *oldreq,\r\nstruct nilfs_palloc_req *newreq)\r\n{\r\nint ret;\r\nret = nilfs_dat_prepare_end(dat, oldreq);\r\nif (!ret) {\r\nret = nilfs_dat_prepare_alloc(dat, newreq);\r\nif (ret < 0)\r\nnilfs_dat_abort_end(dat, oldreq);\r\n}\r\nreturn ret;\r\n}\r\nvoid nilfs_dat_commit_update(struct inode *dat,\r\nstruct nilfs_palloc_req *oldreq,\r\nstruct nilfs_palloc_req *newreq, int dead)\r\n{\r\nnilfs_dat_commit_end(dat, oldreq, dead);\r\nnilfs_dat_commit_alloc(dat, newreq);\r\n}\r\nvoid nilfs_dat_abort_update(struct inode *dat,\r\nstruct nilfs_palloc_req *oldreq,\r\nstruct nilfs_palloc_req *newreq)\r\n{\r\nnilfs_dat_abort_end(dat, oldreq);\r\nnilfs_dat_abort_alloc(dat, newreq);\r\n}\r\nint nilfs_dat_mark_dirty(struct inode *dat, __u64 vblocknr)\r\n{\r\nstruct nilfs_palloc_req req;\r\nint ret;\r\nreq.pr_entry_nr = vblocknr;\r\nret = nilfs_dat_prepare_entry(dat, &req, 0);\r\nif (ret == 0)\r\nnilfs_dat_commit_entry(dat, &req);\r\nreturn ret;\r\n}\r\nint nilfs_dat_freev(struct inode *dat, __u64 *vblocknrs, size_t nitems)\r\n{\r\nreturn nilfs_palloc_freev(dat, vblocknrs, nitems);\r\n}\r\nint nilfs_dat_move(struct inode *dat, __u64 vblocknr, sector_t blocknr)\r\n{\r\nstruct buffer_head *entry_bh;\r\nstruct nilfs_dat_entry *entry;\r\nvoid *kaddr;\r\nint ret;\r\nret = nilfs_palloc_get_entry_block(dat, vblocknr, 0, &entry_bh);\r\nif (ret < 0)\r\nreturn ret;\r\nif (!buffer_nilfs_redirected(entry_bh)) {\r\nret = nilfs_mdt_freeze_buffer(dat, entry_bh);\r\nif (ret) {\r\nbrelse(entry_bh);\r\nreturn ret;\r\n}\r\n}\r\nkaddr = kmap_atomic(entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, vblocknr, entry_bh, kaddr);\r\nif (unlikely(entry->de_blocknr == cpu_to_le64(0))) {\r\nprintk(KERN_CRIT "%s: vbn = %llu, [%llu, %llu)\n", __func__,\r\n(unsigned long long)vblocknr,\r\n(unsigned long long)le64_to_cpu(entry->de_start),\r\n(unsigned long long)le64_to_cpu(entry->de_end));\r\nkunmap_atomic(kaddr);\r\nbrelse(entry_bh);\r\nreturn -EINVAL;\r\n}\r\nWARN_ON(blocknr == 0);\r\nentry->de_blocknr = cpu_to_le64(blocknr);\r\nkunmap_atomic(kaddr);\r\nmark_buffer_dirty(entry_bh);\r\nnilfs_mdt_mark_dirty(dat);\r\nbrelse(entry_bh);\r\nreturn 0;\r\n}\r\nint nilfs_dat_translate(struct inode *dat, __u64 vblocknr, sector_t *blocknrp)\r\n{\r\nstruct buffer_head *entry_bh, *bh;\r\nstruct nilfs_dat_entry *entry;\r\nsector_t blocknr;\r\nvoid *kaddr;\r\nint ret;\r\nret = nilfs_palloc_get_entry_block(dat, vblocknr, 0, &entry_bh);\r\nif (ret < 0)\r\nreturn ret;\r\nif (!nilfs_doing_gc() && buffer_nilfs_redirected(entry_bh)) {\r\nbh = nilfs_mdt_get_frozen_buffer(dat, entry_bh);\r\nif (bh) {\r\nWARN_ON(!buffer_uptodate(bh));\r\nbrelse(entry_bh);\r\nentry_bh = bh;\r\n}\r\n}\r\nkaddr = kmap_atomic(entry_bh->b_page);\r\nentry = nilfs_palloc_block_get_entry(dat, vblocknr, entry_bh, kaddr);\r\nblocknr = le64_to_cpu(entry->de_blocknr);\r\nif (blocknr == 0) {\r\nret = -ENOENT;\r\ngoto out;\r\n}\r\n*blocknrp = blocknr;\r\nout:\r\nkunmap_atomic(kaddr);\r\nbrelse(entry_bh);\r\nreturn ret;\r\n}\r\nssize_t nilfs_dat_get_vinfo(struct inode *dat, void *buf, unsigned visz,\r\nsize_t nvi)\r\n{\r\nstruct buffer_head *entry_bh;\r\nstruct nilfs_dat_entry *entry;\r\nstruct nilfs_vinfo *vinfo = buf;\r\n__u64 first, last;\r\nvoid *kaddr;\r\nunsigned long entries_per_block = NILFS_MDT(dat)->mi_entries_per_block;\r\nint i, j, n, ret;\r\nfor (i = 0; i < nvi; i += n) {\r\nret = nilfs_palloc_get_entry_block(dat, vinfo->vi_vblocknr,\r\n0, &entry_bh);\r\nif (ret < 0)\r\nreturn ret;\r\nkaddr = kmap_atomic(entry_bh->b_page);\r\nfirst = vinfo->vi_vblocknr;\r\ndo_div(first, entries_per_block);\r\nfirst *= entries_per_block;\r\nlast = first + entries_per_block - 1;\r\nfor (j = i, n = 0;\r\nj < nvi && vinfo->vi_vblocknr >= first &&\r\nvinfo->vi_vblocknr <= last;\r\nj++, n++, vinfo = (void *)vinfo + visz) {\r\nentry = nilfs_palloc_block_get_entry(\r\ndat, vinfo->vi_vblocknr, entry_bh, kaddr);\r\nvinfo->vi_start = le64_to_cpu(entry->de_start);\r\nvinfo->vi_end = le64_to_cpu(entry->de_end);\r\nvinfo->vi_blocknr = le64_to_cpu(entry->de_blocknr);\r\n}\r\nkunmap_atomic(kaddr);\r\nbrelse(entry_bh);\r\n}\r\nreturn nvi;\r\n}\r\nint nilfs_dat_read(struct super_block *sb, size_t entry_size,\r\nstruct nilfs_inode *raw_inode, struct inode **inodep)\r\n{\r\nstatic struct lock_class_key dat_lock_key;\r\nstruct inode *dat;\r\nstruct nilfs_dat_info *di;\r\nint err;\r\nif (entry_size > sb->s_blocksize) {\r\nprintk(KERN_ERR\r\n"NILFS: too large DAT entry size: %zu bytes.\n",\r\nentry_size);\r\nreturn -EINVAL;\r\n} else if (entry_size < NILFS_MIN_DAT_ENTRY_SIZE) {\r\nprintk(KERN_ERR\r\n"NILFS: too small DAT entry size: %zu bytes.\n",\r\nentry_size);\r\nreturn -EINVAL;\r\n}\r\ndat = nilfs_iget_locked(sb, NULL, NILFS_DAT_INO);\r\nif (unlikely(!dat))\r\nreturn -ENOMEM;\r\nif (!(dat->i_state & I_NEW))\r\ngoto out;\r\nerr = nilfs_mdt_init(dat, NILFS_MDT_GFP, sizeof(*di));\r\nif (err)\r\ngoto failed;\r\nerr = nilfs_palloc_init_blockgroup(dat, entry_size);\r\nif (err)\r\ngoto failed;\r\ndi = NILFS_DAT_I(dat);\r\nlockdep_set_class(&di->mi.mi_sem, &dat_lock_key);\r\nnilfs_palloc_setup_cache(dat, &di->palloc_cache);\r\nnilfs_mdt_setup_shadow_map(dat, &di->shadow);\r\nerr = nilfs_read_inode_common(dat, raw_inode);\r\nif (err)\r\ngoto failed;\r\nunlock_new_inode(dat);\r\nout:\r\n*inodep = dat;\r\nreturn 0;\r\nfailed:\r\niget_failed(dat);\r\nreturn err;\r\n}
