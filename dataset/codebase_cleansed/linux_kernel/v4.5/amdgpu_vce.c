int amdgpu_vce_sw_init(struct amdgpu_device *adev, unsigned long size)\r\n{\r\nconst char *fw_name;\r\nconst struct common_firmware_header *hdr;\r\nunsigned ucode_version, version_major, version_minor, binary_id;\r\nint i, r;\r\nINIT_DELAYED_WORK(&adev->vce.idle_work, amdgpu_vce_idle_work_handler);\r\nswitch (adev->asic_type) {\r\n#ifdef CONFIG_DRM_AMDGPU_CIK\r\ncase CHIP_BONAIRE:\r\nfw_name = FIRMWARE_BONAIRE;\r\nbreak;\r\ncase CHIP_KAVERI:\r\nfw_name = FIRMWARE_KAVERI;\r\nbreak;\r\ncase CHIP_KABINI:\r\nfw_name = FIRMWARE_KABINI;\r\nbreak;\r\ncase CHIP_HAWAII:\r\nfw_name = FIRMWARE_HAWAII;\r\nbreak;\r\ncase CHIP_MULLINS:\r\nfw_name = FIRMWARE_MULLINS;\r\nbreak;\r\n#endif\r\ncase CHIP_TONGA:\r\nfw_name = FIRMWARE_TONGA;\r\nbreak;\r\ncase CHIP_CARRIZO:\r\nfw_name = FIRMWARE_CARRIZO;\r\nbreak;\r\ncase CHIP_FIJI:\r\nfw_name = FIRMWARE_FIJI;\r\nbreak;\r\ncase CHIP_STONEY:\r\nfw_name = FIRMWARE_STONEY;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nr = request_firmware(&adev->vce.fw, fw_name, adev->dev);\r\nif (r) {\r\ndev_err(adev->dev, "amdgpu_vce: Can't load firmware \"%s\"\n",\r\nfw_name);\r\nreturn r;\r\n}\r\nr = amdgpu_ucode_validate(adev->vce.fw);\r\nif (r) {\r\ndev_err(adev->dev, "amdgpu_vce: Can't validate firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(adev->vce.fw);\r\nadev->vce.fw = NULL;\r\nreturn r;\r\n}\r\nhdr = (const struct common_firmware_header *)adev->vce.fw->data;\r\nucode_version = le32_to_cpu(hdr->ucode_version);\r\nversion_major = (ucode_version >> 20) & 0xfff;\r\nversion_minor = (ucode_version >> 8) & 0xfff;\r\nbinary_id = ucode_version & 0xff;\r\nDRM_INFO("Found VCE firmware Version: %hhd.%hhd Binary ID: %hhd\n",\r\nversion_major, version_minor, binary_id);\r\nadev->vce.fw_version = ((version_major << 24) | (version_minor << 16) |\r\n(binary_id << 8));\r\nr = amdgpu_bo_create(adev, size, PAGE_SIZE, true,\r\nAMDGPU_GEM_DOMAIN_VRAM,\r\nAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED,\r\nNULL, NULL, &adev->vce.vcpu_bo);\r\nif (r) {\r\ndev_err(adev->dev, "(%d) failed to allocate VCE bo\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_reserve(adev->vce.vcpu_bo, false);\r\nif (r) {\r\namdgpu_bo_unref(&adev->vce.vcpu_bo);\r\ndev_err(adev->dev, "(%d) failed to reserve VCE bo\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_pin(adev->vce.vcpu_bo, AMDGPU_GEM_DOMAIN_VRAM,\r\n&adev->vce.gpu_addr);\r\namdgpu_bo_unreserve(adev->vce.vcpu_bo);\r\nif (r) {\r\namdgpu_bo_unref(&adev->vce.vcpu_bo);\r\ndev_err(adev->dev, "(%d) VCE bo pin failed\n", r);\r\nreturn r;\r\n}\r\nfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\r\natomic_set(&adev->vce.handles[i], 0);\r\nadev->vce.filp[i] = NULL;\r\n}\r\nreturn 0;\r\n}\r\nint amdgpu_vce_sw_fini(struct amdgpu_device *adev)\r\n{\r\nif (adev->vce.vcpu_bo == NULL)\r\nreturn 0;\r\namdgpu_bo_unref(&adev->vce.vcpu_bo);\r\namdgpu_ring_fini(&adev->vce.ring[0]);\r\namdgpu_ring_fini(&adev->vce.ring[1]);\r\nrelease_firmware(adev->vce.fw);\r\nreturn 0;\r\n}\r\nint amdgpu_vce_suspend(struct amdgpu_device *adev)\r\n{\r\nint i;\r\nif (adev->vce.vcpu_bo == NULL)\r\nreturn 0;\r\nfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i)\r\nif (atomic_read(&adev->vce.handles[i]))\r\nbreak;\r\nif (i == AMDGPU_MAX_VCE_HANDLES)\r\nreturn 0;\r\nreturn -EINVAL;\r\n}\r\nint amdgpu_vce_resume(struct amdgpu_device *adev)\r\n{\r\nvoid *cpu_addr;\r\nconst struct common_firmware_header *hdr;\r\nunsigned offset;\r\nint r;\r\nif (adev->vce.vcpu_bo == NULL)\r\nreturn -EINVAL;\r\nr = amdgpu_bo_reserve(adev->vce.vcpu_bo, false);\r\nif (r) {\r\ndev_err(adev->dev, "(%d) failed to reserve VCE bo\n", r);\r\nreturn r;\r\n}\r\nr = amdgpu_bo_kmap(adev->vce.vcpu_bo, &cpu_addr);\r\nif (r) {\r\namdgpu_bo_unreserve(adev->vce.vcpu_bo);\r\ndev_err(adev->dev, "(%d) VCE map failed\n", r);\r\nreturn r;\r\n}\r\nhdr = (const struct common_firmware_header *)adev->vce.fw->data;\r\noffset = le32_to_cpu(hdr->ucode_array_offset_bytes);\r\nmemcpy(cpu_addr, (adev->vce.fw->data) + offset,\r\n(adev->vce.fw->size) - offset);\r\namdgpu_bo_kunmap(adev->vce.vcpu_bo);\r\namdgpu_bo_unreserve(adev->vce.vcpu_bo);\r\nreturn 0;\r\n}\r\nstatic void amdgpu_vce_idle_work_handler(struct work_struct *work)\r\n{\r\nstruct amdgpu_device *adev =\r\ncontainer_of(work, struct amdgpu_device, vce.idle_work.work);\r\nif ((amdgpu_fence_count_emitted(&adev->vce.ring[0]) == 0) &&\r\n(amdgpu_fence_count_emitted(&adev->vce.ring[1]) == 0)) {\r\nif (adev->pm.dpm_enabled) {\r\namdgpu_dpm_enable_vce(adev, false);\r\n} else {\r\namdgpu_asic_set_vce_clocks(adev, 0, 0);\r\n}\r\n} else {\r\nschedule_delayed_work(&adev->vce.idle_work,\r\nmsecs_to_jiffies(VCE_IDLE_TIMEOUT_MS));\r\n}\r\n}\r\nstatic void amdgpu_vce_note_usage(struct amdgpu_device *adev)\r\n{\r\nbool streams_changed = false;\r\nbool set_clocks = !cancel_delayed_work_sync(&adev->vce.idle_work);\r\nset_clocks &= schedule_delayed_work(&adev->vce.idle_work,\r\nmsecs_to_jiffies(VCE_IDLE_TIMEOUT_MS));\r\nif (adev->pm.dpm_enabled) {\r\nstreams_changed = false;\r\n}\r\nif (set_clocks || streams_changed) {\r\nif (adev->pm.dpm_enabled) {\r\namdgpu_dpm_enable_vce(adev, true);\r\n} else {\r\namdgpu_asic_set_vce_clocks(adev, 53300, 40000);\r\n}\r\n}\r\n}\r\nvoid amdgpu_vce_free_handles(struct amdgpu_device *adev, struct drm_file *filp)\r\n{\r\nstruct amdgpu_ring *ring = &adev->vce.ring[0];\r\nint i, r;\r\nfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\r\nuint32_t handle = atomic_read(&adev->vce.handles[i]);\r\nif (!handle || adev->vce.filp[i] != filp)\r\ncontinue;\r\namdgpu_vce_note_usage(adev);\r\nr = amdgpu_vce_get_destroy_msg(ring, handle, NULL);\r\nif (r)\r\nDRM_ERROR("Error destroying VCE handle (%d)!\n", r);\r\nadev->vce.filp[i] = NULL;\r\natomic_set(&adev->vce.handles[i], 0);\r\n}\r\n}\r\nstatic int amdgpu_vce_free_job(\r\nstruct amdgpu_job *job)\r\n{\r\namdgpu_ib_free(job->adev, job->ibs);\r\nkfree(job->ibs);\r\nreturn 0;\r\n}\r\nint amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,\r\nstruct fence **fence)\r\n{\r\nconst unsigned ib_size_dw = 1024;\r\nstruct amdgpu_ib *ib = NULL;\r\nstruct fence *f = NULL;\r\nstruct amdgpu_device *adev = ring->adev;\r\nuint64_t dummy;\r\nint i, r;\r\nib = kzalloc(sizeof(struct amdgpu_ib), GFP_KERNEL);\r\nif (!ib)\r\nreturn -ENOMEM;\r\nr = amdgpu_ib_get(ring, NULL, ib_size_dw * 4, ib);\r\nif (r) {\r\nDRM_ERROR("amdgpu: failed to get ib (%d).\n", r);\r\nkfree(ib);\r\nreturn r;\r\n}\r\ndummy = ib->gpu_addr + 1024;\r\nib->length_dw = 0;\r\nib->ptr[ib->length_dw++] = 0x0000000c;\r\nib->ptr[ib->length_dw++] = 0x00000001;\r\nib->ptr[ib->length_dw++] = handle;\r\nif ((ring->adev->vce.fw_version >> 24) >= 52)\r\nib->ptr[ib->length_dw++] = 0x00000040;\r\nelse\r\nib->ptr[ib->length_dw++] = 0x00000030;\r\nib->ptr[ib->length_dw++] = 0x01000001;\r\nib->ptr[ib->length_dw++] = 0x00000000;\r\nib->ptr[ib->length_dw++] = 0x00000042;\r\nib->ptr[ib->length_dw++] = 0x0000000a;\r\nib->ptr[ib->length_dw++] = 0x00000001;\r\nib->ptr[ib->length_dw++] = 0x00000080;\r\nib->ptr[ib->length_dw++] = 0x00000060;\r\nib->ptr[ib->length_dw++] = 0x00000100;\r\nib->ptr[ib->length_dw++] = 0x00000100;\r\nib->ptr[ib->length_dw++] = 0x0000000c;\r\nib->ptr[ib->length_dw++] = 0x00000000;\r\nif ((ring->adev->vce.fw_version >> 24) >= 52) {\r\nib->ptr[ib->length_dw++] = 0x00000000;\r\nib->ptr[ib->length_dw++] = 0x00000000;\r\nib->ptr[ib->length_dw++] = 0x00000000;\r\nib->ptr[ib->length_dw++] = 0x00000000;\r\n}\r\nib->ptr[ib->length_dw++] = 0x00000014;\r\nib->ptr[ib->length_dw++] = 0x05000005;\r\nib->ptr[ib->length_dw++] = upper_32_bits(dummy);\r\nib->ptr[ib->length_dw++] = dummy;\r\nib->ptr[ib->length_dw++] = 0x00000001;\r\nfor (i = ib->length_dw; i < ib_size_dw; ++i)\r\nib->ptr[i] = 0x0;\r\nr = amdgpu_sched_ib_submit_kernel_helper(adev, ring, ib, 1,\r\n&amdgpu_vce_free_job,\r\nAMDGPU_FENCE_OWNER_UNDEFINED,\r\n&f);\r\nif (r)\r\ngoto err;\r\nif (fence)\r\n*fence = fence_get(f);\r\nfence_put(f);\r\nif (amdgpu_enable_scheduler)\r\nreturn 0;\r\nerr:\r\namdgpu_ib_free(adev, ib);\r\nkfree(ib);\r\nreturn r;\r\n}\r\nint amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,\r\nstruct fence **fence)\r\n{\r\nconst unsigned ib_size_dw = 1024;\r\nstruct amdgpu_ib *ib = NULL;\r\nstruct fence *f = NULL;\r\nstruct amdgpu_device *adev = ring->adev;\r\nuint64_t dummy;\r\nint i, r;\r\nib = kzalloc(sizeof(struct amdgpu_ib), GFP_KERNEL);\r\nif (!ib)\r\nreturn -ENOMEM;\r\nr = amdgpu_ib_get(ring, NULL, ib_size_dw * 4, ib);\r\nif (r) {\r\nkfree(ib);\r\nDRM_ERROR("amdgpu: failed to get ib (%d).\n", r);\r\nreturn r;\r\n}\r\ndummy = ib->gpu_addr + 1024;\r\nib->length_dw = 0;\r\nib->ptr[ib->length_dw++] = 0x0000000c;\r\nib->ptr[ib->length_dw++] = 0x00000001;\r\nib->ptr[ib->length_dw++] = handle;\r\nib->ptr[ib->length_dw++] = 0x00000014;\r\nib->ptr[ib->length_dw++] = 0x05000005;\r\nib->ptr[ib->length_dw++] = upper_32_bits(dummy);\r\nib->ptr[ib->length_dw++] = dummy;\r\nib->ptr[ib->length_dw++] = 0x00000001;\r\nib->ptr[ib->length_dw++] = 0x00000008;\r\nib->ptr[ib->length_dw++] = 0x02000001;\r\nfor (i = ib->length_dw; i < ib_size_dw; ++i)\r\nib->ptr[i] = 0x0;\r\nr = amdgpu_sched_ib_submit_kernel_helper(adev, ring, ib, 1,\r\n&amdgpu_vce_free_job,\r\nAMDGPU_FENCE_OWNER_UNDEFINED,\r\n&f);\r\nif (r)\r\ngoto err;\r\nif (fence)\r\n*fence = fence_get(f);\r\nfence_put(f);\r\nif (amdgpu_enable_scheduler)\r\nreturn 0;\r\nerr:\r\namdgpu_ib_free(adev, ib);\r\nkfree(ib);\r\nreturn r;\r\n}\r\nstatic int amdgpu_vce_cs_reloc(struct amdgpu_cs_parser *p, uint32_t ib_idx,\r\nint lo, int hi, unsigned size, uint32_t index)\r\n{\r\nstruct amdgpu_bo_va_mapping *mapping;\r\nstruct amdgpu_ib *ib = &p->ibs[ib_idx];\r\nstruct amdgpu_bo *bo;\r\nuint64_t addr;\r\nif (index == 0xffffffff)\r\nindex = 0;\r\naddr = ((uint64_t)amdgpu_get_ib_value(p, ib_idx, lo)) |\r\n((uint64_t)amdgpu_get_ib_value(p, ib_idx, hi)) << 32;\r\naddr += ((uint64_t)size) * ((uint64_t)index);\r\nmapping = amdgpu_cs_find_mapping(p, addr, &bo);\r\nif (mapping == NULL) {\r\nDRM_ERROR("Can't find BO for addr 0x%010Lx %d %d %d %d\n",\r\naddr, lo, hi, size, index);\r\nreturn -EINVAL;\r\n}\r\nif ((addr + (uint64_t)size) >\r\n((uint64_t)mapping->it.last + 1) * AMDGPU_GPU_PAGE_SIZE) {\r\nDRM_ERROR("BO to small for addr 0x%010Lx %d %d\n",\r\naddr, lo, hi);\r\nreturn -EINVAL;\r\n}\r\naddr -= ((uint64_t)mapping->it.start) * AMDGPU_GPU_PAGE_SIZE;\r\naddr += amdgpu_bo_gpu_offset(bo);\r\naddr -= ((uint64_t)size) * ((uint64_t)index);\r\nib->ptr[lo] = addr & 0xFFFFFFFF;\r\nib->ptr[hi] = addr >> 32;\r\nreturn 0;\r\n}\r\nstatic int amdgpu_vce_validate_handle(struct amdgpu_cs_parser *p,\r\nuint32_t handle, bool *allocated)\r\n{\r\nunsigned i;\r\n*allocated = false;\r\nfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\r\nif (atomic_read(&p->adev->vce.handles[i]) == handle) {\r\nif (p->adev->vce.filp[i] != p->filp) {\r\nDRM_ERROR("VCE handle collision detected!\n");\r\nreturn -EINVAL;\r\n}\r\nreturn i;\r\n}\r\n}\r\nfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\r\nif (!atomic_cmpxchg(&p->adev->vce.handles[i], 0, handle)) {\r\np->adev->vce.filp[i] = p->filp;\r\np->adev->vce.img_size[i] = 0;\r\n*allocated = true;\r\nreturn i;\r\n}\r\n}\r\nDRM_ERROR("No more free VCE handles!\n");\r\nreturn -EINVAL;\r\n}\r\nint amdgpu_vce_ring_parse_cs(struct amdgpu_cs_parser *p, uint32_t ib_idx)\r\n{\r\nstruct amdgpu_ib *ib = &p->ibs[ib_idx];\r\nunsigned fb_idx = 0, bs_idx = 0;\r\nint session_idx = -1;\r\nbool destroyed = false;\r\nbool created = false;\r\nbool allocated = false;\r\nuint32_t tmp, handle = 0;\r\nuint32_t *size = &tmp;\r\nint i, r = 0, idx = 0;\r\namdgpu_vce_note_usage(p->adev);\r\nwhile (idx < ib->length_dw) {\r\nuint32_t len = amdgpu_get_ib_value(p, ib_idx, idx);\r\nuint32_t cmd = amdgpu_get_ib_value(p, ib_idx, idx + 1);\r\nif ((len < 8) || (len & 3)) {\r\nDRM_ERROR("invalid VCE command length (%d)!\n", len);\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nif (destroyed) {\r\nDRM_ERROR("No other command allowed after destroy!\n");\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nswitch (cmd) {\r\ncase 0x00000001:\r\nhandle = amdgpu_get_ib_value(p, ib_idx, idx + 2);\r\nsession_idx = amdgpu_vce_validate_handle(p, handle,\r\n&allocated);\r\nif (session_idx < 0)\r\nreturn session_idx;\r\nsize = &p->adev->vce.img_size[session_idx];\r\nbreak;\r\ncase 0x00000002:\r\nfb_idx = amdgpu_get_ib_value(p, ib_idx, idx + 6);\r\nbs_idx = amdgpu_get_ib_value(p, ib_idx, idx + 7);\r\nbreak;\r\ncase 0x01000001:\r\ncreated = true;\r\nif (!allocated) {\r\nDRM_ERROR("Handle already in use!\n");\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\n*size = amdgpu_get_ib_value(p, ib_idx, idx + 8) *\r\namdgpu_get_ib_value(p, ib_idx, idx + 10) *\r\n8 * 3 / 2;\r\nbreak;\r\ncase 0x04000001:\r\ncase 0x04000002:\r\ncase 0x04000005:\r\ncase 0x04000007:\r\ncase 0x04000008:\r\ncase 0x04000009:\r\ncase 0x05000002:\r\nbreak;\r\ncase 0x03000001:\r\nr = amdgpu_vce_cs_reloc(p, ib_idx, idx + 10, idx + 9,\r\n*size, 0);\r\nif (r)\r\ngoto out;\r\nr = amdgpu_vce_cs_reloc(p, ib_idx, idx + 12, idx + 11,\r\n*size / 3, 0);\r\nif (r)\r\ngoto out;\r\nbreak;\r\ncase 0x02000001:\r\ndestroyed = true;\r\nbreak;\r\ncase 0x05000001:\r\nr = amdgpu_vce_cs_reloc(p, ib_idx, idx + 3, idx + 2,\r\n*size * 2, 0);\r\nif (r)\r\ngoto out;\r\nbreak;\r\ncase 0x05000004:\r\ntmp = amdgpu_get_ib_value(p, ib_idx, idx + 4);\r\nr = amdgpu_vce_cs_reloc(p, ib_idx, idx + 3, idx + 2,\r\ntmp, bs_idx);\r\nif (r)\r\ngoto out;\r\nbreak;\r\ncase 0x05000005:\r\nr = amdgpu_vce_cs_reloc(p, ib_idx, idx + 3, idx + 2,\r\n4096, fb_idx);\r\nif (r)\r\ngoto out;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("invalid VCE command (0x%x)!\n", cmd);\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nif (session_idx == -1) {\r\nDRM_ERROR("no session command at start of IB\n");\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nidx += len / 4;\r\n}\r\nif (allocated && !created) {\r\nDRM_ERROR("New session without create command!\n");\r\nr = -ENOENT;\r\n}\r\nout:\r\nif ((!r && destroyed) || (r && allocated)) {\r\nfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i)\r\natomic_cmpxchg(&p->adev->vce.handles[i], handle, 0);\r\n}\r\nreturn r;\r\n}\r\nbool amdgpu_vce_ring_emit_semaphore(struct amdgpu_ring *ring,\r\nstruct amdgpu_semaphore *semaphore,\r\nbool emit_wait)\r\n{\r\nuint64_t addr = semaphore->gpu_addr;\r\namdgpu_ring_write(ring, VCE_CMD_SEMAPHORE);\r\namdgpu_ring_write(ring, (addr >> 3) & 0x000FFFFF);\r\namdgpu_ring_write(ring, (addr >> 23) & 0x000FFFFF);\r\namdgpu_ring_write(ring, 0x01003000 | (emit_wait ? 1 : 0));\r\nif (!emit_wait)\r\namdgpu_ring_write(ring, VCE_CMD_END);\r\nreturn true;\r\n}\r\nvoid amdgpu_vce_ring_emit_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\r\n{\r\namdgpu_ring_write(ring, VCE_CMD_IB);\r\namdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\r\namdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\r\namdgpu_ring_write(ring, ib->length_dw);\r\n}\r\nvoid amdgpu_vce_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\r\nunsigned flags)\r\n{\r\nWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\r\namdgpu_ring_write(ring, VCE_CMD_FENCE);\r\namdgpu_ring_write(ring, addr);\r\namdgpu_ring_write(ring, upper_32_bits(addr));\r\namdgpu_ring_write(ring, seq);\r\namdgpu_ring_write(ring, VCE_CMD_TRAP);\r\namdgpu_ring_write(ring, VCE_CMD_END);\r\n}\r\nint amdgpu_vce_ring_test_ring(struct amdgpu_ring *ring)\r\n{\r\nstruct amdgpu_device *adev = ring->adev;\r\nuint32_t rptr = amdgpu_ring_get_rptr(ring);\r\nunsigned i;\r\nint r;\r\nr = amdgpu_ring_lock(ring, 16);\r\nif (r) {\r\nDRM_ERROR("amdgpu: vce failed to lock ring %d (%d).\n",\r\nring->idx, r);\r\nreturn r;\r\n}\r\namdgpu_ring_write(ring, VCE_CMD_END);\r\namdgpu_ring_unlock_commit(ring);\r\nfor (i = 0; i < adev->usec_timeout; i++) {\r\nif (amdgpu_ring_get_rptr(ring) != rptr)\r\nbreak;\r\nDRM_UDELAY(1);\r\n}\r\nif (i < adev->usec_timeout) {\r\nDRM_INFO("ring test on %d succeeded in %d usecs\n",\r\nring->idx, i);\r\n} else {\r\nDRM_ERROR("amdgpu: ring %d test failed\n",\r\nring->idx);\r\nr = -ETIMEDOUT;\r\n}\r\nreturn r;\r\n}\r\nint amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring)\r\n{\r\nstruct fence *fence = NULL;\r\nint r;\r\nif (ring == &ring->adev->vce.ring[1])\r\nreturn 0;\r\nr = amdgpu_vce_get_create_msg(ring, 1, NULL);\r\nif (r) {\r\nDRM_ERROR("amdgpu: failed to get create msg (%d).\n", r);\r\ngoto error;\r\n}\r\nr = amdgpu_vce_get_destroy_msg(ring, 1, &fence);\r\nif (r) {\r\nDRM_ERROR("amdgpu: failed to get destroy ib (%d).\n", r);\r\ngoto error;\r\n}\r\nr = fence_wait(fence, false);\r\nif (r) {\r\nDRM_ERROR("amdgpu: fence wait failed (%d).\n", r);\r\n} else {\r\nDRM_INFO("ib test on ring %d succeeded\n", ring->idx);\r\n}\r\nerror:\r\nfence_put(fence);\r\nreturn r;\r\n}
