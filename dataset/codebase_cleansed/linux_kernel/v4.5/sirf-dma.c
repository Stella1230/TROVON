static inline\r\nstruct sirfsoc_dma_chan *dma_chan_to_sirfsoc_dma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct sirfsoc_dma_chan, chan);\r\n}\r\nstatic inline struct sirfsoc_dma *dma_chan_to_sirfsoc_dma(struct dma_chan *c)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(c);\r\nreturn container_of(schan, struct sirfsoc_dma, channels[c->chan_id]);\r\n}\r\nstatic void sirfsoc_dma_execute_hw_a7v2(struct sirfsoc_dma_desc *sdesc,\r\nint cid, int burst_mode, void __iomem *base)\r\n{\r\nif (sdesc->chain) {\r\nwritel_relaxed((sdesc->dir << SIRFSOC_DMA_DIR_CTRL_BIT_ATLAS7) |\r\n(sdesc->chain <<\r\nSIRFSOC_DMA_CHAIN_CTRL_BIT_ATLAS7) |\r\n(0x8 << SIRFSOC_DMA_TAB_NUM_ATLAS7) | 0x3,\r\nbase + SIRFSOC_DMA_CH_CTRL);\r\n} else {\r\nwritel_relaxed(sdesc->xlen, base + SIRFSOC_DMA_CH_XLEN);\r\nwritel_relaxed(sdesc->ylen, base + SIRFSOC_DMA_CH_YLEN);\r\nwritel_relaxed(sdesc->width, base + SIRFSOC_DMA_WIDTH_ATLAS7);\r\nwritel_relaxed((sdesc->width*((sdesc->ylen+1)>>1)),\r\nbase + SIRFSOC_DMA_MUL_ATLAS7);\r\nwritel_relaxed((sdesc->dir << SIRFSOC_DMA_DIR_CTRL_BIT_ATLAS7) |\r\n(sdesc->chain <<\r\nSIRFSOC_DMA_CHAIN_CTRL_BIT_ATLAS7) |\r\n0x3, base + SIRFSOC_DMA_CH_CTRL);\r\n}\r\nwritel_relaxed(sdesc->chain ? SIRFSOC_DMA_INT_END_INT_ATLAS7 :\r\n(SIRFSOC_DMA_INT_FINI_INT_ATLAS7 |\r\nSIRFSOC_DMA_INT_LOOP_INT_ATLAS7),\r\nbase + SIRFSOC_DMA_INT_EN_ATLAS7);\r\nwritel(sdesc->addr, base + SIRFSOC_DMA_CH_ADDR);\r\nif (sdesc->cyclic)\r\nwritel(0x10001, base + SIRFSOC_DMA_LOOP_CTRL_ATLAS7);\r\n}\r\nstatic void sirfsoc_dma_execute_hw_a7v1(struct sirfsoc_dma_desc *sdesc,\r\nint cid, int burst_mode, void __iomem *base)\r\n{\r\nwritel_relaxed(1, base + SIRFSOC_DMA_IOBG_SCMD_EN);\r\nwritel_relaxed((1 << cid), base + SIRFSOC_DMA_EARLY_RESP_SET);\r\nwritel_relaxed(sdesc->width, base + SIRFSOC_DMA_WIDTH_0 + cid * 4);\r\nwritel_relaxed(cid | (burst_mode << SIRFSOC_DMA_MODE_CTRL_BIT) |\r\n(sdesc->dir << SIRFSOC_DMA_DIR_CTRL_BIT),\r\nbase + cid * 0x10 + SIRFSOC_DMA_CH_CTRL);\r\nwritel_relaxed(sdesc->xlen, base + cid * 0x10 + SIRFSOC_DMA_CH_XLEN);\r\nwritel_relaxed(sdesc->ylen, base + cid * 0x10 + SIRFSOC_DMA_CH_YLEN);\r\nwritel_relaxed(readl_relaxed(base + SIRFSOC_DMA_INT_EN) |\r\n(1 << cid), base + SIRFSOC_DMA_INT_EN);\r\nwritel(sdesc->addr >> 2, base + cid * 0x10 + SIRFSOC_DMA_CH_ADDR);\r\nif (sdesc->cyclic) {\r\nwritel((1 << cid) | 1 << (cid + 16) |\r\nreadl_relaxed(base + SIRFSOC_DMA_CH_LOOP_CTRL_ATLAS7),\r\nbase + SIRFSOC_DMA_CH_LOOP_CTRL_ATLAS7);\r\n}\r\n}\r\nstatic void sirfsoc_dma_execute_hw_a6(struct sirfsoc_dma_desc *sdesc,\r\nint cid, int burst_mode, void __iomem *base)\r\n{\r\nwritel_relaxed(sdesc->width, base + SIRFSOC_DMA_WIDTH_0 + cid * 4);\r\nwritel_relaxed(cid | (burst_mode << SIRFSOC_DMA_MODE_CTRL_BIT) |\r\n(sdesc->dir << SIRFSOC_DMA_DIR_CTRL_BIT),\r\nbase + cid * 0x10 + SIRFSOC_DMA_CH_CTRL);\r\nwritel_relaxed(sdesc->xlen, base + cid * 0x10 + SIRFSOC_DMA_CH_XLEN);\r\nwritel_relaxed(sdesc->ylen, base + cid * 0x10 + SIRFSOC_DMA_CH_YLEN);\r\nwritel_relaxed(readl_relaxed(base + SIRFSOC_DMA_INT_EN) |\r\n(1 << cid), base + SIRFSOC_DMA_INT_EN);\r\nwritel(sdesc->addr >> 2, base + cid * 0x10 + SIRFSOC_DMA_CH_ADDR);\r\nif (sdesc->cyclic) {\r\nwritel((1 << cid) | 1 << (cid + 16) |\r\nreadl_relaxed(base + SIRFSOC_DMA_CH_LOOP_CTRL),\r\nbase + SIRFSOC_DMA_CH_LOOP_CTRL);\r\n}\r\n}\r\nstatic void sirfsoc_dma_execute(struct sirfsoc_dma_chan *schan)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(&schan->chan);\r\nint cid = schan->chan.chan_id;\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nvoid __iomem *base;\r\nbase = sdma->base;\r\nsdesc = list_first_entry(&schan->queued, struct sirfsoc_dma_desc,\r\nnode);\r\nlist_move_tail(&sdesc->node, &schan->active);\r\nif (sdma->type == SIRFSOC_DMA_VER_A7V2)\r\ncid = 0;\r\nsdma->exec_desc(sdesc, cid, schan->mode, base);\r\nif (sdesc->cyclic)\r\nschan->happened_cyclic = schan->completed_cyclic = 0;\r\n}\r\nstatic irqreturn_t sirfsoc_dma_irq(int irq, void *data)\r\n{\r\nstruct sirfsoc_dma *sdma = data;\r\nstruct sirfsoc_dma_chan *schan;\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nu32 is;\r\nbool chain;\r\nint ch;\r\nvoid __iomem *reg;\r\nswitch (sdma->type) {\r\ncase SIRFSOC_DMA_VER_A6:\r\ncase SIRFSOC_DMA_VER_A7V1:\r\nis = readl(sdma->base + SIRFSOC_DMA_CH_INT);\r\nreg = sdma->base + SIRFSOC_DMA_CH_INT;\r\nwhile ((ch = fls(is) - 1) >= 0) {\r\nis &= ~(1 << ch);\r\nwritel_relaxed(1 << ch, reg);\r\nschan = &sdma->channels[ch];\r\nspin_lock(&schan->lock);\r\nsdesc = list_first_entry(&schan->active,\r\nstruct sirfsoc_dma_desc, node);\r\nif (!sdesc->cyclic) {\r\nlist_splice_tail_init(&schan->active,\r\n&schan->completed);\r\ndma_cookie_complete(&sdesc->desc);\r\nif (!list_empty(&schan->queued))\r\nsirfsoc_dma_execute(schan);\r\n} else\r\nschan->happened_cyclic++;\r\nspin_unlock(&schan->lock);\r\n}\r\nbreak;\r\ncase SIRFSOC_DMA_VER_A7V2:\r\nis = readl(sdma->base + SIRFSOC_DMA_INT_ATLAS7);\r\nreg = sdma->base + SIRFSOC_DMA_INT_ATLAS7;\r\nwritel_relaxed(SIRFSOC_DMA_INT_ALL_ATLAS7, reg);\r\nschan = &sdma->channels[0];\r\nspin_lock(&schan->lock);\r\nsdesc = list_first_entry(&schan->active,\r\nstruct sirfsoc_dma_desc, node);\r\nif (!sdesc->cyclic) {\r\nchain = sdesc->chain;\r\nif ((chain && (is & SIRFSOC_DMA_INT_END_INT_ATLAS7)) ||\r\n(!chain &&\r\n(is & SIRFSOC_DMA_INT_FINI_INT_ATLAS7))) {\r\nlist_splice_tail_init(&schan->active,\r\n&schan->completed);\r\ndma_cookie_complete(&sdesc->desc);\r\nif (!list_empty(&schan->queued))\r\nsirfsoc_dma_execute(schan);\r\n}\r\n} else if (sdesc->cyclic && (is &\r\nSIRFSOC_DMA_INT_LOOP_INT_ATLAS7))\r\nschan->happened_cyclic++;\r\nspin_unlock(&schan->lock);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\ntasklet_schedule(&sdma->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void sirfsoc_dma_process_completed(struct sirfsoc_dma *sdma)\r\n{\r\ndma_cookie_t last_cookie = 0;\r\nstruct sirfsoc_dma_chan *schan;\r\nstruct sirfsoc_dma_desc *sdesc;\r\nstruct dma_async_tx_descriptor *desc;\r\nunsigned long flags;\r\nunsigned long happened_cyclic;\r\nLIST_HEAD(list);\r\nint i;\r\nfor (i = 0; i < sdma->dma.chancnt; i++) {\r\nschan = &sdma->channels[i];\r\nspin_lock_irqsave(&schan->lock, flags);\r\nif (!list_empty(&schan->completed)) {\r\nlist_splice_tail_init(&schan->completed, &list);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nlist_for_each_entry(sdesc, &list, node) {\r\ndesc = &sdesc->desc;\r\nif (desc->callback)\r\ndesc->callback(desc->callback_param);\r\nlast_cookie = desc->cookie;\r\ndma_run_dependencies(desc);\r\n}\r\nspin_lock_irqsave(&schan->lock, flags);\r\nlist_splice_tail_init(&list, &schan->free);\r\nschan->chan.completed_cookie = last_cookie;\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\n} else {\r\nif (list_empty(&schan->active)) {\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\ncontinue;\r\n}\r\nsdesc = list_first_entry(&schan->active,\r\nstruct sirfsoc_dma_desc, node);\r\nhappened_cyclic = schan->happened_cyclic;\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\ndesc = &sdesc->desc;\r\nwhile (happened_cyclic != schan->completed_cyclic) {\r\nif (desc->callback)\r\ndesc->callback(desc->callback_param);\r\nschan->completed_cyclic++;\r\n}\r\n}\r\n}\r\n}\r\nstatic void sirfsoc_dma_tasklet(unsigned long data)\r\n{\r\nstruct sirfsoc_dma *sdma = (void *)data;\r\nsirfsoc_dma_process_completed(sdma);\r\n}\r\nstatic dma_cookie_t sirfsoc_dma_tx_submit(struct dma_async_tx_descriptor *txd)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(txd->chan);\r\nstruct sirfsoc_dma_desc *sdesc;\r\nunsigned long flags;\r\ndma_cookie_t cookie;\r\nsdesc = container_of(txd, struct sirfsoc_dma_desc, desc);\r\nspin_lock_irqsave(&schan->lock, flags);\r\nlist_move_tail(&sdesc->node, &schan->queued);\r\ncookie = dma_cookie_assign(txd);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic int sirfsoc_dma_slave_config(struct dma_chan *chan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nunsigned long flags;\r\nif ((config->src_addr_width != DMA_SLAVE_BUSWIDTH_4_BYTES) ||\r\n(config->dst_addr_width != DMA_SLAVE_BUSWIDTH_4_BYTES))\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nschan->mode = (config->src_maxburst == 4 ? 1 : 0);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(&schan->chan);\r\nint cid = schan->chan.chan_id;\r\nunsigned long flags;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nswitch (sdma->type) {\r\ncase SIRFSOC_DMA_VER_A7V1:\r\nwritel_relaxed(1 << cid, sdma->base + SIRFSOC_DMA_INT_EN_CLR);\r\nwritel_relaxed(1 << cid, sdma->base + SIRFSOC_DMA_CH_INT);\r\nwritel_relaxed((1 << cid) | 1 << (cid + 16),\r\nsdma->base +\r\nSIRFSOC_DMA_CH_LOOP_CTRL_CLR_ATLAS7);\r\nwritel_relaxed(1 << cid, sdma->base + SIRFSOC_DMA_CH_VALID);\r\nbreak;\r\ncase SIRFSOC_DMA_VER_A7V2:\r\nwritel_relaxed(0, sdma->base + SIRFSOC_DMA_INT_EN_ATLAS7);\r\nwritel_relaxed(SIRFSOC_DMA_INT_ALL_ATLAS7,\r\nsdma->base + SIRFSOC_DMA_INT_ATLAS7);\r\nwritel_relaxed(0, sdma->base + SIRFSOC_DMA_LOOP_CTRL_ATLAS7);\r\nwritel_relaxed(0, sdma->base + SIRFSOC_DMA_VALID_ATLAS7);\r\nbreak;\r\ncase SIRFSOC_DMA_VER_A6:\r\nwritel_relaxed(readl_relaxed(sdma->base + SIRFSOC_DMA_INT_EN) &\r\n~(1 << cid), sdma->base + SIRFSOC_DMA_INT_EN);\r\nwritel_relaxed(readl_relaxed(sdma->base +\r\nSIRFSOC_DMA_CH_LOOP_CTRL) &\r\n~((1 << cid) | 1 << (cid + 16)),\r\nsdma->base + SIRFSOC_DMA_CH_LOOP_CTRL);\r\nwritel_relaxed(1 << cid, sdma->base + SIRFSOC_DMA_CH_VALID);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nlist_splice_tail_init(&schan->active, &schan->free);\r\nlist_splice_tail_init(&schan->queued, &schan->free);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_pause_chan(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(&schan->chan);\r\nint cid = schan->chan.chan_id;\r\nunsigned long flags;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nswitch (sdma->type) {\r\ncase SIRFSOC_DMA_VER_A7V1:\r\nwritel_relaxed((1 << cid) | 1 << (cid + 16),\r\nsdma->base +\r\nSIRFSOC_DMA_CH_LOOP_CTRL_CLR_ATLAS7);\r\nbreak;\r\ncase SIRFSOC_DMA_VER_A7V2:\r\nwritel_relaxed(0, sdma->base + SIRFSOC_DMA_LOOP_CTRL_ATLAS7);\r\nbreak;\r\ncase SIRFSOC_DMA_VER_A6:\r\nwritel_relaxed(readl_relaxed(sdma->base +\r\nSIRFSOC_DMA_CH_LOOP_CTRL) &\r\n~((1 << cid) | 1 << (cid + 16)),\r\nsdma->base + SIRFSOC_DMA_CH_LOOP_CTRL);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_resume_chan(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(&schan->chan);\r\nint cid = schan->chan.chan_id;\r\nunsigned long flags;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nswitch (sdma->type) {\r\ncase SIRFSOC_DMA_VER_A7V1:\r\nwritel_relaxed((1 << cid) | 1 << (cid + 16),\r\nsdma->base + SIRFSOC_DMA_CH_LOOP_CTRL_ATLAS7);\r\nbreak;\r\ncase SIRFSOC_DMA_VER_A7V2:\r\nwritel_relaxed(0x10001,\r\nsdma->base + SIRFSOC_DMA_LOOP_CTRL_ATLAS7);\r\nbreak;\r\ncase SIRFSOC_DMA_VER_A6:\r\nwritel_relaxed(readl_relaxed(sdma->base +\r\nSIRFSOC_DMA_CH_LOOP_CTRL) |\r\n((1 << cid) | 1 << (cid + 16)),\r\nsdma->base + SIRFSOC_DMA_CH_LOOP_CTRL);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(chan);\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma_desc *sdesc;\r\nunsigned long flags;\r\nLIST_HEAD(descs);\r\nint i;\r\npm_runtime_get_sync(sdma->dma.dev);\r\nfor (i = 0; i < SIRFSOC_DMA_DESCRIPTORS; i++) {\r\nsdesc = kzalloc(sizeof(*sdesc), GFP_KERNEL);\r\nif (!sdesc) {\r\ndev_notice(sdma->dma.dev, "Memory allocation error. "\r\n"Allocated only %u descriptors\n", i);\r\nbreak;\r\n}\r\ndma_async_tx_descriptor_init(&sdesc->desc, chan);\r\nsdesc->desc.flags = DMA_CTRL_ACK;\r\nsdesc->desc.tx_submit = sirfsoc_dma_tx_submit;\r\nlist_add_tail(&sdesc->node, &descs);\r\n}\r\nif (i == 0)\r\nreturn -ENOMEM;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nlist_splice_tail_init(&descs, &schan->free);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn i;\r\n}\r\nstatic void sirfsoc_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(chan);\r\nstruct sirfsoc_dma_desc *sdesc, *tmp;\r\nunsigned long flags;\r\nLIST_HEAD(descs);\r\nspin_lock_irqsave(&schan->lock, flags);\r\nBUG_ON(!list_empty(&schan->prepared));\r\nBUG_ON(!list_empty(&schan->queued));\r\nBUG_ON(!list_empty(&schan->active));\r\nBUG_ON(!list_empty(&schan->completed));\r\nlist_splice_tail_init(&schan->free, &descs);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nlist_for_each_entry_safe(sdesc, tmp, &descs, node)\r\nkfree(sdesc);\r\npm_runtime_put(sdma->dma.dev);\r\n}\r\nstatic void sirfsoc_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nif (list_empty(&schan->active) && !list_empty(&schan->queued))\r\nsirfsoc_dma_execute(schan);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\n}\r\nstatic enum dma_status\r\nsirfsoc_dma_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(chan);\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nunsigned long flags;\r\nenum dma_status ret;\r\nstruct sirfsoc_dma_desc *sdesc;\r\nint cid = schan->chan.chan_id;\r\nunsigned long dma_pos;\r\nunsigned long dma_request_bytes;\r\nunsigned long residue;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nif (list_empty(&schan->active)) {\r\nret = dma_cookie_status(chan, cookie, txstate);\r\ndma_set_residue(txstate, 0);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn ret;\r\n}\r\nsdesc = list_first_entry(&schan->active, struct sirfsoc_dma_desc, node);\r\nif (sdesc->cyclic)\r\ndma_request_bytes = (sdesc->xlen + 1) * (sdesc->ylen + 1) *\r\n(sdesc->width * SIRFSOC_DMA_WORD_LEN);\r\nelse\r\ndma_request_bytes = sdesc->xlen * SIRFSOC_DMA_WORD_LEN;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (sdma->type == SIRFSOC_DMA_VER_A7V2)\r\ncid = 0;\r\nif (sdma->type == SIRFSOC_DMA_VER_A7V2) {\r\ndma_pos = readl_relaxed(sdma->base + SIRFSOC_DMA_CUR_DATA_ADDR);\r\n} else {\r\ndma_pos = readl_relaxed(\r\nsdma->base + cid * 0x10 + SIRFSOC_DMA_CH_ADDR) << 2;\r\n}\r\nresidue = dma_request_bytes - (dma_pos - sdesc->addr);\r\ndma_set_residue(txstate, residue);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn ret;\r\n}\r\nstatic struct dma_async_tx_descriptor *sirfsoc_dma_prep_interleaved(\r\nstruct dma_chan *chan, struct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(chan);\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nunsigned long iflags;\r\nint ret;\r\nif ((xt->dir != DMA_MEM_TO_DEV) && (xt->dir != DMA_DEV_TO_MEM)) {\r\nret = -EINVAL;\r\ngoto err_dir;\r\n}\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nif (!list_empty(&schan->free)) {\r\nsdesc = list_first_entry(&schan->free, struct sirfsoc_dma_desc,\r\nnode);\r\nlist_del(&sdesc->node);\r\n}\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nif (!sdesc) {\r\nsirfsoc_dma_process_completed(sdma);\r\nret = 0;\r\ngoto no_desc;\r\n}\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nif ((xt->frame_size == 1) && (xt->numf > 0)) {\r\nsdesc->cyclic = 0;\r\nsdesc->xlen = xt->sgl[0].size / SIRFSOC_DMA_WORD_LEN;\r\nsdesc->width = (xt->sgl[0].size + xt->sgl[0].icg) /\r\nSIRFSOC_DMA_WORD_LEN;\r\nsdesc->ylen = xt->numf - 1;\r\nif (xt->dir == DMA_MEM_TO_DEV) {\r\nsdesc->addr = xt->src_start;\r\nsdesc->dir = 1;\r\n} else {\r\nsdesc->addr = xt->dst_start;\r\nsdesc->dir = 0;\r\n}\r\nlist_add_tail(&sdesc->node, &schan->prepared);\r\n} else {\r\npr_err("sirfsoc DMA Invalid xfer\n");\r\nret = -EINVAL;\r\ngoto err_xfer;\r\n}\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nreturn &sdesc->desc;\r\nerr_xfer:\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nno_desc:\r\nerr_dir:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nsirfsoc_dma_prep_cyclic(struct dma_chan *chan, dma_addr_t addr,\r\nsize_t buf_len, size_t period_len,\r\nenum dma_transfer_direction direction, unsigned long flags)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nunsigned long iflags;\r\nif (buf_len != 2 * period_len)\r\nreturn ERR_PTR(-EINVAL);\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nif (!list_empty(&schan->free)) {\r\nsdesc = list_first_entry(&schan->free, struct sirfsoc_dma_desc,\r\nnode);\r\nlist_del(&sdesc->node);\r\n}\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nif (!sdesc)\r\nreturn NULL;\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nsdesc->addr = addr;\r\nsdesc->cyclic = 1;\r\nsdesc->xlen = 0;\r\nsdesc->ylen = buf_len / SIRFSOC_DMA_WORD_LEN - 1;\r\nsdesc->width = 1;\r\nlist_add_tail(&sdesc->node, &schan->prepared);\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nreturn &sdesc->desc;\r\n}\r\nbool sirfsoc_dma_filter_id(struct dma_chan *chan, void *chan_id)\r\n{\r\nunsigned int ch_nr = (unsigned int) chan_id;\r\nif (ch_nr == chan->chan_id +\r\nchan->device->dev_id * SIRFSOC_DMA_CHANNELS)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic struct dma_chan *of_dma_sirfsoc_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct sirfsoc_dma *sdma = ofdma->of_dma_data;\r\nunsigned int request = dma_spec->args[0];\r\nif (request >= SIRFSOC_DMA_CHANNELS)\r\nreturn NULL;\r\nreturn dma_get_slave_channel(&sdma->channels[request].chan);\r\n}\r\nstatic int sirfsoc_dma_probe(struct platform_device *op)\r\n{\r\nstruct device_node *dn = op->dev.of_node;\r\nstruct device *dev = &op->dev;\r\nstruct dma_device *dma;\r\nstruct sirfsoc_dma *sdma;\r\nstruct sirfsoc_dma_chan *schan;\r\nstruct sirfsoc_dmadata *data;\r\nstruct resource res;\r\nulong regs_start, regs_size;\r\nu32 id;\r\nint ret, i;\r\nsdma = devm_kzalloc(dev, sizeof(*sdma), GFP_KERNEL);\r\nif (!sdma) {\r\ndev_err(dev, "Memory exhausted!\n");\r\nreturn -ENOMEM;\r\n}\r\ndata = (struct sirfsoc_dmadata *)\r\n(of_match_device(op->dev.driver->of_match_table,\r\n&op->dev)->data);\r\nsdma->exec_desc = data->exec;\r\nsdma->type = data->type;\r\nif (of_property_read_u32(dn, "cell-index", &id)) {\r\ndev_err(dev, "Fail to get DMAC index\n");\r\nreturn -ENODEV;\r\n}\r\nsdma->irq = irq_of_parse_and_map(dn, 0);\r\nif (sdma->irq == NO_IRQ) {\r\ndev_err(dev, "Error mapping IRQ!\n");\r\nreturn -EINVAL;\r\n}\r\nsdma->clk = devm_clk_get(dev, NULL);\r\nif (IS_ERR(sdma->clk)) {\r\ndev_err(dev, "failed to get a clock.\n");\r\nreturn PTR_ERR(sdma->clk);\r\n}\r\nret = of_address_to_resource(dn, 0, &res);\r\nif (ret) {\r\ndev_err(dev, "Error parsing memory region!\n");\r\ngoto irq_dispose;\r\n}\r\nregs_start = res.start;\r\nregs_size = resource_size(&res);\r\nsdma->base = devm_ioremap(dev, regs_start, regs_size);\r\nif (!sdma->base) {\r\ndev_err(dev, "Error mapping memory region!\n");\r\nret = -ENOMEM;\r\ngoto irq_dispose;\r\n}\r\nret = request_irq(sdma->irq, &sirfsoc_dma_irq, 0, DRV_NAME, sdma);\r\nif (ret) {\r\ndev_err(dev, "Error requesting IRQ!\n");\r\nret = -EINVAL;\r\ngoto irq_dispose;\r\n}\r\ndma = &sdma->dma;\r\ndma->dev = dev;\r\ndma->device_alloc_chan_resources = sirfsoc_dma_alloc_chan_resources;\r\ndma->device_free_chan_resources = sirfsoc_dma_free_chan_resources;\r\ndma->device_issue_pending = sirfsoc_dma_issue_pending;\r\ndma->device_config = sirfsoc_dma_slave_config;\r\ndma->device_pause = sirfsoc_dma_pause_chan;\r\ndma->device_resume = sirfsoc_dma_resume_chan;\r\ndma->device_terminate_all = sirfsoc_dma_terminate_all;\r\ndma->device_tx_status = sirfsoc_dma_tx_status;\r\ndma->device_prep_interleaved_dma = sirfsoc_dma_prep_interleaved;\r\ndma->device_prep_dma_cyclic = sirfsoc_dma_prep_cyclic;\r\ndma->src_addr_widths = SIRFSOC_DMA_BUSWIDTHS;\r\ndma->dst_addr_widths = SIRFSOC_DMA_BUSWIDTHS;\r\ndma->directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nINIT_LIST_HEAD(&dma->channels);\r\ndma_cap_set(DMA_SLAVE, dma->cap_mask);\r\ndma_cap_set(DMA_CYCLIC, dma->cap_mask);\r\ndma_cap_set(DMA_INTERLEAVE, dma->cap_mask);\r\ndma_cap_set(DMA_PRIVATE, dma->cap_mask);\r\nfor (i = 0; i < SIRFSOC_DMA_CHANNELS; i++) {\r\nschan = &sdma->channels[i];\r\nschan->chan.device = dma;\r\ndma_cookie_init(&schan->chan);\r\nINIT_LIST_HEAD(&schan->free);\r\nINIT_LIST_HEAD(&schan->prepared);\r\nINIT_LIST_HEAD(&schan->queued);\r\nINIT_LIST_HEAD(&schan->active);\r\nINIT_LIST_HEAD(&schan->completed);\r\nspin_lock_init(&schan->lock);\r\nlist_add_tail(&schan->chan.device_node, &dma->channels);\r\n}\r\ntasklet_init(&sdma->tasklet, sirfsoc_dma_tasklet, (unsigned long)sdma);\r\ndev_set_drvdata(dev, sdma);\r\nret = dma_async_device_register(dma);\r\nif (ret)\r\ngoto free_irq;\r\nret = of_dma_controller_register(dn, of_dma_sirfsoc_xlate, sdma);\r\nif (ret) {\r\ndev_err(dev, "failed to register DMA controller\n");\r\ngoto unreg_dma_dev;\r\n}\r\npm_runtime_enable(&op->dev);\r\ndev_info(dev, "initialized SIRFSOC DMAC driver\n");\r\nreturn 0;\r\nunreg_dma_dev:\r\ndma_async_device_unregister(dma);\r\nfree_irq:\r\nfree_irq(sdma->irq, sdma);\r\nirq_dispose:\r\nirq_dispose_mapping(sdma->irq);\r\nreturn ret;\r\n}\r\nstatic int sirfsoc_dma_remove(struct platform_device *op)\r\n{\r\nstruct device *dev = &op->dev;\r\nstruct sirfsoc_dma *sdma = dev_get_drvdata(dev);\r\nof_dma_controller_free(op->dev.of_node);\r\ndma_async_device_unregister(&sdma->dma);\r\nfree_irq(sdma->irq, sdma);\r\nirq_dispose_mapping(sdma->irq);\r\npm_runtime_disable(&op->dev);\r\nif (!pm_runtime_status_suspended(&op->dev))\r\nsirfsoc_dma_runtime_suspend(&op->dev);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_runtime_suspend(struct device *dev)\r\n{\r\nstruct sirfsoc_dma *sdma = dev_get_drvdata(dev);\r\nclk_disable_unprepare(sdma->clk);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_runtime_resume(struct device *dev)\r\n{\r\nstruct sirfsoc_dma *sdma = dev_get_drvdata(dev);\r\nint ret;\r\nret = clk_prepare_enable(sdma->clk);\r\nif (ret < 0) {\r\ndev_err(dev, "clk_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_pm_suspend(struct device *dev)\r\n{\r\nstruct sirfsoc_dma *sdma = dev_get_drvdata(dev);\r\nstruct sirfsoc_dma_regs *save = &sdma->regs_save;\r\nstruct sirfsoc_dma_desc *sdesc;\r\nstruct sirfsoc_dma_chan *schan;\r\nint ch;\r\nint ret;\r\nint count;\r\nu32 int_offset;\r\nif (pm_runtime_status_suspended(dev)) {\r\nret = sirfsoc_dma_runtime_resume(dev);\r\nif (ret < 0)\r\nreturn ret;\r\n}\r\nif (sdma->type == SIRFSOC_DMA_VER_A7V2) {\r\ncount = 1;\r\nint_offset = SIRFSOC_DMA_INT_EN_ATLAS7;\r\n} else {\r\ncount = SIRFSOC_DMA_CHANNELS;\r\nint_offset = SIRFSOC_DMA_INT_EN;\r\n}\r\nfor (ch = 0; ch < count; ch++) {\r\nschan = &sdma->channels[ch];\r\nif (list_empty(&schan->active))\r\ncontinue;\r\nsdesc = list_first_entry(&schan->active,\r\nstruct sirfsoc_dma_desc,\r\nnode);\r\nsave->ctrl[ch] = readl_relaxed(sdma->base +\r\nch * 0x10 + SIRFSOC_DMA_CH_CTRL);\r\n}\r\nsave->interrupt_en = readl_relaxed(sdma->base + int_offset);\r\nsirfsoc_dma_runtime_suspend(dev);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_pm_resume(struct device *dev)\r\n{\r\nstruct sirfsoc_dma *sdma = dev_get_drvdata(dev);\r\nstruct sirfsoc_dma_regs *save = &sdma->regs_save;\r\nstruct sirfsoc_dma_desc *sdesc;\r\nstruct sirfsoc_dma_chan *schan;\r\nint ch;\r\nint ret;\r\nint count;\r\nu32 int_offset;\r\nu32 width_offset;\r\nret = sirfsoc_dma_runtime_resume(dev);\r\nif (ret < 0)\r\nreturn ret;\r\nif (sdma->type == SIRFSOC_DMA_VER_A7V2) {\r\ncount = 1;\r\nint_offset = SIRFSOC_DMA_INT_EN_ATLAS7;\r\nwidth_offset = SIRFSOC_DMA_WIDTH_ATLAS7;\r\n} else {\r\ncount = SIRFSOC_DMA_CHANNELS;\r\nint_offset = SIRFSOC_DMA_INT_EN;\r\nwidth_offset = SIRFSOC_DMA_WIDTH_0;\r\n}\r\nwritel_relaxed(save->interrupt_en, sdma->base + int_offset);\r\nfor (ch = 0; ch < count; ch++) {\r\nschan = &sdma->channels[ch];\r\nif (list_empty(&schan->active))\r\ncontinue;\r\nsdesc = list_first_entry(&schan->active,\r\nstruct sirfsoc_dma_desc,\r\nnode);\r\nwritel_relaxed(sdesc->width,\r\nsdma->base + width_offset + ch * 4);\r\nwritel_relaxed(sdesc->xlen,\r\nsdma->base + ch * 0x10 + SIRFSOC_DMA_CH_XLEN);\r\nwritel_relaxed(sdesc->ylen,\r\nsdma->base + ch * 0x10 + SIRFSOC_DMA_CH_YLEN);\r\nwritel_relaxed(save->ctrl[ch],\r\nsdma->base + ch * 0x10 + SIRFSOC_DMA_CH_CTRL);\r\nif (sdma->type == SIRFSOC_DMA_VER_A7V2) {\r\nwritel_relaxed(sdesc->addr,\r\nsdma->base + SIRFSOC_DMA_CH_ADDR);\r\n} else {\r\nwritel_relaxed(sdesc->addr >> 2,\r\nsdma->base + ch * 0x10 + SIRFSOC_DMA_CH_ADDR);\r\n}\r\n}\r\nif (pm_runtime_status_suspended(dev))\r\nsirfsoc_dma_runtime_suspend(dev);\r\nreturn 0;\r\n}\r\nstatic __init int sirfsoc_dma_init(void)\r\n{\r\nreturn platform_driver_register(&sirfsoc_dma_driver);\r\n}\r\nstatic void __exit sirfsoc_dma_exit(void)\r\n{\r\nplatform_driver_unregister(&sirfsoc_dma_driver);\r\n}
