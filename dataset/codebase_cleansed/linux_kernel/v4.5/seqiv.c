static void seqiv_complete2(struct skcipher_givcrypt_request *req, int err)\r\n{\r\nstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\r\nstruct crypto_ablkcipher *geniv;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nif (err)\r\ngoto out;\r\ngeniv = skcipher_givcrypt_reqtfm(req);\r\nmemcpy(req->creq.info, subreq->info, crypto_ablkcipher_ivsize(geniv));\r\nout:\r\nkfree(subreq->info);\r\n}\r\nstatic void seqiv_complete(struct crypto_async_request *base, int err)\r\n{\r\nstruct skcipher_givcrypt_request *req = base->data;\r\nseqiv_complete2(req, err);\r\nskcipher_givcrypt_complete(req, err);\r\n}\r\nstatic void seqiv_aead_encrypt_complete2(struct aead_request *req, int err)\r\n{\r\nstruct aead_request *subreq = aead_request_ctx(req);\r\nstruct crypto_aead *geniv;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nif (err)\r\ngoto out;\r\ngeniv = crypto_aead_reqtfm(req);\r\nmemcpy(req->iv, subreq->iv, crypto_aead_ivsize(geniv));\r\nout:\r\nkzfree(subreq->iv);\r\n}\r\nstatic void seqiv_aead_encrypt_complete(struct crypto_async_request *base,\r\nint err)\r\n{\r\nstruct aead_request *req = base->data;\r\nseqiv_aead_encrypt_complete2(req, err);\r\naead_request_complete(req, err);\r\n}\r\nstatic void seqiv_geniv(struct seqiv_ctx *ctx, u8 *info, u64 seq,\r\nunsigned int ivsize)\r\n{\r\nunsigned int len = ivsize;\r\nif (ivsize > sizeof(u64)) {\r\nmemset(info, 0, ivsize - sizeof(u64));\r\nlen = sizeof(u64);\r\n}\r\nseq = cpu_to_be64(seq);\r\nmemcpy(info + ivsize - len, &seq, len);\r\ncrypto_xor(info, ctx->salt, ivsize);\r\n}\r\nstatic int seqiv_givencrypt(struct skcipher_givcrypt_request *req)\r\n{\r\nstruct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);\r\nstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\r\nstruct ablkcipher_request *subreq = skcipher_givcrypt_reqctx(req);\r\ncrypto_completion_t compl;\r\nvoid *data;\r\nu8 *info;\r\nunsigned int ivsize;\r\nint err;\r\nablkcipher_request_set_tfm(subreq, skcipher_geniv_cipher(geniv));\r\ncompl = req->creq.base.complete;\r\ndata = req->creq.base.data;\r\ninfo = req->creq.info;\r\nivsize = crypto_ablkcipher_ivsize(geniv);\r\nif (unlikely(!IS_ALIGNED((unsigned long)info,\r\ncrypto_ablkcipher_alignmask(geniv) + 1))) {\r\ninfo = kmalloc(ivsize, req->creq.base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:\r\nGFP_ATOMIC);\r\nif (!info)\r\nreturn -ENOMEM;\r\ncompl = seqiv_complete;\r\ndata = req;\r\n}\r\nablkcipher_request_set_callback(subreq, req->creq.base.flags, compl,\r\ndata);\r\nablkcipher_request_set_crypt(subreq, req->creq.src, req->creq.dst,\r\nreq->creq.nbytes, info);\r\nseqiv_geniv(ctx, info, req->seq, ivsize);\r\nmemcpy(req->giv, info, ivsize);\r\nerr = crypto_ablkcipher_encrypt(subreq);\r\nif (unlikely(info != req->creq.info))\r\nseqiv_complete2(req, err);\r\nreturn err;\r\n}\r\nstatic int seqiv_aead_encrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *geniv = crypto_aead_reqtfm(req);\r\nstruct aead_geniv_ctx *ctx = crypto_aead_ctx(geniv);\r\nstruct aead_request *subreq = aead_request_ctx(req);\r\ncrypto_completion_t compl;\r\nvoid *data;\r\nu8 *info;\r\nunsigned int ivsize = 8;\r\nint err;\r\nif (req->cryptlen < ivsize)\r\nreturn -EINVAL;\r\naead_request_set_tfm(subreq, ctx->child);\r\ncompl = req->base.complete;\r\ndata = req->base.data;\r\ninfo = req->iv;\r\nif (req->src != req->dst) {\r\nstruct blkcipher_desc desc = {\r\n.tfm = ctx->null,\r\n};\r\nerr = crypto_blkcipher_encrypt(&desc, req->dst, req->src,\r\nreq->assoclen + req->cryptlen);\r\nif (err)\r\nreturn err;\r\n}\r\nif (unlikely(!IS_ALIGNED((unsigned long)info,\r\ncrypto_aead_alignmask(geniv) + 1))) {\r\ninfo = kmalloc(ivsize, req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:\r\nGFP_ATOMIC);\r\nif (!info)\r\nreturn -ENOMEM;\r\nmemcpy(info, req->iv, ivsize);\r\ncompl = seqiv_aead_encrypt_complete;\r\ndata = req;\r\n}\r\naead_request_set_callback(subreq, req->base.flags, compl, data);\r\naead_request_set_crypt(subreq, req->dst, req->dst,\r\nreq->cryptlen - ivsize, info);\r\naead_request_set_ad(subreq, req->assoclen + ivsize);\r\ncrypto_xor(info, ctx->salt, ivsize);\r\nscatterwalk_map_and_copy(info, req->dst, req->assoclen, ivsize, 1);\r\nerr = crypto_aead_encrypt(subreq);\r\nif (unlikely(info != req->iv))\r\nseqiv_aead_encrypt_complete2(req, err);\r\nreturn err;\r\n}\r\nstatic int seqiv_aead_decrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *geniv = crypto_aead_reqtfm(req);\r\nstruct aead_geniv_ctx *ctx = crypto_aead_ctx(geniv);\r\nstruct aead_request *subreq = aead_request_ctx(req);\r\ncrypto_completion_t compl;\r\nvoid *data;\r\nunsigned int ivsize = 8;\r\nif (req->cryptlen < ivsize + crypto_aead_authsize(geniv))\r\nreturn -EINVAL;\r\naead_request_set_tfm(subreq, ctx->child);\r\ncompl = req->base.complete;\r\ndata = req->base.data;\r\naead_request_set_callback(subreq, req->base.flags, compl, data);\r\naead_request_set_crypt(subreq, req->src, req->dst,\r\nreq->cryptlen - ivsize, req->iv);\r\naead_request_set_ad(subreq, req->assoclen + ivsize);\r\nscatterwalk_map_and_copy(req->iv, req->src, req->assoclen, ivsize, 0);\r\nreturn crypto_aead_decrypt(subreq);\r\n}\r\nstatic int seqiv_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_ablkcipher *geniv = __crypto_ablkcipher_cast(tfm);\r\nstruct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);\r\nint err;\r\nspin_lock_init(&ctx->lock);\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct ablkcipher_request);\r\nerr = 0;\r\nif (!crypto_get_default_rng()) {\r\ncrypto_ablkcipher_crt(geniv)->givencrypt = seqiv_givencrypt;\r\nerr = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,\r\ncrypto_ablkcipher_ivsize(geniv));\r\ncrypto_put_default_rng();\r\n}\r\nreturn err ?: skcipher_geniv_init(tfm);\r\n}\r\nstatic int seqiv_ablkcipher_create(struct crypto_template *tmpl,\r\nstruct rtattr **tb)\r\n{\r\nstruct crypto_instance *inst;\r\nint err;\r\ninst = skcipher_geniv_alloc(tmpl, tb, 0, 0);\r\nif (IS_ERR(inst))\r\nreturn PTR_ERR(inst);\r\nerr = -EINVAL;\r\nif (inst->alg.cra_ablkcipher.ivsize < sizeof(u64))\r\ngoto free_inst;\r\ninst->alg.cra_init = seqiv_init;\r\ninst->alg.cra_exit = skcipher_geniv_exit;\r\ninst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;\r\ninst->alg.cra_ctxsize += sizeof(struct seqiv_ctx);\r\ninst->alg.cra_alignmask |= __alignof__(u32) - 1;\r\nerr = crypto_register_instance(tmpl, inst);\r\nif (err)\r\ngoto free_inst;\r\nout:\r\nreturn err;\r\nfree_inst:\r\nskcipher_geniv_free(inst);\r\ngoto out;\r\n}\r\nstatic int seqiv_aead_create(struct crypto_template *tmpl, struct rtattr **tb)\r\n{\r\nstruct aead_instance *inst;\r\nstruct crypto_aead_spawn *spawn;\r\nstruct aead_alg *alg;\r\nint err;\r\ninst = aead_geniv_alloc(tmpl, tb, 0, 0);\r\nif (IS_ERR(inst))\r\nreturn PTR_ERR(inst);\r\ninst->alg.base.cra_alignmask |= __alignof__(u32) - 1;\r\nspawn = aead_instance_ctx(inst);\r\nalg = crypto_spawn_aead_alg(spawn);\r\nerr = -EINVAL;\r\nif (inst->alg.ivsize != sizeof(u64))\r\ngoto free_inst;\r\ninst->alg.encrypt = seqiv_aead_encrypt;\r\ninst->alg.decrypt = seqiv_aead_decrypt;\r\ninst->alg.init = aead_init_geniv;\r\ninst->alg.exit = aead_exit_geniv;\r\ninst->alg.base.cra_ctxsize = sizeof(struct aead_geniv_ctx);\r\ninst->alg.base.cra_ctxsize += inst->alg.ivsize;\r\nerr = aead_register_instance(tmpl, inst);\r\nif (err)\r\ngoto free_inst;\r\nout:\r\nreturn err;\r\nfree_inst:\r\naead_geniv_free(inst);\r\ngoto out;\r\n}\r\nstatic int seqiv_create(struct crypto_template *tmpl, struct rtattr **tb)\r\n{\r\nstruct crypto_attr_type *algt;\r\nint err;\r\nalgt = crypto_get_attr_type(tb);\r\nif (IS_ERR(algt))\r\nreturn PTR_ERR(algt);\r\nif ((algt->type ^ CRYPTO_ALG_TYPE_AEAD) & CRYPTO_ALG_TYPE_MASK)\r\nerr = seqiv_ablkcipher_create(tmpl, tb);\r\nelse\r\nerr = seqiv_aead_create(tmpl, tb);\r\nreturn err;\r\n}\r\nstatic void seqiv_free(struct crypto_instance *inst)\r\n{\r\nif ((inst->alg.cra_flags ^ CRYPTO_ALG_TYPE_AEAD) & CRYPTO_ALG_TYPE_MASK)\r\nskcipher_geniv_free(inst);\r\nelse\r\naead_geniv_free(aead_instance(inst));\r\n}\r\nstatic int __init seqiv_module_init(void)\r\n{\r\nreturn crypto_register_template(&seqiv_tmpl);\r\n}\r\nstatic void __exit seqiv_module_exit(void)\r\n{\r\ncrypto_unregister_template(&seqiv_tmpl);\r\n}
