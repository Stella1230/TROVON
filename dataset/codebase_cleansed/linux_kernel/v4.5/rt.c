static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)\r\n{\r\nstruct rt_bandwidth *rt_b =\r\ncontainer_of(timer, struct rt_bandwidth, rt_period_timer);\r\nint idle = 0;\r\nint overrun;\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nfor (;;) {\r\noverrun = hrtimer_forward_now(timer, rt_b->rt_period);\r\nif (!overrun)\r\nbreak;\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\nidle = do_sched_rt_period_timer(rt_b, overrun);\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\n}\r\nif (idle)\r\nrt_b->rt_period_active = 0;\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\nreturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\r\n}\r\nvoid init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)\r\n{\r\nrt_b->rt_period = ns_to_ktime(period);\r\nrt_b->rt_runtime = runtime;\r\nraw_spin_lock_init(&rt_b->rt_runtime_lock);\r\nhrtimer_init(&rt_b->rt_period_timer,\r\nCLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nrt_b->rt_period_timer.function = sched_rt_period_timer;\r\n}\r\nstatic void start_rt_bandwidth(struct rt_bandwidth *rt_b)\r\n{\r\nif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)\r\nreturn;\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nif (!rt_b->rt_period_active) {\r\nrt_b->rt_period_active = 1;\r\nhrtimer_forward_now(&rt_b->rt_period_timer, rt_b->rt_period);\r\nhrtimer_start_expires(&rt_b->rt_period_timer, HRTIMER_MODE_ABS_PINNED);\r\n}\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\n}\r\nvoid init_rt_rq(struct rt_rq *rt_rq)\r\n{\r\nstruct rt_prio_array *array;\r\nint i;\r\narray = &rt_rq->active;\r\nfor (i = 0; i < MAX_RT_PRIO; i++) {\r\nINIT_LIST_HEAD(array->queue + i);\r\n__clear_bit(i, array->bitmap);\r\n}\r\n__set_bit(MAX_RT_PRIO, array->bitmap);\r\n#if defined CONFIG_SMP\r\nrt_rq->highest_prio.curr = MAX_RT_PRIO;\r\nrt_rq->highest_prio.next = MAX_RT_PRIO;\r\nrt_rq->rt_nr_migratory = 0;\r\nrt_rq->overloaded = 0;\r\nplist_head_init(&rt_rq->pushable_tasks);\r\n#ifdef HAVE_RT_PUSH_IPI\r\nrt_rq->push_flags = 0;\r\nrt_rq->push_cpu = nr_cpu_ids;\r\nraw_spin_lock_init(&rt_rq->push_lock);\r\ninit_irq_work(&rt_rq->push_work, push_irq_work_func);\r\n#endif\r\n#endif\r\nrt_rq->rt_queued = 0;\r\nrt_rq->rt_time = 0;\r\nrt_rq->rt_throttled = 0;\r\nrt_rq->rt_runtime = 0;\r\nraw_spin_lock_init(&rt_rq->rt_runtime_lock);\r\n}\r\nstatic void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)\r\n{\r\nhrtimer_cancel(&rt_b->rt_period_timer);\r\n}\r\nstatic inline struct task_struct *rt_task_of(struct sched_rt_entity *rt_se)\r\n{\r\n#ifdef CONFIG_SCHED_DEBUG\r\nWARN_ON_ONCE(!rt_entity_is_task(rt_se));\r\n#endif\r\nreturn container_of(rt_se, struct task_struct, rt);\r\n}\r\nstatic inline struct rq *rq_of_rt_rq(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rq;\r\n}\r\nstatic inline struct rt_rq *rt_rq_of_se(struct sched_rt_entity *rt_se)\r\n{\r\nreturn rt_se->rt_rq;\r\n}\r\nstatic inline struct rq *rq_of_rt_se(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rt_rq *rt_rq = rt_se->rt_rq;\r\nreturn rt_rq->rq;\r\n}\r\nvoid free_rt_sched_group(struct task_group *tg)\r\n{\r\nint i;\r\nif (tg->rt_se)\r\ndestroy_rt_bandwidth(&tg->rt_bandwidth);\r\nfor_each_possible_cpu(i) {\r\nif (tg->rt_rq)\r\nkfree(tg->rt_rq[i]);\r\nif (tg->rt_se)\r\nkfree(tg->rt_se[i]);\r\n}\r\nkfree(tg->rt_rq);\r\nkfree(tg->rt_se);\r\n}\r\nvoid init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\r\nstruct sched_rt_entity *rt_se, int cpu,\r\nstruct sched_rt_entity *parent)\r\n{\r\nstruct rq *rq = cpu_rq(cpu);\r\nrt_rq->highest_prio.curr = MAX_RT_PRIO;\r\nrt_rq->rt_nr_boosted = 0;\r\nrt_rq->rq = rq;\r\nrt_rq->tg = tg;\r\ntg->rt_rq[cpu] = rt_rq;\r\ntg->rt_se[cpu] = rt_se;\r\nif (!rt_se)\r\nreturn;\r\nif (!parent)\r\nrt_se->rt_rq = &rq->rt;\r\nelse\r\nrt_se->rt_rq = parent->my_q;\r\nrt_se->my_q = rt_rq;\r\nrt_se->parent = parent;\r\nINIT_LIST_HEAD(&rt_se->run_list);\r\n}\r\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\r\n{\r\nstruct rt_rq *rt_rq;\r\nstruct sched_rt_entity *rt_se;\r\nint i;\r\ntg->rt_rq = kzalloc(sizeof(rt_rq) * nr_cpu_ids, GFP_KERNEL);\r\nif (!tg->rt_rq)\r\ngoto err;\r\ntg->rt_se = kzalloc(sizeof(rt_se) * nr_cpu_ids, GFP_KERNEL);\r\nif (!tg->rt_se)\r\ngoto err;\r\ninit_rt_bandwidth(&tg->rt_bandwidth,\r\nktime_to_ns(def_rt_bandwidth.rt_period), 0);\r\nfor_each_possible_cpu(i) {\r\nrt_rq = kzalloc_node(sizeof(struct rt_rq),\r\nGFP_KERNEL, cpu_to_node(i));\r\nif (!rt_rq)\r\ngoto err;\r\nrt_se = kzalloc_node(sizeof(struct sched_rt_entity),\r\nGFP_KERNEL, cpu_to_node(i));\r\nif (!rt_se)\r\ngoto err_free_rq;\r\ninit_rt_rq(rt_rq);\r\nrt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;\r\ninit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);\r\n}\r\nreturn 1;\r\nerr_free_rq:\r\nkfree(rt_rq);\r\nerr:\r\nreturn 0;\r\n}\r\nstatic inline struct task_struct *rt_task_of(struct sched_rt_entity *rt_se)\r\n{\r\nreturn container_of(rt_se, struct task_struct, rt);\r\n}\r\nstatic inline struct rq *rq_of_rt_rq(struct rt_rq *rt_rq)\r\n{\r\nreturn container_of(rt_rq, struct rq, rt);\r\n}\r\nstatic inline struct rq *rq_of_rt_se(struct sched_rt_entity *rt_se)\r\n{\r\nstruct task_struct *p = rt_task_of(rt_se);\r\nreturn task_rq(p);\r\n}\r\nstatic inline struct rt_rq *rt_rq_of_se(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rq *rq = rq_of_rt_se(rt_se);\r\nreturn &rq->rt;\r\n}\r\nvoid free_rt_sched_group(struct task_group *tg) { }\r\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\r\n{\r\nreturn 1;\r\n}\r\nstatic inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)\r\n{\r\nreturn rq->rt.highest_prio.curr > prev->prio;\r\n}\r\nstatic inline int rt_overloaded(struct rq *rq)\r\n{\r\nreturn atomic_read(&rq->rd->rto_count);\r\n}\r\nstatic inline void rt_set_overload(struct rq *rq)\r\n{\r\nif (!rq->online)\r\nreturn;\r\ncpumask_set_cpu(rq->cpu, rq->rd->rto_mask);\r\nsmp_wmb();\r\natomic_inc(&rq->rd->rto_count);\r\n}\r\nstatic inline void rt_clear_overload(struct rq *rq)\r\n{\r\nif (!rq->online)\r\nreturn;\r\natomic_dec(&rq->rd->rto_count);\r\ncpumask_clear_cpu(rq->cpu, rq->rd->rto_mask);\r\n}\r\nstatic void update_rt_migration(struct rt_rq *rt_rq)\r\n{\r\nif (rt_rq->rt_nr_migratory && rt_rq->rt_nr_total > 1) {\r\nif (!rt_rq->overloaded) {\r\nrt_set_overload(rq_of_rt_rq(rt_rq));\r\nrt_rq->overloaded = 1;\r\n}\r\n} else if (rt_rq->overloaded) {\r\nrt_clear_overload(rq_of_rt_rq(rt_rq));\r\nrt_rq->overloaded = 0;\r\n}\r\n}\r\nstatic void inc_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nstruct task_struct *p;\r\nif (!rt_entity_is_task(rt_se))\r\nreturn;\r\np = rt_task_of(rt_se);\r\nrt_rq = &rq_of_rt_rq(rt_rq)->rt;\r\nrt_rq->rt_nr_total++;\r\nif (p->nr_cpus_allowed > 1)\r\nrt_rq->rt_nr_migratory++;\r\nupdate_rt_migration(rt_rq);\r\n}\r\nstatic void dec_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nstruct task_struct *p;\r\nif (!rt_entity_is_task(rt_se))\r\nreturn;\r\np = rt_task_of(rt_se);\r\nrt_rq = &rq_of_rt_rq(rt_rq)->rt;\r\nrt_rq->rt_nr_total--;\r\nif (p->nr_cpus_allowed > 1)\r\nrt_rq->rt_nr_migratory--;\r\nupdate_rt_migration(rt_rq);\r\n}\r\nstatic inline int has_pushable_tasks(struct rq *rq)\r\n{\r\nreturn !plist_head_empty(&rq->rt.pushable_tasks);\r\n}\r\nstatic inline void queue_push_tasks(struct rq *rq)\r\n{\r\nif (!has_pushable_tasks(rq))\r\nreturn;\r\nqueue_balance_callback(rq, &per_cpu(rt_push_head, rq->cpu), push_rt_tasks);\r\n}\r\nstatic inline void queue_pull_task(struct rq *rq)\r\n{\r\nqueue_balance_callback(rq, &per_cpu(rt_pull_head, rq->cpu), pull_rt_task);\r\n}\r\nstatic void enqueue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\nplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);\r\nplist_node_init(&p->pushable_tasks, p->prio);\r\nplist_add(&p->pushable_tasks, &rq->rt.pushable_tasks);\r\nif (p->prio < rq->rt.highest_prio.next)\r\nrq->rt.highest_prio.next = p->prio;\r\n}\r\nstatic void dequeue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\nplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);\r\nif (has_pushable_tasks(rq)) {\r\np = plist_first_entry(&rq->rt.pushable_tasks,\r\nstruct task_struct, pushable_tasks);\r\nrq->rt.highest_prio.next = p->prio;\r\n} else\r\nrq->rt.highest_prio.next = MAX_RT_PRIO;\r\n}\r\nstatic inline void enqueue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\n}\r\nstatic inline void dequeue_pushable_task(struct rq *rq, struct task_struct *p)\r\n{\r\n}\r\nstatic inline\r\nvoid inc_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\n}\r\nstatic inline\r\nvoid dec_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\n}\r\nstatic inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)\r\n{\r\nreturn false;\r\n}\r\nstatic inline void pull_rt_task(struct rq *this_rq)\r\n{\r\n}\r\nstatic inline void queue_push_tasks(struct rq *rq)\r\n{\r\n}\r\nstatic inline int on_rt_rq(struct sched_rt_entity *rt_se)\r\n{\r\nreturn !list_empty(&rt_se->run_list);\r\n}\r\nstatic inline u64 sched_rt_runtime(struct rt_rq *rt_rq)\r\n{\r\nif (!rt_rq->tg)\r\nreturn RUNTIME_INF;\r\nreturn rt_rq->rt_runtime;\r\n}\r\nstatic inline u64 sched_rt_period(struct rt_rq *rt_rq)\r\n{\r\nreturn ktime_to_ns(rt_rq->tg->rt_bandwidth.rt_period);\r\n}\r\nstatic inline struct task_group *next_task_group(struct task_group *tg)\r\n{\r\ndo {\r\ntg = list_entry_rcu(tg->list.next,\r\ntypeof(struct task_group), list);\r\n} while (&tg->list != &task_groups && task_group_is_autogroup(tg));\r\nif (&tg->list == &task_groups)\r\ntg = NULL;\r\nreturn tg;\r\n}\r\nstatic inline struct rt_rq *group_rt_rq(struct sched_rt_entity *rt_se)\r\n{\r\nreturn rt_se->my_q;\r\n}\r\nstatic void sched_rt_rq_enqueue(struct rt_rq *rt_rq)\r\n{\r\nstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nstruct sched_rt_entity *rt_se;\r\nint cpu = cpu_of(rq);\r\nrt_se = rt_rq->tg->rt_se[cpu];\r\nif (rt_rq->rt_nr_running) {\r\nif (!rt_se)\r\nenqueue_top_rt_rq(rt_rq);\r\nelse if (!on_rt_rq(rt_se))\r\nenqueue_rt_entity(rt_se, false);\r\nif (rt_rq->highest_prio.curr < curr->prio)\r\nresched_curr(rq);\r\n}\r\n}\r\nstatic void sched_rt_rq_dequeue(struct rt_rq *rt_rq)\r\n{\r\nstruct sched_rt_entity *rt_se;\r\nint cpu = cpu_of(rq_of_rt_rq(rt_rq));\r\nrt_se = rt_rq->tg->rt_se[cpu];\r\nif (!rt_se)\r\ndequeue_top_rt_rq(rt_rq);\r\nelse if (on_rt_rq(rt_se))\r\ndequeue_rt_entity(rt_se);\r\n}\r\nstatic inline int rt_rq_throttled(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;\r\n}\r\nstatic int rt_se_boosted(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rt_rq *rt_rq = group_rt_rq(rt_se);\r\nstruct task_struct *p;\r\nif (rt_rq)\r\nreturn !!rt_rq->rt_nr_boosted;\r\np = rt_task_of(rt_se);\r\nreturn p->prio != p->normal_prio;\r\n}\r\nstatic inline const struct cpumask *sched_rt_period_mask(void)\r\n{\r\nreturn this_rq()->rd->span;\r\n}\r\nstatic inline const struct cpumask *sched_rt_period_mask(void)\r\n{\r\nreturn cpu_online_mask;\r\n}\r\nstatic inline\r\nstruct rt_rq *sched_rt_period_rt_rq(struct rt_bandwidth *rt_b, int cpu)\r\n{\r\nreturn container_of(rt_b, struct task_group, rt_bandwidth)->rt_rq[cpu];\r\n}\r\nstatic inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)\r\n{\r\nreturn &rt_rq->tg->rt_bandwidth;\r\n}\r\nstatic inline u64 sched_rt_runtime(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rt_runtime;\r\n}\r\nstatic inline u64 sched_rt_period(struct rt_rq *rt_rq)\r\n{\r\nreturn ktime_to_ns(def_rt_bandwidth.rt_period);\r\n}\r\nstatic inline struct rt_rq *group_rt_rq(struct sched_rt_entity *rt_se)\r\n{\r\nreturn NULL;\r\n}\r\nstatic inline void sched_rt_rq_enqueue(struct rt_rq *rt_rq)\r\n{\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nif (!rt_rq->rt_nr_running)\r\nreturn;\r\nenqueue_top_rt_rq(rt_rq);\r\nresched_curr(rq);\r\n}\r\nstatic inline void sched_rt_rq_dequeue(struct rt_rq *rt_rq)\r\n{\r\ndequeue_top_rt_rq(rt_rq);\r\n}\r\nstatic inline int rt_rq_throttled(struct rt_rq *rt_rq)\r\n{\r\nreturn rt_rq->rt_throttled;\r\n}\r\nstatic inline const struct cpumask *sched_rt_period_mask(void)\r\n{\r\nreturn cpu_online_mask;\r\n}\r\nstatic inline\r\nstruct rt_rq *sched_rt_period_rt_rq(struct rt_bandwidth *rt_b, int cpu)\r\n{\r\nreturn &cpu_rq(cpu)->rt;\r\n}\r\nstatic inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)\r\n{\r\nreturn &def_rt_bandwidth;\r\n}\r\nbool sched_rt_bandwidth_account(struct rt_rq *rt_rq)\r\n{\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\nreturn (hrtimer_active(&rt_b->rt_period_timer) ||\r\nrt_rq->rt_time < rt_b->rt_runtime);\r\n}\r\nstatic void do_balance_runtime(struct rt_rq *rt_rq)\r\n{\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\nstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;\r\nint i, weight;\r\nu64 rt_period;\r\nweight = cpumask_weight(rd->span);\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nrt_period = ktime_to_ns(rt_b->rt_period);\r\nfor_each_cpu(i, rd->span) {\r\nstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);\r\ns64 diff;\r\nif (iter == rt_rq)\r\ncontinue;\r\nraw_spin_lock(&iter->rt_runtime_lock);\r\nif (iter->rt_runtime == RUNTIME_INF)\r\ngoto next;\r\ndiff = iter->rt_runtime - iter->rt_time;\r\nif (diff > 0) {\r\ndiff = div_u64((u64)diff, weight);\r\nif (rt_rq->rt_runtime + diff > rt_period)\r\ndiff = rt_period - rt_rq->rt_runtime;\r\niter->rt_runtime -= diff;\r\nrt_rq->rt_runtime += diff;\r\nif (rt_rq->rt_runtime == rt_period) {\r\nraw_spin_unlock(&iter->rt_runtime_lock);\r\nbreak;\r\n}\r\n}\r\nnext:\r\nraw_spin_unlock(&iter->rt_runtime_lock);\r\n}\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\n}\r\nstatic void __disable_runtime(struct rq *rq)\r\n{\r\nstruct root_domain *rd = rq->rd;\r\nrt_rq_iter_t iter;\r\nstruct rt_rq *rt_rq;\r\nif (unlikely(!scheduler_running))\r\nreturn;\r\nfor_each_rt_rq(rt_rq, iter, rq) {\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\ns64 want;\r\nint i;\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nif (rt_rq->rt_runtime == RUNTIME_INF ||\r\nrt_rq->rt_runtime == rt_b->rt_runtime)\r\ngoto balanced;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\nwant = rt_b->rt_runtime - rt_rq->rt_runtime;\r\nfor_each_cpu(i, rd->span) {\r\nstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);\r\ns64 diff;\r\nif (iter == rt_rq || iter->rt_runtime == RUNTIME_INF)\r\ncontinue;\r\nraw_spin_lock(&iter->rt_runtime_lock);\r\nif (want > 0) {\r\ndiff = min_t(s64, iter->rt_runtime, want);\r\niter->rt_runtime -= diff;\r\nwant -= diff;\r\n} else {\r\niter->rt_runtime -= want;\r\nwant -= want;\r\n}\r\nraw_spin_unlock(&iter->rt_runtime_lock);\r\nif (!want)\r\nbreak;\r\n}\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nBUG_ON(want);\r\nbalanced:\r\nrt_rq->rt_runtime = RUNTIME_INF;\r\nrt_rq->rt_throttled = 0;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\nsched_rt_rq_enqueue(rt_rq);\r\n}\r\n}\r\nstatic void __enable_runtime(struct rq *rq)\r\n{\r\nrt_rq_iter_t iter;\r\nstruct rt_rq *rt_rq;\r\nif (unlikely(!scheduler_running))\r\nreturn;\r\nfor_each_rt_rq(rt_rq, iter, rq) {\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\nraw_spin_lock(&rt_b->rt_runtime_lock);\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nrt_rq->rt_runtime = rt_b->rt_runtime;\r\nrt_rq->rt_time = 0;\r\nrt_rq->rt_throttled = 0;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\nraw_spin_unlock(&rt_b->rt_runtime_lock);\r\n}\r\n}\r\nstatic void balance_runtime(struct rt_rq *rt_rq)\r\n{\r\nif (!sched_feat(RT_RUNTIME_SHARE))\r\nreturn;\r\nif (rt_rq->rt_time > rt_rq->rt_runtime) {\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\ndo_balance_runtime(rt_rq);\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\n}\r\n}\r\nstatic inline void balance_runtime(struct rt_rq *rt_rq) {}\r\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)\r\n{\r\nint i, idle = 1, throttled = 0;\r\nconst struct cpumask *span;\r\nspan = sched_rt_period_mask();\r\n#ifdef CONFIG_RT_GROUP_SCHED\r\nif (rt_b == &root_task_group.rt_bandwidth)\r\nspan = cpu_online_mask;\r\n#endif\r\nfor_each_cpu(i, span) {\r\nint enqueue = 0;\r\nstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nraw_spin_lock(&rq->lock);\r\nif (rt_rq->rt_time) {\r\nu64 runtime;\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nif (rt_rq->rt_throttled)\r\nbalance_runtime(rt_rq);\r\nruntime = rt_rq->rt_runtime;\r\nrt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);\r\nif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {\r\nrt_rq->rt_throttled = 0;\r\nenqueue = 1;\r\nif (rt_rq->rt_nr_running && rq->curr == rq->idle)\r\nrq_clock_skip_update(rq, false);\r\n}\r\nif (rt_rq->rt_time || rt_rq->rt_nr_running)\r\nidle = 0;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\n} else if (rt_rq->rt_nr_running) {\r\nidle = 0;\r\nif (!rt_rq_throttled(rt_rq))\r\nenqueue = 1;\r\n}\r\nif (rt_rq->rt_throttled)\r\nthrottled = 1;\r\nif (enqueue)\r\nsched_rt_rq_enqueue(rt_rq);\r\nraw_spin_unlock(&rq->lock);\r\n}\r\nif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))\r\nreturn 1;\r\nreturn idle;\r\n}\r\nstatic inline int rt_se_prio(struct sched_rt_entity *rt_se)\r\n{\r\n#ifdef CONFIG_RT_GROUP_SCHED\r\nstruct rt_rq *rt_rq = group_rt_rq(rt_se);\r\nif (rt_rq)\r\nreturn rt_rq->highest_prio.curr;\r\n#endif\r\nreturn rt_task_of(rt_se)->prio;\r\n}\r\nstatic int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)\r\n{\r\nu64 runtime = sched_rt_runtime(rt_rq);\r\nif (rt_rq->rt_throttled)\r\nreturn rt_rq_throttled(rt_rq);\r\nif (runtime >= sched_rt_period(rt_rq))\r\nreturn 0;\r\nbalance_runtime(rt_rq);\r\nruntime = sched_rt_runtime(rt_rq);\r\nif (runtime == RUNTIME_INF)\r\nreturn 0;\r\nif (rt_rq->rt_time > runtime) {\r\nstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\r\nif (likely(rt_b->rt_runtime)) {\r\nrt_rq->rt_throttled = 1;\r\nprintk_deferred_once("sched: RT throttling activated\n");\r\n} else {\r\nrt_rq->rt_time = 0;\r\n}\r\nif (rt_rq_throttled(rt_rq)) {\r\nsched_rt_rq_dequeue(rt_rq);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void update_curr_rt(struct rq *rq)\r\n{\r\nstruct task_struct *curr = rq->curr;\r\nstruct sched_rt_entity *rt_se = &curr->rt;\r\nu64 delta_exec;\r\nif (curr->sched_class != &rt_sched_class)\r\nreturn;\r\ndelta_exec = rq_clock_task(rq) - curr->se.exec_start;\r\nif (unlikely((s64)delta_exec <= 0))\r\nreturn;\r\nschedstat_set(curr->se.statistics.exec_max,\r\nmax(curr->se.statistics.exec_max, delta_exec));\r\ncurr->se.sum_exec_runtime += delta_exec;\r\naccount_group_exec_runtime(curr, delta_exec);\r\ncurr->se.exec_start = rq_clock_task(rq);\r\ncpuacct_charge(curr, delta_exec);\r\nsched_rt_avg_update(rq, delta_exec);\r\nif (!rt_bandwidth_enabled())\r\nreturn;\r\nfor_each_sched_rt_entity(rt_se) {\r\nstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\r\nif (sched_rt_runtime(rt_rq) != RUNTIME_INF) {\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nrt_rq->rt_time += delta_exec;\r\nif (sched_rt_runtime_exceeded(rt_rq))\r\nresched_curr(rq);\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\n}\r\n}\r\n}\r\nstatic void\r\ndequeue_top_rt_rq(struct rt_rq *rt_rq)\r\n{\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nBUG_ON(&rq->rt != rt_rq);\r\nif (!rt_rq->rt_queued)\r\nreturn;\r\nBUG_ON(!rq->nr_running);\r\nsub_nr_running(rq, rt_rq->rt_nr_running);\r\nrt_rq->rt_queued = 0;\r\n}\r\nstatic void\r\nenqueue_top_rt_rq(struct rt_rq *rt_rq)\r\n{\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\nBUG_ON(&rq->rt != rt_rq);\r\nif (rt_rq->rt_queued)\r\nreturn;\r\nif (rt_rq_throttled(rt_rq) || !rt_rq->rt_nr_running)\r\nreturn;\r\nadd_nr_running(rq, rt_rq->rt_nr_running);\r\nrt_rq->rt_queued = 1;\r\n}\r\nstatic void\r\ninc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)\r\n{\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\n#ifdef CONFIG_RT_GROUP_SCHED\r\nif (&rq->rt != rt_rq)\r\nreturn;\r\n#endif\r\nif (rq->online && prio < prev_prio)\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, prio);\r\n}\r\nstatic void\r\ndec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)\r\n{\r\nstruct rq *rq = rq_of_rt_rq(rt_rq);\r\n#ifdef CONFIG_RT_GROUP_SCHED\r\nif (&rq->rt != rt_rq)\r\nreturn;\r\n#endif\r\nif (rq->online && rt_rq->highest_prio.curr != prev_prio)\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, rt_rq->highest_prio.curr);\r\n}\r\nstatic inline\r\nvoid inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}\r\nstatic inline\r\nvoid dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}\r\nstatic void\r\ninc_rt_prio(struct rt_rq *rt_rq, int prio)\r\n{\r\nint prev_prio = rt_rq->highest_prio.curr;\r\nif (prio < prev_prio)\r\nrt_rq->highest_prio.curr = prio;\r\ninc_rt_prio_smp(rt_rq, prio, prev_prio);\r\n}\r\nstatic void\r\ndec_rt_prio(struct rt_rq *rt_rq, int prio)\r\n{\r\nint prev_prio = rt_rq->highest_prio.curr;\r\nif (rt_rq->rt_nr_running) {\r\nWARN_ON(prio < prev_prio);\r\nif (prio == prev_prio) {\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nrt_rq->highest_prio.curr =\r\nsched_find_first_bit(array->bitmap);\r\n}\r\n} else\r\nrt_rq->highest_prio.curr = MAX_RT_PRIO;\r\ndec_rt_prio_smp(rt_rq, prio, prev_prio);\r\n}\r\nstatic inline void inc_rt_prio(struct rt_rq *rt_rq, int prio) {}\r\nstatic inline void dec_rt_prio(struct rt_rq *rt_rq, int prio) {}\r\nstatic void\r\ninc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nif (rt_se_boosted(rt_se))\r\nrt_rq->rt_nr_boosted++;\r\nif (rt_rq->tg)\r\nstart_rt_bandwidth(&rt_rq->tg->rt_bandwidth);\r\n}\r\nstatic void\r\ndec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nif (rt_se_boosted(rt_se))\r\nrt_rq->rt_nr_boosted--;\r\nWARN_ON(!rt_rq->rt_nr_running && rt_rq->rt_nr_boosted);\r\n}\r\nstatic void\r\ninc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nstart_rt_bandwidth(&def_rt_bandwidth);\r\n}\r\nstatic inline\r\nvoid dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq) {}\r\nstatic inline\r\nunsigned int rt_se_nr_running(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rt_rq *group_rq = group_rt_rq(rt_se);\r\nif (group_rq)\r\nreturn group_rq->rt_nr_running;\r\nelse\r\nreturn 1;\r\n}\r\nstatic inline\r\nvoid inc_rt_tasks(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nint prio = rt_se_prio(rt_se);\r\nWARN_ON(!rt_prio(prio));\r\nrt_rq->rt_nr_running += rt_se_nr_running(rt_se);\r\ninc_rt_prio(rt_rq, prio);\r\ninc_rt_migration(rt_se, rt_rq);\r\ninc_rt_group(rt_se, rt_rq);\r\n}\r\nstatic inline\r\nvoid dec_rt_tasks(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\r\n{\r\nWARN_ON(!rt_prio(rt_se_prio(rt_se)));\r\nWARN_ON(!rt_rq->rt_nr_running);\r\nrt_rq->rt_nr_running -= rt_se_nr_running(rt_se);\r\ndec_rt_prio(rt_rq, rt_se_prio(rt_se));\r\ndec_rt_migration(rt_se, rt_rq);\r\ndec_rt_group(rt_se, rt_rq);\r\n}\r\nstatic void __enqueue_rt_entity(struct sched_rt_entity *rt_se, bool head)\r\n{\r\nstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nstruct rt_rq *group_rq = group_rt_rq(rt_se);\r\nstruct list_head *queue = array->queue + rt_se_prio(rt_se);\r\nif (group_rq && (rt_rq_throttled(group_rq) || !group_rq->rt_nr_running))\r\nreturn;\r\nif (head)\r\nlist_add(&rt_se->run_list, queue);\r\nelse\r\nlist_add_tail(&rt_se->run_list, queue);\r\n__set_bit(rt_se_prio(rt_se), array->bitmap);\r\ninc_rt_tasks(rt_se, rt_rq);\r\n}\r\nstatic void __dequeue_rt_entity(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nlist_del_init(&rt_se->run_list);\r\nif (list_empty(array->queue + rt_se_prio(rt_se)))\r\n__clear_bit(rt_se_prio(rt_se), array->bitmap);\r\ndec_rt_tasks(rt_se, rt_rq);\r\n}\r\nstatic void dequeue_rt_stack(struct sched_rt_entity *rt_se)\r\n{\r\nstruct sched_rt_entity *back = NULL;\r\nfor_each_sched_rt_entity(rt_se) {\r\nrt_se->back = back;\r\nback = rt_se;\r\n}\r\ndequeue_top_rt_rq(rt_rq_of_se(back));\r\nfor (rt_se = back; rt_se; rt_se = rt_se->back) {\r\nif (on_rt_rq(rt_se))\r\n__dequeue_rt_entity(rt_se);\r\n}\r\n}\r\nstatic void enqueue_rt_entity(struct sched_rt_entity *rt_se, bool head)\r\n{\r\nstruct rq *rq = rq_of_rt_se(rt_se);\r\ndequeue_rt_stack(rt_se);\r\nfor_each_sched_rt_entity(rt_se)\r\n__enqueue_rt_entity(rt_se, head);\r\nenqueue_top_rt_rq(&rq->rt);\r\n}\r\nstatic void dequeue_rt_entity(struct sched_rt_entity *rt_se)\r\n{\r\nstruct rq *rq = rq_of_rt_se(rt_se);\r\ndequeue_rt_stack(rt_se);\r\nfor_each_sched_rt_entity(rt_se) {\r\nstruct rt_rq *rt_rq = group_rt_rq(rt_se);\r\nif (rt_rq && rt_rq->rt_nr_running)\r\n__enqueue_rt_entity(rt_se, false);\r\n}\r\nenqueue_top_rt_rq(&rq->rt);\r\n}\r\nstatic void\r\nenqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nif (flags & ENQUEUE_WAKEUP)\r\nrt_se->timeout = 0;\r\nenqueue_rt_entity(rt_se, flags & ENQUEUE_HEAD);\r\nif (!task_current(rq, p) && p->nr_cpus_allowed > 1)\r\nenqueue_pushable_task(rq, p);\r\n}\r\nstatic void dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nupdate_curr_rt(rq);\r\ndequeue_rt_entity(rt_se);\r\ndequeue_pushable_task(rq, p);\r\n}\r\nstatic void\r\nrequeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)\r\n{\r\nif (on_rt_rq(rt_se)) {\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nstruct list_head *queue = array->queue + rt_se_prio(rt_se);\r\nif (head)\r\nlist_move(&rt_se->run_list, queue);\r\nelse\r\nlist_move_tail(&rt_se->run_list, queue);\r\n}\r\n}\r\nstatic void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nstruct rt_rq *rt_rq;\r\nfor_each_sched_rt_entity(rt_se) {\r\nrt_rq = rt_rq_of_se(rt_se);\r\nrequeue_rt_entity(rt_rq, rt_se, head);\r\n}\r\n}\r\nstatic void yield_task_rt(struct rq *rq)\r\n{\r\nrequeue_task_rt(rq, rq->curr, 0);\r\n}\r\nstatic int\r\nselect_task_rq_rt(struct task_struct *p, int cpu, int sd_flag, int flags)\r\n{\r\nstruct task_struct *curr;\r\nstruct rq *rq;\r\nif (sd_flag != SD_BALANCE_WAKE && sd_flag != SD_BALANCE_FORK)\r\ngoto out;\r\nrq = cpu_rq(cpu);\r\nrcu_read_lock();\r\ncurr = READ_ONCE(rq->curr);\r\nif (curr && unlikely(rt_task(curr)) &&\r\n(curr->nr_cpus_allowed < 2 ||\r\ncurr->prio <= p->prio)) {\r\nint target = find_lowest_rq(p);\r\nif (target != -1 &&\r\np->prio < cpu_rq(target)->rt.highest_prio.curr)\r\ncpu = target;\r\n}\r\nrcu_read_unlock();\r\nout:\r\nreturn cpu;\r\n}\r\nstatic void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)\r\n{\r\nif (rq->curr->nr_cpus_allowed == 1 ||\r\n!cpupri_find(&rq->rd->cpupri, rq->curr, NULL))\r\nreturn;\r\nif (p->nr_cpus_allowed != 1\r\n&& cpupri_find(&rq->rd->cpupri, p, NULL))\r\nreturn;\r\nrequeue_task_rt(rq, p, 1);\r\nresched_curr(rq);\r\n}\r\nstatic void check_preempt_curr_rt(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nif (p->prio < rq->curr->prio) {\r\nresched_curr(rq);\r\nreturn;\r\n}\r\n#ifdef CONFIG_SMP\r\nif (p->prio == rq->curr->prio && !test_tsk_need_resched(rq->curr))\r\ncheck_preempt_equal_prio(rq, p);\r\n#endif\r\n}\r\nstatic struct sched_rt_entity *pick_next_rt_entity(struct rq *rq,\r\nstruct rt_rq *rt_rq)\r\n{\r\nstruct rt_prio_array *array = &rt_rq->active;\r\nstruct sched_rt_entity *next = NULL;\r\nstruct list_head *queue;\r\nint idx;\r\nidx = sched_find_first_bit(array->bitmap);\r\nBUG_ON(idx >= MAX_RT_PRIO);\r\nqueue = array->queue + idx;\r\nnext = list_entry(queue->next, struct sched_rt_entity, run_list);\r\nreturn next;\r\n}\r\nstatic struct task_struct *_pick_next_task_rt(struct rq *rq)\r\n{\r\nstruct sched_rt_entity *rt_se;\r\nstruct task_struct *p;\r\nstruct rt_rq *rt_rq = &rq->rt;\r\ndo {\r\nrt_se = pick_next_rt_entity(rq, rt_rq);\r\nBUG_ON(!rt_se);\r\nrt_rq = group_rt_rq(rt_se);\r\n} while (rt_rq);\r\np = rt_task_of(rt_se);\r\np->se.exec_start = rq_clock_task(rq);\r\nreturn p;\r\n}\r\nstatic struct task_struct *\r\npick_next_task_rt(struct rq *rq, struct task_struct *prev)\r\n{\r\nstruct task_struct *p;\r\nstruct rt_rq *rt_rq = &rq->rt;\r\nif (need_pull_rt_task(rq, prev)) {\r\nlockdep_unpin_lock(&rq->lock);\r\npull_rt_task(rq);\r\nlockdep_pin_lock(&rq->lock);\r\nif (unlikely((rq->stop && task_on_rq_queued(rq->stop)) ||\r\nrq->dl.dl_nr_running))\r\nreturn RETRY_TASK;\r\n}\r\nif (prev->sched_class == &rt_sched_class)\r\nupdate_curr_rt(rq);\r\nif (!rt_rq->rt_queued)\r\nreturn NULL;\r\nput_prev_task(rq, prev);\r\np = _pick_next_task_rt(rq);\r\ndequeue_pushable_task(rq, p);\r\nqueue_push_tasks(rq);\r\nreturn p;\r\n}\r\nstatic void put_prev_task_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nupdate_curr_rt(rq);\r\nif (on_rt_rq(&p->rt) && p->nr_cpus_allowed > 1)\r\nenqueue_pushable_task(rq, p);\r\n}\r\nstatic int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)\r\n{\r\nif (!task_running(rq, p) &&\r\ncpumask_test_cpu(cpu, tsk_cpus_allowed(p)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic struct task_struct *pick_highest_pushable_task(struct rq *rq, int cpu)\r\n{\r\nstruct plist_head *head = &rq->rt.pushable_tasks;\r\nstruct task_struct *p;\r\nif (!has_pushable_tasks(rq))\r\nreturn NULL;\r\nplist_for_each_entry(p, head, pushable_tasks) {\r\nif (pick_rt_task(rq, p, cpu))\r\nreturn p;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int find_lowest_rq(struct task_struct *task)\r\n{\r\nstruct sched_domain *sd;\r\nstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);\r\nint this_cpu = smp_processor_id();\r\nint cpu = task_cpu(task);\r\nif (unlikely(!lowest_mask))\r\nreturn -1;\r\nif (task->nr_cpus_allowed == 1)\r\nreturn -1;\r\nif (!cpupri_find(&task_rq(task)->rd->cpupri, task, lowest_mask))\r\nreturn -1;\r\nif (cpumask_test_cpu(cpu, lowest_mask))\r\nreturn cpu;\r\nif (!cpumask_test_cpu(this_cpu, lowest_mask))\r\nthis_cpu = -1;\r\nrcu_read_lock();\r\nfor_each_domain(cpu, sd) {\r\nif (sd->flags & SD_WAKE_AFFINE) {\r\nint best_cpu;\r\nif (this_cpu != -1 &&\r\ncpumask_test_cpu(this_cpu, sched_domain_span(sd))) {\r\nrcu_read_unlock();\r\nreturn this_cpu;\r\n}\r\nbest_cpu = cpumask_first_and(lowest_mask,\r\nsched_domain_span(sd));\r\nif (best_cpu < nr_cpu_ids) {\r\nrcu_read_unlock();\r\nreturn best_cpu;\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\nif (this_cpu != -1)\r\nreturn this_cpu;\r\ncpu = cpumask_any(lowest_mask);\r\nif (cpu < nr_cpu_ids)\r\nreturn cpu;\r\nreturn -1;\r\n}\r\nstatic struct rq *find_lock_lowest_rq(struct task_struct *task, struct rq *rq)\r\n{\r\nstruct rq *lowest_rq = NULL;\r\nint tries;\r\nint cpu;\r\nfor (tries = 0; tries < RT_MAX_TRIES; tries++) {\r\ncpu = find_lowest_rq(task);\r\nif ((cpu == -1) || (cpu == rq->cpu))\r\nbreak;\r\nlowest_rq = cpu_rq(cpu);\r\nif (lowest_rq->rt.highest_prio.curr <= task->prio) {\r\nlowest_rq = NULL;\r\nbreak;\r\n}\r\nif (double_lock_balance(rq, lowest_rq)) {\r\nif (unlikely(task_rq(task) != rq ||\r\n!cpumask_test_cpu(lowest_rq->cpu,\r\ntsk_cpus_allowed(task)) ||\r\ntask_running(rq, task) ||\r\n!task_on_rq_queued(task))) {\r\ndouble_unlock_balance(rq, lowest_rq);\r\nlowest_rq = NULL;\r\nbreak;\r\n}\r\n}\r\nif (lowest_rq->rt.highest_prio.curr > task->prio)\r\nbreak;\r\ndouble_unlock_balance(rq, lowest_rq);\r\nlowest_rq = NULL;\r\n}\r\nreturn lowest_rq;\r\n}\r\nstatic struct task_struct *pick_next_pushable_task(struct rq *rq)\r\n{\r\nstruct task_struct *p;\r\nif (!has_pushable_tasks(rq))\r\nreturn NULL;\r\np = plist_first_entry(&rq->rt.pushable_tasks,\r\nstruct task_struct, pushable_tasks);\r\nBUG_ON(rq->cpu != task_cpu(p));\r\nBUG_ON(task_current(rq, p));\r\nBUG_ON(p->nr_cpus_allowed <= 1);\r\nBUG_ON(!task_on_rq_queued(p));\r\nBUG_ON(!rt_task(p));\r\nreturn p;\r\n}\r\nstatic int push_rt_task(struct rq *rq)\r\n{\r\nstruct task_struct *next_task;\r\nstruct rq *lowest_rq;\r\nint ret = 0;\r\nif (!rq->rt.overloaded)\r\nreturn 0;\r\nnext_task = pick_next_pushable_task(rq);\r\nif (!next_task)\r\nreturn 0;\r\nretry:\r\nif (unlikely(next_task == rq->curr)) {\r\nWARN_ON(1);\r\nreturn 0;\r\n}\r\nif (unlikely(next_task->prio < rq->curr->prio)) {\r\nresched_curr(rq);\r\nreturn 0;\r\n}\r\nget_task_struct(next_task);\r\nlowest_rq = find_lock_lowest_rq(next_task, rq);\r\nif (!lowest_rq) {\r\nstruct task_struct *task;\r\ntask = pick_next_pushable_task(rq);\r\nif (task_cpu(next_task) == rq->cpu && task == next_task) {\r\ngoto out;\r\n}\r\nif (!task)\r\ngoto out;\r\nput_task_struct(next_task);\r\nnext_task = task;\r\ngoto retry;\r\n}\r\ndeactivate_task(rq, next_task, 0);\r\nset_task_cpu(next_task, lowest_rq->cpu);\r\nactivate_task(lowest_rq, next_task, 0);\r\nret = 1;\r\nresched_curr(lowest_rq);\r\ndouble_unlock_balance(rq, lowest_rq);\r\nout:\r\nput_task_struct(next_task);\r\nreturn ret;\r\n}\r\nstatic void push_rt_tasks(struct rq *rq)\r\n{\r\nwhile (push_rt_task(rq))\r\n;\r\n}\r\nstatic int rto_next_cpu(struct rq *rq)\r\n{\r\nint prev_cpu = rq->rt.push_cpu;\r\nint cpu;\r\ncpu = cpumask_next(prev_cpu, rq->rd->rto_mask);\r\nif (prev_cpu < rq->cpu) {\r\nif (cpu >= rq->cpu)\r\nreturn nr_cpu_ids;\r\n} else if (cpu >= nr_cpu_ids) {\r\ncpu = cpumask_first(rq->rd->rto_mask);\r\nif (cpu >= rq->cpu)\r\nreturn nr_cpu_ids;\r\n}\r\nrq->rt.push_cpu = cpu;\r\nreturn cpu;\r\n}\r\nstatic int find_next_push_cpu(struct rq *rq)\r\n{\r\nstruct rq *next_rq;\r\nint cpu;\r\nwhile (1) {\r\ncpu = rto_next_cpu(rq);\r\nif (cpu >= nr_cpu_ids)\r\nbreak;\r\nnext_rq = cpu_rq(cpu);\r\nif (next_rq->rt.highest_prio.next < rq->rt.highest_prio.curr)\r\nbreak;\r\n}\r\nreturn cpu;\r\n}\r\nstatic void tell_cpu_to_push(struct rq *rq)\r\n{\r\nint cpu;\r\nif (rq->rt.push_flags & RT_PUSH_IPI_EXECUTING) {\r\nraw_spin_lock(&rq->rt.push_lock);\r\nif (rq->rt.push_flags & RT_PUSH_IPI_EXECUTING) {\r\nrq->rt.push_flags |= RT_PUSH_IPI_RESTART;\r\nraw_spin_unlock(&rq->rt.push_lock);\r\nreturn;\r\n}\r\nraw_spin_unlock(&rq->rt.push_lock);\r\n}\r\nrq->rt.push_cpu = rq->cpu;\r\ncpu = find_next_push_cpu(rq);\r\nif (cpu >= nr_cpu_ids)\r\nreturn;\r\nrq->rt.push_flags = RT_PUSH_IPI_EXECUTING;\r\nirq_work_queue_on(&rq->rt.push_work, cpu);\r\n}\r\nstatic void try_to_push_tasks(void *arg)\r\n{\r\nstruct rt_rq *rt_rq = arg;\r\nstruct rq *rq, *src_rq;\r\nint this_cpu;\r\nint cpu;\r\nthis_cpu = rt_rq->push_cpu;\r\nBUG_ON(this_cpu != smp_processor_id());\r\nrq = cpu_rq(this_cpu);\r\nsrc_rq = rq_of_rt_rq(rt_rq);\r\nagain:\r\nif (has_pushable_tasks(rq)) {\r\nraw_spin_lock(&rq->lock);\r\npush_rt_task(rq);\r\nraw_spin_unlock(&rq->lock);\r\n}\r\nraw_spin_lock(&rt_rq->push_lock);\r\nif (rt_rq->push_flags & RT_PUSH_IPI_RESTART) {\r\nrt_rq->push_flags &= ~RT_PUSH_IPI_RESTART;\r\nrt_rq->push_cpu = src_rq->cpu;\r\n}\r\ncpu = find_next_push_cpu(src_rq);\r\nif (cpu >= nr_cpu_ids)\r\nrt_rq->push_flags &= ~RT_PUSH_IPI_EXECUTING;\r\nraw_spin_unlock(&rt_rq->push_lock);\r\nif (cpu >= nr_cpu_ids)\r\nreturn;\r\nif (unlikely(cpu == rq->cpu))\r\ngoto again;\r\nirq_work_queue_on(&rt_rq->push_work, cpu);\r\n}\r\nstatic void push_irq_work_func(struct irq_work *work)\r\n{\r\nstruct rt_rq *rt_rq = container_of(work, struct rt_rq, push_work);\r\ntry_to_push_tasks(rt_rq);\r\n}\r\nstatic void pull_rt_task(struct rq *this_rq)\r\n{\r\nint this_cpu = this_rq->cpu, cpu;\r\nbool resched = false;\r\nstruct task_struct *p;\r\nstruct rq *src_rq;\r\nif (likely(!rt_overloaded(this_rq)))\r\nreturn;\r\nsmp_rmb();\r\n#ifdef HAVE_RT_PUSH_IPI\r\nif (sched_feat(RT_PUSH_IPI)) {\r\ntell_cpu_to_push(this_rq);\r\nreturn;\r\n}\r\n#endif\r\nfor_each_cpu(cpu, this_rq->rd->rto_mask) {\r\nif (this_cpu == cpu)\r\ncontinue;\r\nsrc_rq = cpu_rq(cpu);\r\nif (src_rq->rt.highest_prio.next >=\r\nthis_rq->rt.highest_prio.curr)\r\ncontinue;\r\ndouble_lock_balance(this_rq, src_rq);\r\np = pick_highest_pushable_task(src_rq, this_cpu);\r\nif (p && (p->prio < this_rq->rt.highest_prio.curr)) {\r\nWARN_ON(p == src_rq->curr);\r\nWARN_ON(!task_on_rq_queued(p));\r\nif (p->prio < src_rq->curr->prio)\r\ngoto skip;\r\nresched = true;\r\ndeactivate_task(src_rq, p, 0);\r\nset_task_cpu(p, this_cpu);\r\nactivate_task(this_rq, p, 0);\r\n}\r\nskip:\r\ndouble_unlock_balance(this_rq, src_rq);\r\n}\r\nif (resched)\r\nresched_curr(this_rq);\r\n}\r\nstatic void task_woken_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nif (!task_running(rq, p) &&\r\n!test_tsk_need_resched(rq->curr) &&\r\np->nr_cpus_allowed > 1 &&\r\n(dl_task(rq->curr) || rt_task(rq->curr)) &&\r\n(rq->curr->nr_cpus_allowed < 2 ||\r\nrq->curr->prio <= p->prio))\r\npush_rt_tasks(rq);\r\n}\r\nstatic void rq_online_rt(struct rq *rq)\r\n{\r\nif (rq->rt.overloaded)\r\nrt_set_overload(rq);\r\n__enable_runtime(rq);\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, rq->rt.highest_prio.curr);\r\n}\r\nstatic void rq_offline_rt(struct rq *rq)\r\n{\r\nif (rq->rt.overloaded)\r\nrt_clear_overload(rq);\r\n__disable_runtime(rq);\r\ncpupri_set(&rq->rd->cpupri, rq->cpu, CPUPRI_INVALID);\r\n}\r\nstatic void switched_from_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nif (!task_on_rq_queued(p) || rq->rt.rt_nr_running)\r\nreturn;\r\nqueue_pull_task(rq);\r\n}\r\nvoid __init init_sched_rt_class(void)\r\n{\r\nunsigned int i;\r\nfor_each_possible_cpu(i) {\r\nzalloc_cpumask_var_node(&per_cpu(local_cpu_mask, i),\r\nGFP_KERNEL, cpu_to_node(i));\r\n}\r\n}\r\nstatic void switched_to_rt(struct rq *rq, struct task_struct *p)\r\n{\r\nif (task_on_rq_queued(p) && rq->curr != p) {\r\n#ifdef CONFIG_SMP\r\nif (p->nr_cpus_allowed > 1 && rq->rt.overloaded)\r\nqueue_push_tasks(rq);\r\n#else\r\nif (p->prio < rq->curr->prio)\r\nresched_curr(rq);\r\n#endif\r\n}\r\n}\r\nstatic void\r\nprio_changed_rt(struct rq *rq, struct task_struct *p, int oldprio)\r\n{\r\nif (!task_on_rq_queued(p))\r\nreturn;\r\nif (rq->curr == p) {\r\n#ifdef CONFIG_SMP\r\nif (oldprio < p->prio)\r\nqueue_pull_task(rq);\r\nif (p->prio > rq->rt.highest_prio.curr)\r\nresched_curr(rq);\r\n#else\r\nif (oldprio < p->prio)\r\nresched_curr(rq);\r\n#endif\r\n} else {\r\nif (p->prio < rq->curr->prio)\r\nresched_curr(rq);\r\n}\r\n}\r\nstatic void watchdog(struct rq *rq, struct task_struct *p)\r\n{\r\nunsigned long soft, hard;\r\nsoft = task_rlimit(p, RLIMIT_RTTIME);\r\nhard = task_rlimit_max(p, RLIMIT_RTTIME);\r\nif (soft != RLIM_INFINITY) {\r\nunsigned long next;\r\nif (p->rt.watchdog_stamp != jiffies) {\r\np->rt.timeout++;\r\np->rt.watchdog_stamp = jiffies;\r\n}\r\nnext = DIV_ROUND_UP(min(soft, hard), USEC_PER_SEC/HZ);\r\nif (p->rt.timeout > next)\r\np->cputime_expires.sched_exp = p->se.sum_exec_runtime;\r\n}\r\n}\r\nstatic void task_tick_rt(struct rq *rq, struct task_struct *p, int queued)\r\n{\r\nstruct sched_rt_entity *rt_se = &p->rt;\r\nupdate_curr_rt(rq);\r\nwatchdog(rq, p);\r\nif (p->policy != SCHED_RR)\r\nreturn;\r\nif (--p->rt.time_slice)\r\nreturn;\r\np->rt.time_slice = sched_rr_timeslice;\r\nfor_each_sched_rt_entity(rt_se) {\r\nif (rt_se->run_list.prev != rt_se->run_list.next) {\r\nrequeue_task_rt(rq, p, 0);\r\nresched_curr(rq);\r\nreturn;\r\n}\r\n}\r\n}\r\nstatic void set_curr_task_rt(struct rq *rq)\r\n{\r\nstruct task_struct *p = rq->curr;\r\np->se.exec_start = rq_clock_task(rq);\r\ndequeue_pushable_task(rq, p);\r\n}\r\nstatic unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)\r\n{\r\nif (task->policy == SCHED_RR)\r\nreturn sched_rr_timeslice;\r\nelse\r\nreturn 0;\r\n}\r\nvoid print_rt_stats(struct seq_file *m, int cpu)\r\n{\r\nrt_rq_iter_t iter;\r\nstruct rt_rq *rt_rq;\r\nrcu_read_lock();\r\nfor_each_rt_rq(rt_rq, iter, cpu_rq(cpu))\r\nprint_rt_rq(m, cpu, rt_rq);\r\nrcu_read_unlock();\r\n}
