int qib_alloc_lkey(struct qib_mregion *mr, int dma_region)\r\n{\r\nunsigned long flags;\r\nu32 r;\r\nu32 n;\r\nint ret = 0;\r\nstruct qib_ibdev *dev = to_idev(mr->pd->device);\r\nstruct qib_lkey_table *rkt = &dev->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nif (dma_region) {\r\nstruct qib_mregion *tmr;\r\ntmr = rcu_access_pointer(dev->dma_mr);\r\nif (!tmr) {\r\nqib_get_mr(mr);\r\nrcu_assign_pointer(dev->dma_mr, mr);\r\nmr->lkey_published = 1;\r\n}\r\ngoto success;\r\n}\r\nr = rkt->next;\r\nn = r;\r\nfor (;;) {\r\nif (rkt->table[r] == NULL)\r\nbreak;\r\nr = (r + 1) & (rkt->max - 1);\r\nif (r == n)\r\ngoto bail;\r\n}\r\nrkt->next = (r + 1) & (rkt->max - 1);\r\nrkt->gen++;\r\nmr->lkey = (r << (32 - ib_qib_lkey_table_size)) |\r\n((((1 << (24 - ib_qib_lkey_table_size)) - 1) & rkt->gen)\r\n<< 8);\r\nif (mr->lkey == 0) {\r\nmr->lkey |= 1 << 8;\r\nrkt->gen++;\r\n}\r\nqib_get_mr(mr);\r\nrcu_assign_pointer(rkt->table[r], mr);\r\nmr->lkey_published = 1;\r\nsuccess:\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nout:\r\nreturn ret;\r\nbail:\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nvoid qib_free_lkey(struct qib_mregion *mr)\r\n{\r\nunsigned long flags;\r\nu32 lkey = mr->lkey;\r\nu32 r;\r\nstruct qib_ibdev *dev = to_idev(mr->pd->device);\r\nstruct qib_lkey_table *rkt = &dev->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nif (!mr->lkey_published)\r\ngoto out;\r\nif (lkey == 0)\r\nRCU_INIT_POINTER(dev->dma_mr, NULL);\r\nelse {\r\nr = lkey >> (32 - ib_qib_lkey_table_size);\r\nRCU_INIT_POINTER(rkt->table[r], NULL);\r\n}\r\nqib_put_mr(mr);\r\nmr->lkey_published = 0;\r\nout:\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\n}\r\nint qib_lkey_ok(struct qib_lkey_table *rkt, struct qib_pd *pd,\r\nstruct qib_sge *isge, struct ib_sge *sge, int acc)\r\n{\r\nstruct qib_mregion *mr;\r\nunsigned n, m;\r\nsize_t off;\r\nrcu_read_lock();\r\nif (sge->lkey == 0) {\r\nstruct qib_ibdev *dev = to_idev(pd->ibpd.device);\r\nif (pd->user)\r\ngoto bail;\r\nmr = rcu_dereference(dev->dma_mr);\r\nif (!mr)\r\ngoto bail;\r\nif (unlikely(!atomic_inc_not_zero(&mr->refcount)))\r\ngoto bail;\r\nrcu_read_unlock();\r\nisge->mr = mr;\r\nisge->vaddr = (void *) sge->addr;\r\nisge->length = sge->length;\r\nisge->sge_length = sge->length;\r\nisge->m = 0;\r\nisge->n = 0;\r\ngoto ok;\r\n}\r\nmr = rcu_dereference(\r\nrkt->table[(sge->lkey >> (32 - ib_qib_lkey_table_size))]);\r\nif (unlikely(!mr || mr->lkey != sge->lkey || mr->pd != &pd->ibpd))\r\ngoto bail;\r\noff = sge->addr - mr->user_base;\r\nif (unlikely(sge->addr < mr->user_base ||\r\noff + sge->length > mr->length ||\r\n(mr->access_flags & acc) != acc))\r\ngoto bail;\r\nif (unlikely(!atomic_inc_not_zero(&mr->refcount)))\r\ngoto bail;\r\nrcu_read_unlock();\r\noff += mr->offset;\r\nif (mr->page_shift) {\r\nsize_t entries_spanned_by_off;\r\nentries_spanned_by_off = off >> mr->page_shift;\r\noff -= (entries_spanned_by_off << mr->page_shift);\r\nm = entries_spanned_by_off/QIB_SEGSZ;\r\nn = entries_spanned_by_off%QIB_SEGSZ;\r\n} else {\r\nm = 0;\r\nn = 0;\r\nwhile (off >= mr->map[m]->segs[n].length) {\r\noff -= mr->map[m]->segs[n].length;\r\nn++;\r\nif (n >= QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\n}\r\nisge->mr = mr;\r\nisge->vaddr = mr->map[m]->segs[n].vaddr + off;\r\nisge->length = mr->map[m]->segs[n].length - off;\r\nisge->sge_length = sge->length;\r\nisge->m = m;\r\nisge->n = n;\r\nok:\r\nreturn 1;\r\nbail:\r\nrcu_read_unlock();\r\nreturn 0;\r\n}\r\nint qib_rkey_ok(struct qib_qp *qp, struct qib_sge *sge,\r\nu32 len, u64 vaddr, u32 rkey, int acc)\r\n{\r\nstruct qib_lkey_table *rkt = &to_idev(qp->ibqp.device)->lk_table;\r\nstruct qib_mregion *mr;\r\nunsigned n, m;\r\nsize_t off;\r\nrcu_read_lock();\r\nif (rkey == 0) {\r\nstruct qib_pd *pd = to_ipd(qp->ibqp.pd);\r\nstruct qib_ibdev *dev = to_idev(pd->ibpd.device);\r\nif (pd->user)\r\ngoto bail;\r\nmr = rcu_dereference(dev->dma_mr);\r\nif (!mr)\r\ngoto bail;\r\nif (unlikely(!atomic_inc_not_zero(&mr->refcount)))\r\ngoto bail;\r\nrcu_read_unlock();\r\nsge->mr = mr;\r\nsge->vaddr = (void *) vaddr;\r\nsge->length = len;\r\nsge->sge_length = len;\r\nsge->m = 0;\r\nsge->n = 0;\r\ngoto ok;\r\n}\r\nmr = rcu_dereference(\r\nrkt->table[(rkey >> (32 - ib_qib_lkey_table_size))]);\r\nif (unlikely(!mr || mr->lkey != rkey || qp->ibqp.pd != mr->pd))\r\ngoto bail;\r\noff = vaddr - mr->iova;\r\nif (unlikely(vaddr < mr->iova || off + len > mr->length ||\r\n(mr->access_flags & acc) == 0))\r\ngoto bail;\r\nif (unlikely(!atomic_inc_not_zero(&mr->refcount)))\r\ngoto bail;\r\nrcu_read_unlock();\r\noff += mr->offset;\r\nif (mr->page_shift) {\r\nsize_t entries_spanned_by_off;\r\nentries_spanned_by_off = off >> mr->page_shift;\r\noff -= (entries_spanned_by_off << mr->page_shift);\r\nm = entries_spanned_by_off/QIB_SEGSZ;\r\nn = entries_spanned_by_off%QIB_SEGSZ;\r\n} else {\r\nm = 0;\r\nn = 0;\r\nwhile (off >= mr->map[m]->segs[n].length) {\r\noff -= mr->map[m]->segs[n].length;\r\nn++;\r\nif (n >= QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\n}\r\nsge->mr = mr;\r\nsge->vaddr = mr->map[m]->segs[n].vaddr + off;\r\nsge->length = mr->map[m]->segs[n].length - off;\r\nsge->sge_length = len;\r\nsge->m = m;\r\nsge->n = n;\r\nok:\r\nreturn 1;\r\nbail:\r\nrcu_read_unlock();\r\nreturn 0;\r\n}\r\nint qib_reg_mr(struct qib_qp *qp, struct ib_reg_wr *wr)\r\n{\r\nstruct qib_lkey_table *rkt = &to_idev(qp->ibqp.device)->lk_table;\r\nstruct qib_pd *pd = to_ipd(qp->ibqp.pd);\r\nstruct qib_mr *mr = to_imr(wr->mr);\r\nstruct qib_mregion *mrg;\r\nu32 key = wr->key;\r\nunsigned i, n, m;\r\nint ret = -EINVAL;\r\nunsigned long flags;\r\nu64 *page_list;\r\nsize_t ps;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nif (pd->user || key == 0)\r\ngoto bail;\r\nmrg = rcu_dereference_protected(\r\nrkt->table[(key >> (32 - ib_qib_lkey_table_size))],\r\nlockdep_is_held(&rkt->lock));\r\nif (unlikely(mrg == NULL || qp->ibqp.pd != mrg->pd))\r\ngoto bail;\r\nif (mr->npages > mrg->max_segs)\r\ngoto bail;\r\nps = mr->ibmr.page_size;\r\nif (mr->ibmr.length > ps * mr->npages)\r\ngoto bail;\r\nmrg->user_base = mr->ibmr.iova;\r\nmrg->iova = mr->ibmr.iova;\r\nmrg->lkey = key;\r\nmrg->length = mr->ibmr.length;\r\nmrg->access_flags = wr->access;\r\npage_list = mr->pages;\r\nm = 0;\r\nn = 0;\r\nfor (i = 0; i < mr->npages; i++) {\r\nmrg->map[m]->segs[n].vaddr = (void *) page_list[i];\r\nmrg->map[m]->segs[n].length = ps;\r\nif (++n == QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nret = 0;\r\nbail:\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nreturn ret;\r\n}
