int cpumask_next_and(int n, const struct cpumask *src1p,\r\nconst struct cpumask *src2p)\r\n{\r\nwhile ((n = cpumask_next(n, src1p)) < nr_cpu_ids)\r\nif (cpumask_test_cpu(n, src2p))\r\nbreak;\r\nreturn n;\r\n}\r\nint cpumask_any_but(const struct cpumask *mask, unsigned int cpu)\r\n{\r\nunsigned int i;\r\ncpumask_check(cpu);\r\nfor_each_cpu(i, mask)\r\nif (i != cpu)\r\nbreak;\r\nreturn i;\r\n}\r\nbool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)\r\n{\r\n*mask = kmalloc_node(cpumask_size(), flags, node);\r\n#ifdef CONFIG_DEBUG_PER_CPU_MAPS\r\nif (!*mask) {\r\nprintk(KERN_ERR "=> alloc_cpumask_var: failed!\n");\r\ndump_stack();\r\n}\r\n#endif\r\nreturn *mask != NULL;\r\n}\r\nbool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)\r\n{\r\nreturn alloc_cpumask_var_node(mask, flags | __GFP_ZERO, node);\r\n}\r\nbool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)\r\n{\r\nreturn alloc_cpumask_var_node(mask, flags, NUMA_NO_NODE);\r\n}\r\nbool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)\r\n{\r\nreturn alloc_cpumask_var(mask, flags | __GFP_ZERO);\r\n}\r\nvoid __init alloc_bootmem_cpumask_var(cpumask_var_t *mask)\r\n{\r\n*mask = memblock_virt_alloc(cpumask_size(), 0);\r\n}\r\nvoid free_cpumask_var(cpumask_var_t mask)\r\n{\r\nkfree(mask);\r\n}\r\nvoid __init free_bootmem_cpumask_var(cpumask_var_t mask)\r\n{\r\nmemblock_free_early(__pa(mask), cpumask_size());\r\n}\r\nunsigned int cpumask_local_spread(unsigned int i, int node)\r\n{\r\nint cpu;\r\ni %= num_online_cpus();\r\nif (node == -1) {\r\nfor_each_cpu(cpu, cpu_online_mask)\r\nif (i-- == 0)\r\nreturn cpu;\r\n} else {\r\nfor_each_cpu_and(cpu, cpumask_of_node(node), cpu_online_mask)\r\nif (i-- == 0)\r\nreturn cpu;\r\nfor_each_cpu(cpu, cpu_online_mask) {\r\nif (cpumask_test_cpu(cpu, cpumask_of_node(node)))\r\ncontinue;\r\nif (i-- == 0)\r\nreturn cpu;\r\n}\r\n}\r\nBUG();\r\n}
