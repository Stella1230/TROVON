int iommu_dma_init(void)\r\n{\r\nreturn iova_cache_get();\r\n}\r\nint iommu_get_dma_cookie(struct iommu_domain *domain)\r\n{\r\nstruct iova_domain *iovad;\r\nif (domain->iova_cookie)\r\nreturn -EEXIST;\r\niovad = kzalloc(sizeof(*iovad), GFP_KERNEL);\r\ndomain->iova_cookie = iovad;\r\nreturn iovad ? 0 : -ENOMEM;\r\n}\r\nvoid iommu_put_dma_cookie(struct iommu_domain *domain)\r\n{\r\nstruct iova_domain *iovad = domain->iova_cookie;\r\nif (!iovad)\r\nreturn;\r\nput_iova_domain(iovad);\r\nkfree(iovad);\r\ndomain->iova_cookie = NULL;\r\n}\r\nint iommu_dma_init_domain(struct iommu_domain *domain, dma_addr_t base, u64 size)\r\n{\r\nstruct iova_domain *iovad = domain->iova_cookie;\r\nunsigned long order, base_pfn, end_pfn;\r\nif (!iovad)\r\nreturn -ENODEV;\r\norder = __ffs(domain->ops->pgsize_bitmap);\r\nbase_pfn = max_t(unsigned long, 1, base >> order);\r\nend_pfn = (base + size - 1) >> order;\r\nif (domain->geometry.force_aperture) {\r\nif (base > domain->geometry.aperture_end ||\r\nbase + size <= domain->geometry.aperture_start) {\r\npr_warn("specified DMA range outside IOMMU capability\n");\r\nreturn -EFAULT;\r\n}\r\nbase_pfn = max_t(unsigned long, base_pfn,\r\ndomain->geometry.aperture_start >> order);\r\nend_pfn = min_t(unsigned long, end_pfn,\r\ndomain->geometry.aperture_end >> order);\r\n}\r\nif (iovad->start_pfn) {\r\nif (1UL << order != iovad->granule ||\r\nbase_pfn != iovad->start_pfn ||\r\nend_pfn < iovad->dma_32bit_pfn) {\r\npr_warn("Incompatible range for DMA domain\n");\r\nreturn -EFAULT;\r\n}\r\niovad->dma_32bit_pfn = end_pfn;\r\n} else {\r\ninit_iova_domain(iovad, 1UL << order, base_pfn, end_pfn);\r\n}\r\nreturn 0;\r\n}\r\nint dma_direction_to_prot(enum dma_data_direction dir, bool coherent)\r\n{\r\nint prot = coherent ? IOMMU_CACHE : 0;\r\nswitch (dir) {\r\ncase DMA_BIDIRECTIONAL:\r\nreturn prot | IOMMU_READ | IOMMU_WRITE;\r\ncase DMA_TO_DEVICE:\r\nreturn prot | IOMMU_READ;\r\ncase DMA_FROM_DEVICE:\r\nreturn prot | IOMMU_WRITE;\r\ndefault:\r\nreturn 0;\r\n}\r\n}\r\nstatic struct iova *__alloc_iova(struct iova_domain *iovad, size_t size,\r\ndma_addr_t dma_limit)\r\n{\r\nunsigned long shift = iova_shift(iovad);\r\nunsigned long length = iova_align(iovad, size) >> shift;\r\nreturn alloc_iova(iovad, length, dma_limit >> shift, true);\r\n}\r\nstatic void __iommu_dma_unmap(struct iommu_domain *domain, dma_addr_t dma_addr)\r\n{\r\nstruct iova_domain *iovad = domain->iova_cookie;\r\nunsigned long shift = iova_shift(iovad);\r\nunsigned long pfn = dma_addr >> shift;\r\nstruct iova *iova = find_iova(iovad, pfn);\r\nsize_t size;\r\nif (WARN_ON(!iova))\r\nreturn;\r\nsize = iova_size(iova) << shift;\r\nsize -= iommu_unmap(domain, pfn << shift, size);\r\nWARN_ON(size > 0);\r\n__free_iova(iovad, iova);\r\n}\r\nstatic void __iommu_dma_free_pages(struct page **pages, int count)\r\n{\r\nwhile (count--)\r\n__free_page(pages[count]);\r\nkvfree(pages);\r\n}\r\nstatic struct page **__iommu_dma_alloc_pages(unsigned int count, gfp_t gfp)\r\n{\r\nstruct page **pages;\r\nunsigned int i = 0, array_size = count * sizeof(*pages);\r\nunsigned int order = MAX_ORDER;\r\nif (array_size <= PAGE_SIZE)\r\npages = kzalloc(array_size, GFP_KERNEL);\r\nelse\r\npages = vzalloc(array_size);\r\nif (!pages)\r\nreturn NULL;\r\ngfp |= __GFP_NOWARN | __GFP_HIGHMEM;\r\nwhile (count) {\r\nstruct page *page = NULL;\r\nint j;\r\nfor (order = min_t(unsigned int, order, __fls(count));\r\norder > 0; order--) {\r\npage = alloc_pages(gfp | __GFP_NORETRY, order);\r\nif (!page)\r\ncontinue;\r\nif (PageCompound(page)) {\r\nif (!split_huge_page(page))\r\nbreak;\r\n__free_pages(page, order);\r\n} else {\r\nsplit_page(page, order);\r\nbreak;\r\n}\r\n}\r\nif (!page)\r\npage = alloc_page(gfp);\r\nif (!page) {\r\n__iommu_dma_free_pages(pages, i);\r\nreturn NULL;\r\n}\r\nj = 1 << order;\r\ncount -= j;\r\nwhile (j--)\r\npages[i++] = page++;\r\n}\r\nreturn pages;\r\n}\r\nvoid iommu_dma_free(struct device *dev, struct page **pages, size_t size,\r\ndma_addr_t *handle)\r\n{\r\n__iommu_dma_unmap(iommu_get_domain_for_dev(dev), *handle);\r\n__iommu_dma_free_pages(pages, PAGE_ALIGN(size) >> PAGE_SHIFT);\r\n*handle = DMA_ERROR_CODE;\r\n}\r\nstruct page **iommu_dma_alloc(struct device *dev, size_t size,\r\ngfp_t gfp, int prot, dma_addr_t *handle,\r\nvoid (*flush_page)(struct device *, const void *, phys_addr_t))\r\n{\r\nstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\r\nstruct iova_domain *iovad = domain->iova_cookie;\r\nstruct iova *iova;\r\nstruct page **pages;\r\nstruct sg_table sgt;\r\ndma_addr_t dma_addr;\r\nunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\r\n*handle = DMA_ERROR_CODE;\r\npages = __iommu_dma_alloc_pages(count, gfp);\r\nif (!pages)\r\nreturn NULL;\r\niova = __alloc_iova(iovad, size, dev->coherent_dma_mask);\r\nif (!iova)\r\ngoto out_free_pages;\r\nsize = iova_align(iovad, size);\r\nif (sg_alloc_table_from_pages(&sgt, pages, count, 0, size, GFP_KERNEL))\r\ngoto out_free_iova;\r\nif (!(prot & IOMMU_CACHE)) {\r\nstruct sg_mapping_iter miter;\r\nsg_miter_start(&miter, sgt.sgl, sgt.orig_nents, SG_MITER_FROM_SG);\r\nwhile (sg_miter_next(&miter))\r\nflush_page(dev, miter.addr, page_to_phys(miter.page));\r\nsg_miter_stop(&miter);\r\n}\r\ndma_addr = iova_dma_addr(iovad, iova);\r\nif (iommu_map_sg(domain, dma_addr, sgt.sgl, sgt.orig_nents, prot)\r\n< size)\r\ngoto out_free_sg;\r\n*handle = dma_addr;\r\nsg_free_table(&sgt);\r\nreturn pages;\r\nout_free_sg:\r\nsg_free_table(&sgt);\r\nout_free_iova:\r\n__free_iova(iovad, iova);\r\nout_free_pages:\r\n__iommu_dma_free_pages(pages, count);\r\nreturn NULL;\r\n}\r\nint iommu_dma_mmap(struct page **pages, size_t size, struct vm_area_struct *vma)\r\n{\r\nunsigned long uaddr = vma->vm_start;\r\nunsigned int i, count = PAGE_ALIGN(size) >> PAGE_SHIFT;\r\nint ret = -ENXIO;\r\nfor (i = vma->vm_pgoff; i < count && uaddr < vma->vm_end; i++) {\r\nret = vm_insert_page(vma, uaddr, pages[i]);\r\nif (ret)\r\nbreak;\r\nuaddr += PAGE_SIZE;\r\n}\r\nreturn ret;\r\n}\r\ndma_addr_t iommu_dma_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size, int prot)\r\n{\r\ndma_addr_t dma_addr;\r\nstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\r\nstruct iova_domain *iovad = domain->iova_cookie;\r\nphys_addr_t phys = page_to_phys(page) + offset;\r\nsize_t iova_off = iova_offset(iovad, phys);\r\nsize_t len = iova_align(iovad, size + iova_off);\r\nstruct iova *iova = __alloc_iova(iovad, len, dma_get_mask(dev));\r\nif (!iova)\r\nreturn DMA_ERROR_CODE;\r\ndma_addr = iova_dma_addr(iovad, iova);\r\nif (iommu_map(domain, dma_addr, phys - iova_off, len, prot)) {\r\n__free_iova(iovad, iova);\r\nreturn DMA_ERROR_CODE;\r\n}\r\nreturn dma_addr + iova_off;\r\n}\r\nvoid iommu_dma_unmap_page(struct device *dev, dma_addr_t handle, size_t size,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\n__iommu_dma_unmap(iommu_get_domain_for_dev(dev), handle);\r\n}\r\nstatic int __finalise_sg(struct device *dev, struct scatterlist *sg, int nents,\r\ndma_addr_t dma_addr)\r\n{\r\nstruct scatterlist *s;\r\nint i;\r\nfor_each_sg(sg, s, nents, i) {\r\nunsigned int s_offset = sg_dma_address(s);\r\nunsigned int s_length = sg_dma_len(s);\r\nunsigned int s_dma_len = s->length;\r\ns->offset = s_offset;\r\ns->length = s_length;\r\nsg_dma_address(s) = dma_addr + s_offset;\r\ndma_addr += s_dma_len;\r\n}\r\nreturn i;\r\n}\r\nstatic void __invalidate_sg(struct scatterlist *sg, int nents)\r\n{\r\nstruct scatterlist *s;\r\nint i;\r\nfor_each_sg(sg, s, nents, i) {\r\nif (sg_dma_address(s) != DMA_ERROR_CODE)\r\ns->offset = sg_dma_address(s);\r\nif (sg_dma_len(s))\r\ns->length = sg_dma_len(s);\r\nsg_dma_address(s) = DMA_ERROR_CODE;\r\nsg_dma_len(s) = 0;\r\n}\r\n}\r\nint iommu_dma_map_sg(struct device *dev, struct scatterlist *sg,\r\nint nents, int prot)\r\n{\r\nstruct iommu_domain *domain = iommu_get_domain_for_dev(dev);\r\nstruct iova_domain *iovad = domain->iova_cookie;\r\nstruct iova *iova;\r\nstruct scatterlist *s, *prev = NULL;\r\ndma_addr_t dma_addr;\r\nsize_t iova_len = 0;\r\nint i;\r\nfor_each_sg(sg, s, nents, i) {\r\nsize_t s_offset = iova_offset(iovad, s->offset);\r\nsize_t s_length = s->length;\r\nsg_dma_address(s) = s_offset;\r\nsg_dma_len(s) = s_length;\r\ns->offset -= s_offset;\r\ns_length = iova_align(iovad, s_length + s_offset);\r\ns->length = s_length;\r\nif (prev) {\r\nsize_t pad_len = roundup_pow_of_two(s_length);\r\npad_len = (pad_len - iova_len) & (pad_len - 1);\r\nprev->length += pad_len;\r\niova_len += pad_len;\r\n}\r\niova_len += s_length;\r\nprev = s;\r\n}\r\niova = __alloc_iova(iovad, iova_len, dma_get_mask(dev));\r\nif (!iova)\r\ngoto out_restore_sg;\r\ndma_addr = iova_dma_addr(iovad, iova);\r\nif (iommu_map_sg(domain, dma_addr, sg, nents, prot) < iova_len)\r\ngoto out_free_iova;\r\nreturn __finalise_sg(dev, sg, nents, dma_addr);\r\nout_free_iova:\r\n__free_iova(iovad, iova);\r\nout_restore_sg:\r\n__invalidate_sg(sg, nents);\r\nreturn 0;\r\n}\r\nvoid iommu_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\n__iommu_dma_unmap(iommu_get_domain_for_dev(dev), sg_dma_address(sg));\r\n}\r\nint iommu_dma_supported(struct device *dev, u64 mask)\r\n{\r\nreturn 1;\r\n}\r\nint iommu_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)\r\n{\r\nreturn dma_addr == DMA_ERROR_CODE;\r\n}
