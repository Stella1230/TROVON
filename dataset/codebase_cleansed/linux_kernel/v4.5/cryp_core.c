static inline u32 uint8p_to_uint32_be(u8 *in)\r\n{\r\nu32 *data = (u32 *)in;\r\nreturn cpu_to_be32p(data);\r\n}\r\nstatic inline u8 swap_bits_in_byte(u8 b)\r\n{\r\n#define R_SHIFT_4_MASK 0xc0\r\n#define R_SHIFT_2_MASK 0x28\r\n#define R_SHIFT_1_MASK 0x1e\r\n#define L_SHIFT_4_MASK 0x03\r\n#define L_SHIFT_2_MASK 0x14\r\n#define L_SHIFT_1_MASK 0x78\r\nu8 n1;\r\nu8 n2;\r\nn1 = ((b & R_SHIFT_4_MASK) >> 4) | (b & ~(R_SHIFT_4_MASK >> 4));\r\nn1 = ((n1 & R_SHIFT_2_MASK) >> 2) | (n1 & ~(R_SHIFT_2_MASK >> 2));\r\nn1 = (n1 & R_SHIFT_1_MASK) >> 1;\r\nn2 = ((b & L_SHIFT_4_MASK) << 4) | (b & ~(L_SHIFT_4_MASK << 4));\r\nn2 = ((n2 & L_SHIFT_2_MASK) << 2) | (n2 & ~(L_SHIFT_2_MASK << 2));\r\nn2 = (n2 & L_SHIFT_1_MASK) << 1;\r\nreturn n1 | n2;\r\n}\r\nstatic inline void swap_words_in_key_and_bits_in_byte(const u8 *in,\r\nu8 *out, u32 len)\r\n{\r\nunsigned int i = 0;\r\nint j;\r\nint index = 0;\r\nj = len - BYTES_PER_WORD;\r\nwhile (j >= 0) {\r\nfor (i = 0; i < BYTES_PER_WORD; i++) {\r\nindex = len - j - BYTES_PER_WORD + i;\r\nout[j + i] =\r\nswap_bits_in_byte(in[index]);\r\n}\r\nj -= BYTES_PER_WORD;\r\n}\r\n}\r\nstatic void add_session_id(struct cryp_ctx *ctx)\r\n{\r\nif (unlikely(atomic_inc_and_test(&session_id)))\r\natomic_inc(&session_id);\r\nctx->session_id = atomic_read(&session_id);\r\n}\r\nstatic irqreturn_t cryp_interrupt_handler(int irq, void *param)\r\n{\r\nstruct cryp_ctx *ctx;\r\nint count;\r\nstruct cryp_device_data *device_data;\r\nif (param == NULL) {\r\nBUG_ON(!param);\r\nreturn IRQ_HANDLED;\r\n}\r\ndevice_data = (struct cryp_device_data *)param;\r\nctx = device_data->current_ctx;\r\nif (ctx == NULL) {\r\nBUG_ON(!ctx);\r\nreturn IRQ_HANDLED;\r\n}\r\ndev_dbg(ctx->device->dev, "[%s] (len: %d) %s, ", __func__, ctx->outlen,\r\ncryp_pending_irq_src(device_data, CRYP_IRQ_SRC_OUTPUT_FIFO) ?\r\n"out" : "in");\r\nif (cryp_pending_irq_src(device_data,\r\nCRYP_IRQ_SRC_OUTPUT_FIFO)) {\r\nif (ctx->outlen / ctx->blocksize > 0) {\r\ncount = ctx->blocksize / 4;\r\nreadsl(&device_data->base->dout, ctx->outdata, count);\r\nctx->outdata += count;\r\nctx->outlen -= count;\r\nif (ctx->outlen == 0) {\r\ncryp_disable_irq_src(device_data,\r\nCRYP_IRQ_SRC_OUTPUT_FIFO);\r\n}\r\n}\r\n} else if (cryp_pending_irq_src(device_data,\r\nCRYP_IRQ_SRC_INPUT_FIFO)) {\r\nif (ctx->datalen / ctx->blocksize > 0) {\r\ncount = ctx->blocksize / 4;\r\nwritesl(&device_data->base->din, ctx->indata, count);\r\nctx->indata += count;\r\nctx->datalen -= count;\r\nif (ctx->datalen == 0)\r\ncryp_disable_irq_src(device_data,\r\nCRYP_IRQ_SRC_INPUT_FIFO);\r\nif (ctx->config.algomode == CRYP_ALGO_AES_XTS) {\r\nCRYP_PUT_BITS(&device_data->base->cr,\r\nCRYP_START_ENABLE,\r\nCRYP_CR_START_POS,\r\nCRYP_CR_START_MASK);\r\ncryp_wait_until_done(device_data);\r\n}\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int mode_is_aes(enum cryp_algo_mode mode)\r\n{\r\nreturn CRYP_ALGO_AES_ECB == mode ||\r\nCRYP_ALGO_AES_CBC == mode ||\r\nCRYP_ALGO_AES_CTR == mode ||\r\nCRYP_ALGO_AES_XTS == mode;\r\n}\r\nstatic int cfg_iv(struct cryp_device_data *device_data, u32 left, u32 right,\r\nenum cryp_init_vector_index index)\r\n{\r\nstruct cryp_init_vector_value vector_value;\r\ndev_dbg(device_data->dev, "[%s]", __func__);\r\nvector_value.init_value_left = left;\r\nvector_value.init_value_right = right;\r\nreturn cryp_configure_init_vector(device_data,\r\nindex,\r\nvector_value);\r\n}\r\nstatic int cfg_ivs(struct cryp_device_data *device_data, struct cryp_ctx *ctx)\r\n{\r\nint i;\r\nint status = 0;\r\nint num_of_regs = ctx->blocksize / 8;\r\nu32 iv[AES_BLOCK_SIZE / 4];\r\ndev_dbg(device_data->dev, "[%s]", __func__);\r\nif (num_of_regs > 2) {\r\ndev_err(device_data->dev, "[%s] Incorrect blocksize %d",\r\n__func__, ctx->blocksize);\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < ctx->blocksize / 4; i++)\r\niv[i] = uint8p_to_uint32_be(ctx->iv + i*4);\r\nfor (i = 0; i < num_of_regs; i++) {\r\nstatus = cfg_iv(device_data, iv[i*2], iv[i*2+1],\r\n(enum cryp_init_vector_index) i);\r\nif (status != 0)\r\nreturn status;\r\n}\r\nreturn status;\r\n}\r\nstatic int set_key(struct cryp_device_data *device_data,\r\nu32 left_key,\r\nu32 right_key,\r\nenum cryp_key_reg_index index)\r\n{\r\nstruct cryp_key_value key_value;\r\nint cryp_error;\r\ndev_dbg(device_data->dev, "[%s]", __func__);\r\nkey_value.key_value_left = left_key;\r\nkey_value.key_value_right = right_key;\r\ncryp_error = cryp_configure_key_values(device_data,\r\nindex,\r\nkey_value);\r\nif (cryp_error != 0)\r\ndev_err(device_data->dev, "[%s]: "\r\n"cryp_configure_key_values() failed!", __func__);\r\nreturn cryp_error;\r\n}\r\nstatic int cfg_keys(struct cryp_ctx *ctx)\r\n{\r\nint i;\r\nint num_of_regs = ctx->keylen / 8;\r\nu32 swapped_key[CRYP_MAX_KEY_SIZE / 4];\r\nint cryp_error = 0;\r\ndev_dbg(ctx->device->dev, "[%s]", __func__);\r\nif (mode_is_aes(ctx->config.algomode)) {\r\nswap_words_in_key_and_bits_in_byte((u8 *)ctx->key,\r\n(u8 *)swapped_key,\r\nctx->keylen);\r\n} else {\r\nfor (i = 0; i < ctx->keylen / 4; i++)\r\nswapped_key[i] = uint8p_to_uint32_be(ctx->key + i*4);\r\n}\r\nfor (i = 0; i < num_of_regs; i++) {\r\ncryp_error = set_key(ctx->device,\r\n*(((u32 *)swapped_key)+i*2),\r\n*(((u32 *)swapped_key)+i*2+1),\r\n(enum cryp_key_reg_index) i);\r\nif (cryp_error != 0) {\r\ndev_err(ctx->device->dev, "[%s]: set_key() failed!",\r\n__func__);\r\nreturn cryp_error;\r\n}\r\n}\r\nreturn cryp_error;\r\n}\r\nstatic int cryp_setup_context(struct cryp_ctx *ctx,\r\nstruct cryp_device_data *device_data)\r\n{\r\nu32 control_register = CRYP_CR_DEFAULT;\r\nswitch (cryp_mode) {\r\ncase CRYP_MODE_INTERRUPT:\r\nwritel_relaxed(CRYP_IMSC_DEFAULT, &device_data->base->imsc);\r\nbreak;\r\ncase CRYP_MODE_DMA:\r\nwritel_relaxed(CRYP_DMACR_DEFAULT, &device_data->base->dmacr);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (ctx->updated == 0) {\r\ncryp_flush_inoutfifo(device_data);\r\nif (cfg_keys(ctx) != 0) {\r\ndev_err(ctx->device->dev, "[%s]: cfg_keys failed!",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nif (ctx->iv &&\r\nCRYP_ALGO_AES_ECB != ctx->config.algomode &&\r\nCRYP_ALGO_DES_ECB != ctx->config.algomode &&\r\nCRYP_ALGO_TDES_ECB != ctx->config.algomode) {\r\nif (cfg_ivs(device_data, ctx) != 0)\r\nreturn -EPERM;\r\n}\r\ncryp_set_configuration(device_data, &ctx->config,\r\n&control_register);\r\nadd_session_id(ctx);\r\n} else if (ctx->updated == 1 &&\r\nctx->session_id != atomic_read(&session_id)) {\r\ncryp_flush_inoutfifo(device_data);\r\ncryp_restore_device_context(device_data, &ctx->dev_ctx);\r\nadd_session_id(ctx);\r\ncontrol_register = ctx->dev_ctx.cr;\r\n} else\r\ncontrol_register = ctx->dev_ctx.cr;\r\nwritel(control_register |\r\n(CRYP_CRYPEN_ENABLE << CRYP_CR_CRYPEN_POS),\r\n&device_data->base->cr);\r\nreturn 0;\r\n}\r\nstatic int cryp_get_device_data(struct cryp_ctx *ctx,\r\nstruct cryp_device_data **device_data)\r\n{\r\nint ret;\r\nstruct klist_iter device_iterator;\r\nstruct klist_node *device_node;\r\nstruct cryp_device_data *local_device_data = NULL;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nret = down_interruptible(&driver_data.device_allocation);\r\nif (ret)\r\nreturn ret;\r\nklist_iter_init(&driver_data.device_list, &device_iterator);\r\ndevice_node = klist_next(&device_iterator);\r\nwhile (device_node) {\r\nlocal_device_data = container_of(device_node,\r\nstruct cryp_device_data, list_node);\r\nspin_lock(&local_device_data->ctx_lock);\r\nif (local_device_data->current_ctx) {\r\ndevice_node = klist_next(&device_iterator);\r\n} else {\r\nlocal_device_data->current_ctx = ctx;\r\nctx->device = local_device_data;\r\nspin_unlock(&local_device_data->ctx_lock);\r\nbreak;\r\n}\r\nspin_unlock(&local_device_data->ctx_lock);\r\n}\r\nklist_iter_exit(&device_iterator);\r\nif (!device_node) {\r\nreturn -EBUSY;\r\n}\r\n*device_data = local_device_data;\r\nreturn 0;\r\n}\r\nstatic void cryp_dma_setup_channel(struct cryp_device_data *device_data,\r\nstruct device *dev)\r\n{\r\nstruct dma_slave_config mem2cryp = {\r\n.direction = DMA_MEM_TO_DEV,\r\n.dst_addr = device_data->phybase + CRYP_DMA_TX_FIFO,\r\n.dst_addr_width = DMA_SLAVE_BUSWIDTH_2_BYTES,\r\n.dst_maxburst = 4,\r\n};\r\nstruct dma_slave_config cryp2mem = {\r\n.direction = DMA_DEV_TO_MEM,\r\n.src_addr = device_data->phybase + CRYP_DMA_RX_FIFO,\r\n.src_addr_width = DMA_SLAVE_BUSWIDTH_2_BYTES,\r\n.src_maxburst = 4,\r\n};\r\ndma_cap_zero(device_data->dma.mask);\r\ndma_cap_set(DMA_SLAVE, device_data->dma.mask);\r\ndevice_data->dma.cfg_mem2cryp = mem_to_engine;\r\ndevice_data->dma.chan_mem2cryp =\r\ndma_request_channel(device_data->dma.mask,\r\nstedma40_filter,\r\ndevice_data->dma.cfg_mem2cryp);\r\ndevice_data->dma.cfg_cryp2mem = engine_to_mem;\r\ndevice_data->dma.chan_cryp2mem =\r\ndma_request_channel(device_data->dma.mask,\r\nstedma40_filter,\r\ndevice_data->dma.cfg_cryp2mem);\r\ndmaengine_slave_config(device_data->dma.chan_mem2cryp, &mem2cryp);\r\ndmaengine_slave_config(device_data->dma.chan_cryp2mem, &cryp2mem);\r\ninit_completion(&device_data->dma.cryp_dma_complete);\r\n}\r\nstatic void cryp_dma_out_callback(void *data)\r\n{\r\nstruct cryp_ctx *ctx = (struct cryp_ctx *) data;\r\ndev_dbg(ctx->device->dev, "[%s]: ", __func__);\r\ncomplete(&ctx->device->dma.cryp_dma_complete);\r\n}\r\nstatic int cryp_set_dma_transfer(struct cryp_ctx *ctx,\r\nstruct scatterlist *sg,\r\nint len,\r\nenum dma_data_direction direction)\r\n{\r\nstruct dma_async_tx_descriptor *desc;\r\nstruct dma_chan *channel = NULL;\r\ndma_cookie_t cookie;\r\ndev_dbg(ctx->device->dev, "[%s]: ", __func__);\r\nif (unlikely(!IS_ALIGNED((u32)sg, 4))) {\r\ndev_err(ctx->device->dev, "[%s]: Data in sg list isn't "\r\n"aligned! Addr: 0x%08x", __func__, (u32)sg);\r\nreturn -EFAULT;\r\n}\r\nswitch (direction) {\r\ncase DMA_TO_DEVICE:\r\nchannel = ctx->device->dma.chan_mem2cryp;\r\nctx->device->dma.sg_src = sg;\r\nctx->device->dma.sg_src_len = dma_map_sg(channel->device->dev,\r\nctx->device->dma.sg_src,\r\nctx->device->dma.nents_src,\r\ndirection);\r\nif (!ctx->device->dma.sg_src_len) {\r\ndev_dbg(ctx->device->dev,\r\n"[%s]: Could not map the sg list (TO_DEVICE)",\r\n__func__);\r\nreturn -EFAULT;\r\n}\r\ndev_dbg(ctx->device->dev, "[%s]: Setting up DMA for buffer "\r\n"(TO_DEVICE)", __func__);\r\ndesc = dmaengine_prep_slave_sg(channel,\r\nctx->device->dma.sg_src,\r\nctx->device->dma.sg_src_len,\r\ndirection, DMA_CTRL_ACK);\r\nbreak;\r\ncase DMA_FROM_DEVICE:\r\nchannel = ctx->device->dma.chan_cryp2mem;\r\nctx->device->dma.sg_dst = sg;\r\nctx->device->dma.sg_dst_len = dma_map_sg(channel->device->dev,\r\nctx->device->dma.sg_dst,\r\nctx->device->dma.nents_dst,\r\ndirection);\r\nif (!ctx->device->dma.sg_dst_len) {\r\ndev_dbg(ctx->device->dev,\r\n"[%s]: Could not map the sg list (FROM_DEVICE)",\r\n__func__);\r\nreturn -EFAULT;\r\n}\r\ndev_dbg(ctx->device->dev, "[%s]: Setting up DMA for buffer "\r\n"(FROM_DEVICE)", __func__);\r\ndesc = dmaengine_prep_slave_sg(channel,\r\nctx->device->dma.sg_dst,\r\nctx->device->dma.sg_dst_len,\r\ndirection,\r\nDMA_CTRL_ACK |\r\nDMA_PREP_INTERRUPT);\r\ndesc->callback = cryp_dma_out_callback;\r\ndesc->callback_param = ctx;\r\nbreak;\r\ndefault:\r\ndev_dbg(ctx->device->dev, "[%s]: Invalid DMA direction",\r\n__func__);\r\nreturn -EFAULT;\r\n}\r\ncookie = dmaengine_submit(desc);\r\ndma_async_issue_pending(channel);\r\nreturn 0;\r\n}\r\nstatic void cryp_dma_done(struct cryp_ctx *ctx)\r\n{\r\nstruct dma_chan *chan;\r\ndev_dbg(ctx->device->dev, "[%s]: ", __func__);\r\nchan = ctx->device->dma.chan_mem2cryp;\r\ndmaengine_terminate_all(chan);\r\ndma_unmap_sg(chan->device->dev, ctx->device->dma.sg_src,\r\nctx->device->dma.sg_src_len, DMA_TO_DEVICE);\r\nchan = ctx->device->dma.chan_cryp2mem;\r\ndmaengine_terminate_all(chan);\r\ndma_unmap_sg(chan->device->dev, ctx->device->dma.sg_dst,\r\nctx->device->dma.sg_dst_len, DMA_FROM_DEVICE);\r\n}\r\nstatic int cryp_dma_write(struct cryp_ctx *ctx, struct scatterlist *sg,\r\nint len)\r\n{\r\nint error = cryp_set_dma_transfer(ctx, sg, len, DMA_TO_DEVICE);\r\ndev_dbg(ctx->device->dev, "[%s]: ", __func__);\r\nif (error) {\r\ndev_dbg(ctx->device->dev, "[%s]: cryp_set_dma_transfer() "\r\n"failed", __func__);\r\nreturn error;\r\n}\r\nreturn len;\r\n}\r\nstatic int cryp_dma_read(struct cryp_ctx *ctx, struct scatterlist *sg, int len)\r\n{\r\nint error = cryp_set_dma_transfer(ctx, sg, len, DMA_FROM_DEVICE);\r\nif (error) {\r\ndev_dbg(ctx->device->dev, "[%s]: cryp_set_dma_transfer() "\r\n"failed", __func__);\r\nreturn error;\r\n}\r\nreturn len;\r\n}\r\nstatic void cryp_polling_mode(struct cryp_ctx *ctx,\r\nstruct cryp_device_data *device_data)\r\n{\r\nint len = ctx->blocksize / BYTES_PER_WORD;\r\nint remaining_length = ctx->datalen;\r\nu32 *indata = (u32 *)ctx->indata;\r\nu32 *outdata = (u32 *)ctx->outdata;\r\nwhile (remaining_length > 0) {\r\nwritesl(&device_data->base->din, indata, len);\r\nindata += len;\r\nremaining_length -= (len * BYTES_PER_WORD);\r\ncryp_wait_until_done(device_data);\r\nreadsl(&device_data->base->dout, outdata, len);\r\noutdata += len;\r\ncryp_wait_until_done(device_data);\r\n}\r\n}\r\nstatic int cryp_disable_power(struct device *dev,\r\nstruct cryp_device_data *device_data,\r\nbool save_device_context)\r\n{\r\nint ret = 0;\r\ndev_dbg(dev, "[%s]", __func__);\r\nspin_lock(&device_data->power_state_spinlock);\r\nif (!device_data->power_state)\r\ngoto out;\r\nspin_lock(&device_data->ctx_lock);\r\nif (save_device_context && device_data->current_ctx) {\r\ncryp_save_device_context(device_data,\r\n&device_data->current_ctx->dev_ctx,\r\ncryp_mode);\r\ndevice_data->restore_dev_ctx = true;\r\n}\r\nspin_unlock(&device_data->ctx_lock);\r\nclk_disable(device_data->clk);\r\nret = regulator_disable(device_data->pwr_regulator);\r\nif (ret)\r\ndev_err(dev, "[%s]: "\r\n"regulator_disable() failed!",\r\n__func__);\r\ndevice_data->power_state = false;\r\nout:\r\nspin_unlock(&device_data->power_state_spinlock);\r\nreturn ret;\r\n}\r\nstatic int cryp_enable_power(\r\nstruct device *dev,\r\nstruct cryp_device_data *device_data,\r\nbool restore_device_context)\r\n{\r\nint ret = 0;\r\ndev_dbg(dev, "[%s]", __func__);\r\nspin_lock(&device_data->power_state_spinlock);\r\nif (!device_data->power_state) {\r\nret = regulator_enable(device_data->pwr_regulator);\r\nif (ret) {\r\ndev_err(dev, "[%s]: regulator_enable() failed!",\r\n__func__);\r\ngoto out;\r\n}\r\nret = clk_enable(device_data->clk);\r\nif (ret) {\r\ndev_err(dev, "[%s]: clk_enable() failed!",\r\n__func__);\r\nregulator_disable(device_data->pwr_regulator);\r\ngoto out;\r\n}\r\ndevice_data->power_state = true;\r\n}\r\nif (device_data->restore_dev_ctx) {\r\nspin_lock(&device_data->ctx_lock);\r\nif (restore_device_context && device_data->current_ctx) {\r\ndevice_data->restore_dev_ctx = false;\r\ncryp_restore_device_context(device_data,\r\n&device_data->current_ctx->dev_ctx);\r\n}\r\nspin_unlock(&device_data->ctx_lock);\r\n}\r\nout:\r\nspin_unlock(&device_data->power_state_spinlock);\r\nreturn ret;\r\n}\r\nstatic int hw_crypt_noxts(struct cryp_ctx *ctx,\r\nstruct cryp_device_data *device_data)\r\n{\r\nint ret = 0;\r\nconst u8 *indata = ctx->indata;\r\nu8 *outdata = ctx->outdata;\r\nu32 datalen = ctx->datalen;\r\nu32 outlen = datalen;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nctx->outlen = ctx->datalen;\r\nif (unlikely(!IS_ALIGNED((u32)indata, 4))) {\r\npr_debug(DEV_DBG_NAME " [%s]: Data isn't aligned! Addr: "\r\n"0x%08x", __func__, (u32)indata);\r\nreturn -EINVAL;\r\n}\r\nret = cryp_setup_context(ctx, device_data);\r\nif (ret)\r\ngoto out;\r\nif (cryp_mode == CRYP_MODE_INTERRUPT) {\r\ncryp_enable_irq_src(device_data, CRYP_IRQ_SRC_INPUT_FIFO |\r\nCRYP_IRQ_SRC_OUTPUT_FIFO);\r\nwhile (ctx->outlen > 0)\r\ncpu_relax();\r\n} else if (cryp_mode == CRYP_MODE_POLLING ||\r\ncryp_mode == CRYP_MODE_DMA) {\r\ncryp_polling_mode(ctx, device_data);\r\n} else {\r\ndev_err(ctx->device->dev, "[%s]: Invalid operation mode!",\r\n__func__);\r\nret = -EPERM;\r\ngoto out;\r\n}\r\ncryp_save_device_context(device_data, &ctx->dev_ctx, cryp_mode);\r\nctx->updated = 1;\r\nout:\r\nctx->indata = indata;\r\nctx->outdata = outdata;\r\nctx->datalen = datalen;\r\nctx->outlen = outlen;\r\nreturn ret;\r\n}\r\nstatic int get_nents(struct scatterlist *sg, int nbytes)\r\n{\r\nint nents = 0;\r\nwhile (nbytes > 0) {\r\nnbytes -= sg->length;\r\nsg = sg_next(sg);\r\nnents++;\r\n}\r\nreturn nents;\r\n}\r\nstatic int ablk_dma_crypt(struct ablkcipher_request *areq)\r\n{\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct cryp_device_data *device_data;\r\nint bytes_written = 0;\r\nint bytes_read = 0;\r\nint ret;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nctx->datalen = areq->nbytes;\r\nctx->outlen = areq->nbytes;\r\nret = cryp_get_device_data(ctx, &device_data);\r\nif (ret)\r\nreturn ret;\r\nret = cryp_setup_context(ctx, device_data);\r\nif (ret)\r\ngoto out;\r\nctx->device->dma.nents_src = get_nents(areq->src, ctx->datalen);\r\nctx->device->dma.nents_dst = get_nents(areq->dst, ctx->outlen);\r\ncryp_configure_for_dma(device_data, CRYP_DMA_ENABLE_BOTH_DIRECTIONS);\r\nbytes_written = cryp_dma_write(ctx, areq->src, ctx->datalen);\r\nbytes_read = cryp_dma_read(ctx, areq->dst, bytes_written);\r\nwait_for_completion(&ctx->device->dma.cryp_dma_complete);\r\ncryp_dma_done(ctx);\r\ncryp_save_device_context(device_data, &ctx->dev_ctx, cryp_mode);\r\nctx->updated = 1;\r\nout:\r\nspin_lock(&device_data->ctx_lock);\r\ndevice_data->current_ctx = NULL;\r\nctx->device = NULL;\r\nspin_unlock(&device_data->ctx_lock);\r\nup(&driver_data.device_allocation);\r\nif (unlikely(bytes_written != bytes_read))\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int ablk_crypt(struct ablkcipher_request *areq)\r\n{\r\nstruct ablkcipher_walk walk;\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct cryp_device_data *device_data;\r\nunsigned long src_paddr;\r\nunsigned long dst_paddr;\r\nint ret;\r\nint nbytes;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nret = cryp_get_device_data(ctx, &device_data);\r\nif (ret)\r\ngoto out;\r\nablkcipher_walk_init(&walk, areq->dst, areq->src, areq->nbytes);\r\nret = ablkcipher_walk_phys(areq, &walk);\r\nif (ret) {\r\npr_err(DEV_DBG_NAME "[%s]: ablkcipher_walk_phys() failed!",\r\n__func__);\r\ngoto out;\r\n}\r\nwhile ((nbytes = walk.nbytes) > 0) {\r\nctx->iv = walk.iv;\r\nsrc_paddr = (page_to_phys(walk.src.page) + walk.src.offset);\r\nctx->indata = phys_to_virt(src_paddr);\r\ndst_paddr = (page_to_phys(walk.dst.page) + walk.dst.offset);\r\nctx->outdata = phys_to_virt(dst_paddr);\r\nctx->datalen = nbytes - (nbytes % ctx->blocksize);\r\nret = hw_crypt_noxts(ctx, device_data);\r\nif (ret)\r\ngoto out;\r\nnbytes -= ctx->datalen;\r\nret = ablkcipher_walk_done(areq, &walk, nbytes);\r\nif (ret)\r\ngoto out;\r\n}\r\nablkcipher_walk_complete(&walk);\r\nout:\r\nspin_lock(&device_data->ctx_lock);\r\ndevice_data->current_ctx = NULL;\r\nctx->device = NULL;\r\nspin_unlock(&device_data->ctx_lock);\r\nup(&driver_data.device_allocation);\r\nreturn ret;\r\n}\r\nstatic int aes_ablkcipher_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nu32 *flags = &cipher->base.crt_flags;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nswitch (keylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->config.keysize = CRYP_KEY_SIZE_128;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->config.keysize = CRYP_KEY_SIZE_192;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->config.keysize = CRYP_KEY_SIZE_256;\r\nbreak;\r\ndefault:\r\npr_err(DEV_DBG_NAME "[%s]: Unknown keylen!", __func__);\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nmemcpy(ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\nctx->updated = 0;\r\nreturn 0;\r\n}\r\nstatic int des_ablkcipher_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nu32 *flags = &cipher->base.crt_flags;\r\nu32 tmp[DES_EXPKEY_WORDS];\r\nint ret;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nif (keylen != DES_KEY_SIZE) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\npr_debug(DEV_DBG_NAME " [%s]: CRYPTO_TFM_RES_BAD_KEY_LEN",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nret = des_ekey(tmp, key);\r\nif (unlikely(ret == 0) && (*flags & CRYPTO_TFM_REQ_WEAK_KEY)) {\r\n*flags |= CRYPTO_TFM_RES_WEAK_KEY;\r\npr_debug(DEV_DBG_NAME " [%s]: CRYPTO_TFM_REQ_WEAK_KEY",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nmemcpy(ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\nctx->updated = 0;\r\nreturn 0;\r\n}\r\nstatic int des3_ablkcipher_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nu32 *flags = &cipher->base.crt_flags;\r\nconst u32 *K = (const u32 *)key;\r\nu32 tmp[DES3_EDE_EXPKEY_WORDS];\r\nint i, ret;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nif (keylen != DES3_EDE_KEY_SIZE) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\npr_debug(DEV_DBG_NAME " [%s]: CRYPTO_TFM_RES_BAD_KEY_LEN",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(!((K[0] ^ K[2]) | (K[1] ^ K[3])) ||\r\n!((K[2] ^ K[4]) | (K[3] ^ K[5]))) &&\r\n(*flags & CRYPTO_TFM_REQ_WEAK_KEY)) {\r\n*flags |= CRYPTO_TFM_RES_WEAK_KEY;\r\npr_debug(DEV_DBG_NAME " [%s]: CRYPTO_TFM_REQ_WEAK_KEY",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < 3; i++) {\r\nret = des_ekey(tmp, key + i*DES_KEY_SIZE);\r\nif (unlikely(ret == 0) && (*flags & CRYPTO_TFM_REQ_WEAK_KEY)) {\r\n*flags |= CRYPTO_TFM_RES_WEAK_KEY;\r\npr_debug(DEV_DBG_NAME " [%s]: "\r\n"CRYPTO_TFM_REQ_WEAK_KEY", __func__);\r\nreturn -EINVAL;\r\n}\r\n}\r\nmemcpy(ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\nctx->updated = 0;\r\nreturn 0;\r\n}\r\nstatic int cryp_blk_encrypt(struct ablkcipher_request *areq)\r\n{\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nctx->config.algodir = CRYP_ALGORITHM_ENCRYPT;\r\nif (cryp_mode == CRYP_MODE_DMA && mode_is_aes(ctx->config.algomode))\r\nreturn ablk_dma_crypt(areq);\r\nreturn ablk_crypt(areq);\r\n}\r\nstatic int cryp_blk_decrypt(struct ablkcipher_request *areq)\r\n{\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct cryp_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nctx->config.algodir = CRYP_ALGORITHM_DECRYPT;\r\nif (cryp_mode == CRYP_MODE_DMA && mode_is_aes(ctx->config.algomode))\r\nreturn ablk_dma_crypt(areq);\r\nreturn ablk_crypt(areq);\r\n}\r\nstatic int cryp_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct cryp_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct cryp_algo_template *cryp_alg = container_of(alg,\r\nstruct cryp_algo_template,\r\ncrypto);\r\nctx->config.algomode = cryp_alg->algomode;\r\nctx->blocksize = crypto_tfm_alg_blocksize(tfm);\r\nreturn 0;\r\n}\r\nstatic int cryp_algs_register_all(void)\r\n{\r\nint ret;\r\nint i;\r\nint count;\r\npr_debug("[%s]", __func__);\r\nfor (i = 0; i < ARRAY_SIZE(cryp_algs); i++) {\r\nret = crypto_register_alg(&cryp_algs[i].crypto);\r\nif (ret) {\r\ncount = i;\r\npr_err("[%s] alg registration failed",\r\ncryp_algs[i].crypto.cra_driver_name);\r\ngoto unreg;\r\n}\r\n}\r\nreturn 0;\r\nunreg:\r\nfor (i = 0; i < count; i++)\r\ncrypto_unregister_alg(&cryp_algs[i].crypto);\r\nreturn ret;\r\n}\r\nstatic void cryp_algs_unregister_all(void)\r\n{\r\nint i;\r\npr_debug(DEV_DBG_NAME " [%s]", __func__);\r\nfor (i = 0; i < ARRAY_SIZE(cryp_algs); i++)\r\ncrypto_unregister_alg(&cryp_algs[i].crypto);\r\n}\r\nstatic int ux500_cryp_probe(struct platform_device *pdev)\r\n{\r\nint ret;\r\nint cryp_error = 0;\r\nstruct resource *res = NULL;\r\nstruct resource *res_irq = NULL;\r\nstruct cryp_device_data *device_data;\r\nstruct cryp_protection_config prot = {\r\n.privilege_access = CRYP_STATE_ENABLE\r\n};\r\nstruct device *dev = &pdev->dev;\r\ndev_dbg(dev, "[%s]", __func__);\r\ndevice_data = devm_kzalloc(dev, sizeof(*device_data), GFP_ATOMIC);\r\nif (!device_data) {\r\ndev_err(dev, "[%s]: kzalloc() failed!", __func__);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ndevice_data->dev = dev;\r\ndevice_data->current_ctx = NULL;\r\nmem_to_engine = &((struct cryp_platform_data *)\r\ndev->platform_data)->mem_to_engine;\r\nengine_to_mem = &((struct cryp_platform_data *)\r\ndev->platform_data)->engine_to_mem;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res) {\r\ndev_err(dev, "[%s]: platform_get_resource() failed",\r\n__func__);\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\ndevice_data->phybase = res->start;\r\ndevice_data->base = devm_ioremap_resource(dev, res);\r\nif (!device_data->base) {\r\ndev_err(dev, "[%s]: ioremap failed!", __func__);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nspin_lock_init(&device_data->ctx_lock);\r\nspin_lock_init(&device_data->power_state_spinlock);\r\ndevice_data->pwr_regulator = regulator_get(&pdev->dev, "v-ape");\r\nif (IS_ERR(device_data->pwr_regulator)) {\r\ndev_err(dev, "[%s]: could not get cryp regulator", __func__);\r\nret = PTR_ERR(device_data->pwr_regulator);\r\ndevice_data->pwr_regulator = NULL;\r\ngoto out;\r\n}\r\ndevice_data->clk = devm_clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(device_data->clk)) {\r\ndev_err(dev, "[%s]: clk_get() failed!", __func__);\r\nret = PTR_ERR(device_data->clk);\r\ngoto out_regulator;\r\n}\r\nret = clk_prepare(device_data->clk);\r\nif (ret) {\r\ndev_err(dev, "[%s]: clk_prepare() failed!", __func__);\r\ngoto out_regulator;\r\n}\r\nret = cryp_enable_power(device_data->dev, device_data, false);\r\nif (ret) {\r\ndev_err(dev, "[%s]: cryp_enable_power() failed!", __func__);\r\ngoto out_clk_unprepare;\r\n}\r\ncryp_error = cryp_check(device_data);\r\nif (cryp_error != 0) {\r\ndev_err(dev, "[%s]: cryp_init() failed!", __func__);\r\nret = -EINVAL;\r\ngoto out_power;\r\n}\r\ncryp_error = cryp_configure_protection(device_data, &prot);\r\nif (cryp_error != 0) {\r\ndev_err(dev, "[%s]: cryp_configure_protection() failed!",\r\n__func__);\r\nret = -EINVAL;\r\ngoto out_power;\r\n}\r\nres_irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!res_irq) {\r\ndev_err(dev, "[%s]: IORESOURCE_IRQ unavailable",\r\n__func__);\r\nret = -ENODEV;\r\ngoto out_power;\r\n}\r\nret = devm_request_irq(&pdev->dev, res_irq->start,\r\ncryp_interrupt_handler, 0, "cryp1", device_data);\r\nif (ret) {\r\ndev_err(dev, "[%s]: Unable to request IRQ", __func__);\r\ngoto out_power;\r\n}\r\nif (cryp_mode == CRYP_MODE_DMA)\r\ncryp_dma_setup_channel(device_data, dev);\r\nplatform_set_drvdata(pdev, device_data);\r\nklist_add_tail(&device_data->list_node, &driver_data.device_list);\r\nup(&driver_data.device_allocation);\r\natomic_set(&session_id, 1);\r\nret = cryp_algs_register_all();\r\nif (ret) {\r\ndev_err(dev, "[%s]: cryp_algs_register_all() failed!",\r\n__func__);\r\ngoto out_power;\r\n}\r\ndev_info(dev, "successfully registered\n");\r\nreturn 0;\r\nout_power:\r\ncryp_disable_power(device_data->dev, device_data, false);\r\nout_clk_unprepare:\r\nclk_unprepare(device_data->clk);\r\nout_regulator:\r\nregulator_put(device_data->pwr_regulator);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ux500_cryp_remove(struct platform_device *pdev)\r\n{\r\nstruct cryp_device_data *device_data;\r\ndev_dbg(&pdev->dev, "[%s]", __func__);\r\ndevice_data = platform_get_drvdata(pdev);\r\nif (!device_data) {\r\ndev_err(&pdev->dev, "[%s]: platform_get_drvdata() failed!",\r\n__func__);\r\nreturn -ENOMEM;\r\n}\r\nif (down_trylock(&driver_data.device_allocation))\r\nreturn -EBUSY;\r\nspin_lock(&device_data->ctx_lock);\r\nif (device_data->current_ctx) {\r\nspin_unlock(&device_data->ctx_lock);\r\nup(&driver_data.device_allocation);\r\nreturn -EBUSY;\r\n}\r\nspin_unlock(&device_data->ctx_lock);\r\nif (klist_node_attached(&device_data->list_node))\r\nklist_remove(&device_data->list_node);\r\nif (list_empty(&driver_data.device_list.k_list))\r\ncryp_algs_unregister_all();\r\nif (cryp_disable_power(&pdev->dev, device_data, false))\r\ndev_err(&pdev->dev, "[%s]: cryp_disable_power() failed",\r\n__func__);\r\nclk_unprepare(device_data->clk);\r\nregulator_put(device_data->pwr_regulator);\r\nreturn 0;\r\n}\r\nstatic void ux500_cryp_shutdown(struct platform_device *pdev)\r\n{\r\nstruct cryp_device_data *device_data;\r\ndev_dbg(&pdev->dev, "[%s]", __func__);\r\ndevice_data = platform_get_drvdata(pdev);\r\nif (!device_data) {\r\ndev_err(&pdev->dev, "[%s]: platform_get_drvdata() failed!",\r\n__func__);\r\nreturn;\r\n}\r\nspin_lock(&device_data->ctx_lock);\r\nif (!device_data->current_ctx) {\r\nif (down_trylock(&driver_data.device_allocation))\r\ndev_dbg(&pdev->dev, "[%s]: Cryp still in use!"\r\n"Shutting down anyway...", __func__);\r\ndevice_data->current_ctx++;\r\n}\r\nspin_unlock(&device_data->ctx_lock);\r\nif (klist_node_attached(&device_data->list_node))\r\nklist_remove(&device_data->list_node);\r\nif (list_empty(&driver_data.device_list.k_list))\r\ncryp_algs_unregister_all();\r\nif (cryp_disable_power(&pdev->dev, device_data, false))\r\ndev_err(&pdev->dev, "[%s]: cryp_disable_power() failed",\r\n__func__);\r\n}\r\nstatic int ux500_cryp_suspend(struct device *dev)\r\n{\r\nint ret;\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct cryp_device_data *device_data;\r\nstruct resource *res_irq;\r\nstruct cryp_ctx *temp_ctx = NULL;\r\ndev_dbg(dev, "[%s]", __func__);\r\ndevice_data = platform_get_drvdata(pdev);\r\nif (!device_data) {\r\ndev_err(dev, "[%s]: platform_get_drvdata() failed!", __func__);\r\nreturn -ENOMEM;\r\n}\r\nres_irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!res_irq)\r\ndev_err(dev, "[%s]: IORESOURCE_IRQ, unavailable", __func__);\r\nelse\r\ndisable_irq(res_irq->start);\r\nspin_lock(&device_data->ctx_lock);\r\nif (!device_data->current_ctx)\r\ndevice_data->current_ctx++;\r\nspin_unlock(&device_data->ctx_lock);\r\nif (device_data->current_ctx == ++temp_ctx) {\r\nif (down_interruptible(&driver_data.device_allocation))\r\ndev_dbg(dev, "[%s]: down_interruptible() failed",\r\n__func__);\r\nret = cryp_disable_power(dev, device_data, false);\r\n} else\r\nret = cryp_disable_power(dev, device_data, true);\r\nif (ret)\r\ndev_err(dev, "[%s]: cryp_disable_power()", __func__);\r\nreturn ret;\r\n}\r\nstatic int ux500_cryp_resume(struct device *dev)\r\n{\r\nint ret = 0;\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct cryp_device_data *device_data;\r\nstruct resource *res_irq;\r\nstruct cryp_ctx *temp_ctx = NULL;\r\ndev_dbg(dev, "[%s]", __func__);\r\ndevice_data = platform_get_drvdata(pdev);\r\nif (!device_data) {\r\ndev_err(dev, "[%s]: platform_get_drvdata() failed!", __func__);\r\nreturn -ENOMEM;\r\n}\r\nspin_lock(&device_data->ctx_lock);\r\nif (device_data->current_ctx == ++temp_ctx)\r\ndevice_data->current_ctx = NULL;\r\nspin_unlock(&device_data->ctx_lock);\r\nif (!device_data->current_ctx)\r\nup(&driver_data.device_allocation);\r\nelse\r\nret = cryp_enable_power(dev, device_data, true);\r\nif (ret)\r\ndev_err(dev, "[%s]: cryp_enable_power() failed!", __func__);\r\nelse {\r\nres_irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (res_irq)\r\nenable_irq(res_irq->start);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __init ux500_cryp_mod_init(void)\r\n{\r\npr_debug("[%s] is called!", __func__);\r\nklist_init(&driver_data.device_list, NULL, NULL);\r\nsema_init(&driver_data.device_allocation, 0);\r\nreturn platform_driver_register(&cryp_driver);\r\n}\r\nstatic void __exit ux500_cryp_mod_fini(void)\r\n{\r\npr_debug("[%s] is called!", __func__);\r\nplatform_driver_unregister(&cryp_driver);\r\nreturn;\r\n}
