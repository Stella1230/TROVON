static inline struct rd_dev *RD_DEV(struct se_device *dev)\r\n{\r\nreturn container_of(dev, struct rd_dev, dev);\r\n}\r\nstatic int rd_attach_hba(struct se_hba *hba, u32 host_id)\r\n{\r\nstruct rd_host *rd_host;\r\nrd_host = kzalloc(sizeof(struct rd_host), GFP_KERNEL);\r\nif (!rd_host) {\r\npr_err("Unable to allocate memory for struct rd_host\n");\r\nreturn -ENOMEM;\r\n}\r\nrd_host->rd_host_id = host_id;\r\nhba->hba_ptr = rd_host;\r\npr_debug("CORE_HBA[%d] - TCM Ramdisk HBA Driver %s on"\r\n" Generic Target Core Stack %s\n", hba->hba_id,\r\nRD_HBA_VERSION, TARGET_CORE_VERSION);\r\nreturn 0;\r\n}\r\nstatic void rd_detach_hba(struct se_hba *hba)\r\n{\r\nstruct rd_host *rd_host = hba->hba_ptr;\r\npr_debug("CORE_HBA[%d] - Detached Ramdisk HBA: %u from"\r\n" Generic Target Core\n", hba->hba_id, rd_host->rd_host_id);\r\nkfree(rd_host);\r\nhba->hba_ptr = NULL;\r\n}\r\nstatic u32 rd_release_sgl_table(struct rd_dev *rd_dev, struct rd_dev_sg_table *sg_table,\r\nu32 sg_table_count)\r\n{\r\nstruct page *pg;\r\nstruct scatterlist *sg;\r\nu32 i, j, page_count = 0, sg_per_table;\r\nfor (i = 0; i < sg_table_count; i++) {\r\nsg = sg_table[i].sg_table;\r\nsg_per_table = sg_table[i].rd_sg_count;\r\nfor (j = 0; j < sg_per_table; j++) {\r\npg = sg_page(&sg[j]);\r\nif (pg) {\r\n__free_page(pg);\r\npage_count++;\r\n}\r\n}\r\nkfree(sg);\r\n}\r\nkfree(sg_table);\r\nreturn page_count;\r\n}\r\nstatic void rd_release_device_space(struct rd_dev *rd_dev)\r\n{\r\nu32 page_count;\r\nif (!rd_dev->sg_table_array || !rd_dev->sg_table_count)\r\nreturn;\r\npage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,\r\nrd_dev->sg_table_count);\r\npr_debug("CORE_RD[%u] - Released device space for Ramdisk"\r\n" Device ID: %u, pages %u in %u tables total bytes %lu\n",\r\nrd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\r\nrd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\r\nrd_dev->sg_table_array = NULL;\r\nrd_dev->sg_table_count = 0;\r\n}\r\nstatic int rd_allocate_sgl_table(struct rd_dev *rd_dev, struct rd_dev_sg_table *sg_table,\r\nu32 total_sg_needed, unsigned char init_payload)\r\n{\r\nu32 i = 0, j, page_offset = 0, sg_per_table;\r\nu32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /\r\nsizeof(struct scatterlist));\r\nstruct page *pg;\r\nstruct scatterlist *sg;\r\nunsigned char *p;\r\nwhile (total_sg_needed) {\r\nunsigned int chain_entry = 0;\r\nsg_per_table = (total_sg_needed > max_sg_per_table) ?\r\nmax_sg_per_table : total_sg_needed;\r\nif (sg_per_table < total_sg_needed)\r\nchain_entry = 1;\r\nsg = kcalloc(sg_per_table + chain_entry, sizeof(*sg),\r\nGFP_KERNEL);\r\nif (!sg) {\r\npr_err("Unable to allocate scatterlist array"\r\n" for struct rd_dev\n");\r\nreturn -ENOMEM;\r\n}\r\nsg_init_table(sg, sg_per_table + chain_entry);\r\nif (i > 0) {\r\nsg_chain(sg_table[i - 1].sg_table,\r\nmax_sg_per_table + 1, sg);\r\n}\r\nsg_table[i].sg_table = sg;\r\nsg_table[i].rd_sg_count = sg_per_table;\r\nsg_table[i].page_start_offset = page_offset;\r\nsg_table[i++].page_end_offset = (page_offset + sg_per_table)\r\n- 1;\r\nfor (j = 0; j < sg_per_table; j++) {\r\npg = alloc_pages(GFP_KERNEL, 0);\r\nif (!pg) {\r\npr_err("Unable to allocate scatterlist"\r\n" pages for struct rd_dev_sg_table\n");\r\nreturn -ENOMEM;\r\n}\r\nsg_assign_page(&sg[j], pg);\r\nsg[j].length = PAGE_SIZE;\r\np = kmap(pg);\r\nmemset(p, init_payload, PAGE_SIZE);\r\nkunmap(pg);\r\n}\r\npage_offset += sg_per_table;\r\ntotal_sg_needed -= sg_per_table;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rd_build_device_space(struct rd_dev *rd_dev)\r\n{\r\nstruct rd_dev_sg_table *sg_table;\r\nu32 sg_tables, total_sg_needed;\r\nu32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /\r\nsizeof(struct scatterlist));\r\nint rc;\r\nif (rd_dev->rd_page_count <= 0) {\r\npr_err("Illegal page count: %u for Ramdisk device\n",\r\nrd_dev->rd_page_count);\r\nreturn -EINVAL;\r\n}\r\nif (rd_dev->rd_flags & RDF_NULLIO)\r\nreturn 0;\r\ntotal_sg_needed = rd_dev->rd_page_count;\r\nsg_tables = (total_sg_needed / max_sg_per_table) + 1;\r\nsg_table = kzalloc(sg_tables * sizeof(struct rd_dev_sg_table), GFP_KERNEL);\r\nif (!sg_table) {\r\npr_err("Unable to allocate memory for Ramdisk"\r\n" scatterlist tables\n");\r\nreturn -ENOMEM;\r\n}\r\nrd_dev->sg_table_array = sg_table;\r\nrd_dev->sg_table_count = sg_tables;\r\nrc = rd_allocate_sgl_table(rd_dev, sg_table, total_sg_needed, 0x00);\r\nif (rc)\r\nreturn rc;\r\npr_debug("CORE_RD[%u] - Built Ramdisk Device ID: %u space of"\r\n" %u pages in %u tables\n", rd_dev->rd_host->rd_host_id,\r\nrd_dev->rd_dev_id, rd_dev->rd_page_count,\r\nrd_dev->sg_table_count);\r\nreturn 0;\r\n}\r\nstatic void rd_release_prot_space(struct rd_dev *rd_dev)\r\n{\r\nu32 page_count;\r\nif (!rd_dev->sg_prot_array || !rd_dev->sg_prot_count)\r\nreturn;\r\npage_count = rd_release_sgl_table(rd_dev, rd_dev->sg_prot_array,\r\nrd_dev->sg_prot_count);\r\npr_debug("CORE_RD[%u] - Released protection space for Ramdisk"\r\n" Device ID: %u, pages %u in %u tables total bytes %lu\n",\r\nrd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,\r\nrd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);\r\nrd_dev->sg_prot_array = NULL;\r\nrd_dev->sg_prot_count = 0;\r\n}\r\nstatic int rd_build_prot_space(struct rd_dev *rd_dev, int prot_length, int block_size)\r\n{\r\nstruct rd_dev_sg_table *sg_table;\r\nu32 total_sg_needed, sg_tables;\r\nu32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /\r\nsizeof(struct scatterlist));\r\nint rc;\r\nif (rd_dev->rd_flags & RDF_NULLIO)\r\nreturn 0;\r\ntotal_sg_needed = (rd_dev->rd_page_count * prot_length / block_size) + 1;\r\nsg_tables = (total_sg_needed / max_sg_per_table) + 1;\r\nsg_table = kzalloc(sg_tables * sizeof(struct rd_dev_sg_table), GFP_KERNEL);\r\nif (!sg_table) {\r\npr_err("Unable to allocate memory for Ramdisk protection"\r\n" scatterlist tables\n");\r\nreturn -ENOMEM;\r\n}\r\nrd_dev->sg_prot_array = sg_table;\r\nrd_dev->sg_prot_count = sg_tables;\r\nrc = rd_allocate_sgl_table(rd_dev, sg_table, total_sg_needed, 0xff);\r\nif (rc)\r\nreturn rc;\r\npr_debug("CORE_RD[%u] - Built Ramdisk Device ID: %u prot space of"\r\n" %u pages in %u tables\n", rd_dev->rd_host->rd_host_id,\r\nrd_dev->rd_dev_id, total_sg_needed, rd_dev->sg_prot_count);\r\nreturn 0;\r\n}\r\nstatic struct se_device *rd_alloc_device(struct se_hba *hba, const char *name)\r\n{\r\nstruct rd_dev *rd_dev;\r\nstruct rd_host *rd_host = hba->hba_ptr;\r\nrd_dev = kzalloc(sizeof(struct rd_dev), GFP_KERNEL);\r\nif (!rd_dev) {\r\npr_err("Unable to allocate memory for struct rd_dev\n");\r\nreturn NULL;\r\n}\r\nrd_dev->rd_host = rd_host;\r\nreturn &rd_dev->dev;\r\n}\r\nstatic int rd_configure_device(struct se_device *dev)\r\n{\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nstruct rd_host *rd_host = dev->se_hba->hba_ptr;\r\nint ret;\r\nif (!(rd_dev->rd_flags & RDF_HAS_PAGE_COUNT)) {\r\npr_debug("Missing rd_pages= parameter\n");\r\nreturn -EINVAL;\r\n}\r\nret = rd_build_device_space(rd_dev);\r\nif (ret < 0)\r\ngoto fail;\r\ndev->dev_attrib.hw_block_size = RD_BLOCKSIZE;\r\ndev->dev_attrib.hw_max_sectors = UINT_MAX;\r\ndev->dev_attrib.hw_queue_depth = RD_MAX_DEVICE_QUEUE_DEPTH;\r\ndev->dev_attrib.is_nonrot = 1;\r\nrd_dev->rd_dev_id = rd_host->rd_host_dev_id_count++;\r\npr_debug("CORE_RD[%u] - Added TCM MEMCPY Ramdisk Device ID: %u of"\r\n" %u pages in %u tables, %lu total bytes\n",\r\nrd_host->rd_host_id, rd_dev->rd_dev_id, rd_dev->rd_page_count,\r\nrd_dev->sg_table_count,\r\n(unsigned long)(rd_dev->rd_page_count * PAGE_SIZE));\r\nreturn 0;\r\nfail:\r\nrd_release_device_space(rd_dev);\r\nreturn ret;\r\n}\r\nstatic void rd_dev_call_rcu(struct rcu_head *p)\r\n{\r\nstruct se_device *dev = container_of(p, struct se_device, rcu_head);\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nkfree(rd_dev);\r\n}\r\nstatic void rd_free_device(struct se_device *dev)\r\n{\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nrd_release_device_space(rd_dev);\r\ncall_rcu(&dev->rcu_head, rd_dev_call_rcu);\r\n}\r\nstatic struct rd_dev_sg_table *rd_get_sg_table(struct rd_dev *rd_dev, u32 page)\r\n{\r\nstruct rd_dev_sg_table *sg_table;\r\nu32 i, sg_per_table = (RD_MAX_ALLOCATION_SIZE /\r\nsizeof(struct scatterlist));\r\ni = page / sg_per_table;\r\nif (i < rd_dev->sg_table_count) {\r\nsg_table = &rd_dev->sg_table_array[i];\r\nif ((sg_table->page_start_offset <= page) &&\r\n(sg_table->page_end_offset >= page))\r\nreturn sg_table;\r\n}\r\npr_err("Unable to locate struct rd_dev_sg_table for page: %u\n",\r\npage);\r\nreturn NULL;\r\n}\r\nstatic struct rd_dev_sg_table *rd_get_prot_table(struct rd_dev *rd_dev, u32 page)\r\n{\r\nstruct rd_dev_sg_table *sg_table;\r\nu32 i, sg_per_table = (RD_MAX_ALLOCATION_SIZE /\r\nsizeof(struct scatterlist));\r\ni = page / sg_per_table;\r\nif (i < rd_dev->sg_prot_count) {\r\nsg_table = &rd_dev->sg_prot_array[i];\r\nif ((sg_table->page_start_offset <= page) &&\r\n(sg_table->page_end_offset >= page))\r\nreturn sg_table;\r\n}\r\npr_err("Unable to locate struct prot rd_dev_sg_table for page: %u\n",\r\npage);\r\nreturn NULL;\r\n}\r\nstatic sense_reason_t rd_do_prot_rw(struct se_cmd *cmd, bool is_read)\r\n{\r\nstruct se_device *se_dev = cmd->se_dev;\r\nstruct rd_dev *dev = RD_DEV(se_dev);\r\nstruct rd_dev_sg_table *prot_table;\r\nbool need_to_release = false;\r\nstruct scatterlist *prot_sg;\r\nu32 sectors = cmd->data_length / se_dev->dev_attrib.block_size;\r\nu32 prot_offset, prot_page;\r\nu32 prot_npages __maybe_unused;\r\nu64 tmp;\r\nsense_reason_t rc = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\ntmp = cmd->t_task_lba * se_dev->prot_length;\r\nprot_offset = do_div(tmp, PAGE_SIZE);\r\nprot_page = tmp;\r\nprot_table = rd_get_prot_table(dev, prot_page);\r\nif (!prot_table)\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\nprot_sg = &prot_table->sg_table[prot_page -\r\nprot_table->page_start_offset];\r\nif (is_read)\r\nrc = sbc_dif_verify(cmd, cmd->t_task_lba, sectors, 0,\r\nprot_sg, prot_offset);\r\nelse\r\nrc = sbc_dif_verify(cmd, cmd->t_task_lba, sectors, 0,\r\ncmd->t_prot_sg, 0);\r\nif (!rc)\r\nsbc_dif_copy_prot(cmd, sectors, is_read, prot_sg, prot_offset);\r\nif (need_to_release)\r\nkfree(prot_sg);\r\nreturn rc;\r\n}\r\nstatic sense_reason_t\r\nrd_execute_rw(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,\r\nenum dma_data_direction data_direction)\r\n{\r\nstruct se_device *se_dev = cmd->se_dev;\r\nstruct rd_dev *dev = RD_DEV(se_dev);\r\nstruct rd_dev_sg_table *table;\r\nstruct scatterlist *rd_sg;\r\nstruct sg_mapping_iter m;\r\nu32 rd_offset;\r\nu32 rd_size;\r\nu32 rd_page;\r\nu32 src_len;\r\nu64 tmp;\r\nsense_reason_t rc;\r\nif (dev->rd_flags & RDF_NULLIO) {\r\ntarget_complete_cmd(cmd, SAM_STAT_GOOD);\r\nreturn 0;\r\n}\r\ntmp = cmd->t_task_lba * se_dev->dev_attrib.block_size;\r\nrd_offset = do_div(tmp, PAGE_SIZE);\r\nrd_page = tmp;\r\nrd_size = cmd->data_length;\r\ntable = rd_get_sg_table(dev, rd_page);\r\nif (!table)\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\nrd_sg = &table->sg_table[rd_page - table->page_start_offset];\r\npr_debug("RD[%u]: %s LBA: %llu, Size: %u Page: %u, Offset: %u\n",\r\ndev->rd_dev_id,\r\ndata_direction == DMA_FROM_DEVICE ? "Read" : "Write",\r\ncmd->t_task_lba, rd_size, rd_page, rd_offset);\r\nif (cmd->prot_type && se_dev->dev_attrib.pi_prot_type &&\r\ndata_direction == DMA_TO_DEVICE) {\r\nrc = rd_do_prot_rw(cmd, false);\r\nif (rc)\r\nreturn rc;\r\n}\r\nsrc_len = PAGE_SIZE - rd_offset;\r\nsg_miter_start(&m, sgl, sgl_nents,\r\ndata_direction == DMA_FROM_DEVICE ?\r\nSG_MITER_TO_SG : SG_MITER_FROM_SG);\r\nwhile (rd_size) {\r\nu32 len;\r\nvoid *rd_addr;\r\nsg_miter_next(&m);\r\nif (!(u32)m.length) {\r\npr_debug("RD[%u]: invalid sgl %p len %zu\n",\r\ndev->rd_dev_id, m.addr, m.length);\r\nsg_miter_stop(&m);\r\nreturn TCM_INCORRECT_AMOUNT_OF_DATA;\r\n}\r\nlen = min((u32)m.length, src_len);\r\nif (len > rd_size) {\r\npr_debug("RD[%u]: size underrun page %d offset %d "\r\n"size %d\n", dev->rd_dev_id,\r\nrd_page, rd_offset, rd_size);\r\nlen = rd_size;\r\n}\r\nm.consumed = len;\r\nrd_addr = sg_virt(rd_sg) + rd_offset;\r\nif (data_direction == DMA_FROM_DEVICE)\r\nmemcpy(m.addr, rd_addr, len);\r\nelse\r\nmemcpy(rd_addr, m.addr, len);\r\nrd_size -= len;\r\nif (!rd_size)\r\ncontinue;\r\nsrc_len -= len;\r\nif (src_len) {\r\nrd_offset += len;\r\ncontinue;\r\n}\r\nrd_page++;\r\nrd_offset = 0;\r\nsrc_len = PAGE_SIZE;\r\nif (rd_page <= table->page_end_offset) {\r\nrd_sg++;\r\ncontinue;\r\n}\r\ntable = rd_get_sg_table(dev, rd_page);\r\nif (!table) {\r\nsg_miter_stop(&m);\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\nrd_sg = table->sg_table;\r\n}\r\nsg_miter_stop(&m);\r\nif (cmd->prot_type && se_dev->dev_attrib.pi_prot_type &&\r\ndata_direction == DMA_FROM_DEVICE) {\r\nrc = rd_do_prot_rw(cmd, true);\r\nif (rc)\r\nreturn rc;\r\n}\r\ntarget_complete_cmd(cmd, SAM_STAT_GOOD);\r\nreturn 0;\r\n}\r\nstatic ssize_t rd_set_configfs_dev_params(struct se_device *dev,\r\nconst char *page, ssize_t count)\r\n{\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nchar *orig, *ptr, *opts;\r\nsubstring_t args[MAX_OPT_ARGS];\r\nint ret = 0, arg, token;\r\nopts = kstrdup(page, GFP_KERNEL);\r\nif (!opts)\r\nreturn -ENOMEM;\r\norig = opts;\r\nwhile ((ptr = strsep(&opts, ",\n")) != NULL) {\r\nif (!*ptr)\r\ncontinue;\r\ntoken = match_token(ptr, tokens, args);\r\nswitch (token) {\r\ncase Opt_rd_pages:\r\nmatch_int(args, &arg);\r\nrd_dev->rd_page_count = arg;\r\npr_debug("RAMDISK: Referencing Page"\r\n" Count: %u\n", rd_dev->rd_page_count);\r\nrd_dev->rd_flags |= RDF_HAS_PAGE_COUNT;\r\nbreak;\r\ncase Opt_rd_nullio:\r\nmatch_int(args, &arg);\r\nif (arg != 1)\r\nbreak;\r\npr_debug("RAMDISK: Setting NULLIO flag: %d\n", arg);\r\nrd_dev->rd_flags |= RDF_NULLIO;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nkfree(orig);\r\nreturn (!ret) ? count : ret;\r\n}\r\nstatic ssize_t rd_show_configfs_dev_params(struct se_device *dev, char *b)\r\n{\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nssize_t bl = sprintf(b, "TCM RamDisk ID: %u RamDisk Makeup: rd_mcp\n",\r\nrd_dev->rd_dev_id);\r\nbl += sprintf(b + bl, " PAGES/PAGE_SIZE: %u*%lu"\r\n" SG_table_count: %u nullio: %d\n", rd_dev->rd_page_count,\r\nPAGE_SIZE, rd_dev->sg_table_count,\r\n!!(rd_dev->rd_flags & RDF_NULLIO));\r\nreturn bl;\r\n}\r\nstatic sector_t rd_get_blocks(struct se_device *dev)\r\n{\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nunsigned long long blocks_long = ((rd_dev->rd_page_count * PAGE_SIZE) /\r\ndev->dev_attrib.block_size) - 1;\r\nreturn blocks_long;\r\n}\r\nstatic int rd_init_prot(struct se_device *dev)\r\n{\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nif (!dev->dev_attrib.pi_prot_type)\r\nreturn 0;\r\nreturn rd_build_prot_space(rd_dev, dev->prot_length,\r\ndev->dev_attrib.block_size);\r\n}\r\nstatic void rd_free_prot(struct se_device *dev)\r\n{\r\nstruct rd_dev *rd_dev = RD_DEV(dev);\r\nrd_release_prot_space(rd_dev);\r\n}\r\nstatic sense_reason_t\r\nrd_parse_cdb(struct se_cmd *cmd)\r\n{\r\nreturn sbc_parse_cdb(cmd, &rd_sbc_ops);\r\n}\r\nint __init rd_module_init(void)\r\n{\r\nreturn transport_backend_register(&rd_mcp_ops);\r\n}\r\nvoid rd_module_exit(void)\r\n{\r\ntarget_backend_unregister(&rd_mcp_ops);\r\n}
