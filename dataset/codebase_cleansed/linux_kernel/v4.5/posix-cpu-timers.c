void update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new)\r\n{\r\ncputime_t cputime = secs_to_cputime(rlim_new);\r\nspin_lock_irq(&task->sighand->siglock);\r\nset_process_cpu_timer(task, CPUCLOCK_PROF, &cputime, NULL);\r\nspin_unlock_irq(&task->sighand->siglock);\r\n}\r\nstatic int check_clock(const clockid_t which_clock)\r\n{\r\nint error = 0;\r\nstruct task_struct *p;\r\nconst pid_t pid = CPUCLOCK_PID(which_clock);\r\nif (CPUCLOCK_WHICH(which_clock) >= CPUCLOCK_MAX)\r\nreturn -EINVAL;\r\nif (pid == 0)\r\nreturn 0;\r\nrcu_read_lock();\r\np = find_task_by_vpid(pid);\r\nif (!p || !(CPUCLOCK_PERTHREAD(which_clock) ?\r\nsame_thread_group(p, current) : has_group_leader_pid(p))) {\r\nerror = -EINVAL;\r\n}\r\nrcu_read_unlock();\r\nreturn error;\r\n}\r\nstatic inline unsigned long long\r\ntimespec_to_sample(const clockid_t which_clock, const struct timespec *tp)\r\n{\r\nunsigned long long ret;\r\nret = 0;\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\r\nret = (unsigned long long)tp->tv_sec * NSEC_PER_SEC + tp->tv_nsec;\r\n} else {\r\nret = cputime_to_expires(timespec_to_cputime(tp));\r\n}\r\nreturn ret;\r\n}\r\nstatic void sample_to_timespec(const clockid_t which_clock,\r\nunsigned long long expires,\r\nstruct timespec *tp)\r\n{\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED)\r\n*tp = ns_to_timespec(expires);\r\nelse\r\ncputime_to_timespec((__force cputime_t)expires, tp);\r\n}\r\nstatic void bump_cpu_timer(struct k_itimer *timer,\r\nunsigned long long now)\r\n{\r\nint i;\r\nunsigned long long delta, incr;\r\nif (timer->it.cpu.incr == 0)\r\nreturn;\r\nif (now < timer->it.cpu.expires)\r\nreturn;\r\nincr = timer->it.cpu.incr;\r\ndelta = now + incr - timer->it.cpu.expires;\r\nfor (i = 0; incr < delta - incr; i++)\r\nincr = incr << 1;\r\nfor (; i >= 0; incr >>= 1, i--) {\r\nif (delta < incr)\r\ncontinue;\r\ntimer->it.cpu.expires += incr;\r\ntimer->it_overrun += 1 << i;\r\ndelta -= incr;\r\n}\r\n}\r\nstatic inline int task_cputime_zero(const struct task_cputime *cputime)\r\n{\r\nif (!cputime->utime && !cputime->stime && !cputime->sum_exec_runtime)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline unsigned long long prof_ticks(struct task_struct *p)\r\n{\r\ncputime_t utime, stime;\r\ntask_cputime(p, &utime, &stime);\r\nreturn cputime_to_expires(utime + stime);\r\n}\r\nstatic inline unsigned long long virt_ticks(struct task_struct *p)\r\n{\r\ncputime_t utime;\r\ntask_cputime(p, &utime, NULL);\r\nreturn cputime_to_expires(utime);\r\n}\r\nstatic int\r\nposix_cpu_clock_getres(const clockid_t which_clock, struct timespec *tp)\r\n{\r\nint error = check_clock(which_clock);\r\nif (!error) {\r\ntp->tv_sec = 0;\r\ntp->tv_nsec = ((NSEC_PER_SEC + HZ - 1) / HZ);\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\r\ntp->tv_nsec = 1;\r\n}\r\n}\r\nreturn error;\r\n}\r\nstatic int\r\nposix_cpu_clock_set(const clockid_t which_clock, const struct timespec *tp)\r\n{\r\nint error = check_clock(which_clock);\r\nif (error == 0) {\r\nerror = -EPERM;\r\n}\r\nreturn error;\r\n}\r\nstatic int cpu_clock_sample(const clockid_t which_clock, struct task_struct *p,\r\nunsigned long long *sample)\r\n{\r\nswitch (CPUCLOCK_WHICH(which_clock)) {\r\ndefault:\r\nreturn -EINVAL;\r\ncase CPUCLOCK_PROF:\r\n*sample = prof_ticks(p);\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\n*sample = virt_ticks(p);\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\n*sample = task_sched_runtime(p);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void __update_gt_cputime(atomic64_t *cputime, u64 sum_cputime)\r\n{\r\nu64 curr_cputime;\r\nretry:\r\ncurr_cputime = atomic64_read(cputime);\r\nif (sum_cputime > curr_cputime) {\r\nif (atomic64_cmpxchg(cputime, curr_cputime, sum_cputime) != curr_cputime)\r\ngoto retry;\r\n}\r\n}\r\nstatic void update_gt_cputime(struct task_cputime_atomic *cputime_atomic, struct task_cputime *sum)\r\n{\r\n__update_gt_cputime(&cputime_atomic->utime, sum->utime);\r\n__update_gt_cputime(&cputime_atomic->stime, sum->stime);\r\n__update_gt_cputime(&cputime_atomic->sum_exec_runtime, sum->sum_exec_runtime);\r\n}\r\nstatic inline void sample_cputime_atomic(struct task_cputime *times,\r\nstruct task_cputime_atomic *atomic_times)\r\n{\r\ntimes->utime = atomic64_read(&atomic_times->utime);\r\ntimes->stime = atomic64_read(&atomic_times->stime);\r\ntimes->sum_exec_runtime = atomic64_read(&atomic_times->sum_exec_runtime);\r\n}\r\nvoid thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)\r\n{\r\nstruct thread_group_cputimer *cputimer = &tsk->signal->cputimer;\r\nstruct task_cputime sum;\r\nif (!READ_ONCE(cputimer->running)) {\r\nthread_group_cputime(tsk, &sum);\r\nupdate_gt_cputime(&cputimer->cputime_atomic, &sum);\r\nWRITE_ONCE(cputimer->running, true);\r\n}\r\nsample_cputime_atomic(times, &cputimer->cputime_atomic);\r\n}\r\nstatic int cpu_clock_sample_group(const clockid_t which_clock,\r\nstruct task_struct *p,\r\nunsigned long long *sample)\r\n{\r\nstruct task_cputime cputime;\r\nswitch (CPUCLOCK_WHICH(which_clock)) {\r\ndefault:\r\nreturn -EINVAL;\r\ncase CPUCLOCK_PROF:\r\nthread_group_cputime(p, &cputime);\r\n*sample = cputime_to_expires(cputime.utime + cputime.stime);\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\nthread_group_cputime(p, &cputime);\r\n*sample = cputime_to_expires(cputime.utime);\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\nthread_group_cputime(p, &cputime);\r\n*sample = cputime.sum_exec_runtime;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int posix_cpu_clock_get_task(struct task_struct *tsk,\r\nconst clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nint err = -EINVAL;\r\nunsigned long long rtn;\r\nif (CPUCLOCK_PERTHREAD(which_clock)) {\r\nif (same_thread_group(tsk, current))\r\nerr = cpu_clock_sample(which_clock, tsk, &rtn);\r\n} else {\r\nif (tsk == current || thread_group_leader(tsk))\r\nerr = cpu_clock_sample_group(which_clock, tsk, &rtn);\r\n}\r\nif (!err)\r\nsample_to_timespec(which_clock, rtn, tp);\r\nreturn err;\r\n}\r\nstatic int posix_cpu_clock_get(const clockid_t which_clock, struct timespec *tp)\r\n{\r\nconst pid_t pid = CPUCLOCK_PID(which_clock);\r\nint err = -EINVAL;\r\nif (pid == 0) {\r\nerr = posix_cpu_clock_get_task(current, which_clock, tp);\r\n} else {\r\nstruct task_struct *p;\r\nrcu_read_lock();\r\np = find_task_by_vpid(pid);\r\nif (p)\r\nerr = posix_cpu_clock_get_task(p, which_clock, tp);\r\nrcu_read_unlock();\r\n}\r\nreturn err;\r\n}\r\nstatic int posix_cpu_timer_create(struct k_itimer *new_timer)\r\n{\r\nint ret = 0;\r\nconst pid_t pid = CPUCLOCK_PID(new_timer->it_clock);\r\nstruct task_struct *p;\r\nif (CPUCLOCK_WHICH(new_timer->it_clock) >= CPUCLOCK_MAX)\r\nreturn -EINVAL;\r\nINIT_LIST_HEAD(&new_timer->it.cpu.entry);\r\nrcu_read_lock();\r\nif (CPUCLOCK_PERTHREAD(new_timer->it_clock)) {\r\nif (pid == 0) {\r\np = current;\r\n} else {\r\np = find_task_by_vpid(pid);\r\nif (p && !same_thread_group(p, current))\r\np = NULL;\r\n}\r\n} else {\r\nif (pid == 0) {\r\np = current->group_leader;\r\n} else {\r\np = find_task_by_vpid(pid);\r\nif (p && !has_group_leader_pid(p))\r\np = NULL;\r\n}\r\n}\r\nnew_timer->it.cpu.task = p;\r\nif (p) {\r\nget_task_struct(p);\r\n} else {\r\nret = -EINVAL;\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int posix_cpu_timer_del(struct k_itimer *timer)\r\n{\r\nint ret = 0;\r\nunsigned long flags;\r\nstruct sighand_struct *sighand;\r\nstruct task_struct *p = timer->it.cpu.task;\r\nWARN_ON_ONCE(p == NULL);\r\nsighand = lock_task_sighand(p, &flags);\r\nif (unlikely(sighand == NULL)) {\r\nWARN_ON_ONCE(!list_empty(&timer->it.cpu.entry));\r\n} else {\r\nif (timer->it.cpu.firing)\r\nret = TIMER_RETRY;\r\nelse\r\nlist_del(&timer->it.cpu.entry);\r\nunlock_task_sighand(p, &flags);\r\n}\r\nif (!ret)\r\nput_task_struct(p);\r\nreturn ret;\r\n}\r\nstatic void cleanup_timers_list(struct list_head *head)\r\n{\r\nstruct cpu_timer_list *timer, *next;\r\nlist_for_each_entry_safe(timer, next, head, entry)\r\nlist_del_init(&timer->entry);\r\n}\r\nstatic void cleanup_timers(struct list_head *head)\r\n{\r\ncleanup_timers_list(head);\r\ncleanup_timers_list(++head);\r\ncleanup_timers_list(++head);\r\n}\r\nvoid posix_cpu_timers_exit(struct task_struct *tsk)\r\n{\r\nadd_device_randomness((const void*) &tsk->se.sum_exec_runtime,\r\nsizeof(unsigned long long));\r\ncleanup_timers(tsk->cpu_timers);\r\n}\r\nvoid posix_cpu_timers_exit_group(struct task_struct *tsk)\r\n{\r\ncleanup_timers(tsk->signal->cpu_timers);\r\n}\r\nstatic inline int expires_gt(cputime_t expires, cputime_t new_exp)\r\n{\r\nreturn expires == 0 || expires > new_exp;\r\n}\r\nstatic void arm_timer(struct k_itimer *timer)\r\n{\r\nstruct task_struct *p = timer->it.cpu.task;\r\nstruct list_head *head, *listpos;\r\nstruct task_cputime *cputime_expires;\r\nstruct cpu_timer_list *const nt = &timer->it.cpu;\r\nstruct cpu_timer_list *next;\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\nhead = p->cpu_timers;\r\ncputime_expires = &p->cputime_expires;\r\n} else {\r\nhead = p->signal->cpu_timers;\r\ncputime_expires = &p->signal->cputime_expires;\r\n}\r\nhead += CPUCLOCK_WHICH(timer->it_clock);\r\nlistpos = head;\r\nlist_for_each_entry(next, head, entry) {\r\nif (nt->expires < next->expires)\r\nbreak;\r\nlistpos = &next->entry;\r\n}\r\nlist_add(&nt->entry, listpos);\r\nif (listpos == head) {\r\nunsigned long long exp = nt->expires;\r\nswitch (CPUCLOCK_WHICH(timer->it_clock)) {\r\ncase CPUCLOCK_PROF:\r\nif (expires_gt(cputime_expires->prof_exp, expires_to_cputime(exp)))\r\ncputime_expires->prof_exp = expires_to_cputime(exp);\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\nif (expires_gt(cputime_expires->virt_exp, expires_to_cputime(exp)))\r\ncputime_expires->virt_exp = expires_to_cputime(exp);\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\nif (cputime_expires->sched_exp == 0 ||\r\ncputime_expires->sched_exp > exp)\r\ncputime_expires->sched_exp = exp;\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void cpu_timer_fire(struct k_itimer *timer)\r\n{\r\nif ((timer->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE) {\r\ntimer->it.cpu.expires = 0;\r\n} else if (unlikely(timer->sigq == NULL)) {\r\nwake_up_process(timer->it_process);\r\ntimer->it.cpu.expires = 0;\r\n} else if (timer->it.cpu.incr == 0) {\r\nposix_timer_event(timer, 0);\r\ntimer->it.cpu.expires = 0;\r\n} else if (posix_timer_event(timer, ++timer->it_requeue_pending)) {\r\nposix_cpu_timer_schedule(timer);\r\n}\r\n}\r\nstatic int cpu_timer_sample_group(const clockid_t which_clock,\r\nstruct task_struct *p,\r\nunsigned long long *sample)\r\n{\r\nstruct task_cputime cputime;\r\nthread_group_cputimer(p, &cputime);\r\nswitch (CPUCLOCK_WHICH(which_clock)) {\r\ndefault:\r\nreturn -EINVAL;\r\ncase CPUCLOCK_PROF:\r\n*sample = cputime_to_expires(cputime.utime + cputime.stime);\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\n*sample = cputime_to_expires(cputime.utime);\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\n*sample = cputime.sum_exec_runtime;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic void nohz_kick_work_fn(struct work_struct *work)\r\n{\r\ntick_nohz_full_kick_all();\r\n}\r\nstatic void posix_cpu_timer_kick_nohz(void)\r\n{\r\nif (context_tracking_is_enabled())\r\nschedule_work(&nohz_kick_work);\r\n}\r\nbool posix_cpu_timers_can_stop_tick(struct task_struct *tsk)\r\n{\r\nif (!task_cputime_zero(&tsk->cputime_expires))\r\nreturn false;\r\nif (READ_ONCE(tsk->signal->cputimer.running))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic inline void posix_cpu_timer_kick_nohz(void) { }\r\nstatic int posix_cpu_timer_set(struct k_itimer *timer, int timer_flags,\r\nstruct itimerspec *new, struct itimerspec *old)\r\n{\r\nunsigned long flags;\r\nstruct sighand_struct *sighand;\r\nstruct task_struct *p = timer->it.cpu.task;\r\nunsigned long long old_expires, new_expires, old_incr, val;\r\nint ret;\r\nWARN_ON_ONCE(p == NULL);\r\nnew_expires = timespec_to_sample(timer->it_clock, &new->it_value);\r\nsighand = lock_task_sighand(p, &flags);\r\nif (unlikely(sighand == NULL)) {\r\nreturn -ESRCH;\r\n}\r\nWARN_ON_ONCE(!irqs_disabled());\r\nret = 0;\r\nold_incr = timer->it.cpu.incr;\r\nold_expires = timer->it.cpu.expires;\r\nif (unlikely(timer->it.cpu.firing)) {\r\ntimer->it.cpu.firing = -1;\r\nret = TIMER_RETRY;\r\n} else\r\nlist_del_init(&timer->it.cpu.entry);\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\ncpu_clock_sample(timer->it_clock, p, &val);\r\n} else {\r\ncpu_timer_sample_group(timer->it_clock, p, &val);\r\n}\r\nif (old) {\r\nif (old_expires == 0) {\r\nold->it_value.tv_sec = 0;\r\nold->it_value.tv_nsec = 0;\r\n} else {\r\nbump_cpu_timer(timer, val);\r\nif (val < timer->it.cpu.expires) {\r\nold_expires = timer->it.cpu.expires - val;\r\nsample_to_timespec(timer->it_clock,\r\nold_expires,\r\n&old->it_value);\r\n} else {\r\nold->it_value.tv_nsec = 1;\r\nold->it_value.tv_sec = 0;\r\n}\r\n}\r\n}\r\nif (unlikely(ret)) {\r\nunlock_task_sighand(p, &flags);\r\ngoto out;\r\n}\r\nif (new_expires != 0 && !(timer_flags & TIMER_ABSTIME)) {\r\nnew_expires += val;\r\n}\r\ntimer->it.cpu.expires = new_expires;\r\nif (new_expires != 0 && val < new_expires) {\r\narm_timer(timer);\r\n}\r\nunlock_task_sighand(p, &flags);\r\ntimer->it.cpu.incr = timespec_to_sample(timer->it_clock,\r\n&new->it_interval);\r\ntimer->it_requeue_pending = (timer->it_requeue_pending + 2) &\r\n~REQUEUE_PENDING;\r\ntimer->it_overrun_last = 0;\r\ntimer->it_overrun = -1;\r\nif (new_expires != 0 && !(val < new_expires)) {\r\ncpu_timer_fire(timer);\r\n}\r\nret = 0;\r\nout:\r\nif (old) {\r\nsample_to_timespec(timer->it_clock,\r\nold_incr, &old->it_interval);\r\n}\r\nif (!ret)\r\nposix_cpu_timer_kick_nohz();\r\nreturn ret;\r\n}\r\nstatic void posix_cpu_timer_get(struct k_itimer *timer, struct itimerspec *itp)\r\n{\r\nunsigned long long now;\r\nstruct task_struct *p = timer->it.cpu.task;\r\nWARN_ON_ONCE(p == NULL);\r\nsample_to_timespec(timer->it_clock,\r\ntimer->it.cpu.incr, &itp->it_interval);\r\nif (timer->it.cpu.expires == 0) {\r\nitp->it_value.tv_sec = itp->it_value.tv_nsec = 0;\r\nreturn;\r\n}\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\ncpu_clock_sample(timer->it_clock, p, &now);\r\n} else {\r\nstruct sighand_struct *sighand;\r\nunsigned long flags;\r\nsighand = lock_task_sighand(p, &flags);\r\nif (unlikely(sighand == NULL)) {\r\ntimer->it.cpu.expires = 0;\r\nsample_to_timespec(timer->it_clock, timer->it.cpu.expires,\r\n&itp->it_value);\r\n} else {\r\ncpu_timer_sample_group(timer->it_clock, p, &now);\r\nunlock_task_sighand(p, &flags);\r\n}\r\n}\r\nif (now < timer->it.cpu.expires) {\r\nsample_to_timespec(timer->it_clock,\r\ntimer->it.cpu.expires - now,\r\n&itp->it_value);\r\n} else {\r\nitp->it_value.tv_nsec = 1;\r\nitp->it_value.tv_sec = 0;\r\n}\r\n}\r\nstatic unsigned long long\r\ncheck_timers_list(struct list_head *timers,\r\nstruct list_head *firing,\r\nunsigned long long curr)\r\n{\r\nint maxfire = 20;\r\nwhile (!list_empty(timers)) {\r\nstruct cpu_timer_list *t;\r\nt = list_first_entry(timers, struct cpu_timer_list, entry);\r\nif (!--maxfire || curr < t->expires)\r\nreturn t->expires;\r\nt->firing = 1;\r\nlist_move_tail(&t->entry, firing);\r\n}\r\nreturn 0;\r\n}\r\nstatic void check_thread_timers(struct task_struct *tsk,\r\nstruct list_head *firing)\r\n{\r\nstruct list_head *timers = tsk->cpu_timers;\r\nstruct signal_struct *const sig = tsk->signal;\r\nstruct task_cputime *tsk_expires = &tsk->cputime_expires;\r\nunsigned long long expires;\r\nunsigned long soft;\r\nif (task_cputime_zero(&tsk->cputime_expires))\r\nreturn;\r\nexpires = check_timers_list(timers, firing, prof_ticks(tsk));\r\ntsk_expires->prof_exp = expires_to_cputime(expires);\r\nexpires = check_timers_list(++timers, firing, virt_ticks(tsk));\r\ntsk_expires->virt_exp = expires_to_cputime(expires);\r\ntsk_expires->sched_exp = check_timers_list(++timers, firing,\r\ntsk->se.sum_exec_runtime);\r\nsoft = READ_ONCE(sig->rlim[RLIMIT_RTTIME].rlim_cur);\r\nif (soft != RLIM_INFINITY) {\r\nunsigned long hard =\r\nREAD_ONCE(sig->rlim[RLIMIT_RTTIME].rlim_max);\r\nif (hard != RLIM_INFINITY &&\r\ntsk->rt.timeout > DIV_ROUND_UP(hard, USEC_PER_SEC/HZ)) {\r\n__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\r\nreturn;\r\n}\r\nif (tsk->rt.timeout > DIV_ROUND_UP(soft, USEC_PER_SEC/HZ)) {\r\nif (soft < hard) {\r\nsoft += USEC_PER_SEC;\r\nsig->rlim[RLIMIT_RTTIME].rlim_cur = soft;\r\n}\r\nprintk(KERN_INFO\r\n"RT Watchdog Timeout: %s[%d]\n",\r\ntsk->comm, task_pid_nr(tsk));\r\n__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\r\n}\r\n}\r\n}\r\nstatic inline void stop_process_timers(struct signal_struct *sig)\r\n{\r\nstruct thread_group_cputimer *cputimer = &sig->cputimer;\r\nWRITE_ONCE(cputimer->running, false);\r\n}\r\nstatic void check_cpu_itimer(struct task_struct *tsk, struct cpu_itimer *it,\r\nunsigned long long *expires,\r\nunsigned long long cur_time, int signo)\r\n{\r\nif (!it->expires)\r\nreturn;\r\nif (cur_time >= it->expires) {\r\nif (it->incr) {\r\nit->expires += it->incr;\r\nit->error += it->incr_error;\r\nif (it->error >= onecputick) {\r\nit->expires -= cputime_one_jiffy;\r\nit->error -= onecputick;\r\n}\r\n} else {\r\nit->expires = 0;\r\n}\r\ntrace_itimer_expire(signo == SIGPROF ?\r\nITIMER_PROF : ITIMER_VIRTUAL,\r\ntsk->signal->leader_pid, cur_time);\r\n__group_send_sig_info(signo, SEND_SIG_PRIV, tsk);\r\n}\r\nif (it->expires && (!*expires || it->expires < *expires)) {\r\n*expires = it->expires;\r\n}\r\n}\r\nstatic void check_process_timers(struct task_struct *tsk,\r\nstruct list_head *firing)\r\n{\r\nstruct signal_struct *const sig = tsk->signal;\r\nunsigned long long utime, ptime, virt_expires, prof_expires;\r\nunsigned long long sum_sched_runtime, sched_expires;\r\nstruct list_head *timers = sig->cpu_timers;\r\nstruct task_cputime cputime;\r\nunsigned long soft;\r\nif (!READ_ONCE(tsk->signal->cputimer.running))\r\nreturn;\r\nsig->cputimer.checking_timer = true;\r\nthread_group_cputimer(tsk, &cputime);\r\nutime = cputime_to_expires(cputime.utime);\r\nptime = utime + cputime_to_expires(cputime.stime);\r\nsum_sched_runtime = cputime.sum_exec_runtime;\r\nprof_expires = check_timers_list(timers, firing, ptime);\r\nvirt_expires = check_timers_list(++timers, firing, utime);\r\nsched_expires = check_timers_list(++timers, firing, sum_sched_runtime);\r\ncheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_PROF], &prof_expires, ptime,\r\nSIGPROF);\r\ncheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_VIRT], &virt_expires, utime,\r\nSIGVTALRM);\r\nsoft = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);\r\nif (soft != RLIM_INFINITY) {\r\nunsigned long psecs = cputime_to_secs(ptime);\r\nunsigned long hard =\r\nREAD_ONCE(sig->rlim[RLIMIT_CPU].rlim_max);\r\ncputime_t x;\r\nif (psecs >= hard) {\r\n__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\r\nreturn;\r\n}\r\nif (psecs >= soft) {\r\n__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\r\nif (soft < hard) {\r\nsoft++;\r\nsig->rlim[RLIMIT_CPU].rlim_cur = soft;\r\n}\r\n}\r\nx = secs_to_cputime(soft);\r\nif (!prof_expires || x < prof_expires) {\r\nprof_expires = x;\r\n}\r\n}\r\nsig->cputime_expires.prof_exp = expires_to_cputime(prof_expires);\r\nsig->cputime_expires.virt_exp = expires_to_cputime(virt_expires);\r\nsig->cputime_expires.sched_exp = sched_expires;\r\nif (task_cputime_zero(&sig->cputime_expires))\r\nstop_process_timers(sig);\r\nsig->cputimer.checking_timer = false;\r\n}\r\nvoid posix_cpu_timer_schedule(struct k_itimer *timer)\r\n{\r\nstruct sighand_struct *sighand;\r\nunsigned long flags;\r\nstruct task_struct *p = timer->it.cpu.task;\r\nunsigned long long now;\r\nWARN_ON_ONCE(p == NULL);\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\ncpu_clock_sample(timer->it_clock, p, &now);\r\nbump_cpu_timer(timer, now);\r\nif (unlikely(p->exit_state))\r\ngoto out;\r\nsighand = lock_task_sighand(p, &flags);\r\nif (!sighand)\r\ngoto out;\r\n} else {\r\nsighand = lock_task_sighand(p, &flags);\r\nif (unlikely(sighand == NULL)) {\r\ntimer->it.cpu.expires = 0;\r\ngoto out;\r\n} else if (unlikely(p->exit_state) && thread_group_empty(p)) {\r\nunlock_task_sighand(p, &flags);\r\ngoto out;\r\n}\r\ncpu_timer_sample_group(timer->it_clock, p, &now);\r\nbump_cpu_timer(timer, now);\r\n}\r\nWARN_ON_ONCE(!irqs_disabled());\r\narm_timer(timer);\r\nunlock_task_sighand(p, &flags);\r\nposix_cpu_timer_kick_nohz();\r\nout:\r\ntimer->it_overrun_last = timer->it_overrun;\r\ntimer->it_overrun = -1;\r\n++timer->it_requeue_pending;\r\n}\r\nstatic inline int task_cputime_expired(const struct task_cputime *sample,\r\nconst struct task_cputime *expires)\r\n{\r\nif (expires->utime && sample->utime >= expires->utime)\r\nreturn 1;\r\nif (expires->stime && sample->utime + sample->stime >= expires->stime)\r\nreturn 1;\r\nif (expires->sum_exec_runtime != 0 &&\r\nsample->sum_exec_runtime >= expires->sum_exec_runtime)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline int fastpath_timer_check(struct task_struct *tsk)\r\n{\r\nstruct signal_struct *sig;\r\nif (!task_cputime_zero(&tsk->cputime_expires)) {\r\nstruct task_cputime task_sample;\r\ntask_cputime(tsk, &task_sample.utime, &task_sample.stime);\r\ntask_sample.sum_exec_runtime = tsk->se.sum_exec_runtime;\r\nif (task_cputime_expired(&task_sample, &tsk->cputime_expires))\r\nreturn 1;\r\n}\r\nsig = tsk->signal;\r\nif (READ_ONCE(sig->cputimer.running) &&\r\n!READ_ONCE(sig->cputimer.checking_timer)) {\r\nstruct task_cputime group_sample;\r\nsample_cputime_atomic(&group_sample, &sig->cputimer.cputime_atomic);\r\nif (task_cputime_expired(&group_sample, &sig->cputime_expires))\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid run_posix_cpu_timers(struct task_struct *tsk)\r\n{\r\nLIST_HEAD(firing);\r\nstruct k_itimer *timer, *next;\r\nunsigned long flags;\r\nWARN_ON_ONCE(!irqs_disabled());\r\nif (!fastpath_timer_check(tsk))\r\nreturn;\r\nif (!lock_task_sighand(tsk, &flags))\r\nreturn;\r\ncheck_thread_timers(tsk, &firing);\r\ncheck_process_timers(tsk, &firing);\r\nunlock_task_sighand(tsk, &flags);\r\nlist_for_each_entry_safe(timer, next, &firing, it.cpu.entry) {\r\nint cpu_firing;\r\nspin_lock(&timer->it_lock);\r\nlist_del_init(&timer->it.cpu.entry);\r\ncpu_firing = timer->it.cpu.firing;\r\ntimer->it.cpu.firing = 0;\r\nif (likely(cpu_firing >= 0))\r\ncpu_timer_fire(timer);\r\nspin_unlock(&timer->it_lock);\r\n}\r\n}\r\nvoid set_process_cpu_timer(struct task_struct *tsk, unsigned int clock_idx,\r\ncputime_t *newval, cputime_t *oldval)\r\n{\r\nunsigned long long now;\r\nWARN_ON_ONCE(clock_idx == CPUCLOCK_SCHED);\r\ncpu_timer_sample_group(clock_idx, tsk, &now);\r\nif (oldval) {\r\nif (*oldval) {\r\nif (*oldval <= now) {\r\n*oldval = cputime_one_jiffy;\r\n} else {\r\n*oldval -= now;\r\n}\r\n}\r\nif (!*newval)\r\ngoto out;\r\n*newval += now;\r\n}\r\nswitch (clock_idx) {\r\ncase CPUCLOCK_PROF:\r\nif (expires_gt(tsk->signal->cputime_expires.prof_exp, *newval))\r\ntsk->signal->cputime_expires.prof_exp = *newval;\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\nif (expires_gt(tsk->signal->cputime_expires.virt_exp, *newval))\r\ntsk->signal->cputime_expires.virt_exp = *newval;\r\nbreak;\r\n}\r\nout:\r\nposix_cpu_timer_kick_nohz();\r\n}\r\nstatic int do_cpu_nanosleep(const clockid_t which_clock, int flags,\r\nstruct timespec *rqtp, struct itimerspec *it)\r\n{\r\nstruct k_itimer timer;\r\nint error;\r\nmemset(&timer, 0, sizeof timer);\r\nspin_lock_init(&timer.it_lock);\r\ntimer.it_clock = which_clock;\r\ntimer.it_overrun = -1;\r\nerror = posix_cpu_timer_create(&timer);\r\ntimer.it_process = current;\r\nif (!error) {\r\nstatic struct itimerspec zero_it;\r\nmemset(it, 0, sizeof *it);\r\nit->it_value = *rqtp;\r\nspin_lock_irq(&timer.it_lock);\r\nerror = posix_cpu_timer_set(&timer, flags, it, NULL);\r\nif (error) {\r\nspin_unlock_irq(&timer.it_lock);\r\nreturn error;\r\n}\r\nwhile (!signal_pending(current)) {\r\nif (timer.it.cpu.expires == 0) {\r\nposix_cpu_timer_del(&timer);\r\nspin_unlock_irq(&timer.it_lock);\r\nreturn 0;\r\n}\r\n__set_current_state(TASK_INTERRUPTIBLE);\r\nspin_unlock_irq(&timer.it_lock);\r\nschedule();\r\nspin_lock_irq(&timer.it_lock);\r\n}\r\nsample_to_timespec(which_clock, timer.it.cpu.expires, rqtp);\r\nerror = posix_cpu_timer_set(&timer, 0, &zero_it, it);\r\nif (!error) {\r\nposix_cpu_timer_del(&timer);\r\n}\r\nspin_unlock_irq(&timer.it_lock);\r\nwhile (error == TIMER_RETRY) {\r\nspin_lock_irq(&timer.it_lock);\r\nerror = posix_cpu_timer_del(&timer);\r\nspin_unlock_irq(&timer.it_lock);\r\n}\r\nif ((it->it_value.tv_sec | it->it_value.tv_nsec) == 0) {\r\nreturn 0;\r\n}\r\nerror = -ERESTART_RESTARTBLOCK;\r\n}\r\nreturn error;\r\n}\r\nstatic int posix_cpu_nsleep(const clockid_t which_clock, int flags,\r\nstruct timespec *rqtp, struct timespec __user *rmtp)\r\n{\r\nstruct restart_block *restart_block = &current->restart_block;\r\nstruct itimerspec it;\r\nint error;\r\nif (CPUCLOCK_PERTHREAD(which_clock) &&\r\n(CPUCLOCK_PID(which_clock) == 0 ||\r\nCPUCLOCK_PID(which_clock) == current->pid))\r\nreturn -EINVAL;\r\nerror = do_cpu_nanosleep(which_clock, flags, rqtp, &it);\r\nif (error == -ERESTART_RESTARTBLOCK) {\r\nif (flags & TIMER_ABSTIME)\r\nreturn -ERESTARTNOHAND;\r\nif (rmtp && copy_to_user(rmtp, &it.it_value, sizeof *rmtp))\r\nreturn -EFAULT;\r\nrestart_block->fn = posix_cpu_nsleep_restart;\r\nrestart_block->nanosleep.clockid = which_clock;\r\nrestart_block->nanosleep.rmtp = rmtp;\r\nrestart_block->nanosleep.expires = timespec_to_ns(rqtp);\r\n}\r\nreturn error;\r\n}\r\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block)\r\n{\r\nclockid_t which_clock = restart_block->nanosleep.clockid;\r\nstruct timespec t;\r\nstruct itimerspec it;\r\nint error;\r\nt = ns_to_timespec(restart_block->nanosleep.expires);\r\nerror = do_cpu_nanosleep(which_clock, TIMER_ABSTIME, &t, &it);\r\nif (error == -ERESTART_RESTARTBLOCK) {\r\nstruct timespec __user *rmtp = restart_block->nanosleep.rmtp;\r\nif (rmtp && copy_to_user(rmtp, &it.it_value, sizeof *rmtp))\r\nreturn -EFAULT;\r\nrestart_block->nanosleep.expires = timespec_to_ns(&t);\r\n}\r\nreturn error;\r\n}\r\nstatic int process_cpu_clock_getres(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_getres(PROCESS_CLOCK, tp);\r\n}\r\nstatic int process_cpu_clock_get(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_get(PROCESS_CLOCK, tp);\r\n}\r\nstatic int process_cpu_timer_create(struct k_itimer *timer)\r\n{\r\ntimer->it_clock = PROCESS_CLOCK;\r\nreturn posix_cpu_timer_create(timer);\r\n}\r\nstatic int process_cpu_nsleep(const clockid_t which_clock, int flags,\r\nstruct timespec *rqtp,\r\nstruct timespec __user *rmtp)\r\n{\r\nreturn posix_cpu_nsleep(PROCESS_CLOCK, flags, rqtp, rmtp);\r\n}\r\nstatic long process_cpu_nsleep_restart(struct restart_block *restart_block)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int thread_cpu_clock_getres(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_getres(THREAD_CLOCK, tp);\r\n}\r\nstatic int thread_cpu_clock_get(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_get(THREAD_CLOCK, tp);\r\n}\r\nstatic int thread_cpu_timer_create(struct k_itimer *timer)\r\n{\r\ntimer->it_clock = THREAD_CLOCK;\r\nreturn posix_cpu_timer_create(timer);\r\n}\r\nstatic __init int init_posix_cpu_timers(void)\r\n{\r\nstruct k_clock process = {\r\n.clock_getres = process_cpu_clock_getres,\r\n.clock_get = process_cpu_clock_get,\r\n.timer_create = process_cpu_timer_create,\r\n.nsleep = process_cpu_nsleep,\r\n.nsleep_restart = process_cpu_nsleep_restart,\r\n};\r\nstruct k_clock thread = {\r\n.clock_getres = thread_cpu_clock_getres,\r\n.clock_get = thread_cpu_clock_get,\r\n.timer_create = thread_cpu_timer_create,\r\n};\r\nstruct timespec ts;\r\nposix_timers_register_clock(CLOCK_PROCESS_CPUTIME_ID, &process);\r\nposix_timers_register_clock(CLOCK_THREAD_CPUTIME_ID, &thread);\r\ncputime_to_timespec(cputime_one_jiffy, &ts);\r\nonecputick = ts.tv_nsec;\r\nWARN_ON(ts.tv_sec != 0);\r\nreturn 0;\r\n}
