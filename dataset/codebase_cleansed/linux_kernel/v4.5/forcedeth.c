static inline struct fe_priv *get_nvpriv(struct net_device *dev)\r\n{\r\nreturn netdev_priv(dev);\r\n}\r\nstatic inline u8 __iomem *get_hwbase(struct net_device *dev)\r\n{\r\nreturn ((struct fe_priv *)netdev_priv(dev))->base;\r\n}\r\nstatic inline void pci_push(u8 __iomem *base)\r\n{\r\nreadl(base);\r\n}\r\nstatic inline u32 nv_descr_getlength(struct ring_desc *prd, u32 v)\r\n{\r\nreturn le32_to_cpu(prd->flaglen)\r\n& ((v == DESC_VER_1) ? LEN_MASK_V1 : LEN_MASK_V2);\r\n}\r\nstatic inline u32 nv_descr_getlength_ex(struct ring_desc_ex *prd, u32 v)\r\n{\r\nreturn le32_to_cpu(prd->flaglen) & LEN_MASK_V2;\r\n}\r\nstatic bool nv_optimized(struct fe_priv *np)\r\n{\r\nif (np->desc_ver == DESC_VER_1 || np->desc_ver == DESC_VER_2)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int reg_delay(struct net_device *dev, int offset, u32 mask, u32 target,\r\nint delay, int delaymax)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\npci_push(base);\r\ndo {\r\nudelay(delay);\r\ndelaymax -= delay;\r\nif (delaymax < 0)\r\nreturn 1;\r\n} while ((readl(base + offset) & mask) != target);\r\nreturn 0;\r\n}\r\nstatic inline u32 dma_low(dma_addr_t addr)\r\n{\r\nreturn addr;\r\n}\r\nstatic inline u32 dma_high(dma_addr_t addr)\r\n{\r\nreturn addr>>31>>1;\r\n}\r\nstatic void setup_hw_rings(struct net_device *dev, int rxtx_flags)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nif (!nv_optimized(np)) {\r\nif (rxtx_flags & NV_SETUP_RX_RING)\r\nwritel(dma_low(np->ring_addr), base + NvRegRxRingPhysAddr);\r\nif (rxtx_flags & NV_SETUP_TX_RING)\r\nwritel(dma_low(np->ring_addr + np->rx_ring_size*sizeof(struct ring_desc)), base + NvRegTxRingPhysAddr);\r\n} else {\r\nif (rxtx_flags & NV_SETUP_RX_RING) {\r\nwritel(dma_low(np->ring_addr), base + NvRegRxRingPhysAddr);\r\nwritel(dma_high(np->ring_addr), base + NvRegRxRingPhysAddrHigh);\r\n}\r\nif (rxtx_flags & NV_SETUP_TX_RING) {\r\nwritel(dma_low(np->ring_addr + np->rx_ring_size*sizeof(struct ring_desc_ex)), base + NvRegTxRingPhysAddr);\r\nwritel(dma_high(np->ring_addr + np->rx_ring_size*sizeof(struct ring_desc_ex)), base + NvRegTxRingPhysAddrHigh);\r\n}\r\n}\r\n}\r\nstatic void free_rings(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nif (!nv_optimized(np)) {\r\nif (np->rx_ring.orig)\r\npci_free_consistent(np->pci_dev, sizeof(struct ring_desc) * (np->rx_ring_size + np->tx_ring_size),\r\nnp->rx_ring.orig, np->ring_addr);\r\n} else {\r\nif (np->rx_ring.ex)\r\npci_free_consistent(np->pci_dev, sizeof(struct ring_desc_ex) * (np->rx_ring_size + np->tx_ring_size),\r\nnp->rx_ring.ex, np->ring_addr);\r\n}\r\nkfree(np->rx_skb);\r\nkfree(np->tx_skb);\r\n}\r\nstatic int using_multi_irqs(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nif (!(np->msi_flags & NV_MSI_X_ENABLED) ||\r\n((np->msi_flags & NV_MSI_X_ENABLED) &&\r\n((np->msi_flags & NV_MSI_X_VECTORS_MASK) == 0x1)))\r\nreturn 0;\r\nelse\r\nreturn 1;\r\n}\r\nstatic void nv_txrx_gate(struct net_device *dev, bool gate)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 powerstate;\r\nif (!np->mac_in_use &&\r\n(np->driver_data & DEV_HAS_POWER_CNTRL)) {\r\npowerstate = readl(base + NvRegPowerState2);\r\nif (gate)\r\npowerstate |= NVREG_POWERSTATE2_GATE_CLOCKS;\r\nelse\r\npowerstate &= ~NVREG_POWERSTATE2_GATE_CLOCKS;\r\nwritel(powerstate, base + NvRegPowerState2);\r\n}\r\n}\r\nstatic void nv_enable_irq(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nif (!using_multi_irqs(dev)) {\r\nif (np->msi_flags & NV_MSI_X_ENABLED)\r\nenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector);\r\nelse\r\nenable_irq(np->pci_dev->irq);\r\n} else {\r\nenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector);\r\nenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector);\r\nenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector);\r\n}\r\n}\r\nstatic void nv_disable_irq(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nif (!using_multi_irqs(dev)) {\r\nif (np->msi_flags & NV_MSI_X_ENABLED)\r\ndisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector);\r\nelse\r\ndisable_irq(np->pci_dev->irq);\r\n} else {\r\ndisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector);\r\ndisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector);\r\ndisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector);\r\n}\r\n}\r\nstatic void nv_enable_hw_interrupts(struct net_device *dev, u32 mask)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nwritel(mask, base + NvRegIrqMask);\r\n}\r\nstatic void nv_disable_hw_interrupts(struct net_device *dev, u32 mask)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nif (np->msi_flags & NV_MSI_X_ENABLED) {\r\nwritel(mask, base + NvRegIrqMask);\r\n} else {\r\nif (np->msi_flags & NV_MSI_ENABLED)\r\nwritel(0, base + NvRegMSIIrqMask);\r\nwritel(0, base + NvRegIrqMask);\r\n}\r\n}\r\nstatic void nv_napi_enable(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nnapi_enable(&np->napi);\r\n}\r\nstatic void nv_napi_disable(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nnapi_disable(&np->napi);\r\n}\r\nstatic int mii_rw(struct net_device *dev, int addr, int miireg, int value)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 reg;\r\nint retval;\r\nwritel(NVREG_MIISTAT_MASK_RW, base + NvRegMIIStatus);\r\nreg = readl(base + NvRegMIIControl);\r\nif (reg & NVREG_MIICTL_INUSE) {\r\nwritel(NVREG_MIICTL_INUSE, base + NvRegMIIControl);\r\nudelay(NV_MIIBUSY_DELAY);\r\n}\r\nreg = (addr << NVREG_MIICTL_ADDRSHIFT) | miireg;\r\nif (value != MII_READ) {\r\nwritel(value, base + NvRegMIIData);\r\nreg |= NVREG_MIICTL_WRITE;\r\n}\r\nwritel(reg, base + NvRegMIIControl);\r\nif (reg_delay(dev, NvRegMIIControl, NVREG_MIICTL_INUSE, 0,\r\nNV_MIIPHY_DELAY, NV_MIIPHY_DELAYMAX)) {\r\nretval = -1;\r\n} else if (value != MII_READ) {\r\nretval = 0;\r\n} else if (readl(base + NvRegMIIStatus) & NVREG_MIISTAT_ERROR) {\r\nretval = -1;\r\n} else {\r\nretval = readl(base + NvRegMIIData);\r\n}\r\nreturn retval;\r\n}\r\nstatic int phy_reset(struct net_device *dev, u32 bmcr_setup)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 miicontrol;\r\nunsigned int tries = 0;\r\nmiicontrol = BMCR_RESET | bmcr_setup;\r\nif (mii_rw(dev, np->phyaddr, MII_BMCR, miicontrol))\r\nreturn -1;\r\nmsleep(500);\r\nwhile (miicontrol & BMCR_RESET) {\r\nusleep_range(10000, 20000);\r\nmiicontrol = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nif (tries++ > 100)\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int init_realtek_8211b(struct net_device *dev, struct fe_priv *np)\r\n{\r\nstatic const struct {\r\nint reg;\r\nint init;\r\n} ri[] = {\r\n{ PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1 },\r\n{ PHY_REALTEK_INIT_REG2, PHY_REALTEK_INIT2 },\r\n{ PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT3 },\r\n{ PHY_REALTEK_INIT_REG3, PHY_REALTEK_INIT4 },\r\n{ PHY_REALTEK_INIT_REG4, PHY_REALTEK_INIT5 },\r\n{ PHY_REALTEK_INIT_REG5, PHY_REALTEK_INIT6 },\r\n{ PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1 },\r\n};\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(ri); i++) {\r\nif (mii_rw(dev, np->phyaddr, ri[i].reg, ri[i].init))\r\nreturn PHY_ERROR;\r\n}\r\nreturn 0;\r\n}\r\nstatic int init_realtek_8211c(struct net_device *dev, struct fe_priv *np)\r\n{\r\nu32 reg;\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 powerstate = readl(base + NvRegPowerState2);\r\npowerstate |= NVREG_POWERSTATE2_PHY_RESET;\r\nwritel(powerstate, base + NvRegPowerState2);\r\nmsleep(25);\r\npowerstate &= ~NVREG_POWERSTATE2_PHY_RESET;\r\nwritel(powerstate, base + NvRegPowerState2);\r\nmsleep(25);\r\nreg = mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG6, MII_READ);\r\nreg |= PHY_REALTEK_INIT9;\r\nif (mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG6, reg))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT10))\r\nreturn PHY_ERROR;\r\nreg = mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG7, MII_READ);\r\nif (!(reg & PHY_REALTEK_INIT11)) {\r\nreg |= PHY_REALTEK_INIT11;\r\nif (mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG7, reg))\r\nreturn PHY_ERROR;\r\n}\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1))\r\nreturn PHY_ERROR;\r\nreturn 0;\r\n}\r\nstatic int init_realtek_8201(struct net_device *dev, struct fe_priv *np)\r\n{\r\nu32 phy_reserved;\r\nif (np->driver_data & DEV_NEED_PHY_INIT_FIX) {\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG6, MII_READ);\r\nphy_reserved |= PHY_REALTEK_INIT7;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG6, phy_reserved))\r\nreturn PHY_ERROR;\r\n}\r\nreturn 0;\r\n}\r\nstatic int init_realtek_8201_cross(struct net_device *dev, struct fe_priv *np)\r\n{\r\nu32 phy_reserved;\r\nif (phy_cross == NV_CROSSOVER_DETECTION_DISABLED) {\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT3))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG2, MII_READ);\r\nphy_reserved &= ~PHY_REALTEK_INIT_MSK1;\r\nphy_reserved |= PHY_REALTEK_INIT3;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG2, phy_reserved))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1))\r\nreturn PHY_ERROR;\r\n}\r\nreturn 0;\r\n}\r\nstatic int init_cicada(struct net_device *dev, struct fe_priv *np,\r\nu32 phyinterface)\r\n{\r\nu32 phy_reserved;\r\nif (phyinterface & PHY_RGMII) {\r\nphy_reserved = mii_rw(dev, np->phyaddr, MII_RESV1, MII_READ);\r\nphy_reserved &= ~(PHY_CICADA_INIT1 | PHY_CICADA_INIT2);\r\nphy_reserved |= (PHY_CICADA_INIT3 | PHY_CICADA_INIT4);\r\nif (mii_rw(dev, np->phyaddr, MII_RESV1, phy_reserved))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr, MII_NCONFIG, MII_READ);\r\nphy_reserved |= PHY_CICADA_INIT5;\r\nif (mii_rw(dev, np->phyaddr, MII_NCONFIG, phy_reserved))\r\nreturn PHY_ERROR;\r\n}\r\nphy_reserved = mii_rw(dev, np->phyaddr, MII_SREVISION, MII_READ);\r\nphy_reserved |= PHY_CICADA_INIT6;\r\nif (mii_rw(dev, np->phyaddr, MII_SREVISION, phy_reserved))\r\nreturn PHY_ERROR;\r\nreturn 0;\r\n}\r\nstatic int init_vitesse(struct net_device *dev, struct fe_priv *np)\r\n{\r\nu32 phy_reserved;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG1, PHY_VITESSE_INIT1))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT2))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG4, MII_READ);\r\nif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG4, phy_reserved))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG3, MII_READ);\r\nphy_reserved &= ~PHY_VITESSE_INIT_MSK1;\r\nphy_reserved |= PHY_VITESSE_INIT3;\r\nif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG3, phy_reserved))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT4))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT5))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG4, MII_READ);\r\nphy_reserved &= ~PHY_VITESSE_INIT_MSK1;\r\nphy_reserved |= PHY_VITESSE_INIT3;\r\nif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG4, phy_reserved))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG3, MII_READ);\r\nif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG3, phy_reserved))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT6))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT7))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG4, MII_READ);\r\nif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG4, phy_reserved))\r\nreturn PHY_ERROR;\r\nphy_reserved = mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG3, MII_READ);\r\nphy_reserved &= ~PHY_VITESSE_INIT_MSK2;\r\nphy_reserved |= PHY_VITESSE_INIT8;\r\nif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG3, phy_reserved))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT9))\r\nreturn PHY_ERROR;\r\nif (mii_rw(dev, np->phyaddr,\r\nPHY_VITESSE_INIT_REG1, PHY_VITESSE_INIT10))\r\nreturn PHY_ERROR;\r\nreturn 0;\r\n}\r\nstatic int phy_init(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 phyinterface;\r\nu32 mii_status, mii_control, mii_control_1000, reg;\r\nif (np->phy_model == PHY_MODEL_MARVELL_E3016) {\r\nreg = mii_rw(dev, np->phyaddr, MII_NCONFIG, MII_READ);\r\nreg &= ~PHY_MARVELL_E3016_INITMASK;\r\nif (mii_rw(dev, np->phyaddr, MII_NCONFIG, reg)) {\r\nnetdev_info(dev, "%s: phy write to errata reg failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n}\r\nif (np->phy_oui == PHY_OUI_REALTEK) {\r\nif (np->phy_model == PHY_MODEL_REALTEK_8211 &&\r\nnp->phy_rev == PHY_REV_REALTEK_8211B) {\r\nif (init_realtek_8211b(dev, np)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n} else if (np->phy_model == PHY_MODEL_REALTEK_8211 &&\r\nnp->phy_rev == PHY_REV_REALTEK_8211C) {\r\nif (init_realtek_8211c(dev, np)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n} else if (np->phy_model == PHY_MODEL_REALTEK_8201) {\r\nif (init_realtek_8201(dev, np)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n}\r\n}\r\nreg = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\r\nreg |= (ADVERTISE_10HALF | ADVERTISE_10FULL |\r\nADVERTISE_100HALF | ADVERTISE_100FULL |\r\nADVERTISE_PAUSE_ASYM | ADVERTISE_PAUSE_CAP);\r\nif (mii_rw(dev, np->phyaddr, MII_ADVERTISE, reg)) {\r\nnetdev_info(dev, "%s: phy write to advertise failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\nphyinterface = readl(base + NvRegPhyInterface);\r\nmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\r\nif (mii_status & PHY_GIGABIT) {\r\nnp->gigabit = PHY_GIGABIT;\r\nmii_control_1000 = mii_rw(dev, np->phyaddr,\r\nMII_CTRL1000, MII_READ);\r\nmii_control_1000 &= ~ADVERTISE_1000HALF;\r\nif (phyinterface & PHY_RGMII)\r\nmii_control_1000 |= ADVERTISE_1000FULL;\r\nelse\r\nmii_control_1000 &= ~ADVERTISE_1000FULL;\r\nif (mii_rw(dev, np->phyaddr, MII_CTRL1000, mii_control_1000)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n} else\r\nnp->gigabit = 0;\r\nmii_control = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nmii_control |= BMCR_ANENABLE;\r\nif (np->phy_oui == PHY_OUI_REALTEK &&\r\nnp->phy_model == PHY_MODEL_REALTEK_8211 &&\r\nnp->phy_rev == PHY_REV_REALTEK_8211C) {\r\nmii_control |= BMCR_ANRESTART;\r\nif (mii_rw(dev, np->phyaddr, MII_BMCR, mii_control)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n} else {\r\nif (phy_reset(dev, mii_control)) {\r\nnetdev_info(dev, "%s: phy reset failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n}\r\nif (np->phy_oui == PHY_OUI_CICADA) {\r\nif (init_cicada(dev, np, phyinterface)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n} else if (np->phy_oui == PHY_OUI_VITESSE) {\r\nif (init_vitesse(dev, np)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n} else if (np->phy_oui == PHY_OUI_REALTEK) {\r\nif (np->phy_model == PHY_MODEL_REALTEK_8211 &&\r\nnp->phy_rev == PHY_REV_REALTEK_8211B) {\r\nif (init_realtek_8211b(dev, np)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n} else if (np->phy_model == PHY_MODEL_REALTEK_8201) {\r\nif (init_realtek_8201(dev, np) ||\r\ninit_realtek_8201_cross(dev, np)) {\r\nnetdev_info(dev, "%s: phy init failed\n",\r\npci_name(np->pci_dev));\r\nreturn PHY_ERROR;\r\n}\r\n}\r\n}\r\nmii_rw(dev, np->phyaddr, MII_ADVERTISE, reg);\r\nmii_control = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nmii_control |= (BMCR_ANRESTART | BMCR_ANENABLE);\r\nif (phy_power_down)\r\nmii_control |= BMCR_PDOWN;\r\nif (mii_rw(dev, np->phyaddr, MII_BMCR, mii_control))\r\nreturn PHY_ERROR;\r\nreturn 0;\r\n}\r\nstatic void nv_start_rx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 rx_ctrl = readl(base + NvRegReceiverControl);\r\nif ((readl(base + NvRegReceiverControl) & NVREG_RCVCTL_START) && !np->mac_in_use) {\r\nrx_ctrl &= ~NVREG_RCVCTL_START;\r\nwritel(rx_ctrl, base + NvRegReceiverControl);\r\npci_push(base);\r\n}\r\nwritel(np->linkspeed, base + NvRegLinkSpeed);\r\npci_push(base);\r\nrx_ctrl |= NVREG_RCVCTL_START;\r\nif (np->mac_in_use)\r\nrx_ctrl &= ~NVREG_RCVCTL_RX_PATH_EN;\r\nwritel(rx_ctrl, base + NvRegReceiverControl);\r\npci_push(base);\r\n}\r\nstatic void nv_stop_rx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 rx_ctrl = readl(base + NvRegReceiverControl);\r\nif (!np->mac_in_use)\r\nrx_ctrl &= ~NVREG_RCVCTL_START;\r\nelse\r\nrx_ctrl |= NVREG_RCVCTL_RX_PATH_EN;\r\nwritel(rx_ctrl, base + NvRegReceiverControl);\r\nif (reg_delay(dev, NvRegReceiverStatus, NVREG_RCVSTAT_BUSY, 0,\r\nNV_RXSTOP_DELAY1, NV_RXSTOP_DELAY1MAX))\r\nnetdev_info(dev, "%s: ReceiverStatus remained busy\n",\r\n__func__);\r\nudelay(NV_RXSTOP_DELAY2);\r\nif (!np->mac_in_use)\r\nwritel(0, base + NvRegLinkSpeed);\r\n}\r\nstatic void nv_start_tx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 tx_ctrl = readl(base + NvRegTransmitterControl);\r\ntx_ctrl |= NVREG_XMITCTL_START;\r\nif (np->mac_in_use)\r\ntx_ctrl &= ~NVREG_XMITCTL_TX_PATH_EN;\r\nwritel(tx_ctrl, base + NvRegTransmitterControl);\r\npci_push(base);\r\n}\r\nstatic void nv_stop_tx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 tx_ctrl = readl(base + NvRegTransmitterControl);\r\nif (!np->mac_in_use)\r\ntx_ctrl &= ~NVREG_XMITCTL_START;\r\nelse\r\ntx_ctrl |= NVREG_XMITCTL_TX_PATH_EN;\r\nwritel(tx_ctrl, base + NvRegTransmitterControl);\r\nif (reg_delay(dev, NvRegTransmitterStatus, NVREG_XMITSTAT_BUSY, 0,\r\nNV_TXSTOP_DELAY1, NV_TXSTOP_DELAY1MAX))\r\nnetdev_info(dev, "%s: TransmitterStatus remained busy\n",\r\n__func__);\r\nudelay(NV_TXSTOP_DELAY2);\r\nif (!np->mac_in_use)\r\nwritel(readl(base + NvRegTransmitPoll) & NVREG_TRANSMITPOLL_MAC_ADDR_REV,\r\nbase + NvRegTransmitPoll);\r\n}\r\nstatic void nv_start_rxtx(struct net_device *dev)\r\n{\r\nnv_start_rx(dev);\r\nnv_start_tx(dev);\r\n}\r\nstatic void nv_stop_rxtx(struct net_device *dev)\r\n{\r\nnv_stop_rx(dev);\r\nnv_stop_tx(dev);\r\n}\r\nstatic void nv_txrx_reset(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nwritel(NVREG_TXRXCTL_BIT2 | NVREG_TXRXCTL_RESET | np->txrxctl_bits, base + NvRegTxRxControl);\r\npci_push(base);\r\nudelay(NV_TXRX_RESET_DELAY);\r\nwritel(NVREG_TXRXCTL_BIT2 | np->txrxctl_bits, base + NvRegTxRxControl);\r\npci_push(base);\r\n}\r\nstatic void nv_mac_reset(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 temp1, temp2, temp3;\r\nwritel(NVREG_TXRXCTL_BIT2 | NVREG_TXRXCTL_RESET | np->txrxctl_bits, base + NvRegTxRxControl);\r\npci_push(base);\r\ntemp1 = readl(base + NvRegMacAddrA);\r\ntemp2 = readl(base + NvRegMacAddrB);\r\ntemp3 = readl(base + NvRegTransmitPoll);\r\nwritel(NVREG_MAC_RESET_ASSERT, base + NvRegMacReset);\r\npci_push(base);\r\nudelay(NV_MAC_RESET_DELAY);\r\nwritel(0, base + NvRegMacReset);\r\npci_push(base);\r\nudelay(NV_MAC_RESET_DELAY);\r\nwritel(temp1, base + NvRegMacAddrA);\r\nwritel(temp2, base + NvRegMacAddrB);\r\nwritel(temp3, base + NvRegTransmitPoll);\r\nwritel(NVREG_TXRXCTL_BIT2 | np->txrxctl_bits, base + NvRegTxRxControl);\r\npci_push(base);\r\n}\r\nstatic void nv_update_stats(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nWARN_ONCE(in_irq(), "forcedeth: estats spin_lock(_bh) from top-half");\r\nassert_spin_locked(&np->hwstats_lock);\r\nnp->estats.tx_bytes += readl(base + NvRegTxCnt);\r\nnp->estats.tx_zero_rexmt += readl(base + NvRegTxZeroReXmt);\r\nnp->estats.tx_one_rexmt += readl(base + NvRegTxOneReXmt);\r\nnp->estats.tx_many_rexmt += readl(base + NvRegTxManyReXmt);\r\nnp->estats.tx_late_collision += readl(base + NvRegTxLateCol);\r\nnp->estats.tx_fifo_errors += readl(base + NvRegTxUnderflow);\r\nnp->estats.tx_carrier_errors += readl(base + NvRegTxLossCarrier);\r\nnp->estats.tx_excess_deferral += readl(base + NvRegTxExcessDef);\r\nnp->estats.tx_retry_error += readl(base + NvRegTxRetryErr);\r\nnp->estats.rx_frame_error += readl(base + NvRegRxFrameErr);\r\nnp->estats.rx_extra_byte += readl(base + NvRegRxExtraByte);\r\nnp->estats.rx_late_collision += readl(base + NvRegRxLateCol);\r\nnp->estats.rx_runt += readl(base + NvRegRxRunt);\r\nnp->estats.rx_frame_too_long += readl(base + NvRegRxFrameTooLong);\r\nnp->estats.rx_over_errors += readl(base + NvRegRxOverflow);\r\nnp->estats.rx_crc_errors += readl(base + NvRegRxFCSErr);\r\nnp->estats.rx_frame_align_error += readl(base + NvRegRxFrameAlignErr);\r\nnp->estats.rx_length_error += readl(base + NvRegRxLenErr);\r\nnp->estats.rx_unicast += readl(base + NvRegRxUnicast);\r\nnp->estats.rx_multicast += readl(base + NvRegRxMulticast);\r\nnp->estats.rx_broadcast += readl(base + NvRegRxBroadcast);\r\nnp->estats.rx_packets =\r\nnp->estats.rx_unicast +\r\nnp->estats.rx_multicast +\r\nnp->estats.rx_broadcast;\r\nnp->estats.rx_errors_total =\r\nnp->estats.rx_crc_errors +\r\nnp->estats.rx_over_errors +\r\nnp->estats.rx_frame_error +\r\n(np->estats.rx_frame_align_error - np->estats.rx_extra_byte) +\r\nnp->estats.rx_late_collision +\r\nnp->estats.rx_runt +\r\nnp->estats.rx_frame_too_long;\r\nnp->estats.tx_errors_total =\r\nnp->estats.tx_late_collision +\r\nnp->estats.tx_fifo_errors +\r\nnp->estats.tx_carrier_errors +\r\nnp->estats.tx_excess_deferral +\r\nnp->estats.tx_retry_error;\r\nif (np->driver_data & DEV_HAS_STATISTICS_V2) {\r\nnp->estats.tx_deferral += readl(base + NvRegTxDef);\r\nnp->estats.tx_packets += readl(base + NvRegTxFrame);\r\nnp->estats.rx_bytes += readl(base + NvRegRxCnt);\r\nnp->estats.tx_pause += readl(base + NvRegTxPause);\r\nnp->estats.rx_pause += readl(base + NvRegRxPause);\r\nnp->estats.rx_drop_frame += readl(base + NvRegRxDropFrame);\r\nnp->estats.rx_errors_total += np->estats.rx_drop_frame;\r\n}\r\nif (np->driver_data & DEV_HAS_STATISTICS_V3) {\r\nnp->estats.tx_unicast += readl(base + NvRegTxUnicast);\r\nnp->estats.tx_multicast += readl(base + NvRegTxMulticast);\r\nnp->estats.tx_broadcast += readl(base + NvRegTxBroadcast);\r\n}\r\n}\r\nstatic struct rtnl_link_stats64*\r\nnv_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *storage)\r\n__acquires(&netdev_priv(dev)->hwstats_lock\r\nstatic int nv_alloc_rx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nstruct ring_desc *less_rx;\r\nless_rx = np->get_rx.orig;\r\nif (less_rx-- == np->first_rx.orig)\r\nless_rx = np->last_rx.orig;\r\nwhile (np->put_rx.orig != less_rx) {\r\nstruct sk_buff *skb = netdev_alloc_skb(dev, np->rx_buf_sz + NV_RX_ALLOC_PAD);\r\nif (skb) {\r\nnp->put_rx_ctx->skb = skb;\r\nnp->put_rx_ctx->dma = pci_map_single(np->pci_dev,\r\nskb->data,\r\nskb_tailroom(skb),\r\nPCI_DMA_FROMDEVICE);\r\nif (pci_dma_mapping_error(np->pci_dev,\r\nnp->put_rx_ctx->dma)) {\r\nkfree_skb(skb);\r\ngoto packet_dropped;\r\n}\r\nnp->put_rx_ctx->dma_len = skb_tailroom(skb);\r\nnp->put_rx.orig->buf = cpu_to_le32(np->put_rx_ctx->dma);\r\nwmb();\r\nnp->put_rx.orig->flaglen = cpu_to_le32(np->rx_buf_sz | NV_RX_AVAIL);\r\nif (unlikely(np->put_rx.orig++ == np->last_rx.orig))\r\nnp->put_rx.orig = np->first_rx.orig;\r\nif (unlikely(np->put_rx_ctx++ == np->last_rx_ctx))\r\nnp->put_rx_ctx = np->first_rx_ctx;\r\n} else {\r\npacket_dropped:\r\nu64_stats_update_begin(&np->swstats_rx_syncp);\r\nnp->stat_rx_dropped++;\r\nu64_stats_update_end(&np->swstats_rx_syncp);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int nv_alloc_rx_optimized(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nstruct ring_desc_ex *less_rx;\r\nless_rx = np->get_rx.ex;\r\nif (less_rx-- == np->first_rx.ex)\r\nless_rx = np->last_rx.ex;\r\nwhile (np->put_rx.ex != less_rx) {\r\nstruct sk_buff *skb = netdev_alloc_skb(dev, np->rx_buf_sz + NV_RX_ALLOC_PAD);\r\nif (skb) {\r\nnp->put_rx_ctx->skb = skb;\r\nnp->put_rx_ctx->dma = pci_map_single(np->pci_dev,\r\nskb->data,\r\nskb_tailroom(skb),\r\nPCI_DMA_FROMDEVICE);\r\nif (pci_dma_mapping_error(np->pci_dev,\r\nnp->put_rx_ctx->dma)) {\r\nkfree_skb(skb);\r\ngoto packet_dropped;\r\n}\r\nnp->put_rx_ctx->dma_len = skb_tailroom(skb);\r\nnp->put_rx.ex->bufhigh = cpu_to_le32(dma_high(np->put_rx_ctx->dma));\r\nnp->put_rx.ex->buflow = cpu_to_le32(dma_low(np->put_rx_ctx->dma));\r\nwmb();\r\nnp->put_rx.ex->flaglen = cpu_to_le32(np->rx_buf_sz | NV_RX2_AVAIL);\r\nif (unlikely(np->put_rx.ex++ == np->last_rx.ex))\r\nnp->put_rx.ex = np->first_rx.ex;\r\nif (unlikely(np->put_rx_ctx++ == np->last_rx_ctx))\r\nnp->put_rx_ctx = np->first_rx_ctx;\r\n} else {\r\npacket_dropped:\r\nu64_stats_update_begin(&np->swstats_rx_syncp);\r\nnp->stat_rx_dropped++;\r\nu64_stats_update_end(&np->swstats_rx_syncp);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void nv_do_rx_refill(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nnapi_schedule(&np->napi);\r\n}\r\nstatic void nv_init_rx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nint i;\r\nnp->get_rx = np->put_rx = np->first_rx = np->rx_ring;\r\nif (!nv_optimized(np))\r\nnp->last_rx.orig = &np->rx_ring.orig[np->rx_ring_size-1];\r\nelse\r\nnp->last_rx.ex = &np->rx_ring.ex[np->rx_ring_size-1];\r\nnp->get_rx_ctx = np->put_rx_ctx = np->first_rx_ctx = np->rx_skb;\r\nnp->last_rx_ctx = &np->rx_skb[np->rx_ring_size-1];\r\nfor (i = 0; i < np->rx_ring_size; i++) {\r\nif (!nv_optimized(np)) {\r\nnp->rx_ring.orig[i].flaglen = 0;\r\nnp->rx_ring.orig[i].buf = 0;\r\n} else {\r\nnp->rx_ring.ex[i].flaglen = 0;\r\nnp->rx_ring.ex[i].txvlan = 0;\r\nnp->rx_ring.ex[i].bufhigh = 0;\r\nnp->rx_ring.ex[i].buflow = 0;\r\n}\r\nnp->rx_skb[i].skb = NULL;\r\nnp->rx_skb[i].dma = 0;\r\n}\r\n}\r\nstatic void nv_init_tx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nint i;\r\nnp->get_tx = np->put_tx = np->first_tx = np->tx_ring;\r\nif (!nv_optimized(np))\r\nnp->last_tx.orig = &np->tx_ring.orig[np->tx_ring_size-1];\r\nelse\r\nnp->last_tx.ex = &np->tx_ring.ex[np->tx_ring_size-1];\r\nnp->get_tx_ctx = np->put_tx_ctx = np->first_tx_ctx = np->tx_skb;\r\nnp->last_tx_ctx = &np->tx_skb[np->tx_ring_size-1];\r\nnetdev_reset_queue(np->dev);\r\nnp->tx_pkts_in_progress = 0;\r\nnp->tx_change_owner = NULL;\r\nnp->tx_end_flip = NULL;\r\nnp->tx_stop = 0;\r\nfor (i = 0; i < np->tx_ring_size; i++) {\r\nif (!nv_optimized(np)) {\r\nnp->tx_ring.orig[i].flaglen = 0;\r\nnp->tx_ring.orig[i].buf = 0;\r\n} else {\r\nnp->tx_ring.ex[i].flaglen = 0;\r\nnp->tx_ring.ex[i].txvlan = 0;\r\nnp->tx_ring.ex[i].bufhigh = 0;\r\nnp->tx_ring.ex[i].buflow = 0;\r\n}\r\nnp->tx_skb[i].skb = NULL;\r\nnp->tx_skb[i].dma = 0;\r\nnp->tx_skb[i].dma_len = 0;\r\nnp->tx_skb[i].dma_single = 0;\r\nnp->tx_skb[i].first_tx_desc = NULL;\r\nnp->tx_skb[i].next_tx_ctx = NULL;\r\n}\r\n}\r\nstatic int nv_init_ring(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nnv_init_tx(dev);\r\nnv_init_rx(dev);\r\nif (!nv_optimized(np))\r\nreturn nv_alloc_rx(dev);\r\nelse\r\nreturn nv_alloc_rx_optimized(dev);\r\n}\r\nstatic void nv_unmap_txskb(struct fe_priv *np, struct nv_skb_map *tx_skb)\r\n{\r\nif (tx_skb->dma) {\r\nif (tx_skb->dma_single)\r\npci_unmap_single(np->pci_dev, tx_skb->dma,\r\ntx_skb->dma_len,\r\nPCI_DMA_TODEVICE);\r\nelse\r\npci_unmap_page(np->pci_dev, tx_skb->dma,\r\ntx_skb->dma_len,\r\nPCI_DMA_TODEVICE);\r\ntx_skb->dma = 0;\r\n}\r\n}\r\nstatic int nv_release_txskb(struct fe_priv *np, struct nv_skb_map *tx_skb)\r\n{\r\nnv_unmap_txskb(np, tx_skb);\r\nif (tx_skb->skb) {\r\ndev_kfree_skb_any(tx_skb->skb);\r\ntx_skb->skb = NULL;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void nv_drain_tx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nunsigned int i;\r\nfor (i = 0; i < np->tx_ring_size; i++) {\r\nif (!nv_optimized(np)) {\r\nnp->tx_ring.orig[i].flaglen = 0;\r\nnp->tx_ring.orig[i].buf = 0;\r\n} else {\r\nnp->tx_ring.ex[i].flaglen = 0;\r\nnp->tx_ring.ex[i].txvlan = 0;\r\nnp->tx_ring.ex[i].bufhigh = 0;\r\nnp->tx_ring.ex[i].buflow = 0;\r\n}\r\nif (nv_release_txskb(np, &np->tx_skb[i])) {\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_dropped++;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\n}\r\nnp->tx_skb[i].dma = 0;\r\nnp->tx_skb[i].dma_len = 0;\r\nnp->tx_skb[i].dma_single = 0;\r\nnp->tx_skb[i].first_tx_desc = NULL;\r\nnp->tx_skb[i].next_tx_ctx = NULL;\r\n}\r\nnp->tx_pkts_in_progress = 0;\r\nnp->tx_change_owner = NULL;\r\nnp->tx_end_flip = NULL;\r\n}\r\nstatic void nv_drain_rx(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nint i;\r\nfor (i = 0; i < np->rx_ring_size; i++) {\r\nif (!nv_optimized(np)) {\r\nnp->rx_ring.orig[i].flaglen = 0;\r\nnp->rx_ring.orig[i].buf = 0;\r\n} else {\r\nnp->rx_ring.ex[i].flaglen = 0;\r\nnp->rx_ring.ex[i].txvlan = 0;\r\nnp->rx_ring.ex[i].bufhigh = 0;\r\nnp->rx_ring.ex[i].buflow = 0;\r\n}\r\nwmb();\r\nif (np->rx_skb[i].skb) {\r\npci_unmap_single(np->pci_dev, np->rx_skb[i].dma,\r\n(skb_end_pointer(np->rx_skb[i].skb) -\r\nnp->rx_skb[i].skb->data),\r\nPCI_DMA_FROMDEVICE);\r\ndev_kfree_skb(np->rx_skb[i].skb);\r\nnp->rx_skb[i].skb = NULL;\r\n}\r\n}\r\n}\r\nstatic void nv_drain_rxtx(struct net_device *dev)\r\n{\r\nnv_drain_tx(dev);\r\nnv_drain_rx(dev);\r\n}\r\nstatic inline u32 nv_get_empty_tx_slots(struct fe_priv *np)\r\n{\r\nreturn (u32)(np->tx_ring_size - ((np->tx_ring_size + (np->put_tx_ctx - np->get_tx_ctx)) % np->tx_ring_size));\r\n}\r\nstatic void nv_legacybackoff_reseed(struct net_device *dev)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 reg;\r\nu32 low;\r\nint tx_status = 0;\r\nreg = readl(base + NvRegSlotTime) & ~NVREG_SLOTTIME_MASK;\r\nget_random_bytes(&low, sizeof(low));\r\nreg |= low & NVREG_SLOTTIME_MASK;\r\ntx_status = readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_START;\r\nif (tx_status)\r\nnv_stop_tx(dev);\r\nnv_stop_rx(dev);\r\nwritel(reg, base + NvRegSlotTime);\r\nif (tx_status)\r\nnv_start_tx(dev);\r\nnv_start_rx(dev);\r\n}\r\nstatic void nv_gear_backoff_reseed(struct net_device *dev)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 miniseed1, miniseed2, miniseed2_reversed, miniseed3, miniseed3_reversed;\r\nu32 temp, seedset, combinedSeed;\r\nint i;\r\nget_random_bytes(&miniseed1, sizeof(miniseed1));\r\nminiseed1 &= 0x0fff;\r\nif (miniseed1 == 0)\r\nminiseed1 = 0xabc;\r\nget_random_bytes(&miniseed2, sizeof(miniseed2));\r\nminiseed2 &= 0x0fff;\r\nif (miniseed2 == 0)\r\nminiseed2 = 0xabc;\r\nminiseed2_reversed =\r\n((miniseed2 & 0xF00) >> 8) |\r\n(miniseed2 & 0x0F0) |\r\n((miniseed2 & 0x00F) << 8);\r\nget_random_bytes(&miniseed3, sizeof(miniseed3));\r\nminiseed3 &= 0x0fff;\r\nif (miniseed3 == 0)\r\nminiseed3 = 0xabc;\r\nminiseed3_reversed =\r\n((miniseed3 & 0xF00) >> 8) |\r\n(miniseed3 & 0x0F0) |\r\n((miniseed3 & 0x00F) << 8);\r\ncombinedSeed = ((miniseed1 ^ miniseed2_reversed) << 12) |\r\n(miniseed2 ^ miniseed3_reversed);\r\nif ((combinedSeed & NVREG_BKOFFCTRL_SEED_MASK) == 0)\r\ncombinedSeed |= 0x08;\r\nif ((combinedSeed & (NVREG_BKOFFCTRL_SEED_MASK << NVREG_BKOFFCTRL_GEAR)) == 0)\r\ncombinedSeed |= 0x8000;\r\ntemp = NVREG_BKOFFCTRL_DEFAULT | (0 << NVREG_BKOFFCTRL_SELECT);\r\ntemp |= combinedSeed & NVREG_BKOFFCTRL_SEED_MASK;\r\ntemp |= combinedSeed >> NVREG_BKOFFCTRL_GEAR;\r\nwritel(temp, base + NvRegBackOffControl);\r\nget_random_bytes(&seedset, sizeof(seedset));\r\nseedset = seedset % BACKOFF_SEEDSET_ROWS;\r\nfor (i = 1; i <= BACKOFF_SEEDSET_LFSRS; i++) {\r\ntemp = NVREG_BKOFFCTRL_DEFAULT | (i << NVREG_BKOFFCTRL_SELECT);\r\ntemp |= main_seedset[seedset][i-1] & 0x3ff;\r\ntemp |= ((gear_seedset[seedset][i-1] & 0x3ff) << NVREG_BKOFFCTRL_GEAR);\r\nwritel(temp, base + NvRegBackOffControl);\r\n}\r\n}\r\nstatic netdev_tx_t nv_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 tx_flags = 0;\r\nu32 tx_flags_extra = (np->desc_ver == DESC_VER_1 ? NV_TX_LASTPACKET : NV_TX2_LASTPACKET);\r\nunsigned int fragments = skb_shinfo(skb)->nr_frags;\r\nunsigned int i;\r\nu32 offset = 0;\r\nu32 bcnt;\r\nu32 size = skb_headlen(skb);\r\nu32 entries = (size >> NV_TX2_TSO_MAX_SHIFT) + ((size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\r\nu32 empty_slots;\r\nstruct ring_desc *put_tx;\r\nstruct ring_desc *start_tx;\r\nstruct ring_desc *prev_tx;\r\nstruct nv_skb_map *prev_tx_ctx;\r\nstruct nv_skb_map *tmp_tx_ctx = NULL, *start_tx_ctx = NULL;\r\nunsigned long flags;\r\nfor (i = 0; i < fragments; i++) {\r\nu32 frag_size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\r\nentries += (frag_size >> NV_TX2_TSO_MAX_SHIFT) +\r\n((frag_size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\r\n}\r\nspin_lock_irqsave(&np->lock, flags);\r\nempty_slots = nv_get_empty_tx_slots(np);\r\nif (unlikely(empty_slots <= entries)) {\r\nnetif_stop_queue(dev);\r\nnp->tx_stop = 1;\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nstart_tx = put_tx = np->put_tx.orig;\r\ndo {\r\nprev_tx = put_tx;\r\nprev_tx_ctx = np->put_tx_ctx;\r\nbcnt = (size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : size;\r\nnp->put_tx_ctx->dma = pci_map_single(np->pci_dev, skb->data + offset, bcnt,\r\nPCI_DMA_TODEVICE);\r\nif (pci_dma_mapping_error(np->pci_dev,\r\nnp->put_tx_ctx->dma)) {\r\ndev_kfree_skb_any(skb);\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_dropped++;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\nreturn NETDEV_TX_OK;\r\n}\r\nnp->put_tx_ctx->dma_len = bcnt;\r\nnp->put_tx_ctx->dma_single = 1;\r\nput_tx->buf = cpu_to_le32(np->put_tx_ctx->dma);\r\nput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\r\ntx_flags = np->tx_flags;\r\noffset += bcnt;\r\nsize -= bcnt;\r\nif (unlikely(put_tx++ == np->last_tx.orig))\r\nput_tx = np->first_tx.orig;\r\nif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\r\nnp->put_tx_ctx = np->first_tx_ctx;\r\n} while (size);\r\nfor (i = 0; i < fragments; i++) {\r\nconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nu32 frag_size = skb_frag_size(frag);\r\noffset = 0;\r\ndo {\r\nprev_tx = put_tx;\r\nprev_tx_ctx = np->put_tx_ctx;\r\nif (!start_tx_ctx)\r\nstart_tx_ctx = tmp_tx_ctx = np->put_tx_ctx;\r\nbcnt = (frag_size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : frag_size;\r\nnp->put_tx_ctx->dma = skb_frag_dma_map(\r\n&np->pci_dev->dev,\r\nfrag, offset,\r\nbcnt,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&np->pci_dev->dev, np->put_tx_ctx->dma)) {\r\ndo {\r\nnv_unmap_txskb(np, start_tx_ctx);\r\nif (unlikely(tmp_tx_ctx++ == np->last_tx_ctx))\r\ntmp_tx_ctx = np->first_tx_ctx;\r\n} while (tmp_tx_ctx != np->put_tx_ctx);\r\ndev_kfree_skb_any(skb);\r\nnp->put_tx_ctx = start_tx_ctx;\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_dropped++;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\nreturn NETDEV_TX_OK;\r\n}\r\nnp->put_tx_ctx->dma_len = bcnt;\r\nnp->put_tx_ctx->dma_single = 0;\r\nput_tx->buf = cpu_to_le32(np->put_tx_ctx->dma);\r\nput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\r\noffset += bcnt;\r\nfrag_size -= bcnt;\r\nif (unlikely(put_tx++ == np->last_tx.orig))\r\nput_tx = np->first_tx.orig;\r\nif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\r\nnp->put_tx_ctx = np->first_tx_ctx;\r\n} while (frag_size);\r\n}\r\nprev_tx->flaglen |= cpu_to_le32(tx_flags_extra);\r\nprev_tx_ctx->skb = skb;\r\nif (skb_is_gso(skb))\r\ntx_flags_extra = NV_TX2_TSO | (skb_shinfo(skb)->gso_size << NV_TX2_TSO_SHIFT);\r\nelse\r\ntx_flags_extra = skb->ip_summed == CHECKSUM_PARTIAL ?\r\nNV_TX2_CHECKSUM_L3 | NV_TX2_CHECKSUM_L4 : 0;\r\nspin_lock_irqsave(&np->lock, flags);\r\nstart_tx->flaglen |= cpu_to_le32(tx_flags | tx_flags_extra);\r\nnetdev_sent_queue(np->dev, skb->len);\r\nskb_tx_timestamp(skb);\r\nnp->put_tx.orig = put_tx;\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic netdev_tx_t nv_start_xmit_optimized(struct sk_buff *skb,\r\nstruct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 tx_flags = 0;\r\nu32 tx_flags_extra;\r\nunsigned int fragments = skb_shinfo(skb)->nr_frags;\r\nunsigned int i;\r\nu32 offset = 0;\r\nu32 bcnt;\r\nu32 size = skb_headlen(skb);\r\nu32 entries = (size >> NV_TX2_TSO_MAX_SHIFT) + ((size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\r\nu32 empty_slots;\r\nstruct ring_desc_ex *put_tx;\r\nstruct ring_desc_ex *start_tx;\r\nstruct ring_desc_ex *prev_tx;\r\nstruct nv_skb_map *prev_tx_ctx;\r\nstruct nv_skb_map *start_tx_ctx = NULL;\r\nstruct nv_skb_map *tmp_tx_ctx = NULL;\r\nunsigned long flags;\r\nfor (i = 0; i < fragments; i++) {\r\nu32 frag_size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\r\nentries += (frag_size >> NV_TX2_TSO_MAX_SHIFT) +\r\n((frag_size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\r\n}\r\nspin_lock_irqsave(&np->lock, flags);\r\nempty_slots = nv_get_empty_tx_slots(np);\r\nif (unlikely(empty_slots <= entries)) {\r\nnetif_stop_queue(dev);\r\nnp->tx_stop = 1;\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nstart_tx = put_tx = np->put_tx.ex;\r\nstart_tx_ctx = np->put_tx_ctx;\r\ndo {\r\nprev_tx = put_tx;\r\nprev_tx_ctx = np->put_tx_ctx;\r\nbcnt = (size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : size;\r\nnp->put_tx_ctx->dma = pci_map_single(np->pci_dev, skb->data + offset, bcnt,\r\nPCI_DMA_TODEVICE);\r\nif (pci_dma_mapping_error(np->pci_dev,\r\nnp->put_tx_ctx->dma)) {\r\ndev_kfree_skb_any(skb);\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_dropped++;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\nreturn NETDEV_TX_OK;\r\n}\r\nnp->put_tx_ctx->dma_len = bcnt;\r\nnp->put_tx_ctx->dma_single = 1;\r\nput_tx->bufhigh = cpu_to_le32(dma_high(np->put_tx_ctx->dma));\r\nput_tx->buflow = cpu_to_le32(dma_low(np->put_tx_ctx->dma));\r\nput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\r\ntx_flags = NV_TX2_VALID;\r\noffset += bcnt;\r\nsize -= bcnt;\r\nif (unlikely(put_tx++ == np->last_tx.ex))\r\nput_tx = np->first_tx.ex;\r\nif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\r\nnp->put_tx_ctx = np->first_tx_ctx;\r\n} while (size);\r\nfor (i = 0; i < fragments; i++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nu32 frag_size = skb_frag_size(frag);\r\noffset = 0;\r\ndo {\r\nprev_tx = put_tx;\r\nprev_tx_ctx = np->put_tx_ctx;\r\nbcnt = (frag_size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : frag_size;\r\nif (!start_tx_ctx)\r\nstart_tx_ctx = tmp_tx_ctx = np->put_tx_ctx;\r\nnp->put_tx_ctx->dma = skb_frag_dma_map(\r\n&np->pci_dev->dev,\r\nfrag, offset,\r\nbcnt,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&np->pci_dev->dev, np->put_tx_ctx->dma)) {\r\ndo {\r\nnv_unmap_txskb(np, start_tx_ctx);\r\nif (unlikely(tmp_tx_ctx++ == np->last_tx_ctx))\r\ntmp_tx_ctx = np->first_tx_ctx;\r\n} while (tmp_tx_ctx != np->put_tx_ctx);\r\ndev_kfree_skb_any(skb);\r\nnp->put_tx_ctx = start_tx_ctx;\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_dropped++;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\nreturn NETDEV_TX_OK;\r\n}\r\nnp->put_tx_ctx->dma_len = bcnt;\r\nnp->put_tx_ctx->dma_single = 0;\r\nput_tx->bufhigh = cpu_to_le32(dma_high(np->put_tx_ctx->dma));\r\nput_tx->buflow = cpu_to_le32(dma_low(np->put_tx_ctx->dma));\r\nput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\r\noffset += bcnt;\r\nfrag_size -= bcnt;\r\nif (unlikely(put_tx++ == np->last_tx.ex))\r\nput_tx = np->first_tx.ex;\r\nif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\r\nnp->put_tx_ctx = np->first_tx_ctx;\r\n} while (frag_size);\r\n}\r\nprev_tx->flaglen |= cpu_to_le32(NV_TX2_LASTPACKET);\r\nprev_tx_ctx->skb = skb;\r\nif (skb_is_gso(skb))\r\ntx_flags_extra = NV_TX2_TSO | (skb_shinfo(skb)->gso_size << NV_TX2_TSO_SHIFT);\r\nelse\r\ntx_flags_extra = skb->ip_summed == CHECKSUM_PARTIAL ?\r\nNV_TX2_CHECKSUM_L3 | NV_TX2_CHECKSUM_L4 : 0;\r\nif (skb_vlan_tag_present(skb))\r\nstart_tx->txvlan = cpu_to_le32(NV_TX3_VLAN_TAG_PRESENT |\r\nskb_vlan_tag_get(skb));\r\nelse\r\nstart_tx->txvlan = 0;\r\nspin_lock_irqsave(&np->lock, flags);\r\nif (np->tx_limit) {\r\nif (np->tx_pkts_in_progress == NV_TX_LIMIT_COUNT) {\r\nif (!np->tx_change_owner)\r\nnp->tx_change_owner = start_tx_ctx;\r\ntx_flags &= ~NV_TX2_VALID;\r\nstart_tx_ctx->first_tx_desc = start_tx;\r\nstart_tx_ctx->next_tx_ctx = np->put_tx_ctx;\r\nnp->tx_end_flip = np->put_tx_ctx;\r\n} else {\r\nnp->tx_pkts_in_progress++;\r\n}\r\n}\r\nstart_tx->flaglen |= cpu_to_le32(tx_flags | tx_flags_extra);\r\nnetdev_sent_queue(np->dev, skb->len);\r\nskb_tx_timestamp(skb);\r\nnp->put_tx.ex = put_tx;\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic inline void nv_tx_flip_ownership(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nnp->tx_pkts_in_progress--;\r\nif (np->tx_change_owner) {\r\nnp->tx_change_owner->first_tx_desc->flaglen |=\r\ncpu_to_le32(NV_TX2_VALID);\r\nnp->tx_pkts_in_progress++;\r\nnp->tx_change_owner = np->tx_change_owner->next_tx_ctx;\r\nif (np->tx_change_owner == np->tx_end_flip)\r\nnp->tx_change_owner = NULL;\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\n}\r\n}\r\nstatic int nv_tx_done(struct net_device *dev, int limit)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 flags;\r\nint tx_work = 0;\r\nstruct ring_desc *orig_get_tx = np->get_tx.orig;\r\nunsigned int bytes_compl = 0;\r\nwhile ((np->get_tx.orig != np->put_tx.orig) &&\r\n!((flags = le32_to_cpu(np->get_tx.orig->flaglen)) & NV_TX_VALID) &&\r\n(tx_work < limit)) {\r\nnv_unmap_txskb(np, np->get_tx_ctx);\r\nif (np->desc_ver == DESC_VER_1) {\r\nif (flags & NV_TX_LASTPACKET) {\r\nif (flags & NV_TX_ERROR) {\r\nif ((flags & NV_TX_RETRYERROR)\r\n&& !(flags & NV_TX_RETRYCOUNT_MASK))\r\nnv_legacybackoff_reseed(dev);\r\n} else {\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_packets++;\r\nnp->stat_tx_bytes += np->get_tx_ctx->skb->len;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\n}\r\nbytes_compl += np->get_tx_ctx->skb->len;\r\ndev_kfree_skb_any(np->get_tx_ctx->skb);\r\nnp->get_tx_ctx->skb = NULL;\r\ntx_work++;\r\n}\r\n} else {\r\nif (flags & NV_TX2_LASTPACKET) {\r\nif (flags & NV_TX2_ERROR) {\r\nif ((flags & NV_TX2_RETRYERROR)\r\n&& !(flags & NV_TX2_RETRYCOUNT_MASK))\r\nnv_legacybackoff_reseed(dev);\r\n} else {\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_packets++;\r\nnp->stat_tx_bytes += np->get_tx_ctx->skb->len;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\n}\r\nbytes_compl += np->get_tx_ctx->skb->len;\r\ndev_kfree_skb_any(np->get_tx_ctx->skb);\r\nnp->get_tx_ctx->skb = NULL;\r\ntx_work++;\r\n}\r\n}\r\nif (unlikely(np->get_tx.orig++ == np->last_tx.orig))\r\nnp->get_tx.orig = np->first_tx.orig;\r\nif (unlikely(np->get_tx_ctx++ == np->last_tx_ctx))\r\nnp->get_tx_ctx = np->first_tx_ctx;\r\n}\r\nnetdev_completed_queue(np->dev, tx_work, bytes_compl);\r\nif (unlikely((np->tx_stop == 1) && (np->get_tx.orig != orig_get_tx))) {\r\nnp->tx_stop = 0;\r\nnetif_wake_queue(dev);\r\n}\r\nreturn tx_work;\r\n}\r\nstatic int nv_tx_done_optimized(struct net_device *dev, int limit)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 flags;\r\nint tx_work = 0;\r\nstruct ring_desc_ex *orig_get_tx = np->get_tx.ex;\r\nunsigned long bytes_cleaned = 0;\r\nwhile ((np->get_tx.ex != np->put_tx.ex) &&\r\n!((flags = le32_to_cpu(np->get_tx.ex->flaglen)) & NV_TX2_VALID) &&\r\n(tx_work < limit)) {\r\nnv_unmap_txskb(np, np->get_tx_ctx);\r\nif (flags & NV_TX2_LASTPACKET) {\r\nif (flags & NV_TX2_ERROR) {\r\nif ((flags & NV_TX2_RETRYERROR)\r\n&& !(flags & NV_TX2_RETRYCOUNT_MASK)) {\r\nif (np->driver_data & DEV_HAS_GEAR_MODE)\r\nnv_gear_backoff_reseed(dev);\r\nelse\r\nnv_legacybackoff_reseed(dev);\r\n}\r\n} else {\r\nu64_stats_update_begin(&np->swstats_tx_syncp);\r\nnp->stat_tx_packets++;\r\nnp->stat_tx_bytes += np->get_tx_ctx->skb->len;\r\nu64_stats_update_end(&np->swstats_tx_syncp);\r\n}\r\nbytes_cleaned += np->get_tx_ctx->skb->len;\r\ndev_kfree_skb_any(np->get_tx_ctx->skb);\r\nnp->get_tx_ctx->skb = NULL;\r\ntx_work++;\r\nif (np->tx_limit)\r\nnv_tx_flip_ownership(dev);\r\n}\r\nif (unlikely(np->get_tx.ex++ == np->last_tx.ex))\r\nnp->get_tx.ex = np->first_tx.ex;\r\nif (unlikely(np->get_tx_ctx++ == np->last_tx_ctx))\r\nnp->get_tx_ctx = np->first_tx_ctx;\r\n}\r\nnetdev_completed_queue(np->dev, tx_work, bytes_cleaned);\r\nif (unlikely((np->tx_stop == 1) && (np->get_tx.ex != orig_get_tx))) {\r\nnp->tx_stop = 0;\r\nnetif_wake_queue(dev);\r\n}\r\nreturn tx_work;\r\n}\r\nstatic void nv_tx_timeout(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 status;\r\nunion ring_type put_tx;\r\nint saved_tx_limit;\r\nif (np->msi_flags & NV_MSI_X_ENABLED)\r\nstatus = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQSTAT_MASK;\r\nelse\r\nstatus = readl(base + NvRegIrqStatus) & NVREG_IRQSTAT_MASK;\r\nnetdev_warn(dev, "Got tx_timeout. irq status: %08x\n", status);\r\nif (unlikely(debug_tx_timeout)) {\r\nint i;\r\nnetdev_info(dev, "Ring at %lx\n", (unsigned long)np->ring_addr);\r\nnetdev_info(dev, "Dumping tx registers\n");\r\nfor (i = 0; i <= np->register_size; i += 32) {\r\nnetdev_info(dev,\r\n"%3x: %08x %08x %08x %08x "\r\n"%08x %08x %08x %08x\n",\r\ni,\r\nreadl(base + i + 0), readl(base + i + 4),\r\nreadl(base + i + 8), readl(base + i + 12),\r\nreadl(base + i + 16), readl(base + i + 20),\r\nreadl(base + i + 24), readl(base + i + 28));\r\n}\r\nnetdev_info(dev, "Dumping tx ring\n");\r\nfor (i = 0; i < np->tx_ring_size; i += 4) {\r\nif (!nv_optimized(np)) {\r\nnetdev_info(dev,\r\n"%03x: %08x %08x // %08x %08x "\r\n"// %08x %08x // %08x %08x\n",\r\ni,\r\nle32_to_cpu(np->tx_ring.orig[i].buf),\r\nle32_to_cpu(np->tx_ring.orig[i].flaglen),\r\nle32_to_cpu(np->tx_ring.orig[i+1].buf),\r\nle32_to_cpu(np->tx_ring.orig[i+1].flaglen),\r\nle32_to_cpu(np->tx_ring.orig[i+2].buf),\r\nle32_to_cpu(np->tx_ring.orig[i+2].flaglen),\r\nle32_to_cpu(np->tx_ring.orig[i+3].buf),\r\nle32_to_cpu(np->tx_ring.orig[i+3].flaglen));\r\n} else {\r\nnetdev_info(dev,\r\n"%03x: %08x %08x %08x "\r\n"// %08x %08x %08x "\r\n"// %08x %08x %08x "\r\n"// %08x %08x %08x\n",\r\ni,\r\nle32_to_cpu(np->tx_ring.ex[i].bufhigh),\r\nle32_to_cpu(np->tx_ring.ex[i].buflow),\r\nle32_to_cpu(np->tx_ring.ex[i].flaglen),\r\nle32_to_cpu(np->tx_ring.ex[i+1].bufhigh),\r\nle32_to_cpu(np->tx_ring.ex[i+1].buflow),\r\nle32_to_cpu(np->tx_ring.ex[i+1].flaglen),\r\nle32_to_cpu(np->tx_ring.ex[i+2].bufhigh),\r\nle32_to_cpu(np->tx_ring.ex[i+2].buflow),\r\nle32_to_cpu(np->tx_ring.ex[i+2].flaglen),\r\nle32_to_cpu(np->tx_ring.ex[i+3].bufhigh),\r\nle32_to_cpu(np->tx_ring.ex[i+3].buflow),\r\nle32_to_cpu(np->tx_ring.ex[i+3].flaglen));\r\n}\r\n}\r\n}\r\nspin_lock_irq(&np->lock);\r\nnv_stop_tx(dev);\r\nsaved_tx_limit = np->tx_limit;\r\nnp->tx_limit = 0;\r\nnp->tx_stop = 0;\r\nif (!nv_optimized(np))\r\nnv_tx_done(dev, np->tx_ring_size);\r\nelse\r\nnv_tx_done_optimized(dev, np->tx_ring_size);\r\nif (np->tx_change_owner)\r\nput_tx.ex = np->tx_change_owner->first_tx_desc;\r\nelse\r\nput_tx = np->put_tx;\r\nnv_drain_tx(dev);\r\nnv_init_tx(dev);\r\nnp->get_tx = np->put_tx = put_tx;\r\nnp->tx_limit = saved_tx_limit;\r\nnv_start_tx(dev);\r\nnetif_wake_queue(dev);\r\nspin_unlock_irq(&np->lock);\r\n}\r\nstatic int nv_getlen(struct net_device *dev, void *packet, int datalen)\r\n{\r\nint hdrlen;\r\nint protolen;\r\nif (((struct vlan_ethhdr *)packet)->h_vlan_proto == htons(ETH_P_8021Q)) {\r\nprotolen = ntohs(((struct vlan_ethhdr *)packet)->h_vlan_encapsulated_proto);\r\nhdrlen = VLAN_HLEN;\r\n} else {\r\nprotolen = ntohs(((struct ethhdr *)packet)->h_proto);\r\nhdrlen = ETH_HLEN;\r\n}\r\nif (protolen > ETH_DATA_LEN)\r\nreturn datalen;\r\nprotolen += hdrlen;\r\nif (datalen > ETH_ZLEN) {\r\nif (datalen >= protolen) {\r\nreturn protolen;\r\n} else {\r\nreturn -1;\r\n}\r\n} else {\r\nif (protolen > ETH_ZLEN) {\r\nreturn -1;\r\n}\r\nreturn datalen;\r\n}\r\n}\r\nstatic int nv_rx_process(struct net_device *dev, int limit)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 flags;\r\nint rx_work = 0;\r\nstruct sk_buff *skb;\r\nint len;\r\nwhile ((np->get_rx.orig != np->put_rx.orig) &&\r\n!((flags = le32_to_cpu(np->get_rx.orig->flaglen)) & NV_RX_AVAIL) &&\r\n(rx_work < limit)) {\r\npci_unmap_single(np->pci_dev, np->get_rx_ctx->dma,\r\nnp->get_rx_ctx->dma_len,\r\nPCI_DMA_FROMDEVICE);\r\nskb = np->get_rx_ctx->skb;\r\nnp->get_rx_ctx->skb = NULL;\r\nif (np->desc_ver == DESC_VER_1) {\r\nif (likely(flags & NV_RX_DESCRIPTORVALID)) {\r\nlen = flags & LEN_MASK_V1;\r\nif (unlikely(flags & NV_RX_ERROR)) {\r\nif ((flags & NV_RX_ERROR_MASK) == NV_RX_ERROR4) {\r\nlen = nv_getlen(dev, skb->data, len);\r\nif (len < 0) {\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n}\r\nelse if ((flags & NV_RX_ERROR_MASK) == NV_RX_FRAMINGERR) {\r\nif (flags & NV_RX_SUBTRACT1)\r\nlen--;\r\n}\r\nelse {\r\nif (flags & NV_RX_MISSEDFRAME) {\r\nu64_stats_update_begin(&np->swstats_rx_syncp);\r\nnp->stat_rx_missed_errors++;\r\nu64_stats_update_end(&np->swstats_rx_syncp);\r\n}\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n}\r\n} else {\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n} else {\r\nif (likely(flags & NV_RX2_DESCRIPTORVALID)) {\r\nlen = flags & LEN_MASK_V2;\r\nif (unlikely(flags & NV_RX2_ERROR)) {\r\nif ((flags & NV_RX2_ERROR_MASK) == NV_RX2_ERROR4) {\r\nlen = nv_getlen(dev, skb->data, len);\r\nif (len < 0) {\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n}\r\nelse if ((flags & NV_RX2_ERROR_MASK) == NV_RX2_FRAMINGERR) {\r\nif (flags & NV_RX2_SUBTRACT1)\r\nlen--;\r\n}\r\nelse {\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n}\r\nif (((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_TCP) ||\r\n((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_UDP))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n} else {\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n}\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nnapi_gro_receive(&np->napi, skb);\r\nu64_stats_update_begin(&np->swstats_rx_syncp);\r\nnp->stat_rx_packets++;\r\nnp->stat_rx_bytes += len;\r\nu64_stats_update_end(&np->swstats_rx_syncp);\r\nnext_pkt:\r\nif (unlikely(np->get_rx.orig++ == np->last_rx.orig))\r\nnp->get_rx.orig = np->first_rx.orig;\r\nif (unlikely(np->get_rx_ctx++ == np->last_rx_ctx))\r\nnp->get_rx_ctx = np->first_rx_ctx;\r\nrx_work++;\r\n}\r\nreturn rx_work;\r\n}\r\nstatic int nv_rx_process_optimized(struct net_device *dev, int limit)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 flags;\r\nu32 vlanflags = 0;\r\nint rx_work = 0;\r\nstruct sk_buff *skb;\r\nint len;\r\nwhile ((np->get_rx.ex != np->put_rx.ex) &&\r\n!((flags = le32_to_cpu(np->get_rx.ex->flaglen)) & NV_RX2_AVAIL) &&\r\n(rx_work < limit)) {\r\npci_unmap_single(np->pci_dev, np->get_rx_ctx->dma,\r\nnp->get_rx_ctx->dma_len,\r\nPCI_DMA_FROMDEVICE);\r\nskb = np->get_rx_ctx->skb;\r\nnp->get_rx_ctx->skb = NULL;\r\nif (likely(flags & NV_RX2_DESCRIPTORVALID)) {\r\nlen = flags & LEN_MASK_V2;\r\nif (unlikely(flags & NV_RX2_ERROR)) {\r\nif ((flags & NV_RX2_ERROR_MASK) == NV_RX2_ERROR4) {\r\nlen = nv_getlen(dev, skb->data, len);\r\nif (len < 0) {\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n}\r\nelse if ((flags & NV_RX2_ERROR_MASK) == NV_RX2_FRAMINGERR) {\r\nif (flags & NV_RX2_SUBTRACT1)\r\nlen--;\r\n}\r\nelse {\r\ndev_kfree_skb(skb);\r\ngoto next_pkt;\r\n}\r\n}\r\nif (((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_TCP) ||\r\n((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_UDP))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nprefetch(skb->data);\r\nvlanflags = le32_to_cpu(np->get_rx.ex->buflow);\r\nif (dev->features & NETIF_F_HW_VLAN_CTAG_RX &&\r\nvlanflags & NV_RX3_VLAN_TAG_PRESENT) {\r\nu16 vid = vlanflags & NV_RX3_VLAN_TAG_MASK;\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);\r\n}\r\nnapi_gro_receive(&np->napi, skb);\r\nu64_stats_update_begin(&np->swstats_rx_syncp);\r\nnp->stat_rx_packets++;\r\nnp->stat_rx_bytes += len;\r\nu64_stats_update_end(&np->swstats_rx_syncp);\r\n} else {\r\ndev_kfree_skb(skb);\r\n}\r\nnext_pkt:\r\nif (unlikely(np->get_rx.ex++ == np->last_rx.ex))\r\nnp->get_rx.ex = np->first_rx.ex;\r\nif (unlikely(np->get_rx_ctx++ == np->last_rx_ctx))\r\nnp->get_rx_ctx = np->first_rx_ctx;\r\nrx_work++;\r\n}\r\nreturn rx_work;\r\n}\r\nstatic void set_bufsize(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nif (dev->mtu <= ETH_DATA_LEN)\r\nnp->rx_buf_sz = ETH_DATA_LEN + NV_RX_HEADERS;\r\nelse\r\nnp->rx_buf_sz = dev->mtu + NV_RX_HEADERS;\r\n}\r\nstatic int nv_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nint old_mtu;\r\nif (new_mtu < 64 || new_mtu > np->pkt_limit)\r\nreturn -EINVAL;\r\nold_mtu = dev->mtu;\r\ndev->mtu = new_mtu;\r\nif (old_mtu <= ETH_DATA_LEN && new_mtu <= ETH_DATA_LEN)\r\nreturn 0;\r\nif (old_mtu == new_mtu)\r\nreturn 0;\r\nif (netif_running(dev)) {\r\nu8 __iomem *base = get_hwbase(dev);\r\nnv_disable_irq(dev);\r\nnv_napi_disable(dev);\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock(&np->lock);\r\nnv_stop_rxtx(dev);\r\nnv_txrx_reset(dev);\r\nnv_drain_rxtx(dev);\r\nset_bufsize(dev);\r\nif (nv_init_ring(dev)) {\r\nif (!np->in_shutdown)\r\nmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\r\n}\r\nwritel(np->rx_buf_sz, base + NvRegOffloadConfig);\r\nsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\r\nwritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\r\nbase + NvRegRingSizes);\r\npci_push(base);\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\npci_push(base);\r\nnv_start_rxtx(dev);\r\nspin_unlock(&np->lock);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\nnv_napi_enable(dev);\r\nnv_enable_irq(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic void nv_copy_mac_to_hw(struct net_device *dev)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 mac[2];\r\nmac[0] = (dev->dev_addr[0] << 0) + (dev->dev_addr[1] << 8) +\r\n(dev->dev_addr[2] << 16) + (dev->dev_addr[3] << 24);\r\nmac[1] = (dev->dev_addr[4] << 0) + (dev->dev_addr[5] << 8);\r\nwritel(mac[0], base + NvRegMacAddrA);\r\nwritel(mac[1], base + NvRegMacAddrB);\r\n}\r\nstatic int nv_set_mac_address(struct net_device *dev, void *addr)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nstruct sockaddr *macaddr = (struct sockaddr *)addr;\r\nif (!is_valid_ether_addr(macaddr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, macaddr->sa_data, ETH_ALEN);\r\nif (netif_running(dev)) {\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock_irq(&np->lock);\r\nnv_stop_rx(dev);\r\nnv_copy_mac_to_hw(dev);\r\nnv_start_rx(dev);\r\nspin_unlock_irq(&np->lock);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\n} else {\r\nnv_copy_mac_to_hw(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic void nv_set_multicast(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 addr[2];\r\nu32 mask[2];\r\nu32 pff = readl(base + NvRegPacketFilterFlags) & NVREG_PFF_PAUSE_RX;\r\nmemset(addr, 0, sizeof(addr));\r\nmemset(mask, 0, sizeof(mask));\r\nif (dev->flags & IFF_PROMISC) {\r\npff |= NVREG_PFF_PROMISC;\r\n} else {\r\npff |= NVREG_PFF_MYADDR;\r\nif (dev->flags & IFF_ALLMULTI || !netdev_mc_empty(dev)) {\r\nu32 alwaysOff[2];\r\nu32 alwaysOn[2];\r\nalwaysOn[0] = alwaysOn[1] = alwaysOff[0] = alwaysOff[1] = 0xffffffff;\r\nif (dev->flags & IFF_ALLMULTI) {\r\nalwaysOn[0] = alwaysOn[1] = alwaysOff[0] = alwaysOff[1] = 0;\r\n} else {\r\nstruct netdev_hw_addr *ha;\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nunsigned char *hw_addr = ha->addr;\r\nu32 a, b;\r\na = le32_to_cpu(*(__le32 *) hw_addr);\r\nb = le16_to_cpu(*(__le16 *) (&hw_addr[4]));\r\nalwaysOn[0] &= a;\r\nalwaysOff[0] &= ~a;\r\nalwaysOn[1] &= b;\r\nalwaysOff[1] &= ~b;\r\n}\r\n}\r\naddr[0] = alwaysOn[0];\r\naddr[1] = alwaysOn[1];\r\nmask[0] = alwaysOn[0] | alwaysOff[0];\r\nmask[1] = alwaysOn[1] | alwaysOff[1];\r\n} else {\r\nmask[0] = NVREG_MCASTMASKA_NONE;\r\nmask[1] = NVREG_MCASTMASKB_NONE;\r\n}\r\n}\r\naddr[0] |= NVREG_MCASTADDRA_FORCE;\r\npff |= NVREG_PFF_ALWAYS;\r\nspin_lock_irq(&np->lock);\r\nnv_stop_rx(dev);\r\nwritel(addr[0], base + NvRegMulticastAddrA);\r\nwritel(addr[1], base + NvRegMulticastAddrB);\r\nwritel(mask[0], base + NvRegMulticastMaskA);\r\nwritel(mask[1], base + NvRegMulticastMaskB);\r\nwritel(pff, base + NvRegPacketFilterFlags);\r\nnv_start_rx(dev);\r\nspin_unlock_irq(&np->lock);\r\n}\r\nstatic void nv_update_pause(struct net_device *dev, u32 pause_flags)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nnp->pause_flags &= ~(NV_PAUSEFRAME_TX_ENABLE | NV_PAUSEFRAME_RX_ENABLE);\r\nif (np->pause_flags & NV_PAUSEFRAME_RX_CAPABLE) {\r\nu32 pff = readl(base + NvRegPacketFilterFlags) & ~NVREG_PFF_PAUSE_RX;\r\nif (pause_flags & NV_PAUSEFRAME_RX_ENABLE) {\r\nwritel(pff|NVREG_PFF_PAUSE_RX, base + NvRegPacketFilterFlags);\r\nnp->pause_flags |= NV_PAUSEFRAME_RX_ENABLE;\r\n} else {\r\nwritel(pff, base + NvRegPacketFilterFlags);\r\n}\r\n}\r\nif (np->pause_flags & NV_PAUSEFRAME_TX_CAPABLE) {\r\nu32 regmisc = readl(base + NvRegMisc1) & ~NVREG_MISC1_PAUSE_TX;\r\nif (pause_flags & NV_PAUSEFRAME_TX_ENABLE) {\r\nu32 pause_enable = NVREG_TX_PAUSEFRAME_ENABLE_V1;\r\nif (np->driver_data & DEV_HAS_PAUSEFRAME_TX_V2)\r\npause_enable = NVREG_TX_PAUSEFRAME_ENABLE_V2;\r\nif (np->driver_data & DEV_HAS_PAUSEFRAME_TX_V3) {\r\npause_enable = NVREG_TX_PAUSEFRAME_ENABLE_V3;\r\nwritel(readl(base + NvRegTxPauseFrameLimit)|NVREG_TX_PAUSEFRAMELIMIT_ENABLE, base + NvRegTxPauseFrameLimit);\r\n}\r\nwritel(pause_enable, base + NvRegTxPauseFrame);\r\nwritel(regmisc|NVREG_MISC1_PAUSE_TX, base + NvRegMisc1);\r\nnp->pause_flags |= NV_PAUSEFRAME_TX_ENABLE;\r\n} else {\r\nwritel(NVREG_TX_PAUSEFRAME_DISABLE, base + NvRegTxPauseFrame);\r\nwritel(regmisc, base + NvRegMisc1);\r\n}\r\n}\r\n}\r\nstatic void nv_force_linkspeed(struct net_device *dev, int speed, int duplex)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 phyreg, txreg;\r\nint mii_status;\r\nnp->linkspeed = NVREG_LINKSPEED_FORCE|speed;\r\nnp->duplex = duplex;\r\nmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\r\nif (mii_status & PHY_GIGABIT) {\r\nnp->gigabit = PHY_GIGABIT;\r\nphyreg = readl(base + NvRegSlotTime);\r\nphyreg &= ~(0x3FF00);\r\nif ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_10)\r\nphyreg |= NVREG_SLOTTIME_10_100_FULL;\r\nelse if ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_100)\r\nphyreg |= NVREG_SLOTTIME_10_100_FULL;\r\nelse if ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_1000)\r\nphyreg |= NVREG_SLOTTIME_1000_FULL;\r\nwritel(phyreg, base + NvRegSlotTime);\r\n}\r\nphyreg = readl(base + NvRegPhyInterface);\r\nphyreg &= ~(PHY_HALF|PHY_100|PHY_1000);\r\nif (np->duplex == 0)\r\nphyreg |= PHY_HALF;\r\nif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_100)\r\nphyreg |= PHY_100;\r\nelse if ((np->linkspeed & NVREG_LINKSPEED_MASK) ==\r\nNVREG_LINKSPEED_1000)\r\nphyreg |= PHY_1000;\r\nwritel(phyreg, base + NvRegPhyInterface);\r\nif (phyreg & PHY_RGMII) {\r\nif ((np->linkspeed & NVREG_LINKSPEED_MASK) ==\r\nNVREG_LINKSPEED_1000)\r\ntxreg = NVREG_TX_DEFERRAL_RGMII_1000;\r\nelse\r\ntxreg = NVREG_TX_DEFERRAL_RGMII_10_100;\r\n} else {\r\ntxreg = NVREG_TX_DEFERRAL_DEFAULT;\r\n}\r\nwritel(txreg, base + NvRegTxDeferral);\r\nif (np->desc_ver == DESC_VER_1) {\r\ntxreg = NVREG_TX_WM_DESC1_DEFAULT;\r\n} else {\r\nif ((np->linkspeed & NVREG_LINKSPEED_MASK) ==\r\nNVREG_LINKSPEED_1000)\r\ntxreg = NVREG_TX_WM_DESC2_3_1000;\r\nelse\r\ntxreg = NVREG_TX_WM_DESC2_3_DEFAULT;\r\n}\r\nwritel(txreg, base + NvRegTxWatermark);\r\nwritel(NVREG_MISC1_FORCE | (np->duplex ? 0 : NVREG_MISC1_HD),\r\nbase + NvRegMisc1);\r\npci_push(base);\r\nwritel(np->linkspeed, base + NvRegLinkSpeed);\r\npci_push(base);\r\nreturn;\r\n}\r\nstatic int nv_update_linkspeed(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint adv = 0;\r\nint lpa = 0;\r\nint adv_lpa, adv_pause, lpa_pause;\r\nint newls = np->linkspeed;\r\nint newdup = np->duplex;\r\nint mii_status;\r\nu32 bmcr;\r\nint retval = 0;\r\nu32 control_1000, status_1000, phyreg, pause_flags, txreg;\r\nu32 txrxFlags = 0;\r\nu32 phy_exp;\r\nbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nif (bmcr & BMCR_LOOPBACK) {\r\nif (netif_running(dev)) {\r\nnv_force_linkspeed(dev, NVREG_LINKSPEED_1000, 1);\r\nif (!netif_carrier_ok(dev))\r\nnetif_carrier_on(dev);\r\n}\r\nreturn 1;\r\n}\r\nmii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\r\nmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\r\nif (!(mii_status & BMSR_LSTATUS)) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnewdup = 0;\r\nretval = 0;\r\ngoto set_speed;\r\n}\r\nif (np->autoneg == 0) {\r\nif (np->fixed_mode & LPA_100FULL) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\r\nnewdup = 1;\r\n} else if (np->fixed_mode & LPA_100HALF) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\r\nnewdup = 0;\r\n} else if (np->fixed_mode & LPA_10FULL) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnewdup = 1;\r\n} else {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnewdup = 0;\r\n}\r\nretval = 1;\r\ngoto set_speed;\r\n}\r\nif (!(mii_status & BMSR_ANEGCOMPLETE)) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnewdup = 0;\r\nretval = 0;\r\ngoto set_speed;\r\n}\r\nadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\r\nlpa = mii_rw(dev, np->phyaddr, MII_LPA, MII_READ);\r\nretval = 1;\r\nif (np->gigabit == PHY_GIGABIT) {\r\ncontrol_1000 = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\r\nstatus_1000 = mii_rw(dev, np->phyaddr, MII_STAT1000, MII_READ);\r\nif ((control_1000 & ADVERTISE_1000FULL) &&\r\n(status_1000 & LPA_1000FULL)) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_1000;\r\nnewdup = 1;\r\ngoto set_speed;\r\n}\r\n}\r\nadv_lpa = lpa & adv;\r\nif (adv_lpa & LPA_100FULL) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\r\nnewdup = 1;\r\n} else if (adv_lpa & LPA_100HALF) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\r\nnewdup = 0;\r\n} else if (adv_lpa & LPA_10FULL) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnewdup = 1;\r\n} else if (adv_lpa & LPA_10HALF) {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnewdup = 0;\r\n} else {\r\nnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnewdup = 0;\r\n}\r\nset_speed:\r\nif (np->duplex == newdup && np->linkspeed == newls)\r\nreturn retval;\r\nnp->duplex = newdup;\r\nnp->linkspeed = newls;\r\nif (readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_START) {\r\ntxrxFlags |= NV_RESTART_TX;\r\nnv_stop_tx(dev);\r\n}\r\nif (readl(base + NvRegReceiverControl) & NVREG_RCVCTL_START) {\r\ntxrxFlags |= NV_RESTART_RX;\r\nnv_stop_rx(dev);\r\n}\r\nif (np->gigabit == PHY_GIGABIT) {\r\nphyreg = readl(base + NvRegSlotTime);\r\nphyreg &= ~(0x3FF00);\r\nif (((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_10) ||\r\n((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_100))\r\nphyreg |= NVREG_SLOTTIME_10_100_FULL;\r\nelse if ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_1000)\r\nphyreg |= NVREG_SLOTTIME_1000_FULL;\r\nwritel(phyreg, base + NvRegSlotTime);\r\n}\r\nphyreg = readl(base + NvRegPhyInterface);\r\nphyreg &= ~(PHY_HALF|PHY_100|PHY_1000);\r\nif (np->duplex == 0)\r\nphyreg |= PHY_HALF;\r\nif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_100)\r\nphyreg |= PHY_100;\r\nelse if ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_1000)\r\nphyreg |= PHY_1000;\r\nwritel(phyreg, base + NvRegPhyInterface);\r\nphy_exp = mii_rw(dev, np->phyaddr, MII_EXPANSION, MII_READ) & EXPANSION_NWAY;\r\nif (phyreg & PHY_RGMII) {\r\nif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_1000) {\r\ntxreg = NVREG_TX_DEFERRAL_RGMII_1000;\r\n} else {\r\nif (!phy_exp && !np->duplex && (np->driver_data & DEV_HAS_COLLISION_FIX)) {\r\nif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_10)\r\ntxreg = NVREG_TX_DEFERRAL_RGMII_STRETCH_10;\r\nelse\r\ntxreg = NVREG_TX_DEFERRAL_RGMII_STRETCH_100;\r\n} else {\r\ntxreg = NVREG_TX_DEFERRAL_RGMII_10_100;\r\n}\r\n}\r\n} else {\r\nif (!phy_exp && !np->duplex && (np->driver_data & DEV_HAS_COLLISION_FIX))\r\ntxreg = NVREG_TX_DEFERRAL_MII_STRETCH;\r\nelse\r\ntxreg = NVREG_TX_DEFERRAL_DEFAULT;\r\n}\r\nwritel(txreg, base + NvRegTxDeferral);\r\nif (np->desc_ver == DESC_VER_1) {\r\ntxreg = NVREG_TX_WM_DESC1_DEFAULT;\r\n} else {\r\nif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_1000)\r\ntxreg = NVREG_TX_WM_DESC2_3_1000;\r\nelse\r\ntxreg = NVREG_TX_WM_DESC2_3_DEFAULT;\r\n}\r\nwritel(txreg, base + NvRegTxWatermark);\r\nwritel(NVREG_MISC1_FORCE | (np->duplex ? 0 : NVREG_MISC1_HD),\r\nbase + NvRegMisc1);\r\npci_push(base);\r\nwritel(np->linkspeed, base + NvRegLinkSpeed);\r\npci_push(base);\r\npause_flags = 0;\r\nif (netif_running(dev) && (np->duplex != 0)) {\r\nif (np->autoneg && np->pause_flags & NV_PAUSEFRAME_AUTONEG) {\r\nadv_pause = adv & (ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\r\nlpa_pause = lpa & (LPA_PAUSE_CAP | LPA_PAUSE_ASYM);\r\nswitch (adv_pause) {\r\ncase ADVERTISE_PAUSE_CAP:\r\nif (lpa_pause & LPA_PAUSE_CAP) {\r\npause_flags |= NV_PAUSEFRAME_RX_ENABLE;\r\nif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\r\npause_flags |= NV_PAUSEFRAME_TX_ENABLE;\r\n}\r\nbreak;\r\ncase ADVERTISE_PAUSE_ASYM:\r\nif (lpa_pause == (LPA_PAUSE_CAP | LPA_PAUSE_ASYM))\r\npause_flags |= NV_PAUSEFRAME_TX_ENABLE;\r\nbreak;\r\ncase ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM:\r\nif (lpa_pause & LPA_PAUSE_CAP) {\r\npause_flags |= NV_PAUSEFRAME_RX_ENABLE;\r\nif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\r\npause_flags |= NV_PAUSEFRAME_TX_ENABLE;\r\n}\r\nif (lpa_pause == LPA_PAUSE_ASYM)\r\npause_flags |= NV_PAUSEFRAME_RX_ENABLE;\r\nbreak;\r\n}\r\n} else {\r\npause_flags = np->pause_flags;\r\n}\r\n}\r\nnv_update_pause(dev, pause_flags);\r\nif (txrxFlags & NV_RESTART_TX)\r\nnv_start_tx(dev);\r\nif (txrxFlags & NV_RESTART_RX)\r\nnv_start_rx(dev);\r\nreturn retval;\r\n}\r\nstatic void nv_linkchange(struct net_device *dev)\r\n{\r\nif (nv_update_linkspeed(dev)) {\r\nif (!netif_carrier_ok(dev)) {\r\nnetif_carrier_on(dev);\r\nnetdev_info(dev, "link up\n");\r\nnv_txrx_gate(dev, false);\r\nnv_start_rx(dev);\r\n}\r\n} else {\r\nif (netif_carrier_ok(dev)) {\r\nnetif_carrier_off(dev);\r\nnetdev_info(dev, "link down\n");\r\nnv_txrx_gate(dev, true);\r\nnv_stop_rx(dev);\r\n}\r\n}\r\n}\r\nstatic void nv_link_irq(struct net_device *dev)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 miistat;\r\nmiistat = readl(base + NvRegMIIStatus);\r\nwritel(NVREG_MIISTAT_LINKCHANGE, base + NvRegMIIStatus);\r\nif (miistat & (NVREG_MIISTAT_LINKCHANGE))\r\nnv_linkchange(dev);\r\n}\r\nstatic void nv_msi_workaround(struct fe_priv *np)\r\n{\r\nif (np->msi_flags & NV_MSI_ENABLED) {\r\nu8 __iomem *base = np->base;\r\nwritel(0, base + NvRegMSIIrqMask);\r\nwritel(NVREG_MSI_VECTOR_0_ENABLED, base + NvRegMSIIrqMask);\r\n}\r\n}\r\nstatic inline int nv_change_interrupt_mode(struct net_device *dev, int total_work)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nif (optimization_mode == NV_OPTIMIZATION_MODE_DYNAMIC) {\r\nif (total_work > NV_DYNAMIC_THRESHOLD) {\r\nnp->quiet_count = 0;\r\nif (np->irqmask != NVREG_IRQMASK_CPU) {\r\nnp->irqmask = NVREG_IRQMASK_CPU;\r\nreturn 1;\r\n}\r\n} else {\r\nif (np->quiet_count < NV_DYNAMIC_MAX_QUIET_COUNT) {\r\nnp->quiet_count++;\r\n} else {\r\nif (np->irqmask != NVREG_IRQMASK_THROUGHPUT) {\r\nnp->irqmask = NVREG_IRQMASK_THROUGHPUT;\r\nreturn 1;\r\n}\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic irqreturn_t nv_nic_irq(int foo, void *data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nif (!(np->msi_flags & NV_MSI_X_ENABLED)) {\r\nnp->events = readl(base + NvRegIrqStatus);\r\nwritel(np->events, base + NvRegIrqStatus);\r\n} else {\r\nnp->events = readl(base + NvRegMSIXIrqStatus);\r\nwritel(np->events, base + NvRegMSIXIrqStatus);\r\n}\r\nif (!(np->events & np->irqmask))\r\nreturn IRQ_NONE;\r\nnv_msi_workaround(np);\r\nif (napi_schedule_prep(&np->napi)) {\r\nwritel(0, base + NvRegIrqMask);\r\n__napi_schedule(&np->napi);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t nv_nic_irq_optimized(int foo, void *data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nif (!(np->msi_flags & NV_MSI_X_ENABLED)) {\r\nnp->events = readl(base + NvRegIrqStatus);\r\nwritel(np->events, base + NvRegIrqStatus);\r\n} else {\r\nnp->events = readl(base + NvRegMSIXIrqStatus);\r\nwritel(np->events, base + NvRegMSIXIrqStatus);\r\n}\r\nif (!(np->events & np->irqmask))\r\nreturn IRQ_NONE;\r\nnv_msi_workaround(np);\r\nif (napi_schedule_prep(&np->napi)) {\r\nwritel(0, base + NvRegIrqMask);\r\n__napi_schedule(&np->napi);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t nv_nic_irq_tx(int foo, void *data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 events;\r\nint i;\r\nunsigned long flags;\r\nfor (i = 0;; i++) {\r\nevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQ_TX_ALL;\r\nwritel(events, base + NvRegMSIXIrqStatus);\r\nnetdev_dbg(dev, "tx irq events: %08x\n", events);\r\nif (!(events & np->irqmask))\r\nbreak;\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_tx_done_optimized(dev, TX_WORK_PER_LOOP);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nif (unlikely(i > max_interrupt_work)) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nwritel(NVREG_IRQ_TX_ALL, base + NvRegIrqMask);\r\npci_push(base);\r\nif (!np->in_shutdown) {\r\nnp->nic_poll_irq |= NVREG_IRQ_TX_ALL;\r\nmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetdev_dbg(dev, "%s: too many iterations (%d)\n",\r\n__func__, i);\r\nbreak;\r\n}\r\n}\r\nreturn IRQ_RETVAL(i);\r\n}\r\nstatic int nv_napi_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct fe_priv *np = container_of(napi, struct fe_priv, napi);\r\nstruct net_device *dev = np->dev;\r\nu8 __iomem *base = get_hwbase(dev);\r\nunsigned long flags;\r\nint retcode;\r\nint rx_count, tx_work = 0, rx_work = 0;\r\ndo {\r\nif (!nv_optimized(np)) {\r\nspin_lock_irqsave(&np->lock, flags);\r\ntx_work += nv_tx_done(dev, np->tx_ring_size);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nrx_count = nv_rx_process(dev, budget - rx_work);\r\nretcode = nv_alloc_rx(dev);\r\n} else {\r\nspin_lock_irqsave(&np->lock, flags);\r\ntx_work += nv_tx_done_optimized(dev, np->tx_ring_size);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nrx_count = nv_rx_process_optimized(dev,\r\nbudget - rx_work);\r\nretcode = nv_alloc_rx_optimized(dev);\r\n}\r\n} while (retcode == 0 &&\r\nrx_count > 0 && (rx_work += rx_count) < budget);\r\nif (retcode) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nif (!np->in_shutdown)\r\nmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\n}\r\nnv_change_interrupt_mode(dev, tx_work + rx_work);\r\nif (unlikely(np->events & NVREG_IRQ_LINK)) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_link_irq(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\n}\r\nif (unlikely(np->need_linktimer && time_after(jiffies, np->link_timeout))) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_linkchange(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnp->link_timeout = jiffies + LINK_TIMEOUT;\r\n}\r\nif (unlikely(np->events & NVREG_IRQ_RECOVER_ERROR)) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nif (!np->in_shutdown) {\r\nnp->nic_poll_irq = np->irqmask;\r\nnp->recover_error = 1;\r\nmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnapi_complete(napi);\r\nreturn rx_work;\r\n}\r\nif (rx_work < budget) {\r\nnapi_complete(napi);\r\nwritel(np->irqmask, base + NvRegIrqMask);\r\n}\r\nreturn rx_work;\r\n}\r\nstatic irqreturn_t nv_nic_irq_rx(int foo, void *data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 events;\r\nint i;\r\nunsigned long flags;\r\nfor (i = 0;; i++) {\r\nevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQ_RX_ALL;\r\nwritel(events, base + NvRegMSIXIrqStatus);\r\nnetdev_dbg(dev, "rx irq events: %08x\n", events);\r\nif (!(events & np->irqmask))\r\nbreak;\r\nif (nv_rx_process_optimized(dev, RX_WORK_PER_LOOP)) {\r\nif (unlikely(nv_alloc_rx_optimized(dev))) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nif (!np->in_shutdown)\r\nmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\n}\r\n}\r\nif (unlikely(i > max_interrupt_work)) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nwritel(NVREG_IRQ_RX_ALL, base + NvRegIrqMask);\r\npci_push(base);\r\nif (!np->in_shutdown) {\r\nnp->nic_poll_irq |= NVREG_IRQ_RX_ALL;\r\nmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetdev_dbg(dev, "%s: too many iterations (%d)\n",\r\n__func__, i);\r\nbreak;\r\n}\r\n}\r\nreturn IRQ_RETVAL(i);\r\n}\r\nstatic irqreturn_t nv_nic_irq_other(int foo, void *data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 events;\r\nint i;\r\nunsigned long flags;\r\nfor (i = 0;; i++) {\r\nevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQ_OTHER;\r\nwritel(events, base + NvRegMSIXIrqStatus);\r\nnetdev_dbg(dev, "irq events: %08x\n", events);\r\nif (!(events & np->irqmask))\r\nbreak;\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_tx_done_optimized(dev, TX_WORK_PER_LOOP);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nif (events & NVREG_IRQ_LINK) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_link_irq(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\n}\r\nif (np->need_linktimer && time_after(jiffies, np->link_timeout)) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_linkchange(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnp->link_timeout = jiffies + LINK_TIMEOUT;\r\n}\r\nif (events & NVREG_IRQ_RECOVER_ERROR) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nwritel(NVREG_IRQ_OTHER, base + NvRegIrqMask);\r\npci_push(base);\r\nif (!np->in_shutdown) {\r\nnp->nic_poll_irq |= NVREG_IRQ_OTHER;\r\nnp->recover_error = 1;\r\nmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nbreak;\r\n}\r\nif (unlikely(i > max_interrupt_work)) {\r\nspin_lock_irqsave(&np->lock, flags);\r\nwritel(NVREG_IRQ_OTHER, base + NvRegIrqMask);\r\npci_push(base);\r\nif (!np->in_shutdown) {\r\nnp->nic_poll_irq |= NVREG_IRQ_OTHER;\r\nmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetdev_dbg(dev, "%s: too many iterations (%d)\n",\r\n__func__, i);\r\nbreak;\r\n}\r\n}\r\nreturn IRQ_RETVAL(i);\r\n}\r\nstatic irqreturn_t nv_nic_irq_test(int foo, void *data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 events;\r\nif (!(np->msi_flags & NV_MSI_X_ENABLED)) {\r\nevents = readl(base + NvRegIrqStatus) & NVREG_IRQSTAT_MASK;\r\nwritel(events & NVREG_IRQ_TIMER, base + NvRegIrqStatus);\r\n} else {\r\nevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQSTAT_MASK;\r\nwritel(events & NVREG_IRQ_TIMER, base + NvRegMSIXIrqStatus);\r\n}\r\npci_push(base);\r\nif (!(events & NVREG_IRQ_TIMER))\r\nreturn IRQ_RETVAL(0);\r\nnv_msi_workaround(np);\r\nspin_lock(&np->lock);\r\nnp->intr_test = 1;\r\nspin_unlock(&np->lock);\r\nreturn IRQ_RETVAL(1);\r\n}\r\nstatic void set_msix_vector_map(struct net_device *dev, u32 vector, u32 irqmask)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nint i;\r\nu32 msixmap = 0;\r\nfor (i = 0; i < 8; i++) {\r\nif ((irqmask >> i) & 0x1)\r\nmsixmap |= vector << (i << 2);\r\n}\r\nwritel(readl(base + NvRegMSIXMap0) | msixmap, base + NvRegMSIXMap0);\r\nmsixmap = 0;\r\nfor (i = 0; i < 8; i++) {\r\nif ((irqmask >> (i + 8)) & 0x1)\r\nmsixmap |= vector << (i << 2);\r\n}\r\nwritel(readl(base + NvRegMSIXMap1) | msixmap, base + NvRegMSIXMap1);\r\n}\r\nstatic int nv_request_irq(struct net_device *dev, int intr_test)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint ret;\r\nint i;\r\nirqreturn_t (*handler)(int foo, void *data);\r\nif (intr_test) {\r\nhandler = nv_nic_irq_test;\r\n} else {\r\nif (nv_optimized(np))\r\nhandler = nv_nic_irq_optimized;\r\nelse\r\nhandler = nv_nic_irq;\r\n}\r\nif (np->msi_flags & NV_MSI_X_CAPABLE) {\r\nfor (i = 0; i < (np->msi_flags & NV_MSI_X_VECTORS_MASK); i++)\r\nnp->msi_x_entry[i].entry = i;\r\nret = pci_enable_msix_range(np->pci_dev,\r\nnp->msi_x_entry,\r\nnp->msi_flags & NV_MSI_X_VECTORS_MASK,\r\nnp->msi_flags & NV_MSI_X_VECTORS_MASK);\r\nif (ret > 0) {\r\nnp->msi_flags |= NV_MSI_X_ENABLED;\r\nif (optimization_mode == NV_OPTIMIZATION_MODE_THROUGHPUT && !intr_test) {\r\nsprintf(np->name_rx, "%s-rx", dev->name);\r\nret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector,\r\nnv_nic_irq_rx, IRQF_SHARED, np->name_rx, dev);\r\nif (ret) {\r\nnetdev_info(dev,\r\n"request_irq failed for rx %d\n",\r\nret);\r\npci_disable_msix(np->pci_dev);\r\nnp->msi_flags &= ~NV_MSI_X_ENABLED;\r\ngoto out_err;\r\n}\r\nsprintf(np->name_tx, "%s-tx", dev->name);\r\nret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector,\r\nnv_nic_irq_tx, IRQF_SHARED, np->name_tx, dev);\r\nif (ret) {\r\nnetdev_info(dev,\r\n"request_irq failed for tx %d\n",\r\nret);\r\npci_disable_msix(np->pci_dev);\r\nnp->msi_flags &= ~NV_MSI_X_ENABLED;\r\ngoto out_free_rx;\r\n}\r\nsprintf(np->name_other, "%s-other", dev->name);\r\nret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector,\r\nnv_nic_irq_other, IRQF_SHARED, np->name_other, dev);\r\nif (ret) {\r\nnetdev_info(dev,\r\n"request_irq failed for link %d\n",\r\nret);\r\npci_disable_msix(np->pci_dev);\r\nnp->msi_flags &= ~NV_MSI_X_ENABLED;\r\ngoto out_free_tx;\r\n}\r\nwritel(0, base + NvRegMSIXMap0);\r\nwritel(0, base + NvRegMSIXMap1);\r\nset_msix_vector_map(dev, NV_MSI_X_VECTOR_RX, NVREG_IRQ_RX_ALL);\r\nset_msix_vector_map(dev, NV_MSI_X_VECTOR_TX, NVREG_IRQ_TX_ALL);\r\nset_msix_vector_map(dev, NV_MSI_X_VECTOR_OTHER, NVREG_IRQ_OTHER);\r\n} else {\r\nret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector,\r\nhandler, IRQF_SHARED, dev->name, dev);\r\nif (ret) {\r\nnetdev_info(dev,\r\n"request_irq failed %d\n",\r\nret);\r\npci_disable_msix(np->pci_dev);\r\nnp->msi_flags &= ~NV_MSI_X_ENABLED;\r\ngoto out_err;\r\n}\r\nwritel(0, base + NvRegMSIXMap0);\r\nwritel(0, base + NvRegMSIXMap1);\r\n}\r\nnetdev_info(dev, "MSI-X enabled\n");\r\nreturn 0;\r\n}\r\n}\r\nif (np->msi_flags & NV_MSI_CAPABLE) {\r\nret = pci_enable_msi(np->pci_dev);\r\nif (ret == 0) {\r\nnp->msi_flags |= NV_MSI_ENABLED;\r\nret = request_irq(np->pci_dev->irq, handler, IRQF_SHARED, dev->name, dev);\r\nif (ret) {\r\nnetdev_info(dev, "request_irq failed %d\n",\r\nret);\r\npci_disable_msi(np->pci_dev);\r\nnp->msi_flags &= ~NV_MSI_ENABLED;\r\ngoto out_err;\r\n}\r\nwritel(0, base + NvRegMSIMap0);\r\nwritel(0, base + NvRegMSIMap1);\r\nwritel(NVREG_MSI_VECTOR_0_ENABLED, base + NvRegMSIIrqMask);\r\nnetdev_info(dev, "MSI enabled\n");\r\nreturn 0;\r\n}\r\n}\r\nif (request_irq(np->pci_dev->irq, handler, IRQF_SHARED, dev->name, dev) != 0)\r\ngoto out_err;\r\nreturn 0;\r\nout_free_tx:\r\nfree_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector, dev);\r\nout_free_rx:\r\nfree_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector, dev);\r\nout_err:\r\nreturn 1;\r\n}\r\nstatic void nv_free_irq(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nint i;\r\nif (np->msi_flags & NV_MSI_X_ENABLED) {\r\nfor (i = 0; i < (np->msi_flags & NV_MSI_X_VECTORS_MASK); i++)\r\nfree_irq(np->msi_x_entry[i].vector, dev);\r\npci_disable_msix(np->pci_dev);\r\nnp->msi_flags &= ~NV_MSI_X_ENABLED;\r\n} else {\r\nfree_irq(np->pci_dev->irq, dev);\r\nif (np->msi_flags & NV_MSI_ENABLED) {\r\npci_disable_msi(np->pci_dev);\r\nnp->msi_flags &= ~NV_MSI_ENABLED;\r\n}\r\n}\r\n}\r\nstatic void nv_do_nic_poll(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 mask = 0;\r\nunsigned long flags;\r\nunsigned int irq = 0;\r\nif (!using_multi_irqs(dev)) {\r\nif (np->msi_flags & NV_MSI_X_ENABLED)\r\nirq = np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector;\r\nelse\r\nirq = np->pci_dev->irq;\r\nmask = np->irqmask;\r\n} else {\r\nif (np->nic_poll_irq & NVREG_IRQ_RX_ALL) {\r\nirq = np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector;\r\nmask |= NVREG_IRQ_RX_ALL;\r\n}\r\nif (np->nic_poll_irq & NVREG_IRQ_TX_ALL) {\r\nirq = np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector;\r\nmask |= NVREG_IRQ_TX_ALL;\r\n}\r\nif (np->nic_poll_irq & NVREG_IRQ_OTHER) {\r\nirq = np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector;\r\nmask |= NVREG_IRQ_OTHER;\r\n}\r\n}\r\ndisable_irq_nosync_lockdep_irqsave(irq, &flags);\r\nsynchronize_irq(irq);\r\nif (np->recover_error) {\r\nnp->recover_error = 0;\r\nnetdev_info(dev, "MAC in recoverable error state\n");\r\nif (netif_running(dev)) {\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock(&np->lock);\r\nnv_stop_rxtx(dev);\r\nif (np->driver_data & DEV_HAS_POWER_CNTRL)\r\nnv_mac_reset(dev);\r\nnv_txrx_reset(dev);\r\nnv_drain_rxtx(dev);\r\nset_bufsize(dev);\r\nif (nv_init_ring(dev)) {\r\nif (!np->in_shutdown)\r\nmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\r\n}\r\nwritel(np->rx_buf_sz, base + NvRegOffloadConfig);\r\nsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\r\nwritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\r\nbase + NvRegRingSizes);\r\npci_push(base);\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\npci_push(base);\r\nif (!(np->msi_flags & NV_MSI_X_ENABLED))\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\r\nelse\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegMSIXIrqStatus);\r\nnv_start_rxtx(dev);\r\nspin_unlock(&np->lock);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\n}\r\n}\r\nwritel(mask, base + NvRegIrqMask);\r\npci_push(base);\r\nif (!using_multi_irqs(dev)) {\r\nnp->nic_poll_irq = 0;\r\nif (nv_optimized(np))\r\nnv_nic_irq_optimized(0, dev);\r\nelse\r\nnv_nic_irq(0, dev);\r\n} else {\r\nif (np->nic_poll_irq & NVREG_IRQ_RX_ALL) {\r\nnp->nic_poll_irq &= ~NVREG_IRQ_RX_ALL;\r\nnv_nic_irq_rx(0, dev);\r\n}\r\nif (np->nic_poll_irq & NVREG_IRQ_TX_ALL) {\r\nnp->nic_poll_irq &= ~NVREG_IRQ_TX_ALL;\r\nnv_nic_irq_tx(0, dev);\r\n}\r\nif (np->nic_poll_irq & NVREG_IRQ_OTHER) {\r\nnp->nic_poll_irq &= ~NVREG_IRQ_OTHER;\r\nnv_nic_irq_other(0, dev);\r\n}\r\n}\r\nenable_irq_lockdep_irqrestore(irq, &flags);\r\n}\r\nstatic void nv_poll_controller(struct net_device *dev)\r\n{\r\nnv_do_nic_poll((unsigned long) dev);\r\n}\r\nstatic void nv_do_stats_poll(unsigned long data)\r\n__acquires(&netdev_priv(dev)->hwstats_lock\r\nstatic void nv_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nstrlcpy(info->version, FORCEDETH_VERSION, sizeof(info->version));\r\nstrlcpy(info->bus_info, pci_name(np->pci_dev), sizeof(info->bus_info));\r\n}\r\nstatic void nv_get_wol(struct net_device *dev, struct ethtool_wolinfo *wolinfo)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nwolinfo->supported = WAKE_MAGIC;\r\nspin_lock_irq(&np->lock);\r\nif (np->wolenabled)\r\nwolinfo->wolopts = WAKE_MAGIC;\r\nspin_unlock_irq(&np->lock);\r\n}\r\nstatic int nv_set_wol(struct net_device *dev, struct ethtool_wolinfo *wolinfo)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 flags = 0;\r\nif (wolinfo->wolopts == 0) {\r\nnp->wolenabled = 0;\r\n} else if (wolinfo->wolopts & WAKE_MAGIC) {\r\nnp->wolenabled = 1;\r\nflags = NVREG_WAKEUPFLAGS_ENABLE;\r\n}\r\nif (netif_running(dev)) {\r\nspin_lock_irq(&np->lock);\r\nwritel(flags, base + NvRegWakeUpFlags);\r\nspin_unlock_irq(&np->lock);\r\n}\r\ndevice_set_wakeup_enable(&np->pci_dev->dev, np->wolenabled);\r\nreturn 0;\r\n}\r\nstatic int nv_get_settings(struct net_device *dev, struct ethtool_cmd *ecmd)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 speed;\r\nint adv;\r\nspin_lock_irq(&np->lock);\r\necmd->port = PORT_MII;\r\nif (!netif_running(dev)) {\r\nif (nv_update_linkspeed(dev)) {\r\nif (!netif_carrier_ok(dev))\r\nnetif_carrier_on(dev);\r\n} else {\r\nif (netif_carrier_ok(dev))\r\nnetif_carrier_off(dev);\r\n}\r\n}\r\nif (netif_carrier_ok(dev)) {\r\nswitch (np->linkspeed & (NVREG_LINKSPEED_MASK)) {\r\ncase NVREG_LINKSPEED_10:\r\nspeed = SPEED_10;\r\nbreak;\r\ncase NVREG_LINKSPEED_100:\r\nspeed = SPEED_100;\r\nbreak;\r\ncase NVREG_LINKSPEED_1000:\r\nspeed = SPEED_1000;\r\nbreak;\r\ndefault:\r\nspeed = -1;\r\nbreak;\r\n}\r\necmd->duplex = DUPLEX_HALF;\r\nif (np->duplex)\r\necmd->duplex = DUPLEX_FULL;\r\n} else {\r\nspeed = SPEED_UNKNOWN;\r\necmd->duplex = DUPLEX_UNKNOWN;\r\n}\r\nethtool_cmd_speed_set(ecmd, speed);\r\necmd->autoneg = np->autoneg;\r\necmd->advertising = ADVERTISED_MII;\r\nif (np->autoneg) {\r\necmd->advertising |= ADVERTISED_Autoneg;\r\nadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\r\nif (adv & ADVERTISE_10HALF)\r\necmd->advertising |= ADVERTISED_10baseT_Half;\r\nif (adv & ADVERTISE_10FULL)\r\necmd->advertising |= ADVERTISED_10baseT_Full;\r\nif (adv & ADVERTISE_100HALF)\r\necmd->advertising |= ADVERTISED_100baseT_Half;\r\nif (adv & ADVERTISE_100FULL)\r\necmd->advertising |= ADVERTISED_100baseT_Full;\r\nif (np->gigabit == PHY_GIGABIT) {\r\nadv = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\r\nif (adv & ADVERTISE_1000FULL)\r\necmd->advertising |= ADVERTISED_1000baseT_Full;\r\n}\r\n}\r\necmd->supported = (SUPPORTED_Autoneg |\r\nSUPPORTED_10baseT_Half | SUPPORTED_10baseT_Full |\r\nSUPPORTED_100baseT_Half | SUPPORTED_100baseT_Full |\r\nSUPPORTED_MII);\r\nif (np->gigabit == PHY_GIGABIT)\r\necmd->supported |= SUPPORTED_1000baseT_Full;\r\necmd->phy_address = np->phyaddr;\r\necmd->transceiver = XCVR_EXTERNAL;\r\nspin_unlock_irq(&np->lock);\r\nreturn 0;\r\n}\r\nstatic int nv_set_settings(struct net_device *dev, struct ethtool_cmd *ecmd)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu32 speed = ethtool_cmd_speed(ecmd);\r\nif (ecmd->port != PORT_MII)\r\nreturn -EINVAL;\r\nif (ecmd->transceiver != XCVR_EXTERNAL)\r\nreturn -EINVAL;\r\nif (ecmd->phy_address != np->phyaddr) {\r\nreturn -EINVAL;\r\n}\r\nif (ecmd->autoneg == AUTONEG_ENABLE) {\r\nu32 mask;\r\nmask = ADVERTISED_10baseT_Half | ADVERTISED_10baseT_Full |\r\nADVERTISED_100baseT_Half | ADVERTISED_100baseT_Full;\r\nif (np->gigabit == PHY_GIGABIT)\r\nmask |= ADVERTISED_1000baseT_Full;\r\nif ((ecmd->advertising & mask) == 0)\r\nreturn -EINVAL;\r\n} else if (ecmd->autoneg == AUTONEG_DISABLE) {\r\nif (speed != SPEED_10 && speed != SPEED_100)\r\nreturn -EINVAL;\r\nif (ecmd->duplex != DUPLEX_HALF && ecmd->duplex != DUPLEX_FULL)\r\nreturn -EINVAL;\r\n} else {\r\nreturn -EINVAL;\r\n}\r\nnetif_carrier_off(dev);\r\nif (netif_running(dev)) {\r\nunsigned long flags;\r\nnv_disable_irq(dev);\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_stop_rxtx(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\n}\r\nif (ecmd->autoneg == AUTONEG_ENABLE) {\r\nint adv, bmcr;\r\nnp->autoneg = 1;\r\nadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\r\nadv &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4 | ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\r\nif (ecmd->advertising & ADVERTISED_10baseT_Half)\r\nadv |= ADVERTISE_10HALF;\r\nif (ecmd->advertising & ADVERTISED_10baseT_Full)\r\nadv |= ADVERTISE_10FULL;\r\nif (ecmd->advertising & ADVERTISED_100baseT_Half)\r\nadv |= ADVERTISE_100HALF;\r\nif (ecmd->advertising & ADVERTISED_100baseT_Full)\r\nadv |= ADVERTISE_100FULL;\r\nif (np->pause_flags & NV_PAUSEFRAME_RX_REQ)\r\nadv |= ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\r\nif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\r\nadv |= ADVERTISE_PAUSE_ASYM;\r\nmii_rw(dev, np->phyaddr, MII_ADVERTISE, adv);\r\nif (np->gigabit == PHY_GIGABIT) {\r\nadv = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\r\nadv &= ~ADVERTISE_1000FULL;\r\nif (ecmd->advertising & ADVERTISED_1000baseT_Full)\r\nadv |= ADVERTISE_1000FULL;\r\nmii_rw(dev, np->phyaddr, MII_CTRL1000, adv);\r\n}\r\nif (netif_running(dev))\r\nnetdev_info(dev, "link down\n");\r\nbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nif (np->phy_model == PHY_MODEL_MARVELL_E3016) {\r\nbmcr |= BMCR_ANENABLE;\r\nif (phy_reset(dev, bmcr)) {\r\nnetdev_info(dev, "phy reset failed\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nbmcr |= (BMCR_ANENABLE | BMCR_ANRESTART);\r\nmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\r\n}\r\n} else {\r\nint adv, bmcr;\r\nnp->autoneg = 0;\r\nadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\r\nadv &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4 | ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\r\nif (speed == SPEED_10 && ecmd->duplex == DUPLEX_HALF)\r\nadv |= ADVERTISE_10HALF;\r\nif (speed == SPEED_10 && ecmd->duplex == DUPLEX_FULL)\r\nadv |= ADVERTISE_10FULL;\r\nif (speed == SPEED_100 && ecmd->duplex == DUPLEX_HALF)\r\nadv |= ADVERTISE_100HALF;\r\nif (speed == SPEED_100 && ecmd->duplex == DUPLEX_FULL)\r\nadv |= ADVERTISE_100FULL;\r\nnp->pause_flags &= ~(NV_PAUSEFRAME_AUTONEG|NV_PAUSEFRAME_RX_ENABLE|NV_PAUSEFRAME_TX_ENABLE);\r\nif (np->pause_flags & NV_PAUSEFRAME_RX_REQ) {\r\nadv |= ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\r\nnp->pause_flags |= NV_PAUSEFRAME_RX_ENABLE;\r\n}\r\nif (np->pause_flags & NV_PAUSEFRAME_TX_REQ) {\r\nadv |= ADVERTISE_PAUSE_ASYM;\r\nnp->pause_flags |= NV_PAUSEFRAME_TX_ENABLE;\r\n}\r\nmii_rw(dev, np->phyaddr, MII_ADVERTISE, adv);\r\nnp->fixed_mode = adv;\r\nif (np->gigabit == PHY_GIGABIT) {\r\nadv = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\r\nadv &= ~ADVERTISE_1000FULL;\r\nmii_rw(dev, np->phyaddr, MII_CTRL1000, adv);\r\n}\r\nbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nbmcr &= ~(BMCR_ANENABLE|BMCR_SPEED100|BMCR_SPEED1000|BMCR_FULLDPLX);\r\nif (np->fixed_mode & (ADVERTISE_10FULL|ADVERTISE_100FULL))\r\nbmcr |= BMCR_FULLDPLX;\r\nif (np->fixed_mode & (ADVERTISE_100HALF|ADVERTISE_100FULL))\r\nbmcr |= BMCR_SPEED100;\r\nif (np->phy_oui == PHY_OUI_MARVELL) {\r\nif (phy_reset(dev, bmcr)) {\r\nnetdev_info(dev, "phy reset failed\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\r\nif (netif_running(dev)) {\r\nudelay(10);\r\nnv_linkchange(dev);\r\n}\r\n}\r\n}\r\nif (netif_running(dev)) {\r\nnv_start_rxtx(dev);\r\nnv_enable_irq(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int nv_get_regs_len(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nreturn np->register_size;\r\n}\r\nstatic void nv_get_regs(struct net_device *dev, struct ethtool_regs *regs, void *buf)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 *rbuf = buf;\r\nint i;\r\nregs->version = FORCEDETH_REGS_VER;\r\nspin_lock_irq(&np->lock);\r\nfor (i = 0; i < np->register_size/sizeof(u32); i++)\r\nrbuf[i] = readl(base + i*sizeof(u32));\r\nspin_unlock_irq(&np->lock);\r\n}\r\nstatic int nv_nway_reset(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nint ret;\r\nif (np->autoneg) {\r\nint bmcr;\r\nnetif_carrier_off(dev);\r\nif (netif_running(dev)) {\r\nnv_disable_irq(dev);\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock(&np->lock);\r\nnv_stop_rxtx(dev);\r\nspin_unlock(&np->lock);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\nnetdev_info(dev, "link down\n");\r\n}\r\nbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nif (np->phy_model == PHY_MODEL_MARVELL_E3016) {\r\nbmcr |= BMCR_ANENABLE;\r\nif (phy_reset(dev, bmcr)) {\r\nnetdev_info(dev, "phy reset failed\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nbmcr |= (BMCR_ANENABLE | BMCR_ANRESTART);\r\nmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\r\n}\r\nif (netif_running(dev)) {\r\nnv_start_rxtx(dev);\r\nnv_enable_irq(dev);\r\n}\r\nret = 0;\r\n} else {\r\nret = -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nstatic void nv_get_ringparam(struct net_device *dev, struct ethtool_ringparam* ring)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nring->rx_max_pending = (np->desc_ver == DESC_VER_1) ? RING_MAX_DESC_VER_1 : RING_MAX_DESC_VER_2_3;\r\nring->tx_max_pending = (np->desc_ver == DESC_VER_1) ? RING_MAX_DESC_VER_1 : RING_MAX_DESC_VER_2_3;\r\nring->rx_pending = np->rx_ring_size;\r\nring->tx_pending = np->tx_ring_size;\r\n}\r\nstatic int nv_set_ringparam(struct net_device *dev, struct ethtool_ringparam* ring)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu8 *rxtx_ring, *rx_skbuff, *tx_skbuff;\r\ndma_addr_t ring_addr;\r\nif (ring->rx_pending < RX_RING_MIN ||\r\nring->tx_pending < TX_RING_MIN ||\r\nring->rx_mini_pending != 0 ||\r\nring->rx_jumbo_pending != 0 ||\r\n(np->desc_ver == DESC_VER_1 &&\r\n(ring->rx_pending > RING_MAX_DESC_VER_1 ||\r\nring->tx_pending > RING_MAX_DESC_VER_1)) ||\r\n(np->desc_ver != DESC_VER_1 &&\r\n(ring->rx_pending > RING_MAX_DESC_VER_2_3 ||\r\nring->tx_pending > RING_MAX_DESC_VER_2_3))) {\r\nreturn -EINVAL;\r\n}\r\nif (!nv_optimized(np)) {\r\nrxtx_ring = pci_alloc_consistent(np->pci_dev,\r\nsizeof(struct ring_desc) * (ring->rx_pending + ring->tx_pending),\r\n&ring_addr);\r\n} else {\r\nrxtx_ring = pci_alloc_consistent(np->pci_dev,\r\nsizeof(struct ring_desc_ex) * (ring->rx_pending + ring->tx_pending),\r\n&ring_addr);\r\n}\r\nrx_skbuff = kmalloc(sizeof(struct nv_skb_map) * ring->rx_pending, GFP_KERNEL);\r\ntx_skbuff = kmalloc(sizeof(struct nv_skb_map) * ring->tx_pending, GFP_KERNEL);\r\nif (!rxtx_ring || !rx_skbuff || !tx_skbuff) {\r\nif (!nv_optimized(np)) {\r\nif (rxtx_ring)\r\npci_free_consistent(np->pci_dev, sizeof(struct ring_desc) * (ring->rx_pending + ring->tx_pending),\r\nrxtx_ring, ring_addr);\r\n} else {\r\nif (rxtx_ring)\r\npci_free_consistent(np->pci_dev, sizeof(struct ring_desc_ex) * (ring->rx_pending + ring->tx_pending),\r\nrxtx_ring, ring_addr);\r\n}\r\nkfree(rx_skbuff);\r\nkfree(tx_skbuff);\r\ngoto exit;\r\n}\r\nif (netif_running(dev)) {\r\nnv_disable_irq(dev);\r\nnv_napi_disable(dev);\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock(&np->lock);\r\nnv_stop_rxtx(dev);\r\nnv_txrx_reset(dev);\r\nnv_drain_rxtx(dev);\r\nfree_rings(dev);\r\n}\r\nnp->rx_ring_size = ring->rx_pending;\r\nnp->tx_ring_size = ring->tx_pending;\r\nif (!nv_optimized(np)) {\r\nnp->rx_ring.orig = (struct ring_desc *)rxtx_ring;\r\nnp->tx_ring.orig = &np->rx_ring.orig[np->rx_ring_size];\r\n} else {\r\nnp->rx_ring.ex = (struct ring_desc_ex *)rxtx_ring;\r\nnp->tx_ring.ex = &np->rx_ring.ex[np->rx_ring_size];\r\n}\r\nnp->rx_skb = (struct nv_skb_map *)rx_skbuff;\r\nnp->tx_skb = (struct nv_skb_map *)tx_skbuff;\r\nnp->ring_addr = ring_addr;\r\nmemset(np->rx_skb, 0, sizeof(struct nv_skb_map) * np->rx_ring_size);\r\nmemset(np->tx_skb, 0, sizeof(struct nv_skb_map) * np->tx_ring_size);\r\nif (netif_running(dev)) {\r\nset_bufsize(dev);\r\nif (nv_init_ring(dev)) {\r\nif (!np->in_shutdown)\r\nmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\r\n}\r\nwritel(np->rx_buf_sz, base + NvRegOffloadConfig);\r\nsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\r\nwritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\r\nbase + NvRegRingSizes);\r\npci_push(base);\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\npci_push(base);\r\nnv_start_rxtx(dev);\r\nspin_unlock(&np->lock);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\nnv_napi_enable(dev);\r\nnv_enable_irq(dev);\r\n}\r\nreturn 0;\r\nexit:\r\nreturn -ENOMEM;\r\n}\r\nstatic void nv_get_pauseparam(struct net_device *dev, struct ethtool_pauseparam* pause)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\npause->autoneg = (np->pause_flags & NV_PAUSEFRAME_AUTONEG) != 0;\r\npause->rx_pause = (np->pause_flags & NV_PAUSEFRAME_RX_ENABLE) != 0;\r\npause->tx_pause = (np->pause_flags & NV_PAUSEFRAME_TX_ENABLE) != 0;\r\n}\r\nstatic int nv_set_pauseparam(struct net_device *dev, struct ethtool_pauseparam* pause)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nint adv, bmcr;\r\nif ((!np->autoneg && np->duplex == 0) ||\r\n(np->autoneg && !pause->autoneg && np->duplex == 0)) {\r\nnetdev_info(dev, "can not set pause settings when forced link is in half duplex\n");\r\nreturn -EINVAL;\r\n}\r\nif (pause->tx_pause && !(np->pause_flags & NV_PAUSEFRAME_TX_CAPABLE)) {\r\nnetdev_info(dev, "hardware does not support tx pause frames\n");\r\nreturn -EINVAL;\r\n}\r\nnetif_carrier_off(dev);\r\nif (netif_running(dev)) {\r\nnv_disable_irq(dev);\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock(&np->lock);\r\nnv_stop_rxtx(dev);\r\nspin_unlock(&np->lock);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\n}\r\nnp->pause_flags &= ~(NV_PAUSEFRAME_RX_REQ|NV_PAUSEFRAME_TX_REQ);\r\nif (pause->rx_pause)\r\nnp->pause_flags |= NV_PAUSEFRAME_RX_REQ;\r\nif (pause->tx_pause)\r\nnp->pause_flags |= NV_PAUSEFRAME_TX_REQ;\r\nif (np->autoneg && pause->autoneg) {\r\nnp->pause_flags |= NV_PAUSEFRAME_AUTONEG;\r\nadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\r\nadv &= ~(ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\r\nif (np->pause_flags & NV_PAUSEFRAME_RX_REQ)\r\nadv |= ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\r\nif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\r\nadv |= ADVERTISE_PAUSE_ASYM;\r\nmii_rw(dev, np->phyaddr, MII_ADVERTISE, adv);\r\nif (netif_running(dev))\r\nnetdev_info(dev, "link down\n");\r\nbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nbmcr |= (BMCR_ANENABLE | BMCR_ANRESTART);\r\nmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\r\n} else {\r\nnp->pause_flags &= ~(NV_PAUSEFRAME_AUTONEG|NV_PAUSEFRAME_RX_ENABLE|NV_PAUSEFRAME_TX_ENABLE);\r\nif (pause->rx_pause)\r\nnp->pause_flags |= NV_PAUSEFRAME_RX_ENABLE;\r\nif (pause->tx_pause)\r\nnp->pause_flags |= NV_PAUSEFRAME_TX_ENABLE;\r\nif (!netif_running(dev))\r\nnv_update_linkspeed(dev);\r\nelse\r\nnv_update_pause(dev, np->pause_flags);\r\n}\r\nif (netif_running(dev)) {\r\nnv_start_rxtx(dev);\r\nnv_enable_irq(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int nv_set_loopback(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nunsigned long flags;\r\nu32 miicontrol;\r\nint err, retval = 0;\r\nspin_lock_irqsave(&np->lock, flags);\r\nmiicontrol = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nif (features & NETIF_F_LOOPBACK) {\r\nif (miicontrol & BMCR_LOOPBACK) {\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetdev_info(dev, "Loopback already enabled\n");\r\nreturn 0;\r\n}\r\nnv_disable_irq(dev);\r\nmiicontrol |= BMCR_LOOPBACK | BMCR_FULLDPLX | BMCR_SPEED1000;\r\nerr = mii_rw(dev, np->phyaddr, MII_BMCR, miicontrol);\r\nif (err) {\r\nretval = PHY_ERROR;\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nphy_init(dev);\r\n} else {\r\nif (netif_running(dev)) {\r\nnv_force_linkspeed(dev, NVREG_LINKSPEED_1000,\r\n1);\r\nnetif_carrier_on(dev);\r\n}\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetdev_info(dev,\r\n"Internal PHY loopback mode enabled.\n");\r\n}\r\n} else {\r\nif (!(miicontrol & BMCR_LOOPBACK)) {\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetdev_info(dev, "Loopback already disabled\n");\r\nreturn 0;\r\n}\r\nnv_disable_irq(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetdev_info(dev, "Internal PHY loopback mode disabled.\n");\r\nphy_init(dev);\r\n}\r\nmsleep(500);\r\nspin_lock_irqsave(&np->lock, flags);\r\nnv_enable_irq(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nreturn retval;\r\n}\r\nstatic netdev_features_t nv_fix_features(struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nif (features & (NETIF_F_HW_VLAN_CTAG_TX|NETIF_F_HW_VLAN_CTAG_RX))\r\nfeatures |= NETIF_F_RXCSUM;\r\nreturn features;\r\n}\r\nstatic void nv_vlan_mode(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct fe_priv *np = get_nvpriv(dev);\r\nspin_lock_irq(&np->lock);\r\nif (features & NETIF_F_HW_VLAN_CTAG_RX)\r\nnp->txrxctl_bits |= NVREG_TXRXCTL_VLANSTRIP;\r\nelse\r\nnp->txrxctl_bits &= ~NVREG_TXRXCTL_VLANSTRIP;\r\nif (features & NETIF_F_HW_VLAN_CTAG_TX)\r\nnp->txrxctl_bits |= NVREG_TXRXCTL_VLANINS;\r\nelse\r\nnp->txrxctl_bits &= ~NVREG_TXRXCTL_VLANINS;\r\nwritel(np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\nspin_unlock_irq(&np->lock);\r\n}\r\nstatic int nv_set_features(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nnetdev_features_t changed = dev->features ^ features;\r\nint retval;\r\nif ((changed & NETIF_F_LOOPBACK) && netif_running(dev)) {\r\nretval = nv_set_loopback(dev, features);\r\nif (retval != 0)\r\nreturn retval;\r\n}\r\nif (changed & NETIF_F_RXCSUM) {\r\nspin_lock_irq(&np->lock);\r\nif (features & NETIF_F_RXCSUM)\r\nnp->txrxctl_bits |= NVREG_TXRXCTL_RXCHECK;\r\nelse\r\nnp->txrxctl_bits &= ~NVREG_TXRXCTL_RXCHECK;\r\nif (netif_running(dev))\r\nwritel(np->txrxctl_bits, base + NvRegTxRxControl);\r\nspin_unlock_irq(&np->lock);\r\n}\r\nif (changed & (NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX))\r\nnv_vlan_mode(dev, features);\r\nreturn 0;\r\n}\r\nstatic int nv_get_sset_count(struct net_device *dev, int sset)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nswitch (sset) {\r\ncase ETH_SS_TEST:\r\nif (np->driver_data & DEV_HAS_TEST_EXTENDED)\r\nreturn NV_TEST_COUNT_EXTENDED;\r\nelse\r\nreturn NV_TEST_COUNT_BASE;\r\ncase ETH_SS_STATS:\r\nif (np->driver_data & DEV_HAS_STATISTICS_V3)\r\nreturn NV_DEV_STATISTICS_V3_COUNT;\r\nelse if (np->driver_data & DEV_HAS_STATISTICS_V2)\r\nreturn NV_DEV_STATISTICS_V2_COUNT;\r\nelse if (np->driver_data & DEV_HAS_STATISTICS_V1)\r\nreturn NV_DEV_STATISTICS_V1_COUNT;\r\nelse\r\nreturn 0;\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void nv_get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *estats, u64 *buffer)\r\n__acquires(&netdev_priv(dev)->hwstats_lock\r\nstatic int nv_link_test(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nint mii_status;\r\nmii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\r\nmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\r\nif (!(mii_status & BMSR_LSTATUS))\r\nreturn 0;\r\nelse\r\nreturn 1;\r\n}\r\nstatic int nv_register_test(struct net_device *dev)\r\n{\r\nu8 __iomem *base = get_hwbase(dev);\r\nint i = 0;\r\nu32 orig_read, new_read;\r\ndo {\r\norig_read = readl(base + nv_registers_test[i].reg);\r\norig_read ^= nv_registers_test[i].mask;\r\nwritel(orig_read, base + nv_registers_test[i].reg);\r\nnew_read = readl(base + nv_registers_test[i].reg);\r\nif ((new_read & nv_registers_test[i].mask) != (orig_read & nv_registers_test[i].mask))\r\nreturn 0;\r\norig_read ^= nv_registers_test[i].mask;\r\nwritel(orig_read, base + nv_registers_test[i].reg);\r\n} while (nv_registers_test[++i].reg != 0);\r\nreturn 1;\r\n}\r\nstatic int nv_interrupt_test(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint ret = 1;\r\nint testcnt;\r\nu32 save_msi_flags, save_poll_interval = 0;\r\nif (netif_running(dev)) {\r\nnv_free_irq(dev);\r\nsave_poll_interval = readl(base+NvRegPollingInterval);\r\n}\r\nnp->intr_test = 0;\r\nsave_msi_flags = np->msi_flags;\r\nnp->msi_flags &= ~NV_MSI_X_VECTORS_MASK;\r\nnp->msi_flags |= 0x001;\r\nif (nv_request_irq(dev, 1))\r\nreturn 0;\r\nwritel(NVREG_POLL_DEFAULT_CPU, base + NvRegPollingInterval);\r\nwritel(NVREG_UNKSETUP6_VAL, base + NvRegUnknownSetupReg6);\r\nnv_enable_hw_interrupts(dev, NVREG_IRQ_TIMER);\r\nmsleep(100);\r\nspin_lock_irq(&np->lock);\r\ntestcnt = np->intr_test;\r\nif (!testcnt)\r\nret = 2;\r\nnv_disable_hw_interrupts(dev, NVREG_IRQ_TIMER);\r\nif (!(np->msi_flags & NV_MSI_X_ENABLED))\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\r\nelse\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegMSIXIrqStatus);\r\nspin_unlock_irq(&np->lock);\r\nnv_free_irq(dev);\r\nnp->msi_flags = save_msi_flags;\r\nif (netif_running(dev)) {\r\nwritel(save_poll_interval, base + NvRegPollingInterval);\r\nwritel(NVREG_UNKSETUP6_VAL, base + NvRegUnknownSetupReg6);\r\nif (nv_request_irq(dev, 0))\r\nreturn 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic int nv_loopback_test(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nstruct sk_buff *tx_skb, *rx_skb;\r\ndma_addr_t test_dma_addr;\r\nu32 tx_flags_extra = (np->desc_ver == DESC_VER_1 ? NV_TX_LASTPACKET : NV_TX2_LASTPACKET);\r\nu32 flags;\r\nint len, i, pkt_len;\r\nu8 *pkt_data;\r\nu32 filter_flags = 0;\r\nu32 misc1_flags = 0;\r\nint ret = 1;\r\nif (netif_running(dev)) {\r\nnv_disable_irq(dev);\r\nfilter_flags = readl(base + NvRegPacketFilterFlags);\r\nmisc1_flags = readl(base + NvRegMisc1);\r\n} else {\r\nnv_txrx_reset(dev);\r\n}\r\nset_bufsize(dev);\r\nnv_init_ring(dev);\r\nwritel(NVREG_MISC1_FORCE, base + NvRegMisc1);\r\nwritel(NVREG_PFF_ALWAYS | NVREG_PFF_LOOPBACK, base + NvRegPacketFilterFlags);\r\nwritel(np->rx_buf_sz, base + NvRegOffloadConfig);\r\nsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\r\nwritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\r\nbase + NvRegRingSizes);\r\npci_push(base);\r\nnv_start_rxtx(dev);\r\npkt_len = ETH_DATA_LEN;\r\ntx_skb = netdev_alloc_skb(dev, pkt_len);\r\nif (!tx_skb) {\r\nret = 0;\r\ngoto out;\r\n}\r\ntest_dma_addr = pci_map_single(np->pci_dev, tx_skb->data,\r\nskb_tailroom(tx_skb),\r\nPCI_DMA_FROMDEVICE);\r\nif (pci_dma_mapping_error(np->pci_dev,\r\ntest_dma_addr)) {\r\ndev_kfree_skb_any(tx_skb);\r\ngoto out;\r\n}\r\npkt_data = skb_put(tx_skb, pkt_len);\r\nfor (i = 0; i < pkt_len; i++)\r\npkt_data[i] = (u8)(i & 0xff);\r\nif (!nv_optimized(np)) {\r\nnp->tx_ring.orig[0].buf = cpu_to_le32(test_dma_addr);\r\nnp->tx_ring.orig[0].flaglen = cpu_to_le32((pkt_len-1) | np->tx_flags | tx_flags_extra);\r\n} else {\r\nnp->tx_ring.ex[0].bufhigh = cpu_to_le32(dma_high(test_dma_addr));\r\nnp->tx_ring.ex[0].buflow = cpu_to_le32(dma_low(test_dma_addr));\r\nnp->tx_ring.ex[0].flaglen = cpu_to_le32((pkt_len-1) | np->tx_flags | tx_flags_extra);\r\n}\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\npci_push(get_hwbase(dev));\r\nmsleep(500);\r\nif (!nv_optimized(np)) {\r\nflags = le32_to_cpu(np->rx_ring.orig[0].flaglen);\r\nlen = nv_descr_getlength(&np->rx_ring.orig[0], np->desc_ver);\r\n} else {\r\nflags = le32_to_cpu(np->rx_ring.ex[0].flaglen);\r\nlen = nv_descr_getlength_ex(&np->rx_ring.ex[0], np->desc_ver);\r\n}\r\nif (flags & NV_RX_AVAIL) {\r\nret = 0;\r\n} else if (np->desc_ver == DESC_VER_1) {\r\nif (flags & NV_RX_ERROR)\r\nret = 0;\r\n} else {\r\nif (flags & NV_RX2_ERROR)\r\nret = 0;\r\n}\r\nif (ret) {\r\nif (len != pkt_len) {\r\nret = 0;\r\n} else {\r\nrx_skb = np->rx_skb[0].skb;\r\nfor (i = 0; i < pkt_len; i++) {\r\nif (rx_skb->data[i] != (u8)(i & 0xff)) {\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\n}\r\n}\r\npci_unmap_single(np->pci_dev, test_dma_addr,\r\n(skb_end_pointer(tx_skb) - tx_skb->data),\r\nPCI_DMA_TODEVICE);\r\ndev_kfree_skb_any(tx_skb);\r\nout:\r\nnv_stop_rxtx(dev);\r\nnv_txrx_reset(dev);\r\nnv_drain_rxtx(dev);\r\nif (netif_running(dev)) {\r\nwritel(misc1_flags, base + NvRegMisc1);\r\nwritel(filter_flags, base + NvRegPacketFilterFlags);\r\nnv_enable_irq(dev);\r\n}\r\nreturn ret;\r\n}\r\nstatic void nv_self_test(struct net_device *dev, struct ethtool_test *test, u64 *buffer)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint result, count;\r\ncount = nv_get_sset_count(dev, ETH_SS_TEST);\r\nmemset(buffer, 0, count * sizeof(u64));\r\nif (!nv_link_test(dev)) {\r\ntest->flags |= ETH_TEST_FL_FAILED;\r\nbuffer[0] = 1;\r\n}\r\nif (test->flags & ETH_TEST_FL_OFFLINE) {\r\nif (netif_running(dev)) {\r\nnetif_stop_queue(dev);\r\nnv_napi_disable(dev);\r\nnetif_tx_lock_bh(dev);\r\nnetif_addr_lock(dev);\r\nspin_lock_irq(&np->lock);\r\nnv_disable_hw_interrupts(dev, np->irqmask);\r\nif (!(np->msi_flags & NV_MSI_X_ENABLED))\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\r\nelse\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegMSIXIrqStatus);\r\nnv_stop_rxtx(dev);\r\nnv_txrx_reset(dev);\r\nnv_drain_rxtx(dev);\r\nspin_unlock_irq(&np->lock);\r\nnetif_addr_unlock(dev);\r\nnetif_tx_unlock_bh(dev);\r\n}\r\nif (!nv_register_test(dev)) {\r\ntest->flags |= ETH_TEST_FL_FAILED;\r\nbuffer[1] = 1;\r\n}\r\nresult = nv_interrupt_test(dev);\r\nif (result != 1) {\r\ntest->flags |= ETH_TEST_FL_FAILED;\r\nbuffer[2] = 1;\r\n}\r\nif (result == 0) {\r\nreturn;\r\n}\r\nif (count > NV_TEST_COUNT_BASE && !nv_loopback_test(dev)) {\r\ntest->flags |= ETH_TEST_FL_FAILED;\r\nbuffer[3] = 1;\r\n}\r\nif (netif_running(dev)) {\r\nset_bufsize(dev);\r\nif (nv_init_ring(dev)) {\r\nif (!np->in_shutdown)\r\nmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\r\n}\r\nwritel(np->rx_buf_sz, base + NvRegOffloadConfig);\r\nsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\r\nwritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\r\nbase + NvRegRingSizes);\r\npci_push(base);\r\nwritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\r\npci_push(base);\r\nnv_start_rxtx(dev);\r\nnetif_start_queue(dev);\r\nnv_napi_enable(dev);\r\nnv_enable_hw_interrupts(dev, np->irqmask);\r\n}\r\n}\r\n}\r\nstatic void nv_get_strings(struct net_device *dev, u32 stringset, u8 *buffer)\r\n{\r\nswitch (stringset) {\r\ncase ETH_SS_STATS:\r\nmemcpy(buffer, &nv_estats_str, nv_get_sset_count(dev, ETH_SS_STATS)*sizeof(struct nv_ethtool_str));\r\nbreak;\r\ncase ETH_SS_TEST:\r\nmemcpy(buffer, &nv_etests_str, nv_get_sset_count(dev, ETH_SS_TEST)*sizeof(struct nv_ethtool_str));\r\nbreak;\r\n}\r\n}\r\nstatic int nv_mgmt_acquire_sema(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint i;\r\nu32 tx_ctrl, mgmt_sema;\r\nfor (i = 0; i < 10; i++) {\r\nmgmt_sema = readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_MGMT_SEMA_MASK;\r\nif (mgmt_sema == NVREG_XMITCTL_MGMT_SEMA_FREE)\r\nbreak;\r\nmsleep(500);\r\n}\r\nif (mgmt_sema != NVREG_XMITCTL_MGMT_SEMA_FREE)\r\nreturn 0;\r\nfor (i = 0; i < 2; i++) {\r\ntx_ctrl = readl(base + NvRegTransmitterControl);\r\ntx_ctrl |= NVREG_XMITCTL_HOST_SEMA_ACQ;\r\nwritel(tx_ctrl, base + NvRegTransmitterControl);\r\ntx_ctrl = readl(base + NvRegTransmitterControl);\r\nif (((tx_ctrl & NVREG_XMITCTL_HOST_SEMA_MASK) == NVREG_XMITCTL_HOST_SEMA_ACQ) &&\r\n((tx_ctrl & NVREG_XMITCTL_MGMT_SEMA_MASK) == NVREG_XMITCTL_MGMT_SEMA_FREE)) {\r\nnp->mgmt_sema = 1;\r\nreturn 1;\r\n} else\r\nudelay(50);\r\n}\r\nreturn 0;\r\n}\r\nstatic void nv_mgmt_release_sema(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 tx_ctrl;\r\nif (np->driver_data & DEV_HAS_MGMT_UNIT) {\r\nif (np->mgmt_sema) {\r\ntx_ctrl = readl(base + NvRegTransmitterControl);\r\ntx_ctrl &= ~NVREG_XMITCTL_HOST_SEMA_ACQ;\r\nwritel(tx_ctrl, base + NvRegTransmitterControl);\r\n}\r\n}\r\n}\r\nstatic int nv_mgmt_get_version(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nu32 data_ready = readl(base + NvRegTransmitterControl);\r\nu32 data_ready2 = 0;\r\nunsigned long start;\r\nint ready = 0;\r\nwritel(NVREG_MGMTUNITGETVERSION, base + NvRegMgmtUnitGetVersion);\r\nwritel(data_ready ^ NVREG_XMITCTL_DATA_START, base + NvRegTransmitterControl);\r\nstart = jiffies;\r\nwhile (time_before(jiffies, start + 5*HZ)) {\r\ndata_ready2 = readl(base + NvRegTransmitterControl);\r\nif ((data_ready & NVREG_XMITCTL_DATA_READY) != (data_ready2 & NVREG_XMITCTL_DATA_READY)) {\r\nready = 1;\r\nbreak;\r\n}\r\nschedule_timeout_uninterruptible(1);\r\n}\r\nif (!ready || (data_ready2 & NVREG_XMITCTL_DATA_ERROR))\r\nreturn 0;\r\nnp->mgmt_version = readl(base + NvRegMgmtUnitVersion) & NVREG_MGMTUNITVERSION;\r\nreturn 1;\r\n}\r\nstatic int nv_open(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint ret = 1;\r\nint oom, i;\r\nu32 low;\r\nmii_rw(dev, np->phyaddr, MII_BMCR,\r\nmii_rw(dev, np->phyaddr, MII_BMCR, MII_READ) & ~BMCR_PDOWN);\r\nnv_txrx_gate(dev, false);\r\nif (np->driver_data & DEV_HAS_POWER_CNTRL)\r\nnv_mac_reset(dev);\r\nwritel(NVREG_MCASTADDRA_FORCE, base + NvRegMulticastAddrA);\r\nwritel(0, base + NvRegMulticastAddrB);\r\nwritel(NVREG_MCASTMASKA_NONE, base + NvRegMulticastMaskA);\r\nwritel(NVREG_MCASTMASKB_NONE, base + NvRegMulticastMaskB);\r\nwritel(0, base + NvRegPacketFilterFlags);\r\nwritel(0, base + NvRegTransmitterControl);\r\nwritel(0, base + NvRegReceiverControl);\r\nwritel(0, base + NvRegAdapterControl);\r\nif (np->pause_flags & NV_PAUSEFRAME_TX_CAPABLE)\r\nwritel(NVREG_TX_PAUSEFRAME_DISABLE, base + NvRegTxPauseFrame);\r\nset_bufsize(dev);\r\noom = nv_init_ring(dev);\r\nwritel(0, base + NvRegLinkSpeed);\r\nwritel(readl(base + NvRegTransmitPoll) & NVREG_TRANSMITPOLL_MAC_ADDR_REV, base + NvRegTransmitPoll);\r\nnv_txrx_reset(dev);\r\nwritel(0, base + NvRegUnknownSetupReg6);\r\nnp->in_shutdown = 0;\r\nsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\r\nwritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\r\nbase + NvRegRingSizes);\r\nwritel(np->linkspeed, base + NvRegLinkSpeed);\r\nif (np->desc_ver == DESC_VER_1)\r\nwritel(NVREG_TX_WM_DESC1_DEFAULT, base + NvRegTxWatermark);\r\nelse\r\nwritel(NVREG_TX_WM_DESC2_3_DEFAULT, base + NvRegTxWatermark);\r\nwritel(np->txrxctl_bits, base + NvRegTxRxControl);\r\nwritel(np->vlanctl_bits, base + NvRegVlanControl);\r\npci_push(base);\r\nwritel(NVREG_TXRXCTL_BIT1|np->txrxctl_bits, base + NvRegTxRxControl);\r\nif (reg_delay(dev, NvRegUnknownSetupReg5,\r\nNVREG_UNKSETUP5_BIT31, NVREG_UNKSETUP5_BIT31,\r\nNV_SETUP5_DELAY, NV_SETUP5_DELAYMAX))\r\nnetdev_info(dev,\r\n"%s: SetupReg5, Bit 31 remained off\n", __func__);\r\nwritel(0, base + NvRegMIIMask);\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\r\nwritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\r\nwritel(NVREG_MISC1_FORCE | NVREG_MISC1_HD, base + NvRegMisc1);\r\nwritel(readl(base + NvRegTransmitterStatus), base + NvRegTransmitterStatus);\r\nwritel(NVREG_PFF_ALWAYS, base + NvRegPacketFilterFlags);\r\nwritel(np->rx_buf_sz, base + NvRegOffloadConfig);\r\nwritel(readl(base + NvRegReceiverStatus), base + NvRegReceiverStatus);\r\nget_random_bytes(&low, sizeof(low));\r\nlow &= NVREG_SLOTTIME_MASK;\r\nif (np->desc_ver == DESC_VER_1) {\r\nwritel(low|NVREG_SLOTTIME_DEFAULT, base + NvRegSlotTime);\r\n} else {\r\nif (!(np->driver_data & DEV_HAS_GEAR_MODE)) {\r\nwritel(NVREG_SLOTTIME_LEGBF_ENABLED|NVREG_SLOTTIME_10_100_FULL|low, base + NvRegSlotTime);\r\n} else {\r\nwritel(NVREG_SLOTTIME_10_100_FULL, base + NvRegSlotTime);\r\nnv_gear_backoff_reseed(dev);\r\n}\r\n}\r\nwritel(NVREG_TX_DEFERRAL_DEFAULT, base + NvRegTxDeferral);\r\nwritel(NVREG_RX_DEFERRAL_DEFAULT, base + NvRegRxDeferral);\r\nif (poll_interval == -1) {\r\nif (optimization_mode == NV_OPTIMIZATION_MODE_THROUGHPUT)\r\nwritel(NVREG_POLL_DEFAULT_THROUGHPUT, base + NvRegPollingInterval);\r\nelse\r\nwritel(NVREG_POLL_DEFAULT_CPU, base + NvRegPollingInterval);\r\n} else\r\nwritel(poll_interval & 0xFFFF, base + NvRegPollingInterval);\r\nwritel(NVREG_UNKSETUP6_VAL, base + NvRegUnknownSetupReg6);\r\nwritel((np->phyaddr << NVREG_ADAPTCTL_PHYSHIFT)|NVREG_ADAPTCTL_PHYVALID|NVREG_ADAPTCTL_RUNNING,\r\nbase + NvRegAdapterControl);\r\nwritel(NVREG_MIISPEED_BIT8|NVREG_MIIDELAY, base + NvRegMIISpeed);\r\nwritel(NVREG_MII_LINKCHANGE, base + NvRegMIIMask);\r\nif (np->wolenabled)\r\nwritel(NVREG_WAKEUPFLAGS_ENABLE , base + NvRegWakeUpFlags);\r\ni = readl(base + NvRegPowerState);\r\nif ((i & NVREG_POWERSTATE_POWEREDUP) == 0)\r\nwritel(NVREG_POWERSTATE_POWEREDUP|i, base + NvRegPowerState);\r\npci_push(base);\r\nudelay(10);\r\nwritel(readl(base + NvRegPowerState) | NVREG_POWERSTATE_VALID, base + NvRegPowerState);\r\nnv_disable_hw_interrupts(dev, np->irqmask);\r\npci_push(base);\r\nwritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\r\nwritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\r\npci_push(base);\r\nif (nv_request_irq(dev, 0))\r\ngoto out_drain;\r\nnv_enable_hw_interrupts(dev, np->irqmask);\r\nspin_lock_irq(&np->lock);\r\nwritel(NVREG_MCASTADDRA_FORCE, base + NvRegMulticastAddrA);\r\nwritel(0, base + NvRegMulticastAddrB);\r\nwritel(NVREG_MCASTMASKA_NONE, base + NvRegMulticastMaskA);\r\nwritel(NVREG_MCASTMASKB_NONE, base + NvRegMulticastMaskB);\r\nwritel(NVREG_PFF_ALWAYS|NVREG_PFF_MYADDR, base + NvRegPacketFilterFlags);\r\n{\r\nu32 miistat;\r\nmiistat = readl(base + NvRegMIIStatus);\r\nwritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\r\n}\r\nnp->linkspeed = 0;\r\nret = nv_update_linkspeed(dev);\r\nnv_start_rxtx(dev);\r\nnetif_start_queue(dev);\r\nnv_napi_enable(dev);\r\nif (ret) {\r\nnetif_carrier_on(dev);\r\n} else {\r\nnetdev_info(dev, "no link during initialization\n");\r\nnetif_carrier_off(dev);\r\n}\r\nif (oom)\r\nmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\r\nif (np->driver_data & (DEV_HAS_STATISTICS_V1|DEV_HAS_STATISTICS_V2|DEV_HAS_STATISTICS_V3))\r\nmod_timer(&np->stats_poll,\r\nround_jiffies(jiffies + STATS_INTERVAL));\r\nspin_unlock_irq(&np->lock);\r\nif (dev->features & NETIF_F_LOOPBACK)\r\nnv_set_loopback(dev, dev->features);\r\nreturn 0;\r\nout_drain:\r\nnv_drain_rxtx(dev);\r\nreturn ret;\r\n}\r\nstatic int nv_close(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base;\r\nspin_lock_irq(&np->lock);\r\nnp->in_shutdown = 1;\r\nspin_unlock_irq(&np->lock);\r\nnv_napi_disable(dev);\r\nsynchronize_irq(np->pci_dev->irq);\r\ndel_timer_sync(&np->oom_kick);\r\ndel_timer_sync(&np->nic_poll);\r\ndel_timer_sync(&np->stats_poll);\r\nnetif_stop_queue(dev);\r\nspin_lock_irq(&np->lock);\r\nnv_update_pause(dev, 0);\r\nnv_stop_rxtx(dev);\r\nnv_txrx_reset(dev);\r\nbase = get_hwbase(dev);\r\nnv_disable_hw_interrupts(dev, np->irqmask);\r\npci_push(base);\r\nspin_unlock_irq(&np->lock);\r\nnv_free_irq(dev);\r\nnv_drain_rxtx(dev);\r\nif (np->wolenabled || !phy_power_down) {\r\nnv_txrx_gate(dev, false);\r\nwritel(NVREG_PFF_ALWAYS|NVREG_PFF_MYADDR, base + NvRegPacketFilterFlags);\r\nnv_start_rx(dev);\r\n} else {\r\nmii_rw(dev, np->phyaddr, MII_BMCR,\r\nmii_rw(dev, np->phyaddr, MII_BMCR, MII_READ)|BMCR_PDOWN);\r\nnv_txrx_gate(dev, true);\r\n}\r\nreturn 0;\r\n}\r\nstatic int nv_probe(struct pci_dev *pci_dev, const struct pci_device_id *id)\r\n{\r\nstruct net_device *dev;\r\nstruct fe_priv *np;\r\nunsigned long addr;\r\nu8 __iomem *base;\r\nint err, i;\r\nu32 powerstate, txreg;\r\nu32 phystate_orig = 0, phystate;\r\nint phyinitialized = 0;\r\nstatic int printed_version;\r\nif (!printed_version++)\r\npr_info("Reverse Engineered nForce ethernet driver. Version %s.\n",\r\nFORCEDETH_VERSION);\r\ndev = alloc_etherdev(sizeof(struct fe_priv));\r\nerr = -ENOMEM;\r\nif (!dev)\r\ngoto out;\r\nnp = netdev_priv(dev);\r\nnp->dev = dev;\r\nnp->pci_dev = pci_dev;\r\nspin_lock_init(&np->lock);\r\nspin_lock_init(&np->hwstats_lock);\r\nSET_NETDEV_DEV(dev, &pci_dev->dev);\r\nu64_stats_init(&np->swstats_rx_syncp);\r\nu64_stats_init(&np->swstats_tx_syncp);\r\ninit_timer(&np->oom_kick);\r\nnp->oom_kick.data = (unsigned long) dev;\r\nnp->oom_kick.function = nv_do_rx_refill;\r\ninit_timer(&np->nic_poll);\r\nnp->nic_poll.data = (unsigned long) dev;\r\nnp->nic_poll.function = nv_do_nic_poll;\r\ninit_timer_deferrable(&np->stats_poll);\r\nnp->stats_poll.data = (unsigned long) dev;\r\nnp->stats_poll.function = nv_do_stats_poll;\r\nerr = pci_enable_device(pci_dev);\r\nif (err)\r\ngoto out_free;\r\npci_set_master(pci_dev);\r\nerr = pci_request_regions(pci_dev, DRV_NAME);\r\nif (err < 0)\r\ngoto out_disable;\r\nif (id->driver_data & (DEV_HAS_VLAN|DEV_HAS_MSI_X|DEV_HAS_POWER_CNTRL|DEV_HAS_STATISTICS_V2|DEV_HAS_STATISTICS_V3))\r\nnp->register_size = NV_PCI_REGSZ_VER3;\r\nelse if (id->driver_data & DEV_HAS_STATISTICS_V1)\r\nnp->register_size = NV_PCI_REGSZ_VER2;\r\nelse\r\nnp->register_size = NV_PCI_REGSZ_VER1;\r\nerr = -EINVAL;\r\naddr = 0;\r\nfor (i = 0; i < DEVICE_COUNT_RESOURCE; i++) {\r\nif (pci_resource_flags(pci_dev, i) & IORESOURCE_MEM &&\r\npci_resource_len(pci_dev, i) >= np->register_size) {\r\naddr = pci_resource_start(pci_dev, i);\r\nbreak;\r\n}\r\n}\r\nif (i == DEVICE_COUNT_RESOURCE) {\r\ndev_info(&pci_dev->dev, "Couldn't find register window\n");\r\ngoto out_relreg;\r\n}\r\nnp->driver_data = id->driver_data;\r\nnp->device_id = id->device;\r\nif (id->driver_data & DEV_HAS_HIGH_DMA) {\r\nnp->desc_ver = DESC_VER_3;\r\nnp->txrxctl_bits = NVREG_TXRXCTL_DESC_3;\r\nif (dma_64bit) {\r\nif (pci_set_dma_mask(pci_dev, DMA_BIT_MASK(39)))\r\ndev_info(&pci_dev->dev,\r\n"64-bit DMA failed, using 32-bit addressing\n");\r\nelse\r\ndev->features |= NETIF_F_HIGHDMA;\r\nif (pci_set_consistent_dma_mask(pci_dev, DMA_BIT_MASK(39))) {\r\ndev_info(&pci_dev->dev,\r\n"64-bit DMA (consistent) failed, using 32-bit ring buffers\n");\r\n}\r\n}\r\n} else if (id->driver_data & DEV_HAS_LARGEDESC) {\r\nnp->desc_ver = DESC_VER_2;\r\nnp->txrxctl_bits = NVREG_TXRXCTL_DESC_2;\r\n} else {\r\nnp->desc_ver = DESC_VER_1;\r\nnp->txrxctl_bits = NVREG_TXRXCTL_DESC_1;\r\n}\r\nnp->pkt_limit = NV_PKTLIMIT_1;\r\nif (id->driver_data & DEV_HAS_LARGEDESC)\r\nnp->pkt_limit = NV_PKTLIMIT_2;\r\nif (id->driver_data & DEV_HAS_CHECKSUM) {\r\nnp->txrxctl_bits |= NVREG_TXRXCTL_RXCHECK;\r\ndev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_SG |\r\nNETIF_F_TSO | NETIF_F_RXCSUM;\r\n}\r\nnp->vlanctl_bits = 0;\r\nif (id->driver_data & DEV_HAS_VLAN) {\r\nnp->vlanctl_bits = NVREG_VLANCONTROL_ENABLE;\r\ndev->hw_features |= NETIF_F_HW_VLAN_CTAG_RX |\r\nNETIF_F_HW_VLAN_CTAG_TX;\r\n}\r\ndev->features |= dev->hw_features;\r\ndev->hw_features |= NETIF_F_LOOPBACK;\r\nnp->pause_flags = NV_PAUSEFRAME_RX_CAPABLE | NV_PAUSEFRAME_RX_REQ | NV_PAUSEFRAME_AUTONEG;\r\nif ((id->driver_data & DEV_HAS_PAUSEFRAME_TX_V1) ||\r\n(id->driver_data & DEV_HAS_PAUSEFRAME_TX_V2) ||\r\n(id->driver_data & DEV_HAS_PAUSEFRAME_TX_V3)) {\r\nnp->pause_flags |= NV_PAUSEFRAME_TX_CAPABLE | NV_PAUSEFRAME_TX_REQ;\r\n}\r\nerr = -ENOMEM;\r\nnp->base = ioremap(addr, np->register_size);\r\nif (!np->base)\r\ngoto out_relreg;\r\nnp->rx_ring_size = RX_RING_DEFAULT;\r\nnp->tx_ring_size = TX_RING_DEFAULT;\r\nif (!nv_optimized(np)) {\r\nnp->rx_ring.orig = pci_alloc_consistent(pci_dev,\r\nsizeof(struct ring_desc) * (np->rx_ring_size + np->tx_ring_size),\r\n&np->ring_addr);\r\nif (!np->rx_ring.orig)\r\ngoto out_unmap;\r\nnp->tx_ring.orig = &np->rx_ring.orig[np->rx_ring_size];\r\n} else {\r\nnp->rx_ring.ex = pci_alloc_consistent(pci_dev,\r\nsizeof(struct ring_desc_ex) * (np->rx_ring_size + np->tx_ring_size),\r\n&np->ring_addr);\r\nif (!np->rx_ring.ex)\r\ngoto out_unmap;\r\nnp->tx_ring.ex = &np->rx_ring.ex[np->rx_ring_size];\r\n}\r\nnp->rx_skb = kcalloc(np->rx_ring_size, sizeof(struct nv_skb_map), GFP_KERNEL);\r\nnp->tx_skb = kcalloc(np->tx_ring_size, sizeof(struct nv_skb_map), GFP_KERNEL);\r\nif (!np->rx_skb || !np->tx_skb)\r\ngoto out_freering;\r\nif (!nv_optimized(np))\r\ndev->netdev_ops = &nv_netdev_ops;\r\nelse\r\ndev->netdev_ops = &nv_netdev_ops_optimized;\r\nnetif_napi_add(dev, &np->napi, nv_napi_poll, RX_WORK_PER_LOOP);\r\ndev->ethtool_ops = &ops;\r\ndev->watchdog_timeo = NV_WATCHDOG_TIMEO;\r\npci_set_drvdata(pci_dev, dev);\r\nbase = get_hwbase(dev);\r\nnp->orig_mac[0] = readl(base + NvRegMacAddrA);\r\nnp->orig_mac[1] = readl(base + NvRegMacAddrB);\r\ntxreg = readl(base + NvRegTransmitPoll);\r\nif (id->driver_data & DEV_HAS_CORRECT_MACADDR) {\r\ndev->dev_addr[0] = (np->orig_mac[0] >> 0) & 0xff;\r\ndev->dev_addr[1] = (np->orig_mac[0] >> 8) & 0xff;\r\ndev->dev_addr[2] = (np->orig_mac[0] >> 16) & 0xff;\r\ndev->dev_addr[3] = (np->orig_mac[0] >> 24) & 0xff;\r\ndev->dev_addr[4] = (np->orig_mac[1] >> 0) & 0xff;\r\ndev->dev_addr[5] = (np->orig_mac[1] >> 8) & 0xff;\r\n} else if (txreg & NVREG_TRANSMITPOLL_MAC_ADDR_REV) {\r\ndev->dev_addr[0] = (np->orig_mac[0] >> 0) & 0xff;\r\ndev->dev_addr[1] = (np->orig_mac[0] >> 8) & 0xff;\r\ndev->dev_addr[2] = (np->orig_mac[0] >> 16) & 0xff;\r\ndev->dev_addr[3] = (np->orig_mac[0] >> 24) & 0xff;\r\ndev->dev_addr[4] = (np->orig_mac[1] >> 0) & 0xff;\r\ndev->dev_addr[5] = (np->orig_mac[1] >> 8) & 0xff;\r\nnp->orig_mac[0] = (dev->dev_addr[5] << 0) + (dev->dev_addr[4] << 8) +\r\n(dev->dev_addr[3] << 16) + (dev->dev_addr[2] << 24);\r\nnp->orig_mac[1] = (dev->dev_addr[1] << 0) + (dev->dev_addr[0] << 8);\r\n} else {\r\ndev->dev_addr[0] = (np->orig_mac[1] >> 8) & 0xff;\r\ndev->dev_addr[1] = (np->orig_mac[1] >> 0) & 0xff;\r\ndev->dev_addr[2] = (np->orig_mac[0] >> 24) & 0xff;\r\ndev->dev_addr[3] = (np->orig_mac[0] >> 16) & 0xff;\r\ndev->dev_addr[4] = (np->orig_mac[0] >> 8) & 0xff;\r\ndev->dev_addr[5] = (np->orig_mac[0] >> 0) & 0xff;\r\nwritel(txreg|NVREG_TRANSMITPOLL_MAC_ADDR_REV, base + NvRegTransmitPoll);\r\ndev_dbg(&pci_dev->dev,\r\n"%s: set workaround bit for reversed mac addr\n",\r\n__func__);\r\n}\r\nif (!is_valid_ether_addr(dev->dev_addr)) {\r\ndev_err(&pci_dev->dev,\r\n"Invalid MAC address detected: %pM - Please complain to your hardware vendor.\n",\r\ndev->dev_addr);\r\neth_hw_addr_random(dev);\r\ndev_err(&pci_dev->dev,\r\n"Using random MAC address: %pM\n", dev->dev_addr);\r\n}\r\nnv_copy_mac_to_hw(dev);\r\nwritel(0, base + NvRegWakeUpFlags);\r\nnp->wolenabled = 0;\r\ndevice_set_wakeup_enable(&pci_dev->dev, false);\r\nif (id->driver_data & DEV_HAS_POWER_CNTRL) {\r\npowerstate = readl(base + NvRegPowerState2);\r\npowerstate &= ~NVREG_POWERSTATE2_POWERUP_MASK;\r\nif ((id->driver_data & DEV_NEED_LOW_POWER_FIX) &&\r\npci_dev->revision >= 0xA3)\r\npowerstate |= NVREG_POWERSTATE2_POWERUP_REV_A3;\r\nwritel(powerstate, base + NvRegPowerState2);\r\n}\r\nif (np->desc_ver == DESC_VER_1)\r\nnp->tx_flags = NV_TX_VALID;\r\nelse\r\nnp->tx_flags = NV_TX2_VALID;\r\nnp->msi_flags = 0;\r\nif ((id->driver_data & DEV_HAS_MSI) && msi)\r\nnp->msi_flags |= NV_MSI_CAPABLE;\r\nif ((id->driver_data & DEV_HAS_MSI_X) && msix) {\r\n#if 0\r\nnp->msi_flags |= NV_MSI_X_CAPABLE;\r\n#endif\r\n}\r\nif (optimization_mode == NV_OPTIMIZATION_MODE_CPU) {\r\nnp->irqmask = NVREG_IRQMASK_CPU;\r\nif (np->msi_flags & NV_MSI_X_CAPABLE)\r\nnp->msi_flags |= 0x0001;\r\n} else if (optimization_mode == NV_OPTIMIZATION_MODE_DYNAMIC &&\r\n!(id->driver_data & DEV_NEED_TIMERIRQ)) {\r\nnp->irqmask = NVREG_IRQMASK_THROUGHPUT;\r\nnp->msi_flags &= ~NV_MSI_X_CAPABLE;\r\n} else {\r\noptimization_mode = NV_OPTIMIZATION_MODE_THROUGHPUT;\r\nnp->irqmask = NVREG_IRQMASK_THROUGHPUT;\r\nif (np->msi_flags & NV_MSI_X_CAPABLE)\r\nnp->msi_flags |= 0x0003;\r\n}\r\nif (id->driver_data & DEV_NEED_TIMERIRQ)\r\nnp->irqmask |= NVREG_IRQ_TIMER;\r\nif (id->driver_data & DEV_NEED_LINKTIMER) {\r\nnp->need_linktimer = 1;\r\nnp->link_timeout = jiffies + LINK_TIMEOUT;\r\n} else {\r\nnp->need_linktimer = 0;\r\n}\r\nif (id->driver_data & DEV_NEED_TX_LIMIT) {\r\nnp->tx_limit = 1;\r\nif (((id->driver_data & DEV_NEED_TX_LIMIT2) == DEV_NEED_TX_LIMIT2) &&\r\npci_dev->revision >= 0xA2)\r\nnp->tx_limit = 0;\r\n}\r\nwritel(0, base + NvRegMIIMask);\r\nphystate = readl(base + NvRegAdapterControl);\r\nif (phystate & NVREG_ADAPTCTL_RUNNING) {\r\nphystate_orig = 1;\r\nphystate &= ~NVREG_ADAPTCTL_RUNNING;\r\nwritel(phystate, base + NvRegAdapterControl);\r\n}\r\nwritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\r\nif (id->driver_data & DEV_HAS_MGMT_UNIT) {\r\nif ((readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_MGMT_ST) &&\r\n(readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_SYNC_PHY_INIT) &&\r\nnv_mgmt_acquire_sema(dev) &&\r\nnv_mgmt_get_version(dev)) {\r\nnp->mac_in_use = 1;\r\nif (np->mgmt_version > 0)\r\nnp->mac_in_use = readl(base + NvRegMgmtUnitControl) & NVREG_MGMTUNITCONTROL_INUSE;\r\nif (np->mac_in_use &&\r\n((readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_SYNC_MASK) ==\r\nNVREG_XMITCTL_SYNC_PHY_INIT)) {\r\nphyinitialized = 1;\r\n} else {\r\n}\r\n}\r\n}\r\nfor (i = 1; i <= 32; i++) {\r\nint id1, id2;\r\nint phyaddr = i & 0x1F;\r\nspin_lock_irq(&np->lock);\r\nid1 = mii_rw(dev, phyaddr, MII_PHYSID1, MII_READ);\r\nspin_unlock_irq(&np->lock);\r\nif (id1 < 0 || id1 == 0xffff)\r\ncontinue;\r\nspin_lock_irq(&np->lock);\r\nid2 = mii_rw(dev, phyaddr, MII_PHYSID2, MII_READ);\r\nspin_unlock_irq(&np->lock);\r\nif (id2 < 0 || id2 == 0xffff)\r\ncontinue;\r\nnp->phy_model = id2 & PHYID2_MODEL_MASK;\r\nid1 = (id1 & PHYID1_OUI_MASK) << PHYID1_OUI_SHFT;\r\nid2 = (id2 & PHYID2_OUI_MASK) >> PHYID2_OUI_SHFT;\r\nnp->phyaddr = phyaddr;\r\nnp->phy_oui = id1 | id2;\r\nif (np->phy_oui == PHY_OUI_REALTEK2)\r\nnp->phy_oui = PHY_OUI_REALTEK;\r\nif (np->phy_oui == PHY_OUI_REALTEK && np->phy_model == PHY_MODEL_REALTEK_8211)\r\nnp->phy_rev = mii_rw(dev, phyaddr, MII_RESV1, MII_READ) & PHY_REV_MASK;\r\nbreak;\r\n}\r\nif (i == 33) {\r\ndev_info(&pci_dev->dev, "open: Could not find a valid PHY\n");\r\ngoto out_error;\r\n}\r\nif (!phyinitialized) {\r\nphy_init(dev);\r\n} else {\r\nu32 mii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\r\nif (mii_status & PHY_GIGABIT)\r\nnp->gigabit = PHY_GIGABIT;\r\n}\r\nnp->linkspeed = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\r\nnp->duplex = 0;\r\nnp->autoneg = 1;\r\nerr = register_netdev(dev);\r\nif (err) {\r\ndev_info(&pci_dev->dev, "unable to register netdev: %d\n", err);\r\ngoto out_error;\r\n}\r\nnetif_carrier_off(dev);\r\nnv_update_pause(dev, 0);\r\nnv_start_tx(dev);\r\nnv_stop_tx(dev);\r\nif (id->driver_data & DEV_HAS_VLAN)\r\nnv_vlan_mode(dev, dev->features);\r\ndev_info(&pci_dev->dev, "ifname %s, PHY OUI 0x%x @ %d, addr %pM\n",\r\ndev->name, np->phy_oui, np->phyaddr, dev->dev_addr);\r\ndev_info(&pci_dev->dev, "%s%s%s%s%s%s%s%s%s%s%sdesc-v%u\n",\r\ndev->features & NETIF_F_HIGHDMA ? "highdma " : "",\r\ndev->features & (NETIF_F_IP_CSUM | NETIF_F_SG) ?\r\n"csum " : "",\r\ndev->features & (NETIF_F_HW_VLAN_CTAG_RX |\r\nNETIF_F_HW_VLAN_CTAG_TX) ?\r\n"vlan " : "",\r\ndev->features & (NETIF_F_LOOPBACK) ?\r\n"loopback " : "",\r\nid->driver_data & DEV_HAS_POWER_CNTRL ? "pwrctl " : "",\r\nid->driver_data & DEV_HAS_MGMT_UNIT ? "mgmt " : "",\r\nid->driver_data & DEV_NEED_TIMERIRQ ? "timirq " : "",\r\nnp->gigabit == PHY_GIGABIT ? "gbit " : "",\r\nnp->need_linktimer ? "lnktim " : "",\r\nnp->msi_flags & NV_MSI_CAPABLE ? "msi " : "",\r\nnp->msi_flags & NV_MSI_X_CAPABLE ? "msi-x " : "",\r\nnp->desc_ver);\r\nreturn 0;\r\nout_error:\r\nif (phystate_orig)\r\nwritel(phystate|NVREG_ADAPTCTL_RUNNING, base + NvRegAdapterControl);\r\nout_freering:\r\nfree_rings(dev);\r\nout_unmap:\r\niounmap(get_hwbase(dev));\r\nout_relreg:\r\npci_release_regions(pci_dev);\r\nout_disable:\r\npci_disable_device(pci_dev);\r\nout_free:\r\nfree_netdev(dev);\r\nout:\r\nreturn err;\r\n}\r\nstatic void nv_restore_phy(struct net_device *dev)\r\n{\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu16 phy_reserved, mii_control;\r\nif (np->phy_oui == PHY_OUI_REALTEK &&\r\nnp->phy_model == PHY_MODEL_REALTEK_8201 &&\r\nphy_cross == NV_CROSSOVER_DETECTION_DISABLED) {\r\nmii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT3);\r\nphy_reserved = mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG2, MII_READ);\r\nphy_reserved &= ~PHY_REALTEK_INIT_MSK1;\r\nphy_reserved |= PHY_REALTEK_INIT8;\r\nmii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG2, phy_reserved);\r\nmii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1);\r\nmii_control = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\r\nmii_control |= (BMCR_ANRESTART | BMCR_ANENABLE);\r\nmii_rw(dev, np->phyaddr, MII_BMCR, mii_control);\r\n}\r\n}\r\nstatic void nv_restore_mac_addr(struct pci_dev *pci_dev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pci_dev);\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nwritel(np->orig_mac[0], base + NvRegMacAddrA);\r\nwritel(np->orig_mac[1], base + NvRegMacAddrB);\r\nwritel(readl(base + NvRegTransmitPoll) & ~NVREG_TRANSMITPOLL_MAC_ADDR_REV,\r\nbase + NvRegTransmitPoll);\r\n}\r\nstatic void nv_remove(struct pci_dev *pci_dev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pci_dev);\r\nunregister_netdev(dev);\r\nnv_restore_mac_addr(pci_dev);\r\nnv_restore_phy(dev);\r\nnv_mgmt_release_sema(dev);\r\nfree_rings(dev);\r\niounmap(get_hwbase(dev));\r\npci_release_regions(pci_dev);\r\npci_disable_device(pci_dev);\r\nfree_netdev(dev);\r\n}\r\nstatic int nv_suspend(struct device *device)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(device);\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint i;\r\nif (netif_running(dev)) {\r\nnv_close(dev);\r\n}\r\nnetif_device_detach(dev);\r\nfor (i = 0; i <= np->register_size/sizeof(u32); i++)\r\nnp->saved_config_space[i] = readl(base + i*sizeof(u32));\r\nreturn 0;\r\n}\r\nstatic int nv_resume(struct device *device)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(device);\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct fe_priv *np = netdev_priv(dev);\r\nu8 __iomem *base = get_hwbase(dev);\r\nint i, rc = 0;\r\nfor (i = 0; i <= np->register_size/sizeof(u32); i++)\r\nwritel(np->saved_config_space[i], base+i*sizeof(u32));\r\nif (np->driver_data & DEV_NEED_MSI_FIX)\r\npci_write_config_dword(pdev, NV_MSI_PRIV_OFFSET, NV_MSI_PRIV_VALUE);\r\nphy_init(dev);\r\nnetif_device_attach(dev);\r\nif (netif_running(dev)) {\r\nrc = nv_open(dev);\r\nnv_set_multicast(dev);\r\n}\r\nreturn rc;\r\n}\r\nstatic void nv_shutdown(struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct fe_priv *np = netdev_priv(dev);\r\nif (netif_running(dev))\r\nnv_close(dev);\r\nif (system_state != SYSTEM_POWER_OFF)\r\nnv_restore_mac_addr(pdev);\r\npci_disable_device(pdev);\r\nif (system_state == SYSTEM_POWER_OFF) {\r\npci_wake_from_d3(pdev, np->wolenabled);\r\npci_set_power_state(pdev, PCI_D3hot);\r\n}\r\n}
