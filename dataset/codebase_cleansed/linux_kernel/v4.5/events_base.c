static void clear_evtchn_to_irq_row(unsigned row)\r\n{\r\nunsigned col;\r\nfor (col = 0; col < EVTCHN_PER_ROW; col++)\r\nevtchn_to_irq[row][col] = -1;\r\n}\r\nstatic void clear_evtchn_to_irq_all(void)\r\n{\r\nunsigned row;\r\nfor (row = 0; row < EVTCHN_ROW(xen_evtchn_max_channels()); row++) {\r\nif (evtchn_to_irq[row] == NULL)\r\ncontinue;\r\nclear_evtchn_to_irq_row(row);\r\n}\r\n}\r\nstatic int set_evtchn_to_irq(unsigned evtchn, unsigned irq)\r\n{\r\nunsigned row;\r\nunsigned col;\r\nif (evtchn >= xen_evtchn_max_channels())\r\nreturn -EINVAL;\r\nrow = EVTCHN_ROW(evtchn);\r\ncol = EVTCHN_COL(evtchn);\r\nif (evtchn_to_irq[row] == NULL) {\r\nif (irq == -1)\r\nreturn 0;\r\nevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\r\nif (evtchn_to_irq[row] == NULL)\r\nreturn -ENOMEM;\r\nclear_evtchn_to_irq_row(row);\r\n}\r\nevtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)] = irq;\r\nreturn 0;\r\n}\r\nint get_evtchn_to_irq(unsigned evtchn)\r\n{\r\nif (evtchn >= xen_evtchn_max_channels())\r\nreturn -1;\r\nif (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)\r\nreturn -1;\r\nreturn evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];\r\n}\r\nstruct irq_info *info_for_irq(unsigned irq)\r\n{\r\nreturn irq_get_handler_data(irq);\r\n}\r\nstatic int xen_irq_info_common_setup(struct irq_info *info,\r\nunsigned irq,\r\nenum xen_irq_type type,\r\nunsigned evtchn,\r\nunsigned short cpu)\r\n{\r\nint ret;\r\nBUG_ON(info->type != IRQT_UNBOUND && info->type != type);\r\ninfo->type = type;\r\ninfo->irq = irq;\r\ninfo->evtchn = evtchn;\r\ninfo->cpu = cpu;\r\nret = set_evtchn_to_irq(evtchn, irq);\r\nif (ret < 0)\r\nreturn ret;\r\nirq_clear_status_flags(irq, IRQ_NOREQUEST|IRQ_NOAUTOEN);\r\nreturn xen_evtchn_port_setup(info);\r\n}\r\nstatic int xen_irq_info_evtchn_setup(unsigned irq,\r\nunsigned evtchn)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nreturn xen_irq_info_common_setup(info, irq, IRQT_EVTCHN, evtchn, 0);\r\n}\r\nstatic int xen_irq_info_ipi_setup(unsigned cpu,\r\nunsigned irq,\r\nunsigned evtchn,\r\nenum ipi_vector ipi)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\ninfo->u.ipi = ipi;\r\nper_cpu(ipi_to_irq, cpu)[ipi] = irq;\r\nreturn xen_irq_info_common_setup(info, irq, IRQT_IPI, evtchn, 0);\r\n}\r\nstatic int xen_irq_info_virq_setup(unsigned cpu,\r\nunsigned irq,\r\nunsigned evtchn,\r\nunsigned virq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\ninfo->u.virq = virq;\r\nper_cpu(virq_to_irq, cpu)[virq] = irq;\r\nreturn xen_irq_info_common_setup(info, irq, IRQT_VIRQ, evtchn, 0);\r\n}\r\nstatic int xen_irq_info_pirq_setup(unsigned irq,\r\nunsigned evtchn,\r\nunsigned pirq,\r\nunsigned gsi,\r\nuint16_t domid,\r\nunsigned char flags)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\ninfo->u.pirq.pirq = pirq;\r\ninfo->u.pirq.gsi = gsi;\r\ninfo->u.pirq.domid = domid;\r\ninfo->u.pirq.flags = flags;\r\nreturn xen_irq_info_common_setup(info, irq, IRQT_PIRQ, evtchn, 0);\r\n}\r\nstatic void xen_irq_info_cleanup(struct irq_info *info)\r\n{\r\nset_evtchn_to_irq(info->evtchn, -1);\r\ninfo->evtchn = 0;\r\n}\r\nunsigned int evtchn_from_irq(unsigned irq)\r\n{\r\nif (unlikely(WARN(irq >= nr_irqs, "Invalid irq %d!\n", irq)))\r\nreturn 0;\r\nreturn info_for_irq(irq)->evtchn;\r\n}\r\nunsigned irq_from_evtchn(unsigned int evtchn)\r\n{\r\nreturn get_evtchn_to_irq(evtchn);\r\n}\r\nint irq_from_virq(unsigned int cpu, unsigned int virq)\r\n{\r\nreturn per_cpu(virq_to_irq, cpu)[virq];\r\n}\r\nstatic enum ipi_vector ipi_from_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info == NULL);\r\nBUG_ON(info->type != IRQT_IPI);\r\nreturn info->u.ipi;\r\n}\r\nstatic unsigned virq_from_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info == NULL);\r\nBUG_ON(info->type != IRQT_VIRQ);\r\nreturn info->u.virq;\r\n}\r\nstatic unsigned pirq_from_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info == NULL);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nreturn info->u.pirq.pirq;\r\n}\r\nstatic enum xen_irq_type type_from_irq(unsigned irq)\r\n{\r\nreturn info_for_irq(irq)->type;\r\n}\r\nunsigned cpu_from_irq(unsigned irq)\r\n{\r\nreturn info_for_irq(irq)->cpu;\r\n}\r\nunsigned int cpu_from_evtchn(unsigned int evtchn)\r\n{\r\nint irq = get_evtchn_to_irq(evtchn);\r\nunsigned ret = 0;\r\nif (irq != -1)\r\nret = cpu_from_irq(irq);\r\nreturn ret;\r\n}\r\nstatic bool pirq_check_eoi_map(unsigned irq)\r\n{\r\nreturn test_bit(pirq_from_irq(irq), pirq_eoi_map);\r\n}\r\nstatic bool pirq_needs_eoi_flag(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nreturn info->u.pirq.flags & PIRQ_NEEDS_EOI;\r\n}\r\nstatic void bind_evtchn_to_cpu(unsigned int chn, unsigned int cpu)\r\n{\r\nint irq = get_evtchn_to_irq(chn);\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(irq == -1);\r\n#ifdef CONFIG_SMP\r\ncpumask_copy(irq_get_affinity_mask(irq), cpumask_of(cpu));\r\n#endif\r\nxen_evtchn_port_bind_to_cpu(info, cpu);\r\ninfo->cpu = cpu;\r\n}\r\nstatic void xen_evtchn_mask_all(void)\r\n{\r\nunsigned int evtchn;\r\nfor (evtchn = 0; evtchn < xen_evtchn_nr_channels(); evtchn++)\r\nmask_evtchn(evtchn);\r\n}\r\nvoid notify_remote_via_irq(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn))\r\nnotify_remote_via_evtchn(evtchn);\r\n}\r\nstatic void xen_irq_init(unsigned irq)\r\n{\r\nstruct irq_info *info;\r\n#ifdef CONFIG_SMP\r\ncpumask_copy(irq_get_affinity_mask(irq), cpumask_of(0));\r\n#endif\r\ninfo = kzalloc(sizeof(*info), GFP_KERNEL);\r\nif (info == NULL)\r\npanic("Unable to allocate metadata for IRQ%d\n", irq);\r\ninfo->type = IRQT_UNBOUND;\r\ninfo->refcnt = -1;\r\nirq_set_handler_data(irq, info);\r\nlist_add_tail(&info->list, &xen_irq_list_head);\r\n}\r\nstatic int __must_check xen_allocate_irqs_dynamic(int nvec)\r\n{\r\nint i, irq = irq_alloc_descs(-1, 0, nvec, -1);\r\nif (irq >= 0) {\r\nfor (i = 0; i < nvec; i++)\r\nxen_irq_init(irq + i);\r\n}\r\nreturn irq;\r\n}\r\nstatic inline int __must_check xen_allocate_irq_dynamic(void)\r\n{\r\nreturn xen_allocate_irqs_dynamic(1);\r\n}\r\nstatic int __must_check xen_allocate_irq_gsi(unsigned gsi)\r\n{\r\nint irq;\r\nif (xen_pv_domain() && !xen_initial_domain())\r\nreturn xen_allocate_irq_dynamic();\r\nif (gsi < nr_legacy_irqs())\r\nirq = gsi;\r\nelse\r\nirq = irq_alloc_desc_at(gsi, -1);\r\nxen_irq_init(irq);\r\nreturn irq;\r\n}\r\nstatic void xen_free_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = irq_get_handler_data(irq);\r\nif (WARN_ON(!info))\r\nreturn;\r\nlist_del(&info->list);\r\nirq_set_handler_data(irq, NULL);\r\nWARN_ON(info->refcnt > 0);\r\nkfree(info);\r\nif (irq < nr_legacy_irqs())\r\nreturn;\r\nirq_free_desc(irq);\r\n}\r\nstatic void xen_evtchn_close(unsigned int port)\r\n{\r\nstruct evtchn_close close;\r\nclose.port = port;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_close, &close) != 0)\r\nBUG();\r\n}\r\nstatic void pirq_query_unmask(int irq)\r\n{\r\nstruct physdev_irq_status_query irq_status;\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nirq_status.irq = pirq_from_irq(irq);\r\nif (HYPERVISOR_physdev_op(PHYSDEVOP_irq_status_query, &irq_status))\r\nirq_status.flags = 0;\r\ninfo->u.pirq.flags &= ~PIRQ_NEEDS_EOI;\r\nif (irq_status.flags & XENIRQSTAT_needs_eoi)\r\ninfo->u.pirq.flags |= PIRQ_NEEDS_EOI;\r\n}\r\nstatic void eoi_pirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nstruct physdev_eoi eoi = { .irq = pirq_from_irq(data->irq) };\r\nint rc = 0;\r\nirq_move_irq(data);\r\nif (VALID_EVTCHN(evtchn))\r\nclear_evtchn(evtchn);\r\nif (pirq_needs_eoi(data->irq)) {\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_eoi, &eoi);\r\nWARN_ON(rc);\r\n}\r\n}\r\nstatic void mask_ack_pirq(struct irq_data *data)\r\n{\r\ndisable_dynirq(data);\r\neoi_pirq(data);\r\n}\r\nstatic unsigned int __startup_pirq(unsigned int irq)\r\n{\r\nstruct evtchn_bind_pirq bind_pirq;\r\nstruct irq_info *info = info_for_irq(irq);\r\nint evtchn = evtchn_from_irq(irq);\r\nint rc;\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nif (VALID_EVTCHN(evtchn))\r\ngoto out;\r\nbind_pirq.pirq = pirq_from_irq(irq);\r\nbind_pirq.flags = info->u.pirq.flags & PIRQ_SHAREABLE ?\r\nBIND_PIRQ__WILL_SHARE : 0;\r\nrc = HYPERVISOR_event_channel_op(EVTCHNOP_bind_pirq, &bind_pirq);\r\nif (rc != 0) {\r\npr_warn("Failed to obtain physical IRQ %d\n", irq);\r\nreturn 0;\r\n}\r\nevtchn = bind_pirq.port;\r\npirq_query_unmask(irq);\r\nrc = set_evtchn_to_irq(evtchn, irq);\r\nif (rc)\r\ngoto err;\r\ninfo->evtchn = evtchn;\r\nbind_evtchn_to_cpu(evtchn, 0);\r\nrc = xen_evtchn_port_setup(info);\r\nif (rc)\r\ngoto err;\r\nout:\r\nunmask_evtchn(evtchn);\r\neoi_pirq(irq_get_irq_data(irq));\r\nreturn 0;\r\nerr:\r\npr_err("irq%d: Failed to set port to irq mapping (%d)\n", irq, rc);\r\nxen_evtchn_close(evtchn);\r\nreturn 0;\r\n}\r\nstatic unsigned int startup_pirq(struct irq_data *data)\r\n{\r\nreturn __startup_pirq(data->irq);\r\n}\r\nstatic void shutdown_pirq(struct irq_data *data)\r\n{\r\nunsigned int irq = data->irq;\r\nstruct irq_info *info = info_for_irq(irq);\r\nunsigned evtchn = evtchn_from_irq(irq);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nif (!VALID_EVTCHN(evtchn))\r\nreturn;\r\nmask_evtchn(evtchn);\r\nxen_evtchn_close(evtchn);\r\nxen_irq_info_cleanup(info);\r\n}\r\nstatic void enable_pirq(struct irq_data *data)\r\n{\r\nstartup_pirq(data);\r\n}\r\nstatic void disable_pirq(struct irq_data *data)\r\n{\r\ndisable_dynirq(data);\r\n}\r\nint xen_irq_from_gsi(unsigned gsi)\r\n{\r\nstruct irq_info *info;\r\nlist_for_each_entry(info, &xen_irq_list_head, list) {\r\nif (info->type != IRQT_PIRQ)\r\ncontinue;\r\nif (info->u.pirq.gsi == gsi)\r\nreturn info->irq;\r\n}\r\nreturn -1;\r\n}\r\nstatic void __unbind_from_irq(unsigned int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nstruct irq_info *info = irq_get_handler_data(irq);\r\nif (info->refcnt > 0) {\r\ninfo->refcnt--;\r\nif (info->refcnt != 0)\r\nreturn;\r\n}\r\nif (VALID_EVTCHN(evtchn)) {\r\nunsigned int cpu = cpu_from_irq(irq);\r\nxen_evtchn_close(evtchn);\r\nswitch (type_from_irq(irq)) {\r\ncase IRQT_VIRQ:\r\nper_cpu(virq_to_irq, cpu)[virq_from_irq(irq)] = -1;\r\nbreak;\r\ncase IRQT_IPI:\r\nper_cpu(ipi_to_irq, cpu)[ipi_from_irq(irq)] = -1;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nxen_irq_info_cleanup(info);\r\n}\r\nBUG_ON(info_for_irq(irq)->type == IRQT_UNBOUND);\r\nxen_free_irq(irq);\r\n}\r\nint xen_bind_pirq_gsi_to_irq(unsigned gsi,\r\nunsigned pirq, int shareable, char *name)\r\n{\r\nint irq = -1;\r\nstruct physdev_irq irq_op;\r\nint ret;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = xen_irq_from_gsi(gsi);\r\nif (irq != -1) {\r\npr_info("%s: returning irq %d for gsi %u\n",\r\n__func__, irq, gsi);\r\ngoto out;\r\n}\r\nirq = xen_allocate_irq_gsi(gsi);\r\nif (irq < 0)\r\ngoto out;\r\nirq_op.irq = irq;\r\nirq_op.vector = 0;\r\nif (xen_initial_domain() &&\r\nHYPERVISOR_physdev_op(PHYSDEVOP_alloc_irq_vector, &irq_op)) {\r\nxen_free_irq(irq);\r\nirq = -ENOSPC;\r\ngoto out;\r\n}\r\nret = xen_irq_info_pirq_setup(irq, 0, pirq, gsi, DOMID_SELF,\r\nshareable ? PIRQ_SHAREABLE : 0);\r\nif (ret < 0) {\r\n__unbind_from_irq(irq);\r\nirq = ret;\r\ngoto out;\r\n}\r\npirq_query_unmask(irq);\r\nif (shareable)\r\nirq_set_chip_and_handler_name(irq, &xen_pirq_chip,\r\nhandle_fasteoi_irq, name);\r\nelse\r\nirq_set_chip_and_handler_name(irq, &xen_pirq_chip,\r\nhandle_edge_irq, name);\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nint xen_allocate_pirq_msi(struct pci_dev *dev, struct msi_desc *msidesc)\r\n{\r\nint rc;\r\nstruct physdev_get_free_pirq op_get_free_pirq;\r\nop_get_free_pirq.type = MAP_PIRQ_TYPE_MSI;\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_get_free_pirq, &op_get_free_pirq);\r\nWARN_ONCE(rc == -ENOSYS,\r\n"hypervisor does not support the PHYSDEVOP_get_free_pirq interface\n");\r\nreturn rc ? -1 : op_get_free_pirq.pirq;\r\n}\r\nint xen_bind_pirq_msi_to_irq(struct pci_dev *dev, struct msi_desc *msidesc,\r\nint pirq, int nvec, const char *name, domid_t domid)\r\n{\r\nint i, irq, ret;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = xen_allocate_irqs_dynamic(nvec);\r\nif (irq < 0)\r\ngoto out;\r\nfor (i = 0; i < nvec; i++) {\r\nirq_set_chip_and_handler_name(irq + i, &xen_pirq_chip, handle_edge_irq, name);\r\nret = xen_irq_info_pirq_setup(irq + i, 0, pirq + i, 0, domid,\r\ni == 0 ? 0 : PIRQ_MSI_GROUP);\r\nif (ret < 0)\r\ngoto error_irq;\r\n}\r\nret = irq_set_msi_desc(irq, msidesc);\r\nif (ret < 0)\r\ngoto error_irq;\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\nerror_irq:\r\nfor (; i >= 0; i--)\r\n__unbind_from_irq(irq + i);\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn ret;\r\n}\r\nint xen_destroy_irq(int irq)\r\n{\r\nstruct physdev_unmap_pirq unmap_irq;\r\nstruct irq_info *info = info_for_irq(irq);\r\nint rc = -ENOENT;\r\nmutex_lock(&irq_mapping_update_lock);\r\nif (xen_initial_domain() && !(info->u.pirq.flags & PIRQ_MSI_GROUP)) {\r\nunmap_irq.pirq = info->u.pirq.pirq;\r\nunmap_irq.domid = info->u.pirq.domid;\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_unmap_pirq, &unmap_irq);\r\nif ((rc == -ESRCH && info->u.pirq.domid != DOMID_SELF))\r\npr_info("domain %d does not have %d anymore\n",\r\ninfo->u.pirq.domid, info->u.pirq.pirq);\r\nelse if (rc) {\r\npr_warn("unmap irq failed %d\n", rc);\r\ngoto out;\r\n}\r\n}\r\nxen_free_irq(irq);\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn rc;\r\n}\r\nint xen_irq_from_pirq(unsigned pirq)\r\n{\r\nint irq;\r\nstruct irq_info *info;\r\nmutex_lock(&irq_mapping_update_lock);\r\nlist_for_each_entry(info, &xen_irq_list_head, list) {\r\nif (info->type != IRQT_PIRQ)\r\ncontinue;\r\nirq = info->irq;\r\nif (info->u.pirq.pirq == pirq)\r\ngoto out;\r\n}\r\nirq = -1;\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nint xen_pirq_from_irq(unsigned irq)\r\n{\r\nreturn pirq_from_irq(irq);\r\n}\r\nint bind_evtchn_to_irq(unsigned int evtchn)\r\n{\r\nint irq;\r\nint ret;\r\nif (evtchn >= xen_evtchn_max_channels())\r\nreturn -ENOMEM;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = get_evtchn_to_irq(evtchn);\r\nif (irq == -1) {\r\nirq = xen_allocate_irq_dynamic();\r\nif (irq < 0)\r\ngoto out;\r\nirq_set_chip_and_handler_name(irq, &xen_dynamic_chip,\r\nhandle_edge_irq, "event");\r\nret = xen_irq_info_evtchn_setup(irq, evtchn);\r\nif (ret < 0) {\r\n__unbind_from_irq(irq);\r\nirq = ret;\r\ngoto out;\r\n}\r\nbind_evtchn_to_cpu(evtchn, 0);\r\n} else {\r\nstruct irq_info *info = info_for_irq(irq);\r\nWARN_ON(info == NULL || info->type != IRQT_EVTCHN);\r\n}\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nstatic int bind_ipi_to_irq(unsigned int ipi, unsigned int cpu)\r\n{\r\nstruct evtchn_bind_ipi bind_ipi;\r\nint evtchn, irq;\r\nint ret;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = per_cpu(ipi_to_irq, cpu)[ipi];\r\nif (irq == -1) {\r\nirq = xen_allocate_irq_dynamic();\r\nif (irq < 0)\r\ngoto out;\r\nirq_set_chip_and_handler_name(irq, &xen_percpu_chip,\r\nhandle_percpu_irq, "ipi");\r\nbind_ipi.vcpu = cpu;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_ipi,\r\n&bind_ipi) != 0)\r\nBUG();\r\nevtchn = bind_ipi.port;\r\nret = xen_irq_info_ipi_setup(cpu, irq, evtchn, ipi);\r\nif (ret < 0) {\r\n__unbind_from_irq(irq);\r\nirq = ret;\r\ngoto out;\r\n}\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n} else {\r\nstruct irq_info *info = info_for_irq(irq);\r\nWARN_ON(info == NULL || info->type != IRQT_IPI);\r\n}\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nint bind_interdomain_evtchn_to_irq(unsigned int remote_domain,\r\nunsigned int remote_port)\r\n{\r\nstruct evtchn_bind_interdomain bind_interdomain;\r\nint err;\r\nbind_interdomain.remote_dom = remote_domain;\r\nbind_interdomain.remote_port = remote_port;\r\nerr = HYPERVISOR_event_channel_op(EVTCHNOP_bind_interdomain,\r\n&bind_interdomain);\r\nreturn err ? : bind_evtchn_to_irq(bind_interdomain.local_port);\r\n}\r\nstatic int find_virq(unsigned int virq, unsigned int cpu)\r\n{\r\nstruct evtchn_status status;\r\nint port, rc = -ENOENT;\r\nmemset(&status, 0, sizeof(status));\r\nfor (port = 0; port < xen_evtchn_max_channels(); port++) {\r\nstatus.dom = DOMID_SELF;\r\nstatus.port = port;\r\nrc = HYPERVISOR_event_channel_op(EVTCHNOP_status, &status);\r\nif (rc < 0)\r\ncontinue;\r\nif (status.status != EVTCHNSTAT_virq)\r\ncontinue;\r\nif (status.u.virq == virq && status.vcpu == cpu) {\r\nrc = port;\r\nbreak;\r\n}\r\n}\r\nreturn rc;\r\n}\r\nunsigned xen_evtchn_nr_channels(void)\r\n{\r\nreturn evtchn_ops->nr_channels();\r\n}\r\nint bind_virq_to_irq(unsigned int virq, unsigned int cpu, bool percpu)\r\n{\r\nstruct evtchn_bind_virq bind_virq;\r\nint evtchn, irq, ret;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = per_cpu(virq_to_irq, cpu)[virq];\r\nif (irq == -1) {\r\nirq = xen_allocate_irq_dynamic();\r\nif (irq < 0)\r\ngoto out;\r\nif (percpu)\r\nirq_set_chip_and_handler_name(irq, &xen_percpu_chip,\r\nhandle_percpu_irq, "virq");\r\nelse\r\nirq_set_chip_and_handler_name(irq, &xen_dynamic_chip,\r\nhandle_edge_irq, "virq");\r\nbind_virq.virq = virq;\r\nbind_virq.vcpu = cpu;\r\nret = HYPERVISOR_event_channel_op(EVTCHNOP_bind_virq,\r\n&bind_virq);\r\nif (ret == 0)\r\nevtchn = bind_virq.port;\r\nelse {\r\nif (ret == -EEXIST)\r\nret = find_virq(virq, cpu);\r\nBUG_ON(ret < 0);\r\nevtchn = ret;\r\n}\r\nret = xen_irq_info_virq_setup(cpu, irq, evtchn, virq);\r\nif (ret < 0) {\r\n__unbind_from_irq(irq);\r\nirq = ret;\r\ngoto out;\r\n}\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n} else {\r\nstruct irq_info *info = info_for_irq(irq);\r\nWARN_ON(info == NULL || info->type != IRQT_VIRQ);\r\n}\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nstatic void unbind_from_irq(unsigned int irq)\r\n{\r\nmutex_lock(&irq_mapping_update_lock);\r\n__unbind_from_irq(irq);\r\nmutex_unlock(&irq_mapping_update_lock);\r\n}\r\nint bind_evtchn_to_irqhandler(unsigned int evtchn,\r\nirq_handler_t handler,\r\nunsigned long irqflags,\r\nconst char *devname, void *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_evtchn_to_irq(evtchn);\r\nif (irq < 0)\r\nreturn irq;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nint bind_interdomain_evtchn_to_irqhandler(unsigned int remote_domain,\r\nunsigned int remote_port,\r\nirq_handler_t handler,\r\nunsigned long irqflags,\r\nconst char *devname,\r\nvoid *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_interdomain_evtchn_to_irq(remote_domain, remote_port);\r\nif (irq < 0)\r\nreturn irq;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nint bind_virq_to_irqhandler(unsigned int virq, unsigned int cpu,\r\nirq_handler_t handler,\r\nunsigned long irqflags, const char *devname, void *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_virq_to_irq(virq, cpu, irqflags & IRQF_PERCPU);\r\nif (irq < 0)\r\nreturn irq;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nint bind_ipi_to_irqhandler(enum ipi_vector ipi,\r\nunsigned int cpu,\r\nirq_handler_t handler,\r\nunsigned long irqflags,\r\nconst char *devname,\r\nvoid *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_ipi_to_irq(ipi, cpu);\r\nif (irq < 0)\r\nreturn irq;\r\nirqflags |= IRQF_NO_SUSPEND | IRQF_FORCE_RESUME | IRQF_EARLY_RESUME;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nvoid unbind_from_irqhandler(unsigned int irq, void *dev_id)\r\n{\r\nstruct irq_info *info = irq_get_handler_data(irq);\r\nif (WARN_ON(!info))\r\nreturn;\r\nfree_irq(irq, dev_id);\r\nunbind_from_irq(irq);\r\n}\r\nint xen_set_irq_priority(unsigned irq, unsigned priority)\r\n{\r\nstruct evtchn_set_priority set_priority;\r\nset_priority.port = evtchn_from_irq(irq);\r\nset_priority.priority = priority;\r\nreturn HYPERVISOR_event_channel_op(EVTCHNOP_set_priority,\r\n&set_priority);\r\n}\r\nint evtchn_make_refcounted(unsigned int evtchn)\r\n{\r\nint irq = get_evtchn_to_irq(evtchn);\r\nstruct irq_info *info;\r\nif (irq == -1)\r\nreturn -ENOENT;\r\ninfo = irq_get_handler_data(irq);\r\nif (!info)\r\nreturn -ENOENT;\r\nWARN_ON(info->refcnt != -1);\r\ninfo->refcnt = 1;\r\nreturn 0;\r\n}\r\nint evtchn_get(unsigned int evtchn)\r\n{\r\nint irq;\r\nstruct irq_info *info;\r\nint err = -ENOENT;\r\nif (evtchn >= xen_evtchn_max_channels())\r\nreturn -EINVAL;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = get_evtchn_to_irq(evtchn);\r\nif (irq == -1)\r\ngoto done;\r\ninfo = irq_get_handler_data(irq);\r\nif (!info)\r\ngoto done;\r\nerr = -EINVAL;\r\nif (info->refcnt <= 0)\r\ngoto done;\r\ninfo->refcnt++;\r\nerr = 0;\r\ndone:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn err;\r\n}\r\nvoid evtchn_put(unsigned int evtchn)\r\n{\r\nint irq = get_evtchn_to_irq(evtchn);\r\nif (WARN_ON(irq == -1))\r\nreturn;\r\nunbind_from_irq(irq);\r\n}\r\nvoid xen_send_IPI_one(unsigned int cpu, enum ipi_vector vector)\r\n{\r\nint irq;\r\n#ifdef CONFIG_X86\r\nif (unlikely(vector == XEN_NMI_VECTOR)) {\r\nint rc = HYPERVISOR_vcpu_op(VCPUOP_send_nmi, cpu, NULL);\r\nif (rc < 0)\r\nprintk(KERN_WARNING "Sending nmi to CPU%d failed (rc:%d)\n", cpu, rc);\r\nreturn;\r\n}\r\n#endif\r\nirq = per_cpu(ipi_to_irq, cpu)[vector];\r\nBUG_ON(irq < 0);\r\nnotify_remote_via_irq(irq);\r\n}\r\nstatic void __xen_evtchn_do_upcall(void)\r\n{\r\nstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\r\nint cpu = get_cpu();\r\nunsigned count;\r\ndo {\r\nvcpu_info->evtchn_upcall_pending = 0;\r\nif (__this_cpu_inc_return(xed_nesting_count) - 1)\r\ngoto out;\r\nxen_evtchn_handle_events(cpu);\r\nBUG_ON(!irqs_disabled());\r\ncount = __this_cpu_read(xed_nesting_count);\r\n__this_cpu_write(xed_nesting_count, 0);\r\n} while (count != 1 || vcpu_info->evtchn_upcall_pending);\r\nout:\r\nput_cpu();\r\n}\r\nvoid xen_evtchn_do_upcall(struct pt_regs *regs)\r\n{\r\nstruct pt_regs *old_regs = set_irq_regs(regs);\r\nirq_enter();\r\n#ifdef CONFIG_X86\r\nexit_idle();\r\ninc_irq_stat(irq_hv_callback_count);\r\n#endif\r\n__xen_evtchn_do_upcall();\r\nirq_exit();\r\nset_irq_regs(old_regs);\r\n}\r\nvoid xen_hvm_evtchn_do_upcall(void)\r\n{\r\n__xen_evtchn_do_upcall();\r\n}\r\nvoid rebind_evtchn_irq(int evtchn, int irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nif (WARN_ON(!info))\r\nreturn;\r\ndisable_irq(irq);\r\nmutex_lock(&irq_mapping_update_lock);\r\nBUG_ON(get_evtchn_to_irq(evtchn) != -1);\r\nBUG_ON(info->type == IRQT_UNBOUND);\r\n(void)xen_irq_info_evtchn_setup(irq, evtchn);\r\nmutex_unlock(&irq_mapping_update_lock);\r\nbind_evtchn_to_cpu(evtchn, info->cpu);\r\nirq_set_affinity(irq, cpumask_of(info->cpu));\r\nenable_irq(irq);\r\n}\r\nstatic int rebind_irq_to_cpu(unsigned irq, unsigned tcpu)\r\n{\r\nstruct evtchn_bind_vcpu bind_vcpu;\r\nint evtchn = evtchn_from_irq(irq);\r\nint masked;\r\nif (!VALID_EVTCHN(evtchn))\r\nreturn -1;\r\nif (!xen_support_evtchn_rebind())\r\nreturn -1;\r\nbind_vcpu.port = evtchn;\r\nbind_vcpu.vcpu = tcpu;\r\nmasked = test_and_set_mask(evtchn);\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_vcpu, &bind_vcpu) >= 0)\r\nbind_evtchn_to_cpu(evtchn, tcpu);\r\nif (!masked)\r\nunmask_evtchn(evtchn);\r\nreturn 0;\r\n}\r\nstatic int set_affinity_irq(struct irq_data *data, const struct cpumask *dest,\r\nbool force)\r\n{\r\nunsigned tcpu = cpumask_first_and(dest, cpu_online_mask);\r\nreturn rebind_irq_to_cpu(data->irq, tcpu);\r\n}\r\nstatic void enable_dynirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nif (VALID_EVTCHN(evtchn))\r\nunmask_evtchn(evtchn);\r\n}\r\nstatic void disable_dynirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nif (VALID_EVTCHN(evtchn))\r\nmask_evtchn(evtchn);\r\n}\r\nstatic void ack_dynirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nirq_move_irq(data);\r\nif (VALID_EVTCHN(evtchn))\r\nclear_evtchn(evtchn);\r\n}\r\nstatic void mask_ack_dynirq(struct irq_data *data)\r\n{\r\ndisable_dynirq(data);\r\nack_dynirq(data);\r\n}\r\nstatic int retrigger_dynirq(struct irq_data *data)\r\n{\r\nunsigned int evtchn = evtchn_from_irq(data->irq);\r\nint masked;\r\nif (!VALID_EVTCHN(evtchn))\r\nreturn 0;\r\nmasked = test_and_set_mask(evtchn);\r\nset_evtchn(evtchn);\r\nif (!masked)\r\nunmask_evtchn(evtchn);\r\nreturn 1;\r\n}\r\nstatic void restore_pirqs(void)\r\n{\r\nint pirq, rc, irq, gsi;\r\nstruct physdev_map_pirq map_irq;\r\nstruct irq_info *info;\r\nlist_for_each_entry(info, &xen_irq_list_head, list) {\r\nif (info->type != IRQT_PIRQ)\r\ncontinue;\r\npirq = info->u.pirq.pirq;\r\ngsi = info->u.pirq.gsi;\r\nirq = info->irq;\r\nif (!gsi)\r\ncontinue;\r\nmap_irq.domid = DOMID_SELF;\r\nmap_irq.type = MAP_PIRQ_TYPE_GSI;\r\nmap_irq.index = gsi;\r\nmap_irq.pirq = pirq;\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_map_pirq, &map_irq);\r\nif (rc) {\r\npr_warn("xen map irq failed gsi=%d irq=%d pirq=%d rc=%d\n",\r\ngsi, irq, pirq, rc);\r\nxen_free_irq(irq);\r\ncontinue;\r\n}\r\nprintk(KERN_DEBUG "xen: --> irq=%d, pirq=%d\n", irq, map_irq.pirq);\r\n__startup_pirq(irq);\r\n}\r\n}\r\nstatic void restore_cpu_virqs(unsigned int cpu)\r\n{\r\nstruct evtchn_bind_virq bind_virq;\r\nint virq, irq, evtchn;\r\nfor (virq = 0; virq < NR_VIRQS; virq++) {\r\nif ((irq = per_cpu(virq_to_irq, cpu)[virq]) == -1)\r\ncontinue;\r\nBUG_ON(virq_from_irq(irq) != virq);\r\nbind_virq.virq = virq;\r\nbind_virq.vcpu = cpu;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_virq,\r\n&bind_virq) != 0)\r\nBUG();\r\nevtchn = bind_virq.port;\r\n(void)xen_irq_info_virq_setup(cpu, irq, evtchn, virq);\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n}\r\n}\r\nstatic void restore_cpu_ipis(unsigned int cpu)\r\n{\r\nstruct evtchn_bind_ipi bind_ipi;\r\nint ipi, irq, evtchn;\r\nfor (ipi = 0; ipi < XEN_NR_IPIS; ipi++) {\r\nif ((irq = per_cpu(ipi_to_irq, cpu)[ipi]) == -1)\r\ncontinue;\r\nBUG_ON(ipi_from_irq(irq) != ipi);\r\nbind_ipi.vcpu = cpu;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_ipi,\r\n&bind_ipi) != 0)\r\nBUG();\r\nevtchn = bind_ipi.port;\r\n(void)xen_irq_info_ipi_setup(cpu, irq, evtchn, ipi);\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n}\r\n}\r\nvoid xen_clear_irq_pending(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn))\r\nclear_evtchn(evtchn);\r\n}\r\nvoid xen_set_irq_pending(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn))\r\nset_evtchn(evtchn);\r\n}\r\nbool xen_test_irq_pending(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nbool ret = false;\r\nif (VALID_EVTCHN(evtchn))\r\nret = test_evtchn(evtchn);\r\nreturn ret;\r\n}\r\nvoid xen_poll_irq_timeout(int irq, u64 timeout)\r\n{\r\nevtchn_port_t evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn)) {\r\nstruct sched_poll poll;\r\npoll.nr_ports = 1;\r\npoll.timeout = timeout;\r\nset_xen_guest_handle(poll.ports, &evtchn);\r\nif (HYPERVISOR_sched_op(SCHEDOP_poll, &poll) != 0)\r\nBUG();\r\n}\r\n}\r\nvoid xen_poll_irq(int irq)\r\n{\r\nxen_poll_irq_timeout(irq, 0 );\r\n}\r\nint xen_test_irq_shared(int irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nstruct physdev_irq_status_query irq_status;\r\nif (WARN_ON(!info))\r\nreturn -ENOENT;\r\nirq_status.irq = info->u.pirq.pirq;\r\nif (HYPERVISOR_physdev_op(PHYSDEVOP_irq_status_query, &irq_status))\r\nreturn 0;\r\nreturn !(irq_status.flags & XENIRQSTAT_shared);\r\n}\r\nvoid xen_irq_resume(void)\r\n{\r\nunsigned int cpu;\r\nstruct irq_info *info;\r\nxen_evtchn_mask_all();\r\nxen_evtchn_resume();\r\nlist_for_each_entry(info, &xen_irq_list_head, list)\r\ninfo->evtchn = 0;\r\nclear_evtchn_to_irq_all();\r\nfor_each_possible_cpu(cpu) {\r\nrestore_cpu_virqs(cpu);\r\nrestore_cpu_ipis(cpu);\r\n}\r\nrestore_pirqs();\r\n}\r\nint xen_set_callback_via(uint64_t via)\r\n{\r\nstruct xen_hvm_param a;\r\na.domid = DOMID_SELF;\r\na.index = HVM_PARAM_CALLBACK_IRQ;\r\na.value = via;\r\nreturn HYPERVISOR_hvm_op(HVMOP_set_param, &a);\r\n}\r\nvoid xen_callback_vector(void)\r\n{\r\nint rc;\r\nuint64_t callback_via;\r\nif (xen_have_vector_callback) {\r\ncallback_via = HVM_CALLBACK_VECTOR(HYPERVISOR_CALLBACK_VECTOR);\r\nrc = xen_set_callback_via(callback_via);\r\nif (rc) {\r\npr_err("Request for Xen HVM callback vector failed\n");\r\nxen_have_vector_callback = 0;\r\nreturn;\r\n}\r\npr_info("Xen HVM callback vector for event delivery is enabled\n");\r\nif (!test_bit(HYPERVISOR_CALLBACK_VECTOR, used_vectors))\r\nalloc_intr_gate(HYPERVISOR_CALLBACK_VECTOR,\r\nxen_hvm_callback_vector);\r\n}\r\n}\r\nvoid xen_callback_vector(void) {}\r\nvoid __init xen_init_IRQ(void)\r\n{\r\nint ret = -EINVAL;\r\nif (fifo_events)\r\nret = xen_evtchn_fifo_init();\r\nif (ret < 0)\r\nxen_evtchn_2l_init();\r\nevtchn_to_irq = kcalloc(EVTCHN_ROW(xen_evtchn_max_channels()),\r\nsizeof(*evtchn_to_irq), GFP_KERNEL);\r\nBUG_ON(!evtchn_to_irq);\r\nxen_evtchn_mask_all();\r\npirq_needs_eoi = pirq_needs_eoi_flag;\r\n#ifdef CONFIG_X86\r\nif (xen_pv_domain()) {\r\nirq_ctx_init(smp_processor_id());\r\nif (xen_initial_domain())\r\npci_xen_initial_domain();\r\n}\r\nif (xen_feature(XENFEAT_hvm_callback_vector))\r\nxen_callback_vector();\r\nif (xen_hvm_domain()) {\r\nnative_init_IRQ();\r\npci_xen_hvm_init();\r\n} else {\r\nint rc;\r\nstruct physdev_pirq_eoi_gmfn eoi_gmfn;\r\npirq_eoi_map = (void *)__get_free_page(GFP_KERNEL|__GFP_ZERO);\r\neoi_gmfn.gmfn = virt_to_gfn(pirq_eoi_map);\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_pirq_eoi_gmfn_v2, &eoi_gmfn);\r\nif (rc != 0) {\r\nfree_page((unsigned long) pirq_eoi_map);\r\npirq_eoi_map = NULL;\r\n} else\r\npirq_needs_eoi = pirq_check_eoi_map;\r\n}\r\n#endif\r\n}
