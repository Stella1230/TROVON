static int ecb_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ecb_crypt_128bit(&serpent_enc, desc, dst, src, nbytes);\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ecb_crypt_128bit(&serpent_dec, desc, dst, src, nbytes);\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_cbc_encrypt_128bit(GLUE_FUNC_CAST(__serpent_encrypt), desc,\r\ndst, src, nbytes);\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_cbc_decrypt_128bit(&serpent_dec_cbc, desc, dst, src,\r\nnbytes);\r\n}\r\nstatic int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ctr_crypt_128bit(&serpent_ctr, desc, dst, src, nbytes);\r\n}\r\nstatic inline bool serpent_fpu_begin(bool fpu_enabled, unsigned int nbytes)\r\n{\r\nreturn glue_fpu_begin(SERPENT_BLOCK_SIZE, 8, NULL, fpu_enabled, nbytes);\r\n}\r\nstatic inline void serpent_fpu_end(bool fpu_enabled)\r\n{\r\nglue_fpu_end(fpu_enabled);\r\n}\r\nstatic void encrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes >= SERPENT_AVX2_PARALLEL_BLOCKS * bsize) {\r\nserpent_ecb_enc_16way(ctx->ctx, srcdst, srcdst);\r\nsrcdst += bsize * SERPENT_AVX2_PARALLEL_BLOCKS;\r\nnbytes -= bsize * SERPENT_AVX2_PARALLEL_BLOCKS;\r\n}\r\nwhile (nbytes >= SERPENT_PARALLEL_BLOCKS * bsize) {\r\nserpent_ecb_enc_8way_avx(ctx->ctx, srcdst, srcdst);\r\nsrcdst += bsize * SERPENT_PARALLEL_BLOCKS;\r\nnbytes -= bsize * SERPENT_PARALLEL_BLOCKS;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_encrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic void decrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes >= SERPENT_AVX2_PARALLEL_BLOCKS * bsize) {\r\nserpent_ecb_dec_16way(ctx->ctx, srcdst, srcdst);\r\nsrcdst += bsize * SERPENT_AVX2_PARALLEL_BLOCKS;\r\nnbytes -= bsize * SERPENT_AVX2_PARALLEL_BLOCKS;\r\n}\r\nwhile (nbytes >= SERPENT_PARALLEL_BLOCKS * bsize) {\r\nserpent_ecb_dec_8way_avx(ctx->ctx, srcdst, srcdst);\r\nsrcdst += bsize * SERPENT_PARALLEL_BLOCKS;\r\nnbytes -= bsize * SERPENT_PARALLEL_BLOCKS;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_decrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic int lrw_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_AVX2_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int lrw_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_AVX2_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nreturn glue_xts_crypt_128bit(&serpent_enc_xts, desc, dst, src, nbytes,\r\nXTS_TWEAK_CAST(__serpent_encrypt),\r\n&ctx->tweak_ctx, &ctx->crypt_ctx);\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nreturn glue_xts_crypt_128bit(&serpent_dec_xts, desc, dst, src, nbytes,\r\nXTS_TWEAK_CAST(__serpent_encrypt),\r\n&ctx->tweak_ctx, &ctx->crypt_ctx);\r\n}\r\nstatic int __init init(void)\r\n{\r\nconst char *feature_name;\r\nif (!cpu_has_avx2 || !cpu_has_osxsave) {\r\npr_info("AVX2 instructions are not detected.\n");\r\nreturn -ENODEV;\r\n}\r\nif (!cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM,\r\n&feature_name)) {\r\npr_info("CPU feature '%s' is not supported.\n", feature_name);\r\nreturn -ENODEV;\r\n}\r\nreturn crypto_register_algs(srp_algs, ARRAY_SIZE(srp_algs));\r\n}\r\nstatic void __exit fini(void)\r\n{\r\ncrypto_unregister_algs(srp_algs, ARRAY_SIZE(srp_algs));\r\n}
