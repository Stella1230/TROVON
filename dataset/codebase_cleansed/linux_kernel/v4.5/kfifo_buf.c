static inline int __iio_allocate_kfifo(struct iio_kfifo *buf,\r\nint bytes_per_datum, int length)\r\n{\r\nif ((length == 0) || (bytes_per_datum == 0))\r\nreturn -EINVAL;\r\nreturn __kfifo_alloc((struct __kfifo *)&buf->kf, length,\r\nbytes_per_datum, GFP_KERNEL);\r\n}\r\nstatic int iio_request_update_kfifo(struct iio_buffer *r)\r\n{\r\nint ret = 0;\r\nstruct iio_kfifo *buf = iio_to_kfifo(r);\r\nmutex_lock(&buf->user_lock);\r\nif (buf->update_needed) {\r\nkfifo_free(&buf->kf);\r\nret = __iio_allocate_kfifo(buf, buf->buffer.bytes_per_datum,\r\nbuf->buffer.length);\r\nif (ret >= 0)\r\nbuf->update_needed = false;\r\n} else {\r\nkfifo_reset_out(&buf->kf);\r\n}\r\nmutex_unlock(&buf->user_lock);\r\nreturn ret;\r\n}\r\nstatic int iio_mark_update_needed_kfifo(struct iio_buffer *r)\r\n{\r\nstruct iio_kfifo *kf = iio_to_kfifo(r);\r\nkf->update_needed = true;\r\nreturn 0;\r\n}\r\nstatic int iio_set_bytes_per_datum_kfifo(struct iio_buffer *r, size_t bpd)\r\n{\r\nif (r->bytes_per_datum != bpd) {\r\nr->bytes_per_datum = bpd;\r\niio_mark_update_needed_kfifo(r);\r\n}\r\nreturn 0;\r\n}\r\nstatic int iio_set_length_kfifo(struct iio_buffer *r, int length)\r\n{\r\nif (length < 2)\r\nlength = 2;\r\nif (r->length != length) {\r\nr->length = length;\r\niio_mark_update_needed_kfifo(r);\r\n}\r\nreturn 0;\r\n}\r\nstatic int iio_store_to_kfifo(struct iio_buffer *r,\r\nconst void *data)\r\n{\r\nint ret;\r\nstruct iio_kfifo *kf = iio_to_kfifo(r);\r\nret = kfifo_in(&kf->kf, data, 1);\r\nif (ret != 1)\r\nreturn -EBUSY;\r\nreturn 0;\r\n}\r\nstatic int iio_read_first_n_kfifo(struct iio_buffer *r,\r\nsize_t n, char __user *buf)\r\n{\r\nint ret, copied;\r\nstruct iio_kfifo *kf = iio_to_kfifo(r);\r\nif (mutex_lock_interruptible(&kf->user_lock))\r\nreturn -ERESTARTSYS;\r\nif (!kfifo_initialized(&kf->kf) || n < kfifo_esize(&kf->kf))\r\nret = -EINVAL;\r\nelse\r\nret = kfifo_to_user(&kf->kf, buf, n, &copied);\r\nmutex_unlock(&kf->user_lock);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn copied;\r\n}\r\nstatic size_t iio_kfifo_buf_data_available(struct iio_buffer *r)\r\n{\r\nstruct iio_kfifo *kf = iio_to_kfifo(r);\r\nsize_t samples;\r\nmutex_lock(&kf->user_lock);\r\nsamples = kfifo_len(&kf->kf);\r\nmutex_unlock(&kf->user_lock);\r\nreturn samples;\r\n}\r\nstatic void iio_kfifo_buffer_release(struct iio_buffer *buffer)\r\n{\r\nstruct iio_kfifo *kf = iio_to_kfifo(buffer);\r\nmutex_destroy(&kf->user_lock);\r\nkfifo_free(&kf->kf);\r\nkfree(kf);\r\n}\r\nstruct iio_buffer *iio_kfifo_allocate(void)\r\n{\r\nstruct iio_kfifo *kf;\r\nkf = kzalloc(sizeof(*kf), GFP_KERNEL);\r\nif (!kf)\r\nreturn NULL;\r\nkf->update_needed = true;\r\niio_buffer_init(&kf->buffer);\r\nkf->buffer.access = &kfifo_access_funcs;\r\nkf->buffer.length = 2;\r\nmutex_init(&kf->user_lock);\r\nreturn &kf->buffer;\r\n}\r\nvoid iio_kfifo_free(struct iio_buffer *r)\r\n{\r\niio_buffer_put(r);\r\n}\r\nstatic void devm_iio_kfifo_release(struct device *dev, void *res)\r\n{\r\niio_kfifo_free(*(struct iio_buffer **)res);\r\n}\r\nstatic int devm_iio_kfifo_match(struct device *dev, void *res, void *data)\r\n{\r\nstruct iio_buffer **r = res;\r\nif (WARN_ON(!r || !*r))\r\nreturn 0;\r\nreturn *r == data;\r\n}\r\nstruct iio_buffer *devm_iio_kfifo_allocate(struct device *dev)\r\n{\r\nstruct iio_buffer **ptr, *r;\r\nptr = devres_alloc(devm_iio_kfifo_release, sizeof(*ptr), GFP_KERNEL);\r\nif (!ptr)\r\nreturn NULL;\r\nr = iio_kfifo_allocate();\r\nif (r) {\r\n*ptr = r;\r\ndevres_add(dev, ptr);\r\n} else {\r\ndevres_free(ptr);\r\n}\r\nreturn r;\r\n}\r\nvoid devm_iio_kfifo_free(struct device *dev, struct iio_buffer *r)\r\n{\r\nWARN_ON(devres_release(dev, devm_iio_kfifo_release,\r\ndevm_iio_kfifo_match, r));\r\n}
