static inline bool __rmid_valid(u32 rmid)\r\n{\r\nif (!rmid || rmid == INVALID_RMID)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic u64 __rmid_read(u32 rmid)\r\n{\r\nu64 val;\r\nwrmsr(MSR_IA32_QM_EVTSEL, QOS_L3_OCCUP_EVENT_ID, rmid);\r\nrdmsrl(MSR_IA32_QM_CTR, val);\r\nreturn val;\r\n}\r\nstatic inline struct cqm_rmid_entry *__rmid_entry(u32 rmid)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nentry = cqm_rmid_ptrs[rmid];\r\nWARN_ON(entry->rmid != rmid);\r\nreturn entry;\r\n}\r\nstatic u32 __get_rmid(void)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nlockdep_assert_held(&cache_mutex);\r\nif (list_empty(&cqm_rmid_free_lru))\r\nreturn INVALID_RMID;\r\nentry = list_first_entry(&cqm_rmid_free_lru, struct cqm_rmid_entry, list);\r\nlist_del(&entry->list);\r\nreturn entry->rmid;\r\n}\r\nstatic void __put_rmid(u32 rmid)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nlockdep_assert_held(&cache_mutex);\r\nWARN_ON(!__rmid_valid(rmid));\r\nentry = __rmid_entry(rmid);\r\nentry->queue_time = jiffies;\r\nentry->state = RMID_YOUNG;\r\nlist_add_tail(&entry->list, &cqm_rmid_limbo_lru);\r\n}\r\nstatic int intel_cqm_setup_rmid_cache(void)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nunsigned int nr_rmids;\r\nint r = 0;\r\nnr_rmids = cqm_max_rmid + 1;\r\ncqm_rmid_ptrs = kmalloc(sizeof(struct cqm_rmid_entry *) *\r\nnr_rmids, GFP_KERNEL);\r\nif (!cqm_rmid_ptrs)\r\nreturn -ENOMEM;\r\nfor (; r <= cqm_max_rmid; r++) {\r\nstruct cqm_rmid_entry *entry;\r\nentry = kmalloc(sizeof(*entry), GFP_KERNEL);\r\nif (!entry)\r\ngoto fail;\r\nINIT_LIST_HEAD(&entry->list);\r\nentry->rmid = r;\r\ncqm_rmid_ptrs[r] = entry;\r\nlist_add_tail(&entry->list, &cqm_rmid_free_lru);\r\n}\r\nentry = __rmid_entry(0);\r\nlist_del(&entry->list);\r\nmutex_lock(&cache_mutex);\r\nintel_cqm_rotation_rmid = __get_rmid();\r\nmutex_unlock(&cache_mutex);\r\nreturn 0;\r\nfail:\r\nwhile (r--)\r\nkfree(cqm_rmid_ptrs[r]);\r\nkfree(cqm_rmid_ptrs);\r\nreturn -ENOMEM;\r\n}\r\nstatic bool __match_event(struct perf_event *a, struct perf_event *b)\r\n{\r\nif ((a->attach_state & PERF_ATTACH_TASK) !=\r\n(b->attach_state & PERF_ATTACH_TASK))\r\nreturn false;\r\n#ifdef CONFIG_CGROUP_PERF\r\nif (a->cgrp != b->cgrp)\r\nreturn false;\r\n#endif\r\nif (!(b->attach_state & PERF_ATTACH_TASK))\r\nreturn true;\r\nif (a->hw.target == b->hw.target)\r\nreturn true;\r\nif (b->parent == a)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline struct perf_cgroup *event_to_cgroup(struct perf_event *event)\r\n{\r\nif (event->attach_state & PERF_ATTACH_TASK)\r\nreturn perf_cgroup_from_task(event->hw.target, event->ctx);\r\nreturn event->cgrp;\r\n}\r\nstatic bool __conflict_event(struct perf_event *a, struct perf_event *b)\r\n{\r\n#ifdef CONFIG_CGROUP_PERF\r\nif (a->cgrp && b->cgrp) {\r\nstruct perf_cgroup *ac = a->cgrp;\r\nstruct perf_cgroup *bc = b->cgrp;\r\nWARN_ON_ONCE(ac == bc);\r\nif (cgroup_is_descendant(ac->css.cgroup, bc->css.cgroup) ||\r\ncgroup_is_descendant(bc->css.cgroup, ac->css.cgroup))\r\nreturn true;\r\nreturn false;\r\n}\r\nif (a->cgrp || b->cgrp) {\r\nstruct perf_cgroup *ac, *bc;\r\nif ((a->cgrp && !(b->attach_state & PERF_ATTACH_TASK)) ||\r\n(b->cgrp && !(a->attach_state & PERF_ATTACH_TASK)))\r\nreturn true;\r\nac = event_to_cgroup(a);\r\nbc = event_to_cgroup(b);\r\nif (ac == bc)\r\nreturn true;\r\nif (!ac || !bc)\r\nreturn false;\r\nif (cgroup_is_descendant(ac->css.cgroup, bc->css.cgroup) ||\r\ncgroup_is_descendant(bc->css.cgroup, ac->css.cgroup))\r\nreturn true;\r\nreturn false;\r\n}\r\n#endif\r\nif (!(a->attach_state & PERF_ATTACH_TASK) ||\r\n!(b->attach_state & PERF_ATTACH_TASK))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic u32 intel_cqm_xchg_rmid(struct perf_event *group, u32 rmid)\r\n{\r\nstruct perf_event *event;\r\nstruct list_head *head = &group->hw.cqm_group_entry;\r\nu32 old_rmid = group->hw.cqm_rmid;\r\nlockdep_assert_held(&cache_mutex);\r\nif (__rmid_valid(old_rmid) && !__rmid_valid(rmid)) {\r\nstruct rmid_read rr = {\r\n.value = ATOMIC64_INIT(0),\r\n.rmid = old_rmid,\r\n};\r\non_each_cpu_mask(&cqm_cpumask, __intel_cqm_event_count,\r\n&rr, 1);\r\nlocal64_set(&group->count, atomic64_read(&rr.value));\r\n}\r\nraw_spin_lock_irq(&cache_lock);\r\ngroup->hw.cqm_rmid = rmid;\r\nlist_for_each_entry(event, head, hw.cqm_group_entry)\r\nevent->hw.cqm_rmid = rmid;\r\nraw_spin_unlock_irq(&cache_lock);\r\nreturn old_rmid;\r\n}\r\nstatic void intel_cqm_stable(void *arg)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nlist_for_each_entry(entry, &cqm_rmid_limbo_lru, list) {\r\nif (entry->state != RMID_AVAILABLE)\r\nbreak;\r\nif (__rmid_read(entry->rmid) > __intel_cqm_threshold)\r\nentry->state = RMID_DIRTY;\r\n}\r\n}\r\nstatic bool intel_cqm_sched_in_event(u32 rmid)\r\n{\r\nstruct perf_event *leader, *event;\r\nlockdep_assert_held(&cache_mutex);\r\nleader = list_first_entry(&cache_groups, struct perf_event,\r\nhw.cqm_groups_entry);\r\nevent = leader;\r\nlist_for_each_entry_continue(event, &cache_groups,\r\nhw.cqm_groups_entry) {\r\nif (__rmid_valid(event->hw.cqm_rmid))\r\ncontinue;\r\nif (__conflict_event(event, leader))\r\ncontinue;\r\nintel_cqm_xchg_rmid(event, rmid);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool intel_cqm_rmid_stabilize(unsigned int *available)\r\n{\r\nstruct cqm_rmid_entry *entry, *tmp;\r\nlockdep_assert_held(&cache_mutex);\r\n*available = 0;\r\nlist_for_each_entry(entry, &cqm_rmid_limbo_lru, list) {\r\nunsigned long min_queue_time;\r\nunsigned long now = jiffies;\r\nmin_queue_time = entry->queue_time +\r\nmsecs_to_jiffies(__rmid_queue_time_ms);\r\nif (time_after(min_queue_time, now))\r\nbreak;\r\nentry->state = RMID_AVAILABLE;\r\n(*available)++;\r\n}\r\nif (!*available)\r\nreturn false;\r\non_each_cpu_mask(&cqm_cpumask, intel_cqm_stable, NULL, true);\r\nlist_for_each_entry_safe(entry, tmp, &cqm_rmid_limbo_lru, list) {\r\nif (entry->state == RMID_YOUNG)\r\nbreak;\r\nif (entry->state == RMID_DIRTY)\r\ncontinue;\r\nlist_del(&entry->list);\r\nif (!__rmid_valid(intel_cqm_rotation_rmid)) {\r\nintel_cqm_rotation_rmid = entry->rmid;\r\ncontinue;\r\n}\r\nif (intel_cqm_sched_in_event(entry->rmid))\r\ncontinue;\r\nlist_add_tail(&entry->list, &cqm_rmid_free_lru);\r\n}\r\nreturn __rmid_valid(intel_cqm_rotation_rmid);\r\n}\r\nstatic void __intel_cqm_pick_and_rotate(struct perf_event *next)\r\n{\r\nstruct perf_event *rotor;\r\nu32 rmid;\r\nlockdep_assert_held(&cache_mutex);\r\nrotor = list_first_entry(&cache_groups, struct perf_event,\r\nhw.cqm_groups_entry);\r\nif (next == rotor)\r\nreturn;\r\nrmid = intel_cqm_xchg_rmid(rotor, INVALID_RMID);\r\n__put_rmid(rmid);\r\nlist_rotate_left(&cache_groups);\r\n}\r\nstatic void intel_cqm_sched_out_conflicting_events(struct perf_event *event)\r\n{\r\nstruct perf_event *group, *g;\r\nu32 rmid;\r\nlockdep_assert_held(&cache_mutex);\r\nlist_for_each_entry_safe(group, g, &cache_groups, hw.cqm_groups_entry) {\r\nif (group == event)\r\ncontinue;\r\nrmid = group->hw.cqm_rmid;\r\nif (!__rmid_valid(rmid))\r\ncontinue;\r\nif (!__conflict_event(group, event))\r\ncontinue;\r\nintel_cqm_xchg_rmid(group, INVALID_RMID);\r\n__put_rmid(rmid);\r\n}\r\n}\r\nstatic bool __intel_cqm_rmid_rotate(void)\r\n{\r\nstruct perf_event *group, *start = NULL;\r\nunsigned int threshold_limit;\r\nunsigned int nr_needed = 0;\r\nunsigned int nr_available;\r\nbool rotated = false;\r\nmutex_lock(&cache_mutex);\r\nagain:\r\nif (list_empty(&cache_groups) && list_empty(&cqm_rmid_limbo_lru))\r\ngoto out;\r\nlist_for_each_entry(group, &cache_groups, hw.cqm_groups_entry) {\r\nif (!__rmid_valid(group->hw.cqm_rmid)) {\r\nif (!start)\r\nstart = group;\r\nnr_needed++;\r\n}\r\n}\r\nif (!nr_needed && list_empty(&cqm_rmid_limbo_lru))\r\ngoto out;\r\nif (!nr_needed)\r\ngoto stabilize;\r\n__intel_cqm_pick_and_rotate(start);\r\nif (__rmid_valid(intel_cqm_rotation_rmid)) {\r\nintel_cqm_xchg_rmid(start, intel_cqm_rotation_rmid);\r\nintel_cqm_rotation_rmid = __get_rmid();\r\nintel_cqm_sched_out_conflicting_events(start);\r\nif (__intel_cqm_threshold)\r\n__intel_cqm_threshold--;\r\n}\r\nrotated = true;\r\nstabilize:\r\nthreshold_limit = __intel_cqm_max_threshold / cqm_l3_scale;\r\nwhile (intel_cqm_rmid_stabilize(&nr_available) &&\r\n__intel_cqm_threshold < threshold_limit) {\r\nunsigned int steal_limit;\r\nif (!nr_needed)\r\nbreak;\r\nsteal_limit = (cqm_max_rmid + 1) / 4;\r\nif (nr_available < steal_limit)\r\ngoto again;\r\n__intel_cqm_threshold++;\r\n}\r\nout:\r\nmutex_unlock(&cache_mutex);\r\nreturn rotated;\r\n}\r\nstatic void intel_cqm_rmid_rotate(struct work_struct *work)\r\n{\r\nunsigned long delay;\r\n__intel_cqm_rmid_rotate();\r\ndelay = msecs_to_jiffies(intel_cqm_pmu.hrtimer_interval_ms);\r\nschedule_delayed_work(&intel_cqm_rmid_work, delay);\r\n}\r\nstatic void intel_cqm_setup_event(struct perf_event *event,\r\nstruct perf_event **group)\r\n{\r\nstruct perf_event *iter;\r\nbool conflict = false;\r\nu32 rmid;\r\nlist_for_each_entry(iter, &cache_groups, hw.cqm_groups_entry) {\r\nrmid = iter->hw.cqm_rmid;\r\nif (__match_event(iter, event)) {\r\nevent->hw.cqm_rmid = rmid;\r\n*group = iter;\r\nreturn;\r\n}\r\nif (__conflict_event(iter, event) && __rmid_valid(rmid))\r\nconflict = true;\r\n}\r\nif (conflict)\r\nrmid = INVALID_RMID;\r\nelse\r\nrmid = __get_rmid();\r\nevent->hw.cqm_rmid = rmid;\r\n}\r\nstatic void intel_cqm_event_read(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nu32 rmid;\r\nu64 val;\r\nif (event->cpu == -1)\r\nreturn;\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nrmid = event->hw.cqm_rmid;\r\nif (!__rmid_valid(rmid))\r\ngoto out;\r\nval = __rmid_read(rmid);\r\nif (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))\r\ngoto out;\r\nlocal64_set(&event->count, val);\r\nout:\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\n}\r\nstatic void __intel_cqm_event_count(void *info)\r\n{\r\nstruct rmid_read *rr = info;\r\nu64 val;\r\nval = __rmid_read(rr->rmid);\r\nif (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))\r\nreturn;\r\natomic64_add(val, &rr->value);\r\n}\r\nstatic inline bool cqm_group_leader(struct perf_event *event)\r\n{\r\nreturn !list_empty(&event->hw.cqm_groups_entry);\r\n}\r\nstatic u64 intel_cqm_event_count(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct rmid_read rr = {\r\n.value = ATOMIC64_INIT(0),\r\n};\r\nif (event->cpu != -1)\r\nreturn __perf_event_count(event);\r\nif (!cqm_group_leader(event))\r\nreturn 0;\r\nif (unlikely(in_interrupt()))\r\ngoto out;\r\nrr.rmid = ACCESS_ONCE(event->hw.cqm_rmid);\r\nif (!__rmid_valid(rr.rmid))\r\ngoto out;\r\non_each_cpu_mask(&cqm_cpumask, __intel_cqm_event_count, &rr, 1);\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nif (event->hw.cqm_rmid == rr.rmid)\r\nlocal64_set(&event->count, atomic64_read(&rr.value));\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\nout:\r\nreturn __perf_event_count(event);\r\n}\r\nstatic void intel_cqm_event_start(struct perf_event *event, int mode)\r\n{\r\nstruct intel_pqr_state *state = this_cpu_ptr(&pqr_state);\r\nu32 rmid = event->hw.cqm_rmid;\r\nif (!(event->hw.cqm_state & PERF_HES_STOPPED))\r\nreturn;\r\nevent->hw.cqm_state &= ~PERF_HES_STOPPED;\r\nif (state->rmid_usecnt++) {\r\nif (!WARN_ON_ONCE(state->rmid != rmid))\r\nreturn;\r\n} else {\r\nWARN_ON_ONCE(state->rmid);\r\n}\r\nstate->rmid = rmid;\r\nwrmsr(MSR_IA32_PQR_ASSOC, rmid, state->closid);\r\n}\r\nstatic void intel_cqm_event_stop(struct perf_event *event, int mode)\r\n{\r\nstruct intel_pqr_state *state = this_cpu_ptr(&pqr_state);\r\nif (event->hw.cqm_state & PERF_HES_STOPPED)\r\nreturn;\r\nevent->hw.cqm_state |= PERF_HES_STOPPED;\r\nintel_cqm_event_read(event);\r\nif (!--state->rmid_usecnt) {\r\nstate->rmid = 0;\r\nwrmsr(MSR_IA32_PQR_ASSOC, 0, state->closid);\r\n} else {\r\nWARN_ON_ONCE(!state->rmid);\r\n}\r\n}\r\nstatic int intel_cqm_event_add(struct perf_event *event, int mode)\r\n{\r\nunsigned long flags;\r\nu32 rmid;\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nevent->hw.cqm_state = PERF_HES_STOPPED;\r\nrmid = event->hw.cqm_rmid;\r\nif (__rmid_valid(rmid) && (mode & PERF_EF_START))\r\nintel_cqm_event_start(event, mode);\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\nreturn 0;\r\n}\r\nstatic void intel_cqm_event_destroy(struct perf_event *event)\r\n{\r\nstruct perf_event *group_other = NULL;\r\nmutex_lock(&cache_mutex);\r\nif (!list_empty(&event->hw.cqm_group_entry)) {\r\ngroup_other = list_first_entry(&event->hw.cqm_group_entry,\r\nstruct perf_event,\r\nhw.cqm_group_entry);\r\nlist_del(&event->hw.cqm_group_entry);\r\n}\r\nif (cqm_group_leader(event)) {\r\nif (group_other) {\r\nlist_replace(&event->hw.cqm_groups_entry,\r\n&group_other->hw.cqm_groups_entry);\r\n} else {\r\nu32 rmid = event->hw.cqm_rmid;\r\nif (__rmid_valid(rmid))\r\n__put_rmid(rmid);\r\nlist_del(&event->hw.cqm_groups_entry);\r\n}\r\n}\r\nmutex_unlock(&cache_mutex);\r\n}\r\nstatic int intel_cqm_event_init(struct perf_event *event)\r\n{\r\nstruct perf_event *group = NULL;\r\nbool rotate = false;\r\nif (event->attr.type != intel_cqm_pmu.type)\r\nreturn -ENOENT;\r\nif (event->attr.config & ~QOS_EVENT_MASK)\r\nreturn -EINVAL;\r\nif (event->attr.exclude_user ||\r\nevent->attr.exclude_kernel ||\r\nevent->attr.exclude_hv ||\r\nevent->attr.exclude_idle ||\r\nevent->attr.exclude_host ||\r\nevent->attr.exclude_guest ||\r\nevent->attr.sample_period)\r\nreturn -EINVAL;\r\nINIT_LIST_HEAD(&event->hw.cqm_group_entry);\r\nINIT_LIST_HEAD(&event->hw.cqm_groups_entry);\r\nevent->destroy = intel_cqm_event_destroy;\r\nmutex_lock(&cache_mutex);\r\nintel_cqm_setup_event(event, &group);\r\nif (group) {\r\nlist_add_tail(&event->hw.cqm_group_entry,\r\n&group->hw.cqm_group_entry);\r\n} else {\r\nlist_add_tail(&event->hw.cqm_groups_entry,\r\n&cache_groups);\r\nif (!__rmid_valid(event->hw.cqm_rmid))\r\nrotate = true;\r\n}\r\nmutex_unlock(&cache_mutex);\r\nif (rotate)\r\nschedule_delayed_work(&intel_cqm_rmid_work, 0);\r\nreturn 0;\r\n}\r\nstatic ssize_t\r\nmax_recycle_threshold_show(struct device *dev, struct device_attribute *attr,\r\nchar *page)\r\n{\r\nssize_t rv;\r\nmutex_lock(&cache_mutex);\r\nrv = snprintf(page, PAGE_SIZE-1, "%u\n", __intel_cqm_max_threshold);\r\nmutex_unlock(&cache_mutex);\r\nreturn rv;\r\n}\r\nstatic ssize_t\r\nmax_recycle_threshold_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int bytes, cachelines;\r\nint ret;\r\nret = kstrtouint(buf, 0, &bytes);\r\nif (ret)\r\nreturn ret;\r\nmutex_lock(&cache_mutex);\r\n__intel_cqm_max_threshold = bytes;\r\ncachelines = bytes / cqm_l3_scale;\r\nif (__intel_cqm_threshold > cachelines)\r\n__intel_cqm_threshold = cachelines;\r\nmutex_unlock(&cache_mutex);\r\nreturn count;\r\n}\r\nstatic inline void cqm_pick_event_reader(int cpu)\r\n{\r\nint phys_id = topology_physical_package_id(cpu);\r\nint i;\r\nfor_each_cpu(i, &cqm_cpumask) {\r\nif (phys_id == topology_physical_package_id(i))\r\nreturn;\r\n}\r\ncpumask_set_cpu(cpu, &cqm_cpumask);\r\n}\r\nstatic void intel_cqm_cpu_starting(unsigned int cpu)\r\n{\r\nstruct intel_pqr_state *state = &per_cpu(pqr_state, cpu);\r\nstruct cpuinfo_x86 *c = &cpu_data(cpu);\r\nstate->rmid = 0;\r\nstate->closid = 0;\r\nstate->rmid_usecnt = 0;\r\nWARN_ON(c->x86_cache_max_rmid != cqm_max_rmid);\r\nWARN_ON(c->x86_cache_occ_scale != cqm_l3_scale);\r\n}\r\nstatic void intel_cqm_cpu_exit(unsigned int cpu)\r\n{\r\nint phys_id = topology_physical_package_id(cpu);\r\nint i;\r\nif (!cpumask_test_and_clear_cpu(cpu, &cqm_cpumask))\r\nreturn;\r\nfor_each_online_cpu(i) {\r\nif (i == cpu)\r\ncontinue;\r\nif (phys_id == topology_physical_package_id(i)) {\r\ncpumask_set_cpu(i, &cqm_cpumask);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic int intel_cqm_cpu_notifier(struct notifier_block *nb,\r\nunsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (unsigned long)hcpu;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_DOWN_PREPARE:\r\nintel_cqm_cpu_exit(cpu);\r\nbreak;\r\ncase CPU_STARTING:\r\nintel_cqm_cpu_starting(cpu);\r\ncqm_pick_event_reader(cpu);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int __init intel_cqm_init(void)\r\n{\r\nchar *str, scale[20];\r\nint i, cpu, ret;\r\nif (!x86_match_cpu(intel_cqm_match))\r\nreturn -ENODEV;\r\ncqm_l3_scale = boot_cpu_data.x86_cache_occ_scale;\r\ncpu_notifier_register_begin();\r\nfor_each_online_cpu(cpu) {\r\nstruct cpuinfo_x86 *c = &cpu_data(cpu);\r\nif (c->x86_cache_max_rmid < cqm_max_rmid)\r\ncqm_max_rmid = c->x86_cache_max_rmid;\r\nif (c->x86_cache_occ_scale != cqm_l3_scale) {\r\npr_err("Multiple LLC scale values, disabling\n");\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\n__intel_cqm_max_threshold =\r\nboot_cpu_data.x86_cache_size * 1024 / (cqm_max_rmid + 1);\r\nsnprintf(scale, sizeof(scale), "%u", cqm_l3_scale);\r\nstr = kstrdup(scale, GFP_KERNEL);\r\nif (!str) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nevent_attr_intel_cqm_llc_scale.event_str = str;\r\nret = intel_cqm_setup_rmid_cache();\r\nif (ret)\r\ngoto out;\r\nfor_each_online_cpu(i) {\r\nintel_cqm_cpu_starting(i);\r\ncqm_pick_event_reader(i);\r\n}\r\n__perf_cpu_notifier(intel_cqm_cpu_notifier);\r\nret = perf_pmu_register(&intel_cqm_pmu, "intel_cqm", -1);\r\nif (ret)\r\npr_err("Intel CQM perf registration failed: %d\n", ret);\r\nelse\r\npr_info("Intel CQM monitoring enabled\n");\r\nout:\r\ncpu_notifier_register_done();\r\nreturn ret;\r\n}
