static region_t dm_rh_sector_to_region(struct dm_region_hash *rh, sector_t sector)\r\n{\r\nreturn sector >> rh->region_shift;\r\n}\r\nsector_t dm_rh_region_to_sector(struct dm_region_hash *rh, region_t region)\r\n{\r\nreturn region << rh->region_shift;\r\n}\r\nregion_t dm_rh_bio_to_region(struct dm_region_hash *rh, struct bio *bio)\r\n{\r\nreturn dm_rh_sector_to_region(rh, bio->bi_iter.bi_sector -\r\nrh->target_begin);\r\n}\r\nvoid *dm_rh_region_context(struct dm_region *reg)\r\n{\r\nreturn reg->rh->context;\r\n}\r\nregion_t dm_rh_get_region_key(struct dm_region *reg)\r\n{\r\nreturn reg->key;\r\n}\r\nsector_t dm_rh_get_region_size(struct dm_region_hash *rh)\r\n{\r\nreturn rh->region_size;\r\n}\r\nstruct dm_region_hash *dm_region_hash_create(\r\nvoid *context, void (*dispatch_bios)(void *context,\r\nstruct bio_list *bios),\r\nvoid (*wakeup_workers)(void *context),\r\nvoid (*wakeup_all_recovery_waiters)(void *context),\r\nsector_t target_begin, unsigned max_recovery,\r\nstruct dm_dirty_log *log, uint32_t region_size,\r\nregion_t nr_regions)\r\n{\r\nstruct dm_region_hash *rh;\r\nunsigned nr_buckets, max_buckets;\r\nsize_t i;\r\nmax_buckets = nr_regions >> 6;\r\nfor (nr_buckets = 128u; nr_buckets < max_buckets; nr_buckets <<= 1)\r\n;\r\nnr_buckets >>= 1;\r\nrh = kmalloc(sizeof(*rh), GFP_KERNEL);\r\nif (!rh) {\r\nDMERR("unable to allocate region hash memory");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nrh->context = context;\r\nrh->dispatch_bios = dispatch_bios;\r\nrh->wakeup_workers = wakeup_workers;\r\nrh->wakeup_all_recovery_waiters = wakeup_all_recovery_waiters;\r\nrh->target_begin = target_begin;\r\nrh->max_recovery = max_recovery;\r\nrh->log = log;\r\nrh->region_size = region_size;\r\nrh->region_shift = __ffs(region_size);\r\nrwlock_init(&rh->hash_lock);\r\nrh->mask = nr_buckets - 1;\r\nrh->nr_buckets = nr_buckets;\r\nrh->shift = RH_HASH_SHIFT;\r\nrh->prime = RH_HASH_MULT;\r\nrh->buckets = vmalloc(nr_buckets * sizeof(*rh->buckets));\r\nif (!rh->buckets) {\r\nDMERR("unable to allocate region hash bucket memory");\r\nkfree(rh);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nfor (i = 0; i < nr_buckets; i++)\r\nINIT_LIST_HEAD(rh->buckets + i);\r\nspin_lock_init(&rh->region_lock);\r\nsema_init(&rh->recovery_count, 0);\r\natomic_set(&rh->recovery_in_flight, 0);\r\nINIT_LIST_HEAD(&rh->clean_regions);\r\nINIT_LIST_HEAD(&rh->quiesced_regions);\r\nINIT_LIST_HEAD(&rh->recovered_regions);\r\nINIT_LIST_HEAD(&rh->failed_recovered_regions);\r\nrh->flush_failure = 0;\r\nrh->region_pool = mempool_create_kmalloc_pool(MIN_REGIONS,\r\nsizeof(struct dm_region));\r\nif (!rh->region_pool) {\r\nvfree(rh->buckets);\r\nkfree(rh);\r\nrh = ERR_PTR(-ENOMEM);\r\n}\r\nreturn rh;\r\n}\r\nvoid dm_region_hash_destroy(struct dm_region_hash *rh)\r\n{\r\nunsigned h;\r\nstruct dm_region *reg, *nreg;\r\nBUG_ON(!list_empty(&rh->quiesced_regions));\r\nfor (h = 0; h < rh->nr_buckets; h++) {\r\nlist_for_each_entry_safe(reg, nreg, rh->buckets + h,\r\nhash_list) {\r\nBUG_ON(atomic_read(&reg->pending));\r\nmempool_free(reg, rh->region_pool);\r\n}\r\n}\r\nif (rh->log)\r\ndm_dirty_log_destroy(rh->log);\r\nmempool_destroy(rh->region_pool);\r\nvfree(rh->buckets);\r\nkfree(rh);\r\n}\r\nstruct dm_dirty_log *dm_rh_dirty_log(struct dm_region_hash *rh)\r\n{\r\nreturn rh->log;\r\n}\r\nstatic unsigned rh_hash(struct dm_region_hash *rh, region_t region)\r\n{\r\nreturn (unsigned) ((region * rh->prime) >> rh->shift) & rh->mask;\r\n}\r\nstatic struct dm_region *__rh_lookup(struct dm_region_hash *rh, region_t region)\r\n{\r\nstruct dm_region *reg;\r\nstruct list_head *bucket = rh->buckets + rh_hash(rh, region);\r\nlist_for_each_entry(reg, bucket, hash_list)\r\nif (reg->key == region)\r\nreturn reg;\r\nreturn NULL;\r\n}\r\nstatic void __rh_insert(struct dm_region_hash *rh, struct dm_region *reg)\r\n{\r\nlist_add(&reg->hash_list, rh->buckets + rh_hash(rh, reg->key));\r\n}\r\nstatic struct dm_region *__rh_alloc(struct dm_region_hash *rh, region_t region)\r\n{\r\nstruct dm_region *reg, *nreg;\r\nnreg = mempool_alloc(rh->region_pool, GFP_ATOMIC);\r\nif (unlikely(!nreg))\r\nnreg = kmalloc(sizeof(*nreg), GFP_NOIO | __GFP_NOFAIL);\r\nnreg->state = rh->log->type->in_sync(rh->log, region, 1) ?\r\nDM_RH_CLEAN : DM_RH_NOSYNC;\r\nnreg->rh = rh;\r\nnreg->key = region;\r\nINIT_LIST_HEAD(&nreg->list);\r\natomic_set(&nreg->pending, 0);\r\nbio_list_init(&nreg->delayed_bios);\r\nwrite_lock_irq(&rh->hash_lock);\r\nreg = __rh_lookup(rh, region);\r\nif (reg)\r\nmempool_free(nreg, rh->region_pool);\r\nelse {\r\n__rh_insert(rh, nreg);\r\nif (nreg->state == DM_RH_CLEAN) {\r\nspin_lock(&rh->region_lock);\r\nlist_add(&nreg->list, &rh->clean_regions);\r\nspin_unlock(&rh->region_lock);\r\n}\r\nreg = nreg;\r\n}\r\nwrite_unlock_irq(&rh->hash_lock);\r\nreturn reg;\r\n}\r\nstatic struct dm_region *__rh_find(struct dm_region_hash *rh, region_t region)\r\n{\r\nstruct dm_region *reg;\r\nreg = __rh_lookup(rh, region);\r\nif (!reg) {\r\nread_unlock(&rh->hash_lock);\r\nreg = __rh_alloc(rh, region);\r\nread_lock(&rh->hash_lock);\r\n}\r\nreturn reg;\r\n}\r\nint dm_rh_get_state(struct dm_region_hash *rh, region_t region, int may_block)\r\n{\r\nint r;\r\nstruct dm_region *reg;\r\nread_lock(&rh->hash_lock);\r\nreg = __rh_lookup(rh, region);\r\nread_unlock(&rh->hash_lock);\r\nif (reg)\r\nreturn reg->state;\r\nr = rh->log->type->in_sync(rh->log, region, may_block);\r\nreturn r == 1 ? DM_RH_CLEAN : DM_RH_NOSYNC;\r\n}\r\nstatic void complete_resync_work(struct dm_region *reg, int success)\r\n{\r\nstruct dm_region_hash *rh = reg->rh;\r\nrh->log->type->set_region_sync(rh->log, reg->key, success);\r\nrh->dispatch_bios(rh->context, &reg->delayed_bios);\r\nif (atomic_dec_and_test(&rh->recovery_in_flight))\r\nrh->wakeup_all_recovery_waiters(rh->context);\r\nup(&rh->recovery_count);\r\n}\r\nvoid dm_rh_mark_nosync(struct dm_region_hash *rh, struct bio *bio)\r\n{\r\nunsigned long flags;\r\nstruct dm_dirty_log *log = rh->log;\r\nstruct dm_region *reg;\r\nregion_t region = dm_rh_bio_to_region(rh, bio);\r\nint recovering = 0;\r\nif (bio->bi_rw & REQ_FLUSH) {\r\nrh->flush_failure = 1;\r\nreturn;\r\n}\r\nif (bio->bi_rw & REQ_DISCARD)\r\nreturn;\r\nlog->type->set_region_sync(log, region, 0);\r\nread_lock(&rh->hash_lock);\r\nreg = __rh_find(rh, region);\r\nread_unlock(&rh->hash_lock);\r\nBUG_ON(!reg);\r\nBUG_ON(!list_empty(&reg->list));\r\nspin_lock_irqsave(&rh->region_lock, flags);\r\nrecovering = (reg->state == DM_RH_RECOVERING);\r\nreg->state = DM_RH_NOSYNC;\r\nBUG_ON(!list_empty(&reg->list));\r\nspin_unlock_irqrestore(&rh->region_lock, flags);\r\nif (recovering)\r\ncomplete_resync_work(reg, 0);\r\n}\r\nvoid dm_rh_update_states(struct dm_region_hash *rh, int errors_handled)\r\n{\r\nstruct dm_region *reg, *next;\r\nLIST_HEAD(clean);\r\nLIST_HEAD(recovered);\r\nLIST_HEAD(failed_recovered);\r\nwrite_lock_irq(&rh->hash_lock);\r\nspin_lock(&rh->region_lock);\r\nif (!list_empty(&rh->clean_regions)) {\r\nlist_splice_init(&rh->clean_regions, &clean);\r\nlist_for_each_entry(reg, &clean, list)\r\nlist_del(&reg->hash_list);\r\n}\r\nif (!list_empty(&rh->recovered_regions)) {\r\nlist_splice_init(&rh->recovered_regions, &recovered);\r\nlist_for_each_entry(reg, &recovered, list)\r\nlist_del(&reg->hash_list);\r\n}\r\nif (!list_empty(&rh->failed_recovered_regions)) {\r\nlist_splice_init(&rh->failed_recovered_regions,\r\n&failed_recovered);\r\nlist_for_each_entry(reg, &failed_recovered, list)\r\nlist_del(&reg->hash_list);\r\n}\r\nspin_unlock(&rh->region_lock);\r\nwrite_unlock_irq(&rh->hash_lock);\r\nlist_for_each_entry_safe(reg, next, &recovered, list) {\r\nrh->log->type->clear_region(rh->log, reg->key);\r\ncomplete_resync_work(reg, 1);\r\nmempool_free(reg, rh->region_pool);\r\n}\r\nlist_for_each_entry_safe(reg, next, &failed_recovered, list) {\r\ncomplete_resync_work(reg, errors_handled ? 0 : 1);\r\nmempool_free(reg, rh->region_pool);\r\n}\r\nlist_for_each_entry_safe(reg, next, &clean, list) {\r\nrh->log->type->clear_region(rh->log, reg->key);\r\nmempool_free(reg, rh->region_pool);\r\n}\r\nrh->log->type->flush(rh->log);\r\n}\r\nstatic void rh_inc(struct dm_region_hash *rh, region_t region)\r\n{\r\nstruct dm_region *reg;\r\nread_lock(&rh->hash_lock);\r\nreg = __rh_find(rh, region);\r\nspin_lock_irq(&rh->region_lock);\r\natomic_inc(&reg->pending);\r\nif (reg->state == DM_RH_CLEAN) {\r\nreg->state = DM_RH_DIRTY;\r\nlist_del_init(&reg->list);\r\nspin_unlock_irq(&rh->region_lock);\r\nrh->log->type->mark_region(rh->log, reg->key);\r\n} else\r\nspin_unlock_irq(&rh->region_lock);\r\nread_unlock(&rh->hash_lock);\r\n}\r\nvoid dm_rh_inc_pending(struct dm_region_hash *rh, struct bio_list *bios)\r\n{\r\nstruct bio *bio;\r\nfor (bio = bios->head; bio; bio = bio->bi_next) {\r\nif (bio->bi_rw & (REQ_FLUSH | REQ_DISCARD))\r\ncontinue;\r\nrh_inc(rh, dm_rh_bio_to_region(rh, bio));\r\n}\r\n}\r\nvoid dm_rh_dec(struct dm_region_hash *rh, region_t region)\r\n{\r\nunsigned long flags;\r\nstruct dm_region *reg;\r\nint should_wake = 0;\r\nread_lock(&rh->hash_lock);\r\nreg = __rh_lookup(rh, region);\r\nread_unlock(&rh->hash_lock);\r\nspin_lock_irqsave(&rh->region_lock, flags);\r\nif (atomic_dec_and_test(&reg->pending)) {\r\nif (unlikely(rh->flush_failure)) {\r\nreg->state = DM_RH_NOSYNC;\r\n} else if (reg->state == DM_RH_RECOVERING) {\r\nlist_add_tail(&reg->list, &rh->quiesced_regions);\r\n} else if (reg->state == DM_RH_DIRTY) {\r\nreg->state = DM_RH_CLEAN;\r\nlist_add(&reg->list, &rh->clean_regions);\r\n}\r\nshould_wake = 1;\r\n}\r\nspin_unlock_irqrestore(&rh->region_lock, flags);\r\nif (should_wake)\r\nrh->wakeup_workers(rh->context);\r\n}\r\nstatic int __rh_recovery_prepare(struct dm_region_hash *rh)\r\n{\r\nint r;\r\nregion_t region;\r\nstruct dm_region *reg;\r\nr = rh->log->type->get_resync_work(rh->log, &region);\r\nif (r <= 0)\r\nreturn r;\r\nread_lock(&rh->hash_lock);\r\nreg = __rh_find(rh, region);\r\nread_unlock(&rh->hash_lock);\r\nspin_lock_irq(&rh->region_lock);\r\nreg->state = DM_RH_RECOVERING;\r\nif (atomic_read(&reg->pending))\r\nlist_del_init(&reg->list);\r\nelse\r\nlist_move(&reg->list, &rh->quiesced_regions);\r\nspin_unlock_irq(&rh->region_lock);\r\nreturn 1;\r\n}\r\nvoid dm_rh_recovery_prepare(struct dm_region_hash *rh)\r\n{\r\natomic_inc(&rh->recovery_in_flight);\r\nwhile (!down_trylock(&rh->recovery_count)) {\r\natomic_inc(&rh->recovery_in_flight);\r\nif (__rh_recovery_prepare(rh) <= 0) {\r\natomic_dec(&rh->recovery_in_flight);\r\nup(&rh->recovery_count);\r\nbreak;\r\n}\r\n}\r\nif (atomic_dec_and_test(&rh->recovery_in_flight))\r\nrh->wakeup_all_recovery_waiters(rh->context);\r\n}\r\nstruct dm_region *dm_rh_recovery_start(struct dm_region_hash *rh)\r\n{\r\nstruct dm_region *reg = NULL;\r\nspin_lock_irq(&rh->region_lock);\r\nif (!list_empty(&rh->quiesced_regions)) {\r\nreg = list_entry(rh->quiesced_regions.next,\r\nstruct dm_region, list);\r\nlist_del_init(&reg->list);\r\n}\r\nspin_unlock_irq(&rh->region_lock);\r\nreturn reg;\r\n}\r\nvoid dm_rh_recovery_end(struct dm_region *reg, int success)\r\n{\r\nstruct dm_region_hash *rh = reg->rh;\r\nspin_lock_irq(&rh->region_lock);\r\nif (success)\r\nlist_add(&reg->list, &reg->rh->recovered_regions);\r\nelse\r\nlist_add(&reg->list, &reg->rh->failed_recovered_regions);\r\nspin_unlock_irq(&rh->region_lock);\r\nrh->wakeup_workers(rh->context);\r\n}\r\nint dm_rh_recovery_in_flight(struct dm_region_hash *rh)\r\n{\r\nreturn atomic_read(&rh->recovery_in_flight);\r\n}\r\nint dm_rh_flush(struct dm_region_hash *rh)\r\n{\r\nreturn rh->log->type->flush(rh->log);\r\n}\r\nvoid dm_rh_delay(struct dm_region_hash *rh, struct bio *bio)\r\n{\r\nstruct dm_region *reg;\r\nread_lock(&rh->hash_lock);\r\nreg = __rh_find(rh, dm_rh_bio_to_region(rh, bio));\r\nbio_list_add(&reg->delayed_bios, bio);\r\nread_unlock(&rh->hash_lock);\r\n}\r\nvoid dm_rh_stop_recovery(struct dm_region_hash *rh)\r\n{\r\nint i;\r\nfor (i = 0; i < rh->max_recovery; i++)\r\ndown(&rh->recovery_count);\r\n}\r\nvoid dm_rh_start_recovery(struct dm_region_hash *rh)\r\n{\r\nint i;\r\nfor (i = 0; i < rh->max_recovery; i++)\r\nup(&rh->recovery_count);\r\nrh->wakeup_workers(rh->context);\r\n}
