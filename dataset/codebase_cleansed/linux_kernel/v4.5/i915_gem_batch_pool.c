void i915_gem_batch_pool_init(struct drm_device *dev,\r\nstruct i915_gem_batch_pool *pool)\r\n{\r\nint n;\r\npool->dev = dev;\r\nfor (n = 0; n < ARRAY_SIZE(pool->cache_list); n++)\r\nINIT_LIST_HEAD(&pool->cache_list[n]);\r\n}\r\nvoid i915_gem_batch_pool_fini(struct i915_gem_batch_pool *pool)\r\n{\r\nint n;\r\nWARN_ON(!mutex_is_locked(&pool->dev->struct_mutex));\r\nfor (n = 0; n < ARRAY_SIZE(pool->cache_list); n++) {\r\nwhile (!list_empty(&pool->cache_list[n])) {\r\nstruct drm_i915_gem_object *obj =\r\nlist_first_entry(&pool->cache_list[n],\r\nstruct drm_i915_gem_object,\r\nbatch_pool_link);\r\nlist_del(&obj->batch_pool_link);\r\ndrm_gem_object_unreference(&obj->base);\r\n}\r\n}\r\n}\r\nstruct drm_i915_gem_object *\r\ni915_gem_batch_pool_get(struct i915_gem_batch_pool *pool,\r\nsize_t size)\r\n{\r\nstruct drm_i915_gem_object *obj = NULL;\r\nstruct drm_i915_gem_object *tmp, *next;\r\nstruct list_head *list;\r\nint n;\r\nWARN_ON(!mutex_is_locked(&pool->dev->struct_mutex));\r\nn = fls(size >> PAGE_SHIFT) - 1;\r\nif (n >= ARRAY_SIZE(pool->cache_list))\r\nn = ARRAY_SIZE(pool->cache_list) - 1;\r\nlist = &pool->cache_list[n];\r\nlist_for_each_entry_safe(tmp, next, list, batch_pool_link) {\r\nif (tmp->active)\r\nbreak;\r\nif (tmp->madv == __I915_MADV_PURGED) {\r\nlist_del(&tmp->batch_pool_link);\r\ndrm_gem_object_unreference(&tmp->base);\r\ncontinue;\r\n}\r\nif (tmp->base.size >= size) {\r\nobj = tmp;\r\nbreak;\r\n}\r\n}\r\nif (obj == NULL) {\r\nint ret;\r\nobj = i915_gem_alloc_object(pool->dev, size);\r\nif (obj == NULL)\r\nreturn ERR_PTR(-ENOMEM);\r\nret = i915_gem_object_get_pages(obj);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nobj->madv = I915_MADV_DONTNEED;\r\n}\r\nlist_move_tail(&obj->batch_pool_link, list);\r\ni915_gem_object_pin_pages(obj);\r\nreturn obj;\r\n}
