static inline struct fw_ohci *fw_ohci(struct fw_card *card)\r\n{\r\nreturn container_of(card, struct fw_ohci, card);\r\n}\r\nstatic void log_irqs(struct fw_ohci *ohci, u32 evt)\r\n{\r\nif (likely(!(param_debug &\r\n(OHCI_PARAM_DEBUG_IRQS | OHCI_PARAM_DEBUG_BUSRESETS))))\r\nreturn;\r\nif (!(param_debug & OHCI_PARAM_DEBUG_IRQS) &&\r\n!(evt & OHCI1394_busReset))\r\nreturn;\r\nohci_notice(ohci, "IRQ %08x%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\n", evt,\r\nevt & OHCI1394_selfIDComplete ? " selfID" : "",\r\nevt & OHCI1394_RQPkt ? " AR_req" : "",\r\nevt & OHCI1394_RSPkt ? " AR_resp" : "",\r\nevt & OHCI1394_reqTxComplete ? " AT_req" : "",\r\nevt & OHCI1394_respTxComplete ? " AT_resp" : "",\r\nevt & OHCI1394_isochRx ? " IR" : "",\r\nevt & OHCI1394_isochTx ? " IT" : "",\r\nevt & OHCI1394_postedWriteErr ? " postedWriteErr" : "",\r\nevt & OHCI1394_cycleTooLong ? " cycleTooLong" : "",\r\nevt & OHCI1394_cycle64Seconds ? " cycle64Seconds" : "",\r\nevt & OHCI1394_cycleInconsistent ? " cycleInconsistent" : "",\r\nevt & OHCI1394_regAccessFail ? " regAccessFail" : "",\r\nevt & OHCI1394_unrecoverableError ? " unrecoverableError" : "",\r\nevt & OHCI1394_busReset ? " busReset" : "",\r\nevt & ~(OHCI1394_selfIDComplete | OHCI1394_RQPkt |\r\nOHCI1394_RSPkt | OHCI1394_reqTxComplete |\r\nOHCI1394_respTxComplete | OHCI1394_isochRx |\r\nOHCI1394_isochTx | OHCI1394_postedWriteErr |\r\nOHCI1394_cycleTooLong | OHCI1394_cycle64Seconds |\r\nOHCI1394_cycleInconsistent |\r\nOHCI1394_regAccessFail | OHCI1394_busReset)\r\n? " ?" : "");\r\n}\r\nstatic char _p(u32 *s, int shift)\r\n{\r\nreturn port[*s >> shift & 3];\r\n}\r\nstatic void log_selfids(struct fw_ohci *ohci, int generation, int self_id_count)\r\n{\r\nu32 *s;\r\nif (likely(!(param_debug & OHCI_PARAM_DEBUG_SELFIDS)))\r\nreturn;\r\nohci_notice(ohci, "%d selfIDs, generation %d, local node ID %04x\n",\r\nself_id_count, generation, ohci->node_id);\r\nfor (s = ohci->self_id_buffer; self_id_count--; ++s)\r\nif ((*s & 1 << 23) == 0)\r\nohci_notice(ohci,\r\n"selfID 0: %08x, phy %d [%c%c%c] %s gc=%d %s %s%s%s\n",\r\n*s, *s >> 24 & 63, _p(s, 6), _p(s, 4), _p(s, 2),\r\nspeed[*s >> 14 & 3], *s >> 16 & 63,\r\npower[*s >> 8 & 7], *s >> 22 & 1 ? "L" : "",\r\n*s >> 11 & 1 ? "c" : "", *s & 2 ? "i" : "");\r\nelse\r\nohci_notice(ohci,\r\n"selfID n: %08x, phy %d [%c%c%c%c%c%c%c%c]\n",\r\n*s, *s >> 24 & 63,\r\n_p(s, 16), _p(s, 14), _p(s, 12), _p(s, 10),\r\n_p(s, 8), _p(s, 6), _p(s, 4), _p(s, 2));\r\n}\r\nstatic void log_ar_at_event(struct fw_ohci *ohci,\r\nchar dir, int speed, u32 *header, int evt)\r\n{\r\nint tcode = header[0] >> 4 & 0xf;\r\nchar specific[12];\r\nif (likely(!(param_debug & OHCI_PARAM_DEBUG_AT_AR)))\r\nreturn;\r\nif (unlikely(evt >= ARRAY_SIZE(evts)))\r\nevt = 0x1f;\r\nif (evt == OHCI1394_evt_bus_reset) {\r\nohci_notice(ohci, "A%c evt_bus_reset, generation %d\n",\r\ndir, (header[2] >> 16) & 0xff);\r\nreturn;\r\n}\r\nswitch (tcode) {\r\ncase 0x0: case 0x6: case 0x8:\r\nsnprintf(specific, sizeof(specific), " = %08x",\r\nbe32_to_cpu((__force __be32)header[3]));\r\nbreak;\r\ncase 0x1: case 0x5: case 0x7: case 0x9: case 0xb:\r\nsnprintf(specific, sizeof(specific), " %x,%x",\r\nheader[3] >> 16, header[3] & 0xffff);\r\nbreak;\r\ndefault:\r\nspecific[0] = '\0';\r\n}\r\nswitch (tcode) {\r\ncase 0xa:\r\nohci_notice(ohci, "A%c %s, %s\n",\r\ndir, evts[evt], tcodes[tcode]);\r\nbreak;\r\ncase 0xe:\r\nohci_notice(ohci, "A%c %s, PHY %08x %08x\n",\r\ndir, evts[evt], header[1], header[2]);\r\nbreak;\r\ncase 0x0: case 0x1: case 0x4: case 0x5: case 0x9:\r\nohci_notice(ohci,\r\n"A%c spd %x tl %02x, %04x -> %04x, %s, %s, %04x%08x%s\n",\r\ndir, speed, header[0] >> 10 & 0x3f,\r\nheader[1] >> 16, header[0] >> 16, evts[evt],\r\ntcodes[tcode], header[1] & 0xffff, header[2], specific);\r\nbreak;\r\ndefault:\r\nohci_notice(ohci,\r\n"A%c spd %x tl %02x, %04x -> %04x, %s, %s%s\n",\r\ndir, speed, header[0] >> 10 & 0x3f,\r\nheader[1] >> 16, header[0] >> 16, evts[evt],\r\ntcodes[tcode], specific);\r\n}\r\n}\r\nstatic inline void reg_write(const struct fw_ohci *ohci, int offset, u32 data)\r\n{\r\nwritel(data, ohci->registers + offset);\r\n}\r\nstatic inline u32 reg_read(const struct fw_ohci *ohci, int offset)\r\n{\r\nreturn readl(ohci->registers + offset);\r\n}\r\nstatic inline void flush_writes(const struct fw_ohci *ohci)\r\n{\r\nreg_read(ohci, OHCI1394_Version);\r\n}\r\nstatic int read_phy_reg(struct fw_ohci *ohci, int addr)\r\n{\r\nu32 val;\r\nint i;\r\nreg_write(ohci, OHCI1394_PhyControl, OHCI1394_PhyControl_Read(addr));\r\nfor (i = 0; i < 3 + 100; i++) {\r\nval = reg_read(ohci, OHCI1394_PhyControl);\r\nif (!~val)\r\nreturn -ENODEV;\r\nif (val & OHCI1394_PhyControl_ReadDone)\r\nreturn OHCI1394_PhyControl_ReadData(val);\r\nif (i >= 3)\r\nmsleep(1);\r\n}\r\nohci_err(ohci, "failed to read phy reg %d\n", addr);\r\ndump_stack();\r\nreturn -EBUSY;\r\n}\r\nstatic int write_phy_reg(const struct fw_ohci *ohci, int addr, u32 val)\r\n{\r\nint i;\r\nreg_write(ohci, OHCI1394_PhyControl,\r\nOHCI1394_PhyControl_Write(addr, val));\r\nfor (i = 0; i < 3 + 100; i++) {\r\nval = reg_read(ohci, OHCI1394_PhyControl);\r\nif (!~val)\r\nreturn -ENODEV;\r\nif (!(val & OHCI1394_PhyControl_WritePending))\r\nreturn 0;\r\nif (i >= 3)\r\nmsleep(1);\r\n}\r\nohci_err(ohci, "failed to write phy reg %d, val %u\n", addr, val);\r\ndump_stack();\r\nreturn -EBUSY;\r\n}\r\nstatic int update_phy_reg(struct fw_ohci *ohci, int addr,\r\nint clear_bits, int set_bits)\r\n{\r\nint ret = read_phy_reg(ohci, addr);\r\nif (ret < 0)\r\nreturn ret;\r\nif (addr == 5)\r\nclear_bits |= PHY_INT_STATUS_BITS;\r\nreturn write_phy_reg(ohci, addr, (ret & ~clear_bits) | set_bits);\r\n}\r\nstatic int read_paged_phy_reg(struct fw_ohci *ohci, int page, int addr)\r\n{\r\nint ret;\r\nret = update_phy_reg(ohci, 7, PHY_PAGE_SELECT, page << 5);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn read_phy_reg(ohci, addr);\r\n}\r\nstatic int ohci_read_phy_reg(struct fw_card *card, int addr)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nint ret;\r\nmutex_lock(&ohci->phy_reg_mutex);\r\nret = read_phy_reg(ohci, addr);\r\nmutex_unlock(&ohci->phy_reg_mutex);\r\nreturn ret;\r\n}\r\nstatic int ohci_update_phy_reg(struct fw_card *card, int addr,\r\nint clear_bits, int set_bits)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nint ret;\r\nmutex_lock(&ohci->phy_reg_mutex);\r\nret = update_phy_reg(ohci, addr, clear_bits, set_bits);\r\nmutex_unlock(&ohci->phy_reg_mutex);\r\nreturn ret;\r\n}\r\nstatic inline dma_addr_t ar_buffer_bus(struct ar_context *ctx, unsigned int i)\r\n{\r\nreturn page_private(ctx->pages[i]);\r\n}\r\nstatic void ar_context_link_page(struct ar_context *ctx, unsigned int index)\r\n{\r\nstruct descriptor *d;\r\nd = &ctx->descriptors[index];\r\nd->branch_address &= cpu_to_le32(~0xf);\r\nd->res_count = cpu_to_le16(PAGE_SIZE);\r\nd->transfer_status = 0;\r\nwmb();\r\nd = &ctx->descriptors[ctx->last_buffer_index];\r\nd->branch_address |= cpu_to_le32(1);\r\nctx->last_buffer_index = index;\r\nreg_write(ctx->ohci, CONTROL_SET(ctx->regs), CONTEXT_WAKE);\r\n}\r\nstatic void ar_context_release(struct ar_context *ctx)\r\n{\r\nunsigned int i;\r\nvunmap(ctx->buffer);\r\nfor (i = 0; i < AR_BUFFERS; i++)\r\nif (ctx->pages[i]) {\r\ndma_unmap_page(ctx->ohci->card.device,\r\nar_buffer_bus(ctx, i),\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\n__free_page(ctx->pages[i]);\r\n}\r\n}\r\nstatic void ar_context_abort(struct ar_context *ctx, const char *error_msg)\r\n{\r\nstruct fw_ohci *ohci = ctx->ohci;\r\nif (reg_read(ohci, CONTROL_CLEAR(ctx->regs)) & CONTEXT_RUN) {\r\nreg_write(ohci, CONTROL_CLEAR(ctx->regs), CONTEXT_RUN);\r\nflush_writes(ohci);\r\nohci_err(ohci, "AR error: %s; DMA stopped\n", error_msg);\r\n}\r\n}\r\nstatic inline unsigned int ar_next_buffer_index(unsigned int index)\r\n{\r\nreturn (index + 1) % AR_BUFFERS;\r\n}\r\nstatic inline unsigned int ar_first_buffer_index(struct ar_context *ctx)\r\n{\r\nreturn ar_next_buffer_index(ctx->last_buffer_index);\r\n}\r\nstatic unsigned int ar_search_last_active_buffer(struct ar_context *ctx,\r\nunsigned int *buffer_offset)\r\n{\r\nunsigned int i, next_i, last = ctx->last_buffer_index;\r\n__le16 res_count, next_res_count;\r\ni = ar_first_buffer_index(ctx);\r\nres_count = ACCESS_ONCE(ctx->descriptors[i].res_count);\r\nwhile (i != last && res_count == 0) {\r\nnext_i = ar_next_buffer_index(i);\r\nrmb();\r\nnext_res_count = ACCESS_ONCE(\r\nctx->descriptors[next_i].res_count);\r\nif (next_res_count == cpu_to_le16(PAGE_SIZE)) {\r\nif (MAX_AR_PACKET_SIZE > PAGE_SIZE && i != last) {\r\nnext_i = ar_next_buffer_index(next_i);\r\nrmb();\r\nnext_res_count = ACCESS_ONCE(\r\nctx->descriptors[next_i].res_count);\r\nif (next_res_count != cpu_to_le16(PAGE_SIZE))\r\ngoto next_buffer_is_active;\r\n}\r\nbreak;\r\n}\r\nnext_buffer_is_active:\r\ni = next_i;\r\nres_count = next_res_count;\r\n}\r\nrmb();\r\n*buffer_offset = PAGE_SIZE - le16_to_cpu(res_count);\r\nif (*buffer_offset > PAGE_SIZE) {\r\n*buffer_offset = 0;\r\nar_context_abort(ctx, "corrupted descriptor");\r\n}\r\nreturn i;\r\n}\r\nstatic void ar_sync_buffers_for_cpu(struct ar_context *ctx,\r\nunsigned int end_buffer_index,\r\nunsigned int end_buffer_offset)\r\n{\r\nunsigned int i;\r\ni = ar_first_buffer_index(ctx);\r\nwhile (i != end_buffer_index) {\r\ndma_sync_single_for_cpu(ctx->ohci->card.device,\r\nar_buffer_bus(ctx, i),\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\ni = ar_next_buffer_index(i);\r\n}\r\nif (end_buffer_offset > 0)\r\ndma_sync_single_for_cpu(ctx->ohci->card.device,\r\nar_buffer_bus(ctx, i),\r\nend_buffer_offset, DMA_FROM_DEVICE);\r\n}\r\nstatic __le32 *handle_ar_packet(struct ar_context *ctx, __le32 *buffer)\r\n{\r\nstruct fw_ohci *ohci = ctx->ohci;\r\nstruct fw_packet p;\r\nu32 status, length, tcode;\r\nint evt;\r\np.header[0] = cond_le32_to_cpu(buffer[0]);\r\np.header[1] = cond_le32_to_cpu(buffer[1]);\r\np.header[2] = cond_le32_to_cpu(buffer[2]);\r\ntcode = (p.header[0] >> 4) & 0x0f;\r\nswitch (tcode) {\r\ncase TCODE_WRITE_QUADLET_REQUEST:\r\ncase TCODE_READ_QUADLET_RESPONSE:\r\np.header[3] = (__force __u32) buffer[3];\r\np.header_length = 16;\r\np.payload_length = 0;\r\nbreak;\r\ncase TCODE_READ_BLOCK_REQUEST :\r\np.header[3] = cond_le32_to_cpu(buffer[3]);\r\np.header_length = 16;\r\np.payload_length = 0;\r\nbreak;\r\ncase TCODE_WRITE_BLOCK_REQUEST:\r\ncase TCODE_READ_BLOCK_RESPONSE:\r\ncase TCODE_LOCK_REQUEST:\r\ncase TCODE_LOCK_RESPONSE:\r\np.header[3] = cond_le32_to_cpu(buffer[3]);\r\np.header_length = 16;\r\np.payload_length = p.header[3] >> 16;\r\nif (p.payload_length > MAX_ASYNC_PAYLOAD) {\r\nar_context_abort(ctx, "invalid packet length");\r\nreturn NULL;\r\n}\r\nbreak;\r\ncase TCODE_WRITE_RESPONSE:\r\ncase TCODE_READ_QUADLET_REQUEST:\r\ncase OHCI_TCODE_PHY_PACKET:\r\np.header_length = 12;\r\np.payload_length = 0;\r\nbreak;\r\ndefault:\r\nar_context_abort(ctx, "invalid tcode");\r\nreturn NULL;\r\n}\r\np.payload = (void *) buffer + p.header_length;\r\nlength = (p.header_length + p.payload_length + 3) / 4;\r\nstatus = cond_le32_to_cpu(buffer[length]);\r\nevt = (status >> 16) & 0x1f;\r\np.ack = evt - 16;\r\np.speed = (status >> 21) & 0x7;\r\np.timestamp = status & 0xffff;\r\np.generation = ohci->request_generation;\r\nlog_ar_at_event(ohci, 'R', p.speed, p.header, evt);\r\nif (evt == OHCI1394_evt_no_status &&\r\n(p.header[0] & 0xff) == (OHCI1394_phy_tcode << 4))\r\np.ack = ACK_COMPLETE;\r\nif (evt == OHCI1394_evt_bus_reset) {\r\nif (!(ohci->quirks & QUIRK_RESET_PACKET))\r\nohci->request_generation = (p.header[2] >> 16) & 0xff;\r\n} else if (ctx == &ohci->ar_request_ctx) {\r\nfw_core_handle_request(&ohci->card, &p);\r\n} else {\r\nfw_core_handle_response(&ohci->card, &p);\r\n}\r\nreturn buffer + length + 1;\r\n}\r\nstatic void *handle_ar_packets(struct ar_context *ctx, void *p, void *end)\r\n{\r\nvoid *next;\r\nwhile (p < end) {\r\nnext = handle_ar_packet(ctx, p);\r\nif (!next)\r\nreturn p;\r\np = next;\r\n}\r\nreturn p;\r\n}\r\nstatic void ar_recycle_buffers(struct ar_context *ctx, unsigned int end_buffer)\r\n{\r\nunsigned int i;\r\ni = ar_first_buffer_index(ctx);\r\nwhile (i != end_buffer) {\r\ndma_sync_single_for_device(ctx->ohci->card.device,\r\nar_buffer_bus(ctx, i),\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nar_context_link_page(ctx, i);\r\ni = ar_next_buffer_index(i);\r\n}\r\n}\r\nstatic void ar_context_tasklet(unsigned long data)\r\n{\r\nstruct ar_context *ctx = (struct ar_context *)data;\r\nunsigned int end_buffer_index, end_buffer_offset;\r\nvoid *p, *end;\r\np = ctx->pointer;\r\nif (!p)\r\nreturn;\r\nend_buffer_index = ar_search_last_active_buffer(ctx,\r\n&end_buffer_offset);\r\nar_sync_buffers_for_cpu(ctx, end_buffer_index, end_buffer_offset);\r\nend = ctx->buffer + end_buffer_index * PAGE_SIZE + end_buffer_offset;\r\nif (end_buffer_index < ar_first_buffer_index(ctx)) {\r\nvoid *buffer_end = ctx->buffer + AR_BUFFERS * PAGE_SIZE;\r\np = handle_ar_packets(ctx, p, buffer_end);\r\nif (p < buffer_end)\r\ngoto error;\r\np -= AR_BUFFERS * PAGE_SIZE;\r\n}\r\np = handle_ar_packets(ctx, p, end);\r\nif (p != end) {\r\nif (p > end)\r\nar_context_abort(ctx, "inconsistent descriptor");\r\ngoto error;\r\n}\r\nctx->pointer = p;\r\nar_recycle_buffers(ctx, end_buffer_index);\r\nreturn;\r\nerror:\r\nctx->pointer = NULL;\r\n}\r\nstatic int ar_context_init(struct ar_context *ctx, struct fw_ohci *ohci,\r\nunsigned int descriptors_offset, u32 regs)\r\n{\r\nunsigned int i;\r\ndma_addr_t dma_addr;\r\nstruct page *pages[AR_BUFFERS + AR_WRAPAROUND_PAGES];\r\nstruct descriptor *d;\r\nctx->regs = regs;\r\nctx->ohci = ohci;\r\ntasklet_init(&ctx->tasklet, ar_context_tasklet, (unsigned long)ctx);\r\nfor (i = 0; i < AR_BUFFERS; i++) {\r\nctx->pages[i] = alloc_page(GFP_KERNEL | GFP_DMA32);\r\nif (!ctx->pages[i])\r\ngoto out_of_memory;\r\ndma_addr = dma_map_page(ohci->card.device, ctx->pages[i],\r\n0, PAGE_SIZE, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(ohci->card.device, dma_addr)) {\r\n__free_page(ctx->pages[i]);\r\nctx->pages[i] = NULL;\r\ngoto out_of_memory;\r\n}\r\nset_page_private(ctx->pages[i], dma_addr);\r\n}\r\nfor (i = 0; i < AR_BUFFERS; i++)\r\npages[i] = ctx->pages[i];\r\nfor (i = 0; i < AR_WRAPAROUND_PAGES; i++)\r\npages[AR_BUFFERS + i] = ctx->pages[i];\r\nctx->buffer = vmap(pages, ARRAY_SIZE(pages), VM_MAP, PAGE_KERNEL);\r\nif (!ctx->buffer)\r\ngoto out_of_memory;\r\nctx->descriptors = ohci->misc_buffer + descriptors_offset;\r\nctx->descriptors_bus = ohci->misc_buffer_bus + descriptors_offset;\r\nfor (i = 0; i < AR_BUFFERS; i++) {\r\nd = &ctx->descriptors[i];\r\nd->req_count = cpu_to_le16(PAGE_SIZE);\r\nd->control = cpu_to_le16(DESCRIPTOR_INPUT_MORE |\r\nDESCRIPTOR_STATUS |\r\nDESCRIPTOR_BRANCH_ALWAYS);\r\nd->data_address = cpu_to_le32(ar_buffer_bus(ctx, i));\r\nd->branch_address = cpu_to_le32(ctx->descriptors_bus +\r\nar_next_buffer_index(i) * sizeof(struct descriptor));\r\n}\r\nreturn 0;\r\nout_of_memory:\r\nar_context_release(ctx);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ar_context_run(struct ar_context *ctx)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < AR_BUFFERS; i++)\r\nar_context_link_page(ctx, i);\r\nctx->pointer = ctx->buffer;\r\nreg_write(ctx->ohci, COMMAND_PTR(ctx->regs), ctx->descriptors_bus | 1);\r\nreg_write(ctx->ohci, CONTROL_SET(ctx->regs), CONTEXT_RUN);\r\n}\r\nstatic struct descriptor *find_branch_descriptor(struct descriptor *d, int z)\r\n{\r\n__le16 branch;\r\nbranch = d->control & cpu_to_le16(DESCRIPTOR_BRANCH_ALWAYS);\r\nif (z == 2 && branch == cpu_to_le16(DESCRIPTOR_BRANCH_ALWAYS))\r\nreturn d;\r\nelse\r\nreturn d + z - 1;\r\n}\r\nstatic void context_tasklet(unsigned long data)\r\n{\r\nstruct context *ctx = (struct context *) data;\r\nstruct descriptor *d, *last;\r\nu32 address;\r\nint z;\r\nstruct descriptor_buffer *desc;\r\ndesc = list_entry(ctx->buffer_list.next,\r\nstruct descriptor_buffer, list);\r\nlast = ctx->last;\r\nwhile (last->branch_address != 0) {\r\nstruct descriptor_buffer *old_desc = desc;\r\naddress = le32_to_cpu(last->branch_address);\r\nz = address & 0xf;\r\naddress &= ~0xf;\r\nctx->current_bus = address;\r\nif (address < desc->buffer_bus ||\r\naddress >= desc->buffer_bus + desc->used)\r\ndesc = list_entry(desc->list.next,\r\nstruct descriptor_buffer, list);\r\nd = desc->buffer + (address - desc->buffer_bus) / sizeof(*d);\r\nlast = find_branch_descriptor(d, z);\r\nif (!ctx->callback(ctx, d, last))\r\nbreak;\r\nif (old_desc != desc) {\r\nunsigned long flags;\r\nold_desc->used = 0;\r\nspin_lock_irqsave(&ctx->ohci->lock, flags);\r\nlist_move_tail(&old_desc->list, &ctx->buffer_list);\r\nspin_unlock_irqrestore(&ctx->ohci->lock, flags);\r\n}\r\nctx->last = last;\r\n}\r\n}\r\nstatic int context_add_buffer(struct context *ctx)\r\n{\r\nstruct descriptor_buffer *desc;\r\ndma_addr_t uninitialized_var(bus_addr);\r\nint offset;\r\nif (ctx->total_allocation >= 16*1024*1024)\r\nreturn -ENOMEM;\r\ndesc = dma_alloc_coherent(ctx->ohci->card.device, PAGE_SIZE,\r\n&bus_addr, GFP_ATOMIC);\r\nif (!desc)\r\nreturn -ENOMEM;\r\noffset = (void *)&desc->buffer - (void *)desc;\r\ndesc->buffer_size = PAGE_SIZE - offset;\r\ndesc->buffer_bus = bus_addr + offset;\r\ndesc->used = 0;\r\nlist_add_tail(&desc->list, &ctx->buffer_list);\r\nctx->total_allocation += PAGE_SIZE;\r\nreturn 0;\r\n}\r\nstatic int context_init(struct context *ctx, struct fw_ohci *ohci,\r\nu32 regs, descriptor_callback_t callback)\r\n{\r\nctx->ohci = ohci;\r\nctx->regs = regs;\r\nctx->total_allocation = 0;\r\nINIT_LIST_HEAD(&ctx->buffer_list);\r\nif (context_add_buffer(ctx) < 0)\r\nreturn -ENOMEM;\r\nctx->buffer_tail = list_entry(ctx->buffer_list.next,\r\nstruct descriptor_buffer, list);\r\ntasklet_init(&ctx->tasklet, context_tasklet, (unsigned long)ctx);\r\nctx->callback = callback;\r\nmemset(ctx->buffer_tail->buffer, 0, sizeof(*ctx->buffer_tail->buffer));\r\nctx->buffer_tail->buffer->control = cpu_to_le16(DESCRIPTOR_OUTPUT_LAST);\r\nctx->buffer_tail->buffer->transfer_status = cpu_to_le16(0x8011);\r\nctx->buffer_tail->used += sizeof(*ctx->buffer_tail->buffer);\r\nctx->last = ctx->buffer_tail->buffer;\r\nctx->prev = ctx->buffer_tail->buffer;\r\nctx->prev_z = 1;\r\nreturn 0;\r\n}\r\nstatic void context_release(struct context *ctx)\r\n{\r\nstruct fw_card *card = &ctx->ohci->card;\r\nstruct descriptor_buffer *desc, *tmp;\r\nlist_for_each_entry_safe(desc, tmp, &ctx->buffer_list, list)\r\ndma_free_coherent(card->device, PAGE_SIZE, desc,\r\ndesc->buffer_bus -\r\n((void *)&desc->buffer - (void *)desc));\r\n}\r\nstatic struct descriptor *context_get_descriptors(struct context *ctx,\r\nint z, dma_addr_t *d_bus)\r\n{\r\nstruct descriptor *d = NULL;\r\nstruct descriptor_buffer *desc = ctx->buffer_tail;\r\nif (z * sizeof(*d) > desc->buffer_size)\r\nreturn NULL;\r\nif (z * sizeof(*d) > desc->buffer_size - desc->used) {\r\nif (desc->list.next == &ctx->buffer_list) {\r\nif (context_add_buffer(ctx) < 0)\r\nreturn NULL;\r\n}\r\ndesc = list_entry(desc->list.next,\r\nstruct descriptor_buffer, list);\r\nctx->buffer_tail = desc;\r\n}\r\nd = desc->buffer + desc->used / sizeof(*d);\r\nmemset(d, 0, z * sizeof(*d));\r\n*d_bus = desc->buffer_bus + desc->used;\r\nreturn d;\r\n}\r\nstatic void context_run(struct context *ctx, u32 extra)\r\n{\r\nstruct fw_ohci *ohci = ctx->ohci;\r\nreg_write(ohci, COMMAND_PTR(ctx->regs),\r\nle32_to_cpu(ctx->last->branch_address));\r\nreg_write(ohci, CONTROL_CLEAR(ctx->regs), ~0);\r\nreg_write(ohci, CONTROL_SET(ctx->regs), CONTEXT_RUN | extra);\r\nctx->running = true;\r\nflush_writes(ohci);\r\n}\r\nstatic void context_append(struct context *ctx,\r\nstruct descriptor *d, int z, int extra)\r\n{\r\ndma_addr_t d_bus;\r\nstruct descriptor_buffer *desc = ctx->buffer_tail;\r\nstruct descriptor *d_branch;\r\nd_bus = desc->buffer_bus + (d - desc->buffer) * sizeof(*d);\r\ndesc->used += (z + extra) * sizeof(*d);\r\nwmb();\r\nd_branch = find_branch_descriptor(ctx->prev, ctx->prev_z);\r\nd_branch->branch_address = cpu_to_le32(d_bus | z);\r\nif (unlikely(ctx->ohci->quirks & QUIRK_IR_WAKE) &&\r\nd_branch != ctx->prev &&\r\n(ctx->prev->control & cpu_to_le16(DESCRIPTOR_CMD)) ==\r\ncpu_to_le16(DESCRIPTOR_INPUT_MORE)) {\r\nctx->prev->branch_address = cpu_to_le32(d_bus | z);\r\n}\r\nctx->prev = d;\r\nctx->prev_z = z;\r\n}\r\nstatic void context_stop(struct context *ctx)\r\n{\r\nstruct fw_ohci *ohci = ctx->ohci;\r\nu32 reg;\r\nint i;\r\nreg_write(ohci, CONTROL_CLEAR(ctx->regs), CONTEXT_RUN);\r\nctx->running = false;\r\nfor (i = 0; i < 1000; i++) {\r\nreg = reg_read(ohci, CONTROL_SET(ctx->regs));\r\nif ((reg & CONTEXT_ACTIVE) == 0)\r\nreturn;\r\nif (i)\r\nudelay(10);\r\n}\r\nohci_err(ohci, "DMA context still active (0x%08x)\n", reg);\r\n}\r\nstatic int at_context_queue_packet(struct context *ctx,\r\nstruct fw_packet *packet)\r\n{\r\nstruct fw_ohci *ohci = ctx->ohci;\r\ndma_addr_t d_bus, uninitialized_var(payload_bus);\r\nstruct driver_data *driver_data;\r\nstruct descriptor *d, *last;\r\n__le32 *header;\r\nint z, tcode;\r\nd = context_get_descriptors(ctx, 4, &d_bus);\r\nif (d == NULL) {\r\npacket->ack = RCODE_SEND_ERROR;\r\nreturn -1;\r\n}\r\nd[0].control = cpu_to_le16(DESCRIPTOR_KEY_IMMEDIATE);\r\nd[0].res_count = cpu_to_le16(packet->timestamp);\r\ntcode = (packet->header[0] >> 4) & 0x0f;\r\nheader = (__le32 *) &d[1];\r\nswitch (tcode) {\r\ncase TCODE_WRITE_QUADLET_REQUEST:\r\ncase TCODE_WRITE_BLOCK_REQUEST:\r\ncase TCODE_WRITE_RESPONSE:\r\ncase TCODE_READ_QUADLET_REQUEST:\r\ncase TCODE_READ_BLOCK_REQUEST:\r\ncase TCODE_READ_QUADLET_RESPONSE:\r\ncase TCODE_READ_BLOCK_RESPONSE:\r\ncase TCODE_LOCK_REQUEST:\r\ncase TCODE_LOCK_RESPONSE:\r\nheader[0] = cpu_to_le32((packet->header[0] & 0xffff) |\r\n(packet->speed << 16));\r\nheader[1] = cpu_to_le32((packet->header[1] & 0xffff) |\r\n(packet->header[0] & 0xffff0000));\r\nheader[2] = cpu_to_le32(packet->header[2]);\r\nif (TCODE_IS_BLOCK_PACKET(tcode))\r\nheader[3] = cpu_to_le32(packet->header[3]);\r\nelse\r\nheader[3] = (__force __le32) packet->header[3];\r\nd[0].req_count = cpu_to_le16(packet->header_length);\r\nbreak;\r\ncase TCODE_LINK_INTERNAL:\r\nheader[0] = cpu_to_le32((OHCI1394_phy_tcode << 4) |\r\n(packet->speed << 16));\r\nheader[1] = cpu_to_le32(packet->header[1]);\r\nheader[2] = cpu_to_le32(packet->header[2]);\r\nd[0].req_count = cpu_to_le16(12);\r\nif (is_ping_packet(&packet->header[1]))\r\nd[0].control |= cpu_to_le16(DESCRIPTOR_PING);\r\nbreak;\r\ncase TCODE_STREAM_DATA:\r\nheader[0] = cpu_to_le32((packet->header[0] & 0xffff) |\r\n(packet->speed << 16));\r\nheader[1] = cpu_to_le32(packet->header[0] & 0xffff0000);\r\nd[0].req_count = cpu_to_le16(8);\r\nbreak;\r\ndefault:\r\npacket->ack = RCODE_SEND_ERROR;\r\nreturn -1;\r\n}\r\nBUILD_BUG_ON(sizeof(struct driver_data) > sizeof(struct descriptor));\r\ndriver_data = (struct driver_data *) &d[3];\r\ndriver_data->packet = packet;\r\npacket->driver_data = driver_data;\r\nif (packet->payload_length > 0) {\r\nif (packet->payload_length > sizeof(driver_data->inline_data)) {\r\npayload_bus = dma_map_single(ohci->card.device,\r\npacket->payload,\r\npacket->payload_length,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(ohci->card.device, payload_bus)) {\r\npacket->ack = RCODE_SEND_ERROR;\r\nreturn -1;\r\n}\r\npacket->payload_bus = payload_bus;\r\npacket->payload_mapped = true;\r\n} else {\r\nmemcpy(driver_data->inline_data, packet->payload,\r\npacket->payload_length);\r\npayload_bus = d_bus + 3 * sizeof(*d);\r\n}\r\nd[2].req_count = cpu_to_le16(packet->payload_length);\r\nd[2].data_address = cpu_to_le32(payload_bus);\r\nlast = &d[2];\r\nz = 3;\r\n} else {\r\nlast = &d[0];\r\nz = 2;\r\n}\r\nlast->control |= cpu_to_le16(DESCRIPTOR_OUTPUT_LAST |\r\nDESCRIPTOR_IRQ_ALWAYS |\r\nDESCRIPTOR_BRANCH_ALWAYS);\r\nif (ohci->generation != packet->generation) {\r\nif (packet->payload_mapped)\r\ndma_unmap_single(ohci->card.device, payload_bus,\r\npacket->payload_length, DMA_TO_DEVICE);\r\npacket->ack = RCODE_GENERATION;\r\nreturn -1;\r\n}\r\ncontext_append(ctx, d, z, 4 - z);\r\nif (ctx->running)\r\nreg_write(ohci, CONTROL_SET(ctx->regs), CONTEXT_WAKE);\r\nelse\r\ncontext_run(ctx, 0);\r\nreturn 0;\r\n}\r\nstatic void at_context_flush(struct context *ctx)\r\n{\r\ntasklet_disable(&ctx->tasklet);\r\nctx->flushing = true;\r\ncontext_tasklet((unsigned long)ctx);\r\nctx->flushing = false;\r\ntasklet_enable(&ctx->tasklet);\r\n}\r\nstatic int handle_at_packet(struct context *context,\r\nstruct descriptor *d,\r\nstruct descriptor *last)\r\n{\r\nstruct driver_data *driver_data;\r\nstruct fw_packet *packet;\r\nstruct fw_ohci *ohci = context->ohci;\r\nint evt;\r\nif (last->transfer_status == 0 && !context->flushing)\r\nreturn 0;\r\ndriver_data = (struct driver_data *) &d[3];\r\npacket = driver_data->packet;\r\nif (packet == NULL)\r\nreturn 1;\r\nif (packet->payload_mapped)\r\ndma_unmap_single(ohci->card.device, packet->payload_bus,\r\npacket->payload_length, DMA_TO_DEVICE);\r\nevt = le16_to_cpu(last->transfer_status) & 0x1f;\r\npacket->timestamp = le16_to_cpu(last->res_count);\r\nlog_ar_at_event(ohci, 'T', packet->speed, packet->header, evt);\r\nswitch (evt) {\r\ncase OHCI1394_evt_timeout:\r\npacket->ack = RCODE_CANCELLED;\r\nbreak;\r\ncase OHCI1394_evt_flushed:\r\npacket->ack = RCODE_GENERATION;\r\nbreak;\r\ncase OHCI1394_evt_missing_ack:\r\nif (context->flushing)\r\npacket->ack = RCODE_GENERATION;\r\nelse {\r\npacket->ack = RCODE_NO_ACK;\r\n}\r\nbreak;\r\ncase ACK_COMPLETE + 0x10:\r\ncase ACK_PENDING + 0x10:\r\ncase ACK_BUSY_X + 0x10:\r\ncase ACK_BUSY_A + 0x10:\r\ncase ACK_BUSY_B + 0x10:\r\ncase ACK_DATA_ERROR + 0x10:\r\ncase ACK_TYPE_ERROR + 0x10:\r\npacket->ack = evt - 0x10;\r\nbreak;\r\ncase OHCI1394_evt_no_status:\r\nif (context->flushing) {\r\npacket->ack = RCODE_GENERATION;\r\nbreak;\r\n}\r\ndefault:\r\npacket->ack = RCODE_SEND_ERROR;\r\nbreak;\r\n}\r\npacket->callback(packet, &ohci->card, packet->ack);\r\nreturn 1;\r\n}\r\nstatic void handle_local_rom(struct fw_ohci *ohci,\r\nstruct fw_packet *packet, u32 csr)\r\n{\r\nstruct fw_packet response;\r\nint tcode, length, i;\r\ntcode = HEADER_GET_TCODE(packet->header[0]);\r\nif (TCODE_IS_BLOCK_PACKET(tcode))\r\nlength = HEADER_GET_DATA_LENGTH(packet->header[3]);\r\nelse\r\nlength = 4;\r\ni = csr - CSR_CONFIG_ROM;\r\nif (i + length > CONFIG_ROM_SIZE) {\r\nfw_fill_response(&response, packet->header,\r\nRCODE_ADDRESS_ERROR, NULL, 0);\r\n} else if (!TCODE_IS_READ_REQUEST(tcode)) {\r\nfw_fill_response(&response, packet->header,\r\nRCODE_TYPE_ERROR, NULL, 0);\r\n} else {\r\nfw_fill_response(&response, packet->header, RCODE_COMPLETE,\r\n(void *) ohci->config_rom + i, length);\r\n}\r\nfw_core_handle_response(&ohci->card, &response);\r\n}\r\nstatic void handle_local_lock(struct fw_ohci *ohci,\r\nstruct fw_packet *packet, u32 csr)\r\n{\r\nstruct fw_packet response;\r\nint tcode, length, ext_tcode, sel, try;\r\n__be32 *payload, lock_old;\r\nu32 lock_arg, lock_data;\r\ntcode = HEADER_GET_TCODE(packet->header[0]);\r\nlength = HEADER_GET_DATA_LENGTH(packet->header[3]);\r\npayload = packet->payload;\r\next_tcode = HEADER_GET_EXTENDED_TCODE(packet->header[3]);\r\nif (tcode == TCODE_LOCK_REQUEST &&\r\next_tcode == EXTCODE_COMPARE_SWAP && length == 8) {\r\nlock_arg = be32_to_cpu(payload[0]);\r\nlock_data = be32_to_cpu(payload[1]);\r\n} else if (tcode == TCODE_READ_QUADLET_REQUEST) {\r\nlock_arg = 0;\r\nlock_data = 0;\r\n} else {\r\nfw_fill_response(&response, packet->header,\r\nRCODE_TYPE_ERROR, NULL, 0);\r\ngoto out;\r\n}\r\nsel = (csr - CSR_BUS_MANAGER_ID) / 4;\r\nreg_write(ohci, OHCI1394_CSRData, lock_data);\r\nreg_write(ohci, OHCI1394_CSRCompareData, lock_arg);\r\nreg_write(ohci, OHCI1394_CSRControl, sel);\r\nfor (try = 0; try < 20; try++)\r\nif (reg_read(ohci, OHCI1394_CSRControl) & 0x80000000) {\r\nlock_old = cpu_to_be32(reg_read(ohci,\r\nOHCI1394_CSRData));\r\nfw_fill_response(&response, packet->header,\r\nRCODE_COMPLETE,\r\n&lock_old, sizeof(lock_old));\r\ngoto out;\r\n}\r\nohci_err(ohci, "swap not done (CSR lock timeout)\n");\r\nfw_fill_response(&response, packet->header, RCODE_BUSY, NULL, 0);\r\nout:\r\nfw_core_handle_response(&ohci->card, &response);\r\n}\r\nstatic void handle_local_request(struct context *ctx, struct fw_packet *packet)\r\n{\r\nu64 offset, csr;\r\nif (ctx == &ctx->ohci->at_request_ctx) {\r\npacket->ack = ACK_PENDING;\r\npacket->callback(packet, &ctx->ohci->card, packet->ack);\r\n}\r\noffset =\r\n((unsigned long long)\r\nHEADER_GET_OFFSET_HIGH(packet->header[1]) << 32) |\r\npacket->header[2];\r\ncsr = offset - CSR_REGISTER_BASE;\r\nif (csr >= CSR_CONFIG_ROM && csr < CSR_CONFIG_ROM_END)\r\nhandle_local_rom(ctx->ohci, packet, csr);\r\nelse switch (csr) {\r\ncase CSR_BUS_MANAGER_ID:\r\ncase CSR_BANDWIDTH_AVAILABLE:\r\ncase CSR_CHANNELS_AVAILABLE_HI:\r\ncase CSR_CHANNELS_AVAILABLE_LO:\r\nhandle_local_lock(ctx->ohci, packet, csr);\r\nbreak;\r\ndefault:\r\nif (ctx == &ctx->ohci->at_request_ctx)\r\nfw_core_handle_request(&ctx->ohci->card, packet);\r\nelse\r\nfw_core_handle_response(&ctx->ohci->card, packet);\r\nbreak;\r\n}\r\nif (ctx == &ctx->ohci->at_response_ctx) {\r\npacket->ack = ACK_COMPLETE;\r\npacket->callback(packet, &ctx->ohci->card, packet->ack);\r\n}\r\n}\r\nstatic void at_context_transmit(struct context *ctx, struct fw_packet *packet)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&ctx->ohci->lock, flags);\r\nif (HEADER_GET_DESTINATION(packet->header[0]) == ctx->ohci->node_id &&\r\nctx->ohci->generation == packet->generation) {\r\nspin_unlock_irqrestore(&ctx->ohci->lock, flags);\r\nhandle_local_request(ctx, packet);\r\nreturn;\r\n}\r\nret = at_context_queue_packet(ctx, packet);\r\nspin_unlock_irqrestore(&ctx->ohci->lock, flags);\r\nif (ret < 0)\r\npacket->callback(packet, &ctx->ohci->card, packet->ack);\r\n}\r\nstatic void detect_dead_context(struct fw_ohci *ohci,\r\nconst char *name, unsigned int regs)\r\n{\r\nu32 ctl;\r\nctl = reg_read(ohci, CONTROL_SET(regs));\r\nif (ctl & CONTEXT_DEAD)\r\nohci_err(ohci, "DMA context %s has stopped, error code: %s\n",\r\nname, evts[ctl & 0x1f]);\r\n}\r\nstatic void handle_dead_contexts(struct fw_ohci *ohci)\r\n{\r\nunsigned int i;\r\nchar name[8];\r\ndetect_dead_context(ohci, "ATReq", OHCI1394_AsReqTrContextBase);\r\ndetect_dead_context(ohci, "ATRsp", OHCI1394_AsRspTrContextBase);\r\ndetect_dead_context(ohci, "ARReq", OHCI1394_AsReqRcvContextBase);\r\ndetect_dead_context(ohci, "ARRsp", OHCI1394_AsRspRcvContextBase);\r\nfor (i = 0; i < 32; ++i) {\r\nif (!(ohci->it_context_support & (1 << i)))\r\ncontinue;\r\nsprintf(name, "IT%u", i);\r\ndetect_dead_context(ohci, name, OHCI1394_IsoXmitContextBase(i));\r\n}\r\nfor (i = 0; i < 32; ++i) {\r\nif (!(ohci->ir_context_support & (1 << i)))\r\ncontinue;\r\nsprintf(name, "IR%u", i);\r\ndetect_dead_context(ohci, name, OHCI1394_IsoRcvContextBase(i));\r\n}\r\n}\r\nstatic u32 cycle_timer_ticks(u32 cycle_timer)\r\n{\r\nu32 ticks;\r\nticks = cycle_timer & 0xfff;\r\nticks += 3072 * ((cycle_timer >> 12) & 0x1fff);\r\nticks += (3072 * 8000) * (cycle_timer >> 25);\r\nreturn ticks;\r\n}\r\nstatic u32 get_cycle_time(struct fw_ohci *ohci)\r\n{\r\nu32 c0, c1, c2;\r\nu32 t0, t1, t2;\r\ns32 diff01, diff12;\r\nint i;\r\nc2 = reg_read(ohci, OHCI1394_IsochronousCycleTimer);\r\nif (ohci->quirks & QUIRK_CYCLE_TIMER) {\r\ni = 0;\r\nc1 = c2;\r\nc2 = reg_read(ohci, OHCI1394_IsochronousCycleTimer);\r\ndo {\r\nc0 = c1;\r\nc1 = c2;\r\nc2 = reg_read(ohci, OHCI1394_IsochronousCycleTimer);\r\nt0 = cycle_timer_ticks(c0);\r\nt1 = cycle_timer_ticks(c1);\r\nt2 = cycle_timer_ticks(c2);\r\ndiff01 = t1 - t0;\r\ndiff12 = t2 - t1;\r\n} while ((diff01 <= 0 || diff12 <= 0 ||\r\ndiff01 / diff12 >= 2 || diff12 / diff01 >= 2)\r\n&& i++ < 20);\r\n}\r\nreturn c2;\r\n}\r\nstatic u32 update_bus_time(struct fw_ohci *ohci)\r\n{\r\nu32 cycle_time_seconds = get_cycle_time(ohci) >> 25;\r\nif (unlikely(!ohci->bus_time_running)) {\r\nreg_write(ohci, OHCI1394_IntMaskSet, OHCI1394_cycle64Seconds);\r\nohci->bus_time = (lower_32_bits(get_seconds()) & ~0x7f) |\r\n(cycle_time_seconds & 0x40);\r\nohci->bus_time_running = true;\r\n}\r\nif ((ohci->bus_time & 0x40) != (cycle_time_seconds & 0x40))\r\nohci->bus_time += 0x40;\r\nreturn ohci->bus_time | cycle_time_seconds;\r\n}\r\nstatic int get_status_for_port(struct fw_ohci *ohci, int port_index)\r\n{\r\nint reg;\r\nmutex_lock(&ohci->phy_reg_mutex);\r\nreg = write_phy_reg(ohci, 7, port_index);\r\nif (reg >= 0)\r\nreg = read_phy_reg(ohci, 8);\r\nmutex_unlock(&ohci->phy_reg_mutex);\r\nif (reg < 0)\r\nreturn reg;\r\nswitch (reg & 0x0f) {\r\ncase 0x06:\r\nreturn 2;\r\ncase 0x0e:\r\nreturn 3;\r\n}\r\nreturn 1;\r\n}\r\nstatic int get_self_id_pos(struct fw_ohci *ohci, u32 self_id,\r\nint self_id_count)\r\n{\r\nint i;\r\nu32 entry;\r\nfor (i = 0; i < self_id_count; i++) {\r\nentry = ohci->self_id_buffer[i];\r\nif ((self_id & 0xff000000) == (entry & 0xff000000))\r\nreturn -1;\r\nif ((self_id & 0xff000000) < (entry & 0xff000000))\r\nreturn i;\r\n}\r\nreturn i;\r\n}\r\nstatic int initiated_reset(struct fw_ohci *ohci)\r\n{\r\nint reg;\r\nint ret = 0;\r\nmutex_lock(&ohci->phy_reg_mutex);\r\nreg = write_phy_reg(ohci, 7, 0xe0);\r\nif (reg >= 0) {\r\nreg = read_phy_reg(ohci, 8);\r\nreg |= 0x40;\r\nreg = write_phy_reg(ohci, 8, reg);\r\nif (reg >= 0) {\r\nreg = read_phy_reg(ohci, 12);\r\nif (reg >= 0) {\r\nif ((reg & 0x08) == 0x08) {\r\nret = 0x2;\r\n}\r\n}\r\n}\r\n}\r\nmutex_unlock(&ohci->phy_reg_mutex);\r\nreturn ret;\r\n}\r\nstatic int find_and_insert_self_id(struct fw_ohci *ohci, int self_id_count)\r\n{\r\nint reg, i, pos, status;\r\nu32 self_id = 0x8040c800;\r\nreg = reg_read(ohci, OHCI1394_NodeID);\r\nif (!(reg & OHCI1394_NodeID_idValid)) {\r\nohci_notice(ohci,\r\n"node ID not valid, new bus reset in progress\n");\r\nreturn -EBUSY;\r\n}\r\nself_id |= ((reg & 0x3f) << 24);\r\nreg = ohci_read_phy_reg(&ohci->card, 4);\r\nif (reg < 0)\r\nreturn reg;\r\nself_id |= ((reg & 0x07) << 8);\r\nreg = ohci_read_phy_reg(&ohci->card, 1);\r\nif (reg < 0)\r\nreturn reg;\r\nself_id |= ((reg & 0x3f) << 16);\r\nfor (i = 0; i < 3; i++) {\r\nstatus = get_status_for_port(ohci, i);\r\nif (status < 0)\r\nreturn status;\r\nself_id |= ((status & 0x3) << (6 - (i * 2)));\r\n}\r\nself_id |= initiated_reset(ohci);\r\npos = get_self_id_pos(ohci, self_id, self_id_count);\r\nif (pos >= 0) {\r\nmemmove(&(ohci->self_id_buffer[pos+1]),\r\n&(ohci->self_id_buffer[pos]),\r\n(self_id_count - pos) * sizeof(*ohci->self_id_buffer));\r\nohci->self_id_buffer[pos] = self_id;\r\nself_id_count++;\r\n}\r\nreturn self_id_count;\r\n}\r\nstatic void bus_reset_work(struct work_struct *work)\r\n{\r\nstruct fw_ohci *ohci =\r\ncontainer_of(work, struct fw_ohci, bus_reset_work);\r\nint self_id_count, generation, new_generation, i, j;\r\nu32 reg;\r\nvoid *free_rom = NULL;\r\ndma_addr_t free_rom_bus = 0;\r\nbool is_new_root;\r\nreg = reg_read(ohci, OHCI1394_NodeID);\r\nif (!(reg & OHCI1394_NodeID_idValid)) {\r\nohci_notice(ohci,\r\n"node ID not valid, new bus reset in progress\n");\r\nreturn;\r\n}\r\nif ((reg & OHCI1394_NodeID_nodeNumber) == 63) {\r\nohci_notice(ohci, "malconfigured bus\n");\r\nreturn;\r\n}\r\nohci->node_id = reg & (OHCI1394_NodeID_busNumber |\r\nOHCI1394_NodeID_nodeNumber);\r\nis_new_root = (reg & OHCI1394_NodeID_root) != 0;\r\nif (!(ohci->is_root && is_new_root))\r\nreg_write(ohci, OHCI1394_LinkControlSet,\r\nOHCI1394_LinkControl_cycleMaster);\r\nohci->is_root = is_new_root;\r\nreg = reg_read(ohci, OHCI1394_SelfIDCount);\r\nif (reg & OHCI1394_SelfIDCount_selfIDError) {\r\nohci_notice(ohci, "self ID receive error\n");\r\nreturn;\r\n}\r\nself_id_count = (reg >> 3) & 0xff;\r\nif (self_id_count > 252) {\r\nohci_notice(ohci, "bad selfIDSize (%08x)\n", reg);\r\nreturn;\r\n}\r\ngeneration = (cond_le32_to_cpu(ohci->self_id[0]) >> 16) & 0xff;\r\nrmb();\r\nfor (i = 1, j = 0; j < self_id_count; i += 2, j++) {\r\nu32 id = cond_le32_to_cpu(ohci->self_id[i]);\r\nu32 id2 = cond_le32_to_cpu(ohci->self_id[i + 1]);\r\nif (id != ~id2) {\r\nif (id == 0xffff008f) {\r\nohci_notice(ohci, "ignoring spurious self IDs\n");\r\nself_id_count = j;\r\nbreak;\r\n}\r\nohci_notice(ohci, "bad self ID %d/%d (%08x != ~%08x)\n",\r\nj, self_id_count, id, id2);\r\nreturn;\r\n}\r\nohci->self_id_buffer[j] = id;\r\n}\r\nif (ohci->quirks & QUIRK_TI_SLLZ059) {\r\nself_id_count = find_and_insert_self_id(ohci, self_id_count);\r\nif (self_id_count < 0) {\r\nohci_notice(ohci,\r\n"could not construct local self ID\n");\r\nreturn;\r\n}\r\n}\r\nif (self_id_count == 0) {\r\nohci_notice(ohci, "no self IDs\n");\r\nreturn;\r\n}\r\nrmb();\r\nnew_generation = (reg_read(ohci, OHCI1394_SelfIDCount) >> 16) & 0xff;\r\nif (new_generation != generation) {\r\nohci_notice(ohci, "new bus reset, discarding self ids\n");\r\nreturn;\r\n}\r\nspin_lock_irq(&ohci->lock);\r\nohci->generation = -1;\r\ncontext_stop(&ohci->at_request_ctx);\r\ncontext_stop(&ohci->at_response_ctx);\r\nspin_unlock_irq(&ohci->lock);\r\nat_context_flush(&ohci->at_request_ctx);\r\nat_context_flush(&ohci->at_response_ctx);\r\nspin_lock_irq(&ohci->lock);\r\nohci->generation = generation;\r\nreg_write(ohci, OHCI1394_IntEventClear, OHCI1394_busReset);\r\nif (ohci->quirks & QUIRK_RESET_PACKET)\r\nohci->request_generation = generation;\r\nif (ohci->next_config_rom != NULL) {\r\nif (ohci->next_config_rom != ohci->config_rom) {\r\nfree_rom = ohci->config_rom;\r\nfree_rom_bus = ohci->config_rom_bus;\r\n}\r\nohci->config_rom = ohci->next_config_rom;\r\nohci->config_rom_bus = ohci->next_config_rom_bus;\r\nohci->next_config_rom = NULL;\r\nreg_write(ohci, OHCI1394_BusOptions,\r\nbe32_to_cpu(ohci->config_rom[2]));\r\nohci->config_rom[0] = ohci->next_header;\r\nreg_write(ohci, OHCI1394_ConfigROMhdr,\r\nbe32_to_cpu(ohci->next_header));\r\n}\r\nif (param_remote_dma) {\r\nreg_write(ohci, OHCI1394_PhyReqFilterHiSet, ~0);\r\nreg_write(ohci, OHCI1394_PhyReqFilterLoSet, ~0);\r\n}\r\nspin_unlock_irq(&ohci->lock);\r\nif (free_rom)\r\ndma_free_coherent(ohci->card.device, CONFIG_ROM_SIZE,\r\nfree_rom, free_rom_bus);\r\nlog_selfids(ohci, generation, self_id_count);\r\nfw_core_handle_bus_reset(&ohci->card, ohci->node_id, generation,\r\nself_id_count, ohci->self_id_buffer,\r\nohci->csr_state_setclear_abdicate);\r\nohci->csr_state_setclear_abdicate = false;\r\n}\r\nstatic irqreturn_t irq_handler(int irq, void *data)\r\n{\r\nstruct fw_ohci *ohci = data;\r\nu32 event, iso_event;\r\nint i;\r\nevent = reg_read(ohci, OHCI1394_IntEventClear);\r\nif (!event || !~event)\r\nreturn IRQ_NONE;\r\nreg_write(ohci, OHCI1394_IntEventClear,\r\nevent & ~(OHCI1394_busReset | OHCI1394_postedWriteErr));\r\nlog_irqs(ohci, event);\r\nif (event & OHCI1394_selfIDComplete)\r\nqueue_work(selfid_workqueue, &ohci->bus_reset_work);\r\nif (event & OHCI1394_RQPkt)\r\ntasklet_schedule(&ohci->ar_request_ctx.tasklet);\r\nif (event & OHCI1394_RSPkt)\r\ntasklet_schedule(&ohci->ar_response_ctx.tasklet);\r\nif (event & OHCI1394_reqTxComplete)\r\ntasklet_schedule(&ohci->at_request_ctx.tasklet);\r\nif (event & OHCI1394_respTxComplete)\r\ntasklet_schedule(&ohci->at_response_ctx.tasklet);\r\nif (event & OHCI1394_isochRx) {\r\niso_event = reg_read(ohci, OHCI1394_IsoRecvIntEventClear);\r\nreg_write(ohci, OHCI1394_IsoRecvIntEventClear, iso_event);\r\nwhile (iso_event) {\r\ni = ffs(iso_event) - 1;\r\ntasklet_schedule(\r\n&ohci->ir_context_list[i].context.tasklet);\r\niso_event &= ~(1 << i);\r\n}\r\n}\r\nif (event & OHCI1394_isochTx) {\r\niso_event = reg_read(ohci, OHCI1394_IsoXmitIntEventClear);\r\nreg_write(ohci, OHCI1394_IsoXmitIntEventClear, iso_event);\r\nwhile (iso_event) {\r\ni = ffs(iso_event) - 1;\r\ntasklet_schedule(\r\n&ohci->it_context_list[i].context.tasklet);\r\niso_event &= ~(1 << i);\r\n}\r\n}\r\nif (unlikely(event & OHCI1394_regAccessFail))\r\nohci_err(ohci, "register access failure\n");\r\nif (unlikely(event & OHCI1394_postedWriteErr)) {\r\nreg_read(ohci, OHCI1394_PostedWriteAddressHi);\r\nreg_read(ohci, OHCI1394_PostedWriteAddressLo);\r\nreg_write(ohci, OHCI1394_IntEventClear,\r\nOHCI1394_postedWriteErr);\r\nif (printk_ratelimit())\r\nohci_err(ohci, "PCI posted write error\n");\r\n}\r\nif (unlikely(event & OHCI1394_cycleTooLong)) {\r\nif (printk_ratelimit())\r\nohci_notice(ohci, "isochronous cycle too long\n");\r\nreg_write(ohci, OHCI1394_LinkControlSet,\r\nOHCI1394_LinkControl_cycleMaster);\r\n}\r\nif (unlikely(event & OHCI1394_cycleInconsistent)) {\r\nif (printk_ratelimit())\r\nohci_notice(ohci, "isochronous cycle inconsistent\n");\r\n}\r\nif (unlikely(event & OHCI1394_unrecoverableError))\r\nhandle_dead_contexts(ohci);\r\nif (event & OHCI1394_cycle64Seconds) {\r\nspin_lock(&ohci->lock);\r\nupdate_bus_time(ohci);\r\nspin_unlock(&ohci->lock);\r\n} else\r\nflush_writes(ohci);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int software_reset(struct fw_ohci *ohci)\r\n{\r\nu32 val;\r\nint i;\r\nreg_write(ohci, OHCI1394_HCControlSet, OHCI1394_HCControl_softReset);\r\nfor (i = 0; i < 500; i++) {\r\nval = reg_read(ohci, OHCI1394_HCControlSet);\r\nif (!~val)\r\nreturn -ENODEV;\r\nif (!(val & OHCI1394_HCControl_softReset))\r\nreturn 0;\r\nmsleep(1);\r\n}\r\nreturn -EBUSY;\r\n}\r\nstatic void copy_config_rom(__be32 *dest, const __be32 *src, size_t length)\r\n{\r\nsize_t size = length * 4;\r\nmemcpy(dest, src, size);\r\nif (size < CONFIG_ROM_SIZE)\r\nmemset(&dest[length], 0, CONFIG_ROM_SIZE - size);\r\n}\r\nstatic int configure_1394a_enhancements(struct fw_ohci *ohci)\r\n{\r\nbool enable_1394a;\r\nint ret, clear, set, offset;\r\nif (!(reg_read(ohci, OHCI1394_HCControlSet) &\r\nOHCI1394_HCControl_programPhyEnable))\r\nreturn 0;\r\nenable_1394a = false;\r\nret = read_phy_reg(ohci, 2);\r\nif (ret < 0)\r\nreturn ret;\r\nif ((ret & PHY_EXTENDED_REGISTERS) == PHY_EXTENDED_REGISTERS) {\r\nret = read_paged_phy_reg(ohci, 1, 8);\r\nif (ret < 0)\r\nreturn ret;\r\nif (ret >= 1)\r\nenable_1394a = true;\r\n}\r\nif (ohci->quirks & QUIRK_NO_1394A)\r\nenable_1394a = false;\r\nif (enable_1394a) {\r\nclear = 0;\r\nset = PHY_ENABLE_ACCEL | PHY_ENABLE_MULTI;\r\n} else {\r\nclear = PHY_ENABLE_ACCEL | PHY_ENABLE_MULTI;\r\nset = 0;\r\n}\r\nret = update_phy_reg(ohci, 5, clear, set);\r\nif (ret < 0)\r\nreturn ret;\r\nif (enable_1394a)\r\noffset = OHCI1394_HCControlSet;\r\nelse\r\noffset = OHCI1394_HCControlClear;\r\nreg_write(ohci, offset, OHCI1394_HCControl_aPhyEnhanceEnable);\r\nreg_write(ohci, OHCI1394_HCControlClear,\r\nOHCI1394_HCControl_programPhyEnable);\r\nreturn 0;\r\n}\r\nstatic int probe_tsb41ba3d(struct fw_ohci *ohci)\r\n{\r\nstatic const u8 id[] = { 0x08, 0x00, 0x28, 0x83, 0x30, 0x05, };\r\nint reg, i;\r\nreg = read_phy_reg(ohci, 2);\r\nif (reg < 0)\r\nreturn reg;\r\nif ((reg & PHY_EXTENDED_REGISTERS) != PHY_EXTENDED_REGISTERS)\r\nreturn 0;\r\nfor (i = ARRAY_SIZE(id) - 1; i >= 0; i--) {\r\nreg = read_paged_phy_reg(ohci, 1, i + 10);\r\nif (reg < 0)\r\nreturn reg;\r\nif (reg != id[i])\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int ohci_enable(struct fw_card *card,\r\nconst __be32 *config_rom, size_t length)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nu32 lps, version, irqs;\r\nint i, ret;\r\nif (software_reset(ohci)) {\r\nohci_err(ohci, "failed to reset ohci card\n");\r\nreturn -EBUSY;\r\n}\r\nreg_write(ohci, OHCI1394_HCControlSet,\r\nOHCI1394_HCControl_LPS |\r\nOHCI1394_HCControl_postedWriteEnable);\r\nflush_writes(ohci);\r\nfor (lps = 0, i = 0; !lps && i < 3; i++) {\r\nmsleep(50);\r\nlps = reg_read(ohci, OHCI1394_HCControlSet) &\r\nOHCI1394_HCControl_LPS;\r\n}\r\nif (!lps) {\r\nohci_err(ohci, "failed to set Link Power Status\n");\r\nreturn -EIO;\r\n}\r\nif (ohci->quirks & QUIRK_TI_SLLZ059) {\r\nret = probe_tsb41ba3d(ohci);\r\nif (ret < 0)\r\nreturn ret;\r\nif (ret)\r\nohci_notice(ohci, "local TSB41BA3D phy\n");\r\nelse\r\nohci->quirks &= ~QUIRK_TI_SLLZ059;\r\n}\r\nreg_write(ohci, OHCI1394_HCControlClear,\r\nOHCI1394_HCControl_noByteSwapData);\r\nreg_write(ohci, OHCI1394_SelfIDBuffer, ohci->self_id_bus);\r\nreg_write(ohci, OHCI1394_LinkControlSet,\r\nOHCI1394_LinkControl_cycleTimerEnable |\r\nOHCI1394_LinkControl_cycleMaster);\r\nreg_write(ohci, OHCI1394_ATRetries,\r\nOHCI1394_MAX_AT_REQ_RETRIES |\r\n(OHCI1394_MAX_AT_RESP_RETRIES << 4) |\r\n(OHCI1394_MAX_PHYS_RESP_RETRIES << 8) |\r\n(200 << 16));\r\nohci->bus_time_running = false;\r\nfor (i = 0; i < 32; i++)\r\nif (ohci->ir_context_support & (1 << i))\r\nreg_write(ohci, OHCI1394_IsoRcvContextControlClear(i),\r\nIR_CONTEXT_MULTI_CHANNEL_MODE);\r\nversion = reg_read(ohci, OHCI1394_Version) & 0x00ff00ff;\r\nif (version >= OHCI_VERSION_1_1) {\r\nreg_write(ohci, OHCI1394_InitialChannelsAvailableHi,\r\n0xfffffffe);\r\ncard->broadcast_channel_auto_allocated = true;\r\n}\r\nreg_write(ohci, OHCI1394_FairnessControl, 0x3f);\r\nohci->pri_req_max = reg_read(ohci, OHCI1394_FairnessControl) & 0x3f;\r\nreg_write(ohci, OHCI1394_FairnessControl, 0);\r\ncard->priority_budget_implemented = ohci->pri_req_max != 0;\r\nreg_write(ohci, OHCI1394_PhyUpperBound, FW_MAX_PHYSICAL_RANGE >> 16);\r\nreg_write(ohci, OHCI1394_IntEventClear, ~0);\r\nreg_write(ohci, OHCI1394_IntMaskClear, ~0);\r\nret = configure_1394a_enhancements(ohci);\r\nif (ret < 0)\r\nreturn ret;\r\nret = ohci_update_phy_reg(card, 4, 0, PHY_LINK_ACTIVE | PHY_CONTENDER);\r\nif (ret < 0)\r\nreturn ret;\r\nif (config_rom) {\r\nohci->next_config_rom =\r\ndma_alloc_coherent(ohci->card.device, CONFIG_ROM_SIZE,\r\n&ohci->next_config_rom_bus,\r\nGFP_KERNEL);\r\nif (ohci->next_config_rom == NULL)\r\nreturn -ENOMEM;\r\ncopy_config_rom(ohci->next_config_rom, config_rom, length);\r\n} else {\r\nohci->next_config_rom = ohci->config_rom;\r\nohci->next_config_rom_bus = ohci->config_rom_bus;\r\n}\r\nohci->next_header = ohci->next_config_rom[0];\r\nohci->next_config_rom[0] = 0;\r\nreg_write(ohci, OHCI1394_ConfigROMhdr, 0);\r\nreg_write(ohci, OHCI1394_BusOptions,\r\nbe32_to_cpu(ohci->next_config_rom[2]));\r\nreg_write(ohci, OHCI1394_ConfigROMmap, ohci->next_config_rom_bus);\r\nreg_write(ohci, OHCI1394_AsReqFilterHiSet, 0x80000000);\r\nirqs = OHCI1394_reqTxComplete | OHCI1394_respTxComplete |\r\nOHCI1394_RQPkt | OHCI1394_RSPkt |\r\nOHCI1394_isochTx | OHCI1394_isochRx |\r\nOHCI1394_postedWriteErr |\r\nOHCI1394_selfIDComplete |\r\nOHCI1394_regAccessFail |\r\nOHCI1394_cycleInconsistent |\r\nOHCI1394_unrecoverableError |\r\nOHCI1394_cycleTooLong |\r\nOHCI1394_masterIntEnable;\r\nif (param_debug & OHCI_PARAM_DEBUG_BUSRESETS)\r\nirqs |= OHCI1394_busReset;\r\nreg_write(ohci, OHCI1394_IntMaskSet, irqs);\r\nreg_write(ohci, OHCI1394_HCControlSet,\r\nOHCI1394_HCControl_linkEnable |\r\nOHCI1394_HCControl_BIBimageValid);\r\nreg_write(ohci, OHCI1394_LinkControlSet,\r\nOHCI1394_LinkControl_rcvSelfID |\r\nOHCI1394_LinkControl_rcvPhyPkt);\r\nar_context_run(&ohci->ar_request_ctx);\r\nar_context_run(&ohci->ar_response_ctx);\r\nflush_writes(ohci);\r\nfw_schedule_bus_reset(&ohci->card, false, true);\r\nreturn 0;\r\n}\r\nstatic int ohci_set_config_rom(struct fw_card *card,\r\nconst __be32 *config_rom, size_t length)\r\n{\r\nstruct fw_ohci *ohci;\r\n__be32 *next_config_rom;\r\ndma_addr_t uninitialized_var(next_config_rom_bus);\r\nohci = fw_ohci(card);\r\nnext_config_rom =\r\ndma_alloc_coherent(ohci->card.device, CONFIG_ROM_SIZE,\r\n&next_config_rom_bus, GFP_KERNEL);\r\nif (next_config_rom == NULL)\r\nreturn -ENOMEM;\r\nspin_lock_irq(&ohci->lock);\r\nif (ohci->next_config_rom == NULL) {\r\nohci->next_config_rom = next_config_rom;\r\nohci->next_config_rom_bus = next_config_rom_bus;\r\nnext_config_rom = NULL;\r\n}\r\ncopy_config_rom(ohci->next_config_rom, config_rom, length);\r\nohci->next_header = config_rom[0];\r\nohci->next_config_rom[0] = 0;\r\nreg_write(ohci, OHCI1394_ConfigROMmap, ohci->next_config_rom_bus);\r\nspin_unlock_irq(&ohci->lock);\r\nif (next_config_rom != NULL)\r\ndma_free_coherent(ohci->card.device, CONFIG_ROM_SIZE,\r\nnext_config_rom, next_config_rom_bus);\r\nfw_schedule_bus_reset(&ohci->card, true, true);\r\nreturn 0;\r\n}\r\nstatic void ohci_send_request(struct fw_card *card, struct fw_packet *packet)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nat_context_transmit(&ohci->at_request_ctx, packet);\r\n}\r\nstatic void ohci_send_response(struct fw_card *card, struct fw_packet *packet)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nat_context_transmit(&ohci->at_response_ctx, packet);\r\n}\r\nstatic int ohci_cancel_packet(struct fw_card *card, struct fw_packet *packet)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nstruct context *ctx = &ohci->at_request_ctx;\r\nstruct driver_data *driver_data = packet->driver_data;\r\nint ret = -ENOENT;\r\ntasklet_disable(&ctx->tasklet);\r\nif (packet->ack != 0)\r\ngoto out;\r\nif (packet->payload_mapped)\r\ndma_unmap_single(ohci->card.device, packet->payload_bus,\r\npacket->payload_length, DMA_TO_DEVICE);\r\nlog_ar_at_event(ohci, 'T', packet->speed, packet->header, 0x20);\r\ndriver_data->packet = NULL;\r\npacket->ack = RCODE_CANCELLED;\r\npacket->callback(packet, &ohci->card, packet->ack);\r\nret = 0;\r\nout:\r\ntasklet_enable(&ctx->tasklet);\r\nreturn ret;\r\n}\r\nstatic int ohci_enable_phys_dma(struct fw_card *card,\r\nint node_id, int generation)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nunsigned long flags;\r\nint n, ret = 0;\r\nif (param_remote_dma)\r\nreturn 0;\r\nspin_lock_irqsave(&ohci->lock, flags);\r\nif (ohci->generation != generation) {\r\nret = -ESTALE;\r\ngoto out;\r\n}\r\nn = (node_id & 0xffc0) == LOCAL_BUS ? node_id & 0x3f : 63;\r\nif (n < 32)\r\nreg_write(ohci, OHCI1394_PhyReqFilterLoSet, 1 << n);\r\nelse\r\nreg_write(ohci, OHCI1394_PhyReqFilterHiSet, 1 << (n - 32));\r\nflush_writes(ohci);\r\nout:\r\nspin_unlock_irqrestore(&ohci->lock, flags);\r\nreturn ret;\r\n}\r\nstatic u32 ohci_read_csr(struct fw_card *card, int csr_offset)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nunsigned long flags;\r\nu32 value;\r\nswitch (csr_offset) {\r\ncase CSR_STATE_CLEAR:\r\ncase CSR_STATE_SET:\r\nif (ohci->is_root &&\r\n(reg_read(ohci, OHCI1394_LinkControlSet) &\r\nOHCI1394_LinkControl_cycleMaster))\r\nvalue = CSR_STATE_BIT_CMSTR;\r\nelse\r\nvalue = 0;\r\nif (ohci->csr_state_setclear_abdicate)\r\nvalue |= CSR_STATE_BIT_ABDICATE;\r\nreturn value;\r\ncase CSR_NODE_IDS:\r\nreturn reg_read(ohci, OHCI1394_NodeID) << 16;\r\ncase CSR_CYCLE_TIME:\r\nreturn get_cycle_time(ohci);\r\ncase CSR_BUS_TIME:\r\nspin_lock_irqsave(&ohci->lock, flags);\r\nvalue = update_bus_time(ohci);\r\nspin_unlock_irqrestore(&ohci->lock, flags);\r\nreturn value;\r\ncase CSR_BUSY_TIMEOUT:\r\nvalue = reg_read(ohci, OHCI1394_ATRetries);\r\nreturn (value >> 4) & 0x0ffff00f;\r\ncase CSR_PRIORITY_BUDGET:\r\nreturn (reg_read(ohci, OHCI1394_FairnessControl) & 0x3f) |\r\n(ohci->pri_req_max << 8);\r\ndefault:\r\nWARN_ON(1);\r\nreturn 0;\r\n}\r\n}\r\nstatic void ohci_write_csr(struct fw_card *card, int csr_offset, u32 value)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nunsigned long flags;\r\nswitch (csr_offset) {\r\ncase CSR_STATE_CLEAR:\r\nif ((value & CSR_STATE_BIT_CMSTR) && ohci->is_root) {\r\nreg_write(ohci, OHCI1394_LinkControlClear,\r\nOHCI1394_LinkControl_cycleMaster);\r\nflush_writes(ohci);\r\n}\r\nif (value & CSR_STATE_BIT_ABDICATE)\r\nohci->csr_state_setclear_abdicate = false;\r\nbreak;\r\ncase CSR_STATE_SET:\r\nif ((value & CSR_STATE_BIT_CMSTR) && ohci->is_root) {\r\nreg_write(ohci, OHCI1394_LinkControlSet,\r\nOHCI1394_LinkControl_cycleMaster);\r\nflush_writes(ohci);\r\n}\r\nif (value & CSR_STATE_BIT_ABDICATE)\r\nohci->csr_state_setclear_abdicate = true;\r\nbreak;\r\ncase CSR_NODE_IDS:\r\nreg_write(ohci, OHCI1394_NodeID, value >> 16);\r\nflush_writes(ohci);\r\nbreak;\r\ncase CSR_CYCLE_TIME:\r\nreg_write(ohci, OHCI1394_IsochronousCycleTimer, value);\r\nreg_write(ohci, OHCI1394_IntEventSet,\r\nOHCI1394_cycleInconsistent);\r\nflush_writes(ohci);\r\nbreak;\r\ncase CSR_BUS_TIME:\r\nspin_lock_irqsave(&ohci->lock, flags);\r\nohci->bus_time = (update_bus_time(ohci) & 0x40) |\r\n(value & ~0x7f);\r\nspin_unlock_irqrestore(&ohci->lock, flags);\r\nbreak;\r\ncase CSR_BUSY_TIMEOUT:\r\nvalue = (value & 0xf) | ((value & 0xf) << 4) |\r\n((value & 0xf) << 8) | ((value & 0x0ffff000) << 4);\r\nreg_write(ohci, OHCI1394_ATRetries, value);\r\nflush_writes(ohci);\r\nbreak;\r\ncase CSR_PRIORITY_BUDGET:\r\nreg_write(ohci, OHCI1394_FairnessControl, value & 0x3f);\r\nflush_writes(ohci);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nbreak;\r\n}\r\n}\r\nstatic void flush_iso_completions(struct iso_context *ctx)\r\n{\r\nctx->base.callback.sc(&ctx->base, ctx->last_timestamp,\r\nctx->header_length, ctx->header,\r\nctx->base.callback_data);\r\nctx->header_length = 0;\r\n}\r\nstatic void copy_iso_headers(struct iso_context *ctx, const u32 *dma_hdr)\r\n{\r\nu32 *ctx_hdr;\r\nif (ctx->header_length + ctx->base.header_size > PAGE_SIZE) {\r\nif (ctx->base.drop_overflow_headers)\r\nreturn;\r\nflush_iso_completions(ctx);\r\n}\r\nctx_hdr = ctx->header + ctx->header_length;\r\nctx->last_timestamp = (u16)le32_to_cpu((__force __le32)dma_hdr[0]);\r\nif (ctx->base.header_size > 0)\r\nctx_hdr[0] = swab32(dma_hdr[1]);\r\nif (ctx->base.header_size > 4)\r\nctx_hdr[1] = swab32(dma_hdr[0]);\r\nif (ctx->base.header_size > 8)\r\nmemcpy(&ctx_hdr[2], &dma_hdr[2], ctx->base.header_size - 8);\r\nctx->header_length += ctx->base.header_size;\r\n}\r\nstatic int handle_ir_packet_per_buffer(struct context *context,\r\nstruct descriptor *d,\r\nstruct descriptor *last)\r\n{\r\nstruct iso_context *ctx =\r\ncontainer_of(context, struct iso_context, context);\r\nstruct descriptor *pd;\r\nu32 buffer_dma;\r\nfor (pd = d; pd <= last; pd++)\r\nif (pd->transfer_status)\r\nbreak;\r\nif (pd > last)\r\nreturn 0;\r\nwhile (!(d->control & cpu_to_le16(DESCRIPTOR_BRANCH_ALWAYS))) {\r\nd++;\r\nbuffer_dma = le32_to_cpu(d->data_address);\r\ndma_sync_single_range_for_cpu(context->ohci->card.device,\r\nbuffer_dma & PAGE_MASK,\r\nbuffer_dma & ~PAGE_MASK,\r\nle16_to_cpu(d->req_count),\r\nDMA_FROM_DEVICE);\r\n}\r\ncopy_iso_headers(ctx, (u32 *) (last + 1));\r\nif (last->control & cpu_to_le16(DESCRIPTOR_IRQ_ALWAYS))\r\nflush_iso_completions(ctx);\r\nreturn 1;\r\n}\r\nstatic int handle_ir_buffer_fill(struct context *context,\r\nstruct descriptor *d,\r\nstruct descriptor *last)\r\n{\r\nstruct iso_context *ctx =\r\ncontainer_of(context, struct iso_context, context);\r\nunsigned int req_count, res_count, completed;\r\nu32 buffer_dma;\r\nreq_count = le16_to_cpu(last->req_count);\r\nres_count = le16_to_cpu(ACCESS_ONCE(last->res_count));\r\ncompleted = req_count - res_count;\r\nbuffer_dma = le32_to_cpu(last->data_address);\r\nif (completed > 0) {\r\nctx->mc_buffer_bus = buffer_dma;\r\nctx->mc_completed = completed;\r\n}\r\nif (res_count != 0)\r\nreturn 0;\r\ndma_sync_single_range_for_cpu(context->ohci->card.device,\r\nbuffer_dma & PAGE_MASK,\r\nbuffer_dma & ~PAGE_MASK,\r\ncompleted, DMA_FROM_DEVICE);\r\nif (last->control & cpu_to_le16(DESCRIPTOR_IRQ_ALWAYS)) {\r\nctx->base.callback.mc(&ctx->base,\r\nbuffer_dma + completed,\r\nctx->base.callback_data);\r\nctx->mc_completed = 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void flush_ir_buffer_fill(struct iso_context *ctx)\r\n{\r\ndma_sync_single_range_for_cpu(ctx->context.ohci->card.device,\r\nctx->mc_buffer_bus & PAGE_MASK,\r\nctx->mc_buffer_bus & ~PAGE_MASK,\r\nctx->mc_completed, DMA_FROM_DEVICE);\r\nctx->base.callback.mc(&ctx->base,\r\nctx->mc_buffer_bus + ctx->mc_completed,\r\nctx->base.callback_data);\r\nctx->mc_completed = 0;\r\n}\r\nstatic inline void sync_it_packet_for_cpu(struct context *context,\r\nstruct descriptor *pd)\r\n{\r\n__le16 control;\r\nu32 buffer_dma;\r\nif (pd->control & cpu_to_le16(DESCRIPTOR_BRANCH_ALWAYS))\r\nreturn;\r\npd += 2;\r\nif ((le32_to_cpu(pd->data_address) & PAGE_MASK) ==\r\n(context->current_bus & PAGE_MASK)) {\r\nif (pd->control & cpu_to_le16(DESCRIPTOR_BRANCH_ALWAYS))\r\nreturn;\r\npd++;\r\n}\r\ndo {\r\nbuffer_dma = le32_to_cpu(pd->data_address);\r\ndma_sync_single_range_for_cpu(context->ohci->card.device,\r\nbuffer_dma & PAGE_MASK,\r\nbuffer_dma & ~PAGE_MASK,\r\nle16_to_cpu(pd->req_count),\r\nDMA_TO_DEVICE);\r\ncontrol = pd->control;\r\npd++;\r\n} while (!(control & cpu_to_le16(DESCRIPTOR_BRANCH_ALWAYS)));\r\n}\r\nstatic int handle_it_packet(struct context *context,\r\nstruct descriptor *d,\r\nstruct descriptor *last)\r\n{\r\nstruct iso_context *ctx =\r\ncontainer_of(context, struct iso_context, context);\r\nstruct descriptor *pd;\r\n__be32 *ctx_hdr;\r\nfor (pd = d; pd <= last; pd++)\r\nif (pd->transfer_status)\r\nbreak;\r\nif (pd > last)\r\nreturn 0;\r\nsync_it_packet_for_cpu(context, d);\r\nif (ctx->header_length + 4 > PAGE_SIZE) {\r\nif (ctx->base.drop_overflow_headers)\r\nreturn 1;\r\nflush_iso_completions(ctx);\r\n}\r\nctx_hdr = ctx->header + ctx->header_length;\r\nctx->last_timestamp = le16_to_cpu(last->res_count);\r\n*ctx_hdr = cpu_to_be32((le16_to_cpu(pd->transfer_status) << 16) |\r\nle16_to_cpu(pd->res_count));\r\nctx->header_length += 4;\r\nif (last->control & cpu_to_le16(DESCRIPTOR_IRQ_ALWAYS))\r\nflush_iso_completions(ctx);\r\nreturn 1;\r\n}\r\nstatic void set_multichannel_mask(struct fw_ohci *ohci, u64 channels)\r\n{\r\nu32 hi = channels >> 32, lo = channels;\r\nreg_write(ohci, OHCI1394_IRMultiChanMaskHiClear, ~hi);\r\nreg_write(ohci, OHCI1394_IRMultiChanMaskLoClear, ~lo);\r\nreg_write(ohci, OHCI1394_IRMultiChanMaskHiSet, hi);\r\nreg_write(ohci, OHCI1394_IRMultiChanMaskLoSet, lo);\r\nmmiowb();\r\nohci->mc_channels = channels;\r\n}\r\nstatic struct fw_iso_context *ohci_allocate_iso_context(struct fw_card *card,\r\nint type, int channel, size_t header_size)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(card);\r\nstruct iso_context *uninitialized_var(ctx);\r\ndescriptor_callback_t uninitialized_var(callback);\r\nu64 *uninitialized_var(channels);\r\nu32 *uninitialized_var(mask), uninitialized_var(regs);\r\nint index, ret = -EBUSY;\r\nspin_lock_irq(&ohci->lock);\r\nswitch (type) {\r\ncase FW_ISO_CONTEXT_TRANSMIT:\r\nmask = &ohci->it_context_mask;\r\ncallback = handle_it_packet;\r\nindex = ffs(*mask) - 1;\r\nif (index >= 0) {\r\n*mask &= ~(1 << index);\r\nregs = OHCI1394_IsoXmitContextBase(index);\r\nctx = &ohci->it_context_list[index];\r\n}\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE:\r\nchannels = &ohci->ir_context_channels;\r\nmask = &ohci->ir_context_mask;\r\ncallback = handle_ir_packet_per_buffer;\r\nindex = *channels & 1ULL << channel ? ffs(*mask) - 1 : -1;\r\nif (index >= 0) {\r\n*channels &= ~(1ULL << channel);\r\n*mask &= ~(1 << index);\r\nregs = OHCI1394_IsoRcvContextBase(index);\r\nctx = &ohci->ir_context_list[index];\r\n}\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\nmask = &ohci->ir_context_mask;\r\ncallback = handle_ir_buffer_fill;\r\nindex = !ohci->mc_allocated ? ffs(*mask) - 1 : -1;\r\nif (index >= 0) {\r\nohci->mc_allocated = true;\r\n*mask &= ~(1 << index);\r\nregs = OHCI1394_IsoRcvContextBase(index);\r\nctx = &ohci->ir_context_list[index];\r\n}\r\nbreak;\r\ndefault:\r\nindex = -1;\r\nret = -ENOSYS;\r\n}\r\nspin_unlock_irq(&ohci->lock);\r\nif (index < 0)\r\nreturn ERR_PTR(ret);\r\nmemset(ctx, 0, sizeof(*ctx));\r\nctx->header_length = 0;\r\nctx->header = (void *) __get_free_page(GFP_KERNEL);\r\nif (ctx->header == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = context_init(&ctx->context, ohci, regs, callback);\r\nif (ret < 0)\r\ngoto out_with_header;\r\nif (type == FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL) {\r\nset_multichannel_mask(ohci, 0);\r\nctx->mc_completed = 0;\r\n}\r\nreturn &ctx->base;\r\nout_with_header:\r\nfree_page((unsigned long)ctx->header);\r\nout:\r\nspin_lock_irq(&ohci->lock);\r\nswitch (type) {\r\ncase FW_ISO_CONTEXT_RECEIVE:\r\n*channels |= 1ULL << channel;\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\nohci->mc_allocated = false;\r\nbreak;\r\n}\r\n*mask |= 1 << index;\r\nspin_unlock_irq(&ohci->lock);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic int ohci_start_iso(struct fw_iso_context *base,\r\ns32 cycle, u32 sync, u32 tags)\r\n{\r\nstruct iso_context *ctx = container_of(base, struct iso_context, base);\r\nstruct fw_ohci *ohci = ctx->context.ohci;\r\nu32 control = IR_CONTEXT_ISOCH_HEADER, match;\r\nint index;\r\nif (ctx->context.last->branch_address == 0)\r\nreturn -ENODATA;\r\nswitch (ctx->base.type) {\r\ncase FW_ISO_CONTEXT_TRANSMIT:\r\nindex = ctx - ohci->it_context_list;\r\nmatch = 0;\r\nif (cycle >= 0)\r\nmatch = IT_CONTEXT_CYCLE_MATCH_ENABLE |\r\n(cycle & 0x7fff) << 16;\r\nreg_write(ohci, OHCI1394_IsoXmitIntEventClear, 1 << index);\r\nreg_write(ohci, OHCI1394_IsoXmitIntMaskSet, 1 << index);\r\ncontext_run(&ctx->context, match);\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\ncontrol |= IR_CONTEXT_BUFFER_FILL|IR_CONTEXT_MULTI_CHANNEL_MODE;\r\ncase FW_ISO_CONTEXT_RECEIVE:\r\nindex = ctx - ohci->ir_context_list;\r\nmatch = (tags << 28) | (sync << 8) | ctx->base.channel;\r\nif (cycle >= 0) {\r\nmatch |= (cycle & 0x07fff) << 12;\r\ncontrol |= IR_CONTEXT_CYCLE_MATCH_ENABLE;\r\n}\r\nreg_write(ohci, OHCI1394_IsoRecvIntEventClear, 1 << index);\r\nreg_write(ohci, OHCI1394_IsoRecvIntMaskSet, 1 << index);\r\nreg_write(ohci, CONTEXT_MATCH(ctx->context.regs), match);\r\ncontext_run(&ctx->context, control);\r\nctx->sync = sync;\r\nctx->tags = tags;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ohci_stop_iso(struct fw_iso_context *base)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(base->card);\r\nstruct iso_context *ctx = container_of(base, struct iso_context, base);\r\nint index;\r\nswitch (ctx->base.type) {\r\ncase FW_ISO_CONTEXT_TRANSMIT:\r\nindex = ctx - ohci->it_context_list;\r\nreg_write(ohci, OHCI1394_IsoXmitIntMaskClear, 1 << index);\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE:\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\nindex = ctx - ohci->ir_context_list;\r\nreg_write(ohci, OHCI1394_IsoRecvIntMaskClear, 1 << index);\r\nbreak;\r\n}\r\nflush_writes(ohci);\r\ncontext_stop(&ctx->context);\r\ntasklet_kill(&ctx->context.tasklet);\r\nreturn 0;\r\n}\r\nstatic void ohci_free_iso_context(struct fw_iso_context *base)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(base->card);\r\nstruct iso_context *ctx = container_of(base, struct iso_context, base);\r\nunsigned long flags;\r\nint index;\r\nohci_stop_iso(base);\r\ncontext_release(&ctx->context);\r\nfree_page((unsigned long)ctx->header);\r\nspin_lock_irqsave(&ohci->lock, flags);\r\nswitch (base->type) {\r\ncase FW_ISO_CONTEXT_TRANSMIT:\r\nindex = ctx - ohci->it_context_list;\r\nohci->it_context_mask |= 1 << index;\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE:\r\nindex = ctx - ohci->ir_context_list;\r\nohci->ir_context_mask |= 1 << index;\r\nohci->ir_context_channels |= 1ULL << base->channel;\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\nindex = ctx - ohci->ir_context_list;\r\nohci->ir_context_mask |= 1 << index;\r\nohci->ir_context_channels |= ohci->mc_channels;\r\nohci->mc_channels = 0;\r\nohci->mc_allocated = false;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ohci->lock, flags);\r\n}\r\nstatic int ohci_set_iso_channels(struct fw_iso_context *base, u64 *channels)\r\n{\r\nstruct fw_ohci *ohci = fw_ohci(base->card);\r\nunsigned long flags;\r\nint ret;\r\nswitch (base->type) {\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\nspin_lock_irqsave(&ohci->lock, flags);\r\nif (~ohci->ir_context_channels & ~ohci->mc_channels & *channels) {\r\n*channels = ohci->ir_context_channels;\r\nret = -EBUSY;\r\n} else {\r\nset_multichannel_mask(ohci, *channels);\r\nret = 0;\r\n}\r\nspin_unlock_irqrestore(&ohci->lock, flags);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nstatic void ohci_resume_iso_dma(struct fw_ohci *ohci)\r\n{\r\nint i;\r\nstruct iso_context *ctx;\r\nfor (i = 0 ; i < ohci->n_ir ; i++) {\r\nctx = &ohci->ir_context_list[i];\r\nif (ctx->context.running)\r\nohci_start_iso(&ctx->base, 0, ctx->sync, ctx->tags);\r\n}\r\nfor (i = 0 ; i < ohci->n_it ; i++) {\r\nctx = &ohci->it_context_list[i];\r\nif (ctx->context.running)\r\nohci_start_iso(&ctx->base, 0, ctx->sync, ctx->tags);\r\n}\r\n}\r\nstatic int queue_iso_transmit(struct iso_context *ctx,\r\nstruct fw_iso_packet *packet,\r\nstruct fw_iso_buffer *buffer,\r\nunsigned long payload)\r\n{\r\nstruct descriptor *d, *last, *pd;\r\nstruct fw_iso_packet *p;\r\n__le32 *header;\r\ndma_addr_t d_bus, page_bus;\r\nu32 z, header_z, payload_z, irq;\r\nu32 payload_index, payload_end_index, next_page_index;\r\nint page, end_page, i, length, offset;\r\np = packet;\r\npayload_index = payload;\r\nif (p->skip)\r\nz = 1;\r\nelse\r\nz = 2;\r\nif (p->header_length > 0)\r\nz++;\r\nend_page = PAGE_ALIGN(payload_index + p->payload_length) >> PAGE_SHIFT;\r\nif (p->payload_length > 0)\r\npayload_z = end_page - (payload_index >> PAGE_SHIFT);\r\nelse\r\npayload_z = 0;\r\nz += payload_z;\r\nheader_z = DIV_ROUND_UP(p->header_length, sizeof(*d));\r\nd = context_get_descriptors(&ctx->context, z + header_z, &d_bus);\r\nif (d == NULL)\r\nreturn -ENOMEM;\r\nif (!p->skip) {\r\nd[0].control = cpu_to_le16(DESCRIPTOR_KEY_IMMEDIATE);\r\nd[0].req_count = cpu_to_le16(8);\r\nd[0].branch_address = cpu_to_le32(d_bus | z);\r\nheader = (__le32 *) &d[1];\r\nheader[0] = cpu_to_le32(IT_HEADER_SY(p->sy) |\r\nIT_HEADER_TAG(p->tag) |\r\nIT_HEADER_TCODE(TCODE_STREAM_DATA) |\r\nIT_HEADER_CHANNEL(ctx->base.channel) |\r\nIT_HEADER_SPEED(ctx->base.speed));\r\nheader[1] =\r\ncpu_to_le32(IT_HEADER_DATA_LENGTH(p->header_length +\r\np->payload_length));\r\n}\r\nif (p->header_length > 0) {\r\nd[2].req_count = cpu_to_le16(p->header_length);\r\nd[2].data_address = cpu_to_le32(d_bus + z * sizeof(*d));\r\nmemcpy(&d[z], p->header, p->header_length);\r\n}\r\npd = d + z - payload_z;\r\npayload_end_index = payload_index + p->payload_length;\r\nfor (i = 0; i < payload_z; i++) {\r\npage = payload_index >> PAGE_SHIFT;\r\noffset = payload_index & ~PAGE_MASK;\r\nnext_page_index = (page + 1) << PAGE_SHIFT;\r\nlength =\r\nmin(next_page_index, payload_end_index) - payload_index;\r\npd[i].req_count = cpu_to_le16(length);\r\npage_bus = page_private(buffer->pages[page]);\r\npd[i].data_address = cpu_to_le32(page_bus + offset);\r\ndma_sync_single_range_for_device(ctx->context.ohci->card.device,\r\npage_bus, offset, length,\r\nDMA_TO_DEVICE);\r\npayload_index += length;\r\n}\r\nif (p->interrupt)\r\nirq = DESCRIPTOR_IRQ_ALWAYS;\r\nelse\r\nirq = DESCRIPTOR_NO_IRQ;\r\nlast = z == 2 ? d : d + z - 1;\r\nlast->control |= cpu_to_le16(DESCRIPTOR_OUTPUT_LAST |\r\nDESCRIPTOR_STATUS |\r\nDESCRIPTOR_BRANCH_ALWAYS |\r\nirq);\r\ncontext_append(&ctx->context, d, z, header_z);\r\nreturn 0;\r\n}\r\nstatic int queue_iso_packet_per_buffer(struct iso_context *ctx,\r\nstruct fw_iso_packet *packet,\r\nstruct fw_iso_buffer *buffer,\r\nunsigned long payload)\r\n{\r\nstruct device *device = ctx->context.ohci->card.device;\r\nstruct descriptor *d, *pd;\r\ndma_addr_t d_bus, page_bus;\r\nu32 z, header_z, rest;\r\nint i, j, length;\r\nint page, offset, packet_count, header_size, payload_per_buffer;\r\npacket_count = packet->header_length / ctx->base.header_size;\r\nheader_size = max(ctx->base.header_size, (size_t)8);\r\nheader_z = DIV_ROUND_UP(header_size, sizeof(*d));\r\npage = payload >> PAGE_SHIFT;\r\noffset = payload & ~PAGE_MASK;\r\npayload_per_buffer = packet->payload_length / packet_count;\r\nfor (i = 0; i < packet_count; i++) {\r\nz = DIV_ROUND_UP(payload_per_buffer + offset, PAGE_SIZE) + 1;\r\nd = context_get_descriptors(&ctx->context,\r\nz + header_z, &d_bus);\r\nif (d == NULL)\r\nreturn -ENOMEM;\r\nd->control = cpu_to_le16(DESCRIPTOR_STATUS |\r\nDESCRIPTOR_INPUT_MORE);\r\nif (packet->skip && i == 0)\r\nd->control |= cpu_to_le16(DESCRIPTOR_WAIT);\r\nd->req_count = cpu_to_le16(header_size);\r\nd->res_count = d->req_count;\r\nd->transfer_status = 0;\r\nd->data_address = cpu_to_le32(d_bus + (z * sizeof(*d)));\r\nrest = payload_per_buffer;\r\npd = d;\r\nfor (j = 1; j < z; j++) {\r\npd++;\r\npd->control = cpu_to_le16(DESCRIPTOR_STATUS |\r\nDESCRIPTOR_INPUT_MORE);\r\nif (offset + rest < PAGE_SIZE)\r\nlength = rest;\r\nelse\r\nlength = PAGE_SIZE - offset;\r\npd->req_count = cpu_to_le16(length);\r\npd->res_count = pd->req_count;\r\npd->transfer_status = 0;\r\npage_bus = page_private(buffer->pages[page]);\r\npd->data_address = cpu_to_le32(page_bus + offset);\r\ndma_sync_single_range_for_device(device, page_bus,\r\noffset, length,\r\nDMA_FROM_DEVICE);\r\noffset = (offset + length) & ~PAGE_MASK;\r\nrest -= length;\r\nif (offset == 0)\r\npage++;\r\n}\r\npd->control = cpu_to_le16(DESCRIPTOR_STATUS |\r\nDESCRIPTOR_INPUT_LAST |\r\nDESCRIPTOR_BRANCH_ALWAYS);\r\nif (packet->interrupt && i == packet_count - 1)\r\npd->control |= cpu_to_le16(DESCRIPTOR_IRQ_ALWAYS);\r\ncontext_append(&ctx->context, d, z, header_z);\r\n}\r\nreturn 0;\r\n}\r\nstatic int queue_iso_buffer_fill(struct iso_context *ctx,\r\nstruct fw_iso_packet *packet,\r\nstruct fw_iso_buffer *buffer,\r\nunsigned long payload)\r\n{\r\nstruct descriptor *d;\r\ndma_addr_t d_bus, page_bus;\r\nint page, offset, rest, z, i, length;\r\npage = payload >> PAGE_SHIFT;\r\noffset = payload & ~PAGE_MASK;\r\nrest = packet->payload_length;\r\nz = DIV_ROUND_UP(offset + rest, PAGE_SIZE);\r\nif (WARN_ON(offset & 3 || rest & 3 || page + z > buffer->page_count))\r\nreturn -EFAULT;\r\nfor (i = 0; i < z; i++) {\r\nd = context_get_descriptors(&ctx->context, 1, &d_bus);\r\nif (d == NULL)\r\nreturn -ENOMEM;\r\nd->control = cpu_to_le16(DESCRIPTOR_INPUT_MORE |\r\nDESCRIPTOR_BRANCH_ALWAYS);\r\nif (packet->skip && i == 0)\r\nd->control |= cpu_to_le16(DESCRIPTOR_WAIT);\r\nif (packet->interrupt && i == z - 1)\r\nd->control |= cpu_to_le16(DESCRIPTOR_IRQ_ALWAYS);\r\nif (offset + rest < PAGE_SIZE)\r\nlength = rest;\r\nelse\r\nlength = PAGE_SIZE - offset;\r\nd->req_count = cpu_to_le16(length);\r\nd->res_count = d->req_count;\r\nd->transfer_status = 0;\r\npage_bus = page_private(buffer->pages[page]);\r\nd->data_address = cpu_to_le32(page_bus + offset);\r\ndma_sync_single_range_for_device(ctx->context.ohci->card.device,\r\npage_bus, offset, length,\r\nDMA_FROM_DEVICE);\r\nrest -= length;\r\noffset = 0;\r\npage++;\r\ncontext_append(&ctx->context, d, 1, 0);\r\n}\r\nreturn 0;\r\n}\r\nstatic int ohci_queue_iso(struct fw_iso_context *base,\r\nstruct fw_iso_packet *packet,\r\nstruct fw_iso_buffer *buffer,\r\nunsigned long payload)\r\n{\r\nstruct iso_context *ctx = container_of(base, struct iso_context, base);\r\nunsigned long flags;\r\nint ret = -ENOSYS;\r\nspin_lock_irqsave(&ctx->context.ohci->lock, flags);\r\nswitch (base->type) {\r\ncase FW_ISO_CONTEXT_TRANSMIT:\r\nret = queue_iso_transmit(ctx, packet, buffer, payload);\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE:\r\nret = queue_iso_packet_per_buffer(ctx, packet, buffer, payload);\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\nret = queue_iso_buffer_fill(ctx, packet, buffer, payload);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ctx->context.ohci->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void ohci_flush_queue_iso(struct fw_iso_context *base)\r\n{\r\nstruct context *ctx =\r\n&container_of(base, struct iso_context, base)->context;\r\nreg_write(ctx->ohci, CONTROL_SET(ctx->regs), CONTEXT_WAKE);\r\n}\r\nstatic int ohci_flush_iso_completions(struct fw_iso_context *base)\r\n{\r\nstruct iso_context *ctx = container_of(base, struct iso_context, base);\r\nint ret = 0;\r\ntasklet_disable(&ctx->context.tasklet);\r\nif (!test_and_set_bit_lock(0, &ctx->flushing_completions)) {\r\ncontext_tasklet((unsigned long)&ctx->context);\r\nswitch (base->type) {\r\ncase FW_ISO_CONTEXT_TRANSMIT:\r\ncase FW_ISO_CONTEXT_RECEIVE:\r\nif (ctx->header_length != 0)\r\nflush_iso_completions(ctx);\r\nbreak;\r\ncase FW_ISO_CONTEXT_RECEIVE_MULTICHANNEL:\r\nif (ctx->mc_completed != 0)\r\nflush_ir_buffer_fill(ctx);\r\nbreak;\r\ndefault:\r\nret = -ENOSYS;\r\n}\r\nclear_bit_unlock(0, &ctx->flushing_completions);\r\nsmp_mb__after_atomic();\r\n}\r\ntasklet_enable(&ctx->context.tasklet);\r\nreturn ret;\r\n}\r\nstatic void pmac_ohci_on(struct pci_dev *dev)\r\n{\r\nif (machine_is(powermac)) {\r\nstruct device_node *ofn = pci_device_to_OF_node(dev);\r\nif (ofn) {\r\npmac_call_feature(PMAC_FTR_1394_CABLE_POWER, ofn, 0, 1);\r\npmac_call_feature(PMAC_FTR_1394_ENABLE, ofn, 0, 1);\r\n}\r\n}\r\n}\r\nstatic void pmac_ohci_off(struct pci_dev *dev)\r\n{\r\nif (machine_is(powermac)) {\r\nstruct device_node *ofn = pci_device_to_OF_node(dev);\r\nif (ofn) {\r\npmac_call_feature(PMAC_FTR_1394_ENABLE, ofn, 0, 0);\r\npmac_call_feature(PMAC_FTR_1394_CABLE_POWER, ofn, 0, 0);\r\n}\r\n}\r\n}\r\nstatic inline void pmac_ohci_on(struct pci_dev *dev) {}\r\nstatic inline void pmac_ohci_off(struct pci_dev *dev) {}\r\nstatic int pci_probe(struct pci_dev *dev,\r\nconst struct pci_device_id *ent)\r\n{\r\nstruct fw_ohci *ohci;\r\nu32 bus_options, max_receive, link_speed, version;\r\nu64 guid;\r\nint i, err;\r\nsize_t size;\r\nif (dev->vendor == PCI_VENDOR_ID_PINNACLE_SYSTEMS) {\r\ndev_err(&dev->dev, "Pinnacle MovieBoard is not yet supported\n");\r\nreturn -ENOSYS;\r\n}\r\nohci = kzalloc(sizeof(*ohci), GFP_KERNEL);\r\nif (ohci == NULL) {\r\nerr = -ENOMEM;\r\ngoto fail;\r\n}\r\nfw_card_initialize(&ohci->card, &ohci_driver, &dev->dev);\r\npmac_ohci_on(dev);\r\nerr = pci_enable_device(dev);\r\nif (err) {\r\ndev_err(&dev->dev, "failed to enable OHCI hardware\n");\r\ngoto fail_free;\r\n}\r\npci_set_master(dev);\r\npci_write_config_dword(dev, OHCI1394_PCI_HCI_Control, 0);\r\npci_set_drvdata(dev, ohci);\r\nspin_lock_init(&ohci->lock);\r\nmutex_init(&ohci->phy_reg_mutex);\r\nINIT_WORK(&ohci->bus_reset_work, bus_reset_work);\r\nif (!(pci_resource_flags(dev, 0) & IORESOURCE_MEM) ||\r\npci_resource_len(dev, 0) < OHCI1394_REGISTER_SIZE) {\r\nohci_err(ohci, "invalid MMIO resource\n");\r\nerr = -ENXIO;\r\ngoto fail_disable;\r\n}\r\nerr = pci_request_region(dev, 0, ohci_driver_name);\r\nif (err) {\r\nohci_err(ohci, "MMIO resource unavailable\n");\r\ngoto fail_disable;\r\n}\r\nohci->registers = pci_iomap(dev, 0, OHCI1394_REGISTER_SIZE);\r\nif (ohci->registers == NULL) {\r\nohci_err(ohci, "failed to remap registers\n");\r\nerr = -ENXIO;\r\ngoto fail_iomem;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(ohci_quirks); i++)\r\nif ((ohci_quirks[i].vendor == dev->vendor) &&\r\n(ohci_quirks[i].device == (unsigned short)PCI_ANY_ID ||\r\nohci_quirks[i].device == dev->device) &&\r\n(ohci_quirks[i].revision == (unsigned short)PCI_ANY_ID ||\r\nohci_quirks[i].revision >= dev->revision)) {\r\nohci->quirks = ohci_quirks[i].flags;\r\nbreak;\r\n}\r\nif (param_quirks)\r\nohci->quirks = param_quirks;\r\nBUILD_BUG_ON(AR_BUFFERS * sizeof(struct descriptor) > PAGE_SIZE/4);\r\nBUILD_BUG_ON(SELF_ID_BUF_SIZE > PAGE_SIZE/2);\r\nohci->misc_buffer = dma_alloc_coherent(ohci->card.device,\r\nPAGE_SIZE,\r\n&ohci->misc_buffer_bus,\r\nGFP_KERNEL);\r\nif (!ohci->misc_buffer) {\r\nerr = -ENOMEM;\r\ngoto fail_iounmap;\r\n}\r\nerr = ar_context_init(&ohci->ar_request_ctx, ohci, 0,\r\nOHCI1394_AsReqRcvContextControlSet);\r\nif (err < 0)\r\ngoto fail_misc_buf;\r\nerr = ar_context_init(&ohci->ar_response_ctx, ohci, PAGE_SIZE/4,\r\nOHCI1394_AsRspRcvContextControlSet);\r\nif (err < 0)\r\ngoto fail_arreq_ctx;\r\nerr = context_init(&ohci->at_request_ctx, ohci,\r\nOHCI1394_AsReqTrContextControlSet, handle_at_packet);\r\nif (err < 0)\r\ngoto fail_arrsp_ctx;\r\nerr = context_init(&ohci->at_response_ctx, ohci,\r\nOHCI1394_AsRspTrContextControlSet, handle_at_packet);\r\nif (err < 0)\r\ngoto fail_atreq_ctx;\r\nreg_write(ohci, OHCI1394_IsoRecvIntMaskSet, ~0);\r\nohci->ir_context_channels = ~0ULL;\r\nohci->ir_context_support = reg_read(ohci, OHCI1394_IsoRecvIntMaskSet);\r\nreg_write(ohci, OHCI1394_IsoRecvIntMaskClear, ~0);\r\nohci->ir_context_mask = ohci->ir_context_support;\r\nohci->n_ir = hweight32(ohci->ir_context_mask);\r\nsize = sizeof(struct iso_context) * ohci->n_ir;\r\nohci->ir_context_list = kzalloc(size, GFP_KERNEL);\r\nreg_write(ohci, OHCI1394_IsoXmitIntMaskSet, ~0);\r\nohci->it_context_support = reg_read(ohci, OHCI1394_IsoXmitIntMaskSet);\r\nif (!ohci->it_context_support) {\r\nohci_notice(ohci, "overriding IsoXmitIntMask\n");\r\nohci->it_context_support = 0xf;\r\n}\r\nreg_write(ohci, OHCI1394_IsoXmitIntMaskClear, ~0);\r\nohci->it_context_mask = ohci->it_context_support;\r\nohci->n_it = hweight32(ohci->it_context_mask);\r\nsize = sizeof(struct iso_context) * ohci->n_it;\r\nohci->it_context_list = kzalloc(size, GFP_KERNEL);\r\nif (ohci->it_context_list == NULL || ohci->ir_context_list == NULL) {\r\nerr = -ENOMEM;\r\ngoto fail_contexts;\r\n}\r\nohci->self_id = ohci->misc_buffer + PAGE_SIZE/2;\r\nohci->self_id_bus = ohci->misc_buffer_bus + PAGE_SIZE/2;\r\nbus_options = reg_read(ohci, OHCI1394_BusOptions);\r\nmax_receive = (bus_options >> 12) & 0xf;\r\nlink_speed = bus_options & 0x7;\r\nguid = ((u64) reg_read(ohci, OHCI1394_GUIDHi) << 32) |\r\nreg_read(ohci, OHCI1394_GUIDLo);\r\nif (!(ohci->quirks & QUIRK_NO_MSI))\r\npci_enable_msi(dev);\r\nif (request_irq(dev->irq, irq_handler,\r\npci_dev_msi_enabled(dev) ? 0 : IRQF_SHARED,\r\nohci_driver_name, ohci)) {\r\nohci_err(ohci, "failed to allocate interrupt %d\n", dev->irq);\r\nerr = -EIO;\r\ngoto fail_msi;\r\n}\r\nerr = fw_card_add(&ohci->card, max_receive, link_speed, guid);\r\nif (err)\r\ngoto fail_irq;\r\nversion = reg_read(ohci, OHCI1394_Version) & 0x00ff00ff;\r\nohci_notice(ohci,\r\n"added OHCI v%x.%x device as card %d, "\r\n"%d IR + %d IT contexts, quirks 0x%x%s\n",\r\nversion >> 16, version & 0xff, ohci->card.index,\r\nohci->n_ir, ohci->n_it, ohci->quirks,\r\nreg_read(ohci, OHCI1394_PhyUpperBound) ?\r\n", physUB" : "");\r\nreturn 0;\r\nfail_irq:\r\nfree_irq(dev->irq, ohci);\r\nfail_msi:\r\npci_disable_msi(dev);\r\nfail_contexts:\r\nkfree(ohci->ir_context_list);\r\nkfree(ohci->it_context_list);\r\ncontext_release(&ohci->at_response_ctx);\r\nfail_atreq_ctx:\r\ncontext_release(&ohci->at_request_ctx);\r\nfail_arrsp_ctx:\r\nar_context_release(&ohci->ar_response_ctx);\r\nfail_arreq_ctx:\r\nar_context_release(&ohci->ar_request_ctx);\r\nfail_misc_buf:\r\ndma_free_coherent(ohci->card.device, PAGE_SIZE,\r\nohci->misc_buffer, ohci->misc_buffer_bus);\r\nfail_iounmap:\r\npci_iounmap(dev, ohci->registers);\r\nfail_iomem:\r\npci_release_region(dev, 0);\r\nfail_disable:\r\npci_disable_device(dev);\r\nfail_free:\r\nkfree(ohci);\r\npmac_ohci_off(dev);\r\nfail:\r\nreturn err;\r\n}\r\nstatic void pci_remove(struct pci_dev *dev)\r\n{\r\nstruct fw_ohci *ohci = pci_get_drvdata(dev);\r\nif (reg_read(ohci, OHCI1394_HCControlSet) & OHCI1394_HCControl_LPS) {\r\nreg_write(ohci, OHCI1394_IntMaskClear, ~0);\r\nflush_writes(ohci);\r\n}\r\ncancel_work_sync(&ohci->bus_reset_work);\r\nfw_core_remove_card(&ohci->card);\r\nsoftware_reset(ohci);\r\nfree_irq(dev->irq, ohci);\r\nif (ohci->next_config_rom && ohci->next_config_rom != ohci->config_rom)\r\ndma_free_coherent(ohci->card.device, CONFIG_ROM_SIZE,\r\nohci->next_config_rom, ohci->next_config_rom_bus);\r\nif (ohci->config_rom)\r\ndma_free_coherent(ohci->card.device, CONFIG_ROM_SIZE,\r\nohci->config_rom, ohci->config_rom_bus);\r\nar_context_release(&ohci->ar_request_ctx);\r\nar_context_release(&ohci->ar_response_ctx);\r\ndma_free_coherent(ohci->card.device, PAGE_SIZE,\r\nohci->misc_buffer, ohci->misc_buffer_bus);\r\ncontext_release(&ohci->at_request_ctx);\r\ncontext_release(&ohci->at_response_ctx);\r\nkfree(ohci->it_context_list);\r\nkfree(ohci->ir_context_list);\r\npci_disable_msi(dev);\r\npci_iounmap(dev, ohci->registers);\r\npci_release_region(dev, 0);\r\npci_disable_device(dev);\r\nkfree(ohci);\r\npmac_ohci_off(dev);\r\ndev_notice(&dev->dev, "removed fw-ohci device\n");\r\n}\r\nstatic int pci_suspend(struct pci_dev *dev, pm_message_t state)\r\n{\r\nstruct fw_ohci *ohci = pci_get_drvdata(dev);\r\nint err;\r\nsoftware_reset(ohci);\r\nerr = pci_save_state(dev);\r\nif (err) {\r\nohci_err(ohci, "pci_save_state failed\n");\r\nreturn err;\r\n}\r\nerr = pci_set_power_state(dev, pci_choose_state(dev, state));\r\nif (err)\r\nohci_err(ohci, "pci_set_power_state failed with %d\n", err);\r\npmac_ohci_off(dev);\r\nreturn 0;\r\n}\r\nstatic int pci_resume(struct pci_dev *dev)\r\n{\r\nstruct fw_ohci *ohci = pci_get_drvdata(dev);\r\nint err;\r\npmac_ohci_on(dev);\r\npci_set_power_state(dev, PCI_D0);\r\npci_restore_state(dev);\r\nerr = pci_enable_device(dev);\r\nif (err) {\r\nohci_err(ohci, "pci_enable_device failed\n");\r\nreturn err;\r\n}\r\nif (!reg_read(ohci, OHCI1394_GUIDLo) &&\r\n!reg_read(ohci, OHCI1394_GUIDHi)) {\r\nreg_write(ohci, OHCI1394_GUIDLo, (u32)ohci->card.guid);\r\nreg_write(ohci, OHCI1394_GUIDHi, (u32)(ohci->card.guid >> 32));\r\n}\r\nerr = ohci_enable(&ohci->card, NULL, 0);\r\nif (err)\r\nreturn err;\r\nohci_resume_iso_dma(ohci);\r\nreturn 0;\r\n}\r\nstatic int __init fw_ohci_init(void)\r\n{\r\nselfid_workqueue = alloc_workqueue(KBUILD_MODNAME, WQ_MEM_RECLAIM, 0);\r\nif (!selfid_workqueue)\r\nreturn -ENOMEM;\r\nreturn pci_register_driver(&fw_ohci_pci_driver);\r\n}\r\nstatic void __exit fw_ohci_cleanup(void)\r\n{\r\npci_unregister_driver(&fw_ohci_pci_driver);\r\ndestroy_workqueue(selfid_workqueue);\r\n}
