static inline int gred_wred_mode(struct gred_sched *table)\r\n{\r\nreturn test_bit(GRED_WRED_MODE, &table->flags);\r\n}\r\nstatic inline void gred_enable_wred_mode(struct gred_sched *table)\r\n{\r\n__set_bit(GRED_WRED_MODE, &table->flags);\r\n}\r\nstatic inline void gred_disable_wred_mode(struct gred_sched *table)\r\n{\r\n__clear_bit(GRED_WRED_MODE, &table->flags);\r\n}\r\nstatic inline int gred_rio_mode(struct gred_sched *table)\r\n{\r\nreturn test_bit(GRED_RIO_MODE, &table->flags);\r\n}\r\nstatic inline void gred_enable_rio_mode(struct gred_sched *table)\r\n{\r\n__set_bit(GRED_RIO_MODE, &table->flags);\r\n}\r\nstatic inline void gred_disable_rio_mode(struct gred_sched *table)\r\n{\r\n__clear_bit(GRED_RIO_MODE, &table->flags);\r\n}\r\nstatic inline int gred_wred_mode_check(struct Qdisc *sch)\r\n{\r\nstruct gred_sched *table = qdisc_priv(sch);\r\nint i;\r\nfor (i = 0; i < table->DPs; i++) {\r\nstruct gred_sched_data *q = table->tab[i];\r\nint n;\r\nif (q == NULL)\r\ncontinue;\r\nfor (n = i + 1; n < table->DPs; n++)\r\nif (table->tab[n] && table->tab[n]->prio == q->prio)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline unsigned int gred_backlog(struct gred_sched *table,\r\nstruct gred_sched_data *q,\r\nstruct Qdisc *sch)\r\n{\r\nif (gred_wred_mode(table))\r\nreturn sch->qstats.backlog;\r\nelse\r\nreturn q->backlog;\r\n}\r\nstatic inline u16 tc_index_to_dp(struct sk_buff *skb)\r\n{\r\nreturn skb->tc_index & GRED_VQ_MASK;\r\n}\r\nstatic inline void gred_load_wred_set(const struct gred_sched *table,\r\nstruct gred_sched_data *q)\r\n{\r\nq->vars.qavg = table->wred_set.qavg;\r\nq->vars.qidlestart = table->wred_set.qidlestart;\r\n}\r\nstatic inline void gred_store_wred_set(struct gred_sched *table,\r\nstruct gred_sched_data *q)\r\n{\r\ntable->wred_set.qavg = q->vars.qavg;\r\ntable->wred_set.qidlestart = q->vars.qidlestart;\r\n}\r\nstatic inline int gred_use_ecn(struct gred_sched *t)\r\n{\r\nreturn t->red_flags & TC_RED_ECN;\r\n}\r\nstatic inline int gred_use_harddrop(struct gred_sched *t)\r\n{\r\nreturn t->red_flags & TC_RED_HARDDROP;\r\n}\r\nstatic int gred_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct gred_sched_data *q = NULL;\r\nstruct gred_sched *t = qdisc_priv(sch);\r\nunsigned long qavg = 0;\r\nu16 dp = tc_index_to_dp(skb);\r\nif (dp >= t->DPs || (q = t->tab[dp]) == NULL) {\r\ndp = t->def;\r\nq = t->tab[dp];\r\nif (!q) {\r\nif (likely(sch->qstats.backlog + qdisc_pkt_len(skb) <=\r\nsch->limit))\r\nreturn qdisc_enqueue_tail(skb, sch);\r\nelse\r\ngoto drop;\r\n}\r\nskb->tc_index = (skb->tc_index & ~GRED_VQ_MASK) | dp;\r\n}\r\nif (!gred_wred_mode(t) && gred_rio_mode(t)) {\r\nint i;\r\nfor (i = 0; i < t->DPs; i++) {\r\nif (t->tab[i] && t->tab[i]->prio < q->prio &&\r\n!red_is_idling(&t->tab[i]->vars))\r\nqavg += t->tab[i]->vars.qavg;\r\n}\r\n}\r\nq->packetsin++;\r\nq->bytesin += qdisc_pkt_len(skb);\r\nif (gred_wred_mode(t))\r\ngred_load_wred_set(t, q);\r\nq->vars.qavg = red_calc_qavg(&q->parms,\r\n&q->vars,\r\ngred_backlog(t, q, sch));\r\nif (red_is_idling(&q->vars))\r\nred_end_of_idle_period(&q->vars);\r\nif (gred_wred_mode(t))\r\ngred_store_wred_set(t, q);\r\nswitch (red_action(&q->parms, &q->vars, q->vars.qavg + qavg)) {\r\ncase RED_DONT_MARK:\r\nbreak;\r\ncase RED_PROB_MARK:\r\nqdisc_qstats_overlimit(sch);\r\nif (!gred_use_ecn(t) || !INET_ECN_set_ce(skb)) {\r\nq->stats.prob_drop++;\r\ngoto congestion_drop;\r\n}\r\nq->stats.prob_mark++;\r\nbreak;\r\ncase RED_HARD_MARK:\r\nqdisc_qstats_overlimit(sch);\r\nif (gred_use_harddrop(t) || !gred_use_ecn(t) ||\r\n!INET_ECN_set_ce(skb)) {\r\nq->stats.forced_drop++;\r\ngoto congestion_drop;\r\n}\r\nq->stats.forced_mark++;\r\nbreak;\r\n}\r\nif (gred_backlog(t, q, sch) + qdisc_pkt_len(skb) <= q->limit) {\r\nq->backlog += qdisc_pkt_len(skb);\r\nreturn qdisc_enqueue_tail(skb, sch);\r\n}\r\nq->stats.pdrop++;\r\ndrop:\r\nreturn qdisc_drop(skb, sch);\r\ncongestion_drop:\r\nqdisc_drop(skb, sch);\r\nreturn NET_XMIT_CN;\r\n}\r\nstatic struct sk_buff *gred_dequeue(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nstruct gred_sched *t = qdisc_priv(sch);\r\nskb = qdisc_dequeue_head(sch);\r\nif (skb) {\r\nstruct gred_sched_data *q;\r\nu16 dp = tc_index_to_dp(skb);\r\nif (dp >= t->DPs || (q = t->tab[dp]) == NULL) {\r\nnet_warn_ratelimited("GRED: Unable to relocate VQ 0x%x after dequeue, screwing up backlog\n",\r\ntc_index_to_dp(skb));\r\n} else {\r\nq->backlog -= qdisc_pkt_len(skb);\r\nif (gred_wred_mode(t)) {\r\nif (!sch->qstats.backlog)\r\nred_start_of_idle_period(&t->wred_set);\r\n} else {\r\nif (!q->backlog)\r\nred_start_of_idle_period(&q->vars);\r\n}\r\n}\r\nreturn skb;\r\n}\r\nreturn NULL;\r\n}\r\nstatic unsigned int gred_drop(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nstruct gred_sched *t = qdisc_priv(sch);\r\nskb = qdisc_dequeue_tail(sch);\r\nif (skb) {\r\nunsigned int len = qdisc_pkt_len(skb);\r\nstruct gred_sched_data *q;\r\nu16 dp = tc_index_to_dp(skb);\r\nif (dp >= t->DPs || (q = t->tab[dp]) == NULL) {\r\nnet_warn_ratelimited("GRED: Unable to relocate VQ 0x%x while dropping, screwing up backlog\n",\r\ntc_index_to_dp(skb));\r\n} else {\r\nq->backlog -= len;\r\nq->stats.other++;\r\nif (gred_wred_mode(t)) {\r\nif (!sch->qstats.backlog)\r\nred_start_of_idle_period(&t->wred_set);\r\n} else {\r\nif (!q->backlog)\r\nred_start_of_idle_period(&q->vars);\r\n}\r\n}\r\nqdisc_drop(skb, sch);\r\nreturn len;\r\n}\r\nreturn 0;\r\n}\r\nstatic void gred_reset(struct Qdisc *sch)\r\n{\r\nint i;\r\nstruct gred_sched *t = qdisc_priv(sch);\r\nqdisc_reset_queue(sch);\r\nfor (i = 0; i < t->DPs; i++) {\r\nstruct gred_sched_data *q = t->tab[i];\r\nif (!q)\r\ncontinue;\r\nred_restart(&q->vars);\r\nq->backlog = 0;\r\n}\r\n}\r\nstatic inline void gred_destroy_vq(struct gred_sched_data *q)\r\n{\r\nkfree(q);\r\n}\r\nstatic inline int gred_change_table_def(struct Qdisc *sch, struct nlattr *dps)\r\n{\r\nstruct gred_sched *table = qdisc_priv(sch);\r\nstruct tc_gred_sopt *sopt;\r\nint i;\r\nif (dps == NULL)\r\nreturn -EINVAL;\r\nsopt = nla_data(dps);\r\nif (sopt->DPs > MAX_DPs || sopt->DPs == 0 || sopt->def_DP >= sopt->DPs)\r\nreturn -EINVAL;\r\nsch_tree_lock(sch);\r\ntable->DPs = sopt->DPs;\r\ntable->def = sopt->def_DP;\r\ntable->red_flags = sopt->flags;\r\nsch_tree_unlock(sch);\r\nif (sopt->grio) {\r\ngred_enable_rio_mode(table);\r\ngred_disable_wred_mode(table);\r\nif (gred_wred_mode_check(sch))\r\ngred_enable_wred_mode(table);\r\n} else {\r\ngred_disable_rio_mode(table);\r\ngred_disable_wred_mode(table);\r\n}\r\nfor (i = table->DPs; i < MAX_DPs; i++) {\r\nif (table->tab[i]) {\r\npr_warn("GRED: Warning: Destroying shadowed VQ 0x%x\n",\r\ni);\r\ngred_destroy_vq(table->tab[i]);\r\ntable->tab[i] = NULL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int gred_change_vq(struct Qdisc *sch, int dp,\r\nstruct tc_gred_qopt *ctl, int prio,\r\nu8 *stab, u32 max_P,\r\nstruct gred_sched_data **prealloc)\r\n{\r\nstruct gred_sched *table = qdisc_priv(sch);\r\nstruct gred_sched_data *q = table->tab[dp];\r\nif (!q) {\r\ntable->tab[dp] = q = *prealloc;\r\n*prealloc = NULL;\r\nif (!q)\r\nreturn -ENOMEM;\r\n}\r\nq->DP = dp;\r\nq->prio = prio;\r\nif (ctl->limit > sch->limit)\r\nq->limit = sch->limit;\r\nelse\r\nq->limit = ctl->limit;\r\nif (q->backlog == 0)\r\nred_end_of_idle_period(&q->vars);\r\nred_set_parms(&q->parms,\r\nctl->qth_min, ctl->qth_max, ctl->Wlog, ctl->Plog,\r\nctl->Scell_log, stab, max_P);\r\nred_set_vars(&q->vars);\r\nreturn 0;\r\n}\r\nstatic int gred_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct gred_sched *table = qdisc_priv(sch);\r\nstruct tc_gred_qopt *ctl;\r\nstruct nlattr *tb[TCA_GRED_MAX + 1];\r\nint err, prio = GRED_DEF_PRIO;\r\nu8 *stab;\r\nu32 max_P;\r\nstruct gred_sched_data *prealloc;\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_GRED_MAX, opt, gred_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_GRED_PARMS] == NULL && tb[TCA_GRED_STAB] == NULL) {\r\nif (tb[TCA_GRED_LIMIT] != NULL)\r\nsch->limit = nla_get_u32(tb[TCA_GRED_LIMIT]);\r\nreturn gred_change_table_def(sch, opt);\r\n}\r\nif (tb[TCA_GRED_PARMS] == NULL ||\r\ntb[TCA_GRED_STAB] == NULL ||\r\ntb[TCA_GRED_LIMIT] != NULL)\r\nreturn -EINVAL;\r\nmax_P = tb[TCA_GRED_MAX_P] ? nla_get_u32(tb[TCA_GRED_MAX_P]) : 0;\r\nerr = -EINVAL;\r\nctl = nla_data(tb[TCA_GRED_PARMS]);\r\nstab = nla_data(tb[TCA_GRED_STAB]);\r\nif (ctl->DP >= table->DPs)\r\ngoto errout;\r\nif (gred_rio_mode(table)) {\r\nif (ctl->prio == 0) {\r\nint def_prio = GRED_DEF_PRIO;\r\nif (table->tab[table->def])\r\ndef_prio = table->tab[table->def]->prio;\r\nprintk(KERN_DEBUG "GRED: DP %u does not have a prio "\r\n"setting default to %d\n", ctl->DP, def_prio);\r\nprio = def_prio;\r\n} else\r\nprio = ctl->prio;\r\n}\r\nprealloc = kzalloc(sizeof(*prealloc), GFP_KERNEL);\r\nsch_tree_lock(sch);\r\nerr = gred_change_vq(sch, ctl->DP, ctl, prio, stab, max_P, &prealloc);\r\nif (err < 0)\r\ngoto errout_locked;\r\nif (gred_rio_mode(table)) {\r\ngred_disable_wred_mode(table);\r\nif (gred_wred_mode_check(sch))\r\ngred_enable_wred_mode(table);\r\n}\r\nerr = 0;\r\nerrout_locked:\r\nsch_tree_unlock(sch);\r\nkfree(prealloc);\r\nerrout:\r\nreturn err;\r\n}\r\nstatic int gred_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct nlattr *tb[TCA_GRED_MAX + 1];\r\nint err;\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_GRED_MAX, opt, gred_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_GRED_PARMS] || tb[TCA_GRED_STAB])\r\nreturn -EINVAL;\r\nif (tb[TCA_GRED_LIMIT])\r\nsch->limit = nla_get_u32(tb[TCA_GRED_LIMIT]);\r\nelse\r\nsch->limit = qdisc_dev(sch)->tx_queue_len\r\n* psched_mtu(qdisc_dev(sch));\r\nreturn gred_change_table_def(sch, tb[TCA_GRED_DPS]);\r\n}\r\nstatic int gred_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct gred_sched *table = qdisc_priv(sch);\r\nstruct nlattr *parms, *opts = NULL;\r\nint i;\r\nu32 max_p[MAX_DPs];\r\nstruct tc_gred_sopt sopt = {\r\n.DPs = table->DPs,\r\n.def_DP = table->def,\r\n.grio = gred_rio_mode(table),\r\n.flags = table->red_flags,\r\n};\r\nopts = nla_nest_start(skb, TCA_OPTIONS);\r\nif (opts == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put(skb, TCA_GRED_DPS, sizeof(sopt), &sopt))\r\ngoto nla_put_failure;\r\nfor (i = 0; i < MAX_DPs; i++) {\r\nstruct gred_sched_data *q = table->tab[i];\r\nmax_p[i] = q ? q->parms.max_P : 0;\r\n}\r\nif (nla_put(skb, TCA_GRED_MAX_P, sizeof(max_p), max_p))\r\ngoto nla_put_failure;\r\nif (nla_put_u32(skb, TCA_GRED_LIMIT, sch->limit))\r\ngoto nla_put_failure;\r\nparms = nla_nest_start(skb, TCA_GRED_PARMS);\r\nif (parms == NULL)\r\ngoto nla_put_failure;\r\nfor (i = 0; i < MAX_DPs; i++) {\r\nstruct gred_sched_data *q = table->tab[i];\r\nstruct tc_gred_qopt opt;\r\nunsigned long qavg;\r\nmemset(&opt, 0, sizeof(opt));\r\nif (!q) {\r\nopt.DP = MAX_DPs + i;\r\ngoto append_opt;\r\n}\r\nopt.limit = q->limit;\r\nopt.DP = q->DP;\r\nopt.backlog = gred_backlog(table, q, sch);\r\nopt.prio = q->prio;\r\nopt.qth_min = q->parms.qth_min >> q->parms.Wlog;\r\nopt.qth_max = q->parms.qth_max >> q->parms.Wlog;\r\nopt.Wlog = q->parms.Wlog;\r\nopt.Plog = q->parms.Plog;\r\nopt.Scell_log = q->parms.Scell_log;\r\nopt.other = q->stats.other;\r\nopt.early = q->stats.prob_drop;\r\nopt.forced = q->stats.forced_drop;\r\nopt.pdrop = q->stats.pdrop;\r\nopt.packets = q->packetsin;\r\nopt.bytesin = q->bytesin;\r\nif (gred_wred_mode(table))\r\ngred_load_wred_set(table, q);\r\nqavg = red_calc_qavg(&q->parms, &q->vars,\r\nq->vars.qavg >> q->parms.Wlog);\r\nopt.qave = qavg >> q->parms.Wlog;\r\nappend_opt:\r\nif (nla_append(skb, sizeof(opt), &opt) < 0)\r\ngoto nla_put_failure;\r\n}\r\nnla_nest_end(skb, parms);\r\nreturn nla_nest_end(skb, opts);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, opts);\r\nreturn -EMSGSIZE;\r\n}\r\nstatic void gred_destroy(struct Qdisc *sch)\r\n{\r\nstruct gred_sched *table = qdisc_priv(sch);\r\nint i;\r\nfor (i = 0; i < table->DPs; i++) {\r\nif (table->tab[i])\r\ngred_destroy_vq(table->tab[i]);\r\n}\r\n}\r\nstatic int __init gred_module_init(void)\r\n{\r\nreturn register_qdisc(&gred_qdisc_ops);\r\n}\r\nstatic void __exit gred_module_exit(void)\r\n{\r\nunregister_qdisc(&gred_qdisc_ops);\r\n}
