static const struct kfd_device_info *lookup_device_info(unsigned short did)\r\n{\r\nsize_t i;\r\nfor (i = 0; i < ARRAY_SIZE(supported_devices); i++) {\r\nif (supported_devices[i].did == did) {\r\nBUG_ON(supported_devices[i].device_info == NULL);\r\nreturn supported_devices[i].device_info;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstruct kfd_dev *kgd2kfd_probe(struct kgd_dev *kgd,\r\nstruct pci_dev *pdev, const struct kfd2kgd_calls *f2g)\r\n{\r\nstruct kfd_dev *kfd;\r\nconst struct kfd_device_info *device_info =\r\nlookup_device_info(pdev->device);\r\nif (!device_info)\r\nreturn NULL;\r\nkfd = kzalloc(sizeof(*kfd), GFP_KERNEL);\r\nif (!kfd)\r\nreturn NULL;\r\nkfd->kgd = kgd;\r\nkfd->device_info = device_info;\r\nkfd->pdev = pdev;\r\nkfd->init_complete = false;\r\nkfd->kfd2kgd = f2g;\r\nmutex_init(&kfd->doorbell_mutex);\r\nmemset(&kfd->doorbell_available_index, 0,\r\nsizeof(kfd->doorbell_available_index));\r\nreturn kfd;\r\n}\r\nstatic bool device_iommu_pasid_init(struct kfd_dev *kfd)\r\n{\r\nconst u32 required_iommu_flags = AMD_IOMMU_DEVICE_FLAG_ATS_SUP |\r\nAMD_IOMMU_DEVICE_FLAG_PRI_SUP |\r\nAMD_IOMMU_DEVICE_FLAG_PASID_SUP;\r\nstruct amd_iommu_device_info iommu_info;\r\nunsigned int pasid_limit;\r\nint err;\r\nerr = amd_iommu_device_info(kfd->pdev, &iommu_info);\r\nif (err < 0) {\r\ndev_err(kfd_device,\r\n"error getting iommu info. is the iommu enabled?\n");\r\nreturn false;\r\n}\r\nif ((iommu_info.flags & required_iommu_flags) != required_iommu_flags) {\r\ndev_err(kfd_device, "error required iommu flags ats(%i), pri(%i), pasid(%i)\n",\r\n(iommu_info.flags & AMD_IOMMU_DEVICE_FLAG_ATS_SUP) != 0,\r\n(iommu_info.flags & AMD_IOMMU_DEVICE_FLAG_PRI_SUP) != 0,\r\n(iommu_info.flags & AMD_IOMMU_DEVICE_FLAG_PASID_SUP) != 0);\r\nreturn false;\r\n}\r\npasid_limit = min_t(unsigned int,\r\n(unsigned int)1 << kfd->device_info->max_pasid_bits,\r\niommu_info.max_pasids);\r\npasid_limit = min_t(unsigned int,\r\npasid_limit,\r\nkfd->doorbell_process_limit - 1);\r\nerr = amd_iommu_init_device(kfd->pdev, pasid_limit);\r\nif (err < 0) {\r\ndev_err(kfd_device, "error initializing iommu device\n");\r\nreturn false;\r\n}\r\nif (!kfd_set_pasid_limit(pasid_limit)) {\r\ndev_err(kfd_device, "error setting pasid limit\n");\r\namd_iommu_free_device(kfd->pdev);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic void iommu_pasid_shutdown_callback(struct pci_dev *pdev, int pasid)\r\n{\r\nstruct kfd_dev *dev = kfd_device_by_pci_dev(pdev);\r\nif (dev)\r\nkfd_unbind_process_from_device(dev, pasid);\r\n}\r\nstatic int iommu_invalid_ppr_cb(struct pci_dev *pdev, int pasid,\r\nunsigned long address, u16 flags)\r\n{\r\nstruct kfd_dev *dev;\r\ndev_warn(kfd_device,\r\n"Invalid PPR device %x:%x.%x pasid %d address 0x%lX flags 0x%X",\r\nPCI_BUS_NUM(pdev->devfn),\r\nPCI_SLOT(pdev->devfn),\r\nPCI_FUNC(pdev->devfn),\r\npasid,\r\naddress,\r\nflags);\r\ndev = kfd_device_by_pci_dev(pdev);\r\nBUG_ON(dev == NULL);\r\nkfd_signal_iommu_event(dev, pasid, address,\r\nflags & PPR_FAULT_WRITE, flags & PPR_FAULT_EXEC);\r\nreturn AMD_IOMMU_INV_PRI_RSP_INVALID;\r\n}\r\nbool kgd2kfd_device_init(struct kfd_dev *kfd,\r\nconst struct kgd2kfd_shared_resources *gpu_resources)\r\n{\r\nunsigned int size;\r\nkfd->shared_resources = *gpu_resources;\r\nsize = max_num_of_queues_per_device *\r\nkfd->device_info->mqd_size_aligned;\r\nsize += (KFD_MAX_NUM_OF_PROCESSES * sizeof(struct pm4_map_process) +\r\nmax_num_of_queues_per_device *\r\nsizeof(struct pm4_map_queues) + sizeof(struct pm4_runlist)) * 2;\r\nsize += KFD_KERNEL_QUEUE_SIZE * 2;\r\nsize += 512 * 1024;\r\nif (kfd->kfd2kgd->init_gtt_mem_allocation(\r\nkfd->kgd, size, &kfd->gtt_mem,\r\n&kfd->gtt_start_gpu_addr, &kfd->gtt_start_cpu_ptr)){\r\ndev_err(kfd_device,\r\n"Could not allocate %d bytes for device (%x:%x)\n",\r\nsize, kfd->pdev->vendor, kfd->pdev->device);\r\ngoto out;\r\n}\r\ndev_info(kfd_device,\r\n"Allocated %d bytes on gart for device(%x:%x)\n",\r\nsize, kfd->pdev->vendor, kfd->pdev->device);\r\nif (kfd_gtt_sa_init(kfd, size, 512) != 0) {\r\ndev_err(kfd_device,\r\n"Error initializing gtt sub-allocator\n");\r\ngoto kfd_gtt_sa_init_error;\r\n}\r\nkfd_doorbell_init(kfd);\r\nif (kfd_topology_add_device(kfd) != 0) {\r\ndev_err(kfd_device,\r\n"Error adding device (%x:%x) to topology\n",\r\nkfd->pdev->vendor, kfd->pdev->device);\r\ngoto kfd_topology_add_device_error;\r\n}\r\nif (kfd_interrupt_init(kfd)) {\r\ndev_err(kfd_device,\r\n"Error initializing interrupts for device (%x:%x)\n",\r\nkfd->pdev->vendor, kfd->pdev->device);\r\ngoto kfd_interrupt_error;\r\n}\r\nif (!device_iommu_pasid_init(kfd)) {\r\ndev_err(kfd_device,\r\n"Error initializing iommuv2 for device (%x:%x)\n",\r\nkfd->pdev->vendor, kfd->pdev->device);\r\ngoto device_iommu_pasid_error;\r\n}\r\namd_iommu_set_invalidate_ctx_cb(kfd->pdev,\r\niommu_pasid_shutdown_callback);\r\namd_iommu_set_invalid_ppr_cb(kfd->pdev, iommu_invalid_ppr_cb);\r\nkfd->dqm = device_queue_manager_init(kfd);\r\nif (!kfd->dqm) {\r\ndev_err(kfd_device,\r\n"Error initializing queue manager for device (%x:%x)\n",\r\nkfd->pdev->vendor, kfd->pdev->device);\r\ngoto device_queue_manager_error;\r\n}\r\nif (kfd->dqm->ops.start(kfd->dqm) != 0) {\r\ndev_err(kfd_device,\r\n"Error starting queuen manager for device (%x:%x)\n",\r\nkfd->pdev->vendor, kfd->pdev->device);\r\ngoto dqm_start_error;\r\n}\r\nkfd->dbgmgr = NULL;\r\nkfd->init_complete = true;\r\ndev_info(kfd_device, "added device (%x:%x)\n", kfd->pdev->vendor,\r\nkfd->pdev->device);\r\npr_debug("kfd: Starting kfd with the following scheduling policy %d\n",\r\nsched_policy);\r\ngoto out;\r\ndqm_start_error:\r\ndevice_queue_manager_uninit(kfd->dqm);\r\ndevice_queue_manager_error:\r\namd_iommu_free_device(kfd->pdev);\r\ndevice_iommu_pasid_error:\r\nkfd_interrupt_exit(kfd);\r\nkfd_interrupt_error:\r\nkfd_topology_remove_device(kfd);\r\nkfd_topology_add_device_error:\r\nkfd_gtt_sa_fini(kfd);\r\nkfd_gtt_sa_init_error:\r\nkfd->kfd2kgd->free_gtt_mem(kfd->kgd, kfd->gtt_mem);\r\ndev_err(kfd_device,\r\n"device (%x:%x) NOT added due to errors\n",\r\nkfd->pdev->vendor, kfd->pdev->device);\r\nout:\r\nreturn kfd->init_complete;\r\n}\r\nvoid kgd2kfd_device_exit(struct kfd_dev *kfd)\r\n{\r\nif (kfd->init_complete) {\r\ndevice_queue_manager_uninit(kfd->dqm);\r\namd_iommu_free_device(kfd->pdev);\r\nkfd_interrupt_exit(kfd);\r\nkfd_topology_remove_device(kfd);\r\nkfd_gtt_sa_fini(kfd);\r\nkfd->kfd2kgd->free_gtt_mem(kfd->kgd, kfd->gtt_mem);\r\n}\r\nkfree(kfd);\r\n}\r\nvoid kgd2kfd_suspend(struct kfd_dev *kfd)\r\n{\r\nBUG_ON(kfd == NULL);\r\nif (kfd->init_complete) {\r\nkfd->dqm->ops.stop(kfd->dqm);\r\namd_iommu_set_invalidate_ctx_cb(kfd->pdev, NULL);\r\namd_iommu_set_invalid_ppr_cb(kfd->pdev, NULL);\r\namd_iommu_free_device(kfd->pdev);\r\n}\r\n}\r\nint kgd2kfd_resume(struct kfd_dev *kfd)\r\n{\r\nunsigned int pasid_limit;\r\nint err;\r\nBUG_ON(kfd == NULL);\r\npasid_limit = kfd_get_pasid_limit();\r\nif (kfd->init_complete) {\r\nerr = amd_iommu_init_device(kfd->pdev, pasid_limit);\r\nif (err < 0)\r\nreturn -ENXIO;\r\namd_iommu_set_invalidate_ctx_cb(kfd->pdev,\r\niommu_pasid_shutdown_callback);\r\namd_iommu_set_invalid_ppr_cb(kfd->pdev, iommu_invalid_ppr_cb);\r\nkfd->dqm->ops.start(kfd->dqm);\r\n}\r\nreturn 0;\r\n}\r\nvoid kgd2kfd_interrupt(struct kfd_dev *kfd, const void *ih_ring_entry)\r\n{\r\nif (!kfd->init_complete)\r\nreturn;\r\nspin_lock(&kfd->interrupt_lock);\r\nif (kfd->interrupts_active\r\n&& interrupt_is_wanted(kfd, ih_ring_entry)\r\n&& enqueue_ih_ring_entry(kfd, ih_ring_entry))\r\nschedule_work(&kfd->interrupt_work);\r\nspin_unlock(&kfd->interrupt_lock);\r\n}\r\nstatic int kfd_gtt_sa_init(struct kfd_dev *kfd, unsigned int buf_size,\r\nunsigned int chunk_size)\r\n{\r\nunsigned int num_of_bits;\r\nBUG_ON(!kfd);\r\nBUG_ON(!kfd->gtt_mem);\r\nBUG_ON(buf_size < chunk_size);\r\nBUG_ON(buf_size == 0);\r\nBUG_ON(chunk_size == 0);\r\nkfd->gtt_sa_chunk_size = chunk_size;\r\nkfd->gtt_sa_num_of_chunks = buf_size / chunk_size;\r\nnum_of_bits = kfd->gtt_sa_num_of_chunks / BITS_PER_BYTE;\r\nBUG_ON(num_of_bits == 0);\r\nkfd->gtt_sa_bitmap = kzalloc(num_of_bits, GFP_KERNEL);\r\nif (!kfd->gtt_sa_bitmap)\r\nreturn -ENOMEM;\r\npr_debug("kfd: gtt_sa_num_of_chunks = %d, gtt_sa_bitmap = %p\n",\r\nkfd->gtt_sa_num_of_chunks, kfd->gtt_sa_bitmap);\r\nmutex_init(&kfd->gtt_sa_lock);\r\nreturn 0;\r\n}\r\nstatic void kfd_gtt_sa_fini(struct kfd_dev *kfd)\r\n{\r\nmutex_destroy(&kfd->gtt_sa_lock);\r\nkfree(kfd->gtt_sa_bitmap);\r\n}\r\nstatic inline uint64_t kfd_gtt_sa_calc_gpu_addr(uint64_t start_addr,\r\nunsigned int bit_num,\r\nunsigned int chunk_size)\r\n{\r\nreturn start_addr + bit_num * chunk_size;\r\n}\r\nstatic inline uint32_t *kfd_gtt_sa_calc_cpu_addr(void *start_addr,\r\nunsigned int bit_num,\r\nunsigned int chunk_size)\r\n{\r\nreturn (uint32_t *) ((uint64_t) start_addr + bit_num * chunk_size);\r\n}\r\nint kfd_gtt_sa_allocate(struct kfd_dev *kfd, unsigned int size,\r\nstruct kfd_mem_obj **mem_obj)\r\n{\r\nunsigned int found, start_search, cur_size;\r\nBUG_ON(!kfd);\r\nif (size == 0)\r\nreturn -EINVAL;\r\nif (size > kfd->gtt_sa_num_of_chunks * kfd->gtt_sa_chunk_size)\r\nreturn -ENOMEM;\r\n*mem_obj = kmalloc(sizeof(struct kfd_mem_obj), GFP_KERNEL);\r\nif ((*mem_obj) == NULL)\r\nreturn -ENOMEM;\r\npr_debug("kfd: allocated mem_obj = %p for size = %d\n", *mem_obj, size);\r\nstart_search = 0;\r\nmutex_lock(&kfd->gtt_sa_lock);\r\nkfd_gtt_restart_search:\r\nfound = find_next_zero_bit(kfd->gtt_sa_bitmap,\r\nkfd->gtt_sa_num_of_chunks,\r\nstart_search);\r\npr_debug("kfd: found = %d\n", found);\r\nif (found == kfd->gtt_sa_num_of_chunks)\r\ngoto kfd_gtt_no_free_chunk;\r\n(*mem_obj)->range_start = found;\r\n(*mem_obj)->range_end = found;\r\n(*mem_obj)->gpu_addr = kfd_gtt_sa_calc_gpu_addr(\r\nkfd->gtt_start_gpu_addr,\r\nfound,\r\nkfd->gtt_sa_chunk_size);\r\n(*mem_obj)->cpu_ptr = kfd_gtt_sa_calc_cpu_addr(\r\nkfd->gtt_start_cpu_ptr,\r\nfound,\r\nkfd->gtt_sa_chunk_size);\r\npr_debug("kfd: gpu_addr = %p, cpu_addr = %p\n",\r\n(uint64_t *) (*mem_obj)->gpu_addr, (*mem_obj)->cpu_ptr);\r\nif (size <= kfd->gtt_sa_chunk_size) {\r\npr_debug("kfd: single bit\n");\r\nset_bit(found, kfd->gtt_sa_bitmap);\r\ngoto kfd_gtt_out;\r\n}\r\ncur_size = size - kfd->gtt_sa_chunk_size;\r\ndo {\r\n(*mem_obj)->range_end =\r\nfind_next_zero_bit(kfd->gtt_sa_bitmap,\r\nkfd->gtt_sa_num_of_chunks, ++found);\r\nif ((*mem_obj)->range_end != found) {\r\nstart_search = found;\r\ngoto kfd_gtt_restart_search;\r\n}\r\nif (found == kfd->gtt_sa_num_of_chunks)\r\ngoto kfd_gtt_no_free_chunk;\r\nif (cur_size <= kfd->gtt_sa_chunk_size)\r\ncur_size = 0;\r\nelse\r\ncur_size -= kfd->gtt_sa_chunk_size;\r\n} while (cur_size > 0);\r\npr_debug("kfd: range_start = %d, range_end = %d\n",\r\n(*mem_obj)->range_start, (*mem_obj)->range_end);\r\nfor (found = (*mem_obj)->range_start;\r\nfound <= (*mem_obj)->range_end;\r\nfound++)\r\nset_bit(found, kfd->gtt_sa_bitmap);\r\nkfd_gtt_out:\r\nmutex_unlock(&kfd->gtt_sa_lock);\r\nreturn 0;\r\nkfd_gtt_no_free_chunk:\r\npr_debug("kfd: allocation failed with mem_obj = %p\n", mem_obj);\r\nmutex_unlock(&kfd->gtt_sa_lock);\r\nkfree(mem_obj);\r\nreturn -ENOMEM;\r\n}\r\nint kfd_gtt_sa_free(struct kfd_dev *kfd, struct kfd_mem_obj *mem_obj)\r\n{\r\nunsigned int bit;\r\nBUG_ON(!kfd);\r\nif (!mem_obj)\r\nreturn 0;\r\npr_debug("kfd: free mem_obj = %p, range_start = %d, range_end = %d\n",\r\nmem_obj, mem_obj->range_start, mem_obj->range_end);\r\nmutex_lock(&kfd->gtt_sa_lock);\r\nfor (bit = mem_obj->range_start;\r\nbit <= mem_obj->range_end;\r\nbit++)\r\nclear_bit(bit, kfd->gtt_sa_bitmap);\r\nmutex_unlock(&kfd->gtt_sa_lock);\r\nkfree(mem_obj);\r\nreturn 0;\r\n}
