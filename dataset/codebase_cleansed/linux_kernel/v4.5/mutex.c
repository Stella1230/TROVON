void\r\n__mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)\r\n{\r\natomic_set(&lock->count, 1);\r\nspin_lock_init(&lock->wait_lock);\r\nINIT_LIST_HEAD(&lock->wait_list);\r\nmutex_clear_owner(lock);\r\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\r\nosq_lock_init(&lock->osq);\r\n#endif\r\ndebug_mutex_init(lock, name, key);\r\n}\r\nvoid __sched mutex_lock(struct mutex *lock)\r\n{\r\nmight_sleep();\r\n__mutex_fastpath_lock(&lock->count, __mutex_lock_slowpath);\r\nmutex_set_owner(lock);\r\n}\r\nstatic __always_inline void ww_mutex_lock_acquired(struct ww_mutex *ww,\r\nstruct ww_acquire_ctx *ww_ctx)\r\n{\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(ww->ctx);\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->done_acquire);\r\nif (ww_ctx->contending_lock) {\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock != ww);\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->acquired > 0);\r\nww_ctx->contending_lock = NULL;\r\n}\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->ww_class != ww->ww_class);\r\n#endif\r\nww_ctx->acquired++;\r\n}\r\nstatic __always_inline void\r\nww_mutex_set_context_fastpath(struct ww_mutex *lock,\r\nstruct ww_acquire_ctx *ctx)\r\n{\r\nunsigned long flags;\r\nstruct mutex_waiter *cur;\r\nww_mutex_lock_acquired(lock, ctx);\r\nlock->ctx = ctx;\r\nsmp_mb();\r\nif (likely(atomic_read(&lock->base.count) == 0))\r\nreturn;\r\nspin_lock_mutex(&lock->base.wait_lock, flags);\r\nlist_for_each_entry(cur, &lock->base.wait_list, list) {\r\ndebug_mutex_wake_waiter(&lock->base, cur);\r\nwake_up_process(cur->task);\r\n}\r\nspin_unlock_mutex(&lock->base.wait_lock, flags);\r\n}\r\nstatic __always_inline void\r\nww_mutex_set_context_slowpath(struct ww_mutex *lock,\r\nstruct ww_acquire_ctx *ctx)\r\n{\r\nstruct mutex_waiter *cur;\r\nww_mutex_lock_acquired(lock, ctx);\r\nlock->ctx = ctx;\r\nlist_for_each_entry(cur, &lock->base.wait_list, list) {\r\ndebug_mutex_wake_waiter(&lock->base, cur);\r\nwake_up_process(cur->task);\r\n}\r\n}\r\nstatic noinline\r\nbool mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner)\r\n{\r\nbool ret = true;\r\nrcu_read_lock();\r\nwhile (lock->owner == owner) {\r\nbarrier();\r\nif (!owner->on_cpu || need_resched()) {\r\nret = false;\r\nbreak;\r\n}\r\ncpu_relax_lowlatency();\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic inline int mutex_can_spin_on_owner(struct mutex *lock)\r\n{\r\nstruct task_struct *owner;\r\nint retval = 1;\r\nif (need_resched())\r\nreturn 0;\r\nrcu_read_lock();\r\nowner = READ_ONCE(lock->owner);\r\nif (owner)\r\nretval = owner->on_cpu;\r\nrcu_read_unlock();\r\nreturn retval;\r\n}\r\nstatic inline bool mutex_try_to_acquire(struct mutex *lock)\r\n{\r\nreturn !mutex_is_locked(lock) &&\r\n(atomic_cmpxchg_acquire(&lock->count, 1, 0) == 1);\r\n}\r\nstatic bool mutex_optimistic_spin(struct mutex *lock,\r\nstruct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)\r\n{\r\nstruct task_struct *task = current;\r\nif (!mutex_can_spin_on_owner(lock))\r\ngoto done;\r\nif (!osq_lock(&lock->osq))\r\ngoto done;\r\nwhile (true) {\r\nstruct task_struct *owner;\r\nif (use_ww_ctx && ww_ctx->acquired > 0) {\r\nstruct ww_mutex *ww;\r\nww = container_of(lock, struct ww_mutex, base);\r\nif (READ_ONCE(ww->ctx))\r\nbreak;\r\n}\r\nowner = READ_ONCE(lock->owner);\r\nif (owner && !mutex_spin_on_owner(lock, owner))\r\nbreak;\r\nif (mutex_try_to_acquire(lock)) {\r\nlock_acquired(&lock->dep_map, ip);\r\nif (use_ww_ctx) {\r\nstruct ww_mutex *ww;\r\nww = container_of(lock, struct ww_mutex, base);\r\nww_mutex_set_context_fastpath(ww, ww_ctx);\r\n}\r\nmutex_set_owner(lock);\r\nosq_unlock(&lock->osq);\r\nreturn true;\r\n}\r\nif (!owner && (need_resched() || rt_task(task)))\r\nbreak;\r\ncpu_relax_lowlatency();\r\n}\r\nosq_unlock(&lock->osq);\r\ndone:\r\nif (need_resched()) {\r\n__set_current_state(TASK_RUNNING);\r\nschedule_preempt_disabled();\r\n}\r\nreturn false;\r\n}\r\nstatic bool mutex_optimistic_spin(struct mutex *lock,\r\nstruct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)\r\n{\r\nreturn false;\r\n}\r\nvoid __sched mutex_unlock(struct mutex *lock)\r\n{\r\n#ifndef CONFIG_DEBUG_MUTEXES\r\nmutex_clear_owner(lock);\r\n#endif\r\n__mutex_fastpath_unlock(&lock->count, __mutex_unlock_slowpath);\r\n}\r\nvoid __sched ww_mutex_unlock(struct ww_mutex *lock)\r\n{\r\nif (lock->ctx) {\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);\r\n#endif\r\nif (lock->ctx->acquired > 0)\r\nlock->ctx->acquired--;\r\nlock->ctx = NULL;\r\n}\r\n#ifndef CONFIG_DEBUG_MUTEXES\r\nmutex_clear_owner(&lock->base);\r\n#endif\r\n__mutex_fastpath_unlock(&lock->base.count, __mutex_unlock_slowpath);\r\n}\r\nstatic inline int __sched\r\n__ww_mutex_lock_check_stamp(struct mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nstruct ww_mutex *ww = container_of(lock, struct ww_mutex, base);\r\nstruct ww_acquire_ctx *hold_ctx = READ_ONCE(ww->ctx);\r\nif (!hold_ctx)\r\nreturn 0;\r\nif (unlikely(ctx == hold_ctx))\r\nreturn -EALREADY;\r\nif (ctx->stamp - hold_ctx->stamp <= LONG_MAX &&\r\n(ctx->stamp != hold_ctx->stamp || ctx > hold_ctx)) {\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(ctx->contending_lock);\r\nctx->contending_lock = ww;\r\n#endif\r\nreturn -EDEADLK;\r\n}\r\nreturn 0;\r\n}\r\nstatic __always_inline int __sched\r\n__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,\r\nstruct lockdep_map *nest_lock, unsigned long ip,\r\nstruct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)\r\n{\r\nstruct task_struct *task = current;\r\nstruct mutex_waiter waiter;\r\nunsigned long flags;\r\nint ret;\r\npreempt_disable();\r\nmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\r\nif (mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx)) {\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\nif (!mutex_is_locked(lock) &&\r\n(atomic_xchg_acquire(&lock->count, 0) == 1))\r\ngoto skip_wait;\r\ndebug_mutex_lock_common(lock, &waiter);\r\ndebug_mutex_add_waiter(lock, &waiter, task_thread_info(task));\r\nlist_add_tail(&waiter.list, &lock->wait_list);\r\nwaiter.task = task;\r\nlock_contended(&lock->dep_map, ip);\r\nfor (;;) {\r\nif (atomic_read(&lock->count) >= 0 &&\r\n(atomic_xchg_acquire(&lock->count, -1) == 1))\r\nbreak;\r\nif (unlikely(signal_pending_state(state, task))) {\r\nret = -EINTR;\r\ngoto err;\r\n}\r\nif (use_ww_ctx && ww_ctx->acquired > 0) {\r\nret = __ww_mutex_lock_check_stamp(lock, ww_ctx);\r\nif (ret)\r\ngoto err;\r\n}\r\n__set_task_state(task, state);\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\nschedule_preempt_disabled();\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\n}\r\n__set_task_state(task, TASK_RUNNING);\r\nmutex_remove_waiter(lock, &waiter, current_thread_info());\r\nif (likely(list_empty(&lock->wait_list)))\r\natomic_set(&lock->count, 0);\r\ndebug_mutex_free_waiter(&waiter);\r\nskip_wait:\r\nlock_acquired(&lock->dep_map, ip);\r\nmutex_set_owner(lock);\r\nif (use_ww_ctx) {\r\nstruct ww_mutex *ww = container_of(lock, struct ww_mutex, base);\r\nww_mutex_set_context_slowpath(ww, ww_ctx);\r\n}\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\npreempt_enable();\r\nreturn 0;\r\nerr:\r\nmutex_remove_waiter(lock, &waiter, task_thread_info(task));\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\ndebug_mutex_free_waiter(&waiter);\r\nmutex_release(&lock->dep_map, 1, ip);\r\npreempt_enable();\r\nreturn ret;\r\n}\r\nvoid __sched\r\nmutex_lock_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nmight_sleep();\r\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE,\r\nsubclass, NULL, _RET_IP_, NULL, 0);\r\n}\r\nvoid __sched\r\n_mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)\r\n{\r\nmight_sleep();\r\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE,\r\n0, nest, _RET_IP_, NULL, 0);\r\n}\r\nint __sched\r\nmutex_lock_killable_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nmight_sleep();\r\nreturn __mutex_lock_common(lock, TASK_KILLABLE,\r\nsubclass, NULL, _RET_IP_, NULL, 0);\r\n}\r\nint __sched\r\nmutex_lock_interruptible_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nmight_sleep();\r\nreturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE,\r\nsubclass, NULL, _RET_IP_, NULL, 0);\r\n}\r\nstatic inline int\r\nww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\n#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH\r\nunsigned tmp;\r\nif (ctx->deadlock_inject_countdown-- == 0) {\r\ntmp = ctx->deadlock_inject_interval;\r\nif (tmp > UINT_MAX/4)\r\ntmp = UINT_MAX;\r\nelse\r\ntmp = tmp*2 + tmp + tmp/2;\r\nctx->deadlock_inject_interval = tmp;\r\nctx->deadlock_inject_countdown = tmp;\r\nctx->contending_lock = lock;\r\nww_mutex_unlock(lock);\r\nreturn -EDEADLK;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nint __sched\r\n__ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_lock_common(&lock->base, TASK_UNINTERRUPTIBLE,\r\n0, &ctx->dep_map, _RET_IP_, ctx, 1);\r\nif (!ret && ctx->acquired > 1)\r\nreturn ww_mutex_deadlock_injection(lock, ctx);\r\nreturn ret;\r\n}\r\nint __sched\r\n__ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_lock_common(&lock->base, TASK_INTERRUPTIBLE,\r\n0, &ctx->dep_map, _RET_IP_, ctx, 1);\r\nif (!ret && ctx->acquired > 1)\r\nreturn ww_mutex_deadlock_injection(lock, ctx);\r\nreturn ret;\r\n}\r\nstatic inline void\r\n__mutex_unlock_common_slowpath(struct mutex *lock, int nested)\r\n{\r\nunsigned long flags;\r\nif (__mutex_slowpath_needs_to_unlock())\r\natomic_set(&lock->count, 1);\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\nmutex_release(&lock->dep_map, nested, _RET_IP_);\r\ndebug_mutex_unlock(lock);\r\nif (!list_empty(&lock->wait_list)) {\r\nstruct mutex_waiter *waiter =\r\nlist_entry(lock->wait_list.next,\r\nstruct mutex_waiter, list);\r\ndebug_mutex_wake_waiter(lock, waiter);\r\nwake_up_process(waiter->task);\r\n}\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\n}\r\n__visible void\r\n__mutex_unlock_slowpath(atomic_t *lock_count)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\n__mutex_unlock_common_slowpath(lock, 1);\r\n}\r\nint __sched mutex_lock_interruptible(struct mutex *lock)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_fastpath_lock_retval(&lock->count);\r\nif (likely(!ret)) {\r\nmutex_set_owner(lock);\r\nreturn 0;\r\n} else\r\nreturn __mutex_lock_interruptible_slowpath(lock);\r\n}\r\nint __sched mutex_lock_killable(struct mutex *lock)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_fastpath_lock_retval(&lock->count);\r\nif (likely(!ret)) {\r\nmutex_set_owner(lock);\r\nreturn 0;\r\n} else\r\nreturn __mutex_lock_killable_slowpath(lock);\r\n}\r\n__visible void __sched\r\n__mutex_lock_slowpath(atomic_t *lock_count)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0,\r\nNULL, _RET_IP_, NULL, 0);\r\n}\r\nstatic noinline int __sched\r\n__mutex_lock_killable_slowpath(struct mutex *lock)\r\n{\r\nreturn __mutex_lock_common(lock, TASK_KILLABLE, 0,\r\nNULL, _RET_IP_, NULL, 0);\r\n}\r\nstatic noinline int __sched\r\n__mutex_lock_interruptible_slowpath(struct mutex *lock)\r\n{\r\nreturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE, 0,\r\nNULL, _RET_IP_, NULL, 0);\r\n}\r\nstatic noinline int __sched\r\n__ww_mutex_lock_slowpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nreturn __mutex_lock_common(&lock->base, TASK_UNINTERRUPTIBLE, 0,\r\nNULL, _RET_IP_, ctx, 1);\r\n}\r\nstatic noinline int __sched\r\n__ww_mutex_lock_interruptible_slowpath(struct ww_mutex *lock,\r\nstruct ww_acquire_ctx *ctx)\r\n{\r\nreturn __mutex_lock_common(&lock->base, TASK_INTERRUPTIBLE, 0,\r\nNULL, _RET_IP_, ctx, 1);\r\n}\r\nstatic inline int __mutex_trylock_slowpath(atomic_t *lock_count)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\nunsigned long flags;\r\nint prev;\r\nif (mutex_is_locked(lock))\r\nreturn 0;\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\nprev = atomic_xchg_acquire(&lock->count, -1);\r\nif (likely(prev == 1)) {\r\nmutex_set_owner(lock);\r\nmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\r\n}\r\nif (likely(list_empty(&lock->wait_list)))\r\natomic_set(&lock->count, 0);\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\nreturn prev == 1;\r\n}\r\nint __sched mutex_trylock(struct mutex *lock)\r\n{\r\nint ret;\r\nret = __mutex_fastpath_trylock(&lock->count, __mutex_trylock_slowpath);\r\nif (ret)\r\nmutex_set_owner(lock);\r\nreturn ret;\r\n}\r\nint __sched\r\n__ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_fastpath_lock_retval(&lock->base.count);\r\nif (likely(!ret)) {\r\nww_mutex_set_context_fastpath(lock, ctx);\r\nmutex_set_owner(&lock->base);\r\n} else\r\nret = __ww_mutex_lock_slowpath(lock, ctx);\r\nreturn ret;\r\n}\r\nint __sched\r\n__ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_fastpath_lock_retval(&lock->base.count);\r\nif (likely(!ret)) {\r\nww_mutex_set_context_fastpath(lock, ctx);\r\nmutex_set_owner(&lock->base);\r\n} else\r\nret = __ww_mutex_lock_interruptible_slowpath(lock, ctx);\r\nreturn ret;\r\n}\r\nint atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)\r\n{\r\nif (atomic_add_unless(cnt, -1, 1))\r\nreturn 0;\r\nmutex_lock(lock);\r\nif (!atomic_dec_and_test(cnt)) {\r\nmutex_unlock(lock);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}
