static void ec_bhf_reset(struct ec_bhf_priv *priv)\r\n{\r\niowrite8(0, priv->mac_io + MAC_FRAME_ERR_CNT);\r\niowrite8(0, priv->mac_io + MAC_RX_ERR_CNT);\r\niowrite8(0, priv->mac_io + MAC_CRC_ERR_CNT);\r\niowrite8(0, priv->mac_io + MAC_LNK_LST_ERR_CNT);\r\niowrite32(0, priv->mac_io + MAC_TX_FRAME_CNT);\r\niowrite32(0, priv->mac_io + MAC_RX_FRAME_CNT);\r\niowrite8(0, priv->mac_io + MAC_DROPPED_FRMS);\r\niowrite8(0, priv->fifo_io + FIFO_TX_RESET);\r\niowrite8(0, priv->fifo_io + FIFO_RX_RESET);\r\niowrite8(0, priv->mac_io + MAC_TX_FIFO_LVL);\r\n}\r\nstatic void ec_bhf_send_packet(struct ec_bhf_priv *priv, struct tx_desc *desc)\r\n{\r\nu32 len = le16_to_cpu(desc->header.len) + sizeof(desc->header);\r\nu32 addr = (u8 *)desc - priv->tx_buf.buf;\r\niowrite32((ALIGN(len, 8) << 24) | addr, priv->fifo_io + FIFO_TX_REG);\r\n}\r\nstatic int ec_bhf_desc_sent(struct tx_desc *desc)\r\n{\r\nreturn le32_to_cpu(desc->header.sent) & TX_HDR_SENT;\r\n}\r\nstatic void ec_bhf_process_tx(struct ec_bhf_priv *priv)\r\n{\r\nif (unlikely(netif_queue_stopped(priv->net_dev))) {\r\nsmp_rmb();\r\nif (ec_bhf_desc_sent(&priv->tx_descs[priv->tx_dnext]))\r\nnetif_wake_queue(priv->net_dev);\r\n}\r\n}\r\nstatic int ec_bhf_pkt_received(struct rx_desc *desc)\r\n{\r\nreturn le32_to_cpu(desc->header.recv) & RXHDR_NEXT_RECV_FLAG;\r\n}\r\nstatic void ec_bhf_add_rx_desc(struct ec_bhf_priv *priv, struct rx_desc *desc)\r\n{\r\niowrite32(FIFO_RX_ADDR_VALID | ((u8 *)(desc) - priv->rx_buf.buf),\r\npriv->fifo_io + FIFO_RX_REG);\r\n}\r\nstatic void ec_bhf_process_rx(struct ec_bhf_priv *priv)\r\n{\r\nstruct rx_desc *desc = &priv->rx_descs[priv->rx_dnext];\r\nwhile (ec_bhf_pkt_received(desc)) {\r\nint pkt_size = (le16_to_cpu(desc->header.len) &\r\nRXHDR_LEN_MASK) - sizeof(struct rx_header) - 4;\r\nu8 *data = desc->data;\r\nstruct sk_buff *skb;\r\nskb = netdev_alloc_skb_ip_align(priv->net_dev, pkt_size);\r\nif (skb) {\r\nmemcpy(skb_put(skb, pkt_size), data, pkt_size);\r\nskb->protocol = eth_type_trans(skb, priv->net_dev);\r\npriv->stat_rx_bytes += pkt_size;\r\nnetif_rx(skb);\r\n} else {\r\ndev_err_ratelimited(PRIV_TO_DEV(priv),\r\n"Couldn't allocate a skb_buff for a packet of size %u\n",\r\npkt_size);\r\n}\r\ndesc->header.recv = 0;\r\nec_bhf_add_rx_desc(priv, desc);\r\npriv->rx_dnext = (priv->rx_dnext + 1) % priv->rx_dcount;\r\ndesc = &priv->rx_descs[priv->rx_dnext];\r\n}\r\n}\r\nstatic enum hrtimer_restart ec_bhf_timer_fun(struct hrtimer *timer)\r\n{\r\nstruct ec_bhf_priv *priv = container_of(timer, struct ec_bhf_priv,\r\nhrtimer);\r\nec_bhf_process_rx(priv);\r\nec_bhf_process_tx(priv);\r\nif (!netif_running(priv->net_dev))\r\nreturn HRTIMER_NORESTART;\r\nhrtimer_forward_now(timer, ktime_set(0, polling_frequency));\r\nreturn HRTIMER_RESTART;\r\n}\r\nstatic int ec_bhf_setup_offsets(struct ec_bhf_priv *priv)\r\n{\r\nstruct device *dev = PRIV_TO_DEV(priv);\r\nunsigned block_count, i;\r\nvoid __iomem *ec_info;\r\nblock_count = ioread8(priv->io + INFO_BLOCK_BLK_CNT);\r\nfor (i = 0; i < block_count; i++) {\r\nu16 type = ioread16(priv->io + i * INFO_BLOCK_SIZE +\r\nINFO_BLOCK_TYPE);\r\nif (type == ETHERCAT_MASTER_ID)\r\nbreak;\r\n}\r\nif (i == block_count) {\r\ndev_err(dev, "EtherCAT master with DMA block not found\n");\r\nreturn -ENODEV;\r\n}\r\nec_info = priv->io + i * INFO_BLOCK_SIZE;\r\npriv->tx_dma_chan = ioread8(ec_info + INFO_BLOCK_TX_CHAN);\r\npriv->rx_dma_chan = ioread8(ec_info + INFO_BLOCK_RX_CHAN);\r\npriv->ec_io = priv->io + ioread32(ec_info + INFO_BLOCK_OFFSET);\r\npriv->mii_io = priv->ec_io + ioread32(priv->ec_io + EC_MII_OFFSET);\r\npriv->fifo_io = priv->ec_io + ioread32(priv->ec_io + EC_FIFO_OFFSET);\r\npriv->mac_io = priv->ec_io + ioread32(priv->ec_io + EC_MAC_OFFSET);\r\nreturn 0;\r\n}\r\nstatic netdev_tx_t ec_bhf_start_xmit(struct sk_buff *skb,\r\nstruct net_device *net_dev)\r\n{\r\nstruct ec_bhf_priv *priv = netdev_priv(net_dev);\r\nstruct tx_desc *desc;\r\nunsigned len;\r\ndesc = &priv->tx_descs[priv->tx_dnext];\r\nskb_copy_and_csum_dev(skb, desc->data);\r\nlen = skb->len;\r\nmemset(&desc->header, 0, sizeof(desc->header));\r\ndesc->header.len = cpu_to_le16(len);\r\ndesc->header.port = TX_HDR_PORT_0;\r\nec_bhf_send_packet(priv, desc);\r\npriv->tx_dnext = (priv->tx_dnext + 1) % priv->tx_dcount;\r\nif (!ec_bhf_desc_sent(&priv->tx_descs[priv->tx_dnext])) {\r\nsmp_wmb();\r\nnetif_stop_queue(net_dev);\r\n}\r\npriv->stat_tx_bytes += len;\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int ec_bhf_alloc_dma_mem(struct ec_bhf_priv *priv,\r\nstruct bhf_dma *buf,\r\nint channel,\r\nint size)\r\n{\r\nint offset = channel * DMA_CHAN_SIZE + DMA_CHAN_OFFSET;\r\nstruct device *dev = PRIV_TO_DEV(priv);\r\nu32 mask;\r\niowrite32(0xffffffff, priv->dma_io + offset);\r\nmask = ioread32(priv->dma_io + offset);\r\nmask &= DMA_WINDOW_SIZE_MASK;\r\nbuf->len = min_t(int, ~mask + 1, size);\r\nbuf->alloc_len = 2 * buf->len;\r\nbuf->alloc = dma_alloc_coherent(dev, buf->alloc_len, &buf->alloc_phys,\r\nGFP_KERNEL);\r\nif (buf->alloc == NULL) {\r\ndev_err(dev, "Failed to allocate buffer\n");\r\nreturn -ENOMEM;\r\n}\r\nbuf->buf_phys = (buf->alloc_phys + buf->len) & mask;\r\nbuf->buf = buf->alloc + (buf->buf_phys - buf->alloc_phys);\r\niowrite32(0, priv->dma_io + offset + 4);\r\niowrite32(buf->buf_phys, priv->dma_io + offset);\r\nreturn 0;\r\n}\r\nstatic void ec_bhf_setup_tx_descs(struct ec_bhf_priv *priv)\r\n{\r\nint i = 0;\r\npriv->tx_dcount = priv->tx_buf.len / sizeof(struct tx_desc);\r\npriv->tx_descs = (struct tx_desc *)priv->tx_buf.buf;\r\npriv->tx_dnext = 0;\r\nfor (i = 0; i < priv->tx_dcount; i++)\r\npriv->tx_descs[i].header.sent = cpu_to_le32(TX_HDR_SENT);\r\n}\r\nstatic void ec_bhf_setup_rx_descs(struct ec_bhf_priv *priv)\r\n{\r\nint i;\r\npriv->rx_dcount = priv->rx_buf.len / sizeof(struct rx_desc);\r\npriv->rx_descs = (struct rx_desc *)priv->rx_buf.buf;\r\npriv->rx_dnext = 0;\r\nfor (i = 0; i < priv->rx_dcount; i++) {\r\nstruct rx_desc *desc = &priv->rx_descs[i];\r\nu32 next;\r\nif (i != priv->rx_dcount - 1)\r\nnext = (u8 *)(desc + 1) - priv->rx_buf.buf;\r\nelse\r\nnext = 0;\r\nnext |= RXHDR_NEXT_VALID;\r\ndesc->header.next = cpu_to_le32(next);\r\ndesc->header.recv = 0;\r\nec_bhf_add_rx_desc(priv, desc);\r\n}\r\n}\r\nstatic int ec_bhf_open(struct net_device *net_dev)\r\n{\r\nstruct ec_bhf_priv *priv = netdev_priv(net_dev);\r\nstruct device *dev = PRIV_TO_DEV(priv);\r\nint err = 0;\r\nec_bhf_reset(priv);\r\nerr = ec_bhf_alloc_dma_mem(priv, &priv->rx_buf, priv->rx_dma_chan,\r\nFIFO_SIZE * sizeof(struct rx_desc));\r\nif (err) {\r\ndev_err(dev, "Failed to allocate rx buffer\n");\r\ngoto out;\r\n}\r\nec_bhf_setup_rx_descs(priv);\r\nerr = ec_bhf_alloc_dma_mem(priv, &priv->tx_buf, priv->tx_dma_chan,\r\nFIFO_SIZE * sizeof(struct tx_desc));\r\nif (err) {\r\ndev_err(dev, "Failed to allocate tx buffer\n");\r\ngoto error_rx_free;\r\n}\r\niowrite8(0, priv->mii_io + MII_MAC_FILT_FLAG);\r\nec_bhf_setup_tx_descs(priv);\r\nnetif_start_queue(net_dev);\r\nhrtimer_init(&priv->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\npriv->hrtimer.function = ec_bhf_timer_fun;\r\nhrtimer_start(&priv->hrtimer, ktime_set(0, polling_frequency),\r\nHRTIMER_MODE_REL);\r\nreturn 0;\r\nerror_rx_free:\r\ndma_free_coherent(dev, priv->rx_buf.alloc_len, priv->rx_buf.alloc,\r\npriv->rx_buf.alloc_len);\r\nout:\r\nreturn err;\r\n}\r\nstatic int ec_bhf_stop(struct net_device *net_dev)\r\n{\r\nstruct ec_bhf_priv *priv = netdev_priv(net_dev);\r\nstruct device *dev = PRIV_TO_DEV(priv);\r\nhrtimer_cancel(&priv->hrtimer);\r\nec_bhf_reset(priv);\r\nnetif_tx_disable(net_dev);\r\ndma_free_coherent(dev, priv->tx_buf.alloc_len,\r\npriv->tx_buf.alloc, priv->tx_buf.alloc_phys);\r\ndma_free_coherent(dev, priv->rx_buf.alloc_len,\r\npriv->rx_buf.alloc, priv->rx_buf.alloc_phys);\r\nreturn 0;\r\n}\r\nstatic struct rtnl_link_stats64 *\r\nec_bhf_get_stats(struct net_device *net_dev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct ec_bhf_priv *priv = netdev_priv(net_dev);\r\nstats->rx_errors = ioread8(priv->mac_io + MAC_RX_ERR_CNT) +\r\nioread8(priv->mac_io + MAC_CRC_ERR_CNT) +\r\nioread8(priv->mac_io + MAC_FRAME_ERR_CNT);\r\nstats->rx_packets = ioread32(priv->mac_io + MAC_RX_FRAME_CNT);\r\nstats->tx_packets = ioread32(priv->mac_io + MAC_TX_FRAME_CNT);\r\nstats->rx_dropped = ioread8(priv->mac_io + MAC_DROPPED_FRMS);\r\nstats->tx_bytes = priv->stat_tx_bytes;\r\nstats->rx_bytes = priv->stat_rx_bytes;\r\nreturn stats;\r\n}\r\nstatic int ec_bhf_probe(struct pci_dev *dev, const struct pci_device_id *id)\r\n{\r\nstruct net_device *net_dev;\r\nstruct ec_bhf_priv *priv;\r\nvoid __iomem *dma_io;\r\nvoid __iomem *io;\r\nint err = 0;\r\nerr = pci_enable_device(dev);\r\nif (err)\r\nreturn err;\r\npci_set_master(dev);\r\nerr = pci_set_dma_mask(dev, DMA_BIT_MASK(32));\r\nif (err) {\r\ndev_err(&dev->dev,\r\n"Required dma mask not supported, failed to initialize device\n");\r\nerr = -EIO;\r\ngoto err_disable_dev;\r\n}\r\nerr = pci_set_consistent_dma_mask(dev, DMA_BIT_MASK(32));\r\nif (err) {\r\ndev_err(&dev->dev,\r\n"Required dma mask not supported, failed to initialize device\n");\r\ngoto err_disable_dev;\r\n}\r\nerr = pci_request_regions(dev, "ec_bhf");\r\nif (err) {\r\ndev_err(&dev->dev, "Failed to request pci memory regions\n");\r\ngoto err_disable_dev;\r\n}\r\nio = pci_iomap(dev, 0, 0);\r\nif (!io) {\r\ndev_err(&dev->dev, "Failed to map pci card memory bar 0");\r\nerr = -EIO;\r\ngoto err_release_regions;\r\n}\r\ndma_io = pci_iomap(dev, 2, 0);\r\nif (!dma_io) {\r\ndev_err(&dev->dev, "Failed to map pci card memory bar 2");\r\nerr = -EIO;\r\ngoto err_unmap;\r\n}\r\nnet_dev = alloc_etherdev(sizeof(struct ec_bhf_priv));\r\nif (net_dev == NULL) {\r\nerr = -ENOMEM;\r\ngoto err_unmap_dma_io;\r\n}\r\npci_set_drvdata(dev, net_dev);\r\nSET_NETDEV_DEV(net_dev, &dev->dev);\r\nnet_dev->features = 0;\r\nnet_dev->flags |= IFF_NOARP;\r\nnet_dev->netdev_ops = &ec_bhf_netdev_ops;\r\npriv = netdev_priv(net_dev);\r\npriv->net_dev = net_dev;\r\npriv->io = io;\r\npriv->dma_io = dma_io;\r\npriv->dev = dev;\r\nerr = ec_bhf_setup_offsets(priv);\r\nif (err < 0)\r\ngoto err_free_net_dev;\r\nmemcpy_fromio(net_dev->dev_addr, priv->mii_io + MII_MAC_ADDR, 6);\r\nerr = register_netdev(net_dev);\r\nif (err < 0)\r\ngoto err_free_net_dev;\r\nreturn 0;\r\nerr_free_net_dev:\r\nfree_netdev(net_dev);\r\nerr_unmap_dma_io:\r\npci_iounmap(dev, dma_io);\r\nerr_unmap:\r\npci_iounmap(dev, io);\r\nerr_release_regions:\r\npci_release_regions(dev);\r\nerr_disable_dev:\r\npci_clear_master(dev);\r\npci_disable_device(dev);\r\nreturn err;\r\n}\r\nstatic void ec_bhf_remove(struct pci_dev *dev)\r\n{\r\nstruct net_device *net_dev = pci_get_drvdata(dev);\r\nstruct ec_bhf_priv *priv = netdev_priv(net_dev);\r\nunregister_netdev(net_dev);\r\nfree_netdev(net_dev);\r\npci_iounmap(dev, priv->dma_io);\r\npci_iounmap(dev, priv->io);\r\npci_release_regions(dev);\r\npci_clear_master(dev);\r\npci_disable_device(dev);\r\n}
