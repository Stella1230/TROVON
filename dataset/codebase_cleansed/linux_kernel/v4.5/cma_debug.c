static int cma_debugfs_get(void *data, u64 *val)\r\n{\r\nunsigned long *p = data;\r\n*val = *p;\r\nreturn 0;\r\n}\r\nstatic int cma_used_get(void *data, u64 *val)\r\n{\r\nstruct cma *cma = data;\r\nunsigned long used;\r\nmutex_lock(&cma->lock);\r\nused = bitmap_weight(cma->bitmap, (int)cma_bitmap_maxno(cma));\r\nmutex_unlock(&cma->lock);\r\n*val = (u64)used << cma->order_per_bit;\r\nreturn 0;\r\n}\r\nstatic int cma_maxchunk_get(void *data, u64 *val)\r\n{\r\nstruct cma *cma = data;\r\nunsigned long maxchunk = 0;\r\nunsigned long start, end = 0;\r\nunsigned long bitmap_maxno = cma_bitmap_maxno(cma);\r\nmutex_lock(&cma->lock);\r\nfor (;;) {\r\nstart = find_next_zero_bit(cma->bitmap, bitmap_maxno, end);\r\nif (start >= cma->count)\r\nbreak;\r\nend = find_next_bit(cma->bitmap, bitmap_maxno, start);\r\nmaxchunk = max(end - start, maxchunk);\r\n}\r\nmutex_unlock(&cma->lock);\r\n*val = (u64)maxchunk << cma->order_per_bit;\r\nreturn 0;\r\n}\r\nstatic void cma_add_to_cma_mem_list(struct cma *cma, struct cma_mem *mem)\r\n{\r\nspin_lock(&cma->mem_head_lock);\r\nhlist_add_head(&mem->node, &cma->mem_head);\r\nspin_unlock(&cma->mem_head_lock);\r\n}\r\nstatic struct cma_mem *cma_get_entry_from_list(struct cma *cma)\r\n{\r\nstruct cma_mem *mem = NULL;\r\nspin_lock(&cma->mem_head_lock);\r\nif (!hlist_empty(&cma->mem_head)) {\r\nmem = hlist_entry(cma->mem_head.first, struct cma_mem, node);\r\nhlist_del_init(&mem->node);\r\n}\r\nspin_unlock(&cma->mem_head_lock);\r\nreturn mem;\r\n}\r\nstatic int cma_free_mem(struct cma *cma, int count)\r\n{\r\nstruct cma_mem *mem = NULL;\r\nwhile (count) {\r\nmem = cma_get_entry_from_list(cma);\r\nif (mem == NULL)\r\nreturn 0;\r\nif (mem->n <= count) {\r\ncma_release(cma, mem->p, mem->n);\r\ncount -= mem->n;\r\nkfree(mem);\r\n} else if (cma->order_per_bit == 0) {\r\ncma_release(cma, mem->p, count);\r\nmem->p += count;\r\nmem->n -= count;\r\ncount = 0;\r\ncma_add_to_cma_mem_list(cma, mem);\r\n} else {\r\npr_debug("cma: cannot release partial block when order_per_bit != 0\n");\r\ncma_add_to_cma_mem_list(cma, mem);\r\nbreak;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cma_free_write(void *data, u64 val)\r\n{\r\nint pages = val;\r\nstruct cma *cma = data;\r\nreturn cma_free_mem(cma, pages);\r\n}\r\nstatic int cma_alloc_mem(struct cma *cma, int count)\r\n{\r\nstruct cma_mem *mem;\r\nstruct page *p;\r\nmem = kzalloc(sizeof(*mem), GFP_KERNEL);\r\nif (!mem)\r\nreturn -ENOMEM;\r\np = cma_alloc(cma, count, 0);\r\nif (!p) {\r\nkfree(mem);\r\nreturn -ENOMEM;\r\n}\r\nmem->p = p;\r\nmem->n = count;\r\ncma_add_to_cma_mem_list(cma, mem);\r\nreturn 0;\r\n}\r\nstatic int cma_alloc_write(void *data, u64 val)\r\n{\r\nint pages = val;\r\nstruct cma *cma = data;\r\nreturn cma_alloc_mem(cma, pages);\r\n}\r\nstatic void cma_debugfs_add_one(struct cma *cma, int idx)\r\n{\r\nstruct dentry *tmp;\r\nchar name[16];\r\nint u32s;\r\nsprintf(name, "cma-%d", idx);\r\ntmp = debugfs_create_dir(name, cma_debugfs_root);\r\ndebugfs_create_file("alloc", S_IWUSR, tmp, cma,\r\n&cma_alloc_fops);\r\ndebugfs_create_file("free", S_IWUSR, tmp, cma,\r\n&cma_free_fops);\r\ndebugfs_create_file("base_pfn", S_IRUGO, tmp,\r\n&cma->base_pfn, &cma_debugfs_fops);\r\ndebugfs_create_file("count", S_IRUGO, tmp,\r\n&cma->count, &cma_debugfs_fops);\r\ndebugfs_create_file("order_per_bit", S_IRUGO, tmp,\r\n&cma->order_per_bit, &cma_debugfs_fops);\r\ndebugfs_create_file("used", S_IRUGO, tmp, cma, &cma_used_fops);\r\ndebugfs_create_file("maxchunk", S_IRUGO, tmp, cma, &cma_maxchunk_fops);\r\nu32s = DIV_ROUND_UP(cma_bitmap_maxno(cma), BITS_PER_BYTE * sizeof(u32));\r\ndebugfs_create_u32_array("bitmap", S_IRUGO, tmp, (u32*)cma->bitmap, u32s);\r\n}\r\nstatic int __init cma_debugfs_init(void)\r\n{\r\nint i;\r\ncma_debugfs_root = debugfs_create_dir("cma", NULL);\r\nif (!cma_debugfs_root)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < cma_area_count; i++)\r\ncma_debugfs_add_one(&cma_areas[i], i);\r\nreturn 0;\r\n}
