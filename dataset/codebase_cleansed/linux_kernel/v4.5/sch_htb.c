static inline struct htb_class *htb_find(u32 handle, struct Qdisc *sch)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct Qdisc_class_common *clc;\r\nclc = qdisc_class_find(&q->clhash, handle);\r\nif (clc == NULL)\r\nreturn NULL;\r\nreturn container_of(clc, struct htb_class, common);\r\n}\r\nstatic struct htb_class *htb_classify(struct sk_buff *skb, struct Qdisc *sch,\r\nint *qerr)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct htb_class *cl;\r\nstruct tcf_result res;\r\nstruct tcf_proto *tcf;\r\nint result;\r\nif (skb->priority == sch->handle)\r\nreturn HTB_DIRECT;\r\ncl = htb_find(skb->priority, sch);\r\nif (cl) {\r\nif (cl->level == 0)\r\nreturn cl;\r\ntcf = rcu_dereference_bh(cl->filter_list);\r\n} else {\r\ntcf = rcu_dereference_bh(q->filter_list);\r\n}\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\nwhile (tcf && (result = tc_classify(skb, tcf, &res, false)) >= 0) {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nswitch (result) {\r\ncase TC_ACT_QUEUED:\r\ncase TC_ACT_STOLEN:\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\r\ncase TC_ACT_SHOT:\r\nreturn NULL;\r\n}\r\n#endif\r\ncl = (void *)res.class;\r\nif (!cl) {\r\nif (res.classid == sch->handle)\r\nreturn HTB_DIRECT;\r\ncl = htb_find(res.classid, sch);\r\nif (!cl)\r\nbreak;\r\n}\r\nif (!cl->level)\r\nreturn cl;\r\ntcf = rcu_dereference_bh(cl->filter_list);\r\n}\r\ncl = htb_find(TC_H_MAKE(TC_H_MAJ(sch->handle), q->defcls), sch);\r\nif (!cl || cl->level)\r\nreturn HTB_DIRECT;\r\nreturn cl;\r\n}\r\nstatic void htb_add_to_id_tree(struct rb_root *root,\r\nstruct htb_class *cl, int prio)\r\n{\r\nstruct rb_node **p = &root->rb_node, *parent = NULL;\r\nwhile (*p) {\r\nstruct htb_class *c;\r\nparent = *p;\r\nc = rb_entry(parent, struct htb_class, node[prio]);\r\nif (cl->common.classid > c->common.classid)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nrb_link_node(&cl->node[prio], parent, p);\r\nrb_insert_color(&cl->node[prio], root);\r\n}\r\nstatic void htb_add_to_wait_tree(struct htb_sched *q,\r\nstruct htb_class *cl, s64 delay)\r\n{\r\nstruct rb_node **p = &q->hlevel[cl->level].wait_pq.rb_node, *parent = NULL;\r\ncl->pq_key = q->now + delay;\r\nif (cl->pq_key == q->now)\r\ncl->pq_key++;\r\nif (q->near_ev_cache[cl->level] > cl->pq_key)\r\nq->near_ev_cache[cl->level] = cl->pq_key;\r\nwhile (*p) {\r\nstruct htb_class *c;\r\nparent = *p;\r\nc = rb_entry(parent, struct htb_class, pq_node);\r\nif (cl->pq_key >= c->pq_key)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nrb_link_node(&cl->pq_node, parent, p);\r\nrb_insert_color(&cl->pq_node, &q->hlevel[cl->level].wait_pq);\r\n}\r\nstatic inline void htb_next_rb_node(struct rb_node **n)\r\n{\r\n*n = rb_next(*n);\r\n}\r\nstatic inline void htb_add_class_to_row(struct htb_sched *q,\r\nstruct htb_class *cl, int mask)\r\n{\r\nq->row_mask[cl->level] |= mask;\r\nwhile (mask) {\r\nint prio = ffz(~mask);\r\nmask &= ~(1 << prio);\r\nhtb_add_to_id_tree(&q->hlevel[cl->level].hprio[prio].row, cl, prio);\r\n}\r\n}\r\nstatic void htb_safe_rb_erase(struct rb_node *rb, struct rb_root *root)\r\n{\r\nif (RB_EMPTY_NODE(rb)) {\r\nWARN_ON(1);\r\n} else {\r\nrb_erase(rb, root);\r\nRB_CLEAR_NODE(rb);\r\n}\r\n}\r\nstatic inline void htb_remove_class_from_row(struct htb_sched *q,\r\nstruct htb_class *cl, int mask)\r\n{\r\nint m = 0;\r\nstruct htb_level *hlevel = &q->hlevel[cl->level];\r\nwhile (mask) {\r\nint prio = ffz(~mask);\r\nstruct htb_prio *hprio = &hlevel->hprio[prio];\r\nmask &= ~(1 << prio);\r\nif (hprio->ptr == cl->node + prio)\r\nhtb_next_rb_node(&hprio->ptr);\r\nhtb_safe_rb_erase(cl->node + prio, &hprio->row);\r\nif (!hprio->row.rb_node)\r\nm |= 1 << prio;\r\n}\r\nq->row_mask[cl->level] &= ~m;\r\n}\r\nstatic void htb_activate_prios(struct htb_sched *q, struct htb_class *cl)\r\n{\r\nstruct htb_class *p = cl->parent;\r\nlong m, mask = cl->prio_activity;\r\nwhile (cl->cmode == HTB_MAY_BORROW && p && mask) {\r\nm = mask;\r\nwhile (m) {\r\nint prio = ffz(~m);\r\nm &= ~(1 << prio);\r\nif (p->un.inner.clprio[prio].feed.rb_node)\r\nmask &= ~(1 << prio);\r\nhtb_add_to_id_tree(&p->un.inner.clprio[prio].feed, cl, prio);\r\n}\r\np->prio_activity |= mask;\r\ncl = p;\r\np = cl->parent;\r\n}\r\nif (cl->cmode == HTB_CAN_SEND && mask)\r\nhtb_add_class_to_row(q, cl, mask);\r\n}\r\nstatic void htb_deactivate_prios(struct htb_sched *q, struct htb_class *cl)\r\n{\r\nstruct htb_class *p = cl->parent;\r\nlong m, mask = cl->prio_activity;\r\nwhile (cl->cmode == HTB_MAY_BORROW && p && mask) {\r\nm = mask;\r\nmask = 0;\r\nwhile (m) {\r\nint prio = ffz(~m);\r\nm &= ~(1 << prio);\r\nif (p->un.inner.clprio[prio].ptr == cl->node + prio) {\r\np->un.inner.clprio[prio].last_ptr_id = cl->common.classid;\r\np->un.inner.clprio[prio].ptr = NULL;\r\n}\r\nhtb_safe_rb_erase(cl->node + prio,\r\n&p->un.inner.clprio[prio].feed);\r\nif (!p->un.inner.clprio[prio].feed.rb_node)\r\nmask |= 1 << prio;\r\n}\r\np->prio_activity &= ~mask;\r\ncl = p;\r\np = cl->parent;\r\n}\r\nif (cl->cmode == HTB_CAN_SEND && mask)\r\nhtb_remove_class_from_row(q, cl, mask);\r\n}\r\nstatic inline s64 htb_lowater(const struct htb_class *cl)\r\n{\r\nif (htb_hysteresis)\r\nreturn cl->cmode != HTB_CANT_SEND ? -cl->cbuffer : 0;\r\nelse\r\nreturn 0;\r\n}\r\nstatic inline s64 htb_hiwater(const struct htb_class *cl)\r\n{\r\nif (htb_hysteresis)\r\nreturn cl->cmode == HTB_CAN_SEND ? -cl->buffer : 0;\r\nelse\r\nreturn 0;\r\n}\r\nstatic inline enum htb_cmode\r\nhtb_class_mode(struct htb_class *cl, s64 *diff)\r\n{\r\ns64 toks;\r\nif ((toks = (cl->ctokens + *diff)) < htb_lowater(cl)) {\r\n*diff = -toks;\r\nreturn HTB_CANT_SEND;\r\n}\r\nif ((toks = (cl->tokens + *diff)) >= htb_hiwater(cl))\r\nreturn HTB_CAN_SEND;\r\n*diff = -toks;\r\nreturn HTB_MAY_BORROW;\r\n}\r\nstatic void\r\nhtb_change_class_mode(struct htb_sched *q, struct htb_class *cl, s64 *diff)\r\n{\r\nenum htb_cmode new_mode = htb_class_mode(cl, diff);\r\nif (new_mode == cl->cmode)\r\nreturn;\r\nif (cl->prio_activity) {\r\nif (cl->cmode != HTB_CANT_SEND)\r\nhtb_deactivate_prios(q, cl);\r\ncl->cmode = new_mode;\r\nif (new_mode != HTB_CANT_SEND)\r\nhtb_activate_prios(q, cl);\r\n} else\r\ncl->cmode = new_mode;\r\n}\r\nstatic inline void htb_activate(struct htb_sched *q, struct htb_class *cl)\r\n{\r\nWARN_ON(cl->level || !cl->un.leaf.q || !cl->un.leaf.q->q.qlen);\r\nif (!cl->prio_activity) {\r\ncl->prio_activity = 1 << cl->prio;\r\nhtb_activate_prios(q, cl);\r\nlist_add_tail(&cl->un.leaf.drop_list,\r\nq->drops + cl->prio);\r\n}\r\n}\r\nstatic inline void htb_deactivate(struct htb_sched *q, struct htb_class *cl)\r\n{\r\nWARN_ON(!cl->prio_activity);\r\nhtb_deactivate_prios(q, cl);\r\ncl->prio_activity = 0;\r\nlist_del_init(&cl->un.leaf.drop_list);\r\n}\r\nstatic int htb_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nint uninitialized_var(ret);\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct htb_class *cl = htb_classify(skb, sch, &ret);\r\nif (cl == HTB_DIRECT) {\r\nif (q->direct_queue.qlen < q->direct_qlen) {\r\n__skb_queue_tail(&q->direct_queue, skb);\r\nq->direct_pkts++;\r\n} else {\r\nreturn qdisc_drop(skb, sch);\r\n}\r\n#ifdef CONFIG_NET_CLS_ACT\r\n} else if (!cl) {\r\nif (ret & __NET_XMIT_BYPASS)\r\nqdisc_qstats_drop(sch);\r\nkfree_skb(skb);\r\nreturn ret;\r\n#endif\r\n} else if ((ret = qdisc_enqueue(skb, cl->un.leaf.q)) != NET_XMIT_SUCCESS) {\r\nif (net_xmit_drop_count(ret)) {\r\nqdisc_qstats_drop(sch);\r\ncl->qstats.drops++;\r\n}\r\nreturn ret;\r\n} else {\r\nhtb_activate(q, cl);\r\n}\r\nsch->q.qlen++;\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic inline void htb_accnt_tokens(struct htb_class *cl, int bytes, s64 diff)\r\n{\r\ns64 toks = diff + cl->tokens;\r\nif (toks > cl->buffer)\r\ntoks = cl->buffer;\r\ntoks -= (s64) psched_l2t_ns(&cl->rate, bytes);\r\nif (toks <= -cl->mbuffer)\r\ntoks = 1 - cl->mbuffer;\r\ncl->tokens = toks;\r\n}\r\nstatic inline void htb_accnt_ctokens(struct htb_class *cl, int bytes, s64 diff)\r\n{\r\ns64 toks = diff + cl->ctokens;\r\nif (toks > cl->cbuffer)\r\ntoks = cl->cbuffer;\r\ntoks -= (s64) psched_l2t_ns(&cl->ceil, bytes);\r\nif (toks <= -cl->mbuffer)\r\ntoks = 1 - cl->mbuffer;\r\ncl->ctokens = toks;\r\n}\r\nstatic void htb_charge_class(struct htb_sched *q, struct htb_class *cl,\r\nint level, struct sk_buff *skb)\r\n{\r\nint bytes = qdisc_pkt_len(skb);\r\nenum htb_cmode old_mode;\r\ns64 diff;\r\nwhile (cl) {\r\ndiff = min_t(s64, q->now - cl->t_c, cl->mbuffer);\r\nif (cl->level >= level) {\r\nif (cl->level == level)\r\ncl->xstats.lends++;\r\nhtb_accnt_tokens(cl, bytes, diff);\r\n} else {\r\ncl->xstats.borrows++;\r\ncl->tokens += diff;\r\n}\r\nhtb_accnt_ctokens(cl, bytes, diff);\r\ncl->t_c = q->now;\r\nold_mode = cl->cmode;\r\ndiff = 0;\r\nhtb_change_class_mode(q, cl, &diff);\r\nif (old_mode != cl->cmode) {\r\nif (old_mode != HTB_CAN_SEND)\r\nhtb_safe_rb_erase(&cl->pq_node, &q->hlevel[cl->level].wait_pq);\r\nif (cl->cmode != HTB_CAN_SEND)\r\nhtb_add_to_wait_tree(q, cl, diff);\r\n}\r\nif (cl->level)\r\nbstats_update(&cl->bstats, skb);\r\ncl = cl->parent;\r\n}\r\n}\r\nstatic s64 htb_do_events(struct htb_sched *q, const int level,\r\nunsigned long start)\r\n{\r\nunsigned long stop_at = start + 2;\r\nstruct rb_root *wait_pq = &q->hlevel[level].wait_pq;\r\nwhile (time_before(jiffies, stop_at)) {\r\nstruct htb_class *cl;\r\ns64 diff;\r\nstruct rb_node *p = rb_first(wait_pq);\r\nif (!p)\r\nreturn 0;\r\ncl = rb_entry(p, struct htb_class, pq_node);\r\nif (cl->pq_key > q->now)\r\nreturn cl->pq_key;\r\nhtb_safe_rb_erase(p, wait_pq);\r\ndiff = min_t(s64, q->now - cl->t_c, cl->mbuffer);\r\nhtb_change_class_mode(q, cl, &diff);\r\nif (cl->cmode != HTB_CAN_SEND)\r\nhtb_add_to_wait_tree(q, cl, diff);\r\n}\r\nif (!(q->warned & HTB_WARN_TOOMANYEVENTS)) {\r\npr_warn("htb: too many events!\n");\r\nq->warned |= HTB_WARN_TOOMANYEVENTS;\r\n}\r\nreturn q->now;\r\n}\r\nstatic struct rb_node *htb_id_find_next_upper(int prio, struct rb_node *n,\r\nu32 id)\r\n{\r\nstruct rb_node *r = NULL;\r\nwhile (n) {\r\nstruct htb_class *cl =\r\nrb_entry(n, struct htb_class, node[prio]);\r\nif (id > cl->common.classid) {\r\nn = n->rb_right;\r\n} else if (id < cl->common.classid) {\r\nr = n;\r\nn = n->rb_left;\r\n} else {\r\nreturn n;\r\n}\r\n}\r\nreturn r;\r\n}\r\nstatic struct htb_class *htb_lookup_leaf(struct htb_prio *hprio, const int prio)\r\n{\r\nint i;\r\nstruct {\r\nstruct rb_node *root;\r\nstruct rb_node **pptr;\r\nu32 *pid;\r\n} stk[TC_HTB_MAXDEPTH], *sp = stk;\r\nBUG_ON(!hprio->row.rb_node);\r\nsp->root = hprio->row.rb_node;\r\nsp->pptr = &hprio->ptr;\r\nsp->pid = &hprio->last_ptr_id;\r\nfor (i = 0; i < 65535; i++) {\r\nif (!*sp->pptr && *sp->pid) {\r\n*sp->pptr =\r\nhtb_id_find_next_upper(prio, sp->root, *sp->pid);\r\n}\r\n*sp->pid = 0;\r\nif (!*sp->pptr) {\r\n*sp->pptr = sp->root;\r\nwhile ((*sp->pptr)->rb_left)\r\n*sp->pptr = (*sp->pptr)->rb_left;\r\nif (sp > stk) {\r\nsp--;\r\nif (!*sp->pptr) {\r\nWARN_ON(1);\r\nreturn NULL;\r\n}\r\nhtb_next_rb_node(sp->pptr);\r\n}\r\n} else {\r\nstruct htb_class *cl;\r\nstruct htb_prio *clp;\r\ncl = rb_entry(*sp->pptr, struct htb_class, node[prio]);\r\nif (!cl->level)\r\nreturn cl;\r\nclp = &cl->un.inner.clprio[prio];\r\n(++sp)->root = clp->feed.rb_node;\r\nsp->pptr = &clp->ptr;\r\nsp->pid = &clp->last_ptr_id;\r\n}\r\n}\r\nWARN_ON(1);\r\nreturn NULL;\r\n}\r\nstatic struct sk_buff *htb_dequeue_tree(struct htb_sched *q, const int prio,\r\nconst int level)\r\n{\r\nstruct sk_buff *skb = NULL;\r\nstruct htb_class *cl, *start;\r\nstruct htb_level *hlevel = &q->hlevel[level];\r\nstruct htb_prio *hprio = &hlevel->hprio[prio];\r\nstart = cl = htb_lookup_leaf(hprio, prio);\r\ndo {\r\nnext:\r\nif (unlikely(!cl))\r\nreturn NULL;\r\nif (unlikely(cl->un.leaf.q->q.qlen == 0)) {\r\nstruct htb_class *next;\r\nhtb_deactivate(q, cl);\r\nif ((q->row_mask[level] & (1 << prio)) == 0)\r\nreturn NULL;\r\nnext = htb_lookup_leaf(hprio, prio);\r\nif (cl == start)\r\nstart = next;\r\ncl = next;\r\ngoto next;\r\n}\r\nskb = cl->un.leaf.q->dequeue(cl->un.leaf.q);\r\nif (likely(skb != NULL))\r\nbreak;\r\nqdisc_warn_nonwc("htb", cl->un.leaf.q);\r\nhtb_next_rb_node(level ? &cl->parent->un.inner.clprio[prio].ptr:\r\n&q->hlevel[0].hprio[prio].ptr);\r\ncl = htb_lookup_leaf(hprio, prio);\r\n} while (cl != start);\r\nif (likely(skb != NULL)) {\r\nbstats_update(&cl->bstats, skb);\r\ncl->un.leaf.deficit[level] -= qdisc_pkt_len(skb);\r\nif (cl->un.leaf.deficit[level] < 0) {\r\ncl->un.leaf.deficit[level] += cl->quantum;\r\nhtb_next_rb_node(level ? &cl->parent->un.inner.clprio[prio].ptr :\r\n&q->hlevel[0].hprio[prio].ptr);\r\n}\r\nif (!cl->un.leaf.q->q.qlen)\r\nhtb_deactivate(q, cl);\r\nhtb_charge_class(q, cl, level, skb);\r\n}\r\nreturn skb;\r\n}\r\nstatic struct sk_buff *htb_dequeue(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nint level;\r\ns64 next_event;\r\nunsigned long start_at;\r\nskb = __skb_dequeue(&q->direct_queue);\r\nif (skb != NULL) {\r\nok:\r\nqdisc_bstats_update(sch, skb);\r\nqdisc_unthrottled(sch);\r\nsch->q.qlen--;\r\nreturn skb;\r\n}\r\nif (!sch->q.qlen)\r\ngoto fin;\r\nq->now = ktime_get_ns();\r\nstart_at = jiffies;\r\nnext_event = q->now + 5LLU * NSEC_PER_SEC;\r\nfor (level = 0; level < TC_HTB_MAXDEPTH; level++) {\r\nint m;\r\ns64 event = q->near_ev_cache[level];\r\nif (q->now >= event) {\r\nevent = htb_do_events(q, level, start_at);\r\nif (!event)\r\nevent = q->now + NSEC_PER_SEC;\r\nq->near_ev_cache[level] = event;\r\n}\r\nif (next_event > event)\r\nnext_event = event;\r\nm = ~q->row_mask[level];\r\nwhile (m != (int)(-1)) {\r\nint prio = ffz(m);\r\nm |= 1 << prio;\r\nskb = htb_dequeue_tree(q, prio, level);\r\nif (likely(skb != NULL))\r\ngoto ok;\r\n}\r\n}\r\nqdisc_qstats_overlimit(sch);\r\nif (likely(next_event > q->now)) {\r\nif (!test_bit(__QDISC_STATE_DEACTIVATED,\r\n&qdisc_root_sleeping(q->watchdog.qdisc)->state)) {\r\nktime_t time = ns_to_ktime(next_event);\r\nqdisc_throttled(q->watchdog.qdisc);\r\nhrtimer_start(&q->watchdog.timer, time,\r\nHRTIMER_MODE_ABS_PINNED);\r\n}\r\n} else {\r\nschedule_work(&q->work);\r\n}\r\nfin:\r\nreturn skb;\r\n}\r\nstatic unsigned int htb_drop(struct Qdisc *sch)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nint prio;\r\nfor (prio = TC_HTB_NUMPRIO - 1; prio >= 0; prio--) {\r\nstruct list_head *p;\r\nlist_for_each(p, q->drops + prio) {\r\nstruct htb_class *cl = list_entry(p, struct htb_class,\r\nun.leaf.drop_list);\r\nunsigned int len;\r\nif (cl->un.leaf.q->ops->drop &&\r\n(len = cl->un.leaf.q->ops->drop(cl->un.leaf.q))) {\r\nsch->q.qlen--;\r\nif (!cl->un.leaf.q->q.qlen)\r\nhtb_deactivate(q, cl);\r\nreturn len;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void htb_reset(struct Qdisc *sch)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct htb_class *cl;\r\nunsigned int i;\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\r\nif (cl->level)\r\nmemset(&cl->un.inner, 0, sizeof(cl->un.inner));\r\nelse {\r\nif (cl->un.leaf.q)\r\nqdisc_reset(cl->un.leaf.q);\r\nINIT_LIST_HEAD(&cl->un.leaf.drop_list);\r\n}\r\ncl->prio_activity = 0;\r\ncl->cmode = HTB_CAN_SEND;\r\n}\r\n}\r\nqdisc_watchdog_cancel(&q->watchdog);\r\n__skb_queue_purge(&q->direct_queue);\r\nsch->q.qlen = 0;\r\nmemset(q->hlevel, 0, sizeof(q->hlevel));\r\nmemset(q->row_mask, 0, sizeof(q->row_mask));\r\nfor (i = 0; i < TC_HTB_NUMPRIO; i++)\r\nINIT_LIST_HEAD(q->drops + i);\r\n}\r\nstatic void htb_work_func(struct work_struct *work)\r\n{\r\nstruct htb_sched *q = container_of(work, struct htb_sched, work);\r\nstruct Qdisc *sch = q->watchdog.qdisc;\r\n__netif_schedule(qdisc_root(sch));\r\n}\r\nstatic int htb_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_HTB_MAX + 1];\r\nstruct tc_htb_glob *gopt;\r\nint err;\r\nint i;\r\nif (!opt)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_HTB_MAX, opt, htb_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (!tb[TCA_HTB_INIT])\r\nreturn -EINVAL;\r\ngopt = nla_data(tb[TCA_HTB_INIT]);\r\nif (gopt->version != HTB_VER >> 16)\r\nreturn -EINVAL;\r\nerr = qdisc_class_hash_init(&q->clhash);\r\nif (err < 0)\r\nreturn err;\r\nfor (i = 0; i < TC_HTB_NUMPRIO; i++)\r\nINIT_LIST_HEAD(q->drops + i);\r\nqdisc_watchdog_init(&q->watchdog, sch);\r\nINIT_WORK(&q->work, htb_work_func);\r\n__skb_queue_head_init(&q->direct_queue);\r\nif (tb[TCA_HTB_DIRECT_QLEN])\r\nq->direct_qlen = nla_get_u32(tb[TCA_HTB_DIRECT_QLEN]);\r\nelse\r\nq->direct_qlen = qdisc_dev(sch)->tx_queue_len;\r\nif ((q->rate2quantum = gopt->rate2quantum) < 1)\r\nq->rate2quantum = 1;\r\nq->defcls = gopt->defcls;\r\nreturn 0;\r\n}\r\nstatic int htb_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct nlattr *nest;\r\nstruct tc_htb_glob gopt;\r\ngopt.direct_pkts = q->direct_pkts;\r\ngopt.version = HTB_VER;\r\ngopt.rate2quantum = q->rate2quantum;\r\ngopt.defcls = q->defcls;\r\ngopt.debug = 0;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put(skb, TCA_HTB_INIT, sizeof(gopt), &gopt) ||\r\nnla_put_u32(skb, TCA_HTB_DIRECT_QLEN, q->direct_qlen))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, nest);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic int htb_dump_class(struct Qdisc *sch, unsigned long arg,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nstruct nlattr *nest;\r\nstruct tc_htb_opt opt;\r\ntcm->tcm_parent = cl->parent ? cl->parent->common.classid : TC_H_ROOT;\r\ntcm->tcm_handle = cl->common.classid;\r\nif (!cl->level && cl->un.leaf.q)\r\ntcm->tcm_info = cl->un.leaf.q->handle;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nmemset(&opt, 0, sizeof(opt));\r\npsched_ratecfg_getrate(&opt.rate, &cl->rate);\r\nopt.buffer = PSCHED_NS2TICKS(cl->buffer);\r\npsched_ratecfg_getrate(&opt.ceil, &cl->ceil);\r\nopt.cbuffer = PSCHED_NS2TICKS(cl->cbuffer);\r\nopt.quantum = cl->quantum;\r\nopt.prio = cl->prio;\r\nopt.level = cl->level;\r\nif (nla_put(skb, TCA_HTB_PARMS, sizeof(opt), &opt))\r\ngoto nla_put_failure;\r\nif ((cl->rate.rate_bytes_ps >= (1ULL << 32)) &&\r\nnla_put_u64(skb, TCA_HTB_RATE64, cl->rate.rate_bytes_ps))\r\ngoto nla_put_failure;\r\nif ((cl->ceil.rate_bytes_ps >= (1ULL << 32)) &&\r\nnla_put_u64(skb, TCA_HTB_CEIL64, cl->ceil.rate_bytes_ps))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, nest);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic int\r\nhtb_dump_class_stats(struct Qdisc *sch, unsigned long arg, struct gnet_dump *d)\r\n{\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\n__u32 qlen = 0;\r\nif (!cl->level && cl->un.leaf.q)\r\nqlen = cl->un.leaf.q->q.qlen;\r\ncl->xstats.tokens = PSCHED_NS2TICKS(cl->tokens);\r\ncl->xstats.ctokens = PSCHED_NS2TICKS(cl->ctokens);\r\nif (gnet_stats_copy_basic(d, NULL, &cl->bstats) < 0 ||\r\ngnet_stats_copy_rate_est(d, NULL, &cl->rate_est) < 0 ||\r\ngnet_stats_copy_queue(d, NULL, &cl->qstats, qlen) < 0)\r\nreturn -1;\r\nreturn gnet_stats_copy_app(d, &cl->xstats, sizeof(cl->xstats));\r\n}\r\nstatic int htb_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\r\nstruct Qdisc **old)\r\n{\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nif (cl->level)\r\nreturn -EINVAL;\r\nif (new == NULL &&\r\n(new = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\r\ncl->common.classid)) == NULL)\r\nreturn -ENOBUFS;\r\nsch_tree_lock(sch);\r\n*old = cl->un.leaf.q;\r\ncl->un.leaf.q = new;\r\nif (*old != NULL) {\r\nqdisc_tree_decrease_qlen(*old, (*old)->q.qlen);\r\nqdisc_reset(*old);\r\n}\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct Qdisc *htb_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nreturn !cl->level ? cl->un.leaf.q : NULL;\r\n}\r\nstatic void htb_qlen_notify(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nif (cl->un.leaf.q->q.qlen == 0)\r\nhtb_deactivate(qdisc_priv(sch), cl);\r\n}\r\nstatic unsigned long htb_get(struct Qdisc *sch, u32 classid)\r\n{\r\nstruct htb_class *cl = htb_find(classid, sch);\r\nif (cl)\r\ncl->refcnt++;\r\nreturn (unsigned long)cl;\r\n}\r\nstatic inline int htb_parent_last_child(struct htb_class *cl)\r\n{\r\nif (!cl->parent)\r\nreturn 0;\r\nif (cl->parent->children > 1)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void htb_parent_to_leaf(struct htb_sched *q, struct htb_class *cl,\r\nstruct Qdisc *new_q)\r\n{\r\nstruct htb_class *parent = cl->parent;\r\nWARN_ON(cl->level || !cl->un.leaf.q || cl->prio_activity);\r\nif (parent->cmode != HTB_CAN_SEND)\r\nhtb_safe_rb_erase(&parent->pq_node,\r\n&q->hlevel[parent->level].wait_pq);\r\nparent->level = 0;\r\nmemset(&parent->un.inner, 0, sizeof(parent->un.inner));\r\nINIT_LIST_HEAD(&parent->un.leaf.drop_list);\r\nparent->un.leaf.q = new_q ? new_q : &noop_qdisc;\r\nparent->tokens = parent->buffer;\r\nparent->ctokens = parent->cbuffer;\r\nparent->t_c = ktime_get_ns();\r\nparent->cmode = HTB_CAN_SEND;\r\n}\r\nstatic void htb_destroy_class(struct Qdisc *sch, struct htb_class *cl)\r\n{\r\nif (!cl->level) {\r\nWARN_ON(!cl->un.leaf.q);\r\nqdisc_destroy(cl->un.leaf.q);\r\n}\r\ngen_kill_estimator(&cl->bstats, &cl->rate_est);\r\ntcf_destroy_chain(&cl->filter_list);\r\nkfree(cl);\r\n}\r\nstatic void htb_destroy(struct Qdisc *sch)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct hlist_node *next;\r\nstruct htb_class *cl;\r\nunsigned int i;\r\ncancel_work_sync(&q->work);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\ntcf_destroy_chain(&q->filter_list);\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode)\r\ntcf_destroy_chain(&cl->filter_list);\r\n}\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry_safe(cl, next, &q->clhash.hash[i],\r\ncommon.hnode)\r\nhtb_destroy_class(sch, cl);\r\n}\r\nqdisc_class_hash_destroy(&q->clhash);\r\n__skb_queue_purge(&q->direct_queue);\r\n}\r\nstatic int htb_delete(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nunsigned int qlen;\r\nstruct Qdisc *new_q = NULL;\r\nint last_child = 0;\r\nif (cl->children || cl->filter_cnt)\r\nreturn -EBUSY;\r\nif (!cl->level && htb_parent_last_child(cl)) {\r\nnew_q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\r\ncl->parent->common.classid);\r\nlast_child = 1;\r\n}\r\nsch_tree_lock(sch);\r\nif (!cl->level) {\r\nqlen = cl->un.leaf.q->q.qlen;\r\nqdisc_reset(cl->un.leaf.q);\r\nqdisc_tree_decrease_qlen(cl->un.leaf.q, qlen);\r\n}\r\nqdisc_class_hash_remove(&q->clhash, &cl->common);\r\nif (cl->parent)\r\ncl->parent->children--;\r\nif (cl->prio_activity)\r\nhtb_deactivate(q, cl);\r\nif (cl->cmode != HTB_CAN_SEND)\r\nhtb_safe_rb_erase(&cl->pq_node,\r\n&q->hlevel[cl->level].wait_pq);\r\nif (last_child)\r\nhtb_parent_to_leaf(q, cl, new_q);\r\nBUG_ON(--cl->refcnt == 0);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic void htb_put(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nif (--cl->refcnt == 0)\r\nhtb_destroy_class(sch, cl);\r\n}\r\nstatic int htb_change_class(struct Qdisc *sch, u32 classid,\r\nu32 parentid, struct nlattr **tca,\r\nunsigned long *arg)\r\n{\r\nint err = -EINVAL;\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct htb_class *cl = (struct htb_class *)*arg, *parent;\r\nstruct nlattr *opt = tca[TCA_OPTIONS];\r\nstruct nlattr *tb[TCA_HTB_MAX + 1];\r\nstruct tc_htb_opt *hopt;\r\nu64 rate64, ceil64;\r\nif (!opt)\r\ngoto failure;\r\nerr = nla_parse_nested(tb, TCA_HTB_MAX, opt, htb_policy);\r\nif (err < 0)\r\ngoto failure;\r\nerr = -EINVAL;\r\nif (tb[TCA_HTB_PARMS] == NULL)\r\ngoto failure;\r\nparent = parentid == TC_H_ROOT ? NULL : htb_find(parentid, sch);\r\nhopt = nla_data(tb[TCA_HTB_PARMS]);\r\nif (!hopt->rate.rate || !hopt->ceil.rate)\r\ngoto failure;\r\nif (hopt->rate.linklayer == TC_LINKLAYER_UNAWARE)\r\nqdisc_put_rtab(qdisc_get_rtab(&hopt->rate, tb[TCA_HTB_RTAB]));\r\nif (hopt->ceil.linklayer == TC_LINKLAYER_UNAWARE)\r\nqdisc_put_rtab(qdisc_get_rtab(&hopt->ceil, tb[TCA_HTB_CTAB]));\r\nif (!cl) {\r\nstruct Qdisc *new_q;\r\nint prio;\r\nstruct {\r\nstruct nlattr nla;\r\nstruct gnet_estimator opt;\r\n} est = {\r\n.nla = {\r\n.nla_len = nla_attr_size(sizeof(est.opt)),\r\n.nla_type = TCA_RATE,\r\n},\r\n.opt = {\r\n.interval = 2,\r\n.ewma_log = 2,\r\n},\r\n};\r\nif (!classid || TC_H_MAJ(classid ^ sch->handle) ||\r\nhtb_find(classid, sch))\r\ngoto failure;\r\nif (parent && parent->parent && parent->parent->level < 2) {\r\npr_err("htb: tree is too deep\n");\r\ngoto failure;\r\n}\r\nerr = -ENOBUFS;\r\ncl = kzalloc(sizeof(*cl), GFP_KERNEL);\r\nif (!cl)\r\ngoto failure;\r\nif (htb_rate_est || tca[TCA_RATE]) {\r\nerr = gen_new_estimator(&cl->bstats, NULL,\r\n&cl->rate_est,\r\nqdisc_root_sleeping_lock(sch),\r\ntca[TCA_RATE] ? : &est.nla);\r\nif (err) {\r\nkfree(cl);\r\ngoto failure;\r\n}\r\n}\r\ncl->refcnt = 1;\r\ncl->children = 0;\r\nINIT_LIST_HEAD(&cl->un.leaf.drop_list);\r\nRB_CLEAR_NODE(&cl->pq_node);\r\nfor (prio = 0; prio < TC_HTB_NUMPRIO; prio++)\r\nRB_CLEAR_NODE(&cl->node[prio]);\r\nnew_q = qdisc_create_dflt(sch->dev_queue,\r\n&pfifo_qdisc_ops, classid);\r\nsch_tree_lock(sch);\r\nif (parent && !parent->level) {\r\nunsigned int qlen = parent->un.leaf.q->q.qlen;\r\nqdisc_reset(parent->un.leaf.q);\r\nqdisc_tree_decrease_qlen(parent->un.leaf.q, qlen);\r\nqdisc_destroy(parent->un.leaf.q);\r\nif (parent->prio_activity)\r\nhtb_deactivate(q, parent);\r\nif (parent->cmode != HTB_CAN_SEND) {\r\nhtb_safe_rb_erase(&parent->pq_node, &q->hlevel[0].wait_pq);\r\nparent->cmode = HTB_CAN_SEND;\r\n}\r\nparent->level = (parent->parent ? parent->parent->level\r\n: TC_HTB_MAXDEPTH) - 1;\r\nmemset(&parent->un.inner, 0, sizeof(parent->un.inner));\r\n}\r\ncl->un.leaf.q = new_q ? new_q : &noop_qdisc;\r\ncl->common.classid = classid;\r\ncl->parent = parent;\r\ncl->tokens = PSCHED_TICKS2NS(hopt->buffer);\r\ncl->ctokens = PSCHED_TICKS2NS(hopt->cbuffer);\r\ncl->mbuffer = 60ULL * NSEC_PER_SEC;\r\ncl->t_c = ktime_get_ns();\r\ncl->cmode = HTB_CAN_SEND;\r\nqdisc_class_hash_insert(&q->clhash, &cl->common);\r\nif (parent)\r\nparent->children++;\r\n} else {\r\nif (tca[TCA_RATE]) {\r\nspinlock_t *lock = qdisc_root_sleeping_lock(sch);\r\nerr = gen_replace_estimator(&cl->bstats, NULL,\r\n&cl->rate_est,\r\nlock,\r\ntca[TCA_RATE]);\r\nif (err)\r\nreturn err;\r\n}\r\nsch_tree_lock(sch);\r\n}\r\nrate64 = tb[TCA_HTB_RATE64] ? nla_get_u64(tb[TCA_HTB_RATE64]) : 0;\r\nceil64 = tb[TCA_HTB_CEIL64] ? nla_get_u64(tb[TCA_HTB_CEIL64]) : 0;\r\npsched_ratecfg_precompute(&cl->rate, &hopt->rate, rate64);\r\npsched_ratecfg_precompute(&cl->ceil, &hopt->ceil, ceil64);\r\nif (!cl->level) {\r\nu64 quantum = cl->rate.rate_bytes_ps;\r\ndo_div(quantum, q->rate2quantum);\r\ncl->quantum = min_t(u64, quantum, INT_MAX);\r\nif (!hopt->quantum && cl->quantum < 1000) {\r\npr_warn("HTB: quantum of class %X is small. Consider r2q change.\n",\r\ncl->common.classid);\r\ncl->quantum = 1000;\r\n}\r\nif (!hopt->quantum && cl->quantum > 200000) {\r\npr_warn("HTB: quantum of class %X is big. Consider r2q change.\n",\r\ncl->common.classid);\r\ncl->quantum = 200000;\r\n}\r\nif (hopt->quantum)\r\ncl->quantum = hopt->quantum;\r\nif ((cl->prio = hopt->prio) >= TC_HTB_NUMPRIO)\r\ncl->prio = TC_HTB_NUMPRIO - 1;\r\n}\r\ncl->buffer = PSCHED_TICKS2NS(hopt->buffer);\r\ncl->cbuffer = PSCHED_TICKS2NS(hopt->cbuffer);\r\nsch_tree_unlock(sch);\r\nqdisc_class_hash_grow(sch, &q->clhash);\r\n*arg = (unsigned long)cl;\r\nreturn 0;\r\nfailure:\r\nreturn err;\r\n}\r\nstatic struct tcf_proto __rcu **htb_find_tcf(struct Qdisc *sch,\r\nunsigned long arg)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nstruct tcf_proto __rcu **fl = cl ? &cl->filter_list : &q->filter_list;\r\nreturn fl;\r\n}\r\nstatic unsigned long htb_bind_filter(struct Qdisc *sch, unsigned long parent,\r\nu32 classid)\r\n{\r\nstruct htb_class *cl = htb_find(classid, sch);\r\nif (cl)\r\ncl->filter_cnt++;\r\nreturn (unsigned long)cl;\r\n}\r\nstatic void htb_unbind_filter(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct htb_class *cl = (struct htb_class *)arg;\r\nif (cl)\r\ncl->filter_cnt--;\r\n}\r\nstatic void htb_walk(struct Qdisc *sch, struct qdisc_walker *arg)\r\n{\r\nstruct htb_sched *q = qdisc_priv(sch);\r\nstruct htb_class *cl;\r\nunsigned int i;\r\nif (arg->stop)\r\nreturn;\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\r\nif (arg->count < arg->skip) {\r\narg->count++;\r\ncontinue;\r\n}\r\nif (arg->fn(sch, (unsigned long)cl, arg) < 0) {\r\narg->stop = 1;\r\nreturn;\r\n}\r\narg->count++;\r\n}\r\n}\r\n}\r\nstatic int __init htb_module_init(void)\r\n{\r\nreturn register_qdisc(&htb_qdisc_ops);\r\n}\r\nstatic void __exit htb_module_exit(void)\r\n{\r\nunregister_qdisc(&htb_qdisc_ops);\r\n}
