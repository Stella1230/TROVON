bool ion_buffer_fault_user_mappings(struct ion_buffer *buffer)\r\n{\r\nreturn (buffer->flags & ION_FLAG_CACHED) &&\r\n!(buffer->flags & ION_FLAG_CACHED_NEEDS_SYNC);\r\n}\r\nbool ion_buffer_cached(struct ion_buffer *buffer)\r\n{\r\nreturn !!(buffer->flags & ION_FLAG_CACHED);\r\n}\r\nstatic inline struct page *ion_buffer_page(struct page *page)\r\n{\r\nreturn (struct page *)((unsigned long)page & ~(1UL));\r\n}\r\nstatic inline bool ion_buffer_page_is_dirty(struct page *page)\r\n{\r\nreturn !!((unsigned long)page & 1UL);\r\n}\r\nstatic inline void ion_buffer_page_dirty(struct page **page)\r\n{\r\n*page = (struct page *)((unsigned long)(*page) | 1UL);\r\n}\r\nstatic inline void ion_buffer_page_clean(struct page **page)\r\n{\r\n*page = (struct page *)((unsigned long)(*page) & ~(1UL));\r\n}\r\nstatic void ion_buffer_add(struct ion_device *dev,\r\nstruct ion_buffer *buffer)\r\n{\r\nstruct rb_node **p = &dev->buffers.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct ion_buffer *entry;\r\nwhile (*p) {\r\nparent = *p;\r\nentry = rb_entry(parent, struct ion_buffer, node);\r\nif (buffer < entry) {\r\np = &(*p)->rb_left;\r\n} else if (buffer > entry) {\r\np = &(*p)->rb_right;\r\n} else {\r\npr_err("%s: buffer already found.", __func__);\r\nBUG();\r\n}\r\n}\r\nrb_link_node(&buffer->node, parent, p);\r\nrb_insert_color(&buffer->node, &dev->buffers);\r\n}\r\nstatic struct ion_buffer *ion_buffer_create(struct ion_heap *heap,\r\nstruct ion_device *dev,\r\nunsigned long len,\r\nunsigned long align,\r\nunsigned long flags)\r\n{\r\nstruct ion_buffer *buffer;\r\nstruct sg_table *table;\r\nstruct scatterlist *sg;\r\nint i, ret;\r\nbuffer = kzalloc(sizeof(struct ion_buffer), GFP_KERNEL);\r\nif (!buffer)\r\nreturn ERR_PTR(-ENOMEM);\r\nbuffer->heap = heap;\r\nbuffer->flags = flags;\r\nkref_init(&buffer->ref);\r\nret = heap->ops->allocate(heap, buffer, len, align, flags);\r\nif (ret) {\r\nif (!(heap->flags & ION_HEAP_FLAG_DEFER_FREE))\r\ngoto err2;\r\nion_heap_freelist_drain(heap, 0);\r\nret = heap->ops->allocate(heap, buffer, len, align,\r\nflags);\r\nif (ret)\r\ngoto err2;\r\n}\r\nbuffer->dev = dev;\r\nbuffer->size = len;\r\ntable = heap->ops->map_dma(heap, buffer);\r\nif (WARN_ONCE(table == NULL,\r\n"heap->ops->map_dma should return ERR_PTR on error"))\r\ntable = ERR_PTR(-EINVAL);\r\nif (IS_ERR(table)) {\r\nret = -EINVAL;\r\ngoto err1;\r\n}\r\nbuffer->sg_table = table;\r\nif (ion_buffer_fault_user_mappings(buffer)) {\r\nint num_pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;\r\nstruct scatterlist *sg;\r\nint i, j, k = 0;\r\nbuffer->pages = vmalloc(sizeof(struct page *) * num_pages);\r\nif (!buffer->pages) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nfor_each_sg(table->sgl, sg, table->nents, i) {\r\nstruct page *page = sg_page(sg);\r\nfor (j = 0; j < sg->length / PAGE_SIZE; j++)\r\nbuffer->pages[k++] = page++;\r\n}\r\n}\r\nbuffer->dev = dev;\r\nbuffer->size = len;\r\nINIT_LIST_HEAD(&buffer->vmas);\r\nmutex_init(&buffer->lock);\r\nfor_each_sg(buffer->sg_table->sgl, sg, buffer->sg_table->nents, i)\r\nsg_dma_address(sg) = sg_phys(sg);\r\nmutex_lock(&dev->buffer_lock);\r\nion_buffer_add(dev, buffer);\r\nmutex_unlock(&dev->buffer_lock);\r\nreturn buffer;\r\nerr:\r\nheap->ops->unmap_dma(heap, buffer);\r\nerr1:\r\nheap->ops->free(buffer);\r\nerr2:\r\nkfree(buffer);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid ion_buffer_destroy(struct ion_buffer *buffer)\r\n{\r\nif (WARN_ON(buffer->kmap_cnt > 0))\r\nbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\r\nbuffer->heap->ops->unmap_dma(buffer->heap, buffer);\r\nbuffer->heap->ops->free(buffer);\r\nvfree(buffer->pages);\r\nkfree(buffer);\r\n}\r\nstatic void _ion_buffer_destroy(struct kref *kref)\r\n{\r\nstruct ion_buffer *buffer = container_of(kref, struct ion_buffer, ref);\r\nstruct ion_heap *heap = buffer->heap;\r\nstruct ion_device *dev = buffer->dev;\r\nmutex_lock(&dev->buffer_lock);\r\nrb_erase(&buffer->node, &dev->buffers);\r\nmutex_unlock(&dev->buffer_lock);\r\nif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\r\nion_heap_freelist_add(heap, buffer);\r\nelse\r\nion_buffer_destroy(buffer);\r\n}\r\nstatic void ion_buffer_get(struct ion_buffer *buffer)\r\n{\r\nkref_get(&buffer->ref);\r\n}\r\nstatic int ion_buffer_put(struct ion_buffer *buffer)\r\n{\r\nreturn kref_put(&buffer->ref, _ion_buffer_destroy);\r\n}\r\nstatic void ion_buffer_add_to_handle(struct ion_buffer *buffer)\r\n{\r\nmutex_lock(&buffer->lock);\r\nbuffer->handle_count++;\r\nmutex_unlock(&buffer->lock);\r\n}\r\nstatic void ion_buffer_remove_from_handle(struct ion_buffer *buffer)\r\n{\r\nmutex_lock(&buffer->lock);\r\nbuffer->handle_count--;\r\nBUG_ON(buffer->handle_count < 0);\r\nif (!buffer->handle_count) {\r\nstruct task_struct *task;\r\ntask = current->group_leader;\r\nget_task_comm(buffer->task_comm, task);\r\nbuffer->pid = task_pid_nr(task);\r\n}\r\nmutex_unlock(&buffer->lock);\r\n}\r\nstatic struct ion_handle *ion_handle_create(struct ion_client *client,\r\nstruct ion_buffer *buffer)\r\n{\r\nstruct ion_handle *handle;\r\nhandle = kzalloc(sizeof(struct ion_handle), GFP_KERNEL);\r\nif (!handle)\r\nreturn ERR_PTR(-ENOMEM);\r\nkref_init(&handle->ref);\r\nRB_CLEAR_NODE(&handle->node);\r\nhandle->client = client;\r\nion_buffer_get(buffer);\r\nion_buffer_add_to_handle(buffer);\r\nhandle->buffer = buffer;\r\nreturn handle;\r\n}\r\nstatic void ion_handle_destroy(struct kref *kref)\r\n{\r\nstruct ion_handle *handle = container_of(kref, struct ion_handle, ref);\r\nstruct ion_client *client = handle->client;\r\nstruct ion_buffer *buffer = handle->buffer;\r\nmutex_lock(&buffer->lock);\r\nwhile (handle->kmap_cnt)\r\nion_handle_kmap_put(handle);\r\nmutex_unlock(&buffer->lock);\r\nidr_remove(&client->idr, handle->id);\r\nif (!RB_EMPTY_NODE(&handle->node))\r\nrb_erase(&handle->node, &client->handles);\r\nion_buffer_remove_from_handle(buffer);\r\nion_buffer_put(buffer);\r\nkfree(handle);\r\n}\r\nstruct ion_buffer *ion_handle_buffer(struct ion_handle *handle)\r\n{\r\nreturn handle->buffer;\r\n}\r\nstatic void ion_handle_get(struct ion_handle *handle)\r\n{\r\nkref_get(&handle->ref);\r\n}\r\nstatic int ion_handle_put(struct ion_handle *handle)\r\n{\r\nstruct ion_client *client = handle->client;\r\nint ret;\r\nmutex_lock(&client->lock);\r\nret = kref_put(&handle->ref, ion_handle_destroy);\r\nmutex_unlock(&client->lock);\r\nreturn ret;\r\n}\r\nstatic struct ion_handle *ion_handle_lookup(struct ion_client *client,\r\nstruct ion_buffer *buffer)\r\n{\r\nstruct rb_node *n = client->handles.rb_node;\r\nwhile (n) {\r\nstruct ion_handle *entry = rb_entry(n, struct ion_handle, node);\r\nif (buffer < entry->buffer)\r\nn = n->rb_left;\r\nelse if (buffer > entry->buffer)\r\nn = n->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\r\nint id)\r\n{\r\nstruct ion_handle *handle;\r\nmutex_lock(&client->lock);\r\nhandle = idr_find(&client->idr, id);\r\nif (handle)\r\nion_handle_get(handle);\r\nmutex_unlock(&client->lock);\r\nreturn handle ? handle : ERR_PTR(-EINVAL);\r\n}\r\nstatic bool ion_handle_validate(struct ion_client *client,\r\nstruct ion_handle *handle)\r\n{\r\nWARN_ON(!mutex_is_locked(&client->lock));\r\nreturn idr_find(&client->idr, handle->id) == handle;\r\n}\r\nstatic int ion_handle_add(struct ion_client *client, struct ion_handle *handle)\r\n{\r\nint id;\r\nstruct rb_node **p = &client->handles.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct ion_handle *entry;\r\nid = idr_alloc(&client->idr, handle, 1, 0, GFP_KERNEL);\r\nif (id < 0)\r\nreturn id;\r\nhandle->id = id;\r\nwhile (*p) {\r\nparent = *p;\r\nentry = rb_entry(parent, struct ion_handle, node);\r\nif (handle->buffer < entry->buffer)\r\np = &(*p)->rb_left;\r\nelse if (handle->buffer > entry->buffer)\r\np = &(*p)->rb_right;\r\nelse\r\nWARN(1, "%s: buffer already found.", __func__);\r\n}\r\nrb_link_node(&handle->node, parent, p);\r\nrb_insert_color(&handle->node, &client->handles);\r\nreturn 0;\r\n}\r\nstruct ion_handle *ion_alloc(struct ion_client *client, size_t len,\r\nsize_t align, unsigned int heap_id_mask,\r\nunsigned int flags)\r\n{\r\nstruct ion_handle *handle;\r\nstruct ion_device *dev = client->dev;\r\nstruct ion_buffer *buffer = NULL;\r\nstruct ion_heap *heap;\r\nint ret;\r\npr_debug("%s: len %zu align %zu heap_id_mask %u flags %x\n", __func__,\r\nlen, align, heap_id_mask, flags);\r\nlen = PAGE_ALIGN(len);\r\nif (!len)\r\nreturn ERR_PTR(-EINVAL);\r\ndown_read(&dev->lock);\r\nplist_for_each_entry(heap, &dev->heaps, node) {\r\nif (!((1 << heap->id) & heap_id_mask))\r\ncontinue;\r\nbuffer = ion_buffer_create(heap, dev, len, align, flags);\r\nif (!IS_ERR(buffer))\r\nbreak;\r\n}\r\nup_read(&dev->lock);\r\nif (buffer == NULL)\r\nreturn ERR_PTR(-ENODEV);\r\nif (IS_ERR(buffer))\r\nreturn ERR_CAST(buffer);\r\nhandle = ion_handle_create(client, buffer);\r\nion_buffer_put(buffer);\r\nif (IS_ERR(handle))\r\nreturn handle;\r\nmutex_lock(&client->lock);\r\nret = ion_handle_add(client, handle);\r\nmutex_unlock(&client->lock);\r\nif (ret) {\r\nion_handle_put(handle);\r\nhandle = ERR_PTR(ret);\r\n}\r\nreturn handle;\r\n}\r\nvoid ion_free(struct ion_client *client, struct ion_handle *handle)\r\n{\r\nbool valid_handle;\r\nBUG_ON(client != handle->client);\r\nmutex_lock(&client->lock);\r\nvalid_handle = ion_handle_validate(client, handle);\r\nif (!valid_handle) {\r\nWARN(1, "%s: invalid handle passed to free.\n", __func__);\r\nmutex_unlock(&client->lock);\r\nreturn;\r\n}\r\nmutex_unlock(&client->lock);\r\nion_handle_put(handle);\r\n}\r\nint ion_phys(struct ion_client *client, struct ion_handle *handle,\r\nion_phys_addr_t *addr, size_t *len)\r\n{\r\nstruct ion_buffer *buffer;\r\nint ret;\r\nmutex_lock(&client->lock);\r\nif (!ion_handle_validate(client, handle)) {\r\nmutex_unlock(&client->lock);\r\nreturn -EINVAL;\r\n}\r\nbuffer = handle->buffer;\r\nif (!buffer->heap->ops->phys) {\r\npr_err("%s: ion_phys is not implemented by this heap (name=%s, type=%d).\n",\r\n__func__, buffer->heap->name, buffer->heap->type);\r\nmutex_unlock(&client->lock);\r\nreturn -ENODEV;\r\n}\r\nmutex_unlock(&client->lock);\r\nret = buffer->heap->ops->phys(buffer->heap, buffer, addr, len);\r\nreturn ret;\r\n}\r\nstatic void *ion_buffer_kmap_get(struct ion_buffer *buffer)\r\n{\r\nvoid *vaddr;\r\nif (buffer->kmap_cnt) {\r\nbuffer->kmap_cnt++;\r\nreturn buffer->vaddr;\r\n}\r\nvaddr = buffer->heap->ops->map_kernel(buffer->heap, buffer);\r\nif (WARN_ONCE(vaddr == NULL,\r\n"heap->ops->map_kernel should return ERR_PTR on error"))\r\nreturn ERR_PTR(-EINVAL);\r\nif (IS_ERR(vaddr))\r\nreturn vaddr;\r\nbuffer->vaddr = vaddr;\r\nbuffer->kmap_cnt++;\r\nreturn vaddr;\r\n}\r\nstatic void *ion_handle_kmap_get(struct ion_handle *handle)\r\n{\r\nstruct ion_buffer *buffer = handle->buffer;\r\nvoid *vaddr;\r\nif (handle->kmap_cnt) {\r\nhandle->kmap_cnt++;\r\nreturn buffer->vaddr;\r\n}\r\nvaddr = ion_buffer_kmap_get(buffer);\r\nif (IS_ERR(vaddr))\r\nreturn vaddr;\r\nhandle->kmap_cnt++;\r\nreturn vaddr;\r\n}\r\nstatic void ion_buffer_kmap_put(struct ion_buffer *buffer)\r\n{\r\nbuffer->kmap_cnt--;\r\nif (!buffer->kmap_cnt) {\r\nbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\r\nbuffer->vaddr = NULL;\r\n}\r\n}\r\nstatic void ion_handle_kmap_put(struct ion_handle *handle)\r\n{\r\nstruct ion_buffer *buffer = handle->buffer;\r\nif (!handle->kmap_cnt) {\r\nWARN(1, "%s: Double unmap detected! bailing...\n", __func__);\r\nreturn;\r\n}\r\nhandle->kmap_cnt--;\r\nif (!handle->kmap_cnt)\r\nion_buffer_kmap_put(buffer);\r\n}\r\nvoid *ion_map_kernel(struct ion_client *client, struct ion_handle *handle)\r\n{\r\nstruct ion_buffer *buffer;\r\nvoid *vaddr;\r\nmutex_lock(&client->lock);\r\nif (!ion_handle_validate(client, handle)) {\r\npr_err("%s: invalid handle passed to map_kernel.\n",\r\n__func__);\r\nmutex_unlock(&client->lock);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nbuffer = handle->buffer;\r\nif (!handle->buffer->heap->ops->map_kernel) {\r\npr_err("%s: map_kernel is not implemented by this heap.\n",\r\n__func__);\r\nmutex_unlock(&client->lock);\r\nreturn ERR_PTR(-ENODEV);\r\n}\r\nmutex_lock(&buffer->lock);\r\nvaddr = ion_handle_kmap_get(handle);\r\nmutex_unlock(&buffer->lock);\r\nmutex_unlock(&client->lock);\r\nreturn vaddr;\r\n}\r\nvoid ion_unmap_kernel(struct ion_client *client, struct ion_handle *handle)\r\n{\r\nstruct ion_buffer *buffer;\r\nmutex_lock(&client->lock);\r\nbuffer = handle->buffer;\r\nmutex_lock(&buffer->lock);\r\nion_handle_kmap_put(handle);\r\nmutex_unlock(&buffer->lock);\r\nmutex_unlock(&client->lock);\r\n}\r\nstatic int ion_debug_client_show(struct seq_file *s, void *unused)\r\n{\r\nstruct ion_client *client = s->private;\r\nstruct rb_node *n;\r\nsize_t sizes[ION_NUM_HEAP_IDS] = {0};\r\nconst char *names[ION_NUM_HEAP_IDS] = {NULL};\r\nint i;\r\nmutex_lock(&client->lock);\r\nfor (n = rb_first(&client->handles); n; n = rb_next(n)) {\r\nstruct ion_handle *handle = rb_entry(n, struct ion_handle,\r\nnode);\r\nunsigned int id = handle->buffer->heap->id;\r\nif (!names[id])\r\nnames[id] = handle->buffer->heap->name;\r\nsizes[id] += handle->buffer->size;\r\n}\r\nmutex_unlock(&client->lock);\r\nseq_printf(s, "%16.16s: %16.16s\n", "heap_name", "size_in_bytes");\r\nfor (i = 0; i < ION_NUM_HEAP_IDS; i++) {\r\nif (!names[i])\r\ncontinue;\r\nseq_printf(s, "%16.16s: %16zu\n", names[i], sizes[i]);\r\n}\r\nreturn 0;\r\n}\r\nstatic int ion_debug_client_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, ion_debug_client_show, inode->i_private);\r\n}\r\nstatic int ion_get_client_serial(const struct rb_root *root,\r\nconst unsigned char *name)\r\n{\r\nint serial = -1;\r\nstruct rb_node *node;\r\nfor (node = rb_first(root); node; node = rb_next(node)) {\r\nstruct ion_client *client = rb_entry(node, struct ion_client,\r\nnode);\r\nif (strcmp(client->name, name))\r\ncontinue;\r\nserial = max(serial, client->display_serial);\r\n}\r\nreturn serial + 1;\r\n}\r\nstruct ion_client *ion_client_create(struct ion_device *dev,\r\nconst char *name)\r\n{\r\nstruct ion_client *client;\r\nstruct task_struct *task;\r\nstruct rb_node **p;\r\nstruct rb_node *parent = NULL;\r\nstruct ion_client *entry;\r\npid_t pid;\r\nif (!name) {\r\npr_err("%s: Name cannot be null\n", __func__);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nget_task_struct(current->group_leader);\r\ntask_lock(current->group_leader);\r\npid = task_pid_nr(current->group_leader);\r\nif (current->group_leader->flags & PF_KTHREAD) {\r\nput_task_struct(current->group_leader);\r\ntask = NULL;\r\n} else {\r\ntask = current->group_leader;\r\n}\r\ntask_unlock(current->group_leader);\r\nclient = kzalloc(sizeof(struct ion_client), GFP_KERNEL);\r\nif (!client)\r\ngoto err_put_task_struct;\r\nclient->dev = dev;\r\nclient->handles = RB_ROOT;\r\nidr_init(&client->idr);\r\nmutex_init(&client->lock);\r\nclient->task = task;\r\nclient->pid = pid;\r\nclient->name = kstrdup(name, GFP_KERNEL);\r\nif (!client->name)\r\ngoto err_free_client;\r\ndown_write(&dev->lock);\r\nclient->display_serial = ion_get_client_serial(&dev->clients, name);\r\nclient->display_name = kasprintf(\r\nGFP_KERNEL, "%s-%d", name, client->display_serial);\r\nif (!client->display_name) {\r\nup_write(&dev->lock);\r\ngoto err_free_client_name;\r\n}\r\np = &dev->clients.rb_node;\r\nwhile (*p) {\r\nparent = *p;\r\nentry = rb_entry(parent, struct ion_client, node);\r\nif (client < entry)\r\np = &(*p)->rb_left;\r\nelse if (client > entry)\r\np = &(*p)->rb_right;\r\n}\r\nrb_link_node(&client->node, parent, p);\r\nrb_insert_color(&client->node, &dev->clients);\r\nclient->debug_root = debugfs_create_file(client->display_name, 0664,\r\ndev->clients_debug_root,\r\nclient, &debug_client_fops);\r\nif (!client->debug_root) {\r\nchar buf[256], *path;\r\npath = dentry_path(dev->clients_debug_root, buf, 256);\r\npr_err("Failed to create client debugfs at %s/%s\n",\r\npath, client->display_name);\r\n}\r\nup_write(&dev->lock);\r\nreturn client;\r\nerr_free_client_name:\r\nkfree(client->name);\r\nerr_free_client:\r\nkfree(client);\r\nerr_put_task_struct:\r\nif (task)\r\nput_task_struct(current->group_leader);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvoid ion_client_destroy(struct ion_client *client)\r\n{\r\nstruct ion_device *dev = client->dev;\r\nstruct rb_node *n;\r\npr_debug("%s: %d\n", __func__, __LINE__);\r\nwhile ((n = rb_first(&client->handles))) {\r\nstruct ion_handle *handle = rb_entry(n, struct ion_handle,\r\nnode);\r\nion_handle_destroy(&handle->ref);\r\n}\r\nidr_destroy(&client->idr);\r\ndown_write(&dev->lock);\r\nif (client->task)\r\nput_task_struct(client->task);\r\nrb_erase(&client->node, &dev->clients);\r\ndebugfs_remove_recursive(client->debug_root);\r\nup_write(&dev->lock);\r\nkfree(client->display_name);\r\nkfree(client->name);\r\nkfree(client);\r\n}\r\nstruct sg_table *ion_sg_table(struct ion_client *client,\r\nstruct ion_handle *handle)\r\n{\r\nstruct ion_buffer *buffer;\r\nstruct sg_table *table;\r\nmutex_lock(&client->lock);\r\nif (!ion_handle_validate(client, handle)) {\r\npr_err("%s: invalid handle passed to map_dma.\n",\r\n__func__);\r\nmutex_unlock(&client->lock);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nbuffer = handle->buffer;\r\ntable = buffer->sg_table;\r\nmutex_unlock(&client->lock);\r\nreturn table;\r\n}\r\nstatic struct sg_table *ion_map_dma_buf(struct dma_buf_attachment *attachment,\r\nenum dma_data_direction direction)\r\n{\r\nstruct dma_buf *dmabuf = attachment->dmabuf;\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nion_buffer_sync_for_device(buffer, attachment->dev, direction);\r\nreturn buffer->sg_table;\r\n}\r\nstatic void ion_unmap_dma_buf(struct dma_buf_attachment *attachment,\r\nstruct sg_table *table,\r\nenum dma_data_direction direction)\r\n{\r\n}\r\nvoid ion_pages_sync_for_device(struct device *dev, struct page *page,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nstruct scatterlist sg;\r\nsg_init_table(&sg, 1);\r\nsg_set_page(&sg, page, size, 0);\r\nsg_dma_address(&sg) = page_to_phys(page);\r\ndma_sync_sg_for_device(dev, &sg, 1, dir);\r\n}\r\nstatic void ion_buffer_sync_for_device(struct ion_buffer *buffer,\r\nstruct device *dev,\r\nenum dma_data_direction dir)\r\n{\r\nstruct ion_vma_list *vma_list;\r\nint pages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;\r\nint i;\r\npr_debug("%s: syncing for device %s\n", __func__,\r\ndev ? dev_name(dev) : "null");\r\nif (!ion_buffer_fault_user_mappings(buffer))\r\nreturn;\r\nmutex_lock(&buffer->lock);\r\nfor (i = 0; i < pages; i++) {\r\nstruct page *page = buffer->pages[i];\r\nif (ion_buffer_page_is_dirty(page))\r\nion_pages_sync_for_device(dev, ion_buffer_page(page),\r\nPAGE_SIZE, dir);\r\nion_buffer_page_clean(buffer->pages + i);\r\n}\r\nlist_for_each_entry(vma_list, &buffer->vmas, list) {\r\nstruct vm_area_struct *vma = vma_list->vma;\r\nzap_page_range(vma, vma->vm_start, vma->vm_end - vma->vm_start,\r\nNULL);\r\n}\r\nmutex_unlock(&buffer->lock);\r\n}\r\nstatic int ion_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct ion_buffer *buffer = vma->vm_private_data;\r\nunsigned long pfn;\r\nint ret;\r\nmutex_lock(&buffer->lock);\r\nion_buffer_page_dirty(buffer->pages + vmf->pgoff);\r\nBUG_ON(!buffer->pages || !buffer->pages[vmf->pgoff]);\r\npfn = page_to_pfn(ion_buffer_page(buffer->pages[vmf->pgoff]));\r\nret = vm_insert_pfn(vma, (unsigned long)vmf->virtual_address, pfn);\r\nmutex_unlock(&buffer->lock);\r\nif (ret)\r\nreturn VM_FAULT_ERROR;\r\nreturn VM_FAULT_NOPAGE;\r\n}\r\nstatic void ion_vm_open(struct vm_area_struct *vma)\r\n{\r\nstruct ion_buffer *buffer = vma->vm_private_data;\r\nstruct ion_vma_list *vma_list;\r\nvma_list = kmalloc(sizeof(struct ion_vma_list), GFP_KERNEL);\r\nif (!vma_list)\r\nreturn;\r\nvma_list->vma = vma;\r\nmutex_lock(&buffer->lock);\r\nlist_add(&vma_list->list, &buffer->vmas);\r\nmutex_unlock(&buffer->lock);\r\npr_debug("%s: adding %p\n", __func__, vma);\r\n}\r\nstatic void ion_vm_close(struct vm_area_struct *vma)\r\n{\r\nstruct ion_buffer *buffer = vma->vm_private_data;\r\nstruct ion_vma_list *vma_list, *tmp;\r\npr_debug("%s\n", __func__);\r\nmutex_lock(&buffer->lock);\r\nlist_for_each_entry_safe(vma_list, tmp, &buffer->vmas, list) {\r\nif (vma_list->vma != vma)\r\ncontinue;\r\nlist_del(&vma_list->list);\r\nkfree(vma_list);\r\npr_debug("%s: deleting %p\n", __func__, vma);\r\nbreak;\r\n}\r\nmutex_unlock(&buffer->lock);\r\n}\r\nstatic int ion_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nint ret = 0;\r\nif (!buffer->heap->ops->map_user) {\r\npr_err("%s: this heap does not define a method for mapping to userspace\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nif (ion_buffer_fault_user_mappings(buffer)) {\r\nvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND |\r\nVM_DONTDUMP;\r\nvma->vm_private_data = buffer;\r\nvma->vm_ops = &ion_vma_ops;\r\nion_vm_open(vma);\r\nreturn 0;\r\n}\r\nif (!(buffer->flags & ION_FLAG_CACHED))\r\nvma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);\r\nmutex_lock(&buffer->lock);\r\nret = buffer->heap->ops->map_user(buffer->heap, buffer, vma);\r\nmutex_unlock(&buffer->lock);\r\nif (ret)\r\npr_err("%s: failure mapping buffer to userspace\n",\r\n__func__);\r\nreturn ret;\r\n}\r\nstatic void ion_dma_buf_release(struct dma_buf *dmabuf)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nion_buffer_put(buffer);\r\n}\r\nstatic void *ion_dma_buf_kmap(struct dma_buf *dmabuf, unsigned long offset)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nreturn buffer->vaddr + offset * PAGE_SIZE;\r\n}\r\nstatic void ion_dma_buf_kunmap(struct dma_buf *dmabuf, unsigned long offset,\r\nvoid *ptr)\r\n{\r\n}\r\nstatic int ion_dma_buf_begin_cpu_access(struct dma_buf *dmabuf, size_t start,\r\nsize_t len,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nvoid *vaddr;\r\nif (!buffer->heap->ops->map_kernel) {\r\npr_err("%s: map kernel is not implemented by this heap.\n",\r\n__func__);\r\nreturn -ENODEV;\r\n}\r\nmutex_lock(&buffer->lock);\r\nvaddr = ion_buffer_kmap_get(buffer);\r\nmutex_unlock(&buffer->lock);\r\nreturn PTR_ERR_OR_ZERO(vaddr);\r\n}\r\nstatic void ion_dma_buf_end_cpu_access(struct dma_buf *dmabuf, size_t start,\r\nsize_t len,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nmutex_lock(&buffer->lock);\r\nion_buffer_kmap_put(buffer);\r\nmutex_unlock(&buffer->lock);\r\n}\r\nstruct dma_buf *ion_share_dma_buf(struct ion_client *client,\r\nstruct ion_handle *handle)\r\n{\r\nDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\r\nstruct ion_buffer *buffer;\r\nstruct dma_buf *dmabuf;\r\nbool valid_handle;\r\nmutex_lock(&client->lock);\r\nvalid_handle = ion_handle_validate(client, handle);\r\nif (!valid_handle) {\r\nWARN(1, "%s: invalid handle passed to share.\n", __func__);\r\nmutex_unlock(&client->lock);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nbuffer = handle->buffer;\r\nion_buffer_get(buffer);\r\nmutex_unlock(&client->lock);\r\nexp_info.ops = &dma_buf_ops;\r\nexp_info.size = buffer->size;\r\nexp_info.flags = O_RDWR;\r\nexp_info.priv = buffer;\r\ndmabuf = dma_buf_export(&exp_info);\r\nif (IS_ERR(dmabuf)) {\r\nion_buffer_put(buffer);\r\nreturn dmabuf;\r\n}\r\nreturn dmabuf;\r\n}\r\nint ion_share_dma_buf_fd(struct ion_client *client, struct ion_handle *handle)\r\n{\r\nstruct dma_buf *dmabuf;\r\nint fd;\r\ndmabuf = ion_share_dma_buf(client, handle);\r\nif (IS_ERR(dmabuf))\r\nreturn PTR_ERR(dmabuf);\r\nfd = dma_buf_fd(dmabuf, O_CLOEXEC);\r\nif (fd < 0)\r\ndma_buf_put(dmabuf);\r\nreturn fd;\r\n}\r\nstruct ion_handle *ion_import_dma_buf(struct ion_client *client, int fd)\r\n{\r\nstruct dma_buf *dmabuf;\r\nstruct ion_buffer *buffer;\r\nstruct ion_handle *handle;\r\nint ret;\r\ndmabuf = dma_buf_get(fd);\r\nif (IS_ERR(dmabuf))\r\nreturn ERR_CAST(dmabuf);\r\nif (dmabuf->ops != &dma_buf_ops) {\r\npr_err("%s: can not import dmabuf from another exporter\n",\r\n__func__);\r\ndma_buf_put(dmabuf);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nbuffer = dmabuf->priv;\r\nmutex_lock(&client->lock);\r\nhandle = ion_handle_lookup(client, buffer);\r\nif (!IS_ERR(handle)) {\r\nion_handle_get(handle);\r\nmutex_unlock(&client->lock);\r\ngoto end;\r\n}\r\nhandle = ion_handle_create(client, buffer);\r\nif (IS_ERR(handle)) {\r\nmutex_unlock(&client->lock);\r\ngoto end;\r\n}\r\nret = ion_handle_add(client, handle);\r\nmutex_unlock(&client->lock);\r\nif (ret) {\r\nion_handle_put(handle);\r\nhandle = ERR_PTR(ret);\r\n}\r\nend:\r\ndma_buf_put(dmabuf);\r\nreturn handle;\r\n}\r\nstatic int ion_sync_for_device(struct ion_client *client, int fd)\r\n{\r\nstruct dma_buf *dmabuf;\r\nstruct ion_buffer *buffer;\r\ndmabuf = dma_buf_get(fd);\r\nif (IS_ERR(dmabuf))\r\nreturn PTR_ERR(dmabuf);\r\nif (dmabuf->ops != &dma_buf_ops) {\r\npr_err("%s: can not sync dmabuf from another exporter\n",\r\n__func__);\r\ndma_buf_put(dmabuf);\r\nreturn -EINVAL;\r\n}\r\nbuffer = dmabuf->priv;\r\ndma_sync_sg_for_device(NULL, buffer->sg_table->sgl,\r\nbuffer->sg_table->nents, DMA_BIDIRECTIONAL);\r\ndma_buf_put(dmabuf);\r\nreturn 0;\r\n}\r\nstatic unsigned int ion_ioctl_dir(unsigned int cmd)\r\n{\r\nswitch (cmd) {\r\ncase ION_IOC_SYNC:\r\ncase ION_IOC_FREE:\r\ncase ION_IOC_CUSTOM:\r\nreturn _IOC_WRITE;\r\ndefault:\r\nreturn _IOC_DIR(cmd);\r\n}\r\n}\r\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\r\n{\r\nstruct ion_client *client = filp->private_data;\r\nstruct ion_device *dev = client->dev;\r\nstruct ion_handle *cleanup_handle = NULL;\r\nint ret = 0;\r\nunsigned int dir;\r\nunion {\r\nstruct ion_fd_data fd;\r\nstruct ion_allocation_data allocation;\r\nstruct ion_handle_data handle;\r\nstruct ion_custom_data custom;\r\n} data;\r\ndir = ion_ioctl_dir(cmd);\r\nif (_IOC_SIZE(cmd) > sizeof(data))\r\nreturn -EINVAL;\r\nif (dir & _IOC_WRITE)\r\nif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\r\nreturn -EFAULT;\r\nswitch (cmd) {\r\ncase ION_IOC_ALLOC:\r\n{\r\nstruct ion_handle *handle;\r\nhandle = ion_alloc(client, data.allocation.len,\r\ndata.allocation.align,\r\ndata.allocation.heap_id_mask,\r\ndata.allocation.flags);\r\nif (IS_ERR(handle))\r\nreturn PTR_ERR(handle);\r\ndata.allocation.handle = handle->id;\r\ncleanup_handle = handle;\r\nbreak;\r\n}\r\ncase ION_IOC_FREE:\r\n{\r\nstruct ion_handle *handle;\r\nhandle = ion_handle_get_by_id(client, data.handle.handle);\r\nif (IS_ERR(handle))\r\nreturn PTR_ERR(handle);\r\nion_free(client, handle);\r\nion_handle_put(handle);\r\nbreak;\r\n}\r\ncase ION_IOC_SHARE:\r\ncase ION_IOC_MAP:\r\n{\r\nstruct ion_handle *handle;\r\nhandle = ion_handle_get_by_id(client, data.handle.handle);\r\nif (IS_ERR(handle))\r\nreturn PTR_ERR(handle);\r\ndata.fd.fd = ion_share_dma_buf_fd(client, handle);\r\nion_handle_put(handle);\r\nif (data.fd.fd < 0)\r\nret = data.fd.fd;\r\nbreak;\r\n}\r\ncase ION_IOC_IMPORT:\r\n{\r\nstruct ion_handle *handle;\r\nhandle = ion_import_dma_buf(client, data.fd.fd);\r\nif (IS_ERR(handle))\r\nret = PTR_ERR(handle);\r\nelse\r\ndata.handle.handle = handle->id;\r\nbreak;\r\n}\r\ncase ION_IOC_SYNC:\r\n{\r\nret = ion_sync_for_device(client, data.fd.fd);\r\nbreak;\r\n}\r\ncase ION_IOC_CUSTOM:\r\n{\r\nif (!dev->custom_ioctl)\r\nreturn -ENOTTY;\r\nret = dev->custom_ioctl(client, data.custom.cmd,\r\ndata.custom.arg);\r\nbreak;\r\n}\r\ndefault:\r\nreturn -ENOTTY;\r\n}\r\nif (dir & _IOC_READ) {\r\nif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\r\nif (cleanup_handle)\r\nion_free(client, cleanup_handle);\r\nreturn -EFAULT;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int ion_release(struct inode *inode, struct file *file)\r\n{\r\nstruct ion_client *client = file->private_data;\r\npr_debug("%s: %d\n", __func__, __LINE__);\r\nion_client_destroy(client);\r\nreturn 0;\r\n}\r\nstatic int ion_open(struct inode *inode, struct file *file)\r\n{\r\nstruct miscdevice *miscdev = file->private_data;\r\nstruct ion_device *dev = container_of(miscdev, struct ion_device, dev);\r\nstruct ion_client *client;\r\nchar debug_name[64];\r\npr_debug("%s: %d\n", __func__, __LINE__);\r\nsnprintf(debug_name, 64, "%u", task_pid_nr(current->group_leader));\r\nclient = ion_client_create(dev, debug_name);\r\nif (IS_ERR(client))\r\nreturn PTR_ERR(client);\r\nfile->private_data = client;\r\nreturn 0;\r\n}\r\nstatic size_t ion_debug_heap_total(struct ion_client *client,\r\nunsigned int id)\r\n{\r\nsize_t size = 0;\r\nstruct rb_node *n;\r\nmutex_lock(&client->lock);\r\nfor (n = rb_first(&client->handles); n; n = rb_next(n)) {\r\nstruct ion_handle *handle = rb_entry(n,\r\nstruct ion_handle,\r\nnode);\r\nif (handle->buffer->heap->id == id)\r\nsize += handle->buffer->size;\r\n}\r\nmutex_unlock(&client->lock);\r\nreturn size;\r\n}\r\nstatic int ion_debug_heap_show(struct seq_file *s, void *unused)\r\n{\r\nstruct ion_heap *heap = s->private;\r\nstruct ion_device *dev = heap->dev;\r\nstruct rb_node *n;\r\nsize_t total_size = 0;\r\nsize_t total_orphaned_size = 0;\r\nseq_printf(s, "%16s %16s %16s\n", "client", "pid", "size");\r\nseq_puts(s, "----------------------------------------------------\n");\r\nfor (n = rb_first(&dev->clients); n; n = rb_next(n)) {\r\nstruct ion_client *client = rb_entry(n, struct ion_client,\r\nnode);\r\nsize_t size = ion_debug_heap_total(client, heap->id);\r\nif (!size)\r\ncontinue;\r\nif (client->task) {\r\nchar task_comm[TASK_COMM_LEN];\r\nget_task_comm(task_comm, client->task);\r\nseq_printf(s, "%16s %16u %16zu\n", task_comm,\r\nclient->pid, size);\r\n} else {\r\nseq_printf(s, "%16s %16u %16zu\n", client->name,\r\nclient->pid, size);\r\n}\r\n}\r\nseq_puts(s, "----------------------------------------------------\n");\r\nseq_puts(s, "orphaned allocations (info is from last known client):\n");\r\nmutex_lock(&dev->buffer_lock);\r\nfor (n = rb_first(&dev->buffers); n; n = rb_next(n)) {\r\nstruct ion_buffer *buffer = rb_entry(n, struct ion_buffer,\r\nnode);\r\nif (buffer->heap->id != heap->id)\r\ncontinue;\r\ntotal_size += buffer->size;\r\nif (!buffer->handle_count) {\r\nseq_printf(s, "%16s %16u %16zu %d %d\n",\r\nbuffer->task_comm, buffer->pid,\r\nbuffer->size, buffer->kmap_cnt,\r\natomic_read(&buffer->ref.refcount));\r\ntotal_orphaned_size += buffer->size;\r\n}\r\n}\r\nmutex_unlock(&dev->buffer_lock);\r\nseq_puts(s, "----------------------------------------------------\n");\r\nseq_printf(s, "%16s %16zu\n", "total orphaned",\r\ntotal_orphaned_size);\r\nseq_printf(s, "%16s %16zu\n", "total ", total_size);\r\nif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\r\nseq_printf(s, "%16s %16zu\n", "deferred free",\r\nheap->free_list_size);\r\nseq_puts(s, "----------------------------------------------------\n");\r\nif (heap->debug_show)\r\nheap->debug_show(heap, s, unused);\r\nreturn 0;\r\n}\r\nstatic int ion_debug_heap_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, ion_debug_heap_show, inode->i_private);\r\n}\r\nstatic int debug_shrink_set(void *data, u64 val)\r\n{\r\nstruct ion_heap *heap = data;\r\nstruct shrink_control sc;\r\nint objs;\r\nsc.gfp_mask = -1;\r\nsc.nr_to_scan = val;\r\nif (!val) {\r\nobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\r\nsc.nr_to_scan = objs;\r\n}\r\nheap->shrinker.scan_objects(&heap->shrinker, &sc);\r\nreturn 0;\r\n}\r\nstatic int debug_shrink_get(void *data, u64 *val)\r\n{\r\nstruct ion_heap *heap = data;\r\nstruct shrink_control sc;\r\nint objs;\r\nsc.gfp_mask = -1;\r\nsc.nr_to_scan = 0;\r\nobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\r\n*val = objs;\r\nreturn 0;\r\n}\r\nvoid ion_device_add_heap(struct ion_device *dev, struct ion_heap *heap)\r\n{\r\nstruct dentry *debug_file;\r\nif (!heap->ops->allocate || !heap->ops->free || !heap->ops->map_dma ||\r\n!heap->ops->unmap_dma)\r\npr_err("%s: can not add heap with invalid ops struct.\n",\r\n__func__);\r\nspin_lock_init(&heap->free_lock);\r\nheap->free_list_size = 0;\r\nif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\r\nion_heap_init_deferred_free(heap);\r\nif ((heap->flags & ION_HEAP_FLAG_DEFER_FREE) || heap->ops->shrink)\r\nion_heap_init_shrinker(heap);\r\nheap->dev = dev;\r\ndown_write(&dev->lock);\r\nplist_node_init(&heap->node, -heap->id);\r\nplist_add(&heap->node, &dev->heaps);\r\ndebug_file = debugfs_create_file(heap->name, 0664,\r\ndev->heaps_debug_root, heap,\r\n&debug_heap_fops);\r\nif (!debug_file) {\r\nchar buf[256], *path;\r\npath = dentry_path(dev->heaps_debug_root, buf, 256);\r\npr_err("Failed to create heap debugfs at %s/%s\n",\r\npath, heap->name);\r\n}\r\nif (heap->shrinker.count_objects && heap->shrinker.scan_objects) {\r\nchar debug_name[64];\r\nsnprintf(debug_name, 64, "%s_shrink", heap->name);\r\ndebug_file = debugfs_create_file(\r\ndebug_name, 0644, dev->heaps_debug_root, heap,\r\n&debug_shrink_fops);\r\nif (!debug_file) {\r\nchar buf[256], *path;\r\npath = dentry_path(dev->heaps_debug_root, buf, 256);\r\npr_err("Failed to create heap shrinker debugfs at %s/%s\n",\r\npath, debug_name);\r\n}\r\n}\r\nup_write(&dev->lock);\r\n}\r\nstruct ion_device *ion_device_create(long (*custom_ioctl)\r\n(struct ion_client *client,\r\nunsigned int cmd,\r\nunsigned long arg))\r\n{\r\nstruct ion_device *idev;\r\nint ret;\r\nidev = kzalloc(sizeof(struct ion_device), GFP_KERNEL);\r\nif (!idev)\r\nreturn ERR_PTR(-ENOMEM);\r\nidev->dev.minor = MISC_DYNAMIC_MINOR;\r\nidev->dev.name = "ion";\r\nidev->dev.fops = &ion_fops;\r\nidev->dev.parent = NULL;\r\nret = misc_register(&idev->dev);\r\nif (ret) {\r\npr_err("ion: failed to register misc device.\n");\r\nkfree(idev);\r\nreturn ERR_PTR(ret);\r\n}\r\nidev->debug_root = debugfs_create_dir("ion", NULL);\r\nif (!idev->debug_root) {\r\npr_err("ion: failed to create debugfs root directory.\n");\r\ngoto debugfs_done;\r\n}\r\nidev->heaps_debug_root = debugfs_create_dir("heaps", idev->debug_root);\r\nif (!idev->heaps_debug_root) {\r\npr_err("ion: failed to create debugfs heaps directory.\n");\r\ngoto debugfs_done;\r\n}\r\nidev->clients_debug_root = debugfs_create_dir("clients",\r\nidev->debug_root);\r\nif (!idev->clients_debug_root)\r\npr_err("ion: failed to create debugfs clients directory.\n");\r\ndebugfs_done:\r\nidev->custom_ioctl = custom_ioctl;\r\nidev->buffers = RB_ROOT;\r\nmutex_init(&idev->buffer_lock);\r\ninit_rwsem(&idev->lock);\r\nplist_head_init(&idev->heaps);\r\nidev->clients = RB_ROOT;\r\nreturn idev;\r\n}\r\nvoid ion_device_destroy(struct ion_device *dev)\r\n{\r\nmisc_deregister(&dev->dev);\r\ndebugfs_remove_recursive(dev->debug_root);\r\nkfree(dev);\r\n}\r\nvoid __init ion_reserve(struct ion_platform_data *data)\r\n{\r\nint i;\r\nfor (i = 0; i < data->nr; i++) {\r\nif (data->heaps[i].size == 0)\r\ncontinue;\r\nif (data->heaps[i].base == 0) {\r\nphys_addr_t paddr;\r\npaddr = memblock_alloc_base(data->heaps[i].size,\r\ndata->heaps[i].align,\r\nMEMBLOCK_ALLOC_ANYWHERE);\r\nif (!paddr) {\r\npr_err("%s: error allocating memblock for heap %d\n",\r\n__func__, i);\r\ncontinue;\r\n}\r\ndata->heaps[i].base = paddr;\r\n} else {\r\nint ret = memblock_reserve(data->heaps[i].base,\r\ndata->heaps[i].size);\r\nif (ret)\r\npr_err("memblock reserve of %zx@%lx failed\n",\r\ndata->heaps[i].size,\r\ndata->heaps[i].base);\r\n}\r\npr_info("%s: %s reserved base %lx size %zu\n", __func__,\r\ndata->heaps[i].name,\r\ndata->heaps[i].base,\r\ndata->heaps[i].size);\r\n}\r\n}
