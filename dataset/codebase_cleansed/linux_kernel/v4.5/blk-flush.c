static unsigned int blk_flush_policy(unsigned int fflags, struct request *rq)\r\n{\r\nunsigned int policy = 0;\r\nif (blk_rq_sectors(rq))\r\npolicy |= REQ_FSEQ_DATA;\r\nif (fflags & REQ_FLUSH) {\r\nif (rq->cmd_flags & REQ_FLUSH)\r\npolicy |= REQ_FSEQ_PREFLUSH;\r\nif (!(fflags & REQ_FUA) && (rq->cmd_flags & REQ_FUA))\r\npolicy |= REQ_FSEQ_POSTFLUSH;\r\n}\r\nreturn policy;\r\n}\r\nstatic unsigned int blk_flush_cur_seq(struct request *rq)\r\n{\r\nreturn 1 << ffz(rq->flush.seq);\r\n}\r\nstatic void blk_flush_restore_request(struct request *rq)\r\n{\r\nrq->bio = rq->biotail;\r\nrq->cmd_flags &= ~REQ_FLUSH_SEQ;\r\nrq->end_io = rq->flush.saved_end_io;\r\n}\r\nstatic bool blk_flush_queue_rq(struct request *rq, bool add_front)\r\n{\r\nif (rq->q->mq_ops) {\r\nstruct request_queue *q = rq->q;\r\nblk_mq_add_to_requeue_list(rq, add_front);\r\nblk_mq_kick_requeue_list(q);\r\nreturn false;\r\n} else {\r\nif (add_front)\r\nlist_add(&rq->queuelist, &rq->q->queue_head);\r\nelse\r\nlist_add_tail(&rq->queuelist, &rq->q->queue_head);\r\nreturn true;\r\n}\r\n}\r\nstatic bool blk_flush_complete_seq(struct request *rq,\r\nstruct blk_flush_queue *fq,\r\nunsigned int seq, int error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\r\nbool queued = false, kicked;\r\nBUG_ON(rq->flush.seq & seq);\r\nrq->flush.seq |= seq;\r\nif (likely(!error))\r\nseq = blk_flush_cur_seq(rq);\r\nelse\r\nseq = REQ_FSEQ_DONE;\r\nswitch (seq) {\r\ncase REQ_FSEQ_PREFLUSH:\r\ncase REQ_FSEQ_POSTFLUSH:\r\nif (list_empty(pending))\r\nfq->flush_pending_since = jiffies;\r\nlist_move_tail(&rq->flush.list, pending);\r\nbreak;\r\ncase REQ_FSEQ_DATA:\r\nlist_move_tail(&rq->flush.list, &fq->flush_data_in_flight);\r\nqueued = blk_flush_queue_rq(rq, true);\r\nbreak;\r\ncase REQ_FSEQ_DONE:\r\nBUG_ON(!list_empty(&rq->queuelist));\r\nlist_del_init(&rq->flush.list);\r\nblk_flush_restore_request(rq);\r\nif (q->mq_ops)\r\nblk_mq_end_request(rq, error);\r\nelse\r\n__blk_end_request_all(rq, error);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nkicked = blk_kick_flush(q, fq);\r\nreturn kicked | queued;\r\n}\r\nstatic void flush_end_io(struct request *flush_rq, int error)\r\n{\r\nstruct request_queue *q = flush_rq->q;\r\nstruct list_head *running;\r\nbool queued = false;\r\nstruct request *rq, *n;\r\nunsigned long flags = 0;\r\nstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\r\nif (q->mq_ops) {\r\nstruct blk_mq_hw_ctx *hctx;\r\nspin_lock_irqsave(&fq->mq_flush_lock, flags);\r\nhctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\r\nblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\r\nflush_rq->tag = -1;\r\n}\r\nrunning = &fq->flush_queue[fq->flush_running_idx];\r\nBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\r\nfq->flush_running_idx ^= 1;\r\nif (!q->mq_ops)\r\nelv_completed_request(q, flush_rq);\r\nlist_for_each_entry_safe(rq, n, running, flush.list) {\r\nunsigned int seq = blk_flush_cur_seq(rq);\r\nBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\r\nqueued |= blk_flush_complete_seq(rq, fq, seq, error);\r\n}\r\nif (queued || fq->flush_queue_delayed) {\r\nWARN_ON(q->mq_ops);\r\nblk_run_queue_async(q);\r\n}\r\nfq->flush_queue_delayed = 0;\r\nif (q->mq_ops)\r\nspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\r\n}\r\nstatic bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\r\n{\r\nstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\r\nstruct request *first_rq =\r\nlist_first_entry(pending, struct request, flush.list);\r\nstruct request *flush_rq = fq->flush_rq;\r\nif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\r\nreturn false;\r\nif (!list_empty(&fq->flush_data_in_flight) &&\r\ntime_before(jiffies,\r\nfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\r\nreturn false;\r\nfq->flush_pending_idx ^= 1;\r\nblk_rq_init(q, flush_rq);\r\nif (q->mq_ops) {\r\nstruct blk_mq_hw_ctx *hctx;\r\nflush_rq->mq_ctx = first_rq->mq_ctx;\r\nflush_rq->tag = first_rq->tag;\r\nfq->orig_rq = first_rq;\r\nhctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\r\nblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\r\n}\r\nflush_rq->cmd_type = REQ_TYPE_FS;\r\nflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\r\nflush_rq->rq_disk = first_rq->rq_disk;\r\nflush_rq->end_io = flush_end_io;\r\nreturn blk_flush_queue_rq(flush_rq, false);\r\n}\r\nstatic void flush_data_end_io(struct request *rq, int error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\r\nif (blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error))\r\nblk_run_queue_async(q);\r\n}\r\nstatic void mq_flush_data_end_io(struct request *rq, int error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct blk_mq_hw_ctx *hctx;\r\nstruct blk_mq_ctx *ctx = rq->mq_ctx;\r\nunsigned long flags;\r\nstruct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);\r\nhctx = q->mq_ops->map_queue(q, ctx->cpu);\r\nspin_lock_irqsave(&fq->mq_flush_lock, flags);\r\nif (blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error))\r\nblk_mq_run_hw_queue(hctx, true);\r\nspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\r\n}\r\nvoid blk_insert_flush(struct request *rq)\r\n{\r\nstruct request_queue *q = rq->q;\r\nunsigned int fflags = q->flush_flags;\r\nunsigned int policy = blk_flush_policy(fflags, rq);\r\nstruct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);\r\nrq->cmd_flags &= ~REQ_FLUSH;\r\nif (!(fflags & REQ_FUA))\r\nrq->cmd_flags &= ~REQ_FUA;\r\nif (!policy) {\r\nif (q->mq_ops)\r\nblk_mq_end_request(rq, 0);\r\nelse\r\n__blk_end_bidi_request(rq, 0, 0, 0);\r\nreturn;\r\n}\r\nBUG_ON(rq->bio != rq->biotail);\r\nif ((policy & REQ_FSEQ_DATA) &&\r\n!(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {\r\nif (q->mq_ops) {\r\nblk_mq_insert_request(rq, false, false, true);\r\n} else\r\nlist_add_tail(&rq->queuelist, &q->queue_head);\r\nreturn;\r\n}\r\nmemset(&rq->flush, 0, sizeof(rq->flush));\r\nINIT_LIST_HEAD(&rq->flush.list);\r\nrq->cmd_flags |= REQ_FLUSH_SEQ;\r\nrq->flush.saved_end_io = rq->end_io;\r\nif (q->mq_ops) {\r\nrq->end_io = mq_flush_data_end_io;\r\nspin_lock_irq(&fq->mq_flush_lock);\r\nblk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);\r\nspin_unlock_irq(&fq->mq_flush_lock);\r\nreturn;\r\n}\r\nrq->end_io = flush_data_end_io;\r\nblk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);\r\n}\r\nint blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,\r\nsector_t *error_sector)\r\n{\r\nstruct request_queue *q;\r\nstruct bio *bio;\r\nint ret = 0;\r\nif (bdev->bd_disk == NULL)\r\nreturn -ENXIO;\r\nq = bdev_get_queue(bdev);\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!q->make_request_fn)\r\nreturn -ENXIO;\r\nbio = bio_alloc(gfp_mask, 0);\r\nbio->bi_bdev = bdev;\r\nret = submit_bio_wait(WRITE_FLUSH, bio);\r\nif (error_sector)\r\n*error_sector = bio->bi_iter.bi_sector;\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nstruct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,\r\nint node, int cmd_size)\r\n{\r\nstruct blk_flush_queue *fq;\r\nint rq_sz = sizeof(struct request);\r\nfq = kzalloc_node(sizeof(*fq), GFP_KERNEL, node);\r\nif (!fq)\r\ngoto fail;\r\nif (q->mq_ops) {\r\nspin_lock_init(&fq->mq_flush_lock);\r\nrq_sz = round_up(rq_sz + cmd_size, cache_line_size());\r\n}\r\nfq->flush_rq = kzalloc_node(rq_sz, GFP_KERNEL, node);\r\nif (!fq->flush_rq)\r\ngoto fail_rq;\r\nINIT_LIST_HEAD(&fq->flush_queue[0]);\r\nINIT_LIST_HEAD(&fq->flush_queue[1]);\r\nINIT_LIST_HEAD(&fq->flush_data_in_flight);\r\nreturn fq;\r\nfail_rq:\r\nkfree(fq);\r\nfail:\r\nreturn NULL;\r\n}\r\nvoid blk_free_flush_queue(struct blk_flush_queue *fq)\r\n{\r\nif (!fq)\r\nreturn;\r\nkfree(fq->flush_rq);\r\nkfree(fq);\r\n}
