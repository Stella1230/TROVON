static inline int call_neg_helper(struct sk_buff *skb, int offset, void *ret,\r\nunsigned int size)\r\n{\r\nvoid *ptr = bpf_internal_load_pointer_neg_helper(skb, offset, size);\r\nif (!ptr)\r\nreturn -EFAULT;\r\nmemcpy(ret, ptr, size);\r\nreturn 0;\r\n}\r\nstatic u64 jit_get_skb_b(struct sk_buff *skb, int offset)\r\n{\r\nu8 ret;\r\nint err;\r\nif (offset < 0)\r\nerr = call_neg_helper(skb, offset, &ret, 1);\r\nelse\r\nerr = skb_copy_bits(skb, offset, &ret, 1);\r\nreturn (u64)err << 32 | ret;\r\n}\r\nstatic u64 jit_get_skb_h(struct sk_buff *skb, int offset)\r\n{\r\nu16 ret;\r\nint err;\r\nif (offset < 0)\r\nerr = call_neg_helper(skb, offset, &ret, 2);\r\nelse\r\nerr = skb_copy_bits(skb, offset, &ret, 2);\r\nreturn (u64)err << 32 | ntohs(ret);\r\n}\r\nstatic u64 jit_get_skb_w(struct sk_buff *skb, int offset)\r\n{\r\nu32 ret;\r\nint err;\r\nif (offset < 0)\r\nerr = call_neg_helper(skb, offset, &ret, 4);\r\nelse\r\nerr = skb_copy_bits(skb, offset, &ret, 4);\r\nreturn (u64)err << 32 | ntohl(ret);\r\n}\r\nstatic u32 jit_udiv(u32 dividend, u32 divisor)\r\n{\r\nreturn dividend / divisor;\r\n}\r\nstatic u32 jit_mod(u32 dividend, u32 divisor)\r\n{\r\nreturn dividend % divisor;\r\n}\r\nstatic inline void _emit(int cond, u32 inst, struct jit_ctx *ctx)\r\n{\r\ninst |= (cond << 28);\r\ninst = __opcode_to_mem_arm(inst);\r\nif (ctx->target != NULL)\r\nctx->target[ctx->idx] = inst;\r\nctx->idx++;\r\n}\r\nstatic inline void emit(u32 inst, struct jit_ctx *ctx)\r\n{\r\n_emit(ARM_COND_AL, inst, ctx);\r\n}\r\nstatic u16 saved_regs(struct jit_ctx *ctx)\r\n{\r\nu16 ret = 0;\r\nif ((ctx->skf->len > 1) ||\r\n(ctx->skf->insns[0].code == (BPF_RET | BPF_A)))\r\nret |= 1 << r_A;\r\n#ifdef CONFIG_FRAME_POINTER\r\nret |= (1 << ARM_FP) | (1 << ARM_IP) | (1 << ARM_LR) | (1 << ARM_PC);\r\n#else\r\nif (ctx->seen & SEEN_CALL)\r\nret |= 1 << ARM_LR;\r\n#endif\r\nif (ctx->seen & (SEEN_DATA | SEEN_SKB))\r\nret |= 1 << r_skb;\r\nif (ctx->seen & SEEN_DATA)\r\nret |= (1 << r_skb_data) | (1 << r_skb_hl);\r\nif (ctx->seen & SEEN_X)\r\nret |= 1 << r_X;\r\nreturn ret;\r\n}\r\nstatic inline int mem_words_used(struct jit_ctx *ctx)\r\n{\r\nreturn fls(ctx->seen & SEEN_MEM);\r\n}\r\nstatic void jit_fill_hole(void *area, unsigned int size)\r\n{\r\nu32 *ptr;\r\nfor (ptr = area; size >= sizeof(u32); size -= sizeof(u32))\r\n*ptr++ = __opcode_to_mem_arm(ARM_INST_UDF);\r\n}\r\nstatic void build_prologue(struct jit_ctx *ctx)\r\n{\r\nu16 reg_set = saved_regs(ctx);\r\nu16 off;\r\n#ifdef CONFIG_FRAME_POINTER\r\nemit(ARM_MOV_R(ARM_IP, ARM_SP), ctx);\r\nemit(ARM_PUSH(reg_set), ctx);\r\nemit(ARM_SUB_I(ARM_FP, ARM_IP, 4), ctx);\r\n#else\r\nif (reg_set)\r\nemit(ARM_PUSH(reg_set), ctx);\r\n#endif\r\nif (ctx->seen & (SEEN_DATA | SEEN_SKB))\r\nemit(ARM_MOV_R(r_skb, ARM_R0), ctx);\r\nif (ctx->seen & SEEN_DATA) {\r\noff = offsetof(struct sk_buff, data);\r\nemit(ARM_LDR_I(r_skb_data, r_skb, off), ctx);\r\noff = offsetof(struct sk_buff, len);\r\nemit(ARM_LDR_I(r_skb_hl, r_skb, off), ctx);\r\noff = offsetof(struct sk_buff, data_len);\r\nemit(ARM_LDR_I(r_scratch, r_skb, off), ctx);\r\nemit(ARM_SUB_R(r_skb_hl, r_skb_hl, r_scratch), ctx);\r\n}\r\nif (ctx->flags & FLAG_NEED_X_RESET)\r\nemit(ARM_MOV_I(r_X, 0), ctx);\r\nif (bpf_needs_clear_a(&ctx->skf->insns[0]))\r\nemit(ARM_MOV_I(r_A, 0), ctx);\r\nif (ctx->seen & SEEN_MEM)\r\nemit(ARM_SUB_I(ARM_SP, ARM_SP, mem_words_used(ctx) * 4), ctx);\r\n}\r\nstatic void build_epilogue(struct jit_ctx *ctx)\r\n{\r\nu16 reg_set = saved_regs(ctx);\r\nif (ctx->seen & SEEN_MEM)\r\nemit(ARM_ADD_I(ARM_SP, ARM_SP, mem_words_used(ctx) * 4), ctx);\r\nreg_set &= ~(1 << ARM_LR);\r\n#ifdef CONFIG_FRAME_POINTER\r\nreg_set &= ~(1 << ARM_IP);\r\nreg_set |= (1 << ARM_SP);\r\nemit(ARM_LDM(ARM_SP, reg_set), ctx);\r\n#else\r\nif (reg_set) {\r\nif (ctx->seen & SEEN_CALL)\r\nreg_set |= 1 << ARM_PC;\r\nemit(ARM_POP(reg_set), ctx);\r\n}\r\nif (!(ctx->seen & SEEN_CALL))\r\nemit(ARM_BX(ARM_LR), ctx);\r\n#endif\r\n}\r\nstatic int16_t imm8m(u32 x)\r\n{\r\nu32 rot;\r\nfor (rot = 0; rot < 16; rot++)\r\nif ((x & ~ror32(0xff, 2 * rot)) == 0)\r\nreturn rol32(x, 2 * rot) | (rot << 8);\r\nreturn -1;\r\n}\r\nstatic u16 imm_offset(u32 k, struct jit_ctx *ctx)\r\n{\r\nunsigned i = 0, offset;\r\nu16 imm;\r\nif (ctx->target == NULL) {\r\nctx->imm_count++;\r\nreturn 0;\r\n}\r\nwhile ((i < ctx->imm_count) && ctx->imms[i]) {\r\nif (ctx->imms[i] == k)\r\nbreak;\r\ni++;\r\n}\r\nif (ctx->imms[i] == 0)\r\nctx->imms[i] = k;\r\noffset = ctx->offsets[ctx->skf->len];\r\noffset += ctx->prologue_bytes;\r\noffset += ctx->epilogue_bytes;\r\noffset += i * 4;\r\nctx->target[offset / 4] = k;\r\nimm = offset - (8 + ctx->idx * 4);\r\nif (imm & ~0xfff) {\r\nctx->flags |= FLAG_IMM_OVERFLOW;\r\nreturn 0;\r\n}\r\nreturn imm;\r\n}\r\nstatic inline void emit_mov_i_no8m(int rd, u32 val, struct jit_ctx *ctx)\r\n{\r\n#if __LINUX_ARM_ARCH__ < 7\r\nemit(ARM_LDR_I(rd, ARM_PC, imm_offset(val, ctx)), ctx);\r\n#else\r\nemit(ARM_MOVW(rd, val & 0xffff), ctx);\r\nif (val > 0xffff)\r\nemit(ARM_MOVT(rd, val >> 16), ctx);\r\n#endif\r\n}\r\nstatic inline void emit_mov_i(int rd, u32 val, struct jit_ctx *ctx)\r\n{\r\nint imm12 = imm8m(val);\r\nif (imm12 >= 0)\r\nemit(ARM_MOV_I(rd, imm12), ctx);\r\nelse\r\nemit_mov_i_no8m(rd, val, ctx);\r\n}\r\nstatic void emit_load_be32(u8 cond, u8 r_res, u8 r_addr, struct jit_ctx *ctx)\r\n{\r\n_emit(cond, ARM_LDRB_I(ARM_R3, r_addr, 1), ctx);\r\n_emit(cond, ARM_LDRB_I(ARM_R1, r_addr, 0), ctx);\r\n_emit(cond, ARM_LDRB_I(ARM_R2, r_addr, 3), ctx);\r\n_emit(cond, ARM_LSL_I(ARM_R3, ARM_R3, 16), ctx);\r\n_emit(cond, ARM_LDRB_I(ARM_R0, r_addr, 2), ctx);\r\n_emit(cond, ARM_ORR_S(ARM_R3, ARM_R3, ARM_R1, SRTYPE_LSL, 24), ctx);\r\n_emit(cond, ARM_ORR_R(ARM_R3, ARM_R3, ARM_R2), ctx);\r\n_emit(cond, ARM_ORR_S(r_res, ARM_R3, ARM_R0, SRTYPE_LSL, 8), ctx);\r\n}\r\nstatic void emit_load_be16(u8 cond, u8 r_res, u8 r_addr, struct jit_ctx *ctx)\r\n{\r\n_emit(cond, ARM_LDRB_I(ARM_R1, r_addr, 0), ctx);\r\n_emit(cond, ARM_LDRB_I(ARM_R2, r_addr, 1), ctx);\r\n_emit(cond, ARM_ORR_S(r_res, ARM_R2, ARM_R1, SRTYPE_LSL, 8), ctx);\r\n}\r\nstatic inline void emit_swap16(u8 r_dst, u8 r_src, struct jit_ctx *ctx)\r\n{\r\nemit(ARM_LSL_I(ARM_R1, r_src, 8), ctx);\r\nemit(ARM_ORR_S(r_dst, ARM_R1, r_src, SRTYPE_LSR, 8), ctx);\r\nemit(ARM_BIC_I(r_dst, r_dst, 0x8ff), ctx);\r\n}\r\nstatic void emit_load_be32(u8 cond, u8 r_res, u8 r_addr, struct jit_ctx *ctx)\r\n{\r\n_emit(cond, ARM_LDR_I(r_res, r_addr, 0), ctx);\r\n#ifdef __LITTLE_ENDIAN\r\n_emit(cond, ARM_REV(r_res, r_res), ctx);\r\n#endif\r\n}\r\nstatic void emit_load_be16(u8 cond, u8 r_res, u8 r_addr, struct jit_ctx *ctx)\r\n{\r\n_emit(cond, ARM_LDRH_I(r_res, r_addr, 0), ctx);\r\n#ifdef __LITTLE_ENDIAN\r\n_emit(cond, ARM_REV16(r_res, r_res), ctx);\r\n#endif\r\n}\r\nstatic inline void emit_swap16(u8 r_dst __maybe_unused,\r\nu8 r_src __maybe_unused,\r\nstruct jit_ctx *ctx __maybe_unused)\r\n{\r\n#ifdef __LITTLE_ENDIAN\r\nemit(ARM_REV16(r_dst, r_src), ctx);\r\n#endif\r\n}\r\nstatic inline u32 b_imm(unsigned tgt, struct jit_ctx *ctx)\r\n{\r\nu32 imm;\r\nif (ctx->target == NULL)\r\nreturn 0;\r\nimm = ctx->offsets[tgt] + ctx->prologue_bytes - (ctx->idx * 4 + 8);\r\nreturn imm >> 2;\r\n}\r\nstatic inline void emit_err_ret(u8 cond, struct jit_ctx *ctx)\r\n{\r\nif (ctx->ret0_fp_idx >= 0) {\r\n_emit(cond, ARM_B(b_imm(ctx->ret0_fp_idx, ctx)), ctx);\r\nemit(ARM_MOV_R(ARM_R0, ARM_R0), ctx);\r\n} else {\r\n_emit(cond, ARM_MOV_I(ARM_R0, 0), ctx);\r\n_emit(cond, ARM_B(b_imm(ctx->skf->len, ctx)), ctx);\r\n}\r\n}\r\nstatic inline void emit_blx_r(u8 tgt_reg, struct jit_ctx *ctx)\r\n{\r\n#if __LINUX_ARM_ARCH__ < 5\r\nemit(ARM_MOV_R(ARM_LR, ARM_PC), ctx);\r\nif (elf_hwcap & HWCAP_THUMB)\r\nemit(ARM_BX(tgt_reg), ctx);\r\nelse\r\nemit(ARM_MOV_R(ARM_PC, tgt_reg), ctx);\r\n#else\r\nemit(ARM_BLX_R(tgt_reg), ctx);\r\n#endif\r\n}\r\nstatic inline void emit_udivmod(u8 rd, u8 rm, u8 rn, struct jit_ctx *ctx,\r\nint bpf_op)\r\n{\r\n#if __LINUX_ARM_ARCH__ == 7\r\nif (elf_hwcap & HWCAP_IDIVA) {\r\nif (bpf_op == BPF_DIV)\r\nemit(ARM_UDIV(rd, rm, rn), ctx);\r\nelse {\r\nemit(ARM_UDIV(ARM_R3, rm, rn), ctx);\r\nemit(ARM_MLS(rd, rn, ARM_R3, rm), ctx);\r\n}\r\nreturn;\r\n}\r\n#endif\r\nif (rn != ARM_R1)\r\nemit(ARM_MOV_R(ARM_R1, rn), ctx);\r\nif (rm != ARM_R0)\r\nemit(ARM_MOV_R(ARM_R0, rm), ctx);\r\nctx->seen |= SEEN_CALL;\r\nemit_mov_i(ARM_R3, bpf_op == BPF_DIV ? (u32)jit_udiv : (u32)jit_mod,\r\nctx);\r\nemit_blx_r(ARM_R3, ctx);\r\nif (rd != ARM_R0)\r\nemit(ARM_MOV_R(rd, ARM_R0), ctx);\r\n}\r\nstatic inline void update_on_xread(struct jit_ctx *ctx)\r\n{\r\nif (!(ctx->seen & SEEN_X))\r\nctx->flags |= FLAG_NEED_X_RESET;\r\nctx->seen |= SEEN_X;\r\n}\r\nstatic int build_body(struct jit_ctx *ctx)\r\n{\r\nvoid *load_func[] = {jit_get_skb_b, jit_get_skb_h, jit_get_skb_w};\r\nconst struct bpf_prog *prog = ctx->skf;\r\nconst struct sock_filter *inst;\r\nunsigned i, load_order, off, condt;\r\nint imm12;\r\nu32 k;\r\nfor (i = 0; i < prog->len; i++) {\r\nu16 code;\r\ninst = &(prog->insns[i]);\r\nk = inst->k;\r\ncode = bpf_anc_helper(inst);\r\nif (ctx->target == NULL)\r\nctx->offsets[i] = ctx->idx * 4;\r\nswitch (code) {\r\ncase BPF_LD | BPF_IMM:\r\nemit_mov_i(r_A, k, ctx);\r\nbreak;\r\ncase BPF_LD | BPF_W | BPF_LEN:\r\nctx->seen |= SEEN_SKB;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);\r\nemit(ARM_LDR_I(r_A, r_skb,\r\noffsetof(struct sk_buff, len)), ctx);\r\nbreak;\r\ncase BPF_LD | BPF_MEM:\r\nctx->seen |= SEEN_MEM_WORD(k);\r\nemit(ARM_LDR_I(r_A, ARM_SP, SCRATCH_OFF(k)), ctx);\r\nbreak;\r\ncase BPF_LD | BPF_W | BPF_ABS:\r\nload_order = 2;\r\ngoto load;\r\ncase BPF_LD | BPF_H | BPF_ABS:\r\nload_order = 1;\r\ngoto load;\r\ncase BPF_LD | BPF_B | BPF_ABS:\r\nload_order = 0;\r\nload:\r\nemit_mov_i(r_off, k, ctx);\r\nload_common:\r\nctx->seen |= SEEN_DATA | SEEN_CALL;\r\nif (load_order > 0) {\r\nemit(ARM_SUB_I(r_scratch, r_skb_hl,\r\n1 << load_order), ctx);\r\nemit(ARM_CMP_R(r_scratch, r_off), ctx);\r\ncondt = ARM_COND_GE;\r\n} else {\r\nemit(ARM_CMP_R(r_skb_hl, r_off), ctx);\r\ncondt = ARM_COND_HI;\r\n}\r\n_emit(condt, ARM_CMP_I(r_off, 0), ctx);\r\n_emit(condt, ARM_ADD_R(r_scratch, r_off, r_skb_data),\r\nctx);\r\nif (load_order == 0)\r\n_emit(condt, ARM_LDRB_I(r_A, r_scratch, 0),\r\nctx);\r\nelse if (load_order == 1)\r\nemit_load_be16(condt, r_A, r_scratch, ctx);\r\nelse if (load_order == 2)\r\nemit_load_be32(condt, r_A, r_scratch, ctx);\r\n_emit(condt, ARM_B(b_imm(i + 1, ctx)), ctx);\r\nemit_mov_i(ARM_R3, (u32)load_func[load_order], ctx);\r\nemit(ARM_MOV_R(ARM_R0, r_skb), ctx);\r\nemit_blx_r(ARM_R3, ctx);\r\nemit(ARM_CMP_I(ARM_R1, 0), ctx);\r\nemit_err_ret(ARM_COND_NE, ctx);\r\nemit(ARM_MOV_R(r_A, ARM_R0), ctx);\r\nbreak;\r\ncase BPF_LD | BPF_W | BPF_IND:\r\nload_order = 2;\r\ngoto load_ind;\r\ncase BPF_LD | BPF_H | BPF_IND:\r\nload_order = 1;\r\ngoto load_ind;\r\ncase BPF_LD | BPF_B | BPF_IND:\r\nload_order = 0;\r\nload_ind:\r\nupdate_on_xread(ctx);\r\nOP_IMM3(ARM_ADD, r_off, r_X, k, ctx);\r\ngoto load_common;\r\ncase BPF_LDX | BPF_IMM:\r\nctx->seen |= SEEN_X;\r\nemit_mov_i(r_X, k, ctx);\r\nbreak;\r\ncase BPF_LDX | BPF_W | BPF_LEN:\r\nctx->seen |= SEEN_X | SEEN_SKB;\r\nemit(ARM_LDR_I(r_X, r_skb,\r\noffsetof(struct sk_buff, len)), ctx);\r\nbreak;\r\ncase BPF_LDX | BPF_MEM:\r\nctx->seen |= SEEN_X | SEEN_MEM_WORD(k);\r\nemit(ARM_LDR_I(r_X, ARM_SP, SCRATCH_OFF(k)), ctx);\r\nbreak;\r\ncase BPF_LDX | BPF_B | BPF_MSH:\r\nctx->seen |= SEEN_X | SEEN_DATA | SEEN_CALL;\r\nif ((int)k < 0)\r\nreturn -1;\r\nemit_mov_i(r_off, k, ctx);\r\nemit(ARM_CMP_R(r_skb_hl, r_off), ctx);\r\n_emit(ARM_COND_HI, ARM_LDRB_R(ARM_R0, r_skb_data,\r\nARM_R1), ctx);\r\n_emit(ARM_COND_HI, ARM_B(b_imm(i + 1, ctx) - 2), ctx);\r\nemit(ARM_MOV_R(ARM_R0, r_skb), ctx);\r\nemit_mov_i(ARM_R3, (u32)jit_get_skb_b, ctx);\r\nemit_blx_r(ARM_R3, ctx);\r\nemit(ARM_CMP_I(ARM_R1, 0), ctx);\r\nemit_err_ret(ARM_COND_NE, ctx);\r\nemit(ARM_AND_I(r_X, ARM_R0, 0x00f), ctx);\r\nemit(ARM_LSL_I(r_X, r_X, 2), ctx);\r\nbreak;\r\ncase BPF_ST:\r\nctx->seen |= SEEN_MEM_WORD(k);\r\nemit(ARM_STR_I(r_A, ARM_SP, SCRATCH_OFF(k)), ctx);\r\nbreak;\r\ncase BPF_STX:\r\nupdate_on_xread(ctx);\r\nctx->seen |= SEEN_MEM_WORD(k);\r\nemit(ARM_STR_I(r_X, ARM_SP, SCRATCH_OFF(k)), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_ADD | BPF_K:\r\nOP_IMM3(ARM_ADD, r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_ADD | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_ADD_R(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_SUB | BPF_K:\r\nOP_IMM3(ARM_SUB, r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_SUB | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_SUB_R(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_MUL | BPF_K:\r\nemit_mov_i(r_scratch, k, ctx);\r\nemit(ARM_MUL(r_A, r_A, r_scratch), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_MUL | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_MUL(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_DIV | BPF_K:\r\nif (k == 1)\r\nbreak;\r\nemit_mov_i(r_scratch, k, ctx);\r\nemit_udivmod(r_A, r_A, r_scratch, ctx, BPF_DIV);\r\nbreak;\r\ncase BPF_ALU | BPF_DIV | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_CMP_I(r_X, 0), ctx);\r\nemit_err_ret(ARM_COND_EQ, ctx);\r\nemit_udivmod(r_A, r_A, r_X, ctx, BPF_DIV);\r\nbreak;\r\ncase BPF_ALU | BPF_MOD | BPF_K:\r\nif (k == 1) {\r\nemit_mov_i(r_A, 0, ctx);\r\nbreak;\r\n}\r\nemit_mov_i(r_scratch, k, ctx);\r\nemit_udivmod(r_A, r_A, r_scratch, ctx, BPF_MOD);\r\nbreak;\r\ncase BPF_ALU | BPF_MOD | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_CMP_I(r_X, 0), ctx);\r\nemit_err_ret(ARM_COND_EQ, ctx);\r\nemit_udivmod(r_A, r_A, r_X, ctx, BPF_MOD);\r\nbreak;\r\ncase BPF_ALU | BPF_OR | BPF_K:\r\nOP_IMM3(ARM_ORR, r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_OR | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_ORR_R(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_XOR | BPF_K:\r\nOP_IMM3(ARM_EOR, r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_ALU_XOR_X:\r\ncase BPF_ALU | BPF_XOR | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_EOR_R(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_AND | BPF_K:\r\nOP_IMM3(ARM_AND, r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_AND | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_AND_R(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_LSH | BPF_K:\r\nif (unlikely(k > 31))\r\nreturn -1;\r\nemit(ARM_LSL_I(r_A, r_A, k), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_LSH | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_LSL_R(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_RSH | BPF_K:\r\nif (unlikely(k > 31))\r\nreturn -1;\r\nif (k)\r\nemit(ARM_LSR_I(r_A, r_A, k), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_RSH | BPF_X:\r\nupdate_on_xread(ctx);\r\nemit(ARM_LSR_R(r_A, r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_NEG:\r\nemit(ARM_RSB_I(r_A, r_A, 0), ctx);\r\nbreak;\r\ncase BPF_JMP | BPF_JA:\r\nemit(ARM_B(b_imm(i + k + 1, ctx)), ctx);\r\nbreak;\r\ncase BPF_JMP | BPF_JEQ | BPF_K:\r\ncondt = ARM_COND_EQ;\r\ngoto cmp_imm;\r\ncase BPF_JMP | BPF_JGT | BPF_K:\r\ncondt = ARM_COND_HI;\r\ngoto cmp_imm;\r\ncase BPF_JMP | BPF_JGE | BPF_K:\r\ncondt = ARM_COND_HS;\r\ncmp_imm:\r\nimm12 = imm8m(k);\r\nif (imm12 < 0) {\r\nemit_mov_i_no8m(r_scratch, k, ctx);\r\nemit(ARM_CMP_R(r_A, r_scratch), ctx);\r\n} else {\r\nemit(ARM_CMP_I(r_A, imm12), ctx);\r\n}\r\ncond_jump:\r\nif (inst->jt)\r\n_emit(condt, ARM_B(b_imm(i + inst->jt + 1,\r\nctx)), ctx);\r\nif (inst->jf)\r\n_emit(condt ^ 1, ARM_B(b_imm(i + inst->jf + 1,\r\nctx)), ctx);\r\nbreak;\r\ncase BPF_JMP | BPF_JEQ | BPF_X:\r\ncondt = ARM_COND_EQ;\r\ngoto cmp_x;\r\ncase BPF_JMP | BPF_JGT | BPF_X:\r\ncondt = ARM_COND_HI;\r\ngoto cmp_x;\r\ncase BPF_JMP | BPF_JGE | BPF_X:\r\ncondt = ARM_COND_CS;\r\ncmp_x:\r\nupdate_on_xread(ctx);\r\nemit(ARM_CMP_R(r_A, r_X), ctx);\r\ngoto cond_jump;\r\ncase BPF_JMP | BPF_JSET | BPF_K:\r\ncondt = ARM_COND_NE;\r\nimm12 = imm8m(k);\r\nif (imm12 < 0) {\r\nemit_mov_i_no8m(r_scratch, k, ctx);\r\nemit(ARM_TST_R(r_A, r_scratch), ctx);\r\n} else {\r\nemit(ARM_TST_I(r_A, imm12), ctx);\r\n}\r\ngoto cond_jump;\r\ncase BPF_JMP | BPF_JSET | BPF_X:\r\nupdate_on_xread(ctx);\r\ncondt = ARM_COND_NE;\r\nemit(ARM_TST_R(r_A, r_X), ctx);\r\ngoto cond_jump;\r\ncase BPF_RET | BPF_A:\r\nemit(ARM_MOV_R(ARM_R0, r_A), ctx);\r\ngoto b_epilogue;\r\ncase BPF_RET | BPF_K:\r\nif ((k == 0) && (ctx->ret0_fp_idx < 0))\r\nctx->ret0_fp_idx = i;\r\nemit_mov_i(ARM_R0, k, ctx);\r\nb_epilogue:\r\nif (i != ctx->skf->len - 1)\r\nemit(ARM_B(b_imm(prog->len, ctx)), ctx);\r\nbreak;\r\ncase BPF_MISC | BPF_TAX:\r\nctx->seen |= SEEN_X;\r\nemit(ARM_MOV_R(r_X, r_A), ctx);\r\nbreak;\r\ncase BPF_MISC | BPF_TXA:\r\nupdate_on_xread(ctx);\r\nemit(ARM_MOV_R(r_A, r_X), ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_PROTOCOL:\r\nctx->seen |= SEEN_SKB;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,\r\nprotocol) != 2);\r\noff = offsetof(struct sk_buff, protocol);\r\nemit(ARM_LDRH_I(r_scratch, r_skb, off), ctx);\r\nemit_swap16(r_A, r_scratch, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_CPU:\r\nOP_IMM3(ARM_BIC, r_scratch, ARM_SP, THREAD_SIZE - 1, ctx);\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct thread_info, cpu) != 4);\r\noff = offsetof(struct thread_info, cpu);\r\nemit(ARM_LDR_I(r_A, r_scratch, off), ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_IFINDEX:\r\ncase BPF_ANC | SKF_AD_HATYPE:\r\nctx->seen |= SEEN_SKB;\r\noff = offsetof(struct sk_buff, dev);\r\nemit(ARM_LDR_I(r_scratch, r_skb, off), ctx);\r\nemit(ARM_CMP_I(r_scratch, 0), ctx);\r\nemit_err_ret(ARM_COND_EQ, ctx);\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct net_device,\r\nifindex) != 4);\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct net_device,\r\ntype) != 2);\r\nif (code == (BPF_ANC | SKF_AD_IFINDEX)) {\r\noff = offsetof(struct net_device, ifindex);\r\nemit(ARM_LDR_I(r_A, r_scratch, off), ctx);\r\n} else {\r\noff = offsetof(struct net_device, type);\r\nemit_mov_i(ARM_R3, off, ctx);\r\nemit(ARM_LDRH_R(r_A, r_scratch, ARM_R3), ctx);\r\n}\r\nbreak;\r\ncase BPF_ANC | SKF_AD_MARK:\r\nctx->seen |= SEEN_SKB;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);\r\noff = offsetof(struct sk_buff, mark);\r\nemit(ARM_LDR_I(r_A, r_skb, off), ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_RXHASH:\r\nctx->seen |= SEEN_SKB;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);\r\noff = offsetof(struct sk_buff, hash);\r\nemit(ARM_LDR_I(r_A, r_skb, off), ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_VLAN_TAG:\r\ncase BPF_ANC | SKF_AD_VLAN_TAG_PRESENT:\r\nctx->seen |= SEEN_SKB;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);\r\noff = offsetof(struct sk_buff, vlan_tci);\r\nemit(ARM_LDRH_I(r_A, r_skb, off), ctx);\r\nif (code == (BPF_ANC | SKF_AD_VLAN_TAG))\r\nOP_IMM3(ARM_AND, r_A, r_A, ~VLAN_TAG_PRESENT, ctx);\r\nelse {\r\nOP_IMM3(ARM_LSR, r_A, r_A, 12, ctx);\r\nOP_IMM3(ARM_AND, r_A, r_A, 0x1, ctx);\r\n}\r\nbreak;\r\ncase BPF_ANC | SKF_AD_PKTTYPE:\r\nctx->seen |= SEEN_SKB;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,\r\n__pkt_type_offset[0]) != 1);\r\noff = PKT_TYPE_OFFSET();\r\nemit(ARM_LDRB_I(r_A, r_skb, off), ctx);\r\nemit(ARM_AND_I(r_A, r_A, PKT_TYPE_MAX), ctx);\r\n#ifdef __BIG_ENDIAN_BITFIELD\r\nemit(ARM_LSR_I(r_A, r_A, 5), ctx);\r\n#endif\r\nbreak;\r\ncase BPF_ANC | SKF_AD_QUEUE:\r\nctx->seen |= SEEN_SKB;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,\r\nqueue_mapping) != 2);\r\nBUILD_BUG_ON(offsetof(struct sk_buff,\r\nqueue_mapping) > 0xff);\r\noff = offsetof(struct sk_buff, queue_mapping);\r\nemit(ARM_LDRH_I(r_A, r_skb, off), ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_PAY_OFFSET:\r\nctx->seen |= SEEN_SKB | SEEN_CALL;\r\nemit(ARM_MOV_R(ARM_R0, r_skb), ctx);\r\nemit_mov_i(ARM_R3, (unsigned int)skb_get_poff, ctx);\r\nemit_blx_r(ARM_R3, ctx);\r\nemit(ARM_MOV_R(r_A, ARM_R0), ctx);\r\nbreak;\r\ncase BPF_LDX | BPF_W | BPF_ABS:\r\nctx->seen |= SEEN_SKB;\r\nemit(ARM_LDR_I(r_A, r_skb, k), ctx);\r\nbreak;\r\ndefault:\r\nreturn -1;\r\n}\r\nif (ctx->flags & FLAG_IMM_OVERFLOW)\r\nreturn -1;\r\n}\r\nif (ctx->target == NULL)\r\nctx->offsets[i] = ctx->idx * 4;\r\nreturn 0;\r\n}\r\nvoid bpf_jit_compile(struct bpf_prog *fp)\r\n{\r\nstruct bpf_binary_header *header;\r\nstruct jit_ctx ctx;\r\nunsigned tmp_idx;\r\nunsigned alloc_size;\r\nu8 *target_ptr;\r\nif (!bpf_jit_enable)\r\nreturn;\r\nmemset(&ctx, 0, sizeof(ctx));\r\nctx.skf = fp;\r\nctx.ret0_fp_idx = -1;\r\nctx.offsets = kzalloc(4 * (ctx.skf->len + 1), GFP_KERNEL);\r\nif (ctx.offsets == NULL)\r\nreturn;\r\nif (unlikely(build_body(&ctx)))\r\ngoto out;\r\ntmp_idx = ctx.idx;\r\nbuild_prologue(&ctx);\r\nctx.prologue_bytes = (ctx.idx - tmp_idx) * 4;\r\n#if __LINUX_ARM_ARCH__ < 7\r\ntmp_idx = ctx.idx;\r\nbuild_epilogue(&ctx);\r\nctx.epilogue_bytes = (ctx.idx - tmp_idx) * 4;\r\nctx.idx += ctx.imm_count;\r\nif (ctx.imm_count) {\r\nctx.imms = kzalloc(4 * ctx.imm_count, GFP_KERNEL);\r\nif (ctx.imms == NULL)\r\ngoto out;\r\n}\r\n#else\r\nbuild_epilogue(&ctx);\r\n#endif\r\nalloc_size = 4 * ctx.idx;\r\nheader = bpf_jit_binary_alloc(alloc_size, &target_ptr,\r\n4, jit_fill_hole);\r\nif (header == NULL)\r\ngoto out;\r\nctx.target = (u32 *) target_ptr;\r\nctx.idx = 0;\r\nbuild_prologue(&ctx);\r\nif (build_body(&ctx) < 0) {\r\n#if __LINUX_ARM_ARCH__ < 7\r\nif (ctx.imm_count)\r\nkfree(ctx.imms);\r\n#endif\r\nbpf_jit_binary_free(header);\r\ngoto out;\r\n}\r\nbuild_epilogue(&ctx);\r\nflush_icache_range((u32)header, (u32)(ctx.target + ctx.idx));\r\n#if __LINUX_ARM_ARCH__ < 7\r\nif (ctx.imm_count)\r\nkfree(ctx.imms);\r\n#endif\r\nif (bpf_jit_enable > 1)\r\nbpf_jit_dump(fp->len, alloc_size, 2, ctx.target);\r\nset_memory_ro((unsigned long)header, header->pages);\r\nfp->bpf_func = (void *)ctx.target;\r\nfp->jited = 1;\r\nout:\r\nkfree(ctx.offsets);\r\nreturn;\r\n}\r\nvoid bpf_jit_free(struct bpf_prog *fp)\r\n{\r\nunsigned long addr = (unsigned long)fp->bpf_func & PAGE_MASK;\r\nstruct bpf_binary_header *header = (void *)addr;\r\nif (!fp->jited)\r\ngoto free_filter;\r\nset_memory_rw(addr, header->pages);\r\nbpf_jit_binary_free(header);\r\nfree_filter:\r\nbpf_prog_unlock_free(fp);\r\n}
