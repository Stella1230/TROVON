static void qat_rsa_cb(struct icp_qat_fw_pke_resp *resp)\r\n{\r\nstruct akcipher_request *areq = (void *)(__force long)resp->opaque;\r\nstruct qat_rsa_request *req = PTR_ALIGN(akcipher_request_ctx(areq), 64);\r\nstruct device *dev = &GET_DEV(req->ctx->inst->accel_dev);\r\nint err = ICP_QAT_FW_PKE_RESP_PKE_STAT_GET(\r\nresp->pke_resp_hdr.comn_resp_flags);\r\nerr = (err == ICP_QAT_FW_COMN_STATUS_FLAG_OK) ? 0 : -EINVAL;\r\nif (req->src_align)\r\ndma_free_coherent(dev, req->ctx->key_sz, req->src_align,\r\nreq->in.enc.m);\r\nelse\r\ndma_unmap_single(dev, req->in.enc.m, req->ctx->key_sz,\r\nDMA_TO_DEVICE);\r\nareq->dst_len = req->ctx->key_sz;\r\nif (req->dst_align) {\r\nchar *ptr = req->dst_align;\r\nwhile (!(*ptr) && areq->dst_len) {\r\nareq->dst_len--;\r\nptr++;\r\n}\r\nif (areq->dst_len != req->ctx->key_sz)\r\nmemmove(req->dst_align, ptr, areq->dst_len);\r\nscatterwalk_map_and_copy(req->dst_align, areq->dst, 0,\r\nareq->dst_len, 1);\r\ndma_free_coherent(dev, req->ctx->key_sz, req->dst_align,\r\nreq->out.enc.c);\r\n} else {\r\nchar *ptr = sg_virt(areq->dst);\r\nwhile (!(*ptr) && areq->dst_len) {\r\nareq->dst_len--;\r\nptr++;\r\n}\r\nif (sg_virt(areq->dst) != ptr && areq->dst_len)\r\nmemmove(sg_virt(areq->dst), ptr, areq->dst_len);\r\ndma_unmap_single(dev, req->out.enc.c, req->ctx->key_sz,\r\nDMA_FROM_DEVICE);\r\n}\r\ndma_unmap_single(dev, req->phy_in, sizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\ndma_unmap_single(dev, req->phy_out,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nakcipher_request_complete(areq, err);\r\n}\r\nvoid qat_alg_asym_callback(void *_resp)\r\n{\r\nstruct icp_qat_fw_pke_resp *resp = _resp;\r\nqat_rsa_cb(resp);\r\n}\r\nstatic unsigned long qat_rsa_enc_fn_id(unsigned int len)\r\n{\r\nunsigned int bitslen = len << 3;\r\nswitch (bitslen) {\r\ncase 512:\r\nreturn PKE_RSA_EP_512;\r\ncase 1024:\r\nreturn PKE_RSA_EP_1024;\r\ncase 1536:\r\nreturn PKE_RSA_EP_1536;\r\ncase 2048:\r\nreturn PKE_RSA_EP_2048;\r\ncase 3072:\r\nreturn PKE_RSA_EP_3072;\r\ncase 4096:\r\nreturn PKE_RSA_EP_4096;\r\ndefault:\r\nreturn 0;\r\n};\r\n}\r\nstatic unsigned long qat_rsa_dec_fn_id(unsigned int len)\r\n{\r\nunsigned int bitslen = len << 3;\r\nswitch (bitslen) {\r\ncase 512:\r\nreturn PKE_RSA_DP1_512;\r\ncase 1024:\r\nreturn PKE_RSA_DP1_1024;\r\ncase 1536:\r\nreturn PKE_RSA_DP1_1536;\r\ncase 2048:\r\nreturn PKE_RSA_DP1_2048;\r\ncase 3072:\r\nreturn PKE_RSA_DP1_3072;\r\ncase 4096:\r\nreturn PKE_RSA_DP1_4096;\r\ndefault:\r\nreturn 0;\r\n};\r\n}\r\nstatic int qat_rsa_enc(struct akcipher_request *req)\r\n{\r\nstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nstruct qat_rsa_request *qat_req =\r\nPTR_ALIGN(akcipher_request_ctx(req), 64);\r\nstruct icp_qat_fw_pke_request *msg = &qat_req->req;\r\nint ret, ctr = 0;\r\nif (unlikely(!ctx->n || !ctx->e))\r\nreturn -EINVAL;\r\nif (req->dst_len < ctx->key_sz) {\r\nreq->dst_len = ctx->key_sz;\r\nreturn -EOVERFLOW;\r\n}\r\nmemset(msg, '\0', sizeof(*msg));\r\nICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\r\nICP_QAT_FW_COMN_REQ_FLAG_SET);\r\nmsg->pke_hdr.cd_pars.func_id = qat_rsa_enc_fn_id(ctx->key_sz);\r\nif (unlikely(!msg->pke_hdr.cd_pars.func_id))\r\nreturn -EINVAL;\r\nqat_req->ctx = ctx;\r\nmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\r\nmsg->pke_hdr.comn_req_flags =\r\nICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\r\nQAT_COMN_CD_FLD_TYPE_64BIT_ADR);\r\nqat_req->in.enc.e = ctx->dma_e;\r\nqat_req->in.enc.n = ctx->dma_n;\r\nret = -ENOMEM;\r\nif (sg_is_last(req->src) && req->src_len == ctx->key_sz) {\r\nqat_req->src_align = NULL;\r\nqat_req->in.enc.m = dma_map_single(dev, sg_virt(req->src),\r\nreq->src_len, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->in.enc.m)))\r\nreturn ret;\r\n} else {\r\nint shift = ctx->key_sz - req->src_len;\r\nqat_req->src_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->in.enc.m,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->src_align))\r\nreturn ret;\r\nscatterwalk_map_and_copy(qat_req->src_align + shift, req->src,\r\n0, req->src_len, 0);\r\n}\r\nif (sg_is_last(req->dst) && req->dst_len == ctx->key_sz) {\r\nqat_req->dst_align = NULL;\r\nqat_req->out.enc.c = dma_map_single(dev, sg_virt(req->dst),\r\nreq->dst_len,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->out.enc.c)))\r\ngoto unmap_src;\r\n} else {\r\nqat_req->dst_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->out.enc.c,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->dst_align))\r\ngoto unmap_src;\r\n}\r\nqat_req->in.in_tab[3] = 0;\r\nqat_req->out.out_tab[1] = 0;\r\nqat_req->phy_in = dma_map_single(dev, &qat_req->in.enc.m,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\r\ngoto unmap_dst;\r\nqat_req->phy_out = dma_map_single(dev, &qat_req->out.enc.c,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\r\ngoto unmap_in_params;\r\nmsg->pke_mid.src_data_addr = qat_req->phy_in;\r\nmsg->pke_mid.dest_data_addr = qat_req->phy_out;\r\nmsg->pke_mid.opaque = (uint64_t)(__force long)req;\r\nmsg->input_param_count = 3;\r\nmsg->output_param_count = 1;\r\ndo {\r\nret = adf_send_message(ctx->inst->pke_tx, (uint32_t *)msg);\r\n} while (ret == -EBUSY && ctr++ < 100);\r\nif (!ret)\r\nreturn -EINPROGRESS;\r\nunmap_src:\r\nif (qat_req->src_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->src_align,\r\nqat_req->in.enc.m);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->in.enc.m))\r\ndma_unmap_single(dev, qat_req->in.enc.m, ctx->key_sz,\r\nDMA_TO_DEVICE);\r\nunmap_dst:\r\nif (qat_req->dst_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->dst_align,\r\nqat_req->out.enc.c);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->out.enc.c))\r\ndma_unmap_single(dev, qat_req->out.enc.c, ctx->key_sz,\r\nDMA_FROM_DEVICE);\r\nunmap_in_params:\r\nif (!dma_mapping_error(dev, qat_req->phy_in))\r\ndma_unmap_single(dev, qat_req->phy_in,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nif (!dma_mapping_error(dev, qat_req->phy_out))\r\ndma_unmap_single(dev, qat_req->phy_out,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nreturn ret;\r\n}\r\nstatic int qat_rsa_dec(struct akcipher_request *req)\r\n{\r\nstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nstruct qat_rsa_request *qat_req =\r\nPTR_ALIGN(akcipher_request_ctx(req), 64);\r\nstruct icp_qat_fw_pke_request *msg = &qat_req->req;\r\nint ret, ctr = 0;\r\nif (unlikely(!ctx->n || !ctx->d))\r\nreturn -EINVAL;\r\nif (req->dst_len < ctx->key_sz) {\r\nreq->dst_len = ctx->key_sz;\r\nreturn -EOVERFLOW;\r\n}\r\nmemset(msg, '\0', sizeof(*msg));\r\nICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\r\nICP_QAT_FW_COMN_REQ_FLAG_SET);\r\nmsg->pke_hdr.cd_pars.func_id = qat_rsa_dec_fn_id(ctx->key_sz);\r\nif (unlikely(!msg->pke_hdr.cd_pars.func_id))\r\nreturn -EINVAL;\r\nqat_req->ctx = ctx;\r\nmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\r\nmsg->pke_hdr.comn_req_flags =\r\nICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\r\nQAT_COMN_CD_FLD_TYPE_64BIT_ADR);\r\nqat_req->in.dec.d = ctx->dma_d;\r\nqat_req->in.dec.n = ctx->dma_n;\r\nret = -ENOMEM;\r\nif (sg_is_last(req->src) && req->src_len == ctx->key_sz) {\r\nqat_req->src_align = NULL;\r\nqat_req->in.dec.c = dma_map_single(dev, sg_virt(req->src),\r\nreq->dst_len, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->in.dec.c)))\r\nreturn ret;\r\n} else {\r\nint shift = ctx->key_sz - req->src_len;\r\nqat_req->src_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->in.dec.c,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->src_align))\r\nreturn ret;\r\nscatterwalk_map_and_copy(qat_req->src_align + shift, req->src,\r\n0, req->src_len, 0);\r\n}\r\nif (sg_is_last(req->dst) && req->dst_len == ctx->key_sz) {\r\nqat_req->dst_align = NULL;\r\nqat_req->out.dec.m = dma_map_single(dev, sg_virt(req->dst),\r\nreq->dst_len,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->out.dec.m)))\r\ngoto unmap_src;\r\n} else {\r\nqat_req->dst_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->out.dec.m,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->dst_align))\r\ngoto unmap_src;\r\n}\r\nqat_req->in.in_tab[3] = 0;\r\nqat_req->out.out_tab[1] = 0;\r\nqat_req->phy_in = dma_map_single(dev, &qat_req->in.dec.c,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\r\ngoto unmap_dst;\r\nqat_req->phy_out = dma_map_single(dev, &qat_req->out.dec.m,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\r\ngoto unmap_in_params;\r\nmsg->pke_mid.src_data_addr = qat_req->phy_in;\r\nmsg->pke_mid.dest_data_addr = qat_req->phy_out;\r\nmsg->pke_mid.opaque = (uint64_t)(__force long)req;\r\nmsg->input_param_count = 3;\r\nmsg->output_param_count = 1;\r\ndo {\r\nret = adf_send_message(ctx->inst->pke_tx, (uint32_t *)msg);\r\n} while (ret == -EBUSY && ctr++ < 100);\r\nif (!ret)\r\nreturn -EINPROGRESS;\r\nunmap_src:\r\nif (qat_req->src_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->src_align,\r\nqat_req->in.dec.c);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->in.dec.c))\r\ndma_unmap_single(dev, qat_req->in.dec.c, ctx->key_sz,\r\nDMA_TO_DEVICE);\r\nunmap_dst:\r\nif (qat_req->dst_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->dst_align,\r\nqat_req->out.dec.m);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->out.dec.m))\r\ndma_unmap_single(dev, qat_req->out.dec.m, ctx->key_sz,\r\nDMA_FROM_DEVICE);\r\nunmap_in_params:\r\nif (!dma_mapping_error(dev, qat_req->phy_in))\r\ndma_unmap_single(dev, qat_req->phy_in,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nif (!dma_mapping_error(dev, qat_req->phy_out))\r\ndma_unmap_single(dev, qat_req->phy_out,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nreturn ret;\r\n}\r\nint qat_rsa_get_n(void *context, size_t hdrlen, unsigned char tag,\r\nconst void *value, size_t vlen)\r\n{\r\nstruct qat_rsa_ctx *ctx = context;\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nconst char *ptr = value;\r\nint ret;\r\nwhile (!*ptr && vlen) {\r\nptr++;\r\nvlen--;\r\n}\r\nctx->key_sz = vlen;\r\nret = -EINVAL;\r\nif (fips_enabled && (ctx->key_sz != 256 && ctx->key_sz != 384)) {\r\npr_err("QAT: RSA: key size not allowed in FIPS mode\n");\r\ngoto err;\r\n}\r\nif (!qat_rsa_enc_fn_id(ctx->key_sz))\r\ngoto err;\r\nret = -ENOMEM;\r\nctx->n = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_n, GFP_KERNEL);\r\nif (!ctx->n)\r\ngoto err;\r\nmemcpy(ctx->n, ptr, ctx->key_sz);\r\nreturn 0;\r\nerr:\r\nctx->key_sz = 0;\r\nctx->n = NULL;\r\nreturn ret;\r\n}\r\nint qat_rsa_get_e(void *context, size_t hdrlen, unsigned char tag,\r\nconst void *value, size_t vlen)\r\n{\r\nstruct qat_rsa_ctx *ctx = context;\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nconst char *ptr = value;\r\nwhile (!*ptr && vlen) {\r\nptr++;\r\nvlen--;\r\n}\r\nif (!ctx->key_sz || !vlen || vlen > ctx->key_sz) {\r\nctx->e = NULL;\r\nreturn -EINVAL;\r\n}\r\nctx->e = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_e, GFP_KERNEL);\r\nif (!ctx->e) {\r\nctx->e = NULL;\r\nreturn -ENOMEM;\r\n}\r\nmemcpy(ctx->e + (ctx->key_sz - vlen), ptr, vlen);\r\nreturn 0;\r\n}\r\nint qat_rsa_get_d(void *context, size_t hdrlen, unsigned char tag,\r\nconst void *value, size_t vlen)\r\n{\r\nstruct qat_rsa_ctx *ctx = context;\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nconst char *ptr = value;\r\nint ret;\r\nwhile (!*ptr && vlen) {\r\nptr++;\r\nvlen--;\r\n}\r\nret = -EINVAL;\r\nif (!ctx->key_sz || !vlen || vlen > ctx->key_sz)\r\ngoto err;\r\nif (fips_enabled && (vlen != 256 && vlen != 384)) {\r\npr_err("QAT: RSA: key size not allowed in FIPS mode\n");\r\ngoto err;\r\n}\r\nret = -ENOMEM;\r\nctx->d = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_d, GFP_KERNEL);\r\nif (!ctx->n)\r\ngoto err;\r\nmemcpy(ctx->d + (ctx->key_sz - vlen), ptr, vlen);\r\nreturn 0;\r\nerr:\r\nctx->d = NULL;\r\nreturn ret;\r\n}\r\nstatic int qat_rsa_setkey(struct crypto_akcipher *tfm, const void *key,\r\nunsigned int keylen, bool private)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\r\nint ret;\r\nif (ctx->n)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->n, ctx->dma_n);\r\nif (ctx->e)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->e, ctx->dma_e);\r\nif (ctx->d) {\r\nmemset(ctx->d, '\0', ctx->key_sz);\r\ndma_free_coherent(dev, ctx->key_sz, ctx->d, ctx->dma_d);\r\n}\r\nctx->n = NULL;\r\nctx->e = NULL;\r\nctx->d = NULL;\r\nif (private)\r\nret = asn1_ber_decoder(&qat_rsaprivkey_decoder, ctx, key,\r\nkeylen);\r\nelse\r\nret = asn1_ber_decoder(&qat_rsapubkey_decoder, ctx, key,\r\nkeylen);\r\nif (ret < 0)\r\ngoto free;\r\nif (!ctx->n || !ctx->e) {\r\nret = -EINVAL;\r\ngoto free;\r\n}\r\nif (private && !ctx->d) {\r\nret = -EINVAL;\r\ngoto free;\r\n}\r\nreturn 0;\r\nfree:\r\nif (ctx->d) {\r\nmemset(ctx->d, '\0', ctx->key_sz);\r\ndma_free_coherent(dev, ctx->key_sz, ctx->d, ctx->dma_d);\r\nctx->d = NULL;\r\n}\r\nif (ctx->e) {\r\ndma_free_coherent(dev, ctx->key_sz, ctx->e, ctx->dma_e);\r\nctx->e = NULL;\r\n}\r\nif (ctx->n) {\r\ndma_free_coherent(dev, ctx->key_sz, ctx->n, ctx->dma_n);\r\nctx->n = NULL;\r\nctx->key_sz = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic int qat_rsa_setpubkey(struct crypto_akcipher *tfm, const void *key,\r\nunsigned int keylen)\r\n{\r\nreturn qat_rsa_setkey(tfm, key, keylen, false);\r\n}\r\nstatic int qat_rsa_setprivkey(struct crypto_akcipher *tfm, const void *key,\r\nunsigned int keylen)\r\n{\r\nreturn qat_rsa_setkey(tfm, key, keylen, true);\r\n}\r\nstatic int qat_rsa_max_size(struct crypto_akcipher *tfm)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nreturn (ctx->n) ? ctx->key_sz : -EINVAL;\r\n}\r\nstatic int qat_rsa_init_tfm(struct crypto_akcipher *tfm)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst =\r\nqat_crypto_get_instance_node(get_current_node());\r\nif (!inst)\r\nreturn -EINVAL;\r\nctx->key_sz = 0;\r\nctx->inst = inst;\r\nreturn 0;\r\n}\r\nstatic void qat_rsa_exit_tfm(struct crypto_akcipher *tfm)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\r\nif (ctx->n)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->n, ctx->dma_n);\r\nif (ctx->e)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->e, ctx->dma_e);\r\nif (ctx->d) {\r\nmemset(ctx->d, '\0', ctx->key_sz);\r\ndma_free_coherent(dev, ctx->key_sz, ctx->d, ctx->dma_d);\r\n}\r\nqat_crypto_put_instance(ctx->inst);\r\nctx->n = NULL;\r\nctx->d = NULL;\r\nctx->d = NULL;\r\n}\r\nint qat_asym_algs_register(void)\r\n{\r\nint ret = 0;\r\nmutex_lock(&algs_lock);\r\nif (++active_devs == 1) {\r\nrsa.base.cra_flags = 0;\r\nret = crypto_register_akcipher(&rsa);\r\n}\r\nmutex_unlock(&algs_lock);\r\nreturn ret;\r\n}\r\nvoid qat_asym_algs_unregister(void)\r\n{\r\nmutex_lock(&algs_lock);\r\nif (--active_devs == 0)\r\ncrypto_unregister_akcipher(&rsa);\r\nmutex_unlock(&algs_lock);\r\n}
