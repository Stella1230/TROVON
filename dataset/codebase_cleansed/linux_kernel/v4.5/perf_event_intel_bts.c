static size_t buf_size(struct page *page)\r\n{\r\nreturn 1 << (PAGE_SHIFT + page_private(page));\r\n}\r\nstatic void *\r\nbts_buffer_setup_aux(int cpu, void **pages, int nr_pages, bool overwrite)\r\n{\r\nstruct bts_buffer *buf;\r\nstruct page *page;\r\nint node = (cpu == -1) ? cpu : cpu_to_node(cpu);\r\nunsigned long offset;\r\nsize_t size = nr_pages << PAGE_SHIFT;\r\nint pg, nbuf, pad;\r\nfor (pg = 0, nbuf = 0; pg < nr_pages;) {\r\npage = virt_to_page(pages[pg]);\r\nif (WARN_ON_ONCE(!PagePrivate(page) && nr_pages > 1))\r\nreturn NULL;\r\npg += 1 << page_private(page);\r\nnbuf++;\r\n}\r\nif (overwrite && nbuf > 1)\r\nreturn NULL;\r\nbuf = kzalloc_node(offsetof(struct bts_buffer, buf[nbuf]), GFP_KERNEL, node);\r\nif (!buf)\r\nreturn NULL;\r\nbuf->nr_pages = nr_pages;\r\nbuf->nr_bufs = nbuf;\r\nbuf->snapshot = overwrite;\r\nbuf->data_pages = pages;\r\nbuf->real_size = size - size % BTS_RECORD_SIZE;\r\nfor (pg = 0, nbuf = 0, offset = 0, pad = 0; nbuf < buf->nr_bufs; nbuf++) {\r\nunsigned int __nr_pages;\r\npage = virt_to_page(pages[pg]);\r\n__nr_pages = PagePrivate(page) ? 1 << page_private(page) : 1;\r\nbuf->buf[nbuf].page = page;\r\nbuf->buf[nbuf].offset = offset;\r\nbuf->buf[nbuf].displacement = (pad ? BTS_RECORD_SIZE - pad : 0);\r\nbuf->buf[nbuf].size = buf_size(page) - buf->buf[nbuf].displacement;\r\npad = buf->buf[nbuf].size % BTS_RECORD_SIZE;\r\nbuf->buf[nbuf].size -= pad;\r\npg += __nr_pages;\r\noffset += __nr_pages << PAGE_SHIFT;\r\n}\r\nreturn buf;\r\n}\r\nstatic void bts_buffer_free_aux(void *data)\r\n{\r\nkfree(data);\r\n}\r\nstatic unsigned long bts_buffer_offset(struct bts_buffer *buf, unsigned int idx)\r\n{\r\nreturn buf->buf[idx].offset + buf->buf[idx].displacement;\r\n}\r\nstatic void\r\nbts_config_buffer(struct bts_buffer *buf)\r\n{\r\nint cpu = raw_smp_processor_id();\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nstruct bts_phys *phys = &buf->buf[buf->cur_buf];\r\nunsigned long index, thresh = 0, end = phys->size;\r\nstruct page *page = phys->page;\r\nindex = local_read(&buf->head);\r\nif (!buf->snapshot) {\r\nif (buf->end < phys->offset + buf_size(page))\r\nend = buf->end - phys->offset - phys->displacement;\r\nindex -= phys->offset + phys->displacement;\r\nif (end - index > BTS_SAFETY_MARGIN)\r\nthresh = end - BTS_SAFETY_MARGIN;\r\nelse if (end - index > BTS_RECORD_SIZE)\r\nthresh = end - BTS_RECORD_SIZE;\r\nelse\r\nthresh = end;\r\n}\r\nds->bts_buffer_base = (u64)(long)page_address(page) + phys->displacement;\r\nds->bts_index = ds->bts_buffer_base + index;\r\nds->bts_absolute_maximum = ds->bts_buffer_base + end;\r\nds->bts_interrupt_threshold = !buf->snapshot\r\n? ds->bts_buffer_base + thresh\r\n: ds->bts_absolute_maximum + BTS_RECORD_SIZE;\r\n}\r\nstatic void bts_buffer_pad_out(struct bts_phys *phys, unsigned long head)\r\n{\r\nunsigned long index = head - phys->offset;\r\nmemset(page_address(phys->page) + index, 0, phys->size - index);\r\n}\r\nstatic bool bts_buffer_is_full(struct bts_buffer *buf, struct bts_ctx *bts)\r\n{\r\nif (buf->snapshot)\r\nreturn false;\r\nif (local_read(&buf->data_size) >= bts->handle.size ||\r\nbts->handle.size - local_read(&buf->data_size) < BTS_RECORD_SIZE)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void bts_update(struct bts_ctx *bts)\r\n{\r\nint cpu = raw_smp_processor_id();\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nstruct bts_buffer *buf = perf_get_aux(&bts->handle);\r\nunsigned long index = ds->bts_index - ds->bts_buffer_base, old, head;\r\nif (!buf)\r\nreturn;\r\nhead = index + bts_buffer_offset(buf, buf->cur_buf);\r\nold = local_xchg(&buf->head, head);\r\nif (!buf->snapshot) {\r\nif (old == head)\r\nreturn;\r\nif (ds->bts_index >= ds->bts_absolute_maximum)\r\nlocal_inc(&buf->lost);\r\nlocal_add(head - old, &buf->data_size);\r\n} else {\r\nlocal_set(&buf->data_size, head);\r\n}\r\n}\r\nstatic void __bts_event_start(struct perf_event *event)\r\n{\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\nstruct bts_buffer *buf = perf_get_aux(&bts->handle);\r\nu64 config = 0;\r\nif (!buf || bts_buffer_is_full(buf, bts))\r\nreturn;\r\nevent->hw.itrace_started = 1;\r\nevent->hw.state = 0;\r\nif (!buf->snapshot)\r\nconfig |= ARCH_PERFMON_EVENTSEL_INT;\r\nif (!event->attr.exclude_kernel)\r\nconfig |= ARCH_PERFMON_EVENTSEL_OS;\r\nif (!event->attr.exclude_user)\r\nconfig |= ARCH_PERFMON_EVENTSEL_USR;\r\nbts_config_buffer(buf);\r\nwmb();\r\nintel_pmu_enable_bts(config);\r\n}\r\nstatic void bts_event_start(struct perf_event *event, int flags)\r\n{\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\n__bts_event_start(event);\r\nACCESS_ONCE(bts->started) = 1;\r\n}\r\nstatic void __bts_event_stop(struct perf_event *event)\r\n{\r\nintel_pmu_disable_bts();\r\nif (event->hw.state & PERF_HES_STOPPED)\r\nreturn;\r\nACCESS_ONCE(event->hw.state) |= PERF_HES_STOPPED;\r\n}\r\nstatic void bts_event_stop(struct perf_event *event, int flags)\r\n{\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\nACCESS_ONCE(bts->started) = 0;\r\n__bts_event_stop(event);\r\nif (flags & PERF_EF_UPDATE)\r\nbts_update(bts);\r\n}\r\nvoid intel_bts_enable_local(void)\r\n{\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\nif (bts->handle.event && bts->started)\r\n__bts_event_start(bts->handle.event);\r\n}\r\nvoid intel_bts_disable_local(void)\r\n{\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\nif (bts->handle.event)\r\n__bts_event_stop(bts->handle.event);\r\n}\r\nstatic int\r\nbts_buffer_reset(struct bts_buffer *buf, struct perf_output_handle *handle)\r\n{\r\nunsigned long head, space, next_space, pad, gap, skip, wakeup;\r\nunsigned int next_buf;\r\nstruct bts_phys *phys, *next_phys;\r\nint ret;\r\nif (buf->snapshot)\r\nreturn 0;\r\nhead = handle->head & ((buf->nr_pages << PAGE_SHIFT) - 1);\r\nif (WARN_ON_ONCE(head != local_read(&buf->head)))\r\nreturn -EINVAL;\r\nphys = &buf->buf[buf->cur_buf];\r\nspace = phys->offset + phys->displacement + phys->size - head;\r\npad = space;\r\nif (space > handle->size) {\r\nspace = handle->size;\r\nspace -= space % BTS_RECORD_SIZE;\r\n}\r\nif (space <= BTS_SAFETY_MARGIN) {\r\nnext_buf = buf->cur_buf + 1;\r\nif (next_buf >= buf->nr_bufs)\r\nnext_buf = 0;\r\nnext_phys = &buf->buf[next_buf];\r\ngap = buf_size(phys->page) - phys->displacement - phys->size +\r\nnext_phys->displacement;\r\nskip = pad + gap;\r\nif (handle->size >= skip) {\r\nnext_space = next_phys->size;\r\nif (next_space + skip > handle->size) {\r\nnext_space = handle->size - skip;\r\nnext_space -= next_space % BTS_RECORD_SIZE;\r\n}\r\nif (next_space > space || !space) {\r\nif (pad)\r\nbts_buffer_pad_out(phys, head);\r\nret = perf_aux_output_skip(handle, skip);\r\nif (ret)\r\nreturn ret;\r\nphys = next_phys;\r\nspace = next_space;\r\nhead = phys->offset + phys->displacement;\r\nbuf->cur_buf = next_buf;\r\nlocal_set(&buf->head, head);\r\n}\r\n}\r\n}\r\nwakeup = BTS_SAFETY_MARGIN + BTS_RECORD_SIZE + handle->wakeup -\r\nhandle->head;\r\nif (space > wakeup) {\r\nspace = wakeup;\r\nspace -= space % BTS_RECORD_SIZE;\r\n}\r\nbuf->end = head + space;\r\nif (!space)\r\nreturn -ENOSPC;\r\nreturn 0;\r\n}\r\nint intel_bts_interrupt(void)\r\n{\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\nstruct perf_event *event = bts->handle.event;\r\nstruct bts_buffer *buf;\r\ns64 old_head;\r\nint err;\r\nif (!event || !bts->started)\r\nreturn 0;\r\nbuf = perf_get_aux(&bts->handle);\r\nif (!buf || buf->snapshot)\r\nreturn 0;\r\nold_head = local_read(&buf->head);\r\nbts_update(bts);\r\nif (old_head == local_read(&buf->head))\r\nreturn 0;\r\nperf_aux_output_end(&bts->handle, local_xchg(&buf->data_size, 0),\r\n!!local_xchg(&buf->lost, 0));\r\nbuf = perf_aux_output_begin(&bts->handle, event);\r\nif (!buf)\r\nreturn 1;\r\nerr = bts_buffer_reset(buf, &bts->handle);\r\nif (err)\r\nperf_aux_output_end(&bts->handle, 0, false);\r\nreturn 1;\r\n}\r\nstatic void bts_event_del(struct perf_event *event, int mode)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\nstruct bts_buffer *buf = perf_get_aux(&bts->handle);\r\nbts_event_stop(event, PERF_EF_UPDATE);\r\nif (buf) {\r\nif (buf->snapshot)\r\nbts->handle.head =\r\nlocal_xchg(&buf->data_size,\r\nbuf->nr_pages << PAGE_SHIFT);\r\nperf_aux_output_end(&bts->handle, local_xchg(&buf->data_size, 0),\r\n!!local_xchg(&buf->lost, 0));\r\n}\r\ncpuc->ds->bts_index = bts->ds_back.bts_buffer_base;\r\ncpuc->ds->bts_buffer_base = bts->ds_back.bts_buffer_base;\r\ncpuc->ds->bts_absolute_maximum = bts->ds_back.bts_absolute_maximum;\r\ncpuc->ds->bts_interrupt_threshold = bts->ds_back.bts_interrupt_threshold;\r\n}\r\nstatic int bts_event_add(struct perf_event *event, int mode)\r\n{\r\nstruct bts_buffer *buf;\r\nstruct bts_ctx *bts = this_cpu_ptr(&bts_ctx);\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint ret = -EBUSY;\r\nevent->hw.state = PERF_HES_STOPPED;\r\nif (test_bit(INTEL_PMC_IDX_FIXED_BTS, cpuc->active_mask))\r\nreturn -EBUSY;\r\nif (bts->handle.event)\r\nreturn -EBUSY;\r\nbuf = perf_aux_output_begin(&bts->handle, event);\r\nif (!buf)\r\nreturn -EINVAL;\r\nret = bts_buffer_reset(buf, &bts->handle);\r\nif (ret) {\r\nperf_aux_output_end(&bts->handle, 0, false);\r\nreturn ret;\r\n}\r\nbts->ds_back.bts_buffer_base = cpuc->ds->bts_buffer_base;\r\nbts->ds_back.bts_absolute_maximum = cpuc->ds->bts_absolute_maximum;\r\nbts->ds_back.bts_interrupt_threshold = cpuc->ds->bts_interrupt_threshold;\r\nif (mode & PERF_EF_START) {\r\nbts_event_start(event, 0);\r\nif (hwc->state & PERF_HES_STOPPED) {\r\nbts_event_del(event, 0);\r\nreturn -EBUSY;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void bts_event_destroy(struct perf_event *event)\r\n{\r\nx86_release_hardware();\r\nx86_del_exclusive(x86_lbr_exclusive_bts);\r\n}\r\nstatic int bts_event_init(struct perf_event *event)\r\n{\r\nint ret;\r\nif (event->attr.type != bts_pmu.type)\r\nreturn -ENOENT;\r\nif (x86_add_exclusive(x86_lbr_exclusive_bts))\r\nreturn -EBUSY;\r\nif (event->attr.exclude_kernel && perf_paranoid_kernel() &&\r\n!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nret = x86_reserve_hardware();\r\nif (ret) {\r\nx86_del_exclusive(x86_lbr_exclusive_bts);\r\nreturn ret;\r\n}\r\nevent->destroy = bts_event_destroy;\r\nreturn 0;\r\n}\r\nstatic void bts_event_read(struct perf_event *event)\r\n{\r\n}\r\nstatic __init int bts_init(void)\r\n{\r\nif (!boot_cpu_has(X86_FEATURE_DTES64) || !x86_pmu.bts)\r\nreturn -ENODEV;\r\nbts_pmu.capabilities = PERF_PMU_CAP_AUX_NO_SG | PERF_PMU_CAP_ITRACE;\r\nbts_pmu.task_ctx_nr = perf_sw_context;\r\nbts_pmu.event_init = bts_event_init;\r\nbts_pmu.add = bts_event_add;\r\nbts_pmu.del = bts_event_del;\r\nbts_pmu.start = bts_event_start;\r\nbts_pmu.stop = bts_event_stop;\r\nbts_pmu.read = bts_event_read;\r\nbts_pmu.setup_aux = bts_buffer_setup_aux;\r\nbts_pmu.free_aux = bts_buffer_free_aux;\r\nreturn perf_pmu_register(&bts_pmu, "intel_bts", -1);\r\n}
