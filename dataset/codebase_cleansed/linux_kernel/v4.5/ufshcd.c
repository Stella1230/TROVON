static inline enum ufs_dev_pwr_mode\r\nufs_get_pm_lvl_to_dev_pwr_mode(enum ufs_pm_level lvl)\r\n{\r\nreturn ufs_pm_lvl_states[lvl].dev_state;\r\n}\r\nstatic inline enum uic_link_state\r\nufs_get_pm_lvl_to_link_pwr_state(enum ufs_pm_level lvl)\r\n{\r\nreturn ufs_pm_lvl_states[lvl].link_state;\r\n}\r\nstatic inline int ufshcd_enable_irq(struct ufs_hba *hba)\r\n{\r\nint ret = 0;\r\nif (!hba->is_irq_enabled) {\r\nret = request_irq(hba->irq, ufshcd_intr, IRQF_SHARED, UFSHCD,\r\nhba);\r\nif (ret)\r\ndev_err(hba->dev, "%s: request_irq failed, ret=%d\n",\r\n__func__, ret);\r\nhba->is_irq_enabled = true;\r\n}\r\nreturn ret;\r\n}\r\nstatic inline void ufshcd_disable_irq(struct ufs_hba *hba)\r\n{\r\nif (hba->is_irq_enabled) {\r\nfree_irq(hba->irq, hba);\r\nhba->is_irq_enabled = false;\r\n}\r\n}\r\nstatic int ufshcd_wait_for_register(struct ufs_hba *hba, u32 reg, u32 mask,\r\nu32 val, unsigned long interval_us, unsigned long timeout_ms)\r\n{\r\nint err = 0;\r\nunsigned long timeout = jiffies + msecs_to_jiffies(timeout_ms);\r\nval = val & mask;\r\nwhile ((ufshcd_readl(hba, reg) & mask) != val) {\r\nusleep_range(interval_us, interval_us + 50);\r\nif (time_after(jiffies, timeout)) {\r\nif ((ufshcd_readl(hba, reg) & mask) != val)\r\nerr = -ETIMEDOUT;\r\nbreak;\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic inline u32 ufshcd_get_intr_mask(struct ufs_hba *hba)\r\n{\r\nif (hba->ufs_version == UFSHCI_VERSION_10)\r\nreturn INTERRUPT_MASK_ALL_VER_10;\r\nelse\r\nreturn INTERRUPT_MASK_ALL_VER_11;\r\n}\r\nstatic inline u32 ufshcd_get_ufs_version(struct ufs_hba *hba)\r\n{\r\nif (hba->quirks & UFSHCD_QUIRK_BROKEN_UFS_HCI_VERSION)\r\nreturn ufshcd_vops_get_ufs_hci_version(hba);\r\nreturn ufshcd_readl(hba, REG_UFS_VERSION);\r\n}\r\nstatic inline int ufshcd_is_device_present(struct ufs_hba *hba)\r\n{\r\nreturn (ufshcd_readl(hba, REG_CONTROLLER_STATUS) &\r\nDEVICE_PRESENT) ? 1 : 0;\r\n}\r\nstatic inline int ufshcd_get_tr_ocs(struct ufshcd_lrb *lrbp)\r\n{\r\nreturn le32_to_cpu(lrbp->utr_descriptor_ptr->header.dword_2) & MASK_OCS;\r\n}\r\nstatic inline int\r\nufshcd_get_tmr_ocs(struct utp_task_req_desc *task_req_descp)\r\n{\r\nreturn le32_to_cpu(task_req_descp->header.dword_2) & MASK_OCS;\r\n}\r\nstatic bool ufshcd_get_tm_free_slot(struct ufs_hba *hba, int *free_slot)\r\n{\r\nint tag;\r\nbool ret = false;\r\nif (!free_slot)\r\ngoto out;\r\ndo {\r\ntag = find_first_zero_bit(&hba->tm_slots_in_use, hba->nutmrs);\r\nif (tag >= hba->nutmrs)\r\ngoto out;\r\n} while (test_and_set_bit_lock(tag, &hba->tm_slots_in_use));\r\n*free_slot = tag;\r\nret = true;\r\nout:\r\nreturn ret;\r\n}\r\nstatic inline void ufshcd_put_tm_slot(struct ufs_hba *hba, int slot)\r\n{\r\nclear_bit_unlock(slot, &hba->tm_slots_in_use);\r\n}\r\nstatic inline void ufshcd_utrl_clear(struct ufs_hba *hba, u32 pos)\r\n{\r\nufshcd_writel(hba, ~(1 << pos), REG_UTP_TRANSFER_REQ_LIST_CLEAR);\r\n}\r\nstatic inline int ufshcd_get_lists_status(u32 reg)\r\n{\r\nreturn (((reg) & (0xFF)) >> 1) ^ (0x07);\r\n}\r\nstatic inline int ufshcd_get_uic_cmd_result(struct ufs_hba *hba)\r\n{\r\nreturn ufshcd_readl(hba, REG_UIC_COMMAND_ARG_2) &\r\nMASK_UIC_COMMAND_RESULT;\r\n}\r\nstatic inline u32 ufshcd_get_dme_attr_val(struct ufs_hba *hba)\r\n{\r\nreturn ufshcd_readl(hba, REG_UIC_COMMAND_ARG_3);\r\n}\r\nstatic inline int\r\nufshcd_get_req_rsp(struct utp_upiu_rsp *ucd_rsp_ptr)\r\n{\r\nreturn be32_to_cpu(ucd_rsp_ptr->header.dword_0) >> 24;\r\n}\r\nstatic inline int\r\nufshcd_get_rsp_upiu_result(struct utp_upiu_rsp *ucd_rsp_ptr)\r\n{\r\nreturn be32_to_cpu(ucd_rsp_ptr->header.dword_1) & MASK_RSP_UPIU_RESULT;\r\n}\r\nstatic inline unsigned int\r\nufshcd_get_rsp_upiu_data_seg_len(struct utp_upiu_rsp *ucd_rsp_ptr)\r\n{\r\nreturn be32_to_cpu(ucd_rsp_ptr->header.dword_2) &\r\nMASK_RSP_UPIU_DATA_SEG_LEN;\r\n}\r\nstatic inline bool ufshcd_is_exception_event(struct utp_upiu_rsp *ucd_rsp_ptr)\r\n{\r\nreturn be32_to_cpu(ucd_rsp_ptr->header.dword_2) &\r\nMASK_RSP_EXCEPTION_EVENT ? true : false;\r\n}\r\nstatic inline void\r\nufshcd_reset_intr_aggr(struct ufs_hba *hba)\r\n{\r\nufshcd_writel(hba, INT_AGGR_ENABLE |\r\nINT_AGGR_COUNTER_AND_TIMER_RESET,\r\nREG_UTP_TRANSFER_REQ_INT_AGG_CONTROL);\r\n}\r\nstatic inline void\r\nufshcd_config_intr_aggr(struct ufs_hba *hba, u8 cnt, u8 tmout)\r\n{\r\nufshcd_writel(hba, INT_AGGR_ENABLE | INT_AGGR_PARAM_WRITE |\r\nINT_AGGR_COUNTER_THLD_VAL(cnt) |\r\nINT_AGGR_TIMEOUT_VAL(tmout),\r\nREG_UTP_TRANSFER_REQ_INT_AGG_CONTROL);\r\n}\r\nstatic inline void ufshcd_disable_intr_aggr(struct ufs_hba *hba)\r\n{\r\nufshcd_writel(hba, 0, REG_UTP_TRANSFER_REQ_INT_AGG_CONTROL);\r\n}\r\nstatic void ufshcd_enable_run_stop_reg(struct ufs_hba *hba)\r\n{\r\nufshcd_writel(hba, UTP_TASK_REQ_LIST_RUN_STOP_BIT,\r\nREG_UTP_TASK_REQ_LIST_RUN_STOP);\r\nufshcd_writel(hba, UTP_TRANSFER_REQ_LIST_RUN_STOP_BIT,\r\nREG_UTP_TRANSFER_REQ_LIST_RUN_STOP);\r\n}\r\nstatic inline void ufshcd_hba_start(struct ufs_hba *hba)\r\n{\r\nufshcd_writel(hba, CONTROLLER_ENABLE, REG_CONTROLLER_ENABLE);\r\n}\r\nstatic inline int ufshcd_is_hba_active(struct ufs_hba *hba)\r\n{\r\nreturn (ufshcd_readl(hba, REG_CONTROLLER_ENABLE) & 0x1) ? 0 : 1;\r\n}\r\nstatic void ufshcd_ungate_work(struct work_struct *work)\r\n{\r\nint ret;\r\nunsigned long flags;\r\nstruct ufs_hba *hba = container_of(work, struct ufs_hba,\r\nclk_gating.ungate_work);\r\ncancel_delayed_work_sync(&hba->clk_gating.gate_work);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (hba->clk_gating.state == CLKS_ON) {\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\ngoto unblock_reqs;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nufshcd_setup_clocks(hba, true);\r\nif (ufshcd_can_hibern8_during_gating(hba)) {\r\nhba->clk_gating.is_suspended = true;\r\nif (ufshcd_is_link_hibern8(hba)) {\r\nret = ufshcd_uic_hibern8_exit(hba);\r\nif (ret)\r\ndev_err(hba->dev, "%s: hibern8 exit failed %d\n",\r\n__func__, ret);\r\nelse\r\nufshcd_set_link_active(hba);\r\n}\r\nhba->clk_gating.is_suspended = false;\r\n}\r\nunblock_reqs:\r\nif (ufshcd_is_clkscaling_enabled(hba))\r\ndevfreq_resume_device(hba->devfreq);\r\nscsi_unblock_requests(hba->host);\r\n}\r\nint ufshcd_hold(struct ufs_hba *hba, bool async)\r\n{\r\nint rc = 0;\r\nunsigned long flags;\r\nif (!ufshcd_is_clkgating_allowed(hba))\r\ngoto out;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->clk_gating.active_reqs++;\r\nstart:\r\nswitch (hba->clk_gating.state) {\r\ncase CLKS_ON:\r\nbreak;\r\ncase REQ_CLKS_OFF:\r\nif (cancel_delayed_work(&hba->clk_gating.gate_work)) {\r\nhba->clk_gating.state = CLKS_ON;\r\nbreak;\r\n}\r\ncase CLKS_OFF:\r\nscsi_block_requests(hba->host);\r\nhba->clk_gating.state = REQ_CLKS_ON;\r\nschedule_work(&hba->clk_gating.ungate_work);\r\ncase REQ_CLKS_ON:\r\nif (async) {\r\nrc = -EAGAIN;\r\nhba->clk_gating.active_reqs--;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nflush_work(&hba->clk_gating.ungate_work);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\ngoto start;\r\ndefault:\r\ndev_err(hba->dev, "%s: clk gating is in invalid state %d\n",\r\n__func__, hba->clk_gating.state);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nout:\r\nreturn rc;\r\n}\r\nstatic void ufshcd_gate_work(struct work_struct *work)\r\n{\r\nstruct ufs_hba *hba = container_of(work, struct ufs_hba,\r\nclk_gating.gate_work.work);\r\nunsigned long flags;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (hba->clk_gating.is_suspended) {\r\nhba->clk_gating.state = CLKS_ON;\r\ngoto rel_lock;\r\n}\r\nif (hba->clk_gating.active_reqs\r\n|| hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL\r\n|| hba->lrb_in_use || hba->outstanding_tasks\r\n|| hba->active_uic_cmd || hba->uic_async_done)\r\ngoto rel_lock;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (ufshcd_can_hibern8_during_gating(hba)) {\r\nif (ufshcd_uic_hibern8_enter(hba)) {\r\nhba->clk_gating.state = CLKS_ON;\r\ngoto out;\r\n}\r\nufshcd_set_link_hibern8(hba);\r\n}\r\nif (ufshcd_is_clkscaling_enabled(hba)) {\r\ndevfreq_suspend_device(hba->devfreq);\r\nhba->clk_scaling.window_start_t = 0;\r\n}\r\nif (!ufshcd_is_link_active(hba))\r\nufshcd_setup_clocks(hba, false);\r\nelse\r\n__ufshcd_setup_clocks(hba, false, true);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (hba->clk_gating.state == REQ_CLKS_OFF)\r\nhba->clk_gating.state = CLKS_OFF;\r\nrel_lock:\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nout:\r\nreturn;\r\n}\r\nstatic void __ufshcd_release(struct ufs_hba *hba)\r\n{\r\nif (!ufshcd_is_clkgating_allowed(hba))\r\nreturn;\r\nhba->clk_gating.active_reqs--;\r\nif (hba->clk_gating.active_reqs || hba->clk_gating.is_suspended\r\n|| hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL\r\n|| hba->lrb_in_use || hba->outstanding_tasks\r\n|| hba->active_uic_cmd || hba->uic_async_done)\r\nreturn;\r\nhba->clk_gating.state = REQ_CLKS_OFF;\r\nschedule_delayed_work(&hba->clk_gating.gate_work,\r\nmsecs_to_jiffies(hba->clk_gating.delay_ms));\r\n}\r\nvoid ufshcd_release(struct ufs_hba *hba)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\n__ufshcd_release(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\n}\r\nstatic ssize_t ufshcd_clkgate_delay_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct ufs_hba *hba = dev_get_drvdata(dev);\r\nreturn snprintf(buf, PAGE_SIZE, "%lu\n", hba->clk_gating.delay_ms);\r\n}\r\nstatic ssize_t ufshcd_clkgate_delay_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t count)\r\n{\r\nstruct ufs_hba *hba = dev_get_drvdata(dev);\r\nunsigned long flags, value;\r\nif (kstrtoul(buf, 0, &value))\r\nreturn -EINVAL;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->clk_gating.delay_ms = value;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn count;\r\n}\r\nstatic void ufshcd_init_clk_gating(struct ufs_hba *hba)\r\n{\r\nif (!ufshcd_is_clkgating_allowed(hba))\r\nreturn;\r\nhba->clk_gating.delay_ms = 150;\r\nINIT_DELAYED_WORK(&hba->clk_gating.gate_work, ufshcd_gate_work);\r\nINIT_WORK(&hba->clk_gating.ungate_work, ufshcd_ungate_work);\r\nhba->clk_gating.delay_attr.show = ufshcd_clkgate_delay_show;\r\nhba->clk_gating.delay_attr.store = ufshcd_clkgate_delay_store;\r\nsysfs_attr_init(&hba->clk_gating.delay_attr.attr);\r\nhba->clk_gating.delay_attr.attr.name = "clkgate_delay_ms";\r\nhba->clk_gating.delay_attr.attr.mode = S_IRUGO | S_IWUSR;\r\nif (device_create_file(hba->dev, &hba->clk_gating.delay_attr))\r\ndev_err(hba->dev, "Failed to create sysfs for clkgate_delay\n");\r\n}\r\nstatic void ufshcd_exit_clk_gating(struct ufs_hba *hba)\r\n{\r\nif (!ufshcd_is_clkgating_allowed(hba))\r\nreturn;\r\ndevice_remove_file(hba->dev, &hba->clk_gating.delay_attr);\r\ncancel_work_sync(&hba->clk_gating.ungate_work);\r\ncancel_delayed_work_sync(&hba->clk_gating.gate_work);\r\n}\r\nstatic void ufshcd_clk_scaling_start_busy(struct ufs_hba *hba)\r\n{\r\nif (!ufshcd_is_clkscaling_enabled(hba))\r\nreturn;\r\nif (!hba->clk_scaling.is_busy_started) {\r\nhba->clk_scaling.busy_start_t = ktime_get();\r\nhba->clk_scaling.is_busy_started = true;\r\n}\r\n}\r\nstatic void ufshcd_clk_scaling_update_busy(struct ufs_hba *hba)\r\n{\r\nstruct ufs_clk_scaling *scaling = &hba->clk_scaling;\r\nif (!ufshcd_is_clkscaling_enabled(hba))\r\nreturn;\r\nif (!hba->outstanding_reqs && scaling->is_busy_started) {\r\nscaling->tot_busy_t += ktime_to_us(ktime_sub(ktime_get(),\r\nscaling->busy_start_t));\r\nscaling->busy_start_t = ktime_set(0, 0);\r\nscaling->is_busy_started = false;\r\n}\r\n}\r\nstatic inline\r\nvoid ufshcd_send_command(struct ufs_hba *hba, unsigned int task_tag)\r\n{\r\nufshcd_clk_scaling_start_busy(hba);\r\n__set_bit(task_tag, &hba->outstanding_reqs);\r\nufshcd_writel(hba, 1 << task_tag, REG_UTP_TRANSFER_REQ_DOOR_BELL);\r\n}\r\nstatic inline void ufshcd_copy_sense_data(struct ufshcd_lrb *lrbp)\r\n{\r\nint len;\r\nif (lrbp->sense_buffer &&\r\nufshcd_get_rsp_upiu_data_seg_len(lrbp->ucd_rsp_ptr)) {\r\nlen = be16_to_cpu(lrbp->ucd_rsp_ptr->sr.sense_data_len);\r\nmemcpy(lrbp->sense_buffer,\r\nlrbp->ucd_rsp_ptr->sr.sense_data,\r\nmin_t(int, len, SCSI_SENSE_BUFFERSIZE));\r\n}\r\n}\r\nstatic\r\nint ufshcd_copy_query_response(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\r\n{\r\nstruct ufs_query_res *query_res = &hba->dev_cmd.query.response;\r\nmemcpy(&query_res->upiu_res, &lrbp->ucd_rsp_ptr->qr, QUERY_OSF_SIZE);\r\nif (lrbp->ucd_rsp_ptr->qr.opcode == UPIU_QUERY_OPCODE_READ_DESC) {\r\nu8 *descp = (u8 *)lrbp->ucd_rsp_ptr +\r\nGENERAL_UPIU_REQUEST_SIZE;\r\nu16 resp_len;\r\nu16 buf_len;\r\nresp_len = be32_to_cpu(lrbp->ucd_rsp_ptr->header.dword_2) &\r\nMASK_QUERY_DATA_SEG_LEN;\r\nbuf_len = be16_to_cpu(\r\nhba->dev_cmd.query.request.upiu_req.length);\r\nif (likely(buf_len >= resp_len)) {\r\nmemcpy(hba->dev_cmd.query.descriptor, descp, resp_len);\r\n} else {\r\ndev_warn(hba->dev,\r\n"%s: Response size is bigger than buffer",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void ufshcd_hba_capabilities(struct ufs_hba *hba)\r\n{\r\nhba->capabilities = ufshcd_readl(hba, REG_CONTROLLER_CAPABILITIES);\r\nhba->nutrs = (hba->capabilities & MASK_TRANSFER_REQUESTS_SLOTS) + 1;\r\nhba->nutmrs =\r\n((hba->capabilities & MASK_TASK_MANAGEMENT_REQUEST_SLOTS) >> 16) + 1;\r\n}\r\nstatic inline bool ufshcd_ready_for_uic_cmd(struct ufs_hba *hba)\r\n{\r\nif (ufshcd_readl(hba, REG_CONTROLLER_STATUS) & UIC_COMMAND_READY)\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic inline u8 ufshcd_get_upmcrs(struct ufs_hba *hba)\r\n{\r\nreturn (ufshcd_readl(hba, REG_CONTROLLER_STATUS) >> 8) & 0x7;\r\n}\r\nstatic inline void\r\nufshcd_dispatch_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd)\r\n{\r\nWARN_ON(hba->active_uic_cmd);\r\nhba->active_uic_cmd = uic_cmd;\r\nufshcd_writel(hba, uic_cmd->argument1, REG_UIC_COMMAND_ARG_1);\r\nufshcd_writel(hba, uic_cmd->argument2, REG_UIC_COMMAND_ARG_2);\r\nufshcd_writel(hba, uic_cmd->argument3, REG_UIC_COMMAND_ARG_3);\r\nufshcd_writel(hba, uic_cmd->command & COMMAND_OPCODE_MASK,\r\nREG_UIC_COMMAND);\r\n}\r\nstatic int\r\nufshcd_wait_for_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd)\r\n{\r\nint ret;\r\nunsigned long flags;\r\nif (wait_for_completion_timeout(&uic_cmd->done,\r\nmsecs_to_jiffies(UIC_CMD_TIMEOUT)))\r\nret = uic_cmd->argument2 & MASK_UIC_COMMAND_RESULT;\r\nelse\r\nret = -ETIMEDOUT;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->active_uic_cmd = NULL;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int\r\n__ufshcd_send_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd)\r\n{\r\nif (!ufshcd_ready_for_uic_cmd(hba)) {\r\ndev_err(hba->dev,\r\n"Controller not ready to accept UIC commands\n");\r\nreturn -EIO;\r\n}\r\ninit_completion(&uic_cmd->done);\r\nufshcd_dispatch_uic_cmd(hba, uic_cmd);\r\nreturn 0;\r\n}\r\nstatic int\r\nufshcd_send_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd)\r\n{\r\nint ret;\r\nunsigned long flags;\r\nufshcd_hold(hba, false);\r\nmutex_lock(&hba->uic_cmd_mutex);\r\nufshcd_add_delay_before_dme_cmd(hba);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nret = __ufshcd_send_uic_cmd(hba, uic_cmd);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (!ret)\r\nret = ufshcd_wait_for_uic_cmd(hba, uic_cmd);\r\nmutex_unlock(&hba->uic_cmd_mutex);\r\nufshcd_release(hba);\r\nreturn ret;\r\n}\r\nstatic int ufshcd_map_sg(struct ufshcd_lrb *lrbp)\r\n{\r\nstruct ufshcd_sg_entry *prd_table;\r\nstruct scatterlist *sg;\r\nstruct scsi_cmnd *cmd;\r\nint sg_segments;\r\nint i;\r\ncmd = lrbp->cmd;\r\nsg_segments = scsi_dma_map(cmd);\r\nif (sg_segments < 0)\r\nreturn sg_segments;\r\nif (sg_segments) {\r\nlrbp->utr_descriptor_ptr->prd_table_length =\r\ncpu_to_le16((u16) (sg_segments));\r\nprd_table = (struct ufshcd_sg_entry *)lrbp->ucd_prdt_ptr;\r\nscsi_for_each_sg(cmd, sg, sg_segments, i) {\r\nprd_table[i].size =\r\ncpu_to_le32(((u32) sg_dma_len(sg))-1);\r\nprd_table[i].base_addr =\r\ncpu_to_le32(lower_32_bits(sg->dma_address));\r\nprd_table[i].upper_addr =\r\ncpu_to_le32(upper_32_bits(sg->dma_address));\r\n}\r\n} else {\r\nlrbp->utr_descriptor_ptr->prd_table_length = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ufshcd_enable_intr(struct ufs_hba *hba, u32 intrs)\r\n{\r\nu32 set = ufshcd_readl(hba, REG_INTERRUPT_ENABLE);\r\nif (hba->ufs_version == UFSHCI_VERSION_10) {\r\nu32 rw;\r\nrw = set & INTERRUPT_MASK_RW_VER_10;\r\nset = rw | ((set ^ intrs) & intrs);\r\n} else {\r\nset |= intrs;\r\n}\r\nufshcd_writel(hba, set, REG_INTERRUPT_ENABLE);\r\n}\r\nstatic void ufshcd_disable_intr(struct ufs_hba *hba, u32 intrs)\r\n{\r\nu32 set = ufshcd_readl(hba, REG_INTERRUPT_ENABLE);\r\nif (hba->ufs_version == UFSHCI_VERSION_10) {\r\nu32 rw;\r\nrw = (set & INTERRUPT_MASK_RW_VER_10) &\r\n~(intrs & INTERRUPT_MASK_RW_VER_10);\r\nset = rw | ((set & intrs) & ~INTERRUPT_MASK_RW_VER_10);\r\n} else {\r\nset &= ~intrs;\r\n}\r\nufshcd_writel(hba, set, REG_INTERRUPT_ENABLE);\r\n}\r\nstatic void ufshcd_prepare_req_desc_hdr(struct ufshcd_lrb *lrbp,\r\nu32 *upiu_flags, enum dma_data_direction cmd_dir)\r\n{\r\nstruct utp_transfer_req_desc *req_desc = lrbp->utr_descriptor_ptr;\r\nu32 data_direction;\r\nu32 dword_0;\r\nif (cmd_dir == DMA_FROM_DEVICE) {\r\ndata_direction = UTP_DEVICE_TO_HOST;\r\n*upiu_flags = UPIU_CMD_FLAGS_READ;\r\n} else if (cmd_dir == DMA_TO_DEVICE) {\r\ndata_direction = UTP_HOST_TO_DEVICE;\r\n*upiu_flags = UPIU_CMD_FLAGS_WRITE;\r\n} else {\r\ndata_direction = UTP_NO_DATA_TRANSFER;\r\n*upiu_flags = UPIU_CMD_FLAGS_NONE;\r\n}\r\ndword_0 = data_direction | (lrbp->command_type\r\n<< UPIU_COMMAND_TYPE_OFFSET);\r\nif (lrbp->intr_cmd)\r\ndword_0 |= UTP_REQ_DESC_INT_CMD;\r\nreq_desc->header.dword_0 = cpu_to_le32(dword_0);\r\nreq_desc->header.dword_2 =\r\ncpu_to_le32(OCS_INVALID_COMMAND_STATUS);\r\n}\r\nstatic\r\nvoid ufshcd_prepare_utp_scsi_cmd_upiu(struct ufshcd_lrb *lrbp, u32 upiu_flags)\r\n{\r\nstruct utp_upiu_req *ucd_req_ptr = lrbp->ucd_req_ptr;\r\nucd_req_ptr->header.dword_0 = UPIU_HEADER_DWORD(\r\nUPIU_TRANSACTION_COMMAND, upiu_flags,\r\nlrbp->lun, lrbp->task_tag);\r\nucd_req_ptr->header.dword_1 = UPIU_HEADER_DWORD(\r\nUPIU_COMMAND_SET_TYPE_SCSI, 0, 0, 0);\r\nucd_req_ptr->header.dword_2 = 0;\r\nucd_req_ptr->sc.exp_data_transfer_len =\r\ncpu_to_be32(lrbp->cmd->sdb.length);\r\nmemcpy(ucd_req_ptr->sc.cdb, lrbp->cmd->cmnd,\r\n(min_t(unsigned short, lrbp->cmd->cmd_len, MAX_CDB_SIZE)));\r\n}\r\nstatic void ufshcd_prepare_utp_query_req_upiu(struct ufs_hba *hba,\r\nstruct ufshcd_lrb *lrbp, u32 upiu_flags)\r\n{\r\nstruct utp_upiu_req *ucd_req_ptr = lrbp->ucd_req_ptr;\r\nstruct ufs_query *query = &hba->dev_cmd.query;\r\nu16 len = be16_to_cpu(query->request.upiu_req.length);\r\nu8 *descp = (u8 *)lrbp->ucd_req_ptr + GENERAL_UPIU_REQUEST_SIZE;\r\nucd_req_ptr->header.dword_0 = UPIU_HEADER_DWORD(\r\nUPIU_TRANSACTION_QUERY_REQ, upiu_flags,\r\nlrbp->lun, lrbp->task_tag);\r\nucd_req_ptr->header.dword_1 = UPIU_HEADER_DWORD(\r\n0, query->request.query_func, 0, 0);\r\nucd_req_ptr->header.dword_2 = UPIU_HEADER_DWORD(\r\n0, 0, len >> 8, (u8)len);\r\nmemcpy(&ucd_req_ptr->qr, &query->request.upiu_req,\r\nQUERY_OSF_SIZE);\r\nif (query->request.upiu_req.opcode == UPIU_QUERY_OPCODE_WRITE_DESC)\r\nmemcpy(descp, query->descriptor, len);\r\n}\r\nstatic inline void ufshcd_prepare_utp_nop_upiu(struct ufshcd_lrb *lrbp)\r\n{\r\nstruct utp_upiu_req *ucd_req_ptr = lrbp->ucd_req_ptr;\r\nmemset(ucd_req_ptr, 0, sizeof(struct utp_upiu_req));\r\nucd_req_ptr->header.dword_0 =\r\nUPIU_HEADER_DWORD(\r\nUPIU_TRANSACTION_NOP_OUT, 0, 0, lrbp->task_tag);\r\n}\r\nstatic int ufshcd_compose_upiu(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\r\n{\r\nu32 upiu_flags;\r\nint ret = 0;\r\nswitch (lrbp->command_type) {\r\ncase UTP_CMD_TYPE_SCSI:\r\nif (likely(lrbp->cmd)) {\r\nufshcd_prepare_req_desc_hdr(lrbp, &upiu_flags,\r\nlrbp->cmd->sc_data_direction);\r\nufshcd_prepare_utp_scsi_cmd_upiu(lrbp, upiu_flags);\r\n} else {\r\nret = -EINVAL;\r\n}\r\nbreak;\r\ncase UTP_CMD_TYPE_DEV_MANAGE:\r\nufshcd_prepare_req_desc_hdr(lrbp, &upiu_flags, DMA_NONE);\r\nif (hba->dev_cmd.type == DEV_CMD_TYPE_QUERY)\r\nufshcd_prepare_utp_query_req_upiu(\r\nhba, lrbp, upiu_flags);\r\nelse if (hba->dev_cmd.type == DEV_CMD_TYPE_NOP)\r\nufshcd_prepare_utp_nop_upiu(lrbp);\r\nelse\r\nret = -EINVAL;\r\nbreak;\r\ncase UTP_CMD_TYPE_UFS:\r\nret = -ENOTSUPP;\r\ndev_err(hba->dev, "%s: UFS native command are not supported\n",\r\n__func__);\r\nbreak;\r\ndefault:\r\nret = -ENOTSUPP;\r\ndev_err(hba->dev, "%s: unknown command type: 0x%x\n",\r\n__func__, lrbp->command_type);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic inline u8 ufshcd_scsi_to_upiu_lun(unsigned int scsi_lun)\r\n{\r\nif (scsi_is_wlun(scsi_lun))\r\nreturn (scsi_lun & UFS_UPIU_MAX_UNIT_NUM_ID)\r\n| UFS_UPIU_WLUN_ID;\r\nelse\r\nreturn scsi_lun & UFS_UPIU_MAX_UNIT_NUM_ID;\r\n}\r\nstatic inline u16 ufshcd_upiu_wlun_to_scsi_wlun(u8 upiu_wlun_id)\r\n{\r\nreturn (upiu_wlun_id & ~UFS_UPIU_WLUN_ID) | SCSI_W_LUN_BASE;\r\n}\r\nstatic int ufshcd_queuecommand(struct Scsi_Host *host, struct scsi_cmnd *cmd)\r\n{\r\nstruct ufshcd_lrb *lrbp;\r\nstruct ufs_hba *hba;\r\nunsigned long flags;\r\nint tag;\r\nint err = 0;\r\nhba = shost_priv(host);\r\ntag = cmd->request->tag;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nswitch (hba->ufshcd_state) {\r\ncase UFSHCD_STATE_OPERATIONAL:\r\nbreak;\r\ncase UFSHCD_STATE_RESET:\r\nerr = SCSI_MLQUEUE_HOST_BUSY;\r\ngoto out_unlock;\r\ncase UFSHCD_STATE_ERROR:\r\nset_host_byte(cmd, DID_ERROR);\r\ncmd->scsi_done(cmd);\r\ngoto out_unlock;\r\ndefault:\r\ndev_WARN_ONCE(hba->dev, 1, "%s: invalid state %d\n",\r\n__func__, hba->ufshcd_state);\r\nset_host_byte(cmd, DID_BAD_TARGET);\r\ncmd->scsi_done(cmd);\r\ngoto out_unlock;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (test_and_set_bit_lock(tag, &hba->lrb_in_use)) {\r\nerr = SCSI_MLQUEUE_HOST_BUSY;\r\ngoto out;\r\n}\r\nerr = ufshcd_hold(hba, true);\r\nif (err) {\r\nerr = SCSI_MLQUEUE_HOST_BUSY;\r\nclear_bit_unlock(tag, &hba->lrb_in_use);\r\ngoto out;\r\n}\r\nWARN_ON(hba->clk_gating.state != CLKS_ON);\r\nlrbp = &hba->lrb[tag];\r\nWARN_ON(lrbp->cmd);\r\nlrbp->cmd = cmd;\r\nlrbp->sense_bufflen = SCSI_SENSE_BUFFERSIZE;\r\nlrbp->sense_buffer = cmd->sense_buffer;\r\nlrbp->task_tag = tag;\r\nlrbp->lun = ufshcd_scsi_to_upiu_lun(cmd->device->lun);\r\nlrbp->intr_cmd = !ufshcd_is_intr_aggr_allowed(hba) ? true : false;\r\nlrbp->command_type = UTP_CMD_TYPE_SCSI;\r\nufshcd_compose_upiu(hba, lrbp);\r\nerr = ufshcd_map_sg(lrbp);\r\nif (err) {\r\nlrbp->cmd = NULL;\r\nclear_bit_unlock(tag, &hba->lrb_in_use);\r\ngoto out;\r\n}\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nufshcd_send_command(hba, tag);\r\nout_unlock:\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_compose_dev_cmd(struct ufs_hba *hba,\r\nstruct ufshcd_lrb *lrbp, enum dev_cmd_type cmd_type, int tag)\r\n{\r\nlrbp->cmd = NULL;\r\nlrbp->sense_bufflen = 0;\r\nlrbp->sense_buffer = NULL;\r\nlrbp->task_tag = tag;\r\nlrbp->lun = 0;\r\nlrbp->command_type = UTP_CMD_TYPE_DEV_MANAGE;\r\nlrbp->intr_cmd = true;\r\nhba->dev_cmd.type = cmd_type;\r\nreturn ufshcd_compose_upiu(hba, lrbp);\r\n}\r\nstatic int\r\nufshcd_clear_cmd(struct ufs_hba *hba, int tag)\r\n{\r\nint err = 0;\r\nunsigned long flags;\r\nu32 mask = 1 << tag;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nufshcd_utrl_clear(hba, tag);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nerr = ufshcd_wait_for_register(hba,\r\nREG_UTP_TRANSFER_REQ_DOOR_BELL,\r\nmask, ~mask, 1000, 1000);\r\nreturn err;\r\n}\r\nstatic int\r\nufshcd_check_query_response(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\r\n{\r\nstruct ufs_query_res *query_res = &hba->dev_cmd.query.response;\r\nquery_res->response = ufshcd_get_rsp_upiu_result(lrbp->ucd_rsp_ptr) >>\r\nUPIU_RSP_CODE_OFFSET;\r\nreturn query_res->response;\r\n}\r\nstatic int\r\nufshcd_dev_cmd_completion(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\r\n{\r\nint resp;\r\nint err = 0;\r\nresp = ufshcd_get_req_rsp(lrbp->ucd_rsp_ptr);\r\nswitch (resp) {\r\ncase UPIU_TRANSACTION_NOP_IN:\r\nif (hba->dev_cmd.type != DEV_CMD_TYPE_NOP) {\r\nerr = -EINVAL;\r\ndev_err(hba->dev, "%s: unexpected response %x\n",\r\n__func__, resp);\r\n}\r\nbreak;\r\ncase UPIU_TRANSACTION_QUERY_RSP:\r\nerr = ufshcd_check_query_response(hba, lrbp);\r\nif (!err)\r\nerr = ufshcd_copy_query_response(hba, lrbp);\r\nbreak;\r\ncase UPIU_TRANSACTION_REJECT_UPIU:\r\nerr = -EPERM;\r\ndev_err(hba->dev, "%s: Reject UPIU not fully implemented\n",\r\n__func__);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\ndev_err(hba->dev, "%s: Invalid device management cmd response: %x\n",\r\n__func__, resp);\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int ufshcd_wait_for_dev_cmd(struct ufs_hba *hba,\r\nstruct ufshcd_lrb *lrbp, int max_timeout)\r\n{\r\nint err = 0;\r\nunsigned long time_left;\r\nunsigned long flags;\r\ntime_left = wait_for_completion_timeout(hba->dev_cmd.complete,\r\nmsecs_to_jiffies(max_timeout));\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->dev_cmd.complete = NULL;\r\nif (likely(time_left)) {\r\nerr = ufshcd_get_tr_ocs(lrbp);\r\nif (!err)\r\nerr = ufshcd_dev_cmd_completion(hba, lrbp);\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (!time_left) {\r\nerr = -ETIMEDOUT;\r\nif (!ufshcd_clear_cmd(hba, lrbp->task_tag))\r\nerr = -EAGAIN;\r\n}\r\nreturn err;\r\n}\r\nstatic bool ufshcd_get_dev_cmd_tag(struct ufs_hba *hba, int *tag_out)\r\n{\r\nint tag;\r\nbool ret = false;\r\nunsigned long tmp;\r\nif (!tag_out)\r\ngoto out;\r\ndo {\r\ntmp = ~hba->lrb_in_use;\r\ntag = find_last_bit(&tmp, hba->nutrs);\r\nif (tag >= hba->nutrs)\r\ngoto out;\r\n} while (test_and_set_bit_lock(tag, &hba->lrb_in_use));\r\n*tag_out = tag;\r\nret = true;\r\nout:\r\nreturn ret;\r\n}\r\nstatic inline void ufshcd_put_dev_cmd_tag(struct ufs_hba *hba, int tag)\r\n{\r\nclear_bit_unlock(tag, &hba->lrb_in_use);\r\n}\r\nstatic int ufshcd_exec_dev_cmd(struct ufs_hba *hba,\r\nenum dev_cmd_type cmd_type, int timeout)\r\n{\r\nstruct ufshcd_lrb *lrbp;\r\nint err;\r\nint tag;\r\nstruct completion wait;\r\nunsigned long flags;\r\nwait_event(hba->dev_cmd.tag_wq, ufshcd_get_dev_cmd_tag(hba, &tag));\r\ninit_completion(&wait);\r\nlrbp = &hba->lrb[tag];\r\nWARN_ON(lrbp->cmd);\r\nerr = ufshcd_compose_dev_cmd(hba, lrbp, cmd_type, tag);\r\nif (unlikely(err))\r\ngoto out_put_tag;\r\nhba->dev_cmd.complete = &wait;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nufshcd_send_command(hba, tag);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nerr = ufshcd_wait_for_dev_cmd(hba, lrbp, timeout);\r\nout_put_tag:\r\nufshcd_put_dev_cmd_tag(hba, tag);\r\nwake_up(&hba->dev_cmd.tag_wq);\r\nreturn err;\r\n}\r\nstatic inline void ufshcd_init_query(struct ufs_hba *hba,\r\nstruct ufs_query_req **request, struct ufs_query_res **response,\r\nenum query_opcode opcode, u8 idn, u8 index, u8 selector)\r\n{\r\n*request = &hba->dev_cmd.query.request;\r\n*response = &hba->dev_cmd.query.response;\r\nmemset(*request, 0, sizeof(struct ufs_query_req));\r\nmemset(*response, 0, sizeof(struct ufs_query_res));\r\n(*request)->upiu_req.opcode = opcode;\r\n(*request)->upiu_req.idn = idn;\r\n(*request)->upiu_req.index = index;\r\n(*request)->upiu_req.selector = selector;\r\n}\r\nstatic int ufshcd_query_flag(struct ufs_hba *hba, enum query_opcode opcode,\r\nenum flag_idn idn, bool *flag_res)\r\n{\r\nstruct ufs_query_req *request = NULL;\r\nstruct ufs_query_res *response = NULL;\r\nint err, index = 0, selector = 0;\r\nBUG_ON(!hba);\r\nufshcd_hold(hba, false);\r\nmutex_lock(&hba->dev_cmd.lock);\r\nufshcd_init_query(hba, &request, &response, opcode, idn, index,\r\nselector);\r\nswitch (opcode) {\r\ncase UPIU_QUERY_OPCODE_SET_FLAG:\r\ncase UPIU_QUERY_OPCODE_CLEAR_FLAG:\r\ncase UPIU_QUERY_OPCODE_TOGGLE_FLAG:\r\nrequest->query_func = UPIU_QUERY_FUNC_STANDARD_WRITE_REQUEST;\r\nbreak;\r\ncase UPIU_QUERY_OPCODE_READ_FLAG:\r\nrequest->query_func = UPIU_QUERY_FUNC_STANDARD_READ_REQUEST;\r\nif (!flag_res) {\r\ndev_err(hba->dev, "%s: Invalid argument for read request\n",\r\n__func__);\r\nerr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nbreak;\r\ndefault:\r\ndev_err(hba->dev,\r\n"%s: Expected query flag opcode but got = %d\n",\r\n__func__, opcode);\r\nerr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nerr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, QUERY_REQ_TIMEOUT);\r\nif (err) {\r\ndev_err(hba->dev,\r\n"%s: Sending flag query for idn %d failed, err = %d\n",\r\n__func__, idn, err);\r\ngoto out_unlock;\r\n}\r\nif (flag_res)\r\n*flag_res = (be32_to_cpu(response->upiu_res.value) &\r\nMASK_QUERY_UPIU_FLAG_LOC) & 0x1;\r\nout_unlock:\r\nmutex_unlock(&hba->dev_cmd.lock);\r\nufshcd_release(hba);\r\nreturn err;\r\n}\r\nstatic int ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,\r\nenum attr_idn idn, u8 index, u8 selector, u32 *attr_val)\r\n{\r\nstruct ufs_query_req *request = NULL;\r\nstruct ufs_query_res *response = NULL;\r\nint err;\r\nBUG_ON(!hba);\r\nufshcd_hold(hba, false);\r\nif (!attr_val) {\r\ndev_err(hba->dev, "%s: attribute value required for opcode 0x%x\n",\r\n__func__, opcode);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nmutex_lock(&hba->dev_cmd.lock);\r\nufshcd_init_query(hba, &request, &response, opcode, idn, index,\r\nselector);\r\nswitch (opcode) {\r\ncase UPIU_QUERY_OPCODE_WRITE_ATTR:\r\nrequest->query_func = UPIU_QUERY_FUNC_STANDARD_WRITE_REQUEST;\r\nrequest->upiu_req.value = cpu_to_be32(*attr_val);\r\nbreak;\r\ncase UPIU_QUERY_OPCODE_READ_ATTR:\r\nrequest->query_func = UPIU_QUERY_FUNC_STANDARD_READ_REQUEST;\r\nbreak;\r\ndefault:\r\ndev_err(hba->dev, "%s: Expected query attr opcode but got = 0x%.2x\n",\r\n__func__, opcode);\r\nerr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nerr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, QUERY_REQ_TIMEOUT);\r\nif (err) {\r\ndev_err(hba->dev, "%s: opcode 0x%.2x for idn %d failed, err = %d\n",\r\n__func__, opcode, idn, err);\r\ngoto out_unlock;\r\n}\r\n*attr_val = be32_to_cpu(response->upiu_res.value);\r\nout_unlock:\r\nmutex_unlock(&hba->dev_cmd.lock);\r\nout:\r\nufshcd_release(hba);\r\nreturn err;\r\n}\r\nstatic int ufshcd_query_descriptor(struct ufs_hba *hba,\r\nenum query_opcode opcode, enum desc_idn idn, u8 index,\r\nu8 selector, u8 *desc_buf, int *buf_len)\r\n{\r\nstruct ufs_query_req *request = NULL;\r\nstruct ufs_query_res *response = NULL;\r\nint err;\r\nBUG_ON(!hba);\r\nufshcd_hold(hba, false);\r\nif (!desc_buf) {\r\ndev_err(hba->dev, "%s: descriptor buffer required for opcode 0x%x\n",\r\n__func__, opcode);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nif (*buf_len <= QUERY_DESC_MIN_SIZE || *buf_len > QUERY_DESC_MAX_SIZE) {\r\ndev_err(hba->dev, "%s: descriptor buffer size (%d) is out of range\n",\r\n__func__, *buf_len);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nmutex_lock(&hba->dev_cmd.lock);\r\nufshcd_init_query(hba, &request, &response, opcode, idn, index,\r\nselector);\r\nhba->dev_cmd.query.descriptor = desc_buf;\r\nrequest->upiu_req.length = cpu_to_be16(*buf_len);\r\nswitch (opcode) {\r\ncase UPIU_QUERY_OPCODE_WRITE_DESC:\r\nrequest->query_func = UPIU_QUERY_FUNC_STANDARD_WRITE_REQUEST;\r\nbreak;\r\ncase UPIU_QUERY_OPCODE_READ_DESC:\r\nrequest->query_func = UPIU_QUERY_FUNC_STANDARD_READ_REQUEST;\r\nbreak;\r\ndefault:\r\ndev_err(hba->dev,\r\n"%s: Expected query descriptor opcode but got = 0x%.2x\n",\r\n__func__, opcode);\r\nerr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nerr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, QUERY_REQ_TIMEOUT);\r\nif (err) {\r\ndev_err(hba->dev, "%s: opcode 0x%.2x for idn %d failed, err = %d\n",\r\n__func__, opcode, idn, err);\r\ngoto out_unlock;\r\n}\r\nhba->dev_cmd.query.descriptor = NULL;\r\n*buf_len = be16_to_cpu(response->upiu_res.length);\r\nout_unlock:\r\nmutex_unlock(&hba->dev_cmd.lock);\r\nout:\r\nufshcd_release(hba);\r\nreturn err;\r\n}\r\nstatic int ufshcd_read_desc_param(struct ufs_hba *hba,\r\nenum desc_idn desc_id,\r\nint desc_index,\r\nu32 param_offset,\r\nu8 *param_read_buf,\r\nu32 param_size)\r\n{\r\nint ret;\r\nu8 *desc_buf;\r\nu32 buff_len;\r\nbool is_kmalloc = true;\r\nif (desc_id >= QUERY_DESC_IDN_MAX)\r\nreturn -EINVAL;\r\nbuff_len = ufs_query_desc_max_size[desc_id];\r\nif ((param_offset + param_size) > buff_len)\r\nreturn -EINVAL;\r\nif (!param_offset && (param_size == buff_len)) {\r\ndesc_buf = param_read_buf;\r\nis_kmalloc = false;\r\n} else {\r\ndesc_buf = kmalloc(buff_len, GFP_KERNEL);\r\nif (!desc_buf)\r\nreturn -ENOMEM;\r\n}\r\nret = ufshcd_query_descriptor(hba, UPIU_QUERY_OPCODE_READ_DESC,\r\ndesc_id, desc_index, 0, desc_buf,\r\n&buff_len);\r\nif (ret || (buff_len < ufs_query_desc_max_size[desc_id]) ||\r\n(desc_buf[QUERY_DESC_LENGTH_OFFSET] !=\r\nufs_query_desc_max_size[desc_id])\r\n|| (desc_buf[QUERY_DESC_DESC_TYPE_OFFSET] != desc_id)) {\r\ndev_err(hba->dev, "%s: Failed reading descriptor. desc_id %d param_offset %d buff_len %d ret %d",\r\n__func__, desc_id, param_offset, buff_len, ret);\r\nif (!ret)\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (is_kmalloc)\r\nmemcpy(param_read_buf, &desc_buf[param_offset], param_size);\r\nout:\r\nif (is_kmalloc)\r\nkfree(desc_buf);\r\nreturn ret;\r\n}\r\nstatic inline int ufshcd_read_desc(struct ufs_hba *hba,\r\nenum desc_idn desc_id,\r\nint desc_index,\r\nu8 *buf,\r\nu32 size)\r\n{\r\nreturn ufshcd_read_desc_param(hba, desc_id, desc_index, 0, buf, size);\r\n}\r\nstatic inline int ufshcd_read_power_desc(struct ufs_hba *hba,\r\nu8 *buf,\r\nu32 size)\r\n{\r\nreturn ufshcd_read_desc(hba, QUERY_DESC_IDN_POWER, 0, buf, size);\r\n}\r\nstatic inline int ufshcd_read_unit_desc_param(struct ufs_hba *hba,\r\nint lun,\r\nenum unit_desc_param param_offset,\r\nu8 *param_read_buf,\r\nu32 param_size)\r\n{\r\nif (lun != UFS_UPIU_RPMB_WLUN && (lun >= UFS_UPIU_MAX_GENERAL_LUN))\r\nreturn -EOPNOTSUPP;\r\nreturn ufshcd_read_desc_param(hba, QUERY_DESC_IDN_UNIT, lun,\r\nparam_offset, param_read_buf, param_size);\r\n}\r\nstatic int ufshcd_memory_alloc(struct ufs_hba *hba)\r\n{\r\nsize_t utmrdl_size, utrdl_size, ucdl_size;\r\nucdl_size = (sizeof(struct utp_transfer_cmd_desc) * hba->nutrs);\r\nhba->ucdl_base_addr = dmam_alloc_coherent(hba->dev,\r\nucdl_size,\r\n&hba->ucdl_dma_addr,\r\nGFP_KERNEL);\r\nif (!hba->ucdl_base_addr ||\r\nWARN_ON(hba->ucdl_dma_addr & (PAGE_SIZE - 1))) {\r\ndev_err(hba->dev,\r\n"Command Descriptor Memory allocation failed\n");\r\ngoto out;\r\n}\r\nutrdl_size = (sizeof(struct utp_transfer_req_desc) * hba->nutrs);\r\nhba->utrdl_base_addr = dmam_alloc_coherent(hba->dev,\r\nutrdl_size,\r\n&hba->utrdl_dma_addr,\r\nGFP_KERNEL);\r\nif (!hba->utrdl_base_addr ||\r\nWARN_ON(hba->utrdl_dma_addr & (PAGE_SIZE - 1))) {\r\ndev_err(hba->dev,\r\n"Transfer Descriptor Memory allocation failed\n");\r\ngoto out;\r\n}\r\nutmrdl_size = sizeof(struct utp_task_req_desc) * hba->nutmrs;\r\nhba->utmrdl_base_addr = dmam_alloc_coherent(hba->dev,\r\nutmrdl_size,\r\n&hba->utmrdl_dma_addr,\r\nGFP_KERNEL);\r\nif (!hba->utmrdl_base_addr ||\r\nWARN_ON(hba->utmrdl_dma_addr & (PAGE_SIZE - 1))) {\r\ndev_err(hba->dev,\r\n"Task Management Descriptor Memory allocation failed\n");\r\ngoto out;\r\n}\r\nhba->lrb = devm_kzalloc(hba->dev,\r\nhba->nutrs * sizeof(struct ufshcd_lrb),\r\nGFP_KERNEL);\r\nif (!hba->lrb) {\r\ndev_err(hba->dev, "LRB Memory allocation failed\n");\r\ngoto out;\r\n}\r\nreturn 0;\r\nout:\r\nreturn -ENOMEM;\r\n}\r\nstatic void ufshcd_host_memory_configure(struct ufs_hba *hba)\r\n{\r\nstruct utp_transfer_cmd_desc *cmd_descp;\r\nstruct utp_transfer_req_desc *utrdlp;\r\ndma_addr_t cmd_desc_dma_addr;\r\ndma_addr_t cmd_desc_element_addr;\r\nu16 response_offset;\r\nu16 prdt_offset;\r\nint cmd_desc_size;\r\nint i;\r\nutrdlp = hba->utrdl_base_addr;\r\ncmd_descp = hba->ucdl_base_addr;\r\nresponse_offset =\r\noffsetof(struct utp_transfer_cmd_desc, response_upiu);\r\nprdt_offset =\r\noffsetof(struct utp_transfer_cmd_desc, prd_table);\r\ncmd_desc_size = sizeof(struct utp_transfer_cmd_desc);\r\ncmd_desc_dma_addr = hba->ucdl_dma_addr;\r\nfor (i = 0; i < hba->nutrs; i++) {\r\ncmd_desc_element_addr =\r\n(cmd_desc_dma_addr + (cmd_desc_size * i));\r\nutrdlp[i].command_desc_base_addr_lo =\r\ncpu_to_le32(lower_32_bits(cmd_desc_element_addr));\r\nutrdlp[i].command_desc_base_addr_hi =\r\ncpu_to_le32(upper_32_bits(cmd_desc_element_addr));\r\nutrdlp[i].response_upiu_offset =\r\ncpu_to_le16((response_offset >> 2));\r\nutrdlp[i].prd_table_offset =\r\ncpu_to_le16((prdt_offset >> 2));\r\nutrdlp[i].response_upiu_length =\r\ncpu_to_le16(ALIGNED_UPIU_SIZE >> 2);\r\nhba->lrb[i].utr_descriptor_ptr = (utrdlp + i);\r\nhba->lrb[i].ucd_req_ptr =\r\n(struct utp_upiu_req *)(cmd_descp + i);\r\nhba->lrb[i].ucd_rsp_ptr =\r\n(struct utp_upiu_rsp *)cmd_descp[i].response_upiu;\r\nhba->lrb[i].ucd_prdt_ptr =\r\n(struct ufshcd_sg_entry *)cmd_descp[i].prd_table;\r\n}\r\n}\r\nstatic int ufshcd_dme_link_startup(struct ufs_hba *hba)\r\n{\r\nstruct uic_command uic_cmd = {0};\r\nint ret;\r\nuic_cmd.command = UIC_CMD_DME_LINK_STARTUP;\r\nret = ufshcd_send_uic_cmd(hba, &uic_cmd);\r\nif (ret)\r\ndev_err(hba->dev,\r\n"dme-link-startup: error code %d\n", ret);\r\nreturn ret;\r\n}\r\nstatic inline void ufshcd_add_delay_before_dme_cmd(struct ufs_hba *hba)\r\n{\r\n#define MIN_DELAY_BEFORE_DME_CMDS_US 1000\r\nunsigned long min_sleep_time_us;\r\nif (!(hba->quirks & UFSHCD_QUIRK_DELAY_BEFORE_DME_CMDS))\r\nreturn;\r\nif (unlikely(!ktime_to_us(hba->last_dme_cmd_tstamp))) {\r\nmin_sleep_time_us = MIN_DELAY_BEFORE_DME_CMDS_US;\r\n} else {\r\nunsigned long delta =\r\n(unsigned long) ktime_to_us(\r\nktime_sub(ktime_get(),\r\nhba->last_dme_cmd_tstamp));\r\nif (delta < MIN_DELAY_BEFORE_DME_CMDS_US)\r\nmin_sleep_time_us =\r\nMIN_DELAY_BEFORE_DME_CMDS_US - delta;\r\nelse\r\nreturn;\r\n}\r\nusleep_range(min_sleep_time_us, min_sleep_time_us + 50);\r\n}\r\nint ufshcd_dme_set_attr(struct ufs_hba *hba, u32 attr_sel,\r\nu8 attr_set, u32 mib_val, u8 peer)\r\n{\r\nstruct uic_command uic_cmd = {0};\r\nstatic const char *const action[] = {\r\n"dme-set",\r\n"dme-peer-set"\r\n};\r\nconst char *set = action[!!peer];\r\nint ret;\r\nuic_cmd.command = peer ?\r\nUIC_CMD_DME_PEER_SET : UIC_CMD_DME_SET;\r\nuic_cmd.argument1 = attr_sel;\r\nuic_cmd.argument2 = UIC_ARG_ATTR_TYPE(attr_set);\r\nuic_cmd.argument3 = mib_val;\r\nret = ufshcd_send_uic_cmd(hba, &uic_cmd);\r\nif (ret)\r\ndev_err(hba->dev, "%s: attr-id 0x%x val 0x%x error code %d\n",\r\nset, UIC_GET_ATTR_ID(attr_sel), mib_val, ret);\r\nreturn ret;\r\n}\r\nint ufshcd_dme_get_attr(struct ufs_hba *hba, u32 attr_sel,\r\nu32 *mib_val, u8 peer)\r\n{\r\nstruct uic_command uic_cmd = {0};\r\nstatic const char *const action[] = {\r\n"dme-get",\r\n"dme-peer-get"\r\n};\r\nconst char *get = action[!!peer];\r\nint ret;\r\nstruct ufs_pa_layer_attr orig_pwr_info;\r\nstruct ufs_pa_layer_attr temp_pwr_info;\r\nbool pwr_mode_change = false;\r\nif (peer && (hba->quirks & UFSHCD_QUIRK_DME_PEER_ACCESS_AUTO_MODE)) {\r\norig_pwr_info = hba->pwr_info;\r\ntemp_pwr_info = orig_pwr_info;\r\nif (orig_pwr_info.pwr_tx == FAST_MODE ||\r\norig_pwr_info.pwr_rx == FAST_MODE) {\r\ntemp_pwr_info.pwr_tx = FASTAUTO_MODE;\r\ntemp_pwr_info.pwr_rx = FASTAUTO_MODE;\r\npwr_mode_change = true;\r\n} else if (orig_pwr_info.pwr_tx == SLOW_MODE ||\r\norig_pwr_info.pwr_rx == SLOW_MODE) {\r\ntemp_pwr_info.pwr_tx = SLOWAUTO_MODE;\r\ntemp_pwr_info.pwr_rx = SLOWAUTO_MODE;\r\npwr_mode_change = true;\r\n}\r\nif (pwr_mode_change) {\r\nret = ufshcd_change_power_mode(hba, &temp_pwr_info);\r\nif (ret)\r\ngoto out;\r\n}\r\n}\r\nuic_cmd.command = peer ?\r\nUIC_CMD_DME_PEER_GET : UIC_CMD_DME_GET;\r\nuic_cmd.argument1 = attr_sel;\r\nret = ufshcd_send_uic_cmd(hba, &uic_cmd);\r\nif (ret) {\r\ndev_err(hba->dev, "%s: attr-id 0x%x error code %d\n",\r\nget, UIC_GET_ATTR_ID(attr_sel), ret);\r\ngoto out;\r\n}\r\nif (mib_val)\r\n*mib_val = uic_cmd.argument3;\r\nif (peer && (hba->quirks & UFSHCD_QUIRK_DME_PEER_ACCESS_AUTO_MODE)\r\n&& pwr_mode_change)\r\nufshcd_change_power_mode(hba, &orig_pwr_info);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_uic_pwr_ctrl(struct ufs_hba *hba, struct uic_command *cmd)\r\n{\r\nstruct completion uic_async_done;\r\nunsigned long flags;\r\nu8 status;\r\nint ret;\r\nmutex_lock(&hba->uic_cmd_mutex);\r\ninit_completion(&uic_async_done);\r\nufshcd_add_delay_before_dme_cmd(hba);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->uic_async_done = &uic_async_done;\r\nret = __ufshcd_send_uic_cmd(hba, cmd);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (ret) {\r\ndev_err(hba->dev,\r\n"pwr ctrl cmd 0x%x with mode 0x%x uic error %d\n",\r\ncmd->command, cmd->argument3, ret);\r\ngoto out;\r\n}\r\nret = ufshcd_wait_for_uic_cmd(hba, cmd);\r\nif (ret) {\r\ndev_err(hba->dev,\r\n"pwr ctrl cmd 0x%x with mode 0x%x uic error %d\n",\r\ncmd->command, cmd->argument3, ret);\r\ngoto out;\r\n}\r\nif (!wait_for_completion_timeout(hba->uic_async_done,\r\nmsecs_to_jiffies(UIC_CMD_TIMEOUT))) {\r\ndev_err(hba->dev,\r\n"pwr ctrl cmd 0x%x with mode 0x%x completion timeout\n",\r\ncmd->command, cmd->argument3);\r\nret = -ETIMEDOUT;\r\ngoto out;\r\n}\r\nstatus = ufshcd_get_upmcrs(hba);\r\nif (status != PWR_LOCAL) {\r\ndev_err(hba->dev,\r\n"pwr ctrl cmd 0x%0x failed, host umpcrs:0x%x\n",\r\ncmd->command, status);\r\nret = (status != PWR_OK) ? status : -1;\r\n}\r\nout:\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->uic_async_done = NULL;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nmutex_unlock(&hba->uic_cmd_mutex);\r\nreturn ret;\r\n}\r\nstatic int ufshcd_uic_change_pwr_mode(struct ufs_hba *hba, u8 mode)\r\n{\r\nstruct uic_command uic_cmd = {0};\r\nint ret;\r\nif (hba->quirks & UFSHCD_QUIRK_BROKEN_PA_RXHSUNTERMCAP) {\r\nret = ufshcd_dme_set(hba,\r\nUIC_ARG_MIB_SEL(PA_RXHSUNTERMCAP, 0), 1);\r\nif (ret) {\r\ndev_err(hba->dev, "%s: failed to enable PA_RXHSUNTERMCAP ret %d\n",\r\n__func__, ret);\r\ngoto out;\r\n}\r\n}\r\nuic_cmd.command = UIC_CMD_DME_SET;\r\nuic_cmd.argument1 = UIC_ARG_MIB(PA_PWRMODE);\r\nuic_cmd.argument3 = mode;\r\nufshcd_hold(hba, false);\r\nret = ufshcd_uic_pwr_ctrl(hba, &uic_cmd);\r\nufshcd_release(hba);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_uic_hibern8_enter(struct ufs_hba *hba)\r\n{\r\nstruct uic_command uic_cmd = {0};\r\nuic_cmd.command = UIC_CMD_DME_HIBER_ENTER;\r\nreturn ufshcd_uic_pwr_ctrl(hba, &uic_cmd);\r\n}\r\nstatic int ufshcd_uic_hibern8_exit(struct ufs_hba *hba)\r\n{\r\nstruct uic_command uic_cmd = {0};\r\nint ret;\r\nuic_cmd.command = UIC_CMD_DME_HIBER_EXIT;\r\nret = ufshcd_uic_pwr_ctrl(hba, &uic_cmd);\r\nif (ret) {\r\nufshcd_set_link_off(hba);\r\nret = ufshcd_host_reset_and_restore(hba);\r\n}\r\nreturn ret;\r\n}\r\nstatic void ufshcd_init_pwr_info(struct ufs_hba *hba)\r\n{\r\nhba->pwr_info.gear_rx = UFS_PWM_G1;\r\nhba->pwr_info.gear_tx = UFS_PWM_G1;\r\nhba->pwr_info.lane_rx = 1;\r\nhba->pwr_info.lane_tx = 1;\r\nhba->pwr_info.pwr_rx = SLOWAUTO_MODE;\r\nhba->pwr_info.pwr_tx = SLOWAUTO_MODE;\r\nhba->pwr_info.hs_rate = 0;\r\n}\r\nstatic int ufshcd_get_max_pwr_mode(struct ufs_hba *hba)\r\n{\r\nstruct ufs_pa_layer_attr *pwr_info = &hba->max_pwr_info.info;\r\nif (hba->max_pwr_info.is_valid)\r\nreturn 0;\r\npwr_info->pwr_tx = FASTAUTO_MODE;\r\npwr_info->pwr_rx = FASTAUTO_MODE;\r\npwr_info->hs_rate = PA_HS_MODE_B;\r\nufshcd_dme_get(hba, UIC_ARG_MIB(PA_CONNECTEDRXDATALANES),\r\n&pwr_info->lane_rx);\r\nufshcd_dme_get(hba, UIC_ARG_MIB(PA_CONNECTEDTXDATALANES),\r\n&pwr_info->lane_tx);\r\nif (!pwr_info->lane_rx || !pwr_info->lane_tx) {\r\ndev_err(hba->dev, "%s: invalid connected lanes value. rx=%d, tx=%d\n",\r\n__func__,\r\npwr_info->lane_rx,\r\npwr_info->lane_tx);\r\nreturn -EINVAL;\r\n}\r\nufshcd_dme_get(hba, UIC_ARG_MIB(PA_MAXRXHSGEAR), &pwr_info->gear_rx);\r\nif (!pwr_info->gear_rx) {\r\nufshcd_dme_get(hba, UIC_ARG_MIB(PA_MAXRXPWMGEAR),\r\n&pwr_info->gear_rx);\r\nif (!pwr_info->gear_rx) {\r\ndev_err(hba->dev, "%s: invalid max pwm rx gear read = %d\n",\r\n__func__, pwr_info->gear_rx);\r\nreturn -EINVAL;\r\n}\r\npwr_info->pwr_rx = SLOWAUTO_MODE;\r\n}\r\nufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_MAXRXHSGEAR),\r\n&pwr_info->gear_tx);\r\nif (!pwr_info->gear_tx) {\r\nufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_MAXRXPWMGEAR),\r\n&pwr_info->gear_tx);\r\nif (!pwr_info->gear_tx) {\r\ndev_err(hba->dev, "%s: invalid max pwm tx gear read = %d\n",\r\n__func__, pwr_info->gear_tx);\r\nreturn -EINVAL;\r\n}\r\npwr_info->pwr_tx = SLOWAUTO_MODE;\r\n}\r\nhba->max_pwr_info.is_valid = true;\r\nreturn 0;\r\n}\r\nstatic int ufshcd_change_power_mode(struct ufs_hba *hba,\r\nstruct ufs_pa_layer_attr *pwr_mode)\r\n{\r\nint ret;\r\nif (pwr_mode->gear_rx == hba->pwr_info.gear_rx &&\r\npwr_mode->gear_tx == hba->pwr_info.gear_tx &&\r\npwr_mode->lane_rx == hba->pwr_info.lane_rx &&\r\npwr_mode->lane_tx == hba->pwr_info.lane_tx &&\r\npwr_mode->pwr_rx == hba->pwr_info.pwr_rx &&\r\npwr_mode->pwr_tx == hba->pwr_info.pwr_tx &&\r\npwr_mode->hs_rate == hba->pwr_info.hs_rate) {\r\ndev_dbg(hba->dev, "%s: power already configured\n", __func__);\r\nreturn 0;\r\n}\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXGEAR), pwr_mode->gear_rx);\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_ACTIVERXDATALANES),\r\npwr_mode->lane_rx);\r\nif (pwr_mode->pwr_rx == FASTAUTO_MODE ||\r\npwr_mode->pwr_rx == FAST_MODE)\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXTERMINATION), TRUE);\r\nelse\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_RXTERMINATION), FALSE);\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXGEAR), pwr_mode->gear_tx);\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_ACTIVETXDATALANES),\r\npwr_mode->lane_tx);\r\nif (pwr_mode->pwr_tx == FASTAUTO_MODE ||\r\npwr_mode->pwr_tx == FAST_MODE)\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXTERMINATION), TRUE);\r\nelse\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_TXTERMINATION), FALSE);\r\nif (pwr_mode->pwr_rx == FASTAUTO_MODE ||\r\npwr_mode->pwr_tx == FASTAUTO_MODE ||\r\npwr_mode->pwr_rx == FAST_MODE ||\r\npwr_mode->pwr_tx == FAST_MODE)\r\nufshcd_dme_set(hba, UIC_ARG_MIB(PA_HSSERIES),\r\npwr_mode->hs_rate);\r\nret = ufshcd_uic_change_pwr_mode(hba, pwr_mode->pwr_rx << 4\r\n| pwr_mode->pwr_tx);\r\nif (ret) {\r\ndev_err(hba->dev,\r\n"%s: power mode change failed %d\n", __func__, ret);\r\n} else {\r\nufshcd_vops_pwr_change_notify(hba, POST_CHANGE, NULL,\r\npwr_mode);\r\nmemcpy(&hba->pwr_info, pwr_mode,\r\nsizeof(struct ufs_pa_layer_attr));\r\n}\r\nreturn ret;\r\n}\r\nstatic int ufshcd_config_pwr_mode(struct ufs_hba *hba,\r\nstruct ufs_pa_layer_attr *desired_pwr_mode)\r\n{\r\nstruct ufs_pa_layer_attr final_params = { 0 };\r\nint ret;\r\nret = ufshcd_vops_pwr_change_notify(hba, PRE_CHANGE,\r\ndesired_pwr_mode, &final_params);\r\nif (ret)\r\nmemcpy(&final_params, desired_pwr_mode, sizeof(final_params));\r\nret = ufshcd_change_power_mode(hba, &final_params);\r\nreturn ret;\r\n}\r\nstatic int ufshcd_complete_dev_init(struct ufs_hba *hba)\r\n{\r\nint i, retries, err = 0;\r\nbool flag_res = 1;\r\nfor (retries = QUERY_REQ_RETRIES; retries > 0; retries--) {\r\nerr = ufshcd_query_flag(hba, UPIU_QUERY_OPCODE_SET_FLAG,\r\nQUERY_FLAG_IDN_FDEVICEINIT, NULL);\r\nif (!err || err == -ETIMEDOUT)\r\nbreak;\r\ndev_dbg(hba->dev, "%s: error %d retrying\n", __func__, err);\r\n}\r\nif (err) {\r\ndev_err(hba->dev,\r\n"%s setting fDeviceInit flag failed with error %d\n",\r\n__func__, err);\r\ngoto out;\r\n}\r\nfor (i = 0; i < 100 && !err && flag_res; i++) {\r\nfor (retries = QUERY_REQ_RETRIES; retries > 0; retries--) {\r\nerr = ufshcd_query_flag(hba,\r\nUPIU_QUERY_OPCODE_READ_FLAG,\r\nQUERY_FLAG_IDN_FDEVICEINIT, &flag_res);\r\nif (!err || err == -ETIMEDOUT)\r\nbreak;\r\ndev_dbg(hba->dev, "%s: error %d retrying\n", __func__,\r\nerr);\r\n}\r\n}\r\nif (err)\r\ndev_err(hba->dev,\r\n"%s reading fDeviceInit flag failed with error %d\n",\r\n__func__, err);\r\nelse if (flag_res)\r\ndev_err(hba->dev,\r\n"%s fDeviceInit was not cleared by the device\n",\r\n__func__);\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_make_hba_operational(struct ufs_hba *hba)\r\n{\r\nint err = 0;\r\nu32 reg;\r\nufshcd_enable_intr(hba, UFSHCD_ENABLE_INTRS);\r\nif (ufshcd_is_intr_aggr_allowed(hba))\r\nufshcd_config_intr_aggr(hba, hba->nutrs - 1, INT_AGGR_DEF_TO);\r\nelse\r\nufshcd_disable_intr_aggr(hba);\r\nufshcd_writel(hba, lower_32_bits(hba->utrdl_dma_addr),\r\nREG_UTP_TRANSFER_REQ_LIST_BASE_L);\r\nufshcd_writel(hba, upper_32_bits(hba->utrdl_dma_addr),\r\nREG_UTP_TRANSFER_REQ_LIST_BASE_H);\r\nufshcd_writel(hba, lower_32_bits(hba->utmrdl_dma_addr),\r\nREG_UTP_TASK_REQ_LIST_BASE_L);\r\nufshcd_writel(hba, upper_32_bits(hba->utmrdl_dma_addr),\r\nREG_UTP_TASK_REQ_LIST_BASE_H);\r\nreg = ufshcd_readl(hba, REG_CONTROLLER_STATUS);\r\nif (!(ufshcd_get_lists_status(reg))) {\r\nufshcd_enable_run_stop_reg(hba);\r\n} else {\r\ndev_err(hba->dev,\r\n"Host controller not ready to process requests");\r\nerr = -EIO;\r\ngoto out;\r\n}\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_hba_enable(struct ufs_hba *hba)\r\n{\r\nint retry;\r\nif (!ufshcd_is_hba_active(hba)) {\r\nufshcd_hba_stop(hba);\r\nmsleep(5);\r\n}\r\nufshcd_set_link_off(hba);\r\nufshcd_vops_hce_enable_notify(hba, PRE_CHANGE);\r\nufshcd_hba_start(hba);\r\nmsleep(1);\r\nretry = 10;\r\nwhile (ufshcd_is_hba_active(hba)) {\r\nif (retry) {\r\nretry--;\r\n} else {\r\ndev_err(hba->dev,\r\n"Controller enable failed\n");\r\nreturn -EIO;\r\n}\r\nmsleep(5);\r\n}\r\nufshcd_enable_intr(hba, UFSHCD_UIC_MASK);\r\nufshcd_vops_hce_enable_notify(hba, POST_CHANGE);\r\nreturn 0;\r\n}\r\nstatic int ufshcd_disable_tx_lcc(struct ufs_hba *hba, bool peer)\r\n{\r\nint tx_lanes, i, err = 0;\r\nif (!peer)\r\nufshcd_dme_get(hba, UIC_ARG_MIB(PA_CONNECTEDTXDATALANES),\r\n&tx_lanes);\r\nelse\r\nufshcd_dme_peer_get(hba, UIC_ARG_MIB(PA_CONNECTEDTXDATALANES),\r\n&tx_lanes);\r\nfor (i = 0; i < tx_lanes; i++) {\r\nif (!peer)\r\nerr = ufshcd_dme_set(hba,\r\nUIC_ARG_MIB_SEL(TX_LCC_ENABLE,\r\nUIC_ARG_MPHY_TX_GEN_SEL_INDEX(i)),\r\n0);\r\nelse\r\nerr = ufshcd_dme_peer_set(hba,\r\nUIC_ARG_MIB_SEL(TX_LCC_ENABLE,\r\nUIC_ARG_MPHY_TX_GEN_SEL_INDEX(i)),\r\n0);\r\nif (err) {\r\ndev_err(hba->dev, "%s: TX LCC Disable failed, peer = %d, lane = %d, err = %d",\r\n__func__, peer, i, err);\r\nbreak;\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic inline int ufshcd_disable_device_tx_lcc(struct ufs_hba *hba)\r\n{\r\nreturn ufshcd_disable_tx_lcc(hba, true);\r\n}\r\nstatic int ufshcd_link_startup(struct ufs_hba *hba)\r\n{\r\nint ret;\r\nint retries = DME_LINKSTARTUP_RETRIES;\r\ndo {\r\nufshcd_vops_link_startup_notify(hba, PRE_CHANGE);\r\nret = ufshcd_dme_link_startup(hba);\r\nif (!ret && !ufshcd_is_device_present(hba)) {\r\ndev_err(hba->dev, "%s: Device not present\n", __func__);\r\nret = -ENXIO;\r\ngoto out;\r\n}\r\nif (ret && ufshcd_hba_enable(hba))\r\ngoto out;\r\n} while (ret && retries--);\r\nif (ret)\r\ngoto out;\r\nif (hba->quirks & UFSHCD_QUIRK_BROKEN_LCC) {\r\nret = ufshcd_disable_device_tx_lcc(hba);\r\nif (ret)\r\ngoto out;\r\n}\r\nret = ufshcd_vops_link_startup_notify(hba, POST_CHANGE);\r\nif (ret)\r\ngoto out;\r\nret = ufshcd_make_hba_operational(hba);\r\nout:\r\nif (ret)\r\ndev_err(hba->dev, "link startup failed %d\n", ret);\r\nreturn ret;\r\n}\r\nstatic int ufshcd_verify_dev_init(struct ufs_hba *hba)\r\n{\r\nint err = 0;\r\nint retries;\r\nufshcd_hold(hba, false);\r\nmutex_lock(&hba->dev_cmd.lock);\r\nfor (retries = NOP_OUT_RETRIES; retries > 0; retries--) {\r\nerr = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_NOP,\r\nNOP_OUT_TIMEOUT);\r\nif (!err || err == -ETIMEDOUT)\r\nbreak;\r\ndev_dbg(hba->dev, "%s: error %d retrying\n", __func__, err);\r\n}\r\nmutex_unlock(&hba->dev_cmd.lock);\r\nufshcd_release(hba);\r\nif (err)\r\ndev_err(hba->dev, "%s: NOP OUT failed %d\n", __func__, err);\r\nreturn err;\r\n}\r\nstatic void ufshcd_set_queue_depth(struct scsi_device *sdev)\r\n{\r\nint ret = 0;\r\nu8 lun_qdepth;\r\nstruct ufs_hba *hba;\r\nhba = shost_priv(sdev->host);\r\nlun_qdepth = hba->nutrs;\r\nret = ufshcd_read_unit_desc_param(hba,\r\nufshcd_scsi_to_upiu_lun(sdev->lun),\r\nUNIT_DESC_PARAM_LU_Q_DEPTH,\r\n&lun_qdepth,\r\nsizeof(lun_qdepth));\r\nif (ret == -EOPNOTSUPP)\r\nlun_qdepth = 1;\r\nelse if (!lun_qdepth)\r\nlun_qdepth = hba->nutrs;\r\nelse\r\nlun_qdepth = min_t(int, lun_qdepth, hba->nutrs);\r\ndev_dbg(hba->dev, "%s: activate tcq with queue depth %d\n",\r\n__func__, lun_qdepth);\r\nscsi_change_queue_depth(sdev, lun_qdepth);\r\n}\r\nstatic int ufshcd_get_lu_wp(struct ufs_hba *hba,\r\nu8 lun,\r\nu8 *b_lu_write_protect)\r\n{\r\nint ret;\r\nif (!b_lu_write_protect)\r\nret = -EINVAL;\r\nelse if (lun >= UFS_UPIU_MAX_GENERAL_LUN)\r\nret = -ENOTSUPP;\r\nelse\r\nret = ufshcd_read_unit_desc_param(hba,\r\nlun,\r\nUNIT_DESC_PARAM_LU_WR_PROTECT,\r\nb_lu_write_protect,\r\nsizeof(*b_lu_write_protect));\r\nreturn ret;\r\n}\r\nstatic inline void ufshcd_get_lu_power_on_wp_status(struct ufs_hba *hba,\r\nstruct scsi_device *sdev)\r\n{\r\nif (hba->dev_info.f_power_on_wp_en &&\r\n!hba->dev_info.is_lu_power_on_wp) {\r\nu8 b_lu_write_protect;\r\nif (!ufshcd_get_lu_wp(hba, ufshcd_scsi_to_upiu_lun(sdev->lun),\r\n&b_lu_write_protect) &&\r\n(b_lu_write_protect == UFS_LU_POWER_ON_WP))\r\nhba->dev_info.is_lu_power_on_wp = true;\r\n}\r\n}\r\nstatic int ufshcd_slave_alloc(struct scsi_device *sdev)\r\n{\r\nstruct ufs_hba *hba;\r\nhba = shost_priv(sdev->host);\r\nsdev->use_10_for_ms = 1;\r\nsdev->allow_restart = 1;\r\nsdev->no_report_opcodes = 1;\r\nufshcd_set_queue_depth(sdev);\r\nufshcd_get_lu_power_on_wp_status(hba, sdev);\r\nreturn 0;\r\n}\r\nstatic int ufshcd_change_queue_depth(struct scsi_device *sdev, int depth)\r\n{\r\nstruct ufs_hba *hba = shost_priv(sdev->host);\r\nif (depth > hba->nutrs)\r\ndepth = hba->nutrs;\r\nreturn scsi_change_queue_depth(sdev, depth);\r\n}\r\nstatic int ufshcd_slave_configure(struct scsi_device *sdev)\r\n{\r\nstruct request_queue *q = sdev->request_queue;\r\nblk_queue_update_dma_pad(q, PRDT_DATA_BYTE_COUNT_PAD - 1);\r\nblk_queue_max_segment_size(q, PRDT_DATA_BYTE_COUNT_MAX);\r\nreturn 0;\r\n}\r\nstatic void ufshcd_slave_destroy(struct scsi_device *sdev)\r\n{\r\nstruct ufs_hba *hba;\r\nhba = shost_priv(sdev->host);\r\nif (ufshcd_scsi_to_upiu_lun(sdev->lun) == UFS_UPIU_UFS_DEVICE_WLUN) {\r\nunsigned long flags;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->sdev_ufs_device = NULL;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\n}\r\n}\r\nstatic int ufshcd_task_req_compl(struct ufs_hba *hba, u32 index, u8 *resp)\r\n{\r\nstruct utp_task_req_desc *task_req_descp;\r\nstruct utp_upiu_task_rsp *task_rsp_upiup;\r\nunsigned long flags;\r\nint ocs_value;\r\nint task_result;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\n__clear_bit(index, &hba->outstanding_tasks);\r\ntask_req_descp = hba->utmrdl_base_addr;\r\nocs_value = ufshcd_get_tmr_ocs(&task_req_descp[index]);\r\nif (ocs_value == OCS_SUCCESS) {\r\ntask_rsp_upiup = (struct utp_upiu_task_rsp *)\r\ntask_req_descp[index].task_rsp_upiu;\r\ntask_result = be32_to_cpu(task_rsp_upiup->header.dword_1);\r\ntask_result = ((task_result & MASK_TASK_RESPONSE) >> 8);\r\nif (resp)\r\n*resp = (u8)task_result;\r\n} else {\r\ndev_err(hba->dev, "%s: failed, ocs = 0x%x\n",\r\n__func__, ocs_value);\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn ocs_value;\r\n}\r\nstatic inline int\r\nufshcd_scsi_cmd_status(struct ufshcd_lrb *lrbp, int scsi_status)\r\n{\r\nint result = 0;\r\nswitch (scsi_status) {\r\ncase SAM_STAT_CHECK_CONDITION:\r\nufshcd_copy_sense_data(lrbp);\r\ncase SAM_STAT_GOOD:\r\nresult |= DID_OK << 16 |\r\nCOMMAND_COMPLETE << 8 |\r\nscsi_status;\r\nbreak;\r\ncase SAM_STAT_TASK_SET_FULL:\r\ncase SAM_STAT_BUSY:\r\ncase SAM_STAT_TASK_ABORTED:\r\nufshcd_copy_sense_data(lrbp);\r\nresult |= scsi_status;\r\nbreak;\r\ndefault:\r\nresult |= DID_ERROR << 16;\r\nbreak;\r\n}\r\nreturn result;\r\n}\r\nstatic inline int\r\nufshcd_transfer_rsp_status(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)\r\n{\r\nint result = 0;\r\nint scsi_status;\r\nint ocs;\r\nocs = ufshcd_get_tr_ocs(lrbp);\r\nswitch (ocs) {\r\ncase OCS_SUCCESS:\r\nresult = ufshcd_get_req_rsp(lrbp->ucd_rsp_ptr);\r\nswitch (result) {\r\ncase UPIU_TRANSACTION_RESPONSE:\r\nresult = ufshcd_get_rsp_upiu_result(lrbp->ucd_rsp_ptr);\r\nscsi_status = result & MASK_SCSI_STATUS;\r\nresult = ufshcd_scsi_cmd_status(lrbp, scsi_status);\r\nif (ufshcd_is_exception_event(lrbp->ucd_rsp_ptr))\r\nschedule_work(&hba->eeh_work);\r\nbreak;\r\ncase UPIU_TRANSACTION_REJECT_UPIU:\r\nresult = DID_ERROR << 16;\r\ndev_err(hba->dev,\r\n"Reject UPIU not fully implemented\n");\r\nbreak;\r\ndefault:\r\nresult = DID_ERROR << 16;\r\ndev_err(hba->dev,\r\n"Unexpected request response code = %x\n",\r\nresult);\r\nbreak;\r\n}\r\nbreak;\r\ncase OCS_ABORTED:\r\nresult |= DID_ABORT << 16;\r\nbreak;\r\ncase OCS_INVALID_COMMAND_STATUS:\r\nresult |= DID_REQUEUE << 16;\r\nbreak;\r\ncase OCS_INVALID_CMD_TABLE_ATTR:\r\ncase OCS_INVALID_PRDT_ATTR:\r\ncase OCS_MISMATCH_DATA_BUF_SIZE:\r\ncase OCS_MISMATCH_RESP_UPIU_SIZE:\r\ncase OCS_PEER_COMM_FAILURE:\r\ncase OCS_FATAL_ERROR:\r\ndefault:\r\nresult |= DID_ERROR << 16;\r\ndev_err(hba->dev,\r\n"OCS error from controller = %x\n", ocs);\r\nbreak;\r\n}\r\nreturn result;\r\n}\r\nstatic void ufshcd_uic_cmd_compl(struct ufs_hba *hba, u32 intr_status)\r\n{\r\nif ((intr_status & UIC_COMMAND_COMPL) && hba->active_uic_cmd) {\r\nhba->active_uic_cmd->argument2 |=\r\nufshcd_get_uic_cmd_result(hba);\r\nhba->active_uic_cmd->argument3 =\r\nufshcd_get_dme_attr_val(hba);\r\ncomplete(&hba->active_uic_cmd->done);\r\n}\r\nif ((intr_status & UFSHCD_UIC_PWR_MASK) && hba->uic_async_done)\r\ncomplete(hba->uic_async_done);\r\n}\r\nstatic void ufshcd_transfer_req_compl(struct ufs_hba *hba)\r\n{\r\nstruct ufshcd_lrb *lrbp;\r\nstruct scsi_cmnd *cmd;\r\nunsigned long completed_reqs;\r\nu32 tr_doorbell;\r\nint result;\r\nint index;\r\nif (ufshcd_is_intr_aggr_allowed(hba))\r\nufshcd_reset_intr_aggr(hba);\r\ntr_doorbell = ufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\r\ncompleted_reqs = tr_doorbell ^ hba->outstanding_reqs;\r\nfor_each_set_bit(index, &completed_reqs, hba->nutrs) {\r\nlrbp = &hba->lrb[index];\r\ncmd = lrbp->cmd;\r\nif (cmd) {\r\nresult = ufshcd_transfer_rsp_status(hba, lrbp);\r\nscsi_dma_unmap(cmd);\r\ncmd->result = result;\r\nlrbp->cmd = NULL;\r\nclear_bit_unlock(index, &hba->lrb_in_use);\r\ncmd->scsi_done(cmd);\r\n__ufshcd_release(hba);\r\n} else if (lrbp->command_type == UTP_CMD_TYPE_DEV_MANAGE) {\r\nif (hba->dev_cmd.complete)\r\ncomplete(hba->dev_cmd.complete);\r\n}\r\n}\r\nhba->outstanding_reqs ^= completed_reqs;\r\nufshcd_clk_scaling_update_busy(hba);\r\nwake_up(&hba->dev_cmd.tag_wq);\r\n}\r\nstatic int ufshcd_disable_ee(struct ufs_hba *hba, u16 mask)\r\n{\r\nint err = 0;\r\nu32 val;\r\nif (!(hba->ee_ctrl_mask & mask))\r\ngoto out;\r\nval = hba->ee_ctrl_mask & ~mask;\r\nval &= 0xFFFF;\r\nerr = ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_WRITE_ATTR,\r\nQUERY_ATTR_IDN_EE_CONTROL, 0, 0, &val);\r\nif (!err)\r\nhba->ee_ctrl_mask &= ~mask;\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_enable_ee(struct ufs_hba *hba, u16 mask)\r\n{\r\nint err = 0;\r\nu32 val;\r\nif (hba->ee_ctrl_mask & mask)\r\ngoto out;\r\nval = hba->ee_ctrl_mask | mask;\r\nval &= 0xFFFF;\r\nerr = ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_WRITE_ATTR,\r\nQUERY_ATTR_IDN_EE_CONTROL, 0, 0, &val);\r\nif (!err)\r\nhba->ee_ctrl_mask |= mask;\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_enable_auto_bkops(struct ufs_hba *hba)\r\n{\r\nint err = 0;\r\nif (hba->auto_bkops_enabled)\r\ngoto out;\r\nerr = ufshcd_query_flag(hba, UPIU_QUERY_OPCODE_SET_FLAG,\r\nQUERY_FLAG_IDN_BKOPS_EN, NULL);\r\nif (err) {\r\ndev_err(hba->dev, "%s: failed to enable bkops %d\n",\r\n__func__, err);\r\ngoto out;\r\n}\r\nhba->auto_bkops_enabled = true;\r\nerr = ufshcd_disable_ee(hba, MASK_EE_URGENT_BKOPS);\r\nif (err)\r\ndev_err(hba->dev, "%s: failed to disable exception event %d\n",\r\n__func__, err);\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_disable_auto_bkops(struct ufs_hba *hba)\r\n{\r\nint err = 0;\r\nif (!hba->auto_bkops_enabled)\r\ngoto out;\r\nerr = ufshcd_enable_ee(hba, MASK_EE_URGENT_BKOPS);\r\nif (err) {\r\ndev_err(hba->dev, "%s: failed to enable exception event %d\n",\r\n__func__, err);\r\ngoto out;\r\n}\r\nerr = ufshcd_query_flag(hba, UPIU_QUERY_OPCODE_CLEAR_FLAG,\r\nQUERY_FLAG_IDN_BKOPS_EN, NULL);\r\nif (err) {\r\ndev_err(hba->dev, "%s: failed to disable bkops %d\n",\r\n__func__, err);\r\nufshcd_disable_ee(hba, MASK_EE_URGENT_BKOPS);\r\ngoto out;\r\n}\r\nhba->auto_bkops_enabled = false;\r\nout:\r\nreturn err;\r\n}\r\nstatic void ufshcd_force_reset_auto_bkops(struct ufs_hba *hba)\r\n{\r\nhba->auto_bkops_enabled = false;\r\nhba->ee_ctrl_mask |= MASK_EE_URGENT_BKOPS;\r\nufshcd_enable_auto_bkops(hba);\r\n}\r\nstatic inline int ufshcd_get_bkops_status(struct ufs_hba *hba, u32 *status)\r\n{\r\nreturn ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_READ_ATTR,\r\nQUERY_ATTR_IDN_BKOPS_STATUS, 0, 0, status);\r\n}\r\nstatic int ufshcd_bkops_ctrl(struct ufs_hba *hba,\r\nenum bkops_status status)\r\n{\r\nint err;\r\nu32 curr_status = 0;\r\nerr = ufshcd_get_bkops_status(hba, &curr_status);\r\nif (err) {\r\ndev_err(hba->dev, "%s: failed to get BKOPS status %d\n",\r\n__func__, err);\r\ngoto out;\r\n} else if (curr_status > BKOPS_STATUS_MAX) {\r\ndev_err(hba->dev, "%s: invalid BKOPS status %d\n",\r\n__func__, curr_status);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nif (curr_status >= status)\r\nerr = ufshcd_enable_auto_bkops(hba);\r\nelse\r\nerr = ufshcd_disable_auto_bkops(hba);\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_urgent_bkops(struct ufs_hba *hba)\r\n{\r\nreturn ufshcd_bkops_ctrl(hba, BKOPS_STATUS_PERF_IMPACT);\r\n}\r\nstatic inline int ufshcd_get_ee_status(struct ufs_hba *hba, u32 *status)\r\n{\r\nreturn ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_READ_ATTR,\r\nQUERY_ATTR_IDN_EE_STATUS, 0, 0, status);\r\n}\r\nstatic void ufshcd_exception_event_handler(struct work_struct *work)\r\n{\r\nstruct ufs_hba *hba;\r\nint err;\r\nu32 status = 0;\r\nhba = container_of(work, struct ufs_hba, eeh_work);\r\npm_runtime_get_sync(hba->dev);\r\nerr = ufshcd_get_ee_status(hba, &status);\r\nif (err) {\r\ndev_err(hba->dev, "%s: failed to get exception status %d\n",\r\n__func__, err);\r\ngoto out;\r\n}\r\nstatus &= hba->ee_ctrl_mask;\r\nif (status & MASK_EE_URGENT_BKOPS) {\r\nerr = ufshcd_urgent_bkops(hba);\r\nif (err < 0)\r\ndev_err(hba->dev, "%s: failed to handle urgent bkops %d\n",\r\n__func__, err);\r\n}\r\nout:\r\npm_runtime_put_sync(hba->dev);\r\nreturn;\r\n}\r\nstatic void ufshcd_err_handler(struct work_struct *work)\r\n{\r\nstruct ufs_hba *hba;\r\nunsigned long flags;\r\nu32 err_xfer = 0;\r\nu32 err_tm = 0;\r\nint err = 0;\r\nint tag;\r\nhba = container_of(work, struct ufs_hba, eh_work);\r\npm_runtime_get_sync(hba->dev);\r\nufshcd_hold(hba, false);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (hba->ufshcd_state == UFSHCD_STATE_RESET) {\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\ngoto out;\r\n}\r\nhba->ufshcd_state = UFSHCD_STATE_RESET;\r\nufshcd_set_eh_in_progress(hba);\r\nufshcd_transfer_req_compl(hba);\r\nufshcd_tmc_handler(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nfor_each_set_bit(tag, &hba->outstanding_reqs, hba->nutrs)\r\nif (ufshcd_clear_cmd(hba, tag))\r\nerr_xfer |= 1 << tag;\r\nfor_each_set_bit(tag, &hba->outstanding_tasks, hba->nutmrs)\r\nif (ufshcd_clear_tm_cmd(hba, tag))\r\nerr_tm |= 1 << tag;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nufshcd_transfer_req_compl(hba);\r\nufshcd_tmc_handler(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (err_xfer || err_tm || (hba->saved_err & INT_FATAL_ERRORS) ||\r\n((hba->saved_err & UIC_ERROR) &&\r\n(hba->saved_uic_err & UFSHCD_UIC_DL_PA_INIT_ERROR))) {\r\nerr = ufshcd_reset_and_restore(hba);\r\nif (err) {\r\ndev_err(hba->dev, "%s: reset and restore failed\n",\r\n__func__);\r\nhba->ufshcd_state = UFSHCD_STATE_ERROR;\r\n}\r\nscsi_report_bus_reset(hba->host, 0);\r\nhba->saved_err = 0;\r\nhba->saved_uic_err = 0;\r\n}\r\nufshcd_clear_eh_in_progress(hba);\r\nout:\r\nscsi_unblock_requests(hba->host);\r\nufshcd_release(hba);\r\npm_runtime_put_sync(hba->dev);\r\n}\r\nstatic void ufshcd_update_uic_error(struct ufs_hba *hba)\r\n{\r\nu32 reg;\r\nreg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_DATA_LINK_LAYER);\r\nif (reg & UIC_DATA_LINK_LAYER_ERROR_PA_INIT)\r\nhba->uic_error |= UFSHCD_UIC_DL_PA_INIT_ERROR;\r\nreg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_NETWORK_LAYER);\r\nif (reg)\r\nhba->uic_error |= UFSHCD_UIC_NL_ERROR;\r\nreg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_TRANSPORT_LAYER);\r\nif (reg)\r\nhba->uic_error |= UFSHCD_UIC_TL_ERROR;\r\nreg = ufshcd_readl(hba, REG_UIC_ERROR_CODE_DME);\r\nif (reg)\r\nhba->uic_error |= UFSHCD_UIC_DME_ERROR;\r\ndev_dbg(hba->dev, "%s: UIC error flags = 0x%08x\n",\r\n__func__, hba->uic_error);\r\n}\r\nstatic void ufshcd_check_errors(struct ufs_hba *hba)\r\n{\r\nbool queue_eh_work = false;\r\nif (hba->errors & INT_FATAL_ERRORS)\r\nqueue_eh_work = true;\r\nif (hba->errors & UIC_ERROR) {\r\nhba->uic_error = 0;\r\nufshcd_update_uic_error(hba);\r\nif (hba->uic_error)\r\nqueue_eh_work = true;\r\n}\r\nif (queue_eh_work) {\r\nif (hba->ufshcd_state == UFSHCD_STATE_OPERATIONAL) {\r\nscsi_block_requests(hba->host);\r\nhba->saved_err |= hba->errors;\r\nhba->saved_uic_err |= hba->uic_error;\r\nhba->ufshcd_state = UFSHCD_STATE_ERROR;\r\nschedule_work(&hba->eh_work);\r\n}\r\n}\r\n}\r\nstatic void ufshcd_tmc_handler(struct ufs_hba *hba)\r\n{\r\nu32 tm_doorbell;\r\ntm_doorbell = ufshcd_readl(hba, REG_UTP_TASK_REQ_DOOR_BELL);\r\nhba->tm_condition = tm_doorbell ^ hba->outstanding_tasks;\r\nwake_up(&hba->tm_wq);\r\n}\r\nstatic void ufshcd_sl_intr(struct ufs_hba *hba, u32 intr_status)\r\n{\r\nhba->errors = UFSHCD_ERROR_MASK & intr_status;\r\nif (hba->errors)\r\nufshcd_check_errors(hba);\r\nif (intr_status & UFSHCD_UIC_MASK)\r\nufshcd_uic_cmd_compl(hba, intr_status);\r\nif (intr_status & UTP_TASK_REQ_COMPL)\r\nufshcd_tmc_handler(hba);\r\nif (intr_status & UTP_TRANSFER_REQ_COMPL)\r\nufshcd_transfer_req_compl(hba);\r\n}\r\nstatic irqreturn_t ufshcd_intr(int irq, void *__hba)\r\n{\r\nu32 intr_status;\r\nirqreturn_t retval = IRQ_NONE;\r\nstruct ufs_hba *hba = __hba;\r\nspin_lock(hba->host->host_lock);\r\nintr_status = ufshcd_readl(hba, REG_INTERRUPT_STATUS);\r\nif (intr_status) {\r\nufshcd_writel(hba, intr_status, REG_INTERRUPT_STATUS);\r\nufshcd_sl_intr(hba, intr_status);\r\nretval = IRQ_HANDLED;\r\n}\r\nspin_unlock(hba->host->host_lock);\r\nreturn retval;\r\n}\r\nstatic int ufshcd_clear_tm_cmd(struct ufs_hba *hba, int tag)\r\n{\r\nint err = 0;\r\nu32 mask = 1 << tag;\r\nunsigned long flags;\r\nif (!test_bit(tag, &hba->outstanding_tasks))\r\ngoto out;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nufshcd_writel(hba, ~(1 << tag), REG_UTP_TASK_REQ_LIST_CLEAR);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nerr = ufshcd_wait_for_register(hba,\r\nREG_UTP_TASK_REQ_DOOR_BELL,\r\nmask, 0, 1000, 1000);\r\nout:\r\nreturn err;\r\n}\r\nstatic int ufshcd_issue_tm_cmd(struct ufs_hba *hba, int lun_id, int task_id,\r\nu8 tm_function, u8 *tm_response)\r\n{\r\nstruct utp_task_req_desc *task_req_descp;\r\nstruct utp_upiu_task_req *task_req_upiup;\r\nstruct Scsi_Host *host;\r\nunsigned long flags;\r\nint free_slot;\r\nint err;\r\nint task_tag;\r\nhost = hba->host;\r\nwait_event(hba->tm_tag_wq, ufshcd_get_tm_free_slot(hba, &free_slot));\r\nufshcd_hold(hba, false);\r\nspin_lock_irqsave(host->host_lock, flags);\r\ntask_req_descp = hba->utmrdl_base_addr;\r\ntask_req_descp += free_slot;\r\ntask_req_descp->header.dword_0 = cpu_to_le32(UTP_REQ_DESC_INT_CMD);\r\ntask_req_descp->header.dword_2 =\r\ncpu_to_le32(OCS_INVALID_COMMAND_STATUS);\r\ntask_req_upiup =\r\n(struct utp_upiu_task_req *) task_req_descp->task_req_upiu;\r\ntask_tag = hba->nutrs + free_slot;\r\ntask_req_upiup->header.dword_0 =\r\nUPIU_HEADER_DWORD(UPIU_TRANSACTION_TASK_REQ, 0,\r\nlun_id, task_tag);\r\ntask_req_upiup->header.dword_1 =\r\nUPIU_HEADER_DWORD(0, tm_function, 0, 0);\r\ntask_req_upiup->input_param1 = cpu_to_be32(lun_id);\r\ntask_req_upiup->input_param2 = cpu_to_be32(task_id);\r\n__set_bit(free_slot, &hba->outstanding_tasks);\r\nufshcd_writel(hba, 1 << free_slot, REG_UTP_TASK_REQ_DOOR_BELL);\r\nspin_unlock_irqrestore(host->host_lock, flags);\r\nerr = wait_event_timeout(hba->tm_wq,\r\ntest_bit(free_slot, &hba->tm_condition),\r\nmsecs_to_jiffies(TM_CMD_TIMEOUT));\r\nif (!err) {\r\ndev_err(hba->dev, "%s: task management cmd 0x%.2x timed-out\n",\r\n__func__, tm_function);\r\nif (ufshcd_clear_tm_cmd(hba, free_slot))\r\ndev_WARN(hba->dev, "%s: unable clear tm cmd (slot %d) after timeout\n",\r\n__func__, free_slot);\r\nerr = -ETIMEDOUT;\r\n} else {\r\nerr = ufshcd_task_req_compl(hba, free_slot, tm_response);\r\n}\r\nclear_bit(free_slot, &hba->tm_condition);\r\nufshcd_put_tm_slot(hba, free_slot);\r\nwake_up(&hba->tm_tag_wq);\r\nufshcd_release(hba);\r\nreturn err;\r\n}\r\nstatic int ufshcd_eh_device_reset_handler(struct scsi_cmnd *cmd)\r\n{\r\nstruct Scsi_Host *host;\r\nstruct ufs_hba *hba;\r\nunsigned int tag;\r\nu32 pos;\r\nint err;\r\nu8 resp = 0xF;\r\nstruct ufshcd_lrb *lrbp;\r\nunsigned long flags;\r\nhost = cmd->device->host;\r\nhba = shost_priv(host);\r\ntag = cmd->request->tag;\r\nlrbp = &hba->lrb[tag];\r\nerr = ufshcd_issue_tm_cmd(hba, lrbp->lun, 0, UFS_LOGICAL_RESET, &resp);\r\nif (err || resp != UPIU_TASK_MANAGEMENT_FUNC_COMPL) {\r\nif (!err)\r\nerr = resp;\r\ngoto out;\r\n}\r\nfor_each_set_bit(pos, &hba->outstanding_reqs, hba->nutrs) {\r\nif (hba->lrb[pos].lun == lrbp->lun) {\r\nerr = ufshcd_clear_cmd(hba, pos);\r\nif (err)\r\nbreak;\r\n}\r\n}\r\nspin_lock_irqsave(host->host_lock, flags);\r\nufshcd_transfer_req_compl(hba);\r\nspin_unlock_irqrestore(host->host_lock, flags);\r\nout:\r\nif (!err) {\r\nerr = SUCCESS;\r\n} else {\r\ndev_err(hba->dev, "%s: failed with err %d\n", __func__, err);\r\nerr = FAILED;\r\n}\r\nreturn err;\r\n}\r\nstatic int ufshcd_abort(struct scsi_cmnd *cmd)\r\n{\r\nstruct Scsi_Host *host;\r\nstruct ufs_hba *hba;\r\nunsigned long flags;\r\nunsigned int tag;\r\nint err = 0;\r\nint poll_cnt;\r\nu8 resp = 0xF;\r\nstruct ufshcd_lrb *lrbp;\r\nu32 reg;\r\nhost = cmd->device->host;\r\nhba = shost_priv(host);\r\ntag = cmd->request->tag;\r\nufshcd_hold(hba, false);\r\nif (!(test_bit(tag, &hba->outstanding_reqs)))\r\ngoto out;\r\nreg = ufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\r\nif (!(reg & (1 << tag))) {\r\ndev_err(hba->dev,\r\n"%s: cmd was completed, but without a notifying intr, tag = %d",\r\n__func__, tag);\r\n}\r\nlrbp = &hba->lrb[tag];\r\nfor (poll_cnt = 100; poll_cnt; poll_cnt--) {\r\nerr = ufshcd_issue_tm_cmd(hba, lrbp->lun, lrbp->task_tag,\r\nUFS_QUERY_TASK, &resp);\r\nif (!err && resp == UPIU_TASK_MANAGEMENT_FUNC_SUCCEEDED) {\r\nbreak;\r\n} else if (!err && resp == UPIU_TASK_MANAGEMENT_FUNC_COMPL) {\r\nreg = ufshcd_readl(hba, REG_UTP_TRANSFER_REQ_DOOR_BELL);\r\nif (reg & (1 << tag)) {\r\nusleep_range(100, 200);\r\ncontinue;\r\n}\r\ngoto out;\r\n} else {\r\nif (!err)\r\nerr = resp;\r\ngoto out;\r\n}\r\n}\r\nif (!poll_cnt) {\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\nerr = ufshcd_issue_tm_cmd(hba, lrbp->lun, lrbp->task_tag,\r\nUFS_ABORT_TASK, &resp);\r\nif (err || resp != UPIU_TASK_MANAGEMENT_FUNC_COMPL) {\r\nif (!err)\r\nerr = resp;\r\ngoto out;\r\n}\r\nerr = ufshcd_clear_cmd(hba, tag);\r\nif (err)\r\ngoto out;\r\nscsi_dma_unmap(cmd);\r\nspin_lock_irqsave(host->host_lock, flags);\r\n__clear_bit(tag, &hba->outstanding_reqs);\r\nhba->lrb[tag].cmd = NULL;\r\nspin_unlock_irqrestore(host->host_lock, flags);\r\nclear_bit_unlock(tag, &hba->lrb_in_use);\r\nwake_up(&hba->dev_cmd.tag_wq);\r\nout:\r\nif (!err) {\r\nerr = SUCCESS;\r\n} else {\r\ndev_err(hba->dev, "%s: failed with err %d\n", __func__, err);\r\nerr = FAILED;\r\n}\r\nufshcd_release(hba);\r\nreturn err;\r\n}\r\nstatic int ufshcd_host_reset_and_restore(struct ufs_hba *hba)\r\n{\r\nint err;\r\nunsigned long flags;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nufshcd_hba_stop(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nerr = ufshcd_hba_enable(hba);\r\nif (err)\r\ngoto out;\r\nerr = ufshcd_probe_hba(hba);\r\nif (!err && (hba->ufshcd_state != UFSHCD_STATE_OPERATIONAL))\r\nerr = -EIO;\r\nout:\r\nif (err)\r\ndev_err(hba->dev, "%s: Host init failed %d\n", __func__, err);\r\nreturn err;\r\n}\r\nstatic int ufshcd_reset_and_restore(struct ufs_hba *hba)\r\n{\r\nint err = 0;\r\nunsigned long flags;\r\nint retries = MAX_HOST_RESET_RETRIES;\r\ndo {\r\nerr = ufshcd_host_reset_and_restore(hba);\r\n} while (err && --retries);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nufshcd_transfer_req_compl(hba);\r\nufshcd_tmc_handler(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn err;\r\n}\r\nstatic int ufshcd_eh_host_reset_handler(struct scsi_cmnd *cmd)\r\n{\r\nint err;\r\nunsigned long flags;\r\nstruct ufs_hba *hba;\r\nhba = shost_priv(cmd->device->host);\r\nufshcd_hold(hba, false);\r\ndo {\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (!(work_pending(&hba->eh_work) ||\r\nhba->ufshcd_state == UFSHCD_STATE_RESET))\r\nbreak;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\ndev_dbg(hba->dev, "%s: reset in progress\n", __func__);\r\nflush_work(&hba->eh_work);\r\n} while (1);\r\nhba->ufshcd_state = UFSHCD_STATE_RESET;\r\nufshcd_set_eh_in_progress(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nerr = ufshcd_reset_and_restore(hba);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (!err) {\r\nerr = SUCCESS;\r\nhba->ufshcd_state = UFSHCD_STATE_OPERATIONAL;\r\n} else {\r\nerr = FAILED;\r\nhba->ufshcd_state = UFSHCD_STATE_ERROR;\r\n}\r\nufshcd_clear_eh_in_progress(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nufshcd_release(hba);\r\nreturn err;\r\n}\r\nstatic u32 ufshcd_get_max_icc_level(int sup_curr_uA, u32 start_scan, char *buff)\r\n{\r\nint i;\r\nint curr_uA;\r\nu16 data;\r\nu16 unit;\r\nfor (i = start_scan; i >= 0; i--) {\r\ndata = be16_to_cpu(*((u16 *)(buff + 2*i)));\r\nunit = (data & ATTR_ICC_LVL_UNIT_MASK) >>\r\nATTR_ICC_LVL_UNIT_OFFSET;\r\ncurr_uA = data & ATTR_ICC_LVL_VALUE_MASK;\r\nswitch (unit) {\r\ncase UFSHCD_NANO_AMP:\r\ncurr_uA = curr_uA / 1000;\r\nbreak;\r\ncase UFSHCD_MILI_AMP:\r\ncurr_uA = curr_uA * 1000;\r\nbreak;\r\ncase UFSHCD_AMP:\r\ncurr_uA = curr_uA * 1000 * 1000;\r\nbreak;\r\ncase UFSHCD_MICRO_AMP:\r\ndefault:\r\nbreak;\r\n}\r\nif (sup_curr_uA >= curr_uA)\r\nbreak;\r\n}\r\nif (i < 0) {\r\ni = 0;\r\npr_err("%s: Couldn't find valid icc_level = %d", __func__, i);\r\n}\r\nreturn (u32)i;\r\n}\r\nstatic u32 ufshcd_find_max_sup_active_icc_level(struct ufs_hba *hba,\r\nu8 *desc_buf, int len)\r\n{\r\nu32 icc_level = 0;\r\nif (!hba->vreg_info.vcc || !hba->vreg_info.vccq ||\r\n!hba->vreg_info.vccq2) {\r\ndev_err(hba->dev,\r\n"%s: Regulator capability was not set, actvIccLevel=%d",\r\n__func__, icc_level);\r\ngoto out;\r\n}\r\nif (hba->vreg_info.vcc)\r\nicc_level = ufshcd_get_max_icc_level(\r\nhba->vreg_info.vcc->max_uA,\r\nPOWER_DESC_MAX_ACTV_ICC_LVLS - 1,\r\n&desc_buf[PWR_DESC_ACTIVE_LVLS_VCC_0]);\r\nif (hba->vreg_info.vccq)\r\nicc_level = ufshcd_get_max_icc_level(\r\nhba->vreg_info.vccq->max_uA,\r\nicc_level,\r\n&desc_buf[PWR_DESC_ACTIVE_LVLS_VCCQ_0]);\r\nif (hba->vreg_info.vccq2)\r\nicc_level = ufshcd_get_max_icc_level(\r\nhba->vreg_info.vccq2->max_uA,\r\nicc_level,\r\n&desc_buf[PWR_DESC_ACTIVE_LVLS_VCCQ2_0]);\r\nout:\r\nreturn icc_level;\r\n}\r\nstatic void ufshcd_init_icc_levels(struct ufs_hba *hba)\r\n{\r\nint ret;\r\nint buff_len = QUERY_DESC_POWER_MAX_SIZE;\r\nu8 desc_buf[QUERY_DESC_POWER_MAX_SIZE];\r\nret = ufshcd_read_power_desc(hba, desc_buf, buff_len);\r\nif (ret) {\r\ndev_err(hba->dev,\r\n"%s: Failed reading power descriptor.len = %d ret = %d",\r\n__func__, buff_len, ret);\r\nreturn;\r\n}\r\nhba->init_prefetch_data.icc_level =\r\nufshcd_find_max_sup_active_icc_level(hba,\r\ndesc_buf, buff_len);\r\ndev_dbg(hba->dev, "%s: setting icc_level 0x%x",\r\n__func__, hba->init_prefetch_data.icc_level);\r\nret = ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_WRITE_ATTR,\r\nQUERY_ATTR_IDN_ACTIVE_ICC_LVL, 0, 0,\r\n&hba->init_prefetch_data.icc_level);\r\nif (ret)\r\ndev_err(hba->dev,\r\n"%s: Failed configuring bActiveICCLevel = %d ret = %d",\r\n__func__, hba->init_prefetch_data.icc_level , ret);\r\n}\r\nstatic int ufshcd_scsi_add_wlus(struct ufs_hba *hba)\r\n{\r\nint ret = 0;\r\nstruct scsi_device *sdev_rpmb;\r\nstruct scsi_device *sdev_boot;\r\nhba->sdev_ufs_device = __scsi_add_device(hba->host, 0, 0,\r\nufshcd_upiu_wlun_to_scsi_wlun(UFS_UPIU_UFS_DEVICE_WLUN), NULL);\r\nif (IS_ERR(hba->sdev_ufs_device)) {\r\nret = PTR_ERR(hba->sdev_ufs_device);\r\nhba->sdev_ufs_device = NULL;\r\ngoto out;\r\n}\r\nscsi_device_put(hba->sdev_ufs_device);\r\nsdev_boot = __scsi_add_device(hba->host, 0, 0,\r\nufshcd_upiu_wlun_to_scsi_wlun(UFS_UPIU_BOOT_WLUN), NULL);\r\nif (IS_ERR(sdev_boot)) {\r\nret = PTR_ERR(sdev_boot);\r\ngoto remove_sdev_ufs_device;\r\n}\r\nscsi_device_put(sdev_boot);\r\nsdev_rpmb = __scsi_add_device(hba->host, 0, 0,\r\nufshcd_upiu_wlun_to_scsi_wlun(UFS_UPIU_RPMB_WLUN), NULL);\r\nif (IS_ERR(sdev_rpmb)) {\r\nret = PTR_ERR(sdev_rpmb);\r\ngoto remove_sdev_boot;\r\n}\r\nscsi_device_put(sdev_rpmb);\r\ngoto out;\r\nremove_sdev_boot:\r\nscsi_remove_device(sdev_boot);\r\nremove_sdev_ufs_device:\r\nscsi_remove_device(hba->sdev_ufs_device);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_probe_hba(struct ufs_hba *hba)\r\n{\r\nint ret;\r\nret = ufshcd_link_startup(hba);\r\nif (ret)\r\ngoto out;\r\nufshcd_init_pwr_info(hba);\r\nufshcd_set_link_active(hba);\r\nret = ufshcd_verify_dev_init(hba);\r\nif (ret)\r\ngoto out;\r\nret = ufshcd_complete_dev_init(hba);\r\nif (ret)\r\ngoto out;\r\nufshcd_set_ufs_dev_active(hba);\r\nufshcd_force_reset_auto_bkops(hba);\r\nhba->ufshcd_state = UFSHCD_STATE_OPERATIONAL;\r\nhba->wlun_dev_clr_ua = true;\r\nif (ufshcd_get_max_pwr_mode(hba)) {\r\ndev_err(hba->dev,\r\n"%s: Failed getting max supported power mode\n",\r\n__func__);\r\n} else {\r\nret = ufshcd_config_pwr_mode(hba, &hba->max_pwr_info.info);\r\nif (ret)\r\ndev_err(hba->dev, "%s: Failed setting power mode, err = %d\n",\r\n__func__, ret);\r\n}\r\nif (!ufshcd_eh_in_progress(hba) && !hba->pm_op_in_progress) {\r\nbool flag;\r\nmemset(&hba->dev_info, 0, sizeof(hba->dev_info));\r\nif (!ufshcd_query_flag(hba, UPIU_QUERY_OPCODE_READ_FLAG,\r\nQUERY_FLAG_IDN_PWR_ON_WPE, &flag))\r\nhba->dev_info.f_power_on_wp_en = flag;\r\nif (!hba->is_init_prefetch)\r\nufshcd_init_icc_levels(hba);\r\nif (ufshcd_scsi_add_wlus(hba))\r\ngoto out;\r\nscsi_scan_host(hba->host);\r\npm_runtime_put_sync(hba->dev);\r\n}\r\nif (!hba->is_init_prefetch)\r\nhba->is_init_prefetch = true;\r\nif (ufshcd_is_clkscaling_enabled(hba))\r\ndevfreq_resume_device(hba->devfreq);\r\nout:\r\nif (ret && !ufshcd_eh_in_progress(hba) && !hba->pm_op_in_progress) {\r\npm_runtime_put_sync(hba->dev);\r\nufshcd_hba_exit(hba);\r\n}\r\nreturn ret;\r\n}\r\nstatic void ufshcd_async_scan(void *data, async_cookie_t cookie)\r\n{\r\nstruct ufs_hba *hba = (struct ufs_hba *)data;\r\nufshcd_probe_hba(hba);\r\n}\r\nstatic int ufshcd_config_vreg_load(struct device *dev, struct ufs_vreg *vreg,\r\nint ua)\r\n{\r\nint ret;\r\nif (!vreg)\r\nreturn 0;\r\nret = regulator_set_load(vreg->reg, ua);\r\nif (ret < 0) {\r\ndev_err(dev, "%s: %s set load (ua=%d) failed, err=%d\n",\r\n__func__, vreg->name, ua, ret);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline int ufshcd_config_vreg_lpm(struct ufs_hba *hba,\r\nstruct ufs_vreg *vreg)\r\n{\r\nreturn ufshcd_config_vreg_load(hba->dev, vreg, UFS_VREG_LPM_LOAD_UA);\r\n}\r\nstatic inline int ufshcd_config_vreg_hpm(struct ufs_hba *hba,\r\nstruct ufs_vreg *vreg)\r\n{\r\nreturn ufshcd_config_vreg_load(hba->dev, vreg, vreg->max_uA);\r\n}\r\nstatic int ufshcd_config_vreg(struct device *dev,\r\nstruct ufs_vreg *vreg, bool on)\r\n{\r\nint ret = 0;\r\nstruct regulator *reg = vreg->reg;\r\nconst char *name = vreg->name;\r\nint min_uV, uA_load;\r\nBUG_ON(!vreg);\r\nif (regulator_count_voltages(reg) > 0) {\r\nmin_uV = on ? vreg->min_uV : 0;\r\nret = regulator_set_voltage(reg, min_uV, vreg->max_uV);\r\nif (ret) {\r\ndev_err(dev, "%s: %s set voltage failed, err=%d\n",\r\n__func__, name, ret);\r\ngoto out;\r\n}\r\nuA_load = on ? vreg->max_uA : 0;\r\nret = ufshcd_config_vreg_load(dev, vreg, uA_load);\r\nif (ret)\r\ngoto out;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_enable_vreg(struct device *dev, struct ufs_vreg *vreg)\r\n{\r\nint ret = 0;\r\nif (!vreg || vreg->enabled)\r\ngoto out;\r\nret = ufshcd_config_vreg(dev, vreg, true);\r\nif (!ret)\r\nret = regulator_enable(vreg->reg);\r\nif (!ret)\r\nvreg->enabled = true;\r\nelse\r\ndev_err(dev, "%s: %s enable failed, err=%d\n",\r\n__func__, vreg->name, ret);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_disable_vreg(struct device *dev, struct ufs_vreg *vreg)\r\n{\r\nint ret = 0;\r\nif (!vreg || !vreg->enabled)\r\ngoto out;\r\nret = regulator_disable(vreg->reg);\r\nif (!ret) {\r\nufshcd_config_vreg(dev, vreg, false);\r\nvreg->enabled = false;\r\n} else {\r\ndev_err(dev, "%s: %s disable failed, err=%d\n",\r\n__func__, vreg->name, ret);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_setup_vreg(struct ufs_hba *hba, bool on)\r\n{\r\nint ret = 0;\r\nstruct device *dev = hba->dev;\r\nstruct ufs_vreg_info *info = &hba->vreg_info;\r\nif (!info)\r\ngoto out;\r\nret = ufshcd_toggle_vreg(dev, info->vcc, on);\r\nif (ret)\r\ngoto out;\r\nret = ufshcd_toggle_vreg(dev, info->vccq, on);\r\nif (ret)\r\ngoto out;\r\nret = ufshcd_toggle_vreg(dev, info->vccq2, on);\r\nif (ret)\r\ngoto out;\r\nout:\r\nif (ret) {\r\nufshcd_toggle_vreg(dev, info->vccq2, false);\r\nufshcd_toggle_vreg(dev, info->vccq, false);\r\nufshcd_toggle_vreg(dev, info->vcc, false);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ufshcd_setup_hba_vreg(struct ufs_hba *hba, bool on)\r\n{\r\nstruct ufs_vreg_info *info = &hba->vreg_info;\r\nif (info)\r\nreturn ufshcd_toggle_vreg(hba->dev, info->vdd_hba, on);\r\nreturn 0;\r\n}\r\nstatic int ufshcd_get_vreg(struct device *dev, struct ufs_vreg *vreg)\r\n{\r\nint ret = 0;\r\nif (!vreg)\r\ngoto out;\r\nvreg->reg = devm_regulator_get(dev, vreg->name);\r\nif (IS_ERR(vreg->reg)) {\r\nret = PTR_ERR(vreg->reg);\r\ndev_err(dev, "%s: %s get failed, err=%d\n",\r\n__func__, vreg->name, ret);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_init_vreg(struct ufs_hba *hba)\r\n{\r\nint ret = 0;\r\nstruct device *dev = hba->dev;\r\nstruct ufs_vreg_info *info = &hba->vreg_info;\r\nif (!info)\r\ngoto out;\r\nret = ufshcd_get_vreg(dev, info->vcc);\r\nif (ret)\r\ngoto out;\r\nret = ufshcd_get_vreg(dev, info->vccq);\r\nif (ret)\r\ngoto out;\r\nret = ufshcd_get_vreg(dev, info->vccq2);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_init_hba_vreg(struct ufs_hba *hba)\r\n{\r\nstruct ufs_vreg_info *info = &hba->vreg_info;\r\nif (info)\r\nreturn ufshcd_get_vreg(hba->dev, info->vdd_hba);\r\nreturn 0;\r\n}\r\nstatic int __ufshcd_setup_clocks(struct ufs_hba *hba, bool on,\r\nbool skip_ref_clk)\r\n{\r\nint ret = 0;\r\nstruct ufs_clk_info *clki;\r\nstruct list_head *head = &hba->clk_list_head;\r\nunsigned long flags;\r\nif (!head || list_empty(head))\r\ngoto out;\r\nlist_for_each_entry(clki, head, list) {\r\nif (!IS_ERR_OR_NULL(clki->clk)) {\r\nif (skip_ref_clk && !strcmp(clki->name, "ref_clk"))\r\ncontinue;\r\nif (on && !clki->enabled) {\r\nret = clk_prepare_enable(clki->clk);\r\nif (ret) {\r\ndev_err(hba->dev, "%s: %s prepare enable failed, %d\n",\r\n__func__, clki->name, ret);\r\ngoto out;\r\n}\r\n} else if (!on && clki->enabled) {\r\nclk_disable_unprepare(clki->clk);\r\n}\r\nclki->enabled = on;\r\ndev_dbg(hba->dev, "%s: clk: %s %sabled\n", __func__,\r\nclki->name, on ? "en" : "dis");\r\n}\r\n}\r\nret = ufshcd_vops_setup_clocks(hba, on);\r\nout:\r\nif (ret) {\r\nlist_for_each_entry(clki, head, list) {\r\nif (!IS_ERR_OR_NULL(clki->clk) && clki->enabled)\r\nclk_disable_unprepare(clki->clk);\r\n}\r\n} else if (on) {\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nhba->clk_gating.state = CLKS_ON;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ufshcd_setup_clocks(struct ufs_hba *hba, bool on)\r\n{\r\nreturn __ufshcd_setup_clocks(hba, on, false);\r\n}\r\nstatic int ufshcd_init_clocks(struct ufs_hba *hba)\r\n{\r\nint ret = 0;\r\nstruct ufs_clk_info *clki;\r\nstruct device *dev = hba->dev;\r\nstruct list_head *head = &hba->clk_list_head;\r\nif (!head || list_empty(head))\r\ngoto out;\r\nlist_for_each_entry(clki, head, list) {\r\nif (!clki->name)\r\ncontinue;\r\nclki->clk = devm_clk_get(dev, clki->name);\r\nif (IS_ERR(clki->clk)) {\r\nret = PTR_ERR(clki->clk);\r\ndev_err(dev, "%s: %s clk get failed, %d\n",\r\n__func__, clki->name, ret);\r\ngoto out;\r\n}\r\nif (clki->max_freq) {\r\nret = clk_set_rate(clki->clk, clki->max_freq);\r\nif (ret) {\r\ndev_err(hba->dev, "%s: %s clk set rate(%dHz) failed, %d\n",\r\n__func__, clki->name,\r\nclki->max_freq, ret);\r\ngoto out;\r\n}\r\nclki->curr_freq = clki->max_freq;\r\n}\r\ndev_dbg(dev, "%s: clk: %s, rate: %lu\n", __func__,\r\nclki->name, clk_get_rate(clki->clk));\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_variant_hba_init(struct ufs_hba *hba)\r\n{\r\nint err = 0;\r\nif (!hba->vops)\r\ngoto out;\r\nerr = ufshcd_vops_init(hba);\r\nif (err)\r\ngoto out;\r\nerr = ufshcd_vops_setup_regulators(hba, true);\r\nif (err)\r\ngoto out_exit;\r\ngoto out;\r\nout_exit:\r\nufshcd_vops_exit(hba);\r\nout:\r\nif (err)\r\ndev_err(hba->dev, "%s: variant %s init failed err %d\n",\r\n__func__, ufshcd_get_var_name(hba), err);\r\nreturn err;\r\n}\r\nstatic void ufshcd_variant_hba_exit(struct ufs_hba *hba)\r\n{\r\nif (!hba->vops)\r\nreturn;\r\nufshcd_vops_setup_clocks(hba, false);\r\nufshcd_vops_setup_regulators(hba, false);\r\nufshcd_vops_exit(hba);\r\n}\r\nstatic int ufshcd_hba_init(struct ufs_hba *hba)\r\n{\r\nint err;\r\nerr = ufshcd_init_hba_vreg(hba);\r\nif (err)\r\ngoto out;\r\nerr = ufshcd_setup_hba_vreg(hba, true);\r\nif (err)\r\ngoto out;\r\nerr = ufshcd_init_clocks(hba);\r\nif (err)\r\ngoto out_disable_hba_vreg;\r\nerr = ufshcd_setup_clocks(hba, true);\r\nif (err)\r\ngoto out_disable_hba_vreg;\r\nerr = ufshcd_init_vreg(hba);\r\nif (err)\r\ngoto out_disable_clks;\r\nerr = ufshcd_setup_vreg(hba, true);\r\nif (err)\r\ngoto out_disable_clks;\r\nerr = ufshcd_variant_hba_init(hba);\r\nif (err)\r\ngoto out_disable_vreg;\r\nhba->is_powered = true;\r\ngoto out;\r\nout_disable_vreg:\r\nufshcd_setup_vreg(hba, false);\r\nout_disable_clks:\r\nufshcd_setup_clocks(hba, false);\r\nout_disable_hba_vreg:\r\nufshcd_setup_hba_vreg(hba, false);\r\nout:\r\nreturn err;\r\n}\r\nstatic void ufshcd_hba_exit(struct ufs_hba *hba)\r\n{\r\nif (hba->is_powered) {\r\nufshcd_variant_hba_exit(hba);\r\nufshcd_setup_vreg(hba, false);\r\nufshcd_setup_clocks(hba, false);\r\nufshcd_setup_hba_vreg(hba, false);\r\nhba->is_powered = false;\r\n}\r\n}\r\nstatic int\r\nufshcd_send_request_sense(struct ufs_hba *hba, struct scsi_device *sdp)\r\n{\r\nunsigned char cmd[6] = {REQUEST_SENSE,\r\n0,\r\n0,\r\n0,\r\nSCSI_SENSE_BUFFERSIZE,\r\n0};\r\nchar *buffer;\r\nint ret;\r\nbuffer = kzalloc(SCSI_SENSE_BUFFERSIZE, GFP_KERNEL);\r\nif (!buffer) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = scsi_execute_req_flags(sdp, cmd, DMA_FROM_DEVICE, buffer,\r\nSCSI_SENSE_BUFFERSIZE, NULL,\r\nmsecs_to_jiffies(1000), 3, NULL, REQ_PM);\r\nif (ret)\r\npr_err("%s: failed with err %d\n", __func__, ret);\r\nkfree(buffer);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_set_dev_pwr_mode(struct ufs_hba *hba,\r\nenum ufs_dev_pwr_mode pwr_mode)\r\n{\r\nunsigned char cmd[6] = { START_STOP };\r\nstruct scsi_sense_hdr sshdr;\r\nstruct scsi_device *sdp;\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nsdp = hba->sdev_ufs_device;\r\nif (sdp) {\r\nret = scsi_device_get(sdp);\r\nif (!ret && !scsi_device_online(sdp)) {\r\nret = -ENODEV;\r\nscsi_device_put(sdp);\r\n}\r\n} else {\r\nret = -ENODEV;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (ret)\r\nreturn ret;\r\nhba->host->eh_noresume = 1;\r\nif (hba->wlun_dev_clr_ua) {\r\nret = ufshcd_send_request_sense(hba, sdp);\r\nif (ret)\r\ngoto out;\r\nhba->wlun_dev_clr_ua = false;\r\n}\r\ncmd[4] = pwr_mode << 4;\r\nret = scsi_execute_req_flags(sdp, cmd, DMA_NONE, NULL, 0, &sshdr,\r\nSTART_STOP_TIMEOUT, 0, NULL, REQ_PM);\r\nif (ret) {\r\nsdev_printk(KERN_WARNING, sdp,\r\n"START_STOP failed for power mode: %d, result %x\n",\r\npwr_mode, ret);\r\nif (driver_byte(ret) & DRIVER_SENSE)\r\nscsi_print_sense_hdr(sdp, NULL, &sshdr);\r\n}\r\nif (!ret)\r\nhba->curr_dev_pwr_mode = pwr_mode;\r\nout:\r\nscsi_device_put(sdp);\r\nhba->host->eh_noresume = 0;\r\nreturn ret;\r\n}\r\nstatic int ufshcd_link_state_transition(struct ufs_hba *hba,\r\nenum uic_link_state req_link_state,\r\nint check_for_bkops)\r\n{\r\nint ret = 0;\r\nif (req_link_state == hba->uic_link_state)\r\nreturn 0;\r\nif (req_link_state == UIC_LINK_HIBERN8_STATE) {\r\nret = ufshcd_uic_hibern8_enter(hba);\r\nif (!ret)\r\nufshcd_set_link_hibern8(hba);\r\nelse\r\ngoto out;\r\n}\r\nelse if ((req_link_state == UIC_LINK_OFF_STATE) &&\r\n(!check_for_bkops || (check_for_bkops &&\r\n!hba->auto_bkops_enabled))) {\r\nufshcd_hba_stop(hba);\r\nufshcd_set_link_off(hba);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic void ufshcd_vreg_set_lpm(struct ufs_hba *hba)\r\n{\r\nif (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba) &&\r\n!hba->dev_info.is_lu_power_on_wp) {\r\nufshcd_setup_vreg(hba, false);\r\n} else if (!ufshcd_is_ufs_dev_active(hba)) {\r\nufshcd_toggle_vreg(hba->dev, hba->vreg_info.vcc, false);\r\nif (!ufshcd_is_link_active(hba)) {\r\nufshcd_config_vreg_lpm(hba, hba->vreg_info.vccq);\r\nufshcd_config_vreg_lpm(hba, hba->vreg_info.vccq2);\r\n}\r\n}\r\n}\r\nstatic int ufshcd_vreg_set_hpm(struct ufs_hba *hba)\r\n{\r\nint ret = 0;\r\nif (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba) &&\r\n!hba->dev_info.is_lu_power_on_wp) {\r\nret = ufshcd_setup_vreg(hba, true);\r\n} else if (!ufshcd_is_ufs_dev_active(hba)) {\r\nret = ufshcd_toggle_vreg(hba->dev, hba->vreg_info.vcc, true);\r\nif (!ret && !ufshcd_is_link_active(hba)) {\r\nret = ufshcd_config_vreg_hpm(hba, hba->vreg_info.vccq);\r\nif (ret)\r\ngoto vcc_disable;\r\nret = ufshcd_config_vreg_hpm(hba, hba->vreg_info.vccq2);\r\nif (ret)\r\ngoto vccq_lpm;\r\n}\r\n}\r\ngoto out;\r\nvccq_lpm:\r\nufshcd_config_vreg_lpm(hba, hba->vreg_info.vccq);\r\nvcc_disable:\r\nufshcd_toggle_vreg(hba->dev, hba->vreg_info.vcc, false);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void ufshcd_hba_vreg_set_lpm(struct ufs_hba *hba)\r\n{\r\nif (ufshcd_is_link_off(hba))\r\nufshcd_setup_hba_vreg(hba, false);\r\n}\r\nstatic void ufshcd_hba_vreg_set_hpm(struct ufs_hba *hba)\r\n{\r\nif (ufshcd_is_link_off(hba))\r\nufshcd_setup_hba_vreg(hba, true);\r\n}\r\nstatic int ufshcd_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op)\r\n{\r\nint ret = 0;\r\nenum ufs_pm_level pm_lvl;\r\nenum ufs_dev_pwr_mode req_dev_pwr_mode;\r\nenum uic_link_state req_link_state;\r\nhba->pm_op_in_progress = 1;\r\nif (!ufshcd_is_shutdown_pm(pm_op)) {\r\npm_lvl = ufshcd_is_runtime_pm(pm_op) ?\r\nhba->rpm_lvl : hba->spm_lvl;\r\nreq_dev_pwr_mode = ufs_get_pm_lvl_to_dev_pwr_mode(pm_lvl);\r\nreq_link_state = ufs_get_pm_lvl_to_link_pwr_state(pm_lvl);\r\n} else {\r\nreq_dev_pwr_mode = UFS_POWERDOWN_PWR_MODE;\r\nreq_link_state = UIC_LINK_OFF_STATE;\r\n}\r\nufshcd_hold(hba, false);\r\nhba->clk_gating.is_suspended = true;\r\nif (req_dev_pwr_mode == UFS_ACTIVE_PWR_MODE &&\r\nreq_link_state == UIC_LINK_ACTIVE_STATE) {\r\ngoto disable_clks;\r\n}\r\nif ((req_dev_pwr_mode == hba->curr_dev_pwr_mode) &&\r\n(req_link_state == hba->uic_link_state))\r\ngoto out;\r\nif (!ufshcd_is_ufs_dev_active(hba) || !ufshcd_is_link_active(hba)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (ufshcd_is_runtime_pm(pm_op)) {\r\nif (ufshcd_can_autobkops_during_suspend(hba)) {\r\nret = ufshcd_urgent_bkops(hba);\r\nif (ret)\r\ngoto enable_gating;\r\n} else {\r\nufshcd_disable_auto_bkops(hba);\r\n}\r\n}\r\nif ((req_dev_pwr_mode != hba->curr_dev_pwr_mode) &&\r\n((ufshcd_is_runtime_pm(pm_op) && !hba->auto_bkops_enabled) ||\r\n!ufshcd_is_runtime_pm(pm_op))) {\r\nufshcd_disable_auto_bkops(hba);\r\nret = ufshcd_set_dev_pwr_mode(hba, req_dev_pwr_mode);\r\nif (ret)\r\ngoto enable_gating;\r\n}\r\nret = ufshcd_link_state_transition(hba, req_link_state, 1);\r\nif (ret)\r\ngoto set_dev_active;\r\nufshcd_vreg_set_lpm(hba);\r\ndisable_clks:\r\nif (ufshcd_is_clkscaling_enabled(hba)) {\r\ndevfreq_suspend_device(hba->devfreq);\r\nhba->clk_scaling.window_start_t = 0;\r\n}\r\nret = ufshcd_vops_suspend(hba, pm_op);\r\nif (ret)\r\ngoto set_link_active;\r\nret = ufshcd_vops_setup_clocks(hba, false);\r\nif (ret)\r\ngoto vops_resume;\r\nif (!ufshcd_is_link_active(hba))\r\nufshcd_setup_clocks(hba, false);\r\nelse\r\n__ufshcd_setup_clocks(hba, false, true);\r\nhba->clk_gating.state = CLKS_OFF;\r\nufshcd_disable_irq(hba);\r\nufshcd_hba_vreg_set_lpm(hba);\r\ngoto out;\r\nvops_resume:\r\nufshcd_vops_resume(hba, pm_op);\r\nset_link_active:\r\nufshcd_vreg_set_hpm(hba);\r\nif (ufshcd_is_link_hibern8(hba) && !ufshcd_uic_hibern8_exit(hba))\r\nufshcd_set_link_active(hba);\r\nelse if (ufshcd_is_link_off(hba))\r\nufshcd_host_reset_and_restore(hba);\r\nset_dev_active:\r\nif (!ufshcd_set_dev_pwr_mode(hba, UFS_ACTIVE_PWR_MODE))\r\nufshcd_disable_auto_bkops(hba);\r\nenable_gating:\r\nhba->clk_gating.is_suspended = false;\r\nufshcd_release(hba);\r\nout:\r\nhba->pm_op_in_progress = 0;\r\nreturn ret;\r\n}\r\nstatic int ufshcd_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)\r\n{\r\nint ret;\r\nenum uic_link_state old_link_state;\r\nhba->pm_op_in_progress = 1;\r\nold_link_state = hba->uic_link_state;\r\nufshcd_hba_vreg_set_hpm(hba);\r\nret = ufshcd_setup_clocks(hba, true);\r\nif (ret)\r\ngoto out;\r\nret = ufshcd_enable_irq(hba);\r\nif (ret)\r\ngoto disable_irq_and_vops_clks;\r\nret = ufshcd_vreg_set_hpm(hba);\r\nif (ret)\r\ngoto disable_irq_and_vops_clks;\r\nret = ufshcd_vops_resume(hba, pm_op);\r\nif (ret)\r\ngoto disable_vreg;\r\nif (ufshcd_is_link_hibern8(hba)) {\r\nret = ufshcd_uic_hibern8_exit(hba);\r\nif (!ret)\r\nufshcd_set_link_active(hba);\r\nelse\r\ngoto vendor_suspend;\r\n} else if (ufshcd_is_link_off(hba)) {\r\nret = ufshcd_host_reset_and_restore(hba);\r\nif (ret || !ufshcd_is_link_active(hba))\r\ngoto vendor_suspend;\r\n}\r\nif (!ufshcd_is_ufs_dev_active(hba)) {\r\nret = ufshcd_set_dev_pwr_mode(hba, UFS_ACTIVE_PWR_MODE);\r\nif (ret)\r\ngoto set_old_link_state;\r\n}\r\nufshcd_urgent_bkops(hba);\r\nhba->clk_gating.is_suspended = false;\r\nif (ufshcd_is_clkscaling_enabled(hba))\r\ndevfreq_resume_device(hba->devfreq);\r\nufshcd_release(hba);\r\ngoto out;\r\nset_old_link_state:\r\nufshcd_link_state_transition(hba, old_link_state, 0);\r\nvendor_suspend:\r\nufshcd_vops_suspend(hba, pm_op);\r\ndisable_vreg:\r\nufshcd_vreg_set_lpm(hba);\r\ndisable_irq_and_vops_clks:\r\nufshcd_disable_irq(hba);\r\nufshcd_setup_clocks(hba, false);\r\nout:\r\nhba->pm_op_in_progress = 0;\r\nreturn ret;\r\n}\r\nint ufshcd_system_suspend(struct ufs_hba *hba)\r\n{\r\nint ret = 0;\r\nif (!hba || !hba->is_powered)\r\nreturn 0;\r\nif (pm_runtime_suspended(hba->dev)) {\r\nif (hba->rpm_lvl == hba->spm_lvl)\r\nif ((ufs_get_pm_lvl_to_dev_pwr_mode(hba->spm_lvl) ==\r\nhba->curr_dev_pwr_mode) && !hba->auto_bkops_enabled)\r\ngoto out;\r\nret = ufshcd_runtime_resume(hba);\r\nif (ret)\r\ngoto out;\r\n}\r\nret = ufshcd_suspend(hba, UFS_SYSTEM_PM);\r\nout:\r\nif (!ret)\r\nhba->is_sys_suspended = true;\r\nreturn ret;\r\n}\r\nint ufshcd_system_resume(struct ufs_hba *hba)\r\n{\r\nif (!hba || !hba->is_powered || pm_runtime_suspended(hba->dev))\r\nreturn 0;\r\nreturn ufshcd_resume(hba, UFS_SYSTEM_PM);\r\n}\r\nint ufshcd_runtime_suspend(struct ufs_hba *hba)\r\n{\r\nif (!hba || !hba->is_powered)\r\nreturn 0;\r\nreturn ufshcd_suspend(hba, UFS_RUNTIME_PM);\r\n}\r\nint ufshcd_runtime_resume(struct ufs_hba *hba)\r\n{\r\nif (!hba || !hba->is_powered)\r\nreturn 0;\r\nelse\r\nreturn ufshcd_resume(hba, UFS_RUNTIME_PM);\r\n}\r\nint ufshcd_runtime_idle(struct ufs_hba *hba)\r\n{\r\nreturn 0;\r\n}\r\nint ufshcd_shutdown(struct ufs_hba *hba)\r\n{\r\nint ret = 0;\r\nif (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba))\r\ngoto out;\r\nif (pm_runtime_suspended(hba->dev)) {\r\nret = ufshcd_runtime_resume(hba);\r\nif (ret)\r\ngoto out;\r\n}\r\nret = ufshcd_suspend(hba, UFS_SHUTDOWN_PM);\r\nout:\r\nif (ret)\r\ndev_err(hba->dev, "%s failed, err %d\n", __func__, ret);\r\nreturn 0;\r\n}\r\nvoid ufshcd_remove(struct ufs_hba *hba)\r\n{\r\nscsi_remove_host(hba->host);\r\nufshcd_disable_intr(hba, hba->intr_mask);\r\nufshcd_hba_stop(hba);\r\nscsi_host_put(hba->host);\r\nufshcd_exit_clk_gating(hba);\r\nif (ufshcd_is_clkscaling_enabled(hba))\r\ndevfreq_remove_device(hba->devfreq);\r\nufshcd_hba_exit(hba);\r\n}\r\nvoid ufshcd_dealloc_host(struct ufs_hba *hba)\r\n{\r\nscsi_host_put(hba->host);\r\n}\r\nstatic int ufshcd_set_dma_mask(struct ufs_hba *hba)\r\n{\r\nif (hba->capabilities & MASK_64_ADDRESSING_SUPPORT) {\r\nif (!dma_set_mask_and_coherent(hba->dev, DMA_BIT_MASK(64)))\r\nreturn 0;\r\n}\r\nreturn dma_set_mask_and_coherent(hba->dev, DMA_BIT_MASK(32));\r\n}\r\nint ufshcd_alloc_host(struct device *dev, struct ufs_hba **hba_handle)\r\n{\r\nstruct Scsi_Host *host;\r\nstruct ufs_hba *hba;\r\nint err = 0;\r\nif (!dev) {\r\ndev_err(dev,\r\n"Invalid memory reference for dev is NULL\n");\r\nerr = -ENODEV;\r\ngoto out_error;\r\n}\r\nhost = scsi_host_alloc(&ufshcd_driver_template,\r\nsizeof(struct ufs_hba));\r\nif (!host) {\r\ndev_err(dev, "scsi_host_alloc failed\n");\r\nerr = -ENOMEM;\r\ngoto out_error;\r\n}\r\nhba = shost_priv(host);\r\nhba->host = host;\r\nhba->dev = dev;\r\n*hba_handle = hba;\r\nout_error:\r\nreturn err;\r\n}\r\nstatic int ufshcd_scale_clks(struct ufs_hba *hba, bool scale_up)\r\n{\r\nint ret = 0;\r\nstruct ufs_clk_info *clki;\r\nstruct list_head *head = &hba->clk_list_head;\r\nif (!head || list_empty(head))\r\ngoto out;\r\nret = ufshcd_vops_clk_scale_notify(hba, scale_up, PRE_CHANGE);\r\nif (ret)\r\nreturn ret;\r\nlist_for_each_entry(clki, head, list) {\r\nif (!IS_ERR_OR_NULL(clki->clk)) {\r\nif (scale_up && clki->max_freq) {\r\nif (clki->curr_freq == clki->max_freq)\r\ncontinue;\r\nret = clk_set_rate(clki->clk, clki->max_freq);\r\nif (ret) {\r\ndev_err(hba->dev, "%s: %s clk set rate(%dHz) failed, %d\n",\r\n__func__, clki->name,\r\nclki->max_freq, ret);\r\nbreak;\r\n}\r\nclki->curr_freq = clki->max_freq;\r\n} else if (!scale_up && clki->min_freq) {\r\nif (clki->curr_freq == clki->min_freq)\r\ncontinue;\r\nret = clk_set_rate(clki->clk, clki->min_freq);\r\nif (ret) {\r\ndev_err(hba->dev, "%s: %s clk set rate(%dHz) failed, %d\n",\r\n__func__, clki->name,\r\nclki->min_freq, ret);\r\nbreak;\r\n}\r\nclki->curr_freq = clki->min_freq;\r\n}\r\n}\r\ndev_dbg(hba->dev, "%s: clk: %s, rate: %lu\n", __func__,\r\nclki->name, clk_get_rate(clki->clk));\r\n}\r\nret = ufshcd_vops_clk_scale_notify(hba, scale_up, POST_CHANGE);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ufshcd_devfreq_target(struct device *dev,\r\nunsigned long *freq, u32 flags)\r\n{\r\nint err = 0;\r\nstruct ufs_hba *hba = dev_get_drvdata(dev);\r\nif (!ufshcd_is_clkscaling_enabled(hba))\r\nreturn -EINVAL;\r\nif (*freq == UINT_MAX)\r\nerr = ufshcd_scale_clks(hba, true);\r\nelse if (*freq == 0)\r\nerr = ufshcd_scale_clks(hba, false);\r\nreturn err;\r\n}\r\nstatic int ufshcd_devfreq_get_dev_status(struct device *dev,\r\nstruct devfreq_dev_status *stat)\r\n{\r\nstruct ufs_hba *hba = dev_get_drvdata(dev);\r\nstruct ufs_clk_scaling *scaling = &hba->clk_scaling;\r\nunsigned long flags;\r\nif (!ufshcd_is_clkscaling_enabled(hba))\r\nreturn -EINVAL;\r\nmemset(stat, 0, sizeof(*stat));\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (!scaling->window_start_t)\r\ngoto start_window;\r\nif (scaling->is_busy_started)\r\nscaling->tot_busy_t += ktime_to_us(ktime_sub(ktime_get(),\r\nscaling->busy_start_t));\r\nstat->total_time = jiffies_to_usecs((long)jiffies -\r\n(long)scaling->window_start_t);\r\nstat->busy_time = scaling->tot_busy_t;\r\nstart_window:\r\nscaling->window_start_t = jiffies;\r\nscaling->tot_busy_t = 0;\r\nif (hba->outstanding_reqs) {\r\nscaling->busy_start_t = ktime_get();\r\nscaling->is_busy_started = true;\r\n} else {\r\nscaling->busy_start_t = ktime_set(0, 0);\r\nscaling->is_busy_started = false;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn 0;\r\n}\r\nint ufshcd_init(struct ufs_hba *hba, void __iomem *mmio_base, unsigned int irq)\r\n{\r\nint err;\r\nstruct Scsi_Host *host = hba->host;\r\nstruct device *dev = hba->dev;\r\nif (!mmio_base) {\r\ndev_err(hba->dev,\r\n"Invalid memory reference for mmio_base is NULL\n");\r\nerr = -ENODEV;\r\ngoto out_error;\r\n}\r\nhba->mmio_base = mmio_base;\r\nhba->irq = irq;\r\nerr = ufshcd_hba_init(hba);\r\nif (err)\r\ngoto out_error;\r\nufshcd_hba_capabilities(hba);\r\nhba->ufs_version = ufshcd_get_ufs_version(hba);\r\nhba->intr_mask = ufshcd_get_intr_mask(hba);\r\nerr = ufshcd_set_dma_mask(hba);\r\nif (err) {\r\ndev_err(hba->dev, "set dma mask failed\n");\r\ngoto out_disable;\r\n}\r\nerr = ufshcd_memory_alloc(hba);\r\nif (err) {\r\ndev_err(hba->dev, "Memory allocation failed\n");\r\ngoto out_disable;\r\n}\r\nufshcd_host_memory_configure(hba);\r\nhost->can_queue = hba->nutrs;\r\nhost->cmd_per_lun = hba->nutrs;\r\nhost->max_id = UFSHCD_MAX_ID;\r\nhost->max_lun = UFS_MAX_LUNS;\r\nhost->max_channel = UFSHCD_MAX_CHANNEL;\r\nhost->unique_id = host->host_no;\r\nhost->max_cmd_len = MAX_CDB_SIZE;\r\nhba->max_pwr_info.is_valid = false;\r\ninit_waitqueue_head(&hba->tm_wq);\r\ninit_waitqueue_head(&hba->tm_tag_wq);\r\nINIT_WORK(&hba->eh_work, ufshcd_err_handler);\r\nINIT_WORK(&hba->eeh_work, ufshcd_exception_event_handler);\r\nmutex_init(&hba->uic_cmd_mutex);\r\nmutex_init(&hba->dev_cmd.lock);\r\ninit_waitqueue_head(&hba->dev_cmd.tag_wq);\r\nufshcd_init_clk_gating(hba);\r\nerr = devm_request_irq(dev, irq, ufshcd_intr, IRQF_SHARED, UFSHCD, hba);\r\nif (err) {\r\ndev_err(hba->dev, "request irq failed\n");\r\ngoto exit_gating;\r\n} else {\r\nhba->is_irq_enabled = true;\r\n}\r\nerr = scsi_add_host(host, hba->dev);\r\nif (err) {\r\ndev_err(hba->dev, "scsi_add_host failed\n");\r\ngoto exit_gating;\r\n}\r\nerr = ufshcd_hba_enable(hba);\r\nif (err) {\r\ndev_err(hba->dev, "Host controller enable failed\n");\r\ngoto out_remove_scsi_host;\r\n}\r\nif (ufshcd_is_clkscaling_enabled(hba)) {\r\nhba->devfreq = devfreq_add_device(dev, &ufs_devfreq_profile,\r\n"simple_ondemand", NULL);\r\nif (IS_ERR(hba->devfreq)) {\r\ndev_err(hba->dev, "Unable to register with devfreq %ld\n",\r\nPTR_ERR(hba->devfreq));\r\ngoto out_remove_scsi_host;\r\n}\r\ndevfreq_suspend_device(hba->devfreq);\r\nhba->clk_scaling.window_start_t = 0;\r\n}\r\npm_runtime_get_sync(dev);\r\nufshcd_set_ufs_dev_poweroff(hba);\r\nasync_schedule(ufshcd_async_scan, hba);\r\nreturn 0;\r\nout_remove_scsi_host:\r\nscsi_remove_host(hba->host);\r\nexit_gating:\r\nufshcd_exit_clk_gating(hba);\r\nout_disable:\r\nhba->is_irq_enabled = false;\r\nscsi_host_put(host);\r\nufshcd_hba_exit(hba);\r\nout_error:\r\nreturn err;\r\n}
