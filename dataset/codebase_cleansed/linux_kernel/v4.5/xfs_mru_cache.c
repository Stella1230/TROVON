STATIC unsigned long\r\n_xfs_mru_cache_migrate(\r\nstruct xfs_mru_cache *mru,\r\nunsigned long now)\r\n{\r\nunsigned int grp;\r\nunsigned int migrated = 0;\r\nstruct list_head *lru_list;\r\nif (!mru->time_zero)\r\nreturn 0;\r\nwhile (mru->time_zero <= now - mru->grp_count * mru->grp_time) {\r\nlru_list = mru->lists + mru->lru_grp;\r\nif (!list_empty(lru_list))\r\nlist_splice_init(lru_list, mru->reap_list.prev);\r\nmru->lru_grp = (mru->lru_grp + 1) % mru->grp_count;\r\nmru->time_zero += mru->grp_time;\r\nif (++migrated == mru->grp_count) {\r\nmru->lru_grp = 0;\r\nmru->time_zero = 0;\r\nreturn 0;\r\n}\r\n}\r\nfor (grp = 0; grp < mru->grp_count; grp++) {\r\nlru_list = mru->lists + ((mru->lru_grp + grp) % mru->grp_count);\r\nif (!list_empty(lru_list))\r\nreturn mru->time_zero +\r\n(mru->grp_count + grp) * mru->grp_time;\r\n}\r\nmru->lru_grp = 0;\r\nmru->time_zero = 0;\r\nreturn 0;\r\n}\r\nSTATIC void\r\n_xfs_mru_cache_list_insert(\r\nstruct xfs_mru_cache *mru,\r\nstruct xfs_mru_cache_elem *elem)\r\n{\r\nunsigned int grp = 0;\r\nunsigned long now = jiffies;\r\nif (!_xfs_mru_cache_migrate(mru, now)) {\r\nmru->time_zero = now;\r\nif (!mru->queued) {\r\nmru->queued = 1;\r\nqueue_delayed_work(xfs_mru_reap_wq, &mru->work,\r\nmru->grp_count * mru->grp_time);\r\n}\r\n} else {\r\ngrp = (now - mru->time_zero) / mru->grp_time;\r\ngrp = (mru->lru_grp + grp) % mru->grp_count;\r\n}\r\nlist_add_tail(&elem->list_node, mru->lists + grp);\r\n}\r\nSTATIC void\r\n_xfs_mru_cache_clear_reap_list(\r\nstruct xfs_mru_cache *mru)\r\n__releases(mru->lock) __acquires(mru->lock)\r\n{\r\nstruct xfs_mru_cache_elem *elem, *next;\r\nstruct list_head tmp;\r\nINIT_LIST_HEAD(&tmp);\r\nlist_for_each_entry_safe(elem, next, &mru->reap_list, list_node) {\r\nradix_tree_delete(&mru->store, elem->key);\r\nlist_move(&elem->list_node, &tmp);\r\n}\r\nspin_unlock(&mru->lock);\r\nlist_for_each_entry_safe(elem, next, &tmp, list_node) {\r\nlist_del_init(&elem->list_node);\r\nmru->free_func(elem);\r\n}\r\nspin_lock(&mru->lock);\r\n}\r\nSTATIC void\r\n_xfs_mru_cache_reap(\r\nstruct work_struct *work)\r\n{\r\nstruct xfs_mru_cache *mru =\r\ncontainer_of(work, struct xfs_mru_cache, work.work);\r\nunsigned long now, next;\r\nASSERT(mru && mru->lists);\r\nif (!mru || !mru->lists)\r\nreturn;\r\nspin_lock(&mru->lock);\r\nnext = _xfs_mru_cache_migrate(mru, jiffies);\r\n_xfs_mru_cache_clear_reap_list(mru);\r\nmru->queued = next;\r\nif ((mru->queued > 0)) {\r\nnow = jiffies;\r\nif (next <= now)\r\nnext = 0;\r\nelse\r\nnext -= now;\r\nqueue_delayed_work(xfs_mru_reap_wq, &mru->work, next);\r\n}\r\nspin_unlock(&mru->lock);\r\n}\r\nint\r\nxfs_mru_cache_init(void)\r\n{\r\nxfs_mru_reap_wq = alloc_workqueue("xfs_mru_cache",\r\nWQ_MEM_RECLAIM|WQ_FREEZABLE, 1);\r\nif (!xfs_mru_reap_wq)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid\r\nxfs_mru_cache_uninit(void)\r\n{\r\ndestroy_workqueue(xfs_mru_reap_wq);\r\n}\r\nint\r\nxfs_mru_cache_create(\r\nstruct xfs_mru_cache **mrup,\r\nunsigned int lifetime_ms,\r\nunsigned int grp_count,\r\nxfs_mru_cache_free_func_t free_func)\r\n{\r\nstruct xfs_mru_cache *mru = NULL;\r\nint err = 0, grp;\r\nunsigned int grp_time;\r\nif (mrup)\r\n*mrup = NULL;\r\nif (!mrup || !grp_count || !lifetime_ms || !free_func)\r\nreturn -EINVAL;\r\nif (!(grp_time = msecs_to_jiffies(lifetime_ms) / grp_count))\r\nreturn -EINVAL;\r\nif (!(mru = kmem_zalloc(sizeof(*mru), KM_SLEEP)))\r\nreturn -ENOMEM;\r\nmru->grp_count = grp_count + 1;\r\nmru->lists = kmem_zalloc(mru->grp_count * sizeof(*mru->lists), KM_SLEEP);\r\nif (!mru->lists) {\r\nerr = -ENOMEM;\r\ngoto exit;\r\n}\r\nfor (grp = 0; grp < mru->grp_count; grp++)\r\nINIT_LIST_HEAD(mru->lists + grp);\r\nINIT_RADIX_TREE(&mru->store, GFP_ATOMIC);\r\nINIT_LIST_HEAD(&mru->reap_list);\r\nspin_lock_init(&mru->lock);\r\nINIT_DELAYED_WORK(&mru->work, _xfs_mru_cache_reap);\r\nmru->grp_time = grp_time;\r\nmru->free_func = free_func;\r\n*mrup = mru;\r\nexit:\r\nif (err && mru && mru->lists)\r\nkmem_free(mru->lists);\r\nif (err && mru)\r\nkmem_free(mru);\r\nreturn err;\r\n}\r\nstatic void\r\nxfs_mru_cache_flush(\r\nstruct xfs_mru_cache *mru)\r\n{\r\nif (!mru || !mru->lists)\r\nreturn;\r\nspin_lock(&mru->lock);\r\nif (mru->queued) {\r\nspin_unlock(&mru->lock);\r\ncancel_delayed_work_sync(&mru->work);\r\nspin_lock(&mru->lock);\r\n}\r\n_xfs_mru_cache_migrate(mru, jiffies + mru->grp_count * mru->grp_time);\r\n_xfs_mru_cache_clear_reap_list(mru);\r\nspin_unlock(&mru->lock);\r\n}\r\nvoid\r\nxfs_mru_cache_destroy(\r\nstruct xfs_mru_cache *mru)\r\n{\r\nif (!mru || !mru->lists)\r\nreturn;\r\nxfs_mru_cache_flush(mru);\r\nkmem_free(mru->lists);\r\nkmem_free(mru);\r\n}\r\nint\r\nxfs_mru_cache_insert(\r\nstruct xfs_mru_cache *mru,\r\nunsigned long key,\r\nstruct xfs_mru_cache_elem *elem)\r\n{\r\nint error;\r\nASSERT(mru && mru->lists);\r\nif (!mru || !mru->lists)\r\nreturn -EINVAL;\r\nif (radix_tree_preload(GFP_NOFS))\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&elem->list_node);\r\nelem->key = key;\r\nspin_lock(&mru->lock);\r\nerror = radix_tree_insert(&mru->store, key, elem);\r\nradix_tree_preload_end();\r\nif (!error)\r\n_xfs_mru_cache_list_insert(mru, elem);\r\nspin_unlock(&mru->lock);\r\nreturn error;\r\n}\r\nstruct xfs_mru_cache_elem *\r\nxfs_mru_cache_remove(\r\nstruct xfs_mru_cache *mru,\r\nunsigned long key)\r\n{\r\nstruct xfs_mru_cache_elem *elem;\r\nASSERT(mru && mru->lists);\r\nif (!mru || !mru->lists)\r\nreturn NULL;\r\nspin_lock(&mru->lock);\r\nelem = radix_tree_delete(&mru->store, key);\r\nif (elem)\r\nlist_del(&elem->list_node);\r\nspin_unlock(&mru->lock);\r\nreturn elem;\r\n}\r\nvoid\r\nxfs_mru_cache_delete(\r\nstruct xfs_mru_cache *mru,\r\nunsigned long key)\r\n{\r\nstruct xfs_mru_cache_elem *elem;\r\nelem = xfs_mru_cache_remove(mru, key);\r\nif (elem)\r\nmru->free_func(elem);\r\n}\r\nstruct xfs_mru_cache_elem *\r\nxfs_mru_cache_lookup(\r\nstruct xfs_mru_cache *mru,\r\nunsigned long key)\r\n{\r\nstruct xfs_mru_cache_elem *elem;\r\nASSERT(mru && mru->lists);\r\nif (!mru || !mru->lists)\r\nreturn NULL;\r\nspin_lock(&mru->lock);\r\nelem = radix_tree_lookup(&mru->store, key);\r\nif (elem) {\r\nlist_del(&elem->list_node);\r\n_xfs_mru_cache_list_insert(mru, elem);\r\n__release(mru_lock);\r\n} else\r\nspin_unlock(&mru->lock);\r\nreturn elem;\r\n}\r\nvoid\r\nxfs_mru_cache_done(\r\nstruct xfs_mru_cache *mru)\r\n__releases(mru->lock)\r\n{\r\nspin_unlock(&mru->lock);\r\n}
