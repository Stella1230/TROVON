static u32 nd_blk_meta_size(struct nd_blk_device *blk_dev)\r\n{\r\nreturn blk_dev->nsblk->lbasize - blk_dev->sector_size;\r\n}\r\nstatic resource_size_t to_dev_offset(struct nd_namespace_blk *nsblk,\r\nresource_size_t ns_offset, unsigned int len)\r\n{\r\nint i;\r\nfor (i = 0; i < nsblk->num_resources; i++) {\r\nif (ns_offset < resource_size(nsblk->res[i])) {\r\nif (ns_offset + len > resource_size(nsblk->res[i])) {\r\ndev_WARN_ONCE(&nsblk->common.dev, 1,\r\n"illegal request\n");\r\nreturn SIZE_MAX;\r\n}\r\nreturn nsblk->res[i]->start + ns_offset;\r\n}\r\nns_offset -= resource_size(nsblk->res[i]);\r\n}\r\ndev_WARN_ONCE(&nsblk->common.dev, 1, "request out of range\n");\r\nreturn SIZE_MAX;\r\n}\r\nstatic int nd_blk_rw_integrity(struct nd_blk_device *blk_dev,\r\nstruct bio_integrity_payload *bip, u64 lba,\r\nint rw)\r\n{\r\nunsigned int len = nd_blk_meta_size(blk_dev);\r\nresource_size_t dev_offset, ns_offset;\r\nstruct nd_namespace_blk *nsblk;\r\nstruct nd_blk_region *ndbr;\r\nint err = 0;\r\nnsblk = blk_dev->nsblk;\r\nndbr = blk_dev->ndbr;\r\nns_offset = lba * blk_dev->internal_lbasize + blk_dev->sector_size;\r\ndev_offset = to_dev_offset(nsblk, ns_offset, len);\r\nif (dev_offset == SIZE_MAX)\r\nreturn -EIO;\r\nwhile (len) {\r\nunsigned int cur_len;\r\nstruct bio_vec bv;\r\nvoid *iobuf;\r\nbv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);\r\ncur_len = min(len, bv.bv_len);\r\niobuf = kmap_atomic(bv.bv_page);\r\nerr = ndbr->do_io(ndbr, dev_offset, iobuf + bv.bv_offset,\r\ncur_len, rw);\r\nkunmap_atomic(iobuf);\r\nif (err)\r\nreturn err;\r\nlen -= cur_len;\r\ndev_offset += cur_len;\r\nbvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len);\r\n}\r\nreturn err;\r\n}\r\nstatic int nd_blk_rw_integrity(struct nd_blk_device *blk_dev,\r\nstruct bio_integrity_payload *bip, u64 lba,\r\nint rw)\r\n{\r\nreturn 0;\r\n}\r\nstatic int nd_blk_do_bvec(struct nd_blk_device *blk_dev,\r\nstruct bio_integrity_payload *bip, struct page *page,\r\nunsigned int len, unsigned int off, int rw,\r\nsector_t sector)\r\n{\r\nstruct nd_blk_region *ndbr = blk_dev->ndbr;\r\nresource_size_t dev_offset, ns_offset;\r\nint err = 0;\r\nvoid *iobuf;\r\nu64 lba;\r\nwhile (len) {\r\nunsigned int cur_len;\r\ncur_len = bip ? min(len, blk_dev->sector_size) : len;\r\nlba = div_u64(sector << SECTOR_SHIFT, blk_dev->sector_size);\r\nns_offset = lba * blk_dev->internal_lbasize;\r\ndev_offset = to_dev_offset(blk_dev->nsblk, ns_offset, cur_len);\r\nif (dev_offset == SIZE_MAX)\r\nreturn -EIO;\r\niobuf = kmap_atomic(page);\r\nerr = ndbr->do_io(ndbr, dev_offset, iobuf + off, cur_len, rw);\r\nkunmap_atomic(iobuf);\r\nif (err)\r\nreturn err;\r\nif (bip) {\r\nerr = nd_blk_rw_integrity(blk_dev, bip, lba, rw);\r\nif (err)\r\nreturn err;\r\n}\r\nlen -= cur_len;\r\noff += cur_len;\r\nsector += blk_dev->sector_size >> SECTOR_SHIFT;\r\n}\r\nreturn err;\r\n}\r\nstatic blk_qc_t nd_blk_make_request(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct block_device *bdev = bio->bi_bdev;\r\nstruct gendisk *disk = bdev->bd_disk;\r\nstruct bio_integrity_payload *bip;\r\nstruct nd_blk_device *blk_dev;\r\nstruct bvec_iter iter;\r\nunsigned long start;\r\nstruct bio_vec bvec;\r\nint err = 0, rw;\r\nbool do_acct;\r\nif (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {\r\nbio->bi_error = -EIO;\r\ngoto out;\r\n}\r\nbip = bio_integrity(bio);\r\nblk_dev = disk->private_data;\r\nrw = bio_data_dir(bio);\r\ndo_acct = nd_iostat_start(bio, &start);\r\nbio_for_each_segment(bvec, bio, iter) {\r\nunsigned int len = bvec.bv_len;\r\nBUG_ON(len > PAGE_SIZE);\r\nerr = nd_blk_do_bvec(blk_dev, bip, bvec.bv_page, len,\r\nbvec.bv_offset, rw, iter.bi_sector);\r\nif (err) {\r\ndev_info(&blk_dev->nsblk->common.dev,\r\n"io error in %s sector %lld, len %d,\n",\r\n(rw == READ) ? "READ" : "WRITE",\r\n(unsigned long long) iter.bi_sector, len);\r\nbio->bi_error = err;\r\nbreak;\r\n}\r\n}\r\nif (do_acct)\r\nnd_iostat_end(bio, start);\r\nout:\r\nbio_endio(bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic int nd_blk_rw_bytes(struct nd_namespace_common *ndns,\r\nresource_size_t offset, void *iobuf, size_t n, int rw)\r\n{\r\nstruct nd_blk_device *blk_dev = dev_get_drvdata(ndns->claim);\r\nstruct nd_namespace_blk *nsblk = blk_dev->nsblk;\r\nstruct nd_blk_region *ndbr = blk_dev->ndbr;\r\nresource_size_t dev_offset;\r\ndev_offset = to_dev_offset(nsblk, offset, n);\r\nif (unlikely(offset + n > blk_dev->disk_size)) {\r\ndev_WARN_ONCE(&ndns->dev, 1, "request out of range\n");\r\nreturn -EFAULT;\r\n}\r\nif (dev_offset == SIZE_MAX)\r\nreturn -EIO;\r\nreturn ndbr->do_io(ndbr, dev_offset, iobuf, n, rw);\r\n}\r\nstatic int nd_blk_attach_disk(struct nd_namespace_common *ndns,\r\nstruct nd_blk_device *blk_dev)\r\n{\r\nresource_size_t available_disk_size;\r\nstruct gendisk *disk;\r\nu64 internal_nlba;\r\ninternal_nlba = div_u64(blk_dev->disk_size, blk_dev->internal_lbasize);\r\navailable_disk_size = internal_nlba * blk_dev->sector_size;\r\nblk_dev->queue = blk_alloc_queue(GFP_KERNEL);\r\nif (!blk_dev->queue)\r\nreturn -ENOMEM;\r\nblk_queue_make_request(blk_dev->queue, nd_blk_make_request);\r\nblk_queue_max_hw_sectors(blk_dev->queue, UINT_MAX);\r\nblk_queue_bounce_limit(blk_dev->queue, BLK_BOUNCE_ANY);\r\nblk_queue_logical_block_size(blk_dev->queue, blk_dev->sector_size);\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, blk_dev->queue);\r\ndisk = blk_dev->disk = alloc_disk(0);\r\nif (!disk) {\r\nblk_cleanup_queue(blk_dev->queue);\r\nreturn -ENOMEM;\r\n}\r\ndisk->driverfs_dev = &ndns->dev;\r\ndisk->major = nd_blk_major;\r\ndisk->first_minor = 0;\r\ndisk->fops = &nd_blk_fops;\r\ndisk->private_data = blk_dev;\r\ndisk->queue = blk_dev->queue;\r\ndisk->flags = GENHD_FL_EXT_DEVT;\r\nnvdimm_namespace_disk_name(ndns, disk->disk_name);\r\nset_capacity(disk, 0);\r\nadd_disk(disk);\r\nif (nd_blk_meta_size(blk_dev)) {\r\nint rc = nd_integrity_init(disk, nd_blk_meta_size(blk_dev));\r\nif (rc) {\r\ndel_gendisk(disk);\r\nput_disk(disk);\r\nblk_cleanup_queue(blk_dev->queue);\r\nreturn rc;\r\n}\r\n}\r\nset_capacity(disk, available_disk_size >> SECTOR_SHIFT);\r\nrevalidate_disk(disk);\r\nreturn 0;\r\n}\r\nstatic int nd_blk_probe(struct device *dev)\r\n{\r\nstruct nd_namespace_common *ndns;\r\nstruct nd_namespace_blk *nsblk;\r\nstruct nd_blk_device *blk_dev;\r\nint rc;\r\nndns = nvdimm_namespace_common_probe(dev);\r\nif (IS_ERR(ndns))\r\nreturn PTR_ERR(ndns);\r\nblk_dev = kzalloc(sizeof(*blk_dev), GFP_KERNEL);\r\nif (!blk_dev)\r\nreturn -ENOMEM;\r\nnsblk = to_nd_namespace_blk(&ndns->dev);\r\nblk_dev->disk_size = nvdimm_namespace_capacity(ndns);\r\nblk_dev->ndbr = to_nd_blk_region(dev->parent);\r\nblk_dev->nsblk = to_nd_namespace_blk(&ndns->dev);\r\nblk_dev->internal_lbasize = roundup(nsblk->lbasize,\r\nINT_LBASIZE_ALIGNMENT);\r\nblk_dev->sector_size = ((nsblk->lbasize >= 4096) ? 4096 : 512);\r\ndev_set_drvdata(dev, blk_dev);\r\nndns->rw_bytes = nd_blk_rw_bytes;\r\nif (is_nd_btt(dev))\r\nrc = nvdimm_namespace_attach_btt(ndns);\r\nelse if (nd_btt_probe(ndns, blk_dev) == 0) {\r\nrc = -ENXIO;\r\n} else\r\nrc = nd_blk_attach_disk(ndns, blk_dev);\r\nif (rc)\r\nkfree(blk_dev);\r\nreturn rc;\r\n}\r\nstatic void nd_blk_detach_disk(struct nd_blk_device *blk_dev)\r\n{\r\ndel_gendisk(blk_dev->disk);\r\nput_disk(blk_dev->disk);\r\nblk_cleanup_queue(blk_dev->queue);\r\n}\r\nstatic int nd_blk_remove(struct device *dev)\r\n{\r\nstruct nd_blk_device *blk_dev = dev_get_drvdata(dev);\r\nif (is_nd_btt(dev))\r\nnvdimm_namespace_detach_btt(to_nd_btt(dev)->ndns);\r\nelse\r\nnd_blk_detach_disk(blk_dev);\r\nkfree(blk_dev);\r\nreturn 0;\r\n}\r\nstatic int __init nd_blk_init(void)\r\n{\r\nint rc;\r\nrc = register_blkdev(0, "nd_blk");\r\nif (rc < 0)\r\nreturn rc;\r\nnd_blk_major = rc;\r\nrc = nd_driver_register(&nd_blk_driver);\r\nif (rc < 0)\r\nunregister_blkdev(nd_blk_major, "nd_blk");\r\nreturn rc;\r\n}\r\nstatic void __exit nd_blk_exit(void)\r\n{\r\ndriver_unregister(&nd_blk_driver.drv);\r\nunregister_blkdev(nd_blk_major, "nd_blk");\r\n}
