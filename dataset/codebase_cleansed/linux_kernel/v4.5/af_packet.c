static int packet_direct_xmit(struct sk_buff *skb)\r\n{\r\nstruct net_device *dev = skb->dev;\r\nnetdev_features_t features;\r\nstruct netdev_queue *txq;\r\nint ret = NETDEV_TX_BUSY;\r\nif (unlikely(!netif_running(dev) ||\r\n!netif_carrier_ok(dev)))\r\ngoto drop;\r\nfeatures = netif_skb_features(skb);\r\nif (skb_needs_linearize(skb, features) &&\r\n__skb_linearize(skb))\r\ngoto drop;\r\ntxq = skb_get_tx_queue(dev, skb);\r\nlocal_bh_disable();\r\nHARD_TX_LOCK(dev, txq, smp_processor_id());\r\nif (!netif_xmit_frozen_or_drv_stopped(txq))\r\nret = netdev_start_xmit(skb, dev, txq, false);\r\nHARD_TX_UNLOCK(dev, txq);\r\nlocal_bh_enable();\r\nif (!dev_xmit_complete(ret))\r\nkfree_skb(skb);\r\nreturn ret;\r\ndrop:\r\natomic_long_inc(&dev->tx_dropped);\r\nkfree_skb(skb);\r\nreturn NET_XMIT_DROP;\r\n}\r\nstatic struct net_device *packet_cached_dev_get(struct packet_sock *po)\r\n{\r\nstruct net_device *dev;\r\nrcu_read_lock();\r\ndev = rcu_dereference(po->cached_dev);\r\nif (likely(dev))\r\ndev_hold(dev);\r\nrcu_read_unlock();\r\nreturn dev;\r\n}\r\nstatic void packet_cached_dev_assign(struct packet_sock *po,\r\nstruct net_device *dev)\r\n{\r\nrcu_assign_pointer(po->cached_dev, dev);\r\n}\r\nstatic void packet_cached_dev_reset(struct packet_sock *po)\r\n{\r\nRCU_INIT_POINTER(po->cached_dev, NULL);\r\n}\r\nstatic bool packet_use_direct_xmit(const struct packet_sock *po)\r\n{\r\nreturn po->xmit == packet_direct_xmit;\r\n}\r\nstatic u16 __packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb)\r\n{\r\nreturn (u16) raw_smp_processor_id() % dev->real_num_tx_queues;\r\n}\r\nstatic void packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb)\r\n{\r\nconst struct net_device_ops *ops = dev->netdev_ops;\r\nu16 queue_index;\r\nif (ops->ndo_select_queue) {\r\nqueue_index = ops->ndo_select_queue(dev, skb, NULL,\r\n__packet_pick_tx_queue);\r\nqueue_index = netdev_cap_txqueue(dev, queue_index);\r\n} else {\r\nqueue_index = __packet_pick_tx_queue(dev, skb);\r\n}\r\nskb_set_queue_mapping(skb, queue_index);\r\n}\r\nstatic void register_prot_hook(struct sock *sk)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\nif (!po->running) {\r\nif (po->fanout)\r\n__fanout_link(sk, po);\r\nelse\r\ndev_add_pack(&po->prot_hook);\r\nsock_hold(sk);\r\npo->running = 1;\r\n}\r\n}\r\nstatic void __unregister_prot_hook(struct sock *sk, bool sync)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\npo->running = 0;\r\nif (po->fanout)\r\n__fanout_unlink(sk, po);\r\nelse\r\n__dev_remove_pack(&po->prot_hook);\r\n__sock_put(sk);\r\nif (sync) {\r\nspin_unlock(&po->bind_lock);\r\nsynchronize_net();\r\nspin_lock(&po->bind_lock);\r\n}\r\n}\r\nstatic void unregister_prot_hook(struct sock *sk, bool sync)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\nif (po->running)\r\n__unregister_prot_hook(sk, sync);\r\n}\r\nstatic inline struct page * __pure pgv_to_page(void *addr)\r\n{\r\nif (is_vmalloc_addr(addr))\r\nreturn vmalloc_to_page(addr);\r\nreturn virt_to_page(addr);\r\n}\r\nstatic void __packet_set_status(struct packet_sock *po, void *frame, int status)\r\n{\r\nunion tpacket_uhdr h;\r\nh.raw = frame;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\nh.h1->tp_status = status;\r\nflush_dcache_page(pgv_to_page(&h.h1->tp_status));\r\nbreak;\r\ncase TPACKET_V2:\r\nh.h2->tp_status = status;\r\nflush_dcache_page(pgv_to_page(&h.h2->tp_status));\r\nbreak;\r\ncase TPACKET_V3:\r\ndefault:\r\nWARN(1, "TPACKET version not supported.\n");\r\nBUG();\r\n}\r\nsmp_wmb();\r\n}\r\nstatic int __packet_get_status(struct packet_sock *po, void *frame)\r\n{\r\nunion tpacket_uhdr h;\r\nsmp_rmb();\r\nh.raw = frame;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\nflush_dcache_page(pgv_to_page(&h.h1->tp_status));\r\nreturn h.h1->tp_status;\r\ncase TPACKET_V2:\r\nflush_dcache_page(pgv_to_page(&h.h2->tp_status));\r\nreturn h.h2->tp_status;\r\ncase TPACKET_V3:\r\ndefault:\r\nWARN(1, "TPACKET version not supported.\n");\r\nBUG();\r\nreturn 0;\r\n}\r\n}\r\nstatic __u32 tpacket_get_timestamp(struct sk_buff *skb, struct timespec *ts,\r\nunsigned int flags)\r\n{\r\nstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\r\nif (shhwtstamps &&\r\n(flags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\r\nktime_to_timespec_cond(shhwtstamps->hwtstamp, ts))\r\nreturn TP_STATUS_TS_RAW_HARDWARE;\r\nif (ktime_to_timespec_cond(skb->tstamp, ts))\r\nreturn TP_STATUS_TS_SOFTWARE;\r\nreturn 0;\r\n}\r\nstatic __u32 __packet_set_timestamp(struct packet_sock *po, void *frame,\r\nstruct sk_buff *skb)\r\n{\r\nunion tpacket_uhdr h;\r\nstruct timespec ts;\r\n__u32 ts_status;\r\nif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\r\nreturn 0;\r\nh.raw = frame;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\nh.h1->tp_sec = ts.tv_sec;\r\nh.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\r\nbreak;\r\ncase TPACKET_V2:\r\nh.h2->tp_sec = ts.tv_sec;\r\nh.h2->tp_nsec = ts.tv_nsec;\r\nbreak;\r\ncase TPACKET_V3:\r\ndefault:\r\nWARN(1, "TPACKET version not supported.\n");\r\nBUG();\r\n}\r\nflush_dcache_page(pgv_to_page(&h.h1->tp_sec));\r\nsmp_wmb();\r\nreturn ts_status;\r\n}\r\nstatic void *packet_lookup_frame(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb,\r\nunsigned int position,\r\nint status)\r\n{\r\nunsigned int pg_vec_pos, frame_offset;\r\nunion tpacket_uhdr h;\r\npg_vec_pos = position / rb->frames_per_block;\r\nframe_offset = position % rb->frames_per_block;\r\nh.raw = rb->pg_vec[pg_vec_pos].buffer +\r\n(frame_offset * rb->frame_size);\r\nif (status != __packet_get_status(po, h.raw))\r\nreturn NULL;\r\nreturn h.raw;\r\n}\r\nstatic void *packet_current_frame(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb,\r\nint status)\r\n{\r\nreturn packet_lookup_frame(po, rb, rb->head, status);\r\n}\r\nstatic void prb_del_retire_blk_timer(struct tpacket_kbdq_core *pkc)\r\n{\r\ndel_timer_sync(&pkc->retire_blk_timer);\r\n}\r\nstatic void prb_shutdown_retire_blk_timer(struct packet_sock *po,\r\nstruct sk_buff_head *rb_queue)\r\n{\r\nstruct tpacket_kbdq_core *pkc;\r\npkc = GET_PBDQC_FROM_RB(&po->rx_ring);\r\nspin_lock_bh(&rb_queue->lock);\r\npkc->delete_blk_timer = 1;\r\nspin_unlock_bh(&rb_queue->lock);\r\nprb_del_retire_blk_timer(pkc);\r\n}\r\nstatic void prb_init_blk_timer(struct packet_sock *po,\r\nstruct tpacket_kbdq_core *pkc,\r\nvoid (*func) (unsigned long))\r\n{\r\ninit_timer(&pkc->retire_blk_timer);\r\npkc->retire_blk_timer.data = (long)po;\r\npkc->retire_blk_timer.function = func;\r\npkc->retire_blk_timer.expires = jiffies;\r\n}\r\nstatic void prb_setup_retire_blk_timer(struct packet_sock *po)\r\n{\r\nstruct tpacket_kbdq_core *pkc;\r\npkc = GET_PBDQC_FROM_RB(&po->rx_ring);\r\nprb_init_blk_timer(po, pkc, prb_retire_rx_blk_timer_expired);\r\n}\r\nstatic int prb_calc_retire_blk_tmo(struct packet_sock *po,\r\nint blk_size_in_bytes)\r\n{\r\nstruct net_device *dev;\r\nunsigned int mbits = 0, msec = 0, div = 0, tmo = 0;\r\nstruct ethtool_cmd ecmd;\r\nint err;\r\nu32 speed;\r\nrtnl_lock();\r\ndev = __dev_get_by_index(sock_net(&po->sk), po->ifindex);\r\nif (unlikely(!dev)) {\r\nrtnl_unlock();\r\nreturn DEFAULT_PRB_RETIRE_TOV;\r\n}\r\nerr = __ethtool_get_settings(dev, &ecmd);\r\nspeed = ethtool_cmd_speed(&ecmd);\r\nrtnl_unlock();\r\nif (!err) {\r\nif (speed < SPEED_1000 || speed == SPEED_UNKNOWN) {\r\nreturn DEFAULT_PRB_RETIRE_TOV;\r\n} else {\r\nmsec = 1;\r\ndiv = speed / 1000;\r\n}\r\n}\r\nmbits = (blk_size_in_bytes * 8) / (1024 * 1024);\r\nif (div)\r\nmbits /= div;\r\ntmo = mbits * msec;\r\nif (div)\r\nreturn tmo+1;\r\nreturn tmo;\r\n}\r\nstatic void prb_init_ft_ops(struct tpacket_kbdq_core *p1,\r\nunion tpacket_req_u *req_u)\r\n{\r\np1->feature_req_word = req_u->req3.tp_feature_req_word;\r\n}\r\nstatic void init_prb_bdqc(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb,\r\nstruct pgv *pg_vec,\r\nunion tpacket_req_u *req_u)\r\n{\r\nstruct tpacket_kbdq_core *p1 = GET_PBDQC_FROM_RB(rb);\r\nstruct tpacket_block_desc *pbd;\r\nmemset(p1, 0x0, sizeof(*p1));\r\np1->knxt_seq_num = 1;\r\np1->pkbdq = pg_vec;\r\npbd = (struct tpacket_block_desc *)pg_vec[0].buffer;\r\np1->pkblk_start = pg_vec[0].buffer;\r\np1->kblk_size = req_u->req3.tp_block_size;\r\np1->knum_blocks = req_u->req3.tp_block_nr;\r\np1->hdrlen = po->tp_hdrlen;\r\np1->version = po->tp_version;\r\np1->last_kactive_blk_num = 0;\r\npo->stats.stats3.tp_freeze_q_cnt = 0;\r\nif (req_u->req3.tp_retire_blk_tov)\r\np1->retire_blk_tov = req_u->req3.tp_retire_blk_tov;\r\nelse\r\np1->retire_blk_tov = prb_calc_retire_blk_tmo(po,\r\nreq_u->req3.tp_block_size);\r\np1->tov_in_jiffies = msecs_to_jiffies(p1->retire_blk_tov);\r\np1->blk_sizeof_priv = req_u->req3.tp_sizeof_priv;\r\np1->max_frame_len = p1->kblk_size - BLK_PLUS_PRIV(p1->blk_sizeof_priv);\r\nprb_init_ft_ops(p1, req_u);\r\nprb_setup_retire_blk_timer(po);\r\nprb_open_block(p1, pbd);\r\n}\r\nstatic void _prb_refresh_rx_retire_blk_timer(struct tpacket_kbdq_core *pkc)\r\n{\r\nmod_timer(&pkc->retire_blk_timer,\r\njiffies + pkc->tov_in_jiffies);\r\npkc->last_kactive_blk_num = pkc->kactive_blk_num;\r\n}\r\nstatic void prb_retire_rx_blk_timer_expired(unsigned long data)\r\n{\r\nstruct packet_sock *po = (struct packet_sock *)data;\r\nstruct tpacket_kbdq_core *pkc = GET_PBDQC_FROM_RB(&po->rx_ring);\r\nunsigned int frozen;\r\nstruct tpacket_block_desc *pbd;\r\nspin_lock(&po->sk.sk_receive_queue.lock);\r\nfrozen = prb_queue_frozen(pkc);\r\npbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\r\nif (unlikely(pkc->delete_blk_timer))\r\ngoto out;\r\nif (BLOCK_NUM_PKTS(pbd)) {\r\nwhile (atomic_read(&pkc->blk_fill_in_prog)) {\r\ncpu_relax();\r\n}\r\n}\r\nif (pkc->last_kactive_blk_num == pkc->kactive_blk_num) {\r\nif (!frozen) {\r\nif (!BLOCK_NUM_PKTS(pbd)) {\r\ngoto refresh_timer;\r\n}\r\nprb_retire_current_block(pkc, po, TP_STATUS_BLK_TMO);\r\nif (!prb_dispatch_next_block(pkc, po))\r\ngoto refresh_timer;\r\nelse\r\ngoto out;\r\n} else {\r\nif (prb_curr_blk_in_use(pkc, pbd)) {\r\ngoto refresh_timer;\r\n} else {\r\nprb_open_block(pkc, pbd);\r\ngoto out;\r\n}\r\n}\r\n}\r\nrefresh_timer:\r\n_prb_refresh_rx_retire_blk_timer(pkc);\r\nout:\r\nspin_unlock(&po->sk.sk_receive_queue.lock);\r\n}\r\nstatic void prb_flush_block(struct tpacket_kbdq_core *pkc1,\r\nstruct tpacket_block_desc *pbd1, __u32 status)\r\n{\r\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\r\nu8 *start, *end;\r\nstart = (u8 *)pbd1;\r\nstart += PAGE_SIZE;\r\nend = (u8 *)PAGE_ALIGN((unsigned long)pkc1->pkblk_end);\r\nfor (; start < end; start += PAGE_SIZE)\r\nflush_dcache_page(pgv_to_page(start));\r\nsmp_wmb();\r\n#endif\r\nBLOCK_STATUS(pbd1) = status;\r\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\r\nstart = (u8 *)pbd1;\r\nflush_dcache_page(pgv_to_page(start));\r\nsmp_wmb();\r\n#endif\r\n}\r\nstatic void prb_close_block(struct tpacket_kbdq_core *pkc1,\r\nstruct tpacket_block_desc *pbd1,\r\nstruct packet_sock *po, unsigned int stat)\r\n{\r\n__u32 status = TP_STATUS_USER | stat;\r\nstruct tpacket3_hdr *last_pkt;\r\nstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\r\nstruct sock *sk = &po->sk;\r\nif (po->stats.stats3.tp_drops)\r\nstatus |= TP_STATUS_LOSING;\r\nlast_pkt = (struct tpacket3_hdr *)pkc1->prev;\r\nlast_pkt->tp_next_offset = 0;\r\nif (BLOCK_NUM_PKTS(pbd1)) {\r\nh1->ts_last_pkt.ts_sec = last_pkt->tp_sec;\r\nh1->ts_last_pkt.ts_nsec = last_pkt->tp_nsec;\r\n} else {\r\nstruct timespec ts;\r\ngetnstimeofday(&ts);\r\nh1->ts_last_pkt.ts_sec = ts.tv_sec;\r\nh1->ts_last_pkt.ts_nsec = ts.tv_nsec;\r\n}\r\nsmp_wmb();\r\nprb_flush_block(pkc1, pbd1, status);\r\nsk->sk_data_ready(sk);\r\npkc1->kactive_blk_num = GET_NEXT_PRB_BLK_NUM(pkc1);\r\n}\r\nstatic void prb_thaw_queue(struct tpacket_kbdq_core *pkc)\r\n{\r\npkc->reset_pending_on_curr_blk = 0;\r\n}\r\nstatic void prb_open_block(struct tpacket_kbdq_core *pkc1,\r\nstruct tpacket_block_desc *pbd1)\r\n{\r\nstruct timespec ts;\r\nstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\r\nsmp_rmb();\r\nBLOCK_SNUM(pbd1) = pkc1->knxt_seq_num++;\r\nBLOCK_NUM_PKTS(pbd1) = 0;\r\nBLOCK_LEN(pbd1) = BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\r\ngetnstimeofday(&ts);\r\nh1->ts_first_pkt.ts_sec = ts.tv_sec;\r\nh1->ts_first_pkt.ts_nsec = ts.tv_nsec;\r\npkc1->pkblk_start = (char *)pbd1;\r\npkc1->nxt_offset = pkc1->pkblk_start + BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\r\nBLOCK_O2FP(pbd1) = (__u32)BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\r\nBLOCK_O2PRIV(pbd1) = BLK_HDR_LEN;\r\npbd1->version = pkc1->version;\r\npkc1->prev = pkc1->nxt_offset;\r\npkc1->pkblk_end = pkc1->pkblk_start + pkc1->kblk_size;\r\nprb_thaw_queue(pkc1);\r\n_prb_refresh_rx_retire_blk_timer(pkc1);\r\nsmp_wmb();\r\n}\r\nstatic void prb_freeze_queue(struct tpacket_kbdq_core *pkc,\r\nstruct packet_sock *po)\r\n{\r\npkc->reset_pending_on_curr_blk = 1;\r\npo->stats.stats3.tp_freeze_q_cnt++;\r\n}\r\nstatic void *prb_dispatch_next_block(struct tpacket_kbdq_core *pkc,\r\nstruct packet_sock *po)\r\n{\r\nstruct tpacket_block_desc *pbd;\r\nsmp_rmb();\r\npbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\r\nif (TP_STATUS_USER & BLOCK_STATUS(pbd)) {\r\nprb_freeze_queue(pkc, po);\r\nreturn NULL;\r\n}\r\nprb_open_block(pkc, pbd);\r\nreturn (void *)pkc->nxt_offset;\r\n}\r\nstatic void prb_retire_current_block(struct tpacket_kbdq_core *pkc,\r\nstruct packet_sock *po, unsigned int status)\r\n{\r\nstruct tpacket_block_desc *pbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\r\nif (likely(TP_STATUS_KERNEL == BLOCK_STATUS(pbd))) {\r\nif (!(status & TP_STATUS_BLK_TMO)) {\r\nwhile (atomic_read(&pkc->blk_fill_in_prog)) {\r\ncpu_relax();\r\n}\r\n}\r\nprb_close_block(pkc, pbd, po, status);\r\nreturn;\r\n}\r\n}\r\nstatic int prb_curr_blk_in_use(struct tpacket_kbdq_core *pkc,\r\nstruct tpacket_block_desc *pbd)\r\n{\r\nreturn TP_STATUS_USER & BLOCK_STATUS(pbd);\r\n}\r\nstatic int prb_queue_frozen(struct tpacket_kbdq_core *pkc)\r\n{\r\nreturn pkc->reset_pending_on_curr_blk;\r\n}\r\nstatic void prb_clear_blk_fill_status(struct packet_ring_buffer *rb)\r\n{\r\nstruct tpacket_kbdq_core *pkc = GET_PBDQC_FROM_RB(rb);\r\natomic_dec(&pkc->blk_fill_in_prog);\r\n}\r\nstatic void prb_fill_rxhash(struct tpacket_kbdq_core *pkc,\r\nstruct tpacket3_hdr *ppd)\r\n{\r\nppd->hv1.tp_rxhash = skb_get_hash(pkc->skb);\r\n}\r\nstatic void prb_clear_rxhash(struct tpacket_kbdq_core *pkc,\r\nstruct tpacket3_hdr *ppd)\r\n{\r\nppd->hv1.tp_rxhash = 0;\r\n}\r\nstatic void prb_fill_vlan_info(struct tpacket_kbdq_core *pkc,\r\nstruct tpacket3_hdr *ppd)\r\n{\r\nif (skb_vlan_tag_present(pkc->skb)) {\r\nppd->hv1.tp_vlan_tci = skb_vlan_tag_get(pkc->skb);\r\nppd->hv1.tp_vlan_tpid = ntohs(pkc->skb->vlan_proto);\r\nppd->tp_status = TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\r\n} else {\r\nppd->hv1.tp_vlan_tci = 0;\r\nppd->hv1.tp_vlan_tpid = 0;\r\nppd->tp_status = TP_STATUS_AVAILABLE;\r\n}\r\n}\r\nstatic void prb_run_all_ft_ops(struct tpacket_kbdq_core *pkc,\r\nstruct tpacket3_hdr *ppd)\r\n{\r\nppd->hv1.tp_padding = 0;\r\nprb_fill_vlan_info(pkc, ppd);\r\nif (pkc->feature_req_word & TP_FT_REQ_FILL_RXHASH)\r\nprb_fill_rxhash(pkc, ppd);\r\nelse\r\nprb_clear_rxhash(pkc, ppd);\r\n}\r\nstatic void prb_fill_curr_block(char *curr,\r\nstruct tpacket_kbdq_core *pkc,\r\nstruct tpacket_block_desc *pbd,\r\nunsigned int len)\r\n{\r\nstruct tpacket3_hdr *ppd;\r\nppd = (struct tpacket3_hdr *)curr;\r\nppd->tp_next_offset = TOTAL_PKT_LEN_INCL_ALIGN(len);\r\npkc->prev = curr;\r\npkc->nxt_offset += TOTAL_PKT_LEN_INCL_ALIGN(len);\r\nBLOCK_LEN(pbd) += TOTAL_PKT_LEN_INCL_ALIGN(len);\r\nBLOCK_NUM_PKTS(pbd) += 1;\r\natomic_inc(&pkc->blk_fill_in_prog);\r\nprb_run_all_ft_ops(pkc, ppd);\r\n}\r\nstatic void *__packet_lookup_frame_in_block(struct packet_sock *po,\r\nstruct sk_buff *skb,\r\nint status,\r\nunsigned int len\r\n)\r\n{\r\nstruct tpacket_kbdq_core *pkc;\r\nstruct tpacket_block_desc *pbd;\r\nchar *curr, *end;\r\npkc = GET_PBDQC_FROM_RB(&po->rx_ring);\r\npbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\r\nif (prb_queue_frozen(pkc)) {\r\nif (prb_curr_blk_in_use(pkc, pbd)) {\r\nreturn NULL;\r\n} else {\r\nprb_open_block(pkc, pbd);\r\n}\r\n}\r\nsmp_mb();\r\ncurr = pkc->nxt_offset;\r\npkc->skb = skb;\r\nend = (char *)pbd + pkc->kblk_size;\r\nif (curr+TOTAL_PKT_LEN_INCL_ALIGN(len) < end) {\r\nprb_fill_curr_block(curr, pkc, pbd, len);\r\nreturn (void *)curr;\r\n}\r\nprb_retire_current_block(pkc, po, 0);\r\ncurr = (char *)prb_dispatch_next_block(pkc, po);\r\nif (curr) {\r\npbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\r\nprb_fill_curr_block(curr, pkc, pbd, len);\r\nreturn (void *)curr;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void *packet_current_rx_frame(struct packet_sock *po,\r\nstruct sk_buff *skb,\r\nint status, unsigned int len)\r\n{\r\nchar *curr = NULL;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\ncase TPACKET_V2:\r\ncurr = packet_lookup_frame(po, &po->rx_ring,\r\npo->rx_ring.head, status);\r\nreturn curr;\r\ncase TPACKET_V3:\r\nreturn __packet_lookup_frame_in_block(po, skb, status, len);\r\ndefault:\r\nWARN(1, "TPACKET version not supported\n");\r\nBUG();\r\nreturn NULL;\r\n}\r\n}\r\nstatic void *prb_lookup_block(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb,\r\nunsigned int idx,\r\nint status)\r\n{\r\nstruct tpacket_kbdq_core *pkc = GET_PBDQC_FROM_RB(rb);\r\nstruct tpacket_block_desc *pbd = GET_PBLOCK_DESC(pkc, idx);\r\nif (status != BLOCK_STATUS(pbd))\r\nreturn NULL;\r\nreturn pbd;\r\n}\r\nstatic int prb_previous_blk_num(struct packet_ring_buffer *rb)\r\n{\r\nunsigned int prev;\r\nif (rb->prb_bdqc.kactive_blk_num)\r\nprev = rb->prb_bdqc.kactive_blk_num-1;\r\nelse\r\nprev = rb->prb_bdqc.knum_blocks-1;\r\nreturn prev;\r\n}\r\nstatic void *__prb_previous_block(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb,\r\nint status)\r\n{\r\nunsigned int previous = prb_previous_blk_num(rb);\r\nreturn prb_lookup_block(po, rb, previous, status);\r\n}\r\nstatic void *packet_previous_rx_frame(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb,\r\nint status)\r\n{\r\nif (po->tp_version <= TPACKET_V2)\r\nreturn packet_previous_frame(po, rb, status);\r\nreturn __prb_previous_block(po, rb, status);\r\n}\r\nstatic void packet_increment_rx_head(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb)\r\n{\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\ncase TPACKET_V2:\r\nreturn packet_increment_head(rb);\r\ncase TPACKET_V3:\r\ndefault:\r\nWARN(1, "TPACKET version not supported.\n");\r\nBUG();\r\nreturn;\r\n}\r\n}\r\nstatic void *packet_previous_frame(struct packet_sock *po,\r\nstruct packet_ring_buffer *rb,\r\nint status)\r\n{\r\nunsigned int previous = rb->head ? rb->head - 1 : rb->frame_max;\r\nreturn packet_lookup_frame(po, rb, previous, status);\r\n}\r\nstatic void packet_increment_head(struct packet_ring_buffer *buff)\r\n{\r\nbuff->head = buff->head != buff->frame_max ? buff->head+1 : 0;\r\n}\r\nstatic void packet_inc_pending(struct packet_ring_buffer *rb)\r\n{\r\nthis_cpu_inc(*rb->pending_refcnt);\r\n}\r\nstatic void packet_dec_pending(struct packet_ring_buffer *rb)\r\n{\r\nthis_cpu_dec(*rb->pending_refcnt);\r\n}\r\nstatic unsigned int packet_read_pending(const struct packet_ring_buffer *rb)\r\n{\r\nunsigned int refcnt = 0;\r\nint cpu;\r\nif (rb->pending_refcnt == NULL)\r\nreturn 0;\r\nfor_each_possible_cpu(cpu)\r\nrefcnt += *per_cpu_ptr(rb->pending_refcnt, cpu);\r\nreturn refcnt;\r\n}\r\nstatic int packet_alloc_pending(struct packet_sock *po)\r\n{\r\npo->rx_ring.pending_refcnt = NULL;\r\npo->tx_ring.pending_refcnt = alloc_percpu(unsigned int);\r\nif (unlikely(po->tx_ring.pending_refcnt == NULL))\r\nreturn -ENOBUFS;\r\nreturn 0;\r\n}\r\nstatic void packet_free_pending(struct packet_sock *po)\r\n{\r\nfree_percpu(po->tx_ring.pending_refcnt);\r\n}\r\nstatic bool __tpacket_has_room(struct packet_sock *po, int pow_off)\r\n{\r\nint idx, len;\r\nlen = po->rx_ring.frame_max + 1;\r\nidx = po->rx_ring.head;\r\nif (pow_off)\r\nidx += len >> pow_off;\r\nif (idx >= len)\r\nidx -= len;\r\nreturn packet_lookup_frame(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\r\n}\r\nstatic bool __tpacket_v3_has_room(struct packet_sock *po, int pow_off)\r\n{\r\nint idx, len;\r\nlen = po->rx_ring.prb_bdqc.knum_blocks;\r\nidx = po->rx_ring.prb_bdqc.kactive_blk_num;\r\nif (pow_off)\r\nidx += len >> pow_off;\r\nif (idx >= len)\r\nidx -= len;\r\nreturn prb_lookup_block(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\r\n}\r\nstatic int __packet_rcv_has_room(struct packet_sock *po, struct sk_buff *skb)\r\n{\r\nstruct sock *sk = &po->sk;\r\nint ret = ROOM_NONE;\r\nif (po->prot_hook.func != tpacket_rcv) {\r\nint avail = sk->sk_rcvbuf - atomic_read(&sk->sk_rmem_alloc)\r\n- (skb ? skb->truesize : 0);\r\nif (avail > (sk->sk_rcvbuf >> ROOM_POW_OFF))\r\nreturn ROOM_NORMAL;\r\nelse if (avail > 0)\r\nreturn ROOM_LOW;\r\nelse\r\nreturn ROOM_NONE;\r\n}\r\nif (po->tp_version == TPACKET_V3) {\r\nif (__tpacket_v3_has_room(po, ROOM_POW_OFF))\r\nret = ROOM_NORMAL;\r\nelse if (__tpacket_v3_has_room(po, 0))\r\nret = ROOM_LOW;\r\n} else {\r\nif (__tpacket_has_room(po, ROOM_POW_OFF))\r\nret = ROOM_NORMAL;\r\nelse if (__tpacket_has_room(po, 0))\r\nret = ROOM_LOW;\r\n}\r\nreturn ret;\r\n}\r\nstatic int packet_rcv_has_room(struct packet_sock *po, struct sk_buff *skb)\r\n{\r\nint ret;\r\nbool has_room;\r\nspin_lock_bh(&po->sk.sk_receive_queue.lock);\r\nret = __packet_rcv_has_room(po, skb);\r\nhas_room = ret == ROOM_NORMAL;\r\nif (po->pressure == has_room)\r\npo->pressure = !has_room;\r\nspin_unlock_bh(&po->sk.sk_receive_queue.lock);\r\nreturn ret;\r\n}\r\nstatic void packet_sock_destruct(struct sock *sk)\r\n{\r\nskb_queue_purge(&sk->sk_error_queue);\r\nWARN_ON(atomic_read(&sk->sk_rmem_alloc));\r\nWARN_ON(atomic_read(&sk->sk_wmem_alloc));\r\nif (!sock_flag(sk, SOCK_DEAD)) {\r\npr_err("Attempt to release alive packet socket: %p\n", sk);\r\nreturn;\r\n}\r\nsk_refcnt_debug_dec(sk);\r\n}\r\nstatic bool fanout_flow_is_huge(struct packet_sock *po, struct sk_buff *skb)\r\n{\r\nu32 rxhash;\r\nint i, count = 0;\r\nrxhash = skb_get_hash(skb);\r\nfor (i = 0; i < ROLLOVER_HLEN; i++)\r\nif (po->rollover->history[i] == rxhash)\r\ncount++;\r\npo->rollover->history[prandom_u32() % ROLLOVER_HLEN] = rxhash;\r\nreturn count > (ROLLOVER_HLEN >> 1);\r\n}\r\nstatic unsigned int fanout_demux_hash(struct packet_fanout *f,\r\nstruct sk_buff *skb,\r\nunsigned int num)\r\n{\r\nreturn reciprocal_scale(skb_get_hash(skb), num);\r\n}\r\nstatic unsigned int fanout_demux_lb(struct packet_fanout *f,\r\nstruct sk_buff *skb,\r\nunsigned int num)\r\n{\r\nunsigned int val = atomic_inc_return(&f->rr_cur);\r\nreturn val % num;\r\n}\r\nstatic unsigned int fanout_demux_cpu(struct packet_fanout *f,\r\nstruct sk_buff *skb,\r\nunsigned int num)\r\n{\r\nreturn smp_processor_id() % num;\r\n}\r\nstatic unsigned int fanout_demux_rnd(struct packet_fanout *f,\r\nstruct sk_buff *skb,\r\nunsigned int num)\r\n{\r\nreturn prandom_u32_max(num);\r\n}\r\nstatic unsigned int fanout_demux_rollover(struct packet_fanout *f,\r\nstruct sk_buff *skb,\r\nunsigned int idx, bool try_self,\r\nunsigned int num)\r\n{\r\nstruct packet_sock *po, *po_next, *po_skip = NULL;\r\nunsigned int i, j, room = ROOM_NONE;\r\npo = pkt_sk(f->arr[idx]);\r\nif (try_self) {\r\nroom = packet_rcv_has_room(po, skb);\r\nif (room == ROOM_NORMAL ||\r\n(room == ROOM_LOW && !fanout_flow_is_huge(po, skb)))\r\nreturn idx;\r\npo_skip = po;\r\n}\r\ni = j = min_t(int, po->rollover->sock, num - 1);\r\ndo {\r\npo_next = pkt_sk(f->arr[i]);\r\nif (po_next != po_skip && !po_next->pressure &&\r\npacket_rcv_has_room(po_next, skb) == ROOM_NORMAL) {\r\nif (i != j)\r\npo->rollover->sock = i;\r\natomic_long_inc(&po->rollover->num);\r\nif (room == ROOM_LOW)\r\natomic_long_inc(&po->rollover->num_huge);\r\nreturn i;\r\n}\r\nif (++i == num)\r\ni = 0;\r\n} while (i != j);\r\natomic_long_inc(&po->rollover->num_failed);\r\nreturn idx;\r\n}\r\nstatic unsigned int fanout_demux_qm(struct packet_fanout *f,\r\nstruct sk_buff *skb,\r\nunsigned int num)\r\n{\r\nreturn skb_get_queue_mapping(skb) % num;\r\n}\r\nstatic unsigned int fanout_demux_bpf(struct packet_fanout *f,\r\nstruct sk_buff *skb,\r\nunsigned int num)\r\n{\r\nstruct bpf_prog *prog;\r\nunsigned int ret = 0;\r\nrcu_read_lock();\r\nprog = rcu_dereference(f->bpf_prog);\r\nif (prog)\r\nret = bpf_prog_run_clear_cb(prog, skb) % num;\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic bool fanout_has_flag(struct packet_fanout *f, u16 flag)\r\n{\r\nreturn f->flags & (flag >> 8);\r\n}\r\nstatic int packet_rcv_fanout(struct sk_buff *skb, struct net_device *dev,\r\nstruct packet_type *pt, struct net_device *orig_dev)\r\n{\r\nstruct packet_fanout *f = pt->af_packet_priv;\r\nunsigned int num = READ_ONCE(f->num_members);\r\nstruct net *net = read_pnet(&f->net);\r\nstruct packet_sock *po;\r\nunsigned int idx;\r\nif (!net_eq(dev_net(dev), net) || !num) {\r\nkfree_skb(skb);\r\nreturn 0;\r\n}\r\nif (fanout_has_flag(f, PACKET_FANOUT_FLAG_DEFRAG)) {\r\nskb = ip_check_defrag(net, skb, IP_DEFRAG_AF_PACKET);\r\nif (!skb)\r\nreturn 0;\r\n}\r\nswitch (f->type) {\r\ncase PACKET_FANOUT_HASH:\r\ndefault:\r\nidx = fanout_demux_hash(f, skb, num);\r\nbreak;\r\ncase PACKET_FANOUT_LB:\r\nidx = fanout_demux_lb(f, skb, num);\r\nbreak;\r\ncase PACKET_FANOUT_CPU:\r\nidx = fanout_demux_cpu(f, skb, num);\r\nbreak;\r\ncase PACKET_FANOUT_RND:\r\nidx = fanout_demux_rnd(f, skb, num);\r\nbreak;\r\ncase PACKET_FANOUT_QM:\r\nidx = fanout_demux_qm(f, skb, num);\r\nbreak;\r\ncase PACKET_FANOUT_ROLLOVER:\r\nidx = fanout_demux_rollover(f, skb, 0, false, num);\r\nbreak;\r\ncase PACKET_FANOUT_CBPF:\r\ncase PACKET_FANOUT_EBPF:\r\nidx = fanout_demux_bpf(f, skb, num);\r\nbreak;\r\n}\r\nif (fanout_has_flag(f, PACKET_FANOUT_FLAG_ROLLOVER))\r\nidx = fanout_demux_rollover(f, skb, idx, true, num);\r\npo = pkt_sk(f->arr[idx]);\r\nreturn po->prot_hook.func(skb, dev, &po->prot_hook, orig_dev);\r\n}\r\nstatic void __fanout_link(struct sock *sk, struct packet_sock *po)\r\n{\r\nstruct packet_fanout *f = po->fanout;\r\nspin_lock(&f->lock);\r\nf->arr[f->num_members] = sk;\r\nsmp_wmb();\r\nf->num_members++;\r\nspin_unlock(&f->lock);\r\n}\r\nstatic void __fanout_unlink(struct sock *sk, struct packet_sock *po)\r\n{\r\nstruct packet_fanout *f = po->fanout;\r\nint i;\r\nspin_lock(&f->lock);\r\nfor (i = 0; i < f->num_members; i++) {\r\nif (f->arr[i] == sk)\r\nbreak;\r\n}\r\nBUG_ON(i >= f->num_members);\r\nf->arr[i] = f->arr[f->num_members - 1];\r\nf->num_members--;\r\nspin_unlock(&f->lock);\r\n}\r\nstatic bool match_fanout_group(struct packet_type *ptype, struct sock *sk)\r\n{\r\nif (sk->sk_family != PF_PACKET)\r\nreturn false;\r\nreturn ptype->af_packet_priv == pkt_sk(sk)->fanout;\r\n}\r\nstatic void fanout_init_data(struct packet_fanout *f)\r\n{\r\nswitch (f->type) {\r\ncase PACKET_FANOUT_LB:\r\natomic_set(&f->rr_cur, 0);\r\nbreak;\r\ncase PACKET_FANOUT_CBPF:\r\ncase PACKET_FANOUT_EBPF:\r\nRCU_INIT_POINTER(f->bpf_prog, NULL);\r\nbreak;\r\n}\r\n}\r\nstatic void __fanout_set_data_bpf(struct packet_fanout *f, struct bpf_prog *new)\r\n{\r\nstruct bpf_prog *old;\r\nspin_lock(&f->lock);\r\nold = rcu_dereference_protected(f->bpf_prog, lockdep_is_held(&f->lock));\r\nrcu_assign_pointer(f->bpf_prog, new);\r\nspin_unlock(&f->lock);\r\nif (old) {\r\nsynchronize_net();\r\nbpf_prog_destroy(old);\r\n}\r\n}\r\nstatic int fanout_set_data_cbpf(struct packet_sock *po, char __user *data,\r\nunsigned int len)\r\n{\r\nstruct bpf_prog *new;\r\nstruct sock_fprog fprog;\r\nint ret;\r\nif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\r\nreturn -EPERM;\r\nif (len != sizeof(fprog))\r\nreturn -EINVAL;\r\nif (copy_from_user(&fprog, data, len))\r\nreturn -EFAULT;\r\nret = bpf_prog_create_from_user(&new, &fprog, NULL, false);\r\nif (ret)\r\nreturn ret;\r\n__fanout_set_data_bpf(po->fanout, new);\r\nreturn 0;\r\n}\r\nstatic int fanout_set_data_ebpf(struct packet_sock *po, char __user *data,\r\nunsigned int len)\r\n{\r\nstruct bpf_prog *new;\r\nu32 fd;\r\nif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\r\nreturn -EPERM;\r\nif (len != sizeof(fd))\r\nreturn -EINVAL;\r\nif (copy_from_user(&fd, data, len))\r\nreturn -EFAULT;\r\nnew = bpf_prog_get(fd);\r\nif (IS_ERR(new))\r\nreturn PTR_ERR(new);\r\nif (new->type != BPF_PROG_TYPE_SOCKET_FILTER) {\r\nbpf_prog_put(new);\r\nreturn -EINVAL;\r\n}\r\n__fanout_set_data_bpf(po->fanout, new);\r\nreturn 0;\r\n}\r\nstatic int fanout_set_data(struct packet_sock *po, char __user *data,\r\nunsigned int len)\r\n{\r\nswitch (po->fanout->type) {\r\ncase PACKET_FANOUT_CBPF:\r\nreturn fanout_set_data_cbpf(po, data, len);\r\ncase PACKET_FANOUT_EBPF:\r\nreturn fanout_set_data_ebpf(po, data, len);\r\ndefault:\r\nreturn -EINVAL;\r\n};\r\n}\r\nstatic void fanout_release_data(struct packet_fanout *f)\r\n{\r\nswitch (f->type) {\r\ncase PACKET_FANOUT_CBPF:\r\ncase PACKET_FANOUT_EBPF:\r\n__fanout_set_data_bpf(f, NULL);\r\n};\r\n}\r\nstatic int fanout_add(struct sock *sk, u16 id, u16 type_flags)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\nstruct packet_fanout *f, *match;\r\nu8 type = type_flags & 0xff;\r\nu8 flags = type_flags >> 8;\r\nint err;\r\nswitch (type) {\r\ncase PACKET_FANOUT_ROLLOVER:\r\nif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\r\nreturn -EINVAL;\r\ncase PACKET_FANOUT_HASH:\r\ncase PACKET_FANOUT_LB:\r\ncase PACKET_FANOUT_CPU:\r\ncase PACKET_FANOUT_RND:\r\ncase PACKET_FANOUT_QM:\r\ncase PACKET_FANOUT_CBPF:\r\ncase PACKET_FANOUT_EBPF:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (!po->running)\r\nreturn -EINVAL;\r\nif (po->fanout)\r\nreturn -EALREADY;\r\nif (type == PACKET_FANOUT_ROLLOVER ||\r\n(type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\r\npo->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);\r\nif (!po->rollover)\r\nreturn -ENOMEM;\r\natomic_long_set(&po->rollover->num, 0);\r\natomic_long_set(&po->rollover->num_huge, 0);\r\natomic_long_set(&po->rollover->num_failed, 0);\r\n}\r\nmutex_lock(&fanout_mutex);\r\nmatch = NULL;\r\nlist_for_each_entry(f, &fanout_list, list) {\r\nif (f->id == id &&\r\nread_pnet(&f->net) == sock_net(sk)) {\r\nmatch = f;\r\nbreak;\r\n}\r\n}\r\nerr = -EINVAL;\r\nif (match && match->flags != flags)\r\ngoto out;\r\nif (!match) {\r\nerr = -ENOMEM;\r\nmatch = kzalloc(sizeof(*match), GFP_KERNEL);\r\nif (!match)\r\ngoto out;\r\nwrite_pnet(&match->net, sock_net(sk));\r\nmatch->id = id;\r\nmatch->type = type;\r\nmatch->flags = flags;\r\nINIT_LIST_HEAD(&match->list);\r\nspin_lock_init(&match->lock);\r\natomic_set(&match->sk_ref, 0);\r\nfanout_init_data(match);\r\nmatch->prot_hook.type = po->prot_hook.type;\r\nmatch->prot_hook.dev = po->prot_hook.dev;\r\nmatch->prot_hook.func = packet_rcv_fanout;\r\nmatch->prot_hook.af_packet_priv = match;\r\nmatch->prot_hook.id_match = match_fanout_group;\r\ndev_add_pack(&match->prot_hook);\r\nlist_add(&match->list, &fanout_list);\r\n}\r\nerr = -EINVAL;\r\nif (match->type == type &&\r\nmatch->prot_hook.type == po->prot_hook.type &&\r\nmatch->prot_hook.dev == po->prot_hook.dev) {\r\nerr = -ENOSPC;\r\nif (atomic_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\r\n__dev_remove_pack(&po->prot_hook);\r\npo->fanout = match;\r\natomic_inc(&match->sk_ref);\r\n__fanout_link(sk, po);\r\nerr = 0;\r\n}\r\n}\r\nout:\r\nmutex_unlock(&fanout_mutex);\r\nif (err) {\r\nkfree(po->rollover);\r\npo->rollover = NULL;\r\n}\r\nreturn err;\r\n}\r\nstatic void fanout_release(struct sock *sk)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\nstruct packet_fanout *f;\r\nf = po->fanout;\r\nif (!f)\r\nreturn;\r\nmutex_lock(&fanout_mutex);\r\npo->fanout = NULL;\r\nif (atomic_dec_and_test(&f->sk_ref)) {\r\nlist_del(&f->list);\r\ndev_remove_pack(&f->prot_hook);\r\nfanout_release_data(f);\r\nkfree(f);\r\n}\r\nmutex_unlock(&fanout_mutex);\r\nif (po->rollover)\r\nkfree_rcu(po->rollover, rcu);\r\n}\r\nstatic bool packet_extra_vlan_len_allowed(const struct net_device *dev,\r\nstruct sk_buff *skb)\r\n{\r\nif (unlikely(dev->type != ARPHRD_ETHER))\r\nreturn false;\r\nskb_reset_mac_header(skb);\r\nreturn likely(eth_hdr(skb)->h_proto == htons(ETH_P_8021Q));\r\n}\r\nstatic int packet_rcv_spkt(struct sk_buff *skb, struct net_device *dev,\r\nstruct packet_type *pt, struct net_device *orig_dev)\r\n{\r\nstruct sock *sk;\r\nstruct sockaddr_pkt *spkt;\r\nsk = pt->af_packet_priv;\r\nif (skb->pkt_type == PACKET_LOOPBACK)\r\ngoto out;\r\nif (!net_eq(dev_net(dev), sock_net(sk)))\r\ngoto out;\r\nskb = skb_share_check(skb, GFP_ATOMIC);\r\nif (skb == NULL)\r\ngoto oom;\r\nskb_dst_drop(skb);\r\nnf_reset(skb);\r\nspkt = &PACKET_SKB_CB(skb)->sa.pkt;\r\nskb_push(skb, skb->data - skb_mac_header(skb));\r\nspkt->spkt_family = dev->type;\r\nstrlcpy(spkt->spkt_device, dev->name, sizeof(spkt->spkt_device));\r\nspkt->spkt_protocol = skb->protocol;\r\nif (sock_queue_rcv_skb(sk, skb) == 0)\r\nreturn 0;\r\nout:\r\nkfree_skb(skb);\r\noom:\r\nreturn 0;\r\n}\r\nstatic int packet_sendmsg_spkt(struct socket *sock, struct msghdr *msg,\r\nsize_t len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nDECLARE_SOCKADDR(struct sockaddr_pkt *, saddr, msg->msg_name);\r\nstruct sk_buff *skb = NULL;\r\nstruct net_device *dev;\r\n__be16 proto = 0;\r\nint err;\r\nint extra_len = 0;\r\nif (saddr) {\r\nif (msg->msg_namelen < sizeof(struct sockaddr))\r\nreturn -EINVAL;\r\nif (msg->msg_namelen == sizeof(struct sockaddr_pkt))\r\nproto = saddr->spkt_protocol;\r\n} else\r\nreturn -ENOTCONN;\r\nsaddr->spkt_device[sizeof(saddr->spkt_device) - 1] = 0;\r\nretry:\r\nrcu_read_lock();\r\ndev = dev_get_by_name_rcu(sock_net(sk), saddr->spkt_device);\r\nerr = -ENODEV;\r\nif (dev == NULL)\r\ngoto out_unlock;\r\nerr = -ENETDOWN;\r\nif (!(dev->flags & IFF_UP))\r\ngoto out_unlock;\r\nif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\r\nif (!netif_supports_nofcs(dev)) {\r\nerr = -EPROTONOSUPPORT;\r\ngoto out_unlock;\r\n}\r\nextra_len = 4;\r\n}\r\nerr = -EMSGSIZE;\r\nif (len > dev->mtu + dev->hard_header_len + VLAN_HLEN + extra_len)\r\ngoto out_unlock;\r\nif (!skb) {\r\nsize_t reserved = LL_RESERVED_SPACE(dev);\r\nint tlen = dev->needed_tailroom;\r\nunsigned int hhlen = dev->header_ops ? dev->hard_header_len : 0;\r\nrcu_read_unlock();\r\nskb = sock_wmalloc(sk, len + reserved + tlen, 0, GFP_KERNEL);\r\nif (skb == NULL)\r\nreturn -ENOBUFS;\r\nskb_reserve(skb, reserved);\r\nskb_reset_network_header(skb);\r\nif (hhlen) {\r\nskb->data -= hhlen;\r\nskb->tail -= hhlen;\r\nif (len < hhlen)\r\nskb_reset_network_header(skb);\r\n}\r\nerr = memcpy_from_msg(skb_put(skb, len), msg, len);\r\nif (err)\r\ngoto out_free;\r\ngoto retry;\r\n}\r\nif (len > (dev->mtu + dev->hard_header_len + extra_len) &&\r\n!packet_extra_vlan_len_allowed(dev, skb)) {\r\nerr = -EMSGSIZE;\r\ngoto out_unlock;\r\n}\r\nskb->protocol = proto;\r\nskb->dev = dev;\r\nskb->priority = sk->sk_priority;\r\nskb->mark = sk->sk_mark;\r\nsock_tx_timestamp(sk, &skb_shinfo(skb)->tx_flags);\r\nif (unlikely(extra_len == 4))\r\nskb->no_fcs = 1;\r\nskb_probe_transport_header(skb, 0);\r\ndev_queue_xmit(skb);\r\nrcu_read_unlock();\r\nreturn len;\r\nout_unlock:\r\nrcu_read_unlock();\r\nout_free:\r\nkfree_skb(skb);\r\nreturn err;\r\n}\r\nstatic unsigned int run_filter(struct sk_buff *skb,\r\nconst struct sock *sk,\r\nunsigned int res)\r\n{\r\nstruct sk_filter *filter;\r\nrcu_read_lock();\r\nfilter = rcu_dereference(sk->sk_filter);\r\nif (filter != NULL)\r\nres = bpf_prog_run_clear_cb(filter->prog, skb);\r\nrcu_read_unlock();\r\nreturn res;\r\n}\r\nstatic int packet_rcv(struct sk_buff *skb, struct net_device *dev,\r\nstruct packet_type *pt, struct net_device *orig_dev)\r\n{\r\nstruct sock *sk;\r\nstruct sockaddr_ll *sll;\r\nstruct packet_sock *po;\r\nu8 *skb_head = skb->data;\r\nint skb_len = skb->len;\r\nunsigned int snaplen, res;\r\nif (skb->pkt_type == PACKET_LOOPBACK)\r\ngoto drop;\r\nsk = pt->af_packet_priv;\r\npo = pkt_sk(sk);\r\nif (!net_eq(dev_net(dev), sock_net(sk)))\r\ngoto drop;\r\nskb->dev = dev;\r\nif (dev->header_ops) {\r\nif (sk->sk_type != SOCK_DGRAM)\r\nskb_push(skb, skb->data - skb_mac_header(skb));\r\nelse if (skb->pkt_type == PACKET_OUTGOING) {\r\nskb_pull(skb, skb_network_offset(skb));\r\n}\r\n}\r\nsnaplen = skb->len;\r\nres = run_filter(skb, sk, snaplen);\r\nif (!res)\r\ngoto drop_n_restore;\r\nif (snaplen > res)\r\nsnaplen = res;\r\nif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\r\ngoto drop_n_acct;\r\nif (skb_shared(skb)) {\r\nstruct sk_buff *nskb = skb_clone(skb, GFP_ATOMIC);\r\nif (nskb == NULL)\r\ngoto drop_n_acct;\r\nif (skb_head != skb->data) {\r\nskb->data = skb_head;\r\nskb->len = skb_len;\r\n}\r\nconsume_skb(skb);\r\nskb = nskb;\r\n}\r\nsock_skb_cb_check_size(sizeof(*PACKET_SKB_CB(skb)) + MAX_ADDR_LEN - 8);\r\nsll = &PACKET_SKB_CB(skb)->sa.ll;\r\nsll->sll_hatype = dev->type;\r\nsll->sll_pkttype = skb->pkt_type;\r\nif (unlikely(po->origdev))\r\nsll->sll_ifindex = orig_dev->ifindex;\r\nelse\r\nsll->sll_ifindex = dev->ifindex;\r\nsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\r\nPACKET_SKB_CB(skb)->sa.origlen = skb->len;\r\nif (pskb_trim(skb, snaplen))\r\ngoto drop_n_acct;\r\nskb_set_owner_r(skb, sk);\r\nskb->dev = NULL;\r\nskb_dst_drop(skb);\r\nnf_reset(skb);\r\nspin_lock(&sk->sk_receive_queue.lock);\r\npo->stats.stats1.tp_packets++;\r\nsock_skb_set_dropcount(sk, skb);\r\n__skb_queue_tail(&sk->sk_receive_queue, skb);\r\nspin_unlock(&sk->sk_receive_queue.lock);\r\nsk->sk_data_ready(sk);\r\nreturn 0;\r\ndrop_n_acct:\r\nspin_lock(&sk->sk_receive_queue.lock);\r\npo->stats.stats1.tp_drops++;\r\natomic_inc(&sk->sk_drops);\r\nspin_unlock(&sk->sk_receive_queue.lock);\r\ndrop_n_restore:\r\nif (skb_head != skb->data && skb_shared(skb)) {\r\nskb->data = skb_head;\r\nskb->len = skb_len;\r\n}\r\ndrop:\r\nconsume_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\r\nstruct packet_type *pt, struct net_device *orig_dev)\r\n{\r\nstruct sock *sk;\r\nstruct packet_sock *po;\r\nstruct sockaddr_ll *sll;\r\nunion tpacket_uhdr h;\r\nu8 *skb_head = skb->data;\r\nint skb_len = skb->len;\r\nunsigned int snaplen, res;\r\nunsigned long status = TP_STATUS_USER;\r\nunsigned short macoff, netoff, hdrlen;\r\nstruct sk_buff *copy_skb = NULL;\r\nstruct timespec ts;\r\n__u32 ts_status;\r\nBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\r\nBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\r\nif (skb->pkt_type == PACKET_LOOPBACK)\r\ngoto drop;\r\nsk = pt->af_packet_priv;\r\npo = pkt_sk(sk);\r\nif (!net_eq(dev_net(dev), sock_net(sk)))\r\ngoto drop;\r\nif (dev->header_ops) {\r\nif (sk->sk_type != SOCK_DGRAM)\r\nskb_push(skb, skb->data - skb_mac_header(skb));\r\nelse if (skb->pkt_type == PACKET_OUTGOING) {\r\nskb_pull(skb, skb_network_offset(skb));\r\n}\r\n}\r\nsnaplen = skb->len;\r\nres = run_filter(skb, sk, snaplen);\r\nif (!res)\r\ngoto drop_n_restore;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\nstatus |= TP_STATUS_CSUMNOTREADY;\r\nelse if (skb->pkt_type != PACKET_OUTGOING &&\r\n(skb->ip_summed == CHECKSUM_COMPLETE ||\r\nskb_csum_unnecessary(skb)))\r\nstatus |= TP_STATUS_CSUM_VALID;\r\nif (snaplen > res)\r\nsnaplen = res;\r\nif (sk->sk_type == SOCK_DGRAM) {\r\nmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\r\npo->tp_reserve;\r\n} else {\r\nunsigned int maclen = skb_network_offset(skb);\r\nnetoff = TPACKET_ALIGN(po->tp_hdrlen +\r\n(maclen < 16 ? 16 : maclen)) +\r\npo->tp_reserve;\r\nmacoff = netoff - maclen;\r\n}\r\nif (po->tp_version <= TPACKET_V2) {\r\nif (macoff + snaplen > po->rx_ring.frame_size) {\r\nif (po->copy_thresh &&\r\natomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\r\nif (skb_shared(skb)) {\r\ncopy_skb = skb_clone(skb, GFP_ATOMIC);\r\n} else {\r\ncopy_skb = skb_get(skb);\r\nskb_head = skb->data;\r\n}\r\nif (copy_skb)\r\nskb_set_owner_r(copy_skb, sk);\r\n}\r\nsnaplen = po->rx_ring.frame_size - macoff;\r\nif ((int)snaplen < 0)\r\nsnaplen = 0;\r\n}\r\n} else if (unlikely(macoff + snaplen >\r\nGET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\r\nu32 nval;\r\nnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\r\npr_err_once("tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\n",\r\nsnaplen, nval, macoff);\r\nsnaplen = nval;\r\nif (unlikely((int)snaplen < 0)) {\r\nsnaplen = 0;\r\nmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\r\n}\r\n}\r\nspin_lock(&sk->sk_receive_queue.lock);\r\nh.raw = packet_current_rx_frame(po, skb,\r\nTP_STATUS_KERNEL, (macoff+snaplen));\r\nif (!h.raw)\r\ngoto ring_is_full;\r\nif (po->tp_version <= TPACKET_V2) {\r\npacket_increment_rx_head(po, &po->rx_ring);\r\nif (po->stats.stats1.tp_drops)\r\nstatus |= TP_STATUS_LOSING;\r\n}\r\npo->stats.stats1.tp_packets++;\r\nif (copy_skb) {\r\nstatus |= TP_STATUS_COPY;\r\n__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\r\n}\r\nspin_unlock(&sk->sk_receive_queue.lock);\r\nskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\r\nif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\r\ngetnstimeofday(&ts);\r\nstatus |= ts_status;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\nh.h1->tp_len = skb->len;\r\nh.h1->tp_snaplen = snaplen;\r\nh.h1->tp_mac = macoff;\r\nh.h1->tp_net = netoff;\r\nh.h1->tp_sec = ts.tv_sec;\r\nh.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\r\nhdrlen = sizeof(*h.h1);\r\nbreak;\r\ncase TPACKET_V2:\r\nh.h2->tp_len = skb->len;\r\nh.h2->tp_snaplen = snaplen;\r\nh.h2->tp_mac = macoff;\r\nh.h2->tp_net = netoff;\r\nh.h2->tp_sec = ts.tv_sec;\r\nh.h2->tp_nsec = ts.tv_nsec;\r\nif (skb_vlan_tag_present(skb)) {\r\nh.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\r\nh.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\r\nstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\r\n} else {\r\nh.h2->tp_vlan_tci = 0;\r\nh.h2->tp_vlan_tpid = 0;\r\n}\r\nmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\r\nhdrlen = sizeof(*h.h2);\r\nbreak;\r\ncase TPACKET_V3:\r\nh.h3->tp_status |= status;\r\nh.h3->tp_len = skb->len;\r\nh.h3->tp_snaplen = snaplen;\r\nh.h3->tp_mac = macoff;\r\nh.h3->tp_net = netoff;\r\nh.h3->tp_sec = ts.tv_sec;\r\nh.h3->tp_nsec = ts.tv_nsec;\r\nmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\r\nhdrlen = sizeof(*h.h3);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nsll = h.raw + TPACKET_ALIGN(hdrlen);\r\nsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\r\nsll->sll_family = AF_PACKET;\r\nsll->sll_hatype = dev->type;\r\nsll->sll_protocol = skb->protocol;\r\nsll->sll_pkttype = skb->pkt_type;\r\nif (unlikely(po->origdev))\r\nsll->sll_ifindex = orig_dev->ifindex;\r\nelse\r\nsll->sll_ifindex = dev->ifindex;\r\nsmp_mb();\r\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\r\nif (po->tp_version <= TPACKET_V2) {\r\nu8 *start, *end;\r\nend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\r\nmacoff + snaplen);\r\nfor (start = h.raw; start < end; start += PAGE_SIZE)\r\nflush_dcache_page(pgv_to_page(start));\r\n}\r\nsmp_wmb();\r\n#endif\r\nif (po->tp_version <= TPACKET_V2) {\r\n__packet_set_status(po, h.raw, status);\r\nsk->sk_data_ready(sk);\r\n} else {\r\nprb_clear_blk_fill_status(&po->rx_ring);\r\n}\r\ndrop_n_restore:\r\nif (skb_head != skb->data && skb_shared(skb)) {\r\nskb->data = skb_head;\r\nskb->len = skb_len;\r\n}\r\ndrop:\r\nkfree_skb(skb);\r\nreturn 0;\r\nring_is_full:\r\npo->stats.stats1.tp_drops++;\r\nspin_unlock(&sk->sk_receive_queue.lock);\r\nsk->sk_data_ready(sk);\r\nkfree_skb(copy_skb);\r\ngoto drop_n_restore;\r\n}\r\nstatic void tpacket_destruct_skb(struct sk_buff *skb)\r\n{\r\nstruct packet_sock *po = pkt_sk(skb->sk);\r\nif (likely(po->tx_ring.pg_vec)) {\r\nvoid *ph;\r\n__u32 ts;\r\nph = skb_shinfo(skb)->destructor_arg;\r\npacket_dec_pending(&po->tx_ring);\r\nts = __packet_set_timestamp(po, ph, skb);\r\n__packet_set_status(po, ph, TP_STATUS_AVAILABLE | ts);\r\n}\r\nsock_wfree(skb);\r\n}\r\nstatic bool ll_header_truncated(const struct net_device *dev, int len)\r\n{\r\nif (unlikely(len < dev->hard_header_len)) {\r\nnet_warn_ratelimited("%s: packet size is too short (%d < %d)\n",\r\ncurrent->comm, len, dev->hard_header_len);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void tpacket_set_protocol(const struct net_device *dev,\r\nstruct sk_buff *skb)\r\n{\r\nif (dev->type == ARPHRD_ETHER) {\r\nskb_reset_mac_header(skb);\r\nskb->protocol = eth_hdr(skb)->h_proto;\r\n}\r\n}\r\nstatic int tpacket_fill_skb(struct packet_sock *po, struct sk_buff *skb,\r\nvoid *frame, struct net_device *dev, int size_max,\r\n__be16 proto, unsigned char *addr, int hlen)\r\n{\r\nunion tpacket_uhdr ph;\r\nint to_write, offset, len, tp_len, nr_frags, len_max;\r\nstruct socket *sock = po->sk.sk_socket;\r\nstruct page *page;\r\nvoid *data;\r\nint err;\r\nph.raw = frame;\r\nskb->protocol = proto;\r\nskb->dev = dev;\r\nskb->priority = po->sk.sk_priority;\r\nskb->mark = po->sk.sk_mark;\r\nsock_tx_timestamp(&po->sk, &skb_shinfo(skb)->tx_flags);\r\nskb_shinfo(skb)->destructor_arg = ph.raw;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V2:\r\ntp_len = ph.h2->tp_len;\r\nbreak;\r\ndefault:\r\ntp_len = ph.h1->tp_len;\r\nbreak;\r\n}\r\nif (unlikely(tp_len > size_max)) {\r\npr_err("packet size is too long (%d > %d)\n", tp_len, size_max);\r\nreturn -EMSGSIZE;\r\n}\r\nskb_reserve(skb, hlen);\r\nskb_reset_network_header(skb);\r\nif (unlikely(po->tp_tx_has_off)) {\r\nint off_min, off_max, off;\r\noff_min = po->tp_hdrlen - sizeof(struct sockaddr_ll);\r\noff_max = po->tx_ring.frame_size - tp_len;\r\nif (sock->type == SOCK_DGRAM) {\r\nswitch (po->tp_version) {\r\ncase TPACKET_V2:\r\noff = ph.h2->tp_net;\r\nbreak;\r\ndefault:\r\noff = ph.h1->tp_net;\r\nbreak;\r\n}\r\n} else {\r\nswitch (po->tp_version) {\r\ncase TPACKET_V2:\r\noff = ph.h2->tp_mac;\r\nbreak;\r\ndefault:\r\noff = ph.h1->tp_mac;\r\nbreak;\r\n}\r\n}\r\nif (unlikely((off < off_min) || (off_max < off)))\r\nreturn -EINVAL;\r\ndata = ph.raw + off;\r\n} else {\r\ndata = ph.raw + po->tp_hdrlen - sizeof(struct sockaddr_ll);\r\n}\r\nto_write = tp_len;\r\nif (sock->type == SOCK_DGRAM) {\r\nerr = dev_hard_header(skb, dev, ntohs(proto), addr,\r\nNULL, tp_len);\r\nif (unlikely(err < 0))\r\nreturn -EINVAL;\r\n} else if (dev->hard_header_len) {\r\nif (ll_header_truncated(dev, tp_len))\r\nreturn -EINVAL;\r\nskb_push(skb, dev->hard_header_len);\r\nerr = skb_store_bits(skb, 0, data,\r\ndev->hard_header_len);\r\nif (unlikely(err))\r\nreturn err;\r\nif (!skb->protocol)\r\ntpacket_set_protocol(dev, skb);\r\ndata += dev->hard_header_len;\r\nto_write -= dev->hard_header_len;\r\n}\r\noffset = offset_in_page(data);\r\nlen_max = PAGE_SIZE - offset;\r\nlen = ((to_write > len_max) ? len_max : to_write);\r\nskb->data_len = to_write;\r\nskb->len += to_write;\r\nskb->truesize += to_write;\r\natomic_add(to_write, &po->sk.sk_wmem_alloc);\r\nwhile (likely(to_write)) {\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\nif (unlikely(nr_frags >= MAX_SKB_FRAGS)) {\r\npr_err("Packet exceed the number of skb frags(%lu)\n",\r\nMAX_SKB_FRAGS);\r\nreturn -EFAULT;\r\n}\r\npage = pgv_to_page(data);\r\ndata += len;\r\nflush_dcache_page(page);\r\nget_page(page);\r\nskb_fill_page_desc(skb, nr_frags, page, offset, len);\r\nto_write -= len;\r\noffset = 0;\r\nlen_max = PAGE_SIZE;\r\nlen = ((to_write > len_max) ? len_max : to_write);\r\n}\r\nskb_probe_transport_header(skb, 0);\r\nreturn tp_len;\r\n}\r\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg)\r\n{\r\nstruct sk_buff *skb;\r\nstruct net_device *dev;\r\n__be16 proto;\r\nint err, reserve = 0;\r\nvoid *ph;\r\nDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\r\nbool need_wait = !(msg->msg_flags & MSG_DONTWAIT);\r\nint tp_len, size_max;\r\nunsigned char *addr;\r\nint len_sum = 0;\r\nint status = TP_STATUS_AVAILABLE;\r\nint hlen, tlen;\r\nmutex_lock(&po->pg_vec_lock);\r\nif (likely(saddr == NULL)) {\r\ndev = packet_cached_dev_get(po);\r\nproto = po->num;\r\naddr = NULL;\r\n} else {\r\nerr = -EINVAL;\r\nif (msg->msg_namelen < sizeof(struct sockaddr_ll))\r\ngoto out;\r\nif (msg->msg_namelen < (saddr->sll_halen\r\n+ offsetof(struct sockaddr_ll,\r\nsll_addr)))\r\ngoto out;\r\nproto = saddr->sll_protocol;\r\naddr = saddr->sll_addr;\r\ndev = dev_get_by_index(sock_net(&po->sk), saddr->sll_ifindex);\r\n}\r\nerr = -ENXIO;\r\nif (unlikely(dev == NULL))\r\ngoto out;\r\nerr = -ENETDOWN;\r\nif (unlikely(!(dev->flags & IFF_UP)))\r\ngoto out_put;\r\nif (po->sk.sk_socket->type == SOCK_RAW)\r\nreserve = dev->hard_header_len;\r\nsize_max = po->tx_ring.frame_size\r\n- (po->tp_hdrlen - sizeof(struct sockaddr_ll));\r\nif (size_max > dev->mtu + reserve + VLAN_HLEN)\r\nsize_max = dev->mtu + reserve + VLAN_HLEN;\r\ndo {\r\nph = packet_current_frame(po, &po->tx_ring,\r\nTP_STATUS_SEND_REQUEST);\r\nif (unlikely(ph == NULL)) {\r\nif (need_wait && need_resched())\r\nschedule();\r\ncontinue;\r\n}\r\nstatus = TP_STATUS_SEND_REQUEST;\r\nhlen = LL_RESERVED_SPACE(dev);\r\ntlen = dev->needed_tailroom;\r\nskb = sock_alloc_send_skb(&po->sk,\r\nhlen + tlen + sizeof(struct sockaddr_ll),\r\n!need_wait, &err);\r\nif (unlikely(skb == NULL)) {\r\nif (likely(len_sum > 0))\r\nerr = len_sum;\r\ngoto out_status;\r\n}\r\ntp_len = tpacket_fill_skb(po, skb, ph, dev, size_max, proto,\r\naddr, hlen);\r\nif (likely(tp_len >= 0) &&\r\ntp_len > dev->mtu + reserve &&\r\n!packet_extra_vlan_len_allowed(dev, skb))\r\ntp_len = -EMSGSIZE;\r\nif (unlikely(tp_len < 0)) {\r\nif (po->tp_loss) {\r\n__packet_set_status(po, ph,\r\nTP_STATUS_AVAILABLE);\r\npacket_increment_head(&po->tx_ring);\r\nkfree_skb(skb);\r\ncontinue;\r\n} else {\r\nstatus = TP_STATUS_WRONG_FORMAT;\r\nerr = tp_len;\r\ngoto out_status;\r\n}\r\n}\r\npacket_pick_tx_queue(dev, skb);\r\nskb->destructor = tpacket_destruct_skb;\r\n__packet_set_status(po, ph, TP_STATUS_SENDING);\r\npacket_inc_pending(&po->tx_ring);\r\nstatus = TP_STATUS_SEND_REQUEST;\r\nerr = po->xmit(skb);\r\nif (unlikely(err > 0)) {\r\nerr = net_xmit_errno(err);\r\nif (err && __packet_get_status(po, ph) ==\r\nTP_STATUS_AVAILABLE) {\r\nskb = NULL;\r\ngoto out_status;\r\n}\r\nerr = 0;\r\n}\r\npacket_increment_head(&po->tx_ring);\r\nlen_sum += tp_len;\r\n} while (likely((ph != NULL) ||\r\n(need_wait && packet_read_pending(&po->tx_ring))));\r\nerr = len_sum;\r\ngoto out_put;\r\nout_status:\r\n__packet_set_status(po, ph, status);\r\nkfree_skb(skb);\r\nout_put:\r\ndev_put(dev);\r\nout:\r\nmutex_unlock(&po->pg_vec_lock);\r\nreturn err;\r\n}\r\nstatic struct sk_buff *packet_alloc_skb(struct sock *sk, size_t prepad,\r\nsize_t reserve, size_t len,\r\nsize_t linear, int noblock,\r\nint *err)\r\n{\r\nstruct sk_buff *skb;\r\nif (prepad + len < PAGE_SIZE || !linear)\r\nlinear = len;\r\nskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\r\nerr, 0);\r\nif (!skb)\r\nreturn NULL;\r\nskb_reserve(skb, reserve);\r\nskb_put(skb, linear);\r\nskb->data_len = len - linear;\r\nskb->len += len - linear;\r\nreturn skb;\r\n}\r\nstatic int packet_snd(struct socket *sock, struct msghdr *msg, size_t len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\r\nstruct sk_buff *skb;\r\nstruct net_device *dev;\r\n__be16 proto;\r\nunsigned char *addr;\r\nint err, reserve = 0;\r\nstruct sockcm_cookie sockc;\r\nstruct virtio_net_hdr vnet_hdr = { 0 };\r\nint offset = 0;\r\nint vnet_hdr_len;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nunsigned short gso_type = 0;\r\nint hlen, tlen;\r\nint extra_len = 0;\r\nssize_t n;\r\nif (likely(saddr == NULL)) {\r\ndev = packet_cached_dev_get(po);\r\nproto = po->num;\r\naddr = NULL;\r\n} else {\r\nerr = -EINVAL;\r\nif (msg->msg_namelen < sizeof(struct sockaddr_ll))\r\ngoto out;\r\nif (msg->msg_namelen < (saddr->sll_halen + offsetof(struct sockaddr_ll, sll_addr)))\r\ngoto out;\r\nproto = saddr->sll_protocol;\r\naddr = saddr->sll_addr;\r\ndev = dev_get_by_index(sock_net(sk), saddr->sll_ifindex);\r\n}\r\nerr = -ENXIO;\r\nif (unlikely(dev == NULL))\r\ngoto out_unlock;\r\nerr = -ENETDOWN;\r\nif (unlikely(!(dev->flags & IFF_UP)))\r\ngoto out_unlock;\r\nsockc.mark = sk->sk_mark;\r\nif (msg->msg_controllen) {\r\nerr = sock_cmsg_send(sk, msg, &sockc);\r\nif (unlikely(err))\r\ngoto out_unlock;\r\n}\r\nif (sock->type == SOCK_RAW)\r\nreserve = dev->hard_header_len;\r\nif (po->has_vnet_hdr) {\r\nvnet_hdr_len = sizeof(vnet_hdr);\r\nerr = -EINVAL;\r\nif (len < vnet_hdr_len)\r\ngoto out_unlock;\r\nlen -= vnet_hdr_len;\r\nerr = -EFAULT;\r\nn = copy_from_iter(&vnet_hdr, vnet_hdr_len, &msg->msg_iter);\r\nif (n != vnet_hdr_len)\r\ngoto out_unlock;\r\nif ((vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\r\n(__virtio16_to_cpu(vio_le(), vnet_hdr.csum_start) +\r\n__virtio16_to_cpu(vio_le(), vnet_hdr.csum_offset) + 2 >\r\n__virtio16_to_cpu(vio_le(), vnet_hdr.hdr_len)))\r\nvnet_hdr.hdr_len = __cpu_to_virtio16(vio_le(),\r\n__virtio16_to_cpu(vio_le(), vnet_hdr.csum_start) +\r\n__virtio16_to_cpu(vio_le(), vnet_hdr.csum_offset) + 2);\r\nerr = -EINVAL;\r\nif (__virtio16_to_cpu(vio_le(), vnet_hdr.hdr_len) > len)\r\ngoto out_unlock;\r\nif (vnet_hdr.gso_type != VIRTIO_NET_HDR_GSO_NONE) {\r\nswitch (vnet_hdr.gso_type & ~VIRTIO_NET_HDR_GSO_ECN) {\r\ncase VIRTIO_NET_HDR_GSO_TCPV4:\r\ngso_type = SKB_GSO_TCPV4;\r\nbreak;\r\ncase VIRTIO_NET_HDR_GSO_TCPV6:\r\ngso_type = SKB_GSO_TCPV6;\r\nbreak;\r\ncase VIRTIO_NET_HDR_GSO_UDP:\r\ngso_type = SKB_GSO_UDP;\r\nbreak;\r\ndefault:\r\ngoto out_unlock;\r\n}\r\nif (vnet_hdr.gso_type & VIRTIO_NET_HDR_GSO_ECN)\r\ngso_type |= SKB_GSO_TCP_ECN;\r\nif (vnet_hdr.gso_size == 0)\r\ngoto out_unlock;\r\n}\r\n}\r\nif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\r\nif (!netif_supports_nofcs(dev)) {\r\nerr = -EPROTONOSUPPORT;\r\ngoto out_unlock;\r\n}\r\nextra_len = 4;\r\n}\r\nerr = -EMSGSIZE;\r\nif (!gso_type && (len > dev->mtu + reserve + VLAN_HLEN + extra_len))\r\ngoto out_unlock;\r\nerr = -ENOBUFS;\r\nhlen = LL_RESERVED_SPACE(dev);\r\ntlen = dev->needed_tailroom;\r\nskb = packet_alloc_skb(sk, hlen + tlen, hlen, len,\r\n__virtio16_to_cpu(vio_le(), vnet_hdr.hdr_len),\r\nmsg->msg_flags & MSG_DONTWAIT, &err);\r\nif (skb == NULL)\r\ngoto out_unlock;\r\nskb_set_network_header(skb, reserve);\r\nerr = -EINVAL;\r\nif (sock->type == SOCK_DGRAM) {\r\noffset = dev_hard_header(skb, dev, ntohs(proto), addr, NULL, len);\r\nif (unlikely(offset < 0))\r\ngoto out_free;\r\n} else {\r\nif (ll_header_truncated(dev, len))\r\ngoto out_free;\r\n}\r\nerr = skb_copy_datagram_from_iter(skb, offset, &msg->msg_iter, len);\r\nif (err)\r\ngoto out_free;\r\nsock_tx_timestamp(sk, &skb_shinfo(skb)->tx_flags);\r\nif (!gso_type && (len > dev->mtu + reserve + extra_len) &&\r\n!packet_extra_vlan_len_allowed(dev, skb)) {\r\nerr = -EMSGSIZE;\r\ngoto out_free;\r\n}\r\nskb->protocol = proto;\r\nskb->dev = dev;\r\nskb->priority = sk->sk_priority;\r\nskb->mark = sockc.mark;\r\npacket_pick_tx_queue(dev, skb);\r\nif (po->has_vnet_hdr) {\r\nif (vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) {\r\nu16 s = __virtio16_to_cpu(vio_le(), vnet_hdr.csum_start);\r\nu16 o = __virtio16_to_cpu(vio_le(), vnet_hdr.csum_offset);\r\nif (!skb_partial_csum_set(skb, s, o)) {\r\nerr = -EINVAL;\r\ngoto out_free;\r\n}\r\n}\r\nskb_shinfo(skb)->gso_size =\r\n__virtio16_to_cpu(vio_le(), vnet_hdr.gso_size);\r\nskb_shinfo(skb)->gso_type = gso_type;\r\nskb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;\r\nskb_shinfo(skb)->gso_segs = 0;\r\nlen += vnet_hdr_len;\r\n}\r\nskb_probe_transport_header(skb, reserve);\r\nif (unlikely(extra_len == 4))\r\nskb->no_fcs = 1;\r\nerr = po->xmit(skb);\r\nif (err > 0 && (err = net_xmit_errno(err)) != 0)\r\ngoto out_unlock;\r\ndev_put(dev);\r\nreturn len;\r\nout_free:\r\nkfree_skb(skb);\r\nout_unlock:\r\nif (dev)\r\ndev_put(dev);\r\nout:\r\nreturn err;\r\n}\r\nstatic int packet_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nif (po->tx_ring.pg_vec)\r\nreturn tpacket_snd(po, msg);\r\nelse\r\nreturn packet_snd(sock, msg, len);\r\n}\r\nstatic int packet_release(struct socket *sock)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct packet_sock *po;\r\nstruct net *net;\r\nunion tpacket_req_u req_u;\r\nif (!sk)\r\nreturn 0;\r\nnet = sock_net(sk);\r\npo = pkt_sk(sk);\r\nmutex_lock(&net->packet.sklist_lock);\r\nsk_del_node_init_rcu(sk);\r\nmutex_unlock(&net->packet.sklist_lock);\r\npreempt_disable();\r\nsock_prot_inuse_add(net, sk->sk_prot, -1);\r\npreempt_enable();\r\nspin_lock(&po->bind_lock);\r\nunregister_prot_hook(sk, false);\r\npacket_cached_dev_reset(po);\r\nif (po->prot_hook.dev) {\r\ndev_put(po->prot_hook.dev);\r\npo->prot_hook.dev = NULL;\r\n}\r\nspin_unlock(&po->bind_lock);\r\npacket_flush_mclist(sk);\r\nif (po->rx_ring.pg_vec) {\r\nmemset(&req_u, 0, sizeof(req_u));\r\npacket_set_ring(sk, &req_u, 1, 0);\r\n}\r\nif (po->tx_ring.pg_vec) {\r\nmemset(&req_u, 0, sizeof(req_u));\r\npacket_set_ring(sk, &req_u, 1, 1);\r\n}\r\nfanout_release(sk);\r\nsynchronize_net();\r\nsock_orphan(sk);\r\nsock->sk = NULL;\r\nskb_queue_purge(&sk->sk_receive_queue);\r\npacket_free_pending(po);\r\nsk_refcnt_debug_release(sk);\r\nsock_put(sk);\r\nreturn 0;\r\n}\r\nstatic int packet_do_bind(struct sock *sk, const char *name, int ifindex,\r\n__be16 proto)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\nstruct net_device *dev_curr;\r\n__be16 proto_curr;\r\nbool need_rehook;\r\nstruct net_device *dev = NULL;\r\nint ret = 0;\r\nbool unlisted = false;\r\nif (po->fanout)\r\nreturn -EINVAL;\r\nlock_sock(sk);\r\nspin_lock(&po->bind_lock);\r\nrcu_read_lock();\r\nif (name) {\r\ndev = dev_get_by_name_rcu(sock_net(sk), name);\r\nif (!dev) {\r\nret = -ENODEV;\r\ngoto out_unlock;\r\n}\r\n} else if (ifindex) {\r\ndev = dev_get_by_index_rcu(sock_net(sk), ifindex);\r\nif (!dev) {\r\nret = -ENODEV;\r\ngoto out_unlock;\r\n}\r\n}\r\nif (dev)\r\ndev_hold(dev);\r\nproto_curr = po->prot_hook.type;\r\ndev_curr = po->prot_hook.dev;\r\nneed_rehook = proto_curr != proto || dev_curr != dev;\r\nif (need_rehook) {\r\nif (po->running) {\r\nrcu_read_unlock();\r\n__unregister_prot_hook(sk, true);\r\nrcu_read_lock();\r\ndev_curr = po->prot_hook.dev;\r\nif (dev)\r\nunlisted = !dev_get_by_index_rcu(sock_net(sk),\r\ndev->ifindex);\r\n}\r\npo->num = proto;\r\npo->prot_hook.type = proto;\r\nif (unlikely(unlisted)) {\r\ndev_put(dev);\r\npo->prot_hook.dev = NULL;\r\npo->ifindex = -1;\r\npacket_cached_dev_reset(po);\r\n} else {\r\npo->prot_hook.dev = dev;\r\npo->ifindex = dev ? dev->ifindex : 0;\r\npacket_cached_dev_assign(po, dev);\r\n}\r\n}\r\nif (dev_curr)\r\ndev_put(dev_curr);\r\nif (proto == 0 || !need_rehook)\r\ngoto out_unlock;\r\nif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\r\nregister_prot_hook(sk);\r\n} else {\r\nsk->sk_err = ENETDOWN;\r\nif (!sock_flag(sk, SOCK_DEAD))\r\nsk->sk_error_report(sk);\r\n}\r\nout_unlock:\r\nrcu_read_unlock();\r\nspin_unlock(&po->bind_lock);\r\nrelease_sock(sk);\r\nreturn ret;\r\n}\r\nstatic int packet_bind_spkt(struct socket *sock, struct sockaddr *uaddr,\r\nint addr_len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nchar name[15];\r\nif (addr_len != sizeof(struct sockaddr))\r\nreturn -EINVAL;\r\nstrlcpy(name, uaddr->sa_data, sizeof(name));\r\nreturn packet_do_bind(sk, name, 0, pkt_sk(sk)->num);\r\n}\r\nstatic int packet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\r\n{\r\nstruct sockaddr_ll *sll = (struct sockaddr_ll *)uaddr;\r\nstruct sock *sk = sock->sk;\r\nif (addr_len < sizeof(struct sockaddr_ll))\r\nreturn -EINVAL;\r\nif (sll->sll_family != AF_PACKET)\r\nreturn -EINVAL;\r\nreturn packet_do_bind(sk, NULL, sll->sll_ifindex,\r\nsll->sll_protocol ? : pkt_sk(sk)->num);\r\n}\r\nstatic int packet_create(struct net *net, struct socket *sock, int protocol,\r\nint kern)\r\n{\r\nstruct sock *sk;\r\nstruct packet_sock *po;\r\n__be16 proto = (__force __be16)protocol;\r\nint err;\r\nif (!ns_capable(net->user_ns, CAP_NET_RAW))\r\nreturn -EPERM;\r\nif (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW &&\r\nsock->type != SOCK_PACKET)\r\nreturn -ESOCKTNOSUPPORT;\r\nsock->state = SS_UNCONNECTED;\r\nerr = -ENOBUFS;\r\nsk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto, kern);\r\nif (sk == NULL)\r\ngoto out;\r\nsock->ops = &packet_ops;\r\nif (sock->type == SOCK_PACKET)\r\nsock->ops = &packet_ops_spkt;\r\nsock_init_data(sock, sk);\r\npo = pkt_sk(sk);\r\nsk->sk_family = PF_PACKET;\r\npo->num = proto;\r\npo->xmit = dev_queue_xmit;\r\nerr = packet_alloc_pending(po);\r\nif (err)\r\ngoto out2;\r\npacket_cached_dev_reset(po);\r\nsk->sk_destruct = packet_sock_destruct;\r\nsk_refcnt_debug_inc(sk);\r\nspin_lock_init(&po->bind_lock);\r\nmutex_init(&po->pg_vec_lock);\r\npo->rollover = NULL;\r\npo->prot_hook.func = packet_rcv;\r\nif (sock->type == SOCK_PACKET)\r\npo->prot_hook.func = packet_rcv_spkt;\r\npo->prot_hook.af_packet_priv = sk;\r\nif (proto) {\r\npo->prot_hook.type = proto;\r\nregister_prot_hook(sk);\r\n}\r\nmutex_lock(&net->packet.sklist_lock);\r\nsk_add_node_rcu(sk, &net->packet.sklist);\r\nmutex_unlock(&net->packet.sklist_lock);\r\npreempt_disable();\r\nsock_prot_inuse_add(net, &packet_proto, 1);\r\npreempt_enable();\r\nreturn 0;\r\nout2:\r\nsk_free(sk);\r\nout:\r\nreturn err;\r\n}\r\nstatic int packet_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\r\nint flags)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct sk_buff *skb;\r\nint copied, err;\r\nint vnet_hdr_len = 0;\r\nunsigned int origlen = 0;\r\nerr = -EINVAL;\r\nif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\r\ngoto out;\r\n#if 0\r\nif (pkt_sk(sk)->ifindex < 0)\r\nreturn -ENODEV;\r\n#endif\r\nif (flags & MSG_ERRQUEUE) {\r\nerr = sock_recv_errqueue(sk, msg, len,\r\nSOL_PACKET, PACKET_TX_TIMESTAMP);\r\ngoto out;\r\n}\r\nskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\r\nif (skb == NULL)\r\ngoto out;\r\nif (pkt_sk(sk)->pressure)\r\npacket_rcv_has_room(pkt_sk(sk), NULL);\r\nif (pkt_sk(sk)->has_vnet_hdr) {\r\nstruct virtio_net_hdr vnet_hdr = { 0 };\r\nerr = -EINVAL;\r\nvnet_hdr_len = sizeof(vnet_hdr);\r\nif (len < vnet_hdr_len)\r\ngoto out_free;\r\nlen -= vnet_hdr_len;\r\nif (skb_is_gso(skb)) {\r\nstruct skb_shared_info *sinfo = skb_shinfo(skb);\r\nvnet_hdr.hdr_len =\r\n__cpu_to_virtio16(vio_le(), skb_headlen(skb));\r\nvnet_hdr.gso_size =\r\n__cpu_to_virtio16(vio_le(), sinfo->gso_size);\r\nif (sinfo->gso_type & SKB_GSO_TCPV4)\r\nvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\r\nelse if (sinfo->gso_type & SKB_GSO_TCPV6)\r\nvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\r\nelse if (sinfo->gso_type & SKB_GSO_UDP)\r\nvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_UDP;\r\nelse if (sinfo->gso_type & SKB_GSO_FCOE)\r\ngoto out_free;\r\nelse\r\nBUG();\r\nif (sinfo->gso_type & SKB_GSO_TCP_ECN)\r\nvnet_hdr.gso_type |= VIRTIO_NET_HDR_GSO_ECN;\r\n} else\r\nvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nvnet_hdr.flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;\r\nvnet_hdr.csum_start = __cpu_to_virtio16(vio_le(),\r\nskb_checksum_start_offset(skb));\r\nvnet_hdr.csum_offset = __cpu_to_virtio16(vio_le(),\r\nskb->csum_offset);\r\n} else if (skb->ip_summed == CHECKSUM_UNNECESSARY) {\r\nvnet_hdr.flags = VIRTIO_NET_HDR_F_DATA_VALID;\r\n}\r\nerr = memcpy_to_msg(msg, (void *)&vnet_hdr, vnet_hdr_len);\r\nif (err < 0)\r\ngoto out_free;\r\n}\r\ncopied = skb->len;\r\nif (copied > len) {\r\ncopied = len;\r\nmsg->msg_flags |= MSG_TRUNC;\r\n}\r\nerr = skb_copy_datagram_msg(skb, 0, msg, copied);\r\nif (err)\r\ngoto out_free;\r\nif (sock->type != SOCK_PACKET) {\r\nstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\r\noriglen = PACKET_SKB_CB(skb)->sa.origlen;\r\nsll->sll_family = AF_PACKET;\r\nsll->sll_protocol = skb->protocol;\r\n}\r\nsock_recv_ts_and_drops(msg, sk, skb);\r\nif (msg->msg_name) {\r\nif (sock->type == SOCK_PACKET) {\r\n__sockaddr_check_size(sizeof(struct sockaddr_pkt));\r\nmsg->msg_namelen = sizeof(struct sockaddr_pkt);\r\n} else {\r\nstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\r\nmsg->msg_namelen = sll->sll_halen +\r\noffsetof(struct sockaddr_ll, sll_addr);\r\n}\r\nmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\r\nmsg->msg_namelen);\r\n}\r\nif (pkt_sk(sk)->auxdata) {\r\nstruct tpacket_auxdata aux;\r\naux.tp_status = TP_STATUS_USER;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\naux.tp_status |= TP_STATUS_CSUMNOTREADY;\r\nelse if (skb->pkt_type != PACKET_OUTGOING &&\r\n(skb->ip_summed == CHECKSUM_COMPLETE ||\r\nskb_csum_unnecessary(skb)))\r\naux.tp_status |= TP_STATUS_CSUM_VALID;\r\naux.tp_len = origlen;\r\naux.tp_snaplen = skb->len;\r\naux.tp_mac = 0;\r\naux.tp_net = skb_network_offset(skb);\r\nif (skb_vlan_tag_present(skb)) {\r\naux.tp_vlan_tci = skb_vlan_tag_get(skb);\r\naux.tp_vlan_tpid = ntohs(skb->vlan_proto);\r\naux.tp_status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\r\n} else {\r\naux.tp_vlan_tci = 0;\r\naux.tp_vlan_tpid = 0;\r\n}\r\nput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\r\n}\r\nerr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\r\nout_free:\r\nskb_free_datagram(sk, skb);\r\nout:\r\nreturn err;\r\n}\r\nstatic int packet_getname_spkt(struct socket *sock, struct sockaddr *uaddr,\r\nint *uaddr_len, int peer)\r\n{\r\nstruct net_device *dev;\r\nstruct sock *sk = sock->sk;\r\nif (peer)\r\nreturn -EOPNOTSUPP;\r\nuaddr->sa_family = AF_PACKET;\r\nmemset(uaddr->sa_data, 0, sizeof(uaddr->sa_data));\r\nrcu_read_lock();\r\ndev = dev_get_by_index_rcu(sock_net(sk), pkt_sk(sk)->ifindex);\r\nif (dev)\r\nstrlcpy(uaddr->sa_data, dev->name, sizeof(uaddr->sa_data));\r\nrcu_read_unlock();\r\n*uaddr_len = sizeof(*uaddr);\r\nreturn 0;\r\n}\r\nstatic int packet_getname(struct socket *sock, struct sockaddr *uaddr,\r\nint *uaddr_len, int peer)\r\n{\r\nstruct net_device *dev;\r\nstruct sock *sk = sock->sk;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nDECLARE_SOCKADDR(struct sockaddr_ll *, sll, uaddr);\r\nif (peer)\r\nreturn -EOPNOTSUPP;\r\nsll->sll_family = AF_PACKET;\r\nsll->sll_ifindex = po->ifindex;\r\nsll->sll_protocol = po->num;\r\nsll->sll_pkttype = 0;\r\nrcu_read_lock();\r\ndev = dev_get_by_index_rcu(sock_net(sk), po->ifindex);\r\nif (dev) {\r\nsll->sll_hatype = dev->type;\r\nsll->sll_halen = dev->addr_len;\r\nmemcpy(sll->sll_addr, dev->dev_addr, dev->addr_len);\r\n} else {\r\nsll->sll_hatype = 0;\r\nsll->sll_halen = 0;\r\n}\r\nrcu_read_unlock();\r\n*uaddr_len = offsetof(struct sockaddr_ll, sll_addr) + sll->sll_halen;\r\nreturn 0;\r\n}\r\nstatic int packet_dev_mc(struct net_device *dev, struct packet_mclist *i,\r\nint what)\r\n{\r\nswitch (i->type) {\r\ncase PACKET_MR_MULTICAST:\r\nif (i->alen != dev->addr_len)\r\nreturn -EINVAL;\r\nif (what > 0)\r\nreturn dev_mc_add(dev, i->addr);\r\nelse\r\nreturn dev_mc_del(dev, i->addr);\r\nbreak;\r\ncase PACKET_MR_PROMISC:\r\nreturn dev_set_promiscuity(dev, what);\r\ncase PACKET_MR_ALLMULTI:\r\nreturn dev_set_allmulti(dev, what);\r\ncase PACKET_MR_UNICAST:\r\nif (i->alen != dev->addr_len)\r\nreturn -EINVAL;\r\nif (what > 0)\r\nreturn dev_uc_add(dev, i->addr);\r\nelse\r\nreturn dev_uc_del(dev, i->addr);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic void packet_dev_mclist_delete(struct net_device *dev,\r\nstruct packet_mclist **mlp)\r\n{\r\nstruct packet_mclist *ml;\r\nwhile ((ml = *mlp) != NULL) {\r\nif (ml->ifindex == dev->ifindex) {\r\npacket_dev_mc(dev, ml, -1);\r\n*mlp = ml->next;\r\nkfree(ml);\r\n} else\r\nmlp = &ml->next;\r\n}\r\n}\r\nstatic int packet_mc_add(struct sock *sk, struct packet_mreq_max *mreq)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\nstruct packet_mclist *ml, *i;\r\nstruct net_device *dev;\r\nint err;\r\nrtnl_lock();\r\nerr = -ENODEV;\r\ndev = __dev_get_by_index(sock_net(sk), mreq->mr_ifindex);\r\nif (!dev)\r\ngoto done;\r\nerr = -EINVAL;\r\nif (mreq->mr_alen > dev->addr_len)\r\ngoto done;\r\nerr = -ENOBUFS;\r\ni = kmalloc(sizeof(*i), GFP_KERNEL);\r\nif (i == NULL)\r\ngoto done;\r\nerr = 0;\r\nfor (ml = po->mclist; ml; ml = ml->next) {\r\nif (ml->ifindex == mreq->mr_ifindex &&\r\nml->type == mreq->mr_type &&\r\nml->alen == mreq->mr_alen &&\r\nmemcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\r\nml->count++;\r\nkfree(i);\r\ngoto done;\r\n}\r\n}\r\ni->type = mreq->mr_type;\r\ni->ifindex = mreq->mr_ifindex;\r\ni->alen = mreq->mr_alen;\r\nmemcpy(i->addr, mreq->mr_address, i->alen);\r\ni->count = 1;\r\ni->next = po->mclist;\r\npo->mclist = i;\r\nerr = packet_dev_mc(dev, i, 1);\r\nif (err) {\r\npo->mclist = i->next;\r\nkfree(i);\r\n}\r\ndone:\r\nrtnl_unlock();\r\nreturn err;\r\n}\r\nstatic int packet_mc_drop(struct sock *sk, struct packet_mreq_max *mreq)\r\n{\r\nstruct packet_mclist *ml, **mlp;\r\nrtnl_lock();\r\nfor (mlp = &pkt_sk(sk)->mclist; (ml = *mlp) != NULL; mlp = &ml->next) {\r\nif (ml->ifindex == mreq->mr_ifindex &&\r\nml->type == mreq->mr_type &&\r\nml->alen == mreq->mr_alen &&\r\nmemcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\r\nif (--ml->count == 0) {\r\nstruct net_device *dev;\r\n*mlp = ml->next;\r\ndev = __dev_get_by_index(sock_net(sk), ml->ifindex);\r\nif (dev)\r\npacket_dev_mc(dev, ml, -1);\r\nkfree(ml);\r\n}\r\nbreak;\r\n}\r\n}\r\nrtnl_unlock();\r\nreturn 0;\r\n}\r\nstatic void packet_flush_mclist(struct sock *sk)\r\n{\r\nstruct packet_sock *po = pkt_sk(sk);\r\nstruct packet_mclist *ml;\r\nif (!po->mclist)\r\nreturn;\r\nrtnl_lock();\r\nwhile ((ml = po->mclist) != NULL) {\r\nstruct net_device *dev;\r\npo->mclist = ml->next;\r\ndev = __dev_get_by_index(sock_net(sk), ml->ifindex);\r\nif (dev != NULL)\r\npacket_dev_mc(dev, ml, -1);\r\nkfree(ml);\r\n}\r\nrtnl_unlock();\r\n}\r\nstatic int\r\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nint ret;\r\nif (level != SOL_PACKET)\r\nreturn -ENOPROTOOPT;\r\nswitch (optname) {\r\ncase PACKET_ADD_MEMBERSHIP:\r\ncase PACKET_DROP_MEMBERSHIP:\r\n{\r\nstruct packet_mreq_max mreq;\r\nint len = optlen;\r\nmemset(&mreq, 0, sizeof(mreq));\r\nif (len < sizeof(struct packet_mreq))\r\nreturn -EINVAL;\r\nif (len > sizeof(mreq))\r\nlen = sizeof(mreq);\r\nif (copy_from_user(&mreq, optval, len))\r\nreturn -EFAULT;\r\nif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\r\nreturn -EINVAL;\r\nif (optname == PACKET_ADD_MEMBERSHIP)\r\nret = packet_mc_add(sk, &mreq);\r\nelse\r\nret = packet_mc_drop(sk, &mreq);\r\nreturn ret;\r\n}\r\ncase PACKET_RX_RING:\r\ncase PACKET_TX_RING:\r\n{\r\nunion tpacket_req_u req_u;\r\nint len;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\ncase TPACKET_V2:\r\nlen = sizeof(req_u.req);\r\nbreak;\r\ncase TPACKET_V3:\r\ndefault:\r\nlen = sizeof(req_u.req3);\r\nbreak;\r\n}\r\nif (optlen < len)\r\nreturn -EINVAL;\r\nif (pkt_sk(sk)->has_vnet_hdr)\r\nreturn -EINVAL;\r\nif (copy_from_user(&req_u.req, optval, len))\r\nreturn -EFAULT;\r\nreturn packet_set_ring(sk, &req_u, 0,\r\noptname == PACKET_TX_RING);\r\n}\r\ncase PACKET_COPY_THRESH:\r\n{\r\nint val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npkt_sk(sk)->copy_thresh = val;\r\nreturn 0;\r\n}\r\ncase PACKET_VERSION:\r\n{\r\nint val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\r\nreturn -EBUSY;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\nswitch (val) {\r\ncase TPACKET_V1:\r\ncase TPACKET_V2:\r\ncase TPACKET_V3:\r\npo->tp_version = val;\r\nreturn 0;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\ncase PACKET_RESERVE:\r\n{\r\nunsigned int val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\r\nreturn -EBUSY;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->tp_reserve = val;\r\nreturn 0;\r\n}\r\ncase PACKET_LOSS:\r\n{\r\nunsigned int val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\r\nreturn -EBUSY;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->tp_loss = !!val;\r\nreturn 0;\r\n}\r\ncase PACKET_AUXDATA:\r\n{\r\nint val;\r\nif (optlen < sizeof(val))\r\nreturn -EINVAL;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->auxdata = !!val;\r\nreturn 0;\r\n}\r\ncase PACKET_ORIGDEV:\r\n{\r\nint val;\r\nif (optlen < sizeof(val))\r\nreturn -EINVAL;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->origdev = !!val;\r\nreturn 0;\r\n}\r\ncase PACKET_VNET_HDR:\r\n{\r\nint val;\r\nif (sock->type != SOCK_RAW)\r\nreturn -EINVAL;\r\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\r\nreturn -EBUSY;\r\nif (optlen < sizeof(val))\r\nreturn -EINVAL;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->has_vnet_hdr = !!val;\r\nreturn 0;\r\n}\r\ncase PACKET_TIMESTAMP:\r\n{\r\nint val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->tp_tstamp = val;\r\nreturn 0;\r\n}\r\ncase PACKET_FANOUT:\r\n{\r\nint val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\nreturn fanout_add(sk, val & 0xffff, val >> 16);\r\n}\r\ncase PACKET_FANOUT_DATA:\r\n{\r\nif (!po->fanout)\r\nreturn -EINVAL;\r\nreturn fanout_set_data(po, optval, optlen);\r\n}\r\ncase PACKET_TX_HAS_OFF:\r\n{\r\nunsigned int val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\r\nreturn -EBUSY;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->tp_tx_has_off = !!val;\r\nreturn 0;\r\n}\r\ncase PACKET_QDISC_BYPASS:\r\n{\r\nint val;\r\nif (optlen != sizeof(val))\r\nreturn -EINVAL;\r\nif (copy_from_user(&val, optval, sizeof(val)))\r\nreturn -EFAULT;\r\npo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\r\nreturn 0;\r\n}\r\ndefault:\r\nreturn -ENOPROTOOPT;\r\n}\r\n}\r\nstatic int packet_getsockopt(struct socket *sock, int level, int optname,\r\nchar __user *optval, int __user *optlen)\r\n{\r\nint len;\r\nint val, lv = sizeof(val);\r\nstruct sock *sk = sock->sk;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nvoid *data = &val;\r\nunion tpacket_stats_u st;\r\nstruct tpacket_rollover_stats rstats;\r\nif (level != SOL_PACKET)\r\nreturn -ENOPROTOOPT;\r\nif (get_user(len, optlen))\r\nreturn -EFAULT;\r\nif (len < 0)\r\nreturn -EINVAL;\r\nswitch (optname) {\r\ncase PACKET_STATISTICS:\r\nspin_lock_bh(&sk->sk_receive_queue.lock);\r\nmemcpy(&st, &po->stats, sizeof(st));\r\nmemset(&po->stats, 0, sizeof(po->stats));\r\nspin_unlock_bh(&sk->sk_receive_queue.lock);\r\nif (po->tp_version == TPACKET_V3) {\r\nlv = sizeof(struct tpacket_stats_v3);\r\nst.stats3.tp_packets += st.stats3.tp_drops;\r\ndata = &st.stats3;\r\n} else {\r\nlv = sizeof(struct tpacket_stats);\r\nst.stats1.tp_packets += st.stats1.tp_drops;\r\ndata = &st.stats1;\r\n}\r\nbreak;\r\ncase PACKET_AUXDATA:\r\nval = po->auxdata;\r\nbreak;\r\ncase PACKET_ORIGDEV:\r\nval = po->origdev;\r\nbreak;\r\ncase PACKET_VNET_HDR:\r\nval = po->has_vnet_hdr;\r\nbreak;\r\ncase PACKET_VERSION:\r\nval = po->tp_version;\r\nbreak;\r\ncase PACKET_HDRLEN:\r\nif (len > sizeof(int))\r\nlen = sizeof(int);\r\nif (copy_from_user(&val, optval, len))\r\nreturn -EFAULT;\r\nswitch (val) {\r\ncase TPACKET_V1:\r\nval = sizeof(struct tpacket_hdr);\r\nbreak;\r\ncase TPACKET_V2:\r\nval = sizeof(struct tpacket2_hdr);\r\nbreak;\r\ncase TPACKET_V3:\r\nval = sizeof(struct tpacket3_hdr);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase PACKET_RESERVE:\r\nval = po->tp_reserve;\r\nbreak;\r\ncase PACKET_LOSS:\r\nval = po->tp_loss;\r\nbreak;\r\ncase PACKET_TIMESTAMP:\r\nval = po->tp_tstamp;\r\nbreak;\r\ncase PACKET_FANOUT:\r\nval = (po->fanout ?\r\n((u32)po->fanout->id |\r\n((u32)po->fanout->type << 16) |\r\n((u32)po->fanout->flags << 24)) :\r\n0);\r\nbreak;\r\ncase PACKET_ROLLOVER_STATS:\r\nif (!po->rollover)\r\nreturn -EINVAL;\r\nrstats.tp_all = atomic_long_read(&po->rollover->num);\r\nrstats.tp_huge = atomic_long_read(&po->rollover->num_huge);\r\nrstats.tp_failed = atomic_long_read(&po->rollover->num_failed);\r\ndata = &rstats;\r\nlv = sizeof(rstats);\r\nbreak;\r\ncase PACKET_TX_HAS_OFF:\r\nval = po->tp_tx_has_off;\r\nbreak;\r\ncase PACKET_QDISC_BYPASS:\r\nval = packet_use_direct_xmit(po);\r\nbreak;\r\ndefault:\r\nreturn -ENOPROTOOPT;\r\n}\r\nif (len > lv)\r\nlen = lv;\r\nif (put_user(len, optlen))\r\nreturn -EFAULT;\r\nif (copy_to_user(optval, data, len))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int packet_notifier(struct notifier_block *this,\r\nunsigned long msg, void *ptr)\r\n{\r\nstruct sock *sk;\r\nstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\r\nstruct net *net = dev_net(dev);\r\nrcu_read_lock();\r\nsk_for_each_rcu(sk, &net->packet.sklist) {\r\nstruct packet_sock *po = pkt_sk(sk);\r\nswitch (msg) {\r\ncase NETDEV_UNREGISTER:\r\nif (po->mclist)\r\npacket_dev_mclist_delete(dev, &po->mclist);\r\ncase NETDEV_DOWN:\r\nif (dev->ifindex == po->ifindex) {\r\nspin_lock(&po->bind_lock);\r\nif (po->running) {\r\n__unregister_prot_hook(sk, false);\r\nsk->sk_err = ENETDOWN;\r\nif (!sock_flag(sk, SOCK_DEAD))\r\nsk->sk_error_report(sk);\r\n}\r\nif (msg == NETDEV_UNREGISTER) {\r\npacket_cached_dev_reset(po);\r\npo->ifindex = -1;\r\nif (po->prot_hook.dev)\r\ndev_put(po->prot_hook.dev);\r\npo->prot_hook.dev = NULL;\r\n}\r\nspin_unlock(&po->bind_lock);\r\n}\r\nbreak;\r\ncase NETDEV_UP:\r\nif (dev->ifindex == po->ifindex) {\r\nspin_lock(&po->bind_lock);\r\nif (po->num)\r\nregister_prot_hook(sk);\r\nspin_unlock(&po->bind_lock);\r\n}\r\nbreak;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic int packet_ioctl(struct socket *sock, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nstruct sock *sk = sock->sk;\r\nswitch (cmd) {\r\ncase SIOCOUTQ:\r\n{\r\nint amount = sk_wmem_alloc_get(sk);\r\nreturn put_user(amount, (int __user *)arg);\r\n}\r\ncase SIOCINQ:\r\n{\r\nstruct sk_buff *skb;\r\nint amount = 0;\r\nspin_lock_bh(&sk->sk_receive_queue.lock);\r\nskb = skb_peek(&sk->sk_receive_queue);\r\nif (skb)\r\namount = skb->len;\r\nspin_unlock_bh(&sk->sk_receive_queue.lock);\r\nreturn put_user(amount, (int __user *)arg);\r\n}\r\ncase SIOCGSTAMP:\r\nreturn sock_get_timestamp(sk, (struct timeval __user *)arg);\r\ncase SIOCGSTAMPNS:\r\nreturn sock_get_timestampns(sk, (struct timespec __user *)arg);\r\n#ifdef CONFIG_INET\r\ncase SIOCADDRT:\r\ncase SIOCDELRT:\r\ncase SIOCDARP:\r\ncase SIOCGARP:\r\ncase SIOCSARP:\r\ncase SIOCGIFADDR:\r\ncase SIOCSIFADDR:\r\ncase SIOCGIFBRDADDR:\r\ncase SIOCSIFBRDADDR:\r\ncase SIOCGIFNETMASK:\r\ncase SIOCSIFNETMASK:\r\ncase SIOCGIFDSTADDR:\r\ncase SIOCSIFDSTADDR:\r\ncase SIOCSIFFLAGS:\r\nreturn inet_dgram_ops.ioctl(sock, cmd, arg);\r\n#endif\r\ndefault:\r\nreturn -ENOIOCTLCMD;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned int packet_poll(struct file *file, struct socket *sock,\r\npoll_table *wait)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nunsigned int mask = datagram_poll(file, sock, wait);\r\nspin_lock_bh(&sk->sk_receive_queue.lock);\r\nif (po->rx_ring.pg_vec) {\r\nif (!packet_previous_rx_frame(po, &po->rx_ring,\r\nTP_STATUS_KERNEL))\r\nmask |= POLLIN | POLLRDNORM;\r\n}\r\nif (po->pressure && __packet_rcv_has_room(po, NULL) == ROOM_NORMAL)\r\npo->pressure = 0;\r\nspin_unlock_bh(&sk->sk_receive_queue.lock);\r\nspin_lock_bh(&sk->sk_write_queue.lock);\r\nif (po->tx_ring.pg_vec) {\r\nif (packet_current_frame(po, &po->tx_ring, TP_STATUS_AVAILABLE))\r\nmask |= POLLOUT | POLLWRNORM;\r\n}\r\nspin_unlock_bh(&sk->sk_write_queue.lock);\r\nreturn mask;\r\n}\r\nstatic void packet_mm_open(struct vm_area_struct *vma)\r\n{\r\nstruct file *file = vma->vm_file;\r\nstruct socket *sock = file->private_data;\r\nstruct sock *sk = sock->sk;\r\nif (sk)\r\natomic_inc(&pkt_sk(sk)->mapped);\r\n}\r\nstatic void packet_mm_close(struct vm_area_struct *vma)\r\n{\r\nstruct file *file = vma->vm_file;\r\nstruct socket *sock = file->private_data;\r\nstruct sock *sk = sock->sk;\r\nif (sk)\r\natomic_dec(&pkt_sk(sk)->mapped);\r\n}\r\nstatic void free_pg_vec(struct pgv *pg_vec, unsigned int order,\r\nunsigned int len)\r\n{\r\nint i;\r\nfor (i = 0; i < len; i++) {\r\nif (likely(pg_vec[i].buffer)) {\r\nif (is_vmalloc_addr(pg_vec[i].buffer))\r\nvfree(pg_vec[i].buffer);\r\nelse\r\nfree_pages((unsigned long)pg_vec[i].buffer,\r\norder);\r\npg_vec[i].buffer = NULL;\r\n}\r\n}\r\nkfree(pg_vec);\r\n}\r\nstatic char *alloc_one_pg_vec_page(unsigned long order)\r\n{\r\nchar *buffer;\r\ngfp_t gfp_flags = GFP_KERNEL | __GFP_COMP |\r\n__GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY;\r\nbuffer = (char *) __get_free_pages(gfp_flags, order);\r\nif (buffer)\r\nreturn buffer;\r\nbuffer = vzalloc((1 << order) * PAGE_SIZE);\r\nif (buffer)\r\nreturn buffer;\r\ngfp_flags &= ~__GFP_NORETRY;\r\nbuffer = (char *) __get_free_pages(gfp_flags, order);\r\nif (buffer)\r\nreturn buffer;\r\nreturn NULL;\r\n}\r\nstatic struct pgv *alloc_pg_vec(struct tpacket_req *req, int order)\r\n{\r\nunsigned int block_nr = req->tp_block_nr;\r\nstruct pgv *pg_vec;\r\nint i;\r\npg_vec = kcalloc(block_nr, sizeof(struct pgv), GFP_KERNEL);\r\nif (unlikely(!pg_vec))\r\ngoto out;\r\nfor (i = 0; i < block_nr; i++) {\r\npg_vec[i].buffer = alloc_one_pg_vec_page(order);\r\nif (unlikely(!pg_vec[i].buffer))\r\ngoto out_free_pgvec;\r\n}\r\nout:\r\nreturn pg_vec;\r\nout_free_pgvec:\r\nfree_pg_vec(pg_vec, order, block_nr);\r\npg_vec = NULL;\r\ngoto out;\r\n}\r\nstatic int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\r\nint closing, int tx_ring)\r\n{\r\nstruct pgv *pg_vec = NULL;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nint was_running, order = 0;\r\nstruct packet_ring_buffer *rb;\r\nstruct sk_buff_head *rb_queue;\r\n__be16 num;\r\nint err = -EINVAL;\r\nstruct tpacket_req *req = &req_u->req;\r\nif (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {\r\nWARN(1, "Tx-ring is not supported.\n");\r\ngoto out;\r\n}\r\nrb = tx_ring ? &po->tx_ring : &po->rx_ring;\r\nrb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\r\nerr = -EBUSY;\r\nif (!closing) {\r\nif (atomic_read(&po->mapped))\r\ngoto out;\r\nif (packet_read_pending(rb))\r\ngoto out;\r\n}\r\nif (req->tp_block_nr) {\r\nerr = -EBUSY;\r\nif (unlikely(rb->pg_vec))\r\ngoto out;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V1:\r\npo->tp_hdrlen = TPACKET_HDRLEN;\r\nbreak;\r\ncase TPACKET_V2:\r\npo->tp_hdrlen = TPACKET2_HDRLEN;\r\nbreak;\r\ncase TPACKET_V3:\r\npo->tp_hdrlen = TPACKET3_HDRLEN;\r\nbreak;\r\n}\r\nerr = -EINVAL;\r\nif (unlikely((int)req->tp_block_size <= 0))\r\ngoto out;\r\nif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\r\ngoto out;\r\nif (po->tp_version >= TPACKET_V3 &&\r\n(int)(req->tp_block_size -\r\nBLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\r\ngoto out;\r\nif (unlikely(req->tp_frame_size < po->tp_hdrlen +\r\npo->tp_reserve))\r\ngoto out;\r\nif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\r\ngoto out;\r\nrb->frames_per_block = req->tp_block_size / req->tp_frame_size;\r\nif (unlikely(rb->frames_per_block == 0))\r\ngoto out;\r\nif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\r\nreq->tp_frame_nr))\r\ngoto out;\r\nerr = -ENOMEM;\r\norder = get_order(req->tp_block_size);\r\npg_vec = alloc_pg_vec(req, order);\r\nif (unlikely(!pg_vec))\r\ngoto out;\r\nswitch (po->tp_version) {\r\ncase TPACKET_V3:\r\nif (!tx_ring)\r\ninit_prb_bdqc(po, rb, pg_vec, req_u);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nelse {\r\nerr = -EINVAL;\r\nif (unlikely(req->tp_frame_nr))\r\ngoto out;\r\n}\r\nlock_sock(sk);\r\nspin_lock(&po->bind_lock);\r\nwas_running = po->running;\r\nnum = po->num;\r\nif (was_running) {\r\npo->num = 0;\r\n__unregister_prot_hook(sk, false);\r\n}\r\nspin_unlock(&po->bind_lock);\r\nsynchronize_net();\r\nerr = -EBUSY;\r\nmutex_lock(&po->pg_vec_lock);\r\nif (closing || atomic_read(&po->mapped) == 0) {\r\nerr = 0;\r\nspin_lock_bh(&rb_queue->lock);\r\nswap(rb->pg_vec, pg_vec);\r\nrb->frame_max = (req->tp_frame_nr - 1);\r\nrb->head = 0;\r\nrb->frame_size = req->tp_frame_size;\r\nspin_unlock_bh(&rb_queue->lock);\r\nswap(rb->pg_vec_order, order);\r\nswap(rb->pg_vec_len, req->tp_block_nr);\r\nrb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\r\npo->prot_hook.func = (po->rx_ring.pg_vec) ?\r\ntpacket_rcv : packet_rcv;\r\nskb_queue_purge(rb_queue);\r\nif (atomic_read(&po->mapped))\r\npr_err("packet_mmap: vma is busy: %d\n",\r\natomic_read(&po->mapped));\r\n}\r\nmutex_unlock(&po->pg_vec_lock);\r\nspin_lock(&po->bind_lock);\r\nif (was_running) {\r\npo->num = num;\r\nregister_prot_hook(sk);\r\n}\r\nspin_unlock(&po->bind_lock);\r\nif (closing && (po->tp_version > TPACKET_V2)) {\r\nif (!tx_ring)\r\nprb_shutdown_retire_blk_timer(po, rb_queue);\r\n}\r\nrelease_sock(sk);\r\nif (pg_vec)\r\nfree_pg_vec(pg_vec, order, req->tp_block_nr);\r\nout:\r\nreturn err;\r\n}\r\nstatic int packet_mmap(struct file *file, struct socket *sock,\r\nstruct vm_area_struct *vma)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct packet_sock *po = pkt_sk(sk);\r\nunsigned long size, expected_size;\r\nstruct packet_ring_buffer *rb;\r\nunsigned long start;\r\nint err = -EINVAL;\r\nint i;\r\nif (vma->vm_pgoff)\r\nreturn -EINVAL;\r\nmutex_lock(&po->pg_vec_lock);\r\nexpected_size = 0;\r\nfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\r\nif (rb->pg_vec) {\r\nexpected_size += rb->pg_vec_len\r\n* rb->pg_vec_pages\r\n* PAGE_SIZE;\r\n}\r\n}\r\nif (expected_size == 0)\r\ngoto out;\r\nsize = vma->vm_end - vma->vm_start;\r\nif (size != expected_size)\r\ngoto out;\r\nstart = vma->vm_start;\r\nfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\r\nif (rb->pg_vec == NULL)\r\ncontinue;\r\nfor (i = 0; i < rb->pg_vec_len; i++) {\r\nstruct page *page;\r\nvoid *kaddr = rb->pg_vec[i].buffer;\r\nint pg_num;\r\nfor (pg_num = 0; pg_num < rb->pg_vec_pages; pg_num++) {\r\npage = pgv_to_page(kaddr);\r\nerr = vm_insert_page(vma, start, page);\r\nif (unlikely(err))\r\ngoto out;\r\nstart += PAGE_SIZE;\r\nkaddr += PAGE_SIZE;\r\n}\r\n}\r\n}\r\natomic_inc(&po->mapped);\r\nvma->vm_ops = &packet_mmap_ops;\r\nerr = 0;\r\nout:\r\nmutex_unlock(&po->pg_vec_lock);\r\nreturn err;\r\n}\r\nstatic void *packet_seq_start(struct seq_file *seq, loff_t *pos)\r\n__acquires(RCU)\r\n{\r\nstruct net *net = seq_file_net(seq);\r\nrcu_read_lock();\r\nreturn seq_hlist_start_head_rcu(&net->packet.sklist, *pos);\r\n}\r\nstatic void *packet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\r\n{\r\nstruct net *net = seq_file_net(seq);\r\nreturn seq_hlist_next_rcu(v, &net->packet.sklist, pos);\r\n}\r\nstatic void packet_seq_stop(struct seq_file *seq, void *v)\r\n__releases(RCU)\r\n{\r\nrcu_read_unlock();\r\n}\r\nstatic int packet_seq_show(struct seq_file *seq, void *v)\r\n{\r\nif (v == SEQ_START_TOKEN)\r\nseq_puts(seq, "sk RefCnt Type Proto Iface R Rmem User Inode\n");\r\nelse {\r\nstruct sock *s = sk_entry(v);\r\nconst struct packet_sock *po = pkt_sk(s);\r\nseq_printf(seq,\r\n"%pK %-6d %-4d %04x %-5d %1d %-6u %-6u %-6lu\n",\r\ns,\r\natomic_read(&s->sk_refcnt),\r\ns->sk_type,\r\nntohs(po->num),\r\npo->ifindex,\r\npo->running,\r\natomic_read(&s->sk_rmem_alloc),\r\nfrom_kuid_munged(seq_user_ns(seq), sock_i_uid(s)),\r\nsock_i_ino(s));\r\n}\r\nreturn 0;\r\n}\r\nstatic int packet_seq_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open_net(inode, file, &packet_seq_ops,\r\nsizeof(struct seq_net_private));\r\n}\r\nstatic int __net_init packet_net_init(struct net *net)\r\n{\r\nmutex_init(&net->packet.sklist_lock);\r\nINIT_HLIST_HEAD(&net->packet.sklist);\r\nif (!proc_create("packet", 0, net->proc_net, &packet_seq_fops))\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void __net_exit packet_net_exit(struct net *net)\r\n{\r\nremove_proc_entry("packet", net->proc_net);\r\n}\r\nstatic void __exit packet_exit(void)\r\n{\r\nunregister_netdevice_notifier(&packet_netdev_notifier);\r\nunregister_pernet_subsys(&packet_net_ops);\r\nsock_unregister(PF_PACKET);\r\nproto_unregister(&packet_proto);\r\n}\r\nstatic int __init packet_init(void)\r\n{\r\nint rc = proto_register(&packet_proto, 0);\r\nif (rc != 0)\r\ngoto out;\r\nsock_register(&packet_family_ops);\r\nregister_pernet_subsys(&packet_net_ops);\r\nregister_netdevice_notifier(&packet_netdev_notifier);\r\nout:\r\nreturn rc;\r\n}
