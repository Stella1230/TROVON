void qed_init_dp(struct qed_dev *cdev,\r\nu32 dp_module, u8 dp_level)\r\n{\r\nu32 i;\r\ncdev->dp_level = dp_level;\r\ncdev->dp_module = dp_module;\r\nfor (i = 0; i < MAX_HWFNS_PER_DEVICE; i++) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\np_hwfn->dp_level = dp_level;\r\np_hwfn->dp_module = dp_module;\r\n}\r\n}\r\nvoid qed_init_struct(struct qed_dev *cdev)\r\n{\r\nu8 i;\r\nfor (i = 0; i < MAX_HWFNS_PER_DEVICE; i++) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\np_hwfn->cdev = cdev;\r\np_hwfn->my_id = i;\r\np_hwfn->b_active = false;\r\nmutex_init(&p_hwfn->dmae_info.mutex);\r\n}\r\ncdev->hwfns[0].b_active = true;\r\ncdev->cache_shift = 7;\r\n}\r\nstatic void qed_qm_info_free(struct qed_hwfn *p_hwfn)\r\n{\r\nstruct qed_qm_info *qm_info = &p_hwfn->qm_info;\r\nkfree(qm_info->qm_pq_params);\r\nqm_info->qm_pq_params = NULL;\r\nkfree(qm_info->qm_vport_params);\r\nqm_info->qm_vport_params = NULL;\r\nkfree(qm_info->qm_port_params);\r\nqm_info->qm_port_params = NULL;\r\n}\r\nvoid qed_resc_free(struct qed_dev *cdev)\r\n{\r\nint i;\r\nkfree(cdev->fw_data);\r\ncdev->fw_data = NULL;\r\nkfree(cdev->reset_stats);\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nkfree(p_hwfn->p_tx_cids);\r\np_hwfn->p_tx_cids = NULL;\r\nkfree(p_hwfn->p_rx_cids);\r\np_hwfn->p_rx_cids = NULL;\r\n}\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nqed_cxt_mngr_free(p_hwfn);\r\nqed_qm_info_free(p_hwfn);\r\nqed_spq_free(p_hwfn);\r\nqed_eq_free(p_hwfn, p_hwfn->p_eq);\r\nqed_consq_free(p_hwfn, p_hwfn->p_consq);\r\nqed_int_free(p_hwfn);\r\nqed_dmae_info_free(p_hwfn);\r\n}\r\n}\r\nstatic int qed_init_qm_info(struct qed_hwfn *p_hwfn)\r\n{\r\nstruct qed_qm_info *qm_info = &p_hwfn->qm_info;\r\nstruct init_qm_port_params *p_qm_port;\r\nu8 num_vports, i, vport_id, num_ports;\r\nu16 num_pqs, multi_cos_tcs = 1;\r\nmemset(qm_info, 0, sizeof(*qm_info));\r\nnum_pqs = multi_cos_tcs + 1;\r\nnum_vports = (u8)RESC_NUM(p_hwfn, QED_VPORT);\r\nif (num_pqs > RESC_NUM(p_hwfn, QED_PQ)) {\r\nDP_ERR(p_hwfn,\r\n"Need too many Physical queues - 0x%04x when only %04x are available\n",\r\nnum_pqs, RESC_NUM(p_hwfn, QED_PQ));\r\nreturn -EINVAL;\r\n}\r\nqm_info->qm_pq_params = kzalloc(sizeof(*qm_info->qm_pq_params) *\r\nnum_pqs, GFP_ATOMIC);\r\nif (!qm_info->qm_pq_params)\r\ngoto alloc_err;\r\nqm_info->qm_vport_params = kzalloc(sizeof(*qm_info->qm_vport_params) *\r\nnum_vports, GFP_ATOMIC);\r\nif (!qm_info->qm_vport_params)\r\ngoto alloc_err;\r\nqm_info->qm_port_params = kzalloc(sizeof(*qm_info->qm_port_params) *\r\nMAX_NUM_PORTS, GFP_ATOMIC);\r\nif (!qm_info->qm_port_params)\r\ngoto alloc_err;\r\nvport_id = (u8)RESC_START(p_hwfn, QED_VPORT);\r\nfor (i = 0; i < multi_cos_tcs; i++) {\r\nstruct init_qm_pq_params *params = &qm_info->qm_pq_params[i];\r\nparams->vport_id = vport_id;\r\nparams->tc_id = p_hwfn->hw_info.non_offload_tc;\r\nparams->wrr_group = 1;\r\n}\r\nqm_info->pure_lb_pq = i;\r\nqm_info->qm_pq_params[i].vport_id = (u8)RESC_START(p_hwfn, QED_VPORT);\r\nqm_info->qm_pq_params[i].tc_id = PURE_LB_TC;\r\nqm_info->qm_pq_params[i].wrr_group = 1;\r\ni++;\r\nqm_info->offload_pq = 0;\r\nqm_info->num_pqs = num_pqs;\r\nqm_info->num_vports = num_vports;\r\nnum_ports = p_hwfn->cdev->num_ports_in_engines;\r\nfor (i = 0; i < num_ports; i++) {\r\np_qm_port = &qm_info->qm_port_params[i];\r\np_qm_port->active = 1;\r\np_qm_port->num_active_phys_tcs = 4;\r\np_qm_port->num_pbf_cmd_lines = PBF_MAX_CMD_LINES / num_ports;\r\np_qm_port->num_btb_blocks = BTB_MAX_BLOCKS / num_ports;\r\n}\r\nqm_info->max_phys_tcs_per_port = NUM_OF_PHYS_TCS;\r\nqm_info->start_pq = (u16)RESC_START(p_hwfn, QED_PQ);\r\nqm_info->start_vport = (u8)RESC_START(p_hwfn, QED_VPORT);\r\nqm_info->pf_wfq = 0;\r\nqm_info->pf_rl = 0;\r\nqm_info->vport_rl_en = 1;\r\nreturn 0;\r\nalloc_err:\r\nDP_NOTICE(p_hwfn, "Failed to allocate memory for QM params\n");\r\nkfree(qm_info->qm_pq_params);\r\nkfree(qm_info->qm_vport_params);\r\nkfree(qm_info->qm_port_params);\r\nreturn -ENOMEM;\r\n}\r\nint qed_resc_alloc(struct qed_dev *cdev)\r\n{\r\nstruct qed_consq *p_consq;\r\nstruct qed_eq *p_eq;\r\nint i, rc = 0;\r\ncdev->fw_data = kzalloc(sizeof(*cdev->fw_data), GFP_KERNEL);\r\nif (!cdev->fw_data)\r\nreturn -ENOMEM;\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nint tx_size = sizeof(struct qed_hw_cid_data) *\r\nRESC_NUM(p_hwfn, QED_L2_QUEUE);\r\nint rx_size = sizeof(struct qed_hw_cid_data) *\r\nRESC_NUM(p_hwfn, QED_L2_QUEUE);\r\np_hwfn->p_tx_cids = kzalloc(tx_size, GFP_KERNEL);\r\nif (!p_hwfn->p_tx_cids) {\r\nDP_NOTICE(p_hwfn,\r\n"Failed to allocate memory for Tx Cids\n");\r\nrc = -ENOMEM;\r\ngoto alloc_err;\r\n}\r\np_hwfn->p_rx_cids = kzalloc(rx_size, GFP_KERNEL);\r\nif (!p_hwfn->p_rx_cids) {\r\nDP_NOTICE(p_hwfn,\r\n"Failed to allocate memory for Rx Cids\n");\r\nrc = -ENOMEM;\r\ngoto alloc_err;\r\n}\r\n}\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nrc = qed_cxt_mngr_alloc(p_hwfn);\r\nif (rc)\r\ngoto alloc_err;\r\nrc = qed_cxt_set_pf_params(p_hwfn);\r\nif (rc)\r\ngoto alloc_err;\r\nrc = qed_init_qm_info(p_hwfn);\r\nif (rc)\r\ngoto alloc_err;\r\nrc = qed_cxt_cfg_ilt_compute(p_hwfn);\r\nif (rc)\r\ngoto alloc_err;\r\nrc = qed_cxt_tables_alloc(p_hwfn);\r\nif (rc)\r\ngoto alloc_err;\r\nrc = qed_spq_alloc(p_hwfn);\r\nif (rc)\r\ngoto alloc_err;\r\np_hwfn->p_dpc_ptt = qed_get_reserved_ptt(p_hwfn,\r\nRESERVED_PTT_DPC);\r\nrc = qed_int_alloc(p_hwfn, p_hwfn->p_main_ptt);\r\nif (rc)\r\ngoto alloc_err;\r\np_eq = qed_eq_alloc(p_hwfn, 256);\r\nif (!p_eq) {\r\nrc = -ENOMEM;\r\ngoto alloc_err;\r\n}\r\np_hwfn->p_eq = p_eq;\r\np_consq = qed_consq_alloc(p_hwfn);\r\nif (!p_consq) {\r\nrc = -ENOMEM;\r\ngoto alloc_err;\r\n}\r\np_hwfn->p_consq = p_consq;\r\nrc = qed_dmae_info_alloc(p_hwfn);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn,\r\n"Failed to allocate memory for dmae_info structure\n");\r\ngoto alloc_err;\r\n}\r\n}\r\ncdev->reset_stats = kzalloc(sizeof(*cdev->reset_stats), GFP_KERNEL);\r\nif (!cdev->reset_stats) {\r\nDP_NOTICE(cdev, "Failed to allocate reset statistics\n");\r\nrc = -ENOMEM;\r\ngoto alloc_err;\r\n}\r\nreturn 0;\r\nalloc_err:\r\nqed_resc_free(cdev);\r\nreturn rc;\r\n}\r\nvoid qed_resc_setup(struct qed_dev *cdev)\r\n{\r\nint i;\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nqed_cxt_mngr_setup(p_hwfn);\r\nqed_spq_setup(p_hwfn);\r\nqed_eq_setup(p_hwfn, p_hwfn->p_eq);\r\nqed_consq_setup(p_hwfn, p_hwfn->p_consq);\r\nqed_mcp_read_mb(p_hwfn, p_hwfn->p_main_ptt);\r\nmemcpy(p_hwfn->mcp_info->mfw_mb_shadow,\r\np_hwfn->mcp_info->mfw_mb_cur,\r\np_hwfn->mcp_info->mfw_mb_length);\r\nqed_int_setup(p_hwfn, p_hwfn->p_main_ptt);\r\n}\r\n}\r\nint qed_final_cleanup(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_ptt,\r\nu16 id)\r\n{\r\nu32 command = 0, addr, count = FINAL_CLEANUP_POLL_CNT;\r\nint rc = -EBUSY;\r\naddr = GTT_BAR0_MAP_REG_USDM_RAM + USTORM_FLR_FINAL_ACK_OFFSET;\r\ncommand |= FINAL_CLEANUP_CMD << FINAL_CLEANUP_CMD_OFFSET;\r\ncommand |= 1 << FINAL_CLEANUP_VALID_OFFSET;\r\ncommand |= id << FINAL_CLEANUP_VFPF_ID_SHIFT;\r\ncommand |= FINAL_CLEANUP_COMP << SDM_OP_GEN_COMP_TYPE_SHIFT;\r\nif (REG_RD(p_hwfn, addr)) {\r\nDP_NOTICE(\r\np_hwfn,\r\n"Unexpected; Found final cleanup notification before initiating final cleanup\n");\r\nREG_WR(p_hwfn, addr, 0);\r\n}\r\nDP_VERBOSE(p_hwfn, QED_MSG_IOV,\r\n"Sending final cleanup for PFVF[%d] [Command %08x\n]",\r\nid, command);\r\nqed_wr(p_hwfn, p_ptt, XSDM_REG_OPERATION_GEN, command);\r\nwhile (!REG_RD(p_hwfn, addr) && count--)\r\nmsleep(FINAL_CLEANUP_POLL_TIME);\r\nif (REG_RD(p_hwfn, addr))\r\nrc = 0;\r\nelse\r\nDP_NOTICE(p_hwfn,\r\n"Failed to receive FW final cleanup notification\n");\r\nREG_WR(p_hwfn, addr, 0);\r\nreturn rc;\r\n}\r\nstatic void qed_calc_hw_mode(struct qed_hwfn *p_hwfn)\r\n{\r\nint hw_mode = 0;\r\nhw_mode = (1 << MODE_BB_A0);\r\nswitch (p_hwfn->cdev->num_ports_in_engines) {\r\ncase 1:\r\nhw_mode |= 1 << MODE_PORTS_PER_ENG_1;\r\nbreak;\r\ncase 2:\r\nhw_mode |= 1 << MODE_PORTS_PER_ENG_2;\r\nbreak;\r\ncase 4:\r\nhw_mode |= 1 << MODE_PORTS_PER_ENG_4;\r\nbreak;\r\ndefault:\r\nDP_NOTICE(p_hwfn, "num_ports_in_engine = %d not supported\n",\r\np_hwfn->cdev->num_ports_in_engines);\r\nreturn;\r\n}\r\nswitch (p_hwfn->cdev->mf_mode) {\r\ncase SF:\r\nhw_mode |= 1 << MODE_SF;\r\nbreak;\r\ncase MF_OVLAN:\r\nhw_mode |= 1 << MODE_MF_SD;\r\nbreak;\r\ncase MF_NPAR:\r\nhw_mode |= 1 << MODE_MF_SI;\r\nbreak;\r\ndefault:\r\nDP_NOTICE(p_hwfn, "Unsupported MF mode, init as SF\n");\r\nhw_mode |= 1 << MODE_SF;\r\n}\r\nhw_mode |= 1 << MODE_ASIC;\r\np_hwfn->hw_info.hw_mode = hw_mode;\r\n}\r\nstatic void qed_init_cau_rt_data(struct qed_dev *cdev)\r\n{\r\nu32 offset = CAU_REG_SB_VAR_MEMORY_RT_OFFSET;\r\nint i, sb_id;\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nstruct qed_igu_info *p_igu_info;\r\nstruct qed_igu_block *p_block;\r\nstruct cau_sb_entry sb_entry;\r\np_igu_info = p_hwfn->hw_info.p_igu_info;\r\nfor (sb_id = 0; sb_id < QED_MAPPING_MEMORY_SIZE(cdev);\r\nsb_id++) {\r\np_block = &p_igu_info->igu_map.igu_blocks[sb_id];\r\nif (!p_block->is_pf)\r\ncontinue;\r\nqed_init_cau_sb_entry(p_hwfn, &sb_entry,\r\np_block->function_id,\r\n0, 0);\r\nSTORE_RT_REG_AGG(p_hwfn, offset + sb_id * 2,\r\nsb_entry);\r\n}\r\n}\r\n}\r\nstatic int qed_hw_init_common(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_ptt,\r\nint hw_mode)\r\n{\r\nstruct qed_qm_info *qm_info = &p_hwfn->qm_info;\r\nstruct qed_qm_common_rt_init_params params;\r\nstruct qed_dev *cdev = p_hwfn->cdev;\r\nint rc = 0;\r\nqed_init_cau_rt_data(cdev);\r\nqed_gtt_init(p_hwfn);\r\nif (p_hwfn->mcp_info) {\r\nif (p_hwfn->mcp_info->func_info.bandwidth_max)\r\nqm_info->pf_rl_en = 1;\r\nif (p_hwfn->mcp_info->func_info.bandwidth_min)\r\nqm_info->pf_wfq_en = 1;\r\n}\r\nmemset(&params, 0, sizeof(params));\r\nparams.max_ports_per_engine = p_hwfn->cdev->num_ports_in_engines;\r\nparams.max_phys_tcs_per_port = qm_info->max_phys_tcs_per_port;\r\nparams.pf_rl_en = qm_info->pf_rl_en;\r\nparams.pf_wfq_en = qm_info->pf_wfq_en;\r\nparams.vport_rl_en = qm_info->vport_rl_en;\r\nparams.vport_wfq_en = qm_info->vport_wfq_en;\r\nparams.port_params = qm_info->qm_port_params;\r\nqed_qm_common_rt_init(p_hwfn, &params);\r\nqed_cxt_hw_init_common(p_hwfn);\r\nqed_wr(p_hwfn, p_ptt, NIG_REG_RX_BRB_OUT_EN, 0);\r\nqed_wr(p_hwfn, p_ptt, NIG_REG_STORM_OUT_EN, 0);\r\nqed_port_pretend(p_hwfn, p_ptt, p_hwfn->port_id ^ 1);\r\nqed_wr(p_hwfn, p_ptt, NIG_REG_RX_BRB_OUT_EN, 0);\r\nqed_wr(p_hwfn, p_ptt, NIG_REG_STORM_OUT_EN, 0);\r\nqed_port_unpretend(p_hwfn, p_ptt);\r\nrc = qed_init_run(p_hwfn, p_ptt, PHASE_ENGINE, ANY_PHASE_ID, hw_mode);\r\nif (rc != 0)\r\nreturn rc;\r\nqed_wr(p_hwfn, p_ptt, PSWRQ2_REG_L2P_VALIDATE_VFID, 0);\r\nqed_wr(p_hwfn, p_ptt, PGLUE_B_REG_USE_CLIENTID_IN_TAG, 1);\r\nqed_wr(p_hwfn, p_ptt, 0x20b4,\r\nqed_rd(p_hwfn, p_ptt, 0x20b4) & ~0x10);\r\nreturn rc;\r\n}\r\nstatic int qed_hw_init_port(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_ptt,\r\nint hw_mode)\r\n{\r\nint rc = 0;\r\nrc = qed_init_run(p_hwfn, p_ptt, PHASE_PORT, p_hwfn->port_id,\r\nhw_mode);\r\nreturn rc;\r\n}\r\nstatic int qed_hw_init_pf(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_ptt,\r\nint hw_mode,\r\nbool b_hw_start,\r\nenum qed_int_mode int_mode,\r\nbool allow_npar_tx_switch)\r\n{\r\nu8 rel_pf_id = p_hwfn->rel_pf_id;\r\nint rc = 0;\r\nif (p_hwfn->mcp_info) {\r\nstruct qed_mcp_function_info *p_info;\r\np_info = &p_hwfn->mcp_info->func_info;\r\nif (p_info->bandwidth_min)\r\np_hwfn->qm_info.pf_wfq = p_info->bandwidth_min;\r\np_hwfn->qm_info.pf_rl = 100;\r\n}\r\nqed_cxt_hw_init_pf(p_hwfn);\r\nqed_int_igu_init_rt(p_hwfn);\r\nif (hw_mode & (1 << MODE_MF_SD)) {\r\nDP_VERBOSE(p_hwfn, NETIF_MSG_HW, "Configuring LLH_FUNC_TAG\n");\r\nSTORE_RT_REG(p_hwfn, NIG_REG_LLH_FUNC_TAG_EN_RT_OFFSET, 1);\r\nSTORE_RT_REG(p_hwfn, NIG_REG_LLH_FUNC_TAG_VALUE_RT_OFFSET,\r\np_hwfn->hw_info.ovlan);\r\n}\r\nif (hw_mode & (1 << MODE_MF_SI)) {\r\nDP_VERBOSE(p_hwfn, NETIF_MSG_HW,\r\n"Configuring TAGMAC_CLS_TYPE\n");\r\nSTORE_RT_REG(p_hwfn,\r\nNIG_REG_LLH_FUNC_TAGMAC_CLS_TYPE_RT_OFFSET, 1);\r\n}\r\nSTORE_RT_REG(p_hwfn, PRS_REG_SEARCH_TCP_RT_OFFSET, 0);\r\nSTORE_RT_REG(p_hwfn, PRS_REG_SEARCH_FCOE_RT_OFFSET, 0);\r\nSTORE_RT_REG(p_hwfn, PRS_REG_SEARCH_ROCE_RT_OFFSET, 0);\r\nrc = qed_final_cleanup(p_hwfn, p_ptt, rel_pf_id);\r\nif (rc != 0)\r\nreturn rc;\r\nrc = qed_init_run(p_hwfn, p_ptt, PHASE_PF, rel_pf_id, hw_mode);\r\nif (rc)\r\nreturn rc;\r\nrc = qed_init_run(p_hwfn, p_ptt, PHASE_QM_PF, rel_pf_id, hw_mode);\r\nif (rc)\r\nreturn rc;\r\nqed_int_igu_init_pure_rt(p_hwfn, p_ptt, true, true);\r\nif (b_hw_start) {\r\nqed_int_igu_enable(p_hwfn, p_ptt, int_mode);\r\nrc = qed_sp_pf_start(p_hwfn, p_hwfn->cdev->mf_mode);\r\nif (rc)\r\nDP_NOTICE(p_hwfn, "Function start ramrod failed\n");\r\n}\r\nreturn rc;\r\n}\r\nstatic int qed_change_pci_hwfn(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_ptt,\r\nu8 enable)\r\n{\r\nu32 delay_idx = 0, val, set_val = enable ? 1 : 0;\r\nqed_wr(p_hwfn, p_ptt,\r\nPGLUE_B_REG_INTERNAL_PFID_ENABLE_MASTER, set_val);\r\nfor (delay_idx = 0; delay_idx < 20000; delay_idx++) {\r\nval = qed_rd(p_hwfn, p_ptt,\r\nPGLUE_B_REG_INTERNAL_PFID_ENABLE_MASTER);\r\nif (val == set_val)\r\nbreak;\r\nusleep_range(50, 60);\r\n}\r\nif (val != set_val) {\r\nDP_NOTICE(p_hwfn,\r\n"PFID_ENABLE_MASTER wasn't changed after a second\n");\r\nreturn -EAGAIN;\r\n}\r\nreturn 0;\r\n}\r\nstatic void qed_reset_mb_shadow(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_main_ptt)\r\n{\r\nqed_mcp_read_mb(p_hwfn, p_main_ptt);\r\nmemcpy(p_hwfn->mcp_info->mfw_mb_shadow,\r\np_hwfn->mcp_info->mfw_mb_cur,\r\np_hwfn->mcp_info->mfw_mb_length);\r\n}\r\nint qed_hw_init(struct qed_dev *cdev,\r\nbool b_hw_start,\r\nenum qed_int_mode int_mode,\r\nbool allow_npar_tx_switch,\r\nconst u8 *bin_fw_data)\r\n{\r\nstruct qed_storm_stats *p_stat;\r\nu32 load_code, param, *p_address;\r\nint rc, mfw_rc, i;\r\nu8 fw_vport = 0;\r\nrc = qed_init_fw_data(cdev, bin_fw_data);\r\nif (rc != 0)\r\nreturn rc;\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nrc = qed_fw_vport(p_hwfn, 0, &fw_vport);\r\nif (rc != 0)\r\nreturn rc;\r\nrc = qed_change_pci_hwfn(p_hwfn, p_hwfn->p_main_ptt, true);\r\nqed_calc_hw_mode(p_hwfn);\r\nrc = qed_mcp_load_req(p_hwfn, p_hwfn->p_main_ptt,\r\n&load_code);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "Failed sending LOAD_REQ command\n");\r\nreturn rc;\r\n}\r\nqed_reset_mb_shadow(p_hwfn, p_hwfn->p_main_ptt);\r\nDP_VERBOSE(p_hwfn, QED_MSG_SP,\r\n"Load request was sent. Resp:0x%x, Load code: 0x%x\n",\r\nrc, load_code);\r\np_hwfn->first_on_engine = (load_code ==\r\nFW_MSG_CODE_DRV_LOAD_ENGINE);\r\nswitch (load_code) {\r\ncase FW_MSG_CODE_DRV_LOAD_ENGINE:\r\nrc = qed_hw_init_common(p_hwfn, p_hwfn->p_main_ptt,\r\np_hwfn->hw_info.hw_mode);\r\nif (rc)\r\nbreak;\r\ncase FW_MSG_CODE_DRV_LOAD_PORT:\r\nrc = qed_hw_init_port(p_hwfn, p_hwfn->p_main_ptt,\r\np_hwfn->hw_info.hw_mode);\r\nif (rc)\r\nbreak;\r\ncase FW_MSG_CODE_DRV_LOAD_FUNCTION:\r\nrc = qed_hw_init_pf(p_hwfn, p_hwfn->p_main_ptt,\r\np_hwfn->hw_info.hw_mode,\r\nb_hw_start, int_mode,\r\nallow_npar_tx_switch);\r\nbreak;\r\ndefault:\r\nrc = -EINVAL;\r\nbreak;\r\n}\r\nif (rc)\r\nDP_NOTICE(p_hwfn,\r\n"init phase failed for loadcode 0x%x (rc %d)\n",\r\nload_code, rc);\r\nmfw_rc = qed_mcp_cmd(p_hwfn, p_hwfn->p_main_ptt,\r\nDRV_MSG_CODE_LOAD_DONE,\r\n0, &load_code, &param);\r\nif (rc)\r\nreturn rc;\r\nif (mfw_rc) {\r\nDP_NOTICE(p_hwfn, "Failed sending LOAD_DONE command\n");\r\nreturn mfw_rc;\r\n}\r\np_hwfn->hw_init_done = true;\r\np_stat = &p_hwfn->storm_stats;\r\np_stat->mstats.address = BAR0_MAP_REG_MSDM_RAM +\r\nMSTORM_QUEUE_STAT_OFFSET(fw_vport);\r\np_stat->mstats.len = sizeof(struct eth_mstorm_per_queue_stat);\r\np_stat->ustats.address = BAR0_MAP_REG_USDM_RAM +\r\nUSTORM_QUEUE_STAT_OFFSET(fw_vport);\r\np_stat->ustats.len = sizeof(struct eth_ustorm_per_queue_stat);\r\np_stat->pstats.address = BAR0_MAP_REG_PSDM_RAM +\r\nPSTORM_QUEUE_STAT_OFFSET(fw_vport);\r\np_stat->pstats.len = sizeof(struct eth_pstorm_per_queue_stat);\r\np_address = &p_stat->tstats.address;\r\n*p_address = BAR0_MAP_REG_TSDM_RAM +\r\nTSTORM_PORT_STAT_OFFSET(MFW_PORT(p_hwfn));\r\np_stat->tstats.len = sizeof(struct tstorm_per_port_stat);\r\n}\r\nreturn 0;\r\n}\r\nint qed_hw_stop(struct qed_dev *cdev)\r\n{\r\nint rc = 0, t_rc;\r\nint i, j;\r\nfor_each_hwfn(cdev, j) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[j];\r\nstruct qed_ptt *p_ptt = p_hwfn->p_main_ptt;\r\nDP_VERBOSE(p_hwfn, NETIF_MSG_IFDOWN, "Stopping hw/fw\n");\r\np_hwfn->hw_init_done = false;\r\nrc = qed_sp_pf_stop(p_hwfn);\r\nif (rc)\r\nreturn rc;\r\nqed_wr(p_hwfn, p_ptt,\r\nNIG_REG_RX_LLH_BRB_GATE_DNTFWD_PERPF, 0x1);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_TCP, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_UDP, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_FCOE, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_ROCE, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_OPENFLOW, 0x0);\r\nqed_wr(p_hwfn, p_ptt, TM_REG_PF_ENABLE_CONN, 0x0);\r\nqed_wr(p_hwfn, p_ptt, TM_REG_PF_ENABLE_TASK, 0x0);\r\nfor (i = 0; i < QED_HW_STOP_RETRY_LIMIT; i++) {\r\nif ((!qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_CONN)) &&\r\n(!qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_TASK)))\r\nbreak;\r\nusleep_range(1000, 2000);\r\n}\r\nif (i == QED_HW_STOP_RETRY_LIMIT)\r\nDP_NOTICE(p_hwfn,\r\n"Timers linear scans are not over [Connection %02x Tasks %02x]\n",\r\n(u8)qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_CONN),\r\n(u8)qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_TASK));\r\nqed_int_igu_disable_int(p_hwfn, p_ptt);\r\nqed_wr(p_hwfn, p_ptt, IGU_REG_LEADING_EDGE_LATCH, 0);\r\nqed_wr(p_hwfn, p_ptt, IGU_REG_TRAILING_EDGE_LATCH, 0);\r\nqed_int_igu_init_pure_rt(p_hwfn, p_ptt, false, true);\r\nusleep_range(1000, 2000);\r\n}\r\nt_rc = qed_change_pci_hwfn(&cdev->hwfns[0],\r\ncdev->hwfns[0].p_main_ptt,\r\nfalse);\r\nif (t_rc != 0)\r\nrc = t_rc;\r\nreturn rc;\r\n}\r\nvoid qed_hw_stop_fastpath(struct qed_dev *cdev)\r\n{\r\nint i, j;\r\nfor_each_hwfn(cdev, j) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[j];\r\nstruct qed_ptt *p_ptt = p_hwfn->p_main_ptt;\r\nDP_VERBOSE(p_hwfn,\r\nNETIF_MSG_IFDOWN,\r\n"Shutting down the fastpath\n");\r\nqed_wr(p_hwfn, p_ptt,\r\nNIG_REG_RX_LLH_BRB_GATE_DNTFWD_PERPF, 0x1);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_TCP, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_UDP, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_FCOE, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_ROCE, 0x0);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_OPENFLOW, 0x0);\r\nqed_wr(p_hwfn, p_ptt, TM_REG_PF_ENABLE_CONN, 0x0);\r\nqed_wr(p_hwfn, p_ptt, TM_REG_PF_ENABLE_TASK, 0x0);\r\nfor (i = 0; i < QED_HW_STOP_RETRY_LIMIT; i++) {\r\nif ((!qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_CONN)) &&\r\n(!qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_TASK)))\r\nbreak;\r\nusleep_range(1000, 2000);\r\n}\r\nif (i == QED_HW_STOP_RETRY_LIMIT)\r\nDP_NOTICE(p_hwfn,\r\n"Timers linear scans are not over [Connection %02x Tasks %02x]\n",\r\n(u8)qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_CONN),\r\n(u8)qed_rd(p_hwfn, p_ptt,\r\nTM_REG_PF_SCAN_ACTIVE_TASK));\r\nqed_int_igu_init_pure_rt(p_hwfn, p_ptt, false, false);\r\nusleep_range(1000, 2000);\r\n}\r\n}\r\nvoid qed_hw_start_fastpath(struct qed_hwfn *p_hwfn)\r\n{\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt,\r\nNIG_REG_RX_LLH_BRB_GATE_DNTFWD_PERPF, 0x0);\r\n}\r\nstatic int qed_reg_assert(struct qed_hwfn *hwfn,\r\nstruct qed_ptt *ptt, u32 reg,\r\nbool expected)\r\n{\r\nu32 assert_val = qed_rd(hwfn, ptt, reg);\r\nif (assert_val != expected) {\r\nDP_NOTICE(hwfn, "Value at address 0x%x != 0x%08x\n",\r\nreg, expected);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint qed_hw_reset(struct qed_dev *cdev)\r\n{\r\nint rc = 0;\r\nu32 unload_resp, unload_param;\r\nint i;\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nDP_VERBOSE(p_hwfn, NETIF_MSG_IFDOWN, "Resetting hw/fw\n");\r\nqed_reg_assert(p_hwfn, p_hwfn->p_main_ptt,\r\nQM_REG_USG_CNT_PF_TX, 0);\r\nqed_reg_assert(p_hwfn, p_hwfn->p_main_ptt,\r\nQM_REG_USG_CNT_PF_OTHER, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt, DORQ_REG_PF_DB_ENABLE, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt, QM_REG_PF_EN, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt,\r\nTCFC_REG_STRONG_ENABLE_PF, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt,\r\nCCFC_REG_STRONG_ENABLE_PF, 0);\r\nrc = qed_mcp_cmd(p_hwfn, p_hwfn->p_main_ptt,\r\nDRV_MSG_CODE_UNLOAD_REQ,\r\nDRV_MB_PARAM_UNLOAD_WOL_MCP,\r\n&unload_resp, &unload_param);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "qed_hw_reset: UNLOAD_REQ failed\n");\r\nunload_resp = FW_MSG_CODE_DRV_UNLOAD_ENGINE;\r\n}\r\nrc = qed_mcp_cmd(p_hwfn, p_hwfn->p_main_ptt,\r\nDRV_MSG_CODE_UNLOAD_DONE,\r\n0, &unload_resp, &unload_param);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "qed_hw_reset: UNLOAD_DONE failed\n");\r\nreturn rc;\r\n}\r\n}\r\nreturn rc;\r\n}\r\nstatic void qed_hw_hwfn_free(struct qed_hwfn *p_hwfn)\r\n{\r\nqed_ptt_pool_free(p_hwfn);\r\nkfree(p_hwfn->hw_info.p_igu_info);\r\n}\r\nstatic int qed_hw_hwfn_prepare(struct qed_hwfn *p_hwfn)\r\n{\r\nint rc;\r\nrc = qed_ptt_pool_alloc(p_hwfn);\r\nif (rc)\r\nreturn rc;\r\np_hwfn->p_main_ptt = qed_get_reserved_ptt(p_hwfn, RESERVED_PTT_MAIN);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt, PGLUE_B_REG_PGL_ADDR_88_F0, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt, PGLUE_B_REG_PGL_ADDR_8C_F0, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt, PGLUE_B_REG_PGL_ADDR_90_F0, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt, PGLUE_B_REG_PGL_ADDR_94_F0, 0);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt,\r\nPGLUE_B_REG_WAS_ERROR_PF_31_0_CLR,\r\n1 << p_hwfn->abs_pf_id);\r\nqed_wr(p_hwfn, p_hwfn->p_main_ptt,\r\nPGLUE_B_REG_INTERNAL_PFID_ENABLE_TARGET_READ, 1);\r\nreturn 0;\r\n}\r\nstatic void get_function_id(struct qed_hwfn *p_hwfn)\r\n{\r\np_hwfn->hw_info.opaque_fid = (u16)REG_RD(p_hwfn, PXP_PF_ME_OPAQUE_ADDR);\r\np_hwfn->hw_info.concrete_fid = REG_RD(p_hwfn, PXP_PF_ME_CONCRETE_ADDR);\r\np_hwfn->abs_pf_id = (p_hwfn->hw_info.concrete_fid >> 16) & 0xf;\r\np_hwfn->rel_pf_id = GET_FIELD(p_hwfn->hw_info.concrete_fid,\r\nPXP_CONCRETE_FID_PFID);\r\np_hwfn->port_id = GET_FIELD(p_hwfn->hw_info.concrete_fid,\r\nPXP_CONCRETE_FID_PORT);\r\n}\r\nstatic void qed_hw_set_feat(struct qed_hwfn *p_hwfn)\r\n{\r\nu32 *feat_num = p_hwfn->hw_info.feat_num;\r\nint num_features = 1;\r\nfeat_num[QED_PF_L2_QUE] = min_t(u32, RESC_NUM(p_hwfn, QED_SB) /\r\nnum_features,\r\nRESC_NUM(p_hwfn, QED_L2_QUEUE));\r\nDP_VERBOSE(p_hwfn, NETIF_MSG_PROBE,\r\n"#PF_L2_QUEUES=%d #SBS=%d num_features=%d\n",\r\nfeat_num[QED_PF_L2_QUE], RESC_NUM(p_hwfn, QED_SB),\r\nnum_features);\r\n}\r\nstatic void qed_hw_get_resc(struct qed_hwfn *p_hwfn)\r\n{\r\nu32 *resc_start = p_hwfn->hw_info.resc_start;\r\nu32 *resc_num = p_hwfn->hw_info.resc_num;\r\nint num_funcs, i;\r\nnum_funcs = IS_MF(p_hwfn) ? MAX_NUM_PFS_BB\r\n: p_hwfn->cdev->num_ports_in_engines;\r\nresc_num[QED_SB] = min_t(u32,\r\n(MAX_SB_PER_PATH_BB / num_funcs),\r\nqed_int_get_num_sbs(p_hwfn, NULL));\r\nresc_num[QED_L2_QUEUE] = MAX_NUM_L2_QUEUES_BB / num_funcs;\r\nresc_num[QED_VPORT] = MAX_NUM_VPORTS_BB / num_funcs;\r\nresc_num[QED_RSS_ENG] = ETH_RSS_ENGINE_NUM_BB / num_funcs;\r\nresc_num[QED_PQ] = MAX_QM_TX_QUEUES_BB / num_funcs;\r\nresc_num[QED_RL] = 8;\r\nresc_num[QED_MAC] = ETH_NUM_MAC_FILTERS / num_funcs;\r\nresc_num[QED_VLAN] = (ETH_NUM_VLAN_FILTERS - 1 ) /\r\nnum_funcs;\r\nresc_num[QED_ILT] = 950;\r\nfor (i = 0; i < QED_MAX_RESC; i++)\r\nresc_start[i] = resc_num[i] * p_hwfn->rel_pf_id;\r\nqed_hw_set_feat(p_hwfn);\r\nDP_VERBOSE(p_hwfn, NETIF_MSG_PROBE,\r\n"The numbers for each resource are:\n"\r\n"SB = %d start = %d\n"\r\n"L2_QUEUE = %d start = %d\n"\r\n"VPORT = %d start = %d\n"\r\n"PQ = %d start = %d\n"\r\n"RL = %d start = %d\n"\r\n"MAC = %d start = %d\n"\r\n"VLAN = %d start = %d\n"\r\n"ILT = %d start = %d\n",\r\np_hwfn->hw_info.resc_num[QED_SB],\r\np_hwfn->hw_info.resc_start[QED_SB],\r\np_hwfn->hw_info.resc_num[QED_L2_QUEUE],\r\np_hwfn->hw_info.resc_start[QED_L2_QUEUE],\r\np_hwfn->hw_info.resc_num[QED_VPORT],\r\np_hwfn->hw_info.resc_start[QED_VPORT],\r\np_hwfn->hw_info.resc_num[QED_PQ],\r\np_hwfn->hw_info.resc_start[QED_PQ],\r\np_hwfn->hw_info.resc_num[QED_RL],\r\np_hwfn->hw_info.resc_start[QED_RL],\r\np_hwfn->hw_info.resc_num[QED_MAC],\r\np_hwfn->hw_info.resc_start[QED_MAC],\r\np_hwfn->hw_info.resc_num[QED_VLAN],\r\np_hwfn->hw_info.resc_start[QED_VLAN],\r\np_hwfn->hw_info.resc_num[QED_ILT],\r\np_hwfn->hw_info.resc_start[QED_ILT]);\r\n}\r\nstatic int qed_hw_get_nvm_info(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_ptt)\r\n{\r\nu32 nvm_cfg1_offset, mf_mode, addr, generic_cont0, core_cfg;\r\nu32 port_cfg_addr, link_temp, val, nvm_cfg_addr;\r\nstruct qed_mcp_link_params *link;\r\nnvm_cfg_addr = qed_rd(p_hwfn, p_ptt, MISC_REG_GEN_PURP_CR0);\r\nif (!nvm_cfg_addr) {\r\nDP_NOTICE(p_hwfn, "Shared memory not initialized\n");\r\nreturn -EINVAL;\r\n}\r\nnvm_cfg1_offset = qed_rd(p_hwfn, p_ptt, nvm_cfg_addr + 4);\r\naddr = MCP_REG_SCRATCH + nvm_cfg1_offset +\r\noffsetof(struct nvm_cfg1, glob) +\r\noffsetof(struct nvm_cfg1_glob, pci_id);\r\np_hwfn->hw_info.vendor_id = qed_rd(p_hwfn, p_ptt, addr) &\r\nNVM_CFG1_GLOB_VENDOR_ID_MASK;\r\naddr = MCP_REG_SCRATCH + nvm_cfg1_offset +\r\noffsetof(struct nvm_cfg1, glob) +\r\noffsetof(struct nvm_cfg1_glob, core_cfg);\r\ncore_cfg = qed_rd(p_hwfn, p_ptt, addr);\r\nswitch ((core_cfg & NVM_CFG1_GLOB_NETWORK_PORT_MODE_MASK) >>\r\nNVM_CFG1_GLOB_NETWORK_PORT_MODE_OFFSET) {\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_2X40G:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_2X40G;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_2X50G:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_2X50G;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_1X100G:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_1X100G;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_4X10G_F:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_4X10G_F;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_4X10G_E:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_4X10G_E;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_4X20G:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_4X20G;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_1X40G:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_1X40G;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_2X25G:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_2X25G;\r\nbreak;\r\ncase NVM_CFG1_GLOB_NETWORK_PORT_MODE_DE_1X25G:\r\np_hwfn->hw_info.port_mode = QED_PORT_MODE_DE_1X25G;\r\nbreak;\r\ndefault:\r\nDP_NOTICE(p_hwfn, "Unknown port mode in 0x%08x\n",\r\ncore_cfg);\r\nbreak;\r\n}\r\naddr = MCP_REG_SCRATCH + nvm_cfg1_offset +\r\noffsetof(struct nvm_cfg1, func[MCP_PF_ID(p_hwfn)]) +\r\noffsetof(struct nvm_cfg1_func, device_id);\r\nval = qed_rd(p_hwfn, p_ptt, addr);\r\nif (IS_MF(p_hwfn)) {\r\np_hwfn->hw_info.device_id =\r\n(val & NVM_CFG1_FUNC_MF_VENDOR_DEVICE_ID_MASK) >>\r\nNVM_CFG1_FUNC_MF_VENDOR_DEVICE_ID_OFFSET;\r\n} else {\r\np_hwfn->hw_info.device_id =\r\n(val & NVM_CFG1_FUNC_VENDOR_DEVICE_ID_MASK) >>\r\nNVM_CFG1_FUNC_VENDOR_DEVICE_ID_OFFSET;\r\n}\r\nlink = &p_hwfn->mcp_info->link_input;\r\nport_cfg_addr = MCP_REG_SCRATCH + nvm_cfg1_offset +\r\noffsetof(struct nvm_cfg1, port[MFW_PORT(p_hwfn)]);\r\nlink_temp = qed_rd(p_hwfn, p_ptt,\r\nport_cfg_addr +\r\noffsetof(struct nvm_cfg1_port, speed_cap_mask));\r\nlink->speed.advertised_speeds =\r\nlink_temp & NVM_CFG1_PORT_DRV_SPEED_CAPABILITY_MASK_MASK;\r\np_hwfn->mcp_info->link_capabilities.speed_capabilities =\r\nlink->speed.advertised_speeds;\r\nlink_temp = qed_rd(p_hwfn, p_ptt,\r\nport_cfg_addr +\r\noffsetof(struct nvm_cfg1_port, link_settings));\r\nswitch ((link_temp & NVM_CFG1_PORT_DRV_LINK_SPEED_MASK) >>\r\nNVM_CFG1_PORT_DRV_LINK_SPEED_OFFSET) {\r\ncase NVM_CFG1_PORT_DRV_LINK_SPEED_AUTONEG:\r\nlink->speed.autoneg = true;\r\nbreak;\r\ncase NVM_CFG1_PORT_DRV_LINK_SPEED_1G:\r\nlink->speed.forced_speed = 1000;\r\nbreak;\r\ncase NVM_CFG1_PORT_DRV_LINK_SPEED_10G:\r\nlink->speed.forced_speed = 10000;\r\nbreak;\r\ncase NVM_CFG1_PORT_DRV_LINK_SPEED_25G:\r\nlink->speed.forced_speed = 25000;\r\nbreak;\r\ncase NVM_CFG1_PORT_DRV_LINK_SPEED_40G:\r\nlink->speed.forced_speed = 40000;\r\nbreak;\r\ncase NVM_CFG1_PORT_DRV_LINK_SPEED_50G:\r\nlink->speed.forced_speed = 50000;\r\nbreak;\r\ncase NVM_CFG1_PORT_DRV_LINK_SPEED_100G:\r\nlink->speed.forced_speed = 100000;\r\nbreak;\r\ndefault:\r\nDP_NOTICE(p_hwfn, "Unknown Speed in 0x%08x\n",\r\nlink_temp);\r\n}\r\nlink_temp &= NVM_CFG1_PORT_DRV_FLOW_CONTROL_MASK;\r\nlink_temp >>= NVM_CFG1_PORT_DRV_FLOW_CONTROL_OFFSET;\r\nlink->pause.autoneg = !!(link_temp &\r\nNVM_CFG1_PORT_DRV_FLOW_CONTROL_AUTONEG);\r\nlink->pause.forced_rx = !!(link_temp &\r\nNVM_CFG1_PORT_DRV_FLOW_CONTROL_RX);\r\nlink->pause.forced_tx = !!(link_temp &\r\nNVM_CFG1_PORT_DRV_FLOW_CONTROL_TX);\r\nlink->loopback_mode = 0;\r\nDP_VERBOSE(p_hwfn, NETIF_MSG_LINK,\r\n"Read default link: Speed 0x%08x, Adv. Speed 0x%08x, AN: 0x%02x, PAUSE AN: 0x%02x\n",\r\nlink->speed.forced_speed, link->speed.advertised_speeds,\r\nlink->speed.autoneg, link->pause.autoneg);\r\naddr = MCP_REG_SCRATCH + nvm_cfg1_offset +\r\noffsetof(struct nvm_cfg1, glob) +\r\noffsetof(struct nvm_cfg1_glob, generic_cont0);\r\ngeneric_cont0 = qed_rd(p_hwfn, p_ptt, addr);\r\nmf_mode = (generic_cont0 & NVM_CFG1_GLOB_MF_MODE_MASK) >>\r\nNVM_CFG1_GLOB_MF_MODE_OFFSET;\r\nswitch (mf_mode) {\r\ncase NVM_CFG1_GLOB_MF_MODE_MF_ALLOWED:\r\np_hwfn->cdev->mf_mode = MF_OVLAN;\r\nbreak;\r\ncase NVM_CFG1_GLOB_MF_MODE_NPAR1_0:\r\np_hwfn->cdev->mf_mode = MF_NPAR;\r\nbreak;\r\ncase NVM_CFG1_GLOB_MF_MODE_FORCED_SF:\r\np_hwfn->cdev->mf_mode = SF;\r\nbreak;\r\n}\r\nDP_INFO(p_hwfn, "Multi function mode is %08x\n",\r\np_hwfn->cdev->mf_mode);\r\nreturn qed_mcp_fill_shmem_func_info(p_hwfn, p_ptt);\r\n}\r\nstatic int\r\nqed_get_hw_info(struct qed_hwfn *p_hwfn,\r\nstruct qed_ptt *p_ptt,\r\nenum qed_pci_personality personality)\r\n{\r\nu32 port_mode;\r\nint rc;\r\nport_mode = qed_rd(p_hwfn, p_ptt,\r\nCNIG_REG_NW_PORT_MODE_BB_B0);\r\nif (port_mode < 3) {\r\np_hwfn->cdev->num_ports_in_engines = 1;\r\n} else if (port_mode <= 5) {\r\np_hwfn->cdev->num_ports_in_engines = 2;\r\n} else {\r\nDP_NOTICE(p_hwfn, "PORT MODE: %d not supported\n",\r\np_hwfn->cdev->num_ports_in_engines);\r\np_hwfn->cdev->num_ports_in_engines = 1;\r\n}\r\nqed_hw_get_nvm_info(p_hwfn, p_ptt);\r\nrc = qed_int_igu_read_cam(p_hwfn, p_ptt);\r\nif (rc)\r\nreturn rc;\r\nif (qed_mcp_is_init(p_hwfn))\r\nether_addr_copy(p_hwfn->hw_info.hw_mac_addr,\r\np_hwfn->mcp_info->func_info.mac);\r\nelse\r\neth_random_addr(p_hwfn->hw_info.hw_mac_addr);\r\nif (qed_mcp_is_init(p_hwfn)) {\r\nif (p_hwfn->mcp_info->func_info.ovlan != QED_MCP_VLAN_UNSET)\r\np_hwfn->hw_info.ovlan =\r\np_hwfn->mcp_info->func_info.ovlan;\r\nqed_mcp_cmd_port_init(p_hwfn, p_ptt);\r\n}\r\nif (qed_mcp_is_init(p_hwfn)) {\r\nenum qed_pci_personality protocol;\r\nprotocol = p_hwfn->mcp_info->func_info.protocol;\r\np_hwfn->hw_info.personality = protocol;\r\n}\r\nqed_hw_get_resc(p_hwfn);\r\nreturn rc;\r\n}\r\nstatic void qed_get_dev_info(struct qed_dev *cdev)\r\n{\r\nu32 tmp;\r\ncdev->chip_num = (u16)qed_rd(cdev->hwfns, cdev->hwfns[0].p_main_ptt,\r\nMISCS_REG_CHIP_NUM);\r\ncdev->chip_rev = (u16)qed_rd(cdev->hwfns, cdev->hwfns[0].p_main_ptt,\r\nMISCS_REG_CHIP_REV);\r\nMASK_FIELD(CHIP_REV, cdev->chip_rev);\r\ntmp = qed_rd(cdev->hwfns, cdev->hwfns[0].p_main_ptt,\r\nMISCS_REG_CMT_ENABLED_FOR_PAIR);\r\nif (tmp & (1 << cdev->hwfns[0].rel_pf_id)) {\r\nDP_NOTICE(cdev->hwfns, "device in CMT mode\n");\r\ncdev->num_hwfns = 2;\r\n} else {\r\ncdev->num_hwfns = 1;\r\n}\r\ncdev->chip_bond_id = qed_rd(cdev->hwfns, cdev->hwfns[0].p_main_ptt,\r\nMISCS_REG_CHIP_TEST_REG) >> 4;\r\nMASK_FIELD(CHIP_BOND_ID, cdev->chip_bond_id);\r\ncdev->chip_metal = (u16)qed_rd(cdev->hwfns, cdev->hwfns[0].p_main_ptt,\r\nMISCS_REG_CHIP_METAL);\r\nMASK_FIELD(CHIP_METAL, cdev->chip_metal);\r\nDP_INFO(cdev->hwfns,\r\n"Chip details - Num: %04x Rev: %04x Bond id: %04x Metal: %04x\n",\r\ncdev->chip_num, cdev->chip_rev,\r\ncdev->chip_bond_id, cdev->chip_metal);\r\n}\r\nstatic int qed_hw_prepare_single(struct qed_hwfn *p_hwfn,\r\nvoid __iomem *p_regview,\r\nvoid __iomem *p_doorbells,\r\nenum qed_pci_personality personality)\r\n{\r\nint rc = 0;\r\np_hwfn->regview = p_regview;\r\np_hwfn->doorbells = p_doorbells;\r\nif (REG_RD(p_hwfn, PXP_PF_ME_OPAQUE_ADDR) == 0xffffffff) {\r\nDP_ERR(p_hwfn,\r\n"Reading the ME register returns all Fs; Preventing further chip access\n");\r\nreturn -EINVAL;\r\n}\r\nget_function_id(p_hwfn);\r\nrc = qed_hw_hwfn_prepare(p_hwfn);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "Failed to prepare hwfn's hw\n");\r\ngoto err0;\r\n}\r\nif (!p_hwfn->my_id)\r\nqed_get_dev_info(p_hwfn->cdev);\r\nrc = qed_mcp_cmd_init(p_hwfn, p_hwfn->p_main_ptt);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "Failed initializing mcp command\n");\r\ngoto err1;\r\n}\r\nrc = qed_get_hw_info(p_hwfn, p_hwfn->p_main_ptt, personality);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "Failed to get HW information\n");\r\ngoto err2;\r\n}\r\nrc = qed_init_alloc(p_hwfn);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "Failed to allocate the init array\n");\r\ngoto err2;\r\n}\r\nreturn rc;\r\nerr2:\r\nqed_mcp_free(p_hwfn);\r\nerr1:\r\nqed_hw_hwfn_free(p_hwfn);\r\nerr0:\r\nreturn rc;\r\n}\r\nstatic u32 qed_hw_bar_size(struct qed_hwfn *p_hwfn,\r\nu8 bar_id)\r\n{\r\nu32 bar_reg = (bar_id == 0 ? PGLUE_B_REG_PF_BAR0_SIZE\r\n: PGLUE_B_REG_PF_BAR1_SIZE);\r\nu32 val = qed_rd(p_hwfn, p_hwfn->p_main_ptt, bar_reg);\r\nreturn 1 << (val + 15);\r\n}\r\nint qed_hw_prepare(struct qed_dev *cdev,\r\nint personality)\r\n{\r\nstruct qed_hwfn *p_hwfn = QED_LEADING_HWFN(cdev);\r\nint rc;\r\nqed_init_iro_array(cdev);\r\nrc = qed_hw_prepare_single(p_hwfn,\r\ncdev->regview,\r\ncdev->doorbells, personality);\r\nif (rc)\r\nreturn rc;\r\npersonality = p_hwfn->hw_info.personality;\r\nif (cdev->num_hwfns > 1) {\r\nvoid __iomem *p_regview, *p_doorbell;\r\nu8 __iomem *addr;\r\naddr = cdev->regview + qed_hw_bar_size(p_hwfn, 0) / 2;\r\np_regview = addr;\r\naddr = cdev->doorbells + qed_hw_bar_size(p_hwfn, 1) / 2;\r\np_doorbell = addr;\r\nrc = qed_hw_prepare_single(&cdev->hwfns[1], p_regview,\r\np_doorbell, personality);\r\nif (rc) {\r\nqed_init_free(p_hwfn);\r\nqed_mcp_free(p_hwfn);\r\nqed_hw_hwfn_free(p_hwfn);\r\n}\r\n}\r\nreturn rc;\r\n}\r\nvoid qed_hw_remove(struct qed_dev *cdev)\r\n{\r\nint i;\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nqed_init_free(p_hwfn);\r\nqed_hw_hwfn_free(p_hwfn);\r\nqed_mcp_free(p_hwfn);\r\n}\r\n}\r\nint qed_chain_alloc(struct qed_dev *cdev,\r\nenum qed_chain_use_mode intended_use,\r\nenum qed_chain_mode mode,\r\nu16 num_elems,\r\nsize_t elem_size,\r\nstruct qed_chain *p_chain)\r\n{\r\ndma_addr_t p_pbl_phys = 0;\r\nvoid *p_pbl_virt = NULL;\r\ndma_addr_t p_phys = 0;\r\nvoid *p_virt = NULL;\r\nu16 page_cnt = 0;\r\nsize_t size;\r\nif (mode == QED_CHAIN_MODE_SINGLE)\r\npage_cnt = 1;\r\nelse\r\npage_cnt = QED_CHAIN_PAGE_CNT(num_elems, elem_size, mode);\r\nsize = page_cnt * QED_CHAIN_PAGE_SIZE;\r\np_virt = dma_alloc_coherent(&cdev->pdev->dev,\r\nsize, &p_phys, GFP_KERNEL);\r\nif (!p_virt) {\r\nDP_NOTICE(cdev, "Failed to allocate chain mem\n");\r\ngoto nomem;\r\n}\r\nif (mode == QED_CHAIN_MODE_PBL) {\r\nsize = page_cnt * QED_CHAIN_PBL_ENTRY_SIZE;\r\np_pbl_virt = dma_alloc_coherent(&cdev->pdev->dev,\r\nsize, &p_pbl_phys,\r\nGFP_KERNEL);\r\nif (!p_pbl_virt) {\r\nDP_NOTICE(cdev, "Failed to allocate chain pbl mem\n");\r\ngoto nomem;\r\n}\r\nqed_chain_pbl_init(p_chain, p_virt, p_phys, page_cnt,\r\n(u8)elem_size, intended_use,\r\np_pbl_phys, p_pbl_virt);\r\n} else {\r\nqed_chain_init(p_chain, p_virt, p_phys, page_cnt,\r\n(u8)elem_size, intended_use, mode);\r\n}\r\nreturn 0;\r\nnomem:\r\ndma_free_coherent(&cdev->pdev->dev,\r\npage_cnt * QED_CHAIN_PAGE_SIZE,\r\np_virt, p_phys);\r\ndma_free_coherent(&cdev->pdev->dev,\r\npage_cnt * QED_CHAIN_PBL_ENTRY_SIZE,\r\np_pbl_virt, p_pbl_phys);\r\nreturn -ENOMEM;\r\n}\r\nvoid qed_chain_free(struct qed_dev *cdev,\r\nstruct qed_chain *p_chain)\r\n{\r\nsize_t size;\r\nif (!p_chain->p_virt_addr)\r\nreturn;\r\nif (p_chain->mode == QED_CHAIN_MODE_PBL) {\r\nsize = p_chain->page_cnt * QED_CHAIN_PBL_ENTRY_SIZE;\r\ndma_free_coherent(&cdev->pdev->dev, size,\r\np_chain->pbl.p_virt_table,\r\np_chain->pbl.p_phys_table);\r\n}\r\nsize = p_chain->page_cnt * QED_CHAIN_PAGE_SIZE;\r\ndma_free_coherent(&cdev->pdev->dev, size,\r\np_chain->p_virt_addr,\r\np_chain->p_phys_addr);\r\n}\r\nstatic void __qed_get_vport_stats(struct qed_dev *cdev,\r\nstruct qed_eth_stats *stats)\r\n{\r\nint i, j;\r\nmemset(stats, 0, sizeof(*stats));\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nstruct eth_mstorm_per_queue_stat mstats;\r\nstruct eth_ustorm_per_queue_stat ustats;\r\nstruct eth_pstorm_per_queue_stat pstats;\r\nstruct tstorm_per_port_stat tstats;\r\nstruct port_stats port_stats;\r\nstruct qed_ptt *p_ptt = qed_ptt_acquire(p_hwfn);\r\nif (!p_ptt) {\r\nDP_ERR(p_hwfn, "Failed to acquire ptt\n");\r\ncontinue;\r\n}\r\nmemset(&mstats, 0, sizeof(mstats));\r\nqed_memcpy_from(p_hwfn, p_ptt, &mstats,\r\np_hwfn->storm_stats.mstats.address,\r\np_hwfn->storm_stats.mstats.len);\r\nmemset(&ustats, 0, sizeof(ustats));\r\nqed_memcpy_from(p_hwfn, p_ptt, &ustats,\r\np_hwfn->storm_stats.ustats.address,\r\np_hwfn->storm_stats.ustats.len);\r\nmemset(&pstats, 0, sizeof(pstats));\r\nqed_memcpy_from(p_hwfn, p_ptt, &pstats,\r\np_hwfn->storm_stats.pstats.address,\r\np_hwfn->storm_stats.pstats.len);\r\nmemset(&tstats, 0, sizeof(tstats));\r\nqed_memcpy_from(p_hwfn, p_ptt, &tstats,\r\np_hwfn->storm_stats.tstats.address,\r\np_hwfn->storm_stats.tstats.len);\r\nmemset(&port_stats, 0, sizeof(port_stats));\r\nif (p_hwfn->mcp_info)\r\nqed_memcpy_from(p_hwfn, p_ptt, &port_stats,\r\np_hwfn->mcp_info->port_addr +\r\noffsetof(struct public_port, stats),\r\nsizeof(port_stats));\r\nqed_ptt_release(p_hwfn, p_ptt);\r\nstats->no_buff_discards +=\r\nHILO_64_REGPAIR(mstats.no_buff_discard);\r\nstats->packet_too_big_discard +=\r\nHILO_64_REGPAIR(mstats.packet_too_big_discard);\r\nstats->ttl0_discard +=\r\nHILO_64_REGPAIR(mstats.ttl0_discard);\r\nstats->tpa_coalesced_pkts +=\r\nHILO_64_REGPAIR(mstats.tpa_coalesced_pkts);\r\nstats->tpa_coalesced_events +=\r\nHILO_64_REGPAIR(mstats.tpa_coalesced_events);\r\nstats->tpa_aborts_num +=\r\nHILO_64_REGPAIR(mstats.tpa_aborts_num);\r\nstats->tpa_coalesced_bytes +=\r\nHILO_64_REGPAIR(mstats.tpa_coalesced_bytes);\r\nstats->rx_ucast_bytes +=\r\nHILO_64_REGPAIR(ustats.rcv_ucast_bytes);\r\nstats->rx_mcast_bytes +=\r\nHILO_64_REGPAIR(ustats.rcv_mcast_bytes);\r\nstats->rx_bcast_bytes +=\r\nHILO_64_REGPAIR(ustats.rcv_bcast_bytes);\r\nstats->rx_ucast_pkts +=\r\nHILO_64_REGPAIR(ustats.rcv_ucast_pkts);\r\nstats->rx_mcast_pkts +=\r\nHILO_64_REGPAIR(ustats.rcv_mcast_pkts);\r\nstats->rx_bcast_pkts +=\r\nHILO_64_REGPAIR(ustats.rcv_bcast_pkts);\r\nstats->mftag_filter_discards +=\r\nHILO_64_REGPAIR(tstats.mftag_filter_discard);\r\nstats->mac_filter_discards +=\r\nHILO_64_REGPAIR(tstats.eth_mac_filter_discard);\r\nstats->tx_ucast_bytes +=\r\nHILO_64_REGPAIR(pstats.sent_ucast_bytes);\r\nstats->tx_mcast_bytes +=\r\nHILO_64_REGPAIR(pstats.sent_mcast_bytes);\r\nstats->tx_bcast_bytes +=\r\nHILO_64_REGPAIR(pstats.sent_bcast_bytes);\r\nstats->tx_ucast_pkts +=\r\nHILO_64_REGPAIR(pstats.sent_ucast_pkts);\r\nstats->tx_mcast_pkts +=\r\nHILO_64_REGPAIR(pstats.sent_mcast_pkts);\r\nstats->tx_bcast_pkts +=\r\nHILO_64_REGPAIR(pstats.sent_bcast_pkts);\r\nstats->tx_err_drop_pkts +=\r\nHILO_64_REGPAIR(pstats.error_drop_pkts);\r\nstats->rx_64_byte_packets += port_stats.pmm.r64;\r\nstats->rx_127_byte_packets += port_stats.pmm.r127;\r\nstats->rx_255_byte_packets += port_stats.pmm.r255;\r\nstats->rx_511_byte_packets += port_stats.pmm.r511;\r\nstats->rx_1023_byte_packets += port_stats.pmm.r1023;\r\nstats->rx_1518_byte_packets += port_stats.pmm.r1518;\r\nstats->rx_1522_byte_packets += port_stats.pmm.r1522;\r\nstats->rx_2047_byte_packets += port_stats.pmm.r2047;\r\nstats->rx_4095_byte_packets += port_stats.pmm.r4095;\r\nstats->rx_9216_byte_packets += port_stats.pmm.r9216;\r\nstats->rx_16383_byte_packets += port_stats.pmm.r16383;\r\nstats->rx_crc_errors += port_stats.pmm.rfcs;\r\nstats->rx_mac_crtl_frames += port_stats.pmm.rxcf;\r\nstats->rx_pause_frames += port_stats.pmm.rxpf;\r\nstats->rx_pfc_frames += port_stats.pmm.rxpp;\r\nstats->rx_align_errors += port_stats.pmm.raln;\r\nstats->rx_carrier_errors += port_stats.pmm.rfcr;\r\nstats->rx_oversize_packets += port_stats.pmm.rovr;\r\nstats->rx_jabbers += port_stats.pmm.rjbr;\r\nstats->rx_undersize_packets += port_stats.pmm.rund;\r\nstats->rx_fragments += port_stats.pmm.rfrg;\r\nstats->tx_64_byte_packets += port_stats.pmm.t64;\r\nstats->tx_65_to_127_byte_packets += port_stats.pmm.t127;\r\nstats->tx_128_to_255_byte_packets += port_stats.pmm.t255;\r\nstats->tx_256_to_511_byte_packets += port_stats.pmm.t511;\r\nstats->tx_512_to_1023_byte_packets += port_stats.pmm.t1023;\r\nstats->tx_1024_to_1518_byte_packets += port_stats.pmm.t1518;\r\nstats->tx_1519_to_2047_byte_packets += port_stats.pmm.t2047;\r\nstats->tx_2048_to_4095_byte_packets += port_stats.pmm.t4095;\r\nstats->tx_4096_to_9216_byte_packets += port_stats.pmm.t9216;\r\nstats->tx_9217_to_16383_byte_packets += port_stats.pmm.t16383;\r\nstats->tx_pause_frames += port_stats.pmm.txpf;\r\nstats->tx_pfc_frames += port_stats.pmm.txpp;\r\nstats->tx_lpi_entry_count += port_stats.pmm.tlpiec;\r\nstats->tx_total_collisions += port_stats.pmm.tncl;\r\nstats->rx_mac_bytes += port_stats.pmm.rbyte;\r\nstats->rx_mac_uc_packets += port_stats.pmm.rxuca;\r\nstats->rx_mac_mc_packets += port_stats.pmm.rxmca;\r\nstats->rx_mac_bc_packets += port_stats.pmm.rxbca;\r\nstats->rx_mac_frames_ok += port_stats.pmm.rxpok;\r\nstats->tx_mac_bytes += port_stats.pmm.tbyte;\r\nstats->tx_mac_uc_packets += port_stats.pmm.txuca;\r\nstats->tx_mac_mc_packets += port_stats.pmm.txmca;\r\nstats->tx_mac_bc_packets += port_stats.pmm.txbca;\r\nstats->tx_mac_ctrl_frames += port_stats.pmm.txcf;\r\nfor (j = 0; j < 8; j++) {\r\nstats->brb_truncates += port_stats.brb.brb_truncate[j];\r\nstats->brb_discards += port_stats.brb.brb_discard[j];\r\n}\r\n}\r\n}\r\nvoid qed_get_vport_stats(struct qed_dev *cdev,\r\nstruct qed_eth_stats *stats)\r\n{\r\nu32 i;\r\nif (!cdev) {\r\nmemset(stats, 0, sizeof(*stats));\r\nreturn;\r\n}\r\n__qed_get_vport_stats(cdev, stats);\r\nif (!cdev->reset_stats)\r\nreturn;\r\nfor (i = 0; i < sizeof(struct qed_eth_stats) / sizeof(u64); i++)\r\n((u64 *)stats)[i] -= ((u64 *)cdev->reset_stats)[i];\r\n}\r\nvoid qed_reset_vport_stats(struct qed_dev *cdev)\r\n{\r\nint i;\r\nfor_each_hwfn(cdev, i) {\r\nstruct qed_hwfn *p_hwfn = &cdev->hwfns[i];\r\nstruct eth_mstorm_per_queue_stat mstats;\r\nstruct eth_ustorm_per_queue_stat ustats;\r\nstruct eth_pstorm_per_queue_stat pstats;\r\nstruct qed_ptt *p_ptt = qed_ptt_acquire(p_hwfn);\r\nif (!p_ptt) {\r\nDP_ERR(p_hwfn, "Failed to acquire ptt\n");\r\ncontinue;\r\n}\r\nmemset(&mstats, 0, sizeof(mstats));\r\nqed_memcpy_to(p_hwfn, p_ptt,\r\np_hwfn->storm_stats.mstats.address,\r\n&mstats,\r\np_hwfn->storm_stats.mstats.len);\r\nmemset(&ustats, 0, sizeof(ustats));\r\nqed_memcpy_to(p_hwfn, p_ptt,\r\np_hwfn->storm_stats.ustats.address,\r\n&ustats,\r\np_hwfn->storm_stats.ustats.len);\r\nmemset(&pstats, 0, sizeof(pstats));\r\nqed_memcpy_to(p_hwfn, p_ptt,\r\np_hwfn->storm_stats.pstats.address,\r\n&pstats,\r\np_hwfn->storm_stats.pstats.len);\r\nqed_ptt_release(p_hwfn, p_ptt);\r\n}\r\nif (!cdev->reset_stats)\r\nDP_INFO(cdev, "Reset stats not allocated\n");\r\nelse\r\n__qed_get_vport_stats(cdev, cdev->reset_stats);\r\n}\r\nint qed_fw_l2_queue(struct qed_hwfn *p_hwfn,\r\nu16 src_id, u16 *dst_id)\r\n{\r\nif (src_id >= RESC_NUM(p_hwfn, QED_L2_QUEUE)) {\r\nu16 min, max;\r\nmin = (u16)RESC_START(p_hwfn, QED_L2_QUEUE);\r\nmax = min + RESC_NUM(p_hwfn, QED_L2_QUEUE);\r\nDP_NOTICE(p_hwfn,\r\n"l2_queue id [%d] is not valid, available indices [%d - %d]\n",\r\nsrc_id, min, max);\r\nreturn -EINVAL;\r\n}\r\n*dst_id = RESC_START(p_hwfn, QED_L2_QUEUE) + src_id;\r\nreturn 0;\r\n}\r\nint qed_fw_vport(struct qed_hwfn *p_hwfn,\r\nu8 src_id, u8 *dst_id)\r\n{\r\nif (src_id >= RESC_NUM(p_hwfn, QED_VPORT)) {\r\nu8 min, max;\r\nmin = (u8)RESC_START(p_hwfn, QED_VPORT);\r\nmax = min + RESC_NUM(p_hwfn, QED_VPORT);\r\nDP_NOTICE(p_hwfn,\r\n"vport id [%d] is not valid, available indices [%d - %d]\n",\r\nsrc_id, min, max);\r\nreturn -EINVAL;\r\n}\r\n*dst_id = RESC_START(p_hwfn, QED_VPORT) + src_id;\r\nreturn 0;\r\n}\r\nint qed_fw_rss_eng(struct qed_hwfn *p_hwfn,\r\nu8 src_id, u8 *dst_id)\r\n{\r\nif (src_id >= RESC_NUM(p_hwfn, QED_RSS_ENG)) {\r\nu8 min, max;\r\nmin = (u8)RESC_START(p_hwfn, QED_RSS_ENG);\r\nmax = min + RESC_NUM(p_hwfn, QED_RSS_ENG);\r\nDP_NOTICE(p_hwfn,\r\n"rss_eng id [%d] is not valid, available indices [%d - %d]\n",\r\nsrc_id, min, max);\r\nreturn -EINVAL;\r\n}\r\n*dst_id = RESC_START(p_hwfn, QED_RSS_ENG) + src_id;\r\nreturn 0;\r\n}
