static bool mutex_is_locked_by(struct mutex *mutex, struct task_struct *task)\r\n{\r\nif (!mutex_is_locked(mutex))\r\nreturn false;\r\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_MUTEXES)\r\nreturn mutex->owner == task;\r\n#else\r\nreturn false;\r\n#endif\r\n}\r\nunsigned long\r\ni915_gem_shrink(struct drm_i915_private *dev_priv,\r\nunsigned long target, unsigned flags)\r\n{\r\nconst struct {\r\nstruct list_head *list;\r\nunsigned int bit;\r\n} phases[] = {\r\n{ &dev_priv->mm.unbound_list, I915_SHRINK_UNBOUND },\r\n{ &dev_priv->mm.bound_list, I915_SHRINK_BOUND },\r\n{ NULL, 0 },\r\n}, *phase;\r\nunsigned long count = 0;\r\ntrace_i915_gem_shrink(dev_priv, target, flags);\r\ni915_gem_retire_requests(dev_priv->dev);\r\nfor (phase = phases; phase->list; phase++) {\r\nstruct list_head still_in_list;\r\nif ((flags & phase->bit) == 0)\r\ncontinue;\r\nINIT_LIST_HEAD(&still_in_list);\r\nwhile (count < target && !list_empty(phase->list)) {\r\nstruct drm_i915_gem_object *obj;\r\nstruct i915_vma *vma, *v;\r\nobj = list_first_entry(phase->list,\r\ntypeof(*obj), global_list);\r\nlist_move_tail(&obj->global_list, &still_in_list);\r\nif (flags & I915_SHRINK_PURGEABLE &&\r\nobj->madv != I915_MADV_DONTNEED)\r\ncontinue;\r\nif ((flags & I915_SHRINK_ACTIVE) == 0 && obj->active)\r\ncontinue;\r\ndrm_gem_object_reference(&obj->base);\r\nlist_for_each_entry_safe(vma, v,\r\n&obj->vma_list, vma_link)\r\nif (i915_vma_unbind(vma))\r\nbreak;\r\nif (i915_gem_object_put_pages(obj) == 0)\r\ncount += obj->base.size >> PAGE_SHIFT;\r\ndrm_gem_object_unreference(&obj->base);\r\n}\r\nlist_splice(&still_in_list, phase->list);\r\n}\r\ni915_gem_retire_requests(dev_priv->dev);\r\nreturn count;\r\n}\r\nunsigned long i915_gem_shrink_all(struct drm_i915_private *dev_priv)\r\n{\r\nreturn i915_gem_shrink(dev_priv, -1UL,\r\nI915_SHRINK_BOUND |\r\nI915_SHRINK_UNBOUND |\r\nI915_SHRINK_ACTIVE);\r\n}\r\nstatic bool i915_gem_shrinker_lock(struct drm_device *dev, bool *unlock)\r\n{\r\nif (!mutex_trylock(&dev->struct_mutex)) {\r\nif (!mutex_is_locked_by(&dev->struct_mutex, current))\r\nreturn false;\r\nif (to_i915(dev)->mm.shrinker_no_lock_stealing)\r\nreturn false;\r\n*unlock = false;\r\n} else\r\n*unlock = true;\r\nreturn true;\r\n}\r\nstatic int num_vma_bound(struct drm_i915_gem_object *obj)\r\n{\r\nstruct i915_vma *vma;\r\nint count = 0;\r\nlist_for_each_entry(vma, &obj->vma_list, vma_link) {\r\nif (drm_mm_node_allocated(&vma->node))\r\ncount++;\r\nif (vma->pin_count)\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nstatic unsigned long\r\ni915_gem_shrinker_count(struct shrinker *shrinker, struct shrink_control *sc)\r\n{\r\nstruct drm_i915_private *dev_priv =\r\ncontainer_of(shrinker, struct drm_i915_private, mm.shrinker);\r\nstruct drm_device *dev = dev_priv->dev;\r\nstruct drm_i915_gem_object *obj;\r\nunsigned long count;\r\nbool unlock;\r\nif (!i915_gem_shrinker_lock(dev, &unlock))\r\nreturn 0;\r\ncount = 0;\r\nlist_for_each_entry(obj, &dev_priv->mm.unbound_list, global_list)\r\nif (obj->pages_pin_count == 0)\r\ncount += obj->base.size >> PAGE_SHIFT;\r\nlist_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {\r\nif (!obj->active && obj->pages_pin_count == num_vma_bound(obj))\r\ncount += obj->base.size >> PAGE_SHIFT;\r\n}\r\nif (unlock)\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn count;\r\n}\r\nstatic unsigned long\r\ni915_gem_shrinker_scan(struct shrinker *shrinker, struct shrink_control *sc)\r\n{\r\nstruct drm_i915_private *dev_priv =\r\ncontainer_of(shrinker, struct drm_i915_private, mm.shrinker);\r\nstruct drm_device *dev = dev_priv->dev;\r\nunsigned long freed;\r\nbool unlock;\r\nif (!i915_gem_shrinker_lock(dev, &unlock))\r\nreturn SHRINK_STOP;\r\nfreed = i915_gem_shrink(dev_priv,\r\nsc->nr_to_scan,\r\nI915_SHRINK_BOUND |\r\nI915_SHRINK_UNBOUND |\r\nI915_SHRINK_PURGEABLE);\r\nif (freed < sc->nr_to_scan)\r\nfreed += i915_gem_shrink(dev_priv,\r\nsc->nr_to_scan - freed,\r\nI915_SHRINK_BOUND |\r\nI915_SHRINK_UNBOUND);\r\nif (unlock)\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn freed;\r\n}\r\nstatic int\r\ni915_gem_shrinker_oom(struct notifier_block *nb, unsigned long event, void *ptr)\r\n{\r\nstruct drm_i915_private *dev_priv =\r\ncontainer_of(nb, struct drm_i915_private, mm.oom_notifier);\r\nstruct drm_device *dev = dev_priv->dev;\r\nstruct drm_i915_gem_object *obj;\r\nunsigned long timeout = msecs_to_jiffies(5000) + 1;\r\nunsigned long pinned, bound, unbound, freed_pages;\r\nbool was_interruptible;\r\nbool unlock;\r\nwhile (!i915_gem_shrinker_lock(dev, &unlock) && --timeout) {\r\nschedule_timeout_killable(1);\r\nif (fatal_signal_pending(current))\r\nreturn NOTIFY_DONE;\r\n}\r\nif (timeout == 0) {\r\npr_err("Unable to purge GPU memory due lock contention.\n");\r\nreturn NOTIFY_DONE;\r\n}\r\nwas_interruptible = dev_priv->mm.interruptible;\r\ndev_priv->mm.interruptible = false;\r\nfreed_pages = i915_gem_shrink_all(dev_priv);\r\ndev_priv->mm.interruptible = was_interruptible;\r\nunbound = bound = pinned = 0;\r\nlist_for_each_entry(obj, &dev_priv->mm.unbound_list, global_list) {\r\nif (!obj->base.filp)\r\ncontinue;\r\nif (obj->pages_pin_count)\r\npinned += obj->base.size;\r\nelse\r\nunbound += obj->base.size;\r\n}\r\nlist_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {\r\nif (!obj->base.filp)\r\ncontinue;\r\nif (obj->pages_pin_count)\r\npinned += obj->base.size;\r\nelse\r\nbound += obj->base.size;\r\n}\r\nif (unlock)\r\nmutex_unlock(&dev->struct_mutex);\r\nif (freed_pages || unbound || bound)\r\npr_info("Purging GPU memory, %lu bytes freed, %lu bytes still pinned.\n",\r\nfreed_pages << PAGE_SHIFT, pinned);\r\nif (unbound || bound)\r\npr_err("%lu and %lu bytes still available in the "\r\n"bound and unbound GPU page lists.\n",\r\nbound, unbound);\r\n*(unsigned long *)ptr += freed_pages;\r\nreturn NOTIFY_DONE;\r\n}\r\nvoid i915_gem_shrinker_init(struct drm_i915_private *dev_priv)\r\n{\r\ndev_priv->mm.shrinker.scan_objects = i915_gem_shrinker_scan;\r\ndev_priv->mm.shrinker.count_objects = i915_gem_shrinker_count;\r\ndev_priv->mm.shrinker.seeks = DEFAULT_SEEKS;\r\nregister_shrinker(&dev_priv->mm.shrinker);\r\ndev_priv->mm.oom_notifier.notifier_call = i915_gem_shrinker_oom;\r\nregister_oom_notifier(&dev_priv->mm.oom_notifier);\r\n}
