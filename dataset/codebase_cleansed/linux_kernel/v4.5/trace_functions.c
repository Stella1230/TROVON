static int allocate_ftrace_ops(struct trace_array *tr)\r\n{\r\nstruct ftrace_ops *ops;\r\nops = kzalloc(sizeof(*ops), GFP_KERNEL);\r\nif (!ops)\r\nreturn -ENOMEM;\r\nops->func = function_trace_call;\r\nops->flags = FTRACE_OPS_FL_RECURSION_SAFE;\r\ntr->ops = ops;\r\nops->private = tr;\r\nreturn 0;\r\n}\r\nint ftrace_create_function_files(struct trace_array *tr,\r\nstruct dentry *parent)\r\n{\r\nint ret;\r\nif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\r\nreturn 0;\r\nret = allocate_ftrace_ops(tr);\r\nif (ret)\r\nreturn ret;\r\nftrace_create_filter_files(tr->ops, parent);\r\nreturn 0;\r\n}\r\nvoid ftrace_destroy_function_files(struct trace_array *tr)\r\n{\r\nftrace_destroy_filter_files(tr->ops);\r\nkfree(tr->ops);\r\ntr->ops = NULL;\r\n}\r\nstatic int function_trace_init(struct trace_array *tr)\r\n{\r\nftrace_func_t func;\r\nif (!tr->ops)\r\nreturn -ENOMEM;\r\nif (tr->flags & TRACE_ARRAY_FL_GLOBAL &&\r\nfunc_flags.val & TRACE_FUNC_OPT_STACK)\r\nfunc = function_stack_trace_call;\r\nelse\r\nfunc = function_trace_call;\r\nftrace_init_array_ops(tr, func);\r\ntr->trace_buffer.cpu = get_cpu();\r\nput_cpu();\r\ntracing_start_cmdline_record();\r\ntracing_start_function_trace(tr);\r\nreturn 0;\r\n}\r\nstatic void function_trace_reset(struct trace_array *tr)\r\n{\r\ntracing_stop_function_trace(tr);\r\ntracing_stop_cmdline_record();\r\nftrace_reset_array_ops(tr);\r\n}\r\nstatic void function_trace_start(struct trace_array *tr)\r\n{\r\ntracing_reset_online_cpus(&tr->trace_buffer);\r\n}\r\nstatic void\r\nfunction_trace_call(unsigned long ip, unsigned long parent_ip,\r\nstruct ftrace_ops *op, struct pt_regs *pt_regs)\r\n{\r\nstruct trace_array *tr = op->private;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint bit;\r\nint cpu;\r\nint pc;\r\nif (unlikely(!tr->function_enabled))\r\nreturn;\r\npc = preempt_count();\r\npreempt_disable_notrace();\r\nbit = trace_test_and_set_recursion(TRACE_FTRACE_START, TRACE_FTRACE_MAX);\r\nif (bit < 0)\r\ngoto out;\r\ncpu = smp_processor_id();\r\ndata = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\nif (!atomic_read(&data->disabled)) {\r\nlocal_save_flags(flags);\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\n}\r\ntrace_clear_recursion(bit);\r\nout:\r\npreempt_enable_notrace();\r\n}\r\nstatic void\r\nfunction_stack_trace_call(unsigned long ip, unsigned long parent_ip,\r\nstruct ftrace_ops *op, struct pt_regs *pt_regs)\r\n{\r\nstruct trace_array *tr = op->private;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nlong disabled;\r\nint cpu;\r\nint pc;\r\nif (unlikely(!tr->function_enabled))\r\nreturn;\r\nlocal_irq_save(flags);\r\ncpu = raw_smp_processor_id();\r\ndata = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\ndisabled = atomic_inc_return(&data->disabled);\r\nif (likely(disabled == 1)) {\r\npc = preempt_count();\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\n__trace_stack(tr, flags, 5, pc);\r\n}\r\natomic_dec(&data->disabled);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void tracing_start_function_trace(struct trace_array *tr)\r\n{\r\ntr->function_enabled = 0;\r\nregister_ftrace_function(tr->ops);\r\ntr->function_enabled = 1;\r\n}\r\nstatic void tracing_stop_function_trace(struct trace_array *tr)\r\n{\r\ntr->function_enabled = 0;\r\nunregister_ftrace_function(tr->ops);\r\n}\r\nstatic int\r\nfunc_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\r\n{\r\nswitch (bit) {\r\ncase TRACE_FUNC_OPT_STACK:\r\nif (!!set == !!(func_flags.val & TRACE_FUNC_OPT_STACK))\r\nbreak;\r\nunregister_ftrace_function(tr->ops);\r\nif (set) {\r\ntr->ops->func = function_stack_trace_call;\r\nregister_ftrace_function(tr->ops);\r\n} else {\r\ntr->ops->func = function_trace_call;\r\nregister_ftrace_function(tr->ops);\r\n}\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void update_traceon_count(void **data, bool on)\r\n{\r\nlong *count = (long *)data;\r\nlong old_count = *count;\r\nif (!old_count)\r\nreturn;\r\nsmp_rmb();\r\nif (on == !!tracing_is_on())\r\nreturn;\r\nif (on)\r\ntracing_on();\r\nelse\r\ntracing_off();\r\nif (old_count == -1)\r\nreturn;\r\nsmp_wmb();\r\n*count = old_count - 1;\r\n}\r\nstatic void\r\nftrace_traceon_count(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\nupdate_traceon_count(data, 1);\r\n}\r\nstatic void\r\nftrace_traceoff_count(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\nupdate_traceon_count(data, 0);\r\n}\r\nstatic void\r\nftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\nif (tracing_is_on())\r\nreturn;\r\ntracing_on();\r\n}\r\nstatic void\r\nftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\nif (!tracing_is_on())\r\nreturn;\r\ntracing_off();\r\n}\r\nstatic void\r\nftrace_stacktrace(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\ntrace_dump_stack(STACK_SKIP);\r\n}\r\nstatic void\r\nftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\nlong *count = (long *)data;\r\nlong old_count;\r\nlong new_count;\r\ndo {\r\nif (!tracing_is_on())\r\nreturn;\r\nold_count = *count;\r\nif (!old_count)\r\nreturn;\r\nif (old_count == -1) {\r\ntrace_dump_stack(STACK_SKIP);\r\nreturn;\r\n}\r\nnew_count = old_count - 1;\r\nnew_count = cmpxchg(count, old_count, new_count);\r\nif (new_count == old_count)\r\ntrace_dump_stack(STACK_SKIP);\r\n} while (new_count != old_count);\r\n}\r\nstatic int update_count(void **data)\r\n{\r\nunsigned long *count = (long *)data;\r\nif (!*count)\r\nreturn 0;\r\nif (*count != -1)\r\n(*count)--;\r\nreturn 1;\r\n}\r\nstatic void\r\nftrace_dump_probe(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\nif (update_count(data))\r\nftrace_dump(DUMP_ALL);\r\n}\r\nstatic void\r\nftrace_cpudump_probe(unsigned long ip, unsigned long parent_ip, void **data)\r\n{\r\nif (update_count(data))\r\nftrace_dump(DUMP_ORIG);\r\n}\r\nstatic int\r\nftrace_probe_print(const char *name, struct seq_file *m,\r\nunsigned long ip, void *data)\r\n{\r\nlong count = (long)data;\r\nseq_printf(m, "%ps:%s", (void *)ip, name);\r\nif (count == -1)\r\nseq_puts(m, ":unlimited\n");\r\nelse\r\nseq_printf(m, ":count=%ld\n", count);\r\nreturn 0;\r\n}\r\nstatic int\r\nftrace_traceon_print(struct seq_file *m, unsigned long ip,\r\nstruct ftrace_probe_ops *ops, void *data)\r\n{\r\nreturn ftrace_probe_print("traceon", m, ip, data);\r\n}\r\nstatic int\r\nftrace_traceoff_print(struct seq_file *m, unsigned long ip,\r\nstruct ftrace_probe_ops *ops, void *data)\r\n{\r\nreturn ftrace_probe_print("traceoff", m, ip, data);\r\n}\r\nstatic int\r\nftrace_stacktrace_print(struct seq_file *m, unsigned long ip,\r\nstruct ftrace_probe_ops *ops, void *data)\r\n{\r\nreturn ftrace_probe_print("stacktrace", m, ip, data);\r\n}\r\nstatic int\r\nftrace_dump_print(struct seq_file *m, unsigned long ip,\r\nstruct ftrace_probe_ops *ops, void *data)\r\n{\r\nreturn ftrace_probe_print("dump", m, ip, data);\r\n}\r\nstatic int\r\nftrace_cpudump_print(struct seq_file *m, unsigned long ip,\r\nstruct ftrace_probe_ops *ops, void *data)\r\n{\r\nreturn ftrace_probe_print("cpudump", m, ip, data);\r\n}\r\nstatic int\r\nftrace_trace_probe_callback(struct ftrace_probe_ops *ops,\r\nstruct ftrace_hash *hash, char *glob,\r\nchar *cmd, char *param, int enable)\r\n{\r\nvoid *count = (void *)-1;\r\nchar *number;\r\nint ret;\r\nif (!enable)\r\nreturn -EINVAL;\r\nif (glob[0] == '!') {\r\nunregister_ftrace_function_probe_func(glob+1, ops);\r\nreturn 0;\r\n}\r\nif (!param)\r\ngoto out_reg;\r\nnumber = strsep(&param, ":");\r\nif (!strlen(number))\r\ngoto out_reg;\r\nret = kstrtoul(number, 0, (unsigned long *)&count);\r\nif (ret)\r\nreturn ret;\r\nout_reg:\r\nret = register_ftrace_function_probe(glob, ops, count);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic int\r\nftrace_trace_onoff_callback(struct ftrace_hash *hash,\r\nchar *glob, char *cmd, char *param, int enable)\r\n{\r\nstruct ftrace_probe_ops *ops;\r\nif (strcmp(cmd, "traceon") == 0)\r\nops = param ? &traceon_count_probe_ops : &traceon_probe_ops;\r\nelse\r\nops = param ? &traceoff_count_probe_ops : &traceoff_probe_ops;\r\nreturn ftrace_trace_probe_callback(ops, hash, glob, cmd,\r\nparam, enable);\r\n}\r\nstatic int\r\nftrace_stacktrace_callback(struct ftrace_hash *hash,\r\nchar *glob, char *cmd, char *param, int enable)\r\n{\r\nstruct ftrace_probe_ops *ops;\r\nops = param ? &stacktrace_count_probe_ops : &stacktrace_probe_ops;\r\nreturn ftrace_trace_probe_callback(ops, hash, glob, cmd,\r\nparam, enable);\r\n}\r\nstatic int\r\nftrace_dump_callback(struct ftrace_hash *hash,\r\nchar *glob, char *cmd, char *param, int enable)\r\n{\r\nstruct ftrace_probe_ops *ops;\r\nops = &dump_probe_ops;\r\nreturn ftrace_trace_probe_callback(ops, hash, glob, cmd,\r\n"1", enable);\r\n}\r\nstatic int\r\nftrace_cpudump_callback(struct ftrace_hash *hash,\r\nchar *glob, char *cmd, char *param, int enable)\r\n{\r\nstruct ftrace_probe_ops *ops;\r\nops = &cpudump_probe_ops;\r\nreturn ftrace_trace_probe_callback(ops, hash, glob, cmd,\r\n"1", enable);\r\n}\r\nstatic int __init init_func_cmd_traceon(void)\r\n{\r\nint ret;\r\nret = register_ftrace_command(&ftrace_traceoff_cmd);\r\nif (ret)\r\nreturn ret;\r\nret = register_ftrace_command(&ftrace_traceon_cmd);\r\nif (ret)\r\ngoto out_free_traceoff;\r\nret = register_ftrace_command(&ftrace_stacktrace_cmd);\r\nif (ret)\r\ngoto out_free_traceon;\r\nret = register_ftrace_command(&ftrace_dump_cmd);\r\nif (ret)\r\ngoto out_free_stacktrace;\r\nret = register_ftrace_command(&ftrace_cpudump_cmd);\r\nif (ret)\r\ngoto out_free_dump;\r\nreturn 0;\r\nout_free_dump:\r\nunregister_ftrace_command(&ftrace_dump_cmd);\r\nout_free_stacktrace:\r\nunregister_ftrace_command(&ftrace_stacktrace_cmd);\r\nout_free_traceon:\r\nunregister_ftrace_command(&ftrace_traceon_cmd);\r\nout_free_traceoff:\r\nunregister_ftrace_command(&ftrace_traceoff_cmd);\r\nreturn ret;\r\n}\r\nstatic inline int init_func_cmd_traceon(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic __init int init_function_trace(void)\r\n{\r\ninit_func_cmd_traceon();\r\nreturn register_tracer(&function_trace);\r\n}
