void hfi1_cq_enter(struct hfi1_cq *cq, struct ib_wc *entry, int solicited)\r\n{\r\nstruct hfi1_cq_wc *wc;\r\nunsigned long flags;\r\nu32 head;\r\nu32 next;\r\nspin_lock_irqsave(&cq->lock, flags);\r\nwc = cq->queue;\r\nhead = wc->head;\r\nif (head >= (unsigned) cq->ibcq.cqe) {\r\nhead = cq->ibcq.cqe;\r\nnext = 0;\r\n} else\r\nnext = head + 1;\r\nif (unlikely(next == wc->tail)) {\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\nif (cq->ibcq.event_handler) {\r\nstruct ib_event ev;\r\nev.device = cq->ibcq.device;\r\nev.element.cq = &cq->ibcq;\r\nev.event = IB_EVENT_CQ_ERR;\r\ncq->ibcq.event_handler(&ev, cq->ibcq.cq_context);\r\n}\r\nreturn;\r\n}\r\nif (cq->ip) {\r\nwc->uqueue[head].wr_id = entry->wr_id;\r\nwc->uqueue[head].status = entry->status;\r\nwc->uqueue[head].opcode = entry->opcode;\r\nwc->uqueue[head].vendor_err = entry->vendor_err;\r\nwc->uqueue[head].byte_len = entry->byte_len;\r\nwc->uqueue[head].ex.imm_data =\r\n(__u32 __force)entry->ex.imm_data;\r\nwc->uqueue[head].qp_num = entry->qp->qp_num;\r\nwc->uqueue[head].src_qp = entry->src_qp;\r\nwc->uqueue[head].wc_flags = entry->wc_flags;\r\nwc->uqueue[head].pkey_index = entry->pkey_index;\r\nwc->uqueue[head].slid = entry->slid;\r\nwc->uqueue[head].sl = entry->sl;\r\nwc->uqueue[head].dlid_path_bits = entry->dlid_path_bits;\r\nwc->uqueue[head].port_num = entry->port_num;\r\nsmp_wmb();\r\n} else\r\nwc->kqueue[head] = *entry;\r\nwc->head = next;\r\nif (cq->notify == IB_CQ_NEXT_COMP ||\r\n(cq->notify == IB_CQ_SOLICITED &&\r\n(solicited || entry->status != IB_WC_SUCCESS))) {\r\nstruct kthread_worker *worker;\r\nsmp_read_barrier_depends();\r\nworker = cq->dd->worker;\r\nif (likely(worker)) {\r\ncq->notify = IB_CQ_NONE;\r\ncq->triggered++;\r\nqueue_kthread_work(worker, &cq->comptask);\r\n}\r\n}\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\n}\r\nint hfi1_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *entry)\r\n{\r\nstruct hfi1_cq *cq = to_icq(ibcq);\r\nstruct hfi1_cq_wc *wc;\r\nunsigned long flags;\r\nint npolled;\r\nu32 tail;\r\nif (cq->ip) {\r\nnpolled = -EINVAL;\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&cq->lock, flags);\r\nwc = cq->queue;\r\ntail = wc->tail;\r\nif (tail > (u32) cq->ibcq.cqe)\r\ntail = (u32) cq->ibcq.cqe;\r\nfor (npolled = 0; npolled < num_entries; ++npolled, ++entry) {\r\nif (tail == wc->head)\r\nbreak;\r\n*entry = wc->kqueue[tail];\r\nif (tail >= cq->ibcq.cqe)\r\ntail = 0;\r\nelse\r\ntail++;\r\n}\r\nwc->tail = tail;\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\nbail:\r\nreturn npolled;\r\n}\r\nstatic void send_complete(struct kthread_work *work)\r\n{\r\nstruct hfi1_cq *cq = container_of(work, struct hfi1_cq, comptask);\r\nfor (;;) {\r\nu8 triggered = cq->triggered;\r\nlocal_bh_disable();\r\ncq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);\r\nlocal_bh_enable();\r\nif (cq->triggered == triggered)\r\nreturn;\r\n}\r\n}\r\nstruct ib_cq *hfi1_create_cq(\r\nstruct ib_device *ibdev,\r\nconst struct ib_cq_init_attr *attr,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nstruct hfi1_ibdev *dev = to_idev(ibdev);\r\nstruct hfi1_cq *cq;\r\nstruct hfi1_cq_wc *wc;\r\nstruct ib_cq *ret;\r\nu32 sz;\r\nunsigned int entries = attr->cqe;\r\nif (attr->flags)\r\nreturn ERR_PTR(-EINVAL);\r\nif (entries < 1 || entries > hfi1_max_cqes)\r\nreturn ERR_PTR(-EINVAL);\r\ncq = kmalloc(sizeof(*cq), GFP_KERNEL);\r\nif (!cq)\r\nreturn ERR_PTR(-ENOMEM);\r\nsz = sizeof(*wc);\r\nif (udata && udata->outlen >= sizeof(__u64))\r\nsz += sizeof(struct ib_uverbs_wc) * (entries + 1);\r\nelse\r\nsz += sizeof(struct ib_wc) * (entries + 1);\r\nwc = vmalloc_user(sz);\r\nif (!wc) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_cq;\r\n}\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\nint err;\r\ncq->ip = hfi1_create_mmap_info(dev, sz, context, wc);\r\nif (!cq->ip) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_wc;\r\n}\r\nerr = ib_copy_to_udata(udata, &cq->ip->offset,\r\nsizeof(cq->ip->offset));\r\nif (err) {\r\nret = ERR_PTR(err);\r\ngoto bail_ip;\r\n}\r\n} else\r\ncq->ip = NULL;\r\nspin_lock(&dev->n_cqs_lock);\r\nif (dev->n_cqs_allocated == hfi1_max_cqs) {\r\nspin_unlock(&dev->n_cqs_lock);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_ip;\r\n}\r\ndev->n_cqs_allocated++;\r\nspin_unlock(&dev->n_cqs_lock);\r\nif (cq->ip) {\r\nspin_lock_irq(&dev->pending_lock);\r\nlist_add(&cq->ip->pending_mmaps, &dev->pending_mmaps);\r\nspin_unlock_irq(&dev->pending_lock);\r\n}\r\ncq->dd = dd_from_dev(dev);\r\ncq->ibcq.cqe = entries;\r\ncq->notify = IB_CQ_NONE;\r\ncq->triggered = 0;\r\nspin_lock_init(&cq->lock);\r\ninit_kthread_work(&cq->comptask, send_complete);\r\nwc->head = 0;\r\nwc->tail = 0;\r\ncq->queue = wc;\r\nret = &cq->ibcq;\r\ngoto done;\r\nbail_ip:\r\nkfree(cq->ip);\r\nbail_wc:\r\nvfree(wc);\r\nbail_cq:\r\nkfree(cq);\r\ndone:\r\nreturn ret;\r\n}\r\nint hfi1_destroy_cq(struct ib_cq *ibcq)\r\n{\r\nstruct hfi1_ibdev *dev = to_idev(ibcq->device);\r\nstruct hfi1_cq *cq = to_icq(ibcq);\r\nflush_kthread_work(&cq->comptask);\r\nspin_lock(&dev->n_cqs_lock);\r\ndev->n_cqs_allocated--;\r\nspin_unlock(&dev->n_cqs_lock);\r\nif (cq->ip)\r\nkref_put(&cq->ip->ref, hfi1_release_mmap_info);\r\nelse\r\nvfree(cq->queue);\r\nkfree(cq);\r\nreturn 0;\r\n}\r\nint hfi1_req_notify_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags notify_flags)\r\n{\r\nstruct hfi1_cq *cq = to_icq(ibcq);\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&cq->lock, flags);\r\nif (cq->notify != IB_CQ_NEXT_COMP)\r\ncq->notify = notify_flags & IB_CQ_SOLICITED_MASK;\r\nif ((notify_flags & IB_CQ_REPORT_MISSED_EVENTS) &&\r\ncq->queue->head != cq->queue->tail)\r\nret = 1;\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\nreturn ret;\r\n}\r\nint hfi1_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata)\r\n{\r\nstruct hfi1_cq *cq = to_icq(ibcq);\r\nstruct hfi1_cq_wc *old_wc;\r\nstruct hfi1_cq_wc *wc;\r\nu32 head, tail, n;\r\nint ret;\r\nu32 sz;\r\nif (cqe < 1 || cqe > hfi1_max_cqes) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nsz = sizeof(*wc);\r\nif (udata && udata->outlen >= sizeof(__u64))\r\nsz += sizeof(struct ib_uverbs_wc) * (cqe + 1);\r\nelse\r\nsz += sizeof(struct ib_wc) * (cqe + 1);\r\nwc = vmalloc_user(sz);\r\nif (!wc) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\n__u64 offset = 0;\r\nret = ib_copy_to_udata(udata, &offset, sizeof(offset));\r\nif (ret)\r\ngoto bail_free;\r\n}\r\nspin_lock_irq(&cq->lock);\r\nold_wc = cq->queue;\r\nhead = old_wc->head;\r\nif (head > (u32) cq->ibcq.cqe)\r\nhead = (u32) cq->ibcq.cqe;\r\ntail = old_wc->tail;\r\nif (tail > (u32) cq->ibcq.cqe)\r\ntail = (u32) cq->ibcq.cqe;\r\nif (head < tail)\r\nn = cq->ibcq.cqe + 1 + head - tail;\r\nelse\r\nn = head - tail;\r\nif (unlikely((u32)cqe < n)) {\r\nret = -EINVAL;\r\ngoto bail_unlock;\r\n}\r\nfor (n = 0; tail != head; n++) {\r\nif (cq->ip)\r\nwc->uqueue[n] = old_wc->uqueue[tail];\r\nelse\r\nwc->kqueue[n] = old_wc->kqueue[tail];\r\nif (tail == (u32) cq->ibcq.cqe)\r\ntail = 0;\r\nelse\r\ntail++;\r\n}\r\ncq->ibcq.cqe = cqe;\r\nwc->head = n;\r\nwc->tail = 0;\r\ncq->queue = wc;\r\nspin_unlock_irq(&cq->lock);\r\nvfree(old_wc);\r\nif (cq->ip) {\r\nstruct hfi1_ibdev *dev = to_idev(ibcq->device);\r\nstruct hfi1_mmap_info *ip = cq->ip;\r\nhfi1_update_mmap_info(dev, ip, sz, wc);\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\nret = ib_copy_to_udata(udata, &ip->offset,\r\nsizeof(ip->offset));\r\nif (ret)\r\ngoto bail;\r\n}\r\nspin_lock_irq(&dev->pending_lock);\r\nif (list_empty(&ip->pending_mmaps))\r\nlist_add(&ip->pending_mmaps, &dev->pending_mmaps);\r\nspin_unlock_irq(&dev->pending_lock);\r\n}\r\nret = 0;\r\ngoto bail;\r\nbail_unlock:\r\nspin_unlock_irq(&cq->lock);\r\nbail_free:\r\nvfree(wc);\r\nbail:\r\nreturn ret;\r\n}\r\nint hfi1_cq_init(struct hfi1_devdata *dd)\r\n{\r\nint ret = 0;\r\nint cpu;\r\nstruct task_struct *task;\r\nif (dd->worker)\r\nreturn 0;\r\ndd->worker = kzalloc(sizeof(*dd->worker), GFP_KERNEL);\r\nif (!dd->worker)\r\nreturn -ENOMEM;\r\ninit_kthread_worker(dd->worker);\r\ntask = kthread_create_on_node(\r\nkthread_worker_fn,\r\ndd->worker,\r\ndd->assigned_node_id,\r\n"hfi1_cq%d", dd->unit);\r\nif (IS_ERR(task))\r\ngoto task_fail;\r\ncpu = cpumask_first(cpumask_of_node(dd->assigned_node_id));\r\nkthread_bind(task, cpu);\r\nwake_up_process(task);\r\nout:\r\nreturn ret;\r\ntask_fail:\r\nret = PTR_ERR(task);\r\nkfree(dd->worker);\r\ndd->worker = NULL;\r\ngoto out;\r\n}\r\nvoid hfi1_cq_exit(struct hfi1_devdata *dd)\r\n{\r\nstruct kthread_worker *worker;\r\nworker = dd->worker;\r\nif (!worker)\r\nreturn;\r\ndd->worker = NULL;\r\nsmp_wmb();\r\nflush_kthread_worker(worker);\r\nkthread_stop(worker->task);\r\nkfree(worker);\r\n}
