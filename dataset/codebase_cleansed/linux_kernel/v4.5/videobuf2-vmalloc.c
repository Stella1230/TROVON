static void *vb2_vmalloc_alloc(void *alloc_ctx, unsigned long size,\r\nenum dma_data_direction dma_dir, gfp_t gfp_flags)\r\n{\r\nstruct vb2_vmalloc_buf *buf;\r\nbuf = kzalloc(sizeof(*buf), GFP_KERNEL | gfp_flags);\r\nif (!buf)\r\nreturn NULL;\r\nbuf->size = size;\r\nbuf->vaddr = vmalloc_user(buf->size);\r\nbuf->dma_dir = dma_dir;\r\nbuf->handler.refcount = &buf->refcount;\r\nbuf->handler.put = vb2_vmalloc_put;\r\nbuf->handler.arg = buf;\r\nif (!buf->vaddr) {\r\npr_debug("vmalloc of size %ld failed\n", buf->size);\r\nkfree(buf);\r\nreturn NULL;\r\n}\r\natomic_inc(&buf->refcount);\r\nreturn buf;\r\n}\r\nstatic void vb2_vmalloc_put(void *buf_priv)\r\n{\r\nstruct vb2_vmalloc_buf *buf = buf_priv;\r\nif (atomic_dec_and_test(&buf->refcount)) {\r\nvfree(buf->vaddr);\r\nkfree(buf);\r\n}\r\n}\r\nstatic void *vb2_vmalloc_get_userptr(void *alloc_ctx, unsigned long vaddr,\r\nunsigned long size,\r\nenum dma_data_direction dma_dir)\r\n{\r\nstruct vb2_vmalloc_buf *buf;\r\nstruct frame_vector *vec;\r\nint n_pages, offset, i;\r\nbuf = kzalloc(sizeof(*buf), GFP_KERNEL);\r\nif (!buf)\r\nreturn NULL;\r\nbuf->dma_dir = dma_dir;\r\noffset = vaddr & ~PAGE_MASK;\r\nbuf->size = size;\r\nvec = vb2_create_framevec(vaddr, size, dma_dir == DMA_FROM_DEVICE);\r\nif (IS_ERR(vec))\r\ngoto fail_pfnvec_create;\r\nbuf->vec = vec;\r\nn_pages = frame_vector_count(vec);\r\nif (frame_vector_to_pages(vec) < 0) {\r\nunsigned long *nums = frame_vector_pfns(vec);\r\nfor (i = 1; i < n_pages; i++)\r\nif (nums[i-1] + 1 != nums[i])\r\ngoto fail_map;\r\nbuf->vaddr = (__force void *)\r\nioremap_nocache(nums[0] << PAGE_SHIFT, size);\r\n} else {\r\nbuf->vaddr = vm_map_ram(frame_vector_pages(vec), n_pages, -1,\r\nPAGE_KERNEL);\r\n}\r\nif (!buf->vaddr)\r\ngoto fail_map;\r\nbuf->vaddr += offset;\r\nreturn buf;\r\nfail_map:\r\nvb2_destroy_framevec(vec);\r\nfail_pfnvec_create:\r\nkfree(buf);\r\nreturn NULL;\r\n}\r\nstatic void vb2_vmalloc_put_userptr(void *buf_priv)\r\n{\r\nstruct vb2_vmalloc_buf *buf = buf_priv;\r\nunsigned long vaddr = (unsigned long)buf->vaddr & PAGE_MASK;\r\nunsigned int i;\r\nstruct page **pages;\r\nunsigned int n_pages;\r\nif (!buf->vec->is_pfns) {\r\nn_pages = frame_vector_count(buf->vec);\r\npages = frame_vector_pages(buf->vec);\r\nif (vaddr)\r\nvm_unmap_ram((void *)vaddr, n_pages);\r\nif (buf->dma_dir == DMA_FROM_DEVICE)\r\nfor (i = 0; i < n_pages; i++)\r\nset_page_dirty_lock(pages[i]);\r\n} else {\r\niounmap((__force void __iomem *)buf->vaddr);\r\n}\r\nvb2_destroy_framevec(buf->vec);\r\nkfree(buf);\r\n}\r\nstatic void *vb2_vmalloc_vaddr(void *buf_priv)\r\n{\r\nstruct vb2_vmalloc_buf *buf = buf_priv;\r\nif (!buf->vaddr) {\r\npr_err("Address of an unallocated plane requested "\r\n"or cannot map user pointer\n");\r\nreturn NULL;\r\n}\r\nreturn buf->vaddr;\r\n}\r\nstatic unsigned int vb2_vmalloc_num_users(void *buf_priv)\r\n{\r\nstruct vb2_vmalloc_buf *buf = buf_priv;\r\nreturn atomic_read(&buf->refcount);\r\n}\r\nstatic int vb2_vmalloc_mmap(void *buf_priv, struct vm_area_struct *vma)\r\n{\r\nstruct vb2_vmalloc_buf *buf = buf_priv;\r\nint ret;\r\nif (!buf) {\r\npr_err("No memory to map\n");\r\nreturn -EINVAL;\r\n}\r\nret = remap_vmalloc_range(vma, buf->vaddr, 0);\r\nif (ret) {\r\npr_err("Remapping vmalloc memory, error: %d\n", ret);\r\nreturn ret;\r\n}\r\nvma->vm_flags |= VM_DONTEXPAND;\r\nvma->vm_private_data = &buf->handler;\r\nvma->vm_ops = &vb2_common_vm_ops;\r\nvma->vm_ops->open(vma);\r\nreturn 0;\r\n}\r\nstatic int vb2_vmalloc_dmabuf_ops_attach(struct dma_buf *dbuf, struct device *dev,\r\nstruct dma_buf_attachment *dbuf_attach)\r\n{\r\nstruct vb2_vmalloc_attachment *attach;\r\nstruct vb2_vmalloc_buf *buf = dbuf->priv;\r\nint num_pages = PAGE_ALIGN(buf->size) / PAGE_SIZE;\r\nstruct sg_table *sgt;\r\nstruct scatterlist *sg;\r\nvoid *vaddr = buf->vaddr;\r\nint ret;\r\nint i;\r\nattach = kzalloc(sizeof(*attach), GFP_KERNEL);\r\nif (!attach)\r\nreturn -ENOMEM;\r\nsgt = &attach->sgt;\r\nret = sg_alloc_table(sgt, num_pages, GFP_KERNEL);\r\nif (ret) {\r\nkfree(attach);\r\nreturn ret;\r\n}\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nstruct page *page = vmalloc_to_page(vaddr);\r\nif (!page) {\r\nsg_free_table(sgt);\r\nkfree(attach);\r\nreturn -ENOMEM;\r\n}\r\nsg_set_page(sg, page, PAGE_SIZE, 0);\r\nvaddr += PAGE_SIZE;\r\n}\r\nattach->dma_dir = DMA_NONE;\r\ndbuf_attach->priv = attach;\r\nreturn 0;\r\n}\r\nstatic void vb2_vmalloc_dmabuf_ops_detach(struct dma_buf *dbuf,\r\nstruct dma_buf_attachment *db_attach)\r\n{\r\nstruct vb2_vmalloc_attachment *attach = db_attach->priv;\r\nstruct sg_table *sgt;\r\nif (!attach)\r\nreturn;\r\nsgt = &attach->sgt;\r\nif (attach->dma_dir != DMA_NONE)\r\ndma_unmap_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\nattach->dma_dir);\r\nsg_free_table(sgt);\r\nkfree(attach);\r\ndb_attach->priv = NULL;\r\n}\r\nstatic struct sg_table *vb2_vmalloc_dmabuf_ops_map(\r\nstruct dma_buf_attachment *db_attach, enum dma_data_direction dma_dir)\r\n{\r\nstruct vb2_vmalloc_attachment *attach = db_attach->priv;\r\nstruct mutex *lock = &db_attach->dmabuf->lock;\r\nstruct sg_table *sgt;\r\nmutex_lock(lock);\r\nsgt = &attach->sgt;\r\nif (attach->dma_dir == dma_dir) {\r\nmutex_unlock(lock);\r\nreturn sgt;\r\n}\r\nif (attach->dma_dir != DMA_NONE) {\r\ndma_unmap_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\nattach->dma_dir);\r\nattach->dma_dir = DMA_NONE;\r\n}\r\nsgt->nents = dma_map_sg(db_attach->dev, sgt->sgl, sgt->orig_nents,\r\ndma_dir);\r\nif (!sgt->nents) {\r\npr_err("failed to map scatterlist\n");\r\nmutex_unlock(lock);\r\nreturn ERR_PTR(-EIO);\r\n}\r\nattach->dma_dir = dma_dir;\r\nmutex_unlock(lock);\r\nreturn sgt;\r\n}\r\nstatic void vb2_vmalloc_dmabuf_ops_unmap(struct dma_buf_attachment *db_attach,\r\nstruct sg_table *sgt, enum dma_data_direction dma_dir)\r\n{\r\n}\r\nstatic void vb2_vmalloc_dmabuf_ops_release(struct dma_buf *dbuf)\r\n{\r\nvb2_vmalloc_put(dbuf->priv);\r\n}\r\nstatic void *vb2_vmalloc_dmabuf_ops_kmap(struct dma_buf *dbuf, unsigned long pgnum)\r\n{\r\nstruct vb2_vmalloc_buf *buf = dbuf->priv;\r\nreturn buf->vaddr + pgnum * PAGE_SIZE;\r\n}\r\nstatic void *vb2_vmalloc_dmabuf_ops_vmap(struct dma_buf *dbuf)\r\n{\r\nstruct vb2_vmalloc_buf *buf = dbuf->priv;\r\nreturn buf->vaddr;\r\n}\r\nstatic int vb2_vmalloc_dmabuf_ops_mmap(struct dma_buf *dbuf,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn vb2_vmalloc_mmap(dbuf->priv, vma);\r\n}\r\nstatic struct dma_buf *vb2_vmalloc_get_dmabuf(void *buf_priv, unsigned long flags)\r\n{\r\nstruct vb2_vmalloc_buf *buf = buf_priv;\r\nstruct dma_buf *dbuf;\r\nDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\r\nexp_info.ops = &vb2_vmalloc_dmabuf_ops;\r\nexp_info.size = buf->size;\r\nexp_info.flags = flags;\r\nexp_info.priv = buf;\r\nif (WARN_ON(!buf->vaddr))\r\nreturn NULL;\r\ndbuf = dma_buf_export(&exp_info);\r\nif (IS_ERR(dbuf))\r\nreturn NULL;\r\natomic_inc(&buf->refcount);\r\nreturn dbuf;\r\n}\r\nstatic int vb2_vmalloc_map_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_vmalloc_buf *buf = mem_priv;\r\nbuf->vaddr = dma_buf_vmap(buf->dbuf);\r\nreturn buf->vaddr ? 0 : -EFAULT;\r\n}\r\nstatic void vb2_vmalloc_unmap_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_vmalloc_buf *buf = mem_priv;\r\ndma_buf_vunmap(buf->dbuf, buf->vaddr);\r\nbuf->vaddr = NULL;\r\n}\r\nstatic void vb2_vmalloc_detach_dmabuf(void *mem_priv)\r\n{\r\nstruct vb2_vmalloc_buf *buf = mem_priv;\r\nif (buf->vaddr)\r\ndma_buf_vunmap(buf->dbuf, buf->vaddr);\r\nkfree(buf);\r\n}\r\nstatic void *vb2_vmalloc_attach_dmabuf(void *alloc_ctx, struct dma_buf *dbuf,\r\nunsigned long size, enum dma_data_direction dma_dir)\r\n{\r\nstruct vb2_vmalloc_buf *buf;\r\nif (dbuf->size < size)\r\nreturn ERR_PTR(-EFAULT);\r\nbuf = kzalloc(sizeof(*buf), GFP_KERNEL);\r\nif (!buf)\r\nreturn ERR_PTR(-ENOMEM);\r\nbuf->dbuf = dbuf;\r\nbuf->dma_dir = dma_dir;\r\nbuf->size = size;\r\nreturn buf;\r\n}
