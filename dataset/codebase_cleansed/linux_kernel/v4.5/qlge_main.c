static int ql_sem_trylock(struct ql_adapter *qdev, u32 sem_mask)\r\n{\r\nu32 sem_bits = 0;\r\nswitch (sem_mask) {\r\ncase SEM_XGMAC0_MASK:\r\nsem_bits = SEM_SET << SEM_XGMAC0_SHIFT;\r\nbreak;\r\ncase SEM_XGMAC1_MASK:\r\nsem_bits = SEM_SET << SEM_XGMAC1_SHIFT;\r\nbreak;\r\ncase SEM_ICB_MASK:\r\nsem_bits = SEM_SET << SEM_ICB_SHIFT;\r\nbreak;\r\ncase SEM_MAC_ADDR_MASK:\r\nsem_bits = SEM_SET << SEM_MAC_ADDR_SHIFT;\r\nbreak;\r\ncase SEM_FLASH_MASK:\r\nsem_bits = SEM_SET << SEM_FLASH_SHIFT;\r\nbreak;\r\ncase SEM_PROBE_MASK:\r\nsem_bits = SEM_SET << SEM_PROBE_SHIFT;\r\nbreak;\r\ncase SEM_RT_IDX_MASK:\r\nsem_bits = SEM_SET << SEM_RT_IDX_SHIFT;\r\nbreak;\r\ncase SEM_PROC_REG_MASK:\r\nsem_bits = SEM_SET << SEM_PROC_REG_SHIFT;\r\nbreak;\r\ndefault:\r\nnetif_alert(qdev, probe, qdev->ndev, "bad Semaphore mask!.\n");\r\nreturn -EINVAL;\r\n}\r\nql_write32(qdev, SEM, sem_bits | sem_mask);\r\nreturn !(ql_read32(qdev, SEM) & sem_bits);\r\n}\r\nint ql_sem_spinlock(struct ql_adapter *qdev, u32 sem_mask)\r\n{\r\nunsigned int wait_count = 30;\r\ndo {\r\nif (!ql_sem_trylock(qdev, sem_mask))\r\nreturn 0;\r\nudelay(100);\r\n} while (--wait_count);\r\nreturn -ETIMEDOUT;\r\n}\r\nvoid ql_sem_unlock(struct ql_adapter *qdev, u32 sem_mask)\r\n{\r\nql_write32(qdev, SEM, sem_mask);\r\nql_read32(qdev, SEM);\r\n}\r\nint ql_wait_reg_rdy(struct ql_adapter *qdev, u32 reg, u32 bit, u32 err_bit)\r\n{\r\nu32 temp;\r\nint count = UDELAY_COUNT;\r\nwhile (count) {\r\ntemp = ql_read32(qdev, reg);\r\nif (temp & err_bit) {\r\nnetif_alert(qdev, probe, qdev->ndev,\r\n"register 0x%.08x access error, value = 0x%.08x!.\n",\r\nreg, temp);\r\nreturn -EIO;\r\n} else if (temp & bit)\r\nreturn 0;\r\nudelay(UDELAY_DELAY);\r\ncount--;\r\n}\r\nnetif_alert(qdev, probe, qdev->ndev,\r\n"Timed out waiting for reg %x to come ready.\n", reg);\r\nreturn -ETIMEDOUT;\r\n}\r\nstatic int ql_wait_cfg(struct ql_adapter *qdev, u32 bit)\r\n{\r\nint count = UDELAY_COUNT;\r\nu32 temp;\r\nwhile (count) {\r\ntemp = ql_read32(qdev, CFG);\r\nif (temp & CFG_LE)\r\nreturn -EIO;\r\nif (!(temp & bit))\r\nreturn 0;\r\nudelay(UDELAY_DELAY);\r\ncount--;\r\n}\r\nreturn -ETIMEDOUT;\r\n}\r\nint ql_write_cfg(struct ql_adapter *qdev, void *ptr, int size, u32 bit,\r\nu16 q_id)\r\n{\r\nu64 map;\r\nint status = 0;\r\nint direction;\r\nu32 mask;\r\nu32 value;\r\ndirection =\r\n(bit & (CFG_LRQ | CFG_LR | CFG_LCQ)) ? PCI_DMA_TODEVICE :\r\nPCI_DMA_FROMDEVICE;\r\nmap = pci_map_single(qdev->pdev, ptr, size, direction);\r\nif (pci_dma_mapping_error(qdev->pdev, map)) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Couldn't map DMA area.\n");\r\nreturn -ENOMEM;\r\n}\r\nstatus = ql_sem_spinlock(qdev, SEM_ICB_MASK);\r\nif (status)\r\nreturn status;\r\nstatus = ql_wait_cfg(qdev, bit);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Timed out waiting for CFG to come ready.\n");\r\ngoto exit;\r\n}\r\nql_write32(qdev, ICB_L, (u32) map);\r\nql_write32(qdev, ICB_H, (u32) (map >> 32));\r\nmask = CFG_Q_MASK | (bit << 16);\r\nvalue = bit | (q_id << CFG_Q_SHIFT);\r\nql_write32(qdev, CFG, (mask | value));\r\nstatus = ql_wait_cfg(qdev, bit);\r\nexit:\r\nql_sem_unlock(qdev, SEM_ICB_MASK);\r\npci_unmap_single(qdev->pdev, map, size, direction);\r\nreturn status;\r\n}\r\nint ql_get_mac_addr_reg(struct ql_adapter *qdev, u32 type, u16 index,\r\nu32 *value)\r\n{\r\nu32 offset = 0;\r\nint status;\r\nswitch (type) {\r\ncase MAC_ADDR_TYPE_MULTI_MAC:\r\ncase MAC_ADDR_TYPE_CAM_MAC:\r\n{\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset++) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\nMAC_ADDR_ADR | MAC_ADDR_RS | type);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MR, 0);\r\nif (status)\r\ngoto exit;\r\n*value++ = ql_read32(qdev, MAC_ADDR_DATA);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset++) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\nMAC_ADDR_ADR | MAC_ADDR_RS | type);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MR, 0);\r\nif (status)\r\ngoto exit;\r\n*value++ = ql_read32(qdev, MAC_ADDR_DATA);\r\nif (type == MAC_ADDR_TYPE_CAM_MAC) {\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset++) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\nMAC_ADDR_ADR | MAC_ADDR_RS | type);\r\nstatus =\r\nql_wait_reg_rdy(qdev, MAC_ADDR_IDX,\r\nMAC_ADDR_MR, 0);\r\nif (status)\r\ngoto exit;\r\n*value++ = ql_read32(qdev, MAC_ADDR_DATA);\r\n}\r\nbreak;\r\n}\r\ncase MAC_ADDR_TYPE_VLAN:\r\ncase MAC_ADDR_TYPE_MULTI_FLTR:\r\ndefault:\r\nnetif_crit(qdev, ifup, qdev->ndev,\r\n"Address type %d not yet supported.\n", type);\r\nstatus = -EPERM;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic int ql_set_mac_addr_reg(struct ql_adapter *qdev, u8 *addr, u32 type,\r\nu16 index)\r\n{\r\nu32 offset = 0;\r\nint status = 0;\r\nswitch (type) {\r\ncase MAC_ADDR_TYPE_MULTI_MAC:\r\n{\r\nu32 upper = (addr[0] << 8) | addr[1];\r\nu32 lower = (addr[2] << 24) | (addr[3] << 16) |\r\n(addr[4] << 8) | (addr[5]);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset++) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\ntype | MAC_ADDR_E);\r\nql_write32(qdev, MAC_ADDR_DATA, lower);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset++) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\ntype | MAC_ADDR_E);\r\nql_write32(qdev, MAC_ADDR_DATA, upper);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nbreak;\r\n}\r\ncase MAC_ADDR_TYPE_CAM_MAC:\r\n{\r\nu32 cam_output;\r\nu32 upper = (addr[0] << 8) | addr[1];\r\nu32 lower =\r\n(addr[2] << 24) | (addr[3] << 16) | (addr[4] << 8) |\r\n(addr[5]);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset++) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\ntype);\r\nql_write32(qdev, MAC_ADDR_DATA, lower);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset++) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\ntype);\r\nql_write32(qdev, MAC_ADDR_DATA, upper);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, (offset) |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\ntype);\r\ncam_output = (CAM_OUT_ROUTE_NIC |\r\n(qdev->\r\nfunc << CAM_OUT_FUNC_SHIFT) |\r\n(0 << CAM_OUT_CQ_ID_SHIFT));\r\nif (qdev->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)\r\ncam_output |= CAM_OUT_RV;\r\nql_write32(qdev, MAC_ADDR_DATA, cam_output);\r\nbreak;\r\n}\r\ncase MAC_ADDR_TYPE_VLAN:\r\n{\r\nu32 enable_bit = *((u32 *) &addr[0]);\r\nstatus =\r\nql_wait_reg_rdy(qdev,\r\nMAC_ADDR_IDX, MAC_ADDR_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, MAC_ADDR_IDX, offset |\r\n(index << MAC_ADDR_IDX_SHIFT) |\r\ntype |\r\nenable_bit);\r\nbreak;\r\n}\r\ncase MAC_ADDR_TYPE_MULTI_FLTR:\r\ndefault:\r\nnetif_crit(qdev, ifup, qdev->ndev,\r\n"Address type %d not yet supported.\n", type);\r\nstatus = -EPERM;\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic int ql_set_mac_addr(struct ql_adapter *qdev, int set)\r\n{\r\nint status;\r\nchar zero_mac_addr[ETH_ALEN];\r\nchar *addr;\r\nif (set) {\r\naddr = &qdev->current_mac_addr[0];\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"Set Mac addr %pM\n", addr);\r\n} else {\r\neth_zero_addr(zero_mac_addr);\r\naddr = &zero_mac_addr[0];\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"Clearing MAC address\n");\r\n}\r\nstatus = ql_sem_spinlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (status)\r\nreturn status;\r\nstatus = ql_set_mac_addr_reg(qdev, (u8 *) addr,\r\nMAC_ADDR_TYPE_CAM_MAC, qdev->func * MAX_CQ);\r\nql_sem_unlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (status)\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init mac address.\n");\r\nreturn status;\r\n}\r\nvoid ql_link_on(struct ql_adapter *qdev)\r\n{\r\nnetif_err(qdev, link, qdev->ndev, "Link is up.\n");\r\nnetif_carrier_on(qdev->ndev);\r\nql_set_mac_addr(qdev, 1);\r\n}\r\nvoid ql_link_off(struct ql_adapter *qdev)\r\n{\r\nnetif_err(qdev, link, qdev->ndev, "Link is down.\n");\r\nnetif_carrier_off(qdev->ndev);\r\nql_set_mac_addr(qdev, 0);\r\n}\r\nint ql_get_routing_reg(struct ql_adapter *qdev, u32 index, u32 *value)\r\n{\r\nint status = 0;\r\nstatus = ql_wait_reg_rdy(qdev, RT_IDX, RT_IDX_MW, 0);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, RT_IDX,\r\nRT_IDX_TYPE_NICQ | RT_IDX_RS | (index << RT_IDX_IDX_SHIFT));\r\nstatus = ql_wait_reg_rdy(qdev, RT_IDX, RT_IDX_MR, 0);\r\nif (status)\r\ngoto exit;\r\n*value = ql_read32(qdev, RT_DATA);\r\nexit:\r\nreturn status;\r\n}\r\nstatic int ql_set_routing_reg(struct ql_adapter *qdev, u32 index, u32 mask,\r\nint enable)\r\n{\r\nint status = -EINVAL;\r\nu32 value = 0;\r\nswitch (mask) {\r\ncase RT_IDX_CAM_HIT:\r\n{\r\nvalue = RT_IDX_DST_CAM_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_CAM_HIT_SLOT << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_VALID:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_PROMISCUOUS_SLOT << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_ERR:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_ALL_ERR_SLOT << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_IP_CSUM_ERR:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_IP_CSUM_ERR_SLOT <<\r\nRT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_TU_CSUM_ERR:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_TCP_UDP_CSUM_ERR_SLOT <<\r\nRT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_BCAST:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_BCAST_SLOT << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_MCAST:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_ALLMULTI_SLOT << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_MCAST_MATCH:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_MCAST_MATCH_SLOT << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase RT_IDX_RSS_MATCH:\r\n{\r\nvalue = RT_IDX_DST_RSS |\r\nRT_IDX_TYPE_NICQ |\r\n(RT_IDX_RSS_MATCH_SLOT << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ncase 0:\r\n{\r\nvalue = RT_IDX_DST_DFLT_Q |\r\nRT_IDX_TYPE_NICQ |\r\n(index << RT_IDX_IDX_SHIFT);\r\nbreak;\r\n}\r\ndefault:\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Mask type %d not yet supported.\n", mask);\r\nstatus = -EPERM;\r\ngoto exit;\r\n}\r\nif (value) {\r\nstatus = ql_wait_reg_rdy(qdev, RT_IDX, RT_IDX_MW, 0);\r\nif (status)\r\ngoto exit;\r\nvalue |= (enable ? RT_IDX_E : 0);\r\nql_write32(qdev, RT_IDX, value);\r\nql_write32(qdev, RT_DATA, enable ? mask : 0);\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nstatic void ql_enable_interrupts(struct ql_adapter *qdev)\r\n{\r\nql_write32(qdev, INTR_EN, (INTR_EN_EI << 16) | INTR_EN_EI);\r\n}\r\nstatic void ql_disable_interrupts(struct ql_adapter *qdev)\r\n{\r\nql_write32(qdev, INTR_EN, (INTR_EN_EI << 16));\r\n}\r\nu32 ql_enable_completion_interrupt(struct ql_adapter *qdev, u32 intr)\r\n{\r\nu32 var = 0;\r\nunsigned long hw_flags = 0;\r\nstruct intr_context *ctx = qdev->intr_context + intr;\r\nif (likely(test_bit(QL_MSIX_ENABLED, &qdev->flags) && intr)) {\r\nql_write32(qdev, INTR_EN,\r\nctx->intr_en_mask);\r\nvar = ql_read32(qdev, STS);\r\nreturn var;\r\n}\r\nspin_lock_irqsave(&qdev->hw_lock, hw_flags);\r\nif (atomic_dec_and_test(&ctx->irq_cnt)) {\r\nql_write32(qdev, INTR_EN,\r\nctx->intr_en_mask);\r\nvar = ql_read32(qdev, STS);\r\n}\r\nspin_unlock_irqrestore(&qdev->hw_lock, hw_flags);\r\nreturn var;\r\n}\r\nstatic u32 ql_disable_completion_interrupt(struct ql_adapter *qdev, u32 intr)\r\n{\r\nu32 var = 0;\r\nstruct intr_context *ctx;\r\nif (likely(test_bit(QL_MSIX_ENABLED, &qdev->flags) && intr))\r\nreturn 0;\r\nctx = qdev->intr_context + intr;\r\nspin_lock(&qdev->hw_lock);\r\nif (!atomic_read(&ctx->irq_cnt)) {\r\nql_write32(qdev, INTR_EN,\r\nctx->intr_dis_mask);\r\nvar = ql_read32(qdev, STS);\r\n}\r\natomic_inc(&ctx->irq_cnt);\r\nspin_unlock(&qdev->hw_lock);\r\nreturn var;\r\n}\r\nstatic void ql_enable_all_completion_interrupts(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nfor (i = 0; i < qdev->intr_count; i++) {\r\nif (unlikely(!test_bit(QL_MSIX_ENABLED, &qdev->flags) ||\r\ni == 0))\r\natomic_set(&qdev->intr_context[i].irq_cnt, 1);\r\nql_enable_completion_interrupt(qdev, i);\r\n}\r\n}\r\nstatic int ql_validate_flash(struct ql_adapter *qdev, u32 size, const char *str)\r\n{\r\nint status, i;\r\nu16 csum = 0;\r\n__le16 *flash = (__le16 *)&qdev->flash;\r\nstatus = strncmp((char *)&qdev->flash, str, 4);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Invalid flash signature.\n");\r\nreturn status;\r\n}\r\nfor (i = 0; i < size; i++)\r\ncsum += le16_to_cpu(*flash++);\r\nif (csum)\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Invalid flash checksum, csum = 0x%.04x.\n", csum);\r\nreturn csum;\r\n}\r\nstatic int ql_read_flash_word(struct ql_adapter *qdev, int offset, __le32 *data)\r\n{\r\nint status = 0;\r\nstatus = ql_wait_reg_rdy(qdev,\r\nFLASH_ADDR, FLASH_ADDR_RDY, FLASH_ADDR_ERR);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, FLASH_ADDR, FLASH_ADDR_R | offset);\r\nstatus = ql_wait_reg_rdy(qdev,\r\nFLASH_ADDR, FLASH_ADDR_RDY, FLASH_ADDR_ERR);\r\nif (status)\r\ngoto exit;\r\n*data = cpu_to_le32(ql_read32(qdev, FLASH_DATA));\r\nexit:\r\nreturn status;\r\n}\r\nstatic int ql_get_8000_flash_params(struct ql_adapter *qdev)\r\n{\r\nu32 i, size;\r\nint status;\r\n__le32 *p = (__le32 *)&qdev->flash;\r\nu32 offset;\r\nu8 mac_addr[6];\r\nif (!qdev->port)\r\noffset = FUNC0_FLASH_OFFSET / sizeof(u32);\r\nelse\r\noffset = FUNC1_FLASH_OFFSET / sizeof(u32);\r\nif (ql_sem_spinlock(qdev, SEM_FLASH_MASK))\r\nreturn -ETIMEDOUT;\r\nsize = sizeof(struct flash_params_8000) / sizeof(u32);\r\nfor (i = 0; i < size; i++, p++) {\r\nstatus = ql_read_flash_word(qdev, i+offset, p);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Error reading flash.\n");\r\ngoto exit;\r\n}\r\n}\r\nstatus = ql_validate_flash(qdev,\r\nsizeof(struct flash_params_8000) / sizeof(u16),\r\n"8000");\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Invalid flash.\n");\r\nstatus = -EINVAL;\r\ngoto exit;\r\n}\r\nif (qdev->flash.flash_params_8000.data_type1 == 2)\r\nmemcpy(mac_addr,\r\nqdev->flash.flash_params_8000.mac_addr1,\r\nqdev->ndev->addr_len);\r\nelse\r\nmemcpy(mac_addr,\r\nqdev->flash.flash_params_8000.mac_addr,\r\nqdev->ndev->addr_len);\r\nif (!is_valid_ether_addr(mac_addr)) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Invalid MAC address.\n");\r\nstatus = -EINVAL;\r\ngoto exit;\r\n}\r\nmemcpy(qdev->ndev->dev_addr,\r\nmac_addr,\r\nqdev->ndev->addr_len);\r\nexit:\r\nql_sem_unlock(qdev, SEM_FLASH_MASK);\r\nreturn status;\r\n}\r\nstatic int ql_get_8012_flash_params(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nint status;\r\n__le32 *p = (__le32 *)&qdev->flash;\r\nu32 offset = 0;\r\nu32 size = sizeof(struct flash_params_8012) / sizeof(u32);\r\nif (qdev->port)\r\noffset = size;\r\nif (ql_sem_spinlock(qdev, SEM_FLASH_MASK))\r\nreturn -ETIMEDOUT;\r\nfor (i = 0; i < size; i++, p++) {\r\nstatus = ql_read_flash_word(qdev, i+offset, p);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Error reading flash.\n");\r\ngoto exit;\r\n}\r\n}\r\nstatus = ql_validate_flash(qdev,\r\nsizeof(struct flash_params_8012) / sizeof(u16),\r\n"8012");\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Invalid flash.\n");\r\nstatus = -EINVAL;\r\ngoto exit;\r\n}\r\nif (!is_valid_ether_addr(qdev->flash.flash_params_8012.mac_addr)) {\r\nstatus = -EINVAL;\r\ngoto exit;\r\n}\r\nmemcpy(qdev->ndev->dev_addr,\r\nqdev->flash.flash_params_8012.mac_addr,\r\nqdev->ndev->addr_len);\r\nexit:\r\nql_sem_unlock(qdev, SEM_FLASH_MASK);\r\nreturn status;\r\n}\r\nstatic int ql_write_xgmac_reg(struct ql_adapter *qdev, u32 reg, u32 data)\r\n{\r\nint status;\r\nstatus = ql_wait_reg_rdy(qdev,\r\nXGMAC_ADDR, XGMAC_ADDR_RDY, XGMAC_ADDR_XME);\r\nif (status)\r\nreturn status;\r\nql_write32(qdev, XGMAC_DATA, data);\r\nql_write32(qdev, XGMAC_ADDR, reg);\r\nreturn status;\r\n}\r\nint ql_read_xgmac_reg(struct ql_adapter *qdev, u32 reg, u32 *data)\r\n{\r\nint status = 0;\r\nstatus = ql_wait_reg_rdy(qdev,\r\nXGMAC_ADDR, XGMAC_ADDR_RDY, XGMAC_ADDR_XME);\r\nif (status)\r\ngoto exit;\r\nql_write32(qdev, XGMAC_ADDR, reg | XGMAC_ADDR_R);\r\nstatus = ql_wait_reg_rdy(qdev,\r\nXGMAC_ADDR, XGMAC_ADDR_RDY, XGMAC_ADDR_XME);\r\nif (status)\r\ngoto exit;\r\n*data = ql_read32(qdev, XGMAC_DATA);\r\nexit:\r\nreturn status;\r\n}\r\nint ql_read_xgmac_reg64(struct ql_adapter *qdev, u32 reg, u64 *data)\r\n{\r\nint status = 0;\r\nu32 hi = 0;\r\nu32 lo = 0;\r\nstatus = ql_read_xgmac_reg(qdev, reg, &lo);\r\nif (status)\r\ngoto exit;\r\nstatus = ql_read_xgmac_reg(qdev, reg + 4, &hi);\r\nif (status)\r\ngoto exit;\r\n*data = (u64) lo | ((u64) hi << 32);\r\nexit:\r\nreturn status;\r\n}\r\nstatic int ql_8000_port_initialize(struct ql_adapter *qdev)\r\n{\r\nint status;\r\nstatus = ql_mb_about_fw(qdev);\r\nif (status)\r\ngoto exit;\r\nstatus = ql_mb_get_fw_state(qdev);\r\nif (status)\r\ngoto exit;\r\nqueue_delayed_work(qdev->workqueue, &qdev->mpi_port_cfg_work, 0);\r\nexit:\r\nreturn status;\r\n}\r\nstatic int ql_8012_port_initialize(struct ql_adapter *qdev)\r\n{\r\nint status = 0;\r\nu32 data;\r\nif (ql_sem_trylock(qdev, qdev->xg_sem_mask)) {\r\nnetif_info(qdev, link, qdev->ndev,\r\n"Another function has the semaphore, so wait for the port init bit to come ready.\n");\r\nstatus = ql_wait_reg_rdy(qdev, STS, qdev->port_init, 0);\r\nif (status) {\r\nnetif_crit(qdev, link, qdev->ndev,\r\n"Port initialize timed out.\n");\r\n}\r\nreturn status;\r\n}\r\nnetif_info(qdev, link, qdev->ndev, "Got xgmac semaphore!.\n");\r\nstatus = ql_read_xgmac_reg(qdev, GLOBAL_CFG, &data);\r\nif (status)\r\ngoto end;\r\ndata |= GLOBAL_CFG_RESET;\r\nstatus = ql_write_xgmac_reg(qdev, GLOBAL_CFG, data);\r\nif (status)\r\ngoto end;\r\ndata &= ~GLOBAL_CFG_RESET;\r\ndata |= GLOBAL_CFG_JUMBO;\r\ndata |= GLOBAL_CFG_TX_STAT_EN;\r\ndata |= GLOBAL_CFG_RX_STAT_EN;\r\nstatus = ql_write_xgmac_reg(qdev, GLOBAL_CFG, data);\r\nif (status)\r\ngoto end;\r\nstatus = ql_read_xgmac_reg(qdev, TX_CFG, &data);\r\nif (status)\r\ngoto end;\r\ndata &= ~TX_CFG_RESET;\r\ndata |= TX_CFG_EN;\r\nstatus = ql_write_xgmac_reg(qdev, TX_CFG, data);\r\nif (status)\r\ngoto end;\r\nstatus = ql_read_xgmac_reg(qdev, RX_CFG, &data);\r\nif (status)\r\ngoto end;\r\ndata &= ~RX_CFG_RESET;\r\ndata |= RX_CFG_EN;\r\nstatus = ql_write_xgmac_reg(qdev, RX_CFG, data);\r\nif (status)\r\ngoto end;\r\nstatus =\r\nql_write_xgmac_reg(qdev, MAC_TX_PARAMS, MAC_TX_PARAMS_JUMBO | (0x2580 << 16));\r\nif (status)\r\ngoto end;\r\nstatus =\r\nql_write_xgmac_reg(qdev, MAC_RX_PARAMS, 0x2580);\r\nif (status)\r\ngoto end;\r\nql_write32(qdev, STS, ((qdev->port_init << 16) | qdev->port_init));\r\nend:\r\nql_sem_unlock(qdev, qdev->xg_sem_mask);\r\nreturn status;\r\n}\r\nstatic inline unsigned int ql_lbq_block_size(struct ql_adapter *qdev)\r\n{\r\nreturn PAGE_SIZE << qdev->lbq_buf_order;\r\n}\r\nstatic struct bq_desc *ql_get_curr_lbuf(struct rx_ring *rx_ring)\r\n{\r\nstruct bq_desc *lbq_desc = &rx_ring->lbq[rx_ring->lbq_curr_idx];\r\nrx_ring->lbq_curr_idx++;\r\nif (rx_ring->lbq_curr_idx == rx_ring->lbq_len)\r\nrx_ring->lbq_curr_idx = 0;\r\nrx_ring->lbq_free_cnt++;\r\nreturn lbq_desc;\r\n}\r\nstatic struct bq_desc *ql_get_curr_lchunk(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring)\r\n{\r\nstruct bq_desc *lbq_desc = ql_get_curr_lbuf(rx_ring);\r\npci_dma_sync_single_for_cpu(qdev->pdev,\r\ndma_unmap_addr(lbq_desc, mapaddr),\r\nrx_ring->lbq_buf_size,\r\nPCI_DMA_FROMDEVICE);\r\nif ((lbq_desc->p.pg_chunk.offset + rx_ring->lbq_buf_size)\r\n== ql_lbq_block_size(qdev))\r\npci_unmap_page(qdev->pdev,\r\nlbq_desc->p.pg_chunk.map,\r\nql_lbq_block_size(qdev),\r\nPCI_DMA_FROMDEVICE);\r\nreturn lbq_desc;\r\n}\r\nstatic struct bq_desc *ql_get_curr_sbuf(struct rx_ring *rx_ring)\r\n{\r\nstruct bq_desc *sbq_desc = &rx_ring->sbq[rx_ring->sbq_curr_idx];\r\nrx_ring->sbq_curr_idx++;\r\nif (rx_ring->sbq_curr_idx == rx_ring->sbq_len)\r\nrx_ring->sbq_curr_idx = 0;\r\nrx_ring->sbq_free_cnt++;\r\nreturn sbq_desc;\r\n}\r\nstatic void ql_update_cq(struct rx_ring *rx_ring)\r\n{\r\nrx_ring->cnsmr_idx++;\r\nrx_ring->curr_entry++;\r\nif (unlikely(rx_ring->cnsmr_idx == rx_ring->cq_len)) {\r\nrx_ring->cnsmr_idx = 0;\r\nrx_ring->curr_entry = rx_ring->cq_base;\r\n}\r\n}\r\nstatic void ql_write_cq_idx(struct rx_ring *rx_ring)\r\n{\r\nql_write_db_reg(rx_ring->cnsmr_idx, rx_ring->cnsmr_idx_db_reg);\r\n}\r\nstatic int ql_get_next_chunk(struct ql_adapter *qdev, struct rx_ring *rx_ring,\r\nstruct bq_desc *lbq_desc)\r\n{\r\nif (!rx_ring->pg_chunk.page) {\r\nu64 map;\r\nrx_ring->pg_chunk.page = alloc_pages(__GFP_COLD | __GFP_COMP |\r\nGFP_ATOMIC,\r\nqdev->lbq_buf_order);\r\nif (unlikely(!rx_ring->pg_chunk.page)) {\r\nnetif_err(qdev, drv, qdev->ndev,\r\n"page allocation failed.\n");\r\nreturn -ENOMEM;\r\n}\r\nrx_ring->pg_chunk.offset = 0;\r\nmap = pci_map_page(qdev->pdev, rx_ring->pg_chunk.page,\r\n0, ql_lbq_block_size(qdev),\r\nPCI_DMA_FROMDEVICE);\r\nif (pci_dma_mapping_error(qdev->pdev, map)) {\r\n__free_pages(rx_ring->pg_chunk.page,\r\nqdev->lbq_buf_order);\r\nrx_ring->pg_chunk.page = NULL;\r\nnetif_err(qdev, drv, qdev->ndev,\r\n"PCI mapping failed.\n");\r\nreturn -ENOMEM;\r\n}\r\nrx_ring->pg_chunk.map = map;\r\nrx_ring->pg_chunk.va = page_address(rx_ring->pg_chunk.page);\r\n}\r\nlbq_desc->p.pg_chunk = rx_ring->pg_chunk;\r\nrx_ring->pg_chunk.offset += rx_ring->lbq_buf_size;\r\nif (rx_ring->pg_chunk.offset == ql_lbq_block_size(qdev)) {\r\nrx_ring->pg_chunk.page = NULL;\r\nlbq_desc->p.pg_chunk.last_flag = 1;\r\n} else {\r\nrx_ring->pg_chunk.va += rx_ring->lbq_buf_size;\r\nget_page(rx_ring->pg_chunk.page);\r\nlbq_desc->p.pg_chunk.last_flag = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ql_update_lbq(struct ql_adapter *qdev, struct rx_ring *rx_ring)\r\n{\r\nu32 clean_idx = rx_ring->lbq_clean_idx;\r\nu32 start_idx = clean_idx;\r\nstruct bq_desc *lbq_desc;\r\nu64 map;\r\nint i;\r\nwhile (rx_ring->lbq_free_cnt > 32) {\r\nfor (i = (rx_ring->lbq_clean_idx % 16); i < 16; i++) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"lbq: try cleaning clean_idx = %d.\n",\r\nclean_idx);\r\nlbq_desc = &rx_ring->lbq[clean_idx];\r\nif (ql_get_next_chunk(qdev, rx_ring, lbq_desc)) {\r\nrx_ring->lbq_clean_idx = clean_idx;\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Could not get a page chunk, i=%d, clean_idx =%d .\n",\r\ni, clean_idx);\r\nreturn;\r\n}\r\nmap = lbq_desc->p.pg_chunk.map +\r\nlbq_desc->p.pg_chunk.offset;\r\ndma_unmap_addr_set(lbq_desc, mapaddr, map);\r\ndma_unmap_len_set(lbq_desc, maplen,\r\nrx_ring->lbq_buf_size);\r\n*lbq_desc->addr = cpu_to_le64(map);\r\npci_dma_sync_single_for_device(qdev->pdev, map,\r\nrx_ring->lbq_buf_size,\r\nPCI_DMA_FROMDEVICE);\r\nclean_idx++;\r\nif (clean_idx == rx_ring->lbq_len)\r\nclean_idx = 0;\r\n}\r\nrx_ring->lbq_clean_idx = clean_idx;\r\nrx_ring->lbq_prod_idx += 16;\r\nif (rx_ring->lbq_prod_idx == rx_ring->lbq_len)\r\nrx_ring->lbq_prod_idx = 0;\r\nrx_ring->lbq_free_cnt -= 16;\r\n}\r\nif (start_idx != clean_idx) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"lbq: updating prod idx = %d.\n",\r\nrx_ring->lbq_prod_idx);\r\nql_write_db_reg(rx_ring->lbq_prod_idx,\r\nrx_ring->lbq_prod_idx_db_reg);\r\n}\r\n}\r\nstatic void ql_update_sbq(struct ql_adapter *qdev, struct rx_ring *rx_ring)\r\n{\r\nu32 clean_idx = rx_ring->sbq_clean_idx;\r\nu32 start_idx = clean_idx;\r\nstruct bq_desc *sbq_desc;\r\nu64 map;\r\nint i;\r\nwhile (rx_ring->sbq_free_cnt > 16) {\r\nfor (i = (rx_ring->sbq_clean_idx % 16); i < 16; i++) {\r\nsbq_desc = &rx_ring->sbq[clean_idx];\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"sbq: try cleaning clean_idx = %d.\n",\r\nclean_idx);\r\nif (sbq_desc->p.skb == NULL) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG,\r\nqdev->ndev,\r\n"sbq: getting new skb for index %d.\n",\r\nsbq_desc->index);\r\nsbq_desc->p.skb =\r\nnetdev_alloc_skb(qdev->ndev,\r\nSMALL_BUFFER_SIZE);\r\nif (sbq_desc->p.skb == NULL) {\r\nrx_ring->sbq_clean_idx = clean_idx;\r\nreturn;\r\n}\r\nskb_reserve(sbq_desc->p.skb, QLGE_SB_PAD);\r\nmap = pci_map_single(qdev->pdev,\r\nsbq_desc->p.skb->data,\r\nrx_ring->sbq_buf_size,\r\nPCI_DMA_FROMDEVICE);\r\nif (pci_dma_mapping_error(qdev->pdev, map)) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"PCI mapping failed.\n");\r\nrx_ring->sbq_clean_idx = clean_idx;\r\ndev_kfree_skb_any(sbq_desc->p.skb);\r\nsbq_desc->p.skb = NULL;\r\nreturn;\r\n}\r\ndma_unmap_addr_set(sbq_desc, mapaddr, map);\r\ndma_unmap_len_set(sbq_desc, maplen,\r\nrx_ring->sbq_buf_size);\r\n*sbq_desc->addr = cpu_to_le64(map);\r\n}\r\nclean_idx++;\r\nif (clean_idx == rx_ring->sbq_len)\r\nclean_idx = 0;\r\n}\r\nrx_ring->sbq_clean_idx = clean_idx;\r\nrx_ring->sbq_prod_idx += 16;\r\nif (rx_ring->sbq_prod_idx == rx_ring->sbq_len)\r\nrx_ring->sbq_prod_idx = 0;\r\nrx_ring->sbq_free_cnt -= 16;\r\n}\r\nif (start_idx != clean_idx) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"sbq: updating prod idx = %d.\n",\r\nrx_ring->sbq_prod_idx);\r\nql_write_db_reg(rx_ring->sbq_prod_idx,\r\nrx_ring->sbq_prod_idx_db_reg);\r\n}\r\n}\r\nstatic void ql_update_buffer_queues(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring)\r\n{\r\nql_update_sbq(qdev, rx_ring);\r\nql_update_lbq(qdev, rx_ring);\r\n}\r\nstatic void ql_unmap_send(struct ql_adapter *qdev,\r\nstruct tx_ring_desc *tx_ring_desc, int mapped)\r\n{\r\nint i;\r\nfor (i = 0; i < mapped; i++) {\r\nif (i == 0 || (i == 7 && mapped > 7)) {\r\nif (i == 7) {\r\nnetif_printk(qdev, tx_done, KERN_DEBUG,\r\nqdev->ndev,\r\n"unmapping OAL area.\n");\r\n}\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(&tx_ring_desc->map[i],\r\nmapaddr),\r\ndma_unmap_len(&tx_ring_desc->map[i],\r\nmaplen),\r\nPCI_DMA_TODEVICE);\r\n} else {\r\nnetif_printk(qdev, tx_done, KERN_DEBUG, qdev->ndev,\r\n"unmapping frag %d.\n", i);\r\npci_unmap_page(qdev->pdev,\r\ndma_unmap_addr(&tx_ring_desc->map[i],\r\nmapaddr),\r\ndma_unmap_len(&tx_ring_desc->map[i],\r\nmaplen), PCI_DMA_TODEVICE);\r\n}\r\n}\r\n}\r\nstatic int ql_map_send(struct ql_adapter *qdev,\r\nstruct ob_mac_iocb_req *mac_iocb_ptr,\r\nstruct sk_buff *skb, struct tx_ring_desc *tx_ring_desc)\r\n{\r\nint len = skb_headlen(skb);\r\ndma_addr_t map;\r\nint frag_idx, err, map_idx = 0;\r\nstruct tx_buf_desc *tbd = mac_iocb_ptr->tbd;\r\nint frag_cnt = skb_shinfo(skb)->nr_frags;\r\nif (frag_cnt) {\r\nnetif_printk(qdev, tx_queued, KERN_DEBUG, qdev->ndev,\r\n"frag_cnt = %d.\n", frag_cnt);\r\n}\r\nmap = pci_map_single(qdev->pdev, skb->data, len, PCI_DMA_TODEVICE);\r\nerr = pci_dma_mapping_error(qdev->pdev, map);\r\nif (err) {\r\nnetif_err(qdev, tx_queued, qdev->ndev,\r\n"PCI mapping failed with error: %d\n", err);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\ntbd->len = cpu_to_le32(len);\r\ntbd->addr = cpu_to_le64(map);\r\ndma_unmap_addr_set(&tx_ring_desc->map[map_idx], mapaddr, map);\r\ndma_unmap_len_set(&tx_ring_desc->map[map_idx], maplen, len);\r\nmap_idx++;\r\nfor (frag_idx = 0; frag_idx < frag_cnt; frag_idx++, map_idx++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[frag_idx];\r\ntbd++;\r\nif (frag_idx == 6 && frag_cnt > 7) {\r\nmap = pci_map_single(qdev->pdev, &tx_ring_desc->oal,\r\nsizeof(struct oal),\r\nPCI_DMA_TODEVICE);\r\nerr = pci_dma_mapping_error(qdev->pdev, map);\r\nif (err) {\r\nnetif_err(qdev, tx_queued, qdev->ndev,\r\n"PCI mapping outbound address list with error: %d\n",\r\nerr);\r\ngoto map_error;\r\n}\r\ntbd->addr = cpu_to_le64(map);\r\ntbd->len =\r\ncpu_to_le32((sizeof(struct tx_buf_desc) *\r\n(frag_cnt - frag_idx)) | TX_DESC_C);\r\ndma_unmap_addr_set(&tx_ring_desc->map[map_idx], mapaddr,\r\nmap);\r\ndma_unmap_len_set(&tx_ring_desc->map[map_idx], maplen,\r\nsizeof(struct oal));\r\ntbd = (struct tx_buf_desc *)&tx_ring_desc->oal;\r\nmap_idx++;\r\n}\r\nmap = skb_frag_dma_map(&qdev->pdev->dev, frag, 0, skb_frag_size(frag),\r\nDMA_TO_DEVICE);\r\nerr = dma_mapping_error(&qdev->pdev->dev, map);\r\nif (err) {\r\nnetif_err(qdev, tx_queued, qdev->ndev,\r\n"PCI mapping frags failed with error: %d.\n",\r\nerr);\r\ngoto map_error;\r\n}\r\ntbd->addr = cpu_to_le64(map);\r\ntbd->len = cpu_to_le32(skb_frag_size(frag));\r\ndma_unmap_addr_set(&tx_ring_desc->map[map_idx], mapaddr, map);\r\ndma_unmap_len_set(&tx_ring_desc->map[map_idx], maplen,\r\nskb_frag_size(frag));\r\n}\r\ntx_ring_desc->map_cnt = map_idx;\r\ntbd->len = cpu_to_le32(le32_to_cpu(tbd->len) | TX_DESC_E);\r\nreturn NETDEV_TX_OK;\r\nmap_error:\r\nql_unmap_send(qdev, tx_ring_desc, map_idx);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nstatic void ql_categorize_rx_err(struct ql_adapter *qdev, u8 rx_err,\r\nstruct rx_ring *rx_ring)\r\n{\r\nstruct nic_stats *stats = &qdev->nic_stats;\r\nstats->rx_err_count++;\r\nrx_ring->rx_errors++;\r\nswitch (rx_err & IB_MAC_IOCB_RSP_ERR_MASK) {\r\ncase IB_MAC_IOCB_RSP_ERR_CODE_ERR:\r\nstats->rx_code_err++;\r\nbreak;\r\ncase IB_MAC_IOCB_RSP_ERR_OVERSIZE:\r\nstats->rx_oversize_err++;\r\nbreak;\r\ncase IB_MAC_IOCB_RSP_ERR_UNDERSIZE:\r\nstats->rx_undersize_err++;\r\nbreak;\r\ncase IB_MAC_IOCB_RSP_ERR_PREAMBLE:\r\nstats->rx_preamble_err++;\r\nbreak;\r\ncase IB_MAC_IOCB_RSP_ERR_FRAME_LEN:\r\nstats->rx_frame_len_err++;\r\nbreak;\r\ncase IB_MAC_IOCB_RSP_ERR_CRC:\r\nstats->rx_crc_err++;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void ql_update_mac_hdr_len(struct ql_adapter *qdev,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp,\r\nvoid *page, size_t *len)\r\n{\r\nu16 *tags;\r\nif (qdev->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)\r\nreturn;\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_V) {\r\ntags = (u16 *)page;\r\nif (tags[6] == ETH_P_8021Q &&\r\ntags[8] == ETH_P_8021Q)\r\n*len += 2 * VLAN_HLEN;\r\nelse\r\n*len += VLAN_HLEN;\r\n}\r\n}\r\nstatic void ql_process_mac_rx_gro_page(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp,\r\nu32 length,\r\nu16 vlan_id)\r\n{\r\nstruct sk_buff *skb;\r\nstruct bq_desc *lbq_desc = ql_get_curr_lchunk(qdev, rx_ring);\r\nstruct napi_struct *napi = &rx_ring->napi;\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_ERR_MASK) {\r\nql_categorize_rx_err(qdev, ib_mac_rsp->flags2, rx_ring);\r\nput_page(lbq_desc->p.pg_chunk.page);\r\nreturn;\r\n}\r\nnapi->dev = qdev->ndev;\r\nskb = napi_get_frags(napi);\r\nif (!skb) {\r\nnetif_err(qdev, drv, qdev->ndev,\r\n"Couldn't get an skb, exiting.\n");\r\nrx_ring->rx_dropped++;\r\nput_page(lbq_desc->p.pg_chunk.page);\r\nreturn;\r\n}\r\nprefetch(lbq_desc->p.pg_chunk.va);\r\n__skb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\r\nlbq_desc->p.pg_chunk.page,\r\nlbq_desc->p.pg_chunk.offset,\r\nlength);\r\nskb->len += length;\r\nskb->data_len += length;\r\nskb->truesize += length;\r\nskb_shinfo(skb)->nr_frags++;\r\nrx_ring->rx_packets++;\r\nrx_ring->rx_bytes += length;\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb_record_rx_queue(skb, rx_ring->cq_id);\r\nif (vlan_id != 0xffff)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_id);\r\nnapi_gro_frags(napi);\r\n}\r\nstatic void ql_process_mac_rx_page(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp,\r\nu32 length,\r\nu16 vlan_id)\r\n{\r\nstruct net_device *ndev = qdev->ndev;\r\nstruct sk_buff *skb = NULL;\r\nvoid *addr;\r\nstruct bq_desc *lbq_desc = ql_get_curr_lchunk(qdev, rx_ring);\r\nstruct napi_struct *napi = &rx_ring->napi;\r\nsize_t hlen = ETH_HLEN;\r\nskb = netdev_alloc_skb(ndev, length);\r\nif (!skb) {\r\nrx_ring->rx_dropped++;\r\nput_page(lbq_desc->p.pg_chunk.page);\r\nreturn;\r\n}\r\naddr = lbq_desc->p.pg_chunk.va;\r\nprefetch(addr);\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_ERR_MASK) {\r\nql_categorize_rx_err(qdev, ib_mac_rsp->flags2, rx_ring);\r\ngoto err_out;\r\n}\r\nql_update_mac_hdr_len(qdev, ib_mac_rsp, addr, &hlen);\r\nif (skb->len > ndev->mtu + hlen) {\r\nnetif_err(qdev, drv, qdev->ndev,\r\n"Segment too small, dropping.\n");\r\nrx_ring->rx_dropped++;\r\ngoto err_out;\r\n}\r\nmemcpy(skb_put(skb, hlen), addr, hlen);\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"%d bytes of headers and data in large. Chain page to new skb and pull tail.\n",\r\nlength);\r\nskb_fill_page_desc(skb, 0, lbq_desc->p.pg_chunk.page,\r\nlbq_desc->p.pg_chunk.offset + hlen,\r\nlength - hlen);\r\nskb->len += length - hlen;\r\nskb->data_len += length - hlen;\r\nskb->truesize += length - hlen;\r\nrx_ring->rx_packets++;\r\nrx_ring->rx_bytes += skb->len;\r\nskb->protocol = eth_type_trans(skb, ndev);\r\nskb_checksum_none_assert(skb);\r\nif ((ndev->features & NETIF_F_RXCSUM) &&\r\n!(ib_mac_rsp->flags1 & IB_MAC_CSUM_ERR_MASK)) {\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_T) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"TCP checksum done!\n");\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n} else if ((ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_U) &&\r\n(ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_V4)) {\r\nstruct iphdr *iph =\r\n(struct iphdr *)((u8 *)addr + hlen);\r\nif (!(iph->frag_off &\r\nhtons(IP_MF|IP_OFFSET))) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nnetif_printk(qdev, rx_status, KERN_DEBUG,\r\nqdev->ndev,\r\n"UDP checksum done!\n");\r\n}\r\n}\r\n}\r\nskb_record_rx_queue(skb, rx_ring->cq_id);\r\nif (vlan_id != 0xffff)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_id);\r\nif (skb->ip_summed == CHECKSUM_UNNECESSARY)\r\nnapi_gro_receive(napi, skb);\r\nelse\r\nnetif_receive_skb(skb);\r\nreturn;\r\nerr_out:\r\ndev_kfree_skb_any(skb);\r\nput_page(lbq_desc->p.pg_chunk.page);\r\n}\r\nstatic void ql_process_mac_rx_skb(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp,\r\nu32 length,\r\nu16 vlan_id)\r\n{\r\nstruct net_device *ndev = qdev->ndev;\r\nstruct sk_buff *skb = NULL;\r\nstruct sk_buff *new_skb = NULL;\r\nstruct bq_desc *sbq_desc = ql_get_curr_sbuf(rx_ring);\r\nskb = sbq_desc->p.skb;\r\nnew_skb = netdev_alloc_skb(qdev->ndev, length + NET_IP_ALIGN);\r\nif (new_skb == NULL) {\r\nrx_ring->rx_dropped++;\r\nreturn;\r\n}\r\nskb_reserve(new_skb, NET_IP_ALIGN);\r\nmemcpy(skb_put(new_skb, length), skb->data, length);\r\nskb = new_skb;\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_ERR_MASK) {\r\nql_categorize_rx_err(qdev, ib_mac_rsp->flags2, rx_ring);\r\ndev_kfree_skb_any(skb);\r\nreturn;\r\n}\r\nif (test_bit(QL_SELFTEST, &qdev->flags)) {\r\nql_check_lb_frame(qdev, skb);\r\ndev_kfree_skb_any(skb);\r\nreturn;\r\n}\r\nif (skb->len > ndev->mtu + ETH_HLEN) {\r\ndev_kfree_skb_any(skb);\r\nrx_ring->rx_dropped++;\r\nreturn;\r\n}\r\nprefetch(skb->data);\r\nif (ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"%s Multicast.\n",\r\n(ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) ==\r\nIB_MAC_IOCB_RSP_M_HASH ? "Hash" :\r\n(ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) ==\r\nIB_MAC_IOCB_RSP_M_REG ? "Registered" :\r\n(ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) ==\r\nIB_MAC_IOCB_RSP_M_PROM ? "Promiscuous" : "");\r\n}\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_P)\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Promiscuous Packet.\n");\r\nrx_ring->rx_packets++;\r\nrx_ring->rx_bytes += skb->len;\r\nskb->protocol = eth_type_trans(skb, ndev);\r\nskb_checksum_none_assert(skb);\r\nif ((ndev->features & NETIF_F_RXCSUM) &&\r\n!(ib_mac_rsp->flags1 & IB_MAC_CSUM_ERR_MASK)) {\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_T) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"TCP checksum done!\n");\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n} else if ((ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_U) &&\r\n(ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_V4)) {\r\nstruct iphdr *iph = (struct iphdr *) skb->data;\r\nif (!(iph->frag_off &\r\nhtons(IP_MF|IP_OFFSET))) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nnetif_printk(qdev, rx_status, KERN_DEBUG,\r\nqdev->ndev,\r\n"UDP checksum done!\n");\r\n}\r\n}\r\n}\r\nskb_record_rx_queue(skb, rx_ring->cq_id);\r\nif (vlan_id != 0xffff)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_id);\r\nif (skb->ip_summed == CHECKSUM_UNNECESSARY)\r\nnapi_gro_receive(&rx_ring->napi, skb);\r\nelse\r\nnetif_receive_skb(skb);\r\n}\r\nstatic void ql_realign_skb(struct sk_buff *skb, int len)\r\n{\r\nvoid *temp_addr = skb->data;\r\nskb->data -= QLGE_SB_PAD - NET_IP_ALIGN;\r\nskb->tail -= QLGE_SB_PAD - NET_IP_ALIGN;\r\nskb_copy_to_linear_data(skb, temp_addr,\r\n(unsigned int)len);\r\n}\r\nstatic struct sk_buff *ql_build_rx_skb(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp)\r\n{\r\nstruct bq_desc *lbq_desc;\r\nstruct bq_desc *sbq_desc;\r\nstruct sk_buff *skb = NULL;\r\nu32 length = le32_to_cpu(ib_mac_rsp->data_len);\r\nu32 hdr_len = le32_to_cpu(ib_mac_rsp->hdr_len);\r\nsize_t hlen = ETH_HLEN;\r\nif (ib_mac_rsp->flags4 & IB_MAC_IOCB_RSP_HV &&\r\nib_mac_rsp->flags4 & IB_MAC_IOCB_RSP_HS) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Header of %d bytes in small buffer.\n", hdr_len);\r\nsbq_desc = ql_get_curr_sbuf(rx_ring);\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(sbq_desc, mapaddr),\r\ndma_unmap_len(sbq_desc, maplen),\r\nPCI_DMA_FROMDEVICE);\r\nskb = sbq_desc->p.skb;\r\nql_realign_skb(skb, hdr_len);\r\nskb_put(skb, hdr_len);\r\nsbq_desc->p.skb = NULL;\r\n}\r\nif (unlikely(!length)) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"No Data buffer in this packet.\n");\r\nreturn skb;\r\n}\r\nif (ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_DS) {\r\nif (ib_mac_rsp->flags4 & IB_MAC_IOCB_RSP_HS) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Headers in small, data of %d bytes in small, combine them.\n",\r\nlength);\r\nsbq_desc = ql_get_curr_sbuf(rx_ring);\r\npci_dma_sync_single_for_cpu(qdev->pdev,\r\ndma_unmap_addr\r\n(sbq_desc, mapaddr),\r\ndma_unmap_len\r\n(sbq_desc, maplen),\r\nPCI_DMA_FROMDEVICE);\r\nmemcpy(skb_put(skb, length),\r\nsbq_desc->p.skb->data, length);\r\npci_dma_sync_single_for_device(qdev->pdev,\r\ndma_unmap_addr\r\n(sbq_desc,\r\nmapaddr),\r\ndma_unmap_len\r\n(sbq_desc,\r\nmaplen),\r\nPCI_DMA_FROMDEVICE);\r\n} else {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"%d bytes in a single small buffer.\n",\r\nlength);\r\nsbq_desc = ql_get_curr_sbuf(rx_ring);\r\nskb = sbq_desc->p.skb;\r\nql_realign_skb(skb, length);\r\nskb_put(skb, length);\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(sbq_desc,\r\nmapaddr),\r\ndma_unmap_len(sbq_desc,\r\nmaplen),\r\nPCI_DMA_FROMDEVICE);\r\nsbq_desc->p.skb = NULL;\r\n}\r\n} else if (ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_DL) {\r\nif (ib_mac_rsp->flags4 & IB_MAC_IOCB_RSP_HS) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Header in small, %d bytes in large. Chain large to small!\n",\r\nlength);\r\nlbq_desc = ql_get_curr_lchunk(qdev, rx_ring);\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Chaining page at offset = %d, for %d bytes to skb.\n",\r\nlbq_desc->p.pg_chunk.offset, length);\r\nskb_fill_page_desc(skb, 0, lbq_desc->p.pg_chunk.page,\r\nlbq_desc->p.pg_chunk.offset,\r\nlength);\r\nskb->len += length;\r\nskb->data_len += length;\r\nskb->truesize += length;\r\n} else {\r\nlbq_desc = ql_get_curr_lchunk(qdev, rx_ring);\r\nskb = netdev_alloc_skb(qdev->ndev, length);\r\nif (skb == NULL) {\r\nnetif_printk(qdev, probe, KERN_DEBUG, qdev->ndev,\r\n"No skb available, drop the packet.\n");\r\nreturn NULL;\r\n}\r\npci_unmap_page(qdev->pdev,\r\ndma_unmap_addr(lbq_desc,\r\nmapaddr),\r\ndma_unmap_len(lbq_desc, maplen),\r\nPCI_DMA_FROMDEVICE);\r\nskb_reserve(skb, NET_IP_ALIGN);\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"%d bytes of headers and data in large. Chain page to new skb and pull tail.\n",\r\nlength);\r\nskb_fill_page_desc(skb, 0,\r\nlbq_desc->p.pg_chunk.page,\r\nlbq_desc->p.pg_chunk.offset,\r\nlength);\r\nskb->len += length;\r\nskb->data_len += length;\r\nskb->truesize += length;\r\nlength -= length;\r\nql_update_mac_hdr_len(qdev, ib_mac_rsp,\r\nlbq_desc->p.pg_chunk.va,\r\n&hlen);\r\n__pskb_pull_tail(skb, hlen);\r\n}\r\n} else {\r\nint size, i = 0;\r\nsbq_desc = ql_get_curr_sbuf(rx_ring);\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(sbq_desc, mapaddr),\r\ndma_unmap_len(sbq_desc, maplen),\r\nPCI_DMA_FROMDEVICE);\r\nif (!(ib_mac_rsp->flags4 & IB_MAC_IOCB_RSP_HS)) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"%d bytes of headers & data in chain of large.\n",\r\nlength);\r\nskb = sbq_desc->p.skb;\r\nsbq_desc->p.skb = NULL;\r\nskb_reserve(skb, NET_IP_ALIGN);\r\n}\r\ndo {\r\nlbq_desc = ql_get_curr_lchunk(qdev, rx_ring);\r\nsize = (length < rx_ring->lbq_buf_size) ? length :\r\nrx_ring->lbq_buf_size;\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Adding page %d to skb for %d bytes.\n",\r\ni, size);\r\nskb_fill_page_desc(skb, i,\r\nlbq_desc->p.pg_chunk.page,\r\nlbq_desc->p.pg_chunk.offset,\r\nsize);\r\nskb->len += size;\r\nskb->data_len += size;\r\nskb->truesize += size;\r\nlength -= size;\r\ni++;\r\n} while (length > 0);\r\nql_update_mac_hdr_len(qdev, ib_mac_rsp, lbq_desc->p.pg_chunk.va,\r\n&hlen);\r\n__pskb_pull_tail(skb, hlen);\r\n}\r\nreturn skb;\r\n}\r\nstatic void ql_process_mac_split_rx_intr(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp,\r\nu16 vlan_id)\r\n{\r\nstruct net_device *ndev = qdev->ndev;\r\nstruct sk_buff *skb = NULL;\r\nQL_DUMP_IB_MAC_RSP(ib_mac_rsp);\r\nskb = ql_build_rx_skb(qdev, rx_ring, ib_mac_rsp);\r\nif (unlikely(!skb)) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"No skb available, drop packet.\n");\r\nrx_ring->rx_dropped++;\r\nreturn;\r\n}\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_ERR_MASK) {\r\nql_categorize_rx_err(qdev, ib_mac_rsp->flags2, rx_ring);\r\ndev_kfree_skb_any(skb);\r\nreturn;\r\n}\r\nif (skb->len > ndev->mtu + ETH_HLEN) {\r\ndev_kfree_skb_any(skb);\r\nrx_ring->rx_dropped++;\r\nreturn;\r\n}\r\nif (test_bit(QL_SELFTEST, &qdev->flags)) {\r\nql_check_lb_frame(qdev, skb);\r\ndev_kfree_skb_any(skb);\r\nreturn;\r\n}\r\nprefetch(skb->data);\r\nif (ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev, "%s Multicast.\n",\r\n(ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) ==\r\nIB_MAC_IOCB_RSP_M_HASH ? "Hash" :\r\n(ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) ==\r\nIB_MAC_IOCB_RSP_M_REG ? "Registered" :\r\n(ib_mac_rsp->flags1 & IB_MAC_IOCB_RSP_M_MASK) ==\r\nIB_MAC_IOCB_RSP_M_PROM ? "Promiscuous" : "");\r\nrx_ring->rx_multicast++;\r\n}\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_P) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Promiscuous Packet.\n");\r\n}\r\nskb->protocol = eth_type_trans(skb, ndev);\r\nskb_checksum_none_assert(skb);\r\nif ((ndev->features & NETIF_F_RXCSUM) &&\r\n!(ib_mac_rsp->flags1 & IB_MAC_CSUM_ERR_MASK)) {\r\nif (ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_T) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"TCP checksum done!\n");\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n} else if ((ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_U) &&\r\n(ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_V4)) {\r\nstruct iphdr *iph = (struct iphdr *) skb->data;\r\nif (!(iph->frag_off &\r\nhtons(IP_MF|IP_OFFSET))) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"TCP checksum done!\n");\r\n}\r\n}\r\n}\r\nrx_ring->rx_packets++;\r\nrx_ring->rx_bytes += skb->len;\r\nskb_record_rx_queue(skb, rx_ring->cq_id);\r\nif (vlan_id != 0xffff)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_id);\r\nif (skb->ip_summed == CHECKSUM_UNNECESSARY)\r\nnapi_gro_receive(&rx_ring->napi, skb);\r\nelse\r\nnetif_receive_skb(skb);\r\n}\r\nstatic unsigned long ql_process_mac_rx_intr(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring,\r\nstruct ib_mac_iocb_rsp *ib_mac_rsp)\r\n{\r\nu32 length = le32_to_cpu(ib_mac_rsp->data_len);\r\nu16 vlan_id = ((ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_V) &&\r\n(qdev->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)) ?\r\n((le16_to_cpu(ib_mac_rsp->vlan_id) &\r\nIB_MAC_IOCB_RSP_VLAN_MASK)) : 0xffff;\r\nQL_DUMP_IB_MAC_RSP(ib_mac_rsp);\r\nif (ib_mac_rsp->flags4 & IB_MAC_IOCB_RSP_HV) {\r\nql_process_mac_split_rx_intr(qdev, rx_ring, ib_mac_rsp,\r\nvlan_id);\r\n} else if (ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_DS) {\r\nql_process_mac_rx_skb(qdev, rx_ring, ib_mac_rsp,\r\nlength, vlan_id);\r\n} else if ((ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_DL) &&\r\n!(ib_mac_rsp->flags1 & IB_MAC_CSUM_ERR_MASK) &&\r\n(ib_mac_rsp->flags2 & IB_MAC_IOCB_RSP_T)) {\r\nql_process_mac_rx_gro_page(qdev, rx_ring, ib_mac_rsp,\r\nlength, vlan_id);\r\n} else if (ib_mac_rsp->flags3 & IB_MAC_IOCB_RSP_DL) {\r\nql_process_mac_rx_page(qdev, rx_ring, ib_mac_rsp,\r\nlength, vlan_id);\r\n} else {\r\nql_process_mac_split_rx_intr(qdev, rx_ring, ib_mac_rsp,\r\nvlan_id);\r\n}\r\nreturn (unsigned long)length;\r\n}\r\nstatic void ql_process_mac_tx_intr(struct ql_adapter *qdev,\r\nstruct ob_mac_iocb_rsp *mac_rsp)\r\n{\r\nstruct tx_ring *tx_ring;\r\nstruct tx_ring_desc *tx_ring_desc;\r\nQL_DUMP_OB_MAC_RSP(mac_rsp);\r\ntx_ring = &qdev->tx_ring[mac_rsp->txq_idx];\r\ntx_ring_desc = &tx_ring->q[mac_rsp->tid];\r\nql_unmap_send(qdev, tx_ring_desc, tx_ring_desc->map_cnt);\r\ntx_ring->tx_bytes += (tx_ring_desc->skb)->len;\r\ntx_ring->tx_packets++;\r\ndev_kfree_skb(tx_ring_desc->skb);\r\ntx_ring_desc->skb = NULL;\r\nif (unlikely(mac_rsp->flags1 & (OB_MAC_IOCB_RSP_E |\r\nOB_MAC_IOCB_RSP_S |\r\nOB_MAC_IOCB_RSP_L |\r\nOB_MAC_IOCB_RSP_P | OB_MAC_IOCB_RSP_B))) {\r\nif (mac_rsp->flags1 & OB_MAC_IOCB_RSP_E) {\r\nnetif_warn(qdev, tx_done, qdev->ndev,\r\n"Total descriptor length did not match transfer length.\n");\r\n}\r\nif (mac_rsp->flags1 & OB_MAC_IOCB_RSP_S) {\r\nnetif_warn(qdev, tx_done, qdev->ndev,\r\n"Frame too short to be valid, not sent.\n");\r\n}\r\nif (mac_rsp->flags1 & OB_MAC_IOCB_RSP_L) {\r\nnetif_warn(qdev, tx_done, qdev->ndev,\r\n"Frame too long, but sent anyway.\n");\r\n}\r\nif (mac_rsp->flags1 & OB_MAC_IOCB_RSP_B) {\r\nnetif_warn(qdev, tx_done, qdev->ndev,\r\n"PCI backplane error. Frame not sent.\n");\r\n}\r\n}\r\natomic_inc(&tx_ring->tx_count);\r\n}\r\nvoid ql_queue_fw_error(struct ql_adapter *qdev)\r\n{\r\nql_link_off(qdev);\r\nqueue_delayed_work(qdev->workqueue, &qdev->mpi_reset_work, 0);\r\n}\r\nvoid ql_queue_asic_error(struct ql_adapter *qdev)\r\n{\r\nql_link_off(qdev);\r\nql_disable_interrupts(qdev);\r\nclear_bit(QL_ADAPTER_UP, &qdev->flags);\r\nset_bit(QL_ASIC_RECOVERY, &qdev->flags);\r\nqueue_delayed_work(qdev->workqueue, &qdev->asic_reset_work, 0);\r\n}\r\nstatic void ql_process_chip_ae_intr(struct ql_adapter *qdev,\r\nstruct ib_ae_iocb_rsp *ib_ae_rsp)\r\n{\r\nswitch (ib_ae_rsp->event) {\r\ncase MGMT_ERR_EVENT:\r\nnetif_err(qdev, rx_err, qdev->ndev,\r\n"Management Processor Fatal Error.\n");\r\nql_queue_fw_error(qdev);\r\nreturn;\r\ncase CAM_LOOKUP_ERR_EVENT:\r\nnetdev_err(qdev->ndev, "Multiple CAM hits lookup occurred.\n");\r\nnetdev_err(qdev->ndev, "This event shouldn't occur.\n");\r\nql_queue_asic_error(qdev);\r\nreturn;\r\ncase SOFT_ECC_ERROR_EVENT:\r\nnetdev_err(qdev->ndev, "Soft ECC error detected.\n");\r\nql_queue_asic_error(qdev);\r\nbreak;\r\ncase PCI_ERR_ANON_BUF_RD:\r\nnetdev_err(qdev->ndev, "PCI error occurred when reading "\r\n"anonymous buffers from rx_ring %d.\n",\r\nib_ae_rsp->q_id);\r\nql_queue_asic_error(qdev);\r\nbreak;\r\ndefault:\r\nnetif_err(qdev, drv, qdev->ndev, "Unexpected event %d.\n",\r\nib_ae_rsp->event);\r\nql_queue_asic_error(qdev);\r\nbreak;\r\n}\r\n}\r\nstatic int ql_clean_outbound_rx_ring(struct rx_ring *rx_ring)\r\n{\r\nstruct ql_adapter *qdev = rx_ring->qdev;\r\nu32 prod = ql_read_sh_reg(rx_ring->prod_idx_sh_reg);\r\nstruct ob_mac_iocb_rsp *net_rsp = NULL;\r\nint count = 0;\r\nstruct tx_ring *tx_ring;\r\nwhile (prod != rx_ring->cnsmr_idx) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"cq_id = %d, prod = %d, cnsmr = %d.\n.",\r\nrx_ring->cq_id, prod, rx_ring->cnsmr_idx);\r\nnet_rsp = (struct ob_mac_iocb_rsp *)rx_ring->curr_entry;\r\nrmb();\r\nswitch (net_rsp->opcode) {\r\ncase OPCODE_OB_MAC_TSO_IOCB:\r\ncase OPCODE_OB_MAC_IOCB:\r\nql_process_mac_tx_intr(qdev, net_rsp);\r\nbreak;\r\ndefault:\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Hit default case, not handled! dropping the packet, opcode = %x.\n",\r\nnet_rsp->opcode);\r\n}\r\ncount++;\r\nql_update_cq(rx_ring);\r\nprod = ql_read_sh_reg(rx_ring->prod_idx_sh_reg);\r\n}\r\nif (!net_rsp)\r\nreturn 0;\r\nql_write_cq_idx(rx_ring);\r\ntx_ring = &qdev->tx_ring[net_rsp->txq_idx];\r\nif (__netif_subqueue_stopped(qdev->ndev, tx_ring->wq_id)) {\r\nif ((atomic_read(&tx_ring->tx_count) > (tx_ring->wq_len / 4)))\r\nnetif_wake_subqueue(qdev->ndev, tx_ring->wq_id);\r\n}\r\nreturn count;\r\n}\r\nstatic int ql_clean_inbound_rx_ring(struct rx_ring *rx_ring, int budget)\r\n{\r\nstruct ql_adapter *qdev = rx_ring->qdev;\r\nu32 prod = ql_read_sh_reg(rx_ring->prod_idx_sh_reg);\r\nstruct ql_net_rsp_iocb *net_rsp;\r\nint count = 0;\r\nwhile (prod != rx_ring->cnsmr_idx) {\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"cq_id = %d, prod = %d, cnsmr = %d.\n.",\r\nrx_ring->cq_id, prod, rx_ring->cnsmr_idx);\r\nnet_rsp = rx_ring->curr_entry;\r\nrmb();\r\nswitch (net_rsp->opcode) {\r\ncase OPCODE_IB_MAC_IOCB:\r\nql_process_mac_rx_intr(qdev, rx_ring,\r\n(struct ib_mac_iocb_rsp *)\r\nnet_rsp);\r\nbreak;\r\ncase OPCODE_IB_AE_IOCB:\r\nql_process_chip_ae_intr(qdev, (struct ib_ae_iocb_rsp *)\r\nnet_rsp);\r\nbreak;\r\ndefault:\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Hit default case, not handled! dropping the packet, opcode = %x.\n",\r\nnet_rsp->opcode);\r\nbreak;\r\n}\r\ncount++;\r\nql_update_cq(rx_ring);\r\nprod = ql_read_sh_reg(rx_ring->prod_idx_sh_reg);\r\nif (count == budget)\r\nbreak;\r\n}\r\nql_update_buffer_queues(qdev, rx_ring);\r\nql_write_cq_idx(rx_ring);\r\nreturn count;\r\n}\r\nstatic int ql_napi_poll_msix(struct napi_struct *napi, int budget)\r\n{\r\nstruct rx_ring *rx_ring = container_of(napi, struct rx_ring, napi);\r\nstruct ql_adapter *qdev = rx_ring->qdev;\r\nstruct rx_ring *trx_ring;\r\nint i, work_done = 0;\r\nstruct intr_context *ctx = &qdev->intr_context[rx_ring->cq_id];\r\nnetif_printk(qdev, rx_status, KERN_DEBUG, qdev->ndev,\r\n"Enter, NAPI POLL cq_id = %d.\n", rx_ring->cq_id);\r\nfor (i = qdev->rss_ring_count; i < qdev->rx_ring_count; i++) {\r\ntrx_ring = &qdev->rx_ring[i];\r\nif ((ctx->irq_mask & (1 << trx_ring->cq_id)) &&\r\n(ql_read_sh_reg(trx_ring->prod_idx_sh_reg) !=\r\ntrx_ring->cnsmr_idx)) {\r\nnetif_printk(qdev, intr, KERN_DEBUG, qdev->ndev,\r\n"%s: Servicing TX completion ring %d.\n",\r\n__func__, trx_ring->cq_id);\r\nql_clean_outbound_rx_ring(trx_ring);\r\n}\r\n}\r\nif (ql_read_sh_reg(rx_ring->prod_idx_sh_reg) !=\r\nrx_ring->cnsmr_idx) {\r\nnetif_printk(qdev, intr, KERN_DEBUG, qdev->ndev,\r\n"%s: Servicing RX completion ring %d.\n",\r\n__func__, rx_ring->cq_id);\r\nwork_done = ql_clean_inbound_rx_ring(rx_ring, budget);\r\n}\r\nif (work_done < budget) {\r\nnapi_complete(napi);\r\nql_enable_completion_interrupt(qdev, rx_ring->irq);\r\n}\r\nreturn work_done;\r\n}\r\nstatic void qlge_vlan_mode(struct net_device *ndev, netdev_features_t features)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nif (features & NETIF_F_HW_VLAN_CTAG_RX) {\r\nql_write32(qdev, NIC_RCV_CFG, NIC_RCV_CFG_VLAN_MASK |\r\nNIC_RCV_CFG_VLAN_MATCH_AND_NON);\r\n} else {\r\nql_write32(qdev, NIC_RCV_CFG, NIC_RCV_CFG_VLAN_MASK);\r\n}\r\n}\r\nstatic int qlge_update_hw_vlan_features(struct net_device *ndev,\r\nnetdev_features_t features)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint status = 0;\r\nbool need_restart = netif_running(ndev);\r\nif (need_restart) {\r\nstatus = ql_adapter_down(qdev);\r\nif (status) {\r\nnetif_err(qdev, link, qdev->ndev,\r\n"Failed to bring down the adapter\n");\r\nreturn status;\r\n}\r\n}\r\nndev->features = features;\r\nif (need_restart) {\r\nstatus = ql_adapter_up(qdev);\r\nif (status) {\r\nnetif_err(qdev, link, qdev->ndev,\r\n"Failed to bring up the adapter\n");\r\nreturn status;\r\n}\r\n}\r\nreturn status;\r\n}\r\nstatic netdev_features_t qlge_fix_features(struct net_device *ndev,\r\nnetdev_features_t features)\r\n{\r\nint err;\r\nerr = qlge_update_hw_vlan_features(ndev, features);\r\nif (err)\r\nreturn err;\r\nreturn features;\r\n}\r\nstatic int qlge_set_features(struct net_device *ndev,\r\nnetdev_features_t features)\r\n{\r\nnetdev_features_t changed = ndev->features ^ features;\r\nif (changed & NETIF_F_HW_VLAN_CTAG_RX)\r\nqlge_vlan_mode(ndev, features);\r\nreturn 0;\r\n}\r\nstatic int __qlge_vlan_rx_add_vid(struct ql_adapter *qdev, u16 vid)\r\n{\r\nu32 enable_bit = MAC_ADDR_E;\r\nint err;\r\nerr = ql_set_mac_addr_reg(qdev, (u8 *) &enable_bit,\r\nMAC_ADDR_TYPE_VLAN, vid);\r\nif (err)\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init vlan address.\n");\r\nreturn err;\r\n}\r\nstatic int qlge_vlan_rx_add_vid(struct net_device *ndev, __be16 proto, u16 vid)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint status;\r\nint err;\r\nstatus = ql_sem_spinlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (status)\r\nreturn status;\r\nerr = __qlge_vlan_rx_add_vid(qdev, vid);\r\nset_bit(vid, qdev->active_vlans);\r\nql_sem_unlock(qdev, SEM_MAC_ADDR_MASK);\r\nreturn err;\r\n}\r\nstatic int __qlge_vlan_rx_kill_vid(struct ql_adapter *qdev, u16 vid)\r\n{\r\nu32 enable_bit = 0;\r\nint err;\r\nerr = ql_set_mac_addr_reg(qdev, (u8 *) &enable_bit,\r\nMAC_ADDR_TYPE_VLAN, vid);\r\nif (err)\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to clear vlan address.\n");\r\nreturn err;\r\n}\r\nstatic int qlge_vlan_rx_kill_vid(struct net_device *ndev, __be16 proto, u16 vid)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint status;\r\nint err;\r\nstatus = ql_sem_spinlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (status)\r\nreturn status;\r\nerr = __qlge_vlan_rx_kill_vid(qdev, vid);\r\nclear_bit(vid, qdev->active_vlans);\r\nql_sem_unlock(qdev, SEM_MAC_ADDR_MASK);\r\nreturn err;\r\n}\r\nstatic void qlge_restore_vlan(struct ql_adapter *qdev)\r\n{\r\nint status;\r\nu16 vid;\r\nstatus = ql_sem_spinlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (status)\r\nreturn;\r\nfor_each_set_bit(vid, qdev->active_vlans, VLAN_N_VID)\r\n__qlge_vlan_rx_add_vid(qdev, vid);\r\nql_sem_unlock(qdev, SEM_MAC_ADDR_MASK);\r\n}\r\nstatic irqreturn_t qlge_msix_rx_isr(int irq, void *dev_id)\r\n{\r\nstruct rx_ring *rx_ring = dev_id;\r\nnapi_schedule(&rx_ring->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t qlge_isr(int irq, void *dev_id)\r\n{\r\nstruct rx_ring *rx_ring = dev_id;\r\nstruct ql_adapter *qdev = rx_ring->qdev;\r\nstruct intr_context *intr_context = &qdev->intr_context[0];\r\nu32 var;\r\nint work_done = 0;\r\nspin_lock(&qdev->hw_lock);\r\nif (atomic_read(&qdev->intr_context[0].irq_cnt)) {\r\nnetif_printk(qdev, intr, KERN_DEBUG, qdev->ndev,\r\n"Shared Interrupt, Not ours!\n");\r\nspin_unlock(&qdev->hw_lock);\r\nreturn IRQ_NONE;\r\n}\r\nspin_unlock(&qdev->hw_lock);\r\nvar = ql_disable_completion_interrupt(qdev, intr_context->intr);\r\nif (var & STS_FE) {\r\nql_queue_asic_error(qdev);\r\nnetdev_err(qdev->ndev, "Got fatal error, STS = %x.\n", var);\r\nvar = ql_read32(qdev, ERR_STS);\r\nnetdev_err(qdev->ndev, "Resetting chip. "\r\n"Error Status Register = 0x%x\n", var);\r\nreturn IRQ_HANDLED;\r\n}\r\nif ((var & STS_PI) &&\r\n(ql_read32(qdev, INTR_MASK) & INTR_MASK_PI)) {\r\nnetif_err(qdev, intr, qdev->ndev,\r\n"Got MPI processor interrupt.\n");\r\nql_disable_completion_interrupt(qdev, intr_context->intr);\r\nql_write32(qdev, INTR_MASK, (INTR_MASK_PI << 16));\r\nqueue_delayed_work_on(smp_processor_id(),\r\nqdev->workqueue, &qdev->mpi_work, 0);\r\nwork_done++;\r\n}\r\nvar = ql_read32(qdev, ISR1);\r\nif (var & intr_context->irq_mask) {\r\nnetif_info(qdev, intr, qdev->ndev,\r\n"Waking handler for rx_ring[0].\n");\r\nql_disable_completion_interrupt(qdev, intr_context->intr);\r\nnapi_schedule(&rx_ring->napi);\r\nwork_done++;\r\n}\r\nql_enable_completion_interrupt(qdev, intr_context->intr);\r\nreturn work_done ? IRQ_HANDLED : IRQ_NONE;\r\n}\r\nstatic int ql_tso(struct sk_buff *skb, struct ob_mac_tso_iocb_req *mac_iocb_ptr)\r\n{\r\nif (skb_is_gso(skb)) {\r\nint err;\r\n__be16 l3_proto = vlan_get_protocol(skb);\r\nerr = skb_cow_head(skb, 0);\r\nif (err < 0)\r\nreturn err;\r\nmac_iocb_ptr->opcode = OPCODE_OB_MAC_TSO_IOCB;\r\nmac_iocb_ptr->flags3 |= OB_MAC_TSO_IOCB_IC;\r\nmac_iocb_ptr->frame_len = cpu_to_le32((u32) skb->len);\r\nmac_iocb_ptr->total_hdrs_len =\r\ncpu_to_le16(skb_transport_offset(skb) + tcp_hdrlen(skb));\r\nmac_iocb_ptr->net_trans_offset =\r\ncpu_to_le16(skb_network_offset(skb) |\r\nskb_transport_offset(skb)\r\n<< OB_MAC_TRANSPORT_HDR_SHIFT);\r\nmac_iocb_ptr->mss = cpu_to_le16(skb_shinfo(skb)->gso_size);\r\nmac_iocb_ptr->flags2 |= OB_MAC_TSO_IOCB_LSO;\r\nif (likely(l3_proto == htons(ETH_P_IP))) {\r\nstruct iphdr *iph = ip_hdr(skb);\r\niph->check = 0;\r\nmac_iocb_ptr->flags1 |= OB_MAC_TSO_IOCB_IP4;\r\ntcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr,\r\niph->daddr, 0,\r\nIPPROTO_TCP,\r\n0);\r\n} else if (l3_proto == htons(ETH_P_IPV6)) {\r\nmac_iocb_ptr->flags1 |= OB_MAC_TSO_IOCB_IP6;\r\ntcp_hdr(skb)->check =\r\n~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\r\n&ipv6_hdr(skb)->daddr,\r\n0, IPPROTO_TCP, 0);\r\n}\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ql_hw_csum_setup(struct sk_buff *skb,\r\nstruct ob_mac_tso_iocb_req *mac_iocb_ptr)\r\n{\r\nint len;\r\nstruct iphdr *iph = ip_hdr(skb);\r\n__sum16 *check;\r\nmac_iocb_ptr->opcode = OPCODE_OB_MAC_TSO_IOCB;\r\nmac_iocb_ptr->frame_len = cpu_to_le32((u32) skb->len);\r\nmac_iocb_ptr->net_trans_offset =\r\ncpu_to_le16(skb_network_offset(skb) |\r\nskb_transport_offset(skb) << OB_MAC_TRANSPORT_HDR_SHIFT);\r\nmac_iocb_ptr->flags1 |= OB_MAC_TSO_IOCB_IP4;\r\nlen = (ntohs(iph->tot_len) - (iph->ihl << 2));\r\nif (likely(iph->protocol == IPPROTO_TCP)) {\r\ncheck = &(tcp_hdr(skb)->check);\r\nmac_iocb_ptr->flags2 |= OB_MAC_TSO_IOCB_TC;\r\nmac_iocb_ptr->total_hdrs_len =\r\ncpu_to_le16(skb_transport_offset(skb) +\r\n(tcp_hdr(skb)->doff << 2));\r\n} else {\r\ncheck = &(udp_hdr(skb)->check);\r\nmac_iocb_ptr->flags2 |= OB_MAC_TSO_IOCB_UC;\r\nmac_iocb_ptr->total_hdrs_len =\r\ncpu_to_le16(skb_transport_offset(skb) +\r\nsizeof(struct udphdr));\r\n}\r\n*check = ~csum_tcpudp_magic(iph->saddr,\r\niph->daddr, len, iph->protocol, 0);\r\n}\r\nstatic netdev_tx_t qlge_send(struct sk_buff *skb, struct net_device *ndev)\r\n{\r\nstruct tx_ring_desc *tx_ring_desc;\r\nstruct ob_mac_iocb_req *mac_iocb_ptr;\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint tso;\r\nstruct tx_ring *tx_ring;\r\nu32 tx_ring_idx = (u32) skb->queue_mapping;\r\ntx_ring = &qdev->tx_ring[tx_ring_idx];\r\nif (skb_padto(skb, ETH_ZLEN))\r\nreturn NETDEV_TX_OK;\r\nif (unlikely(atomic_read(&tx_ring->tx_count) < 2)) {\r\nnetif_info(qdev, tx_queued, qdev->ndev,\r\n"%s: BUG! shutting down tx queue %d due to lack of resources.\n",\r\n__func__, tx_ring_idx);\r\nnetif_stop_subqueue(ndev, tx_ring->wq_id);\r\ntx_ring->tx_errors++;\r\nreturn NETDEV_TX_BUSY;\r\n}\r\ntx_ring_desc = &tx_ring->q[tx_ring->prod_idx];\r\nmac_iocb_ptr = tx_ring_desc->queue_entry;\r\nmemset((void *)mac_iocb_ptr, 0, sizeof(*mac_iocb_ptr));\r\nmac_iocb_ptr->opcode = OPCODE_OB_MAC_IOCB;\r\nmac_iocb_ptr->tid = tx_ring_desc->index;\r\nmac_iocb_ptr->txq_idx = tx_ring_idx;\r\ntx_ring_desc->skb = skb;\r\nmac_iocb_ptr->frame_len = cpu_to_le16((u16) skb->len);\r\nif (skb_vlan_tag_present(skb)) {\r\nnetif_printk(qdev, tx_queued, KERN_DEBUG, qdev->ndev,\r\n"Adding a vlan tag %d.\n", skb_vlan_tag_get(skb));\r\nmac_iocb_ptr->flags3 |= OB_MAC_IOCB_V;\r\nmac_iocb_ptr->vlan_tci = cpu_to_le16(skb_vlan_tag_get(skb));\r\n}\r\ntso = ql_tso(skb, (struct ob_mac_tso_iocb_req *)mac_iocb_ptr);\r\nif (tso < 0) {\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n} else if (unlikely(!tso) && (skb->ip_summed == CHECKSUM_PARTIAL)) {\r\nql_hw_csum_setup(skb,\r\n(struct ob_mac_tso_iocb_req *)mac_iocb_ptr);\r\n}\r\nif (ql_map_send(qdev, mac_iocb_ptr, skb, tx_ring_desc) !=\r\nNETDEV_TX_OK) {\r\nnetif_err(qdev, tx_queued, qdev->ndev,\r\n"Could not map the segments.\n");\r\ntx_ring->tx_errors++;\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nQL_DUMP_OB_MAC_IOCB(mac_iocb_ptr);\r\ntx_ring->prod_idx++;\r\nif (tx_ring->prod_idx == tx_ring->wq_len)\r\ntx_ring->prod_idx = 0;\r\nwmb();\r\nql_write_db_reg(tx_ring->prod_idx, tx_ring->prod_idx_db_reg);\r\nnetif_printk(qdev, tx_queued, KERN_DEBUG, qdev->ndev,\r\n"tx queued, slot %d, len %d\n",\r\ntx_ring->prod_idx, skb->len);\r\natomic_dec(&tx_ring->tx_count);\r\nif (unlikely(atomic_read(&tx_ring->tx_count) < 2)) {\r\nnetif_stop_subqueue(ndev, tx_ring->wq_id);\r\nif ((atomic_read(&tx_ring->tx_count) > (tx_ring->wq_len / 4)))\r\nnetif_wake_subqueue(qdev->ndev, tx_ring->wq_id);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void ql_free_shadow_space(struct ql_adapter *qdev)\r\n{\r\nif (qdev->rx_ring_shadow_reg_area) {\r\npci_free_consistent(qdev->pdev,\r\nPAGE_SIZE,\r\nqdev->rx_ring_shadow_reg_area,\r\nqdev->rx_ring_shadow_reg_dma);\r\nqdev->rx_ring_shadow_reg_area = NULL;\r\n}\r\nif (qdev->tx_ring_shadow_reg_area) {\r\npci_free_consistent(qdev->pdev,\r\nPAGE_SIZE,\r\nqdev->tx_ring_shadow_reg_area,\r\nqdev->tx_ring_shadow_reg_dma);\r\nqdev->tx_ring_shadow_reg_area = NULL;\r\n}\r\n}\r\nstatic int ql_alloc_shadow_space(struct ql_adapter *qdev)\r\n{\r\nqdev->rx_ring_shadow_reg_area =\r\npci_zalloc_consistent(qdev->pdev, PAGE_SIZE,\r\n&qdev->rx_ring_shadow_reg_dma);\r\nif (qdev->rx_ring_shadow_reg_area == NULL) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Allocation of RX shadow space failed.\n");\r\nreturn -ENOMEM;\r\n}\r\nqdev->tx_ring_shadow_reg_area =\r\npci_zalloc_consistent(qdev->pdev, PAGE_SIZE,\r\n&qdev->tx_ring_shadow_reg_dma);\r\nif (qdev->tx_ring_shadow_reg_area == NULL) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Allocation of TX shadow space failed.\n");\r\ngoto err_wqp_sh_area;\r\n}\r\nreturn 0;\r\nerr_wqp_sh_area:\r\npci_free_consistent(qdev->pdev,\r\nPAGE_SIZE,\r\nqdev->rx_ring_shadow_reg_area,\r\nqdev->rx_ring_shadow_reg_dma);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ql_init_tx_ring(struct ql_adapter *qdev, struct tx_ring *tx_ring)\r\n{\r\nstruct tx_ring_desc *tx_ring_desc;\r\nint i;\r\nstruct ob_mac_iocb_req *mac_iocb_ptr;\r\nmac_iocb_ptr = tx_ring->wq_base;\r\ntx_ring_desc = tx_ring->q;\r\nfor (i = 0; i < tx_ring->wq_len; i++) {\r\ntx_ring_desc->index = i;\r\ntx_ring_desc->skb = NULL;\r\ntx_ring_desc->queue_entry = mac_iocb_ptr;\r\nmac_iocb_ptr++;\r\ntx_ring_desc++;\r\n}\r\natomic_set(&tx_ring->tx_count, tx_ring->wq_len);\r\n}\r\nstatic void ql_free_tx_resources(struct ql_adapter *qdev,\r\nstruct tx_ring *tx_ring)\r\n{\r\nif (tx_ring->wq_base) {\r\npci_free_consistent(qdev->pdev, tx_ring->wq_size,\r\ntx_ring->wq_base, tx_ring->wq_base_dma);\r\ntx_ring->wq_base = NULL;\r\n}\r\nkfree(tx_ring->q);\r\ntx_ring->q = NULL;\r\n}\r\nstatic int ql_alloc_tx_resources(struct ql_adapter *qdev,\r\nstruct tx_ring *tx_ring)\r\n{\r\ntx_ring->wq_base =\r\npci_alloc_consistent(qdev->pdev, tx_ring->wq_size,\r\n&tx_ring->wq_base_dma);\r\nif ((tx_ring->wq_base == NULL) ||\r\ntx_ring->wq_base_dma & WQ_ADDR_ALIGN)\r\ngoto pci_alloc_err;\r\ntx_ring->q =\r\nkmalloc(tx_ring->wq_len * sizeof(struct tx_ring_desc), GFP_KERNEL);\r\nif (tx_ring->q == NULL)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\npci_free_consistent(qdev->pdev, tx_ring->wq_size,\r\ntx_ring->wq_base, tx_ring->wq_base_dma);\r\ntx_ring->wq_base = NULL;\r\npci_alloc_err:\r\nnetif_err(qdev, ifup, qdev->ndev, "tx_ring alloc failed.\n");\r\nreturn -ENOMEM;\r\n}\r\nstatic void ql_free_lbq_buffers(struct ql_adapter *qdev, struct rx_ring *rx_ring)\r\n{\r\nstruct bq_desc *lbq_desc;\r\nuint32_t curr_idx, clean_idx;\r\ncurr_idx = rx_ring->lbq_curr_idx;\r\nclean_idx = rx_ring->lbq_clean_idx;\r\nwhile (curr_idx != clean_idx) {\r\nlbq_desc = &rx_ring->lbq[curr_idx];\r\nif (lbq_desc->p.pg_chunk.last_flag) {\r\npci_unmap_page(qdev->pdev,\r\nlbq_desc->p.pg_chunk.map,\r\nql_lbq_block_size(qdev),\r\nPCI_DMA_FROMDEVICE);\r\nlbq_desc->p.pg_chunk.last_flag = 0;\r\n}\r\nput_page(lbq_desc->p.pg_chunk.page);\r\nlbq_desc->p.pg_chunk.page = NULL;\r\nif (++curr_idx == rx_ring->lbq_len)\r\ncurr_idx = 0;\r\n}\r\nif (rx_ring->pg_chunk.page) {\r\npci_unmap_page(qdev->pdev, rx_ring->pg_chunk.map,\r\nql_lbq_block_size(qdev), PCI_DMA_FROMDEVICE);\r\nput_page(rx_ring->pg_chunk.page);\r\nrx_ring->pg_chunk.page = NULL;\r\n}\r\n}\r\nstatic void ql_free_sbq_buffers(struct ql_adapter *qdev, struct rx_ring *rx_ring)\r\n{\r\nint i;\r\nstruct bq_desc *sbq_desc;\r\nfor (i = 0; i < rx_ring->sbq_len; i++) {\r\nsbq_desc = &rx_ring->sbq[i];\r\nif (sbq_desc == NULL) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"sbq_desc %d is NULL.\n", i);\r\nreturn;\r\n}\r\nif (sbq_desc->p.skb) {\r\npci_unmap_single(qdev->pdev,\r\ndma_unmap_addr(sbq_desc, mapaddr),\r\ndma_unmap_len(sbq_desc, maplen),\r\nPCI_DMA_FROMDEVICE);\r\ndev_kfree_skb(sbq_desc->p.skb);\r\nsbq_desc->p.skb = NULL;\r\n}\r\n}\r\n}\r\nstatic void ql_free_rx_buffers(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nstruct rx_ring *rx_ring;\r\nfor (i = 0; i < qdev->rx_ring_count; i++) {\r\nrx_ring = &qdev->rx_ring[i];\r\nif (rx_ring->lbq)\r\nql_free_lbq_buffers(qdev, rx_ring);\r\nif (rx_ring->sbq)\r\nql_free_sbq_buffers(qdev, rx_ring);\r\n}\r\n}\r\nstatic void ql_alloc_rx_buffers(struct ql_adapter *qdev)\r\n{\r\nstruct rx_ring *rx_ring;\r\nint i;\r\nfor (i = 0; i < qdev->rx_ring_count; i++) {\r\nrx_ring = &qdev->rx_ring[i];\r\nif (rx_ring->type != TX_Q)\r\nql_update_buffer_queues(qdev, rx_ring);\r\n}\r\n}\r\nstatic void ql_init_lbq_ring(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring)\r\n{\r\nint i;\r\nstruct bq_desc *lbq_desc;\r\n__le64 *bq = rx_ring->lbq_base;\r\nmemset(rx_ring->lbq, 0, rx_ring->lbq_len * sizeof(struct bq_desc));\r\nfor (i = 0; i < rx_ring->lbq_len; i++) {\r\nlbq_desc = &rx_ring->lbq[i];\r\nmemset(lbq_desc, 0, sizeof(*lbq_desc));\r\nlbq_desc->index = i;\r\nlbq_desc->addr = bq;\r\nbq++;\r\n}\r\n}\r\nstatic void ql_init_sbq_ring(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring)\r\n{\r\nint i;\r\nstruct bq_desc *sbq_desc;\r\n__le64 *bq = rx_ring->sbq_base;\r\nmemset(rx_ring->sbq, 0, rx_ring->sbq_len * sizeof(struct bq_desc));\r\nfor (i = 0; i < rx_ring->sbq_len; i++) {\r\nsbq_desc = &rx_ring->sbq[i];\r\nmemset(sbq_desc, 0, sizeof(*sbq_desc));\r\nsbq_desc->index = i;\r\nsbq_desc->addr = bq;\r\nbq++;\r\n}\r\n}\r\nstatic void ql_free_rx_resources(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring)\r\n{\r\nif (rx_ring->sbq_base) {\r\npci_free_consistent(qdev->pdev,\r\nrx_ring->sbq_size,\r\nrx_ring->sbq_base, rx_ring->sbq_base_dma);\r\nrx_ring->sbq_base = NULL;\r\n}\r\nkfree(rx_ring->sbq);\r\nrx_ring->sbq = NULL;\r\nif (rx_ring->lbq_base) {\r\npci_free_consistent(qdev->pdev,\r\nrx_ring->lbq_size,\r\nrx_ring->lbq_base, rx_ring->lbq_base_dma);\r\nrx_ring->lbq_base = NULL;\r\n}\r\nkfree(rx_ring->lbq);\r\nrx_ring->lbq = NULL;\r\nif (rx_ring->cq_base) {\r\npci_free_consistent(qdev->pdev,\r\nrx_ring->cq_size,\r\nrx_ring->cq_base, rx_ring->cq_base_dma);\r\nrx_ring->cq_base = NULL;\r\n}\r\n}\r\nstatic int ql_alloc_rx_resources(struct ql_adapter *qdev,\r\nstruct rx_ring *rx_ring)\r\n{\r\nrx_ring->cq_base =\r\npci_alloc_consistent(qdev->pdev, rx_ring->cq_size,\r\n&rx_ring->cq_base_dma);\r\nif (rx_ring->cq_base == NULL) {\r\nnetif_err(qdev, ifup, qdev->ndev, "rx_ring alloc failed.\n");\r\nreturn -ENOMEM;\r\n}\r\nif (rx_ring->sbq_len) {\r\nrx_ring->sbq_base =\r\npci_alloc_consistent(qdev->pdev, rx_ring->sbq_size,\r\n&rx_ring->sbq_base_dma);\r\nif (rx_ring->sbq_base == NULL) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Small buffer queue allocation failed.\n");\r\ngoto err_mem;\r\n}\r\nrx_ring->sbq = kmalloc_array(rx_ring->sbq_len,\r\nsizeof(struct bq_desc),\r\nGFP_KERNEL);\r\nif (rx_ring->sbq == NULL)\r\ngoto err_mem;\r\nql_init_sbq_ring(qdev, rx_ring);\r\n}\r\nif (rx_ring->lbq_len) {\r\nrx_ring->lbq_base =\r\npci_alloc_consistent(qdev->pdev, rx_ring->lbq_size,\r\n&rx_ring->lbq_base_dma);\r\nif (rx_ring->lbq_base == NULL) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Large buffer queue allocation failed.\n");\r\ngoto err_mem;\r\n}\r\nrx_ring->lbq = kmalloc_array(rx_ring->lbq_len,\r\nsizeof(struct bq_desc),\r\nGFP_KERNEL);\r\nif (rx_ring->lbq == NULL)\r\ngoto err_mem;\r\nql_init_lbq_ring(qdev, rx_ring);\r\n}\r\nreturn 0;\r\nerr_mem:\r\nql_free_rx_resources(qdev, rx_ring);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ql_tx_ring_clean(struct ql_adapter *qdev)\r\n{\r\nstruct tx_ring *tx_ring;\r\nstruct tx_ring_desc *tx_ring_desc;\r\nint i, j;\r\nfor (j = 0; j < qdev->tx_ring_count; j++) {\r\ntx_ring = &qdev->tx_ring[j];\r\nfor (i = 0; i < tx_ring->wq_len; i++) {\r\ntx_ring_desc = &tx_ring->q[i];\r\nif (tx_ring_desc && tx_ring_desc->skb) {\r\nnetif_err(qdev, ifdown, qdev->ndev,\r\n"Freeing lost SKB %p, from queue %d, index %d.\n",\r\ntx_ring_desc->skb, j,\r\ntx_ring_desc->index);\r\nql_unmap_send(qdev, tx_ring_desc,\r\ntx_ring_desc->map_cnt);\r\ndev_kfree_skb(tx_ring_desc->skb);\r\ntx_ring_desc->skb = NULL;\r\n}\r\n}\r\n}\r\n}\r\nstatic void ql_free_mem_resources(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nfor (i = 0; i < qdev->tx_ring_count; i++)\r\nql_free_tx_resources(qdev, &qdev->tx_ring[i]);\r\nfor (i = 0; i < qdev->rx_ring_count; i++)\r\nql_free_rx_resources(qdev, &qdev->rx_ring[i]);\r\nql_free_shadow_space(qdev);\r\n}\r\nstatic int ql_alloc_mem_resources(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nif (ql_alloc_shadow_space(qdev))\r\nreturn -ENOMEM;\r\nfor (i = 0; i < qdev->rx_ring_count; i++) {\r\nif (ql_alloc_rx_resources(qdev, &qdev->rx_ring[i]) != 0) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"RX resource allocation failed.\n");\r\ngoto err_mem;\r\n}\r\n}\r\nfor (i = 0; i < qdev->tx_ring_count; i++) {\r\nif (ql_alloc_tx_resources(qdev, &qdev->tx_ring[i]) != 0) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"TX resource allocation failed.\n");\r\ngoto err_mem;\r\n}\r\n}\r\nreturn 0;\r\nerr_mem:\r\nql_free_mem_resources(qdev);\r\nreturn -ENOMEM;\r\n}\r\nstatic int ql_start_rx_ring(struct ql_adapter *qdev, struct rx_ring *rx_ring)\r\n{\r\nstruct cqicb *cqicb = &rx_ring->cqicb;\r\nvoid *shadow_reg = qdev->rx_ring_shadow_reg_area +\r\n(rx_ring->cq_id * RX_RING_SHADOW_SPACE);\r\nu64 shadow_reg_dma = qdev->rx_ring_shadow_reg_dma +\r\n(rx_ring->cq_id * RX_RING_SHADOW_SPACE);\r\nvoid __iomem *doorbell_area =\r\nqdev->doorbell_area + (DB_PAGE_SIZE * (128 + rx_ring->cq_id));\r\nint err = 0;\r\nu16 bq_len;\r\nu64 tmp;\r\n__le64 *base_indirect_ptr;\r\nint page_entries;\r\nrx_ring->prod_idx_sh_reg = shadow_reg;\r\nrx_ring->prod_idx_sh_reg_dma = shadow_reg_dma;\r\n*rx_ring->prod_idx_sh_reg = 0;\r\nshadow_reg += sizeof(u64);\r\nshadow_reg_dma += sizeof(u64);\r\nrx_ring->lbq_base_indirect = shadow_reg;\r\nrx_ring->lbq_base_indirect_dma = shadow_reg_dma;\r\nshadow_reg += (sizeof(u64) * MAX_DB_PAGES_PER_BQ(rx_ring->lbq_len));\r\nshadow_reg_dma += (sizeof(u64) * MAX_DB_PAGES_PER_BQ(rx_ring->lbq_len));\r\nrx_ring->sbq_base_indirect = shadow_reg;\r\nrx_ring->sbq_base_indirect_dma = shadow_reg_dma;\r\nrx_ring->cnsmr_idx_db_reg = (u32 __iomem *) doorbell_area;\r\nrx_ring->cnsmr_idx = 0;\r\nrx_ring->curr_entry = rx_ring->cq_base;\r\nrx_ring->valid_db_reg = doorbell_area + 0x04;\r\nrx_ring->lbq_prod_idx_db_reg = (u32 __iomem *) (doorbell_area + 0x18);\r\nrx_ring->sbq_prod_idx_db_reg = (u32 __iomem *) (doorbell_area + 0x1c);\r\nmemset((void *)cqicb, 0, sizeof(struct cqicb));\r\ncqicb->msix_vect = rx_ring->irq;\r\nbq_len = (rx_ring->cq_len == 65536) ? 0 : (u16) rx_ring->cq_len;\r\ncqicb->len = cpu_to_le16(bq_len | LEN_V | LEN_CPP_CONT);\r\ncqicb->addr = cpu_to_le64(rx_ring->cq_base_dma);\r\ncqicb->prod_idx_addr = cpu_to_le64(rx_ring->prod_idx_sh_reg_dma);\r\ncqicb->flags = FLAGS_LC |\r\nFLAGS_LV |\r\nFLAGS_LI;\r\nif (rx_ring->lbq_len) {\r\ncqicb->flags |= FLAGS_LL;\r\ntmp = (u64)rx_ring->lbq_base_dma;\r\nbase_indirect_ptr = rx_ring->lbq_base_indirect;\r\npage_entries = 0;\r\ndo {\r\n*base_indirect_ptr = cpu_to_le64(tmp);\r\ntmp += DB_PAGE_SIZE;\r\nbase_indirect_ptr++;\r\npage_entries++;\r\n} while (page_entries < MAX_DB_PAGES_PER_BQ(rx_ring->lbq_len));\r\ncqicb->lbq_addr =\r\ncpu_to_le64(rx_ring->lbq_base_indirect_dma);\r\nbq_len = (rx_ring->lbq_buf_size == 65536) ? 0 :\r\n(u16) rx_ring->lbq_buf_size;\r\ncqicb->lbq_buf_size = cpu_to_le16(bq_len);\r\nbq_len = (rx_ring->lbq_len == 65536) ? 0 :\r\n(u16) rx_ring->lbq_len;\r\ncqicb->lbq_len = cpu_to_le16(bq_len);\r\nrx_ring->lbq_prod_idx = 0;\r\nrx_ring->lbq_curr_idx = 0;\r\nrx_ring->lbq_clean_idx = 0;\r\nrx_ring->lbq_free_cnt = rx_ring->lbq_len;\r\n}\r\nif (rx_ring->sbq_len) {\r\ncqicb->flags |= FLAGS_LS;\r\ntmp = (u64)rx_ring->sbq_base_dma;\r\nbase_indirect_ptr = rx_ring->sbq_base_indirect;\r\npage_entries = 0;\r\ndo {\r\n*base_indirect_ptr = cpu_to_le64(tmp);\r\ntmp += DB_PAGE_SIZE;\r\nbase_indirect_ptr++;\r\npage_entries++;\r\n} while (page_entries < MAX_DB_PAGES_PER_BQ(rx_ring->sbq_len));\r\ncqicb->sbq_addr =\r\ncpu_to_le64(rx_ring->sbq_base_indirect_dma);\r\ncqicb->sbq_buf_size =\r\ncpu_to_le16((u16)(rx_ring->sbq_buf_size));\r\nbq_len = (rx_ring->sbq_len == 65536) ? 0 :\r\n(u16) rx_ring->sbq_len;\r\ncqicb->sbq_len = cpu_to_le16(bq_len);\r\nrx_ring->sbq_prod_idx = 0;\r\nrx_ring->sbq_curr_idx = 0;\r\nrx_ring->sbq_clean_idx = 0;\r\nrx_ring->sbq_free_cnt = rx_ring->sbq_len;\r\n}\r\nswitch (rx_ring->type) {\r\ncase TX_Q:\r\ncqicb->irq_delay = cpu_to_le16(qdev->tx_coalesce_usecs);\r\ncqicb->pkt_delay = cpu_to_le16(qdev->tx_max_coalesced_frames);\r\nbreak;\r\ncase RX_Q:\r\nnetif_napi_add(qdev->ndev, &rx_ring->napi, ql_napi_poll_msix,\r\n64);\r\ncqicb->irq_delay = cpu_to_le16(qdev->rx_coalesce_usecs);\r\ncqicb->pkt_delay = cpu_to_le16(qdev->rx_max_coalesced_frames);\r\nbreak;\r\ndefault:\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"Invalid rx_ring->type = %d.\n", rx_ring->type);\r\n}\r\nerr = ql_write_cfg(qdev, cqicb, sizeof(struct cqicb),\r\nCFG_LCQ, rx_ring->cq_id);\r\nif (err) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to load CQICB.\n");\r\nreturn err;\r\n}\r\nreturn err;\r\n}\r\nstatic int ql_start_tx_ring(struct ql_adapter *qdev, struct tx_ring *tx_ring)\r\n{\r\nstruct wqicb *wqicb = (struct wqicb *)tx_ring;\r\nvoid __iomem *doorbell_area =\r\nqdev->doorbell_area + (DB_PAGE_SIZE * tx_ring->wq_id);\r\nvoid *shadow_reg = qdev->tx_ring_shadow_reg_area +\r\n(tx_ring->wq_id * sizeof(u64));\r\nu64 shadow_reg_dma = qdev->tx_ring_shadow_reg_dma +\r\n(tx_ring->wq_id * sizeof(u64));\r\nint err = 0;\r\ntx_ring->prod_idx_db_reg = (u32 __iomem *) doorbell_area;\r\ntx_ring->prod_idx = 0;\r\ntx_ring->valid_db_reg = doorbell_area + 0x04;\r\ntx_ring->cnsmr_idx_sh_reg = shadow_reg;\r\ntx_ring->cnsmr_idx_sh_reg_dma = shadow_reg_dma;\r\nwqicb->len = cpu_to_le16(tx_ring->wq_len | Q_LEN_V | Q_LEN_CPP_CONT);\r\nwqicb->flags = cpu_to_le16(Q_FLAGS_LC |\r\nQ_FLAGS_LB | Q_FLAGS_LI | Q_FLAGS_LO);\r\nwqicb->cq_id_rss = cpu_to_le16(tx_ring->cq_id);\r\nwqicb->rid = 0;\r\nwqicb->addr = cpu_to_le64(tx_ring->wq_base_dma);\r\nwqicb->cnsmr_idx_addr = cpu_to_le64(tx_ring->cnsmr_idx_sh_reg_dma);\r\nql_init_tx_ring(qdev, tx_ring);\r\nerr = ql_write_cfg(qdev, wqicb, sizeof(*wqicb), CFG_LRQ,\r\n(u16) tx_ring->wq_id);\r\nif (err) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to load tx_ring.\n");\r\nreturn err;\r\n}\r\nreturn err;\r\n}\r\nstatic void ql_disable_msix(struct ql_adapter *qdev)\r\n{\r\nif (test_bit(QL_MSIX_ENABLED, &qdev->flags)) {\r\npci_disable_msix(qdev->pdev);\r\nclear_bit(QL_MSIX_ENABLED, &qdev->flags);\r\nkfree(qdev->msi_x_entry);\r\nqdev->msi_x_entry = NULL;\r\n} else if (test_bit(QL_MSI_ENABLED, &qdev->flags)) {\r\npci_disable_msi(qdev->pdev);\r\nclear_bit(QL_MSI_ENABLED, &qdev->flags);\r\n}\r\n}\r\nstatic void ql_enable_msix(struct ql_adapter *qdev)\r\n{\r\nint i, err;\r\nif (qlge_irq_type == MSIX_IRQ) {\r\nqdev->msi_x_entry = kcalloc(qdev->intr_count,\r\nsizeof(struct msix_entry),\r\nGFP_KERNEL);\r\nif (!qdev->msi_x_entry) {\r\nqlge_irq_type = MSI_IRQ;\r\ngoto msi;\r\n}\r\nfor (i = 0; i < qdev->intr_count; i++)\r\nqdev->msi_x_entry[i].entry = i;\r\nerr = pci_enable_msix_range(qdev->pdev, qdev->msi_x_entry,\r\n1, qdev->intr_count);\r\nif (err < 0) {\r\nkfree(qdev->msi_x_entry);\r\nqdev->msi_x_entry = NULL;\r\nnetif_warn(qdev, ifup, qdev->ndev,\r\n"MSI-X Enable failed, trying MSI.\n");\r\nqlge_irq_type = MSI_IRQ;\r\n} else {\r\nqdev->intr_count = err;\r\nset_bit(QL_MSIX_ENABLED, &qdev->flags);\r\nnetif_info(qdev, ifup, qdev->ndev,\r\n"MSI-X Enabled, got %d vectors.\n",\r\nqdev->intr_count);\r\nreturn;\r\n}\r\n}\r\nmsi:\r\nqdev->intr_count = 1;\r\nif (qlge_irq_type == MSI_IRQ) {\r\nif (!pci_enable_msi(qdev->pdev)) {\r\nset_bit(QL_MSI_ENABLED, &qdev->flags);\r\nnetif_info(qdev, ifup, qdev->ndev,\r\n"Running with MSI interrupts.\n");\r\nreturn;\r\n}\r\n}\r\nqlge_irq_type = LEG_IRQ;\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"Running with legacy interrupts.\n");\r\n}\r\nstatic void ql_set_tx_vect(struct ql_adapter *qdev)\r\n{\r\nint i, j, vect;\r\nu32 tx_rings_per_vector = qdev->tx_ring_count / qdev->intr_count;\r\nif (likely(test_bit(QL_MSIX_ENABLED, &qdev->flags))) {\r\nfor (vect = 0, j = 0, i = qdev->rss_ring_count;\r\ni < qdev->rx_ring_count; i++) {\r\nif (j == tx_rings_per_vector) {\r\nvect++;\r\nj = 0;\r\n}\r\nqdev->rx_ring[i].irq = vect;\r\nj++;\r\n}\r\n} else {\r\nfor (i = 0; i < qdev->rx_ring_count; i++)\r\nqdev->rx_ring[i].irq = 0;\r\n}\r\n}\r\nstatic void ql_set_irq_mask(struct ql_adapter *qdev, struct intr_context *ctx)\r\n{\r\nint j, vect = ctx->intr;\r\nu32 tx_rings_per_vector = qdev->tx_ring_count / qdev->intr_count;\r\nif (likely(test_bit(QL_MSIX_ENABLED, &qdev->flags))) {\r\nctx->irq_mask = (1 << qdev->rx_ring[vect].cq_id);\r\nfor (j = 0; j < tx_rings_per_vector; j++) {\r\nctx->irq_mask |=\r\n(1 << qdev->rx_ring[qdev->rss_ring_count +\r\n(vect * tx_rings_per_vector) + j].cq_id);\r\n}\r\n} else {\r\nfor (j = 0; j < qdev->rx_ring_count; j++)\r\nctx->irq_mask |= (1 << qdev->rx_ring[j].cq_id);\r\n}\r\n}\r\nstatic void ql_resolve_queues_to_irqs(struct ql_adapter *qdev)\r\n{\r\nint i = 0;\r\nstruct intr_context *intr_context = &qdev->intr_context[0];\r\nif (likely(test_bit(QL_MSIX_ENABLED, &qdev->flags))) {\r\nfor (i = 0; i < qdev->intr_count; i++, intr_context++) {\r\nqdev->rx_ring[i].irq = i;\r\nintr_context->intr = i;\r\nintr_context->qdev = qdev;\r\nql_set_irq_mask(qdev, intr_context);\r\nintr_context->intr_en_mask =\r\nINTR_EN_TYPE_MASK | INTR_EN_INTR_MASK |\r\nINTR_EN_TYPE_ENABLE | INTR_EN_IHD_MASK | INTR_EN_IHD\r\n| i;\r\nintr_context->intr_dis_mask =\r\nINTR_EN_TYPE_MASK | INTR_EN_INTR_MASK |\r\nINTR_EN_TYPE_DISABLE | INTR_EN_IHD_MASK |\r\nINTR_EN_IHD | i;\r\nintr_context->intr_read_mask =\r\nINTR_EN_TYPE_MASK | INTR_EN_INTR_MASK |\r\nINTR_EN_TYPE_READ | INTR_EN_IHD_MASK | INTR_EN_IHD |\r\ni;\r\nif (i == 0) {\r\nintr_context->handler = qlge_isr;\r\nsprintf(intr_context->name, "%s-rx-%d",\r\nqdev->ndev->name, i);\r\n} else {\r\nintr_context->handler = qlge_msix_rx_isr;\r\nsprintf(intr_context->name, "%s-rx-%d",\r\nqdev->ndev->name, i);\r\n}\r\n}\r\n} else {\r\nintr_context->intr = 0;\r\nintr_context->qdev = qdev;\r\nintr_context->intr_en_mask =\r\nINTR_EN_TYPE_MASK | INTR_EN_INTR_MASK | INTR_EN_TYPE_ENABLE;\r\nintr_context->intr_dis_mask =\r\nINTR_EN_TYPE_MASK | INTR_EN_INTR_MASK |\r\nINTR_EN_TYPE_DISABLE;\r\nintr_context->intr_read_mask =\r\nINTR_EN_TYPE_MASK | INTR_EN_INTR_MASK | INTR_EN_TYPE_READ;\r\nintr_context->handler = qlge_isr;\r\nsprintf(intr_context->name, "%s-single_irq", qdev->ndev->name);\r\nql_set_irq_mask(qdev, intr_context);\r\n}\r\nql_set_tx_vect(qdev);\r\n}\r\nstatic void ql_free_irq(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nstruct intr_context *intr_context = &qdev->intr_context[0];\r\nfor (i = 0; i < qdev->intr_count; i++, intr_context++) {\r\nif (intr_context->hooked) {\r\nif (test_bit(QL_MSIX_ENABLED, &qdev->flags)) {\r\nfree_irq(qdev->msi_x_entry[i].vector,\r\n&qdev->rx_ring[i]);\r\n} else {\r\nfree_irq(qdev->pdev->irq, &qdev->rx_ring[0]);\r\n}\r\n}\r\n}\r\nql_disable_msix(qdev);\r\n}\r\nstatic int ql_request_irq(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nint status = 0;\r\nstruct pci_dev *pdev = qdev->pdev;\r\nstruct intr_context *intr_context = &qdev->intr_context[0];\r\nql_resolve_queues_to_irqs(qdev);\r\nfor (i = 0; i < qdev->intr_count; i++, intr_context++) {\r\natomic_set(&intr_context->irq_cnt, 0);\r\nif (test_bit(QL_MSIX_ENABLED, &qdev->flags)) {\r\nstatus = request_irq(qdev->msi_x_entry[i].vector,\r\nintr_context->handler,\r\n0,\r\nintr_context->name,\r\n&qdev->rx_ring[i]);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed request for MSIX interrupt %d.\n",\r\ni);\r\ngoto err_irq;\r\n}\r\n} else {\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"trying msi or legacy interrupts.\n");\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"%s: irq = %d.\n", __func__, pdev->irq);\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"%s: context->name = %s.\n", __func__,\r\nintr_context->name);\r\nnetif_printk(qdev, ifup, KERN_DEBUG, qdev->ndev,\r\n"%s: dev_id = 0x%p.\n", __func__,\r\n&qdev->rx_ring[0]);\r\nstatus =\r\nrequest_irq(pdev->irq, qlge_isr,\r\ntest_bit(QL_MSI_ENABLED,\r\n&qdev->\r\nflags) ? 0 : IRQF_SHARED,\r\nintr_context->name, &qdev->rx_ring[0]);\r\nif (status)\r\ngoto err_irq;\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Hooked intr %d, queue type %s, with name %s.\n",\r\ni,\r\nqdev->rx_ring[0].type == DEFAULT_Q ?\r\n"DEFAULT_Q" :\r\nqdev->rx_ring[0].type == TX_Q ? "TX_Q" :\r\nqdev->rx_ring[0].type == RX_Q ? "RX_Q" : "",\r\nintr_context->name);\r\n}\r\nintr_context->hooked = 1;\r\n}\r\nreturn status;\r\nerr_irq:\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to get the interrupts!!!\n");\r\nql_free_irq(qdev);\r\nreturn status;\r\n}\r\nstatic int ql_start_rss(struct ql_adapter *qdev)\r\n{\r\nstatic const u8 init_hash_seed[] = {\r\n0x6d, 0x5a, 0x56, 0xda, 0x25, 0x5b, 0x0e, 0xc2,\r\n0x41, 0x67, 0x25, 0x3d, 0x43, 0xa3, 0x8f, 0xb0,\r\n0xd0, 0xca, 0x2b, 0xcb, 0xae, 0x7b, 0x30, 0xb4,\r\n0x77, 0xcb, 0x2d, 0xa3, 0x80, 0x30, 0xf2, 0x0c,\r\n0x6a, 0x42, 0xb7, 0x3b, 0xbe, 0xac, 0x01, 0xfa\r\n};\r\nstruct ricb *ricb = &qdev->ricb;\r\nint status = 0;\r\nint i;\r\nu8 *hash_id = (u8 *) ricb->hash_cq_id;\r\nmemset((void *)ricb, 0, sizeof(*ricb));\r\nricb->base_cq = RSS_L4K;\r\nricb->flags =\r\n(RSS_L6K | RSS_LI | RSS_LB | RSS_LM | RSS_RT4 | RSS_RT6);\r\nricb->mask = cpu_to_le16((u16)(0x3ff));\r\nfor (i = 0; i < 1024; i++)\r\nhash_id[i] = (i & (qdev->rss_ring_count - 1));\r\nmemcpy((void *)&ricb->ipv6_hash_key[0], init_hash_seed, 40);\r\nmemcpy((void *)&ricb->ipv4_hash_key[0], init_hash_seed, 16);\r\nstatus = ql_write_cfg(qdev, ricb, sizeof(*ricb), CFG_LR, 0);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to load RICB.\n");\r\nreturn status;\r\n}\r\nreturn status;\r\n}\r\nstatic int ql_clear_routing_entries(struct ql_adapter *qdev)\r\n{\r\nint i, status = 0;\r\nstatus = ql_sem_spinlock(qdev, SEM_RT_IDX_MASK);\r\nif (status)\r\nreturn status;\r\nfor (i = 0; i < 16; i++) {\r\nstatus = ql_set_routing_reg(qdev, i, 0, 0);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init routing register for CAM packets.\n");\r\nbreak;\r\n}\r\n}\r\nql_sem_unlock(qdev, SEM_RT_IDX_MASK);\r\nreturn status;\r\n}\r\nstatic int ql_route_initialize(struct ql_adapter *qdev)\r\n{\r\nint status = 0;\r\nstatus = ql_clear_routing_entries(qdev);\r\nif (status)\r\nreturn status;\r\nstatus = ql_sem_spinlock(qdev, SEM_RT_IDX_MASK);\r\nif (status)\r\nreturn status;\r\nstatus = ql_set_routing_reg(qdev, RT_IDX_IP_CSUM_ERR_SLOT,\r\nRT_IDX_IP_CSUM_ERR, 1);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init routing register "\r\n"for IP CSUM error packets.\n");\r\ngoto exit;\r\n}\r\nstatus = ql_set_routing_reg(qdev, RT_IDX_TCP_UDP_CSUM_ERR_SLOT,\r\nRT_IDX_TU_CSUM_ERR, 1);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init routing register "\r\n"for TCP/UDP CSUM error packets.\n");\r\ngoto exit;\r\n}\r\nstatus = ql_set_routing_reg(qdev, RT_IDX_BCAST_SLOT, RT_IDX_BCAST, 1);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init routing register for broadcast packets.\n");\r\ngoto exit;\r\n}\r\nif (qdev->rss_ring_count > 1) {\r\nstatus = ql_set_routing_reg(qdev, RT_IDX_RSS_MATCH_SLOT,\r\nRT_IDX_RSS_MATCH, 1);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init routing register for MATCH RSS packets.\n");\r\ngoto exit;\r\n}\r\n}\r\nstatus = ql_set_routing_reg(qdev, RT_IDX_CAM_HIT_SLOT,\r\nRT_IDX_CAM_HIT, 1);\r\nif (status)\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init routing register for CAM packets.\n");\r\nexit:\r\nql_sem_unlock(qdev, SEM_RT_IDX_MASK);\r\nreturn status;\r\n}\r\nint ql_cam_route_initialize(struct ql_adapter *qdev)\r\n{\r\nint status, set;\r\nset = ql_read32(qdev, STS);\r\nset &= qdev->port_link_up;\r\nstatus = ql_set_mac_addr(qdev, set);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to init mac address.\n");\r\nreturn status;\r\n}\r\nstatus = ql_route_initialize(qdev);\r\nif (status)\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to init routing table.\n");\r\nreturn status;\r\n}\r\nstatic int ql_adapter_initialize(struct ql_adapter *qdev)\r\n{\r\nu32 value, mask;\r\nint i;\r\nint status = 0;\r\nvalue = SYS_EFE | SYS_FAE;\r\nmask = value << 16;\r\nql_write32(qdev, SYS, mask | value);\r\nvalue = NIC_RCV_CFG_DFQ;\r\nmask = NIC_RCV_CFG_DFQ_MASK;\r\nif (qdev->ndev->features & NETIF_F_HW_VLAN_CTAG_RX) {\r\nvalue |= NIC_RCV_CFG_RV;\r\nmask |= (NIC_RCV_CFG_RV << 16);\r\n}\r\nql_write32(qdev, NIC_RCV_CFG, (mask | value));\r\nql_write32(qdev, INTR_MASK, (INTR_MASK_PI << 16) | INTR_MASK_PI);\r\nvalue = FSC_FE | FSC_EPC_INBOUND | FSC_EPC_OUTBOUND |\r\nFSC_EC | FSC_VM_PAGE_4K;\r\nvalue |= SPLT_SETTING;\r\nmask = FSC_VM_PAGESIZE_MASK |\r\nFSC_DBL_MASK | FSC_DBRST_MASK | (value << 16);\r\nql_write32(qdev, FSC, mask | value);\r\nql_write32(qdev, SPLT_HDR, SPLT_LEN);\r\nql_write32(qdev, RST_FO, RST_FO_RR_MASK | RST_FO_RR_RCV_FUNC_CQ);\r\nvalue = ql_read32(qdev, MGMT_RCV_CFG);\r\nvalue &= ~MGMT_RCV_CFG_RM;\r\nmask = 0xffff0000;\r\nql_write32(qdev, MGMT_RCV_CFG, mask);\r\nql_write32(qdev, MGMT_RCV_CFG, mask | value);\r\nif (qdev->pdev->subsystem_device == 0x0068 ||\r\nqdev->pdev->subsystem_device == 0x0180)\r\nqdev->wol = WAKE_MAGIC;\r\nfor (i = 0; i < qdev->rx_ring_count; i++) {\r\nstatus = ql_start_rx_ring(qdev, &qdev->rx_ring[i]);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to start rx ring[%d].\n", i);\r\nreturn status;\r\n}\r\n}\r\nif (qdev->rss_ring_count > 1) {\r\nstatus = ql_start_rss(qdev);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to start RSS.\n");\r\nreturn status;\r\n}\r\n}\r\nfor (i = 0; i < qdev->tx_ring_count; i++) {\r\nstatus = ql_start_tx_ring(qdev, &qdev->tx_ring[i]);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to start tx ring[%d].\n", i);\r\nreturn status;\r\n}\r\n}\r\nstatus = qdev->nic_ops->port_initialize(qdev);\r\nif (status)\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to start port.\n");\r\nstatus = ql_cam_route_initialize(qdev);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Failed to init CAM/Routing tables.\n");\r\nreturn status;\r\n}\r\nfor (i = 0; i < qdev->rss_ring_count; i++)\r\nnapi_enable(&qdev->rx_ring[i].napi);\r\nreturn status;\r\n}\r\nstatic int ql_adapter_reset(struct ql_adapter *qdev)\r\n{\r\nu32 value;\r\nint status = 0;\r\nunsigned long end_jiffies;\r\nstatus = ql_clear_routing_entries(qdev);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Failed to clear routing bits.\n");\r\nreturn status;\r\n}\r\nif (!test_bit(QL_ASIC_RECOVERY, &qdev->flags)) {\r\nql_mb_set_mgmnt_traffic_ctl(qdev, MB_SET_MPI_TFK_STOP);\r\nql_wait_fifo_empty(qdev);\r\n} else\r\nclear_bit(QL_ASIC_RECOVERY, &qdev->flags);\r\nql_write32(qdev, RST_FO, (RST_FO_FR << 16) | RST_FO_FR);\r\nend_jiffies = jiffies + usecs_to_jiffies(30);\r\ndo {\r\nvalue = ql_read32(qdev, RST_FO);\r\nif ((value & RST_FO_FR) == 0)\r\nbreak;\r\ncpu_relax();\r\n} while (time_before(jiffies, end_jiffies));\r\nif (value & RST_FO_FR) {\r\nnetif_err(qdev, ifdown, qdev->ndev,\r\n"ETIMEDOUT!!! errored out of resetting the chip!\n");\r\nstatus = -ETIMEDOUT;\r\n}\r\nql_mb_set_mgmnt_traffic_ctl(qdev, MB_SET_MPI_TFK_RESUME);\r\nreturn status;\r\n}\r\nstatic void ql_display_dev_info(struct net_device *ndev)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nnetif_info(qdev, probe, qdev->ndev,\r\n"Function #%d, Port %d, NIC Roll %d, NIC Rev = %d, "\r\n"XG Roll = %d, XG Rev = %d.\n",\r\nqdev->func,\r\nqdev->port,\r\nqdev->chip_rev_id & 0x0000000f,\r\nqdev->chip_rev_id >> 4 & 0x0000000f,\r\nqdev->chip_rev_id >> 8 & 0x0000000f,\r\nqdev->chip_rev_id >> 12 & 0x0000000f);\r\nnetif_info(qdev, probe, qdev->ndev,\r\n"MAC address %pM\n", ndev->dev_addr);\r\n}\r\nstatic int ql_wol(struct ql_adapter *qdev)\r\n{\r\nint status = 0;\r\nu32 wol = MB_WOL_DISABLE;\r\nif (qdev->wol & (WAKE_ARP | WAKE_MAGICSECURE | WAKE_PHY | WAKE_UCAST |\r\nWAKE_MCAST | WAKE_BCAST)) {\r\nnetif_err(qdev, ifdown, qdev->ndev,\r\n"Unsupported WOL parameter. qdev->wol = 0x%x.\n",\r\nqdev->wol);\r\nreturn -EINVAL;\r\n}\r\nif (qdev->wol & WAKE_MAGIC) {\r\nstatus = ql_mb_wol_set_magic(qdev, 1);\r\nif (status) {\r\nnetif_err(qdev, ifdown, qdev->ndev,\r\n"Failed to set magic packet on %s.\n",\r\nqdev->ndev->name);\r\nreturn status;\r\n} else\r\nnetif_info(qdev, drv, qdev->ndev,\r\n"Enabled magic packet successfully on %s.\n",\r\nqdev->ndev->name);\r\nwol |= MB_WOL_MAGIC_PKT;\r\n}\r\nif (qdev->wol) {\r\nwol |= MB_WOL_MODE_ON;\r\nstatus = ql_mb_wol_mode(qdev, wol);\r\nnetif_err(qdev, drv, qdev->ndev,\r\n"WOL %s (wol code 0x%x) on %s\n",\r\n(status == 0) ? "Successfully set" : "Failed",\r\nwol, qdev->ndev->name);\r\n}\r\nreturn status;\r\n}\r\nstatic void ql_cancel_all_work_sync(struct ql_adapter *qdev)\r\n{\r\nif (test_bit(QL_ADAPTER_UP, &qdev->flags))\r\ncancel_delayed_work_sync(&qdev->asic_reset_work);\r\ncancel_delayed_work_sync(&qdev->mpi_reset_work);\r\ncancel_delayed_work_sync(&qdev->mpi_work);\r\ncancel_delayed_work_sync(&qdev->mpi_idc_work);\r\ncancel_delayed_work_sync(&qdev->mpi_core_to_log);\r\ncancel_delayed_work_sync(&qdev->mpi_port_cfg_work);\r\n}\r\nstatic int ql_adapter_down(struct ql_adapter *qdev)\r\n{\r\nint i, status = 0;\r\nql_link_off(qdev);\r\nql_cancel_all_work_sync(qdev);\r\nfor (i = 0; i < qdev->rss_ring_count; i++)\r\nnapi_disable(&qdev->rx_ring[i].napi);\r\nclear_bit(QL_ADAPTER_UP, &qdev->flags);\r\nql_disable_interrupts(qdev);\r\nql_tx_ring_clean(qdev);\r\nfor (i = 0; i < qdev->rss_ring_count; i++)\r\nnetif_napi_del(&qdev->rx_ring[i].napi);\r\nstatus = ql_adapter_reset(qdev);\r\nif (status)\r\nnetif_err(qdev, ifdown, qdev->ndev, "reset(func #%d) FAILED!\n",\r\nqdev->func);\r\nql_free_rx_buffers(qdev);\r\nreturn status;\r\n}\r\nstatic int ql_adapter_up(struct ql_adapter *qdev)\r\n{\r\nint err = 0;\r\nerr = ql_adapter_initialize(qdev);\r\nif (err) {\r\nnetif_info(qdev, ifup, qdev->ndev, "Unable to initialize adapter.\n");\r\ngoto err_init;\r\n}\r\nset_bit(QL_ADAPTER_UP, &qdev->flags);\r\nql_alloc_rx_buffers(qdev);\r\nif ((ql_read32(qdev, STS) & qdev->port_init) &&\r\n(ql_read32(qdev, STS) & qdev->port_link_up))\r\nql_link_on(qdev);\r\nclear_bit(QL_ALLMULTI, &qdev->flags);\r\nclear_bit(QL_PROMISCUOUS, &qdev->flags);\r\nqlge_set_multicast_list(qdev->ndev);\r\nqlge_restore_vlan(qdev);\r\nql_enable_interrupts(qdev);\r\nql_enable_all_completion_interrupts(qdev);\r\nnetif_tx_start_all_queues(qdev->ndev);\r\nreturn 0;\r\nerr_init:\r\nql_adapter_reset(qdev);\r\nreturn err;\r\n}\r\nstatic void ql_release_adapter_resources(struct ql_adapter *qdev)\r\n{\r\nql_free_mem_resources(qdev);\r\nql_free_irq(qdev);\r\n}\r\nstatic int ql_get_adapter_resources(struct ql_adapter *qdev)\r\n{\r\nint status = 0;\r\nif (ql_alloc_mem_resources(qdev)) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Unable to allocate memory.\n");\r\nreturn -ENOMEM;\r\n}\r\nstatus = ql_request_irq(qdev);\r\nreturn status;\r\n}\r\nstatic int qlge_close(struct net_device *ndev)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nif (test_bit(QL_EEH_FATAL, &qdev->flags)) {\r\nnetif_err(qdev, drv, qdev->ndev, "EEH fatal did unload.\n");\r\nclear_bit(QL_EEH_FATAL, &qdev->flags);\r\nreturn 0;\r\n}\r\nwhile (!test_bit(QL_ADAPTER_UP, &qdev->flags))\r\nmsleep(1);\r\nql_adapter_down(qdev);\r\nql_release_adapter_resources(qdev);\r\nreturn 0;\r\n}\r\nstatic int ql_configure_rings(struct ql_adapter *qdev)\r\n{\r\nint i;\r\nstruct rx_ring *rx_ring;\r\nstruct tx_ring *tx_ring;\r\nint cpu_cnt = min(MAX_CPUS, (int)num_online_cpus());\r\nunsigned int lbq_buf_len = (qdev->ndev->mtu > 1500) ?\r\nLARGE_BUFFER_MAX_SIZE : LARGE_BUFFER_MIN_SIZE;\r\nqdev->lbq_buf_order = get_order(lbq_buf_len);\r\nqdev->intr_count = cpu_cnt;\r\nql_enable_msix(qdev);\r\nqdev->rss_ring_count = qdev->intr_count;\r\nqdev->tx_ring_count = cpu_cnt;\r\nqdev->rx_ring_count = qdev->tx_ring_count + qdev->rss_ring_count;\r\nfor (i = 0; i < qdev->tx_ring_count; i++) {\r\ntx_ring = &qdev->tx_ring[i];\r\nmemset((void *)tx_ring, 0, sizeof(*tx_ring));\r\ntx_ring->qdev = qdev;\r\ntx_ring->wq_id = i;\r\ntx_ring->wq_len = qdev->tx_ring_size;\r\ntx_ring->wq_size =\r\ntx_ring->wq_len * sizeof(struct ob_mac_iocb_req);\r\ntx_ring->cq_id = qdev->rss_ring_count + i;\r\n}\r\nfor (i = 0; i < qdev->rx_ring_count; i++) {\r\nrx_ring = &qdev->rx_ring[i];\r\nmemset((void *)rx_ring, 0, sizeof(*rx_ring));\r\nrx_ring->qdev = qdev;\r\nrx_ring->cq_id = i;\r\nrx_ring->cpu = i % cpu_cnt;\r\nif (i < qdev->rss_ring_count) {\r\nrx_ring->cq_len = qdev->rx_ring_size;\r\nrx_ring->cq_size =\r\nrx_ring->cq_len * sizeof(struct ql_net_rsp_iocb);\r\nrx_ring->lbq_len = NUM_LARGE_BUFFERS;\r\nrx_ring->lbq_size =\r\nrx_ring->lbq_len * sizeof(__le64);\r\nrx_ring->lbq_buf_size = (u16)lbq_buf_len;\r\nrx_ring->sbq_len = NUM_SMALL_BUFFERS;\r\nrx_ring->sbq_size =\r\nrx_ring->sbq_len * sizeof(__le64);\r\nrx_ring->sbq_buf_size = SMALL_BUF_MAP_SIZE;\r\nrx_ring->type = RX_Q;\r\n} else {\r\nrx_ring->cq_len = qdev->tx_ring_size;\r\nrx_ring->cq_size =\r\nrx_ring->cq_len * sizeof(struct ql_net_rsp_iocb);\r\nrx_ring->lbq_len = 0;\r\nrx_ring->lbq_size = 0;\r\nrx_ring->lbq_buf_size = 0;\r\nrx_ring->sbq_len = 0;\r\nrx_ring->sbq_size = 0;\r\nrx_ring->sbq_buf_size = 0;\r\nrx_ring->type = TX_Q;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int qlge_open(struct net_device *ndev)\r\n{\r\nint err = 0;\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nerr = ql_adapter_reset(qdev);\r\nif (err)\r\nreturn err;\r\nerr = ql_configure_rings(qdev);\r\nif (err)\r\nreturn err;\r\nerr = ql_get_adapter_resources(qdev);\r\nif (err)\r\ngoto error_up;\r\nerr = ql_adapter_up(qdev);\r\nif (err)\r\ngoto error_up;\r\nreturn err;\r\nerror_up:\r\nql_release_adapter_resources(qdev);\r\nreturn err;\r\n}\r\nstatic int ql_change_rx_buffers(struct ql_adapter *qdev)\r\n{\r\nstruct rx_ring *rx_ring;\r\nint i, status;\r\nu32 lbq_buf_len;\r\nif (!test_bit(QL_ADAPTER_UP, &qdev->flags)) {\r\nint i = 4;\r\nwhile (--i && !test_bit(QL_ADAPTER_UP, &qdev->flags)) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Waiting for adapter UP...\n");\r\nssleep(1);\r\n}\r\nif (!i) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Timed out waiting for adapter UP\n");\r\nreturn -ETIMEDOUT;\r\n}\r\n}\r\nstatus = ql_adapter_down(qdev);\r\nif (status)\r\ngoto error;\r\nlbq_buf_len = (qdev->ndev->mtu > 1500) ?\r\nLARGE_BUFFER_MAX_SIZE : LARGE_BUFFER_MIN_SIZE;\r\nqdev->lbq_buf_order = get_order(lbq_buf_len);\r\nfor (i = 0; i < qdev->rss_ring_count; i++) {\r\nrx_ring = &qdev->rx_ring[i];\r\nrx_ring->lbq_buf_size = lbq_buf_len;\r\n}\r\nstatus = ql_adapter_up(qdev);\r\nif (status)\r\ngoto error;\r\nreturn status;\r\nerror:\r\nnetif_alert(qdev, ifup, qdev->ndev,\r\n"Driver up/down cycle failed, closing device.\n");\r\nset_bit(QL_ADAPTER_UP, &qdev->flags);\r\ndev_close(qdev->ndev);\r\nreturn status;\r\n}\r\nstatic int qlge_change_mtu(struct net_device *ndev, int new_mtu)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint status;\r\nif (ndev->mtu == 1500 && new_mtu == 9000) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Changing to jumbo MTU.\n");\r\n} else if (ndev->mtu == 9000 && new_mtu == 1500) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Changing to normal MTU.\n");\r\n} else\r\nreturn -EINVAL;\r\nqueue_delayed_work(qdev->workqueue,\r\n&qdev->mpi_port_cfg_work, 3*HZ);\r\nndev->mtu = new_mtu;\r\nif (!netif_running(qdev->ndev)) {\r\nreturn 0;\r\n}\r\nstatus = ql_change_rx_buffers(qdev);\r\nif (status) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Changing MTU failed.\n");\r\n}\r\nreturn status;\r\n}\r\nstatic struct net_device_stats *qlge_get_stats(struct net_device\r\n*ndev)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nstruct rx_ring *rx_ring = &qdev->rx_ring[0];\r\nstruct tx_ring *tx_ring = &qdev->tx_ring[0];\r\nunsigned long pkts, mcast, dropped, errors, bytes;\r\nint i;\r\npkts = mcast = dropped = errors = bytes = 0;\r\nfor (i = 0; i < qdev->rss_ring_count; i++, rx_ring++) {\r\npkts += rx_ring->rx_packets;\r\nbytes += rx_ring->rx_bytes;\r\ndropped += rx_ring->rx_dropped;\r\nerrors += rx_ring->rx_errors;\r\nmcast += rx_ring->rx_multicast;\r\n}\r\nndev->stats.rx_packets = pkts;\r\nndev->stats.rx_bytes = bytes;\r\nndev->stats.rx_dropped = dropped;\r\nndev->stats.rx_errors = errors;\r\nndev->stats.multicast = mcast;\r\npkts = errors = bytes = 0;\r\nfor (i = 0; i < qdev->tx_ring_count; i++, tx_ring++) {\r\npkts += tx_ring->tx_packets;\r\nbytes += tx_ring->tx_bytes;\r\nerrors += tx_ring->tx_errors;\r\n}\r\nndev->stats.tx_packets = pkts;\r\nndev->stats.tx_bytes = bytes;\r\nndev->stats.tx_errors = errors;\r\nreturn &ndev->stats;\r\n}\r\nstatic void qlge_set_multicast_list(struct net_device *ndev)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nstruct netdev_hw_addr *ha;\r\nint i, status;\r\nstatus = ql_sem_spinlock(qdev, SEM_RT_IDX_MASK);\r\nif (status)\r\nreturn;\r\nif (ndev->flags & IFF_PROMISC) {\r\nif (!test_bit(QL_PROMISCUOUS, &qdev->flags)) {\r\nif (ql_set_routing_reg\r\n(qdev, RT_IDX_PROMISCUOUS_SLOT, RT_IDX_VALID, 1)) {\r\nnetif_err(qdev, hw, qdev->ndev,\r\n"Failed to set promiscuous mode.\n");\r\n} else {\r\nset_bit(QL_PROMISCUOUS, &qdev->flags);\r\n}\r\n}\r\n} else {\r\nif (test_bit(QL_PROMISCUOUS, &qdev->flags)) {\r\nif (ql_set_routing_reg\r\n(qdev, RT_IDX_PROMISCUOUS_SLOT, RT_IDX_VALID, 0)) {\r\nnetif_err(qdev, hw, qdev->ndev,\r\n"Failed to clear promiscuous mode.\n");\r\n} else {\r\nclear_bit(QL_PROMISCUOUS, &qdev->flags);\r\n}\r\n}\r\n}\r\nif ((ndev->flags & IFF_ALLMULTI) ||\r\n(netdev_mc_count(ndev) > MAX_MULTICAST_ENTRIES)) {\r\nif (!test_bit(QL_ALLMULTI, &qdev->flags)) {\r\nif (ql_set_routing_reg\r\n(qdev, RT_IDX_ALLMULTI_SLOT, RT_IDX_MCAST, 1)) {\r\nnetif_err(qdev, hw, qdev->ndev,\r\n"Failed to set all-multi mode.\n");\r\n} else {\r\nset_bit(QL_ALLMULTI, &qdev->flags);\r\n}\r\n}\r\n} else {\r\nif (test_bit(QL_ALLMULTI, &qdev->flags)) {\r\nif (ql_set_routing_reg\r\n(qdev, RT_IDX_ALLMULTI_SLOT, RT_IDX_MCAST, 0)) {\r\nnetif_err(qdev, hw, qdev->ndev,\r\n"Failed to clear all-multi mode.\n");\r\n} else {\r\nclear_bit(QL_ALLMULTI, &qdev->flags);\r\n}\r\n}\r\n}\r\nif (!netdev_mc_empty(ndev)) {\r\nstatus = ql_sem_spinlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (status)\r\ngoto exit;\r\ni = 0;\r\nnetdev_for_each_mc_addr(ha, ndev) {\r\nif (ql_set_mac_addr_reg(qdev, (u8 *) ha->addr,\r\nMAC_ADDR_TYPE_MULTI_MAC, i)) {\r\nnetif_err(qdev, hw, qdev->ndev,\r\n"Failed to loadmulticast address.\n");\r\nql_sem_unlock(qdev, SEM_MAC_ADDR_MASK);\r\ngoto exit;\r\n}\r\ni++;\r\n}\r\nql_sem_unlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (ql_set_routing_reg\r\n(qdev, RT_IDX_MCAST_MATCH_SLOT, RT_IDX_MCAST_MATCH, 1)) {\r\nnetif_err(qdev, hw, qdev->ndev,\r\n"Failed to set multicast match mode.\n");\r\n} else {\r\nset_bit(QL_ALLMULTI, &qdev->flags);\r\n}\r\n}\r\nexit:\r\nql_sem_unlock(qdev, SEM_RT_IDX_MASK);\r\n}\r\nstatic int qlge_set_mac_address(struct net_device *ndev, void *p)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nstruct sockaddr *addr = p;\r\nint status;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(ndev->dev_addr, addr->sa_data, ndev->addr_len);\r\nmemcpy(qdev->current_mac_addr, ndev->dev_addr, ndev->addr_len);\r\nstatus = ql_sem_spinlock(qdev, SEM_MAC_ADDR_MASK);\r\nif (status)\r\nreturn status;\r\nstatus = ql_set_mac_addr_reg(qdev, (u8 *) ndev->dev_addr,\r\nMAC_ADDR_TYPE_CAM_MAC, qdev->func * MAX_CQ);\r\nif (status)\r\nnetif_err(qdev, hw, qdev->ndev, "Failed to load MAC address.\n");\r\nql_sem_unlock(qdev, SEM_MAC_ADDR_MASK);\r\nreturn status;\r\n}\r\nstatic void qlge_tx_timeout(struct net_device *ndev)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nql_queue_asic_error(qdev);\r\n}\r\nstatic void ql_asic_reset_work(struct work_struct *work)\r\n{\r\nstruct ql_adapter *qdev =\r\ncontainer_of(work, struct ql_adapter, asic_reset_work.work);\r\nint status;\r\nrtnl_lock();\r\nstatus = ql_adapter_down(qdev);\r\nif (status)\r\ngoto error;\r\nstatus = ql_adapter_up(qdev);\r\nif (status)\r\ngoto error;\r\nclear_bit(QL_ALLMULTI, &qdev->flags);\r\nclear_bit(QL_PROMISCUOUS, &qdev->flags);\r\nqlge_set_multicast_list(qdev->ndev);\r\nrtnl_unlock();\r\nreturn;\r\nerror:\r\nnetif_alert(qdev, ifup, qdev->ndev,\r\n"Driver up/down cycle failed, closing device\n");\r\nset_bit(QL_ADAPTER_UP, &qdev->flags);\r\ndev_close(qdev->ndev);\r\nrtnl_unlock();\r\n}\r\nstatic int ql_get_alt_pcie_func(struct ql_adapter *qdev)\r\n{\r\nint status = 0;\r\nu32 temp;\r\nu32 nic_func1, nic_func2;\r\nstatus = ql_read_mpi_reg(qdev, MPI_TEST_FUNC_PORT_CFG,\r\n&temp);\r\nif (status)\r\nreturn status;\r\nnic_func1 = ((temp >> MPI_TEST_NIC1_FUNC_SHIFT) &\r\nMPI_TEST_NIC_FUNC_MASK);\r\nnic_func2 = ((temp >> MPI_TEST_NIC2_FUNC_SHIFT) &\r\nMPI_TEST_NIC_FUNC_MASK);\r\nif (qdev->func == nic_func1)\r\nqdev->alt_func = nic_func2;\r\nelse if (qdev->func == nic_func2)\r\nqdev->alt_func = nic_func1;\r\nelse\r\nstatus = -EIO;\r\nreturn status;\r\n}\r\nstatic int ql_get_board_info(struct ql_adapter *qdev)\r\n{\r\nint status;\r\nqdev->func =\r\n(ql_read32(qdev, STS) & STS_FUNC_ID_MASK) >> STS_FUNC_ID_SHIFT;\r\nif (qdev->func > 3)\r\nreturn -EIO;\r\nstatus = ql_get_alt_pcie_func(qdev);\r\nif (status)\r\nreturn status;\r\nqdev->port = (qdev->func < qdev->alt_func) ? 0 : 1;\r\nif (qdev->port) {\r\nqdev->xg_sem_mask = SEM_XGMAC1_MASK;\r\nqdev->port_link_up = STS_PL1;\r\nqdev->port_init = STS_PI1;\r\nqdev->mailbox_in = PROC_ADDR_MPI_RISC | PROC_ADDR_FUNC2_MBI;\r\nqdev->mailbox_out = PROC_ADDR_MPI_RISC | PROC_ADDR_FUNC2_MBO;\r\n} else {\r\nqdev->xg_sem_mask = SEM_XGMAC0_MASK;\r\nqdev->port_link_up = STS_PL0;\r\nqdev->port_init = STS_PI0;\r\nqdev->mailbox_in = PROC_ADDR_MPI_RISC | PROC_ADDR_FUNC0_MBI;\r\nqdev->mailbox_out = PROC_ADDR_MPI_RISC | PROC_ADDR_FUNC0_MBO;\r\n}\r\nqdev->chip_rev_id = ql_read32(qdev, REV_ID);\r\nqdev->device_id = qdev->pdev->device;\r\nif (qdev->device_id == QLGE_DEVICE_ID_8012)\r\nqdev->nic_ops = &qla8012_nic_ops;\r\nelse if (qdev->device_id == QLGE_DEVICE_ID_8000)\r\nqdev->nic_ops = &qla8000_nic_ops;\r\nreturn status;\r\n}\r\nstatic void ql_release_all(struct pci_dev *pdev)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nif (qdev->workqueue) {\r\ndestroy_workqueue(qdev->workqueue);\r\nqdev->workqueue = NULL;\r\n}\r\nif (qdev->reg_base)\r\niounmap(qdev->reg_base);\r\nif (qdev->doorbell_area)\r\niounmap(qdev->doorbell_area);\r\nvfree(qdev->mpi_coredump);\r\npci_release_regions(pdev);\r\n}\r\nstatic int ql_init_device(struct pci_dev *pdev, struct net_device *ndev,\r\nint cards_found)\r\n{\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint err = 0;\r\nmemset((void *)qdev, 0, sizeof(*qdev));\r\nerr = pci_enable_device(pdev);\r\nif (err) {\r\ndev_err(&pdev->dev, "PCI device enable failed.\n");\r\nreturn err;\r\n}\r\nqdev->ndev = ndev;\r\nqdev->pdev = pdev;\r\npci_set_drvdata(pdev, ndev);\r\nerr = pcie_set_readrq(pdev, 4096);\r\nif (err) {\r\ndev_err(&pdev->dev, "Set readrq failed.\n");\r\ngoto err_out1;\r\n}\r\nerr = pci_request_regions(pdev, DRV_NAME);\r\nif (err) {\r\ndev_err(&pdev->dev, "PCI region request failed.\n");\r\nreturn err;\r\n}\r\npci_set_master(pdev);\r\nif (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\nset_bit(QL_DMA64, &qdev->flags);\r\nerr = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));\r\n} else {\r\nerr = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (!err)\r\nerr = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\n}\r\nif (err) {\r\ndev_err(&pdev->dev, "No usable DMA configuration.\n");\r\ngoto err_out2;\r\n}\r\npdev->needs_freset = 1;\r\npci_save_state(pdev);\r\nqdev->reg_base =\r\nioremap_nocache(pci_resource_start(pdev, 1),\r\npci_resource_len(pdev, 1));\r\nif (!qdev->reg_base) {\r\ndev_err(&pdev->dev, "Register mapping failed.\n");\r\nerr = -ENOMEM;\r\ngoto err_out2;\r\n}\r\nqdev->doorbell_area_size = pci_resource_len(pdev, 3);\r\nqdev->doorbell_area =\r\nioremap_nocache(pci_resource_start(pdev, 3),\r\npci_resource_len(pdev, 3));\r\nif (!qdev->doorbell_area) {\r\ndev_err(&pdev->dev, "Doorbell register mapping failed.\n");\r\nerr = -ENOMEM;\r\ngoto err_out2;\r\n}\r\nerr = ql_get_board_info(qdev);\r\nif (err) {\r\ndev_err(&pdev->dev, "Register access failed.\n");\r\nerr = -EIO;\r\ngoto err_out2;\r\n}\r\nqdev->msg_enable = netif_msg_init(debug, default_msg);\r\nspin_lock_init(&qdev->hw_lock);\r\nspin_lock_init(&qdev->stats_lock);\r\nif (qlge_mpi_coredump) {\r\nqdev->mpi_coredump =\r\nvmalloc(sizeof(struct ql_mpi_coredump));\r\nif (qdev->mpi_coredump == NULL) {\r\nerr = -ENOMEM;\r\ngoto err_out2;\r\n}\r\nif (qlge_force_coredump)\r\nset_bit(QL_FRC_COREDUMP, &qdev->flags);\r\n}\r\nerr = qdev->nic_ops->get_flash(qdev);\r\nif (err) {\r\ndev_err(&pdev->dev, "Invalid FLASH.\n");\r\ngoto err_out2;\r\n}\r\nmemcpy(qdev->current_mac_addr, ndev->dev_addr, ndev->addr_len);\r\nqdev->tx_ring_size = NUM_TX_RING_ENTRIES;\r\nqdev->rx_ring_size = NUM_RX_RING_ENTRIES;\r\nqdev->rx_coalesce_usecs = DFLT_COALESCE_WAIT;\r\nqdev->tx_coalesce_usecs = DFLT_COALESCE_WAIT;\r\nqdev->rx_max_coalesced_frames = DFLT_INTER_FRAME_WAIT;\r\nqdev->tx_max_coalesced_frames = DFLT_INTER_FRAME_WAIT;\r\nqdev->workqueue = create_singlethread_workqueue(ndev->name);\r\nINIT_DELAYED_WORK(&qdev->asic_reset_work, ql_asic_reset_work);\r\nINIT_DELAYED_WORK(&qdev->mpi_reset_work, ql_mpi_reset_work);\r\nINIT_DELAYED_WORK(&qdev->mpi_work, ql_mpi_work);\r\nINIT_DELAYED_WORK(&qdev->mpi_port_cfg_work, ql_mpi_port_cfg_work);\r\nINIT_DELAYED_WORK(&qdev->mpi_idc_work, ql_mpi_idc_work);\r\nINIT_DELAYED_WORK(&qdev->mpi_core_to_log, ql_mpi_core_to_log);\r\ninit_completion(&qdev->ide_completion);\r\nmutex_init(&qdev->mpi_mutex);\r\nif (!cards_found) {\r\ndev_info(&pdev->dev, "%s\n", DRV_STRING);\r\ndev_info(&pdev->dev, "Driver name: %s, Version: %s.\n",\r\nDRV_NAME, DRV_VERSION);\r\n}\r\nreturn 0;\r\nerr_out2:\r\nql_release_all(pdev);\r\nerr_out1:\r\npci_disable_device(pdev);\r\nreturn err;\r\n}\r\nstatic void ql_timer(unsigned long data)\r\n{\r\nstruct ql_adapter *qdev = (struct ql_adapter *)data;\r\nu32 var = 0;\r\nvar = ql_read32(qdev, STS);\r\nif (pci_channel_offline(qdev->pdev)) {\r\nnetif_err(qdev, ifup, qdev->ndev, "EEH STS = 0x%.08x.\n", var);\r\nreturn;\r\n}\r\nmod_timer(&qdev->timer, jiffies + (5*HZ));\r\n}\r\nstatic int qlge_probe(struct pci_dev *pdev,\r\nconst struct pci_device_id *pci_entry)\r\n{\r\nstruct net_device *ndev = NULL;\r\nstruct ql_adapter *qdev = NULL;\r\nstatic int cards_found = 0;\r\nint err = 0;\r\nndev = alloc_etherdev_mq(sizeof(struct ql_adapter),\r\nmin(MAX_CPUS, netif_get_num_default_rss_queues()));\r\nif (!ndev)\r\nreturn -ENOMEM;\r\nerr = ql_init_device(pdev, ndev, cards_found);\r\nif (err < 0) {\r\nfree_netdev(ndev);\r\nreturn err;\r\n}\r\nqdev = netdev_priv(ndev);\r\nSET_NETDEV_DEV(ndev, &pdev->dev);\r\nndev->hw_features = NETIF_F_SG |\r\nNETIF_F_IP_CSUM |\r\nNETIF_F_TSO |\r\nNETIF_F_TSO_ECN |\r\nNETIF_F_HW_VLAN_CTAG_TX |\r\nNETIF_F_HW_VLAN_CTAG_RX |\r\nNETIF_F_HW_VLAN_CTAG_FILTER |\r\nNETIF_F_RXCSUM;\r\nndev->features = ndev->hw_features;\r\nndev->vlan_features = ndev->hw_features;\r\nndev->vlan_features &= ~(NETIF_F_HW_VLAN_CTAG_FILTER |\r\nNETIF_F_HW_VLAN_CTAG_TX |\r\nNETIF_F_HW_VLAN_CTAG_RX);\r\nif (test_bit(QL_DMA64, &qdev->flags))\r\nndev->features |= NETIF_F_HIGHDMA;\r\nndev->tx_queue_len = qdev->tx_ring_size;\r\nndev->irq = pdev->irq;\r\nndev->netdev_ops = &qlge_netdev_ops;\r\nndev->ethtool_ops = &qlge_ethtool_ops;\r\nndev->watchdog_timeo = 10 * HZ;\r\nerr = register_netdev(ndev);\r\nif (err) {\r\ndev_err(&pdev->dev, "net device registration failed.\n");\r\nql_release_all(pdev);\r\npci_disable_device(pdev);\r\nfree_netdev(ndev);\r\nreturn err;\r\n}\r\ninit_timer_deferrable(&qdev->timer);\r\nqdev->timer.data = (unsigned long)qdev;\r\nqdev->timer.function = ql_timer;\r\nqdev->timer.expires = jiffies + (5*HZ);\r\nadd_timer(&qdev->timer);\r\nql_link_off(qdev);\r\nql_display_dev_info(ndev);\r\natomic_set(&qdev->lb_count, 0);\r\ncards_found++;\r\nreturn 0;\r\n}\r\nnetdev_tx_t ql_lb_send(struct sk_buff *skb, struct net_device *ndev)\r\n{\r\nreturn qlge_send(skb, ndev);\r\n}\r\nint ql_clean_lb_rx_ring(struct rx_ring *rx_ring, int budget)\r\n{\r\nreturn ql_clean_inbound_rx_ring(rx_ring, budget);\r\n}\r\nstatic void qlge_remove(struct pci_dev *pdev)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\ndel_timer_sync(&qdev->timer);\r\nql_cancel_all_work_sync(qdev);\r\nunregister_netdev(ndev);\r\nql_release_all(pdev);\r\npci_disable_device(pdev);\r\nfree_netdev(ndev);\r\n}\r\nstatic void ql_eeh_close(struct net_device *ndev)\r\n{\r\nint i;\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nif (netif_carrier_ok(ndev)) {\r\nnetif_carrier_off(ndev);\r\nnetif_stop_queue(ndev);\r\n}\r\ndel_timer_sync(&qdev->timer);\r\nql_cancel_all_work_sync(qdev);\r\nfor (i = 0; i < qdev->rss_ring_count; i++)\r\nnetif_napi_del(&qdev->rx_ring[i].napi);\r\nclear_bit(QL_ADAPTER_UP, &qdev->flags);\r\nql_tx_ring_clean(qdev);\r\nql_free_rx_buffers(qdev);\r\nql_release_adapter_resources(qdev);\r\n}\r\nstatic pci_ers_result_t qlge_io_error_detected(struct pci_dev *pdev,\r\nenum pci_channel_state state)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nswitch (state) {\r\ncase pci_channel_io_normal:\r\nreturn PCI_ERS_RESULT_CAN_RECOVER;\r\ncase pci_channel_io_frozen:\r\nnetif_device_detach(ndev);\r\nif (netif_running(ndev))\r\nql_eeh_close(ndev);\r\npci_disable_device(pdev);\r\nreturn PCI_ERS_RESULT_NEED_RESET;\r\ncase pci_channel_io_perm_failure:\r\ndev_err(&pdev->dev,\r\n"%s: pci_channel_io_perm_failure.\n", __func__);\r\nql_eeh_close(ndev);\r\nset_bit(QL_EEH_FATAL, &qdev->flags);\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\nreturn PCI_ERS_RESULT_NEED_RESET;\r\n}\r\nstatic pci_ers_result_t qlge_io_slot_reset(struct pci_dev *pdev)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\npdev->error_state = pci_channel_io_normal;\r\npci_restore_state(pdev);\r\nif (pci_enable_device(pdev)) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Cannot re-enable PCI device after reset.\n");\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\npci_set_master(pdev);\r\nif (ql_adapter_reset(qdev)) {\r\nnetif_err(qdev, drv, qdev->ndev, "reset FAILED!\n");\r\nset_bit(QL_EEH_FATAL, &qdev->flags);\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n}\r\nstatic void qlge_io_resume(struct pci_dev *pdev)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint err = 0;\r\nif (netif_running(ndev)) {\r\nerr = qlge_open(ndev);\r\nif (err) {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Device initialization failed after reset.\n");\r\nreturn;\r\n}\r\n} else {\r\nnetif_err(qdev, ifup, qdev->ndev,\r\n"Device was not running prior to EEH.\n");\r\n}\r\nmod_timer(&qdev->timer, jiffies + (5*HZ));\r\nnetif_device_attach(ndev);\r\n}\r\nstatic int qlge_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint err;\r\nnetif_device_detach(ndev);\r\ndel_timer_sync(&qdev->timer);\r\nif (netif_running(ndev)) {\r\nerr = ql_adapter_down(qdev);\r\nif (!err)\r\nreturn err;\r\n}\r\nql_wol(qdev);\r\nerr = pci_save_state(pdev);\r\nif (err)\r\nreturn err;\r\npci_disable_device(pdev);\r\npci_set_power_state(pdev, pci_choose_state(pdev, state));\r\nreturn 0;\r\n}\r\nstatic int qlge_resume(struct pci_dev *pdev)\r\n{\r\nstruct net_device *ndev = pci_get_drvdata(pdev);\r\nstruct ql_adapter *qdev = netdev_priv(ndev);\r\nint err;\r\npci_set_power_state(pdev, PCI_D0);\r\npci_restore_state(pdev);\r\nerr = pci_enable_device(pdev);\r\nif (err) {\r\nnetif_err(qdev, ifup, qdev->ndev, "Cannot enable PCI device from suspend\n");\r\nreturn err;\r\n}\r\npci_set_master(pdev);\r\npci_enable_wake(pdev, PCI_D3hot, 0);\r\npci_enable_wake(pdev, PCI_D3cold, 0);\r\nif (netif_running(ndev)) {\r\nerr = ql_adapter_up(qdev);\r\nif (err)\r\nreturn err;\r\n}\r\nmod_timer(&qdev->timer, jiffies + (5*HZ));\r\nnetif_device_attach(ndev);\r\nreturn 0;\r\n}\r\nstatic void qlge_shutdown(struct pci_dev *pdev)\r\n{\r\nqlge_suspend(pdev, PMSG_SUSPEND);\r\n}
