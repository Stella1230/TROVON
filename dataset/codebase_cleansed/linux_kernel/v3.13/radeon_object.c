void radeon_bo_clear_va(struct radeon_bo *bo)\r\n{\r\nstruct radeon_bo_va *bo_va, *tmp;\r\nlist_for_each_entry_safe(bo_va, tmp, &bo->va, bo_list) {\r\nradeon_vm_bo_rmv(bo->rdev, bo_va);\r\n}\r\n}\r\nstatic void radeon_ttm_bo_destroy(struct ttm_buffer_object *tbo)\r\n{\r\nstruct radeon_bo *bo;\r\nbo = container_of(tbo, struct radeon_bo, tbo);\r\nmutex_lock(&bo->rdev->gem.mutex);\r\nlist_del_init(&bo->list);\r\nmutex_unlock(&bo->rdev->gem.mutex);\r\nradeon_bo_clear_surface_reg(bo);\r\nradeon_bo_clear_va(bo);\r\ndrm_gem_object_release(&bo->gem_base);\r\nkfree(bo);\r\n}\r\nbool radeon_ttm_bo_is_radeon_bo(struct ttm_buffer_object *bo)\r\n{\r\nif (bo->destroy == &radeon_ttm_bo_destroy)\r\nreturn true;\r\nreturn false;\r\n}\r\nvoid radeon_ttm_placement_from_domain(struct radeon_bo *rbo, u32 domain)\r\n{\r\nu32 c = 0;\r\nrbo->placement.fpfn = 0;\r\nrbo->placement.lpfn = 0;\r\nrbo->placement.placement = rbo->placements;\r\nrbo->placement.busy_placement = rbo->placements;\r\nif (domain & RADEON_GEM_DOMAIN_VRAM)\r\nrbo->placements[c++] = TTM_PL_FLAG_WC | TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_VRAM;\r\nif (domain & RADEON_GEM_DOMAIN_GTT) {\r\nif (rbo->rdev->flags & RADEON_IS_AGP) {\r\nrbo->placements[c++] = TTM_PL_FLAG_WC | TTM_PL_FLAG_TT;\r\n} else {\r\nrbo->placements[c++] = TTM_PL_FLAG_CACHED | TTM_PL_FLAG_TT;\r\n}\r\n}\r\nif (domain & RADEON_GEM_DOMAIN_CPU) {\r\nif (rbo->rdev->flags & RADEON_IS_AGP) {\r\nrbo->placements[c++] = TTM_PL_FLAG_WC | TTM_PL_FLAG_SYSTEM;\r\n} else {\r\nrbo->placements[c++] = TTM_PL_FLAG_CACHED | TTM_PL_FLAG_SYSTEM;\r\n}\r\n}\r\nif (!c)\r\nrbo->placements[c++] = TTM_PL_MASK_CACHING | TTM_PL_FLAG_SYSTEM;\r\nrbo->placement.num_placement = c;\r\nrbo->placement.num_busy_placement = c;\r\n}\r\nint radeon_bo_create(struct radeon_device *rdev,\r\nunsigned long size, int byte_align, bool kernel, u32 domain,\r\nstruct sg_table *sg, struct radeon_bo **bo_ptr)\r\n{\r\nstruct radeon_bo *bo;\r\nenum ttm_bo_type type;\r\nunsigned long page_align = roundup(byte_align, PAGE_SIZE) >> PAGE_SHIFT;\r\nsize_t acc_size;\r\nint r;\r\nsize = ALIGN(size, PAGE_SIZE);\r\nrdev->mman.bdev.dev_mapping = rdev->ddev->dev_mapping;\r\nif (kernel) {\r\ntype = ttm_bo_type_kernel;\r\n} else if (sg) {\r\ntype = ttm_bo_type_sg;\r\n} else {\r\ntype = ttm_bo_type_device;\r\n}\r\n*bo_ptr = NULL;\r\nacc_size = ttm_bo_dma_acc_size(&rdev->mman.bdev, size,\r\nsizeof(struct radeon_bo));\r\nbo = kzalloc(sizeof(struct radeon_bo), GFP_KERNEL);\r\nif (bo == NULL)\r\nreturn -ENOMEM;\r\nr = drm_gem_object_init(rdev->ddev, &bo->gem_base, size);\r\nif (unlikely(r)) {\r\nkfree(bo);\r\nreturn r;\r\n}\r\nbo->rdev = rdev;\r\nbo->surface_reg = -1;\r\nINIT_LIST_HEAD(&bo->list);\r\nINIT_LIST_HEAD(&bo->va);\r\nradeon_ttm_placement_from_domain(bo, domain);\r\ndown_read(&rdev->pm.mclk_lock);\r\nr = ttm_bo_init(&rdev->mman.bdev, &bo->tbo, size, type,\r\n&bo->placement, page_align, !kernel, NULL,\r\nacc_size, sg, &radeon_ttm_bo_destroy);\r\nup_read(&rdev->pm.mclk_lock);\r\nif (unlikely(r != 0)) {\r\nreturn r;\r\n}\r\n*bo_ptr = bo;\r\ntrace_radeon_bo_create(bo);\r\nreturn 0;\r\n}\r\nint radeon_bo_kmap(struct radeon_bo *bo, void **ptr)\r\n{\r\nbool is_iomem;\r\nint r;\r\nif (bo->kptr) {\r\nif (ptr) {\r\n*ptr = bo->kptr;\r\n}\r\nreturn 0;\r\n}\r\nr = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages, &bo->kmap);\r\nif (r) {\r\nreturn r;\r\n}\r\nbo->kptr = ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);\r\nif (ptr) {\r\n*ptr = bo->kptr;\r\n}\r\nradeon_bo_check_tiling(bo, 0, 0);\r\nreturn 0;\r\n}\r\nvoid radeon_bo_kunmap(struct radeon_bo *bo)\r\n{\r\nif (bo->kptr == NULL)\r\nreturn;\r\nbo->kptr = NULL;\r\nradeon_bo_check_tiling(bo, 0, 0);\r\nttm_bo_kunmap(&bo->kmap);\r\n}\r\nvoid radeon_bo_unref(struct radeon_bo **bo)\r\n{\r\nstruct ttm_buffer_object *tbo;\r\nstruct radeon_device *rdev;\r\nif ((*bo) == NULL)\r\nreturn;\r\nrdev = (*bo)->rdev;\r\ntbo = &((*bo)->tbo);\r\ndown_read(&rdev->pm.mclk_lock);\r\nttm_bo_unref(&tbo);\r\nup_read(&rdev->pm.mclk_lock);\r\nif (tbo == NULL)\r\n*bo = NULL;\r\n}\r\nint radeon_bo_pin_restricted(struct radeon_bo *bo, u32 domain, u64 max_offset,\r\nu64 *gpu_addr)\r\n{\r\nint r, i;\r\nif (bo->pin_count) {\r\nbo->pin_count++;\r\nif (gpu_addr)\r\n*gpu_addr = radeon_bo_gpu_offset(bo);\r\nif (max_offset != 0) {\r\nu64 domain_start;\r\nif (domain == RADEON_GEM_DOMAIN_VRAM)\r\ndomain_start = bo->rdev->mc.vram_start;\r\nelse\r\ndomain_start = bo->rdev->mc.gtt_start;\r\nWARN_ON_ONCE(max_offset <\r\n(radeon_bo_gpu_offset(bo) - domain_start));\r\n}\r\nreturn 0;\r\n}\r\nradeon_ttm_placement_from_domain(bo, domain);\r\nif (domain == RADEON_GEM_DOMAIN_VRAM) {\r\nbo->placement.lpfn = bo->rdev->mc.visible_vram_size >> PAGE_SHIFT;\r\n}\r\nif (max_offset) {\r\nu64 lpfn = max_offset >> PAGE_SHIFT;\r\nif (!bo->placement.lpfn)\r\nbo->placement.lpfn = bo->rdev->mc.gtt_size >> PAGE_SHIFT;\r\nif (lpfn < bo->placement.lpfn)\r\nbo->placement.lpfn = lpfn;\r\n}\r\nfor (i = 0; i < bo->placement.num_placement; i++)\r\nbo->placements[i] |= TTM_PL_FLAG_NO_EVICT;\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (likely(r == 0)) {\r\nbo->pin_count = 1;\r\nif (gpu_addr != NULL)\r\n*gpu_addr = radeon_bo_gpu_offset(bo);\r\n}\r\nif (unlikely(r != 0))\r\ndev_err(bo->rdev->dev, "%p pin failed\n", bo);\r\nreturn r;\r\n}\r\nint radeon_bo_pin(struct radeon_bo *bo, u32 domain, u64 *gpu_addr)\r\n{\r\nreturn radeon_bo_pin_restricted(bo, domain, 0, gpu_addr);\r\n}\r\nint radeon_bo_unpin(struct radeon_bo *bo)\r\n{\r\nint r, i;\r\nif (!bo->pin_count) {\r\ndev_warn(bo->rdev->dev, "%p unpin not necessary\n", bo);\r\nreturn 0;\r\n}\r\nbo->pin_count--;\r\nif (bo->pin_count)\r\nreturn 0;\r\nfor (i = 0; i < bo->placement.num_placement; i++)\r\nbo->placements[i] &= ~TTM_PL_FLAG_NO_EVICT;\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (unlikely(r != 0))\r\ndev_err(bo->rdev->dev, "%p validate failed for unpin\n", bo);\r\nreturn r;\r\n}\r\nint radeon_bo_evict_vram(struct radeon_device *rdev)\r\n{\r\nif (0 && (rdev->flags & RADEON_IS_IGP)) {\r\nif (rdev->mc.igp_sideport_enabled == false)\r\nreturn 0;\r\n}\r\nreturn ttm_bo_evict_mm(&rdev->mman.bdev, TTM_PL_VRAM);\r\n}\r\nvoid radeon_bo_force_delete(struct radeon_device *rdev)\r\n{\r\nstruct radeon_bo *bo, *n;\r\nif (list_empty(&rdev->gem.objects)) {\r\nreturn;\r\n}\r\ndev_err(rdev->dev, "Userspace still has active objects !\n");\r\nlist_for_each_entry_safe(bo, n, &rdev->gem.objects, list) {\r\nmutex_lock(&rdev->ddev->struct_mutex);\r\ndev_err(rdev->dev, "%p %p %lu %lu force free\n",\r\n&bo->gem_base, bo, (unsigned long)bo->gem_base.size,\r\n*((unsigned long *)&bo->gem_base.refcount));\r\nmutex_lock(&bo->rdev->gem.mutex);\r\nlist_del_init(&bo->list);\r\nmutex_unlock(&bo->rdev->gem.mutex);\r\ndrm_gem_object_unreference(&bo->gem_base);\r\nmutex_unlock(&rdev->ddev->struct_mutex);\r\n}\r\n}\r\nint radeon_bo_init(struct radeon_device *rdev)\r\n{\r\nif (!rdev->fastfb_working) {\r\nrdev->mc.vram_mtrr = arch_phys_wc_add(rdev->mc.aper_base,\r\nrdev->mc.aper_size);\r\n}\r\nDRM_INFO("Detected VRAM RAM=%lluM, BAR=%lluM\n",\r\nrdev->mc.mc_vram_size >> 20,\r\n(unsigned long long)rdev->mc.aper_size >> 20);\r\nDRM_INFO("RAM width %dbits %cDR\n",\r\nrdev->mc.vram_width, rdev->mc.vram_is_ddr ? 'D' : 'S');\r\nreturn radeon_ttm_init(rdev);\r\n}\r\nvoid radeon_bo_fini(struct radeon_device *rdev)\r\n{\r\nradeon_ttm_fini(rdev);\r\narch_phys_wc_del(rdev->mc.vram_mtrr);\r\n}\r\nvoid radeon_bo_list_add_object(struct radeon_bo_list *lobj,\r\nstruct list_head *head)\r\n{\r\nif (lobj->written) {\r\nlist_add(&lobj->tv.head, head);\r\n} else {\r\nlist_add_tail(&lobj->tv.head, head);\r\n}\r\n}\r\nint radeon_bo_list_validate(struct ww_acquire_ctx *ticket,\r\nstruct list_head *head, int ring)\r\n{\r\nstruct radeon_bo_list *lobj;\r\nstruct radeon_bo *bo;\r\nu32 domain;\r\nint r;\r\nr = ttm_eu_reserve_buffers(ticket, head);\r\nif (unlikely(r != 0)) {\r\nreturn r;\r\n}\r\nlist_for_each_entry(lobj, head, tv.head) {\r\nbo = lobj->bo;\r\nif (!bo->pin_count) {\r\ndomain = lobj->domain;\r\nretry:\r\nradeon_ttm_placement_from_domain(bo, domain);\r\nif (ring == R600_RING_TYPE_UVD_INDEX)\r\nradeon_uvd_force_into_uvd_segment(bo);\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement,\r\ntrue, false);\r\nif (unlikely(r)) {\r\nif (r != -ERESTARTSYS && domain != lobj->alt_domain) {\r\ndomain = lobj->alt_domain;\r\ngoto retry;\r\n}\r\nttm_eu_backoff_reservation(ticket, head);\r\nreturn r;\r\n}\r\n}\r\nlobj->gpu_offset = radeon_bo_gpu_offset(bo);\r\nlobj->tiling_flags = bo->tiling_flags;\r\n}\r\nreturn 0;\r\n}\r\nint radeon_bo_fbdev_mmap(struct radeon_bo *bo,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn ttm_fbdev_mmap(vma, &bo->tbo);\r\n}\r\nint radeon_bo_get_surface_reg(struct radeon_bo *bo)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nstruct radeon_surface_reg *reg;\r\nstruct radeon_bo *old_object;\r\nint steal;\r\nint i;\r\nlockdep_assert_held(&bo->tbo.resv->lock.base);\r\nif (!bo->tiling_flags)\r\nreturn 0;\r\nif (bo->surface_reg >= 0) {\r\nreg = &rdev->surface_regs[bo->surface_reg];\r\ni = bo->surface_reg;\r\ngoto out;\r\n}\r\nsteal = -1;\r\nfor (i = 0; i < RADEON_GEM_MAX_SURFACES; i++) {\r\nreg = &rdev->surface_regs[i];\r\nif (!reg->bo)\r\nbreak;\r\nold_object = reg->bo;\r\nif (old_object->pin_count == 0)\r\nsteal = i;\r\n}\r\nif (i == RADEON_GEM_MAX_SURFACES) {\r\nif (steal == -1)\r\nreturn -ENOMEM;\r\nreg = &rdev->surface_regs[steal];\r\nold_object = reg->bo;\r\nDRM_DEBUG("stealing surface reg %d from %p\n", steal, old_object);\r\nttm_bo_unmap_virtual(&old_object->tbo);\r\nold_object->surface_reg = -1;\r\ni = steal;\r\n}\r\nbo->surface_reg = i;\r\nreg->bo = bo;\r\nout:\r\nradeon_set_surface_reg(rdev, i, bo->tiling_flags, bo->pitch,\r\nbo->tbo.mem.start << PAGE_SHIFT,\r\nbo->tbo.num_pages << PAGE_SHIFT);\r\nreturn 0;\r\n}\r\nstatic void radeon_bo_clear_surface_reg(struct radeon_bo *bo)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nstruct radeon_surface_reg *reg;\r\nif (bo->surface_reg == -1)\r\nreturn;\r\nreg = &rdev->surface_regs[bo->surface_reg];\r\nradeon_clear_surface_reg(rdev, bo->surface_reg);\r\nreg->bo = NULL;\r\nbo->surface_reg = -1;\r\n}\r\nint radeon_bo_set_tiling_flags(struct radeon_bo *bo,\r\nuint32_t tiling_flags, uint32_t pitch)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nint r;\r\nif (rdev->family >= CHIP_CEDAR) {\r\nunsigned bankw, bankh, mtaspect, tilesplit, stilesplit;\r\nbankw = (tiling_flags >> RADEON_TILING_EG_BANKW_SHIFT) & RADEON_TILING_EG_BANKW_MASK;\r\nbankh = (tiling_flags >> RADEON_TILING_EG_BANKH_SHIFT) & RADEON_TILING_EG_BANKH_MASK;\r\nmtaspect = (tiling_flags >> RADEON_TILING_EG_MACRO_TILE_ASPECT_SHIFT) & RADEON_TILING_EG_MACRO_TILE_ASPECT_MASK;\r\ntilesplit = (tiling_flags >> RADEON_TILING_EG_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_TILE_SPLIT_MASK;\r\nstilesplit = (tiling_flags >> RADEON_TILING_EG_STENCIL_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_STENCIL_TILE_SPLIT_MASK;\r\nswitch (bankw) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nswitch (bankh) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nswitch (mtaspect) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (tilesplit > 6) {\r\nreturn -EINVAL;\r\n}\r\nif (stilesplit > 6) {\r\nreturn -EINVAL;\r\n}\r\n}\r\nr = radeon_bo_reserve(bo, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nbo->tiling_flags = tiling_flags;\r\nbo->pitch = pitch;\r\nradeon_bo_unreserve(bo);\r\nreturn 0;\r\n}\r\nvoid radeon_bo_get_tiling_flags(struct radeon_bo *bo,\r\nuint32_t *tiling_flags,\r\nuint32_t *pitch)\r\n{\r\nlockdep_assert_held(&bo->tbo.resv->lock.base);\r\nif (tiling_flags)\r\n*tiling_flags = bo->tiling_flags;\r\nif (pitch)\r\n*pitch = bo->pitch;\r\n}\r\nint radeon_bo_check_tiling(struct radeon_bo *bo, bool has_moved,\r\nbool force_drop)\r\n{\r\nif (!force_drop)\r\nlockdep_assert_held(&bo->tbo.resv->lock.base);\r\nif (!(bo->tiling_flags & RADEON_TILING_SURFACE))\r\nreturn 0;\r\nif (force_drop) {\r\nradeon_bo_clear_surface_reg(bo);\r\nreturn 0;\r\n}\r\nif (bo->tbo.mem.mem_type != TTM_PL_VRAM) {\r\nif (!has_moved)\r\nreturn 0;\r\nif (bo->surface_reg >= 0)\r\nradeon_bo_clear_surface_reg(bo);\r\nreturn 0;\r\n}\r\nif ((bo->surface_reg >= 0) && !has_moved)\r\nreturn 0;\r\nreturn radeon_bo_get_surface_reg(bo);\r\n}\r\nvoid radeon_bo_move_notify(struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct radeon_bo *rbo;\r\nif (!radeon_ttm_bo_is_radeon_bo(bo))\r\nreturn;\r\nrbo = container_of(bo, struct radeon_bo, tbo);\r\nradeon_bo_check_tiling(rbo, 0, 1);\r\nradeon_vm_bo_invalidate(rbo->rdev, rbo);\r\n}\r\nint radeon_bo_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nstruct radeon_device *rdev;\r\nstruct radeon_bo *rbo;\r\nunsigned long offset, size;\r\nint r;\r\nif (!radeon_ttm_bo_is_radeon_bo(bo))\r\nreturn 0;\r\nrbo = container_of(bo, struct radeon_bo, tbo);\r\nradeon_bo_check_tiling(rbo, 0, 0);\r\nrdev = rbo->rdev;\r\nif (bo->mem.mem_type == TTM_PL_VRAM) {\r\nsize = bo->mem.num_pages << PAGE_SHIFT;\r\noffset = bo->mem.start << PAGE_SHIFT;\r\nif ((offset + size) > rdev->mc.visible_vram_size) {\r\nradeon_ttm_placement_from_domain(rbo, RADEON_GEM_DOMAIN_VRAM);\r\nrbo->placement.lpfn = rdev->mc.visible_vram_size >> PAGE_SHIFT;\r\nr = ttm_bo_validate(bo, &rbo->placement, false, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\noffset = bo->mem.start << PAGE_SHIFT;\r\nif ((offset + size) > rdev->mc.visible_vram_size)\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint radeon_bo_wait(struct radeon_bo *bo, u32 *mem_type, bool no_wait)\r\n{\r\nint r;\r\nr = ttm_bo_reserve(&bo->tbo, true, no_wait, false, 0);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nspin_lock(&bo->tbo.bdev->fence_lock);\r\nif (mem_type)\r\n*mem_type = bo->tbo.mem.mem_type;\r\nif (bo->tbo.sync_obj)\r\nr = ttm_bo_wait(&bo->tbo, true, true, no_wait);\r\nspin_unlock(&bo->tbo.bdev->fence_lock);\r\nttm_bo_unreserve(&bo->tbo);\r\nreturn r;\r\n}
