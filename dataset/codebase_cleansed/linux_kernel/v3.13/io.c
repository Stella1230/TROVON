static void bch_bi_idx_hack_endio(struct bio *bio, int error)\r\n{\r\nstruct bio *p = bio->bi_private;\r\nbio_endio(p, error);\r\nbio_put(bio);\r\n}\r\nstatic void bch_generic_make_request_hack(struct bio *bio)\r\n{\r\nif (bio->bi_idx) {\r\nstruct bio *clone = bio_alloc(GFP_NOIO, bio_segments(bio));\r\nmemcpy(clone->bi_io_vec,\r\nbio_iovec(bio),\r\nbio_segments(bio) * sizeof(struct bio_vec));\r\nclone->bi_sector = bio->bi_sector;\r\nclone->bi_bdev = bio->bi_bdev;\r\nclone->bi_rw = bio->bi_rw;\r\nclone->bi_vcnt = bio_segments(bio);\r\nclone->bi_size = bio->bi_size;\r\nclone->bi_private = bio;\r\nclone->bi_end_io = bch_bi_idx_hack_endio;\r\nbio = clone;\r\n}\r\nbio->bi_max_vecs = bio->bi_vcnt;\r\ngeneric_make_request(bio);\r\n}\r\nstruct bio *bch_bio_split(struct bio *bio, int sectors,\r\ngfp_t gfp, struct bio_set *bs)\r\n{\r\nunsigned idx = bio->bi_idx, vcnt = 0, nbytes = sectors << 9;\r\nstruct bio_vec *bv;\r\nstruct bio *ret = NULL;\r\nBUG_ON(sectors <= 0);\r\nif (sectors >= bio_sectors(bio))\r\nreturn bio;\r\nif (bio->bi_rw & REQ_DISCARD) {\r\nret = bio_alloc_bioset(gfp, 1, bs);\r\nif (!ret)\r\nreturn NULL;\r\nidx = 0;\r\ngoto out;\r\n}\r\nbio_for_each_segment(bv, bio, idx) {\r\nvcnt = idx - bio->bi_idx;\r\nif (!nbytes) {\r\nret = bio_alloc_bioset(gfp, vcnt, bs);\r\nif (!ret)\r\nreturn NULL;\r\nmemcpy(ret->bi_io_vec, bio_iovec(bio),\r\nsizeof(struct bio_vec) * vcnt);\r\nbreak;\r\n} else if (nbytes < bv->bv_len) {\r\nret = bio_alloc_bioset(gfp, ++vcnt, bs);\r\nif (!ret)\r\nreturn NULL;\r\nmemcpy(ret->bi_io_vec, bio_iovec(bio),\r\nsizeof(struct bio_vec) * vcnt);\r\nret->bi_io_vec[vcnt - 1].bv_len = nbytes;\r\nbv->bv_offset += nbytes;\r\nbv->bv_len -= nbytes;\r\nbreak;\r\n}\r\nnbytes -= bv->bv_len;\r\n}\r\nout:\r\nret->bi_bdev = bio->bi_bdev;\r\nret->bi_sector = bio->bi_sector;\r\nret->bi_size = sectors << 9;\r\nret->bi_rw = bio->bi_rw;\r\nret->bi_vcnt = vcnt;\r\nret->bi_max_vecs = vcnt;\r\nbio->bi_sector += sectors;\r\nbio->bi_size -= sectors << 9;\r\nbio->bi_idx = idx;\r\nif (bio_integrity(bio)) {\r\nif (bio_integrity_clone(ret, bio, gfp)) {\r\nbio_put(ret);\r\nreturn NULL;\r\n}\r\nbio_integrity_trim(ret, 0, bio_sectors(ret));\r\nbio_integrity_trim(bio, bio_sectors(ret), bio_sectors(bio));\r\n}\r\nreturn ret;\r\n}\r\nstatic unsigned bch_bio_max_sectors(struct bio *bio)\r\n{\r\nunsigned ret = bio_sectors(bio);\r\nstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\r\nunsigned max_segments = min_t(unsigned, BIO_MAX_PAGES,\r\nqueue_max_segments(q));\r\nif (bio->bi_rw & REQ_DISCARD)\r\nreturn min(ret, q->limits.max_discard_sectors);\r\nif (bio_segments(bio) > max_segments ||\r\nq->merge_bvec_fn) {\r\nstruct bio_vec *bv;\r\nint i, seg = 0;\r\nret = 0;\r\nbio_for_each_segment(bv, bio, i) {\r\nstruct bvec_merge_data bvm = {\r\n.bi_bdev = bio->bi_bdev,\r\n.bi_sector = bio->bi_sector,\r\n.bi_size = ret << 9,\r\n.bi_rw = bio->bi_rw,\r\n};\r\nif (seg == max_segments)\r\nbreak;\r\nif (q->merge_bvec_fn &&\r\nq->merge_bvec_fn(q, &bvm, bv) < (int) bv->bv_len)\r\nbreak;\r\nseg++;\r\nret += bv->bv_len >> 9;\r\n}\r\n}\r\nret = min(ret, queue_max_sectors(q));\r\nWARN_ON(!ret);\r\nret = max_t(int, ret, bio_iovec(bio)->bv_len >> 9);\r\nreturn ret;\r\n}\r\nstatic void bch_bio_submit_split_done(struct closure *cl)\r\n{\r\nstruct bio_split_hook *s = container_of(cl, struct bio_split_hook, cl);\r\ns->bio->bi_end_io = s->bi_end_io;\r\ns->bio->bi_private = s->bi_private;\r\nbio_endio(s->bio, 0);\r\nclosure_debug_destroy(&s->cl);\r\nmempool_free(s, s->p->bio_split_hook);\r\n}\r\nstatic void bch_bio_submit_split_endio(struct bio *bio, int error)\r\n{\r\nstruct closure *cl = bio->bi_private;\r\nstruct bio_split_hook *s = container_of(cl, struct bio_split_hook, cl);\r\nif (error)\r\nclear_bit(BIO_UPTODATE, &s->bio->bi_flags);\r\nbio_put(bio);\r\nclosure_put(cl);\r\n}\r\nvoid bch_generic_make_request(struct bio *bio, struct bio_split_pool *p)\r\n{\r\nstruct bio_split_hook *s;\r\nstruct bio *n;\r\nif (!bio_has_data(bio) && !(bio->bi_rw & REQ_DISCARD))\r\ngoto submit;\r\nif (bio_sectors(bio) <= bch_bio_max_sectors(bio))\r\ngoto submit;\r\ns = mempool_alloc(p->bio_split_hook, GFP_NOIO);\r\nclosure_init(&s->cl, NULL);\r\ns->bio = bio;\r\ns->p = p;\r\ns->bi_end_io = bio->bi_end_io;\r\ns->bi_private = bio->bi_private;\r\nbio_get(bio);\r\ndo {\r\nn = bch_bio_split(bio, bch_bio_max_sectors(bio),\r\nGFP_NOIO, s->p->bio_split);\r\nn->bi_end_io = bch_bio_submit_split_endio;\r\nn->bi_private = &s->cl;\r\nclosure_get(&s->cl);\r\nbch_generic_make_request_hack(n);\r\n} while (n != bio);\r\ncontinue_at(&s->cl, bch_bio_submit_split_done, NULL);\r\nsubmit:\r\nbch_generic_make_request_hack(bio);\r\n}\r\nvoid bch_bbio_free(struct bio *bio, struct cache_set *c)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nmempool_free(b, c->bio_meta);\r\n}\r\nstruct bio *bch_bbio_alloc(struct cache_set *c)\r\n{\r\nstruct bbio *b = mempool_alloc(c->bio_meta, GFP_NOIO);\r\nstruct bio *bio = &b->bio;\r\nbio_init(bio);\r\nbio->bi_flags |= BIO_POOL_NONE << BIO_POOL_OFFSET;\r\nbio->bi_max_vecs = bucket_pages(c);\r\nbio->bi_io_vec = bio->bi_inline_vecs;\r\nreturn bio;\r\n}\r\nvoid __bch_submit_bbio(struct bio *bio, struct cache_set *c)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nbio->bi_sector = PTR_OFFSET(&b->key, 0);\r\nbio->bi_bdev = PTR_CACHE(c, &b->key, 0)->bdev;\r\nb->submit_time_us = local_clock_us();\r\nclosure_bio_submit(bio, bio->bi_private, PTR_CACHE(c, &b->key, 0));\r\n}\r\nvoid bch_submit_bbio(struct bio *bio, struct cache_set *c,\r\nstruct bkey *k, unsigned ptr)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nbch_bkey_copy_single_ptr(&b->key, k, ptr);\r\n__bch_submit_bbio(bio, c);\r\n}\r\nvoid bch_count_io_errors(struct cache *ca, int error, const char *m)\r\n{\r\nif (ca->set->error_decay) {\r\nunsigned count = atomic_inc_return(&ca->io_count);\r\nwhile (count > ca->set->error_decay) {\r\nunsigned errors;\r\nunsigned old = count;\r\nunsigned new = count - ca->set->error_decay;\r\ncount = atomic_cmpxchg(&ca->io_count, old, new);\r\nif (count == old) {\r\ncount = new;\r\nerrors = atomic_read(&ca->io_errors);\r\ndo {\r\nold = errors;\r\nnew = ((uint64_t) errors * 127) / 128;\r\nerrors = atomic_cmpxchg(&ca->io_errors,\r\nold, new);\r\n} while (old != errors);\r\n}\r\n}\r\n}\r\nif (error) {\r\nchar buf[BDEVNAME_SIZE];\r\nunsigned errors = atomic_add_return(1 << IO_ERROR_SHIFT,\r\n&ca->io_errors);\r\nerrors >>= IO_ERROR_SHIFT;\r\nif (errors < ca->set->error_limit)\r\npr_err("%s: IO error on %s, recovering",\r\nbdevname(ca->bdev, buf), m);\r\nelse\r\nbch_cache_set_error(ca->set,\r\n"%s: too many IO errors %s",\r\nbdevname(ca->bdev, buf), m);\r\n}\r\n}\r\nvoid bch_bbio_count_io_errors(struct cache_set *c, struct bio *bio,\r\nint error, const char *m)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nstruct cache *ca = PTR_CACHE(c, &b->key, 0);\r\nunsigned threshold = bio->bi_rw & REQ_WRITE\r\n? c->congested_write_threshold_us\r\n: c->congested_read_threshold_us;\r\nif (threshold) {\r\nunsigned t = local_clock_us();\r\nint us = t - b->submit_time_us;\r\nint congested = atomic_read(&c->congested);\r\nif (us > (int) threshold) {\r\nint ms = us / 1024;\r\nc->congested_last_us = t;\r\nms = min(ms, CONGESTED_MAX + congested);\r\natomic_sub(ms, &c->congested);\r\n} else if (congested < 0)\r\natomic_inc(&c->congested);\r\n}\r\nbch_count_io_errors(ca, error, m);\r\n}\r\nvoid bch_bbio_endio(struct cache_set *c, struct bio *bio,\r\nint error, const char *m)\r\n{\r\nstruct closure *cl = bio->bi_private;\r\nbch_bbio_count_io_errors(c, bio, error, m);\r\nbio_put(bio);\r\nclosure_put(cl);\r\n}
