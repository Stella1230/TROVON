int ore_verify_layout(unsigned total_comps, struct ore_layout *layout)\r\n{\r\nu64 stripe_length;\r\nswitch (layout->raid_algorithm) {\r\ncase PNFS_OSD_RAID_0:\r\nlayout->parity = 0;\r\nbreak;\r\ncase PNFS_OSD_RAID_5:\r\nlayout->parity = 1;\r\nbreak;\r\ncase PNFS_OSD_RAID_PQ:\r\ncase PNFS_OSD_RAID_4:\r\ndefault:\r\nORE_ERR("Only RAID_0/5 for now\n");\r\nreturn -EINVAL;\r\n}\r\nif (0 != (layout->stripe_unit & ~PAGE_MASK)) {\r\nORE_ERR("Stripe Unit(0x%llx)"\r\n" must be Multples of PAGE_SIZE(0x%lx)\n",\r\n_LLU(layout->stripe_unit), PAGE_SIZE);\r\nreturn -EINVAL;\r\n}\r\nif (layout->group_width) {\r\nif (!layout->group_depth) {\r\nORE_ERR("group_depth == 0 && group_width != 0\n");\r\nreturn -EINVAL;\r\n}\r\nif (total_comps < (layout->group_width * layout->mirrors_p1)) {\r\nORE_ERR("Data Map wrong, "\r\n"numdevs=%d < group_width=%d * mirrors=%d\n",\r\ntotal_comps, layout->group_width,\r\nlayout->mirrors_p1);\r\nreturn -EINVAL;\r\n}\r\nlayout->group_count = total_comps / layout->mirrors_p1 /\r\nlayout->group_width;\r\n} else {\r\nif (layout->group_depth) {\r\nprintk(KERN_NOTICE "Warning: group_depth ignored "\r\n"group_width == 0 && group_depth == %lld\n",\r\n_LLU(layout->group_depth));\r\n}\r\nlayout->group_width = total_comps / layout->mirrors_p1;\r\nlayout->group_depth = -1;\r\nlayout->group_count = 1;\r\n}\r\nstripe_length = (u64)layout->group_width * layout->stripe_unit;\r\nif (stripe_length >= (1ULL << 32)) {\r\nORE_ERR("Stripe_length(0x%llx) >= 32bit is not supported\n",\r\n_LLU(stripe_length));\r\nreturn -EINVAL;\r\n}\r\nlayout->max_io_length =\r\n(BIO_MAX_PAGES_KMALLOC * PAGE_SIZE - layout->stripe_unit) *\r\nlayout->group_width;\r\nif (layout->parity) {\r\nunsigned stripe_length =\r\n(layout->group_width - layout->parity) *\r\nlayout->stripe_unit;\r\nlayout->max_io_length /= stripe_length;\r\nlayout->max_io_length *= stripe_length;\r\n}\r\nreturn 0;\r\n}\r\nstatic u8 *_ios_cred(struct ore_io_state *ios, unsigned index)\r\n{\r\nreturn ios->oc->comps[index & ios->oc->single_comp].cred;\r\n}\r\nstatic struct osd_obj_id *_ios_obj(struct ore_io_state *ios, unsigned index)\r\n{\r\nreturn &ios->oc->comps[index & ios->oc->single_comp].obj;\r\n}\r\nstatic struct osd_dev *_ios_od(struct ore_io_state *ios, unsigned index)\r\n{\r\nORE_DBGMSG2("oc->first_dev=%d oc->numdevs=%d i=%d oc->ods=%p\n",\r\nios->oc->first_dev, ios->oc->numdevs, index,\r\nios->oc->ods);\r\nreturn ore_comp_dev(ios->oc, index);\r\n}\r\nint _ore_get_io_state(struct ore_layout *layout,\r\nstruct ore_components *oc, unsigned numdevs,\r\nunsigned sgs_per_dev, unsigned num_par_pages,\r\nstruct ore_io_state **pios)\r\n{\r\nstruct ore_io_state *ios;\r\nstruct page **pages;\r\nstruct osd_sg_entry *sgilist;\r\nstruct __alloc_all_io_state {\r\nstruct ore_io_state ios;\r\nstruct ore_per_dev_state per_dev[numdevs];\r\nunion {\r\nstruct osd_sg_entry sglist[sgs_per_dev * numdevs];\r\nstruct page *pages[num_par_pages];\r\n};\r\n} *_aios;\r\nif (likely(sizeof(*_aios) <= PAGE_SIZE)) {\r\n_aios = kzalloc(sizeof(*_aios), GFP_KERNEL);\r\nif (unlikely(!_aios)) {\r\nORE_DBGMSG("Failed kzalloc bytes=%zd\n",\r\nsizeof(*_aios));\r\n*pios = NULL;\r\nreturn -ENOMEM;\r\n}\r\npages = num_par_pages ? _aios->pages : NULL;\r\nsgilist = sgs_per_dev ? _aios->sglist : NULL;\r\nios = &_aios->ios;\r\n} else {\r\nstruct __alloc_small_io_state {\r\nstruct ore_io_state ios;\r\nstruct ore_per_dev_state per_dev[numdevs];\r\n} *_aio_small;\r\nunion __extra_part {\r\nstruct osd_sg_entry sglist[sgs_per_dev * numdevs];\r\nstruct page *pages[num_par_pages];\r\n} *extra_part;\r\n_aio_small = kzalloc(sizeof(*_aio_small), GFP_KERNEL);\r\nif (unlikely(!_aio_small)) {\r\nORE_DBGMSG("Failed alloc first part bytes=%zd\n",\r\nsizeof(*_aio_small));\r\n*pios = NULL;\r\nreturn -ENOMEM;\r\n}\r\nextra_part = kzalloc(sizeof(*extra_part), GFP_KERNEL);\r\nif (unlikely(!extra_part)) {\r\nORE_DBGMSG("Failed alloc second part bytes=%zd\n",\r\nsizeof(*extra_part));\r\nkfree(_aio_small);\r\n*pios = NULL;\r\nreturn -ENOMEM;\r\n}\r\npages = num_par_pages ? extra_part->pages : NULL;\r\nsgilist = sgs_per_dev ? extra_part->sglist : NULL;\r\nios = &_aio_small->ios;\r\nios->extra_part_alloc = true;\r\n}\r\nif (pages) {\r\nios->parity_pages = pages;\r\nios->max_par_pages = num_par_pages;\r\n}\r\nif (sgilist) {\r\nunsigned d;\r\nfor (d = 0; d < numdevs; ++d) {\r\nios->per_dev[d].sglist = sgilist;\r\nsgilist += sgs_per_dev;\r\n}\r\nios->sgs_per_dev = sgs_per_dev;\r\n}\r\nios->layout = layout;\r\nios->oc = oc;\r\n*pios = ios;\r\nreturn 0;\r\n}\r\nint ore_get_rw_state(struct ore_layout *layout, struct ore_components *oc,\r\nbool is_reading, u64 offset, u64 length,\r\nstruct ore_io_state **pios)\r\n{\r\nstruct ore_io_state *ios;\r\nunsigned numdevs = layout->group_width * layout->mirrors_p1;\r\nunsigned sgs_per_dev = 0, max_par_pages = 0;\r\nint ret;\r\nif (layout->parity && length) {\r\nunsigned data_devs = layout->group_width - layout->parity;\r\nunsigned stripe_size = layout->stripe_unit * data_devs;\r\nunsigned pages_in_unit = layout->stripe_unit / PAGE_SIZE;\r\nu32 remainder;\r\nu64 num_stripes;\r\nu64 num_raid_units;\r\nnum_stripes = div_u64_rem(length, stripe_size, &remainder);\r\nif (remainder)\r\n++num_stripes;\r\nnum_raid_units = num_stripes * layout->parity;\r\nif (is_reading) {\r\nnum_raid_units += layout->group_width;\r\nsgs_per_dev = div_u64(num_raid_units, data_devs) + 2;\r\n} else {\r\nmax_par_pages = num_raid_units * pages_in_unit *\r\nsizeof(struct page *);\r\n}\r\n}\r\nret = _ore_get_io_state(layout, oc, numdevs, sgs_per_dev, max_par_pages,\r\npios);\r\nif (unlikely(ret))\r\nreturn ret;\r\nios = *pios;\r\nios->reading = is_reading;\r\nios->offset = offset;\r\nif (length) {\r\nore_calc_stripe_info(layout, offset, length, &ios->si);\r\nios->length = ios->si.length;\r\nios->nr_pages = (ios->length + PAGE_SIZE - 1) / PAGE_SIZE;\r\nif (layout->parity)\r\n_ore_post_alloc_raid_stuff(ios);\r\n}\r\nreturn 0;\r\n}\r\nint ore_get_io_state(struct ore_layout *layout, struct ore_components *oc,\r\nstruct ore_io_state **pios)\r\n{\r\nreturn _ore_get_io_state(layout, oc, oc->numdevs, 0, 0, pios);\r\n}\r\nvoid ore_put_io_state(struct ore_io_state *ios)\r\n{\r\nif (ios) {\r\nunsigned i;\r\nfor (i = 0; i < ios->numdevs; i++) {\r\nstruct ore_per_dev_state *per_dev = &ios->per_dev[i];\r\nif (per_dev->or)\r\nosd_end_request(per_dev->or);\r\nif (per_dev->bio)\r\nbio_put(per_dev->bio);\r\n}\r\n_ore_free_raid_stuff(ios);\r\nkfree(ios);\r\n}\r\n}\r\nstatic void _sync_done(struct ore_io_state *ios, void *p)\r\n{\r\nstruct completion *waiting = p;\r\ncomplete(waiting);\r\n}\r\nstatic void _last_io(struct kref *kref)\r\n{\r\nstruct ore_io_state *ios = container_of(\r\nkref, struct ore_io_state, kref);\r\nios->done(ios, ios->private);\r\n}\r\nstatic void _done_io(struct osd_request *or, void *p)\r\n{\r\nstruct ore_io_state *ios = p;\r\nkref_put(&ios->kref, _last_io);\r\n}\r\nint ore_io_execute(struct ore_io_state *ios)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nbool sync = (ios->done == NULL);\r\nint i, ret;\r\nif (sync) {\r\nios->done = _sync_done;\r\nios->private = &wait;\r\n}\r\nfor (i = 0; i < ios->numdevs; i++) {\r\nstruct osd_request *or = ios->per_dev[i].or;\r\nif (unlikely(!or))\r\ncontinue;\r\nret = osd_finalize_request(or, 0, _ios_cred(ios, i), NULL);\r\nif (unlikely(ret)) {\r\nORE_DBGMSG("Failed to osd_finalize_request() => %d\n",\r\nret);\r\nreturn ret;\r\n}\r\n}\r\nkref_init(&ios->kref);\r\nfor (i = 0; i < ios->numdevs; i++) {\r\nstruct osd_request *or = ios->per_dev[i].or;\r\nif (unlikely(!or))\r\ncontinue;\r\nkref_get(&ios->kref);\r\nosd_execute_request_async(or, _done_io, ios);\r\n}\r\nkref_put(&ios->kref, _last_io);\r\nret = 0;\r\nif (sync) {\r\nwait_for_completion(&wait);\r\nret = ore_check_io(ios, NULL);\r\n}\r\nreturn ret;\r\n}\r\nstatic void _clear_bio(struct bio *bio)\r\n{\r\nstruct bio_vec *bv;\r\nunsigned i;\r\nbio_for_each_segment_all(bv, bio, i) {\r\nunsigned this_count = bv->bv_len;\r\nif (likely(PAGE_SIZE == this_count))\r\nclear_highpage(bv->bv_page);\r\nelse\r\nzero_user(bv->bv_page, bv->bv_offset, this_count);\r\n}\r\n}\r\nint ore_check_io(struct ore_io_state *ios, ore_on_dev_error on_dev_error)\r\n{\r\nenum osd_err_priority acumulated_osd_err = 0;\r\nint acumulated_lin_err = 0;\r\nint i;\r\nfor (i = 0; i < ios->numdevs; i++) {\r\nstruct osd_sense_info osi;\r\nstruct ore_per_dev_state *per_dev = &ios->per_dev[i];\r\nstruct osd_request *or = per_dev->or;\r\nint ret;\r\nif (unlikely(!or))\r\ncontinue;\r\nret = osd_req_decode_sense(or, &osi);\r\nif (likely(!ret))\r\ncontinue;\r\nif (OSD_ERR_PRI_CLEAR_PAGES == osi.osd_err_pri) {\r\n_clear_bio(per_dev->bio);\r\nORE_DBGMSG("start read offset passed end of file "\r\n"offset=0x%llx, length=0x%llx\n",\r\n_LLU(per_dev->offset),\r\n_LLU(per_dev->length));\r\ncontinue;\r\n}\r\nif (on_dev_error) {\r\nu64 residual = ios->reading ?\r\nor->in.residual : or->out.residual;\r\nu64 offset = (ios->offset + ios->length) - residual;\r\nunsigned dev = per_dev->dev - ios->oc->first_dev;\r\nstruct ore_dev *od = ios->oc->ods[dev];\r\non_dev_error(ios, od, dev, osi.osd_err_pri,\r\noffset, residual);\r\n}\r\nif (osi.osd_err_pri >= acumulated_osd_err) {\r\nacumulated_osd_err = osi.osd_err_pri;\r\nacumulated_lin_err = ret;\r\n}\r\n}\r\nreturn acumulated_lin_err;\r\n}\r\nvoid ore_calc_stripe_info(struct ore_layout *layout, u64 file_offset,\r\nu64 length, struct ore_striping_info *si)\r\n{\r\nu32 stripe_unit = layout->stripe_unit;\r\nu32 group_width = layout->group_width;\r\nu64 group_depth = layout->group_depth;\r\nu32 parity = layout->parity;\r\nu32 D = group_width - parity;\r\nu32 U = D * stripe_unit;\r\nu64 T = U * group_depth;\r\nu64 S = T * layout->group_count;\r\nu64 M = div64_u64(file_offset, S);\r\nu64 LmodS = file_offset - M * S;\r\nu32 G = div64_u64(LmodS, T);\r\nu64 H = LmodS - G * T;\r\nu32 N = div_u64(H, U);\r\nu32 C = (u32)(H - (N * U)) / stripe_unit + G * group_width;\r\ndiv_u64_rem(file_offset, stripe_unit, &si->unit_off);\r\nsi->obj_offset = si->unit_off + (N * stripe_unit) +\r\n(M * group_depth * stripe_unit);\r\nif (parity) {\r\nu32 LCMdP = lcm(group_width, parity) / parity;\r\nu32 RxP = (N % LCMdP) * parity;\r\nu32 first_dev = C - C % group_width;\r\nsi->par_dev = (group_width + group_width - parity - RxP) %\r\ngroup_width + first_dev;\r\nsi->dev = (group_width + C - RxP) % group_width + first_dev;\r\nsi->bytes_in_stripe = U;\r\nsi->first_stripe_start = M * S + G * T + N * U;\r\n} else {\r\nsi->par_dev = group_width;\r\nsi->dev = C;\r\n}\r\nsi->dev *= layout->mirrors_p1;\r\nsi->par_dev *= layout->mirrors_p1;\r\nsi->offset = file_offset;\r\nsi->length = T - H;\r\nif (si->length > length)\r\nsi->length = length;\r\nsi->M = M;\r\n}\r\nint _ore_add_stripe_unit(struct ore_io_state *ios, unsigned *cur_pg,\r\nunsigned pgbase, struct page **pages,\r\nstruct ore_per_dev_state *per_dev, int cur_len)\r\n{\r\nunsigned pg = *cur_pg;\r\nstruct request_queue *q =\r\nosd_request_queue(_ios_od(ios, per_dev->dev));\r\nunsigned len = cur_len;\r\nint ret;\r\nif (per_dev->bio == NULL) {\r\nunsigned pages_in_stripe = ios->layout->group_width *\r\n(ios->layout->stripe_unit / PAGE_SIZE);\r\nunsigned nr_pages = ios->nr_pages * ios->layout->group_width /\r\n(ios->layout->group_width -\r\nios->layout->parity);\r\nunsigned bio_size = (nr_pages + pages_in_stripe) /\r\nios->layout->group_width;\r\nper_dev->bio = bio_kmalloc(GFP_KERNEL, bio_size);\r\nif (unlikely(!per_dev->bio)) {\r\nORE_DBGMSG("Failed to allocate BIO size=%u\n",\r\nbio_size);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\n}\r\nwhile (cur_len > 0) {\r\nunsigned pglen = min_t(unsigned, PAGE_SIZE - pgbase, cur_len);\r\nunsigned added_len;\r\ncur_len -= pglen;\r\nadded_len = bio_add_pc_page(q, per_dev->bio, pages[pg],\r\npglen, pgbase);\r\nif (unlikely(pglen != added_len)) {\r\nORE_DBGMSG("Failed bio_add_pc_page bi_vcnt=%u\n",\r\nper_dev->bio->bi_vcnt);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\n_add_stripe_page(ios->sp2d, &ios->si, pages[pg]);\r\npgbase = 0;\r\n++pg;\r\n}\r\nBUG_ON(cur_len);\r\nper_dev->length += len;\r\n*cur_pg = pg;\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic int _prepare_for_striping(struct ore_io_state *ios)\r\n{\r\nstruct ore_striping_info *si = &ios->si;\r\nunsigned stripe_unit = ios->layout->stripe_unit;\r\nunsigned mirrors_p1 = ios->layout->mirrors_p1;\r\nunsigned group_width = ios->layout->group_width;\r\nunsigned devs_in_group = group_width * mirrors_p1;\r\nunsigned dev = si->dev;\r\nunsigned first_dev = dev - (dev % devs_in_group);\r\nunsigned dev_order;\r\nunsigned cur_pg = ios->pages_consumed;\r\nu64 length = ios->length;\r\nint ret = 0;\r\nif (!ios->pages) {\r\nios->numdevs = ios->layout->mirrors_p1;\r\nreturn 0;\r\n}\r\nBUG_ON(length > si->length);\r\ndev_order = _dev_order(devs_in_group, mirrors_p1, si->par_dev, dev);\r\nsi->cur_comp = dev_order;\r\nsi->cur_pg = si->unit_off / PAGE_SIZE;\r\nwhile (length) {\r\nunsigned comp = dev - first_dev;\r\nstruct ore_per_dev_state *per_dev = &ios->per_dev[comp];\r\nunsigned cur_len, page_off = 0;\r\nif (!per_dev->length) {\r\nper_dev->dev = dev;\r\nif (dev == si->dev) {\r\nWARN_ON(dev == si->par_dev);\r\nper_dev->offset = si->obj_offset;\r\ncur_len = stripe_unit - si->unit_off;\r\npage_off = si->unit_off & ~PAGE_MASK;\r\nBUG_ON(page_off && (page_off != ios->pgbase));\r\n} else {\r\nif (si->cur_comp > dev_order)\r\nper_dev->offset =\r\nsi->obj_offset - si->unit_off;\r\nelse\r\nper_dev->offset =\r\nsi->obj_offset + stripe_unit -\r\nsi->unit_off;\r\ncur_len = stripe_unit;\r\n}\r\n} else {\r\ncur_len = stripe_unit;\r\n}\r\nif (cur_len >= length)\r\ncur_len = length;\r\nret = _ore_add_stripe_unit(ios, &cur_pg, page_off, ios->pages,\r\nper_dev, cur_len);\r\nif (unlikely(ret))\r\ngoto out;\r\ndev += mirrors_p1;\r\ndev = (dev % devs_in_group) + first_dev;\r\nlength -= cur_len;\r\nsi->cur_comp = (si->cur_comp + 1) % group_width;\r\nif (unlikely((dev == si->par_dev) || (!length && ios->sp2d))) {\r\nif (!length && ios->sp2d) {\r\ndev = si->par_dev;\r\n}\r\nif (ios->sp2d)\r\ncur_len = length;\r\nper_dev = &ios->per_dev[dev - first_dev];\r\nif (!per_dev->length) {\r\nper_dev->dev = dev;\r\nper_dev->offset = si->obj_offset - si->unit_off;\r\n}\r\nret = _ore_add_parity_unit(ios, si, per_dev, cur_len);\r\nif (unlikely(ret))\r\ngoto out;\r\nsi->par_dev = (devs_in_group + si->par_dev -\r\nios->layout->parity * mirrors_p1) %\r\ndevs_in_group + first_dev;\r\nsi->cur_comp = 0;\r\nsi->cur_pg = 0;\r\n}\r\n}\r\nout:\r\nios->numdevs = devs_in_group;\r\nios->pages_consumed = cur_pg;\r\nreturn ret;\r\n}\r\nint ore_create(struct ore_io_state *ios)\r\n{\r\nint i, ret;\r\nfor (i = 0; i < ios->oc->numdevs; i++) {\r\nstruct osd_request *or;\r\nor = osd_start_request(_ios_od(ios, i), GFP_KERNEL);\r\nif (unlikely(!or)) {\r\nORE_ERR("%s: osd_start_request failed\n", __func__);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nios->per_dev[i].or = or;\r\nios->numdevs++;\r\nosd_req_create_object(or, _ios_obj(ios, i));\r\n}\r\nret = ore_io_execute(ios);\r\nout:\r\nreturn ret;\r\n}\r\nint ore_remove(struct ore_io_state *ios)\r\n{\r\nint i, ret;\r\nfor (i = 0; i < ios->oc->numdevs; i++) {\r\nstruct osd_request *or;\r\nor = osd_start_request(_ios_od(ios, i), GFP_KERNEL);\r\nif (unlikely(!or)) {\r\nORE_ERR("%s: osd_start_request failed\n", __func__);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nios->per_dev[i].or = or;\r\nios->numdevs++;\r\nosd_req_remove_object(or, _ios_obj(ios, i));\r\n}\r\nret = ore_io_execute(ios);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int _write_mirror(struct ore_io_state *ios, int cur_comp)\r\n{\r\nstruct ore_per_dev_state *master_dev = &ios->per_dev[cur_comp];\r\nunsigned dev = ios->per_dev[cur_comp].dev;\r\nunsigned last_comp = cur_comp + ios->layout->mirrors_p1;\r\nint ret = 0;\r\nif (ios->pages && !master_dev->length)\r\nreturn 0;\r\nfor (; cur_comp < last_comp; ++cur_comp, ++dev) {\r\nstruct ore_per_dev_state *per_dev = &ios->per_dev[cur_comp];\r\nstruct osd_request *or;\r\nor = osd_start_request(_ios_od(ios, dev), GFP_KERNEL);\r\nif (unlikely(!or)) {\r\nORE_ERR("%s: osd_start_request failed\n", __func__);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nper_dev->or = or;\r\nif (ios->pages) {\r\nstruct bio *bio;\r\nif (per_dev != master_dev) {\r\nbio = bio_clone_kmalloc(master_dev->bio,\r\nGFP_KERNEL);\r\nif (unlikely(!bio)) {\r\nORE_DBGMSG(\r\n"Failed to allocate BIO size=%u\n",\r\nmaster_dev->bio->bi_max_vecs);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nbio->bi_bdev = NULL;\r\nbio->bi_next = NULL;\r\nper_dev->offset = master_dev->offset;\r\nper_dev->length = master_dev->length;\r\nper_dev->bio = bio;\r\nper_dev->dev = dev;\r\n} else {\r\nbio = master_dev->bio;\r\nbio->bi_rw |= REQ_WRITE;\r\n}\r\nosd_req_write(or, _ios_obj(ios, cur_comp),\r\nper_dev->offset, bio, per_dev->length);\r\nORE_DBGMSG("write(0x%llx) offset=0x%llx "\r\n"length=0x%llx dev=%d\n",\r\n_LLU(_ios_obj(ios, cur_comp)->id),\r\n_LLU(per_dev->offset),\r\n_LLU(per_dev->length), dev);\r\n} else if (ios->kern_buff) {\r\nper_dev->offset = ios->si.obj_offset;\r\nper_dev->dev = ios->si.dev + dev;\r\nBUG_ON((ios->layout->group_width > 1) &&\r\n(ios->si.unit_off + ios->length >\r\nios->layout->stripe_unit));\r\nret = osd_req_write_kern(or, _ios_obj(ios, cur_comp),\r\nper_dev->offset,\r\nios->kern_buff, ios->length);\r\nif (unlikely(ret))\r\ngoto out;\r\nORE_DBGMSG2("write_kern(0x%llx) offset=0x%llx "\r\n"length=0x%llx dev=%d\n",\r\n_LLU(_ios_obj(ios, cur_comp)->id),\r\n_LLU(per_dev->offset),\r\n_LLU(ios->length), per_dev->dev);\r\n} else {\r\nosd_req_set_attributes(or, _ios_obj(ios, cur_comp));\r\nORE_DBGMSG2("obj(0x%llx) set_attributes=%d dev=%d\n",\r\n_LLU(_ios_obj(ios, cur_comp)->id),\r\nios->out_attr_len, dev);\r\n}\r\nif (ios->out_attr)\r\nosd_req_add_set_attr_list(or, ios->out_attr,\r\nios->out_attr_len);\r\nif (ios->in_attr)\r\nosd_req_add_get_attr_list(or, ios->in_attr,\r\nios->in_attr_len);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nint ore_write(struct ore_io_state *ios)\r\n{\r\nint i;\r\nint ret;\r\nif (unlikely(ios->sp2d && !ios->r4w)) {\r\nWARN_ON_ONCE(1);\r\nreturn -ENOTSUPP;\r\n}\r\nret = _prepare_for_striping(ios);\r\nif (unlikely(ret))\r\nreturn ret;\r\nfor (i = 0; i < ios->numdevs; i += ios->layout->mirrors_p1) {\r\nret = _write_mirror(ios, i);\r\nif (unlikely(ret))\r\nreturn ret;\r\n}\r\nret = ore_io_execute(ios);\r\nreturn ret;\r\n}\r\nint _ore_read_mirror(struct ore_io_state *ios, unsigned cur_comp)\r\n{\r\nstruct osd_request *or;\r\nstruct ore_per_dev_state *per_dev = &ios->per_dev[cur_comp];\r\nstruct osd_obj_id *obj = _ios_obj(ios, cur_comp);\r\nunsigned first_dev = (unsigned)obj->id;\r\nif (ios->pages && !per_dev->length)\r\nreturn 0;\r\nfirst_dev = per_dev->dev + first_dev % ios->layout->mirrors_p1;\r\nor = osd_start_request(_ios_od(ios, first_dev), GFP_KERNEL);\r\nif (unlikely(!or)) {\r\nORE_ERR("%s: osd_start_request failed\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nper_dev->or = or;\r\nif (ios->pages) {\r\nif (per_dev->cur_sg) {\r\n_ore_add_sg_seg(per_dev, 0, false);\r\nif (unlikely(!per_dev->cur_sg))\r\nreturn 0;\r\nosd_req_read_sg(or, obj, per_dev->bio,\r\nper_dev->sglist, per_dev->cur_sg);\r\n} else {\r\nosd_req_read(or, obj, per_dev->offset,\r\nper_dev->bio, per_dev->length);\r\n}\r\nORE_DBGMSG("read(0x%llx) offset=0x%llx length=0x%llx"\r\n" dev=%d sg_len=%d\n", _LLU(obj->id),\r\n_LLU(per_dev->offset), _LLU(per_dev->length),\r\nfirst_dev, per_dev->cur_sg);\r\n} else {\r\nBUG_ON(ios->kern_buff);\r\nosd_req_get_attributes(or, obj);\r\nORE_DBGMSG2("obj(0x%llx) get_attributes=%d dev=%d\n",\r\n_LLU(obj->id),\r\nios->in_attr_len, first_dev);\r\n}\r\nif (ios->out_attr)\r\nosd_req_add_set_attr_list(or, ios->out_attr, ios->out_attr_len);\r\nif (ios->in_attr)\r\nosd_req_add_get_attr_list(or, ios->in_attr, ios->in_attr_len);\r\nreturn 0;\r\n}\r\nint ore_read(struct ore_io_state *ios)\r\n{\r\nint i;\r\nint ret;\r\nret = _prepare_for_striping(ios);\r\nif (unlikely(ret))\r\nreturn ret;\r\nfor (i = 0; i < ios->numdevs; i += ios->layout->mirrors_p1) {\r\nret = _ore_read_mirror(ios, i);\r\nif (unlikely(ret))\r\nreturn ret;\r\n}\r\nret = ore_io_execute(ios);\r\nreturn ret;\r\n}\r\nint extract_attr_from_ios(struct ore_io_state *ios, struct osd_attr *attr)\r\n{\r\nstruct osd_attr cur_attr = {.attr_page = 0};\r\nvoid *iter = NULL;\r\nint nelem;\r\ndo {\r\nnelem = 1;\r\nosd_req_decode_get_attr_list(ios->per_dev[0].or,\r\n&cur_attr, &nelem, &iter);\r\nif ((cur_attr.attr_page == attr->attr_page) &&\r\n(cur_attr.attr_id == attr->attr_id)) {\r\nattr->len = cur_attr.len;\r\nattr->val_ptr = cur_attr.val_ptr;\r\nreturn 0;\r\n}\r\n} while (iter);\r\nreturn -EIO;\r\n}\r\nstatic int _truncate_mirrors(struct ore_io_state *ios, unsigned cur_comp,\r\nstruct osd_attr *attr)\r\n{\r\nint last_comp = cur_comp + ios->layout->mirrors_p1;\r\nfor (; cur_comp < last_comp; ++cur_comp) {\r\nstruct ore_per_dev_state *per_dev = &ios->per_dev[cur_comp];\r\nstruct osd_request *or;\r\nor = osd_start_request(_ios_od(ios, cur_comp), GFP_KERNEL);\r\nif (unlikely(!or)) {\r\nORE_ERR("%s: osd_start_request failed\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nper_dev->or = or;\r\nosd_req_set_attributes(or, _ios_obj(ios, cur_comp));\r\nosd_req_add_set_attr_list(or, attr, 1);\r\n}\r\nreturn 0;\r\n}\r\nstatic void _calc_trunk_info(struct ore_layout *layout, u64 file_offset,\r\nstruct _trunc_info *ti)\r\n{\r\nunsigned stripe_unit = layout->stripe_unit;\r\nore_calc_stripe_info(layout, file_offset, 0, &ti->si);\r\nti->prev_group_obj_off = ti->si.M * stripe_unit;\r\nti->next_group_obj_off = ti->si.M ? (ti->si.M - 1) * stripe_unit : 0;\r\nti->first_group_dev = ti->si.dev - (ti->si.dev % layout->group_width);\r\nti->nex_group_dev = ti->first_group_dev + layout->group_width;\r\n}\r\nint ore_truncate(struct ore_layout *layout, struct ore_components *oc,\r\nu64 size)\r\n{\r\nstruct ore_io_state *ios;\r\nstruct exofs_trunc_attr {\r\nstruct osd_attr attr;\r\n__be64 newsize;\r\n} *size_attrs;\r\nstruct _trunc_info ti;\r\nint i, ret;\r\nret = ore_get_io_state(layout, oc, &ios);\r\nif (unlikely(ret))\r\nreturn ret;\r\n_calc_trunk_info(ios->layout, size, &ti);\r\nsize_attrs = kcalloc(ios->oc->numdevs, sizeof(*size_attrs),\r\nGFP_KERNEL);\r\nif (unlikely(!size_attrs)) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nios->numdevs = ios->oc->numdevs;\r\nfor (i = 0; i < ios->numdevs; ++i) {\r\nstruct exofs_trunc_attr *size_attr = &size_attrs[i];\r\nu64 obj_size;\r\nif (i < ti.first_group_dev)\r\nobj_size = ti.prev_group_obj_off;\r\nelse if (i >= ti.nex_group_dev)\r\nobj_size = ti.next_group_obj_off;\r\nelse if (i < ti.si.dev)\r\nobj_size = ti.si.obj_offset +\r\nios->layout->stripe_unit - ti.si.unit_off;\r\nelse if (i == ti.si.dev)\r\nobj_size = ti.si.obj_offset;\r\nelse\r\nobj_size = ti.si.obj_offset - ti.si.unit_off;\r\nsize_attr->newsize = cpu_to_be64(obj_size);\r\nsize_attr->attr = g_attr_logical_length;\r\nsize_attr->attr.val_ptr = &size_attr->newsize;\r\nORE_DBGMSG("trunc(0x%llx) obj_offset=0x%llx dev=%d\n",\r\n_LLU(oc->comps->obj.id), _LLU(obj_size), i);\r\nret = _truncate_mirrors(ios, i * ios->layout->mirrors_p1,\r\n&size_attr->attr);\r\nif (unlikely(ret))\r\ngoto out;\r\n}\r\nret = ore_io_execute(ios);\r\nout:\r\nkfree(size_attrs);\r\nore_put_io_state(ios);\r\nreturn ret;\r\n}
