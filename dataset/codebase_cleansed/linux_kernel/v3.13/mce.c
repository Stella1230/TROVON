void mce_setup(struct mce *m)\r\n{\r\nmemset(m, 0, sizeof(struct mce));\r\nm->cpu = m->extcpu = smp_processor_id();\r\nrdtscll(m->tsc);\r\nm->time = get_seconds();\r\nm->cpuvendor = boot_cpu_data.x86_vendor;\r\nm->cpuid = cpuid_eax(1);\r\nm->socketid = cpu_data(m->extcpu).phys_proc_id;\r\nm->apicid = cpu_data(m->extcpu).initial_apicid;\r\nrdmsrl(MSR_IA32_MCG_CAP, m->mcgcap);\r\n}\r\nvoid mce_log(struct mce *mce)\r\n{\r\nunsigned next, entry;\r\nint ret = 0;\r\ntrace_mce_record(mce);\r\nret = atomic_notifier_call_chain(&x86_mce_decoder_chain, 0, mce);\r\nif (ret == NOTIFY_STOP)\r\nreturn;\r\nmce->finished = 0;\r\nwmb();\r\nfor (;;) {\r\nentry = rcu_dereference_check_mce(mcelog.next);\r\nfor (;;) {\r\nif (entry >= MCE_LOG_LEN) {\r\nset_bit(MCE_OVERFLOW,\r\n(unsigned long *)&mcelog.flags);\r\nreturn;\r\n}\r\nif (mcelog.entry[entry].finished) {\r\nentry++;\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nsmp_rmb();\r\nnext = entry + 1;\r\nif (cmpxchg(&mcelog.next, entry, next) == entry)\r\nbreak;\r\n}\r\nmemcpy(mcelog.entry + entry, mce, sizeof(struct mce));\r\nwmb();\r\nmcelog.entry[entry].finished = 1;\r\nwmb();\r\nmce->finished = 1;\r\nset_bit(0, &mce_need_notify);\r\n}\r\nstatic void drain_mcelog_buffer(void)\r\n{\r\nunsigned int next, i, prev = 0;\r\nnext = ACCESS_ONCE(mcelog.next);\r\ndo {\r\nstruct mce *m;\r\nfor (i = prev; i < next; i++) {\r\nunsigned long start = jiffies;\r\nunsigned retries = 1;\r\nm = &mcelog.entry[i];\r\nwhile (!m->finished) {\r\nif (time_after_eq(jiffies, start + 2*retries))\r\nretries++;\r\ncpu_relax();\r\nif (!m->finished && retries >= 4) {\r\npr_err("skipping error being logged currently!\n");\r\nbreak;\r\n}\r\n}\r\nsmp_rmb();\r\natomic_notifier_call_chain(&x86_mce_decoder_chain, 0, m);\r\n}\r\nmemset(mcelog.entry + prev, 0, (next - prev) * sizeof(*m));\r\nprev = next;\r\nnext = cmpxchg(&mcelog.next, prev, 0);\r\n} while (next != prev);\r\n}\r\nvoid mce_register_decode_chain(struct notifier_block *nb)\r\n{\r\natomic_notifier_chain_register(&x86_mce_decoder_chain, nb);\r\ndrain_mcelog_buffer();\r\n}\r\nvoid mce_unregister_decode_chain(struct notifier_block *nb)\r\n{\r\natomic_notifier_chain_unregister(&x86_mce_decoder_chain, nb);\r\n}\r\nstatic void print_mce(struct mce *m)\r\n{\r\nint ret = 0;\r\npr_emerg(HW_ERR "CPU %d: Machine Check Exception: %Lx Bank %d: %016Lx\n",\r\nm->extcpu, m->mcgstatus, m->bank, m->status);\r\nif (m->ip) {\r\npr_emerg(HW_ERR "RIP%s %02x:<%016Lx> ",\r\n!(m->mcgstatus & MCG_STATUS_EIPV) ? " !INEXACT!" : "",\r\nm->cs, m->ip);\r\nif (m->cs == __KERNEL_CS)\r\nprint_symbol("{%s}", m->ip);\r\npr_cont("\n");\r\n}\r\npr_emerg(HW_ERR "TSC %llx ", m->tsc);\r\nif (m->addr)\r\npr_cont("ADDR %llx ", m->addr);\r\nif (m->misc)\r\npr_cont("MISC %llx ", m->misc);\r\npr_cont("\n");\r\npr_emerg(HW_ERR "PROCESSOR %u:%x TIME %llu SOCKET %u APIC %x microcode %x\n",\r\nm->cpuvendor, m->cpuid, m->time, m->socketid, m->apicid,\r\ncpu_data(m->extcpu).microcode);\r\nret = atomic_notifier_call_chain(&x86_mce_decoder_chain, 0, m);\r\nif (ret == NOTIFY_STOP)\r\nreturn;\r\npr_emerg_ratelimited(HW_ERR "Run the above through 'mcelog --ascii'\n");\r\n}\r\nstatic void wait_for_panic(void)\r\n{\r\nlong timeout = PANIC_TIMEOUT*USEC_PER_SEC;\r\npreempt_disable();\r\nlocal_irq_enable();\r\nwhile (timeout-- > 0)\r\nudelay(1);\r\nif (panic_timeout == 0)\r\npanic_timeout = mca_cfg.panic_timeout;\r\npanic("Panicing machine check CPU died");\r\n}\r\nstatic void mce_panic(char *msg, struct mce *final, char *exp)\r\n{\r\nint i, apei_err = 0;\r\nif (!fake_panic) {\r\nif (atomic_inc_return(&mce_paniced) > 1)\r\nwait_for_panic();\r\nbarrier();\r\nbust_spinlocks(1);\r\nconsole_verbose();\r\n} else {\r\nif (atomic_inc_return(&mce_fake_paniced) > 1)\r\nreturn;\r\n}\r\nfor (i = 0; i < MCE_LOG_LEN; i++) {\r\nstruct mce *m = &mcelog.entry[i];\r\nif (!(m->status & MCI_STATUS_VAL))\r\ncontinue;\r\nif (!(m->status & MCI_STATUS_UC)) {\r\nprint_mce(m);\r\nif (!apei_err)\r\napei_err = apei_write_mce(m);\r\n}\r\n}\r\nfor (i = 0; i < MCE_LOG_LEN; i++) {\r\nstruct mce *m = &mcelog.entry[i];\r\nif (!(m->status & MCI_STATUS_VAL))\r\ncontinue;\r\nif (!(m->status & MCI_STATUS_UC))\r\ncontinue;\r\nif (!final || memcmp(m, final, sizeof(struct mce))) {\r\nprint_mce(m);\r\nif (!apei_err)\r\napei_err = apei_write_mce(m);\r\n}\r\n}\r\nif (final) {\r\nprint_mce(final);\r\nif (!apei_err)\r\napei_err = apei_write_mce(final);\r\n}\r\nif (cpu_missing)\r\npr_emerg(HW_ERR "Some CPUs didn't answer in synchronization\n");\r\nif (exp)\r\npr_emerg(HW_ERR "Machine check: %s\n", exp);\r\nif (!fake_panic) {\r\nif (panic_timeout == 0)\r\npanic_timeout = mca_cfg.panic_timeout;\r\npanic(msg);\r\n} else\r\npr_emerg(HW_ERR "Fake kernel panic: %s\n", msg);\r\n}\r\nstatic int msr_to_offset(u32 msr)\r\n{\r\nunsigned bank = __this_cpu_read(injectm.bank);\r\nif (msr == mca_cfg.rip_msr)\r\nreturn offsetof(struct mce, ip);\r\nif (msr == MSR_IA32_MCx_STATUS(bank))\r\nreturn offsetof(struct mce, status);\r\nif (msr == MSR_IA32_MCx_ADDR(bank))\r\nreturn offsetof(struct mce, addr);\r\nif (msr == MSR_IA32_MCx_MISC(bank))\r\nreturn offsetof(struct mce, misc);\r\nif (msr == MSR_IA32_MCG_STATUS)\r\nreturn offsetof(struct mce, mcgstatus);\r\nreturn -1;\r\n}\r\nstatic u64 mce_rdmsrl(u32 msr)\r\n{\r\nu64 v;\r\nif (__this_cpu_read(injectm.finished)) {\r\nint offset = msr_to_offset(msr);\r\nif (offset < 0)\r\nreturn 0;\r\nreturn *(u64 *)((char *)&__get_cpu_var(injectm) + offset);\r\n}\r\nif (rdmsrl_safe(msr, &v)) {\r\nWARN_ONCE(1, "mce: Unable to read msr %d!\n", msr);\r\nv = 0;\r\n}\r\nreturn v;\r\n}\r\nstatic void mce_wrmsrl(u32 msr, u64 v)\r\n{\r\nif (__this_cpu_read(injectm.finished)) {\r\nint offset = msr_to_offset(msr);\r\nif (offset >= 0)\r\n*(u64 *)((char *)&__get_cpu_var(injectm) + offset) = v;\r\nreturn;\r\n}\r\nwrmsrl(msr, v);\r\n}\r\nstatic inline void mce_gather_info(struct mce *m, struct pt_regs *regs)\r\n{\r\nmce_setup(m);\r\nm->mcgstatus = mce_rdmsrl(MSR_IA32_MCG_STATUS);\r\nif (regs) {\r\nif (m->mcgstatus & (MCG_STATUS_RIPV|MCG_STATUS_EIPV)) {\r\nm->ip = regs->ip;\r\nm->cs = regs->cs;\r\nif (v8086_mode(regs))\r\nm->cs |= 3;\r\n}\r\nif (mca_cfg.rip_msr)\r\nm->ip = mce_rdmsrl(mca_cfg.rip_msr);\r\n}\r\n}\r\nstatic int mce_ring_empty(void)\r\n{\r\nstruct mce_ring *r = &__get_cpu_var(mce_ring);\r\nreturn r->start == r->end;\r\n}\r\nstatic int mce_ring_get(unsigned long *pfn)\r\n{\r\nstruct mce_ring *r;\r\nint ret = 0;\r\n*pfn = 0;\r\nget_cpu();\r\nr = &__get_cpu_var(mce_ring);\r\nif (r->start == r->end)\r\ngoto out;\r\n*pfn = r->ring[r->start];\r\nr->start = (r->start + 1) % MCE_RING_SIZE;\r\nret = 1;\r\nout:\r\nput_cpu();\r\nreturn ret;\r\n}\r\nstatic int mce_ring_add(unsigned long pfn)\r\n{\r\nstruct mce_ring *r = &__get_cpu_var(mce_ring);\r\nunsigned next;\r\nnext = (r->end + 1) % MCE_RING_SIZE;\r\nif (next == r->start)\r\nreturn -1;\r\nr->ring[r->end] = pfn;\r\nwmb();\r\nr->end = next;\r\nreturn 0;\r\n}\r\nint mce_available(struct cpuinfo_x86 *c)\r\n{\r\nif (mca_cfg.disabled)\r\nreturn 0;\r\nreturn cpu_has(c, X86_FEATURE_MCE) && cpu_has(c, X86_FEATURE_MCA);\r\n}\r\nstatic void mce_schedule_work(void)\r\n{\r\nif (!mce_ring_empty())\r\nschedule_work(&__get_cpu_var(mce_work));\r\n}\r\nstatic void mce_irq_work_cb(struct irq_work *entry)\r\n{\r\nmce_notify_irq();\r\nmce_schedule_work();\r\n}\r\nstatic void mce_report_event(struct pt_regs *regs)\r\n{\r\nif (regs->flags & (X86_VM_MASK|X86_EFLAGS_IF)) {\r\nmce_notify_irq();\r\nmce_schedule_work();\r\nreturn;\r\n}\r\nirq_work_queue(&__get_cpu_var(mce_irq_work));\r\n}\r\nstatic void mce_read_aux(struct mce *m, int i)\r\n{\r\nif (m->status & MCI_STATUS_MISCV)\r\nm->misc = mce_rdmsrl(MSR_IA32_MCx_MISC(i));\r\nif (m->status & MCI_STATUS_ADDRV) {\r\nm->addr = mce_rdmsrl(MSR_IA32_MCx_ADDR(i));\r\nif (mca_cfg.ser && (m->status & MCI_STATUS_MISCV)) {\r\nu8 shift = MCI_MISC_ADDR_LSB(m->misc);\r\nm->addr >>= shift;\r\nm->addr <<= shift;\r\n}\r\n}\r\n}\r\nvoid machine_check_poll(enum mcp_flags flags, mce_banks_t *b)\r\n{\r\nstruct mce m;\r\nint i;\r\nthis_cpu_inc(mce_poll_count);\r\nmce_gather_info(&m, NULL);\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nif (!mce_banks[i].ctl || !test_bit(i, *b))\r\ncontinue;\r\nm.misc = 0;\r\nm.addr = 0;\r\nm.bank = i;\r\nm.tsc = 0;\r\nbarrier();\r\nm.status = mce_rdmsrl(MSR_IA32_MCx_STATUS(i));\r\nif (!(m.status & MCI_STATUS_VAL))\r\ncontinue;\r\nif (!(flags & MCP_UC) &&\r\n(m.status & (mca_cfg.ser ? MCI_STATUS_S : MCI_STATUS_UC)))\r\ncontinue;\r\nmce_read_aux(&m, i);\r\nif (!(flags & MCP_TIMESTAMP))\r\nm.tsc = 0;\r\nif (!(flags & MCP_DONTLOG) && !mca_cfg.dont_log_ce)\r\nmce_log(&m);\r\nmce_wrmsrl(MSR_IA32_MCx_STATUS(i), 0);\r\n}\r\nsync_core();\r\n}\r\nstatic int mce_no_way_out(struct mce *m, char **msg, unsigned long *validp,\r\nstruct pt_regs *regs)\r\n{\r\nint i, ret = 0;\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nm->status = mce_rdmsrl(MSR_IA32_MCx_STATUS(i));\r\nif (m->status & MCI_STATUS_VAL) {\r\n__set_bit(i, validp);\r\nif (quirk_no_way_out)\r\nquirk_no_way_out(i, m, regs);\r\n}\r\nif (mce_severity(m, mca_cfg.tolerant, msg) >= MCE_PANIC_SEVERITY)\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic int mce_timed_out(u64 *t)\r\n{\r\nrmb();\r\nif (atomic_read(&mce_paniced))\r\nwait_for_panic();\r\nif (!mca_cfg.monarch_timeout)\r\ngoto out;\r\nif ((s64)*t < SPINUNIT) {\r\nif (mca_cfg.tolerant < 1)\r\nmce_panic("Timeout synchronizing machine check over CPUs",\r\nNULL, NULL);\r\ncpu_missing = 1;\r\nreturn 1;\r\n}\r\n*t -= SPINUNIT;\r\nout:\r\ntouch_nmi_watchdog();\r\nreturn 0;\r\n}\r\nstatic void mce_reign(void)\r\n{\r\nint cpu;\r\nstruct mce *m = NULL;\r\nint global_worst = 0;\r\nchar *msg = NULL;\r\nchar *nmsg = NULL;\r\nfor_each_possible_cpu(cpu) {\r\nint severity = mce_severity(&per_cpu(mces_seen, cpu),\r\nmca_cfg.tolerant,\r\n&nmsg);\r\nif (severity > global_worst) {\r\nmsg = nmsg;\r\nglobal_worst = severity;\r\nm = &per_cpu(mces_seen, cpu);\r\n}\r\n}\r\nif (m && global_worst >= MCE_PANIC_SEVERITY && mca_cfg.tolerant < 3)\r\nmce_panic("Fatal Machine check", m, msg);\r\nif (global_worst <= MCE_KEEP_SEVERITY && mca_cfg.tolerant < 3)\r\nmce_panic("Machine check from unknown source", NULL, NULL);\r\nfor_each_possible_cpu(cpu)\r\nmemset(&per_cpu(mces_seen, cpu), 0, sizeof(struct mce));\r\n}\r\nstatic int mce_start(int *no_way_out)\r\n{\r\nint order;\r\nint cpus = num_online_cpus();\r\nu64 timeout = (u64)mca_cfg.monarch_timeout * NSEC_PER_USEC;\r\nif (!timeout)\r\nreturn -1;\r\natomic_add(*no_way_out, &global_nwo);\r\nsmp_wmb();\r\norder = atomic_inc_return(&mce_callin);\r\nwhile (atomic_read(&mce_callin) != cpus) {\r\nif (mce_timed_out(&timeout)) {\r\natomic_set(&global_nwo, 0);\r\nreturn -1;\r\n}\r\nndelay(SPINUNIT);\r\n}\r\nsmp_rmb();\r\nif (order == 1) {\r\natomic_set(&mce_executing, 1);\r\n} else {\r\nwhile (atomic_read(&mce_executing) < order) {\r\nif (mce_timed_out(&timeout)) {\r\natomic_set(&global_nwo, 0);\r\nreturn -1;\r\n}\r\nndelay(SPINUNIT);\r\n}\r\n}\r\n*no_way_out = atomic_read(&global_nwo);\r\nreturn order;\r\n}\r\nstatic int mce_end(int order)\r\n{\r\nint ret = -1;\r\nu64 timeout = (u64)mca_cfg.monarch_timeout * NSEC_PER_USEC;\r\nif (!timeout)\r\ngoto reset;\r\nif (order < 0)\r\ngoto reset;\r\natomic_inc(&mce_executing);\r\nif (order == 1) {\r\nint cpus = num_online_cpus();\r\nwhile (atomic_read(&mce_executing) <= cpus) {\r\nif (mce_timed_out(&timeout))\r\ngoto reset;\r\nndelay(SPINUNIT);\r\n}\r\nmce_reign();\r\nbarrier();\r\nret = 0;\r\n} else {\r\nwhile (atomic_read(&mce_executing) != 0) {\r\nif (mce_timed_out(&timeout))\r\ngoto reset;\r\nndelay(SPINUNIT);\r\n}\r\nreturn 0;\r\n}\r\nreset:\r\natomic_set(&global_nwo, 0);\r\natomic_set(&mce_callin, 0);\r\nbarrier();\r\natomic_set(&mce_executing, 0);\r\nreturn ret;\r\n}\r\nstatic int mce_usable_address(struct mce *m)\r\n{\r\nif (!(m->status & MCI_STATUS_MISCV) || !(m->status & MCI_STATUS_ADDRV))\r\nreturn 0;\r\nif (MCI_MISC_ADDR_LSB(m->misc) > PAGE_SHIFT)\r\nreturn 0;\r\nif (MCI_MISC_ADDR_MODE(m->misc) != MCI_MISC_ADDR_PHYS)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void mce_clear_state(unsigned long *toclear)\r\n{\r\nint i;\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nif (test_bit(i, toclear))\r\nmce_wrmsrl(MSR_IA32_MCx_STATUS(i), 0);\r\n}\r\n}\r\nstatic void mce_save_info(__u64 addr, int c)\r\n{\r\nstruct mce_info *mi;\r\nfor (mi = mce_info; mi < &mce_info[MCE_INFO_MAX]; mi++) {\r\nif (atomic_cmpxchg(&mi->inuse, 0, 1) == 0) {\r\nmi->t = current;\r\nmi->paddr = addr;\r\nmi->restartable = c;\r\nreturn;\r\n}\r\n}\r\nmce_panic("Too many concurrent recoverable errors", NULL, NULL);\r\n}\r\nstatic struct mce_info *mce_find_info(void)\r\n{\r\nstruct mce_info *mi;\r\nfor (mi = mce_info; mi < &mce_info[MCE_INFO_MAX]; mi++)\r\nif (atomic_read(&mi->inuse) && mi->t == current)\r\nreturn mi;\r\nreturn NULL;\r\n}\r\nstatic void mce_clear_info(struct mce_info *mi)\r\n{\r\natomic_set(&mi->inuse, 0);\r\n}\r\nvoid do_machine_check(struct pt_regs *regs, long error_code)\r\n{\r\nstruct mca_config *cfg = &mca_cfg;\r\nstruct mce m, *final;\r\nint i;\r\nint worst = 0;\r\nint severity;\r\nint order;\r\nint no_way_out = 0;\r\nint kill_it = 0;\r\nDECLARE_BITMAP(toclear, MAX_NR_BANKS);\r\nDECLARE_BITMAP(valid_banks, MAX_NR_BANKS);\r\nchar *msg = "Unknown";\r\natomic_inc(&mce_entry);\r\nthis_cpu_inc(mce_exception_count);\r\nif (!cfg->banks)\r\ngoto out;\r\nmce_gather_info(&m, regs);\r\nfinal = &__get_cpu_var(mces_seen);\r\n*final = m;\r\nmemset(valid_banks, 0, sizeof(valid_banks));\r\nno_way_out = mce_no_way_out(&m, &msg, valid_banks, regs);\r\nbarrier();\r\nif (!(m.mcgstatus & MCG_STATUS_RIPV))\r\nkill_it = 1;\r\norder = mce_start(&no_way_out);\r\nfor (i = 0; i < cfg->banks; i++) {\r\n__clear_bit(i, toclear);\r\nif (!test_bit(i, valid_banks))\r\ncontinue;\r\nif (!mce_banks[i].ctl)\r\ncontinue;\r\nm.misc = 0;\r\nm.addr = 0;\r\nm.bank = i;\r\nm.status = mce_rdmsrl(MSR_IA32_MCx_STATUS(i));\r\nif ((m.status & MCI_STATUS_VAL) == 0)\r\ncontinue;\r\nif (!(m.status & (cfg->ser ? MCI_STATUS_S : MCI_STATUS_UC)) &&\r\n!no_way_out)\r\ncontinue;\r\nadd_taint(TAINT_MACHINE_CHECK, LOCKDEP_NOW_UNRELIABLE);\r\nseverity = mce_severity(&m, cfg->tolerant, NULL);\r\nif (severity == MCE_KEEP_SEVERITY && !no_way_out)\r\ncontinue;\r\n__set_bit(i, toclear);\r\nif (severity == MCE_NO_SEVERITY) {\r\ncontinue;\r\n}\r\nmce_read_aux(&m, i);\r\nif (severity == MCE_AO_SEVERITY && mce_usable_address(&m))\r\nmce_ring_add(m.addr >> PAGE_SHIFT);\r\nmce_log(&m);\r\nif (severity > worst) {\r\n*final = m;\r\nworst = severity;\r\n}\r\n}\r\nm = *final;\r\nif (!no_way_out)\r\nmce_clear_state(toclear);\r\nif (mce_end(order) < 0)\r\nno_way_out = worst >= MCE_PANIC_SEVERITY;\r\nif (cfg->tolerant < 3) {\r\nif (no_way_out)\r\nmce_panic("Fatal machine check on current CPU", &m, msg);\r\nif (worst == MCE_AR_SEVERITY) {\r\nmce_save_info(m.addr, m.mcgstatus & MCG_STATUS_RIPV);\r\nset_thread_flag(TIF_MCE_NOTIFY);\r\n} else if (kill_it) {\r\nforce_sig(SIGBUS, current);\r\n}\r\n}\r\nif (worst > 0)\r\nmce_report_event(regs);\r\nmce_wrmsrl(MSR_IA32_MCG_STATUS, 0);\r\nout:\r\natomic_dec(&mce_entry);\r\nsync_core();\r\n}\r\nint memory_failure(unsigned long pfn, int vector, int flags)\r\n{\r\nBUG_ON(flags & MF_ACTION_REQUIRED);\r\npr_err("Uncorrected memory error in page 0x%lx ignored\n"\r\n"Rebuild kernel with CONFIG_MEMORY_FAILURE=y for smarter handling\n",\r\npfn);\r\nreturn 0;\r\n}\r\nvoid mce_notify_process(void)\r\n{\r\nunsigned long pfn;\r\nstruct mce_info *mi = mce_find_info();\r\nint flags = MF_ACTION_REQUIRED;\r\nif (!mi)\r\nmce_panic("Lost physical address for unconsumed uncorrectable error", NULL, NULL);\r\npfn = mi->paddr >> PAGE_SHIFT;\r\nclear_thread_flag(TIF_MCE_NOTIFY);\r\npr_err("Uncorrected hardware memory error in user-access at %llx",\r\nmi->paddr);\r\nif (!mi->restartable)\r\nflags |= MF_MUST_KILL;\r\nif (memory_failure(pfn, MCE_VECTOR, flags) < 0) {\r\npr_err("Memory error not recovered");\r\nforce_sig(SIGBUS, current);\r\n}\r\nmce_clear_info(mi);\r\n}\r\nstatic void mce_process_work(struct work_struct *dummy)\r\n{\r\nunsigned long pfn;\r\nwhile (mce_ring_get(&pfn))\r\nmemory_failure(pfn, MCE_VECTOR, 0);\r\n}\r\nvoid mce_log_therm_throt_event(__u64 status)\r\n{\r\nstruct mce m;\r\nmce_setup(&m);\r\nm.bank = MCE_THERMAL_BANK;\r\nm.status = status;\r\nmce_log(&m);\r\n}\r\nstatic unsigned long mce_adjust_timer_default(unsigned long interval)\r\n{\r\nreturn interval;\r\n}\r\nstatic void mce_timer_fn(unsigned long data)\r\n{\r\nstruct timer_list *t = &__get_cpu_var(mce_timer);\r\nunsigned long iv;\r\nWARN_ON(smp_processor_id() != data);\r\nif (mce_available(__this_cpu_ptr(&cpu_info))) {\r\nmachine_check_poll(MCP_TIMESTAMP,\r\n&__get_cpu_var(mce_poll_banks));\r\nmce_intel_cmci_poll();\r\n}\r\niv = __this_cpu_read(mce_next_interval);\r\nif (mce_notify_irq()) {\r\niv = max(iv / 2, (unsigned long) HZ/100);\r\n} else {\r\niv = min(iv * 2, round_jiffies_relative(check_interval * HZ));\r\niv = mce_adjust_timer(iv);\r\n}\r\n__this_cpu_write(mce_next_interval, iv);\r\nif (iv) {\r\nt->expires = jiffies + iv;\r\nadd_timer_on(t, smp_processor_id());\r\n}\r\n}\r\nvoid mce_timer_kick(unsigned long interval)\r\n{\r\nstruct timer_list *t = &__get_cpu_var(mce_timer);\r\nunsigned long when = jiffies + interval;\r\nunsigned long iv = __this_cpu_read(mce_next_interval);\r\nif (timer_pending(t)) {\r\nif (time_before(when, t->expires))\r\nmod_timer_pinned(t, when);\r\n} else {\r\nt->expires = round_jiffies(when);\r\nadd_timer_on(t, smp_processor_id());\r\n}\r\nif (interval < iv)\r\n__this_cpu_write(mce_next_interval, interval);\r\n}\r\nstatic void mce_timer_delete_all(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\ndel_timer_sync(&per_cpu(mce_timer, cpu));\r\n}\r\nstatic void mce_do_trigger(struct work_struct *work)\r\n{\r\ncall_usermodehelper(mce_helper, mce_helper_argv, NULL, UMH_NO_WAIT);\r\n}\r\nint mce_notify_irq(void)\r\n{\r\nstatic DEFINE_RATELIMIT_STATE(ratelimit, 60*HZ, 2);\r\nif (test_and_clear_bit(0, &mce_need_notify)) {\r\nwake_up_interruptible(&mce_chrdev_wait);\r\nif (mce_helper[0])\r\nschedule_work(&mce_trigger_work);\r\nif (__ratelimit(&ratelimit))\r\npr_info(HW_ERR "Machine check events logged\n");\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __mcheck_cpu_mce_banks_init(void)\r\n{\r\nint i;\r\nu8 num_banks = mca_cfg.banks;\r\nmce_banks = kzalloc(num_banks * sizeof(struct mce_bank), GFP_KERNEL);\r\nif (!mce_banks)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < num_banks; i++) {\r\nstruct mce_bank *b = &mce_banks[i];\r\nb->ctl = -1ULL;\r\nb->init = 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __mcheck_cpu_cap_init(void)\r\n{\r\nunsigned b;\r\nu64 cap;\r\nrdmsrl(MSR_IA32_MCG_CAP, cap);\r\nb = cap & MCG_BANKCNT_MASK;\r\nif (!mca_cfg.banks)\r\npr_info("CPU supports %d MCE banks\n", b);\r\nif (b > MAX_NR_BANKS) {\r\npr_warn("Using only %u machine check banks out of %u\n",\r\nMAX_NR_BANKS, b);\r\nb = MAX_NR_BANKS;\r\n}\r\nWARN_ON(mca_cfg.banks != 0 && b != mca_cfg.banks);\r\nmca_cfg.banks = b;\r\nif (!mce_banks) {\r\nint err = __mcheck_cpu_mce_banks_init();\r\nif (err)\r\nreturn err;\r\n}\r\nif ((cap & MCG_EXT_P) && MCG_EXT_CNT(cap) >= 9)\r\nmca_cfg.rip_msr = MSR_IA32_MCG_EIP;\r\nif (cap & MCG_SER_P)\r\nmca_cfg.ser = true;\r\nreturn 0;\r\n}\r\nstatic void __mcheck_cpu_init_generic(void)\r\n{\r\nenum mcp_flags m_fl = 0;\r\nmce_banks_t all_banks;\r\nu64 cap;\r\nint i;\r\nif (!mca_cfg.bootlog)\r\nm_fl = MCP_DONTLOG;\r\nbitmap_fill(all_banks, MAX_NR_BANKS);\r\nmachine_check_poll(MCP_UC | m_fl, &all_banks);\r\nset_in_cr4(X86_CR4_MCE);\r\nrdmsrl(MSR_IA32_MCG_CAP, cap);\r\nif (cap & MCG_CTL_P)\r\nwrmsr(MSR_IA32_MCG_CTL, 0xffffffff, 0xffffffff);\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nstruct mce_bank *b = &mce_banks[i];\r\nif (!b->init)\r\ncontinue;\r\nwrmsrl(MSR_IA32_MCx_CTL(i), b->ctl);\r\nwrmsrl(MSR_IA32_MCx_STATUS(i), 0);\r\n}\r\n}\r\nstatic void quirk_sandybridge_ifu(int bank, struct mce *m, struct pt_regs *regs)\r\n{\r\nif (bank != 0)\r\nreturn;\r\nif ((m->mcgstatus & (MCG_STATUS_EIPV|MCG_STATUS_RIPV)) != 0)\r\nreturn;\r\nif ((m->status & (MCI_STATUS_OVER|MCI_STATUS_UC|\r\nMCI_STATUS_EN|MCI_STATUS_MISCV|MCI_STATUS_ADDRV|\r\nMCI_STATUS_PCC|MCI_STATUS_S|MCI_STATUS_AR|\r\nMCACOD)) !=\r\n(MCI_STATUS_UC|MCI_STATUS_EN|\r\nMCI_STATUS_MISCV|MCI_STATUS_ADDRV|MCI_STATUS_S|\r\nMCI_STATUS_AR|MCACOD_INSTR))\r\nreturn;\r\nm->mcgstatus |= MCG_STATUS_EIPV;\r\nm->ip = regs->ip;\r\nm->cs = regs->cs;\r\n}\r\nstatic int __mcheck_cpu_apply_quirks(struct cpuinfo_x86 *c)\r\n{\r\nstruct mca_config *cfg = &mca_cfg;\r\nif (c->x86_vendor == X86_VENDOR_UNKNOWN) {\r\npr_info("unknown CPU type - not enabling MCE support\n");\r\nreturn -EOPNOTSUPP;\r\n}\r\nif (c->x86_vendor == X86_VENDOR_AMD) {\r\nif (c->x86 == 15 && cfg->banks > 4) {\r\nclear_bit(10, (unsigned long *)&mce_banks[4].ctl);\r\n}\r\nif (c->x86 <= 17 && cfg->bootlog < 0) {\r\ncfg->bootlog = 0;\r\n}\r\nif (c->x86 == 6 && cfg->banks > 0)\r\nmce_banks[0].ctl = 0;\r\nif (c->x86 == 0x15 &&\r\n(c->x86_model >= 0x10 && c->x86_model <= 0x1f)) {\r\nint i;\r\nu64 val, hwcr;\r\nbool need_toggle;\r\nu32 msrs[] = {\r\n0x00000413,\r\n0xc0000408,\r\n};\r\nrdmsrl(MSR_K7_HWCR, hwcr);\r\nneed_toggle = !(hwcr & BIT(18));\r\nif (need_toggle)\r\nwrmsrl(MSR_K7_HWCR, hwcr | BIT(18));\r\nfor (i = 0; i < ARRAY_SIZE(msrs); i++) {\r\nrdmsrl(msrs[i], val);\r\nif (val & BIT_64(62)) {\r\nval &= ~BIT_64(62);\r\nwrmsrl(msrs[i], val);\r\n}\r\n}\r\nif (need_toggle)\r\nwrmsrl(MSR_K7_HWCR, hwcr);\r\n}\r\n}\r\nif (c->x86_vendor == X86_VENDOR_INTEL) {\r\nif (c->x86 == 6 && c->x86_model < 0x1A && cfg->banks > 0)\r\nmce_banks[0].init = 0;\r\nif ((c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xe)) &&\r\ncfg->monarch_timeout < 0)\r\ncfg->monarch_timeout = USEC_PER_SEC;\r\nif (c->x86 == 6 && c->x86_model <= 13 && cfg->bootlog < 0)\r\ncfg->bootlog = 0;\r\nif (c->x86 == 6 && c->x86_model == 45)\r\nquirk_no_way_out = quirk_sandybridge_ifu;\r\n}\r\nif (cfg->monarch_timeout < 0)\r\ncfg->monarch_timeout = 0;\r\nif (cfg->bootlog != 0)\r\ncfg->panic_timeout = 30;\r\nreturn 0;\r\n}\r\nstatic int __mcheck_cpu_ancient_init(struct cpuinfo_x86 *c)\r\n{\r\nif (c->x86 != 5)\r\nreturn 0;\r\nswitch (c->x86_vendor) {\r\ncase X86_VENDOR_INTEL:\r\nintel_p5_mcheck_init(c);\r\nreturn 1;\r\nbreak;\r\ncase X86_VENDOR_CENTAUR:\r\nwinchip_mcheck_init(c);\r\nreturn 1;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __mcheck_cpu_init_vendor(struct cpuinfo_x86 *c)\r\n{\r\nswitch (c->x86_vendor) {\r\ncase X86_VENDOR_INTEL:\r\nmce_intel_feature_init(c);\r\nmce_adjust_timer = mce_intel_adjust_timer;\r\nbreak;\r\ncase X86_VENDOR_AMD:\r\nmce_amd_feature_init(c);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void mce_start_timer(unsigned int cpu, struct timer_list *t)\r\n{\r\nunsigned long iv = mce_adjust_timer(check_interval * HZ);\r\n__this_cpu_write(mce_next_interval, iv);\r\nif (mca_cfg.ignore_ce || !iv)\r\nreturn;\r\nt->expires = round_jiffies(jiffies + iv);\r\nadd_timer_on(t, smp_processor_id());\r\n}\r\nstatic void __mcheck_cpu_init_timer(void)\r\n{\r\nstruct timer_list *t = &__get_cpu_var(mce_timer);\r\nunsigned int cpu = smp_processor_id();\r\nsetup_timer(t, mce_timer_fn, cpu);\r\nmce_start_timer(cpu, t);\r\n}\r\nstatic void unexpected_machine_check(struct pt_regs *regs, long error_code)\r\n{\r\npr_err("CPU#%d: Unexpected int18 (Machine Check)\n",\r\nsmp_processor_id());\r\n}\r\nvoid mcheck_cpu_init(struct cpuinfo_x86 *c)\r\n{\r\nif (mca_cfg.disabled)\r\nreturn;\r\nif (__mcheck_cpu_ancient_init(c))\r\nreturn;\r\nif (!mce_available(c))\r\nreturn;\r\nif (__mcheck_cpu_cap_init() < 0 || __mcheck_cpu_apply_quirks(c) < 0) {\r\nmca_cfg.disabled = true;\r\nreturn;\r\n}\r\nmachine_check_vector = do_machine_check;\r\n__mcheck_cpu_init_generic();\r\n__mcheck_cpu_init_vendor(c);\r\n__mcheck_cpu_init_timer();\r\nINIT_WORK(&__get_cpu_var(mce_work), mce_process_work);\r\ninit_irq_work(&__get_cpu_var(mce_irq_work), &mce_irq_work_cb);\r\n}\r\nstatic int mce_chrdev_open(struct inode *inode, struct file *file)\r\n{\r\nspin_lock(&mce_chrdev_state_lock);\r\nif (mce_chrdev_open_exclu ||\r\n(mce_chrdev_open_count && (file->f_flags & O_EXCL))) {\r\nspin_unlock(&mce_chrdev_state_lock);\r\nreturn -EBUSY;\r\n}\r\nif (file->f_flags & O_EXCL)\r\nmce_chrdev_open_exclu = 1;\r\nmce_chrdev_open_count++;\r\nspin_unlock(&mce_chrdev_state_lock);\r\nreturn nonseekable_open(inode, file);\r\n}\r\nstatic int mce_chrdev_release(struct inode *inode, struct file *file)\r\n{\r\nspin_lock(&mce_chrdev_state_lock);\r\nmce_chrdev_open_count--;\r\nmce_chrdev_open_exclu = 0;\r\nspin_unlock(&mce_chrdev_state_lock);\r\nreturn 0;\r\n}\r\nstatic void collect_tscs(void *data)\r\n{\r\nunsigned long *cpu_tsc = (unsigned long *)data;\r\nrdtscll(cpu_tsc[smp_processor_id()]);\r\n}\r\nstatic int __mce_read_apei(char __user **ubuf, size_t usize)\r\n{\r\nint rc;\r\nu64 record_id;\r\nstruct mce m;\r\nif (usize < sizeof(struct mce))\r\nreturn -EINVAL;\r\nrc = apei_read_mce(&m, &record_id);\r\nif (rc <= 0) {\r\nmce_apei_read_done = 1;\r\nif (rc == -ENODEV)\r\nreturn 0;\r\nreturn rc;\r\n}\r\nrc = -EFAULT;\r\nif (copy_to_user(*ubuf, &m, sizeof(struct mce)))\r\nreturn rc;\r\nrc = apei_clear_mce(record_id);\r\nif (rc) {\r\nmce_apei_read_done = 1;\r\nreturn rc;\r\n}\r\n*ubuf += sizeof(struct mce);\r\nreturn 0;\r\n}\r\nstatic ssize_t mce_chrdev_read(struct file *filp, char __user *ubuf,\r\nsize_t usize, loff_t *off)\r\n{\r\nchar __user *buf = ubuf;\r\nunsigned long *cpu_tsc;\r\nunsigned prev, next;\r\nint i, err;\r\ncpu_tsc = kmalloc(nr_cpu_ids * sizeof(long), GFP_KERNEL);\r\nif (!cpu_tsc)\r\nreturn -ENOMEM;\r\nmutex_lock(&mce_chrdev_read_mutex);\r\nif (!mce_apei_read_done) {\r\nerr = __mce_read_apei(&buf, usize);\r\nif (err || buf != ubuf)\r\ngoto out;\r\n}\r\nnext = rcu_dereference_check_mce(mcelog.next);\r\nerr = -EINVAL;\r\nif (*off != 0 || usize < MCE_LOG_LEN*sizeof(struct mce))\r\ngoto out;\r\nerr = 0;\r\nprev = 0;\r\ndo {\r\nfor (i = prev; i < next; i++) {\r\nunsigned long start = jiffies;\r\nstruct mce *m = &mcelog.entry[i];\r\nwhile (!m->finished) {\r\nif (time_after_eq(jiffies, start + 2)) {\r\nmemset(m, 0, sizeof(*m));\r\ngoto timeout;\r\n}\r\ncpu_relax();\r\n}\r\nsmp_rmb();\r\nerr |= copy_to_user(buf, m, sizeof(*m));\r\nbuf += sizeof(*m);\r\ntimeout:\r\n;\r\n}\r\nmemset(mcelog.entry + prev, 0,\r\n(next - prev) * sizeof(struct mce));\r\nprev = next;\r\nnext = cmpxchg(&mcelog.next, prev, 0);\r\n} while (next != prev);\r\nsynchronize_sched();\r\non_each_cpu(collect_tscs, cpu_tsc, 1);\r\nfor (i = next; i < MCE_LOG_LEN; i++) {\r\nstruct mce *m = &mcelog.entry[i];\r\nif (m->finished && m->tsc < cpu_tsc[m->cpu]) {\r\nerr |= copy_to_user(buf, m, sizeof(*m));\r\nsmp_rmb();\r\nbuf += sizeof(*m);\r\nmemset(m, 0, sizeof(*m));\r\n}\r\n}\r\nif (err)\r\nerr = -EFAULT;\r\nout:\r\nmutex_unlock(&mce_chrdev_read_mutex);\r\nkfree(cpu_tsc);\r\nreturn err ? err : buf - ubuf;\r\n}\r\nstatic unsigned int mce_chrdev_poll(struct file *file, poll_table *wait)\r\n{\r\npoll_wait(file, &mce_chrdev_wait, wait);\r\nif (rcu_access_index(mcelog.next))\r\nreturn POLLIN | POLLRDNORM;\r\nif (!mce_apei_read_done && apei_check_mce())\r\nreturn POLLIN | POLLRDNORM;\r\nreturn 0;\r\n}\r\nstatic long mce_chrdev_ioctl(struct file *f, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nint __user *p = (int __user *)arg;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nswitch (cmd) {\r\ncase MCE_GET_RECORD_LEN:\r\nreturn put_user(sizeof(struct mce), p);\r\ncase MCE_GET_LOG_LEN:\r\nreturn put_user(MCE_LOG_LEN, p);\r\ncase MCE_GETCLEAR_FLAGS: {\r\nunsigned flags;\r\ndo {\r\nflags = mcelog.flags;\r\n} while (cmpxchg(&mcelog.flags, flags, 0) != flags);\r\nreturn put_user(flags, p);\r\n}\r\ndefault:\r\nreturn -ENOTTY;\r\n}\r\n}\r\nvoid register_mce_write_callback(ssize_t (*fn)(struct file *filp,\r\nconst char __user *ubuf,\r\nsize_t usize, loff_t *off))\r\n{\r\nmce_write = fn;\r\n}\r\nssize_t mce_chrdev_write(struct file *filp, const char __user *ubuf,\r\nsize_t usize, loff_t *off)\r\n{\r\nif (mce_write)\r\nreturn mce_write(filp, ubuf, usize, off);\r\nelse\r\nreturn -EINVAL;\r\n}\r\nstatic void __mce_disable_bank(void *arg)\r\n{\r\nint bank = *((int *)arg);\r\n__clear_bit(bank, __get_cpu_var(mce_poll_banks));\r\ncmci_disable_bank(bank);\r\n}\r\nvoid mce_disable_bank(int bank)\r\n{\r\nif (bank >= mca_cfg.banks) {\r\npr_warn(FW_BUG\r\n"Ignoring request to disable invalid MCA bank %d.\n",\r\nbank);\r\nreturn;\r\n}\r\nset_bit(bank, mce_banks_ce_disabled);\r\non_each_cpu(__mce_disable_bank, &bank, 1);\r\n}\r\nstatic int __init mcheck_enable(char *str)\r\n{\r\nstruct mca_config *cfg = &mca_cfg;\r\nif (*str == 0) {\r\nenable_p5_mce();\r\nreturn 1;\r\n}\r\nif (*str == '=')\r\nstr++;\r\nif (!strcmp(str, "off"))\r\ncfg->disabled = true;\r\nelse if (!strcmp(str, "no_cmci"))\r\ncfg->cmci_disabled = true;\r\nelse if (!strcmp(str, "dont_log_ce"))\r\ncfg->dont_log_ce = true;\r\nelse if (!strcmp(str, "ignore_ce"))\r\ncfg->ignore_ce = true;\r\nelse if (!strcmp(str, "bootlog") || !strcmp(str, "nobootlog"))\r\ncfg->bootlog = (str[0] == 'b');\r\nelse if (!strcmp(str, "bios_cmci_threshold"))\r\ncfg->bios_cmci_threshold = true;\r\nelse if (isdigit(str[0])) {\r\nget_option(&str, &(cfg->tolerant));\r\nif (*str == ',') {\r\n++str;\r\nget_option(&str, &(cfg->monarch_timeout));\r\n}\r\n} else {\r\npr_info("mce argument %s ignored. Please use /sys\n", str);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nint __init mcheck_init(void)\r\n{\r\nmcheck_intel_therm_init();\r\nreturn 0;\r\n}\r\nstatic int mce_disable_error_reporting(void)\r\n{\r\nint i;\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nstruct mce_bank *b = &mce_banks[i];\r\nif (b->init)\r\nwrmsrl(MSR_IA32_MCx_CTL(i), 0);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mce_syscore_suspend(void)\r\n{\r\nreturn mce_disable_error_reporting();\r\n}\r\nstatic void mce_syscore_shutdown(void)\r\n{\r\nmce_disable_error_reporting();\r\n}\r\nstatic void mce_syscore_resume(void)\r\n{\r\n__mcheck_cpu_init_generic();\r\n__mcheck_cpu_init_vendor(__this_cpu_ptr(&cpu_info));\r\n}\r\nstatic void mce_cpu_restart(void *data)\r\n{\r\nif (!mce_available(__this_cpu_ptr(&cpu_info)))\r\nreturn;\r\n__mcheck_cpu_init_generic();\r\n__mcheck_cpu_init_timer();\r\n}\r\nstatic void mce_restart(void)\r\n{\r\nmce_timer_delete_all();\r\non_each_cpu(mce_cpu_restart, NULL, 1);\r\n}\r\nstatic void mce_disable_cmci(void *data)\r\n{\r\nif (!mce_available(__this_cpu_ptr(&cpu_info)))\r\nreturn;\r\ncmci_clear();\r\n}\r\nstatic void mce_enable_ce(void *all)\r\n{\r\nif (!mce_available(__this_cpu_ptr(&cpu_info)))\r\nreturn;\r\ncmci_reenable();\r\ncmci_recheck();\r\nif (all)\r\n__mcheck_cpu_init_timer();\r\n}\r\nstatic inline struct mce_bank *attr_to_bank(struct device_attribute *attr)\r\n{\r\nreturn container_of(attr, struct mce_bank, attr);\r\n}\r\nstatic ssize_t show_bank(struct device *s, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%llx\n", attr_to_bank(attr)->ctl);\r\n}\r\nstatic ssize_t set_bank(struct device *s, struct device_attribute *attr,\r\nconst char *buf, size_t size)\r\n{\r\nu64 new;\r\nif (strict_strtoull(buf, 0, &new) < 0)\r\nreturn -EINVAL;\r\nattr_to_bank(attr)->ctl = new;\r\nmce_restart();\r\nreturn size;\r\n}\r\nstatic ssize_t\r\nshow_trigger(struct device *s, struct device_attribute *attr, char *buf)\r\n{\r\nstrcpy(buf, mce_helper);\r\nstrcat(buf, "\n");\r\nreturn strlen(mce_helper) + 1;\r\n}\r\nstatic ssize_t set_trigger(struct device *s, struct device_attribute *attr,\r\nconst char *buf, size_t siz)\r\n{\r\nchar *p;\r\nstrncpy(mce_helper, buf, sizeof(mce_helper));\r\nmce_helper[sizeof(mce_helper)-1] = 0;\r\np = strchr(mce_helper, '\n');\r\nif (p)\r\n*p = 0;\r\nreturn strlen(mce_helper) + !!p;\r\n}\r\nstatic ssize_t set_ignore_ce(struct device *s,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t size)\r\n{\r\nu64 new;\r\nif (strict_strtoull(buf, 0, &new) < 0)\r\nreturn -EINVAL;\r\nif (mca_cfg.ignore_ce ^ !!new) {\r\nif (new) {\r\nmce_timer_delete_all();\r\non_each_cpu(mce_disable_cmci, NULL, 1);\r\nmca_cfg.ignore_ce = true;\r\n} else {\r\nmca_cfg.ignore_ce = false;\r\non_each_cpu(mce_enable_ce, (void *)1, 1);\r\n}\r\n}\r\nreturn size;\r\n}\r\nstatic ssize_t set_cmci_disabled(struct device *s,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t size)\r\n{\r\nu64 new;\r\nif (strict_strtoull(buf, 0, &new) < 0)\r\nreturn -EINVAL;\r\nif (mca_cfg.cmci_disabled ^ !!new) {\r\nif (new) {\r\non_each_cpu(mce_disable_cmci, NULL, 1);\r\nmca_cfg.cmci_disabled = true;\r\n} else {\r\nmca_cfg.cmci_disabled = false;\r\non_each_cpu(mce_enable_ce, NULL, 1);\r\n}\r\n}\r\nreturn size;\r\n}\r\nstatic ssize_t store_int_with_restart(struct device *s,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t size)\r\n{\r\nssize_t ret = device_store_int(s, attr, buf, size);\r\nmce_restart();\r\nreturn ret;\r\n}\r\nstatic void mce_device_release(struct device *dev)\r\n{\r\nkfree(dev);\r\n}\r\nstatic int mce_device_create(unsigned int cpu)\r\n{\r\nstruct device *dev;\r\nint err;\r\nint i, j;\r\nif (!mce_available(&boot_cpu_data))\r\nreturn -EIO;\r\ndev = kzalloc(sizeof *dev, GFP_KERNEL);\r\nif (!dev)\r\nreturn -ENOMEM;\r\ndev->id = cpu;\r\ndev->bus = &mce_subsys;\r\ndev->release = &mce_device_release;\r\nerr = device_register(dev);\r\nif (err)\r\nreturn err;\r\nfor (i = 0; mce_device_attrs[i]; i++) {\r\nerr = device_create_file(dev, mce_device_attrs[i]);\r\nif (err)\r\ngoto error;\r\n}\r\nfor (j = 0; j < mca_cfg.banks; j++) {\r\nerr = device_create_file(dev, &mce_banks[j].attr);\r\nif (err)\r\ngoto error2;\r\n}\r\ncpumask_set_cpu(cpu, mce_device_initialized);\r\nper_cpu(mce_device, cpu) = dev;\r\nreturn 0;\r\nerror2:\r\nwhile (--j >= 0)\r\ndevice_remove_file(dev, &mce_banks[j].attr);\r\nerror:\r\nwhile (--i >= 0)\r\ndevice_remove_file(dev, mce_device_attrs[i]);\r\ndevice_unregister(dev);\r\nreturn err;\r\n}\r\nstatic void mce_device_remove(unsigned int cpu)\r\n{\r\nstruct device *dev = per_cpu(mce_device, cpu);\r\nint i;\r\nif (!cpumask_test_cpu(cpu, mce_device_initialized))\r\nreturn;\r\nfor (i = 0; mce_device_attrs[i]; i++)\r\ndevice_remove_file(dev, mce_device_attrs[i]);\r\nfor (i = 0; i < mca_cfg.banks; i++)\r\ndevice_remove_file(dev, &mce_banks[i].attr);\r\ndevice_unregister(dev);\r\ncpumask_clear_cpu(cpu, mce_device_initialized);\r\nper_cpu(mce_device, cpu) = NULL;\r\n}\r\nstatic void mce_disable_cpu(void *h)\r\n{\r\nunsigned long action = *(unsigned long *)h;\r\nint i;\r\nif (!mce_available(__this_cpu_ptr(&cpu_info)))\r\nreturn;\r\nif (!(action & CPU_TASKS_FROZEN))\r\ncmci_clear();\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nstruct mce_bank *b = &mce_banks[i];\r\nif (b->init)\r\nwrmsrl(MSR_IA32_MCx_CTL(i), 0);\r\n}\r\n}\r\nstatic void mce_reenable_cpu(void *h)\r\n{\r\nunsigned long action = *(unsigned long *)h;\r\nint i;\r\nif (!mce_available(__this_cpu_ptr(&cpu_info)))\r\nreturn;\r\nif (!(action & CPU_TASKS_FROZEN))\r\ncmci_reenable();\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nstruct mce_bank *b = &mce_banks[i];\r\nif (b->init)\r\nwrmsrl(MSR_IA32_MCx_CTL(i), b->ctl);\r\n}\r\n}\r\nstatic int\r\nmce_cpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (unsigned long)hcpu;\r\nstruct timer_list *t = &per_cpu(mce_timer, cpu);\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_ONLINE:\r\nmce_device_create(cpu);\r\nif (threshold_cpu_callback)\r\nthreshold_cpu_callback(action, cpu);\r\nbreak;\r\ncase CPU_DEAD:\r\nif (threshold_cpu_callback)\r\nthreshold_cpu_callback(action, cpu);\r\nmce_device_remove(cpu);\r\nmce_intel_hcpu_update(cpu);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\nsmp_call_function_single(cpu, mce_disable_cpu, &action, 1);\r\ndel_timer_sync(t);\r\nbreak;\r\ncase CPU_DOWN_FAILED:\r\nsmp_call_function_single(cpu, mce_reenable_cpu, &action, 1);\r\nmce_start_timer(cpu, t);\r\nbreak;\r\n}\r\nif (action == CPU_POST_DEAD) {\r\ncmci_rediscover();\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic __init void mce_init_banks(void)\r\n{\r\nint i;\r\nfor (i = 0; i < mca_cfg.banks; i++) {\r\nstruct mce_bank *b = &mce_banks[i];\r\nstruct device_attribute *a = &b->attr;\r\nsysfs_attr_init(&a->attr);\r\na->attr.name = b->attrname;\r\nsnprintf(b->attrname, ATTR_LEN, "bank%d", i);\r\na->attr.mode = 0644;\r\na->show = show_bank;\r\na->store = set_bank;\r\n}\r\n}\r\nstatic __init int mcheck_init_device(void)\r\n{\r\nint err;\r\nint i = 0;\r\nif (!mce_available(&boot_cpu_data))\r\nreturn -EIO;\r\nzalloc_cpumask_var(&mce_device_initialized, GFP_KERNEL);\r\nmce_init_banks();\r\nerr = subsys_system_register(&mce_subsys, NULL);\r\nif (err)\r\nreturn err;\r\nfor_each_online_cpu(i) {\r\nerr = mce_device_create(i);\r\nif (err)\r\nreturn err;\r\n}\r\nregister_syscore_ops(&mce_syscore_ops);\r\nregister_hotcpu_notifier(&mce_cpu_notifier);\r\nmisc_register(&mce_chrdev_device);\r\nreturn err;\r\n}\r\nstatic int __init mcheck_disable(char *str)\r\n{\r\nmca_cfg.disabled = true;\r\nreturn 1;\r\n}\r\nstruct dentry *mce_get_debugfs_dir(void)\r\n{\r\nstatic struct dentry *dmce;\r\nif (!dmce)\r\ndmce = debugfs_create_dir("mce", NULL);\r\nreturn dmce;\r\n}\r\nstatic void mce_reset(void)\r\n{\r\ncpu_missing = 0;\r\natomic_set(&mce_fake_paniced, 0);\r\natomic_set(&mce_executing, 0);\r\natomic_set(&mce_callin, 0);\r\natomic_set(&global_nwo, 0);\r\n}\r\nstatic int fake_panic_get(void *data, u64 *val)\r\n{\r\n*val = fake_panic;\r\nreturn 0;\r\n}\r\nstatic int fake_panic_set(void *data, u64 val)\r\n{\r\nmce_reset();\r\nfake_panic = val;\r\nreturn 0;\r\n}\r\nstatic int __init mcheck_debugfs_init(void)\r\n{\r\nstruct dentry *dmce, *ffake_panic;\r\ndmce = mce_get_debugfs_dir();\r\nif (!dmce)\r\nreturn -ENOMEM;\r\nffake_panic = debugfs_create_file("fake_panic", 0444, dmce, NULL,\r\n&fake_panic_fops);\r\nif (!ffake_panic)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}
