static struct irq_info *info_for_irq(unsigned irq)\r\n{\r\nreturn irq_get_handler_data(irq);\r\n}\r\nstatic void xen_irq_info_common_init(struct irq_info *info,\r\nunsigned irq,\r\nenum xen_irq_type type,\r\nunsigned short evtchn,\r\nunsigned short cpu)\r\n{\r\nBUG_ON(info->type != IRQT_UNBOUND && info->type != type);\r\ninfo->type = type;\r\ninfo->irq = irq;\r\ninfo->evtchn = evtchn;\r\ninfo->cpu = cpu;\r\nevtchn_to_irq[evtchn] = irq;\r\nirq_clear_status_flags(irq, IRQ_NOREQUEST|IRQ_NOAUTOEN);\r\n}\r\nstatic void xen_irq_info_evtchn_init(unsigned irq,\r\nunsigned short evtchn)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nxen_irq_info_common_init(info, irq, IRQT_EVTCHN, evtchn, 0);\r\n}\r\nstatic void xen_irq_info_ipi_init(unsigned cpu,\r\nunsigned irq,\r\nunsigned short evtchn,\r\nenum ipi_vector ipi)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nxen_irq_info_common_init(info, irq, IRQT_IPI, evtchn, 0);\r\ninfo->u.ipi = ipi;\r\nper_cpu(ipi_to_irq, cpu)[ipi] = irq;\r\n}\r\nstatic void xen_irq_info_virq_init(unsigned cpu,\r\nunsigned irq,\r\nunsigned short evtchn,\r\nunsigned short virq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nxen_irq_info_common_init(info, irq, IRQT_VIRQ, evtchn, 0);\r\ninfo->u.virq = virq;\r\nper_cpu(virq_to_irq, cpu)[virq] = irq;\r\n}\r\nstatic void xen_irq_info_pirq_init(unsigned irq,\r\nunsigned short evtchn,\r\nunsigned short pirq,\r\nunsigned short gsi,\r\nuint16_t domid,\r\nunsigned char flags)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nxen_irq_info_common_init(info, irq, IRQT_PIRQ, evtchn, 0);\r\ninfo->u.pirq.pirq = pirq;\r\ninfo->u.pirq.gsi = gsi;\r\ninfo->u.pirq.domid = domid;\r\ninfo->u.pirq.flags = flags;\r\n}\r\nstatic unsigned int evtchn_from_irq(unsigned irq)\r\n{\r\nif (unlikely(WARN(irq < 0 || irq >= nr_irqs, "Invalid irq %d!\n", irq)))\r\nreturn 0;\r\nreturn info_for_irq(irq)->evtchn;\r\n}\r\nunsigned irq_from_evtchn(unsigned int evtchn)\r\n{\r\nreturn evtchn_to_irq[evtchn];\r\n}\r\nstatic enum ipi_vector ipi_from_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info == NULL);\r\nBUG_ON(info->type != IRQT_IPI);\r\nreturn info->u.ipi;\r\n}\r\nstatic unsigned virq_from_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info == NULL);\r\nBUG_ON(info->type != IRQT_VIRQ);\r\nreturn info->u.virq;\r\n}\r\nstatic unsigned pirq_from_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info == NULL);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nreturn info->u.pirq.pirq;\r\n}\r\nstatic enum xen_irq_type type_from_irq(unsigned irq)\r\n{\r\nreturn info_for_irq(irq)->type;\r\n}\r\nstatic unsigned cpu_from_irq(unsigned irq)\r\n{\r\nreturn info_for_irq(irq)->cpu;\r\n}\r\nstatic unsigned int cpu_from_evtchn(unsigned int evtchn)\r\n{\r\nint irq = evtchn_to_irq[evtchn];\r\nunsigned ret = 0;\r\nif (irq != -1)\r\nret = cpu_from_irq(irq);\r\nreturn ret;\r\n}\r\nstatic bool pirq_check_eoi_map(unsigned irq)\r\n{\r\nreturn test_bit(pirq_from_irq(irq), pirq_eoi_map);\r\n}\r\nstatic bool pirq_needs_eoi_flag(unsigned irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nreturn info->u.pirq.flags & PIRQ_NEEDS_EOI;\r\n}\r\nstatic inline xen_ulong_t active_evtchns(unsigned int cpu,\r\nstruct shared_info *sh,\r\nunsigned int idx)\r\n{\r\nreturn sh->evtchn_pending[idx] &\r\nper_cpu(cpu_evtchn_mask, cpu)[idx] &\r\n~sh->evtchn_mask[idx];\r\n}\r\nstatic void bind_evtchn_to_cpu(unsigned int chn, unsigned int cpu)\r\n{\r\nint irq = evtchn_to_irq[chn];\r\nBUG_ON(irq == -1);\r\n#ifdef CONFIG_SMP\r\ncpumask_copy(irq_to_desc(irq)->irq_data.affinity, cpumask_of(cpu));\r\n#endif\r\nclear_bit(chn, BM(per_cpu(cpu_evtchn_mask, cpu_from_irq(irq))));\r\nset_bit(chn, BM(per_cpu(cpu_evtchn_mask, cpu)));\r\ninfo_for_irq(irq)->cpu = cpu;\r\n}\r\nstatic void init_evtchn_cpu_bindings(void)\r\n{\r\nint i;\r\n#ifdef CONFIG_SMP\r\nstruct irq_info *info;\r\nlist_for_each_entry(info, &xen_irq_list_head, list) {\r\nstruct irq_desc *desc = irq_to_desc(info->irq);\r\ncpumask_copy(desc->irq_data.affinity, cpumask_of(0));\r\n}\r\n#endif\r\nfor_each_possible_cpu(i)\r\nmemset(per_cpu(cpu_evtchn_mask, i),\r\n(i == 0) ? ~0 : 0, NR_EVENT_CHANNELS/8);\r\n}\r\nstatic inline void clear_evtchn(int port)\r\n{\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nsync_clear_bit(port, BM(&s->evtchn_pending[0]));\r\n}\r\nstatic inline void set_evtchn(int port)\r\n{\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nsync_set_bit(port, BM(&s->evtchn_pending[0]));\r\n}\r\nstatic inline int test_evtchn(int port)\r\n{\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nreturn sync_test_bit(port, BM(&s->evtchn_pending[0]));\r\n}\r\nvoid notify_remote_via_irq(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn))\r\nnotify_remote_via_evtchn(evtchn);\r\n}\r\nstatic void mask_evtchn(int port)\r\n{\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nsync_set_bit(port, BM(&s->evtchn_mask[0]));\r\n}\r\nstatic void unmask_evtchn(int port)\r\n{\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nunsigned int cpu = get_cpu();\r\nint do_hypercall = 0, evtchn_pending = 0;\r\nBUG_ON(!irqs_disabled());\r\nif (unlikely((cpu != cpu_from_evtchn(port))))\r\ndo_hypercall = 1;\r\nelse {\r\nsync_clear_bit(port, BM(&s->evtchn_mask[0]));\r\nevtchn_pending = sync_test_bit(port, BM(&s->evtchn_pending[0]));\r\nif (unlikely(evtchn_pending && xen_hvm_domain())) {\r\nsync_set_bit(port, BM(&s->evtchn_mask[0]));\r\ndo_hypercall = 1;\r\n}\r\n}\r\nif (do_hypercall) {\r\nstruct evtchn_unmask unmask = { .port = port };\r\n(void)HYPERVISOR_event_channel_op(EVTCHNOP_unmask, &unmask);\r\n} else {\r\nstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\r\nif (evtchn_pending &&\r\n!sync_test_and_set_bit(port / BITS_PER_EVTCHN_WORD,\r\nBM(&vcpu_info->evtchn_pending_sel)))\r\nvcpu_info->evtchn_upcall_pending = 1;\r\n}\r\nput_cpu();\r\n}\r\nstatic void xen_irq_init(unsigned irq)\r\n{\r\nstruct irq_info *info;\r\n#ifdef CONFIG_SMP\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\ncpumask_copy(desc->irq_data.affinity, cpumask_of(0));\r\n#endif\r\ninfo = kzalloc(sizeof(*info), GFP_KERNEL);\r\nif (info == NULL)\r\npanic("Unable to allocate metadata for IRQ%d\n", irq);\r\ninfo->type = IRQT_UNBOUND;\r\ninfo->refcnt = -1;\r\nirq_set_handler_data(irq, info);\r\nlist_add_tail(&info->list, &xen_irq_list_head);\r\n}\r\nstatic int __must_check xen_allocate_irq_dynamic(void)\r\n{\r\nint first = 0;\r\nint irq;\r\n#ifdef CONFIG_X86_IO_APIC\r\nif (xen_initial_domain() || xen_hvm_domain())\r\nfirst = get_nr_irqs_gsi();\r\n#endif\r\nirq = irq_alloc_desc_from(first, -1);\r\nif (irq >= 0)\r\nxen_irq_init(irq);\r\nreturn irq;\r\n}\r\nstatic int __must_check xen_allocate_irq_gsi(unsigned gsi)\r\n{\r\nint irq;\r\nif (xen_pv_domain() && !xen_initial_domain())\r\nreturn xen_allocate_irq_dynamic();\r\nif (gsi < NR_IRQS_LEGACY)\r\nirq = gsi;\r\nelse\r\nirq = irq_alloc_desc_at(gsi, -1);\r\nxen_irq_init(irq);\r\nreturn irq;\r\n}\r\nstatic void xen_free_irq(unsigned irq)\r\n{\r\nstruct irq_info *info = irq_get_handler_data(irq);\r\nif (WARN_ON(!info))\r\nreturn;\r\nlist_del(&info->list);\r\nirq_set_handler_data(irq, NULL);\r\nWARN_ON(info->refcnt > 0);\r\nkfree(info);\r\nif (irq < NR_IRQS_LEGACY)\r\nreturn;\r\nirq_free_desc(irq);\r\n}\r\nstatic void pirq_query_unmask(int irq)\r\n{\r\nstruct physdev_irq_status_query irq_status;\r\nstruct irq_info *info = info_for_irq(irq);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nirq_status.irq = pirq_from_irq(irq);\r\nif (HYPERVISOR_physdev_op(PHYSDEVOP_irq_status_query, &irq_status))\r\nirq_status.flags = 0;\r\ninfo->u.pirq.flags &= ~PIRQ_NEEDS_EOI;\r\nif (irq_status.flags & XENIRQSTAT_needs_eoi)\r\ninfo->u.pirq.flags |= PIRQ_NEEDS_EOI;\r\n}\r\nstatic bool probing_irq(int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nreturn desc && desc->action == NULL;\r\n}\r\nstatic void eoi_pirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nstruct physdev_eoi eoi = { .irq = pirq_from_irq(data->irq) };\r\nint rc = 0;\r\nirq_move_irq(data);\r\nif (VALID_EVTCHN(evtchn))\r\nclear_evtchn(evtchn);\r\nif (pirq_needs_eoi(data->irq)) {\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_eoi, &eoi);\r\nWARN_ON(rc);\r\n}\r\n}\r\nstatic void mask_ack_pirq(struct irq_data *data)\r\n{\r\ndisable_dynirq(data);\r\neoi_pirq(data);\r\n}\r\nstatic unsigned int __startup_pirq(unsigned int irq)\r\n{\r\nstruct evtchn_bind_pirq bind_pirq;\r\nstruct irq_info *info = info_for_irq(irq);\r\nint evtchn = evtchn_from_irq(irq);\r\nint rc;\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nif (VALID_EVTCHN(evtchn))\r\ngoto out;\r\nbind_pirq.pirq = pirq_from_irq(irq);\r\nbind_pirq.flags = info->u.pirq.flags & PIRQ_SHAREABLE ?\r\nBIND_PIRQ__WILL_SHARE : 0;\r\nrc = HYPERVISOR_event_channel_op(EVTCHNOP_bind_pirq, &bind_pirq);\r\nif (rc != 0) {\r\nif (!probing_irq(irq))\r\npr_info("Failed to obtain physical IRQ %d\n", irq);\r\nreturn 0;\r\n}\r\nevtchn = bind_pirq.port;\r\npirq_query_unmask(irq);\r\nevtchn_to_irq[evtchn] = irq;\r\nbind_evtchn_to_cpu(evtchn, 0);\r\ninfo->evtchn = evtchn;\r\nout:\r\nunmask_evtchn(evtchn);\r\neoi_pirq(irq_get_irq_data(irq));\r\nreturn 0;\r\n}\r\nstatic unsigned int startup_pirq(struct irq_data *data)\r\n{\r\nreturn __startup_pirq(data->irq);\r\n}\r\nstatic void shutdown_pirq(struct irq_data *data)\r\n{\r\nstruct evtchn_close close;\r\nunsigned int irq = data->irq;\r\nstruct irq_info *info = info_for_irq(irq);\r\nint evtchn = evtchn_from_irq(irq);\r\nBUG_ON(info->type != IRQT_PIRQ);\r\nif (!VALID_EVTCHN(evtchn))\r\nreturn;\r\nmask_evtchn(evtchn);\r\nclose.port = evtchn;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_close, &close) != 0)\r\nBUG();\r\nbind_evtchn_to_cpu(evtchn, 0);\r\nevtchn_to_irq[evtchn] = -1;\r\ninfo->evtchn = 0;\r\n}\r\nstatic void enable_pirq(struct irq_data *data)\r\n{\r\nstartup_pirq(data);\r\n}\r\nstatic void disable_pirq(struct irq_data *data)\r\n{\r\ndisable_dynirq(data);\r\n}\r\nint xen_irq_from_gsi(unsigned gsi)\r\n{\r\nstruct irq_info *info;\r\nlist_for_each_entry(info, &xen_irq_list_head, list) {\r\nif (info->type != IRQT_PIRQ)\r\ncontinue;\r\nif (info->u.pirq.gsi == gsi)\r\nreturn info->irq;\r\n}\r\nreturn -1;\r\n}\r\nint xen_bind_pirq_gsi_to_irq(unsigned gsi,\r\nunsigned pirq, int shareable, char *name)\r\n{\r\nint irq = -1;\r\nstruct physdev_irq irq_op;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = xen_irq_from_gsi(gsi);\r\nif (irq != -1) {\r\npr_info("%s: returning irq %d for gsi %u\n",\r\n__func__, irq, gsi);\r\ngoto out;\r\n}\r\nirq = xen_allocate_irq_gsi(gsi);\r\nif (irq < 0)\r\ngoto out;\r\nirq_op.irq = irq;\r\nirq_op.vector = 0;\r\nif (xen_initial_domain() &&\r\nHYPERVISOR_physdev_op(PHYSDEVOP_alloc_irq_vector, &irq_op)) {\r\nxen_free_irq(irq);\r\nirq = -ENOSPC;\r\ngoto out;\r\n}\r\nxen_irq_info_pirq_init(irq, 0, pirq, gsi, DOMID_SELF,\r\nshareable ? PIRQ_SHAREABLE : 0);\r\npirq_query_unmask(irq);\r\nif (shareable)\r\nirq_set_chip_and_handler_name(irq, &xen_pirq_chip,\r\nhandle_fasteoi_irq, name);\r\nelse\r\nirq_set_chip_and_handler_name(irq, &xen_pirq_chip,\r\nhandle_edge_irq, name);\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nint xen_allocate_pirq_msi(struct pci_dev *dev, struct msi_desc *msidesc)\r\n{\r\nint rc;\r\nstruct physdev_get_free_pirq op_get_free_pirq;\r\nop_get_free_pirq.type = MAP_PIRQ_TYPE_MSI;\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_get_free_pirq, &op_get_free_pirq);\r\nWARN_ONCE(rc == -ENOSYS,\r\n"hypervisor does not support the PHYSDEVOP_get_free_pirq interface\n");\r\nreturn rc ? -1 : op_get_free_pirq.pirq;\r\n}\r\nint xen_bind_pirq_msi_to_irq(struct pci_dev *dev, struct msi_desc *msidesc,\r\nint pirq, const char *name, domid_t domid)\r\n{\r\nint irq, ret;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = xen_allocate_irq_dynamic();\r\nif (irq < 0)\r\ngoto out;\r\nirq_set_chip_and_handler_name(irq, &xen_pirq_chip, handle_edge_irq,\r\nname);\r\nxen_irq_info_pirq_init(irq, 0, pirq, 0, domid, 0);\r\nret = irq_set_msi_desc(irq, msidesc);\r\nif (ret < 0)\r\ngoto error_irq;\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\nerror_irq:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nxen_free_irq(irq);\r\nreturn ret;\r\n}\r\nint xen_destroy_irq(int irq)\r\n{\r\nstruct irq_desc *desc;\r\nstruct physdev_unmap_pirq unmap_irq;\r\nstruct irq_info *info = info_for_irq(irq);\r\nint rc = -ENOENT;\r\nmutex_lock(&irq_mapping_update_lock);\r\ndesc = irq_to_desc(irq);\r\nif (!desc)\r\ngoto out;\r\nif (xen_initial_domain()) {\r\nunmap_irq.pirq = info->u.pirq.pirq;\r\nunmap_irq.domid = info->u.pirq.domid;\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_unmap_pirq, &unmap_irq);\r\nif ((rc == -ESRCH && info->u.pirq.domid != DOMID_SELF))\r\npr_info("domain %d does not have %d anymore\n",\r\ninfo->u.pirq.domid, info->u.pirq.pirq);\r\nelse if (rc) {\r\npr_warn("unmap irq failed %d\n", rc);\r\ngoto out;\r\n}\r\n}\r\nxen_free_irq(irq);\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn rc;\r\n}\r\nint xen_irq_from_pirq(unsigned pirq)\r\n{\r\nint irq;\r\nstruct irq_info *info;\r\nmutex_lock(&irq_mapping_update_lock);\r\nlist_for_each_entry(info, &xen_irq_list_head, list) {\r\nif (info->type != IRQT_PIRQ)\r\ncontinue;\r\nirq = info->irq;\r\nif (info->u.pirq.pirq == pirq)\r\ngoto out;\r\n}\r\nirq = -1;\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nint xen_pirq_from_irq(unsigned irq)\r\n{\r\nreturn pirq_from_irq(irq);\r\n}\r\nint bind_evtchn_to_irq(unsigned int evtchn)\r\n{\r\nint irq;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = evtchn_to_irq[evtchn];\r\nif (irq == -1) {\r\nirq = xen_allocate_irq_dynamic();\r\nif (irq < 0)\r\ngoto out;\r\nirq_set_chip_and_handler_name(irq, &xen_dynamic_chip,\r\nhandle_edge_irq, "event");\r\nxen_irq_info_evtchn_init(irq, evtchn);\r\n} else {\r\nstruct irq_info *info = info_for_irq(irq);\r\nWARN_ON(info == NULL || info->type != IRQT_EVTCHN);\r\n}\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nstatic int bind_ipi_to_irq(unsigned int ipi, unsigned int cpu)\r\n{\r\nstruct evtchn_bind_ipi bind_ipi;\r\nint evtchn, irq;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = per_cpu(ipi_to_irq, cpu)[ipi];\r\nif (irq == -1) {\r\nirq = xen_allocate_irq_dynamic();\r\nif (irq < 0)\r\ngoto out;\r\nirq_set_chip_and_handler_name(irq, &xen_percpu_chip,\r\nhandle_percpu_irq, "ipi");\r\nbind_ipi.vcpu = cpu;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_ipi,\r\n&bind_ipi) != 0)\r\nBUG();\r\nevtchn = bind_ipi.port;\r\nxen_irq_info_ipi_init(cpu, irq, evtchn, ipi);\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n} else {\r\nstruct irq_info *info = info_for_irq(irq);\r\nWARN_ON(info == NULL || info->type != IRQT_IPI);\r\n}\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nstatic int bind_interdomain_evtchn_to_irq(unsigned int remote_domain,\r\nunsigned int remote_port)\r\n{\r\nstruct evtchn_bind_interdomain bind_interdomain;\r\nint err;\r\nbind_interdomain.remote_dom = remote_domain;\r\nbind_interdomain.remote_port = remote_port;\r\nerr = HYPERVISOR_event_channel_op(EVTCHNOP_bind_interdomain,\r\n&bind_interdomain);\r\nreturn err ? : bind_evtchn_to_irq(bind_interdomain.local_port);\r\n}\r\nstatic int find_virq(unsigned int virq, unsigned int cpu)\r\n{\r\nstruct evtchn_status status;\r\nint port, rc = -ENOENT;\r\nmemset(&status, 0, sizeof(status));\r\nfor (port = 0; port <= NR_EVENT_CHANNELS; port++) {\r\nstatus.dom = DOMID_SELF;\r\nstatus.port = port;\r\nrc = HYPERVISOR_event_channel_op(EVTCHNOP_status, &status);\r\nif (rc < 0)\r\ncontinue;\r\nif (status.status != EVTCHNSTAT_virq)\r\ncontinue;\r\nif (status.u.virq == virq && status.vcpu == cpu) {\r\nrc = port;\r\nbreak;\r\n}\r\n}\r\nreturn rc;\r\n}\r\nint bind_virq_to_irq(unsigned int virq, unsigned int cpu)\r\n{\r\nstruct evtchn_bind_virq bind_virq;\r\nint evtchn, irq, ret;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = per_cpu(virq_to_irq, cpu)[virq];\r\nif (irq == -1) {\r\nirq = xen_allocate_irq_dynamic();\r\nif (irq < 0)\r\ngoto out;\r\nirq_set_chip_and_handler_name(irq, &xen_percpu_chip,\r\nhandle_percpu_irq, "virq");\r\nbind_virq.virq = virq;\r\nbind_virq.vcpu = cpu;\r\nret = HYPERVISOR_event_channel_op(EVTCHNOP_bind_virq,\r\n&bind_virq);\r\nif (ret == 0)\r\nevtchn = bind_virq.port;\r\nelse {\r\nif (ret == -EEXIST)\r\nret = find_virq(virq, cpu);\r\nBUG_ON(ret < 0);\r\nevtchn = ret;\r\n}\r\nxen_irq_info_virq_init(cpu, irq, evtchn, virq);\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n} else {\r\nstruct irq_info *info = info_for_irq(irq);\r\nWARN_ON(info == NULL || info->type != IRQT_VIRQ);\r\n}\r\nout:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn irq;\r\n}\r\nstatic void unbind_from_irq(unsigned int irq)\r\n{\r\nstruct evtchn_close close;\r\nint evtchn = evtchn_from_irq(irq);\r\nstruct irq_info *info = irq_get_handler_data(irq);\r\nif (WARN_ON(!info))\r\nreturn;\r\nmutex_lock(&irq_mapping_update_lock);\r\nif (info->refcnt > 0) {\r\ninfo->refcnt--;\r\nif (info->refcnt != 0)\r\ngoto done;\r\n}\r\nif (VALID_EVTCHN(evtchn)) {\r\nclose.port = evtchn;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_close, &close) != 0)\r\nBUG();\r\nswitch (type_from_irq(irq)) {\r\ncase IRQT_VIRQ:\r\nper_cpu(virq_to_irq, cpu_from_evtchn(evtchn))\r\n[virq_from_irq(irq)] = -1;\r\nbreak;\r\ncase IRQT_IPI:\r\nper_cpu(ipi_to_irq, cpu_from_evtchn(evtchn))\r\n[ipi_from_irq(irq)] = -1;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbind_evtchn_to_cpu(evtchn, 0);\r\nevtchn_to_irq[evtchn] = -1;\r\n}\r\nBUG_ON(info_for_irq(irq)->type == IRQT_UNBOUND);\r\nxen_free_irq(irq);\r\ndone:\r\nmutex_unlock(&irq_mapping_update_lock);\r\n}\r\nint bind_evtchn_to_irqhandler(unsigned int evtchn,\r\nirq_handler_t handler,\r\nunsigned long irqflags,\r\nconst char *devname, void *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_evtchn_to_irq(evtchn);\r\nif (irq < 0)\r\nreturn irq;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nint bind_interdomain_evtchn_to_irqhandler(unsigned int remote_domain,\r\nunsigned int remote_port,\r\nirq_handler_t handler,\r\nunsigned long irqflags,\r\nconst char *devname,\r\nvoid *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_interdomain_evtchn_to_irq(remote_domain, remote_port);\r\nif (irq < 0)\r\nreturn irq;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nint bind_virq_to_irqhandler(unsigned int virq, unsigned int cpu,\r\nirq_handler_t handler,\r\nunsigned long irqflags, const char *devname, void *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_virq_to_irq(virq, cpu);\r\nif (irq < 0)\r\nreturn irq;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nint bind_ipi_to_irqhandler(enum ipi_vector ipi,\r\nunsigned int cpu,\r\nirq_handler_t handler,\r\nunsigned long irqflags,\r\nconst char *devname,\r\nvoid *dev_id)\r\n{\r\nint irq, retval;\r\nirq = bind_ipi_to_irq(ipi, cpu);\r\nif (irq < 0)\r\nreturn irq;\r\nirqflags |= IRQF_NO_SUSPEND | IRQF_FORCE_RESUME | IRQF_EARLY_RESUME;\r\nretval = request_irq(irq, handler, irqflags, devname, dev_id);\r\nif (retval != 0) {\r\nunbind_from_irq(irq);\r\nreturn retval;\r\n}\r\nreturn irq;\r\n}\r\nvoid unbind_from_irqhandler(unsigned int irq, void *dev_id)\r\n{\r\nstruct irq_info *info = irq_get_handler_data(irq);\r\nif (WARN_ON(!info))\r\nreturn;\r\nfree_irq(irq, dev_id);\r\nunbind_from_irq(irq);\r\n}\r\nint evtchn_make_refcounted(unsigned int evtchn)\r\n{\r\nint irq = evtchn_to_irq[evtchn];\r\nstruct irq_info *info;\r\nif (irq == -1)\r\nreturn -ENOENT;\r\ninfo = irq_get_handler_data(irq);\r\nif (!info)\r\nreturn -ENOENT;\r\nWARN_ON(info->refcnt != -1);\r\ninfo->refcnt = 1;\r\nreturn 0;\r\n}\r\nint evtchn_get(unsigned int evtchn)\r\n{\r\nint irq;\r\nstruct irq_info *info;\r\nint err = -ENOENT;\r\nif (evtchn >= NR_EVENT_CHANNELS)\r\nreturn -EINVAL;\r\nmutex_lock(&irq_mapping_update_lock);\r\nirq = evtchn_to_irq[evtchn];\r\nif (irq == -1)\r\ngoto done;\r\ninfo = irq_get_handler_data(irq);\r\nif (!info)\r\ngoto done;\r\nerr = -EINVAL;\r\nif (info->refcnt <= 0)\r\ngoto done;\r\ninfo->refcnt++;\r\nerr = 0;\r\ndone:\r\nmutex_unlock(&irq_mapping_update_lock);\r\nreturn err;\r\n}\r\nvoid evtchn_put(unsigned int evtchn)\r\n{\r\nint irq = evtchn_to_irq[evtchn];\r\nif (WARN_ON(irq == -1))\r\nreturn;\r\nunbind_from_irq(irq);\r\n}\r\nvoid xen_send_IPI_one(unsigned int cpu, enum ipi_vector vector)\r\n{\r\nint irq;\r\n#ifdef CONFIG_X86\r\nif (unlikely(vector == XEN_NMI_VECTOR)) {\r\nint rc = HYPERVISOR_vcpu_op(VCPUOP_send_nmi, cpu, NULL);\r\nif (rc < 0)\r\nprintk(KERN_WARNING "Sending nmi to CPU%d failed (rc:%d)\n", cpu, rc);\r\nreturn;\r\n}\r\n#endif\r\nirq = per_cpu(ipi_to_irq, cpu)[vector];\r\nBUG_ON(irq < 0);\r\nnotify_remote_via_irq(irq);\r\n}\r\nirqreturn_t xen_debug_interrupt(int irq, void *dev_id)\r\n{\r\nstruct shared_info *sh = HYPERVISOR_shared_info;\r\nint cpu = smp_processor_id();\r\nxen_ulong_t *cpu_evtchn = per_cpu(cpu_evtchn_mask, cpu);\r\nint i;\r\nunsigned long flags;\r\nstatic DEFINE_SPINLOCK(debug_lock);\r\nstruct vcpu_info *v;\r\nspin_lock_irqsave(&debug_lock, flags);\r\nprintk("\nvcpu %d\n ", cpu);\r\nfor_each_online_cpu(i) {\r\nint pending;\r\nv = per_cpu(xen_vcpu, i);\r\npending = (get_irq_regs() && i == cpu)\r\n? xen_irqs_disabled(get_irq_regs())\r\n: v->evtchn_upcall_mask;\r\nprintk("%d: masked=%d pending=%d event_sel %0*"PRI_xen_ulong"\n ", i,\r\npending, v->evtchn_upcall_pending,\r\n(int)(sizeof(v->evtchn_pending_sel)*2),\r\nv->evtchn_pending_sel);\r\n}\r\nv = per_cpu(xen_vcpu, cpu);\r\nprintk("\npending:\n ");\r\nfor (i = ARRAY_SIZE(sh->evtchn_pending)-1; i >= 0; i--)\r\nprintk("%0*"PRI_xen_ulong"%s",\r\n(int)sizeof(sh->evtchn_pending[0])*2,\r\nsh->evtchn_pending[i],\r\ni % 8 == 0 ? "\n " : " ");\r\nprintk("\nglobal mask:\n ");\r\nfor (i = ARRAY_SIZE(sh->evtchn_mask)-1; i >= 0; i--)\r\nprintk("%0*"PRI_xen_ulong"%s",\r\n(int)(sizeof(sh->evtchn_mask[0])*2),\r\nsh->evtchn_mask[i],\r\ni % 8 == 0 ? "\n " : " ");\r\nprintk("\nglobally unmasked:\n ");\r\nfor (i = ARRAY_SIZE(sh->evtchn_mask)-1; i >= 0; i--)\r\nprintk("%0*"PRI_xen_ulong"%s",\r\n(int)(sizeof(sh->evtchn_mask[0])*2),\r\nsh->evtchn_pending[i] & ~sh->evtchn_mask[i],\r\ni % 8 == 0 ? "\n " : " ");\r\nprintk("\nlocal cpu%d mask:\n ", cpu);\r\nfor (i = (NR_EVENT_CHANNELS/BITS_PER_EVTCHN_WORD)-1; i >= 0; i--)\r\nprintk("%0*"PRI_xen_ulong"%s", (int)(sizeof(cpu_evtchn[0])*2),\r\ncpu_evtchn[i],\r\ni % 8 == 0 ? "\n " : " ");\r\nprintk("\nlocally unmasked:\n ");\r\nfor (i = ARRAY_SIZE(sh->evtchn_mask)-1; i >= 0; i--) {\r\nxen_ulong_t pending = sh->evtchn_pending[i]\r\n& ~sh->evtchn_mask[i]\r\n& cpu_evtchn[i];\r\nprintk("%0*"PRI_xen_ulong"%s",\r\n(int)(sizeof(sh->evtchn_mask[0])*2),\r\npending, i % 8 == 0 ? "\n " : " ");\r\n}\r\nprintk("\npending list:\n");\r\nfor (i = 0; i < NR_EVENT_CHANNELS; i++) {\r\nif (sync_test_bit(i, BM(sh->evtchn_pending))) {\r\nint word_idx = i / BITS_PER_EVTCHN_WORD;\r\nprintk(" %d: event %d -> irq %d%s%s%s\n",\r\ncpu_from_evtchn(i), i,\r\nevtchn_to_irq[i],\r\nsync_test_bit(word_idx, BM(&v->evtchn_pending_sel))\r\n? "" : " l2-clear",\r\n!sync_test_bit(i, BM(sh->evtchn_mask))\r\n? "" : " globally-masked",\r\nsync_test_bit(i, BM(cpu_evtchn))\r\n? "" : " locally-masked");\r\n}\r\n}\r\nspin_unlock_irqrestore(&debug_lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void __xen_evtchn_do_upcall(void)\r\n{\r\nint start_word_idx, start_bit_idx;\r\nint word_idx, bit_idx;\r\nint i, irq;\r\nint cpu = get_cpu();\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nstruct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);\r\nunsigned count;\r\ndo {\r\nxen_ulong_t pending_words;\r\nxen_ulong_t pending_bits;\r\nstruct irq_desc *desc;\r\nvcpu_info->evtchn_upcall_pending = 0;\r\nif (__this_cpu_inc_return(xed_nesting_count) - 1)\r\ngoto out;\r\nif ((irq = per_cpu(virq_to_irq, cpu)[VIRQ_TIMER]) != -1) {\r\nint evtchn = evtchn_from_irq(irq);\r\nword_idx = evtchn / BITS_PER_LONG;\r\npending_bits = evtchn % BITS_PER_LONG;\r\nif (active_evtchns(cpu, s, word_idx) & (1ULL << pending_bits)) {\r\ndesc = irq_to_desc(irq);\r\nif (desc)\r\ngeneric_handle_irq_desc(irq, desc);\r\n}\r\n}\r\npending_words = xchg_xen_ulong(&vcpu_info->evtchn_pending_sel, 0);\r\nstart_word_idx = __this_cpu_read(current_word_idx);\r\nstart_bit_idx = __this_cpu_read(current_bit_idx);\r\nword_idx = start_word_idx;\r\nfor (i = 0; pending_words != 0; i++) {\r\nxen_ulong_t words;\r\nwords = MASK_LSBS(pending_words, word_idx);\r\nif (words == 0) {\r\nword_idx = 0;\r\nbit_idx = 0;\r\ncontinue;\r\n}\r\nword_idx = EVTCHN_FIRST_BIT(words);\r\npending_bits = active_evtchns(cpu, s, word_idx);\r\nbit_idx = 0;\r\nif (word_idx == start_word_idx) {\r\nif (i == 0)\r\nbit_idx = start_bit_idx;\r\n}\r\ndo {\r\nxen_ulong_t bits;\r\nint port;\r\nbits = MASK_LSBS(pending_bits, bit_idx);\r\nif (bits == 0)\r\nbreak;\r\nbit_idx = EVTCHN_FIRST_BIT(bits);\r\nport = (word_idx * BITS_PER_EVTCHN_WORD) + bit_idx;\r\nirq = evtchn_to_irq[port];\r\nif (irq != -1) {\r\ndesc = irq_to_desc(irq);\r\nif (desc)\r\ngeneric_handle_irq_desc(irq, desc);\r\n}\r\nbit_idx = (bit_idx + 1) % BITS_PER_EVTCHN_WORD;\r\n__this_cpu_write(current_word_idx,\r\nbit_idx ? word_idx :\r\n(word_idx+1) % BITS_PER_EVTCHN_WORD);\r\n__this_cpu_write(current_bit_idx, bit_idx);\r\n} while (bit_idx != 0);\r\nif ((word_idx != start_word_idx) || (i != 0))\r\npending_words &= ~(1UL << word_idx);\r\nword_idx = (word_idx + 1) % BITS_PER_EVTCHN_WORD;\r\n}\r\nBUG_ON(!irqs_disabled());\r\ncount = __this_cpu_read(xed_nesting_count);\r\n__this_cpu_write(xed_nesting_count, 0);\r\n} while (count != 1 || vcpu_info->evtchn_upcall_pending);\r\nout:\r\nput_cpu();\r\n}\r\nvoid xen_evtchn_do_upcall(struct pt_regs *regs)\r\n{\r\nstruct pt_regs *old_regs = set_irq_regs(regs);\r\nirq_enter();\r\n#ifdef CONFIG_X86\r\nexit_idle();\r\n#endif\r\n__xen_evtchn_do_upcall();\r\nirq_exit();\r\nset_irq_regs(old_regs);\r\n}\r\nvoid xen_hvm_evtchn_do_upcall(void)\r\n{\r\n__xen_evtchn_do_upcall();\r\n}\r\nvoid rebind_evtchn_irq(int evtchn, int irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nif (WARN_ON(!info))\r\nreturn;\r\ndisable_irq(irq);\r\nmutex_lock(&irq_mapping_update_lock);\r\nBUG_ON(evtchn_to_irq[evtchn] != -1);\r\nBUG_ON(info->type == IRQT_UNBOUND);\r\nxen_irq_info_evtchn_init(irq, evtchn);\r\nmutex_unlock(&irq_mapping_update_lock);\r\nirq_set_affinity(irq, cpumask_of(0));\r\nenable_irq(irq);\r\n}\r\nstatic int rebind_irq_to_cpu(unsigned irq, unsigned tcpu)\r\n{\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nstruct evtchn_bind_vcpu bind_vcpu;\r\nint evtchn = evtchn_from_irq(irq);\r\nint masked;\r\nif (!VALID_EVTCHN(evtchn))\r\nreturn -1;\r\nif (xen_hvm_domain() && !xen_have_vector_callback)\r\nreturn -1;\r\nbind_vcpu.port = evtchn;\r\nbind_vcpu.vcpu = tcpu;\r\nmasked = sync_test_and_set_bit(evtchn, BM(s->evtchn_mask));\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_vcpu, &bind_vcpu) >= 0)\r\nbind_evtchn_to_cpu(evtchn, tcpu);\r\nif (!masked)\r\nunmask_evtchn(evtchn);\r\nreturn 0;\r\n}\r\nstatic int set_affinity_irq(struct irq_data *data, const struct cpumask *dest,\r\nbool force)\r\n{\r\nunsigned tcpu = cpumask_first(dest);\r\nreturn rebind_irq_to_cpu(data->irq, tcpu);\r\n}\r\nint resend_irq_on_evtchn(unsigned int irq)\r\n{\r\nint masked, evtchn = evtchn_from_irq(irq);\r\nstruct shared_info *s = HYPERVISOR_shared_info;\r\nif (!VALID_EVTCHN(evtchn))\r\nreturn 1;\r\nmasked = sync_test_and_set_bit(evtchn, BM(s->evtchn_mask));\r\nsync_set_bit(evtchn, BM(s->evtchn_pending));\r\nif (!masked)\r\nunmask_evtchn(evtchn);\r\nreturn 1;\r\n}\r\nstatic void enable_dynirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nif (VALID_EVTCHN(evtchn))\r\nunmask_evtchn(evtchn);\r\n}\r\nstatic void disable_dynirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nif (VALID_EVTCHN(evtchn))\r\nmask_evtchn(evtchn);\r\n}\r\nstatic void ack_dynirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nirq_move_irq(data);\r\nif (VALID_EVTCHN(evtchn))\r\nclear_evtchn(evtchn);\r\n}\r\nstatic void mask_ack_dynirq(struct irq_data *data)\r\n{\r\ndisable_dynirq(data);\r\nack_dynirq(data);\r\n}\r\nstatic int retrigger_dynirq(struct irq_data *data)\r\n{\r\nint evtchn = evtchn_from_irq(data->irq);\r\nstruct shared_info *sh = HYPERVISOR_shared_info;\r\nint ret = 0;\r\nif (VALID_EVTCHN(evtchn)) {\r\nint masked;\r\nmasked = sync_test_and_set_bit(evtchn, BM(sh->evtchn_mask));\r\nsync_set_bit(evtchn, BM(sh->evtchn_pending));\r\nif (!masked)\r\nunmask_evtchn(evtchn);\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic void restore_pirqs(void)\r\n{\r\nint pirq, rc, irq, gsi;\r\nstruct physdev_map_pirq map_irq;\r\nstruct irq_info *info;\r\nlist_for_each_entry(info, &xen_irq_list_head, list) {\r\nif (info->type != IRQT_PIRQ)\r\ncontinue;\r\npirq = info->u.pirq.pirq;\r\ngsi = info->u.pirq.gsi;\r\nirq = info->irq;\r\nif (!gsi)\r\ncontinue;\r\nmap_irq.domid = DOMID_SELF;\r\nmap_irq.type = MAP_PIRQ_TYPE_GSI;\r\nmap_irq.index = gsi;\r\nmap_irq.pirq = pirq;\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_map_pirq, &map_irq);\r\nif (rc) {\r\npr_warn("xen map irq failed gsi=%d irq=%d pirq=%d rc=%d\n",\r\ngsi, irq, pirq, rc);\r\nxen_free_irq(irq);\r\ncontinue;\r\n}\r\nprintk(KERN_DEBUG "xen: --> irq=%d, pirq=%d\n", irq, map_irq.pirq);\r\n__startup_pirq(irq);\r\n}\r\n}\r\nstatic void restore_cpu_virqs(unsigned int cpu)\r\n{\r\nstruct evtchn_bind_virq bind_virq;\r\nint virq, irq, evtchn;\r\nfor (virq = 0; virq < NR_VIRQS; virq++) {\r\nif ((irq = per_cpu(virq_to_irq, cpu)[virq]) == -1)\r\ncontinue;\r\nBUG_ON(virq_from_irq(irq) != virq);\r\nbind_virq.virq = virq;\r\nbind_virq.vcpu = cpu;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_virq,\r\n&bind_virq) != 0)\r\nBUG();\r\nevtchn = bind_virq.port;\r\nxen_irq_info_virq_init(cpu, irq, evtchn, virq);\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n}\r\n}\r\nstatic void restore_cpu_ipis(unsigned int cpu)\r\n{\r\nstruct evtchn_bind_ipi bind_ipi;\r\nint ipi, irq, evtchn;\r\nfor (ipi = 0; ipi < XEN_NR_IPIS; ipi++) {\r\nif ((irq = per_cpu(ipi_to_irq, cpu)[ipi]) == -1)\r\ncontinue;\r\nBUG_ON(ipi_from_irq(irq) != ipi);\r\nbind_ipi.vcpu = cpu;\r\nif (HYPERVISOR_event_channel_op(EVTCHNOP_bind_ipi,\r\n&bind_ipi) != 0)\r\nBUG();\r\nevtchn = bind_ipi.port;\r\nxen_irq_info_ipi_init(cpu, irq, evtchn, ipi);\r\nbind_evtchn_to_cpu(evtchn, cpu);\r\n}\r\n}\r\nvoid xen_clear_irq_pending(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn))\r\nclear_evtchn(evtchn);\r\n}\r\nvoid xen_set_irq_pending(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn))\r\nset_evtchn(evtchn);\r\n}\r\nbool xen_test_irq_pending(int irq)\r\n{\r\nint evtchn = evtchn_from_irq(irq);\r\nbool ret = false;\r\nif (VALID_EVTCHN(evtchn))\r\nret = test_evtchn(evtchn);\r\nreturn ret;\r\n}\r\nvoid xen_poll_irq_timeout(int irq, u64 timeout)\r\n{\r\nevtchn_port_t evtchn = evtchn_from_irq(irq);\r\nif (VALID_EVTCHN(evtchn)) {\r\nstruct sched_poll poll;\r\npoll.nr_ports = 1;\r\npoll.timeout = timeout;\r\nset_xen_guest_handle(poll.ports, &evtchn);\r\nif (HYPERVISOR_sched_op(SCHEDOP_poll, &poll) != 0)\r\nBUG();\r\n}\r\n}\r\nvoid xen_poll_irq(int irq)\r\n{\r\nxen_poll_irq_timeout(irq, 0 );\r\n}\r\nint xen_test_irq_shared(int irq)\r\n{\r\nstruct irq_info *info = info_for_irq(irq);\r\nstruct physdev_irq_status_query irq_status;\r\nif (WARN_ON(!info))\r\nreturn -ENOENT;\r\nirq_status.irq = info->u.pirq.pirq;\r\nif (HYPERVISOR_physdev_op(PHYSDEVOP_irq_status_query, &irq_status))\r\nreturn 0;\r\nreturn !(irq_status.flags & XENIRQSTAT_shared);\r\n}\r\nvoid xen_irq_resume(void)\r\n{\r\nunsigned int cpu, evtchn;\r\nstruct irq_info *info;\r\ninit_evtchn_cpu_bindings();\r\nfor (evtchn = 0; evtchn < NR_EVENT_CHANNELS; evtchn++)\r\nmask_evtchn(evtchn);\r\nlist_for_each_entry(info, &xen_irq_list_head, list)\r\ninfo->evtchn = 0;\r\nfor (evtchn = 0; evtchn < NR_EVENT_CHANNELS; evtchn++)\r\nevtchn_to_irq[evtchn] = -1;\r\nfor_each_possible_cpu(cpu) {\r\nrestore_cpu_virqs(cpu);\r\nrestore_cpu_ipis(cpu);\r\n}\r\nrestore_pirqs();\r\n}\r\nint xen_set_callback_via(uint64_t via)\r\n{\r\nstruct xen_hvm_param a;\r\na.domid = DOMID_SELF;\r\na.index = HVM_PARAM_CALLBACK_IRQ;\r\na.value = via;\r\nreturn HYPERVISOR_hvm_op(HVMOP_set_param, &a);\r\n}\r\nvoid xen_callback_vector(void)\r\n{\r\nint rc;\r\nuint64_t callback_via;\r\nif (xen_have_vector_callback) {\r\ncallback_via = HVM_CALLBACK_VECTOR(HYPERVISOR_CALLBACK_VECTOR);\r\nrc = xen_set_callback_via(callback_via);\r\nif (rc) {\r\npr_err("Request for Xen HVM callback vector failed\n");\r\nxen_have_vector_callback = 0;\r\nreturn;\r\n}\r\npr_info("Xen HVM callback vector for event delivery is enabled\n");\r\nif (!test_bit(HYPERVISOR_CALLBACK_VECTOR, used_vectors))\r\nalloc_intr_gate(HYPERVISOR_CALLBACK_VECTOR,\r\nxen_hvm_callback_vector);\r\n}\r\n}\r\nvoid xen_callback_vector(void) {}\r\nvoid __init xen_init_IRQ(void)\r\n{\r\nint i;\r\nevtchn_to_irq = kcalloc(NR_EVENT_CHANNELS, sizeof(*evtchn_to_irq),\r\nGFP_KERNEL);\r\nBUG_ON(!evtchn_to_irq);\r\nfor (i = 0; i < NR_EVENT_CHANNELS; i++)\r\nevtchn_to_irq[i] = -1;\r\ninit_evtchn_cpu_bindings();\r\nfor (i = 0; i < NR_EVENT_CHANNELS; i++)\r\nmask_evtchn(i);\r\npirq_needs_eoi = pirq_needs_eoi_flag;\r\n#ifdef CONFIG_X86\r\nif (xen_hvm_domain()) {\r\nxen_callback_vector();\r\nnative_init_IRQ();\r\npci_xen_hvm_init();\r\n} else {\r\nint rc;\r\nstruct physdev_pirq_eoi_gmfn eoi_gmfn;\r\nirq_ctx_init(smp_processor_id());\r\nif (xen_initial_domain())\r\npci_xen_initial_domain();\r\npirq_eoi_map = (void *)__get_free_page(GFP_KERNEL|__GFP_ZERO);\r\neoi_gmfn.gmfn = virt_to_mfn(pirq_eoi_map);\r\nrc = HYPERVISOR_physdev_op(PHYSDEVOP_pirq_eoi_gmfn_v2, &eoi_gmfn);\r\nif (rc != 0) {\r\nfree_page((unsigned long) pirq_eoi_map);\r\npirq_eoi_map = NULL;\r\n} else\r\npirq_needs_eoi = pirq_check_eoi_map;\r\n}\r\n#endif\r\n}
