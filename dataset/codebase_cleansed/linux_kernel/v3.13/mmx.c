static int raid6_have_mmx(void)\r\n{\r\nreturn boot_cpu_has(X86_FEATURE_MMX);\r\n}\r\nstatic void raid6_mmx1_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("movq %0,%%mm0" : : "m" (raid6_mmx_constants.x1d));\r\nasm volatile("pxor %mm5,%mm5");\r\nfor ( d = 0 ; d < bytes ; d += 8 ) {\r\nasm volatile("movq %0,%%mm2" : : "m" (dptr[z0][d]));\r\nasm volatile("movq %mm2,%mm4");\r\nfor ( z = z0-1 ; z >= 0 ; z-- ) {\r\nasm volatile("movq %0,%%mm6" : : "m" (dptr[z][d]));\r\nasm volatile("pcmpgtb %mm4,%mm5");\r\nasm volatile("paddb %mm4,%mm4");\r\nasm volatile("pand %mm0,%mm5");\r\nasm volatile("pxor %mm5,%mm4");\r\nasm volatile("pxor %mm5,%mm5");\r\nasm volatile("pxor %mm6,%mm2");\r\nasm volatile("pxor %mm6,%mm4");\r\n}\r\nasm volatile("movq %%mm2,%0" : "=m" (p[d]));\r\nasm volatile("pxor %mm2,%mm2");\r\nasm volatile("movq %%mm4,%0" : "=m" (q[d]));\r\nasm volatile("pxor %mm4,%mm4");\r\n}\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_mmx2_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("movq %0,%%mm0" : : "m" (raid6_mmx_constants.x1d));\r\nasm volatile("pxor %mm5,%mm5");\r\nasm volatile("pxor %mm7,%mm7");\r\nfor ( d = 0 ; d < bytes ; d += 16 ) {\r\nasm volatile("movq %0,%%mm2" : : "m" (dptr[z0][d]));\r\nasm volatile("movq %0,%%mm3" : : "m" (dptr[z0][d+8]));\r\nasm volatile("movq %mm2,%mm4");\r\nasm volatile("movq %mm3,%mm6");\r\nfor ( z = z0-1 ; z >= 0 ; z-- ) {\r\nasm volatile("pcmpgtb %mm4,%mm5");\r\nasm volatile("pcmpgtb %mm6,%mm7");\r\nasm volatile("paddb %mm4,%mm4");\r\nasm volatile("paddb %mm6,%mm6");\r\nasm volatile("pand %mm0,%mm5");\r\nasm volatile("pand %mm0,%mm7");\r\nasm volatile("pxor %mm5,%mm4");\r\nasm volatile("pxor %mm7,%mm6");\r\nasm volatile("movq %0,%%mm5" : : "m" (dptr[z][d]));\r\nasm volatile("movq %0,%%mm7" : : "m" (dptr[z][d+8]));\r\nasm volatile("pxor %mm5,%mm2");\r\nasm volatile("pxor %mm7,%mm3");\r\nasm volatile("pxor %mm5,%mm4");\r\nasm volatile("pxor %mm7,%mm6");\r\nasm volatile("pxor %mm5,%mm5");\r\nasm volatile("pxor %mm7,%mm7");\r\n}\r\nasm volatile("movq %%mm2,%0" : "=m" (p[d]));\r\nasm volatile("movq %%mm3,%0" : "=m" (p[d+8]));\r\nasm volatile("movq %%mm4,%0" : "=m" (q[d]));\r\nasm volatile("movq %%mm6,%0" : "=m" (q[d+8]));\r\n}\r\nkernel_fpu_end();\r\n}
