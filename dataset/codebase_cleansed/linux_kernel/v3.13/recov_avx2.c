static int raid6_has_avx2(void)\r\n{\r\nreturn boot_cpu_has(X86_FEATURE_AVX2) &&\r\nboot_cpu_has(X86_FEATURE_AVX);\r\n}\r\nstatic void raid6_2data_recov_avx2(int disks, size_t bytes, int faila,\r\nint failb, void **ptrs)\r\n{\r\nu8 *p, *q, *dp, *dq;\r\nconst u8 *pbmul;\r\nconst u8 *qmul;\r\nconst u8 x0f = 0x0f;\r\np = (u8 *)ptrs[disks-2];\r\nq = (u8 *)ptrs[disks-1];\r\ndp = (u8 *)ptrs[faila];\r\nptrs[faila] = (void *)raid6_empty_zero_page;\r\nptrs[disks-2] = dp;\r\ndq = (u8 *)ptrs[failb];\r\nptrs[failb] = (void *)raid6_empty_zero_page;\r\nptrs[disks-1] = dq;\r\nraid6_call.gen_syndrome(disks, bytes, ptrs);\r\nptrs[faila] = dp;\r\nptrs[failb] = dq;\r\nptrs[disks-2] = p;\r\nptrs[disks-1] = q;\r\npbmul = raid6_vgfmul[raid6_gfexi[failb-faila]];\r\nqmul = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila] ^\r\nraid6_gfexp[failb]]];\r\nkernel_fpu_begin();\r\nasm volatile("vpbroadcastb %0, %%ymm7" : : "m" (x0f));\r\nwhile (bytes) {\r\n#ifdef CONFIG_X86_64\r\nasm volatile("vmovdqa %0, %%ymm1" : : "m" (q[0]));\r\nasm volatile("vmovdqa %0, %%ymm9" : : "m" (q[32]));\r\nasm volatile("vmovdqa %0, %%ymm0" : : "m" (p[0]));\r\nasm volatile("vmovdqa %0, %%ymm8" : : "m" (p[32]));\r\nasm volatile("vpxor %0, %%ymm1, %%ymm1" : : "m" (dq[0]));\r\nasm volatile("vpxor %0, %%ymm9, %%ymm9" : : "m" (dq[32]));\r\nasm volatile("vpxor %0, %%ymm0, %%ymm0" : : "m" (dp[0]));\r\nasm volatile("vpxor %0, %%ymm8, %%ymm8" : : "m" (dp[32]));\r\nasm volatile("vbroadcasti128 %0, %%ymm4" : : "m" (qmul[0]));\r\nasm volatile("vbroadcasti128 %0, %%ymm5" : : "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %ymm1, %ymm3");\r\nasm volatile("vpsraw $4, %ymm9, %ymm12");\r\nasm volatile("vpand %ymm7, %ymm1, %ymm1");\r\nasm volatile("vpand %ymm7, %ymm9, %ymm9");\r\nasm volatile("vpand %ymm7, %ymm3, %ymm3");\r\nasm volatile("vpand %ymm7, %ymm12, %ymm12");\r\nasm volatile("vpshufb %ymm9, %ymm4, %ymm14");\r\nasm volatile("vpshufb %ymm1, %ymm4, %ymm4");\r\nasm volatile("vpshufb %ymm12, %ymm5, %ymm15");\r\nasm volatile("vpshufb %ymm3, %ymm5, %ymm5");\r\nasm volatile("vpxor %ymm14, %ymm15, %ymm15");\r\nasm volatile("vpxor %ymm4, %ymm5, %ymm5");\r\nasm volatile("vbroadcasti128 %0, %%ymm4" : : "m" (pbmul[0]));\r\nasm volatile("vbroadcasti128 %0, %%ymm1" : : "m" (pbmul[16]));\r\nasm volatile("vpsraw $4, %ymm0, %ymm2");\r\nasm volatile("vpsraw $4, %ymm8, %ymm6");\r\nasm volatile("vpand %ymm7, %ymm0, %ymm3");\r\nasm volatile("vpand %ymm7, %ymm8, %ymm14");\r\nasm volatile("vpand %ymm7, %ymm2, %ymm2");\r\nasm volatile("vpand %ymm7, %ymm6, %ymm6");\r\nasm volatile("vpshufb %ymm14, %ymm4, %ymm12");\r\nasm volatile("vpshufb %ymm3, %ymm4, %ymm4");\r\nasm volatile("vpshufb %ymm6, %ymm1, %ymm13");\r\nasm volatile("vpshufb %ymm2, %ymm1, %ymm1");\r\nasm volatile("vpxor %ymm4, %ymm1, %ymm1");\r\nasm volatile("vpxor %ymm12, %ymm13, %ymm13");\r\nasm volatile("vpxor %ymm5, %ymm1, %ymm1");\r\nasm volatile("vpxor %ymm15, %ymm13, %ymm13");\r\nasm volatile("vmovdqa %%ymm1, %0" : "=m" (dq[0]));\r\nasm volatile("vmovdqa %%ymm13,%0" : "=m" (dq[32]));\r\nasm volatile("vpxor %ymm1, %ymm0, %ymm0");\r\nasm volatile("vpxor %ymm13, %ymm8, %ymm8");\r\nasm volatile("vmovdqa %%ymm0, %0" : "=m" (dp[0]));\r\nasm volatile("vmovdqa %%ymm8, %0" : "=m" (dp[32]));\r\nbytes -= 64;\r\np += 64;\r\nq += 64;\r\ndp += 64;\r\ndq += 64;\r\n#else\r\nasm volatile("vmovdqa %0, %%ymm1" : : "m" (*q));\r\nasm volatile("vmovdqa %0, %%ymm0" : : "m" (*p));\r\nasm volatile("vpxor %0, %%ymm1, %%ymm1" : : "m" (*dq));\r\nasm volatile("vpxor %0, %%ymm0, %%ymm0" : : "m" (*dp));\r\nasm volatile("vbroadcasti128 %0, %%ymm4" : : "m" (qmul[0]));\r\nasm volatile("vbroadcasti128 %0, %%ymm5" : : "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %ymm1, %ymm3");\r\nasm volatile("vpand %ymm7, %ymm1, %ymm1");\r\nasm volatile("vpand %ymm7, %ymm3, %ymm3");\r\nasm volatile("vpshufb %ymm1, %ymm4, %ymm4");\r\nasm volatile("vpshufb %ymm3, %ymm5, %ymm5");\r\nasm volatile("vpxor %ymm4, %ymm5, %ymm5");\r\nasm volatile("vbroadcasti128 %0, %%ymm4" : : "m" (pbmul[0]));\r\nasm volatile("vbroadcasti128 %0, %%ymm1" : : "m" (pbmul[16]));\r\nasm volatile("vpsraw $4, %ymm0, %ymm2");\r\nasm volatile("vpand %ymm7, %ymm0, %ymm3");\r\nasm volatile("vpand %ymm7, %ymm2, %ymm2");\r\nasm volatile("vpshufb %ymm3, %ymm4, %ymm4");\r\nasm volatile("vpshufb %ymm2, %ymm1, %ymm1");\r\nasm volatile("vpxor %ymm4, %ymm1, %ymm1");\r\nasm volatile("vpxor %ymm5, %ymm1, %ymm1");\r\nasm volatile("vmovdqa %%ymm1, %0" : "=m" (dq[0]));\r\nasm volatile("vpxor %ymm1, %ymm0, %ymm0");\r\nasm volatile("vmovdqa %%ymm0, %0" : "=m" (dp[0]));\r\nbytes -= 32;\r\np += 32;\r\nq += 32;\r\ndp += 32;\r\ndq += 32;\r\n#endif\r\n}\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_datap_recov_avx2(int disks, size_t bytes, int faila,\r\nvoid **ptrs)\r\n{\r\nu8 *p, *q, *dq;\r\nconst u8 *qmul;\r\nconst u8 x0f = 0x0f;\r\np = (u8 *)ptrs[disks-2];\r\nq = (u8 *)ptrs[disks-1];\r\ndq = (u8 *)ptrs[faila];\r\nptrs[faila] = (void *)raid6_empty_zero_page;\r\nptrs[disks-1] = dq;\r\nraid6_call.gen_syndrome(disks, bytes, ptrs);\r\nptrs[faila] = dq;\r\nptrs[disks-1] = q;\r\nqmul = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila]]];\r\nkernel_fpu_begin();\r\nasm volatile("vpbroadcastb %0, %%ymm7" : : "m" (x0f));\r\nwhile (bytes) {\r\n#ifdef CONFIG_X86_64\r\nasm volatile("vmovdqa %0, %%ymm3" : : "m" (dq[0]));\r\nasm volatile("vmovdqa %0, %%ymm8" : : "m" (dq[32]));\r\nasm volatile("vpxor %0, %%ymm3, %%ymm3" : : "m" (q[0]));\r\nasm volatile("vpxor %0, %%ymm8, %%ymm8" : : "m" (q[32]));\r\nasm volatile("vbroadcasti128 %0, %%ymm0" : : "m" (qmul[0]));\r\nasm volatile("vmovapd %ymm0, %ymm13");\r\nasm volatile("vbroadcasti128 %0, %%ymm1" : : "m" (qmul[16]));\r\nasm volatile("vmovapd %ymm1, %ymm14");\r\nasm volatile("vpsraw $4, %ymm3, %ymm6");\r\nasm volatile("vpsraw $4, %ymm8, %ymm12");\r\nasm volatile("vpand %ymm7, %ymm3, %ymm3");\r\nasm volatile("vpand %ymm7, %ymm8, %ymm8");\r\nasm volatile("vpand %ymm7, %ymm6, %ymm6");\r\nasm volatile("vpand %ymm7, %ymm12, %ymm12");\r\nasm volatile("vpshufb %ymm3, %ymm0, %ymm0");\r\nasm volatile("vpshufb %ymm8, %ymm13, %ymm13");\r\nasm volatile("vpshufb %ymm6, %ymm1, %ymm1");\r\nasm volatile("vpshufb %ymm12, %ymm14, %ymm14");\r\nasm volatile("vpxor %ymm0, %ymm1, %ymm1");\r\nasm volatile("vpxor %ymm13, %ymm14, %ymm14");\r\nasm volatile("vmovdqa %0, %%ymm2" : : "m" (p[0]));\r\nasm volatile("vmovdqa %0, %%ymm12" : : "m" (p[32]));\r\nasm volatile("vpxor %ymm1, %ymm2, %ymm2");\r\nasm volatile("vpxor %ymm14, %ymm12, %ymm12");\r\nasm volatile("vmovdqa %%ymm1, %0" : "=m" (dq[0]));\r\nasm volatile("vmovdqa %%ymm14, %0" : "=m" (dq[32]));\r\nasm volatile("vmovdqa %%ymm2, %0" : "=m" (p[0]));\r\nasm volatile("vmovdqa %%ymm12,%0" : "=m" (p[32]));\r\nbytes -= 64;\r\np += 64;\r\nq += 64;\r\ndq += 64;\r\n#else\r\nasm volatile("vmovdqa %0, %%ymm3" : : "m" (dq[0]));\r\nasm volatile("vpxor %0, %%ymm3, %%ymm3" : : "m" (q[0]));\r\nasm volatile("vbroadcasti128 %0, %%ymm0" : : "m" (qmul[0]));\r\nasm volatile("vbroadcasti128 %0, %%ymm1" : : "m" (qmul[16]));\r\nasm volatile("vpsraw $4, %ymm3, %ymm6");\r\nasm volatile("vpand %ymm7, %ymm3, %ymm3");\r\nasm volatile("vpand %ymm7, %ymm6, %ymm6");\r\nasm volatile("vpshufb %ymm3, %ymm0, %ymm0");\r\nasm volatile("vpshufb %ymm6, %ymm1, %ymm1");\r\nasm volatile("vpxor %ymm0, %ymm1, %ymm1");\r\nasm volatile("vmovdqa %0, %%ymm2" : : "m" (p[0]));\r\nasm volatile("vpxor %ymm1, %ymm2, %ymm2");\r\nasm volatile("vmovdqa %%ymm1, %0" : "=m" (dq[0]));\r\nasm volatile("vmovdqa %%ymm2, %0" : "=m" (p[0]));\r\nbytes -= 32;\r\np += 32;\r\nq += 32;\r\ndq += 32;\r\n#endif\r\n}\r\nkernel_fpu_end();\r\n}
