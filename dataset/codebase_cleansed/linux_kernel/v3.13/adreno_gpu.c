int adreno_get_param(struct msm_gpu *gpu, uint32_t param, uint64_t *value)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nswitch (param) {\r\ncase MSM_PARAM_GPU_ID:\r\n*value = adreno_gpu->info->revn;\r\nreturn 0;\r\ncase MSM_PARAM_GMEM_SIZE:\r\n*value = adreno_gpu->info->gmem;\r\nreturn 0;\r\ndefault:\r\nDBG("%s: invalid param: %u", gpu->name, param);\r\nreturn -EINVAL;\r\n}\r\n}\r\nint adreno_hw_init(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nDBG("%s", gpu->name);\r\ngpu_write(gpu, REG_AXXX_CP_RB_CNTL,\r\nAXXX_CP_RB_CNTL_BUFSZ(ilog2(gpu->rb->size / 8)) |\r\nAXXX_CP_RB_CNTL_BLKSZ(RB_BLKSIZE));\r\ngpu_write(gpu, REG_AXXX_CP_RB_BASE, gpu->rb_iova);\r\ngpu_write(gpu, REG_AXXX_CP_RB_RPTR_ADDR, rbmemptr(adreno_gpu, rptr));\r\ngpu_write(gpu, REG_AXXX_SCRATCH_ADDR, rbmemptr(adreno_gpu, fence));\r\ngpu_write(gpu, REG_AXXX_SCRATCH_UMSK, 0x1);\r\nreturn 0;\r\n}\r\nstatic uint32_t get_wptr(struct msm_ringbuffer *ring)\r\n{\r\nreturn ring->cur - ring->start;\r\n}\r\nuint32_t adreno_last_fence(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nreturn adreno_gpu->memptrs->fence;\r\n}\r\nvoid adreno_recover(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nstruct drm_device *dev = gpu->dev;\r\nint ret;\r\ngpu->funcs->pm_suspend(gpu);\r\ngpu->rb->cur = gpu->rb->start;\r\nadreno_gpu->memptrs->fence = gpu->submitted_fence;\r\nadreno_gpu->memptrs->rptr = 0;\r\nadreno_gpu->memptrs->wptr = 0;\r\ngpu->funcs->pm_resume(gpu);\r\nret = gpu->funcs->hw_init(gpu);\r\nif (ret) {\r\ndev_err(dev->dev, "gpu hw init failed: %d\n", ret);\r\n}\r\n}\r\nint adreno_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit,\r\nstruct msm_file_private *ctx)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nstruct msm_drm_private *priv = gpu->dev->dev_private;\r\nstruct msm_ringbuffer *ring = gpu->rb;\r\nunsigned i, ibs = 0;\r\nfor (i = 0; i < submit->nr_cmds; i++) {\r\nswitch (submit->cmd[i].type) {\r\ncase MSM_SUBMIT_CMD_IB_TARGET_BUF:\r\nbreak;\r\ncase MSM_SUBMIT_CMD_CTX_RESTORE_BUF:\r\nif (priv->lastctx == ctx)\r\nbreak;\r\ncase MSM_SUBMIT_CMD_BUF:\r\nOUT_PKT3(ring, CP_INDIRECT_BUFFER_PFD, 2);\r\nOUT_RING(ring, submit->cmd[i].iova);\r\nOUT_RING(ring, submit->cmd[i].size);\r\nibs++;\r\nbreak;\r\n}\r\n}\r\nif (ibs % 2)\r\nOUT_PKT2(ring);\r\nOUT_PKT0(ring, REG_AXXX_CP_SCRATCH_REG2, 1);\r\nOUT_RING(ring, submit->fence);\r\nif (adreno_is_a3xx(adreno_gpu)) {\r\nOUT_PKT3(ring, CP_EVENT_WRITE, 1);\r\nOUT_RING(ring, HLSQ_FLUSH);\r\nOUT_PKT3(ring, CP_WAIT_FOR_IDLE, 1);\r\nOUT_RING(ring, 0x00000000);\r\n}\r\nOUT_PKT3(ring, CP_EVENT_WRITE, 3);\r\nOUT_RING(ring, CACHE_FLUSH_TS);\r\nOUT_RING(ring, rbmemptr(adreno_gpu, fence));\r\nOUT_RING(ring, submit->fence);\r\nOUT_PKT3(ring, CP_INTERRUPT, 1);\r\nOUT_RING(ring, 0x80000000);\r\n#if 0\r\nif (adreno_is_a3xx(adreno_gpu)) {\r\nOUT_PKT3(ring, CP_SET_CONSTANT, 2);\r\nOUT_RING(ring, CP_REG(REG_A3XX_HLSQ_CL_KERNEL_GROUP_X_REG));\r\nOUT_RING(ring, 0x00000000);\r\n}\r\n#endif\r\ngpu->funcs->flush(gpu);\r\nreturn 0;\r\n}\r\nvoid adreno_flush(struct msm_gpu *gpu)\r\n{\r\nuint32_t wptr = get_wptr(gpu->rb);\r\nmb();\r\ngpu_write(gpu, REG_AXXX_CP_RB_WPTR, wptr);\r\n}\r\nvoid adreno_idle(struct msm_gpu *gpu)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nuint32_t rptr, wptr = get_wptr(gpu->rb);\r\nunsigned long t;\r\nt = jiffies + ADRENO_IDLE_TIMEOUT;\r\ndo {\r\nrptr = adreno_gpu->memptrs->rptr;\r\nif (rptr == wptr)\r\nreturn;\r\n} while(time_before(jiffies, t));\r\nDRM_ERROR("%s: timeout waiting to drain ringbuffer!\n", gpu->name);\r\n}\r\nvoid adreno_show(struct msm_gpu *gpu, struct seq_file *m)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nseq_printf(m, "revision: %d (%d.%d.%d.%d)\n",\r\nadreno_gpu->info->revn, adreno_gpu->rev.core,\r\nadreno_gpu->rev.major, adreno_gpu->rev.minor,\r\nadreno_gpu->rev.patchid);\r\nseq_printf(m, "fence: %d/%d\n", adreno_gpu->memptrs->fence,\r\ngpu->submitted_fence);\r\nseq_printf(m, "rptr: %d\n", adreno_gpu->memptrs->rptr);\r\nseq_printf(m, "wptr: %d\n", adreno_gpu->memptrs->wptr);\r\nseq_printf(m, "rb wptr: %d\n", get_wptr(gpu->rb));\r\n}\r\nvoid adreno_wait_ring(struct msm_gpu *gpu, uint32_t ndwords)\r\n{\r\nstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\r\nuint32_t freedwords;\r\nunsigned long t = jiffies + ADRENO_IDLE_TIMEOUT;\r\ndo {\r\nuint32_t size = gpu->rb->size / 4;\r\nuint32_t wptr = get_wptr(gpu->rb);\r\nuint32_t rptr = adreno_gpu->memptrs->rptr;\r\nfreedwords = (rptr + (size - 1) - wptr) % size;\r\nif (time_after(jiffies, t)) {\r\nDRM_ERROR("%s: timeout waiting for ringbuffer space\n", gpu->name);\r\nbreak;\r\n}\r\n} while(freedwords < ndwords);\r\n}\r\nstatic inline bool _rev_match(uint8_t entry, uint8_t id)\r\n{\r\nreturn (entry == ANY_ID) || (entry == id);\r\n}\r\nint adreno_gpu_init(struct drm_device *drm, struct platform_device *pdev,\r\nstruct adreno_gpu *gpu, const struct adreno_gpu_funcs *funcs,\r\nstruct adreno_rev rev)\r\n{\r\nint i, ret;\r\nfor (i = 0; i < ARRAY_SIZE(gpulist); i++) {\r\nconst struct adreno_info *info = &gpulist[i];\r\nif (_rev_match(info->rev.core, rev.core) &&\r\n_rev_match(info->rev.major, rev.major) &&\r\n_rev_match(info->rev.minor, rev.minor) &&\r\n_rev_match(info->rev.patchid, rev.patchid)) {\r\ngpu->info = info;\r\ngpu->revn = info->revn;\r\nbreak;\r\n}\r\n}\r\nif (i == ARRAY_SIZE(gpulist)) {\r\ndev_err(drm->dev, "Unknown GPU revision: %u.%u.%u.%u\n",\r\nrev.core, rev.major, rev.minor, rev.patchid);\r\nreturn -ENXIO;\r\n}\r\nDBG("Found GPU: %s (%u.%u.%u.%u)", gpu->info->name,\r\nrev.core, rev.major, rev.minor, rev.patchid);\r\ngpu->funcs = funcs;\r\ngpu->rev = rev;\r\nret = request_firmware(&gpu->pm4, gpu->info->pm4fw, drm->dev);\r\nif (ret) {\r\ndev_err(drm->dev, "failed to load %s PM4 firmware: %d\n",\r\ngpu->info->pm4fw, ret);\r\nreturn ret;\r\n}\r\nret = request_firmware(&gpu->pfp, gpu->info->pfpfw, drm->dev);\r\nif (ret) {\r\ndev_err(drm->dev, "failed to load %s PFP firmware: %d\n",\r\ngpu->info->pfpfw, ret);\r\nreturn ret;\r\n}\r\nret = msm_gpu_init(drm, pdev, &gpu->base, &funcs->base,\r\ngpu->info->name, "kgsl_3d0_reg_memory", "kgsl_3d0_irq",\r\nRB_SIZE);\r\nif (ret)\r\nreturn ret;\r\nret = msm_iommu_attach(drm, gpu->base.iommu,\r\niommu_ports, ARRAY_SIZE(iommu_ports));\r\nif (ret)\r\nreturn ret;\r\ngpu->memptrs_bo = msm_gem_new(drm, sizeof(*gpu->memptrs),\r\nMSM_BO_UNCACHED);\r\nif (IS_ERR(gpu->memptrs_bo)) {\r\nret = PTR_ERR(gpu->memptrs_bo);\r\ngpu->memptrs_bo = NULL;\r\ndev_err(drm->dev, "could not allocate memptrs: %d\n", ret);\r\nreturn ret;\r\n}\r\ngpu->memptrs = msm_gem_vaddr_locked(gpu->memptrs_bo);\r\nif (!gpu->memptrs) {\r\ndev_err(drm->dev, "could not vmap memptrs\n");\r\nreturn -ENOMEM;\r\n}\r\nret = msm_gem_get_iova_locked(gpu->memptrs_bo, gpu->base.id,\r\n&gpu->memptrs_iova);\r\nif (ret) {\r\ndev_err(drm->dev, "could not map memptrs: %d\n", ret);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nvoid adreno_gpu_cleanup(struct adreno_gpu *gpu)\r\n{\r\nif (gpu->memptrs_bo) {\r\nif (gpu->memptrs_iova)\r\nmsm_gem_put_iova(gpu->memptrs_bo, gpu->base.id);\r\ndrm_gem_object_unreference(gpu->memptrs_bo);\r\n}\r\nif (gpu->pm4)\r\nrelease_firmware(gpu->pm4);\r\nif (gpu->pfp)\r\nrelease_firmware(gpu->pfp);\r\nmsm_gpu_cleanup(&gpu->base);\r\n}
