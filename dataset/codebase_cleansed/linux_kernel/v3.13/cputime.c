void enable_sched_clock_irqtime(void)\r\n{\r\nsched_clock_irqtime = 1;\r\n}\r\nvoid disable_sched_clock_irqtime(void)\r\n{\r\nsched_clock_irqtime = 0;\r\n}\r\nvoid irqtime_account_irq(struct task_struct *curr)\r\n{\r\nunsigned long flags;\r\ns64 delta;\r\nint cpu;\r\nif (!sched_clock_irqtime)\r\nreturn;\r\nlocal_irq_save(flags);\r\ncpu = smp_processor_id();\r\ndelta = sched_clock_cpu(cpu) - __this_cpu_read(irq_start_time);\r\n__this_cpu_add(irq_start_time, delta);\r\nirq_time_write_begin();\r\nif (hardirq_count())\r\n__this_cpu_add(cpu_hardirq_time, delta);\r\nelse if (in_serving_softirq() && curr != this_cpu_ksoftirqd())\r\n__this_cpu_add(cpu_softirq_time, delta);\r\nirq_time_write_end();\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int irqtime_account_hi_update(void)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\nunsigned long flags;\r\nu64 latest_ns;\r\nint ret = 0;\r\nlocal_irq_save(flags);\r\nlatest_ns = this_cpu_read(cpu_hardirq_time);\r\nif (nsecs_to_cputime64(latest_ns) > cpustat[CPUTIME_IRQ])\r\nret = 1;\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nstatic int irqtime_account_si_update(void)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\nunsigned long flags;\r\nu64 latest_ns;\r\nint ret = 0;\r\nlocal_irq_save(flags);\r\nlatest_ns = this_cpu_read(cpu_softirq_time);\r\nif (nsecs_to_cputime64(latest_ns) > cpustat[CPUTIME_SOFTIRQ])\r\nret = 1;\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nstatic inline void task_group_account_field(struct task_struct *p, int index,\r\nu64 tmp)\r\n{\r\n__this_cpu_add(kernel_cpustat.cpustat[index], tmp);\r\ncpuacct_account_field(p, index, tmp);\r\n}\r\nvoid account_user_time(struct task_struct *p, cputime_t cputime,\r\ncputime_t cputime_scaled)\r\n{\r\nint index;\r\np->utime += cputime;\r\np->utimescaled += cputime_scaled;\r\naccount_group_user_time(p, cputime);\r\nindex = (TASK_NICE(p) > 0) ? CPUTIME_NICE : CPUTIME_USER;\r\ntask_group_account_field(p, index, (__force u64) cputime);\r\nacct_account_cputime(p);\r\n}\r\nstatic void account_guest_time(struct task_struct *p, cputime_t cputime,\r\ncputime_t cputime_scaled)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\np->utime += cputime;\r\np->utimescaled += cputime_scaled;\r\naccount_group_user_time(p, cputime);\r\np->gtime += cputime;\r\nif (TASK_NICE(p) > 0) {\r\ncpustat[CPUTIME_NICE] += (__force u64) cputime;\r\ncpustat[CPUTIME_GUEST_NICE] += (__force u64) cputime;\r\n} else {\r\ncpustat[CPUTIME_USER] += (__force u64) cputime;\r\ncpustat[CPUTIME_GUEST] += (__force u64) cputime;\r\n}\r\n}\r\nstatic inline\r\nvoid __account_system_time(struct task_struct *p, cputime_t cputime,\r\ncputime_t cputime_scaled, int index)\r\n{\r\np->stime += cputime;\r\np->stimescaled += cputime_scaled;\r\naccount_group_system_time(p, cputime);\r\ntask_group_account_field(p, index, (__force u64) cputime);\r\nacct_account_cputime(p);\r\n}\r\nvoid account_system_time(struct task_struct *p, int hardirq_offset,\r\ncputime_t cputime, cputime_t cputime_scaled)\r\n{\r\nint index;\r\nif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0)) {\r\naccount_guest_time(p, cputime, cputime_scaled);\r\nreturn;\r\n}\r\nif (hardirq_count() - hardirq_offset)\r\nindex = CPUTIME_IRQ;\r\nelse if (in_serving_softirq())\r\nindex = CPUTIME_SOFTIRQ;\r\nelse\r\nindex = CPUTIME_SYSTEM;\r\n__account_system_time(p, cputime, cputime_scaled, index);\r\n}\r\nvoid account_steal_time(cputime_t cputime)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\ncpustat[CPUTIME_STEAL] += (__force u64) cputime;\r\n}\r\nvoid account_idle_time(cputime_t cputime)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\nstruct rq *rq = this_rq();\r\nif (atomic_read(&rq->nr_iowait) > 0)\r\ncpustat[CPUTIME_IOWAIT] += (__force u64) cputime;\r\nelse\r\ncpustat[CPUTIME_IDLE] += (__force u64) cputime;\r\n}\r\nstatic __always_inline bool steal_account_process_tick(void)\r\n{\r\n#ifdef CONFIG_PARAVIRT\r\nif (static_key_false(&paravirt_steal_enabled)) {\r\nu64 steal, st = 0;\r\nsteal = paravirt_steal_clock(smp_processor_id());\r\nsteal -= this_rq()->prev_steal_time;\r\nst = steal_ticks(steal);\r\nthis_rq()->prev_steal_time += st * TICK_NSEC;\r\naccount_steal_time(st);\r\nreturn st;\r\n}\r\n#endif\r\nreturn false;\r\n}\r\nvoid thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)\r\n{\r\nstruct signal_struct *sig = tsk->signal;\r\ncputime_t utime, stime;\r\nstruct task_struct *t;\r\ntimes->utime = sig->utime;\r\ntimes->stime = sig->stime;\r\ntimes->sum_exec_runtime = sig->sum_sched_runtime;\r\nrcu_read_lock();\r\nif (!likely(pid_alive(tsk)))\r\ngoto out;\r\nt = tsk;\r\ndo {\r\ntask_cputime(t, &utime, &stime);\r\ntimes->utime += utime;\r\ntimes->stime += stime;\r\ntimes->sum_exec_runtime += task_sched_runtime(t);\r\n} while_each_thread(tsk, t);\r\nvoid irqtime_account_process_tick(struct task_struct *p, int user_tick,\r\nstruct rq *rq)\r\n{\r\ncputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\nif (steal_account_process_tick())\r\nreturn;\r\nif (irqtime_account_hi_update()) {\r\ncpustat[CPUTIME_IRQ] += (__force u64) cputime_one_jiffy;\r\n} else if (irqtime_account_si_update()) {\r\ncpustat[CPUTIME_SOFTIRQ] += (__force u64) cputime_one_jiffy;\r\n} else if (this_cpu_ksoftirqd() == p) {\r\n__account_system_time(p, cputime_one_jiffy, one_jiffy_scaled,\r\nCPUTIME_SOFTIRQ);\r\n} else if (user_tick) {\r\naccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\r\n} else if (p == rq->idle) {\r\naccount_idle_time(cputime_one_jiffy);\r\n} else if (p->flags & PF_VCPU) {\r\naccount_guest_time(p, cputime_one_jiffy, one_jiffy_scaled);\r\n} else {\r\n__account_system_time(p, cputime_one_jiffy, one_jiffy_scaled,\r\nCPUTIME_SYSTEM);\r\n}\r\n}\r\nstatic void irqtime_account_idle_ticks(int ticks)\r\n{\r\nint i;\r\nstruct rq *rq = this_rq();\r\nfor (i = 0; i < ticks; i++)\r\nirqtime_account_process_tick(current, 0, rq);\r\n}\r\nstatic inline void irqtime_account_idle_ticks(int ticks) {}\r\nstatic inline void irqtime_account_process_tick(struct task_struct *p, int user_tick,\r\nstruct rq *rq) {}\r\nvoid vtime_common_task_switch(struct task_struct *prev)\r\n{\r\nif (is_idle_task(prev))\r\nvtime_account_idle(prev);\r\nelse\r\nvtime_account_system(prev);\r\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\r\nvtime_account_user(prev);\r\n#endif\r\narch_vtime_task_switch(prev);\r\n}\r\nvoid vtime_common_account_irq_enter(struct task_struct *tsk)\r\n{\r\nif (!in_interrupt()) {\r\nif (context_tracking_in_user()) {\r\nvtime_account_user(tsk);\r\nreturn;\r\n}\r\nif (is_idle_task(tsk)) {\r\nvtime_account_idle(tsk);\r\nreturn;\r\n}\r\n}\r\nvtime_account_system(tsk);\r\n}\r\nvoid task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st)\r\n{\r\n*ut = p->utime;\r\n*st = p->stime;\r\n}\r\nvoid thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st)\r\n{\r\nstruct task_cputime cputime;\r\nthread_group_cputime(p, &cputime);\r\n*ut = cputime.utime;\r\n*st = cputime.stime;\r\n}\r\nvoid account_process_tick(struct task_struct *p, int user_tick)\r\n{\r\ncputime_t one_jiffy_scaled = cputime_to_scaled(cputime_one_jiffy);\r\nstruct rq *rq = this_rq();\r\nif (vtime_accounting_enabled())\r\nreturn;\r\nif (sched_clock_irqtime) {\r\nirqtime_account_process_tick(p, user_tick, rq);\r\nreturn;\r\n}\r\nif (steal_account_process_tick())\r\nreturn;\r\nif (user_tick)\r\naccount_user_time(p, cputime_one_jiffy, one_jiffy_scaled);\r\nelse if ((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))\r\naccount_system_time(p, HARDIRQ_OFFSET, cputime_one_jiffy,\r\none_jiffy_scaled);\r\nelse\r\naccount_idle_time(cputime_one_jiffy);\r\n}\r\nvoid account_steal_ticks(unsigned long ticks)\r\n{\r\naccount_steal_time(jiffies_to_cputime(ticks));\r\n}\r\nvoid account_idle_ticks(unsigned long ticks)\r\n{\r\nif (sched_clock_irqtime) {\r\nirqtime_account_idle_ticks(ticks);\r\nreturn;\r\n}\r\naccount_idle_time(jiffies_to_cputime(ticks));\r\n}\r\nstatic cputime_t scale_stime(u64 stime, u64 rtime, u64 total)\r\n{\r\nu64 scaled;\r\nfor (;;) {\r\nif (stime > rtime)\r\nswap(rtime, stime);\r\nif (total >> 32)\r\ngoto drop_precision;\r\nif (!(rtime >> 32))\r\nbreak;\r\nif (stime >> 31)\r\ngoto drop_precision;\r\nstime <<= 1;\r\nrtime >>= 1;\r\ncontinue;\r\ndrop_precision:\r\nrtime >>= 1;\r\ntotal >>= 1;\r\n}\r\nscaled = div_u64((u64) (u32) stime * (u64) (u32) rtime, (u32)total);\r\nreturn (__force cputime_t) scaled;\r\n}\r\nstatic void cputime_adjust(struct task_cputime *curr,\r\nstruct cputime *prev,\r\ncputime_t *ut, cputime_t *st)\r\n{\r\ncputime_t rtime, stime, utime;\r\nrtime = nsecs_to_cputime(curr->sum_exec_runtime);\r\nif (prev->stime + prev->utime >= rtime)\r\ngoto out;\r\nstime = curr->stime;\r\nutime = curr->utime;\r\nif (utime == 0) {\r\nstime = rtime;\r\n} else if (stime == 0) {\r\nutime = rtime;\r\n} else {\r\ncputime_t total = stime + utime;\r\nstime = scale_stime((__force u64)stime,\r\n(__force u64)rtime, (__force u64)total);\r\nutime = rtime - stime;\r\n}\r\nprev->stime = max(prev->stime, stime);\r\nprev->utime = max(prev->utime, utime);\r\nout:\r\n*ut = prev->utime;\r\n*st = prev->stime;\r\n}\r\nvoid task_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st)\r\n{\r\nstruct task_cputime cputime = {\r\n.sum_exec_runtime = p->se.sum_exec_runtime,\r\n};\r\ntask_cputime(p, &cputime.utime, &cputime.stime);\r\ncputime_adjust(&cputime, &p->prev_cputime, ut, st);\r\n}\r\nvoid thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut, cputime_t *st)\r\n{\r\nstruct task_cputime cputime;\r\nthread_group_cputime(p, &cputime);\r\ncputime_adjust(&cputime, &p->signal->prev_cputime, ut, st);\r\n}\r\nstatic unsigned long long vtime_delta(struct task_struct *tsk)\r\n{\r\nunsigned long long clock;\r\nclock = local_clock();\r\nif (clock < tsk->vtime_snap)\r\nreturn 0;\r\nreturn clock - tsk->vtime_snap;\r\n}\r\nstatic cputime_t get_vtime_delta(struct task_struct *tsk)\r\n{\r\nunsigned long long delta = vtime_delta(tsk);\r\nWARN_ON_ONCE(tsk->vtime_snap_whence == VTIME_SLEEPING);\r\ntsk->vtime_snap += delta;\r\nreturn nsecs_to_cputime(delta);\r\n}\r\nstatic void __vtime_account_system(struct task_struct *tsk)\r\n{\r\ncputime_t delta_cpu = get_vtime_delta(tsk);\r\naccount_system_time(tsk, irq_count(), delta_cpu, cputime_to_scaled(delta_cpu));\r\n}\r\nvoid vtime_account_system(struct task_struct *tsk)\r\n{\r\nwrite_seqlock(&tsk->vtime_seqlock);\r\n__vtime_account_system(tsk);\r\nwrite_sequnlock(&tsk->vtime_seqlock);\r\n}\r\nvoid vtime_gen_account_irq_exit(struct task_struct *tsk)\r\n{\r\nwrite_seqlock(&tsk->vtime_seqlock);\r\n__vtime_account_system(tsk);\r\nif (context_tracking_in_user())\r\ntsk->vtime_snap_whence = VTIME_USER;\r\nwrite_sequnlock(&tsk->vtime_seqlock);\r\n}\r\nvoid vtime_account_user(struct task_struct *tsk)\r\n{\r\ncputime_t delta_cpu;\r\nwrite_seqlock(&tsk->vtime_seqlock);\r\ndelta_cpu = get_vtime_delta(tsk);\r\ntsk->vtime_snap_whence = VTIME_SYS;\r\naccount_user_time(tsk, delta_cpu, cputime_to_scaled(delta_cpu));\r\nwrite_sequnlock(&tsk->vtime_seqlock);\r\n}\r\nvoid vtime_user_enter(struct task_struct *tsk)\r\n{\r\nwrite_seqlock(&tsk->vtime_seqlock);\r\n__vtime_account_system(tsk);\r\ntsk->vtime_snap_whence = VTIME_USER;\r\nwrite_sequnlock(&tsk->vtime_seqlock);\r\n}\r\nvoid vtime_guest_enter(struct task_struct *tsk)\r\n{\r\nwrite_seqlock(&tsk->vtime_seqlock);\r\n__vtime_account_system(tsk);\r\ncurrent->flags |= PF_VCPU;\r\nwrite_sequnlock(&tsk->vtime_seqlock);\r\n}\r\nvoid vtime_guest_exit(struct task_struct *tsk)\r\n{\r\nwrite_seqlock(&tsk->vtime_seqlock);\r\n__vtime_account_system(tsk);\r\ncurrent->flags &= ~PF_VCPU;\r\nwrite_sequnlock(&tsk->vtime_seqlock);\r\n}\r\nvoid vtime_account_idle(struct task_struct *tsk)\r\n{\r\ncputime_t delta_cpu = get_vtime_delta(tsk);\r\naccount_idle_time(delta_cpu);\r\n}\r\nvoid arch_vtime_task_switch(struct task_struct *prev)\r\n{\r\nwrite_seqlock(&prev->vtime_seqlock);\r\nprev->vtime_snap_whence = VTIME_SLEEPING;\r\nwrite_sequnlock(&prev->vtime_seqlock);\r\nwrite_seqlock(&current->vtime_seqlock);\r\ncurrent->vtime_snap_whence = VTIME_SYS;\r\ncurrent->vtime_snap = sched_clock_cpu(smp_processor_id());\r\nwrite_sequnlock(&current->vtime_seqlock);\r\n}\r\nvoid vtime_init_idle(struct task_struct *t, int cpu)\r\n{\r\nunsigned long flags;\r\nwrite_seqlock_irqsave(&t->vtime_seqlock, flags);\r\nt->vtime_snap_whence = VTIME_SYS;\r\nt->vtime_snap = sched_clock_cpu(cpu);\r\nwrite_sequnlock_irqrestore(&t->vtime_seqlock, flags);\r\n}\r\ncputime_t task_gtime(struct task_struct *t)\r\n{\r\nunsigned int seq;\r\ncputime_t gtime;\r\ndo {\r\nseq = read_seqbegin(&t->vtime_seqlock);\r\ngtime = t->gtime;\r\nif (t->flags & PF_VCPU)\r\ngtime += vtime_delta(t);\r\n} while (read_seqretry(&t->vtime_seqlock, seq));\r\nreturn gtime;\r\n}\r\nstatic void\r\nfetch_task_cputime(struct task_struct *t,\r\ncputime_t *u_dst, cputime_t *s_dst,\r\ncputime_t *u_src, cputime_t *s_src,\r\ncputime_t *udelta, cputime_t *sdelta)\r\n{\r\nunsigned int seq;\r\nunsigned long long delta;\r\ndo {\r\n*udelta = 0;\r\n*sdelta = 0;\r\nseq = read_seqbegin(&t->vtime_seqlock);\r\nif (u_dst)\r\n*u_dst = *u_src;\r\nif (s_dst)\r\n*s_dst = *s_src;\r\nif (t->vtime_snap_whence == VTIME_SLEEPING ||\r\nis_idle_task(t))\r\ncontinue;\r\ndelta = vtime_delta(t);\r\nif (t->vtime_snap_whence == VTIME_USER || t->flags & PF_VCPU) {\r\n*udelta = delta;\r\n} else {\r\nif (t->vtime_snap_whence == VTIME_SYS)\r\n*sdelta = delta;\r\n}\r\n} while (read_seqretry(&t->vtime_seqlock, seq));\r\n}\r\nvoid task_cputime(struct task_struct *t, cputime_t *utime, cputime_t *stime)\r\n{\r\ncputime_t udelta, sdelta;\r\nfetch_task_cputime(t, utime, stime, &t->utime,\r\n&t->stime, &udelta, &sdelta);\r\nif (utime)\r\n*utime += udelta;\r\nif (stime)\r\n*stime += sdelta;\r\n}\r\nvoid task_cputime_scaled(struct task_struct *t,\r\ncputime_t *utimescaled, cputime_t *stimescaled)\r\n{\r\ncputime_t udelta, sdelta;\r\nfetch_task_cputime(t, utimescaled, stimescaled,\r\n&t->utimescaled, &t->stimescaled, &udelta, &sdelta);\r\nif (utimescaled)\r\n*utimescaled += cputime_to_scaled(udelta);\r\nif (stimescaled)\r\n*stimescaled += cputime_to_scaled(sdelta);\r\n}
