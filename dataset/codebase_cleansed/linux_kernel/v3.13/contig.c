void show_mem(unsigned int filter)\r\n{\r\nint i, total_reserved = 0;\r\nint total_shared = 0, total_cached = 0;\r\nunsigned long total_present = 0;\r\npg_data_t *pgdat;\r\nprintk(KERN_INFO "Mem-info:\n");\r\nshow_free_areas(filter);\r\nprintk(KERN_INFO "Node memory in pages:\n");\r\nif (filter & SHOW_MEM_FILTER_PAGE_COUNT)\r\nreturn;\r\nfor_each_online_pgdat(pgdat) {\r\nunsigned long present;\r\nunsigned long flags;\r\nint shared = 0, cached = 0, reserved = 0;\r\nint nid = pgdat->node_id;\r\nif (skip_free_areas_node(filter, nid))\r\ncontinue;\r\npgdat_resize_lock(pgdat, &flags);\r\npresent = pgdat->node_present_pages;\r\nfor(i = 0; i < pgdat->node_spanned_pages; i++) {\r\nstruct page *page;\r\nif (unlikely(i % MAX_ORDER_NR_PAGES == 0))\r\ntouch_nmi_watchdog();\r\nif (pfn_valid(pgdat->node_start_pfn + i))\r\npage = pfn_to_page(pgdat->node_start_pfn + i);\r\nelse {\r\n#ifdef CONFIG_VIRTUAL_MEM_MAP\r\nif (max_gap < LARGE_GAP)\r\ncontinue;\r\n#endif\r\ni = vmemmap_find_next_valid_pfn(nid, i) - 1;\r\ncontinue;\r\n}\r\nif (PageReserved(page))\r\nreserved++;\r\nelse if (PageSwapCache(page))\r\ncached++;\r\nelse if (page_count(page))\r\nshared += page_count(page)-1;\r\n}\r\npgdat_resize_unlock(pgdat, &flags);\r\ntotal_present += present;\r\ntotal_reserved += reserved;\r\ntotal_cached += cached;\r\ntotal_shared += shared;\r\nprintk(KERN_INFO "Node %4d: RAM: %11ld, rsvd: %8d, "\r\n"shrd: %10d, swpd: %10d\n", nid,\r\npresent, reserved, shared, cached);\r\n}\r\nprintk(KERN_INFO "%ld pages of RAM\n", total_present);\r\nprintk(KERN_INFO "%d reserved pages\n", total_reserved);\r\nprintk(KERN_INFO "%d pages shared\n", total_shared);\r\nprintk(KERN_INFO "%d pages swap cached\n", total_cached);\r\nprintk(KERN_INFO "Total of %ld pages in page table cache\n",\r\nquicklist_total_size());\r\nprintk(KERN_INFO "%ld free buffer pages\n", nr_free_buffer_pages());\r\n}\r\nstatic int __init\r\nfind_bootmap_location (u64 start, u64 end, void *arg)\r\n{\r\nu64 needed = *(unsigned long *)arg;\r\nu64 range_start, range_end, free_start;\r\nint i;\r\n#if IGNORE_PFN0\r\nif (start == PAGE_OFFSET) {\r\nstart += PAGE_SIZE;\r\nif (start >= end)\r\nreturn 0;\r\n}\r\n#endif\r\nfree_start = PAGE_OFFSET;\r\nfor (i = 0; i < num_rsvd_regions; i++) {\r\nrange_start = max(start, free_start);\r\nrange_end = min(end, rsvd_region[i].start & PAGE_MASK);\r\nfree_start = PAGE_ALIGN(rsvd_region[i].end);\r\nif (range_end <= range_start)\r\ncontinue;\r\nif (range_end - range_start >= needed) {\r\nbootmap_start = __pa(range_start);\r\nreturn -1;\r\n}\r\nif (range_end == end)\r\nreturn 0;\r\n}\r\nreturn 0;\r\n}\r\nvoid *per_cpu_init(void)\r\n{\r\nstatic bool first_time = true;\r\nvoid *cpu0_data = __cpu0_per_cpu;\r\nunsigned int cpu;\r\nif (!first_time)\r\ngoto skip;\r\nfirst_time = false;\r\nfor_each_possible_cpu(cpu) {\r\nvoid *src = cpu == 0 ? cpu0_data : __phys_per_cpu_start;\r\nmemcpy(cpu_data, src, __per_cpu_end - __per_cpu_start);\r\n__per_cpu_offset[cpu] = (char *)cpu_data - __per_cpu_start;\r\nper_cpu(local_per_cpu_offset, cpu) = __per_cpu_offset[cpu];\r\nif (cpu == 0)\r\nia64_set_kr(IA64_KR_PER_CPU_DATA, __pa(cpu_data) -\r\n(unsigned long)__per_cpu_start);\r\ncpu_data += PERCPU_PAGE_SIZE;\r\n}\r\nskip:\r\nreturn __per_cpu_start + __per_cpu_offset[smp_processor_id()];\r\n}\r\nstatic inline void\r\nalloc_per_cpu_data(void)\r\n{\r\ncpu_data = __alloc_bootmem(PERCPU_PAGE_SIZE * num_possible_cpus(),\r\nPERCPU_PAGE_SIZE, __pa(MAX_DMA_ADDRESS));\r\n}\r\nvoid __init\r\nsetup_per_cpu_areas(void)\r\n{\r\nstruct pcpu_alloc_info *ai;\r\nstruct pcpu_group_info *gi;\r\nunsigned int cpu;\r\nssize_t static_size, reserved_size, dyn_size;\r\nint rc;\r\nai = pcpu_alloc_alloc_info(1, num_possible_cpus());\r\nif (!ai)\r\npanic("failed to allocate pcpu_alloc_info");\r\ngi = &ai->groups[0];\r\nfor_each_possible_cpu(cpu)\r\ngi->cpu_map[gi->nr_units++] = cpu;\r\nstatic_size = __per_cpu_end - __per_cpu_start;\r\nreserved_size = PERCPU_MODULE_RESERVE;\r\ndyn_size = PERCPU_PAGE_SIZE - static_size - reserved_size;\r\nif (dyn_size < 0)\r\npanic("percpu area overflow static=%zd reserved=%zd\n",\r\nstatic_size, reserved_size);\r\nai->static_size = static_size;\r\nai->reserved_size = reserved_size;\r\nai->dyn_size = dyn_size;\r\nai->unit_size = PERCPU_PAGE_SIZE;\r\nai->atom_size = PAGE_SIZE;\r\nai->alloc_size = PERCPU_PAGE_SIZE;\r\nrc = pcpu_setup_first_chunk(ai, __per_cpu_start + __per_cpu_offset[0]);\r\nif (rc)\r\npanic("failed to setup percpu area (err=%d)", rc);\r\npcpu_free_alloc_info(ai);\r\n}\r\nvoid __init\r\nfind_memory (void)\r\n{\r\nunsigned long bootmap_size;\r\nreserve_memory();\r\nmin_low_pfn = ~0UL;\r\nmax_low_pfn = 0;\r\nefi_memmap_walk(find_max_min_low_pfn, NULL);\r\nmax_pfn = max_low_pfn;\r\nbootmap_size = bootmem_bootmap_pages(max_pfn) << PAGE_SHIFT;\r\nbootmap_start = ~0UL;\r\nefi_memmap_walk(find_bootmap_location, &bootmap_size);\r\nif (bootmap_start == ~0UL)\r\npanic("Cannot find %ld bytes for bootmap\n", bootmap_size);\r\nbootmap_size = init_bootmem_node(NODE_DATA(0),\r\n(bootmap_start >> PAGE_SHIFT), 0, max_pfn);\r\nefi_memmap_walk(filter_rsvd_memory, free_bootmem);\r\nreserve_bootmem(bootmap_start, bootmap_size, BOOTMEM_DEFAULT);\r\nfind_initrd();\r\nalloc_per_cpu_data();\r\n}\r\nvoid __init\r\npaging_init (void)\r\n{\r\nunsigned long max_dma;\r\nunsigned long max_zone_pfns[MAX_NR_ZONES];\r\nmemset(max_zone_pfns, 0, sizeof(max_zone_pfns));\r\n#ifdef CONFIG_ZONE_DMA\r\nmax_dma = virt_to_phys((void *) MAX_DMA_ADDRESS) >> PAGE_SHIFT;\r\nmax_zone_pfns[ZONE_DMA] = max_dma;\r\n#endif\r\nmax_zone_pfns[ZONE_NORMAL] = max_low_pfn;\r\n#ifdef CONFIG_VIRTUAL_MEM_MAP\r\nefi_memmap_walk(filter_memory, register_active_ranges);\r\nefi_memmap_walk(find_largest_hole, (u64 *)&max_gap);\r\nif (max_gap < LARGE_GAP) {\r\nvmem_map = (struct page *) 0;\r\nfree_area_init_nodes(max_zone_pfns);\r\n} else {\r\nunsigned long map_size;\r\nmap_size = PAGE_ALIGN(ALIGN(max_low_pfn, MAX_ORDER_NR_PAGES) *\r\nsizeof(struct page));\r\nVMALLOC_END -= map_size;\r\nvmem_map = (struct page *) VMALLOC_END;\r\nefi_memmap_walk(create_mem_map_page_table, NULL);\r\nNODE_DATA(0)->node_mem_map = vmem_map +\r\nfind_min_pfn_with_active_regions();\r\nfree_area_init_nodes(max_zone_pfns);\r\nprintk("Virtual mem_map starts at 0x%p\n", mem_map);\r\n}\r\n#else\r\nmemblock_add_node(0, PFN_PHYS(max_low_pfn), 0);\r\nfree_area_init_nodes(max_zone_pfns);\r\n#endif\r\nzero_page_memmap_ptr = virt_to_page(ia64_imva(empty_zero_page));\r\n}
