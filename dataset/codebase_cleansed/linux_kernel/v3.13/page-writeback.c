static unsigned long highmem_dirtyable_memory(unsigned long total)\r\n{\r\n#ifdef CONFIG_HIGHMEM\r\nint node;\r\nunsigned long x = 0;\r\nfor_each_node_state(node, N_HIGH_MEMORY) {\r\nstruct zone *z =\r\n&NODE_DATA(node)->node_zones[ZONE_HIGHMEM];\r\nx += zone_page_state(z, NR_FREE_PAGES) +\r\nzone_reclaimable_pages(z) - z->dirty_balance_reserve;\r\n}\r\nif ((long)x < 0)\r\nx = 0;\r\nreturn min(x, total);\r\n#else\r\nreturn 0;\r\n#endif\r\n}\r\nstatic unsigned long global_dirtyable_memory(void)\r\n{\r\nunsigned long x;\r\nx = global_page_state(NR_FREE_PAGES) + global_reclaimable_pages();\r\nx -= min(x, dirty_balance_reserve);\r\nif (!vm_highmem_is_dirtyable)\r\nx -= highmem_dirtyable_memory(x);\r\nreturn x + 1;\r\n}\r\nvoid global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty)\r\n{\r\nunsigned long background;\r\nunsigned long dirty;\r\nunsigned long uninitialized_var(available_memory);\r\nstruct task_struct *tsk;\r\nif (!vm_dirty_bytes || !dirty_background_bytes)\r\navailable_memory = global_dirtyable_memory();\r\nif (vm_dirty_bytes)\r\ndirty = DIV_ROUND_UP(vm_dirty_bytes, PAGE_SIZE);\r\nelse\r\ndirty = (vm_dirty_ratio * available_memory) / 100;\r\nif (dirty_background_bytes)\r\nbackground = DIV_ROUND_UP(dirty_background_bytes, PAGE_SIZE);\r\nelse\r\nbackground = (dirty_background_ratio * available_memory) / 100;\r\nif (background >= dirty)\r\nbackground = dirty / 2;\r\ntsk = current;\r\nif (tsk->flags & PF_LESS_THROTTLE || rt_task(tsk)) {\r\nbackground += background / 4;\r\ndirty += dirty / 4;\r\n}\r\n*pbackground = background;\r\n*pdirty = dirty;\r\ntrace_global_dirty_state(background, dirty);\r\n}\r\nstatic unsigned long zone_dirtyable_memory(struct zone *zone)\r\n{\r\nunsigned long nr_pages = zone_page_state(zone, NR_FREE_PAGES) +\r\nzone_reclaimable_pages(zone);\r\nnr_pages -= min(nr_pages, zone->dirty_balance_reserve);\r\nreturn nr_pages;\r\n}\r\nstatic unsigned long zone_dirty_limit(struct zone *zone)\r\n{\r\nunsigned long zone_memory = zone_dirtyable_memory(zone);\r\nstruct task_struct *tsk = current;\r\nunsigned long dirty;\r\nif (vm_dirty_bytes)\r\ndirty = DIV_ROUND_UP(vm_dirty_bytes, PAGE_SIZE) *\r\nzone_memory / global_dirtyable_memory();\r\nelse\r\ndirty = vm_dirty_ratio * zone_memory / 100;\r\nif (tsk->flags & PF_LESS_THROTTLE || rt_task(tsk))\r\ndirty += dirty / 4;\r\nreturn dirty;\r\n}\r\nbool zone_dirty_ok(struct zone *zone)\r\n{\r\nunsigned long limit = zone_dirty_limit(zone);\r\nreturn zone_page_state(zone, NR_FILE_DIRTY) +\r\nzone_page_state(zone, NR_UNSTABLE_NFS) +\r\nzone_page_state(zone, NR_WRITEBACK) <= limit;\r\n}\r\nint dirty_background_ratio_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp,\r\nloff_t *ppos)\r\n{\r\nint ret;\r\nret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\r\nif (ret == 0 && write)\r\ndirty_background_bytes = 0;\r\nreturn ret;\r\n}\r\nint dirty_background_bytes_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp,\r\nloff_t *ppos)\r\n{\r\nint ret;\r\nret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);\r\nif (ret == 0 && write)\r\ndirty_background_ratio = 0;\r\nreturn ret;\r\n}\r\nint dirty_ratio_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp,\r\nloff_t *ppos)\r\n{\r\nint old_ratio = vm_dirty_ratio;\r\nint ret;\r\nret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\r\nif (ret == 0 && write && vm_dirty_ratio != old_ratio) {\r\nwriteback_set_ratelimit();\r\nvm_dirty_bytes = 0;\r\n}\r\nreturn ret;\r\n}\r\nint dirty_bytes_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp,\r\nloff_t *ppos)\r\n{\r\nunsigned long old_bytes = vm_dirty_bytes;\r\nint ret;\r\nret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);\r\nif (ret == 0 && write && vm_dirty_bytes != old_bytes) {\r\nwriteback_set_ratelimit();\r\nvm_dirty_ratio = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic unsigned long wp_next_time(unsigned long cur_time)\r\n{\r\ncur_time += VM_COMPLETIONS_PERIOD_LEN;\r\nif (!cur_time)\r\nreturn 1;\r\nreturn cur_time;\r\n}\r\nstatic inline void __bdi_writeout_inc(struct backing_dev_info *bdi)\r\n{\r\n__inc_bdi_stat(bdi, BDI_WRITTEN);\r\n__fprop_inc_percpu_max(&writeout_completions, &bdi->completions,\r\nbdi->max_prop_frac);\r\nif (!unlikely(writeout_period_time)) {\r\nwriteout_period_time = wp_next_time(jiffies);\r\nmod_timer(&writeout_period_timer, writeout_period_time);\r\n}\r\n}\r\nvoid bdi_writeout_inc(struct backing_dev_info *bdi)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\n__bdi_writeout_inc(bdi);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void bdi_writeout_fraction(struct backing_dev_info *bdi,\r\nlong *numerator, long *denominator)\r\n{\r\nfprop_fraction_percpu(&writeout_completions, &bdi->completions,\r\nnumerator, denominator);\r\n}\r\nstatic void writeout_period(unsigned long t)\r\n{\r\nint miss_periods = (jiffies - writeout_period_time) /\r\nVM_COMPLETIONS_PERIOD_LEN;\r\nif (fprop_new_period(&writeout_completions, miss_periods + 1)) {\r\nwriteout_period_time = wp_next_time(writeout_period_time +\r\nmiss_periods * VM_COMPLETIONS_PERIOD_LEN);\r\nmod_timer(&writeout_period_timer, writeout_period_time);\r\n} else {\r\nwriteout_period_time = 0;\r\n}\r\n}\r\nint bdi_set_min_ratio(struct backing_dev_info *bdi, unsigned int min_ratio)\r\n{\r\nint ret = 0;\r\nspin_lock_bh(&bdi_lock);\r\nif (min_ratio > bdi->max_ratio) {\r\nret = -EINVAL;\r\n} else {\r\nmin_ratio -= bdi->min_ratio;\r\nif (bdi_min_ratio + min_ratio < 100) {\r\nbdi_min_ratio += min_ratio;\r\nbdi->min_ratio += min_ratio;\r\n} else {\r\nret = -EINVAL;\r\n}\r\n}\r\nspin_unlock_bh(&bdi_lock);\r\nreturn ret;\r\n}\r\nint bdi_set_max_ratio(struct backing_dev_info *bdi, unsigned max_ratio)\r\n{\r\nint ret = 0;\r\nif (max_ratio > 100)\r\nreturn -EINVAL;\r\nspin_lock_bh(&bdi_lock);\r\nif (bdi->min_ratio > max_ratio) {\r\nret = -EINVAL;\r\n} else {\r\nbdi->max_ratio = max_ratio;\r\nbdi->max_prop_frac = (FPROP_FRAC_BASE * max_ratio) / 100;\r\n}\r\nspin_unlock_bh(&bdi_lock);\r\nreturn ret;\r\n}\r\nstatic unsigned long dirty_freerun_ceiling(unsigned long thresh,\r\nunsigned long bg_thresh)\r\n{\r\nreturn (thresh + bg_thresh) / 2;\r\n}\r\nstatic unsigned long hard_dirty_limit(unsigned long thresh)\r\n{\r\nreturn max(thresh, global_dirty_limit);\r\n}\r\nunsigned long bdi_dirty_limit(struct backing_dev_info *bdi, unsigned long dirty)\r\n{\r\nu64 bdi_dirty;\r\nlong numerator, denominator;\r\nbdi_writeout_fraction(bdi, &numerator, &denominator);\r\nbdi_dirty = (dirty * (100 - bdi_min_ratio)) / 100;\r\nbdi_dirty *= numerator;\r\ndo_div(bdi_dirty, denominator);\r\nbdi_dirty += (dirty * bdi->min_ratio) / 100;\r\nif (bdi_dirty > (dirty * bdi->max_ratio) / 100)\r\nbdi_dirty = dirty * bdi->max_ratio / 100;\r\nreturn bdi_dirty;\r\n}\r\nstatic inline long long pos_ratio_polynom(unsigned long setpoint,\r\nunsigned long dirty,\r\nunsigned long limit)\r\n{\r\nlong long pos_ratio;\r\nlong x;\r\nx = div_s64(((s64)setpoint - (s64)dirty) << RATELIMIT_CALC_SHIFT,\r\nlimit - setpoint + 1);\r\npos_ratio = x;\r\npos_ratio = pos_ratio * x >> RATELIMIT_CALC_SHIFT;\r\npos_ratio = pos_ratio * x >> RATELIMIT_CALC_SHIFT;\r\npos_ratio += 1 << RATELIMIT_CALC_SHIFT;\r\nreturn clamp(pos_ratio, 0LL, 2LL << RATELIMIT_CALC_SHIFT);\r\n}\r\nstatic unsigned long bdi_position_ratio(struct backing_dev_info *bdi,\r\nunsigned long thresh,\r\nunsigned long bg_thresh,\r\nunsigned long dirty,\r\nunsigned long bdi_thresh,\r\nunsigned long bdi_dirty)\r\n{\r\nunsigned long write_bw = bdi->avg_write_bandwidth;\r\nunsigned long freerun = dirty_freerun_ceiling(thresh, bg_thresh);\r\nunsigned long limit = hard_dirty_limit(thresh);\r\nunsigned long x_intercept;\r\nunsigned long setpoint;\r\nunsigned long bdi_setpoint;\r\nunsigned long span;\r\nlong long pos_ratio;\r\nlong x;\r\nif (unlikely(dirty >= limit))\r\nreturn 0;\r\nsetpoint = (freerun + limit) / 2;\r\npos_ratio = pos_ratio_polynom(setpoint, dirty, limit);\r\nif (unlikely(bdi->capabilities & BDI_CAP_STRICTLIMIT)) {\r\nlong long bdi_pos_ratio;\r\nunsigned long bdi_bg_thresh;\r\nif (bdi_dirty < 8)\r\nreturn min_t(long long, pos_ratio * 2,\r\n2 << RATELIMIT_CALC_SHIFT);\r\nif (bdi_dirty >= bdi_thresh)\r\nreturn 0;\r\nbdi_bg_thresh = div_u64((u64)bdi_thresh * bg_thresh, thresh);\r\nbdi_setpoint = dirty_freerun_ceiling(bdi_thresh,\r\nbdi_bg_thresh);\r\nif (bdi_setpoint == 0 || bdi_setpoint == bdi_thresh)\r\nreturn 0;\r\nbdi_pos_ratio = pos_ratio_polynom(bdi_setpoint, bdi_dirty,\r\nbdi_thresh);\r\nreturn min(pos_ratio, bdi_pos_ratio);\r\n}\r\nif (unlikely(bdi_thresh > thresh))\r\nbdi_thresh = thresh;\r\nbdi_thresh = max(bdi_thresh, (limit - dirty) / 8);\r\nx = div_u64((u64)bdi_thresh << 16, thresh + 1);\r\nbdi_setpoint = setpoint * (u64)x >> 16;\r\nspan = (thresh - bdi_thresh + 8 * write_bw) * (u64)x >> 16;\r\nx_intercept = bdi_setpoint + span;\r\nif (bdi_dirty < x_intercept - span / 4) {\r\npos_ratio = div_u64(pos_ratio * (x_intercept - bdi_dirty),\r\nx_intercept - bdi_setpoint + 1);\r\n} else\r\npos_ratio /= 4;\r\nx_intercept = bdi_thresh / 2;\r\nif (bdi_dirty < x_intercept) {\r\nif (bdi_dirty > x_intercept / 8)\r\npos_ratio = div_u64(pos_ratio * x_intercept, bdi_dirty);\r\nelse\r\npos_ratio *= 8;\r\n}\r\nreturn pos_ratio;\r\n}\r\nstatic void bdi_update_write_bandwidth(struct backing_dev_info *bdi,\r\nunsigned long elapsed,\r\nunsigned long written)\r\n{\r\nconst unsigned long period = roundup_pow_of_two(3 * HZ);\r\nunsigned long avg = bdi->avg_write_bandwidth;\r\nunsigned long old = bdi->write_bandwidth;\r\nu64 bw;\r\nbw = written - bdi->written_stamp;\r\nbw *= HZ;\r\nif (unlikely(elapsed > period)) {\r\ndo_div(bw, elapsed);\r\navg = bw;\r\ngoto out;\r\n}\r\nbw += (u64)bdi->write_bandwidth * (period - elapsed);\r\nbw >>= ilog2(period);\r\nif (avg > old && old >= (unsigned long)bw)\r\navg -= (avg - old) >> 3;\r\nif (avg < old && old <= (unsigned long)bw)\r\navg += (old - avg) >> 3;\r\nout:\r\nbdi->write_bandwidth = bw;\r\nbdi->avg_write_bandwidth = avg;\r\n}\r\nstatic void update_dirty_limit(unsigned long thresh, unsigned long dirty)\r\n{\r\nunsigned long limit = global_dirty_limit;\r\nif (limit < thresh) {\r\nlimit = thresh;\r\ngoto update;\r\n}\r\nthresh = max(thresh, dirty);\r\nif (limit > thresh) {\r\nlimit -= (limit - thresh) >> 5;\r\ngoto update;\r\n}\r\nreturn;\r\nupdate:\r\nglobal_dirty_limit = limit;\r\n}\r\nstatic void global_update_bandwidth(unsigned long thresh,\r\nunsigned long dirty,\r\nunsigned long now)\r\n{\r\nstatic DEFINE_SPINLOCK(dirty_lock);\r\nstatic unsigned long update_time;\r\nif (time_before(now, update_time + BANDWIDTH_INTERVAL))\r\nreturn;\r\nspin_lock(&dirty_lock);\r\nif (time_after_eq(now, update_time + BANDWIDTH_INTERVAL)) {\r\nupdate_dirty_limit(thresh, dirty);\r\nupdate_time = now;\r\n}\r\nspin_unlock(&dirty_lock);\r\n}\r\nstatic void bdi_update_dirty_ratelimit(struct backing_dev_info *bdi,\r\nunsigned long thresh,\r\nunsigned long bg_thresh,\r\nunsigned long dirty,\r\nunsigned long bdi_thresh,\r\nunsigned long bdi_dirty,\r\nunsigned long dirtied,\r\nunsigned long elapsed)\r\n{\r\nunsigned long freerun = dirty_freerun_ceiling(thresh, bg_thresh);\r\nunsigned long limit = hard_dirty_limit(thresh);\r\nunsigned long setpoint = (freerun + limit) / 2;\r\nunsigned long write_bw = bdi->avg_write_bandwidth;\r\nunsigned long dirty_ratelimit = bdi->dirty_ratelimit;\r\nunsigned long dirty_rate;\r\nunsigned long task_ratelimit;\r\nunsigned long balanced_dirty_ratelimit;\r\nunsigned long pos_ratio;\r\nunsigned long step;\r\nunsigned long x;\r\ndirty_rate = (dirtied - bdi->dirtied_stamp) * HZ / elapsed;\r\npos_ratio = bdi_position_ratio(bdi, thresh, bg_thresh, dirty,\r\nbdi_thresh, bdi_dirty);\r\ntask_ratelimit = (u64)dirty_ratelimit *\r\npos_ratio >> RATELIMIT_CALC_SHIFT;\r\ntask_ratelimit++;\r\nbalanced_dirty_ratelimit = div_u64((u64)task_ratelimit * write_bw,\r\ndirty_rate | 1);\r\nif (unlikely(balanced_dirty_ratelimit > write_bw))\r\nbalanced_dirty_ratelimit = write_bw;\r\nstep = 0;\r\nif (unlikely(bdi->capabilities & BDI_CAP_STRICTLIMIT)) {\r\ndirty = bdi_dirty;\r\nif (bdi_dirty < 8)\r\nsetpoint = bdi_dirty + 1;\r\nelse\r\nsetpoint = (bdi_thresh +\r\nbdi_dirty_limit(bdi, bg_thresh)) / 2;\r\n}\r\nif (dirty < setpoint) {\r\nx = min(bdi->balanced_dirty_ratelimit,\r\nmin(balanced_dirty_ratelimit, task_ratelimit));\r\nif (dirty_ratelimit < x)\r\nstep = x - dirty_ratelimit;\r\n} else {\r\nx = max(bdi->balanced_dirty_ratelimit,\r\nmax(balanced_dirty_ratelimit, task_ratelimit));\r\nif (dirty_ratelimit > x)\r\nstep = dirty_ratelimit - x;\r\n}\r\nstep >>= dirty_ratelimit / (2 * step + 1);\r\nstep = (step + 7) / 8;\r\nif (dirty_ratelimit < balanced_dirty_ratelimit)\r\ndirty_ratelimit += step;\r\nelse\r\ndirty_ratelimit -= step;\r\nbdi->dirty_ratelimit = max(dirty_ratelimit, 1UL);\r\nbdi->balanced_dirty_ratelimit = balanced_dirty_ratelimit;\r\ntrace_bdi_dirty_ratelimit(bdi, dirty_rate, task_ratelimit);\r\n}\r\nvoid __bdi_update_bandwidth(struct backing_dev_info *bdi,\r\nunsigned long thresh,\r\nunsigned long bg_thresh,\r\nunsigned long dirty,\r\nunsigned long bdi_thresh,\r\nunsigned long bdi_dirty,\r\nunsigned long start_time)\r\n{\r\nunsigned long now = jiffies;\r\nunsigned long elapsed = now - bdi->bw_time_stamp;\r\nunsigned long dirtied;\r\nunsigned long written;\r\nif (elapsed < BANDWIDTH_INTERVAL)\r\nreturn;\r\ndirtied = percpu_counter_read(&bdi->bdi_stat[BDI_DIRTIED]);\r\nwritten = percpu_counter_read(&bdi->bdi_stat[BDI_WRITTEN]);\r\nif (elapsed > HZ && time_before(bdi->bw_time_stamp, start_time))\r\ngoto snapshot;\r\nif (thresh) {\r\nglobal_update_bandwidth(thresh, dirty, now);\r\nbdi_update_dirty_ratelimit(bdi, thresh, bg_thresh, dirty,\r\nbdi_thresh, bdi_dirty,\r\ndirtied, elapsed);\r\n}\r\nbdi_update_write_bandwidth(bdi, elapsed, written);\r\nsnapshot:\r\nbdi->dirtied_stamp = dirtied;\r\nbdi->written_stamp = written;\r\nbdi->bw_time_stamp = now;\r\n}\r\nstatic void bdi_update_bandwidth(struct backing_dev_info *bdi,\r\nunsigned long thresh,\r\nunsigned long bg_thresh,\r\nunsigned long dirty,\r\nunsigned long bdi_thresh,\r\nunsigned long bdi_dirty,\r\nunsigned long start_time)\r\n{\r\nif (time_is_after_eq_jiffies(bdi->bw_time_stamp + BANDWIDTH_INTERVAL))\r\nreturn;\r\nspin_lock(&bdi->wb.list_lock);\r\n__bdi_update_bandwidth(bdi, thresh, bg_thresh, dirty,\r\nbdi_thresh, bdi_dirty, start_time);\r\nspin_unlock(&bdi->wb.list_lock);\r\n}\r\nstatic unsigned long dirty_poll_interval(unsigned long dirty,\r\nunsigned long thresh)\r\n{\r\nif (thresh > dirty)\r\nreturn 1UL << (ilog2(thresh - dirty) >> 1);\r\nreturn 1;\r\n}\r\nstatic unsigned long bdi_max_pause(struct backing_dev_info *bdi,\r\nunsigned long bdi_dirty)\r\n{\r\nunsigned long bw = bdi->avg_write_bandwidth;\r\nunsigned long t;\r\nt = bdi_dirty / (1 + bw / roundup_pow_of_two(1 + HZ / 8));\r\nt++;\r\nreturn min_t(unsigned long, t, MAX_PAUSE);\r\n}\r\nstatic long bdi_min_pause(struct backing_dev_info *bdi,\r\nlong max_pause,\r\nunsigned long task_ratelimit,\r\nunsigned long dirty_ratelimit,\r\nint *nr_dirtied_pause)\r\n{\r\nlong hi = ilog2(bdi->avg_write_bandwidth);\r\nlong lo = ilog2(bdi->dirty_ratelimit);\r\nlong t;\r\nlong pause;\r\nint pages;\r\nt = max(1, HZ / 100);\r\nif (hi > lo)\r\nt += (hi - lo) * (10 * HZ) / 1024;\r\nt = min(t, 1 + max_pause / 2);\r\npages = dirty_ratelimit * t / roundup_pow_of_two(HZ);\r\nif (pages < DIRTY_POLL_THRESH) {\r\nt = max_pause;\r\npages = dirty_ratelimit * t / roundup_pow_of_two(HZ);\r\nif (pages > DIRTY_POLL_THRESH) {\r\npages = DIRTY_POLL_THRESH;\r\nt = HZ * DIRTY_POLL_THRESH / dirty_ratelimit;\r\n}\r\n}\r\npause = HZ * pages / (task_ratelimit + 1);\r\nif (pause > max_pause) {\r\nt = max_pause;\r\npages = task_ratelimit * t / roundup_pow_of_two(HZ);\r\n}\r\n*nr_dirtied_pause = pages;\r\nreturn pages >= DIRTY_POLL_THRESH ? 1 + t / 2 : t;\r\n}\r\nstatic inline void bdi_dirty_limits(struct backing_dev_info *bdi,\r\nunsigned long dirty_thresh,\r\nunsigned long background_thresh,\r\nunsigned long *bdi_dirty,\r\nunsigned long *bdi_thresh,\r\nunsigned long *bdi_bg_thresh)\r\n{\r\nunsigned long bdi_reclaimable;\r\n*bdi_thresh = bdi_dirty_limit(bdi, dirty_thresh);\r\nif (bdi_bg_thresh)\r\n*bdi_bg_thresh = div_u64((u64)*bdi_thresh *\r\nbackground_thresh,\r\ndirty_thresh);\r\nif (*bdi_thresh < 2 * bdi_stat_error(bdi)) {\r\nbdi_reclaimable = bdi_stat_sum(bdi, BDI_RECLAIMABLE);\r\n*bdi_dirty = bdi_reclaimable +\r\nbdi_stat_sum(bdi, BDI_WRITEBACK);\r\n} else {\r\nbdi_reclaimable = bdi_stat(bdi, BDI_RECLAIMABLE);\r\n*bdi_dirty = bdi_reclaimable +\r\nbdi_stat(bdi, BDI_WRITEBACK);\r\n}\r\n}\r\nstatic void balance_dirty_pages(struct address_space *mapping,\r\nunsigned long pages_dirtied)\r\n{\r\nunsigned long nr_reclaimable;\r\nunsigned long nr_dirty;\r\nunsigned long background_thresh;\r\nunsigned long dirty_thresh;\r\nlong period;\r\nlong pause;\r\nlong max_pause;\r\nlong min_pause;\r\nint nr_dirtied_pause;\r\nbool dirty_exceeded = false;\r\nunsigned long task_ratelimit;\r\nunsigned long dirty_ratelimit;\r\nunsigned long pos_ratio;\r\nstruct backing_dev_info *bdi = mapping->backing_dev_info;\r\nbool strictlimit = bdi->capabilities & BDI_CAP_STRICTLIMIT;\r\nunsigned long start_time = jiffies;\r\nfor (;;) {\r\nunsigned long now = jiffies;\r\nunsigned long uninitialized_var(bdi_thresh);\r\nunsigned long thresh;\r\nunsigned long uninitialized_var(bdi_dirty);\r\nunsigned long dirty;\r\nunsigned long bg_thresh;\r\nnr_reclaimable = global_page_state(NR_FILE_DIRTY) +\r\nglobal_page_state(NR_UNSTABLE_NFS);\r\nnr_dirty = nr_reclaimable + global_page_state(NR_WRITEBACK);\r\nglobal_dirty_limits(&background_thresh, &dirty_thresh);\r\nif (unlikely(strictlimit)) {\r\nbdi_dirty_limits(bdi, dirty_thresh, background_thresh,\r\n&bdi_dirty, &bdi_thresh, &bg_thresh);\r\ndirty = bdi_dirty;\r\nthresh = bdi_thresh;\r\n} else {\r\ndirty = nr_dirty;\r\nthresh = dirty_thresh;\r\nbg_thresh = background_thresh;\r\n}\r\nif (dirty <= dirty_freerun_ceiling(thresh, bg_thresh)) {\r\ncurrent->dirty_paused_when = now;\r\ncurrent->nr_dirtied = 0;\r\ncurrent->nr_dirtied_pause =\r\ndirty_poll_interval(dirty, thresh);\r\nbreak;\r\n}\r\nif (unlikely(!writeback_in_progress(bdi)))\r\nbdi_start_background_writeback(bdi);\r\nif (!strictlimit)\r\nbdi_dirty_limits(bdi, dirty_thresh, background_thresh,\r\n&bdi_dirty, &bdi_thresh, NULL);\r\ndirty_exceeded = (bdi_dirty > bdi_thresh) &&\r\n((nr_dirty > dirty_thresh) || strictlimit);\r\nif (dirty_exceeded && !bdi->dirty_exceeded)\r\nbdi->dirty_exceeded = 1;\r\nbdi_update_bandwidth(bdi, dirty_thresh, background_thresh,\r\nnr_dirty, bdi_thresh, bdi_dirty,\r\nstart_time);\r\ndirty_ratelimit = bdi->dirty_ratelimit;\r\npos_ratio = bdi_position_ratio(bdi, dirty_thresh,\r\nbackground_thresh, nr_dirty,\r\nbdi_thresh, bdi_dirty);\r\ntask_ratelimit = ((u64)dirty_ratelimit * pos_ratio) >>\r\nRATELIMIT_CALC_SHIFT;\r\nmax_pause = bdi_max_pause(bdi, bdi_dirty);\r\nmin_pause = bdi_min_pause(bdi, max_pause,\r\ntask_ratelimit, dirty_ratelimit,\r\n&nr_dirtied_pause);\r\nif (unlikely(task_ratelimit == 0)) {\r\nperiod = max_pause;\r\npause = max_pause;\r\ngoto pause;\r\n}\r\nperiod = HZ * pages_dirtied / task_ratelimit;\r\npause = period;\r\nif (current->dirty_paused_when)\r\npause -= now - current->dirty_paused_when;\r\nif (pause < min_pause) {\r\ntrace_balance_dirty_pages(bdi,\r\ndirty_thresh,\r\nbackground_thresh,\r\nnr_dirty,\r\nbdi_thresh,\r\nbdi_dirty,\r\ndirty_ratelimit,\r\ntask_ratelimit,\r\npages_dirtied,\r\nperiod,\r\nmin(pause, 0L),\r\nstart_time);\r\nif (pause < -HZ) {\r\ncurrent->dirty_paused_when = now;\r\ncurrent->nr_dirtied = 0;\r\n} else if (period) {\r\ncurrent->dirty_paused_when += period;\r\ncurrent->nr_dirtied = 0;\r\n} else if (current->nr_dirtied_pause <= pages_dirtied)\r\ncurrent->nr_dirtied_pause += pages_dirtied;\r\nbreak;\r\n}\r\nif (unlikely(pause > max_pause)) {\r\nnow += min(pause - max_pause, max_pause);\r\npause = max_pause;\r\n}\r\npause:\r\ntrace_balance_dirty_pages(bdi,\r\ndirty_thresh,\r\nbackground_thresh,\r\nnr_dirty,\r\nbdi_thresh,\r\nbdi_dirty,\r\ndirty_ratelimit,\r\ntask_ratelimit,\r\npages_dirtied,\r\nperiod,\r\npause,\r\nstart_time);\r\n__set_current_state(TASK_KILLABLE);\r\nio_schedule_timeout(pause);\r\ncurrent->dirty_paused_when = now + pause;\r\ncurrent->nr_dirtied = 0;\r\ncurrent->nr_dirtied_pause = nr_dirtied_pause;\r\nif (task_ratelimit)\r\nbreak;\r\nif (bdi_dirty <= bdi_stat_error(bdi))\r\nbreak;\r\nif (fatal_signal_pending(current))\r\nbreak;\r\n}\r\nif (!dirty_exceeded && bdi->dirty_exceeded)\r\nbdi->dirty_exceeded = 0;\r\nif (writeback_in_progress(bdi))\r\nreturn;\r\nif (laptop_mode)\r\nreturn;\r\nif (nr_reclaimable > background_thresh)\r\nbdi_start_background_writeback(bdi);\r\n}\r\nvoid set_page_dirty_balance(struct page *page, int page_mkwrite)\r\n{\r\nif (set_page_dirty(page) || page_mkwrite) {\r\nstruct address_space *mapping = page_mapping(page);\r\nif (mapping)\r\nbalance_dirty_pages_ratelimited(mapping);\r\n}\r\n}\r\nvoid balance_dirty_pages_ratelimited(struct address_space *mapping)\r\n{\r\nstruct backing_dev_info *bdi = mapping->backing_dev_info;\r\nint ratelimit;\r\nint *p;\r\nif (!bdi_cap_account_dirty(bdi))\r\nreturn;\r\nratelimit = current->nr_dirtied_pause;\r\nif (bdi->dirty_exceeded)\r\nratelimit = min(ratelimit, 32 >> (PAGE_SHIFT - 10));\r\npreempt_disable();\r\np = &__get_cpu_var(bdp_ratelimits);\r\nif (unlikely(current->nr_dirtied >= ratelimit))\r\n*p = 0;\r\nelse if (unlikely(*p >= ratelimit_pages)) {\r\n*p = 0;\r\nratelimit = 0;\r\n}\r\np = &__get_cpu_var(dirty_throttle_leaks);\r\nif (*p > 0 && current->nr_dirtied < ratelimit) {\r\nunsigned long nr_pages_dirtied;\r\nnr_pages_dirtied = min(*p, ratelimit - current->nr_dirtied);\r\n*p -= nr_pages_dirtied;\r\ncurrent->nr_dirtied += nr_pages_dirtied;\r\n}\r\npreempt_enable();\r\nif (unlikely(current->nr_dirtied >= ratelimit))\r\nbalance_dirty_pages(mapping, current->nr_dirtied);\r\n}\r\nvoid throttle_vm_writeout(gfp_t gfp_mask)\r\n{\r\nunsigned long background_thresh;\r\nunsigned long dirty_thresh;\r\nfor ( ; ; ) {\r\nglobal_dirty_limits(&background_thresh, &dirty_thresh);\r\ndirty_thresh = hard_dirty_limit(dirty_thresh);\r\ndirty_thresh += dirty_thresh / 10;\r\nif (global_page_state(NR_UNSTABLE_NFS) +\r\nglobal_page_state(NR_WRITEBACK) <= dirty_thresh)\r\nbreak;\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\nif ((gfp_mask & (__GFP_FS|__GFP_IO)) != (__GFP_FS|__GFP_IO))\r\nbreak;\r\n}\r\n}\r\nint dirty_writeback_centisecs_handler(ctl_table *table, int write,\r\nvoid __user *buffer, size_t *length, loff_t *ppos)\r\n{\r\nproc_dointvec(table, write, buffer, length, ppos);\r\nreturn 0;\r\n}\r\nvoid laptop_mode_timer_fn(unsigned long data)\r\n{\r\nstruct request_queue *q = (struct request_queue *)data;\r\nint nr_pages = global_page_state(NR_FILE_DIRTY) +\r\nglobal_page_state(NR_UNSTABLE_NFS);\r\nif (bdi_has_dirty_io(&q->backing_dev_info))\r\nbdi_start_writeback(&q->backing_dev_info, nr_pages,\r\nWB_REASON_LAPTOP_TIMER);\r\n}\r\nvoid laptop_io_completion(struct backing_dev_info *info)\r\n{\r\nmod_timer(&info->laptop_mode_wb_timer, jiffies + laptop_mode);\r\n}\r\nvoid laptop_sync_completion(void)\r\n{\r\nstruct backing_dev_info *bdi;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list)\r\ndel_timer(&bdi->laptop_mode_wb_timer);\r\nrcu_read_unlock();\r\n}\r\nvoid writeback_set_ratelimit(void)\r\n{\r\nunsigned long background_thresh;\r\nunsigned long dirty_thresh;\r\nglobal_dirty_limits(&background_thresh, &dirty_thresh);\r\nglobal_dirty_limit = dirty_thresh;\r\nratelimit_pages = dirty_thresh / (num_online_cpus() * 32);\r\nif (ratelimit_pages < 16)\r\nratelimit_pages = 16;\r\n}\r\nstatic int\r\nratelimit_handler(struct notifier_block *self, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_ONLINE:\r\ncase CPU_DEAD:\r\nwriteback_set_ratelimit();\r\nreturn NOTIFY_OK;\r\ndefault:\r\nreturn NOTIFY_DONE;\r\n}\r\n}\r\nvoid __init page_writeback_init(void)\r\n{\r\nwriteback_set_ratelimit();\r\nregister_cpu_notifier(&ratelimit_nb);\r\nfprop_global_init(&writeout_completions);\r\n}\r\nvoid tag_pages_for_writeback(struct address_space *mapping,\r\npgoff_t start, pgoff_t end)\r\n{\r\n#define WRITEBACK_TAG_BATCH 4096\r\nunsigned long tagged;\r\ndo {\r\nspin_lock_irq(&mapping->tree_lock);\r\ntagged = radix_tree_range_tag_if_tagged(&mapping->page_tree,\r\n&start, end, WRITEBACK_TAG_BATCH,\r\nPAGECACHE_TAG_DIRTY, PAGECACHE_TAG_TOWRITE);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nWARN_ON_ONCE(tagged > WRITEBACK_TAG_BATCH);\r\ncond_resched();\r\n} while (tagged >= WRITEBACK_TAG_BATCH && start);\r\n}\r\nint write_cache_pages(struct address_space *mapping,\r\nstruct writeback_control *wbc, writepage_t writepage,\r\nvoid *data)\r\n{\r\nint ret = 0;\r\nint done = 0;\r\nstruct pagevec pvec;\r\nint nr_pages;\r\npgoff_t uninitialized_var(writeback_index);\r\npgoff_t index;\r\npgoff_t end;\r\npgoff_t done_index;\r\nint cycled;\r\nint range_whole = 0;\r\nint tag;\r\npagevec_init(&pvec, 0);\r\nif (wbc->range_cyclic) {\r\nwriteback_index = mapping->writeback_index;\r\nindex = writeback_index;\r\nif (index == 0)\r\ncycled = 1;\r\nelse\r\ncycled = 0;\r\nend = -1;\r\n} else {\r\nindex = wbc->range_start >> PAGE_CACHE_SHIFT;\r\nend = wbc->range_end >> PAGE_CACHE_SHIFT;\r\nif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\r\nrange_whole = 1;\r\ncycled = 1;\r\n}\r\nif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\r\ntag = PAGECACHE_TAG_TOWRITE;\r\nelse\r\ntag = PAGECACHE_TAG_DIRTY;\r\nretry:\r\nif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\r\ntag_pages_for_writeback(mapping, index, end);\r\ndone_index = index;\r\nwhile (!done && (index <= end)) {\r\nint i;\r\nnr_pages = pagevec_lookup_tag(&pvec, mapping, &index, tag,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1);\r\nif (nr_pages == 0)\r\nbreak;\r\nfor (i = 0; i < nr_pages; i++) {\r\nstruct page *page = pvec.pages[i];\r\nif (page->index > end) {\r\ndone = 1;\r\nbreak;\r\n}\r\ndone_index = page->index;\r\nlock_page(page);\r\nif (unlikely(page->mapping != mapping)) {\r\ncontinue_unlock:\r\nunlock_page(page);\r\ncontinue;\r\n}\r\nif (!PageDirty(page)) {\r\ngoto continue_unlock;\r\n}\r\nif (PageWriteback(page)) {\r\nif (wbc->sync_mode != WB_SYNC_NONE)\r\nwait_on_page_writeback(page);\r\nelse\r\ngoto continue_unlock;\r\n}\r\nBUG_ON(PageWriteback(page));\r\nif (!clear_page_dirty_for_io(page))\r\ngoto continue_unlock;\r\ntrace_wbc_writepage(wbc, mapping->backing_dev_info);\r\nret = (*writepage)(page, wbc, data);\r\nif (unlikely(ret)) {\r\nif (ret == AOP_WRITEPAGE_ACTIVATE) {\r\nunlock_page(page);\r\nret = 0;\r\n} else {\r\ndone_index = page->index + 1;\r\ndone = 1;\r\nbreak;\r\n}\r\n}\r\nif (--wbc->nr_to_write <= 0 &&\r\nwbc->sync_mode == WB_SYNC_NONE) {\r\ndone = 1;\r\nbreak;\r\n}\r\n}\r\npagevec_release(&pvec);\r\ncond_resched();\r\n}\r\nif (!cycled && !done) {\r\ncycled = 1;\r\nindex = 0;\r\nend = writeback_index - 1;\r\ngoto retry;\r\n}\r\nif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\r\nmapping->writeback_index = done_index;\r\nreturn ret;\r\n}\r\nstatic int __writepage(struct page *page, struct writeback_control *wbc,\r\nvoid *data)\r\n{\r\nstruct address_space *mapping = data;\r\nint ret = mapping->a_ops->writepage(page, wbc);\r\nmapping_set_error(mapping, ret);\r\nreturn ret;\r\n}\r\nint generic_writepages(struct address_space *mapping,\r\nstruct writeback_control *wbc)\r\n{\r\nstruct blk_plug plug;\r\nint ret;\r\nif (!mapping->a_ops->writepage)\r\nreturn 0;\r\nblk_start_plug(&plug);\r\nret = write_cache_pages(mapping, wbc, __writepage, mapping);\r\nblk_finish_plug(&plug);\r\nreturn ret;\r\n}\r\nint do_writepages(struct address_space *mapping, struct writeback_control *wbc)\r\n{\r\nint ret;\r\nif (wbc->nr_to_write <= 0)\r\nreturn 0;\r\nif (mapping->a_ops->writepages)\r\nret = mapping->a_ops->writepages(mapping, wbc);\r\nelse\r\nret = generic_writepages(mapping, wbc);\r\nreturn ret;\r\n}\r\nint write_one_page(struct page *page, int wait)\r\n{\r\nstruct address_space *mapping = page->mapping;\r\nint ret = 0;\r\nstruct writeback_control wbc = {\r\n.sync_mode = WB_SYNC_ALL,\r\n.nr_to_write = 1,\r\n};\r\nBUG_ON(!PageLocked(page));\r\nif (wait)\r\nwait_on_page_writeback(page);\r\nif (clear_page_dirty_for_io(page)) {\r\npage_cache_get(page);\r\nret = mapping->a_ops->writepage(page, &wbc);\r\nif (ret == 0 && wait) {\r\nwait_on_page_writeback(page);\r\nif (PageError(page))\r\nret = -EIO;\r\n}\r\npage_cache_release(page);\r\n} else {\r\nunlock_page(page);\r\n}\r\nreturn ret;\r\n}\r\nint __set_page_dirty_no_writeback(struct page *page)\r\n{\r\nif (!PageDirty(page))\r\nreturn !TestSetPageDirty(page);\r\nreturn 0;\r\n}\r\nvoid account_page_dirtied(struct page *page, struct address_space *mapping)\r\n{\r\ntrace_writeback_dirty_page(page, mapping);\r\nif (mapping_cap_account_dirty(mapping)) {\r\n__inc_zone_page_state(page, NR_FILE_DIRTY);\r\n__inc_zone_page_state(page, NR_DIRTIED);\r\n__inc_bdi_stat(mapping->backing_dev_info, BDI_RECLAIMABLE);\r\n__inc_bdi_stat(mapping->backing_dev_info, BDI_DIRTIED);\r\ntask_io_account_write(PAGE_CACHE_SIZE);\r\ncurrent->nr_dirtied++;\r\nthis_cpu_inc(bdp_ratelimits);\r\n}\r\n}\r\nvoid account_page_writeback(struct page *page)\r\n{\r\nmem_cgroup_inc_page_stat(page, MEM_CGROUP_STAT_WRITEBACK);\r\ninc_zone_page_state(page, NR_WRITEBACK);\r\n}\r\nint __set_page_dirty_nobuffers(struct page *page)\r\n{\r\nif (!TestSetPageDirty(page)) {\r\nstruct address_space *mapping = page_mapping(page);\r\nstruct address_space *mapping2;\r\nif (!mapping)\r\nreturn 1;\r\nspin_lock_irq(&mapping->tree_lock);\r\nmapping2 = page_mapping(page);\r\nif (mapping2) {\r\nBUG_ON(mapping2 != mapping);\r\nWARN_ON_ONCE(!PagePrivate(page) && !PageUptodate(page));\r\naccount_page_dirtied(page, mapping);\r\nradix_tree_tag_set(&mapping->page_tree,\r\npage_index(page), PAGECACHE_TAG_DIRTY);\r\n}\r\nspin_unlock_irq(&mapping->tree_lock);\r\nif (mapping->host) {\r\n__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\r\n}\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid account_page_redirty(struct page *page)\r\n{\r\nstruct address_space *mapping = page->mapping;\r\nif (mapping && mapping_cap_account_dirty(mapping)) {\r\ncurrent->nr_dirtied--;\r\ndec_zone_page_state(page, NR_DIRTIED);\r\ndec_bdi_stat(mapping->backing_dev_info, BDI_DIRTIED);\r\n}\r\n}\r\nint redirty_page_for_writepage(struct writeback_control *wbc, struct page *page)\r\n{\r\nwbc->pages_skipped++;\r\naccount_page_redirty(page);\r\nreturn __set_page_dirty_nobuffers(page);\r\n}\r\nint set_page_dirty(struct page *page)\r\n{\r\nstruct address_space *mapping = page_mapping(page);\r\nif (likely(mapping)) {\r\nint (*spd)(struct page *) = mapping->a_ops->set_page_dirty;\r\nClearPageReclaim(page);\r\n#ifdef CONFIG_BLOCK\r\nif (!spd)\r\nspd = __set_page_dirty_buffers;\r\n#endif\r\nreturn (*spd)(page);\r\n}\r\nif (!PageDirty(page)) {\r\nif (!TestSetPageDirty(page))\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nint set_page_dirty_lock(struct page *page)\r\n{\r\nint ret;\r\nlock_page(page);\r\nret = set_page_dirty(page);\r\nunlock_page(page);\r\nreturn ret;\r\n}\r\nint clear_page_dirty_for_io(struct page *page)\r\n{\r\nstruct address_space *mapping = page_mapping(page);\r\nBUG_ON(!PageLocked(page));\r\nif (mapping && mapping_cap_account_dirty(mapping)) {\r\nif (page_mkclean(page))\r\nset_page_dirty(page);\r\nif (TestClearPageDirty(page)) {\r\ndec_zone_page_state(page, NR_FILE_DIRTY);\r\ndec_bdi_stat(mapping->backing_dev_info,\r\nBDI_RECLAIMABLE);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nreturn TestClearPageDirty(page);\r\n}\r\nint test_clear_page_writeback(struct page *page)\r\n{\r\nstruct address_space *mapping = page_mapping(page);\r\nint ret;\r\nbool locked;\r\nunsigned long memcg_flags;\r\nmem_cgroup_begin_update_page_stat(page, &locked, &memcg_flags);\r\nif (mapping) {\r\nstruct backing_dev_info *bdi = mapping->backing_dev_info;\r\nunsigned long flags;\r\nspin_lock_irqsave(&mapping->tree_lock, flags);\r\nret = TestClearPageWriteback(page);\r\nif (ret) {\r\nradix_tree_tag_clear(&mapping->page_tree,\r\npage_index(page),\r\nPAGECACHE_TAG_WRITEBACK);\r\nif (bdi_cap_account_writeback(bdi)) {\r\n__dec_bdi_stat(bdi, BDI_WRITEBACK);\r\n__bdi_writeout_inc(bdi);\r\n}\r\n}\r\nspin_unlock_irqrestore(&mapping->tree_lock, flags);\r\n} else {\r\nret = TestClearPageWriteback(page);\r\n}\r\nif (ret) {\r\nmem_cgroup_dec_page_stat(page, MEM_CGROUP_STAT_WRITEBACK);\r\ndec_zone_page_state(page, NR_WRITEBACK);\r\ninc_zone_page_state(page, NR_WRITTEN);\r\n}\r\nmem_cgroup_end_update_page_stat(page, &locked, &memcg_flags);\r\nreturn ret;\r\n}\r\nint test_set_page_writeback(struct page *page)\r\n{\r\nstruct address_space *mapping = page_mapping(page);\r\nint ret;\r\nbool locked;\r\nunsigned long memcg_flags;\r\nmem_cgroup_begin_update_page_stat(page, &locked, &memcg_flags);\r\nif (mapping) {\r\nstruct backing_dev_info *bdi = mapping->backing_dev_info;\r\nunsigned long flags;\r\nspin_lock_irqsave(&mapping->tree_lock, flags);\r\nret = TestSetPageWriteback(page);\r\nif (!ret) {\r\nradix_tree_tag_set(&mapping->page_tree,\r\npage_index(page),\r\nPAGECACHE_TAG_WRITEBACK);\r\nif (bdi_cap_account_writeback(bdi))\r\n__inc_bdi_stat(bdi, BDI_WRITEBACK);\r\n}\r\nif (!PageDirty(page))\r\nradix_tree_tag_clear(&mapping->page_tree,\r\npage_index(page),\r\nPAGECACHE_TAG_DIRTY);\r\nradix_tree_tag_clear(&mapping->page_tree,\r\npage_index(page),\r\nPAGECACHE_TAG_TOWRITE);\r\nspin_unlock_irqrestore(&mapping->tree_lock, flags);\r\n} else {\r\nret = TestSetPageWriteback(page);\r\n}\r\nif (!ret)\r\naccount_page_writeback(page);\r\nmem_cgroup_end_update_page_stat(page, &locked, &memcg_flags);\r\nreturn ret;\r\n}\r\nint mapping_tagged(struct address_space *mapping, int tag)\r\n{\r\nreturn radix_tree_tagged(&mapping->page_tree, tag);\r\n}\r\nvoid wait_for_stable_page(struct page *page)\r\n{\r\nstruct address_space *mapping = page_mapping(page);\r\nstruct backing_dev_info *bdi = mapping->backing_dev_info;\r\nif (!bdi_cap_stable_pages_required(bdi))\r\nreturn;\r\nwait_on_page_writeback(page);\r\n}
