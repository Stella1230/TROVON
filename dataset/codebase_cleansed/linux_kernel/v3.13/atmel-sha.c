static inline u32 atmel_sha_read(struct atmel_sha_dev *dd, u32 offset)\r\n{\r\nreturn readl_relaxed(dd->io_base + offset);\r\n}\r\nstatic inline void atmel_sha_write(struct atmel_sha_dev *dd,\r\nu32 offset, u32 value)\r\n{\r\nwritel_relaxed(value, dd->io_base + offset);\r\n}\r\nstatic size_t atmel_sha_append_sg(struct atmel_sha_reqctx *ctx)\r\n{\r\nsize_t count;\r\nwhile ((ctx->bufcnt < ctx->buflen) && ctx->total) {\r\ncount = min(ctx->sg->length - ctx->offset, ctx->total);\r\ncount = min(count, ctx->buflen - ctx->bufcnt);\r\nif (count <= 0)\r\nbreak;\r\nscatterwalk_map_and_copy(ctx->buffer + ctx->bufcnt, ctx->sg,\r\nctx->offset, count, 0);\r\nctx->bufcnt += count;\r\nctx->offset += count;\r\nctx->total -= count;\r\nif (ctx->offset == ctx->sg->length) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\nelse\r\nctx->total = 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_fill_padding(struct atmel_sha_reqctx *ctx, int length)\r\n{\r\nunsigned int index, padlen;\r\nu64 bits[2];\r\nu64 size[2];\r\nsize[0] = ctx->digcnt[0];\r\nsize[1] = ctx->digcnt[1];\r\nsize[0] += ctx->bufcnt;\r\nif (size[0] < ctx->bufcnt)\r\nsize[1]++;\r\nsize[0] += length;\r\nif (size[0] < length)\r\nsize[1]++;\r\nbits[1] = cpu_to_be64(size[0] << 3);\r\nbits[0] = cpu_to_be64(size[1] << 3 | size[0] >> 61);\r\nif (ctx->flags & (SHA_FLAGS_SHA384 | SHA_FLAGS_SHA512)) {\r\nindex = ctx->bufcnt & 0x7f;\r\npadlen = (index < 112) ? (112 - index) : ((128+112) - index);\r\n*(ctx->buffer + ctx->bufcnt) = 0x80;\r\nmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen-1);\r\nmemcpy(ctx->buffer + ctx->bufcnt + padlen, bits, 16);\r\nctx->bufcnt += padlen + 16;\r\nctx->flags |= SHA_FLAGS_PAD;\r\n} else {\r\nindex = ctx->bufcnt & 0x3f;\r\npadlen = (index < 56) ? (56 - index) : ((64+56) - index);\r\n*(ctx->buffer + ctx->bufcnt) = 0x80;\r\nmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen-1);\r\nmemcpy(ctx->buffer + ctx->bufcnt + padlen, &bits[1], 8);\r\nctx->bufcnt += padlen + 8;\r\nctx->flags |= SHA_FLAGS_PAD;\r\n}\r\n}\r\nstatic int atmel_sha_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_dev *dd = NULL;\r\nstruct atmel_sha_dev *tmp;\r\nspin_lock_bh(&atmel_sha.lock);\r\nif (!tctx->dd) {\r\nlist_for_each_entry(tmp, &atmel_sha.dev_list, list) {\r\ndd = tmp;\r\nbreak;\r\n}\r\ntctx->dd = dd;\r\n} else {\r\ndd = tctx->dd;\r\n}\r\nspin_unlock_bh(&atmel_sha.lock);\r\nctx->dd = dd;\r\nctx->flags = 0;\r\ndev_dbg(dd->dev, "init: digest size: %d\n",\r\ncrypto_ahash_digestsize(tfm));\r\nswitch (crypto_ahash_digestsize(tfm)) {\r\ncase SHA1_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA1;\r\nctx->block_size = SHA1_BLOCK_SIZE;\r\nbreak;\r\ncase SHA224_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA224;\r\nctx->block_size = SHA224_BLOCK_SIZE;\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA256;\r\nctx->block_size = SHA256_BLOCK_SIZE;\r\nbreak;\r\ncase SHA384_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA384;\r\nctx->block_size = SHA384_BLOCK_SIZE;\r\nbreak;\r\ncase SHA512_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA512;\r\nctx->block_size = SHA512_BLOCK_SIZE;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\nbreak;\r\n}\r\nctx->bufcnt = 0;\r\nctx->digcnt[0] = 0;\r\nctx->digcnt[1] = 0;\r\nctx->buflen = SHA_BUFFER_LEN;\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_write_ctrl(struct atmel_sha_dev *dd, int dma)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nu32 valcr = 0, valmr = SHA_MR_MODE_AUTO;\r\nif (likely(dma)) {\r\nif (!dd->caps.has_dma)\r\natmel_sha_write(dd, SHA_IER, SHA_INT_TXBUFE);\r\nvalmr = SHA_MR_MODE_PDC;\r\nif (dd->caps.has_dualbuff)\r\nvalmr |= SHA_MR_DUALBUFF;\r\n} else {\r\natmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\r\n}\r\nif (ctx->flags & SHA_FLAGS_SHA1)\r\nvalmr |= SHA_MR_ALGO_SHA1;\r\nelse if (ctx->flags & SHA_FLAGS_SHA224)\r\nvalmr |= SHA_MR_ALGO_SHA224;\r\nelse if (ctx->flags & SHA_FLAGS_SHA256)\r\nvalmr |= SHA_MR_ALGO_SHA256;\r\nelse if (ctx->flags & SHA_FLAGS_SHA384)\r\nvalmr |= SHA_MR_ALGO_SHA384;\r\nelse if (ctx->flags & SHA_FLAGS_SHA512)\r\nvalmr |= SHA_MR_ALGO_SHA512;\r\nif (!(ctx->digcnt[0] || ctx->digcnt[1]))\r\nvalcr = SHA_CR_FIRST;\r\natmel_sha_write(dd, SHA_CR, valcr);\r\natmel_sha_write(dd, SHA_MR, valmr);\r\n}\r\nstatic int atmel_sha_xmit_cpu(struct atmel_sha_dev *dd, const u8 *buf,\r\nsize_t length, int final)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint count, len32;\r\nconst u32 *buffer = (const u32 *)buf;\r\ndev_dbg(dd->dev, "xmit_cpu: digcnt: 0x%llx 0x%llx, length: %d, final: %d\n",\r\nctx->digcnt[1], ctx->digcnt[0], length, final);\r\natmel_sha_write_ctrl(dd, 0);\r\nctx->digcnt[0] += length;\r\nif (ctx->digcnt[0] < length)\r\nctx->digcnt[1]++;\r\nif (final)\r\ndd->flags |= SHA_FLAGS_FINAL;\r\nlen32 = DIV_ROUND_UP(length, sizeof(u32));\r\ndd->flags |= SHA_FLAGS_CPU;\r\nfor (count = 0; count < len32; count++)\r\natmel_sha_write(dd, SHA_REG_DIN(count), buffer[count]);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int atmel_sha_xmit_pdc(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\r\nsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint len32;\r\ndev_dbg(dd->dev, "xmit_pdc: digcnt: 0x%llx 0x%llx, length: %d, final: %d\n",\r\nctx->digcnt[1], ctx->digcnt[0], length1, final);\r\nlen32 = DIV_ROUND_UP(length1, sizeof(u32));\r\natmel_sha_write(dd, SHA_PTCR, SHA_PTCR_TXTDIS);\r\natmel_sha_write(dd, SHA_TPR, dma_addr1);\r\natmel_sha_write(dd, SHA_TCR, len32);\r\nlen32 = DIV_ROUND_UP(length2, sizeof(u32));\r\natmel_sha_write(dd, SHA_TNPR, dma_addr2);\r\natmel_sha_write(dd, SHA_TNCR, len32);\r\natmel_sha_write_ctrl(dd, 1);\r\nctx->digcnt[0] += length1;\r\nif (ctx->digcnt[0] < length1)\r\nctx->digcnt[1]++;\r\nif (final)\r\ndd->flags |= SHA_FLAGS_FINAL;\r\ndd->flags |= SHA_FLAGS_DMA_ACTIVE;\r\natmel_sha_write(dd, SHA_PTCR, SHA_PTCR_TXTEN);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void atmel_sha_dma_callback(void *data)\r\n{\r\nstruct atmel_sha_dev *dd = data;\r\natmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\r\n}\r\nstatic int atmel_sha_xmit_dma(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\r\nsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nstruct dma_async_tx_descriptor *in_desc;\r\nstruct scatterlist sg[2];\r\ndev_dbg(dd->dev, "xmit_dma: digcnt: 0x%llx 0x%llx, length: %d, final: %d\n",\r\nctx->digcnt[1], ctx->digcnt[0], length1, final);\r\nif (ctx->flags & (SHA_FLAGS_SHA1 | SHA_FLAGS_SHA224 |\r\nSHA_FLAGS_SHA256)) {\r\ndd->dma_lch_in.dma_conf.src_maxburst = 16;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = 16;\r\n} else {\r\ndd->dma_lch_in.dma_conf.src_maxburst = 32;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = 32;\r\n}\r\ndmaengine_slave_config(dd->dma_lch_in.chan, &dd->dma_lch_in.dma_conf);\r\nif (length2) {\r\nsg_init_table(sg, 2);\r\nsg_dma_address(&sg[0]) = dma_addr1;\r\nsg_dma_len(&sg[0]) = length1;\r\nsg_dma_address(&sg[1]) = dma_addr2;\r\nsg_dma_len(&sg[1]) = length2;\r\nin_desc = dmaengine_prep_slave_sg(dd->dma_lch_in.chan, sg, 2,\r\nDMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\n} else {\r\nsg_init_table(sg, 1);\r\nsg_dma_address(&sg[0]) = dma_addr1;\r\nsg_dma_len(&sg[0]) = length1;\r\nin_desc = dmaengine_prep_slave_sg(dd->dma_lch_in.chan, sg, 1,\r\nDMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\n}\r\nif (!in_desc)\r\nreturn -EINVAL;\r\nin_desc->callback = atmel_sha_dma_callback;\r\nin_desc->callback_param = dd;\r\natmel_sha_write_ctrl(dd, 1);\r\nctx->digcnt[0] += length1;\r\nif (ctx->digcnt[0] < length1)\r\nctx->digcnt[1]++;\r\nif (final)\r\ndd->flags |= SHA_FLAGS_FINAL;\r\ndd->flags |= SHA_FLAGS_DMA_ACTIVE;\r\ndmaengine_submit(in_desc);\r\ndma_async_issue_pending(dd->dma_lch_in.chan);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int atmel_sha_xmit_start(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\r\nsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\r\n{\r\nif (dd->caps.has_dma)\r\nreturn atmel_sha_xmit_dma(dd, dma_addr1, length1,\r\ndma_addr2, length2, final);\r\nelse\r\nreturn atmel_sha_xmit_pdc(dd, dma_addr1, length1,\r\ndma_addr2, length2, final);\r\n}\r\nstatic int atmel_sha_update_cpu(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint bufcnt;\r\natmel_sha_append_sg(ctx);\r\natmel_sha_fill_padding(ctx, 0);\r\nbufcnt = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_cpu(dd, ctx->buffer, bufcnt, 1);\r\n}\r\nstatic int atmel_sha_xmit_dma_map(struct atmel_sha_dev *dd,\r\nstruct atmel_sha_reqctx *ctx,\r\nsize_t length, int final)\r\n{\r\nctx->dma_addr = dma_map_single(dd->dev, ctx->buffer,\r\nctx->buflen + ctx->block_size, DMA_TO_DEVICE);\r\nif (dma_mapping_error(dd->dev, ctx->dma_addr)) {\r\ndev_err(dd->dev, "dma %u bytes error\n", ctx->buflen +\r\nctx->block_size);\r\nreturn -EINVAL;\r\n}\r\nctx->flags &= ~SHA_FLAGS_SG;\r\nreturn atmel_sha_xmit_start(dd, ctx->dma_addr, length, 0, 0, final);\r\n}\r\nstatic int atmel_sha_update_dma_slow(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nunsigned int final;\r\nsize_t count;\r\natmel_sha_append_sg(ctx);\r\nfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\r\ndev_dbg(dd->dev, "slow: bufcnt: %u, digcnt: 0x%llx 0x%llx, final: %d\n",\r\nctx->bufcnt, ctx->digcnt[1], ctx->digcnt[0], final);\r\nif (final)\r\natmel_sha_fill_padding(ctx, 0);\r\nif (final || (ctx->bufcnt == ctx->buflen && ctx->total)) {\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_dma_map(dd, ctx, count, final);\r\n}\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_update_dma_start(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nunsigned int length, final, tail;\r\nstruct scatterlist *sg;\r\nunsigned int count;\r\nif (!ctx->total)\r\nreturn 0;\r\nif (ctx->bufcnt || ctx->offset)\r\nreturn atmel_sha_update_dma_slow(dd);\r\ndev_dbg(dd->dev, "fast: digcnt: 0x%llx 0x%llx, bufcnt: %u, total: %u\n",\r\nctx->digcnt[1], ctx->digcnt[0], ctx->bufcnt, ctx->total);\r\nsg = ctx->sg;\r\nif (!IS_ALIGNED(sg->offset, sizeof(u32)))\r\nreturn atmel_sha_update_dma_slow(dd);\r\nif (!sg_is_last(sg) && !IS_ALIGNED(sg->length, ctx->block_size))\r\nreturn atmel_sha_update_dma_slow(dd);\r\nlength = min(ctx->total, sg->length);\r\nif (sg_is_last(sg)) {\r\nif (!(ctx->flags & SHA_FLAGS_FINUP)) {\r\ntail = length & (ctx->block_size - 1);\r\nlength -= tail;\r\n}\r\n}\r\nctx->total -= length;\r\nctx->offset = length;\r\nfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\r\nif (final) {\r\ntail = length & (ctx->block_size - 1);\r\nlength -= tail;\r\nctx->total += tail;\r\nctx->offset = length;\r\nsg = ctx->sg;\r\natmel_sha_append_sg(ctx);\r\natmel_sha_fill_padding(ctx, length);\r\nctx->dma_addr = dma_map_single(dd->dev, ctx->buffer,\r\nctx->buflen + ctx->block_size, DMA_TO_DEVICE);\r\nif (dma_mapping_error(dd->dev, ctx->dma_addr)) {\r\ndev_err(dd->dev, "dma %u bytes error\n",\r\nctx->buflen + ctx->block_size);\r\nreturn -EINVAL;\r\n}\r\nif (length == 0) {\r\nctx->flags &= ~SHA_FLAGS_SG;\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_start(dd, ctx->dma_addr, count, 0,\r\n0, final);\r\n} else {\r\nctx->sg = sg;\r\nif (!dma_map_sg(dd->dev, ctx->sg, 1,\r\nDMA_TO_DEVICE)) {\r\ndev_err(dd->dev, "dma_map_sg error\n");\r\nreturn -EINVAL;\r\n}\r\nctx->flags |= SHA_FLAGS_SG;\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_start(dd, sg_dma_address(ctx->sg),\r\nlength, ctx->dma_addr, count, final);\r\n}\r\n}\r\nif (!dma_map_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE)) {\r\ndev_err(dd->dev, "dma_map_sg error\n");\r\nreturn -EINVAL;\r\n}\r\nctx->flags |= SHA_FLAGS_SG;\r\nreturn atmel_sha_xmit_start(dd, sg_dma_address(ctx->sg), length, 0,\r\n0, final);\r\n}\r\nstatic int atmel_sha_update_dma_stop(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nif (ctx->flags & SHA_FLAGS_SG) {\r\ndma_unmap_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE);\r\nif (ctx->sg->length == ctx->offset) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\n}\r\nif (ctx->flags & SHA_FLAGS_PAD) {\r\ndma_unmap_single(dd->dev, ctx->dma_addr,\r\nctx->buflen + ctx->block_size, DMA_TO_DEVICE);\r\n}\r\n} else {\r\ndma_unmap_single(dd->dev, ctx->dma_addr, ctx->buflen +\r\nctx->block_size, DMA_TO_DEVICE);\r\n}\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_update_req(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err;\r\ndev_dbg(dd->dev, "update_req: total: %u, digcnt: 0x%llx 0x%llx\n",\r\nctx->total, ctx->digcnt[1], ctx->digcnt[0]);\r\nif (ctx->flags & SHA_FLAGS_CPU)\r\nerr = atmel_sha_update_cpu(dd);\r\nelse\r\nerr = atmel_sha_update_dma_start(dd);\r\ndev_dbg(dd->dev, "update: err: %d, digcnt: 0x%llx 0%llx\n",\r\nerr, ctx->digcnt[1], ctx->digcnt[0]);\r\nreturn err;\r\n}\r\nstatic int atmel_sha_final_req(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err = 0;\r\nint count;\r\nif (ctx->bufcnt >= ATMEL_SHA_DMA_THRESHOLD) {\r\natmel_sha_fill_padding(ctx, 0);\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nerr = atmel_sha_xmit_dma_map(dd, ctx, count, 1);\r\n}\r\nelse {\r\natmel_sha_fill_padding(ctx, 0);\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nerr = atmel_sha_xmit_cpu(dd, ctx->buffer, count, 1);\r\n}\r\ndev_dbg(dd->dev, "final_req: err: %d\n", err);\r\nreturn err;\r\n}\r\nstatic void atmel_sha_copy_hash(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nu32 *hash = (u32 *)ctx->digest;\r\nint i;\r\nif (ctx->flags & SHA_FLAGS_SHA1)\r\nfor (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++)\r\nhash[i] = atmel_sha_read(ctx->dd, SHA_REG_DIGEST(i));\r\nelse if (ctx->flags & SHA_FLAGS_SHA224)\r\nfor (i = 0; i < SHA224_DIGEST_SIZE / sizeof(u32); i++)\r\nhash[i] = atmel_sha_read(ctx->dd, SHA_REG_DIGEST(i));\r\nelse if (ctx->flags & SHA_FLAGS_SHA256)\r\nfor (i = 0; i < SHA256_DIGEST_SIZE / sizeof(u32); i++)\r\nhash[i] = atmel_sha_read(ctx->dd, SHA_REG_DIGEST(i));\r\nelse if (ctx->flags & SHA_FLAGS_SHA384)\r\nfor (i = 0; i < SHA384_DIGEST_SIZE / sizeof(u32); i++)\r\nhash[i] = atmel_sha_read(ctx->dd, SHA_REG_DIGEST(i));\r\nelse\r\nfor (i = 0; i < SHA512_DIGEST_SIZE / sizeof(u32); i++)\r\nhash[i] = atmel_sha_read(ctx->dd, SHA_REG_DIGEST(i));\r\n}\r\nstatic void atmel_sha_copy_ready_hash(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nif (!req->result)\r\nreturn;\r\nif (ctx->flags & SHA_FLAGS_SHA1)\r\nmemcpy(req->result, ctx->digest, SHA1_DIGEST_SIZE);\r\nelse if (ctx->flags & SHA_FLAGS_SHA224)\r\nmemcpy(req->result, ctx->digest, SHA224_DIGEST_SIZE);\r\nelse if (ctx->flags & SHA_FLAGS_SHA256)\r\nmemcpy(req->result, ctx->digest, SHA256_DIGEST_SIZE);\r\nelse if (ctx->flags & SHA_FLAGS_SHA384)\r\nmemcpy(req->result, ctx->digest, SHA384_DIGEST_SIZE);\r\nelse\r\nmemcpy(req->result, ctx->digest, SHA512_DIGEST_SIZE);\r\n}\r\nstatic int atmel_sha_finish(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_dev *dd = ctx->dd;\r\nint err = 0;\r\nif (ctx->digcnt[0] || ctx->digcnt[1])\r\natmel_sha_copy_ready_hash(req);\r\ndev_dbg(dd->dev, "digcnt: 0x%llx 0x%llx, bufcnt: %d\n", ctx->digcnt[1],\r\nctx->digcnt[0], ctx->bufcnt);\r\nreturn err;\r\n}\r\nstatic void atmel_sha_finish_req(struct ahash_request *req, int err)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_dev *dd = ctx->dd;\r\nif (!err) {\r\natmel_sha_copy_hash(req);\r\nif (SHA_FLAGS_FINAL & dd->flags)\r\nerr = atmel_sha_finish(req);\r\n} else {\r\nctx->flags |= SHA_FLAGS_ERROR;\r\n}\r\ndd->flags &= ~(SHA_FLAGS_BUSY | SHA_FLAGS_FINAL | SHA_FLAGS_CPU |\r\nSHA_FLAGS_DMA_READY | SHA_FLAGS_OUTPUT_READY);\r\nclk_disable_unprepare(dd->iclk);\r\nif (req->base.complete)\r\nreq->base.complete(&req->base, err);\r\ntasklet_schedule(&dd->done_task);\r\n}\r\nstatic int atmel_sha_hw_init(struct atmel_sha_dev *dd)\r\n{\r\nclk_prepare_enable(dd->iclk);\r\nif (!(SHA_FLAGS_INIT & dd->flags)) {\r\natmel_sha_write(dd, SHA_CR, SHA_CR_SWRST);\r\ndd->flags |= SHA_FLAGS_INIT;\r\ndd->err = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline unsigned int atmel_sha_get_version(struct atmel_sha_dev *dd)\r\n{\r\nreturn atmel_sha_read(dd, SHA_HW_VERSION) & 0x00000fff;\r\n}\r\nstatic void atmel_sha_hw_version_init(struct atmel_sha_dev *dd)\r\n{\r\natmel_sha_hw_init(dd);\r\ndd->hw_version = atmel_sha_get_version(dd);\r\ndev_info(dd->dev,\r\n"version: 0x%x\n", dd->hw_version);\r\nclk_disable_unprepare(dd->iclk);\r\n}\r\nstatic int atmel_sha_handle_queue(struct atmel_sha_dev *dd,\r\nstruct ahash_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct atmel_sha_reqctx *ctx;\r\nunsigned long flags;\r\nint err = 0, ret = 0;\r\nspin_lock_irqsave(&dd->lock, flags);\r\nif (req)\r\nret = ahash_enqueue_request(&dd->queue, req);\r\nif (SHA_FLAGS_BUSY & dd->flags) {\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&dd->queue);\r\nasync_req = crypto_dequeue_request(&dd->queue);\r\nif (async_req)\r\ndd->flags |= SHA_FLAGS_BUSY;\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ahash_request_cast(async_req);\r\ndd->req = req;\r\nctx = ahash_request_ctx(req);\r\ndev_dbg(dd->dev, "handling new req, op: %lu, nbytes: %d\n",\r\nctx->op, req->nbytes);\r\nerr = atmel_sha_hw_init(dd);\r\nif (err)\r\ngoto err1;\r\nif (ctx->op == SHA_OP_UPDATE) {\r\nerr = atmel_sha_update_req(dd);\r\nif (err != -EINPROGRESS && (ctx->flags & SHA_FLAGS_FINUP))\r\nerr = atmel_sha_final_req(dd);\r\n} else if (ctx->op == SHA_OP_FINAL) {\r\nerr = atmel_sha_final_req(dd);\r\n}\r\nerr1:\r\nif (err != -EINPROGRESS)\r\natmel_sha_finish_req(req, err);\r\ndev_dbg(dd->dev, "exit, err: %d\n", err);\r\nreturn ret;\r\n}\r\nstatic int atmel_sha_enqueue(struct ahash_request *req, unsigned int op)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct atmel_sha_dev *dd = tctx->dd;\r\nctx->op = op;\r\nreturn atmel_sha_handle_queue(dd, req);\r\n}\r\nstatic int atmel_sha_update(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nif (!req->nbytes)\r\nreturn 0;\r\nctx->total = req->nbytes;\r\nctx->sg = req->src;\r\nctx->offset = 0;\r\nif (ctx->flags & SHA_FLAGS_FINUP) {\r\nif (ctx->bufcnt + ctx->total < ATMEL_SHA_DMA_THRESHOLD)\r\nctx->flags |= SHA_FLAGS_CPU;\r\n} else if (ctx->bufcnt + ctx->total < ctx->buflen) {\r\natmel_sha_append_sg(ctx);\r\nreturn 0;\r\n}\r\nreturn atmel_sha_enqueue(req, SHA_OP_UPDATE);\r\n}\r\nstatic int atmel_sha_final(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct atmel_sha_dev *dd = tctx->dd;\r\nint err = 0;\r\nctx->flags |= SHA_FLAGS_FINUP;\r\nif (ctx->flags & SHA_FLAGS_ERROR)\r\nreturn 0;\r\nif (ctx->bufcnt) {\r\nreturn atmel_sha_enqueue(req, SHA_OP_FINAL);\r\n} else if (!(ctx->flags & SHA_FLAGS_PAD)) {\r\nerr = atmel_sha_hw_init(dd);\r\nif (err)\r\ngoto err1;\r\ndd->flags |= SHA_FLAGS_BUSY;\r\nerr = atmel_sha_final_req(dd);\r\n} else {\r\nreturn atmel_sha_finish(req);\r\n}\r\nerr1:\r\nif (err != -EINPROGRESS)\r\natmel_sha_finish_req(req, err);\r\nreturn err;\r\n}\r\nstatic int atmel_sha_finup(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err1, err2;\r\nctx->flags |= SHA_FLAGS_FINUP;\r\nerr1 = atmel_sha_update(req);\r\nif (err1 == -EINPROGRESS || err1 == -EBUSY)\r\nreturn err1;\r\nerr2 = atmel_sha_final(req);\r\nreturn err1 ?: err2;\r\n}\r\nstatic int atmel_sha_digest(struct ahash_request *req)\r\n{\r\nreturn atmel_sha_init(req) ?: atmel_sha_finup(req);\r\n}\r\nstatic int atmel_sha_cra_init_alg(struct crypto_tfm *tfm, const char *alg_base)\r\n{\r\nstruct atmel_sha_ctx *tctx = crypto_tfm_ctx(tfm);\r\nconst char *alg_name = crypto_tfm_alg_name(tfm);\r\ntctx->fallback = crypto_alloc_shash(alg_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(tctx->fallback)) {\r\npr_err("atmel-sha: fallback driver '%s' could not be loaded.\n",\r\nalg_name);\r\nreturn PTR_ERR(tctx->fallback);\r\n}\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct atmel_sha_reqctx) +\r\nSHA_BUFFER_LEN + SHA512_BLOCK_SIZE);\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_cra_init(struct crypto_tfm *tfm)\r\n{\r\nreturn atmel_sha_cra_init_alg(tfm, NULL);\r\n}\r\nstatic void atmel_sha_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct atmel_sha_ctx *tctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_shash(tctx->fallback);\r\ntctx->fallback = NULL;\r\n}\r\nstatic void atmel_sha_done_task(unsigned long data)\r\n{\r\nstruct atmel_sha_dev *dd = (struct atmel_sha_dev *)data;\r\nint err = 0;\r\nif (!(SHA_FLAGS_BUSY & dd->flags)) {\r\natmel_sha_handle_queue(dd, NULL);\r\nreturn;\r\n}\r\nif (SHA_FLAGS_CPU & dd->flags) {\r\nif (SHA_FLAGS_OUTPUT_READY & dd->flags) {\r\ndd->flags &= ~SHA_FLAGS_OUTPUT_READY;\r\ngoto finish;\r\n}\r\n} else if (SHA_FLAGS_DMA_READY & dd->flags) {\r\nif (SHA_FLAGS_DMA_ACTIVE & dd->flags) {\r\ndd->flags &= ~SHA_FLAGS_DMA_ACTIVE;\r\natmel_sha_update_dma_stop(dd);\r\nif (dd->err) {\r\nerr = dd->err;\r\ngoto finish;\r\n}\r\n}\r\nif (SHA_FLAGS_OUTPUT_READY & dd->flags) {\r\ndd->flags &= ~(SHA_FLAGS_DMA_READY |\r\nSHA_FLAGS_OUTPUT_READY);\r\nerr = atmel_sha_update_dma_start(dd);\r\nif (err != -EINPROGRESS)\r\ngoto finish;\r\n}\r\n}\r\nreturn;\r\nfinish:\r\natmel_sha_finish_req(dd->req, err);\r\n}\r\nstatic irqreturn_t atmel_sha_irq(int irq, void *dev_id)\r\n{\r\nstruct atmel_sha_dev *sha_dd = dev_id;\r\nu32 reg;\r\nreg = atmel_sha_read(sha_dd, SHA_ISR);\r\nif (reg & atmel_sha_read(sha_dd, SHA_IMR)) {\r\natmel_sha_write(sha_dd, SHA_IDR, reg);\r\nif (SHA_FLAGS_BUSY & sha_dd->flags) {\r\nsha_dd->flags |= SHA_FLAGS_OUTPUT_READY;\r\nif (!(SHA_FLAGS_CPU & sha_dd->flags))\r\nsha_dd->flags |= SHA_FLAGS_DMA_READY;\r\ntasklet_schedule(&sha_dd->done_task);\r\n} else {\r\ndev_warn(sha_dd->dev, "SHA interrupt when no active requests.\n");\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nreturn IRQ_NONE;\r\n}\r\nstatic void atmel_sha_unregister_algs(struct atmel_sha_dev *dd)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(sha_1_256_algs); i++)\r\ncrypto_unregister_ahash(&sha_1_256_algs[i]);\r\nif (dd->caps.has_sha224)\r\ncrypto_unregister_ahash(&sha_224_alg);\r\nif (dd->caps.has_sha_384_512) {\r\nfor (i = 0; i < ARRAY_SIZE(sha_384_512_algs); i++)\r\ncrypto_unregister_ahash(&sha_384_512_algs[i]);\r\n}\r\n}\r\nstatic int atmel_sha_register_algs(struct atmel_sha_dev *dd)\r\n{\r\nint err, i, j;\r\nfor (i = 0; i < ARRAY_SIZE(sha_1_256_algs); i++) {\r\nerr = crypto_register_ahash(&sha_1_256_algs[i]);\r\nif (err)\r\ngoto err_sha_1_256_algs;\r\n}\r\nif (dd->caps.has_sha224) {\r\nerr = crypto_register_ahash(&sha_224_alg);\r\nif (err)\r\ngoto err_sha_224_algs;\r\n}\r\nif (dd->caps.has_sha_384_512) {\r\nfor (i = 0; i < ARRAY_SIZE(sha_384_512_algs); i++) {\r\nerr = crypto_register_ahash(&sha_384_512_algs[i]);\r\nif (err)\r\ngoto err_sha_384_512_algs;\r\n}\r\n}\r\nreturn 0;\r\nerr_sha_384_512_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_ahash(&sha_384_512_algs[j]);\r\ncrypto_unregister_ahash(&sha_224_alg);\r\nerr_sha_224_algs:\r\ni = ARRAY_SIZE(sha_1_256_algs);\r\nerr_sha_1_256_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_ahash(&sha_1_256_algs[j]);\r\nreturn err;\r\n}\r\nstatic bool atmel_sha_filter(struct dma_chan *chan, void *slave)\r\n{\r\nstruct at_dma_slave *sl = slave;\r\nif (sl && sl->dma_dev == chan->device->dev) {\r\nchan->private = sl;\r\nreturn true;\r\n} else {\r\nreturn false;\r\n}\r\n}\r\nstatic int atmel_sha_dma_init(struct atmel_sha_dev *dd,\r\nstruct crypto_platform_data *pdata)\r\n{\r\nint err = -ENOMEM;\r\ndma_cap_mask_t mask_in;\r\nif (pdata && pdata->dma_slave->rxdata.dma_dev) {\r\ndma_cap_zero(mask_in);\r\ndma_cap_set(DMA_SLAVE, mask_in);\r\ndd->dma_lch_in.chan = dma_request_channel(mask_in,\r\natmel_sha_filter, &pdata->dma_slave->rxdata);\r\nif (!dd->dma_lch_in.chan)\r\nreturn err;\r\ndd->dma_lch_in.dma_conf.direction = DMA_MEM_TO_DEV;\r\ndd->dma_lch_in.dma_conf.dst_addr = dd->phys_base +\r\nSHA_REG_DIN(0);\r\ndd->dma_lch_in.dma_conf.src_maxburst = 1;\r\ndd->dma_lch_in.dma_conf.src_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = 1;\r\ndd->dma_lch_in.dma_conf.dst_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_in.dma_conf.device_fc = false;\r\nreturn 0;\r\n}\r\nreturn -ENODEV;\r\n}\r\nstatic void atmel_sha_dma_cleanup(struct atmel_sha_dev *dd)\r\n{\r\ndma_release_channel(dd->dma_lch_in.chan);\r\n}\r\nstatic void atmel_sha_get_cap(struct atmel_sha_dev *dd)\r\n{\r\ndd->caps.has_dma = 0;\r\ndd->caps.has_dualbuff = 0;\r\ndd->caps.has_sha224 = 0;\r\ndd->caps.has_sha_384_512 = 0;\r\nswitch (dd->hw_version & 0xff0) {\r\ncase 0x410:\r\ndd->caps.has_dma = 1;\r\ndd->caps.has_dualbuff = 1;\r\ndd->caps.has_sha224 = 1;\r\ndd->caps.has_sha_384_512 = 1;\r\nbreak;\r\ncase 0x400:\r\ndd->caps.has_dma = 1;\r\ndd->caps.has_dualbuff = 1;\r\ndd->caps.has_sha224 = 1;\r\nbreak;\r\ncase 0x320:\r\nbreak;\r\ndefault:\r\ndev_warn(dd->dev,\r\n"Unmanaged sha version, set minimum capabilities\n");\r\nbreak;\r\n}\r\n}\r\nstatic int atmel_sha_probe(struct platform_device *pdev)\r\n{\r\nstruct atmel_sha_dev *sha_dd;\r\nstruct crypto_platform_data *pdata;\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *sha_res;\r\nunsigned long sha_phys_size;\r\nint err;\r\nsha_dd = kzalloc(sizeof(struct atmel_sha_dev), GFP_KERNEL);\r\nif (sha_dd == NULL) {\r\ndev_err(dev, "unable to alloc data struct.\n");\r\nerr = -ENOMEM;\r\ngoto sha_dd_err;\r\n}\r\nsha_dd->dev = dev;\r\nplatform_set_drvdata(pdev, sha_dd);\r\nINIT_LIST_HEAD(&sha_dd->list);\r\ntasklet_init(&sha_dd->done_task, atmel_sha_done_task,\r\n(unsigned long)sha_dd);\r\ncrypto_init_queue(&sha_dd->queue, ATMEL_SHA_QUEUE_LENGTH);\r\nsha_dd->irq = -1;\r\nsha_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!sha_res) {\r\ndev_err(dev, "no MEM resource info\n");\r\nerr = -ENODEV;\r\ngoto res_err;\r\n}\r\nsha_dd->phys_base = sha_res->start;\r\nsha_phys_size = resource_size(sha_res);\r\nsha_dd->irq = platform_get_irq(pdev, 0);\r\nif (sha_dd->irq < 0) {\r\ndev_err(dev, "no IRQ resource info\n");\r\nerr = sha_dd->irq;\r\ngoto res_err;\r\n}\r\nerr = request_irq(sha_dd->irq, atmel_sha_irq, IRQF_SHARED, "atmel-sha",\r\nsha_dd);\r\nif (err) {\r\ndev_err(dev, "unable to request sha irq.\n");\r\ngoto res_err;\r\n}\r\nsha_dd->iclk = clk_get(&pdev->dev, "sha_clk");\r\nif (IS_ERR(sha_dd->iclk)) {\r\ndev_err(dev, "clock intialization failed.\n");\r\nerr = PTR_ERR(sha_dd->iclk);\r\ngoto clk_err;\r\n}\r\nsha_dd->io_base = ioremap(sha_dd->phys_base, sha_phys_size);\r\nif (!sha_dd->io_base) {\r\ndev_err(dev, "can't ioremap\n");\r\nerr = -ENOMEM;\r\ngoto sha_io_err;\r\n}\r\natmel_sha_hw_version_init(sha_dd);\r\natmel_sha_get_cap(sha_dd);\r\nif (sha_dd->caps.has_dma) {\r\npdata = pdev->dev.platform_data;\r\nif (!pdata) {\r\ndev_err(&pdev->dev, "platform data not available\n");\r\nerr = -ENXIO;\r\ngoto err_pdata;\r\n}\r\nerr = atmel_sha_dma_init(sha_dd, pdata);\r\nif (err)\r\ngoto err_sha_dma;\r\n}\r\nspin_lock(&atmel_sha.lock);\r\nlist_add_tail(&sha_dd->list, &atmel_sha.dev_list);\r\nspin_unlock(&atmel_sha.lock);\r\nerr = atmel_sha_register_algs(sha_dd);\r\nif (err)\r\ngoto err_algs;\r\ndev_info(dev, "Atmel SHA1/SHA256\n");\r\nreturn 0;\r\nerr_algs:\r\nspin_lock(&atmel_sha.lock);\r\nlist_del(&sha_dd->list);\r\nspin_unlock(&atmel_sha.lock);\r\nif (sha_dd->caps.has_dma)\r\natmel_sha_dma_cleanup(sha_dd);\r\nerr_sha_dma:\r\nerr_pdata:\r\niounmap(sha_dd->io_base);\r\nsha_io_err:\r\nclk_put(sha_dd->iclk);\r\nclk_err:\r\nfree_irq(sha_dd->irq, sha_dd);\r\nres_err:\r\ntasklet_kill(&sha_dd->done_task);\r\nkfree(sha_dd);\r\nsha_dd = NULL;\r\nsha_dd_err:\r\ndev_err(dev, "initialization failed.\n");\r\nreturn err;\r\n}\r\nstatic int atmel_sha_remove(struct platform_device *pdev)\r\n{\r\nstatic struct atmel_sha_dev *sha_dd;\r\nsha_dd = platform_get_drvdata(pdev);\r\nif (!sha_dd)\r\nreturn -ENODEV;\r\nspin_lock(&atmel_sha.lock);\r\nlist_del(&sha_dd->list);\r\nspin_unlock(&atmel_sha.lock);\r\natmel_sha_unregister_algs(sha_dd);\r\ntasklet_kill(&sha_dd->done_task);\r\nif (sha_dd->caps.has_dma)\r\natmel_sha_dma_cleanup(sha_dd);\r\niounmap(sha_dd->io_base);\r\nclk_put(sha_dd->iclk);\r\nif (sha_dd->irq >= 0)\r\nfree_irq(sha_dd->irq, sha_dd);\r\nkfree(sha_dd);\r\nsha_dd = NULL;\r\nreturn 0;\r\n}
