static int cl_lock_invariant_trusted(const struct lu_env *env,\r\nconst struct cl_lock *lock)\r\n{\r\nreturn ergo(lock->cll_state == CLS_FREEING, lock->cll_holds == 0) &&\r\natomic_read(&lock->cll_ref) >= lock->cll_holds &&\r\nlock->cll_holds >= lock->cll_users &&\r\nlock->cll_holds >= 0 &&\r\nlock->cll_users >= 0 &&\r\nlock->cll_depth >= 0;\r\n}\r\nstatic int cl_lock_invariant(const struct lu_env *env,\r\nconst struct cl_lock *lock)\r\n{\r\nint result;\r\nresult = atomic_read(&lock->cll_ref) > 0 &&\r\ncl_lock_invariant_trusted(env, lock);\r\nif (!result && env != NULL)\r\nCL_LOCK_DEBUG(D_ERROR, env, lock, "invariant broken");\r\nreturn result;\r\n}\r\nstatic enum clt_nesting_level cl_lock_nesting(const struct cl_lock *lock)\r\n{\r\nreturn cl_object_header(lock->cll_descr.cld_obj)->coh_nesting;\r\n}\r\nstatic struct cl_thread_counters *cl_lock_counters(const struct lu_env *env,\r\nconst struct cl_lock *lock)\r\n{\r\nstruct cl_thread_info *info;\r\nenum clt_nesting_level nesting;\r\ninfo = cl_env_info(env);\r\nnesting = cl_lock_nesting(lock);\r\nLASSERT(nesting < ARRAY_SIZE(info->clt_counters));\r\nreturn &info->clt_counters[nesting];\r\n}\r\nstatic void cl_lock_trace0(int level, const struct lu_env *env,\r\nconst char *prefix, const struct cl_lock *lock,\r\nconst char *func, const int line)\r\n{\r\nstruct cl_object_header *h = cl_object_header(lock->cll_descr.cld_obj);\r\nCDEBUG(level, "%s: %p@(%d %p %d %d %d %d %d %lx)"\r\n"(%p/%d/%d) at %s():%d\n",\r\nprefix, lock, atomic_read(&lock->cll_ref),\r\nlock->cll_guarder, lock->cll_depth,\r\nlock->cll_state, lock->cll_error, lock->cll_holds,\r\nlock->cll_users, lock->cll_flags,\r\nenv, h->coh_nesting, cl_lock_nr_mutexed(env),\r\nfunc, line);\r\n}\r\nstatic void cl_lock_lockdep_init(struct cl_lock *lock)\r\n{\r\nlockdep_set_class_and_name(lock, &cl_lock_key, "EXT");\r\n}\r\nstatic void cl_lock_lockdep_acquire(const struct lu_env *env,\r\nstruct cl_lock *lock, __u32 enqflags)\r\n{\r\ncl_lock_counters(env, lock)->ctc_nr_locks_acquired++;\r\nlock_map_acquire(&lock->dep_map);\r\n}\r\nstatic void cl_lock_lockdep_release(const struct lu_env *env,\r\nstruct cl_lock *lock)\r\n{\r\ncl_lock_counters(env, lock)->ctc_nr_locks_acquired--;\r\nlock_release(&lock->dep_map, 0, RETIP);\r\n}\r\nstatic void cl_lock_lockdep_init(struct cl_lock *lock)\r\n{}\r\nstatic void cl_lock_lockdep_acquire(const struct lu_env *env,\r\nstruct cl_lock *lock, __u32 enqflags)\r\n{}\r\nstatic void cl_lock_lockdep_release(const struct lu_env *env,\r\nstruct cl_lock *lock)\r\n{}\r\nvoid cl_lock_slice_add(struct cl_lock *lock, struct cl_lock_slice *slice,\r\nstruct cl_object *obj,\r\nconst struct cl_lock_operations *ops)\r\n{\r\nslice->cls_lock = lock;\r\nlist_add_tail(&slice->cls_linkage, &lock->cll_layers);\r\nslice->cls_obj = obj;\r\nslice->cls_ops = ops;\r\n}\r\nint cl_lock_mode_match(enum cl_lock_mode has, enum cl_lock_mode need)\r\n{\r\nLINVRNT(need == CLM_READ || need == CLM_WRITE ||\r\nneed == CLM_PHANTOM || need == CLM_GROUP);\r\nLINVRNT(has == CLM_READ || has == CLM_WRITE ||\r\nhas == CLM_PHANTOM || has == CLM_GROUP);\r\nCLASSERT(CLM_PHANTOM < CLM_READ);\r\nCLASSERT(CLM_READ < CLM_WRITE);\r\nCLASSERT(CLM_WRITE < CLM_GROUP);\r\nif (has != CLM_GROUP)\r\nreturn need <= has;\r\nelse\r\nreturn need == has;\r\n}\r\nint cl_lock_ext_match(const struct cl_lock_descr *has,\r\nconst struct cl_lock_descr *need)\r\n{\r\nreturn\r\nhas->cld_start <= need->cld_start &&\r\nhas->cld_end >= need->cld_end &&\r\ncl_lock_mode_match(has->cld_mode, need->cld_mode) &&\r\n(has->cld_mode != CLM_GROUP || has->cld_gid == need->cld_gid);\r\n}\r\nint cl_lock_descr_match(const struct cl_lock_descr *has,\r\nconst struct cl_lock_descr *need)\r\n{\r\nreturn\r\ncl_object_same(has->cld_obj, need->cld_obj) &&\r\ncl_lock_ext_match(has, need);\r\n}\r\nstatic void cl_lock_free(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nstruct cl_object *obj = lock->cll_descr.cld_obj;\r\nLINVRNT(!cl_lock_is_mutexed(lock));\r\ncl_lock_trace(D_DLMTRACE, env, "free lock", lock);\r\nmight_sleep();\r\nwhile (!list_empty(&lock->cll_layers)) {\r\nstruct cl_lock_slice *slice;\r\nslice = list_entry(lock->cll_layers.next,\r\nstruct cl_lock_slice, cls_linkage);\r\nlist_del_init(lock->cll_layers.next);\r\nslice->cls_ops->clo_fini(env, slice);\r\n}\r\nCS_LOCK_DEC(obj, total);\r\nCS_LOCKSTATE_DEC(obj, lock->cll_state);\r\nlu_object_ref_del_at(&obj->co_lu, &lock->cll_obj_ref, "cl_lock", lock);\r\ncl_object_put(env, obj);\r\nlu_ref_fini(&lock->cll_reference);\r\nlu_ref_fini(&lock->cll_holders);\r\nmutex_destroy(&lock->cll_guard);\r\nOBD_SLAB_FREE_PTR(lock, cl_lock_kmem);\r\n}\r\nvoid cl_lock_put(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nstruct cl_object *obj;\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nobj = lock->cll_descr.cld_obj;\r\nLINVRNT(obj != NULL);\r\nCDEBUG(D_TRACE, "releasing reference: %d %p %lu\n",\r\natomic_read(&lock->cll_ref), lock, RETIP);\r\nif (atomic_dec_and_test(&lock->cll_ref)) {\r\nif (lock->cll_state == CLS_FREEING) {\r\nLASSERT(list_empty(&lock->cll_linkage));\r\ncl_lock_free(env, lock);\r\n}\r\nCS_LOCK_DEC(obj, busy);\r\n}\r\n}\r\nvoid cl_lock_get(struct cl_lock *lock)\r\n{\r\nLINVRNT(cl_lock_invariant(NULL, lock));\r\nCDEBUG(D_TRACE, "acquiring reference: %d %p %lu\n",\r\natomic_read(&lock->cll_ref), lock, RETIP);\r\natomic_inc(&lock->cll_ref);\r\n}\r\nvoid cl_lock_get_trust(struct cl_lock *lock)\r\n{\r\nCDEBUG(D_TRACE, "acquiring trusted reference: %d %p %lu\n",\r\natomic_read(&lock->cll_ref), lock, RETIP);\r\nif (atomic_inc_return(&lock->cll_ref) == 1)\r\nCS_LOCK_INC(lock->cll_descr.cld_obj, busy);\r\n}\r\nstatic void cl_lock_finish(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\ncl_lock_mutex_get(env, lock);\r\ncl_lock_cancel(env, lock);\r\ncl_lock_delete(env, lock);\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_put(env, lock);\r\n}\r\nstatic struct cl_lock *cl_lock_alloc(const struct lu_env *env,\r\nstruct cl_object *obj,\r\nconst struct cl_io *io,\r\nconst struct cl_lock_descr *descr)\r\n{\r\nstruct cl_lock *lock;\r\nstruct lu_object_header *head;\r\nOBD_SLAB_ALLOC_PTR_GFP(lock, cl_lock_kmem, __GFP_IO);\r\nif (lock != NULL) {\r\natomic_set(&lock->cll_ref, 1);\r\nlock->cll_descr = *descr;\r\nlock->cll_state = CLS_NEW;\r\ncl_object_get(obj);\r\nlu_object_ref_add_at(&obj->co_lu, &lock->cll_obj_ref, "cl_lock",\r\nlock);\r\nINIT_LIST_HEAD(&lock->cll_layers);\r\nINIT_LIST_HEAD(&lock->cll_linkage);\r\nINIT_LIST_HEAD(&lock->cll_inclosure);\r\nlu_ref_init(&lock->cll_reference);\r\nlu_ref_init(&lock->cll_holders);\r\nmutex_init(&lock->cll_guard);\r\nlockdep_set_class(&lock->cll_guard, &cl_lock_guard_class);\r\ninit_waitqueue_head(&lock->cll_wq);\r\nhead = obj->co_lu.lo_header;\r\nCS_LOCKSTATE_INC(obj, CLS_NEW);\r\nCS_LOCK_INC(obj, total);\r\nCS_LOCK_INC(obj, create);\r\ncl_lock_lockdep_init(lock);\r\nlist_for_each_entry(obj, &head->loh_layers,\r\nco_lu.lo_linkage) {\r\nint err;\r\nerr = obj->co_ops->coo_lock_init(env, obj, lock, io);\r\nif (err != 0) {\r\ncl_lock_finish(env, lock);\r\nlock = ERR_PTR(err);\r\nbreak;\r\n}\r\n}\r\n} else\r\nlock = ERR_PTR(-ENOMEM);\r\nreturn lock;\r\n}\r\nenum cl_lock_state cl_lock_intransit(const struct lu_env *env,\r\nstruct cl_lock *lock)\r\n{\r\nenum cl_lock_state state = lock->cll_state;\r\nLASSERT(cl_lock_is_mutexed(lock));\r\nLASSERT(state != CLS_INTRANSIT);\r\nLASSERTF(state >= CLS_ENQUEUED && state <= CLS_CACHED,\r\n"Malformed lock state %d.\n", state);\r\ncl_lock_state_set(env, lock, CLS_INTRANSIT);\r\nlock->cll_intransit_owner = current;\r\ncl_lock_hold_add(env, lock, "intransit", current);\r\nreturn state;\r\n}\r\nvoid cl_lock_extransit(const struct lu_env *env, struct cl_lock *lock,\r\nenum cl_lock_state state)\r\n{\r\nLASSERT(cl_lock_is_mutexed(lock));\r\nLASSERT(lock->cll_state == CLS_INTRANSIT);\r\nLASSERT(state != CLS_INTRANSIT);\r\nLASSERT(lock->cll_intransit_owner == current);\r\nlock->cll_intransit_owner = NULL;\r\ncl_lock_state_set(env, lock, state);\r\ncl_lock_unhold(env, lock, "intransit", current);\r\n}\r\nint cl_lock_is_intransit(struct cl_lock *lock)\r\n{\r\nLASSERT(cl_lock_is_mutexed(lock));\r\nreturn lock->cll_state == CLS_INTRANSIT &&\r\nlock->cll_intransit_owner != current;\r\n}\r\nstatic int cl_lock_fits_into(const struct lu_env *env,\r\nconst struct cl_lock *lock,\r\nconst struct cl_lock_descr *need,\r\nconst struct cl_io *io)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nLINVRNT(cl_lock_invariant_trusted(env, lock));\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_ops->clo_fits_into != NULL &&\r\n!slice->cls_ops->clo_fits_into(env, slice, need, io))\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic struct cl_lock *cl_lock_lookup(const struct lu_env *env,\r\nstruct cl_object *obj,\r\nconst struct cl_io *io,\r\nconst struct cl_lock_descr *need)\r\n{\r\nstruct cl_lock *lock;\r\nstruct cl_object_header *head;\r\nhead = cl_object_header(obj);\r\nLINVRNT(spin_is_locked(&head->coh_lock_guard));\r\nCS_LOCK_INC(obj, lookup);\r\nlist_for_each_entry(lock, &head->coh_locks, cll_linkage) {\r\nint matched;\r\nmatched = cl_lock_ext_match(&lock->cll_descr, need) &&\r\nlock->cll_state < CLS_FREEING &&\r\nlock->cll_error == 0 &&\r\n!(lock->cll_flags & CLF_CANCELLED) &&\r\ncl_lock_fits_into(env, lock, need, io);\r\nCDEBUG(D_DLMTRACE, "has: "DDESCR"(%d) need: "DDESCR": %d\n",\r\nPDESCR(&lock->cll_descr), lock->cll_state, PDESCR(need),\r\nmatched);\r\nif (matched) {\r\ncl_lock_get_trust(lock);\r\nCS_LOCK_INC(obj, hit);\r\nreturn lock;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct cl_lock *cl_lock_find(const struct lu_env *env,\r\nconst struct cl_io *io,\r\nconst struct cl_lock_descr *need)\r\n{\r\nstruct cl_object_header *head;\r\nstruct cl_object *obj;\r\nstruct cl_lock *lock;\r\nobj = need->cld_obj;\r\nhead = cl_object_header(obj);\r\nspin_lock(&head->coh_lock_guard);\r\nlock = cl_lock_lookup(env, obj, io, need);\r\nspin_unlock(&head->coh_lock_guard);\r\nif (lock == NULL) {\r\nlock = cl_lock_alloc(env, obj, io, need);\r\nif (!IS_ERR(lock)) {\r\nstruct cl_lock *ghost;\r\nspin_lock(&head->coh_lock_guard);\r\nghost = cl_lock_lookup(env, obj, io, need);\r\nif (ghost == NULL) {\r\nlist_add_tail(&lock->cll_linkage,\r\n&head->coh_locks);\r\nspin_unlock(&head->coh_lock_guard);\r\nCS_LOCK_INC(obj, busy);\r\n} else {\r\nspin_unlock(&head->coh_lock_guard);\r\ncl_lock_finish(env, lock);\r\nlock = ghost;\r\n}\r\n}\r\n}\r\nreturn lock;\r\n}\r\nstruct cl_lock *cl_lock_peek(const struct lu_env *env, const struct cl_io *io,\r\nconst struct cl_lock_descr *need,\r\nconst char *scope, const void *source)\r\n{\r\nstruct cl_object_header *head;\r\nstruct cl_object *obj;\r\nstruct cl_lock *lock;\r\nobj = need->cld_obj;\r\nhead = cl_object_header(obj);\r\ndo {\r\nspin_lock(&head->coh_lock_guard);\r\nlock = cl_lock_lookup(env, obj, io, need);\r\nspin_unlock(&head->coh_lock_guard);\r\nif (lock == NULL)\r\nreturn NULL;\r\ncl_lock_mutex_get(env, lock);\r\nif (lock->cll_state == CLS_INTRANSIT)\r\ncl_lock_state_wait(env, lock);\r\nif (lock->cll_state == CLS_FREEING) {\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_put(env, lock);\r\nlock = NULL;\r\n}\r\n} while (lock == NULL);\r\ncl_lock_hold_add(env, lock, scope, source);\r\ncl_lock_user_add(env, lock);\r\nif (lock->cll_state == CLS_CACHED)\r\ncl_use_try(env, lock, 1);\r\nif (lock->cll_state == CLS_HELD) {\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_lockdep_acquire(env, lock, 0);\r\ncl_lock_put(env, lock);\r\n} else {\r\ncl_unuse_try(env, lock);\r\ncl_lock_unhold(env, lock, scope, source);\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_put(env, lock);\r\nlock = NULL;\r\n}\r\nreturn lock;\r\n}\r\nconst struct cl_lock_slice *cl_lock_at(const struct cl_lock *lock,\r\nconst struct lu_device_type *dtype)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nLINVRNT(cl_lock_invariant_trusted(NULL, lock));\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_obj->co_lu.lo_dev->ld_type == dtype)\r\nreturn slice;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void cl_lock_mutex_tail(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nstruct cl_thread_counters *counters;\r\ncounters = cl_lock_counters(env, lock);\r\nlock->cll_depth++;\r\ncounters->ctc_nr_locks_locked++;\r\nlu_ref_add(&counters->ctc_locks_locked, "cll_guard", lock);\r\ncl_lock_trace(D_TRACE, env, "got mutex", lock);\r\n}\r\nvoid cl_lock_mutex_get(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nif (lock->cll_guarder == current) {\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(lock->cll_depth > 0);\r\n} else {\r\nstruct cl_object_header *hdr;\r\nstruct cl_thread_info *info;\r\nint i;\r\nLINVRNT(lock->cll_guarder != current);\r\nhdr = cl_object_header(lock->cll_descr.cld_obj);\r\ninfo = cl_env_info(env);\r\nfor (i = 0; i < hdr->coh_nesting; ++i)\r\nLASSERT(info->clt_counters[i].ctc_nr_locks_locked == 0);\r\nmutex_lock_nested(&lock->cll_guard, hdr->coh_nesting);\r\nlock->cll_guarder = current;\r\nLINVRNT(lock->cll_depth == 0);\r\n}\r\ncl_lock_mutex_tail(env, lock);\r\n}\r\nint cl_lock_mutex_try(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nint result;\r\nLINVRNT(cl_lock_invariant_trusted(env, lock));\r\nresult = 0;\r\nif (lock->cll_guarder == current) {\r\nLINVRNT(lock->cll_depth > 0);\r\ncl_lock_mutex_tail(env, lock);\r\n} else if (mutex_trylock(&lock->cll_guard)) {\r\nLINVRNT(lock->cll_depth == 0);\r\nlock->cll_guarder = current;\r\ncl_lock_mutex_tail(env, lock);\r\n} else\r\nresult = -EBUSY;\r\nreturn result;\r\n}\r\nvoid cl_lock_mutex_put(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nstruct cl_thread_counters *counters;\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(lock->cll_guarder == current);\r\nLINVRNT(lock->cll_depth > 0);\r\ncounters = cl_lock_counters(env, lock);\r\nLINVRNT(counters->ctc_nr_locks_locked > 0);\r\ncl_lock_trace(D_TRACE, env, "put mutex", lock);\r\nlu_ref_del(&counters->ctc_locks_locked, "cll_guard", lock);\r\ncounters->ctc_nr_locks_locked--;\r\nif (--lock->cll_depth == 0) {\r\nlock->cll_guarder = NULL;\r\nmutex_unlock(&lock->cll_guard);\r\n}\r\n}\r\nint cl_lock_is_mutexed(struct cl_lock *lock)\r\n{\r\nreturn lock->cll_guarder == current;\r\n}\r\nint cl_lock_nr_mutexed(const struct lu_env *env)\r\n{\r\nstruct cl_thread_info *info;\r\nint i;\r\nint locked;\r\ninfo = cl_env_info(env);\r\nfor (i = 0, locked = 0; i < ARRAY_SIZE(info->clt_counters); ++i)\r\nlocked += info->clt_counters[i].ctc_nr_locks_locked;\r\nreturn locked;\r\n}\r\nstatic void cl_lock_cancel0(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nif (!(lock->cll_flags & CLF_CANCELLED)) {\r\nconst struct cl_lock_slice *slice;\r\nlock->cll_flags |= CLF_CANCELLED;\r\nlist_for_each_entry_reverse(slice, &lock->cll_layers,\r\ncls_linkage) {\r\nif (slice->cls_ops->clo_cancel != NULL)\r\nslice->cls_ops->clo_cancel(env, slice);\r\n}\r\n}\r\n}\r\nstatic void cl_lock_delete0(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nstruct cl_object_header *head;\r\nconst struct cl_lock_slice *slice;\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nif (lock->cll_state < CLS_FREEING) {\r\nLASSERT(lock->cll_state != CLS_INTRANSIT);\r\ncl_lock_state_set(env, lock, CLS_FREEING);\r\nhead = cl_object_header(lock->cll_descr.cld_obj);\r\nspin_lock(&head->coh_lock_guard);\r\nlist_del_init(&lock->cll_linkage);\r\nspin_unlock(&head->coh_lock_guard);\r\nlist_for_each_entry_reverse(slice, &lock->cll_layers,\r\ncls_linkage) {\r\nif (slice->cls_ops->clo_delete != NULL)\r\nslice->cls_ops->clo_delete(env, slice);\r\n}\r\n}\r\n}\r\nstatic void cl_lock_hold_mod(const struct lu_env *env, struct cl_lock *lock,\r\nint delta)\r\n{\r\nstruct cl_thread_counters *counters;\r\nenum clt_nesting_level nesting;\r\nlock->cll_holds += delta;\r\nnesting = cl_lock_nesting(lock);\r\nif (nesting == CNL_TOP) {\r\ncounters = &cl_env_info(env)->clt_counters[CNL_TOP];\r\ncounters->ctc_nr_held += delta;\r\nLASSERT(counters->ctc_nr_held >= 0);\r\n}\r\n}\r\nstatic void cl_lock_used_mod(const struct lu_env *env, struct cl_lock *lock,\r\nint delta)\r\n{\r\nstruct cl_thread_counters *counters;\r\nenum clt_nesting_level nesting;\r\nlock->cll_users += delta;\r\nnesting = cl_lock_nesting(lock);\r\nif (nesting == CNL_TOP) {\r\ncounters = &cl_env_info(env)->clt_counters[CNL_TOP];\r\ncounters->ctc_nr_used += delta;\r\nLASSERT(counters->ctc_nr_used >= 0);\r\n}\r\n}\r\nvoid cl_lock_hold_release(const struct lu_env *env, struct cl_lock *lock,\r\nconst char *scope, const void *source)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERT(lock->cll_holds > 0);\r\ncl_lock_trace(D_DLMTRACE, env, "hold release lock", lock);\r\nlu_ref_del(&lock->cll_holders, scope, source);\r\ncl_lock_hold_mod(env, lock, -1);\r\nif (lock->cll_holds == 0) {\r\nCL_LOCK_ASSERT(lock->cll_state != CLS_HELD, env, lock);\r\nif (lock->cll_descr.cld_mode == CLM_PHANTOM ||\r\nlock->cll_descr.cld_mode == CLM_GROUP ||\r\nlock->cll_state != CLS_CACHED)\r\nlock->cll_flags |= CLF_CANCELPEND|CLF_DOOMED;\r\nif (lock->cll_flags & CLF_CANCELPEND) {\r\nlock->cll_flags &= ~CLF_CANCELPEND;\r\ncl_lock_cancel0(env, lock);\r\n}\r\nif (lock->cll_flags & CLF_DOOMED) {\r\nlock->cll_flags &= ~CLF_DOOMED;\r\ncl_lock_delete0(env, lock);\r\n}\r\n}\r\n}\r\nint cl_lock_state_wait(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nwait_queue_t waiter;\r\nsigset_t blocked;\r\nint result;\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERT(lock->cll_depth == 1);\r\nLASSERT(lock->cll_state != CLS_FREEING);\r\ncl_lock_trace(D_DLMTRACE, env, "state wait lock", lock);\r\nresult = lock->cll_error;\r\nif (result == 0) {\r\nblocked = cfs_block_sigsinv(LUSTRE_FATAL_SIGS);\r\ninit_waitqueue_entry_current(&waiter);\r\nadd_wait_queue(&lock->cll_wq, &waiter);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\ncl_lock_mutex_put(env, lock);\r\nLASSERT(cl_lock_nr_mutexed(env) == 0);\r\nresult = -ERESTARTSYS;\r\nif (likely(!OBD_FAIL_CHECK(OBD_FAIL_LOCK_STATE_WAIT_INTR))) {\r\nwaitq_wait(&waiter, TASK_INTERRUPTIBLE);\r\nif (!cfs_signal_pending())\r\nresult = 0;\r\n}\r\ncl_lock_mutex_get(env, lock);\r\nset_current_state(TASK_RUNNING);\r\nremove_wait_queue(&lock->cll_wq, &waiter);\r\ncfs_restore_sigs(blocked);\r\n}\r\nreturn result;\r\n}\r\nstatic void cl_lock_state_signal(const struct lu_env *env, struct cl_lock *lock,\r\nenum cl_lock_state state)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage)\r\nif (slice->cls_ops->clo_state != NULL)\r\nslice->cls_ops->clo_state(env, slice, state);\r\nwake_up_all(&lock->cll_wq);\r\n}\r\nvoid cl_lock_signal(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\ncl_lock_trace(D_DLMTRACE, env, "state signal lock", lock);\r\ncl_lock_state_signal(env, lock, lock->cll_state);\r\n}\r\nvoid cl_lock_state_set(const struct lu_env *env, struct cl_lock *lock,\r\nenum cl_lock_state state)\r\n{\r\nLASSERT(lock->cll_state <= state ||\r\n(lock->cll_state == CLS_CACHED &&\r\n(state == CLS_HELD ||\r\nstate == CLS_NEW ||\r\nstate == CLS_INTRANSIT)) ||\r\nlock->cll_state == CLS_INTRANSIT);\r\nif (lock->cll_state != state) {\r\nCS_LOCKSTATE_DEC(lock->cll_descr.cld_obj, lock->cll_state);\r\nCS_LOCKSTATE_INC(lock->cll_descr.cld_obj, state);\r\ncl_lock_state_signal(env, lock, state);\r\nlock->cll_state = state;\r\n}\r\n}\r\nstatic int cl_unuse_try_internal(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nint result;\r\ndo {\r\nresult = 0;\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERT(lock->cll_state == CLS_INTRANSIT);\r\nresult = -ENOSYS;\r\nlist_for_each_entry_reverse(slice, &lock->cll_layers,\r\ncls_linkage) {\r\nif (slice->cls_ops->clo_unuse != NULL) {\r\nresult = slice->cls_ops->clo_unuse(env, slice);\r\nif (result != 0)\r\nbreak;\r\n}\r\n}\r\nLASSERT(result != -ENOSYS);\r\n} while (result == CLO_REPEAT);\r\nreturn result;\r\n}\r\nint cl_use_try(const struct lu_env *env, struct cl_lock *lock, int atomic)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nint result;\r\nenum cl_lock_state state;\r\ncl_lock_trace(D_DLMTRACE, env, "use lock", lock);\r\nLASSERT(lock->cll_state == CLS_CACHED);\r\nif (lock->cll_error)\r\nreturn lock->cll_error;\r\nresult = -ENOSYS;\r\nstate = cl_lock_intransit(env, lock);\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_ops->clo_use != NULL) {\r\nresult = slice->cls_ops->clo_use(env, slice);\r\nif (result != 0)\r\nbreak;\r\n}\r\n}\r\nLASSERT(result != -ENOSYS);\r\nLASSERTF(lock->cll_state == CLS_INTRANSIT, "Wrong state %d.\n",\r\nlock->cll_state);\r\nif (result == 0) {\r\nstate = CLS_HELD;\r\n} else {\r\nif (result == -ESTALE) {\r\nstate = CLS_NEW;\r\nresult = CLO_REPEAT;\r\n}\r\nif (atomic) {\r\nint rc;\r\nrc = cl_unuse_try_internal(env, lock);\r\nif (rc < 0 && result > 0)\r\nresult = rc;\r\n}\r\n}\r\ncl_lock_extransit(env, lock, state);\r\nreturn result;\r\n}\r\nstatic int cl_enqueue_kick(const struct lu_env *env,\r\nstruct cl_lock *lock,\r\nstruct cl_io *io, __u32 flags)\r\n{\r\nint result;\r\nconst struct cl_lock_slice *slice;\r\nresult = -ENOSYS;\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_ops->clo_enqueue != NULL) {\r\nresult = slice->cls_ops->clo_enqueue(env,\r\nslice, io, flags);\r\nif (result != 0)\r\nbreak;\r\n}\r\n}\r\nLASSERT(result != -ENOSYS);\r\nreturn result;\r\n}\r\nint cl_enqueue_try(const struct lu_env *env, struct cl_lock *lock,\r\nstruct cl_io *io, __u32 flags)\r\n{\r\nint result;\r\ncl_lock_trace(D_DLMTRACE, env, "enqueue lock", lock);\r\ndo {\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nresult = lock->cll_error;\r\nif (result != 0)\r\nbreak;\r\nswitch (lock->cll_state) {\r\ncase CLS_NEW:\r\ncl_lock_state_set(env, lock, CLS_QUEUING);\r\ncase CLS_QUEUING:\r\nresult = cl_enqueue_kick(env, lock, io, flags);\r\nif (result == 0 && lock->cll_state == CLS_QUEUING)\r\ncl_lock_state_set(env, lock, CLS_ENQUEUED);\r\nbreak;\r\ncase CLS_INTRANSIT:\r\nLASSERT(cl_lock_is_intransit(lock));\r\nresult = CLO_WAIT;\r\nbreak;\r\ncase CLS_CACHED:\r\nresult = cl_use_try(env, lock, 0);\r\nbreak;\r\ncase CLS_ENQUEUED:\r\ncase CLS_HELD:\r\nresult = 0;\r\nbreak;\r\ndefault:\r\ncase CLS_FREEING:\r\nLBUG();\r\n}\r\n} while (result == CLO_REPEAT);\r\nreturn result;\r\n}\r\nint cl_lock_enqueue_wait(const struct lu_env *env,\r\nstruct cl_lock *lock,\r\nint keep_mutex)\r\n{\r\nstruct cl_lock *conflict;\r\nint rc = 0;\r\nLASSERT(cl_lock_is_mutexed(lock));\r\nLASSERT(lock->cll_state == CLS_QUEUING);\r\nLASSERT(lock->cll_conflict != NULL);\r\nconflict = lock->cll_conflict;\r\nlock->cll_conflict = NULL;\r\ncl_lock_mutex_put(env, lock);\r\nLASSERT(cl_lock_nr_mutexed(env) == 0);\r\ncl_lock_mutex_get(env, conflict);\r\ncl_lock_trace(D_DLMTRACE, env, "enqueue wait", conflict);\r\ncl_lock_cancel(env, conflict);\r\ncl_lock_delete(env, conflict);\r\nwhile (conflict->cll_state != CLS_FREEING) {\r\nrc = cl_lock_state_wait(env, conflict);\r\nif (rc != 0)\r\nbreak;\r\n}\r\ncl_lock_mutex_put(env, conflict);\r\nlu_ref_del(&conflict->cll_reference, "cancel-wait", lock);\r\ncl_lock_put(env, conflict);\r\nif (keep_mutex)\r\ncl_lock_mutex_get(env, lock);\r\nLASSERT(rc <= 0);\r\nreturn rc;\r\n}\r\nstatic int cl_enqueue_locked(const struct lu_env *env, struct cl_lock *lock,\r\nstruct cl_io *io, __u32 enqflags)\r\n{\r\nint result;\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERT(lock->cll_holds > 0);\r\ncl_lock_user_add(env, lock);\r\ndo {\r\nresult = cl_enqueue_try(env, lock, io, enqflags);\r\nif (result == CLO_WAIT) {\r\nif (lock->cll_conflict != NULL)\r\nresult = cl_lock_enqueue_wait(env, lock, 1);\r\nelse\r\nresult = cl_lock_state_wait(env, lock);\r\nif (result == 0)\r\ncontinue;\r\n}\r\nbreak;\r\n} while (1);\r\nif (result != 0)\r\ncl_unuse_try(env, lock);\r\nLASSERT(ergo(result == 0 && !(enqflags & CEF_AGL),\r\nlock->cll_state == CLS_ENQUEUED ||\r\nlock->cll_state == CLS_HELD));\r\nreturn result;\r\n}\r\nint cl_enqueue(const struct lu_env *env, struct cl_lock *lock,\r\nstruct cl_io *io, __u32 enqflags)\r\n{\r\nint result;\r\ncl_lock_lockdep_acquire(env, lock, enqflags);\r\ncl_lock_mutex_get(env, lock);\r\nresult = cl_enqueue_locked(env, lock, io, enqflags);\r\ncl_lock_mutex_put(env, lock);\r\nif (result != 0)\r\ncl_lock_lockdep_release(env, lock);\r\nLASSERT(ergo(result == 0, lock->cll_state == CLS_ENQUEUED ||\r\nlock->cll_state == CLS_HELD));\r\nreturn result;\r\n}\r\nint cl_unuse_try(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nint result;\r\nenum cl_lock_state state = CLS_NEW;\r\ncl_lock_trace(D_DLMTRACE, env, "unuse lock", lock);\r\nif (lock->cll_users > 1) {\r\ncl_lock_user_del(env, lock);\r\nreturn 0;\r\n}\r\nif (!(lock->cll_state == CLS_HELD || lock->cll_state == CLS_ENQUEUED)) {\r\ncl_lock_user_del(env, lock);\r\nreturn 0;\r\n}\r\nstate = cl_lock_intransit(env, lock);\r\nresult = cl_unuse_try_internal(env, lock);\r\nLASSERT(lock->cll_state == CLS_INTRANSIT);\r\nLASSERT(result != CLO_WAIT);\r\ncl_lock_user_del(env, lock);\r\nif (result == 0 || result == -ESTALE) {\r\nif (state == CLS_HELD && result == 0)\r\nstate = CLS_CACHED;\r\nelse\r\nstate = CLS_NEW;\r\ncl_lock_extransit(env, lock, state);\r\nresult = 0;\r\n} else {\r\nCERROR("result = %d, this is unlikely!\n", result);\r\nstate = CLS_NEW;\r\ncl_lock_extransit(env, lock, state);\r\n}\r\nreturn result ?: lock->cll_error;\r\n}\r\nstatic void cl_unuse_locked(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nint result;\r\nresult = cl_unuse_try(env, lock);\r\nif (result)\r\nCL_LOCK_DEBUG(D_ERROR, env, lock, "unuse return %d\n", result);\r\n}\r\nvoid cl_unuse(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\ncl_lock_mutex_get(env, lock);\r\ncl_unuse_locked(env, lock);\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_lockdep_release(env, lock);\r\n}\r\nint cl_wait_try(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nint result;\r\ncl_lock_trace(D_DLMTRACE, env, "wait lock try", lock);\r\ndo {\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERTF(lock->cll_state == CLS_QUEUING ||\r\nlock->cll_state == CLS_ENQUEUED ||\r\nlock->cll_state == CLS_HELD ||\r\nlock->cll_state == CLS_INTRANSIT,\r\n"lock state: %d\n", lock->cll_state);\r\nLASSERT(lock->cll_users > 0);\r\nLASSERT(lock->cll_holds > 0);\r\nresult = lock->cll_error;\r\nif (result != 0)\r\nbreak;\r\nif (cl_lock_is_intransit(lock)) {\r\nresult = CLO_WAIT;\r\nbreak;\r\n}\r\nif (lock->cll_state == CLS_HELD)\r\nbreak;\r\nresult = -ENOSYS;\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_ops->clo_wait != NULL) {\r\nresult = slice->cls_ops->clo_wait(env, slice);\r\nif (result != 0)\r\nbreak;\r\n}\r\n}\r\nLASSERT(result != -ENOSYS);\r\nif (result == 0) {\r\nLASSERT(lock->cll_state != CLS_INTRANSIT);\r\ncl_lock_state_set(env, lock, CLS_HELD);\r\n}\r\n} while (result == CLO_REPEAT);\r\nreturn result;\r\n}\r\nint cl_wait(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nint result;\r\ncl_lock_mutex_get(env, lock);\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERTF(lock->cll_state == CLS_ENQUEUED || lock->cll_state == CLS_HELD,\r\n"Wrong state %d \n", lock->cll_state);\r\nLASSERT(lock->cll_holds > 0);\r\ndo {\r\nresult = cl_wait_try(env, lock);\r\nif (result == CLO_WAIT) {\r\nresult = cl_lock_state_wait(env, lock);\r\nif (result == 0)\r\ncontinue;\r\n}\r\nbreak;\r\n} while (1);\r\nif (result < 0) {\r\ncl_unuse_try(env, lock);\r\ncl_lock_lockdep_release(env, lock);\r\n}\r\ncl_lock_trace(D_DLMTRACE, env, "wait lock", lock);\r\ncl_lock_mutex_put(env, lock);\r\nLASSERT(ergo(result == 0, lock->cll_state == CLS_HELD));\r\nreturn result;\r\n}\r\nunsigned long cl_lock_weigh(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nunsigned long pound;\r\nunsigned long ounce;\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\npound = 0;\r\nlist_for_each_entry_reverse(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_ops->clo_weigh != NULL) {\r\nounce = slice->cls_ops->clo_weigh(env, slice);\r\npound += ounce;\r\nif (pound < ounce)\r\npound = ~0UL;\r\n}\r\n}\r\nreturn pound;\r\n}\r\nint cl_lock_modify(const struct lu_env *env, struct cl_lock *lock,\r\nconst struct cl_lock_descr *desc)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nstruct cl_object *obj = lock->cll_descr.cld_obj;\r\nstruct cl_object_header *hdr = cl_object_header(obj);\r\nint result;\r\ncl_lock_trace(D_DLMTRACE, env, "modify lock", lock);\r\nLASSERT(obj == desc->cld_obj);\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nlist_for_each_entry_reverse(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_ops->clo_modify != NULL) {\r\nresult = slice->cls_ops->clo_modify(env, slice, desc);\r\nif (result != 0)\r\nreturn result;\r\n}\r\n}\r\nCL_LOCK_DEBUG(D_DLMTRACE, env, lock, " -> "DDESCR"@"DFID"\n",\r\nPDESCR(desc), PFID(lu_object_fid(&desc->cld_obj->co_lu)));\r\nspin_lock(&hdr->coh_lock_guard);\r\nlock->cll_descr = *desc;\r\nspin_unlock(&hdr->coh_lock_guard);\r\nreturn 0;\r\n}\r\nvoid cl_lock_closure_init(const struct lu_env *env,\r\nstruct cl_lock_closure *closure,\r\nstruct cl_lock *origin, int wait)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(origin));\r\nLINVRNT(cl_lock_invariant(env, origin));\r\nINIT_LIST_HEAD(&closure->clc_list);\r\nclosure->clc_origin = origin;\r\nclosure->clc_wait = wait;\r\nclosure->clc_nr = 0;\r\n}\r\nint cl_lock_closure_build(const struct lu_env *env, struct cl_lock *lock,\r\nstruct cl_lock_closure *closure)\r\n{\r\nconst struct cl_lock_slice *slice;\r\nint result;\r\nLINVRNT(cl_lock_is_mutexed(closure->clc_origin));\r\nLINVRNT(cl_lock_invariant(env, closure->clc_origin));\r\nresult = cl_lock_enclosure(env, lock, closure);\r\nif (result == 0) {\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage) {\r\nif (slice->cls_ops->clo_closure != NULL) {\r\nresult = slice->cls_ops->clo_closure(env, slice,\r\nclosure);\r\nif (result != 0)\r\nbreak;\r\n}\r\n}\r\n}\r\nif (result != 0)\r\ncl_lock_disclosure(env, closure);\r\nreturn result;\r\n}\r\nint cl_lock_enclosure(const struct lu_env *env, struct cl_lock *lock,\r\nstruct cl_lock_closure *closure)\r\n{\r\nint result = 0;\r\ncl_lock_trace(D_DLMTRACE, env, "enclosure lock", lock);\r\nif (!cl_lock_mutex_try(env, lock)) {\r\nif (list_empty(&lock->cll_inclosure)) {\r\ncl_lock_get_trust(lock);\r\nlu_ref_add(&lock->cll_reference, "closure", closure);\r\nlist_add(&lock->cll_inclosure, &closure->clc_list);\r\nclosure->clc_nr++;\r\n} else\r\ncl_lock_mutex_put(env, lock);\r\nresult = 0;\r\n} else {\r\ncl_lock_disclosure(env, closure);\r\nif (closure->clc_wait) {\r\ncl_lock_get_trust(lock);\r\nlu_ref_add(&lock->cll_reference, "closure-w", closure);\r\ncl_lock_mutex_put(env, closure->clc_origin);\r\nLASSERT(cl_lock_nr_mutexed(env) == 0);\r\ncl_lock_mutex_get(env, lock);\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_mutex_get(env, closure->clc_origin);\r\nlu_ref_del(&lock->cll_reference, "closure-w", closure);\r\ncl_lock_put(env, lock);\r\n}\r\nresult = CLO_REPEAT;\r\n}\r\nreturn result;\r\n}\r\nvoid cl_lock_disclosure(const struct lu_env *env,\r\nstruct cl_lock_closure *closure)\r\n{\r\nstruct cl_lock *scan;\r\nstruct cl_lock *temp;\r\ncl_lock_trace(D_DLMTRACE, env, "disclosure lock", closure->clc_origin);\r\nlist_for_each_entry_safe(scan, temp, &closure->clc_list,\r\ncll_inclosure){\r\nlist_del_init(&scan->cll_inclosure);\r\ncl_lock_mutex_put(env, scan);\r\nlu_ref_del(&scan->cll_reference, "closure", closure);\r\ncl_lock_put(env, scan);\r\nclosure->clc_nr--;\r\n}\r\nLASSERT(closure->clc_nr == 0);\r\n}\r\nvoid cl_lock_closure_fini(struct cl_lock_closure *closure)\r\n{\r\nLASSERT(closure->clc_nr == 0);\r\nLASSERT(list_empty(&closure->clc_list));\r\n}\r\nvoid cl_lock_delete(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERT(ergo(cl_lock_nesting(lock) == CNL_TOP,\r\ncl_lock_nr_mutexed(env) == 1));\r\ncl_lock_trace(D_DLMTRACE, env, "delete lock", lock);\r\nif (lock->cll_holds == 0)\r\ncl_lock_delete0(env, lock);\r\nelse\r\nlock->cll_flags |= CLF_DOOMED;\r\n}\r\nvoid cl_lock_error(const struct lu_env *env, struct cl_lock *lock, int error)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nif (lock->cll_error == 0 && error != 0) {\r\ncl_lock_trace(D_DLMTRACE, env, "set lock error", lock);\r\nlock->cll_error = error;\r\ncl_lock_signal(env, lock);\r\ncl_lock_cancel(env, lock);\r\ncl_lock_delete(env, lock);\r\n}\r\n}\r\nvoid cl_lock_cancel(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\ncl_lock_trace(D_DLMTRACE, env, "cancel lock", lock);\r\nif (lock->cll_holds == 0)\r\ncl_lock_cancel0(env, lock);\r\nelse\r\nlock->cll_flags |= CLF_CANCELPEND;\r\n}\r\nstruct cl_lock *cl_lock_at_pgoff(const struct lu_env *env,\r\nstruct cl_object *obj, pgoff_t index,\r\nstruct cl_lock *except,\r\nint pending, int canceld)\r\n{\r\nstruct cl_object_header *head;\r\nstruct cl_lock *scan;\r\nstruct cl_lock *lock;\r\nstruct cl_lock_descr *need;\r\nhead = cl_object_header(obj);\r\nneed = &cl_env_info(env)->clt_descr;\r\nlock = NULL;\r\nneed->cld_mode = CLM_READ;\r\nneed->cld_start = need->cld_end = index;\r\nneed->cld_enq_flags = 0;\r\nspin_lock(&head->coh_lock_guard);\r\nlist_for_each_entry(scan, &head->coh_locks, cll_linkage) {\r\nif (scan != except &&\r\n(scan->cll_descr.cld_mode == CLM_GROUP ||\r\ncl_lock_ext_match(&scan->cll_descr, need)) &&\r\nscan->cll_state >= CLS_HELD &&\r\nscan->cll_state < CLS_FREEING &&\r\n(canceld || !(scan->cll_flags & CLF_CANCELLED)) &&\r\n(pending || !(scan->cll_flags & CLF_CANCELPEND))) {\r\ncl_lock_get_trust(scan);\r\nlock = scan;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&head->coh_lock_guard);\r\nreturn lock;\r\n}\r\nstatic pgoff_t pgoff_at_lock(struct cl_page *page, struct cl_lock *lock)\r\n{\r\nstruct lu_device_type *dtype;\r\nconst struct cl_page_slice *slice;\r\ndtype = lock->cll_descr.cld_obj->co_lu.lo_dev->ld_type;\r\nslice = cl_page_at(page, dtype);\r\nLASSERT(slice != NULL);\r\nreturn slice->cpl_page->cp_index;\r\n}\r\nstatic int check_and_discard_cb(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *page, void *cbdata)\r\n{\r\nstruct cl_thread_info *info = cl_env_info(env);\r\nstruct cl_lock *lock = cbdata;\r\npgoff_t index = pgoff_at_lock(page, lock);\r\nif (index >= info->clt_fn_index) {\r\nstruct cl_lock *tmp;\r\ntmp = cl_lock_at_pgoff(env, lock->cll_descr.cld_obj, index,\r\nlock, 1, 0);\r\nif (tmp != NULL) {\r\ninfo->clt_fn_index = tmp->cll_descr.cld_end + 1;\r\nif (tmp->cll_descr.cld_end == CL_PAGE_EOF)\r\ninfo->clt_fn_index = CL_PAGE_EOF;\r\ncl_lock_put(env, tmp);\r\n} else if (cl_page_own(env, io, page) == 0) {\r\ncl_page_unmap(env, io, page);\r\ncl_page_discard(env, io, page);\r\ncl_page_disown(env, io, page);\r\n} else {\r\nLASSERT(page->cp_state == CPS_FREEING);\r\n}\r\n}\r\ninfo->clt_next_index = index + 1;\r\nreturn CLP_GANG_OKAY;\r\n}\r\nstatic int discard_cb(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page *page, void *cbdata)\r\n{\r\nstruct cl_thread_info *info = cl_env_info(env);\r\nstruct cl_lock *lock = cbdata;\r\nLASSERT(lock->cll_descr.cld_mode >= CLM_WRITE);\r\nKLASSERT(ergo(page->cp_type == CPT_CACHEABLE,\r\n!PageWriteback(cl_page_vmpage(env, page))));\r\nKLASSERT(ergo(page->cp_type == CPT_CACHEABLE,\r\n!PageDirty(cl_page_vmpage(env, page))));\r\ninfo->clt_next_index = pgoff_at_lock(page, lock) + 1;\r\nif (cl_page_own(env, io, page) == 0) {\r\ncl_page_unmap(env, io, page);\r\ncl_page_discard(env, io, page);\r\ncl_page_disown(env, io, page);\r\n} else {\r\nLASSERT(page->cp_state == CPS_FREEING);\r\n}\r\nreturn CLP_GANG_OKAY;\r\n}\r\nint cl_lock_discard_pages(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nstruct cl_thread_info *info = cl_env_info(env);\r\nstruct cl_io *io = &info->clt_io;\r\nstruct cl_lock_descr *descr = &lock->cll_descr;\r\ncl_page_gang_cb_t cb;\r\nint res;\r\nint result;\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nio->ci_obj = cl_object_top(descr->cld_obj);\r\nio->ci_ignore_layout = 1;\r\nresult = cl_io_init(env, io, CIT_MISC, io->ci_obj);\r\nif (result != 0)\r\nGOTO(out, result);\r\ncb = descr->cld_mode == CLM_READ ? check_and_discard_cb : discard_cb;\r\ninfo->clt_fn_index = info->clt_next_index = descr->cld_start;\r\ndo {\r\nres = cl_page_gang_lookup(env, descr->cld_obj, io,\r\ninfo->clt_next_index, descr->cld_end,\r\ncb, (void *)lock);\r\nif (info->clt_next_index > descr->cld_end)\r\nbreak;\r\nif (res == CLP_GANG_RESCHED)\r\ncond_resched();\r\n} while (res != CLP_GANG_OKAY);\r\nout:\r\ncl_io_fini(env, io);\r\nreturn result;\r\n}\r\nvoid cl_locks_prune(const struct lu_env *env, struct cl_object *obj, int cancel)\r\n{\r\nstruct cl_object_header *head;\r\nstruct cl_lock *lock;\r\nhead = cl_object_header(obj);\r\nLASSERT(ergo(!cancel,\r\nhead->coh_tree.rnode == NULL && head->coh_pages == 0));\r\nspin_lock(&head->coh_lock_guard);\r\nwhile (!list_empty(&head->coh_locks)) {\r\nlock = container_of(head->coh_locks.next,\r\nstruct cl_lock, cll_linkage);\r\ncl_lock_get_trust(lock);\r\nspin_unlock(&head->coh_lock_guard);\r\nlu_ref_add(&lock->cll_reference, "prune", current);\r\nagain:\r\ncl_lock_mutex_get(env, lock);\r\nif (lock->cll_state < CLS_FREEING) {\r\nLASSERT(lock->cll_users <= 1);\r\nif (unlikely(lock->cll_users == 1)) {\r\nstruct l_wait_info lwi = { 0 };\r\ncl_lock_mutex_put(env, lock);\r\nl_wait_event(lock->cll_wq,\r\nlock->cll_users == 0,\r\n&lwi);\r\ngoto again;\r\n}\r\nif (cancel)\r\ncl_lock_cancel(env, lock);\r\ncl_lock_delete(env, lock);\r\n}\r\ncl_lock_mutex_put(env, lock);\r\nlu_ref_del(&lock->cll_reference, "prune", current);\r\ncl_lock_put(env, lock);\r\nspin_lock(&head->coh_lock_guard);\r\n}\r\nspin_unlock(&head->coh_lock_guard);\r\n}\r\nstatic struct cl_lock *cl_lock_hold_mutex(const struct lu_env *env,\r\nconst struct cl_io *io,\r\nconst struct cl_lock_descr *need,\r\nconst char *scope, const void *source)\r\n{\r\nstruct cl_lock *lock;\r\nwhile (1) {\r\nlock = cl_lock_find(env, io, need);\r\nif (IS_ERR(lock))\r\nbreak;\r\ncl_lock_mutex_get(env, lock);\r\nif (lock->cll_state < CLS_FREEING &&\r\n!(lock->cll_flags & CLF_CANCELLED)) {\r\ncl_lock_hold_mod(env, lock, +1);\r\nlu_ref_add(&lock->cll_holders, scope, source);\r\nlu_ref_add(&lock->cll_reference, scope, source);\r\nbreak;\r\n}\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_put(env, lock);\r\n}\r\nreturn lock;\r\n}\r\nstruct cl_lock *cl_lock_hold(const struct lu_env *env, const struct cl_io *io,\r\nconst struct cl_lock_descr *need,\r\nconst char *scope, const void *source)\r\n{\r\nstruct cl_lock *lock;\r\nlock = cl_lock_hold_mutex(env, io, need, scope, source);\r\nif (!IS_ERR(lock))\r\ncl_lock_mutex_put(env, lock);\r\nreturn lock;\r\n}\r\nstruct cl_lock *cl_lock_request(const struct lu_env *env, struct cl_io *io,\r\nconst struct cl_lock_descr *need,\r\nconst char *scope, const void *source)\r\n{\r\nstruct cl_lock *lock;\r\nint rc;\r\n__u32 enqflags = need->cld_enq_flags;\r\ndo {\r\nlock = cl_lock_hold_mutex(env, io, need, scope, source);\r\nif (IS_ERR(lock))\r\nbreak;\r\nrc = cl_enqueue_locked(env, lock, io, enqflags);\r\nif (rc == 0) {\r\nif (cl_lock_fits_into(env, lock, need, io)) {\r\nif (!(enqflags & CEF_AGL)) {\r\ncl_lock_mutex_put(env, lock);\r\ncl_lock_lockdep_acquire(env, lock,\r\nenqflags);\r\nbreak;\r\n}\r\nrc = 1;\r\n}\r\ncl_unuse_locked(env, lock);\r\n}\r\ncl_lock_trace(D_DLMTRACE, env,\r\nrc <= 0 ? "enqueue failed" : "agl succeed", lock);\r\ncl_lock_hold_release(env, lock, scope, source);\r\ncl_lock_mutex_put(env, lock);\r\nlu_ref_del(&lock->cll_reference, scope, source);\r\ncl_lock_put(env, lock);\r\nif (rc > 0) {\r\nLASSERT(enqflags & CEF_AGL);\r\nlock = NULL;\r\n} else if (rc != 0) {\r\nlock = ERR_PTR(rc);\r\n}\r\n} while (rc == 0);\r\nreturn lock;\r\n}\r\nvoid cl_lock_hold_add(const struct lu_env *env, struct cl_lock *lock,\r\nconst char *scope, const void *source)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERT(lock->cll_state != CLS_FREEING);\r\ncl_lock_hold_mod(env, lock, +1);\r\ncl_lock_get(lock);\r\nlu_ref_add(&lock->cll_holders, scope, source);\r\nlu_ref_add(&lock->cll_reference, scope, source);\r\n}\r\nvoid cl_lock_unhold(const struct lu_env *env, struct cl_lock *lock,\r\nconst char *scope, const void *source)\r\n{\r\nLINVRNT(cl_lock_invariant(env, lock));\r\ncl_lock_hold_release(env, lock, scope, source);\r\nlu_ref_del(&lock->cll_reference, scope, source);\r\ncl_lock_put(env, lock);\r\n}\r\nvoid cl_lock_release(const struct lu_env *env, struct cl_lock *lock,\r\nconst char *scope, const void *source)\r\n{\r\nLINVRNT(cl_lock_invariant(env, lock));\r\ncl_lock_trace(D_DLMTRACE, env, "release lock", lock);\r\ncl_lock_mutex_get(env, lock);\r\ncl_lock_hold_release(env, lock, scope, source);\r\ncl_lock_mutex_put(env, lock);\r\nlu_ref_del(&lock->cll_reference, scope, source);\r\ncl_lock_put(env, lock);\r\n}\r\nvoid cl_lock_user_add(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\ncl_lock_used_mod(env, lock, +1);\r\n}\r\nvoid cl_lock_user_del(const struct lu_env *env, struct cl_lock *lock)\r\n{\r\nLINVRNT(cl_lock_is_mutexed(lock));\r\nLINVRNT(cl_lock_invariant(env, lock));\r\nLASSERT(lock->cll_users > 0);\r\ncl_lock_used_mod(env, lock, -1);\r\nif (lock->cll_users == 0)\r\nwake_up_all(&lock->cll_wq);\r\n}\r\nconst char *cl_lock_mode_name(const enum cl_lock_mode mode)\r\n{\r\nstatic const char *names[] = {\r\n[CLM_PHANTOM] = "P",\r\n[CLM_READ] = "R",\r\n[CLM_WRITE] = "W",\r\n[CLM_GROUP] = "G"\r\n};\r\nif (0 <= mode && mode < ARRAY_SIZE(names))\r\nreturn names[mode];\r\nelse\r\nreturn "U";\r\n}\r\nvoid cl_lock_descr_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer,\r\nconst struct cl_lock_descr *descr)\r\n{\r\nconst struct lu_fid *fid;\r\nfid = lu_object_fid(&descr->cld_obj->co_lu);\r\n(*printer)(env, cookie, DDESCR"@"DFID, PDESCR(descr), PFID(fid));\r\n}\r\nvoid cl_lock_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer, const struct cl_lock *lock)\r\n{\r\nconst struct cl_lock_slice *slice;\r\n(*printer)(env, cookie, "lock@%p[%d %d %d %d %d %08lx] ",\r\nlock, atomic_read(&lock->cll_ref),\r\nlock->cll_state, lock->cll_error, lock->cll_holds,\r\nlock->cll_users, lock->cll_flags);\r\ncl_lock_descr_print(env, cookie, printer, &lock->cll_descr);\r\n(*printer)(env, cookie, " {\n");\r\nlist_for_each_entry(slice, &lock->cll_layers, cls_linkage) {\r\n(*printer)(env, cookie, " %s@%p: ",\r\nslice->cls_obj->co_lu.lo_dev->ld_type->ldt_name,\r\nslice);\r\nif (slice->cls_ops->clo_print != NULL)\r\nslice->cls_ops->clo_print(env, cookie, printer, slice);\r\n(*printer)(env, cookie, "\n");\r\n}\r\n(*printer)(env, cookie, "} lock@%p\n", lock);\r\n}\r\nint cl_lock_init(void)\r\n{\r\nreturn lu_kmem_init(cl_lock_caches);\r\n}\r\nvoid cl_lock_fini(void)\r\n{\r\nlu_kmem_fini(cl_lock_caches);\r\n}
