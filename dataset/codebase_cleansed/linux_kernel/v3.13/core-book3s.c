static inline unsigned long perf_ip_adjust(struct pt_regs *regs)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp) { }\r\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void perf_read_regs(struct pt_regs *regs)\r\n{\r\nregs->result = 0;\r\n}\r\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline int siar_valid(struct pt_regs *regs)\r\n{\r\nreturn 1;\r\n}\r\nstatic bool is_ebb_event(struct perf_event *event) { return false; }\r\nstatic int ebb_event_check(struct perf_event *event) { return 0; }\r\nstatic void ebb_event_add(struct perf_event *event) { }\r\nstatic void ebb_switch_out(unsigned long mmcr0) { }\r\nstatic unsigned long ebb_switch_in(bool ebb, unsigned long mmcr0)\r\n{\r\nreturn mmcr0;\r\n}\r\nstatic inline void power_pmu_bhrb_enable(struct perf_event *event) {}\r\nstatic inline void power_pmu_bhrb_disable(struct perf_event *event) {}\r\nvoid power_pmu_flush_branch_stack(void) {}\r\nstatic inline void power_pmu_bhrb_read(struct cpu_hw_events *cpuhw) {}\r\nstatic bool regs_use_siar(struct pt_regs *regs)\r\n{\r\nreturn !!regs->result;\r\n}\r\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\r\n{\r\nunsigned long mmcra = regs->dsisr;\r\nif ((ppmu->flags & PPMU_HAS_SSLOT) && (mmcra & MMCRA_SAMPLE_ENABLE)) {\r\nunsigned long slot = (mmcra & MMCRA_SLOT) >> MMCRA_SLOT_SHIFT;\r\nif (slot > 1)\r\nreturn 4 * (slot - 1);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void perf_get_data_addr(struct pt_regs *regs, u64 *addrp)\r\n{\r\nunsigned long mmcra = regs->dsisr;\r\nbool sdar_valid;\r\nif (ppmu->flags & PPMU_HAS_SIER)\r\nsdar_valid = regs->dar & SIER_SDAR_VALID;\r\nelse {\r\nunsigned long sdsync;\r\nif (ppmu->flags & PPMU_SIAR_VALID)\r\nsdsync = POWER7P_MMCRA_SDAR_VALID;\r\nelse if (ppmu->flags & PPMU_ALT_SIPR)\r\nsdsync = POWER6_MMCRA_SDSYNC;\r\nelse\r\nsdsync = MMCRA_SDSYNC;\r\nsdar_valid = mmcra & sdsync;\r\n}\r\nif (!(mmcra & MMCRA_SAMPLE_ENABLE) || sdar_valid)\r\n*addrp = mfspr(SPRN_SDAR);\r\n}\r\nstatic bool regs_sihv(struct pt_regs *regs)\r\n{\r\nunsigned long sihv = MMCRA_SIHV;\r\nif (ppmu->flags & PPMU_HAS_SIER)\r\nreturn !!(regs->dar & SIER_SIHV);\r\nif (ppmu->flags & PPMU_ALT_SIPR)\r\nsihv = POWER6_MMCRA_SIHV;\r\nreturn !!(regs->dsisr & sihv);\r\n}\r\nstatic bool regs_sipr(struct pt_regs *regs)\r\n{\r\nunsigned long sipr = MMCRA_SIPR;\r\nif (ppmu->flags & PPMU_HAS_SIER)\r\nreturn !!(regs->dar & SIER_SIPR);\r\nif (ppmu->flags & PPMU_ALT_SIPR)\r\nsipr = POWER6_MMCRA_SIPR;\r\nreturn !!(regs->dsisr & sipr);\r\n}\r\nstatic inline u32 perf_flags_from_msr(struct pt_regs *regs)\r\n{\r\nif (regs->msr & MSR_PR)\r\nreturn PERF_RECORD_MISC_USER;\r\nif ((regs->msr & MSR_HV) && freeze_events_kernel != MMCR0_FCHV)\r\nreturn PERF_RECORD_MISC_HYPERVISOR;\r\nreturn PERF_RECORD_MISC_KERNEL;\r\n}\r\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\r\n{\r\nbool use_siar = regs_use_siar(regs);\r\nif (!use_siar)\r\nreturn perf_flags_from_msr(regs);\r\nif (ppmu->flags & PPMU_NO_SIPR) {\r\nunsigned long siar = mfspr(SPRN_SIAR);\r\nif (siar >= PAGE_OFFSET)\r\nreturn PERF_RECORD_MISC_KERNEL;\r\nreturn PERF_RECORD_MISC_USER;\r\n}\r\nif (regs_sipr(regs))\r\nreturn PERF_RECORD_MISC_USER;\r\nif (regs_sihv(regs) && (freeze_events_kernel != MMCR0_FCHV))\r\nreturn PERF_RECORD_MISC_HYPERVISOR;\r\nreturn PERF_RECORD_MISC_KERNEL;\r\n}\r\nstatic inline void perf_read_regs(struct pt_regs *regs)\r\n{\r\nunsigned long mmcra = mfspr(SPRN_MMCRA);\r\nint marked = mmcra & MMCRA_SAMPLE_ENABLE;\r\nint use_siar;\r\nregs->dsisr = mmcra;\r\nif (ppmu->flags & PPMU_HAS_SIER)\r\nregs->dar = mfspr(SPRN_SIER);\r\nif (TRAP(regs) != 0xf00)\r\nuse_siar = 0;\r\nelse if (marked)\r\nuse_siar = 1;\r\nelse if ((ppmu->flags & PPMU_NO_CONT_SAMPLING))\r\nuse_siar = 0;\r\nelse if (!(ppmu->flags & PPMU_NO_SIPR) && regs_sipr(regs))\r\nuse_siar = 0;\r\nelse\r\nuse_siar = 1;\r\nregs->result = use_siar;\r\n}\r\nstatic inline int perf_intr_is_nmi(struct pt_regs *regs)\r\n{\r\nreturn !regs->softe;\r\n}\r\nstatic inline int siar_valid(struct pt_regs *regs)\r\n{\r\nunsigned long mmcra = regs->dsisr;\r\nint marked = mmcra & MMCRA_SAMPLE_ENABLE;\r\nif (marked) {\r\nif (ppmu->flags & PPMU_HAS_SIER)\r\nreturn regs->dar & SIER_SIAR_VALID;\r\nif (ppmu->flags & PPMU_SIAR_VALID)\r\nreturn mmcra & POWER7P_MMCRA_SIAR_VALID;\r\n}\r\nreturn 1;\r\n}\r\nstatic void power_pmu_bhrb_reset(void)\r\n{\r\nasm volatile(PPC_CLRBHRB);\r\n}\r\nstatic void power_pmu_bhrb_enable(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\r\nif (!ppmu->bhrb_nr)\r\nreturn;\r\nif (event->ctx->task && cpuhw->bhrb_context != event->ctx) {\r\npower_pmu_bhrb_reset();\r\ncpuhw->bhrb_context = event->ctx;\r\n}\r\ncpuhw->bhrb_users++;\r\n}\r\nstatic void power_pmu_bhrb_disable(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\r\nif (!ppmu->bhrb_nr)\r\nreturn;\r\ncpuhw->bhrb_users--;\r\nWARN_ON_ONCE(cpuhw->bhrb_users < 0);\r\nif (!cpuhw->disabled && !cpuhw->bhrb_users) {\r\ncpuhw->bhrb_context = NULL;\r\n}\r\n}\r\nvoid power_pmu_flush_branch_stack(void)\r\n{\r\nif (ppmu->bhrb_nr)\r\npower_pmu_bhrb_reset();\r\n}\r\nstatic __u64 power_pmu_bhrb_to(u64 addr)\r\n{\r\nunsigned int instr;\r\nint ret;\r\n__u64 target;\r\nif (is_kernel_addr(addr))\r\nreturn branch_target((unsigned int *)addr);\r\npagefault_disable();\r\nret = __get_user_inatomic(instr, (unsigned int __user *)addr);\r\nif (ret) {\r\npagefault_enable();\r\nreturn 0;\r\n}\r\npagefault_enable();\r\ntarget = branch_target(&instr);\r\nif ((!target) || (instr & BRANCH_ABSOLUTE))\r\nreturn target;\r\nreturn target - (unsigned long)&instr + addr;\r\n}\r\nvoid power_pmu_bhrb_read(struct cpu_hw_events *cpuhw)\r\n{\r\nu64 val;\r\nu64 addr;\r\nint r_index, u_index, pred;\r\nr_index = 0;\r\nu_index = 0;\r\nwhile (r_index < ppmu->bhrb_nr) {\r\nval = read_bhrb(r_index++);\r\nif (!val)\r\nbreak;\r\nelse {\r\naddr = val & BHRB_EA;\r\npred = val & BHRB_PREDICTION;\r\nif (!addr)\r\ncontinue;\r\nif (val & BHRB_TARGET) {\r\ncpuhw->bhrb_entries[u_index].to = addr;\r\ncpuhw->bhrb_entries[u_index].mispred = pred;\r\ncpuhw->bhrb_entries[u_index].predicted = ~pred;\r\nval = read_bhrb(r_index++);\r\naddr = val & BHRB_EA;\r\nif (val & BHRB_TARGET) {\r\nr_index--;\r\naddr = 0;\r\n}\r\ncpuhw->bhrb_entries[u_index].from = addr;\r\n} else {\r\ncpuhw->bhrb_entries[u_index].from = addr;\r\ncpuhw->bhrb_entries[u_index].to =\r\npower_pmu_bhrb_to(addr);\r\ncpuhw->bhrb_entries[u_index].mispred = pred;\r\ncpuhw->bhrb_entries[u_index].predicted = ~pred;\r\n}\r\nu_index++;\r\n}\r\n}\r\ncpuhw->bhrb_stack.nr = u_index;\r\nreturn;\r\n}\r\nstatic bool is_ebb_event(struct perf_event *event)\r\n{\r\nreturn (ppmu->flags & PPMU_EBB) &&\r\n((event->attr.config >> PERF_EVENT_CONFIG_EBB_SHIFT) & 1);\r\n}\r\nstatic int ebb_event_check(struct perf_event *event)\r\n{\r\nstruct perf_event *leader = event->group_leader;\r\nif (is_ebb_event(leader) != is_ebb_event(event))\r\nreturn -EINVAL;\r\nif (is_ebb_event(event)) {\r\nif (!(event->attach_state & PERF_ATTACH_TASK))\r\nreturn -EINVAL;\r\nif (!leader->attr.pinned || !leader->attr.exclusive)\r\nreturn -EINVAL;\r\nif (event->attr.inherit || event->attr.sample_period ||\r\nevent->attr.enable_on_exec || event->attr.freq)\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ebb_event_add(struct perf_event *event)\r\n{\r\nif (!is_ebb_event(event) || current->thread.used_ebb)\r\nreturn;\r\ncurrent->thread.used_ebb = 1;\r\ncurrent->thread.mmcr0 |= MMCR0_PMXE;\r\n}\r\nstatic void ebb_switch_out(unsigned long mmcr0)\r\n{\r\nif (!(mmcr0 & MMCR0_EBE))\r\nreturn;\r\ncurrent->thread.siar = mfspr(SPRN_SIAR);\r\ncurrent->thread.sier = mfspr(SPRN_SIER);\r\ncurrent->thread.sdar = mfspr(SPRN_SDAR);\r\ncurrent->thread.mmcr0 = mmcr0 & MMCR0_USER_MASK;\r\ncurrent->thread.mmcr2 = mfspr(SPRN_MMCR2) & MMCR2_USER_MASK;\r\n}\r\nstatic unsigned long ebb_switch_in(bool ebb, unsigned long mmcr0)\r\n{\r\nif (!ebb)\r\ngoto out;\r\nmmcr0 |= MMCR0_EBE | MMCR0_PMCC_U6;\r\nmmcr0 |= current->thread.mmcr0;\r\nif (!(current->thread.mmcr0 & MMCR0_PMXE))\r\nmmcr0 &= ~MMCR0_PMXE;\r\nmtspr(SPRN_SIAR, current->thread.siar);\r\nmtspr(SPRN_SIER, current->thread.sier);\r\nmtspr(SPRN_SDAR, current->thread.sdar);\r\nmtspr(SPRN_MMCR2, current->thread.mmcr2);\r\nout:\r\nreturn mmcr0;\r\n}\r\nvoid perf_event_print_debug(void)\r\n{\r\n}\r\nstatic unsigned long read_pmc(int idx)\r\n{\r\nunsigned long val;\r\nswitch (idx) {\r\ncase 1:\r\nval = mfspr(SPRN_PMC1);\r\nbreak;\r\ncase 2:\r\nval = mfspr(SPRN_PMC2);\r\nbreak;\r\ncase 3:\r\nval = mfspr(SPRN_PMC3);\r\nbreak;\r\ncase 4:\r\nval = mfspr(SPRN_PMC4);\r\nbreak;\r\ncase 5:\r\nval = mfspr(SPRN_PMC5);\r\nbreak;\r\ncase 6:\r\nval = mfspr(SPRN_PMC6);\r\nbreak;\r\n#ifdef CONFIG_PPC64\r\ncase 7:\r\nval = mfspr(SPRN_PMC7);\r\nbreak;\r\ncase 8:\r\nval = mfspr(SPRN_PMC8);\r\nbreak;\r\n#endif\r\ndefault:\r\nprintk(KERN_ERR "oops trying to read PMC%d\n", idx);\r\nval = 0;\r\n}\r\nreturn val;\r\n}\r\nstatic void write_pmc(int idx, unsigned long val)\r\n{\r\nswitch (idx) {\r\ncase 1:\r\nmtspr(SPRN_PMC1, val);\r\nbreak;\r\ncase 2:\r\nmtspr(SPRN_PMC2, val);\r\nbreak;\r\ncase 3:\r\nmtspr(SPRN_PMC3, val);\r\nbreak;\r\ncase 4:\r\nmtspr(SPRN_PMC4, val);\r\nbreak;\r\ncase 5:\r\nmtspr(SPRN_PMC5, val);\r\nbreak;\r\ncase 6:\r\nmtspr(SPRN_PMC6, val);\r\nbreak;\r\n#ifdef CONFIG_PPC64\r\ncase 7:\r\nmtspr(SPRN_PMC7, val);\r\nbreak;\r\ncase 8:\r\nmtspr(SPRN_PMC8, val);\r\nbreak;\r\n#endif\r\ndefault:\r\nprintk(KERN_ERR "oops trying to write PMC%d\n", idx);\r\n}\r\n}\r\nstatic int power_check_constraints(struct cpu_hw_events *cpuhw,\r\nu64 event_id[], unsigned int cflags[],\r\nint n_ev)\r\n{\r\nunsigned long mask, value, nv;\r\nunsigned long smasks[MAX_HWEVENTS], svalues[MAX_HWEVENTS];\r\nint n_alt[MAX_HWEVENTS], choice[MAX_HWEVENTS];\r\nint i, j;\r\nunsigned long addf = ppmu->add_fields;\r\nunsigned long tadd = ppmu->test_adder;\r\nif (n_ev > ppmu->n_counter)\r\nreturn -1;\r\nfor (i = 0; i < n_ev; ++i) {\r\nif ((cflags[i] & PPMU_LIMITED_PMC_REQD)\r\n&& !ppmu->limited_pmc_event(event_id[i])) {\r\nppmu->get_alternatives(event_id[i], cflags[i],\r\ncpuhw->alternatives[i]);\r\nevent_id[i] = cpuhw->alternatives[i][0];\r\n}\r\nif (ppmu->get_constraint(event_id[i], &cpuhw->amasks[i][0],\r\n&cpuhw->avalues[i][0]))\r\nreturn -1;\r\n}\r\nvalue = mask = 0;\r\nfor (i = 0; i < n_ev; ++i) {\r\nnv = (value | cpuhw->avalues[i][0]) +\r\n(value & cpuhw->avalues[i][0] & addf);\r\nif ((((nv + tadd) ^ value) & mask) != 0 ||\r\n(((nv + tadd) ^ cpuhw->avalues[i][0]) &\r\ncpuhw->amasks[i][0]) != 0)\r\nbreak;\r\nvalue = nv;\r\nmask |= cpuhw->amasks[i][0];\r\n}\r\nif (i == n_ev)\r\nreturn 0;\r\nif (!ppmu->get_alternatives)\r\nreturn -1;\r\nfor (i = 0; i < n_ev; ++i) {\r\nchoice[i] = 0;\r\nn_alt[i] = ppmu->get_alternatives(event_id[i], cflags[i],\r\ncpuhw->alternatives[i]);\r\nfor (j = 1; j < n_alt[i]; ++j)\r\nppmu->get_constraint(cpuhw->alternatives[i][j],\r\n&cpuhw->amasks[i][j],\r\n&cpuhw->avalues[i][j]);\r\n}\r\ni = 0;\r\nj = -1;\r\nvalue = mask = nv = 0;\r\nwhile (i < n_ev) {\r\nif (j >= 0) {\r\nvalue = svalues[i];\r\nmask = smasks[i];\r\nj = choice[i];\r\n}\r\nwhile (++j < n_alt[i]) {\r\nnv = (value | cpuhw->avalues[i][j]) +\r\n(value & cpuhw->avalues[i][j] & addf);\r\nif ((((nv + tadd) ^ value) & mask) == 0 &&\r\n(((nv + tadd) ^ cpuhw->avalues[i][j])\r\n& cpuhw->amasks[i][j]) == 0)\r\nbreak;\r\n}\r\nif (j >= n_alt[i]) {\r\nif (--i < 0)\r\nreturn -1;\r\n} else {\r\nchoice[i] = j;\r\nsvalues[i] = value;\r\nsmasks[i] = mask;\r\nvalue = nv;\r\nmask |= cpuhw->amasks[i][j];\r\n++i;\r\nj = -1;\r\n}\r\n}\r\nfor (i = 0; i < n_ev; ++i)\r\nevent_id[i] = cpuhw->alternatives[i][choice[i]];\r\nreturn 0;\r\n}\r\nstatic int check_excludes(struct perf_event **ctrs, unsigned int cflags[],\r\nint n_prev, int n_new)\r\n{\r\nint eu = 0, ek = 0, eh = 0;\r\nint i, n, first;\r\nstruct perf_event *event;\r\nn = n_prev + n_new;\r\nif (n <= 1)\r\nreturn 0;\r\nfirst = 1;\r\nfor (i = 0; i < n; ++i) {\r\nif (cflags[i] & PPMU_LIMITED_PMC_OK) {\r\ncflags[i] &= ~PPMU_LIMITED_PMC_REQD;\r\ncontinue;\r\n}\r\nevent = ctrs[i];\r\nif (first) {\r\neu = event->attr.exclude_user;\r\nek = event->attr.exclude_kernel;\r\neh = event->attr.exclude_hv;\r\nfirst = 0;\r\n} else if (event->attr.exclude_user != eu ||\r\nevent->attr.exclude_kernel != ek ||\r\nevent->attr.exclude_hv != eh) {\r\nreturn -EAGAIN;\r\n}\r\n}\r\nif (eu || ek || eh)\r\nfor (i = 0; i < n; ++i)\r\nif (cflags[i] & PPMU_LIMITED_PMC_OK)\r\ncflags[i] |= PPMU_LIMITED_PMC_REQD;\r\nreturn 0;\r\n}\r\nstatic u64 check_and_compute_delta(u64 prev, u64 val)\r\n{\r\nu64 delta = (val - prev) & 0xfffffffful;\r\nif (prev > val && (prev - val) < 256)\r\ndelta = 0;\r\nreturn delta;\r\n}\r\nstatic void power_pmu_read(struct perf_event *event)\r\n{\r\ns64 val, delta, prev;\r\nif (event->hw.state & PERF_HES_STOPPED)\r\nreturn;\r\nif (!event->hw.idx)\r\nreturn;\r\nif (is_ebb_event(event)) {\r\nval = read_pmc(event->hw.idx);\r\nlocal64_set(&event->hw.prev_count, val);\r\nreturn;\r\n}\r\ndo {\r\nprev = local64_read(&event->hw.prev_count);\r\nbarrier();\r\nval = read_pmc(event->hw.idx);\r\ndelta = check_and_compute_delta(prev, val);\r\nif (!delta)\r\nreturn;\r\n} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\r\nlocal64_add(delta, &event->count);\r\nlocal64_sub(delta, &event->hw.period_left);\r\n}\r\nstatic int is_limited_pmc(int pmcnum)\r\n{\r\nreturn (ppmu->flags & PPMU_LIMITED_PMC5_6)\r\n&& (pmcnum == 5 || pmcnum == 6);\r\n}\r\nstatic void freeze_limited_counters(struct cpu_hw_events *cpuhw,\r\nunsigned long pmc5, unsigned long pmc6)\r\n{\r\nstruct perf_event *event;\r\nu64 val, prev, delta;\r\nint i;\r\nfor (i = 0; i < cpuhw->n_limited; ++i) {\r\nevent = cpuhw->limited_counter[i];\r\nif (!event->hw.idx)\r\ncontinue;\r\nval = (event->hw.idx == 5) ? pmc5 : pmc6;\r\nprev = local64_read(&event->hw.prev_count);\r\nevent->hw.idx = 0;\r\ndelta = check_and_compute_delta(prev, val);\r\nif (delta)\r\nlocal64_add(delta, &event->count);\r\n}\r\n}\r\nstatic void thaw_limited_counters(struct cpu_hw_events *cpuhw,\r\nunsigned long pmc5, unsigned long pmc6)\r\n{\r\nstruct perf_event *event;\r\nu64 val, prev;\r\nint i;\r\nfor (i = 0; i < cpuhw->n_limited; ++i) {\r\nevent = cpuhw->limited_counter[i];\r\nevent->hw.idx = cpuhw->limited_hwidx[i];\r\nval = (event->hw.idx == 5) ? pmc5 : pmc6;\r\nprev = local64_read(&event->hw.prev_count);\r\nif (check_and_compute_delta(prev, val))\r\nlocal64_set(&event->hw.prev_count, val);\r\nperf_event_update_userpage(event);\r\n}\r\n}\r\nstatic void write_mmcr0(struct cpu_hw_events *cpuhw, unsigned long mmcr0)\r\n{\r\nunsigned long pmc5, pmc6;\r\nif (!cpuhw->n_limited) {\r\nmtspr(SPRN_MMCR0, mmcr0);\r\nreturn;\r\n}\r\nasm volatile("mtspr %3,%2; mfspr %0,%4; mfspr %1,%5"\r\n: "=&r" (pmc5), "=&r" (pmc6)\r\n: "r" (mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE)),\r\n"i" (SPRN_MMCR0),\r\n"i" (SPRN_PMC5), "i" (SPRN_PMC6));\r\nif (mmcr0 & MMCR0_FC)\r\nfreeze_limited_counters(cpuhw, pmc5, pmc6);\r\nelse\r\nthaw_limited_counters(cpuhw, pmc5, pmc6);\r\nif (mmcr0 & (MMCR0_PMC1CE | MMCR0_PMCjCE))\r\nmtspr(SPRN_MMCR0, mmcr0);\r\n}\r\nstatic void power_pmu_disable(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nunsigned long flags, mmcr0, val;\r\nif (!ppmu)\r\nreturn;\r\nlocal_irq_save(flags);\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\nif (!cpuhw->disabled) {\r\nif (!cpuhw->pmcs_enabled) {\r\nppc_enable_pmcs();\r\ncpuhw->pmcs_enabled = 1;\r\n}\r\nval = mmcr0 = mfspr(SPRN_MMCR0);\r\nval |= MMCR0_FC;\r\nval &= ~(MMCR0_EBE | MMCR0_PMCC | MMCR0_PMAO | MMCR0_FC56);\r\nwrite_mmcr0(cpuhw, val);\r\nmb();\r\nif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\r\nmtspr(SPRN_MMCRA,\r\ncpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\r\nmb();\r\n}\r\ncpuhw->disabled = 1;\r\ncpuhw->n_added = 0;\r\nebb_switch_out(mmcr0);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void power_pmu_enable(struct pmu *pmu)\r\n{\r\nstruct perf_event *event;\r\nstruct cpu_hw_events *cpuhw;\r\nunsigned long flags;\r\nlong i;\r\nunsigned long val, mmcr0;\r\ns64 left;\r\nunsigned int hwc_index[MAX_HWEVENTS];\r\nint n_lim;\r\nint idx;\r\nbool ebb;\r\nif (!ppmu)\r\nreturn;\r\nlocal_irq_save(flags);\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\nif (!cpuhw->disabled)\r\ngoto out;\r\nif (cpuhw->n_events == 0) {\r\nppc_set_pmu_inuse(0);\r\ngoto out;\r\n}\r\ncpuhw->disabled = 0;\r\nebb = is_ebb_event(cpuhw->event[0]);\r\nif (!cpuhw->n_added) {\r\nmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\r\nmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\r\ngoto out_enable;\r\n}\r\nif (ppmu->compute_mmcr(cpuhw->events, cpuhw->n_events, hwc_index,\r\ncpuhw->mmcr)) {\r\nprintk(KERN_ERR "oops compute_mmcr failed\n");\r\ngoto out;\r\n}\r\nevent = cpuhw->event[0];\r\nif (event->attr.exclude_user)\r\ncpuhw->mmcr[0] |= MMCR0_FCP;\r\nif (event->attr.exclude_kernel)\r\ncpuhw->mmcr[0] |= freeze_events_kernel;\r\nif (event->attr.exclude_hv)\r\ncpuhw->mmcr[0] |= MMCR0_FCHV;\r\nppc_set_pmu_inuse(1);\r\nmtspr(SPRN_MMCRA, cpuhw->mmcr[2] & ~MMCRA_SAMPLE_ENABLE);\r\nmtspr(SPRN_MMCR1, cpuhw->mmcr[1]);\r\nmtspr(SPRN_MMCR0, (cpuhw->mmcr[0] & ~(MMCR0_PMC1CE | MMCR0_PMCjCE))\r\n| MMCR0_FC);\r\nfor (i = 0; i < cpuhw->n_events; ++i) {\r\nevent = cpuhw->event[i];\r\nif (event->hw.idx && event->hw.idx != hwc_index[i] + 1) {\r\npower_pmu_read(event);\r\nwrite_pmc(event->hw.idx, 0);\r\nevent->hw.idx = 0;\r\n}\r\n}\r\ncpuhw->n_limited = n_lim = 0;\r\nfor (i = 0; i < cpuhw->n_events; ++i) {\r\nevent = cpuhw->event[i];\r\nif (event->hw.idx)\r\ncontinue;\r\nidx = hwc_index[i] + 1;\r\nif (is_limited_pmc(idx)) {\r\ncpuhw->limited_counter[n_lim] = event;\r\ncpuhw->limited_hwidx[n_lim] = idx;\r\n++n_lim;\r\ncontinue;\r\n}\r\nif (ebb)\r\nval = local64_read(&event->hw.prev_count);\r\nelse {\r\nval = 0;\r\nif (event->hw.sample_period) {\r\nleft = local64_read(&event->hw.period_left);\r\nif (left < 0x80000000L)\r\nval = 0x80000000L - left;\r\n}\r\nlocal64_set(&event->hw.prev_count, val);\r\n}\r\nevent->hw.idx = idx;\r\nif (event->hw.state & PERF_HES_STOPPED)\r\nval = 0;\r\nwrite_pmc(idx, val);\r\nperf_event_update_userpage(event);\r\n}\r\ncpuhw->n_limited = n_lim;\r\ncpuhw->mmcr[0] |= MMCR0_PMXE | MMCR0_FCECE;\r\nout_enable:\r\nmmcr0 = ebb_switch_in(ebb, cpuhw->mmcr[0]);\r\nmb();\r\nwrite_mmcr0(cpuhw, mmcr0);\r\nif (cpuhw->mmcr[2] & MMCRA_SAMPLE_ENABLE) {\r\nmb();\r\nmtspr(SPRN_MMCRA, cpuhw->mmcr[2]);\r\n}\r\nout:\r\nif (cpuhw->bhrb_users)\r\nppmu->config_bhrb(cpuhw->bhrb_filter);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int collect_events(struct perf_event *group, int max_count,\r\nstruct perf_event *ctrs[], u64 *events,\r\nunsigned int *flags)\r\n{\r\nint n = 0;\r\nstruct perf_event *event;\r\nif (!is_software_event(group)) {\r\nif (n >= max_count)\r\nreturn -1;\r\nctrs[n] = group;\r\nflags[n] = group->hw.event_base;\r\nevents[n++] = group->hw.config;\r\n}\r\nlist_for_each_entry(event, &group->sibling_list, group_entry) {\r\nif (!is_software_event(event) &&\r\nevent->state != PERF_EVENT_STATE_OFF) {\r\nif (n >= max_count)\r\nreturn -1;\r\nctrs[n] = event;\r\nflags[n] = event->hw.event_base;\r\nevents[n++] = event->hw.config;\r\n}\r\n}\r\nreturn n;\r\n}\r\nstatic int power_pmu_add(struct perf_event *event, int ef_flags)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nunsigned long flags;\r\nint n0;\r\nint ret = -EAGAIN;\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\nn0 = cpuhw->n_events;\r\nif (n0 >= ppmu->n_counter)\r\ngoto out;\r\ncpuhw->event[n0] = event;\r\ncpuhw->events[n0] = event->hw.config;\r\ncpuhw->flags[n0] = event->hw.event_base;\r\nif (!(ef_flags & PERF_EF_START))\r\nevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nelse\r\nevent->hw.state = 0;\r\nif (cpuhw->group_flag & PERF_EVENT_TXN)\r\ngoto nocheck;\r\nif (check_excludes(cpuhw->event, cpuhw->flags, n0, 1))\r\ngoto out;\r\nif (power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n0 + 1))\r\ngoto out;\r\nevent->hw.config = cpuhw->events[n0];\r\nnocheck:\r\nebb_event_add(event);\r\n++cpuhw->n_events;\r\n++cpuhw->n_added;\r\nret = 0;\r\nout:\r\nif (has_branch_stack(event)) {\r\npower_pmu_bhrb_enable(event);\r\ncpuhw->bhrb_filter = ppmu->bhrb_filter_map(\r\nevent->attr.branch_sample_type);\r\n}\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nstatic void power_pmu_del(struct perf_event *event, int ef_flags)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nlong i;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\npower_pmu_read(event);\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\nfor (i = 0; i < cpuhw->n_events; ++i) {\r\nif (event == cpuhw->event[i]) {\r\nwhile (++i < cpuhw->n_events) {\r\ncpuhw->event[i-1] = cpuhw->event[i];\r\ncpuhw->events[i-1] = cpuhw->events[i];\r\ncpuhw->flags[i-1] = cpuhw->flags[i];\r\n}\r\n--cpuhw->n_events;\r\nppmu->disable_pmc(event->hw.idx - 1, cpuhw->mmcr);\r\nif (event->hw.idx) {\r\nwrite_pmc(event->hw.idx, 0);\r\nevent->hw.idx = 0;\r\n}\r\nperf_event_update_userpage(event);\r\nbreak;\r\n}\r\n}\r\nfor (i = 0; i < cpuhw->n_limited; ++i)\r\nif (event == cpuhw->limited_counter[i])\r\nbreak;\r\nif (i < cpuhw->n_limited) {\r\nwhile (++i < cpuhw->n_limited) {\r\ncpuhw->limited_counter[i-1] = cpuhw->limited_counter[i];\r\ncpuhw->limited_hwidx[i-1] = cpuhw->limited_hwidx[i];\r\n}\r\n--cpuhw->n_limited;\r\n}\r\nif (cpuhw->n_events == 0) {\r\ncpuhw->mmcr[0] &= ~(MMCR0_PMXE | MMCR0_FCECE);\r\n}\r\nif (has_branch_stack(event))\r\npower_pmu_bhrb_disable(event);\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void power_pmu_start(struct perf_event *event, int ef_flags)\r\n{\r\nunsigned long flags;\r\ns64 left;\r\nunsigned long val;\r\nif (!event->hw.idx || !event->hw.sample_period)\r\nreturn;\r\nif (!(event->hw.state & PERF_HES_STOPPED))\r\nreturn;\r\nif (ef_flags & PERF_EF_RELOAD)\r\nWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\nevent->hw.state = 0;\r\nleft = local64_read(&event->hw.period_left);\r\nval = 0;\r\nif (left < 0x80000000L)\r\nval = 0x80000000L - left;\r\nwrite_pmc(event->hw.idx, val);\r\nperf_event_update_userpage(event);\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void power_pmu_stop(struct perf_event *event, int ef_flags)\r\n{\r\nunsigned long flags;\r\nif (!event->hw.idx || !event->hw.sample_period)\r\nreturn;\r\nif (event->hw.state & PERF_HES_STOPPED)\r\nreturn;\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\npower_pmu_read(event);\r\nevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nwrite_pmc(event->hw.idx, 0);\r\nperf_event_update_userpage(event);\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid power_pmu_start_txn(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\r\nperf_pmu_disable(pmu);\r\ncpuhw->group_flag |= PERF_EVENT_TXN;\r\ncpuhw->n_txn_start = cpuhw->n_events;\r\n}\r\nvoid power_pmu_cancel_txn(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\r\ncpuhw->group_flag &= ~PERF_EVENT_TXN;\r\nperf_pmu_enable(pmu);\r\n}\r\nint power_pmu_commit_txn(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nlong i, n;\r\nif (!ppmu)\r\nreturn -EAGAIN;\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\nn = cpuhw->n_events;\r\nif (check_excludes(cpuhw->event, cpuhw->flags, 0, n))\r\nreturn -EAGAIN;\r\ni = power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n);\r\nif (i < 0)\r\nreturn -EAGAIN;\r\nfor (i = cpuhw->n_txn_start; i < n; ++i)\r\ncpuhw->event[i]->hw.config = cpuhw->events[i];\r\ncpuhw->group_flag &= ~PERF_EVENT_TXN;\r\nperf_pmu_enable(pmu);\r\nreturn 0;\r\n}\r\nstatic int can_go_on_limited_pmc(struct perf_event *event, u64 ev,\r\nunsigned int flags)\r\n{\r\nint n;\r\nu64 alt[MAX_EVENT_ALTERNATIVES];\r\nif (event->attr.exclude_user\r\n|| event->attr.exclude_kernel\r\n|| event->attr.exclude_hv\r\n|| event->attr.sample_period)\r\nreturn 0;\r\nif (ppmu->limited_pmc_event(ev))\r\nreturn 1;\r\nif (!ppmu->get_alternatives)\r\nreturn 0;\r\nflags |= PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD;\r\nn = ppmu->get_alternatives(ev, flags, alt);\r\nreturn n > 0;\r\n}\r\nstatic u64 normal_pmc_alternative(u64 ev, unsigned long flags)\r\n{\r\nu64 alt[MAX_EVENT_ALTERNATIVES];\r\nint n;\r\nflags &= ~(PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD);\r\nn = ppmu->get_alternatives(ev, flags, alt);\r\nif (!n)\r\nreturn 0;\r\nreturn alt[0];\r\n}\r\nstatic void hw_perf_event_destroy(struct perf_event *event)\r\n{\r\nif (!atomic_add_unless(&num_events, -1, 1)) {\r\nmutex_lock(&pmc_reserve_mutex);\r\nif (atomic_dec_return(&num_events) == 0)\r\nrelease_pmc_hardware();\r\nmutex_unlock(&pmc_reserve_mutex);\r\n}\r\n}\r\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\r\n{\r\nunsigned long type, op, result;\r\nint ev;\r\nif (!ppmu->cache_events)\r\nreturn -EINVAL;\r\ntype = config & 0xff;\r\nop = (config >> 8) & 0xff;\r\nresult = (config >> 16) & 0xff;\r\nif (type >= PERF_COUNT_HW_CACHE_MAX ||\r\nop >= PERF_COUNT_HW_CACHE_OP_MAX ||\r\nresult >= PERF_COUNT_HW_CACHE_RESULT_MAX)\r\nreturn -EINVAL;\r\nev = (*ppmu->cache_events)[type][op][result];\r\nif (ev == 0)\r\nreturn -EOPNOTSUPP;\r\nif (ev == -1)\r\nreturn -EINVAL;\r\n*eventp = ev;\r\nreturn 0;\r\n}\r\nstatic int power_pmu_event_init(struct perf_event *event)\r\n{\r\nu64 ev;\r\nunsigned long flags;\r\nstruct perf_event *ctrs[MAX_HWEVENTS];\r\nu64 events[MAX_HWEVENTS];\r\nunsigned int cflags[MAX_HWEVENTS];\r\nint n;\r\nint err;\r\nstruct cpu_hw_events *cpuhw;\r\nif (!ppmu)\r\nreturn -ENOENT;\r\nif (has_branch_stack(event)) {\r\nif (!(ppmu->flags & PPMU_BHRB))\r\nreturn -EOPNOTSUPP;\r\n}\r\nswitch (event->attr.type) {\r\ncase PERF_TYPE_HARDWARE:\r\nev = event->attr.config;\r\nif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\r\nreturn -EOPNOTSUPP;\r\nev = ppmu->generic_events[ev];\r\nbreak;\r\ncase PERF_TYPE_HW_CACHE:\r\nerr = hw_perf_cache_event(event->attr.config, &ev);\r\nif (err)\r\nreturn err;\r\nbreak;\r\ncase PERF_TYPE_RAW:\r\nev = event->attr.config;\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nevent->hw.config_base = ev;\r\nevent->hw.idx = 0;\r\nif (!firmware_has_feature(FW_FEATURE_LPAR))\r\nevent->attr.exclude_hv = 0;\r\nflags = 0;\r\nif (event->attach_state & PERF_ATTACH_TASK)\r\nflags |= PPMU_ONLY_COUNT_RUN;\r\nif (ppmu->flags & PPMU_LIMITED_PMC5_6) {\r\nif (can_go_on_limited_pmc(event, ev, flags)) {\r\nflags |= PPMU_LIMITED_PMC_OK;\r\n} else if (ppmu->limited_pmc_event(ev)) {\r\nev = normal_pmc_alternative(ev, flags);\r\nif (!ev)\r\nreturn -EINVAL;\r\n}\r\n}\r\nerr = ebb_event_check(event);\r\nif (err)\r\nreturn err;\r\nn = 0;\r\nif (event->group_leader != event) {\r\nn = collect_events(event->group_leader, ppmu->n_counter - 1,\r\nctrs, events, cflags);\r\nif (n < 0)\r\nreturn -EINVAL;\r\n}\r\nevents[n] = ev;\r\nctrs[n] = event;\r\ncflags[n] = flags;\r\nif (check_excludes(ctrs, cflags, n, 1))\r\nreturn -EINVAL;\r\ncpuhw = &get_cpu_var(cpu_hw_events);\r\nerr = power_check_constraints(cpuhw, events, cflags, n + 1);\r\nif (has_branch_stack(event)) {\r\ncpuhw->bhrb_filter = ppmu->bhrb_filter_map(\r\nevent->attr.branch_sample_type);\r\nif(cpuhw->bhrb_filter == -1)\r\nreturn -EOPNOTSUPP;\r\n}\r\nput_cpu_var(cpu_hw_events);\r\nif (err)\r\nreturn -EINVAL;\r\nevent->hw.config = events[n];\r\nevent->hw.event_base = cflags[n];\r\nevent->hw.last_period = event->hw.sample_period;\r\nlocal64_set(&event->hw.period_left, event->hw.last_period);\r\nif (is_ebb_event(event))\r\nlocal64_set(&event->hw.prev_count, 0);\r\nerr = 0;\r\nif (!atomic_inc_not_zero(&num_events)) {\r\nmutex_lock(&pmc_reserve_mutex);\r\nif (atomic_read(&num_events) == 0 &&\r\nreserve_pmc_hardware(perf_event_interrupt))\r\nerr = -EBUSY;\r\nelse\r\natomic_inc(&num_events);\r\nmutex_unlock(&pmc_reserve_mutex);\r\n}\r\nevent->destroy = hw_perf_event_destroy;\r\nreturn err;\r\n}\r\nstatic int power_pmu_event_idx(struct perf_event *event)\r\n{\r\nreturn event->hw.idx;\r\n}\r\nssize_t power_events_sysfs_show(struct device *dev,\r\nstruct device_attribute *attr, char *page)\r\n{\r\nstruct perf_pmu_events_attr *pmu_attr;\r\npmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\r\nreturn sprintf(page, "event=0x%02llx\n", pmu_attr->id);\r\n}\r\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\r\nstruct pt_regs *regs)\r\n{\r\nu64 period = event->hw.sample_period;\r\ns64 prev, delta, left;\r\nint record = 0;\r\nif (event->hw.state & PERF_HES_STOPPED) {\r\nwrite_pmc(event->hw.idx, 0);\r\nreturn;\r\n}\r\nprev = local64_read(&event->hw.prev_count);\r\ndelta = check_and_compute_delta(prev, val);\r\nlocal64_add(delta, &event->count);\r\nval = 0;\r\nleft = local64_read(&event->hw.period_left) - delta;\r\nif (delta == 0)\r\nleft++;\r\nif (period) {\r\nif (left <= 0) {\r\nleft += period;\r\nif (left <= 0)\r\nleft = period;\r\nrecord = siar_valid(regs);\r\nevent->hw.last_period = event->hw.sample_period;\r\n}\r\nif (left < 0x80000000LL)\r\nval = 0x80000000LL - left;\r\n}\r\nwrite_pmc(event->hw.idx, val);\r\nlocal64_set(&event->hw.prev_count, val);\r\nlocal64_set(&event->hw.period_left, left);\r\nperf_event_update_userpage(event);\r\nif (record) {\r\nstruct perf_sample_data data;\r\nperf_sample_data_init(&data, ~0ULL, event->hw.last_period);\r\nif (event->attr.sample_type & PERF_SAMPLE_ADDR)\r\nperf_get_data_addr(regs, &data.addr);\r\nif (event->attr.sample_type & PERF_SAMPLE_BRANCH_STACK) {\r\nstruct cpu_hw_events *cpuhw;\r\ncpuhw = &__get_cpu_var(cpu_hw_events);\r\npower_pmu_bhrb_read(cpuhw);\r\ndata.br_stack = &cpuhw->bhrb_stack;\r\n}\r\nif (perf_event_overflow(event, &data, regs))\r\npower_pmu_stop(event, 0);\r\n}\r\n}\r\nunsigned long perf_misc_flags(struct pt_regs *regs)\r\n{\r\nu32 flags = perf_get_misc_flags(regs);\r\nif (flags)\r\nreturn flags;\r\nreturn user_mode(regs) ? PERF_RECORD_MISC_USER :\r\nPERF_RECORD_MISC_KERNEL;\r\n}\r\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\r\n{\r\nbool use_siar = regs_use_siar(regs);\r\nif (use_siar && siar_valid(regs))\r\nreturn mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\r\nelse if (use_siar)\r\nreturn 0;\r\nelse\r\nreturn regs->nip;\r\n}\r\nstatic bool pmc_overflow_power7(unsigned long val)\r\n{\r\nif ((0x80000000 - val) <= 256)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool pmc_overflow(unsigned long val)\r\n{\r\nif ((int)val < 0)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void perf_event_interrupt(struct pt_regs *regs)\r\n{\r\nint i, j;\r\nstruct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);\r\nstruct perf_event *event;\r\nunsigned long val[8];\r\nint found, active;\r\nint nmi;\r\nif (cpuhw->n_limited)\r\nfreeze_limited_counters(cpuhw, mfspr(SPRN_PMC5),\r\nmfspr(SPRN_PMC6));\r\nperf_read_regs(regs);\r\nnmi = perf_intr_is_nmi(regs);\r\nif (nmi)\r\nnmi_enter();\r\nelse\r\nirq_enter();\r\nfor (i = 0; i < ppmu->n_counter; ++i)\r\nval[i] = read_pmc(i + 1);\r\nfound = 0;\r\nfor (i = 0; i < ppmu->n_counter; ++i) {\r\nif (!pmc_overflow(val[i]))\r\ncontinue;\r\nif (is_limited_pmc(i + 1))\r\ncontinue;\r\nfound = 1;\r\nactive = 0;\r\nfor (j = 0; j < cpuhw->n_events; ++j) {\r\nevent = cpuhw->event[j];\r\nif (event->hw.idx == (i + 1)) {\r\nactive = 1;\r\nrecord_and_restart(event, val[i], regs);\r\nbreak;\r\n}\r\n}\r\nif (!active)\r\nwrite_pmc(i + 1, 0);\r\n}\r\nif (!found && pvr_version_is(PVR_POWER7)) {\r\nfor (i = 0; i < cpuhw->n_events; ++i) {\r\nevent = cpuhw->event[i];\r\nif (!event->hw.idx || is_limited_pmc(event->hw.idx))\r\ncontinue;\r\nif (pmc_overflow_power7(val[event->hw.idx - 1])) {\r\nfound = 1;\r\nrecord_and_restart(event,\r\nval[event->hw.idx - 1],\r\nregs);\r\n}\r\n}\r\n}\r\nif (!found && !nmi && printk_ratelimit())\r\nprintk(KERN_WARNING "Can't find PMC that caused IRQ\n");\r\nwrite_mmcr0(cpuhw, cpuhw->mmcr[0]);\r\nif (nmi)\r\nnmi_exit();\r\nelse\r\nirq_exit();\r\n}\r\nstatic void power_pmu_setup(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\r\nif (!ppmu)\r\nreturn;\r\nmemset(cpuhw, 0, sizeof(*cpuhw));\r\ncpuhw->mmcr[0] = MMCR0_FC;\r\n}\r\nstatic int\r\npower_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (long)hcpu;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_UP_PREPARE:\r\npower_pmu_setup(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nint register_power_pmu(struct power_pmu *pmu)\r\n{\r\nif (ppmu)\r\nreturn -EBUSY;\r\nppmu = pmu;\r\npr_info("%s performance monitor hardware support registered\n",\r\npmu->name);\r\npower_pmu.attr_groups = ppmu->attr_groups;\r\n#ifdef MSR_HV\r\nif (mfmsr() & MSR_HV)\r\nfreeze_events_kernel = MMCR0_FCHV;\r\n#endif\r\nperf_pmu_register(&power_pmu, "cpu", PERF_TYPE_RAW);\r\nperf_cpu_notifier(power_pmu_notifier);\r\nreturn 0;\r\n}
