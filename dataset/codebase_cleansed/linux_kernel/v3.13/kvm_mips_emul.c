unsigned long kvm_compute_return_epc(struct kvm_vcpu *vcpu,\r\nunsigned long instpc)\r\n{\r\nunsigned int dspcontrol;\r\nunion mips_instruction insn;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nlong epc = instpc;\r\nlong nextpc = KVM_INVALID_INST;\r\nif (epc & 3)\r\ngoto unaligned;\r\ninsn.word = kvm_get_inst((uint32_t *) epc, vcpu);\r\nif (insn.word == KVM_INVALID_INST)\r\nreturn KVM_INVALID_INST;\r\nswitch (insn.i_format.opcode) {\r\ncase spec_op:\r\nswitch (insn.r_format.func) {\r\ncase jalr_op:\r\narch->gprs[insn.r_format.rd] = epc + 8;\r\ncase jr_op:\r\nnextpc = arch->gprs[insn.r_format.rs];\r\nbreak;\r\n}\r\nbreak;\r\ncase bcond_op:\r\nswitch (insn.i_format.rt) {\r\ncase bltz_op:\r\ncase bltzl_op:\r\nif ((long)arch->gprs[insn.i_format.rs] < 0)\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase bgez_op:\r\ncase bgezl_op:\r\nif ((long)arch->gprs[insn.i_format.rs] >= 0)\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase bltzal_op:\r\ncase bltzall_op:\r\narch->gprs[31] = epc + 8;\r\nif ((long)arch->gprs[insn.i_format.rs] < 0)\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase bgezal_op:\r\ncase bgezall_op:\r\narch->gprs[31] = epc + 8;\r\nif ((long)arch->gprs[insn.i_format.rs] >= 0)\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase bposge32_op:\r\nif (!cpu_has_dsp)\r\ngoto sigill;\r\ndspcontrol = rddsp(0x01);\r\nif (dspcontrol >= 32) {\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\n} else\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\n}\r\nbreak;\r\ncase jal_op:\r\narch->gprs[31] = instpc + 8;\r\ncase j_op:\r\nepc += 4;\r\nepc >>= 28;\r\nepc <<= 28;\r\nepc |= (insn.j_format.target << 2);\r\nnextpc = epc;\r\nbreak;\r\ncase beq_op:\r\ncase beql_op:\r\nif (arch->gprs[insn.i_format.rs] ==\r\narch->gprs[insn.i_format.rt])\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase bne_op:\r\ncase bnel_op:\r\nif (arch->gprs[insn.i_format.rs] !=\r\narch->gprs[insn.i_format.rt])\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase blez_op:\r\ncase blezl_op:\r\nif ((long)arch->gprs[insn.i_format.rs] <= 0)\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase bgtz_op:\r\ncase bgtzl_op:\r\nif ((long)arch->gprs[insn.i_format.rs] > 0)\r\nepc = epc + 4 + (insn.i_format.simmediate << 2);\r\nelse\r\nepc += 8;\r\nnextpc = epc;\r\nbreak;\r\ncase cop1_op:\r\nprintk("%s: unsupported cop1_op\n", __func__);\r\nbreak;\r\n}\r\nreturn nextpc;\r\nunaligned:\r\nprintk("%s: unaligned epc\n", __func__);\r\nreturn nextpc;\r\nsigill:\r\nprintk("%s: DSP branch but not DSP ASE\n", __func__);\r\nreturn nextpc;\r\n}\r\nenum emulation_result update_pc(struct kvm_vcpu *vcpu, uint32_t cause)\r\n{\r\nunsigned long branch_pc;\r\nenum emulation_result er = EMULATE_DONE;\r\nif (cause & CAUSEF_BD) {\r\nbranch_pc = kvm_compute_return_epc(vcpu, vcpu->arch.pc);\r\nif (branch_pc == KVM_INVALID_INST) {\r\ner = EMULATE_FAIL;\r\n} else {\r\nvcpu->arch.pc = branch_pc;\r\nkvm_debug("BD update_pc(): New PC: %#lx\n", vcpu->arch.pc);\r\n}\r\n} else\r\nvcpu->arch.pc += 4;\r\nkvm_debug("update_pc(): New PC: %#lx\n", vcpu->arch.pc);\r\nreturn er;\r\n}\r\nenum emulation_result kvm_mips_emulate_count(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nenum emulation_result er = EMULATE_DONE;\r\nif (!(kvm_read_c0_guest_cause(cop0) & CAUSEF_DC)) {\r\nhrtimer_try_to_cancel(&vcpu->arch.comparecount_timer);\r\nhrtimer_start(&vcpu->arch.comparecount_timer,\r\nktime_set(0, MS_TO_NS(10)), HRTIMER_MODE_REL);\r\n} else {\r\nhrtimer_try_to_cancel(&vcpu->arch.comparecount_timer);\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result kvm_mips_emul_eret(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nenum emulation_result er = EMULATE_DONE;\r\nif (kvm_read_c0_guest_status(cop0) & ST0_EXL) {\r\nkvm_debug("[%#lx] ERET to %#lx\n", vcpu->arch.pc,\r\nkvm_read_c0_guest_epc(cop0));\r\nkvm_clear_c0_guest_status(cop0, ST0_EXL);\r\nvcpu->arch.pc = kvm_read_c0_guest_epc(cop0);\r\n} else if (kvm_read_c0_guest_status(cop0) & ST0_ERL) {\r\nkvm_clear_c0_guest_status(cop0, ST0_ERL);\r\nvcpu->arch.pc = kvm_read_c0_guest_errorepc(cop0);\r\n} else {\r\nprintk("[%#lx] ERET when MIPS_SR_EXL|MIPS_SR_ERL == 0\n",\r\nvcpu->arch.pc);\r\ner = EMULATE_FAIL;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result kvm_mips_emul_wait(struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\nkvm_debug("[%#lx] !!!WAIT!!! (%#lx)\n", vcpu->arch.pc,\r\nvcpu->arch.pending_exceptions);\r\n++vcpu->stat.wait_exits;\r\ntrace_kvm_exit(vcpu, WAIT_EXITS);\r\nif (!vcpu->arch.pending_exceptions) {\r\nvcpu->arch.wait = 1;\r\nkvm_vcpu_block(vcpu);\r\nif (kvm_check_request(KVM_REQ_UNHALT, vcpu)) {\r\nclear_bit(KVM_REQ_UNHALT, &vcpu->requests);\r\nvcpu->run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;\r\n}\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result kvm_mips_emul_tlbr(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nenum emulation_result er = EMULATE_FAIL;\r\nuint32_t pc = vcpu->arch.pc;\r\nprintk("[%#x] COP0_TLBR [%ld]\n", pc, kvm_read_c0_guest_index(cop0));\r\nreturn er;\r\n}\r\nenum emulation_result kvm_mips_emul_tlbwi(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nint index = kvm_read_c0_guest_index(cop0);\r\nenum emulation_result er = EMULATE_DONE;\r\nstruct kvm_mips_tlb *tlb = NULL;\r\nuint32_t pc = vcpu->arch.pc;\r\nif (index < 0 || index >= KVM_MIPS_GUEST_TLB_SIZE) {\r\nprintk("%s: illegal index: %d\n", __func__, index);\r\nprintk\r\n("[%#x] COP0_TLBWI [%d] (entryhi: %#lx, entrylo0: %#lx entrylo1: %#lx, mask: %#lx)\n",\r\npc, index, kvm_read_c0_guest_entryhi(cop0),\r\nkvm_read_c0_guest_entrylo0(cop0),\r\nkvm_read_c0_guest_entrylo1(cop0),\r\nkvm_read_c0_guest_pagemask(cop0));\r\nindex = (index & ~0x80000000) % KVM_MIPS_GUEST_TLB_SIZE;\r\n}\r\ntlb = &vcpu->arch.guest_tlb[index];\r\n#if 1\r\nkvm_mips_host_tlb_inv(vcpu, tlb->tlb_hi);\r\n#endif\r\ntlb->tlb_mask = kvm_read_c0_guest_pagemask(cop0);\r\ntlb->tlb_hi = kvm_read_c0_guest_entryhi(cop0);\r\ntlb->tlb_lo0 = kvm_read_c0_guest_entrylo0(cop0);\r\ntlb->tlb_lo1 = kvm_read_c0_guest_entrylo1(cop0);\r\nkvm_debug\r\n("[%#x] COP0_TLBWI [%d] (entryhi: %#lx, entrylo0: %#lx entrylo1: %#lx, mask: %#lx)\n",\r\npc, index, kvm_read_c0_guest_entryhi(cop0),\r\nkvm_read_c0_guest_entrylo0(cop0), kvm_read_c0_guest_entrylo1(cop0),\r\nkvm_read_c0_guest_pagemask(cop0));\r\nreturn er;\r\n}\r\nenum emulation_result kvm_mips_emul_tlbwr(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nenum emulation_result er = EMULATE_DONE;\r\nstruct kvm_mips_tlb *tlb = NULL;\r\nuint32_t pc = vcpu->arch.pc;\r\nint index;\r\n#if 1\r\nget_random_bytes(&index, sizeof(index));\r\nindex &= (KVM_MIPS_GUEST_TLB_SIZE - 1);\r\n#else\r\nindex = jiffies % KVM_MIPS_GUEST_TLB_SIZE;\r\n#endif\r\nif (index < 0 || index >= KVM_MIPS_GUEST_TLB_SIZE) {\r\nprintk("%s: illegal index: %d\n", __func__, index);\r\nreturn EMULATE_FAIL;\r\n}\r\ntlb = &vcpu->arch.guest_tlb[index];\r\n#if 1\r\nkvm_mips_host_tlb_inv(vcpu, tlb->tlb_hi);\r\n#endif\r\ntlb->tlb_mask = kvm_read_c0_guest_pagemask(cop0);\r\ntlb->tlb_hi = kvm_read_c0_guest_entryhi(cop0);\r\ntlb->tlb_lo0 = kvm_read_c0_guest_entrylo0(cop0);\r\ntlb->tlb_lo1 = kvm_read_c0_guest_entrylo1(cop0);\r\nkvm_debug\r\n("[%#x] COP0_TLBWR[%d] (entryhi: %#lx, entrylo0: %#lx entrylo1: %#lx)\n",\r\npc, index, kvm_read_c0_guest_entryhi(cop0),\r\nkvm_read_c0_guest_entrylo0(cop0),\r\nkvm_read_c0_guest_entrylo1(cop0));\r\nreturn er;\r\n}\r\nenum emulation_result kvm_mips_emul_tlbp(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nlong entryhi = kvm_read_c0_guest_entryhi(cop0);\r\nenum emulation_result er = EMULATE_DONE;\r\nuint32_t pc = vcpu->arch.pc;\r\nint index = -1;\r\nindex = kvm_mips_guest_tlb_lookup(vcpu, entryhi);\r\nkvm_write_c0_guest_index(cop0, index);\r\nkvm_debug("[%#x] COP0_TLBP (entryhi: %#lx), index: %d\n", pc, entryhi,\r\nindex);\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_CP0(uint32_t inst, uint32_t *opc, uint32_t cause,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nenum emulation_result er = EMULATE_DONE;\r\nint32_t rt, rd, copz, sel, co_bit, op;\r\nuint32_t pc = vcpu->arch.pc;\r\nunsigned long curr_pc;\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, cause);\r\nif (er == EMULATE_FAIL) {\r\nreturn er;\r\n}\r\ncopz = (inst >> 21) & 0x1f;\r\nrt = (inst >> 16) & 0x1f;\r\nrd = (inst >> 11) & 0x1f;\r\nsel = inst & 0x7;\r\nco_bit = (inst >> 25) & 1;\r\nif (rd > MIPS_CP0_DESAVE) {\r\nprintk("Invalid rd: %d\n", rd);\r\ner = EMULATE_FAIL;\r\ngoto done;\r\n}\r\nif (co_bit) {\r\nop = (inst) & 0xff;\r\nswitch (op) {\r\ncase tlbr_op:\r\ner = kvm_mips_emul_tlbr(vcpu);\r\nbreak;\r\ncase tlbwi_op:\r\ner = kvm_mips_emul_tlbwi(vcpu);\r\nbreak;\r\ncase tlbwr_op:\r\ner = kvm_mips_emul_tlbwr(vcpu);\r\nbreak;\r\ncase tlbp_op:\r\ner = kvm_mips_emul_tlbp(vcpu);\r\nbreak;\r\ncase rfe_op:\r\nprintk("!!!COP0_RFE!!!\n");\r\nbreak;\r\ncase eret_op:\r\ner = kvm_mips_emul_eret(vcpu);\r\ngoto dont_update_pc;\r\nbreak;\r\ncase wait_op:\r\ner = kvm_mips_emul_wait(vcpu);\r\nbreak;\r\n}\r\n} else {\r\nswitch (copz) {\r\ncase mfc_op:\r\n#ifdef CONFIG_KVM_MIPS_DEBUG_COP0_COUNTERS\r\ncop0->stat[rd][sel]++;\r\n#endif\r\nif ((rd == MIPS_CP0_COUNT) && (sel == 0)) {\r\nvcpu->arch.gprs[rt] = (read_c0_count() >> 2);\r\n} else if ((rd == MIPS_CP0_ERRCTL) && (sel == 0)) {\r\nvcpu->arch.gprs[rt] = 0x0;\r\n#ifdef CONFIG_KVM_MIPS_DYN_TRANS\r\nkvm_mips_trans_mfc0(inst, opc, vcpu);\r\n#endif\r\n}\r\nelse {\r\nvcpu->arch.gprs[rt] = cop0->reg[rd][sel];\r\n#ifdef CONFIG_KVM_MIPS_DYN_TRANS\r\nkvm_mips_trans_mfc0(inst, opc, vcpu);\r\n#endif\r\n}\r\nkvm_debug\r\n("[%#x] MFCz[%d][%d], vcpu->arch.gprs[%d]: %#lx\n",\r\npc, rd, sel, rt, vcpu->arch.gprs[rt]);\r\nbreak;\r\ncase dmfc_op:\r\nvcpu->arch.gprs[rt] = cop0->reg[rd][sel];\r\nbreak;\r\ncase mtc_op:\r\n#ifdef CONFIG_KVM_MIPS_DEBUG_COP0_COUNTERS\r\ncop0->stat[rd][sel]++;\r\n#endif\r\nif ((rd == MIPS_CP0_TLB_INDEX)\r\n&& (vcpu->arch.gprs[rt] >=\r\nKVM_MIPS_GUEST_TLB_SIZE)) {\r\nprintk("Invalid TLB Index: %ld",\r\nvcpu->arch.gprs[rt]);\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\n#define C0_EBASE_CORE_MASK 0xff\r\nif ((rd == MIPS_CP0_PRID) && (sel == 1)) {\r\nkvm_change_c0_guest_ebase(cop0,\r\n~(C0_EBASE_CORE_MASK),\r\nvcpu->arch.gprs[rt]);\r\nprintk("MTCz, cop0->reg[EBASE]: %#lx\n",\r\nkvm_read_c0_guest_ebase(cop0));\r\n} else if (rd == MIPS_CP0_TLB_HI && sel == 0) {\r\nuint32_t nasid =\r\nvcpu->arch.gprs[rt] & ASID_MASK;\r\nif ((KSEGX(vcpu->arch.gprs[rt]) != CKSEG0)\r\n&&\r\n((kvm_read_c0_guest_entryhi(cop0) &\r\nASID_MASK) != nasid)) {\r\nkvm_debug\r\n("MTCz, change ASID from %#lx to %#lx\n",\r\nkvm_read_c0_guest_entryhi(cop0) &\r\nASID_MASK,\r\nvcpu->arch.gprs[rt] & ASID_MASK);\r\nkvm_mips_flush_host_tlb(1);\r\n}\r\nkvm_write_c0_guest_entryhi(cop0,\r\nvcpu->arch.gprs[rt]);\r\n}\r\nelse if ((rd == MIPS_CP0_COUNT) && (sel == 0)) {\r\ngoto done;\r\n} else if ((rd == MIPS_CP0_COMPARE) && (sel == 0)) {\r\nkvm_debug("[%#x] MTCz, COMPARE %#lx <- %#lx\n",\r\npc, kvm_read_c0_guest_compare(cop0),\r\nvcpu->arch.gprs[rt]);\r\nkvm_mips_callbacks->dequeue_timer_int(vcpu);\r\nkvm_write_c0_guest_compare(cop0,\r\nvcpu->arch.gprs[rt]);\r\n} else if ((rd == MIPS_CP0_STATUS) && (sel == 0)) {\r\nkvm_write_c0_guest_status(cop0,\r\nvcpu->arch.gprs[rt]);\r\nkvm_clear_c0_guest_status(cop0,\r\n(ST0_CU1 | ST0_NMI));\r\n#ifdef CONFIG_KVM_MIPS_DYN_TRANS\r\nkvm_mips_trans_mtc0(inst, opc, vcpu);\r\n#endif\r\n} else {\r\ncop0->reg[rd][sel] = vcpu->arch.gprs[rt];\r\n#ifdef CONFIG_KVM_MIPS_DYN_TRANS\r\nkvm_mips_trans_mtc0(inst, opc, vcpu);\r\n#endif\r\n}\r\nkvm_debug("[%#x] MTCz, cop0->reg[%d][%d]: %#lx\n", pc,\r\nrd, sel, cop0->reg[rd][sel]);\r\nbreak;\r\ncase dmtc_op:\r\nprintk\r\n("!!!!!!![%#lx]dmtc_op: rt: %d, rd: %d, sel: %d!!!!!!\n",\r\nvcpu->arch.pc, rt, rd, sel);\r\ner = EMULATE_FAIL;\r\nbreak;\r\ncase mfmcz_op:\r\n#ifdef KVM_MIPS_DEBUG_COP0_COUNTERS\r\ncop0->stat[MIPS_CP0_STATUS][0]++;\r\n#endif\r\nif (rt != 0) {\r\nvcpu->arch.gprs[rt] =\r\nkvm_read_c0_guest_status(cop0);\r\n}\r\nif (inst & 0x20) {\r\nkvm_debug("[%#lx] mfmcz_op: EI\n",\r\nvcpu->arch.pc);\r\nkvm_set_c0_guest_status(cop0, ST0_IE);\r\n} else {\r\nkvm_debug("[%#lx] mfmcz_op: DI\n",\r\nvcpu->arch.pc);\r\nkvm_clear_c0_guest_status(cop0, ST0_IE);\r\n}\r\nbreak;\r\ncase wrpgpr_op:\r\n{\r\nuint32_t css =\r\ncop0->reg[MIPS_CP0_STATUS][2] & 0xf;\r\nuint32_t pss =\r\n(cop0->reg[MIPS_CP0_STATUS][2] >> 6) & 0xf;\r\nif (css || pss) {\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nkvm_debug("WRPGPR[%d][%d] = %#lx\n", pss, rd,\r\nvcpu->arch.gprs[rt]);\r\nvcpu->arch.gprs[rd] = vcpu->arch.gprs[rt];\r\n}\r\nbreak;\r\ndefault:\r\nprintk\r\n("[%#lx]MachEmulateCP0: unsupported COP0, copz: 0x%x\n",\r\nvcpu->arch.pc, copz);\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\n}\r\ndone:\r\nif (er == EMULATE_FAIL) {\r\nvcpu->arch.pc = curr_pc;\r\n}\r\ndont_update_pc:\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_store(uint32_t inst, uint32_t cause,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DO_MMIO;\r\nint32_t op, base, rt, offset;\r\nuint32_t bytes;\r\nvoid *data = run->mmio.data;\r\nunsigned long curr_pc;\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, cause);\r\nif (er == EMULATE_FAIL)\r\nreturn er;\r\nrt = (inst >> 16) & 0x1f;\r\nbase = (inst >> 21) & 0x1f;\r\noffset = inst & 0xffff;\r\nop = (inst >> 26) & 0x3f;\r\nswitch (op) {\r\ncase sb_op:\r\nbytes = 1;\r\nif (bytes > sizeof(run->mmio.data)) {\r\nkvm_err("%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\n}\r\nrun->mmio.phys_addr =\r\nkvm_mips_callbacks->gva_to_gpa(vcpu->arch.\r\nhost_cp0_badvaddr);\r\nif (run->mmio.phys_addr == KVM_INVALID_ADDR) {\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 1;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 1;\r\n*(u8 *) data = vcpu->arch.gprs[rt];\r\nkvm_debug("OP_SB: eaddr: %#lx, gpr: %#lx, data: %#x\n",\r\nvcpu->arch.host_cp0_badvaddr, vcpu->arch.gprs[rt],\r\n*(uint8_t *) data);\r\nbreak;\r\ncase sw_op:\r\nbytes = 4;\r\nif (bytes > sizeof(run->mmio.data)) {\r\nkvm_err("%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\n}\r\nrun->mmio.phys_addr =\r\nkvm_mips_callbacks->gva_to_gpa(vcpu->arch.\r\nhost_cp0_badvaddr);\r\nif (run->mmio.phys_addr == KVM_INVALID_ADDR) {\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 1;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 1;\r\n*(uint32_t *) data = vcpu->arch.gprs[rt];\r\nkvm_debug("[%#lx] OP_SW: eaddr: %#lx, gpr: %#lx, data: %#x\n",\r\nvcpu->arch.pc, vcpu->arch.host_cp0_badvaddr,\r\nvcpu->arch.gprs[rt], *(uint32_t *) data);\r\nbreak;\r\ncase sh_op:\r\nbytes = 2;\r\nif (bytes > sizeof(run->mmio.data)) {\r\nkvm_err("%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\n}\r\nrun->mmio.phys_addr =\r\nkvm_mips_callbacks->gva_to_gpa(vcpu->arch.\r\nhost_cp0_badvaddr);\r\nif (run->mmio.phys_addr == KVM_INVALID_ADDR) {\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 1;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 1;\r\n*(uint16_t *) data = vcpu->arch.gprs[rt];\r\nkvm_debug("[%#lx] OP_SH: eaddr: %#lx, gpr: %#lx, data: %#x\n",\r\nvcpu->arch.pc, vcpu->arch.host_cp0_badvaddr,\r\nvcpu->arch.gprs[rt], *(uint32_t *) data);\r\nbreak;\r\ndefault:\r\nprintk("Store not yet supported");\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nif (er == EMULATE_FAIL) {\r\nvcpu->arch.pc = curr_pc;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_load(uint32_t inst, uint32_t cause,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DO_MMIO;\r\nint32_t op, base, rt, offset;\r\nuint32_t bytes;\r\nrt = (inst >> 16) & 0x1f;\r\nbase = (inst >> 21) & 0x1f;\r\noffset = inst & 0xffff;\r\nop = (inst >> 26) & 0x3f;\r\nvcpu->arch.pending_load_cause = cause;\r\nvcpu->arch.io_gpr = rt;\r\nswitch (op) {\r\ncase lw_op:\r\nbytes = 4;\r\nif (bytes > sizeof(run->mmio.data)) {\r\nkvm_err("%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.phys_addr =\r\nkvm_mips_callbacks->gva_to_gpa(vcpu->arch.\r\nhost_cp0_badvaddr);\r\nif (run->mmio.phys_addr == KVM_INVALID_ADDR) {\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 0;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 0;\r\nbreak;\r\ncase lh_op:\r\ncase lhu_op:\r\nbytes = 2;\r\nif (bytes > sizeof(run->mmio.data)) {\r\nkvm_err("%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.phys_addr =\r\nkvm_mips_callbacks->gva_to_gpa(vcpu->arch.\r\nhost_cp0_badvaddr);\r\nif (run->mmio.phys_addr == KVM_INVALID_ADDR) {\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 0;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 0;\r\nif (op == lh_op)\r\nvcpu->mmio_needed = 2;\r\nelse\r\nvcpu->mmio_needed = 1;\r\nbreak;\r\ncase lbu_op:\r\ncase lb_op:\r\nbytes = 1;\r\nif (bytes > sizeof(run->mmio.data)) {\r\nkvm_err("%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.phys_addr =\r\nkvm_mips_callbacks->gva_to_gpa(vcpu->arch.\r\nhost_cp0_badvaddr);\r\nif (run->mmio.phys_addr == KVM_INVALID_ADDR) {\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 0;\r\nvcpu->mmio_is_write = 0;\r\nif (op == lb_op)\r\nvcpu->mmio_needed = 2;\r\nelse\r\nvcpu->mmio_needed = 1;\r\nbreak;\r\ndefault:\r\nprintk("Load not yet supported");\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nreturn er;\r\n}\r\nint kvm_mips_sync_icache(unsigned long va, struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long offset = (va & ~PAGE_MASK);\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long pa;\r\ngfn_t gfn;\r\npfn_t pfn;\r\ngfn = va >> PAGE_SHIFT;\r\nif (gfn >= kvm->arch.guest_pmap_npages) {\r\nprintk("%s: Invalid gfn: %#llx\n", __func__, gfn);\r\nkvm_mips_dump_host_tlbs();\r\nkvm_arch_vcpu_dump_regs(vcpu);\r\nreturn -1;\r\n}\r\npfn = kvm->arch.guest_pmap[gfn];\r\npa = (pfn << PAGE_SHIFT) | offset;\r\nprintk("%s: va: %#lx, unmapped: %#x\n", __func__, va, CKSEG0ADDR(pa));\r\nmips32_SyncICache(CKSEG0ADDR(pa), 32);\r\nreturn 0;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_cache(uint32_t inst, uint32_t *opc, uint32_t cause,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nextern void (*r4k_blast_dcache) (void);\r\nextern void (*r4k_blast_icache) (void);\r\nenum emulation_result er = EMULATE_DONE;\r\nint32_t offset, cache, op_inst, op, base;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nunsigned long va;\r\nunsigned long curr_pc;\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, cause);\r\nif (er == EMULATE_FAIL)\r\nreturn er;\r\nbase = (inst >> 21) & 0x1f;\r\nop_inst = (inst >> 16) & 0x1f;\r\noffset = inst & 0xffff;\r\ncache = (inst >> 16) & 0x3;\r\nop = (inst >> 18) & 0x7;\r\nva = arch->gprs[base] + offset;\r\nkvm_debug("CACHE (cache: %#x, op: %#x, base[%d]: %#lx, offset: %#x\n",\r\ncache, op, base, arch->gprs[base], offset);\r\nif (op == MIPS_CACHE_OP_INDEX_INV) {\r\nkvm_debug\r\n("@ %#lx/%#lx CACHE (cache: %#x, op: %#x, base[%d]: %#lx, offset: %#x\n",\r\nvcpu->arch.pc, vcpu->arch.gprs[31], cache, op, base,\r\narch->gprs[base], offset);\r\nif (cache == MIPS_CACHE_DCACHE)\r\nr4k_blast_dcache();\r\nelse if (cache == MIPS_CACHE_ICACHE)\r\nr4k_blast_icache();\r\nelse {\r\nprintk("%s: unsupported CACHE INDEX operation\n",\r\n__func__);\r\nreturn EMULATE_FAIL;\r\n}\r\n#ifdef CONFIG_KVM_MIPS_DYN_TRANS\r\nkvm_mips_trans_cache_index(inst, opc, vcpu);\r\n#endif\r\ngoto done;\r\n}\r\npreempt_disable();\r\nif (KVM_GUEST_KSEGX(va) == KVM_GUEST_KSEG0) {\r\nif (kvm_mips_host_tlb_lookup(vcpu, va) < 0) {\r\nkvm_mips_handle_kseg0_tlb_fault(va, vcpu);\r\n}\r\n} else if ((KVM_GUEST_KSEGX(va) < KVM_GUEST_KSEG0) ||\r\nKVM_GUEST_KSEGX(va) == KVM_GUEST_KSEG23) {\r\nint index;\r\nif (kvm_mips_host_tlb_lookup(vcpu, va) >= 0) {\r\ngoto skip_fault;\r\n}\r\nindex = kvm_mips_guest_tlb_lookup(vcpu, (va & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi\r\n(cop0) & ASID_MASK));\r\nif (index < 0) {\r\nvcpu->arch.host_cp0_entryhi = (va & VPN2_MASK);\r\nvcpu->arch.host_cp0_badvaddr = va;\r\ner = kvm_mips_emulate_tlbmiss_ld(cause, NULL, run,\r\nvcpu);\r\npreempt_enable();\r\ngoto dont_update_pc;\r\n} else {\r\nstruct kvm_mips_tlb *tlb = &vcpu->arch.guest_tlb[index];\r\nif (!TLB_IS_VALID(*tlb, va)) {\r\ner = kvm_mips_emulate_tlbinv_ld(cause, NULL,\r\nrun, vcpu);\r\npreempt_enable();\r\ngoto dont_update_pc;\r\n} else {\r\nkvm_mips_handle_mapped_seg_tlb_fault(vcpu, tlb,\r\nNULL,\r\nNULL);\r\n}\r\n}\r\n} else {\r\nprintk\r\n("INVALID CACHE INDEX/ADDRESS (cache: %#x, op: %#x, base[%d]: %#lx, offset: %#x\n",\r\ncache, op, base, arch->gprs[base], offset);\r\ner = EMULATE_FAIL;\r\npreempt_enable();\r\ngoto dont_update_pc;\r\n}\r\nskip_fault:\r\nif (cache == MIPS_CACHE_DCACHE\r\n&& (op == MIPS_CACHE_OP_FILL_WB_INV\r\n|| op == MIPS_CACHE_OP_HIT_INV)) {\r\nflush_dcache_line(va);\r\n#ifdef CONFIG_KVM_MIPS_DYN_TRANS\r\nkvm_mips_trans_cache_va(inst, opc, vcpu);\r\n#endif\r\n} else if (op == MIPS_CACHE_OP_HIT_INV && cache == MIPS_CACHE_ICACHE) {\r\nflush_dcache_line(va);\r\nflush_icache_line(va);\r\n#ifdef CONFIG_KVM_MIPS_DYN_TRANS\r\nkvm_mips_trans_cache_va(inst, opc, vcpu);\r\n#endif\r\n} else {\r\nprintk\r\n("NO-OP CACHE (cache: %#x, op: %#x, base[%d]: %#lx, offset: %#x\n",\r\ncache, op, base, arch->gprs[base], offset);\r\ner = EMULATE_FAIL;\r\npreempt_enable();\r\ngoto dont_update_pc;\r\n}\r\npreempt_enable();\r\ndont_update_pc:\r\nvcpu->arch.pc = curr_pc;\r\ndone:\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_inst(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\nuint32_t inst;\r\nif (cause & CAUSEF_BD) {\r\nopc += 1;\r\n}\r\ninst = kvm_get_inst(opc, vcpu);\r\nswitch (((union mips_instruction)inst).r_format.opcode) {\r\ncase cop0_op:\r\ner = kvm_mips_emulate_CP0(inst, opc, cause, run, vcpu);\r\nbreak;\r\ncase sb_op:\r\ncase sh_op:\r\ncase sw_op:\r\ner = kvm_mips_emulate_store(inst, cause, run, vcpu);\r\nbreak;\r\ncase lb_op:\r\ncase lbu_op:\r\ncase lhu_op:\r\ncase lh_op:\r\ncase lw_op:\r\ner = kvm_mips_emulate_load(inst, cause, run, vcpu);\r\nbreak;\r\ncase cache_op:\r\n++vcpu->stat.cache_exits;\r\ntrace_kvm_exit(vcpu, CACHE_EXITS);\r\ner = kvm_mips_emulate_cache(inst, opc, cause, run, vcpu);\r\nbreak;\r\ndefault:\r\nprintk("Instruction emulation not supported (%p/%#x)\n", opc,\r\ninst);\r\nkvm_arch_vcpu_dump_regs(vcpu);\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_syscall(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("Delivering SYSCALL @ pc %#lx\n", arch->pc);\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_SYSCALL << CAUSEB_EXCCODE));\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n} else {\r\nprintk("Trying to deliver SYSCALL when EXL is already set\n");\r\ner = EMULATE_FAIL;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_tlbmiss_ld(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nunsigned long entryhi = (vcpu->arch. host_cp0_badvaddr & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi(cop0) & ASID_MASK);\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("[EXL == 0] delivering TLB MISS @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x0;\r\n} else {\r\nkvm_debug("[EXL == 1] delivering TLB MISS @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n}\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_TLB_LD_MISS << CAUSEB_EXCCODE));\r\nkvm_write_c0_guest_badvaddr(cop0, vcpu->arch.host_cp0_badvaddr);\r\nkvm_write_c0_guest_entryhi(cop0, entryhi);\r\nkvm_mips_flush_host_tlb(1);\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_tlbinv_ld(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nunsigned long entryhi =\r\n(vcpu->arch.host_cp0_badvaddr & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi(cop0) & ASID_MASK);\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("[EXL == 0] delivering TLB INV @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n} else {\r\nkvm_debug("[EXL == 1] delivering TLB MISS @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n}\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_TLB_LD_MISS << CAUSEB_EXCCODE));\r\nkvm_write_c0_guest_badvaddr(cop0, vcpu->arch.host_cp0_badvaddr);\r\nkvm_write_c0_guest_entryhi(cop0, entryhi);\r\nkvm_mips_flush_host_tlb(1);\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_tlbmiss_st(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nunsigned long entryhi = (vcpu->arch.host_cp0_badvaddr & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi(cop0) & ASID_MASK);\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("[EXL == 0] Delivering TLB MISS @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x0;\r\n} else {\r\nkvm_debug("[EXL == 1] Delivering TLB MISS @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n}\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_TLB_ST_MISS << CAUSEB_EXCCODE));\r\nkvm_write_c0_guest_badvaddr(cop0, vcpu->arch.host_cp0_badvaddr);\r\nkvm_write_c0_guest_entryhi(cop0, entryhi);\r\nkvm_mips_flush_host_tlb(1);\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_tlbinv_st(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nunsigned long entryhi = (vcpu->arch.host_cp0_badvaddr & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi(cop0) & ASID_MASK);\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("[EXL == 0] Delivering TLB MISS @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n} else {\r\nkvm_debug("[EXL == 1] Delivering TLB MISS @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n}\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_TLB_ST_MISS << CAUSEB_EXCCODE));\r\nkvm_write_c0_guest_badvaddr(cop0, vcpu->arch.host_cp0_badvaddr);\r\nkvm_write_c0_guest_entryhi(cop0, entryhi);\r\nkvm_mips_flush_host_tlb(1);\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_handle_tlbmod(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\n#ifdef DEBUG\r\nindex = kvm_mips_guest_tlb_lookup(vcpu, entryhi);\r\nif (index < 0) {\r\nkvm_mips_host_tlb_inv(vcpu, vcpu->arch.host_cp0_badvaddr);\r\nkvm_err("%s: host got TLBMOD for %#lx but entry not present in Guest TLB\n",\r\n__func__, entryhi);\r\nkvm_mips_dump_guest_tlbs(vcpu);\r\nkvm_mips_dump_host_tlbs();\r\nreturn EMULATE_FAIL;\r\n}\r\n#endif\r\ner = kvm_mips_emulate_tlbmod(cause, opc, run, vcpu);\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_tlbmod(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nunsigned long entryhi = (vcpu->arch.host_cp0_badvaddr & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi(cop0) & ASID_MASK);\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("[EXL == 0] Delivering TLB MOD @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n} else {\r\nkvm_debug("[EXL == 1] Delivering TLB MOD @ pc %#lx\n",\r\narch->pc);\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n}\r\nkvm_change_c0_guest_cause(cop0, (0xff), (T_TLB_MOD << CAUSEB_EXCCODE));\r\nkvm_write_c0_guest_badvaddr(cop0, vcpu->arch.host_cp0_badvaddr);\r\nkvm_write_c0_guest_entryhi(cop0, entryhi);\r\nkvm_mips_flush_host_tlb(1);\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_fpu_exc(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\n}\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_COP_UNUSABLE << CAUSEB_EXCCODE));\r\nkvm_change_c0_guest_cause(cop0, (CAUSEF_CE), (0x1 << CAUSEB_CE));\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_ri_exc(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("Delivering RI @ pc %#lx\n", arch->pc);\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_RES_INST << CAUSEB_EXCCODE));\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n} else {\r\nkvm_err("Trying to deliver RI when EXL is already set\n");\r\ner = EMULATE_FAIL;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_emulate_bp_exc(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_debug("Delivering BP @ pc %#lx\n", arch->pc);\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(T_BREAK << CAUSEB_EXCCODE));\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\n} else {\r\nprintk("Trying to deliver BP when EXL is already set\n");\r\ner = EMULATE_FAIL;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_handle_ri(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nunsigned long curr_pc;\r\nuint32_t inst;\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, cause);\r\nif (er == EMULATE_FAIL)\r\nreturn er;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\ninst = kvm_get_inst(opc, vcpu);\r\nif (inst == KVM_INVALID_INST) {\r\nprintk("%s: Cannot get inst @ %p\n", __func__, opc);\r\nreturn EMULATE_FAIL;\r\n}\r\nif ((inst & OPCODE) == SPEC3 && (inst & FUNC) == RDHWR) {\r\nint rd = (inst & RD) >> 11;\r\nint rt = (inst & RT) >> 16;\r\nswitch (rd) {\r\ncase 0:\r\narch->gprs[rt] = 0;\r\nbreak;\r\ncase 1:\r\narch->gprs[rt] = min(current_cpu_data.dcache.linesz,\r\ncurrent_cpu_data.icache.linesz);\r\nbreak;\r\ncase 2:\r\nprintk("RDHWR: Cont register\n");\r\narch->gprs[rt] = kvm_read_c0_guest_count(cop0);\r\nbreak;\r\ncase 3:\r\nswitch (current_cpu_data.cputype) {\r\ncase CPU_20KC:\r\ncase CPU_25KF:\r\narch->gprs[rt] = 1;\r\nbreak;\r\ndefault:\r\narch->gprs[rt] = 2;\r\n}\r\nbreak;\r\ncase 29:\r\n#if 1\r\narch->gprs[rt] = kvm_read_c0_guest_userlocal(cop0);\r\n#else\r\ner = kvm_mips_emulate_ri_exc(cause, opc, run, vcpu);\r\n#endif\r\nbreak;\r\ndefault:\r\nprintk("RDHWR not supported\n");\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\n} else {\r\nprintk("Emulate RI not supported @ %p: %#x\n", opc, inst);\r\ner = EMULATE_FAIL;\r\n}\r\nif (er == EMULATE_FAIL) {\r\nvcpu->arch.pc = curr_pc;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_complete_mmio_load(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nunsigned long *gpr = &vcpu->arch.gprs[vcpu->arch.io_gpr];\r\nenum emulation_result er = EMULATE_DONE;\r\nunsigned long curr_pc;\r\nif (run->mmio.len > sizeof(*gpr)) {\r\nprintk("Bad MMIO length: %d", run->mmio.len);\r\ner = EMULATE_FAIL;\r\ngoto done;\r\n}\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, vcpu->arch.pending_load_cause);\r\nif (er == EMULATE_FAIL)\r\nreturn er;\r\nswitch (run->mmio.len) {\r\ncase 4:\r\n*gpr = *(int32_t *) run->mmio.data;\r\nbreak;\r\ncase 2:\r\nif (vcpu->mmio_needed == 2)\r\n*gpr = *(int16_t *) run->mmio.data;\r\nelse\r\n*gpr = *(int16_t *) run->mmio.data;\r\nbreak;\r\ncase 1:\r\nif (vcpu->mmio_needed == 2)\r\n*gpr = *(int8_t *) run->mmio.data;\r\nelse\r\n*gpr = *(u8 *) run->mmio.data;\r\nbreak;\r\n}\r\nif (vcpu->arch.pending_load_cause & CAUSEF_BD)\r\nkvm_debug\r\n("[%#lx] Completing %d byte BD Load to gpr %d (0x%08lx) type %d\n",\r\nvcpu->arch.pc, run->mmio.len, vcpu->arch.io_gpr, *gpr,\r\nvcpu->mmio_needed);\r\ndone:\r\nreturn er;\r\n}\r\nstatic enum emulation_result\r\nkvm_mips_emulate_exc(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nuint32_t exccode = (cause >> CAUSEB_EXCCODE) & 0x1f;\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nenum emulation_result er = EMULATE_DONE;\r\nif ((kvm_read_c0_guest_status(cop0) & ST0_EXL) == 0) {\r\nkvm_write_c0_guest_epc(cop0, arch->pc);\r\nkvm_set_c0_guest_status(cop0, ST0_EXL);\r\nif (cause & CAUSEF_BD)\r\nkvm_set_c0_guest_cause(cop0, CAUSEF_BD);\r\nelse\r\nkvm_clear_c0_guest_cause(cop0, CAUSEF_BD);\r\nkvm_change_c0_guest_cause(cop0, (0xff),\r\n(exccode << CAUSEB_EXCCODE));\r\narch->pc = KVM_GUEST_KSEG0 + 0x180;\r\nkvm_write_c0_guest_badvaddr(cop0, vcpu->arch.host_cp0_badvaddr);\r\nkvm_debug("Delivering EXC %d @ pc %#lx, badVaddr: %#lx\n",\r\nexccode, kvm_read_c0_guest_epc(cop0),\r\nkvm_read_c0_guest_badvaddr(cop0));\r\n} else {\r\nprintk("Trying to deliver EXC when EXL is already set\n");\r\ner = EMULATE_FAIL;\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_check_privilege(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\nuint32_t exccode = (cause >> CAUSEB_EXCCODE) & 0x1f;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nint usermode = !KVM_GUEST_KERNEL_MODE(vcpu);\r\nif (usermode) {\r\nswitch (exccode) {\r\ncase T_INT:\r\ncase T_SYSCALL:\r\ncase T_BREAK:\r\ncase T_RES_INST:\r\nbreak;\r\ncase T_COP_UNUSABLE:\r\nif (((cause & CAUSEF_CE) >> CAUSEB_CE) == 0)\r\ner = EMULATE_PRIV_FAIL;\r\nbreak;\r\ncase T_TLB_MOD:\r\nbreak;\r\ncase T_TLB_LD_MISS:\r\nif (badvaddr >= (unsigned long) KVM_GUEST_KSEG0) {\r\nprintk("%s: LD MISS @ %#lx\n", __func__,\r\nbadvaddr);\r\ncause &= ~0xff;\r\ncause |= (T_ADDR_ERR_LD << CAUSEB_EXCCODE);\r\ner = EMULATE_PRIV_FAIL;\r\n}\r\nbreak;\r\ncase T_TLB_ST_MISS:\r\nif (badvaddr >= (unsigned long) KVM_GUEST_KSEG0) {\r\nprintk("%s: ST MISS @ %#lx\n", __func__,\r\nbadvaddr);\r\ncause &= ~0xff;\r\ncause |= (T_ADDR_ERR_ST << CAUSEB_EXCCODE);\r\ner = EMULATE_PRIV_FAIL;\r\n}\r\nbreak;\r\ncase T_ADDR_ERR_ST:\r\nprintk("%s: address error ST @ %#lx\n", __func__,\r\nbadvaddr);\r\nif ((badvaddr & PAGE_MASK) == KVM_GUEST_COMMPAGE_ADDR) {\r\ncause &= ~0xff;\r\ncause |= (T_TLB_ST_MISS << CAUSEB_EXCCODE);\r\n}\r\ner = EMULATE_PRIV_FAIL;\r\nbreak;\r\ncase T_ADDR_ERR_LD:\r\nprintk("%s: address error LD @ %#lx\n", __func__,\r\nbadvaddr);\r\nif ((badvaddr & PAGE_MASK) == KVM_GUEST_COMMPAGE_ADDR) {\r\ncause &= ~0xff;\r\ncause |= (T_TLB_LD_MISS << CAUSEB_EXCCODE);\r\n}\r\ner = EMULATE_PRIV_FAIL;\r\nbreak;\r\ndefault:\r\ner = EMULATE_PRIV_FAIL;\r\nbreak;\r\n}\r\n}\r\nif (er == EMULATE_PRIV_FAIL) {\r\nkvm_mips_emulate_exc(cause, opc, run, vcpu);\r\n}\r\nreturn er;\r\n}\r\nenum emulation_result\r\nkvm_mips_handle_tlbmiss(unsigned long cause, uint32_t *opc,\r\nstruct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\nuint32_t exccode = (cause >> CAUSEB_EXCCODE) & 0x1f;\r\nunsigned long va = vcpu->arch.host_cp0_badvaddr;\r\nint index;\r\nkvm_debug("kvm_mips_handle_tlbmiss: badvaddr: %#lx, entryhi: %#lx\n",\r\nvcpu->arch.host_cp0_badvaddr, vcpu->arch.host_cp0_entryhi);\r\nindex = kvm_mips_guest_tlb_lookup(vcpu,\r\n(va & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi\r\n(vcpu->arch.cop0) & ASID_MASK));\r\nif (index < 0) {\r\nif (exccode == T_TLB_LD_MISS) {\r\ner = kvm_mips_emulate_tlbmiss_ld(cause, opc, run, vcpu);\r\n} else if (exccode == T_TLB_ST_MISS) {\r\ner = kvm_mips_emulate_tlbmiss_st(cause, opc, run, vcpu);\r\n} else {\r\nprintk("%s: invalid exc code: %d\n", __func__, exccode);\r\ner = EMULATE_FAIL;\r\n}\r\n} else {\r\nstruct kvm_mips_tlb *tlb = &vcpu->arch.guest_tlb[index];\r\nif (!TLB_IS_VALID(*tlb, va)) {\r\nif (exccode == T_TLB_LD_MISS) {\r\ner = kvm_mips_emulate_tlbinv_ld(cause, opc, run,\r\nvcpu);\r\n} else if (exccode == T_TLB_ST_MISS) {\r\ner = kvm_mips_emulate_tlbinv_st(cause, opc, run,\r\nvcpu);\r\n} else {\r\nprintk("%s: invalid exc code: %d\n", __func__,\r\nexccode);\r\ner = EMULATE_FAIL;\r\n}\r\n} else {\r\n#ifdef DEBUG\r\nkvm_debug\r\n("Injecting hi: %#lx, lo0: %#lx, lo1: %#lx into shadow host TLB\n",\r\ntlb->tlb_hi, tlb->tlb_lo0, tlb->tlb_lo1);\r\n#endif\r\nkvm_mips_handle_mapped_seg_tlb_fault(vcpu, tlb, NULL,\r\nNULL);\r\n}\r\n}\r\nreturn er;\r\n}
