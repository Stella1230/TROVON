static inline void dma_sync_desc_cpu(struct net_device *dev, void *addr)\r\n{\r\ndma_cache_sync(dev->dev.parent, addr, sizeof(struct sgiseeq_rx_desc),\r\nDMA_FROM_DEVICE);\r\n}\r\nstatic inline void dma_sync_desc_dev(struct net_device *dev, void *addr)\r\n{\r\ndma_cache_sync(dev->dev.parent, addr, sizeof(struct sgiseeq_rx_desc),\r\nDMA_TO_DEVICE);\r\n}\r\nstatic inline void hpc3_eth_reset(struct hpc3_ethregs *hregs)\r\n{\r\nhregs->reset = HPC3_ERST_CRESET | HPC3_ERST_CLRIRQ;\r\nudelay(20);\r\nhregs->reset = 0;\r\n}\r\nstatic inline void reset_hpc3_and_seeq(struct hpc3_ethregs *hregs,\r\nstruct sgiseeq_regs *sregs)\r\n{\r\nhregs->rx_ctrl = hregs->tx_ctrl = 0;\r\nhpc3_eth_reset(hregs);\r\n}\r\nstatic inline void seeq_go(struct sgiseeq_private *sp,\r\nstruct hpc3_ethregs *hregs,\r\nstruct sgiseeq_regs *sregs)\r\n{\r\nsregs->rstat = sp->mode | RSTAT_GO_BITS;\r\nhregs->rx_ctrl = HPC3_ERXCTRL_ACTIVE;\r\n}\r\nstatic inline void __sgiseeq_set_mac_address(struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nstruct sgiseeq_regs *sregs = sp->sregs;\r\nint i;\r\nsregs->tstat = SEEQ_TCMD_RB0;\r\nfor (i = 0; i < 6; i++)\r\nsregs->rw.eth_addr[i] = dev->dev_addr[i];\r\n}\r\nstatic int sgiseeq_set_mac_address(struct net_device *dev, void *addr)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nstruct sockaddr *sa = addr;\r\nmemcpy(dev->dev_addr, sa->sa_data, dev->addr_len);\r\nspin_lock_irq(&sp->tx_lock);\r\n__sgiseeq_set_mac_address(dev);\r\nspin_unlock_irq(&sp->tx_lock);\r\nreturn 0;\r\n}\r\nstatic int seeq_init_ring(struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nint i;\r\nnetif_stop_queue(dev);\r\nsp->rx_new = sp->tx_new = 0;\r\nsp->rx_old = sp->tx_old = 0;\r\n__sgiseeq_set_mac_address(dev);\r\nfor(i = 0; i < SEEQ_TX_BUFFERS; i++) {\r\nsp->tx_desc[i].tdma.cntinfo = TCNTINFO_INIT;\r\ndma_sync_desc_dev(dev, &sp->tx_desc[i]);\r\n}\r\nfor (i = 0; i < SEEQ_RX_BUFFERS; i++) {\r\nif (!sp->rx_desc[i].skb) {\r\ndma_addr_t dma_addr;\r\nstruct sk_buff *skb = netdev_alloc_skb(dev, PKT_BUF_SZ);\r\nif (skb == NULL)\r\nreturn -ENOMEM;\r\nskb_reserve(skb, 2);\r\ndma_addr = dma_map_single(dev->dev.parent,\r\nskb->data - 2,\r\nPKT_BUF_SZ, DMA_FROM_DEVICE);\r\nsp->rx_desc[i].skb = skb;\r\nsp->rx_desc[i].rdma.pbuf = dma_addr;\r\n}\r\nsp->rx_desc[i].rdma.cntinfo = RCNTINFO_INIT;\r\ndma_sync_desc_dev(dev, &sp->rx_desc[i]);\r\n}\r\nsp->rx_desc[i - 1].rdma.cntinfo |= HPCDMA_EOR;\r\ndma_sync_desc_dev(dev, &sp->rx_desc[i - 1]);\r\nreturn 0;\r\n}\r\nstatic void seeq_purge_ring(struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nint i;\r\nfor (i = 0; i < SEEQ_TX_BUFFERS; i++) {\r\nif (sp->tx_desc[i].skb) {\r\ndev_kfree_skb(sp->tx_desc[i].skb);\r\nsp->tx_desc[i].skb = NULL;\r\n}\r\n}\r\nfor (i = 0; i < SEEQ_RX_BUFFERS; i++) {\r\nif (sp->rx_desc[i].skb) {\r\ndev_kfree_skb(sp->rx_desc[i].skb);\r\nsp->rx_desc[i].skb = NULL;\r\n}\r\n}\r\n}\r\nstatic void sgiseeq_dump_rings(void)\r\n{\r\nstatic int once;\r\nstruct sgiseeq_rx_desc *r = gpriv->rx_desc;\r\nstruct sgiseeq_tx_desc *t = gpriv->tx_desc;\r\nstruct hpc3_ethregs *hregs = gpriv->hregs;\r\nint i;\r\nif (once)\r\nreturn;\r\nonce++;\r\nprintk("RING DUMP:\n");\r\nfor (i = 0; i < SEEQ_RX_BUFFERS; i++) {\r\nprintk("RX [%d]: @(%p) [%08x,%08x,%08x] ",\r\ni, (&r[i]), r[i].rdma.pbuf, r[i].rdma.cntinfo,\r\nr[i].rdma.pnext);\r\ni += 1;\r\nprintk("-- [%d]: @(%p) [%08x,%08x,%08x]\n",\r\ni, (&r[i]), r[i].rdma.pbuf, r[i].rdma.cntinfo,\r\nr[i].rdma.pnext);\r\n}\r\nfor (i = 0; i < SEEQ_TX_BUFFERS; i++) {\r\nprintk("TX [%d]: @(%p) [%08x,%08x,%08x] ",\r\ni, (&t[i]), t[i].tdma.pbuf, t[i].tdma.cntinfo,\r\nt[i].tdma.pnext);\r\ni += 1;\r\nprintk("-- [%d]: @(%p) [%08x,%08x,%08x]\n",\r\ni, (&t[i]), t[i].tdma.pbuf, t[i].tdma.cntinfo,\r\nt[i].tdma.pnext);\r\n}\r\nprintk("INFO: [rx_new = %d rx_old=%d] [tx_new = %d tx_old = %d]\n",\r\ngpriv->rx_new, gpriv->rx_old, gpriv->tx_new, gpriv->tx_old);\r\nprintk("RREGS: rx_cbptr[%08x] rx_ndptr[%08x] rx_ctrl[%08x]\n",\r\nhregs->rx_cbptr, hregs->rx_ndptr, hregs->rx_ctrl);\r\nprintk("TREGS: tx_cbptr[%08x] tx_ndptr[%08x] tx_ctrl[%08x]\n",\r\nhregs->tx_cbptr, hregs->tx_ndptr, hregs->tx_ctrl);\r\n}\r\nstatic int init_seeq(struct net_device *dev, struct sgiseeq_private *sp,\r\nstruct sgiseeq_regs *sregs)\r\n{\r\nstruct hpc3_ethregs *hregs = sp->hregs;\r\nint err;\r\nreset_hpc3_and_seeq(hregs, sregs);\r\nerr = seeq_init_ring(dev);\r\nif (err)\r\nreturn err;\r\nif (sp->is_edlc) {\r\nsregs->tstat = TSTAT_INIT_EDLC;\r\nsregs->rw.wregs.control = sp->control;\r\nsregs->rw.wregs.frame_gap = 0;\r\n} else {\r\nsregs->tstat = TSTAT_INIT_SEEQ;\r\n}\r\nhregs->rx_ndptr = VIRT_TO_DMA(sp, sp->rx_desc);\r\nhregs->tx_ndptr = VIRT_TO_DMA(sp, sp->tx_desc);\r\nseeq_go(sp, hregs, sregs);\r\nreturn 0;\r\n}\r\nstatic void record_rx_errors(struct net_device *dev, unsigned char status)\r\n{\r\nif (status & SEEQ_RSTAT_OVERF ||\r\nstatus & SEEQ_RSTAT_SFRAME)\r\ndev->stats.rx_over_errors++;\r\nif (status & SEEQ_RSTAT_CERROR)\r\ndev->stats.rx_crc_errors++;\r\nif (status & SEEQ_RSTAT_DERROR)\r\ndev->stats.rx_frame_errors++;\r\nif (status & SEEQ_RSTAT_REOF)\r\ndev->stats.rx_errors++;\r\n}\r\nstatic inline void rx_maybe_restart(struct sgiseeq_private *sp,\r\nstruct hpc3_ethregs *hregs,\r\nstruct sgiseeq_regs *sregs)\r\n{\r\nif (!(hregs->rx_ctrl & HPC3_ERXCTRL_ACTIVE)) {\r\nhregs->rx_ndptr = VIRT_TO_DMA(sp, sp->rx_desc + sp->rx_new);\r\nseeq_go(sp, hregs, sregs);\r\n}\r\n}\r\nstatic inline void sgiseeq_rx(struct net_device *dev, struct sgiseeq_private *sp,\r\nstruct hpc3_ethregs *hregs,\r\nstruct sgiseeq_regs *sregs)\r\n{\r\nstruct sgiseeq_rx_desc *rd;\r\nstruct sk_buff *skb = NULL;\r\nstruct sk_buff *newskb;\r\nunsigned char pkt_status;\r\nint len = 0;\r\nunsigned int orig_end = PREV_RX(sp->rx_new);\r\nrd = &sp->rx_desc[sp->rx_new];\r\ndma_sync_desc_cpu(dev, rd);\r\nwhile (!(rd->rdma.cntinfo & HPCDMA_OWN)) {\r\nlen = PKT_BUF_SZ - (rd->rdma.cntinfo & HPCDMA_BCNT) - 3;\r\ndma_unmap_single(dev->dev.parent, rd->rdma.pbuf,\r\nPKT_BUF_SZ, DMA_FROM_DEVICE);\r\npkt_status = rd->skb->data[len];\r\nif (pkt_status & SEEQ_RSTAT_FIG) {\r\nif (memcmp(rd->skb->data + 6, dev->dev_addr, ETH_ALEN)) {\r\nif (len > rx_copybreak) {\r\nskb = rd->skb;\r\nnewskb = netdev_alloc_skb(dev, PKT_BUF_SZ);\r\nif (!newskb) {\r\nnewskb = skb;\r\nskb = NULL;\r\ngoto memory_squeeze;\r\n}\r\nskb_reserve(newskb, 2);\r\n} else {\r\nskb = netdev_alloc_skb_ip_align(dev, len);\r\nif (skb)\r\nskb_copy_to_linear_data(skb, rd->skb->data, len);\r\nnewskb = rd->skb;\r\n}\r\nmemory_squeeze:\r\nif (skb) {\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nnetif_rx(skb);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += len;\r\n} else {\r\ndev->stats.rx_dropped++;\r\n}\r\n} else {\r\nnewskb = rd->skb;\r\n}\r\n} else {\r\nrecord_rx_errors(dev, pkt_status);\r\nnewskb = rd->skb;\r\n}\r\nrd->skb = newskb;\r\nrd->rdma.pbuf = dma_map_single(dev->dev.parent,\r\nnewskb->data - 2,\r\nPKT_BUF_SZ, DMA_FROM_DEVICE);\r\nrd->rdma.cntinfo = RCNTINFO_INIT;\r\nsp->rx_new = NEXT_RX(sp->rx_new);\r\ndma_sync_desc_dev(dev, rd);\r\nrd = &sp->rx_desc[sp->rx_new];\r\ndma_sync_desc_cpu(dev, rd);\r\n}\r\ndma_sync_desc_cpu(dev, &sp->rx_desc[orig_end]);\r\nsp->rx_desc[orig_end].rdma.cntinfo &= ~(HPCDMA_EOR);\r\ndma_sync_desc_dev(dev, &sp->rx_desc[orig_end]);\r\ndma_sync_desc_cpu(dev, &sp->rx_desc[PREV_RX(sp->rx_new)]);\r\nsp->rx_desc[PREV_RX(sp->rx_new)].rdma.cntinfo |= HPCDMA_EOR;\r\ndma_sync_desc_dev(dev, &sp->rx_desc[PREV_RX(sp->rx_new)]);\r\nrx_maybe_restart(sp, hregs, sregs);\r\n}\r\nstatic inline void tx_maybe_reset_collisions(struct sgiseeq_private *sp,\r\nstruct sgiseeq_regs *sregs)\r\n{\r\nif (sp->is_edlc) {\r\nsregs->rw.wregs.control = sp->control & ~(SEEQ_CTRL_XCNT);\r\nsregs->rw.wregs.control = sp->control;\r\n}\r\n}\r\nstatic inline void kick_tx(struct net_device *dev,\r\nstruct sgiseeq_private *sp,\r\nstruct hpc3_ethregs *hregs)\r\n{\r\nstruct sgiseeq_tx_desc *td;\r\nint i = sp->tx_old;\r\ntd = &sp->tx_desc[i];\r\ndma_sync_desc_cpu(dev, td);\r\nwhile ((td->tdma.cntinfo & (HPCDMA_XIU | HPCDMA_ETXD)) ==\r\n(HPCDMA_XIU | HPCDMA_ETXD)) {\r\ni = NEXT_TX(i);\r\ntd = &sp->tx_desc[i];\r\ndma_sync_desc_cpu(dev, td);\r\n}\r\nif (td->tdma.cntinfo & HPCDMA_XIU) {\r\nhregs->tx_ndptr = VIRT_TO_DMA(sp, td);\r\nhregs->tx_ctrl = HPC3_ETXCTRL_ACTIVE;\r\n}\r\n}\r\nstatic inline void sgiseeq_tx(struct net_device *dev, struct sgiseeq_private *sp,\r\nstruct hpc3_ethregs *hregs,\r\nstruct sgiseeq_regs *sregs)\r\n{\r\nstruct sgiseeq_tx_desc *td;\r\nunsigned long status = hregs->tx_ctrl;\r\nint j;\r\ntx_maybe_reset_collisions(sp, sregs);\r\nif (!(status & (HPC3_ETXCTRL_ACTIVE | SEEQ_TSTAT_PTRANS))) {\r\nif (status & SEEQ_TSTAT_R16)\r\ndev->stats.tx_aborted_errors++;\r\nif (status & SEEQ_TSTAT_UFLOW)\r\ndev->stats.tx_fifo_errors++;\r\nif (status & SEEQ_TSTAT_LCLS)\r\ndev->stats.collisions++;\r\n}\r\nfor (j = sp->tx_old; j != sp->tx_new; j = NEXT_TX(j)) {\r\ntd = &sp->tx_desc[j];\r\ndma_sync_desc_cpu(dev, td);\r\nif (!(td->tdma.cntinfo & (HPCDMA_XIU)))\r\nbreak;\r\nif (!(td->tdma.cntinfo & (HPCDMA_ETXD))) {\r\nif (!(status & HPC3_ETXCTRL_ACTIVE)) {\r\nhregs->tx_ndptr = VIRT_TO_DMA(sp, td);\r\nhregs->tx_ctrl = HPC3_ETXCTRL_ACTIVE;\r\n}\r\nbreak;\r\n}\r\ndev->stats.tx_packets++;\r\nsp->tx_old = NEXT_TX(sp->tx_old);\r\ntd->tdma.cntinfo &= ~(HPCDMA_XIU | HPCDMA_XIE);\r\ntd->tdma.cntinfo |= HPCDMA_EOX;\r\nif (td->skb) {\r\ndev_kfree_skb_any(td->skb);\r\ntd->skb = NULL;\r\n}\r\ndma_sync_desc_dev(dev, td);\r\n}\r\n}\r\nstatic irqreturn_t sgiseeq_interrupt(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = (struct net_device *) dev_id;\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nstruct hpc3_ethregs *hregs = sp->hregs;\r\nstruct sgiseeq_regs *sregs = sp->sregs;\r\nspin_lock(&sp->tx_lock);\r\nhregs->reset = HPC3_ERST_CLRIRQ;\r\nsgiseeq_rx(dev, sp, hregs, sregs);\r\nif (sp->tx_old != sp->tx_new)\r\nsgiseeq_tx(dev, sp, hregs, sregs);\r\nif ((TX_BUFFS_AVAIL(sp) > 0) && netif_queue_stopped(dev)) {\r\nnetif_wake_queue(dev);\r\n}\r\nspin_unlock(&sp->tx_lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int sgiseeq_open(struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nstruct sgiseeq_regs *sregs = sp->sregs;\r\nunsigned int irq = dev->irq;\r\nint err;\r\nif (request_irq(irq, sgiseeq_interrupt, 0, sgiseeqstr, dev)) {\r\nprintk(KERN_ERR "Seeq8003: Can't get irq %d\n", dev->irq);\r\nreturn -EAGAIN;\r\n}\r\nerr = init_seeq(dev, sp, sregs);\r\nif (err)\r\ngoto out_free_irq;\r\nnetif_start_queue(dev);\r\nreturn 0;\r\nout_free_irq:\r\nfree_irq(irq, dev);\r\nreturn err;\r\n}\r\nstatic int sgiseeq_close(struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nstruct sgiseeq_regs *sregs = sp->sregs;\r\nunsigned int irq = dev->irq;\r\nnetif_stop_queue(dev);\r\nreset_hpc3_and_seeq(sp->hregs, sregs);\r\nfree_irq(irq, dev);\r\nseeq_purge_ring(dev);\r\nreturn 0;\r\n}\r\nstatic inline int sgiseeq_reset(struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nstruct sgiseeq_regs *sregs = sp->sregs;\r\nint err;\r\nerr = init_seeq(dev, sp, sregs);\r\nif (err)\r\nreturn err;\r\ndev->trans_start = jiffies;\r\nnetif_wake_queue(dev);\r\nreturn 0;\r\n}\r\nstatic int sgiseeq_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nstruct hpc3_ethregs *hregs = sp->hregs;\r\nunsigned long flags;\r\nstruct sgiseeq_tx_desc *td;\r\nint len, entry;\r\nspin_lock_irqsave(&sp->tx_lock, flags);\r\nlen = skb->len;\r\nif (len < ETH_ZLEN) {\r\nif (skb_padto(skb, ETH_ZLEN)) {\r\nspin_unlock_irqrestore(&sp->tx_lock, flags);\r\nreturn NETDEV_TX_OK;\r\n}\r\nlen = ETH_ZLEN;\r\n}\r\ndev->stats.tx_bytes += len;\r\nentry = sp->tx_new;\r\ntd = &sp->tx_desc[entry];\r\ndma_sync_desc_cpu(dev, td);\r\ntd->skb = skb;\r\ntd->tdma.pbuf = dma_map_single(dev->dev.parent, skb->data,\r\nlen, DMA_TO_DEVICE);\r\ntd->tdma.cntinfo = (len & HPCDMA_BCNT) |\r\nHPCDMA_XIU | HPCDMA_EOXP | HPCDMA_XIE | HPCDMA_EOX;\r\ndma_sync_desc_dev(dev, td);\r\nif (sp->tx_old != sp->tx_new) {\r\nstruct sgiseeq_tx_desc *backend;\r\nbackend = &sp->tx_desc[PREV_TX(sp->tx_new)];\r\ndma_sync_desc_cpu(dev, backend);\r\nbackend->tdma.cntinfo &= ~HPCDMA_EOX;\r\ndma_sync_desc_dev(dev, backend);\r\n}\r\nsp->tx_new = NEXT_TX(sp->tx_new);\r\nif (!(hregs->tx_ctrl & HPC3_ETXCTRL_ACTIVE))\r\nkick_tx(dev, sp, hregs);\r\nif (!TX_BUFFS_AVAIL(sp))\r\nnetif_stop_queue(dev);\r\nspin_unlock_irqrestore(&sp->tx_lock, flags);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void timeout(struct net_device *dev)\r\n{\r\nprintk(KERN_NOTICE "%s: transmit timed out, resetting\n", dev->name);\r\nsgiseeq_reset(dev);\r\ndev->trans_start = jiffies;\r\nnetif_wake_queue(dev);\r\n}\r\nstatic void sgiseeq_set_multicast(struct net_device *dev)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nunsigned char oldmode = sp->mode;\r\nif(dev->flags & IFF_PROMISC)\r\nsp->mode = SEEQ_RCMD_RANY;\r\nelse if ((dev->flags & IFF_ALLMULTI) || !netdev_mc_empty(dev))\r\nsp->mode = SEEQ_RCMD_RBMCAST;\r\nelse\r\nsp->mode = SEEQ_RCMD_RBCAST;\r\nif (oldmode != sp->mode)\r\nsgiseeq_reset(dev);\r\n}\r\nstatic inline void setup_tx_ring(struct net_device *dev,\r\nstruct sgiseeq_tx_desc *buf,\r\nint nbufs)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nint i = 0;\r\nwhile (i < (nbufs - 1)) {\r\nbuf[i].tdma.pnext = VIRT_TO_DMA(sp, buf + i + 1);\r\nbuf[i].tdma.pbuf = 0;\r\ndma_sync_desc_dev(dev, &buf[i]);\r\ni++;\r\n}\r\nbuf[i].tdma.pnext = VIRT_TO_DMA(sp, buf);\r\ndma_sync_desc_dev(dev, &buf[i]);\r\n}\r\nstatic inline void setup_rx_ring(struct net_device *dev,\r\nstruct sgiseeq_rx_desc *buf,\r\nint nbufs)\r\n{\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nint i = 0;\r\nwhile (i < (nbufs - 1)) {\r\nbuf[i].rdma.pnext = VIRT_TO_DMA(sp, buf + i + 1);\r\nbuf[i].rdma.pbuf = 0;\r\ndma_sync_desc_dev(dev, &buf[i]);\r\ni++;\r\n}\r\nbuf[i].rdma.pbuf = 0;\r\nbuf[i].rdma.pnext = VIRT_TO_DMA(sp, buf);\r\ndma_sync_desc_dev(dev, &buf[i]);\r\n}\r\nstatic int sgiseeq_probe(struct platform_device *pdev)\r\n{\r\nstruct sgiseeq_platform_data *pd = dev_get_platdata(&pdev->dev);\r\nstruct hpc3_regs *hpcregs = pd->hpc;\r\nstruct sgiseeq_init_block *sr;\r\nunsigned int irq = pd->irq;\r\nstruct sgiseeq_private *sp;\r\nstruct net_device *dev;\r\nint err;\r\ndev = alloc_etherdev(sizeof (struct sgiseeq_private));\r\nif (!dev) {\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\nplatform_set_drvdata(pdev, dev);\r\nsp = netdev_priv(dev);\r\nsr = dma_alloc_noncoherent(&pdev->dev, sizeof(*sp->srings),\r\n&sp->srings_dma, GFP_KERNEL);\r\nif (!sr) {\r\nprintk(KERN_ERR "Sgiseeq: Page alloc failed, aborting.\n");\r\nerr = -ENOMEM;\r\ngoto err_out_free_dev;\r\n}\r\nsp->srings = sr;\r\nsp->rx_desc = sp->srings->rxvector;\r\nsp->tx_desc = sp->srings->txvector;\r\nspin_lock_init(&sp->tx_lock);\r\nsetup_rx_ring(dev, sp->rx_desc, SEEQ_RX_BUFFERS);\r\nsetup_tx_ring(dev, sp->tx_desc, SEEQ_TX_BUFFERS);\r\nmemcpy(dev->dev_addr, pd->mac, ETH_ALEN);\r\n#ifdef DEBUG\r\ngpriv = sp;\r\ngdev = dev;\r\n#endif\r\nsp->sregs = (struct sgiseeq_regs *) &hpcregs->eth_ext[0];\r\nsp->hregs = &hpcregs->ethregs;\r\nsp->name = sgiseeqstr;\r\nsp->mode = SEEQ_RCMD_RBCAST;\r\nsp->hregs->pconfig = 0x161;\r\nsp->hregs->dconfig = HPC3_EDCFG_FIRQ | HPC3_EDCFG_FEOP |\r\nHPC3_EDCFG_FRXDC | HPC3_EDCFG_PTO | 0x026;\r\nsp->hregs->pconfig = 0x161;\r\nsp->hregs->dconfig = HPC3_EDCFG_FIRQ | HPC3_EDCFG_FEOP |\r\nHPC3_EDCFG_FRXDC | HPC3_EDCFG_PTO | 0x026;\r\nhpc3_eth_reset(sp->hregs);\r\nsp->is_edlc = !(sp->sregs->rw.rregs.collision_tx[0] & 0xff);\r\nif (sp->is_edlc)\r\nsp->control = SEEQ_CTRL_XCNT | SEEQ_CTRL_ACCNT |\r\nSEEQ_CTRL_SFLAG | SEEQ_CTRL_ESHORT |\r\nSEEQ_CTRL_ENCARR;\r\ndev->netdev_ops = &sgiseeq_netdev_ops;\r\ndev->watchdog_timeo = (200 * HZ) / 1000;\r\ndev->irq = irq;\r\nif (register_netdev(dev)) {\r\nprintk(KERN_ERR "Sgiseeq: Cannot register net device, "\r\n"aborting.\n");\r\nerr = -ENODEV;\r\ngoto err_out_free_page;\r\n}\r\nprintk(KERN_INFO "%s: %s %pM\n", dev->name, sgiseeqstr, dev->dev_addr);\r\nreturn 0;\r\nerr_out_free_page:\r\nfree_page((unsigned long) sp->srings);\r\nerr_out_free_dev:\r\nfree_netdev(dev);\r\nerr_out:\r\nreturn err;\r\n}\r\nstatic int __exit sgiseeq_remove(struct platform_device *pdev)\r\n{\r\nstruct net_device *dev = platform_get_drvdata(pdev);\r\nstruct sgiseeq_private *sp = netdev_priv(dev);\r\nunregister_netdev(dev);\r\ndma_free_noncoherent(&pdev->dev, sizeof(*sp->srings), sp->srings,\r\nsp->srings_dma);\r\nfree_netdev(dev);\r\nreturn 0;\r\n}
