static inline void pat_disable(const char *reason)\r\n{\r\npat_enabled = 0;\r\nprintk(KERN_INFO "%s\n", reason);\r\n}\r\nstatic int __init nopat(char *str)\r\n{\r\npat_disable("PAT support disabled.");\r\nreturn 0;\r\n}\r\nstatic inline void pat_disable(const char *reason)\r\n{\r\n(void)reason;\r\n}\r\nstatic int __init pat_debug_setup(char *str)\r\n{\r\npat_debug_enable = 1;\r\nreturn 0;\r\n}\r\nvoid pat_init(void)\r\n{\r\nu64 pat;\r\nbool boot_cpu = !boot_pat_state;\r\nif (!pat_enabled)\r\nreturn;\r\nif (!cpu_has_pat) {\r\nif (!boot_pat_state) {\r\npat_disable("PAT not supported by CPU.");\r\nreturn;\r\n} else {\r\nprintk(KERN_ERR "PAT enabled, "\r\n"but not supported by secondary CPU\n");\r\nBUG();\r\n}\r\n}\r\npat = PAT(0, WB) | PAT(1, WC) | PAT(2, UC_MINUS) | PAT(3, UC) |\r\nPAT(4, WB) | PAT(5, WC) | PAT(6, UC_MINUS) | PAT(7, UC);\r\nif (!boot_pat_state)\r\nrdmsrl(MSR_IA32_CR_PAT, boot_pat_state);\r\nwrmsrl(MSR_IA32_CR_PAT, pat);\r\nif (boot_cpu)\r\nprintk(KERN_INFO "x86 PAT enabled: cpu %d, old 0x%Lx, new 0x%Lx\n",\r\nsmp_processor_id(), boot_pat_state, pat);\r\n}\r\nstatic unsigned long pat_x_mtrr_type(u64 start, u64 end, unsigned long req_type)\r\n{\r\nif (req_type == _PAGE_CACHE_WB) {\r\nu8 mtrr_type;\r\nmtrr_type = mtrr_type_lookup(start, end);\r\nif (mtrr_type != MTRR_TYPE_WRBACK)\r\nreturn _PAGE_CACHE_UC_MINUS;\r\nreturn _PAGE_CACHE_WB;\r\n}\r\nreturn req_type;\r\n}\r\nstatic int\r\npagerange_is_ram_callback(unsigned long initial_pfn, unsigned long total_nr_pages, void *arg)\r\n{\r\nstruct pagerange_state *state = arg;\r\nstate->not_ram |= initial_pfn > state->cur_pfn;\r\nstate->ram |= total_nr_pages > 0;\r\nstate->cur_pfn = initial_pfn + total_nr_pages;\r\nreturn state->ram && state->not_ram;\r\n}\r\nstatic int pat_pagerange_is_ram(resource_size_t start, resource_size_t end)\r\n{\r\nint ret = 0;\r\nunsigned long start_pfn = start >> PAGE_SHIFT;\r\nunsigned long end_pfn = (end + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nstruct pagerange_state state = {start_pfn, 0, 0};\r\nif (start_pfn < ISA_END_ADDRESS >> PAGE_SHIFT)\r\nstart_pfn = ISA_END_ADDRESS >> PAGE_SHIFT;\r\nif (start_pfn < end_pfn) {\r\nret = walk_system_ram_range(start_pfn, end_pfn - start_pfn,\r\n&state, pagerange_is_ram_callback);\r\n}\r\nreturn (ret > 0) ? -1 : (state.ram ? 1 : 0);\r\n}\r\nstatic int reserve_ram_pages_type(u64 start, u64 end, unsigned long req_type,\r\nunsigned long *new_type)\r\n{\r\nstruct page *page;\r\nu64 pfn;\r\nif (req_type == _PAGE_CACHE_UC) {\r\nWARN_ON_ONCE(1);\r\nreq_type = _PAGE_CACHE_UC_MINUS;\r\n}\r\nfor (pfn = (start >> PAGE_SHIFT); pfn < (end >> PAGE_SHIFT); ++pfn) {\r\nunsigned long type;\r\npage = pfn_to_page(pfn);\r\ntype = get_page_memtype(page);\r\nif (type != -1) {\r\nprintk(KERN_INFO "reserve_ram_pages_type failed [mem %#010Lx-%#010Lx], track 0x%lx, req 0x%lx\n",\r\nstart, end - 1, type, req_type);\r\nif (new_type)\r\n*new_type = type;\r\nreturn -EBUSY;\r\n}\r\n}\r\nif (new_type)\r\n*new_type = req_type;\r\nfor (pfn = (start >> PAGE_SHIFT); pfn < (end >> PAGE_SHIFT); ++pfn) {\r\npage = pfn_to_page(pfn);\r\nset_page_memtype(page, req_type);\r\n}\r\nreturn 0;\r\n}\r\nstatic int free_ram_pages_type(u64 start, u64 end)\r\n{\r\nstruct page *page;\r\nu64 pfn;\r\nfor (pfn = (start >> PAGE_SHIFT); pfn < (end >> PAGE_SHIFT); ++pfn) {\r\npage = pfn_to_page(pfn);\r\nset_page_memtype(page, -1);\r\n}\r\nreturn 0;\r\n}\r\nint reserve_memtype(u64 start, u64 end, unsigned long req_type,\r\nunsigned long *new_type)\r\n{\r\nstruct memtype *new;\r\nunsigned long actual_type;\r\nint is_range_ram;\r\nint err = 0;\r\nBUG_ON(start >= end);\r\nif (!pat_enabled) {\r\nif (new_type) {\r\nif (req_type == _PAGE_CACHE_WC)\r\n*new_type = _PAGE_CACHE_UC_MINUS;\r\nelse\r\n*new_type = req_type & _PAGE_CACHE_MASK;\r\n}\r\nreturn 0;\r\n}\r\nif (x86_platform.is_untracked_pat_range(start, end)) {\r\nif (new_type)\r\n*new_type = _PAGE_CACHE_WB;\r\nreturn 0;\r\n}\r\nactual_type = pat_x_mtrr_type(start, end, req_type & _PAGE_CACHE_MASK);\r\nif (new_type)\r\n*new_type = actual_type;\r\nis_range_ram = pat_pagerange_is_ram(start, end);\r\nif (is_range_ram == 1) {\r\nerr = reserve_ram_pages_type(start, end, req_type, new_type);\r\nreturn err;\r\n} else if (is_range_ram < 0) {\r\nreturn -EINVAL;\r\n}\r\nnew = kzalloc(sizeof(struct memtype), GFP_KERNEL);\r\nif (!new)\r\nreturn -ENOMEM;\r\nnew->start = start;\r\nnew->end = end;\r\nnew->type = actual_type;\r\nspin_lock(&memtype_lock);\r\nerr = rbt_memtype_check_insert(new, new_type);\r\nif (err) {\r\nprintk(KERN_INFO "reserve_memtype failed [mem %#010Lx-%#010Lx], track %s, req %s\n",\r\nstart, end - 1,\r\ncattr_name(new->type), cattr_name(req_type));\r\nkfree(new);\r\nspin_unlock(&memtype_lock);\r\nreturn err;\r\n}\r\nspin_unlock(&memtype_lock);\r\ndprintk("reserve_memtype added [mem %#010Lx-%#010Lx], track %s, req %s, ret %s\n",\r\nstart, end - 1, cattr_name(new->type), cattr_name(req_type),\r\nnew_type ? cattr_name(*new_type) : "-");\r\nreturn err;\r\n}\r\nint free_memtype(u64 start, u64 end)\r\n{\r\nint err = -EINVAL;\r\nint is_range_ram;\r\nstruct memtype *entry;\r\nif (!pat_enabled)\r\nreturn 0;\r\nif (x86_platform.is_untracked_pat_range(start, end))\r\nreturn 0;\r\nis_range_ram = pat_pagerange_is_ram(start, end);\r\nif (is_range_ram == 1) {\r\nerr = free_ram_pages_type(start, end);\r\nreturn err;\r\n} else if (is_range_ram < 0) {\r\nreturn -EINVAL;\r\n}\r\nspin_lock(&memtype_lock);\r\nentry = rbt_memtype_erase(start, end);\r\nspin_unlock(&memtype_lock);\r\nif (!entry) {\r\nprintk(KERN_INFO "%s:%d freeing invalid memtype [mem %#010Lx-%#010Lx]\n",\r\ncurrent->comm, current->pid, start, end - 1);\r\nreturn -EINVAL;\r\n}\r\nkfree(entry);\r\ndprintk("free_memtype request [mem %#010Lx-%#010Lx]\n", start, end - 1);\r\nreturn 0;\r\n}\r\nstatic unsigned long lookup_memtype(u64 paddr)\r\n{\r\nint rettype = _PAGE_CACHE_WB;\r\nstruct memtype *entry;\r\nif (x86_platform.is_untracked_pat_range(paddr, paddr + PAGE_SIZE))\r\nreturn rettype;\r\nif (pat_pagerange_is_ram(paddr, paddr + PAGE_SIZE)) {\r\nstruct page *page;\r\npage = pfn_to_page(paddr >> PAGE_SHIFT);\r\nrettype = get_page_memtype(page);\r\nif (rettype == -1)\r\nrettype = _PAGE_CACHE_WB;\r\nreturn rettype;\r\n}\r\nspin_lock(&memtype_lock);\r\nentry = rbt_memtype_lookup(paddr);\r\nif (entry != NULL)\r\nrettype = entry->type;\r\nelse\r\nrettype = _PAGE_CACHE_UC_MINUS;\r\nspin_unlock(&memtype_lock);\r\nreturn rettype;\r\n}\r\nint io_reserve_memtype(resource_size_t start, resource_size_t end,\r\nunsigned long *type)\r\n{\r\nresource_size_t size = end - start;\r\nunsigned long req_type = *type;\r\nunsigned long new_type;\r\nint ret;\r\nWARN_ON_ONCE(iomem_map_sanity_check(start, size));\r\nret = reserve_memtype(start, end, req_type, &new_type);\r\nif (ret)\r\ngoto out_err;\r\nif (!is_new_memtype_allowed(start, size, req_type, new_type))\r\ngoto out_free;\r\nif (kernel_map_sync_memtype(start, size, new_type) < 0)\r\ngoto out_free;\r\n*type = new_type;\r\nreturn 0;\r\nout_free:\r\nfree_memtype(start, end);\r\nret = -EBUSY;\r\nout_err:\r\nreturn ret;\r\n}\r\nvoid io_free_memtype(resource_size_t start, resource_size_t end)\r\n{\r\nfree_memtype(start, end);\r\n}\r\npgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,\r\nunsigned long size, pgprot_t vma_prot)\r\n{\r\nreturn vma_prot;\r\n}\r\nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\r\n{\r\nreturn 1;\r\n}\r\nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\r\n{\r\nu64 from = ((u64)pfn) << PAGE_SHIFT;\r\nu64 to = from + size;\r\nu64 cursor = from;\r\nif (!pat_enabled)\r\nreturn 1;\r\nwhile (cursor < to) {\r\nif (!devmem_is_allowed(pfn)) {\r\nprintk(KERN_INFO "Program %s tried to access /dev/mem between [mem %#010Lx-%#010Lx]\n",\r\ncurrent->comm, from, to - 1);\r\nreturn 0;\r\n}\r\ncursor += PAGE_SIZE;\r\npfn++;\r\n}\r\nreturn 1;\r\n}\r\nint phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,\r\nunsigned long size, pgprot_t *vma_prot)\r\n{\r\nunsigned long flags = _PAGE_CACHE_WB;\r\nif (!range_is_allowed(pfn, size))\r\nreturn 0;\r\nif (file->f_flags & O_DSYNC)\r\nflags = _PAGE_CACHE_UC_MINUS;\r\n#ifdef CONFIG_X86_32\r\nif (!pat_enabled &&\r\n!(boot_cpu_has(X86_FEATURE_MTRR) ||\r\nboot_cpu_has(X86_FEATURE_K6_MTRR) ||\r\nboot_cpu_has(X86_FEATURE_CYRIX_ARR) ||\r\nboot_cpu_has(X86_FEATURE_CENTAUR_MCR)) &&\r\n(pfn << PAGE_SHIFT) >= __pa(high_memory)) {\r\nflags = _PAGE_CACHE_UC;\r\n}\r\n#endif\r\n*vma_prot = __pgprot((pgprot_val(*vma_prot) & ~_PAGE_CACHE_MASK) |\r\nflags);\r\nreturn 1;\r\n}\r\nint kernel_map_sync_memtype(u64 base, unsigned long size, unsigned long flags)\r\n{\r\nunsigned long id_sz;\r\nif (base > __pa(high_memory-1))\r\nreturn 0;\r\nif (!page_is_ram(base >> PAGE_SHIFT))\r\nreturn 0;\r\nid_sz = (__pa(high_memory-1) <= base + size) ?\r\n__pa(high_memory) - base :\r\nsize;\r\nif (ioremap_change_attr((unsigned long)__va(base), id_sz, flags) < 0) {\r\nprintk(KERN_INFO "%s:%d ioremap_change_attr failed %s "\r\n"for [mem %#010Lx-%#010Lx]\n",\r\ncurrent->comm, current->pid,\r\ncattr_name(flags),\r\nbase, (unsigned long long)(base + size-1));\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int reserve_pfn_range(u64 paddr, unsigned long size, pgprot_t *vma_prot,\r\nint strict_prot)\r\n{\r\nint is_ram = 0;\r\nint ret;\r\nunsigned long want_flags = (pgprot_val(*vma_prot) & _PAGE_CACHE_MASK);\r\nunsigned long flags = want_flags;\r\nis_ram = pat_pagerange_is_ram(paddr, paddr + size);\r\nif (is_ram) {\r\nif (!pat_enabled)\r\nreturn 0;\r\nflags = lookup_memtype(paddr);\r\nif (want_flags != flags) {\r\nprintk(KERN_WARNING "%s:%d map pfn RAM range req %s for [mem %#010Lx-%#010Lx], got %s\n",\r\ncurrent->comm, current->pid,\r\ncattr_name(want_flags),\r\n(unsigned long long)paddr,\r\n(unsigned long long)(paddr + size - 1),\r\ncattr_name(flags));\r\n*vma_prot = __pgprot((pgprot_val(*vma_prot) &\r\n(~_PAGE_CACHE_MASK)) |\r\nflags);\r\n}\r\nreturn 0;\r\n}\r\nret = reserve_memtype(paddr, paddr + size, want_flags, &flags);\r\nif (ret)\r\nreturn ret;\r\nif (flags != want_flags) {\r\nif (strict_prot ||\r\n!is_new_memtype_allowed(paddr, size, want_flags, flags)) {\r\nfree_memtype(paddr, paddr + size);\r\nprintk(KERN_ERR "%s:%d map pfn expected mapping type %s"\r\n" for [mem %#010Lx-%#010Lx], got %s\n",\r\ncurrent->comm, current->pid,\r\ncattr_name(want_flags),\r\n(unsigned long long)paddr,\r\n(unsigned long long)(paddr + size - 1),\r\ncattr_name(flags));\r\nreturn -EINVAL;\r\n}\r\n*vma_prot = __pgprot((pgprot_val(*vma_prot) &\r\n(~_PAGE_CACHE_MASK)) |\r\nflags);\r\n}\r\nif (kernel_map_sync_memtype(paddr, size, flags) < 0) {\r\nfree_memtype(paddr, paddr + size);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void free_pfn_range(u64 paddr, unsigned long size)\r\n{\r\nint is_ram;\r\nis_ram = pat_pagerange_is_ram(paddr, paddr + size);\r\nif (is_ram == 0)\r\nfree_memtype(paddr, paddr + size);\r\n}\r\nint track_pfn_copy(struct vm_area_struct *vma)\r\n{\r\nresource_size_t paddr;\r\nunsigned long prot;\r\nunsigned long vma_size = vma->vm_end - vma->vm_start;\r\npgprot_t pgprot;\r\nif (vma->vm_flags & VM_PAT) {\r\nif (follow_phys(vma, vma->vm_start, 0, &prot, &paddr)) {\r\nWARN_ON_ONCE(1);\r\nreturn -EINVAL;\r\n}\r\npgprot = __pgprot(prot);\r\nreturn reserve_pfn_range(paddr, vma_size, &pgprot, 1);\r\n}\r\nreturn 0;\r\n}\r\nint track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,\r\nunsigned long pfn, unsigned long addr, unsigned long size)\r\n{\r\nresource_size_t paddr = (resource_size_t)pfn << PAGE_SHIFT;\r\nunsigned long flags;\r\nif (addr == vma->vm_start && size == (vma->vm_end - vma->vm_start)) {\r\nint ret;\r\nret = reserve_pfn_range(paddr, size, prot, 0);\r\nif (!ret)\r\nvma->vm_flags |= VM_PAT;\r\nreturn ret;\r\n}\r\nif (!pat_enabled)\r\nreturn 0;\r\nflags = lookup_memtype(paddr);\r\nwhile (size > PAGE_SIZE) {\r\nsize -= PAGE_SIZE;\r\npaddr += PAGE_SIZE;\r\nif (flags != lookup_memtype(paddr))\r\nreturn -EINVAL;\r\n}\r\n*prot = __pgprot((pgprot_val(vma->vm_page_prot) & (~_PAGE_CACHE_MASK)) |\r\nflags);\r\nreturn 0;\r\n}\r\nint track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot,\r\nunsigned long pfn)\r\n{\r\nunsigned long flags;\r\nif (!pat_enabled)\r\nreturn 0;\r\nflags = lookup_memtype((resource_size_t)pfn << PAGE_SHIFT);\r\n*prot = __pgprot((pgprot_val(vma->vm_page_prot) & (~_PAGE_CACHE_MASK)) |\r\nflags);\r\nreturn 0;\r\n}\r\nvoid untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,\r\nunsigned long size)\r\n{\r\nresource_size_t paddr;\r\nunsigned long prot;\r\nif (!(vma->vm_flags & VM_PAT))\r\nreturn;\r\npaddr = (resource_size_t)pfn << PAGE_SHIFT;\r\nif (!paddr && !size) {\r\nif (follow_phys(vma, vma->vm_start, 0, &prot, &paddr)) {\r\nWARN_ON_ONCE(1);\r\nreturn;\r\n}\r\nsize = vma->vm_end - vma->vm_start;\r\n}\r\nfree_pfn_range(paddr, size);\r\nvma->vm_flags &= ~VM_PAT;\r\n}\r\npgprot_t pgprot_writecombine(pgprot_t prot)\r\n{\r\nif (pat_enabled)\r\nreturn __pgprot(pgprot_val(prot) | _PAGE_CACHE_WC);\r\nelse\r\nreturn pgprot_noncached(prot);\r\n}\r\nstatic struct memtype *memtype_get_idx(loff_t pos)\r\n{\r\nstruct memtype *print_entry;\r\nint ret;\r\nprint_entry = kzalloc(sizeof(struct memtype), GFP_KERNEL);\r\nif (!print_entry)\r\nreturn NULL;\r\nspin_lock(&memtype_lock);\r\nret = rbt_memtype_copy_nth_element(print_entry, pos);\r\nspin_unlock(&memtype_lock);\r\nif (!ret) {\r\nreturn print_entry;\r\n} else {\r\nkfree(print_entry);\r\nreturn NULL;\r\n}\r\n}\r\nstatic void *memtype_seq_start(struct seq_file *seq, loff_t *pos)\r\n{\r\nif (*pos == 0) {\r\n++*pos;\r\nseq_printf(seq, "PAT memtype list:\n");\r\n}\r\nreturn memtype_get_idx(*pos);\r\n}\r\nstatic void *memtype_seq_next(struct seq_file *seq, void *v, loff_t *pos)\r\n{\r\n++*pos;\r\nreturn memtype_get_idx(*pos);\r\n}\r\nstatic void memtype_seq_stop(struct seq_file *seq, void *v)\r\n{\r\n}\r\nstatic int memtype_seq_show(struct seq_file *seq, void *v)\r\n{\r\nstruct memtype *print_entry = (struct memtype *)v;\r\nseq_printf(seq, "%s @ 0x%Lx-0x%Lx\n", cattr_name(print_entry->type),\r\nprint_entry->start, print_entry->end);\r\nkfree(print_entry);\r\nreturn 0;\r\n}\r\nstatic int memtype_seq_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &memtype_seq_ops);\r\n}\r\nstatic int __init pat_memtype_list_init(void)\r\n{\r\nif (pat_enabled) {\r\ndebugfs_create_file("pat_memtype_list", S_IRUSR,\r\narch_debugfs_dir, NULL, &memtype_fops);\r\n}\r\nreturn 0;\r\n}
