static void hv_bring_pgs_online(unsigned long start_pfn, unsigned long size)\r\n{\r\nint i;\r\nfor (i = 0; i < size; i++) {\r\nstruct page *pg;\r\npg = pfn_to_page(start_pfn + i);\r\n__online_page_set_limits(pg);\r\n__online_page_increment_counters(pg);\r\n__online_page_free(pg);\r\n}\r\n}\r\nstatic void hv_mem_hot_add(unsigned long start, unsigned long size,\r\nunsigned long pfn_count,\r\nstruct hv_hotadd_state *has)\r\n{\r\nint ret = 0;\r\nint i, nid;\r\nunsigned long start_pfn;\r\nunsigned long processed_pfn;\r\nunsigned long total_pfn = pfn_count;\r\nfor (i = 0; i < (size/HA_CHUNK); i++) {\r\nstart_pfn = start + (i * HA_CHUNK);\r\nhas->ha_end_pfn += HA_CHUNK;\r\nif (total_pfn > HA_CHUNK) {\r\nprocessed_pfn = HA_CHUNK;\r\ntotal_pfn -= HA_CHUNK;\r\n} else {\r\nprocessed_pfn = total_pfn;\r\ntotal_pfn = 0;\r\n}\r\nhas->covered_end_pfn += processed_pfn;\r\ninit_completion(&dm_device.ol_waitevent);\r\ndm_device.ha_waiting = true;\r\nnid = memory_add_physaddr_to_nid(PFN_PHYS(start_pfn));\r\nret = add_memory(nid, PFN_PHYS((start_pfn)),\r\n(HA_CHUNK << PAGE_SHIFT));\r\nif (ret) {\r\npr_info("hot_add memory failed error is %d\n", ret);\r\nif (ret == -EEXIST) {\r\ndo_hot_add = false;\r\n}\r\nhas->ha_end_pfn -= HA_CHUNK;\r\nhas->covered_end_pfn -= processed_pfn;\r\nbreak;\r\n}\r\nwait_for_completion_timeout(&dm_device.ol_waitevent, 5*HZ);\r\n}\r\nreturn;\r\n}\r\nstatic void hv_online_page(struct page *pg)\r\n{\r\nstruct list_head *cur;\r\nstruct hv_hotadd_state *has;\r\nunsigned long cur_start_pgp;\r\nunsigned long cur_end_pgp;\r\nif (dm_device.ha_waiting) {\r\ndm_device.ha_waiting = false;\r\ncomplete(&dm_device.ol_waitevent);\r\n}\r\nlist_for_each(cur, &dm_device.ha_region_list) {\r\nhas = list_entry(cur, struct hv_hotadd_state, list);\r\ncur_start_pgp = (unsigned long)\r\npfn_to_page(has->covered_start_pfn);\r\ncur_end_pgp = (unsigned long)pfn_to_page(has->covered_end_pfn);\r\nif (((unsigned long)pg >= cur_start_pgp) &&\r\n((unsigned long)pg < cur_end_pgp)) {\r\n__online_page_set_limits(pg);\r\n__online_page_increment_counters(pg);\r\n__online_page_free(pg);\r\nhas->covered_start_pfn++;\r\n}\r\n}\r\n}\r\nstatic bool pfn_covered(unsigned long start_pfn, unsigned long pfn_cnt)\r\n{\r\nstruct list_head *cur;\r\nstruct hv_hotadd_state *has;\r\nunsigned long residual, new_inc;\r\nif (list_empty(&dm_device.ha_region_list))\r\nreturn false;\r\nlist_for_each(cur, &dm_device.ha_region_list) {\r\nhas = list_entry(cur, struct hv_hotadd_state, list);\r\nif ((start_pfn >= has->end_pfn))\r\ncontinue;\r\nif ((start_pfn + pfn_cnt) > has->end_pfn) {\r\nresidual = (start_pfn + pfn_cnt - has->end_pfn);\r\nnew_inc = (residual / HA_CHUNK) * HA_CHUNK;\r\nif (residual % HA_CHUNK)\r\nnew_inc += HA_CHUNK;\r\nhas->end_pfn += new_inc;\r\n}\r\nif (has->covered_end_pfn != start_pfn) {\r\nhas->covered_end_pfn = start_pfn;\r\nhas->covered_start_pfn = start_pfn;\r\n}\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic unsigned long handle_pg_range(unsigned long pg_start,\r\nunsigned long pg_count)\r\n{\r\nunsigned long start_pfn = pg_start;\r\nunsigned long pfn_cnt = pg_count;\r\nunsigned long size;\r\nstruct list_head *cur;\r\nstruct hv_hotadd_state *has;\r\nunsigned long pgs_ol = 0;\r\nunsigned long old_covered_state;\r\nif (list_empty(&dm_device.ha_region_list))\r\nreturn 0;\r\nlist_for_each(cur, &dm_device.ha_region_list) {\r\nhas = list_entry(cur, struct hv_hotadd_state, list);\r\nif ((start_pfn >= has->end_pfn))\r\ncontinue;\r\nold_covered_state = has->covered_end_pfn;\r\nif (start_pfn < has->ha_end_pfn) {\r\npgs_ol = has->ha_end_pfn - start_pfn;\r\nif (pgs_ol > pfn_cnt)\r\npgs_ol = pfn_cnt;\r\nhv_bring_pgs_online(start_pfn, pgs_ol);\r\nhas->covered_end_pfn += pgs_ol;\r\nhas->covered_start_pfn += pgs_ol;\r\npfn_cnt -= pgs_ol;\r\n}\r\nif ((has->ha_end_pfn < has->end_pfn) && (pfn_cnt > 0)) {\r\nsize = (has->end_pfn - has->ha_end_pfn);\r\nif (pfn_cnt <= size) {\r\nsize = ((pfn_cnt / HA_CHUNK) * HA_CHUNK);\r\nif (pfn_cnt % HA_CHUNK)\r\nsize += HA_CHUNK;\r\n} else {\r\npfn_cnt = size;\r\n}\r\nhv_mem_hot_add(has->ha_end_pfn, size, pfn_cnt, has);\r\n}\r\nreturn has->covered_end_pfn - old_covered_state;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned long process_hot_add(unsigned long pg_start,\r\nunsigned long pfn_cnt,\r\nunsigned long rg_start,\r\nunsigned long rg_size)\r\n{\r\nstruct hv_hotadd_state *ha_region = NULL;\r\nif (pfn_cnt == 0)\r\nreturn 0;\r\nif (!dm_device.host_specified_ha_region)\r\nif (pfn_covered(pg_start, pfn_cnt))\r\ngoto do_pg_range;\r\nif (rg_size != 0) {\r\nha_region = kzalloc(sizeof(struct hv_hotadd_state), GFP_KERNEL);\r\nif (!ha_region)\r\nreturn 0;\r\nINIT_LIST_HEAD(&ha_region->list);\r\nlist_add_tail(&ha_region->list, &dm_device.ha_region_list);\r\nha_region->start_pfn = rg_start;\r\nha_region->ha_end_pfn = rg_start;\r\nha_region->covered_start_pfn = pg_start;\r\nha_region->covered_end_pfn = pg_start;\r\nha_region->end_pfn = rg_start + rg_size;\r\n}\r\ndo_pg_range:\r\nreturn handle_pg_range(pg_start, pfn_cnt);\r\n}\r\nstatic void hot_add_req(struct work_struct *dummy)\r\n{\r\nstruct dm_hot_add_response resp;\r\n#ifdef CONFIG_MEMORY_HOTPLUG\r\nunsigned long pg_start, pfn_cnt;\r\nunsigned long rg_start, rg_sz;\r\n#endif\r\nstruct hv_dynmem_device *dm = &dm_device;\r\nmemset(&resp, 0, sizeof(struct dm_hot_add_response));\r\nresp.hdr.type = DM_MEM_HOT_ADD_RESPONSE;\r\nresp.hdr.size = sizeof(struct dm_hot_add_response);\r\n#ifdef CONFIG_MEMORY_HOTPLUG\r\npg_start = dm->ha_wrk.ha_page_range.finfo.start_page;\r\npfn_cnt = dm->ha_wrk.ha_page_range.finfo.page_cnt;\r\nrg_start = dm->ha_wrk.ha_region_range.finfo.start_page;\r\nrg_sz = dm->ha_wrk.ha_region_range.finfo.page_cnt;\r\nif ((rg_start == 0) && (!dm->host_specified_ha_region)) {\r\nunsigned long region_size;\r\nunsigned long region_start;\r\nregion_start = pg_start;\r\nregion_size = (pfn_cnt / HA_CHUNK) * HA_CHUNK;\r\nif (pfn_cnt % HA_CHUNK)\r\nregion_size += HA_CHUNK;\r\nregion_start = (pg_start / HA_CHUNK) * HA_CHUNK;\r\nrg_start = region_start;\r\nrg_sz = region_size;\r\n}\r\nif (do_hot_add)\r\nresp.page_count = process_hot_add(pg_start, pfn_cnt,\r\nrg_start, rg_sz);\r\n#endif\r\nif (resp.page_count > 0)\r\nresp.result = 1;\r\nelse if (!do_hot_add)\r\nresp.result = 1;\r\nelse\r\nresp.result = 0;\r\nif (!do_hot_add || (resp.page_count == 0))\r\npr_info("Memory hot add failed\n");\r\ndm->state = DM_INITIALIZED;\r\nresp.hdr.trans_id = atomic_inc_return(&trans_id);\r\nvmbus_sendpacket(dm->dev->channel, &resp,\r\nsizeof(struct dm_hot_add_response),\r\n(unsigned long)NULL,\r\nVM_PKT_DATA_INBAND, 0);\r\n}\r\nstatic void process_info(struct hv_dynmem_device *dm, struct dm_info_msg *msg)\r\n{\r\nstruct dm_info_header *info_hdr;\r\ninfo_hdr = (struct dm_info_header *)msg->info;\r\nswitch (info_hdr->type) {\r\ncase INFO_TYPE_MAX_PAGE_CNT:\r\npr_info("Received INFO_TYPE_MAX_PAGE_CNT\n");\r\npr_info("Data Size is %d\n", info_hdr->data_size);\r\nbreak;\r\ndefault:\r\npr_info("Received Unknown type: %d\n", info_hdr->type);\r\n}\r\n}\r\nstatic unsigned long compute_balloon_floor(void)\r\n{\r\nunsigned long min_pages;\r\n#define MB2PAGES(mb) ((mb) << (20 - PAGE_SHIFT))\r\nif (totalram_pages < MB2PAGES(128))\r\nmin_pages = MB2PAGES(8) + (totalram_pages >> 1);\r\nelse if (totalram_pages < MB2PAGES(512))\r\nmin_pages = MB2PAGES(40) + (totalram_pages >> 2);\r\nelse if (totalram_pages < MB2PAGES(2048))\r\nmin_pages = MB2PAGES(104) + (totalram_pages >> 3);\r\nelse\r\nmin_pages = MB2PAGES(296) + (totalram_pages >> 5);\r\n#undef MB2PAGES\r\nreturn min_pages;\r\n}\r\nstatic void post_status(struct hv_dynmem_device *dm)\r\n{\r\nstruct dm_status status;\r\nstruct sysinfo val;\r\nif (pressure_report_delay > 0) {\r\n--pressure_report_delay;\r\nreturn;\r\n}\r\nsi_meminfo(&val);\r\nmemset(&status, 0, sizeof(struct dm_status));\r\nstatus.hdr.type = DM_STATUS_REPORT;\r\nstatus.hdr.size = sizeof(struct dm_status);\r\nstatus.hdr.trans_id = atomic_inc_return(&trans_id);\r\nstatus.num_avail = val.freeram;\r\nstatus.num_committed = vm_memory_committed() +\r\ndm->num_pages_ballooned +\r\ncompute_balloon_floor();\r\nif (status.hdr.trans_id != atomic_read(&trans_id))\r\nreturn;\r\nvmbus_sendpacket(dm->dev->channel, &status,\r\nsizeof(struct dm_status),\r\n(unsigned long)NULL,\r\nVM_PKT_DATA_INBAND, 0);\r\n}\r\nstatic void free_balloon_pages(struct hv_dynmem_device *dm,\r\nunion dm_mem_page_range *range_array)\r\n{\r\nint num_pages = range_array->finfo.page_cnt;\r\n__u64 start_frame = range_array->finfo.start_page;\r\nstruct page *pg;\r\nint i;\r\nfor (i = 0; i < num_pages; i++) {\r\npg = pfn_to_page(i + start_frame);\r\n__free_page(pg);\r\ndm->num_pages_ballooned--;\r\n}\r\n}\r\nstatic int alloc_balloon_pages(struct hv_dynmem_device *dm, int num_pages,\r\nstruct dm_balloon_response *bl_resp, int alloc_unit,\r\nbool *alloc_error)\r\n{\r\nint i = 0;\r\nstruct page *pg;\r\nif (num_pages < alloc_unit)\r\nreturn 0;\r\nfor (i = 0; (i * alloc_unit) < num_pages; i++) {\r\nif (bl_resp->hdr.size + sizeof(union dm_mem_page_range) >\r\nPAGE_SIZE)\r\nreturn i * alloc_unit;\r\npg = alloc_pages(GFP_HIGHUSER | __GFP_NORETRY |\r\n__GFP_NOMEMALLOC | __GFP_NOWARN,\r\nget_order(alloc_unit << PAGE_SHIFT));\r\nif (!pg) {\r\n*alloc_error = true;\r\nreturn i * alloc_unit;\r\n}\r\ndm->num_pages_ballooned += alloc_unit;\r\nif (alloc_unit != 1)\r\nsplit_page(pg, get_order(alloc_unit << PAGE_SHIFT));\r\nbl_resp->range_count++;\r\nbl_resp->range_array[i].finfo.start_page =\r\npage_to_pfn(pg);\r\nbl_resp->range_array[i].finfo.page_cnt = alloc_unit;\r\nbl_resp->hdr.size += sizeof(union dm_mem_page_range);\r\n}\r\nreturn num_pages;\r\n}\r\nstatic void balloon_up(struct work_struct *dummy)\r\n{\r\nint num_pages = dm_device.balloon_wrk.num_pages;\r\nint num_ballooned = 0;\r\nstruct dm_balloon_response *bl_resp;\r\nint alloc_unit;\r\nint ret;\r\nbool alloc_error = false;\r\nbool done = false;\r\nint i;\r\nalloc_unit = 512;\r\nwhile (!done) {\r\nbl_resp = (struct dm_balloon_response *)send_buffer;\r\nmemset(send_buffer, 0, PAGE_SIZE);\r\nbl_resp->hdr.type = DM_BALLOON_RESPONSE;\r\nbl_resp->hdr.size = sizeof(struct dm_balloon_response);\r\nbl_resp->more_pages = 1;\r\nnum_pages -= num_ballooned;\r\nnum_ballooned = alloc_balloon_pages(&dm_device, num_pages,\r\nbl_resp, alloc_unit,\r\n&alloc_error);\r\nif ((alloc_error) && (alloc_unit != 1)) {\r\nalloc_unit = 1;\r\ncontinue;\r\n}\r\nif ((alloc_error) || (num_ballooned == num_pages)) {\r\nbl_resp->more_pages = 0;\r\ndone = true;\r\ndm_device.state = DM_INITIALIZED;\r\n}\r\ndo {\r\nbl_resp->hdr.trans_id = atomic_inc_return(&trans_id);\r\nret = vmbus_sendpacket(dm_device.dev->channel,\r\nbl_resp,\r\nbl_resp->hdr.size,\r\n(unsigned long)NULL,\r\nVM_PKT_DATA_INBAND, 0);\r\nif (ret == -EAGAIN)\r\nmsleep(20);\r\n} while (ret == -EAGAIN);\r\nif (ret) {\r\npr_info("Balloon response failed\n");\r\nfor (i = 0; i < bl_resp->range_count; i++)\r\nfree_balloon_pages(&dm_device,\r\n&bl_resp->range_array[i]);\r\ndone = true;\r\n}\r\n}\r\n}\r\nstatic void balloon_down(struct hv_dynmem_device *dm,\r\nstruct dm_unballoon_request *req)\r\n{\r\nunion dm_mem_page_range *range_array = req->range_array;\r\nint range_count = req->range_count;\r\nstruct dm_unballoon_response resp;\r\nint i;\r\nfor (i = 0; i < range_count; i++)\r\nfree_balloon_pages(dm, &range_array[i]);\r\nif (req->more_pages == 1)\r\nreturn;\r\nmemset(&resp, 0, sizeof(struct dm_unballoon_response));\r\nresp.hdr.type = DM_UNBALLOON_RESPONSE;\r\nresp.hdr.trans_id = atomic_inc_return(&trans_id);\r\nresp.hdr.size = sizeof(struct dm_unballoon_response);\r\nvmbus_sendpacket(dm_device.dev->channel, &resp,\r\nsizeof(struct dm_unballoon_response),\r\n(unsigned long)NULL,\r\nVM_PKT_DATA_INBAND, 0);\r\ndm->state = DM_INITIALIZED;\r\n}\r\nstatic int dm_thread_func(void *dm_dev)\r\n{\r\nstruct hv_dynmem_device *dm = dm_dev;\r\nint t;\r\nwhile (!kthread_should_stop()) {\r\nt = wait_for_completion_timeout(&dm_device.config_event, 1*HZ);\r\nif (t == 0)\r\npost_status(dm);\r\n}\r\nreturn 0;\r\n}\r\nstatic void version_resp(struct hv_dynmem_device *dm,\r\nstruct dm_version_response *vresp)\r\n{\r\nstruct dm_version_request version_req;\r\nint ret;\r\nif (vresp->is_accepted) {\r\ncomplete(&dm->host_event);\r\nreturn;\r\n}\r\nif (dm->next_version == 0)\r\ngoto version_error;\r\ndm->next_version = 0;\r\nmemset(&version_req, 0, sizeof(struct dm_version_request));\r\nversion_req.hdr.type = DM_VERSION_REQUEST;\r\nversion_req.hdr.size = sizeof(struct dm_version_request);\r\nversion_req.hdr.trans_id = atomic_inc_return(&trans_id);\r\nversion_req.version.version = DYNMEM_PROTOCOL_VERSION_WIN7;\r\nversion_req.is_last_attempt = 1;\r\nret = vmbus_sendpacket(dm->dev->channel, &version_req,\r\nsizeof(struct dm_version_request),\r\n(unsigned long)NULL,\r\nVM_PKT_DATA_INBAND, 0);\r\nif (ret)\r\ngoto version_error;\r\nreturn;\r\nversion_error:\r\ndm->state = DM_INIT_ERROR;\r\ncomplete(&dm->host_event);\r\n}\r\nstatic void cap_resp(struct hv_dynmem_device *dm,\r\nstruct dm_capabilities_resp_msg *cap_resp)\r\n{\r\nif (!cap_resp->is_accepted) {\r\npr_info("Capabilities not accepted by host\n");\r\ndm->state = DM_INIT_ERROR;\r\n}\r\ncomplete(&dm->host_event);\r\n}\r\nstatic void balloon_onchannelcallback(void *context)\r\n{\r\nstruct hv_device *dev = context;\r\nu32 recvlen;\r\nu64 requestid;\r\nstruct dm_message *dm_msg;\r\nstruct dm_header *dm_hdr;\r\nstruct hv_dynmem_device *dm = hv_get_drvdata(dev);\r\nstruct dm_balloon *bal_msg;\r\nstruct dm_hot_add *ha_msg;\r\nunion dm_mem_page_range *ha_pg_range;\r\nunion dm_mem_page_range *ha_region;\r\nmemset(recv_buffer, 0, sizeof(recv_buffer));\r\nvmbus_recvpacket(dev->channel, recv_buffer,\r\nPAGE_SIZE, &recvlen, &requestid);\r\nif (recvlen > 0) {\r\ndm_msg = (struct dm_message *)recv_buffer;\r\ndm_hdr = &dm_msg->hdr;\r\nswitch (dm_hdr->type) {\r\ncase DM_VERSION_RESPONSE:\r\nversion_resp(dm,\r\n(struct dm_version_response *)dm_msg);\r\nbreak;\r\ncase DM_CAPABILITIES_RESPONSE:\r\ncap_resp(dm,\r\n(struct dm_capabilities_resp_msg *)dm_msg);\r\nbreak;\r\ncase DM_BALLOON_REQUEST:\r\nif (dm->state == DM_BALLOON_UP)\r\npr_warn("Currently ballooning\n");\r\nbal_msg = (struct dm_balloon *)recv_buffer;\r\ndm->state = DM_BALLOON_UP;\r\ndm_device.balloon_wrk.num_pages = bal_msg->num_pages;\r\nschedule_work(&dm_device.balloon_wrk.wrk);\r\nbreak;\r\ncase DM_UNBALLOON_REQUEST:\r\ndm->state = DM_BALLOON_DOWN;\r\nballoon_down(dm,\r\n(struct dm_unballoon_request *)recv_buffer);\r\nbreak;\r\ncase DM_MEM_HOT_ADD_REQUEST:\r\nif (dm->state == DM_HOT_ADD)\r\npr_warn("Currently hot-adding\n");\r\ndm->state = DM_HOT_ADD;\r\nha_msg = (struct dm_hot_add *)recv_buffer;\r\nif (ha_msg->hdr.size == sizeof(struct dm_hot_add)) {\r\nha_pg_range = &ha_msg->range;\r\ndm->ha_wrk.ha_page_range = *ha_pg_range;\r\ndm->ha_wrk.ha_region_range.page_range = 0;\r\n} else {\r\ndm->host_specified_ha_region = true;\r\nha_pg_range = &ha_msg->range;\r\nha_region = &ha_pg_range[1];\r\ndm->ha_wrk.ha_page_range = *ha_pg_range;\r\ndm->ha_wrk.ha_region_range = *ha_region;\r\n}\r\nschedule_work(&dm_device.ha_wrk.wrk);\r\nbreak;\r\ncase DM_INFO_MESSAGE:\r\nprocess_info(dm, (struct dm_info_msg *)dm_msg);\r\nbreak;\r\ndefault:\r\npr_err("Unhandled message: type: %d\n", dm_hdr->type);\r\n}\r\n}\r\n}\r\nstatic int balloon_probe(struct hv_device *dev,\r\nconst struct hv_vmbus_device_id *dev_id)\r\n{\r\nint ret, t;\r\nstruct dm_version_request version_req;\r\nstruct dm_capabilities cap_msg;\r\ndo_hot_add = hot_add;\r\nsend_buffer = kmalloc(PAGE_SIZE, GFP_KERNEL);\r\nif (!send_buffer)\r\nreturn -ENOMEM;\r\nret = vmbus_open(dev->channel, dm_ring_size, dm_ring_size, NULL, 0,\r\nballoon_onchannelcallback, dev);\r\nif (ret)\r\ngoto probe_error0;\r\ndm_device.dev = dev;\r\ndm_device.state = DM_INITIALIZING;\r\ndm_device.next_version = DYNMEM_PROTOCOL_VERSION_WIN7;\r\ninit_completion(&dm_device.host_event);\r\ninit_completion(&dm_device.config_event);\r\nINIT_LIST_HEAD(&dm_device.ha_region_list);\r\nINIT_WORK(&dm_device.balloon_wrk.wrk, balloon_up);\r\nINIT_WORK(&dm_device.ha_wrk.wrk, hot_add_req);\r\ndm_device.host_specified_ha_region = false;\r\ndm_device.thread =\r\nkthread_run(dm_thread_func, &dm_device, "hv_balloon");\r\nif (IS_ERR(dm_device.thread)) {\r\nret = PTR_ERR(dm_device.thread);\r\ngoto probe_error1;\r\n}\r\n#ifdef CONFIG_MEMORY_HOTPLUG\r\nset_online_page_callback(&hv_online_page);\r\n#endif\r\nhv_set_drvdata(dev, &dm_device);\r\nmemset(&version_req, 0, sizeof(struct dm_version_request));\r\nversion_req.hdr.type = DM_VERSION_REQUEST;\r\nversion_req.hdr.size = sizeof(struct dm_version_request);\r\nversion_req.hdr.trans_id = atomic_inc_return(&trans_id);\r\nversion_req.version.version = DYNMEM_PROTOCOL_VERSION_WIN8;\r\nversion_req.is_last_attempt = 0;\r\nret = vmbus_sendpacket(dev->channel, &version_req,\r\nsizeof(struct dm_version_request),\r\n(unsigned long)NULL,\r\nVM_PKT_DATA_INBAND, 0);\r\nif (ret)\r\ngoto probe_error2;\r\nt = wait_for_completion_timeout(&dm_device.host_event, 5*HZ);\r\nif (t == 0) {\r\nret = -ETIMEDOUT;\r\ngoto probe_error2;\r\n}\r\nif (dm_device.state == DM_INIT_ERROR) {\r\nret = -ETIMEDOUT;\r\ngoto probe_error2;\r\n}\r\nmemset(&cap_msg, 0, sizeof(struct dm_capabilities));\r\ncap_msg.hdr.type = DM_CAPABILITIES_REPORT;\r\ncap_msg.hdr.size = sizeof(struct dm_capabilities);\r\ncap_msg.hdr.trans_id = atomic_inc_return(&trans_id);\r\ncap_msg.caps.cap_bits.balloon = 1;\r\ncap_msg.caps.cap_bits.hot_add = 1;\r\ncap_msg.caps.cap_bits.hot_add_alignment = 7;\r\ncap_msg.min_page_cnt = 0;\r\ncap_msg.max_page_number = -1;\r\nret = vmbus_sendpacket(dev->channel, &cap_msg,\r\nsizeof(struct dm_capabilities),\r\n(unsigned long)NULL,\r\nVM_PKT_DATA_INBAND, 0);\r\nif (ret)\r\ngoto probe_error2;\r\nt = wait_for_completion_timeout(&dm_device.host_event, 5*HZ);\r\nif (t == 0) {\r\nret = -ETIMEDOUT;\r\ngoto probe_error2;\r\n}\r\nif (dm_device.state == DM_INIT_ERROR) {\r\nret = -ETIMEDOUT;\r\ngoto probe_error2;\r\n}\r\ndm_device.state = DM_INITIALIZED;\r\nreturn 0;\r\nprobe_error2:\r\n#ifdef CONFIG_MEMORY_HOTPLUG\r\nrestore_online_page_callback(&hv_online_page);\r\n#endif\r\nkthread_stop(dm_device.thread);\r\nprobe_error1:\r\nvmbus_close(dev->channel);\r\nprobe_error0:\r\nkfree(send_buffer);\r\nreturn ret;\r\n}\r\nstatic int balloon_remove(struct hv_device *dev)\r\n{\r\nstruct hv_dynmem_device *dm = hv_get_drvdata(dev);\r\nstruct list_head *cur, *tmp;\r\nstruct hv_hotadd_state *has;\r\nif (dm->num_pages_ballooned != 0)\r\npr_warn("Ballooned pages: %d\n", dm->num_pages_ballooned);\r\ncancel_work_sync(&dm->balloon_wrk.wrk);\r\ncancel_work_sync(&dm->ha_wrk.wrk);\r\nvmbus_close(dev->channel);\r\nkthread_stop(dm->thread);\r\nkfree(send_buffer);\r\n#ifdef CONFIG_MEMORY_HOTPLUG\r\nrestore_online_page_callback(&hv_online_page);\r\n#endif\r\nlist_for_each_safe(cur, tmp, &dm->ha_region_list) {\r\nhas = list_entry(cur, struct hv_hotadd_state, list);\r\nlist_del(&has->list);\r\nkfree(has);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init init_balloon_drv(void)\r\n{\r\nreturn vmbus_driver_register(&balloon_drv);\r\n}
