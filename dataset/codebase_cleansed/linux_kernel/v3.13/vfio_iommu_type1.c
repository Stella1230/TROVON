static struct vfio_dma *vfio_find_dma(struct vfio_iommu *iommu,\r\ndma_addr_t start, size_t size)\r\n{\r\nstruct rb_node *node = iommu->dma_list.rb_node;\r\nwhile (node) {\r\nstruct vfio_dma *dma = rb_entry(node, struct vfio_dma, node);\r\nif (start + size <= dma->iova)\r\nnode = node->rb_left;\r\nelse if (start >= dma->iova + dma->size)\r\nnode = node->rb_right;\r\nelse\r\nreturn dma;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void vfio_insert_dma(struct vfio_iommu *iommu, struct vfio_dma *new)\r\n{\r\nstruct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;\r\nstruct vfio_dma *dma;\r\nwhile (*link) {\r\nparent = *link;\r\ndma = rb_entry(parent, struct vfio_dma, node);\r\nif (new->iova + new->size <= dma->iova)\r\nlink = &(*link)->rb_left;\r\nelse\r\nlink = &(*link)->rb_right;\r\n}\r\nrb_link_node(&new->node, parent, link);\r\nrb_insert_color(&new->node, &iommu->dma_list);\r\n}\r\nstatic void vfio_remove_dma(struct vfio_iommu *iommu, struct vfio_dma *old)\r\n{\r\nrb_erase(&old->node, &iommu->dma_list);\r\n}\r\nstatic void vfio_lock_acct_bg(struct work_struct *work)\r\n{\r\nstruct vwork *vwork = container_of(work, struct vwork, work);\r\nstruct mm_struct *mm;\r\nmm = vwork->mm;\r\ndown_write(&mm->mmap_sem);\r\nmm->locked_vm += vwork->npage;\r\nup_write(&mm->mmap_sem);\r\nmmput(mm);\r\nkfree(vwork);\r\n}\r\nstatic void vfio_lock_acct(long npage)\r\n{\r\nstruct vwork *vwork;\r\nstruct mm_struct *mm;\r\nif (!current->mm || !npage)\r\nreturn;\r\nif (down_write_trylock(&current->mm->mmap_sem)) {\r\ncurrent->mm->locked_vm += npage;\r\nup_write(&current->mm->mmap_sem);\r\nreturn;\r\n}\r\nvwork = kmalloc(sizeof(struct vwork), GFP_KERNEL);\r\nif (!vwork)\r\nreturn;\r\nmm = get_task_mm(current);\r\nif (!mm) {\r\nkfree(vwork);\r\nreturn;\r\n}\r\nINIT_WORK(&vwork->work, vfio_lock_acct_bg);\r\nvwork->mm = mm;\r\nvwork->npage = npage;\r\nschedule_work(&vwork->work);\r\n}\r\nstatic bool is_invalid_reserved_pfn(unsigned long pfn)\r\n{\r\nif (pfn_valid(pfn)) {\r\nbool reserved;\r\nstruct page *tail = pfn_to_page(pfn);\r\nstruct page *head = compound_trans_head(tail);\r\nreserved = !!(PageReserved(head));\r\nif (head != tail) {\r\nsmp_rmb();\r\nif (PageTail(tail))\r\nreturn reserved;\r\n}\r\nreturn PageReserved(tail);\r\n}\r\nreturn true;\r\n}\r\nstatic int put_pfn(unsigned long pfn, int prot)\r\n{\r\nif (!is_invalid_reserved_pfn(pfn)) {\r\nstruct page *page = pfn_to_page(pfn);\r\nif (prot & IOMMU_WRITE)\r\nSetPageDirty(page);\r\nput_page(page);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int vaddr_get_pfn(unsigned long vaddr, int prot, unsigned long *pfn)\r\n{\r\nstruct page *page[1];\r\nstruct vm_area_struct *vma;\r\nint ret = -EFAULT;\r\nif (get_user_pages_fast(vaddr, 1, !!(prot & IOMMU_WRITE), page) == 1) {\r\n*pfn = page_to_pfn(page[0]);\r\nreturn 0;\r\n}\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma_intersection(current->mm, vaddr, vaddr + 1);\r\nif (vma && vma->vm_flags & VM_PFNMAP) {\r\n*pfn = ((vaddr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nif (is_invalid_reserved_pfn(*pfn))\r\nret = 0;\r\n}\r\nup_read(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic long vfio_pin_pages(unsigned long vaddr, long npage,\r\nint prot, unsigned long *pfn_base)\r\n{\r\nunsigned long limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nbool lock_cap = capable(CAP_IPC_LOCK);\r\nlong ret, i;\r\nif (!current->mm)\r\nreturn -ENODEV;\r\nret = vaddr_get_pfn(vaddr, prot, pfn_base);\r\nif (ret)\r\nreturn ret;\r\nif (is_invalid_reserved_pfn(*pfn_base))\r\nreturn 1;\r\nif (!lock_cap && current->mm->locked_vm + 1 > limit) {\r\nput_pfn(*pfn_base, prot);\r\npr_warn("%s: RLIMIT_MEMLOCK (%ld) exceeded\n", __func__,\r\nlimit << PAGE_SHIFT);\r\nreturn -ENOMEM;\r\n}\r\nif (unlikely(disable_hugepages)) {\r\nvfio_lock_acct(1);\r\nreturn 1;\r\n}\r\nfor (i = 1, vaddr += PAGE_SIZE; i < npage; i++, vaddr += PAGE_SIZE) {\r\nunsigned long pfn = 0;\r\nret = vaddr_get_pfn(vaddr, prot, &pfn);\r\nif (ret)\r\nbreak;\r\nif (pfn != *pfn_base + i || is_invalid_reserved_pfn(pfn)) {\r\nput_pfn(pfn, prot);\r\nbreak;\r\n}\r\nif (!lock_cap && current->mm->locked_vm + i + 1 > limit) {\r\nput_pfn(pfn, prot);\r\npr_warn("%s: RLIMIT_MEMLOCK (%ld) exceeded\n",\r\n__func__, limit << PAGE_SHIFT);\r\nbreak;\r\n}\r\n}\r\nvfio_lock_acct(i);\r\nreturn i;\r\n}\r\nstatic long vfio_unpin_pages(unsigned long pfn, long npage,\r\nint prot, bool do_accounting)\r\n{\r\nunsigned long unlocked = 0;\r\nlong i;\r\nfor (i = 0; i < npage; i++)\r\nunlocked += put_pfn(pfn++, prot);\r\nif (do_accounting)\r\nvfio_lock_acct(-unlocked);\r\nreturn unlocked;\r\n}\r\nstatic int vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,\r\ndma_addr_t iova, size_t *size)\r\n{\r\ndma_addr_t start = iova, end = iova + *size;\r\nlong unlocked = 0;\r\nwhile (iova < end) {\r\nsize_t unmapped;\r\nphys_addr_t phys;\r\nphys = iommu_iova_to_phys(iommu->domain, iova);\r\nif (WARN_ON(!phys)) {\r\niova += PAGE_SIZE;\r\ncontinue;\r\n}\r\nunmapped = iommu_unmap(iommu->domain, iova, PAGE_SIZE);\r\nif (!unmapped)\r\nbreak;\r\nunlocked += vfio_unpin_pages(phys >> PAGE_SHIFT,\r\nunmapped >> PAGE_SHIFT,\r\ndma->prot, false);\r\niova += unmapped;\r\n}\r\nvfio_lock_acct(-unlocked);\r\n*size = iova - start;\r\nreturn 0;\r\n}\r\nstatic int vfio_remove_dma_overlap(struct vfio_iommu *iommu, dma_addr_t start,\r\nsize_t *size, struct vfio_dma *dma)\r\n{\r\nsize_t offset, overlap, tmp;\r\nstruct vfio_dma *split;\r\nint ret;\r\nif (!*size)\r\nreturn 0;\r\nif (likely(start <= dma->iova &&\r\nstart + *size >= dma->iova + dma->size)) {\r\n*size = dma->size;\r\nret = vfio_unmap_unpin(iommu, dma, dma->iova, size);\r\nif (ret)\r\nreturn ret;\r\nWARN_ON(*size != dma->size);\r\nvfio_remove_dma(iommu, dma);\r\nkfree(dma);\r\nreturn 0;\r\n}\r\nif (start <= dma->iova) {\r\noverlap = start + *size - dma->iova;\r\nret = vfio_unmap_unpin(iommu, dma, dma->iova, &overlap);\r\nif (ret)\r\nreturn ret;\r\nvfio_remove_dma(iommu, dma);\r\nif (overlap < dma->size) {\r\ndma->iova += overlap;\r\ndma->vaddr += overlap;\r\ndma->size -= overlap;\r\nvfio_insert_dma(iommu, dma);\r\n} else\r\nkfree(dma);\r\n*size = overlap;\r\nreturn 0;\r\n}\r\nif (start + *size >= dma->iova + dma->size) {\r\noffset = start - dma->iova;\r\noverlap = dma->size - offset;\r\nret = vfio_unmap_unpin(iommu, dma, start, &overlap);\r\nif (ret)\r\nreturn ret;\r\ndma->size -= overlap;\r\n*size = overlap;\r\nreturn 0;\r\n}\r\nsplit = kzalloc(sizeof(*split), GFP_KERNEL);\r\nif (!split)\r\nreturn -ENOMEM;\r\noffset = start - dma->iova;\r\nret = vfio_unmap_unpin(iommu, dma, start, size);\r\nif (ret || !*size) {\r\nkfree(split);\r\nreturn ret;\r\n}\r\ntmp = dma->size;\r\ndma->size = offset;\r\nif (likely(offset + *size < tmp)) {\r\nsplit->size = tmp - offset - *size;\r\nsplit->iova = dma->iova + offset + *size;\r\nsplit->vaddr = dma->vaddr + offset + *size;\r\nsplit->prot = dma->prot;\r\nvfio_insert_dma(iommu, split);\r\n} else\r\nkfree(split);\r\nreturn 0;\r\n}\r\nstatic int vfio_dma_do_unmap(struct vfio_iommu *iommu,\r\nstruct vfio_iommu_type1_dma_unmap *unmap)\r\n{\r\nuint64_t mask;\r\nstruct vfio_dma *dma;\r\nsize_t unmapped = 0, size;\r\nint ret = 0;\r\nmask = ((uint64_t)1 << __ffs(iommu->domain->ops->pgsize_bitmap)) - 1;\r\nif (unmap->iova & mask)\r\nreturn -EINVAL;\r\nif (!unmap->size || unmap->size & mask)\r\nreturn -EINVAL;\r\nWARN_ON(mask & PAGE_MASK);\r\nmutex_lock(&iommu->lock);\r\nwhile ((dma = vfio_find_dma(iommu, unmap->iova, unmap->size))) {\r\nsize = unmap->size;\r\nret = vfio_remove_dma_overlap(iommu, unmap->iova, &size, dma);\r\nif (ret || !size)\r\nbreak;\r\nunmapped += size;\r\n}\r\nmutex_unlock(&iommu->lock);\r\nunmap->size = unmapped;\r\nreturn ret;\r\n}\r\nstatic int map_try_harder(struct vfio_iommu *iommu, dma_addr_t iova,\r\nunsigned long pfn, long npage, int prot)\r\n{\r\nlong i;\r\nint ret;\r\nfor (i = 0; i < npage; i++, pfn++, iova += PAGE_SIZE) {\r\nret = iommu_map(iommu->domain, iova,\r\n(phys_addr_t)pfn << PAGE_SHIFT,\r\nPAGE_SIZE, prot);\r\nif (ret)\r\nbreak;\r\n}\r\nfor (; i < npage && i > 0; i--, iova -= PAGE_SIZE)\r\niommu_unmap(iommu->domain, iova, PAGE_SIZE);\r\nreturn ret;\r\n}\r\nstatic int vfio_dma_do_map(struct vfio_iommu *iommu,\r\nstruct vfio_iommu_type1_dma_map *map)\r\n{\r\ndma_addr_t end, iova;\r\nunsigned long vaddr = map->vaddr;\r\nsize_t size = map->size;\r\nlong npage;\r\nint ret = 0, prot = 0;\r\nuint64_t mask;\r\nstruct vfio_dma *dma = NULL;\r\nunsigned long pfn;\r\nend = map->iova + map->size;\r\nmask = ((uint64_t)1 << __ffs(iommu->domain->ops->pgsize_bitmap)) - 1;\r\nif (map->flags & VFIO_DMA_MAP_FLAG_WRITE)\r\nprot |= IOMMU_WRITE;\r\nif (map->flags & VFIO_DMA_MAP_FLAG_READ)\r\nprot |= IOMMU_READ;\r\nif (!prot)\r\nreturn -EINVAL;\r\nif (iommu->cache)\r\nprot |= IOMMU_CACHE;\r\nif (vaddr & mask)\r\nreturn -EINVAL;\r\nif (map->iova & mask)\r\nreturn -EINVAL;\r\nif (!map->size || map->size & mask)\r\nreturn -EINVAL;\r\nWARN_ON(mask & PAGE_MASK);\r\nif (end && end < map->iova)\r\nreturn -EINVAL;\r\nif (vaddr + map->size && vaddr + map->size < vaddr)\r\nreturn -EINVAL;\r\nmutex_lock(&iommu->lock);\r\nif (vfio_find_dma(iommu, map->iova, map->size)) {\r\nmutex_unlock(&iommu->lock);\r\nreturn -EEXIST;\r\n}\r\nfor (iova = map->iova; iova < end; iova += size, vaddr += size) {\r\nlong i;\r\nnpage = vfio_pin_pages(vaddr, (end - iova) >> PAGE_SHIFT,\r\nprot, &pfn);\r\nif (npage <= 0) {\r\nWARN_ON(!npage);\r\nret = (int)npage;\r\ngoto out;\r\n}\r\nfor (i = 0; i < npage; i++) {\r\nif (iommu_iova_to_phys(iommu->domain,\r\niova + (i << PAGE_SHIFT))) {\r\nret = -EBUSY;\r\ngoto out_unpin;\r\n}\r\n}\r\nret = iommu_map(iommu->domain, iova,\r\n(phys_addr_t)pfn << PAGE_SHIFT,\r\nnpage << PAGE_SHIFT, prot);\r\nif (ret) {\r\nif (ret != -EBUSY ||\r\nmap_try_harder(iommu, iova, pfn, npage, prot)) {\r\ngoto out_unpin;\r\n}\r\n}\r\nsize = npage << PAGE_SHIFT;\r\nif (likely(iova)) {\r\nstruct vfio_dma *tmp;\r\ntmp = vfio_find_dma(iommu, iova - 1, 1);\r\nif (tmp && tmp->prot == prot &&\r\ntmp->vaddr + tmp->size == vaddr) {\r\ntmp->size += size;\r\niova = tmp->iova;\r\nsize = tmp->size;\r\nvaddr = tmp->vaddr;\r\ndma = tmp;\r\n}\r\n}\r\nif (likely(iova + size)) {\r\nstruct vfio_dma *tmp;\r\ntmp = vfio_find_dma(iommu, iova + size, 1);\r\nif (tmp && tmp->prot == prot &&\r\ntmp->vaddr == vaddr + size) {\r\nvfio_remove_dma(iommu, tmp);\r\nif (dma) {\r\ndma->size += tmp->size;\r\nkfree(tmp);\r\n} else {\r\nsize += tmp->size;\r\ntmp->size = size;\r\ntmp->iova = iova;\r\ntmp->vaddr = vaddr;\r\nvfio_insert_dma(iommu, tmp);\r\ndma = tmp;\r\n}\r\n}\r\n}\r\nif (!dma) {\r\ndma = kzalloc(sizeof(*dma), GFP_KERNEL);\r\nif (!dma) {\r\niommu_unmap(iommu->domain, iova, size);\r\nret = -ENOMEM;\r\ngoto out_unpin;\r\n}\r\ndma->size = size;\r\ndma->iova = iova;\r\ndma->vaddr = vaddr;\r\ndma->prot = prot;\r\nvfio_insert_dma(iommu, dma);\r\n}\r\n}\r\nWARN_ON(ret);\r\nmutex_unlock(&iommu->lock);\r\nreturn ret;\r\nout_unpin:\r\nvfio_unpin_pages(pfn, npage, prot, true);\r\nout:\r\niova = map->iova;\r\nsize = map->size;\r\nwhile ((dma = vfio_find_dma(iommu, iova, size))) {\r\nint r = vfio_remove_dma_overlap(iommu, iova,\r\n&size, dma);\r\nif (WARN_ON(r || !size))\r\nbreak;\r\n}\r\nmutex_unlock(&iommu->lock);\r\nreturn ret;\r\n}\r\nstatic int vfio_iommu_type1_attach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_group *group, *tmp;\r\nint ret;\r\ngroup = kzalloc(sizeof(*group), GFP_KERNEL);\r\nif (!group)\r\nreturn -ENOMEM;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(tmp, &iommu->group_list, next) {\r\nif (tmp->iommu_group == iommu_group) {\r\nmutex_unlock(&iommu->lock);\r\nkfree(group);\r\nreturn -EINVAL;\r\n}\r\n}\r\nret = iommu_attach_group(iommu->domain, iommu_group);\r\nif (ret) {\r\nmutex_unlock(&iommu->lock);\r\nkfree(group);\r\nreturn ret;\r\n}\r\ngroup->iommu_group = iommu_group;\r\nlist_add(&group->next, &iommu->group_list);\r\nmutex_unlock(&iommu->lock);\r\nreturn 0;\r\n}\r\nstatic void vfio_iommu_type1_detach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_group *group;\r\nmutex_lock(&iommu->lock);\r\nlist_for_each_entry(group, &iommu->group_list, next) {\r\nif (group->iommu_group == iommu_group) {\r\niommu_detach_group(iommu->domain, iommu_group);\r\nlist_del(&group->next);\r\nkfree(group);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&iommu->lock);\r\n}\r\nstatic void *vfio_iommu_type1_open(unsigned long arg)\r\n{\r\nstruct vfio_iommu *iommu;\r\nif (arg != VFIO_TYPE1_IOMMU)\r\nreturn ERR_PTR(-EINVAL);\r\niommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\r\nif (!iommu)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_LIST_HEAD(&iommu->group_list);\r\niommu->dma_list = RB_ROOT;\r\nmutex_init(&iommu->lock);\r\niommu->domain = iommu_domain_alloc(&pci_bus_type);\r\nif (!iommu->domain) {\r\nkfree(iommu);\r\nreturn ERR_PTR(-EIO);\r\n}\r\nif (!allow_unsafe_interrupts &&\r\n!iommu_domain_has_cap(iommu->domain, IOMMU_CAP_INTR_REMAP)) {\r\npr_warn("%s: No interrupt remapping support. Use the module param \"allow_unsafe_interrupts\" to enable VFIO IOMMU support on this platform\n",\r\n__func__);\r\niommu_domain_free(iommu->domain);\r\nkfree(iommu);\r\nreturn ERR_PTR(-EPERM);\r\n}\r\nreturn iommu;\r\n}\r\nstatic void vfio_iommu_type1_release(void *iommu_data)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nstruct vfio_group *group, *group_tmp;\r\nstruct rb_node *node;\r\nlist_for_each_entry_safe(group, group_tmp, &iommu->group_list, next) {\r\niommu_detach_group(iommu->domain, group->iommu_group);\r\nlist_del(&group->next);\r\nkfree(group);\r\n}\r\nwhile ((node = rb_first(&iommu->dma_list))) {\r\nstruct vfio_dma *dma = rb_entry(node, struct vfio_dma, node);\r\nsize_t size = dma->size;\r\nvfio_remove_dma_overlap(iommu, dma->iova, &size, dma);\r\nif (WARN_ON(!size))\r\nbreak;\r\n}\r\niommu_domain_free(iommu->domain);\r\niommu->domain = NULL;\r\nkfree(iommu);\r\n}\r\nstatic long vfio_iommu_type1_ioctl(void *iommu_data,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nstruct vfio_iommu *iommu = iommu_data;\r\nunsigned long minsz;\r\nif (cmd == VFIO_CHECK_EXTENSION) {\r\nswitch (arg) {\r\ncase VFIO_TYPE1_IOMMU:\r\nreturn 1;\r\ndefault:\r\nreturn 0;\r\n}\r\n} else if (cmd == VFIO_IOMMU_GET_INFO) {\r\nstruct vfio_iommu_type1_info info;\r\nminsz = offsetofend(struct vfio_iommu_type1_info, iova_pgsizes);\r\nif (copy_from_user(&info, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (info.argsz < minsz)\r\nreturn -EINVAL;\r\ninfo.flags = 0;\r\ninfo.iova_pgsizes = iommu->domain->ops->pgsize_bitmap;\r\nreturn copy_to_user((void __user *)arg, &info, minsz);\r\n} else if (cmd == VFIO_IOMMU_MAP_DMA) {\r\nstruct vfio_iommu_type1_dma_map map;\r\nuint32_t mask = VFIO_DMA_MAP_FLAG_READ |\r\nVFIO_DMA_MAP_FLAG_WRITE;\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_map, size);\r\nif (copy_from_user(&map, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (map.argsz < minsz || map.flags & ~mask)\r\nreturn -EINVAL;\r\nreturn vfio_dma_do_map(iommu, &map);\r\n} else if (cmd == VFIO_IOMMU_UNMAP_DMA) {\r\nstruct vfio_iommu_type1_dma_unmap unmap;\r\nlong ret;\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_unmap, size);\r\nif (copy_from_user(&unmap, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (unmap.argsz < minsz || unmap.flags)\r\nreturn -EINVAL;\r\nret = vfio_dma_do_unmap(iommu, &unmap);\r\nif (ret)\r\nreturn ret;\r\nreturn copy_to_user((void __user *)arg, &unmap, minsz);\r\n}\r\nreturn -ENOTTY;\r\n}\r\nstatic int __init vfio_iommu_type1_init(void)\r\n{\r\nif (!iommu_present(&pci_bus_type))\r\nreturn -ENODEV;\r\nreturn vfio_register_iommu_driver(&vfio_iommu_driver_ops_type1);\r\n}\r\nstatic void __exit vfio_iommu_type1_cleanup(void)\r\n{\r\nvfio_unregister_iommu_driver(&vfio_iommu_driver_ops_type1);\r\n}
