static inline int slob_page_free(struct page *sp)\r\n{\r\nreturn PageSlobFree(sp);\r\n}\r\nstatic void set_slob_page_free(struct page *sp, struct list_head *list)\r\n{\r\nlist_add(&sp->list, list);\r\n__SetPageSlobFree(sp);\r\n}\r\nstatic inline void clear_slob_page_free(struct page *sp)\r\n{\r\nlist_del(&sp->list);\r\n__ClearPageSlobFree(sp);\r\n}\r\nstatic void set_slob(slob_t *s, slobidx_t size, slob_t *next)\r\n{\r\nslob_t *base = (slob_t *)((unsigned long)s & PAGE_MASK);\r\nslobidx_t offset = next - base;\r\nif (size > 1) {\r\ns[0].units = size;\r\ns[1].units = offset;\r\n} else\r\ns[0].units = -offset;\r\n}\r\nstatic slobidx_t slob_units(slob_t *s)\r\n{\r\nif (s->units > 0)\r\nreturn s->units;\r\nreturn 1;\r\n}\r\nstatic slob_t *slob_next(slob_t *s)\r\n{\r\nslob_t *base = (slob_t *)((unsigned long)s & PAGE_MASK);\r\nslobidx_t next;\r\nif (s[0].units < 0)\r\nnext = -s[0].units;\r\nelse\r\nnext = s[1].units;\r\nreturn base+next;\r\n}\r\nstatic int slob_last(slob_t *s)\r\n{\r\nreturn !((unsigned long)slob_next(s) & ~PAGE_MASK);\r\n}\r\nstatic void *slob_new_pages(gfp_t gfp, int order, int node)\r\n{\r\nvoid *page;\r\n#ifdef CONFIG_NUMA\r\nif (node != NUMA_NO_NODE)\r\npage = alloc_pages_exact_node(node, gfp, order);\r\nelse\r\n#endif\r\npage = alloc_pages(gfp, order);\r\nif (!page)\r\nreturn NULL;\r\nreturn page_address(page);\r\n}\r\nstatic void slob_free_pages(void *b, int order)\r\n{\r\nif (current->reclaim_state)\r\ncurrent->reclaim_state->reclaimed_slab += 1 << order;\r\nfree_pages((unsigned long)b, order);\r\n}\r\nstatic void *slob_page_alloc(struct page *sp, size_t size, int align)\r\n{\r\nslob_t *prev, *cur, *aligned = NULL;\r\nint delta = 0, units = SLOB_UNITS(size);\r\nfor (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {\r\nslobidx_t avail = slob_units(cur);\r\nif (align) {\r\naligned = (slob_t *)ALIGN((unsigned long)cur, align);\r\ndelta = aligned - cur;\r\n}\r\nif (avail >= units + delta) {\r\nslob_t *next;\r\nif (delta) {\r\nnext = slob_next(cur);\r\nset_slob(aligned, avail - delta, next);\r\nset_slob(cur, delta, aligned);\r\nprev = cur;\r\ncur = aligned;\r\navail = slob_units(cur);\r\n}\r\nnext = slob_next(cur);\r\nif (avail == units) {\r\nif (prev)\r\nset_slob(prev, slob_units(prev), next);\r\nelse\r\nsp->freelist = next;\r\n} else {\r\nif (prev)\r\nset_slob(prev, slob_units(prev), cur + units);\r\nelse\r\nsp->freelist = cur + units;\r\nset_slob(cur + units, avail - units, next);\r\n}\r\nsp->units -= units;\r\nif (!sp->units)\r\nclear_slob_page_free(sp);\r\nreturn cur;\r\n}\r\nif (slob_last(cur))\r\nreturn NULL;\r\n}\r\n}\r\nstatic void *slob_alloc(size_t size, gfp_t gfp, int align, int node)\r\n{\r\nstruct page *sp;\r\nstruct list_head *prev;\r\nstruct list_head *slob_list;\r\nslob_t *b = NULL;\r\nunsigned long flags;\r\nif (size < SLOB_BREAK1)\r\nslob_list = &free_slob_small;\r\nelse if (size < SLOB_BREAK2)\r\nslob_list = &free_slob_medium;\r\nelse\r\nslob_list = &free_slob_large;\r\nspin_lock_irqsave(&slob_lock, flags);\r\nlist_for_each_entry(sp, slob_list, list) {\r\n#ifdef CONFIG_NUMA\r\nif (node != NUMA_NO_NODE && page_to_nid(sp) != node)\r\ncontinue;\r\n#endif\r\nif (sp->units < SLOB_UNITS(size))\r\ncontinue;\r\nprev = sp->list.prev;\r\nb = slob_page_alloc(sp, size, align);\r\nif (!b)\r\ncontinue;\r\nif (prev != slob_list->prev &&\r\nslob_list->next != prev->next)\r\nlist_move_tail(slob_list, prev->next);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\nif (!b) {\r\nb = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);\r\nif (!b)\r\nreturn NULL;\r\nsp = virt_to_page(b);\r\n__SetPageSlab(sp);\r\nspin_lock_irqsave(&slob_lock, flags);\r\nsp->units = SLOB_UNITS(PAGE_SIZE);\r\nsp->freelist = b;\r\nINIT_LIST_HEAD(&sp->list);\r\nset_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));\r\nset_slob_page_free(sp, slob_list);\r\nb = slob_page_alloc(sp, size, align);\r\nBUG_ON(!b);\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\n}\r\nif (unlikely((gfp & __GFP_ZERO) && b))\r\nmemset(b, 0, size);\r\nreturn b;\r\n}\r\nstatic void slob_free(void *block, int size)\r\n{\r\nstruct page *sp;\r\nslob_t *prev, *next, *b = (slob_t *)block;\r\nslobidx_t units;\r\nunsigned long flags;\r\nstruct list_head *slob_list;\r\nif (unlikely(ZERO_OR_NULL_PTR(block)))\r\nreturn;\r\nBUG_ON(!size);\r\nsp = virt_to_page(block);\r\nunits = SLOB_UNITS(size);\r\nspin_lock_irqsave(&slob_lock, flags);\r\nif (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {\r\nif (slob_page_free(sp))\r\nclear_slob_page_free(sp);\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\n__ClearPageSlab(sp);\r\npage_mapcount_reset(sp);\r\nslob_free_pages(b, 0);\r\nreturn;\r\n}\r\nif (!slob_page_free(sp)) {\r\nsp->units = units;\r\nsp->freelist = b;\r\nset_slob(b, units,\r\n(void *)((unsigned long)(b +\r\nSLOB_UNITS(PAGE_SIZE)) & PAGE_MASK));\r\nif (size < SLOB_BREAK1)\r\nslob_list = &free_slob_small;\r\nelse if (size < SLOB_BREAK2)\r\nslob_list = &free_slob_medium;\r\nelse\r\nslob_list = &free_slob_large;\r\nset_slob_page_free(sp, slob_list);\r\ngoto out;\r\n}\r\nsp->units += units;\r\nif (b < (slob_t *)sp->freelist) {\r\nif (b + units == sp->freelist) {\r\nunits += slob_units(sp->freelist);\r\nsp->freelist = slob_next(sp->freelist);\r\n}\r\nset_slob(b, units, sp->freelist);\r\nsp->freelist = b;\r\n} else {\r\nprev = sp->freelist;\r\nnext = slob_next(prev);\r\nwhile (b > next) {\r\nprev = next;\r\nnext = slob_next(prev);\r\n}\r\nif (!slob_last(prev) && b + units == next) {\r\nunits += slob_units(next);\r\nset_slob(b, units, slob_next(next));\r\n} else\r\nset_slob(b, units, next);\r\nif (prev + slob_units(prev) == b) {\r\nunits = slob_units(b) + slob_units(prev);\r\nset_slob(prev, units, slob_next(b));\r\n} else\r\nset_slob(prev, slob_units(prev), b);\r\n}\r\nout:\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\n}\r\nstatic __always_inline void *\r\n__do_kmalloc_node(size_t size, gfp_t gfp, int node, unsigned long caller)\r\n{\r\nunsigned int *m;\r\nint align = max_t(size_t, ARCH_KMALLOC_MINALIGN, ARCH_SLAB_MINALIGN);\r\nvoid *ret;\r\ngfp &= gfp_allowed_mask;\r\nlockdep_trace_alloc(gfp);\r\nif (size < PAGE_SIZE - align) {\r\nif (!size)\r\nreturn ZERO_SIZE_PTR;\r\nm = slob_alloc(size + align, gfp, align, node);\r\nif (!m)\r\nreturn NULL;\r\n*m = size;\r\nret = (void *)m + align;\r\ntrace_kmalloc_node(caller, ret,\r\nsize, size + align, gfp, node);\r\n} else {\r\nunsigned int order = get_order(size);\r\nif (likely(order))\r\ngfp |= __GFP_COMP;\r\nret = slob_new_pages(gfp, order, node);\r\ntrace_kmalloc_node(caller, ret,\r\nsize, PAGE_SIZE << order, gfp, node);\r\n}\r\nkmemleak_alloc(ret, size, 1, gfp);\r\nreturn ret;\r\n}\r\nvoid *__kmalloc(size_t size, gfp_t gfp)\r\n{\r\nreturn __do_kmalloc_node(size, gfp, NUMA_NO_NODE, _RET_IP_);\r\n}\r\nvoid *__kmalloc_track_caller(size_t size, gfp_t gfp, unsigned long caller)\r\n{\r\nreturn __do_kmalloc_node(size, gfp, NUMA_NO_NODE, caller);\r\n}\r\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t gfp,\r\nint node, unsigned long caller)\r\n{\r\nreturn __do_kmalloc_node(size, gfp, node, caller);\r\n}\r\nvoid kfree(const void *block)\r\n{\r\nstruct page *sp;\r\ntrace_kfree(_RET_IP_, block);\r\nif (unlikely(ZERO_OR_NULL_PTR(block)))\r\nreturn;\r\nkmemleak_free(block);\r\nsp = virt_to_page(block);\r\nif (PageSlab(sp)) {\r\nint align = max_t(size_t, ARCH_KMALLOC_MINALIGN, ARCH_SLAB_MINALIGN);\r\nunsigned int *m = (unsigned int *)(block - align);\r\nslob_free(m, *m + align);\r\n} else\r\n__free_pages(sp, compound_order(sp));\r\n}\r\nsize_t ksize(const void *block)\r\n{\r\nstruct page *sp;\r\nint align;\r\nunsigned int *m;\r\nBUG_ON(!block);\r\nif (unlikely(block == ZERO_SIZE_PTR))\r\nreturn 0;\r\nsp = virt_to_page(block);\r\nif (unlikely(!PageSlab(sp)))\r\nreturn PAGE_SIZE << compound_order(sp);\r\nalign = max_t(size_t, ARCH_KMALLOC_MINALIGN, ARCH_SLAB_MINALIGN);\r\nm = (unsigned int *)(block - align);\r\nreturn SLOB_UNITS(*m) * SLOB_UNIT;\r\n}\r\nint __kmem_cache_create(struct kmem_cache *c, unsigned long flags)\r\n{\r\nif (flags & SLAB_DESTROY_BY_RCU) {\r\nc->size += sizeof(struct slob_rcu);\r\n}\r\nc->flags = flags;\r\nreturn 0;\r\n}\r\nvoid *slob_alloc_node(struct kmem_cache *c, gfp_t flags, int node)\r\n{\r\nvoid *b;\r\nflags &= gfp_allowed_mask;\r\nlockdep_trace_alloc(flags);\r\nif (c->size < PAGE_SIZE) {\r\nb = slob_alloc(c->size, flags, c->align, node);\r\ntrace_kmem_cache_alloc_node(_RET_IP_, b, c->object_size,\r\nSLOB_UNITS(c->size) * SLOB_UNIT,\r\nflags, node);\r\n} else {\r\nb = slob_new_pages(flags, get_order(c->size), node);\r\ntrace_kmem_cache_alloc_node(_RET_IP_, b, c->object_size,\r\nPAGE_SIZE << get_order(c->size),\r\nflags, node);\r\n}\r\nif (b && c->ctor)\r\nc->ctor(b);\r\nkmemleak_alloc_recursive(b, c->size, 1, c->flags, flags);\r\nreturn b;\r\n}\r\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nreturn slob_alloc_node(cachep, flags, NUMA_NO_NODE);\r\n}\r\nvoid *__kmalloc_node(size_t size, gfp_t gfp, int node)\r\n{\r\nreturn __do_kmalloc_node(size, gfp, node, _RET_IP_);\r\n}\r\nvoid *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t gfp, int node)\r\n{\r\nreturn slob_alloc_node(cachep, gfp, node);\r\n}\r\nstatic void __kmem_cache_free(void *b, int size)\r\n{\r\nif (size < PAGE_SIZE)\r\nslob_free(b, size);\r\nelse\r\nslob_free_pages(b, get_order(size));\r\n}\r\nstatic void kmem_rcu_free(struct rcu_head *head)\r\n{\r\nstruct slob_rcu *slob_rcu = (struct slob_rcu *)head;\r\nvoid *b = (void *)slob_rcu - (slob_rcu->size - sizeof(struct slob_rcu));\r\n__kmem_cache_free(b, slob_rcu->size);\r\n}\r\nvoid kmem_cache_free(struct kmem_cache *c, void *b)\r\n{\r\nkmemleak_free_recursive(b, c->flags);\r\nif (unlikely(c->flags & SLAB_DESTROY_BY_RCU)) {\r\nstruct slob_rcu *slob_rcu;\r\nslob_rcu = b + (c->size - sizeof(struct slob_rcu));\r\nslob_rcu->size = c->size;\r\ncall_rcu(&slob_rcu->head, kmem_rcu_free);\r\n} else {\r\n__kmem_cache_free(b, c->size);\r\n}\r\ntrace_kmem_cache_free(_RET_IP_, b);\r\n}\r\nint __kmem_cache_shutdown(struct kmem_cache *c)\r\n{\r\nreturn 0;\r\n}\r\nint kmem_cache_shrink(struct kmem_cache *d)\r\n{\r\nreturn 0;\r\n}\r\nvoid __init kmem_cache_init(void)\r\n{\r\nkmem_cache = &kmem_cache_boot;\r\nslab_state = UP;\r\n}\r\nvoid __init kmem_cache_init_late(void)\r\n{\r\nslab_state = FULL;\r\n}
