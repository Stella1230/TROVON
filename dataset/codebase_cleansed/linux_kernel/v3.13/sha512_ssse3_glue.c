static int sha512_ssse3_init(struct shash_desc *desc)\r\n{\r\nstruct sha512_state *sctx = shash_desc_ctx(desc);\r\nsctx->state[0] = SHA512_H0;\r\nsctx->state[1] = SHA512_H1;\r\nsctx->state[2] = SHA512_H2;\r\nsctx->state[3] = SHA512_H3;\r\nsctx->state[4] = SHA512_H4;\r\nsctx->state[5] = SHA512_H5;\r\nsctx->state[6] = SHA512_H6;\r\nsctx->state[7] = SHA512_H7;\r\nsctx->count[0] = sctx->count[1] = 0;\r\nreturn 0;\r\n}\r\nstatic int __sha512_ssse3_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, unsigned int partial)\r\n{\r\nstruct sha512_state *sctx = shash_desc_ctx(desc);\r\nunsigned int done = 0;\r\nsctx->count[0] += len;\r\nif (sctx->count[0] < len)\r\nsctx->count[1]++;\r\nif (partial) {\r\ndone = SHA512_BLOCK_SIZE - partial;\r\nmemcpy(sctx->buf + partial, data, done);\r\nsha512_transform_asm(sctx->buf, sctx->state, 1);\r\n}\r\nif (len - done >= SHA512_BLOCK_SIZE) {\r\nconst unsigned int rounds = (len - done) / SHA512_BLOCK_SIZE;\r\nsha512_transform_asm(data + done, sctx->state, (u64) rounds);\r\ndone += rounds * SHA512_BLOCK_SIZE;\r\n}\r\nmemcpy(sctx->buf, data + done, len - done);\r\nreturn 0;\r\n}\r\nstatic int sha512_ssse3_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nstruct sha512_state *sctx = shash_desc_ctx(desc);\r\nunsigned int partial = sctx->count[0] % SHA512_BLOCK_SIZE;\r\nint res;\r\nif (partial + len < SHA512_BLOCK_SIZE) {\r\nsctx->count[0] += len;\r\nif (sctx->count[0] < len)\r\nsctx->count[1]++;\r\nmemcpy(sctx->buf + partial, data, len);\r\nreturn 0;\r\n}\r\nif (!irq_fpu_usable()) {\r\nres = crypto_sha512_update(desc, data, len);\r\n} else {\r\nkernel_fpu_begin();\r\nres = __sha512_ssse3_update(desc, data, len, partial);\r\nkernel_fpu_end();\r\n}\r\nreturn res;\r\n}\r\nstatic int sha512_ssse3_final(struct shash_desc *desc, u8 *out)\r\n{\r\nstruct sha512_state *sctx = shash_desc_ctx(desc);\r\nunsigned int i, index, padlen;\r\n__be64 *dst = (__be64 *)out;\r\n__be64 bits[2];\r\nstatic const u8 padding[SHA512_BLOCK_SIZE] = { 0x80, };\r\nbits[1] = cpu_to_be64(sctx->count[0] << 3);\r\nbits[0] = cpu_to_be64(sctx->count[1] << 3) | sctx->count[0] >> 61;\r\nindex = sctx->count[0] & 0x7f;\r\npadlen = (index < 112) ? (112 - index) : ((128+112) - index);\r\nif (!irq_fpu_usable()) {\r\ncrypto_sha512_update(desc, padding, padlen);\r\ncrypto_sha512_update(desc, (const u8 *)&bits, sizeof(bits));\r\n} else {\r\nkernel_fpu_begin();\r\nif (padlen <= 112) {\r\nsctx->count[0] += padlen;\r\nif (sctx->count[0] < padlen)\r\nsctx->count[1]++;\r\nmemcpy(sctx->buf + index, padding, padlen);\r\n} else {\r\n__sha512_ssse3_update(desc, padding, padlen, index);\r\n}\r\n__sha512_ssse3_update(desc, (const u8 *)&bits,\r\nsizeof(bits), 112);\r\nkernel_fpu_end();\r\n}\r\nfor (i = 0; i < 8; i++)\r\ndst[i] = cpu_to_be64(sctx->state[i]);\r\nmemset(sctx, 0, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha512_ssse3_export(struct shash_desc *desc, void *out)\r\n{\r\nstruct sha512_state *sctx = shash_desc_ctx(desc);\r\nmemcpy(out, sctx, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha512_ssse3_import(struct shash_desc *desc, const void *in)\r\n{\r\nstruct sha512_state *sctx = shash_desc_ctx(desc);\r\nmemcpy(sctx, in, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha384_ssse3_init(struct shash_desc *desc)\r\n{\r\nstruct sha512_state *sctx = shash_desc_ctx(desc);\r\nsctx->state[0] = SHA384_H0;\r\nsctx->state[1] = SHA384_H1;\r\nsctx->state[2] = SHA384_H2;\r\nsctx->state[3] = SHA384_H3;\r\nsctx->state[4] = SHA384_H4;\r\nsctx->state[5] = SHA384_H5;\r\nsctx->state[6] = SHA384_H6;\r\nsctx->state[7] = SHA384_H7;\r\nsctx->count[0] = sctx->count[1] = 0;\r\nreturn 0;\r\n}\r\nstatic int sha384_ssse3_final(struct shash_desc *desc, u8 *hash)\r\n{\r\nu8 D[SHA512_DIGEST_SIZE];\r\nsha512_ssse3_final(desc, D);\r\nmemcpy(hash, D, SHA384_DIGEST_SIZE);\r\nmemset(D, 0, SHA512_DIGEST_SIZE);\r\nreturn 0;\r\n}\r\nstatic bool __init avx_usable(void)\r\n{\r\nu64 xcr0;\r\nif (!cpu_has_avx || !cpu_has_osxsave)\r\nreturn false;\r\nxcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\r\nif ((xcr0 & (XSTATE_SSE | XSTATE_YMM)) != (XSTATE_SSE | XSTATE_YMM)) {\r\npr_info("AVX detected but unusable.\n");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int __init sha512_ssse3_mod_init(void)\r\n{\r\nif (cpu_has_ssse3)\r\nsha512_transform_asm = sha512_transform_ssse3;\r\n#ifdef CONFIG_AS_AVX\r\nif (avx_usable()) {\r\n#ifdef CONFIG_AS_AVX2\r\nif (boot_cpu_has(X86_FEATURE_AVX2))\r\nsha512_transform_asm = sha512_transform_rorx;\r\nelse\r\n#endif\r\nsha512_transform_asm = sha512_transform_avx;\r\n}\r\n#endif\r\nif (sha512_transform_asm) {\r\n#ifdef CONFIG_AS_AVX\r\nif (sha512_transform_asm == sha512_transform_avx)\r\npr_info("Using AVX optimized SHA-512 implementation\n");\r\n#ifdef CONFIG_AS_AVX2\r\nelse if (sha512_transform_asm == sha512_transform_rorx)\r\npr_info("Using AVX2 optimized SHA-512 implementation\n");\r\n#endif\r\nelse\r\n#endif\r\npr_info("Using SSSE3 optimized SHA-512 implementation\n");\r\nreturn crypto_register_shashes(algs, ARRAY_SIZE(algs));\r\n}\r\npr_info("Neither AVX nor SSSE3 is available/usable.\n");\r\nreturn -ENODEV;\r\n}\r\nstatic void __exit sha512_ssse3_mod_fini(void)\r\n{\r\ncrypto_unregister_shashes(algs, ARRAY_SIZE(algs));\r\n}
