void\r\nnouveau_vm_map_at(struct nouveau_vma *vma, u64 delta, struct nouveau_mem *node)\r\n{\r\nstruct nouveau_vm *vm = vma->vm;\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nstruct nouveau_mm_node *r;\r\nint big = vma->node->type != vmm->spg_shift;\r\nu32 offset = vma->node->offset + (delta >> 12);\r\nu32 bits = vma->node->type - 12;\r\nu32 pde = (offset >> vmm->pgt_bits) - vm->fpde;\r\nu32 pte = (offset & ((1 << vmm->pgt_bits) - 1)) >> bits;\r\nu32 max = 1 << (vmm->pgt_bits - bits);\r\nu32 end, len;\r\ndelta = 0;\r\nlist_for_each_entry(r, &node->regions, rl_entry) {\r\nu64 phys = (u64)r->offset << 12;\r\nu32 num = r->length >> bits;\r\nwhile (num) {\r\nstruct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];\r\nend = (pte + num);\r\nif (unlikely(end >= max))\r\nend = max;\r\nlen = end - pte;\r\nvmm->map(vma, pgt, node, pte, len, phys, delta);\r\nnum -= len;\r\npte += len;\r\nif (unlikely(end >= max)) {\r\nphys += len << (bits + 12);\r\npde++;\r\npte = 0;\r\n}\r\ndelta += (u64)len << vma->node->type;\r\n}\r\n}\r\nvmm->flush(vm);\r\n}\r\nvoid\r\nnouveau_vm_map(struct nouveau_vma *vma, struct nouveau_mem *node)\r\n{\r\nnouveau_vm_map_at(vma, 0, node);\r\n}\r\nvoid\r\nnouveau_vm_map_sg_table(struct nouveau_vma *vma, u64 delta, u64 length,\r\nstruct nouveau_mem *mem)\r\n{\r\nstruct nouveau_vm *vm = vma->vm;\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nint big = vma->node->type != vmm->spg_shift;\r\nu32 offset = vma->node->offset + (delta >> 12);\r\nu32 bits = vma->node->type - 12;\r\nu32 num = length >> vma->node->type;\r\nu32 pde = (offset >> vmm->pgt_bits) - vm->fpde;\r\nu32 pte = (offset & ((1 << vmm->pgt_bits) - 1)) >> bits;\r\nu32 max = 1 << (vmm->pgt_bits - bits);\r\nunsigned m, sglen;\r\nu32 end, len;\r\nint i;\r\nstruct scatterlist *sg;\r\nfor_each_sg(mem->sg->sgl, sg, mem->sg->nents, i) {\r\nstruct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];\r\nsglen = sg_dma_len(sg) >> PAGE_SHIFT;\r\nend = pte + sglen;\r\nif (unlikely(end >= max))\r\nend = max;\r\nlen = end - pte;\r\nfor (m = 0; m < len; m++) {\r\ndma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);\r\nvmm->map_sg(vma, pgt, mem, pte, 1, &addr);\r\nnum--;\r\npte++;\r\nif (num == 0)\r\ngoto finish;\r\n}\r\nif (unlikely(end >= max)) {\r\npde++;\r\npte = 0;\r\n}\r\nif (m < sglen) {\r\nfor (; m < sglen; m++) {\r\ndma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);\r\nvmm->map_sg(vma, pgt, mem, pte, 1, &addr);\r\nnum--;\r\npte++;\r\nif (num == 0)\r\ngoto finish;\r\n}\r\n}\r\n}\r\nfinish:\r\nvmm->flush(vm);\r\n}\r\nvoid\r\nnouveau_vm_map_sg(struct nouveau_vma *vma, u64 delta, u64 length,\r\nstruct nouveau_mem *mem)\r\n{\r\nstruct nouveau_vm *vm = vma->vm;\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\ndma_addr_t *list = mem->pages;\r\nint big = vma->node->type != vmm->spg_shift;\r\nu32 offset = vma->node->offset + (delta >> 12);\r\nu32 bits = vma->node->type - 12;\r\nu32 num = length >> vma->node->type;\r\nu32 pde = (offset >> vmm->pgt_bits) - vm->fpde;\r\nu32 pte = (offset & ((1 << vmm->pgt_bits) - 1)) >> bits;\r\nu32 max = 1 << (vmm->pgt_bits - bits);\r\nu32 end, len;\r\nwhile (num) {\r\nstruct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];\r\nend = (pte + num);\r\nif (unlikely(end >= max))\r\nend = max;\r\nlen = end - pte;\r\nvmm->map_sg(vma, pgt, mem, pte, len, list);\r\nnum -= len;\r\npte += len;\r\nlist += len;\r\nif (unlikely(end >= max)) {\r\npde++;\r\npte = 0;\r\n}\r\n}\r\nvmm->flush(vm);\r\n}\r\nvoid\r\nnouveau_vm_unmap_at(struct nouveau_vma *vma, u64 delta, u64 length)\r\n{\r\nstruct nouveau_vm *vm = vma->vm;\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nint big = vma->node->type != vmm->spg_shift;\r\nu32 offset = vma->node->offset + (delta >> 12);\r\nu32 bits = vma->node->type - 12;\r\nu32 num = length >> vma->node->type;\r\nu32 pde = (offset >> vmm->pgt_bits) - vm->fpde;\r\nu32 pte = (offset & ((1 << vmm->pgt_bits) - 1)) >> bits;\r\nu32 max = 1 << (vmm->pgt_bits - bits);\r\nu32 end, len;\r\nwhile (num) {\r\nstruct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];\r\nend = (pte + num);\r\nif (unlikely(end >= max))\r\nend = max;\r\nlen = end - pte;\r\nvmm->unmap(pgt, pte, len);\r\nnum -= len;\r\npte += len;\r\nif (unlikely(end >= max)) {\r\npde++;\r\npte = 0;\r\n}\r\n}\r\nvmm->flush(vm);\r\n}\r\nvoid\r\nnouveau_vm_unmap(struct nouveau_vma *vma)\r\n{\r\nnouveau_vm_unmap_at(vma, 0, (u64)vma->node->length << 12);\r\n}\r\nstatic void\r\nnouveau_vm_unmap_pgt(struct nouveau_vm *vm, int big, u32 fpde, u32 lpde)\r\n{\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nstruct nouveau_vm_pgd *vpgd;\r\nstruct nouveau_vm_pgt *vpgt;\r\nstruct nouveau_gpuobj *pgt;\r\nu32 pde;\r\nfor (pde = fpde; pde <= lpde; pde++) {\r\nvpgt = &vm->pgt[pde - vm->fpde];\r\nif (--vpgt->refcount[big])\r\ncontinue;\r\npgt = vpgt->obj[big];\r\nvpgt->obj[big] = NULL;\r\nlist_for_each_entry(vpgd, &vm->pgd_list, head) {\r\nvmm->map_pgt(vpgd->obj, pde, vpgt->obj);\r\n}\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nnouveau_gpuobj_ref(NULL, &pgt);\r\nmutex_lock(&nv_subdev(vmm)->mutex);\r\n}\r\n}\r\nstatic int\r\nnouveau_vm_map_pgt(struct nouveau_vm *vm, u32 pde, u32 type)\r\n{\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nstruct nouveau_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];\r\nstruct nouveau_vm_pgd *vpgd;\r\nstruct nouveau_gpuobj *pgt;\r\nint big = (type != vmm->spg_shift);\r\nu32 pgt_size;\r\nint ret;\r\npgt_size = (1 << (vmm->pgt_bits + 12)) >> type;\r\npgt_size *= 8;\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nret = nouveau_gpuobj_new(nv_object(vm->vmm), NULL, pgt_size, 0x1000,\r\nNVOBJ_FLAG_ZERO_ALLOC, &pgt);\r\nmutex_lock(&nv_subdev(vmm)->mutex);\r\nif (unlikely(ret))\r\nreturn ret;\r\nif (unlikely(vpgt->refcount[big]++)) {\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nnouveau_gpuobj_ref(NULL, &pgt);\r\nmutex_lock(&nv_subdev(vmm)->mutex);\r\nreturn 0;\r\n}\r\nvpgt->obj[big] = pgt;\r\nlist_for_each_entry(vpgd, &vm->pgd_list, head) {\r\nvmm->map_pgt(vpgd->obj, pde, vpgt->obj);\r\n}\r\nreturn 0;\r\n}\r\nint\r\nnouveau_vm_get(struct nouveau_vm *vm, u64 size, u32 page_shift,\r\nu32 access, struct nouveau_vma *vma)\r\n{\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nu32 align = (1 << page_shift) >> 12;\r\nu32 msize = size >> 12;\r\nu32 fpde, lpde, pde;\r\nint ret;\r\nmutex_lock(&nv_subdev(vmm)->mutex);\r\nret = nouveau_mm_head(&vm->mm, page_shift, msize, msize, align,\r\n&vma->node);\r\nif (unlikely(ret != 0)) {\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nreturn ret;\r\n}\r\nfpde = (vma->node->offset >> vmm->pgt_bits);\r\nlpde = (vma->node->offset + vma->node->length - 1) >> vmm->pgt_bits;\r\nfor (pde = fpde; pde <= lpde; pde++) {\r\nstruct nouveau_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];\r\nint big = (vma->node->type != vmm->spg_shift);\r\nif (likely(vpgt->refcount[big])) {\r\nvpgt->refcount[big]++;\r\ncontinue;\r\n}\r\nret = nouveau_vm_map_pgt(vm, pde, vma->node->type);\r\nif (ret) {\r\nif (pde != fpde)\r\nnouveau_vm_unmap_pgt(vm, big, fpde, pde - 1);\r\nnouveau_mm_free(&vm->mm, &vma->node);\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nreturn ret;\r\n}\r\n}\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nvma->vm = NULL;\r\nnouveau_vm_ref(vm, &vma->vm, NULL);\r\nvma->offset = (u64)vma->node->offset << 12;\r\nvma->access = access;\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_vm_put(struct nouveau_vma *vma)\r\n{\r\nstruct nouveau_vm *vm = vma->vm;\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nu32 fpde, lpde;\r\nif (unlikely(vma->node == NULL))\r\nreturn;\r\nfpde = (vma->node->offset >> vmm->pgt_bits);\r\nlpde = (vma->node->offset + vma->node->length - 1) >> vmm->pgt_bits;\r\nmutex_lock(&nv_subdev(vmm)->mutex);\r\nnouveau_vm_unmap_pgt(vm, vma->node->type != vmm->spg_shift, fpde, lpde);\r\nnouveau_mm_free(&vm->mm, &vma->node);\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nnouveau_vm_ref(NULL, &vma->vm, NULL);\r\n}\r\nint\r\nnouveau_vm_create(struct nouveau_vmmgr *vmm, u64 offset, u64 length,\r\nu64 mm_offset, u32 block, struct nouveau_vm **pvm)\r\n{\r\nstruct nouveau_vm *vm;\r\nu64 mm_length = (offset + length) - mm_offset;\r\nint ret;\r\nvm = kzalloc(sizeof(*vm), GFP_KERNEL);\r\nif (!vm)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&vm->pgd_list);\r\nvm->vmm = vmm;\r\nkref_init(&vm->refcount);\r\nvm->fpde = offset >> (vmm->pgt_bits + 12);\r\nvm->lpde = (offset + length - 1) >> (vmm->pgt_bits + 12);\r\nvm->pgt = vzalloc((vm->lpde - vm->fpde + 1) * sizeof(*vm->pgt));\r\nif (!vm->pgt) {\r\nkfree(vm);\r\nreturn -ENOMEM;\r\n}\r\nret = nouveau_mm_init(&vm->mm, mm_offset >> 12, mm_length >> 12,\r\nblock >> 12);\r\nif (ret) {\r\nvfree(vm->pgt);\r\nkfree(vm);\r\nreturn ret;\r\n}\r\n*pvm = vm;\r\nreturn 0;\r\n}\r\nint\r\nnouveau_vm_new(struct nouveau_device *device, u64 offset, u64 length,\r\nu64 mm_offset, struct nouveau_vm **pvm)\r\n{\r\nstruct nouveau_vmmgr *vmm = nouveau_vmmgr(device);\r\nreturn vmm->create(vmm, offset, length, mm_offset, pvm);\r\n}\r\nstatic int\r\nnouveau_vm_link(struct nouveau_vm *vm, struct nouveau_gpuobj *pgd)\r\n{\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nstruct nouveau_vm_pgd *vpgd;\r\nint i;\r\nif (!pgd)\r\nreturn 0;\r\nvpgd = kzalloc(sizeof(*vpgd), GFP_KERNEL);\r\nif (!vpgd)\r\nreturn -ENOMEM;\r\nnouveau_gpuobj_ref(pgd, &vpgd->obj);\r\nmutex_lock(&nv_subdev(vmm)->mutex);\r\nfor (i = vm->fpde; i <= vm->lpde; i++)\r\nvmm->map_pgt(pgd, i, vm->pgt[i - vm->fpde].obj);\r\nlist_add(&vpgd->head, &vm->pgd_list);\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_vm_unlink(struct nouveau_vm *vm, struct nouveau_gpuobj *mpgd)\r\n{\r\nstruct nouveau_vmmgr *vmm = vm->vmm;\r\nstruct nouveau_vm_pgd *vpgd, *tmp;\r\nstruct nouveau_gpuobj *pgd = NULL;\r\nif (!mpgd)\r\nreturn;\r\nmutex_lock(&nv_subdev(vmm)->mutex);\r\nlist_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {\r\nif (vpgd->obj == mpgd) {\r\npgd = vpgd->obj;\r\nlist_del(&vpgd->head);\r\nkfree(vpgd);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&nv_subdev(vmm)->mutex);\r\nnouveau_gpuobj_ref(NULL, &pgd);\r\n}\r\nstatic void\r\nnouveau_vm_del(struct kref *kref)\r\n{\r\nstruct nouveau_vm *vm = container_of(kref, typeof(*vm), refcount);\r\nstruct nouveau_vm_pgd *vpgd, *tmp;\r\nlist_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {\r\nnouveau_vm_unlink(vm, vpgd->obj);\r\n}\r\nnouveau_mm_fini(&vm->mm);\r\nvfree(vm->pgt);\r\nkfree(vm);\r\n}\r\nint\r\nnouveau_vm_ref(struct nouveau_vm *ref, struct nouveau_vm **ptr,\r\nstruct nouveau_gpuobj *pgd)\r\n{\r\nif (ref) {\r\nint ret = nouveau_vm_link(ref, pgd);\r\nif (ret)\r\nreturn ret;\r\nkref_get(&ref->refcount);\r\n}\r\nif (*ptr) {\r\nnouveau_vm_unlink(*ptr, pgd);\r\nkref_put(&(*ptr)->refcount, nouveau_vm_del);\r\n}\r\n*ptr = ref;\r\nreturn 0;\r\n}
