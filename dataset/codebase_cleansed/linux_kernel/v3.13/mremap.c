static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npgd = pgd_offset(mm, addr);\r\nif (pgd_none_or_clear_bad(pgd))\r\nreturn NULL;\r\npud = pud_offset(pgd, addr);\r\nif (pud_none_or_clear_bad(pud))\r\nreturn NULL;\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd))\r\nreturn NULL;\r\nreturn pmd;\r\n}\r\nstatic pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,\r\nunsigned long addr)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npgd = pgd_offset(mm, addr);\r\npud = pud_alloc(mm, pgd, addr);\r\nif (!pud)\r\nreturn NULL;\r\npmd = pmd_alloc(mm, pud, addr);\r\nif (!pmd)\r\nreturn NULL;\r\nVM_BUG_ON(pmd_trans_huge(*pmd));\r\nreturn pmd;\r\n}\r\nstatic pte_t move_soft_dirty_pte(pte_t pte)\r\n{\r\n#ifdef CONFIG_MEM_SOFT_DIRTY\r\nif (pte_present(pte))\r\npte = pte_mksoft_dirty(pte);\r\nelse if (is_swap_pte(pte))\r\npte = pte_swp_mksoft_dirty(pte);\r\nelse if (pte_file(pte))\r\npte = pte_file_mksoft_dirty(pte);\r\n#endif\r\nreturn pte;\r\n}\r\nstatic void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\r\nunsigned long old_addr, unsigned long old_end,\r\nstruct vm_area_struct *new_vma, pmd_t *new_pmd,\r\nunsigned long new_addr, bool need_rmap_locks)\r\n{\r\nstruct address_space *mapping = NULL;\r\nstruct anon_vma *anon_vma = NULL;\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *old_pte, *new_pte, pte;\r\nspinlock_t *old_ptl, *new_ptl;\r\nif (need_rmap_locks) {\r\nif (vma->vm_file) {\r\nmapping = vma->vm_file->f_mapping;\r\nmutex_lock(&mapping->i_mmap_mutex);\r\n}\r\nif (vma->anon_vma) {\r\nanon_vma = vma->anon_vma;\r\nanon_vma_lock_write(anon_vma);\r\n}\r\n}\r\nold_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);\r\nnew_pte = pte_offset_map(new_pmd, new_addr);\r\nnew_ptl = pte_lockptr(mm, new_pmd);\r\nif (new_ptl != old_ptl)\r\nspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\r\narch_enter_lazy_mmu_mode();\r\nfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,\r\nnew_pte++, new_addr += PAGE_SIZE) {\r\nif (pte_none(*old_pte))\r\ncontinue;\r\npte = ptep_get_and_clear(mm, old_addr, old_pte);\r\npte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\r\npte = move_soft_dirty_pte(pte);\r\nset_pte_at(mm, new_addr, new_pte, pte);\r\n}\r\narch_leave_lazy_mmu_mode();\r\nif (new_ptl != old_ptl)\r\nspin_unlock(new_ptl);\r\npte_unmap(new_pte - 1);\r\npte_unmap_unlock(old_pte - 1, old_ptl);\r\nif (anon_vma)\r\nanon_vma_unlock_write(anon_vma);\r\nif (mapping)\r\nmutex_unlock(&mapping->i_mmap_mutex);\r\n}\r\nunsigned long move_page_tables(struct vm_area_struct *vma,\r\nunsigned long old_addr, struct vm_area_struct *new_vma,\r\nunsigned long new_addr, unsigned long len,\r\nbool need_rmap_locks)\r\n{\r\nunsigned long extent, next, old_end;\r\npmd_t *old_pmd, *new_pmd;\r\nbool need_flush = false;\r\nunsigned long mmun_start;\r\nunsigned long mmun_end;\r\nold_end = old_addr + len;\r\nflush_cache_range(vma, old_addr, old_end);\r\nmmun_start = old_addr;\r\nmmun_end = old_end;\r\nmmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);\r\nfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\r\ncond_resched();\r\nnext = (old_addr + PMD_SIZE) & PMD_MASK;\r\nextent = next - old_addr;\r\nif (extent > old_end - old_addr)\r\nextent = old_end - old_addr;\r\nold_pmd = get_old_pmd(vma->vm_mm, old_addr);\r\nif (!old_pmd)\r\ncontinue;\r\nnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\r\nif (!new_pmd)\r\nbreak;\r\nif (pmd_trans_huge(*old_pmd)) {\r\nint err = 0;\r\nif (extent == HPAGE_PMD_SIZE)\r\nerr = move_huge_pmd(vma, new_vma, old_addr,\r\nnew_addr, old_end,\r\nold_pmd, new_pmd);\r\nif (err > 0) {\r\nneed_flush = true;\r\ncontinue;\r\n} else if (!err) {\r\nsplit_huge_page_pmd(vma, old_addr, old_pmd);\r\n}\r\nVM_BUG_ON(pmd_trans_huge(*old_pmd));\r\n}\r\nif (pmd_none(*new_pmd) && __pte_alloc(new_vma->vm_mm, new_vma,\r\nnew_pmd, new_addr))\r\nbreak;\r\nnext = (new_addr + PMD_SIZE) & PMD_MASK;\r\nif (extent > next - new_addr)\r\nextent = next - new_addr;\r\nif (extent > LATENCY_LIMIT)\r\nextent = LATENCY_LIMIT;\r\nmove_ptes(vma, old_pmd, old_addr, old_addr + extent,\r\nnew_vma, new_pmd, new_addr, need_rmap_locks);\r\nneed_flush = true;\r\n}\r\nif (likely(need_flush))\r\nflush_tlb_range(vma, old_end-len, old_addr);\r\nmmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);\r\nreturn len + old_addr - old_end;\r\n}\r\nstatic unsigned long move_vma(struct vm_area_struct *vma,\r\nunsigned long old_addr, unsigned long old_len,\r\nunsigned long new_len, unsigned long new_addr, bool *locked)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct vm_area_struct *new_vma;\r\nunsigned long vm_flags = vma->vm_flags;\r\nunsigned long new_pgoff;\r\nunsigned long moved_len;\r\nunsigned long excess = 0;\r\nunsigned long hiwater_vm;\r\nint split = 0;\r\nint err;\r\nbool need_rmap_locks;\r\nif (mm->map_count >= sysctl_max_map_count - 3)\r\nreturn -ENOMEM;\r\nerr = ksm_madvise(vma, old_addr, old_addr + old_len,\r\nMADV_UNMERGEABLE, &vm_flags);\r\nif (err)\r\nreturn err;\r\nnew_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);\r\nnew_vma = copy_vma(&vma, new_addr, new_len, new_pgoff,\r\n&need_rmap_locks);\r\nif (!new_vma)\r\nreturn -ENOMEM;\r\nmoved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len,\r\nneed_rmap_locks);\r\nif (moved_len < old_len) {\r\nmove_page_tables(new_vma, new_addr, vma, old_addr, moved_len,\r\ntrue);\r\nvma = new_vma;\r\nold_len = new_len;\r\nold_addr = new_addr;\r\nnew_addr = -ENOMEM;\r\n}\r\nif (vm_flags & VM_ACCOUNT) {\r\nvma->vm_flags &= ~VM_ACCOUNT;\r\nexcess = vma->vm_end - vma->vm_start - old_len;\r\nif (old_addr > vma->vm_start &&\r\nold_addr + old_len < vma->vm_end)\r\nsplit = 1;\r\n}\r\nhiwater_vm = mm->hiwater_vm;\r\nvm_stat_account(mm, vma->vm_flags, vma->vm_file, new_len>>PAGE_SHIFT);\r\nif (do_munmap(mm, old_addr, old_len) < 0) {\r\nvm_unacct_memory(excess >> PAGE_SHIFT);\r\nexcess = 0;\r\n}\r\nmm->hiwater_vm = hiwater_vm;\r\nif (excess) {\r\nvma->vm_flags |= VM_ACCOUNT;\r\nif (split)\r\nvma->vm_next->vm_flags |= VM_ACCOUNT;\r\n}\r\nif (vm_flags & VM_LOCKED) {\r\nmm->locked_vm += new_len >> PAGE_SHIFT;\r\n*locked = true;\r\n}\r\nreturn new_addr;\r\n}\r\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\r\nunsigned long old_len, unsigned long new_len, unsigned long *p)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma = find_vma(mm, addr);\r\nif (!vma || vma->vm_start > addr)\r\ngoto Efault;\r\nif (is_vm_hugetlb_page(vma))\r\ngoto Einval;\r\nif (old_len > vma->vm_end - addr)\r\ngoto Efault;\r\nif (new_len > old_len) {\r\nunsigned long pgoff;\r\nif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\r\ngoto Efault;\r\npgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\r\npgoff += vma->vm_pgoff;\r\nif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\r\ngoto Einval;\r\n}\r\nif (vma->vm_flags & VM_LOCKED) {\r\nunsigned long locked, lock_limit;\r\nlocked = mm->locked_vm << PAGE_SHIFT;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK);\r\nlocked += new_len - old_len;\r\nif (locked > lock_limit && !capable(CAP_IPC_LOCK))\r\ngoto Eagain;\r\n}\r\nif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\r\ngoto Enomem;\r\nif (vma->vm_flags & VM_ACCOUNT) {\r\nunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\r\nif (security_vm_enough_memory_mm(mm, charged))\r\ngoto Efault;\r\n*p = charged;\r\n}\r\nreturn vma;\r\nEfault:\r\nreturn ERR_PTR(-EFAULT);\r\nEinval:\r\nreturn ERR_PTR(-EINVAL);\r\nEnomem:\r\nreturn ERR_PTR(-ENOMEM);\r\nEagain:\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nstatic unsigned long mremap_to(unsigned long addr, unsigned long old_len,\r\nunsigned long new_addr, unsigned long new_len, bool *locked)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nunsigned long ret = -EINVAL;\r\nunsigned long charged = 0;\r\nunsigned long map_flags;\r\nif (new_addr & ~PAGE_MASK)\r\ngoto out;\r\nif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\r\ngoto out;\r\nif ((new_addr <= addr) && (new_addr+new_len) > addr)\r\ngoto out;\r\nif ((addr <= new_addr) && (addr+old_len) > new_addr)\r\ngoto out;\r\nret = do_munmap(mm, new_addr, new_len);\r\nif (ret)\r\ngoto out;\r\nif (old_len >= new_len) {\r\nret = do_munmap(mm, addr+new_len, old_len - new_len);\r\nif (ret && old_len != new_len)\r\ngoto out;\r\nold_len = new_len;\r\n}\r\nvma = vma_to_resize(addr, old_len, new_len, &charged);\r\nif (IS_ERR(vma)) {\r\nret = PTR_ERR(vma);\r\ngoto out;\r\n}\r\nmap_flags = MAP_FIXED;\r\nif (vma->vm_flags & VM_MAYSHARE)\r\nmap_flags |= MAP_SHARED;\r\nret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\r\n((addr - vma->vm_start) >> PAGE_SHIFT),\r\nmap_flags);\r\nif (ret & ~PAGE_MASK)\r\ngoto out1;\r\nret = move_vma(vma, addr, old_len, new_len, new_addr, locked);\r\nif (!(ret & ~PAGE_MASK))\r\ngoto out;\r\nout1:\r\nvm_unacct_memory(charged);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int vma_expandable(struct vm_area_struct *vma, unsigned long delta)\r\n{\r\nunsigned long end = vma->vm_end + delta;\r\nif (end < vma->vm_end)\r\nreturn 0;\r\nif (vma->vm_next && vma->vm_next->vm_start < end)\r\nreturn 0;\r\nif (get_unmapped_area(NULL, vma->vm_start, end - vma->vm_start,\r\n0, MAP_FIXED) & ~PAGE_MASK)\r\nreturn 0;\r\nreturn 1;\r\n}
