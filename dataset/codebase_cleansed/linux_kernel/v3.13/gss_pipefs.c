static int gss_sec_pipe_upcall_init(struct gss_sec *gsec)\r\n{\r\nreturn 0;\r\n}\r\nstatic void gss_sec_pipe_upcall_fini(struct gss_sec *gsec)\r\n{\r\n}\r\nstatic\r\nstruct ptlrpc_cli_ctx *ctx_create_pf(struct ptlrpc_sec *sec,\r\nstruct vfs_cred *vcred)\r\n{\r\nstruct gss_cli_ctx *gctx;\r\nint rc;\r\nOBD_ALLOC_PTR(gctx);\r\nif (gctx == NULL)\r\nreturn NULL;\r\nrc = gss_cli_ctx_init_common(sec, &gctx->gc_base,\r\n&gss_pipefs_ctxops, vcred);\r\nif (rc) {\r\nOBD_FREE_PTR(gctx);\r\nreturn NULL;\r\n}\r\nreturn &gctx->gc_base;\r\n}\r\nstatic\r\nvoid ctx_destroy_pf(struct ptlrpc_sec *sec, struct ptlrpc_cli_ctx *ctx)\r\n{\r\nstruct gss_cli_ctx *gctx = ctx2gctx(ctx);\r\nif (gss_cli_ctx_fini_common(sec, ctx))\r\nreturn;\r\nOBD_FREE_PTR(gctx);\r\natomic_dec(&sec->ps_nctx);\r\nsptlrpc_sec_put(sec);\r\n}\r\nstatic\r\nvoid ctx_enhash_pf(struct ptlrpc_cli_ctx *ctx, struct hlist_head *hash)\r\n{\r\nset_bit(PTLRPC_CTX_CACHED_BIT, &ctx->cc_flags);\r\natomic_inc(&ctx->cc_refcount);\r\nhlist_add_head(&ctx->cc_cache, hash);\r\n}\r\nstatic\r\nvoid ctx_unhash_pf(struct ptlrpc_cli_ctx *ctx, struct hlist_head *freelist)\r\n{\r\nLASSERT(spin_is_locked(&ctx->cc_sec->ps_lock));\r\nLASSERT(atomic_read(&ctx->cc_refcount) > 0);\r\nLASSERT(test_bit(PTLRPC_CTX_CACHED_BIT, &ctx->cc_flags));\r\nLASSERT(!hlist_unhashed(&ctx->cc_cache));\r\nclear_bit(PTLRPC_CTX_CACHED_BIT, &ctx->cc_flags);\r\nif (atomic_dec_and_test(&ctx->cc_refcount)) {\r\n__hlist_del(&ctx->cc_cache);\r\nhlist_add_head(&ctx->cc_cache, freelist);\r\n} else {\r\nhlist_del_init(&ctx->cc_cache);\r\n}\r\n}\r\nstatic\r\nint ctx_check_death_pf(struct ptlrpc_cli_ctx *ctx,\r\nstruct hlist_head *freelist)\r\n{\r\nif (cli_ctx_check_death(ctx)) {\r\nif (freelist)\r\nctx_unhash_pf(ctx, freelist);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline\r\nint ctx_check_death_locked_pf(struct ptlrpc_cli_ctx *ctx,\r\nstruct hlist_head *freelist)\r\n{\r\nLASSERT(ctx->cc_sec);\r\nLASSERT(atomic_read(&ctx->cc_refcount) > 0);\r\nLASSERT(test_bit(PTLRPC_CTX_CACHED_BIT, &ctx->cc_flags));\r\nreturn ctx_check_death_pf(ctx, freelist);\r\n}\r\nstatic inline\r\nint ctx_match_pf(struct ptlrpc_cli_ctx *ctx, struct vfs_cred *vcred)\r\n{\r\nif (!ctx->cc_ops->match)\r\nreturn 1;\r\nreturn ctx->cc_ops->match(ctx, vcred);\r\n}\r\nstatic\r\nvoid ctx_list_destroy_pf(struct hlist_head *head)\r\n{\r\nstruct ptlrpc_cli_ctx *ctx;\r\nwhile (!hlist_empty(head)) {\r\nctx = hlist_entry(head->first, struct ptlrpc_cli_ctx,\r\ncc_cache);\r\nLASSERT(atomic_read(&ctx->cc_refcount) == 0);\r\nLASSERT(test_bit(PTLRPC_CTX_CACHED_BIT,\r\n&ctx->cc_flags) == 0);\r\nhlist_del_init(&ctx->cc_cache);\r\nctx_destroy_pf(ctx->cc_sec, ctx);\r\n}\r\n}\r\nstatic\r\nint gss_cli_ctx_validate_pf(struct ptlrpc_cli_ctx *ctx)\r\n{\r\nif (ctx_check_death_pf(ctx, NULL))\r\nreturn 1;\r\nif (cli_ctx_is_ready(ctx))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic\r\nvoid gss_cli_ctx_die_pf(struct ptlrpc_cli_ctx *ctx, int grace)\r\n{\r\nLASSERT(ctx->cc_sec);\r\nLASSERT(atomic_read(&ctx->cc_refcount) > 0);\r\ncli_ctx_expire(ctx);\r\nspin_lock(&ctx->cc_sec->ps_lock);\r\nif (test_and_clear_bit(PTLRPC_CTX_CACHED_BIT, &ctx->cc_flags)) {\r\nLASSERT(!hlist_unhashed(&ctx->cc_cache));\r\nLASSERT(atomic_read(&ctx->cc_refcount) > 1);\r\nhlist_del_init(&ctx->cc_cache);\r\nif (atomic_dec_and_test(&ctx->cc_refcount))\r\nLBUG();\r\n}\r\nspin_unlock(&ctx->cc_sec->ps_lock);\r\n}\r\nstatic inline\r\nunsigned int ctx_hash_index(int hashsize, __u64 key)\r\n{\r\nreturn (unsigned int) (key & ((__u64) hashsize - 1));\r\n}\r\nstatic\r\nvoid gss_sec_ctx_replace_pf(struct gss_sec *gsec,\r\nstruct ptlrpc_cli_ctx *new)\r\n{\r\nstruct gss_sec_pipefs *gsec_pf;\r\nstruct ptlrpc_cli_ctx *ctx;\r\nstruct hlist_node *next;\r\nHLIST_HEAD(freelist);\r\nunsigned int hash;\r\ngsec_pf = container_of(gsec, struct gss_sec_pipefs, gsp_base);\r\nhash = ctx_hash_index(gsec_pf->gsp_chash_size,\r\n(__u64) new->cc_vcred.vc_uid);\r\nLASSERT(hash < gsec_pf->gsp_chash_size);\r\nspin_lock(&gsec->gs_base.ps_lock);\r\nhlist_for_each_entry_safe(ctx, next,\r\n&gsec_pf->gsp_chash[hash], cc_cache) {\r\nif (!ctx_match_pf(ctx, &new->cc_vcred))\r\ncontinue;\r\ncli_ctx_expire(ctx);\r\nctx_unhash_pf(ctx, &freelist);\r\nbreak;\r\n}\r\nctx_enhash_pf(new, &gsec_pf->gsp_chash[hash]);\r\nspin_unlock(&gsec->gs_base.ps_lock);\r\nctx_list_destroy_pf(&freelist);\r\n}\r\nstatic\r\nint gss_install_rvs_cli_ctx_pf(struct gss_sec *gsec,\r\nstruct ptlrpc_svc_ctx *svc_ctx)\r\n{\r\nstruct vfs_cred vcred;\r\nstruct ptlrpc_cli_ctx *cli_ctx;\r\nint rc;\r\nvcred.vc_uid = 0;\r\nvcred.vc_gid = 0;\r\ncli_ctx = ctx_create_pf(&gsec->gs_base, &vcred);\r\nif (!cli_ctx)\r\nreturn -ENOMEM;\r\nrc = gss_copy_rvc_cli_ctx(cli_ctx, svc_ctx);\r\nif (rc) {\r\nctx_destroy_pf(cli_ctx->cc_sec, cli_ctx);\r\nreturn rc;\r\n}\r\ngss_sec_ctx_replace_pf(gsec, cli_ctx);\r\nreturn 0;\r\n}\r\nstatic\r\nvoid gss_ctx_cache_gc_pf(struct gss_sec_pipefs *gsec_pf,\r\nstruct hlist_head *freelist)\r\n{\r\nstruct ptlrpc_sec *sec;\r\nstruct ptlrpc_cli_ctx *ctx;\r\nstruct hlist_node *next;\r\nint i;\r\nsec = &gsec_pf->gsp_base.gs_base;\r\nCDEBUG(D_SEC, "do gc on sec %s@%p\n", sec->ps_policy->sp_name, sec);\r\nfor (i = 0; i < gsec_pf->gsp_chash_size; i++) {\r\nhlist_for_each_entry_safe(ctx, next,\r\n&gsec_pf->gsp_chash[i], cc_cache)\r\nctx_check_death_locked_pf(ctx, freelist);\r\n}\r\nsec->ps_gc_next = cfs_time_current_sec() + sec->ps_gc_interval;\r\n}\r\nstatic\r\nstruct ptlrpc_sec* gss_sec_create_pf(struct obd_import *imp,\r\nstruct ptlrpc_svc_ctx *ctx,\r\nstruct sptlrpc_flavor *sf)\r\n{\r\nstruct gss_sec_pipefs *gsec_pf;\r\nint alloc_size, hash_size, i;\r\n#define GSS_SEC_PIPEFS_CTX_HASH_SIZE (32)\r\nif (ctx ||\r\nsf->sf_flags & (PTLRPC_SEC_FL_ROOTONLY | PTLRPC_SEC_FL_REVERSE))\r\nhash_size = 1;\r\nelse\r\nhash_size = GSS_SEC_PIPEFS_CTX_HASH_SIZE;\r\nalloc_size = sizeof(*gsec_pf) +\r\nsizeof(struct hlist_head) * hash_size;\r\nOBD_ALLOC(gsec_pf, alloc_size);\r\nif (!gsec_pf)\r\nreturn NULL;\r\ngsec_pf->gsp_chash_size = hash_size;\r\nfor (i = 0; i < hash_size; i++)\r\nINIT_HLIST_HEAD(&gsec_pf->gsp_chash[i]);\r\nif (gss_sec_create_common(&gsec_pf->gsp_base, &gss_policy_pipefs,\r\nimp, ctx, sf))\r\ngoto err_free;\r\nif (ctx == NULL) {\r\nif (gss_sec_pipe_upcall_init(&gsec_pf->gsp_base))\r\ngoto err_destroy;\r\n} else {\r\nif (gss_install_rvs_cli_ctx_pf(&gsec_pf->gsp_base, ctx))\r\ngoto err_destroy;\r\n}\r\nreturn &gsec_pf->gsp_base.gs_base;\r\nerr_destroy:\r\ngss_sec_destroy_common(&gsec_pf->gsp_base);\r\nerr_free:\r\nOBD_FREE(gsec_pf, alloc_size);\r\nreturn NULL;\r\n}\r\nstatic\r\nvoid gss_sec_destroy_pf(struct ptlrpc_sec *sec)\r\n{\r\nstruct gss_sec_pipefs *gsec_pf;\r\nstruct gss_sec *gsec;\r\nCWARN("destroy %s@%p\n", sec->ps_policy->sp_name, sec);\r\ngsec = container_of(sec, struct gss_sec, gs_base);\r\ngsec_pf = container_of(gsec, struct gss_sec_pipefs, gsp_base);\r\nLASSERT(gsec_pf->gsp_chash);\r\nLASSERT(gsec_pf->gsp_chash_size);\r\ngss_sec_pipe_upcall_fini(gsec);\r\ngss_sec_destroy_common(gsec);\r\nOBD_FREE(gsec, sizeof(*gsec_pf) +\r\nsizeof(struct hlist_head) * gsec_pf->gsp_chash_size);\r\n}\r\nstatic\r\nstruct ptlrpc_cli_ctx * gss_sec_lookup_ctx_pf(struct ptlrpc_sec *sec,\r\nstruct vfs_cred *vcred,\r\nint create, int remove_dead)\r\n{\r\nstruct gss_sec *gsec;\r\nstruct gss_sec_pipefs *gsec_pf;\r\nstruct ptlrpc_cli_ctx *ctx = NULL, *new = NULL;\r\nstruct hlist_head *hash_head;\r\nstruct hlist_node *next;\r\nHLIST_HEAD(freelist);\r\nunsigned int hash, gc = 0, found = 0;\r\nmight_sleep();\r\ngsec = container_of(sec, struct gss_sec, gs_base);\r\ngsec_pf = container_of(gsec, struct gss_sec_pipefs, gsp_base);\r\nhash = ctx_hash_index(gsec_pf->gsp_chash_size,\r\n(__u64) vcred->vc_uid);\r\nhash_head = &gsec_pf->gsp_chash[hash];\r\nLASSERT(hash < gsec_pf->gsp_chash_size);\r\nretry:\r\nspin_lock(&sec->ps_lock);\r\nif (remove_dead && sec->ps_gc_next &&\r\ncfs_time_after(cfs_time_current_sec(), sec->ps_gc_next)) {\r\ngss_ctx_cache_gc_pf(gsec_pf, &freelist);\r\ngc = 1;\r\n}\r\nhlist_for_each_entry_safe(ctx, next, hash_head, cc_cache) {\r\nif (gc == 0 &&\r\nctx_check_death_locked_pf(ctx,\r\nremove_dead ? &freelist : NULL))\r\ncontinue;\r\nif (ctx_match_pf(ctx, vcred)) {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nif (found) {\r\nif (new && new != ctx) {\r\nhlist_add_head(&new->cc_cache, &freelist);\r\nnew = NULL;\r\n}\r\nif (hash_head->first != &ctx->cc_cache) {\r\n__hlist_del(&ctx->cc_cache);\r\nhlist_add_head(&ctx->cc_cache, hash_head);\r\n}\r\n} else {\r\nif (sec_is_reverse(sec)) {\r\nspin_unlock(&sec->ps_lock);\r\nreturn NULL;\r\n}\r\nif (new) {\r\nctx_enhash_pf(new, hash_head);\r\nctx = new;\r\n} else if (create) {\r\nspin_unlock(&sec->ps_lock);\r\nnew = ctx_create_pf(sec, vcred);\r\nif (new) {\r\nclear_bit(PTLRPC_CTX_NEW_BIT, &new->cc_flags);\r\ngoto retry;\r\n}\r\n} else {\r\nctx = NULL;\r\n}\r\n}\r\nif (ctx)\r\natomic_inc(&ctx->cc_refcount);\r\nspin_unlock(&sec->ps_lock);\r\nif (new) {\r\nLASSERT(new == ctx);\r\ngss_cli_ctx_refresh_pf(new);\r\n}\r\nctx_list_destroy_pf(&freelist);\r\nreturn ctx;\r\n}\r\nstatic\r\nvoid gss_sec_release_ctx_pf(struct ptlrpc_sec *sec,\r\nstruct ptlrpc_cli_ctx *ctx,\r\nint sync)\r\n{\r\nLASSERT(test_bit(PTLRPC_CTX_CACHED_BIT, &ctx->cc_flags) == 0);\r\nLASSERT(hlist_unhashed(&ctx->cc_cache));\r\nif (!sync)\r\nclear_bit(PTLRPC_CTX_UPTODATE_BIT, &ctx->cc_flags);\r\nctx_destroy_pf(sec, ctx);\r\n}\r\nstatic\r\nint gss_sec_flush_ctx_cache_pf(struct ptlrpc_sec *sec,\r\nuid_t uid,\r\nint grace, int force)\r\n{\r\nstruct gss_sec *gsec;\r\nstruct gss_sec_pipefs *gsec_pf;\r\nstruct ptlrpc_cli_ctx *ctx;\r\nstruct hlist_node *next;\r\nHLIST_HEAD(freelist);\r\nint i, busy = 0;\r\nmight_sleep_if(grace);\r\ngsec = container_of(sec, struct gss_sec, gs_base);\r\ngsec_pf = container_of(gsec, struct gss_sec_pipefs, gsp_base);\r\nspin_lock(&sec->ps_lock);\r\nfor (i = 0; i < gsec_pf->gsp_chash_size; i++) {\r\nhlist_for_each_entry_safe(ctx, next,\r\n&gsec_pf->gsp_chash[i],\r\ncc_cache) {\r\nLASSERT(atomic_read(&ctx->cc_refcount) > 0);\r\nif (uid != -1 && uid != ctx->cc_vcred.vc_uid)\r\ncontinue;\r\nif (atomic_read(&ctx->cc_refcount) > 1) {\r\nbusy++;\r\nif (!force)\r\ncontinue;\r\nCWARN("flush busy(%d) ctx %p(%u->%s) by force, "\r\n"grace %d\n",\r\natomic_read(&ctx->cc_refcount),\r\nctx, ctx->cc_vcred.vc_uid,\r\nsec2target_str(ctx->cc_sec), grace);\r\n}\r\nctx_unhash_pf(ctx, &freelist);\r\nset_bit(PTLRPC_CTX_DEAD_BIT, &ctx->cc_flags);\r\nif (!grace)\r\nclear_bit(PTLRPC_CTX_UPTODATE_BIT,\r\n&ctx->cc_flags);\r\n}\r\n}\r\nspin_unlock(&sec->ps_lock);\r\nctx_list_destroy_pf(&freelist);\r\nreturn busy;\r\n}\r\nstatic\r\nint gss_svc_accept_pf(struct ptlrpc_request *req)\r\n{\r\nreturn gss_svc_accept(&gss_policy_pipefs, req);\r\n}\r\nstatic\r\nint gss_svc_install_rctx_pf(struct obd_import *imp,\r\nstruct ptlrpc_svc_ctx *ctx)\r\n{\r\nstruct ptlrpc_sec *sec;\r\nint rc;\r\nsec = sptlrpc_import_sec_ref(imp);\r\nLASSERT(sec);\r\nrc = gss_install_rvs_cli_ctx_pf(sec2gsec(sec), ctx);\r\nsptlrpc_sec_put(sec);\r\nreturn rc;\r\n}\r\nstatic inline\r\n__u32 upcall_get_sequence(void)\r\n{\r\nreturn (__u32) atomic_inc_return(&upcall_seq);\r\n}\r\nstatic inline\r\n__u32 mech_name2idx(const char *name)\r\n{\r\nLASSERT(!strcmp(name, "krb5"));\r\nreturn MECH_KRB5;\r\n}\r\nstatic inline\r\nvoid upcall_list_lock(int idx)\r\n{\r\nspin_lock(&upcall_locks[idx]);\r\n}\r\nstatic inline\r\nvoid upcall_list_unlock(int idx)\r\n{\r\nspin_unlock(&upcall_locks[idx]);\r\n}\r\nstatic\r\nvoid upcall_msg_enlist(struct gss_upcall_msg *msg)\r\n{\r\n__u32 idx = msg->gum_mechidx;\r\nupcall_list_lock(idx);\r\nlist_add(&msg->gum_list, &upcall_lists[idx]);\r\nupcall_list_unlock(idx);\r\n}\r\nstatic\r\nvoid upcall_msg_delist(struct gss_upcall_msg *msg)\r\n{\r\n__u32 idx = msg->gum_mechidx;\r\nupcall_list_lock(idx);\r\nlist_del_init(&msg->gum_list);\r\nupcall_list_unlock(idx);\r\n}\r\nstatic\r\nvoid gss_release_msg(struct gss_upcall_msg *gmsg)\r\n{\r\nLASSERT(atomic_read(&gmsg->gum_refcount) > 0);\r\nif (!atomic_dec_and_test(&gmsg->gum_refcount)) {\r\nreturn;\r\n}\r\nif (gmsg->gum_gctx) {\r\nsptlrpc_cli_ctx_wakeup(&gmsg->gum_gctx->gc_base);\r\nsptlrpc_cli_ctx_put(&gmsg->gum_gctx->gc_base, 1);\r\ngmsg->gum_gctx = NULL;\r\n}\r\nLASSERT(list_empty(&gmsg->gum_list));\r\nLASSERT(list_empty(&gmsg->gum_base.list));\r\nOBD_FREE_PTR(gmsg);\r\n}\r\nstatic\r\nvoid gss_unhash_msg_nolock(struct gss_upcall_msg *gmsg)\r\n{\r\n__u32 idx = gmsg->gum_mechidx;\r\nLASSERT(idx < MECH_MAX);\r\nLASSERT(spin_is_locked(&upcall_locks[idx]));\r\nif (list_empty(&gmsg->gum_list))\r\nreturn;\r\nlist_del_init(&gmsg->gum_list);\r\nLASSERT(atomic_read(&gmsg->gum_refcount) > 1);\r\natomic_dec(&gmsg->gum_refcount);\r\n}\r\nstatic\r\nvoid gss_unhash_msg(struct gss_upcall_msg *gmsg)\r\n{\r\n__u32 idx = gmsg->gum_mechidx;\r\nLASSERT(idx < MECH_MAX);\r\nupcall_list_lock(idx);\r\ngss_unhash_msg_nolock(gmsg);\r\nupcall_list_unlock(idx);\r\n}\r\nstatic\r\nvoid gss_msg_fail_ctx(struct gss_upcall_msg *gmsg)\r\n{\r\nif (gmsg->gum_gctx) {\r\nstruct ptlrpc_cli_ctx *ctx = &gmsg->gum_gctx->gc_base;\r\nLASSERT(atomic_read(&ctx->cc_refcount) > 0);\r\nsptlrpc_cli_ctx_expire(ctx);\r\nset_bit(PTLRPC_CTX_ERROR_BIT, &ctx->cc_flags);\r\n}\r\n}\r\nstatic\r\nstruct gss_upcall_msg * gss_find_upcall(__u32 mechidx, __u32 seq)\r\n{\r\nstruct gss_upcall_msg *gmsg;\r\nupcall_list_lock(mechidx);\r\nlist_for_each_entry(gmsg, &upcall_lists[mechidx], gum_list) {\r\nif (gmsg->gum_data.gum_seq != seq)\r\ncontinue;\r\nLASSERT(atomic_read(&gmsg->gum_refcount) > 0);\r\nLASSERT(gmsg->gum_mechidx == mechidx);\r\natomic_inc(&gmsg->gum_refcount);\r\nupcall_list_unlock(mechidx);\r\nreturn gmsg;\r\n}\r\nupcall_list_unlock(mechidx);\r\nreturn NULL;\r\n}\r\nstatic\r\nint simple_get_bytes(char **buf, __u32 *buflen, void *res, __u32 reslen)\r\n{\r\nif (*buflen < reslen) {\r\nCERROR("buflen %u < %u\n", *buflen, reslen);\r\nreturn -EINVAL;\r\n}\r\nmemcpy(res, *buf, reslen);\r\n*buf += reslen;\r\n*buflen -= reslen;\r\nreturn 0;\r\n}\r\nstatic\r\nssize_t gss_pipe_upcall(struct file *filp, struct rpc_pipe_msg *msg,\r\nchar *dst, size_t buflen)\r\n{\r\nchar *data = (char *)msg->data + msg->copied;\r\nssize_t mlen = msg->len;\r\nssize_t left;\r\nif (mlen > buflen)\r\nmlen = buflen;\r\nleft = copy_to_user(dst, data, mlen);\r\nif (left < 0) {\r\nmsg->errno = left;\r\nreturn left;\r\n}\r\nmlen -= left;\r\nmsg->copied += mlen;\r\nmsg->errno = 0;\r\nreturn mlen;\r\n}\r\nstatic\r\nssize_t gss_pipe_downcall(struct file *filp, const char *src, size_t mlen)\r\n{\r\nstruct rpc_inode *rpci = RPC_I(filp->f_dentry->d_inode);\r\nstruct gss_upcall_msg *gss_msg;\r\nstruct ptlrpc_cli_ctx *ctx;\r\nstruct gss_cli_ctx *gctx = NULL;\r\nchar *buf, *data;\r\nint datalen;\r\nint timeout, rc;\r\n__u32 mechidx, seq, gss_err;\r\nmechidx = (__u32) (long) rpci->private;\r\nLASSERT(mechidx < MECH_MAX);\r\nOBD_ALLOC(buf, mlen);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nif (copy_from_user(buf, src, mlen)) {\r\nCERROR("failed copy user space data\n");\r\nGOTO(out_free, rc = -EFAULT);\r\n}\r\ndata = buf;\r\ndatalen = mlen;\r\nif (simple_get_bytes(&data, &datalen, &seq, sizeof(seq))) {\r\nCERROR("fail to get seq\n");\r\nGOTO(out_free, rc = -EFAULT);\r\n}\r\ngss_msg = gss_find_upcall(mechidx, seq);\r\nif (!gss_msg) {\r\nCERROR("upcall %u has aborted earlier\n", seq);\r\nGOTO(out_free, rc = -EINVAL);\r\n}\r\ngss_unhash_msg(gss_msg);\r\ngctx = gss_msg->gum_gctx;\r\nLASSERT(gctx);\r\nLASSERT(atomic_read(&gctx->gc_base.cc_refcount) > 0);\r\nif (simple_get_bytes(&data, &datalen, &timeout, sizeof(timeout)))\r\nGOTO(out_msg, rc = -EFAULT);\r\nif (simple_get_bytes(&data, &datalen, &gctx->gc_win,\r\nsizeof(gctx->gc_win)))\r\nGOTO(out_msg, rc = -EFAULT);\r\nif (gctx->gc_win == 0) {\r\nif (simple_get_bytes(&data, &datalen, &rc, sizeof(rc)))\r\nGOTO(out_msg, rc = -EFAULT);\r\nif (simple_get_bytes(&data, &datalen, &gss_err,sizeof(gss_err)))\r\nGOTO(out_msg, rc = -EFAULT);\r\nif (rc == 0 && gss_err == GSS_S_COMPLETE) {\r\nCWARN("both rpc & gss error code not set\n");\r\nrc = -EPERM;\r\n}\r\n} else {\r\nrawobj_t tmpobj;\r\nif (rawobj_extract_local(&tmpobj, (__u32 **) &data, &datalen))\r\nGOTO(out_msg, rc = -EFAULT);\r\nif (rawobj_dup(&gctx->gc_handle, &tmpobj))\r\nGOTO(out_msg, rc = -ENOMEM);\r\nif (rawobj_extract_local(&tmpobj, (__u32 **) &data, &datalen))\r\nGOTO(out_msg, rc = -EFAULT);\r\ngss_err = lgss_import_sec_context(&tmpobj,\r\ngss_msg->gum_gsec->gs_mech,\r\n&gctx->gc_mechctx);\r\nrc = 0;\r\n}\r\nif (likely(rc == 0 && gss_err == GSS_S_COMPLETE)) {\r\ngss_cli_ctx_uptodate(gctx);\r\n} else {\r\nctx = &gctx->gc_base;\r\nsptlrpc_cli_ctx_expire(ctx);\r\nif (rc != -ERESTART || gss_err != GSS_S_COMPLETE)\r\nset_bit(PTLRPC_CTX_ERROR_BIT, &ctx->cc_flags);\r\nCERROR("refresh ctx %p(uid %d) failed: %d/0x%08x: %s\n",\r\nctx, ctx->cc_vcred.vc_uid, rc, gss_err,\r\ntest_bit(PTLRPC_CTX_ERROR_BIT, &ctx->cc_flags) ?\r\n"fatal error" : "non-fatal");\r\n}\r\nrc = mlen;\r\nout_msg:\r\ngss_release_msg(gss_msg);\r\nout_free:\r\nOBD_FREE(buf, mlen);\r\nrc = mlen;\r\nreturn rc;\r\n}\r\nstatic\r\nvoid gss_pipe_destroy_msg(struct rpc_pipe_msg *msg)\r\n{\r\nstruct gss_upcall_msg *gmsg;\r\nstruct gss_upcall_msg_data *gumd;\r\nstatic cfs_time_t ratelimit = 0;\r\nLASSERT(list_empty(&msg->list));\r\nif (msg->errno >= 0) {\r\nreturn;\r\n}\r\ngmsg = container_of(msg, struct gss_upcall_msg, gum_base);\r\ngumd = &gmsg->gum_data;\r\nLASSERT(atomic_read(&gmsg->gum_refcount) > 0);\r\nCERROR("failed msg %p (seq %u, uid %u, svc %u, nid "LPX64", obd %.*s): "\r\n"errno %d\n", msg, gumd->gum_seq, gumd->gum_uid, gumd->gum_svc,\r\ngumd->gum_nid, (int) sizeof(gumd->gum_obd),\r\ngumd->gum_obd, msg->errno);\r\natomic_inc(&gmsg->gum_refcount);\r\ngss_unhash_msg(gmsg);\r\nif (msg->errno == -ETIMEDOUT || msg->errno == -EPIPE) {\r\ncfs_time_t now = cfs_time_current_sec();\r\nif (cfs_time_after(now, ratelimit)) {\r\nCWARN("upcall timed out, is lgssd running?\n");\r\nratelimit = now + 15;\r\n}\r\n}\r\ngss_msg_fail_ctx(gmsg);\r\ngss_release_msg(gmsg);\r\n}\r\nstatic\r\nvoid gss_pipe_release(struct inode *inode)\r\n{\r\nstruct rpc_inode *rpci = RPC_I(inode);\r\n__u32 idx;\r\nidx = (__u32) (long) rpci->private;\r\nLASSERT(idx < MECH_MAX);\r\nupcall_list_lock(idx);\r\nwhile (!list_empty(&upcall_lists[idx])) {\r\nstruct gss_upcall_msg *gmsg;\r\nstruct gss_upcall_msg_data *gumd;\r\ngmsg = list_entry(upcall_lists[idx].next,\r\nstruct gss_upcall_msg, gum_list);\r\ngumd = &gmsg->gum_data;\r\nLASSERT(list_empty(&gmsg->gum_base.list));\r\nCERROR("failing remaining msg %p:seq %u, uid %u, svc %u, "\r\n"nid "LPX64", obd %.*s\n", gmsg,\r\ngumd->gum_seq, gumd->gum_uid, gumd->gum_svc,\r\ngumd->gum_nid, (int) sizeof(gumd->gum_obd),\r\ngumd->gum_obd);\r\ngmsg->gum_base.errno = -EPIPE;\r\natomic_inc(&gmsg->gum_refcount);\r\ngss_unhash_msg_nolock(gmsg);\r\ngss_msg_fail_ctx(gmsg);\r\nupcall_list_unlock(idx);\r\ngss_release_msg(gmsg);\r\nupcall_list_lock(idx);\r\n}\r\nupcall_list_unlock(idx);\r\n}\r\nstatic\r\nint gss_ctx_refresh_pf(struct ptlrpc_cli_ctx *ctx)\r\n{\r\nstruct obd_import *imp;\r\nstruct gss_sec *gsec;\r\nstruct gss_upcall_msg *gmsg;\r\nint rc = 0;\r\nmight_sleep();\r\nLASSERT(ctx->cc_sec);\r\nLASSERT(ctx->cc_sec->ps_import);\r\nLASSERT(ctx->cc_sec->ps_import->imp_obd);\r\nimp = ctx->cc_sec->ps_import;\r\nif (!imp->imp_connection) {\r\nCERROR("import has no connection set\n");\r\nreturn -EINVAL;\r\n}\r\ngsec = container_of(ctx->cc_sec, struct gss_sec, gs_base);\r\nOBD_ALLOC_PTR(gmsg);\r\nif (!gmsg)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&gmsg->gum_base.list);\r\ngmsg->gum_base.data = &gmsg->gum_data;\r\ngmsg->gum_base.len = sizeof(gmsg->gum_data);\r\ngmsg->gum_base.copied = 0;\r\ngmsg->gum_base.errno = 0;\r\natomic_set(&gmsg->gum_refcount, 1);\r\ngmsg->gum_mechidx = mech_name2idx(gsec->gs_mech->gm_name);\r\ngmsg->gum_gsec = gsec;\r\ngmsg->gum_gctx = container_of(sptlrpc_cli_ctx_get(ctx),\r\nstruct gss_cli_ctx, gc_base);\r\ngmsg->gum_data.gum_seq = upcall_get_sequence();\r\ngmsg->gum_data.gum_uid = ctx->cc_vcred.vc_uid;\r\ngmsg->gum_data.gum_gid = 0;\r\ngmsg->gum_data.gum_svc = import_to_gss_svc(imp);\r\ngmsg->gum_data.gum_nid = imp->imp_connection->c_peer.nid;\r\nstrncpy(gmsg->gum_data.gum_obd, imp->imp_obd->obd_name,\r\nsizeof(gmsg->gum_data.gum_obd));\r\nif (ctx->cc_flags & PTLRPC_CTX_STATUS_MASK) {\r\nCWARN("ctx %p(%u->%s) was set flags %lx unexpectedly\n",\r\nctx, ctx->cc_vcred.vc_uid, sec2target_str(ctx->cc_sec),\r\nctx->cc_flags);\r\nLASSERT(!(ctx->cc_flags & PTLRPC_CTX_UPTODATE));\r\nctx->cc_flags |= PTLRPC_CTX_DEAD | PTLRPC_CTX_ERROR;\r\nrc = -EIO;\r\ngoto err_free;\r\n}\r\nupcall_msg_enlist(gmsg);\r\nrc = rpc_queue_upcall(de_pipes[gmsg->gum_mechidx]->d_inode,\r\n&gmsg->gum_base);\r\nif (rc) {\r\nCERROR("rpc_queue_upcall failed: %d\n", rc);\r\nupcall_msg_delist(gmsg);\r\ngoto err_free;\r\n}\r\nreturn 0;\r\nerr_free:\r\nOBD_FREE_PTR(gmsg);\r\nreturn rc;\r\n}\r\nstatic\r\nint gss_cli_ctx_refresh_pf(struct ptlrpc_cli_ctx *ctx)\r\n{\r\nif (ctx->cc_vcred.vc_uid == 0) {\r\nstruct gss_sec *gsec;\r\ngsec = container_of(ctx->cc_sec, struct gss_sec, gs_base);\r\ngsec->gs_rvs_hdl = gss_get_next_ctx_index();\r\n}\r\nreturn gss_ctx_refresh_pf(ctx);\r\n}\r\nstatic\r\nint __init gss_init_pipefs_upcall(void)\r\n{\r\nstruct dentry *de;\r\nde = rpc_mkdir(LUSTRE_PIPE_ROOT, NULL);\r\nif (IS_ERR(de) && PTR_ERR(de) != -EEXIST) {\r\nCERROR("Failed to create gss pipe dir: %ld\n", PTR_ERR(de));\r\nreturn PTR_ERR(de);\r\n}\r\nde = rpc_mkpipe(LUSTRE_PIPE_KRB5, (void *) MECH_KRB5, &gss_upcall_ops,\r\nRPC_PIPE_WAIT_FOR_OPEN);\r\nif (!de || IS_ERR(de)) {\r\nCERROR("failed to make rpc_pipe %s: %ld\n",\r\nLUSTRE_PIPE_KRB5, PTR_ERR(de));\r\nrpc_rmdir(LUSTRE_PIPE_ROOT);\r\nreturn PTR_ERR(de);\r\n}\r\nde_pipes[MECH_KRB5] = de;\r\nINIT_LIST_HEAD(&upcall_lists[MECH_KRB5]);\r\nspin_lock_init(&upcall_locks[MECH_KRB5]);\r\nreturn 0;\r\n}\r\nstatic\r\nvoid __exit gss_exit_pipefs_upcall(void)\r\n{\r\n__u32 i;\r\nfor (i = 0; i < MECH_MAX; i++) {\r\nLASSERT(list_empty(&upcall_lists[i]));\r\nde_pipes[i] = NULL;\r\n}\r\nrpc_unlink(LUSTRE_PIPE_KRB5);\r\nrpc_rmdir(LUSTRE_PIPE_ROOT);\r\n}\r\nint __init gss_init_pipefs(void)\r\n{\r\nint rc;\r\nrc = gss_init_pipefs_upcall();\r\nif (rc)\r\nreturn rc;\r\nrc = sptlrpc_register_policy(&gss_policy_pipefs);\r\nif (rc) {\r\ngss_exit_pipefs_upcall();\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nvoid __exit gss_exit_pipefs(void)\r\n{\r\ngss_exit_pipefs_upcall();\r\nsptlrpc_unregister_policy(&gss_policy_pipefs);\r\n}
