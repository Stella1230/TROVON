static struct qib_mcast_qp *qib_mcast_qp_alloc(struct qib_qp *qp)\r\n{\r\nstruct qib_mcast_qp *mqp;\r\nmqp = kmalloc(sizeof *mqp, GFP_KERNEL);\r\nif (!mqp)\r\ngoto bail;\r\nmqp->qp = qp;\r\natomic_inc(&qp->refcount);\r\nbail:\r\nreturn mqp;\r\n}\r\nstatic void qib_mcast_qp_free(struct qib_mcast_qp *mqp)\r\n{\r\nstruct qib_qp *qp = mqp->qp;\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\nkfree(mqp);\r\n}\r\nstatic struct qib_mcast *qib_mcast_alloc(union ib_gid *mgid)\r\n{\r\nstruct qib_mcast *mcast;\r\nmcast = kmalloc(sizeof *mcast, GFP_KERNEL);\r\nif (!mcast)\r\ngoto bail;\r\nmcast->mgid = *mgid;\r\nINIT_LIST_HEAD(&mcast->qp_list);\r\ninit_waitqueue_head(&mcast->wait);\r\natomic_set(&mcast->refcount, 0);\r\nmcast->n_attached = 0;\r\nbail:\r\nreturn mcast;\r\n}\r\nstatic void qib_mcast_free(struct qib_mcast *mcast)\r\n{\r\nstruct qib_mcast_qp *p, *tmp;\r\nlist_for_each_entry_safe(p, tmp, &mcast->qp_list, list)\r\nqib_mcast_qp_free(p);\r\nkfree(mcast);\r\n}\r\nstruct qib_mcast *qib_mcast_find(struct qib_ibport *ibp, union ib_gid *mgid)\r\n{\r\nstruct rb_node *n;\r\nunsigned long flags;\r\nstruct qib_mcast *mcast;\r\nspin_lock_irqsave(&ibp->lock, flags);\r\nn = ibp->mcast_tree.rb_node;\r\nwhile (n) {\r\nint ret;\r\nmcast = rb_entry(n, struct qib_mcast, rb_node);\r\nret = memcmp(mgid->raw, mcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0)\r\nn = n->rb_left;\r\nelse if (ret > 0)\r\nn = n->rb_right;\r\nelse {\r\natomic_inc(&mcast->refcount);\r\nspin_unlock_irqrestore(&ibp->lock, flags);\r\ngoto bail;\r\n}\r\n}\r\nspin_unlock_irqrestore(&ibp->lock, flags);\r\nmcast = NULL;\r\nbail:\r\nreturn mcast;\r\n}\r\nstatic int qib_mcast_add(struct qib_ibdev *dev, struct qib_ibport *ibp,\r\nstruct qib_mcast *mcast, struct qib_mcast_qp *mqp)\r\n{\r\nstruct rb_node **n = &ibp->mcast_tree.rb_node;\r\nstruct rb_node *pn = NULL;\r\nint ret;\r\nspin_lock_irq(&ibp->lock);\r\nwhile (*n) {\r\nstruct qib_mcast *tmcast;\r\nstruct qib_mcast_qp *p;\r\npn = *n;\r\ntmcast = rb_entry(pn, struct qib_mcast, rb_node);\r\nret = memcmp(mcast->mgid.raw, tmcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0) {\r\nn = &pn->rb_left;\r\ncontinue;\r\n}\r\nif (ret > 0) {\r\nn = &pn->rb_right;\r\ncontinue;\r\n}\r\nlist_for_each_entry_rcu(p, &tmcast->qp_list, list) {\r\nif (p->qp == mqp->qp) {\r\nret = ESRCH;\r\ngoto bail;\r\n}\r\n}\r\nif (tmcast->n_attached == ib_qib_max_mcast_qp_attached) {\r\nret = ENOMEM;\r\ngoto bail;\r\n}\r\ntmcast->n_attached++;\r\nlist_add_tail_rcu(&mqp->list, &tmcast->qp_list);\r\nret = EEXIST;\r\ngoto bail;\r\n}\r\nspin_lock(&dev->n_mcast_grps_lock);\r\nif (dev->n_mcast_grps_allocated == ib_qib_max_mcast_grps) {\r\nspin_unlock(&dev->n_mcast_grps_lock);\r\nret = ENOMEM;\r\ngoto bail;\r\n}\r\ndev->n_mcast_grps_allocated++;\r\nspin_unlock(&dev->n_mcast_grps_lock);\r\nmcast->n_attached++;\r\nlist_add_tail_rcu(&mqp->list, &mcast->qp_list);\r\natomic_inc(&mcast->refcount);\r\nrb_link_node(&mcast->rb_node, pn, n);\r\nrb_insert_color(&mcast->rb_node, &ibp->mcast_tree);\r\nret = 0;\r\nbail:\r\nspin_unlock_irq(&ibp->lock);\r\nreturn ret;\r\n}\r\nint qib_multicast_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\r\n{\r\nstruct qib_qp *qp = to_iqp(ibqp);\r\nstruct qib_ibdev *dev = to_idev(ibqp->device);\r\nstruct qib_ibport *ibp;\r\nstruct qib_mcast *mcast;\r\nstruct qib_mcast_qp *mqp;\r\nint ret;\r\nif (ibqp->qp_num <= 1 || qp->state == IB_QPS_RESET) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nmcast = qib_mcast_alloc(gid);\r\nif (mcast == NULL) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nmqp = qib_mcast_qp_alloc(qp);\r\nif (mqp == NULL) {\r\nqib_mcast_free(mcast);\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nibp = to_iport(ibqp->device, qp->port_num);\r\nswitch (qib_mcast_add(dev, ibp, mcast, mqp)) {\r\ncase ESRCH:\r\nqib_mcast_qp_free(mqp);\r\nqib_mcast_free(mcast);\r\nbreak;\r\ncase EEXIST:\r\nqib_mcast_free(mcast);\r\nbreak;\r\ncase ENOMEM:\r\nqib_mcast_qp_free(mqp);\r\nqib_mcast_free(mcast);\r\nret = -ENOMEM;\r\ngoto bail;\r\ndefault:\r\nbreak;\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_multicast_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)\r\n{\r\nstruct qib_qp *qp = to_iqp(ibqp);\r\nstruct qib_ibdev *dev = to_idev(ibqp->device);\r\nstruct qib_ibport *ibp = to_iport(ibqp->device, qp->port_num);\r\nstruct qib_mcast *mcast = NULL;\r\nstruct qib_mcast_qp *p, *tmp;\r\nstruct rb_node *n;\r\nint last = 0;\r\nint ret;\r\nif (ibqp->qp_num <= 1 || qp->state == IB_QPS_RESET) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nspin_lock_irq(&ibp->lock);\r\nn = ibp->mcast_tree.rb_node;\r\nwhile (1) {\r\nif (n == NULL) {\r\nspin_unlock_irq(&ibp->lock);\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nmcast = rb_entry(n, struct qib_mcast, rb_node);\r\nret = memcmp(gid->raw, mcast->mgid.raw,\r\nsizeof(union ib_gid));\r\nif (ret < 0)\r\nn = n->rb_left;\r\nelse if (ret > 0)\r\nn = n->rb_right;\r\nelse\r\nbreak;\r\n}\r\nlist_for_each_entry_safe(p, tmp, &mcast->qp_list, list) {\r\nif (p->qp != qp)\r\ncontinue;\r\nlist_del_rcu(&p->list);\r\nmcast->n_attached--;\r\nif (list_empty(&mcast->qp_list)) {\r\nrb_erase(&mcast->rb_node, &ibp->mcast_tree);\r\nlast = 1;\r\n}\r\nbreak;\r\n}\r\nspin_unlock_irq(&ibp->lock);\r\nif (p) {\r\nwait_event(mcast->wait, atomic_read(&mcast->refcount) <= 1);\r\nqib_mcast_qp_free(p);\r\n}\r\nif (last) {\r\natomic_dec(&mcast->refcount);\r\nwait_event(mcast->wait, !atomic_read(&mcast->refcount));\r\nqib_mcast_free(mcast);\r\nspin_lock_irq(&dev->n_mcast_grps_lock);\r\ndev->n_mcast_grps_allocated--;\r\nspin_unlock_irq(&dev->n_mcast_grps_lock);\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_mcast_tree_empty(struct qib_ibport *ibp)\r\n{\r\nreturn ibp->mcast_tree.rb_node == NULL;\r\n}
