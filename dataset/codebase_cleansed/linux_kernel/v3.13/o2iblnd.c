__u32\r\nkiblnd_cksum (void *ptr, int nob)\r\n{\r\nchar *c = ptr;\r\n__u32 sum = 0;\r\nwhile (nob-- > 0)\r\nsum = ((sum << 1) | (sum >> 31)) + *c++;\r\nreturn (sum == 0) ? 1 : sum;\r\n}\r\nstatic char *\r\nkiblnd_msgtype2str(int type)\r\n{\r\nswitch (type) {\r\ncase IBLND_MSG_CONNREQ:\r\nreturn "CONNREQ";\r\ncase IBLND_MSG_CONNACK:\r\nreturn "CONNACK";\r\ncase IBLND_MSG_NOOP:\r\nreturn "NOOP";\r\ncase IBLND_MSG_IMMEDIATE:\r\nreturn "IMMEDIATE";\r\ncase IBLND_MSG_PUT_REQ:\r\nreturn "PUT_REQ";\r\ncase IBLND_MSG_PUT_NAK:\r\nreturn "PUT_NAK";\r\ncase IBLND_MSG_PUT_ACK:\r\nreturn "PUT_ACK";\r\ncase IBLND_MSG_PUT_DONE:\r\nreturn "PUT_DONE";\r\ncase IBLND_MSG_GET_REQ:\r\nreturn "GET_REQ";\r\ncase IBLND_MSG_GET_DONE:\r\nreturn "GET_DONE";\r\ndefault:\r\nreturn "???";\r\n}\r\n}\r\nstatic int\r\nkiblnd_msgtype2size(int type)\r\n{\r\nconst int hdr_size = offsetof(kib_msg_t, ibm_u);\r\nswitch (type) {\r\ncase IBLND_MSG_CONNREQ:\r\ncase IBLND_MSG_CONNACK:\r\nreturn hdr_size + sizeof(kib_connparams_t);\r\ncase IBLND_MSG_NOOP:\r\nreturn hdr_size;\r\ncase IBLND_MSG_IMMEDIATE:\r\nreturn offsetof(kib_msg_t, ibm_u.immediate.ibim_payload[0]);\r\ncase IBLND_MSG_PUT_REQ:\r\nreturn hdr_size + sizeof(kib_putreq_msg_t);\r\ncase IBLND_MSG_PUT_ACK:\r\nreturn hdr_size + sizeof(kib_putack_msg_t);\r\ncase IBLND_MSG_GET_REQ:\r\nreturn hdr_size + sizeof(kib_get_msg_t);\r\ncase IBLND_MSG_PUT_NAK:\r\ncase IBLND_MSG_PUT_DONE:\r\ncase IBLND_MSG_GET_DONE:\r\nreturn hdr_size + sizeof(kib_completion_msg_t);\r\ndefault:\r\nreturn -1;\r\n}\r\n}\r\nstatic int\r\nkiblnd_unpack_rd(kib_msg_t *msg, int flip)\r\n{\r\nkib_rdma_desc_t *rd;\r\nint nob;\r\nint n;\r\nint i;\r\nLASSERT (msg->ibm_type == IBLND_MSG_GET_REQ ||\r\nmsg->ibm_type == IBLND_MSG_PUT_ACK);\r\nrd = msg->ibm_type == IBLND_MSG_GET_REQ ?\r\n&msg->ibm_u.get.ibgm_rd :\r\n&msg->ibm_u.putack.ibpam_rd;\r\nif (flip) {\r\n__swab32s(&rd->rd_key);\r\n__swab32s(&rd->rd_nfrags);\r\n}\r\nn = rd->rd_nfrags;\r\nif (n <= 0 || n > IBLND_MAX_RDMA_FRAGS) {\r\nCERROR("Bad nfrags: %d, should be 0 < n <= %d\n",\r\nn, IBLND_MAX_RDMA_FRAGS);\r\nreturn 1;\r\n}\r\nnob = offsetof (kib_msg_t, ibm_u) +\r\nkiblnd_rd_msg_size(rd, msg->ibm_type, n);\r\nif (msg->ibm_nob < nob) {\r\nCERROR("Short %s: %d(%d)\n",\r\nkiblnd_msgtype2str(msg->ibm_type), msg->ibm_nob, nob);\r\nreturn 1;\r\n}\r\nif (!flip)\r\nreturn 0;\r\nfor (i = 0; i < n; i++) {\r\n__swab32s(&rd->rd_frags[i].rf_nob);\r\n__swab64s(&rd->rd_frags[i].rf_addr);\r\n}\r\nreturn 0;\r\n}\r\nvoid\r\nkiblnd_pack_msg (lnet_ni_t *ni, kib_msg_t *msg, int version,\r\nint credits, lnet_nid_t dstnid, __u64 dststamp)\r\n{\r\nkib_net_t *net = ni->ni_data;\r\nmsg->ibm_magic = IBLND_MSG_MAGIC;\r\nmsg->ibm_version = version;\r\nmsg->ibm_credits = credits;\r\nmsg->ibm_cksum = 0;\r\nmsg->ibm_srcnid = ni->ni_nid;\r\nmsg->ibm_srcstamp = net->ibn_incarnation;\r\nmsg->ibm_dstnid = dstnid;\r\nmsg->ibm_dststamp = dststamp;\r\nif (*kiblnd_tunables.kib_cksum) {\r\nmsg->ibm_cksum = kiblnd_cksum(msg, msg->ibm_nob);\r\n}\r\n}\r\nint\r\nkiblnd_unpack_msg(kib_msg_t *msg, int nob)\r\n{\r\nconst int hdr_size = offsetof(kib_msg_t, ibm_u);\r\n__u32 msg_cksum;\r\n__u16 version;\r\nint msg_nob;\r\nint flip;\r\nif (nob < 6) {\r\nCERROR("Short message: %d\n", nob);\r\nreturn -EPROTO;\r\n}\r\nif (msg->ibm_magic == IBLND_MSG_MAGIC) {\r\nflip = 0;\r\n} else if (msg->ibm_magic == __swab32(IBLND_MSG_MAGIC)) {\r\nflip = 1;\r\n} else {\r\nCERROR("Bad magic: %08x\n", msg->ibm_magic);\r\nreturn -EPROTO;\r\n}\r\nversion = flip ? __swab16(msg->ibm_version) : msg->ibm_version;\r\nif (version != IBLND_MSG_VERSION &&\r\nversion != IBLND_MSG_VERSION_1) {\r\nCERROR("Bad version: %x\n", version);\r\nreturn -EPROTO;\r\n}\r\nif (nob < hdr_size) {\r\nCERROR("Short message: %d\n", nob);\r\nreturn -EPROTO;\r\n}\r\nmsg_nob = flip ? __swab32(msg->ibm_nob) : msg->ibm_nob;\r\nif (msg_nob > nob) {\r\nCERROR("Short message: got %d, wanted %d\n", nob, msg_nob);\r\nreturn -EPROTO;\r\n}\r\nmsg_cksum = flip ? __swab32(msg->ibm_cksum) : msg->ibm_cksum;\r\nmsg->ibm_cksum = 0;\r\nif (msg_cksum != 0 &&\r\nmsg_cksum != kiblnd_cksum(msg, msg_nob)) {\r\nCERROR("Bad checksum\n");\r\nreturn -EPROTO;\r\n}\r\nmsg->ibm_cksum = msg_cksum;\r\nif (flip) {\r\nmsg->ibm_version = version;\r\nCLASSERT (sizeof(msg->ibm_type) == 1);\r\nCLASSERT (sizeof(msg->ibm_credits) == 1);\r\nmsg->ibm_nob = msg_nob;\r\n__swab64s(&msg->ibm_srcnid);\r\n__swab64s(&msg->ibm_srcstamp);\r\n__swab64s(&msg->ibm_dstnid);\r\n__swab64s(&msg->ibm_dststamp);\r\n}\r\nif (msg->ibm_srcnid == LNET_NID_ANY) {\r\nCERROR("Bad src nid: %s\n", libcfs_nid2str(msg->ibm_srcnid));\r\nreturn -EPROTO;\r\n}\r\nif (msg_nob < kiblnd_msgtype2size(msg->ibm_type)) {\r\nCERROR("Short %s: %d(%d)\n", kiblnd_msgtype2str(msg->ibm_type),\r\nmsg_nob, kiblnd_msgtype2size(msg->ibm_type));\r\nreturn -EPROTO;\r\n}\r\nswitch (msg->ibm_type) {\r\ndefault:\r\nCERROR("Unknown message type %x\n", msg->ibm_type);\r\nreturn -EPROTO;\r\ncase IBLND_MSG_NOOP:\r\ncase IBLND_MSG_IMMEDIATE:\r\ncase IBLND_MSG_PUT_REQ:\r\nbreak;\r\ncase IBLND_MSG_PUT_ACK:\r\ncase IBLND_MSG_GET_REQ:\r\nif (kiblnd_unpack_rd(msg, flip))\r\nreturn -EPROTO;\r\nbreak;\r\ncase IBLND_MSG_PUT_NAK:\r\ncase IBLND_MSG_PUT_DONE:\r\ncase IBLND_MSG_GET_DONE:\r\nif (flip)\r\n__swab32s(&msg->ibm_u.completion.ibcm_status);\r\nbreak;\r\ncase IBLND_MSG_CONNREQ:\r\ncase IBLND_MSG_CONNACK:\r\nif (flip) {\r\n__swab16s(&msg->ibm_u.connparams.ibcp_queue_depth);\r\n__swab16s(&msg->ibm_u.connparams.ibcp_max_frags);\r\n__swab32s(&msg->ibm_u.connparams.ibcp_max_msg_size);\r\n}\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nint\r\nkiblnd_create_peer(lnet_ni_t *ni, kib_peer_t **peerp, lnet_nid_t nid)\r\n{\r\nkib_peer_t *peer;\r\nkib_net_t *net = ni->ni_data;\r\nint cpt = lnet_cpt_of_nid(nid);\r\nunsigned long flags;\r\nLASSERT(net != NULL);\r\nLASSERT(nid != LNET_NID_ANY);\r\nLIBCFS_CPT_ALLOC(peer, lnet_cpt_table(), cpt, sizeof(*peer));\r\nif (peer == NULL) {\r\nCERROR("Cannot allocate peer\n");\r\nreturn -ENOMEM;\r\n}\r\nmemset(peer, 0, sizeof(*peer));\r\npeer->ibp_ni = ni;\r\npeer->ibp_nid = nid;\r\npeer->ibp_error = 0;\r\npeer->ibp_last_alive = 0;\r\natomic_set(&peer->ibp_refcount, 1);\r\nINIT_LIST_HEAD(&peer->ibp_list);\r\nINIT_LIST_HEAD(&peer->ibp_conns);\r\nINIT_LIST_HEAD(&peer->ibp_tx_queue);\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nLASSERT (net->ibn_shutdown == 0);\r\natomic_inc(&net->ibn_npeers);\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\n*peerp = peer;\r\nreturn 0;\r\n}\r\nvoid\r\nkiblnd_destroy_peer (kib_peer_t *peer)\r\n{\r\nkib_net_t *net = peer->ibp_ni->ni_data;\r\nLASSERT (net != NULL);\r\nLASSERT (atomic_read(&peer->ibp_refcount) == 0);\r\nLASSERT (!kiblnd_peer_active(peer));\r\nLASSERT (peer->ibp_connecting == 0);\r\nLASSERT (peer->ibp_accepting == 0);\r\nLASSERT (list_empty(&peer->ibp_conns));\r\nLASSERT (list_empty(&peer->ibp_tx_queue));\r\nLIBCFS_FREE(peer, sizeof(*peer));\r\natomic_dec(&net->ibn_npeers);\r\n}\r\nkib_peer_t *\r\nkiblnd_find_peer_locked (lnet_nid_t nid)\r\n{\r\nstruct list_head *peer_list = kiblnd_nid2peerlist(nid);\r\nstruct list_head *tmp;\r\nkib_peer_t *peer;\r\nlist_for_each (tmp, peer_list) {\r\npeer = list_entry(tmp, kib_peer_t, ibp_list);\r\nLASSERT (peer->ibp_connecting > 0 ||\r\npeer->ibp_accepting > 0 ||\r\n!list_empty(&peer->ibp_conns));\r\nif (peer->ibp_nid != nid)\r\ncontinue;\r\nCDEBUG(D_NET, "got peer [%p] -> %s (%d) version: %x\n",\r\npeer, libcfs_nid2str(nid),\r\natomic_read(&peer->ibp_refcount),\r\npeer->ibp_version);\r\nreturn peer;\r\n}\r\nreturn NULL;\r\n}\r\nvoid\r\nkiblnd_unlink_peer_locked (kib_peer_t *peer)\r\n{\r\nLASSERT (list_empty(&peer->ibp_conns));\r\nLASSERT (kiblnd_peer_active(peer));\r\nlist_del_init(&peer->ibp_list);\r\nkiblnd_peer_decref(peer);\r\n}\r\nint\r\nkiblnd_get_peer_info (lnet_ni_t *ni, int index,\r\nlnet_nid_t *nidp, int *count)\r\n{\r\nkib_peer_t *peer;\r\nstruct list_head *ptmp;\r\nint i;\r\nunsigned long flags;\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nfor (i = 0; i < kiblnd_data.kib_peer_hash_size; i++) {\r\nlist_for_each (ptmp, &kiblnd_data.kib_peers[i]) {\r\npeer = list_entry(ptmp, kib_peer_t, ibp_list);\r\nLASSERT (peer->ibp_connecting > 0 ||\r\npeer->ibp_accepting > 0 ||\r\n!list_empty(&peer->ibp_conns));\r\nif (peer->ibp_ni != ni)\r\ncontinue;\r\nif (index-- > 0)\r\ncontinue;\r\n*nidp = peer->ibp_nid;\r\n*count = atomic_read(&peer->ibp_refcount);\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock,\r\nflags);\r\nreturn 0;\r\n}\r\n}\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nreturn -ENOENT;\r\n}\r\nvoid\r\nkiblnd_del_peer_locked (kib_peer_t *peer)\r\n{\r\nstruct list_head *ctmp;\r\nstruct list_head *cnxt;\r\nkib_conn_t *conn;\r\nif (list_empty(&peer->ibp_conns)) {\r\nkiblnd_unlink_peer_locked(peer);\r\n} else {\r\nlist_for_each_safe (ctmp, cnxt, &peer->ibp_conns) {\r\nconn = list_entry(ctmp, kib_conn_t, ibc_list);\r\nkiblnd_close_conn_locked(conn, 0);\r\n}\r\n}\r\n}\r\nint\r\nkiblnd_del_peer (lnet_ni_t *ni, lnet_nid_t nid)\r\n{\r\nLIST_HEAD (zombies);\r\nstruct list_head *ptmp;\r\nstruct list_head *pnxt;\r\nkib_peer_t *peer;\r\nint lo;\r\nint hi;\r\nint i;\r\nunsigned long flags;\r\nint rc = -ENOENT;\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nif (nid != LNET_NID_ANY) {\r\nlo = hi = kiblnd_nid2peerlist(nid) - kiblnd_data.kib_peers;\r\n} else {\r\nlo = 0;\r\nhi = kiblnd_data.kib_peer_hash_size - 1;\r\n}\r\nfor (i = lo; i <= hi; i++) {\r\nlist_for_each_safe (ptmp, pnxt, &kiblnd_data.kib_peers[i]) {\r\npeer = list_entry(ptmp, kib_peer_t, ibp_list);\r\nLASSERT (peer->ibp_connecting > 0 ||\r\npeer->ibp_accepting > 0 ||\r\n!list_empty(&peer->ibp_conns));\r\nif (peer->ibp_ni != ni)\r\ncontinue;\r\nif (!(nid == LNET_NID_ANY || peer->ibp_nid == nid))\r\ncontinue;\r\nif (!list_empty(&peer->ibp_tx_queue)) {\r\nLASSERT (list_empty(&peer->ibp_conns));\r\nlist_splice_init(&peer->ibp_tx_queue,\r\n&zombies);\r\n}\r\nkiblnd_del_peer_locked(peer);\r\nrc = 0;\r\n}\r\n}\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nkiblnd_txlist_done(ni, &zombies, -EIO);\r\nreturn rc;\r\n}\r\nkib_conn_t *\r\nkiblnd_get_conn_by_idx (lnet_ni_t *ni, int index)\r\n{\r\nkib_peer_t *peer;\r\nstruct list_head *ptmp;\r\nkib_conn_t *conn;\r\nstruct list_head *ctmp;\r\nint i;\r\nunsigned long flags;\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nfor (i = 0; i < kiblnd_data.kib_peer_hash_size; i++) {\r\nlist_for_each (ptmp, &kiblnd_data.kib_peers[i]) {\r\npeer = list_entry(ptmp, kib_peer_t, ibp_list);\r\nLASSERT (peer->ibp_connecting > 0 ||\r\npeer->ibp_accepting > 0 ||\r\n!list_empty(&peer->ibp_conns));\r\nif (peer->ibp_ni != ni)\r\ncontinue;\r\nlist_for_each (ctmp, &peer->ibp_conns) {\r\nif (index-- > 0)\r\ncontinue;\r\nconn = list_entry(ctmp, kib_conn_t,\r\nibc_list);\r\nkiblnd_conn_addref(conn);\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock,\r\nflags);\r\nreturn conn;\r\n}\r\n}\r\n}\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nreturn NULL;\r\n}\r\nvoid\r\nkiblnd_debug_rx (kib_rx_t *rx)\r\n{\r\nCDEBUG(D_CONSOLE, " %p status %d msg_type %x cred %d\n",\r\nrx, rx->rx_status, rx->rx_msg->ibm_type,\r\nrx->rx_msg->ibm_credits);\r\n}\r\nvoid\r\nkiblnd_debug_tx (kib_tx_t *tx)\r\n{\r\nCDEBUG(D_CONSOLE, " %p snd %d q %d w %d rc %d dl %lx "\r\n"cookie "LPX64" msg %s%s type %x cred %d\n",\r\ntx, tx->tx_sending, tx->tx_queued, tx->tx_waiting,\r\ntx->tx_status, tx->tx_deadline, tx->tx_cookie,\r\ntx->tx_lntmsg[0] == NULL ? "-" : "!",\r\ntx->tx_lntmsg[1] == NULL ? "-" : "!",\r\ntx->tx_msg->ibm_type, tx->tx_msg->ibm_credits);\r\n}\r\nvoid\r\nkiblnd_debug_conn (kib_conn_t *conn)\r\n{\r\nstruct list_head *tmp;\r\nint i;\r\nspin_lock(&conn->ibc_lock);\r\nCDEBUG(D_CONSOLE, "conn[%d] %p [version %x] -> %s: \n",\r\natomic_read(&conn->ibc_refcount), conn,\r\nconn->ibc_version, libcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nCDEBUG(D_CONSOLE, " state %d nposted %d/%d cred %d o_cred %d r_cred %d\n",\r\nconn->ibc_state, conn->ibc_noops_posted,\r\nconn->ibc_nsends_posted, conn->ibc_credits,\r\nconn->ibc_outstanding_credits, conn->ibc_reserved_credits);\r\nCDEBUG(D_CONSOLE, " comms_err %d\n", conn->ibc_comms_error);\r\nCDEBUG(D_CONSOLE, " early_rxs:\n");\r\nlist_for_each(tmp, &conn->ibc_early_rxs)\r\nkiblnd_debug_rx(list_entry(tmp, kib_rx_t, rx_list));\r\nCDEBUG(D_CONSOLE, " tx_noops:\n");\r\nlist_for_each(tmp, &conn->ibc_tx_noops)\r\nkiblnd_debug_tx(list_entry(tmp, kib_tx_t, tx_list));\r\nCDEBUG(D_CONSOLE, " tx_queue_nocred:\n");\r\nlist_for_each(tmp, &conn->ibc_tx_queue_nocred)\r\nkiblnd_debug_tx(list_entry(tmp, kib_tx_t, tx_list));\r\nCDEBUG(D_CONSOLE, " tx_queue_rsrvd:\n");\r\nlist_for_each(tmp, &conn->ibc_tx_queue_rsrvd)\r\nkiblnd_debug_tx(list_entry(tmp, kib_tx_t, tx_list));\r\nCDEBUG(D_CONSOLE, " tx_queue:\n");\r\nlist_for_each(tmp, &conn->ibc_tx_queue)\r\nkiblnd_debug_tx(list_entry(tmp, kib_tx_t, tx_list));\r\nCDEBUG(D_CONSOLE, " active_txs:\n");\r\nlist_for_each(tmp, &conn->ibc_active_txs)\r\nkiblnd_debug_tx(list_entry(tmp, kib_tx_t, tx_list));\r\nCDEBUG(D_CONSOLE, " rxs:\n");\r\nfor (i = 0; i < IBLND_RX_MSGS(conn->ibc_version); i++)\r\nkiblnd_debug_rx(&conn->ibc_rxs[i]);\r\nspin_unlock(&conn->ibc_lock);\r\n}\r\nint\r\nkiblnd_translate_mtu(int value)\r\n{\r\nswitch (value) {\r\ndefault:\r\nreturn -1;\r\ncase 0:\r\nreturn 0;\r\ncase 256:\r\nreturn IB_MTU_256;\r\ncase 512:\r\nreturn IB_MTU_512;\r\ncase 1024:\r\nreturn IB_MTU_1024;\r\ncase 2048:\r\nreturn IB_MTU_2048;\r\ncase 4096:\r\nreturn IB_MTU_4096;\r\n}\r\n}\r\nstatic void\r\nkiblnd_setup_mtu_locked(struct rdma_cm_id *cmid)\r\n{\r\nint mtu;\r\nif (cmid->route.path_rec == NULL)\r\nreturn;\r\nmtu = kiblnd_translate_mtu(*kiblnd_tunables.kib_ib_mtu);\r\nLASSERT (mtu >= 0);\r\nif (mtu != 0)\r\ncmid->route.path_rec->mtu = mtu;\r\n}\r\nstatic int\r\nkiblnd_get_completion_vector(kib_conn_t *conn, int cpt)\r\n{\r\ncpumask_t *mask;\r\nint vectors;\r\nint off;\r\nint i;\r\nlnet_nid_t nid = conn->ibc_peer->ibp_nid;\r\nvectors = conn->ibc_cmid->device->num_comp_vectors;\r\nif (vectors <= 1)\r\nreturn 0;\r\nmask = cfs_cpt_cpumask(lnet_cpt_table(), cpt);\r\nif (mask == NULL)\r\nreturn 0;\r\noff = do_div(nid, cpus_weight(*mask));\r\nfor_each_cpu_mask(i, *mask) {\r\nif (off-- == 0)\r\nreturn i % vectors;\r\n}\r\nLBUG();\r\nreturn 1;\r\n}\r\nkib_conn_t *\r\nkiblnd_create_conn(kib_peer_t *peer, struct rdma_cm_id *cmid,\r\nint state, int version)\r\n{\r\nrwlock_t *glock = &kiblnd_data.kib_global_lock;\r\nkib_net_t *net = peer->ibp_ni->ni_data;\r\nkib_dev_t *dev;\r\nstruct ib_qp_init_attr *init_qp_attr;\r\nstruct kib_sched_info *sched;\r\nkib_conn_t *conn;\r\nstruct ib_cq *cq;\r\nunsigned long flags;\r\nint cpt;\r\nint rc;\r\nint i;\r\nLASSERT(net != NULL);\r\nLASSERT(!in_interrupt());\r\ndev = net->ibn_dev;\r\ncpt = lnet_cpt_of_nid(peer->ibp_nid);\r\nsched = kiblnd_data.kib_scheds[cpt];\r\nLASSERT(sched->ibs_nthreads > 0);\r\nLIBCFS_CPT_ALLOC(init_qp_attr, lnet_cpt_table(), cpt,\r\nsizeof(*init_qp_attr));\r\nif (init_qp_attr == NULL) {\r\nCERROR("Can't allocate qp_attr for %s\n",\r\nlibcfs_nid2str(peer->ibp_nid));\r\ngoto failed_0;\r\n}\r\nLIBCFS_CPT_ALLOC(conn, lnet_cpt_table(), cpt, sizeof(*conn));\r\nif (conn == NULL) {\r\nCERROR("Can't allocate connection for %s\n",\r\nlibcfs_nid2str(peer->ibp_nid));\r\ngoto failed_1;\r\n}\r\nconn->ibc_state = IBLND_CONN_INIT;\r\nconn->ibc_version = version;\r\nconn->ibc_peer = peer;\r\ncmid->context = conn;\r\nconn->ibc_cmid = cmid;\r\nINIT_LIST_HEAD(&conn->ibc_early_rxs);\r\nINIT_LIST_HEAD(&conn->ibc_tx_noops);\r\nINIT_LIST_HEAD(&conn->ibc_tx_queue);\r\nINIT_LIST_HEAD(&conn->ibc_tx_queue_rsrvd);\r\nINIT_LIST_HEAD(&conn->ibc_tx_queue_nocred);\r\nINIT_LIST_HEAD(&conn->ibc_active_txs);\r\nspin_lock_init(&conn->ibc_lock);\r\nLIBCFS_CPT_ALLOC(conn->ibc_connvars, lnet_cpt_table(), cpt,\r\nsizeof(*conn->ibc_connvars));\r\nif (conn->ibc_connvars == NULL) {\r\nCERROR("Can't allocate in-progress connection state\n");\r\ngoto failed_2;\r\n}\r\nwrite_lock_irqsave(glock, flags);\r\nif (dev->ibd_failover) {\r\nwrite_unlock_irqrestore(glock, flags);\r\nCERROR("%s: failover in progress\n", dev->ibd_ifname);\r\ngoto failed_2;\r\n}\r\nif (dev->ibd_hdev->ibh_ibdev != cmid->device) {\r\nif (kiblnd_dev_can_failover(dev)) {\r\nlist_add_tail(&dev->ibd_fail_list,\r\n&kiblnd_data.kib_failed_devs);\r\nwake_up(&kiblnd_data.kib_failover_waitq);\r\n}\r\nwrite_unlock_irqrestore(glock, flags);\r\nCERROR("cmid HCA(%s), kib_dev(%s) need failover\n",\r\ncmid->device->name, dev->ibd_ifname);\r\ngoto failed_2;\r\n}\r\nkiblnd_hdev_addref_locked(dev->ibd_hdev);\r\nconn->ibc_hdev = dev->ibd_hdev;\r\nkiblnd_setup_mtu_locked(cmid);\r\nwrite_unlock_irqrestore(glock, flags);\r\nLIBCFS_CPT_ALLOC(conn->ibc_rxs, lnet_cpt_table(), cpt,\r\nIBLND_RX_MSGS(version) * sizeof(kib_rx_t));\r\nif (conn->ibc_rxs == NULL) {\r\nCERROR("Cannot allocate RX buffers\n");\r\ngoto failed_2;\r\n}\r\nrc = kiblnd_alloc_pages(&conn->ibc_rx_pages, cpt,\r\nIBLND_RX_MSG_PAGES(version));\r\nif (rc != 0)\r\ngoto failed_2;\r\nkiblnd_map_rx_descs(conn);\r\ncq = ib_create_cq(cmid->device,\r\nkiblnd_cq_completion, kiblnd_cq_event, conn,\r\nIBLND_CQ_ENTRIES(version),\r\nkiblnd_get_completion_vector(conn, cpt));\r\nif (IS_ERR(cq)) {\r\nCERROR("Can't create CQ: %ld, cqe: %d\n",\r\nPTR_ERR(cq), IBLND_CQ_ENTRIES(version));\r\ngoto failed_2;\r\n}\r\nconn->ibc_cq = cq;\r\nrc = ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\r\nif (rc != 0) {\r\nCERROR("Can't request completion notificiation: %d\n", rc);\r\ngoto failed_2;\r\n}\r\ninit_qp_attr->event_handler = kiblnd_qp_event;\r\ninit_qp_attr->qp_context = conn;\r\ninit_qp_attr->cap.max_send_wr = IBLND_SEND_WRS(version);\r\ninit_qp_attr->cap.max_recv_wr = IBLND_RECV_WRS(version);\r\ninit_qp_attr->cap.max_send_sge = 1;\r\ninit_qp_attr->cap.max_recv_sge = 1;\r\ninit_qp_attr->sq_sig_type = IB_SIGNAL_REQ_WR;\r\ninit_qp_attr->qp_type = IB_QPT_RC;\r\ninit_qp_attr->send_cq = cq;\r\ninit_qp_attr->recv_cq = cq;\r\nconn->ibc_sched = sched;\r\nrc = rdma_create_qp(cmid, conn->ibc_hdev->ibh_pd, init_qp_attr);\r\nif (rc != 0) {\r\nCERROR("Can't create QP: %d, send_wr: %d, recv_wr: %d\n",\r\nrc, init_qp_attr->cap.max_send_wr,\r\ninit_qp_attr->cap.max_recv_wr);\r\ngoto failed_2;\r\n}\r\nLIBCFS_FREE(init_qp_attr, sizeof(*init_qp_attr));\r\natomic_set(&conn->ibc_refcount, 1 + IBLND_RX_MSGS(version));\r\nconn->ibc_nrx = IBLND_RX_MSGS(version);\r\nfor (i = 0; i < IBLND_RX_MSGS(version); i++) {\r\nrc = kiblnd_post_rx(&conn->ibc_rxs[i],\r\nIBLND_POSTRX_NO_CREDIT);\r\nif (rc != 0) {\r\nCERROR("Can't post rxmsg: %d\n", rc);\r\nkiblnd_abort_receives(conn);\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\nconn->ibc_nrx -= IBLND_RX_MSGS(version) - i;\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\nrdma_destroy_qp(conn->ibc_cmid);\r\nconn->ibc_cmid = NULL;\r\nwhile (i++ <= IBLND_RX_MSGS(version))\r\nkiblnd_conn_decref(conn);\r\nreturn NULL;\r\n}\r\n}\r\nLASSERT (state == IBLND_CONN_ACTIVE_CONNECT ||\r\nstate == IBLND_CONN_PASSIVE_WAIT);\r\nconn->ibc_state = state;\r\natomic_inc(&net->ibn_nconns);\r\nreturn conn;\r\nfailed_2:\r\nkiblnd_destroy_conn(conn);\r\nfailed_1:\r\nLIBCFS_FREE(init_qp_attr, sizeof(*init_qp_attr));\r\nfailed_0:\r\nreturn NULL;\r\n}\r\nvoid\r\nkiblnd_destroy_conn (kib_conn_t *conn)\r\n{\r\nstruct rdma_cm_id *cmid = conn->ibc_cmid;\r\nkib_peer_t *peer = conn->ibc_peer;\r\nint rc;\r\nLASSERT (!in_interrupt());\r\nLASSERT (atomic_read(&conn->ibc_refcount) == 0);\r\nLASSERT (list_empty(&conn->ibc_early_rxs));\r\nLASSERT (list_empty(&conn->ibc_tx_noops));\r\nLASSERT (list_empty(&conn->ibc_tx_queue));\r\nLASSERT (list_empty(&conn->ibc_tx_queue_rsrvd));\r\nLASSERT (list_empty(&conn->ibc_tx_queue_nocred));\r\nLASSERT (list_empty(&conn->ibc_active_txs));\r\nLASSERT (conn->ibc_noops_posted == 0);\r\nLASSERT (conn->ibc_nsends_posted == 0);\r\nswitch (conn->ibc_state) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_CONN_DISCONNECTED:\r\nLASSERT (conn->ibc_connvars == NULL);\r\nbreak;\r\ncase IBLND_CONN_INIT:\r\nbreak;\r\n}\r\nif (cmid != NULL && cmid->qp != NULL)\r\nrdma_destroy_qp(cmid);\r\nif (conn->ibc_cq != NULL) {\r\nrc = ib_destroy_cq(conn->ibc_cq);\r\nif (rc != 0)\r\nCWARN("Error destroying CQ: %d\n", rc);\r\n}\r\nif (conn->ibc_rx_pages != NULL)\r\nkiblnd_unmap_rx_descs(conn);\r\nif (conn->ibc_rxs != NULL) {\r\nLIBCFS_FREE(conn->ibc_rxs,\r\nIBLND_RX_MSGS(conn->ibc_version) * sizeof(kib_rx_t));\r\n}\r\nif (conn->ibc_connvars != NULL)\r\nLIBCFS_FREE(conn->ibc_connvars, sizeof(*conn->ibc_connvars));\r\nif (conn->ibc_hdev != NULL)\r\nkiblnd_hdev_decref(conn->ibc_hdev);\r\nif (conn->ibc_state != IBLND_CONN_INIT) {\r\nkib_net_t *net = peer->ibp_ni->ni_data;\r\nkiblnd_peer_decref(peer);\r\nrdma_destroy_id(cmid);\r\natomic_dec(&net->ibn_nconns);\r\n}\r\nLIBCFS_FREE(conn, sizeof(*conn));\r\n}\r\nint\r\nkiblnd_close_peer_conns_locked (kib_peer_t *peer, int why)\r\n{\r\nkib_conn_t *conn;\r\nstruct list_head *ctmp;\r\nstruct list_head *cnxt;\r\nint count = 0;\r\nlist_for_each_safe (ctmp, cnxt, &peer->ibp_conns) {\r\nconn = list_entry(ctmp, kib_conn_t, ibc_list);\r\nCDEBUG(D_NET, "Closing conn -> %s, "\r\n"version: %x, reason: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nconn->ibc_version, why);\r\nkiblnd_close_conn_locked(conn, why);\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nint\r\nkiblnd_close_stale_conns_locked (kib_peer_t *peer,\r\nint version, __u64 incarnation)\r\n{\r\nkib_conn_t *conn;\r\nstruct list_head *ctmp;\r\nstruct list_head *cnxt;\r\nint count = 0;\r\nlist_for_each_safe (ctmp, cnxt, &peer->ibp_conns) {\r\nconn = list_entry(ctmp, kib_conn_t, ibc_list);\r\nif (conn->ibc_version == version &&\r\nconn->ibc_incarnation == incarnation)\r\ncontinue;\r\nCDEBUG(D_NET, "Closing stale conn -> %s version: %x, "\r\n"incarnation:"LPX64"(%x, "LPX64")\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nconn->ibc_version, conn->ibc_incarnation,\r\nversion, incarnation);\r\nkiblnd_close_conn_locked(conn, -ESTALE);\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nint\r\nkiblnd_close_matching_conns (lnet_ni_t *ni, lnet_nid_t nid)\r\n{\r\nkib_peer_t *peer;\r\nstruct list_head *ptmp;\r\nstruct list_head *pnxt;\r\nint lo;\r\nint hi;\r\nint i;\r\nunsigned long flags;\r\nint count = 0;\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nif (nid != LNET_NID_ANY)\r\nlo = hi = kiblnd_nid2peerlist(nid) - kiblnd_data.kib_peers;\r\nelse {\r\nlo = 0;\r\nhi = kiblnd_data.kib_peer_hash_size - 1;\r\n}\r\nfor (i = lo; i <= hi; i++) {\r\nlist_for_each_safe (ptmp, pnxt, &kiblnd_data.kib_peers[i]) {\r\npeer = list_entry(ptmp, kib_peer_t, ibp_list);\r\nLASSERT (peer->ibp_connecting > 0 ||\r\npeer->ibp_accepting > 0 ||\r\n!list_empty(&peer->ibp_conns));\r\nif (peer->ibp_ni != ni)\r\ncontinue;\r\nif (!(nid == LNET_NID_ANY || nid == peer->ibp_nid))\r\ncontinue;\r\ncount += kiblnd_close_peer_conns_locked(peer, 0);\r\n}\r\n}\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nif (nid == LNET_NID_ANY)\r\nreturn 0;\r\nreturn (count == 0) ? -ENOENT : 0;\r\n}\r\nint\r\nkiblnd_ctl(lnet_ni_t *ni, unsigned int cmd, void *arg)\r\n{\r\nstruct libcfs_ioctl_data *data = arg;\r\nint rc = -EINVAL;\r\nswitch(cmd) {\r\ncase IOC_LIBCFS_GET_PEER: {\r\nlnet_nid_t nid = 0;\r\nint count = 0;\r\nrc = kiblnd_get_peer_info(ni, data->ioc_count,\r\n&nid, &count);\r\ndata->ioc_nid = nid;\r\ndata->ioc_count = count;\r\nbreak;\r\n}\r\ncase IOC_LIBCFS_DEL_PEER: {\r\nrc = kiblnd_del_peer(ni, data->ioc_nid);\r\nbreak;\r\n}\r\ncase IOC_LIBCFS_GET_CONN: {\r\nkib_conn_t *conn;\r\nrc = 0;\r\nconn = kiblnd_get_conn_by_idx(ni, data->ioc_count);\r\nif (conn == NULL) {\r\nrc = -ENOENT;\r\nbreak;\r\n}\r\nLASSERT (conn->ibc_cmid != NULL);\r\ndata->ioc_nid = conn->ibc_peer->ibp_nid;\r\nif (conn->ibc_cmid->route.path_rec == NULL)\r\ndata->ioc_u32[0] = 0;\r\nelse\r\ndata->ioc_u32[0] =\r\nib_mtu_enum_to_int(conn->ibc_cmid->route.path_rec->mtu);\r\nkiblnd_conn_decref(conn);\r\nbreak;\r\n}\r\ncase IOC_LIBCFS_CLOSE_CONNECTION: {\r\nrc = kiblnd_close_matching_conns(ni, data->ioc_nid);\r\nbreak;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\nreturn rc;\r\n}\r\nvoid\r\nkiblnd_query (lnet_ni_t *ni, lnet_nid_t nid, cfs_time_t *when)\r\n{\r\ncfs_time_t last_alive = 0;\r\ncfs_time_t now = cfs_time_current();\r\nrwlock_t *glock = &kiblnd_data.kib_global_lock;\r\nkib_peer_t *peer;\r\nunsigned long flags;\r\nread_lock_irqsave(glock, flags);\r\npeer = kiblnd_find_peer_locked(nid);\r\nif (peer != NULL) {\r\nLASSERT (peer->ibp_connecting > 0 ||\r\npeer->ibp_accepting > 0 ||\r\n!list_empty(&peer->ibp_conns));\r\nlast_alive = peer->ibp_last_alive;\r\n}\r\nread_unlock_irqrestore(glock, flags);\r\nif (last_alive != 0)\r\n*when = last_alive;\r\nif (peer == NULL)\r\nkiblnd_launch_tx(ni, NULL, nid);\r\nCDEBUG(D_NET, "Peer %s %p, alive %ld secs ago\n",\r\nlibcfs_nid2str(nid), peer,\r\nlast_alive ? cfs_duration_sec(now - last_alive) : -1);\r\nreturn;\r\n}\r\nvoid\r\nkiblnd_free_pages(kib_pages_t *p)\r\n{\r\nint npages = p->ibp_npages;\r\nint i;\r\nfor (i = 0; i < npages; i++) {\r\nif (p->ibp_pages[i] != NULL)\r\n__free_page(p->ibp_pages[i]);\r\n}\r\nLIBCFS_FREE(p, offsetof(kib_pages_t, ibp_pages[npages]));\r\n}\r\nint\r\nkiblnd_alloc_pages(kib_pages_t **pp, int cpt, int npages)\r\n{\r\nkib_pages_t *p;\r\nint i;\r\nLIBCFS_CPT_ALLOC(p, lnet_cpt_table(), cpt,\r\noffsetof(kib_pages_t, ibp_pages[npages]));\r\nif (p == NULL) {\r\nCERROR("Can't allocate descriptor for %d pages\n", npages);\r\nreturn -ENOMEM;\r\n}\r\nmemset(p, 0, offsetof(kib_pages_t, ibp_pages[npages]));\r\np->ibp_npages = npages;\r\nfor (i = 0; i < npages; i++) {\r\np->ibp_pages[i] = alloc_pages_node(\r\ncfs_cpt_spread_node(lnet_cpt_table(), cpt),\r\n__GFP_IO, 0);\r\nif (p->ibp_pages[i] == NULL) {\r\nCERROR("Can't allocate page %d of %d\n", i, npages);\r\nkiblnd_free_pages(p);\r\nreturn -ENOMEM;\r\n}\r\n}\r\n*pp = p;\r\nreturn 0;\r\n}\r\nvoid\r\nkiblnd_unmap_rx_descs(kib_conn_t *conn)\r\n{\r\nkib_rx_t *rx;\r\nint i;\r\nLASSERT (conn->ibc_rxs != NULL);\r\nLASSERT (conn->ibc_hdev != NULL);\r\nfor (i = 0; i < IBLND_RX_MSGS(conn->ibc_version); i++) {\r\nrx = &conn->ibc_rxs[i];\r\nLASSERT (rx->rx_nob >= 0);\r\nkiblnd_dma_unmap_single(conn->ibc_hdev->ibh_ibdev,\r\nKIBLND_UNMAP_ADDR(rx, rx_msgunmap,\r\nrx->rx_msgaddr),\r\nIBLND_MSG_SIZE, DMA_FROM_DEVICE);\r\n}\r\nkiblnd_free_pages(conn->ibc_rx_pages);\r\nconn->ibc_rx_pages = NULL;\r\n}\r\nvoid\r\nkiblnd_map_rx_descs(kib_conn_t *conn)\r\n{\r\nkib_rx_t *rx;\r\nstruct page *pg;\r\nint pg_off;\r\nint ipg;\r\nint i;\r\nfor (pg_off = ipg = i = 0;\r\ni < IBLND_RX_MSGS(conn->ibc_version); i++) {\r\npg = conn->ibc_rx_pages->ibp_pages[ipg];\r\nrx = &conn->ibc_rxs[i];\r\nrx->rx_conn = conn;\r\nrx->rx_msg = (kib_msg_t *)(((char *)page_address(pg)) + pg_off);\r\nrx->rx_msgaddr = kiblnd_dma_map_single(conn->ibc_hdev->ibh_ibdev,\r\nrx->rx_msg, IBLND_MSG_SIZE,\r\nDMA_FROM_DEVICE);\r\nLASSERT (!kiblnd_dma_mapping_error(conn->ibc_hdev->ibh_ibdev,\r\nrx->rx_msgaddr));\r\nKIBLND_UNMAP_ADDR_SET(rx, rx_msgunmap, rx->rx_msgaddr);\r\nCDEBUG(D_NET,"rx %d: %p "LPX64"("LPX64")\n",\r\ni, rx->rx_msg, rx->rx_msgaddr,\r\nlnet_page2phys(pg) + pg_off);\r\npg_off += IBLND_MSG_SIZE;\r\nLASSERT (pg_off <= PAGE_SIZE);\r\nif (pg_off == PAGE_SIZE) {\r\npg_off = 0;\r\nipg++;\r\nLASSERT (ipg <= IBLND_RX_MSG_PAGES(conn->ibc_version));\r\n}\r\n}\r\n}\r\nstatic void\r\nkiblnd_unmap_tx_pool(kib_tx_pool_t *tpo)\r\n{\r\nkib_hca_dev_t *hdev = tpo->tpo_hdev;\r\nkib_tx_t *tx;\r\nint i;\r\nLASSERT (tpo->tpo_pool.po_allocated == 0);\r\nif (hdev == NULL)\r\nreturn;\r\nfor (i = 0; i < tpo->tpo_pool.po_size; i++) {\r\ntx = &tpo->tpo_tx_descs[i];\r\nkiblnd_dma_unmap_single(hdev->ibh_ibdev,\r\nKIBLND_UNMAP_ADDR(tx, tx_msgunmap,\r\ntx->tx_msgaddr),\r\nIBLND_MSG_SIZE, DMA_TO_DEVICE);\r\n}\r\nkiblnd_hdev_decref(hdev);\r\ntpo->tpo_hdev = NULL;\r\n}\r\nstatic kib_hca_dev_t *\r\nkiblnd_current_hdev(kib_dev_t *dev)\r\n{\r\nkib_hca_dev_t *hdev;\r\nunsigned long flags;\r\nint i = 0;\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nwhile (dev->ibd_failover) {\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nif (i++ % 50 == 0)\r\nCDEBUG(D_NET, "%s: Wait for failover\n",\r\ndev->ibd_ifname);\r\nschedule_timeout(cfs_time_seconds(1) / 100);\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\n}\r\nkiblnd_hdev_addref_locked(dev->ibd_hdev);\r\nhdev = dev->ibd_hdev;\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nreturn hdev;\r\n}\r\nstatic void\r\nkiblnd_map_tx_pool(kib_tx_pool_t *tpo)\r\n{\r\nkib_pages_t *txpgs = tpo->tpo_tx_pages;\r\nkib_pool_t *pool = &tpo->tpo_pool;\r\nkib_net_t *net = pool->po_owner->ps_net;\r\nkib_dev_t *dev;\r\nstruct page *page;\r\nkib_tx_t *tx;\r\nint page_offset;\r\nint ipage;\r\nint i;\r\nLASSERT (net != NULL);\r\ndev = net->ibn_dev;\r\nCLASSERT (IBLND_MSG_SIZE <= PAGE_SIZE);\r\nCLASSERT (PAGE_SIZE % IBLND_MSG_SIZE == 0);\r\ntpo->tpo_hdev = kiblnd_current_hdev(dev);\r\nfor (ipage = page_offset = i = 0; i < pool->po_size; i++) {\r\npage = txpgs->ibp_pages[ipage];\r\ntx = &tpo->tpo_tx_descs[i];\r\ntx->tx_msg = (kib_msg_t *)(((char *)page_address(page)) +\r\npage_offset);\r\ntx->tx_msgaddr = kiblnd_dma_map_single(\r\ntpo->tpo_hdev->ibh_ibdev, tx->tx_msg,\r\nIBLND_MSG_SIZE, DMA_TO_DEVICE);\r\nLASSERT (!kiblnd_dma_mapping_error(tpo->tpo_hdev->ibh_ibdev,\r\ntx->tx_msgaddr));\r\nKIBLND_UNMAP_ADDR_SET(tx, tx_msgunmap, tx->tx_msgaddr);\r\nlist_add(&tx->tx_list, &pool->po_free_list);\r\npage_offset += IBLND_MSG_SIZE;\r\nLASSERT (page_offset <= PAGE_SIZE);\r\nif (page_offset == PAGE_SIZE) {\r\npage_offset = 0;\r\nipage++;\r\nLASSERT (ipage <= txpgs->ibp_npages);\r\n}\r\n}\r\n}\r\nstruct ib_mr *\r\nkiblnd_find_dma_mr(kib_hca_dev_t *hdev, __u64 addr, __u64 size)\r\n{\r\n__u64 index;\r\nLASSERT (hdev->ibh_mrs[0] != NULL);\r\nif (hdev->ibh_nmrs == 1)\r\nreturn hdev->ibh_mrs[0];\r\nindex = addr >> hdev->ibh_mr_shift;\r\nif (index < hdev->ibh_nmrs &&\r\nindex == ((addr + size - 1) >> hdev->ibh_mr_shift))\r\nreturn hdev->ibh_mrs[index];\r\nreturn NULL;\r\n}\r\nstruct ib_mr *\r\nkiblnd_find_rd_dma_mr(kib_hca_dev_t *hdev, kib_rdma_desc_t *rd)\r\n{\r\nstruct ib_mr *prev_mr;\r\nstruct ib_mr *mr;\r\nint i;\r\nLASSERT (hdev->ibh_mrs[0] != NULL);\r\nif (*kiblnd_tunables.kib_map_on_demand > 0 &&\r\n*kiblnd_tunables.kib_map_on_demand <= rd->rd_nfrags)\r\nreturn NULL;\r\nif (hdev->ibh_nmrs == 1)\r\nreturn hdev->ibh_mrs[0];\r\nfor (i = 0, mr = prev_mr = NULL;\r\ni < rd->rd_nfrags; i++) {\r\nmr = kiblnd_find_dma_mr(hdev,\r\nrd->rd_frags[i].rf_addr,\r\nrd->rd_frags[i].rf_nob);\r\nif (prev_mr == NULL)\r\nprev_mr = mr;\r\nif (mr == NULL || prev_mr != mr) {\r\nmr = NULL;\r\nbreak;\r\n}\r\n}\r\nreturn mr;\r\n}\r\nvoid\r\nkiblnd_destroy_fmr_pool(kib_fmr_pool_t *pool)\r\n{\r\nLASSERT (pool->fpo_map_count == 0);\r\nif (pool->fpo_fmr_pool != NULL)\r\nib_destroy_fmr_pool(pool->fpo_fmr_pool);\r\nif (pool->fpo_hdev != NULL)\r\nkiblnd_hdev_decref(pool->fpo_hdev);\r\nLIBCFS_FREE(pool, sizeof(kib_fmr_pool_t));\r\n}\r\nvoid\r\nkiblnd_destroy_fmr_pool_list(struct list_head *head)\r\n{\r\nkib_fmr_pool_t *pool;\r\nwhile (!list_empty(head)) {\r\npool = list_entry(head->next, kib_fmr_pool_t, fpo_list);\r\nlist_del(&pool->fpo_list);\r\nkiblnd_destroy_fmr_pool(pool);\r\n}\r\n}\r\nstatic int kiblnd_fmr_pool_size(int ncpts)\r\n{\r\nint size = *kiblnd_tunables.kib_fmr_pool_size / ncpts;\r\nreturn max(IBLND_FMR_POOL, size);\r\n}\r\nstatic int kiblnd_fmr_flush_trigger(int ncpts)\r\n{\r\nint size = *kiblnd_tunables.kib_fmr_flush_trigger / ncpts;\r\nreturn max(IBLND_FMR_POOL_FLUSH, size);\r\n}\r\nint\r\nkiblnd_create_fmr_pool(kib_fmr_poolset_t *fps, kib_fmr_pool_t **pp_fpo)\r\n{\r\nkib_dev_t *dev = fps->fps_net->ibn_dev;\r\nkib_fmr_pool_t *fpo;\r\nstruct ib_fmr_pool_param param = {\r\n.max_pages_per_fmr = LNET_MAX_PAYLOAD/PAGE_SIZE,\r\n.page_shift = PAGE_SHIFT,\r\n.access = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE),\r\n.pool_size = fps->fps_pool_size,\r\n.dirty_watermark = fps->fps_flush_trigger,\r\n.flush_function = NULL,\r\n.flush_arg = NULL,\r\n.cache = !!*kiblnd_tunables.kib_fmr_cache};\r\nint rc;\r\nLIBCFS_CPT_ALLOC(fpo, lnet_cpt_table(), fps->fps_cpt, sizeof(*fpo));\r\nif (fpo == NULL)\r\nreturn -ENOMEM;\r\nfpo->fpo_hdev = kiblnd_current_hdev(dev);\r\nfpo->fpo_fmr_pool = ib_create_fmr_pool(fpo->fpo_hdev->ibh_pd, &param);\r\nif (IS_ERR(fpo->fpo_fmr_pool)) {\r\nrc = PTR_ERR(fpo->fpo_fmr_pool);\r\nCERROR("Failed to create FMR pool: %d\n", rc);\r\nkiblnd_hdev_decref(fpo->fpo_hdev);\r\nLIBCFS_FREE(fpo, sizeof(kib_fmr_pool_t));\r\nreturn rc;\r\n}\r\nfpo->fpo_deadline = cfs_time_shift(IBLND_POOL_DEADLINE);\r\nfpo->fpo_owner = fps;\r\n*pp_fpo = fpo;\r\nreturn 0;\r\n}\r\nstatic void\r\nkiblnd_fail_fmr_poolset(kib_fmr_poolset_t *fps, struct list_head *zombies)\r\n{\r\nif (fps->fps_net == NULL)\r\nreturn;\r\nspin_lock(&fps->fps_lock);\r\nwhile (!list_empty(&fps->fps_pool_list)) {\r\nkib_fmr_pool_t *fpo = list_entry(fps->fps_pool_list.next,\r\nkib_fmr_pool_t, fpo_list);\r\nfpo->fpo_failed = 1;\r\nlist_del(&fpo->fpo_list);\r\nif (fpo->fpo_map_count == 0)\r\nlist_add(&fpo->fpo_list, zombies);\r\nelse\r\nlist_add(&fpo->fpo_list, &fps->fps_failed_pool_list);\r\n}\r\nspin_unlock(&fps->fps_lock);\r\n}\r\nstatic void\r\nkiblnd_fini_fmr_poolset(kib_fmr_poolset_t *fps)\r\n{\r\nif (fps->fps_net != NULL) {\r\nkiblnd_destroy_fmr_pool_list(&fps->fps_failed_pool_list);\r\nkiblnd_destroy_fmr_pool_list(&fps->fps_pool_list);\r\n}\r\n}\r\nstatic int\r\nkiblnd_init_fmr_poolset(kib_fmr_poolset_t *fps, int cpt, kib_net_t *net,\r\nint pool_size, int flush_trigger)\r\n{\r\nkib_fmr_pool_t *fpo;\r\nint rc;\r\nmemset(fps, 0, sizeof(kib_fmr_poolset_t));\r\nfps->fps_net = net;\r\nfps->fps_cpt = cpt;\r\nfps->fps_pool_size = pool_size;\r\nfps->fps_flush_trigger = flush_trigger;\r\nspin_lock_init(&fps->fps_lock);\r\nINIT_LIST_HEAD(&fps->fps_pool_list);\r\nINIT_LIST_HEAD(&fps->fps_failed_pool_list);\r\nrc = kiblnd_create_fmr_pool(fps, &fpo);\r\nif (rc == 0)\r\nlist_add_tail(&fpo->fpo_list, &fps->fps_pool_list);\r\nreturn rc;\r\n}\r\nstatic int\r\nkiblnd_fmr_pool_is_idle(kib_fmr_pool_t *fpo, cfs_time_t now)\r\n{\r\nif (fpo->fpo_map_count != 0)\r\nreturn 0;\r\nif (fpo->fpo_failed)\r\nreturn 1;\r\nreturn cfs_time_aftereq(now, fpo->fpo_deadline);\r\n}\r\nvoid\r\nkiblnd_fmr_pool_unmap(kib_fmr_t *fmr, int status)\r\n{\r\nLIST_HEAD (zombies);\r\nkib_fmr_pool_t *fpo = fmr->fmr_pool;\r\nkib_fmr_poolset_t *fps = fpo->fpo_owner;\r\ncfs_time_t now = cfs_time_current();\r\nkib_fmr_pool_t *tmp;\r\nint rc;\r\nrc = ib_fmr_pool_unmap(fmr->fmr_pfmr);\r\nLASSERT (rc == 0);\r\nif (status != 0) {\r\nrc = ib_flush_fmr_pool(fpo->fpo_fmr_pool);\r\nLASSERT (rc == 0);\r\n}\r\nfmr->fmr_pool = NULL;\r\nfmr->fmr_pfmr = NULL;\r\nspin_lock(&fps->fps_lock);\r\nfpo->fpo_map_count --;\r\nlist_for_each_entry_safe(fpo, tmp, &fps->fps_pool_list, fpo_list) {\r\nif (fps->fps_pool_list.next == &fpo->fpo_list)\r\ncontinue;\r\nif (kiblnd_fmr_pool_is_idle(fpo, now)) {\r\nlist_move(&fpo->fpo_list, &zombies);\r\nfps->fps_version ++;\r\n}\r\n}\r\nspin_unlock(&fps->fps_lock);\r\nif (!list_empty(&zombies))\r\nkiblnd_destroy_fmr_pool_list(&zombies);\r\n}\r\nint\r\nkiblnd_fmr_pool_map(kib_fmr_poolset_t *fps, __u64 *pages, int npages,\r\n__u64 iov, kib_fmr_t *fmr)\r\n{\r\nstruct ib_pool_fmr *pfmr;\r\nkib_fmr_pool_t *fpo;\r\n__u64 version;\r\nint rc;\r\nagain:\r\nspin_lock(&fps->fps_lock);\r\nversion = fps->fps_version;\r\nlist_for_each_entry(fpo, &fps->fps_pool_list, fpo_list) {\r\nfpo->fpo_deadline = cfs_time_shift(IBLND_POOL_DEADLINE);\r\nfpo->fpo_map_count++;\r\nspin_unlock(&fps->fps_lock);\r\npfmr = ib_fmr_pool_map_phys(fpo->fpo_fmr_pool,\r\npages, npages, iov);\r\nif (likely(!IS_ERR(pfmr))) {\r\nfmr->fmr_pool = fpo;\r\nfmr->fmr_pfmr = pfmr;\r\nreturn 0;\r\n}\r\nspin_lock(&fps->fps_lock);\r\nfpo->fpo_map_count--;\r\nif (PTR_ERR(pfmr) != -EAGAIN) {\r\nspin_unlock(&fps->fps_lock);\r\nreturn PTR_ERR(pfmr);\r\n}\r\nif (version != fps->fps_version) {\r\nspin_unlock(&fps->fps_lock);\r\ngoto again;\r\n}\r\n}\r\nif (fps->fps_increasing) {\r\nspin_unlock(&fps->fps_lock);\r\nCDEBUG(D_NET, "Another thread is allocating new "\r\n"FMR pool, waiting for her to complete\n");\r\nschedule();\r\ngoto again;\r\n}\r\nif (cfs_time_before(cfs_time_current(), fps->fps_next_retry)) {\r\nspin_unlock(&fps->fps_lock);\r\nreturn -EAGAIN;\r\n}\r\nfps->fps_increasing = 1;\r\nspin_unlock(&fps->fps_lock);\r\nCDEBUG(D_NET, "Allocate new FMR pool\n");\r\nrc = kiblnd_create_fmr_pool(fps, &fpo);\r\nspin_lock(&fps->fps_lock);\r\nfps->fps_increasing = 0;\r\nif (rc == 0) {\r\nfps->fps_version++;\r\nlist_add_tail(&fpo->fpo_list, &fps->fps_pool_list);\r\n} else {\r\nfps->fps_next_retry = cfs_time_shift(IBLND_POOL_RETRY);\r\n}\r\nspin_unlock(&fps->fps_lock);\r\ngoto again;\r\n}\r\nstatic void\r\nkiblnd_fini_pool(kib_pool_t *pool)\r\n{\r\nLASSERT (list_empty(&pool->po_free_list));\r\nLASSERT (pool->po_allocated == 0);\r\nCDEBUG(D_NET, "Finalize %s pool\n", pool->po_owner->ps_name);\r\n}\r\nstatic void\r\nkiblnd_init_pool(kib_poolset_t *ps, kib_pool_t *pool, int size)\r\n{\r\nCDEBUG(D_NET, "Initialize %s pool\n", ps->ps_name);\r\nmemset(pool, 0, sizeof(kib_pool_t));\r\nINIT_LIST_HEAD(&pool->po_free_list);\r\npool->po_deadline = cfs_time_shift(IBLND_POOL_DEADLINE);\r\npool->po_owner = ps;\r\npool->po_size = size;\r\n}\r\nvoid\r\nkiblnd_destroy_pool_list(struct list_head *head)\r\n{\r\nkib_pool_t *pool;\r\nwhile (!list_empty(head)) {\r\npool = list_entry(head->next, kib_pool_t, po_list);\r\nlist_del(&pool->po_list);\r\nLASSERT (pool->po_owner != NULL);\r\npool->po_owner->ps_pool_destroy(pool);\r\n}\r\n}\r\nstatic void\r\nkiblnd_fail_poolset(kib_poolset_t *ps, struct list_head *zombies)\r\n{\r\nif (ps->ps_net == NULL)\r\nreturn;\r\nspin_lock(&ps->ps_lock);\r\nwhile (!list_empty(&ps->ps_pool_list)) {\r\nkib_pool_t *po = list_entry(ps->ps_pool_list.next,\r\nkib_pool_t, po_list);\r\npo->po_failed = 1;\r\nlist_del(&po->po_list);\r\nif (po->po_allocated == 0)\r\nlist_add(&po->po_list, zombies);\r\nelse\r\nlist_add(&po->po_list, &ps->ps_failed_pool_list);\r\n}\r\nspin_unlock(&ps->ps_lock);\r\n}\r\nstatic void\r\nkiblnd_fini_poolset(kib_poolset_t *ps)\r\n{\r\nif (ps->ps_net != NULL) {\r\nkiblnd_destroy_pool_list(&ps->ps_failed_pool_list);\r\nkiblnd_destroy_pool_list(&ps->ps_pool_list);\r\n}\r\n}\r\nstatic int\r\nkiblnd_init_poolset(kib_poolset_t *ps, int cpt,\r\nkib_net_t *net, char *name, int size,\r\nkib_ps_pool_create_t po_create,\r\nkib_ps_pool_destroy_t po_destroy,\r\nkib_ps_node_init_t nd_init,\r\nkib_ps_node_fini_t nd_fini)\r\n{\r\nkib_pool_t *pool;\r\nint rc;\r\nmemset(ps, 0, sizeof(kib_poolset_t));\r\nps->ps_cpt = cpt;\r\nps->ps_net = net;\r\nps->ps_pool_create = po_create;\r\nps->ps_pool_destroy = po_destroy;\r\nps->ps_node_init = nd_init;\r\nps->ps_node_fini = nd_fini;\r\nps->ps_pool_size = size;\r\nif (strlcpy(ps->ps_name, name, sizeof(ps->ps_name))\r\n>= sizeof(ps->ps_name))\r\nreturn -E2BIG;\r\nspin_lock_init(&ps->ps_lock);\r\nINIT_LIST_HEAD(&ps->ps_pool_list);\r\nINIT_LIST_HEAD(&ps->ps_failed_pool_list);\r\nrc = ps->ps_pool_create(ps, size, &pool);\r\nif (rc == 0)\r\nlist_add(&pool->po_list, &ps->ps_pool_list);\r\nelse\r\nCERROR("Failed to create the first pool for %s\n", ps->ps_name);\r\nreturn rc;\r\n}\r\nstatic int\r\nkiblnd_pool_is_idle(kib_pool_t *pool, cfs_time_t now)\r\n{\r\nif (pool->po_allocated != 0)\r\nreturn 0;\r\nif (pool->po_failed)\r\nreturn 1;\r\nreturn cfs_time_aftereq(now, pool->po_deadline);\r\n}\r\nvoid\r\nkiblnd_pool_free_node(kib_pool_t *pool, struct list_head *node)\r\n{\r\nLIST_HEAD (zombies);\r\nkib_poolset_t *ps = pool->po_owner;\r\nkib_pool_t *tmp;\r\ncfs_time_t now = cfs_time_current();\r\nspin_lock(&ps->ps_lock);\r\nif (ps->ps_node_fini != NULL)\r\nps->ps_node_fini(pool, node);\r\nLASSERT (pool->po_allocated > 0);\r\nlist_add(node, &pool->po_free_list);\r\npool->po_allocated --;\r\nlist_for_each_entry_safe(pool, tmp, &ps->ps_pool_list, po_list) {\r\nif (ps->ps_pool_list.next == &pool->po_list)\r\ncontinue;\r\nif (kiblnd_pool_is_idle(pool, now))\r\nlist_move(&pool->po_list, &zombies);\r\n}\r\nspin_unlock(&ps->ps_lock);\r\nif (!list_empty(&zombies))\r\nkiblnd_destroy_pool_list(&zombies);\r\n}\r\nstruct list_head *\r\nkiblnd_pool_alloc_node(kib_poolset_t *ps)\r\n{\r\nstruct list_head *node;\r\nkib_pool_t *pool;\r\nint rc;\r\nagain:\r\nspin_lock(&ps->ps_lock);\r\nlist_for_each_entry(pool, &ps->ps_pool_list, po_list) {\r\nif (list_empty(&pool->po_free_list))\r\ncontinue;\r\npool->po_allocated ++;\r\npool->po_deadline = cfs_time_shift(IBLND_POOL_DEADLINE);\r\nnode = pool->po_free_list.next;\r\nlist_del(node);\r\nif (ps->ps_node_init != NULL) {\r\nps->ps_node_init(pool, node);\r\n}\r\nspin_unlock(&ps->ps_lock);\r\nreturn node;\r\n}\r\nif (ps->ps_increasing) {\r\nspin_unlock(&ps->ps_lock);\r\nCDEBUG(D_NET, "Another thread is allocating new "\r\n"%s pool, waiting for her to complete\n",\r\nps->ps_name);\r\nschedule();\r\ngoto again;\r\n}\r\nif (cfs_time_before(cfs_time_current(), ps->ps_next_retry)) {\r\nspin_unlock(&ps->ps_lock);\r\nreturn NULL;\r\n}\r\nps->ps_increasing = 1;\r\nspin_unlock(&ps->ps_lock);\r\nCDEBUG(D_NET, "%s pool exhausted, allocate new pool\n", ps->ps_name);\r\nrc = ps->ps_pool_create(ps, ps->ps_pool_size, &pool);\r\nspin_lock(&ps->ps_lock);\r\nps->ps_increasing = 0;\r\nif (rc == 0) {\r\nlist_add_tail(&pool->po_list, &ps->ps_pool_list);\r\n} else {\r\nps->ps_next_retry = cfs_time_shift(IBLND_POOL_RETRY);\r\nCERROR("Can't allocate new %s pool because out of memory\n",\r\nps->ps_name);\r\n}\r\nspin_unlock(&ps->ps_lock);\r\ngoto again;\r\n}\r\nvoid\r\nkiblnd_pmr_pool_unmap(kib_phys_mr_t *pmr)\r\n{\r\nkib_pmr_pool_t *ppo = pmr->pmr_pool;\r\nstruct ib_mr *mr = pmr->pmr_mr;\r\npmr->pmr_mr = NULL;\r\nkiblnd_pool_free_node(&ppo->ppo_pool, &pmr->pmr_list);\r\nif (mr != NULL)\r\nib_dereg_mr(mr);\r\n}\r\nint\r\nkiblnd_pmr_pool_map(kib_pmr_poolset_t *pps, kib_hca_dev_t *hdev,\r\nkib_rdma_desc_t *rd, __u64 *iova, kib_phys_mr_t **pp_pmr)\r\n{\r\nkib_phys_mr_t *pmr;\r\nstruct list_head *node;\r\nint rc;\r\nint i;\r\nnode = kiblnd_pool_alloc_node(&pps->pps_poolset);\r\nif (node == NULL) {\r\nCERROR("Failed to allocate PMR descriptor\n");\r\nreturn -ENOMEM;\r\n}\r\npmr = container_of(node, kib_phys_mr_t, pmr_list);\r\nif (pmr->pmr_pool->ppo_hdev != hdev) {\r\nkiblnd_pool_free_node(&pmr->pmr_pool->ppo_pool, node);\r\nreturn -EAGAIN;\r\n}\r\nfor (i = 0; i < rd->rd_nfrags; i ++) {\r\npmr->pmr_ipb[i].addr = rd->rd_frags[i].rf_addr;\r\npmr->pmr_ipb[i].size = rd->rd_frags[i].rf_nob;\r\n}\r\npmr->pmr_mr = ib_reg_phys_mr(hdev->ibh_pd,\r\npmr->pmr_ipb, rd->rd_nfrags,\r\nIB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE,\r\niova);\r\nif (!IS_ERR(pmr->pmr_mr)) {\r\npmr->pmr_iova = *iova;\r\n*pp_pmr = pmr;\r\nreturn 0;\r\n}\r\nrc = PTR_ERR(pmr->pmr_mr);\r\nCERROR("Failed ib_reg_phys_mr: %d\n", rc);\r\npmr->pmr_mr = NULL;\r\nkiblnd_pool_free_node(&pmr->pmr_pool->ppo_pool, node);\r\nreturn rc;\r\n}\r\nstatic void\r\nkiblnd_destroy_pmr_pool(kib_pool_t *pool)\r\n{\r\nkib_pmr_pool_t *ppo = container_of(pool, kib_pmr_pool_t, ppo_pool);\r\nkib_phys_mr_t *pmr;\r\nLASSERT (pool->po_allocated == 0);\r\nwhile (!list_empty(&pool->po_free_list)) {\r\npmr = list_entry(pool->po_free_list.next,\r\nkib_phys_mr_t, pmr_list);\r\nLASSERT (pmr->pmr_mr == NULL);\r\nlist_del(&pmr->pmr_list);\r\nif (pmr->pmr_ipb != NULL) {\r\nLIBCFS_FREE(pmr->pmr_ipb,\r\nIBLND_MAX_RDMA_FRAGS *\r\nsizeof(struct ib_phys_buf));\r\n}\r\nLIBCFS_FREE(pmr, sizeof(kib_phys_mr_t));\r\n}\r\nkiblnd_fini_pool(pool);\r\nif (ppo->ppo_hdev != NULL)\r\nkiblnd_hdev_decref(ppo->ppo_hdev);\r\nLIBCFS_FREE(ppo, sizeof(kib_pmr_pool_t));\r\n}\r\nstatic inline int kiblnd_pmr_pool_size(int ncpts)\r\n{\r\nint size = *kiblnd_tunables.kib_pmr_pool_size / ncpts;\r\nreturn max(IBLND_PMR_POOL, size);\r\n}\r\nstatic int\r\nkiblnd_create_pmr_pool(kib_poolset_t *ps, int size, kib_pool_t **pp_po)\r\n{\r\nstruct kib_pmr_pool *ppo;\r\nstruct kib_pool *pool;\r\nkib_phys_mr_t *pmr;\r\nint i;\r\nLIBCFS_CPT_ALLOC(ppo, lnet_cpt_table(),\r\nps->ps_cpt, sizeof(kib_pmr_pool_t));\r\nif (ppo == NULL) {\r\nCERROR("Failed to allocate PMR pool\n");\r\nreturn -ENOMEM;\r\n}\r\npool = &ppo->ppo_pool;\r\nkiblnd_init_pool(ps, pool, size);\r\nfor (i = 0; i < size; i++) {\r\nLIBCFS_CPT_ALLOC(pmr, lnet_cpt_table(),\r\nps->ps_cpt, sizeof(kib_phys_mr_t));\r\nif (pmr == NULL)\r\nbreak;\r\npmr->pmr_pool = ppo;\r\nLIBCFS_CPT_ALLOC(pmr->pmr_ipb, lnet_cpt_table(), ps->ps_cpt,\r\nIBLND_MAX_RDMA_FRAGS * sizeof(*pmr->pmr_ipb));\r\nif (pmr->pmr_ipb == NULL)\r\nbreak;\r\nlist_add(&pmr->pmr_list, &pool->po_free_list);\r\n}\r\nif (i < size) {\r\nps->ps_pool_destroy(pool);\r\nreturn -ENOMEM;\r\n}\r\nppo->ppo_hdev = kiblnd_current_hdev(ps->ps_net->ibn_dev);\r\n*pp_po = pool;\r\nreturn 0;\r\n}\r\nstatic void\r\nkiblnd_destroy_tx_pool(kib_pool_t *pool)\r\n{\r\nkib_tx_pool_t *tpo = container_of(pool, kib_tx_pool_t, tpo_pool);\r\nint i;\r\nLASSERT (pool->po_allocated == 0);\r\nif (tpo->tpo_tx_pages != NULL) {\r\nkiblnd_unmap_tx_pool(tpo);\r\nkiblnd_free_pages(tpo->tpo_tx_pages);\r\n}\r\nif (tpo->tpo_tx_descs == NULL)\r\ngoto out;\r\nfor (i = 0; i < pool->po_size; i++) {\r\nkib_tx_t *tx = &tpo->tpo_tx_descs[i];\r\nlist_del(&tx->tx_list);\r\nif (tx->tx_pages != NULL)\r\nLIBCFS_FREE(tx->tx_pages,\r\nLNET_MAX_IOV *\r\nsizeof(*tx->tx_pages));\r\nif (tx->tx_frags != NULL)\r\nLIBCFS_FREE(tx->tx_frags,\r\nIBLND_MAX_RDMA_FRAGS *\r\nsizeof(*tx->tx_frags));\r\nif (tx->tx_wrq != NULL)\r\nLIBCFS_FREE(tx->tx_wrq,\r\n(1 + IBLND_MAX_RDMA_FRAGS) *\r\nsizeof(*tx->tx_wrq));\r\nif (tx->tx_sge != NULL)\r\nLIBCFS_FREE(tx->tx_sge,\r\n(1 + IBLND_MAX_RDMA_FRAGS) *\r\nsizeof(*tx->tx_sge));\r\nif (tx->tx_rd != NULL)\r\nLIBCFS_FREE(tx->tx_rd,\r\noffsetof(kib_rdma_desc_t,\r\nrd_frags[IBLND_MAX_RDMA_FRAGS]));\r\n}\r\nLIBCFS_FREE(tpo->tpo_tx_descs,\r\npool->po_size * sizeof(kib_tx_t));\r\nout:\r\nkiblnd_fini_pool(pool);\r\nLIBCFS_FREE(tpo, sizeof(kib_tx_pool_t));\r\n}\r\nstatic int kiblnd_tx_pool_size(int ncpts)\r\n{\r\nint ntx = *kiblnd_tunables.kib_ntx / ncpts;\r\nreturn max(IBLND_TX_POOL, ntx);\r\n}\r\nstatic int\r\nkiblnd_create_tx_pool(kib_poolset_t *ps, int size, kib_pool_t **pp_po)\r\n{\r\nint i;\r\nint npg;\r\nkib_pool_t *pool;\r\nkib_tx_pool_t *tpo;\r\nLIBCFS_CPT_ALLOC(tpo, lnet_cpt_table(), ps->ps_cpt, sizeof(*tpo));\r\nif (tpo == NULL) {\r\nCERROR("Failed to allocate TX pool\n");\r\nreturn -ENOMEM;\r\n}\r\npool = &tpo->tpo_pool;\r\nkiblnd_init_pool(ps, pool, size);\r\ntpo->tpo_tx_descs = NULL;\r\ntpo->tpo_tx_pages = NULL;\r\nnpg = (size * IBLND_MSG_SIZE + PAGE_SIZE - 1) / PAGE_SIZE;\r\nif (kiblnd_alloc_pages(&tpo->tpo_tx_pages, ps->ps_cpt, npg) != 0) {\r\nCERROR("Can't allocate tx pages: %d\n", npg);\r\nLIBCFS_FREE(tpo, sizeof(kib_tx_pool_t));\r\nreturn -ENOMEM;\r\n}\r\nLIBCFS_CPT_ALLOC(tpo->tpo_tx_descs, lnet_cpt_table(), ps->ps_cpt,\r\nsize * sizeof(kib_tx_t));\r\nif (tpo->tpo_tx_descs == NULL) {\r\nCERROR("Can't allocate %d tx descriptors\n", size);\r\nps->ps_pool_destroy(pool);\r\nreturn -ENOMEM;\r\n}\r\nmemset(tpo->tpo_tx_descs, 0, size * sizeof(kib_tx_t));\r\nfor (i = 0; i < size; i++) {\r\nkib_tx_t *tx = &tpo->tpo_tx_descs[i];\r\ntx->tx_pool = tpo;\r\nif (ps->ps_net->ibn_fmr_ps != NULL) {\r\nLIBCFS_CPT_ALLOC(tx->tx_pages,\r\nlnet_cpt_table(), ps->ps_cpt,\r\nLNET_MAX_IOV * sizeof(*tx->tx_pages));\r\nif (tx->tx_pages == NULL)\r\nbreak;\r\n}\r\nLIBCFS_CPT_ALLOC(tx->tx_frags, lnet_cpt_table(), ps->ps_cpt,\r\nIBLND_MAX_RDMA_FRAGS * sizeof(*tx->tx_frags));\r\nif (tx->tx_frags == NULL)\r\nbreak;\r\nsg_init_table(tx->tx_frags, IBLND_MAX_RDMA_FRAGS);\r\nLIBCFS_CPT_ALLOC(tx->tx_wrq, lnet_cpt_table(), ps->ps_cpt,\r\n(1 + IBLND_MAX_RDMA_FRAGS) *\r\nsizeof(*tx->tx_wrq));\r\nif (tx->tx_wrq == NULL)\r\nbreak;\r\nLIBCFS_CPT_ALLOC(tx->tx_sge, lnet_cpt_table(), ps->ps_cpt,\r\n(1 + IBLND_MAX_RDMA_FRAGS) *\r\nsizeof(*tx->tx_sge));\r\nif (tx->tx_sge == NULL)\r\nbreak;\r\nLIBCFS_CPT_ALLOC(tx->tx_rd, lnet_cpt_table(), ps->ps_cpt,\r\noffsetof(kib_rdma_desc_t,\r\nrd_frags[IBLND_MAX_RDMA_FRAGS]));\r\nif (tx->tx_rd == NULL)\r\nbreak;\r\n}\r\nif (i == size) {\r\nkiblnd_map_tx_pool(tpo);\r\n*pp_po = pool;\r\nreturn 0;\r\n}\r\nps->ps_pool_destroy(pool);\r\nreturn -ENOMEM;\r\n}\r\nstatic void\r\nkiblnd_tx_init(kib_pool_t *pool, struct list_head *node)\r\n{\r\nkib_tx_poolset_t *tps = container_of(pool->po_owner, kib_tx_poolset_t,\r\ntps_poolset);\r\nkib_tx_t *tx = list_entry(node, kib_tx_t, tx_list);\r\ntx->tx_cookie = tps->tps_next_tx_cookie ++;\r\n}\r\nvoid\r\nkiblnd_net_fini_pools(kib_net_t *net)\r\n{\r\nint i;\r\ncfs_cpt_for_each(i, lnet_cpt_table()) {\r\nkib_tx_poolset_t *tps;\r\nkib_fmr_poolset_t *fps;\r\nkib_pmr_poolset_t *pps;\r\nif (net->ibn_tx_ps != NULL) {\r\ntps = net->ibn_tx_ps[i];\r\nkiblnd_fini_poolset(&tps->tps_poolset);\r\n}\r\nif (net->ibn_fmr_ps != NULL) {\r\nfps = net->ibn_fmr_ps[i];\r\nkiblnd_fini_fmr_poolset(fps);\r\n}\r\nif (net->ibn_pmr_ps != NULL) {\r\npps = net->ibn_pmr_ps[i];\r\nkiblnd_fini_poolset(&pps->pps_poolset);\r\n}\r\n}\r\nif (net->ibn_tx_ps != NULL) {\r\ncfs_percpt_free(net->ibn_tx_ps);\r\nnet->ibn_tx_ps = NULL;\r\n}\r\nif (net->ibn_fmr_ps != NULL) {\r\ncfs_percpt_free(net->ibn_fmr_ps);\r\nnet->ibn_fmr_ps = NULL;\r\n}\r\nif (net->ibn_pmr_ps != NULL) {\r\ncfs_percpt_free(net->ibn_pmr_ps);\r\nnet->ibn_pmr_ps = NULL;\r\n}\r\n}\r\nint\r\nkiblnd_net_init_pools(kib_net_t *net, __u32 *cpts, int ncpts)\r\n{\r\nunsigned long flags;\r\nint cpt;\r\nint rc;\r\nint i;\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nif (*kiblnd_tunables.kib_map_on_demand == 0 &&\r\nnet->ibn_dev->ibd_hdev->ibh_nmrs == 1) {\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock,\r\nflags);\r\ngoto create_tx_pool;\r\n}\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nif (*kiblnd_tunables.kib_fmr_pool_size <\r\n*kiblnd_tunables.kib_ntx / 4) {\r\nCERROR("Can't set fmr pool size (%d) < ntx / 4(%d)\n",\r\n*kiblnd_tunables.kib_fmr_pool_size,\r\n*kiblnd_tunables.kib_ntx / 4);\r\nrc = -EINVAL;\r\ngoto failed;\r\n}\r\nLASSERT(net->ibn_tx_ps == NULL);\r\nnet->ibn_fmr_ps = cfs_percpt_alloc(lnet_cpt_table(),\r\nsizeof(kib_fmr_poolset_t));\r\nif (net->ibn_fmr_ps == NULL) {\r\nCERROR("Failed to allocate FMR pool array\n");\r\nrc = -ENOMEM;\r\ngoto failed;\r\n}\r\nfor (i = 0; i < ncpts; i++) {\r\ncpt = (cpts == NULL) ? i : cpts[i];\r\nrc = kiblnd_init_fmr_poolset(net->ibn_fmr_ps[cpt], cpt, net,\r\nkiblnd_fmr_pool_size(ncpts),\r\nkiblnd_fmr_flush_trigger(ncpts));\r\nif (rc == -ENOSYS && i == 0)\r\nbreak;\r\nif (rc != 0) {\r\nCERROR("Can't initialize FMR pool for CPT %d: %d\n",\r\ncpt, rc);\r\ngoto failed;\r\n}\r\n}\r\nif (i > 0) {\r\nLASSERT(i == ncpts);\r\ngoto create_tx_pool;\r\n}\r\ncfs_percpt_free(net->ibn_fmr_ps);\r\nnet->ibn_fmr_ps = NULL;\r\nCWARN("Device does not support FMR, failing back to PMR\n");\r\nif (*kiblnd_tunables.kib_pmr_pool_size <\r\n*kiblnd_tunables.kib_ntx / 4) {\r\nCERROR("Can't set pmr pool size (%d) < ntx / 4(%d)\n",\r\n*kiblnd_tunables.kib_pmr_pool_size,\r\n*kiblnd_tunables.kib_ntx / 4);\r\nrc = -EINVAL;\r\ngoto failed;\r\n}\r\nnet->ibn_pmr_ps = cfs_percpt_alloc(lnet_cpt_table(),\r\nsizeof(kib_pmr_poolset_t));\r\nif (net->ibn_pmr_ps == NULL) {\r\nCERROR("Failed to allocate PMR pool array\n");\r\nrc = -ENOMEM;\r\ngoto failed;\r\n}\r\nfor (i = 0; i < ncpts; i++) {\r\ncpt = (cpts == NULL) ? i : cpts[i];\r\nrc = kiblnd_init_poolset(&net->ibn_pmr_ps[cpt]->pps_poolset,\r\ncpt, net, "PMR",\r\nkiblnd_pmr_pool_size(ncpts),\r\nkiblnd_create_pmr_pool,\r\nkiblnd_destroy_pmr_pool, NULL, NULL);\r\nif (rc != 0) {\r\nCERROR("Can't initialize PMR pool for CPT %d: %d\n",\r\ncpt, rc);\r\ngoto failed;\r\n}\r\n}\r\ncreate_tx_pool:\r\nnet->ibn_tx_ps = cfs_percpt_alloc(lnet_cpt_table(),\r\nsizeof(kib_tx_poolset_t));\r\nif (net->ibn_tx_ps == NULL) {\r\nCERROR("Failed to allocate tx pool array\n");\r\nrc = -ENOMEM;\r\ngoto failed;\r\n}\r\nfor (i = 0; i < ncpts; i++) {\r\ncpt = (cpts == NULL) ? i : cpts[i];\r\nrc = kiblnd_init_poolset(&net->ibn_tx_ps[cpt]->tps_poolset,\r\ncpt, net, "TX",\r\nkiblnd_tx_pool_size(ncpts),\r\nkiblnd_create_tx_pool,\r\nkiblnd_destroy_tx_pool,\r\nkiblnd_tx_init, NULL);\r\nif (rc != 0) {\r\nCERROR("Can't initialize TX pool for CPT %d: %d\n",\r\ncpt, rc);\r\ngoto failed;\r\n}\r\n}\r\nreturn 0;\r\nfailed:\r\nkiblnd_net_fini_pools(net);\r\nLASSERT(rc != 0);\r\nreturn rc;\r\n}\r\nstatic int\r\nkiblnd_hdev_get_attr(kib_hca_dev_t *hdev)\r\n{\r\nstruct ib_device_attr *attr;\r\nint rc;\r\nhdev->ibh_page_shift = PAGE_SHIFT;\r\nhdev->ibh_page_size = 1 << PAGE_SHIFT;\r\nhdev->ibh_page_mask = ~((__u64)hdev->ibh_page_size - 1);\r\nLIBCFS_ALLOC(attr, sizeof(*attr));\r\nif (attr == NULL) {\r\nCERROR("Out of memory\n");\r\nreturn -ENOMEM;\r\n}\r\nrc = ib_query_device(hdev->ibh_ibdev, attr);\r\nif (rc == 0)\r\nhdev->ibh_mr_size = attr->max_mr_size;\r\nLIBCFS_FREE(attr, sizeof(*attr));\r\nif (rc != 0) {\r\nCERROR("Failed to query IB device: %d\n", rc);\r\nreturn rc;\r\n}\r\nif (hdev->ibh_mr_size == ~0ULL) {\r\nhdev->ibh_mr_shift = 64;\r\nreturn 0;\r\n}\r\nfor (hdev->ibh_mr_shift = 0;\r\nhdev->ibh_mr_shift < 64; hdev->ibh_mr_shift ++) {\r\nif (hdev->ibh_mr_size == (1ULL << hdev->ibh_mr_shift) ||\r\nhdev->ibh_mr_size == (1ULL << hdev->ibh_mr_shift) - 1)\r\nreturn 0;\r\n}\r\nCERROR("Invalid mr size: "LPX64"\n", hdev->ibh_mr_size);\r\nreturn -EINVAL;\r\n}\r\nvoid\r\nkiblnd_hdev_cleanup_mrs(kib_hca_dev_t *hdev)\r\n{\r\nint i;\r\nif (hdev->ibh_nmrs == 0 || hdev->ibh_mrs == NULL)\r\nreturn;\r\nfor (i = 0; i < hdev->ibh_nmrs; i++) {\r\nif (hdev->ibh_mrs[i] == NULL)\r\nbreak;\r\nib_dereg_mr(hdev->ibh_mrs[i]);\r\n}\r\nLIBCFS_FREE(hdev->ibh_mrs, sizeof(*hdev->ibh_mrs) * hdev->ibh_nmrs);\r\nhdev->ibh_mrs = NULL;\r\nhdev->ibh_nmrs = 0;\r\n}\r\nvoid\r\nkiblnd_hdev_destroy(kib_hca_dev_t *hdev)\r\n{\r\nkiblnd_hdev_cleanup_mrs(hdev);\r\nif (hdev->ibh_pd != NULL)\r\nib_dealloc_pd(hdev->ibh_pd);\r\nif (hdev->ibh_cmid != NULL)\r\nrdma_destroy_id(hdev->ibh_cmid);\r\nLIBCFS_FREE(hdev, sizeof(*hdev));\r\n}\r\nint\r\nkiblnd_hdev_setup_mrs(kib_hca_dev_t *hdev)\r\n{\r\nstruct ib_mr *mr;\r\nint i;\r\nint rc;\r\n__u64 mm_size;\r\n__u64 mr_size;\r\nint acflags = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE;\r\nrc = kiblnd_hdev_get_attr(hdev);\r\nif (rc != 0)\r\nreturn rc;\r\nif (hdev->ibh_mr_shift == 64) {\r\nLIBCFS_ALLOC(hdev->ibh_mrs, 1 * sizeof(*hdev->ibh_mrs));\r\nif (hdev->ibh_mrs == NULL) {\r\nCERROR("Failed to allocate MRs table\n");\r\nreturn -ENOMEM;\r\n}\r\nhdev->ibh_mrs[0] = NULL;\r\nhdev->ibh_nmrs = 1;\r\nmr = ib_get_dma_mr(hdev->ibh_pd, acflags);\r\nif (IS_ERR(mr)) {\r\nCERROR("Failed ib_get_dma_mr : %ld\n", PTR_ERR(mr));\r\nkiblnd_hdev_cleanup_mrs(hdev);\r\nreturn PTR_ERR(mr);\r\n}\r\nhdev->ibh_mrs[0] = mr;\r\ngoto out;\r\n}\r\nmr_size = (1ULL << hdev->ibh_mr_shift);\r\nmm_size = (unsigned long)high_memory - PAGE_OFFSET;\r\nhdev->ibh_nmrs = (int)((mm_size + mr_size - 1) >> hdev->ibh_mr_shift);\r\nif (hdev->ibh_mr_shift < 32 || hdev->ibh_nmrs > 1024) {\r\nCERROR("Can't support memory size: x"LPX64\r\n" with MR size: x"LPX64"\n", mm_size, mr_size);\r\nreturn -EINVAL;\r\n}\r\nLIBCFS_ALLOC(hdev->ibh_mrs, sizeof(*hdev->ibh_mrs) * hdev->ibh_nmrs);\r\nif (hdev->ibh_mrs == NULL) {\r\nCERROR("Failed to allocate MRs' table\n");\r\nreturn -ENOMEM;\r\n}\r\nmemset(hdev->ibh_mrs, 0, sizeof(*hdev->ibh_mrs) * hdev->ibh_nmrs);\r\nfor (i = 0; i < hdev->ibh_nmrs; i++) {\r\nstruct ib_phys_buf ipb;\r\n__u64 iova;\r\nipb.size = hdev->ibh_mr_size;\r\nipb.addr = i * mr_size;\r\niova = ipb.addr;\r\nmr = ib_reg_phys_mr(hdev->ibh_pd, &ipb, 1, acflags, &iova);\r\nif (IS_ERR(mr)) {\r\nCERROR("Failed ib_reg_phys_mr addr "LPX64\r\n" size "LPX64" : %ld\n",\r\nipb.addr, ipb.size, PTR_ERR(mr));\r\nkiblnd_hdev_cleanup_mrs(hdev);\r\nreturn PTR_ERR(mr);\r\n}\r\nLASSERT (iova == ipb.addr);\r\nhdev->ibh_mrs[i] = mr;\r\n}\r\nout:\r\nif (hdev->ibh_mr_size != ~0ULL || hdev->ibh_nmrs != 1)\r\nLCONSOLE_INFO("Register global MR array, MR size: "\r\nLPX64", array size: %d\n",\r\nhdev->ibh_mr_size, hdev->ibh_nmrs);\r\nreturn 0;\r\n}\r\nstatic int\r\nkiblnd_dummy_callback(struct rdma_cm_id *cmid, struct rdma_cm_event *event)\r\n{\r\nreturn 0;\r\n}\r\nstatic int\r\nkiblnd_dev_need_failover(kib_dev_t *dev)\r\n{\r\nstruct rdma_cm_id *cmid;\r\nstruct sockaddr_in srcaddr;\r\nstruct sockaddr_in dstaddr;\r\nint rc;\r\nif (dev->ibd_hdev == NULL ||\r\ndev->ibd_hdev->ibh_cmid == NULL ||\r\n*kiblnd_tunables.kib_dev_failover > 1)\r\nreturn 1;\r\ncmid = kiblnd_rdma_create_id(kiblnd_dummy_callback, dev, RDMA_PS_TCP,\r\nIB_QPT_RC);\r\nif (IS_ERR(cmid)) {\r\nrc = PTR_ERR(cmid);\r\nCERROR("Failed to create cmid for failover: %d\n", rc);\r\nreturn rc;\r\n}\r\nmemset(&srcaddr, 0, sizeof(srcaddr));\r\nsrcaddr.sin_family = AF_INET;\r\nsrcaddr.sin_addr.s_addr = (__force u32)htonl(dev->ibd_ifip);\r\nmemset(&dstaddr, 0, sizeof(dstaddr));\r\ndstaddr.sin_family = AF_INET;\r\nrc = rdma_resolve_addr(cmid, (struct sockaddr *)&srcaddr,\r\n(struct sockaddr *)&dstaddr, 1);\r\nif (rc != 0 || cmid->device == NULL) {\r\nCERROR("Failed to bind %s:%pI4h to device(%p): %d\n",\r\ndev->ibd_ifname, &dev->ibd_ifip,\r\ncmid->device, rc);\r\nrdma_destroy_id(cmid);\r\nreturn rc;\r\n}\r\nif (dev->ibd_hdev->ibh_ibdev == cmid->device) {\r\nrdma_destroy_id(cmid);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nint\r\nkiblnd_dev_failover(kib_dev_t *dev)\r\n{\r\nLIST_HEAD (zombie_tpo);\r\nLIST_HEAD (zombie_ppo);\r\nLIST_HEAD (zombie_fpo);\r\nstruct rdma_cm_id *cmid = NULL;\r\nkib_hca_dev_t *hdev = NULL;\r\nkib_hca_dev_t *old;\r\nstruct ib_pd *pd;\r\nkib_net_t *net;\r\nstruct sockaddr_in addr;\r\nunsigned long flags;\r\nint rc = 0;\r\nint i;\r\nLASSERT (*kiblnd_tunables.kib_dev_failover > 1 ||\r\ndev->ibd_can_failover ||\r\ndev->ibd_hdev == NULL);\r\nrc = kiblnd_dev_need_failover(dev);\r\nif (rc <= 0)\r\ngoto out;\r\nif (dev->ibd_hdev != NULL &&\r\ndev->ibd_hdev->ibh_cmid != NULL) {\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\ncmid = dev->ibd_hdev->ibh_cmid;\r\ndev->ibd_hdev->ibh_cmid = NULL;\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nrdma_destroy_id(cmid);\r\n}\r\ncmid = kiblnd_rdma_create_id(kiblnd_cm_callback, dev, RDMA_PS_TCP,\r\nIB_QPT_RC);\r\nif (IS_ERR(cmid)) {\r\nrc = PTR_ERR(cmid);\r\nCERROR("Failed to create cmid for failover: %d\n", rc);\r\ngoto out;\r\n}\r\nmemset(&addr, 0, sizeof(addr));\r\naddr.sin_family = AF_INET;\r\naddr.sin_addr.s_addr = (__force u32)htonl(dev->ibd_ifip);\r\naddr.sin_port = htons(*kiblnd_tunables.kib_service);\r\nrc = rdma_bind_addr(cmid, (struct sockaddr *)&addr);\r\nif (rc != 0 || cmid->device == NULL) {\r\nCERROR("Failed to bind %s:%pI4h to device(%p): %d\n",\r\ndev->ibd_ifname, &dev->ibd_ifip,\r\ncmid->device, rc);\r\nrdma_destroy_id(cmid);\r\ngoto out;\r\n}\r\nLIBCFS_ALLOC(hdev, sizeof(*hdev));\r\nif (hdev == NULL) {\r\nCERROR("Failed to allocate kib_hca_dev\n");\r\nrdma_destroy_id(cmid);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\natomic_set(&hdev->ibh_ref, 1);\r\nhdev->ibh_dev = dev;\r\nhdev->ibh_cmid = cmid;\r\nhdev->ibh_ibdev = cmid->device;\r\npd = ib_alloc_pd(cmid->device);\r\nif (IS_ERR(pd)) {\r\nrc = PTR_ERR(pd);\r\nCERROR("Can't allocate PD: %d\n", rc);\r\ngoto out;\r\n}\r\nhdev->ibh_pd = pd;\r\nrc = rdma_listen(cmid, 0);\r\nif (rc != 0) {\r\nCERROR("Can't start new listener: %d\n", rc);\r\ngoto out;\r\n}\r\nrc = kiblnd_hdev_setup_mrs(hdev);\r\nif (rc != 0) {\r\nCERROR("Can't setup device: %d\n", rc);\r\ngoto out;\r\n}\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nold = dev->ibd_hdev;\r\ndev->ibd_hdev = hdev;\r\nhdev = old;\r\nlist_for_each_entry(net, &dev->ibd_nets, ibn_list) {\r\ncfs_cpt_for_each(i, lnet_cpt_table()) {\r\nkiblnd_fail_poolset(&net->ibn_tx_ps[i]->tps_poolset,\r\n&zombie_tpo);\r\nif (net->ibn_fmr_ps != NULL) {\r\nkiblnd_fail_fmr_poolset(net->ibn_fmr_ps[i],\r\n&zombie_fpo);\r\n} else if (net->ibn_pmr_ps != NULL) {\r\nkiblnd_fail_poolset(&net->ibn_pmr_ps[i]->\r\npps_poolset, &zombie_ppo);\r\n}\r\n}\r\n}\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nout:\r\nif (!list_empty(&zombie_tpo))\r\nkiblnd_destroy_pool_list(&zombie_tpo);\r\nif (!list_empty(&zombie_ppo))\r\nkiblnd_destroy_pool_list(&zombie_ppo);\r\nif (!list_empty(&zombie_fpo))\r\nkiblnd_destroy_fmr_pool_list(&zombie_fpo);\r\nif (hdev != NULL)\r\nkiblnd_hdev_decref(hdev);\r\nif (rc != 0)\r\ndev->ibd_failed_failover++;\r\nelse\r\ndev->ibd_failed_failover = 0;\r\nreturn rc;\r\n}\r\nvoid\r\nkiblnd_destroy_dev (kib_dev_t *dev)\r\n{\r\nLASSERT (dev->ibd_nnets == 0);\r\nLASSERT (list_empty(&dev->ibd_nets));\r\nlist_del(&dev->ibd_fail_list);\r\nlist_del(&dev->ibd_list);\r\nif (dev->ibd_hdev != NULL)\r\nkiblnd_hdev_decref(dev->ibd_hdev);\r\nLIBCFS_FREE(dev, sizeof(*dev));\r\n}\r\nkib_dev_t *\r\nkiblnd_create_dev(char *ifname)\r\n{\r\nstruct net_device *netdev;\r\nkib_dev_t *dev;\r\n__u32 netmask;\r\n__u32 ip;\r\nint up;\r\nint rc;\r\nrc = libcfs_ipif_query(ifname, &up, &ip, &netmask);\r\nif (rc != 0) {\r\nCERROR("Can't query IPoIB interface %s: %d\n",\r\nifname, rc);\r\nreturn NULL;\r\n}\r\nif (!up) {\r\nCERROR("Can't query IPoIB interface %s: it's down\n", ifname);\r\nreturn NULL;\r\n}\r\nLIBCFS_ALLOC(dev, sizeof(*dev));\r\nif (dev == NULL)\r\nreturn NULL;\r\nmemset(dev, 0, sizeof(*dev));\r\nnetdev = dev_get_by_name(&init_net, ifname);\r\nif (netdev == NULL) {\r\ndev->ibd_can_failover = 0;\r\n} else {\r\ndev->ibd_can_failover = !!(netdev->flags & IFF_MASTER);\r\ndev_put(netdev);\r\n}\r\nINIT_LIST_HEAD(&dev->ibd_nets);\r\nINIT_LIST_HEAD(&dev->ibd_list);\r\nINIT_LIST_HEAD(&dev->ibd_fail_list);\r\ndev->ibd_ifip = ip;\r\nstrcpy(&dev->ibd_ifname[0], ifname);\r\nrc = kiblnd_dev_failover(dev);\r\nif (rc != 0) {\r\nCERROR("Can't initialize device: %d\n", rc);\r\nLIBCFS_FREE(dev, sizeof(*dev));\r\nreturn NULL;\r\n}\r\nlist_add_tail(&dev->ibd_list,\r\n&kiblnd_data.kib_devs);\r\nreturn dev;\r\n}\r\nvoid\r\nkiblnd_base_shutdown(void)\r\n{\r\nstruct kib_sched_info *sched;\r\nint i;\r\nLASSERT (list_empty(&kiblnd_data.kib_devs));\r\nCDEBUG(D_MALLOC, "before LND base cleanup: kmem %d\n",\r\natomic_read(&libcfs_kmemory));\r\nswitch (kiblnd_data.kib_init) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_INIT_ALL:\r\ncase IBLND_INIT_DATA:\r\nLASSERT (kiblnd_data.kib_peers != NULL);\r\nfor (i = 0; i < kiblnd_data.kib_peer_hash_size; i++) {\r\nLASSERT (list_empty(&kiblnd_data.kib_peers[i]));\r\n}\r\nLASSERT (list_empty(&kiblnd_data.kib_connd_zombies));\r\nLASSERT (list_empty(&kiblnd_data.kib_connd_conns));\r\nkiblnd_data.kib_shutdown = 1;\r\ncfs_percpt_for_each(sched, i, kiblnd_data.kib_scheds)\r\nwake_up_all(&sched->ibs_waitq);\r\nwake_up_all(&kiblnd_data.kib_connd_waitq);\r\nwake_up_all(&kiblnd_data.kib_failover_waitq);\r\ni = 2;\r\nwhile (atomic_read(&kiblnd_data.kib_nthreads) != 0) {\r\ni++;\r\nCDEBUG(((i & (-i)) == i) ? D_WARNING : D_NET,\r\n"Waiting for %d threads to terminate\n",\r\natomic_read(&kiblnd_data.kib_nthreads));\r\ncfs_pause(cfs_time_seconds(1));\r\n}\r\ncase IBLND_INIT_NOTHING:\r\nbreak;\r\n}\r\nif (kiblnd_data.kib_peers != NULL) {\r\nLIBCFS_FREE(kiblnd_data.kib_peers,\r\nsizeof(struct list_head) *\r\nkiblnd_data.kib_peer_hash_size);\r\n}\r\nif (kiblnd_data.kib_scheds != NULL)\r\ncfs_percpt_free(kiblnd_data.kib_scheds);\r\nCDEBUG(D_MALLOC, "after LND base cleanup: kmem %d\n",\r\natomic_read(&libcfs_kmemory));\r\nkiblnd_data.kib_init = IBLND_INIT_NOTHING;\r\nmodule_put(THIS_MODULE);\r\n}\r\nvoid\r\nkiblnd_shutdown (lnet_ni_t *ni)\r\n{\r\nkib_net_t *net = ni->ni_data;\r\nrwlock_t *g_lock = &kiblnd_data.kib_global_lock;\r\nint i;\r\nunsigned long flags;\r\nLASSERT(kiblnd_data.kib_init == IBLND_INIT_ALL);\r\nif (net == NULL)\r\ngoto out;\r\nCDEBUG(D_MALLOC, "before LND net cleanup: kmem %d\n",\r\natomic_read(&libcfs_kmemory));\r\nwrite_lock_irqsave(g_lock, flags);\r\nnet->ibn_shutdown = 1;\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nswitch (net->ibn_init) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_INIT_ALL:\r\nkiblnd_del_peer(ni, LNET_NID_ANY);\r\ni = 2;\r\nwhile (atomic_read(&net->ibn_npeers) != 0) {\r\ni++;\r\nCDEBUG(((i & (-i)) == i) ? D_WARNING : D_NET,\r\n"%s: waiting for %d peers to disconnect\n",\r\nlibcfs_nid2str(ni->ni_nid),\r\natomic_read(&net->ibn_npeers));\r\ncfs_pause(cfs_time_seconds(1));\r\n}\r\nkiblnd_net_fini_pools(net);\r\nwrite_lock_irqsave(g_lock, flags);\r\nLASSERT(net->ibn_dev->ibd_nnets > 0);\r\nnet->ibn_dev->ibd_nnets--;\r\nlist_del(&net->ibn_list);\r\nwrite_unlock_irqrestore(g_lock, flags);\r\ncase IBLND_INIT_NOTHING:\r\nLASSERT (atomic_read(&net->ibn_nconns) == 0);\r\nif (net->ibn_dev != NULL &&\r\nnet->ibn_dev->ibd_nnets == 0)\r\nkiblnd_destroy_dev(net->ibn_dev);\r\nbreak;\r\n}\r\nCDEBUG(D_MALLOC, "after LND net cleanup: kmem %d\n",\r\natomic_read(&libcfs_kmemory));\r\nnet->ibn_init = IBLND_INIT_NOTHING;\r\nni->ni_data = NULL;\r\nLIBCFS_FREE(net, sizeof(*net));\r\nout:\r\nif (list_empty(&kiblnd_data.kib_devs))\r\nkiblnd_base_shutdown();\r\nreturn;\r\n}\r\nint\r\nkiblnd_base_startup(void)\r\n{\r\nstruct kib_sched_info *sched;\r\nint rc;\r\nint i;\r\nLASSERT (kiblnd_data.kib_init == IBLND_INIT_NOTHING);\r\ntry_module_get(THIS_MODULE);\r\nmemset(&kiblnd_data, 0, sizeof(kiblnd_data));\r\nrwlock_init(&kiblnd_data.kib_global_lock);\r\nINIT_LIST_HEAD(&kiblnd_data.kib_devs);\r\nINIT_LIST_HEAD(&kiblnd_data.kib_failed_devs);\r\nkiblnd_data.kib_peer_hash_size = IBLND_PEER_HASH_SIZE;\r\nLIBCFS_ALLOC(kiblnd_data.kib_peers,\r\nsizeof(struct list_head) *\r\nkiblnd_data.kib_peer_hash_size);\r\nif (kiblnd_data.kib_peers == NULL) {\r\ngoto failed;\r\n}\r\nfor (i = 0; i < kiblnd_data.kib_peer_hash_size; i++)\r\nINIT_LIST_HEAD(&kiblnd_data.kib_peers[i]);\r\nspin_lock_init(&kiblnd_data.kib_connd_lock);\r\nINIT_LIST_HEAD(&kiblnd_data.kib_connd_conns);\r\nINIT_LIST_HEAD(&kiblnd_data.kib_connd_zombies);\r\ninit_waitqueue_head(&kiblnd_data.kib_connd_waitq);\r\ninit_waitqueue_head(&kiblnd_data.kib_failover_waitq);\r\nkiblnd_data.kib_scheds = cfs_percpt_alloc(lnet_cpt_table(),\r\nsizeof(*sched));\r\nif (kiblnd_data.kib_scheds == NULL)\r\ngoto failed;\r\ncfs_percpt_for_each(sched, i, kiblnd_data.kib_scheds) {\r\nint nthrs;\r\nspin_lock_init(&sched->ibs_lock);\r\nINIT_LIST_HEAD(&sched->ibs_conns);\r\ninit_waitqueue_head(&sched->ibs_waitq);\r\nnthrs = cfs_cpt_weight(lnet_cpt_table(), i);\r\nif (*kiblnd_tunables.kib_nscheds > 0) {\r\nnthrs = min(nthrs, *kiblnd_tunables.kib_nscheds);\r\n} else {\r\nnthrs = min(max(IBLND_N_SCHED, nthrs >> 1), nthrs);\r\n}\r\nsched->ibs_nthreads_max = nthrs;\r\nsched->ibs_cpt = i;\r\n}\r\nkiblnd_data.kib_error_qpa.qp_state = IB_QPS_ERR;\r\nkiblnd_data.kib_init = IBLND_INIT_DATA;\r\nrc = kiblnd_thread_start(kiblnd_connd, NULL, "kiblnd_connd");\r\nif (rc != 0) {\r\nCERROR("Can't spawn o2iblnd connd: %d\n", rc);\r\ngoto failed;\r\n}\r\nif (*kiblnd_tunables.kib_dev_failover != 0)\r\nrc = kiblnd_thread_start(kiblnd_failover_thread, NULL,\r\n"kiblnd_failover");\r\nif (rc != 0) {\r\nCERROR("Can't spawn o2iblnd failover thread: %d\n", rc);\r\ngoto failed;\r\n}\r\nkiblnd_data.kib_init = IBLND_INIT_ALL;\r\nreturn 0;\r\nfailed:\r\nkiblnd_base_shutdown();\r\nreturn -ENETDOWN;\r\n}\r\nint\r\nkiblnd_start_schedulers(struct kib_sched_info *sched)\r\n{\r\nint rc = 0;\r\nint nthrs;\r\nint i;\r\nif (sched->ibs_nthreads == 0) {\r\nif (*kiblnd_tunables.kib_nscheds > 0) {\r\nnthrs = sched->ibs_nthreads_max;\r\n} else {\r\nnthrs = cfs_cpt_weight(lnet_cpt_table(),\r\nsched->ibs_cpt);\r\nnthrs = min(max(IBLND_N_SCHED, nthrs >> 1), nthrs);\r\nnthrs = min(IBLND_N_SCHED_HIGH, nthrs);\r\n}\r\n} else {\r\nLASSERT(sched->ibs_nthreads <= sched->ibs_nthreads_max);\r\nnthrs = (sched->ibs_nthreads < sched->ibs_nthreads_max);\r\n}\r\nfor (i = 0; i < nthrs; i++) {\r\nlong id;\r\nchar name[20];\r\nid = KIB_THREAD_ID(sched->ibs_cpt, sched->ibs_nthreads + i);\r\nsnprintf(name, sizeof(name), "kiblnd_sd_%02ld_%02ld",\r\nKIB_THREAD_CPT(id), KIB_THREAD_TID(id));\r\nrc = kiblnd_thread_start(kiblnd_scheduler, (void *)id, name);\r\nif (rc == 0)\r\ncontinue;\r\nCERROR("Can't spawn thread %d for scheduler[%d]: %d\n",\r\nsched->ibs_cpt, sched->ibs_nthreads + i, rc);\r\nbreak;\r\n}\r\nsched->ibs_nthreads += i;\r\nreturn rc;\r\n}\r\nint\r\nkiblnd_dev_start_threads(kib_dev_t *dev, int newdev, __u32 *cpts, int ncpts)\r\n{\r\nint cpt;\r\nint rc;\r\nint i;\r\nfor (i = 0; i < ncpts; i++) {\r\nstruct kib_sched_info *sched;\r\ncpt = (cpts == NULL) ? i : cpts[i];\r\nsched = kiblnd_data.kib_scheds[cpt];\r\nif (!newdev && sched->ibs_nthreads > 0)\r\ncontinue;\r\nrc = kiblnd_start_schedulers(kiblnd_data.kib_scheds[cpt]);\r\nif (rc != 0) {\r\nCERROR("Failed to start scheduler threads for %s\n",\r\ndev->ibd_ifname);\r\nreturn rc;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nkib_dev_t *\r\nkiblnd_dev_search(char *ifname)\r\n{\r\nkib_dev_t *alias = NULL;\r\nkib_dev_t *dev;\r\nchar *colon;\r\nchar *colon2;\r\ncolon = strchr(ifname, ':');\r\nlist_for_each_entry(dev, &kiblnd_data.kib_devs, ibd_list) {\r\nif (strcmp(&dev->ibd_ifname[0], ifname) == 0)\r\nreturn dev;\r\nif (alias != NULL)\r\ncontinue;\r\ncolon2 = strchr(dev->ibd_ifname, ':');\r\nif (colon != NULL)\r\n*colon = 0;\r\nif (colon2 != NULL)\r\n*colon2 = 0;\r\nif (strcmp(&dev->ibd_ifname[0], ifname) == 0)\r\nalias = dev;\r\nif (colon != NULL)\r\n*colon = ':';\r\nif (colon2 != NULL)\r\n*colon2 = ':';\r\n}\r\nreturn alias;\r\n}\r\nint\r\nkiblnd_startup (lnet_ni_t *ni)\r\n{\r\nchar *ifname;\r\nkib_dev_t *ibdev = NULL;\r\nkib_net_t *net;\r\nstruct timeval tv;\r\nunsigned long flags;\r\nint rc;\r\nint newdev;\r\nLASSERT (ni->ni_lnd == &the_o2iblnd);\r\nif (kiblnd_data.kib_init == IBLND_INIT_NOTHING) {\r\nrc = kiblnd_base_startup();\r\nif (rc != 0)\r\nreturn rc;\r\n}\r\nLIBCFS_ALLOC(net, sizeof(*net));\r\nni->ni_data = net;\r\nif (net == NULL)\r\ngoto failed;\r\nmemset(net, 0, sizeof(*net));\r\ndo_gettimeofday(&tv);\r\nnet->ibn_incarnation = (((__u64)tv.tv_sec) * 1000000) + tv.tv_usec;\r\nni->ni_peertimeout = *kiblnd_tunables.kib_peertimeout;\r\nni->ni_maxtxcredits = *kiblnd_tunables.kib_credits;\r\nni->ni_peertxcredits = *kiblnd_tunables.kib_peertxcredits;\r\nni->ni_peerrtrcredits = *kiblnd_tunables.kib_peerrtrcredits;\r\nif (ni->ni_interfaces[0] != NULL) {\r\nCLASSERT (LNET_MAX_INTERFACES > 1);\r\nif (ni->ni_interfaces[1] != NULL) {\r\nCERROR("Multiple interfaces not supported\n");\r\ngoto failed;\r\n}\r\nifname = ni->ni_interfaces[0];\r\n} else {\r\nifname = *kiblnd_tunables.kib_default_ipif;\r\n}\r\nif (strlen(ifname) >= sizeof(ibdev->ibd_ifname)) {\r\nCERROR("IPoIB interface name too long: %s\n", ifname);\r\ngoto failed;\r\n}\r\nibdev = kiblnd_dev_search(ifname);\r\nnewdev = ibdev == NULL;\r\nif (ibdev == NULL || strcmp(&ibdev->ibd_ifname[0], ifname) != 0)\r\nibdev = kiblnd_create_dev(ifname);\r\nif (ibdev == NULL)\r\ngoto failed;\r\nnet->ibn_dev = ibdev;\r\nni->ni_nid = LNET_MKNID(LNET_NIDNET(ni->ni_nid), ibdev->ibd_ifip);\r\nrc = kiblnd_dev_start_threads(ibdev, newdev,\r\nni->ni_cpts, ni->ni_ncpts);\r\nif (rc != 0)\r\ngoto failed;\r\nrc = kiblnd_net_init_pools(net, ni->ni_cpts, ni->ni_ncpts);\r\nif (rc != 0) {\r\nCERROR("Failed to initialize NI pools: %d\n", rc);\r\ngoto failed;\r\n}\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nibdev->ibd_nnets++;\r\nlist_add_tail(&net->ibn_list, &ibdev->ibd_nets);\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nnet->ibn_init = IBLND_INIT_ALL;\r\nreturn 0;\r\nfailed:\r\nif (net->ibn_dev == NULL && ibdev != NULL)\r\nkiblnd_destroy_dev(ibdev);\r\nkiblnd_shutdown(ni);\r\nCDEBUG(D_NET, "kiblnd_startup failed\n");\r\nreturn -ENETDOWN;\r\n}\r\nvoid __exit\r\nkiblnd_module_fini (void)\r\n{\r\nlnet_unregister_lnd(&the_o2iblnd);\r\nkiblnd_tunables_fini();\r\n}\r\nint __init\r\nkiblnd_module_init (void)\r\n{\r\nint rc;\r\nCLASSERT (sizeof(kib_msg_t) <= IBLND_MSG_SIZE);\r\nCLASSERT (offsetof(kib_msg_t, ibm_u.get.ibgm_rd.rd_frags[IBLND_MAX_RDMA_FRAGS])\r\n<= IBLND_MSG_SIZE);\r\nCLASSERT (offsetof(kib_msg_t, ibm_u.putack.ibpam_rd.rd_frags[IBLND_MAX_RDMA_FRAGS])\r\n<= IBLND_MSG_SIZE);\r\nrc = kiblnd_tunables_init();\r\nif (rc != 0)\r\nreturn rc;\r\nlnet_register_lnd(&the_o2iblnd);\r\nreturn 0;\r\n}
