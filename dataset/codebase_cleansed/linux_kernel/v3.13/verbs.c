static void\r\nrpcrdma_run_tasklet(unsigned long data)\r\n{\r\nstruct rpcrdma_rep *rep;\r\nvoid (*func)(struct rpcrdma_rep *);\r\nunsigned long flags;\r\ndata = data;\r\nspin_lock_irqsave(&rpcrdma_tk_lock_g, flags);\r\nwhile (!list_empty(&rpcrdma_tasklets_g)) {\r\nrep = list_entry(rpcrdma_tasklets_g.next,\r\nstruct rpcrdma_rep, rr_list);\r\nlist_del(&rep->rr_list);\r\nfunc = rep->rr_func;\r\nrep->rr_func = NULL;\r\nspin_unlock_irqrestore(&rpcrdma_tk_lock_g, flags);\r\nif (func)\r\nfunc(rep);\r\nelse\r\nrpcrdma_recv_buffer_put(rep);\r\nspin_lock_irqsave(&rpcrdma_tk_lock_g, flags);\r\n}\r\nspin_unlock_irqrestore(&rpcrdma_tk_lock_g, flags);\r\n}\r\nstatic inline void\r\nrpcrdma_schedule_tasklet(struct rpcrdma_rep *rep)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&rpcrdma_tk_lock_g, flags);\r\nlist_add_tail(&rep->rr_list, &rpcrdma_tasklets_g);\r\nspin_unlock_irqrestore(&rpcrdma_tk_lock_g, flags);\r\ntasklet_schedule(&rpcrdma_tasklet_g);\r\n}\r\nstatic void\r\nrpcrdma_qp_async_error_upcall(struct ib_event *event, void *context)\r\n{\r\nstruct rpcrdma_ep *ep = context;\r\ndprintk("RPC: %s: QP error %X on device %s ep %p\n",\r\n__func__, event->event, event->device->name, context);\r\nif (ep->rep_connected == 1) {\r\nep->rep_connected = -EIO;\r\nep->rep_func(ep);\r\nwake_up_all(&ep->rep_connect_wait);\r\n}\r\n}\r\nstatic void\r\nrpcrdma_cq_async_error_upcall(struct ib_event *event, void *context)\r\n{\r\nstruct rpcrdma_ep *ep = context;\r\ndprintk("RPC: %s: CQ error %X on device %s ep %p\n",\r\n__func__, event->event, event->device->name, context);\r\nif (ep->rep_connected == 1) {\r\nep->rep_connected = -EIO;\r\nep->rep_func(ep);\r\nwake_up_all(&ep->rep_connect_wait);\r\n}\r\n}\r\nstatic inline\r\nvoid rpcrdma_event_process(struct ib_wc *wc)\r\n{\r\nstruct rpcrdma_mw *frmr;\r\nstruct rpcrdma_rep *rep =\r\n(struct rpcrdma_rep *)(unsigned long) wc->wr_id;\r\ndprintk("RPC: %s: event rep %p status %X opcode %X length %u\n",\r\n__func__, rep, wc->status, wc->opcode, wc->byte_len);\r\nif (!rep)\r\nreturn;\r\nif (IB_WC_SUCCESS != wc->status) {\r\ndprintk("RPC: %s: WC opcode %d status %X, connection lost\n",\r\n__func__, wc->opcode, wc->status);\r\nrep->rr_len = ~0U;\r\nif (wc->opcode != IB_WC_FAST_REG_MR && wc->opcode != IB_WC_LOCAL_INV)\r\nrpcrdma_schedule_tasklet(rep);\r\nreturn;\r\n}\r\nswitch (wc->opcode) {\r\ncase IB_WC_FAST_REG_MR:\r\nfrmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;\r\nfrmr->r.frmr.state = FRMR_IS_VALID;\r\nbreak;\r\ncase IB_WC_LOCAL_INV:\r\nfrmr = (struct rpcrdma_mw *)(unsigned long)wc->wr_id;\r\nfrmr->r.frmr.state = FRMR_IS_INVALID;\r\nbreak;\r\ncase IB_WC_RECV:\r\nrep->rr_len = wc->byte_len;\r\nib_dma_sync_single_for_cpu(\r\nrdmab_to_ia(rep->rr_buffer)->ri_id->device,\r\nrep->rr_iov.addr, rep->rr_len, DMA_FROM_DEVICE);\r\nif (rep->rr_len >= 16) {\r\nstruct rpcrdma_msg *p =\r\n(struct rpcrdma_msg *) rep->rr_base;\r\nunsigned int credits = ntohl(p->rm_credit);\r\nif (credits == 0) {\r\ndprintk("RPC: %s: server"\r\n" dropped credits to 0!\n", __func__);\r\ncredits = 1;\r\n} else if (credits > rep->rr_buffer->rb_max_requests) {\r\ndprintk("RPC: %s: server"\r\n" over-crediting: %d (%d)\n",\r\n__func__, credits,\r\nrep->rr_buffer->rb_max_requests);\r\ncredits = rep->rr_buffer->rb_max_requests;\r\n}\r\natomic_set(&rep->rr_buffer->rb_credits, credits);\r\n}\r\ncase IB_WC_BIND_MW:\r\nrpcrdma_schedule_tasklet(rep);\r\nbreak;\r\ndefault:\r\ndprintk("RPC: %s: unexpected WC event %X\n",\r\n__func__, wc->opcode);\r\nbreak;\r\n}\r\n}\r\nstatic inline int\r\nrpcrdma_cq_poll(struct ib_cq *cq)\r\n{\r\nstruct ib_wc wc;\r\nint rc;\r\nfor (;;) {\r\nrc = ib_poll_cq(cq, 1, &wc);\r\nif (rc < 0) {\r\ndprintk("RPC: %s: ib_poll_cq failed %i\n",\r\n__func__, rc);\r\nreturn rc;\r\n}\r\nif (rc == 0)\r\nbreak;\r\nrpcrdma_event_process(&wc);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nrpcrdma_cq_event_upcall(struct ib_cq *cq, void *context)\r\n{\r\nint rc;\r\nrc = rpcrdma_cq_poll(cq);\r\nif (rc)\r\nreturn;\r\nrc = ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_req_notify_cq failed %i\n",\r\n__func__, rc);\r\nreturn;\r\n}\r\nrpcrdma_cq_poll(cq);\r\n}\r\nstatic int\r\nrpcrdma_conn_upcall(struct rdma_cm_id *id, struct rdma_cm_event *event)\r\n{\r\nstruct rpcrdma_xprt *xprt = id->context;\r\nstruct rpcrdma_ia *ia = &xprt->rx_ia;\r\nstruct rpcrdma_ep *ep = &xprt->rx_ep;\r\n#ifdef RPC_DEBUG\r\nstruct sockaddr_in *addr = (struct sockaddr_in *) &ep->rep_remote_addr;\r\n#endif\r\nstruct ib_qp_attr attr;\r\nstruct ib_qp_init_attr iattr;\r\nint connstate = 0;\r\nswitch (event->event) {\r\ncase RDMA_CM_EVENT_ADDR_RESOLVED:\r\ncase RDMA_CM_EVENT_ROUTE_RESOLVED:\r\nia->ri_async_rc = 0;\r\ncomplete(&ia->ri_done);\r\nbreak;\r\ncase RDMA_CM_EVENT_ADDR_ERROR:\r\nia->ri_async_rc = -EHOSTUNREACH;\r\ndprintk("RPC: %s: CM address resolution error, ep 0x%p\n",\r\n__func__, ep);\r\ncomplete(&ia->ri_done);\r\nbreak;\r\ncase RDMA_CM_EVENT_ROUTE_ERROR:\r\nia->ri_async_rc = -ENETUNREACH;\r\ndprintk("RPC: %s: CM route resolution error, ep 0x%p\n",\r\n__func__, ep);\r\ncomplete(&ia->ri_done);\r\nbreak;\r\ncase RDMA_CM_EVENT_ESTABLISHED:\r\nconnstate = 1;\r\nib_query_qp(ia->ri_id->qp, &attr,\r\nIB_QP_MAX_QP_RD_ATOMIC | IB_QP_MAX_DEST_RD_ATOMIC,\r\n&iattr);\r\ndprintk("RPC: %s: %d responder resources"\r\n" (%d initiator)\n",\r\n__func__, attr.max_dest_rd_atomic, attr.max_rd_atomic);\r\ngoto connected;\r\ncase RDMA_CM_EVENT_CONNECT_ERROR:\r\nconnstate = -ENOTCONN;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_UNREACHABLE:\r\nconnstate = -ENETDOWN;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_REJECTED:\r\nconnstate = -ECONNREFUSED;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_DISCONNECTED:\r\nconnstate = -ECONNABORTED;\r\ngoto connected;\r\ncase RDMA_CM_EVENT_DEVICE_REMOVAL:\r\nconnstate = -ENODEV;\r\nconnected:\r\ndprintk("RPC: %s: %s: %pI4:%u (ep 0x%p event 0x%x)\n",\r\n__func__,\r\n(event->event <= 11) ? conn[event->event] :\r\n"unknown connection error",\r\n&addr->sin_addr.s_addr,\r\nntohs(addr->sin_port),\r\nep, event->event);\r\natomic_set(&rpcx_to_rdmax(ep->rep_xprt)->rx_buf.rb_credits, 1);\r\ndprintk("RPC: %s: %sconnected\n",\r\n__func__, connstate > 0 ? "" : "dis");\r\nep->rep_connected = connstate;\r\nep->rep_func(ep);\r\nwake_up_all(&ep->rep_connect_wait);\r\nbreak;\r\ndefault:\r\ndprintk("RPC: %s: unexpected CM event %d\n",\r\n__func__, event->event);\r\nbreak;\r\n}\r\n#ifdef RPC_DEBUG\r\nif (connstate == 1) {\r\nint ird = attr.max_dest_rd_atomic;\r\nint tird = ep->rep_remote_cma.responder_resources;\r\nprintk(KERN_INFO "rpcrdma: connection to %pI4:%u "\r\n"on %s, memreg %d slots %d ird %d%s\n",\r\n&addr->sin_addr.s_addr,\r\nntohs(addr->sin_port),\r\nia->ri_id->device->name,\r\nia->ri_memreg_strategy,\r\nxprt->rx_buf.rb_max_requests,\r\nird, ird < 4 && ird < tird / 2 ? " (low!)" : "");\r\n} else if (connstate < 0) {\r\nprintk(KERN_INFO "rpcrdma: connection to %pI4:%u closed (%d)\n",\r\n&addr->sin_addr.s_addr,\r\nntohs(addr->sin_port),\r\nconnstate);\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic struct rdma_cm_id *\r\nrpcrdma_create_id(struct rpcrdma_xprt *xprt,\r\nstruct rpcrdma_ia *ia, struct sockaddr *addr)\r\n{\r\nstruct rdma_cm_id *id;\r\nint rc;\r\ninit_completion(&ia->ri_done);\r\nid = rdma_create_id(rpcrdma_conn_upcall, xprt, RDMA_PS_TCP, IB_QPT_RC);\r\nif (IS_ERR(id)) {\r\nrc = PTR_ERR(id);\r\ndprintk("RPC: %s: rdma_create_id() failed %i\n",\r\n__func__, rc);\r\nreturn id;\r\n}\r\nia->ri_async_rc = -ETIMEDOUT;\r\nrc = rdma_resolve_addr(id, NULL, addr, RDMA_RESOLVE_TIMEOUT);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_resolve_addr() failed %i\n",\r\n__func__, rc);\r\ngoto out;\r\n}\r\nwait_for_completion_interruptible_timeout(&ia->ri_done,\r\nmsecs_to_jiffies(RDMA_RESOLVE_TIMEOUT) + 1);\r\nrc = ia->ri_async_rc;\r\nif (rc)\r\ngoto out;\r\nia->ri_async_rc = -ETIMEDOUT;\r\nrc = rdma_resolve_route(id, RDMA_RESOLVE_TIMEOUT);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_resolve_route() failed %i\n",\r\n__func__, rc);\r\ngoto out;\r\n}\r\nwait_for_completion_interruptible_timeout(&ia->ri_done,\r\nmsecs_to_jiffies(RDMA_RESOLVE_TIMEOUT) + 1);\r\nrc = ia->ri_async_rc;\r\nif (rc)\r\ngoto out;\r\nreturn id;\r\nout:\r\nrdma_destroy_id(id);\r\nreturn ERR_PTR(rc);\r\n}\r\nstatic void\r\nrpcrdma_clean_cq(struct ib_cq *cq)\r\n{\r\nstruct ib_wc wc;\r\nint count = 0;\r\nwhile (1 == ib_poll_cq(cq, 1, &wc))\r\n++count;\r\nif (count)\r\ndprintk("RPC: %s: flushed %d events (last 0x%x)\n",\r\n__func__, count, wc.opcode);\r\n}\r\nint\r\nrpcrdma_ia_open(struct rpcrdma_xprt *xprt, struct sockaddr *addr, int memreg)\r\n{\r\nint rc, mem_priv;\r\nstruct ib_device_attr devattr;\r\nstruct rpcrdma_ia *ia = &xprt->rx_ia;\r\nia->ri_id = rpcrdma_create_id(xprt, ia, addr);\r\nif (IS_ERR(ia->ri_id)) {\r\nrc = PTR_ERR(ia->ri_id);\r\ngoto out1;\r\n}\r\nia->ri_pd = ib_alloc_pd(ia->ri_id->device);\r\nif (IS_ERR(ia->ri_pd)) {\r\nrc = PTR_ERR(ia->ri_pd);\r\ndprintk("RPC: %s: ib_alloc_pd() failed %i\n",\r\n__func__, rc);\r\ngoto out2;\r\n}\r\nrc = ib_query_device(ia->ri_id->device, &devattr);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_query_device failed %d\n",\r\n__func__, rc);\r\ngoto out2;\r\n}\r\nif (devattr.device_cap_flags & IB_DEVICE_LOCAL_DMA_LKEY) {\r\nia->ri_have_dma_lkey = 1;\r\nia->ri_dma_lkey = ia->ri_id->device->local_dma_lkey;\r\n}\r\nswitch (memreg) {\r\ncase RPCRDMA_MEMWINDOWS:\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\nif (!(devattr.device_cap_flags & IB_DEVICE_MEM_WINDOW)) {\r\ndprintk("RPC: %s: MEMWINDOWS registration "\r\n"specified but not supported by adapter, "\r\n"using slower RPCRDMA_REGISTER\n",\r\n__func__);\r\nmemreg = RPCRDMA_REGISTER;\r\n}\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nif (!ia->ri_id->device->alloc_fmr) {\r\n#if RPCRDMA_PERSISTENT_REGISTRATION\r\ndprintk("RPC: %s: MTHCAFMR registration "\r\n"specified but not supported by adapter, "\r\n"using riskier RPCRDMA_ALLPHYSICAL\n",\r\n__func__);\r\nmemreg = RPCRDMA_ALLPHYSICAL;\r\n#else\r\ndprintk("RPC: %s: MTHCAFMR registration "\r\n"specified but not supported by adapter, "\r\n"using slower RPCRDMA_REGISTER\n",\r\n__func__);\r\nmemreg = RPCRDMA_REGISTER;\r\n#endif\r\n}\r\nbreak;\r\ncase RPCRDMA_FRMR:\r\nif ((devattr.device_cap_flags &\r\n(IB_DEVICE_MEM_MGT_EXTENSIONS|IB_DEVICE_LOCAL_DMA_LKEY)) !=\r\n(IB_DEVICE_MEM_MGT_EXTENSIONS|IB_DEVICE_LOCAL_DMA_LKEY)) {\r\n#if RPCRDMA_PERSISTENT_REGISTRATION\r\ndprintk("RPC: %s: FRMR registration "\r\n"specified but not supported by adapter, "\r\n"using riskier RPCRDMA_ALLPHYSICAL\n",\r\n__func__);\r\nmemreg = RPCRDMA_ALLPHYSICAL;\r\n#else\r\ndprintk("RPC: %s: FRMR registration "\r\n"specified but not supported by adapter, "\r\n"using slower RPCRDMA_REGISTER\n",\r\n__func__);\r\nmemreg = RPCRDMA_REGISTER;\r\n#endif\r\n}\r\nbreak;\r\n}\r\nswitch (memreg) {\r\ncase RPCRDMA_BOUNCEBUFFERS:\r\ncase RPCRDMA_REGISTER:\r\ncase RPCRDMA_FRMR:\r\nbreak;\r\n#if RPCRDMA_PERSISTENT_REGISTRATION\r\ncase RPCRDMA_ALLPHYSICAL:\r\nmem_priv = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_READ;\r\ngoto register_setup;\r\n#endif\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nmem_priv = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_MW_BIND;\r\ngoto register_setup;\r\ncase RPCRDMA_MTHCAFMR:\r\nif (ia->ri_have_dma_lkey)\r\nbreak;\r\nmem_priv = IB_ACCESS_LOCAL_WRITE;\r\nregister_setup:\r\nia->ri_bind_mem = ib_get_dma_mr(ia->ri_pd, mem_priv);\r\nif (IS_ERR(ia->ri_bind_mem)) {\r\nprintk(KERN_ALERT "%s: ib_get_dma_mr for "\r\n"phys register failed with %lX\n\t"\r\n"Will continue with degraded performance\n",\r\n__func__, PTR_ERR(ia->ri_bind_mem));\r\nmemreg = RPCRDMA_REGISTER;\r\nia->ri_bind_mem = NULL;\r\n}\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "%s: invalid memory registration mode %d\n",\r\n__func__, memreg);\r\nrc = -EINVAL;\r\ngoto out2;\r\n}\r\ndprintk("RPC: %s: memory registration strategy is %d\n",\r\n__func__, memreg);\r\nia->ri_memreg_strategy = memreg;\r\nreturn 0;\r\nout2:\r\nrdma_destroy_id(ia->ri_id);\r\nia->ri_id = NULL;\r\nout1:\r\nreturn rc;\r\n}\r\nvoid\r\nrpcrdma_ia_close(struct rpcrdma_ia *ia)\r\n{\r\nint rc;\r\ndprintk("RPC: %s: entering\n", __func__);\r\nif (ia->ri_bind_mem != NULL) {\r\nrc = ib_dereg_mr(ia->ri_bind_mem);\r\ndprintk("RPC: %s: ib_dereg_mr returned %i\n",\r\n__func__, rc);\r\n}\r\nif (ia->ri_id != NULL && !IS_ERR(ia->ri_id)) {\r\nif (ia->ri_id->qp)\r\nrdma_destroy_qp(ia->ri_id);\r\nrdma_destroy_id(ia->ri_id);\r\nia->ri_id = NULL;\r\n}\r\nif (ia->ri_pd != NULL && !IS_ERR(ia->ri_pd)) {\r\nrc = ib_dealloc_pd(ia->ri_pd);\r\ndprintk("RPC: %s: ib_dealloc_pd returned %i\n",\r\n__func__, rc);\r\n}\r\n}\r\nint\r\nrpcrdma_ep_create(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia,\r\nstruct rpcrdma_create_data_internal *cdata)\r\n{\r\nstruct ib_device_attr devattr;\r\nint rc, err;\r\nrc = ib_query_device(ia->ri_id->device, &devattr);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_query_device failed %d\n",\r\n__func__, rc);\r\nreturn rc;\r\n}\r\nif (cdata->max_requests > devattr.max_qp_wr)\r\ncdata->max_requests = devattr.max_qp_wr;\r\nep->rep_attr.event_handler = rpcrdma_qp_async_error_upcall;\r\nep->rep_attr.qp_context = ep;\r\nep->rep_attr.srq = NULL;\r\nep->rep_attr.cap.max_send_wr = cdata->max_requests;\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\nep->rep_attr.cap.max_send_wr *= 7;\r\nif (ep->rep_attr.cap.max_send_wr > devattr.max_qp_wr) {\r\ncdata->max_requests = devattr.max_qp_wr / 7;\r\nif (!cdata->max_requests)\r\nreturn -EINVAL;\r\nep->rep_attr.cap.max_send_wr = cdata->max_requests * 7;\r\n}\r\nbreak;\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nep->rep_attr.cap.max_send_wr++;\r\nep->rep_attr.cap.max_send_wr *= (2 * RPCRDMA_MAX_SEGS);\r\nif (ep->rep_attr.cap.max_send_wr > devattr.max_qp_wr)\r\nreturn -EINVAL;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nep->rep_attr.cap.max_recv_wr = cdata->max_requests;\r\nep->rep_attr.cap.max_send_sge = (cdata->padding ? 4 : 2);\r\nep->rep_attr.cap.max_recv_sge = 1;\r\nep->rep_attr.cap.max_inline_data = 0;\r\nep->rep_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\r\nep->rep_attr.qp_type = IB_QPT_RC;\r\nep->rep_attr.port_num = ~0;\r\ndprintk("RPC: %s: requested max: dtos: send %d recv %d; "\r\n"iovs: send %d recv %d\n",\r\n__func__,\r\nep->rep_attr.cap.max_send_wr,\r\nep->rep_attr.cap.max_recv_wr,\r\nep->rep_attr.cap.max_send_sge,\r\nep->rep_attr.cap.max_recv_sge);\r\nep->rep_cqinit = ep->rep_attr.cap.max_send_wr/2 ;\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nep->rep_cqinit -= RPCRDMA_MAX_SEGS;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (ep->rep_cqinit <= 2)\r\nep->rep_cqinit = 0;\r\nINIT_CQCOUNT(ep);\r\nep->rep_ia = ia;\r\ninit_waitqueue_head(&ep->rep_connect_wait);\r\nep->rep_cq = ib_create_cq(ia->ri_id->device, rpcrdma_cq_event_upcall,\r\nrpcrdma_cq_async_error_upcall, NULL,\r\nep->rep_attr.cap.max_recv_wr +\r\nep->rep_attr.cap.max_send_wr + 1, 0);\r\nif (IS_ERR(ep->rep_cq)) {\r\nrc = PTR_ERR(ep->rep_cq);\r\ndprintk("RPC: %s: ib_create_cq failed: %i\n",\r\n__func__, rc);\r\ngoto out1;\r\n}\r\nrc = ib_req_notify_cq(ep->rep_cq, IB_CQ_NEXT_COMP);\r\nif (rc) {\r\ndprintk("RPC: %s: ib_req_notify_cq failed: %i\n",\r\n__func__, rc);\r\ngoto out2;\r\n}\r\nep->rep_attr.send_cq = ep->rep_cq;\r\nep->rep_attr.recv_cq = ep->rep_cq;\r\nep->rep_remote_cma.private_data = NULL;\r\nep->rep_remote_cma.private_data_len = 0;\r\nep->rep_remote_cma.initiator_depth = 0;\r\nif (ia->ri_memreg_strategy == RPCRDMA_BOUNCEBUFFERS)\r\nep->rep_remote_cma.responder_resources = 0;\r\nelse if (devattr.max_qp_rd_atom > 32)\r\nep->rep_remote_cma.responder_resources = 32;\r\nelse\r\nep->rep_remote_cma.responder_resources = devattr.max_qp_rd_atom;\r\nep->rep_remote_cma.retry_count = 7;\r\nep->rep_remote_cma.flow_control = 0;\r\nep->rep_remote_cma.rnr_retry_count = 0;\r\nreturn 0;\r\nout2:\r\nerr = ib_destroy_cq(ep->rep_cq);\r\nif (err)\r\ndprintk("RPC: %s: ib_destroy_cq returned %i\n",\r\n__func__, err);\r\nout1:\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_ep_destroy(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia)\r\n{\r\nint rc;\r\ndprintk("RPC: %s: entering, connected is %d\n",\r\n__func__, ep->rep_connected);\r\nif (ia->ri_id->qp) {\r\nrc = rpcrdma_ep_disconnect(ep, ia);\r\nif (rc)\r\ndprintk("RPC: %s: rpcrdma_ep_disconnect"\r\n" returned %i\n", __func__, rc);\r\nrdma_destroy_qp(ia->ri_id);\r\nia->ri_id->qp = NULL;\r\n}\r\nif (ep->rep_pad_mr) {\r\nrpcrdma_deregister_internal(ia, ep->rep_pad_mr, &ep->rep_pad);\r\nep->rep_pad_mr = NULL;\r\n}\r\nrpcrdma_clean_cq(ep->rep_cq);\r\nrc = ib_destroy_cq(ep->rep_cq);\r\nif (rc)\r\ndprintk("RPC: %s: ib_destroy_cq returned %i\n",\r\n__func__, rc);\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_ep_connect(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia)\r\n{\r\nstruct rdma_cm_id *id;\r\nint rc = 0;\r\nint retry_count = 0;\r\nif (ep->rep_connected != 0) {\r\nstruct rpcrdma_xprt *xprt;\r\nretry:\r\nrc = rpcrdma_ep_disconnect(ep, ia);\r\nif (rc && rc != -ENOTCONN)\r\ndprintk("RPC: %s: rpcrdma_ep_disconnect"\r\n" status %i\n", __func__, rc);\r\nrpcrdma_clean_cq(ep->rep_cq);\r\nxprt = container_of(ia, struct rpcrdma_xprt, rx_ia);\r\nid = rpcrdma_create_id(xprt, ia,\r\n(struct sockaddr *)&xprt->rx_data.addr);\r\nif (IS_ERR(id)) {\r\nrc = PTR_ERR(id);\r\ngoto out;\r\n}\r\nif (ia->ri_id->device != id->device) {\r\nprintk("RPC: %s: can't reconnect on "\r\n"different device!\n", __func__);\r\nrdma_destroy_id(id);\r\nrc = -ENETDOWN;\r\ngoto out;\r\n}\r\nrdma_destroy_qp(ia->ri_id);\r\nrdma_destroy_id(ia->ri_id);\r\nia->ri_id = id;\r\n}\r\nrc = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_create_qp failed %i\n",\r\n__func__, rc);\r\ngoto out;\r\n}\r\nif (strnicmp(ia->ri_id->device->dma_device->bus->name, "pci", 3) == 0) {\r\nstruct pci_dev *pcid = to_pci_dev(ia->ri_id->device->dma_device);\r\nif (pcid->device == PCI_DEVICE_ID_MELLANOX_TAVOR &&\r\n(pcid->vendor == PCI_VENDOR_ID_MELLANOX ||\r\npcid->vendor == PCI_VENDOR_ID_TOPSPIN)) {\r\nstruct ib_qp_attr attr = {\r\n.path_mtu = IB_MTU_1024\r\n};\r\nrc = ib_modify_qp(ia->ri_id->qp, &attr, IB_QP_PATH_MTU);\r\n}\r\n}\r\nep->rep_connected = 0;\r\nrc = rdma_connect(ia->ri_id, &ep->rep_remote_cma);\r\nif (rc) {\r\ndprintk("RPC: %s: rdma_connect() failed with %i\n",\r\n__func__, rc);\r\ngoto out;\r\n}\r\nwait_event_interruptible(ep->rep_connect_wait, ep->rep_connected != 0);\r\nif (ep->rep_connected == -ECONNREFUSED &&\r\n++retry_count <= RDMA_CONNECT_RETRY_MAX) {\r\ndprintk("RPC: %s: non-peer_reject, retry\n", __func__);\r\ngoto retry;\r\n}\r\nif (ep->rep_connected <= 0) {\r\nif (retry_count++ <= RDMA_CONNECT_RETRY_MAX + 1 &&\r\n(ep->rep_remote_cma.responder_resources == 0 ||\r\nep->rep_remote_cma.initiator_depth !=\r\nep->rep_remote_cma.responder_resources)) {\r\nif (ep->rep_remote_cma.responder_resources == 0)\r\nep->rep_remote_cma.responder_resources = 1;\r\nep->rep_remote_cma.initiator_depth =\r\nep->rep_remote_cma.responder_resources;\r\ngoto retry;\r\n}\r\nrc = ep->rep_connected;\r\n} else {\r\ndprintk("RPC: %s: connected\n", __func__);\r\n}\r\nout:\r\nif (rc)\r\nep->rep_connected = rc;\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_ep_disconnect(struct rpcrdma_ep *ep, struct rpcrdma_ia *ia)\r\n{\r\nint rc;\r\nrpcrdma_clean_cq(ep->rep_cq);\r\nrc = rdma_disconnect(ia->ri_id);\r\nif (!rc) {\r\nwait_event_interruptible(ep->rep_connect_wait,\r\nep->rep_connected != 1);\r\ndprintk("RPC: %s: after wait, %sconnected\n", __func__,\r\n(ep->rep_connected == 1) ? "still " : "dis");\r\n} else {\r\ndprintk("RPC: %s: rdma_disconnect %i\n", __func__, rc);\r\nep->rep_connected = rc;\r\n}\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_buffer_create(struct rpcrdma_buffer *buf, struct rpcrdma_ep *ep,\r\nstruct rpcrdma_ia *ia, struct rpcrdma_create_data_internal *cdata)\r\n{\r\nchar *p;\r\nsize_t len;\r\nint i, rc;\r\nstruct rpcrdma_mw *r;\r\nbuf->rb_max_requests = cdata->max_requests;\r\nspin_lock_init(&buf->rb_lock);\r\natomic_set(&buf->rb_credits, 1);\r\nlen = buf->rb_max_requests *\r\n(sizeof(struct rpcrdma_req *) + sizeof(struct rpcrdma_rep *));\r\nlen += cdata->padding;\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\nlen += buf->rb_max_requests * RPCRDMA_MAX_SEGS *\r\nsizeof(struct rpcrdma_mw);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nlen += (buf->rb_max_requests + 1) * RPCRDMA_MAX_SEGS *\r\nsizeof(struct rpcrdma_mw);\r\nbreak;\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nlen += (buf->rb_max_requests + 1) * RPCRDMA_MAX_SEGS *\r\nsizeof(struct rpcrdma_mw);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\np = kzalloc(len, GFP_KERNEL);\r\nif (p == NULL) {\r\ndprintk("RPC: %s: req_t/rep_t/pad kzalloc(%zd) failed\n",\r\n__func__, len);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nbuf->rb_pool = p;\r\nbuf->rb_send_bufs = (struct rpcrdma_req **) p;\r\np = (char *) &buf->rb_send_bufs[buf->rb_max_requests];\r\nbuf->rb_recv_bufs = (struct rpcrdma_rep **) p;\r\np = (char *) &buf->rb_recv_bufs[buf->rb_max_requests];\r\nif (cdata->padding) {\r\nrc = rpcrdma_register_internal(ia, p, cdata->padding,\r\n&ep->rep_pad_mr, &ep->rep_pad);\r\nif (rc)\r\ngoto out;\r\n}\r\np += cdata->padding;\r\nINIT_LIST_HEAD(&buf->rb_mws);\r\nr = (struct rpcrdma_mw *)p;\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\nfor (i = buf->rb_max_requests * RPCRDMA_MAX_SEGS; i; i--) {\r\nr->r.frmr.fr_mr = ib_alloc_fast_reg_mr(ia->ri_pd,\r\nRPCRDMA_MAX_SEGS);\r\nif (IS_ERR(r->r.frmr.fr_mr)) {\r\nrc = PTR_ERR(r->r.frmr.fr_mr);\r\ndprintk("RPC: %s: ib_alloc_fast_reg_mr"\r\n" failed %i\n", __func__, rc);\r\ngoto out;\r\n}\r\nr->r.frmr.fr_pgl =\r\nib_alloc_fast_reg_page_list(ia->ri_id->device,\r\nRPCRDMA_MAX_SEGS);\r\nif (IS_ERR(r->r.frmr.fr_pgl)) {\r\nrc = PTR_ERR(r->r.frmr.fr_pgl);\r\ndprintk("RPC: %s: "\r\n"ib_alloc_fast_reg_page_list "\r\n"failed %i\n", __func__, rc);\r\ngoto out;\r\n}\r\nlist_add(&r->mw_list, &buf->rb_mws);\r\n++r;\r\n}\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nfor (i = (buf->rb_max_requests+1) * RPCRDMA_MAX_SEGS; i; i--) {\r\nstatic struct ib_fmr_attr fa =\r\n{ RPCRDMA_MAX_DATA_SEGS, 1, PAGE_SHIFT };\r\nr->r.fmr = ib_alloc_fmr(ia->ri_pd,\r\nIB_ACCESS_REMOTE_WRITE | IB_ACCESS_REMOTE_READ,\r\n&fa);\r\nif (IS_ERR(r->r.fmr)) {\r\nrc = PTR_ERR(r->r.fmr);\r\ndprintk("RPC: %s: ib_alloc_fmr"\r\n" failed %i\n", __func__, rc);\r\ngoto out;\r\n}\r\nlist_add(&r->mw_list, &buf->rb_mws);\r\n++r;\r\n}\r\nbreak;\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nfor (i = (buf->rb_max_requests+1) * RPCRDMA_MAX_SEGS; i; i--) {\r\nr->r.mw = ib_alloc_mw(ia->ri_pd, IB_MW_TYPE_1);\r\nif (IS_ERR(r->r.mw)) {\r\nrc = PTR_ERR(r->r.mw);\r\ndprintk("RPC: %s: ib_alloc_mw"\r\n" failed %i\n", __func__, rc);\r\ngoto out;\r\n}\r\nlist_add(&r->mw_list, &buf->rb_mws);\r\n++r;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nfor (i = 0; i < buf->rb_max_requests; i++) {\r\nstruct rpcrdma_req *req;\r\nstruct rpcrdma_rep *rep;\r\nlen = cdata->inline_wsize + sizeof(struct rpcrdma_req);\r\nif (len < 4096)\r\nlen = 4096;\r\nreq = kmalloc(len, GFP_KERNEL);\r\nif (req == NULL) {\r\ndprintk("RPC: %s: request buffer %d alloc"\r\n" failed\n", __func__, i);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nmemset(req, 0, sizeof(struct rpcrdma_req));\r\nbuf->rb_send_bufs[i] = req;\r\nbuf->rb_send_bufs[i]->rl_buffer = buf;\r\nrc = rpcrdma_register_internal(ia, req->rl_base,\r\nlen - offsetof(struct rpcrdma_req, rl_base),\r\n&buf->rb_send_bufs[i]->rl_handle,\r\n&buf->rb_send_bufs[i]->rl_iov);\r\nif (rc)\r\ngoto out;\r\nbuf->rb_send_bufs[i]->rl_size = len-sizeof(struct rpcrdma_req);\r\nlen = cdata->inline_rsize + sizeof(struct rpcrdma_rep);\r\nrep = kmalloc(len, GFP_KERNEL);\r\nif (rep == NULL) {\r\ndprintk("RPC: %s: reply buffer %d alloc failed\n",\r\n__func__, i);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nmemset(rep, 0, sizeof(struct rpcrdma_rep));\r\nbuf->rb_recv_bufs[i] = rep;\r\nbuf->rb_recv_bufs[i]->rr_buffer = buf;\r\ninit_waitqueue_head(&rep->rr_unbind);\r\nrc = rpcrdma_register_internal(ia, rep->rr_base,\r\nlen - offsetof(struct rpcrdma_rep, rr_base),\r\n&buf->rb_recv_bufs[i]->rr_handle,\r\n&buf->rb_recv_bufs[i]->rr_iov);\r\nif (rc)\r\ngoto out;\r\n}\r\ndprintk("RPC: %s: max_requests %d\n",\r\n__func__, buf->rb_max_requests);\r\nreturn 0;\r\nout:\r\nrpcrdma_buffer_destroy(buf);\r\nreturn rc;\r\n}\r\nvoid\r\nrpcrdma_buffer_destroy(struct rpcrdma_buffer *buf)\r\n{\r\nint rc, i;\r\nstruct rpcrdma_ia *ia = rdmab_to_ia(buf);\r\nstruct rpcrdma_mw *r;\r\ndprintk("RPC: %s: entering\n", __func__);\r\nfor (i = 0; i < buf->rb_max_requests; i++) {\r\nif (buf->rb_recv_bufs && buf->rb_recv_bufs[i]) {\r\nrpcrdma_deregister_internal(ia,\r\nbuf->rb_recv_bufs[i]->rr_handle,\r\n&buf->rb_recv_bufs[i]->rr_iov);\r\nkfree(buf->rb_recv_bufs[i]);\r\n}\r\nif (buf->rb_send_bufs && buf->rb_send_bufs[i]) {\r\nwhile (!list_empty(&buf->rb_mws)) {\r\nr = list_entry(buf->rb_mws.next,\r\nstruct rpcrdma_mw, mw_list);\r\nlist_del(&r->mw_list);\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\nrc = ib_dereg_mr(r->r.frmr.fr_mr);\r\nif (rc)\r\ndprintk("RPC: %s:"\r\n" ib_dereg_mr"\r\n" failed %i\n",\r\n__func__, rc);\r\nib_free_fast_reg_page_list(r->r.frmr.fr_pgl);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nrc = ib_dealloc_fmr(r->r.fmr);\r\nif (rc)\r\ndprintk("RPC: %s:"\r\n" ib_dealloc_fmr"\r\n" failed %i\n",\r\n__func__, rc);\r\nbreak;\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nrc = ib_dealloc_mw(r->r.mw);\r\nif (rc)\r\ndprintk("RPC: %s:"\r\n" ib_dealloc_mw"\r\n" failed %i\n",\r\n__func__, rc);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nrpcrdma_deregister_internal(ia,\r\nbuf->rb_send_bufs[i]->rl_handle,\r\n&buf->rb_send_bufs[i]->rl_iov);\r\nkfree(buf->rb_send_bufs[i]);\r\n}\r\n}\r\nkfree(buf->rb_pool);\r\n}\r\nstruct rpcrdma_req *\r\nrpcrdma_buffer_get(struct rpcrdma_buffer *buffers)\r\n{\r\nstruct rpcrdma_req *req;\r\nunsigned long flags;\r\nint i;\r\nstruct rpcrdma_mw *r;\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nif (buffers->rb_send_index == buffers->rb_max_requests) {\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\ndprintk("RPC: %s: out of request buffers\n", __func__);\r\nreturn ((struct rpcrdma_req *)NULL);\r\n}\r\nreq = buffers->rb_send_bufs[buffers->rb_send_index];\r\nif (buffers->rb_send_index < buffers->rb_recv_index) {\r\ndprintk("RPC: %s: %d extra receives outstanding (ok)\n",\r\n__func__,\r\nbuffers->rb_recv_index - buffers->rb_send_index);\r\nreq->rl_reply = NULL;\r\n} else {\r\nreq->rl_reply = buffers->rb_recv_bufs[buffers->rb_recv_index];\r\nbuffers->rb_recv_bufs[buffers->rb_recv_index++] = NULL;\r\n}\r\nbuffers->rb_send_bufs[buffers->rb_send_index++] = NULL;\r\nif (!list_empty(&buffers->rb_mws)) {\r\ni = RPCRDMA_MAX_SEGS - 1;\r\ndo {\r\nr = list_entry(buffers->rb_mws.next,\r\nstruct rpcrdma_mw, mw_list);\r\nlist_del(&r->mw_list);\r\nreq->rl_segments[i].mr_chunk.rl_mw = r;\r\n} while (--i >= 0);\r\n}\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\nreturn req;\r\n}\r\nvoid\r\nrpcrdma_buffer_put(struct rpcrdma_req *req)\r\n{\r\nstruct rpcrdma_buffer *buffers = req->rl_buffer;\r\nstruct rpcrdma_ia *ia = rdmab_to_ia(buffers);\r\nint i;\r\nunsigned long flags;\r\nBUG_ON(req->rl_nchunks != 0);\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nbuffers->rb_send_bufs[--buffers->rb_send_index] = req;\r\nreq->rl_niovs = 0;\r\nif (req->rl_reply) {\r\nbuffers->rb_recv_bufs[--buffers->rb_recv_index] = req->rl_reply;\r\ninit_waitqueue_head(&req->rl_reply->rr_unbind);\r\nreq->rl_reply->rr_func = NULL;\r\nreq->rl_reply = NULL;\r\n}\r\nswitch (ia->ri_memreg_strategy) {\r\ncase RPCRDMA_FRMR:\r\ncase RPCRDMA_MTHCAFMR:\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\ni = 1;\r\ndo {\r\nstruct rpcrdma_mw **mw;\r\nmw = &req->rl_segments[i].mr_chunk.rl_mw;\r\nlist_add_tail(&(*mw)->mw_list, &buffers->rb_mws);\r\n*mw = NULL;\r\n} while (++i < RPCRDMA_MAX_SEGS);\r\nlist_add_tail(&req->rl_segments[0].mr_chunk.rl_mw->mw_list,\r\n&buffers->rb_mws);\r\nreq->rl_segments[0].mr_chunk.rl_mw = NULL;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\n}\r\nvoid\r\nrpcrdma_recv_buffer_get(struct rpcrdma_req *req)\r\n{\r\nstruct rpcrdma_buffer *buffers = req->rl_buffer;\r\nunsigned long flags;\r\nif (req->rl_iov.length == 0)\r\nbuffers = ((struct rpcrdma_req *) buffers)->rl_buffer;\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nif (buffers->rb_recv_index < buffers->rb_max_requests) {\r\nreq->rl_reply = buffers->rb_recv_bufs[buffers->rb_recv_index];\r\nbuffers->rb_recv_bufs[buffers->rb_recv_index++] = NULL;\r\n}\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\n}\r\nvoid\r\nrpcrdma_recv_buffer_put(struct rpcrdma_rep *rep)\r\n{\r\nstruct rpcrdma_buffer *buffers = rep->rr_buffer;\r\nunsigned long flags;\r\nrep->rr_func = NULL;\r\nspin_lock_irqsave(&buffers->rb_lock, flags);\r\nbuffers->rb_recv_bufs[--buffers->rb_recv_index] = rep;\r\nspin_unlock_irqrestore(&buffers->rb_lock, flags);\r\n}\r\nint\r\nrpcrdma_register_internal(struct rpcrdma_ia *ia, void *va, int len,\r\nstruct ib_mr **mrp, struct ib_sge *iov)\r\n{\r\nstruct ib_phys_buf ipb;\r\nstruct ib_mr *mr;\r\nint rc;\r\niov->addr = ib_dma_map_single(ia->ri_id->device,\r\nva, len, DMA_BIDIRECTIONAL);\r\niov->length = len;\r\nif (ia->ri_have_dma_lkey) {\r\n*mrp = NULL;\r\niov->lkey = ia->ri_dma_lkey;\r\nreturn 0;\r\n} else if (ia->ri_bind_mem != NULL) {\r\n*mrp = NULL;\r\niov->lkey = ia->ri_bind_mem->lkey;\r\nreturn 0;\r\n}\r\nipb.addr = iov->addr;\r\nipb.size = iov->length;\r\nmr = ib_reg_phys_mr(ia->ri_pd, &ipb, 1,\r\nIB_ACCESS_LOCAL_WRITE, &iov->addr);\r\ndprintk("RPC: %s: phys convert: 0x%llx "\r\n"registered 0x%llx length %d\n",\r\n__func__, (unsigned long long)ipb.addr,\r\n(unsigned long long)iov->addr, len);\r\nif (IS_ERR(mr)) {\r\n*mrp = NULL;\r\nrc = PTR_ERR(mr);\r\ndprintk("RPC: %s: failed with %i\n", __func__, rc);\r\n} else {\r\n*mrp = mr;\r\niov->lkey = mr->lkey;\r\nrc = 0;\r\n}\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_deregister_internal(struct rpcrdma_ia *ia,\r\nstruct ib_mr *mr, struct ib_sge *iov)\r\n{\r\nint rc;\r\nib_dma_unmap_single(ia->ri_id->device,\r\niov->addr, iov->length, DMA_BIDIRECTIONAL);\r\nif (NULL == mr)\r\nreturn 0;\r\nrc = ib_dereg_mr(mr);\r\nif (rc)\r\ndprintk("RPC: %s: ib_dereg_mr failed %i\n", __func__, rc);\r\nreturn rc;\r\n}\r\nstatic void\r\nrpcrdma_map_one(struct rpcrdma_ia *ia, struct rpcrdma_mr_seg *seg, int writing)\r\n{\r\nseg->mr_dir = writing ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\r\nseg->mr_dmalen = seg->mr_len;\r\nif (seg->mr_page)\r\nseg->mr_dma = ib_dma_map_page(ia->ri_id->device,\r\nseg->mr_page, offset_in_page(seg->mr_offset),\r\nseg->mr_dmalen, seg->mr_dir);\r\nelse\r\nseg->mr_dma = ib_dma_map_single(ia->ri_id->device,\r\nseg->mr_offset,\r\nseg->mr_dmalen, seg->mr_dir);\r\nif (ib_dma_mapping_error(ia->ri_id->device, seg->mr_dma)) {\r\ndprintk("RPC: %s: mr_dma %llx mr_offset %p mr_dma_len %zu\n",\r\n__func__,\r\n(unsigned long long)seg->mr_dma,\r\nseg->mr_offset, seg->mr_dmalen);\r\n}\r\n}\r\nstatic void\r\nrpcrdma_unmap_one(struct rpcrdma_ia *ia, struct rpcrdma_mr_seg *seg)\r\n{\r\nif (seg->mr_page)\r\nib_dma_unmap_page(ia->ri_id->device,\r\nseg->mr_dma, seg->mr_dmalen, seg->mr_dir);\r\nelse\r\nib_dma_unmap_single(ia->ri_id->device,\r\nseg->mr_dma, seg->mr_dmalen, seg->mr_dir);\r\n}\r\nstatic int\r\nrpcrdma_register_frmr_external(struct rpcrdma_mr_seg *seg,\r\nint *nsegs, int writing, struct rpcrdma_ia *ia,\r\nstruct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nstruct ib_send_wr invalidate_wr, frmr_wr, *bad_wr, *post_wr;\r\nu8 key;\r\nint len, pageoff;\r\nint i, rc;\r\nint seg_len;\r\nu64 pa;\r\nint page_no;\r\npageoff = offset_in_page(seg1->mr_offset);\r\nseg1->mr_offset -= pageoff;\r\nseg1->mr_len += pageoff;\r\nlen = -pageoff;\r\nif (*nsegs > RPCRDMA_MAX_DATA_SEGS)\r\n*nsegs = RPCRDMA_MAX_DATA_SEGS;\r\nfor (page_no = i = 0; i < *nsegs;) {\r\nrpcrdma_map_one(ia, seg, writing);\r\npa = seg->mr_dma;\r\nfor (seg_len = seg->mr_len; seg_len > 0; seg_len -= PAGE_SIZE) {\r\nseg1->mr_chunk.rl_mw->r.frmr.fr_pgl->\r\npage_list[page_no++] = pa;\r\npa += PAGE_SIZE;\r\n}\r\nlen += seg->mr_len;\r\n++seg;\r\n++i;\r\nif ((i < *nsegs && offset_in_page(seg->mr_offset)) ||\r\noffset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))\r\nbreak;\r\n}\r\ndprintk("RPC: %s: Using frmr %p to map %d segments\n",\r\n__func__, seg1->mr_chunk.rl_mw, i);\r\nif (unlikely(seg1->mr_chunk.rl_mw->r.frmr.state == FRMR_IS_VALID)) {\r\ndprintk("RPC: %s: frmr %x left valid, posting invalidate.\n",\r\n__func__,\r\nseg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey);\r\nmemset(&invalidate_wr, 0, sizeof invalidate_wr);\r\ninvalidate_wr.wr_id = (unsigned long)(void *)seg1->mr_chunk.rl_mw;\r\ninvalidate_wr.next = &frmr_wr;\r\ninvalidate_wr.opcode = IB_WR_LOCAL_INV;\r\ninvalidate_wr.send_flags = IB_SEND_SIGNALED;\r\ninvalidate_wr.ex.invalidate_rkey =\r\nseg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\npost_wr = &invalidate_wr;\r\n} else\r\npost_wr = &frmr_wr;\r\nkey = (u8)(seg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey & 0x000000FF);\r\nib_update_fast_reg_key(seg1->mr_chunk.rl_mw->r.frmr.fr_mr, ++key);\r\nmemset(&frmr_wr, 0, sizeof frmr_wr);\r\nfrmr_wr.wr_id = (unsigned long)(void *)seg1->mr_chunk.rl_mw;\r\nfrmr_wr.opcode = IB_WR_FAST_REG_MR;\r\nfrmr_wr.send_flags = IB_SEND_SIGNALED;\r\nfrmr_wr.wr.fast_reg.iova_start = seg1->mr_dma;\r\nfrmr_wr.wr.fast_reg.page_list = seg1->mr_chunk.rl_mw->r.frmr.fr_pgl;\r\nfrmr_wr.wr.fast_reg.page_list_len = page_no;\r\nfrmr_wr.wr.fast_reg.page_shift = PAGE_SHIFT;\r\nfrmr_wr.wr.fast_reg.length = page_no << PAGE_SHIFT;\r\nBUG_ON(frmr_wr.wr.fast_reg.length < len);\r\nfrmr_wr.wr.fast_reg.access_flags = (writing ?\r\nIB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :\r\nIB_ACCESS_REMOTE_READ);\r\nfrmr_wr.wr.fast_reg.rkey = seg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\nrc = ib_post_send(ia->ri_id->qp, post_wr, &bad_wr);\r\nif (rc) {\r\ndprintk("RPC: %s: failed ib_post_send for register,"\r\n" status %i\n", __func__, rc);\r\nwhile (i--)\r\nrpcrdma_unmap_one(ia, --seg);\r\n} else {\r\nseg1->mr_rkey = seg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey;\r\nseg1->mr_base = seg1->mr_dma + pageoff;\r\nseg1->mr_nsegs = i;\r\nseg1->mr_len = len;\r\n}\r\n*nsegs = i;\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_deregister_frmr_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_ia *ia, struct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nstruct ib_send_wr invalidate_wr, *bad_wr;\r\nint rc;\r\nwhile (seg1->mr_nsegs--)\r\nrpcrdma_unmap_one(ia, seg++);\r\nmemset(&invalidate_wr, 0, sizeof invalidate_wr);\r\ninvalidate_wr.wr_id = (unsigned long)(void *)seg1->mr_chunk.rl_mw;\r\ninvalidate_wr.opcode = IB_WR_LOCAL_INV;\r\ninvalidate_wr.send_flags = IB_SEND_SIGNALED;\r\ninvalidate_wr.ex.invalidate_rkey = seg1->mr_chunk.rl_mw->r.frmr.fr_mr->rkey;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\nrc = ib_post_send(ia->ri_id->qp, &invalidate_wr, &bad_wr);\r\nif (rc)\r\ndprintk("RPC: %s: failed ib_post_send for invalidate,"\r\n" status %i\n", __func__, rc);\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_register_fmr_external(struct rpcrdma_mr_seg *seg,\r\nint *nsegs, int writing, struct rpcrdma_ia *ia)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nu64 physaddrs[RPCRDMA_MAX_DATA_SEGS];\r\nint len, pageoff, i, rc;\r\npageoff = offset_in_page(seg1->mr_offset);\r\nseg1->mr_offset -= pageoff;\r\nseg1->mr_len += pageoff;\r\nlen = -pageoff;\r\nif (*nsegs > RPCRDMA_MAX_DATA_SEGS)\r\n*nsegs = RPCRDMA_MAX_DATA_SEGS;\r\nfor (i = 0; i < *nsegs;) {\r\nrpcrdma_map_one(ia, seg, writing);\r\nphysaddrs[i] = seg->mr_dma;\r\nlen += seg->mr_len;\r\n++seg;\r\n++i;\r\nif ((i < *nsegs && offset_in_page(seg->mr_offset)) ||\r\noffset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))\r\nbreak;\r\n}\r\nrc = ib_map_phys_fmr(seg1->mr_chunk.rl_mw->r.fmr,\r\nphysaddrs, i, seg1->mr_dma);\r\nif (rc) {\r\ndprintk("RPC: %s: failed ib_map_phys_fmr "\r\n"%u@0x%llx+%i (%d)... status %i\n", __func__,\r\nlen, (unsigned long long)seg1->mr_dma,\r\npageoff, i, rc);\r\nwhile (i--)\r\nrpcrdma_unmap_one(ia, --seg);\r\n} else {\r\nseg1->mr_rkey = seg1->mr_chunk.rl_mw->r.fmr->rkey;\r\nseg1->mr_base = seg1->mr_dma + pageoff;\r\nseg1->mr_nsegs = i;\r\nseg1->mr_len = len;\r\n}\r\n*nsegs = i;\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_deregister_fmr_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_ia *ia)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nLIST_HEAD(l);\r\nint rc;\r\nlist_add(&seg1->mr_chunk.rl_mw->r.fmr->list, &l);\r\nrc = ib_unmap_fmr(&l);\r\nwhile (seg1->mr_nsegs--)\r\nrpcrdma_unmap_one(ia, seg++);\r\nif (rc)\r\ndprintk("RPC: %s: failed ib_unmap_fmr,"\r\n" status %i\n", __func__, rc);\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_register_memwin_external(struct rpcrdma_mr_seg *seg,\r\nint *nsegs, int writing, struct rpcrdma_ia *ia,\r\nstruct rpcrdma_xprt *r_xprt)\r\n{\r\nint mem_priv = (writing ? IB_ACCESS_REMOTE_WRITE :\r\nIB_ACCESS_REMOTE_READ);\r\nstruct ib_mw_bind param;\r\nint rc;\r\n*nsegs = 1;\r\nrpcrdma_map_one(ia, seg, writing);\r\nparam.bind_info.mr = ia->ri_bind_mem;\r\nparam.wr_id = 0ULL;\r\nparam.bind_info.addr = seg->mr_dma;\r\nparam.bind_info.length = seg->mr_len;\r\nparam.send_flags = 0;\r\nparam.bind_info.mw_access_flags = mem_priv;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\nrc = ib_bind_mw(ia->ri_id->qp, seg->mr_chunk.rl_mw->r.mw, &param);\r\nif (rc) {\r\ndprintk("RPC: %s: failed ib_bind_mw "\r\n"%u@0x%llx status %i\n",\r\n__func__, seg->mr_len,\r\n(unsigned long long)seg->mr_dma, rc);\r\nrpcrdma_unmap_one(ia, seg);\r\n} else {\r\nseg->mr_rkey = seg->mr_chunk.rl_mw->r.mw->rkey;\r\nseg->mr_base = param.bind_info.addr;\r\nseg->mr_nsegs = 1;\r\n}\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_deregister_memwin_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_ia *ia,\r\nstruct rpcrdma_xprt *r_xprt, void **r)\r\n{\r\nstruct ib_mw_bind param;\r\nLIST_HEAD(l);\r\nint rc;\r\nBUG_ON(seg->mr_nsegs != 1);\r\nparam.bind_info.mr = ia->ri_bind_mem;\r\nparam.bind_info.addr = 0ULL;\r\nparam.bind_info.length = 0;\r\nparam.bind_info.mw_access_flags = 0;\r\nif (*r) {\r\nparam.wr_id = (u64) (unsigned long) *r;\r\nparam.send_flags = IB_SEND_SIGNALED;\r\nINIT_CQCOUNT(&r_xprt->rx_ep);\r\n} else {\r\nparam.wr_id = 0ULL;\r\nparam.send_flags = 0;\r\nDECR_CQCOUNT(&r_xprt->rx_ep);\r\n}\r\nrc = ib_bind_mw(ia->ri_id->qp, seg->mr_chunk.rl_mw->r.mw, &param);\r\nrpcrdma_unmap_one(ia, seg);\r\nif (rc)\r\ndprintk("RPC: %s: failed ib_(un)bind_mw,"\r\n" status %i\n", __func__, rc);\r\nelse\r\n*r = NULL;\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_register_default_external(struct rpcrdma_mr_seg *seg,\r\nint *nsegs, int writing, struct rpcrdma_ia *ia)\r\n{\r\nint mem_priv = (writing ? IB_ACCESS_REMOTE_WRITE :\r\nIB_ACCESS_REMOTE_READ);\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nstruct ib_phys_buf ipb[RPCRDMA_MAX_DATA_SEGS];\r\nint len, i, rc = 0;\r\nif (*nsegs > RPCRDMA_MAX_DATA_SEGS)\r\n*nsegs = RPCRDMA_MAX_DATA_SEGS;\r\nfor (len = 0, i = 0; i < *nsegs;) {\r\nrpcrdma_map_one(ia, seg, writing);\r\nipb[i].addr = seg->mr_dma;\r\nipb[i].size = seg->mr_len;\r\nlen += seg->mr_len;\r\n++seg;\r\n++i;\r\nif ((i < *nsegs && offset_in_page(seg->mr_offset)) ||\r\noffset_in_page((seg-1)->mr_offset+(seg-1)->mr_len))\r\nbreak;\r\n}\r\nseg1->mr_base = seg1->mr_dma;\r\nseg1->mr_chunk.rl_mr = ib_reg_phys_mr(ia->ri_pd,\r\nipb, i, mem_priv, &seg1->mr_base);\r\nif (IS_ERR(seg1->mr_chunk.rl_mr)) {\r\nrc = PTR_ERR(seg1->mr_chunk.rl_mr);\r\ndprintk("RPC: %s: failed ib_reg_phys_mr "\r\n"%u@0x%llx (%d)... status %i\n",\r\n__func__, len,\r\n(unsigned long long)seg1->mr_dma, i, rc);\r\nwhile (i--)\r\nrpcrdma_unmap_one(ia, --seg);\r\n} else {\r\nseg1->mr_rkey = seg1->mr_chunk.rl_mr->rkey;\r\nseg1->mr_nsegs = i;\r\nseg1->mr_len = len;\r\n}\r\n*nsegs = i;\r\nreturn rc;\r\n}\r\nstatic int\r\nrpcrdma_deregister_default_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_ia *ia)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nint rc;\r\nrc = ib_dereg_mr(seg1->mr_chunk.rl_mr);\r\nseg1->mr_chunk.rl_mr = NULL;\r\nwhile (seg1->mr_nsegs--)\r\nrpcrdma_unmap_one(ia, seg++);\r\nif (rc)\r\ndprintk("RPC: %s: failed ib_dereg_mr,"\r\n" status %i\n", __func__, rc);\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_register_external(struct rpcrdma_mr_seg *seg,\r\nint nsegs, int writing, struct rpcrdma_xprt *r_xprt)\r\n{\r\nstruct rpcrdma_ia *ia = &r_xprt->rx_ia;\r\nint rc = 0;\r\nswitch (ia->ri_memreg_strategy) {\r\n#if RPCRDMA_PERSISTENT_REGISTRATION\r\ncase RPCRDMA_ALLPHYSICAL:\r\nrpcrdma_map_one(ia, seg, writing);\r\nseg->mr_rkey = ia->ri_bind_mem->rkey;\r\nseg->mr_base = seg->mr_dma;\r\nseg->mr_nsegs = 1;\r\nnsegs = 1;\r\nbreak;\r\n#endif\r\ncase RPCRDMA_FRMR:\r\nrc = rpcrdma_register_frmr_external(seg, &nsegs, writing, ia, r_xprt);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nrc = rpcrdma_register_fmr_external(seg, &nsegs, writing, ia);\r\nbreak;\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nrc = rpcrdma_register_memwin_external(seg, &nsegs, writing, ia, r_xprt);\r\nbreak;\r\ndefault:\r\nrc = rpcrdma_register_default_external(seg, &nsegs, writing, ia);\r\nbreak;\r\n}\r\nif (rc)\r\nreturn -1;\r\nreturn nsegs;\r\n}\r\nint\r\nrpcrdma_deregister_external(struct rpcrdma_mr_seg *seg,\r\nstruct rpcrdma_xprt *r_xprt, void *r)\r\n{\r\nstruct rpcrdma_ia *ia = &r_xprt->rx_ia;\r\nint nsegs = seg->mr_nsegs, rc;\r\nswitch (ia->ri_memreg_strategy) {\r\n#if RPCRDMA_PERSISTENT_REGISTRATION\r\ncase RPCRDMA_ALLPHYSICAL:\r\nBUG_ON(nsegs != 1);\r\nrpcrdma_unmap_one(ia, seg);\r\nrc = 0;\r\nbreak;\r\n#endif\r\ncase RPCRDMA_FRMR:\r\nrc = rpcrdma_deregister_frmr_external(seg, ia, r_xprt);\r\nbreak;\r\ncase RPCRDMA_MTHCAFMR:\r\nrc = rpcrdma_deregister_fmr_external(seg, ia);\r\nbreak;\r\ncase RPCRDMA_MEMWINDOWS_ASYNC:\r\ncase RPCRDMA_MEMWINDOWS:\r\nrc = rpcrdma_deregister_memwin_external(seg, ia, r_xprt, &r);\r\nbreak;\r\ndefault:\r\nrc = rpcrdma_deregister_default_external(seg, ia);\r\nbreak;\r\n}\r\nif (r) {\r\nstruct rpcrdma_rep *rep = r;\r\nvoid (*func)(struct rpcrdma_rep *) = rep->rr_func;\r\nrep->rr_func = NULL;\r\nfunc(rep);\r\n}\r\nreturn nsegs;\r\n}\r\nint\r\nrpcrdma_ep_post(struct rpcrdma_ia *ia,\r\nstruct rpcrdma_ep *ep,\r\nstruct rpcrdma_req *req)\r\n{\r\nstruct ib_send_wr send_wr, *send_wr_fail;\r\nstruct rpcrdma_rep *rep = req->rl_reply;\r\nint rc;\r\nif (rep) {\r\nrc = rpcrdma_ep_post_recv(ia, ep, rep);\r\nif (rc)\r\ngoto out;\r\nreq->rl_reply = NULL;\r\n}\r\nsend_wr.next = NULL;\r\nsend_wr.wr_id = 0ULL;\r\nsend_wr.sg_list = req->rl_send_iov;\r\nsend_wr.num_sge = req->rl_niovs;\r\nsend_wr.opcode = IB_WR_SEND;\r\nif (send_wr.num_sge == 4)\r\nib_dma_sync_single_for_device(ia->ri_id->device,\r\nreq->rl_send_iov[3].addr, req->rl_send_iov[3].length,\r\nDMA_TO_DEVICE);\r\nib_dma_sync_single_for_device(ia->ri_id->device,\r\nreq->rl_send_iov[1].addr, req->rl_send_iov[1].length,\r\nDMA_TO_DEVICE);\r\nib_dma_sync_single_for_device(ia->ri_id->device,\r\nreq->rl_send_iov[0].addr, req->rl_send_iov[0].length,\r\nDMA_TO_DEVICE);\r\nif (DECR_CQCOUNT(ep) > 0)\r\nsend_wr.send_flags = 0;\r\nelse {\r\nINIT_CQCOUNT(ep);\r\nsend_wr.send_flags = IB_SEND_SIGNALED;\r\n}\r\nrc = ib_post_send(ia->ri_id->qp, &send_wr, &send_wr_fail);\r\nif (rc)\r\ndprintk("RPC: %s: ib_post_send returned %i\n", __func__,\r\nrc);\r\nout:\r\nreturn rc;\r\n}\r\nint\r\nrpcrdma_ep_post_recv(struct rpcrdma_ia *ia,\r\nstruct rpcrdma_ep *ep,\r\nstruct rpcrdma_rep *rep)\r\n{\r\nstruct ib_recv_wr recv_wr, *recv_wr_fail;\r\nint rc;\r\nrecv_wr.next = NULL;\r\nrecv_wr.wr_id = (u64) (unsigned long) rep;\r\nrecv_wr.sg_list = &rep->rr_iov;\r\nrecv_wr.num_sge = 1;\r\nib_dma_sync_single_for_cpu(ia->ri_id->device,\r\nrep->rr_iov.addr, rep->rr_iov.length, DMA_BIDIRECTIONAL);\r\nDECR_CQCOUNT(ep);\r\nrc = ib_post_recv(ia->ri_id->qp, &recv_wr, &recv_wr_fail);\r\nif (rc)\r\ndprintk("RPC: %s: ib_post_recv returned %i\n", __func__,\r\nrc);\r\nreturn rc;\r\n}
