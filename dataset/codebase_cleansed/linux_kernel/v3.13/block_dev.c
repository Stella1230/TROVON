static inline struct bdev_inode *BDEV_I(struct inode *inode)\r\n{\r\nreturn container_of(inode, struct bdev_inode, vfs_inode);\r\n}\r\ninline struct block_device *I_BDEV(struct inode *inode)\r\n{\r\nreturn &BDEV_I(inode)->bdev;\r\n}\r\nstatic void bdev_inode_switch_bdi(struct inode *inode,\r\nstruct backing_dev_info *dst)\r\n{\r\nstruct backing_dev_info *old = inode->i_data.backing_dev_info;\r\nbool wakeup_bdi = false;\r\nif (unlikely(dst == old))\r\nreturn;\r\nbdi_lock_two(&old->wb, &dst->wb);\r\nspin_lock(&inode->i_lock);\r\ninode->i_data.backing_dev_info = dst;\r\nif (inode->i_state & I_DIRTY) {\r\nif (bdi_cap_writeback_dirty(dst) && !wb_has_dirty_io(&dst->wb))\r\nwakeup_bdi = true;\r\nlist_move(&inode->i_wb_list, &dst->wb.b_dirty);\r\n}\r\nspin_unlock(&inode->i_lock);\r\nspin_unlock(&old->wb.list_lock);\r\nspin_unlock(&dst->wb.list_lock);\r\nif (wakeup_bdi)\r\nbdi_wakeup_thread_delayed(dst);\r\n}\r\nvoid kill_bdev(struct block_device *bdev)\r\n{\r\nstruct address_space *mapping = bdev->bd_inode->i_mapping;\r\nif (mapping->nrpages == 0)\r\nreturn;\r\ninvalidate_bh_lrus();\r\ntruncate_inode_pages(mapping, 0);\r\n}\r\nvoid invalidate_bdev(struct block_device *bdev)\r\n{\r\nstruct address_space *mapping = bdev->bd_inode->i_mapping;\r\nif (mapping->nrpages == 0)\r\nreturn;\r\ninvalidate_bh_lrus();\r\nlru_add_drain_all();\r\ninvalidate_mapping_pages(mapping, 0, -1);\r\ncleancache_invalidate_inode(mapping);\r\n}\r\nint set_blocksize(struct block_device *bdev, int size)\r\n{\r\nif (size > PAGE_SIZE || size < 512 || !is_power_of_2(size))\r\nreturn -EINVAL;\r\nif (size < bdev_logical_block_size(bdev))\r\nreturn -EINVAL;\r\nif (bdev->bd_block_size != size) {\r\nsync_blockdev(bdev);\r\nbdev->bd_block_size = size;\r\nbdev->bd_inode->i_blkbits = blksize_bits(size);\r\nkill_bdev(bdev);\r\n}\r\nreturn 0;\r\n}\r\nint sb_set_blocksize(struct super_block *sb, int size)\r\n{\r\nif (set_blocksize(sb->s_bdev, size))\r\nreturn 0;\r\nsb->s_blocksize = size;\r\nsb->s_blocksize_bits = blksize_bits(size);\r\nreturn sb->s_blocksize;\r\n}\r\nint sb_min_blocksize(struct super_block *sb, int size)\r\n{\r\nint minsize = bdev_logical_block_size(sb->s_bdev);\r\nif (size < minsize)\r\nsize = minsize;\r\nreturn sb_set_blocksize(sb, size);\r\n}\r\nstatic int\r\nblkdev_get_block(struct inode *inode, sector_t iblock,\r\nstruct buffer_head *bh, int create)\r\n{\r\nbh->b_bdev = I_BDEV(inode);\r\nbh->b_blocknr = iblock;\r\nset_buffer_mapped(bh);\r\nreturn 0;\r\n}\r\nstatic ssize_t\r\nblkdev_direct_IO(int rw, struct kiocb *iocb, const struct iovec *iov,\r\nloff_t offset, unsigned long nr_segs)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct inode *inode = file->f_mapping->host;\r\nreturn __blockdev_direct_IO(rw, iocb, inode, I_BDEV(inode), iov, offset,\r\nnr_segs, blkdev_get_block, NULL, NULL, 0);\r\n}\r\nint __sync_blockdev(struct block_device *bdev, int wait)\r\n{\r\nif (!bdev)\r\nreturn 0;\r\nif (!wait)\r\nreturn filemap_flush(bdev->bd_inode->i_mapping);\r\nreturn filemap_write_and_wait(bdev->bd_inode->i_mapping);\r\n}\r\nint sync_blockdev(struct block_device *bdev)\r\n{\r\nreturn __sync_blockdev(bdev, 1);\r\n}\r\nint fsync_bdev(struct block_device *bdev)\r\n{\r\nstruct super_block *sb = get_super(bdev);\r\nif (sb) {\r\nint res = sync_filesystem(sb);\r\ndrop_super(sb);\r\nreturn res;\r\n}\r\nreturn sync_blockdev(bdev);\r\n}\r\nstruct super_block *freeze_bdev(struct block_device *bdev)\r\n{\r\nstruct super_block *sb;\r\nint error = 0;\r\nmutex_lock(&bdev->bd_fsfreeze_mutex);\r\nif (++bdev->bd_fsfreeze_count > 1) {\r\nsb = get_super(bdev);\r\ndrop_super(sb);\r\nmutex_unlock(&bdev->bd_fsfreeze_mutex);\r\nreturn sb;\r\n}\r\nsb = get_active_super(bdev);\r\nif (!sb)\r\ngoto out;\r\nerror = freeze_super(sb);\r\nif (error) {\r\ndeactivate_super(sb);\r\nbdev->bd_fsfreeze_count--;\r\nmutex_unlock(&bdev->bd_fsfreeze_mutex);\r\nreturn ERR_PTR(error);\r\n}\r\ndeactivate_super(sb);\r\nout:\r\nsync_blockdev(bdev);\r\nmutex_unlock(&bdev->bd_fsfreeze_mutex);\r\nreturn sb;\r\n}\r\nint thaw_bdev(struct block_device *bdev, struct super_block *sb)\r\n{\r\nint error = -EINVAL;\r\nmutex_lock(&bdev->bd_fsfreeze_mutex);\r\nif (!bdev->bd_fsfreeze_count)\r\ngoto out;\r\nerror = 0;\r\nif (--bdev->bd_fsfreeze_count > 0)\r\ngoto out;\r\nif (!sb)\r\ngoto out;\r\nerror = thaw_super(sb);\r\nif (error) {\r\nbdev->bd_fsfreeze_count++;\r\nmutex_unlock(&bdev->bd_fsfreeze_mutex);\r\nreturn error;\r\n}\r\nout:\r\nmutex_unlock(&bdev->bd_fsfreeze_mutex);\r\nreturn 0;\r\n}\r\nstatic int blkdev_writepage(struct page *page, struct writeback_control *wbc)\r\n{\r\nreturn block_write_full_page(page, blkdev_get_block, wbc);\r\n}\r\nstatic int blkdev_readpage(struct file * file, struct page * page)\r\n{\r\nreturn block_read_full_page(page, blkdev_get_block);\r\n}\r\nstatic int blkdev_write_begin(struct file *file, struct address_space *mapping,\r\nloff_t pos, unsigned len, unsigned flags,\r\nstruct page **pagep, void **fsdata)\r\n{\r\nreturn block_write_begin(mapping, pos, len, flags, pagep,\r\nblkdev_get_block);\r\n}\r\nstatic int blkdev_write_end(struct file *file, struct address_space *mapping,\r\nloff_t pos, unsigned len, unsigned copied,\r\nstruct page *page, void *fsdata)\r\n{\r\nint ret;\r\nret = block_write_end(file, mapping, pos, len, copied, page, fsdata);\r\nunlock_page(page);\r\npage_cache_release(page);\r\nreturn ret;\r\n}\r\nstatic loff_t block_llseek(struct file *file, loff_t offset, int whence)\r\n{\r\nstruct inode *bd_inode = file->f_mapping->host;\r\nloff_t retval;\r\nmutex_lock(&bd_inode->i_mutex);\r\nretval = fixed_size_llseek(file, offset, whence, i_size_read(bd_inode));\r\nmutex_unlock(&bd_inode->i_mutex);\r\nreturn retval;\r\n}\r\nint blkdev_fsync(struct file *filp, loff_t start, loff_t end, int datasync)\r\n{\r\nstruct inode *bd_inode = filp->f_mapping->host;\r\nstruct block_device *bdev = I_BDEV(bd_inode);\r\nint error;\r\nerror = filemap_write_and_wait_range(filp->f_mapping, start, end);\r\nif (error)\r\nreturn error;\r\nerror = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);\r\nif (error == -EOPNOTSUPP)\r\nerror = 0;\r\nreturn error;\r\n}\r\nstatic struct inode *bdev_alloc_inode(struct super_block *sb)\r\n{\r\nstruct bdev_inode *ei = kmem_cache_alloc(bdev_cachep, GFP_KERNEL);\r\nif (!ei)\r\nreturn NULL;\r\nreturn &ei->vfs_inode;\r\n}\r\nstatic void bdev_i_callback(struct rcu_head *head)\r\n{\r\nstruct inode *inode = container_of(head, struct inode, i_rcu);\r\nstruct bdev_inode *bdi = BDEV_I(inode);\r\nkmem_cache_free(bdev_cachep, bdi);\r\n}\r\nstatic void bdev_destroy_inode(struct inode *inode)\r\n{\r\ncall_rcu(&inode->i_rcu, bdev_i_callback);\r\n}\r\nstatic void init_once(void *foo)\r\n{\r\nstruct bdev_inode *ei = (struct bdev_inode *) foo;\r\nstruct block_device *bdev = &ei->bdev;\r\nmemset(bdev, 0, sizeof(*bdev));\r\nmutex_init(&bdev->bd_mutex);\r\nINIT_LIST_HEAD(&bdev->bd_inodes);\r\nINIT_LIST_HEAD(&bdev->bd_list);\r\n#ifdef CONFIG_SYSFS\r\nINIT_LIST_HEAD(&bdev->bd_holder_disks);\r\n#endif\r\ninode_init_once(&ei->vfs_inode);\r\nmutex_init(&bdev->bd_fsfreeze_mutex);\r\n}\r\nstatic inline void __bd_forget(struct inode *inode)\r\n{\r\nlist_del_init(&inode->i_devices);\r\ninode->i_bdev = NULL;\r\ninode->i_mapping = &inode->i_data;\r\n}\r\nstatic void bdev_evict_inode(struct inode *inode)\r\n{\r\nstruct block_device *bdev = &BDEV_I(inode)->bdev;\r\nstruct list_head *p;\r\ntruncate_inode_pages(&inode->i_data, 0);\r\ninvalidate_inode_buffers(inode);\r\nclear_inode(inode);\r\nspin_lock(&bdev_lock);\r\nwhile ( (p = bdev->bd_inodes.next) != &bdev->bd_inodes ) {\r\n__bd_forget(list_entry(p, struct inode, i_devices));\r\n}\r\nlist_del_init(&bdev->bd_list);\r\nspin_unlock(&bdev_lock);\r\n}\r\nstatic struct dentry *bd_mount(struct file_system_type *fs_type,\r\nint flags, const char *dev_name, void *data)\r\n{\r\nreturn mount_pseudo(fs_type, "bdev:", &bdev_sops, NULL, BDEVFS_MAGIC);\r\n}\r\nvoid __init bdev_cache_init(void)\r\n{\r\nint err;\r\nstatic struct vfsmount *bd_mnt;\r\nbdev_cachep = kmem_cache_create("bdev_cache", sizeof(struct bdev_inode),\r\n0, (SLAB_HWCACHE_ALIGN|SLAB_RECLAIM_ACCOUNT|\r\nSLAB_MEM_SPREAD|SLAB_PANIC),\r\ninit_once);\r\nerr = register_filesystem(&bd_type);\r\nif (err)\r\npanic("Cannot register bdev pseudo-fs");\r\nbd_mnt = kern_mount(&bd_type);\r\nif (IS_ERR(bd_mnt))\r\npanic("Cannot create bdev pseudo-fs");\r\nblockdev_superblock = bd_mnt->mnt_sb;\r\n}\r\nstatic inline unsigned long hash(dev_t dev)\r\n{\r\nreturn MAJOR(dev)+MINOR(dev);\r\n}\r\nstatic int bdev_test(struct inode *inode, void *data)\r\n{\r\nreturn BDEV_I(inode)->bdev.bd_dev == *(dev_t *)data;\r\n}\r\nstatic int bdev_set(struct inode *inode, void *data)\r\n{\r\nBDEV_I(inode)->bdev.bd_dev = *(dev_t *)data;\r\nreturn 0;\r\n}\r\nstruct block_device *bdget(dev_t dev)\r\n{\r\nstruct block_device *bdev;\r\nstruct inode *inode;\r\ninode = iget5_locked(blockdev_superblock, hash(dev),\r\nbdev_test, bdev_set, &dev);\r\nif (!inode)\r\nreturn NULL;\r\nbdev = &BDEV_I(inode)->bdev;\r\nif (inode->i_state & I_NEW) {\r\nbdev->bd_contains = NULL;\r\nbdev->bd_super = NULL;\r\nbdev->bd_inode = inode;\r\nbdev->bd_block_size = (1 << inode->i_blkbits);\r\nbdev->bd_part_count = 0;\r\nbdev->bd_invalidated = 0;\r\ninode->i_mode = S_IFBLK;\r\ninode->i_rdev = dev;\r\ninode->i_bdev = bdev;\r\ninode->i_data.a_ops = &def_blk_aops;\r\nmapping_set_gfp_mask(&inode->i_data, GFP_USER);\r\ninode->i_data.backing_dev_info = &default_backing_dev_info;\r\nspin_lock(&bdev_lock);\r\nlist_add(&bdev->bd_list, &all_bdevs);\r\nspin_unlock(&bdev_lock);\r\nunlock_new_inode(inode);\r\n}\r\nreturn bdev;\r\n}\r\nstruct block_device *bdgrab(struct block_device *bdev)\r\n{\r\nihold(bdev->bd_inode);\r\nreturn bdev;\r\n}\r\nlong nr_blockdev_pages(void)\r\n{\r\nstruct block_device *bdev;\r\nlong ret = 0;\r\nspin_lock(&bdev_lock);\r\nlist_for_each_entry(bdev, &all_bdevs, bd_list) {\r\nret += bdev->bd_inode->i_mapping->nrpages;\r\n}\r\nspin_unlock(&bdev_lock);\r\nreturn ret;\r\n}\r\nvoid bdput(struct block_device *bdev)\r\n{\r\niput(bdev->bd_inode);\r\n}\r\nstatic struct block_device *bd_acquire(struct inode *inode)\r\n{\r\nstruct block_device *bdev;\r\nspin_lock(&bdev_lock);\r\nbdev = inode->i_bdev;\r\nif (bdev) {\r\nihold(bdev->bd_inode);\r\nspin_unlock(&bdev_lock);\r\nreturn bdev;\r\n}\r\nspin_unlock(&bdev_lock);\r\nbdev = bdget(inode->i_rdev);\r\nif (bdev) {\r\nspin_lock(&bdev_lock);\r\nif (!inode->i_bdev) {\r\nihold(bdev->bd_inode);\r\ninode->i_bdev = bdev;\r\ninode->i_mapping = bdev->bd_inode->i_mapping;\r\nlist_add(&inode->i_devices, &bdev->bd_inodes);\r\n}\r\nspin_unlock(&bdev_lock);\r\n}\r\nreturn bdev;\r\n}\r\nint sb_is_blkdev_sb(struct super_block *sb)\r\n{\r\nreturn sb == blockdev_superblock;\r\n}\r\nvoid bd_forget(struct inode *inode)\r\n{\r\nstruct block_device *bdev = NULL;\r\nspin_lock(&bdev_lock);\r\nif (!sb_is_blkdev_sb(inode->i_sb))\r\nbdev = inode->i_bdev;\r\n__bd_forget(inode);\r\nspin_unlock(&bdev_lock);\r\nif (bdev)\r\niput(bdev->bd_inode);\r\n}\r\nstatic bool bd_may_claim(struct block_device *bdev, struct block_device *whole,\r\nvoid *holder)\r\n{\r\nif (bdev->bd_holder == holder)\r\nreturn true;\r\nelse if (bdev->bd_holder != NULL)\r\nreturn false;\r\nelse if (bdev->bd_contains == bdev)\r\nreturn true;\r\nelse if (whole->bd_holder == bd_may_claim)\r\nreturn true;\r\nelse if (whole->bd_holder != NULL)\r\nreturn false;\r\nelse\r\nreturn true;\r\n}\r\nstatic int bd_prepare_to_claim(struct block_device *bdev,\r\nstruct block_device *whole, void *holder)\r\n{\r\nretry:\r\nif (!bd_may_claim(bdev, whole, holder))\r\nreturn -EBUSY;\r\nif (whole->bd_claiming) {\r\nwait_queue_head_t *wq = bit_waitqueue(&whole->bd_claiming, 0);\r\nDEFINE_WAIT(wait);\r\nprepare_to_wait(wq, &wait, TASK_UNINTERRUPTIBLE);\r\nspin_unlock(&bdev_lock);\r\nschedule();\r\nfinish_wait(wq, &wait);\r\nspin_lock(&bdev_lock);\r\ngoto retry;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct block_device *bd_start_claiming(struct block_device *bdev,\r\nvoid *holder)\r\n{\r\nstruct gendisk *disk;\r\nstruct block_device *whole;\r\nint partno, err;\r\nmight_sleep();\r\ndisk = get_gendisk(bdev->bd_dev, &partno);\r\nif (!disk)\r\nreturn ERR_PTR(-ENXIO);\r\nif (partno)\r\nwhole = bdget_disk(disk, 0);\r\nelse\r\nwhole = bdgrab(bdev);\r\nmodule_put(disk->fops->owner);\r\nput_disk(disk);\r\nif (!whole)\r\nreturn ERR_PTR(-ENOMEM);\r\nspin_lock(&bdev_lock);\r\nerr = bd_prepare_to_claim(bdev, whole, holder);\r\nif (err == 0) {\r\nwhole->bd_claiming = holder;\r\nspin_unlock(&bdev_lock);\r\nreturn whole;\r\n} else {\r\nspin_unlock(&bdev_lock);\r\nbdput(whole);\r\nreturn ERR_PTR(err);\r\n}\r\n}\r\nstatic struct bd_holder_disk *bd_find_holder_disk(struct block_device *bdev,\r\nstruct gendisk *disk)\r\n{\r\nstruct bd_holder_disk *holder;\r\nlist_for_each_entry(holder, &bdev->bd_holder_disks, list)\r\nif (holder->disk == disk)\r\nreturn holder;\r\nreturn NULL;\r\n}\r\nstatic int add_symlink(struct kobject *from, struct kobject *to)\r\n{\r\nreturn sysfs_create_link(from, to, kobject_name(to));\r\n}\r\nstatic void del_symlink(struct kobject *from, struct kobject *to)\r\n{\r\nsysfs_remove_link(from, kobject_name(to));\r\n}\r\nint bd_link_disk_holder(struct block_device *bdev, struct gendisk *disk)\r\n{\r\nstruct bd_holder_disk *holder;\r\nint ret = 0;\r\nmutex_lock(&bdev->bd_mutex);\r\nWARN_ON_ONCE(!bdev->bd_holder);\r\nif (WARN_ON(!disk->slave_dir || !bdev->bd_part->holder_dir))\r\ngoto out_unlock;\r\nholder = bd_find_holder_disk(bdev, disk);\r\nif (holder) {\r\nholder->refcnt++;\r\ngoto out_unlock;\r\n}\r\nholder = kzalloc(sizeof(*holder), GFP_KERNEL);\r\nif (!holder) {\r\nret = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\nINIT_LIST_HEAD(&holder->list);\r\nholder->disk = disk;\r\nholder->refcnt = 1;\r\nret = add_symlink(disk->slave_dir, &part_to_dev(bdev->bd_part)->kobj);\r\nif (ret)\r\ngoto out_free;\r\nret = add_symlink(bdev->bd_part->holder_dir, &disk_to_dev(disk)->kobj);\r\nif (ret)\r\ngoto out_del;\r\nkobject_get(bdev->bd_part->holder_dir);\r\nlist_add(&holder->list, &bdev->bd_holder_disks);\r\ngoto out_unlock;\r\nout_del:\r\ndel_symlink(disk->slave_dir, &part_to_dev(bdev->bd_part)->kobj);\r\nout_free:\r\nkfree(holder);\r\nout_unlock:\r\nmutex_unlock(&bdev->bd_mutex);\r\nreturn ret;\r\n}\r\nvoid bd_unlink_disk_holder(struct block_device *bdev, struct gendisk *disk)\r\n{\r\nstruct bd_holder_disk *holder;\r\nmutex_lock(&bdev->bd_mutex);\r\nholder = bd_find_holder_disk(bdev, disk);\r\nif (!WARN_ON_ONCE(holder == NULL) && !--holder->refcnt) {\r\ndel_symlink(disk->slave_dir, &part_to_dev(bdev->bd_part)->kobj);\r\ndel_symlink(bdev->bd_part->holder_dir,\r\n&disk_to_dev(disk)->kobj);\r\nkobject_put(bdev->bd_part->holder_dir);\r\nlist_del_init(&holder->list);\r\nkfree(holder);\r\n}\r\nmutex_unlock(&bdev->bd_mutex);\r\n}\r\nstatic void flush_disk(struct block_device *bdev, bool kill_dirty)\r\n{\r\nif (__invalidate_device(bdev, kill_dirty)) {\r\nchar name[BDEVNAME_SIZE] = "";\r\nif (bdev->bd_disk)\r\ndisk_name(bdev->bd_disk, 0, name);\r\nprintk(KERN_WARNING "VFS: busy inodes on changed media or "\r\n"resized disk %s\n", name);\r\n}\r\nif (!bdev->bd_disk)\r\nreturn;\r\nif (disk_part_scan_enabled(bdev->bd_disk))\r\nbdev->bd_invalidated = 1;\r\n}\r\nvoid check_disk_size_change(struct gendisk *disk, struct block_device *bdev)\r\n{\r\nloff_t disk_size, bdev_size;\r\ndisk_size = (loff_t)get_capacity(disk) << 9;\r\nbdev_size = i_size_read(bdev->bd_inode);\r\nif (disk_size != bdev_size) {\r\nchar name[BDEVNAME_SIZE];\r\ndisk_name(disk, 0, name);\r\nprintk(KERN_INFO\r\n"%s: detected capacity change from %lld to %lld\n",\r\nname, bdev_size, disk_size);\r\ni_size_write(bdev->bd_inode, disk_size);\r\nflush_disk(bdev, false);\r\n}\r\n}\r\nint revalidate_disk(struct gendisk *disk)\r\n{\r\nstruct block_device *bdev;\r\nint ret = 0;\r\nif (disk->fops->revalidate_disk)\r\nret = disk->fops->revalidate_disk(disk);\r\nbdev = bdget_disk(disk, 0);\r\nif (!bdev)\r\nreturn ret;\r\nmutex_lock(&bdev->bd_mutex);\r\ncheck_disk_size_change(disk, bdev);\r\nbdev->bd_invalidated = 0;\r\nmutex_unlock(&bdev->bd_mutex);\r\nbdput(bdev);\r\nreturn ret;\r\n}\r\nint check_disk_change(struct block_device *bdev)\r\n{\r\nstruct gendisk *disk = bdev->bd_disk;\r\nconst struct block_device_operations *bdops = disk->fops;\r\nunsigned int events;\r\nevents = disk_clear_events(disk, DISK_EVENT_MEDIA_CHANGE |\r\nDISK_EVENT_EJECT_REQUEST);\r\nif (!(events & DISK_EVENT_MEDIA_CHANGE))\r\nreturn 0;\r\nflush_disk(bdev, true);\r\nif (bdops->revalidate_disk)\r\nbdops->revalidate_disk(bdev->bd_disk);\r\nreturn 1;\r\n}\r\nvoid bd_set_size(struct block_device *bdev, loff_t size)\r\n{\r\nunsigned bsize = bdev_logical_block_size(bdev);\r\nmutex_lock(&bdev->bd_inode->i_mutex);\r\ni_size_write(bdev->bd_inode, size);\r\nmutex_unlock(&bdev->bd_inode->i_mutex);\r\nwhile (bsize < PAGE_CACHE_SIZE) {\r\nif (size & bsize)\r\nbreak;\r\nbsize <<= 1;\r\n}\r\nbdev->bd_block_size = bsize;\r\nbdev->bd_inode->i_blkbits = blksize_bits(bsize);\r\n}\r\nstatic int __blkdev_get(struct block_device *bdev, fmode_t mode, int for_part)\r\n{\r\nstruct gendisk *disk;\r\nstruct module *owner;\r\nint ret;\r\nint partno;\r\nint perm = 0;\r\nif (mode & FMODE_READ)\r\nperm |= MAY_READ;\r\nif (mode & FMODE_WRITE)\r\nperm |= MAY_WRITE;\r\nif (!for_part) {\r\nret = devcgroup_inode_permission(bdev->bd_inode, perm);\r\nif (ret != 0) {\r\nbdput(bdev);\r\nreturn ret;\r\n}\r\n}\r\nrestart:\r\nret = -ENXIO;\r\ndisk = get_gendisk(bdev->bd_dev, &partno);\r\nif (!disk)\r\ngoto out;\r\nowner = disk->fops->owner;\r\ndisk_block_events(disk);\r\nmutex_lock_nested(&bdev->bd_mutex, for_part);\r\nif (!bdev->bd_openers) {\r\nbdev->bd_disk = disk;\r\nbdev->bd_queue = disk->queue;\r\nbdev->bd_contains = bdev;\r\nif (!partno) {\r\nstruct backing_dev_info *bdi;\r\nret = -ENXIO;\r\nbdev->bd_part = disk_get_part(disk, partno);\r\nif (!bdev->bd_part)\r\ngoto out_clear;\r\nret = 0;\r\nif (disk->fops->open) {\r\nret = disk->fops->open(bdev, mode);\r\nif (ret == -ERESTARTSYS) {\r\ndisk_put_part(bdev->bd_part);\r\nbdev->bd_part = NULL;\r\nbdev->bd_disk = NULL;\r\nbdev->bd_queue = NULL;\r\nmutex_unlock(&bdev->bd_mutex);\r\ndisk_unblock_events(disk);\r\nput_disk(disk);\r\nmodule_put(owner);\r\ngoto restart;\r\n}\r\n}\r\nif (!ret) {\r\nbd_set_size(bdev,(loff_t)get_capacity(disk)<<9);\r\nbdi = blk_get_backing_dev_info(bdev);\r\nif (bdi == NULL)\r\nbdi = &default_backing_dev_info;\r\nbdev_inode_switch_bdi(bdev->bd_inode, bdi);\r\n}\r\nif (bdev->bd_invalidated) {\r\nif (!ret)\r\nrescan_partitions(disk, bdev);\r\nelse if (ret == -ENOMEDIUM)\r\ninvalidate_partitions(disk, bdev);\r\n}\r\nif (ret)\r\ngoto out_clear;\r\n} else {\r\nstruct block_device *whole;\r\nwhole = bdget_disk(disk, 0);\r\nret = -ENOMEM;\r\nif (!whole)\r\ngoto out_clear;\r\nBUG_ON(for_part);\r\nret = __blkdev_get(whole, mode, 1);\r\nif (ret)\r\ngoto out_clear;\r\nbdev->bd_contains = whole;\r\nbdev_inode_switch_bdi(bdev->bd_inode,\r\nwhole->bd_inode->i_data.backing_dev_info);\r\nbdev->bd_part = disk_get_part(disk, partno);\r\nif (!(disk->flags & GENHD_FL_UP) ||\r\n!bdev->bd_part || !bdev->bd_part->nr_sects) {\r\nret = -ENXIO;\r\ngoto out_clear;\r\n}\r\nbd_set_size(bdev, (loff_t)bdev->bd_part->nr_sects << 9);\r\n}\r\n} else {\r\nif (bdev->bd_contains == bdev) {\r\nret = 0;\r\nif (bdev->bd_disk->fops->open)\r\nret = bdev->bd_disk->fops->open(bdev, mode);\r\nif (bdev->bd_invalidated) {\r\nif (!ret)\r\nrescan_partitions(bdev->bd_disk, bdev);\r\nelse if (ret == -ENOMEDIUM)\r\ninvalidate_partitions(bdev->bd_disk, bdev);\r\n}\r\nif (ret)\r\ngoto out_unlock_bdev;\r\n}\r\nput_disk(disk);\r\nmodule_put(owner);\r\n}\r\nbdev->bd_openers++;\r\nif (for_part)\r\nbdev->bd_part_count++;\r\nmutex_unlock(&bdev->bd_mutex);\r\ndisk_unblock_events(disk);\r\nreturn 0;\r\nout_clear:\r\ndisk_put_part(bdev->bd_part);\r\nbdev->bd_disk = NULL;\r\nbdev->bd_part = NULL;\r\nbdev->bd_queue = NULL;\r\nbdev_inode_switch_bdi(bdev->bd_inode, &default_backing_dev_info);\r\nif (bdev != bdev->bd_contains)\r\n__blkdev_put(bdev->bd_contains, mode, 1);\r\nbdev->bd_contains = NULL;\r\nout_unlock_bdev:\r\nmutex_unlock(&bdev->bd_mutex);\r\ndisk_unblock_events(disk);\r\nput_disk(disk);\r\nmodule_put(owner);\r\nout:\r\nbdput(bdev);\r\nreturn ret;\r\n}\r\nint blkdev_get(struct block_device *bdev, fmode_t mode, void *holder)\r\n{\r\nstruct block_device *whole = NULL;\r\nint res;\r\nWARN_ON_ONCE((mode & FMODE_EXCL) && !holder);\r\nif ((mode & FMODE_EXCL) && holder) {\r\nwhole = bd_start_claiming(bdev, holder);\r\nif (IS_ERR(whole)) {\r\nbdput(bdev);\r\nreturn PTR_ERR(whole);\r\n}\r\n}\r\nres = __blkdev_get(bdev, mode, 0);\r\nif (whole) {\r\nstruct gendisk *disk = whole->bd_disk;\r\nmutex_lock(&bdev->bd_mutex);\r\nspin_lock(&bdev_lock);\r\nif (!res) {\r\nBUG_ON(!bd_may_claim(bdev, whole, holder));\r\nwhole->bd_holders++;\r\nwhole->bd_holder = bd_may_claim;\r\nbdev->bd_holders++;\r\nbdev->bd_holder = holder;\r\n}\r\nBUG_ON(whole->bd_claiming != holder);\r\nwhole->bd_claiming = NULL;\r\nwake_up_bit(&whole->bd_claiming, 0);\r\nspin_unlock(&bdev_lock);\r\nif (!res && (mode & FMODE_WRITE) && !bdev->bd_write_holder &&\r\n(disk->flags & GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE)) {\r\nbdev->bd_write_holder = true;\r\ndisk_block_events(disk);\r\n}\r\nmutex_unlock(&bdev->bd_mutex);\r\nbdput(whole);\r\n}\r\nreturn res;\r\n}\r\nstruct block_device *blkdev_get_by_path(const char *path, fmode_t mode,\r\nvoid *holder)\r\n{\r\nstruct block_device *bdev;\r\nint err;\r\nbdev = lookup_bdev(path);\r\nif (IS_ERR(bdev))\r\nreturn bdev;\r\nerr = blkdev_get(bdev, mode, holder);\r\nif (err)\r\nreturn ERR_PTR(err);\r\nif ((mode & FMODE_WRITE) && bdev_read_only(bdev)) {\r\nblkdev_put(bdev, mode);\r\nreturn ERR_PTR(-EACCES);\r\n}\r\nreturn bdev;\r\n}\r\nstruct block_device *blkdev_get_by_dev(dev_t dev, fmode_t mode, void *holder)\r\n{\r\nstruct block_device *bdev;\r\nint err;\r\nbdev = bdget(dev);\r\nif (!bdev)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = blkdev_get(bdev, mode, holder);\r\nif (err)\r\nreturn ERR_PTR(err);\r\nreturn bdev;\r\n}\r\nstatic int blkdev_open(struct inode * inode, struct file * filp)\r\n{\r\nstruct block_device *bdev;\r\nfilp->f_flags |= O_LARGEFILE;\r\nif (filp->f_flags & O_NDELAY)\r\nfilp->f_mode |= FMODE_NDELAY;\r\nif (filp->f_flags & O_EXCL)\r\nfilp->f_mode |= FMODE_EXCL;\r\nif ((filp->f_flags & O_ACCMODE) == 3)\r\nfilp->f_mode |= FMODE_WRITE_IOCTL;\r\nbdev = bd_acquire(inode);\r\nif (bdev == NULL)\r\nreturn -ENOMEM;\r\nfilp->f_mapping = bdev->bd_inode->i_mapping;\r\nreturn blkdev_get(bdev, filp->f_mode, filp);\r\n}\r\nstatic void __blkdev_put(struct block_device *bdev, fmode_t mode, int for_part)\r\n{\r\nstruct gendisk *disk = bdev->bd_disk;\r\nstruct block_device *victim = NULL;\r\nmutex_lock_nested(&bdev->bd_mutex, for_part);\r\nif (for_part)\r\nbdev->bd_part_count--;\r\nif (!--bdev->bd_openers) {\r\nWARN_ON_ONCE(bdev->bd_holders);\r\nsync_blockdev(bdev);\r\nkill_bdev(bdev);\r\nbdev_inode_switch_bdi(bdev->bd_inode,\r\n&default_backing_dev_info);\r\n}\r\nif (bdev->bd_contains == bdev) {\r\nif (disk->fops->release)\r\ndisk->fops->release(disk, mode);\r\n}\r\nif (!bdev->bd_openers) {\r\nstruct module *owner = disk->fops->owner;\r\ndisk_put_part(bdev->bd_part);\r\nbdev->bd_part = NULL;\r\nbdev->bd_disk = NULL;\r\nif (bdev != bdev->bd_contains)\r\nvictim = bdev->bd_contains;\r\nbdev->bd_contains = NULL;\r\nput_disk(disk);\r\nmodule_put(owner);\r\n}\r\nmutex_unlock(&bdev->bd_mutex);\r\nbdput(bdev);\r\nif (victim)\r\n__blkdev_put(victim, mode, 1);\r\n}\r\nvoid blkdev_put(struct block_device *bdev, fmode_t mode)\r\n{\r\nmutex_lock(&bdev->bd_mutex);\r\nif (mode & FMODE_EXCL) {\r\nbool bdev_free;\r\nspin_lock(&bdev_lock);\r\nWARN_ON_ONCE(--bdev->bd_holders < 0);\r\nWARN_ON_ONCE(--bdev->bd_contains->bd_holders < 0);\r\nif ((bdev_free = !bdev->bd_holders))\r\nbdev->bd_holder = NULL;\r\nif (!bdev->bd_contains->bd_holders)\r\nbdev->bd_contains->bd_holder = NULL;\r\nspin_unlock(&bdev_lock);\r\nif (bdev_free && bdev->bd_write_holder) {\r\ndisk_unblock_events(bdev->bd_disk);\r\nbdev->bd_write_holder = false;\r\n}\r\n}\r\ndisk_flush_events(bdev->bd_disk, DISK_EVENT_MEDIA_CHANGE);\r\nmutex_unlock(&bdev->bd_mutex);\r\n__blkdev_put(bdev, mode, 0);\r\n}\r\nstatic int blkdev_close(struct inode * inode, struct file * filp)\r\n{\r\nstruct block_device *bdev = I_BDEV(filp->f_mapping->host);\r\nblkdev_put(bdev, filp->f_mode);\r\nreturn 0;\r\n}\r\nstatic long block_ioctl(struct file *file, unsigned cmd, unsigned long arg)\r\n{\r\nstruct block_device *bdev = I_BDEV(file->f_mapping->host);\r\nfmode_t mode = file->f_mode;\r\nif (file->f_flags & O_NDELAY)\r\nmode |= FMODE_NDELAY;\r\nelse\r\nmode &= ~FMODE_NDELAY;\r\nreturn blkdev_ioctl(bdev, mode, cmd, arg);\r\n}\r\nssize_t blkdev_aio_write(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long nr_segs, loff_t pos)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct blk_plug plug;\r\nssize_t ret;\r\nBUG_ON(iocb->ki_pos != pos);\r\nblk_start_plug(&plug);\r\nret = __generic_file_aio_write(iocb, iov, nr_segs, &iocb->ki_pos);\r\nif (ret > 0) {\r\nssize_t err;\r\nerr = generic_write_sync(file, pos, ret);\r\nif (err < 0 && ret > 0)\r\nret = err;\r\n}\r\nblk_finish_plug(&plug);\r\nreturn ret;\r\n}\r\nstatic ssize_t blkdev_aio_read(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long nr_segs, loff_t pos)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct inode *bd_inode = file->f_mapping->host;\r\nloff_t size = i_size_read(bd_inode);\r\nif (pos >= size)\r\nreturn 0;\r\nsize -= pos;\r\nif (size < iocb->ki_nbytes)\r\nnr_segs = iov_shorten((struct iovec *)iov, nr_segs, size);\r\nreturn generic_file_aio_read(iocb, iov, nr_segs, pos);\r\n}\r\nstatic int blkdev_releasepage(struct page *page, gfp_t wait)\r\n{\r\nstruct super_block *super = BDEV_I(page->mapping->host)->bdev.bd_super;\r\nif (super && super->s_op->bdev_try_to_free_page)\r\nreturn super->s_op->bdev_try_to_free_page(super, page, wait);\r\nreturn try_to_free_buffers(page);\r\n}\r\nint ioctl_by_bdev(struct block_device *bdev, unsigned cmd, unsigned long arg)\r\n{\r\nint res;\r\nmm_segment_t old_fs = get_fs();\r\nset_fs(KERNEL_DS);\r\nres = blkdev_ioctl(bdev, 0, cmd, arg);\r\nset_fs(old_fs);\r\nreturn res;\r\n}\r\nstruct block_device *lookup_bdev(const char *pathname)\r\n{\r\nstruct block_device *bdev;\r\nstruct inode *inode;\r\nstruct path path;\r\nint error;\r\nif (!pathname || !*pathname)\r\nreturn ERR_PTR(-EINVAL);\r\nerror = kern_path(pathname, LOOKUP_FOLLOW, &path);\r\nif (error)\r\nreturn ERR_PTR(error);\r\ninode = path.dentry->d_inode;\r\nerror = -ENOTBLK;\r\nif (!S_ISBLK(inode->i_mode))\r\ngoto fail;\r\nerror = -EACCES;\r\nif (path.mnt->mnt_flags & MNT_NODEV)\r\ngoto fail;\r\nerror = -ENOMEM;\r\nbdev = bd_acquire(inode);\r\nif (!bdev)\r\ngoto fail;\r\nout:\r\npath_put(&path);\r\nreturn bdev;\r\nfail:\r\nbdev = ERR_PTR(error);\r\ngoto out;\r\n}\r\nint __invalidate_device(struct block_device *bdev, bool kill_dirty)\r\n{\r\nstruct super_block *sb = get_super(bdev);\r\nint res = 0;\r\nif (sb) {\r\nshrink_dcache_sb(sb);\r\nres = invalidate_inodes(sb, kill_dirty);\r\ndrop_super(sb);\r\n}\r\ninvalidate_bdev(bdev);\r\nreturn res;\r\n}\r\nvoid iterate_bdevs(void (*func)(struct block_device *, void *), void *arg)\r\n{\r\nstruct inode *inode, *old_inode = NULL;\r\nspin_lock(&inode_sb_list_lock);\r\nlist_for_each_entry(inode, &blockdev_superblock->s_inodes, i_sb_list) {\r\nstruct address_space *mapping = inode->i_mapping;\r\nspin_lock(&inode->i_lock);\r\nif (inode->i_state & (I_FREEING|I_WILL_FREE|I_NEW) ||\r\nmapping->nrpages == 0) {\r\nspin_unlock(&inode->i_lock);\r\ncontinue;\r\n}\r\n__iget(inode);\r\nspin_unlock(&inode->i_lock);\r\nspin_unlock(&inode_sb_list_lock);\r\niput(old_inode);\r\nold_inode = inode;\r\nfunc(I_BDEV(inode), arg);\r\nspin_lock(&inode_sb_list_lock);\r\n}\r\nspin_unlock(&inode_sb_list_lock);\r\niput(old_inode);\r\n}
