static bool global_reclaim(struct scan_control *sc)\r\n{\r\nreturn !sc->target_mem_cgroup;\r\n}\r\nstatic bool global_reclaim(struct scan_control *sc)\r\n{\r\nreturn true;\r\n}\r\nunsigned long zone_reclaimable_pages(struct zone *zone)\r\n{\r\nint nr;\r\nnr = zone_page_state(zone, NR_ACTIVE_FILE) +\r\nzone_page_state(zone, NR_INACTIVE_FILE);\r\nif (get_nr_swap_pages() > 0)\r\nnr += zone_page_state(zone, NR_ACTIVE_ANON) +\r\nzone_page_state(zone, NR_INACTIVE_ANON);\r\nreturn nr;\r\n}\r\nbool zone_reclaimable(struct zone *zone)\r\n{\r\nreturn zone->pages_scanned < zone_reclaimable_pages(zone) * 6;\r\n}\r\nstatic unsigned long get_lru_size(struct lruvec *lruvec, enum lru_list lru)\r\n{\r\nif (!mem_cgroup_disabled())\r\nreturn mem_cgroup_get_lru_size(lruvec, lru);\r\nreturn zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru);\r\n}\r\nint register_shrinker(struct shrinker *shrinker)\r\n{\r\nsize_t size = sizeof(*shrinker->nr_deferred);\r\nif (nr_node_ids == 1)\r\nshrinker->flags &= ~SHRINKER_NUMA_AWARE;\r\nif (shrinker->flags & SHRINKER_NUMA_AWARE)\r\nsize *= nr_node_ids;\r\nshrinker->nr_deferred = kzalloc(size, GFP_KERNEL);\r\nif (!shrinker->nr_deferred)\r\nreturn -ENOMEM;\r\ndown_write(&shrinker_rwsem);\r\nlist_add_tail(&shrinker->list, &shrinker_list);\r\nup_write(&shrinker_rwsem);\r\nreturn 0;\r\n}\r\nvoid unregister_shrinker(struct shrinker *shrinker)\r\n{\r\ndown_write(&shrinker_rwsem);\r\nlist_del(&shrinker->list);\r\nup_write(&shrinker_rwsem);\r\nkfree(shrinker->nr_deferred);\r\n}\r\nstatic unsigned long\r\nshrink_slab_node(struct shrink_control *shrinkctl, struct shrinker *shrinker,\r\nunsigned long nr_pages_scanned, unsigned long lru_pages)\r\n{\r\nunsigned long freed = 0;\r\nunsigned long long delta;\r\nlong total_scan;\r\nlong max_pass;\r\nlong nr;\r\nlong new_nr;\r\nint nid = shrinkctl->nid;\r\nlong batch_size = shrinker->batch ? shrinker->batch\r\n: SHRINK_BATCH;\r\nmax_pass = shrinker->count_objects(shrinker, shrinkctl);\r\nif (max_pass == 0)\r\nreturn 0;\r\nnr = atomic_long_xchg(&shrinker->nr_deferred[nid], 0);\r\ntotal_scan = nr;\r\ndelta = (4 * nr_pages_scanned) / shrinker->seeks;\r\ndelta *= max_pass;\r\ndo_div(delta, lru_pages + 1);\r\ntotal_scan += delta;\r\nif (total_scan < 0) {\r\nprintk(KERN_ERR\r\n"shrink_slab: %pF negative objects to delete nr=%ld\n",\r\nshrinker->scan_objects, total_scan);\r\ntotal_scan = max_pass;\r\n}\r\nif (delta < max_pass / 4)\r\ntotal_scan = min(total_scan, max_pass / 2);\r\nif (total_scan > max_pass * 2)\r\ntotal_scan = max_pass * 2;\r\ntrace_mm_shrink_slab_start(shrinker, shrinkctl, nr,\r\nnr_pages_scanned, lru_pages,\r\nmax_pass, delta, total_scan);\r\nwhile (total_scan >= batch_size) {\r\nunsigned long ret;\r\nshrinkctl->nr_to_scan = batch_size;\r\nret = shrinker->scan_objects(shrinker, shrinkctl);\r\nif (ret == SHRINK_STOP)\r\nbreak;\r\nfreed += ret;\r\ncount_vm_events(SLABS_SCANNED, batch_size);\r\ntotal_scan -= batch_size;\r\ncond_resched();\r\n}\r\nif (total_scan > 0)\r\nnew_nr = atomic_long_add_return(total_scan,\r\n&shrinker->nr_deferred[nid]);\r\nelse\r\nnew_nr = atomic_long_read(&shrinker->nr_deferred[nid]);\r\ntrace_mm_shrink_slab_end(shrinker, freed, nr, new_nr);\r\nreturn freed;\r\n}\r\nunsigned long shrink_slab(struct shrink_control *shrinkctl,\r\nunsigned long nr_pages_scanned,\r\nunsigned long lru_pages)\r\n{\r\nstruct shrinker *shrinker;\r\nunsigned long freed = 0;\r\nif (nr_pages_scanned == 0)\r\nnr_pages_scanned = SWAP_CLUSTER_MAX;\r\nif (!down_read_trylock(&shrinker_rwsem)) {\r\nfreed = 1;\r\ngoto out;\r\n}\r\nlist_for_each_entry(shrinker, &shrinker_list, list) {\r\nfor_each_node_mask(shrinkctl->nid, shrinkctl->nodes_to_scan) {\r\nif (!node_online(shrinkctl->nid))\r\ncontinue;\r\nif (!(shrinker->flags & SHRINKER_NUMA_AWARE) &&\r\n(shrinkctl->nid != 0))\r\nbreak;\r\nfreed += shrink_slab_node(shrinkctl, shrinker,\r\nnr_pages_scanned, lru_pages);\r\n}\r\n}\r\nup_read(&shrinker_rwsem);\r\nout:\r\ncond_resched();\r\nreturn freed;\r\n}\r\nstatic inline int is_page_cache_freeable(struct page *page)\r\n{\r\nreturn page_count(page) - page_has_private(page) == 2;\r\n}\r\nstatic int may_write_to_queue(struct backing_dev_info *bdi,\r\nstruct scan_control *sc)\r\n{\r\nif (current->flags & PF_SWAPWRITE)\r\nreturn 1;\r\nif (!bdi_write_congested(bdi))\r\nreturn 1;\r\nif (bdi == current->backing_dev_info)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void handle_write_error(struct address_space *mapping,\r\nstruct page *page, int error)\r\n{\r\nlock_page(page);\r\nif (page_mapping(page) == mapping)\r\nmapping_set_error(mapping, error);\r\nunlock_page(page);\r\n}\r\nstatic pageout_t pageout(struct page *page, struct address_space *mapping,\r\nstruct scan_control *sc)\r\n{\r\nif (!is_page_cache_freeable(page))\r\nreturn PAGE_KEEP;\r\nif (!mapping) {\r\nif (page_has_private(page)) {\r\nif (try_to_free_buffers(page)) {\r\nClearPageDirty(page);\r\nprintk("%s: orphaned page\n", __func__);\r\nreturn PAGE_CLEAN;\r\n}\r\n}\r\nreturn PAGE_KEEP;\r\n}\r\nif (mapping->a_ops->writepage == NULL)\r\nreturn PAGE_ACTIVATE;\r\nif (!may_write_to_queue(mapping->backing_dev_info, sc))\r\nreturn PAGE_KEEP;\r\nif (clear_page_dirty_for_io(page)) {\r\nint res;\r\nstruct writeback_control wbc = {\r\n.sync_mode = WB_SYNC_NONE,\r\n.nr_to_write = SWAP_CLUSTER_MAX,\r\n.range_start = 0,\r\n.range_end = LLONG_MAX,\r\n.for_reclaim = 1,\r\n};\r\nSetPageReclaim(page);\r\nres = mapping->a_ops->writepage(page, &wbc);\r\nif (res < 0)\r\nhandle_write_error(mapping, page, res);\r\nif (res == AOP_WRITEPAGE_ACTIVATE) {\r\nClearPageReclaim(page);\r\nreturn PAGE_ACTIVATE;\r\n}\r\nif (!PageWriteback(page)) {\r\nClearPageReclaim(page);\r\n}\r\ntrace_mm_vmscan_writepage(page, trace_reclaim_flags(page));\r\ninc_zone_page_state(page, NR_VMSCAN_WRITE);\r\nreturn PAGE_SUCCESS;\r\n}\r\nreturn PAGE_CLEAN;\r\n}\r\nstatic int __remove_mapping(struct address_space *mapping, struct page *page)\r\n{\r\nBUG_ON(!PageLocked(page));\r\nBUG_ON(mapping != page_mapping(page));\r\nspin_lock_irq(&mapping->tree_lock);\r\nif (!page_freeze_refs(page, 2))\r\ngoto cannot_free;\r\nif (unlikely(PageDirty(page))) {\r\npage_unfreeze_refs(page, 2);\r\ngoto cannot_free;\r\n}\r\nif (PageSwapCache(page)) {\r\nswp_entry_t swap = { .val = page_private(page) };\r\n__delete_from_swap_cache(page);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nswapcache_free(swap, page);\r\n} else {\r\nvoid (*freepage)(struct page *);\r\nfreepage = mapping->a_ops->freepage;\r\n__delete_from_page_cache(page);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nmem_cgroup_uncharge_cache_page(page);\r\nif (freepage != NULL)\r\nfreepage(page);\r\n}\r\nreturn 1;\r\ncannot_free:\r\nspin_unlock_irq(&mapping->tree_lock);\r\nreturn 0;\r\n}\r\nint remove_mapping(struct address_space *mapping, struct page *page)\r\n{\r\nif (__remove_mapping(mapping, page)) {\r\npage_unfreeze_refs(page, 1);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid putback_lru_page(struct page *page)\r\n{\r\nbool is_unevictable;\r\nint was_unevictable = PageUnevictable(page);\r\nVM_BUG_ON(PageLRU(page));\r\nredo:\r\nClearPageUnevictable(page);\r\nif (page_evictable(page)) {\r\nis_unevictable = false;\r\nlru_cache_add(page);\r\n} else {\r\nis_unevictable = true;\r\nadd_page_to_unevictable_list(page);\r\nsmp_mb();\r\n}\r\nif (is_unevictable && page_evictable(page)) {\r\nif (!isolate_lru_page(page)) {\r\nput_page(page);\r\ngoto redo;\r\n}\r\n}\r\nif (was_unevictable && !is_unevictable)\r\ncount_vm_event(UNEVICTABLE_PGRESCUED);\r\nelse if (!was_unevictable && is_unevictable)\r\ncount_vm_event(UNEVICTABLE_PGCULLED);\r\nput_page(page);\r\n}\r\nstatic enum page_references page_check_references(struct page *page,\r\nstruct scan_control *sc)\r\n{\r\nint referenced_ptes, referenced_page;\r\nunsigned long vm_flags;\r\nreferenced_ptes = page_referenced(page, 1, sc->target_mem_cgroup,\r\n&vm_flags);\r\nreferenced_page = TestClearPageReferenced(page);\r\nif (vm_flags & VM_LOCKED)\r\nreturn PAGEREF_RECLAIM;\r\nif (referenced_ptes) {\r\nif (PageSwapBacked(page))\r\nreturn PAGEREF_ACTIVATE;\r\nSetPageReferenced(page);\r\nif (referenced_page || referenced_ptes > 1)\r\nreturn PAGEREF_ACTIVATE;\r\nif (vm_flags & VM_EXEC)\r\nreturn PAGEREF_ACTIVATE;\r\nreturn PAGEREF_KEEP;\r\n}\r\nif (referenced_page && !PageSwapBacked(page))\r\nreturn PAGEREF_RECLAIM_CLEAN;\r\nreturn PAGEREF_RECLAIM;\r\n}\r\nstatic void page_check_dirty_writeback(struct page *page,\r\nbool *dirty, bool *writeback)\r\n{\r\nstruct address_space *mapping;\r\nif (!page_is_file_cache(page)) {\r\n*dirty = false;\r\n*writeback = false;\r\nreturn;\r\n}\r\n*dirty = PageDirty(page);\r\n*writeback = PageWriteback(page);\r\nif (!page_has_private(page))\r\nreturn;\r\nmapping = page_mapping(page);\r\nif (mapping && mapping->a_ops->is_dirty_writeback)\r\nmapping->a_ops->is_dirty_writeback(page, dirty, writeback);\r\n}\r\nstatic unsigned long shrink_page_list(struct list_head *page_list,\r\nstruct zone *zone,\r\nstruct scan_control *sc,\r\nenum ttu_flags ttu_flags,\r\nunsigned long *ret_nr_dirty,\r\nunsigned long *ret_nr_unqueued_dirty,\r\nunsigned long *ret_nr_congested,\r\nunsigned long *ret_nr_writeback,\r\nunsigned long *ret_nr_immediate,\r\nbool force_reclaim)\r\n{\r\nLIST_HEAD(ret_pages);\r\nLIST_HEAD(free_pages);\r\nint pgactivate = 0;\r\nunsigned long nr_unqueued_dirty = 0;\r\nunsigned long nr_dirty = 0;\r\nunsigned long nr_congested = 0;\r\nunsigned long nr_reclaimed = 0;\r\nunsigned long nr_writeback = 0;\r\nunsigned long nr_immediate = 0;\r\ncond_resched();\r\nmem_cgroup_uncharge_start();\r\nwhile (!list_empty(page_list)) {\r\nstruct address_space *mapping;\r\nstruct page *page;\r\nint may_enter_fs;\r\nenum page_references references = PAGEREF_RECLAIM_CLEAN;\r\nbool dirty, writeback;\r\ncond_resched();\r\npage = lru_to_page(page_list);\r\nlist_del(&page->lru);\r\nif (!trylock_page(page))\r\ngoto keep;\r\nVM_BUG_ON(PageActive(page));\r\nVM_BUG_ON(page_zone(page) != zone);\r\nsc->nr_scanned++;\r\nif (unlikely(!page_evictable(page)))\r\ngoto cull_mlocked;\r\nif (!sc->may_unmap && page_mapped(page))\r\ngoto keep_locked;\r\nif (page_mapped(page) || PageSwapCache(page))\r\nsc->nr_scanned++;\r\nmay_enter_fs = (sc->gfp_mask & __GFP_FS) ||\r\n(PageSwapCache(page) && (sc->gfp_mask & __GFP_IO));\r\npage_check_dirty_writeback(page, &dirty, &writeback);\r\nif (dirty || writeback)\r\nnr_dirty++;\r\nif (dirty && !writeback)\r\nnr_unqueued_dirty++;\r\nmapping = page_mapping(page);\r\nif ((mapping && bdi_write_congested(mapping->backing_dev_info)) ||\r\n(writeback && PageReclaim(page)))\r\nnr_congested++;\r\nif (PageWriteback(page)) {\r\nif (current_is_kswapd() &&\r\nPageReclaim(page) &&\r\nzone_is_reclaim_writeback(zone)) {\r\nnr_immediate++;\r\ngoto keep_locked;\r\n} else if (global_reclaim(sc) ||\r\n!PageReclaim(page) || !(sc->gfp_mask & __GFP_IO)) {\r\nSetPageReclaim(page);\r\nnr_writeback++;\r\ngoto keep_locked;\r\n} else {\r\nwait_on_page_writeback(page);\r\n}\r\n}\r\nif (!force_reclaim)\r\nreferences = page_check_references(page, sc);\r\nswitch (references) {\r\ncase PAGEREF_ACTIVATE:\r\ngoto activate_locked;\r\ncase PAGEREF_KEEP:\r\ngoto keep_locked;\r\ncase PAGEREF_RECLAIM:\r\ncase PAGEREF_RECLAIM_CLEAN:\r\n;\r\n}\r\nif (PageAnon(page) && !PageSwapCache(page)) {\r\nif (!(sc->gfp_mask & __GFP_IO))\r\ngoto keep_locked;\r\nif (!add_to_swap(page, page_list))\r\ngoto activate_locked;\r\nmay_enter_fs = 1;\r\nmapping = page_mapping(page);\r\n}\r\nif (page_mapped(page) && mapping) {\r\nswitch (try_to_unmap(page, ttu_flags)) {\r\ncase SWAP_FAIL:\r\ngoto activate_locked;\r\ncase SWAP_AGAIN:\r\ngoto keep_locked;\r\ncase SWAP_MLOCK:\r\ngoto cull_mlocked;\r\ncase SWAP_SUCCESS:\r\n;\r\n}\r\n}\r\nif (PageDirty(page)) {\r\nif (page_is_file_cache(page) &&\r\n(!current_is_kswapd() ||\r\n!zone_is_reclaim_dirty(zone))) {\r\ninc_zone_page_state(page, NR_VMSCAN_IMMEDIATE);\r\nSetPageReclaim(page);\r\ngoto keep_locked;\r\n}\r\nif (references == PAGEREF_RECLAIM_CLEAN)\r\ngoto keep_locked;\r\nif (!may_enter_fs)\r\ngoto keep_locked;\r\nif (!sc->may_writepage)\r\ngoto keep_locked;\r\nswitch (pageout(page, mapping, sc)) {\r\ncase PAGE_KEEP:\r\ngoto keep_locked;\r\ncase PAGE_ACTIVATE:\r\ngoto activate_locked;\r\ncase PAGE_SUCCESS:\r\nif (PageWriteback(page))\r\ngoto keep;\r\nif (PageDirty(page))\r\ngoto keep;\r\nif (!trylock_page(page))\r\ngoto keep;\r\nif (PageDirty(page) || PageWriteback(page))\r\ngoto keep_locked;\r\nmapping = page_mapping(page);\r\ncase PAGE_CLEAN:\r\n;\r\n}\r\n}\r\nif (page_has_private(page)) {\r\nif (!try_to_release_page(page, sc->gfp_mask))\r\ngoto activate_locked;\r\nif (!mapping && page_count(page) == 1) {\r\nunlock_page(page);\r\nif (put_page_testzero(page))\r\ngoto free_it;\r\nelse {\r\nnr_reclaimed++;\r\ncontinue;\r\n}\r\n}\r\n}\r\nif (!mapping || !__remove_mapping(mapping, page))\r\ngoto keep_locked;\r\n__clear_page_locked(page);\r\nfree_it:\r\nnr_reclaimed++;\r\nlist_add(&page->lru, &free_pages);\r\ncontinue;\r\ncull_mlocked:\r\nif (PageSwapCache(page))\r\ntry_to_free_swap(page);\r\nunlock_page(page);\r\nputback_lru_page(page);\r\ncontinue;\r\nactivate_locked:\r\nif (PageSwapCache(page) && vm_swap_full())\r\ntry_to_free_swap(page);\r\nVM_BUG_ON(PageActive(page));\r\nSetPageActive(page);\r\npgactivate++;\r\nkeep_locked:\r\nunlock_page(page);\r\nkeep:\r\nlist_add(&page->lru, &ret_pages);\r\nVM_BUG_ON(PageLRU(page) || PageUnevictable(page));\r\n}\r\nfree_hot_cold_page_list(&free_pages, 1);\r\nlist_splice(&ret_pages, page_list);\r\ncount_vm_events(PGACTIVATE, pgactivate);\r\nmem_cgroup_uncharge_end();\r\n*ret_nr_dirty += nr_dirty;\r\n*ret_nr_congested += nr_congested;\r\n*ret_nr_unqueued_dirty += nr_unqueued_dirty;\r\n*ret_nr_writeback += nr_writeback;\r\n*ret_nr_immediate += nr_immediate;\r\nreturn nr_reclaimed;\r\n}\r\nunsigned long reclaim_clean_pages_from_list(struct zone *zone,\r\nstruct list_head *page_list)\r\n{\r\nstruct scan_control sc = {\r\n.gfp_mask = GFP_KERNEL,\r\n.priority = DEF_PRIORITY,\r\n.may_unmap = 1,\r\n};\r\nunsigned long ret, dummy1, dummy2, dummy3, dummy4, dummy5;\r\nstruct page *page, *next;\r\nLIST_HEAD(clean_pages);\r\nlist_for_each_entry_safe(page, next, page_list, lru) {\r\nif (page_is_file_cache(page) && !PageDirty(page) &&\r\n!isolated_balloon_page(page)) {\r\nClearPageActive(page);\r\nlist_move(&page->lru, &clean_pages);\r\n}\r\n}\r\nret = shrink_page_list(&clean_pages, zone, &sc,\r\nTTU_UNMAP|TTU_IGNORE_ACCESS,\r\n&dummy1, &dummy2, &dummy3, &dummy4, &dummy5, true);\r\nlist_splice(&clean_pages, page_list);\r\n__mod_zone_page_state(zone, NR_ISOLATED_FILE, -ret);\r\nreturn ret;\r\n}\r\nint __isolate_lru_page(struct page *page, isolate_mode_t mode)\r\n{\r\nint ret = -EINVAL;\r\nif (!PageLRU(page))\r\nreturn ret;\r\nif (PageUnevictable(page) && !(mode & ISOLATE_UNEVICTABLE))\r\nreturn ret;\r\nret = -EBUSY;\r\nif (mode & (ISOLATE_CLEAN|ISOLATE_ASYNC_MIGRATE)) {\r\nif (PageWriteback(page))\r\nreturn ret;\r\nif (PageDirty(page)) {\r\nstruct address_space *mapping;\r\nif (mode & ISOLATE_CLEAN)\r\nreturn ret;\r\nmapping = page_mapping(page);\r\nif (mapping && !mapping->a_ops->migratepage)\r\nreturn ret;\r\n}\r\n}\r\nif ((mode & ISOLATE_UNMAPPED) && page_mapped(page))\r\nreturn ret;\r\nif (likely(get_page_unless_zero(page))) {\r\nClearPageLRU(page);\r\nret = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic unsigned long isolate_lru_pages(unsigned long nr_to_scan,\r\nstruct lruvec *lruvec, struct list_head *dst,\r\nunsigned long *nr_scanned, struct scan_control *sc,\r\nisolate_mode_t mode, enum lru_list lru)\r\n{\r\nstruct list_head *src = &lruvec->lists[lru];\r\nunsigned long nr_taken = 0;\r\nunsigned long scan;\r\nfor (scan = 0; scan < nr_to_scan && !list_empty(src); scan++) {\r\nstruct page *page;\r\nint nr_pages;\r\npage = lru_to_page(src);\r\nprefetchw_prev_lru_page(page, src, flags);\r\nVM_BUG_ON(!PageLRU(page));\r\nswitch (__isolate_lru_page(page, mode)) {\r\ncase 0:\r\nnr_pages = hpage_nr_pages(page);\r\nmem_cgroup_update_lru_size(lruvec, lru, -nr_pages);\r\nlist_move(&page->lru, dst);\r\nnr_taken += nr_pages;\r\nbreak;\r\ncase -EBUSY:\r\nlist_move(&page->lru, src);\r\ncontinue;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\n*nr_scanned = scan;\r\ntrace_mm_vmscan_lru_isolate(sc->order, nr_to_scan, scan,\r\nnr_taken, mode, is_file_lru(lru));\r\nreturn nr_taken;\r\n}\r\nint isolate_lru_page(struct page *page)\r\n{\r\nint ret = -EBUSY;\r\nVM_BUG_ON(!page_count(page));\r\nif (PageLRU(page)) {\r\nstruct zone *zone = page_zone(page);\r\nstruct lruvec *lruvec;\r\nspin_lock_irq(&zone->lru_lock);\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nif (PageLRU(page)) {\r\nint lru = page_lru(page);\r\nget_page(page);\r\nClearPageLRU(page);\r\ndel_page_from_lru_list(page, lruvec, lru);\r\nret = 0;\r\n}\r\nspin_unlock_irq(&zone->lru_lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic int too_many_isolated(struct zone *zone, int file,\r\nstruct scan_control *sc)\r\n{\r\nunsigned long inactive, isolated;\r\nif (current_is_kswapd())\r\nreturn 0;\r\nif (!global_reclaim(sc))\r\nreturn 0;\r\nif (file) {\r\ninactive = zone_page_state(zone, NR_INACTIVE_FILE);\r\nisolated = zone_page_state(zone, NR_ISOLATED_FILE);\r\n} else {\r\ninactive = zone_page_state(zone, NR_INACTIVE_ANON);\r\nisolated = zone_page_state(zone, NR_ISOLATED_ANON);\r\n}\r\nif ((sc->gfp_mask & GFP_IOFS) == GFP_IOFS)\r\ninactive >>= 3;\r\nreturn isolated > inactive;\r\n}\r\nstatic noinline_for_stack void\r\nputback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)\r\n{\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nLIST_HEAD(pages_to_free);\r\nwhile (!list_empty(page_list)) {\r\nstruct page *page = lru_to_page(page_list);\r\nint lru;\r\nVM_BUG_ON(PageLRU(page));\r\nlist_del(&page->lru);\r\nif (unlikely(!page_evictable(page))) {\r\nspin_unlock_irq(&zone->lru_lock);\r\nputback_lru_page(page);\r\nspin_lock_irq(&zone->lru_lock);\r\ncontinue;\r\n}\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nSetPageLRU(page);\r\nlru = page_lru(page);\r\nadd_page_to_lru_list(page, lruvec, lru);\r\nif (is_active_lru(lru)) {\r\nint file = is_file_lru(lru);\r\nint numpages = hpage_nr_pages(page);\r\nreclaim_stat->recent_rotated[file] += numpages;\r\n}\r\nif (put_page_testzero(page)) {\r\n__ClearPageLRU(page);\r\n__ClearPageActive(page);\r\ndel_page_from_lru_list(page, lruvec, lru);\r\nif (unlikely(PageCompound(page))) {\r\nspin_unlock_irq(&zone->lru_lock);\r\n(*get_compound_page_dtor(page))(page);\r\nspin_lock_irq(&zone->lru_lock);\r\n} else\r\nlist_add(&page->lru, &pages_to_free);\r\n}\r\n}\r\nlist_splice(&pages_to_free, page_list);\r\n}\r\nstatic noinline_for_stack unsigned long\r\nshrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,\r\nstruct scan_control *sc, enum lru_list lru)\r\n{\r\nLIST_HEAD(page_list);\r\nunsigned long nr_scanned;\r\nunsigned long nr_reclaimed = 0;\r\nunsigned long nr_taken;\r\nunsigned long nr_dirty = 0;\r\nunsigned long nr_congested = 0;\r\nunsigned long nr_unqueued_dirty = 0;\r\nunsigned long nr_writeback = 0;\r\nunsigned long nr_immediate = 0;\r\nisolate_mode_t isolate_mode = 0;\r\nint file = is_file_lru(lru);\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nwhile (unlikely(too_many_isolated(zone, file, sc))) {\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\nif (fatal_signal_pending(current))\r\nreturn SWAP_CLUSTER_MAX;\r\n}\r\nlru_add_drain();\r\nif (!sc->may_unmap)\r\nisolate_mode |= ISOLATE_UNMAPPED;\r\nif (!sc->may_writepage)\r\nisolate_mode |= ISOLATE_CLEAN;\r\nspin_lock_irq(&zone->lru_lock);\r\nnr_taken = isolate_lru_pages(nr_to_scan, lruvec, &page_list,\r\n&nr_scanned, sc, isolate_mode, lru);\r\n__mod_zone_page_state(zone, NR_LRU_BASE + lru, -nr_taken);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, nr_taken);\r\nif (global_reclaim(sc)) {\r\nzone->pages_scanned += nr_scanned;\r\nif (current_is_kswapd())\r\n__count_zone_vm_events(PGSCAN_KSWAPD, zone, nr_scanned);\r\nelse\r\n__count_zone_vm_events(PGSCAN_DIRECT, zone, nr_scanned);\r\n}\r\nspin_unlock_irq(&zone->lru_lock);\r\nif (nr_taken == 0)\r\nreturn 0;\r\nnr_reclaimed = shrink_page_list(&page_list, zone, sc, TTU_UNMAP,\r\n&nr_dirty, &nr_unqueued_dirty, &nr_congested,\r\n&nr_writeback, &nr_immediate,\r\nfalse);\r\nspin_lock_irq(&zone->lru_lock);\r\nreclaim_stat->recent_scanned[file] += nr_taken;\r\nif (global_reclaim(sc)) {\r\nif (current_is_kswapd())\r\n__count_zone_vm_events(PGSTEAL_KSWAPD, zone,\r\nnr_reclaimed);\r\nelse\r\n__count_zone_vm_events(PGSTEAL_DIRECT, zone,\r\nnr_reclaimed);\r\n}\r\nputback_inactive_pages(lruvec, &page_list);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, -nr_taken);\r\nspin_unlock_irq(&zone->lru_lock);\r\nfree_hot_cold_page_list(&page_list, 1);\r\nif (nr_writeback && nr_writeback == nr_taken)\r\nzone_set_flag(zone, ZONE_WRITEBACK);\r\nif (global_reclaim(sc)) {\r\nif (nr_dirty && nr_dirty == nr_congested)\r\nzone_set_flag(zone, ZONE_CONGESTED);\r\nif (nr_unqueued_dirty == nr_taken)\r\nzone_set_flag(zone, ZONE_TAIL_LRU_DIRTY);\r\nif (nr_unqueued_dirty == nr_taken || nr_immediate)\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\n}\r\nif (!sc->hibernation_mode && !current_is_kswapd())\r\nwait_iff_congested(zone, BLK_RW_ASYNC, HZ/10);\r\ntrace_mm_vmscan_lru_shrink_inactive(zone->zone_pgdat->node_id,\r\nzone_idx(zone),\r\nnr_scanned, nr_reclaimed,\r\nsc->priority,\r\ntrace_shrink_flags(file));\r\nreturn nr_reclaimed;\r\n}\r\nstatic void move_active_pages_to_lru(struct lruvec *lruvec,\r\nstruct list_head *list,\r\nstruct list_head *pages_to_free,\r\nenum lru_list lru)\r\n{\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nunsigned long pgmoved = 0;\r\nstruct page *page;\r\nint nr_pages;\r\nwhile (!list_empty(list)) {\r\npage = lru_to_page(list);\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nVM_BUG_ON(PageLRU(page));\r\nSetPageLRU(page);\r\nnr_pages = hpage_nr_pages(page);\r\nmem_cgroup_update_lru_size(lruvec, lru, nr_pages);\r\nlist_move(&page->lru, &lruvec->lists[lru]);\r\npgmoved += nr_pages;\r\nif (put_page_testzero(page)) {\r\n__ClearPageLRU(page);\r\n__ClearPageActive(page);\r\ndel_page_from_lru_list(page, lruvec, lru);\r\nif (unlikely(PageCompound(page))) {\r\nspin_unlock_irq(&zone->lru_lock);\r\n(*get_compound_page_dtor(page))(page);\r\nspin_lock_irq(&zone->lru_lock);\r\n} else\r\nlist_add(&page->lru, pages_to_free);\r\n}\r\n}\r\n__mod_zone_page_state(zone, NR_LRU_BASE + lru, pgmoved);\r\nif (!is_active_lru(lru))\r\n__count_vm_events(PGDEACTIVATE, pgmoved);\r\n}\r\nstatic void shrink_active_list(unsigned long nr_to_scan,\r\nstruct lruvec *lruvec,\r\nstruct scan_control *sc,\r\nenum lru_list lru)\r\n{\r\nunsigned long nr_taken;\r\nunsigned long nr_scanned;\r\nunsigned long vm_flags;\r\nLIST_HEAD(l_hold);\r\nLIST_HEAD(l_active);\r\nLIST_HEAD(l_inactive);\r\nstruct page *page;\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nunsigned long nr_rotated = 0;\r\nisolate_mode_t isolate_mode = 0;\r\nint file = is_file_lru(lru);\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nlru_add_drain();\r\nif (!sc->may_unmap)\r\nisolate_mode |= ISOLATE_UNMAPPED;\r\nif (!sc->may_writepage)\r\nisolate_mode |= ISOLATE_CLEAN;\r\nspin_lock_irq(&zone->lru_lock);\r\nnr_taken = isolate_lru_pages(nr_to_scan, lruvec, &l_hold,\r\n&nr_scanned, sc, isolate_mode, lru);\r\nif (global_reclaim(sc))\r\nzone->pages_scanned += nr_scanned;\r\nreclaim_stat->recent_scanned[file] += nr_taken;\r\n__count_zone_vm_events(PGREFILL, zone, nr_scanned);\r\n__mod_zone_page_state(zone, NR_LRU_BASE + lru, -nr_taken);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, nr_taken);\r\nspin_unlock_irq(&zone->lru_lock);\r\nwhile (!list_empty(&l_hold)) {\r\ncond_resched();\r\npage = lru_to_page(&l_hold);\r\nlist_del(&page->lru);\r\nif (unlikely(!page_evictable(page))) {\r\nputback_lru_page(page);\r\ncontinue;\r\n}\r\nif (unlikely(buffer_heads_over_limit)) {\r\nif (page_has_private(page) && trylock_page(page)) {\r\nif (page_has_private(page))\r\ntry_to_release_page(page, 0);\r\nunlock_page(page);\r\n}\r\n}\r\nif (page_referenced(page, 0, sc->target_mem_cgroup,\r\n&vm_flags)) {\r\nnr_rotated += hpage_nr_pages(page);\r\nif ((vm_flags & VM_EXEC) && page_is_file_cache(page)) {\r\nlist_add(&page->lru, &l_active);\r\ncontinue;\r\n}\r\n}\r\nClearPageActive(page);\r\nlist_add(&page->lru, &l_inactive);\r\n}\r\nspin_lock_irq(&zone->lru_lock);\r\nreclaim_stat->recent_rotated[file] += nr_rotated;\r\nmove_active_pages_to_lru(lruvec, &l_active, &l_hold, lru);\r\nmove_active_pages_to_lru(lruvec, &l_inactive, &l_hold, lru - LRU_ACTIVE);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, -nr_taken);\r\nspin_unlock_irq(&zone->lru_lock);\r\nfree_hot_cold_page_list(&l_hold, 1);\r\n}\r\nstatic int inactive_anon_is_low_global(struct zone *zone)\r\n{\r\nunsigned long active, inactive;\r\nactive = zone_page_state(zone, NR_ACTIVE_ANON);\r\ninactive = zone_page_state(zone, NR_INACTIVE_ANON);\r\nif (inactive * zone->inactive_ratio < active)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int inactive_anon_is_low(struct lruvec *lruvec)\r\n{\r\nif (!total_swap_pages)\r\nreturn 0;\r\nif (!mem_cgroup_disabled())\r\nreturn mem_cgroup_inactive_anon_is_low(lruvec);\r\nreturn inactive_anon_is_low_global(lruvec_zone(lruvec));\r\n}\r\nstatic inline int inactive_anon_is_low(struct lruvec *lruvec)\r\n{\r\nreturn 0;\r\n}\r\nstatic int inactive_file_is_low(struct lruvec *lruvec)\r\n{\r\nunsigned long inactive;\r\nunsigned long active;\r\ninactive = get_lru_size(lruvec, LRU_INACTIVE_FILE);\r\nactive = get_lru_size(lruvec, LRU_ACTIVE_FILE);\r\nreturn active > inactive;\r\n}\r\nstatic int inactive_list_is_low(struct lruvec *lruvec, enum lru_list lru)\r\n{\r\nif (is_file_lru(lru))\r\nreturn inactive_file_is_low(lruvec);\r\nelse\r\nreturn inactive_anon_is_low(lruvec);\r\n}\r\nstatic unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,\r\nstruct lruvec *lruvec, struct scan_control *sc)\r\n{\r\nif (is_active_lru(lru)) {\r\nif (inactive_list_is_low(lruvec, lru))\r\nshrink_active_list(nr_to_scan, lruvec, sc, lru);\r\nreturn 0;\r\n}\r\nreturn shrink_inactive_list(nr_to_scan, lruvec, sc, lru);\r\n}\r\nstatic int vmscan_swappiness(struct scan_control *sc)\r\n{\r\nif (global_reclaim(sc))\r\nreturn vm_swappiness;\r\nreturn mem_cgroup_swappiness(sc->target_mem_cgroup);\r\n}\r\nstatic void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,\r\nunsigned long *nr)\r\n{\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nu64 fraction[2];\r\nu64 denominator = 0;\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nunsigned long anon_prio, file_prio;\r\nenum scan_balance scan_balance;\r\nunsigned long anon, file, free;\r\nbool force_scan = false;\r\nunsigned long ap, fp;\r\nenum lru_list lru;\r\nif (current_is_kswapd() && !zone_reclaimable(zone))\r\nforce_scan = true;\r\nif (!global_reclaim(sc))\r\nforce_scan = true;\r\nif (!sc->may_swap || (get_nr_swap_pages() <= 0)) {\r\nscan_balance = SCAN_FILE;\r\ngoto out;\r\n}\r\nif (!global_reclaim(sc) && !vmscan_swappiness(sc)) {\r\nscan_balance = SCAN_FILE;\r\ngoto out;\r\n}\r\nif (!sc->priority && vmscan_swappiness(sc)) {\r\nscan_balance = SCAN_EQUAL;\r\ngoto out;\r\n}\r\nanon = get_lru_size(lruvec, LRU_ACTIVE_ANON) +\r\nget_lru_size(lruvec, LRU_INACTIVE_ANON);\r\nfile = get_lru_size(lruvec, LRU_ACTIVE_FILE) +\r\nget_lru_size(lruvec, LRU_INACTIVE_FILE);\r\nif (global_reclaim(sc)) {\r\nfree = zone_page_state(zone, NR_FREE_PAGES);\r\nif (unlikely(file + free <= high_wmark_pages(zone))) {\r\nscan_balance = SCAN_ANON;\r\ngoto out;\r\n}\r\n}\r\nif (!inactive_file_is_low(lruvec)) {\r\nscan_balance = SCAN_FILE;\r\ngoto out;\r\n}\r\nscan_balance = SCAN_FRACT;\r\nanon_prio = vmscan_swappiness(sc);\r\nfile_prio = 200 - anon_prio;\r\nspin_lock_irq(&zone->lru_lock);\r\nif (unlikely(reclaim_stat->recent_scanned[0] > anon / 4)) {\r\nreclaim_stat->recent_scanned[0] /= 2;\r\nreclaim_stat->recent_rotated[0] /= 2;\r\n}\r\nif (unlikely(reclaim_stat->recent_scanned[1] > file / 4)) {\r\nreclaim_stat->recent_scanned[1] /= 2;\r\nreclaim_stat->recent_rotated[1] /= 2;\r\n}\r\nap = anon_prio * (reclaim_stat->recent_scanned[0] + 1);\r\nap /= reclaim_stat->recent_rotated[0] + 1;\r\nfp = file_prio * (reclaim_stat->recent_scanned[1] + 1);\r\nfp /= reclaim_stat->recent_rotated[1] + 1;\r\nspin_unlock_irq(&zone->lru_lock);\r\nfraction[0] = ap;\r\nfraction[1] = fp;\r\ndenominator = ap + fp + 1;\r\nout:\r\nfor_each_evictable_lru(lru) {\r\nint file = is_file_lru(lru);\r\nunsigned long size;\r\nunsigned long scan;\r\nsize = get_lru_size(lruvec, lru);\r\nscan = size >> sc->priority;\r\nif (!scan && force_scan)\r\nscan = min(size, SWAP_CLUSTER_MAX);\r\nswitch (scan_balance) {\r\ncase SCAN_EQUAL:\r\nbreak;\r\ncase SCAN_FRACT:\r\nscan = div64_u64(scan * fraction[file], denominator);\r\nbreak;\r\ncase SCAN_FILE:\r\ncase SCAN_ANON:\r\nif ((scan_balance == SCAN_FILE) != file)\r\nscan = 0;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nnr[lru] = scan;\r\n}\r\n}\r\nstatic void shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)\r\n{\r\nunsigned long nr[NR_LRU_LISTS];\r\nunsigned long targets[NR_LRU_LISTS];\r\nunsigned long nr_to_scan;\r\nenum lru_list lru;\r\nunsigned long nr_reclaimed = 0;\r\nunsigned long nr_to_reclaim = sc->nr_to_reclaim;\r\nstruct blk_plug plug;\r\nbool scan_adjusted = false;\r\nget_scan_count(lruvec, sc, nr);\r\nmemcpy(targets, nr, sizeof(nr));\r\nblk_start_plug(&plug);\r\nwhile (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||\r\nnr[LRU_INACTIVE_FILE]) {\r\nunsigned long nr_anon, nr_file, percentage;\r\nunsigned long nr_scanned;\r\nfor_each_evictable_lru(lru) {\r\nif (nr[lru]) {\r\nnr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX);\r\nnr[lru] -= nr_to_scan;\r\nnr_reclaimed += shrink_list(lru, nr_to_scan,\r\nlruvec, sc);\r\n}\r\n}\r\nif (nr_reclaimed < nr_to_reclaim || scan_adjusted)\r\ncontinue;\r\nif (global_reclaim(sc) && !current_is_kswapd())\r\nbreak;\r\nnr_file = nr[LRU_INACTIVE_FILE] + nr[LRU_ACTIVE_FILE];\r\nnr_anon = nr[LRU_INACTIVE_ANON] + nr[LRU_ACTIVE_ANON];\r\nif (nr_file > nr_anon) {\r\nunsigned long scan_target = targets[LRU_INACTIVE_ANON] +\r\ntargets[LRU_ACTIVE_ANON] + 1;\r\nlru = LRU_BASE;\r\npercentage = nr_anon * 100 / scan_target;\r\n} else {\r\nunsigned long scan_target = targets[LRU_INACTIVE_FILE] +\r\ntargets[LRU_ACTIVE_FILE] + 1;\r\nlru = LRU_FILE;\r\npercentage = nr_file * 100 / scan_target;\r\n}\r\nnr[lru] = 0;\r\nnr[lru + LRU_ACTIVE] = 0;\r\nlru = (lru == LRU_FILE) ? LRU_BASE : LRU_FILE;\r\nnr_scanned = targets[lru] - nr[lru];\r\nnr[lru] = targets[lru] * (100 - percentage) / 100;\r\nnr[lru] -= min(nr[lru], nr_scanned);\r\nlru += LRU_ACTIVE;\r\nnr_scanned = targets[lru] - nr[lru];\r\nnr[lru] = targets[lru] * (100 - percentage) / 100;\r\nnr[lru] -= min(nr[lru], nr_scanned);\r\nscan_adjusted = true;\r\n}\r\nblk_finish_plug(&plug);\r\nsc->nr_reclaimed += nr_reclaimed;\r\nif (inactive_anon_is_low(lruvec))\r\nshrink_active_list(SWAP_CLUSTER_MAX, lruvec,\r\nsc, LRU_ACTIVE_ANON);\r\nthrottle_vm_writeout(sc->gfp_mask);\r\n}\r\nstatic bool in_reclaim_compaction(struct scan_control *sc)\r\n{\r\nif (IS_ENABLED(CONFIG_COMPACTION) && sc->order &&\r\n(sc->order > PAGE_ALLOC_COSTLY_ORDER ||\r\nsc->priority < DEF_PRIORITY - 2))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline bool should_continue_reclaim(struct zone *zone,\r\nunsigned long nr_reclaimed,\r\nunsigned long nr_scanned,\r\nstruct scan_control *sc)\r\n{\r\nunsigned long pages_for_compaction;\r\nunsigned long inactive_lru_pages;\r\nif (!in_reclaim_compaction(sc))\r\nreturn false;\r\nif (sc->gfp_mask & __GFP_REPEAT) {\r\nif (!nr_reclaimed && !nr_scanned)\r\nreturn false;\r\n} else {\r\nif (!nr_reclaimed)\r\nreturn false;\r\n}\r\npages_for_compaction = (2UL << sc->order);\r\ninactive_lru_pages = zone_page_state(zone, NR_INACTIVE_FILE);\r\nif (get_nr_swap_pages() > 0)\r\ninactive_lru_pages += zone_page_state(zone, NR_INACTIVE_ANON);\r\nif (sc->nr_reclaimed < pages_for_compaction &&\r\ninactive_lru_pages > pages_for_compaction)\r\nreturn true;\r\nswitch (compaction_suitable(zone, sc->order)) {\r\ncase COMPACT_PARTIAL:\r\ncase COMPACT_CONTINUE:\r\nreturn false;\r\ndefault:\r\nreturn true;\r\n}\r\n}\r\nstatic void shrink_zone(struct zone *zone, struct scan_control *sc)\r\n{\r\nunsigned long nr_reclaimed, nr_scanned;\r\ndo {\r\nstruct mem_cgroup *root = sc->target_mem_cgroup;\r\nstruct mem_cgroup_reclaim_cookie reclaim = {\r\n.zone = zone,\r\n.priority = sc->priority,\r\n};\r\nstruct mem_cgroup *memcg;\r\nnr_reclaimed = sc->nr_reclaimed;\r\nnr_scanned = sc->nr_scanned;\r\nmemcg = mem_cgroup_iter(root, NULL, &reclaim);\r\ndo {\r\nstruct lruvec *lruvec;\r\nlruvec = mem_cgroup_zone_lruvec(zone, memcg);\r\nshrink_lruvec(lruvec, sc);\r\nif (!global_reclaim(sc) &&\r\nsc->nr_reclaimed >= sc->nr_to_reclaim) {\r\nmem_cgroup_iter_break(root, memcg);\r\nbreak;\r\n}\r\nmemcg = mem_cgroup_iter(root, memcg, &reclaim);\r\n} while (memcg);\r\nvmpressure(sc->gfp_mask, sc->target_mem_cgroup,\r\nsc->nr_scanned - nr_scanned,\r\nsc->nr_reclaimed - nr_reclaimed);\r\n} while (should_continue_reclaim(zone, sc->nr_reclaimed - nr_reclaimed,\r\nsc->nr_scanned - nr_scanned, sc));\r\n}\r\nstatic inline bool compaction_ready(struct zone *zone, struct scan_control *sc)\r\n{\r\nunsigned long balance_gap, watermark;\r\nbool watermark_ok;\r\nif (sc->order <= PAGE_ALLOC_COSTLY_ORDER)\r\nreturn false;\r\nbalance_gap = min(low_wmark_pages(zone),\r\n(zone->managed_pages + KSWAPD_ZONE_BALANCE_GAP_RATIO-1) /\r\nKSWAPD_ZONE_BALANCE_GAP_RATIO);\r\nwatermark = high_wmark_pages(zone) + balance_gap + (2UL << sc->order);\r\nwatermark_ok = zone_watermark_ok_safe(zone, 0, watermark, 0, 0);\r\nif (compaction_deferred(zone, sc->order))\r\nreturn watermark_ok;\r\nif (!compaction_suitable(zone, sc->order))\r\nreturn false;\r\nreturn watermark_ok;\r\n}\r\nstatic bool shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\r\n{\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nunsigned long nr_soft_reclaimed;\r\nunsigned long nr_soft_scanned;\r\nbool aborted_reclaim = false;\r\nif (buffer_heads_over_limit)\r\nsc->gfp_mask |= __GFP_HIGHMEM;\r\nfor_each_zone_zonelist_nodemask(zone, z, zonelist,\r\ngfp_zone(sc->gfp_mask), sc->nodemask) {\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (global_reclaim(sc)) {\r\nif (!cpuset_zone_allowed_hardwall(zone, GFP_KERNEL))\r\ncontinue;\r\nif (sc->priority != DEF_PRIORITY &&\r\n!zone_reclaimable(zone))\r\ncontinue;\r\nif (IS_ENABLED(CONFIG_COMPACTION)) {\r\nif (compaction_ready(zone, sc)) {\r\naborted_reclaim = true;\r\ncontinue;\r\n}\r\n}\r\nnr_soft_scanned = 0;\r\nnr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,\r\nsc->order, sc->gfp_mask,\r\n&nr_soft_scanned);\r\nsc->nr_reclaimed += nr_soft_reclaimed;\r\nsc->nr_scanned += nr_soft_scanned;\r\n}\r\nshrink_zone(zone, sc);\r\n}\r\nreturn aborted_reclaim;\r\n}\r\nstatic bool all_unreclaimable(struct zonelist *zonelist,\r\nstruct scan_control *sc)\r\n{\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nfor_each_zone_zonelist_nodemask(zone, z, zonelist,\r\ngfp_zone(sc->gfp_mask), sc->nodemask) {\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (!cpuset_zone_allowed_hardwall(zone, GFP_KERNEL))\r\ncontinue;\r\nif (zone_reclaimable(zone))\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic unsigned long do_try_to_free_pages(struct zonelist *zonelist,\r\nstruct scan_control *sc,\r\nstruct shrink_control *shrink)\r\n{\r\nunsigned long total_scanned = 0;\r\nstruct reclaim_state *reclaim_state = current->reclaim_state;\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nunsigned long writeback_threshold;\r\nbool aborted_reclaim;\r\ndelayacct_freepages_start();\r\nif (global_reclaim(sc))\r\ncount_vm_event(ALLOCSTALL);\r\ndo {\r\nvmpressure_prio(sc->gfp_mask, sc->target_mem_cgroup,\r\nsc->priority);\r\nsc->nr_scanned = 0;\r\naborted_reclaim = shrink_zones(zonelist, sc);\r\nif (global_reclaim(sc)) {\r\nunsigned long lru_pages = 0;\r\nnodes_clear(shrink->nodes_to_scan);\r\nfor_each_zone_zonelist(zone, z, zonelist,\r\ngfp_zone(sc->gfp_mask)) {\r\nif (!cpuset_zone_allowed_hardwall(zone, GFP_KERNEL))\r\ncontinue;\r\nlru_pages += zone_reclaimable_pages(zone);\r\nnode_set(zone_to_nid(zone),\r\nshrink->nodes_to_scan);\r\n}\r\nshrink_slab(shrink, sc->nr_scanned, lru_pages);\r\nif (reclaim_state) {\r\nsc->nr_reclaimed += reclaim_state->reclaimed_slab;\r\nreclaim_state->reclaimed_slab = 0;\r\n}\r\n}\r\ntotal_scanned += sc->nr_scanned;\r\nif (sc->nr_reclaimed >= sc->nr_to_reclaim)\r\ngoto out;\r\nif (sc->priority < DEF_PRIORITY - 2)\r\nsc->may_writepage = 1;\r\nwriteback_threshold = sc->nr_to_reclaim + sc->nr_to_reclaim / 2;\r\nif (total_scanned > writeback_threshold) {\r\nwakeup_flusher_threads(laptop_mode ? 0 : total_scanned,\r\nWB_REASON_TRY_TO_FREE_PAGES);\r\nsc->may_writepage = 1;\r\n}\r\n} while (--sc->priority >= 0 && !aborted_reclaim);\r\nout:\r\ndelayacct_freepages_end();\r\nif (sc->nr_reclaimed)\r\nreturn sc->nr_reclaimed;\r\nif (oom_killer_disabled)\r\nreturn 0;\r\nif (aborted_reclaim)\r\nreturn 1;\r\nif (global_reclaim(sc) && !all_unreclaimable(zonelist, sc))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic bool pfmemalloc_watermark_ok(pg_data_t *pgdat)\r\n{\r\nstruct zone *zone;\r\nunsigned long pfmemalloc_reserve = 0;\r\nunsigned long free_pages = 0;\r\nint i;\r\nbool wmark_ok;\r\nfor (i = 0; i <= ZONE_NORMAL; i++) {\r\nzone = &pgdat->node_zones[i];\r\npfmemalloc_reserve += min_wmark_pages(zone);\r\nfree_pages += zone_page_state(zone, NR_FREE_PAGES);\r\n}\r\nwmark_ok = free_pages > pfmemalloc_reserve / 2;\r\nif (!wmark_ok && waitqueue_active(&pgdat->kswapd_wait)) {\r\npgdat->classzone_idx = min(pgdat->classzone_idx,\r\n(enum zone_type)ZONE_NORMAL);\r\nwake_up_interruptible(&pgdat->kswapd_wait);\r\n}\r\nreturn wmark_ok;\r\n}\r\nstatic bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,\r\nnodemask_t *nodemask)\r\n{\r\nstruct zone *zone;\r\nint high_zoneidx = gfp_zone(gfp_mask);\r\npg_data_t *pgdat;\r\nif (current->flags & PF_KTHREAD)\r\ngoto out;\r\nif (fatal_signal_pending(current))\r\ngoto out;\r\nfirst_zones_zonelist(zonelist, high_zoneidx, NULL, &zone);\r\npgdat = zone->zone_pgdat;\r\nif (pfmemalloc_watermark_ok(pgdat))\r\ngoto out;\r\ncount_vm_event(PGSCAN_DIRECT_THROTTLE);\r\nif (!(gfp_mask & __GFP_FS)) {\r\nwait_event_interruptible_timeout(pgdat->pfmemalloc_wait,\r\npfmemalloc_watermark_ok(pgdat), HZ);\r\ngoto check_pending;\r\n}\r\nwait_event_killable(zone->zone_pgdat->pfmemalloc_wait,\r\npfmemalloc_watermark_ok(pgdat));\r\ncheck_pending:\r\nif (fatal_signal_pending(current))\r\nreturn true;\r\nout:\r\nreturn false;\r\n}\r\nunsigned long try_to_free_pages(struct zonelist *zonelist, int order,\r\ngfp_t gfp_mask, nodemask_t *nodemask)\r\n{\r\nunsigned long nr_reclaimed;\r\nstruct scan_control sc = {\r\n.gfp_mask = (gfp_mask = memalloc_noio_flags(gfp_mask)),\r\n.may_writepage = !laptop_mode,\r\n.nr_to_reclaim = SWAP_CLUSTER_MAX,\r\n.may_unmap = 1,\r\n.may_swap = 1,\r\n.order = order,\r\n.priority = DEF_PRIORITY,\r\n.target_mem_cgroup = NULL,\r\n.nodemask = nodemask,\r\n};\r\nstruct shrink_control shrink = {\r\n.gfp_mask = sc.gfp_mask,\r\n};\r\nif (throttle_direct_reclaim(gfp_mask, zonelist, nodemask))\r\nreturn 1;\r\ntrace_mm_vmscan_direct_reclaim_begin(order,\r\nsc.may_writepage,\r\ngfp_mask);\r\nnr_reclaimed = do_try_to_free_pages(zonelist, &sc, &shrink);\r\ntrace_mm_vmscan_direct_reclaim_end(nr_reclaimed);\r\nreturn nr_reclaimed;\r\n}\r\nunsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,\r\ngfp_t gfp_mask, bool noswap,\r\nstruct zone *zone,\r\nunsigned long *nr_scanned)\r\n{\r\nstruct scan_control sc = {\r\n.nr_scanned = 0,\r\n.nr_to_reclaim = SWAP_CLUSTER_MAX,\r\n.may_writepage = !laptop_mode,\r\n.may_unmap = 1,\r\n.may_swap = !noswap,\r\n.order = 0,\r\n.priority = 0,\r\n.target_mem_cgroup = memcg,\r\n};\r\nstruct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);\r\nsc.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |\r\n(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK);\r\ntrace_mm_vmscan_memcg_softlimit_reclaim_begin(sc.order,\r\nsc.may_writepage,\r\nsc.gfp_mask);\r\nshrink_lruvec(lruvec, &sc);\r\ntrace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);\r\n*nr_scanned = sc.nr_scanned;\r\nreturn sc.nr_reclaimed;\r\n}\r\nunsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\r\ngfp_t gfp_mask,\r\nbool noswap)\r\n{\r\nstruct zonelist *zonelist;\r\nunsigned long nr_reclaimed;\r\nint nid;\r\nstruct scan_control sc = {\r\n.may_writepage = !laptop_mode,\r\n.may_unmap = 1,\r\n.may_swap = !noswap,\r\n.nr_to_reclaim = SWAP_CLUSTER_MAX,\r\n.order = 0,\r\n.priority = DEF_PRIORITY,\r\n.target_mem_cgroup = memcg,\r\n.nodemask = NULL,\r\n.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |\r\n(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK),\r\n};\r\nstruct shrink_control shrink = {\r\n.gfp_mask = sc.gfp_mask,\r\n};\r\nnid = mem_cgroup_select_victim_node(memcg);\r\nzonelist = NODE_DATA(nid)->node_zonelists;\r\ntrace_mm_vmscan_memcg_reclaim_begin(0,\r\nsc.may_writepage,\r\nsc.gfp_mask);\r\nnr_reclaimed = do_try_to_free_pages(zonelist, &sc, &shrink);\r\ntrace_mm_vmscan_memcg_reclaim_end(nr_reclaimed);\r\nreturn nr_reclaimed;\r\n}\r\nstatic void age_active_anon(struct zone *zone, struct scan_control *sc)\r\n{\r\nstruct mem_cgroup *memcg;\r\nif (!total_swap_pages)\r\nreturn;\r\nmemcg = mem_cgroup_iter(NULL, NULL, NULL);\r\ndo {\r\nstruct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);\r\nif (inactive_anon_is_low(lruvec))\r\nshrink_active_list(SWAP_CLUSTER_MAX, lruvec,\r\nsc, LRU_ACTIVE_ANON);\r\nmemcg = mem_cgroup_iter(NULL, memcg, NULL);\r\n} while (memcg);\r\n}\r\nstatic bool zone_balanced(struct zone *zone, int order,\r\nunsigned long balance_gap, int classzone_idx)\r\n{\r\nif (!zone_watermark_ok_safe(zone, order, high_wmark_pages(zone) +\r\nbalance_gap, classzone_idx, 0))\r\nreturn false;\r\nif (IS_ENABLED(CONFIG_COMPACTION) && order &&\r\n!compaction_suitable(zone, order))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool pgdat_balanced(pg_data_t *pgdat, int order, int classzone_idx)\r\n{\r\nunsigned long managed_pages = 0;\r\nunsigned long balanced_pages = 0;\r\nint i;\r\nfor (i = 0; i <= classzone_idx; i++) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nmanaged_pages += zone->managed_pages;\r\nif (!zone_reclaimable(zone)) {\r\nbalanced_pages += zone->managed_pages;\r\ncontinue;\r\n}\r\nif (zone_balanced(zone, order, 0, i))\r\nbalanced_pages += zone->managed_pages;\r\nelse if (!order)\r\nreturn false;\r\n}\r\nif (order)\r\nreturn balanced_pages >= (managed_pages >> 2);\r\nelse\r\nreturn true;\r\n}\r\nstatic bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, long remaining,\r\nint classzone_idx)\r\n{\r\nif (remaining)\r\nreturn false;\r\nif (waitqueue_active(&pgdat->pfmemalloc_wait)) {\r\nwake_up(&pgdat->pfmemalloc_wait);\r\nreturn false;\r\n}\r\nreturn pgdat_balanced(pgdat, order, classzone_idx);\r\n}\r\nstatic bool kswapd_shrink_zone(struct zone *zone,\r\nint classzone_idx,\r\nstruct scan_control *sc,\r\nunsigned long lru_pages,\r\nunsigned long *nr_attempted)\r\n{\r\nint testorder = sc->order;\r\nunsigned long balance_gap;\r\nstruct reclaim_state *reclaim_state = current->reclaim_state;\r\nstruct shrink_control shrink = {\r\n.gfp_mask = sc->gfp_mask,\r\n};\r\nbool lowmem_pressure;\r\nsc->nr_to_reclaim = max(SWAP_CLUSTER_MAX, high_wmark_pages(zone));\r\nif (IS_ENABLED(CONFIG_COMPACTION) && sc->order &&\r\ncompaction_suitable(zone, sc->order) !=\r\nCOMPACT_SKIPPED)\r\ntestorder = 0;\r\nbalance_gap = min(low_wmark_pages(zone),\r\n(zone->managed_pages + KSWAPD_ZONE_BALANCE_GAP_RATIO-1) /\r\nKSWAPD_ZONE_BALANCE_GAP_RATIO);\r\nlowmem_pressure = (buffer_heads_over_limit && is_highmem(zone));\r\nif (!lowmem_pressure && zone_balanced(zone, testorder,\r\nbalance_gap, classzone_idx))\r\nreturn true;\r\nshrink_zone(zone, sc);\r\nnodes_clear(shrink.nodes_to_scan);\r\nnode_set(zone_to_nid(zone), shrink.nodes_to_scan);\r\nreclaim_state->reclaimed_slab = 0;\r\nshrink_slab(&shrink, sc->nr_scanned, lru_pages);\r\nsc->nr_reclaimed += reclaim_state->reclaimed_slab;\r\n*nr_attempted += sc->nr_to_reclaim;\r\nzone_clear_flag(zone, ZONE_WRITEBACK);\r\nif (zone_reclaimable(zone) &&\r\nzone_balanced(zone, testorder, 0, classzone_idx)) {\r\nzone_clear_flag(zone, ZONE_CONGESTED);\r\nzone_clear_flag(zone, ZONE_TAIL_LRU_DIRTY);\r\n}\r\nreturn sc->nr_scanned >= sc->nr_to_reclaim;\r\n}\r\nstatic unsigned long balance_pgdat(pg_data_t *pgdat, int order,\r\nint *classzone_idx)\r\n{\r\nint i;\r\nint end_zone = 0;\r\nunsigned long nr_soft_reclaimed;\r\nunsigned long nr_soft_scanned;\r\nstruct scan_control sc = {\r\n.gfp_mask = GFP_KERNEL,\r\n.priority = DEF_PRIORITY,\r\n.may_unmap = 1,\r\n.may_swap = 1,\r\n.may_writepage = !laptop_mode,\r\n.order = order,\r\n.target_mem_cgroup = NULL,\r\n};\r\ncount_vm_event(PAGEOUTRUN);\r\ndo {\r\nunsigned long lru_pages = 0;\r\nunsigned long nr_attempted = 0;\r\nbool raise_priority = true;\r\nbool pgdat_needs_compaction = (order > 0);\r\nsc.nr_reclaimed = 0;\r\nfor (i = pgdat->nr_zones - 1; i >= 0; i--) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (sc.priority != DEF_PRIORITY &&\r\n!zone_reclaimable(zone))\r\ncontinue;\r\nage_active_anon(zone, &sc);\r\nif (buffer_heads_over_limit && is_highmem_idx(i)) {\r\nend_zone = i;\r\nbreak;\r\n}\r\nif (!zone_balanced(zone, order, 0, 0)) {\r\nend_zone = i;\r\nbreak;\r\n} else {\r\nzone_clear_flag(zone, ZONE_CONGESTED);\r\nzone_clear_flag(zone, ZONE_TAIL_LRU_DIRTY);\r\n}\r\n}\r\nif (i < 0)\r\ngoto out;\r\nfor (i = 0; i <= end_zone; i++) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nlru_pages += zone_reclaimable_pages(zone);\r\nif (pgdat_needs_compaction &&\r\nzone_watermark_ok(zone, order,\r\nlow_wmark_pages(zone),\r\n*classzone_idx, 0))\r\npgdat_needs_compaction = false;\r\n}\r\nif (sc.priority < DEF_PRIORITY - 2)\r\nsc.may_writepage = 1;\r\nfor (i = 0; i <= end_zone; i++) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (sc.priority != DEF_PRIORITY &&\r\n!zone_reclaimable(zone))\r\ncontinue;\r\nsc.nr_scanned = 0;\r\nnr_soft_scanned = 0;\r\nnr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,\r\norder, sc.gfp_mask,\r\n&nr_soft_scanned);\r\nsc.nr_reclaimed += nr_soft_reclaimed;\r\nif (kswapd_shrink_zone(zone, end_zone, &sc,\r\nlru_pages, &nr_attempted))\r\nraise_priority = false;\r\n}\r\nif (waitqueue_active(&pgdat->pfmemalloc_wait) &&\r\npfmemalloc_watermark_ok(pgdat))\r\nwake_up(&pgdat->pfmemalloc_wait);\r\nif (order && sc.nr_reclaimed >= 2UL << order)\r\norder = sc.order = 0;\r\nif (try_to_freeze() || kthread_should_stop())\r\nbreak;\r\nif (pgdat_needs_compaction && sc.nr_reclaimed > nr_attempted)\r\ncompact_pgdat(pgdat, order);\r\nif (raise_priority || !sc.nr_reclaimed)\r\nsc.priority--;\r\n} while (sc.priority >= 1 &&\r\n!pgdat_balanced(pgdat, order, *classzone_idx));\r\nout:\r\n*classzone_idx = end_zone;\r\nreturn order;\r\n}\r\nstatic void kswapd_try_to_sleep(pg_data_t *pgdat, int order, int classzone_idx)\r\n{\r\nlong remaining = 0;\r\nDEFINE_WAIT(wait);\r\nif (freezing(current) || kthread_should_stop())\r\nreturn;\r\nprepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);\r\nif (prepare_kswapd_sleep(pgdat, order, remaining, classzone_idx)) {\r\nremaining = schedule_timeout(HZ/10);\r\nfinish_wait(&pgdat->kswapd_wait, &wait);\r\nprepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);\r\n}\r\nif (prepare_kswapd_sleep(pgdat, order, remaining, classzone_idx)) {\r\ntrace_mm_vmscan_kswapd_sleep(pgdat->node_id);\r\nset_pgdat_percpu_threshold(pgdat, calculate_normal_threshold);\r\nreset_isolation_suitable(pgdat);\r\nif (!kthread_should_stop())\r\nschedule();\r\nset_pgdat_percpu_threshold(pgdat, calculate_pressure_threshold);\r\n} else {\r\nif (remaining)\r\ncount_vm_event(KSWAPD_LOW_WMARK_HIT_QUICKLY);\r\nelse\r\ncount_vm_event(KSWAPD_HIGH_WMARK_HIT_QUICKLY);\r\n}\r\nfinish_wait(&pgdat->kswapd_wait, &wait);\r\n}\r\nstatic int kswapd(void *p)\r\n{\r\nunsigned long order, new_order;\r\nunsigned balanced_order;\r\nint classzone_idx, new_classzone_idx;\r\nint balanced_classzone_idx;\r\npg_data_t *pgdat = (pg_data_t*)p;\r\nstruct task_struct *tsk = current;\r\nstruct reclaim_state reclaim_state = {\r\n.reclaimed_slab = 0,\r\n};\r\nconst struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);\r\nlockdep_set_current_reclaim_state(GFP_KERNEL);\r\nif (!cpumask_empty(cpumask))\r\nset_cpus_allowed_ptr(tsk, cpumask);\r\ncurrent->reclaim_state = &reclaim_state;\r\ntsk->flags |= PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD;\r\nset_freezable();\r\norder = new_order = 0;\r\nbalanced_order = 0;\r\nclasszone_idx = new_classzone_idx = pgdat->nr_zones - 1;\r\nbalanced_classzone_idx = classzone_idx;\r\nfor ( ; ; ) {\r\nbool ret;\r\nif (balanced_classzone_idx >= new_classzone_idx &&\r\nbalanced_order == new_order) {\r\nnew_order = pgdat->kswapd_max_order;\r\nnew_classzone_idx = pgdat->classzone_idx;\r\npgdat->kswapd_max_order = 0;\r\npgdat->classzone_idx = pgdat->nr_zones - 1;\r\n}\r\nif (order < new_order || classzone_idx > new_classzone_idx) {\r\norder = new_order;\r\nclasszone_idx = new_classzone_idx;\r\n} else {\r\nkswapd_try_to_sleep(pgdat, balanced_order,\r\nbalanced_classzone_idx);\r\norder = pgdat->kswapd_max_order;\r\nclasszone_idx = pgdat->classzone_idx;\r\nnew_order = order;\r\nnew_classzone_idx = classzone_idx;\r\npgdat->kswapd_max_order = 0;\r\npgdat->classzone_idx = pgdat->nr_zones - 1;\r\n}\r\nret = try_to_freeze();\r\nif (kthread_should_stop())\r\nbreak;\r\nif (!ret) {\r\ntrace_mm_vmscan_kswapd_wake(pgdat->node_id, order);\r\nbalanced_classzone_idx = classzone_idx;\r\nbalanced_order = balance_pgdat(pgdat, order,\r\n&balanced_classzone_idx);\r\n}\r\n}\r\ncurrent->reclaim_state = NULL;\r\nreturn 0;\r\n}\r\nvoid wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx)\r\n{\r\npg_data_t *pgdat;\r\nif (!populated_zone(zone))\r\nreturn;\r\nif (!cpuset_zone_allowed_hardwall(zone, GFP_KERNEL))\r\nreturn;\r\npgdat = zone->zone_pgdat;\r\nif (pgdat->kswapd_max_order < order) {\r\npgdat->kswapd_max_order = order;\r\npgdat->classzone_idx = min(pgdat->classzone_idx, classzone_idx);\r\n}\r\nif (!waitqueue_active(&pgdat->kswapd_wait))\r\nreturn;\r\nif (zone_balanced(zone, order, 0, 0))\r\nreturn;\r\ntrace_mm_vmscan_wakeup_kswapd(pgdat->node_id, zone_idx(zone), order);\r\nwake_up_interruptible(&pgdat->kswapd_wait);\r\n}\r\nunsigned long global_reclaimable_pages(void)\r\n{\r\nint nr;\r\nnr = global_page_state(NR_ACTIVE_FILE) +\r\nglobal_page_state(NR_INACTIVE_FILE);\r\nif (get_nr_swap_pages() > 0)\r\nnr += global_page_state(NR_ACTIVE_ANON) +\r\nglobal_page_state(NR_INACTIVE_ANON);\r\nreturn nr;\r\n}\r\nunsigned long shrink_all_memory(unsigned long nr_to_reclaim)\r\n{\r\nstruct reclaim_state reclaim_state;\r\nstruct scan_control sc = {\r\n.gfp_mask = GFP_HIGHUSER_MOVABLE,\r\n.may_swap = 1,\r\n.may_unmap = 1,\r\n.may_writepage = 1,\r\n.nr_to_reclaim = nr_to_reclaim,\r\n.hibernation_mode = 1,\r\n.order = 0,\r\n.priority = DEF_PRIORITY,\r\n};\r\nstruct shrink_control shrink = {\r\n.gfp_mask = sc.gfp_mask,\r\n};\r\nstruct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);\r\nstruct task_struct *p = current;\r\nunsigned long nr_reclaimed;\r\np->flags |= PF_MEMALLOC;\r\nlockdep_set_current_reclaim_state(sc.gfp_mask);\r\nreclaim_state.reclaimed_slab = 0;\r\np->reclaim_state = &reclaim_state;\r\nnr_reclaimed = do_try_to_free_pages(zonelist, &sc, &shrink);\r\np->reclaim_state = NULL;\r\nlockdep_clear_current_reclaim_state();\r\np->flags &= ~PF_MEMALLOC;\r\nreturn nr_reclaimed;\r\n}\r\nstatic int cpu_callback(struct notifier_block *nfb, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nint nid;\r\nif (action == CPU_ONLINE || action == CPU_ONLINE_FROZEN) {\r\nfor_each_node_state(nid, N_MEMORY) {\r\npg_data_t *pgdat = NODE_DATA(nid);\r\nconst struct cpumask *mask;\r\nmask = cpumask_of_node(pgdat->node_id);\r\nif (cpumask_any_and(cpu_online_mask, mask) < nr_cpu_ids)\r\nset_cpus_allowed_ptr(pgdat->kswapd, mask);\r\n}\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nint kswapd_run(int nid)\r\n{\r\npg_data_t *pgdat = NODE_DATA(nid);\r\nint ret = 0;\r\nif (pgdat->kswapd)\r\nreturn 0;\r\npgdat->kswapd = kthread_run(kswapd, pgdat, "kswapd%d", nid);\r\nif (IS_ERR(pgdat->kswapd)) {\r\nBUG_ON(system_state == SYSTEM_BOOTING);\r\npr_err("Failed to start kswapd on node %d\n", nid);\r\nret = PTR_ERR(pgdat->kswapd);\r\npgdat->kswapd = NULL;\r\n}\r\nreturn ret;\r\n}\r\nvoid kswapd_stop(int nid)\r\n{\r\nstruct task_struct *kswapd = NODE_DATA(nid)->kswapd;\r\nif (kswapd) {\r\nkthread_stop(kswapd);\r\nNODE_DATA(nid)->kswapd = NULL;\r\n}\r\n}\r\nstatic int __init kswapd_init(void)\r\n{\r\nint nid;\r\nswap_setup();\r\nfor_each_node_state(nid, N_MEMORY)\r\nkswapd_run(nid);\r\nhotcpu_notifier(cpu_callback, 0);\r\nreturn 0;\r\n}\r\nstatic inline unsigned long zone_unmapped_file_pages(struct zone *zone)\r\n{\r\nunsigned long file_mapped = zone_page_state(zone, NR_FILE_MAPPED);\r\nunsigned long file_lru = zone_page_state(zone, NR_INACTIVE_FILE) +\r\nzone_page_state(zone, NR_ACTIVE_FILE);\r\nreturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;\r\n}\r\nstatic long zone_pagecache_reclaimable(struct zone *zone)\r\n{\r\nlong nr_pagecache_reclaimable;\r\nlong delta = 0;\r\nif (zone_reclaim_mode & RECLAIM_SWAP)\r\nnr_pagecache_reclaimable = zone_page_state(zone, NR_FILE_PAGES);\r\nelse\r\nnr_pagecache_reclaimable = zone_unmapped_file_pages(zone);\r\nif (!(zone_reclaim_mode & RECLAIM_WRITE))\r\ndelta += zone_page_state(zone, NR_FILE_DIRTY);\r\nif (unlikely(delta > nr_pagecache_reclaimable))\r\ndelta = nr_pagecache_reclaimable;\r\nreturn nr_pagecache_reclaimable - delta;\r\n}\r\nstatic int __zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)\r\n{\r\nconst unsigned long nr_pages = 1 << order;\r\nstruct task_struct *p = current;\r\nstruct reclaim_state reclaim_state;\r\nstruct scan_control sc = {\r\n.may_writepage = !!(zone_reclaim_mode & RECLAIM_WRITE),\r\n.may_unmap = !!(zone_reclaim_mode & RECLAIM_SWAP),\r\n.may_swap = 1,\r\n.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),\r\n.gfp_mask = (gfp_mask = memalloc_noio_flags(gfp_mask)),\r\n.order = order,\r\n.priority = ZONE_RECLAIM_PRIORITY,\r\n};\r\nstruct shrink_control shrink = {\r\n.gfp_mask = sc.gfp_mask,\r\n};\r\nunsigned long nr_slab_pages0, nr_slab_pages1;\r\ncond_resched();\r\np->flags |= PF_MEMALLOC | PF_SWAPWRITE;\r\nlockdep_set_current_reclaim_state(gfp_mask);\r\nreclaim_state.reclaimed_slab = 0;\r\np->reclaim_state = &reclaim_state;\r\nif (zone_pagecache_reclaimable(zone) > zone->min_unmapped_pages) {\r\ndo {\r\nshrink_zone(zone, &sc);\r\n} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);\r\n}\r\nnr_slab_pages0 = zone_page_state(zone, NR_SLAB_RECLAIMABLE);\r\nif (nr_slab_pages0 > zone->min_slab_pages) {\r\nnodes_clear(shrink.nodes_to_scan);\r\nnode_set(zone_to_nid(zone), shrink.nodes_to_scan);\r\nfor (;;) {\r\nunsigned long lru_pages = zone_reclaimable_pages(zone);\r\nif (!shrink_slab(&shrink, sc.nr_scanned, lru_pages))\r\nbreak;\r\nnr_slab_pages1 = zone_page_state(zone,\r\nNR_SLAB_RECLAIMABLE);\r\nif (nr_slab_pages1 + nr_pages <= nr_slab_pages0)\r\nbreak;\r\n}\r\nnr_slab_pages1 = zone_page_state(zone, NR_SLAB_RECLAIMABLE);\r\nif (nr_slab_pages1 < nr_slab_pages0)\r\nsc.nr_reclaimed += nr_slab_pages0 - nr_slab_pages1;\r\n}\r\np->reclaim_state = NULL;\r\ncurrent->flags &= ~(PF_MEMALLOC | PF_SWAPWRITE);\r\nlockdep_clear_current_reclaim_state();\r\nreturn sc.nr_reclaimed >= nr_pages;\r\n}\r\nint zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)\r\n{\r\nint node_id;\r\nint ret;\r\nif (zone_pagecache_reclaimable(zone) <= zone->min_unmapped_pages &&\r\nzone_page_state(zone, NR_SLAB_RECLAIMABLE) <= zone->min_slab_pages)\r\nreturn ZONE_RECLAIM_FULL;\r\nif (!zone_reclaimable(zone))\r\nreturn ZONE_RECLAIM_FULL;\r\nif (!(gfp_mask & __GFP_WAIT) || (current->flags & PF_MEMALLOC))\r\nreturn ZONE_RECLAIM_NOSCAN;\r\nnode_id = zone_to_nid(zone);\r\nif (node_state(node_id, N_CPU) && node_id != numa_node_id())\r\nreturn ZONE_RECLAIM_NOSCAN;\r\nif (zone_test_and_set_flag(zone, ZONE_RECLAIM_LOCKED))\r\nreturn ZONE_RECLAIM_NOSCAN;\r\nret = __zone_reclaim(zone, gfp_mask, order);\r\nzone_clear_flag(zone, ZONE_RECLAIM_LOCKED);\r\nif (!ret)\r\ncount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);\r\nreturn ret;\r\n}\r\nint page_evictable(struct page *page)\r\n{\r\nreturn !mapping_unevictable(page_mapping(page)) && !PageMlocked(page);\r\n}\r\nvoid check_move_unevictable_pages(struct page **pages, int nr_pages)\r\n{\r\nstruct lruvec *lruvec;\r\nstruct zone *zone = NULL;\r\nint pgscanned = 0;\r\nint pgrescued = 0;\r\nint i;\r\nfor (i = 0; i < nr_pages; i++) {\r\nstruct page *page = pages[i];\r\nstruct zone *pagezone;\r\npgscanned++;\r\npagezone = page_zone(page);\r\nif (pagezone != zone) {\r\nif (zone)\r\nspin_unlock_irq(&zone->lru_lock);\r\nzone = pagezone;\r\nspin_lock_irq(&zone->lru_lock);\r\n}\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nif (!PageLRU(page) || !PageUnevictable(page))\r\ncontinue;\r\nif (page_evictable(page)) {\r\nenum lru_list lru = page_lru_base_type(page);\r\nVM_BUG_ON(PageActive(page));\r\nClearPageUnevictable(page);\r\ndel_page_from_lru_list(page, lruvec, LRU_UNEVICTABLE);\r\nadd_page_to_lru_list(page, lruvec, lru);\r\npgrescued++;\r\n}\r\n}\r\nif (zone) {\r\n__count_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);\r\n__count_vm_events(UNEVICTABLE_PGSCANNED, pgscanned);\r\nspin_unlock_irq(&zone->lru_lock);\r\n}\r\n}\r\nstatic void warn_scan_unevictable_pages(void)\r\n{\r\nprintk_once(KERN_WARNING\r\n"%s: The scan_unevictable_pages sysctl/node-interface has been "\r\n"disabled for lack of a legitimate use case. If you have "\r\n"one, please send an email to linux-mm@kvack.org.\n",\r\ncurrent->comm);\r\n}\r\nint scan_unevictable_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer,\r\nsize_t *length, loff_t *ppos)\r\n{\r\nwarn_scan_unevictable_pages();\r\nproc_doulongvec_minmax(table, write, buffer, length, ppos);\r\nscan_unevictable_pages = 0;\r\nreturn 0;\r\n}\r\nstatic ssize_t read_scan_unevictable_node(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nwarn_scan_unevictable_pages();\r\nreturn sprintf(buf, "0\n");\r\n}\r\nstatic ssize_t write_scan_unevictable_node(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nwarn_scan_unevictable_pages();\r\nreturn 1;\r\n}\r\nint scan_unevictable_register_node(struct node *node)\r\n{\r\nreturn device_create_file(&node->dev, &dev_attr_scan_unevictable_pages);\r\n}\r\nvoid scan_unevictable_unregister_node(struct node *node)\r\n{\r\ndevice_remove_file(&node->dev, &dev_attr_scan_unevictable_pages);\r\n}
