struct sg_table *radeon_gem_prime_get_sg_table(struct drm_gem_object *obj)\r\n{\r\nstruct radeon_bo *bo = gem_to_radeon_bo(obj);\r\nint npages = bo->tbo.num_pages;\r\nreturn drm_prime_pages_to_sg(bo->tbo.ttm->pages, npages);\r\n}\r\nvoid *radeon_gem_prime_vmap(struct drm_gem_object *obj)\r\n{\r\nstruct radeon_bo *bo = gem_to_radeon_bo(obj);\r\nint ret;\r\nret = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages,\r\n&bo->dma_buf_vmap);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nreturn bo->dma_buf_vmap.virtual;\r\n}\r\nvoid radeon_gem_prime_vunmap(struct drm_gem_object *obj, void *vaddr)\r\n{\r\nstruct radeon_bo *bo = gem_to_radeon_bo(obj);\r\nttm_bo_kunmap(&bo->dma_buf_vmap);\r\n}\r\nstruct drm_gem_object *radeon_gem_prime_import_sg_table(struct drm_device *dev,\r\nsize_t size,\r\nstruct sg_table *sg)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct radeon_bo *bo;\r\nint ret;\r\nret = radeon_bo_create(rdev, size, PAGE_SIZE, false,\r\nRADEON_GEM_DOMAIN_GTT, sg, &bo);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nmutex_lock(&rdev->gem.mutex);\r\nlist_add_tail(&bo->list, &rdev->gem.objects);\r\nmutex_unlock(&rdev->gem.mutex);\r\nreturn &bo->gem_base;\r\n}\r\nint radeon_gem_prime_pin(struct drm_gem_object *obj)\r\n{\r\nstruct radeon_bo *bo = gem_to_radeon_bo(obj);\r\nint ret = 0;\r\nret = radeon_bo_reserve(bo, false);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = radeon_bo_pin(bo, RADEON_GEM_DOMAIN_GTT, NULL);\r\nradeon_bo_unreserve(bo);\r\nreturn ret;\r\n}\r\nvoid radeon_gem_prime_unpin(struct drm_gem_object *obj)\r\n{\r\nstruct radeon_bo *bo = gem_to_radeon_bo(obj);\r\nint ret = 0;\r\nret = radeon_bo_reserve(bo, false);\r\nif (unlikely(ret != 0))\r\nreturn;\r\nradeon_bo_unpin(bo);\r\nradeon_bo_unreserve(bo);\r\n}
