static int __init hardlockup_panic_setup(char *str)\r\n{\r\nif (!strncmp(str, "panic", 5))\r\nhardlockup_panic = 1;\r\nelse if (!strncmp(str, "nopanic", 7))\r\nhardlockup_panic = 0;\r\nelse if (!strncmp(str, "0", 1))\r\nwatchdog_user_enabled = 0;\r\nreturn 1;\r\n}\r\nstatic int __init softlockup_panic_setup(char *str)\r\n{\r\nsoftlockup_panic = simple_strtoul(str, NULL, 0);\r\nreturn 1;\r\n}\r\nstatic int __init nowatchdog_setup(char *str)\r\n{\r\nwatchdog_user_enabled = 0;\r\nreturn 1;\r\n}\r\nstatic int __init nosoftlockup_setup(char *str)\r\n{\r\nwatchdog_user_enabled = 0;\r\nreturn 1;\r\n}\r\nstatic int get_softlockup_thresh(void)\r\n{\r\nreturn watchdog_thresh * 2;\r\n}\r\nstatic unsigned long get_timestamp(void)\r\n{\r\nreturn local_clock() >> 30LL;\r\n}\r\nstatic void set_sample_period(void)\r\n{\r\nsample_period = get_softlockup_thresh() * ((u64)NSEC_PER_SEC / 5);\r\n}\r\nstatic void __touch_watchdog(void)\r\n{\r\n__this_cpu_write(watchdog_touch_ts, get_timestamp());\r\n}\r\nvoid touch_softlockup_watchdog(void)\r\n{\r\n__this_cpu_write(watchdog_touch_ts, 0);\r\n}\r\nvoid touch_all_softlockup_watchdogs(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nper_cpu(watchdog_touch_ts, cpu) = 0;\r\n}\r\nvoid touch_nmi_watchdog(void)\r\n{\r\nif (watchdog_user_enabled) {\r\nunsigned cpu;\r\nfor_each_present_cpu(cpu) {\r\nif (per_cpu(watchdog_nmi_touch, cpu) != true)\r\nper_cpu(watchdog_nmi_touch, cpu) = true;\r\n}\r\n}\r\ntouch_softlockup_watchdog();\r\n}\r\nvoid touch_softlockup_watchdog_sync(void)\r\n{\r\n__raw_get_cpu_var(softlockup_touch_sync) = true;\r\n__raw_get_cpu_var(watchdog_touch_ts) = 0;\r\n}\r\nstatic int is_hardlockup(void)\r\n{\r\nunsigned long hrint = __this_cpu_read(hrtimer_interrupts);\r\nif (__this_cpu_read(hrtimer_interrupts_saved) == hrint)\r\nreturn 1;\r\n__this_cpu_write(hrtimer_interrupts_saved, hrint);\r\nreturn 0;\r\n}\r\nstatic int is_softlockup(unsigned long touch_ts)\r\n{\r\nunsigned long now = get_timestamp();\r\nif (time_after(now, touch_ts + get_softlockup_thresh()))\r\nreturn now - touch_ts;\r\nreturn 0;\r\n}\r\nstatic void watchdog_overflow_callback(struct perf_event *event,\r\nstruct perf_sample_data *data,\r\nstruct pt_regs *regs)\r\n{\r\nevent->hw.interrupts = 0;\r\nif (__this_cpu_read(watchdog_nmi_touch) == true) {\r\n__this_cpu_write(watchdog_nmi_touch, false);\r\nreturn;\r\n}\r\nif (is_hardlockup()) {\r\nint this_cpu = smp_processor_id();\r\nif (__this_cpu_read(hard_watchdog_warn) == true)\r\nreturn;\r\nif (hardlockup_panic)\r\npanic("Watchdog detected hard LOCKUP on cpu %d", this_cpu);\r\nelse\r\nWARN(1, "Watchdog detected hard LOCKUP on cpu %d", this_cpu);\r\n__this_cpu_write(hard_watchdog_warn, true);\r\nreturn;\r\n}\r\n__this_cpu_write(hard_watchdog_warn, false);\r\nreturn;\r\n}\r\nstatic void watchdog_interrupt_count(void)\r\n{\r\n__this_cpu_inc(hrtimer_interrupts);\r\n}\r\nstatic enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)\r\n{\r\nunsigned long touch_ts = __this_cpu_read(watchdog_touch_ts);\r\nstruct pt_regs *regs = get_irq_regs();\r\nint duration;\r\nwatchdog_interrupt_count();\r\nwake_up_process(__this_cpu_read(softlockup_watchdog));\r\nhrtimer_forward_now(hrtimer, ns_to_ktime(sample_period));\r\nif (touch_ts == 0) {\r\nif (unlikely(__this_cpu_read(softlockup_touch_sync))) {\r\n__this_cpu_write(softlockup_touch_sync, false);\r\nsched_clock_tick();\r\n}\r\nkvm_check_and_clear_guest_paused();\r\n__touch_watchdog();\r\nreturn HRTIMER_RESTART;\r\n}\r\nduration = is_softlockup(touch_ts);\r\nif (unlikely(duration)) {\r\nif (kvm_check_and_clear_guest_paused())\r\nreturn HRTIMER_RESTART;\r\nif (__this_cpu_read(soft_watchdog_warn) == true)\r\nreturn HRTIMER_RESTART;\r\nprintk(KERN_EMERG "BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\n",\r\nsmp_processor_id(), duration,\r\ncurrent->comm, task_pid_nr(current));\r\nprint_modules();\r\nprint_irqtrace_events(current);\r\nif (regs)\r\nshow_regs(regs);\r\nelse\r\ndump_stack();\r\nif (softlockup_panic)\r\npanic("softlockup: hung tasks");\r\n__this_cpu_write(soft_watchdog_warn, true);\r\n} else\r\n__this_cpu_write(soft_watchdog_warn, false);\r\nreturn HRTIMER_RESTART;\r\n}\r\nstatic void watchdog_set_prio(unsigned int policy, unsigned int prio)\r\n{\r\nstruct sched_param param = { .sched_priority = prio };\r\nsched_setscheduler(current, policy, &param);\r\n}\r\nstatic void watchdog_enable(unsigned int cpu)\r\n{\r\nstruct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);\r\nhrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nhrtimer->function = watchdog_timer_fn;\r\nwatchdog_nmi_enable(cpu);\r\nhrtimer_start(hrtimer, ns_to_ktime(sample_period),\r\nHRTIMER_MODE_REL_PINNED);\r\nwatchdog_set_prio(SCHED_FIFO, MAX_RT_PRIO - 1);\r\n__touch_watchdog();\r\n}\r\nstatic void watchdog_disable(unsigned int cpu)\r\n{\r\nstruct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);\r\nwatchdog_set_prio(SCHED_NORMAL, 0);\r\nhrtimer_cancel(hrtimer);\r\nwatchdog_nmi_disable(cpu);\r\n}\r\nstatic void watchdog_cleanup(unsigned int cpu, bool online)\r\n{\r\nwatchdog_disable(cpu);\r\n}\r\nstatic int watchdog_should_run(unsigned int cpu)\r\n{\r\nreturn __this_cpu_read(hrtimer_interrupts) !=\r\n__this_cpu_read(soft_lockup_hrtimer_cnt);\r\n}\r\nstatic void watchdog(unsigned int cpu)\r\n{\r\n__this_cpu_write(soft_lockup_hrtimer_cnt,\r\n__this_cpu_read(hrtimer_interrupts));\r\n__touch_watchdog();\r\n}\r\nstatic int watchdog_nmi_enable(unsigned int cpu)\r\n{\r\nstruct perf_event_attr *wd_attr;\r\nstruct perf_event *event = per_cpu(watchdog_ev, cpu);\r\nif (event && event->state > PERF_EVENT_STATE_OFF)\r\ngoto out;\r\nif (event != NULL)\r\ngoto out_enable;\r\nwd_attr = &wd_hw_attr;\r\nwd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);\r\nevent = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);\r\nif (cpu == 0 && IS_ERR(event))\r\ncpu0_err = PTR_ERR(event);\r\nif (!IS_ERR(event)) {\r\nif (cpu == 0 || cpu0_err)\r\npr_info("enabled on all CPUs, permanently consumes one hw-PMU counter.\n");\r\ngoto out_save;\r\n}\r\nif (cpu > 0 && (PTR_ERR(event) == cpu0_err))\r\nreturn PTR_ERR(event);\r\nif (PTR_ERR(event) == -EOPNOTSUPP)\r\npr_info("disabled (cpu%i): not supported (no LAPIC?)\n", cpu);\r\nelse if (PTR_ERR(event) == -ENOENT)\r\npr_warning("disabled (cpu%i): hardware events not enabled\n",\r\ncpu);\r\nelse\r\npr_err("disabled (cpu%i): unable to create perf event: %ld\n",\r\ncpu, PTR_ERR(event));\r\nreturn PTR_ERR(event);\r\nout_save:\r\nper_cpu(watchdog_ev, cpu) = event;\r\nout_enable:\r\nperf_event_enable(per_cpu(watchdog_ev, cpu));\r\nout:\r\nreturn 0;\r\n}\r\nstatic void watchdog_nmi_disable(unsigned int cpu)\r\n{\r\nstruct perf_event *event = per_cpu(watchdog_ev, cpu);\r\nif (event) {\r\nperf_event_disable(event);\r\nper_cpu(watchdog_ev, cpu) = NULL;\r\nperf_event_release_kernel(event);\r\n}\r\nreturn;\r\n}\r\nstatic int watchdog_nmi_enable(unsigned int cpu) { return 0; }\r\nstatic void watchdog_nmi_disable(unsigned int cpu) { return; }\r\nstatic void restart_watchdog_hrtimer(void *info)\r\n{\r\nstruct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);\r\nint ret;\r\nret = hrtimer_try_to_cancel(hrtimer);\r\nif (ret == 1)\r\nhrtimer_start(hrtimer, ns_to_ktime(sample_period),\r\nHRTIMER_MODE_REL_PINNED);\r\n}\r\nstatic void update_timers(int cpu)\r\n{\r\nstruct call_single_data data = {.func = restart_watchdog_hrtimer};\r\nwatchdog_nmi_disable(cpu);\r\n__smp_call_function_single(cpu, &data, 1);\r\nwatchdog_nmi_enable(cpu);\r\n}\r\nstatic void update_timers_all_cpus(void)\r\n{\r\nint cpu;\r\nget_online_cpus();\r\npreempt_disable();\r\nfor_each_online_cpu(cpu)\r\nupdate_timers(cpu);\r\npreempt_enable();\r\nput_online_cpus();\r\n}\r\nstatic int watchdog_enable_all_cpus(bool sample_period_changed)\r\n{\r\nint err = 0;\r\nif (!watchdog_running) {\r\nerr = smpboot_register_percpu_thread(&watchdog_threads);\r\nif (err)\r\npr_err("Failed to create watchdog threads, disabled\n");\r\nelse\r\nwatchdog_running = 1;\r\n} else if (sample_period_changed) {\r\nupdate_timers_all_cpus();\r\n}\r\nreturn err;\r\n}\r\nstatic void watchdog_disable_all_cpus(void)\r\n{\r\nif (watchdog_running) {\r\nwatchdog_running = 0;\r\nsmpboot_unregister_percpu_thread(&watchdog_threads);\r\n}\r\n}\r\nint proc_dowatchdog(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp, loff_t *ppos)\r\n{\r\nint err, old_thresh, old_enabled;\r\nstatic DEFINE_MUTEX(watchdog_proc_mutex);\r\nmutex_lock(&watchdog_proc_mutex);\r\nold_thresh = ACCESS_ONCE(watchdog_thresh);\r\nold_enabled = ACCESS_ONCE(watchdog_user_enabled);\r\nerr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\r\nif (err || !write)\r\ngoto out;\r\nset_sample_period();\r\nif (watchdog_user_enabled && watchdog_thresh)\r\nerr = watchdog_enable_all_cpus(old_thresh != watchdog_thresh);\r\nelse\r\nwatchdog_disable_all_cpus();\r\nif (err) {\r\nwatchdog_thresh = old_thresh;\r\nwatchdog_user_enabled = old_enabled;\r\n}\r\nout:\r\nmutex_unlock(&watchdog_proc_mutex);\r\nreturn err;\r\n}\r\nvoid __init lockup_detector_init(void)\r\n{\r\nset_sample_period();\r\nif (watchdog_user_enabled)\r\nwatchdog_enable_all_cpus(false);\r\n}
