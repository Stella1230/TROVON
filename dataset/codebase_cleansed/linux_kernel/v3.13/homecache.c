static int __init set_noallocl2(char *str)\r\n{\r\nnoallocl2 = 1;\r\nreturn 0;\r\n}\r\nstatic void hv_flush_update(const struct cpumask *cache_cpumask,\r\nstruct cpumask *tlb_cpumask,\r\nunsigned long tlb_va, unsigned long tlb_length,\r\nHV_Remote_ASID *asids, int asidcount)\r\n{\r\nstruct cpumask mask;\r\nint i, cpu;\r\ncpumask_clear(&mask);\r\nif (cache_cpumask)\r\ncpumask_or(&mask, &mask, cache_cpumask);\r\nif (tlb_cpumask && tlb_length) {\r\ncpumask_or(&mask, &mask, tlb_cpumask);\r\n}\r\nfor (i = 0; i < asidcount; ++i)\r\ncpumask_set_cpu(asids[i].y * smp_width + asids[i].x, &mask);\r\nfor_each_cpu(cpu, &mask)\r\n++per_cpu(irq_stat, cpu).irq_hv_flush_count;\r\n}\r\nvoid flush_remote(unsigned long cache_pfn, unsigned long cache_control,\r\nconst struct cpumask *cache_cpumask_orig,\r\nHV_VirtAddr tlb_va, unsigned long tlb_length,\r\nunsigned long tlb_pgsize,\r\nconst struct cpumask *tlb_cpumask_orig,\r\nHV_Remote_ASID *asids, int asidcount)\r\n{\r\nint rc;\r\nstruct cpumask cache_cpumask_copy, tlb_cpumask_copy;\r\nstruct cpumask *cache_cpumask, *tlb_cpumask;\r\nHV_PhysAddr cache_pa;\r\nchar cache_buf[NR_CPUS*5], tlb_buf[NR_CPUS*5];\r\nmb();\r\nif (cache_cpumask_orig && cache_control) {\r\ncpumask_copy(&cache_cpumask_copy, cache_cpumask_orig);\r\ncache_cpumask = &cache_cpumask_copy;\r\n} else {\r\ncpumask_clear(&cache_cpumask_copy);\r\ncache_cpumask = NULL;\r\n}\r\nif (cache_cpumask == NULL)\r\ncache_control = 0;\r\nif (tlb_cpumask_orig && tlb_length) {\r\ncpumask_copy(&tlb_cpumask_copy, tlb_cpumask_orig);\r\ntlb_cpumask = &tlb_cpumask_copy;\r\n} else {\r\ncpumask_clear(&tlb_cpumask_copy);\r\ntlb_cpumask = NULL;\r\n}\r\nhv_flush_update(cache_cpumask, tlb_cpumask, tlb_va, tlb_length,\r\nasids, asidcount);\r\ncache_pa = (HV_PhysAddr)cache_pfn << PAGE_SHIFT;\r\nrc = hv_flush_remote(cache_pa, cache_control,\r\ncpumask_bits(cache_cpumask),\r\ntlb_va, tlb_length, tlb_pgsize,\r\ncpumask_bits(tlb_cpumask),\r\nasids, asidcount);\r\nif (rc == 0)\r\nreturn;\r\ncpumask_scnprintf(cache_buf, sizeof(cache_buf), &cache_cpumask_copy);\r\ncpumask_scnprintf(tlb_buf, sizeof(tlb_buf), &tlb_cpumask_copy);\r\npr_err("hv_flush_remote(%#llx, %#lx, %p [%s],"\r\n" %#lx, %#lx, %#lx, %p [%s], %p, %d) = %d\n",\r\ncache_pa, cache_control, cache_cpumask, cache_buf,\r\n(unsigned long)tlb_va, tlb_length, tlb_pgsize,\r\ntlb_cpumask, tlb_buf,\r\nasids, asidcount, rc);\r\npanic("Unsafe to continue.");\r\n}\r\nstatic void homecache_finv_page_va(void* va, int home)\r\n{\r\nint cpu = get_cpu();\r\nif (home == cpu) {\r\nfinv_buffer_local(va, PAGE_SIZE);\r\n} else if (home == PAGE_HOME_HASH) {\r\nfinv_buffer_remote(va, PAGE_SIZE, 1);\r\n} else {\r\nBUG_ON(home < 0 || home >= NR_CPUS);\r\nfinv_buffer_remote(va, PAGE_SIZE, 0);\r\n}\r\nput_cpu();\r\n}\r\nvoid homecache_finv_map_page(struct page *page, int home)\r\n{\r\nunsigned long flags;\r\nunsigned long va;\r\npte_t *ptep;\r\npte_t pte;\r\nif (home == PAGE_HOME_UNCACHED)\r\nreturn;\r\nlocal_irq_save(flags);\r\n#ifdef CONFIG_HIGHMEM\r\nva = __fix_to_virt(FIX_KMAP_BEGIN + kmap_atomic_idx_push() +\r\n(KM_TYPE_NR * smp_processor_id()));\r\n#else\r\nva = __fix_to_virt(FIX_HOMECACHE_BEGIN + smp_processor_id());\r\n#endif\r\nptep = virt_to_kpte(va);\r\npte = pfn_pte(page_to_pfn(page), PAGE_KERNEL);\r\n__set_pte(ptep, pte_set_home(pte, home));\r\nhomecache_finv_page_va((void *)va, home);\r\n__pte_clear(ptep);\r\nhv_flush_page(va, PAGE_SIZE);\r\n#ifdef CONFIG_HIGHMEM\r\nkmap_atomic_idx_pop();\r\n#endif\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void homecache_finv_page_home(struct page *page, int home)\r\n{\r\nif (!PageHighMem(page) && home == page_home(page))\r\nhomecache_finv_page_va(page_address(page), home);\r\nelse\r\nhomecache_finv_map_page(page, home);\r\n}\r\nstatic inline bool incoherent_home(int home)\r\n{\r\nreturn home == PAGE_HOME_IMMUTABLE || home == PAGE_HOME_INCOHERENT;\r\n}\r\nstatic void homecache_finv_page_internal(struct page *page, int force_map)\r\n{\r\nint home = page_home(page);\r\nif (home == PAGE_HOME_UNCACHED)\r\nreturn;\r\nif (incoherent_home(home)) {\r\nint cpu;\r\nfor_each_cpu(cpu, &cpu_cacheable_map)\r\nhomecache_finv_map_page(page, cpu);\r\n} else if (force_map) {\r\nhomecache_finv_map_page(page, home);\r\n} else {\r\nhomecache_finv_page_home(page, home);\r\n}\r\nsim_validate_lines_evicted(PFN_PHYS(page_to_pfn(page)), PAGE_SIZE);\r\n}\r\nvoid homecache_finv_page(struct page *page)\r\n{\r\nhomecache_finv_page_internal(page, 0);\r\n}\r\nvoid homecache_evict(const struct cpumask *mask)\r\n{\r\nflush_remote(0, HV_FLUSH_EVICT_L2, mask, 0, 0, 0, NULL, NULL, 0);\r\n}\r\nstatic int pte_to_home(pte_t pte)\r\n{\r\nif (hv_pte_get_nc(pte))\r\nreturn PAGE_HOME_IMMUTABLE;\r\nswitch (hv_pte_get_mode(pte)) {\r\ncase HV_PTE_MODE_CACHE_TILE_L3:\r\nreturn get_remote_cache_cpu(pte);\r\ncase HV_PTE_MODE_CACHE_NO_L3:\r\nreturn PAGE_HOME_INCOHERENT;\r\ncase HV_PTE_MODE_UNCACHED:\r\nreturn PAGE_HOME_UNCACHED;\r\ncase HV_PTE_MODE_CACHE_HASH_L3:\r\nreturn PAGE_HOME_HASH;\r\n}\r\npanic("Bad PTE %#llx\n", pte.val);\r\n}\r\npte_t pte_set_home(pte_t pte, int home)\r\n{\r\nif (pte_file(pte))\r\nreturn pte;\r\n#if CHIP_HAS_MMIO()\r\nif (hv_pte_get_mode(pte) == HV_PTE_MODE_MMIO)\r\nreturn pte;\r\n#endif\r\nif (hv_pte_get_nc(pte) && home != PAGE_HOME_IMMUTABLE) {\r\npte = hv_pte_clear_nc(pte);\r\npr_err("non-immutable page incoherently referenced: %#llx\n",\r\npte.val);\r\n}\r\nswitch (home) {\r\ncase PAGE_HOME_UNCACHED:\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_UNCACHED);\r\nbreak;\r\ncase PAGE_HOME_INCOHERENT:\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_NO_L3);\r\nbreak;\r\ncase PAGE_HOME_IMMUTABLE:\r\nBUG_ON(hv_pte_get_writable(pte));\r\nif (pte_get_forcecache(pte)) {\r\nif (hv_pte_get_mode(pte) == HV_PTE_MODE_CACHE_TILE_L3\r\n&& pte_get_anyhome(pte)) {\r\npte = hv_pte_set_mode(pte,\r\nHV_PTE_MODE_CACHE_NO_L3);\r\n}\r\n} else\r\nif (hash_default)\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_HASH_L3);\r\nelse\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_NO_L3);\r\npte = hv_pte_set_nc(pte);\r\nbreak;\r\ncase PAGE_HOME_HASH:\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_HASH_L3);\r\nbreak;\r\ndefault:\r\nBUG_ON(home < 0 || home >= NR_CPUS ||\r\n!cpu_is_valid_lotar(home));\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_TILE_L3);\r\npte = set_remote_cache_cpu(pte, home);\r\nbreak;\r\n}\r\nif (noallocl2)\r\npte = hv_pte_set_no_alloc_l2(pte);\r\nif (hv_pte_get_no_alloc_l2(pte) && hv_pte_get_no_alloc_l1(pte) &&\r\nhv_pte_get_mode(pte) == HV_PTE_MODE_CACHE_NO_L3) {\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_UNCACHED);\r\n}\r\nBUG_ON(hv_pte_get_mode(pte) == 0);\r\nreturn pte;\r\n}\r\nint page_home(struct page *page)\r\n{\r\nif (PageHighMem(page)) {\r\nreturn PAGE_HOME_HASH;\r\n} else {\r\nunsigned long kva = (unsigned long)page_address(page);\r\nreturn pte_to_home(*virt_to_kpte(kva));\r\n}\r\n}\r\nvoid homecache_change_page_home(struct page *page, int order, int home)\r\n{\r\nint i, pages = (1 << order);\r\nunsigned long kva;\r\nBUG_ON(PageHighMem(page));\r\nBUG_ON(page_count(page) > 1);\r\nBUG_ON(page_mapcount(page) != 0);\r\nkva = (unsigned long) page_address(page);\r\nflush_remote(0, HV_FLUSH_EVICT_L2, &cpu_cacheable_map,\r\nkva, pages * PAGE_SIZE, PAGE_SIZE, cpu_online_mask,\r\nNULL, 0);\r\nfor (i = 0; i < pages; ++i, kva += PAGE_SIZE) {\r\npte_t *ptep = virt_to_kpte(kva);\r\npte_t pteval = *ptep;\r\nBUG_ON(!pte_present(pteval) || pte_huge(pteval));\r\n__set_pte(ptep, pte_set_home(pteval, home));\r\n}\r\n}\r\nstruct page *homecache_alloc_pages(gfp_t gfp_mask,\r\nunsigned int order, int home)\r\n{\r\nstruct page *page;\r\nBUG_ON(gfp_mask & __GFP_HIGHMEM);\r\npage = alloc_pages(gfp_mask, order);\r\nif (page)\r\nhomecache_change_page_home(page, order, home);\r\nreturn page;\r\n}\r\nstruct page *homecache_alloc_pages_node(int nid, gfp_t gfp_mask,\r\nunsigned int order, int home)\r\n{\r\nstruct page *page;\r\nBUG_ON(gfp_mask & __GFP_HIGHMEM);\r\npage = alloc_pages_node(nid, gfp_mask, order);\r\nif (page)\r\nhomecache_change_page_home(page, order, home);\r\nreturn page;\r\n}\r\nvoid __homecache_free_pages(struct page *page, unsigned int order)\r\n{\r\nif (put_page_testzero(page)) {\r\nhomecache_change_page_home(page, order, PAGE_HOME_HASH);\r\nif (order == 0) {\r\nfree_hot_cold_page(page, 0);\r\n} else {\r\ninit_page_count(page);\r\n__free_pages(page, order);\r\n}\r\n}\r\n}\r\nvoid homecache_free_pages(unsigned long addr, unsigned int order)\r\n{\r\nif (addr != 0) {\r\nVM_BUG_ON(!virt_addr_valid((void *)addr));\r\n__homecache_free_pages(virt_to_page((void *)addr), order);\r\n}\r\n}
