static inline void sahara_write(struct sahara_dev *dev, u32 data, u32 reg)\r\n{\r\nwritel(data, dev->regs_base + reg);\r\n}\r\nstatic inline unsigned int sahara_read(struct sahara_dev *dev, u32 reg)\r\n{\r\nreturn readl(dev->regs_base + reg);\r\n}\r\nstatic u32 sahara_aes_key_hdr(struct sahara_dev *dev)\r\n{\r\nu32 hdr = SAHARA_HDR_BASE | SAHARA_HDR_SKHA_ALG_AES |\r\nSAHARA_HDR_FORM_KEY | SAHARA_HDR_LLO |\r\nSAHARA_HDR_CHA_SKHA | SAHARA_HDR_PARITY_BIT;\r\nif (dev->flags & FLAGS_CBC) {\r\nhdr |= SAHARA_HDR_SKHA_MODE_CBC;\r\nhdr ^= SAHARA_HDR_PARITY_BIT;\r\n}\r\nif (dev->flags & FLAGS_ENCRYPT) {\r\nhdr |= SAHARA_HDR_SKHA_OP_ENC;\r\nhdr ^= SAHARA_HDR_PARITY_BIT;\r\n}\r\nreturn hdr;\r\n}\r\nstatic u32 sahara_aes_data_link_hdr(struct sahara_dev *dev)\r\n{\r\nreturn SAHARA_HDR_BASE | SAHARA_HDR_FORM_DATA |\r\nSAHARA_HDR_CHA_SKHA | SAHARA_HDR_PARITY_BIT;\r\n}\r\nstatic int sahara_sg_length(struct scatterlist *sg,\r\nunsigned int total)\r\n{\r\nint sg_nb;\r\nunsigned int len;\r\nstruct scatterlist *sg_list;\r\nsg_nb = 0;\r\nsg_list = sg;\r\nwhile (total) {\r\nlen = min(sg_list->length, total);\r\nsg_nb++;\r\ntotal -= len;\r\nsg_list = sg_next(sg_list);\r\nif (!sg_list)\r\ntotal = 0;\r\n}\r\nreturn sg_nb;\r\n}\r\nstatic void sahara_decode_error(struct sahara_dev *dev, unsigned int error)\r\n{\r\nu8 source = SAHARA_ERRSTATUS_GET_SOURCE(error);\r\nu16 chasrc = ffs(SAHARA_ERRSTATUS_GET_CHASRC(error));\r\ndev_err(dev->device, "%s: Error Register = 0x%08x\n", __func__, error);\r\ndev_err(dev->device, " - %s.\n", sahara_err_src[source]);\r\nif (source == SAHARA_ERRSOURCE_DMA) {\r\nif (error & SAHARA_ERRSTATUS_DMA_DIR)\r\ndev_err(dev->device, " * DMA read.\n");\r\nelse\r\ndev_err(dev->device, " * DMA write.\n");\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_err_dmasize[SAHARA_ERRSTATUS_GET_DMASZ(error)]);\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_err_dmasrc[SAHARA_ERRSTATUS_GET_DMASRC(error)]);\r\n} else if (source == SAHARA_ERRSOURCE_CHA) {\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_cha_errsrc[chasrc]);\r\ndev_err(dev->device, " * %s.\n",\r\nsahara_cha_err[SAHARA_ERRSTATUS_GET_CHAERR(error)]);\r\n}\r\ndev_err(dev->device, "\n");\r\n}\r\nstatic void sahara_decode_status(struct sahara_dev *dev, unsigned int status)\r\n{\r\nu8 state;\r\nif (!IS_ENABLED(DEBUG))\r\nreturn;\r\nstate = SAHARA_STATUS_GET_STATE(status);\r\ndev_dbg(dev->device, "%s: Status Register = 0x%08x\n",\r\n__func__, status);\r\ndev_dbg(dev->device, " - State = %d:\n", state);\r\nif (state & SAHARA_STATE_COMP_FLAG)\r\ndev_dbg(dev->device, " * Descriptor completed. IRQ pending.\n");\r\ndev_dbg(dev->device, " * %s.\n",\r\nsahara_state[state & ~SAHARA_STATE_COMP_FLAG]);\r\nif (status & SAHARA_STATUS_DAR_FULL)\r\ndev_dbg(dev->device, " - DAR Full.\n");\r\nif (status & SAHARA_STATUS_ERROR)\r\ndev_dbg(dev->device, " - Error.\n");\r\nif (status & SAHARA_STATUS_SECURE)\r\ndev_dbg(dev->device, " - Secure.\n");\r\nif (status & SAHARA_STATUS_FAIL)\r\ndev_dbg(dev->device, " - Fail.\n");\r\nif (status & SAHARA_STATUS_RNG_RESEED)\r\ndev_dbg(dev->device, " - RNG Reseed Request.\n");\r\nif (status & SAHARA_STATUS_ACTIVE_RNG)\r\ndev_dbg(dev->device, " - RNG Active.\n");\r\nif (status & SAHARA_STATUS_ACTIVE_MDHA)\r\ndev_dbg(dev->device, " - MDHA Active.\n");\r\nif (status & SAHARA_STATUS_ACTIVE_SKHA)\r\ndev_dbg(dev->device, " - SKHA Active.\n");\r\nif (status & SAHARA_STATUS_MODE_BATCH)\r\ndev_dbg(dev->device, " - Batch Mode.\n");\r\nelse if (status & SAHARA_STATUS_MODE_DEDICATED)\r\ndev_dbg(dev->device, " - Decidated Mode.\n");\r\nelse if (status & SAHARA_STATUS_MODE_DEBUG)\r\ndev_dbg(dev->device, " - Debug Mode.\n");\r\ndev_dbg(dev->device, " - Internal state = 0x%02x\n",\r\nSAHARA_STATUS_GET_ISTATE(status));\r\ndev_dbg(dev->device, "Current DAR: 0x%08x\n",\r\nsahara_read(dev, SAHARA_REG_CDAR));\r\ndev_dbg(dev->device, "Initial DAR: 0x%08x\n\n",\r\nsahara_read(dev, SAHARA_REG_IDAR));\r\n}\r\nstatic void sahara_dump_descriptors(struct sahara_dev *dev)\r\n{\r\nint i;\r\nif (!IS_ENABLED(DEBUG))\r\nreturn;\r\nfor (i = 0; i < SAHARA_MAX_HW_DESC; i++) {\r\ndev_dbg(dev->device, "Descriptor (%d) (0x%08x):\n",\r\ni, dev->hw_phys_desc[i]);\r\ndev_dbg(dev->device, "\thdr = 0x%08x\n", dev->hw_desc[i]->hdr);\r\ndev_dbg(dev->device, "\tlen1 = %u\n", dev->hw_desc[i]->len1);\r\ndev_dbg(dev->device, "\tp1 = 0x%08x\n", dev->hw_desc[i]->p1);\r\ndev_dbg(dev->device, "\tlen2 = %u\n", dev->hw_desc[i]->len2);\r\ndev_dbg(dev->device, "\tp2 = 0x%08x\n", dev->hw_desc[i]->p2);\r\ndev_dbg(dev->device, "\tnext = 0x%08x\n",\r\ndev->hw_desc[i]->next);\r\n}\r\ndev_dbg(dev->device, "\n");\r\n}\r\nstatic void sahara_dump_links(struct sahara_dev *dev)\r\n{\r\nint i;\r\nif (!IS_ENABLED(DEBUG))\r\nreturn;\r\nfor (i = 0; i < SAHARA_MAX_HW_LINK; i++) {\r\ndev_dbg(dev->device, "Link (%d) (0x%08x):\n",\r\ni, dev->hw_phys_link[i]);\r\ndev_dbg(dev->device, "\tlen = %u\n", dev->hw_link[i]->len);\r\ndev_dbg(dev->device, "\tp = 0x%08x\n", dev->hw_link[i]->p);\r\ndev_dbg(dev->device, "\tnext = 0x%08x\n",\r\ndev->hw_link[i]->next);\r\n}\r\ndev_dbg(dev->device, "\n");\r\n}\r\nstatic void sahara_aes_done_task(unsigned long data)\r\n{\r\nstruct sahara_dev *dev = (struct sahara_dev *)data;\r\ndma_unmap_sg(dev->device, dev->out_sg, dev->nb_out_sg,\r\nDMA_TO_DEVICE);\r\ndma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\r\nDMA_FROM_DEVICE);\r\nspin_lock(&dev->lock);\r\nclear_bit(FLAGS_BUSY, &dev->flags);\r\nspin_unlock(&dev->lock);\r\ndev->req->base.complete(&dev->req->base, dev->error);\r\n}\r\nstatic void sahara_watchdog(unsigned long data)\r\n{\r\nstruct sahara_dev *dev = (struct sahara_dev *)data;\r\nunsigned int err = sahara_read(dev, SAHARA_REG_ERRSTATUS);\r\nunsigned int stat = sahara_read(dev, SAHARA_REG_STATUS);\r\nsahara_decode_status(dev, stat);\r\nsahara_decode_error(dev, err);\r\ndev->error = -ETIMEDOUT;\r\nsahara_aes_done_task(data);\r\n}\r\nstatic int sahara_hw_descriptor_create(struct sahara_dev *dev)\r\n{\r\nstruct sahara_ctx *ctx = dev->ctx;\r\nstruct scatterlist *sg;\r\nint ret;\r\nint i, j;\r\nif (ctx->flags & FLAGS_NEW_KEY) {\r\nmemcpy(dev->key_base, ctx->key, ctx->keylen);\r\nctx->flags &= ~FLAGS_NEW_KEY;\r\nif (dev->flags & FLAGS_CBC) {\r\ndev->hw_desc[0]->len1 = AES_BLOCK_SIZE;\r\ndev->hw_desc[0]->p1 = dev->iv_phys_base;\r\n} else {\r\ndev->hw_desc[0]->len1 = 0;\r\ndev->hw_desc[0]->p1 = 0;\r\n}\r\ndev->hw_desc[0]->len2 = ctx->keylen;\r\ndev->hw_desc[0]->p2 = dev->key_phys_base;\r\ndev->hw_desc[0]->next = dev->hw_phys_desc[1];\r\n}\r\ndev->hw_desc[0]->hdr = sahara_aes_key_hdr(dev);\r\ndev->nb_in_sg = sahara_sg_length(dev->in_sg, dev->total);\r\ndev->nb_out_sg = sahara_sg_length(dev->out_sg, dev->total);\r\nif ((dev->nb_in_sg + dev->nb_out_sg) > SAHARA_MAX_HW_LINK) {\r\ndev_err(dev->device, "not enough hw links (%d)\n",\r\ndev->nb_in_sg + dev->nb_out_sg);\r\nreturn -EINVAL;\r\n}\r\nret = dma_map_sg(dev->device, dev->in_sg, dev->nb_in_sg,\r\nDMA_TO_DEVICE);\r\nif (ret != dev->nb_in_sg) {\r\ndev_err(dev->device, "couldn't map in sg\n");\r\ngoto unmap_in;\r\n}\r\nret = dma_map_sg(dev->device, dev->out_sg, dev->nb_out_sg,\r\nDMA_FROM_DEVICE);\r\nif (ret != dev->nb_out_sg) {\r\ndev_err(dev->device, "couldn't map out sg\n");\r\ngoto unmap_out;\r\n}\r\ndev->hw_desc[1]->p1 = dev->hw_phys_link[0];\r\nsg = dev->in_sg;\r\nfor (i = 0; i < dev->nb_in_sg; i++) {\r\ndev->hw_link[i]->len = sg->length;\r\ndev->hw_link[i]->p = sg->dma_address;\r\nif (i == (dev->nb_in_sg - 1)) {\r\ndev->hw_link[i]->next = 0;\r\n} else {\r\ndev->hw_link[i]->next = dev->hw_phys_link[i + 1];\r\nsg = sg_next(sg);\r\n}\r\n}\r\ndev->hw_desc[1]->p2 = dev->hw_phys_link[i];\r\nsg = dev->out_sg;\r\nfor (j = i; j < dev->nb_out_sg + i; j++) {\r\ndev->hw_link[j]->len = sg->length;\r\ndev->hw_link[j]->p = sg->dma_address;\r\nif (j == (dev->nb_out_sg + i - 1)) {\r\ndev->hw_link[j]->next = 0;\r\n} else {\r\ndev->hw_link[j]->next = dev->hw_phys_link[j + 1];\r\nsg = sg_next(sg);\r\n}\r\n}\r\ndev->hw_desc[1]->hdr = sahara_aes_data_link_hdr(dev);\r\ndev->hw_desc[1]->len1 = dev->total;\r\ndev->hw_desc[1]->len2 = dev->total;\r\ndev->hw_desc[1]->next = 0;\r\nsahara_dump_descriptors(dev);\r\nsahara_dump_links(dev);\r\nmod_timer(&dev->watchdog,\r\njiffies + msecs_to_jiffies(SAHARA_TIMEOUT_MS));\r\nsahara_write(dev, dev->hw_phys_desc[0], SAHARA_REG_DAR);\r\nreturn 0;\r\nunmap_out:\r\ndma_unmap_sg(dev->device, dev->out_sg, dev->nb_out_sg,\r\nDMA_TO_DEVICE);\r\nunmap_in:\r\ndma_unmap_sg(dev->device, dev->in_sg, dev->nb_in_sg,\r\nDMA_FROM_DEVICE);\r\nreturn -EINVAL;\r\n}\r\nstatic void sahara_aes_queue_task(unsigned long data)\r\n{\r\nstruct sahara_dev *dev = (struct sahara_dev *)data;\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct sahara_ctx *ctx;\r\nstruct sahara_aes_reqctx *rctx;\r\nstruct ablkcipher_request *req;\r\nint ret;\r\nspin_lock(&dev->lock);\r\nbacklog = crypto_get_backlog(&dev->queue);\r\nasync_req = crypto_dequeue_request(&dev->queue);\r\nif (!async_req)\r\nclear_bit(FLAGS_BUSY, &dev->flags);\r\nspin_unlock(&dev->lock);\r\nif (!async_req)\r\nreturn;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ablkcipher_request_cast(async_req);\r\ndev_dbg(dev->device,\r\n"dispatch request (nbytes=%d, src=%p, dst=%p)\n",\r\nreq->nbytes, req->src, req->dst);\r\ndev->req = req;\r\ndev->total = req->nbytes;\r\ndev->in_sg = req->src;\r\ndev->out_sg = req->dst;\r\nrctx = ablkcipher_request_ctx(req);\r\nctx = crypto_ablkcipher_ctx(crypto_ablkcipher_reqtfm(req));\r\nrctx->mode &= FLAGS_MODE_MASK;\r\ndev->flags = (dev->flags & ~FLAGS_MODE_MASK) | rctx->mode;\r\nif ((dev->flags & FLAGS_CBC) && req->info)\r\nmemcpy(dev->iv_base, req->info, AES_KEYSIZE_128);\r\nctx->dev = dev;\r\ndev->ctx = ctx;\r\nret = sahara_hw_descriptor_create(dev);\r\nif (ret < 0) {\r\nspin_lock(&dev->lock);\r\nclear_bit(FLAGS_BUSY, &dev->flags);\r\nspin_unlock(&dev->lock);\r\ndev->req->base.complete(&dev->req->base, ret);\r\n}\r\n}\r\nstatic int sahara_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nint ret;\r\nctx->keylen = keylen;\r\nif (keylen == AES_KEYSIZE_128) {\r\nmemcpy(ctx->key, key, keylen);\r\nctx->flags |= FLAGS_NEW_KEY;\r\nreturn 0;\r\n}\r\nif (keylen != AES_KEYSIZE_128 &&\r\nkeylen != AES_KEYSIZE_192 && keylen != AES_KEYSIZE_256)\r\nreturn -EINVAL;\r\nctx->fallback->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nctx->fallback->base.crt_flags |=\r\n(tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);\r\nret = crypto_ablkcipher_setkey(ctx->fallback, key, keylen);\r\nif (ret) {\r\nstruct crypto_tfm *tfm_aux = crypto_ablkcipher_tfm(tfm);\r\ntfm_aux->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm_aux->crt_flags |=\r\n(ctx->fallback->base.crt_flags & CRYPTO_TFM_RES_MASK);\r\n}\r\nreturn ret;\r\n}\r\nstatic int sahara_aes_crypt(struct ablkcipher_request *req, unsigned long mode)\r\n{\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nstruct sahara_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct sahara_dev *dev = dev_ptr;\r\nint err = 0;\r\nint busy;\r\ndev_dbg(dev->device, "nbytes: %d, enc: %d, cbc: %d\n",\r\nreq->nbytes, !!(mode & FLAGS_ENCRYPT), !!(mode & FLAGS_CBC));\r\nif (!IS_ALIGNED(req->nbytes, AES_BLOCK_SIZE)) {\r\ndev_err(dev->device,\r\n"request size is not exact amount of AES blocks\n");\r\nreturn -EINVAL;\r\n}\r\nctx->dev = dev;\r\nrctx->mode = mode;\r\nspin_lock_bh(&dev->lock);\r\nerr = ablkcipher_enqueue_request(&dev->queue, req);\r\nbusy = test_and_set_bit(FLAGS_BUSY, &dev->flags);\r\nspin_unlock_bh(&dev->lock);\r\nif (!busy)\r\ntasklet_schedule(&dev->queue_task);\r\nreturn err;\r\n}\r\nstatic int sahara_aes_ecb_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_encrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, FLAGS_ENCRYPT);\r\n}\r\nstatic int sahara_aes_ecb_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_decrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, 0);\r\n}\r\nstatic int sahara_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_encrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_CBC);\r\n}\r\nstatic int sahara_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct sahara_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nint err;\r\nif (unlikely(ctx->keylen != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->fallback);\r\nerr = crypto_ablkcipher_decrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn err;\r\n}\r\nreturn sahara_aes_crypt(req, FLAGS_CBC);\r\n}\r\nstatic int sahara_aes_cra_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = crypto_tfm_alg_name(tfm);\r\nstruct sahara_ctx *ctx = crypto_tfm_ctx(tfm);\r\nctx->fallback = crypto_alloc_ablkcipher(name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ctx->fallback)) {\r\npr_err("Error allocating fallback algo %s\n", name);\r\nreturn PTR_ERR(ctx->fallback);\r\n}\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct sahara_aes_reqctx);\r\nreturn 0;\r\n}\r\nstatic void sahara_aes_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct sahara_ctx *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->fallback)\r\ncrypto_free_ablkcipher(ctx->fallback);\r\nctx->fallback = NULL;\r\n}\r\nstatic irqreturn_t sahara_irq_handler(int irq, void *data)\r\n{\r\nstruct sahara_dev *dev = (struct sahara_dev *)data;\r\nunsigned int stat = sahara_read(dev, SAHARA_REG_STATUS);\r\nunsigned int err = sahara_read(dev, SAHARA_REG_ERRSTATUS);\r\ndel_timer(&dev->watchdog);\r\nsahara_write(dev, SAHARA_CMD_CLEAR_INT | SAHARA_CMD_CLEAR_ERR,\r\nSAHARA_REG_CMD);\r\nsahara_decode_status(dev, stat);\r\nif (SAHARA_STATUS_GET_STATE(stat) == SAHARA_STATE_BUSY) {\r\nreturn IRQ_NONE;\r\n} else if (SAHARA_STATUS_GET_STATE(stat) == SAHARA_STATE_COMPLETE) {\r\ndev->error = 0;\r\n} else {\r\nsahara_decode_error(dev, err);\r\ndev->error = -EINVAL;\r\n}\r\ntasklet_schedule(&dev->done_task);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int sahara_register_algs(struct sahara_dev *dev)\r\n{\r\nint err, i, j;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\r\nINIT_LIST_HEAD(&aes_algs[i].cra_list);\r\nerr = crypto_register_alg(&aes_algs[i]);\r\nif (err)\r\ngoto err_aes_algs;\r\n}\r\nreturn 0;\r\nerr_aes_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_alg(&aes_algs[j]);\r\nreturn err;\r\n}\r\nstatic void sahara_unregister_algs(struct sahara_dev *dev)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++)\r\ncrypto_unregister_alg(&aes_algs[i]);\r\n}\r\nstatic int sahara_probe(struct platform_device *pdev)\r\n{\r\nstruct sahara_dev *dev;\r\nstruct resource *res;\r\nu32 version;\r\nint irq;\r\nint err;\r\nint i;\r\ndev = devm_kzalloc(&pdev->dev, sizeof(struct sahara_dev), GFP_KERNEL);\r\nif (dev == NULL) {\r\ndev_err(&pdev->dev, "unable to alloc data struct.\n");\r\nreturn -ENOMEM;\r\n}\r\ndev->device = &pdev->dev;\r\nplatform_set_drvdata(pdev, dev);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndev->regs_base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(dev->regs_base))\r\nreturn PTR_ERR(dev->regs_base);\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0) {\r\ndev_err(&pdev->dev, "failed to get irq resource\n");\r\nreturn irq;\r\n}\r\nerr = devm_request_irq(&pdev->dev, irq, sahara_irq_handler,\r\n0, dev_name(&pdev->dev), dev);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to request irq\n");\r\nreturn err;\r\n}\r\ndev->clk_ipg = devm_clk_get(&pdev->dev, "ipg");\r\nif (IS_ERR(dev->clk_ipg)) {\r\ndev_err(&pdev->dev, "Could not get ipg clock\n");\r\nreturn PTR_ERR(dev->clk_ipg);\r\n}\r\ndev->clk_ahb = devm_clk_get(&pdev->dev, "ahb");\r\nif (IS_ERR(dev->clk_ahb)) {\r\ndev_err(&pdev->dev, "Could not get ahb clock\n");\r\nreturn PTR_ERR(dev->clk_ahb);\r\n}\r\ndev->hw_desc[0] = dma_alloc_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_DESC * sizeof(struct sahara_hw_desc),\r\n&dev->hw_phys_desc[0], GFP_KERNEL);\r\nif (!dev->hw_desc[0]) {\r\ndev_err(&pdev->dev, "Could not allocate hw descriptors\n");\r\nreturn -ENOMEM;\r\n}\r\ndev->hw_desc[1] = dev->hw_desc[0] + 1;\r\ndev->hw_phys_desc[1] = dev->hw_phys_desc[0] +\r\nsizeof(struct sahara_hw_desc);\r\ndev->key_base = dma_alloc_coherent(&pdev->dev, 2 * AES_KEYSIZE_128,\r\n&dev->key_phys_base, GFP_KERNEL);\r\nif (!dev->key_base) {\r\ndev_err(&pdev->dev, "Could not allocate memory for key\n");\r\nerr = -ENOMEM;\r\ngoto err_key;\r\n}\r\ndev->iv_base = dev->key_base + AES_KEYSIZE_128;\r\ndev->iv_phys_base = dev->key_phys_base + AES_KEYSIZE_128;\r\ndev->hw_link[0] = dma_alloc_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_LINK * sizeof(struct sahara_hw_link),\r\n&dev->hw_phys_link[0], GFP_KERNEL);\r\nif (!dev->hw_link[0]) {\r\ndev_err(&pdev->dev, "Could not allocate hw links\n");\r\nerr = -ENOMEM;\r\ngoto err_link;\r\n}\r\nfor (i = 1; i < SAHARA_MAX_HW_LINK; i++) {\r\ndev->hw_phys_link[i] = dev->hw_phys_link[i - 1] +\r\nsizeof(struct sahara_hw_link);\r\ndev->hw_link[i] = dev->hw_link[i - 1] + 1;\r\n}\r\ncrypto_init_queue(&dev->queue, SAHARA_QUEUE_LENGTH);\r\ndev_ptr = dev;\r\ntasklet_init(&dev->queue_task, sahara_aes_queue_task,\r\n(unsigned long)dev);\r\ntasklet_init(&dev->done_task, sahara_aes_done_task,\r\n(unsigned long)dev);\r\ninit_timer(&dev->watchdog);\r\ndev->watchdog.function = &sahara_watchdog;\r\ndev->watchdog.data = (unsigned long)dev;\r\nclk_prepare_enable(dev->clk_ipg);\r\nclk_prepare_enable(dev->clk_ahb);\r\nversion = sahara_read(dev, SAHARA_REG_VERSION);\r\nif (version != SAHARA_VERSION_3) {\r\ndev_err(&pdev->dev, "SAHARA version %d not supported\n",\r\nversion);\r\nerr = -ENODEV;\r\ngoto err_algs;\r\n}\r\nsahara_write(dev, SAHARA_CMD_RESET | SAHARA_CMD_MODE_BATCH,\r\nSAHARA_REG_CMD);\r\nsahara_write(dev, SAHARA_CONTROL_SET_THROTTLE(0) |\r\nSAHARA_CONTROL_SET_MAXBURST(8) |\r\nSAHARA_CONTROL_RNG_AUTORSD |\r\nSAHARA_CONTROL_ENABLE_INT,\r\nSAHARA_REG_CONTROL);\r\nerr = sahara_register_algs(dev);\r\nif (err)\r\ngoto err_algs;\r\ndev_info(&pdev->dev, "SAHARA version %d initialized\n", version);\r\nreturn 0;\r\nerr_algs:\r\ndma_free_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_LINK * sizeof(struct sahara_hw_link),\r\ndev->hw_link[0], dev->hw_phys_link[0]);\r\nclk_disable_unprepare(dev->clk_ipg);\r\nclk_disable_unprepare(dev->clk_ahb);\r\ndev_ptr = NULL;\r\nerr_link:\r\ndma_free_coherent(&pdev->dev,\r\n2 * AES_KEYSIZE_128,\r\ndev->key_base, dev->key_phys_base);\r\nerr_key:\r\ndma_free_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_DESC * sizeof(struct sahara_hw_desc),\r\ndev->hw_desc[0], dev->hw_phys_desc[0]);\r\nreturn err;\r\n}\r\nstatic int sahara_remove(struct platform_device *pdev)\r\n{\r\nstruct sahara_dev *dev = platform_get_drvdata(pdev);\r\ndma_free_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_LINK * sizeof(struct sahara_hw_link),\r\ndev->hw_link[0], dev->hw_phys_link[0]);\r\ndma_free_coherent(&pdev->dev,\r\n2 * AES_KEYSIZE_128,\r\ndev->key_base, dev->key_phys_base);\r\ndma_free_coherent(&pdev->dev,\r\nSAHARA_MAX_HW_DESC * sizeof(struct sahara_hw_desc),\r\ndev->hw_desc[0], dev->hw_phys_desc[0]);\r\ntasklet_kill(&dev->done_task);\r\ntasklet_kill(&dev->queue_task);\r\nsahara_unregister_algs(dev);\r\nclk_disable_unprepare(dev->clk_ipg);\r\nclk_disable_unprepare(dev->clk_ahb);\r\ndev_ptr = NULL;\r\nreturn 0;\r\n}
