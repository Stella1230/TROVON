void fsnotify_get_mark(struct fsnotify_mark *mark)\r\n{\r\natomic_inc(&mark->refcnt);\r\n}\r\nvoid fsnotify_put_mark(struct fsnotify_mark *mark)\r\n{\r\nif (atomic_dec_and_test(&mark->refcnt)) {\r\nif (mark->group)\r\nfsnotify_put_group(mark->group);\r\nmark->free_mark(mark);\r\n}\r\n}\r\nvoid fsnotify_destroy_mark_locked(struct fsnotify_mark *mark,\r\nstruct fsnotify_group *group)\r\n{\r\nstruct inode *inode = NULL;\r\nBUG_ON(!mutex_is_locked(&group->mark_mutex));\r\nspin_lock(&mark->lock);\r\nif (!(mark->flags & FSNOTIFY_MARK_FLAG_ALIVE)) {\r\nspin_unlock(&mark->lock);\r\nreturn;\r\n}\r\nmark->flags &= ~FSNOTIFY_MARK_FLAG_ALIVE;\r\nif (mark->flags & FSNOTIFY_MARK_FLAG_INODE) {\r\ninode = mark->i.inode;\r\nfsnotify_destroy_inode_mark(mark);\r\n} else if (mark->flags & FSNOTIFY_MARK_FLAG_VFSMOUNT)\r\nfsnotify_destroy_vfsmount_mark(mark);\r\nelse\r\nBUG();\r\nlist_del_init(&mark->g_list);\r\nspin_unlock(&mark->lock);\r\nif (inode && (mark->flags & FSNOTIFY_MARK_FLAG_OBJECT_PINNED))\r\niput(inode);\r\nmutex_unlock(&group->mark_mutex);\r\nspin_lock(&destroy_lock);\r\nlist_add(&mark->destroy_list, &destroy_list);\r\nspin_unlock(&destroy_lock);\r\nwake_up(&destroy_waitq);\r\nif (group->ops->freeing_mark)\r\ngroup->ops->freeing_mark(mark, group);\r\natomic_dec(&group->num_marks);\r\nmutex_lock_nested(&group->mark_mutex, SINGLE_DEPTH_NESTING);\r\n}\r\nvoid fsnotify_destroy_mark(struct fsnotify_mark *mark,\r\nstruct fsnotify_group *group)\r\n{\r\nmutex_lock_nested(&group->mark_mutex, SINGLE_DEPTH_NESTING);\r\nfsnotify_destroy_mark_locked(mark, group);\r\nmutex_unlock(&group->mark_mutex);\r\n}\r\nvoid fsnotify_set_mark_mask_locked(struct fsnotify_mark *mark, __u32 mask)\r\n{\r\nassert_spin_locked(&mark->lock);\r\nmark->mask = mask;\r\nif (mark->flags & FSNOTIFY_MARK_FLAG_INODE)\r\nfsnotify_set_inode_mark_mask_locked(mark, mask);\r\n}\r\nvoid fsnotify_set_mark_ignored_mask_locked(struct fsnotify_mark *mark, __u32 mask)\r\n{\r\nassert_spin_locked(&mark->lock);\r\nmark->ignored_mask = mask;\r\n}\r\nint fsnotify_add_mark_locked(struct fsnotify_mark *mark,\r\nstruct fsnotify_group *group, struct inode *inode,\r\nstruct vfsmount *mnt, int allow_dups)\r\n{\r\nint ret = 0;\r\nBUG_ON(inode && mnt);\r\nBUG_ON(!inode && !mnt);\r\nBUG_ON(!mutex_is_locked(&group->mark_mutex));\r\nspin_lock(&mark->lock);\r\nmark->flags |= FSNOTIFY_MARK_FLAG_ALIVE;\r\nfsnotify_get_group(group);\r\nmark->group = group;\r\nlist_add(&mark->g_list, &group->marks_list);\r\natomic_inc(&group->num_marks);\r\nfsnotify_get_mark(mark);\r\nif (inode) {\r\nret = fsnotify_add_inode_mark(mark, group, inode, allow_dups);\r\nif (ret)\r\ngoto err;\r\n} else if (mnt) {\r\nret = fsnotify_add_vfsmount_mark(mark, group, mnt, allow_dups);\r\nif (ret)\r\ngoto err;\r\n} else {\r\nBUG();\r\n}\r\nfsnotify_set_mark_mask_locked(mark, mark->mask);\r\nspin_unlock(&mark->lock);\r\nif (inode)\r\n__fsnotify_update_child_dentry_flags(inode);\r\nreturn ret;\r\nerr:\r\nmark->flags &= ~FSNOTIFY_MARK_FLAG_ALIVE;\r\nlist_del_init(&mark->g_list);\r\nfsnotify_put_group(group);\r\nmark->group = NULL;\r\natomic_dec(&group->num_marks);\r\nspin_unlock(&mark->lock);\r\nspin_lock(&destroy_lock);\r\nlist_add(&mark->destroy_list, &destroy_list);\r\nspin_unlock(&destroy_lock);\r\nwake_up(&destroy_waitq);\r\nreturn ret;\r\n}\r\nint fsnotify_add_mark(struct fsnotify_mark *mark, struct fsnotify_group *group,\r\nstruct inode *inode, struct vfsmount *mnt, int allow_dups)\r\n{\r\nint ret;\r\nmutex_lock(&group->mark_mutex);\r\nret = fsnotify_add_mark_locked(mark, group, inode, mnt, allow_dups);\r\nmutex_unlock(&group->mark_mutex);\r\nreturn ret;\r\n}\r\nvoid fsnotify_clear_marks_by_group_flags(struct fsnotify_group *group,\r\nunsigned int flags)\r\n{\r\nstruct fsnotify_mark *lmark, *mark;\r\nmutex_lock_nested(&group->mark_mutex, SINGLE_DEPTH_NESTING);\r\nlist_for_each_entry_safe(mark, lmark, &group->marks_list, g_list) {\r\nif (mark->flags & flags) {\r\nfsnotify_get_mark(mark);\r\nfsnotify_destroy_mark_locked(mark, group);\r\nfsnotify_put_mark(mark);\r\n}\r\n}\r\nmutex_unlock(&group->mark_mutex);\r\n}\r\nvoid fsnotify_clear_marks_by_group(struct fsnotify_group *group)\r\n{\r\nfsnotify_clear_marks_by_group_flags(group, (unsigned int)-1);\r\n}\r\nvoid fsnotify_duplicate_mark(struct fsnotify_mark *new, struct fsnotify_mark *old)\r\n{\r\nassert_spin_locked(&old->lock);\r\nnew->i.inode = old->i.inode;\r\nnew->m.mnt = old->m.mnt;\r\nif (old->group)\r\nfsnotify_get_group(old->group);\r\nnew->group = old->group;\r\nnew->mask = old->mask;\r\nnew->free_mark = old->free_mark;\r\n}\r\nvoid fsnotify_init_mark(struct fsnotify_mark *mark,\r\nvoid (*free_mark)(struct fsnotify_mark *mark))\r\n{\r\nmemset(mark, 0, sizeof(*mark));\r\nspin_lock_init(&mark->lock);\r\natomic_set(&mark->refcnt, 1);\r\nmark->free_mark = free_mark;\r\n}\r\nstatic int fsnotify_mark_destroy(void *ignored)\r\n{\r\nstruct fsnotify_mark *mark, *next;\r\nstruct list_head private_destroy_list;\r\nfor (;;) {\r\nspin_lock(&destroy_lock);\r\nlist_replace_init(&destroy_list, &private_destroy_list);\r\nspin_unlock(&destroy_lock);\r\nsynchronize_srcu(&fsnotify_mark_srcu);\r\nlist_for_each_entry_safe(mark, next, &private_destroy_list, destroy_list) {\r\nlist_del_init(&mark->destroy_list);\r\nfsnotify_put_mark(mark);\r\n}\r\nwait_event_interruptible(destroy_waitq, !list_empty(&destroy_list));\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init fsnotify_mark_init(void)\r\n{\r\nstruct task_struct *thread;\r\nthread = kthread_run(fsnotify_mark_destroy, NULL,\r\n"fsnotify_mark");\r\nif (IS_ERR(thread))\r\npanic("unable to start fsnotify mark destruction thread.");\r\nreturn 0;\r\n}
