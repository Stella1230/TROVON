int __first_cpu(const cpumask_t *srcp)\r\n{\r\nreturn min_t(int, NR_CPUS, find_first_bit(srcp->bits, NR_CPUS));\r\n}\r\nint __next_cpu(int n, const cpumask_t *srcp)\r\n{\r\nreturn min_t(int, NR_CPUS, find_next_bit(srcp->bits, NR_CPUS, n+1));\r\n}\r\nint __next_cpu_nr(int n, const cpumask_t *srcp)\r\n{\r\nreturn min_t(int, nr_cpu_ids,\r\nfind_next_bit(srcp->bits, nr_cpu_ids, n+1));\r\n}\r\nint cpumask_next_and(int n, const struct cpumask *src1p,\r\nconst struct cpumask *src2p)\r\n{\r\nwhile ((n = cpumask_next(n, src1p)) < nr_cpu_ids)\r\nif (cpumask_test_cpu(n, src2p))\r\nbreak;\r\nreturn n;\r\n}\r\nint cpumask_any_but(const struct cpumask *mask, unsigned int cpu)\r\n{\r\nunsigned int i;\r\ncpumask_check(cpu);\r\nfor_each_cpu(i, mask)\r\nif (i != cpu)\r\nbreak;\r\nreturn i;\r\n}\r\nbool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)\r\n{\r\n*mask = kmalloc_node(cpumask_size(), flags, node);\r\n#ifdef CONFIG_DEBUG_PER_CPU_MAPS\r\nif (!*mask) {\r\nprintk(KERN_ERR "=> alloc_cpumask_var: failed!\n");\r\ndump_stack();\r\n}\r\n#endif\r\nif (*mask) {\r\nunsigned char *ptr = (unsigned char *)cpumask_bits(*mask);\r\nunsigned int tail;\r\ntail = BITS_TO_LONGS(NR_CPUS - nr_cpumask_bits) * sizeof(long);\r\nmemset(ptr + cpumask_size() - tail, 0, tail);\r\n}\r\nreturn *mask != NULL;\r\n}\r\nbool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)\r\n{\r\nreturn alloc_cpumask_var_node(mask, flags | __GFP_ZERO, node);\r\n}\r\nbool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)\r\n{\r\nreturn alloc_cpumask_var_node(mask, flags, NUMA_NO_NODE);\r\n}\r\nbool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)\r\n{\r\nreturn alloc_cpumask_var(mask, flags | __GFP_ZERO);\r\n}\r\nvoid __init alloc_bootmem_cpumask_var(cpumask_var_t *mask)\r\n{\r\n*mask = memblock_virt_alloc(cpumask_size(), 0);\r\n}\r\nvoid free_cpumask_var(cpumask_var_t mask)\r\n{\r\nkfree(mask);\r\n}\r\nvoid __init free_bootmem_cpumask_var(cpumask_var_t mask)\r\n{\r\nmemblock_free_early(__pa(mask), cpumask_size());\r\n}\r\nint cpumask_set_cpu_local_first(int i, int numa_node, cpumask_t *dstp)\r\n{\r\ncpumask_var_t mask;\r\nint cpu;\r\nint ret = 0;\r\nif (!zalloc_cpumask_var(&mask, GFP_KERNEL))\r\nreturn -ENOMEM;\r\ni %= num_online_cpus();\r\nif (numa_node == -1 || !cpumask_of_node(numa_node)) {\r\ncpumask_copy(mask, cpu_online_mask);\r\n} else {\r\nint n;\r\ncpumask_and(mask,\r\ncpumask_of_node(numa_node), cpu_online_mask);\r\nn = cpumask_weight(mask);\r\nif (i >= n) {\r\ni -= n;\r\ncpumask_andnot(mask, cpu_online_mask, mask);\r\n}\r\n}\r\nfor_each_cpu(cpu, mask) {\r\nif (--i < 0)\r\ngoto out;\r\n}\r\nret = -EAGAIN;\r\nout:\r\nfree_cpumask_var(mask);\r\nif (!ret)\r\ncpumask_set_cpu(cpu, dstp);\r\nreturn ret;\r\n}
