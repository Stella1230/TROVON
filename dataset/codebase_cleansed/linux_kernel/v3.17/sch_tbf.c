static u64 psched_ns_t2l(const struct psched_ratecfg *r,\r\nu64 time_in_ns)\r\n{\r\nu64 len = time_in_ns * r->rate_bytes_ps;\r\ndo_div(len, NSEC_PER_SEC);\r\nif (unlikely(r->linklayer == TC_LINKLAYER_ATM)) {\r\ndo_div(len, 53);\r\nlen = len * 48;\r\n}\r\nif (len > r->overhead)\r\nlen -= r->overhead;\r\nelse\r\nlen = 0;\r\nreturn len;\r\n}\r\nstatic unsigned int skb_gso_mac_seglen(const struct sk_buff *skb)\r\n{\r\nunsigned int hdr_len = skb_transport_header(skb) - skb_mac_header(skb);\r\nreturn hdr_len + skb_gso_transport_seglen(skb);\r\n}\r\nstatic int tbf_segment(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *segs, *nskb;\r\nnetdev_features_t features = netif_skb_features(skb);\r\nint ret, nb;\r\nsegs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);\r\nif (IS_ERR_OR_NULL(segs))\r\nreturn qdisc_reshape_fail(skb, sch);\r\nnb = 0;\r\nwhile (segs) {\r\nnskb = segs->next;\r\nsegs->next = NULL;\r\nqdisc_skb_cb(segs)->pkt_len = segs->len;\r\nret = qdisc_enqueue(segs, q->qdisc);\r\nif (ret != NET_XMIT_SUCCESS) {\r\nif (net_xmit_drop_count(ret))\r\nsch->qstats.drops++;\r\n} else {\r\nnb++;\r\n}\r\nsegs = nskb;\r\n}\r\nsch->q.qlen += nb;\r\nif (nb > 1)\r\nqdisc_tree_decrease_qlen(sch, 1 - nb);\r\nconsume_skb(skb);\r\nreturn nb > 0 ? NET_XMIT_SUCCESS : NET_XMIT_DROP;\r\n}\r\nstatic int tbf_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nint ret;\r\nif (qdisc_pkt_len(skb) > q->max_size) {\r\nif (skb_is_gso(skb) && skb_gso_mac_seglen(skb) <= q->max_size)\r\nreturn tbf_segment(skb, sch);\r\nreturn qdisc_reshape_fail(skb, sch);\r\n}\r\nret = qdisc_enqueue(skb, q->qdisc);\r\nif (ret != NET_XMIT_SUCCESS) {\r\nif (net_xmit_drop_count(ret))\r\nsch->qstats.drops++;\r\nreturn ret;\r\n}\r\nsch->q.qlen++;\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic unsigned int tbf_drop(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nunsigned int len = 0;\r\nif (q->qdisc->ops->drop && (len = q->qdisc->ops->drop(q->qdisc)) != 0) {\r\nsch->q.qlen--;\r\nsch->qstats.drops++;\r\n}\r\nreturn len;\r\n}\r\nstatic bool tbf_peak_present(const struct tbf_sched_data *q)\r\n{\r\nreturn q->peak.rate_bytes_ps;\r\n}\r\nstatic struct sk_buff *tbf_dequeue(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nskb = q->qdisc->ops->peek(q->qdisc);\r\nif (skb) {\r\ns64 now;\r\ns64 toks;\r\ns64 ptoks = 0;\r\nunsigned int len = qdisc_pkt_len(skb);\r\nnow = ktime_to_ns(ktime_get());\r\ntoks = min_t(s64, now - q->t_c, q->buffer);\r\nif (tbf_peak_present(q)) {\r\nptoks = toks + q->ptokens;\r\nif (ptoks > q->mtu)\r\nptoks = q->mtu;\r\nptoks -= (s64) psched_l2t_ns(&q->peak, len);\r\n}\r\ntoks += q->tokens;\r\nif (toks > q->buffer)\r\ntoks = q->buffer;\r\ntoks -= (s64) psched_l2t_ns(&q->rate, len);\r\nif ((toks|ptoks) >= 0) {\r\nskb = qdisc_dequeue_peeked(q->qdisc);\r\nif (unlikely(!skb))\r\nreturn NULL;\r\nq->t_c = now;\r\nq->tokens = toks;\r\nq->ptokens = ptoks;\r\nsch->q.qlen--;\r\nqdisc_unthrottled(sch);\r\nqdisc_bstats_update(sch, skb);\r\nreturn skb;\r\n}\r\nqdisc_watchdog_schedule_ns(&q->watchdog,\r\nnow + max_t(long, -toks, -ptoks));\r\nsch->qstats.overlimits++;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void tbf_reset(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nqdisc_reset(q->qdisc);\r\nsch->q.qlen = 0;\r\nq->t_c = ktime_to_ns(ktime_get());\r\nq->tokens = q->buffer;\r\nq->ptokens = q->mtu;\r\nqdisc_watchdog_cancel(&q->watchdog);\r\n}\r\nstatic int tbf_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nint err;\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_TBF_MAX + 1];\r\nstruct tc_tbf_qopt *qopt;\r\nstruct Qdisc *child = NULL;\r\nstruct psched_ratecfg rate;\r\nstruct psched_ratecfg peak;\r\nu64 max_size;\r\ns64 buffer, mtu;\r\nu64 rate64 = 0, prate64 = 0;\r\nerr = nla_parse_nested(tb, TCA_TBF_MAX, opt, tbf_policy);\r\nif (err < 0)\r\nreturn err;\r\nerr = -EINVAL;\r\nif (tb[TCA_TBF_PARMS] == NULL)\r\ngoto done;\r\nqopt = nla_data(tb[TCA_TBF_PARMS]);\r\nif (qopt->rate.linklayer == TC_LINKLAYER_UNAWARE)\r\nqdisc_put_rtab(qdisc_get_rtab(&qopt->rate,\r\ntb[TCA_TBF_RTAB]));\r\nif (qopt->peakrate.linklayer == TC_LINKLAYER_UNAWARE)\r\nqdisc_put_rtab(qdisc_get_rtab(&qopt->peakrate,\r\ntb[TCA_TBF_PTAB]));\r\nbuffer = min_t(u64, PSCHED_TICKS2NS(qopt->buffer), ~0U);\r\nmtu = min_t(u64, PSCHED_TICKS2NS(qopt->mtu), ~0U);\r\nif (tb[TCA_TBF_RATE64])\r\nrate64 = nla_get_u64(tb[TCA_TBF_RATE64]);\r\npsched_ratecfg_precompute(&rate, &qopt->rate, rate64);\r\nif (tb[TCA_TBF_BURST]) {\r\nmax_size = nla_get_u32(tb[TCA_TBF_BURST]);\r\nbuffer = psched_l2t_ns(&rate, max_size);\r\n} else {\r\nmax_size = min_t(u64, psched_ns_t2l(&rate, buffer), ~0U);\r\n}\r\nif (qopt->peakrate.rate) {\r\nif (tb[TCA_TBF_PRATE64])\r\nprate64 = nla_get_u64(tb[TCA_TBF_PRATE64]);\r\npsched_ratecfg_precompute(&peak, &qopt->peakrate, prate64);\r\nif (peak.rate_bytes_ps <= rate.rate_bytes_ps) {\r\npr_warn_ratelimited("sch_tbf: peakrate %llu is lower than or equals to rate %llu !\n",\r\npeak.rate_bytes_ps, rate.rate_bytes_ps);\r\nerr = -EINVAL;\r\ngoto done;\r\n}\r\nif (tb[TCA_TBF_PBURST]) {\r\nu32 pburst = nla_get_u32(tb[TCA_TBF_PBURST]);\r\nmax_size = min_t(u32, max_size, pburst);\r\nmtu = psched_l2t_ns(&peak, pburst);\r\n} else {\r\nmax_size = min_t(u64, max_size, psched_ns_t2l(&peak, mtu));\r\n}\r\n} else {\r\nmemset(&peak, 0, sizeof(peak));\r\n}\r\nif (max_size < psched_mtu(qdisc_dev(sch)))\r\npr_warn_ratelimited("sch_tbf: burst %llu is lower than device %s mtu (%u) !\n",\r\nmax_size, qdisc_dev(sch)->name,\r\npsched_mtu(qdisc_dev(sch)));\r\nif (!max_size) {\r\nerr = -EINVAL;\r\ngoto done;\r\n}\r\nif (q->qdisc != &noop_qdisc) {\r\nerr = fifo_set_limit(q->qdisc, qopt->limit);\r\nif (err)\r\ngoto done;\r\n} else if (qopt->limit > 0) {\r\nchild = fifo_create_dflt(sch, &bfifo_qdisc_ops, qopt->limit);\r\nif (IS_ERR(child)) {\r\nerr = PTR_ERR(child);\r\ngoto done;\r\n}\r\n}\r\nsch_tree_lock(sch);\r\nif (child) {\r\nqdisc_tree_decrease_qlen(q->qdisc, q->qdisc->q.qlen);\r\nqdisc_destroy(q->qdisc);\r\nq->qdisc = child;\r\n}\r\nq->limit = qopt->limit;\r\nif (tb[TCA_TBF_PBURST])\r\nq->mtu = mtu;\r\nelse\r\nq->mtu = PSCHED_TICKS2NS(qopt->mtu);\r\nq->max_size = max_size;\r\nif (tb[TCA_TBF_BURST])\r\nq->buffer = buffer;\r\nelse\r\nq->buffer = PSCHED_TICKS2NS(qopt->buffer);\r\nq->tokens = q->buffer;\r\nq->ptokens = q->mtu;\r\nmemcpy(&q->rate, &rate, sizeof(struct psched_ratecfg));\r\nmemcpy(&q->peak, &peak, sizeof(struct psched_ratecfg));\r\nsch_tree_unlock(sch);\r\nerr = 0;\r\ndone:\r\nreturn err;\r\n}\r\nstatic int tbf_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nq->t_c = ktime_to_ns(ktime_get());\r\nqdisc_watchdog_init(&q->watchdog, sch);\r\nq->qdisc = &noop_qdisc;\r\nreturn tbf_change(sch, opt);\r\n}\r\nstatic void tbf_destroy(struct Qdisc *sch)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\nqdisc_destroy(q->qdisc);\r\n}\r\nstatic int tbf_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *nest;\r\nstruct tc_tbf_qopt opt;\r\nsch->qstats.backlog = q->qdisc->qstats.backlog;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nopt.limit = q->limit;\r\npsched_ratecfg_getrate(&opt.rate, &q->rate);\r\nif (tbf_peak_present(q))\r\npsched_ratecfg_getrate(&opt.peakrate, &q->peak);\r\nelse\r\nmemset(&opt.peakrate, 0, sizeof(opt.peakrate));\r\nopt.mtu = PSCHED_NS2TICKS(q->mtu);\r\nopt.buffer = PSCHED_NS2TICKS(q->buffer);\r\nif (nla_put(skb, TCA_TBF_PARMS, sizeof(opt), &opt))\r\ngoto nla_put_failure;\r\nif (q->rate.rate_bytes_ps >= (1ULL << 32) &&\r\nnla_put_u64(skb, TCA_TBF_RATE64, q->rate.rate_bytes_ps))\r\ngoto nla_put_failure;\r\nif (tbf_peak_present(q) &&\r\nq->peak.rate_bytes_ps >= (1ULL << 32) &&\r\nnla_put_u64(skb, TCA_TBF_PRATE64, q->peak.rate_bytes_ps))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, nest);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic int tbf_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\ntcm->tcm_handle |= TC_H_MIN(1);\r\ntcm->tcm_info = q->qdisc->handle;\r\nreturn 0;\r\n}\r\nstatic int tbf_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\r\nstruct Qdisc **old)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nif (new == NULL)\r\nnew = &noop_qdisc;\r\nsch_tree_lock(sch);\r\n*old = q->qdisc;\r\nq->qdisc = new;\r\nqdisc_tree_decrease_qlen(*old, (*old)->q.qlen);\r\nqdisc_reset(*old);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct Qdisc *tbf_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct tbf_sched_data *q = qdisc_priv(sch);\r\nreturn q->qdisc;\r\n}\r\nstatic unsigned long tbf_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 1;\r\n}\r\nstatic void tbf_put(struct Qdisc *sch, unsigned long arg)\r\n{\r\n}\r\nstatic void tbf_walk(struct Qdisc *sch, struct qdisc_walker *walker)\r\n{\r\nif (!walker->stop) {\r\nif (walker->count >= walker->skip)\r\nif (walker->fn(sch, 1, walker) < 0) {\r\nwalker->stop = 1;\r\nreturn;\r\n}\r\nwalker->count++;\r\n}\r\n}\r\nstatic int __init tbf_module_init(void)\r\n{\r\nreturn register_qdisc(&tbf_qdisc_ops);\r\n}\r\nstatic void __exit tbf_module_exit(void)\r\n{\r\nunregister_qdisc(&tbf_qdisc_ops);\r\n}
