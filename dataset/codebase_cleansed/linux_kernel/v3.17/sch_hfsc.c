static void\r\neltree_insert(struct hfsc_class *cl)\r\n{\r\nstruct rb_node **p = &cl->sched->eligible.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct hfsc_class *cl1;\r\nwhile (*p != NULL) {\r\nparent = *p;\r\ncl1 = rb_entry(parent, struct hfsc_class, el_node);\r\nif (cl->cl_e >= cl1->cl_e)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nrb_link_node(&cl->el_node, parent, p);\r\nrb_insert_color(&cl->el_node, &cl->sched->eligible);\r\n}\r\nstatic inline void\r\neltree_remove(struct hfsc_class *cl)\r\n{\r\nrb_erase(&cl->el_node, &cl->sched->eligible);\r\n}\r\nstatic inline void\r\neltree_update(struct hfsc_class *cl)\r\n{\r\neltree_remove(cl);\r\neltree_insert(cl);\r\n}\r\nstatic inline struct hfsc_class *\r\neltree_get_mindl(struct hfsc_sched *q, u64 cur_time)\r\n{\r\nstruct hfsc_class *p, *cl = NULL;\r\nstruct rb_node *n;\r\nfor (n = rb_first(&q->eligible); n != NULL; n = rb_next(n)) {\r\np = rb_entry(n, struct hfsc_class, el_node);\r\nif (p->cl_e > cur_time)\r\nbreak;\r\nif (cl == NULL || p->cl_d < cl->cl_d)\r\ncl = p;\r\n}\r\nreturn cl;\r\n}\r\nstatic inline struct hfsc_class *\r\neltree_get_minel(struct hfsc_sched *q)\r\n{\r\nstruct rb_node *n;\r\nn = rb_first(&q->eligible);\r\nif (n == NULL)\r\nreturn NULL;\r\nreturn rb_entry(n, struct hfsc_class, el_node);\r\n}\r\nstatic void\r\nvttree_insert(struct hfsc_class *cl)\r\n{\r\nstruct rb_node **p = &cl->cl_parent->vt_tree.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct hfsc_class *cl1;\r\nwhile (*p != NULL) {\r\nparent = *p;\r\ncl1 = rb_entry(parent, struct hfsc_class, vt_node);\r\nif (cl->cl_vt >= cl1->cl_vt)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nrb_link_node(&cl->vt_node, parent, p);\r\nrb_insert_color(&cl->vt_node, &cl->cl_parent->vt_tree);\r\n}\r\nstatic inline void\r\nvttree_remove(struct hfsc_class *cl)\r\n{\r\nrb_erase(&cl->vt_node, &cl->cl_parent->vt_tree);\r\n}\r\nstatic inline void\r\nvttree_update(struct hfsc_class *cl)\r\n{\r\nvttree_remove(cl);\r\nvttree_insert(cl);\r\n}\r\nstatic inline struct hfsc_class *\r\nvttree_firstfit(struct hfsc_class *cl, u64 cur_time)\r\n{\r\nstruct hfsc_class *p;\r\nstruct rb_node *n;\r\nfor (n = rb_first(&cl->vt_tree); n != NULL; n = rb_next(n)) {\r\np = rb_entry(n, struct hfsc_class, vt_node);\r\nif (p->cl_f <= cur_time)\r\nreturn p;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct hfsc_class *\r\nvttree_get_minvt(struct hfsc_class *cl, u64 cur_time)\r\n{\r\nif (cl->cl_cfmin > cur_time)\r\nreturn NULL;\r\nwhile (cl->level > 0) {\r\ncl = vttree_firstfit(cl, cur_time);\r\nif (cl == NULL)\r\nreturn NULL;\r\nif (cl->cl_parent->cl_cvtmin < cl->cl_vt)\r\ncl->cl_parent->cl_cvtmin = cl->cl_vt;\r\n}\r\nreturn cl;\r\n}\r\nstatic void\r\ncftree_insert(struct hfsc_class *cl)\r\n{\r\nstruct rb_node **p = &cl->cl_parent->cf_tree.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct hfsc_class *cl1;\r\nwhile (*p != NULL) {\r\nparent = *p;\r\ncl1 = rb_entry(parent, struct hfsc_class, cf_node);\r\nif (cl->cl_f >= cl1->cl_f)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nrb_link_node(&cl->cf_node, parent, p);\r\nrb_insert_color(&cl->cf_node, &cl->cl_parent->cf_tree);\r\n}\r\nstatic inline void\r\ncftree_remove(struct hfsc_class *cl)\r\n{\r\nrb_erase(&cl->cf_node, &cl->cl_parent->cf_tree);\r\n}\r\nstatic inline void\r\ncftree_update(struct hfsc_class *cl)\r\n{\r\ncftree_remove(cl);\r\ncftree_insert(cl);\r\n}\r\nstatic inline u64\r\nseg_x2y(u64 x, u64 sm)\r\n{\r\nu64 y;\r\ny = (x >> SM_SHIFT) * sm + (((x & SM_MASK) * sm) >> SM_SHIFT);\r\nreturn y;\r\n}\r\nstatic inline u64\r\nseg_y2x(u64 y, u64 ism)\r\n{\r\nu64 x;\r\nif (y == 0)\r\nx = 0;\r\nelse if (ism == HT_INFINITY)\r\nx = HT_INFINITY;\r\nelse {\r\nx = (y >> ISM_SHIFT) * ism\r\n+ (((y & ISM_MASK) * ism) >> ISM_SHIFT);\r\n}\r\nreturn x;\r\n}\r\nstatic u64\r\nm2sm(u32 m)\r\n{\r\nu64 sm;\r\nsm = ((u64)m << SM_SHIFT);\r\nsm += PSCHED_TICKS_PER_SEC - 1;\r\ndo_div(sm, PSCHED_TICKS_PER_SEC);\r\nreturn sm;\r\n}\r\nstatic u64\r\nm2ism(u32 m)\r\n{\r\nu64 ism;\r\nif (m == 0)\r\nism = HT_INFINITY;\r\nelse {\r\nism = ((u64)PSCHED_TICKS_PER_SEC << ISM_SHIFT);\r\nism += m - 1;\r\ndo_div(ism, m);\r\n}\r\nreturn ism;\r\n}\r\nstatic u64\r\nd2dx(u32 d)\r\n{\r\nu64 dx;\r\ndx = ((u64)d * PSCHED_TICKS_PER_SEC);\r\ndx += USEC_PER_SEC - 1;\r\ndo_div(dx, USEC_PER_SEC);\r\nreturn dx;\r\n}\r\nstatic u32\r\nsm2m(u64 sm)\r\n{\r\nu64 m;\r\nm = (sm * PSCHED_TICKS_PER_SEC) >> SM_SHIFT;\r\nreturn (u32)m;\r\n}\r\nstatic u32\r\ndx2d(u64 dx)\r\n{\r\nu64 d;\r\nd = dx * USEC_PER_SEC;\r\ndo_div(d, PSCHED_TICKS_PER_SEC);\r\nreturn (u32)d;\r\n}\r\nstatic void\r\nsc2isc(struct tc_service_curve *sc, struct internal_sc *isc)\r\n{\r\nisc->sm1 = m2sm(sc->m1);\r\nisc->ism1 = m2ism(sc->m1);\r\nisc->dx = d2dx(sc->d);\r\nisc->dy = seg_x2y(isc->dx, isc->sm1);\r\nisc->sm2 = m2sm(sc->m2);\r\nisc->ism2 = m2ism(sc->m2);\r\n}\r\nstatic void\r\nrtsc_init(struct runtime_sc *rtsc, struct internal_sc *isc, u64 x, u64 y)\r\n{\r\nrtsc->x = x;\r\nrtsc->y = y;\r\nrtsc->sm1 = isc->sm1;\r\nrtsc->ism1 = isc->ism1;\r\nrtsc->dx = isc->dx;\r\nrtsc->dy = isc->dy;\r\nrtsc->sm2 = isc->sm2;\r\nrtsc->ism2 = isc->ism2;\r\n}\r\nstatic u64\r\nrtsc_y2x(struct runtime_sc *rtsc, u64 y)\r\n{\r\nu64 x;\r\nif (y < rtsc->y)\r\nx = rtsc->x;\r\nelse if (y <= rtsc->y + rtsc->dy) {\r\nif (rtsc->dy == 0)\r\nx = rtsc->x + rtsc->dx;\r\nelse\r\nx = rtsc->x + seg_y2x(y - rtsc->y, rtsc->ism1);\r\n} else {\r\nx = rtsc->x + rtsc->dx\r\n+ seg_y2x(y - rtsc->y - rtsc->dy, rtsc->ism2);\r\n}\r\nreturn x;\r\n}\r\nstatic u64\r\nrtsc_x2y(struct runtime_sc *rtsc, u64 x)\r\n{\r\nu64 y;\r\nif (x <= rtsc->x)\r\ny = rtsc->y;\r\nelse if (x <= rtsc->x + rtsc->dx)\r\ny = rtsc->y + seg_x2y(x - rtsc->x, rtsc->sm1);\r\nelse\r\ny = rtsc->y + rtsc->dy\r\n+ seg_x2y(x - rtsc->x - rtsc->dx, rtsc->sm2);\r\nreturn y;\r\n}\r\nstatic void\r\nrtsc_min(struct runtime_sc *rtsc, struct internal_sc *isc, u64 x, u64 y)\r\n{\r\nu64 y1, y2, dx, dy;\r\nu32 dsm;\r\nif (isc->sm1 <= isc->sm2) {\r\ny1 = rtsc_x2y(rtsc, x);\r\nif (y1 < y)\r\nreturn;\r\nrtsc->x = x;\r\nrtsc->y = y;\r\nreturn;\r\n}\r\ny1 = rtsc_x2y(rtsc, x);\r\nif (y1 <= y) {\r\nreturn;\r\n}\r\ny2 = rtsc_x2y(rtsc, x + isc->dx);\r\nif (y2 >= y + isc->dy) {\r\nrtsc->x = x;\r\nrtsc->y = y;\r\nrtsc->dx = isc->dx;\r\nrtsc->dy = isc->dy;\r\nreturn;\r\n}\r\ndx = (y1 - y) << SM_SHIFT;\r\ndsm = isc->sm1 - isc->sm2;\r\ndo_div(dx, dsm);\r\nif (rtsc->x + rtsc->dx > x)\r\ndx += rtsc->x + rtsc->dx - x;\r\ndy = seg_x2y(dx, isc->sm1);\r\nrtsc->x = x;\r\nrtsc->y = y;\r\nrtsc->dx = dx;\r\nrtsc->dy = dy;\r\n}\r\nstatic void\r\ninit_ed(struct hfsc_class *cl, unsigned int next_len)\r\n{\r\nu64 cur_time = psched_get_time();\r\nrtsc_min(&cl->cl_deadline, &cl->cl_rsc, cur_time, cl->cl_cumul);\r\ncl->cl_eligible = cl->cl_deadline;\r\nif (cl->cl_rsc.sm1 <= cl->cl_rsc.sm2) {\r\ncl->cl_eligible.dx = 0;\r\ncl->cl_eligible.dy = 0;\r\n}\r\ncl->cl_e = rtsc_y2x(&cl->cl_eligible, cl->cl_cumul);\r\ncl->cl_d = rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);\r\neltree_insert(cl);\r\n}\r\nstatic void\r\nupdate_ed(struct hfsc_class *cl, unsigned int next_len)\r\n{\r\ncl->cl_e = rtsc_y2x(&cl->cl_eligible, cl->cl_cumul);\r\ncl->cl_d = rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);\r\neltree_update(cl);\r\n}\r\nstatic inline void\r\nupdate_d(struct hfsc_class *cl, unsigned int next_len)\r\n{\r\ncl->cl_d = rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);\r\n}\r\nstatic inline void\r\nupdate_cfmin(struct hfsc_class *cl)\r\n{\r\nstruct rb_node *n = rb_first(&cl->cf_tree);\r\nstruct hfsc_class *p;\r\nif (n == NULL) {\r\ncl->cl_cfmin = 0;\r\nreturn;\r\n}\r\np = rb_entry(n, struct hfsc_class, cf_node);\r\ncl->cl_cfmin = p->cl_f;\r\n}\r\nstatic void\r\ninit_vf(struct hfsc_class *cl, unsigned int len)\r\n{\r\nstruct hfsc_class *max_cl;\r\nstruct rb_node *n;\r\nu64 vt, f, cur_time;\r\nint go_active;\r\ncur_time = 0;\r\ngo_active = 1;\r\nfor (; cl->cl_parent != NULL; cl = cl->cl_parent) {\r\nif (go_active && cl->cl_nactive++ == 0)\r\ngo_active = 1;\r\nelse\r\ngo_active = 0;\r\nif (go_active) {\r\nn = rb_last(&cl->cl_parent->vt_tree);\r\nif (n != NULL) {\r\nmax_cl = rb_entry(n, struct hfsc_class, vt_node);\r\nvt = max_cl->cl_vt;\r\nif (cl->cl_parent->cl_cvtmin != 0)\r\nvt = (cl->cl_parent->cl_cvtmin + vt)/2;\r\nif (cl->cl_parent->cl_vtperiod !=\r\ncl->cl_parentperiod || vt > cl->cl_vt)\r\ncl->cl_vt = vt;\r\n} else {\r\nvt = cl->cl_parent->cl_cvtmax;\r\ncl->cl_parent->cl_cvtoff += vt;\r\ncl->cl_parent->cl_cvtmax = 0;\r\ncl->cl_parent->cl_cvtmin = 0;\r\ncl->cl_vt = 0;\r\n}\r\ncl->cl_vtoff = cl->cl_parent->cl_cvtoff -\r\ncl->cl_pcvtoff;\r\nvt = cl->cl_vt + cl->cl_vtoff;\r\nrtsc_min(&cl->cl_virtual, &cl->cl_fsc, vt,\r\ncl->cl_total);\r\nif (cl->cl_virtual.x == vt) {\r\ncl->cl_virtual.x -= cl->cl_vtoff;\r\ncl->cl_vtoff = 0;\r\n}\r\ncl->cl_vtadj = 0;\r\ncl->cl_vtperiod++;\r\ncl->cl_parentperiod = cl->cl_parent->cl_vtperiod;\r\nif (cl->cl_parent->cl_nactive == 0)\r\ncl->cl_parentperiod++;\r\ncl->cl_f = 0;\r\nvttree_insert(cl);\r\ncftree_insert(cl);\r\nif (cl->cl_flags & HFSC_USC) {\r\nif (cur_time == 0)\r\ncur_time = psched_get_time();\r\nrtsc_min(&cl->cl_ulimit, &cl->cl_usc, cur_time,\r\ncl->cl_total);\r\ncl->cl_myf = rtsc_y2x(&cl->cl_ulimit,\r\ncl->cl_total);\r\ncl->cl_myfadj = 0;\r\n}\r\n}\r\nf = max(cl->cl_myf, cl->cl_cfmin);\r\nif (f != cl->cl_f) {\r\ncl->cl_f = f;\r\ncftree_update(cl);\r\n}\r\nupdate_cfmin(cl->cl_parent);\r\n}\r\n}\r\nstatic void\r\nupdate_vf(struct hfsc_class *cl, unsigned int len, u64 cur_time)\r\n{\r\nu64 f;\r\nint go_passive = 0;\r\nif (cl->qdisc->q.qlen == 0 && cl->cl_flags & HFSC_FSC)\r\ngo_passive = 1;\r\nfor (; cl->cl_parent != NULL; cl = cl->cl_parent) {\r\ncl->cl_total += len;\r\nif (!(cl->cl_flags & HFSC_FSC) || cl->cl_nactive == 0)\r\ncontinue;\r\nif (go_passive && --cl->cl_nactive == 0)\r\ngo_passive = 1;\r\nelse\r\ngo_passive = 0;\r\nif (go_passive) {\r\nif (cl->cl_vt > cl->cl_parent->cl_cvtmax)\r\ncl->cl_parent->cl_cvtmax = cl->cl_vt;\r\nvttree_remove(cl);\r\ncftree_remove(cl);\r\nupdate_cfmin(cl->cl_parent);\r\ncontinue;\r\n}\r\ncl->cl_vt = rtsc_y2x(&cl->cl_virtual, cl->cl_total)\r\n- cl->cl_vtoff + cl->cl_vtadj;\r\nif (cl->cl_vt < cl->cl_parent->cl_cvtmin) {\r\ncl->cl_vtadj += cl->cl_parent->cl_cvtmin - cl->cl_vt;\r\ncl->cl_vt = cl->cl_parent->cl_cvtmin;\r\n}\r\nvttree_update(cl);\r\nif (cl->cl_flags & HFSC_USC) {\r\ncl->cl_myf = cl->cl_myfadj + rtsc_y2x(&cl->cl_ulimit,\r\ncl->cl_total);\r\n#if 0\r\nmyf_bound = cur_time - PSCHED_JIFFIE2US(1);\r\nif (cl->cl_myf < myf_bound) {\r\ndelta = cur_time - cl->cl_myf;\r\ncl->cl_myfadj += delta;\r\ncl->cl_myf += delta;\r\n}\r\n#endif\r\n}\r\nf = max(cl->cl_myf, cl->cl_cfmin);\r\nif (f != cl->cl_f) {\r\ncl->cl_f = f;\r\ncftree_update(cl);\r\nupdate_cfmin(cl->cl_parent);\r\n}\r\n}\r\n}\r\nstatic void\r\nset_active(struct hfsc_class *cl, unsigned int len)\r\n{\r\nif (cl->cl_flags & HFSC_RSC)\r\ninit_ed(cl, len);\r\nif (cl->cl_flags & HFSC_FSC)\r\ninit_vf(cl, len);\r\nlist_add_tail(&cl->dlist, &cl->sched->droplist);\r\n}\r\nstatic void\r\nset_passive(struct hfsc_class *cl)\r\n{\r\nif (cl->cl_flags & HFSC_RSC)\r\neltree_remove(cl);\r\nlist_del(&cl->dlist);\r\n}\r\nstatic unsigned int\r\nqdisc_peek_len(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nunsigned int len;\r\nskb = sch->ops->peek(sch);\r\nif (skb == NULL) {\r\nqdisc_warn_nonwc("qdisc_peek_len", sch);\r\nreturn 0;\r\n}\r\nlen = qdisc_pkt_len(skb);\r\nreturn len;\r\n}\r\nstatic void\r\nhfsc_purge_queue(struct Qdisc *sch, struct hfsc_class *cl)\r\n{\r\nunsigned int len = cl->qdisc->q.qlen;\r\nqdisc_reset(cl->qdisc);\r\nqdisc_tree_decrease_qlen(cl->qdisc, len);\r\n}\r\nstatic void\r\nhfsc_adjust_levels(struct hfsc_class *cl)\r\n{\r\nstruct hfsc_class *p;\r\nunsigned int level;\r\ndo {\r\nlevel = 0;\r\nlist_for_each_entry(p, &cl->children, siblings) {\r\nif (p->level >= level)\r\nlevel = p->level + 1;\r\n}\r\ncl->level = level;\r\n} while ((cl = cl->cl_parent) != NULL);\r\n}\r\nstatic inline struct hfsc_class *\r\nhfsc_find_class(u32 classid, struct Qdisc *sch)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct Qdisc_class_common *clc;\r\nclc = qdisc_class_find(&q->clhash, classid);\r\nif (clc == NULL)\r\nreturn NULL;\r\nreturn container_of(clc, struct hfsc_class, cl_common);\r\n}\r\nstatic void\r\nhfsc_change_rsc(struct hfsc_class *cl, struct tc_service_curve *rsc,\r\nu64 cur_time)\r\n{\r\nsc2isc(rsc, &cl->cl_rsc);\r\nrtsc_init(&cl->cl_deadline, &cl->cl_rsc, cur_time, cl->cl_cumul);\r\ncl->cl_eligible = cl->cl_deadline;\r\nif (cl->cl_rsc.sm1 <= cl->cl_rsc.sm2) {\r\ncl->cl_eligible.dx = 0;\r\ncl->cl_eligible.dy = 0;\r\n}\r\ncl->cl_flags |= HFSC_RSC;\r\n}\r\nstatic void\r\nhfsc_change_fsc(struct hfsc_class *cl, struct tc_service_curve *fsc)\r\n{\r\nsc2isc(fsc, &cl->cl_fsc);\r\nrtsc_init(&cl->cl_virtual, &cl->cl_fsc, cl->cl_vt, cl->cl_total);\r\ncl->cl_flags |= HFSC_FSC;\r\n}\r\nstatic void\r\nhfsc_change_usc(struct hfsc_class *cl, struct tc_service_curve *usc,\r\nu64 cur_time)\r\n{\r\nsc2isc(usc, &cl->cl_usc);\r\nrtsc_init(&cl->cl_ulimit, &cl->cl_usc, cur_time, cl->cl_total);\r\ncl->cl_flags |= HFSC_USC;\r\n}\r\nstatic int\r\nhfsc_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\r\nstruct nlattr **tca, unsigned long *arg)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl = (struct hfsc_class *)*arg;\r\nstruct hfsc_class *parent = NULL;\r\nstruct nlattr *opt = tca[TCA_OPTIONS];\r\nstruct nlattr *tb[TCA_HFSC_MAX + 1];\r\nstruct tc_service_curve *rsc = NULL, *fsc = NULL, *usc = NULL;\r\nu64 cur_time;\r\nint err;\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_HFSC_MAX, opt, hfsc_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_HFSC_RSC]) {\r\nrsc = nla_data(tb[TCA_HFSC_RSC]);\r\nif (rsc->m1 == 0 && rsc->m2 == 0)\r\nrsc = NULL;\r\n}\r\nif (tb[TCA_HFSC_FSC]) {\r\nfsc = nla_data(tb[TCA_HFSC_FSC]);\r\nif (fsc->m1 == 0 && fsc->m2 == 0)\r\nfsc = NULL;\r\n}\r\nif (tb[TCA_HFSC_USC]) {\r\nusc = nla_data(tb[TCA_HFSC_USC]);\r\nif (usc->m1 == 0 && usc->m2 == 0)\r\nusc = NULL;\r\n}\r\nif (cl != NULL) {\r\nif (parentid) {\r\nif (cl->cl_parent &&\r\ncl->cl_parent->cl_common.classid != parentid)\r\nreturn -EINVAL;\r\nif (cl->cl_parent == NULL && parentid != TC_H_ROOT)\r\nreturn -EINVAL;\r\n}\r\ncur_time = psched_get_time();\r\nif (tca[TCA_RATE]) {\r\nerr = gen_replace_estimator(&cl->bstats, &cl->rate_est,\r\nqdisc_root_sleeping_lock(sch),\r\ntca[TCA_RATE]);\r\nif (err)\r\nreturn err;\r\n}\r\nsch_tree_lock(sch);\r\nif (rsc != NULL)\r\nhfsc_change_rsc(cl, rsc, cur_time);\r\nif (fsc != NULL)\r\nhfsc_change_fsc(cl, fsc);\r\nif (usc != NULL)\r\nhfsc_change_usc(cl, usc, cur_time);\r\nif (cl->qdisc->q.qlen != 0) {\r\nif (cl->cl_flags & HFSC_RSC)\r\nupdate_ed(cl, qdisc_peek_len(cl->qdisc));\r\nif (cl->cl_flags & HFSC_FSC)\r\nupdate_vf(cl, 0, cur_time);\r\n}\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nif (parentid == TC_H_ROOT)\r\nreturn -EEXIST;\r\nparent = &q->root;\r\nif (parentid) {\r\nparent = hfsc_find_class(parentid, sch);\r\nif (parent == NULL)\r\nreturn -ENOENT;\r\n}\r\nif (classid == 0 || TC_H_MAJ(classid ^ sch->handle) != 0)\r\nreturn -EINVAL;\r\nif (hfsc_find_class(classid, sch))\r\nreturn -EEXIST;\r\nif (rsc == NULL && fsc == NULL)\r\nreturn -EINVAL;\r\ncl = kzalloc(sizeof(struct hfsc_class), GFP_KERNEL);\r\nif (cl == NULL)\r\nreturn -ENOBUFS;\r\nif (tca[TCA_RATE]) {\r\nerr = gen_new_estimator(&cl->bstats, &cl->rate_est,\r\nqdisc_root_sleeping_lock(sch),\r\ntca[TCA_RATE]);\r\nif (err) {\r\nkfree(cl);\r\nreturn err;\r\n}\r\n}\r\nif (rsc != NULL)\r\nhfsc_change_rsc(cl, rsc, 0);\r\nif (fsc != NULL)\r\nhfsc_change_fsc(cl, fsc);\r\nif (usc != NULL)\r\nhfsc_change_usc(cl, usc, 0);\r\ncl->cl_common.classid = classid;\r\ncl->refcnt = 1;\r\ncl->sched = q;\r\ncl->cl_parent = parent;\r\ncl->qdisc = qdisc_create_dflt(sch->dev_queue,\r\n&pfifo_qdisc_ops, classid);\r\nif (cl->qdisc == NULL)\r\ncl->qdisc = &noop_qdisc;\r\nINIT_LIST_HEAD(&cl->children);\r\ncl->vt_tree = RB_ROOT;\r\ncl->cf_tree = RB_ROOT;\r\nsch_tree_lock(sch);\r\nqdisc_class_hash_insert(&q->clhash, &cl->cl_common);\r\nlist_add_tail(&cl->siblings, &parent->children);\r\nif (parent->level == 0)\r\nhfsc_purge_queue(sch, parent);\r\nhfsc_adjust_levels(parent);\r\ncl->cl_pcvtoff = parent->cl_cvtoff;\r\nsch_tree_unlock(sch);\r\nqdisc_class_hash_grow(sch, &q->clhash);\r\n*arg = (unsigned long)cl;\r\nreturn 0;\r\n}\r\nstatic void\r\nhfsc_destroy_class(struct Qdisc *sch, struct hfsc_class *cl)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\ntcf_destroy_chain(&cl->filter_list);\r\nqdisc_destroy(cl->qdisc);\r\ngen_kill_estimator(&cl->bstats, &cl->rate_est);\r\nif (cl != &q->root)\r\nkfree(cl);\r\n}\r\nstatic int\r\nhfsc_delete_class(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nif (cl->level > 0 || cl->filter_cnt > 0 || cl == &q->root)\r\nreturn -EBUSY;\r\nsch_tree_lock(sch);\r\nlist_del(&cl->siblings);\r\nhfsc_adjust_levels(cl->cl_parent);\r\nhfsc_purge_queue(sch, cl);\r\nqdisc_class_hash_remove(&q->clhash, &cl->cl_common);\r\nBUG_ON(--cl->refcnt == 0);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct hfsc_class *\r\nhfsc_classify(struct sk_buff *skb, struct Qdisc *sch, int *qerr)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *head, *cl;\r\nstruct tcf_result res;\r\nstruct tcf_proto *tcf;\r\nint result;\r\nif (TC_H_MAJ(skb->priority ^ sch->handle) == 0 &&\r\n(cl = hfsc_find_class(skb->priority, sch)) != NULL)\r\nif (cl->level == 0)\r\nreturn cl;\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\nhead = &q->root;\r\ntcf = q->root.filter_list;\r\nwhile (tcf && (result = tc_classify(skb, tcf, &res)) >= 0) {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nswitch (result) {\r\ncase TC_ACT_QUEUED:\r\ncase TC_ACT_STOLEN:\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\r\ncase TC_ACT_SHOT:\r\nreturn NULL;\r\n}\r\n#endif\r\ncl = (struct hfsc_class *)res.class;\r\nif (!cl) {\r\ncl = hfsc_find_class(res.classid, sch);\r\nif (!cl)\r\nbreak;\r\nif (cl->level >= head->level)\r\nbreak;\r\n}\r\nif (cl->level == 0)\r\nreturn cl;\r\ntcf = cl->filter_list;\r\nhead = cl;\r\n}\r\ncl = hfsc_find_class(TC_H_MAKE(TC_H_MAJ(sch->handle), q->defcls), sch);\r\nif (cl == NULL || cl->level > 0)\r\nreturn NULL;\r\nreturn cl;\r\n}\r\nstatic int\r\nhfsc_graft_class(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\r\nstruct Qdisc **old)\r\n{\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nif (cl->level > 0)\r\nreturn -EINVAL;\r\nif (new == NULL) {\r\nnew = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\r\ncl->cl_common.classid);\r\nif (new == NULL)\r\nnew = &noop_qdisc;\r\n}\r\nsch_tree_lock(sch);\r\nhfsc_purge_queue(sch, cl);\r\n*old = cl->qdisc;\r\ncl->qdisc = new;\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct Qdisc *\r\nhfsc_class_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nif (cl->level == 0)\r\nreturn cl->qdisc;\r\nreturn NULL;\r\n}\r\nstatic void\r\nhfsc_qlen_notify(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nif (cl->qdisc->q.qlen == 0) {\r\nupdate_vf(cl, 0, 0);\r\nset_passive(cl);\r\n}\r\n}\r\nstatic unsigned long\r\nhfsc_get_class(struct Qdisc *sch, u32 classid)\r\n{\r\nstruct hfsc_class *cl = hfsc_find_class(classid, sch);\r\nif (cl != NULL)\r\ncl->refcnt++;\r\nreturn (unsigned long)cl;\r\n}\r\nstatic void\r\nhfsc_put_class(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nif (--cl->refcnt == 0)\r\nhfsc_destroy_class(sch, cl);\r\n}\r\nstatic unsigned long\r\nhfsc_bind_tcf(struct Qdisc *sch, unsigned long parent, u32 classid)\r\n{\r\nstruct hfsc_class *p = (struct hfsc_class *)parent;\r\nstruct hfsc_class *cl = hfsc_find_class(classid, sch);\r\nif (cl != NULL) {\r\nif (p != NULL && p->level <= cl->level)\r\nreturn 0;\r\ncl->filter_cnt++;\r\n}\r\nreturn (unsigned long)cl;\r\n}\r\nstatic void\r\nhfsc_unbind_tcf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\ncl->filter_cnt--;\r\n}\r\nstatic struct tcf_proto **\r\nhfsc_tcf_chain(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nif (cl == NULL)\r\ncl = &q->root;\r\nreturn &cl->filter_list;\r\n}\r\nstatic int\r\nhfsc_dump_sc(struct sk_buff *skb, int attr, struct internal_sc *sc)\r\n{\r\nstruct tc_service_curve tsc;\r\ntsc.m1 = sm2m(sc->sm1);\r\ntsc.d = dx2d(sc->dx);\r\ntsc.m2 = sm2m(sc->sm2);\r\nif (nla_put(skb, attr, sizeof(tsc), &tsc))\r\ngoto nla_put_failure;\r\nreturn skb->len;\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nstatic int\r\nhfsc_dump_curves(struct sk_buff *skb, struct hfsc_class *cl)\r\n{\r\nif ((cl->cl_flags & HFSC_RSC) &&\r\n(hfsc_dump_sc(skb, TCA_HFSC_RSC, &cl->cl_rsc) < 0))\r\ngoto nla_put_failure;\r\nif ((cl->cl_flags & HFSC_FSC) &&\r\n(hfsc_dump_sc(skb, TCA_HFSC_FSC, &cl->cl_fsc) < 0))\r\ngoto nla_put_failure;\r\nif ((cl->cl_flags & HFSC_USC) &&\r\n(hfsc_dump_sc(skb, TCA_HFSC_USC, &cl->cl_usc) < 0))\r\ngoto nla_put_failure;\r\nreturn skb->len;\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nstatic int\r\nhfsc_dump_class(struct Qdisc *sch, unsigned long arg, struct sk_buff *skb,\r\nstruct tcmsg *tcm)\r\n{\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nstruct nlattr *nest;\r\ntcm->tcm_parent = cl->cl_parent ? cl->cl_parent->cl_common.classid :\r\nTC_H_ROOT;\r\ntcm->tcm_handle = cl->cl_common.classid;\r\nif (cl->level == 0)\r\ntcm->tcm_info = cl->qdisc->handle;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nif (hfsc_dump_curves(skb, cl) < 0)\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, nest);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -EMSGSIZE;\r\n}\r\nstatic int\r\nhfsc_dump_class_stats(struct Qdisc *sch, unsigned long arg,\r\nstruct gnet_dump *d)\r\n{\r\nstruct hfsc_class *cl = (struct hfsc_class *)arg;\r\nstruct tc_hfsc_stats xstats;\r\ncl->qstats.qlen = cl->qdisc->q.qlen;\r\ncl->qstats.backlog = cl->qdisc->qstats.backlog;\r\nxstats.level = cl->level;\r\nxstats.period = cl->cl_vtperiod;\r\nxstats.work = cl->cl_total;\r\nxstats.rtwork = cl->cl_cumul;\r\nif (gnet_stats_copy_basic(d, &cl->bstats) < 0 ||\r\ngnet_stats_copy_rate_est(d, &cl->bstats, &cl->rate_est) < 0 ||\r\ngnet_stats_copy_queue(d, &cl->qstats) < 0)\r\nreturn -1;\r\nreturn gnet_stats_copy_app(d, &xstats, sizeof(xstats));\r\n}\r\nstatic void\r\nhfsc_walk(struct Qdisc *sch, struct qdisc_walker *arg)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl;\r\nunsigned int i;\r\nif (arg->stop)\r\nreturn;\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry(cl, &q->clhash.hash[i],\r\ncl_common.hnode) {\r\nif (arg->count < arg->skip) {\r\narg->count++;\r\ncontinue;\r\n}\r\nif (arg->fn(sch, (unsigned long)cl, arg) < 0) {\r\narg->stop = 1;\r\nreturn;\r\n}\r\narg->count++;\r\n}\r\n}\r\n}\r\nstatic void\r\nhfsc_schedule_watchdog(struct Qdisc *sch)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl;\r\nu64 next_time = 0;\r\ncl = eltree_get_minel(q);\r\nif (cl)\r\nnext_time = cl->cl_e;\r\nif (q->root.cl_cfmin != 0) {\r\nif (next_time == 0 || next_time > q->root.cl_cfmin)\r\nnext_time = q->root.cl_cfmin;\r\n}\r\nWARN_ON(next_time == 0);\r\nqdisc_watchdog_schedule(&q->watchdog, next_time);\r\n}\r\nstatic int\r\nhfsc_init_qdisc(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct tc_hfsc_qopt *qopt;\r\nint err;\r\nif (opt == NULL || nla_len(opt) < sizeof(*qopt))\r\nreturn -EINVAL;\r\nqopt = nla_data(opt);\r\nq->defcls = qopt->defcls;\r\nerr = qdisc_class_hash_init(&q->clhash);\r\nif (err < 0)\r\nreturn err;\r\nq->eligible = RB_ROOT;\r\nINIT_LIST_HEAD(&q->droplist);\r\nq->root.cl_common.classid = sch->handle;\r\nq->root.refcnt = 1;\r\nq->root.sched = q;\r\nq->root.qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\r\nsch->handle);\r\nif (q->root.qdisc == NULL)\r\nq->root.qdisc = &noop_qdisc;\r\nINIT_LIST_HEAD(&q->root.children);\r\nq->root.vt_tree = RB_ROOT;\r\nq->root.cf_tree = RB_ROOT;\r\nqdisc_class_hash_insert(&q->clhash, &q->root.cl_common);\r\nqdisc_class_hash_grow(sch, &q->clhash);\r\nqdisc_watchdog_init(&q->watchdog, sch);\r\nreturn 0;\r\n}\r\nstatic int\r\nhfsc_change_qdisc(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct tc_hfsc_qopt *qopt;\r\nif (opt == NULL || nla_len(opt) < sizeof(*qopt))\r\nreturn -EINVAL;\r\nqopt = nla_data(opt);\r\nsch_tree_lock(sch);\r\nq->defcls = qopt->defcls;\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic void\r\nhfsc_reset_class(struct hfsc_class *cl)\r\n{\r\ncl->cl_total = 0;\r\ncl->cl_cumul = 0;\r\ncl->cl_d = 0;\r\ncl->cl_e = 0;\r\ncl->cl_vt = 0;\r\ncl->cl_vtadj = 0;\r\ncl->cl_vtoff = 0;\r\ncl->cl_cvtmin = 0;\r\ncl->cl_cvtmax = 0;\r\ncl->cl_cvtoff = 0;\r\ncl->cl_pcvtoff = 0;\r\ncl->cl_vtperiod = 0;\r\ncl->cl_parentperiod = 0;\r\ncl->cl_f = 0;\r\ncl->cl_myf = 0;\r\ncl->cl_myfadj = 0;\r\ncl->cl_cfmin = 0;\r\ncl->cl_nactive = 0;\r\ncl->vt_tree = RB_ROOT;\r\ncl->cf_tree = RB_ROOT;\r\nqdisc_reset(cl->qdisc);\r\nif (cl->cl_flags & HFSC_RSC)\r\nrtsc_init(&cl->cl_deadline, &cl->cl_rsc, 0, 0);\r\nif (cl->cl_flags & HFSC_FSC)\r\nrtsc_init(&cl->cl_virtual, &cl->cl_fsc, 0, 0);\r\nif (cl->cl_flags & HFSC_USC)\r\nrtsc_init(&cl->cl_ulimit, &cl->cl_usc, 0, 0);\r\n}\r\nstatic void\r\nhfsc_reset_qdisc(struct Qdisc *sch)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl;\r\nunsigned int i;\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry(cl, &q->clhash.hash[i], cl_common.hnode)\r\nhfsc_reset_class(cl);\r\n}\r\nq->eligible = RB_ROOT;\r\nINIT_LIST_HEAD(&q->droplist);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\nsch->q.qlen = 0;\r\n}\r\nstatic void\r\nhfsc_destroy_qdisc(struct Qdisc *sch)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hlist_node *next;\r\nstruct hfsc_class *cl;\r\nunsigned int i;\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry(cl, &q->clhash.hash[i], cl_common.hnode)\r\ntcf_destroy_chain(&cl->filter_list);\r\n}\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry_safe(cl, next, &q->clhash.hash[i],\r\ncl_common.hnode)\r\nhfsc_destroy_class(sch, cl);\r\n}\r\nqdisc_class_hash_destroy(&q->clhash);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\n}\r\nstatic int\r\nhfsc_dump_qdisc(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_hfsc_qopt qopt;\r\nstruct hfsc_class *cl;\r\nunsigned int i;\r\nsch->qstats.backlog = 0;\r\nfor (i = 0; i < q->clhash.hashsize; i++) {\r\nhlist_for_each_entry(cl, &q->clhash.hash[i], cl_common.hnode)\r\nsch->qstats.backlog += cl->qdisc->qstats.backlog;\r\n}\r\nqopt.defcls = q->defcls;\r\nif (nla_put(skb, TCA_OPTIONS, sizeof(qopt), &qopt))\r\ngoto nla_put_failure;\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic int\r\nhfsc_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct hfsc_class *cl;\r\nint uninitialized_var(err);\r\ncl = hfsc_classify(skb, sch, &err);\r\nif (cl == NULL) {\r\nif (err & __NET_XMIT_BYPASS)\r\nsch->qstats.drops++;\r\nkfree_skb(skb);\r\nreturn err;\r\n}\r\nerr = qdisc_enqueue(skb, cl->qdisc);\r\nif (unlikely(err != NET_XMIT_SUCCESS)) {\r\nif (net_xmit_drop_count(err)) {\r\ncl->qstats.drops++;\r\nsch->qstats.drops++;\r\n}\r\nreturn err;\r\n}\r\nif (cl->qdisc->q.qlen == 1)\r\nset_active(cl, qdisc_pkt_len(skb));\r\nsch->q.qlen++;\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic struct sk_buff *\r\nhfsc_dequeue(struct Qdisc *sch)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl;\r\nstruct sk_buff *skb;\r\nu64 cur_time;\r\nunsigned int next_len;\r\nint realtime = 0;\r\nif (sch->q.qlen == 0)\r\nreturn NULL;\r\ncur_time = psched_get_time();\r\ncl = eltree_get_mindl(q, cur_time);\r\nif (cl) {\r\nrealtime = 1;\r\n} else {\r\ncl = vttree_get_minvt(&q->root, cur_time);\r\nif (cl == NULL) {\r\nsch->qstats.overlimits++;\r\nhfsc_schedule_watchdog(sch);\r\nreturn NULL;\r\n}\r\n}\r\nskb = qdisc_dequeue_peeked(cl->qdisc);\r\nif (skb == NULL) {\r\nqdisc_warn_nonwc("HFSC", cl->qdisc);\r\nreturn NULL;\r\n}\r\nbstats_update(&cl->bstats, skb);\r\nupdate_vf(cl, qdisc_pkt_len(skb), cur_time);\r\nif (realtime)\r\ncl->cl_cumul += qdisc_pkt_len(skb);\r\nif (cl->qdisc->q.qlen != 0) {\r\nif (cl->cl_flags & HFSC_RSC) {\r\nnext_len = qdisc_peek_len(cl->qdisc);\r\nif (realtime)\r\nupdate_ed(cl, next_len);\r\nelse\r\nupdate_d(cl, next_len);\r\n}\r\n} else {\r\nset_passive(cl);\r\n}\r\nqdisc_unthrottled(sch);\r\nqdisc_bstats_update(sch, skb);\r\nsch->q.qlen--;\r\nreturn skb;\r\n}\r\nstatic unsigned int\r\nhfsc_drop(struct Qdisc *sch)\r\n{\r\nstruct hfsc_sched *q = qdisc_priv(sch);\r\nstruct hfsc_class *cl;\r\nunsigned int len;\r\nlist_for_each_entry(cl, &q->droplist, dlist) {\r\nif (cl->qdisc->ops->drop != NULL &&\r\n(len = cl->qdisc->ops->drop(cl->qdisc)) > 0) {\r\nif (cl->qdisc->q.qlen == 0) {\r\nupdate_vf(cl, 0, 0);\r\nset_passive(cl);\r\n} else {\r\nlist_move_tail(&cl->dlist, &q->droplist);\r\n}\r\ncl->qstats.drops++;\r\nsch->qstats.drops++;\r\nsch->q.qlen--;\r\nreturn len;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init\r\nhfsc_init(void)\r\n{\r\nreturn register_qdisc(&hfsc_qdisc_ops);\r\n}\r\nstatic void __exit\r\nhfsc_cleanup(void)\r\n{\r\nunregister_qdisc(&hfsc_qdisc_ops);\r\n}
