void hv_begin_read(struct hv_ring_buffer_info *rbi)\r\n{\r\nrbi->ring_buffer->interrupt_mask = 1;\r\nmb();\r\n}\r\nu32 hv_end_read(struct hv_ring_buffer_info *rbi)\r\n{\r\nu32 read;\r\nu32 write;\r\nrbi->ring_buffer->interrupt_mask = 0;\r\nmb();\r\nhv_get_ringbuffer_availbytes(rbi, &read, &write);\r\nreturn read;\r\n}\r\nstatic bool hv_need_to_signal(u32 old_write, struct hv_ring_buffer_info *rbi)\r\n{\r\nmb();\r\nif (rbi->ring_buffer->interrupt_mask)\r\nreturn false;\r\nrmb();\r\nif (old_write == rbi->ring_buffer->read_index)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool hv_need_to_signal_on_read(u32 old_rd,\r\nstruct hv_ring_buffer_info *rbi)\r\n{\r\nu32 prev_write_sz;\r\nu32 cur_write_sz;\r\nu32 r_size;\r\nu32 write_loc = rbi->ring_buffer->write_index;\r\nu32 read_loc = rbi->ring_buffer->read_index;\r\nu32 pending_sz = rbi->ring_buffer->pending_send_sz;\r\nif (pending_sz == 0)\r\nreturn false;\r\nr_size = rbi->ring_datasize;\r\ncur_write_sz = write_loc >= read_loc ? r_size - (write_loc - read_loc) :\r\nread_loc - write_loc;\r\nprev_write_sz = write_loc >= old_rd ? r_size - (write_loc - old_rd) :\r\nold_rd - write_loc;\r\nif ((prev_write_sz < pending_sz) && (cur_write_sz >= pending_sz))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline u32\r\nhv_get_next_write_location(struct hv_ring_buffer_info *ring_info)\r\n{\r\nu32 next = ring_info->ring_buffer->write_index;\r\nreturn next;\r\n}\r\nstatic inline void\r\nhv_set_next_write_location(struct hv_ring_buffer_info *ring_info,\r\nu32 next_write_location)\r\n{\r\nring_info->ring_buffer->write_index = next_write_location;\r\n}\r\nstatic inline u32\r\nhv_get_next_read_location(struct hv_ring_buffer_info *ring_info)\r\n{\r\nu32 next = ring_info->ring_buffer->read_index;\r\nreturn next;\r\n}\r\nstatic inline u32\r\nhv_get_next_readlocation_withoffset(struct hv_ring_buffer_info *ring_info,\r\nu32 offset)\r\n{\r\nu32 next = ring_info->ring_buffer->read_index;\r\nnext += offset;\r\nnext %= ring_info->ring_datasize;\r\nreturn next;\r\n}\r\nstatic inline void\r\nhv_set_next_read_location(struct hv_ring_buffer_info *ring_info,\r\nu32 next_read_location)\r\n{\r\nring_info->ring_buffer->read_index = next_read_location;\r\n}\r\nstatic inline void *\r\nhv_get_ring_buffer(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn (void *)ring_info->ring_buffer->buffer;\r\n}\r\nstatic inline u32\r\nhv_get_ring_buffersize(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn ring_info->ring_datasize;\r\n}\r\nstatic inline u64\r\nhv_get_ring_bufferindices(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn (u64)ring_info->ring_buffer->write_index << 32;\r\n}\r\nstatic u32 hv_copyfrom_ringbuffer(\r\nstruct hv_ring_buffer_info *ring_info,\r\nvoid *dest,\r\nu32 destlen,\r\nu32 start_read_offset)\r\n{\r\nvoid *ring_buffer = hv_get_ring_buffer(ring_info);\r\nu32 ring_buffer_size = hv_get_ring_buffersize(ring_info);\r\nu32 frag_len;\r\nif (destlen > ring_buffer_size - start_read_offset) {\r\nfrag_len = ring_buffer_size - start_read_offset;\r\nmemcpy(dest, ring_buffer + start_read_offset, frag_len);\r\nmemcpy(dest + frag_len, ring_buffer, destlen - frag_len);\r\n} else\r\nmemcpy(dest, ring_buffer + start_read_offset, destlen);\r\nstart_read_offset += destlen;\r\nstart_read_offset %= ring_buffer_size;\r\nreturn start_read_offset;\r\n}\r\nstatic u32 hv_copyto_ringbuffer(\r\nstruct hv_ring_buffer_info *ring_info,\r\nu32 start_write_offset,\r\nvoid *src,\r\nu32 srclen)\r\n{\r\nvoid *ring_buffer = hv_get_ring_buffer(ring_info);\r\nu32 ring_buffer_size = hv_get_ring_buffersize(ring_info);\r\nu32 frag_len;\r\nif (srclen > ring_buffer_size - start_write_offset) {\r\nfrag_len = ring_buffer_size - start_write_offset;\r\nmemcpy(ring_buffer + start_write_offset, src, frag_len);\r\nmemcpy(ring_buffer, src + frag_len, srclen - frag_len);\r\n} else\r\nmemcpy(ring_buffer + start_write_offset, src, srclen);\r\nstart_write_offset += srclen;\r\nstart_write_offset %= ring_buffer_size;\r\nreturn start_write_offset;\r\n}\r\nvoid hv_ringbuffer_get_debuginfo(struct hv_ring_buffer_info *ring_info,\r\nstruct hv_ring_buffer_debug_info *debug_info)\r\n{\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nif (ring_info->ring_buffer) {\r\nhv_get_ringbuffer_availbytes(ring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\ndebug_info->bytes_avail_toread = bytes_avail_toread;\r\ndebug_info->bytes_avail_towrite = bytes_avail_towrite;\r\ndebug_info->current_read_index =\r\nring_info->ring_buffer->read_index;\r\ndebug_info->current_write_index =\r\nring_info->ring_buffer->write_index;\r\ndebug_info->current_interrupt_mask =\r\nring_info->ring_buffer->interrupt_mask;\r\n}\r\n}\r\nint hv_ringbuffer_init(struct hv_ring_buffer_info *ring_info,\r\nvoid *buffer, u32 buflen)\r\n{\r\nif (sizeof(struct hv_ring_buffer) != PAGE_SIZE)\r\nreturn -EINVAL;\r\nmemset(ring_info, 0, sizeof(struct hv_ring_buffer_info));\r\nring_info->ring_buffer = (struct hv_ring_buffer *)buffer;\r\nring_info->ring_buffer->read_index =\r\nring_info->ring_buffer->write_index = 0;\r\nring_info->ring_size = buflen;\r\nring_info->ring_datasize = buflen - sizeof(struct hv_ring_buffer);\r\nspin_lock_init(&ring_info->ring_lock);\r\nreturn 0;\r\n}\r\nvoid hv_ringbuffer_cleanup(struct hv_ring_buffer_info *ring_info)\r\n{\r\n}\r\nint hv_ringbuffer_write(struct hv_ring_buffer_info *outring_info,\r\nstruct kvec *kv_list, u32 kv_count, bool *signal)\r\n{\r\nint i = 0;\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nu32 totalbytes_towrite = 0;\r\nu32 next_write_location;\r\nu32 old_write;\r\nu64 prev_indices = 0;\r\nunsigned long flags;\r\nfor (i = 0; i < kv_count; i++)\r\ntotalbytes_towrite += kv_list[i].iov_len;\r\ntotalbytes_towrite += sizeof(u64);\r\nspin_lock_irqsave(&outring_info->ring_lock, flags);\r\nhv_get_ringbuffer_availbytes(outring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\nif (bytes_avail_towrite <= totalbytes_towrite) {\r\nspin_unlock_irqrestore(&outring_info->ring_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nnext_write_location = hv_get_next_write_location(outring_info);\r\nold_write = next_write_location;\r\nfor (i = 0; i < kv_count; i++) {\r\nnext_write_location = hv_copyto_ringbuffer(outring_info,\r\nnext_write_location,\r\nkv_list[i].iov_base,\r\nkv_list[i].iov_len);\r\n}\r\nprev_indices = hv_get_ring_bufferindices(outring_info);\r\nnext_write_location = hv_copyto_ringbuffer(outring_info,\r\nnext_write_location,\r\n&prev_indices,\r\nsizeof(u64));\r\nmb();\r\nhv_set_next_write_location(outring_info, next_write_location);\r\nspin_unlock_irqrestore(&outring_info->ring_lock, flags);\r\n*signal = hv_need_to_signal(old_write, outring_info);\r\nreturn 0;\r\n}\r\nint hv_ringbuffer_peek(struct hv_ring_buffer_info *Inring_info,\r\nvoid *Buffer, u32 buflen)\r\n{\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nu32 next_read_location = 0;\r\nunsigned long flags;\r\nspin_lock_irqsave(&Inring_info->ring_lock, flags);\r\nhv_get_ringbuffer_availbytes(Inring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\nif (bytes_avail_toread < buflen) {\r\nspin_unlock_irqrestore(&Inring_info->ring_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nnext_read_location = hv_get_next_read_location(Inring_info);\r\nnext_read_location = hv_copyfrom_ringbuffer(Inring_info,\r\nBuffer,\r\nbuflen,\r\nnext_read_location);\r\nspin_unlock_irqrestore(&Inring_info->ring_lock, flags);\r\nreturn 0;\r\n}\r\nint hv_ringbuffer_read(struct hv_ring_buffer_info *inring_info, void *buffer,\r\nu32 buflen, u32 offset, bool *signal)\r\n{\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nu32 next_read_location = 0;\r\nu64 prev_indices = 0;\r\nunsigned long flags;\r\nu32 old_read;\r\nif (buflen <= 0)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&inring_info->ring_lock, flags);\r\nhv_get_ringbuffer_availbytes(inring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\nold_read = bytes_avail_toread;\r\nif (bytes_avail_toread < buflen) {\r\nspin_unlock_irqrestore(&inring_info->ring_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nnext_read_location =\r\nhv_get_next_readlocation_withoffset(inring_info, offset);\r\nnext_read_location = hv_copyfrom_ringbuffer(inring_info,\r\nbuffer,\r\nbuflen,\r\nnext_read_location);\r\nnext_read_location = hv_copyfrom_ringbuffer(inring_info,\r\n&prev_indices,\r\nsizeof(u64),\r\nnext_read_location);\r\nmb();\r\nhv_set_next_read_location(inring_info, next_read_location);\r\nspin_unlock_irqrestore(&inring_info->ring_lock, flags);\r\n*signal = hv_need_to_signal_on_read(old_read, inring_info);\r\nreturn 0;\r\n}
