static int atmel_aes_sg_length(struct ablkcipher_request *req,\r\nstruct scatterlist *sg)\r\n{\r\nunsigned int total = req->nbytes;\r\nint sg_nb;\r\nunsigned int len;\r\nstruct scatterlist *sg_list;\r\nsg_nb = 0;\r\nsg_list = sg;\r\ntotal = req->nbytes;\r\nwhile (total) {\r\nlen = min(sg_list->length, total);\r\nsg_nb++;\r\ntotal -= len;\r\nsg_list = sg_next(sg_list);\r\nif (!sg_list)\r\ntotal = 0;\r\n}\r\nreturn sg_nb;\r\n}\r\nstatic int atmel_aes_sg_copy(struct scatterlist **sg, size_t *offset,\r\nvoid *buf, size_t buflen, size_t total, int out)\r\n{\r\nunsigned int count, off = 0;\r\nwhile (buflen && total) {\r\ncount = min((*sg)->length - *offset, total);\r\ncount = min(count, buflen);\r\nif (!count)\r\nreturn off;\r\nscatterwalk_map_and_copy(buf + off, *sg, *offset, count, out);\r\noff += count;\r\nbuflen -= count;\r\n*offset += count;\r\ntotal -= count;\r\nif (*offset == (*sg)->length) {\r\n*sg = sg_next(*sg);\r\nif (*sg)\r\n*offset = 0;\r\nelse\r\ntotal = 0;\r\n}\r\n}\r\nreturn off;\r\n}\r\nstatic inline u32 atmel_aes_read(struct atmel_aes_dev *dd, u32 offset)\r\n{\r\nreturn readl_relaxed(dd->io_base + offset);\r\n}\r\nstatic inline void atmel_aes_write(struct atmel_aes_dev *dd,\r\nu32 offset, u32 value)\r\n{\r\nwritel_relaxed(value, dd->io_base + offset);\r\n}\r\nstatic void atmel_aes_read_n(struct atmel_aes_dev *dd, u32 offset,\r\nu32 *value, int count)\r\n{\r\nfor (; count--; value++, offset += 4)\r\n*value = atmel_aes_read(dd, offset);\r\n}\r\nstatic void atmel_aes_write_n(struct atmel_aes_dev *dd, u32 offset,\r\nu32 *value, int count)\r\n{\r\nfor (; count--; value++, offset += 4)\r\natmel_aes_write(dd, offset, *value);\r\n}\r\nstatic struct atmel_aes_dev *atmel_aes_find_dev(struct atmel_aes_ctx *ctx)\r\n{\r\nstruct atmel_aes_dev *aes_dd = NULL;\r\nstruct atmel_aes_dev *tmp;\r\nspin_lock_bh(&atmel_aes.lock);\r\nif (!ctx->dd) {\r\nlist_for_each_entry(tmp, &atmel_aes.dev_list, list) {\r\naes_dd = tmp;\r\nbreak;\r\n}\r\nctx->dd = aes_dd;\r\n} else {\r\naes_dd = ctx->dd;\r\n}\r\nspin_unlock_bh(&atmel_aes.lock);\r\nreturn aes_dd;\r\n}\r\nstatic int atmel_aes_hw_init(struct atmel_aes_dev *dd)\r\n{\r\nclk_prepare_enable(dd->iclk);\r\nif (!(dd->flags & AES_FLAGS_INIT)) {\r\natmel_aes_write(dd, AES_CR, AES_CR_SWRST);\r\natmel_aes_write(dd, AES_MR, 0xE << AES_MR_CKEY_OFFSET);\r\ndd->flags |= AES_FLAGS_INIT;\r\ndd->err = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline unsigned int atmel_aes_get_version(struct atmel_aes_dev *dd)\r\n{\r\nreturn atmel_aes_read(dd, AES_HW_VERSION) & 0x00000fff;\r\n}\r\nstatic void atmel_aes_hw_version_init(struct atmel_aes_dev *dd)\r\n{\r\natmel_aes_hw_init(dd);\r\ndd->hw_version = atmel_aes_get_version(dd);\r\ndev_info(dd->dev,\r\n"version: 0x%x\n", dd->hw_version);\r\nclk_disable_unprepare(dd->iclk);\r\n}\r\nstatic void atmel_aes_finish_req(struct atmel_aes_dev *dd, int err)\r\n{\r\nstruct ablkcipher_request *req = dd->req;\r\nclk_disable_unprepare(dd->iclk);\r\ndd->flags &= ~AES_FLAGS_BUSY;\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic void atmel_aes_dma_callback(void *data)\r\n{\r\nstruct atmel_aes_dev *dd = data;\r\ntasklet_schedule(&dd->done_task);\r\n}\r\nstatic int atmel_aes_crypt_dma(struct atmel_aes_dev *dd,\r\ndma_addr_t dma_addr_in, dma_addr_t dma_addr_out, int length)\r\n{\r\nstruct scatterlist sg[2];\r\nstruct dma_async_tx_descriptor *in_desc, *out_desc;\r\ndd->dma_size = length;\r\nif (!(dd->flags & AES_FLAGS_FAST)) {\r\ndma_sync_single_for_device(dd->dev, dma_addr_in, length,\r\nDMA_TO_DEVICE);\r\n}\r\nif (dd->flags & AES_FLAGS_CFB8) {\r\ndd->dma_lch_in.dma_conf.dst_addr_width =\r\nDMA_SLAVE_BUSWIDTH_1_BYTE;\r\ndd->dma_lch_out.dma_conf.src_addr_width =\r\nDMA_SLAVE_BUSWIDTH_1_BYTE;\r\n} else if (dd->flags & AES_FLAGS_CFB16) {\r\ndd->dma_lch_in.dma_conf.dst_addr_width =\r\nDMA_SLAVE_BUSWIDTH_2_BYTES;\r\ndd->dma_lch_out.dma_conf.src_addr_width =\r\nDMA_SLAVE_BUSWIDTH_2_BYTES;\r\n} else {\r\ndd->dma_lch_in.dma_conf.dst_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_out.dma_conf.src_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\n}\r\nif (dd->flags & (AES_FLAGS_CFB8 | AES_FLAGS_CFB16 |\r\nAES_FLAGS_CFB32 | AES_FLAGS_CFB64)) {\r\ndd->dma_lch_in.dma_conf.src_maxburst = 1;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = 1;\r\ndd->dma_lch_out.dma_conf.src_maxburst = 1;\r\ndd->dma_lch_out.dma_conf.dst_maxburst = 1;\r\n} else {\r\ndd->dma_lch_in.dma_conf.src_maxburst = dd->caps.max_burst_size;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = dd->caps.max_burst_size;\r\ndd->dma_lch_out.dma_conf.src_maxburst = dd->caps.max_burst_size;\r\ndd->dma_lch_out.dma_conf.dst_maxburst = dd->caps.max_burst_size;\r\n}\r\ndmaengine_slave_config(dd->dma_lch_in.chan, &dd->dma_lch_in.dma_conf);\r\ndmaengine_slave_config(dd->dma_lch_out.chan, &dd->dma_lch_out.dma_conf);\r\ndd->flags |= AES_FLAGS_DMA;\r\nsg_init_table(&sg[0], 1);\r\nsg_dma_address(&sg[0]) = dma_addr_in;\r\nsg_dma_len(&sg[0]) = length;\r\nsg_init_table(&sg[1], 1);\r\nsg_dma_address(&sg[1]) = dma_addr_out;\r\nsg_dma_len(&sg[1]) = length;\r\nin_desc = dmaengine_prep_slave_sg(dd->dma_lch_in.chan, &sg[0],\r\n1, DMA_MEM_TO_DEV,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!in_desc)\r\nreturn -EINVAL;\r\nout_desc = dmaengine_prep_slave_sg(dd->dma_lch_out.chan, &sg[1],\r\n1, DMA_DEV_TO_MEM,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!out_desc)\r\nreturn -EINVAL;\r\nout_desc->callback = atmel_aes_dma_callback;\r\nout_desc->callback_param = dd;\r\ndmaengine_submit(out_desc);\r\ndma_async_issue_pending(dd->dma_lch_out.chan);\r\ndmaengine_submit(in_desc);\r\ndma_async_issue_pending(dd->dma_lch_in.chan);\r\nreturn 0;\r\n}\r\nstatic int atmel_aes_crypt_cpu_start(struct atmel_aes_dev *dd)\r\n{\r\ndd->flags &= ~AES_FLAGS_DMA;\r\ndd->nb_in_sg = atmel_aes_sg_length(dd->req, dd->in_sg);\r\nif (!dd->nb_in_sg)\r\nreturn -EINVAL;\r\ndd->nb_out_sg = atmel_aes_sg_length(dd->req, dd->out_sg);\r\nif (!dd->nb_out_sg)\r\nreturn -EINVAL;\r\ndd->bufcnt = sg_copy_to_buffer(dd->in_sg, dd->nb_in_sg,\r\ndd->buf_in, dd->total);\r\nif (!dd->bufcnt)\r\nreturn -EINVAL;\r\ndd->total -= dd->bufcnt;\r\natmel_aes_write(dd, AES_IER, AES_INT_DATARDY);\r\natmel_aes_write_n(dd, AES_IDATAR(0), (u32 *) dd->buf_in,\r\ndd->bufcnt >> 2);\r\nreturn 0;\r\n}\r\nstatic int atmel_aes_crypt_dma_start(struct atmel_aes_dev *dd)\r\n{\r\nint err, fast = 0, in, out;\r\nsize_t count;\r\ndma_addr_t addr_in, addr_out;\r\nif ((!dd->in_offset) && (!dd->out_offset)) {\r\nin = IS_ALIGNED((u32)dd->in_sg->offset, sizeof(u32)) &&\r\nIS_ALIGNED(dd->in_sg->length, dd->ctx->block_size);\r\nout = IS_ALIGNED((u32)dd->out_sg->offset, sizeof(u32)) &&\r\nIS_ALIGNED(dd->out_sg->length, dd->ctx->block_size);\r\nfast = in && out;\r\nif (sg_dma_len(dd->in_sg) != sg_dma_len(dd->out_sg))\r\nfast = 0;\r\n}\r\nif (fast) {\r\ncount = min(dd->total, sg_dma_len(dd->in_sg));\r\ncount = min(count, sg_dma_len(dd->out_sg));\r\nerr = dma_map_sg(dd->dev, dd->in_sg, 1, DMA_TO_DEVICE);\r\nif (!err) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\nreturn -EINVAL;\r\n}\r\nerr = dma_map_sg(dd->dev, dd->out_sg, 1,\r\nDMA_FROM_DEVICE);\r\nif (!err) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\ndma_unmap_sg(dd->dev, dd->in_sg, 1,\r\nDMA_TO_DEVICE);\r\nreturn -EINVAL;\r\n}\r\naddr_in = sg_dma_address(dd->in_sg);\r\naddr_out = sg_dma_address(dd->out_sg);\r\ndd->flags |= AES_FLAGS_FAST;\r\n} else {\r\ncount = atmel_aes_sg_copy(&dd->in_sg, &dd->in_offset,\r\ndd->buf_in, dd->buflen, dd->total, 0);\r\naddr_in = dd->dma_addr_in;\r\naddr_out = dd->dma_addr_out;\r\ndd->flags &= ~AES_FLAGS_FAST;\r\n}\r\ndd->total -= count;\r\nerr = atmel_aes_crypt_dma(dd, addr_in, addr_out, count);\r\nif (err && (dd->flags & AES_FLAGS_FAST)) {\r\ndma_unmap_sg(dd->dev, dd->in_sg, 1, DMA_TO_DEVICE);\r\ndma_unmap_sg(dd->dev, dd->out_sg, 1, DMA_TO_DEVICE);\r\n}\r\nreturn err;\r\n}\r\nstatic int atmel_aes_write_ctrl(struct atmel_aes_dev *dd)\r\n{\r\nint err;\r\nu32 valcr = 0, valmr = 0;\r\nerr = atmel_aes_hw_init(dd);\r\nif (err)\r\nreturn err;\r\nif (dd->ctx->keylen == AES_KEYSIZE_128)\r\nvalmr |= AES_MR_KEYSIZE_128;\r\nelse if (dd->ctx->keylen == AES_KEYSIZE_192)\r\nvalmr |= AES_MR_KEYSIZE_192;\r\nelse\r\nvalmr |= AES_MR_KEYSIZE_256;\r\nif (dd->flags & AES_FLAGS_CBC) {\r\nvalmr |= AES_MR_OPMOD_CBC;\r\n} else if (dd->flags & AES_FLAGS_CFB) {\r\nvalmr |= AES_MR_OPMOD_CFB;\r\nif (dd->flags & AES_FLAGS_CFB8)\r\nvalmr |= AES_MR_CFBS_8b;\r\nelse if (dd->flags & AES_FLAGS_CFB16)\r\nvalmr |= AES_MR_CFBS_16b;\r\nelse if (dd->flags & AES_FLAGS_CFB32)\r\nvalmr |= AES_MR_CFBS_32b;\r\nelse if (dd->flags & AES_FLAGS_CFB64)\r\nvalmr |= AES_MR_CFBS_64b;\r\nelse if (dd->flags & AES_FLAGS_CFB128)\r\nvalmr |= AES_MR_CFBS_128b;\r\n} else if (dd->flags & AES_FLAGS_OFB) {\r\nvalmr |= AES_MR_OPMOD_OFB;\r\n} else if (dd->flags & AES_FLAGS_CTR) {\r\nvalmr |= AES_MR_OPMOD_CTR;\r\n} else {\r\nvalmr |= AES_MR_OPMOD_ECB;\r\n}\r\nif (dd->flags & AES_FLAGS_ENCRYPT)\r\nvalmr |= AES_MR_CYPHER_ENC;\r\nif (dd->total > ATMEL_AES_DMA_THRESHOLD) {\r\nvalmr |= AES_MR_SMOD_IDATAR0;\r\nif (dd->caps.has_dualbuff)\r\nvalmr |= AES_MR_DUALBUFF;\r\n} else {\r\nvalmr |= AES_MR_SMOD_AUTO;\r\n}\r\natmel_aes_write(dd, AES_CR, valcr);\r\natmel_aes_write(dd, AES_MR, valmr);\r\natmel_aes_write_n(dd, AES_KEYWR(0), dd->ctx->key,\r\ndd->ctx->keylen >> 2);\r\nif (((dd->flags & AES_FLAGS_CBC) || (dd->flags & AES_FLAGS_CFB) ||\r\n(dd->flags & AES_FLAGS_OFB) || (dd->flags & AES_FLAGS_CTR)) &&\r\ndd->req->info) {\r\natmel_aes_write_n(dd, AES_IVR(0), dd->req->info, 4);\r\n}\r\nreturn 0;\r\n}\r\nstatic int atmel_aes_handle_queue(struct atmel_aes_dev *dd,\r\nstruct ablkcipher_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct atmel_aes_ctx *ctx;\r\nstruct atmel_aes_reqctx *rctx;\r\nunsigned long flags;\r\nint err, ret = 0;\r\nspin_lock_irqsave(&dd->lock, flags);\r\nif (req)\r\nret = ablkcipher_enqueue_request(&dd->queue, req);\r\nif (dd->flags & AES_FLAGS_BUSY) {\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&dd->queue);\r\nasync_req = crypto_dequeue_request(&dd->queue);\r\nif (async_req)\r\ndd->flags |= AES_FLAGS_BUSY;\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ablkcipher_request_cast(async_req);\r\ndd->req = req;\r\ndd->total = req->nbytes;\r\ndd->in_offset = 0;\r\ndd->in_sg = req->src;\r\ndd->out_offset = 0;\r\ndd->out_sg = req->dst;\r\nrctx = ablkcipher_request_ctx(req);\r\nctx = crypto_ablkcipher_ctx(crypto_ablkcipher_reqtfm(req));\r\nrctx->mode &= AES_FLAGS_MODE_MASK;\r\ndd->flags = (dd->flags & ~AES_FLAGS_MODE_MASK) | rctx->mode;\r\ndd->ctx = ctx;\r\nctx->dd = dd;\r\nerr = atmel_aes_write_ctrl(dd);\r\nif (!err) {\r\nif (dd->total > ATMEL_AES_DMA_THRESHOLD)\r\nerr = atmel_aes_crypt_dma_start(dd);\r\nelse\r\nerr = atmel_aes_crypt_cpu_start(dd);\r\n}\r\nif (err) {\r\natmel_aes_finish_req(dd, err);\r\ntasklet_schedule(&dd->queue_task);\r\n}\r\nreturn ret;\r\n}\r\nstatic int atmel_aes_crypt_dma_stop(struct atmel_aes_dev *dd)\r\n{\r\nint err = -EINVAL;\r\nsize_t count;\r\nif (dd->flags & AES_FLAGS_DMA) {\r\nerr = 0;\r\nif (dd->flags & AES_FLAGS_FAST) {\r\ndma_unmap_sg(dd->dev, dd->out_sg, 1, DMA_FROM_DEVICE);\r\ndma_unmap_sg(dd->dev, dd->in_sg, 1, DMA_TO_DEVICE);\r\n} else {\r\ndma_sync_single_for_device(dd->dev, dd->dma_addr_out,\r\ndd->dma_size, DMA_FROM_DEVICE);\r\ncount = atmel_aes_sg_copy(&dd->out_sg, &dd->out_offset,\r\ndd->buf_out, dd->buflen, dd->dma_size, 1);\r\nif (count != dd->dma_size) {\r\nerr = -EINVAL;\r\npr_err("not all data converted: %u\n", count);\r\n}\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic int atmel_aes_buff_init(struct atmel_aes_dev *dd)\r\n{\r\nint err = -ENOMEM;\r\ndd->buf_in = (void *)__get_free_pages(GFP_KERNEL, 0);\r\ndd->buf_out = (void *)__get_free_pages(GFP_KERNEL, 0);\r\ndd->buflen = PAGE_SIZE;\r\ndd->buflen &= ~(AES_BLOCK_SIZE - 1);\r\nif (!dd->buf_in || !dd->buf_out) {\r\ndev_err(dd->dev, "unable to alloc pages.\n");\r\ngoto err_alloc;\r\n}\r\ndd->dma_addr_in = dma_map_single(dd->dev, dd->buf_in,\r\ndd->buflen, DMA_TO_DEVICE);\r\nif (dma_mapping_error(dd->dev, dd->dma_addr_in)) {\r\ndev_err(dd->dev, "dma %d bytes error\n", dd->buflen);\r\nerr = -EINVAL;\r\ngoto err_map_in;\r\n}\r\ndd->dma_addr_out = dma_map_single(dd->dev, dd->buf_out,\r\ndd->buflen, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(dd->dev, dd->dma_addr_out)) {\r\ndev_err(dd->dev, "dma %d bytes error\n", dd->buflen);\r\nerr = -EINVAL;\r\ngoto err_map_out;\r\n}\r\nreturn 0;\r\nerr_map_out:\r\ndma_unmap_single(dd->dev, dd->dma_addr_in, dd->buflen,\r\nDMA_TO_DEVICE);\r\nerr_map_in:\r\nfree_page((unsigned long)dd->buf_out);\r\nfree_page((unsigned long)dd->buf_in);\r\nerr_alloc:\r\nif (err)\r\npr_err("error: %d\n", err);\r\nreturn err;\r\n}\r\nstatic void atmel_aes_buff_cleanup(struct atmel_aes_dev *dd)\r\n{\r\ndma_unmap_single(dd->dev, dd->dma_addr_out, dd->buflen,\r\nDMA_FROM_DEVICE);\r\ndma_unmap_single(dd->dev, dd->dma_addr_in, dd->buflen,\r\nDMA_TO_DEVICE);\r\nfree_page((unsigned long)dd->buf_out);\r\nfree_page((unsigned long)dd->buf_in);\r\n}\r\nstatic int atmel_aes_crypt(struct ablkcipher_request *req, unsigned long mode)\r\n{\r\nstruct atmel_aes_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nstruct atmel_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct atmel_aes_dev *dd;\r\nif (mode & AES_FLAGS_CFB8) {\r\nif (!IS_ALIGNED(req->nbytes, CFB8_BLOCK_SIZE)) {\r\npr_err("request size is not exact amount of CFB8 blocks\n");\r\nreturn -EINVAL;\r\n}\r\nctx->block_size = CFB8_BLOCK_SIZE;\r\n} else if (mode & AES_FLAGS_CFB16) {\r\nif (!IS_ALIGNED(req->nbytes, CFB16_BLOCK_SIZE)) {\r\npr_err("request size is not exact amount of CFB16 blocks\n");\r\nreturn -EINVAL;\r\n}\r\nctx->block_size = CFB16_BLOCK_SIZE;\r\n} else if (mode & AES_FLAGS_CFB32) {\r\nif (!IS_ALIGNED(req->nbytes, CFB32_BLOCK_SIZE)) {\r\npr_err("request size is not exact amount of CFB32 blocks\n");\r\nreturn -EINVAL;\r\n}\r\nctx->block_size = CFB32_BLOCK_SIZE;\r\n} else if (mode & AES_FLAGS_CFB64) {\r\nif (!IS_ALIGNED(req->nbytes, CFB64_BLOCK_SIZE)) {\r\npr_err("request size is not exact amount of CFB64 blocks\n");\r\nreturn -EINVAL;\r\n}\r\nctx->block_size = CFB64_BLOCK_SIZE;\r\n} else {\r\nif (!IS_ALIGNED(req->nbytes, AES_BLOCK_SIZE)) {\r\npr_err("request size is not exact amount of AES blocks\n");\r\nreturn -EINVAL;\r\n}\r\nctx->block_size = AES_BLOCK_SIZE;\r\n}\r\ndd = atmel_aes_find_dev(ctx);\r\nif (!dd)\r\nreturn -ENODEV;\r\nrctx->mode = mode;\r\nreturn atmel_aes_handle_queue(dd, req);\r\n}\r\nstatic bool atmel_aes_filter(struct dma_chan *chan, void *slave)\r\n{\r\nstruct at_dma_slave *sl = slave;\r\nif (sl && sl->dma_dev == chan->device->dev) {\r\nchan->private = sl;\r\nreturn true;\r\n} else {\r\nreturn false;\r\n}\r\n}\r\nstatic int atmel_aes_dma_init(struct atmel_aes_dev *dd,\r\nstruct crypto_platform_data *pdata)\r\n{\r\nint err = -ENOMEM;\r\ndma_cap_mask_t mask;\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\ndd->dma_lch_in.chan = dma_request_slave_channel_compat(mask,\r\natmel_aes_filter, &pdata->dma_slave->rxdata, dd->dev, "tx");\r\nif (!dd->dma_lch_in.chan)\r\ngoto err_dma_in;\r\ndd->dma_lch_in.dma_conf.direction = DMA_MEM_TO_DEV;\r\ndd->dma_lch_in.dma_conf.dst_addr = dd->phys_base +\r\nAES_IDATAR(0);\r\ndd->dma_lch_in.dma_conf.src_maxburst = dd->caps.max_burst_size;\r\ndd->dma_lch_in.dma_conf.src_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = dd->caps.max_burst_size;\r\ndd->dma_lch_in.dma_conf.dst_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_in.dma_conf.device_fc = false;\r\ndd->dma_lch_out.chan = dma_request_slave_channel_compat(mask,\r\natmel_aes_filter, &pdata->dma_slave->txdata, dd->dev, "rx");\r\nif (!dd->dma_lch_out.chan)\r\ngoto err_dma_out;\r\ndd->dma_lch_out.dma_conf.direction = DMA_DEV_TO_MEM;\r\ndd->dma_lch_out.dma_conf.src_addr = dd->phys_base +\r\nAES_ODATAR(0);\r\ndd->dma_lch_out.dma_conf.src_maxburst = dd->caps.max_burst_size;\r\ndd->dma_lch_out.dma_conf.src_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_out.dma_conf.dst_maxburst = dd->caps.max_burst_size;\r\ndd->dma_lch_out.dma_conf.dst_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_out.dma_conf.device_fc = false;\r\nreturn 0;\r\nerr_dma_out:\r\ndma_release_channel(dd->dma_lch_in.chan);\r\nerr_dma_in:\r\ndev_warn(dd->dev, "no DMA channel available\n");\r\nreturn err;\r\n}\r\nstatic void atmel_aes_dma_cleanup(struct atmel_aes_dev *dd)\r\n{\r\ndma_release_channel(dd->dma_lch_in.chan);\r\ndma_release_channel(dd->dma_lch_out.chan);\r\n}\r\nstatic int atmel_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct atmel_aes_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nif (keylen != AES_KEYSIZE_128 && keylen != AES_KEYSIZE_192 &&\r\nkeylen != AES_KEYSIZE_256) {\r\ncrypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nmemcpy(ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\nreturn 0;\r\n}\r\nstatic int atmel_aes_ecb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT);\r\n}\r\nstatic int atmel_aes_ecb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\n0);\r\n}\r\nstatic int atmel_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_CBC);\r\n}\r\nstatic int atmel_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_CBC);\r\n}\r\nstatic int atmel_aes_ofb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_OFB);\r\n}\r\nstatic int atmel_aes_ofb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_OFB);\r\n}\r\nstatic int atmel_aes_cfb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_CFB | AES_FLAGS_CFB128);\r\n}\r\nstatic int atmel_aes_cfb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_CFB | AES_FLAGS_CFB128);\r\n}\r\nstatic int atmel_aes_cfb64_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_CFB | AES_FLAGS_CFB64);\r\n}\r\nstatic int atmel_aes_cfb64_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_CFB | AES_FLAGS_CFB64);\r\n}\r\nstatic int atmel_aes_cfb32_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_CFB | AES_FLAGS_CFB32);\r\n}\r\nstatic int atmel_aes_cfb32_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_CFB | AES_FLAGS_CFB32);\r\n}\r\nstatic int atmel_aes_cfb16_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_CFB | AES_FLAGS_CFB16);\r\n}\r\nstatic int atmel_aes_cfb16_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_CFB | AES_FLAGS_CFB16);\r\n}\r\nstatic int atmel_aes_cfb8_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_CFB | AES_FLAGS_CFB8);\r\n}\r\nstatic int atmel_aes_cfb8_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_CFB | AES_FLAGS_CFB8);\r\n}\r\nstatic int atmel_aes_ctr_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_ENCRYPT | AES_FLAGS_CTR);\r\n}\r\nstatic int atmel_aes_ctr_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn atmel_aes_crypt(req,\r\nAES_FLAGS_CTR);\r\n}\r\nstatic int atmel_aes_cra_init(struct crypto_tfm *tfm)\r\n{\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct atmel_aes_reqctx);\r\nreturn 0;\r\n}\r\nstatic void atmel_aes_cra_exit(struct crypto_tfm *tfm)\r\n{\r\n}\r\nstatic void atmel_aes_queue_task(unsigned long data)\r\n{\r\nstruct atmel_aes_dev *dd = (struct atmel_aes_dev *)data;\r\natmel_aes_handle_queue(dd, NULL);\r\n}\r\nstatic void atmel_aes_done_task(unsigned long data)\r\n{\r\nstruct atmel_aes_dev *dd = (struct atmel_aes_dev *) data;\r\nint err;\r\nif (!(dd->flags & AES_FLAGS_DMA)) {\r\natmel_aes_read_n(dd, AES_ODATAR(0), (u32 *) dd->buf_out,\r\ndd->bufcnt >> 2);\r\nif (sg_copy_from_buffer(dd->out_sg, dd->nb_out_sg,\r\ndd->buf_out, dd->bufcnt))\r\nerr = 0;\r\nelse\r\nerr = -EINVAL;\r\ngoto cpu_end;\r\n}\r\nerr = atmel_aes_crypt_dma_stop(dd);\r\nerr = dd->err ? : err;\r\nif (dd->total && !err) {\r\nif (dd->flags & AES_FLAGS_FAST) {\r\ndd->in_sg = sg_next(dd->in_sg);\r\ndd->out_sg = sg_next(dd->out_sg);\r\nif (!dd->in_sg || !dd->out_sg)\r\nerr = -EINVAL;\r\n}\r\nif (!err)\r\nerr = atmel_aes_crypt_dma_start(dd);\r\nif (!err)\r\nreturn;\r\n}\r\ncpu_end:\r\natmel_aes_finish_req(dd, err);\r\natmel_aes_handle_queue(dd, NULL);\r\n}\r\nstatic irqreturn_t atmel_aes_irq(int irq, void *dev_id)\r\n{\r\nstruct atmel_aes_dev *aes_dd = dev_id;\r\nu32 reg;\r\nreg = atmel_aes_read(aes_dd, AES_ISR);\r\nif (reg & atmel_aes_read(aes_dd, AES_IMR)) {\r\natmel_aes_write(aes_dd, AES_IDR, reg);\r\nif (AES_FLAGS_BUSY & aes_dd->flags)\r\ntasklet_schedule(&aes_dd->done_task);\r\nelse\r\ndev_warn(aes_dd->dev, "AES interrupt when no active requests.\n");\r\nreturn IRQ_HANDLED;\r\n}\r\nreturn IRQ_NONE;\r\n}\r\nstatic void atmel_aes_unregister_algs(struct atmel_aes_dev *dd)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++)\r\ncrypto_unregister_alg(&aes_algs[i]);\r\nif (dd->caps.has_cfb64)\r\ncrypto_unregister_alg(&aes_cfb64_alg);\r\n}\r\nstatic int atmel_aes_register_algs(struct atmel_aes_dev *dd)\r\n{\r\nint err, i, j;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\r\nerr = crypto_register_alg(&aes_algs[i]);\r\nif (err)\r\ngoto err_aes_algs;\r\n}\r\nif (dd->caps.has_cfb64) {\r\nerr = crypto_register_alg(&aes_cfb64_alg);\r\nif (err)\r\ngoto err_aes_cfb64_alg;\r\n}\r\nreturn 0;\r\nerr_aes_cfb64_alg:\r\ni = ARRAY_SIZE(aes_algs);\r\nerr_aes_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_alg(&aes_algs[j]);\r\nreturn err;\r\n}\r\nstatic void atmel_aes_get_cap(struct atmel_aes_dev *dd)\r\n{\r\ndd->caps.has_dualbuff = 0;\r\ndd->caps.has_cfb64 = 0;\r\ndd->caps.max_burst_size = 1;\r\nswitch (dd->hw_version & 0xff0) {\r\ncase 0x130:\r\ndd->caps.has_dualbuff = 1;\r\ndd->caps.has_cfb64 = 1;\r\ndd->caps.max_burst_size = 4;\r\nbreak;\r\ncase 0x120:\r\nbreak;\r\ndefault:\r\ndev_warn(dd->dev,\r\n"Unmanaged aes version, set minimum capabilities\n");\r\nbreak;\r\n}\r\n}\r\nstatic struct crypto_platform_data *atmel_aes_of_init(struct platform_device *pdev)\r\n{\r\nstruct device_node *np = pdev->dev.of_node;\r\nstruct crypto_platform_data *pdata;\r\nif (!np) {\r\ndev_err(&pdev->dev, "device node not found\n");\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\npdata = devm_kzalloc(&pdev->dev, sizeof(*pdata), GFP_KERNEL);\r\nif (!pdata) {\r\ndev_err(&pdev->dev, "could not allocate memory for pdata\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npdata->dma_slave = devm_kzalloc(&pdev->dev,\r\nsizeof(*(pdata->dma_slave)),\r\nGFP_KERNEL);\r\nif (!pdata->dma_slave) {\r\ndev_err(&pdev->dev, "could not allocate memory for dma_slave\n");\r\ndevm_kfree(&pdev->dev, pdata);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nreturn pdata;\r\n}\r\nstatic inline struct crypto_platform_data *atmel_aes_of_init(struct platform_device *pdev)\r\n{\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic int atmel_aes_probe(struct platform_device *pdev)\r\n{\r\nstruct atmel_aes_dev *aes_dd;\r\nstruct crypto_platform_data *pdata;\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *aes_res;\r\nunsigned long aes_phys_size;\r\nint err;\r\npdata = pdev->dev.platform_data;\r\nif (!pdata) {\r\npdata = atmel_aes_of_init(pdev);\r\nif (IS_ERR(pdata)) {\r\nerr = PTR_ERR(pdata);\r\ngoto aes_dd_err;\r\n}\r\n}\r\nif (!pdata->dma_slave) {\r\nerr = -ENXIO;\r\ngoto aes_dd_err;\r\n}\r\naes_dd = kzalloc(sizeof(struct atmel_aes_dev), GFP_KERNEL);\r\nif (aes_dd == NULL) {\r\ndev_err(dev, "unable to alloc data struct.\n");\r\nerr = -ENOMEM;\r\ngoto aes_dd_err;\r\n}\r\naes_dd->dev = dev;\r\nplatform_set_drvdata(pdev, aes_dd);\r\nINIT_LIST_HEAD(&aes_dd->list);\r\ntasklet_init(&aes_dd->done_task, atmel_aes_done_task,\r\n(unsigned long)aes_dd);\r\ntasklet_init(&aes_dd->queue_task, atmel_aes_queue_task,\r\n(unsigned long)aes_dd);\r\ncrypto_init_queue(&aes_dd->queue, ATMEL_AES_QUEUE_LENGTH);\r\naes_dd->irq = -1;\r\naes_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!aes_res) {\r\ndev_err(dev, "no MEM resource info\n");\r\nerr = -ENODEV;\r\ngoto res_err;\r\n}\r\naes_dd->phys_base = aes_res->start;\r\naes_phys_size = resource_size(aes_res);\r\naes_dd->irq = platform_get_irq(pdev, 0);\r\nif (aes_dd->irq < 0) {\r\ndev_err(dev, "no IRQ resource info\n");\r\nerr = aes_dd->irq;\r\ngoto aes_irq_err;\r\n}\r\nerr = request_irq(aes_dd->irq, atmel_aes_irq, IRQF_SHARED, "atmel-aes",\r\naes_dd);\r\nif (err) {\r\ndev_err(dev, "unable to request aes irq.\n");\r\ngoto aes_irq_err;\r\n}\r\naes_dd->iclk = clk_get(&pdev->dev, "aes_clk");\r\nif (IS_ERR(aes_dd->iclk)) {\r\ndev_err(dev, "clock intialization failed.\n");\r\nerr = PTR_ERR(aes_dd->iclk);\r\ngoto clk_err;\r\n}\r\naes_dd->io_base = ioremap(aes_dd->phys_base, aes_phys_size);\r\nif (!aes_dd->io_base) {\r\ndev_err(dev, "can't ioremap\n");\r\nerr = -ENOMEM;\r\ngoto aes_io_err;\r\n}\r\natmel_aes_hw_version_init(aes_dd);\r\natmel_aes_get_cap(aes_dd);\r\nerr = atmel_aes_buff_init(aes_dd);\r\nif (err)\r\ngoto err_aes_buff;\r\nerr = atmel_aes_dma_init(aes_dd, pdata);\r\nif (err)\r\ngoto err_aes_dma;\r\nspin_lock(&atmel_aes.lock);\r\nlist_add_tail(&aes_dd->list, &atmel_aes.dev_list);\r\nspin_unlock(&atmel_aes.lock);\r\nerr = atmel_aes_register_algs(aes_dd);\r\nif (err)\r\ngoto err_algs;\r\ndev_info(dev, "Atmel AES - Using %s, %s for DMA transfers\n",\r\ndma_chan_name(aes_dd->dma_lch_in.chan),\r\ndma_chan_name(aes_dd->dma_lch_out.chan));\r\nreturn 0;\r\nerr_algs:\r\nspin_lock(&atmel_aes.lock);\r\nlist_del(&aes_dd->list);\r\nspin_unlock(&atmel_aes.lock);\r\natmel_aes_dma_cleanup(aes_dd);\r\nerr_aes_dma:\r\natmel_aes_buff_cleanup(aes_dd);\r\nerr_aes_buff:\r\niounmap(aes_dd->io_base);\r\naes_io_err:\r\nclk_put(aes_dd->iclk);\r\nclk_err:\r\nfree_irq(aes_dd->irq, aes_dd);\r\naes_irq_err:\r\nres_err:\r\ntasklet_kill(&aes_dd->done_task);\r\ntasklet_kill(&aes_dd->queue_task);\r\nkfree(aes_dd);\r\naes_dd = NULL;\r\naes_dd_err:\r\ndev_err(dev, "initialization failed.\n");\r\nreturn err;\r\n}\r\nstatic int atmel_aes_remove(struct platform_device *pdev)\r\n{\r\nstatic struct atmel_aes_dev *aes_dd;\r\naes_dd = platform_get_drvdata(pdev);\r\nif (!aes_dd)\r\nreturn -ENODEV;\r\nspin_lock(&atmel_aes.lock);\r\nlist_del(&aes_dd->list);\r\nspin_unlock(&atmel_aes.lock);\r\natmel_aes_unregister_algs(aes_dd);\r\ntasklet_kill(&aes_dd->done_task);\r\ntasklet_kill(&aes_dd->queue_task);\r\natmel_aes_dma_cleanup(aes_dd);\r\niounmap(aes_dd->io_base);\r\nclk_put(aes_dd->iclk);\r\nif (aes_dd->irq > 0)\r\nfree_irq(aes_dd->irq, aes_dd);\r\nkfree(aes_dd);\r\naes_dd = NULL;\r\nreturn 0;\r\n}
