static struct mempolicy *get_task_policy(struct task_struct *p)\r\n{\r\nstruct mempolicy *pol = p->mempolicy;\r\nif (!pol) {\r\nint node = numa_node_id();\r\nif (node != NUMA_NO_NODE) {\r\npol = &preferred_node_policy[node];\r\nif (!pol->mode)\r\npol = NULL;\r\n}\r\n}\r\nreturn pol;\r\n}\r\nstatic int is_valid_nodemask(const nodemask_t *nodemask)\r\n{\r\nreturn nodes_intersects(*nodemask, node_states[N_MEMORY]);\r\n}\r\nstatic inline int mpol_store_user_nodemask(const struct mempolicy *pol)\r\n{\r\nreturn pol->flags & MPOL_MODE_FLAGS;\r\n}\r\nstatic void mpol_relative_nodemask(nodemask_t *ret, const nodemask_t *orig,\r\nconst nodemask_t *rel)\r\n{\r\nnodemask_t tmp;\r\nnodes_fold(tmp, *orig, nodes_weight(*rel));\r\nnodes_onto(*ret, tmp, *rel);\r\n}\r\nstatic int mpol_new_interleave(struct mempolicy *pol, const nodemask_t *nodes)\r\n{\r\nif (nodes_empty(*nodes))\r\nreturn -EINVAL;\r\npol->v.nodes = *nodes;\r\nreturn 0;\r\n}\r\nstatic int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\r\n{\r\nif (!nodes)\r\npol->flags |= MPOL_F_LOCAL;\r\nelse if (nodes_empty(*nodes))\r\nreturn -EINVAL;\r\nelse\r\npol->v.preferred_node = first_node(*nodes);\r\nreturn 0;\r\n}\r\nstatic int mpol_new_bind(struct mempolicy *pol, const nodemask_t *nodes)\r\n{\r\nif (!is_valid_nodemask(nodes))\r\nreturn -EINVAL;\r\npol->v.nodes = *nodes;\r\nreturn 0;\r\n}\r\nstatic int mpol_set_nodemask(struct mempolicy *pol,\r\nconst nodemask_t *nodes, struct nodemask_scratch *nsc)\r\n{\r\nint ret;\r\nif (pol == NULL)\r\nreturn 0;\r\nnodes_and(nsc->mask1,\r\ncpuset_current_mems_allowed, node_states[N_MEMORY]);\r\nVM_BUG_ON(!nodes);\r\nif (pol->mode == MPOL_PREFERRED && nodes_empty(*nodes))\r\nnodes = NULL;\r\nelse {\r\nif (pol->flags & MPOL_F_RELATIVE_NODES)\r\nmpol_relative_nodemask(&nsc->mask2, nodes,&nsc->mask1);\r\nelse\r\nnodes_and(nsc->mask2, *nodes, nsc->mask1);\r\nif (mpol_store_user_nodemask(pol))\r\npol->w.user_nodemask = *nodes;\r\nelse\r\npol->w.cpuset_mems_allowed =\r\ncpuset_current_mems_allowed;\r\n}\r\nif (nodes)\r\nret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\r\nelse\r\nret = mpol_ops[pol->mode].create(pol, NULL);\r\nreturn ret;\r\n}\r\nstatic struct mempolicy *mpol_new(unsigned short mode, unsigned short flags,\r\nnodemask_t *nodes)\r\n{\r\nstruct mempolicy *policy;\r\npr_debug("setting mode %d flags %d nodes[0] %lx\n",\r\nmode, flags, nodes ? nodes_addr(*nodes)[0] : NUMA_NO_NODE);\r\nif (mode == MPOL_DEFAULT) {\r\nif (nodes && !nodes_empty(*nodes))\r\nreturn ERR_PTR(-EINVAL);\r\nreturn NULL;\r\n}\r\nVM_BUG_ON(!nodes);\r\nif (mode == MPOL_PREFERRED) {\r\nif (nodes_empty(*nodes)) {\r\nif (((flags & MPOL_F_STATIC_NODES) ||\r\n(flags & MPOL_F_RELATIVE_NODES)))\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\n} else if (mode == MPOL_LOCAL) {\r\nif (!nodes_empty(*nodes))\r\nreturn ERR_PTR(-EINVAL);\r\nmode = MPOL_PREFERRED;\r\n} else if (nodes_empty(*nodes))\r\nreturn ERR_PTR(-EINVAL);\r\npolicy = kmem_cache_alloc(policy_cache, GFP_KERNEL);\r\nif (!policy)\r\nreturn ERR_PTR(-ENOMEM);\r\natomic_set(&policy->refcnt, 1);\r\npolicy->mode = mode;\r\npolicy->flags = flags;\r\nreturn policy;\r\n}\r\nvoid __mpol_put(struct mempolicy *p)\r\n{\r\nif (!atomic_dec_and_test(&p->refcnt))\r\nreturn;\r\nkmem_cache_free(policy_cache, p);\r\n}\r\nstatic void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes,\r\nenum mpol_rebind_step step)\r\n{\r\n}\r\nstatic void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes,\r\nenum mpol_rebind_step step)\r\n{\r\nnodemask_t tmp;\r\nif (pol->flags & MPOL_F_STATIC_NODES)\r\nnodes_and(tmp, pol->w.user_nodemask, *nodes);\r\nelse if (pol->flags & MPOL_F_RELATIVE_NODES)\r\nmpol_relative_nodemask(&tmp, &pol->w.user_nodemask, nodes);\r\nelse {\r\nif (step == MPOL_REBIND_ONCE || step == MPOL_REBIND_STEP1) {\r\nnodes_remap(tmp, pol->v.nodes,\r\npol->w.cpuset_mems_allowed, *nodes);\r\npol->w.cpuset_mems_allowed = step ? tmp : *nodes;\r\n} else if (step == MPOL_REBIND_STEP2) {\r\ntmp = pol->w.cpuset_mems_allowed;\r\npol->w.cpuset_mems_allowed = *nodes;\r\n} else\r\nBUG();\r\n}\r\nif (nodes_empty(tmp))\r\ntmp = *nodes;\r\nif (step == MPOL_REBIND_STEP1)\r\nnodes_or(pol->v.nodes, pol->v.nodes, tmp);\r\nelse if (step == MPOL_REBIND_ONCE || step == MPOL_REBIND_STEP2)\r\npol->v.nodes = tmp;\r\nelse\r\nBUG();\r\nif (!node_isset(current->il_next, tmp)) {\r\ncurrent->il_next = next_node(current->il_next, tmp);\r\nif (current->il_next >= MAX_NUMNODES)\r\ncurrent->il_next = first_node(tmp);\r\nif (current->il_next >= MAX_NUMNODES)\r\ncurrent->il_next = numa_node_id();\r\n}\r\n}\r\nstatic void mpol_rebind_preferred(struct mempolicy *pol,\r\nconst nodemask_t *nodes,\r\nenum mpol_rebind_step step)\r\n{\r\nnodemask_t tmp;\r\nif (pol->flags & MPOL_F_STATIC_NODES) {\r\nint node = first_node(pol->w.user_nodemask);\r\nif (node_isset(node, *nodes)) {\r\npol->v.preferred_node = node;\r\npol->flags &= ~MPOL_F_LOCAL;\r\n} else\r\npol->flags |= MPOL_F_LOCAL;\r\n} else if (pol->flags & MPOL_F_RELATIVE_NODES) {\r\nmpol_relative_nodemask(&tmp, &pol->w.user_nodemask, nodes);\r\npol->v.preferred_node = first_node(tmp);\r\n} else if (!(pol->flags & MPOL_F_LOCAL)) {\r\npol->v.preferred_node = node_remap(pol->v.preferred_node,\r\npol->w.cpuset_mems_allowed,\r\n*nodes);\r\npol->w.cpuset_mems_allowed = *nodes;\r\n}\r\n}\r\nstatic void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask,\r\nenum mpol_rebind_step step)\r\n{\r\nif (!pol)\r\nreturn;\r\nif (!mpol_store_user_nodemask(pol) && step == MPOL_REBIND_ONCE &&\r\nnodes_equal(pol->w.cpuset_mems_allowed, *newmask))\r\nreturn;\r\nif (step == MPOL_REBIND_STEP1 && (pol->flags & MPOL_F_REBINDING))\r\nreturn;\r\nif (step == MPOL_REBIND_STEP2 && !(pol->flags & MPOL_F_REBINDING))\r\nBUG();\r\nif (step == MPOL_REBIND_STEP1)\r\npol->flags |= MPOL_F_REBINDING;\r\nelse if (step == MPOL_REBIND_STEP2)\r\npol->flags &= ~MPOL_F_REBINDING;\r\nelse if (step >= MPOL_REBIND_NSTEP)\r\nBUG();\r\nmpol_ops[pol->mode].rebind(pol, newmask, step);\r\n}\r\nvoid mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,\r\nenum mpol_rebind_step step)\r\n{\r\nmpol_rebind_policy(tsk->mempolicy, new, step);\r\n}\r\nvoid mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)\r\n{\r\nstruct vm_area_struct *vma;\r\ndown_write(&mm->mmap_sem);\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next)\r\nmpol_rebind_policy(vma->vm_policy, new, MPOL_REBIND_ONCE);\r\nup_write(&mm->mmap_sem);\r\n}\r\nstatic int queue_pages_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\r\nunsigned long addr, unsigned long end,\r\nconst nodemask_t *nodes, unsigned long flags,\r\nvoid *private)\r\n{\r\npte_t *orig_pte;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\norig_pte = pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\r\ndo {\r\nstruct page *page;\r\nint nid;\r\nif (!pte_present(*pte))\r\ncontinue;\r\npage = vm_normal_page(vma, addr, *pte);\r\nif (!page)\r\ncontinue;\r\nif (PageReserved(page))\r\ncontinue;\r\nnid = page_to_nid(page);\r\nif (node_isset(nid, *nodes) == !!(flags & MPOL_MF_INVERT))\r\ncontinue;\r\nif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL))\r\nmigrate_page_add(page, private, flags);\r\nelse\r\nbreak;\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\npte_unmap_unlock(orig_pte, ptl);\r\nreturn addr != end;\r\n}\r\nstatic void queue_pages_hugetlb_pmd_range(struct vm_area_struct *vma,\r\npmd_t *pmd, const nodemask_t *nodes, unsigned long flags,\r\nvoid *private)\r\n{\r\n#ifdef CONFIG_HUGETLB_PAGE\r\nint nid;\r\nstruct page *page;\r\nspinlock_t *ptl;\r\npte_t entry;\r\nptl = huge_pte_lock(hstate_vma(vma), vma->vm_mm, (pte_t *)pmd);\r\nentry = huge_ptep_get((pte_t *)pmd);\r\nif (!pte_present(entry))\r\ngoto unlock;\r\npage = pte_page(entry);\r\nnid = page_to_nid(page);\r\nif (node_isset(nid, *nodes) == !!(flags & MPOL_MF_INVERT))\r\ngoto unlock;\r\nif (flags & (MPOL_MF_MOVE_ALL) ||\r\n(flags & MPOL_MF_MOVE && page_mapcount(page) == 1))\r\nisolate_huge_page(page, private);\r\nunlock:\r\nspin_unlock(ptl);\r\n#else\r\nBUG();\r\n#endif\r\n}\r\nstatic inline int queue_pages_pmd_range(struct vm_area_struct *vma, pud_t *pud,\r\nunsigned long addr, unsigned long end,\r\nconst nodemask_t *nodes, unsigned long flags,\r\nvoid *private)\r\n{\r\npmd_t *pmd;\r\nunsigned long next;\r\npmd = pmd_offset(pud, addr);\r\ndo {\r\nnext = pmd_addr_end(addr, end);\r\nif (!pmd_present(*pmd))\r\ncontinue;\r\nif (pmd_huge(*pmd) && is_vm_hugetlb_page(vma)) {\r\nqueue_pages_hugetlb_pmd_range(vma, pmd, nodes,\r\nflags, private);\r\ncontinue;\r\n}\r\nsplit_huge_page_pmd(vma, addr, pmd);\r\nif (pmd_none_or_trans_huge_or_clear_bad(pmd))\r\ncontinue;\r\nif (queue_pages_pte_range(vma, pmd, addr, next, nodes,\r\nflags, private))\r\nreturn -EIO;\r\n} while (pmd++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nstatic inline int queue_pages_pud_range(struct vm_area_struct *vma, pgd_t *pgd,\r\nunsigned long addr, unsigned long end,\r\nconst nodemask_t *nodes, unsigned long flags,\r\nvoid *private)\r\n{\r\npud_t *pud;\r\nunsigned long next;\r\npud = pud_offset(pgd, addr);\r\ndo {\r\nnext = pud_addr_end(addr, end);\r\nif (pud_huge(*pud) && is_vm_hugetlb_page(vma))\r\ncontinue;\r\nif (pud_none_or_clear_bad(pud))\r\ncontinue;\r\nif (queue_pages_pmd_range(vma, pud, addr, next, nodes,\r\nflags, private))\r\nreturn -EIO;\r\n} while (pud++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nstatic inline int queue_pages_pgd_range(struct vm_area_struct *vma,\r\nunsigned long addr, unsigned long end,\r\nconst nodemask_t *nodes, unsigned long flags,\r\nvoid *private)\r\n{\r\npgd_t *pgd;\r\nunsigned long next;\r\npgd = pgd_offset(vma->vm_mm, addr);\r\ndo {\r\nnext = pgd_addr_end(addr, end);\r\nif (pgd_none_or_clear_bad(pgd))\r\ncontinue;\r\nif (queue_pages_pud_range(vma, pgd, addr, next, nodes,\r\nflags, private))\r\nreturn -EIO;\r\n} while (pgd++, addr = next, addr != end);\r\nreturn 0;\r\n}\r\nunsigned long change_prot_numa(struct vm_area_struct *vma,\r\nunsigned long addr, unsigned long end)\r\n{\r\nint nr_updated;\r\nnr_updated = change_protection(vma, addr, end, vma->vm_page_prot, 0, 1);\r\nif (nr_updated)\r\ncount_vm_numa_events(NUMA_PTE_UPDATES, nr_updated);\r\nreturn nr_updated;\r\n}\r\nstatic unsigned long change_prot_numa(struct vm_area_struct *vma,\r\nunsigned long addr, unsigned long end)\r\n{\r\nreturn 0;\r\n}\r\nstatic int\r\nqueue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,\r\nconst nodemask_t *nodes, unsigned long flags, void *private)\r\n{\r\nint err = 0;\r\nstruct vm_area_struct *vma, *prev;\r\nvma = find_vma(mm, start);\r\nif (!vma)\r\nreturn -EFAULT;\r\nprev = NULL;\r\nfor (; vma && vma->vm_start < end; vma = vma->vm_next) {\r\nunsigned long endvma = vma->vm_end;\r\nif (endvma > end)\r\nendvma = end;\r\nif (vma->vm_start > start)\r\nstart = vma->vm_start;\r\nif (!(flags & MPOL_MF_DISCONTIG_OK)) {\r\nif (!vma->vm_next && vma->vm_end < end)\r\nreturn -EFAULT;\r\nif (prev && prev->vm_end < vma->vm_start)\r\nreturn -EFAULT;\r\n}\r\nif (flags & MPOL_MF_LAZY) {\r\nchange_prot_numa(vma, start, endvma);\r\ngoto next;\r\n}\r\nif ((flags & MPOL_MF_STRICT) ||\r\n((flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) &&\r\nvma_migratable(vma))) {\r\nerr = queue_pages_pgd_range(vma, start, endvma, nodes,\r\nflags, private);\r\nif (err)\r\nbreak;\r\n}\r\nnext:\r\nprev = vma;\r\n}\r\nreturn err;\r\n}\r\nstatic int vma_replace_policy(struct vm_area_struct *vma,\r\nstruct mempolicy *pol)\r\n{\r\nint err;\r\nstruct mempolicy *old;\r\nstruct mempolicy *new;\r\npr_debug("vma %lx-%lx/%lx vm_ops %p vm_file %p set_policy %p\n",\r\nvma->vm_start, vma->vm_end, vma->vm_pgoff,\r\nvma->vm_ops, vma->vm_file,\r\nvma->vm_ops ? vma->vm_ops->set_policy : NULL);\r\nnew = mpol_dup(pol);\r\nif (IS_ERR(new))\r\nreturn PTR_ERR(new);\r\nif (vma->vm_ops && vma->vm_ops->set_policy) {\r\nerr = vma->vm_ops->set_policy(vma, new);\r\nif (err)\r\ngoto err_out;\r\n}\r\nold = vma->vm_policy;\r\nvma->vm_policy = new;\r\nmpol_put(old);\r\nreturn 0;\r\nerr_out:\r\nmpol_put(new);\r\nreturn err;\r\n}\r\nstatic int mbind_range(struct mm_struct *mm, unsigned long start,\r\nunsigned long end, struct mempolicy *new_pol)\r\n{\r\nstruct vm_area_struct *next;\r\nstruct vm_area_struct *prev;\r\nstruct vm_area_struct *vma;\r\nint err = 0;\r\npgoff_t pgoff;\r\nunsigned long vmstart;\r\nunsigned long vmend;\r\nvma = find_vma(mm, start);\r\nif (!vma || vma->vm_start > start)\r\nreturn -EFAULT;\r\nprev = vma->vm_prev;\r\nif (start > vma->vm_start)\r\nprev = vma;\r\nfor (; vma && vma->vm_start < end; prev = vma, vma = next) {\r\nnext = vma->vm_next;\r\nvmstart = max(start, vma->vm_start);\r\nvmend = min(end, vma->vm_end);\r\nif (mpol_equal(vma_policy(vma), new_pol))\r\ncontinue;\r\npgoff = vma->vm_pgoff +\r\n((vmstart - vma->vm_start) >> PAGE_SHIFT);\r\nprev = vma_merge(mm, prev, vmstart, vmend, vma->vm_flags,\r\nvma->anon_vma, vma->vm_file, pgoff,\r\nnew_pol);\r\nif (prev) {\r\nvma = prev;\r\nnext = vma->vm_next;\r\nif (mpol_equal(vma_policy(vma), new_pol))\r\ncontinue;\r\ngoto replace;\r\n}\r\nif (vma->vm_start != vmstart) {\r\nerr = split_vma(vma->vm_mm, vma, vmstart, 1);\r\nif (err)\r\ngoto out;\r\n}\r\nif (vma->vm_end != vmend) {\r\nerr = split_vma(vma->vm_mm, vma, vmend, 0);\r\nif (err)\r\ngoto out;\r\n}\r\nreplace:\r\nerr = vma_replace_policy(vma, new_pol);\r\nif (err)\r\ngoto out;\r\n}\r\nout:\r\nreturn err;\r\n}\r\nstatic long do_set_mempolicy(unsigned short mode, unsigned short flags,\r\nnodemask_t *nodes)\r\n{\r\nstruct mempolicy *new, *old;\r\nstruct mm_struct *mm = current->mm;\r\nNODEMASK_SCRATCH(scratch);\r\nint ret;\r\nif (!scratch)\r\nreturn -ENOMEM;\r\nnew = mpol_new(mode, flags, nodes);\r\nif (IS_ERR(new)) {\r\nret = PTR_ERR(new);\r\ngoto out;\r\n}\r\nif (mm)\r\ndown_write(&mm->mmap_sem);\r\ntask_lock(current);\r\nret = mpol_set_nodemask(new, nodes, scratch);\r\nif (ret) {\r\ntask_unlock(current);\r\nif (mm)\r\nup_write(&mm->mmap_sem);\r\nmpol_put(new);\r\ngoto out;\r\n}\r\nold = current->mempolicy;\r\ncurrent->mempolicy = new;\r\nif (new && new->mode == MPOL_INTERLEAVE &&\r\nnodes_weight(new->v.nodes))\r\ncurrent->il_next = first_node(new->v.nodes);\r\ntask_unlock(current);\r\nif (mm)\r\nup_write(&mm->mmap_sem);\r\nmpol_put(old);\r\nret = 0;\r\nout:\r\nNODEMASK_SCRATCH_FREE(scratch);\r\nreturn ret;\r\n}\r\nstatic void get_policy_nodemask(struct mempolicy *p, nodemask_t *nodes)\r\n{\r\nnodes_clear(*nodes);\r\nif (p == &default_policy)\r\nreturn;\r\nswitch (p->mode) {\r\ncase MPOL_BIND:\r\ncase MPOL_INTERLEAVE:\r\n*nodes = p->v.nodes;\r\nbreak;\r\ncase MPOL_PREFERRED:\r\nif (!(p->flags & MPOL_F_LOCAL))\r\nnode_set(p->v.preferred_node, *nodes);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic int lookup_node(struct mm_struct *mm, unsigned long addr)\r\n{\r\nstruct page *p;\r\nint err;\r\nerr = get_user_pages(current, mm, addr & PAGE_MASK, 1, 0, 0, &p, NULL);\r\nif (err >= 0) {\r\nerr = page_to_nid(p);\r\nput_page(p);\r\n}\r\nreturn err;\r\n}\r\nstatic long do_get_mempolicy(int *policy, nodemask_t *nmask,\r\nunsigned long addr, unsigned long flags)\r\n{\r\nint err;\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma = NULL;\r\nstruct mempolicy *pol = current->mempolicy;\r\nif (flags &\r\n~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))\r\nreturn -EINVAL;\r\nif (flags & MPOL_F_MEMS_ALLOWED) {\r\nif (flags & (MPOL_F_NODE|MPOL_F_ADDR))\r\nreturn -EINVAL;\r\n*policy = 0;\r\ntask_lock(current);\r\n*nmask = cpuset_current_mems_allowed;\r\ntask_unlock(current);\r\nreturn 0;\r\n}\r\nif (flags & MPOL_F_ADDR) {\r\ndown_read(&mm->mmap_sem);\r\nvma = find_vma_intersection(mm, addr, addr+1);\r\nif (!vma) {\r\nup_read(&mm->mmap_sem);\r\nreturn -EFAULT;\r\n}\r\nif (vma->vm_ops && vma->vm_ops->get_policy)\r\npol = vma->vm_ops->get_policy(vma, addr);\r\nelse\r\npol = vma->vm_policy;\r\n} else if (addr)\r\nreturn -EINVAL;\r\nif (!pol)\r\npol = &default_policy;\r\nif (flags & MPOL_F_NODE) {\r\nif (flags & MPOL_F_ADDR) {\r\nerr = lookup_node(mm, addr);\r\nif (err < 0)\r\ngoto out;\r\n*policy = err;\r\n} else if (pol == current->mempolicy &&\r\npol->mode == MPOL_INTERLEAVE) {\r\n*policy = current->il_next;\r\n} else {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\n} else {\r\n*policy = pol == &default_policy ? MPOL_DEFAULT :\r\npol->mode;\r\n*policy |= (pol->flags & MPOL_MODE_FLAGS);\r\n}\r\nif (vma) {\r\nup_read(&current->mm->mmap_sem);\r\nvma = NULL;\r\n}\r\nerr = 0;\r\nif (nmask) {\r\nif (mpol_store_user_nodemask(pol)) {\r\n*nmask = pol->w.user_nodemask;\r\n} else {\r\ntask_lock(current);\r\nget_policy_nodemask(pol, nmask);\r\ntask_unlock(current);\r\n}\r\n}\r\nout:\r\nmpol_cond_put(pol);\r\nif (vma)\r\nup_read(&current->mm->mmap_sem);\r\nreturn err;\r\n}\r\nstatic void migrate_page_add(struct page *page, struct list_head *pagelist,\r\nunsigned long flags)\r\n{\r\nif ((flags & MPOL_MF_MOVE_ALL) || page_mapcount(page) == 1) {\r\nif (!isolate_lru_page(page)) {\r\nlist_add_tail(&page->lru, pagelist);\r\ninc_zone_page_state(page, NR_ISOLATED_ANON +\r\npage_is_file_cache(page));\r\n}\r\n}\r\n}\r\nstatic struct page *new_node_page(struct page *page, unsigned long node, int **x)\r\n{\r\nif (PageHuge(page))\r\nreturn alloc_huge_page_node(page_hstate(compound_head(page)),\r\nnode);\r\nelse\r\nreturn alloc_pages_exact_node(node, GFP_HIGHUSER_MOVABLE, 0);\r\n}\r\nstatic int migrate_to_node(struct mm_struct *mm, int source, int dest,\r\nint flags)\r\n{\r\nnodemask_t nmask;\r\nLIST_HEAD(pagelist);\r\nint err = 0;\r\nnodes_clear(nmask);\r\nnode_set(source, nmask);\r\nVM_BUG_ON(!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)));\r\nqueue_pages_range(mm, mm->mmap->vm_start, mm->task_size, &nmask,\r\nflags | MPOL_MF_DISCONTIG_OK, &pagelist);\r\nif (!list_empty(&pagelist)) {\r\nerr = migrate_pages(&pagelist, new_node_page, NULL, dest,\r\nMIGRATE_SYNC, MR_SYSCALL);\r\nif (err)\r\nputback_movable_pages(&pagelist);\r\n}\r\nreturn err;\r\n}\r\nint do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,\r\nconst nodemask_t *to, int flags)\r\n{\r\nint busy = 0;\r\nint err;\r\nnodemask_t tmp;\r\nerr = migrate_prep();\r\nif (err)\r\nreturn err;\r\ndown_read(&mm->mmap_sem);\r\nerr = migrate_vmas(mm, from, to, flags);\r\nif (err)\r\ngoto out;\r\ntmp = *from;\r\nwhile (!nodes_empty(tmp)) {\r\nint s,d;\r\nint source = NUMA_NO_NODE;\r\nint dest = 0;\r\nfor_each_node_mask(s, tmp) {\r\nif ((nodes_weight(*from) != nodes_weight(*to)) &&\r\n(node_isset(s, *to)))\r\ncontinue;\r\nd = node_remap(s, *from, *to);\r\nif (s == d)\r\ncontinue;\r\nsource = s;\r\ndest = d;\r\nif (!node_isset(dest, tmp))\r\nbreak;\r\n}\r\nif (source == NUMA_NO_NODE)\r\nbreak;\r\nnode_clear(source, tmp);\r\nerr = migrate_to_node(mm, source, dest, flags);\r\nif (err > 0)\r\nbusy += err;\r\nif (err < 0)\r\nbreak;\r\n}\r\nout:\r\nup_read(&mm->mmap_sem);\r\nif (err < 0)\r\nreturn err;\r\nreturn busy;\r\n}\r\nstatic struct page *new_page(struct page *page, unsigned long start, int **x)\r\n{\r\nstruct vm_area_struct *vma;\r\nunsigned long uninitialized_var(address);\r\nvma = find_vma(current->mm, start);\r\nwhile (vma) {\r\naddress = page_address_in_vma(page, vma);\r\nif (address != -EFAULT)\r\nbreak;\r\nvma = vma->vm_next;\r\n}\r\nif (PageHuge(page)) {\r\nBUG_ON(!vma);\r\nreturn alloc_huge_page_noerr(vma, address, 1);\r\n}\r\nreturn alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);\r\n}\r\nstatic void migrate_page_add(struct page *page, struct list_head *pagelist,\r\nunsigned long flags)\r\n{\r\n}\r\nint do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,\r\nconst nodemask_t *to, int flags)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic struct page *new_page(struct page *page, unsigned long start, int **x)\r\n{\r\nreturn NULL;\r\n}\r\nstatic long do_mbind(unsigned long start, unsigned long len,\r\nunsigned short mode, unsigned short mode_flags,\r\nnodemask_t *nmask, unsigned long flags)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct mempolicy *new;\r\nunsigned long end;\r\nint err;\r\nLIST_HEAD(pagelist);\r\nif (flags & ~(unsigned long)MPOL_MF_VALID)\r\nreturn -EINVAL;\r\nif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\r\nreturn -EPERM;\r\nif (start & ~PAGE_MASK)\r\nreturn -EINVAL;\r\nif (mode == MPOL_DEFAULT)\r\nflags &= ~MPOL_MF_STRICT;\r\nlen = (len + PAGE_SIZE - 1) & PAGE_MASK;\r\nend = start + len;\r\nif (end < start)\r\nreturn -EINVAL;\r\nif (end == start)\r\nreturn 0;\r\nnew = mpol_new(mode, mode_flags, nmask);\r\nif (IS_ERR(new))\r\nreturn PTR_ERR(new);\r\nif (flags & MPOL_MF_LAZY)\r\nnew->flags |= MPOL_F_MOF;\r\nif (!new)\r\nflags |= MPOL_MF_DISCONTIG_OK;\r\npr_debug("mbind %lx-%lx mode:%d flags:%d nodes:%lx\n",\r\nstart, start + len, mode, mode_flags,\r\nnmask ? nodes_addr(*nmask)[0] : NUMA_NO_NODE);\r\nif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) {\r\nerr = migrate_prep();\r\nif (err)\r\ngoto mpol_out;\r\n}\r\n{\r\nNODEMASK_SCRATCH(scratch);\r\nif (scratch) {\r\ndown_write(&mm->mmap_sem);\r\ntask_lock(current);\r\nerr = mpol_set_nodemask(new, nmask, scratch);\r\ntask_unlock(current);\r\nif (err)\r\nup_write(&mm->mmap_sem);\r\n} else\r\nerr = -ENOMEM;\r\nNODEMASK_SCRATCH_FREE(scratch);\r\n}\r\nif (err)\r\ngoto mpol_out;\r\nerr = queue_pages_range(mm, start, end, nmask,\r\nflags | MPOL_MF_INVERT, &pagelist);\r\nif (!err)\r\nerr = mbind_range(mm, start, end, new);\r\nif (!err) {\r\nint nr_failed = 0;\r\nif (!list_empty(&pagelist)) {\r\nWARN_ON_ONCE(flags & MPOL_MF_LAZY);\r\nnr_failed = migrate_pages(&pagelist, new_page, NULL,\r\nstart, MIGRATE_SYNC, MR_MEMPOLICY_MBIND);\r\nif (nr_failed)\r\nputback_movable_pages(&pagelist);\r\n}\r\nif (nr_failed && (flags & MPOL_MF_STRICT))\r\nerr = -EIO;\r\n} else\r\nputback_movable_pages(&pagelist);\r\nup_write(&mm->mmap_sem);\r\nmpol_out:\r\nmpol_put(new);\r\nreturn err;\r\n}\r\nstatic int get_nodes(nodemask_t *nodes, const unsigned long __user *nmask,\r\nunsigned long maxnode)\r\n{\r\nunsigned long k;\r\nunsigned long nlongs;\r\nunsigned long endmask;\r\n--maxnode;\r\nnodes_clear(*nodes);\r\nif (maxnode == 0 || !nmask)\r\nreturn 0;\r\nif (maxnode > PAGE_SIZE*BITS_PER_BYTE)\r\nreturn -EINVAL;\r\nnlongs = BITS_TO_LONGS(maxnode);\r\nif ((maxnode % BITS_PER_LONG) == 0)\r\nendmask = ~0UL;\r\nelse\r\nendmask = (1UL << (maxnode % BITS_PER_LONG)) - 1;\r\nif (nlongs > BITS_TO_LONGS(MAX_NUMNODES)) {\r\nif (nlongs > PAGE_SIZE/sizeof(long))\r\nreturn -EINVAL;\r\nfor (k = BITS_TO_LONGS(MAX_NUMNODES); k < nlongs; k++) {\r\nunsigned long t;\r\nif (get_user(t, nmask + k))\r\nreturn -EFAULT;\r\nif (k == nlongs - 1) {\r\nif (t & endmask)\r\nreturn -EINVAL;\r\n} else if (t)\r\nreturn -EINVAL;\r\n}\r\nnlongs = BITS_TO_LONGS(MAX_NUMNODES);\r\nendmask = ~0UL;\r\n}\r\nif (copy_from_user(nodes_addr(*nodes), nmask, nlongs*sizeof(unsigned long)))\r\nreturn -EFAULT;\r\nnodes_addr(*nodes)[nlongs-1] &= endmask;\r\nreturn 0;\r\n}\r\nstatic int copy_nodes_to_user(unsigned long __user *mask, unsigned long maxnode,\r\nnodemask_t *nodes)\r\n{\r\nunsigned long copy = ALIGN(maxnode-1, 64) / 8;\r\nconst int nbytes = BITS_TO_LONGS(MAX_NUMNODES) * sizeof(long);\r\nif (copy > nbytes) {\r\nif (copy > PAGE_SIZE)\r\nreturn -EINVAL;\r\nif (clear_user((char __user *)mask + nbytes, copy - nbytes))\r\nreturn -EFAULT;\r\ncopy = nbytes;\r\n}\r\nreturn copy_to_user(mask, nodes_addr(*nodes), copy) ? -EFAULT : 0;\r\n}\r\nstruct mempolicy *get_vma_policy(struct task_struct *task,\r\nstruct vm_area_struct *vma, unsigned long addr)\r\n{\r\nstruct mempolicy *pol = get_task_policy(task);\r\nif (vma) {\r\nif (vma->vm_ops && vma->vm_ops->get_policy) {\r\nstruct mempolicy *vpol = vma->vm_ops->get_policy(vma,\r\naddr);\r\nif (vpol)\r\npol = vpol;\r\n} else if (vma->vm_policy) {\r\npol = vma->vm_policy;\r\nif (mpol_needs_cond_ref(pol))\r\nmpol_get(pol);\r\n}\r\n}\r\nif (!pol)\r\npol = &default_policy;\r\nreturn pol;\r\n}\r\nbool vma_policy_mof(struct task_struct *task, struct vm_area_struct *vma)\r\n{\r\nstruct mempolicy *pol = get_task_policy(task);\r\nif (vma) {\r\nif (vma->vm_ops && vma->vm_ops->get_policy) {\r\nbool ret = false;\r\npol = vma->vm_ops->get_policy(vma, vma->vm_start);\r\nif (pol && (pol->flags & MPOL_F_MOF))\r\nret = true;\r\nmpol_cond_put(pol);\r\nreturn ret;\r\n} else if (vma->vm_policy) {\r\npol = vma->vm_policy;\r\n}\r\n}\r\nif (!pol)\r\nreturn default_policy.flags & MPOL_F_MOF;\r\nreturn pol->flags & MPOL_F_MOF;\r\n}\r\nstatic int apply_policy_zone(struct mempolicy *policy, enum zone_type zone)\r\n{\r\nenum zone_type dynamic_policy_zone = policy_zone;\r\nBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);\r\nif (!nodes_intersects(policy->v.nodes, node_states[N_HIGH_MEMORY]))\r\ndynamic_policy_zone = ZONE_MOVABLE;\r\nreturn zone >= dynamic_policy_zone;\r\n}\r\nstatic nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *policy)\r\n{\r\nif (unlikely(policy->mode == MPOL_BIND) &&\r\napply_policy_zone(policy, gfp_zone(gfp)) &&\r\ncpuset_nodemask_valid_mems_allowed(&policy->v.nodes))\r\nreturn &policy->v.nodes;\r\nreturn NULL;\r\n}\r\nstatic struct zonelist *policy_zonelist(gfp_t gfp, struct mempolicy *policy,\r\nint nd)\r\n{\r\nswitch (policy->mode) {\r\ncase MPOL_PREFERRED:\r\nif (!(policy->flags & MPOL_F_LOCAL))\r\nnd = policy->v.preferred_node;\r\nbreak;\r\ncase MPOL_BIND:\r\nif (unlikely(gfp & __GFP_THISNODE) &&\r\nunlikely(!node_isset(nd, policy->v.nodes)))\r\nnd = first_node(policy->v.nodes);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn node_zonelist(nd, gfp);\r\n}\r\nstatic unsigned interleave_nodes(struct mempolicy *policy)\r\n{\r\nunsigned nid, next;\r\nstruct task_struct *me = current;\r\nnid = me->il_next;\r\nnext = next_node(nid, policy->v.nodes);\r\nif (next >= MAX_NUMNODES)\r\nnext = first_node(policy->v.nodes);\r\nif (next < MAX_NUMNODES)\r\nme->il_next = next;\r\nreturn nid;\r\n}\r\nunsigned int mempolicy_slab_node(void)\r\n{\r\nstruct mempolicy *policy;\r\nint node = numa_mem_id();\r\nif (in_interrupt())\r\nreturn node;\r\npolicy = current->mempolicy;\r\nif (!policy || policy->flags & MPOL_F_LOCAL)\r\nreturn node;\r\nswitch (policy->mode) {\r\ncase MPOL_PREFERRED:\r\nreturn policy->v.preferred_node;\r\ncase MPOL_INTERLEAVE:\r\nreturn interleave_nodes(policy);\r\ncase MPOL_BIND: {\r\nstruct zonelist *zonelist;\r\nstruct zone *zone;\r\nenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);\r\nzonelist = &NODE_DATA(node)->node_zonelists[0];\r\n(void)first_zones_zonelist(zonelist, highest_zoneidx,\r\n&policy->v.nodes,\r\n&zone);\r\nreturn zone ? zone->node : node;\r\n}\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic unsigned offset_il_node(struct mempolicy *pol,\r\nstruct vm_area_struct *vma, unsigned long off)\r\n{\r\nunsigned nnodes = nodes_weight(pol->v.nodes);\r\nunsigned target;\r\nint c;\r\nint nid = NUMA_NO_NODE;\r\nif (!nnodes)\r\nreturn numa_node_id();\r\ntarget = (unsigned int)off % nnodes;\r\nc = 0;\r\ndo {\r\nnid = next_node(nid, pol->v.nodes);\r\nc++;\r\n} while (c <= target);\r\nreturn nid;\r\n}\r\nstatic inline unsigned interleave_nid(struct mempolicy *pol,\r\nstruct vm_area_struct *vma, unsigned long addr, int shift)\r\n{\r\nif (vma) {\r\nunsigned long off;\r\nBUG_ON(shift < PAGE_SHIFT);\r\noff = vma->vm_pgoff >> (shift - PAGE_SHIFT);\r\noff += (addr - vma->vm_start) >> shift;\r\nreturn offset_il_node(pol, vma, off);\r\n} else\r\nreturn interleave_nodes(pol);\r\n}\r\nint node_random(const nodemask_t *maskp)\r\n{\r\nint w, bit = NUMA_NO_NODE;\r\nw = nodes_weight(*maskp);\r\nif (w)\r\nbit = bitmap_ord_to_pos(maskp->bits,\r\nget_random_int() % w, MAX_NUMNODES);\r\nreturn bit;\r\n}\r\nstruct zonelist *huge_zonelist(struct vm_area_struct *vma, unsigned long addr,\r\ngfp_t gfp_flags, struct mempolicy **mpol,\r\nnodemask_t **nodemask)\r\n{\r\nstruct zonelist *zl;\r\n*mpol = get_vma_policy(current, vma, addr);\r\n*nodemask = NULL;\r\nif (unlikely((*mpol)->mode == MPOL_INTERLEAVE)) {\r\nzl = node_zonelist(interleave_nid(*mpol, vma, addr,\r\nhuge_page_shift(hstate_vma(vma))), gfp_flags);\r\n} else {\r\nzl = policy_zonelist(gfp_flags, *mpol, numa_node_id());\r\nif ((*mpol)->mode == MPOL_BIND)\r\n*nodemask = &(*mpol)->v.nodes;\r\n}\r\nreturn zl;\r\n}\r\nbool init_nodemask_of_mempolicy(nodemask_t *mask)\r\n{\r\nstruct mempolicy *mempolicy;\r\nint nid;\r\nif (!(mask && current->mempolicy))\r\nreturn false;\r\ntask_lock(current);\r\nmempolicy = current->mempolicy;\r\nswitch (mempolicy->mode) {\r\ncase MPOL_PREFERRED:\r\nif (mempolicy->flags & MPOL_F_LOCAL)\r\nnid = numa_node_id();\r\nelse\r\nnid = mempolicy->v.preferred_node;\r\ninit_nodemask_of_node(mask, nid);\r\nbreak;\r\ncase MPOL_BIND:\r\ncase MPOL_INTERLEAVE:\r\n*mask = mempolicy->v.nodes;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\ntask_unlock(current);\r\nreturn true;\r\n}\r\nbool mempolicy_nodemask_intersects(struct task_struct *tsk,\r\nconst nodemask_t *mask)\r\n{\r\nstruct mempolicy *mempolicy;\r\nbool ret = true;\r\nif (!mask)\r\nreturn ret;\r\ntask_lock(tsk);\r\nmempolicy = tsk->mempolicy;\r\nif (!mempolicy)\r\ngoto out;\r\nswitch (mempolicy->mode) {\r\ncase MPOL_PREFERRED:\r\nbreak;\r\ncase MPOL_BIND:\r\ncase MPOL_INTERLEAVE:\r\nret = nodes_intersects(mempolicy->v.nodes, *mask);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nout:\r\ntask_unlock(tsk);\r\nreturn ret;\r\n}\r\nstatic struct page *alloc_page_interleave(gfp_t gfp, unsigned order,\r\nunsigned nid)\r\n{\r\nstruct zonelist *zl;\r\nstruct page *page;\r\nzl = node_zonelist(nid, gfp);\r\npage = __alloc_pages(gfp, order, zl);\r\nif (page && page_zone(page) == zonelist_zone(&zl->_zonerefs[0]))\r\ninc_zone_page_state(page, NUMA_INTERLEAVE_HIT);\r\nreturn page;\r\n}\r\nstruct page *\r\nalloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,\r\nunsigned long addr, int node)\r\n{\r\nstruct mempolicy *pol;\r\nstruct page *page;\r\nunsigned int cpuset_mems_cookie;\r\nretry_cpuset:\r\npol = get_vma_policy(current, vma, addr);\r\ncpuset_mems_cookie = read_mems_allowed_begin();\r\nif (unlikely(pol->mode == MPOL_INTERLEAVE)) {\r\nunsigned nid;\r\nnid = interleave_nid(pol, vma, addr, PAGE_SHIFT + order);\r\nmpol_cond_put(pol);\r\npage = alloc_page_interleave(gfp, order, nid);\r\nif (unlikely(!page && read_mems_allowed_retry(cpuset_mems_cookie)))\r\ngoto retry_cpuset;\r\nreturn page;\r\n}\r\npage = __alloc_pages_nodemask(gfp, order,\r\npolicy_zonelist(gfp, pol, node),\r\npolicy_nodemask(gfp, pol));\r\nif (unlikely(mpol_needs_cond_ref(pol)))\r\n__mpol_put(pol);\r\nif (unlikely(!page && read_mems_allowed_retry(cpuset_mems_cookie)))\r\ngoto retry_cpuset;\r\nreturn page;\r\n}\r\nstruct page *alloc_pages_current(gfp_t gfp, unsigned order)\r\n{\r\nstruct mempolicy *pol = get_task_policy(current);\r\nstruct page *page;\r\nunsigned int cpuset_mems_cookie;\r\nif (!pol || in_interrupt() || (gfp & __GFP_THISNODE))\r\npol = &default_policy;\r\nretry_cpuset:\r\ncpuset_mems_cookie = read_mems_allowed_begin();\r\nif (pol->mode == MPOL_INTERLEAVE)\r\npage = alloc_page_interleave(gfp, order, interleave_nodes(pol));\r\nelse\r\npage = __alloc_pages_nodemask(gfp, order,\r\npolicy_zonelist(gfp, pol, numa_node_id()),\r\npolicy_nodemask(gfp, pol));\r\nif (unlikely(!page && read_mems_allowed_retry(cpuset_mems_cookie)))\r\ngoto retry_cpuset;\r\nreturn page;\r\n}\r\nint vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)\r\n{\r\nstruct mempolicy *pol = mpol_dup(vma_policy(src));\r\nif (IS_ERR(pol))\r\nreturn PTR_ERR(pol);\r\ndst->vm_policy = pol;\r\nreturn 0;\r\n}\r\nstruct mempolicy *__mpol_dup(struct mempolicy *old)\r\n{\r\nstruct mempolicy *new = kmem_cache_alloc(policy_cache, GFP_KERNEL);\r\nif (!new)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (old == current->mempolicy) {\r\ntask_lock(current);\r\n*new = *old;\r\ntask_unlock(current);\r\n} else\r\n*new = *old;\r\nif (current_cpuset_is_being_rebound()) {\r\nnodemask_t mems = cpuset_mems_allowed(current);\r\nif (new->flags & MPOL_F_REBINDING)\r\nmpol_rebind_policy(new, &mems, MPOL_REBIND_STEP2);\r\nelse\r\nmpol_rebind_policy(new, &mems, MPOL_REBIND_ONCE);\r\n}\r\natomic_set(&new->refcnt, 1);\r\nreturn new;\r\n}\r\nbool __mpol_equal(struct mempolicy *a, struct mempolicy *b)\r\n{\r\nif (!a || !b)\r\nreturn false;\r\nif (a->mode != b->mode)\r\nreturn false;\r\nif (a->flags != b->flags)\r\nreturn false;\r\nif (mpol_store_user_nodemask(a))\r\nif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))\r\nreturn false;\r\nswitch (a->mode) {\r\ncase MPOL_BIND:\r\ncase MPOL_INTERLEAVE:\r\nreturn !!nodes_equal(a->v.nodes, b->v.nodes);\r\ncase MPOL_PREFERRED:\r\nreturn a->v.preferred_node == b->v.preferred_node;\r\ndefault:\r\nBUG();\r\nreturn false;\r\n}\r\n}\r\nstatic struct sp_node *\r\nsp_lookup(struct shared_policy *sp, unsigned long start, unsigned long end)\r\n{\r\nstruct rb_node *n = sp->root.rb_node;\r\nwhile (n) {\r\nstruct sp_node *p = rb_entry(n, struct sp_node, nd);\r\nif (start >= p->end)\r\nn = n->rb_right;\r\nelse if (end <= p->start)\r\nn = n->rb_left;\r\nelse\r\nbreak;\r\n}\r\nif (!n)\r\nreturn NULL;\r\nfor (;;) {\r\nstruct sp_node *w = NULL;\r\nstruct rb_node *prev = rb_prev(n);\r\nif (!prev)\r\nbreak;\r\nw = rb_entry(prev, struct sp_node, nd);\r\nif (w->end <= start)\r\nbreak;\r\nn = prev;\r\n}\r\nreturn rb_entry(n, struct sp_node, nd);\r\n}\r\nstatic void sp_insert(struct shared_policy *sp, struct sp_node *new)\r\n{\r\nstruct rb_node **p = &sp->root.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct sp_node *nd;\r\nwhile (*p) {\r\nparent = *p;\r\nnd = rb_entry(parent, struct sp_node, nd);\r\nif (new->start < nd->start)\r\np = &(*p)->rb_left;\r\nelse if (new->end > nd->end)\r\np = &(*p)->rb_right;\r\nelse\r\nBUG();\r\n}\r\nrb_link_node(&new->nd, parent, p);\r\nrb_insert_color(&new->nd, &sp->root);\r\npr_debug("inserting %lx-%lx: %d\n", new->start, new->end,\r\nnew->policy ? new->policy->mode : 0);\r\n}\r\nstruct mempolicy *\r\nmpol_shared_policy_lookup(struct shared_policy *sp, unsigned long idx)\r\n{\r\nstruct mempolicy *pol = NULL;\r\nstruct sp_node *sn;\r\nif (!sp->root.rb_node)\r\nreturn NULL;\r\nspin_lock(&sp->lock);\r\nsn = sp_lookup(sp, idx, idx+1);\r\nif (sn) {\r\nmpol_get(sn->policy);\r\npol = sn->policy;\r\n}\r\nspin_unlock(&sp->lock);\r\nreturn pol;\r\n}\r\nstatic void sp_free(struct sp_node *n)\r\n{\r\nmpol_put(n->policy);\r\nkmem_cache_free(sn_cache, n);\r\n}\r\nint mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr)\r\n{\r\nstruct mempolicy *pol;\r\nstruct zone *zone;\r\nint curnid = page_to_nid(page);\r\nunsigned long pgoff;\r\nint thiscpu = raw_smp_processor_id();\r\nint thisnid = cpu_to_node(thiscpu);\r\nint polnid = -1;\r\nint ret = -1;\r\nBUG_ON(!vma);\r\npol = get_vma_policy(current, vma, addr);\r\nif (!(pol->flags & MPOL_F_MOF))\r\ngoto out;\r\nswitch (pol->mode) {\r\ncase MPOL_INTERLEAVE:\r\nBUG_ON(addr >= vma->vm_end);\r\nBUG_ON(addr < vma->vm_start);\r\npgoff = vma->vm_pgoff;\r\npgoff += (addr - vma->vm_start) >> PAGE_SHIFT;\r\npolnid = offset_il_node(pol, vma, pgoff);\r\nbreak;\r\ncase MPOL_PREFERRED:\r\nif (pol->flags & MPOL_F_LOCAL)\r\npolnid = numa_node_id();\r\nelse\r\npolnid = pol->v.preferred_node;\r\nbreak;\r\ncase MPOL_BIND:\r\nif (node_isset(curnid, pol->v.nodes))\r\ngoto out;\r\n(void)first_zones_zonelist(\r\nnode_zonelist(numa_node_id(), GFP_HIGHUSER),\r\ngfp_zone(GFP_HIGHUSER),\r\n&pol->v.nodes, &zone);\r\npolnid = zone->node;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nif (pol->flags & MPOL_F_MORON) {\r\npolnid = thisnid;\r\nif (!should_numa_migrate_memory(current, page, curnid, thiscpu))\r\ngoto out;\r\n}\r\nif (curnid != polnid)\r\nret = polnid;\r\nout:\r\nmpol_cond_put(pol);\r\nreturn ret;\r\n}\r\nstatic void sp_delete(struct shared_policy *sp, struct sp_node *n)\r\n{\r\npr_debug("deleting %lx-l%lx\n", n->start, n->end);\r\nrb_erase(&n->nd, &sp->root);\r\nsp_free(n);\r\n}\r\nstatic void sp_node_init(struct sp_node *node, unsigned long start,\r\nunsigned long end, struct mempolicy *pol)\r\n{\r\nnode->start = start;\r\nnode->end = end;\r\nnode->policy = pol;\r\n}\r\nstatic struct sp_node *sp_alloc(unsigned long start, unsigned long end,\r\nstruct mempolicy *pol)\r\n{\r\nstruct sp_node *n;\r\nstruct mempolicy *newpol;\r\nn = kmem_cache_alloc(sn_cache, GFP_KERNEL);\r\nif (!n)\r\nreturn NULL;\r\nnewpol = mpol_dup(pol);\r\nif (IS_ERR(newpol)) {\r\nkmem_cache_free(sn_cache, n);\r\nreturn NULL;\r\n}\r\nnewpol->flags |= MPOL_F_SHARED;\r\nsp_node_init(n, start, end, newpol);\r\nreturn n;\r\n}\r\nstatic int shared_policy_replace(struct shared_policy *sp, unsigned long start,\r\nunsigned long end, struct sp_node *new)\r\n{\r\nstruct sp_node *n;\r\nstruct sp_node *n_new = NULL;\r\nstruct mempolicy *mpol_new = NULL;\r\nint ret = 0;\r\nrestart:\r\nspin_lock(&sp->lock);\r\nn = sp_lookup(sp, start, end);\r\nwhile (n && n->start < end) {\r\nstruct rb_node *next = rb_next(&n->nd);\r\nif (n->start >= start) {\r\nif (n->end <= end)\r\nsp_delete(sp, n);\r\nelse\r\nn->start = end;\r\n} else {\r\nif (n->end > end) {\r\nif (!n_new)\r\ngoto alloc_new;\r\n*mpol_new = *n->policy;\r\natomic_set(&mpol_new->refcnt, 1);\r\nsp_node_init(n_new, end, n->end, mpol_new);\r\nn->end = start;\r\nsp_insert(sp, n_new);\r\nn_new = NULL;\r\nmpol_new = NULL;\r\nbreak;\r\n} else\r\nn->end = start;\r\n}\r\nif (!next)\r\nbreak;\r\nn = rb_entry(next, struct sp_node, nd);\r\n}\r\nif (new)\r\nsp_insert(sp, new);\r\nspin_unlock(&sp->lock);\r\nret = 0;\r\nerr_out:\r\nif (mpol_new)\r\nmpol_put(mpol_new);\r\nif (n_new)\r\nkmem_cache_free(sn_cache, n_new);\r\nreturn ret;\r\nalloc_new:\r\nspin_unlock(&sp->lock);\r\nret = -ENOMEM;\r\nn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);\r\nif (!n_new)\r\ngoto err_out;\r\nmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);\r\nif (!mpol_new)\r\ngoto err_out;\r\ngoto restart;\r\n}\r\nvoid mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)\r\n{\r\nint ret;\r\nsp->root = RB_ROOT;\r\nspin_lock_init(&sp->lock);\r\nif (mpol) {\r\nstruct vm_area_struct pvma;\r\nstruct mempolicy *new;\r\nNODEMASK_SCRATCH(scratch);\r\nif (!scratch)\r\ngoto put_mpol;\r\nnew = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);\r\nif (IS_ERR(new))\r\ngoto free_scratch;\r\ntask_lock(current);\r\nret = mpol_set_nodemask(new, &mpol->w.user_nodemask, scratch);\r\ntask_unlock(current);\r\nif (ret)\r\ngoto put_new;\r\nmemset(&pvma, 0, sizeof(struct vm_area_struct));\r\npvma.vm_end = TASK_SIZE;\r\nmpol_set_shared_policy(sp, &pvma, new);\r\nput_new:\r\nmpol_put(new);\r\nfree_scratch:\r\nNODEMASK_SCRATCH_FREE(scratch);\r\nput_mpol:\r\nmpol_put(mpol);\r\n}\r\n}\r\nint mpol_set_shared_policy(struct shared_policy *info,\r\nstruct vm_area_struct *vma, struct mempolicy *npol)\r\n{\r\nint err;\r\nstruct sp_node *new = NULL;\r\nunsigned long sz = vma_pages(vma);\r\npr_debug("set_shared_policy %lx sz %lu %d %d %lx\n",\r\nvma->vm_pgoff,\r\nsz, npol ? npol->mode : -1,\r\nnpol ? npol->flags : -1,\r\nnpol ? nodes_addr(npol->v.nodes)[0] : NUMA_NO_NODE);\r\nif (npol) {\r\nnew = sp_alloc(vma->vm_pgoff, vma->vm_pgoff + sz, npol);\r\nif (!new)\r\nreturn -ENOMEM;\r\n}\r\nerr = shared_policy_replace(info, vma->vm_pgoff, vma->vm_pgoff+sz, new);\r\nif (err && new)\r\nsp_free(new);\r\nreturn err;\r\n}\r\nvoid mpol_free_shared_policy(struct shared_policy *p)\r\n{\r\nstruct sp_node *n;\r\nstruct rb_node *next;\r\nif (!p->root.rb_node)\r\nreturn;\r\nspin_lock(&p->lock);\r\nnext = rb_first(&p->root);\r\nwhile (next) {\r\nn = rb_entry(next, struct sp_node, nd);\r\nnext = rb_next(&n->nd);\r\nsp_delete(p, n);\r\n}\r\nspin_unlock(&p->lock);\r\n}\r\nstatic void __init check_numabalancing_enable(void)\r\n{\r\nbool numabalancing_default = false;\r\nif (IS_ENABLED(CONFIG_NUMA_BALANCING_DEFAULT_ENABLED))\r\nnumabalancing_default = true;\r\nif (numabalancing_override)\r\nset_numabalancing_state(numabalancing_override == 1);\r\nif (nr_node_ids > 1 && !numabalancing_override) {\r\npr_info("%s automatic NUMA balancing. "\r\n"Configure with numa_balancing= or the "\r\n"kernel.numa_balancing sysctl",\r\nnumabalancing_default ? "Enabling" : "Disabling");\r\nset_numabalancing_state(numabalancing_default);\r\n}\r\n}\r\nstatic int __init setup_numabalancing(char *str)\r\n{\r\nint ret = 0;\r\nif (!str)\r\ngoto out;\r\nif (!strcmp(str, "enable")) {\r\nnumabalancing_override = 1;\r\nret = 1;\r\n} else if (!strcmp(str, "disable")) {\r\nnumabalancing_override = -1;\r\nret = 1;\r\n}\r\nout:\r\nif (!ret)\r\npr_warn("Unable to parse numa_balancing=\n");\r\nreturn ret;\r\n}\r\nstatic inline void __init check_numabalancing_enable(void)\r\n{\r\n}\r\nvoid __init numa_policy_init(void)\r\n{\r\nnodemask_t interleave_nodes;\r\nunsigned long largest = 0;\r\nint nid, prefer = 0;\r\npolicy_cache = kmem_cache_create("numa_policy",\r\nsizeof(struct mempolicy),\r\n0, SLAB_PANIC, NULL);\r\nsn_cache = kmem_cache_create("shared_policy_node",\r\nsizeof(struct sp_node),\r\n0, SLAB_PANIC, NULL);\r\nfor_each_node(nid) {\r\npreferred_node_policy[nid] = (struct mempolicy) {\r\n.refcnt = ATOMIC_INIT(1),\r\n.mode = MPOL_PREFERRED,\r\n.flags = MPOL_F_MOF | MPOL_F_MORON,\r\n.v = { .preferred_node = nid, },\r\n};\r\n}\r\nnodes_clear(interleave_nodes);\r\nfor_each_node_state(nid, N_MEMORY) {\r\nunsigned long total_pages = node_present_pages(nid);\r\nif (largest < total_pages) {\r\nlargest = total_pages;\r\nprefer = nid;\r\n}\r\nif ((total_pages << PAGE_SHIFT) >= (16 << 20))\r\nnode_set(nid, interleave_nodes);\r\n}\r\nif (unlikely(nodes_empty(interleave_nodes)))\r\nnode_set(prefer, interleave_nodes);\r\nif (do_set_mempolicy(MPOL_INTERLEAVE, 0, &interleave_nodes))\r\npr_err("%s: interleaving failed\n", __func__);\r\ncheck_numabalancing_enable();\r\n}\r\nvoid numa_default_policy(void)\r\n{\r\ndo_set_mempolicy(MPOL_DEFAULT, 0, NULL);\r\n}\r\nint mpol_parse_str(char *str, struct mempolicy **mpol)\r\n{\r\nstruct mempolicy *new = NULL;\r\nunsigned short mode;\r\nunsigned short mode_flags;\r\nnodemask_t nodes;\r\nchar *nodelist = strchr(str, ':');\r\nchar *flags = strchr(str, '=');\r\nint err = 1;\r\nif (nodelist) {\r\n*nodelist++ = '\0';\r\nif (nodelist_parse(nodelist, nodes))\r\ngoto out;\r\nif (!nodes_subset(nodes, node_states[N_MEMORY]))\r\ngoto out;\r\n} else\r\nnodes_clear(nodes);\r\nif (flags)\r\n*flags++ = '\0';\r\nfor (mode = 0; mode < MPOL_MAX; mode++) {\r\nif (!strcmp(str, policy_modes[mode])) {\r\nbreak;\r\n}\r\n}\r\nif (mode >= MPOL_MAX)\r\ngoto out;\r\nswitch (mode) {\r\ncase MPOL_PREFERRED:\r\nif (nodelist) {\r\nchar *rest = nodelist;\r\nwhile (isdigit(*rest))\r\nrest++;\r\nif (*rest)\r\ngoto out;\r\n}\r\nbreak;\r\ncase MPOL_INTERLEAVE:\r\nif (!nodelist)\r\nnodes = node_states[N_MEMORY];\r\nbreak;\r\ncase MPOL_LOCAL:\r\nif (nodelist)\r\ngoto out;\r\nmode = MPOL_PREFERRED;\r\nbreak;\r\ncase MPOL_DEFAULT:\r\nif (!nodelist)\r\nerr = 0;\r\ngoto out;\r\ncase MPOL_BIND:\r\nif (!nodelist)\r\ngoto out;\r\n}\r\nmode_flags = 0;\r\nif (flags) {\r\nif (!strcmp(flags, "static"))\r\nmode_flags |= MPOL_F_STATIC_NODES;\r\nelse if (!strcmp(flags, "relative"))\r\nmode_flags |= MPOL_F_RELATIVE_NODES;\r\nelse\r\ngoto out;\r\n}\r\nnew = mpol_new(mode, mode_flags, &nodes);\r\nif (IS_ERR(new))\r\ngoto out;\r\nif (mode != MPOL_PREFERRED)\r\nnew->v.nodes = nodes;\r\nelse if (nodelist)\r\nnew->v.preferred_node = first_node(nodes);\r\nelse\r\nnew->flags |= MPOL_F_LOCAL;\r\nnew->w.user_nodemask = nodes;\r\nerr = 0;\r\nout:\r\nif (nodelist)\r\n*--nodelist = ':';\r\nif (flags)\r\n*--flags = '=';\r\nif (!err)\r\n*mpol = new;\r\nreturn err;\r\n}\r\nvoid mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)\r\n{\r\nchar *p = buffer;\r\nnodemask_t nodes = NODE_MASK_NONE;\r\nunsigned short mode = MPOL_DEFAULT;\r\nunsigned short flags = 0;\r\nif (pol && pol != &default_policy && !(pol->flags & MPOL_F_MORON)) {\r\nmode = pol->mode;\r\nflags = pol->flags;\r\n}\r\nswitch (mode) {\r\ncase MPOL_DEFAULT:\r\nbreak;\r\ncase MPOL_PREFERRED:\r\nif (flags & MPOL_F_LOCAL)\r\nmode = MPOL_LOCAL;\r\nelse\r\nnode_set(pol->v.preferred_node, nodes);\r\nbreak;\r\ncase MPOL_BIND:\r\ncase MPOL_INTERLEAVE:\r\nnodes = pol->v.nodes;\r\nbreak;\r\ndefault:\r\nWARN_ON_ONCE(1);\r\nsnprintf(p, maxlen, "unknown");\r\nreturn;\r\n}\r\np += snprintf(p, maxlen, "%s", policy_modes[mode]);\r\nif (flags & MPOL_MODE_FLAGS) {\r\np += snprintf(p, buffer + maxlen - p, "=");\r\nif (flags & MPOL_F_STATIC_NODES)\r\np += snprintf(p, buffer + maxlen - p, "static");\r\nelse if (flags & MPOL_F_RELATIVE_NODES)\r\np += snprintf(p, buffer + maxlen - p, "relative");\r\n}\r\nif (!nodes_empty(nodes)) {\r\np += snprintf(p, buffer + maxlen - p, ":");\r\np += nodelist_scnprintf(p, buffer + maxlen - p, nodes);\r\n}\r\n}
