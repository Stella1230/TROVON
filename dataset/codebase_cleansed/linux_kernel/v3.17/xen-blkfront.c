static int get_id_from_freelist(struct blkfront_info *info)\r\n{\r\nunsigned long free = info->shadow_free;\r\nBUG_ON(free >= BLK_RING_SIZE);\r\ninfo->shadow_free = info->shadow[free].req.u.rw.id;\r\ninfo->shadow[free].req.u.rw.id = 0x0fffffee;\r\nreturn free;\r\n}\r\nstatic int add_id_to_freelist(struct blkfront_info *info,\r\nunsigned long id)\r\n{\r\nif (info->shadow[id].req.u.rw.id != id)\r\nreturn -EINVAL;\r\nif (info->shadow[id].request == NULL)\r\nreturn -EINVAL;\r\ninfo->shadow[id].req.u.rw.id = info->shadow_free;\r\ninfo->shadow[id].request = NULL;\r\ninfo->shadow_free = id;\r\nreturn 0;\r\n}\r\nstatic int fill_grant_buffer(struct blkfront_info *info, int num)\r\n{\r\nstruct page *granted_page;\r\nstruct grant *gnt_list_entry, *n;\r\nint i = 0;\r\nwhile(i < num) {\r\ngnt_list_entry = kzalloc(sizeof(struct grant), GFP_NOIO);\r\nif (!gnt_list_entry)\r\ngoto out_of_memory;\r\nif (info->feature_persistent) {\r\ngranted_page = alloc_page(GFP_NOIO);\r\nif (!granted_page) {\r\nkfree(gnt_list_entry);\r\ngoto out_of_memory;\r\n}\r\ngnt_list_entry->pfn = page_to_pfn(granted_page);\r\n}\r\ngnt_list_entry->gref = GRANT_INVALID_REF;\r\nlist_add(&gnt_list_entry->node, &info->grants);\r\ni++;\r\n}\r\nreturn 0;\r\nout_of_memory:\r\nlist_for_each_entry_safe(gnt_list_entry, n,\r\n&info->grants, node) {\r\nlist_del(&gnt_list_entry->node);\r\nif (info->feature_persistent)\r\n__free_page(pfn_to_page(gnt_list_entry->pfn));\r\nkfree(gnt_list_entry);\r\ni--;\r\n}\r\nBUG_ON(i != 0);\r\nreturn -ENOMEM;\r\n}\r\nstatic struct grant *get_grant(grant_ref_t *gref_head,\r\nunsigned long pfn,\r\nstruct blkfront_info *info)\r\n{\r\nstruct grant *gnt_list_entry;\r\nunsigned long buffer_mfn;\r\nBUG_ON(list_empty(&info->grants));\r\ngnt_list_entry = list_first_entry(&info->grants, struct grant,\r\nnode);\r\nlist_del(&gnt_list_entry->node);\r\nif (gnt_list_entry->gref != GRANT_INVALID_REF) {\r\ninfo->persistent_gnts_c--;\r\nreturn gnt_list_entry;\r\n}\r\ngnt_list_entry->gref = gnttab_claim_grant_reference(gref_head);\r\nBUG_ON(gnt_list_entry->gref == -ENOSPC);\r\nif (!info->feature_persistent) {\r\nBUG_ON(!pfn);\r\ngnt_list_entry->pfn = pfn;\r\n}\r\nbuffer_mfn = pfn_to_mfn(gnt_list_entry->pfn);\r\ngnttab_grant_foreign_access_ref(gnt_list_entry->gref,\r\ninfo->xbdev->otherend_id,\r\nbuffer_mfn, 0);\r\nreturn gnt_list_entry;\r\n}\r\nstatic const char *op_name(int op)\r\n{\r\nstatic const char *const names[] = {\r\n[BLKIF_OP_READ] = "read",\r\n[BLKIF_OP_WRITE] = "write",\r\n[BLKIF_OP_WRITE_BARRIER] = "barrier",\r\n[BLKIF_OP_FLUSH_DISKCACHE] = "flush",\r\n[BLKIF_OP_DISCARD] = "discard" };\r\nif (op < 0 || op >= ARRAY_SIZE(names))\r\nreturn "unknown";\r\nif (!names[op])\r\nreturn "reserved";\r\nreturn names[op];\r\n}\r\nstatic int xlbd_reserve_minors(unsigned int minor, unsigned int nr)\r\n{\r\nunsigned int end = minor + nr;\r\nint rc;\r\nif (end > nr_minors) {\r\nunsigned long *bitmap, *old;\r\nbitmap = kcalloc(BITS_TO_LONGS(end), sizeof(*bitmap),\r\nGFP_KERNEL);\r\nif (bitmap == NULL)\r\nreturn -ENOMEM;\r\nspin_lock(&minor_lock);\r\nif (end > nr_minors) {\r\nold = minors;\r\nmemcpy(bitmap, minors,\r\nBITS_TO_LONGS(nr_minors) * sizeof(*bitmap));\r\nminors = bitmap;\r\nnr_minors = BITS_TO_LONGS(end) * BITS_PER_LONG;\r\n} else\r\nold = bitmap;\r\nspin_unlock(&minor_lock);\r\nkfree(old);\r\n}\r\nspin_lock(&minor_lock);\r\nif (find_next_bit(minors, end, minor) >= end) {\r\nbitmap_set(minors, minor, nr);\r\nrc = 0;\r\n} else\r\nrc = -EBUSY;\r\nspin_unlock(&minor_lock);\r\nreturn rc;\r\n}\r\nstatic void xlbd_release_minors(unsigned int minor, unsigned int nr)\r\n{\r\nunsigned int end = minor + nr;\r\nBUG_ON(end > nr_minors);\r\nspin_lock(&minor_lock);\r\nbitmap_clear(minors, minor, nr);\r\nspin_unlock(&minor_lock);\r\n}\r\nstatic void blkif_restart_queue_callback(void *arg)\r\n{\r\nstruct blkfront_info *info = (struct blkfront_info *)arg;\r\nschedule_work(&info->work);\r\n}\r\nstatic int blkif_getgeo(struct block_device *bd, struct hd_geometry *hg)\r\n{\r\nsector_t nsect = get_capacity(bd->bd_disk);\r\nsector_t cylinders = nsect;\r\nhg->heads = 0xff;\r\nhg->sectors = 0x3f;\r\nsector_div(cylinders, hg->heads * hg->sectors);\r\nhg->cylinders = cylinders;\r\nif ((sector_t)(hg->cylinders + 1) * hg->heads * hg->sectors < nsect)\r\nhg->cylinders = 0xffff;\r\nreturn 0;\r\n}\r\nstatic int blkif_ioctl(struct block_device *bdev, fmode_t mode,\r\nunsigned command, unsigned long argument)\r\n{\r\nstruct blkfront_info *info = bdev->bd_disk->private_data;\r\nint i;\r\ndev_dbg(&info->xbdev->dev, "command: 0x%x, argument: 0x%lx\n",\r\ncommand, (long)argument);\r\nswitch (command) {\r\ncase CDROMMULTISESSION:\r\ndev_dbg(&info->xbdev->dev, "FIXME: support multisession CDs later\n");\r\nfor (i = 0; i < sizeof(struct cdrom_multisession); i++)\r\nif (put_user(0, (char __user *)(argument + i)))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase CDROM_GET_CAPABILITY: {\r\nstruct gendisk *gd = info->gd;\r\nif (gd->flags & GENHD_FL_CD)\r\nreturn 0;\r\nreturn -EINVAL;\r\n}\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int blkif_queue_request(struct request *req)\r\n{\r\nstruct blkfront_info *info = req->rq_disk->private_data;\r\nstruct blkif_request *ring_req;\r\nunsigned long id;\r\nunsigned int fsect, lsect;\r\nint i, ref, n;\r\nstruct blkif_request_segment *segments = NULL;\r\nbool new_persistent_gnts;\r\ngrant_ref_t gref_head;\r\nstruct grant *gnt_list_entry = NULL;\r\nstruct scatterlist *sg;\r\nint nseg, max_grefs;\r\nif (unlikely(info->connected != BLKIF_STATE_CONNECTED))\r\nreturn 1;\r\nmax_grefs = req->nr_phys_segments;\r\nif (max_grefs > BLKIF_MAX_SEGMENTS_PER_REQUEST)\r\nmax_grefs += INDIRECT_GREFS(req->nr_phys_segments);\r\nif (info->persistent_gnts_c < max_grefs) {\r\nnew_persistent_gnts = 1;\r\nif (gnttab_alloc_grant_references(\r\nmax_grefs - info->persistent_gnts_c,\r\n&gref_head) < 0) {\r\ngnttab_request_free_callback(\r\n&info->callback,\r\nblkif_restart_queue_callback,\r\ninfo,\r\nmax_grefs);\r\nreturn 1;\r\n}\r\n} else\r\nnew_persistent_gnts = 0;\r\nring_req = RING_GET_REQUEST(&info->ring, info->ring.req_prod_pvt);\r\nid = get_id_from_freelist(info);\r\ninfo->shadow[id].request = req;\r\nif (unlikely(req->cmd_flags & (REQ_DISCARD | REQ_SECURE))) {\r\nring_req->operation = BLKIF_OP_DISCARD;\r\nring_req->u.discard.nr_sectors = blk_rq_sectors(req);\r\nring_req->u.discard.id = id;\r\nring_req->u.discard.sector_number = (blkif_sector_t)blk_rq_pos(req);\r\nif ((req->cmd_flags & REQ_SECURE) && info->feature_secdiscard)\r\nring_req->u.discard.flag = BLKIF_DISCARD_SECURE;\r\nelse\r\nring_req->u.discard.flag = 0;\r\n} else {\r\nBUG_ON(info->max_indirect_segments == 0 &&\r\nreq->nr_phys_segments > BLKIF_MAX_SEGMENTS_PER_REQUEST);\r\nBUG_ON(info->max_indirect_segments &&\r\nreq->nr_phys_segments > info->max_indirect_segments);\r\nnseg = blk_rq_map_sg(req->q, req, info->shadow[id].sg);\r\nring_req->u.rw.id = id;\r\nif (nseg > BLKIF_MAX_SEGMENTS_PER_REQUEST) {\r\nBUG_ON(req->cmd_flags & (REQ_FLUSH | REQ_FUA));\r\nring_req->operation = BLKIF_OP_INDIRECT;\r\nring_req->u.indirect.indirect_op = rq_data_dir(req) ?\r\nBLKIF_OP_WRITE : BLKIF_OP_READ;\r\nring_req->u.indirect.sector_number = (blkif_sector_t)blk_rq_pos(req);\r\nring_req->u.indirect.handle = info->handle;\r\nring_req->u.indirect.nr_segments = nseg;\r\n} else {\r\nring_req->u.rw.sector_number = (blkif_sector_t)blk_rq_pos(req);\r\nring_req->u.rw.handle = info->handle;\r\nring_req->operation = rq_data_dir(req) ?\r\nBLKIF_OP_WRITE : BLKIF_OP_READ;\r\nif (req->cmd_flags & (REQ_FLUSH | REQ_FUA)) {\r\nring_req->operation = info->flush_op;\r\n}\r\nring_req->u.rw.nr_segments = nseg;\r\n}\r\nfor_each_sg(info->shadow[id].sg, sg, nseg, i) {\r\nfsect = sg->offset >> 9;\r\nlsect = fsect + (sg->length >> 9) - 1;\r\nif ((ring_req->operation == BLKIF_OP_INDIRECT) &&\r\n(i % SEGS_PER_INDIRECT_FRAME == 0)) {\r\nunsigned long uninitialized_var(pfn);\r\nif (segments)\r\nkunmap_atomic(segments);\r\nn = i / SEGS_PER_INDIRECT_FRAME;\r\nif (!info->feature_persistent) {\r\nstruct page *indirect_page;\r\nBUG_ON(list_empty(&info->indirect_pages));\r\nindirect_page = list_first_entry(&info->indirect_pages,\r\nstruct page, lru);\r\nlist_del(&indirect_page->lru);\r\npfn = page_to_pfn(indirect_page);\r\n}\r\ngnt_list_entry = get_grant(&gref_head, pfn, info);\r\ninfo->shadow[id].indirect_grants[n] = gnt_list_entry;\r\nsegments = kmap_atomic(pfn_to_page(gnt_list_entry->pfn));\r\nring_req->u.indirect.indirect_grefs[n] = gnt_list_entry->gref;\r\n}\r\ngnt_list_entry = get_grant(&gref_head, page_to_pfn(sg_page(sg)), info);\r\nref = gnt_list_entry->gref;\r\ninfo->shadow[id].grants_used[i] = gnt_list_entry;\r\nif (rq_data_dir(req) && info->feature_persistent) {\r\nchar *bvec_data;\r\nvoid *shared_data;\r\nBUG_ON(sg->offset + sg->length > PAGE_SIZE);\r\nshared_data = kmap_atomic(pfn_to_page(gnt_list_entry->pfn));\r\nbvec_data = kmap_atomic(sg_page(sg));\r\nmemcpy(shared_data + sg->offset,\r\nbvec_data + sg->offset,\r\nsg->length);\r\nkunmap_atomic(bvec_data);\r\nkunmap_atomic(shared_data);\r\n}\r\nif (ring_req->operation != BLKIF_OP_INDIRECT) {\r\nring_req->u.rw.seg[i] =\r\n(struct blkif_request_segment) {\r\n.gref = ref,\r\n.first_sect = fsect,\r\n.last_sect = lsect };\r\n} else {\r\nn = i % SEGS_PER_INDIRECT_FRAME;\r\nsegments[n] =\r\n(struct blkif_request_segment) {\r\n.gref = ref,\r\n.first_sect = fsect,\r\n.last_sect = lsect };\r\n}\r\n}\r\nif (segments)\r\nkunmap_atomic(segments);\r\n}\r\ninfo->ring.req_prod_pvt++;\r\ninfo->shadow[id].req = *ring_req;\r\nif (new_persistent_gnts)\r\ngnttab_free_grant_references(gref_head);\r\nreturn 0;\r\n}\r\nstatic inline void flush_requests(struct blkfront_info *info)\r\n{\r\nint notify;\r\nRING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&info->ring, notify);\r\nif (notify)\r\nnotify_remote_via_irq(info->irq);\r\n}\r\nstatic void do_blkif_request(struct request_queue *rq)\r\n{\r\nstruct blkfront_info *info = NULL;\r\nstruct request *req;\r\nint queued;\r\npr_debug("Entered do_blkif_request\n");\r\nqueued = 0;\r\nwhile ((req = blk_peek_request(rq)) != NULL) {\r\ninfo = req->rq_disk->private_data;\r\nif (RING_FULL(&info->ring))\r\ngoto wait;\r\nblk_start_request(req);\r\nif ((req->cmd_type != REQ_TYPE_FS) ||\r\n((req->cmd_flags & (REQ_FLUSH | REQ_FUA)) &&\r\n!info->flush_op)) {\r\n__blk_end_request_all(req, -EIO);\r\ncontinue;\r\n}\r\npr_debug("do_blk_req %p: cmd %p, sec %lx, "\r\n"(%u/%u) [%s]\n",\r\nreq, req->cmd, (unsigned long)blk_rq_pos(req),\r\nblk_rq_cur_sectors(req), blk_rq_sectors(req),\r\nrq_data_dir(req) ? "write" : "read");\r\nif (blkif_queue_request(req)) {\r\nblk_requeue_request(rq, req);\r\nwait:\r\nblk_stop_queue(rq);\r\nbreak;\r\n}\r\nqueued++;\r\n}\r\nif (queued != 0)\r\nflush_requests(info);\r\n}\r\nstatic int xlvbd_init_blk_queue(struct gendisk *gd, u16 sector_size,\r\nunsigned int physical_sector_size,\r\nunsigned int segments)\r\n{\r\nstruct request_queue *rq;\r\nstruct blkfront_info *info = gd->private_data;\r\nrq = blk_init_queue(do_blkif_request, &info->io_lock);\r\nif (rq == NULL)\r\nreturn -1;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_VIRT, rq);\r\nif (info->feature_discard) {\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, rq);\r\nblk_queue_max_discard_sectors(rq, get_capacity(gd));\r\nrq->limits.discard_granularity = info->discard_granularity;\r\nrq->limits.discard_alignment = info->discard_alignment;\r\nif (info->feature_secdiscard)\r\nqueue_flag_set_unlocked(QUEUE_FLAG_SECDISCARD, rq);\r\n}\r\nblk_queue_logical_block_size(rq, sector_size);\r\nblk_queue_physical_block_size(rq, physical_sector_size);\r\nblk_queue_max_hw_sectors(rq, (segments * PAGE_SIZE) / 512);\r\nblk_queue_segment_boundary(rq, PAGE_SIZE - 1);\r\nblk_queue_max_segment_size(rq, PAGE_SIZE);\r\nblk_queue_max_segments(rq, segments);\r\nblk_queue_dma_alignment(rq, 511);\r\nblk_queue_bounce_limit(rq, BLK_BOUNCE_ANY);\r\ngd->queue = rq;\r\nreturn 0;\r\n}\r\nstatic void xlvbd_flush(struct blkfront_info *info)\r\n{\r\nblk_queue_flush(info->rq, info->feature_flush);\r\nprintk(KERN_INFO "blkfront: %s: %s: %s %s %s %s %s\n",\r\ninfo->gd->disk_name,\r\ninfo->flush_op == BLKIF_OP_WRITE_BARRIER ?\r\n"barrier" : (info->flush_op == BLKIF_OP_FLUSH_DISKCACHE ?\r\n"flush diskcache" : "barrier or flush"),\r\ninfo->feature_flush ? "enabled;" : "disabled;",\r\n"persistent grants:",\r\ninfo->feature_persistent ? "enabled;" : "disabled;",\r\n"indirect descriptors:",\r\ninfo->max_indirect_segments ? "enabled;" : "disabled;");\r\n}\r\nstatic int xen_translate_vdev(int vdevice, int *minor, unsigned int *offset)\r\n{\r\nint major;\r\nmajor = BLKIF_MAJOR(vdevice);\r\n*minor = BLKIF_MINOR(vdevice);\r\nswitch (major) {\r\ncase XEN_IDE0_MAJOR:\r\n*offset = (*minor / 64) + EMULATED_HD_DISK_NAME_OFFSET;\r\n*minor = ((*minor / 64) * PARTS_PER_DISK) +\r\nEMULATED_HD_DISK_MINOR_OFFSET;\r\nbreak;\r\ncase XEN_IDE1_MAJOR:\r\n*offset = (*minor / 64) + 2 + EMULATED_HD_DISK_NAME_OFFSET;\r\n*minor = (((*minor / 64) + 2) * PARTS_PER_DISK) +\r\nEMULATED_HD_DISK_MINOR_OFFSET;\r\nbreak;\r\ncase XEN_SCSI_DISK0_MAJOR:\r\n*offset = (*minor / PARTS_PER_DISK) + EMULATED_SD_DISK_NAME_OFFSET;\r\n*minor = *minor + EMULATED_SD_DISK_MINOR_OFFSET;\r\nbreak;\r\ncase XEN_SCSI_DISK1_MAJOR:\r\ncase XEN_SCSI_DISK2_MAJOR:\r\ncase XEN_SCSI_DISK3_MAJOR:\r\ncase XEN_SCSI_DISK4_MAJOR:\r\ncase XEN_SCSI_DISK5_MAJOR:\r\ncase XEN_SCSI_DISK6_MAJOR:\r\ncase XEN_SCSI_DISK7_MAJOR:\r\n*offset = (*minor / PARTS_PER_DISK) +\r\n((major - XEN_SCSI_DISK1_MAJOR + 1) * 16) +\r\nEMULATED_SD_DISK_NAME_OFFSET;\r\n*minor = *minor +\r\n((major - XEN_SCSI_DISK1_MAJOR + 1) * 16 * PARTS_PER_DISK) +\r\nEMULATED_SD_DISK_MINOR_OFFSET;\r\nbreak;\r\ncase XEN_SCSI_DISK8_MAJOR:\r\ncase XEN_SCSI_DISK9_MAJOR:\r\ncase XEN_SCSI_DISK10_MAJOR:\r\ncase XEN_SCSI_DISK11_MAJOR:\r\ncase XEN_SCSI_DISK12_MAJOR:\r\ncase XEN_SCSI_DISK13_MAJOR:\r\ncase XEN_SCSI_DISK14_MAJOR:\r\ncase XEN_SCSI_DISK15_MAJOR:\r\n*offset = (*minor / PARTS_PER_DISK) +\r\n((major - XEN_SCSI_DISK8_MAJOR + 8) * 16) +\r\nEMULATED_SD_DISK_NAME_OFFSET;\r\n*minor = *minor +\r\n((major - XEN_SCSI_DISK8_MAJOR + 8) * 16 * PARTS_PER_DISK) +\r\nEMULATED_SD_DISK_MINOR_OFFSET;\r\nbreak;\r\ncase XENVBD_MAJOR:\r\n*offset = *minor / PARTS_PER_DISK;\r\nbreak;\r\ndefault:\r\nprintk(KERN_WARNING "blkfront: your disk configuration is "\r\n"incorrect, please use an xvd device instead\n");\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic char *encode_disk_name(char *ptr, unsigned int n)\r\n{\r\nif (n >= 26)\r\nptr = encode_disk_name(ptr, n / 26 - 1);\r\n*ptr = 'a' + n % 26;\r\nreturn ptr + 1;\r\n}\r\nstatic int xlvbd_alloc_gendisk(blkif_sector_t capacity,\r\nstruct blkfront_info *info,\r\nu16 vdisk_info, u16 sector_size,\r\nunsigned int physical_sector_size)\r\n{\r\nstruct gendisk *gd;\r\nint nr_minors = 1;\r\nint err;\r\nunsigned int offset;\r\nint minor;\r\nint nr_parts;\r\nchar *ptr;\r\nBUG_ON(info->gd != NULL);\r\nBUG_ON(info->rq != NULL);\r\nif ((info->vdevice>>EXT_SHIFT) > 1) {\r\nprintk(KERN_WARNING "blkfront: vdevice 0x%x is above the extended range; ignoring\n", info->vdevice);\r\nreturn -ENODEV;\r\n}\r\nif (!VDEV_IS_EXTENDED(info->vdevice)) {\r\nerr = xen_translate_vdev(info->vdevice, &minor, &offset);\r\nif (err)\r\nreturn err;\r\nnr_parts = PARTS_PER_DISK;\r\n} else {\r\nminor = BLKIF_MINOR_EXT(info->vdevice);\r\nnr_parts = PARTS_PER_EXT_DISK;\r\noffset = minor / nr_parts;\r\nif (xen_hvm_domain() && offset < EMULATED_HD_DISK_NAME_OFFSET + 4)\r\nprintk(KERN_WARNING "blkfront: vdevice 0x%x might conflict with "\r\n"emulated IDE disks,\n\t choose an xvd device name"\r\n"from xvde on\n", info->vdevice);\r\n}\r\nif (minor >> MINORBITS) {\r\npr_warn("blkfront: %#x's minor (%#x) out of range; ignoring\n",\r\ninfo->vdevice, minor);\r\nreturn -ENODEV;\r\n}\r\nif ((minor % nr_parts) == 0)\r\nnr_minors = nr_parts;\r\nerr = xlbd_reserve_minors(minor, nr_minors);\r\nif (err)\r\ngoto out;\r\nerr = -ENODEV;\r\ngd = alloc_disk(nr_minors);\r\nif (gd == NULL)\r\ngoto release;\r\nstrcpy(gd->disk_name, DEV_NAME);\r\nptr = encode_disk_name(gd->disk_name + sizeof(DEV_NAME) - 1, offset);\r\nBUG_ON(ptr >= gd->disk_name + DISK_NAME_LEN);\r\nif (nr_minors > 1)\r\n*ptr = 0;\r\nelse\r\nsnprintf(ptr, gd->disk_name + DISK_NAME_LEN - ptr,\r\n"%d", minor & (nr_parts - 1));\r\ngd->major = XENVBD_MAJOR;\r\ngd->first_minor = minor;\r\ngd->fops = &xlvbd_block_fops;\r\ngd->private_data = info;\r\ngd->driverfs_dev = &(info->xbdev->dev);\r\nset_capacity(gd, capacity);\r\nif (xlvbd_init_blk_queue(gd, sector_size, physical_sector_size,\r\ninfo->max_indirect_segments ? :\r\nBLKIF_MAX_SEGMENTS_PER_REQUEST)) {\r\ndel_gendisk(gd);\r\ngoto release;\r\n}\r\ninfo->rq = gd->queue;\r\ninfo->gd = gd;\r\nxlvbd_flush(info);\r\nif (vdisk_info & VDISK_READONLY)\r\nset_disk_ro(gd, 1);\r\nif (vdisk_info & VDISK_REMOVABLE)\r\ngd->flags |= GENHD_FL_REMOVABLE;\r\nif (vdisk_info & VDISK_CDROM)\r\ngd->flags |= GENHD_FL_CD;\r\nreturn 0;\r\nrelease:\r\nxlbd_release_minors(minor, nr_minors);\r\nout:\r\nreturn err;\r\n}\r\nstatic void xlvbd_release_gendisk(struct blkfront_info *info)\r\n{\r\nunsigned int minor, nr_minors;\r\nunsigned long flags;\r\nif (info->rq == NULL)\r\nreturn;\r\nspin_lock_irqsave(&info->io_lock, flags);\r\nblk_stop_queue(info->rq);\r\ngnttab_cancel_free_callback(&info->callback);\r\nspin_unlock_irqrestore(&info->io_lock, flags);\r\nflush_work(&info->work);\r\ndel_gendisk(info->gd);\r\nminor = info->gd->first_minor;\r\nnr_minors = info->gd->minors;\r\nxlbd_release_minors(minor, nr_minors);\r\nblk_cleanup_queue(info->rq);\r\ninfo->rq = NULL;\r\nput_disk(info->gd);\r\ninfo->gd = NULL;\r\n}\r\nstatic void kick_pending_request_queues(struct blkfront_info *info)\r\n{\r\nif (!RING_FULL(&info->ring)) {\r\nblk_start_queue(info->rq);\r\ndo_blkif_request(info->rq);\r\n}\r\n}\r\nstatic void blkif_restart_queue(struct work_struct *work)\r\n{\r\nstruct blkfront_info *info = container_of(work, struct blkfront_info, work);\r\nspin_lock_irq(&info->io_lock);\r\nif (info->connected == BLKIF_STATE_CONNECTED)\r\nkick_pending_request_queues(info);\r\nspin_unlock_irq(&info->io_lock);\r\n}\r\nstatic void blkif_free(struct blkfront_info *info, int suspend)\r\n{\r\nstruct grant *persistent_gnt;\r\nstruct grant *n;\r\nint i, j, segs;\r\nspin_lock_irq(&info->io_lock);\r\ninfo->connected = suspend ?\r\nBLKIF_STATE_SUSPENDED : BLKIF_STATE_DISCONNECTED;\r\nif (info->rq)\r\nblk_stop_queue(info->rq);\r\nif (!list_empty(&info->grants)) {\r\nlist_for_each_entry_safe(persistent_gnt, n,\r\n&info->grants, node) {\r\nlist_del(&persistent_gnt->node);\r\nif (persistent_gnt->gref != GRANT_INVALID_REF) {\r\ngnttab_end_foreign_access(persistent_gnt->gref,\r\n0, 0UL);\r\ninfo->persistent_gnts_c--;\r\n}\r\nif (info->feature_persistent)\r\n__free_page(pfn_to_page(persistent_gnt->pfn));\r\nkfree(persistent_gnt);\r\n}\r\n}\r\nBUG_ON(info->persistent_gnts_c != 0);\r\nif (!list_empty(&info->indirect_pages)) {\r\nstruct page *indirect_page, *n;\r\nBUG_ON(info->feature_persistent);\r\nlist_for_each_entry_safe(indirect_page, n, &info->indirect_pages, lru) {\r\nlist_del(&indirect_page->lru);\r\n__free_page(indirect_page);\r\n}\r\n}\r\nfor (i = 0; i < BLK_RING_SIZE; i++) {\r\nif (!info->shadow[i].request)\r\ngoto free_shadow;\r\nsegs = info->shadow[i].req.operation == BLKIF_OP_INDIRECT ?\r\ninfo->shadow[i].req.u.indirect.nr_segments :\r\ninfo->shadow[i].req.u.rw.nr_segments;\r\nfor (j = 0; j < segs; j++) {\r\npersistent_gnt = info->shadow[i].grants_used[j];\r\ngnttab_end_foreign_access(persistent_gnt->gref, 0, 0UL);\r\nif (info->feature_persistent)\r\n__free_page(pfn_to_page(persistent_gnt->pfn));\r\nkfree(persistent_gnt);\r\n}\r\nif (info->shadow[i].req.operation != BLKIF_OP_INDIRECT)\r\ngoto free_shadow;\r\nfor (j = 0; j < INDIRECT_GREFS(segs); j++) {\r\npersistent_gnt = info->shadow[i].indirect_grants[j];\r\ngnttab_end_foreign_access(persistent_gnt->gref, 0, 0UL);\r\n__free_page(pfn_to_page(persistent_gnt->pfn));\r\nkfree(persistent_gnt);\r\n}\r\nfree_shadow:\r\nkfree(info->shadow[i].grants_used);\r\ninfo->shadow[i].grants_used = NULL;\r\nkfree(info->shadow[i].indirect_grants);\r\ninfo->shadow[i].indirect_grants = NULL;\r\nkfree(info->shadow[i].sg);\r\ninfo->shadow[i].sg = NULL;\r\n}\r\ngnttab_cancel_free_callback(&info->callback);\r\nspin_unlock_irq(&info->io_lock);\r\nflush_work(&info->work);\r\nif (info->ring_ref != GRANT_INVALID_REF) {\r\ngnttab_end_foreign_access(info->ring_ref, 0,\r\n(unsigned long)info->ring.sring);\r\ninfo->ring_ref = GRANT_INVALID_REF;\r\ninfo->ring.sring = NULL;\r\n}\r\nif (info->irq)\r\nunbind_from_irqhandler(info->irq, info);\r\ninfo->evtchn = info->irq = 0;\r\n}\r\nstatic void blkif_completion(struct blk_shadow *s, struct blkfront_info *info,\r\nstruct blkif_response *bret)\r\n{\r\nint i = 0;\r\nstruct scatterlist *sg;\r\nchar *bvec_data;\r\nvoid *shared_data;\r\nint nseg;\r\nnseg = s->req.operation == BLKIF_OP_INDIRECT ?\r\ns->req.u.indirect.nr_segments : s->req.u.rw.nr_segments;\r\nif (bret->operation == BLKIF_OP_READ && info->feature_persistent) {\r\nfor_each_sg(s->sg, sg, nseg, i) {\r\nBUG_ON(sg->offset + sg->length > PAGE_SIZE);\r\nshared_data = kmap_atomic(\r\npfn_to_page(s->grants_used[i]->pfn));\r\nbvec_data = kmap_atomic(sg_page(sg));\r\nmemcpy(bvec_data + sg->offset,\r\nshared_data + sg->offset,\r\nsg->length);\r\nkunmap_atomic(bvec_data);\r\nkunmap_atomic(shared_data);\r\n}\r\n}\r\nfor (i = 0; i < nseg; i++) {\r\nif (gnttab_query_foreign_access(s->grants_used[i]->gref)) {\r\nif (!info->feature_persistent)\r\npr_alert_ratelimited("backed has not unmapped grant: %u\n",\r\ns->grants_used[i]->gref);\r\nlist_add(&s->grants_used[i]->node, &info->grants);\r\ninfo->persistent_gnts_c++;\r\n} else {\r\ngnttab_end_foreign_access(s->grants_used[i]->gref, 0, 0UL);\r\ns->grants_used[i]->gref = GRANT_INVALID_REF;\r\nlist_add_tail(&s->grants_used[i]->node, &info->grants);\r\n}\r\n}\r\nif (s->req.operation == BLKIF_OP_INDIRECT) {\r\nfor (i = 0; i < INDIRECT_GREFS(nseg); i++) {\r\nif (gnttab_query_foreign_access(s->indirect_grants[i]->gref)) {\r\nif (!info->feature_persistent)\r\npr_alert_ratelimited("backed has not unmapped grant: %u\n",\r\ns->indirect_grants[i]->gref);\r\nlist_add(&s->indirect_grants[i]->node, &info->grants);\r\ninfo->persistent_gnts_c++;\r\n} else {\r\nstruct page *indirect_page;\r\ngnttab_end_foreign_access(s->indirect_grants[i]->gref, 0, 0UL);\r\nindirect_page = pfn_to_page(s->indirect_grants[i]->pfn);\r\nlist_add(&indirect_page->lru, &info->indirect_pages);\r\ns->indirect_grants[i]->gref = GRANT_INVALID_REF;\r\nlist_add_tail(&s->indirect_grants[i]->node, &info->grants);\r\n}\r\n}\r\n}\r\n}\r\nstatic irqreturn_t blkif_interrupt(int irq, void *dev_id)\r\n{\r\nstruct request *req;\r\nstruct blkif_response *bret;\r\nRING_IDX i, rp;\r\nunsigned long flags;\r\nstruct blkfront_info *info = (struct blkfront_info *)dev_id;\r\nint error;\r\nspin_lock_irqsave(&info->io_lock, flags);\r\nif (unlikely(info->connected != BLKIF_STATE_CONNECTED)) {\r\nspin_unlock_irqrestore(&info->io_lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nagain:\r\nrp = info->ring.sring->rsp_prod;\r\nrmb();\r\nfor (i = info->ring.rsp_cons; i != rp; i++) {\r\nunsigned long id;\r\nbret = RING_GET_RESPONSE(&info->ring, i);\r\nid = bret->id;\r\nif (id >= BLK_RING_SIZE) {\r\nWARN(1, "%s: response to %s has incorrect id (%ld)\n",\r\ninfo->gd->disk_name, op_name(bret->operation), id);\r\ncontinue;\r\n}\r\nreq = info->shadow[id].request;\r\nif (bret->operation != BLKIF_OP_DISCARD)\r\nblkif_completion(&info->shadow[id], info, bret);\r\nif (add_id_to_freelist(info, id)) {\r\nWARN(1, "%s: response to %s (id %ld) couldn't be recycled!\n",\r\ninfo->gd->disk_name, op_name(bret->operation), id);\r\ncontinue;\r\n}\r\nerror = (bret->status == BLKIF_RSP_OKAY) ? 0 : -EIO;\r\nswitch (bret->operation) {\r\ncase BLKIF_OP_DISCARD:\r\nif (unlikely(bret->status == BLKIF_RSP_EOPNOTSUPP)) {\r\nstruct request_queue *rq = info->rq;\r\nprintk(KERN_WARNING "blkfront: %s: %s op failed\n",\r\ninfo->gd->disk_name, op_name(bret->operation));\r\nerror = -EOPNOTSUPP;\r\ninfo->feature_discard = 0;\r\ninfo->feature_secdiscard = 0;\r\nqueue_flag_clear(QUEUE_FLAG_DISCARD, rq);\r\nqueue_flag_clear(QUEUE_FLAG_SECDISCARD, rq);\r\n}\r\n__blk_end_request_all(req, error);\r\nbreak;\r\ncase BLKIF_OP_FLUSH_DISKCACHE:\r\ncase BLKIF_OP_WRITE_BARRIER:\r\nif (unlikely(bret->status == BLKIF_RSP_EOPNOTSUPP)) {\r\nprintk(KERN_WARNING "blkfront: %s: %s op failed\n",\r\ninfo->gd->disk_name, op_name(bret->operation));\r\nerror = -EOPNOTSUPP;\r\n}\r\nif (unlikely(bret->status == BLKIF_RSP_ERROR &&\r\ninfo->shadow[id].req.u.rw.nr_segments == 0)) {\r\nprintk(KERN_WARNING "blkfront: %s: empty %s op failed\n",\r\ninfo->gd->disk_name, op_name(bret->operation));\r\nerror = -EOPNOTSUPP;\r\n}\r\nif (unlikely(error)) {\r\nif (error == -EOPNOTSUPP)\r\nerror = 0;\r\ninfo->feature_flush = 0;\r\ninfo->flush_op = 0;\r\nxlvbd_flush(info);\r\n}\r\ncase BLKIF_OP_READ:\r\ncase BLKIF_OP_WRITE:\r\nif (unlikely(bret->status != BLKIF_RSP_OKAY))\r\ndev_dbg(&info->xbdev->dev, "Bad return from blkdev data "\r\n"request: %x\n", bret->status);\r\n__blk_end_request_all(req, error);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\ninfo->ring.rsp_cons = i;\r\nif (i != info->ring.req_prod_pvt) {\r\nint more_to_do;\r\nRING_FINAL_CHECK_FOR_RESPONSES(&info->ring, more_to_do);\r\nif (more_to_do)\r\ngoto again;\r\n} else\r\ninfo->ring.sring->rsp_event = i + 1;\r\nkick_pending_request_queues(info);\r\nspin_unlock_irqrestore(&info->io_lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int setup_blkring(struct xenbus_device *dev,\r\nstruct blkfront_info *info)\r\n{\r\nstruct blkif_sring *sring;\r\nint err;\r\ninfo->ring_ref = GRANT_INVALID_REF;\r\nsring = (struct blkif_sring *)__get_free_page(GFP_NOIO | __GFP_HIGH);\r\nif (!sring) {\r\nxenbus_dev_fatal(dev, -ENOMEM, "allocating shared ring");\r\nreturn -ENOMEM;\r\n}\r\nSHARED_RING_INIT(sring);\r\nFRONT_RING_INIT(&info->ring, sring, PAGE_SIZE);\r\nerr = xenbus_grant_ring(dev, virt_to_mfn(info->ring.sring));\r\nif (err < 0) {\r\nfree_page((unsigned long)sring);\r\ninfo->ring.sring = NULL;\r\ngoto fail;\r\n}\r\ninfo->ring_ref = err;\r\nerr = xenbus_alloc_evtchn(dev, &info->evtchn);\r\nif (err)\r\ngoto fail;\r\nerr = bind_evtchn_to_irqhandler(info->evtchn, blkif_interrupt, 0,\r\n"blkif", info);\r\nif (err <= 0) {\r\nxenbus_dev_fatal(dev, err,\r\n"bind_evtchn_to_irqhandler failed");\r\ngoto fail;\r\n}\r\ninfo->irq = err;\r\nreturn 0;\r\nfail:\r\nblkif_free(info, 0);\r\nreturn err;\r\n}\r\nstatic int talk_to_blkback(struct xenbus_device *dev,\r\nstruct blkfront_info *info)\r\n{\r\nconst char *message = NULL;\r\nstruct xenbus_transaction xbt;\r\nint err;\r\nerr = setup_blkring(dev, info);\r\nif (err)\r\ngoto out;\r\nagain:\r\nerr = xenbus_transaction_start(&xbt);\r\nif (err) {\r\nxenbus_dev_fatal(dev, err, "starting transaction");\r\ngoto destroy_blkring;\r\n}\r\nerr = xenbus_printf(xbt, dev->nodename,\r\n"ring-ref", "%u", info->ring_ref);\r\nif (err) {\r\nmessage = "writing ring-ref";\r\ngoto abort_transaction;\r\n}\r\nerr = xenbus_printf(xbt, dev->nodename,\r\n"event-channel", "%u", info->evtchn);\r\nif (err) {\r\nmessage = "writing event-channel";\r\ngoto abort_transaction;\r\n}\r\nerr = xenbus_printf(xbt, dev->nodename, "protocol", "%s",\r\nXEN_IO_PROTO_ABI_NATIVE);\r\nif (err) {\r\nmessage = "writing protocol";\r\ngoto abort_transaction;\r\n}\r\nerr = xenbus_printf(xbt, dev->nodename,\r\n"feature-persistent", "%u", 1);\r\nif (err)\r\ndev_warn(&dev->dev,\r\n"writing persistent grants feature to xenbus");\r\nerr = xenbus_transaction_end(xbt, 0);\r\nif (err) {\r\nif (err == -EAGAIN)\r\ngoto again;\r\nxenbus_dev_fatal(dev, err, "completing transaction");\r\ngoto destroy_blkring;\r\n}\r\nxenbus_switch_state(dev, XenbusStateInitialised);\r\nreturn 0;\r\nabort_transaction:\r\nxenbus_transaction_end(xbt, 1);\r\nif (message)\r\nxenbus_dev_fatal(dev, err, "%s", message);\r\ndestroy_blkring:\r\nblkif_free(info, 0);\r\nout:\r\nreturn err;\r\n}\r\nstatic int blkfront_probe(struct xenbus_device *dev,\r\nconst struct xenbus_device_id *id)\r\n{\r\nint err, vdevice, i;\r\nstruct blkfront_info *info;\r\nerr = xenbus_scanf(XBT_NIL, dev->nodename,\r\n"virtual-device", "%i", &vdevice);\r\nif (err != 1) {\r\nerr = xenbus_scanf(XBT_NIL, dev->nodename, "virtual-device-ext",\r\n"%i", &vdevice);\r\nif (err != 1) {\r\nxenbus_dev_fatal(dev, err, "reading virtual-device");\r\nreturn err;\r\n}\r\n}\r\nif (xen_hvm_domain()) {\r\nchar *type;\r\nint len;\r\nif (xen_has_pv_and_legacy_disk_devices()) {\r\nint major;\r\nif (!VDEV_IS_EXTENDED(vdevice))\r\nmajor = BLKIF_MAJOR(vdevice);\r\nelse\r\nmajor = XENVBD_MAJOR;\r\nif (major != XENVBD_MAJOR) {\r\nprintk(KERN_INFO\r\n"%s: HVM does not support vbd %d as xen block device\n",\r\n__FUNCTION__, vdevice);\r\nreturn -ENODEV;\r\n}\r\n}\r\ntype = xenbus_read(XBT_NIL, dev->nodename, "device-type", &len);\r\nif (IS_ERR(type))\r\nreturn -ENODEV;\r\nif (strncmp(type, "cdrom", 5) == 0) {\r\nkfree(type);\r\nreturn -ENODEV;\r\n}\r\nkfree(type);\r\n}\r\ninfo = kzalloc(sizeof(*info), GFP_KERNEL);\r\nif (!info) {\r\nxenbus_dev_fatal(dev, -ENOMEM, "allocating info structure");\r\nreturn -ENOMEM;\r\n}\r\nmutex_init(&info->mutex);\r\nspin_lock_init(&info->io_lock);\r\ninfo->xbdev = dev;\r\ninfo->vdevice = vdevice;\r\nINIT_LIST_HEAD(&info->grants);\r\nINIT_LIST_HEAD(&info->indirect_pages);\r\ninfo->persistent_gnts_c = 0;\r\ninfo->connected = BLKIF_STATE_DISCONNECTED;\r\nINIT_WORK(&info->work, blkif_restart_queue);\r\nfor (i = 0; i < BLK_RING_SIZE; i++)\r\ninfo->shadow[i].req.u.rw.id = i+1;\r\ninfo->shadow[BLK_RING_SIZE-1].req.u.rw.id = 0x0fffffff;\r\ninfo->handle = simple_strtoul(strrchr(dev->nodename, '/')+1, NULL, 0);\r\ndev_set_drvdata(&dev->dev, info);\r\nerr = talk_to_blkback(dev, info);\r\nif (err) {\r\nkfree(info);\r\ndev_set_drvdata(&dev->dev, NULL);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void split_bio_end(struct bio *bio, int error)\r\n{\r\nstruct split_bio *split_bio = bio->bi_private;\r\nif (error)\r\nsplit_bio->err = error;\r\nif (atomic_dec_and_test(&split_bio->pending)) {\r\nsplit_bio->bio->bi_phys_segments = 0;\r\nbio_endio(split_bio->bio, split_bio->err);\r\nkfree(split_bio);\r\n}\r\nbio_put(bio);\r\n}\r\nstatic int blkif_recover(struct blkfront_info *info)\r\n{\r\nint i;\r\nstruct request *req, *n;\r\nstruct blk_shadow *copy;\r\nint rc;\r\nstruct bio *bio, *cloned_bio;\r\nstruct bio_list bio_list, merge_bio;\r\nunsigned int segs, offset;\r\nint pending, size;\r\nstruct split_bio *split_bio;\r\nstruct list_head requests;\r\ncopy = kmemdup(info->shadow, sizeof(info->shadow),\r\nGFP_NOIO | __GFP_REPEAT | __GFP_HIGH);\r\nif (!copy)\r\nreturn -ENOMEM;\r\nmemset(&info->shadow, 0, sizeof(info->shadow));\r\nfor (i = 0; i < BLK_RING_SIZE; i++)\r\ninfo->shadow[i].req.u.rw.id = i+1;\r\ninfo->shadow_free = info->ring.req_prod_pvt;\r\ninfo->shadow[BLK_RING_SIZE-1].req.u.rw.id = 0x0fffffff;\r\nrc = blkfront_setup_indirect(info);\r\nif (rc) {\r\nkfree(copy);\r\nreturn rc;\r\n}\r\nsegs = info->max_indirect_segments ? : BLKIF_MAX_SEGMENTS_PER_REQUEST;\r\nblk_queue_max_segments(info->rq, segs);\r\nbio_list_init(&bio_list);\r\nINIT_LIST_HEAD(&requests);\r\nfor (i = 0; i < BLK_RING_SIZE; i++) {\r\nif (!copy[i].request)\r\ncontinue;\r\nif (copy[i].request->cmd_flags &\r\n(REQ_FLUSH | REQ_FUA | REQ_DISCARD | REQ_SECURE)) {\r\nlist_add(&copy[i].request->queuelist, &requests);\r\ncontinue;\r\n}\r\nmerge_bio.head = copy[i].request->bio;\r\nmerge_bio.tail = copy[i].request->biotail;\r\nbio_list_merge(&bio_list, &merge_bio);\r\ncopy[i].request->bio = NULL;\r\nblk_put_request(copy[i].request);\r\n}\r\nkfree(copy);\r\nspin_lock_irq(&info->io_lock);\r\nwhile ((req = blk_fetch_request(info->rq)) != NULL) {\r\nif (req->cmd_flags &\r\n(REQ_FLUSH | REQ_FUA | REQ_DISCARD | REQ_SECURE)) {\r\nlist_add(&req->queuelist, &requests);\r\ncontinue;\r\n}\r\nmerge_bio.head = req->bio;\r\nmerge_bio.tail = req->biotail;\r\nbio_list_merge(&bio_list, &merge_bio);\r\nreq->bio = NULL;\r\nif (req->cmd_flags & (REQ_FLUSH | REQ_FUA))\r\npr_alert("diskcache flush request found!\n");\r\n__blk_put_request(info->rq, req);\r\n}\r\nspin_unlock_irq(&info->io_lock);\r\nxenbus_switch_state(info->xbdev, XenbusStateConnected);\r\nspin_lock_irq(&info->io_lock);\r\ninfo->connected = BLKIF_STATE_CONNECTED;\r\nkick_pending_request_queues(info);\r\nlist_for_each_entry_safe(req, n, &requests, queuelist) {\r\nlist_del_init(&req->queuelist);\r\nBUG_ON(req->nr_phys_segments > segs);\r\nblk_requeue_request(info->rq, req);\r\n}\r\nspin_unlock_irq(&info->io_lock);\r\nwhile ((bio = bio_list_pop(&bio_list)) != NULL) {\r\nif (bio_segments(bio) > segs) {\r\npending = (bio_segments(bio) + segs - 1) / segs;\r\nsplit_bio = kzalloc(sizeof(*split_bio), GFP_NOIO);\r\nBUG_ON(split_bio == NULL);\r\natomic_set(&split_bio->pending, pending);\r\nsplit_bio->bio = bio;\r\nfor (i = 0; i < pending; i++) {\r\noffset = (i * segs * PAGE_SIZE) >> 9;\r\nsize = min((unsigned int)(segs * PAGE_SIZE) >> 9,\r\n(unsigned int)bio_sectors(bio) - offset);\r\ncloned_bio = bio_clone(bio, GFP_NOIO);\r\nBUG_ON(cloned_bio == NULL);\r\nbio_trim(cloned_bio, offset, size);\r\ncloned_bio->bi_private = split_bio;\r\ncloned_bio->bi_end_io = split_bio_end;\r\nsubmit_bio(cloned_bio->bi_rw, cloned_bio);\r\n}\r\ncontinue;\r\n}\r\nsubmit_bio(bio->bi_rw, bio);\r\n}\r\nreturn 0;\r\n}\r\nstatic int blkfront_resume(struct xenbus_device *dev)\r\n{\r\nstruct blkfront_info *info = dev_get_drvdata(&dev->dev);\r\nint err;\r\ndev_dbg(&dev->dev, "blkfront_resume: %s\n", dev->nodename);\r\nblkif_free(info, info->connected == BLKIF_STATE_CONNECTED);\r\nerr = talk_to_blkback(dev, info);\r\nreturn err;\r\n}\r\nstatic void\r\nblkfront_closing(struct blkfront_info *info)\r\n{\r\nstruct xenbus_device *xbdev = info->xbdev;\r\nstruct block_device *bdev = NULL;\r\nmutex_lock(&info->mutex);\r\nif (xbdev->state == XenbusStateClosing) {\r\nmutex_unlock(&info->mutex);\r\nreturn;\r\n}\r\nif (info->gd)\r\nbdev = bdget_disk(info->gd, 0);\r\nmutex_unlock(&info->mutex);\r\nif (!bdev) {\r\nxenbus_frontend_closed(xbdev);\r\nreturn;\r\n}\r\nmutex_lock(&bdev->bd_mutex);\r\nif (bdev->bd_openers) {\r\nxenbus_dev_error(xbdev, -EBUSY,\r\n"Device in use; refusing to close");\r\nxenbus_switch_state(xbdev, XenbusStateClosing);\r\n} else {\r\nxlvbd_release_gendisk(info);\r\nxenbus_frontend_closed(xbdev);\r\n}\r\nmutex_unlock(&bdev->bd_mutex);\r\nbdput(bdev);\r\n}\r\nstatic void blkfront_setup_discard(struct blkfront_info *info)\r\n{\r\nint err;\r\nunsigned int discard_granularity;\r\nunsigned int discard_alignment;\r\nunsigned int discard_secure;\r\ninfo->feature_discard = 1;\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"discard-granularity", "%u", &discard_granularity,\r\n"discard-alignment", "%u", &discard_alignment,\r\nNULL);\r\nif (!err) {\r\ninfo->discard_granularity = discard_granularity;\r\ninfo->discard_alignment = discard_alignment;\r\n}\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"discard-secure", "%d", &discard_secure,\r\nNULL);\r\nif (!err)\r\ninfo->feature_secdiscard = !!discard_secure;\r\n}\r\nstatic int blkfront_setup_indirect(struct blkfront_info *info)\r\n{\r\nunsigned int indirect_segments, segs;\r\nint err, i;\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"feature-max-indirect-segments", "%u", &indirect_segments,\r\nNULL);\r\nif (err) {\r\ninfo->max_indirect_segments = 0;\r\nsegs = BLKIF_MAX_SEGMENTS_PER_REQUEST;\r\n} else {\r\ninfo->max_indirect_segments = min(indirect_segments,\r\nxen_blkif_max_segments);\r\nsegs = info->max_indirect_segments;\r\n}\r\nerr = fill_grant_buffer(info, (segs + INDIRECT_GREFS(segs)) * BLK_RING_SIZE);\r\nif (err)\r\ngoto out_of_memory;\r\nif (!info->feature_persistent && info->max_indirect_segments) {\r\nint num = INDIRECT_GREFS(segs) * BLK_RING_SIZE;\r\nBUG_ON(!list_empty(&info->indirect_pages));\r\nfor (i = 0; i < num; i++) {\r\nstruct page *indirect_page = alloc_page(GFP_NOIO);\r\nif (!indirect_page)\r\ngoto out_of_memory;\r\nlist_add(&indirect_page->lru, &info->indirect_pages);\r\n}\r\n}\r\nfor (i = 0; i < BLK_RING_SIZE; i++) {\r\ninfo->shadow[i].grants_used = kzalloc(\r\nsizeof(info->shadow[i].grants_used[0]) * segs,\r\nGFP_NOIO);\r\ninfo->shadow[i].sg = kzalloc(sizeof(info->shadow[i].sg[0]) * segs, GFP_NOIO);\r\nif (info->max_indirect_segments)\r\ninfo->shadow[i].indirect_grants = kzalloc(\r\nsizeof(info->shadow[i].indirect_grants[0]) *\r\nINDIRECT_GREFS(segs),\r\nGFP_NOIO);\r\nif ((info->shadow[i].grants_used == NULL) ||\r\n(info->shadow[i].sg == NULL) ||\r\n(info->max_indirect_segments &&\r\n(info->shadow[i].indirect_grants == NULL)))\r\ngoto out_of_memory;\r\nsg_init_table(info->shadow[i].sg, segs);\r\n}\r\nreturn 0;\r\nout_of_memory:\r\nfor (i = 0; i < BLK_RING_SIZE; i++) {\r\nkfree(info->shadow[i].grants_used);\r\ninfo->shadow[i].grants_used = NULL;\r\nkfree(info->shadow[i].sg);\r\ninfo->shadow[i].sg = NULL;\r\nkfree(info->shadow[i].indirect_grants);\r\ninfo->shadow[i].indirect_grants = NULL;\r\n}\r\nif (!list_empty(&info->indirect_pages)) {\r\nstruct page *indirect_page, *n;\r\nlist_for_each_entry_safe(indirect_page, n, &info->indirect_pages, lru) {\r\nlist_del(&indirect_page->lru);\r\n__free_page(indirect_page);\r\n}\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void blkfront_connect(struct blkfront_info *info)\r\n{\r\nunsigned long long sectors;\r\nunsigned long sector_size;\r\nunsigned int physical_sector_size;\r\nunsigned int binfo;\r\nint err;\r\nint barrier, flush, discard, persistent;\r\nswitch (info->connected) {\r\ncase BLKIF_STATE_CONNECTED:\r\nerr = xenbus_scanf(XBT_NIL, info->xbdev->otherend,\r\n"sectors", "%Lu", &sectors);\r\nif (XENBUS_EXIST_ERR(err))\r\nreturn;\r\nprintk(KERN_INFO "Setting capacity to %Lu\n",\r\nsectors);\r\nset_capacity(info->gd, sectors);\r\nrevalidate_disk(info->gd);\r\nreturn;\r\ncase BLKIF_STATE_SUSPENDED:\r\nblkif_recover(info);\r\nreturn;\r\ndefault:\r\nbreak;\r\n}\r\ndev_dbg(&info->xbdev->dev, "%s:%s.\n",\r\n__func__, info->xbdev->otherend);\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"sectors", "%llu", &sectors,\r\n"info", "%u", &binfo,\r\n"sector-size", "%lu", &sector_size,\r\nNULL);\r\nif (err) {\r\nxenbus_dev_fatal(info->xbdev, err,\r\n"reading backend fields at %s",\r\ninfo->xbdev->otherend);\r\nreturn;\r\n}\r\nerr = xenbus_scanf(XBT_NIL, info->xbdev->otherend,\r\n"physical-sector-size", "%u", &physical_sector_size);\r\nif (err != 1)\r\nphysical_sector_size = sector_size;\r\ninfo->feature_flush = 0;\r\ninfo->flush_op = 0;\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"feature-barrier", "%d", &barrier,\r\nNULL);\r\nif (!err && barrier) {\r\ninfo->feature_flush = REQ_FLUSH | REQ_FUA;\r\ninfo->flush_op = BLKIF_OP_WRITE_BARRIER;\r\n}\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"feature-flush-cache", "%d", &flush,\r\nNULL);\r\nif (!err && flush) {\r\ninfo->feature_flush = REQ_FLUSH;\r\ninfo->flush_op = BLKIF_OP_FLUSH_DISKCACHE;\r\n}\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"feature-discard", "%d", &discard,\r\nNULL);\r\nif (!err && discard)\r\nblkfront_setup_discard(info);\r\nerr = xenbus_gather(XBT_NIL, info->xbdev->otherend,\r\n"feature-persistent", "%u", &persistent,\r\nNULL);\r\nif (err)\r\ninfo->feature_persistent = 0;\r\nelse\r\ninfo->feature_persistent = persistent;\r\nerr = blkfront_setup_indirect(info);\r\nif (err) {\r\nxenbus_dev_fatal(info->xbdev, err, "setup_indirect at %s",\r\ninfo->xbdev->otherend);\r\nreturn;\r\n}\r\nerr = xlvbd_alloc_gendisk(sectors, info, binfo, sector_size,\r\nphysical_sector_size);\r\nif (err) {\r\nxenbus_dev_fatal(info->xbdev, err, "xlvbd_add at %s",\r\ninfo->xbdev->otherend);\r\nreturn;\r\n}\r\nxenbus_switch_state(info->xbdev, XenbusStateConnected);\r\nspin_lock_irq(&info->io_lock);\r\ninfo->connected = BLKIF_STATE_CONNECTED;\r\nkick_pending_request_queues(info);\r\nspin_unlock_irq(&info->io_lock);\r\nadd_disk(info->gd);\r\ninfo->is_ready = 1;\r\n}\r\nstatic void blkback_changed(struct xenbus_device *dev,\r\nenum xenbus_state backend_state)\r\n{\r\nstruct blkfront_info *info = dev_get_drvdata(&dev->dev);\r\ndev_dbg(&dev->dev, "blkfront:blkback_changed to state %d.\n", backend_state);\r\nswitch (backend_state) {\r\ncase XenbusStateInitialising:\r\ncase XenbusStateInitWait:\r\ncase XenbusStateInitialised:\r\ncase XenbusStateReconfiguring:\r\ncase XenbusStateReconfigured:\r\ncase XenbusStateUnknown:\r\nbreak;\r\ncase XenbusStateConnected:\r\nblkfront_connect(info);\r\nbreak;\r\ncase XenbusStateClosed:\r\nif (dev->state == XenbusStateClosed)\r\nbreak;\r\ncase XenbusStateClosing:\r\nblkfront_closing(info);\r\nbreak;\r\n}\r\n}\r\nstatic int blkfront_remove(struct xenbus_device *xbdev)\r\n{\r\nstruct blkfront_info *info = dev_get_drvdata(&xbdev->dev);\r\nstruct block_device *bdev = NULL;\r\nstruct gendisk *disk;\r\ndev_dbg(&xbdev->dev, "%s removed", xbdev->nodename);\r\nblkif_free(info, 0);\r\nmutex_lock(&info->mutex);\r\ndisk = info->gd;\r\nif (disk)\r\nbdev = bdget_disk(disk, 0);\r\ninfo->xbdev = NULL;\r\nmutex_unlock(&info->mutex);\r\nif (!bdev) {\r\nkfree(info);\r\nreturn 0;\r\n}\r\nmutex_lock(&bdev->bd_mutex);\r\ninfo = disk->private_data;\r\ndev_warn(disk_to_dev(disk),\r\n"%s was hot-unplugged, %d stale handles\n",\r\nxbdev->nodename, bdev->bd_openers);\r\nif (info && !bdev->bd_openers) {\r\nxlvbd_release_gendisk(info);\r\ndisk->private_data = NULL;\r\nkfree(info);\r\n}\r\nmutex_unlock(&bdev->bd_mutex);\r\nbdput(bdev);\r\nreturn 0;\r\n}\r\nstatic int blkfront_is_ready(struct xenbus_device *dev)\r\n{\r\nstruct blkfront_info *info = dev_get_drvdata(&dev->dev);\r\nreturn info->is_ready && info->xbdev;\r\n}\r\nstatic int blkif_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nstruct gendisk *disk = bdev->bd_disk;\r\nstruct blkfront_info *info;\r\nint err = 0;\r\nmutex_lock(&blkfront_mutex);\r\ninfo = disk->private_data;\r\nif (!info) {\r\nerr = -ERESTARTSYS;\r\ngoto out;\r\n}\r\nmutex_lock(&info->mutex);\r\nif (!info->gd)\r\nerr = -ERESTARTSYS;\r\nmutex_unlock(&info->mutex);\r\nout:\r\nmutex_unlock(&blkfront_mutex);\r\nreturn err;\r\n}\r\nstatic void blkif_release(struct gendisk *disk, fmode_t mode)\r\n{\r\nstruct blkfront_info *info = disk->private_data;\r\nstruct block_device *bdev;\r\nstruct xenbus_device *xbdev;\r\nmutex_lock(&blkfront_mutex);\r\nbdev = bdget_disk(disk, 0);\r\nif (!bdev) {\r\nWARN(1, "Block device %s yanked out from us!\n", disk->disk_name);\r\ngoto out_mutex;\r\n}\r\nif (bdev->bd_openers)\r\ngoto out;\r\nmutex_lock(&info->mutex);\r\nxbdev = info->xbdev;\r\nif (xbdev && xbdev->state == XenbusStateClosing) {\r\ndev_info(disk_to_dev(bdev->bd_disk), "releasing disk\n");\r\nxlvbd_release_gendisk(info);\r\nxenbus_frontend_closed(info->xbdev);\r\n}\r\nmutex_unlock(&info->mutex);\r\nif (!xbdev) {\r\ndev_info(disk_to_dev(bdev->bd_disk), "releasing disk\n");\r\nxlvbd_release_gendisk(info);\r\ndisk->private_data = NULL;\r\nkfree(info);\r\n}\r\nout:\r\nbdput(bdev);\r\nout_mutex:\r\nmutex_unlock(&blkfront_mutex);\r\n}\r\nstatic int __init xlblk_init(void)\r\n{\r\nint ret;\r\nif (!xen_domain())\r\nreturn -ENODEV;\r\nif (!xen_has_pv_disk_devices())\r\nreturn -ENODEV;\r\nif (register_blkdev(XENVBD_MAJOR, DEV_NAME)) {\r\nprintk(KERN_WARNING "xen_blk: can't get major %d with name %s\n",\r\nXENVBD_MAJOR, DEV_NAME);\r\nreturn -ENODEV;\r\n}\r\nret = xenbus_register_frontend(&blkfront_driver);\r\nif (ret) {\r\nunregister_blkdev(XENVBD_MAJOR, DEV_NAME);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __exit xlblk_exit(void)\r\n{\r\nxenbus_unregister_driver(&blkfront_driver);\r\nunregister_blkdev(XENVBD_MAJOR, DEV_NAME);\r\nkfree(minors);\r\n}
