static inline u32 enet_readl(struct bcm_enet_priv *priv, u32 off)\r\n{\r\nreturn bcm_readl(priv->base + off);\r\n}\r\nstatic inline void enet_writel(struct bcm_enet_priv *priv,\r\nu32 val, u32 off)\r\n{\r\nbcm_writel(val, priv->base + off);\r\n}\r\nstatic inline u32 enetsw_readl(struct bcm_enet_priv *priv, u32 off)\r\n{\r\nreturn bcm_readl(priv->base + off);\r\n}\r\nstatic inline void enetsw_writel(struct bcm_enet_priv *priv,\r\nu32 val, u32 off)\r\n{\r\nbcm_writel(val, priv->base + off);\r\n}\r\nstatic inline u16 enetsw_readw(struct bcm_enet_priv *priv, u32 off)\r\n{\r\nreturn bcm_readw(priv->base + off);\r\n}\r\nstatic inline void enetsw_writew(struct bcm_enet_priv *priv,\r\nu16 val, u32 off)\r\n{\r\nbcm_writew(val, priv->base + off);\r\n}\r\nstatic inline u8 enetsw_readb(struct bcm_enet_priv *priv, u32 off)\r\n{\r\nreturn bcm_readb(priv->base + off);\r\n}\r\nstatic inline void enetsw_writeb(struct bcm_enet_priv *priv,\r\nu8 val, u32 off)\r\n{\r\nbcm_writeb(val, priv->base + off);\r\n}\r\nstatic inline u32 enet_dma_readl(struct bcm_enet_priv *priv, u32 off)\r\n{\r\nreturn bcm_readl(bcm_enet_shared_base[0] + off);\r\n}\r\nstatic inline void enet_dma_writel(struct bcm_enet_priv *priv,\r\nu32 val, u32 off)\r\n{\r\nbcm_writel(val, bcm_enet_shared_base[0] + off);\r\n}\r\nstatic inline u32 enet_dmac_readl(struct bcm_enet_priv *priv, u32 off, int chan)\r\n{\r\nreturn bcm_readl(bcm_enet_shared_base[1] +\r\nbcm63xx_enetdmacreg(off) + chan * priv->dma_chan_width);\r\n}\r\nstatic inline void enet_dmac_writel(struct bcm_enet_priv *priv,\r\nu32 val, u32 off, int chan)\r\n{\r\nbcm_writel(val, bcm_enet_shared_base[1] +\r\nbcm63xx_enetdmacreg(off) + chan * priv->dma_chan_width);\r\n}\r\nstatic inline u32 enet_dmas_readl(struct bcm_enet_priv *priv, u32 off, int chan)\r\n{\r\nreturn bcm_readl(bcm_enet_shared_base[2] + off + chan * priv->dma_chan_width);\r\n}\r\nstatic inline void enet_dmas_writel(struct bcm_enet_priv *priv,\r\nu32 val, u32 off, int chan)\r\n{\r\nbcm_writel(val, bcm_enet_shared_base[2] + off + chan * priv->dma_chan_width);\r\n}\r\nstatic int do_mdio_op(struct bcm_enet_priv *priv, unsigned int data)\r\n{\r\nint limit;\r\nenet_writel(priv, ENET_IR_MII, ENET_IR_REG);\r\nenet_writel(priv, data, ENET_MIIDATA_REG);\r\nwmb();\r\nlimit = 1000;\r\ndo {\r\nif (enet_readl(priv, ENET_IR_REG) & ENET_IR_MII)\r\nbreak;\r\nudelay(1);\r\n} while (limit-- > 0);\r\nreturn (limit < 0) ? 1 : 0;\r\n}\r\nstatic int bcm_enet_mdio_read(struct bcm_enet_priv *priv, int mii_id,\r\nint regnum)\r\n{\r\nu32 tmp, val;\r\ntmp = regnum << ENET_MIIDATA_REG_SHIFT;\r\ntmp |= 0x2 << ENET_MIIDATA_TA_SHIFT;\r\ntmp |= mii_id << ENET_MIIDATA_PHYID_SHIFT;\r\ntmp |= ENET_MIIDATA_OP_READ_MASK;\r\nif (do_mdio_op(priv, tmp))\r\nreturn -1;\r\nval = enet_readl(priv, ENET_MIIDATA_REG);\r\nval &= 0xffff;\r\nreturn val;\r\n}\r\nstatic int bcm_enet_mdio_write(struct bcm_enet_priv *priv, int mii_id,\r\nint regnum, u16 value)\r\n{\r\nu32 tmp;\r\ntmp = (value & 0xffff) << ENET_MIIDATA_DATA_SHIFT;\r\ntmp |= 0x2 << ENET_MIIDATA_TA_SHIFT;\r\ntmp |= regnum << ENET_MIIDATA_REG_SHIFT;\r\ntmp |= mii_id << ENET_MIIDATA_PHYID_SHIFT;\r\ntmp |= ENET_MIIDATA_OP_WRITE_MASK;\r\n(void)do_mdio_op(priv, tmp);\r\nreturn 0;\r\n}\r\nstatic int bcm_enet_mdio_read_phylib(struct mii_bus *bus, int mii_id,\r\nint regnum)\r\n{\r\nreturn bcm_enet_mdio_read(bus->priv, mii_id, regnum);\r\n}\r\nstatic int bcm_enet_mdio_write_phylib(struct mii_bus *bus, int mii_id,\r\nint regnum, u16 value)\r\n{\r\nreturn bcm_enet_mdio_write(bus->priv, mii_id, regnum, value);\r\n}\r\nstatic int bcm_enet_mdio_read_mii(struct net_device *dev, int mii_id,\r\nint regnum)\r\n{\r\nreturn bcm_enet_mdio_read(netdev_priv(dev), mii_id, regnum);\r\n}\r\nstatic void bcm_enet_mdio_write_mii(struct net_device *dev, int mii_id,\r\nint regnum, int value)\r\n{\r\nbcm_enet_mdio_write(netdev_priv(dev), mii_id, regnum, value);\r\n}\r\nstatic int bcm_enet_refill_rx(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nwhile (priv->rx_desc_count < priv->rx_ring_size) {\r\nstruct bcm_enet_desc *desc;\r\nstruct sk_buff *skb;\r\ndma_addr_t p;\r\nint desc_idx;\r\nu32 len_stat;\r\ndesc_idx = priv->rx_dirty_desc;\r\ndesc = &priv->rx_desc_cpu[desc_idx];\r\nif (!priv->rx_skb[desc_idx]) {\r\nskb = netdev_alloc_skb(dev, priv->rx_skb_size);\r\nif (!skb)\r\nbreak;\r\npriv->rx_skb[desc_idx] = skb;\r\np = dma_map_single(&priv->pdev->dev, skb->data,\r\npriv->rx_skb_size,\r\nDMA_FROM_DEVICE);\r\ndesc->address = p;\r\n}\r\nlen_stat = priv->rx_skb_size << DMADESC_LENGTH_SHIFT;\r\nlen_stat |= DMADESC_OWNER_MASK;\r\nif (priv->rx_dirty_desc == priv->rx_ring_size - 1) {\r\nlen_stat |= (DMADESC_WRAP_MASK >> priv->dma_desc_shift);\r\npriv->rx_dirty_desc = 0;\r\n} else {\r\npriv->rx_dirty_desc++;\r\n}\r\nwmb();\r\ndesc->len_stat = len_stat;\r\npriv->rx_desc_count++;\r\nif (priv->dma_has_sram)\r\nenet_dma_writel(priv, 1, ENETDMA_BUFALLOC_REG(priv->rx_chan));\r\nelse\r\nenet_dmac_writel(priv, 1, ENETDMAC_BUFALLOC, priv->rx_chan);\r\n}\r\nif (priv->rx_desc_count == 0 && netif_running(dev)) {\r\ndev_warn(&priv->pdev->dev, "unable to refill rx ring\n");\r\npriv->rx_timeout.expires = jiffies + HZ;\r\nadd_timer(&priv->rx_timeout);\r\n}\r\nreturn 0;\r\n}\r\nstatic void bcm_enet_refill_rx_timer(unsigned long data)\r\n{\r\nstruct net_device *dev;\r\nstruct bcm_enet_priv *priv;\r\ndev = (struct net_device *)data;\r\npriv = netdev_priv(dev);\r\nspin_lock(&priv->rx_lock);\r\nbcm_enet_refill_rx((struct net_device *)data);\r\nspin_unlock(&priv->rx_lock);\r\n}\r\nstatic int bcm_enet_receive_queue(struct net_device *dev, int budget)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct device *kdev;\r\nint processed;\r\npriv = netdev_priv(dev);\r\nkdev = &priv->pdev->dev;\r\nprocessed = 0;\r\nif (budget > priv->rx_desc_count)\r\nbudget = priv->rx_desc_count;\r\ndo {\r\nstruct bcm_enet_desc *desc;\r\nstruct sk_buff *skb;\r\nint desc_idx;\r\nu32 len_stat;\r\nunsigned int len;\r\ndesc_idx = priv->rx_curr_desc;\r\ndesc = &priv->rx_desc_cpu[desc_idx];\r\nrmb();\r\nlen_stat = desc->len_stat;\r\nif (len_stat & DMADESC_OWNER_MASK)\r\nbreak;\r\nprocessed++;\r\npriv->rx_curr_desc++;\r\nif (priv->rx_curr_desc == priv->rx_ring_size)\r\npriv->rx_curr_desc = 0;\r\npriv->rx_desc_count--;\r\nif ((len_stat & (DMADESC_ESOP_MASK >> priv->dma_desc_shift)) !=\r\n(DMADESC_ESOP_MASK >> priv->dma_desc_shift)) {\r\ndev->stats.rx_dropped++;\r\ncontinue;\r\n}\r\nif (!priv->enet_is_sw &&\r\nunlikely(len_stat & DMADESC_ERR_MASK)) {\r\ndev->stats.rx_errors++;\r\nif (len_stat & DMADESC_OVSIZE_MASK)\r\ndev->stats.rx_length_errors++;\r\nif (len_stat & DMADESC_CRC_MASK)\r\ndev->stats.rx_crc_errors++;\r\nif (len_stat & DMADESC_UNDER_MASK)\r\ndev->stats.rx_frame_errors++;\r\nif (len_stat & DMADESC_OV_MASK)\r\ndev->stats.rx_fifo_errors++;\r\ncontinue;\r\n}\r\nskb = priv->rx_skb[desc_idx];\r\nlen = (len_stat & DMADESC_LENGTH_MASK) >> DMADESC_LENGTH_SHIFT;\r\nlen -= 4;\r\nif (len < copybreak) {\r\nstruct sk_buff *nskb;\r\nnskb = netdev_alloc_skb_ip_align(dev, len);\r\nif (!nskb) {\r\ndev->stats.rx_dropped++;\r\ncontinue;\r\n}\r\ndma_sync_single_for_cpu(kdev, desc->address,\r\nlen, DMA_FROM_DEVICE);\r\nmemcpy(nskb->data, skb->data, len);\r\ndma_sync_single_for_device(kdev, desc->address,\r\nlen, DMA_FROM_DEVICE);\r\nskb = nskb;\r\n} else {\r\ndma_unmap_single(&priv->pdev->dev, desc->address,\r\npriv->rx_skb_size, DMA_FROM_DEVICE);\r\npriv->rx_skb[desc_idx] = NULL;\r\n}\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += len;\r\nnetif_receive_skb(skb);\r\n} while (--budget > 0);\r\nif (processed || !priv->rx_desc_count) {\r\nbcm_enet_refill_rx(dev);\r\nenet_dmac_writel(priv, priv->dma_chan_en_mask,\r\nENETDMAC_CHANCFG, priv->rx_chan);\r\n}\r\nreturn processed;\r\n}\r\nstatic int bcm_enet_tx_reclaim(struct net_device *dev, int force)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nint released;\r\npriv = netdev_priv(dev);\r\nreleased = 0;\r\nwhile (priv->tx_desc_count < priv->tx_ring_size) {\r\nstruct bcm_enet_desc *desc;\r\nstruct sk_buff *skb;\r\nspin_lock(&priv->tx_lock);\r\ndesc = &priv->tx_desc_cpu[priv->tx_dirty_desc];\r\nif (!force && (desc->len_stat & DMADESC_OWNER_MASK)) {\r\nspin_unlock(&priv->tx_lock);\r\nbreak;\r\n}\r\nrmb();\r\nskb = priv->tx_skb[priv->tx_dirty_desc];\r\npriv->tx_skb[priv->tx_dirty_desc] = NULL;\r\ndma_unmap_single(&priv->pdev->dev, desc->address, skb->len,\r\nDMA_TO_DEVICE);\r\npriv->tx_dirty_desc++;\r\nif (priv->tx_dirty_desc == priv->tx_ring_size)\r\npriv->tx_dirty_desc = 0;\r\npriv->tx_desc_count++;\r\nspin_unlock(&priv->tx_lock);\r\nif (desc->len_stat & DMADESC_UNDER_MASK)\r\ndev->stats.tx_errors++;\r\ndev_kfree_skb(skb);\r\nreleased++;\r\n}\r\nif (netif_queue_stopped(dev) && released)\r\nnetif_wake_queue(dev);\r\nreturn released;\r\n}\r\nstatic int bcm_enet_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct net_device *dev;\r\nint tx_work_done, rx_work_done;\r\npriv = container_of(napi, struct bcm_enet_priv, napi);\r\ndev = priv->net_dev;\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IR, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IR, priv->tx_chan);\r\ntx_work_done = bcm_enet_tx_reclaim(dev, 0);\r\nspin_lock(&priv->rx_lock);\r\nrx_work_done = bcm_enet_receive_queue(dev, budget);\r\nspin_unlock(&priv->rx_lock);\r\nif (rx_work_done >= budget || tx_work_done > 0) {\r\nreturn rx_work_done;\r\n}\r\nnapi_complete(napi);\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IRMASK, priv->tx_chan);\r\nreturn rx_work_done;\r\n}\r\nstatic irqreturn_t bcm_enet_isr_mac(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev;\r\nstruct bcm_enet_priv *priv;\r\nu32 stat;\r\ndev = dev_id;\r\npriv = netdev_priv(dev);\r\nstat = enet_readl(priv, ENET_IR_REG);\r\nif (!(stat & ENET_IR_MIB))\r\nreturn IRQ_NONE;\r\nenet_writel(priv, ENET_IR_MIB, ENET_IR_REG);\r\nenet_writel(priv, 0, ENET_IRMASK_REG);\r\nschedule_work(&priv->mib_update_task);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t bcm_enet_isr_dma(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev;\r\nstruct bcm_enet_priv *priv;\r\ndev = dev_id;\r\npriv = netdev_priv(dev);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);\r\nnapi_schedule(&priv->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int bcm_enet_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct bcm_enet_desc *desc;\r\nu32 len_stat;\r\nint ret;\r\npriv = netdev_priv(dev);\r\nspin_lock(&priv->tx_lock);\r\nif (unlikely(!priv->tx_desc_count)) {\r\nnetif_stop_queue(dev);\r\ndev_err(&priv->pdev->dev, "xmit called with no tx desc "\r\n"available?\n");\r\nret = NETDEV_TX_BUSY;\r\ngoto out_unlock;\r\n}\r\nif (priv->enet_is_sw && skb->len < 64) {\r\nint needed = 64 - skb->len;\r\nchar *data;\r\nif (unlikely(skb_tailroom(skb) < needed)) {\r\nstruct sk_buff *nskb;\r\nnskb = skb_copy_expand(skb, 0, needed, GFP_ATOMIC);\r\nif (!nskb) {\r\nret = NETDEV_TX_BUSY;\r\ngoto out_unlock;\r\n}\r\ndev_kfree_skb(skb);\r\nskb = nskb;\r\n}\r\ndata = skb_put(skb, needed);\r\nmemset(data, 0, needed);\r\n}\r\ndesc = &priv->tx_desc_cpu[priv->tx_curr_desc];\r\npriv->tx_skb[priv->tx_curr_desc] = skb;\r\ndesc->address = dma_map_single(&priv->pdev->dev, skb->data, skb->len,\r\nDMA_TO_DEVICE);\r\nlen_stat = (skb->len << DMADESC_LENGTH_SHIFT) & DMADESC_LENGTH_MASK;\r\nlen_stat |= (DMADESC_ESOP_MASK >> priv->dma_desc_shift) |\r\nDMADESC_APPEND_CRC |\r\nDMADESC_OWNER_MASK;\r\npriv->tx_curr_desc++;\r\nif (priv->tx_curr_desc == priv->tx_ring_size) {\r\npriv->tx_curr_desc = 0;\r\nlen_stat |= (DMADESC_WRAP_MASK >> priv->dma_desc_shift);\r\n}\r\npriv->tx_desc_count--;\r\nwmb();\r\ndesc->len_stat = len_stat;\r\nwmb();\r\nenet_dmac_writel(priv, priv->dma_chan_en_mask,\r\nENETDMAC_CHANCFG, priv->tx_chan);\r\nif (!priv->tx_desc_count)\r\nnetif_stop_queue(dev);\r\ndev->stats.tx_bytes += skb->len;\r\ndev->stats.tx_packets++;\r\nret = NETDEV_TX_OK;\r\nout_unlock:\r\nspin_unlock(&priv->tx_lock);\r\nreturn ret;\r\n}\r\nstatic int bcm_enet_set_mac_address(struct net_device *dev, void *p)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct sockaddr *addr = p;\r\nu32 val;\r\npriv = netdev_priv(dev);\r\nmemcpy(dev->dev_addr, addr->sa_data, ETH_ALEN);\r\nval = (dev->dev_addr[2] << 24) | (dev->dev_addr[3] << 16) |\r\n(dev->dev_addr[4] << 8) | dev->dev_addr[5];\r\nenet_writel(priv, val, ENET_PML_REG(0));\r\nval = (dev->dev_addr[0] << 8 | dev->dev_addr[1]);\r\nval |= ENET_PMH_DATAVALID_MASK;\r\nenet_writel(priv, val, ENET_PMH_REG(0));\r\nreturn 0;\r\n}\r\nstatic void bcm_enet_set_multicast_list(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct netdev_hw_addr *ha;\r\nu32 val;\r\nint i;\r\npriv = netdev_priv(dev);\r\nval = enet_readl(priv, ENET_RXCFG_REG);\r\nif (dev->flags & IFF_PROMISC)\r\nval |= ENET_RXCFG_PROMISC_MASK;\r\nelse\r\nval &= ~ENET_RXCFG_PROMISC_MASK;\r\nif ((dev->flags & IFF_ALLMULTI) || netdev_mc_count(dev) > 3)\r\nval |= ENET_RXCFG_ALLMCAST_MASK;\r\nelse\r\nval &= ~ENET_RXCFG_ALLMCAST_MASK;\r\nif (val & ENET_RXCFG_ALLMCAST_MASK) {\r\nenet_writel(priv, val, ENET_RXCFG_REG);\r\nreturn;\r\n}\r\ni = 0;\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nu8 *dmi_addr;\r\nu32 tmp;\r\nif (i == 3)\r\nbreak;\r\ndmi_addr = ha->addr;\r\ntmp = (dmi_addr[2] << 24) | (dmi_addr[3] << 16) |\r\n(dmi_addr[4] << 8) | dmi_addr[5];\r\nenet_writel(priv, tmp, ENET_PML_REG(i + 1));\r\ntmp = (dmi_addr[0] << 8 | dmi_addr[1]);\r\ntmp |= ENET_PMH_DATAVALID_MASK;\r\nenet_writel(priv, tmp, ENET_PMH_REG(i++ + 1));\r\n}\r\nfor (; i < 3; i++) {\r\nenet_writel(priv, 0, ENET_PML_REG(i + 1));\r\nenet_writel(priv, 0, ENET_PMH_REG(i + 1));\r\n}\r\nenet_writel(priv, val, ENET_RXCFG_REG);\r\n}\r\nstatic void bcm_enet_set_duplex(struct bcm_enet_priv *priv, int fullduplex)\r\n{\r\nu32 val;\r\nval = enet_readl(priv, ENET_TXCTL_REG);\r\nif (fullduplex)\r\nval |= ENET_TXCTL_FD_MASK;\r\nelse\r\nval &= ~ENET_TXCTL_FD_MASK;\r\nenet_writel(priv, val, ENET_TXCTL_REG);\r\n}\r\nstatic void bcm_enet_set_flow(struct bcm_enet_priv *priv, int rx_en, int tx_en)\r\n{\r\nu32 val;\r\nval = enet_readl(priv, ENET_RXCFG_REG);\r\nif (rx_en)\r\nval |= ENET_RXCFG_ENFLOW_MASK;\r\nelse\r\nval &= ~ENET_RXCFG_ENFLOW_MASK;\r\nenet_writel(priv, val, ENET_RXCFG_REG);\r\nif (!priv->dma_has_sram)\r\nreturn;\r\nval = enet_dma_readl(priv, ENETDMA_CFG_REG);\r\nif (tx_en)\r\nval |= ENETDMA_CFG_FLOWCH_MASK(priv->rx_chan);\r\nelse\r\nval &= ~ENETDMA_CFG_FLOWCH_MASK(priv->rx_chan);\r\nenet_dma_writel(priv, val, ENETDMA_CFG_REG);\r\n}\r\nstatic void bcm_enet_adjust_phy_link(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct phy_device *phydev;\r\nint status_changed;\r\npriv = netdev_priv(dev);\r\nphydev = priv->phydev;\r\nstatus_changed = 0;\r\nif (priv->old_link != phydev->link) {\r\nstatus_changed = 1;\r\npriv->old_link = phydev->link;\r\n}\r\nif (phydev->link && phydev->duplex != priv->old_duplex) {\r\nbcm_enet_set_duplex(priv,\r\n(phydev->duplex == DUPLEX_FULL) ? 1 : 0);\r\nstatus_changed = 1;\r\npriv->old_duplex = phydev->duplex;\r\n}\r\nif (phydev->link && phydev->pause != priv->old_pause) {\r\nint rx_pause_en, tx_pause_en;\r\nif (phydev->pause) {\r\nrx_pause_en = 1;\r\ntx_pause_en = 1;\r\n} else if (!priv->pause_auto) {\r\nrx_pause_en = priv->pause_rx;\r\ntx_pause_en = priv->pause_tx;\r\n} else {\r\nrx_pause_en = 0;\r\ntx_pause_en = 0;\r\n}\r\nbcm_enet_set_flow(priv, rx_pause_en, tx_pause_en);\r\nstatus_changed = 1;\r\npriv->old_pause = phydev->pause;\r\n}\r\nif (status_changed) {\r\npr_info("%s: link %s", dev->name, phydev->link ?\r\n"UP" : "DOWN");\r\nif (phydev->link)\r\npr_cont(" - %d/%s - flow control %s", phydev->speed,\r\nDUPLEX_FULL == phydev->duplex ? "full" : "half",\r\nphydev->pause == 1 ? "rx&tx" : "off");\r\npr_cont("\n");\r\n}\r\n}\r\nstatic void bcm_enet_adjust_link(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nbcm_enet_set_duplex(priv, priv->force_duplex_full);\r\nbcm_enet_set_flow(priv, priv->pause_rx, priv->pause_tx);\r\nnetif_carrier_on(dev);\r\npr_info("%s: link forced UP - %d/%s - flow control %s/%s\n",\r\ndev->name,\r\npriv->force_speed_100 ? 100 : 10,\r\npriv->force_duplex_full ? "full" : "half",\r\npriv->pause_rx ? "rx" : "off",\r\npriv->pause_tx ? "tx" : "off");\r\n}\r\nstatic int bcm_enet_open(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct sockaddr addr;\r\nstruct device *kdev;\r\nstruct phy_device *phydev;\r\nint i, ret;\r\nunsigned int size;\r\nchar phy_id[MII_BUS_ID_SIZE + 3];\r\nvoid *p;\r\nu32 val;\r\npriv = netdev_priv(dev);\r\nkdev = &priv->pdev->dev;\r\nif (priv->has_phy) {\r\nsnprintf(phy_id, sizeof(phy_id), PHY_ID_FMT,\r\npriv->mii_bus->id, priv->phy_id);\r\nphydev = phy_connect(dev, phy_id, bcm_enet_adjust_phy_link,\r\nPHY_INTERFACE_MODE_MII);\r\nif (IS_ERR(phydev)) {\r\ndev_err(kdev, "could not attach to PHY\n");\r\nreturn PTR_ERR(phydev);\r\n}\r\nphydev->supported &= (SUPPORTED_10baseT_Half |\r\nSUPPORTED_10baseT_Full |\r\nSUPPORTED_100baseT_Half |\r\nSUPPORTED_100baseT_Full |\r\nSUPPORTED_Autoneg |\r\nSUPPORTED_Pause |\r\nSUPPORTED_MII);\r\nphydev->advertising = phydev->supported;\r\nif (priv->pause_auto && priv->pause_rx && priv->pause_tx)\r\nphydev->advertising |= SUPPORTED_Pause;\r\nelse\r\nphydev->advertising &= ~SUPPORTED_Pause;\r\ndev_info(kdev, "attached PHY at address %d [%s]\n",\r\nphydev->addr, phydev->drv->name);\r\npriv->old_link = 0;\r\npriv->old_duplex = -1;\r\npriv->old_pause = -1;\r\npriv->phydev = phydev;\r\n}\r\nenet_writel(priv, 0, ENET_IRMASK_REG);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);\r\nret = request_irq(dev->irq, bcm_enet_isr_mac, 0, dev->name, dev);\r\nif (ret)\r\ngoto out_phy_disconnect;\r\nret = request_irq(priv->irq_rx, bcm_enet_isr_dma, 0,\r\ndev->name, dev);\r\nif (ret)\r\ngoto out_freeirq;\r\nret = request_irq(priv->irq_tx, bcm_enet_isr_dma,\r\n0, dev->name, dev);\r\nif (ret)\r\ngoto out_freeirq_rx;\r\nfor (i = 0; i < 4; i++) {\r\nenet_writel(priv, 0, ENET_PML_REG(i));\r\nenet_writel(priv, 0, ENET_PMH_REG(i));\r\n}\r\nmemcpy(addr.sa_data, dev->dev_addr, ETH_ALEN);\r\nbcm_enet_set_mac_address(dev, &addr);\r\nsize = priv->rx_ring_size * sizeof(struct bcm_enet_desc);\r\np = dma_zalloc_coherent(kdev, size, &priv->rx_desc_dma, GFP_KERNEL);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out_freeirq_tx;\r\n}\r\npriv->rx_desc_alloc_size = size;\r\npriv->rx_desc_cpu = p;\r\nsize = priv->tx_ring_size * sizeof(struct bcm_enet_desc);\r\np = dma_zalloc_coherent(kdev, size, &priv->tx_desc_dma, GFP_KERNEL);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out_free_rx_ring;\r\n}\r\npriv->tx_desc_alloc_size = size;\r\npriv->tx_desc_cpu = p;\r\npriv->tx_skb = kcalloc(priv->tx_ring_size, sizeof(struct sk_buff *),\r\nGFP_KERNEL);\r\nif (!priv->tx_skb) {\r\nret = -ENOMEM;\r\ngoto out_free_tx_ring;\r\n}\r\npriv->tx_desc_count = priv->tx_ring_size;\r\npriv->tx_dirty_desc = 0;\r\npriv->tx_curr_desc = 0;\r\nspin_lock_init(&priv->tx_lock);\r\npriv->rx_skb = kcalloc(priv->rx_ring_size, sizeof(struct sk_buff *),\r\nGFP_KERNEL);\r\nif (!priv->rx_skb) {\r\nret = -ENOMEM;\r\ngoto out_free_tx_skb;\r\n}\r\npriv->rx_desc_count = 0;\r\npriv->rx_dirty_desc = 0;\r\npriv->rx_curr_desc = 0;\r\nif (priv->dma_has_sram)\r\nenet_dma_writel(priv, ENETDMA_BUFALLOC_FORCE_MASK | 0,\r\nENETDMA_BUFALLOC_REG(priv->rx_chan));\r\nelse\r\nenet_dmac_writel(priv, ENETDMA_BUFALLOC_FORCE_MASK | 0,\r\nENETDMAC_BUFALLOC, priv->rx_chan);\r\nif (bcm_enet_refill_rx(dev)) {\r\ndev_err(kdev, "cannot allocate rx skb queue\n");\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (priv->dma_has_sram) {\r\nenet_dmas_writel(priv, priv->rx_desc_dma,\r\nENETDMAS_RSTART_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, priv->tx_desc_dma,\r\nENETDMAS_RSTART_REG, priv->tx_chan);\r\n} else {\r\nenet_dmac_writel(priv, priv->rx_desc_dma,\r\nENETDMAC_RSTART, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->tx_desc_dma,\r\nENETDMAC_RSTART, priv->tx_chan);\r\n}\r\nif (priv->dma_has_sram) {\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->tx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->tx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->tx_chan);\r\n} else {\r\nenet_dmac_writel(priv, 0, ENETDMAC_FC, priv->rx_chan);\r\nenet_dmac_writel(priv, 0, ENETDMAC_FC, priv->tx_chan);\r\n}\r\nenet_writel(priv, priv->hw_mtu, ENET_RXMAXLEN_REG);\r\nenet_writel(priv, priv->hw_mtu, ENET_TXMAXLEN_REG);\r\nenet_dmac_writel(priv, priv->dma_maxburst,\r\nENETDMAC_MAXBURST, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->dma_maxburst,\r\nENETDMAC_MAXBURST, priv->tx_chan);\r\nenet_writel(priv, BCMENET_TX_FIFO_TRESH, ENET_TXWMARK_REG);\r\nif (priv->dma_has_sram) {\r\nval = priv->rx_ring_size / 3;\r\nenet_dma_writel(priv, val, ENETDMA_FLOWCL_REG(priv->rx_chan));\r\nval = (priv->rx_ring_size * 2) / 3;\r\nenet_dma_writel(priv, val, ENETDMA_FLOWCH_REG(priv->rx_chan));\r\n} else {\r\nenet_dmac_writel(priv, 5, ENETDMAC_FC, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->rx_ring_size, ENETDMAC_LEN, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->tx_ring_size, ENETDMAC_LEN, priv->tx_chan);\r\n}\r\nwmb();\r\nval = enet_readl(priv, ENET_CTL_REG);\r\nval |= ENET_CTL_ENABLE_MASK;\r\nenet_writel(priv, val, ENET_CTL_REG);\r\nenet_dma_writel(priv, ENETDMA_CFG_EN_MASK, ENETDMA_CFG_REG);\r\nenet_dmac_writel(priv, priv->dma_chan_en_mask,\r\nENETDMAC_CHANCFG, priv->rx_chan);\r\nenet_writel(priv, ENET_IR_MIB, ENET_IR_REG);\r\nenet_writel(priv, ENET_IR_MIB, ENET_IRMASK_REG);\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IR, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IR, priv->tx_chan);\r\nnapi_enable(&priv->napi);\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->dma_chan_int_mask,\r\nENETDMAC_IRMASK, priv->tx_chan);\r\nif (priv->has_phy)\r\nphy_start(priv->phydev);\r\nelse\r\nbcm_enet_adjust_link(dev);\r\nnetif_start_queue(dev);\r\nreturn 0;\r\nout:\r\nfor (i = 0; i < priv->rx_ring_size; i++) {\r\nstruct bcm_enet_desc *desc;\r\nif (!priv->rx_skb[i])\r\ncontinue;\r\ndesc = &priv->rx_desc_cpu[i];\r\ndma_unmap_single(kdev, desc->address, priv->rx_skb_size,\r\nDMA_FROM_DEVICE);\r\nkfree_skb(priv->rx_skb[i]);\r\n}\r\nkfree(priv->rx_skb);\r\nout_free_tx_skb:\r\nkfree(priv->tx_skb);\r\nout_free_tx_ring:\r\ndma_free_coherent(kdev, priv->tx_desc_alloc_size,\r\npriv->tx_desc_cpu, priv->tx_desc_dma);\r\nout_free_rx_ring:\r\ndma_free_coherent(kdev, priv->rx_desc_alloc_size,\r\npriv->rx_desc_cpu, priv->rx_desc_dma);\r\nout_freeirq_tx:\r\nfree_irq(priv->irq_tx, dev);\r\nout_freeirq_rx:\r\nfree_irq(priv->irq_rx, dev);\r\nout_freeirq:\r\nfree_irq(dev->irq, dev);\r\nout_phy_disconnect:\r\nphy_disconnect(priv->phydev);\r\nreturn ret;\r\n}\r\nstatic void bcm_enet_disable_mac(struct bcm_enet_priv *priv)\r\n{\r\nint limit;\r\nu32 val;\r\nval = enet_readl(priv, ENET_CTL_REG);\r\nval |= ENET_CTL_DISABLE_MASK;\r\nenet_writel(priv, val, ENET_CTL_REG);\r\nlimit = 1000;\r\ndo {\r\nu32 val;\r\nval = enet_readl(priv, ENET_CTL_REG);\r\nif (!(val & ENET_CTL_DISABLE_MASK))\r\nbreak;\r\nudelay(1);\r\n} while (limit--);\r\n}\r\nstatic void bcm_enet_disable_dma(struct bcm_enet_priv *priv, int chan)\r\n{\r\nint limit;\r\nenet_dmac_writel(priv, 0, ENETDMAC_CHANCFG, chan);\r\nlimit = 1000;\r\ndo {\r\nu32 val;\r\nval = enet_dmac_readl(priv, ENETDMAC_CHANCFG, chan);\r\nif (!(val & ENETDMAC_CHANCFG_EN_MASK))\r\nbreak;\r\nudelay(1);\r\n} while (limit--);\r\n}\r\nstatic int bcm_enet_stop(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct device *kdev;\r\nint i;\r\npriv = netdev_priv(dev);\r\nkdev = &priv->pdev->dev;\r\nnetif_stop_queue(dev);\r\nnapi_disable(&priv->napi);\r\nif (priv->has_phy)\r\nphy_stop(priv->phydev);\r\ndel_timer_sync(&priv->rx_timeout);\r\nenet_writel(priv, 0, ENET_IRMASK_REG);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);\r\ncancel_work_sync(&priv->mib_update_task);\r\nbcm_enet_disable_dma(priv, priv->tx_chan);\r\nbcm_enet_disable_dma(priv, priv->rx_chan);\r\nbcm_enet_disable_mac(priv);\r\nbcm_enet_tx_reclaim(dev, 1);\r\nfor (i = 0; i < priv->rx_ring_size; i++) {\r\nstruct bcm_enet_desc *desc;\r\nif (!priv->rx_skb[i])\r\ncontinue;\r\ndesc = &priv->rx_desc_cpu[i];\r\ndma_unmap_single(kdev, desc->address, priv->rx_skb_size,\r\nDMA_FROM_DEVICE);\r\nkfree_skb(priv->rx_skb[i]);\r\n}\r\nkfree(priv->rx_skb);\r\nkfree(priv->tx_skb);\r\ndma_free_coherent(kdev, priv->rx_desc_alloc_size,\r\npriv->rx_desc_cpu, priv->rx_desc_dma);\r\ndma_free_coherent(kdev, priv->tx_desc_alloc_size,\r\npriv->tx_desc_cpu, priv->tx_desc_dma);\r\nfree_irq(priv->irq_tx, dev);\r\nfree_irq(priv->irq_rx, dev);\r\nfree_irq(dev->irq, dev);\r\nif (priv->has_phy) {\r\nphy_disconnect(priv->phydev);\r\npriv->phydev = NULL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void bcm_enet_get_drvinfo(struct net_device *netdev,\r\nstruct ethtool_drvinfo *drvinfo)\r\n{\r\nstrlcpy(drvinfo->driver, bcm_enet_driver_name, sizeof(drvinfo->driver));\r\nstrlcpy(drvinfo->version, bcm_enet_driver_version,\r\nsizeof(drvinfo->version));\r\nstrlcpy(drvinfo->fw_version, "N/A", sizeof(drvinfo->fw_version));\r\nstrlcpy(drvinfo->bus_info, "bcm63xx", sizeof(drvinfo->bus_info));\r\ndrvinfo->n_stats = BCM_ENET_STATS_LEN;\r\n}\r\nstatic int bcm_enet_get_sset_count(struct net_device *netdev,\r\nint string_set)\r\n{\r\nswitch (string_set) {\r\ncase ETH_SS_STATS:\r\nreturn BCM_ENET_STATS_LEN;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic void bcm_enet_get_strings(struct net_device *netdev,\r\nu32 stringset, u8 *data)\r\n{\r\nint i;\r\nswitch (stringset) {\r\ncase ETH_SS_STATS:\r\nfor (i = 0; i < BCM_ENET_STATS_LEN; i++) {\r\nmemcpy(data + i * ETH_GSTRING_LEN,\r\nbcm_enet_gstrings_stats[i].stat_string,\r\nETH_GSTRING_LEN);\r\n}\r\nbreak;\r\n}\r\n}\r\nstatic void update_mib_counters(struct bcm_enet_priv *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < BCM_ENET_STATS_LEN; i++) {\r\nconst struct bcm_enet_stats *s;\r\nu32 val;\r\nchar *p;\r\ns = &bcm_enet_gstrings_stats[i];\r\nif (s->mib_reg == -1)\r\ncontinue;\r\nval = enet_readl(priv, ENET_MIB_REG(s->mib_reg));\r\np = (char *)priv + s->stat_offset;\r\nif (s->sizeof_stat == sizeof(u64))\r\n*(u64 *)p += val;\r\nelse\r\n*(u32 *)p += val;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(unused_mib_regs); i++)\r\n(void)enet_readl(priv, ENET_MIB_REG(unused_mib_regs[i]));\r\n}\r\nstatic void bcm_enet_update_mib_counters_defer(struct work_struct *t)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = container_of(t, struct bcm_enet_priv, mib_update_task);\r\nmutex_lock(&priv->mib_update_lock);\r\nupdate_mib_counters(priv);\r\nmutex_unlock(&priv->mib_update_lock);\r\nif (netif_running(priv->net_dev))\r\nenet_writel(priv, ENET_IR_MIB, ENET_IRMASK_REG);\r\n}\r\nstatic void bcm_enet_get_ethtool_stats(struct net_device *netdev,\r\nstruct ethtool_stats *stats,\r\nu64 *data)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nint i;\r\npriv = netdev_priv(netdev);\r\nmutex_lock(&priv->mib_update_lock);\r\nupdate_mib_counters(priv);\r\nfor (i = 0; i < BCM_ENET_STATS_LEN; i++) {\r\nconst struct bcm_enet_stats *s;\r\nchar *p;\r\ns = &bcm_enet_gstrings_stats[i];\r\nif (s->mib_reg == -1)\r\np = (char *)&netdev->stats;\r\nelse\r\np = (char *)priv;\r\np += s->stat_offset;\r\ndata[i] = (s->sizeof_stat == sizeof(u64)) ?\r\n*(u64 *)p : *(u32 *)p;\r\n}\r\nmutex_unlock(&priv->mib_update_lock);\r\n}\r\nstatic int bcm_enet_nway_reset(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nif (priv->has_phy) {\r\nif (!priv->phydev)\r\nreturn -ENODEV;\r\nreturn genphy_restart_aneg(priv->phydev);\r\n}\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int bcm_enet_get_settings(struct net_device *dev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\ncmd->maxrxpkt = 0;\r\ncmd->maxtxpkt = 0;\r\nif (priv->has_phy) {\r\nif (!priv->phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_gset(priv->phydev, cmd);\r\n} else {\r\ncmd->autoneg = 0;\r\nethtool_cmd_speed_set(cmd, ((priv->force_speed_100)\r\n? SPEED_100 : SPEED_10));\r\ncmd->duplex = (priv->force_duplex_full) ?\r\nDUPLEX_FULL : DUPLEX_HALF;\r\ncmd->supported = ADVERTISED_10baseT_Half |\r\nADVERTISED_10baseT_Full |\r\nADVERTISED_100baseT_Half |\r\nADVERTISED_100baseT_Full;\r\ncmd->advertising = 0;\r\ncmd->port = PORT_MII;\r\ncmd->transceiver = XCVR_EXTERNAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int bcm_enet_set_settings(struct net_device *dev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nif (priv->has_phy) {\r\nif (!priv->phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_sset(priv->phydev, cmd);\r\n} else {\r\nif (cmd->autoneg ||\r\n(cmd->speed != SPEED_100 && cmd->speed != SPEED_10) ||\r\ncmd->port != PORT_MII)\r\nreturn -EINVAL;\r\npriv->force_speed_100 = (cmd->speed == SPEED_100) ? 1 : 0;\r\npriv->force_duplex_full = (cmd->duplex == DUPLEX_FULL) ? 1 : 0;\r\nif (netif_running(dev))\r\nbcm_enet_adjust_link(dev);\r\nreturn 0;\r\n}\r\n}\r\nstatic void bcm_enet_get_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ering)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nering->rx_max_pending = 8192;\r\nering->tx_max_pending = 8192;\r\nering->rx_pending = priv->rx_ring_size;\r\nering->tx_pending = priv->tx_ring_size;\r\n}\r\nstatic int bcm_enet_set_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ering)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nint was_running;\r\npriv = netdev_priv(dev);\r\nwas_running = 0;\r\nif (netif_running(dev)) {\r\nbcm_enet_stop(dev);\r\nwas_running = 1;\r\n}\r\npriv->rx_ring_size = ering->rx_pending;\r\npriv->tx_ring_size = ering->tx_pending;\r\nif (was_running) {\r\nint err;\r\nerr = bcm_enet_open(dev);\r\nif (err)\r\ndev_close(dev);\r\nelse\r\nbcm_enet_set_multicast_list(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic void bcm_enet_get_pauseparam(struct net_device *dev,\r\nstruct ethtool_pauseparam *ecmd)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\necmd->autoneg = priv->pause_auto;\r\necmd->rx_pause = priv->pause_rx;\r\necmd->tx_pause = priv->pause_tx;\r\n}\r\nstatic int bcm_enet_set_pauseparam(struct net_device *dev,\r\nstruct ethtool_pauseparam *ecmd)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nif (priv->has_phy) {\r\nif (ecmd->autoneg && (ecmd->rx_pause != ecmd->tx_pause)) {\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nif (ecmd->autoneg)\r\nreturn -EINVAL;\r\n}\r\npriv->pause_auto = ecmd->autoneg;\r\npriv->pause_rx = ecmd->rx_pause;\r\npriv->pause_tx = ecmd->tx_pause;\r\nreturn 0;\r\n}\r\nstatic int bcm_enet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nif (priv->has_phy) {\r\nif (!priv->phydev)\r\nreturn -ENODEV;\r\nreturn phy_mii_ioctl(priv->phydev, rq, cmd);\r\n} else {\r\nstruct mii_if_info mii;\r\nmii.dev = dev;\r\nmii.mdio_read = bcm_enet_mdio_read_mii;\r\nmii.mdio_write = bcm_enet_mdio_write_mii;\r\nmii.phy_id = 0;\r\nmii.phy_id_mask = 0x3f;\r\nmii.reg_num_mask = 0x1f;\r\nreturn generic_mii_ioctl(&mii, if_mii(rq), cmd, NULL);\r\n}\r\n}\r\nstatic int compute_hw_mtu(struct bcm_enet_priv *priv, int mtu)\r\n{\r\nint actual_mtu;\r\nactual_mtu = mtu;\r\nactual_mtu += VLAN_ETH_HLEN;\r\nif (actual_mtu < 64 || actual_mtu > BCMENET_MAX_MTU)\r\nreturn -EINVAL;\r\npriv->hw_mtu = actual_mtu;\r\npriv->rx_skb_size = ALIGN(actual_mtu + ETH_FCS_LEN,\r\npriv->dma_maxburst * 4);\r\nreturn 0;\r\n}\r\nstatic int bcm_enet_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nint ret;\r\nif (netif_running(dev))\r\nreturn -EBUSY;\r\nret = compute_hw_mtu(netdev_priv(dev), new_mtu);\r\nif (ret)\r\nreturn ret;\r\ndev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic void bcm_enet_hw_preinit(struct bcm_enet_priv *priv)\r\n{\r\nu32 val;\r\nint limit;\r\nbcm_enet_disable_mac(priv);\r\nval = ENET_CTL_SRESET_MASK;\r\nenet_writel(priv, val, ENET_CTL_REG);\r\nwmb();\r\nlimit = 1000;\r\ndo {\r\nval = enet_readl(priv, ENET_CTL_REG);\r\nif (!(val & ENET_CTL_SRESET_MASK))\r\nbreak;\r\nudelay(1);\r\n} while (limit--);\r\nval = enet_readl(priv, ENET_CTL_REG);\r\nif (priv->use_external_mii)\r\nval |= ENET_CTL_EPHYSEL_MASK;\r\nelse\r\nval &= ~ENET_CTL_EPHYSEL_MASK;\r\nenet_writel(priv, val, ENET_CTL_REG);\r\nenet_writel(priv, (0x1f << ENET_MIISC_MDCFREQDIV_SHIFT) |\r\nENET_MIISC_PREAMBLEEN_MASK, ENET_MIISC_REG);\r\nval = enet_readl(priv, ENET_MIBCTL_REG);\r\nval |= ENET_MIBCTL_RDCLEAR_MASK;\r\nenet_writel(priv, val, ENET_MIBCTL_REG);\r\n}\r\nstatic int bcm_enet_probe(struct platform_device *pdev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct net_device *dev;\r\nstruct bcm63xx_enet_platform_data *pd;\r\nstruct resource *res_mem, *res_irq, *res_irq_rx, *res_irq_tx;\r\nstruct mii_bus *bus;\r\nconst char *clk_name;\r\nint i, ret;\r\nif (!bcm_enet_shared_base[0])\r\nreturn -ENODEV;\r\nres_irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nres_irq_rx = platform_get_resource(pdev, IORESOURCE_IRQ, 1);\r\nres_irq_tx = platform_get_resource(pdev, IORESOURCE_IRQ, 2);\r\nif (!res_irq || !res_irq_rx || !res_irq_tx)\r\nreturn -ENODEV;\r\nret = 0;\r\ndev = alloc_etherdev(sizeof(*priv));\r\nif (!dev)\r\nreturn -ENOMEM;\r\npriv = netdev_priv(dev);\r\npriv->enet_is_sw = false;\r\npriv->dma_maxburst = BCMENET_DMA_MAXBURST;\r\nret = compute_hw_mtu(priv, dev->mtu);\r\nif (ret)\r\ngoto out;\r\nres_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\npriv->base = devm_ioremap_resource(&pdev->dev, res_mem);\r\nif (IS_ERR(priv->base)) {\r\nret = PTR_ERR(priv->base);\r\ngoto out;\r\n}\r\ndev->irq = priv->irq = res_irq->start;\r\npriv->irq_rx = res_irq_rx->start;\r\npriv->irq_tx = res_irq_tx->start;\r\npriv->mac_id = pdev->id;\r\nif (priv->mac_id == 0) {\r\npriv->rx_chan = 0;\r\npriv->tx_chan = 1;\r\nclk_name = "enet0";\r\n} else {\r\npriv->rx_chan = 2;\r\npriv->tx_chan = 3;\r\nclk_name = "enet1";\r\n}\r\npriv->mac_clk = clk_get(&pdev->dev, clk_name);\r\nif (IS_ERR(priv->mac_clk)) {\r\nret = PTR_ERR(priv->mac_clk);\r\ngoto out;\r\n}\r\nclk_prepare_enable(priv->mac_clk);\r\npriv->rx_ring_size = BCMENET_DEF_RX_DESC;\r\npriv->tx_ring_size = BCMENET_DEF_TX_DESC;\r\npd = dev_get_platdata(&pdev->dev);\r\nif (pd) {\r\nmemcpy(dev->dev_addr, pd->mac_addr, ETH_ALEN);\r\npriv->has_phy = pd->has_phy;\r\npriv->phy_id = pd->phy_id;\r\npriv->has_phy_interrupt = pd->has_phy_interrupt;\r\npriv->phy_interrupt = pd->phy_interrupt;\r\npriv->use_external_mii = !pd->use_internal_phy;\r\npriv->pause_auto = pd->pause_auto;\r\npriv->pause_rx = pd->pause_rx;\r\npriv->pause_tx = pd->pause_tx;\r\npriv->force_duplex_full = pd->force_duplex_full;\r\npriv->force_speed_100 = pd->force_speed_100;\r\npriv->dma_chan_en_mask = pd->dma_chan_en_mask;\r\npriv->dma_chan_int_mask = pd->dma_chan_int_mask;\r\npriv->dma_chan_width = pd->dma_chan_width;\r\npriv->dma_has_sram = pd->dma_has_sram;\r\npriv->dma_desc_shift = pd->dma_desc_shift;\r\n}\r\nif (priv->mac_id == 0 && priv->has_phy && !priv->use_external_mii) {\r\npriv->phy_clk = clk_get(&pdev->dev, "ephy");\r\nif (IS_ERR(priv->phy_clk)) {\r\nret = PTR_ERR(priv->phy_clk);\r\npriv->phy_clk = NULL;\r\ngoto out_put_clk_mac;\r\n}\r\nclk_prepare_enable(priv->phy_clk);\r\n}\r\nbcm_enet_hw_preinit(priv);\r\nif (priv->has_phy) {\r\npriv->mii_bus = mdiobus_alloc();\r\nif (!priv->mii_bus) {\r\nret = -ENOMEM;\r\ngoto out_uninit_hw;\r\n}\r\nbus = priv->mii_bus;\r\nbus->name = "bcm63xx_enet MII bus";\r\nbus->parent = &pdev->dev;\r\nbus->priv = priv;\r\nbus->read = bcm_enet_mdio_read_phylib;\r\nbus->write = bcm_enet_mdio_write_phylib;\r\nsprintf(bus->id, "%s-%d", pdev->name, priv->mac_id);\r\nbus->phy_mask = ~(1 << priv->phy_id);\r\nbus->irq = devm_kzalloc(&pdev->dev, sizeof(int) * PHY_MAX_ADDR,\r\nGFP_KERNEL);\r\nif (!bus->irq) {\r\nret = -ENOMEM;\r\ngoto out_free_mdio;\r\n}\r\nif (priv->has_phy_interrupt)\r\nbus->irq[priv->phy_id] = priv->phy_interrupt;\r\nelse\r\nbus->irq[priv->phy_id] = PHY_POLL;\r\nret = mdiobus_register(bus);\r\nif (ret) {\r\ndev_err(&pdev->dev, "unable to register mdio bus\n");\r\ngoto out_free_mdio;\r\n}\r\n} else {\r\nif (pd->mii_config &&\r\npd->mii_config(dev, 1, bcm_enet_mdio_read_mii,\r\nbcm_enet_mdio_write_mii)) {\r\ndev_err(&pdev->dev, "unable to configure mdio bus\n");\r\ngoto out_uninit_hw;\r\n}\r\n}\r\nspin_lock_init(&priv->rx_lock);\r\ninit_timer(&priv->rx_timeout);\r\npriv->rx_timeout.function = bcm_enet_refill_rx_timer;\r\npriv->rx_timeout.data = (unsigned long)dev;\r\nmutex_init(&priv->mib_update_lock);\r\nINIT_WORK(&priv->mib_update_task, bcm_enet_update_mib_counters_defer);\r\nfor (i = 0; i < ENET_MIB_REG_COUNT; i++)\r\nenet_writel(priv, 0, ENET_MIB_REG(i));\r\ndev->netdev_ops = &bcm_enet_ops;\r\nnetif_napi_add(dev, &priv->napi, bcm_enet_poll, 16);\r\ndev->ethtool_ops = &bcm_enet_ethtool_ops;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nret = register_netdev(dev);\r\nif (ret)\r\ngoto out_unregister_mdio;\r\nnetif_carrier_off(dev);\r\nplatform_set_drvdata(pdev, dev);\r\npriv->pdev = pdev;\r\npriv->net_dev = dev;\r\nreturn 0;\r\nout_unregister_mdio:\r\nif (priv->mii_bus)\r\nmdiobus_unregister(priv->mii_bus);\r\nout_free_mdio:\r\nif (priv->mii_bus)\r\nmdiobus_free(priv->mii_bus);\r\nout_uninit_hw:\r\nenet_writel(priv, 0, ENET_MIISC_REG);\r\nif (priv->phy_clk) {\r\nclk_disable_unprepare(priv->phy_clk);\r\nclk_put(priv->phy_clk);\r\n}\r\nout_put_clk_mac:\r\nclk_disable_unprepare(priv->mac_clk);\r\nclk_put(priv->mac_clk);\r\nout:\r\nfree_netdev(dev);\r\nreturn ret;\r\n}\r\nstatic int bcm_enet_remove(struct platform_device *pdev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct net_device *dev;\r\ndev = platform_get_drvdata(pdev);\r\npriv = netdev_priv(dev);\r\nunregister_netdev(dev);\r\nenet_writel(priv, 0, ENET_MIISC_REG);\r\nif (priv->has_phy) {\r\nmdiobus_unregister(priv->mii_bus);\r\nmdiobus_free(priv->mii_bus);\r\n} else {\r\nstruct bcm63xx_enet_platform_data *pd;\r\npd = dev_get_platdata(&pdev->dev);\r\nif (pd && pd->mii_config)\r\npd->mii_config(dev, 0, bcm_enet_mdio_read_mii,\r\nbcm_enet_mdio_write_mii);\r\n}\r\nif (priv->phy_clk) {\r\nclk_disable_unprepare(priv->phy_clk);\r\nclk_put(priv->phy_clk);\r\n}\r\nclk_disable_unprepare(priv->mac_clk);\r\nclk_put(priv->mac_clk);\r\nfree_netdev(dev);\r\nreturn 0;\r\n}\r\nstatic int bcmenet_sw_mdio_read(struct bcm_enet_priv *priv,\r\nint ext, int phy_id, int location)\r\n{\r\nu32 reg;\r\nint ret;\r\nspin_lock_bh(&priv->enetsw_mdio_lock);\r\nenetsw_writel(priv, 0, ENETSW_MDIOC_REG);\r\nreg = ENETSW_MDIOC_RD_MASK |\r\n(phy_id << ENETSW_MDIOC_PHYID_SHIFT) |\r\n(location << ENETSW_MDIOC_REG_SHIFT);\r\nif (ext)\r\nreg |= ENETSW_MDIOC_EXT_MASK;\r\nenetsw_writel(priv, reg, ENETSW_MDIOC_REG);\r\nudelay(50);\r\nret = enetsw_readw(priv, ENETSW_MDIOD_REG);\r\nspin_unlock_bh(&priv->enetsw_mdio_lock);\r\nreturn ret;\r\n}\r\nstatic void bcmenet_sw_mdio_write(struct bcm_enet_priv *priv,\r\nint ext, int phy_id, int location,\r\nuint16_t data)\r\n{\r\nu32 reg;\r\nspin_lock_bh(&priv->enetsw_mdio_lock);\r\nenetsw_writel(priv, 0, ENETSW_MDIOC_REG);\r\nreg = ENETSW_MDIOC_WR_MASK |\r\n(phy_id << ENETSW_MDIOC_PHYID_SHIFT) |\r\n(location << ENETSW_MDIOC_REG_SHIFT);\r\nif (ext)\r\nreg |= ENETSW_MDIOC_EXT_MASK;\r\nreg |= data;\r\nenetsw_writel(priv, reg, ENETSW_MDIOC_REG);\r\nudelay(50);\r\nspin_unlock_bh(&priv->enetsw_mdio_lock);\r\n}\r\nstatic inline int bcm_enet_port_is_rgmii(int portid)\r\n{\r\nreturn portid >= ENETSW_RGMII_PORT0;\r\n}\r\nstatic void swphy_poll_timer(unsigned long data)\r\n{\r\nstruct bcm_enet_priv *priv = (struct bcm_enet_priv *)data;\r\nunsigned int i;\r\nfor (i = 0; i < priv->num_ports; i++) {\r\nstruct bcm63xx_enetsw_port *port;\r\nint val, j, up, advertise, lpa, lpa2, speed, duplex, media;\r\nint external_phy = bcm_enet_port_is_rgmii(i);\r\nu8 override;\r\nport = &priv->used_ports[i];\r\nif (!port->used)\r\ncontinue;\r\nif (port->bypass_link)\r\ncontinue;\r\nfor (j = 0; j < 2; j++)\r\nval = bcmenet_sw_mdio_read(priv, external_phy,\r\nport->phy_id, MII_BMSR);\r\nif (val == 0xffff)\r\ncontinue;\r\nup = (val & BMSR_LSTATUS) ? 1 : 0;\r\nif (!(up ^ priv->sw_port_link[i]))\r\ncontinue;\r\npriv->sw_port_link[i] = up;\r\nif (!up) {\r\ndev_info(&priv->pdev->dev, "link DOWN on %s\n",\r\nport->name);\r\nenetsw_writeb(priv, ENETSW_PORTOV_ENABLE_MASK,\r\nENETSW_PORTOV_REG(i));\r\nenetsw_writeb(priv, ENETSW_PTCTRL_RXDIS_MASK |\r\nENETSW_PTCTRL_TXDIS_MASK,\r\nENETSW_PTCTRL_REG(i));\r\ncontinue;\r\n}\r\nadvertise = bcmenet_sw_mdio_read(priv, external_phy,\r\nport->phy_id, MII_ADVERTISE);\r\nlpa = bcmenet_sw_mdio_read(priv, external_phy, port->phy_id,\r\nMII_LPA);\r\nlpa2 = bcmenet_sw_mdio_read(priv, external_phy, port->phy_id,\r\nMII_STAT1000);\r\nmedia = mii_nway_result(lpa & advertise);\r\nduplex = (media & ADVERTISE_FULL) ? 1 : 0;\r\nif (lpa2 & LPA_1000FULL)\r\nduplex = 1;\r\nif (lpa2 & (LPA_1000FULL | LPA_1000HALF))\r\nspeed = 1000;\r\nelse {\r\nif (media & (ADVERTISE_100FULL | ADVERTISE_100HALF))\r\nspeed = 100;\r\nelse\r\nspeed = 10;\r\n}\r\ndev_info(&priv->pdev->dev,\r\n"link UP on %s, %dMbps, %s-duplex\n",\r\nport->name, speed, duplex ? "full" : "half");\r\noverride = ENETSW_PORTOV_ENABLE_MASK |\r\nENETSW_PORTOV_LINKUP_MASK;\r\nif (speed == 1000)\r\noverride |= ENETSW_IMPOV_1000_MASK;\r\nelse if (speed == 100)\r\noverride |= ENETSW_IMPOV_100_MASK;\r\nif (duplex)\r\noverride |= ENETSW_IMPOV_FDX_MASK;\r\nenetsw_writeb(priv, override, ENETSW_PORTOV_REG(i));\r\nenetsw_writeb(priv, 0, ENETSW_PTCTRL_REG(i));\r\n}\r\npriv->swphy_poll.expires = jiffies + HZ;\r\nadd_timer(&priv->swphy_poll);\r\n}\r\nstatic int bcm_enetsw_open(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct device *kdev;\r\nint i, ret;\r\nunsigned int size;\r\nvoid *p;\r\nu32 val;\r\npriv = netdev_priv(dev);\r\nkdev = &priv->pdev->dev;\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);\r\nret = request_irq(priv->irq_rx, bcm_enet_isr_dma,\r\n0, dev->name, dev);\r\nif (ret)\r\ngoto out_freeirq;\r\nif (priv->irq_tx != -1) {\r\nret = request_irq(priv->irq_tx, bcm_enet_isr_dma,\r\n0, dev->name, dev);\r\nif (ret)\r\ngoto out_freeirq_rx;\r\n}\r\nsize = priv->rx_ring_size * sizeof(struct bcm_enet_desc);\r\np = dma_alloc_coherent(kdev, size, &priv->rx_desc_dma, GFP_KERNEL);\r\nif (!p) {\r\ndev_err(kdev, "cannot allocate rx ring %u\n", size);\r\nret = -ENOMEM;\r\ngoto out_freeirq_tx;\r\n}\r\nmemset(p, 0, size);\r\npriv->rx_desc_alloc_size = size;\r\npriv->rx_desc_cpu = p;\r\nsize = priv->tx_ring_size * sizeof(struct bcm_enet_desc);\r\np = dma_alloc_coherent(kdev, size, &priv->tx_desc_dma, GFP_KERNEL);\r\nif (!p) {\r\ndev_err(kdev, "cannot allocate tx ring\n");\r\nret = -ENOMEM;\r\ngoto out_free_rx_ring;\r\n}\r\nmemset(p, 0, size);\r\npriv->tx_desc_alloc_size = size;\r\npriv->tx_desc_cpu = p;\r\npriv->tx_skb = kzalloc(sizeof(struct sk_buff *) * priv->tx_ring_size,\r\nGFP_KERNEL);\r\nif (!priv->tx_skb) {\r\ndev_err(kdev, "cannot allocate rx skb queue\n");\r\nret = -ENOMEM;\r\ngoto out_free_tx_ring;\r\n}\r\npriv->tx_desc_count = priv->tx_ring_size;\r\npriv->tx_dirty_desc = 0;\r\npriv->tx_curr_desc = 0;\r\nspin_lock_init(&priv->tx_lock);\r\npriv->rx_skb = kzalloc(sizeof(struct sk_buff *) * priv->rx_ring_size,\r\nGFP_KERNEL);\r\nif (!priv->rx_skb) {\r\ndev_err(kdev, "cannot allocate rx skb queue\n");\r\nret = -ENOMEM;\r\ngoto out_free_tx_skb;\r\n}\r\npriv->rx_desc_count = 0;\r\npriv->rx_dirty_desc = 0;\r\npriv->rx_curr_desc = 0;\r\nfor (i = 0; i < priv->num_ports; i++) {\r\nenetsw_writeb(priv, ENETSW_PORTOV_ENABLE_MASK,\r\nENETSW_PORTOV_REG(i));\r\nenetsw_writeb(priv, ENETSW_PTCTRL_RXDIS_MASK |\r\nENETSW_PTCTRL_TXDIS_MASK,\r\nENETSW_PTCTRL_REG(i));\r\npriv->sw_port_link[i] = 0;\r\n}\r\nval = enetsw_readb(priv, ENETSW_GMCR_REG);\r\nval |= ENETSW_GMCR_RST_MIB_MASK;\r\nenetsw_writeb(priv, val, ENETSW_GMCR_REG);\r\nmdelay(1);\r\nval &= ~ENETSW_GMCR_RST_MIB_MASK;\r\nenetsw_writeb(priv, val, ENETSW_GMCR_REG);\r\nmdelay(1);\r\nval = enetsw_readb(priv, ENETSW_IMPOV_REG);\r\nval |= ENETSW_IMPOV_FORCE_MASK | ENETSW_IMPOV_LINKUP_MASK;\r\nenetsw_writeb(priv, val, ENETSW_IMPOV_REG);\r\nval = enetsw_readb(priv, ENETSW_SWMODE_REG);\r\nval |= ENETSW_SWMODE_FWD_EN_MASK;\r\nenetsw_writeb(priv, val, ENETSW_SWMODE_REG);\r\nenetsw_writel(priv, 0x1ff, ENETSW_JMBCTL_PORT_REG);\r\nenetsw_writew(priv, 9728, ENETSW_JMBCTL_MAXSIZE_REG);\r\nenet_dma_writel(priv, ENETDMA_BUFALLOC_FORCE_MASK | 0,\r\nENETDMA_BUFALLOC_REG(priv->rx_chan));\r\nif (bcm_enet_refill_rx(dev)) {\r\ndev_err(kdev, "cannot allocate rx skb queue\n");\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nenet_dmas_writel(priv, priv->rx_desc_dma,\r\nENETDMAS_RSTART_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, priv->tx_desc_dma,\r\nENETDMAS_RSTART_REG, priv->tx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->tx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->tx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->rx_chan);\r\nenet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->tx_chan);\r\nenet_dmac_writel(priv, priv->dma_maxburst,\r\nENETDMAC_MAXBURST, priv->rx_chan);\r\nenet_dmac_writel(priv, priv->dma_maxburst,\r\nENETDMAC_MAXBURST, priv->tx_chan);\r\nval = priv->rx_ring_size / 3;\r\nenet_dma_writel(priv, val, ENETDMA_FLOWCL_REG(priv->rx_chan));\r\nval = (priv->rx_ring_size * 2) / 3;\r\nenet_dma_writel(priv, val, ENETDMA_FLOWCH_REG(priv->rx_chan));\r\nwmb();\r\nenet_dma_writel(priv, ENETDMA_CFG_EN_MASK, ENETDMA_CFG_REG);\r\nenet_dmac_writel(priv, ENETDMAC_CHANCFG_EN_MASK,\r\nENETDMAC_CHANCFG, priv->rx_chan);\r\nenet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,\r\nENETDMAC_IR, priv->rx_chan);\r\nenet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,\r\nENETDMAC_IR, priv->tx_chan);\r\nnapi_enable(&priv->napi);\r\nenet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,\r\nENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,\r\nENETDMAC_IRMASK, priv->tx_chan);\r\nnetif_carrier_on(dev);\r\nnetif_start_queue(dev);\r\nfor (i = 0; i < priv->num_ports; i++) {\r\nstruct bcm63xx_enetsw_port *port;\r\nu8 override;\r\nport = &priv->used_ports[i];\r\nif (!port->used)\r\ncontinue;\r\nif (!port->bypass_link)\r\ncontinue;\r\noverride = ENETSW_PORTOV_ENABLE_MASK |\r\nENETSW_PORTOV_LINKUP_MASK;\r\nswitch (port->force_speed) {\r\ncase 1000:\r\noverride |= ENETSW_IMPOV_1000_MASK;\r\nbreak;\r\ncase 100:\r\noverride |= ENETSW_IMPOV_100_MASK;\r\nbreak;\r\ncase 10:\r\nbreak;\r\ndefault:\r\npr_warn("invalid forced speed on port %s: assume 10\n",\r\nport->name);\r\nbreak;\r\n}\r\nif (port->force_duplex_full)\r\noverride |= ENETSW_IMPOV_FDX_MASK;\r\nenetsw_writeb(priv, override, ENETSW_PORTOV_REG(i));\r\nenetsw_writeb(priv, 0, ENETSW_PTCTRL_REG(i));\r\n}\r\ninit_timer(&priv->swphy_poll);\r\npriv->swphy_poll.function = swphy_poll_timer;\r\npriv->swphy_poll.data = (unsigned long)priv;\r\npriv->swphy_poll.expires = jiffies;\r\nadd_timer(&priv->swphy_poll);\r\nreturn 0;\r\nout:\r\nfor (i = 0; i < priv->rx_ring_size; i++) {\r\nstruct bcm_enet_desc *desc;\r\nif (!priv->rx_skb[i])\r\ncontinue;\r\ndesc = &priv->rx_desc_cpu[i];\r\ndma_unmap_single(kdev, desc->address, priv->rx_skb_size,\r\nDMA_FROM_DEVICE);\r\nkfree_skb(priv->rx_skb[i]);\r\n}\r\nkfree(priv->rx_skb);\r\nout_free_tx_skb:\r\nkfree(priv->tx_skb);\r\nout_free_tx_ring:\r\ndma_free_coherent(kdev, priv->tx_desc_alloc_size,\r\npriv->tx_desc_cpu, priv->tx_desc_dma);\r\nout_free_rx_ring:\r\ndma_free_coherent(kdev, priv->rx_desc_alloc_size,\r\npriv->rx_desc_cpu, priv->rx_desc_dma);\r\nout_freeirq_tx:\r\nif (priv->irq_tx != -1)\r\nfree_irq(priv->irq_tx, dev);\r\nout_freeirq_rx:\r\nfree_irq(priv->irq_rx, dev);\r\nout_freeirq:\r\nreturn ret;\r\n}\r\nstatic int bcm_enetsw_stop(struct net_device *dev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct device *kdev;\r\nint i;\r\npriv = netdev_priv(dev);\r\nkdev = &priv->pdev->dev;\r\ndel_timer_sync(&priv->swphy_poll);\r\nnetif_stop_queue(dev);\r\nnapi_disable(&priv->napi);\r\ndel_timer_sync(&priv->rx_timeout);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);\r\nenet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);\r\nbcm_enet_disable_dma(priv, priv->tx_chan);\r\nbcm_enet_disable_dma(priv, priv->rx_chan);\r\nbcm_enet_tx_reclaim(dev, 1);\r\nfor (i = 0; i < priv->rx_ring_size; i++) {\r\nstruct bcm_enet_desc *desc;\r\nif (!priv->rx_skb[i])\r\ncontinue;\r\ndesc = &priv->rx_desc_cpu[i];\r\ndma_unmap_single(kdev, desc->address, priv->rx_skb_size,\r\nDMA_FROM_DEVICE);\r\nkfree_skb(priv->rx_skb[i]);\r\n}\r\nkfree(priv->rx_skb);\r\nkfree(priv->tx_skb);\r\ndma_free_coherent(kdev, priv->rx_desc_alloc_size,\r\npriv->rx_desc_cpu, priv->rx_desc_dma);\r\ndma_free_coherent(kdev, priv->tx_desc_alloc_size,\r\npriv->tx_desc_cpu, priv->tx_desc_dma);\r\nif (priv->irq_tx != -1)\r\nfree_irq(priv->irq_tx, dev);\r\nfree_irq(priv->irq_rx, dev);\r\nreturn 0;\r\n}\r\nstatic int bcm_enetsw_phy_is_external(struct bcm_enet_priv *priv, int phy_id)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_ports; ++i) {\r\nif (!priv->used_ports[i].used)\r\ncontinue;\r\nif (priv->used_ports[i].phy_id == phy_id)\r\nreturn bcm_enet_port_is_rgmii(i);\r\n}\r\nprintk_once(KERN_WARNING "bcm63xx_enet: could not find a used port with phy_id %i, assuming phy is external\n",\r\nphy_id);\r\nreturn 1;\r\n}\r\nstatic int bcm_enetsw_mii_mdio_read(struct net_device *dev, int phy_id,\r\nint location)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nreturn bcmenet_sw_mdio_read(priv,\r\nbcm_enetsw_phy_is_external(priv, phy_id),\r\nphy_id, location);\r\n}\r\nstatic void bcm_enetsw_mii_mdio_write(struct net_device *dev, int phy_id,\r\nint location,\r\nint val)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nbcmenet_sw_mdio_write(priv, bcm_enetsw_phy_is_external(priv, phy_id),\r\nphy_id, location, val);\r\n}\r\nstatic int bcm_enetsw_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct mii_if_info mii;\r\nmii.dev = dev;\r\nmii.mdio_read = bcm_enetsw_mii_mdio_read;\r\nmii.mdio_write = bcm_enetsw_mii_mdio_write;\r\nmii.phy_id = 0;\r\nmii.phy_id_mask = 0x3f;\r\nmii.reg_num_mask = 0x1f;\r\nreturn generic_mii_ioctl(&mii, if_mii(rq), cmd, NULL);\r\n}\r\nstatic void bcm_enetsw_get_strings(struct net_device *netdev,\r\nu32 stringset, u8 *data)\r\n{\r\nint i;\r\nswitch (stringset) {\r\ncase ETH_SS_STATS:\r\nfor (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {\r\nmemcpy(data + i * ETH_GSTRING_LEN,\r\nbcm_enetsw_gstrings_stats[i].stat_string,\r\nETH_GSTRING_LEN);\r\n}\r\nbreak;\r\n}\r\n}\r\nstatic int bcm_enetsw_get_sset_count(struct net_device *netdev,\r\nint string_set)\r\n{\r\nswitch (string_set) {\r\ncase ETH_SS_STATS:\r\nreturn BCM_ENETSW_STATS_LEN;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic void bcm_enetsw_get_drvinfo(struct net_device *netdev,\r\nstruct ethtool_drvinfo *drvinfo)\r\n{\r\nstrncpy(drvinfo->driver, bcm_enet_driver_name, 32);\r\nstrncpy(drvinfo->version, bcm_enet_driver_version, 32);\r\nstrncpy(drvinfo->fw_version, "N/A", 32);\r\nstrncpy(drvinfo->bus_info, "bcm63xx", 32);\r\ndrvinfo->n_stats = BCM_ENETSW_STATS_LEN;\r\n}\r\nstatic void bcm_enetsw_get_ethtool_stats(struct net_device *netdev,\r\nstruct ethtool_stats *stats,\r\nu64 *data)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nint i;\r\npriv = netdev_priv(netdev);\r\nfor (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {\r\nconst struct bcm_enet_stats *s;\r\nu32 lo, hi;\r\nchar *p;\r\nint reg;\r\ns = &bcm_enetsw_gstrings_stats[i];\r\nreg = s->mib_reg;\r\nif (reg == -1)\r\ncontinue;\r\nlo = enetsw_readl(priv, ENETSW_MIB_REG(reg));\r\np = (char *)priv + s->stat_offset;\r\nif (s->sizeof_stat == sizeof(u64)) {\r\nhi = enetsw_readl(priv, ENETSW_MIB_REG(reg + 1));\r\n*(u64 *)p = ((u64)hi << 32 | lo);\r\n} else {\r\n*(u32 *)p = lo;\r\n}\r\n}\r\nfor (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {\r\nconst struct bcm_enet_stats *s;\r\nchar *p;\r\ns = &bcm_enetsw_gstrings_stats[i];\r\nif (s->mib_reg == -1)\r\np = (char *)&netdev->stats + s->stat_offset;\r\nelse\r\np = (char *)priv + s->stat_offset;\r\ndata[i] = (s->sizeof_stat == sizeof(u64)) ?\r\n*(u64 *)p : *(u32 *)p;\r\n}\r\n}\r\nstatic void bcm_enetsw_get_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ering)\r\n{\r\nstruct bcm_enet_priv *priv;\r\npriv = netdev_priv(dev);\r\nering->rx_max_pending = 8192;\r\nering->tx_max_pending = 8192;\r\nering->rx_mini_max_pending = 0;\r\nering->rx_jumbo_max_pending = 0;\r\nering->rx_pending = priv->rx_ring_size;\r\nering->tx_pending = priv->tx_ring_size;\r\n}\r\nstatic int bcm_enetsw_set_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ering)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nint was_running;\r\npriv = netdev_priv(dev);\r\nwas_running = 0;\r\nif (netif_running(dev)) {\r\nbcm_enetsw_stop(dev);\r\nwas_running = 1;\r\n}\r\npriv->rx_ring_size = ering->rx_pending;\r\npriv->tx_ring_size = ering->tx_pending;\r\nif (was_running) {\r\nint err;\r\nerr = bcm_enetsw_open(dev);\r\nif (err)\r\ndev_close(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int bcm_enetsw_probe(struct platform_device *pdev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct net_device *dev;\r\nstruct bcm63xx_enetsw_platform_data *pd;\r\nstruct resource *res_mem;\r\nint ret, irq_rx, irq_tx;\r\nif (!bcm_enet_shared_base[0])\r\nreturn -ENODEV;\r\nres_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nirq_rx = platform_get_irq(pdev, 0);\r\nirq_tx = platform_get_irq(pdev, 1);\r\nif (!res_mem || irq_rx < 0)\r\nreturn -ENODEV;\r\nret = 0;\r\ndev = alloc_etherdev(sizeof(*priv));\r\nif (!dev)\r\nreturn -ENOMEM;\r\npriv = netdev_priv(dev);\r\nmemset(priv, 0, sizeof(*priv));\r\npriv->enet_is_sw = true;\r\npriv->irq_rx = irq_rx;\r\npriv->irq_tx = irq_tx;\r\npriv->rx_ring_size = BCMENET_DEF_RX_DESC;\r\npriv->tx_ring_size = BCMENET_DEF_TX_DESC;\r\npriv->dma_maxburst = BCMENETSW_DMA_MAXBURST;\r\npd = dev_get_platdata(&pdev->dev);\r\nif (pd) {\r\nmemcpy(dev->dev_addr, pd->mac_addr, ETH_ALEN);\r\nmemcpy(priv->used_ports, pd->used_ports,\r\nsizeof(pd->used_ports));\r\npriv->num_ports = pd->num_ports;\r\npriv->dma_has_sram = pd->dma_has_sram;\r\npriv->dma_chan_en_mask = pd->dma_chan_en_mask;\r\npriv->dma_chan_int_mask = pd->dma_chan_int_mask;\r\npriv->dma_chan_width = pd->dma_chan_width;\r\n}\r\nret = compute_hw_mtu(priv, dev->mtu);\r\nif (ret)\r\ngoto out;\r\nif (!request_mem_region(res_mem->start, resource_size(res_mem),\r\n"bcm63xx_enetsw")) {\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\npriv->base = ioremap(res_mem->start, resource_size(res_mem));\r\nif (priv->base == NULL) {\r\nret = -ENOMEM;\r\ngoto out_release_mem;\r\n}\r\npriv->mac_clk = clk_get(&pdev->dev, "enetsw");\r\nif (IS_ERR(priv->mac_clk)) {\r\nret = PTR_ERR(priv->mac_clk);\r\ngoto out_unmap;\r\n}\r\nclk_enable(priv->mac_clk);\r\npriv->rx_chan = 0;\r\npriv->tx_chan = 1;\r\nspin_lock_init(&priv->rx_lock);\r\ninit_timer(&priv->rx_timeout);\r\npriv->rx_timeout.function = bcm_enet_refill_rx_timer;\r\npriv->rx_timeout.data = (unsigned long)dev;\r\ndev->netdev_ops = &bcm_enetsw_ops;\r\nnetif_napi_add(dev, &priv->napi, bcm_enet_poll, 16);\r\ndev->ethtool_ops = &bcm_enetsw_ethtool_ops;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nspin_lock_init(&priv->enetsw_mdio_lock);\r\nret = register_netdev(dev);\r\nif (ret)\r\ngoto out_put_clk;\r\nnetif_carrier_off(dev);\r\nplatform_set_drvdata(pdev, dev);\r\npriv->pdev = pdev;\r\npriv->net_dev = dev;\r\nreturn 0;\r\nout_put_clk:\r\nclk_put(priv->mac_clk);\r\nout_unmap:\r\niounmap(priv->base);\r\nout_release_mem:\r\nrelease_mem_region(res_mem->start, resource_size(res_mem));\r\nout:\r\nfree_netdev(dev);\r\nreturn ret;\r\n}\r\nstatic int bcm_enetsw_remove(struct platform_device *pdev)\r\n{\r\nstruct bcm_enet_priv *priv;\r\nstruct net_device *dev;\r\nstruct resource *res;\r\ndev = platform_get_drvdata(pdev);\r\npriv = netdev_priv(dev);\r\nunregister_netdev(dev);\r\niounmap(priv->base);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nrelease_mem_region(res->start, resource_size(res));\r\nfree_netdev(dev);\r\nreturn 0;\r\n}\r\nstatic int bcm_enet_shared_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *res;\r\nvoid __iomem *p[3];\r\nunsigned int i;\r\nmemset(bcm_enet_shared_base, 0, sizeof(bcm_enet_shared_base));\r\nfor (i = 0; i < 3; i++) {\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, i);\r\np[i] = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(p[i]))\r\nreturn PTR_ERR(p[i]);\r\n}\r\nmemcpy(bcm_enet_shared_base, p, sizeof(bcm_enet_shared_base));\r\nreturn 0;\r\n}\r\nstatic int bcm_enet_shared_remove(struct platform_device *pdev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int __init bcm_enet_init(void)\r\n{\r\nint ret;\r\nret = platform_driver_register(&bcm63xx_enet_shared_driver);\r\nif (ret)\r\nreturn ret;\r\nret = platform_driver_register(&bcm63xx_enet_driver);\r\nif (ret)\r\nplatform_driver_unregister(&bcm63xx_enet_shared_driver);\r\nret = platform_driver_register(&bcm63xx_enetsw_driver);\r\nif (ret) {\r\nplatform_driver_unregister(&bcm63xx_enet_driver);\r\nplatform_driver_unregister(&bcm63xx_enet_shared_driver);\r\n}\r\nreturn ret;\r\n}\r\nstatic void __exit bcm_enet_exit(void)\r\n{\r\nplatform_driver_unregister(&bcm63xx_enet_driver);\r\nplatform_driver_unregister(&bcm63xx_enetsw_driver);\r\nplatform_driver_unregister(&bcm63xx_enet_shared_driver);\r\n}
