static inline int find_unassigned_irq(void)\r\n{\r\nint irq;\r\nfor (irq = IA64_FIRST_DEVICE_VECTOR; irq < NR_IRQS; irq++)\r\nif (irq_status[irq] == IRQ_UNUSED)\r\nreturn irq;\r\nreturn -ENOSPC;\r\n}\r\nstatic inline int find_unassigned_vector(cpumask_t domain)\r\n{\r\ncpumask_t mask;\r\nint pos, vector;\r\ncpumask_and(&mask, &domain, cpu_online_mask);\r\nif (cpus_empty(mask))\r\nreturn -EINVAL;\r\nfor (pos = 0; pos < IA64_NUM_DEVICE_VECTORS; pos++) {\r\nvector = IA64_FIRST_DEVICE_VECTOR + pos;\r\ncpus_and(mask, domain, vector_table[vector]);\r\nif (!cpus_empty(mask))\r\ncontinue;\r\nreturn vector;\r\n}\r\nreturn -ENOSPC;\r\n}\r\nstatic int __bind_irq_vector(int irq, int vector, cpumask_t domain)\r\n{\r\ncpumask_t mask;\r\nint cpu;\r\nstruct irq_cfg *cfg = &irq_cfg[irq];\r\nBUG_ON((unsigned)irq >= NR_IRQS);\r\nBUG_ON((unsigned)vector >= IA64_NUM_VECTORS);\r\ncpumask_and(&mask, &domain, cpu_online_mask);\r\nif (cpus_empty(mask))\r\nreturn -EINVAL;\r\nif ((cfg->vector == vector) && cpus_equal(cfg->domain, domain))\r\nreturn 0;\r\nif (cfg->vector != IRQ_VECTOR_UNASSIGNED)\r\nreturn -EBUSY;\r\nfor_each_cpu_mask(cpu, mask)\r\nper_cpu(vector_irq, cpu)[vector] = irq;\r\ncfg->vector = vector;\r\ncfg->domain = domain;\r\nirq_status[irq] = IRQ_USED;\r\ncpus_or(vector_table[vector], vector_table[vector], domain);\r\nreturn 0;\r\n}\r\nint bind_irq_vector(int irq, int vector, cpumask_t domain)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&vector_lock, flags);\r\nret = __bind_irq_vector(irq, vector, domain);\r\nspin_unlock_irqrestore(&vector_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void __clear_irq_vector(int irq)\r\n{\r\nint vector, cpu;\r\ncpumask_t mask;\r\ncpumask_t domain;\r\nstruct irq_cfg *cfg = &irq_cfg[irq];\r\nBUG_ON((unsigned)irq >= NR_IRQS);\r\nBUG_ON(cfg->vector == IRQ_VECTOR_UNASSIGNED);\r\nvector = cfg->vector;\r\ndomain = cfg->domain;\r\ncpumask_and(&mask, &cfg->domain, cpu_online_mask);\r\nfor_each_cpu_mask(cpu, mask)\r\nper_cpu(vector_irq, cpu)[vector] = -1;\r\ncfg->vector = IRQ_VECTOR_UNASSIGNED;\r\ncfg->domain = CPU_MASK_NONE;\r\nirq_status[irq] = IRQ_UNUSED;\r\ncpus_andnot(vector_table[vector], vector_table[vector], domain);\r\n}\r\nstatic void clear_irq_vector(int irq)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&vector_lock, flags);\r\n__clear_irq_vector(irq);\r\nspin_unlock_irqrestore(&vector_lock, flags);\r\n}\r\nint\r\nia64_native_assign_irq_vector (int irq)\r\n{\r\nunsigned long flags;\r\nint vector, cpu;\r\ncpumask_t domain = CPU_MASK_NONE;\r\nvector = -ENOSPC;\r\nspin_lock_irqsave(&vector_lock, flags);\r\nfor_each_online_cpu(cpu) {\r\ndomain = vector_allocation_domain(cpu);\r\nvector = find_unassigned_vector(domain);\r\nif (vector >= 0)\r\nbreak;\r\n}\r\nif (vector < 0)\r\ngoto out;\r\nif (irq == AUTO_ASSIGN)\r\nirq = vector;\r\nBUG_ON(__bind_irq_vector(irq, vector, domain));\r\nout:\r\nspin_unlock_irqrestore(&vector_lock, flags);\r\nreturn vector;\r\n}\r\nvoid\r\nia64_native_free_irq_vector (int vector)\r\n{\r\nif (vector < IA64_FIRST_DEVICE_VECTOR ||\r\nvector > IA64_LAST_DEVICE_VECTOR)\r\nreturn;\r\nclear_irq_vector(vector);\r\n}\r\nint\r\nreserve_irq_vector (int vector)\r\n{\r\nif (vector < IA64_FIRST_DEVICE_VECTOR ||\r\nvector > IA64_LAST_DEVICE_VECTOR)\r\nreturn -EINVAL;\r\nreturn !!bind_irq_vector(vector, vector, CPU_MASK_ALL);\r\n}\r\nvoid __setup_vector_irq(int cpu)\r\n{\r\nint irq, vector;\r\nfor (vector = 0; vector < IA64_NUM_VECTORS; ++vector)\r\nper_cpu(vector_irq, cpu)[vector] = -1;\r\nfor (irq = 0; irq < NR_IRQS; ++irq) {\r\nif (!cpu_isset(cpu, irq_cfg[irq].domain))\r\ncontinue;\r\nvector = irq_to_vector(irq);\r\nper_cpu(vector_irq, cpu)[vector] = irq;\r\n}\r\n}\r\nstatic cpumask_t vector_allocation_domain(int cpu)\r\n{\r\nif (vector_domain_type == VECTOR_DOMAIN_PERCPU)\r\nreturn cpumask_of_cpu(cpu);\r\nreturn CPU_MASK_ALL;\r\n}\r\nstatic int __irq_prepare_move(int irq, int cpu)\r\n{\r\nstruct irq_cfg *cfg = &irq_cfg[irq];\r\nint vector;\r\ncpumask_t domain;\r\nif (cfg->move_in_progress || cfg->move_cleanup_count)\r\nreturn -EBUSY;\r\nif (cfg->vector == IRQ_VECTOR_UNASSIGNED || !cpu_online(cpu))\r\nreturn -EINVAL;\r\nif (cpu_isset(cpu, cfg->domain))\r\nreturn 0;\r\ndomain = vector_allocation_domain(cpu);\r\nvector = find_unassigned_vector(domain);\r\nif (vector < 0)\r\nreturn -ENOSPC;\r\ncfg->move_in_progress = 1;\r\ncfg->old_domain = cfg->domain;\r\ncfg->vector = IRQ_VECTOR_UNASSIGNED;\r\ncfg->domain = CPU_MASK_NONE;\r\nBUG_ON(__bind_irq_vector(irq, vector, domain));\r\nreturn 0;\r\n}\r\nint irq_prepare_move(int irq, int cpu)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&vector_lock, flags);\r\nret = __irq_prepare_move(irq, cpu);\r\nspin_unlock_irqrestore(&vector_lock, flags);\r\nreturn ret;\r\n}\r\nvoid irq_complete_move(unsigned irq)\r\n{\r\nstruct irq_cfg *cfg = &irq_cfg[irq];\r\ncpumask_t cleanup_mask;\r\nint i;\r\nif (likely(!cfg->move_in_progress))\r\nreturn;\r\nif (unlikely(cpu_isset(smp_processor_id(), cfg->old_domain)))\r\nreturn;\r\ncpumask_and(&cleanup_mask, &cfg->old_domain, cpu_online_mask);\r\ncfg->move_cleanup_count = cpus_weight(cleanup_mask);\r\nfor_each_cpu_mask(i, cleanup_mask)\r\nplatform_send_ipi(i, IA64_IRQ_MOVE_VECTOR, IA64_IPI_DM_INT, 0);\r\ncfg->move_in_progress = 0;\r\n}\r\nstatic irqreturn_t smp_irq_move_cleanup_interrupt(int irq, void *dev_id)\r\n{\r\nint me = smp_processor_id();\r\nia64_vector vector;\r\nunsigned long flags;\r\nfor (vector = IA64_FIRST_DEVICE_VECTOR;\r\nvector < IA64_LAST_DEVICE_VECTOR; vector++) {\r\nint irq;\r\nstruct irq_desc *desc;\r\nstruct irq_cfg *cfg;\r\nirq = __get_cpu_var(vector_irq)[vector];\r\nif (irq < 0)\r\ncontinue;\r\ndesc = irq_to_desc(irq);\r\ncfg = irq_cfg + irq;\r\nraw_spin_lock(&desc->lock);\r\nif (!cfg->move_cleanup_count)\r\ngoto unlock;\r\nif (!cpu_isset(me, cfg->old_domain))\r\ngoto unlock;\r\nspin_lock_irqsave(&vector_lock, flags);\r\n__get_cpu_var(vector_irq)[vector] = -1;\r\ncpu_clear(me, vector_table[vector]);\r\nspin_unlock_irqrestore(&vector_lock, flags);\r\ncfg->move_cleanup_count--;\r\nunlock:\r\nraw_spin_unlock(&desc->lock);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int __init parse_vector_domain(char *arg)\r\n{\r\nif (!arg)\r\nreturn -EINVAL;\r\nif (!strcmp(arg, "percpu")) {\r\nvector_domain_type = VECTOR_DOMAIN_PERCPU;\r\nno_int_routing = 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic cpumask_t vector_allocation_domain(int cpu)\r\n{\r\nreturn CPU_MASK_ALL;\r\n}\r\nvoid destroy_and_reserve_irq(unsigned int irq)\r\n{\r\nunsigned long flags;\r\nirq_init_desc(irq);\r\nspin_lock_irqsave(&vector_lock, flags);\r\n__clear_irq_vector(irq);\r\nirq_status[irq] = IRQ_RSVD;\r\nspin_unlock_irqrestore(&vector_lock, flags);\r\n}\r\nint create_irq(void)\r\n{\r\nunsigned long flags;\r\nint irq, vector, cpu;\r\ncpumask_t domain = CPU_MASK_NONE;\r\nirq = vector = -ENOSPC;\r\nspin_lock_irqsave(&vector_lock, flags);\r\nfor_each_online_cpu(cpu) {\r\ndomain = vector_allocation_domain(cpu);\r\nvector = find_unassigned_vector(domain);\r\nif (vector >= 0)\r\nbreak;\r\n}\r\nif (vector < 0)\r\ngoto out;\r\nirq = find_unassigned_irq();\r\nif (irq < 0)\r\ngoto out;\r\nBUG_ON(__bind_irq_vector(irq, vector, domain));\r\nout:\r\nspin_unlock_irqrestore(&vector_lock, flags);\r\nif (irq >= 0)\r\nirq_init_desc(irq);\r\nreturn irq;\r\n}\r\nvoid destroy_irq(unsigned int irq)\r\n{\r\nirq_init_desc(irq);\r\nclear_irq_vector(irq);\r\n}\r\nvoid\r\nia64_handle_irq (ia64_vector vector, struct pt_regs *regs)\r\n{\r\nstruct pt_regs *old_regs = set_irq_regs(regs);\r\nunsigned long saved_tpr;\r\n#if IRQ_DEBUG\r\n{\r\nunsigned long bsp, sp;\r\nbsp = ia64_getreg(_IA64_REG_AR_BSP);\r\nsp = ia64_getreg(_IA64_REG_SP);\r\nif ((sp - bsp) < 1024) {\r\nstatic DEFINE_RATELIMIT_STATE(ratelimit, 5 * HZ, 5);\r\nif (__ratelimit(&ratelimit)) {\r\nprintk("ia64_handle_irq: DANGER: less than "\r\n"1KB of free stack space!!\n"\r\n"(bsp=0x%lx, sp=%lx)\n", bsp, sp);\r\n}\r\n}\r\n}\r\n#endif\r\nirq_enter();\r\nsaved_tpr = ia64_getreg(_IA64_REG_CR_TPR);\r\nia64_srlz_d();\r\nwhile (vector != IA64_SPURIOUS_INT_VECTOR) {\r\nint irq = local_vector_to_irq(vector);\r\nif (unlikely(IS_LOCAL_TLB_FLUSH(vector))) {\r\nsmp_local_flush_tlb();\r\nkstat_incr_irq_this_cpu(irq);\r\n} else if (unlikely(IS_RESCHEDULE(vector))) {\r\nscheduler_ipi();\r\nkstat_incr_irq_this_cpu(irq);\r\n} else {\r\nia64_setreg(_IA64_REG_CR_TPR, vector);\r\nia64_srlz_d();\r\nif (unlikely(irq < 0)) {\r\nprintk(KERN_ERR "%s: Unexpected interrupt "\r\n"vector %d on CPU %d is not mapped "\r\n"to any IRQ!\n", __func__, vector,\r\nsmp_processor_id());\r\n} else\r\ngeneric_handle_irq(irq);\r\nlocal_irq_disable();\r\nia64_setreg(_IA64_REG_CR_TPR, saved_tpr);\r\n}\r\nia64_eoi();\r\nvector = ia64_get_ivr();\r\n}\r\nirq_exit();\r\nset_irq_regs(old_regs);\r\n}\r\nvoid ia64_process_pending_intr(void)\r\n{\r\nia64_vector vector;\r\nunsigned long saved_tpr;\r\nextern unsigned int vectors_in_migration[NR_IRQS];\r\nvector = ia64_get_ivr();\r\nirq_enter();\r\nsaved_tpr = ia64_getreg(_IA64_REG_CR_TPR);\r\nia64_srlz_d();\r\nwhile (vector != IA64_SPURIOUS_INT_VECTOR) {\r\nint irq = local_vector_to_irq(vector);\r\nif (unlikely(IS_LOCAL_TLB_FLUSH(vector))) {\r\nsmp_local_flush_tlb();\r\nkstat_incr_irq_this_cpu(irq);\r\n} else if (unlikely(IS_RESCHEDULE(vector))) {\r\nkstat_incr_irq_this_cpu(irq);\r\n} else {\r\nstruct pt_regs *old_regs = set_irq_regs(NULL);\r\nia64_setreg(_IA64_REG_CR_TPR, vector);\r\nia64_srlz_d();\r\nif (unlikely(irq < 0)) {\r\nprintk(KERN_ERR "%s: Unexpected interrupt "\r\n"vector %d on CPU %d not being mapped "\r\n"to any IRQ!!\n", __func__, vector,\r\nsmp_processor_id());\r\n} else {\r\nvectors_in_migration[irq]=0;\r\ngeneric_handle_irq(irq);\r\n}\r\nset_irq_regs(old_regs);\r\nlocal_irq_disable();\r\nia64_setreg(_IA64_REG_CR_TPR, saved_tpr);\r\n}\r\nia64_eoi();\r\nvector = ia64_get_ivr();\r\n}\r\nirq_exit();\r\n}\r\nstatic irqreturn_t dummy_handler (int irq, void *dev_id)\r\n{\r\nBUG();\r\n}\r\nvoid\r\nia64_native_register_percpu_irq (ia64_vector vec, struct irqaction *action)\r\n{\r\nunsigned int irq;\r\nirq = vec;\r\nBUG_ON(bind_irq_vector(irq, vec, CPU_MASK_ALL));\r\nirq_set_status_flags(irq, IRQ_PER_CPU);\r\nirq_set_chip(irq, &irq_type_ia64_lsapic);\r\nif (action)\r\nsetup_irq(irq, action);\r\nirq_set_handler(irq, handle_percpu_irq);\r\n}\r\nvoid __init\r\nia64_native_register_ipi(void)\r\n{\r\n#ifdef CONFIG_SMP\r\nregister_percpu_irq(IA64_IPI_VECTOR, &ipi_irqaction);\r\nregister_percpu_irq(IA64_IPI_RESCHEDULE, &resched_irqaction);\r\nregister_percpu_irq(IA64_IPI_LOCAL_TLB_FLUSH, &tlb_irqaction);\r\n#endif\r\n}\r\nvoid __init\r\ninit_IRQ (void)\r\n{\r\n#ifdef CONFIG_ACPI\r\nacpi_boot_init();\r\n#endif\r\nia64_register_ipi();\r\nregister_percpu_irq(IA64_SPURIOUS_INT_VECTOR, NULL);\r\n#ifdef CONFIG_SMP\r\n#if defined(CONFIG_IA64_GENERIC) || defined(CONFIG_IA64_DIG)\r\nif (vector_domain_type != VECTOR_DOMAIN_NONE)\r\nregister_percpu_irq(IA64_IRQ_MOVE_VECTOR, &irq_move_irqaction);\r\n#endif\r\n#endif\r\n#ifdef CONFIG_PERFMON\r\npfm_init_percpu();\r\n#endif\r\nplatform_irq_init();\r\n}\r\nvoid\r\nia64_send_ipi (int cpu, int vector, int delivery_mode, int redirect)\r\n{\r\nvoid __iomem *ipi_addr;\r\nunsigned long ipi_data;\r\nunsigned long phys_cpu_id;\r\nphys_cpu_id = cpu_physical_id(cpu);\r\nipi_data = (delivery_mode << 8) | (vector & 0xff);\r\nipi_addr = ipi_base_addr + ((phys_cpu_id << 4) | ((redirect & 1) << 3));\r\nwriteq(ipi_data, ipi_addr);\r\n}
