void cpuidle_coupled_parallel_barrier(struct cpuidle_device *dev, atomic_t *a)\r\n{\r\nint n = dev->coupled->online_count;\r\nsmp_mb__before_atomic();\r\natomic_inc(a);\r\nwhile (atomic_read(a) < n)\r\ncpu_relax();\r\nif (atomic_inc_return(a) == n * 2) {\r\natomic_set(a, 0);\r\nreturn;\r\n}\r\nwhile (atomic_read(a) > n)\r\ncpu_relax();\r\n}\r\nbool cpuidle_state_is_coupled(struct cpuidle_device *dev,\r\nstruct cpuidle_driver *drv, int state)\r\n{\r\nreturn drv->states[state].flags & CPUIDLE_FLAG_COUPLED;\r\n}\r\nstatic inline void cpuidle_coupled_set_ready(struct cpuidle_coupled *coupled)\r\n{\r\natomic_add(MAX_WAITING_CPUS, &coupled->ready_waiting_counts);\r\n}\r\nstatic\r\ninline int cpuidle_coupled_set_not_ready(struct cpuidle_coupled *coupled)\r\n{\r\nint all;\r\nint ret;\r\nall = coupled->online_count | (coupled->online_count << WAITING_BITS);\r\nret = atomic_add_unless(&coupled->ready_waiting_counts,\r\n-MAX_WAITING_CPUS, all);\r\nreturn ret ? 0 : -EINVAL;\r\n}\r\nstatic inline int cpuidle_coupled_no_cpus_ready(struct cpuidle_coupled *coupled)\r\n{\r\nint r = atomic_read(&coupled->ready_waiting_counts) >> WAITING_BITS;\r\nreturn r == 0;\r\n}\r\nstatic inline bool cpuidle_coupled_cpus_ready(struct cpuidle_coupled *coupled)\r\n{\r\nint r = atomic_read(&coupled->ready_waiting_counts) >> WAITING_BITS;\r\nreturn r == coupled->online_count;\r\n}\r\nstatic inline bool cpuidle_coupled_cpus_waiting(struct cpuidle_coupled *coupled)\r\n{\r\nint w = atomic_read(&coupled->ready_waiting_counts) & WAITING_MASK;\r\nreturn w == coupled->online_count;\r\n}\r\nstatic inline int cpuidle_coupled_no_cpus_waiting(struct cpuidle_coupled *coupled)\r\n{\r\nint w = atomic_read(&coupled->ready_waiting_counts) & WAITING_MASK;\r\nreturn w == 0;\r\n}\r\nstatic inline int cpuidle_coupled_get_state(struct cpuidle_device *dev,\r\nstruct cpuidle_coupled *coupled)\r\n{\r\nint i;\r\nint state = INT_MAX;\r\nsmp_rmb();\r\nfor_each_cpu_mask(i, coupled->coupled_cpus)\r\nif (cpu_online(i) && coupled->requested_state[i] < state)\r\nstate = coupled->requested_state[i];\r\nreturn state;\r\n}\r\nstatic void cpuidle_coupled_handle_poke(void *info)\r\n{\r\nint cpu = (unsigned long)info;\r\ncpumask_set_cpu(cpu, &cpuidle_coupled_poked);\r\ncpumask_clear_cpu(cpu, &cpuidle_coupled_poke_pending);\r\n}\r\nstatic void cpuidle_coupled_poke(int cpu)\r\n{\r\nstruct call_single_data *csd = &per_cpu(cpuidle_coupled_poke_cb, cpu);\r\nif (!cpumask_test_and_set_cpu(cpu, &cpuidle_coupled_poke_pending))\r\nsmp_call_function_single_async(cpu, csd);\r\n}\r\nstatic void cpuidle_coupled_poke_others(int this_cpu,\r\nstruct cpuidle_coupled *coupled)\r\n{\r\nint cpu;\r\nfor_each_cpu_mask(cpu, coupled->coupled_cpus)\r\nif (cpu != this_cpu && cpu_online(cpu))\r\ncpuidle_coupled_poke(cpu);\r\n}\r\nstatic int cpuidle_coupled_set_waiting(int cpu,\r\nstruct cpuidle_coupled *coupled, int next_state)\r\n{\r\ncoupled->requested_state[cpu] = next_state;\r\nreturn atomic_inc_return(&coupled->ready_waiting_counts) & WAITING_MASK;\r\n}\r\nstatic void cpuidle_coupled_set_not_waiting(int cpu,\r\nstruct cpuidle_coupled *coupled)\r\n{\r\natomic_dec(&coupled->ready_waiting_counts);\r\ncoupled->requested_state[cpu] = CPUIDLE_COUPLED_NOT_IDLE;\r\n}\r\nstatic void cpuidle_coupled_set_done(int cpu, struct cpuidle_coupled *coupled)\r\n{\r\ncpuidle_coupled_set_not_waiting(cpu, coupled);\r\natomic_sub(MAX_WAITING_CPUS, &coupled->ready_waiting_counts);\r\n}\r\nstatic int cpuidle_coupled_clear_pokes(int cpu)\r\n{\r\nif (!cpumask_test_cpu(cpu, &cpuidle_coupled_poke_pending))\r\nreturn 0;\r\nlocal_irq_enable();\r\nwhile (cpumask_test_cpu(cpu, &cpuidle_coupled_poke_pending))\r\ncpu_relax();\r\nlocal_irq_disable();\r\nreturn 1;\r\n}\r\nstatic bool cpuidle_coupled_any_pokes_pending(struct cpuidle_coupled *coupled)\r\n{\r\ncpumask_t cpus;\r\nint ret;\r\ncpumask_and(&cpus, cpu_online_mask, &coupled->coupled_cpus);\r\nret = cpumask_and(&cpus, &cpuidle_coupled_poke_pending, &cpus);\r\nreturn ret;\r\n}\r\nint cpuidle_enter_state_coupled(struct cpuidle_device *dev,\r\nstruct cpuidle_driver *drv, int next_state)\r\n{\r\nint entered_state = -1;\r\nstruct cpuidle_coupled *coupled = dev->coupled;\r\nint w;\r\nif (!coupled)\r\nreturn -EINVAL;\r\nwhile (coupled->prevent) {\r\ncpuidle_coupled_clear_pokes(dev->cpu);\r\nif (need_resched()) {\r\nlocal_irq_enable();\r\nreturn entered_state;\r\n}\r\nentered_state = cpuidle_enter_state(dev, drv,\r\ndev->safe_state_index);\r\nlocal_irq_disable();\r\n}\r\nsmp_rmb();\r\nreset:\r\ncpumask_clear_cpu(dev->cpu, &cpuidle_coupled_poked);\r\nw = cpuidle_coupled_set_waiting(dev->cpu, coupled, next_state);\r\nif (w == coupled->online_count) {\r\ncpumask_set_cpu(dev->cpu, &cpuidle_coupled_poked);\r\ncpuidle_coupled_poke_others(dev->cpu, coupled);\r\n}\r\nretry:\r\nwhile (!cpuidle_coupled_cpus_waiting(coupled) ||\r\n!cpumask_test_cpu(dev->cpu, &cpuidle_coupled_poked)) {\r\nif (cpuidle_coupled_clear_pokes(dev->cpu))\r\ncontinue;\r\nif (need_resched()) {\r\ncpuidle_coupled_set_not_waiting(dev->cpu, coupled);\r\ngoto out;\r\n}\r\nif (coupled->prevent) {\r\ncpuidle_coupled_set_not_waiting(dev->cpu, coupled);\r\ngoto out;\r\n}\r\nentered_state = cpuidle_enter_state(dev, drv,\r\ndev->safe_state_index);\r\nlocal_irq_disable();\r\n}\r\ncpuidle_coupled_clear_pokes(dev->cpu);\r\nif (need_resched()) {\r\ncpuidle_coupled_set_not_waiting(dev->cpu, coupled);\r\ngoto out;\r\n}\r\nsmp_wmb();\r\ncpuidle_coupled_set_ready(coupled);\r\nwhile (!cpuidle_coupled_cpus_ready(coupled)) {\r\nif (!cpuidle_coupled_cpus_waiting(coupled))\r\nif (!cpuidle_coupled_set_not_ready(coupled))\r\ngoto retry;\r\ncpu_relax();\r\n}\r\nsmp_rmb();\r\nif (cpuidle_coupled_any_pokes_pending(coupled)) {\r\ncpuidle_coupled_set_done(dev->cpu, coupled);\r\ncpuidle_coupled_parallel_barrier(dev, &coupled->abort_barrier);\r\ngoto reset;\r\n}\r\nnext_state = cpuidle_coupled_get_state(dev, coupled);\r\nentered_state = cpuidle_enter_state(dev, drv, next_state);\r\ncpuidle_coupled_set_done(dev->cpu, coupled);\r\nout:\r\nlocal_irq_enable();\r\nwhile (!cpuidle_coupled_no_cpus_ready(coupled))\r\ncpu_relax();\r\nreturn entered_state;\r\n}\r\nstatic void cpuidle_coupled_update_online_cpus(struct cpuidle_coupled *coupled)\r\n{\r\ncpumask_t cpus;\r\ncpumask_and(&cpus, cpu_online_mask, &coupled->coupled_cpus);\r\ncoupled->online_count = cpumask_weight(&cpus);\r\n}\r\nint cpuidle_coupled_register_device(struct cpuidle_device *dev)\r\n{\r\nint cpu;\r\nstruct cpuidle_device *other_dev;\r\nstruct call_single_data *csd;\r\nstruct cpuidle_coupled *coupled;\r\nif (cpumask_empty(&dev->coupled_cpus))\r\nreturn 0;\r\nfor_each_cpu_mask(cpu, dev->coupled_cpus) {\r\nother_dev = per_cpu(cpuidle_devices, cpu);\r\nif (other_dev && other_dev->coupled) {\r\ncoupled = other_dev->coupled;\r\ngoto have_coupled;\r\n}\r\n}\r\ncoupled = kzalloc(sizeof(struct cpuidle_coupled), GFP_KERNEL);\r\nif (!coupled)\r\nreturn -ENOMEM;\r\ncoupled->coupled_cpus = dev->coupled_cpus;\r\nhave_coupled:\r\ndev->coupled = coupled;\r\nif (WARN_ON(!cpumask_equal(&dev->coupled_cpus, &coupled->coupled_cpus)))\r\ncoupled->prevent++;\r\ncpuidle_coupled_update_online_cpus(coupled);\r\ncoupled->refcnt++;\r\ncsd = &per_cpu(cpuidle_coupled_poke_cb, dev->cpu);\r\ncsd->func = cpuidle_coupled_handle_poke;\r\ncsd->info = (void *)(unsigned long)dev->cpu;\r\nreturn 0;\r\n}\r\nvoid cpuidle_coupled_unregister_device(struct cpuidle_device *dev)\r\n{\r\nstruct cpuidle_coupled *coupled = dev->coupled;\r\nif (cpumask_empty(&dev->coupled_cpus))\r\nreturn;\r\nif (--coupled->refcnt)\r\nkfree(coupled);\r\ndev->coupled = NULL;\r\n}\r\nstatic void cpuidle_coupled_prevent_idle(struct cpuidle_coupled *coupled)\r\n{\r\nint cpu = get_cpu();\r\ncoupled->prevent++;\r\ncpuidle_coupled_poke_others(cpu, coupled);\r\nput_cpu();\r\nwhile (!cpuidle_coupled_no_cpus_waiting(coupled))\r\ncpu_relax();\r\n}\r\nstatic void cpuidle_coupled_allow_idle(struct cpuidle_coupled *coupled)\r\n{\r\nint cpu = get_cpu();\r\nsmp_wmb();\r\ncoupled->prevent--;\r\ncpuidle_coupled_poke_others(cpu, coupled);\r\nput_cpu();\r\n}\r\nstatic int cpuidle_coupled_cpu_notify(struct notifier_block *nb,\r\nunsigned long action, void *hcpu)\r\n{\r\nint cpu = (unsigned long)hcpu;\r\nstruct cpuidle_device *dev;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_DOWN_PREPARE:\r\ncase CPU_ONLINE:\r\ncase CPU_DEAD:\r\ncase CPU_UP_CANCELED:\r\ncase CPU_DOWN_FAILED:\r\nbreak;\r\ndefault:\r\nreturn NOTIFY_OK;\r\n}\r\nmutex_lock(&cpuidle_lock);\r\ndev = per_cpu(cpuidle_devices, cpu);\r\nif (!dev || !dev->coupled)\r\ngoto out;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_DOWN_PREPARE:\r\ncpuidle_coupled_prevent_idle(dev->coupled);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_DEAD:\r\ncpuidle_coupled_update_online_cpus(dev->coupled);\r\ncase CPU_UP_CANCELED:\r\ncase CPU_DOWN_FAILED:\r\ncpuidle_coupled_allow_idle(dev->coupled);\r\nbreak;\r\n}\r\nout:\r\nmutex_unlock(&cpuidle_lock);\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int __init cpuidle_coupled_init(void)\r\n{\r\nreturn register_cpu_notifier(&cpuidle_coupled_cpu_notifier);\r\n}
