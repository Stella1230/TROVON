void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask)\r\n{\r\nshadow_mmio_mask = mmio_mask;\r\n}\r\nstatic u64 generation_mmio_spte_mask(unsigned int gen)\r\n{\r\nu64 mask;\r\nWARN_ON(gen > MMIO_MAX_GEN);\r\nmask = (gen & MMIO_GEN_LOW_MASK) << MMIO_SPTE_GEN_LOW_SHIFT;\r\nmask |= ((u64)gen >> MMIO_GEN_LOW_SHIFT) << MMIO_SPTE_GEN_HIGH_SHIFT;\r\nreturn mask;\r\n}\r\nstatic unsigned int get_mmio_spte_generation(u64 spte)\r\n{\r\nunsigned int gen;\r\nspte &= ~shadow_mmio_mask;\r\ngen = (spte >> MMIO_SPTE_GEN_LOW_SHIFT) & MMIO_GEN_LOW_MASK;\r\ngen |= (spte >> MMIO_SPTE_GEN_HIGH_SHIFT) << MMIO_GEN_LOW_SHIFT;\r\nreturn gen;\r\n}\r\nstatic unsigned int kvm_current_mmio_generation(struct kvm *kvm)\r\n{\r\nreturn (kvm_memslots(kvm)->generation +\r\nMMIO_MAX_GEN - 150) & MMIO_GEN_MASK;\r\n}\r\nstatic void mark_mmio_spte(struct kvm *kvm, u64 *sptep, u64 gfn,\r\nunsigned access)\r\n{\r\nunsigned int gen = kvm_current_mmio_generation(kvm);\r\nu64 mask = generation_mmio_spte_mask(gen);\r\naccess &= ACC_WRITE_MASK | ACC_USER_MASK;\r\nmask |= shadow_mmio_mask | access | gfn << PAGE_SHIFT;\r\ntrace_mark_mmio_spte(sptep, gfn, access, gen);\r\nmmu_spte_set(sptep, mask);\r\n}\r\nstatic bool is_mmio_spte(u64 spte)\r\n{\r\nreturn (spte & shadow_mmio_mask) == shadow_mmio_mask;\r\n}\r\nstatic gfn_t get_mmio_spte_gfn(u64 spte)\r\n{\r\nu64 mask = generation_mmio_spte_mask(MMIO_MAX_GEN) | shadow_mmio_mask;\r\nreturn (spte & ~mask) >> PAGE_SHIFT;\r\n}\r\nstatic unsigned get_mmio_spte_access(u64 spte)\r\n{\r\nu64 mask = generation_mmio_spte_mask(MMIO_MAX_GEN) | shadow_mmio_mask;\r\nreturn (spte & ~mask) & ~PAGE_MASK;\r\n}\r\nstatic bool set_mmio_spte(struct kvm *kvm, u64 *sptep, gfn_t gfn,\r\npfn_t pfn, unsigned access)\r\n{\r\nif (unlikely(is_noslot_pfn(pfn))) {\r\nmark_mmio_spte(kvm, sptep, gfn, access);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool check_mmio_spte(struct kvm *kvm, u64 spte)\r\n{\r\nunsigned int kvm_gen, spte_gen;\r\nkvm_gen = kvm_current_mmio_generation(kvm);\r\nspte_gen = get_mmio_spte_generation(spte);\r\ntrace_check_mmio_spte(spte, kvm_gen, spte_gen);\r\nreturn likely(kvm_gen == spte_gen);\r\n}\r\nstatic inline u64 rsvd_bits(int s, int e)\r\n{\r\nreturn ((1ULL << (e - s + 1)) - 1) << s;\r\n}\r\nvoid kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,\r\nu64 dirty_mask, u64 nx_mask, u64 x_mask)\r\n{\r\nshadow_user_mask = user_mask;\r\nshadow_accessed_mask = accessed_mask;\r\nshadow_dirty_mask = dirty_mask;\r\nshadow_nx_mask = nx_mask;\r\nshadow_x_mask = x_mask;\r\n}\r\nstatic int is_cpuid_PSE36(void)\r\n{\r\nreturn 1;\r\n}\r\nstatic int is_nx(struct kvm_vcpu *vcpu)\r\n{\r\nreturn vcpu->arch.efer & EFER_NX;\r\n}\r\nstatic int is_shadow_present_pte(u64 pte)\r\n{\r\nreturn pte & PT_PRESENT_MASK && !is_mmio_spte(pte);\r\n}\r\nstatic int is_large_pte(u64 pte)\r\n{\r\nreturn pte & PT_PAGE_SIZE_MASK;\r\n}\r\nstatic int is_rmap_spte(u64 pte)\r\n{\r\nreturn is_shadow_present_pte(pte);\r\n}\r\nstatic int is_last_spte(u64 pte, int level)\r\n{\r\nif (level == PT_PAGE_TABLE_LEVEL)\r\nreturn 1;\r\nif (is_large_pte(pte))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic pfn_t spte_to_pfn(u64 pte)\r\n{\r\nreturn (pte & PT64_BASE_ADDR_MASK) >> PAGE_SHIFT;\r\n}\r\nstatic gfn_t pse36_gfn_delta(u32 gpte)\r\n{\r\nint shift = 32 - PT32_DIR_PSE36_SHIFT - PAGE_SHIFT;\r\nreturn (gpte & PT32_DIR_PSE36_MASK) << shift;\r\n}\r\nstatic void __set_spte(u64 *sptep, u64 spte)\r\n{\r\n*sptep = spte;\r\n}\r\nstatic void __update_clear_spte_fast(u64 *sptep, u64 spte)\r\n{\r\n*sptep = spte;\r\n}\r\nstatic u64 __update_clear_spte_slow(u64 *sptep, u64 spte)\r\n{\r\nreturn xchg(sptep, spte);\r\n}\r\nstatic u64 __get_spte_lockless(u64 *sptep)\r\n{\r\nreturn ACCESS_ONCE(*sptep);\r\n}\r\nstatic bool __check_direct_spte_mmio_pf(u64 spte)\r\n{\r\nreturn spte == 0ull;\r\n}\r\nstatic void count_spte_clear(u64 *sptep, u64 spte)\r\n{\r\nstruct kvm_mmu_page *sp = page_header(__pa(sptep));\r\nif (is_shadow_present_pte(spte))\r\nreturn;\r\nsmp_wmb();\r\nsp->clear_spte_count++;\r\n}\r\nstatic void __set_spte(u64 *sptep, u64 spte)\r\n{\r\nunion split_spte *ssptep, sspte;\r\nssptep = (union split_spte *)sptep;\r\nsspte = (union split_spte)spte;\r\nssptep->spte_high = sspte.spte_high;\r\nsmp_wmb();\r\nssptep->spte_low = sspte.spte_low;\r\n}\r\nstatic void __update_clear_spte_fast(u64 *sptep, u64 spte)\r\n{\r\nunion split_spte *ssptep, sspte;\r\nssptep = (union split_spte *)sptep;\r\nsspte = (union split_spte)spte;\r\nssptep->spte_low = sspte.spte_low;\r\nsmp_wmb();\r\nssptep->spte_high = sspte.spte_high;\r\ncount_spte_clear(sptep, spte);\r\n}\r\nstatic u64 __update_clear_spte_slow(u64 *sptep, u64 spte)\r\n{\r\nunion split_spte *ssptep, sspte, orig;\r\nssptep = (union split_spte *)sptep;\r\nsspte = (union split_spte)spte;\r\norig.spte_low = xchg(&ssptep->spte_low, sspte.spte_low);\r\norig.spte_high = ssptep->spte_high;\r\nssptep->spte_high = sspte.spte_high;\r\ncount_spte_clear(sptep, spte);\r\nreturn orig.spte;\r\n}\r\nstatic u64 __get_spte_lockless(u64 *sptep)\r\n{\r\nstruct kvm_mmu_page *sp = page_header(__pa(sptep));\r\nunion split_spte spte, *orig = (union split_spte *)sptep;\r\nint count;\r\nretry:\r\ncount = sp->clear_spte_count;\r\nsmp_rmb();\r\nspte.spte_low = orig->spte_low;\r\nsmp_rmb();\r\nspte.spte_high = orig->spte_high;\r\nsmp_rmb();\r\nif (unlikely(spte.spte_low != orig->spte_low ||\r\ncount != sp->clear_spte_count))\r\ngoto retry;\r\nreturn spte.spte;\r\n}\r\nstatic bool __check_direct_spte_mmio_pf(u64 spte)\r\n{\r\nunion split_spte sspte = (union split_spte)spte;\r\nu32 high_mmio_mask = shadow_mmio_mask >> 32;\r\nif (spte == 0ull)\r\nreturn true;\r\nif (sspte.spte_low == 0ull &&\r\n(sspte.spte_high & high_mmio_mask) == high_mmio_mask)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool spte_is_locklessly_modifiable(u64 spte)\r\n{\r\nreturn (spte & (SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE)) ==\r\n(SPTE_HOST_WRITEABLE | SPTE_MMU_WRITEABLE);\r\n}\r\nstatic bool spte_has_volatile_bits(u64 spte)\r\n{\r\nif (spte_is_locklessly_modifiable(spte))\r\nreturn true;\r\nif (!shadow_accessed_mask)\r\nreturn false;\r\nif (!is_shadow_present_pte(spte))\r\nreturn false;\r\nif ((spte & shadow_accessed_mask) &&\r\n(!is_writable_pte(spte) || (spte & shadow_dirty_mask)))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool spte_is_bit_cleared(u64 old_spte, u64 new_spte, u64 bit_mask)\r\n{\r\nreturn (old_spte & bit_mask) && !(new_spte & bit_mask);\r\n}\r\nstatic void mmu_spte_set(u64 *sptep, u64 new_spte)\r\n{\r\nWARN_ON(is_shadow_present_pte(*sptep));\r\n__set_spte(sptep, new_spte);\r\n}\r\nstatic bool mmu_spte_update(u64 *sptep, u64 new_spte)\r\n{\r\nu64 old_spte = *sptep;\r\nbool ret = false;\r\nWARN_ON(!is_rmap_spte(new_spte));\r\nif (!is_shadow_present_pte(old_spte)) {\r\nmmu_spte_set(sptep, new_spte);\r\nreturn ret;\r\n}\r\nif (!spte_has_volatile_bits(old_spte))\r\n__update_clear_spte_fast(sptep, new_spte);\r\nelse\r\nold_spte = __update_clear_spte_slow(sptep, new_spte);\r\nif (spte_is_locklessly_modifiable(old_spte) &&\r\n!is_writable_pte(new_spte))\r\nret = true;\r\nif (!shadow_accessed_mask)\r\nreturn ret;\r\nif (spte_is_bit_cleared(old_spte, new_spte, shadow_accessed_mask))\r\nkvm_set_pfn_accessed(spte_to_pfn(old_spte));\r\nif (spte_is_bit_cleared(old_spte, new_spte, shadow_dirty_mask))\r\nkvm_set_pfn_dirty(spte_to_pfn(old_spte));\r\nreturn ret;\r\n}\r\nstatic int mmu_spte_clear_track_bits(u64 *sptep)\r\n{\r\npfn_t pfn;\r\nu64 old_spte = *sptep;\r\nif (!spte_has_volatile_bits(old_spte))\r\n__update_clear_spte_fast(sptep, 0ull);\r\nelse\r\nold_spte = __update_clear_spte_slow(sptep, 0ull);\r\nif (!is_rmap_spte(old_spte))\r\nreturn 0;\r\npfn = spte_to_pfn(old_spte);\r\nWARN_ON(!kvm_is_mmio_pfn(pfn) && !page_count(pfn_to_page(pfn)));\r\nif (!shadow_accessed_mask || old_spte & shadow_accessed_mask)\r\nkvm_set_pfn_accessed(pfn);\r\nif (!shadow_dirty_mask || (old_spte & shadow_dirty_mask))\r\nkvm_set_pfn_dirty(pfn);\r\nreturn 1;\r\n}\r\nstatic void mmu_spte_clear_no_track(u64 *sptep)\r\n{\r\n__update_clear_spte_fast(sptep, 0ull);\r\n}\r\nstatic u64 mmu_spte_get_lockless(u64 *sptep)\r\n{\r\nreturn __get_spte_lockless(sptep);\r\n}\r\nstatic void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)\r\n{\r\nlocal_irq_disable();\r\nvcpu->mode = READING_SHADOW_PAGE_TABLES;\r\nsmp_mb();\r\n}\r\nstatic void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)\r\n{\r\nsmp_mb();\r\nvcpu->mode = OUTSIDE_GUEST_MODE;\r\nlocal_irq_enable();\r\n}\r\nstatic int mmu_topup_memory_cache(struct kvm_mmu_memory_cache *cache,\r\nstruct kmem_cache *base_cache, int min)\r\n{\r\nvoid *obj;\r\nif (cache->nobjs >= min)\r\nreturn 0;\r\nwhile (cache->nobjs < ARRAY_SIZE(cache->objects)) {\r\nobj = kmem_cache_zalloc(base_cache, GFP_KERNEL);\r\nif (!obj)\r\nreturn -ENOMEM;\r\ncache->objects[cache->nobjs++] = obj;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mmu_memory_cache_free_objects(struct kvm_mmu_memory_cache *cache)\r\n{\r\nreturn cache->nobjs;\r\n}\r\nstatic void mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc,\r\nstruct kmem_cache *cache)\r\n{\r\nwhile (mc->nobjs)\r\nkmem_cache_free(cache, mc->objects[--mc->nobjs]);\r\n}\r\nstatic int mmu_topup_memory_cache_page(struct kvm_mmu_memory_cache *cache,\r\nint min)\r\n{\r\nvoid *page;\r\nif (cache->nobjs >= min)\r\nreturn 0;\r\nwhile (cache->nobjs < ARRAY_SIZE(cache->objects)) {\r\npage = (void *)__get_free_page(GFP_KERNEL);\r\nif (!page)\r\nreturn -ENOMEM;\r\ncache->objects[cache->nobjs++] = page;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mmu_free_memory_cache_page(struct kvm_mmu_memory_cache *mc)\r\n{\r\nwhile (mc->nobjs)\r\nfree_page((unsigned long)mc->objects[--mc->nobjs]);\r\n}\r\nstatic int mmu_topup_memory_caches(struct kvm_vcpu *vcpu)\r\n{\r\nint r;\r\nr = mmu_topup_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,\r\npte_list_desc_cache, 8 + PTE_PREFETCH_NUM);\r\nif (r)\r\ngoto out;\r\nr = mmu_topup_memory_cache_page(&vcpu->arch.mmu_page_cache, 8);\r\nif (r)\r\ngoto out;\r\nr = mmu_topup_memory_cache(&vcpu->arch.mmu_page_header_cache,\r\nmmu_page_header_cache, 4);\r\nout:\r\nreturn r;\r\n}\r\nstatic void mmu_free_memory_caches(struct kvm_vcpu *vcpu)\r\n{\r\nmmu_free_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,\r\npte_list_desc_cache);\r\nmmu_free_memory_cache_page(&vcpu->arch.mmu_page_cache);\r\nmmu_free_memory_cache(&vcpu->arch.mmu_page_header_cache,\r\nmmu_page_header_cache);\r\n}\r\nstatic void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)\r\n{\r\nvoid *p;\r\nBUG_ON(!mc->nobjs);\r\np = mc->objects[--mc->nobjs];\r\nreturn p;\r\n}\r\nstatic struct pte_list_desc *mmu_alloc_pte_list_desc(struct kvm_vcpu *vcpu)\r\n{\r\nreturn mmu_memory_cache_alloc(&vcpu->arch.mmu_pte_list_desc_cache);\r\n}\r\nstatic void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)\r\n{\r\nkmem_cache_free(pte_list_desc_cache, pte_list_desc);\r\n}\r\nstatic gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)\r\n{\r\nif (!sp->role.direct)\r\nreturn sp->gfns[index];\r\nreturn sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));\r\n}\r\nstatic void kvm_mmu_page_set_gfn(struct kvm_mmu_page *sp, int index, gfn_t gfn)\r\n{\r\nif (sp->role.direct)\r\nBUG_ON(gfn != kvm_mmu_page_get_gfn(sp, index));\r\nelse\r\nsp->gfns[index] = gfn;\r\n}\r\nstatic struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,\r\nstruct kvm_memory_slot *slot,\r\nint level)\r\n{\r\nunsigned long idx;\r\nidx = gfn_to_index(gfn, slot->base_gfn, level);\r\nreturn &slot->arch.lpage_info[level - 2][idx];\r\n}\r\nstatic void account_shadowed(struct kvm *kvm, gfn_t gfn)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nstruct kvm_lpage_info *linfo;\r\nint i;\r\nslot = gfn_to_memslot(kvm, gfn);\r\nfor (i = PT_DIRECTORY_LEVEL;\r\ni < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++i) {\r\nlinfo = lpage_info_slot(gfn, slot, i);\r\nlinfo->write_count += 1;\r\n}\r\nkvm->arch.indirect_shadow_pages++;\r\n}\r\nstatic void unaccount_shadowed(struct kvm *kvm, gfn_t gfn)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nstruct kvm_lpage_info *linfo;\r\nint i;\r\nslot = gfn_to_memslot(kvm, gfn);\r\nfor (i = PT_DIRECTORY_LEVEL;\r\ni < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++i) {\r\nlinfo = lpage_info_slot(gfn, slot, i);\r\nlinfo->write_count -= 1;\r\nWARN_ON(linfo->write_count < 0);\r\n}\r\nkvm->arch.indirect_shadow_pages--;\r\n}\r\nstatic int has_wrprotected_page(struct kvm *kvm,\r\ngfn_t gfn,\r\nint level)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nstruct kvm_lpage_info *linfo;\r\nslot = gfn_to_memslot(kvm, gfn);\r\nif (slot) {\r\nlinfo = lpage_info_slot(gfn, slot, level);\r\nreturn linfo->write_count;\r\n}\r\nreturn 1;\r\n}\r\nstatic int host_mapping_level(struct kvm *kvm, gfn_t gfn)\r\n{\r\nunsigned long page_size;\r\nint i, ret = 0;\r\npage_size = kvm_host_page_size(kvm, gfn);\r\nfor (i = PT_PAGE_TABLE_LEVEL;\r\ni < (PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES); ++i) {\r\nif (page_size >= KVM_HPAGE_SIZE(i))\r\nret = i;\r\nelse\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct kvm_memory_slot *\r\ngfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu, gfn_t gfn,\r\nbool no_dirty_log)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nslot = gfn_to_memslot(vcpu->kvm, gfn);\r\nif (!slot || slot->flags & KVM_MEMSLOT_INVALID ||\r\n(no_dirty_log && slot->dirty_bitmap))\r\nslot = NULL;\r\nreturn slot;\r\n}\r\nstatic bool mapping_level_dirty_bitmap(struct kvm_vcpu *vcpu, gfn_t large_gfn)\r\n{\r\nreturn !gfn_to_memslot_dirty_bitmap(vcpu, large_gfn, true);\r\n}\r\nstatic int mapping_level(struct kvm_vcpu *vcpu, gfn_t large_gfn)\r\n{\r\nint host_level, level, max_level;\r\nhost_level = host_mapping_level(vcpu->kvm, large_gfn);\r\nif (host_level == PT_PAGE_TABLE_LEVEL)\r\nreturn host_level;\r\nmax_level = min(kvm_x86_ops->get_lpage_level(), host_level);\r\nfor (level = PT_DIRECTORY_LEVEL; level <= max_level; ++level)\r\nif (has_wrprotected_page(vcpu->kvm, large_gfn, level))\r\nbreak;\r\nreturn level - 1;\r\n}\r\nstatic int pte_list_add(struct kvm_vcpu *vcpu, u64 *spte,\r\nunsigned long *pte_list)\r\n{\r\nstruct pte_list_desc *desc;\r\nint i, count = 0;\r\nif (!*pte_list) {\r\nrmap_printk("pte_list_add: %p %llx 0->1\n", spte, *spte);\r\n*pte_list = (unsigned long)spte;\r\n} else if (!(*pte_list & 1)) {\r\nrmap_printk("pte_list_add: %p %llx 1->many\n", spte, *spte);\r\ndesc = mmu_alloc_pte_list_desc(vcpu);\r\ndesc->sptes[0] = (u64 *)*pte_list;\r\ndesc->sptes[1] = spte;\r\n*pte_list = (unsigned long)desc | 1;\r\n++count;\r\n} else {\r\nrmap_printk("pte_list_add: %p %llx many->many\n", spte, *spte);\r\ndesc = (struct pte_list_desc *)(*pte_list & ~1ul);\r\nwhile (desc->sptes[PTE_LIST_EXT-1] && desc->more) {\r\ndesc = desc->more;\r\ncount += PTE_LIST_EXT;\r\n}\r\nif (desc->sptes[PTE_LIST_EXT-1]) {\r\ndesc->more = mmu_alloc_pte_list_desc(vcpu);\r\ndesc = desc->more;\r\n}\r\nfor (i = 0; desc->sptes[i]; ++i)\r\n++count;\r\ndesc->sptes[i] = spte;\r\n}\r\nreturn count;\r\n}\r\nstatic void\r\npte_list_desc_remove_entry(unsigned long *pte_list, struct pte_list_desc *desc,\r\nint i, struct pte_list_desc *prev_desc)\r\n{\r\nint j;\r\nfor (j = PTE_LIST_EXT - 1; !desc->sptes[j] && j > i; --j)\r\n;\r\ndesc->sptes[i] = desc->sptes[j];\r\ndesc->sptes[j] = NULL;\r\nif (j != 0)\r\nreturn;\r\nif (!prev_desc && !desc->more)\r\n*pte_list = (unsigned long)desc->sptes[0];\r\nelse\r\nif (prev_desc)\r\nprev_desc->more = desc->more;\r\nelse\r\n*pte_list = (unsigned long)desc->more | 1;\r\nmmu_free_pte_list_desc(desc);\r\n}\r\nstatic void pte_list_remove(u64 *spte, unsigned long *pte_list)\r\n{\r\nstruct pte_list_desc *desc;\r\nstruct pte_list_desc *prev_desc;\r\nint i;\r\nif (!*pte_list) {\r\nprintk(KERN_ERR "pte_list_remove: %p 0->BUG\n", spte);\r\nBUG();\r\n} else if (!(*pte_list & 1)) {\r\nrmap_printk("pte_list_remove: %p 1->0\n", spte);\r\nif ((u64 *)*pte_list != spte) {\r\nprintk(KERN_ERR "pte_list_remove: %p 1->BUG\n", spte);\r\nBUG();\r\n}\r\n*pte_list = 0;\r\n} else {\r\nrmap_printk("pte_list_remove: %p many->many\n", spte);\r\ndesc = (struct pte_list_desc *)(*pte_list & ~1ul);\r\nprev_desc = NULL;\r\nwhile (desc) {\r\nfor (i = 0; i < PTE_LIST_EXT && desc->sptes[i]; ++i)\r\nif (desc->sptes[i] == spte) {\r\npte_list_desc_remove_entry(pte_list,\r\ndesc, i,\r\nprev_desc);\r\nreturn;\r\n}\r\nprev_desc = desc;\r\ndesc = desc->more;\r\n}\r\npr_err("pte_list_remove: %p many->many\n", spte);\r\nBUG();\r\n}\r\n}\r\nstatic void pte_list_walk(unsigned long *pte_list, pte_list_walk_fn fn)\r\n{\r\nstruct pte_list_desc *desc;\r\nint i;\r\nif (!*pte_list)\r\nreturn;\r\nif (!(*pte_list & 1))\r\nreturn fn((u64 *)*pte_list);\r\ndesc = (struct pte_list_desc *)(*pte_list & ~1ul);\r\nwhile (desc) {\r\nfor (i = 0; i < PTE_LIST_EXT && desc->sptes[i]; ++i)\r\nfn(desc->sptes[i]);\r\ndesc = desc->more;\r\n}\r\n}\r\nstatic unsigned long *__gfn_to_rmap(gfn_t gfn, int level,\r\nstruct kvm_memory_slot *slot)\r\n{\r\nunsigned long idx;\r\nidx = gfn_to_index(gfn, slot->base_gfn, level);\r\nreturn &slot->arch.rmap[level - PT_PAGE_TABLE_LEVEL][idx];\r\n}\r\nstatic unsigned long *gfn_to_rmap(struct kvm *kvm, gfn_t gfn, int level)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nslot = gfn_to_memslot(kvm, gfn);\r\nreturn __gfn_to_rmap(gfn, level, slot);\r\n}\r\nstatic bool rmap_can_add(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_mmu_memory_cache *cache;\r\ncache = &vcpu->arch.mmu_pte_list_desc_cache;\r\nreturn mmu_memory_cache_free_objects(cache);\r\n}\r\nstatic int rmap_add(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nunsigned long *rmapp;\r\nsp = page_header(__pa(spte));\r\nkvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);\r\nrmapp = gfn_to_rmap(vcpu->kvm, gfn, sp->role.level);\r\nreturn pte_list_add(vcpu, spte, rmapp);\r\n}\r\nstatic void rmap_remove(struct kvm *kvm, u64 *spte)\r\n{\r\nstruct kvm_mmu_page *sp;\r\ngfn_t gfn;\r\nunsigned long *rmapp;\r\nsp = page_header(__pa(spte));\r\ngfn = kvm_mmu_page_get_gfn(sp, spte - sp->spt);\r\nrmapp = gfn_to_rmap(kvm, gfn, sp->role.level);\r\npte_list_remove(spte, rmapp);\r\n}\r\nstatic u64 *rmap_get_first(unsigned long rmap, struct rmap_iterator *iter)\r\n{\r\nif (!rmap)\r\nreturn NULL;\r\nif (!(rmap & 1)) {\r\niter->desc = NULL;\r\nreturn (u64 *)rmap;\r\n}\r\niter->desc = (struct pte_list_desc *)(rmap & ~1ul);\r\niter->pos = 0;\r\nreturn iter->desc->sptes[iter->pos];\r\n}\r\nstatic u64 *rmap_get_next(struct rmap_iterator *iter)\r\n{\r\nif (iter->desc) {\r\nif (iter->pos < PTE_LIST_EXT - 1) {\r\nu64 *sptep;\r\n++iter->pos;\r\nsptep = iter->desc->sptes[iter->pos];\r\nif (sptep)\r\nreturn sptep;\r\n}\r\niter->desc = iter->desc->more;\r\nif (iter->desc) {\r\niter->pos = 0;\r\nreturn iter->desc->sptes[iter->pos];\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void drop_spte(struct kvm *kvm, u64 *sptep)\r\n{\r\nif (mmu_spte_clear_track_bits(sptep))\r\nrmap_remove(kvm, sptep);\r\n}\r\nstatic bool __drop_large_spte(struct kvm *kvm, u64 *sptep)\r\n{\r\nif (is_large_pte(*sptep)) {\r\nWARN_ON(page_header(__pa(sptep))->role.level ==\r\nPT_PAGE_TABLE_LEVEL);\r\ndrop_spte(kvm, sptep);\r\n--kvm->stat.lpages;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)\r\n{\r\nif (__drop_large_spte(vcpu->kvm, sptep))\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\n}\r\nstatic bool spte_write_protect(struct kvm *kvm, u64 *sptep, bool pt_protect)\r\n{\r\nu64 spte = *sptep;\r\nif (!is_writable_pte(spte) &&\r\n!(pt_protect && spte_is_locklessly_modifiable(spte)))\r\nreturn false;\r\nrmap_printk("rmap_write_protect: spte %p %llx\n", sptep, *sptep);\r\nif (pt_protect)\r\nspte &= ~SPTE_MMU_WRITEABLE;\r\nspte = spte & ~PT_WRITABLE_MASK;\r\nreturn mmu_spte_update(sptep, spte);\r\n}\r\nstatic bool __rmap_write_protect(struct kvm *kvm, unsigned long *rmapp,\r\nbool pt_protect)\r\n{\r\nu64 *sptep;\r\nstruct rmap_iterator iter;\r\nbool flush = false;\r\nfor (sptep = rmap_get_first(*rmapp, &iter); sptep;) {\r\nBUG_ON(!(*sptep & PT_PRESENT_MASK));\r\nflush |= spte_write_protect(kvm, sptep, pt_protect);\r\nsptep = rmap_get_next(&iter);\r\n}\r\nreturn flush;\r\n}\r\nvoid kvm_mmu_write_protect_pt_masked(struct kvm *kvm,\r\nstruct kvm_memory_slot *slot,\r\ngfn_t gfn_offset, unsigned long mask)\r\n{\r\nunsigned long *rmapp;\r\nwhile (mask) {\r\nrmapp = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),\r\nPT_PAGE_TABLE_LEVEL, slot);\r\n__rmap_write_protect(kvm, rmapp, false);\r\nmask &= mask - 1;\r\n}\r\n}\r\nstatic bool rmap_write_protect(struct kvm *kvm, u64 gfn)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nunsigned long *rmapp;\r\nint i;\r\nbool write_protected = false;\r\nslot = gfn_to_memslot(kvm, gfn);\r\nfor (i = PT_PAGE_TABLE_LEVEL;\r\ni < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++i) {\r\nrmapp = __gfn_to_rmap(gfn, i, slot);\r\nwrite_protected |= __rmap_write_protect(kvm, rmapp, true);\r\n}\r\nreturn write_protected;\r\n}\r\nstatic int kvm_unmap_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nstruct kvm_memory_slot *slot, unsigned long data)\r\n{\r\nu64 *sptep;\r\nstruct rmap_iterator iter;\r\nint need_tlb_flush = 0;\r\nwhile ((sptep = rmap_get_first(*rmapp, &iter))) {\r\nBUG_ON(!(*sptep & PT_PRESENT_MASK));\r\nrmap_printk("kvm_rmap_unmap_hva: spte %p %llx\n", sptep, *sptep);\r\ndrop_spte(kvm, sptep);\r\nneed_tlb_flush = 1;\r\n}\r\nreturn need_tlb_flush;\r\n}\r\nstatic int kvm_set_pte_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nstruct kvm_memory_slot *slot, unsigned long data)\r\n{\r\nu64 *sptep;\r\nstruct rmap_iterator iter;\r\nint need_flush = 0;\r\nu64 new_spte;\r\npte_t *ptep = (pte_t *)data;\r\npfn_t new_pfn;\r\nWARN_ON(pte_huge(*ptep));\r\nnew_pfn = pte_pfn(*ptep);\r\nfor (sptep = rmap_get_first(*rmapp, &iter); sptep;) {\r\nBUG_ON(!is_shadow_present_pte(*sptep));\r\nrmap_printk("kvm_set_pte_rmapp: spte %p %llx\n", sptep, *sptep);\r\nneed_flush = 1;\r\nif (pte_write(*ptep)) {\r\ndrop_spte(kvm, sptep);\r\nsptep = rmap_get_first(*rmapp, &iter);\r\n} else {\r\nnew_spte = *sptep & ~PT64_BASE_ADDR_MASK;\r\nnew_spte |= (u64)new_pfn << PAGE_SHIFT;\r\nnew_spte &= ~PT_WRITABLE_MASK;\r\nnew_spte &= ~SPTE_HOST_WRITEABLE;\r\nnew_spte &= ~shadow_accessed_mask;\r\nmmu_spte_clear_track_bits(sptep);\r\nmmu_spte_set(sptep, new_spte);\r\nsptep = rmap_get_next(&iter);\r\n}\r\n}\r\nif (need_flush)\r\nkvm_flush_remote_tlbs(kvm);\r\nreturn 0;\r\n}\r\nstatic int kvm_handle_hva_range(struct kvm *kvm,\r\nunsigned long start,\r\nunsigned long end,\r\nunsigned long data,\r\nint (*handler)(struct kvm *kvm,\r\nunsigned long *rmapp,\r\nstruct kvm_memory_slot *slot,\r\nunsigned long data))\r\n{\r\nint j;\r\nint ret = 0;\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nunsigned long hva_start, hva_end;\r\ngfn_t gfn_start, gfn_end;\r\nhva_start = max(start, memslot->userspace_addr);\r\nhva_end = min(end, memslot->userspace_addr +\r\n(memslot->npages << PAGE_SHIFT));\r\nif (hva_start >= hva_end)\r\ncontinue;\r\ngfn_start = hva_to_gfn_memslot(hva_start, memslot);\r\ngfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);\r\nfor (j = PT_PAGE_TABLE_LEVEL;\r\nj < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++j) {\r\nunsigned long idx, idx_end;\r\nunsigned long *rmapp;\r\nidx = gfn_to_index(gfn_start, memslot->base_gfn, j);\r\nidx_end = gfn_to_index(gfn_end - 1, memslot->base_gfn, j);\r\nrmapp = __gfn_to_rmap(gfn_start, j, memslot);\r\nfor (; idx <= idx_end; ++idx)\r\nret |= handler(kvm, rmapp++, memslot, data);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_handle_hva(struct kvm *kvm, unsigned long hva,\r\nunsigned long data,\r\nint (*handler)(struct kvm *kvm, unsigned long *rmapp,\r\nstruct kvm_memory_slot *slot,\r\nunsigned long data))\r\n{\r\nreturn kvm_handle_hva_range(kvm, hva, hva + 1, data, handler);\r\n}\r\nint kvm_unmap_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn kvm_handle_hva(kvm, hva, 0, kvm_unmap_rmapp);\r\n}\r\nint kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nreturn kvm_handle_hva_range(kvm, start, end, 0, kvm_unmap_rmapp);\r\n}\r\nvoid kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)\r\n{\r\nkvm_handle_hva(kvm, hva, (unsigned long)&pte, kvm_set_pte_rmapp);\r\n}\r\nstatic int kvm_age_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nstruct kvm_memory_slot *slot, unsigned long data)\r\n{\r\nu64 *sptep;\r\nstruct rmap_iterator uninitialized_var(iter);\r\nint young = 0;\r\nif (!shadow_accessed_mask) {\r\nyoung = kvm_unmap_rmapp(kvm, rmapp, slot, data);\r\ngoto out;\r\n}\r\nfor (sptep = rmap_get_first(*rmapp, &iter); sptep;\r\nsptep = rmap_get_next(&iter)) {\r\nBUG_ON(!is_shadow_present_pte(*sptep));\r\nif (*sptep & shadow_accessed_mask) {\r\nyoung = 1;\r\nclear_bit((ffs(shadow_accessed_mask) - 1),\r\n(unsigned long *)sptep);\r\n}\r\n}\r\nout:\r\ntrace_kvm_age_page(data, slot, young);\r\nreturn young;\r\n}\r\nstatic int kvm_test_age_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nstruct kvm_memory_slot *slot, unsigned long data)\r\n{\r\nu64 *sptep;\r\nstruct rmap_iterator iter;\r\nint young = 0;\r\nif (!shadow_accessed_mask)\r\ngoto out;\r\nfor (sptep = rmap_get_first(*rmapp, &iter); sptep;\r\nsptep = rmap_get_next(&iter)) {\r\nBUG_ON(!is_shadow_present_pte(*sptep));\r\nif (*sptep & shadow_accessed_mask) {\r\nyoung = 1;\r\nbreak;\r\n}\r\n}\r\nout:\r\nreturn young;\r\n}\r\nstatic void rmap_recycle(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)\r\n{\r\nunsigned long *rmapp;\r\nstruct kvm_mmu_page *sp;\r\nsp = page_header(__pa(spte));\r\nrmapp = gfn_to_rmap(vcpu->kvm, gfn, sp->role.level);\r\nkvm_unmap_rmapp(vcpu->kvm, rmapp, NULL, 0);\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\n}\r\nint kvm_age_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn kvm_handle_hva(kvm, hva, hva, kvm_age_rmapp);\r\n}\r\nint kvm_test_age_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn kvm_handle_hva(kvm, hva, 0, kvm_test_age_rmapp);\r\n}\r\nstatic int is_empty_shadow_page(u64 *spt)\r\n{\r\nu64 *pos;\r\nu64 *end;\r\nfor (pos = spt, end = pos + PAGE_SIZE / sizeof(u64); pos != end; pos++)\r\nif (is_shadow_present_pte(*pos)) {\r\nprintk(KERN_ERR "%s: %p %llx\n", __func__,\r\npos, *pos);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic inline void kvm_mod_used_mmu_pages(struct kvm *kvm, int nr)\r\n{\r\nkvm->arch.n_used_mmu_pages += nr;\r\npercpu_counter_add(&kvm_total_used_mmu_pages, nr);\r\n}\r\nstatic void kvm_mmu_free_page(struct kvm_mmu_page *sp)\r\n{\r\nASSERT(is_empty_shadow_page(sp->spt));\r\nhlist_del(&sp->hash_link);\r\nlist_del(&sp->link);\r\nfree_page((unsigned long)sp->spt);\r\nif (!sp->role.direct)\r\nfree_page((unsigned long)sp->gfns);\r\nkmem_cache_free(mmu_page_header_cache, sp);\r\n}\r\nstatic unsigned kvm_page_table_hashfn(gfn_t gfn)\r\n{\r\nreturn gfn & ((1 << KVM_MMU_HASH_SHIFT) - 1);\r\n}\r\nstatic void mmu_page_add_parent_pte(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *sp, u64 *parent_pte)\r\n{\r\nif (!parent_pte)\r\nreturn;\r\npte_list_add(vcpu, parent_pte, &sp->parent_ptes);\r\n}\r\nstatic void mmu_page_remove_parent_pte(struct kvm_mmu_page *sp,\r\nu64 *parent_pte)\r\n{\r\npte_list_remove(parent_pte, &sp->parent_ptes);\r\n}\r\nstatic void drop_parent_pte(struct kvm_mmu_page *sp,\r\nu64 *parent_pte)\r\n{\r\nmmu_page_remove_parent_pte(sp, parent_pte);\r\nmmu_spte_clear_no_track(parent_pte);\r\n}\r\nstatic struct kvm_mmu_page *kvm_mmu_alloc_page(struct kvm_vcpu *vcpu,\r\nu64 *parent_pte, int direct)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nsp = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);\r\nsp->spt = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);\r\nif (!direct)\r\nsp->gfns = mmu_memory_cache_alloc(&vcpu->arch.mmu_page_cache);\r\nset_page_private(virt_to_page(sp->spt), (unsigned long)sp);\r\nlist_add(&sp->link, &vcpu->kvm->arch.active_mmu_pages);\r\nsp->parent_ptes = 0;\r\nmmu_page_add_parent_pte(vcpu, sp, parent_pte);\r\nkvm_mod_used_mmu_pages(vcpu->kvm, +1);\r\nreturn sp;\r\n}\r\nstatic void kvm_mmu_mark_parents_unsync(struct kvm_mmu_page *sp)\r\n{\r\npte_list_walk(&sp->parent_ptes, mark_unsync);\r\n}\r\nstatic void mark_unsync(u64 *spte)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nunsigned int index;\r\nsp = page_header(__pa(spte));\r\nindex = spte - sp->spt;\r\nif (__test_and_set_bit(index, sp->unsync_child_bitmap))\r\nreturn;\r\nif (sp->unsync_children++)\r\nreturn;\r\nkvm_mmu_mark_parents_unsync(sp);\r\n}\r\nstatic int nonpaging_sync_page(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *sp)\r\n{\r\nreturn 1;\r\n}\r\nstatic void nonpaging_invlpg(struct kvm_vcpu *vcpu, gva_t gva)\r\n{\r\n}\r\nstatic void nonpaging_update_pte(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *sp, u64 *spte,\r\nconst void *pte)\r\n{\r\nWARN_ON(1);\r\n}\r\nstatic int mmu_pages_add(struct kvm_mmu_pages *pvec, struct kvm_mmu_page *sp,\r\nint idx)\r\n{\r\nint i;\r\nif (sp->unsync)\r\nfor (i=0; i < pvec->nr; i++)\r\nif (pvec->page[i].sp == sp)\r\nreturn 0;\r\npvec->page[pvec->nr].sp = sp;\r\npvec->page[pvec->nr].idx = idx;\r\npvec->nr++;\r\nreturn (pvec->nr == KVM_PAGE_ARRAY_NR);\r\n}\r\nstatic int __mmu_unsync_walk(struct kvm_mmu_page *sp,\r\nstruct kvm_mmu_pages *pvec)\r\n{\r\nint i, ret, nr_unsync_leaf = 0;\r\nfor_each_set_bit(i, sp->unsync_child_bitmap, 512) {\r\nstruct kvm_mmu_page *child;\r\nu64 ent = sp->spt[i];\r\nif (!is_shadow_present_pte(ent) || is_large_pte(ent))\r\ngoto clear_child_bitmap;\r\nchild = page_header(ent & PT64_BASE_ADDR_MASK);\r\nif (child->unsync_children) {\r\nif (mmu_pages_add(pvec, child, i))\r\nreturn -ENOSPC;\r\nret = __mmu_unsync_walk(child, pvec);\r\nif (!ret)\r\ngoto clear_child_bitmap;\r\nelse if (ret > 0)\r\nnr_unsync_leaf += ret;\r\nelse\r\nreturn ret;\r\n} else if (child->unsync) {\r\nnr_unsync_leaf++;\r\nif (mmu_pages_add(pvec, child, i))\r\nreturn -ENOSPC;\r\n} else\r\ngoto clear_child_bitmap;\r\ncontinue;\r\nclear_child_bitmap:\r\n__clear_bit(i, sp->unsync_child_bitmap);\r\nsp->unsync_children--;\r\nWARN_ON((int)sp->unsync_children < 0);\r\n}\r\nreturn nr_unsync_leaf;\r\n}\r\nstatic int mmu_unsync_walk(struct kvm_mmu_page *sp,\r\nstruct kvm_mmu_pages *pvec)\r\n{\r\nif (!sp->unsync_children)\r\nreturn 0;\r\nmmu_pages_add(pvec, sp, 0);\r\nreturn __mmu_unsync_walk(sp, pvec);\r\n}\r\nstatic void kvm_unlink_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)\r\n{\r\nWARN_ON(!sp->unsync);\r\ntrace_kvm_mmu_sync_page(sp);\r\nsp->unsync = 0;\r\n--kvm->stat.mmu_unsync;\r\n}\r\nstatic int __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\r\nstruct list_head *invalid_list, bool clear_unsync)\r\n{\r\nif (sp->role.cr4_pae != !!is_pae(vcpu)) {\r\nkvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);\r\nreturn 1;\r\n}\r\nif (clear_unsync)\r\nkvm_unlink_unsync_page(vcpu->kvm, sp);\r\nif (vcpu->arch.mmu.sync_page(vcpu, sp)) {\r\nkvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);\r\nreturn 1;\r\n}\r\nkvm_mmu_flush_tlb(vcpu);\r\nreturn 0;\r\n}\r\nstatic int kvm_sync_page_transient(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *sp)\r\n{\r\nLIST_HEAD(invalid_list);\r\nint ret;\r\nret = __kvm_sync_page(vcpu, sp, &invalid_list, false);\r\nif (ret)\r\nkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\r\nreturn ret;\r\n}\r\nstatic void kvm_mmu_audit(struct kvm_vcpu *vcpu, int point) { }\r\nstatic void mmu_audit_disable(void) { }\r\nstatic int kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\r\nstruct list_head *invalid_list)\r\n{\r\nreturn __kvm_sync_page(vcpu, sp, invalid_list, true);\r\n}\r\nstatic void kvm_sync_pages(struct kvm_vcpu *vcpu, gfn_t gfn)\r\n{\r\nstruct kvm_mmu_page *s;\r\nLIST_HEAD(invalid_list);\r\nbool flush = false;\r\nfor_each_gfn_indirect_valid_sp(vcpu->kvm, s, gfn) {\r\nif (!s->unsync)\r\ncontinue;\r\nWARN_ON(s->role.level != PT_PAGE_TABLE_LEVEL);\r\nkvm_unlink_unsync_page(vcpu->kvm, s);\r\nif ((s->role.cr4_pae != !!is_pae(vcpu)) ||\r\n(vcpu->arch.mmu.sync_page(vcpu, s))) {\r\nkvm_mmu_prepare_zap_page(vcpu->kvm, s, &invalid_list);\r\ncontinue;\r\n}\r\nflush = true;\r\n}\r\nkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\r\nif (flush)\r\nkvm_mmu_flush_tlb(vcpu);\r\n}\r\nstatic int mmu_pages_next(struct kvm_mmu_pages *pvec,\r\nstruct mmu_page_path *parents,\r\nint i)\r\n{\r\nint n;\r\nfor (n = i+1; n < pvec->nr; n++) {\r\nstruct kvm_mmu_page *sp = pvec->page[n].sp;\r\nif (sp->role.level == PT_PAGE_TABLE_LEVEL) {\r\nparents->idx[0] = pvec->page[n].idx;\r\nreturn n;\r\n}\r\nparents->parent[sp->role.level-2] = sp;\r\nparents->idx[sp->role.level-1] = pvec->page[n].idx;\r\n}\r\nreturn n;\r\n}\r\nstatic void mmu_pages_clear_parents(struct mmu_page_path *parents)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nunsigned int level = 0;\r\ndo {\r\nunsigned int idx = parents->idx[level];\r\nsp = parents->parent[level];\r\nif (!sp)\r\nreturn;\r\n--sp->unsync_children;\r\nWARN_ON((int)sp->unsync_children < 0);\r\n__clear_bit(idx, sp->unsync_child_bitmap);\r\nlevel++;\r\n} while (level < PT64_ROOT_LEVEL-1 && !sp->unsync_children);\r\n}\r\nstatic void kvm_mmu_pages_init(struct kvm_mmu_page *parent,\r\nstruct mmu_page_path *parents,\r\nstruct kvm_mmu_pages *pvec)\r\n{\r\nparents->parent[parent->role.level-1] = NULL;\r\npvec->nr = 0;\r\n}\r\nstatic void mmu_sync_children(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *parent)\r\n{\r\nint i;\r\nstruct kvm_mmu_page *sp;\r\nstruct mmu_page_path parents;\r\nstruct kvm_mmu_pages pages;\r\nLIST_HEAD(invalid_list);\r\nkvm_mmu_pages_init(parent, &parents, &pages);\r\nwhile (mmu_unsync_walk(parent, &pages)) {\r\nbool protected = false;\r\nfor_each_sp(pages, sp, parents, i)\r\nprotected |= rmap_write_protect(vcpu->kvm, sp->gfn);\r\nif (protected)\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\nfor_each_sp(pages, sp, parents, i) {\r\nkvm_sync_page(vcpu, sp, &invalid_list);\r\nmmu_pages_clear_parents(&parents);\r\n}\r\nkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\r\ncond_resched_lock(&vcpu->kvm->mmu_lock);\r\nkvm_mmu_pages_init(parent, &parents, &pages);\r\n}\r\n}\r\nstatic void init_shadow_page_table(struct kvm_mmu_page *sp)\r\n{\r\nint i;\r\nfor (i = 0; i < PT64_ENT_PER_PAGE; ++i)\r\nsp->spt[i] = 0ull;\r\n}\r\nstatic void __clear_sp_write_flooding_count(struct kvm_mmu_page *sp)\r\n{\r\nsp->write_flooding_count = 0;\r\n}\r\nstatic void clear_sp_write_flooding_count(u64 *spte)\r\n{\r\nstruct kvm_mmu_page *sp = page_header(__pa(spte));\r\n__clear_sp_write_flooding_count(sp);\r\n}\r\nstatic bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)\r\n{\r\nreturn unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);\r\n}\r\nstatic struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,\r\ngfn_t gfn,\r\ngva_t gaddr,\r\nunsigned level,\r\nint direct,\r\nunsigned access,\r\nu64 *parent_pte)\r\n{\r\nunion kvm_mmu_page_role role;\r\nunsigned quadrant;\r\nstruct kvm_mmu_page *sp;\r\nbool need_sync = false;\r\nrole = vcpu->arch.mmu.base_role;\r\nrole.level = level;\r\nrole.direct = direct;\r\nif (role.direct)\r\nrole.cr4_pae = 0;\r\nrole.access = access;\r\nif (!vcpu->arch.mmu.direct_map\r\n&& vcpu->arch.mmu.root_level <= PT32_ROOT_LEVEL) {\r\nquadrant = gaddr >> (PAGE_SHIFT + (PT64_PT_BITS * level));\r\nquadrant &= (1 << ((PT32_PT_BITS - PT64_PT_BITS) * level)) - 1;\r\nrole.quadrant = quadrant;\r\n}\r\nfor_each_gfn_sp(vcpu->kvm, sp, gfn) {\r\nif (is_obsolete_sp(vcpu->kvm, sp))\r\ncontinue;\r\nif (!need_sync && sp->unsync)\r\nneed_sync = true;\r\nif (sp->role.word != role.word)\r\ncontinue;\r\nif (sp->unsync && kvm_sync_page_transient(vcpu, sp))\r\nbreak;\r\nmmu_page_add_parent_pte(vcpu, sp, parent_pte);\r\nif (sp->unsync_children) {\r\nkvm_make_request(KVM_REQ_MMU_SYNC, vcpu);\r\nkvm_mmu_mark_parents_unsync(sp);\r\n} else if (sp->unsync)\r\nkvm_mmu_mark_parents_unsync(sp);\r\n__clear_sp_write_flooding_count(sp);\r\ntrace_kvm_mmu_get_page(sp, false);\r\nreturn sp;\r\n}\r\n++vcpu->kvm->stat.mmu_cache_miss;\r\nsp = kvm_mmu_alloc_page(vcpu, parent_pte, direct);\r\nif (!sp)\r\nreturn sp;\r\nsp->gfn = gfn;\r\nsp->role = role;\r\nhlist_add_head(&sp->hash_link,\r\n&vcpu->kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)]);\r\nif (!direct) {\r\nif (rmap_write_protect(vcpu->kvm, gfn))\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\nif (level > PT_PAGE_TABLE_LEVEL && need_sync)\r\nkvm_sync_pages(vcpu, gfn);\r\naccount_shadowed(vcpu->kvm, gfn);\r\n}\r\nsp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;\r\ninit_shadow_page_table(sp);\r\ntrace_kvm_mmu_get_page(sp, true);\r\nreturn sp;\r\n}\r\nstatic void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,\r\nstruct kvm_vcpu *vcpu, u64 addr)\r\n{\r\niterator->addr = addr;\r\niterator->shadow_addr = vcpu->arch.mmu.root_hpa;\r\niterator->level = vcpu->arch.mmu.shadow_root_level;\r\nif (iterator->level == PT64_ROOT_LEVEL &&\r\nvcpu->arch.mmu.root_level < PT64_ROOT_LEVEL &&\r\n!vcpu->arch.mmu.direct_map)\r\n--iterator->level;\r\nif (iterator->level == PT32E_ROOT_LEVEL) {\r\niterator->shadow_addr\r\n= vcpu->arch.mmu.pae_root[(addr >> 30) & 3];\r\niterator->shadow_addr &= PT64_BASE_ADDR_MASK;\r\n--iterator->level;\r\nif (!iterator->shadow_addr)\r\niterator->level = 0;\r\n}\r\n}\r\nstatic bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)\r\n{\r\nif (iterator->level < PT_PAGE_TABLE_LEVEL)\r\nreturn false;\r\niterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);\r\niterator->sptep = ((u64 *)__va(iterator->shadow_addr)) + iterator->index;\r\nreturn true;\r\n}\r\nstatic void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,\r\nu64 spte)\r\n{\r\nif (is_last_spte(spte, iterator->level)) {\r\niterator->level = 0;\r\nreturn;\r\n}\r\niterator->shadow_addr = spte & PT64_BASE_ADDR_MASK;\r\n--iterator->level;\r\n}\r\nstatic void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)\r\n{\r\nreturn __shadow_walk_next(iterator, *iterator->sptep);\r\n}\r\nstatic void link_shadow_page(u64 *sptep, struct kvm_mmu_page *sp, bool accessed)\r\n{\r\nu64 spte;\r\nBUILD_BUG_ON(VMX_EPT_READABLE_MASK != PT_PRESENT_MASK ||\r\nVMX_EPT_WRITABLE_MASK != PT_WRITABLE_MASK);\r\nspte = __pa(sp->spt) | PT_PRESENT_MASK | PT_WRITABLE_MASK |\r\nshadow_user_mask | shadow_x_mask;\r\nif (accessed)\r\nspte |= shadow_accessed_mask;\r\nmmu_spte_set(sptep, spte);\r\n}\r\nstatic void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,\r\nunsigned direct_access)\r\n{\r\nif (is_shadow_present_pte(*sptep) && !is_large_pte(*sptep)) {\r\nstruct kvm_mmu_page *child;\r\nchild = page_header(*sptep & PT64_BASE_ADDR_MASK);\r\nif (child->role.access == direct_access)\r\nreturn;\r\ndrop_parent_pte(child, sptep);\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\n}\r\n}\r\nstatic bool mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,\r\nu64 *spte)\r\n{\r\nu64 pte;\r\nstruct kvm_mmu_page *child;\r\npte = *spte;\r\nif (is_shadow_present_pte(pte)) {\r\nif (is_last_spte(pte, sp->role.level)) {\r\ndrop_spte(kvm, spte);\r\nif (is_large_pte(pte))\r\n--kvm->stat.lpages;\r\n} else {\r\nchild = page_header(pte & PT64_BASE_ADDR_MASK);\r\ndrop_parent_pte(child, spte);\r\n}\r\nreturn true;\r\n}\r\nif (is_mmio_spte(pte))\r\nmmu_spte_clear_no_track(spte);\r\nreturn false;\r\n}\r\nstatic void kvm_mmu_page_unlink_children(struct kvm *kvm,\r\nstruct kvm_mmu_page *sp)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < PT64_ENT_PER_PAGE; ++i)\r\nmmu_page_zap_pte(kvm, sp, sp->spt + i);\r\n}\r\nstatic void kvm_mmu_put_page(struct kvm_mmu_page *sp, u64 *parent_pte)\r\n{\r\nmmu_page_remove_parent_pte(sp, parent_pte);\r\n}\r\nstatic void kvm_mmu_unlink_parents(struct kvm *kvm, struct kvm_mmu_page *sp)\r\n{\r\nu64 *sptep;\r\nstruct rmap_iterator iter;\r\nwhile ((sptep = rmap_get_first(sp->parent_ptes, &iter)))\r\ndrop_parent_pte(sp, sptep);\r\n}\r\nstatic int mmu_zap_unsync_children(struct kvm *kvm,\r\nstruct kvm_mmu_page *parent,\r\nstruct list_head *invalid_list)\r\n{\r\nint i, zapped = 0;\r\nstruct mmu_page_path parents;\r\nstruct kvm_mmu_pages pages;\r\nif (parent->role.level == PT_PAGE_TABLE_LEVEL)\r\nreturn 0;\r\nkvm_mmu_pages_init(parent, &parents, &pages);\r\nwhile (mmu_unsync_walk(parent, &pages)) {\r\nstruct kvm_mmu_page *sp;\r\nfor_each_sp(pages, sp, parents, i) {\r\nkvm_mmu_prepare_zap_page(kvm, sp, invalid_list);\r\nmmu_pages_clear_parents(&parents);\r\nzapped++;\r\n}\r\nkvm_mmu_pages_init(parent, &parents, &pages);\r\n}\r\nreturn zapped;\r\n}\r\nstatic int kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,\r\nstruct list_head *invalid_list)\r\n{\r\nint ret;\r\ntrace_kvm_mmu_prepare_zap_page(sp);\r\n++kvm->stat.mmu_shadow_zapped;\r\nret = mmu_zap_unsync_children(kvm, sp, invalid_list);\r\nkvm_mmu_page_unlink_children(kvm, sp);\r\nkvm_mmu_unlink_parents(kvm, sp);\r\nif (!sp->role.invalid && !sp->role.direct)\r\nunaccount_shadowed(kvm, sp->gfn);\r\nif (sp->unsync)\r\nkvm_unlink_unsync_page(kvm, sp);\r\nif (!sp->root_count) {\r\nret++;\r\nlist_move(&sp->link, invalid_list);\r\nkvm_mod_used_mmu_pages(kvm, -1);\r\n} else {\r\nlist_move(&sp->link, &kvm->arch.active_mmu_pages);\r\nif (!sp->role.invalid && !is_obsolete_sp(kvm, sp))\r\nkvm_reload_remote_mmus(kvm);\r\n}\r\nsp->role.invalid = 1;\r\nreturn ret;\r\n}\r\nstatic void kvm_mmu_commit_zap_page(struct kvm *kvm,\r\nstruct list_head *invalid_list)\r\n{\r\nstruct kvm_mmu_page *sp, *nsp;\r\nif (list_empty(invalid_list))\r\nreturn;\r\nsmp_mb();\r\nkvm_flush_remote_tlbs(kvm);\r\nlist_for_each_entry_safe(sp, nsp, invalid_list, link) {\r\nWARN_ON(!sp->role.invalid || sp->root_count);\r\nkvm_mmu_free_page(sp);\r\n}\r\n}\r\nstatic bool prepare_zap_oldest_mmu_page(struct kvm *kvm,\r\nstruct list_head *invalid_list)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nif (list_empty(&kvm->arch.active_mmu_pages))\r\nreturn false;\r\nsp = list_entry(kvm->arch.active_mmu_pages.prev,\r\nstruct kvm_mmu_page, link);\r\nkvm_mmu_prepare_zap_page(kvm, sp, invalid_list);\r\nreturn true;\r\n}\r\nvoid kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int goal_nr_mmu_pages)\r\n{\r\nLIST_HEAD(invalid_list);\r\nspin_lock(&kvm->mmu_lock);\r\nif (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {\r\nwhile (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages)\r\nif (!prepare_zap_oldest_mmu_page(kvm, &invalid_list))\r\nbreak;\r\nkvm_mmu_commit_zap_page(kvm, &invalid_list);\r\ngoal_nr_mmu_pages = kvm->arch.n_used_mmu_pages;\r\n}\r\nkvm->arch.n_max_mmu_pages = goal_nr_mmu_pages;\r\nspin_unlock(&kvm->mmu_lock);\r\n}\r\nint kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nLIST_HEAD(invalid_list);\r\nint r;\r\npgprintk("%s: looking for gfn %llx\n", __func__, gfn);\r\nr = 0;\r\nspin_lock(&kvm->mmu_lock);\r\nfor_each_gfn_indirect_valid_sp(kvm, sp, gfn) {\r\npgprintk("%s: gfn %llx role %x\n", __func__, gfn,\r\nsp->role.word);\r\nr = 1;\r\nkvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);\r\n}\r\nkvm_mmu_commit_zap_page(kvm, &invalid_list);\r\nspin_unlock(&kvm->mmu_lock);\r\nreturn r;\r\n}\r\nstatic int get_mtrr_type(struct mtrr_state_type *mtrr_state,\r\nu64 start, u64 end)\r\n{\r\nint i;\r\nu64 base, mask;\r\nu8 prev_match, curr_match;\r\nint num_var_ranges = KVM_NR_VAR_MTRR;\r\nif (!mtrr_state->enabled)\r\nreturn 0xFF;\r\nend--;\r\nif (mtrr_state->have_fixed && (start < 0x100000)) {\r\nint idx;\r\nif (start < 0x80000) {\r\nidx = 0;\r\nidx += (start >> 16);\r\nreturn mtrr_state->fixed_ranges[idx];\r\n} else if (start < 0xC0000) {\r\nidx = 1 * 8;\r\nidx += ((start - 0x80000) >> 14);\r\nreturn mtrr_state->fixed_ranges[idx];\r\n} else if (start < 0x1000000) {\r\nidx = 3 * 8;\r\nidx += ((start - 0xC0000) >> 12);\r\nreturn mtrr_state->fixed_ranges[idx];\r\n}\r\n}\r\nif (!(mtrr_state->enabled & 2))\r\nreturn mtrr_state->def_type;\r\nprev_match = 0xFF;\r\nfor (i = 0; i < num_var_ranges; ++i) {\r\nunsigned short start_state, end_state;\r\nif (!(mtrr_state->var_ranges[i].mask_lo & (1 << 11)))\r\ncontinue;\r\nbase = (((u64)mtrr_state->var_ranges[i].base_hi) << 32) +\r\n(mtrr_state->var_ranges[i].base_lo & PAGE_MASK);\r\nmask = (((u64)mtrr_state->var_ranges[i].mask_hi) << 32) +\r\n(mtrr_state->var_ranges[i].mask_lo & PAGE_MASK);\r\nstart_state = ((start & mask) == (base & mask));\r\nend_state = ((end & mask) == (base & mask));\r\nif (start_state != end_state)\r\nreturn 0xFE;\r\nif ((start & mask) != (base & mask))\r\ncontinue;\r\ncurr_match = mtrr_state->var_ranges[i].base_lo & 0xff;\r\nif (prev_match == 0xFF) {\r\nprev_match = curr_match;\r\ncontinue;\r\n}\r\nif (prev_match == MTRR_TYPE_UNCACHABLE ||\r\ncurr_match == MTRR_TYPE_UNCACHABLE)\r\nreturn MTRR_TYPE_UNCACHABLE;\r\nif ((prev_match == MTRR_TYPE_WRBACK &&\r\ncurr_match == MTRR_TYPE_WRTHROUGH) ||\r\n(prev_match == MTRR_TYPE_WRTHROUGH &&\r\ncurr_match == MTRR_TYPE_WRBACK)) {\r\nprev_match = MTRR_TYPE_WRTHROUGH;\r\ncurr_match = MTRR_TYPE_WRTHROUGH;\r\n}\r\nif (prev_match != curr_match)\r\nreturn MTRR_TYPE_UNCACHABLE;\r\n}\r\nif (prev_match != 0xFF)\r\nreturn prev_match;\r\nreturn mtrr_state->def_type;\r\n}\r\nu8 kvm_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn)\r\n{\r\nu8 mtrr;\r\nmtrr = get_mtrr_type(&vcpu->arch.mtrr_state, gfn << PAGE_SHIFT,\r\n(gfn << PAGE_SHIFT) + PAGE_SIZE);\r\nif (mtrr == 0xfe || mtrr == 0xff)\r\nmtrr = MTRR_TYPE_WRBACK;\r\nreturn mtrr;\r\n}\r\nstatic void __kvm_unsync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)\r\n{\r\ntrace_kvm_mmu_unsync_page(sp);\r\n++vcpu->kvm->stat.mmu_unsync;\r\nsp->unsync = 1;\r\nkvm_mmu_mark_parents_unsync(sp);\r\n}\r\nstatic void kvm_unsync_pages(struct kvm_vcpu *vcpu, gfn_t gfn)\r\n{\r\nstruct kvm_mmu_page *s;\r\nfor_each_gfn_indirect_valid_sp(vcpu->kvm, s, gfn) {\r\nif (s->unsync)\r\ncontinue;\r\nWARN_ON(s->role.level != PT_PAGE_TABLE_LEVEL);\r\n__kvm_unsync_page(vcpu, s);\r\n}\r\n}\r\nstatic int mmu_need_write_protect(struct kvm_vcpu *vcpu, gfn_t gfn,\r\nbool can_unsync)\r\n{\r\nstruct kvm_mmu_page *s;\r\nbool need_unsync = false;\r\nfor_each_gfn_indirect_valid_sp(vcpu->kvm, s, gfn) {\r\nif (!can_unsync)\r\nreturn 1;\r\nif (s->role.level != PT_PAGE_TABLE_LEVEL)\r\nreturn 1;\r\nif (!s->unsync)\r\nneed_unsync = true;\r\n}\r\nif (need_unsync)\r\nkvm_unsync_pages(vcpu, gfn);\r\nreturn 0;\r\n}\r\nstatic int set_spte(struct kvm_vcpu *vcpu, u64 *sptep,\r\nunsigned pte_access, int level,\r\ngfn_t gfn, pfn_t pfn, bool speculative,\r\nbool can_unsync, bool host_writable)\r\n{\r\nu64 spte;\r\nint ret = 0;\r\nif (set_mmio_spte(vcpu->kvm, sptep, gfn, pfn, pte_access))\r\nreturn 0;\r\nspte = PT_PRESENT_MASK;\r\nif (!speculative)\r\nspte |= shadow_accessed_mask;\r\nif (pte_access & ACC_EXEC_MASK)\r\nspte |= shadow_x_mask;\r\nelse\r\nspte |= shadow_nx_mask;\r\nif (pte_access & ACC_USER_MASK)\r\nspte |= shadow_user_mask;\r\nif (level > PT_PAGE_TABLE_LEVEL)\r\nspte |= PT_PAGE_SIZE_MASK;\r\nif (tdp_enabled)\r\nspte |= kvm_x86_ops->get_mt_mask(vcpu, gfn,\r\nkvm_is_mmio_pfn(pfn));\r\nif (host_writable)\r\nspte |= SPTE_HOST_WRITEABLE;\r\nelse\r\npte_access &= ~ACC_WRITE_MASK;\r\nspte |= (u64)pfn << PAGE_SHIFT;\r\nif (pte_access & ACC_WRITE_MASK) {\r\nif (level > PT_PAGE_TABLE_LEVEL &&\r\nhas_wrprotected_page(vcpu->kvm, gfn, level))\r\ngoto done;\r\nspte |= PT_WRITABLE_MASK | SPTE_MMU_WRITEABLE;\r\nif (!can_unsync && is_writable_pte(*sptep))\r\ngoto set_pte;\r\nif (mmu_need_write_protect(vcpu, gfn, can_unsync)) {\r\npgprintk("%s: found shadow page for %llx, marking ro\n",\r\n__func__, gfn);\r\nret = 1;\r\npte_access &= ~ACC_WRITE_MASK;\r\nspte &= ~(PT_WRITABLE_MASK | SPTE_MMU_WRITEABLE);\r\n}\r\n}\r\nif (pte_access & ACC_WRITE_MASK)\r\nmark_page_dirty(vcpu->kvm, gfn);\r\nset_pte:\r\nif (mmu_spte_update(sptep, spte))\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,\r\nunsigned pte_access, int write_fault, int *emulate,\r\nint level, gfn_t gfn, pfn_t pfn, bool speculative,\r\nbool host_writable)\r\n{\r\nint was_rmapped = 0;\r\nint rmap_count;\r\npgprintk("%s: spte %llx write_fault %d gfn %llx\n", __func__,\r\n*sptep, write_fault, gfn);\r\nif (is_rmap_spte(*sptep)) {\r\nif (level > PT_PAGE_TABLE_LEVEL &&\r\n!is_large_pte(*sptep)) {\r\nstruct kvm_mmu_page *child;\r\nu64 pte = *sptep;\r\nchild = page_header(pte & PT64_BASE_ADDR_MASK);\r\ndrop_parent_pte(child, sptep);\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\n} else if (pfn != spte_to_pfn(*sptep)) {\r\npgprintk("hfn old %llx new %llx\n",\r\nspte_to_pfn(*sptep), pfn);\r\ndrop_spte(vcpu->kvm, sptep);\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\n} else\r\nwas_rmapped = 1;\r\n}\r\nif (set_spte(vcpu, sptep, pte_access, level, gfn, pfn, speculative,\r\ntrue, host_writable)) {\r\nif (write_fault)\r\n*emulate = 1;\r\nkvm_mmu_flush_tlb(vcpu);\r\n}\r\nif (unlikely(is_mmio_spte(*sptep) && emulate))\r\n*emulate = 1;\r\npgprintk("%s: setting spte %llx\n", __func__, *sptep);\r\npgprintk("instantiating %s PTE (%s) at %llx (%llx) addr %p\n",\r\nis_large_pte(*sptep)? "2MB" : "4kB",\r\n*sptep & PT_PRESENT_MASK ?"RW":"R", gfn,\r\n*sptep, sptep);\r\nif (!was_rmapped && is_large_pte(*sptep))\r\n++vcpu->kvm->stat.lpages;\r\nif (is_shadow_present_pte(*sptep)) {\r\nif (!was_rmapped) {\r\nrmap_count = rmap_add(vcpu, sptep, gfn);\r\nif (rmap_count > RMAP_RECYCLE_THRESHOLD)\r\nrmap_recycle(vcpu, sptep, gfn);\r\n}\r\n}\r\nkvm_release_pfn_clean(pfn);\r\n}\r\nstatic pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,\r\nbool no_dirty_log)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nslot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, no_dirty_log);\r\nif (!slot)\r\nreturn KVM_PFN_ERR_FAULT;\r\nreturn gfn_to_pfn_memslot_atomic(slot, gfn);\r\n}\r\nstatic int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *sp,\r\nu64 *start, u64 *end)\r\n{\r\nstruct page *pages[PTE_PREFETCH_NUM];\r\nunsigned access = sp->role.access;\r\nint i, ret;\r\ngfn_t gfn;\r\ngfn = kvm_mmu_page_get_gfn(sp, start - sp->spt);\r\nif (!gfn_to_memslot_dirty_bitmap(vcpu, gfn, access & ACC_WRITE_MASK))\r\nreturn -1;\r\nret = gfn_to_page_many_atomic(vcpu->kvm, gfn, pages, end - start);\r\nif (ret <= 0)\r\nreturn -1;\r\nfor (i = 0; i < ret; i++, gfn++, start++)\r\nmmu_set_spte(vcpu, start, access, 0, NULL,\r\nsp->role.level, gfn, page_to_pfn(pages[i]),\r\ntrue, true);\r\nreturn 0;\r\n}\r\nstatic void __direct_pte_prefetch(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *sp, u64 *sptep)\r\n{\r\nu64 *spte, *start = NULL;\r\nint i;\r\nWARN_ON(!sp->role.direct);\r\ni = (sptep - sp->spt) & ~(PTE_PREFETCH_NUM - 1);\r\nspte = sp->spt + i;\r\nfor (i = 0; i < PTE_PREFETCH_NUM; i++, spte++) {\r\nif (is_shadow_present_pte(*spte) || spte == sptep) {\r\nif (!start)\r\ncontinue;\r\nif (direct_pte_prefetch_many(vcpu, sp, start, spte) < 0)\r\nbreak;\r\nstart = NULL;\r\n} else if (!start)\r\nstart = spte;\r\n}\r\n}\r\nstatic void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nif (!shadow_accessed_mask)\r\nreturn;\r\nsp = page_header(__pa(sptep));\r\nif (sp->role.level > PT_PAGE_TABLE_LEVEL)\r\nreturn;\r\n__direct_pte_prefetch(vcpu, sp, sptep);\r\n}\r\nstatic int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write,\r\nint map_writable, int level, gfn_t gfn, pfn_t pfn,\r\nbool prefault)\r\n{\r\nstruct kvm_shadow_walk_iterator iterator;\r\nstruct kvm_mmu_page *sp;\r\nint emulate = 0;\r\ngfn_t pseudo_gfn;\r\nif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\r\nreturn 0;\r\nfor_each_shadow_entry(vcpu, (u64)gfn << PAGE_SHIFT, iterator) {\r\nif (iterator.level == level) {\r\nmmu_set_spte(vcpu, iterator.sptep, ACC_ALL,\r\nwrite, &emulate, level, gfn, pfn,\r\nprefault, map_writable);\r\ndirect_pte_prefetch(vcpu, iterator.sptep);\r\n++vcpu->stat.pf_fixed;\r\nbreak;\r\n}\r\ndrop_large_spte(vcpu, iterator.sptep);\r\nif (!is_shadow_present_pte(*iterator.sptep)) {\r\nu64 base_addr = iterator.addr;\r\nbase_addr &= PT64_LVL_ADDR_MASK(iterator.level);\r\npseudo_gfn = base_addr >> PAGE_SHIFT;\r\nsp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr,\r\niterator.level - 1,\r\n1, ACC_ALL, iterator.sptep);\r\nlink_shadow_page(iterator.sptep, sp, true);\r\n}\r\n}\r\nreturn emulate;\r\n}\r\nstatic void kvm_send_hwpoison_signal(unsigned long address, struct task_struct *tsk)\r\n{\r\nsiginfo_t info;\r\ninfo.si_signo = SIGBUS;\r\ninfo.si_errno = 0;\r\ninfo.si_code = BUS_MCEERR_AR;\r\ninfo.si_addr = (void __user *)address;\r\ninfo.si_addr_lsb = PAGE_SHIFT;\r\nsend_sig_info(SIGBUS, &info, tsk);\r\n}\r\nstatic int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, pfn_t pfn)\r\n{\r\nif (pfn == KVM_PFN_ERR_RO_FAULT)\r\nreturn 1;\r\nif (pfn == KVM_PFN_ERR_HWPOISON) {\r\nkvm_send_hwpoison_signal(gfn_to_hva(vcpu->kvm, gfn), current);\r\nreturn 0;\r\n}\r\nreturn -EFAULT;\r\n}\r\nstatic void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,\r\ngfn_t *gfnp, pfn_t *pfnp, int *levelp)\r\n{\r\npfn_t pfn = *pfnp;\r\ngfn_t gfn = *gfnp;\r\nint level = *levelp;\r\nif (!is_error_noslot_pfn(pfn) && !kvm_is_mmio_pfn(pfn) &&\r\nlevel == PT_PAGE_TABLE_LEVEL &&\r\nPageTransCompound(pfn_to_page(pfn)) &&\r\n!has_wrprotected_page(vcpu->kvm, gfn, PT_DIRECTORY_LEVEL)) {\r\nunsigned long mask;\r\n*levelp = level = PT_DIRECTORY_LEVEL;\r\nmask = KVM_PAGES_PER_HPAGE(level) - 1;\r\nVM_BUG_ON((gfn & mask) != (pfn & mask));\r\nif (pfn & mask) {\r\ngfn &= ~mask;\r\n*gfnp = gfn;\r\nkvm_release_pfn_clean(pfn);\r\npfn &= ~mask;\r\nkvm_get_pfn(pfn);\r\n*pfnp = pfn;\r\n}\r\n}\r\n}\r\nstatic bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,\r\npfn_t pfn, unsigned access, int *ret_val)\r\n{\r\nbool ret = true;\r\nif (unlikely(is_error_pfn(pfn))) {\r\n*ret_val = kvm_handle_bad_page(vcpu, gfn, pfn);\r\ngoto exit;\r\n}\r\nif (unlikely(is_noslot_pfn(pfn)))\r\nvcpu_cache_mmio_info(vcpu, gva, gfn, access);\r\nret = false;\r\nexit:\r\nreturn ret;\r\n}\r\nstatic bool page_fault_can_be_fast(u32 error_code)\r\n{\r\nif (unlikely(error_code & PFERR_RSVD_MASK))\r\nreturn false;\r\nif (!(error_code & PFERR_PRESENT_MASK) ||\r\n!(error_code & PFERR_WRITE_MASK))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool\r\nfast_pf_fix_direct_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\r\nu64 *sptep, u64 spte)\r\n{\r\ngfn_t gfn;\r\nWARN_ON(!sp->role.direct);\r\ngfn = kvm_mmu_page_get_gfn(sp, sptep - sp->spt);\r\nif (cmpxchg64(sptep, spte, spte | PT_WRITABLE_MASK) == spte)\r\nmark_page_dirty(vcpu->kvm, gfn);\r\nreturn true;\r\n}\r\nstatic bool fast_page_fault(struct kvm_vcpu *vcpu, gva_t gva, int level,\r\nu32 error_code)\r\n{\r\nstruct kvm_shadow_walk_iterator iterator;\r\nstruct kvm_mmu_page *sp;\r\nbool ret = false;\r\nu64 spte = 0ull;\r\nif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\r\nreturn false;\r\nif (!page_fault_can_be_fast(error_code))\r\nreturn false;\r\nwalk_shadow_page_lockless_begin(vcpu);\r\nfor_each_shadow_entry_lockless(vcpu, gva, iterator, spte)\r\nif (!is_shadow_present_pte(spte) || iterator.level < level)\r\nbreak;\r\nif (!is_rmap_spte(spte)) {\r\nret = true;\r\ngoto exit;\r\n}\r\nsp = page_header(__pa(iterator.sptep));\r\nif (!is_last_spte(spte, sp->role.level))\r\ngoto exit;\r\nif (is_writable_pte(spte)) {\r\nret = true;\r\ngoto exit;\r\n}\r\nif (!spte_is_locklessly_modifiable(spte))\r\ngoto exit;\r\nif (sp->role.level > PT_PAGE_TABLE_LEVEL)\r\ngoto exit;\r\nret = fast_pf_fix_direct_spte(vcpu, sp, iterator.sptep, spte);\r\nexit:\r\ntrace_fast_page_fault(vcpu, gva, error_code, iterator.sptep,\r\nspte, ret);\r\nwalk_shadow_page_lockless_end(vcpu);\r\nreturn ret;\r\n}\r\nstatic int nonpaging_map(struct kvm_vcpu *vcpu, gva_t v, u32 error_code,\r\ngfn_t gfn, bool prefault)\r\n{\r\nint r;\r\nint level;\r\nint force_pt_level;\r\npfn_t pfn;\r\nunsigned long mmu_seq;\r\nbool map_writable, write = error_code & PFERR_WRITE_MASK;\r\nforce_pt_level = mapping_level_dirty_bitmap(vcpu, gfn);\r\nif (likely(!force_pt_level)) {\r\nlevel = mapping_level(vcpu, gfn);\r\nif (level > PT_DIRECTORY_LEVEL)\r\nlevel = PT_DIRECTORY_LEVEL;\r\ngfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);\r\n} else\r\nlevel = PT_PAGE_TABLE_LEVEL;\r\nif (fast_page_fault(vcpu, v, level, error_code))\r\nreturn 0;\r\nmmu_seq = vcpu->kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\nif (try_async_pf(vcpu, prefault, gfn, v, &pfn, write, &map_writable))\r\nreturn 0;\r\nif (handle_abnormal_pfn(vcpu, v, gfn, pfn, ACC_ALL, &r))\r\nreturn r;\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nif (mmu_notifier_retry(vcpu->kvm, mmu_seq))\r\ngoto out_unlock;\r\nmake_mmu_pages_available(vcpu);\r\nif (likely(!force_pt_level))\r\ntransparent_hugepage_adjust(vcpu, &gfn, &pfn, &level);\r\nr = __direct_map(vcpu, v, write, map_writable, level, gfn, pfn,\r\nprefault);\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nreturn r;\r\nout_unlock:\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nkvm_release_pfn_clean(pfn);\r\nreturn 0;\r\n}\r\nstatic void mmu_free_roots(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nstruct kvm_mmu_page *sp;\r\nLIST_HEAD(invalid_list);\r\nif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\r\nreturn;\r\nif (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL &&\r\n(vcpu->arch.mmu.root_level == PT64_ROOT_LEVEL ||\r\nvcpu->arch.mmu.direct_map)) {\r\nhpa_t root = vcpu->arch.mmu.root_hpa;\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nsp = page_header(root);\r\n--sp->root_count;\r\nif (!sp->root_count && sp->role.invalid) {\r\nkvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);\r\nkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\r\n}\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nvcpu->arch.mmu.root_hpa = INVALID_PAGE;\r\nreturn;\r\n}\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nfor (i = 0; i < 4; ++i) {\r\nhpa_t root = vcpu->arch.mmu.pae_root[i];\r\nif (root) {\r\nroot &= PT64_BASE_ADDR_MASK;\r\nsp = page_header(root);\r\n--sp->root_count;\r\nif (!sp->root_count && sp->role.invalid)\r\nkvm_mmu_prepare_zap_page(vcpu->kvm, sp,\r\n&invalid_list);\r\n}\r\nvcpu->arch.mmu.pae_root[i] = INVALID_PAGE;\r\n}\r\nkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nvcpu->arch.mmu.root_hpa = INVALID_PAGE;\r\n}\r\nstatic int mmu_check_root(struct kvm_vcpu *vcpu, gfn_t root_gfn)\r\n{\r\nint ret = 0;\r\nif (!kvm_is_visible_gfn(vcpu->kvm, root_gfn)) {\r\nkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nunsigned i;\r\nif (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL) {\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nmake_mmu_pages_available(vcpu);\r\nsp = kvm_mmu_get_page(vcpu, 0, 0, PT64_ROOT_LEVEL,\r\n1, ACC_ALL, NULL);\r\n++sp->root_count;\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nvcpu->arch.mmu.root_hpa = __pa(sp->spt);\r\n} else if (vcpu->arch.mmu.shadow_root_level == PT32E_ROOT_LEVEL) {\r\nfor (i = 0; i < 4; ++i) {\r\nhpa_t root = vcpu->arch.mmu.pae_root[i];\r\nASSERT(!VALID_PAGE(root));\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nmake_mmu_pages_available(vcpu);\r\nsp = kvm_mmu_get_page(vcpu, i << (30 - PAGE_SHIFT),\r\ni << 30,\r\nPT32_ROOT_LEVEL, 1, ACC_ALL,\r\nNULL);\r\nroot = __pa(sp->spt);\r\n++sp->root_count;\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nvcpu->arch.mmu.pae_root[i] = root | PT_PRESENT_MASK;\r\n}\r\nvcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.pae_root);\r\n} else\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_mmu_page *sp;\r\nu64 pdptr, pm_mask;\r\ngfn_t root_gfn;\r\nint i;\r\nroot_gfn = vcpu->arch.mmu.get_cr3(vcpu) >> PAGE_SHIFT;\r\nif (mmu_check_root(vcpu, root_gfn))\r\nreturn 1;\r\nif (vcpu->arch.mmu.root_level == PT64_ROOT_LEVEL) {\r\nhpa_t root = vcpu->arch.mmu.root_hpa;\r\nASSERT(!VALID_PAGE(root));\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nmake_mmu_pages_available(vcpu);\r\nsp = kvm_mmu_get_page(vcpu, root_gfn, 0, PT64_ROOT_LEVEL,\r\n0, ACC_ALL, NULL);\r\nroot = __pa(sp->spt);\r\n++sp->root_count;\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nvcpu->arch.mmu.root_hpa = root;\r\nreturn 0;\r\n}\r\npm_mask = PT_PRESENT_MASK;\r\nif (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL)\r\npm_mask |= PT_ACCESSED_MASK | PT_WRITABLE_MASK | PT_USER_MASK;\r\nfor (i = 0; i < 4; ++i) {\r\nhpa_t root = vcpu->arch.mmu.pae_root[i];\r\nASSERT(!VALID_PAGE(root));\r\nif (vcpu->arch.mmu.root_level == PT32E_ROOT_LEVEL) {\r\npdptr = vcpu->arch.mmu.get_pdptr(vcpu, i);\r\nif (!is_present_gpte(pdptr)) {\r\nvcpu->arch.mmu.pae_root[i] = 0;\r\ncontinue;\r\n}\r\nroot_gfn = pdptr >> PAGE_SHIFT;\r\nif (mmu_check_root(vcpu, root_gfn))\r\nreturn 1;\r\n}\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nmake_mmu_pages_available(vcpu);\r\nsp = kvm_mmu_get_page(vcpu, root_gfn, i << 30,\r\nPT32_ROOT_LEVEL, 0,\r\nACC_ALL, NULL);\r\nroot = __pa(sp->spt);\r\n++sp->root_count;\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nvcpu->arch.mmu.pae_root[i] = root | pm_mask;\r\n}\r\nvcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.pae_root);\r\nif (vcpu->arch.mmu.shadow_root_level == PT64_ROOT_LEVEL) {\r\nif (vcpu->arch.mmu.lm_root == NULL) {\r\nu64 *lm_root;\r\nlm_root = (void*)get_zeroed_page(GFP_KERNEL);\r\nif (lm_root == NULL)\r\nreturn 1;\r\nlm_root[0] = __pa(vcpu->arch.mmu.pae_root) | pm_mask;\r\nvcpu->arch.mmu.lm_root = lm_root;\r\n}\r\nvcpu->arch.mmu.root_hpa = __pa(vcpu->arch.mmu.lm_root);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mmu_alloc_roots(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.mmu.direct_map)\r\nreturn mmu_alloc_direct_roots(vcpu);\r\nelse\r\nreturn mmu_alloc_shadow_roots(vcpu);\r\n}\r\nstatic void mmu_sync_roots(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nstruct kvm_mmu_page *sp;\r\nif (vcpu->arch.mmu.direct_map)\r\nreturn;\r\nif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\r\nreturn;\r\nvcpu_clear_mmio_info(vcpu, ~0ul);\r\nkvm_mmu_audit(vcpu, AUDIT_PRE_SYNC);\r\nif (vcpu->arch.mmu.root_level == PT64_ROOT_LEVEL) {\r\nhpa_t root = vcpu->arch.mmu.root_hpa;\r\nsp = page_header(root);\r\nmmu_sync_children(vcpu, sp);\r\nkvm_mmu_audit(vcpu, AUDIT_POST_SYNC);\r\nreturn;\r\n}\r\nfor (i = 0; i < 4; ++i) {\r\nhpa_t root = vcpu->arch.mmu.pae_root[i];\r\nif (root && VALID_PAGE(root)) {\r\nroot &= PT64_BASE_ADDR_MASK;\r\nsp = page_header(root);\r\nmmu_sync_children(vcpu, sp);\r\n}\r\n}\r\nkvm_mmu_audit(vcpu, AUDIT_POST_SYNC);\r\n}\r\nvoid kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)\r\n{\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nmmu_sync_roots(vcpu);\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\n}\r\nstatic gpa_t nonpaging_gva_to_gpa(struct kvm_vcpu *vcpu, gva_t vaddr,\r\nu32 access, struct x86_exception *exception)\r\n{\r\nif (exception)\r\nexception->error_code = 0;\r\nreturn vaddr;\r\n}\r\nstatic gpa_t nonpaging_gva_to_gpa_nested(struct kvm_vcpu *vcpu, gva_t vaddr,\r\nu32 access,\r\nstruct x86_exception *exception)\r\n{\r\nif (exception)\r\nexception->error_code = 0;\r\nreturn vcpu->arch.nested_mmu.translate_gpa(vcpu, vaddr, access);\r\n}\r\nstatic bool quickly_check_mmio_pf(struct kvm_vcpu *vcpu, u64 addr, bool direct)\r\n{\r\nif (direct)\r\nreturn vcpu_match_mmio_gpa(vcpu, addr);\r\nreturn vcpu_match_mmio_gva(vcpu, addr);\r\n}\r\nstatic bool check_direct_spte_mmio_pf(u64 spte)\r\n{\r\nreturn __check_direct_spte_mmio_pf(spte);\r\n}\r\nstatic u64 walk_shadow_page_get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr)\r\n{\r\nstruct kvm_shadow_walk_iterator iterator;\r\nu64 spte = 0ull;\r\nif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\r\nreturn spte;\r\nwalk_shadow_page_lockless_begin(vcpu);\r\nfor_each_shadow_entry_lockless(vcpu, addr, iterator, spte)\r\nif (!is_shadow_present_pte(spte))\r\nbreak;\r\nwalk_shadow_page_lockless_end(vcpu);\r\nreturn spte;\r\n}\r\nint handle_mmio_page_fault_common(struct kvm_vcpu *vcpu, u64 addr, bool direct)\r\n{\r\nu64 spte;\r\nif (quickly_check_mmio_pf(vcpu, addr, direct))\r\nreturn RET_MMIO_PF_EMULATE;\r\nspte = walk_shadow_page_get_mmio_spte(vcpu, addr);\r\nif (is_mmio_spte(spte)) {\r\ngfn_t gfn = get_mmio_spte_gfn(spte);\r\nunsigned access = get_mmio_spte_access(spte);\r\nif (!check_mmio_spte(vcpu->kvm, spte))\r\nreturn RET_MMIO_PF_INVALID;\r\nif (direct)\r\naddr = 0;\r\ntrace_handle_mmio_page_fault(addr, gfn, access);\r\nvcpu_cache_mmio_info(vcpu, addr, gfn, access);\r\nreturn RET_MMIO_PF_EMULATE;\r\n}\r\nif (direct && !check_direct_spte_mmio_pf(spte))\r\nreturn RET_MMIO_PF_BUG;\r\nreturn RET_MMIO_PF_RETRY;\r\n}\r\nstatic int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr,\r\nu32 error_code, bool direct)\r\n{\r\nint ret;\r\nret = handle_mmio_page_fault_common(vcpu, addr, direct);\r\nWARN_ON(ret == RET_MMIO_PF_BUG);\r\nreturn ret;\r\n}\r\nstatic int nonpaging_page_fault(struct kvm_vcpu *vcpu, gva_t gva,\r\nu32 error_code, bool prefault)\r\n{\r\ngfn_t gfn;\r\nint r;\r\npgprintk("%s: gva %lx error %x\n", __func__, gva, error_code);\r\nif (unlikely(error_code & PFERR_RSVD_MASK)) {\r\nr = handle_mmio_page_fault(vcpu, gva, error_code, true);\r\nif (likely(r != RET_MMIO_PF_INVALID))\r\nreturn r;\r\n}\r\nr = mmu_topup_memory_caches(vcpu);\r\nif (r)\r\nreturn r;\r\nASSERT(vcpu);\r\nASSERT(VALID_PAGE(vcpu->arch.mmu.root_hpa));\r\ngfn = gva >> PAGE_SHIFT;\r\nreturn nonpaging_map(vcpu, gva & PAGE_MASK,\r\nerror_code, gfn, prefault);\r\n}\r\nstatic int kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn)\r\n{\r\nstruct kvm_arch_async_pf arch;\r\narch.token = (vcpu->arch.apf.id++ << 12) | vcpu->vcpu_id;\r\narch.gfn = gfn;\r\narch.direct_map = vcpu->arch.mmu.direct_map;\r\narch.cr3 = vcpu->arch.mmu.get_cr3(vcpu);\r\nreturn kvm_setup_async_pf(vcpu, gva, gfn_to_hva(vcpu->kvm, gfn), &arch);\r\n}\r\nstatic bool can_do_async_pf(struct kvm_vcpu *vcpu)\r\n{\r\nif (unlikely(!irqchip_in_kernel(vcpu->kvm) ||\r\nkvm_event_needs_reinjection(vcpu)))\r\nreturn false;\r\nreturn kvm_x86_ops->interrupt_allowed(vcpu);\r\n}\r\nstatic bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,\r\ngva_t gva, pfn_t *pfn, bool write, bool *writable)\r\n{\r\nbool async;\r\n*pfn = gfn_to_pfn_async(vcpu->kvm, gfn, &async, write, writable);\r\nif (!async)\r\nreturn false;\r\nif (!prefault && can_do_async_pf(vcpu)) {\r\ntrace_kvm_try_async_get_page(gva, gfn);\r\nif (kvm_find_async_pf_gfn(vcpu, gfn)) {\r\ntrace_kvm_async_pf_doublefault(gva, gfn);\r\nkvm_make_request(KVM_REQ_APF_HALT, vcpu);\r\nreturn true;\r\n} else if (kvm_arch_setup_async_pf(vcpu, gva, gfn))\r\nreturn true;\r\n}\r\n*pfn = gfn_to_pfn_prot(vcpu->kvm, gfn, write, writable);\r\nreturn false;\r\n}\r\nstatic int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,\r\nbool prefault)\r\n{\r\npfn_t pfn;\r\nint r;\r\nint level;\r\nint force_pt_level;\r\ngfn_t gfn = gpa >> PAGE_SHIFT;\r\nunsigned long mmu_seq;\r\nint write = error_code & PFERR_WRITE_MASK;\r\nbool map_writable;\r\nASSERT(vcpu);\r\nASSERT(VALID_PAGE(vcpu->arch.mmu.root_hpa));\r\nif (unlikely(error_code & PFERR_RSVD_MASK)) {\r\nr = handle_mmio_page_fault(vcpu, gpa, error_code, true);\r\nif (likely(r != RET_MMIO_PF_INVALID))\r\nreturn r;\r\n}\r\nr = mmu_topup_memory_caches(vcpu);\r\nif (r)\r\nreturn r;\r\nforce_pt_level = mapping_level_dirty_bitmap(vcpu, gfn);\r\nif (likely(!force_pt_level)) {\r\nlevel = mapping_level(vcpu, gfn);\r\ngfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);\r\n} else\r\nlevel = PT_PAGE_TABLE_LEVEL;\r\nif (fast_page_fault(vcpu, gpa, level, error_code))\r\nreturn 0;\r\nmmu_seq = vcpu->kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\nif (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))\r\nreturn 0;\r\nif (handle_abnormal_pfn(vcpu, 0, gfn, pfn, ACC_ALL, &r))\r\nreturn r;\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\nif (mmu_notifier_retry(vcpu->kvm, mmu_seq))\r\ngoto out_unlock;\r\nmake_mmu_pages_available(vcpu);\r\nif (likely(!force_pt_level))\r\ntransparent_hugepage_adjust(vcpu, &gfn, &pfn, &level);\r\nr = __direct_map(vcpu, gpa, write, map_writable,\r\nlevel, gfn, pfn, prefault);\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nreturn r;\r\nout_unlock:\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\nkvm_release_pfn_clean(pfn);\r\nreturn 0;\r\n}\r\nstatic void nonpaging_init_context(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *context)\r\n{\r\ncontext->page_fault = nonpaging_page_fault;\r\ncontext->gva_to_gpa = nonpaging_gva_to_gpa;\r\ncontext->sync_page = nonpaging_sync_page;\r\ncontext->invlpg = nonpaging_invlpg;\r\ncontext->update_pte = nonpaging_update_pte;\r\ncontext->root_level = 0;\r\ncontext->shadow_root_level = PT32E_ROOT_LEVEL;\r\ncontext->root_hpa = INVALID_PAGE;\r\ncontext->direct_map = true;\r\ncontext->nx = false;\r\n}\r\nvoid kvm_mmu_flush_tlb(struct kvm_vcpu *vcpu)\r\n{\r\n++vcpu->stat.tlb_flush;\r\nkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\r\n}\r\nvoid kvm_mmu_new_cr3(struct kvm_vcpu *vcpu)\r\n{\r\nmmu_free_roots(vcpu);\r\n}\r\nstatic unsigned long get_cr3(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_read_cr3(vcpu);\r\n}\r\nstatic void inject_page_fault(struct kvm_vcpu *vcpu,\r\nstruct x86_exception *fault)\r\n{\r\nvcpu->arch.mmu.inject_page_fault(vcpu, fault);\r\n}\r\nstatic bool sync_mmio_spte(struct kvm *kvm, u64 *sptep, gfn_t gfn,\r\nunsigned access, int *nr_present)\r\n{\r\nif (unlikely(is_mmio_spte(*sptep))) {\r\nif (gfn != get_mmio_spte_gfn(*sptep)) {\r\nmmu_spte_clear_no_track(sptep);\r\nreturn true;\r\n}\r\n(*nr_present)++;\r\nmark_mmio_spte(kvm, sptep, gfn, access);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic inline bool is_last_gpte(struct kvm_mmu *mmu, unsigned level, unsigned gpte)\r\n{\r\nunsigned index;\r\nindex = level - 1;\r\nindex |= (gpte & PT_PAGE_SIZE_MASK) >> (PT_PAGE_SIZE_SHIFT - 2);\r\nreturn mmu->last_pte_bitmap & (1 << index);\r\n}\r\nstatic void reset_rsvds_bits_mask(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *context)\r\n{\r\nint maxphyaddr = cpuid_maxphyaddr(vcpu);\r\nu64 exb_bit_rsvd = 0;\r\nu64 gbpages_bit_rsvd = 0;\r\ncontext->bad_mt_xwr = 0;\r\nif (!context->nx)\r\nexb_bit_rsvd = rsvd_bits(63, 63);\r\nif (!guest_cpuid_has_gbpages(vcpu))\r\ngbpages_bit_rsvd = rsvd_bits(7, 7);\r\nswitch (context->root_level) {\r\ncase PT32_ROOT_LEVEL:\r\ncontext->rsvd_bits_mask[0][1] = 0;\r\ncontext->rsvd_bits_mask[0][0] = 0;\r\ncontext->rsvd_bits_mask[1][0] = context->rsvd_bits_mask[0][0];\r\nif (!is_pse(vcpu)) {\r\ncontext->rsvd_bits_mask[1][1] = 0;\r\nbreak;\r\n}\r\nif (is_cpuid_PSE36())\r\ncontext->rsvd_bits_mask[1][1] = rsvd_bits(17, 21);\r\nelse\r\ncontext->rsvd_bits_mask[1][1] = rsvd_bits(13, 21);\r\nbreak;\r\ncase PT32E_ROOT_LEVEL:\r\ncontext->rsvd_bits_mask[0][2] =\r\nrsvd_bits(maxphyaddr, 63) |\r\nrsvd_bits(5, 8) | rsvd_bits(1, 2);\r\ncontext->rsvd_bits_mask[0][1] = exb_bit_rsvd |\r\nrsvd_bits(maxphyaddr, 62);\r\ncontext->rsvd_bits_mask[0][0] = exb_bit_rsvd |\r\nrsvd_bits(maxphyaddr, 62);\r\ncontext->rsvd_bits_mask[1][1] = exb_bit_rsvd |\r\nrsvd_bits(maxphyaddr, 62) |\r\nrsvd_bits(13, 20);\r\ncontext->rsvd_bits_mask[1][0] = context->rsvd_bits_mask[0][0];\r\nbreak;\r\ncase PT64_ROOT_LEVEL:\r\ncontext->rsvd_bits_mask[0][3] = exb_bit_rsvd |\r\nrsvd_bits(maxphyaddr, 51) | rsvd_bits(7, 7);\r\ncontext->rsvd_bits_mask[0][2] = exb_bit_rsvd |\r\ngbpages_bit_rsvd | rsvd_bits(maxphyaddr, 51);\r\ncontext->rsvd_bits_mask[0][1] = exb_bit_rsvd |\r\nrsvd_bits(maxphyaddr, 51);\r\ncontext->rsvd_bits_mask[0][0] = exb_bit_rsvd |\r\nrsvd_bits(maxphyaddr, 51);\r\ncontext->rsvd_bits_mask[1][3] = context->rsvd_bits_mask[0][3];\r\ncontext->rsvd_bits_mask[1][2] = exb_bit_rsvd |\r\ngbpages_bit_rsvd | rsvd_bits(maxphyaddr, 51) |\r\nrsvd_bits(13, 29);\r\ncontext->rsvd_bits_mask[1][1] = exb_bit_rsvd |\r\nrsvd_bits(maxphyaddr, 51) |\r\nrsvd_bits(13, 20);\r\ncontext->rsvd_bits_mask[1][0] = context->rsvd_bits_mask[0][0];\r\nbreak;\r\n}\r\n}\r\nstatic void reset_rsvds_bits_mask_ept(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *context, bool execonly)\r\n{\r\nint maxphyaddr = cpuid_maxphyaddr(vcpu);\r\nint pte;\r\ncontext->rsvd_bits_mask[0][3] =\r\nrsvd_bits(maxphyaddr, 51) | rsvd_bits(3, 7);\r\ncontext->rsvd_bits_mask[0][2] =\r\nrsvd_bits(maxphyaddr, 51) | rsvd_bits(3, 6);\r\ncontext->rsvd_bits_mask[0][1] =\r\nrsvd_bits(maxphyaddr, 51) | rsvd_bits(3, 6);\r\ncontext->rsvd_bits_mask[0][0] = rsvd_bits(maxphyaddr, 51);\r\ncontext->rsvd_bits_mask[1][3] = context->rsvd_bits_mask[0][3];\r\ncontext->rsvd_bits_mask[1][2] =\r\nrsvd_bits(maxphyaddr, 51) | rsvd_bits(12, 29);\r\ncontext->rsvd_bits_mask[1][1] =\r\nrsvd_bits(maxphyaddr, 51) | rsvd_bits(12, 20);\r\ncontext->rsvd_bits_mask[1][0] = context->rsvd_bits_mask[0][0];\r\nfor (pte = 0; pte < 64; pte++) {\r\nint rwx_bits = pte & 7;\r\nint mt = pte >> 3;\r\nif (mt == 0x2 || mt == 0x3 || mt == 0x7 ||\r\nrwx_bits == 0x2 || rwx_bits == 0x6 ||\r\n(rwx_bits == 0x4 && !execonly))\r\ncontext->bad_mt_xwr |= (1ull << pte);\r\n}\r\n}\r\nvoid update_permission_bitmask(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *mmu, bool ept)\r\n{\r\nunsigned bit, byte, pfec;\r\nu8 map;\r\nbool fault, x, w, u, wf, uf, ff, smapf, cr4_smap, cr4_smep, smap = 0;\r\ncr4_smep = kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);\r\ncr4_smap = kvm_read_cr4_bits(vcpu, X86_CR4_SMAP);\r\nfor (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {\r\npfec = byte << 1;\r\nmap = 0;\r\nwf = pfec & PFERR_WRITE_MASK;\r\nuf = pfec & PFERR_USER_MASK;\r\nff = pfec & PFERR_FETCH_MASK;\r\nsmapf = !(pfec & PFERR_RSVD_MASK);\r\nfor (bit = 0; bit < 8; ++bit) {\r\nx = bit & ACC_EXEC_MASK;\r\nw = bit & ACC_WRITE_MASK;\r\nu = bit & ACC_USER_MASK;\r\nif (!ept) {\r\nx |= !mmu->nx;\r\nw |= !is_write_protection(vcpu) && !uf;\r\nx &= !(cr4_smep && u && !uf);\r\nsmap = cr4_smap && u && !uf && !ff;\r\n} else\r\nu = 1;\r\nfault = (ff && !x) || (uf && !u) || (wf && !w) ||\r\n(smapf && smap);\r\nmap |= fault << bit;\r\n}\r\nmmu->permissions[byte] = map;\r\n}\r\n}\r\nstatic void update_last_pte_bitmap(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)\r\n{\r\nu8 map;\r\nunsigned level, root_level = mmu->root_level;\r\nconst unsigned ps_set_index = 1 << 2;\r\nif (root_level == PT32E_ROOT_LEVEL)\r\n--root_level;\r\nmap = 1 | (1 << ps_set_index);\r\nfor (level = PT_DIRECTORY_LEVEL; level <= root_level; ++level) {\r\nif (level <= PT_PDPE_LEVEL\r\n&& (mmu->root_level >= PT32E_ROOT_LEVEL || is_pse(vcpu)))\r\nmap |= 1 << (ps_set_index | (level - 1));\r\n}\r\nmmu->last_pte_bitmap = map;\r\n}\r\nstatic void paging64_init_context_common(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *context,\r\nint level)\r\n{\r\ncontext->nx = is_nx(vcpu);\r\ncontext->root_level = level;\r\nreset_rsvds_bits_mask(vcpu, context);\r\nupdate_permission_bitmask(vcpu, context, false);\r\nupdate_last_pte_bitmap(vcpu, context);\r\nASSERT(is_pae(vcpu));\r\ncontext->page_fault = paging64_page_fault;\r\ncontext->gva_to_gpa = paging64_gva_to_gpa;\r\ncontext->sync_page = paging64_sync_page;\r\ncontext->invlpg = paging64_invlpg;\r\ncontext->update_pte = paging64_update_pte;\r\ncontext->shadow_root_level = level;\r\ncontext->root_hpa = INVALID_PAGE;\r\ncontext->direct_map = false;\r\n}\r\nstatic void paging64_init_context(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *context)\r\n{\r\npaging64_init_context_common(vcpu, context, PT64_ROOT_LEVEL);\r\n}\r\nstatic void paging32_init_context(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *context)\r\n{\r\ncontext->nx = false;\r\ncontext->root_level = PT32_ROOT_LEVEL;\r\nreset_rsvds_bits_mask(vcpu, context);\r\nupdate_permission_bitmask(vcpu, context, false);\r\nupdate_last_pte_bitmap(vcpu, context);\r\ncontext->page_fault = paging32_page_fault;\r\ncontext->gva_to_gpa = paging32_gva_to_gpa;\r\ncontext->sync_page = paging32_sync_page;\r\ncontext->invlpg = paging32_invlpg;\r\ncontext->update_pte = paging32_update_pte;\r\ncontext->shadow_root_level = PT32E_ROOT_LEVEL;\r\ncontext->root_hpa = INVALID_PAGE;\r\ncontext->direct_map = false;\r\n}\r\nstatic void paging32E_init_context(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu *context)\r\n{\r\npaging64_init_context_common(vcpu, context, PT32E_ROOT_LEVEL);\r\n}\r\nstatic void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_mmu *context = vcpu->arch.walk_mmu;\r\ncontext->base_role.word = 0;\r\ncontext->page_fault = tdp_page_fault;\r\ncontext->sync_page = nonpaging_sync_page;\r\ncontext->invlpg = nonpaging_invlpg;\r\ncontext->update_pte = nonpaging_update_pte;\r\ncontext->shadow_root_level = kvm_x86_ops->get_tdp_level();\r\ncontext->root_hpa = INVALID_PAGE;\r\ncontext->direct_map = true;\r\ncontext->set_cr3 = kvm_x86_ops->set_tdp_cr3;\r\ncontext->get_cr3 = get_cr3;\r\ncontext->get_pdptr = kvm_pdptr_read;\r\ncontext->inject_page_fault = kvm_inject_page_fault;\r\nif (!is_paging(vcpu)) {\r\ncontext->nx = false;\r\ncontext->gva_to_gpa = nonpaging_gva_to_gpa;\r\ncontext->root_level = 0;\r\n} else if (is_long_mode(vcpu)) {\r\ncontext->nx = is_nx(vcpu);\r\ncontext->root_level = PT64_ROOT_LEVEL;\r\nreset_rsvds_bits_mask(vcpu, context);\r\ncontext->gva_to_gpa = paging64_gva_to_gpa;\r\n} else if (is_pae(vcpu)) {\r\ncontext->nx = is_nx(vcpu);\r\ncontext->root_level = PT32E_ROOT_LEVEL;\r\nreset_rsvds_bits_mask(vcpu, context);\r\ncontext->gva_to_gpa = paging64_gva_to_gpa;\r\n} else {\r\ncontext->nx = false;\r\ncontext->root_level = PT32_ROOT_LEVEL;\r\nreset_rsvds_bits_mask(vcpu, context);\r\ncontext->gva_to_gpa = paging32_gva_to_gpa;\r\n}\r\nupdate_permission_bitmask(vcpu, context, false);\r\nupdate_last_pte_bitmap(vcpu, context);\r\n}\r\nvoid kvm_init_shadow_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context)\r\n{\r\nbool smep = kvm_read_cr4_bits(vcpu, X86_CR4_SMEP);\r\nASSERT(vcpu);\r\nASSERT(!VALID_PAGE(vcpu->arch.mmu.root_hpa));\r\nif (!is_paging(vcpu))\r\nnonpaging_init_context(vcpu, context);\r\nelse if (is_long_mode(vcpu))\r\npaging64_init_context(vcpu, context);\r\nelse if (is_pae(vcpu))\r\npaging32E_init_context(vcpu, context);\r\nelse\r\npaging32_init_context(vcpu, context);\r\nvcpu->arch.mmu.base_role.nxe = is_nx(vcpu);\r\nvcpu->arch.mmu.base_role.cr4_pae = !!is_pae(vcpu);\r\nvcpu->arch.mmu.base_role.cr0_wp = is_write_protection(vcpu);\r\nvcpu->arch.mmu.base_role.smep_andnot_wp\r\n= smep && !is_write_protection(vcpu);\r\n}\r\nvoid kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *context,\r\nbool execonly)\r\n{\r\nASSERT(vcpu);\r\nASSERT(!VALID_PAGE(vcpu->arch.mmu.root_hpa));\r\ncontext->shadow_root_level = kvm_x86_ops->get_tdp_level();\r\ncontext->nx = true;\r\ncontext->page_fault = ept_page_fault;\r\ncontext->gva_to_gpa = ept_gva_to_gpa;\r\ncontext->sync_page = ept_sync_page;\r\ncontext->invlpg = ept_invlpg;\r\ncontext->update_pte = ept_update_pte;\r\ncontext->root_level = context->shadow_root_level;\r\ncontext->root_hpa = INVALID_PAGE;\r\ncontext->direct_map = false;\r\nupdate_permission_bitmask(vcpu, context, true);\r\nreset_rsvds_bits_mask_ept(vcpu, context, execonly);\r\n}\r\nstatic void init_kvm_softmmu(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_init_shadow_mmu(vcpu, vcpu->arch.walk_mmu);\r\nvcpu->arch.walk_mmu->set_cr3 = kvm_x86_ops->set_cr3;\r\nvcpu->arch.walk_mmu->get_cr3 = get_cr3;\r\nvcpu->arch.walk_mmu->get_pdptr = kvm_pdptr_read;\r\nvcpu->arch.walk_mmu->inject_page_fault = kvm_inject_page_fault;\r\n}\r\nstatic void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_mmu *g_context = &vcpu->arch.nested_mmu;\r\ng_context->get_cr3 = get_cr3;\r\ng_context->get_pdptr = kvm_pdptr_read;\r\ng_context->inject_page_fault = kvm_inject_page_fault;\r\nif (!is_paging(vcpu)) {\r\ng_context->nx = false;\r\ng_context->root_level = 0;\r\ng_context->gva_to_gpa = nonpaging_gva_to_gpa_nested;\r\n} else if (is_long_mode(vcpu)) {\r\ng_context->nx = is_nx(vcpu);\r\ng_context->root_level = PT64_ROOT_LEVEL;\r\nreset_rsvds_bits_mask(vcpu, g_context);\r\ng_context->gva_to_gpa = paging64_gva_to_gpa_nested;\r\n} else if (is_pae(vcpu)) {\r\ng_context->nx = is_nx(vcpu);\r\ng_context->root_level = PT32E_ROOT_LEVEL;\r\nreset_rsvds_bits_mask(vcpu, g_context);\r\ng_context->gva_to_gpa = paging64_gva_to_gpa_nested;\r\n} else {\r\ng_context->nx = false;\r\ng_context->root_level = PT32_ROOT_LEVEL;\r\nreset_rsvds_bits_mask(vcpu, g_context);\r\ng_context->gva_to_gpa = paging32_gva_to_gpa_nested;\r\n}\r\nupdate_permission_bitmask(vcpu, g_context, false);\r\nupdate_last_pte_bitmap(vcpu, g_context);\r\n}\r\nstatic void init_kvm_mmu(struct kvm_vcpu *vcpu)\r\n{\r\nif (mmu_is_nested(vcpu))\r\nreturn init_kvm_nested_mmu(vcpu);\r\nelse if (tdp_enabled)\r\nreturn init_kvm_tdp_mmu(vcpu);\r\nelse\r\nreturn init_kvm_softmmu(vcpu);\r\n}\r\nvoid kvm_mmu_reset_context(struct kvm_vcpu *vcpu)\r\n{\r\nASSERT(vcpu);\r\nkvm_mmu_unload(vcpu);\r\ninit_kvm_mmu(vcpu);\r\n}\r\nint kvm_mmu_load(struct kvm_vcpu *vcpu)\r\n{\r\nint r;\r\nr = mmu_topup_memory_caches(vcpu);\r\nif (r)\r\ngoto out;\r\nr = mmu_alloc_roots(vcpu);\r\nkvm_mmu_sync_roots(vcpu);\r\nif (r)\r\ngoto out;\r\nvcpu->arch.mmu.set_cr3(vcpu, vcpu->arch.mmu.root_hpa);\r\nout:\r\nreturn r;\r\n}\r\nvoid kvm_mmu_unload(struct kvm_vcpu *vcpu)\r\n{\r\nmmu_free_roots(vcpu);\r\nWARN_ON(VALID_PAGE(vcpu->arch.mmu.root_hpa));\r\n}\r\nstatic void mmu_pte_write_new_pte(struct kvm_vcpu *vcpu,\r\nstruct kvm_mmu_page *sp, u64 *spte,\r\nconst void *new)\r\n{\r\nif (sp->role.level != PT_PAGE_TABLE_LEVEL) {\r\n++vcpu->kvm->stat.mmu_pde_zapped;\r\nreturn;\r\n}\r\n++vcpu->kvm->stat.mmu_pte_updated;\r\nvcpu->arch.mmu.update_pte(vcpu, sp, spte, new);\r\n}\r\nstatic bool need_remote_flush(u64 old, u64 new)\r\n{\r\nif (!is_shadow_present_pte(old))\r\nreturn false;\r\nif (!is_shadow_present_pte(new))\r\nreturn true;\r\nif ((old ^ new) & PT64_BASE_ADDR_MASK)\r\nreturn true;\r\nold ^= shadow_nx_mask;\r\nnew ^= shadow_nx_mask;\r\nreturn (old & ~new & PT64_PERM_MASK) != 0;\r\n}\r\nstatic void mmu_pte_write_flush_tlb(struct kvm_vcpu *vcpu, bool zap_page,\r\nbool remote_flush, bool local_flush)\r\n{\r\nif (zap_page)\r\nreturn;\r\nif (remote_flush)\r\nkvm_flush_remote_tlbs(vcpu->kvm);\r\nelse if (local_flush)\r\nkvm_mmu_flush_tlb(vcpu);\r\n}\r\nstatic u64 mmu_pte_write_fetch_gpte(struct kvm_vcpu *vcpu, gpa_t *gpa,\r\nconst u8 *new, int *bytes)\r\n{\r\nu64 gentry;\r\nint r;\r\nif (is_pae(vcpu) && *bytes == 4) {\r\n*gpa &= ~(gpa_t)7;\r\n*bytes = 8;\r\nr = kvm_read_guest(vcpu->kvm, *gpa, &gentry, 8);\r\nif (r)\r\ngentry = 0;\r\nnew = (const u8 *)&gentry;\r\n}\r\nswitch (*bytes) {\r\ncase 4:\r\ngentry = *(const u32 *)new;\r\nbreak;\r\ncase 8:\r\ngentry = *(const u64 *)new;\r\nbreak;\r\ndefault:\r\ngentry = 0;\r\nbreak;\r\n}\r\nreturn gentry;\r\n}\r\nstatic bool detect_write_flooding(struct kvm_mmu_page *sp)\r\n{\r\nif (sp->role.level == PT_PAGE_TABLE_LEVEL)\r\nreturn false;\r\nreturn ++sp->write_flooding_count >= 3;\r\n}\r\nstatic bool detect_write_misaligned(struct kvm_mmu_page *sp, gpa_t gpa,\r\nint bytes)\r\n{\r\nunsigned offset, pte_size, misaligned;\r\npgprintk("misaligned: gpa %llx bytes %d role %x\n",\r\ngpa, bytes, sp->role.word);\r\noffset = offset_in_page(gpa);\r\npte_size = sp->role.cr4_pae ? 8 : 4;\r\nif (!(offset & (pte_size - 1)) && bytes == 1)\r\nreturn false;\r\nmisaligned = (offset ^ (offset + bytes - 1)) & ~(pte_size - 1);\r\nmisaligned |= bytes < 4;\r\nreturn misaligned;\r\n}\r\nstatic u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)\r\n{\r\nunsigned page_offset, quadrant;\r\nu64 *spte;\r\nint level;\r\npage_offset = offset_in_page(gpa);\r\nlevel = sp->role.level;\r\n*nspte = 1;\r\nif (!sp->role.cr4_pae) {\r\npage_offset <<= 1;\r\nif (level == PT32_ROOT_LEVEL) {\r\npage_offset &= ~7;\r\npage_offset <<= 1;\r\n*nspte = 2;\r\n}\r\nquadrant = page_offset >> PAGE_SHIFT;\r\npage_offset &= ~PAGE_MASK;\r\nif (quadrant != sp->role.quadrant)\r\nreturn NULL;\r\n}\r\nspte = &sp->spt[page_offset / sizeof(*spte)];\r\nreturn spte;\r\n}\r\nvoid kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,\r\nconst u8 *new, int bytes)\r\n{\r\ngfn_t gfn = gpa >> PAGE_SHIFT;\r\nunion kvm_mmu_page_role mask = { .word = 0 };\r\nstruct kvm_mmu_page *sp;\r\nLIST_HEAD(invalid_list);\r\nu64 entry, gentry, *spte;\r\nint npte;\r\nbool remote_flush, local_flush, zap_page;\r\nif (!ACCESS_ONCE(vcpu->kvm->arch.indirect_shadow_pages))\r\nreturn;\r\nzap_page = remote_flush = local_flush = false;\r\npgprintk("%s: gpa %llx bytes %d\n", __func__, gpa, bytes);\r\ngentry = mmu_pte_write_fetch_gpte(vcpu, &gpa, new, &bytes);\r\nmmu_topup_memory_caches(vcpu);\r\nspin_lock(&vcpu->kvm->mmu_lock);\r\n++vcpu->kvm->stat.mmu_pte_write;\r\nkvm_mmu_audit(vcpu, AUDIT_PRE_PTE_WRITE);\r\nmask.cr0_wp = mask.cr4_pae = mask.nxe = 1;\r\nfor_each_gfn_indirect_valid_sp(vcpu->kvm, sp, gfn) {\r\nif (detect_write_misaligned(sp, gpa, bytes) ||\r\ndetect_write_flooding(sp)) {\r\nzap_page |= !!kvm_mmu_prepare_zap_page(vcpu->kvm, sp,\r\n&invalid_list);\r\n++vcpu->kvm->stat.mmu_flooded;\r\ncontinue;\r\n}\r\nspte = get_written_sptes(sp, gpa, &npte);\r\nif (!spte)\r\ncontinue;\r\nlocal_flush = true;\r\nwhile (npte--) {\r\nentry = *spte;\r\nmmu_page_zap_pte(vcpu->kvm, sp, spte);\r\nif (gentry &&\r\n!((sp->role.word ^ vcpu->arch.mmu.base_role.word)\r\n& mask.word) && rmap_can_add(vcpu))\r\nmmu_pte_write_new_pte(vcpu, sp, spte, &gentry);\r\nif (need_remote_flush(entry, *spte))\r\nremote_flush = true;\r\n++spte;\r\n}\r\n}\r\nmmu_pte_write_flush_tlb(vcpu, zap_page, remote_flush, local_flush);\r\nkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\r\nkvm_mmu_audit(vcpu, AUDIT_POST_PTE_WRITE);\r\nspin_unlock(&vcpu->kvm->mmu_lock);\r\n}\r\nint kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva)\r\n{\r\ngpa_t gpa;\r\nint r;\r\nif (vcpu->arch.mmu.direct_map)\r\nreturn 0;\r\ngpa = kvm_mmu_gva_to_gpa_read(vcpu, gva, NULL);\r\nr = kvm_mmu_unprotect_page(vcpu->kvm, gpa >> PAGE_SHIFT);\r\nreturn r;\r\n}\r\nstatic void make_mmu_pages_available(struct kvm_vcpu *vcpu)\r\n{\r\nLIST_HEAD(invalid_list);\r\nif (likely(kvm_mmu_available_pages(vcpu->kvm) >= KVM_MIN_FREE_MMU_PAGES))\r\nreturn;\r\nwhile (kvm_mmu_available_pages(vcpu->kvm) < KVM_REFILL_PAGES) {\r\nif (!prepare_zap_oldest_mmu_page(vcpu->kvm, &invalid_list))\r\nbreak;\r\n++vcpu->kvm->stat.mmu_recycled;\r\n}\r\nkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\r\n}\r\nstatic bool is_mmio_page_fault(struct kvm_vcpu *vcpu, gva_t addr)\r\n{\r\nif (vcpu->arch.mmu.direct_map || mmu_is_nested(vcpu))\r\nreturn vcpu_match_mmio_gpa(vcpu, addr);\r\nreturn vcpu_match_mmio_gva(vcpu, addr);\r\n}\r\nint kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t cr2, u32 error_code,\r\nvoid *insn, int insn_len)\r\n{\r\nint r, emulation_type = EMULTYPE_RETRY;\r\nenum emulation_result er;\r\nr = vcpu->arch.mmu.page_fault(vcpu, cr2, error_code, false);\r\nif (r < 0)\r\ngoto out;\r\nif (!r) {\r\nr = 1;\r\ngoto out;\r\n}\r\nif (is_mmio_page_fault(vcpu, cr2))\r\nemulation_type = 0;\r\ner = x86_emulate_instruction(vcpu, cr2, emulation_type, insn, insn_len);\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nreturn 1;\r\ncase EMULATE_USER_EXIT:\r\n++vcpu->stat.mmio_exits;\r\ncase EMULATE_FAIL:\r\nreturn 0;\r\ndefault:\r\nBUG();\r\n}\r\nout:\r\nreturn r;\r\n}\r\nvoid kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)\r\n{\r\nvcpu->arch.mmu.invlpg(vcpu, gva);\r\nkvm_mmu_flush_tlb(vcpu);\r\n++vcpu->stat.invlpg;\r\n}\r\nvoid kvm_enable_tdp(void)\r\n{\r\ntdp_enabled = true;\r\n}\r\nvoid kvm_disable_tdp(void)\r\n{\r\ntdp_enabled = false;\r\n}\r\nstatic void free_mmu_pages(struct kvm_vcpu *vcpu)\r\n{\r\nfree_page((unsigned long)vcpu->arch.mmu.pae_root);\r\nif (vcpu->arch.mmu.lm_root != NULL)\r\nfree_page((unsigned long)vcpu->arch.mmu.lm_root);\r\n}\r\nstatic int alloc_mmu_pages(struct kvm_vcpu *vcpu)\r\n{\r\nstruct page *page;\r\nint i;\r\nASSERT(vcpu);\r\npage = alloc_page(GFP_KERNEL | __GFP_DMA32);\r\nif (!page)\r\nreturn -ENOMEM;\r\nvcpu->arch.mmu.pae_root = page_address(page);\r\nfor (i = 0; i < 4; ++i)\r\nvcpu->arch.mmu.pae_root[i] = INVALID_PAGE;\r\nreturn 0;\r\n}\r\nint kvm_mmu_create(struct kvm_vcpu *vcpu)\r\n{\r\nASSERT(vcpu);\r\nvcpu->arch.walk_mmu = &vcpu->arch.mmu;\r\nvcpu->arch.mmu.root_hpa = INVALID_PAGE;\r\nvcpu->arch.mmu.translate_gpa = translate_gpa;\r\nvcpu->arch.nested_mmu.translate_gpa = translate_nested_gpa;\r\nreturn alloc_mmu_pages(vcpu);\r\n}\r\nvoid kvm_mmu_setup(struct kvm_vcpu *vcpu)\r\n{\r\nASSERT(vcpu);\r\nASSERT(!VALID_PAGE(vcpu->arch.mmu.root_hpa));\r\ninit_kvm_mmu(vcpu);\r\n}\r\nvoid kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot)\r\n{\r\nstruct kvm_memory_slot *memslot;\r\ngfn_t last_gfn;\r\nint i;\r\nmemslot = id_to_memslot(kvm->memslots, slot);\r\nlast_gfn = memslot->base_gfn + memslot->npages - 1;\r\nspin_lock(&kvm->mmu_lock);\r\nfor (i = PT_PAGE_TABLE_LEVEL;\r\ni < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++i) {\r\nunsigned long *rmapp;\r\nunsigned long last_index, index;\r\nrmapp = memslot->arch.rmap[i - PT_PAGE_TABLE_LEVEL];\r\nlast_index = gfn_to_index(last_gfn, memslot->base_gfn, i);\r\nfor (index = 0; index <= last_index; ++index, ++rmapp) {\r\nif (*rmapp)\r\n__rmap_write_protect(kvm, rmapp, false);\r\nif (need_resched() || spin_needbreak(&kvm->mmu_lock))\r\ncond_resched_lock(&kvm->mmu_lock);\r\n}\r\n}\r\nspin_unlock(&kvm->mmu_lock);\r\nlockdep_assert_held(&kvm->slots_lock);\r\nkvm_flush_remote_tlbs(kvm);\r\n}\r\nstatic void kvm_zap_obsolete_pages(struct kvm *kvm)\r\n{\r\nstruct kvm_mmu_page *sp, *node;\r\nint batch = 0;\r\nrestart:\r\nlist_for_each_entry_safe_reverse(sp, node,\r\n&kvm->arch.active_mmu_pages, link) {\r\nint ret;\r\nif (!is_obsolete_sp(kvm, sp))\r\nbreak;\r\nif (sp->role.invalid)\r\ncontinue;\r\nif (batch >= BATCH_ZAP_PAGES &&\r\ncond_resched_lock(&kvm->mmu_lock)) {\r\nbatch = 0;\r\ngoto restart;\r\n}\r\nret = kvm_mmu_prepare_zap_page(kvm, sp,\r\n&kvm->arch.zapped_obsolete_pages);\r\nbatch += ret;\r\nif (ret)\r\ngoto restart;\r\n}\r\nkvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);\r\n}\r\nvoid kvm_mmu_invalidate_zap_all_pages(struct kvm *kvm)\r\n{\r\nspin_lock(&kvm->mmu_lock);\r\ntrace_kvm_mmu_invalidate_zap_all_pages(kvm);\r\nkvm->arch.mmu_valid_gen++;\r\nkvm_reload_remote_mmus(kvm);\r\nkvm_zap_obsolete_pages(kvm);\r\nspin_unlock(&kvm->mmu_lock);\r\n}\r\nstatic bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)\r\n{\r\nreturn unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));\r\n}\r\nvoid kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm)\r\n{\r\nif (unlikely(kvm_current_mmio_generation(kvm) >= MMIO_MAX_GEN)) {\r\nprintk_ratelimited(KERN_INFO "kvm: zapping shadow pages for mmio generation wraparound\n");\r\nkvm_mmu_invalidate_zap_all_pages(kvm);\r\n}\r\n}\r\nstatic unsigned long\r\nmmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nstruct kvm *kvm;\r\nint nr_to_scan = sc->nr_to_scan;\r\nunsigned long freed = 0;\r\nspin_lock(&kvm_lock);\r\nlist_for_each_entry(kvm, &vm_list, vm_list) {\r\nint idx;\r\nLIST_HEAD(invalid_list);\r\nif (!nr_to_scan--)\r\nbreak;\r\nif (!kvm->arch.n_used_mmu_pages &&\r\n!kvm_has_zapped_obsolete_pages(kvm))\r\ncontinue;\r\nidx = srcu_read_lock(&kvm->srcu);\r\nspin_lock(&kvm->mmu_lock);\r\nif (kvm_has_zapped_obsolete_pages(kvm)) {\r\nkvm_mmu_commit_zap_page(kvm,\r\n&kvm->arch.zapped_obsolete_pages);\r\ngoto unlock;\r\n}\r\nif (prepare_zap_oldest_mmu_page(kvm, &invalid_list))\r\nfreed++;\r\nkvm_mmu_commit_zap_page(kvm, &invalid_list);\r\nunlock:\r\nspin_unlock(&kvm->mmu_lock);\r\nsrcu_read_unlock(&kvm->srcu, idx);\r\nlist_move_tail(&kvm->vm_list, &vm_list);\r\nbreak;\r\n}\r\nspin_unlock(&kvm_lock);\r\nreturn freed;\r\n}\r\nstatic unsigned long\r\nmmu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nreturn percpu_counter_read_positive(&kvm_total_used_mmu_pages);\r\n}\r\nstatic void mmu_destroy_caches(void)\r\n{\r\nif (pte_list_desc_cache)\r\nkmem_cache_destroy(pte_list_desc_cache);\r\nif (mmu_page_header_cache)\r\nkmem_cache_destroy(mmu_page_header_cache);\r\n}\r\nint kvm_mmu_module_init(void)\r\n{\r\npte_list_desc_cache = kmem_cache_create("pte_list_desc",\r\nsizeof(struct pte_list_desc),\r\n0, 0, NULL);\r\nif (!pte_list_desc_cache)\r\ngoto nomem;\r\nmmu_page_header_cache = kmem_cache_create("kvm_mmu_page_header",\r\nsizeof(struct kvm_mmu_page),\r\n0, 0, NULL);\r\nif (!mmu_page_header_cache)\r\ngoto nomem;\r\nif (percpu_counter_init(&kvm_total_used_mmu_pages, 0))\r\ngoto nomem;\r\nregister_shrinker(&mmu_shrinker);\r\nreturn 0;\r\nnomem:\r\nmmu_destroy_caches();\r\nreturn -ENOMEM;\r\n}\r\nunsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm)\r\n{\r\nunsigned int nr_mmu_pages;\r\nunsigned int nr_pages = 0;\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots)\r\nnr_pages += memslot->npages;\r\nnr_mmu_pages = nr_pages * KVM_PERMILLE_MMU_PAGES / 1000;\r\nnr_mmu_pages = max(nr_mmu_pages,\r\n(unsigned int) KVM_MIN_ALLOC_MMU_PAGES);\r\nreturn nr_mmu_pages;\r\n}\r\nint kvm_mmu_get_spte_hierarchy(struct kvm_vcpu *vcpu, u64 addr, u64 sptes[4])\r\n{\r\nstruct kvm_shadow_walk_iterator iterator;\r\nu64 spte;\r\nint nr_sptes = 0;\r\nif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\r\nreturn nr_sptes;\r\nwalk_shadow_page_lockless_begin(vcpu);\r\nfor_each_shadow_entry_lockless(vcpu, addr, iterator, spte) {\r\nsptes[iterator.level-1] = spte;\r\nnr_sptes++;\r\nif (!is_shadow_present_pte(spte))\r\nbreak;\r\n}\r\nwalk_shadow_page_lockless_end(vcpu);\r\nreturn nr_sptes;\r\n}\r\nvoid kvm_mmu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nASSERT(vcpu);\r\nkvm_mmu_unload(vcpu);\r\nfree_mmu_pages(vcpu);\r\nmmu_free_memory_caches(vcpu);\r\n}\r\nvoid kvm_mmu_module_exit(void)\r\n{\r\nmmu_destroy_caches();\r\npercpu_counter_destroy(&kvm_total_used_mmu_pages);\r\nunregister_shrinker(&mmu_shrinker);\r\nmmu_audit_disable();\r\n}
