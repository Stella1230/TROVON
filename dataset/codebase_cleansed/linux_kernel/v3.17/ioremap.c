void __iomem * __ioremap(unsigned long phys_addr, unsigned long size, unsigned long flags)\r\n{\r\nvoid __iomem *addr;\r\nstruct vm_struct *area;\r\nunsigned long offset, last_addr;\r\npgprot_t pgprot;\r\n#ifdef CONFIG_EISA\r\nunsigned long end = phys_addr + size - 1;\r\nif ((phys_addr >= 0x00080000 && end < 0x000fffff) ||\r\n(phys_addr >= 0x00500000 && end < 0x03bfffff)) {\r\nphys_addr |= F_EXTEND(0xfc000000);\r\nflags |= _PAGE_NO_CACHE;\r\n}\r\n#endif\r\nlast_addr = phys_addr + size - 1;\r\nif (!size || last_addr < phys_addr)\r\nreturn NULL;\r\nif (phys_addr < virt_to_phys(high_memory)) {\r\nchar *t_addr, *t_end;\r\nstruct page *page;\r\nt_addr = __va(phys_addr);\r\nt_end = t_addr + (size - 1);\r\nfor (page = virt_to_page(t_addr);\r\npage <= virt_to_page(t_end); page++) {\r\nif(!PageReserved(page))\r\nreturn NULL;\r\n}\r\n}\r\npgprot = __pgprot(_PAGE_PRESENT | _PAGE_RW | _PAGE_DIRTY |\r\n_PAGE_ACCESSED | flags);\r\noffset = phys_addr & ~PAGE_MASK;\r\nphys_addr &= PAGE_MASK;\r\nsize = PAGE_ALIGN(last_addr + 1) - phys_addr;\r\narea = get_vm_area(size, VM_IOREMAP);\r\nif (!area)\r\nreturn NULL;\r\naddr = (void __iomem *) area->addr;\r\nif (ioremap_page_range((unsigned long)addr, (unsigned long)addr + size,\r\nphys_addr, pgprot)) {\r\nvfree(addr);\r\nreturn NULL;\r\n}\r\nreturn (void __iomem *) (offset + (char __iomem *)addr);\r\n}\r\nvoid iounmap(const volatile void __iomem *addr)\r\n{\r\nif (addr > high_memory)\r\nreturn vfree((void *) (PAGE_MASK & (unsigned long __force) addr));\r\n}
