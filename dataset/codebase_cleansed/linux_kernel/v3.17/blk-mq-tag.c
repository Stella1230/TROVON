static bool bt_has_free_tags(struct blk_mq_bitmap_tags *bt)\r\n{\r\nint i;\r\nfor (i = 0; i < bt->map_nr; i++) {\r\nstruct blk_align_bitmap *bm = &bt->map[i];\r\nint ret;\r\nret = find_first_zero_bit(&bm->word, bm->depth);\r\nif (ret < bm->depth)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nbool blk_mq_has_free_tags(struct blk_mq_tags *tags)\r\n{\r\nif (!tags)\r\nreturn true;\r\nreturn bt_has_free_tags(&tags->bitmap_tags);\r\n}\r\nstatic inline int bt_index_inc(int index)\r\n{\r\nreturn (index + 1) & (BT_WAIT_QUEUES - 1);\r\n}\r\nstatic inline void bt_index_atomic_inc(atomic_t *index)\r\n{\r\nint old = atomic_read(index);\r\nint new = bt_index_inc(old);\r\natomic_cmpxchg(index, old, new);\r\n}\r\nbool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)\r\n{\r\nif (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&\r\n!test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\r\natomic_inc(&hctx->tags->active_queues);\r\nreturn true;\r\n}\r\nstatic void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags)\r\n{\r\nstruct blk_mq_bitmap_tags *bt;\r\nint i, wake_index;\r\nbt = &tags->bitmap_tags;\r\nwake_index = atomic_read(&bt->wake_index);\r\nfor (i = 0; i < BT_WAIT_QUEUES; i++) {\r\nstruct bt_wait_state *bs = &bt->bs[wake_index];\r\nif (waitqueue_active(&bs->wait))\r\nwake_up(&bs->wait);\r\nwake_index = bt_index_inc(wake_index);\r\n}\r\n}\r\nvoid __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)\r\n{\r\nstruct blk_mq_tags *tags = hctx->tags;\r\nif (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\r\nreturn;\r\natomic_dec(&tags->active_queues);\r\nblk_mq_tag_wakeup_all(tags);\r\n}\r\nstatic inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,\r\nstruct blk_mq_bitmap_tags *bt)\r\n{\r\nunsigned int depth, users;\r\nif (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))\r\nreturn true;\r\nif (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\r\nreturn true;\r\nif (bt->depth == 1)\r\nreturn true;\r\nusers = atomic_read(&hctx->tags->active_queues);\r\nif (!users)\r\nreturn true;\r\ndepth = max((bt->depth + users - 1) / users, 4U);\r\nreturn atomic_read(&hctx->nr_active) < depth;\r\n}\r\nstatic int __bt_get_word(struct blk_align_bitmap *bm, unsigned int last_tag)\r\n{\r\nint tag, org_last_tag, end;\r\norg_last_tag = last_tag;\r\nend = bm->depth;\r\ndo {\r\nrestart:\r\ntag = find_next_zero_bit(&bm->word, end, last_tag);\r\nif (unlikely(tag >= end)) {\r\nif (org_last_tag && last_tag) {\r\nend = last_tag;\r\nlast_tag = 0;\r\ngoto restart;\r\n}\r\nreturn -1;\r\n}\r\nlast_tag = tag + 1;\r\n} while (test_and_set_bit_lock(tag, &bm->word));\r\nreturn tag;\r\n}\r\nstatic int __bt_get(struct blk_mq_hw_ctx *hctx, struct blk_mq_bitmap_tags *bt,\r\nunsigned int *tag_cache)\r\n{\r\nunsigned int last_tag, org_last_tag;\r\nint index, i, tag;\r\nif (!hctx_may_queue(hctx, bt))\r\nreturn -1;\r\nlast_tag = org_last_tag = *tag_cache;\r\nindex = TAG_TO_INDEX(bt, last_tag);\r\nfor (i = 0; i < bt->map_nr; i++) {\r\ntag = __bt_get_word(&bt->map[index], TAG_TO_BIT(bt, last_tag));\r\nif (tag != -1) {\r\ntag += (index << bt->bits_per_word);\r\ngoto done;\r\n}\r\nlast_tag = 0;\r\nif (++index >= bt->map_nr)\r\nindex = 0;\r\n}\r\n*tag_cache = 0;\r\nreturn -1;\r\ndone:\r\nif (tag == org_last_tag) {\r\nlast_tag = tag + 1;\r\nif (last_tag >= bt->depth - 1)\r\nlast_tag = 0;\r\n*tag_cache = last_tag;\r\n}\r\nreturn tag;\r\n}\r\nstatic struct bt_wait_state *bt_wait_ptr(struct blk_mq_bitmap_tags *bt,\r\nstruct blk_mq_hw_ctx *hctx)\r\n{\r\nstruct bt_wait_state *bs;\r\nint wait_index;\r\nif (!hctx)\r\nreturn &bt->bs[0];\r\nwait_index = atomic_read(&hctx->wait_index);\r\nbs = &bt->bs[wait_index];\r\nbt_index_atomic_inc(&hctx->wait_index);\r\nreturn bs;\r\n}\r\nstatic int bt_get(struct blk_mq_alloc_data *data,\r\nstruct blk_mq_bitmap_tags *bt,\r\nstruct blk_mq_hw_ctx *hctx,\r\nunsigned int *last_tag)\r\n{\r\nstruct bt_wait_state *bs;\r\nDEFINE_WAIT(wait);\r\nint tag;\r\ntag = __bt_get(hctx, bt, last_tag);\r\nif (tag != -1)\r\nreturn tag;\r\nif (!(data->gfp & __GFP_WAIT))\r\nreturn -1;\r\nbs = bt_wait_ptr(bt, hctx);\r\ndo {\r\nprepare_to_wait(&bs->wait, &wait, TASK_UNINTERRUPTIBLE);\r\ntag = __bt_get(hctx, bt, last_tag);\r\nif (tag != -1)\r\nbreak;\r\nblk_mq_put_ctx(data->ctx);\r\nio_schedule();\r\ndata->ctx = blk_mq_get_ctx(data->q);\r\ndata->hctx = data->q->mq_ops->map_queue(data->q,\r\ndata->ctx->cpu);\r\nif (data->reserved) {\r\nbt = &data->hctx->tags->breserved_tags;\r\n} else {\r\nlast_tag = &data->ctx->last_tag;\r\nhctx = data->hctx;\r\nbt = &hctx->tags->bitmap_tags;\r\n}\r\nfinish_wait(&bs->wait, &wait);\r\nbs = bt_wait_ptr(bt, hctx);\r\n} while (1);\r\nfinish_wait(&bs->wait, &wait);\r\nreturn tag;\r\n}\r\nstatic unsigned int __blk_mq_get_tag(struct blk_mq_alloc_data *data)\r\n{\r\nint tag;\r\ntag = bt_get(data, &data->hctx->tags->bitmap_tags, data->hctx,\r\n&data->ctx->last_tag);\r\nif (tag >= 0)\r\nreturn tag + data->hctx->tags->nr_reserved_tags;\r\nreturn BLK_MQ_TAG_FAIL;\r\n}\r\nstatic unsigned int __blk_mq_get_reserved_tag(struct blk_mq_alloc_data *data)\r\n{\r\nint tag, zero = 0;\r\nif (unlikely(!data->hctx->tags->nr_reserved_tags)) {\r\nWARN_ON_ONCE(1);\r\nreturn BLK_MQ_TAG_FAIL;\r\n}\r\ntag = bt_get(data, &data->hctx->tags->breserved_tags, NULL, &zero);\r\nif (tag < 0)\r\nreturn BLK_MQ_TAG_FAIL;\r\nreturn tag;\r\n}\r\nunsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)\r\n{\r\nif (!data->reserved)\r\nreturn __blk_mq_get_tag(data);\r\nreturn __blk_mq_get_reserved_tag(data);\r\n}\r\nstatic struct bt_wait_state *bt_wake_ptr(struct blk_mq_bitmap_tags *bt)\r\n{\r\nint i, wake_index;\r\nwake_index = atomic_read(&bt->wake_index);\r\nfor (i = 0; i < BT_WAIT_QUEUES; i++) {\r\nstruct bt_wait_state *bs = &bt->bs[wake_index];\r\nif (waitqueue_active(&bs->wait)) {\r\nint o = atomic_read(&bt->wake_index);\r\nif (wake_index != o)\r\natomic_cmpxchg(&bt->wake_index, o, wake_index);\r\nreturn bs;\r\n}\r\nwake_index = bt_index_inc(wake_index);\r\n}\r\nreturn NULL;\r\n}\r\nstatic void bt_clear_tag(struct blk_mq_bitmap_tags *bt, unsigned int tag)\r\n{\r\nconst int index = TAG_TO_INDEX(bt, tag);\r\nstruct bt_wait_state *bs;\r\nint wait_cnt;\r\nclear_bit_unlock(TAG_TO_BIT(bt, tag), &bt->map[index].word);\r\nbs = bt_wake_ptr(bt);\r\nif (!bs)\r\nreturn;\r\nwait_cnt = atomic_dec_return(&bs->wait_cnt);\r\nif (wait_cnt == 0) {\r\nwake:\r\natomic_add(bt->wake_cnt, &bs->wait_cnt);\r\nbt_index_atomic_inc(&bt->wake_index);\r\nwake_up(&bs->wait);\r\n} else if (wait_cnt < 0) {\r\nwait_cnt = atomic_inc_return(&bs->wait_cnt);\r\nif (!wait_cnt)\r\ngoto wake;\r\n}\r\n}\r\nstatic void __blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag)\r\n{\r\nBUG_ON(tag >= tags->nr_tags);\r\nbt_clear_tag(&tags->bitmap_tags, tag);\r\n}\r\nstatic void __blk_mq_put_reserved_tag(struct blk_mq_tags *tags,\r\nunsigned int tag)\r\n{\r\nBUG_ON(tag >= tags->nr_reserved_tags);\r\nbt_clear_tag(&tags->breserved_tags, tag);\r\n}\r\nvoid blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag,\r\nunsigned int *last_tag)\r\n{\r\nstruct blk_mq_tags *tags = hctx->tags;\r\nif (tag >= tags->nr_reserved_tags) {\r\nconst int real_tag = tag - tags->nr_reserved_tags;\r\n__blk_mq_put_tag(tags, real_tag);\r\n*last_tag = real_tag;\r\n} else\r\n__blk_mq_put_reserved_tag(tags, tag);\r\n}\r\nstatic void bt_for_each_free(struct blk_mq_bitmap_tags *bt,\r\nunsigned long *free_map, unsigned int off)\r\n{\r\nint i;\r\nfor (i = 0; i < bt->map_nr; i++) {\r\nstruct blk_align_bitmap *bm = &bt->map[i];\r\nint bit = 0;\r\ndo {\r\nbit = find_next_zero_bit(&bm->word, bm->depth, bit);\r\nif (bit >= bm->depth)\r\nbreak;\r\n__set_bit(bit + off, free_map);\r\nbit++;\r\n} while (1);\r\noff += (1 << bt->bits_per_word);\r\n}\r\n}\r\nvoid blk_mq_tag_busy_iter(struct blk_mq_tags *tags,\r\nvoid (*fn)(void *, unsigned long *), void *data)\r\n{\r\nunsigned long *tag_map;\r\nsize_t map_size;\r\nmap_size = ALIGN(tags->nr_tags, BITS_PER_LONG) / BITS_PER_LONG;\r\ntag_map = kzalloc(map_size * sizeof(unsigned long), GFP_ATOMIC);\r\nif (!tag_map)\r\nreturn;\r\nbt_for_each_free(&tags->bitmap_tags, tag_map, tags->nr_reserved_tags);\r\nif (tags->nr_reserved_tags)\r\nbt_for_each_free(&tags->breserved_tags, tag_map, 0);\r\nfn(data, tag_map);\r\nkfree(tag_map);\r\n}\r\nstatic unsigned int bt_unused_tags(struct blk_mq_bitmap_tags *bt)\r\n{\r\nunsigned int i, used;\r\nfor (i = 0, used = 0; i < bt->map_nr; i++) {\r\nstruct blk_align_bitmap *bm = &bt->map[i];\r\nused += bitmap_weight(&bm->word, bm->depth);\r\n}\r\nreturn bt->depth - used;\r\n}\r\nstatic void bt_update_count(struct blk_mq_bitmap_tags *bt,\r\nunsigned int depth)\r\n{\r\nunsigned int tags_per_word = 1U << bt->bits_per_word;\r\nunsigned int map_depth = depth;\r\nif (depth) {\r\nint i;\r\nfor (i = 0; i < bt->map_nr; i++) {\r\nbt->map[i].depth = min(map_depth, tags_per_word);\r\nmap_depth -= bt->map[i].depth;\r\n}\r\n}\r\nbt->wake_cnt = BT_WAIT_BATCH;\r\nif (bt->wake_cnt > depth / 4)\r\nbt->wake_cnt = max(1U, depth / 4);\r\nbt->depth = depth;\r\n}\r\nstatic int bt_alloc(struct blk_mq_bitmap_tags *bt, unsigned int depth,\r\nint node, bool reserved)\r\n{\r\nint i;\r\nbt->bits_per_word = ilog2(BITS_PER_LONG);\r\nif (depth) {\r\nunsigned int nr, tags_per_word;\r\ntags_per_word = (1 << bt->bits_per_word);\r\nif (depth >= 4) {\r\nwhile (tags_per_word * 4 > depth) {\r\nbt->bits_per_word--;\r\ntags_per_word = (1 << bt->bits_per_word);\r\n}\r\n}\r\nnr = ALIGN(depth, tags_per_word) / tags_per_word;\r\nbt->map = kzalloc_node(nr * sizeof(struct blk_align_bitmap),\r\nGFP_KERNEL, node);\r\nif (!bt->map)\r\nreturn -ENOMEM;\r\nbt->map_nr = nr;\r\n}\r\nbt->bs = kzalloc(BT_WAIT_QUEUES * sizeof(*bt->bs), GFP_KERNEL);\r\nif (!bt->bs) {\r\nkfree(bt->map);\r\nreturn -ENOMEM;\r\n}\r\nbt_update_count(bt, depth);\r\nfor (i = 0; i < BT_WAIT_QUEUES; i++) {\r\ninit_waitqueue_head(&bt->bs[i].wait);\r\natomic_set(&bt->bs[i].wait_cnt, bt->wake_cnt);\r\n}\r\nreturn 0;\r\n}\r\nstatic void bt_free(struct blk_mq_bitmap_tags *bt)\r\n{\r\nkfree(bt->map);\r\nkfree(bt->bs);\r\n}\r\nstatic struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,\r\nint node)\r\n{\r\nunsigned int depth = tags->nr_tags - tags->nr_reserved_tags;\r\nif (bt_alloc(&tags->bitmap_tags, depth, node, false))\r\ngoto enomem;\r\nif (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, node, true))\r\ngoto enomem;\r\nreturn tags;\r\nenomem:\r\nbt_free(&tags->bitmap_tags);\r\nkfree(tags);\r\nreturn NULL;\r\n}\r\nstruct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,\r\nunsigned int reserved_tags, int node)\r\n{\r\nstruct blk_mq_tags *tags;\r\nif (total_tags > BLK_MQ_TAG_MAX) {\r\npr_err("blk-mq: tag depth too large\n");\r\nreturn NULL;\r\n}\r\ntags = kzalloc_node(sizeof(*tags), GFP_KERNEL, node);\r\nif (!tags)\r\nreturn NULL;\r\ntags->nr_tags = total_tags;\r\ntags->nr_reserved_tags = reserved_tags;\r\nreturn blk_mq_init_bitmap_tags(tags, node);\r\n}\r\nvoid blk_mq_free_tags(struct blk_mq_tags *tags)\r\n{\r\nbt_free(&tags->bitmap_tags);\r\nbt_free(&tags->breserved_tags);\r\nkfree(tags);\r\n}\r\nvoid blk_mq_tag_init_last_tag(struct blk_mq_tags *tags, unsigned int *tag)\r\n{\r\nunsigned int depth = tags->nr_tags - tags->nr_reserved_tags;\r\n*tag = prandom_u32() % depth;\r\n}\r\nint blk_mq_tag_update_depth(struct blk_mq_tags *tags, unsigned int tdepth)\r\n{\r\ntdepth -= tags->nr_reserved_tags;\r\nif (tdepth > tags->nr_tags)\r\nreturn -EINVAL;\r\nbt_update_count(&tags->bitmap_tags, tdepth);\r\nblk_mq_tag_wakeup_all(tags);\r\nreturn 0;\r\n}\r\nssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page)\r\n{\r\nchar *orig_page = page;\r\nunsigned int free, res;\r\nif (!tags)\r\nreturn 0;\r\npage += sprintf(page, "nr_tags=%u, reserved_tags=%u, "\r\n"bits_per_word=%u\n",\r\ntags->nr_tags, tags->nr_reserved_tags,\r\ntags->bitmap_tags.bits_per_word);\r\nfree = bt_unused_tags(&tags->bitmap_tags);\r\nres = bt_unused_tags(&tags->breserved_tags);\r\npage += sprintf(page, "nr_free=%u, nr_reserved=%u\n", free, res);\r\npage += sprintf(page, "active_queues=%u\n", atomic_read(&tags->active_queues));\r\nreturn page - orig_page;\r\n}
