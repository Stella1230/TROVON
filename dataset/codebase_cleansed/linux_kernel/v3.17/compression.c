static inline int compressed_bio_size(struct btrfs_root *root,\r\nunsigned long disk_size)\r\n{\r\nu16 csum_size = btrfs_super_csum_size(root->fs_info->super_copy);\r\nreturn sizeof(struct compressed_bio) +\r\n((disk_size + root->sectorsize - 1) / root->sectorsize) *\r\ncsum_size;\r\n}\r\nstatic struct bio *compressed_bio_alloc(struct block_device *bdev,\r\nu64 first_byte, gfp_t gfp_flags)\r\n{\r\nint nr_vecs;\r\nnr_vecs = bio_get_nr_vecs(bdev);\r\nreturn btrfs_bio_alloc(bdev, first_byte >> 9, nr_vecs, gfp_flags);\r\n}\r\nstatic int check_compressed_csum(struct inode *inode,\r\nstruct compressed_bio *cb,\r\nu64 disk_start)\r\n{\r\nint ret;\r\nstruct page *page;\r\nunsigned long i;\r\nchar *kaddr;\r\nu32 csum;\r\nu32 *cb_sum = &cb->sums;\r\nif (BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM)\r\nreturn 0;\r\nfor (i = 0; i < cb->nr_pages; i++) {\r\npage = cb->compressed_pages[i];\r\ncsum = ~(u32)0;\r\nkaddr = kmap_atomic(page);\r\ncsum = btrfs_csum_data(kaddr, csum, PAGE_CACHE_SIZE);\r\nbtrfs_csum_final(csum, (char *)&csum);\r\nkunmap_atomic(kaddr);\r\nif (csum != *cb_sum) {\r\nbtrfs_info(BTRFS_I(inode)->root->fs_info,\r\n"csum failed ino %llu extent %llu csum %u wanted %u mirror %d",\r\nbtrfs_ino(inode), disk_start, csum, *cb_sum,\r\ncb->mirror_num);\r\nret = -EIO;\r\ngoto fail;\r\n}\r\ncb_sum++;\r\n}\r\nret = 0;\r\nfail:\r\nreturn ret;\r\n}\r\nstatic void end_compressed_bio_read(struct bio *bio, int err)\r\n{\r\nstruct compressed_bio *cb = bio->bi_private;\r\nstruct inode *inode;\r\nstruct page *page;\r\nunsigned long index;\r\nint ret;\r\nif (err)\r\ncb->errors = 1;\r\nif (!atomic_dec_and_test(&cb->pending_bios))\r\ngoto out;\r\ninode = cb->inode;\r\nret = check_compressed_csum(inode, cb,\r\n(u64)bio->bi_iter.bi_sector << 9);\r\nif (ret)\r\ngoto csum_failed;\r\nret = btrfs_decompress_biovec(cb->compress_type,\r\ncb->compressed_pages,\r\ncb->start,\r\ncb->orig_bio->bi_io_vec,\r\ncb->orig_bio->bi_vcnt,\r\ncb->compressed_len);\r\ncsum_failed:\r\nif (ret)\r\ncb->errors = 1;\r\nindex = 0;\r\nfor (index = 0; index < cb->nr_pages; index++) {\r\npage = cb->compressed_pages[index];\r\npage->mapping = NULL;\r\npage_cache_release(page);\r\n}\r\nif (cb->errors) {\r\nbio_io_error(cb->orig_bio);\r\n} else {\r\nint i;\r\nstruct bio_vec *bvec;\r\nbio_for_each_segment_all(bvec, cb->orig_bio, i)\r\nSetPageChecked(bvec->bv_page);\r\nbio_endio(cb->orig_bio, 0);\r\n}\r\nkfree(cb->compressed_pages);\r\nkfree(cb);\r\nout:\r\nbio_put(bio);\r\n}\r\nstatic noinline void end_compressed_writeback(struct inode *inode, u64 start,\r\nunsigned long ram_size)\r\n{\r\nunsigned long index = start >> PAGE_CACHE_SHIFT;\r\nunsigned long end_index = (start + ram_size - 1) >> PAGE_CACHE_SHIFT;\r\nstruct page *pages[16];\r\nunsigned long nr_pages = end_index - index + 1;\r\nint i;\r\nint ret;\r\nwhile (nr_pages > 0) {\r\nret = find_get_pages_contig(inode->i_mapping, index,\r\nmin_t(unsigned long,\r\nnr_pages, ARRAY_SIZE(pages)), pages);\r\nif (ret == 0) {\r\nnr_pages -= 1;\r\nindex += 1;\r\ncontinue;\r\n}\r\nfor (i = 0; i < ret; i++) {\r\nend_page_writeback(pages[i]);\r\npage_cache_release(pages[i]);\r\n}\r\nnr_pages -= ret;\r\nindex += ret;\r\n}\r\n}\r\nstatic void end_compressed_bio_write(struct bio *bio, int err)\r\n{\r\nstruct extent_io_tree *tree;\r\nstruct compressed_bio *cb = bio->bi_private;\r\nstruct inode *inode;\r\nstruct page *page;\r\nunsigned long index;\r\nif (err)\r\ncb->errors = 1;\r\nif (!atomic_dec_and_test(&cb->pending_bios))\r\ngoto out;\r\ninode = cb->inode;\r\ntree = &BTRFS_I(inode)->io_tree;\r\ncb->compressed_pages[0]->mapping = cb->inode->i_mapping;\r\ntree->ops->writepage_end_io_hook(cb->compressed_pages[0],\r\ncb->start,\r\ncb->start + cb->len - 1,\r\nNULL, 1);\r\ncb->compressed_pages[0]->mapping = NULL;\r\nend_compressed_writeback(inode, cb->start, cb->len);\r\nindex = 0;\r\nfor (index = 0; index < cb->nr_pages; index++) {\r\npage = cb->compressed_pages[index];\r\npage->mapping = NULL;\r\npage_cache_release(page);\r\n}\r\nkfree(cb->compressed_pages);\r\nkfree(cb);\r\nout:\r\nbio_put(bio);\r\n}\r\nint btrfs_submit_compressed_write(struct inode *inode, u64 start,\r\nunsigned long len, u64 disk_start,\r\nunsigned long compressed_len,\r\nstruct page **compressed_pages,\r\nunsigned long nr_pages)\r\n{\r\nstruct bio *bio = NULL;\r\nstruct btrfs_root *root = BTRFS_I(inode)->root;\r\nstruct compressed_bio *cb;\r\nunsigned long bytes_left;\r\nstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\r\nint pg_index = 0;\r\nstruct page *page;\r\nu64 first_byte = disk_start;\r\nstruct block_device *bdev;\r\nint ret;\r\nint skip_sum = BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM;\r\nWARN_ON(start & ((u64)PAGE_CACHE_SIZE - 1));\r\ncb = kmalloc(compressed_bio_size(root, compressed_len), GFP_NOFS);\r\nif (!cb)\r\nreturn -ENOMEM;\r\natomic_set(&cb->pending_bios, 0);\r\ncb->errors = 0;\r\ncb->inode = inode;\r\ncb->start = start;\r\ncb->len = len;\r\ncb->mirror_num = 0;\r\ncb->compressed_pages = compressed_pages;\r\ncb->compressed_len = compressed_len;\r\ncb->orig_bio = NULL;\r\ncb->nr_pages = nr_pages;\r\nbdev = BTRFS_I(inode)->root->fs_info->fs_devices->latest_bdev;\r\nbio = compressed_bio_alloc(bdev, first_byte, GFP_NOFS);\r\nif (!bio) {\r\nkfree(cb);\r\nreturn -ENOMEM;\r\n}\r\nbio->bi_private = cb;\r\nbio->bi_end_io = end_compressed_bio_write;\r\natomic_inc(&cb->pending_bios);\r\nbytes_left = compressed_len;\r\nfor (pg_index = 0; pg_index < cb->nr_pages; pg_index++) {\r\npage = compressed_pages[pg_index];\r\npage->mapping = inode->i_mapping;\r\nif (bio->bi_iter.bi_size)\r\nret = io_tree->ops->merge_bio_hook(WRITE, page, 0,\r\nPAGE_CACHE_SIZE,\r\nbio, 0);\r\nelse\r\nret = 0;\r\npage->mapping = NULL;\r\nif (ret || bio_add_page(bio, page, PAGE_CACHE_SIZE, 0) <\r\nPAGE_CACHE_SIZE) {\r\nbio_get(bio);\r\natomic_inc(&cb->pending_bios);\r\nret = btrfs_bio_wq_end_io(root->fs_info, bio, 0);\r\nBUG_ON(ret);\r\nif (!skip_sum) {\r\nret = btrfs_csum_one_bio(root, inode, bio,\r\nstart, 1);\r\nBUG_ON(ret);\r\n}\r\nret = btrfs_map_bio(root, WRITE, bio, 0, 1);\r\nBUG_ON(ret);\r\nbio_put(bio);\r\nbio = compressed_bio_alloc(bdev, first_byte, GFP_NOFS);\r\nBUG_ON(!bio);\r\nbio->bi_private = cb;\r\nbio->bi_end_io = end_compressed_bio_write;\r\nbio_add_page(bio, page, PAGE_CACHE_SIZE, 0);\r\n}\r\nif (bytes_left < PAGE_CACHE_SIZE) {\r\nbtrfs_info(BTRFS_I(inode)->root->fs_info,\r\n"bytes left %lu compress len %lu nr %lu",\r\nbytes_left, cb->compressed_len, cb->nr_pages);\r\n}\r\nbytes_left -= PAGE_CACHE_SIZE;\r\nfirst_byte += PAGE_CACHE_SIZE;\r\ncond_resched();\r\n}\r\nbio_get(bio);\r\nret = btrfs_bio_wq_end_io(root->fs_info, bio, 0);\r\nBUG_ON(ret);\r\nif (!skip_sum) {\r\nret = btrfs_csum_one_bio(root, inode, bio, start, 1);\r\nBUG_ON(ret);\r\n}\r\nret = btrfs_map_bio(root, WRITE, bio, 0, 1);\r\nBUG_ON(ret);\r\nbio_put(bio);\r\nreturn 0;\r\n}\r\nstatic noinline int add_ra_bio_pages(struct inode *inode,\r\nu64 compressed_end,\r\nstruct compressed_bio *cb)\r\n{\r\nunsigned long end_index;\r\nunsigned long pg_index;\r\nu64 last_offset;\r\nu64 isize = i_size_read(inode);\r\nint ret;\r\nstruct page *page;\r\nunsigned long nr_pages = 0;\r\nstruct extent_map *em;\r\nstruct address_space *mapping = inode->i_mapping;\r\nstruct extent_map_tree *em_tree;\r\nstruct extent_io_tree *tree;\r\nu64 end;\r\nint misses = 0;\r\npage = cb->orig_bio->bi_io_vec[cb->orig_bio->bi_vcnt - 1].bv_page;\r\nlast_offset = (page_offset(page) + PAGE_CACHE_SIZE);\r\nem_tree = &BTRFS_I(inode)->extent_tree;\r\ntree = &BTRFS_I(inode)->io_tree;\r\nif (isize == 0)\r\nreturn 0;\r\nend_index = (i_size_read(inode) - 1) >> PAGE_CACHE_SHIFT;\r\nwhile (last_offset < compressed_end) {\r\npg_index = last_offset >> PAGE_CACHE_SHIFT;\r\nif (pg_index > end_index)\r\nbreak;\r\nrcu_read_lock();\r\npage = radix_tree_lookup(&mapping->page_tree, pg_index);\r\nrcu_read_unlock();\r\nif (page && !radix_tree_exceptional_entry(page)) {\r\nmisses++;\r\nif (misses > 4)\r\nbreak;\r\ngoto next;\r\n}\r\npage = __page_cache_alloc(mapping_gfp_mask(mapping) &\r\n~__GFP_FS);\r\nif (!page)\r\nbreak;\r\nif (add_to_page_cache_lru(page, mapping, pg_index,\r\nGFP_NOFS)) {\r\npage_cache_release(page);\r\ngoto next;\r\n}\r\nend = last_offset + PAGE_CACHE_SIZE - 1;\r\nset_page_extent_mapped(page);\r\nlock_extent(tree, last_offset, end);\r\nread_lock(&em_tree->lock);\r\nem = lookup_extent_mapping(em_tree, last_offset,\r\nPAGE_CACHE_SIZE);\r\nread_unlock(&em_tree->lock);\r\nif (!em || last_offset < em->start ||\r\n(last_offset + PAGE_CACHE_SIZE > extent_map_end(em)) ||\r\n(em->block_start >> 9) != cb->orig_bio->bi_iter.bi_sector) {\r\nfree_extent_map(em);\r\nunlock_extent(tree, last_offset, end);\r\nunlock_page(page);\r\npage_cache_release(page);\r\nbreak;\r\n}\r\nfree_extent_map(em);\r\nif (page->index == end_index) {\r\nchar *userpage;\r\nsize_t zero_offset = isize & (PAGE_CACHE_SIZE - 1);\r\nif (zero_offset) {\r\nint zeros;\r\nzeros = PAGE_CACHE_SIZE - zero_offset;\r\nuserpage = kmap_atomic(page);\r\nmemset(userpage + zero_offset, 0, zeros);\r\nflush_dcache_page(page);\r\nkunmap_atomic(userpage);\r\n}\r\n}\r\nret = bio_add_page(cb->orig_bio, page,\r\nPAGE_CACHE_SIZE, 0);\r\nif (ret == PAGE_CACHE_SIZE) {\r\nnr_pages++;\r\npage_cache_release(page);\r\n} else {\r\nunlock_extent(tree, last_offset, end);\r\nunlock_page(page);\r\npage_cache_release(page);\r\nbreak;\r\n}\r\nnext:\r\nlast_offset += PAGE_CACHE_SIZE;\r\n}\r\nreturn 0;\r\n}\r\nint btrfs_submit_compressed_read(struct inode *inode, struct bio *bio,\r\nint mirror_num, unsigned long bio_flags)\r\n{\r\nstruct extent_io_tree *tree;\r\nstruct extent_map_tree *em_tree;\r\nstruct compressed_bio *cb;\r\nstruct btrfs_root *root = BTRFS_I(inode)->root;\r\nunsigned long uncompressed_len = bio->bi_vcnt * PAGE_CACHE_SIZE;\r\nunsigned long compressed_len;\r\nunsigned long nr_pages;\r\nunsigned long pg_index;\r\nstruct page *page;\r\nstruct block_device *bdev;\r\nstruct bio *comp_bio;\r\nu64 cur_disk_byte = (u64)bio->bi_iter.bi_sector << 9;\r\nu64 em_len;\r\nu64 em_start;\r\nstruct extent_map *em;\r\nint ret = -ENOMEM;\r\nint faili = 0;\r\nu32 *sums;\r\ntree = &BTRFS_I(inode)->io_tree;\r\nem_tree = &BTRFS_I(inode)->extent_tree;\r\nread_lock(&em_tree->lock);\r\nem = lookup_extent_mapping(em_tree,\r\npage_offset(bio->bi_io_vec->bv_page),\r\nPAGE_CACHE_SIZE);\r\nread_unlock(&em_tree->lock);\r\nif (!em)\r\nreturn -EIO;\r\ncompressed_len = em->block_len;\r\ncb = kmalloc(compressed_bio_size(root, compressed_len), GFP_NOFS);\r\nif (!cb)\r\ngoto out;\r\natomic_set(&cb->pending_bios, 0);\r\ncb->errors = 0;\r\ncb->inode = inode;\r\ncb->mirror_num = mirror_num;\r\nsums = &cb->sums;\r\ncb->start = em->orig_start;\r\nem_len = em->len;\r\nem_start = em->start;\r\nfree_extent_map(em);\r\nem = NULL;\r\ncb->len = uncompressed_len;\r\ncb->compressed_len = compressed_len;\r\ncb->compress_type = extent_compress_type(bio_flags);\r\ncb->orig_bio = bio;\r\nnr_pages = (compressed_len + PAGE_CACHE_SIZE - 1) /\r\nPAGE_CACHE_SIZE;\r\ncb->compressed_pages = kzalloc(sizeof(struct page *) * nr_pages,\r\nGFP_NOFS);\r\nif (!cb->compressed_pages)\r\ngoto fail1;\r\nbdev = BTRFS_I(inode)->root->fs_info->fs_devices->latest_bdev;\r\nfor (pg_index = 0; pg_index < nr_pages; pg_index++) {\r\ncb->compressed_pages[pg_index] = alloc_page(GFP_NOFS |\r\n__GFP_HIGHMEM);\r\nif (!cb->compressed_pages[pg_index]) {\r\nfaili = pg_index - 1;\r\nret = -ENOMEM;\r\ngoto fail2;\r\n}\r\n}\r\nfaili = nr_pages - 1;\r\ncb->nr_pages = nr_pages;\r\nif (!(bio_flags & EXTENT_BIO_PARENT_LOCKED))\r\nadd_ra_bio_pages(inode, em_start + em_len, cb);\r\nuncompressed_len = bio->bi_vcnt * PAGE_CACHE_SIZE;\r\ncb->len = uncompressed_len;\r\ncomp_bio = compressed_bio_alloc(bdev, cur_disk_byte, GFP_NOFS);\r\nif (!comp_bio)\r\ngoto fail2;\r\ncomp_bio->bi_private = cb;\r\ncomp_bio->bi_end_io = end_compressed_bio_read;\r\natomic_inc(&cb->pending_bios);\r\nfor (pg_index = 0; pg_index < nr_pages; pg_index++) {\r\npage = cb->compressed_pages[pg_index];\r\npage->mapping = inode->i_mapping;\r\npage->index = em_start >> PAGE_CACHE_SHIFT;\r\nif (comp_bio->bi_iter.bi_size)\r\nret = tree->ops->merge_bio_hook(READ, page, 0,\r\nPAGE_CACHE_SIZE,\r\ncomp_bio, 0);\r\nelse\r\nret = 0;\r\npage->mapping = NULL;\r\nif (ret || bio_add_page(comp_bio, page, PAGE_CACHE_SIZE, 0) <\r\nPAGE_CACHE_SIZE) {\r\nbio_get(comp_bio);\r\nret = btrfs_bio_wq_end_io(root->fs_info, comp_bio, 0);\r\nBUG_ON(ret);\r\natomic_inc(&cb->pending_bios);\r\nif (!(BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM)) {\r\nret = btrfs_lookup_bio_sums(root, inode,\r\ncomp_bio, sums);\r\nBUG_ON(ret);\r\n}\r\nsums += (comp_bio->bi_iter.bi_size +\r\nroot->sectorsize - 1) / root->sectorsize;\r\nret = btrfs_map_bio(root, READ, comp_bio,\r\nmirror_num, 0);\r\nif (ret)\r\nbio_endio(comp_bio, ret);\r\nbio_put(comp_bio);\r\ncomp_bio = compressed_bio_alloc(bdev, cur_disk_byte,\r\nGFP_NOFS);\r\nBUG_ON(!comp_bio);\r\ncomp_bio->bi_private = cb;\r\ncomp_bio->bi_end_io = end_compressed_bio_read;\r\nbio_add_page(comp_bio, page, PAGE_CACHE_SIZE, 0);\r\n}\r\ncur_disk_byte += PAGE_CACHE_SIZE;\r\n}\r\nbio_get(comp_bio);\r\nret = btrfs_bio_wq_end_io(root->fs_info, comp_bio, 0);\r\nBUG_ON(ret);\r\nif (!(BTRFS_I(inode)->flags & BTRFS_INODE_NODATASUM)) {\r\nret = btrfs_lookup_bio_sums(root, inode, comp_bio, sums);\r\nBUG_ON(ret);\r\n}\r\nret = btrfs_map_bio(root, READ, comp_bio, mirror_num, 0);\r\nif (ret)\r\nbio_endio(comp_bio, ret);\r\nbio_put(comp_bio);\r\nreturn 0;\r\nfail2:\r\nwhile (faili >= 0) {\r\n__free_page(cb->compressed_pages[faili]);\r\nfaili--;\r\n}\r\nkfree(cb->compressed_pages);\r\nfail1:\r\nkfree(cb);\r\nout:\r\nfree_extent_map(em);\r\nreturn ret;\r\n}\r\nvoid __init btrfs_init_compress(void)\r\n{\r\nint i;\r\nfor (i = 0; i < BTRFS_COMPRESS_TYPES; i++) {\r\nINIT_LIST_HEAD(&comp_idle_workspace[i]);\r\nspin_lock_init(&comp_workspace_lock[i]);\r\natomic_set(&comp_alloc_workspace[i], 0);\r\ninit_waitqueue_head(&comp_workspace_wait[i]);\r\n}\r\n}\r\nstatic struct list_head *find_workspace(int type)\r\n{\r\nstruct list_head *workspace;\r\nint cpus = num_online_cpus();\r\nint idx = type - 1;\r\nstruct list_head *idle_workspace = &comp_idle_workspace[idx];\r\nspinlock_t *workspace_lock = &comp_workspace_lock[idx];\r\natomic_t *alloc_workspace = &comp_alloc_workspace[idx];\r\nwait_queue_head_t *workspace_wait = &comp_workspace_wait[idx];\r\nint *num_workspace = &comp_num_workspace[idx];\r\nagain:\r\nspin_lock(workspace_lock);\r\nif (!list_empty(idle_workspace)) {\r\nworkspace = idle_workspace->next;\r\nlist_del(workspace);\r\n(*num_workspace)--;\r\nspin_unlock(workspace_lock);\r\nreturn workspace;\r\n}\r\nif (atomic_read(alloc_workspace) > cpus) {\r\nDEFINE_WAIT(wait);\r\nspin_unlock(workspace_lock);\r\nprepare_to_wait(workspace_wait, &wait, TASK_UNINTERRUPTIBLE);\r\nif (atomic_read(alloc_workspace) > cpus && !*num_workspace)\r\nschedule();\r\nfinish_wait(workspace_wait, &wait);\r\ngoto again;\r\n}\r\natomic_inc(alloc_workspace);\r\nspin_unlock(workspace_lock);\r\nworkspace = btrfs_compress_op[idx]->alloc_workspace();\r\nif (IS_ERR(workspace)) {\r\natomic_dec(alloc_workspace);\r\nwake_up(workspace_wait);\r\n}\r\nreturn workspace;\r\n}\r\nstatic void free_workspace(int type, struct list_head *workspace)\r\n{\r\nint idx = type - 1;\r\nstruct list_head *idle_workspace = &comp_idle_workspace[idx];\r\nspinlock_t *workspace_lock = &comp_workspace_lock[idx];\r\natomic_t *alloc_workspace = &comp_alloc_workspace[idx];\r\nwait_queue_head_t *workspace_wait = &comp_workspace_wait[idx];\r\nint *num_workspace = &comp_num_workspace[idx];\r\nspin_lock(workspace_lock);\r\nif (*num_workspace < num_online_cpus()) {\r\nlist_add(workspace, idle_workspace);\r\n(*num_workspace)++;\r\nspin_unlock(workspace_lock);\r\ngoto wake;\r\n}\r\nspin_unlock(workspace_lock);\r\nbtrfs_compress_op[idx]->free_workspace(workspace);\r\natomic_dec(alloc_workspace);\r\nwake:\r\nsmp_mb();\r\nif (waitqueue_active(workspace_wait))\r\nwake_up(workspace_wait);\r\n}\r\nstatic void free_workspaces(void)\r\n{\r\nstruct list_head *workspace;\r\nint i;\r\nfor (i = 0; i < BTRFS_COMPRESS_TYPES; i++) {\r\nwhile (!list_empty(&comp_idle_workspace[i])) {\r\nworkspace = comp_idle_workspace[i].next;\r\nlist_del(workspace);\r\nbtrfs_compress_op[i]->free_workspace(workspace);\r\natomic_dec(&comp_alloc_workspace[i]);\r\n}\r\n}\r\n}\r\nint btrfs_compress_pages(int type, struct address_space *mapping,\r\nu64 start, unsigned long len,\r\nstruct page **pages,\r\nunsigned long nr_dest_pages,\r\nunsigned long *out_pages,\r\nunsigned long *total_in,\r\nunsigned long *total_out,\r\nunsigned long max_out)\r\n{\r\nstruct list_head *workspace;\r\nint ret;\r\nworkspace = find_workspace(type);\r\nif (IS_ERR(workspace))\r\nreturn PTR_ERR(workspace);\r\nret = btrfs_compress_op[type-1]->compress_pages(workspace, mapping,\r\nstart, len, pages,\r\nnr_dest_pages, out_pages,\r\ntotal_in, total_out,\r\nmax_out);\r\nfree_workspace(type, workspace);\r\nreturn ret;\r\n}\r\nstatic int btrfs_decompress_biovec(int type, struct page **pages_in,\r\nu64 disk_start, struct bio_vec *bvec,\r\nint vcnt, size_t srclen)\r\n{\r\nstruct list_head *workspace;\r\nint ret;\r\nworkspace = find_workspace(type);\r\nif (IS_ERR(workspace))\r\nreturn PTR_ERR(workspace);\r\nret = btrfs_compress_op[type-1]->decompress_biovec(workspace, pages_in,\r\ndisk_start,\r\nbvec, vcnt, srclen);\r\nfree_workspace(type, workspace);\r\nreturn ret;\r\n}\r\nint btrfs_decompress(int type, unsigned char *data_in, struct page *dest_page,\r\nunsigned long start_byte, size_t srclen, size_t destlen)\r\n{\r\nstruct list_head *workspace;\r\nint ret;\r\nworkspace = find_workspace(type);\r\nif (IS_ERR(workspace))\r\nreturn PTR_ERR(workspace);\r\nret = btrfs_compress_op[type-1]->decompress(workspace, data_in,\r\ndest_page, start_byte,\r\nsrclen, destlen);\r\nfree_workspace(type, workspace);\r\nreturn ret;\r\n}\r\nvoid btrfs_exit_compress(void)\r\n{\r\nfree_workspaces();\r\n}\r\nint btrfs_decompress_buf2page(char *buf, unsigned long buf_start,\r\nunsigned long total_out, u64 disk_start,\r\nstruct bio_vec *bvec, int vcnt,\r\nunsigned long *pg_index,\r\nunsigned long *pg_offset)\r\n{\r\nunsigned long buf_offset;\r\nunsigned long current_buf_start;\r\nunsigned long start_byte;\r\nunsigned long working_bytes = total_out - buf_start;\r\nunsigned long bytes;\r\nchar *kaddr;\r\nstruct page *page_out = bvec[*pg_index].bv_page;\r\nstart_byte = page_offset(page_out) - disk_start;\r\nif (total_out <= start_byte)\r\nreturn 1;\r\nif (total_out > start_byte && buf_start < start_byte) {\r\nbuf_offset = start_byte - buf_start;\r\nworking_bytes -= buf_offset;\r\n} else {\r\nbuf_offset = 0;\r\n}\r\ncurrent_buf_start = buf_start;\r\nwhile (working_bytes > 0) {\r\nbytes = min(PAGE_CACHE_SIZE - *pg_offset,\r\nPAGE_CACHE_SIZE - buf_offset);\r\nbytes = min(bytes, working_bytes);\r\nkaddr = kmap_atomic(page_out);\r\nmemcpy(kaddr + *pg_offset, buf + buf_offset, bytes);\r\nif (*pg_index == (vcnt - 1) && *pg_offset == 0)\r\nmemset(kaddr + bytes, 0, PAGE_CACHE_SIZE - bytes);\r\nkunmap_atomic(kaddr);\r\nflush_dcache_page(page_out);\r\n*pg_offset += bytes;\r\nbuf_offset += bytes;\r\nworking_bytes -= bytes;\r\ncurrent_buf_start += bytes;\r\nif (*pg_offset == PAGE_CACHE_SIZE) {\r\n(*pg_index)++;\r\nif (*pg_index >= vcnt)\r\nreturn 0;\r\npage_out = bvec[*pg_index].bv_page;\r\n*pg_offset = 0;\r\nstart_byte = page_offset(page_out) - disk_start;\r\nif (total_out <= start_byte)\r\nreturn 1;\r\nif (total_out > start_byte &&\r\ncurrent_buf_start < start_byte) {\r\nbuf_offset = start_byte - buf_start;\r\nworking_bytes = total_out - start_byte;\r\ncurrent_buf_start = buf_start + buf_offset;\r\n}\r\n}\r\n}\r\nreturn 1;\r\n}
