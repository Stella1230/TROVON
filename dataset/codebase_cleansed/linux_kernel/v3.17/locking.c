void btrfs_set_lock_blocking_rw(struct extent_buffer *eb, int rw)\r\n{\r\nif (eb->lock_nested && current->pid == eb->lock_owner)\r\nreturn;\r\nif (rw == BTRFS_WRITE_LOCK) {\r\nif (atomic_read(&eb->blocking_writers) == 0) {\r\nWARN_ON(atomic_read(&eb->spinning_writers) != 1);\r\natomic_dec(&eb->spinning_writers);\r\nbtrfs_assert_tree_locked(eb);\r\natomic_inc(&eb->blocking_writers);\r\nwrite_unlock(&eb->lock);\r\n}\r\n} else if (rw == BTRFS_READ_LOCK) {\r\nbtrfs_assert_tree_read_locked(eb);\r\natomic_inc(&eb->blocking_readers);\r\nWARN_ON(atomic_read(&eb->spinning_readers) == 0);\r\natomic_dec(&eb->spinning_readers);\r\nread_unlock(&eb->lock);\r\n}\r\nreturn;\r\n}\r\nvoid btrfs_clear_lock_blocking_rw(struct extent_buffer *eb, int rw)\r\n{\r\nif (eb->lock_nested && current->pid == eb->lock_owner)\r\nreturn;\r\nif (rw == BTRFS_WRITE_LOCK_BLOCKING) {\r\nBUG_ON(atomic_read(&eb->blocking_writers) != 1);\r\nwrite_lock(&eb->lock);\r\nWARN_ON(atomic_read(&eb->spinning_writers));\r\natomic_inc(&eb->spinning_writers);\r\nif (atomic_dec_and_test(&eb->blocking_writers) &&\r\nwaitqueue_active(&eb->write_lock_wq))\r\nwake_up(&eb->write_lock_wq);\r\n} else if (rw == BTRFS_READ_LOCK_BLOCKING) {\r\nBUG_ON(atomic_read(&eb->blocking_readers) == 0);\r\nread_lock(&eb->lock);\r\natomic_inc(&eb->spinning_readers);\r\nif (atomic_dec_and_test(&eb->blocking_readers) &&\r\nwaitqueue_active(&eb->read_lock_wq))\r\nwake_up(&eb->read_lock_wq);\r\n}\r\nreturn;\r\n}\r\nvoid btrfs_tree_read_lock(struct extent_buffer *eb)\r\n{\r\nagain:\r\nBUG_ON(!atomic_read(&eb->blocking_writers) &&\r\ncurrent->pid == eb->lock_owner);\r\nread_lock(&eb->lock);\r\nif (atomic_read(&eb->blocking_writers) &&\r\ncurrent->pid == eb->lock_owner) {\r\nBUG_ON(eb->lock_nested);\r\neb->lock_nested = 1;\r\nread_unlock(&eb->lock);\r\nreturn;\r\n}\r\nif (atomic_read(&eb->blocking_writers)) {\r\nread_unlock(&eb->lock);\r\nwait_event(eb->write_lock_wq,\r\natomic_read(&eb->blocking_writers) == 0);\r\ngoto again;\r\n}\r\natomic_inc(&eb->read_locks);\r\natomic_inc(&eb->spinning_readers);\r\n}\r\nint btrfs_try_tree_read_lock(struct extent_buffer *eb)\r\n{\r\nif (atomic_read(&eb->blocking_writers))\r\nreturn 0;\r\nif (!read_trylock(&eb->lock))\r\nreturn 0;\r\nif (atomic_read(&eb->blocking_writers)) {\r\nread_unlock(&eb->lock);\r\nreturn 0;\r\n}\r\natomic_inc(&eb->read_locks);\r\natomic_inc(&eb->spinning_readers);\r\nreturn 1;\r\n}\r\nint btrfs_try_tree_write_lock(struct extent_buffer *eb)\r\n{\r\nif (atomic_read(&eb->blocking_writers) ||\r\natomic_read(&eb->blocking_readers))\r\nreturn 0;\r\nif (!write_trylock(&eb->lock))\r\nreturn 0;\r\nif (atomic_read(&eb->blocking_writers) ||\r\natomic_read(&eb->blocking_readers)) {\r\nwrite_unlock(&eb->lock);\r\nreturn 0;\r\n}\r\natomic_inc(&eb->write_locks);\r\natomic_inc(&eb->spinning_writers);\r\neb->lock_owner = current->pid;\r\nreturn 1;\r\n}\r\nvoid btrfs_tree_read_unlock(struct extent_buffer *eb)\r\n{\r\nif (eb->lock_nested && current->pid == eb->lock_owner) {\r\neb->lock_nested = 0;\r\nreturn;\r\n}\r\nbtrfs_assert_tree_read_locked(eb);\r\nWARN_ON(atomic_read(&eb->spinning_readers) == 0);\r\natomic_dec(&eb->spinning_readers);\r\natomic_dec(&eb->read_locks);\r\nread_unlock(&eb->lock);\r\n}\r\nvoid btrfs_tree_read_unlock_blocking(struct extent_buffer *eb)\r\n{\r\nif (eb->lock_nested && current->pid == eb->lock_owner) {\r\neb->lock_nested = 0;\r\nreturn;\r\n}\r\nbtrfs_assert_tree_read_locked(eb);\r\nWARN_ON(atomic_read(&eb->blocking_readers) == 0);\r\nif (atomic_dec_and_test(&eb->blocking_readers) &&\r\nwaitqueue_active(&eb->read_lock_wq))\r\nwake_up(&eb->read_lock_wq);\r\natomic_dec(&eb->read_locks);\r\n}\r\nvoid btrfs_tree_lock(struct extent_buffer *eb)\r\n{\r\nagain:\r\nwait_event(eb->read_lock_wq, atomic_read(&eb->blocking_readers) == 0);\r\nwait_event(eb->write_lock_wq, atomic_read(&eb->blocking_writers) == 0);\r\nwrite_lock(&eb->lock);\r\nif (atomic_read(&eb->blocking_readers)) {\r\nwrite_unlock(&eb->lock);\r\nwait_event(eb->read_lock_wq,\r\natomic_read(&eb->blocking_readers) == 0);\r\ngoto again;\r\n}\r\nif (atomic_read(&eb->blocking_writers)) {\r\nwrite_unlock(&eb->lock);\r\nwait_event(eb->write_lock_wq,\r\natomic_read(&eb->blocking_writers) == 0);\r\ngoto again;\r\n}\r\nWARN_ON(atomic_read(&eb->spinning_writers));\r\natomic_inc(&eb->spinning_writers);\r\natomic_inc(&eb->write_locks);\r\neb->lock_owner = current->pid;\r\n}\r\nvoid btrfs_tree_unlock(struct extent_buffer *eb)\r\n{\r\nint blockers = atomic_read(&eb->blocking_writers);\r\nBUG_ON(blockers > 1);\r\nbtrfs_assert_tree_locked(eb);\r\neb->lock_owner = 0;\r\natomic_dec(&eb->write_locks);\r\nif (blockers) {\r\nWARN_ON(atomic_read(&eb->spinning_writers));\r\natomic_dec(&eb->blocking_writers);\r\nsmp_mb();\r\nif (waitqueue_active(&eb->write_lock_wq))\r\nwake_up(&eb->write_lock_wq);\r\n} else {\r\nWARN_ON(atomic_read(&eb->spinning_writers) != 1);\r\natomic_dec(&eb->spinning_writers);\r\nwrite_unlock(&eb->lock);\r\n}\r\n}\r\nvoid btrfs_assert_tree_locked(struct extent_buffer *eb)\r\n{\r\nBUG_ON(!atomic_read(&eb->write_locks));\r\n}\r\nstatic void btrfs_assert_tree_read_locked(struct extent_buffer *eb)\r\n{\r\nBUG_ON(!atomic_read(&eb->read_locks));\r\n}
