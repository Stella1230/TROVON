static inline void count_compact_event(enum vm_event_item item)\r\n{\r\ncount_vm_event(item);\r\n}\r\nstatic inline void count_compact_events(enum vm_event_item item, long delta)\r\n{\r\ncount_vm_events(item, delta);\r\n}\r\nstatic unsigned long release_freepages(struct list_head *freelist)\r\n{\r\nstruct page *page, *next;\r\nunsigned long count = 0;\r\nlist_for_each_entry_safe(page, next, freelist, lru) {\r\nlist_del(&page->lru);\r\n__free_page(page);\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nstatic void map_pages(struct list_head *list)\r\n{\r\nstruct page *page;\r\nlist_for_each_entry(page, list, lru) {\r\narch_alloc_page(page, 0);\r\nkernel_map_pages(page, 1, 1);\r\n}\r\n}\r\nstatic inline bool migrate_async_suitable(int migratetype)\r\n{\r\nreturn is_migrate_cma(migratetype) || migratetype == MIGRATE_MOVABLE;\r\n}\r\nstatic inline bool isolation_suitable(struct compact_control *cc,\r\nstruct page *page)\r\n{\r\nif (cc->ignore_skip_hint)\r\nreturn true;\r\nreturn !get_pageblock_skip(page);\r\n}\r\nstatic void __reset_isolation_suitable(struct zone *zone)\r\n{\r\nunsigned long start_pfn = zone->zone_start_pfn;\r\nunsigned long end_pfn = zone_end_pfn(zone);\r\nunsigned long pfn;\r\nzone->compact_cached_migrate_pfn[0] = start_pfn;\r\nzone->compact_cached_migrate_pfn[1] = start_pfn;\r\nzone->compact_cached_free_pfn = end_pfn;\r\nzone->compact_blockskip_flush = false;\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {\r\nstruct page *page;\r\ncond_resched();\r\nif (!pfn_valid(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (zone != page_zone(page))\r\ncontinue;\r\nclear_pageblock_skip(page);\r\n}\r\n}\r\nvoid reset_isolation_suitable(pg_data_t *pgdat)\r\n{\r\nint zoneid;\r\nfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\r\nstruct zone *zone = &pgdat->node_zones[zoneid];\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (zone->compact_blockskip_flush)\r\n__reset_isolation_suitable(zone);\r\n}\r\n}\r\nstatic void update_pageblock_skip(struct compact_control *cc,\r\nstruct page *page, unsigned long nr_isolated,\r\nbool set_unsuitable, bool migrate_scanner)\r\n{\r\nstruct zone *zone = cc->zone;\r\nunsigned long pfn;\r\nif (cc->ignore_skip_hint)\r\nreturn;\r\nif (!page)\r\nreturn;\r\nif (nr_isolated)\r\nreturn;\r\nif (set_unsuitable)\r\nset_pageblock_skip(page);\r\npfn = page_to_pfn(page);\r\nif (migrate_scanner) {\r\nif (cc->finished_update_migrate)\r\nreturn;\r\nif (pfn > zone->compact_cached_migrate_pfn[0])\r\nzone->compact_cached_migrate_pfn[0] = pfn;\r\nif (cc->mode != MIGRATE_ASYNC &&\r\npfn > zone->compact_cached_migrate_pfn[1])\r\nzone->compact_cached_migrate_pfn[1] = pfn;\r\n} else {\r\nif (cc->finished_update_free)\r\nreturn;\r\nif (pfn < zone->compact_cached_free_pfn)\r\nzone->compact_cached_free_pfn = pfn;\r\n}\r\n}\r\nstatic inline bool isolation_suitable(struct compact_control *cc,\r\nstruct page *page)\r\n{\r\nreturn true;\r\n}\r\nstatic void update_pageblock_skip(struct compact_control *cc,\r\nstruct page *page, unsigned long nr_isolated,\r\nbool set_unsuitable, bool migrate_scanner)\r\n{\r\n}\r\nstatic inline bool should_release_lock(spinlock_t *lock)\r\n{\r\nreturn need_resched() || spin_is_contended(lock);\r\n}\r\nstatic bool compact_checklock_irqsave(spinlock_t *lock, unsigned long *flags,\r\nbool locked, struct compact_control *cc)\r\n{\r\nif (should_release_lock(lock)) {\r\nif (locked) {\r\nspin_unlock_irqrestore(lock, *flags);\r\nlocked = false;\r\n}\r\nif (cc->mode == MIGRATE_ASYNC) {\r\ncc->contended = true;\r\nreturn false;\r\n}\r\ncond_resched();\r\n}\r\nif (!locked)\r\nspin_lock_irqsave(lock, *flags);\r\nreturn true;\r\n}\r\nstatic inline bool compact_should_abort(struct compact_control *cc)\r\n{\r\nif (need_resched()) {\r\nif (cc->mode == MIGRATE_ASYNC) {\r\ncc->contended = true;\r\nreturn true;\r\n}\r\ncond_resched();\r\n}\r\nreturn false;\r\n}\r\nstatic bool suitable_migration_target(struct page *page)\r\n{\r\nif (PageBuddy(page) && page_order(page) >= pageblock_order)\r\nreturn false;\r\nif (migrate_async_suitable(get_pageblock_migratetype(page)))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\r\nunsigned long blockpfn,\r\nunsigned long end_pfn,\r\nstruct list_head *freelist,\r\nbool strict)\r\n{\r\nint nr_scanned = 0, total_isolated = 0;\r\nstruct page *cursor, *valid_page = NULL;\r\nunsigned long flags;\r\nbool locked = false;\r\nbool checked_pageblock = false;\r\ncursor = pfn_to_page(blockpfn);\r\nfor (; blockpfn < end_pfn; blockpfn++, cursor++) {\r\nint isolated, i;\r\nstruct page *page = cursor;\r\nnr_scanned++;\r\nif (!pfn_valid_within(blockpfn))\r\ngoto isolate_fail;\r\nif (!valid_page)\r\nvalid_page = page;\r\nif (!PageBuddy(page))\r\ngoto isolate_fail;\r\nlocked = compact_checklock_irqsave(&cc->zone->lock, &flags,\r\nlocked, cc);\r\nif (!locked)\r\nbreak;\r\nif (!strict && !checked_pageblock) {\r\nchecked_pageblock = true;\r\nif (!suitable_migration_target(page))\r\nbreak;\r\n}\r\nif (!PageBuddy(page))\r\ngoto isolate_fail;\r\nisolated = split_free_page(page);\r\ntotal_isolated += isolated;\r\nfor (i = 0; i < isolated; i++) {\r\nlist_add(&page->lru, freelist);\r\npage++;\r\n}\r\nif (isolated) {\r\nblockpfn += isolated - 1;\r\ncursor += isolated - 1;\r\ncontinue;\r\n}\r\nisolate_fail:\r\nif (strict)\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\ntrace_mm_compaction_isolate_freepages(nr_scanned, total_isolated);\r\nif (strict && blockpfn < end_pfn)\r\ntotal_isolated = 0;\r\nif (locked)\r\nspin_unlock_irqrestore(&cc->zone->lock, flags);\r\nif (blockpfn == end_pfn)\r\nupdate_pageblock_skip(cc, valid_page, total_isolated, true,\r\nfalse);\r\ncount_compact_events(COMPACTFREE_SCANNED, nr_scanned);\r\nif (total_isolated)\r\ncount_compact_events(COMPACTISOLATED, total_isolated);\r\nreturn total_isolated;\r\n}\r\nunsigned long\r\nisolate_freepages_range(struct compact_control *cc,\r\nunsigned long start_pfn, unsigned long end_pfn)\r\n{\r\nunsigned long isolated, pfn, block_end_pfn;\r\nLIST_HEAD(freelist);\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn += isolated) {\r\nif (!pfn_valid(pfn) || cc->zone != page_zone(pfn_to_page(pfn)))\r\nbreak;\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\nisolated = isolate_freepages_block(cc, pfn, block_end_pfn,\r\n&freelist, true);\r\nif (!isolated)\r\nbreak;\r\n}\r\nmap_pages(&freelist);\r\nif (pfn < end_pfn) {\r\nrelease_freepages(&freelist);\r\nreturn 0;\r\n}\r\nreturn pfn;\r\n}\r\nstatic void acct_isolated(struct zone *zone, bool locked, struct compact_control *cc)\r\n{\r\nstruct page *page;\r\nunsigned int count[2] = { 0, };\r\nlist_for_each_entry(page, &cc->migratepages, lru)\r\ncount[!!page_is_file_cache(page)]++;\r\nif (locked) {\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON, count[0]);\r\n__mod_zone_page_state(zone, NR_ISOLATED_FILE, count[1]);\r\n} else {\r\nmod_zone_page_state(zone, NR_ISOLATED_ANON, count[0]);\r\nmod_zone_page_state(zone, NR_ISOLATED_FILE, count[1]);\r\n}\r\n}\r\nstatic bool too_many_isolated(struct zone *zone)\r\n{\r\nunsigned long active, inactive, isolated;\r\ninactive = zone_page_state(zone, NR_INACTIVE_FILE) +\r\nzone_page_state(zone, NR_INACTIVE_ANON);\r\nactive = zone_page_state(zone, NR_ACTIVE_FILE) +\r\nzone_page_state(zone, NR_ACTIVE_ANON);\r\nisolated = zone_page_state(zone, NR_ISOLATED_FILE) +\r\nzone_page_state(zone, NR_ISOLATED_ANON);\r\nreturn isolated > (inactive + active) / 2;\r\n}\r\nunsigned long\r\nisolate_migratepages_range(struct zone *zone, struct compact_control *cc,\r\nunsigned long low_pfn, unsigned long end_pfn, bool unevictable)\r\n{\r\nunsigned long last_pageblock_nr = 0, pageblock_nr;\r\nunsigned long nr_scanned = 0, nr_isolated = 0;\r\nstruct list_head *migratelist = &cc->migratepages;\r\nstruct lruvec *lruvec;\r\nunsigned long flags;\r\nbool locked = false;\r\nstruct page *page = NULL, *valid_page = NULL;\r\nbool set_unsuitable = true;\r\nconst isolate_mode_t mode = (cc->mode == MIGRATE_ASYNC ?\r\nISOLATE_ASYNC_MIGRATE : 0) |\r\n(unevictable ? ISOLATE_UNEVICTABLE : 0);\r\nwhile (unlikely(too_many_isolated(zone))) {\r\nif (cc->mode == MIGRATE_ASYNC)\r\nreturn 0;\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\nif (fatal_signal_pending(current))\r\nreturn 0;\r\n}\r\nif (compact_should_abort(cc))\r\nreturn 0;\r\nfor (; low_pfn < end_pfn; low_pfn++) {\r\nif (locked && !(low_pfn % SWAP_CLUSTER_MAX)) {\r\nif (should_release_lock(&zone->lru_lock)) {\r\nspin_unlock_irqrestore(&zone->lru_lock, flags);\r\nlocked = false;\r\n}\r\n}\r\nif ((low_pfn & (MAX_ORDER_NR_PAGES - 1)) == 0) {\r\nif (!pfn_valid(low_pfn)) {\r\nlow_pfn += MAX_ORDER_NR_PAGES - 1;\r\ncontinue;\r\n}\r\n}\r\nif (!pfn_valid_within(low_pfn))\r\ncontinue;\r\nnr_scanned++;\r\npage = pfn_to_page(low_pfn);\r\nif (page_zone(page) != zone)\r\ncontinue;\r\nif (!valid_page)\r\nvalid_page = page;\r\npageblock_nr = low_pfn >> pageblock_order;\r\nif (last_pageblock_nr != pageblock_nr) {\r\nint mt;\r\nlast_pageblock_nr = pageblock_nr;\r\nif (!isolation_suitable(cc, page))\r\ngoto next_pageblock;\r\nmt = get_pageblock_migratetype(page);\r\nif (cc->mode == MIGRATE_ASYNC &&\r\n!migrate_async_suitable(mt)) {\r\nset_unsuitable = false;\r\ngoto next_pageblock;\r\n}\r\n}\r\nif (PageBuddy(page))\r\ncontinue;\r\nif (!PageLRU(page)) {\r\nif (unlikely(balloon_page_movable(page))) {\r\nif (locked && balloon_page_isolate(page)) {\r\ngoto isolate_success;\r\n}\r\n}\r\ncontinue;\r\n}\r\nif (PageTransHuge(page)) {\r\nif (!locked)\r\ngoto next_pageblock;\r\nlow_pfn += (1 << compound_order(page)) - 1;\r\ncontinue;\r\n}\r\nif (!page_mapping(page) &&\r\npage_count(page) > page_mapcount(page))\r\ncontinue;\r\nlocked = compact_checklock_irqsave(&zone->lru_lock, &flags,\r\nlocked, cc);\r\nif (!locked || fatal_signal_pending(current))\r\nbreak;\r\nif (!PageLRU(page))\r\ncontinue;\r\nif (PageTransHuge(page)) {\r\nlow_pfn += (1 << compound_order(page)) - 1;\r\ncontinue;\r\n}\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nif (__isolate_lru_page(page, mode) != 0)\r\ncontinue;\r\nVM_BUG_ON_PAGE(PageTransCompound(page), page);\r\ndel_page_from_lru_list(page, lruvec, page_lru(page));\r\nisolate_success:\r\ncc->finished_update_migrate = true;\r\nlist_add(&page->lru, migratelist);\r\ncc->nr_migratepages++;\r\nnr_isolated++;\r\nif (cc->nr_migratepages == COMPACT_CLUSTER_MAX) {\r\n++low_pfn;\r\nbreak;\r\n}\r\ncontinue;\r\nnext_pageblock:\r\nlow_pfn = ALIGN(low_pfn + 1, pageblock_nr_pages) - 1;\r\n}\r\nacct_isolated(zone, locked, cc);\r\nif (locked)\r\nspin_unlock_irqrestore(&zone->lru_lock, flags);\r\nif (low_pfn == end_pfn)\r\nupdate_pageblock_skip(cc, valid_page, nr_isolated,\r\nset_unsuitable, true);\r\ntrace_mm_compaction_isolate_migratepages(nr_scanned, nr_isolated);\r\ncount_compact_events(COMPACTMIGRATE_SCANNED, nr_scanned);\r\nif (nr_isolated)\r\ncount_compact_events(COMPACTISOLATED, nr_isolated);\r\nreturn low_pfn;\r\n}\r\nstatic void isolate_freepages(struct zone *zone,\r\nstruct compact_control *cc)\r\n{\r\nstruct page *page;\r\nunsigned long block_start_pfn;\r\nunsigned long block_end_pfn;\r\nunsigned long low_pfn;\r\nint nr_freepages = cc->nr_freepages;\r\nstruct list_head *freelist = &cc->freepages;\r\nblock_start_pfn = cc->free_pfn & ~(pageblock_nr_pages-1);\r\nblock_end_pfn = min(block_start_pfn + pageblock_nr_pages,\r\nzone_end_pfn(zone));\r\nlow_pfn = ALIGN(cc->migrate_pfn + 1, pageblock_nr_pages);\r\nfor (; block_start_pfn >= low_pfn && cc->nr_migratepages > nr_freepages;\r\nblock_end_pfn = block_start_pfn,\r\nblock_start_pfn -= pageblock_nr_pages) {\r\nunsigned long isolated;\r\nif (!(block_start_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages))\r\n&& compact_should_abort(cc))\r\nbreak;\r\nif (!pfn_valid(block_start_pfn))\r\ncontinue;\r\npage = pfn_to_page(block_start_pfn);\r\nif (page_zone(page) != zone)\r\ncontinue;\r\nif (!suitable_migration_target(page))\r\ncontinue;\r\nif (!isolation_suitable(cc, page))\r\ncontinue;\r\ncc->free_pfn = block_start_pfn;\r\nisolated = isolate_freepages_block(cc, block_start_pfn,\r\nblock_end_pfn, freelist, false);\r\nnr_freepages += isolated;\r\nif (isolated)\r\ncc->finished_update_free = true;\r\nif (cc->contended)\r\nbreak;\r\n}\r\nmap_pages(freelist);\r\nif (block_start_pfn < low_pfn)\r\ncc->free_pfn = cc->migrate_pfn;\r\ncc->nr_freepages = nr_freepages;\r\n}\r\nstatic struct page *compaction_alloc(struct page *migratepage,\r\nunsigned long data,\r\nint **result)\r\n{\r\nstruct compact_control *cc = (struct compact_control *)data;\r\nstruct page *freepage;\r\nif (list_empty(&cc->freepages)) {\r\nif (!cc->contended)\r\nisolate_freepages(cc->zone, cc);\r\nif (list_empty(&cc->freepages))\r\nreturn NULL;\r\n}\r\nfreepage = list_entry(cc->freepages.next, struct page, lru);\r\nlist_del(&freepage->lru);\r\ncc->nr_freepages--;\r\nreturn freepage;\r\n}\r\nstatic void compaction_free(struct page *page, unsigned long data)\r\n{\r\nstruct compact_control *cc = (struct compact_control *)data;\r\nlist_add(&page->lru, &cc->freepages);\r\ncc->nr_freepages++;\r\n}\r\nstatic isolate_migrate_t isolate_migratepages(struct zone *zone,\r\nstruct compact_control *cc)\r\n{\r\nunsigned long low_pfn, end_pfn;\r\nlow_pfn = max(cc->migrate_pfn, zone->zone_start_pfn);\r\nend_pfn = ALIGN(low_pfn + 1, pageblock_nr_pages);\r\nif (end_pfn > cc->free_pfn || !pfn_valid(low_pfn)) {\r\ncc->migrate_pfn = end_pfn;\r\nreturn ISOLATE_NONE;\r\n}\r\nlow_pfn = isolate_migratepages_range(zone, cc, low_pfn, end_pfn, false);\r\nif (!low_pfn || cc->contended)\r\nreturn ISOLATE_ABORT;\r\ncc->migrate_pfn = low_pfn;\r\nreturn ISOLATE_SUCCESS;\r\n}\r\nstatic int compact_finished(struct zone *zone,\r\nstruct compact_control *cc)\r\n{\r\nunsigned int order;\r\nunsigned long watermark;\r\nif (cc->contended || fatal_signal_pending(current))\r\nreturn COMPACT_PARTIAL;\r\nif (cc->free_pfn <= cc->migrate_pfn) {\r\nzone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;\r\nzone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;\r\nzone->compact_cached_free_pfn = zone_end_pfn(zone);\r\nif (!current_is_kswapd())\r\nzone->compact_blockskip_flush = true;\r\nreturn COMPACT_COMPLETE;\r\n}\r\nif (cc->order == -1)\r\nreturn COMPACT_CONTINUE;\r\nwatermark = low_wmark_pages(zone);\r\nwatermark += (1 << cc->order);\r\nif (!zone_watermark_ok(zone, cc->order, watermark, 0, 0))\r\nreturn COMPACT_CONTINUE;\r\nfor (order = cc->order; order < MAX_ORDER; order++) {\r\nstruct free_area *area = &zone->free_area[order];\r\nif (!list_empty(&area->free_list[cc->migratetype]))\r\nreturn COMPACT_PARTIAL;\r\nif (cc->order >= pageblock_order && area->nr_free)\r\nreturn COMPACT_PARTIAL;\r\n}\r\nreturn COMPACT_CONTINUE;\r\n}\r\nunsigned long compaction_suitable(struct zone *zone, int order)\r\n{\r\nint fragindex;\r\nunsigned long watermark;\r\nif (order == -1)\r\nreturn COMPACT_CONTINUE;\r\nwatermark = low_wmark_pages(zone) + (2UL << order);\r\nif (!zone_watermark_ok(zone, 0, watermark, 0, 0))\r\nreturn COMPACT_SKIPPED;\r\nfragindex = fragmentation_index(zone, order);\r\nif (fragindex >= 0 && fragindex <= sysctl_extfrag_threshold)\r\nreturn COMPACT_SKIPPED;\r\nif (fragindex == -1000 && zone_watermark_ok(zone, order, watermark,\r\n0, 0))\r\nreturn COMPACT_PARTIAL;\r\nreturn COMPACT_CONTINUE;\r\n}\r\nstatic int compact_zone(struct zone *zone, struct compact_control *cc)\r\n{\r\nint ret;\r\nunsigned long start_pfn = zone->zone_start_pfn;\r\nunsigned long end_pfn = zone_end_pfn(zone);\r\nconst bool sync = cc->mode != MIGRATE_ASYNC;\r\nret = compaction_suitable(zone, cc->order);\r\nswitch (ret) {\r\ncase COMPACT_PARTIAL:\r\ncase COMPACT_SKIPPED:\r\nreturn ret;\r\ncase COMPACT_CONTINUE:\r\n;\r\n}\r\nif (compaction_restarting(zone, cc->order) && !current_is_kswapd())\r\n__reset_isolation_suitable(zone);\r\ncc->migrate_pfn = zone->compact_cached_migrate_pfn[sync];\r\ncc->free_pfn = zone->compact_cached_free_pfn;\r\nif (cc->free_pfn < start_pfn || cc->free_pfn > end_pfn) {\r\ncc->free_pfn = end_pfn & ~(pageblock_nr_pages-1);\r\nzone->compact_cached_free_pfn = cc->free_pfn;\r\n}\r\nif (cc->migrate_pfn < start_pfn || cc->migrate_pfn > end_pfn) {\r\ncc->migrate_pfn = start_pfn;\r\nzone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;\r\nzone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;\r\n}\r\ntrace_mm_compaction_begin(start_pfn, cc->migrate_pfn, cc->free_pfn, end_pfn);\r\nmigrate_prep_local();\r\nwhile ((ret = compact_finished(zone, cc)) == COMPACT_CONTINUE) {\r\nint err;\r\nswitch (isolate_migratepages(zone, cc)) {\r\ncase ISOLATE_ABORT:\r\nret = COMPACT_PARTIAL;\r\nputback_movable_pages(&cc->migratepages);\r\ncc->nr_migratepages = 0;\r\ngoto out;\r\ncase ISOLATE_NONE:\r\ncontinue;\r\ncase ISOLATE_SUCCESS:\r\n;\r\n}\r\nif (!cc->nr_migratepages)\r\ncontinue;\r\nerr = migrate_pages(&cc->migratepages, compaction_alloc,\r\ncompaction_free, (unsigned long)cc, cc->mode,\r\nMR_COMPACTION);\r\ntrace_mm_compaction_migratepages(cc->nr_migratepages, err,\r\n&cc->migratepages);\r\ncc->nr_migratepages = 0;\r\nif (err) {\r\nputback_movable_pages(&cc->migratepages);\r\nif (err == -ENOMEM && cc->free_pfn > cc->migrate_pfn) {\r\nret = COMPACT_PARTIAL;\r\ngoto out;\r\n}\r\n}\r\n}\r\nout:\r\ncc->nr_freepages -= release_freepages(&cc->freepages);\r\nVM_BUG_ON(cc->nr_freepages != 0);\r\ntrace_mm_compaction_end(ret);\r\nreturn ret;\r\n}\r\nstatic unsigned long compact_zone_order(struct zone *zone, int order,\r\ngfp_t gfp_mask, enum migrate_mode mode, bool *contended)\r\n{\r\nunsigned long ret;\r\nstruct compact_control cc = {\r\n.nr_freepages = 0,\r\n.nr_migratepages = 0,\r\n.order = order,\r\n.migratetype = allocflags_to_migratetype(gfp_mask),\r\n.zone = zone,\r\n.mode = mode,\r\n};\r\nINIT_LIST_HEAD(&cc.freepages);\r\nINIT_LIST_HEAD(&cc.migratepages);\r\nret = compact_zone(zone, &cc);\r\nVM_BUG_ON(!list_empty(&cc.freepages));\r\nVM_BUG_ON(!list_empty(&cc.migratepages));\r\n*contended = cc.contended;\r\nreturn ret;\r\n}\r\nunsigned long try_to_compact_pages(struct zonelist *zonelist,\r\nint order, gfp_t gfp_mask, nodemask_t *nodemask,\r\nenum migrate_mode mode, bool *contended)\r\n{\r\nenum zone_type high_zoneidx = gfp_zone(gfp_mask);\r\nint may_enter_fs = gfp_mask & __GFP_FS;\r\nint may_perform_io = gfp_mask & __GFP_IO;\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nint rc = COMPACT_SKIPPED;\r\nint alloc_flags = 0;\r\nif (!order || !may_enter_fs || !may_perform_io)\r\nreturn rc;\r\ncount_compact_event(COMPACTSTALL);\r\n#ifdef CONFIG_CMA\r\nif (allocflags_to_migratetype(gfp_mask) == MIGRATE_MOVABLE)\r\nalloc_flags |= ALLOC_CMA;\r\n#endif\r\nfor_each_zone_zonelist_nodemask(zone, z, zonelist, high_zoneidx,\r\nnodemask) {\r\nint status;\r\nstatus = compact_zone_order(zone, order, gfp_mask, mode,\r\ncontended);\r\nrc = max(status, rc);\r\nif (zone_watermark_ok(zone, order, low_wmark_pages(zone), 0,\r\nalloc_flags))\r\nbreak;\r\n}\r\nreturn rc;\r\n}\r\nstatic void __compact_pgdat(pg_data_t *pgdat, struct compact_control *cc)\r\n{\r\nint zoneid;\r\nstruct zone *zone;\r\nfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\r\nzone = &pgdat->node_zones[zoneid];\r\nif (!populated_zone(zone))\r\ncontinue;\r\ncc->nr_freepages = 0;\r\ncc->nr_migratepages = 0;\r\ncc->zone = zone;\r\nINIT_LIST_HEAD(&cc->freepages);\r\nINIT_LIST_HEAD(&cc->migratepages);\r\nif (cc->order == -1 || !compaction_deferred(zone, cc->order))\r\ncompact_zone(zone, cc);\r\nif (cc->order > 0) {\r\nif (zone_watermark_ok(zone, cc->order,\r\nlow_wmark_pages(zone), 0, 0))\r\ncompaction_defer_reset(zone, cc->order, false);\r\n}\r\nVM_BUG_ON(!list_empty(&cc->freepages));\r\nVM_BUG_ON(!list_empty(&cc->migratepages));\r\n}\r\n}\r\nvoid compact_pgdat(pg_data_t *pgdat, int order)\r\n{\r\nstruct compact_control cc = {\r\n.order = order,\r\n.mode = MIGRATE_ASYNC,\r\n};\r\nif (!order)\r\nreturn;\r\n__compact_pgdat(pgdat, &cc);\r\n}\r\nstatic void compact_node(int nid)\r\n{\r\nstruct compact_control cc = {\r\n.order = -1,\r\n.mode = MIGRATE_SYNC,\r\n.ignore_skip_hint = true,\r\n};\r\n__compact_pgdat(NODE_DATA(nid), &cc);\r\n}\r\nstatic void compact_nodes(void)\r\n{\r\nint nid;\r\nlru_add_drain_all();\r\nfor_each_online_node(nid)\r\ncompact_node(nid);\r\n}\r\nint sysctl_compaction_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *length, loff_t *ppos)\r\n{\r\nif (write)\r\ncompact_nodes();\r\nreturn 0;\r\n}\r\nint sysctl_extfrag_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *length, loff_t *ppos)\r\n{\r\nproc_dointvec_minmax(table, write, buffer, length, ppos);\r\nreturn 0;\r\n}\r\nstatic ssize_t sysfs_compact_node(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nint nid = dev->id;\r\nif (nid >= 0 && nid < nr_node_ids && node_online(nid)) {\r\nlru_add_drain_all();\r\ncompact_node(nid);\r\n}\r\nreturn count;\r\n}\r\nint compaction_register_node(struct node *node)\r\n{\r\nreturn device_create_file(&node->dev, &dev_attr_compact);\r\n}\r\nvoid compaction_unregister_node(struct node *node)\r\n{\r\nreturn device_remove_file(&node->dev, &dev_attr_compact);\r\n}
