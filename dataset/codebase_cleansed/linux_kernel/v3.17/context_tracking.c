void context_tracking_cpu_set(int cpu)\r\n{\r\nif (!per_cpu(context_tracking.active, cpu)) {\r\nper_cpu(context_tracking.active, cpu) = true;\r\nstatic_key_slow_inc(&context_tracking_enabled);\r\n}\r\n}\r\nvoid context_tracking_user_enter(void)\r\n{\r\nunsigned long flags;\r\nif (!context_tracking_is_enabled())\r\nreturn;\r\nif (in_interrupt())\r\nreturn;\r\nWARN_ON_ONCE(!current->mm);\r\nlocal_irq_save(flags);\r\nif ( __this_cpu_read(context_tracking.state) != IN_USER) {\r\nif (__this_cpu_read(context_tracking.active)) {\r\ntrace_user_enter(0);\r\nvtime_user_enter(current);\r\nrcu_user_enter();\r\n}\r\n__this_cpu_write(context_tracking.state, IN_USER);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nasmlinkage __visible void __sched notrace preempt_schedule_context(void)\r\n{\r\nenum ctx_state prev_ctx;\r\nif (likely(!preemptible()))\r\nreturn;\r\npreempt_disable_notrace();\r\nprev_ctx = exception_enter();\r\npreempt_enable_no_resched_notrace();\r\npreempt_schedule();\r\npreempt_disable_notrace();\r\nexception_exit(prev_ctx);\r\npreempt_enable_notrace();\r\n}\r\nvoid context_tracking_user_exit(void)\r\n{\r\nunsigned long flags;\r\nif (!context_tracking_is_enabled())\r\nreturn;\r\nif (in_interrupt())\r\nreturn;\r\nlocal_irq_save(flags);\r\nif (__this_cpu_read(context_tracking.state) == IN_USER) {\r\nif (__this_cpu_read(context_tracking.active)) {\r\nrcu_user_exit();\r\nvtime_user_exit(current);\r\ntrace_user_exit(0);\r\n}\r\n__this_cpu_write(context_tracking.state, IN_KERNEL);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid __context_tracking_task_switch(struct task_struct *prev,\r\nstruct task_struct *next)\r\n{\r\nclear_tsk_thread_flag(prev, TIF_NOHZ);\r\nset_tsk_thread_flag(next, TIF_NOHZ);\r\n}\r\nvoid __init context_tracking_init(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\ncontext_tracking_cpu_set(cpu);\r\n}
