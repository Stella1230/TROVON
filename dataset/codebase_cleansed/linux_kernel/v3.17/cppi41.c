static struct cppi41_channel *to_cpp41_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct cppi41_channel, chan);\r\n}\r\nstatic struct cppi41_channel *desc_to_chan(struct cppi41_dd *cdd, u32 desc)\r\n{\r\nstruct cppi41_channel *c;\r\nu32 descs_size;\r\nu32 desc_num;\r\ndescs_size = sizeof(struct cppi41_desc) * ALLOC_DECS_NUM;\r\nif (!((desc >= cdd->descs_phys) &&\r\n(desc < (cdd->descs_phys + descs_size)))) {\r\nreturn NULL;\r\n}\r\ndesc_num = (desc - cdd->descs_phys) / sizeof(struct cppi41_desc);\r\nBUG_ON(desc_num >= ALLOC_DECS_NUM);\r\nc = cdd->chan_busy[desc_num];\r\ncdd->chan_busy[desc_num] = NULL;\r\nreturn c;\r\n}\r\nstatic void cppi_writel(u32 val, void *__iomem *mem)\r\n{\r\n__raw_writel(val, mem);\r\n}\r\nstatic u32 cppi_readl(void *__iomem *mem)\r\n{\r\nreturn __raw_readl(mem);\r\n}\r\nstatic u32 pd_trans_len(u32 val)\r\n{\r\nreturn val & ((1 << (DESC_LENGTH_BITS_NUM + 1)) - 1);\r\n}\r\nstatic u32 cppi41_pop_desc(struct cppi41_dd *cdd, unsigned queue_num)\r\n{\r\nu32 desc;\r\ndesc = cppi_readl(cdd->qmgr_mem + QMGR_QUEUE_D(queue_num));\r\ndesc &= ~0x1f;\r\nreturn desc;\r\n}\r\nstatic irqreturn_t cppi41_irq(int irq, void *data)\r\n{\r\nstruct cppi41_dd *cdd = data;\r\nstruct cppi41_channel *c;\r\nu32 status;\r\nint i;\r\nstatus = cppi_readl(cdd->usbss_mem + USBSS_IRQ_STATUS);\r\nif (!(status & USBSS_IRQ_PD_COMP))\r\nreturn IRQ_NONE;\r\ncppi_writel(status, cdd->usbss_mem + USBSS_IRQ_STATUS);\r\nfor (i = QMGR_PENDING_SLOT_Q(FIST_COMPLETION_QUEUE); i < QMGR_NUM_PEND;\r\ni++) {\r\nu32 val;\r\nu32 q_num;\r\nval = cppi_readl(cdd->qmgr_mem + QMGR_PEND(i));\r\nif (i == QMGR_PENDING_SLOT_Q(FIST_COMPLETION_QUEUE) && val) {\r\nu32 mask;\r\nmask = 1 << QMGR_PENDING_BIT_Q(FIST_COMPLETION_QUEUE);\r\nmask--;\r\nval &= ~mask;\r\n}\r\nif (val)\r\n__iormb();\r\nwhile (val) {\r\nu32 desc, len;\r\nq_num = __fls(val);\r\nval &= ~(1 << q_num);\r\nq_num += 32 * i;\r\ndesc = cppi41_pop_desc(cdd, q_num);\r\nc = desc_to_chan(cdd, desc);\r\nif (WARN_ON(!c)) {\r\npr_err("%s() q %d desc %08x\n", __func__,\r\nq_num, desc);\r\ncontinue;\r\n}\r\nif (c->desc->pd2 & PD2_ZERO_LENGTH)\r\nlen = 0;\r\nelse\r\nlen = pd_trans_len(c->desc->pd0);\r\nc->residue = pd_trans_len(c->desc->pd6) - len;\r\ndma_cookie_complete(&c->txd);\r\nc->txd.callback(c->txd.callback_param);\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic dma_cookie_t cppi41_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\ndma_cookie_t cookie;\r\ncookie = dma_cookie_assign(tx);\r\nreturn cookie;\r\n}\r\nstatic int cppi41_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\ndma_cookie_init(chan);\r\ndma_async_tx_descriptor_init(&c->txd, chan);\r\nc->txd.tx_submit = cppi41_tx_submit;\r\nif (!c->is_tx)\r\ncppi_writel(c->q_num, c->gcr_reg + RXHPCRA0);\r\nreturn 0;\r\n}\r\nstatic void cppi41_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\n}\r\nstatic enum dma_status cppi41_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (txstate && ret == DMA_COMPLETE)\r\ntxstate->residue = c->residue;\r\nreturn ret;\r\n}\r\nstatic void push_desc_queue(struct cppi41_channel *c)\r\n{\r\nstruct cppi41_dd *cdd = c->cdd;\r\nu32 desc_num;\r\nu32 desc_phys;\r\nu32 reg;\r\ndesc_phys = lower_32_bits(c->desc_phys);\r\ndesc_num = (desc_phys - cdd->descs_phys) / sizeof(struct cppi41_desc);\r\nWARN_ON(cdd->chan_busy[desc_num]);\r\ncdd->chan_busy[desc_num] = c;\r\nreg = (sizeof(struct cppi41_desc) - 24) / 4;\r\nreg |= desc_phys;\r\ncppi_writel(reg, cdd->qmgr_mem + QMGR_QUEUE_D(c->q_num));\r\n}\r\nstatic void cppi41_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nu32 reg;\r\nc->residue = 0;\r\nreg = GCR_CHAN_ENABLE;\r\nif (!c->is_tx) {\r\nreg |= GCR_STARV_RETRY;\r\nreg |= GCR_DESC_TYPE_HOST;\r\nreg |= c->q_comp_num;\r\n}\r\ncppi_writel(reg, c->gcr_reg);\r\n__iowmb();\r\npush_desc_queue(c);\r\n}\r\nstatic u32 get_host_pd0(u32 length)\r\n{\r\nu32 reg;\r\nreg = DESC_TYPE_HOST << DESC_TYPE;\r\nreg |= length;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd1(struct cppi41_channel *c)\r\n{\r\nu32 reg;\r\nreg = 0;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd2(struct cppi41_channel *c)\r\n{\r\nu32 reg;\r\nreg = DESC_TYPE_USB;\r\nreg |= c->q_comp_num;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd3(u32 length)\r\n{\r\nu32 reg;\r\nreg = length;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd6(u32 length)\r\n{\r\nu32 reg;\r\nreg = DESC_PD_COMPLETE;\r\nreg |= length;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd4_or_7(u32 addr)\r\n{\r\nu32 reg;\r\nreg = addr;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd5(void)\r\n{\r\nu32 reg;\r\nreg = 0;\r\nreturn reg;\r\n}\r\nstatic struct dma_async_tx_descriptor *cppi41_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned sg_len,\r\nenum dma_transfer_direction dir, unsigned long tx_flags, void *context)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nstruct cppi41_desc *d;\r\nstruct scatterlist *sg;\r\nunsigned int i;\r\nunsigned int num;\r\nnum = 0;\r\nd = c->desc;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nu32 addr;\r\nu32 len;\r\nBUG_ON(num > 0);\r\naddr = lower_32_bits(sg_dma_address(sg));\r\nlen = sg_dma_len(sg);\r\nd->pd0 = get_host_pd0(len);\r\nd->pd1 = get_host_pd1(c);\r\nd->pd2 = get_host_pd2(c);\r\nd->pd3 = get_host_pd3(len);\r\nd->pd4 = get_host_pd4_or_7(addr);\r\nd->pd5 = get_host_pd5();\r\nd->pd6 = get_host_pd6(len);\r\nd->pd7 = get_host_pd4_or_7(addr);\r\nd++;\r\n}\r\nreturn &c->txd;\r\n}\r\nstatic int cpp41_cfg_chan(struct cppi41_channel *c,\r\nstruct dma_slave_config *cfg)\r\n{\r\nreturn 0;\r\n}\r\nstatic void cppi41_compute_td_desc(struct cppi41_desc *d)\r\n{\r\nd->pd0 = DESC_TYPE_TEARD << DESC_TYPE;\r\n}\r\nstatic int cppi41_tear_down_chan(struct cppi41_channel *c)\r\n{\r\nstruct cppi41_dd *cdd = c->cdd;\r\nstruct cppi41_desc *td;\r\nu32 reg;\r\nu32 desc_phys;\r\nu32 td_desc_phys;\r\ntd = cdd->cd;\r\ntd += cdd->first_td_desc;\r\ntd_desc_phys = cdd->descs_phys;\r\ntd_desc_phys += cdd->first_td_desc * sizeof(struct cppi41_desc);\r\nif (!c->td_queued) {\r\ncppi41_compute_td_desc(td);\r\n__iowmb();\r\nreg = (sizeof(struct cppi41_desc) - 24) / 4;\r\nreg |= td_desc_phys;\r\ncppi_writel(reg, cdd->qmgr_mem +\r\nQMGR_QUEUE_D(cdd->td_queue.submit));\r\nreg = GCR_CHAN_ENABLE;\r\nif (!c->is_tx) {\r\nreg |= GCR_STARV_RETRY;\r\nreg |= GCR_DESC_TYPE_HOST;\r\nreg |= c->q_comp_num;\r\n}\r\nreg |= GCR_TEARDOWN;\r\ncppi_writel(reg, c->gcr_reg);\r\nc->td_queued = 1;\r\nc->td_retry = 100;\r\n}\r\nif (!c->td_seen || !c->td_desc_seen) {\r\ndesc_phys = cppi41_pop_desc(cdd, cdd->td_queue.complete);\r\nif (!desc_phys)\r\ndesc_phys = cppi41_pop_desc(cdd, c->q_comp_num);\r\nif (desc_phys == c->desc_phys) {\r\nc->td_desc_seen = 1;\r\n} else if (desc_phys == td_desc_phys) {\r\nu32 pd0;\r\n__iormb();\r\npd0 = td->pd0;\r\nWARN_ON((pd0 >> DESC_TYPE) != DESC_TYPE_TEARD);\r\nWARN_ON(!c->is_tx && !(pd0 & TD_DESC_IS_RX));\r\nWARN_ON((pd0 & 0x1f) != c->port_num);\r\nc->td_seen = 1;\r\n} else if (desc_phys) {\r\nWARN_ON_ONCE(1);\r\n}\r\n}\r\nc->td_retry--;\r\nif (!c->td_seen && c->td_retry)\r\nreturn -EAGAIN;\r\nWARN_ON(!c->td_retry);\r\nif (!c->td_desc_seen) {\r\ndesc_phys = cppi41_pop_desc(cdd, c->q_num);\r\nWARN_ON(!desc_phys);\r\n}\r\nc->td_queued = 0;\r\nc->td_seen = 0;\r\nc->td_desc_seen = 0;\r\ncppi_writel(0, c->gcr_reg);\r\nreturn 0;\r\n}\r\nstatic int cppi41_stop_chan(struct dma_chan *chan)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nstruct cppi41_dd *cdd = c->cdd;\r\nu32 desc_num;\r\nu32 desc_phys;\r\nint ret;\r\ndesc_phys = lower_32_bits(c->desc_phys);\r\ndesc_num = (desc_phys - cdd->descs_phys) / sizeof(struct cppi41_desc);\r\nif (!cdd->chan_busy[desc_num])\r\nreturn 0;\r\nret = cppi41_tear_down_chan(c);\r\nif (ret)\r\nreturn ret;\r\nWARN_ON(!cdd->chan_busy[desc_num]);\r\ncdd->chan_busy[desc_num] = NULL;\r\nreturn 0;\r\n}\r\nstatic int cppi41_dma_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nint ret;\r\nswitch (cmd) {\r\ncase DMA_SLAVE_CONFIG:\r\nret = cpp41_cfg_chan(c, (struct dma_slave_config *) arg);\r\nbreak;\r\ncase DMA_TERMINATE_ALL:\r\nret = cppi41_stop_chan(chan);\r\nbreak;\r\ndefault:\r\nret = -ENXIO;\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic void cleanup_chans(struct cppi41_dd *cdd)\r\n{\r\nwhile (!list_empty(&cdd->ddev.channels)) {\r\nstruct cppi41_channel *cchan;\r\ncchan = list_first_entry(&cdd->ddev.channels,\r\nstruct cppi41_channel, chan.device_node);\r\nlist_del(&cchan->chan.device_node);\r\nkfree(cchan);\r\n}\r\n}\r\nstatic int cppi41_add_chans(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nstruct cppi41_channel *cchan;\r\nint i;\r\nint ret;\r\nu32 n_chans;\r\nret = of_property_read_u32(dev->of_node, "#dma-channels",\r\n&n_chans);\r\nif (ret)\r\nreturn ret;\r\nn_chans *= 2;\r\nfor (i = 0; i < n_chans; i++) {\r\ncchan = kzalloc(sizeof(*cchan), GFP_KERNEL);\r\nif (!cchan)\r\ngoto err;\r\ncchan->cdd = cdd;\r\nif (i & 1) {\r\ncchan->gcr_reg = cdd->ctrl_mem + DMA_TXGCR(i >> 1);\r\ncchan->is_tx = 1;\r\n} else {\r\ncchan->gcr_reg = cdd->ctrl_mem + DMA_RXGCR(i >> 1);\r\ncchan->is_tx = 0;\r\n}\r\ncchan->port_num = i >> 1;\r\ncchan->desc = &cdd->cd[i];\r\ncchan->desc_phys = cdd->descs_phys;\r\ncchan->desc_phys += i * sizeof(struct cppi41_desc);\r\ncchan->chan.device = &cdd->ddev;\r\nlist_add_tail(&cchan->chan.device_node, &cdd->ddev.channels);\r\n}\r\ncdd->first_td_desc = n_chans;\r\nreturn 0;\r\nerr:\r\ncleanup_chans(cdd);\r\nreturn -ENOMEM;\r\n}\r\nstatic void purge_descs(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nunsigned int mem_decs;\r\nint i;\r\nmem_decs = ALLOC_DECS_NUM * sizeof(struct cppi41_desc);\r\nfor (i = 0; i < DESCS_AREAS; i++) {\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_MEMBASE(i));\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_MEMCTRL(i));\r\ndma_free_coherent(dev, mem_decs, cdd->cd,\r\ncdd->descs_phys);\r\n}\r\n}\r\nstatic void disable_sched(struct cppi41_dd *cdd)\r\n{\r\ncppi_writel(0, cdd->sched_mem + DMA_SCHED_CTRL);\r\n}\r\nstatic void deinit_cppi41(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\ndisable_sched(cdd);\r\npurge_descs(dev, cdd);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ndma_free_coherent(dev, QMGR_SCRATCH_SIZE, cdd->qmgr_scratch,\r\ncdd->scratch_phys);\r\n}\r\nstatic int init_descs(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nunsigned int desc_size;\r\nunsigned int mem_decs;\r\nint i;\r\nu32 reg;\r\nu32 idx;\r\nBUILD_BUG_ON(sizeof(struct cppi41_desc) &\r\n(sizeof(struct cppi41_desc) - 1));\r\nBUILD_BUG_ON(sizeof(struct cppi41_desc) < 32);\r\nBUILD_BUG_ON(ALLOC_DECS_NUM < 32);\r\ndesc_size = sizeof(struct cppi41_desc);\r\nmem_decs = ALLOC_DECS_NUM * desc_size;\r\nidx = 0;\r\nfor (i = 0; i < DESCS_AREAS; i++) {\r\nreg = idx << QMGR_MEMCTRL_IDX_SH;\r\nreg |= (ilog2(desc_size) - 5) << QMGR_MEMCTRL_DESC_SH;\r\nreg |= ilog2(ALLOC_DECS_NUM) - 5;\r\nBUILD_BUG_ON(DESCS_AREAS != 1);\r\ncdd->cd = dma_alloc_coherent(dev, mem_decs,\r\n&cdd->descs_phys, GFP_KERNEL);\r\nif (!cdd->cd)\r\nreturn -ENOMEM;\r\ncppi_writel(cdd->descs_phys, cdd->qmgr_mem + QMGR_MEMBASE(i));\r\ncppi_writel(reg, cdd->qmgr_mem + QMGR_MEMCTRL(i));\r\nidx += ALLOC_DECS_NUM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void init_sched(struct cppi41_dd *cdd)\r\n{\r\nunsigned ch;\r\nunsigned word;\r\nu32 reg;\r\nword = 0;\r\ncppi_writel(0, cdd->sched_mem + DMA_SCHED_CTRL);\r\nfor (ch = 0; ch < 15 * 2; ch += 2) {\r\nreg = SCHED_ENTRY0_CHAN(ch);\r\nreg |= SCHED_ENTRY1_CHAN(ch) | SCHED_ENTRY1_IS_RX;\r\nreg |= SCHED_ENTRY2_CHAN(ch + 1);\r\nreg |= SCHED_ENTRY3_CHAN(ch + 1) | SCHED_ENTRY3_IS_RX;\r\ncppi_writel(reg, cdd->sched_mem + DMA_SCHED_WORD(word));\r\nword++;\r\n}\r\nreg = 15 * 2 * 2 - 1;\r\nreg |= DMA_SCHED_CTRL_EN;\r\ncppi_writel(reg, cdd->sched_mem + DMA_SCHED_CTRL);\r\n}\r\nstatic int init_cppi41(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nint ret;\r\nBUILD_BUG_ON(QMGR_SCRATCH_SIZE > ((1 << 14) - 1));\r\ncdd->qmgr_scratch = dma_alloc_coherent(dev, QMGR_SCRATCH_SIZE,\r\n&cdd->scratch_phys, GFP_KERNEL);\r\nif (!cdd->qmgr_scratch)\r\nreturn -ENOMEM;\r\ncppi_writel(cdd->scratch_phys, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ncppi_writel(QMGR_SCRATCH_SIZE, cdd->qmgr_mem + QMGR_LRAM_SIZE);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM1_BASE);\r\nret = init_descs(dev, cdd);\r\nif (ret)\r\ngoto err_td;\r\ncppi_writel(cdd->td_queue.submit, cdd->ctrl_mem + DMA_TDFDQ);\r\ninit_sched(cdd);\r\nreturn 0;\r\nerr_td:\r\ndeinit_cppi41(dev, cdd);\r\nreturn ret;\r\n}\r\nstatic bool cpp41_dma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nstruct cppi41_channel *cchan;\r\nstruct cppi41_dd *cdd;\r\nconst struct chan_queues *queues;\r\nu32 *num = param;\r\nif (chan->device->dev->driver != &cpp41_dma_driver.driver)\r\nreturn false;\r\ncchan = to_cpp41_chan(chan);\r\nif (cchan->port_num != num[INFO_PORT])\r\nreturn false;\r\nif (cchan->is_tx && !num[INFO_IS_TX])\r\nreturn false;\r\ncdd = cchan->cdd;\r\nif (cchan->is_tx)\r\nqueues = cdd->queues_tx;\r\nelse\r\nqueues = cdd->queues_rx;\r\nBUILD_BUG_ON(ARRAY_SIZE(usb_queues_rx) != ARRAY_SIZE(usb_queues_tx));\r\nif (WARN_ON(cchan->port_num > ARRAY_SIZE(usb_queues_rx)))\r\nreturn false;\r\ncchan->q_num = queues[cchan->port_num].submit;\r\ncchan->q_comp_num = queues[cchan->port_num].complete;\r\nreturn true;\r\n}\r\nstatic struct dma_chan *cppi41_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nint count = dma_spec->args_count;\r\nstruct of_dma_filter_info *info = ofdma->of_dma_data;\r\nif (!info || !info->filter_fn)\r\nreturn NULL;\r\nif (count != 2)\r\nreturn NULL;\r\nreturn dma_request_channel(info->dma_cap, info->filter_fn,\r\n&dma_spec->args[0]);\r\n}\r\nstatic const struct cppi_glue_infos *get_glue_info(struct device *dev)\r\n{\r\nconst struct of_device_id *of_id;\r\nof_id = of_match_node(cppi41_dma_ids, dev->of_node);\r\nif (!of_id)\r\nreturn NULL;\r\nreturn of_id->data;\r\n}\r\nstatic int cppi41_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct cppi41_dd *cdd;\r\nstruct device *dev = &pdev->dev;\r\nconst struct cppi_glue_infos *glue_info;\r\nint irq;\r\nint ret;\r\nglue_info = get_glue_info(dev);\r\nif (!glue_info)\r\nreturn -EINVAL;\r\ncdd = kzalloc(sizeof(*cdd), GFP_KERNEL);\r\nif (!cdd)\r\nreturn -ENOMEM;\r\ndma_cap_set(DMA_SLAVE, cdd->ddev.cap_mask);\r\ncdd->ddev.device_alloc_chan_resources = cppi41_dma_alloc_chan_resources;\r\ncdd->ddev.device_free_chan_resources = cppi41_dma_free_chan_resources;\r\ncdd->ddev.device_tx_status = cppi41_dma_tx_status;\r\ncdd->ddev.device_issue_pending = cppi41_dma_issue_pending;\r\ncdd->ddev.device_prep_slave_sg = cppi41_dma_prep_slave_sg;\r\ncdd->ddev.device_control = cppi41_dma_control;\r\ncdd->ddev.dev = dev;\r\nINIT_LIST_HEAD(&cdd->ddev.channels);\r\ncpp41_dma_info.dma_cap = cdd->ddev.cap_mask;\r\ncdd->usbss_mem = of_iomap(dev->of_node, 0);\r\ncdd->ctrl_mem = of_iomap(dev->of_node, 1);\r\ncdd->sched_mem = of_iomap(dev->of_node, 2);\r\ncdd->qmgr_mem = of_iomap(dev->of_node, 3);\r\nif (!cdd->usbss_mem || !cdd->ctrl_mem || !cdd->sched_mem ||\r\n!cdd->qmgr_mem) {\r\nret = -ENXIO;\r\ngoto err_remap;\r\n}\r\npm_runtime_enable(dev);\r\nret = pm_runtime_get_sync(dev);\r\nif (ret < 0)\r\ngoto err_get_sync;\r\ncdd->queues_rx = glue_info->queues_rx;\r\ncdd->queues_tx = glue_info->queues_tx;\r\ncdd->td_queue = glue_info->td_queue;\r\nret = init_cppi41(dev, cdd);\r\nif (ret)\r\ngoto err_init_cppi;\r\nret = cppi41_add_chans(dev, cdd);\r\nif (ret)\r\ngoto err_chans;\r\nirq = irq_of_parse_and_map(dev->of_node, 0);\r\nif (!irq) {\r\nret = -EINVAL;\r\ngoto err_irq;\r\n}\r\ncppi_writel(USBSS_IRQ_PD_COMP, cdd->usbss_mem + USBSS_IRQ_ENABLER);\r\nret = request_irq(irq, glue_info->isr, IRQF_SHARED,\r\ndev_name(dev), cdd);\r\nif (ret)\r\ngoto err_irq;\r\ncdd->irq = irq;\r\nret = dma_async_device_register(&cdd->ddev);\r\nif (ret)\r\ngoto err_dma_reg;\r\nret = of_dma_controller_register(dev->of_node,\r\ncppi41_dma_xlate, &cpp41_dma_info);\r\nif (ret)\r\ngoto err_of;\r\nplatform_set_drvdata(pdev, cdd);\r\nreturn 0;\r\nerr_of:\r\ndma_async_device_unregister(&cdd->ddev);\r\nerr_dma_reg:\r\nfree_irq(irq, cdd);\r\nerr_irq:\r\ncppi_writel(0, cdd->usbss_mem + USBSS_IRQ_CLEARR);\r\ncleanup_chans(cdd);\r\nerr_chans:\r\ndeinit_cppi41(dev, cdd);\r\nerr_init_cppi:\r\npm_runtime_put(dev);\r\nerr_get_sync:\r\npm_runtime_disable(dev);\r\niounmap(cdd->usbss_mem);\r\niounmap(cdd->ctrl_mem);\r\niounmap(cdd->sched_mem);\r\niounmap(cdd->qmgr_mem);\r\nerr_remap:\r\nkfree(cdd);\r\nreturn ret;\r\n}\r\nstatic int cppi41_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct cppi41_dd *cdd = platform_get_drvdata(pdev);\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&cdd->ddev);\r\ncppi_writel(0, cdd->usbss_mem + USBSS_IRQ_CLEARR);\r\nfree_irq(cdd->irq, cdd);\r\ncleanup_chans(cdd);\r\ndeinit_cppi41(&pdev->dev, cdd);\r\niounmap(cdd->usbss_mem);\r\niounmap(cdd->ctrl_mem);\r\niounmap(cdd->sched_mem);\r\niounmap(cdd->qmgr_mem);\r\npm_runtime_put(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nkfree(cdd);\r\nreturn 0;\r\n}\r\nstatic int cppi41_suspend(struct device *dev)\r\n{\r\nstruct cppi41_dd *cdd = dev_get_drvdata(dev);\r\ncdd->dma_tdfdq = cppi_readl(cdd->ctrl_mem + DMA_TDFDQ);\r\ncppi_writel(0, cdd->usbss_mem + USBSS_IRQ_CLEARR);\r\ndisable_sched(cdd);\r\nreturn 0;\r\n}\r\nstatic int cppi41_resume(struct device *dev)\r\n{\r\nstruct cppi41_dd *cdd = dev_get_drvdata(dev);\r\nstruct cppi41_channel *c;\r\nint i;\r\nfor (i = 0; i < DESCS_AREAS; i++)\r\ncppi_writel(cdd->descs_phys, cdd->qmgr_mem + QMGR_MEMBASE(i));\r\nlist_for_each_entry(c, &cdd->ddev.channels, chan.device_node)\r\nif (!c->is_tx)\r\ncppi_writel(c->q_num, c->gcr_reg + RXHPCRA0);\r\ninit_sched(cdd);\r\ncppi_writel(cdd->dma_tdfdq, cdd->ctrl_mem + DMA_TDFDQ);\r\ncppi_writel(cdd->scratch_phys, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ncppi_writel(QMGR_SCRATCH_SIZE, cdd->qmgr_mem + QMGR_LRAM_SIZE);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM1_BASE);\r\ncppi_writel(USBSS_IRQ_PD_COMP, cdd->usbss_mem + USBSS_IRQ_ENABLER);\r\nreturn 0;\r\n}
