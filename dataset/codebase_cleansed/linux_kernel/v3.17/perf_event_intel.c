static u64 intel_pmu_event_map(int hw_event)\r\n{\r\nreturn intel_perfmon_event_map[hw_event];\r\n}\r\nstatic inline bool intel_pmu_needs_lbr_smpl(struct perf_event *event)\r\n{\r\nif (has_branch_stack(event))\r\nreturn true;\r\nif (x86_pmu.intel_cap.pebs_trap && event->attr.precise_ip > 1 &&\r\nx86_pmu.intel_cap.pebs_format < 2)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void intel_pmu_disable_all(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nwrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0);\r\nif (test_bit(INTEL_PMC_IDX_FIXED_BTS, cpuc->active_mask))\r\nintel_pmu_disable_bts();\r\nintel_pmu_pebs_disable_all();\r\nintel_pmu_lbr_disable_all();\r\n}\r\nstatic void intel_pmu_enable_all(int added)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nintel_pmu_pebs_enable_all();\r\nintel_pmu_lbr_enable_all();\r\nwrmsrl(MSR_CORE_PERF_GLOBAL_CTRL,\r\nx86_pmu.intel_ctrl & ~cpuc->intel_ctrl_guest_mask);\r\nif (test_bit(INTEL_PMC_IDX_FIXED_BTS, cpuc->active_mask)) {\r\nstruct perf_event *event =\r\ncpuc->events[INTEL_PMC_IDX_FIXED_BTS];\r\nif (WARN_ON_ONCE(!event))\r\nreturn;\r\nintel_pmu_enable_bts(event->hw.config);\r\n}\r\n}\r\nstatic void intel_pmu_nhm_workaround(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstatic const unsigned long nhm_magic[4] = {\r\n0x4300B5,\r\n0x4300D2,\r\n0x4300B1,\r\n0x4300B1\r\n};\r\nstruct perf_event *event;\r\nint i;\r\nfor (i = 0; i < 4; i++) {\r\nevent = cpuc->events[i];\r\nif (event)\r\nx86_perf_event_update(event);\r\n}\r\nfor (i = 0; i < 4; i++) {\r\nwrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, nhm_magic[i]);\r\nwrmsrl(MSR_ARCH_PERFMON_PERFCTR0 + i, 0x0);\r\n}\r\nwrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0xf);\r\nwrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0x0);\r\nfor (i = 0; i < 4; i++) {\r\nevent = cpuc->events[i];\r\nif (event) {\r\nx86_perf_event_set_period(event);\r\n__x86_pmu_enable_event(&event->hw,\r\nARCH_PERFMON_EVENTSEL_ENABLE);\r\n} else\r\nwrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, 0x0);\r\n}\r\n}\r\nstatic void intel_pmu_nhm_enable_all(int added)\r\n{\r\nif (added)\r\nintel_pmu_nhm_workaround();\r\nintel_pmu_enable_all(added);\r\n}\r\nstatic inline u64 intel_pmu_get_status(void)\r\n{\r\nu64 status;\r\nrdmsrl(MSR_CORE_PERF_GLOBAL_STATUS, status);\r\nreturn status;\r\n}\r\nstatic inline void intel_pmu_ack_status(u64 ack)\r\n{\r\nwrmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, ack);\r\n}\r\nstatic void intel_pmu_disable_fixed(struct hw_perf_event *hwc)\r\n{\r\nint idx = hwc->idx - INTEL_PMC_IDX_FIXED;\r\nu64 ctrl_val, mask;\r\nmask = 0xfULL << (idx * 4);\r\nrdmsrl(hwc->config_base, ctrl_val);\r\nctrl_val &= ~mask;\r\nwrmsrl(hwc->config_base, ctrl_val);\r\n}\r\nstatic inline bool event_is_checkpointed(struct perf_event *event)\r\n{\r\nreturn (event->hw.config & HSW_IN_TX_CHECKPOINTED) != 0;\r\n}\r\nstatic void intel_pmu_disable_event(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nif (unlikely(hwc->idx == INTEL_PMC_IDX_FIXED_BTS)) {\r\nintel_pmu_disable_bts();\r\nintel_pmu_drain_bts_buffer();\r\nreturn;\r\n}\r\ncpuc->intel_ctrl_guest_mask &= ~(1ull << hwc->idx);\r\ncpuc->intel_ctrl_host_mask &= ~(1ull << hwc->idx);\r\ncpuc->intel_cp_status &= ~(1ull << hwc->idx);\r\nif (intel_pmu_needs_lbr_smpl(event))\r\nintel_pmu_lbr_disable(event);\r\nif (unlikely(hwc->config_base == MSR_ARCH_PERFMON_FIXED_CTR_CTRL)) {\r\nintel_pmu_disable_fixed(hwc);\r\nreturn;\r\n}\r\nx86_pmu_disable_event(event);\r\nif (unlikely(event->attr.precise_ip))\r\nintel_pmu_pebs_disable(event);\r\n}\r\nstatic void intel_pmu_enable_fixed(struct hw_perf_event *hwc)\r\n{\r\nint idx = hwc->idx - INTEL_PMC_IDX_FIXED;\r\nu64 ctrl_val, bits, mask;\r\nbits = 0x8ULL;\r\nif (hwc->config & ARCH_PERFMON_EVENTSEL_USR)\r\nbits |= 0x2;\r\nif (hwc->config & ARCH_PERFMON_EVENTSEL_OS)\r\nbits |= 0x1;\r\nif (x86_pmu.version > 2 && hwc->config & ARCH_PERFMON_EVENTSEL_ANY)\r\nbits |= 0x4;\r\nbits <<= (idx * 4);\r\nmask = 0xfULL << (idx * 4);\r\nrdmsrl(hwc->config_base, ctrl_val);\r\nctrl_val &= ~mask;\r\nctrl_val |= bits;\r\nwrmsrl(hwc->config_base, ctrl_val);\r\n}\r\nstatic void intel_pmu_enable_event(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nif (unlikely(hwc->idx == INTEL_PMC_IDX_FIXED_BTS)) {\r\nif (!__this_cpu_read(cpu_hw_events.enabled))\r\nreturn;\r\nintel_pmu_enable_bts(hwc->config);\r\nreturn;\r\n}\r\nif (intel_pmu_needs_lbr_smpl(event))\r\nintel_pmu_lbr_enable(event);\r\nif (event->attr.exclude_host)\r\ncpuc->intel_ctrl_guest_mask |= (1ull << hwc->idx);\r\nif (event->attr.exclude_guest)\r\ncpuc->intel_ctrl_host_mask |= (1ull << hwc->idx);\r\nif (unlikely(event_is_checkpointed(event)))\r\ncpuc->intel_cp_status |= (1ull << hwc->idx);\r\nif (unlikely(hwc->config_base == MSR_ARCH_PERFMON_FIXED_CTR_CTRL)) {\r\nintel_pmu_enable_fixed(hwc);\r\nreturn;\r\n}\r\nif (unlikely(event->attr.precise_ip))\r\nintel_pmu_pebs_enable(event);\r\n__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\r\n}\r\nint intel_pmu_save_and_restart(struct perf_event *event)\r\n{\r\nx86_perf_event_update(event);\r\nif (unlikely(event_is_checkpointed(event))) {\r\nwrmsrl(event->hw.event_base, 0);\r\nlocal64_set(&event->hw.prev_count, 0);\r\n}\r\nreturn x86_perf_event_set_period(event);\r\n}\r\nstatic void intel_pmu_reset(void)\r\n{\r\nstruct debug_store *ds = __this_cpu_read(cpu_hw_events.ds);\r\nunsigned long flags;\r\nint idx;\r\nif (!x86_pmu.num_counters)\r\nreturn;\r\nlocal_irq_save(flags);\r\npr_info("clearing PMU state on CPU#%d\n", smp_processor_id());\r\nfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\r\nwrmsrl_safe(x86_pmu_config_addr(idx), 0ull);\r\nwrmsrl_safe(x86_pmu_event_addr(idx), 0ull);\r\n}\r\nfor (idx = 0; idx < x86_pmu.num_counters_fixed; idx++)\r\nwrmsrl_safe(MSR_ARCH_PERFMON_FIXED_CTR0 + idx, 0ull);\r\nif (ds)\r\nds->bts_index = ds->bts_buffer_base;\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int intel_pmu_handle_irq(struct pt_regs *regs)\r\n{\r\nstruct perf_sample_data data;\r\nstruct cpu_hw_events *cpuc;\r\nint bit, loops;\r\nu64 status;\r\nint handled;\r\ncpuc = &__get_cpu_var(cpu_hw_events);\r\nif (!x86_pmu.late_ack)\r\napic_write(APIC_LVTPC, APIC_DM_NMI);\r\nintel_pmu_disable_all();\r\nhandled = intel_pmu_drain_bts_buffer();\r\nstatus = intel_pmu_get_status();\r\nif (!status)\r\ngoto done;\r\nloops = 0;\r\nagain:\r\nintel_pmu_ack_status(status);\r\nif (++loops > 100) {\r\nstatic bool warned = false;\r\nif (!warned) {\r\nWARN(1, "perfevents: irq loop stuck!\n");\r\nperf_event_print_debug();\r\nwarned = true;\r\n}\r\nintel_pmu_reset();\r\ngoto done;\r\n}\r\ninc_irq_stat(apic_perf_irqs);\r\nintel_pmu_lbr_read();\r\nif (__test_and_clear_bit(63, (unsigned long *)&status)) {\r\nif (!status)\r\ngoto done;\r\n}\r\nif (__test_and_clear_bit(62, (unsigned long *)&status)) {\r\nhandled++;\r\nx86_pmu.drain_pebs(regs);\r\n}\r\nstatus |= cpuc->intel_cp_status;\r\nfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\r\nstruct perf_event *event = cpuc->events[bit];\r\nhandled++;\r\nif (!test_bit(bit, cpuc->active_mask))\r\ncontinue;\r\nif (!intel_pmu_save_and_restart(event))\r\ncontinue;\r\nperf_sample_data_init(&data, 0, event->hw.last_period);\r\nif (has_branch_stack(event))\r\ndata.br_stack = &cpuc->lbr_stack;\r\nif (perf_event_overflow(event, &data, regs))\r\nx86_pmu_stop(event, 0);\r\n}\r\nstatus = intel_pmu_get_status();\r\nif (status)\r\ngoto again;\r\ndone:\r\nintel_pmu_enable_all(0);\r\nif (x86_pmu.late_ack)\r\napic_write(APIC_LVTPC, APIC_DM_NMI);\r\nreturn handled;\r\n}\r\nstatic struct event_constraint *\r\nintel_bts_constraints(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned int hw_event, bts_event;\r\nif (event->attr.freq)\r\nreturn NULL;\r\nhw_event = hwc->config & INTEL_ARCH_EVENT_MASK;\r\nbts_event = x86_pmu.event_map(PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\r\nif (unlikely(hw_event == bts_event && hwc->sample_period == 1))\r\nreturn &bts_constraint;\r\nreturn NULL;\r\n}\r\nstatic int intel_alt_er(int idx)\r\n{\r\nif (!(x86_pmu.er_flags & ERF_HAS_RSP_1))\r\nreturn idx;\r\nif (idx == EXTRA_REG_RSP_0)\r\nreturn EXTRA_REG_RSP_1;\r\nif (idx == EXTRA_REG_RSP_1)\r\nreturn EXTRA_REG_RSP_0;\r\nreturn idx;\r\n}\r\nstatic void intel_fixup_er(struct perf_event *event, int idx)\r\n{\r\nevent->hw.extra_reg.idx = idx;\r\nif (idx == EXTRA_REG_RSP_0) {\r\nevent->hw.config &= ~INTEL_ARCH_EVENT_MASK;\r\nevent->hw.config |= x86_pmu.extra_regs[EXTRA_REG_RSP_0].event;\r\nevent->hw.extra_reg.reg = MSR_OFFCORE_RSP_0;\r\n} else if (idx == EXTRA_REG_RSP_1) {\r\nevent->hw.config &= ~INTEL_ARCH_EVENT_MASK;\r\nevent->hw.config |= x86_pmu.extra_regs[EXTRA_REG_RSP_1].event;\r\nevent->hw.extra_reg.reg = MSR_OFFCORE_RSP_1;\r\n}\r\n}\r\nstatic struct event_constraint *\r\n__intel_shared_reg_get_constraints(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event,\r\nstruct hw_perf_event_extra *reg)\r\n{\r\nstruct event_constraint *c = &emptyconstraint;\r\nstruct er_account *era;\r\nunsigned long flags;\r\nint idx = reg->idx;\r\nif (reg->alloc && !cpuc->is_fake)\r\nreturn NULL;\r\nagain:\r\nera = &cpuc->shared_regs->regs[idx];\r\nraw_spin_lock_irqsave(&era->lock, flags);\r\nif (!atomic_read(&era->ref) || era->config == reg->config) {\r\nif (!cpuc->is_fake) {\r\nif (idx != reg->idx)\r\nintel_fixup_er(event, idx);\r\nreg->alloc = 1;\r\n}\r\nera->config = reg->config;\r\nera->reg = reg->reg;\r\natomic_inc(&era->ref);\r\nc = NULL;\r\n} else {\r\nidx = intel_alt_er(idx);\r\nif (idx != reg->idx) {\r\nraw_spin_unlock_irqrestore(&era->lock, flags);\r\ngoto again;\r\n}\r\n}\r\nraw_spin_unlock_irqrestore(&era->lock, flags);\r\nreturn c;\r\n}\r\nstatic void\r\n__intel_shared_reg_put_constraints(struct cpu_hw_events *cpuc,\r\nstruct hw_perf_event_extra *reg)\r\n{\r\nstruct er_account *era;\r\nif (!reg->alloc || cpuc->is_fake)\r\nreturn;\r\nera = &cpuc->shared_regs->regs[reg->idx];\r\natomic_dec(&era->ref);\r\nreg->alloc = 0;\r\n}\r\nstatic struct event_constraint *\r\nintel_shared_regs_constraints(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nstruct event_constraint *c = NULL, *d;\r\nstruct hw_perf_event_extra *xreg, *breg;\r\nxreg = &event->hw.extra_reg;\r\nif (xreg->idx != EXTRA_REG_NONE) {\r\nc = __intel_shared_reg_get_constraints(cpuc, event, xreg);\r\nif (c == &emptyconstraint)\r\nreturn c;\r\n}\r\nbreg = &event->hw.branch_reg;\r\nif (breg->idx != EXTRA_REG_NONE) {\r\nd = __intel_shared_reg_get_constraints(cpuc, event, breg);\r\nif (d == &emptyconstraint) {\r\n__intel_shared_reg_put_constraints(cpuc, xreg);\r\nc = d;\r\n}\r\n}\r\nreturn c;\r\n}\r\nstruct event_constraint *\r\nx86_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\r\n{\r\nstruct event_constraint *c;\r\nif (x86_pmu.event_constraints) {\r\nfor_each_event_constraint(c, x86_pmu.event_constraints) {\r\nif ((event->hw.config & c->cmask) == c->code) {\r\nevent->hw.flags |= c->flags;\r\nreturn c;\r\n}\r\n}\r\n}\r\nreturn &unconstrained;\r\n}\r\nstatic struct event_constraint *\r\nintel_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\r\n{\r\nstruct event_constraint *c;\r\nc = intel_bts_constraints(event);\r\nif (c)\r\nreturn c;\r\nc = intel_pebs_constraints(event);\r\nif (c)\r\nreturn c;\r\nc = intel_shared_regs_constraints(cpuc, event);\r\nif (c)\r\nreturn c;\r\nreturn x86_get_event_constraints(cpuc, event);\r\n}\r\nstatic void\r\nintel_put_shared_regs_event_constraints(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg;\r\nreg = &event->hw.extra_reg;\r\nif (reg->idx != EXTRA_REG_NONE)\r\n__intel_shared_reg_put_constraints(cpuc, reg);\r\nreg = &event->hw.branch_reg;\r\nif (reg->idx != EXTRA_REG_NONE)\r\n__intel_shared_reg_put_constraints(cpuc, reg);\r\n}\r\nstatic void intel_put_event_constraints(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nintel_put_shared_regs_event_constraints(cpuc, event);\r\n}\r\nstatic void intel_pebs_aliases_core2(struct perf_event *event)\r\n{\r\nif ((event->hw.config & X86_RAW_EVENT_MASK) == 0x003c) {\r\nu64 alt_config = X86_CONFIG(.event=0xc0, .inv=1, .cmask=16);\r\nalt_config |= (event->hw.config & ~X86_RAW_EVENT_MASK);\r\nevent->hw.config = alt_config;\r\n}\r\n}\r\nstatic void intel_pebs_aliases_snb(struct perf_event *event)\r\n{\r\nif ((event->hw.config & X86_RAW_EVENT_MASK) == 0x003c) {\r\nu64 alt_config = X86_CONFIG(.event=0xc2, .umask=0x01, .inv=1, .cmask=16);\r\nalt_config |= (event->hw.config & ~X86_RAW_EVENT_MASK);\r\nevent->hw.config = alt_config;\r\n}\r\n}\r\nstatic int intel_pmu_hw_config(struct perf_event *event)\r\n{\r\nint ret = x86_pmu_hw_config(event);\r\nif (ret)\r\nreturn ret;\r\nif (event->attr.precise_ip && x86_pmu.pebs_aliases)\r\nx86_pmu.pebs_aliases(event);\r\nif (intel_pmu_needs_lbr_smpl(event)) {\r\nret = intel_pmu_setup_lbr_filter(event);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (event->attr.type != PERF_TYPE_RAW)\r\nreturn 0;\r\nif (!(event->attr.config & ARCH_PERFMON_EVENTSEL_ANY))\r\nreturn 0;\r\nif (x86_pmu.version < 3)\r\nreturn -EINVAL;\r\nif (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nevent->hw.config |= ARCH_PERFMON_EVENTSEL_ANY;\r\nreturn 0;\r\n}\r\nstruct perf_guest_switch_msr *perf_guest_get_msrs(int *nr)\r\n{\r\nif (x86_pmu.guest_get_msrs)\r\nreturn x86_pmu.guest_get_msrs(nr);\r\n*nr = 0;\r\nreturn NULL;\r\n}\r\nstatic struct perf_guest_switch_msr *intel_guest_get_msrs(int *nr)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct perf_guest_switch_msr *arr = cpuc->guest_switch_msrs;\r\narr[0].msr = MSR_CORE_PERF_GLOBAL_CTRL;\r\narr[0].host = x86_pmu.intel_ctrl & ~cpuc->intel_ctrl_guest_mask;\r\narr[0].guest = x86_pmu.intel_ctrl & ~cpuc->intel_ctrl_host_mask;\r\narr[1].msr = MSR_IA32_PEBS_ENABLE;\r\narr[1].host = cpuc->pebs_enabled;\r\narr[1].guest = 0;\r\n*nr = 2;\r\nreturn arr;\r\n}\r\nstatic struct perf_guest_switch_msr *core_guest_get_msrs(int *nr)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct perf_guest_switch_msr *arr = cpuc->guest_switch_msrs;\r\nint idx;\r\nfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\r\nstruct perf_event *event = cpuc->events[idx];\r\narr[idx].msr = x86_pmu_config_addr(idx);\r\narr[idx].host = arr[idx].guest = 0;\r\nif (!test_bit(idx, cpuc->active_mask))\r\ncontinue;\r\narr[idx].host = arr[idx].guest =\r\nevent->hw.config | ARCH_PERFMON_EVENTSEL_ENABLE;\r\nif (event->attr.exclude_host)\r\narr[idx].host &= ~ARCH_PERFMON_EVENTSEL_ENABLE;\r\nelse if (event->attr.exclude_guest)\r\narr[idx].guest &= ~ARCH_PERFMON_EVENTSEL_ENABLE;\r\n}\r\n*nr = x86_pmu.num_counters;\r\nreturn arr;\r\n}\r\nstatic void core_pmu_enable_event(struct perf_event *event)\r\n{\r\nif (!event->attr.exclude_host)\r\nx86_pmu_enable_event(event);\r\n}\r\nstatic void core_pmu_enable_all(int added)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nint idx;\r\nfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\r\nstruct hw_perf_event *hwc = &cpuc->events[idx]->hw;\r\nif (!test_bit(idx, cpuc->active_mask) ||\r\ncpuc->events[idx]->attr.exclude_host)\r\ncontinue;\r\n__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\r\n}\r\n}\r\nstatic int hsw_hw_config(struct perf_event *event)\r\n{\r\nint ret = intel_pmu_hw_config(event);\r\nif (ret)\r\nreturn ret;\r\nif (!boot_cpu_has(X86_FEATURE_RTM) && !boot_cpu_has(X86_FEATURE_HLE))\r\nreturn 0;\r\nevent->hw.config |= event->attr.config & (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED);\r\nif ((event->hw.config & (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED)) &&\r\n((event->hw.config & ARCH_PERFMON_EVENTSEL_ANY) ||\r\nevent->attr.precise_ip > 0))\r\nreturn -EOPNOTSUPP;\r\nif (event_is_checkpointed(event)) {\r\nif (event->attr.sample_period > 0 &&\r\nevent->attr.sample_period < 0x7fffffff)\r\nreturn -EOPNOTSUPP;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct event_constraint *\r\nhsw_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\r\n{\r\nstruct event_constraint *c = intel_get_event_constraints(cpuc, event);\r\nif (event->hw.config & HSW_IN_TX_CHECKPOINTED) {\r\nif (c->idxmsk64 & (1U << 2))\r\nreturn &counter2_constraint;\r\nreturn &emptyconstraint;\r\n}\r\nreturn c;\r\n}\r\nssize_t intel_event_sysfs_show(char *page, u64 config)\r\n{\r\nu64 event = (config & ARCH_PERFMON_EVENTSEL_EVENT);\r\nreturn x86_event_sysfs_show(page, config, event);\r\n}\r\nstruct intel_shared_regs *allocate_shared_regs(int cpu)\r\n{\r\nstruct intel_shared_regs *regs;\r\nint i;\r\nregs = kzalloc_node(sizeof(struct intel_shared_regs),\r\nGFP_KERNEL, cpu_to_node(cpu));\r\nif (regs) {\r\nfor (i = 0; i < EXTRA_REG_MAX; i++)\r\nraw_spin_lock_init(&regs->regs[i].lock);\r\nregs->core_id = -1;\r\n}\r\nreturn regs;\r\n}\r\nstatic int intel_pmu_cpu_prepare(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nif (!(x86_pmu.extra_regs || x86_pmu.lbr_sel_map))\r\nreturn NOTIFY_OK;\r\ncpuc->shared_regs = allocate_shared_regs(cpu);\r\nif (!cpuc->shared_regs)\r\nreturn NOTIFY_BAD;\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void intel_pmu_cpu_starting(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nint core_id = topology_core_id(cpu);\r\nint i;\r\ninit_debug_store_on_cpu(cpu);\r\nintel_pmu_lbr_reset();\r\ncpuc->lbr_sel = NULL;\r\nif (!cpuc->shared_regs)\r\nreturn;\r\nif (!(x86_pmu.er_flags & ERF_NO_HT_SHARING)) {\r\nfor_each_cpu(i, topology_thread_cpumask(cpu)) {\r\nstruct intel_shared_regs *pc;\r\npc = per_cpu(cpu_hw_events, i).shared_regs;\r\nif (pc && pc->core_id == core_id) {\r\ncpuc->kfree_on_online = cpuc->shared_regs;\r\ncpuc->shared_regs = pc;\r\nbreak;\r\n}\r\n}\r\ncpuc->shared_regs->core_id = core_id;\r\ncpuc->shared_regs->refcnt++;\r\n}\r\nif (x86_pmu.lbr_sel_map)\r\ncpuc->lbr_sel = &cpuc->shared_regs->regs[EXTRA_REG_LBR];\r\n}\r\nstatic void intel_pmu_cpu_dying(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nstruct intel_shared_regs *pc;\r\npc = cpuc->shared_regs;\r\nif (pc) {\r\nif (pc->core_id == -1 || --pc->refcnt == 0)\r\nkfree(pc);\r\ncpuc->shared_regs = NULL;\r\n}\r\nfini_debug_store_on_cpu(cpu);\r\n}\r\nstatic void intel_pmu_flush_branch_stack(void)\r\n{\r\nif (x86_pmu.lbr_nr)\r\nintel_pmu_lbr_reset();\r\n}\r\nstatic __init void intel_clovertown_quirk(void)\r\n{\r\npr_warn("PEBS disabled due to CPU errata\n");\r\nx86_pmu.pebs = 0;\r\nx86_pmu.pebs_constraints = NULL;\r\n}\r\nstatic int intel_snb_pebs_broken(int cpu)\r\n{\r\nu32 rev = UINT_MAX;\r\nswitch (cpu_data(cpu).x86_model) {\r\ncase 42:\r\nrev = 0x28;\r\nbreak;\r\ncase 45:\r\nswitch (cpu_data(cpu).x86_mask) {\r\ncase 6: rev = 0x618; break;\r\ncase 7: rev = 0x70c; break;\r\n}\r\n}\r\nreturn (cpu_data(cpu).microcode < rev);\r\n}\r\nstatic void intel_snb_check_microcode(void)\r\n{\r\nint pebs_broken = 0;\r\nint cpu;\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu) {\r\nif ((pebs_broken = intel_snb_pebs_broken(cpu)))\r\nbreak;\r\n}\r\nput_online_cpus();\r\nif (pebs_broken == x86_pmu.pebs_broken)\r\nreturn;\r\nif (x86_pmu.pebs_broken) {\r\npr_info("PEBS enabled due to microcode update\n");\r\nx86_pmu.pebs_broken = 0;\r\n} else {\r\npr_info("PEBS disabled due to CPU errata, please upgrade microcode\n");\r\nx86_pmu.pebs_broken = 1;\r\n}\r\n}\r\nstatic bool check_msr(unsigned long msr, u64 mask)\r\n{\r\nu64 val_old, val_new, val_tmp;\r\nif (rdmsrl_safe(msr, &val_old))\r\nreturn false;\r\nval_tmp = val_old ^ mask;\r\nif (wrmsrl_safe(msr, val_tmp) ||\r\nrdmsrl_safe(msr, &val_new))\r\nreturn false;\r\nif (val_new != val_tmp)\r\nreturn false;\r\nwrmsrl(msr, val_old);\r\nreturn true;\r\n}\r\nstatic __init void intel_sandybridge_quirk(void)\r\n{\r\nx86_pmu.check_microcode = intel_snb_check_microcode;\r\nintel_snb_check_microcode();\r\n}\r\nstatic __init void intel_arch_events_quirk(void)\r\n{\r\nint bit;\r\nfor_each_set_bit(bit, x86_pmu.events_mask, ARRAY_SIZE(intel_arch_events_map)) {\r\nintel_perfmon_event_map[intel_arch_events_map[bit].id] = 0;\r\npr_warn("CPUID marked event: \'%s\' unavailable\n",\r\nintel_arch_events_map[bit].name);\r\n}\r\n}\r\nstatic __init void intel_nehalem_quirk(void)\r\n{\r\nunion cpuid10_ebx ebx;\r\nebx.full = x86_pmu.events_maskl;\r\nif (ebx.split.no_branch_misses_retired) {\r\nintel_perfmon_event_map[PERF_COUNT_HW_BRANCH_MISSES] = 0x7f89;\r\nebx.split.no_branch_misses_retired = 0;\r\nx86_pmu.events_maskl = ebx.full;\r\npr_info("CPU erratum AAJ80 worked around\n");\r\n}\r\n}\r\n__init int intel_pmu_init(void)\r\n{\r\nunion cpuid10_edx edx;\r\nunion cpuid10_eax eax;\r\nunion cpuid10_ebx ebx;\r\nstruct event_constraint *c;\r\nunsigned int unused;\r\nstruct extra_reg *er;\r\nint version, i;\r\nif (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {\r\nswitch (boot_cpu_data.x86) {\r\ncase 0x6:\r\nreturn p6_pmu_init();\r\ncase 0xb:\r\nreturn knc_pmu_init();\r\ncase 0xf:\r\nreturn p4_pmu_init();\r\n}\r\nreturn -ENODEV;\r\n}\r\ncpuid(10, &eax.full, &ebx.full, &unused, &edx.full);\r\nif (eax.split.mask_length < ARCH_PERFMON_EVENTS_COUNT)\r\nreturn -ENODEV;\r\nversion = eax.split.version_id;\r\nif (version < 2)\r\nx86_pmu = core_pmu;\r\nelse\r\nx86_pmu = intel_pmu;\r\nx86_pmu.version = version;\r\nx86_pmu.num_counters = eax.split.num_counters;\r\nx86_pmu.cntval_bits = eax.split.bit_width;\r\nx86_pmu.cntval_mask = (1ULL << eax.split.bit_width) - 1;\r\nx86_pmu.events_maskl = ebx.full;\r\nx86_pmu.events_mask_len = eax.split.mask_length;\r\nx86_pmu.max_pebs_events = min_t(unsigned, MAX_PEBS_EVENTS, x86_pmu.num_counters);\r\nif (version > 1)\r\nx86_pmu.num_counters_fixed = max((int)edx.split.num_counters_fixed, 3);\r\nif (boot_cpu_has(X86_FEATURE_PDCM)) {\r\nu64 capabilities;\r\nrdmsrl(MSR_IA32_PERF_CAPABILITIES, capabilities);\r\nx86_pmu.intel_cap.capabilities = capabilities;\r\n}\r\nintel_ds_init();\r\nx86_add_quirk(intel_arch_events_quirk);\r\nswitch (boot_cpu_data.x86_model) {\r\ncase 14:\r\npr_cont("Core events, ");\r\nbreak;\r\ncase 15:\r\nx86_add_quirk(intel_clovertown_quirk);\r\ncase 22:\r\ncase 23:\r\ncase 29:\r\nmemcpy(hw_cache_event_ids, core2_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nintel_pmu_lbr_init_core();\r\nx86_pmu.event_constraints = intel_core2_event_constraints;\r\nx86_pmu.pebs_constraints = intel_core2_pebs_event_constraints;\r\npr_cont("Core2 events, ");\r\nbreak;\r\ncase 26:\r\ncase 30:\r\ncase 46:\r\nmemcpy(hw_cache_event_ids, nehalem_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\r\nsizeof(hw_cache_extra_regs));\r\nintel_pmu_lbr_init_nhm();\r\nx86_pmu.event_constraints = intel_nehalem_event_constraints;\r\nx86_pmu.pebs_constraints = intel_nehalem_pebs_event_constraints;\r\nx86_pmu.enable_all = intel_pmu_nhm_enable_all;\r\nx86_pmu.extra_regs = intel_nehalem_extra_regs;\r\nx86_pmu.cpu_events = nhm_events_attrs;\r\nintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\r\nX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\r\nintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\r\nX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\r\nx86_add_quirk(intel_nehalem_quirk);\r\npr_cont("Nehalem events, ");\r\nbreak;\r\ncase 28:\r\ncase 38:\r\ncase 39:\r\ncase 53:\r\ncase 54:\r\nmemcpy(hw_cache_event_ids, atom_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nintel_pmu_lbr_init_atom();\r\nx86_pmu.event_constraints = intel_gen_event_constraints;\r\nx86_pmu.pebs_constraints = intel_atom_pebs_event_constraints;\r\npr_cont("Atom events, ");\r\nbreak;\r\ncase 55:\r\ncase 77:\r\nmemcpy(hw_cache_event_ids, slm_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nmemcpy(hw_cache_extra_regs, slm_hw_cache_extra_regs,\r\nsizeof(hw_cache_extra_regs));\r\nintel_pmu_lbr_init_atom();\r\nx86_pmu.event_constraints = intel_slm_event_constraints;\r\nx86_pmu.pebs_constraints = intel_slm_pebs_event_constraints;\r\nx86_pmu.extra_regs = intel_slm_extra_regs;\r\nx86_pmu.er_flags |= ERF_HAS_RSP_1;\r\npr_cont("Silvermont events, ");\r\nbreak;\r\ncase 37:\r\ncase 44:\r\ncase 47:\r\nmemcpy(hw_cache_event_ids, westmere_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\r\nsizeof(hw_cache_extra_regs));\r\nintel_pmu_lbr_init_nhm();\r\nx86_pmu.event_constraints = intel_westmere_event_constraints;\r\nx86_pmu.enable_all = intel_pmu_nhm_enable_all;\r\nx86_pmu.pebs_constraints = intel_westmere_pebs_event_constraints;\r\nx86_pmu.extra_regs = intel_westmere_extra_regs;\r\nx86_pmu.er_flags |= ERF_HAS_RSP_1;\r\nx86_pmu.cpu_events = nhm_events_attrs;\r\nintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\r\nX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\r\nintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\r\nX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\r\npr_cont("Westmere events, ");\r\nbreak;\r\ncase 42:\r\ncase 45:\r\nx86_add_quirk(intel_sandybridge_quirk);\r\nmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\r\nsizeof(hw_cache_extra_regs));\r\nintel_pmu_lbr_init_snb();\r\nx86_pmu.event_constraints = intel_snb_event_constraints;\r\nx86_pmu.pebs_constraints = intel_snb_pebs_event_constraints;\r\nx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\r\nif (boot_cpu_data.x86_model == 45)\r\nx86_pmu.extra_regs = intel_snbep_extra_regs;\r\nelse\r\nx86_pmu.extra_regs = intel_snb_extra_regs;\r\nx86_pmu.er_flags |= ERF_HAS_RSP_1;\r\nx86_pmu.er_flags |= ERF_NO_HT_SHARING;\r\nx86_pmu.cpu_events = snb_events_attrs;\r\nintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\r\nX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\r\nintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\r\nX86_CONFIG(.event=0xb1, .umask=0x01, .inv=1, .cmask=1);\r\npr_cont("SandyBridge events, ");\r\nbreak;\r\ncase 58:\r\ncase 62:\r\nmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nhw_cache_event_ids[C(DTLB)][C(OP_READ)][C(RESULT_MISS)] = 0x8108;\r\nmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\r\nsizeof(hw_cache_extra_regs));\r\nintel_pmu_lbr_init_snb();\r\nx86_pmu.event_constraints = intel_ivb_event_constraints;\r\nx86_pmu.pebs_constraints = intel_ivb_pebs_event_constraints;\r\nx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\r\nif (boot_cpu_data.x86_model == 62)\r\nx86_pmu.extra_regs = intel_snbep_extra_regs;\r\nelse\r\nx86_pmu.extra_regs = intel_snb_extra_regs;\r\nx86_pmu.er_flags |= ERF_HAS_RSP_1;\r\nx86_pmu.er_flags |= ERF_NO_HT_SHARING;\r\nx86_pmu.cpu_events = snb_events_attrs;\r\nintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\r\nX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\r\npr_cont("IvyBridge events, ");\r\nbreak;\r\ncase 60:\r\ncase 70:\r\ncase 71:\r\ncase 63:\r\ncase 69:\r\nx86_pmu.late_ack = true;\r\nmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids, sizeof(hw_cache_event_ids));\r\nmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs, sizeof(hw_cache_extra_regs));\r\nintel_pmu_lbr_init_snb();\r\nx86_pmu.event_constraints = intel_hsw_event_constraints;\r\nx86_pmu.pebs_constraints = intel_hsw_pebs_event_constraints;\r\nx86_pmu.extra_regs = intel_snb_extra_regs;\r\nx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\r\nx86_pmu.er_flags |= ERF_HAS_RSP_1;\r\nx86_pmu.er_flags |= ERF_NO_HT_SHARING;\r\nx86_pmu.hw_config = hsw_hw_config;\r\nx86_pmu.get_event_constraints = hsw_get_event_constraints;\r\nx86_pmu.cpu_events = hsw_events_attrs;\r\nx86_pmu.lbr_double_abort = true;\r\npr_cont("Haswell events, ");\r\nbreak;\r\ndefault:\r\nswitch (x86_pmu.version) {\r\ncase 1:\r\nx86_pmu.event_constraints = intel_v1_event_constraints;\r\npr_cont("generic architected perfmon v1, ");\r\nbreak;\r\ndefault:\r\nx86_pmu.event_constraints = intel_gen_event_constraints;\r\npr_cont("generic architected perfmon, ");\r\nbreak;\r\n}\r\n}\r\nif (x86_pmu.num_counters > INTEL_PMC_MAX_GENERIC) {\r\nWARN(1, KERN_ERR "hw perf events %d > max(%d), clipping!",\r\nx86_pmu.num_counters, INTEL_PMC_MAX_GENERIC);\r\nx86_pmu.num_counters = INTEL_PMC_MAX_GENERIC;\r\n}\r\nx86_pmu.intel_ctrl = (1 << x86_pmu.num_counters) - 1;\r\nif (x86_pmu.num_counters_fixed > INTEL_PMC_MAX_FIXED) {\r\nWARN(1, KERN_ERR "hw perf events fixed %d > max(%d), clipping!",\r\nx86_pmu.num_counters_fixed, INTEL_PMC_MAX_FIXED);\r\nx86_pmu.num_counters_fixed = INTEL_PMC_MAX_FIXED;\r\n}\r\nx86_pmu.intel_ctrl |=\r\n((1LL << x86_pmu.num_counters_fixed)-1) << INTEL_PMC_IDX_FIXED;\r\nif (x86_pmu.event_constraints) {\r\nfor_each_event_constraint(c, x86_pmu.event_constraints) {\r\nif (c->cmask != FIXED_EVENT_FLAGS\r\n|| c->idxmsk64 == INTEL_PMC_MSK_FIXED_REF_CYCLES) {\r\ncontinue;\r\n}\r\nc->idxmsk64 |= (1ULL << x86_pmu.num_counters) - 1;\r\nc->weight += x86_pmu.num_counters;\r\n}\r\n}\r\nif (x86_pmu.lbr_nr && !check_msr(x86_pmu.lbr_tos, 0x3UL))\r\nx86_pmu.lbr_nr = 0;\r\nfor (i = 0; i < x86_pmu.lbr_nr; i++) {\r\nif (!(check_msr(x86_pmu.lbr_from + i, 0xffffUL) &&\r\ncheck_msr(x86_pmu.lbr_to + i, 0xffffUL)))\r\nx86_pmu.lbr_nr = 0;\r\n}\r\nif (x86_pmu.extra_regs) {\r\nfor (er = x86_pmu.extra_regs; er->msr; er++) {\r\ner->extra_msr_access = check_msr(er->msr, 0x1ffUL);\r\nif ((er->idx == EXTRA_REG_LBR) && !er->extra_msr_access)\r\nx86_pmu.lbr_sel_map = NULL;\r\n}\r\n}\r\nif (x86_pmu.intel_cap.full_width_write) {\r\nx86_pmu.max_period = x86_pmu.cntval_mask;\r\nx86_pmu.perfctr = MSR_IA32_PMC0;\r\npr_cont("full-width counters, ");\r\n}\r\nreturn 0;\r\n}
