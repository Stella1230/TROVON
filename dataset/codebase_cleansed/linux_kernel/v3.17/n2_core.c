static unsigned long spu_next_offset(struct spu_queue *q, unsigned long off)\r\n{\r\nif (q->q_type == HV_NCS_QTYPE_MAU) {\r\noff += MAU_ENTRY_SIZE;\r\nif (off == (MAU_ENTRY_SIZE * MAU_NUM_ENTRIES))\r\noff = 0;\r\n} else {\r\noff += CWQ_ENTRY_SIZE;\r\nif (off == (CWQ_ENTRY_SIZE * CWQ_NUM_ENTRIES))\r\noff = 0;\r\n}\r\nreturn off;\r\n}\r\nstatic inline bool job_finished(struct spu_queue *q, unsigned int offset,\r\nunsigned long old_head, unsigned long new_head)\r\n{\r\nif (old_head <= new_head) {\r\nif (offset > old_head && offset <= new_head)\r\nreturn true;\r\n} else {\r\nif (offset > old_head || offset <= new_head)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic irqreturn_t cwq_intr(int irq, void *dev_id)\r\n{\r\nunsigned long off, new_head, hv_ret;\r\nstruct spu_queue *q = dev_id;\r\npr_err("CPU[%d]: Got CWQ interrupt for qhdl[%lx]\n",\r\nsmp_processor_id(), q->qhandle);\r\nspin_lock(&q->lock);\r\nhv_ret = sun4v_ncs_gethead(q->qhandle, &new_head);\r\npr_err("CPU[%d]: CWQ gethead[%lx] hv_ret[%lu]\n",\r\nsmp_processor_id(), new_head, hv_ret);\r\nfor (off = q->head; off != new_head; off = spu_next_offset(q, off)) {\r\n}\r\nhv_ret = sun4v_ncs_sethead_marker(q->qhandle, new_head);\r\nif (hv_ret == HV_EOK)\r\nq->head = new_head;\r\nspin_unlock(&q->lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t mau_intr(int irq, void *dev_id)\r\n{\r\nstruct spu_queue *q = dev_id;\r\nunsigned long head, hv_ret;\r\nspin_lock(&q->lock);\r\npr_err("CPU[%d]: Got MAU interrupt for qhdl[%lx]\n",\r\nsmp_processor_id(), q->qhandle);\r\nhv_ret = sun4v_ncs_gethead(q->qhandle, &head);\r\npr_err("CPU[%d]: MAU gethead[%lx] hv_ret[%lu]\n",\r\nsmp_processor_id(), head, hv_ret);\r\nsun4v_ncs_sethead_marker(q->qhandle, head);\r\nspin_unlock(&q->lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void *spu_queue_next(struct spu_queue *q, void *cur)\r\n{\r\nreturn q->q + spu_next_offset(q, cur - q->q);\r\n}\r\nstatic int spu_queue_num_free(struct spu_queue *q)\r\n{\r\nunsigned long head = q->head;\r\nunsigned long tail = q->tail;\r\nunsigned long end = (CWQ_ENTRY_SIZE * CWQ_NUM_ENTRIES);\r\nunsigned long diff;\r\nif (head > tail)\r\ndiff = head - tail;\r\nelse\r\ndiff = (end - tail) + head;\r\nreturn (diff / CWQ_ENTRY_SIZE) - 1;\r\n}\r\nstatic void *spu_queue_alloc(struct spu_queue *q, int num_entries)\r\n{\r\nint avail = spu_queue_num_free(q);\r\nif (avail >= num_entries)\r\nreturn q->q + q->tail;\r\nreturn NULL;\r\n}\r\nstatic unsigned long spu_queue_submit(struct spu_queue *q, void *last)\r\n{\r\nunsigned long hv_ret, new_tail;\r\nnew_tail = spu_next_offset(q, last - q->q);\r\nhv_ret = sun4v_ncs_settail(q->qhandle, new_tail);\r\nif (hv_ret == HV_EOK)\r\nq->tail = new_tail;\r\nreturn hv_ret;\r\n}\r\nstatic u64 control_word_base(unsigned int len, unsigned int hmac_key_len,\r\nint enc_type, int auth_type,\r\nunsigned int hash_len,\r\nbool sfas, bool sob, bool eob, bool encrypt,\r\nint opcode)\r\n{\r\nu64 word = (len - 1) & CONTROL_LEN;\r\nword |= ((u64) opcode << CONTROL_OPCODE_SHIFT);\r\nword |= ((u64) enc_type << CONTROL_ENC_TYPE_SHIFT);\r\nword |= ((u64) auth_type << CONTROL_AUTH_TYPE_SHIFT);\r\nif (sfas)\r\nword |= CONTROL_STORE_FINAL_AUTH_STATE;\r\nif (sob)\r\nword |= CONTROL_START_OF_BLOCK;\r\nif (eob)\r\nword |= CONTROL_END_OF_BLOCK;\r\nif (encrypt)\r\nword |= CONTROL_ENCRYPT;\r\nif (hmac_key_len)\r\nword |= ((u64) (hmac_key_len - 1)) << CONTROL_HMAC_KEY_LEN_SHIFT;\r\nif (hash_len)\r\nword |= ((u64) (hash_len - 1)) << CONTROL_HASH_LEN_SHIFT;\r\nreturn word;\r\n}\r\nstatic inline struct n2_ahash_alg *n2_ahash_alg(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct ahash_alg *ahash_alg;\r\nahash_alg = container_of(alg, struct ahash_alg, halg.base);\r\nreturn container_of(ahash_alg, struct n2_ahash_alg, alg);\r\n}\r\nstatic inline struct n2_hmac_alg *n2_hmac_alg(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct ahash_alg *ahash_alg;\r\nahash_alg = container_of(alg, struct ahash_alg, halg.base);\r\nreturn container_of(ahash_alg, struct n2_hmac_alg, derived.alg);\r\n}\r\nstatic int n2_hash_async_init(struct ahash_request *req)\r\n{\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_ahash_init(&rctx->fallback_req);\r\n}\r\nstatic int n2_hash_async_update(struct ahash_request *req)\r\n{\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nreturn crypto_ahash_update(&rctx->fallback_req);\r\n}\r\nstatic int n2_hash_async_final(struct ahash_request *req)\r\n{\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_final(&rctx->fallback_req);\r\n}\r\nstatic int n2_hash_async_finup(struct ahash_request *req)\r\n{\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_finup(&rctx->fallback_req);\r\n}\r\nstatic int n2_hash_cra_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *fallback_driver_name = crypto_tfm_alg_name(tfm);\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct crypto_ahash *fallback_tfm;\r\nint err;\r\nfallback_tfm = crypto_alloc_ahash(fallback_driver_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(fallback_tfm)) {\r\npr_warning("Fallback driver '%s' could not be loaded!\n",\r\nfallback_driver_name);\r\nerr = PTR_ERR(fallback_tfm);\r\ngoto out;\r\n}\r\ncrypto_ahash_set_reqsize(ahash, (sizeof(struct n2_hash_req_ctx) +\r\ncrypto_ahash_reqsize(fallback_tfm)));\r\nctx->fallback_tfm = fallback_tfm;\r\nreturn 0;\r\nout:\r\nreturn err;\r\n}\r\nstatic void n2_hash_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\ncrypto_free_ahash(ctx->fallback_tfm);\r\n}\r\nstatic int n2_hmac_cra_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *fallback_driver_name = crypto_tfm_alg_name(tfm);\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct n2_hmac_alg *n2alg = n2_hmac_alg(tfm);\r\nstruct crypto_ahash *fallback_tfm;\r\nstruct crypto_shash *child_shash;\r\nint err;\r\nfallback_tfm = crypto_alloc_ahash(fallback_driver_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(fallback_tfm)) {\r\npr_warning("Fallback driver '%s' could not be loaded!\n",\r\nfallback_driver_name);\r\nerr = PTR_ERR(fallback_tfm);\r\ngoto out;\r\n}\r\nchild_shash = crypto_alloc_shash(n2alg->child_alg, 0, 0);\r\nif (IS_ERR(child_shash)) {\r\npr_warning("Child shash '%s' could not be loaded!\n",\r\nn2alg->child_alg);\r\nerr = PTR_ERR(child_shash);\r\ngoto out_free_fallback;\r\n}\r\ncrypto_ahash_set_reqsize(ahash, (sizeof(struct n2_hash_req_ctx) +\r\ncrypto_ahash_reqsize(fallback_tfm)));\r\nctx->child_shash = child_shash;\r\nctx->base.fallback_tfm = fallback_tfm;\r\nreturn 0;\r\nout_free_fallback:\r\ncrypto_free_ahash(fallback_tfm);\r\nout:\r\nreturn err;\r\n}\r\nstatic void n2_hmac_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(ahash);\r\ncrypto_free_ahash(ctx->base.fallback_tfm);\r\ncrypto_free_shash(ctx->child_shash);\r\n}\r\nstatic int n2_hmac_async_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(tfm);\r\nstruct crypto_shash *child_shash = ctx->child_shash;\r\nstruct crypto_ahash *fallback_tfm;\r\nstruct {\r\nstruct shash_desc shash;\r\nchar ctx[crypto_shash_descsize(child_shash)];\r\n} desc;\r\nint err, bs, ds;\r\nfallback_tfm = ctx->base.fallback_tfm;\r\nerr = crypto_ahash_setkey(fallback_tfm, key, keylen);\r\nif (err)\r\nreturn err;\r\ndesc.shash.tfm = child_shash;\r\ndesc.shash.flags = crypto_ahash_get_flags(tfm) &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nbs = crypto_shash_blocksize(child_shash);\r\nds = crypto_shash_digestsize(child_shash);\r\nBUG_ON(ds > N2_HASH_KEY_MAX);\r\nif (keylen > bs) {\r\nerr = crypto_shash_digest(&desc.shash, key, keylen,\r\nctx->hash_key);\r\nif (err)\r\nreturn err;\r\nkeylen = ds;\r\n} else if (keylen <= N2_HASH_KEY_MAX)\r\nmemcpy(ctx->hash_key, key, keylen);\r\nctx->hash_key_len = keylen;\r\nreturn err;\r\n}\r\nstatic unsigned long wait_for_tail(struct spu_queue *qp)\r\n{\r\nunsigned long head, hv_ret;\r\ndo {\r\nhv_ret = sun4v_ncs_gethead(qp->qhandle, &head);\r\nif (hv_ret != HV_EOK) {\r\npr_err("Hypervisor error on gethead\n");\r\nbreak;\r\n}\r\nif (head == qp->tail) {\r\nqp->head = head;\r\nbreak;\r\n}\r\n} while (1);\r\nreturn hv_ret;\r\n}\r\nstatic unsigned long submit_and_wait_for_tail(struct spu_queue *qp,\r\nstruct cwq_initial_entry *ent)\r\n{\r\nunsigned long hv_ret = spu_queue_submit(qp, ent);\r\nif (hv_ret == HV_EOK)\r\nhv_ret = wait_for_tail(qp);\r\nreturn hv_ret;\r\n}\r\nstatic int n2_do_async_digest(struct ahash_request *req,\r\nunsigned int auth_type, unsigned int digest_size,\r\nunsigned int result_size, void *hash_loc,\r\nunsigned long auth_key, unsigned int auth_key_len)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct cwq_initial_entry *ent;\r\nstruct crypto_hash_walk walk;\r\nstruct spu_queue *qp;\r\nunsigned long flags;\r\nint err = -ENODEV;\r\nint nbytes, cpu;\r\nif (unlikely(req->nbytes > (1 << 16))) {\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags =\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_digest(&rctx->fallback_req);\r\n}\r\nnbytes = crypto_hash_walk_first(req, &walk);\r\ncpu = get_cpu();\r\nqp = cpu_to_cwq[cpu];\r\nif (!qp)\r\ngoto out;\r\nspin_lock_irqsave(&qp->lock, flags);\r\nent = qp->q + qp->tail;\r\nent->control = control_word_base(nbytes, auth_key_len, 0,\r\nauth_type, digest_size,\r\nfalse, true, false, false,\r\nOPCODE_INPLACE_BIT |\r\nOPCODE_AUTH_MAC);\r\nent->src_addr = __pa(walk.data);\r\nent->auth_key_addr = auth_key;\r\nent->auth_iv_addr = __pa(hash_loc);\r\nent->final_auth_state_addr = 0UL;\r\nent->enc_key_addr = 0UL;\r\nent->enc_iv_addr = 0UL;\r\nent->dest_addr = __pa(hash_loc);\r\nnbytes = crypto_hash_walk_done(&walk, 0);\r\nwhile (nbytes > 0) {\r\nent = spu_queue_next(qp, ent);\r\nent->control = (nbytes - 1);\r\nent->src_addr = __pa(walk.data);\r\nent->auth_key_addr = 0UL;\r\nent->auth_iv_addr = 0UL;\r\nent->final_auth_state_addr = 0UL;\r\nent->enc_key_addr = 0UL;\r\nent->enc_iv_addr = 0UL;\r\nent->dest_addr = 0UL;\r\nnbytes = crypto_hash_walk_done(&walk, 0);\r\n}\r\nent->control |= CONTROL_END_OF_BLOCK;\r\nif (submit_and_wait_for_tail(qp, ent) != HV_EOK)\r\nerr = -EINVAL;\r\nelse\r\nerr = 0;\r\nspin_unlock_irqrestore(&qp->lock, flags);\r\nif (!err)\r\nmemcpy(req->result, hash_loc, result_size);\r\nout:\r\nput_cpu();\r\nreturn err;\r\n}\r\nstatic int n2_hash_async_digest(struct ahash_request *req)\r\n{\r\nstruct n2_ahash_alg *n2alg = n2_ahash_alg(req->base.tfm);\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nint ds;\r\nds = n2alg->digest_size;\r\nif (unlikely(req->nbytes == 0)) {\r\nmemcpy(req->result, n2alg->hash_zero, ds);\r\nreturn 0;\r\n}\r\nmemcpy(&rctx->u, n2alg->hash_init, n2alg->hw_op_hashsz);\r\nreturn n2_do_async_digest(req, n2alg->auth_type,\r\nn2alg->hw_op_hashsz, ds,\r\n&rctx->u, 0UL, 0);\r\n}\r\nstatic int n2_hmac_async_digest(struct ahash_request *req)\r\n{\r\nstruct n2_hmac_alg *n2alg = n2_hmac_alg(req->base.tfm);\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct n2_hmac_ctx *ctx = crypto_ahash_ctx(tfm);\r\nint ds;\r\nds = n2alg->derived.digest_size;\r\nif (unlikely(req->nbytes == 0) ||\r\nunlikely(ctx->hash_key_len > N2_HASH_KEY_MAX)) {\r\nstruct n2_hash_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct n2_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags =\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_digest(&rctx->fallback_req);\r\n}\r\nmemcpy(&rctx->u, n2alg->derived.hash_init,\r\nn2alg->derived.hw_op_hashsz);\r\nreturn n2_do_async_digest(req, n2alg->derived.hmac_type,\r\nn2alg->derived.hw_op_hashsz, ds,\r\n&rctx->u,\r\n__pa(&ctx->hash_key),\r\nctx->hash_key_len);\r\n}\r\nstatic inline struct n2_cipher_alg *n2_cipher_alg(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nreturn container_of(alg, struct n2_cipher_alg, alg);\r\n}\r\nstatic int n2_aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);\r\nstruct n2_cipher_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct n2_cipher_alg *n2alg = n2_cipher_alg(tfm);\r\nctx->enc_type = (n2alg->enc_type & ENC_TYPE_CHAINING_MASK);\r\nswitch (keylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->enc_type |= ENC_TYPE_ALG_AES128;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->enc_type |= ENC_TYPE_ALG_AES192;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->enc_type |= ENC_TYPE_ALG_AES256;\r\nbreak;\r\ndefault:\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nctx->key_len = keylen;\r\nmemcpy(ctx->key.aes, key, keylen);\r\nreturn 0;\r\n}\r\nstatic int n2_des_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);\r\nstruct n2_cipher_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct n2_cipher_alg *n2alg = n2_cipher_alg(tfm);\r\nu32 tmp[DES_EXPKEY_WORDS];\r\nint err;\r\nctx->enc_type = n2alg->enc_type;\r\nif (keylen != DES_KEY_SIZE) {\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nerr = des_ekey(tmp, key);\r\nif (err == 0 && (tfm->crt_flags & CRYPTO_TFM_REQ_WEAK_KEY)) {\r\ntfm->crt_flags |= CRYPTO_TFM_RES_WEAK_KEY;\r\nreturn -EINVAL;\r\n}\r\nctx->key_len = keylen;\r\nmemcpy(ctx->key.des, key, keylen);\r\nreturn 0;\r\n}\r\nstatic int n2_3des_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);\r\nstruct n2_cipher_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct n2_cipher_alg *n2alg = n2_cipher_alg(tfm);\r\nctx->enc_type = n2alg->enc_type;\r\nif (keylen != (3 * DES_KEY_SIZE)) {\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nctx->key_len = keylen;\r\nmemcpy(ctx->key.des3, key, keylen);\r\nreturn 0;\r\n}\r\nstatic int n2_arc4_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);\r\nstruct n2_cipher_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct n2_cipher_alg *n2alg = n2_cipher_alg(tfm);\r\nu8 *s = ctx->key.arc4;\r\nu8 *x = s + 256;\r\nu8 *y = x + 1;\r\nint i, j, k;\r\nctx->enc_type = n2alg->enc_type;\r\nj = k = 0;\r\n*x = 0;\r\n*y = 0;\r\nfor (i = 0; i < 256; i++)\r\ns[i] = i;\r\nfor (i = 0; i < 256; i++) {\r\nu8 a = s[i];\r\nj = (j + key[k] + a) & 0xff;\r\ns[i] = s[j];\r\ns[j] = a;\r\nif (++k >= keylen)\r\nk = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int cipher_descriptor_len(int nbytes, unsigned int block_size)\r\n{\r\nint this_len = nbytes;\r\nthis_len -= (nbytes & (block_size - 1));\r\nreturn this_len > (1 << 16) ? (1 << 16) : this_len;\r\n}\r\nstatic int __n2_crypt_chunk(struct crypto_tfm *tfm, struct n2_crypto_chunk *cp,\r\nstruct spu_queue *qp, bool encrypt)\r\n{\r\nstruct n2_cipher_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct cwq_initial_entry *ent;\r\nbool in_place;\r\nint i;\r\nent = spu_queue_alloc(qp, cp->arr_len);\r\nif (!ent) {\r\npr_info("queue_alloc() of %d fails\n",\r\ncp->arr_len);\r\nreturn -EBUSY;\r\n}\r\nin_place = (cp->dest_paddr == cp->arr[0].src_paddr);\r\nent->control = control_word_base(cp->arr[0].src_len,\r\n0, ctx->enc_type, 0, 0,\r\nfalse, true, false, encrypt,\r\nOPCODE_ENCRYPT |\r\n(in_place ? OPCODE_INPLACE_BIT : 0));\r\nent->src_addr = cp->arr[0].src_paddr;\r\nent->auth_key_addr = 0UL;\r\nent->auth_iv_addr = 0UL;\r\nent->final_auth_state_addr = 0UL;\r\nent->enc_key_addr = __pa(&ctx->key);\r\nent->enc_iv_addr = cp->iv_paddr;\r\nent->dest_addr = (in_place ? 0UL : cp->dest_paddr);\r\nfor (i = 1; i < cp->arr_len; i++) {\r\nent = spu_queue_next(qp, ent);\r\nent->control = cp->arr[i].src_len - 1;\r\nent->src_addr = cp->arr[i].src_paddr;\r\nent->auth_key_addr = 0UL;\r\nent->auth_iv_addr = 0UL;\r\nent->final_auth_state_addr = 0UL;\r\nent->enc_key_addr = 0UL;\r\nent->enc_iv_addr = 0UL;\r\nent->dest_addr = 0UL;\r\n}\r\nent->control |= CONTROL_END_OF_BLOCK;\r\nreturn (spu_queue_submit(qp, ent) != HV_EOK) ? -EINVAL : 0;\r\n}\r\nstatic int n2_compute_chunks(struct ablkcipher_request *req)\r\n{\r\nstruct n2_request_context *rctx = ablkcipher_request_ctx(req);\r\nstruct ablkcipher_walk *walk = &rctx->walk;\r\nstruct n2_crypto_chunk *chunk;\r\nunsigned long dest_prev;\r\nunsigned int tot_len;\r\nbool prev_in_place;\r\nint err, nbytes;\r\nablkcipher_walk_init(walk, req->dst, req->src, req->nbytes);\r\nerr = ablkcipher_walk_phys(req, walk);\r\nif (err)\r\nreturn err;\r\nINIT_LIST_HEAD(&rctx->chunk_list);\r\nchunk = &rctx->chunk;\r\nINIT_LIST_HEAD(&chunk->entry);\r\nchunk->iv_paddr = 0UL;\r\nchunk->arr_len = 0;\r\nchunk->dest_paddr = 0UL;\r\nprev_in_place = false;\r\ndest_prev = ~0UL;\r\ntot_len = 0;\r\nwhile ((nbytes = walk->nbytes) != 0) {\r\nunsigned long dest_paddr, src_paddr;\r\nbool in_place;\r\nint this_len;\r\nsrc_paddr = (page_to_phys(walk->src.page) +\r\nwalk->src.offset);\r\ndest_paddr = (page_to_phys(walk->dst.page) +\r\nwalk->dst.offset);\r\nin_place = (src_paddr == dest_paddr);\r\nthis_len = cipher_descriptor_len(nbytes, walk->blocksize);\r\nif (chunk->arr_len != 0) {\r\nif (in_place != prev_in_place ||\r\n(!prev_in_place &&\r\ndest_paddr != dest_prev) ||\r\nchunk->arr_len == N2_CHUNK_ARR_LEN ||\r\ntot_len + this_len > (1 << 16)) {\r\nchunk->dest_final = dest_prev;\r\nlist_add_tail(&chunk->entry,\r\n&rctx->chunk_list);\r\nchunk = kzalloc(sizeof(*chunk), GFP_ATOMIC);\r\nif (!chunk) {\r\nerr = -ENOMEM;\r\nbreak;\r\n}\r\nINIT_LIST_HEAD(&chunk->entry);\r\n}\r\n}\r\nif (chunk->arr_len == 0) {\r\nchunk->dest_paddr = dest_paddr;\r\ntot_len = 0;\r\n}\r\nchunk->arr[chunk->arr_len].src_paddr = src_paddr;\r\nchunk->arr[chunk->arr_len].src_len = this_len;\r\nchunk->arr_len++;\r\ndest_prev = dest_paddr + this_len;\r\nprev_in_place = in_place;\r\ntot_len += this_len;\r\nerr = ablkcipher_walk_done(req, walk, nbytes - this_len);\r\nif (err)\r\nbreak;\r\n}\r\nif (!err && chunk->arr_len != 0) {\r\nchunk->dest_final = dest_prev;\r\nlist_add_tail(&chunk->entry, &rctx->chunk_list);\r\n}\r\nreturn err;\r\n}\r\nstatic void n2_chunk_complete(struct ablkcipher_request *req, void *final_iv)\r\n{\r\nstruct n2_request_context *rctx = ablkcipher_request_ctx(req);\r\nstruct n2_crypto_chunk *c, *tmp;\r\nif (final_iv)\r\nmemcpy(rctx->walk.iv, final_iv, rctx->walk.blocksize);\r\nablkcipher_walk_complete(&rctx->walk);\r\nlist_for_each_entry_safe(c, tmp, &rctx->chunk_list, entry) {\r\nlist_del(&c->entry);\r\nif (unlikely(c != &rctx->chunk))\r\nkfree(c);\r\n}\r\n}\r\nstatic int n2_do_ecb(struct ablkcipher_request *req, bool encrypt)\r\n{\r\nstruct n2_request_context *rctx = ablkcipher_request_ctx(req);\r\nstruct crypto_tfm *tfm = req->base.tfm;\r\nint err = n2_compute_chunks(req);\r\nstruct n2_crypto_chunk *c, *tmp;\r\nunsigned long flags, hv_ret;\r\nstruct spu_queue *qp;\r\nif (err)\r\nreturn err;\r\nqp = cpu_to_cwq[get_cpu()];\r\nerr = -ENODEV;\r\nif (!qp)\r\ngoto out;\r\nspin_lock_irqsave(&qp->lock, flags);\r\nlist_for_each_entry_safe(c, tmp, &rctx->chunk_list, entry) {\r\nerr = __n2_crypt_chunk(tfm, c, qp, encrypt);\r\nif (err)\r\nbreak;\r\nlist_del(&c->entry);\r\nif (unlikely(c != &rctx->chunk))\r\nkfree(c);\r\n}\r\nif (!err) {\r\nhv_ret = wait_for_tail(qp);\r\nif (hv_ret != HV_EOK)\r\nerr = -EINVAL;\r\n}\r\nspin_unlock_irqrestore(&qp->lock, flags);\r\nout:\r\nput_cpu();\r\nn2_chunk_complete(req, NULL);\r\nreturn err;\r\n}\r\nstatic int n2_encrypt_ecb(struct ablkcipher_request *req)\r\n{\r\nreturn n2_do_ecb(req, true);\r\n}\r\nstatic int n2_decrypt_ecb(struct ablkcipher_request *req)\r\n{\r\nreturn n2_do_ecb(req, false);\r\n}\r\nstatic int n2_do_chaining(struct ablkcipher_request *req, bool encrypt)\r\n{\r\nstruct n2_request_context *rctx = ablkcipher_request_ctx(req);\r\nstruct crypto_tfm *tfm = req->base.tfm;\r\nunsigned long flags, hv_ret, iv_paddr;\r\nint err = n2_compute_chunks(req);\r\nstruct n2_crypto_chunk *c, *tmp;\r\nstruct spu_queue *qp;\r\nvoid *final_iv_addr;\r\nfinal_iv_addr = NULL;\r\nif (err)\r\nreturn err;\r\nqp = cpu_to_cwq[get_cpu()];\r\nerr = -ENODEV;\r\nif (!qp)\r\ngoto out;\r\nspin_lock_irqsave(&qp->lock, flags);\r\nif (encrypt) {\r\niv_paddr = __pa(rctx->walk.iv);\r\nlist_for_each_entry_safe(c, tmp, &rctx->chunk_list,\r\nentry) {\r\nc->iv_paddr = iv_paddr;\r\nerr = __n2_crypt_chunk(tfm, c, qp, true);\r\nif (err)\r\nbreak;\r\niv_paddr = c->dest_final - rctx->walk.blocksize;\r\nlist_del(&c->entry);\r\nif (unlikely(c != &rctx->chunk))\r\nkfree(c);\r\n}\r\nfinal_iv_addr = __va(iv_paddr);\r\n} else {\r\nlist_for_each_entry_safe_reverse(c, tmp, &rctx->chunk_list,\r\nentry) {\r\nif (c == &rctx->chunk) {\r\niv_paddr = __pa(rctx->walk.iv);\r\n} else {\r\niv_paddr = (tmp->arr[tmp->arr_len-1].src_paddr +\r\ntmp->arr[tmp->arr_len-1].src_len -\r\nrctx->walk.blocksize);\r\n}\r\nif (!final_iv_addr) {\r\nunsigned long pa;\r\npa = (c->arr[c->arr_len-1].src_paddr +\r\nc->arr[c->arr_len-1].src_len -\r\nrctx->walk.blocksize);\r\nfinal_iv_addr = rctx->temp_iv;\r\nmemcpy(rctx->temp_iv, __va(pa),\r\nrctx->walk.blocksize);\r\n}\r\nc->iv_paddr = iv_paddr;\r\nerr = __n2_crypt_chunk(tfm, c, qp, false);\r\nif (err)\r\nbreak;\r\nlist_del(&c->entry);\r\nif (unlikely(c != &rctx->chunk))\r\nkfree(c);\r\n}\r\n}\r\nif (!err) {\r\nhv_ret = wait_for_tail(qp);\r\nif (hv_ret != HV_EOK)\r\nerr = -EINVAL;\r\n}\r\nspin_unlock_irqrestore(&qp->lock, flags);\r\nout:\r\nput_cpu();\r\nn2_chunk_complete(req, err ? NULL : final_iv_addr);\r\nreturn err;\r\n}\r\nstatic int n2_encrypt_chaining(struct ablkcipher_request *req)\r\n{\r\nreturn n2_do_chaining(req, true);\r\n}\r\nstatic int n2_decrypt_chaining(struct ablkcipher_request *req)\r\n{\r\nreturn n2_do_chaining(req, false);\r\n}\r\nstatic void __n2_unregister_algs(void)\r\n{\r\nstruct n2_cipher_alg *cipher, *cipher_tmp;\r\nstruct n2_ahash_alg *alg, *alg_tmp;\r\nstruct n2_hmac_alg *hmac, *hmac_tmp;\r\nlist_for_each_entry_safe(cipher, cipher_tmp, &cipher_algs, entry) {\r\ncrypto_unregister_alg(&cipher->alg);\r\nlist_del(&cipher->entry);\r\nkfree(cipher);\r\n}\r\nlist_for_each_entry_safe(hmac, hmac_tmp, &hmac_algs, derived.entry) {\r\ncrypto_unregister_ahash(&hmac->derived.alg);\r\nlist_del(&hmac->derived.entry);\r\nkfree(hmac);\r\n}\r\nlist_for_each_entry_safe(alg, alg_tmp, &ahash_algs, entry) {\r\ncrypto_unregister_ahash(&alg->alg);\r\nlist_del(&alg->entry);\r\nkfree(alg);\r\n}\r\n}\r\nstatic int n2_cipher_cra_init(struct crypto_tfm *tfm)\r\n{\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct n2_request_context);\r\nreturn 0;\r\n}\r\nstatic int __n2_register_one_cipher(const struct n2_cipher_tmpl *tmpl)\r\n{\r\nstruct n2_cipher_alg *p = kzalloc(sizeof(*p), GFP_KERNEL);\r\nstruct crypto_alg *alg;\r\nint err;\r\nif (!p)\r\nreturn -ENOMEM;\r\nalg = &p->alg;\r\nsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", tmpl->name);\r\nsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s-n2", tmpl->drv_name);\r\nalg->cra_priority = N2_CRA_PRIORITY;\r\nalg->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |\r\nCRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC;\r\nalg->cra_blocksize = tmpl->block_size;\r\np->enc_type = tmpl->enc_type;\r\nalg->cra_ctxsize = sizeof(struct n2_cipher_context);\r\nalg->cra_type = &crypto_ablkcipher_type;\r\nalg->cra_u.ablkcipher = tmpl->ablkcipher;\r\nalg->cra_init = n2_cipher_cra_init;\r\nalg->cra_module = THIS_MODULE;\r\nlist_add(&p->entry, &cipher_algs);\r\nerr = crypto_register_alg(alg);\r\nif (err) {\r\npr_err("%s alg registration failed\n", alg->cra_name);\r\nlist_del(&p->entry);\r\nkfree(p);\r\n} else {\r\npr_info("%s alg registered\n", alg->cra_name);\r\n}\r\nreturn err;\r\n}\r\nstatic int __n2_register_one_hmac(struct n2_ahash_alg *n2ahash)\r\n{\r\nstruct n2_hmac_alg *p = kzalloc(sizeof(*p), GFP_KERNEL);\r\nstruct ahash_alg *ahash;\r\nstruct crypto_alg *base;\r\nint err;\r\nif (!p)\r\nreturn -ENOMEM;\r\np->child_alg = n2ahash->alg.halg.base.cra_name;\r\nmemcpy(&p->derived, n2ahash, sizeof(struct n2_ahash_alg));\r\nINIT_LIST_HEAD(&p->derived.entry);\r\nahash = &p->derived.alg;\r\nahash->digest = n2_hmac_async_digest;\r\nahash->setkey = n2_hmac_async_setkey;\r\nbase = &ahash->halg.base;\r\nsnprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, "hmac(%s)", p->child_alg);\r\nsnprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, "hmac-%s-n2", p->child_alg);\r\nbase->cra_ctxsize = sizeof(struct n2_hmac_ctx);\r\nbase->cra_init = n2_hmac_cra_init;\r\nbase->cra_exit = n2_hmac_cra_exit;\r\nlist_add(&p->derived.entry, &hmac_algs);\r\nerr = crypto_register_ahash(ahash);\r\nif (err) {\r\npr_err("%s alg registration failed\n", base->cra_name);\r\nlist_del(&p->derived.entry);\r\nkfree(p);\r\n} else {\r\npr_info("%s alg registered\n", base->cra_name);\r\n}\r\nreturn err;\r\n}\r\nstatic int __n2_register_one_ahash(const struct n2_hash_tmpl *tmpl)\r\n{\r\nstruct n2_ahash_alg *p = kzalloc(sizeof(*p), GFP_KERNEL);\r\nstruct hash_alg_common *halg;\r\nstruct crypto_alg *base;\r\nstruct ahash_alg *ahash;\r\nint err;\r\nif (!p)\r\nreturn -ENOMEM;\r\np->hash_zero = tmpl->hash_zero;\r\np->hash_init = tmpl->hash_init;\r\np->auth_type = tmpl->auth_type;\r\np->hmac_type = tmpl->hmac_type;\r\np->hw_op_hashsz = tmpl->hw_op_hashsz;\r\np->digest_size = tmpl->digest_size;\r\nahash = &p->alg;\r\nahash->init = n2_hash_async_init;\r\nahash->update = n2_hash_async_update;\r\nahash->final = n2_hash_async_final;\r\nahash->finup = n2_hash_async_finup;\r\nahash->digest = n2_hash_async_digest;\r\nhalg = &ahash->halg;\r\nhalg->digestsize = tmpl->digest_size;\r\nbase = &halg->base;\r\nsnprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, "%s", tmpl->name);\r\nsnprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s-n2", tmpl->name);\r\nbase->cra_priority = N2_CRA_PRIORITY;\r\nbase->cra_flags = CRYPTO_ALG_TYPE_AHASH |\r\nCRYPTO_ALG_KERN_DRIVER_ONLY |\r\nCRYPTO_ALG_NEED_FALLBACK;\r\nbase->cra_blocksize = tmpl->block_size;\r\nbase->cra_ctxsize = sizeof(struct n2_hash_ctx);\r\nbase->cra_module = THIS_MODULE;\r\nbase->cra_init = n2_hash_cra_init;\r\nbase->cra_exit = n2_hash_cra_exit;\r\nlist_add(&p->entry, &ahash_algs);\r\nerr = crypto_register_ahash(ahash);\r\nif (err) {\r\npr_err("%s alg registration failed\n", base->cra_name);\r\nlist_del(&p->entry);\r\nkfree(p);\r\n} else {\r\npr_info("%s alg registered\n", base->cra_name);\r\n}\r\nif (!err && p->hmac_type != AUTH_TYPE_RESERVED)\r\nerr = __n2_register_one_hmac(p);\r\nreturn err;\r\n}\r\nstatic int n2_register_algs(void)\r\n{\r\nint i, err = 0;\r\nmutex_lock(&spu_lock);\r\nif (algs_registered++)\r\ngoto out;\r\nfor (i = 0; i < NUM_HASH_TMPLS; i++) {\r\nerr = __n2_register_one_ahash(&hash_tmpls[i]);\r\nif (err) {\r\n__n2_unregister_algs();\r\ngoto out;\r\n}\r\n}\r\nfor (i = 0; i < NUM_CIPHER_TMPLS; i++) {\r\nerr = __n2_register_one_cipher(&cipher_tmpls[i]);\r\nif (err) {\r\n__n2_unregister_algs();\r\ngoto out;\r\n}\r\n}\r\nout:\r\nmutex_unlock(&spu_lock);\r\nreturn err;\r\n}\r\nstatic void n2_unregister_algs(void)\r\n{\r\nmutex_lock(&spu_lock);\r\nif (!--algs_registered)\r\n__n2_unregister_algs();\r\nmutex_unlock(&spu_lock);\r\n}\r\nstatic int find_devino_index(struct platform_device *dev, struct spu_mdesc_info *ip,\r\nunsigned long dev_ino)\r\n{\r\nconst unsigned int *dev_intrs;\r\nunsigned int intr;\r\nint i;\r\nfor (i = 0; i < ip->num_intrs; i++) {\r\nif (ip->ino_table[i].ino == dev_ino)\r\nbreak;\r\n}\r\nif (i == ip->num_intrs)\r\nreturn -ENODEV;\r\nintr = ip->ino_table[i].intr;\r\ndev_intrs = of_get_property(dev->dev.of_node, "interrupts", NULL);\r\nif (!dev_intrs)\r\nreturn -ENODEV;\r\nfor (i = 0; i < dev->archdata.num_irqs; i++) {\r\nif (dev_intrs[i] == intr)\r\nreturn i;\r\n}\r\nreturn -ENODEV;\r\n}\r\nstatic int spu_map_ino(struct platform_device *dev, struct spu_mdesc_info *ip,\r\nconst char *irq_name, struct spu_queue *p,\r\nirq_handler_t handler)\r\n{\r\nunsigned long herr;\r\nint index;\r\nherr = sun4v_ncs_qhandle_to_devino(p->qhandle, &p->devino);\r\nif (herr)\r\nreturn -EINVAL;\r\nindex = find_devino_index(dev, ip, p->devino);\r\nif (index < 0)\r\nreturn index;\r\np->irq = dev->archdata.irqs[index];\r\nsprintf(p->irq_name, "%s-%d", irq_name, index);\r\nreturn request_irq(p->irq, handler, 0, p->irq_name, p);\r\n}\r\nstatic void *new_queue(unsigned long q_type)\r\n{\r\nreturn kmem_cache_zalloc(queue_cache[q_type - 1], GFP_KERNEL);\r\n}\r\nstatic void free_queue(void *p, unsigned long q_type)\r\n{\r\nreturn kmem_cache_free(queue_cache[q_type - 1], p);\r\n}\r\nstatic int queue_cache_init(void)\r\n{\r\nif (!queue_cache[HV_NCS_QTYPE_MAU - 1])\r\nqueue_cache[HV_NCS_QTYPE_MAU - 1] =\r\nkmem_cache_create("mau_queue",\r\n(MAU_NUM_ENTRIES *\r\nMAU_ENTRY_SIZE),\r\nMAU_ENTRY_SIZE, 0, NULL);\r\nif (!queue_cache[HV_NCS_QTYPE_MAU - 1])\r\nreturn -ENOMEM;\r\nif (!queue_cache[HV_NCS_QTYPE_CWQ - 1])\r\nqueue_cache[HV_NCS_QTYPE_CWQ - 1] =\r\nkmem_cache_create("cwq_queue",\r\n(CWQ_NUM_ENTRIES *\r\nCWQ_ENTRY_SIZE),\r\nCWQ_ENTRY_SIZE, 0, NULL);\r\nif (!queue_cache[HV_NCS_QTYPE_CWQ - 1]) {\r\nkmem_cache_destroy(queue_cache[HV_NCS_QTYPE_MAU - 1]);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void queue_cache_destroy(void)\r\n{\r\nkmem_cache_destroy(queue_cache[HV_NCS_QTYPE_MAU - 1]);\r\nkmem_cache_destroy(queue_cache[HV_NCS_QTYPE_CWQ - 1]);\r\n}\r\nstatic int spu_queue_register(struct spu_queue *p, unsigned long q_type)\r\n{\r\ncpumask_var_t old_allowed;\r\nunsigned long hv_ret;\r\nif (cpumask_empty(&p->sharing))\r\nreturn -EINVAL;\r\nif (!alloc_cpumask_var(&old_allowed, GFP_KERNEL))\r\nreturn -ENOMEM;\r\ncpumask_copy(old_allowed, &current->cpus_allowed);\r\nset_cpus_allowed_ptr(current, &p->sharing);\r\nhv_ret = sun4v_ncs_qconf(q_type, __pa(p->q),\r\nCWQ_NUM_ENTRIES, &p->qhandle);\r\nif (!hv_ret)\r\nsun4v_ncs_sethead_marker(p->qhandle, 0);\r\nset_cpus_allowed_ptr(current, old_allowed);\r\nfree_cpumask_var(old_allowed);\r\nreturn (hv_ret ? -EINVAL : 0);\r\n}\r\nstatic int spu_queue_setup(struct spu_queue *p)\r\n{\r\nint err;\r\np->q = new_queue(p->q_type);\r\nif (!p->q)\r\nreturn -ENOMEM;\r\nerr = spu_queue_register(p, p->q_type);\r\nif (err) {\r\nfree_queue(p->q, p->q_type);\r\np->q = NULL;\r\n}\r\nreturn err;\r\n}\r\nstatic void spu_queue_destroy(struct spu_queue *p)\r\n{\r\nunsigned long hv_ret;\r\nif (!p->q)\r\nreturn;\r\nhv_ret = sun4v_ncs_qconf(p->q_type, p->qhandle, 0, &p->qhandle);\r\nif (!hv_ret)\r\nfree_queue(p->q, p->q_type);\r\n}\r\nstatic void spu_list_destroy(struct list_head *list)\r\n{\r\nstruct spu_queue *p, *n;\r\nlist_for_each_entry_safe(p, n, list, list) {\r\nint i;\r\nfor (i = 0; i < NR_CPUS; i++) {\r\nif (cpu_to_cwq[i] == p)\r\ncpu_to_cwq[i] = NULL;\r\n}\r\nif (p->irq) {\r\nfree_irq(p->irq, p);\r\np->irq = 0;\r\n}\r\nspu_queue_destroy(p);\r\nlist_del(&p->list);\r\nkfree(p);\r\n}\r\n}\r\nstatic int spu_mdesc_walk_arcs(struct mdesc_handle *mdesc,\r\nstruct platform_device *dev,\r\nu64 node, struct spu_queue *p,\r\nstruct spu_queue **table)\r\n{\r\nu64 arc;\r\nmdesc_for_each_arc(arc, mdesc, node, MDESC_ARC_TYPE_BACK) {\r\nu64 tgt = mdesc_arc_target(mdesc, arc);\r\nconst char *name = mdesc_node_name(mdesc, tgt);\r\nconst u64 *id;\r\nif (strcmp(name, "cpu"))\r\ncontinue;\r\nid = mdesc_get_property(mdesc, tgt, "id", NULL);\r\nif (table[*id] != NULL) {\r\ndev_err(&dev->dev, "%s: SPU cpu slot already set.\n",\r\ndev->dev.of_node->full_name);\r\nreturn -EINVAL;\r\n}\r\ncpu_set(*id, p->sharing);\r\ntable[*id] = p;\r\n}\r\nreturn 0;\r\n}\r\nstatic int handle_exec_unit(struct spu_mdesc_info *ip, struct list_head *list,\r\nstruct platform_device *dev, struct mdesc_handle *mdesc,\r\nu64 node, const char *iname, unsigned long q_type,\r\nirq_handler_t handler, struct spu_queue **table)\r\n{\r\nstruct spu_queue *p;\r\nint err;\r\np = kzalloc(sizeof(struct spu_queue), GFP_KERNEL);\r\nif (!p) {\r\ndev_err(&dev->dev, "%s: Could not allocate SPU queue.\n",\r\ndev->dev.of_node->full_name);\r\nreturn -ENOMEM;\r\n}\r\ncpus_clear(p->sharing);\r\nspin_lock_init(&p->lock);\r\np->q_type = q_type;\r\nINIT_LIST_HEAD(&p->jobs);\r\nlist_add(&p->list, list);\r\nerr = spu_mdesc_walk_arcs(mdesc, dev, node, p, table);\r\nif (err)\r\nreturn err;\r\nerr = spu_queue_setup(p);\r\nif (err)\r\nreturn err;\r\nreturn spu_map_ino(dev, ip, iname, p, handler);\r\n}\r\nstatic int spu_mdesc_scan(struct mdesc_handle *mdesc, struct platform_device *dev,\r\nstruct spu_mdesc_info *ip, struct list_head *list,\r\nconst char *exec_name, unsigned long q_type,\r\nirq_handler_t handler, struct spu_queue **table)\r\n{\r\nint err = 0;\r\nu64 node;\r\nmdesc_for_each_node_by_name(mdesc, node, "exec-unit") {\r\nconst char *type;\r\ntype = mdesc_get_property(mdesc, node, "type", NULL);\r\nif (!type || strcmp(type, exec_name))\r\ncontinue;\r\nerr = handle_exec_unit(ip, list, dev, mdesc, node,\r\nexec_name, q_type, handler, table);\r\nif (err) {\r\nspu_list_destroy(list);\r\nbreak;\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic int get_irq_props(struct mdesc_handle *mdesc, u64 node,\r\nstruct spu_mdesc_info *ip)\r\n{\r\nconst u64 *ino;\r\nint ino_len;\r\nint i;\r\nino = mdesc_get_property(mdesc, node, "ino", &ino_len);\r\nif (!ino) {\r\nprintk("NO 'ino'\n");\r\nreturn -ENODEV;\r\n}\r\nip->num_intrs = ino_len / sizeof(u64);\r\nip->ino_table = kzalloc((sizeof(struct ino_blob) *\r\nip->num_intrs),\r\nGFP_KERNEL);\r\nif (!ip->ino_table)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < ip->num_intrs; i++) {\r\nstruct ino_blob *b = &ip->ino_table[i];\r\nb->intr = i + 1;\r\nb->ino = ino[i];\r\n}\r\nreturn 0;\r\n}\r\nstatic int grab_mdesc_irq_props(struct mdesc_handle *mdesc,\r\nstruct platform_device *dev,\r\nstruct spu_mdesc_info *ip,\r\nconst char *node_name)\r\n{\r\nconst unsigned int *reg;\r\nu64 node;\r\nreg = of_get_property(dev->dev.of_node, "reg", NULL);\r\nif (!reg)\r\nreturn -ENODEV;\r\nmdesc_for_each_node_by_name(mdesc, node, "virtual-device") {\r\nconst char *name;\r\nconst u64 *chdl;\r\nname = mdesc_get_property(mdesc, node, "name", NULL);\r\nif (!name || strcmp(name, node_name))\r\ncontinue;\r\nchdl = mdesc_get_property(mdesc, node, "cfg-handle", NULL);\r\nif (!chdl || (*chdl != *reg))\r\ncontinue;\r\nip->cfg_handle = *chdl;\r\nreturn get_irq_props(mdesc, node, ip);\r\n}\r\nreturn -ENODEV;\r\n}\r\nstatic int n2_spu_hvapi_register(void)\r\n{\r\nint err;\r\nn2_spu_hvapi_major = 2;\r\nn2_spu_hvapi_minor = 0;\r\nerr = sun4v_hvapi_register(HV_GRP_NCS,\r\nn2_spu_hvapi_major,\r\n&n2_spu_hvapi_minor);\r\nif (!err)\r\npr_info("Registered NCS HVAPI version %lu.%lu\n",\r\nn2_spu_hvapi_major,\r\nn2_spu_hvapi_minor);\r\nreturn err;\r\n}\r\nstatic void n2_spu_hvapi_unregister(void)\r\n{\r\nsun4v_hvapi_unregister(HV_GRP_NCS);\r\n}\r\nstatic int grab_global_resources(void)\r\n{\r\nint err = 0;\r\nmutex_lock(&spu_lock);\r\nif (global_ref++)\r\ngoto out;\r\nerr = n2_spu_hvapi_register();\r\nif (err)\r\ngoto out;\r\nerr = queue_cache_init();\r\nif (err)\r\ngoto out_hvapi_release;\r\nerr = -ENOMEM;\r\ncpu_to_cwq = kzalloc(sizeof(struct spu_queue *) * NR_CPUS,\r\nGFP_KERNEL);\r\nif (!cpu_to_cwq)\r\ngoto out_queue_cache_destroy;\r\ncpu_to_mau = kzalloc(sizeof(struct spu_queue *) * NR_CPUS,\r\nGFP_KERNEL);\r\nif (!cpu_to_mau)\r\ngoto out_free_cwq_table;\r\nerr = 0;\r\nout:\r\nif (err)\r\nglobal_ref--;\r\nmutex_unlock(&spu_lock);\r\nreturn err;\r\nout_free_cwq_table:\r\nkfree(cpu_to_cwq);\r\ncpu_to_cwq = NULL;\r\nout_queue_cache_destroy:\r\nqueue_cache_destroy();\r\nout_hvapi_release:\r\nn2_spu_hvapi_unregister();\r\ngoto out;\r\n}\r\nstatic void release_global_resources(void)\r\n{\r\nmutex_lock(&spu_lock);\r\nif (!--global_ref) {\r\nkfree(cpu_to_cwq);\r\ncpu_to_cwq = NULL;\r\nkfree(cpu_to_mau);\r\ncpu_to_mau = NULL;\r\nqueue_cache_destroy();\r\nn2_spu_hvapi_unregister();\r\n}\r\nmutex_unlock(&spu_lock);\r\n}\r\nstatic struct n2_crypto *alloc_n2cp(void)\r\n{\r\nstruct n2_crypto *np = kzalloc(sizeof(struct n2_crypto), GFP_KERNEL);\r\nif (np)\r\nINIT_LIST_HEAD(&np->cwq_list);\r\nreturn np;\r\n}\r\nstatic void free_n2cp(struct n2_crypto *np)\r\n{\r\nif (np->cwq_info.ino_table) {\r\nkfree(np->cwq_info.ino_table);\r\nnp->cwq_info.ino_table = NULL;\r\n}\r\nkfree(np);\r\n}\r\nstatic void n2_spu_driver_version(void)\r\n{\r\nstatic int n2_spu_version_printed;\r\nif (n2_spu_version_printed++ == 0)\r\npr_info("%s", version);\r\n}\r\nstatic int n2_crypto_probe(struct platform_device *dev)\r\n{\r\nstruct mdesc_handle *mdesc;\r\nconst char *full_name;\r\nstruct n2_crypto *np;\r\nint err;\r\nn2_spu_driver_version();\r\nfull_name = dev->dev.of_node->full_name;\r\npr_info("Found N2CP at %s\n", full_name);\r\nnp = alloc_n2cp();\r\nif (!np) {\r\ndev_err(&dev->dev, "%s: Unable to allocate n2cp.\n",\r\nfull_name);\r\nreturn -ENOMEM;\r\n}\r\nerr = grab_global_resources();\r\nif (err) {\r\ndev_err(&dev->dev, "%s: Unable to grab "\r\n"global resources.\n", full_name);\r\ngoto out_free_n2cp;\r\n}\r\nmdesc = mdesc_grab();\r\nif (!mdesc) {\r\ndev_err(&dev->dev, "%s: Unable to grab MDESC.\n",\r\nfull_name);\r\nerr = -ENODEV;\r\ngoto out_free_global;\r\n}\r\nerr = grab_mdesc_irq_props(mdesc, dev, &np->cwq_info, "n2cp");\r\nif (err) {\r\ndev_err(&dev->dev, "%s: Unable to grab IRQ props.\n",\r\nfull_name);\r\nmdesc_release(mdesc);\r\ngoto out_free_global;\r\n}\r\nerr = spu_mdesc_scan(mdesc, dev, &np->cwq_info, &np->cwq_list,\r\n"cwq", HV_NCS_QTYPE_CWQ, cwq_intr,\r\ncpu_to_cwq);\r\nmdesc_release(mdesc);\r\nif (err) {\r\ndev_err(&dev->dev, "%s: CWQ MDESC scan failed.\n",\r\nfull_name);\r\ngoto out_free_global;\r\n}\r\nerr = n2_register_algs();\r\nif (err) {\r\ndev_err(&dev->dev, "%s: Unable to register algorithms.\n",\r\nfull_name);\r\ngoto out_free_spu_list;\r\n}\r\ndev_set_drvdata(&dev->dev, np);\r\nreturn 0;\r\nout_free_spu_list:\r\nspu_list_destroy(&np->cwq_list);\r\nout_free_global:\r\nrelease_global_resources();\r\nout_free_n2cp:\r\nfree_n2cp(np);\r\nreturn err;\r\n}\r\nstatic int n2_crypto_remove(struct platform_device *dev)\r\n{\r\nstruct n2_crypto *np = dev_get_drvdata(&dev->dev);\r\nn2_unregister_algs();\r\nspu_list_destroy(&np->cwq_list);\r\nrelease_global_resources();\r\nfree_n2cp(np);\r\nreturn 0;\r\n}\r\nstatic struct n2_mau *alloc_ncp(void)\r\n{\r\nstruct n2_mau *mp = kzalloc(sizeof(struct n2_mau), GFP_KERNEL);\r\nif (mp)\r\nINIT_LIST_HEAD(&mp->mau_list);\r\nreturn mp;\r\n}\r\nstatic void free_ncp(struct n2_mau *mp)\r\n{\r\nif (mp->mau_info.ino_table) {\r\nkfree(mp->mau_info.ino_table);\r\nmp->mau_info.ino_table = NULL;\r\n}\r\nkfree(mp);\r\n}\r\nstatic int n2_mau_probe(struct platform_device *dev)\r\n{\r\nstruct mdesc_handle *mdesc;\r\nconst char *full_name;\r\nstruct n2_mau *mp;\r\nint err;\r\nn2_spu_driver_version();\r\nfull_name = dev->dev.of_node->full_name;\r\npr_info("Found NCP at %s\n", full_name);\r\nmp = alloc_ncp();\r\nif (!mp) {\r\ndev_err(&dev->dev, "%s: Unable to allocate ncp.\n",\r\nfull_name);\r\nreturn -ENOMEM;\r\n}\r\nerr = grab_global_resources();\r\nif (err) {\r\ndev_err(&dev->dev, "%s: Unable to grab "\r\n"global resources.\n", full_name);\r\ngoto out_free_ncp;\r\n}\r\nmdesc = mdesc_grab();\r\nif (!mdesc) {\r\ndev_err(&dev->dev, "%s: Unable to grab MDESC.\n",\r\nfull_name);\r\nerr = -ENODEV;\r\ngoto out_free_global;\r\n}\r\nerr = grab_mdesc_irq_props(mdesc, dev, &mp->mau_info, "ncp");\r\nif (err) {\r\ndev_err(&dev->dev, "%s: Unable to grab IRQ props.\n",\r\nfull_name);\r\nmdesc_release(mdesc);\r\ngoto out_free_global;\r\n}\r\nerr = spu_mdesc_scan(mdesc, dev, &mp->mau_info, &mp->mau_list,\r\n"mau", HV_NCS_QTYPE_MAU, mau_intr,\r\ncpu_to_mau);\r\nmdesc_release(mdesc);\r\nif (err) {\r\ndev_err(&dev->dev, "%s: MAU MDESC scan failed.\n",\r\nfull_name);\r\ngoto out_free_global;\r\n}\r\ndev_set_drvdata(&dev->dev, mp);\r\nreturn 0;\r\nout_free_global:\r\nrelease_global_resources();\r\nout_free_ncp:\r\nfree_ncp(mp);\r\nreturn err;\r\n}\r\nstatic int n2_mau_remove(struct platform_device *dev)\r\n{\r\nstruct n2_mau *mp = dev_get_drvdata(&dev->dev);\r\nspu_list_destroy(&mp->mau_list);\r\nrelease_global_resources();\r\nfree_ncp(mp);\r\nreturn 0;\r\n}\r\nstatic int __init n2_init(void)\r\n{\r\nint err = platform_driver_register(&n2_crypto_driver);\r\nif (!err) {\r\nerr = platform_driver_register(&n2_mau_driver);\r\nif (err)\r\nplatform_driver_unregister(&n2_crypto_driver);\r\n}\r\nreturn err;\r\n}\r\nstatic void __exit n2_exit(void)\r\n{\r\nplatform_driver_unregister(&n2_mau_driver);\r\nplatform_driver_unregister(&n2_crypto_driver);\r\n}
