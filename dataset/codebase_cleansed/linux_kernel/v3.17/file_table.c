static void file_free_rcu(struct rcu_head *head)\r\n{\r\nstruct file *f = container_of(head, struct file, f_u.fu_rcuhead);\r\nput_cred(f->f_cred);\r\nkmem_cache_free(filp_cachep, f);\r\n}\r\nstatic inline void file_free(struct file *f)\r\n{\r\npercpu_counter_dec(&nr_files);\r\ncall_rcu(&f->f_u.fu_rcuhead, file_free_rcu);\r\n}\r\nstatic long get_nr_files(void)\r\n{\r\nreturn percpu_counter_read_positive(&nr_files);\r\n}\r\nunsigned long get_max_files(void)\r\n{\r\nreturn files_stat.max_files;\r\n}\r\nint proc_nr_files(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp, loff_t *ppos)\r\n{\r\nfiles_stat.nr_files = get_nr_files();\r\nreturn proc_doulongvec_minmax(table, write, buffer, lenp, ppos);\r\n}\r\nint proc_nr_files(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp, loff_t *ppos)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstruct file *get_empty_filp(void)\r\n{\r\nconst struct cred *cred = current_cred();\r\nstatic long old_max;\r\nstruct file *f;\r\nint error;\r\nif (get_nr_files() >= files_stat.max_files && !capable(CAP_SYS_ADMIN)) {\r\nif (percpu_counter_sum_positive(&nr_files) >= files_stat.max_files)\r\ngoto over;\r\n}\r\nf = kmem_cache_zalloc(filp_cachep, GFP_KERNEL);\r\nif (unlikely(!f))\r\nreturn ERR_PTR(-ENOMEM);\r\npercpu_counter_inc(&nr_files);\r\nf->f_cred = get_cred(cred);\r\nerror = security_file_alloc(f);\r\nif (unlikely(error)) {\r\nfile_free(f);\r\nreturn ERR_PTR(error);\r\n}\r\natomic_long_set(&f->f_count, 1);\r\nrwlock_init(&f->f_owner.lock);\r\nspin_lock_init(&f->f_lock);\r\nmutex_init(&f->f_pos_lock);\r\neventpoll_init_file(f);\r\nreturn f;\r\nover:\r\nif (get_nr_files() > old_max) {\r\npr_info("VFS: file-max limit %lu reached\n", get_max_files());\r\nold_max = get_nr_files();\r\n}\r\nreturn ERR_PTR(-ENFILE);\r\n}\r\nstruct file *alloc_file(struct path *path, fmode_t mode,\r\nconst struct file_operations *fop)\r\n{\r\nstruct file *file;\r\nfile = get_empty_filp();\r\nif (IS_ERR(file))\r\nreturn file;\r\nfile->f_path = *path;\r\nfile->f_inode = path->dentry->d_inode;\r\nfile->f_mapping = path->dentry->d_inode->i_mapping;\r\nif ((mode & FMODE_READ) &&\r\nlikely(fop->read || fop->aio_read || fop->read_iter))\r\nmode |= FMODE_CAN_READ;\r\nif ((mode & FMODE_WRITE) &&\r\nlikely(fop->write || fop->aio_write || fop->write_iter))\r\nmode |= FMODE_CAN_WRITE;\r\nfile->f_mode = mode;\r\nfile->f_op = fop;\r\nif ((mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ)\r\ni_readcount_inc(path->dentry->d_inode);\r\nreturn file;\r\n}\r\nstatic void __fput(struct file *file)\r\n{\r\nstruct dentry *dentry = file->f_path.dentry;\r\nstruct vfsmount *mnt = file->f_path.mnt;\r\nstruct inode *inode = file->f_inode;\r\nmight_sleep();\r\nfsnotify_close(file);\r\neventpoll_release(file);\r\nlocks_remove_file(file);\r\nif (unlikely(file->f_flags & FASYNC)) {\r\nif (file->f_op->fasync)\r\nfile->f_op->fasync(-1, file, 0);\r\n}\r\nima_file_free(file);\r\nif (file->f_op->release)\r\nfile->f_op->release(inode, file);\r\nsecurity_file_free(file);\r\nif (unlikely(S_ISCHR(inode->i_mode) && inode->i_cdev != NULL &&\r\n!(file->f_mode & FMODE_PATH))) {\r\ncdev_put(inode->i_cdev);\r\n}\r\nfops_put(file->f_op);\r\nput_pid(file->f_owner.pid);\r\nif ((file->f_mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ)\r\ni_readcount_dec(inode);\r\nif (file->f_mode & FMODE_WRITER) {\r\nput_write_access(inode);\r\n__mnt_drop_write(mnt);\r\n}\r\nfile->f_path.dentry = NULL;\r\nfile->f_path.mnt = NULL;\r\nfile->f_inode = NULL;\r\nfile_free(file);\r\ndput(dentry);\r\nmntput(mnt);\r\n}\r\nstatic void delayed_fput(struct work_struct *unused)\r\n{\r\nstruct llist_node *node = llist_del_all(&delayed_fput_list);\r\nstruct llist_node *next;\r\nfor (; node; node = next) {\r\nnext = llist_next(node);\r\n__fput(llist_entry(node, struct file, f_u.fu_llist));\r\n}\r\n}\r\nstatic void ____fput(struct callback_head *work)\r\n{\r\n__fput(container_of(work, struct file, f_u.fu_rcuhead));\r\n}\r\nvoid flush_delayed_fput(void)\r\n{\r\ndelayed_fput(NULL);\r\n}\r\nvoid fput(struct file *file)\r\n{\r\nif (atomic_long_dec_and_test(&file->f_count)) {\r\nstruct task_struct *task = current;\r\nif (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {\r\ninit_task_work(&file->f_u.fu_rcuhead, ____fput);\r\nif (!task_work_add(task, &file->f_u.fu_rcuhead, true))\r\nreturn;\r\n}\r\nif (llist_add(&file->f_u.fu_llist, &delayed_fput_list))\r\nschedule_delayed_work(&delayed_fput_work, 1);\r\n}\r\n}\r\nvoid __fput_sync(struct file *file)\r\n{\r\nif (atomic_long_dec_and_test(&file->f_count)) {\r\nstruct task_struct *task = current;\r\nBUG_ON(!(task->flags & PF_KTHREAD));\r\n__fput(file);\r\n}\r\n}\r\nvoid put_filp(struct file *file)\r\n{\r\nif (atomic_long_dec_and_test(&file->f_count)) {\r\nsecurity_file_free(file);\r\nfile_free(file);\r\n}\r\n}\r\nvoid __init files_init(unsigned long mempages)\r\n{\r\nunsigned long n;\r\nfilp_cachep = kmem_cache_create("filp", sizeof(struct file), 0,\r\nSLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\r\nn = (mempages * (PAGE_SIZE / 1024)) / 10;\r\nfiles_stat.max_files = max_t(unsigned long, n, NR_FILE);\r\npercpu_counter_init(&nr_files, 0);\r\n}
