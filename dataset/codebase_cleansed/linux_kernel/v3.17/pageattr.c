void update_page_count(int level, unsigned long pages)\r\n{\r\nspin_lock(&pgd_lock);\r\ndirect_pages_count[level] += pages;\r\nspin_unlock(&pgd_lock);\r\n}\r\nstatic void split_page_count(int level)\r\n{\r\ndirect_pages_count[level]--;\r\ndirect_pages_count[level - 1] += PTRS_PER_PTE;\r\n}\r\nvoid arch_report_meminfo(struct seq_file *m)\r\n{\r\nseq_printf(m, "DirectMap4k: %8lu kB\n",\r\ndirect_pages_count[PG_LEVEL_4K] << 2);\r\n#if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)\r\nseq_printf(m, "DirectMap2M: %8lu kB\n",\r\ndirect_pages_count[PG_LEVEL_2M] << 11);\r\n#else\r\nseq_printf(m, "DirectMap4M: %8lu kB\n",\r\ndirect_pages_count[PG_LEVEL_2M] << 12);\r\n#endif\r\n#ifdef CONFIG_X86_64\r\nif (direct_gbpages)\r\nseq_printf(m, "DirectMap1G: %8lu kB\n",\r\ndirect_pages_count[PG_LEVEL_1G] << 20);\r\n#endif\r\n}\r\nstatic inline void split_page_count(int level) { }\r\nstatic inline unsigned long highmap_start_pfn(void)\r\n{\r\nreturn __pa_symbol(_text) >> PAGE_SHIFT;\r\n}\r\nstatic inline unsigned long highmap_end_pfn(void)\r\n{\r\nreturn __pa_symbol(roundup(_brk_end, PMD_SIZE)) >> PAGE_SHIFT;\r\n}\r\nstatic inline int\r\nwithin(unsigned long addr, unsigned long start, unsigned long end)\r\n{\r\nreturn addr >= start && addr < end;\r\n}\r\nvoid clflush_cache_range(void *vaddr, unsigned int size)\r\n{\r\nvoid *vend = vaddr + size - 1;\r\nmb();\r\nfor (; vaddr < vend; vaddr += boot_cpu_data.x86_clflush_size)\r\nclflushopt(vaddr);\r\nclflushopt(vend);\r\nmb();\r\n}\r\nstatic void __cpa_flush_all(void *arg)\r\n{\r\nunsigned long cache = (unsigned long)arg;\r\n__flush_tlb_all();\r\nif (cache && boot_cpu_data.x86 >= 4)\r\nwbinvd();\r\n}\r\nstatic void cpa_flush_all(unsigned long cache)\r\n{\r\nBUG_ON(irqs_disabled());\r\non_each_cpu(__cpa_flush_all, (void *) cache, 1);\r\n}\r\nstatic void __cpa_flush_range(void *arg)\r\n{\r\n__flush_tlb_all();\r\n}\r\nstatic void cpa_flush_range(unsigned long start, int numpages, int cache)\r\n{\r\nunsigned int i, level;\r\nunsigned long addr;\r\nBUG_ON(irqs_disabled());\r\nWARN_ON(PAGE_ALIGN(start) != start);\r\non_each_cpu(__cpa_flush_range, NULL, 1);\r\nif (!cache)\r\nreturn;\r\nfor (i = 0, addr = start; i < numpages; i++, addr += PAGE_SIZE) {\r\npte_t *pte = lookup_address(addr, &level);\r\nif (pte && (pte_val(*pte) & _PAGE_PRESENT))\r\nclflush_cache_range((void *) addr, PAGE_SIZE);\r\n}\r\n}\r\nstatic void cpa_flush_array(unsigned long *start, int numpages, int cache,\r\nint in_flags, struct page **pages)\r\n{\r\nunsigned int i, level;\r\nunsigned long do_wbinvd = cache && numpages >= 1024;\r\nBUG_ON(irqs_disabled());\r\non_each_cpu(__cpa_flush_all, (void *) do_wbinvd, 1);\r\nif (!cache || do_wbinvd)\r\nreturn;\r\nfor (i = 0; i < numpages; i++) {\r\nunsigned long addr;\r\npte_t *pte;\r\nif (in_flags & CPA_PAGES_ARRAY)\r\naddr = (unsigned long)page_address(pages[i]);\r\nelse\r\naddr = start[i];\r\npte = lookup_address(addr, &level);\r\nif (pte && (pte_val(*pte) & _PAGE_PRESENT))\r\nclflush_cache_range((void *)addr, PAGE_SIZE);\r\n}\r\n}\r\nstatic inline pgprot_t static_protections(pgprot_t prot, unsigned long address,\r\nunsigned long pfn)\r\n{\r\npgprot_t forbidden = __pgprot(0);\r\n#ifdef CONFIG_PCI_BIOS\r\nif (pcibios_enabled && within(pfn, BIOS_BEGIN >> PAGE_SHIFT, BIOS_END >> PAGE_SHIFT))\r\npgprot_val(forbidden) |= _PAGE_NX;\r\n#endif\r\nif (within(address, (unsigned long)_text, (unsigned long)_etext))\r\npgprot_val(forbidden) |= _PAGE_NX;\r\nif (within(pfn, __pa_symbol(__start_rodata) >> PAGE_SHIFT,\r\n__pa_symbol(__end_rodata) >> PAGE_SHIFT))\r\npgprot_val(forbidden) |= _PAGE_RW;\r\n#if defined(CONFIG_X86_64) && defined(CONFIG_DEBUG_RODATA)\r\nif (kernel_set_to_readonly &&\r\nwithin(address, (unsigned long)_text,\r\n(unsigned long)__end_rodata_hpage_align)) {\r\nunsigned int level;\r\nif (lookup_address(address, &level) && (level != PG_LEVEL_4K))\r\npgprot_val(forbidden) |= _PAGE_RW;\r\n}\r\n#endif\r\nprot = __pgprot(pgprot_val(prot) & ~pgprot_val(forbidden));\r\nreturn prot;\r\n}\r\npte_t *lookup_address_in_pgd(pgd_t *pgd, unsigned long address,\r\nunsigned int *level)\r\n{\r\npud_t *pud;\r\npmd_t *pmd;\r\n*level = PG_LEVEL_NONE;\r\nif (pgd_none(*pgd))\r\nreturn NULL;\r\npud = pud_offset(pgd, address);\r\nif (pud_none(*pud))\r\nreturn NULL;\r\n*level = PG_LEVEL_1G;\r\nif (pud_large(*pud) || !pud_present(*pud))\r\nreturn (pte_t *)pud;\r\npmd = pmd_offset(pud, address);\r\nif (pmd_none(*pmd))\r\nreturn NULL;\r\n*level = PG_LEVEL_2M;\r\nif (pmd_large(*pmd) || !pmd_present(*pmd))\r\nreturn (pte_t *)pmd;\r\n*level = PG_LEVEL_4K;\r\nreturn pte_offset_kernel(pmd, address);\r\n}\r\npte_t *lookup_address(unsigned long address, unsigned int *level)\r\n{\r\nreturn lookup_address_in_pgd(pgd_offset_k(address), address, level);\r\n}\r\nstatic pte_t *_lookup_address_cpa(struct cpa_data *cpa, unsigned long address,\r\nunsigned int *level)\r\n{\r\nif (cpa->pgd)\r\nreturn lookup_address_in_pgd(cpa->pgd + pgd_index(address),\r\naddress, level);\r\nreturn lookup_address(address, level);\r\n}\r\nphys_addr_t slow_virt_to_phys(void *__virt_addr)\r\n{\r\nunsigned long virt_addr = (unsigned long)__virt_addr;\r\nphys_addr_t phys_addr;\r\nunsigned long offset;\r\nenum pg_level level;\r\nunsigned long psize;\r\nunsigned long pmask;\r\npte_t *pte;\r\npte = lookup_address(virt_addr, &level);\r\nBUG_ON(!pte);\r\npsize = page_level_size(level);\r\npmask = page_level_mask(level);\r\noffset = virt_addr & ~pmask;\r\nphys_addr = pte_pfn(*pte) << PAGE_SHIFT;\r\nreturn (phys_addr | offset);\r\n}\r\nstatic void __set_pmd_pte(pte_t *kpte, unsigned long address, pte_t pte)\r\n{\r\nset_pte_atomic(kpte, pte);\r\n#ifdef CONFIG_X86_32\r\nif (!SHARED_KERNEL_PMD) {\r\nstruct page *page;\r\nlist_for_each_entry(page, &pgd_list, lru) {\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npgd = (pgd_t *)page_address(page) + pgd_index(address);\r\npud = pud_offset(pgd, address);\r\npmd = pmd_offset(pud, address);\r\nset_pte_atomic((pte_t *)pmd, pte);\r\n}\r\n}\r\n#endif\r\n}\r\nstatic int\r\ntry_preserve_large_page(pte_t *kpte, unsigned long address,\r\nstruct cpa_data *cpa)\r\n{\r\nunsigned long nextpage_addr, numpages, pmask, psize, addr, pfn;\r\npte_t new_pte, old_pte, *tmp;\r\npgprot_t old_prot, new_prot, req_prot;\r\nint i, do_split = 1;\r\nenum pg_level level;\r\nif (cpa->force_split)\r\nreturn 1;\r\nspin_lock(&pgd_lock);\r\ntmp = _lookup_address_cpa(cpa, address, &level);\r\nif (tmp != kpte)\r\ngoto out_unlock;\r\nswitch (level) {\r\ncase PG_LEVEL_2M:\r\n#ifdef CONFIG_X86_64\r\ncase PG_LEVEL_1G:\r\n#endif\r\npsize = page_level_size(level);\r\npmask = page_level_mask(level);\r\nbreak;\r\ndefault:\r\ndo_split = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nnextpage_addr = (address + psize) & pmask;\r\nnumpages = (nextpage_addr - address) >> PAGE_SHIFT;\r\nif (numpages < cpa->numpages)\r\ncpa->numpages = numpages;\r\nold_pte = *kpte;\r\nold_prot = req_prot = pte_pgprot(old_pte);\r\npgprot_val(req_prot) &= ~pgprot_val(cpa->mask_clr);\r\npgprot_val(req_prot) |= pgprot_val(cpa->mask_set);\r\nif (pgprot_val(req_prot) & _PAGE_PRESENT)\r\npgprot_val(req_prot) |= _PAGE_PSE | _PAGE_GLOBAL;\r\nelse\r\npgprot_val(req_prot) &= ~(_PAGE_PSE | _PAGE_GLOBAL);\r\nreq_prot = canon_pgprot(req_prot);\r\npfn = pte_pfn(old_pte) + ((address & (psize - 1)) >> PAGE_SHIFT);\r\ncpa->pfn = pfn;\r\nnew_prot = static_protections(req_prot, address, pfn);\r\naddr = address & pmask;\r\npfn = pte_pfn(old_pte);\r\nfor (i = 0; i < (psize >> PAGE_SHIFT); i++, addr += PAGE_SIZE, pfn++) {\r\npgprot_t chk_prot = static_protections(req_prot, addr, pfn);\r\nif (pgprot_val(chk_prot) != pgprot_val(new_prot))\r\ngoto out_unlock;\r\n}\r\nif (pgprot_val(new_prot) == pgprot_val(old_prot)) {\r\ndo_split = 0;\r\ngoto out_unlock;\r\n}\r\nif (address == (address & pmask) && cpa->numpages == (psize >> PAGE_SHIFT)) {\r\nnew_pte = pfn_pte(pte_pfn(old_pte), new_prot);\r\n__set_pmd_pte(kpte, address, new_pte);\r\ncpa->flags |= CPA_FLUSHTLB;\r\ndo_split = 0;\r\n}\r\nout_unlock:\r\nspin_unlock(&pgd_lock);\r\nreturn do_split;\r\n}\r\nstatic int\r\n__split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,\r\nstruct page *base)\r\n{\r\npte_t *pbase = (pte_t *)page_address(base);\r\nunsigned long pfn, pfninc = 1;\r\nunsigned int i, level;\r\npte_t *tmp;\r\npgprot_t ref_prot;\r\nspin_lock(&pgd_lock);\r\ntmp = _lookup_address_cpa(cpa, address, &level);\r\nif (tmp != kpte) {\r\nspin_unlock(&pgd_lock);\r\nreturn 1;\r\n}\r\nparavirt_alloc_pte(&init_mm, page_to_pfn(base));\r\nref_prot = pte_pgprot(pte_clrhuge(*kpte));\r\nWARN_ON_ONCE(pgprot_val(ref_prot) & _PAGE_PAT_LARGE);\r\n#ifdef CONFIG_X86_64\r\nif (level == PG_LEVEL_1G) {\r\npfninc = PMD_PAGE_SIZE >> PAGE_SHIFT;\r\nif (pgprot_val(ref_prot) & _PAGE_PRESENT)\r\npgprot_val(ref_prot) |= _PAGE_PSE;\r\nelse\r\npgprot_val(ref_prot) &= ~_PAGE_PSE;\r\n}\r\n#endif\r\nif (pgprot_val(ref_prot) & _PAGE_PRESENT)\r\npgprot_val(ref_prot) |= _PAGE_GLOBAL;\r\nelse\r\npgprot_val(ref_prot) &= ~_PAGE_GLOBAL;\r\npfn = pte_pfn(*kpte);\r\nfor (i = 0; i < PTRS_PER_PTE; i++, pfn += pfninc)\r\nset_pte(&pbase[i], pfn_pte(pfn, canon_pgprot(ref_prot)));\r\nif (pfn_range_is_mapped(PFN_DOWN(__pa(address)),\r\nPFN_DOWN(__pa(address)) + 1))\r\nsplit_page_count(level);\r\n__set_pmd_pte(kpte, address, mk_pte(base, __pgprot(_KERNPG_TABLE)));\r\n__flush_tlb_all();\r\nspin_unlock(&pgd_lock);\r\nreturn 0;\r\n}\r\nstatic int split_large_page(struct cpa_data *cpa, pte_t *kpte,\r\nunsigned long address)\r\n{\r\nstruct page *base;\r\nif (!debug_pagealloc)\r\nspin_unlock(&cpa_lock);\r\nbase = alloc_pages(GFP_KERNEL | __GFP_NOTRACK, 0);\r\nif (!debug_pagealloc)\r\nspin_lock(&cpa_lock);\r\nif (!base)\r\nreturn -ENOMEM;\r\nif (__split_large_page(cpa, kpte, address, base))\r\n__free_page(base);\r\nreturn 0;\r\n}\r\nstatic bool try_to_free_pte_page(pte_t *pte)\r\n{\r\nint i;\r\nfor (i = 0; i < PTRS_PER_PTE; i++)\r\nif (!pte_none(pte[i]))\r\nreturn false;\r\nfree_page((unsigned long)pte);\r\nreturn true;\r\n}\r\nstatic bool try_to_free_pmd_page(pmd_t *pmd)\r\n{\r\nint i;\r\nfor (i = 0; i < PTRS_PER_PMD; i++)\r\nif (!pmd_none(pmd[i]))\r\nreturn false;\r\nfree_page((unsigned long)pmd);\r\nreturn true;\r\n}\r\nstatic bool try_to_free_pud_page(pud_t *pud)\r\n{\r\nint i;\r\nfor (i = 0; i < PTRS_PER_PUD; i++)\r\nif (!pud_none(pud[i]))\r\nreturn false;\r\nfree_page((unsigned long)pud);\r\nreturn true;\r\n}\r\nstatic bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)\r\n{\r\npte_t *pte = pte_offset_kernel(pmd, start);\r\nwhile (start < end) {\r\nset_pte(pte, __pte(0));\r\nstart += PAGE_SIZE;\r\npte++;\r\n}\r\nif (try_to_free_pte_page((pte_t *)pmd_page_vaddr(*pmd))) {\r\npmd_clear(pmd);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void __unmap_pmd_range(pud_t *pud, pmd_t *pmd,\r\nunsigned long start, unsigned long end)\r\n{\r\nif (unmap_pte_range(pmd, start, end))\r\nif (try_to_free_pmd_page((pmd_t *)pud_page_vaddr(*pud)))\r\npud_clear(pud);\r\n}\r\nstatic void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)\r\n{\r\npmd_t *pmd = pmd_offset(pud, start);\r\nif (start & (PMD_SIZE - 1)) {\r\nunsigned long next_page = (start + PMD_SIZE) & PMD_MASK;\r\nunsigned long pre_end = min_t(unsigned long, end, next_page);\r\n__unmap_pmd_range(pud, pmd, start, pre_end);\r\nstart = pre_end;\r\npmd++;\r\n}\r\nwhile (end - start >= PMD_SIZE) {\r\nif (pmd_large(*pmd))\r\npmd_clear(pmd);\r\nelse\r\n__unmap_pmd_range(pud, pmd, start, start + PMD_SIZE);\r\nstart += PMD_SIZE;\r\npmd++;\r\n}\r\nif (start < end)\r\nreturn __unmap_pmd_range(pud, pmd, start, end);\r\nif (!pud_none(*pud))\r\nif (try_to_free_pmd_page((pmd_t *)pud_page_vaddr(*pud)))\r\npud_clear(pud);\r\n}\r\nstatic void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)\r\n{\r\npud_t *pud = pud_offset(pgd, start);\r\nif (start & (PUD_SIZE - 1)) {\r\nunsigned long next_page = (start + PUD_SIZE) & PUD_MASK;\r\nunsigned long pre_end = min_t(unsigned long, end, next_page);\r\nunmap_pmd_range(pud, start, pre_end);\r\nstart = pre_end;\r\npud++;\r\n}\r\nwhile (end - start >= PUD_SIZE) {\r\nif (pud_large(*pud))\r\npud_clear(pud);\r\nelse\r\nunmap_pmd_range(pud, start, start + PUD_SIZE);\r\nstart += PUD_SIZE;\r\npud++;\r\n}\r\nif (start < end)\r\nunmap_pmd_range(pud, start, end);\r\n}\r\nstatic void unmap_pgd_range(pgd_t *root, unsigned long addr, unsigned long end)\r\n{\r\npgd_t *pgd_entry = root + pgd_index(addr);\r\nunmap_pud_range(pgd_entry, addr, end);\r\nif (try_to_free_pud_page((pud_t *)pgd_page_vaddr(*pgd_entry)))\r\npgd_clear(pgd_entry);\r\n}\r\nstatic int alloc_pte_page(pmd_t *pmd)\r\n{\r\npte_t *pte = (pte_t *)get_zeroed_page(GFP_KERNEL | __GFP_NOTRACK);\r\nif (!pte)\r\nreturn -1;\r\nset_pmd(pmd, __pmd(__pa(pte) | _KERNPG_TABLE));\r\nreturn 0;\r\n}\r\nstatic int alloc_pmd_page(pud_t *pud)\r\n{\r\npmd_t *pmd = (pmd_t *)get_zeroed_page(GFP_KERNEL | __GFP_NOTRACK);\r\nif (!pmd)\r\nreturn -1;\r\nset_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE));\r\nreturn 0;\r\n}\r\nstatic void populate_pte(struct cpa_data *cpa,\r\nunsigned long start, unsigned long end,\r\nunsigned num_pages, pmd_t *pmd, pgprot_t pgprot)\r\n{\r\npte_t *pte;\r\npte = pte_offset_kernel(pmd, start);\r\nwhile (num_pages-- && start < end) {\r\nif (!(pgprot_val(pgprot) & _PAGE_NX))\r\ncpa->pfn &= ~_PAGE_NX;\r\nset_pte(pte, pfn_pte(cpa->pfn >> PAGE_SHIFT, pgprot));\r\nstart += PAGE_SIZE;\r\ncpa->pfn += PAGE_SIZE;\r\npte++;\r\n}\r\n}\r\nstatic int populate_pmd(struct cpa_data *cpa,\r\nunsigned long start, unsigned long end,\r\nunsigned num_pages, pud_t *pud, pgprot_t pgprot)\r\n{\r\nunsigned int cur_pages = 0;\r\npmd_t *pmd;\r\nif (start & (PMD_SIZE - 1)) {\r\nunsigned long pre_end = start + (num_pages << PAGE_SHIFT);\r\nunsigned long next_page = (start + PMD_SIZE) & PMD_MASK;\r\npre_end = min_t(unsigned long, pre_end, next_page);\r\ncur_pages = (pre_end - start) >> PAGE_SHIFT;\r\ncur_pages = min_t(unsigned int, num_pages, cur_pages);\r\npmd = pmd_offset(pud, start);\r\nif (pmd_none(*pmd))\r\nif (alloc_pte_page(pmd))\r\nreturn -1;\r\npopulate_pte(cpa, start, pre_end, cur_pages, pmd, pgprot);\r\nstart = pre_end;\r\n}\r\nif (num_pages == cur_pages)\r\nreturn cur_pages;\r\nwhile (end - start >= PMD_SIZE) {\r\nif (pud_none(*pud))\r\nif (alloc_pmd_page(pud))\r\nreturn -1;\r\npmd = pmd_offset(pud, start);\r\nset_pmd(pmd, __pmd(cpa->pfn | _PAGE_PSE | massage_pgprot(pgprot)));\r\nstart += PMD_SIZE;\r\ncpa->pfn += PMD_SIZE;\r\ncur_pages += PMD_SIZE >> PAGE_SHIFT;\r\n}\r\nif (start < end) {\r\npmd = pmd_offset(pud, start);\r\nif (pmd_none(*pmd))\r\nif (alloc_pte_page(pmd))\r\nreturn -1;\r\npopulate_pte(cpa, start, end, num_pages - cur_pages,\r\npmd, pgprot);\r\n}\r\nreturn num_pages;\r\n}\r\nstatic int populate_pud(struct cpa_data *cpa, unsigned long start, pgd_t *pgd,\r\npgprot_t pgprot)\r\n{\r\npud_t *pud;\r\nunsigned long end;\r\nint cur_pages = 0;\r\nend = start + (cpa->numpages << PAGE_SHIFT);\r\nif (start & (PUD_SIZE - 1)) {\r\nunsigned long pre_end;\r\nunsigned long next_page = (start + PUD_SIZE) & PUD_MASK;\r\npre_end = min_t(unsigned long, end, next_page);\r\ncur_pages = (pre_end - start) >> PAGE_SHIFT;\r\ncur_pages = min_t(int, (int)cpa->numpages, cur_pages);\r\npud = pud_offset(pgd, start);\r\nif (pud_none(*pud))\r\nif (alloc_pmd_page(pud))\r\nreturn -1;\r\ncur_pages = populate_pmd(cpa, start, pre_end, cur_pages,\r\npud, pgprot);\r\nif (cur_pages < 0)\r\nreturn cur_pages;\r\nstart = pre_end;\r\n}\r\nif (cpa->numpages == cur_pages)\r\nreturn cur_pages;\r\npud = pud_offset(pgd, start);\r\nwhile (end - start >= PUD_SIZE) {\r\nset_pud(pud, __pud(cpa->pfn | _PAGE_PSE | massage_pgprot(pgprot)));\r\nstart += PUD_SIZE;\r\ncpa->pfn += PUD_SIZE;\r\ncur_pages += PUD_SIZE >> PAGE_SHIFT;\r\npud++;\r\n}\r\nif (start < end) {\r\nint tmp;\r\npud = pud_offset(pgd, start);\r\nif (pud_none(*pud))\r\nif (alloc_pmd_page(pud))\r\nreturn -1;\r\ntmp = populate_pmd(cpa, start, end, cpa->numpages - cur_pages,\r\npud, pgprot);\r\nif (tmp < 0)\r\nreturn cur_pages;\r\ncur_pages += tmp;\r\n}\r\nreturn cur_pages;\r\n}\r\nstatic int populate_pgd(struct cpa_data *cpa, unsigned long addr)\r\n{\r\npgprot_t pgprot = __pgprot(_KERNPG_TABLE);\r\npud_t *pud = NULL;\r\npgd_t *pgd_entry;\r\nint ret;\r\npgd_entry = cpa->pgd + pgd_index(addr);\r\nif (pgd_none(*pgd_entry)) {\r\npud = (pud_t *)get_zeroed_page(GFP_KERNEL | __GFP_NOTRACK);\r\nif (!pud)\r\nreturn -1;\r\nset_pgd(pgd_entry, __pgd(__pa(pud) | _KERNPG_TABLE));\r\n}\r\npgprot_val(pgprot) &= ~pgprot_val(cpa->mask_clr);\r\npgprot_val(pgprot) |= pgprot_val(cpa->mask_set);\r\nret = populate_pud(cpa, addr, pgd_entry, pgprot);\r\nif (ret < 0) {\r\nunmap_pgd_range(cpa->pgd, addr,\r\naddr + (cpa->numpages << PAGE_SHIFT));\r\nreturn ret;\r\n}\r\ncpa->numpages = ret;\r\nreturn 0;\r\n}\r\nstatic int __cpa_process_fault(struct cpa_data *cpa, unsigned long vaddr,\r\nint primary)\r\n{\r\nif (cpa->pgd)\r\nreturn populate_pgd(cpa, vaddr);\r\nif (!primary)\r\nreturn 0;\r\nif (within(vaddr, PAGE_OFFSET,\r\nPAGE_OFFSET + (max_pfn_mapped << PAGE_SHIFT))) {\r\ncpa->numpages = 1;\r\ncpa->pfn = __pa(vaddr) >> PAGE_SHIFT;\r\nreturn 0;\r\n} else {\r\nWARN(1, KERN_WARNING "CPA: called for zero pte. "\r\n"vaddr = %lx cpa->vaddr = %lx\n", vaddr,\r\n*cpa->vaddr);\r\nreturn -EFAULT;\r\n}\r\n}\r\nstatic int __change_page_attr(struct cpa_data *cpa, int primary)\r\n{\r\nunsigned long address;\r\nint do_split, err;\r\nunsigned int level;\r\npte_t *kpte, old_pte;\r\nif (cpa->flags & CPA_PAGES_ARRAY) {\r\nstruct page *page = cpa->pages[cpa->curpage];\r\nif (unlikely(PageHighMem(page)))\r\nreturn 0;\r\naddress = (unsigned long)page_address(page);\r\n} else if (cpa->flags & CPA_ARRAY)\r\naddress = cpa->vaddr[cpa->curpage];\r\nelse\r\naddress = *cpa->vaddr;\r\nrepeat:\r\nkpte = _lookup_address_cpa(cpa, address, &level);\r\nif (!kpte)\r\nreturn __cpa_process_fault(cpa, address, primary);\r\nold_pte = *kpte;\r\nif (!pte_val(old_pte))\r\nreturn __cpa_process_fault(cpa, address, primary);\r\nif (level == PG_LEVEL_4K) {\r\npte_t new_pte;\r\npgprot_t new_prot = pte_pgprot(old_pte);\r\nunsigned long pfn = pte_pfn(old_pte);\r\npgprot_val(new_prot) &= ~pgprot_val(cpa->mask_clr);\r\npgprot_val(new_prot) |= pgprot_val(cpa->mask_set);\r\nnew_prot = static_protections(new_prot, address, pfn);\r\nif (pgprot_val(new_prot) & _PAGE_PRESENT)\r\npgprot_val(new_prot) |= _PAGE_GLOBAL;\r\nelse\r\npgprot_val(new_prot) &= ~_PAGE_GLOBAL;\r\nnew_pte = pfn_pte(pfn, canon_pgprot(new_prot));\r\ncpa->pfn = pfn;\r\nif (pte_val(old_pte) != pte_val(new_pte)) {\r\nset_pte_atomic(kpte, new_pte);\r\ncpa->flags |= CPA_FLUSHTLB;\r\n}\r\ncpa->numpages = 1;\r\nreturn 0;\r\n}\r\ndo_split = try_preserve_large_page(kpte, address, cpa);\r\nif (do_split <= 0)\r\nreturn do_split;\r\nerr = split_large_page(cpa, kpte, address);\r\nif (!err) {\r\nflush_tlb_all();\r\ngoto repeat;\r\n}\r\nreturn err;\r\n}\r\nstatic int cpa_process_alias(struct cpa_data *cpa)\r\n{\r\nstruct cpa_data alias_cpa;\r\nunsigned long laddr = (unsigned long)__va(cpa->pfn << PAGE_SHIFT);\r\nunsigned long vaddr;\r\nint ret;\r\nif (!pfn_range_is_mapped(cpa->pfn, cpa->pfn + 1))\r\nreturn 0;\r\nif (cpa->flags & CPA_PAGES_ARRAY) {\r\nstruct page *page = cpa->pages[cpa->curpage];\r\nif (unlikely(PageHighMem(page)))\r\nreturn 0;\r\nvaddr = (unsigned long)page_address(page);\r\n} else if (cpa->flags & CPA_ARRAY)\r\nvaddr = cpa->vaddr[cpa->curpage];\r\nelse\r\nvaddr = *cpa->vaddr;\r\nif (!(within(vaddr, PAGE_OFFSET,\r\nPAGE_OFFSET + (max_pfn_mapped << PAGE_SHIFT)))) {\r\nalias_cpa = *cpa;\r\nalias_cpa.vaddr = &laddr;\r\nalias_cpa.flags &= ~(CPA_PAGES_ARRAY | CPA_ARRAY);\r\nret = __change_page_attr_set_clr(&alias_cpa, 0);\r\nif (ret)\r\nreturn ret;\r\n}\r\n#ifdef CONFIG_X86_64\r\nif (!within(vaddr, (unsigned long)_text, _brk_end) &&\r\nwithin(cpa->pfn, highmap_start_pfn(), highmap_end_pfn())) {\r\nunsigned long temp_cpa_vaddr = (cpa->pfn << PAGE_SHIFT) +\r\n__START_KERNEL_map - phys_base;\r\nalias_cpa = *cpa;\r\nalias_cpa.vaddr = &temp_cpa_vaddr;\r\nalias_cpa.flags &= ~(CPA_PAGES_ARRAY | CPA_ARRAY);\r\n__change_page_attr_set_clr(&alias_cpa, 0);\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int __change_page_attr_set_clr(struct cpa_data *cpa, int checkalias)\r\n{\r\nint ret, numpages = cpa->numpages;\r\nwhile (numpages) {\r\ncpa->numpages = numpages;\r\nif (cpa->flags & (CPA_ARRAY | CPA_PAGES_ARRAY))\r\ncpa->numpages = 1;\r\nif (!debug_pagealloc)\r\nspin_lock(&cpa_lock);\r\nret = __change_page_attr(cpa, checkalias);\r\nif (!debug_pagealloc)\r\nspin_unlock(&cpa_lock);\r\nif (ret)\r\nreturn ret;\r\nif (checkalias) {\r\nret = cpa_process_alias(cpa);\r\nif (ret)\r\nreturn ret;\r\n}\r\nBUG_ON(cpa->numpages > numpages);\r\nnumpages -= cpa->numpages;\r\nif (cpa->flags & (CPA_PAGES_ARRAY | CPA_ARRAY))\r\ncpa->curpage++;\r\nelse\r\n*cpa->vaddr += cpa->numpages * PAGE_SIZE;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int cache_attr(pgprot_t attr)\r\n{\r\nreturn pgprot_val(attr) &\r\n(_PAGE_PAT | _PAGE_PAT_LARGE | _PAGE_PWT | _PAGE_PCD);\r\n}\r\nstatic int change_page_attr_set_clr(unsigned long *addr, int numpages,\r\npgprot_t mask_set, pgprot_t mask_clr,\r\nint force_split, int in_flag,\r\nstruct page **pages)\r\n{\r\nstruct cpa_data cpa;\r\nint ret, cache, checkalias;\r\nunsigned long baddr = 0;\r\nmemset(&cpa, 0, sizeof(cpa));\r\nmask_set = canon_pgprot(mask_set);\r\nmask_clr = canon_pgprot(mask_clr);\r\nif (!pgprot_val(mask_set) && !pgprot_val(mask_clr) && !force_split)\r\nreturn 0;\r\nif (in_flag & CPA_ARRAY) {\r\nint i;\r\nfor (i = 0; i < numpages; i++) {\r\nif (addr[i] & ~PAGE_MASK) {\r\naddr[i] &= PAGE_MASK;\r\nWARN_ON_ONCE(1);\r\n}\r\n}\r\n} else if (!(in_flag & CPA_PAGES_ARRAY)) {\r\nif (*addr & ~PAGE_MASK) {\r\n*addr &= PAGE_MASK;\r\nWARN_ON_ONCE(1);\r\n}\r\nbaddr = *addr;\r\n}\r\nkmap_flush_unused();\r\nvm_unmap_aliases();\r\ncpa.vaddr = addr;\r\ncpa.pages = pages;\r\ncpa.numpages = numpages;\r\ncpa.mask_set = mask_set;\r\ncpa.mask_clr = mask_clr;\r\ncpa.flags = 0;\r\ncpa.curpage = 0;\r\ncpa.force_split = force_split;\r\nif (in_flag & (CPA_ARRAY | CPA_PAGES_ARRAY))\r\ncpa.flags |= in_flag;\r\ncheckalias = (pgprot_val(mask_set) | pgprot_val(mask_clr)) != _PAGE_NX;\r\nret = __change_page_attr_set_clr(&cpa, checkalias);\r\nif (!(cpa.flags & CPA_FLUSHTLB))\r\ngoto out;\r\ncache = cache_attr(mask_set);\r\nif (!ret && cpu_has_clflush) {\r\nif (cpa.flags & (CPA_PAGES_ARRAY | CPA_ARRAY)) {\r\ncpa_flush_array(addr, numpages, cache,\r\ncpa.flags, pages);\r\n} else\r\ncpa_flush_range(baddr, numpages, cache);\r\n} else\r\ncpa_flush_all(cache);\r\nout:\r\nreturn ret;\r\n}\r\nstatic inline int change_page_attr_set(unsigned long *addr, int numpages,\r\npgprot_t mask, int array)\r\n{\r\nreturn change_page_attr_set_clr(addr, numpages, mask, __pgprot(0), 0,\r\n(array ? CPA_ARRAY : 0), NULL);\r\n}\r\nstatic inline int change_page_attr_clear(unsigned long *addr, int numpages,\r\npgprot_t mask, int array)\r\n{\r\nreturn change_page_attr_set_clr(addr, numpages, __pgprot(0), mask, 0,\r\n(array ? CPA_ARRAY : 0), NULL);\r\n}\r\nstatic inline int cpa_set_pages_array(struct page **pages, int numpages,\r\npgprot_t mask)\r\n{\r\nreturn change_page_attr_set_clr(NULL, numpages, mask, __pgprot(0), 0,\r\nCPA_PAGES_ARRAY, pages);\r\n}\r\nstatic inline int cpa_clear_pages_array(struct page **pages, int numpages,\r\npgprot_t mask)\r\n{\r\nreturn change_page_attr_set_clr(NULL, numpages, __pgprot(0), mask, 0,\r\nCPA_PAGES_ARRAY, pages);\r\n}\r\nint _set_memory_uc(unsigned long addr, int numpages)\r\n{\r\nreturn change_page_attr_set(&addr, numpages,\r\n__pgprot(_PAGE_CACHE_UC_MINUS), 0);\r\n}\r\nint set_memory_uc(unsigned long addr, int numpages)\r\n{\r\nint ret;\r\nret = reserve_memtype(__pa(addr), __pa(addr) + numpages * PAGE_SIZE,\r\n_PAGE_CACHE_UC_MINUS, NULL);\r\nif (ret)\r\ngoto out_err;\r\nret = _set_memory_uc(addr, numpages);\r\nif (ret)\r\ngoto out_free;\r\nreturn 0;\r\nout_free:\r\nfree_memtype(__pa(addr), __pa(addr) + numpages * PAGE_SIZE);\r\nout_err:\r\nreturn ret;\r\n}\r\nstatic int _set_memory_array(unsigned long *addr, int addrinarray,\r\nunsigned long new_type)\r\n{\r\nint i, j;\r\nint ret;\r\nfor (i = 0; i < addrinarray; i++) {\r\nret = reserve_memtype(__pa(addr[i]), __pa(addr[i]) + PAGE_SIZE,\r\nnew_type, NULL);\r\nif (ret)\r\ngoto out_free;\r\n}\r\nret = change_page_attr_set(addr, addrinarray,\r\n__pgprot(_PAGE_CACHE_UC_MINUS), 1);\r\nif (!ret && new_type == _PAGE_CACHE_WC)\r\nret = change_page_attr_set_clr(addr, addrinarray,\r\n__pgprot(_PAGE_CACHE_WC),\r\n__pgprot(_PAGE_CACHE_MASK),\r\n0, CPA_ARRAY, NULL);\r\nif (ret)\r\ngoto out_free;\r\nreturn 0;\r\nout_free:\r\nfor (j = 0; j < i; j++)\r\nfree_memtype(__pa(addr[j]), __pa(addr[j]) + PAGE_SIZE);\r\nreturn ret;\r\n}\r\nint set_memory_array_uc(unsigned long *addr, int addrinarray)\r\n{\r\nreturn _set_memory_array(addr, addrinarray, _PAGE_CACHE_UC_MINUS);\r\n}\r\nint set_memory_array_wc(unsigned long *addr, int addrinarray)\r\n{\r\nreturn _set_memory_array(addr, addrinarray, _PAGE_CACHE_WC);\r\n}\r\nint _set_memory_wc(unsigned long addr, int numpages)\r\n{\r\nint ret;\r\nunsigned long addr_copy = addr;\r\nret = change_page_attr_set(&addr, numpages,\r\n__pgprot(_PAGE_CACHE_UC_MINUS), 0);\r\nif (!ret) {\r\nret = change_page_attr_set_clr(&addr_copy, numpages,\r\n__pgprot(_PAGE_CACHE_WC),\r\n__pgprot(_PAGE_CACHE_MASK),\r\n0, 0, NULL);\r\n}\r\nreturn ret;\r\n}\r\nint set_memory_wc(unsigned long addr, int numpages)\r\n{\r\nint ret;\r\nif (!pat_enabled)\r\nreturn set_memory_uc(addr, numpages);\r\nret = reserve_memtype(__pa(addr), __pa(addr) + numpages * PAGE_SIZE,\r\n_PAGE_CACHE_WC, NULL);\r\nif (ret)\r\ngoto out_err;\r\nret = _set_memory_wc(addr, numpages);\r\nif (ret)\r\ngoto out_free;\r\nreturn 0;\r\nout_free:\r\nfree_memtype(__pa(addr), __pa(addr) + numpages * PAGE_SIZE);\r\nout_err:\r\nreturn ret;\r\n}\r\nint _set_memory_wb(unsigned long addr, int numpages)\r\n{\r\nreturn change_page_attr_clear(&addr, numpages,\r\n__pgprot(_PAGE_CACHE_MASK), 0);\r\n}\r\nint set_memory_wb(unsigned long addr, int numpages)\r\n{\r\nint ret;\r\nret = _set_memory_wb(addr, numpages);\r\nif (ret)\r\nreturn ret;\r\nfree_memtype(__pa(addr), __pa(addr) + numpages * PAGE_SIZE);\r\nreturn 0;\r\n}\r\nint set_memory_array_wb(unsigned long *addr, int addrinarray)\r\n{\r\nint i;\r\nint ret;\r\nret = change_page_attr_clear(addr, addrinarray,\r\n__pgprot(_PAGE_CACHE_MASK), 1);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < addrinarray; i++)\r\nfree_memtype(__pa(addr[i]), __pa(addr[i]) + PAGE_SIZE);\r\nreturn 0;\r\n}\r\nint set_memory_x(unsigned long addr, int numpages)\r\n{\r\nif (!(__supported_pte_mask & _PAGE_NX))\r\nreturn 0;\r\nreturn change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_NX), 0);\r\n}\r\nint set_memory_nx(unsigned long addr, int numpages)\r\n{\r\nif (!(__supported_pte_mask & _PAGE_NX))\r\nreturn 0;\r\nreturn change_page_attr_set(&addr, numpages, __pgprot(_PAGE_NX), 0);\r\n}\r\nint set_memory_ro(unsigned long addr, int numpages)\r\n{\r\nreturn change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_RW), 0);\r\n}\r\nint set_memory_rw(unsigned long addr, int numpages)\r\n{\r\nreturn change_page_attr_set(&addr, numpages, __pgprot(_PAGE_RW), 0);\r\n}\r\nint set_memory_np(unsigned long addr, int numpages)\r\n{\r\nreturn change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_PRESENT), 0);\r\n}\r\nint set_memory_4k(unsigned long addr, int numpages)\r\n{\r\nreturn change_page_attr_set_clr(&addr, numpages, __pgprot(0),\r\n__pgprot(0), 1, 0, NULL);\r\n}\r\nint set_pages_uc(struct page *page, int numpages)\r\n{\r\nunsigned long addr = (unsigned long)page_address(page);\r\nreturn set_memory_uc(addr, numpages);\r\n}\r\nstatic int _set_pages_array(struct page **pages, int addrinarray,\r\nunsigned long new_type)\r\n{\r\nunsigned long start;\r\nunsigned long end;\r\nint i;\r\nint free_idx;\r\nint ret;\r\nfor (i = 0; i < addrinarray; i++) {\r\nif (PageHighMem(pages[i]))\r\ncontinue;\r\nstart = page_to_pfn(pages[i]) << PAGE_SHIFT;\r\nend = start + PAGE_SIZE;\r\nif (reserve_memtype(start, end, new_type, NULL))\r\ngoto err_out;\r\n}\r\nret = cpa_set_pages_array(pages, addrinarray,\r\n__pgprot(_PAGE_CACHE_UC_MINUS));\r\nif (!ret && new_type == _PAGE_CACHE_WC)\r\nret = change_page_attr_set_clr(NULL, addrinarray,\r\n__pgprot(_PAGE_CACHE_WC),\r\n__pgprot(_PAGE_CACHE_MASK),\r\n0, CPA_PAGES_ARRAY, pages);\r\nif (ret)\r\ngoto err_out;\r\nreturn 0;\r\nerr_out:\r\nfree_idx = i;\r\nfor (i = 0; i < free_idx; i++) {\r\nif (PageHighMem(pages[i]))\r\ncontinue;\r\nstart = page_to_pfn(pages[i]) << PAGE_SHIFT;\r\nend = start + PAGE_SIZE;\r\nfree_memtype(start, end);\r\n}\r\nreturn -EINVAL;\r\n}\r\nint set_pages_array_uc(struct page **pages, int addrinarray)\r\n{\r\nreturn _set_pages_array(pages, addrinarray, _PAGE_CACHE_UC_MINUS);\r\n}\r\nint set_pages_array_wc(struct page **pages, int addrinarray)\r\n{\r\nreturn _set_pages_array(pages, addrinarray, _PAGE_CACHE_WC);\r\n}\r\nint set_pages_wb(struct page *page, int numpages)\r\n{\r\nunsigned long addr = (unsigned long)page_address(page);\r\nreturn set_memory_wb(addr, numpages);\r\n}\r\nint set_pages_array_wb(struct page **pages, int addrinarray)\r\n{\r\nint retval;\r\nunsigned long start;\r\nunsigned long end;\r\nint i;\r\nretval = cpa_clear_pages_array(pages, addrinarray,\r\n__pgprot(_PAGE_CACHE_MASK));\r\nif (retval)\r\nreturn retval;\r\nfor (i = 0; i < addrinarray; i++) {\r\nif (PageHighMem(pages[i]))\r\ncontinue;\r\nstart = page_to_pfn(pages[i]) << PAGE_SHIFT;\r\nend = start + PAGE_SIZE;\r\nfree_memtype(start, end);\r\n}\r\nreturn 0;\r\n}\r\nint set_pages_x(struct page *page, int numpages)\r\n{\r\nunsigned long addr = (unsigned long)page_address(page);\r\nreturn set_memory_x(addr, numpages);\r\n}\r\nint set_pages_nx(struct page *page, int numpages)\r\n{\r\nunsigned long addr = (unsigned long)page_address(page);\r\nreturn set_memory_nx(addr, numpages);\r\n}\r\nint set_pages_ro(struct page *page, int numpages)\r\n{\r\nunsigned long addr = (unsigned long)page_address(page);\r\nreturn set_memory_ro(addr, numpages);\r\n}\r\nint set_pages_rw(struct page *page, int numpages)\r\n{\r\nunsigned long addr = (unsigned long)page_address(page);\r\nreturn set_memory_rw(addr, numpages);\r\n}\r\nstatic int __set_pages_p(struct page *page, int numpages)\r\n{\r\nunsigned long tempaddr = (unsigned long) page_address(page);\r\nstruct cpa_data cpa = { .vaddr = &tempaddr,\r\n.pgd = NULL,\r\n.numpages = numpages,\r\n.mask_set = __pgprot(_PAGE_PRESENT | _PAGE_RW),\r\n.mask_clr = __pgprot(0),\r\n.flags = 0};\r\nreturn __change_page_attr_set_clr(&cpa, 0);\r\n}\r\nstatic int __set_pages_np(struct page *page, int numpages)\r\n{\r\nunsigned long tempaddr = (unsigned long) page_address(page);\r\nstruct cpa_data cpa = { .vaddr = &tempaddr,\r\n.pgd = NULL,\r\n.numpages = numpages,\r\n.mask_set = __pgprot(0),\r\n.mask_clr = __pgprot(_PAGE_PRESENT | _PAGE_RW),\r\n.flags = 0};\r\nreturn __change_page_attr_set_clr(&cpa, 0);\r\n}\r\nvoid kernel_map_pages(struct page *page, int numpages, int enable)\r\n{\r\nif (PageHighMem(page))\r\nreturn;\r\nif (!enable) {\r\ndebug_check_no_locks_freed(page_address(page),\r\nnumpages * PAGE_SIZE);\r\n}\r\nif (enable)\r\n__set_pages_p(page, numpages);\r\nelse\r\n__set_pages_np(page, numpages);\r\n__flush_tlb_all();\r\narch_flush_lazy_mmu_mode();\r\n}\r\nbool kernel_page_present(struct page *page)\r\n{\r\nunsigned int level;\r\npte_t *pte;\r\nif (PageHighMem(page))\r\nreturn false;\r\npte = lookup_address((unsigned long)page_address(page), &level);\r\nreturn (pte_val(*pte) & _PAGE_PRESENT);\r\n}\r\nint kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,\r\nunsigned numpages, unsigned long page_flags)\r\n{\r\nint retval = -EINVAL;\r\nstruct cpa_data cpa = {\r\n.vaddr = &address,\r\n.pfn = pfn,\r\n.pgd = pgd,\r\n.numpages = numpages,\r\n.mask_set = __pgprot(0),\r\n.mask_clr = __pgprot(0),\r\n.flags = 0,\r\n};\r\nif (!(__supported_pte_mask & _PAGE_NX))\r\ngoto out;\r\nif (!(page_flags & _PAGE_NX))\r\ncpa.mask_clr = __pgprot(_PAGE_NX);\r\ncpa.mask_set = __pgprot(_PAGE_PRESENT | page_flags);\r\nretval = __change_page_attr_set_clr(&cpa, 0);\r\n__flush_tlb_all();\r\nout:\r\nreturn retval;\r\n}\r\nvoid kernel_unmap_pages_in_pgd(pgd_t *root, unsigned long address,\r\nunsigned numpages)\r\n{\r\nunmap_pgd_range(root, address, address + (numpages << PAGE_SHIFT));\r\n}
