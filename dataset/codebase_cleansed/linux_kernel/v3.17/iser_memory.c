static int iser_start_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nstruct iser_data_buf *data_copy,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_device *dev = iser_task->ib_conn->device->ib_device;\r\nstruct scatterlist *sgl = (struct scatterlist *)data->buf;\r\nstruct scatterlist *sg;\r\nchar *mem = NULL;\r\nunsigned long cmd_data_len = 0;\r\nint dma_nents, i;\r\nfor_each_sg(sgl, sg, data->size, i)\r\ncmd_data_len += ib_sg_dma_len(dev, sg);\r\nif (cmd_data_len > ISER_KMALLOC_THRESHOLD)\r\nmem = (void *)__get_free_pages(GFP_ATOMIC,\r\nilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);\r\nelse\r\nmem = kmalloc(cmd_data_len, GFP_ATOMIC);\r\nif (mem == NULL) {\r\niser_err("Failed to allocate mem size %d %d for copying sglist\n",\r\ndata->size, (int)cmd_data_len);\r\nreturn -ENOMEM;\r\n}\r\nif (cmd_dir == ISER_DIR_OUT) {\r\nint i;\r\nchar *p, *from;\r\nsgl = (struct scatterlist *)data->buf;\r\np = mem;\r\nfor_each_sg(sgl, sg, data->size, i) {\r\nfrom = kmap_atomic(sg_page(sg));\r\nmemcpy(p,\r\nfrom + sg->offset,\r\nsg->length);\r\nkunmap_atomic(from);\r\np += sg->length;\r\n}\r\n}\r\nsg_init_one(&data_copy->sg_single, mem, cmd_data_len);\r\ndata_copy->buf = &data_copy->sg_single;\r\ndata_copy->size = 1;\r\ndata_copy->copy_buf = mem;\r\ndma_nents = ib_dma_map_sg(dev, &data_copy->sg_single, 1,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nBUG_ON(dma_nents == 0);\r\ndata_copy->dma_nents = dma_nents;\r\ndata_copy->data_len = cmd_data_len;\r\nreturn 0;\r\n}\r\nvoid iser_finalize_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nstruct iser_data_buf *data_copy,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_device *dev;\r\nunsigned long cmd_data_len;\r\ndev = iser_task->ib_conn->device->ib_device;\r\nib_dma_unmap_sg(dev, &data_copy->sg_single, 1,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nif (cmd_dir == ISER_DIR_IN) {\r\nchar *mem;\r\nstruct scatterlist *sgl, *sg;\r\nunsigned char *p, *to;\r\nunsigned int sg_size;\r\nint i;\r\nmem = data_copy->copy_buf;\r\nsgl = (struct scatterlist *)data->buf;\r\nsg_size = data->size;\r\np = mem;\r\nfor_each_sg(sgl, sg, sg_size, i) {\r\nto = kmap_atomic(sg_page(sg));\r\nmemcpy(to + sg->offset,\r\np,\r\nsg->length);\r\nkunmap_atomic(to);\r\np += sg->length;\r\n}\r\n}\r\ncmd_data_len = data->data_len;\r\nif (cmd_data_len > ISER_KMALLOC_THRESHOLD)\r\nfree_pages((unsigned long)data_copy->copy_buf,\r\nilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);\r\nelse\r\nkfree(data_copy->copy_buf);\r\ndata_copy->copy_buf = NULL;\r\n}\r\nstatic int iser_sg_to_page_vec(struct iser_data_buf *data,\r\nstruct ib_device *ibdev, u64 *pages,\r\nint *offset, int *data_size)\r\n{\r\nstruct scatterlist *sg, *sgl = (struct scatterlist *)data->buf;\r\nu64 start_addr, end_addr, page, chunk_start = 0;\r\nunsigned long total_sz = 0;\r\nunsigned int dma_len;\r\nint i, new_chunk, cur_page, last_ent = data->dma_nents - 1;\r\n*offset = (u64) sgl[0].offset & ~MASK_4K;\r\nnew_chunk = 1;\r\ncur_page = 0;\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nstart_addr = ib_sg_dma_address(ibdev, sg);\r\nif (new_chunk)\r\nchunk_start = start_addr;\r\ndma_len = ib_sg_dma_len(ibdev, sg);\r\nend_addr = start_addr + dma_len;\r\ntotal_sz += dma_len;\r\nif (!IS_4K_ALIGNED(end_addr) && i < last_ent) {\r\nnew_chunk = 0;\r\ncontinue;\r\n}\r\nnew_chunk = 1;\r\npage = chunk_start & MASK_4K;\r\ndo {\r\npages[cur_page++] = page;\r\npage += SIZE_4K;\r\n} while (page < end_addr);\r\n}\r\n*data_size = total_sz;\r\niser_dbg("page_vec->data_size:%d cur_page %d\n",\r\n*data_size, cur_page);\r\nreturn cur_page;\r\n}\r\nstatic int iser_data_buf_aligned_len(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sgl, *sg, *next_sg = NULL;\r\nu64 start_addr, end_addr;\r\nint i, ret_len, start_check = 0;\r\nif (data->dma_nents == 1)\r\nreturn 1;\r\nsgl = (struct scatterlist *)data->buf;\r\nstart_addr = ib_sg_dma_address(ibdev, sgl);\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nif (start_check && !IS_4K_ALIGNED(start_addr))\r\nbreak;\r\nnext_sg = sg_next(sg);\r\nif (!next_sg)\r\nbreak;\r\nend_addr = start_addr + ib_sg_dma_len(ibdev, sg);\r\nstart_addr = ib_sg_dma_address(ibdev, next_sg);\r\nif (end_addr == start_addr) {\r\nstart_check = 0;\r\ncontinue;\r\n} else\r\nstart_check = 1;\r\nif (!IS_4K_ALIGNED(end_addr))\r\nbreak;\r\n}\r\nret_len = (next_sg) ? i : i+1;\r\niser_dbg("Found %d aligned entries out of %d in sg:0x%p\n",\r\nret_len, data->dma_nents, data);\r\nreturn ret_len;\r\n}\r\nstatic void iser_data_buf_dump(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sgl = (struct scatterlist *)data->buf;\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, data->dma_nents, i)\r\niser_dbg("sg[%d] dma_addr:0x%lX page:0x%p "\r\n"off:0x%x sz:0x%x dma_len:0x%x\n",\r\ni, (unsigned long)ib_sg_dma_address(ibdev, sg),\r\nsg_page(sg), sg->offset,\r\nsg->length, ib_sg_dma_len(ibdev, sg));\r\n}\r\nstatic void iser_dump_page_vec(struct iser_page_vec *page_vec)\r\n{\r\nint i;\r\niser_err("page vec length %d data size %d\n",\r\npage_vec->length, page_vec->data_size);\r\nfor (i = 0; i < page_vec->length; i++)\r\niser_err("%d %lx\n",i,(unsigned long)page_vec->pages[i]);\r\n}\r\nstatic void iser_page_vec_build(struct iser_data_buf *data,\r\nstruct iser_page_vec *page_vec,\r\nstruct ib_device *ibdev)\r\n{\r\nint page_vec_len = 0;\r\npage_vec->length = 0;\r\npage_vec->offset = 0;\r\niser_dbg("Translating sg sz: %d\n", data->dma_nents);\r\npage_vec_len = iser_sg_to_page_vec(data, ibdev, page_vec->pages,\r\n&page_vec->offset,\r\n&page_vec->data_size);\r\niser_dbg("sg len %d page_vec_len %d\n", data->dma_nents, page_vec_len);\r\npage_vec->length = page_vec_len;\r\nif (page_vec_len * SIZE_4K < page_vec->data_size) {\r\niser_err("page_vec too short to hold this SG\n");\r\niser_data_buf_dump(data, ibdev);\r\niser_dump_page_vec(page_vec);\r\nBUG();\r\n}\r\n}\r\nint iser_dma_map_task_data(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nenum iser_data_dir iser_dir,\r\nenum dma_data_direction dma_dir)\r\n{\r\nstruct ib_device *dev;\r\niser_task->dir[iser_dir] = 1;\r\ndev = iser_task->ib_conn->device->ib_device;\r\ndata->dma_nents = ib_dma_map_sg(dev, data->buf, data->size, dma_dir);\r\nif (data->dma_nents == 0) {\r\niser_err("dma_map_sg failed!!!\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid iser_dma_unmap_task_data(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data)\r\n{\r\nstruct ib_device *dev;\r\ndev = iser_task->ib_conn->device->ib_device;\r\nib_dma_unmap_sg(dev, data->buf, data->size, DMA_FROM_DEVICE);\r\n}\r\nstatic int fall_to_bounce_buf(struct iscsi_iser_task *iser_task,\r\nstruct ib_device *ibdev,\r\nstruct iser_data_buf *mem,\r\nstruct iser_data_buf *mem_copy,\r\nenum iser_data_dir cmd_dir,\r\nint aligned_len)\r\n{\r\nstruct iscsi_conn *iscsi_conn = iser_task->ib_conn->iscsi_conn;\r\niscsi_conn->fmr_unalign_cnt++;\r\niser_warn("rdma alignment violation (%d/%d aligned) or FMR not supported\n",\r\naligned_len, mem->size);\r\nif (iser_debug_level > 0)\r\niser_data_buf_dump(mem, ibdev);\r\niser_dma_unmap_task_data(iser_task, mem);\r\nif (iser_start_rdma_unaligned_sg(iser_task, mem, mem_copy, cmd_dir) != 0)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint iser_reg_rdma_mem_fmr(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct iser_conn *ib_conn = iser_task->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\nstruct iser_regd_buf *regd_buf;\r\nint aligned_len;\r\nint err;\r\nint i;\r\nstruct scatterlist *sg;\r\nregd_buf = &iser_task->rdma_regd[cmd_dir];\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, ibdev, mem,\r\n&iser_task->data_copy[cmd_dir],\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\nmem = &iser_task->data_copy[cmd_dir];\r\n}\r\nif (mem->dma_nents == 1) {\r\nsg = (struct scatterlist *)mem->buf;\r\nregd_buf->reg.lkey = device->mr->lkey;\r\nregd_buf->reg.rkey = device->mr->rkey;\r\nregd_buf->reg.len = ib_sg_dma_len(ibdev, &sg[0]);\r\nregd_buf->reg.va = ib_sg_dma_address(ibdev, &sg[0]);\r\nregd_buf->reg.is_mr = 0;\r\niser_dbg("PHYSICAL Mem.register: lkey: 0x%08X rkey: 0x%08X "\r\n"va: 0x%08lX sz: %ld]\n",\r\n(unsigned int)regd_buf->reg.lkey,\r\n(unsigned int)regd_buf->reg.rkey,\r\n(unsigned long)regd_buf->reg.va,\r\n(unsigned long)regd_buf->reg.len);\r\n} else {\r\niser_page_vec_build(mem, ib_conn->fmr.page_vec, ibdev);\r\nerr = iser_reg_page_vec(ib_conn, ib_conn->fmr.page_vec,\r\n&regd_buf->reg);\r\nif (err && err != -EAGAIN) {\r\niser_data_buf_dump(mem, ibdev);\r\niser_err("mem->dma_nents = %d (dlength = 0x%x)\n",\r\nmem->dma_nents,\r\nntoh24(iser_task->desc.iscsi_header.dlength));\r\niser_err("page_vec: data_size = 0x%x, length = %d, offset = 0x%x\n",\r\nib_conn->fmr.page_vec->data_size,\r\nib_conn->fmr.page_vec->length,\r\nib_conn->fmr.page_vec->offset);\r\nfor (i = 0; i < ib_conn->fmr.page_vec->length; i++)\r\niser_err("page_vec[%d] = 0x%llx\n", i,\r\n(unsigned long long) ib_conn->fmr.page_vec->pages[i]);\r\n}\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline enum ib_t10_dif_type\r\nscsi2ib_prot_type(unsigned char prot_type)\r\n{\r\nswitch (prot_type) {\r\ncase SCSI_PROT_DIF_TYPE0:\r\nreturn IB_T10DIF_NONE;\r\ncase SCSI_PROT_DIF_TYPE1:\r\nreturn IB_T10DIF_TYPE1;\r\ncase SCSI_PROT_DIF_TYPE2:\r\nreturn IB_T10DIF_TYPE2;\r\ncase SCSI_PROT_DIF_TYPE3:\r\nreturn IB_T10DIF_TYPE3;\r\ndefault:\r\nreturn IB_T10DIF_NONE;\r\n}\r\n}\r\nstatic int\r\niser_set_sig_attrs(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs)\r\n{\r\nunsigned char scsi_ptype = scsi_get_prot_type(sc);\r\nsig_attrs->mem.sig_type = IB_SIG_TYPE_T10_DIF;\r\nsig_attrs->wire.sig_type = IB_SIG_TYPE_T10_DIF;\r\nsig_attrs->mem.sig.dif.pi_interval = sc->device->sector_size;\r\nsig_attrs->wire.sig.dif.pi_interval = sc->device->sector_size;\r\nswitch (scsi_get_prot_op(sc)) {\r\ncase SCSI_PROT_WRITE_INSERT:\r\ncase SCSI_PROT_READ_STRIP:\r\nsig_attrs->mem.sig.dif.type = IB_T10DIF_NONE;\r\nsig_attrs->wire.sig.dif.type = scsi2ib_prot_type(scsi_ptype);\r\nsig_attrs->wire.sig.dif.bg_type = IB_T10DIF_CRC;\r\nsig_attrs->wire.sig.dif.ref_tag = scsi_get_lba(sc) &\r\n0xffffffff;\r\nbreak;\r\ncase SCSI_PROT_READ_INSERT:\r\ncase SCSI_PROT_WRITE_STRIP:\r\nsig_attrs->mem.sig.dif.type = scsi2ib_prot_type(scsi_ptype);\r\nsig_attrs->mem.sig.dif.bg_type = IB_T10DIF_CRC;\r\nsig_attrs->mem.sig.dif.ref_tag = scsi_get_lba(sc) &\r\n0xffffffff;\r\nsig_attrs->wire.sig.dif.type = IB_T10DIF_NONE;\r\nbreak;\r\ncase SCSI_PROT_READ_PASS:\r\ncase SCSI_PROT_WRITE_PASS:\r\nsig_attrs->mem.sig.dif.type = scsi2ib_prot_type(scsi_ptype);\r\nsig_attrs->mem.sig.dif.bg_type = IB_T10DIF_CRC;\r\nsig_attrs->mem.sig.dif.ref_tag = scsi_get_lba(sc) &\r\n0xffffffff;\r\nsig_attrs->wire.sig.dif.type = scsi2ib_prot_type(scsi_ptype);\r\nsig_attrs->wire.sig.dif.bg_type = IB_T10DIF_CRC;\r\nsig_attrs->wire.sig.dif.ref_tag = scsi_get_lba(sc) &\r\n0xffffffff;\r\nbreak;\r\ndefault:\r\niser_err("Unsupported PI operation %d\n",\r\nscsi_get_prot_op(sc));\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\niser_set_prot_checks(struct scsi_cmnd *sc, u8 *mask)\r\n{\r\nswitch (scsi_get_prot_type(sc)) {\r\ncase SCSI_PROT_DIF_TYPE0:\r\n*mask = 0x0;\r\nbreak;\r\ncase SCSI_PROT_DIF_TYPE1:\r\ncase SCSI_PROT_DIF_TYPE2:\r\n*mask = ISER_CHECK_GUARD | ISER_CHECK_REFTAG;\r\nbreak;\r\ncase SCSI_PROT_DIF_TYPE3:\r\n*mask = ISER_CHECK_GUARD;\r\nbreak;\r\ndefault:\r\niser_err("Unsupported protection type %d\n",\r\nscsi_get_prot_type(sc));\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\niser_reg_sig_mr(struct iscsi_iser_task *iser_task,\r\nstruct fast_reg_descriptor *desc, struct ib_sge *data_sge,\r\nstruct ib_sge *prot_sge, struct ib_sge *sig_sge)\r\n{\r\nstruct iser_conn *ib_conn = iser_task->ib_conn;\r\nstruct iser_pi_context *pi_ctx = desc->pi_ctx;\r\nstruct ib_send_wr sig_wr, inv_wr;\r\nstruct ib_send_wr *bad_wr, *wr = NULL;\r\nstruct ib_sig_attrs sig_attrs;\r\nint ret;\r\nu32 key;\r\nmemset(&sig_attrs, 0, sizeof(sig_attrs));\r\nret = iser_set_sig_attrs(iser_task->sc, &sig_attrs);\r\nif (ret)\r\ngoto err;\r\nret = iser_set_prot_checks(iser_task->sc, &sig_attrs.check_mask);\r\nif (ret)\r\ngoto err;\r\nif (!(desc->reg_indicators & ISER_SIG_KEY_VALID)) {\r\nmemset(&inv_wr, 0, sizeof(inv_wr));\r\ninv_wr.opcode = IB_WR_LOCAL_INV;\r\ninv_wr.wr_id = ISER_FASTREG_LI_WRID;\r\ninv_wr.ex.invalidate_rkey = pi_ctx->sig_mr->rkey;\r\nwr = &inv_wr;\r\nkey = (u8)(pi_ctx->sig_mr->rkey & 0x000000FF);\r\nib_update_fast_reg_key(pi_ctx->sig_mr, ++key);\r\n}\r\nmemset(&sig_wr, 0, sizeof(sig_wr));\r\nsig_wr.opcode = IB_WR_REG_SIG_MR;\r\nsig_wr.wr_id = ISER_FASTREG_LI_WRID;\r\nsig_wr.sg_list = data_sge;\r\nsig_wr.num_sge = 1;\r\nsig_wr.wr.sig_handover.sig_attrs = &sig_attrs;\r\nsig_wr.wr.sig_handover.sig_mr = pi_ctx->sig_mr;\r\nif (scsi_prot_sg_count(iser_task->sc))\r\nsig_wr.wr.sig_handover.prot = prot_sge;\r\nsig_wr.wr.sig_handover.access_flags = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE;\r\nif (!wr)\r\nwr = &sig_wr;\r\nelse\r\nwr->next = &sig_wr;\r\nret = ib_post_send(ib_conn->qp, wr, &bad_wr);\r\nif (ret) {\r\niser_err("reg_sig_mr failed, ret:%d\n", ret);\r\ngoto err;\r\n}\r\ndesc->reg_indicators &= ~ISER_SIG_KEY_VALID;\r\nsig_sge->lkey = pi_ctx->sig_mr->lkey;\r\nsig_sge->addr = 0;\r\nsig_sge->length = data_sge->length + prot_sge->length;\r\nif (scsi_get_prot_op(iser_task->sc) == SCSI_PROT_WRITE_INSERT ||\r\nscsi_get_prot_op(iser_task->sc) == SCSI_PROT_READ_STRIP) {\r\nsig_sge->length += (data_sge->length /\r\niser_task->sc->device->sector_size) * 8;\r\n}\r\niser_dbg("sig_sge: addr: 0x%llx length: %u lkey: 0x%x\n",\r\nsig_sge->addr, sig_sge->length,\r\nsig_sge->lkey);\r\nerr:\r\nreturn ret;\r\n}\r\nstatic int iser_fast_reg_mr(struct iscsi_iser_task *iser_task,\r\nstruct iser_regd_buf *regd_buf,\r\nstruct iser_data_buf *mem,\r\nenum iser_reg_indicator ind,\r\nstruct ib_sge *sge)\r\n{\r\nstruct fast_reg_descriptor *desc = regd_buf->reg.mem_h;\r\nstruct iser_conn *ib_conn = iser_task->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct ib_mr *mr;\r\nstruct ib_fast_reg_page_list *frpl;\r\nstruct ib_send_wr fastreg_wr, inv_wr;\r\nstruct ib_send_wr *bad_wr, *wr = NULL;\r\nu8 key;\r\nint ret, offset, size, plen;\r\nif (mem->dma_nents == 1) {\r\nstruct scatterlist *sg = (struct scatterlist *)mem->buf;\r\nsge->lkey = device->mr->lkey;\r\nsge->addr = ib_sg_dma_address(ibdev, &sg[0]);\r\nsge->length = ib_sg_dma_len(ibdev, &sg[0]);\r\niser_dbg("Single DMA entry: lkey=0x%x, addr=0x%llx, length=0x%x\n",\r\nsge->lkey, sge->addr, sge->length);\r\nreturn 0;\r\n}\r\nif (ind == ISER_DATA_KEY_VALID) {\r\nmr = desc->data_mr;\r\nfrpl = desc->data_frpl;\r\n} else {\r\nmr = desc->pi_ctx->prot_mr;\r\nfrpl = desc->pi_ctx->prot_frpl;\r\n}\r\nplen = iser_sg_to_page_vec(mem, device->ib_device, frpl->page_list,\r\n&offset, &size);\r\nif (plen * SIZE_4K < size) {\r\niser_err("fast reg page_list too short to hold this SG\n");\r\nreturn -EINVAL;\r\n}\r\nif (!(desc->reg_indicators & ind)) {\r\nmemset(&inv_wr, 0, sizeof(inv_wr));\r\ninv_wr.wr_id = ISER_FASTREG_LI_WRID;\r\ninv_wr.opcode = IB_WR_LOCAL_INV;\r\ninv_wr.ex.invalidate_rkey = mr->rkey;\r\nwr = &inv_wr;\r\nkey = (u8)(mr->rkey & 0x000000FF);\r\nib_update_fast_reg_key(mr, ++key);\r\n}\r\nmemset(&fastreg_wr, 0, sizeof(fastreg_wr));\r\nfastreg_wr.wr_id = ISER_FASTREG_LI_WRID;\r\nfastreg_wr.opcode = IB_WR_FAST_REG_MR;\r\nfastreg_wr.wr.fast_reg.iova_start = frpl->page_list[0] + offset;\r\nfastreg_wr.wr.fast_reg.page_list = frpl;\r\nfastreg_wr.wr.fast_reg.page_list_len = plen;\r\nfastreg_wr.wr.fast_reg.page_shift = SHIFT_4K;\r\nfastreg_wr.wr.fast_reg.length = size;\r\nfastreg_wr.wr.fast_reg.rkey = mr->rkey;\r\nfastreg_wr.wr.fast_reg.access_flags = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_READ);\r\nif (!wr)\r\nwr = &fastreg_wr;\r\nelse\r\nwr->next = &fastreg_wr;\r\nret = ib_post_send(ib_conn->qp, wr, &bad_wr);\r\nif (ret) {\r\niser_err("fast registration failed, ret:%d\n", ret);\r\nreturn ret;\r\n}\r\ndesc->reg_indicators &= ~ind;\r\nsge->lkey = mr->lkey;\r\nsge->addr = frpl->page_list[0] + offset;\r\nsge->length = size;\r\nreturn ret;\r\n}\r\nint iser_reg_rdma_mem_fastreg(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct iser_conn *ib_conn = iser_task->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\nstruct iser_regd_buf *regd_buf = &iser_task->rdma_regd[cmd_dir];\r\nstruct fast_reg_descriptor *desc = NULL;\r\nstruct ib_sge data_sge;\r\nint err, aligned_len;\r\nunsigned long flags;\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, ibdev, mem,\r\n&iser_task->data_copy[cmd_dir],\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\nmem = &iser_task->data_copy[cmd_dir];\r\n}\r\nif (mem->dma_nents != 1 ||\r\nscsi_get_prot_op(iser_task->sc) != SCSI_PROT_NORMAL) {\r\nspin_lock_irqsave(&ib_conn->lock, flags);\r\ndesc = list_first_entry(&ib_conn->fastreg.pool,\r\nstruct fast_reg_descriptor, list);\r\nlist_del(&desc->list);\r\nspin_unlock_irqrestore(&ib_conn->lock, flags);\r\nregd_buf->reg.mem_h = desc;\r\n}\r\nerr = iser_fast_reg_mr(iser_task, regd_buf, mem,\r\nISER_DATA_KEY_VALID, &data_sge);\r\nif (err)\r\ngoto err_reg;\r\nif (scsi_get_prot_op(iser_task->sc) != SCSI_PROT_NORMAL) {\r\nstruct ib_sge prot_sge, sig_sge;\r\nmemset(&prot_sge, 0, sizeof(prot_sge));\r\nif (scsi_prot_sg_count(iser_task->sc)) {\r\nmem = &iser_task->prot[cmd_dir];\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, ibdev, mem,\r\n&iser_task->prot_copy[cmd_dir],\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\nmem = &iser_task->prot_copy[cmd_dir];\r\n}\r\nerr = iser_fast_reg_mr(iser_task, regd_buf, mem,\r\nISER_PROT_KEY_VALID, &prot_sge);\r\nif (err)\r\ngoto err_reg;\r\n}\r\nerr = iser_reg_sig_mr(iser_task, desc, &data_sge,\r\n&prot_sge, &sig_sge);\r\nif (err) {\r\niser_err("Failed to register signature mr\n");\r\nreturn err;\r\n}\r\ndesc->reg_indicators |= ISER_FASTREG_PROTECTED;\r\nregd_buf->reg.lkey = sig_sge.lkey;\r\nregd_buf->reg.rkey = desc->pi_ctx->sig_mr->rkey;\r\nregd_buf->reg.va = sig_sge.addr;\r\nregd_buf->reg.len = sig_sge.length;\r\nregd_buf->reg.is_mr = 1;\r\n} else {\r\nif (desc) {\r\nregd_buf->reg.rkey = desc->data_mr->rkey;\r\nregd_buf->reg.is_mr = 1;\r\n} else {\r\nregd_buf->reg.rkey = device->mr->rkey;\r\nregd_buf->reg.is_mr = 0;\r\n}\r\nregd_buf->reg.lkey = data_sge.lkey;\r\nregd_buf->reg.va = data_sge.addr;\r\nregd_buf->reg.len = data_sge.length;\r\n}\r\nreturn 0;\r\nerr_reg:\r\nif (desc) {\r\nspin_lock_irqsave(&ib_conn->lock, flags);\r\nlist_add_tail(&desc->list, &ib_conn->fastreg.pool);\r\nspin_unlock_irqrestore(&ib_conn->lock, flags);\r\n}\r\nreturn err;\r\n}
