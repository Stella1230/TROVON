int __init ext4_init_pageio(void)\r\n{\r\nio_end_cachep = KMEM_CACHE(ext4_io_end, SLAB_RECLAIM_ACCOUNT);\r\nif (io_end_cachep == NULL)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid ext4_exit_pageio(void)\r\n{\r\nkmem_cache_destroy(io_end_cachep);\r\n}\r\nstatic void buffer_io_error(struct buffer_head *bh)\r\n{\r\nchar b[BDEVNAME_SIZE];\r\nprintk_ratelimited(KERN_ERR "Buffer I/O error on device %s, logical block %llu\n",\r\nbdevname(bh->b_bdev, b),\r\n(unsigned long long)bh->b_blocknr);\r\n}\r\nstatic void ext4_finish_bio(struct bio *bio)\r\n{\r\nint i;\r\nint error = !test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct bio_vec *bvec;\r\nbio_for_each_segment_all(bvec, bio, i) {\r\nstruct page *page = bvec->bv_page;\r\nstruct buffer_head *bh, *head;\r\nunsigned bio_start = bvec->bv_offset;\r\nunsigned bio_end = bio_start + bvec->bv_len;\r\nunsigned under_io = 0;\r\nunsigned long flags;\r\nif (!page)\r\ncontinue;\r\nif (error) {\r\nSetPageError(page);\r\nset_bit(AS_EIO, &page->mapping->flags);\r\n}\r\nbh = head = page_buffers(page);\r\nlocal_irq_save(flags);\r\nbit_spin_lock(BH_Uptodate_Lock, &head->b_state);\r\ndo {\r\nif (bh_offset(bh) < bio_start ||\r\nbh_offset(bh) + bh->b_size > bio_end) {\r\nif (buffer_async_write(bh))\r\nunder_io++;\r\ncontinue;\r\n}\r\nclear_buffer_async_write(bh);\r\nif (error)\r\nbuffer_io_error(bh);\r\n} while ((bh = bh->b_this_page) != head);\r\nbit_spin_unlock(BH_Uptodate_Lock, &head->b_state);\r\nlocal_irq_restore(flags);\r\nif (!under_io)\r\nend_page_writeback(page);\r\n}\r\n}\r\nstatic void ext4_release_io_end(ext4_io_end_t *io_end)\r\n{\r\nstruct bio *bio, *next_bio;\r\nBUG_ON(!list_empty(&io_end->list));\r\nBUG_ON(io_end->flag & EXT4_IO_END_UNWRITTEN);\r\nWARN_ON(io_end->handle);\r\nif (atomic_dec_and_test(&EXT4_I(io_end->inode)->i_ioend_count))\r\nwake_up_all(ext4_ioend_wq(io_end->inode));\r\nfor (bio = io_end->bio; bio; bio = next_bio) {\r\nnext_bio = bio->bi_private;\r\next4_finish_bio(bio);\r\nbio_put(bio);\r\n}\r\nkmem_cache_free(io_end_cachep, io_end);\r\n}\r\nstatic void ext4_clear_io_unwritten_flag(ext4_io_end_t *io_end)\r\n{\r\nstruct inode *inode = io_end->inode;\r\nio_end->flag &= ~EXT4_IO_END_UNWRITTEN;\r\nif (atomic_dec_and_test(&EXT4_I(inode)->i_unwritten))\r\nwake_up_all(ext4_ioend_wq(inode));\r\n}\r\nstatic int ext4_end_io(ext4_io_end_t *io)\r\n{\r\nstruct inode *inode = io->inode;\r\nloff_t offset = io->offset;\r\nssize_t size = io->size;\r\nhandle_t *handle = io->handle;\r\nint ret = 0;\r\next4_debug("ext4_end_io_nolock: io 0x%p from inode %lu,list->next 0x%p,"\r\n"list->prev 0x%p\n",\r\nio, inode->i_ino, io->list.next, io->list.prev);\r\nio->handle = NULL;\r\nret = ext4_convert_unwritten_extents(handle, inode, offset, size);\r\nif (ret < 0) {\r\next4_msg(inode->i_sb, KERN_EMERG,\r\n"failed to convert unwritten extents to written "\r\n"extents -- potential data loss! "\r\n"(inode %lu, offset %llu, size %zd, error %d)",\r\ninode->i_ino, offset, size, ret);\r\n}\r\next4_clear_io_unwritten_flag(io);\r\next4_release_io_end(io);\r\nreturn ret;\r\n}\r\nstatic void dump_completed_IO(struct inode *inode, struct list_head *head)\r\n{\r\n#ifdef EXT4FS_DEBUG\r\nstruct list_head *cur, *before, *after;\r\next4_io_end_t *io, *io0, *io1;\r\nif (list_empty(head))\r\nreturn;\r\next4_debug("Dump inode %lu completed io list\n", inode->i_ino);\r\nlist_for_each_entry(io, head, list) {\r\ncur = &io->list;\r\nbefore = cur->prev;\r\nio0 = container_of(before, ext4_io_end_t, list);\r\nafter = cur->next;\r\nio1 = container_of(after, ext4_io_end_t, list);\r\next4_debug("io 0x%p from inode %lu,prev 0x%p,next 0x%p\n",\r\nio, inode->i_ino, io0, io1);\r\n}\r\n#endif\r\n}\r\nstatic void ext4_add_complete_io(ext4_io_end_t *io_end)\r\n{\r\nstruct ext4_inode_info *ei = EXT4_I(io_end->inode);\r\nstruct ext4_sb_info *sbi = EXT4_SB(io_end->inode->i_sb);\r\nstruct workqueue_struct *wq;\r\nunsigned long flags;\r\nWARN_ON(!(io_end->flag & EXT4_IO_END_UNWRITTEN));\r\nWARN_ON(!io_end->handle && sbi->s_journal);\r\nspin_lock_irqsave(&ei->i_completed_io_lock, flags);\r\nwq = sbi->rsv_conversion_wq;\r\nif (list_empty(&ei->i_rsv_conversion_list))\r\nqueue_work(wq, &ei->i_rsv_conversion_work);\r\nlist_add_tail(&io_end->list, &ei->i_rsv_conversion_list);\r\nspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\r\n}\r\nstatic int ext4_do_flush_completed_IO(struct inode *inode,\r\nstruct list_head *head)\r\n{\r\next4_io_end_t *io;\r\nstruct list_head unwritten;\r\nunsigned long flags;\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\nint err, ret = 0;\r\nspin_lock_irqsave(&ei->i_completed_io_lock, flags);\r\ndump_completed_IO(inode, head);\r\nlist_replace_init(head, &unwritten);\r\nspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\r\nwhile (!list_empty(&unwritten)) {\r\nio = list_entry(unwritten.next, ext4_io_end_t, list);\r\nBUG_ON(!(io->flag & EXT4_IO_END_UNWRITTEN));\r\nlist_del_init(&io->list);\r\nerr = ext4_end_io(io);\r\nif (unlikely(!ret && err))\r\nret = err;\r\n}\r\nreturn ret;\r\n}\r\nvoid ext4_end_io_rsv_work(struct work_struct *work)\r\n{\r\nstruct ext4_inode_info *ei = container_of(work, struct ext4_inode_info,\r\ni_rsv_conversion_work);\r\next4_do_flush_completed_IO(&ei->vfs_inode, &ei->i_rsv_conversion_list);\r\n}\r\next4_io_end_t *ext4_init_io_end(struct inode *inode, gfp_t flags)\r\n{\r\next4_io_end_t *io = kmem_cache_zalloc(io_end_cachep, flags);\r\nif (io) {\r\natomic_inc(&EXT4_I(inode)->i_ioend_count);\r\nio->inode = inode;\r\nINIT_LIST_HEAD(&io->list);\r\natomic_set(&io->count, 1);\r\n}\r\nreturn io;\r\n}\r\nvoid ext4_put_io_end_defer(ext4_io_end_t *io_end)\r\n{\r\nif (atomic_dec_and_test(&io_end->count)) {\r\nif (!(io_end->flag & EXT4_IO_END_UNWRITTEN) || !io_end->size) {\r\next4_release_io_end(io_end);\r\nreturn;\r\n}\r\next4_add_complete_io(io_end);\r\n}\r\n}\r\nint ext4_put_io_end(ext4_io_end_t *io_end)\r\n{\r\nint err = 0;\r\nif (atomic_dec_and_test(&io_end->count)) {\r\nif (io_end->flag & EXT4_IO_END_UNWRITTEN) {\r\nerr = ext4_convert_unwritten_extents(io_end->handle,\r\nio_end->inode, io_end->offset,\r\nio_end->size);\r\nio_end->handle = NULL;\r\next4_clear_io_unwritten_flag(io_end);\r\n}\r\next4_release_io_end(io_end);\r\n}\r\nreturn err;\r\n}\r\next4_io_end_t *ext4_get_io_end(ext4_io_end_t *io_end)\r\n{\r\natomic_inc(&io_end->count);\r\nreturn io_end;\r\n}\r\nstatic void ext4_end_bio(struct bio *bio, int error)\r\n{\r\next4_io_end_t *io_end = bio->bi_private;\r\nsector_t bi_sector = bio->bi_iter.bi_sector;\r\nBUG_ON(!io_end);\r\nbio->bi_end_io = NULL;\r\nif (test_bit(BIO_UPTODATE, &bio->bi_flags))\r\nerror = 0;\r\nif (error) {\r\nstruct inode *inode = io_end->inode;\r\next4_warning(inode->i_sb, "I/O error %d writing to inode %lu "\r\n"(offset %llu size %ld starting block %llu)",\r\nerror, inode->i_ino,\r\n(unsigned long long) io_end->offset,\r\n(long) io_end->size,\r\n(unsigned long long)\r\nbi_sector >> (inode->i_blkbits - 9));\r\nmapping_set_error(inode->i_mapping, error);\r\n}\r\nif (io_end->flag & EXT4_IO_END_UNWRITTEN) {\r\nbio->bi_private = xchg(&io_end->bio, bio);\r\next4_put_io_end_defer(io_end);\r\n} else {\r\next4_put_io_end_defer(io_end);\r\next4_finish_bio(bio);\r\nbio_put(bio);\r\n}\r\n}\r\nvoid ext4_io_submit(struct ext4_io_submit *io)\r\n{\r\nstruct bio *bio = io->io_bio;\r\nif (bio) {\r\nbio_get(io->io_bio);\r\nsubmit_bio(io->io_op, io->io_bio);\r\nBUG_ON(bio_flagged(io->io_bio, BIO_EOPNOTSUPP));\r\nbio_put(io->io_bio);\r\n}\r\nio->io_bio = NULL;\r\n}\r\nvoid ext4_io_submit_init(struct ext4_io_submit *io,\r\nstruct writeback_control *wbc)\r\n{\r\nio->io_op = (wbc->sync_mode == WB_SYNC_ALL ? WRITE_SYNC : WRITE);\r\nio->io_bio = NULL;\r\nio->io_end = NULL;\r\n}\r\nstatic int io_submit_init_bio(struct ext4_io_submit *io,\r\nstruct buffer_head *bh)\r\n{\r\nint nvecs = bio_get_nr_vecs(bh->b_bdev);\r\nstruct bio *bio;\r\nbio = bio_alloc(GFP_NOIO, min(nvecs, BIO_MAX_PAGES));\r\nif (!bio)\r\nreturn -ENOMEM;\r\nbio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);\r\nbio->bi_bdev = bh->b_bdev;\r\nbio->bi_end_io = ext4_end_bio;\r\nbio->bi_private = ext4_get_io_end(io->io_end);\r\nio->io_bio = bio;\r\nio->io_next_block = bh->b_blocknr;\r\nreturn 0;\r\n}\r\nstatic int io_submit_add_bh(struct ext4_io_submit *io,\r\nstruct inode *inode,\r\nstruct buffer_head *bh)\r\n{\r\nint ret;\r\nif (io->io_bio && bh->b_blocknr != io->io_next_block) {\r\nsubmit_and_retry:\r\next4_io_submit(io);\r\n}\r\nif (io->io_bio == NULL) {\r\nret = io_submit_init_bio(io, bh);\r\nif (ret)\r\nreturn ret;\r\n}\r\nret = bio_add_page(io->io_bio, bh->b_page, bh->b_size, bh_offset(bh));\r\nif (ret != bh->b_size)\r\ngoto submit_and_retry;\r\nio->io_next_block++;\r\nreturn 0;\r\n}\r\nint ext4_bio_write_page(struct ext4_io_submit *io,\r\nstruct page *page,\r\nint len,\r\nstruct writeback_control *wbc,\r\nbool keep_towrite)\r\n{\r\nstruct inode *inode = page->mapping->host;\r\nunsigned block_start, blocksize;\r\nstruct buffer_head *bh, *head;\r\nint ret = 0;\r\nint nr_submitted = 0;\r\nblocksize = 1 << inode->i_blkbits;\r\nBUG_ON(!PageLocked(page));\r\nBUG_ON(PageWriteback(page));\r\nif (keep_towrite)\r\nset_page_writeback_keepwrite(page);\r\nelse\r\nset_page_writeback(page);\r\nClearPageError(page);\r\nif (len < PAGE_CACHE_SIZE)\r\nzero_user_segment(page, len, PAGE_CACHE_SIZE);\r\nbh = head = page_buffers(page);\r\ndo {\r\nblock_start = bh_offset(bh);\r\nif (block_start >= len) {\r\nclear_buffer_dirty(bh);\r\nset_buffer_uptodate(bh);\r\ncontinue;\r\n}\r\nif (!buffer_dirty(bh) || buffer_delay(bh) ||\r\n!buffer_mapped(bh) || buffer_unwritten(bh)) {\r\nif (!buffer_mapped(bh))\r\nclear_buffer_dirty(bh);\r\nif (io->io_bio)\r\next4_io_submit(io);\r\ncontinue;\r\n}\r\nif (buffer_new(bh)) {\r\nclear_buffer_new(bh);\r\nunmap_underlying_metadata(bh->b_bdev, bh->b_blocknr);\r\n}\r\nset_buffer_async_write(bh);\r\n} while ((bh = bh->b_this_page) != head);\r\nbh = head = page_buffers(page);\r\ndo {\r\nif (!buffer_async_write(bh))\r\ncontinue;\r\nret = io_submit_add_bh(io, inode, bh);\r\nif (ret) {\r\nredirty_page_for_writepage(wbc, page);\r\nbreak;\r\n}\r\nnr_submitted++;\r\nclear_buffer_dirty(bh);\r\n} while ((bh = bh->b_this_page) != head);\r\nif (ret) {\r\ndo {\r\nclear_buffer_async_write(bh);\r\nbh = bh->b_this_page;\r\n} while (bh != head);\r\n}\r\nunlock_page(page);\r\nif (!nr_submitted)\r\nend_page_writeback(page);\r\nreturn ret;\r\n}
