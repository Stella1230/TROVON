static u64 precise_store_data(u64 status)\r\n{\r\nunion intel_x86_pebs_dse dse;\r\nu64 val = P(OP, STORE) | P(SNOOP, NA) | P(LVL, L1) | P(TLB, L2);\r\ndse.val = status;\r\nif (dse.st_stlb_miss)\r\nval |= P(TLB, MISS);\r\nelse\r\nval |= P(TLB, HIT);\r\nif (dse.st_l1d_hit)\r\nval |= P(LVL, HIT);\r\nelse\r\nval |= P(LVL, MISS);\r\nif (dse.st_locked)\r\nval |= P(LOCK, LOCKED);\r\nreturn val;\r\n}\r\nstatic u64 precise_store_data_hsw(struct perf_event *event, u64 status)\r\n{\r\nunion perf_mem_data_src dse;\r\nu64 cfg = event->hw.config & INTEL_ARCH_EVENT_MASK;\r\ndse.val = 0;\r\ndse.mem_op = PERF_MEM_OP_STORE;\r\ndse.mem_lvl = PERF_MEM_LVL_NA;\r\nif (cfg != 0x12d0 && cfg != 0x22d0 && cfg != 0x42d0 && cfg != 0x82d0)\r\nreturn dse.mem_lvl;\r\nif (status & 1)\r\ndse.mem_lvl = PERF_MEM_LVL_L1 | PERF_MEM_LVL_HIT;\r\nelse\r\ndse.mem_lvl = PERF_MEM_LVL_L1 | PERF_MEM_LVL_MISS;\r\nreturn dse.val;\r\n}\r\nstatic u64 load_latency_data(u64 status)\r\n{\r\nunion intel_x86_pebs_dse dse;\r\nu64 val;\r\nint model = boot_cpu_data.x86_model;\r\nint fam = boot_cpu_data.x86;\r\ndse.val = status;\r\nval = pebs_data_source[dse.ld_dse];\r\nif (fam == 0x6 && (model == 26 || model == 30\r\n|| model == 31 || model == 46)) {\r\nval |= P(TLB, NA) | P(LOCK, NA);\r\nreturn val;\r\n}\r\nif (dse.ld_stlb_miss)\r\nval |= P(TLB, MISS) | P(TLB, L2);\r\nelse\r\nval |= P(TLB, HIT) | P(TLB, L1) | P(TLB, L2);\r\nif (dse.ld_locked)\r\nval |= P(LOCK, LOCKED);\r\nreturn val;\r\n}\r\nvoid init_debug_store_on_cpu(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds)\r\nreturn;\r\nwrmsr_on_cpu(cpu, MSR_IA32_DS_AREA,\r\n(u32)((u64)(unsigned long)ds),\r\n(u32)((u64)(unsigned long)ds >> 32));\r\n}\r\nvoid fini_debug_store_on_cpu(int cpu)\r\n{\r\nif (!per_cpu(cpu_hw_events, cpu).ds)\r\nreturn;\r\nwrmsr_on_cpu(cpu, MSR_IA32_DS_AREA, 0, 0);\r\n}\r\nstatic int alloc_pebs_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nint node = cpu_to_node(cpu);\r\nint max, thresh = 1;\r\nvoid *buffer, *ibuffer;\r\nif (!x86_pmu.pebs)\r\nreturn 0;\r\nbuffer = kzalloc_node(PEBS_BUFFER_SIZE, GFP_KERNEL, node);\r\nif (unlikely(!buffer))\r\nreturn -ENOMEM;\r\nif (x86_pmu.intel_cap.pebs_format < 2) {\r\nibuffer = kzalloc_node(PEBS_FIXUP_SIZE, GFP_KERNEL, node);\r\nif (!ibuffer) {\r\nkfree(buffer);\r\nreturn -ENOMEM;\r\n}\r\nper_cpu(insn_buffer, cpu) = ibuffer;\r\n}\r\nmax = PEBS_BUFFER_SIZE / x86_pmu.pebs_record_size;\r\nds->pebs_buffer_base = (u64)(unsigned long)buffer;\r\nds->pebs_index = ds->pebs_buffer_base;\r\nds->pebs_absolute_maximum = ds->pebs_buffer_base +\r\nmax * x86_pmu.pebs_record_size;\r\nds->pebs_interrupt_threshold = ds->pebs_buffer_base +\r\nthresh * x86_pmu.pebs_record_size;\r\nreturn 0;\r\n}\r\nstatic void release_pebs_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds || !x86_pmu.pebs)\r\nreturn;\r\nkfree(per_cpu(insn_buffer, cpu));\r\nper_cpu(insn_buffer, cpu) = NULL;\r\nkfree((void *)(unsigned long)ds->pebs_buffer_base);\r\nds->pebs_buffer_base = 0;\r\n}\r\nstatic int alloc_bts_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nint node = cpu_to_node(cpu);\r\nint max, thresh;\r\nvoid *buffer;\r\nif (!x86_pmu.bts)\r\nreturn 0;\r\nbuffer = kzalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, node);\r\nif (unlikely(!buffer)) {\r\nWARN_ONCE(1, "%s: BTS buffer allocation failure\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nmax = BTS_BUFFER_SIZE / BTS_RECORD_SIZE;\r\nthresh = max / 16;\r\nds->bts_buffer_base = (u64)(unsigned long)buffer;\r\nds->bts_index = ds->bts_buffer_base;\r\nds->bts_absolute_maximum = ds->bts_buffer_base +\r\nmax * BTS_RECORD_SIZE;\r\nds->bts_interrupt_threshold = ds->bts_absolute_maximum -\r\nthresh * BTS_RECORD_SIZE;\r\nreturn 0;\r\n}\r\nstatic void release_bts_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds || !x86_pmu.bts)\r\nreturn;\r\nkfree((void *)(unsigned long)ds->bts_buffer_base);\r\nds->bts_buffer_base = 0;\r\n}\r\nstatic int alloc_ds_buffer(int cpu)\r\n{\r\nint node = cpu_to_node(cpu);\r\nstruct debug_store *ds;\r\nds = kzalloc_node(sizeof(*ds), GFP_KERNEL, node);\r\nif (unlikely(!ds))\r\nreturn -ENOMEM;\r\nper_cpu(cpu_hw_events, cpu).ds = ds;\r\nreturn 0;\r\n}\r\nstatic void release_ds_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds)\r\nreturn;\r\nper_cpu(cpu_hw_events, cpu).ds = NULL;\r\nkfree(ds);\r\n}\r\nvoid release_ds_buffers(void)\r\n{\r\nint cpu;\r\nif (!x86_pmu.bts && !x86_pmu.pebs)\r\nreturn;\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu)\r\nfini_debug_store_on_cpu(cpu);\r\nfor_each_possible_cpu(cpu) {\r\nrelease_pebs_buffer(cpu);\r\nrelease_bts_buffer(cpu);\r\nrelease_ds_buffer(cpu);\r\n}\r\nput_online_cpus();\r\n}\r\nvoid reserve_ds_buffers(void)\r\n{\r\nint bts_err = 0, pebs_err = 0;\r\nint cpu;\r\nx86_pmu.bts_active = 0;\r\nx86_pmu.pebs_active = 0;\r\nif (!x86_pmu.bts && !x86_pmu.pebs)\r\nreturn;\r\nif (!x86_pmu.bts)\r\nbts_err = 1;\r\nif (!x86_pmu.pebs)\r\npebs_err = 1;\r\nget_online_cpus();\r\nfor_each_possible_cpu(cpu) {\r\nif (alloc_ds_buffer(cpu)) {\r\nbts_err = 1;\r\npebs_err = 1;\r\n}\r\nif (!bts_err && alloc_bts_buffer(cpu))\r\nbts_err = 1;\r\nif (!pebs_err && alloc_pebs_buffer(cpu))\r\npebs_err = 1;\r\nif (bts_err && pebs_err)\r\nbreak;\r\n}\r\nif (bts_err) {\r\nfor_each_possible_cpu(cpu)\r\nrelease_bts_buffer(cpu);\r\n}\r\nif (pebs_err) {\r\nfor_each_possible_cpu(cpu)\r\nrelease_pebs_buffer(cpu);\r\n}\r\nif (bts_err && pebs_err) {\r\nfor_each_possible_cpu(cpu)\r\nrelease_ds_buffer(cpu);\r\n} else {\r\nif (x86_pmu.bts && !bts_err)\r\nx86_pmu.bts_active = 1;\r\nif (x86_pmu.pebs && !pebs_err)\r\nx86_pmu.pebs_active = 1;\r\nfor_each_online_cpu(cpu)\r\ninit_debug_store_on_cpu(cpu);\r\n}\r\nput_online_cpus();\r\n}\r\nvoid intel_pmu_enable_bts(u64 config)\r\n{\r\nunsigned long debugctlmsr;\r\ndebugctlmsr = get_debugctlmsr();\r\ndebugctlmsr |= DEBUGCTLMSR_TR;\r\ndebugctlmsr |= DEBUGCTLMSR_BTS;\r\ndebugctlmsr |= DEBUGCTLMSR_BTINT;\r\nif (!(config & ARCH_PERFMON_EVENTSEL_OS))\r\ndebugctlmsr |= DEBUGCTLMSR_BTS_OFF_OS;\r\nif (!(config & ARCH_PERFMON_EVENTSEL_USR))\r\ndebugctlmsr |= DEBUGCTLMSR_BTS_OFF_USR;\r\nupdate_debugctlmsr(debugctlmsr);\r\n}\r\nvoid intel_pmu_disable_bts(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nunsigned long debugctlmsr;\r\nif (!cpuc->ds)\r\nreturn;\r\ndebugctlmsr = get_debugctlmsr();\r\ndebugctlmsr &=\r\n~(DEBUGCTLMSR_TR | DEBUGCTLMSR_BTS | DEBUGCTLMSR_BTINT |\r\nDEBUGCTLMSR_BTS_OFF_OS | DEBUGCTLMSR_BTS_OFF_USR);\r\nupdate_debugctlmsr(debugctlmsr);\r\n}\r\nint intel_pmu_drain_bts_buffer(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct debug_store *ds = cpuc->ds;\r\nstruct bts_record {\r\nu64 from;\r\nu64 to;\r\nu64 flags;\r\n};\r\nstruct perf_event *event = cpuc->events[INTEL_PMC_IDX_FIXED_BTS];\r\nstruct bts_record *at, *top;\r\nstruct perf_output_handle handle;\r\nstruct perf_event_header header;\r\nstruct perf_sample_data data;\r\nstruct pt_regs regs;\r\nif (!event)\r\nreturn 0;\r\nif (!x86_pmu.bts_active)\r\nreturn 0;\r\nat = (struct bts_record *)(unsigned long)ds->bts_buffer_base;\r\ntop = (struct bts_record *)(unsigned long)ds->bts_index;\r\nif (top <= at)\r\nreturn 0;\r\nmemset(&regs, 0, sizeof(regs));\r\nds->bts_index = ds->bts_buffer_base;\r\nperf_sample_data_init(&data, 0, event->hw.last_period);\r\nperf_prepare_sample(&header, &data, event, &regs);\r\nif (perf_output_begin(&handle, event, header.size * (top - at)))\r\nreturn 1;\r\nfor (; at < top; at++) {\r\ndata.ip = at->from;\r\ndata.addr = at->to;\r\nperf_output_sample(&handle, &header, &data, event);\r\n}\r\nperf_output_end(&handle);\r\nevent->hw.interrupts++;\r\nevent->pending_kill = POLL_IN;\r\nreturn 1;\r\n}\r\nstruct event_constraint *intel_pebs_constraints(struct perf_event *event)\r\n{\r\nstruct event_constraint *c;\r\nif (!event->attr.precise_ip)\r\nreturn NULL;\r\nif (x86_pmu.pebs_constraints) {\r\nfor_each_event_constraint(c, x86_pmu.pebs_constraints) {\r\nif ((event->hw.config & c->cmask) == c->code) {\r\nevent->hw.flags |= c->flags;\r\nreturn c;\r\n}\r\n}\r\n}\r\nreturn &emptyconstraint;\r\n}\r\nvoid intel_pmu_pebs_enable(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nhwc->config &= ~ARCH_PERFMON_EVENTSEL_INT;\r\ncpuc->pebs_enabled |= 1ULL << hwc->idx;\r\nif (event->hw.flags & PERF_X86_EVENT_PEBS_LDLAT)\r\ncpuc->pebs_enabled |= 1ULL << (hwc->idx + 32);\r\nelse if (event->hw.flags & PERF_X86_EVENT_PEBS_ST)\r\ncpuc->pebs_enabled |= 1ULL << 63;\r\n}\r\nvoid intel_pmu_pebs_disable(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\ncpuc->pebs_enabled &= ~(1ULL << hwc->idx);\r\nif (event->hw.constraint->flags & PERF_X86_EVENT_PEBS_LDLAT)\r\ncpuc->pebs_enabled &= ~(1ULL << (hwc->idx + 32));\r\nelse if (event->hw.constraint->flags & PERF_X86_EVENT_PEBS_ST)\r\ncpuc->pebs_enabled &= ~(1ULL << 63);\r\nif (cpuc->enabled)\r\nwrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\r\nhwc->config |= ARCH_PERFMON_EVENTSEL_INT;\r\n}\r\nvoid intel_pmu_pebs_enable_all(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nif (cpuc->pebs_enabled)\r\nwrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\r\n}\r\nvoid intel_pmu_pebs_disable_all(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nif (cpuc->pebs_enabled)\r\nwrmsrl(MSR_IA32_PEBS_ENABLE, 0);\r\n}\r\nstatic int intel_pmu_pebs_fixup_ip(struct pt_regs *regs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nunsigned long from = cpuc->lbr_entries[0].from;\r\nunsigned long old_to, to = cpuc->lbr_entries[0].to;\r\nunsigned long ip = regs->ip;\r\nint is_64bit = 0;\r\nvoid *kaddr;\r\nif (!x86_pmu.intel_cap.pebs_trap)\r\nreturn 1;\r\nif (!cpuc->lbr_stack.nr || !from || !to)\r\nreturn 0;\r\nif (kernel_ip(ip) != kernel_ip(to))\r\nreturn 0;\r\nif ((ip - to) > PEBS_FIXUP_SIZE)\r\nreturn 0;\r\nif (ip == to) {\r\nset_linear_ip(regs, from);\r\nreturn 1;\r\n}\r\nif (!kernel_ip(ip)) {\r\nint size, bytes;\r\nu8 *buf = this_cpu_read(insn_buffer);\r\nsize = ip - to;\r\nbytes = copy_from_user_nmi(buf, (void __user *)to, size);\r\nif (bytes != 0)\r\nreturn 0;\r\nkaddr = buf;\r\n} else {\r\nkaddr = (void *)to;\r\n}\r\ndo {\r\nstruct insn insn;\r\nold_to = to;\r\n#ifdef CONFIG_X86_64\r\nis_64bit = kernel_ip(to) || !test_thread_flag(TIF_IA32);\r\n#endif\r\ninsn_init(&insn, kaddr, is_64bit);\r\ninsn_get_length(&insn);\r\nto += insn.length;\r\nkaddr += insn.length;\r\n} while (to < ip);\r\nif (to == ip) {\r\nset_linear_ip(regs, old_to);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline u64 intel_hsw_weight(struct pebs_record_hsw *pebs)\r\n{\r\nif (pebs->tsx_tuning) {\r\nunion hsw_tsx_tuning tsx = { .value = pebs->tsx_tuning };\r\nreturn tsx.cycles_last_block;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline u64 intel_hsw_transaction(struct pebs_record_hsw *pebs)\r\n{\r\nu64 txn = (pebs->tsx_tuning & PEBS_HSW_TSX_FLAGS) >> 32;\r\nif ((txn & PERF_TXN_TRANSACTION) && (pebs->ax & 1))\r\ntxn |= ((pebs->ax >> 24) & 0xff) << PERF_TXN_ABORT_SHIFT;\r\nreturn txn;\r\n}\r\nstatic void __intel_pmu_pebs_event(struct perf_event *event,\r\nstruct pt_regs *iregs, void *__pebs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct pebs_record_hsw *pebs = __pebs;\r\nstruct perf_sample_data data;\r\nstruct pt_regs regs;\r\nu64 sample_type;\r\nint fll, fst;\r\nif (!intel_pmu_save_and_restart(event))\r\nreturn;\r\nfll = event->hw.flags & PERF_X86_EVENT_PEBS_LDLAT;\r\nfst = event->hw.flags & (PERF_X86_EVENT_PEBS_ST |\r\nPERF_X86_EVENT_PEBS_ST_HSW);\r\nperf_sample_data_init(&data, 0, event->hw.last_period);\r\ndata.period = event->hw.last_period;\r\nsample_type = event->attr.sample_type;\r\nif (fll || fst) {\r\nif (fll && (sample_type & PERF_SAMPLE_WEIGHT))\r\ndata.weight = pebs->lat;\r\nif (sample_type & PERF_SAMPLE_DATA_SRC) {\r\nif (fll)\r\ndata.data_src.val = load_latency_data(pebs->dse);\r\nelse if (event->hw.flags & PERF_X86_EVENT_PEBS_ST_HSW)\r\ndata.data_src.val =\r\nprecise_store_data_hsw(event, pebs->dse);\r\nelse\r\ndata.data_src.val = precise_store_data(pebs->dse);\r\n}\r\n}\r\nregs = *iregs;\r\nregs.flags = pebs->flags;\r\nset_linear_ip(&regs, pebs->ip);\r\nregs.bp = pebs->bp;\r\nregs.sp = pebs->sp;\r\nif (event->attr.precise_ip > 1 && x86_pmu.intel_cap.pebs_format >= 2) {\r\nregs.ip = pebs->real_ip;\r\nregs.flags |= PERF_EFLAGS_EXACT;\r\n} else if (event->attr.precise_ip > 1 && intel_pmu_pebs_fixup_ip(&regs))\r\nregs.flags |= PERF_EFLAGS_EXACT;\r\nelse\r\nregs.flags &= ~PERF_EFLAGS_EXACT;\r\nif ((event->attr.sample_type & PERF_SAMPLE_ADDR) &&\r\nx86_pmu.intel_cap.pebs_format >= 1)\r\ndata.addr = pebs->dla;\r\nif (x86_pmu.intel_cap.pebs_format >= 2) {\r\nif ((event->attr.sample_type & PERF_SAMPLE_WEIGHT) && !fll)\r\ndata.weight = intel_hsw_weight(pebs);\r\nif (event->attr.sample_type & PERF_SAMPLE_TRANSACTION)\r\ndata.txn = intel_hsw_transaction(pebs);\r\n}\r\nif (has_branch_stack(event))\r\ndata.br_stack = &cpuc->lbr_stack;\r\nif (perf_event_overflow(event, &data, &regs))\r\nx86_pmu_stop(event, 0);\r\n}\r\nstatic void intel_pmu_drain_pebs_core(struct pt_regs *iregs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct debug_store *ds = cpuc->ds;\r\nstruct perf_event *event = cpuc->events[0];\r\nstruct pebs_record_core *at, *top;\r\nint n;\r\nif (!x86_pmu.pebs_active)\r\nreturn;\r\nat = (struct pebs_record_core *)(unsigned long)ds->pebs_buffer_base;\r\ntop = (struct pebs_record_core *)(unsigned long)ds->pebs_index;\r\nds->pebs_index = ds->pebs_buffer_base;\r\nif (!test_bit(0, cpuc->active_mask))\r\nreturn;\r\nWARN_ON_ONCE(!event);\r\nif (!event->attr.precise_ip)\r\nreturn;\r\nn = top - at;\r\nif (n <= 0)\r\nreturn;\r\nWARN_ONCE(n > 1, "bad leftover pebs %d\n", n);\r\nat += n - 1;\r\n__intel_pmu_pebs_event(event, iregs, at);\r\n}\r\nstatic void intel_pmu_drain_pebs_nhm(struct pt_regs *iregs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct debug_store *ds = cpuc->ds;\r\nstruct perf_event *event = NULL;\r\nvoid *at, *top;\r\nu64 status = 0;\r\nint bit;\r\nif (!x86_pmu.pebs_active)\r\nreturn;\r\nat = (struct pebs_record_nhm *)(unsigned long)ds->pebs_buffer_base;\r\ntop = (struct pebs_record_nhm *)(unsigned long)ds->pebs_index;\r\nds->pebs_index = ds->pebs_buffer_base;\r\nif (unlikely(at > top))\r\nreturn;\r\nWARN_ONCE(top - at > x86_pmu.max_pebs_events * x86_pmu.pebs_record_size,\r\n"Unexpected number of pebs records %ld\n",\r\n(long)(top - at) / x86_pmu.pebs_record_size);\r\nfor (; at < top; at += x86_pmu.pebs_record_size) {\r\nstruct pebs_record_nhm *p = at;\r\nfor_each_set_bit(bit, (unsigned long *)&p->status,\r\nx86_pmu.max_pebs_events) {\r\nevent = cpuc->events[bit];\r\nif (!test_bit(bit, cpuc->active_mask))\r\ncontinue;\r\nWARN_ON_ONCE(!event);\r\nif (!event->attr.precise_ip)\r\ncontinue;\r\nif (__test_and_set_bit(bit, (unsigned long *)&status))\r\ncontinue;\r\nbreak;\r\n}\r\nif (!event || bit >= x86_pmu.max_pebs_events)\r\ncontinue;\r\n__intel_pmu_pebs_event(event, iregs, at);\r\n}\r\n}\r\nvoid intel_ds_init(void)\r\n{\r\nif (!boot_cpu_has(X86_FEATURE_DTES64))\r\nreturn;\r\nx86_pmu.bts = boot_cpu_has(X86_FEATURE_BTS);\r\nx86_pmu.pebs = boot_cpu_has(X86_FEATURE_PEBS);\r\nif (x86_pmu.pebs) {\r\nchar pebs_type = x86_pmu.intel_cap.pebs_trap ? '+' : '-';\r\nint format = x86_pmu.intel_cap.pebs_format;\r\nswitch (format) {\r\ncase 0:\r\nprintk(KERN_CONT "PEBS fmt0%c, ", pebs_type);\r\nx86_pmu.pebs_record_size = sizeof(struct pebs_record_core);\r\nx86_pmu.drain_pebs = intel_pmu_drain_pebs_core;\r\nbreak;\r\ncase 1:\r\nprintk(KERN_CONT "PEBS fmt1%c, ", pebs_type);\r\nx86_pmu.pebs_record_size = sizeof(struct pebs_record_nhm);\r\nx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\r\nbreak;\r\ncase 2:\r\npr_cont("PEBS fmt2%c, ", pebs_type);\r\nx86_pmu.pebs_record_size = sizeof(struct pebs_record_hsw);\r\nx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\r\nbreak;\r\ndefault:\r\nprintk(KERN_CONT "no PEBS fmt%d%c, ", format, pebs_type);\r\nx86_pmu.pebs = 0;\r\n}\r\n}\r\n}\r\nvoid perf_restore_debug_store(void)\r\n{\r\nstruct debug_store *ds = __this_cpu_read(cpu_hw_events.ds);\r\nif (!x86_pmu.bts && !x86_pmu.pebs)\r\nreturn;\r\nwrmsrl(MSR_IA32_DS_AREA, (unsigned long)ds);\r\n}
