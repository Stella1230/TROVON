int iptunnel_xmit(struct sock *sk, struct rtable *rt, struct sk_buff *skb,\r\n__be32 src, __be32 dst, __u8 proto,\r\n__u8 tos, __u8 ttl, __be16 df, bool xnet)\r\n{\r\nint pkt_len = skb->len;\r\nstruct iphdr *iph;\r\nint err;\r\nskb_scrub_packet(skb, xnet);\r\nskb_clear_hash(skb);\r\nskb_dst_set(skb, &rt->dst);\r\nmemset(IPCB(skb), 0, sizeof(*IPCB(skb)));\r\nskb_push(skb, sizeof(struct iphdr));\r\nskb_reset_network_header(skb);\r\niph = ip_hdr(skb);\r\niph->version = 4;\r\niph->ihl = sizeof(struct iphdr) >> 2;\r\niph->frag_off = df;\r\niph->protocol = proto;\r\niph->tos = tos;\r\niph->daddr = dst;\r\niph->saddr = src;\r\niph->ttl = ttl;\r\n__ip_select_ident(iph, skb_shinfo(skb)->gso_segs ?: 1);\r\nerr = ip_local_out_sk(sk, skb);\r\nif (unlikely(net_xmit_eval(err)))\r\npkt_len = 0;\r\nreturn pkt_len;\r\n}\r\nint iptunnel_pull_header(struct sk_buff *skb, int hdr_len, __be16 inner_proto)\r\n{\r\nif (unlikely(!pskb_may_pull(skb, hdr_len)))\r\nreturn -ENOMEM;\r\nskb_pull_rcsum(skb, hdr_len);\r\nif (inner_proto == htons(ETH_P_TEB)) {\r\nstruct ethhdr *eh = (struct ethhdr *)skb->data;\r\nif (unlikely(!pskb_may_pull(skb, ETH_HLEN)))\r\nreturn -ENOMEM;\r\nif (likely(ntohs(eh->h_proto) >= ETH_P_802_3_MIN))\r\nskb->protocol = eh->h_proto;\r\nelse\r\nskb->protocol = htons(ETH_P_802_2);\r\n} else {\r\nskb->protocol = inner_proto;\r\n}\r\nnf_reset(skb);\r\nsecpath_reset(skb);\r\nskb_clear_hash_if_not_l4(skb);\r\nskb_dst_drop(skb);\r\nskb->vlan_tci = 0;\r\nskb_set_queue_mapping(skb, 0);\r\nskb->pkt_type = PACKET_HOST;\r\nreturn 0;\r\n}\r\nstruct sk_buff *iptunnel_handle_offloads(struct sk_buff *skb,\r\nbool csum_help,\r\nint gso_type_mask)\r\n{\r\nint err;\r\nif (likely(!skb->encapsulation)) {\r\nskb_reset_inner_headers(skb);\r\nskb->encapsulation = 1;\r\n}\r\nif (skb_is_gso(skb)) {\r\nerr = skb_unclone(skb, GFP_ATOMIC);\r\nif (unlikely(err))\r\ngoto error;\r\nskb_shinfo(skb)->gso_type |= gso_type_mask;\r\nreturn skb;\r\n}\r\nif (csum_help)\r\nskb->encapsulation = 0;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL && csum_help) {\r\nerr = skb_checksum_help(skb);\r\nif (unlikely(err))\r\ngoto error;\r\n} else if (skb->ip_summed != CHECKSUM_PARTIAL)\r\nskb->ip_summed = CHECKSUM_NONE;\r\nreturn skb;\r\nerror:\r\nkfree_skb(skb);\r\nreturn ERR_PTR(err);\r\n}\r\nstruct rtnl_link_stats64 *ip_tunnel_get_stats64(struct net_device *dev,\r\nstruct rtnl_link_stats64 *tot)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nconst struct pcpu_sw_netstats *tstats =\r\nper_cpu_ptr(dev->tstats, i);\r\nu64 rx_packets, rx_bytes, tx_packets, tx_bytes;\r\nunsigned int start;\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&tstats->syncp);\r\nrx_packets = tstats->rx_packets;\r\ntx_packets = tstats->tx_packets;\r\nrx_bytes = tstats->rx_bytes;\r\ntx_bytes = tstats->tx_bytes;\r\n} while (u64_stats_fetch_retry_irq(&tstats->syncp, start));\r\ntot->rx_packets += rx_packets;\r\ntot->tx_packets += tx_packets;\r\ntot->rx_bytes += rx_bytes;\r\ntot->tx_bytes += tx_bytes;\r\n}\r\ntot->multicast = dev->stats.multicast;\r\ntot->rx_crc_errors = dev->stats.rx_crc_errors;\r\ntot->rx_fifo_errors = dev->stats.rx_fifo_errors;\r\ntot->rx_length_errors = dev->stats.rx_length_errors;\r\ntot->rx_frame_errors = dev->stats.rx_frame_errors;\r\ntot->rx_errors = dev->stats.rx_errors;\r\ntot->tx_fifo_errors = dev->stats.tx_fifo_errors;\r\ntot->tx_carrier_errors = dev->stats.tx_carrier_errors;\r\ntot->tx_dropped = dev->stats.tx_dropped;\r\ntot->tx_aborted_errors = dev->stats.tx_aborted_errors;\r\ntot->tx_errors = dev->stats.tx_errors;\r\ntot->collisions = dev->stats.collisions;\r\nreturn tot;\r\n}
