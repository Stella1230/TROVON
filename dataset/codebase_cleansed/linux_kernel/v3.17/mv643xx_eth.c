static inline u32 rdl(struct mv643xx_eth_private *mp, int offset)\r\n{\r\nreturn readl(mp->shared->base + offset);\r\n}\r\nstatic inline u32 rdlp(struct mv643xx_eth_private *mp, int offset)\r\n{\r\nreturn readl(mp->base + offset);\r\n}\r\nstatic inline void wrl(struct mv643xx_eth_private *mp, int offset, u32 data)\r\n{\r\nwritel(data, mp->shared->base + offset);\r\n}\r\nstatic inline void wrlp(struct mv643xx_eth_private *mp, int offset, u32 data)\r\n{\r\nwritel(data, mp->base + offset);\r\n}\r\nstatic struct mv643xx_eth_private *rxq_to_mp(struct rx_queue *rxq)\r\n{\r\nreturn container_of(rxq, struct mv643xx_eth_private, rxq[rxq->index]);\r\n}\r\nstatic struct mv643xx_eth_private *txq_to_mp(struct tx_queue *txq)\r\n{\r\nreturn container_of(txq, struct mv643xx_eth_private, txq[txq->index]);\r\n}\r\nstatic void rxq_enable(struct rx_queue *rxq)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nwrlp(mp, RXQ_COMMAND, 1 << rxq->index);\r\n}\r\nstatic void rxq_disable(struct rx_queue *rxq)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nu8 mask = 1 << rxq->index;\r\nwrlp(mp, RXQ_COMMAND, mask << 8);\r\nwhile (rdlp(mp, RXQ_COMMAND) & mask)\r\nudelay(10);\r\n}\r\nstatic void txq_reset_hw_ptr(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nu32 addr;\r\naddr = (u32)txq->tx_desc_dma;\r\naddr += txq->tx_curr_desc * sizeof(struct tx_desc);\r\nwrlp(mp, TXQ_CURRENT_DESC_PTR(txq->index), addr);\r\n}\r\nstatic void txq_enable(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nwrlp(mp, TXQ_COMMAND, 1 << txq->index);\r\n}\r\nstatic void txq_disable(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nu8 mask = 1 << txq->index;\r\nwrlp(mp, TXQ_COMMAND, mask << 8);\r\nwhile (rdlp(mp, TXQ_COMMAND) & mask)\r\nudelay(10);\r\n}\r\nstatic void txq_maybe_wake(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\r\nif (netif_tx_queue_stopped(nq)) {\r\n__netif_tx_lock(nq, smp_processor_id());\r\nif (txq->tx_desc_count <= txq->tx_wake_threshold)\r\nnetif_tx_wake_queue(nq);\r\n__netif_tx_unlock(nq);\r\n}\r\n}\r\nstatic int rxq_process(struct rx_queue *rxq, int budget)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nstruct net_device_stats *stats = &mp->dev->stats;\r\nint rx;\r\nrx = 0;\r\nwhile (rx < budget && rxq->rx_desc_count) {\r\nstruct rx_desc *rx_desc;\r\nunsigned int cmd_sts;\r\nstruct sk_buff *skb;\r\nu16 byte_cnt;\r\nrx_desc = &rxq->rx_desc_area[rxq->rx_curr_desc];\r\ncmd_sts = rx_desc->cmd_sts;\r\nif (cmd_sts & BUFFER_OWNED_BY_DMA)\r\nbreak;\r\nrmb();\r\nskb = rxq->rx_skb[rxq->rx_curr_desc];\r\nrxq->rx_skb[rxq->rx_curr_desc] = NULL;\r\nrxq->rx_curr_desc++;\r\nif (rxq->rx_curr_desc == rxq->rx_ring_size)\r\nrxq->rx_curr_desc = 0;\r\ndma_unmap_single(mp->dev->dev.parent, rx_desc->buf_ptr,\r\nrx_desc->buf_size, DMA_FROM_DEVICE);\r\nrxq->rx_desc_count--;\r\nrx++;\r\nmp->work_rx_refill |= 1 << rxq->index;\r\nbyte_cnt = rx_desc->byte_cnt;\r\nstats->rx_packets++;\r\nstats->rx_bytes += byte_cnt - 2;\r\nif ((cmd_sts & (RX_FIRST_DESC | RX_LAST_DESC | ERROR_SUMMARY))\r\n!= (RX_FIRST_DESC | RX_LAST_DESC))\r\ngoto err;\r\nskb_put(skb, byte_cnt - 2 - 4);\r\nif (cmd_sts & LAYER_4_CHECKSUM_OK)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb->protocol = eth_type_trans(skb, mp->dev);\r\nnapi_gro_receive(&mp->napi, skb);\r\ncontinue;\r\nerr:\r\nstats->rx_dropped++;\r\nif ((cmd_sts & (RX_FIRST_DESC | RX_LAST_DESC)) !=\r\n(RX_FIRST_DESC | RX_LAST_DESC)) {\r\nif (net_ratelimit())\r\nnetdev_err(mp->dev,\r\n"received packet spanning multiple descriptors\n");\r\n}\r\nif (cmd_sts & ERROR_SUMMARY)\r\nstats->rx_errors++;\r\ndev_kfree_skb(skb);\r\n}\r\nif (rx < budget)\r\nmp->work_rx &= ~(1 << rxq->index);\r\nreturn rx;\r\n}\r\nstatic int rxq_refill(struct rx_queue *rxq, int budget)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nint refilled;\r\nrefilled = 0;\r\nwhile (refilled < budget && rxq->rx_desc_count < rxq->rx_ring_size) {\r\nstruct sk_buff *skb;\r\nint rx;\r\nstruct rx_desc *rx_desc;\r\nint size;\r\nskb = netdev_alloc_skb(mp->dev, mp->skb_size);\r\nif (skb == NULL) {\r\nmp->oom = 1;\r\ngoto oom;\r\n}\r\nif (SKB_DMA_REALIGN)\r\nskb_reserve(skb, SKB_DMA_REALIGN);\r\nrefilled++;\r\nrxq->rx_desc_count++;\r\nrx = rxq->rx_used_desc++;\r\nif (rxq->rx_used_desc == rxq->rx_ring_size)\r\nrxq->rx_used_desc = 0;\r\nrx_desc = rxq->rx_desc_area + rx;\r\nsize = skb_end_pointer(skb) - skb->data;\r\nrx_desc->buf_ptr = dma_map_single(mp->dev->dev.parent,\r\nskb->data, size,\r\nDMA_FROM_DEVICE);\r\nrx_desc->buf_size = size;\r\nrxq->rx_skb[rx] = skb;\r\nwmb();\r\nrx_desc->cmd_sts = BUFFER_OWNED_BY_DMA | RX_ENABLE_INTERRUPT;\r\nwmb();\r\nskb_reserve(skb, 2);\r\n}\r\nif (refilled < budget)\r\nmp->work_rx_refill &= ~(1 << rxq->index);\r\noom:\r\nreturn refilled;\r\n}\r\nstatic inline unsigned int has_tiny_unaligned_frags(struct sk_buff *skb)\r\n{\r\nint frag;\r\nfor (frag = 0; frag < skb_shinfo(skb)->nr_frags; frag++) {\r\nconst skb_frag_t *fragp = &skb_shinfo(skb)->frags[frag];\r\nif (skb_frag_size(fragp) <= 8 && fragp->page_offset & 7)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline __be16 sum16_as_be(__sum16 sum)\r\n{\r\nreturn (__force __be16)sum;\r\n}\r\nstatic int skb_tx_csum(struct mv643xx_eth_private *mp, struct sk_buff *skb,\r\nu16 *l4i_chk, u32 *command, int length)\r\n{\r\nint ret;\r\nu32 cmd = 0;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nint hdr_len;\r\nint tag_bytes;\r\nBUG_ON(skb->protocol != htons(ETH_P_IP) &&\r\nskb->protocol != htons(ETH_P_8021Q));\r\nhdr_len = (void *)ip_hdr(skb) - (void *)skb->data;\r\ntag_bytes = hdr_len - ETH_HLEN;\r\nif (length - hdr_len > mp->shared->tx_csum_limit ||\r\nunlikely(tag_bytes & ~12)) {\r\nret = skb_checksum_help(skb);\r\nif (!ret)\r\ngoto no_csum;\r\nreturn ret;\r\n}\r\nif (tag_bytes & 4)\r\ncmd |= MAC_HDR_EXTRA_4_BYTES;\r\nif (tag_bytes & 8)\r\ncmd |= MAC_HDR_EXTRA_8_BYTES;\r\ncmd |= GEN_TCP_UDP_CHECKSUM | GEN_TCP_UDP_CHK_FULL |\r\nGEN_IP_V4_CHECKSUM |\r\nip_hdr(skb)->ihl << TX_IHL_SHIFT;\r\nswitch (ip_hdr(skb)->protocol) {\r\ncase IPPROTO_UDP:\r\ncmd |= UDP_FRAME;\r\n*l4i_chk = 0;\r\nbreak;\r\ncase IPPROTO_TCP:\r\n*l4i_chk = 0;\r\nbreak;\r\ndefault:\r\nWARN(1, "protocol not supported");\r\n}\r\n} else {\r\nno_csum:\r\ncmd |= 5 << TX_IHL_SHIFT;\r\n}\r\n*command = cmd;\r\nreturn 0;\r\n}\r\nstatic inline int\r\ntxq_put_data_tso(struct net_device *dev, struct tx_queue *txq,\r\nstruct sk_buff *skb, char *data, int length,\r\nbool last_tcp, bool is_last)\r\n{\r\nint tx_index;\r\nu32 cmd_sts;\r\nstruct tx_desc *desc;\r\ntx_index = txq->tx_curr_desc++;\r\nif (txq->tx_curr_desc == txq->tx_ring_size)\r\ntxq->tx_curr_desc = 0;\r\ndesc = &txq->tx_desc_area[tx_index];\r\ndesc->l4i_chk = 0;\r\ndesc->byte_cnt = length;\r\ndesc->buf_ptr = dma_map_single(dev->dev.parent, data,\r\nlength, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev->dev.parent, desc->buf_ptr))) {\r\nWARN(1, "dma_map_single failed!\n");\r\nreturn -ENOMEM;\r\n}\r\ncmd_sts = BUFFER_OWNED_BY_DMA;\r\nif (last_tcp) {\r\ncmd_sts |= ZERO_PADDING | TX_LAST_DESC;\r\nif (is_last)\r\ncmd_sts |= TX_ENABLE_INTERRUPT;\r\n}\r\ndesc->cmd_sts = cmd_sts;\r\nreturn 0;\r\n}\r\nstatic inline void\r\ntxq_put_hdr_tso(struct sk_buff *skb, struct tx_queue *txq, int length)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nint tx_index;\r\nstruct tx_desc *desc;\r\nint ret;\r\nu32 cmd_csum = 0;\r\nu16 l4i_chk = 0;\r\ntx_index = txq->tx_curr_desc;\r\ndesc = &txq->tx_desc_area[tx_index];\r\nret = skb_tx_csum(mp, skb, &l4i_chk, &cmd_csum, length);\r\nif (ret)\r\nWARN(1, "failed to prepare checksum!");\r\ndesc->l4i_chk = 0;\r\ndesc->byte_cnt = hdr_len;\r\ndesc->buf_ptr = txq->tso_hdrs_dma +\r\ntxq->tx_curr_desc * TSO_HEADER_SIZE;\r\ndesc->cmd_sts = cmd_csum | BUFFER_OWNED_BY_DMA | TX_FIRST_DESC |\r\nGEN_CRC;\r\ntxq->tx_curr_desc++;\r\nif (txq->tx_curr_desc == txq->tx_ring_size)\r\ntxq->tx_curr_desc = 0;\r\n}\r\nstatic int txq_submit_tso(struct tx_queue *txq, struct sk_buff *skb,\r\nstruct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint total_len, data_left, ret;\r\nint desc_count = 0;\r\nstruct tso_t tso;\r\nint hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nif ((txq->tx_desc_count + tso_count_descs(skb)) >= txq->tx_ring_size) {\r\nnetdev_dbg(dev, "not enough descriptors for TSO!\n");\r\nreturn -EBUSY;\r\n}\r\ntso_start(skb, &tso);\r\ntotal_len = skb->len - hdr_len;\r\nwhile (total_len > 0) {\r\nchar *hdr;\r\ndata_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);\r\ntotal_len -= data_left;\r\ndesc_count++;\r\nhdr = txq->tso_hdrs + txq->tx_curr_desc * TSO_HEADER_SIZE;\r\ntso_build_hdr(skb, hdr, &tso, data_left, total_len == 0);\r\ntxq_put_hdr_tso(skb, txq, data_left);\r\nwhile (data_left > 0) {\r\nint size;\r\ndesc_count++;\r\nsize = min_t(int, tso.size, data_left);\r\nret = txq_put_data_tso(dev, txq, skb, tso.data, size,\r\nsize == data_left,\r\ntotal_len == 0);\r\nif (ret)\r\ngoto err_release;\r\ndata_left -= size;\r\ntso_build_data(skb, &tso, size);\r\n}\r\n}\r\n__skb_queue_tail(&txq->tx_skb, skb);\r\nskb_tx_timestamp(skb);\r\nmp->work_tx_end &= ~(1 << txq->index);\r\nwmb();\r\ntxq_enable(txq);\r\ntxq->tx_desc_count += desc_count;\r\nreturn 0;\r\nerr_release:\r\nreturn ret;\r\n}\r\nstatic void txq_submit_frag_skb(struct tx_queue *txq, struct sk_buff *skb)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint nr_frags = skb_shinfo(skb)->nr_frags;\r\nint frag;\r\nfor (frag = 0; frag < nr_frags; frag++) {\r\nskb_frag_t *this_frag;\r\nint tx_index;\r\nstruct tx_desc *desc;\r\nvoid *addr;\r\nthis_frag = &skb_shinfo(skb)->frags[frag];\r\naddr = page_address(this_frag->page.p) + this_frag->page_offset;\r\ntx_index = txq->tx_curr_desc++;\r\nif (txq->tx_curr_desc == txq->tx_ring_size)\r\ntxq->tx_curr_desc = 0;\r\ndesc = &txq->tx_desc_area[tx_index];\r\nif (frag == nr_frags - 1) {\r\ndesc->cmd_sts = BUFFER_OWNED_BY_DMA |\r\nZERO_PADDING | TX_LAST_DESC |\r\nTX_ENABLE_INTERRUPT;\r\n} else {\r\ndesc->cmd_sts = BUFFER_OWNED_BY_DMA;\r\n}\r\ndesc->l4i_chk = 0;\r\ndesc->byte_cnt = skb_frag_size(this_frag);\r\ndesc->buf_ptr = dma_map_single(mp->dev->dev.parent, addr,\r\ndesc->byte_cnt, DMA_TO_DEVICE);\r\n}\r\n}\r\nstatic int txq_submit_skb(struct tx_queue *txq, struct sk_buff *skb,\r\nstruct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint nr_frags = skb_shinfo(skb)->nr_frags;\r\nint tx_index;\r\nstruct tx_desc *desc;\r\nu32 cmd_sts;\r\nu16 l4i_chk;\r\nint length, ret;\r\ncmd_sts = 0;\r\nl4i_chk = 0;\r\nif (txq->tx_ring_size - txq->tx_desc_count < MAX_SKB_FRAGS + 1) {\r\nif (net_ratelimit())\r\nnetdev_err(dev, "tx queue full?!\n");\r\nreturn -EBUSY;\r\n}\r\nret = skb_tx_csum(mp, skb, &l4i_chk, &cmd_sts, skb->len);\r\nif (ret)\r\nreturn ret;\r\ncmd_sts |= TX_FIRST_DESC | GEN_CRC | BUFFER_OWNED_BY_DMA;\r\ntx_index = txq->tx_curr_desc++;\r\nif (txq->tx_curr_desc == txq->tx_ring_size)\r\ntxq->tx_curr_desc = 0;\r\ndesc = &txq->tx_desc_area[tx_index];\r\nif (nr_frags) {\r\ntxq_submit_frag_skb(txq, skb);\r\nlength = skb_headlen(skb);\r\n} else {\r\ncmd_sts |= ZERO_PADDING | TX_LAST_DESC | TX_ENABLE_INTERRUPT;\r\nlength = skb->len;\r\n}\r\ndesc->l4i_chk = l4i_chk;\r\ndesc->byte_cnt = length;\r\ndesc->buf_ptr = dma_map_single(mp->dev->dev.parent, skb->data,\r\nlength, DMA_TO_DEVICE);\r\n__skb_queue_tail(&txq->tx_skb, skb);\r\nskb_tx_timestamp(skb);\r\nwmb();\r\ndesc->cmd_sts = cmd_sts;\r\nmp->work_tx_end &= ~(1 << txq->index);\r\nwmb();\r\ntxq_enable(txq);\r\ntxq->tx_desc_count += nr_frags + 1;\r\nreturn 0;\r\n}\r\nstatic netdev_tx_t mv643xx_eth_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint length, queue, ret;\r\nstruct tx_queue *txq;\r\nstruct netdev_queue *nq;\r\nqueue = skb_get_queue_mapping(skb);\r\ntxq = mp->txq + queue;\r\nnq = netdev_get_tx_queue(dev, queue);\r\nif (has_tiny_unaligned_frags(skb) && __skb_linearize(skb)) {\r\nnetdev_printk(KERN_DEBUG, dev,\r\n"failed to linearize skb with tiny unaligned fragment\n");\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nlength = skb->len;\r\nif (skb_is_gso(skb))\r\nret = txq_submit_tso(txq, skb, dev);\r\nelse\r\nret = txq_submit_skb(txq, skb, dev);\r\nif (!ret) {\r\ntxq->tx_bytes += length;\r\ntxq->tx_packets++;\r\nif (txq->tx_desc_count >= txq->tx_stop_threshold)\r\nnetif_tx_stop_queue(nq);\r\n} else {\r\ntxq->tx_dropped++;\r\ndev_kfree_skb_any(skb);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void txq_kick(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\r\nu32 hw_desc_ptr;\r\nu32 expected_ptr;\r\n__netif_tx_lock(nq, smp_processor_id());\r\nif (rdlp(mp, TXQ_COMMAND) & (1 << txq->index))\r\ngoto out;\r\nhw_desc_ptr = rdlp(mp, TXQ_CURRENT_DESC_PTR(txq->index));\r\nexpected_ptr = (u32)txq->tx_desc_dma +\r\ntxq->tx_curr_desc * sizeof(struct tx_desc);\r\nif (hw_desc_ptr != expected_ptr)\r\ntxq_enable(txq);\r\nout:\r\n__netif_tx_unlock(nq);\r\nmp->work_tx_end &= ~(1 << txq->index);\r\n}\r\nstatic int txq_reclaim(struct tx_queue *txq, int budget, int force)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\r\nint reclaimed;\r\n__netif_tx_lock_bh(nq);\r\nreclaimed = 0;\r\nwhile (reclaimed < budget && txq->tx_desc_count > 0) {\r\nint tx_index;\r\nstruct tx_desc *desc;\r\nu32 cmd_sts;\r\nstruct sk_buff *skb;\r\ntx_index = txq->tx_used_desc;\r\ndesc = &txq->tx_desc_area[tx_index];\r\ncmd_sts = desc->cmd_sts;\r\nif (cmd_sts & BUFFER_OWNED_BY_DMA) {\r\nif (!force)\r\nbreak;\r\ndesc->cmd_sts = cmd_sts & ~BUFFER_OWNED_BY_DMA;\r\n}\r\ntxq->tx_used_desc = tx_index + 1;\r\nif (txq->tx_used_desc == txq->tx_ring_size)\r\ntxq->tx_used_desc = 0;\r\nreclaimed++;\r\ntxq->tx_desc_count--;\r\nskb = NULL;\r\nif (cmd_sts & TX_LAST_DESC)\r\nskb = __skb_dequeue(&txq->tx_skb);\r\nif (cmd_sts & ERROR_SUMMARY) {\r\nnetdev_info(mp->dev, "tx error\n");\r\nmp->dev->stats.tx_errors++;\r\n}\r\nif (!IS_TSO_HEADER(txq, desc->buf_ptr))\r\ndma_unmap_single(mp->dev->dev.parent, desc->buf_ptr,\r\ndesc->byte_cnt, DMA_TO_DEVICE);\r\ndev_kfree_skb(skb);\r\n}\r\n__netif_tx_unlock_bh(nq);\r\nif (reclaimed < budget)\r\nmp->work_tx &= ~(1 << txq->index);\r\nreturn reclaimed;\r\n}\r\nstatic void tx_set_rate(struct mv643xx_eth_private *mp, int rate, int burst)\r\n{\r\nint token_rate;\r\nint mtu;\r\nint bucket_size;\r\ntoken_rate = ((rate / 1000) * 64) / (mp->t_clk / 1000);\r\nif (token_rate > 1023)\r\ntoken_rate = 1023;\r\nmtu = (mp->dev->mtu + 255) >> 8;\r\nif (mtu > 63)\r\nmtu = 63;\r\nbucket_size = (burst + 255) >> 8;\r\nif (bucket_size > 65535)\r\nbucket_size = 65535;\r\nswitch (mp->shared->tx_bw_control) {\r\ncase TX_BW_CONTROL_OLD_LAYOUT:\r\nwrlp(mp, TX_BW_RATE, token_rate);\r\nwrlp(mp, TX_BW_MTU, mtu);\r\nwrlp(mp, TX_BW_BURST, bucket_size);\r\nbreak;\r\ncase TX_BW_CONTROL_NEW_LAYOUT:\r\nwrlp(mp, TX_BW_RATE_MOVED, token_rate);\r\nwrlp(mp, TX_BW_MTU_MOVED, mtu);\r\nwrlp(mp, TX_BW_BURST_MOVED, bucket_size);\r\nbreak;\r\n}\r\n}\r\nstatic void txq_set_rate(struct tx_queue *txq, int rate, int burst)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint token_rate;\r\nint bucket_size;\r\ntoken_rate = ((rate / 1000) * 64) / (mp->t_clk / 1000);\r\nif (token_rate > 1023)\r\ntoken_rate = 1023;\r\nbucket_size = (burst + 255) >> 8;\r\nif (bucket_size > 65535)\r\nbucket_size = 65535;\r\nwrlp(mp, TXQ_BW_TOKENS(txq->index), token_rate << 14);\r\nwrlp(mp, TXQ_BW_CONF(txq->index), (bucket_size << 10) | token_rate);\r\n}\r\nstatic void txq_set_fixed_prio_mode(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint off;\r\nu32 val;\r\noff = 0;\r\nswitch (mp->shared->tx_bw_control) {\r\ncase TX_BW_CONTROL_OLD_LAYOUT:\r\noff = TXQ_FIX_PRIO_CONF;\r\nbreak;\r\ncase TX_BW_CONTROL_NEW_LAYOUT:\r\noff = TXQ_FIX_PRIO_CONF_MOVED;\r\nbreak;\r\n}\r\nif (off) {\r\nval = rdlp(mp, off);\r\nval |= 1 << txq->index;\r\nwrlp(mp, off, val);\r\n}\r\n}\r\nstatic void mv643xx_eth_adjust_link(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nu32 pscr = rdlp(mp, PORT_SERIAL_CONTROL);\r\nu32 autoneg_disable = FORCE_LINK_PASS |\r\nDISABLE_AUTO_NEG_SPEED_GMII |\r\nDISABLE_AUTO_NEG_FOR_FLOW_CTRL |\r\nDISABLE_AUTO_NEG_FOR_DUPLEX;\r\nif (mp->phy->autoneg == AUTONEG_ENABLE) {\r\npscr &= ~autoneg_disable;\r\ngoto out_write;\r\n}\r\npscr |= autoneg_disable;\r\nif (mp->phy->speed == SPEED_1000) {\r\npscr |= SET_GMII_SPEED_TO_1000;\r\npscr |= SET_FULL_DUPLEX_MODE;\r\ngoto out_write;\r\n}\r\npscr &= ~SET_GMII_SPEED_TO_1000;\r\nif (mp->phy->speed == SPEED_100)\r\npscr |= SET_MII_SPEED_TO_100;\r\nelse\r\npscr &= ~SET_MII_SPEED_TO_100;\r\nif (mp->phy->duplex == DUPLEX_FULL)\r\npscr |= SET_FULL_DUPLEX_MODE;\r\nelse\r\npscr &= ~SET_FULL_DUPLEX_MODE;\r\nout_write:\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\n}\r\nstatic struct net_device_stats *mv643xx_eth_get_stats(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nstruct net_device_stats *stats = &dev->stats;\r\nunsigned long tx_packets = 0;\r\nunsigned long tx_bytes = 0;\r\nunsigned long tx_dropped = 0;\r\nint i;\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nstruct tx_queue *txq = mp->txq + i;\r\ntx_packets += txq->tx_packets;\r\ntx_bytes += txq->tx_bytes;\r\ntx_dropped += txq->tx_dropped;\r\n}\r\nstats->tx_packets = tx_packets;\r\nstats->tx_bytes = tx_bytes;\r\nstats->tx_dropped = tx_dropped;\r\nreturn stats;\r\n}\r\nstatic inline u32 mib_read(struct mv643xx_eth_private *mp, int offset)\r\n{\r\nreturn rdl(mp, MIB_COUNTERS(mp->port_num) + offset);\r\n}\r\nstatic void mib_counters_clear(struct mv643xx_eth_private *mp)\r\n{\r\nint i;\r\nfor (i = 0; i < 0x80; i += 4)\r\nmib_read(mp, i);\r\nrdlp(mp, RX_DISCARD_FRAME_CNT);\r\nrdlp(mp, RX_OVERRUN_FRAME_CNT);\r\n}\r\nstatic void mib_counters_update(struct mv643xx_eth_private *mp)\r\n{\r\nstruct mib_counters *p = &mp->mib_counters;\r\nspin_lock_bh(&mp->mib_counters_lock);\r\np->good_octets_received += mib_read(mp, 0x00);\r\np->bad_octets_received += mib_read(mp, 0x08);\r\np->internal_mac_transmit_err += mib_read(mp, 0x0c);\r\np->good_frames_received += mib_read(mp, 0x10);\r\np->bad_frames_received += mib_read(mp, 0x14);\r\np->broadcast_frames_received += mib_read(mp, 0x18);\r\np->multicast_frames_received += mib_read(mp, 0x1c);\r\np->frames_64_octets += mib_read(mp, 0x20);\r\np->frames_65_to_127_octets += mib_read(mp, 0x24);\r\np->frames_128_to_255_octets += mib_read(mp, 0x28);\r\np->frames_256_to_511_octets += mib_read(mp, 0x2c);\r\np->frames_512_to_1023_octets += mib_read(mp, 0x30);\r\np->frames_1024_to_max_octets += mib_read(mp, 0x34);\r\np->good_octets_sent += mib_read(mp, 0x38);\r\np->good_frames_sent += mib_read(mp, 0x40);\r\np->excessive_collision += mib_read(mp, 0x44);\r\np->multicast_frames_sent += mib_read(mp, 0x48);\r\np->broadcast_frames_sent += mib_read(mp, 0x4c);\r\np->unrec_mac_control_received += mib_read(mp, 0x50);\r\np->fc_sent += mib_read(mp, 0x54);\r\np->good_fc_received += mib_read(mp, 0x58);\r\np->bad_fc_received += mib_read(mp, 0x5c);\r\np->undersize_received += mib_read(mp, 0x60);\r\np->fragments_received += mib_read(mp, 0x64);\r\np->oversize_received += mib_read(mp, 0x68);\r\np->jabber_received += mib_read(mp, 0x6c);\r\np->mac_receive_error += mib_read(mp, 0x70);\r\np->bad_crc_event += mib_read(mp, 0x74);\r\np->collision += mib_read(mp, 0x78);\r\np->late_collision += mib_read(mp, 0x7c);\r\np->rx_discard += rdlp(mp, RX_DISCARD_FRAME_CNT);\r\np->rx_overrun += rdlp(mp, RX_OVERRUN_FRAME_CNT);\r\nspin_unlock_bh(&mp->mib_counters_lock);\r\n}\r\nstatic void mib_counters_timer_wrapper(unsigned long _mp)\r\n{\r\nstruct mv643xx_eth_private *mp = (void *)_mp;\r\nmib_counters_update(mp);\r\nmod_timer(&mp->mib_counters_timer, jiffies + 30 * HZ);\r\n}\r\nstatic unsigned int get_rx_coal(struct mv643xx_eth_private *mp)\r\n{\r\nu32 val = rdlp(mp, SDMA_CONFIG);\r\nu64 temp;\r\nif (mp->shared->extended_rx_coal_limit)\r\ntemp = ((val & 0x02000000) >> 10) | ((val & 0x003fff80) >> 7);\r\nelse\r\ntemp = (val & 0x003fff00) >> 8;\r\ntemp *= 64000000;\r\ndo_div(temp, mp->t_clk);\r\nreturn (unsigned int)temp;\r\n}\r\nstatic void set_rx_coal(struct mv643xx_eth_private *mp, unsigned int usec)\r\n{\r\nu64 temp;\r\nu32 val;\r\ntemp = (u64)usec * mp->t_clk;\r\ntemp += 31999999;\r\ndo_div(temp, 64000000);\r\nval = rdlp(mp, SDMA_CONFIG);\r\nif (mp->shared->extended_rx_coal_limit) {\r\nif (temp > 0xffff)\r\ntemp = 0xffff;\r\nval &= ~0x023fff80;\r\nval |= (temp & 0x8000) << 10;\r\nval |= (temp & 0x7fff) << 7;\r\n} else {\r\nif (temp > 0x3fff)\r\ntemp = 0x3fff;\r\nval &= ~0x003fff00;\r\nval |= (temp & 0x3fff) << 8;\r\n}\r\nwrlp(mp, SDMA_CONFIG, val);\r\n}\r\nstatic unsigned int get_tx_coal(struct mv643xx_eth_private *mp)\r\n{\r\nu64 temp;\r\ntemp = (rdlp(mp, TX_FIFO_URGENT_THRESHOLD) & 0x3fff0) >> 4;\r\ntemp *= 64000000;\r\ndo_div(temp, mp->t_clk);\r\nreturn (unsigned int)temp;\r\n}\r\nstatic void set_tx_coal(struct mv643xx_eth_private *mp, unsigned int usec)\r\n{\r\nu64 temp;\r\ntemp = (u64)usec * mp->t_clk;\r\ntemp += 31999999;\r\ndo_div(temp, 64000000);\r\nif (temp > 0x3fff)\r\ntemp = 0x3fff;\r\nwrlp(mp, TX_FIFO_URGENT_THRESHOLD, temp << 4);\r\n}\r\nstatic int\r\nmv643xx_eth_get_settings_phy(struct mv643xx_eth_private *mp,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nint err;\r\nerr = phy_read_status(mp->phy);\r\nif (err == 0)\r\nerr = phy_ethtool_gset(mp->phy, cmd);\r\ncmd->supported &= ~SUPPORTED_1000baseT_Half;\r\ncmd->advertising &= ~ADVERTISED_1000baseT_Half;\r\nreturn err;\r\n}\r\nstatic int\r\nmv643xx_eth_get_settings_phyless(struct mv643xx_eth_private *mp,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nu32 port_status;\r\nport_status = rdlp(mp, PORT_STATUS);\r\ncmd->supported = SUPPORTED_MII;\r\ncmd->advertising = ADVERTISED_MII;\r\nswitch (port_status & PORT_SPEED_MASK) {\r\ncase PORT_SPEED_10:\r\nethtool_cmd_speed_set(cmd, SPEED_10);\r\nbreak;\r\ncase PORT_SPEED_100:\r\nethtool_cmd_speed_set(cmd, SPEED_100);\r\nbreak;\r\ncase PORT_SPEED_1000:\r\nethtool_cmd_speed_set(cmd, SPEED_1000);\r\nbreak;\r\ndefault:\r\ncmd->speed = -1;\r\nbreak;\r\n}\r\ncmd->duplex = (port_status & FULL_DUPLEX) ? DUPLEX_FULL : DUPLEX_HALF;\r\ncmd->port = PORT_MII;\r\ncmd->phy_address = 0;\r\ncmd->transceiver = XCVR_INTERNAL;\r\ncmd->autoneg = AUTONEG_DISABLE;\r\ncmd->maxtxpkt = 1;\r\ncmd->maxrxpkt = 1;\r\nreturn 0;\r\n}\r\nstatic void\r\nmv643xx_eth_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nwol->supported = 0;\r\nwol->wolopts = 0;\r\nif (mp->phy)\r\nphy_ethtool_get_wol(mp->phy, wol);\r\n}\r\nstatic int\r\nmv643xx_eth_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint err;\r\nif (mp->phy == NULL)\r\nreturn -EOPNOTSUPP;\r\nerr = phy_ethtool_set_wol(mp->phy, wol);\r\nif (err == -EOPNOTSUPP)\r\nnetdev_info(dev, "The PHY does not support set_wol, was CONFIG_MARVELL_PHY enabled?\n");\r\nreturn err;\r\n}\r\nstatic int\r\nmv643xx_eth_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (mp->phy != NULL)\r\nreturn mv643xx_eth_get_settings_phy(mp, cmd);\r\nelse\r\nreturn mv643xx_eth_get_settings_phyless(mp, cmd);\r\n}\r\nstatic int\r\nmv643xx_eth_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint ret;\r\nif (mp->phy == NULL)\r\nreturn -EINVAL;\r\ncmd->advertising &= ~ADVERTISED_1000baseT_Half;\r\nret = phy_ethtool_sset(mp->phy, cmd);\r\nif (!ret)\r\nmv643xx_eth_adjust_link(dev);\r\nreturn ret;\r\n}\r\nstatic void mv643xx_eth_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *drvinfo)\r\n{\r\nstrlcpy(drvinfo->driver, mv643xx_eth_driver_name,\r\nsizeof(drvinfo->driver));\r\nstrlcpy(drvinfo->version, mv643xx_eth_driver_version,\r\nsizeof(drvinfo->version));\r\nstrlcpy(drvinfo->fw_version, "N/A", sizeof(drvinfo->fw_version));\r\nstrlcpy(drvinfo->bus_info, "platform", sizeof(drvinfo->bus_info));\r\ndrvinfo->n_stats = ARRAY_SIZE(mv643xx_eth_stats);\r\n}\r\nstatic int mv643xx_eth_nway_reset(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (mp->phy == NULL)\r\nreturn -EINVAL;\r\nreturn genphy_restart_aneg(mp->phy);\r\n}\r\nstatic int\r\nmv643xx_eth_get_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nec->rx_coalesce_usecs = get_rx_coal(mp);\r\nec->tx_coalesce_usecs = get_tx_coal(mp);\r\nreturn 0;\r\n}\r\nstatic int\r\nmv643xx_eth_set_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nset_rx_coal(mp, ec->rx_coalesce_usecs);\r\nset_tx_coal(mp, ec->tx_coalesce_usecs);\r\nreturn 0;\r\n}\r\nstatic void\r\nmv643xx_eth_get_ringparam(struct net_device *dev, struct ethtool_ringparam *er)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\ner->rx_max_pending = 4096;\r\ner->tx_max_pending = 4096;\r\ner->rx_pending = mp->rx_ring_size;\r\ner->tx_pending = mp->tx_ring_size;\r\n}\r\nstatic int\r\nmv643xx_eth_set_ringparam(struct net_device *dev, struct ethtool_ringparam *er)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (er->rx_mini_pending || er->rx_jumbo_pending)\r\nreturn -EINVAL;\r\nmp->rx_ring_size = er->rx_pending < 4096 ? er->rx_pending : 4096;\r\nmp->tx_ring_size = clamp_t(unsigned int, er->tx_pending,\r\nMV643XX_MAX_SKB_DESCS * 2, 4096);\r\nif (mp->tx_ring_size != er->tx_pending)\r\nnetdev_warn(dev, "TX queue size set to %u (requested %u)\n",\r\nmp->tx_ring_size, er->tx_pending);\r\nif (netif_running(dev)) {\r\nmv643xx_eth_stop(dev);\r\nif (mv643xx_eth_open(dev)) {\r\nnetdev_err(dev,\r\n"fatal error on re-opening device after ring param change\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nmv643xx_eth_set_features(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nbool rx_csum = features & NETIF_F_RXCSUM;\r\nwrlp(mp, PORT_CONFIG, rx_csum ? 0x02000000 : 0x00000000);\r\nreturn 0;\r\n}\r\nstatic void mv643xx_eth_get_strings(struct net_device *dev,\r\nuint32_t stringset, uint8_t *data)\r\n{\r\nint i;\r\nif (stringset == ETH_SS_STATS) {\r\nfor (i = 0; i < ARRAY_SIZE(mv643xx_eth_stats); i++) {\r\nmemcpy(data + i * ETH_GSTRING_LEN,\r\nmv643xx_eth_stats[i].stat_string,\r\nETH_GSTRING_LEN);\r\n}\r\n}\r\n}\r\nstatic void mv643xx_eth_get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *stats,\r\nuint64_t *data)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint i;\r\nmv643xx_eth_get_stats(dev);\r\nmib_counters_update(mp);\r\nfor (i = 0; i < ARRAY_SIZE(mv643xx_eth_stats); i++) {\r\nconst struct mv643xx_eth_stats *stat;\r\nvoid *p;\r\nstat = mv643xx_eth_stats + i;\r\nif (stat->netdev_off >= 0)\r\np = ((void *)mp->dev) + stat->netdev_off;\r\nelse\r\np = ((void *)mp) + stat->mp_off;\r\ndata[i] = (stat->sizeof_stat == 8) ?\r\n*(uint64_t *)p : *(uint32_t *)p;\r\n}\r\n}\r\nstatic int mv643xx_eth_get_sset_count(struct net_device *dev, int sset)\r\n{\r\nif (sset == ETH_SS_STATS)\r\nreturn ARRAY_SIZE(mv643xx_eth_stats);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic void uc_addr_get(struct mv643xx_eth_private *mp, unsigned char *addr)\r\n{\r\nunsigned int mac_h = rdlp(mp, MAC_ADDR_HIGH);\r\nunsigned int mac_l = rdlp(mp, MAC_ADDR_LOW);\r\naddr[0] = (mac_h >> 24) & 0xff;\r\naddr[1] = (mac_h >> 16) & 0xff;\r\naddr[2] = (mac_h >> 8) & 0xff;\r\naddr[3] = mac_h & 0xff;\r\naddr[4] = (mac_l >> 8) & 0xff;\r\naddr[5] = mac_l & 0xff;\r\n}\r\nstatic void uc_addr_set(struct mv643xx_eth_private *mp, unsigned char *addr)\r\n{\r\nwrlp(mp, MAC_ADDR_HIGH,\r\n(addr[0] << 24) | (addr[1] << 16) | (addr[2] << 8) | addr[3]);\r\nwrlp(mp, MAC_ADDR_LOW, (addr[4] << 8) | addr[5]);\r\n}\r\nstatic u32 uc_addr_filter_mask(struct net_device *dev)\r\n{\r\nstruct netdev_hw_addr *ha;\r\nu32 nibbles;\r\nif (dev->flags & IFF_PROMISC)\r\nreturn 0;\r\nnibbles = 1 << (dev->dev_addr[5] & 0x0f);\r\nnetdev_for_each_uc_addr(ha, dev) {\r\nif (memcmp(dev->dev_addr, ha->addr, 5))\r\nreturn 0;\r\nif ((dev->dev_addr[5] ^ ha->addr[5]) & 0xf0)\r\nreturn 0;\r\nnibbles |= 1 << (ha->addr[5] & 0x0f);\r\n}\r\nreturn nibbles;\r\n}\r\nstatic void mv643xx_eth_program_unicast_filter(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nu32 port_config;\r\nu32 nibbles;\r\nint i;\r\nuc_addr_set(mp, dev->dev_addr);\r\nport_config = rdlp(mp, PORT_CONFIG) & ~UNICAST_PROMISCUOUS_MODE;\r\nnibbles = uc_addr_filter_mask(dev);\r\nif (!nibbles) {\r\nport_config |= UNICAST_PROMISCUOUS_MODE;\r\nnibbles = 0xffff;\r\n}\r\nfor (i = 0; i < 16; i += 4) {\r\nint off = UNICAST_TABLE(mp->port_num) + i;\r\nu32 v;\r\nv = 0;\r\nif (nibbles & 1)\r\nv |= 0x00000001;\r\nif (nibbles & 2)\r\nv |= 0x00000100;\r\nif (nibbles & 4)\r\nv |= 0x00010000;\r\nif (nibbles & 8)\r\nv |= 0x01000000;\r\nnibbles >>= 4;\r\nwrl(mp, off, v);\r\n}\r\nwrlp(mp, PORT_CONFIG, port_config);\r\n}\r\nstatic int addr_crc(unsigned char *addr)\r\n{\r\nint crc = 0;\r\nint i;\r\nfor (i = 0; i < 6; i++) {\r\nint j;\r\ncrc = (crc ^ addr[i]) << 8;\r\nfor (j = 7; j >= 0; j--) {\r\nif (crc & (0x100 << j))\r\ncrc ^= 0x107 << j;\r\n}\r\n}\r\nreturn crc;\r\n}\r\nstatic void mv643xx_eth_program_multicast_filter(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nu32 *mc_spec;\r\nu32 *mc_other;\r\nstruct netdev_hw_addr *ha;\r\nint i;\r\nif (dev->flags & (IFF_PROMISC | IFF_ALLMULTI)) {\r\nint port_num;\r\nu32 accept;\r\noom:\r\nport_num = mp->port_num;\r\naccept = 0x01010101;\r\nfor (i = 0; i < 0x100; i += 4) {\r\nwrl(mp, SPECIAL_MCAST_TABLE(port_num) + i, accept);\r\nwrl(mp, OTHER_MCAST_TABLE(port_num) + i, accept);\r\n}\r\nreturn;\r\n}\r\nmc_spec = kmalloc(0x200, GFP_ATOMIC);\r\nif (mc_spec == NULL)\r\ngoto oom;\r\nmc_other = mc_spec + (0x100 >> 2);\r\nmemset(mc_spec, 0, 0x100);\r\nmemset(mc_other, 0, 0x100);\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nu8 *a = ha->addr;\r\nu32 *table;\r\nint entry;\r\nif (memcmp(a, "\x01\x00\x5e\x00\x00", 5) == 0) {\r\ntable = mc_spec;\r\nentry = a[5];\r\n} else {\r\ntable = mc_other;\r\nentry = addr_crc(a);\r\n}\r\ntable[entry >> 2] |= 1 << (8 * (entry & 3));\r\n}\r\nfor (i = 0; i < 0x100; i += 4) {\r\nwrl(mp, SPECIAL_MCAST_TABLE(mp->port_num) + i, mc_spec[i >> 2]);\r\nwrl(mp, OTHER_MCAST_TABLE(mp->port_num) + i, mc_other[i >> 2]);\r\n}\r\nkfree(mc_spec);\r\n}\r\nstatic void mv643xx_eth_set_rx_mode(struct net_device *dev)\r\n{\r\nmv643xx_eth_program_unicast_filter(dev);\r\nmv643xx_eth_program_multicast_filter(dev);\r\n}\r\nstatic int mv643xx_eth_set_mac_address(struct net_device *dev, void *addr)\r\n{\r\nstruct sockaddr *sa = addr;\r\nif (!is_valid_ether_addr(sa->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);\r\nnetif_addr_lock_bh(dev);\r\nmv643xx_eth_program_unicast_filter(dev);\r\nnetif_addr_unlock_bh(dev);\r\nreturn 0;\r\n}\r\nstatic int rxq_init(struct mv643xx_eth_private *mp, int index)\r\n{\r\nstruct rx_queue *rxq = mp->rxq + index;\r\nstruct rx_desc *rx_desc;\r\nint size;\r\nint i;\r\nrxq->index = index;\r\nrxq->rx_ring_size = mp->rx_ring_size;\r\nrxq->rx_desc_count = 0;\r\nrxq->rx_curr_desc = 0;\r\nrxq->rx_used_desc = 0;\r\nsize = rxq->rx_ring_size * sizeof(struct rx_desc);\r\nif (index == 0 && size <= mp->rx_desc_sram_size) {\r\nrxq->rx_desc_area = ioremap(mp->rx_desc_sram_addr,\r\nmp->rx_desc_sram_size);\r\nrxq->rx_desc_dma = mp->rx_desc_sram_addr;\r\n} else {\r\nrxq->rx_desc_area = dma_alloc_coherent(mp->dev->dev.parent,\r\nsize, &rxq->rx_desc_dma,\r\nGFP_KERNEL);\r\n}\r\nif (rxq->rx_desc_area == NULL) {\r\nnetdev_err(mp->dev,\r\n"can't allocate rx ring (%d bytes)\n", size);\r\ngoto out;\r\n}\r\nmemset(rxq->rx_desc_area, 0, size);\r\nrxq->rx_desc_area_size = size;\r\nrxq->rx_skb = kcalloc(rxq->rx_ring_size, sizeof(*rxq->rx_skb),\r\nGFP_KERNEL);\r\nif (rxq->rx_skb == NULL)\r\ngoto out_free;\r\nrx_desc = rxq->rx_desc_area;\r\nfor (i = 0; i < rxq->rx_ring_size; i++) {\r\nint nexti;\r\nnexti = i + 1;\r\nif (nexti == rxq->rx_ring_size)\r\nnexti = 0;\r\nrx_desc[i].next_desc_ptr = rxq->rx_desc_dma +\r\nnexti * sizeof(struct rx_desc);\r\n}\r\nreturn 0;\r\nout_free:\r\nif (index == 0 && size <= mp->rx_desc_sram_size)\r\niounmap(rxq->rx_desc_area);\r\nelse\r\ndma_free_coherent(mp->dev->dev.parent, size,\r\nrxq->rx_desc_area,\r\nrxq->rx_desc_dma);\r\nout:\r\nreturn -ENOMEM;\r\n}\r\nstatic void rxq_deinit(struct rx_queue *rxq)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nint i;\r\nrxq_disable(rxq);\r\nfor (i = 0; i < rxq->rx_ring_size; i++) {\r\nif (rxq->rx_skb[i]) {\r\ndev_kfree_skb(rxq->rx_skb[i]);\r\nrxq->rx_desc_count--;\r\n}\r\n}\r\nif (rxq->rx_desc_count) {\r\nnetdev_err(mp->dev, "error freeing rx ring -- %d skbs stuck\n",\r\nrxq->rx_desc_count);\r\n}\r\nif (rxq->index == 0 &&\r\nrxq->rx_desc_area_size <= mp->rx_desc_sram_size)\r\niounmap(rxq->rx_desc_area);\r\nelse\r\ndma_free_coherent(mp->dev->dev.parent, rxq->rx_desc_area_size,\r\nrxq->rx_desc_area, rxq->rx_desc_dma);\r\nkfree(rxq->rx_skb);\r\n}\r\nstatic int txq_init(struct mv643xx_eth_private *mp, int index)\r\n{\r\nstruct tx_queue *txq = mp->txq + index;\r\nstruct tx_desc *tx_desc;\r\nint size;\r\nint i;\r\ntxq->index = index;\r\ntxq->tx_ring_size = mp->tx_ring_size;\r\ntxq->tx_stop_threshold = txq->tx_ring_size - MV643XX_MAX_SKB_DESCS;\r\ntxq->tx_wake_threshold = txq->tx_stop_threshold / 2;\r\ntxq->tx_desc_count = 0;\r\ntxq->tx_curr_desc = 0;\r\ntxq->tx_used_desc = 0;\r\nsize = txq->tx_ring_size * sizeof(struct tx_desc);\r\nif (index == 0 && size <= mp->tx_desc_sram_size) {\r\ntxq->tx_desc_area = ioremap(mp->tx_desc_sram_addr,\r\nmp->tx_desc_sram_size);\r\ntxq->tx_desc_dma = mp->tx_desc_sram_addr;\r\n} else {\r\ntxq->tx_desc_area = dma_alloc_coherent(mp->dev->dev.parent,\r\nsize, &txq->tx_desc_dma,\r\nGFP_KERNEL);\r\n}\r\nif (txq->tx_desc_area == NULL) {\r\nnetdev_err(mp->dev,\r\n"can't allocate tx ring (%d bytes)\n", size);\r\nreturn -ENOMEM;\r\n}\r\nmemset(txq->tx_desc_area, 0, size);\r\ntxq->tx_desc_area_size = size;\r\ntx_desc = txq->tx_desc_area;\r\nfor (i = 0; i < txq->tx_ring_size; i++) {\r\nstruct tx_desc *txd = tx_desc + i;\r\nint nexti;\r\nnexti = i + 1;\r\nif (nexti == txq->tx_ring_size)\r\nnexti = 0;\r\ntxd->cmd_sts = 0;\r\ntxd->next_desc_ptr = txq->tx_desc_dma +\r\nnexti * sizeof(struct tx_desc);\r\n}\r\ntxq->tso_hdrs = dma_alloc_coherent(mp->dev->dev.parent,\r\ntxq->tx_ring_size * TSO_HEADER_SIZE,\r\n&txq->tso_hdrs_dma, GFP_KERNEL);\r\nif (txq->tso_hdrs == NULL) {\r\ndma_free_coherent(mp->dev->dev.parent, txq->tx_desc_area_size,\r\ntxq->tx_desc_area, txq->tx_desc_dma);\r\nreturn -ENOMEM;\r\n}\r\nskb_queue_head_init(&txq->tx_skb);\r\nreturn 0;\r\n}\r\nstatic void txq_deinit(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\ntxq_disable(txq);\r\ntxq_reclaim(txq, txq->tx_ring_size, 1);\r\nBUG_ON(txq->tx_used_desc != txq->tx_curr_desc);\r\nif (txq->index == 0 &&\r\ntxq->tx_desc_area_size <= mp->tx_desc_sram_size)\r\niounmap(txq->tx_desc_area);\r\nelse\r\ndma_free_coherent(mp->dev->dev.parent, txq->tx_desc_area_size,\r\ntxq->tx_desc_area, txq->tx_desc_dma);\r\nif (txq->tso_hdrs)\r\ndma_free_coherent(mp->dev->dev.parent,\r\ntxq->tx_ring_size * TSO_HEADER_SIZE,\r\ntxq->tso_hdrs, txq->tso_hdrs_dma);\r\n}\r\nstatic int mv643xx_eth_collect_events(struct mv643xx_eth_private *mp)\r\n{\r\nu32 int_cause;\r\nu32 int_cause_ext;\r\nint_cause = rdlp(mp, INT_CAUSE) & mp->int_mask;\r\nif (int_cause == 0)\r\nreturn 0;\r\nint_cause_ext = 0;\r\nif (int_cause & INT_EXT) {\r\nint_cause &= ~INT_EXT;\r\nint_cause_ext = rdlp(mp, INT_CAUSE_EXT);\r\n}\r\nif (int_cause) {\r\nwrlp(mp, INT_CAUSE, ~int_cause);\r\nmp->work_tx_end |= ((int_cause & INT_TX_END) >> 19) &\r\n~(rdlp(mp, TXQ_COMMAND) & 0xff);\r\nmp->work_rx |= (int_cause & INT_RX) >> 2;\r\n}\r\nint_cause_ext &= INT_EXT_LINK_PHY | INT_EXT_TX;\r\nif (int_cause_ext) {\r\nwrlp(mp, INT_CAUSE_EXT, ~int_cause_ext);\r\nif (int_cause_ext & INT_EXT_LINK_PHY)\r\nmp->work_link = 1;\r\nmp->work_tx |= int_cause_ext & INT_EXT_TX;\r\n}\r\nreturn 1;\r\n}\r\nstatic irqreturn_t mv643xx_eth_irq(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = (struct net_device *)dev_id;\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (unlikely(!mv643xx_eth_collect_events(mp)))\r\nreturn IRQ_NONE;\r\nwrlp(mp, INT_MASK, 0);\r\nnapi_schedule(&mp->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void handle_link_event(struct mv643xx_eth_private *mp)\r\n{\r\nstruct net_device *dev = mp->dev;\r\nu32 port_status;\r\nint speed;\r\nint duplex;\r\nint fc;\r\nport_status = rdlp(mp, PORT_STATUS);\r\nif (!(port_status & LINK_UP)) {\r\nif (netif_carrier_ok(dev)) {\r\nint i;\r\nnetdev_info(dev, "link down\n");\r\nnetif_carrier_off(dev);\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nstruct tx_queue *txq = mp->txq + i;\r\ntxq_reclaim(txq, txq->tx_ring_size, 1);\r\ntxq_reset_hw_ptr(txq);\r\n}\r\n}\r\nreturn;\r\n}\r\nswitch (port_status & PORT_SPEED_MASK) {\r\ncase PORT_SPEED_10:\r\nspeed = 10;\r\nbreak;\r\ncase PORT_SPEED_100:\r\nspeed = 100;\r\nbreak;\r\ncase PORT_SPEED_1000:\r\nspeed = 1000;\r\nbreak;\r\ndefault:\r\nspeed = -1;\r\nbreak;\r\n}\r\nduplex = (port_status & FULL_DUPLEX) ? 1 : 0;\r\nfc = (port_status & FLOW_CONTROL_ENABLED) ? 1 : 0;\r\nnetdev_info(dev, "link up, %d Mb/s, %s duplex, flow control %sabled\n",\r\nspeed, duplex ? "full" : "half", fc ? "en" : "dis");\r\nif (!netif_carrier_ok(dev))\r\nnetif_carrier_on(dev);\r\n}\r\nstatic int mv643xx_eth_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct mv643xx_eth_private *mp;\r\nint work_done;\r\nmp = container_of(napi, struct mv643xx_eth_private, napi);\r\nif (unlikely(mp->oom)) {\r\nmp->oom = 0;\r\ndel_timer(&mp->rx_oom);\r\n}\r\nwork_done = 0;\r\nwhile (work_done < budget) {\r\nu8 queue_mask;\r\nint queue;\r\nint work_tbd;\r\nif (mp->work_link) {\r\nmp->work_link = 0;\r\nhandle_link_event(mp);\r\nwork_done++;\r\ncontinue;\r\n}\r\nqueue_mask = mp->work_tx | mp->work_tx_end | mp->work_rx;\r\nif (likely(!mp->oom))\r\nqueue_mask |= mp->work_rx_refill;\r\nif (!queue_mask) {\r\nif (mv643xx_eth_collect_events(mp))\r\ncontinue;\r\nbreak;\r\n}\r\nqueue = fls(queue_mask) - 1;\r\nqueue_mask = 1 << queue;\r\nwork_tbd = budget - work_done;\r\nif (work_tbd > 16)\r\nwork_tbd = 16;\r\nif (mp->work_tx_end & queue_mask) {\r\ntxq_kick(mp->txq + queue);\r\n} else if (mp->work_tx & queue_mask) {\r\nwork_done += txq_reclaim(mp->txq + queue, work_tbd, 0);\r\ntxq_maybe_wake(mp->txq + queue);\r\n} else if (mp->work_rx & queue_mask) {\r\nwork_done += rxq_process(mp->rxq + queue, work_tbd);\r\n} else if (!mp->oom && (mp->work_rx_refill & queue_mask)) {\r\nwork_done += rxq_refill(mp->rxq + queue, work_tbd);\r\n} else {\r\nBUG();\r\n}\r\n}\r\nif (work_done < budget) {\r\nif (mp->oom)\r\nmod_timer(&mp->rx_oom, jiffies + (HZ / 10));\r\nnapi_complete(napi);\r\nwrlp(mp, INT_MASK, mp->int_mask);\r\n}\r\nreturn work_done;\r\n}\r\nstatic inline void oom_timer_wrapper(unsigned long data)\r\n{\r\nstruct mv643xx_eth_private *mp = (void *)data;\r\nnapi_schedule(&mp->napi);\r\n}\r\nstatic void port_start(struct mv643xx_eth_private *mp)\r\n{\r\nu32 pscr;\r\nint i;\r\nif (mp->phy != NULL) {\r\nstruct ethtool_cmd cmd;\r\nmv643xx_eth_get_settings(mp->dev, &cmd);\r\nphy_init_hw(mp->phy);\r\nmv643xx_eth_set_settings(mp->dev, &cmd);\r\nphy_start(mp->phy);\r\n}\r\npscr = rdlp(mp, PORT_SERIAL_CONTROL);\r\npscr |= SERIAL_PORT_ENABLE;\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\npscr |= DO_NOT_FORCE_LINK_FAIL;\r\nif (mp->phy == NULL)\r\npscr |= FORCE_LINK_PASS;\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\ntx_set_rate(mp, 1000000000, 16777216);\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nstruct tx_queue *txq = mp->txq + i;\r\ntxq_reset_hw_ptr(txq);\r\ntxq_set_rate(txq, 1000000000, 16777216);\r\ntxq_set_fixed_prio_mode(txq);\r\n}\r\nmv643xx_eth_set_features(mp->dev, mp->dev->features);\r\nwrlp(mp, PORT_CONFIG_EXT, 0x00000000);\r\nmv643xx_eth_program_unicast_filter(mp->dev);\r\nfor (i = 0; i < mp->rxq_count; i++) {\r\nstruct rx_queue *rxq = mp->rxq + i;\r\nu32 addr;\r\naddr = (u32)rxq->rx_desc_dma;\r\naddr += rxq->rx_curr_desc * sizeof(struct rx_desc);\r\nwrlp(mp, RXQ_CURRENT_DESC_PTR(i), addr);\r\nrxq_enable(rxq);\r\n}\r\n}\r\nstatic void mv643xx_eth_recalc_skb_size(struct mv643xx_eth_private *mp)\r\n{\r\nint skb_size;\r\nskb_size = mp->dev->mtu + 36;\r\nmp->skb_size = (skb_size + 7) & ~7;\r\nmp->skb_size += SKB_DMA_REALIGN;\r\n}\r\nstatic int mv643xx_eth_open(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint err;\r\nint i;\r\nwrlp(mp, INT_CAUSE, 0);\r\nwrlp(mp, INT_CAUSE_EXT, 0);\r\nrdlp(mp, INT_CAUSE_EXT);\r\nerr = request_irq(dev->irq, mv643xx_eth_irq,\r\nIRQF_SHARED, dev->name, dev);\r\nif (err) {\r\nnetdev_err(dev, "can't assign irq\n");\r\nreturn -EAGAIN;\r\n}\r\nmv643xx_eth_recalc_skb_size(mp);\r\nnapi_enable(&mp->napi);\r\nmp->int_mask = INT_EXT;\r\nfor (i = 0; i < mp->rxq_count; i++) {\r\nerr = rxq_init(mp, i);\r\nif (err) {\r\nwhile (--i >= 0)\r\nrxq_deinit(mp->rxq + i);\r\ngoto out;\r\n}\r\nrxq_refill(mp->rxq + i, INT_MAX);\r\nmp->int_mask |= INT_RX_0 << i;\r\n}\r\nif (mp->oom) {\r\nmp->rx_oom.expires = jiffies + (HZ / 10);\r\nadd_timer(&mp->rx_oom);\r\n}\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nerr = txq_init(mp, i);\r\nif (err) {\r\nwhile (--i >= 0)\r\ntxq_deinit(mp->txq + i);\r\ngoto out_free;\r\n}\r\nmp->int_mask |= INT_TX_END_0 << i;\r\n}\r\nadd_timer(&mp->mib_counters_timer);\r\nport_start(mp);\r\nwrlp(mp, INT_MASK_EXT, INT_EXT_LINK_PHY | INT_EXT_TX);\r\nwrlp(mp, INT_MASK, mp->int_mask);\r\nreturn 0;\r\nout_free:\r\nfor (i = 0; i < mp->rxq_count; i++)\r\nrxq_deinit(mp->rxq + i);\r\nout:\r\nfree_irq(dev->irq, dev);\r\nreturn err;\r\n}\r\nstatic void port_reset(struct mv643xx_eth_private *mp)\r\n{\r\nunsigned int data;\r\nint i;\r\nfor (i = 0; i < mp->rxq_count; i++)\r\nrxq_disable(mp->rxq + i);\r\nfor (i = 0; i < mp->txq_count; i++)\r\ntxq_disable(mp->txq + i);\r\nwhile (1) {\r\nu32 ps = rdlp(mp, PORT_STATUS);\r\nif ((ps & (TX_IN_PROGRESS | TX_FIFO_EMPTY)) == TX_FIFO_EMPTY)\r\nbreak;\r\nudelay(10);\r\n}\r\ndata = rdlp(mp, PORT_SERIAL_CONTROL);\r\ndata &= ~(SERIAL_PORT_ENABLE |\r\nDO_NOT_FORCE_LINK_FAIL |\r\nFORCE_LINK_PASS);\r\nwrlp(mp, PORT_SERIAL_CONTROL, data);\r\n}\r\nstatic int mv643xx_eth_stop(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint i;\r\nwrlp(mp, INT_MASK_EXT, 0x00000000);\r\nwrlp(mp, INT_MASK, 0x00000000);\r\nrdlp(mp, INT_MASK);\r\nnapi_disable(&mp->napi);\r\ndel_timer_sync(&mp->rx_oom);\r\nnetif_carrier_off(dev);\r\nif (mp->phy)\r\nphy_stop(mp->phy);\r\nfree_irq(dev->irq, dev);\r\nport_reset(mp);\r\nmv643xx_eth_get_stats(dev);\r\nmib_counters_update(mp);\r\ndel_timer_sync(&mp->mib_counters_timer);\r\nfor (i = 0; i < mp->rxq_count; i++)\r\nrxq_deinit(mp->rxq + i);\r\nfor (i = 0; i < mp->txq_count; i++)\r\ntxq_deinit(mp->txq + i);\r\nreturn 0;\r\n}\r\nstatic int mv643xx_eth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint ret;\r\nif (mp->phy == NULL)\r\nreturn -ENOTSUPP;\r\nret = phy_mii_ioctl(mp->phy, ifr, cmd);\r\nif (!ret)\r\nmv643xx_eth_adjust_link(dev);\r\nreturn ret;\r\n}\r\nstatic int mv643xx_eth_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (new_mtu < 64 || new_mtu > 9500)\r\nreturn -EINVAL;\r\ndev->mtu = new_mtu;\r\nmv643xx_eth_recalc_skb_size(mp);\r\ntx_set_rate(mp, 1000000000, 16777216);\r\nif (!netif_running(dev))\r\nreturn 0;\r\nmv643xx_eth_stop(dev);\r\nif (mv643xx_eth_open(dev)) {\r\nnetdev_err(dev,\r\n"fatal error on re-opening device after MTU change\n");\r\n}\r\nreturn 0;\r\n}\r\nstatic void tx_timeout_task(struct work_struct *ugly)\r\n{\r\nstruct mv643xx_eth_private *mp;\r\nmp = container_of(ugly, struct mv643xx_eth_private, tx_timeout_task);\r\nif (netif_running(mp->dev)) {\r\nnetif_tx_stop_all_queues(mp->dev);\r\nport_reset(mp);\r\nport_start(mp);\r\nnetif_tx_wake_all_queues(mp->dev);\r\n}\r\n}\r\nstatic void mv643xx_eth_tx_timeout(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nnetdev_info(dev, "tx timeout\n");\r\nschedule_work(&mp->tx_timeout_task);\r\n}\r\nstatic void mv643xx_eth_netpoll(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nwrlp(mp, INT_MASK, 0x00000000);\r\nrdlp(mp, INT_MASK);\r\nmv643xx_eth_irq(dev->irq, dev);\r\nwrlp(mp, INT_MASK, mp->int_mask);\r\n}\r\nstatic void\r\nmv643xx_eth_conf_mbus_windows(struct mv643xx_eth_shared_private *msp,\r\nconst struct mbus_dram_target_info *dram)\r\n{\r\nvoid __iomem *base = msp->base;\r\nu32 win_enable;\r\nu32 win_protect;\r\nint i;\r\nfor (i = 0; i < 6; i++) {\r\nwritel(0, base + WINDOW_BASE(i));\r\nwritel(0, base + WINDOW_SIZE(i));\r\nif (i < 4)\r\nwritel(0, base + WINDOW_REMAP_HIGH(i));\r\n}\r\nwin_enable = 0x3f;\r\nwin_protect = 0;\r\nfor (i = 0; i < dram->num_cs; i++) {\r\nconst struct mbus_dram_window *cs = dram->cs + i;\r\nwritel((cs->base & 0xffff0000) |\r\n(cs->mbus_attr << 8) |\r\ndram->mbus_dram_target_id, base + WINDOW_BASE(i));\r\nwritel((cs->size - 1) & 0xffff0000, base + WINDOW_SIZE(i));\r\nwin_enable &= ~(1 << i);\r\nwin_protect |= 3 << (2 * i);\r\n}\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE);\r\nmsp->win_protect = win_protect;\r\n}\r\nstatic void infer_hw_params(struct mv643xx_eth_shared_private *msp)\r\n{\r\nwritel(0x02000000, msp->base + 0x0400 + SDMA_CONFIG);\r\nif (readl(msp->base + 0x0400 + SDMA_CONFIG) & 0x02000000)\r\nmsp->extended_rx_coal_limit = 1;\r\nelse\r\nmsp->extended_rx_coal_limit = 0;\r\nwritel(1, msp->base + 0x0400 + TX_BW_MTU_MOVED);\r\nif (readl(msp->base + 0x0400 + TX_BW_MTU_MOVED) & 1) {\r\nmsp->tx_bw_control = TX_BW_CONTROL_NEW_LAYOUT;\r\n} else {\r\nwritel(7, msp->base + 0x0400 + TX_BW_RATE);\r\nif (readl(msp->base + 0x0400 + TX_BW_RATE) & 7)\r\nmsp->tx_bw_control = TX_BW_CONTROL_OLD_LAYOUT;\r\nelse\r\nmsp->tx_bw_control = TX_BW_CONTROL_ABSENT;\r\n}\r\n}\r\nstatic int mv643xx_eth_shared_of_add_port(struct platform_device *pdev,\r\nstruct device_node *pnp)\r\n{\r\nstruct platform_device *ppdev;\r\nstruct mv643xx_eth_platform_data ppd;\r\nstruct resource res;\r\nconst char *mac_addr;\r\nint ret;\r\nint dev_num = 0;\r\nmemset(&ppd, 0, sizeof(ppd));\r\nppd.shared = pdev;\r\nmemset(&res, 0, sizeof(res));\r\nif (!of_irq_to_resource(pnp, 0, &res)) {\r\ndev_err(&pdev->dev, "missing interrupt on %s\n", pnp->name);\r\nreturn -EINVAL;\r\n}\r\nif (of_property_read_u32(pnp, "reg", &ppd.port_number)) {\r\ndev_err(&pdev->dev, "missing reg property on %s\n", pnp->name);\r\nreturn -EINVAL;\r\n}\r\nif (ppd.port_number >= 3) {\r\ndev_err(&pdev->dev, "invalid reg property on %s\n", pnp->name);\r\nreturn -EINVAL;\r\n}\r\nwhile (dev_num < 3 && port_platdev[dev_num])\r\ndev_num++;\r\nif (dev_num == 3) {\r\ndev_err(&pdev->dev, "too many ports registered\n");\r\nreturn -EINVAL;\r\n}\r\nmac_addr = of_get_mac_address(pnp);\r\nif (mac_addr)\r\nmemcpy(ppd.mac_addr, mac_addr, ETH_ALEN);\r\nmv643xx_eth_property(pnp, "tx-queue-size", ppd.tx_queue_size);\r\nmv643xx_eth_property(pnp, "tx-sram-addr", ppd.tx_sram_addr);\r\nmv643xx_eth_property(pnp, "tx-sram-size", ppd.tx_sram_size);\r\nmv643xx_eth_property(pnp, "rx-queue-size", ppd.rx_queue_size);\r\nmv643xx_eth_property(pnp, "rx-sram-addr", ppd.rx_sram_addr);\r\nmv643xx_eth_property(pnp, "rx-sram-size", ppd.rx_sram_size);\r\nppd.phy_node = of_parse_phandle(pnp, "phy-handle", 0);\r\nif (!ppd.phy_node) {\r\nppd.phy_addr = MV643XX_ETH_PHY_NONE;\r\nof_property_read_u32(pnp, "speed", &ppd.speed);\r\nof_property_read_u32(pnp, "duplex", &ppd.duplex);\r\n}\r\nppdev = platform_device_alloc(MV643XX_ETH_NAME, dev_num);\r\nif (!ppdev)\r\nreturn -ENOMEM;\r\nppdev->dev.coherent_dma_mask = DMA_BIT_MASK(32);\r\nppdev->dev.of_node = pnp;\r\nret = platform_device_add_resources(ppdev, &res, 1);\r\nif (ret)\r\ngoto port_err;\r\nret = platform_device_add_data(ppdev, &ppd, sizeof(ppd));\r\nif (ret)\r\ngoto port_err;\r\nret = platform_device_add(ppdev);\r\nif (ret)\r\ngoto port_err;\r\nport_platdev[dev_num] = ppdev;\r\nreturn 0;\r\nport_err:\r\nplatform_device_put(ppdev);\r\nreturn ret;\r\n}\r\nstatic int mv643xx_eth_shared_of_probe(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_shared_platform_data *pd;\r\nstruct device_node *pnp, *np = pdev->dev.of_node;\r\nint ret;\r\nif (!np)\r\nreturn 0;\r\npd = devm_kzalloc(&pdev->dev, sizeof(*pd), GFP_KERNEL);\r\nif (!pd)\r\nreturn -ENOMEM;\r\npdev->dev.platform_data = pd;\r\nmv643xx_eth_property(np, "tx-checksum-limit", pd->tx_csum_limit);\r\nfor_each_available_child_of_node(np, pnp) {\r\nret = mv643xx_eth_shared_of_add_port(pdev, pnp);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mv643xx_eth_shared_of_remove(void)\r\n{\r\nint n;\r\nfor (n = 0; n < 3; n++) {\r\nplatform_device_del(port_platdev[n]);\r\nport_platdev[n] = NULL;\r\n}\r\n}\r\nstatic inline int mv643xx_eth_shared_of_probe(struct platform_device *pdev)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void mv643xx_eth_shared_of_remove(void)\r\n{\r\n}\r\nstatic int mv643xx_eth_shared_probe(struct platform_device *pdev)\r\n{\r\nstatic int mv643xx_eth_version_printed;\r\nstruct mv643xx_eth_shared_platform_data *pd;\r\nstruct mv643xx_eth_shared_private *msp;\r\nconst struct mbus_dram_target_info *dram;\r\nstruct resource *res;\r\nint ret;\r\nif (!mv643xx_eth_version_printed++)\r\npr_notice("MV-643xx 10/100/1000 ethernet driver version %s\n",\r\nmv643xx_eth_driver_version);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (res == NULL)\r\nreturn -EINVAL;\r\nmsp = devm_kzalloc(&pdev->dev, sizeof(*msp), GFP_KERNEL);\r\nif (msp == NULL)\r\nreturn -ENOMEM;\r\nplatform_set_drvdata(pdev, msp);\r\nmsp->base = devm_ioremap(&pdev->dev, res->start, resource_size(res));\r\nif (msp->base == NULL)\r\nreturn -ENOMEM;\r\nmsp->clk = devm_clk_get(&pdev->dev, NULL);\r\nif (!IS_ERR(msp->clk))\r\nclk_prepare_enable(msp->clk);\r\ndram = mv_mbus_dram_info();\r\nif (dram)\r\nmv643xx_eth_conf_mbus_windows(msp, dram);\r\nret = mv643xx_eth_shared_of_probe(pdev);\r\nif (ret)\r\nreturn ret;\r\npd = dev_get_platdata(&pdev->dev);\r\nmsp->tx_csum_limit = (pd != NULL && pd->tx_csum_limit) ?\r\npd->tx_csum_limit : 9 * 1024;\r\ninfer_hw_params(msp);\r\nreturn 0;\r\n}\r\nstatic int mv643xx_eth_shared_remove(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_shared_private *msp = platform_get_drvdata(pdev);\r\nmv643xx_eth_shared_of_remove();\r\nif (!IS_ERR(msp->clk))\r\nclk_disable_unprepare(msp->clk);\r\nreturn 0;\r\n}\r\nstatic void phy_addr_set(struct mv643xx_eth_private *mp, int phy_addr)\r\n{\r\nint addr_shift = 5 * mp->port_num;\r\nu32 data;\r\ndata = rdl(mp, PHY_ADDR);\r\ndata &= ~(0x1f << addr_shift);\r\ndata |= (phy_addr & 0x1f) << addr_shift;\r\nwrl(mp, PHY_ADDR, data);\r\n}\r\nstatic int phy_addr_get(struct mv643xx_eth_private *mp)\r\n{\r\nunsigned int data;\r\ndata = rdl(mp, PHY_ADDR);\r\nreturn (data >> (5 * mp->port_num)) & 0x1f;\r\n}\r\nstatic void set_params(struct mv643xx_eth_private *mp,\r\nstruct mv643xx_eth_platform_data *pd)\r\n{\r\nstruct net_device *dev = mp->dev;\r\nunsigned int tx_ring_size;\r\nif (is_valid_ether_addr(pd->mac_addr))\r\nmemcpy(dev->dev_addr, pd->mac_addr, ETH_ALEN);\r\nelse\r\nuc_addr_get(mp, dev->dev_addr);\r\nmp->rx_ring_size = DEFAULT_RX_QUEUE_SIZE;\r\nif (pd->rx_queue_size)\r\nmp->rx_ring_size = pd->rx_queue_size;\r\nmp->rx_desc_sram_addr = pd->rx_sram_addr;\r\nmp->rx_desc_sram_size = pd->rx_sram_size;\r\nmp->rxq_count = pd->rx_queue_count ? : 1;\r\ntx_ring_size = DEFAULT_TX_QUEUE_SIZE;\r\nif (pd->tx_queue_size)\r\ntx_ring_size = pd->tx_queue_size;\r\nmp->tx_ring_size = clamp_t(unsigned int, tx_ring_size,\r\nMV643XX_MAX_SKB_DESCS * 2, 4096);\r\nif (mp->tx_ring_size != tx_ring_size)\r\nnetdev_warn(dev, "TX queue size set to %u (requested %u)\n",\r\nmp->tx_ring_size, tx_ring_size);\r\nmp->tx_desc_sram_addr = pd->tx_sram_addr;\r\nmp->tx_desc_sram_size = pd->tx_sram_size;\r\nmp->txq_count = pd->tx_queue_count ? : 1;\r\n}\r\nstatic struct phy_device *phy_scan(struct mv643xx_eth_private *mp,\r\nint phy_addr)\r\n{\r\nstruct phy_device *phydev;\r\nint start;\r\nint num;\r\nint i;\r\nchar phy_id[MII_BUS_ID_SIZE + 3];\r\nif (phy_addr == MV643XX_ETH_PHY_ADDR_DEFAULT) {\r\nstart = phy_addr_get(mp) & 0x1f;\r\nnum = 32;\r\n} else {\r\nstart = phy_addr & 0x1f;\r\nnum = 1;\r\n}\r\nphydev = ERR_PTR(-ENODEV);\r\nfor (i = 0; i < num; i++) {\r\nint addr = (start + i) & 0x1f;\r\nsnprintf(phy_id, sizeof(phy_id), PHY_ID_FMT,\r\n"orion-mdio-mii", addr);\r\nphydev = phy_connect(mp->dev, phy_id, mv643xx_eth_adjust_link,\r\nPHY_INTERFACE_MODE_GMII);\r\nif (!IS_ERR(phydev)) {\r\nphy_addr_set(mp, addr);\r\nbreak;\r\n}\r\n}\r\nreturn phydev;\r\n}\r\nstatic void phy_init(struct mv643xx_eth_private *mp, int speed, int duplex)\r\n{\r\nstruct phy_device *phy = mp->phy;\r\nif (speed == 0) {\r\nphy->autoneg = AUTONEG_ENABLE;\r\nphy->speed = 0;\r\nphy->duplex = 0;\r\nphy->advertising = phy->supported | ADVERTISED_Autoneg;\r\n} else {\r\nphy->autoneg = AUTONEG_DISABLE;\r\nphy->advertising = 0;\r\nphy->speed = speed;\r\nphy->duplex = duplex;\r\n}\r\nphy_start_aneg(phy);\r\n}\r\nstatic void init_pscr(struct mv643xx_eth_private *mp, int speed, int duplex)\r\n{\r\nu32 pscr;\r\npscr = rdlp(mp, PORT_SERIAL_CONTROL);\r\nif (pscr & SERIAL_PORT_ENABLE) {\r\npscr &= ~SERIAL_PORT_ENABLE;\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\n}\r\npscr = MAX_RX_PACKET_9700BYTE | SERIAL_PORT_CONTROL_RESERVED;\r\nif (mp->phy == NULL) {\r\npscr |= DISABLE_AUTO_NEG_SPEED_GMII;\r\nif (speed == SPEED_1000)\r\npscr |= SET_GMII_SPEED_TO_1000;\r\nelse if (speed == SPEED_100)\r\npscr |= SET_MII_SPEED_TO_100;\r\npscr |= DISABLE_AUTO_NEG_FOR_FLOW_CTRL;\r\npscr |= DISABLE_AUTO_NEG_FOR_DUPLEX;\r\nif (duplex == DUPLEX_FULL)\r\npscr |= SET_FULL_DUPLEX_MODE;\r\n}\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\n}\r\nstatic int mv643xx_eth_probe(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_platform_data *pd;\r\nstruct mv643xx_eth_private *mp;\r\nstruct net_device *dev;\r\nstruct resource *res;\r\nint err;\r\npd = dev_get_platdata(&pdev->dev);\r\nif (pd == NULL) {\r\ndev_err(&pdev->dev, "no mv643xx_eth_platform_data\n");\r\nreturn -ENODEV;\r\n}\r\nif (pd->shared == NULL) {\r\ndev_err(&pdev->dev, "no mv643xx_eth_platform_data->shared\n");\r\nreturn -ENODEV;\r\n}\r\ndev = alloc_etherdev_mq(sizeof(struct mv643xx_eth_private), 8);\r\nif (!dev)\r\nreturn -ENOMEM;\r\nmp = netdev_priv(dev);\r\nplatform_set_drvdata(pdev, mp);\r\nmp->shared = platform_get_drvdata(pd->shared);\r\nmp->base = mp->shared->base + 0x0400 + (pd->port_number << 10);\r\nmp->port_num = pd->port_number;\r\nmp->dev = dev;\r\nif (of_device_is_compatible(pdev->dev.of_node,\r\n"marvell,kirkwood-eth-port"))\r\nwrlp(mp, PORT_SERIAL_CONTROL1,\r\nrdlp(mp, PORT_SERIAL_CONTROL1) & ~CLK125_BYPASS_EN);\r\nmp->t_clk = 133000000;\r\nmp->clk = devm_clk_get(&pdev->dev, NULL);\r\nif (!IS_ERR(mp->clk)) {\r\nclk_prepare_enable(mp->clk);\r\nmp->t_clk = clk_get_rate(mp->clk);\r\n} else if (!IS_ERR(mp->shared->clk)) {\r\nmp->t_clk = clk_get_rate(mp->shared->clk);\r\n}\r\nset_params(mp, pd);\r\nnetif_set_real_num_tx_queues(dev, mp->txq_count);\r\nnetif_set_real_num_rx_queues(dev, mp->rxq_count);\r\nerr = 0;\r\nif (pd->phy_node) {\r\nmp->phy = of_phy_connect(mp->dev, pd->phy_node,\r\nmv643xx_eth_adjust_link, 0,\r\nPHY_INTERFACE_MODE_GMII);\r\nif (!mp->phy)\r\nerr = -ENODEV;\r\nelse\r\nphy_addr_set(mp, mp->phy->addr);\r\n} else if (pd->phy_addr != MV643XX_ETH_PHY_NONE) {\r\nmp->phy = phy_scan(mp, pd->phy_addr);\r\nif (IS_ERR(mp->phy))\r\nerr = PTR_ERR(mp->phy);\r\nelse\r\nphy_init(mp, pd->speed, pd->duplex);\r\n}\r\nif (err == -ENODEV) {\r\nerr = -EPROBE_DEFER;\r\ngoto out;\r\n}\r\nif (err)\r\ngoto out;\r\ndev->ethtool_ops = &mv643xx_eth_ethtool_ops;\r\ninit_pscr(mp, pd->speed, pd->duplex);\r\nmib_counters_clear(mp);\r\ninit_timer(&mp->mib_counters_timer);\r\nmp->mib_counters_timer.data = (unsigned long)mp;\r\nmp->mib_counters_timer.function = mib_counters_timer_wrapper;\r\nmp->mib_counters_timer.expires = jiffies + 30 * HZ;\r\nspin_lock_init(&mp->mib_counters_lock);\r\nINIT_WORK(&mp->tx_timeout_task, tx_timeout_task);\r\nnetif_napi_add(dev, &mp->napi, mv643xx_eth_poll, NAPI_POLL_WEIGHT);\r\ninit_timer(&mp->rx_oom);\r\nmp->rx_oom.data = (unsigned long)mp;\r\nmp->rx_oom.function = oom_timer_wrapper;\r\nres = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nBUG_ON(!res);\r\ndev->irq = res->start;\r\ndev->netdev_ops = &mv643xx_eth_netdev_ops;\r\ndev->watchdog_timeo = 2 * HZ;\r\ndev->base_addr = 0;\r\ndev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO;\r\ndev->vlan_features = dev->features;\r\ndev->features |= NETIF_F_RXCSUM;\r\ndev->hw_features = dev->features;\r\ndev->priv_flags |= IFF_UNICAST_FLT;\r\ndev->gso_max_segs = MV643XX_MAX_TSO_SEGS;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nif (mp->shared->win_protect)\r\nwrl(mp, WINDOW_PROTECT(mp->port_num), mp->shared->win_protect);\r\nnetif_carrier_off(dev);\r\nwrlp(mp, SDMA_CONFIG, PORT_SDMA_CONFIG_DEFAULT_VALUE);\r\nset_rx_coal(mp, 250);\r\nset_tx_coal(mp, 0);\r\nerr = register_netdev(dev);\r\nif (err)\r\ngoto out;\r\nnetdev_notice(dev, "port %d with MAC address %pM\n",\r\nmp->port_num, dev->dev_addr);\r\nif (mp->tx_desc_sram_size > 0)\r\nnetdev_notice(dev, "configured with sram\n");\r\nreturn 0;\r\nout:\r\nif (!IS_ERR(mp->clk))\r\nclk_disable_unprepare(mp->clk);\r\nfree_netdev(dev);\r\nreturn err;\r\n}\r\nstatic int mv643xx_eth_remove(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_private *mp = platform_get_drvdata(pdev);\r\nunregister_netdev(mp->dev);\r\nif (mp->phy != NULL)\r\nphy_disconnect(mp->phy);\r\ncancel_work_sync(&mp->tx_timeout_task);\r\nif (!IS_ERR(mp->clk))\r\nclk_disable_unprepare(mp->clk);\r\nfree_netdev(mp->dev);\r\nreturn 0;\r\n}\r\nstatic void mv643xx_eth_shutdown(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_private *mp = platform_get_drvdata(pdev);\r\nwrlp(mp, INT_MASK, 0);\r\nrdlp(mp, INT_MASK);\r\nif (netif_running(mp->dev))\r\nport_reset(mp);\r\n}\r\nstatic int __init mv643xx_eth_init_module(void)\r\n{\r\nint rc;\r\nrc = platform_driver_register(&mv643xx_eth_shared_driver);\r\nif (!rc) {\r\nrc = platform_driver_register(&mv643xx_eth_driver);\r\nif (rc)\r\nplatform_driver_unregister(&mv643xx_eth_shared_driver);\r\n}\r\nreturn rc;\r\n}\r\nstatic void __exit mv643xx_eth_cleanup_module(void)\r\n{\r\nplatform_driver_unregister(&mv643xx_eth_driver);\r\nplatform_driver_unregister(&mv643xx_eth_shared_driver);\r\n}
