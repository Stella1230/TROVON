static struct cpdma_desc_pool *\r\ncpdma_desc_pool_create(struct device *dev, u32 phys, u32 hw_addr,\r\nint size, int align)\r\n{\r\nint bitmap_size;\r\nstruct cpdma_desc_pool *pool;\r\npool = devm_kzalloc(dev, sizeof(*pool), GFP_KERNEL);\r\nif (!pool)\r\ngoto fail;\r\nspin_lock_init(&pool->lock);\r\npool->dev = dev;\r\npool->mem_size = size;\r\npool->desc_size = ALIGN(sizeof(struct cpdma_desc), align);\r\npool->num_desc = size / pool->desc_size;\r\nbitmap_size = (pool->num_desc / BITS_PER_LONG) * sizeof(long);\r\npool->bitmap = devm_kzalloc(dev, bitmap_size, GFP_KERNEL);\r\nif (!pool->bitmap)\r\ngoto fail;\r\nif (phys) {\r\npool->phys = phys;\r\npool->iomap = ioremap(phys, size);\r\npool->hw_addr = hw_addr;\r\n} else {\r\npool->cpumap = dma_alloc_coherent(dev, size, &pool->phys,\r\nGFP_KERNEL);\r\npool->iomap = pool->cpumap;\r\npool->hw_addr = pool->phys;\r\n}\r\nif (pool->iomap)\r\nreturn pool;\r\nfail:\r\nreturn NULL;\r\n}\r\nstatic void cpdma_desc_pool_destroy(struct cpdma_desc_pool *pool)\r\n{\r\nunsigned long flags;\r\nif (!pool)\r\nreturn;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nWARN_ON(pool->used_desc);\r\nif (pool->cpumap) {\r\ndma_free_coherent(pool->dev, pool->mem_size, pool->cpumap,\r\npool->phys);\r\n} else {\r\niounmap(pool->iomap);\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic inline dma_addr_t desc_phys(struct cpdma_desc_pool *pool,\r\nstruct cpdma_desc __iomem *desc)\r\n{\r\nif (!desc)\r\nreturn 0;\r\nreturn pool->hw_addr + (__force long)desc - (__force long)pool->iomap;\r\n}\r\nstatic inline struct cpdma_desc __iomem *\r\ndesc_from_phys(struct cpdma_desc_pool *pool, dma_addr_t dma)\r\n{\r\nreturn dma ? pool->iomap + dma - pool->hw_addr : NULL;\r\n}\r\nstatic struct cpdma_desc __iomem *\r\ncpdma_desc_alloc(struct cpdma_desc_pool *pool, int num_desc, bool is_rx)\r\n{\r\nunsigned long flags;\r\nint index;\r\nint desc_start;\r\nint desc_end;\r\nstruct cpdma_desc __iomem *desc = NULL;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (is_rx) {\r\ndesc_start = 0;\r\ndesc_end = pool->num_desc/2;\r\n} else {\r\ndesc_start = pool->num_desc/2;\r\ndesc_end = pool->num_desc;\r\n}\r\nindex = bitmap_find_next_zero_area(pool->bitmap,\r\ndesc_end, desc_start, num_desc, 0);\r\nif (index < desc_end) {\r\nbitmap_set(pool->bitmap, index, num_desc);\r\ndesc = pool->iomap + pool->desc_size * index;\r\npool->used_desc++;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn desc;\r\n}\r\nstatic void cpdma_desc_free(struct cpdma_desc_pool *pool,\r\nstruct cpdma_desc __iomem *desc, int num_desc)\r\n{\r\nunsigned long flags, index;\r\nindex = ((unsigned long)desc - (unsigned long)pool->iomap) /\r\npool->desc_size;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbitmap_clear(pool->bitmap, index, num_desc);\r\npool->used_desc--;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstruct cpdma_ctlr *cpdma_ctlr_create(struct cpdma_params *params)\r\n{\r\nstruct cpdma_ctlr *ctlr;\r\nctlr = devm_kzalloc(params->dev, sizeof(*ctlr), GFP_KERNEL);\r\nif (!ctlr)\r\nreturn NULL;\r\nctlr->state = CPDMA_STATE_IDLE;\r\nctlr->params = *params;\r\nctlr->dev = params->dev;\r\nspin_lock_init(&ctlr->lock);\r\nctlr->pool = cpdma_desc_pool_create(ctlr->dev,\r\nctlr->params.desc_mem_phys,\r\nctlr->params.desc_hw_addr,\r\nctlr->params.desc_mem_size,\r\nctlr->params.desc_align);\r\nif (!ctlr->pool)\r\nreturn NULL;\r\nif (WARN_ON(ctlr->num_chan > CPDMA_MAX_CHANNELS))\r\nctlr->num_chan = CPDMA_MAX_CHANNELS;\r\nreturn ctlr;\r\n}\r\nint cpdma_ctlr_start(struct cpdma_ctlr *ctlr)\r\n{\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nif (ctlr->state != CPDMA_STATE_IDLE) {\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn -EBUSY;\r\n}\r\nif (ctlr->params.has_soft_reset) {\r\nunsigned timeout = 10 * 100;\r\ndma_reg_write(ctlr, CPDMA_SOFTRESET, 1);\r\nwhile (timeout) {\r\nif (dma_reg_read(ctlr, CPDMA_SOFTRESET) == 0)\r\nbreak;\r\nudelay(10);\r\ntimeout--;\r\n}\r\nWARN_ON(!timeout);\r\n}\r\nfor (i = 0; i < ctlr->num_chan; i++) {\r\n__raw_writel(0, ctlr->params.txhdp + 4 * i);\r\n__raw_writel(0, ctlr->params.rxhdp + 4 * i);\r\n__raw_writel(0, ctlr->params.txcp + 4 * i);\r\n__raw_writel(0, ctlr->params.rxcp + 4 * i);\r\n}\r\ndma_reg_write(ctlr, CPDMA_RXINTMASKCLEAR, 0xffffffff);\r\ndma_reg_write(ctlr, CPDMA_TXINTMASKCLEAR, 0xffffffff);\r\ndma_reg_write(ctlr, CPDMA_TXCONTROL, 1);\r\ndma_reg_write(ctlr, CPDMA_RXCONTROL, 1);\r\nctlr->state = CPDMA_STATE_ACTIVE;\r\nfor (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {\r\nif (ctlr->channels[i])\r\ncpdma_chan_start(ctlr->channels[i]);\r\n}\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn 0;\r\n}\r\nint cpdma_ctlr_stop(struct cpdma_ctlr *ctlr)\r\n{\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nif (ctlr->state == CPDMA_STATE_TEARDOWN) {\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn -EINVAL;\r\n}\r\nctlr->state = CPDMA_STATE_TEARDOWN;\r\nfor (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {\r\nif (ctlr->channels[i])\r\ncpdma_chan_stop(ctlr->channels[i]);\r\n}\r\ndma_reg_write(ctlr, CPDMA_RXINTMASKCLEAR, 0xffffffff);\r\ndma_reg_write(ctlr, CPDMA_TXINTMASKCLEAR, 0xffffffff);\r\ndma_reg_write(ctlr, CPDMA_TXCONTROL, 0);\r\ndma_reg_write(ctlr, CPDMA_RXCONTROL, 0);\r\nctlr->state = CPDMA_STATE_IDLE;\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn 0;\r\n}\r\nint cpdma_ctlr_dump(struct cpdma_ctlr *ctlr)\r\n{\r\nstruct device *dev = ctlr->dev;\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\ndev_info(dev, "CPDMA: state: %s", cpdma_state_str[ctlr->state]);\r\ndev_info(dev, "CPDMA: txidver: %x",\r\ndma_reg_read(ctlr, CPDMA_TXIDVER));\r\ndev_info(dev, "CPDMA: txcontrol: %x",\r\ndma_reg_read(ctlr, CPDMA_TXCONTROL));\r\ndev_info(dev, "CPDMA: txteardown: %x",\r\ndma_reg_read(ctlr, CPDMA_TXTEARDOWN));\r\ndev_info(dev, "CPDMA: rxidver: %x",\r\ndma_reg_read(ctlr, CPDMA_RXIDVER));\r\ndev_info(dev, "CPDMA: rxcontrol: %x",\r\ndma_reg_read(ctlr, CPDMA_RXCONTROL));\r\ndev_info(dev, "CPDMA: softreset: %x",\r\ndma_reg_read(ctlr, CPDMA_SOFTRESET));\r\ndev_info(dev, "CPDMA: rxteardown: %x",\r\ndma_reg_read(ctlr, CPDMA_RXTEARDOWN));\r\ndev_info(dev, "CPDMA: txintstatraw: %x",\r\ndma_reg_read(ctlr, CPDMA_TXINTSTATRAW));\r\ndev_info(dev, "CPDMA: txintstatmasked: %x",\r\ndma_reg_read(ctlr, CPDMA_TXINTSTATMASKED));\r\ndev_info(dev, "CPDMA: txintmaskset: %x",\r\ndma_reg_read(ctlr, CPDMA_TXINTMASKSET));\r\ndev_info(dev, "CPDMA: txintmaskclear: %x",\r\ndma_reg_read(ctlr, CPDMA_TXINTMASKCLEAR));\r\ndev_info(dev, "CPDMA: macinvector: %x",\r\ndma_reg_read(ctlr, CPDMA_MACINVECTOR));\r\ndev_info(dev, "CPDMA: maceoivector: %x",\r\ndma_reg_read(ctlr, CPDMA_MACEOIVECTOR));\r\ndev_info(dev, "CPDMA: rxintstatraw: %x",\r\ndma_reg_read(ctlr, CPDMA_RXINTSTATRAW));\r\ndev_info(dev, "CPDMA: rxintstatmasked: %x",\r\ndma_reg_read(ctlr, CPDMA_RXINTSTATMASKED));\r\ndev_info(dev, "CPDMA: rxintmaskset: %x",\r\ndma_reg_read(ctlr, CPDMA_RXINTMASKSET));\r\ndev_info(dev, "CPDMA: rxintmaskclear: %x",\r\ndma_reg_read(ctlr, CPDMA_RXINTMASKCLEAR));\r\ndev_info(dev, "CPDMA: dmaintstatraw: %x",\r\ndma_reg_read(ctlr, CPDMA_DMAINTSTATRAW));\r\ndev_info(dev, "CPDMA: dmaintstatmasked: %x",\r\ndma_reg_read(ctlr, CPDMA_DMAINTSTATMASKED));\r\ndev_info(dev, "CPDMA: dmaintmaskset: %x",\r\ndma_reg_read(ctlr, CPDMA_DMAINTMASKSET));\r\ndev_info(dev, "CPDMA: dmaintmaskclear: %x",\r\ndma_reg_read(ctlr, CPDMA_DMAINTMASKCLEAR));\r\nif (!ctlr->params.has_ext_regs) {\r\ndev_info(dev, "CPDMA: dmacontrol: %x",\r\ndma_reg_read(ctlr, CPDMA_DMACONTROL));\r\ndev_info(dev, "CPDMA: dmastatus: %x",\r\ndma_reg_read(ctlr, CPDMA_DMASTATUS));\r\ndev_info(dev, "CPDMA: rxbuffofs: %x",\r\ndma_reg_read(ctlr, CPDMA_RXBUFFOFS));\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(ctlr->channels); i++)\r\nif (ctlr->channels[i])\r\ncpdma_chan_dump(ctlr->channels[i]);\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn 0;\r\n}\r\nint cpdma_ctlr_destroy(struct cpdma_ctlr *ctlr)\r\n{\r\nunsigned long flags;\r\nint ret = 0, i;\r\nif (!ctlr)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nif (ctlr->state != CPDMA_STATE_IDLE)\r\ncpdma_ctlr_stop(ctlr);\r\nfor (i = 0; i < ARRAY_SIZE(ctlr->channels); i++)\r\ncpdma_chan_destroy(ctlr->channels[i]);\r\ncpdma_desc_pool_destroy(ctlr->pool);\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn ret;\r\n}\r\nint cpdma_ctlr_int_ctrl(struct cpdma_ctlr *ctlr, bool enable)\r\n{\r\nunsigned long flags;\r\nint i, reg;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nif (ctlr->state != CPDMA_STATE_ACTIVE) {\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn -EINVAL;\r\n}\r\nreg = enable ? CPDMA_DMAINTMASKSET : CPDMA_DMAINTMASKCLEAR;\r\ndma_reg_write(ctlr, reg, CPDMA_DMAINT_HOSTERR);\r\nfor (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {\r\nif (ctlr->channels[i])\r\ncpdma_chan_int_ctrl(ctlr->channels[i], enable);\r\n}\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn 0;\r\n}\r\nvoid cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr, u32 value)\r\n{\r\ndma_reg_write(ctlr, CPDMA_MACEOIVECTOR, value);\r\n}\r\nstruct cpdma_chan *cpdma_chan_create(struct cpdma_ctlr *ctlr, int chan_num,\r\ncpdma_handler_fn handler)\r\n{\r\nstruct cpdma_chan *chan;\r\nint offset = (chan_num % CPDMA_MAX_CHANNELS) * 4;\r\nunsigned long flags;\r\nif (__chan_linear(chan_num) >= ctlr->num_chan)\r\nreturn NULL;\r\nchan = devm_kzalloc(ctlr->dev, sizeof(*chan), GFP_KERNEL);\r\nif (!chan)\r\nreturn ERR_PTR(-ENOMEM);\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nif (ctlr->channels[chan_num]) {\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\ndevm_kfree(ctlr->dev, chan);\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nchan->ctlr = ctlr;\r\nchan->state = CPDMA_STATE_IDLE;\r\nchan->chan_num = chan_num;\r\nchan->handler = handler;\r\nif (is_rx_chan(chan)) {\r\nchan->hdp = ctlr->params.rxhdp + offset;\r\nchan->cp = ctlr->params.rxcp + offset;\r\nchan->rxfree = ctlr->params.rxfree + offset;\r\nchan->int_set = CPDMA_RXINTMASKSET;\r\nchan->int_clear = CPDMA_RXINTMASKCLEAR;\r\nchan->td = CPDMA_RXTEARDOWN;\r\nchan->dir = DMA_FROM_DEVICE;\r\n} else {\r\nchan->hdp = ctlr->params.txhdp + offset;\r\nchan->cp = ctlr->params.txcp + offset;\r\nchan->int_set = CPDMA_TXINTMASKSET;\r\nchan->int_clear = CPDMA_TXINTMASKCLEAR;\r\nchan->td = CPDMA_TXTEARDOWN;\r\nchan->dir = DMA_TO_DEVICE;\r\n}\r\nchan->mask = BIT(chan_linear(chan));\r\nspin_lock_init(&chan->lock);\r\nctlr->channels[chan_num] = chan;\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn chan;\r\n}\r\nint cpdma_chan_destroy(struct cpdma_chan *chan)\r\n{\r\nstruct cpdma_ctlr *ctlr;\r\nunsigned long flags;\r\nif (!chan)\r\nreturn -EINVAL;\r\nctlr = chan->ctlr;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nif (chan->state != CPDMA_STATE_IDLE)\r\ncpdma_chan_stop(chan);\r\nctlr->channels[chan->chan_num] = NULL;\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nkfree(chan);\r\nreturn 0;\r\n}\r\nint cpdma_chan_get_stats(struct cpdma_chan *chan,\r\nstruct cpdma_chan_stats *stats)\r\n{\r\nunsigned long flags;\r\nif (!chan)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nmemcpy(stats, &chan->stats, sizeof(*stats));\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn 0;\r\n}\r\nint cpdma_chan_dump(struct cpdma_chan *chan)\r\n{\r\nunsigned long flags;\r\nstruct device *dev = chan->ctlr->dev;\r\nspin_lock_irqsave(&chan->lock, flags);\r\ndev_info(dev, "channel %d (%s %d) state %s",\r\nchan->chan_num, is_rx_chan(chan) ? "rx" : "tx",\r\nchan_linear(chan), cpdma_state_str[chan->state]);\r\ndev_info(dev, "\thdp: %x\n", chan_read(chan, hdp));\r\ndev_info(dev, "\tcp: %x\n", chan_read(chan, cp));\r\nif (chan->rxfree) {\r\ndev_info(dev, "\trxfree: %x\n",\r\nchan_read(chan, rxfree));\r\n}\r\ndev_info(dev, "\tstats head_enqueue: %d\n",\r\nchan->stats.head_enqueue);\r\ndev_info(dev, "\tstats tail_enqueue: %d\n",\r\nchan->stats.tail_enqueue);\r\ndev_info(dev, "\tstats pad_enqueue: %d\n",\r\nchan->stats.pad_enqueue);\r\ndev_info(dev, "\tstats misqueued: %d\n",\r\nchan->stats.misqueued);\r\ndev_info(dev, "\tstats desc_alloc_fail: %d\n",\r\nchan->stats.desc_alloc_fail);\r\ndev_info(dev, "\tstats pad_alloc_fail: %d\n",\r\nchan->stats.pad_alloc_fail);\r\ndev_info(dev, "\tstats runt_receive_buff: %d\n",\r\nchan->stats.runt_receive_buff);\r\ndev_info(dev, "\tstats runt_transmit_buff: %d\n",\r\nchan->stats.runt_transmit_buff);\r\ndev_info(dev, "\tstats empty_dequeue: %d\n",\r\nchan->stats.empty_dequeue);\r\ndev_info(dev, "\tstats busy_dequeue: %d\n",\r\nchan->stats.busy_dequeue);\r\ndev_info(dev, "\tstats good_dequeue: %d\n",\r\nchan->stats.good_dequeue);\r\ndev_info(dev, "\tstats requeue: %d\n",\r\nchan->stats.requeue);\r\ndev_info(dev, "\tstats teardown_dequeue: %d\n",\r\nchan->stats.teardown_dequeue);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic void __cpdma_chan_submit(struct cpdma_chan *chan,\r\nstruct cpdma_desc __iomem *desc)\r\n{\r\nstruct cpdma_ctlr *ctlr = chan->ctlr;\r\nstruct cpdma_desc __iomem *prev = chan->tail;\r\nstruct cpdma_desc_pool *pool = ctlr->pool;\r\ndma_addr_t desc_dma;\r\nu32 mode;\r\ndesc_dma = desc_phys(pool, desc);\r\nif (!chan->head) {\r\nchan->stats.head_enqueue++;\r\nchan->head = desc;\r\nchan->tail = desc;\r\nif (chan->state == CPDMA_STATE_ACTIVE)\r\nchan_write(chan, hdp, desc_dma);\r\nreturn;\r\n}\r\ndesc_write(prev, hw_next, desc_dma);\r\nchan->tail = desc;\r\nchan->stats.tail_enqueue++;\r\nmode = desc_read(prev, hw_mode);\r\nif (((mode & (CPDMA_DESC_EOQ | CPDMA_DESC_OWNER)) == CPDMA_DESC_EOQ) &&\r\n(chan->state == CPDMA_STATE_ACTIVE)) {\r\ndesc_write(prev, hw_mode, mode & ~CPDMA_DESC_EOQ);\r\nchan_write(chan, hdp, desc_dma);\r\nchan->stats.misqueued++;\r\n}\r\n}\r\nint cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,\r\nint len, int directed)\r\n{\r\nstruct cpdma_ctlr *ctlr = chan->ctlr;\r\nstruct cpdma_desc __iomem *desc;\r\ndma_addr_t buffer;\r\nunsigned long flags;\r\nu32 mode;\r\nint ret = 0;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nif (chan->state == CPDMA_STATE_TEARDOWN) {\r\nret = -EINVAL;\r\ngoto unlock_ret;\r\n}\r\ndesc = cpdma_desc_alloc(ctlr->pool, 1, is_rx_chan(chan));\r\nif (!desc) {\r\nchan->stats.desc_alloc_fail++;\r\nret = -ENOMEM;\r\ngoto unlock_ret;\r\n}\r\nif (len < ctlr->params.min_packet_size) {\r\nlen = ctlr->params.min_packet_size;\r\nchan->stats.runt_transmit_buff++;\r\n}\r\nbuffer = dma_map_single(ctlr->dev, data, len, chan->dir);\r\nret = dma_mapping_error(ctlr->dev, buffer);\r\nif (ret) {\r\ncpdma_desc_free(ctlr->pool, desc, 1);\r\nret = -EINVAL;\r\ngoto unlock_ret;\r\n}\r\nmode = CPDMA_DESC_OWNER | CPDMA_DESC_SOP | CPDMA_DESC_EOP;\r\ncpdma_desc_to_port(chan, mode, directed);\r\ndesc_write(desc, hw_next, 0);\r\ndesc_write(desc, hw_buffer, buffer);\r\ndesc_write(desc, hw_len, len);\r\ndesc_write(desc, hw_mode, mode | len);\r\ndesc_write(desc, sw_token, token);\r\ndesc_write(desc, sw_buffer, buffer);\r\ndesc_write(desc, sw_len, len);\r\n__cpdma_chan_submit(chan, desc);\r\nif (chan->state == CPDMA_STATE_ACTIVE && chan->rxfree)\r\nchan_write(chan, rxfree, 1);\r\nchan->count++;\r\nunlock_ret:\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn ret;\r\n}\r\nbool cpdma_check_free_tx_desc(struct cpdma_chan *chan)\r\n{\r\nunsigned long flags;\r\nint index;\r\nbool ret;\r\nstruct cpdma_ctlr *ctlr = chan->ctlr;\r\nstruct cpdma_desc_pool *pool = ctlr->pool;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nindex = bitmap_find_next_zero_area(pool->bitmap,\r\npool->num_desc, pool->num_desc/2, 1, 0);\r\nif (index < pool->num_desc)\r\nret = true;\r\nelse\r\nret = false;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void __cpdma_chan_free(struct cpdma_chan *chan,\r\nstruct cpdma_desc __iomem *desc,\r\nint outlen, int status)\r\n{\r\nstruct cpdma_ctlr *ctlr = chan->ctlr;\r\nstruct cpdma_desc_pool *pool = ctlr->pool;\r\ndma_addr_t buff_dma;\r\nint origlen;\r\nvoid *token;\r\ntoken = (void *)desc_read(desc, sw_token);\r\nbuff_dma = desc_read(desc, sw_buffer);\r\noriglen = desc_read(desc, sw_len);\r\ndma_unmap_single(ctlr->dev, buff_dma, origlen, chan->dir);\r\ncpdma_desc_free(pool, desc, 1);\r\n(*chan->handler)(token, outlen, status);\r\n}\r\nstatic int __cpdma_chan_process(struct cpdma_chan *chan)\r\n{\r\nstruct cpdma_ctlr *ctlr = chan->ctlr;\r\nstruct cpdma_desc __iomem *desc;\r\nint status, outlen;\r\nint cb_status = 0;\r\nstruct cpdma_desc_pool *pool = ctlr->pool;\r\ndma_addr_t desc_dma;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\ndesc = chan->head;\r\nif (!desc) {\r\nchan->stats.empty_dequeue++;\r\nstatus = -ENOENT;\r\ngoto unlock_ret;\r\n}\r\ndesc_dma = desc_phys(pool, desc);\r\nstatus = __raw_readl(&desc->hw_mode);\r\noutlen = status & 0x7ff;\r\nif (status & CPDMA_DESC_OWNER) {\r\nchan->stats.busy_dequeue++;\r\nstatus = -EBUSY;\r\ngoto unlock_ret;\r\n}\r\nif (status & CPDMA_DESC_PASS_CRC)\r\noutlen -= CPDMA_DESC_CRC_LEN;\r\nstatus = status & (CPDMA_DESC_EOQ | CPDMA_DESC_TD_COMPLETE |\r\nCPDMA_DESC_PORT_MASK);\r\nchan->head = desc_from_phys(pool, desc_read(desc, hw_next));\r\nchan_write(chan, cp, desc_dma);\r\nchan->count--;\r\nchan->stats.good_dequeue++;\r\nif (status & CPDMA_DESC_EOQ) {\r\nchan->stats.requeue++;\r\nchan_write(chan, hdp, desc_phys(pool, chan->head));\r\n}\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nif (unlikely(status & CPDMA_DESC_TD_COMPLETE))\r\ncb_status = -ENOSYS;\r\nelse\r\ncb_status = status;\r\n__cpdma_chan_free(chan, desc, outlen, cb_status);\r\nreturn status;\r\nunlock_ret:\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn status;\r\n}\r\nint cpdma_chan_process(struct cpdma_chan *chan, int quota)\r\n{\r\nint used = 0, ret = 0;\r\nif (chan->state != CPDMA_STATE_ACTIVE)\r\nreturn -EINVAL;\r\nwhile (used < quota) {\r\nret = __cpdma_chan_process(chan);\r\nif (ret < 0)\r\nbreak;\r\nused++;\r\n}\r\nreturn used;\r\n}\r\nint cpdma_chan_start(struct cpdma_chan *chan)\r\n{\r\nstruct cpdma_ctlr *ctlr = chan->ctlr;\r\nstruct cpdma_desc_pool *pool = ctlr->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nif (chan->state != CPDMA_STATE_IDLE) {\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn -EBUSY;\r\n}\r\nif (ctlr->state != CPDMA_STATE_ACTIVE) {\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn -EINVAL;\r\n}\r\ndma_reg_write(ctlr, chan->int_set, chan->mask);\r\nchan->state = CPDMA_STATE_ACTIVE;\r\nif (chan->head) {\r\nchan_write(chan, hdp, desc_phys(pool, chan->head));\r\nif (chan->rxfree)\r\nchan_write(chan, rxfree, chan->count);\r\n}\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn 0;\r\n}\r\nint cpdma_chan_stop(struct cpdma_chan *chan)\r\n{\r\nstruct cpdma_ctlr *ctlr = chan->ctlr;\r\nstruct cpdma_desc_pool *pool = ctlr->pool;\r\nunsigned long flags;\r\nint ret;\r\nunsigned timeout;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nif (chan->state == CPDMA_STATE_TEARDOWN) {\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn -EINVAL;\r\n}\r\nchan->state = CPDMA_STATE_TEARDOWN;\r\ndma_reg_write(ctlr, chan->int_clear, chan->mask);\r\ndma_reg_write(ctlr, chan->td, chan_linear(chan));\r\ntimeout = 100 * 100;\r\nwhile (timeout) {\r\nu32 cp = chan_read(chan, cp);\r\nif ((cp & CPDMA_TEARDOWN_VALUE) == CPDMA_TEARDOWN_VALUE)\r\nbreak;\r\nudelay(10);\r\ntimeout--;\r\n}\r\nWARN_ON(!timeout);\r\nchan_write(chan, cp, CPDMA_TEARDOWN_VALUE);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\ndo {\r\nret = __cpdma_chan_process(chan);\r\nif (ret < 0)\r\nbreak;\r\n} while ((ret & CPDMA_DESC_TD_COMPLETE) == 0);\r\nspin_lock_irqsave(&chan->lock, flags);\r\nwhile (chan->head) {\r\nstruct cpdma_desc __iomem *desc = chan->head;\r\ndma_addr_t next_dma;\r\nnext_dma = desc_read(desc, hw_next);\r\nchan->head = desc_from_phys(pool, next_dma);\r\nchan->count--;\r\nchan->stats.teardown_dequeue++;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n__cpdma_chan_free(chan, desc, 0, -ENOSYS);\r\nspin_lock_irqsave(&chan->lock, flags);\r\n}\r\nchan->state = CPDMA_STATE_IDLE;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn 0;\r\n}\r\nint cpdma_chan_int_ctrl(struct cpdma_chan *chan, bool enable)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nif (chan->state != CPDMA_STATE_ACTIVE) {\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn -EINVAL;\r\n}\r\ndma_reg_write(chan->ctlr, enable ? chan->int_set : chan->int_clear,\r\nchan->mask);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn 0;\r\n}\r\nint cpdma_control_get(struct cpdma_ctlr *ctlr, int control)\r\n{\r\nunsigned long flags;\r\nstruct cpdma_control_info *info = &controls[control];\r\nint ret;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nret = -ENOTSUPP;\r\nif (!ctlr->params.has_ext_regs)\r\ngoto unlock_ret;\r\nret = -EINVAL;\r\nif (ctlr->state != CPDMA_STATE_ACTIVE)\r\ngoto unlock_ret;\r\nret = -ENOENT;\r\nif (control < 0 || control >= ARRAY_SIZE(controls))\r\ngoto unlock_ret;\r\nret = -EPERM;\r\nif ((info->access & ACCESS_RO) != ACCESS_RO)\r\ngoto unlock_ret;\r\nret = (dma_reg_read(ctlr, info->reg) >> info->shift) & info->mask;\r\nunlock_ret:\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn ret;\r\n}\r\nint cpdma_control_set(struct cpdma_ctlr *ctlr, int control, int value)\r\n{\r\nunsigned long flags;\r\nstruct cpdma_control_info *info = &controls[control];\r\nint ret;\r\nu32 val;\r\nspin_lock_irqsave(&ctlr->lock, flags);\r\nret = -ENOTSUPP;\r\nif (!ctlr->params.has_ext_regs)\r\ngoto unlock_ret;\r\nret = -EINVAL;\r\nif (ctlr->state != CPDMA_STATE_ACTIVE)\r\ngoto unlock_ret;\r\nret = -ENOENT;\r\nif (control < 0 || control >= ARRAY_SIZE(controls))\r\ngoto unlock_ret;\r\nret = -EPERM;\r\nif ((info->access & ACCESS_WO) != ACCESS_WO)\r\ngoto unlock_ret;\r\nval = dma_reg_read(ctlr, info->reg);\r\nval &= ~(info->mask << info->shift);\r\nval |= (value & info->mask) << info->shift;\r\ndma_reg_write(ctlr, info->reg, val);\r\nret = 0;\r\nunlock_ret:\r\nspin_unlock_irqrestore(&ctlr->lock, flags);\r\nreturn ret;\r\n}
