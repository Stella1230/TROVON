static inline int parent(int i)\r\n{\r\nreturn (i - 1) >> 1;\r\n}\r\nstatic inline int left_child(int i)\r\n{\r\nreturn (i << 1) + 1;\r\n}\r\nstatic inline int right_child(int i)\r\n{\r\nreturn (i << 1) + 2;\r\n}\r\nstatic inline int dl_time_before(u64 a, u64 b)\r\n{\r\nreturn (s64)(a - b) < 0;\r\n}\r\nstatic void cpudl_exchange(struct cpudl *cp, int a, int b)\r\n{\r\nint cpu_a = cp->elements[a].cpu, cpu_b = cp->elements[b].cpu;\r\nswap(cp->elements[a].cpu, cp->elements[b].cpu);\r\nswap(cp->elements[a].dl , cp->elements[b].dl );\r\nswap(cp->elements[cpu_a].idx, cp->elements[cpu_b].idx);\r\n}\r\nstatic void cpudl_heapify(struct cpudl *cp, int idx)\r\n{\r\nint l, r, largest;\r\nwhile(1) {\r\nl = left_child(idx);\r\nr = right_child(idx);\r\nlargest = idx;\r\nif ((l < cp->size) && dl_time_before(cp->elements[idx].dl,\r\ncp->elements[l].dl))\r\nlargest = l;\r\nif ((r < cp->size) && dl_time_before(cp->elements[largest].dl,\r\ncp->elements[r].dl))\r\nlargest = r;\r\nif (largest == idx)\r\nbreak;\r\ncpudl_exchange(cp, largest, idx);\r\nidx = largest;\r\n}\r\n}\r\nstatic void cpudl_change_key(struct cpudl *cp, int idx, u64 new_dl)\r\n{\r\nWARN_ON(idx == IDX_INVALID || !cpu_present(idx));\r\nif (dl_time_before(new_dl, cp->elements[idx].dl)) {\r\ncp->elements[idx].dl = new_dl;\r\ncpudl_heapify(cp, idx);\r\n} else {\r\ncp->elements[idx].dl = new_dl;\r\nwhile (idx > 0 && dl_time_before(cp->elements[parent(idx)].dl,\r\ncp->elements[idx].dl)) {\r\ncpudl_exchange(cp, idx, parent(idx));\r\nidx = parent(idx);\r\n}\r\n}\r\n}\r\nstatic inline int cpudl_maximum(struct cpudl *cp)\r\n{\r\nreturn cp->elements[0].cpu;\r\n}\r\nint cpudl_find(struct cpudl *cp, struct task_struct *p,\r\nstruct cpumask *later_mask)\r\n{\r\nint best_cpu = -1;\r\nconst struct sched_dl_entity *dl_se = &p->dl;\r\nif (later_mask && cpumask_and(later_mask, cp->free_cpus,\r\n&p->cpus_allowed) && cpumask_and(later_mask,\r\nlater_mask, cpu_active_mask)) {\r\nbest_cpu = cpumask_any(later_mask);\r\ngoto out;\r\n} else if (cpumask_test_cpu(cpudl_maximum(cp), &p->cpus_allowed) &&\r\ndl_time_before(dl_se->deadline, cp->elements[0].dl)) {\r\nbest_cpu = cpudl_maximum(cp);\r\nif (later_mask)\r\ncpumask_set_cpu(best_cpu, later_mask);\r\n}\r\nout:\r\nWARN_ON(best_cpu != -1 && !cpu_present(best_cpu));\r\nreturn best_cpu;\r\n}\r\nvoid cpudl_set(struct cpudl *cp, int cpu, u64 dl, int is_valid)\r\n{\r\nint old_idx, new_cpu;\r\nunsigned long flags;\r\nWARN_ON(!cpu_present(cpu));\r\nraw_spin_lock_irqsave(&cp->lock, flags);\r\nold_idx = cp->elements[cpu].idx;\r\nif (!is_valid) {\r\nif (old_idx == IDX_INVALID) {\r\ngoto out;\r\n}\r\nnew_cpu = cp->elements[cp->size - 1].cpu;\r\ncp->elements[old_idx].dl = cp->elements[cp->size - 1].dl;\r\ncp->elements[old_idx].cpu = new_cpu;\r\ncp->size--;\r\ncp->elements[new_cpu].idx = old_idx;\r\ncp->elements[cpu].idx = IDX_INVALID;\r\nwhile (old_idx > 0 && dl_time_before(\r\ncp->elements[parent(old_idx)].dl,\r\ncp->elements[old_idx].dl)) {\r\ncpudl_exchange(cp, old_idx, parent(old_idx));\r\nold_idx = parent(old_idx);\r\n}\r\ncpumask_set_cpu(cpu, cp->free_cpus);\r\ncpudl_heapify(cp, old_idx);\r\ngoto out;\r\n}\r\nif (old_idx == IDX_INVALID) {\r\ncp->size++;\r\ncp->elements[cp->size - 1].dl = 0;\r\ncp->elements[cp->size - 1].cpu = cpu;\r\ncp->elements[cpu].idx = cp->size - 1;\r\ncpudl_change_key(cp, cp->size - 1, dl);\r\ncpumask_clear_cpu(cpu, cp->free_cpus);\r\n} else {\r\ncpudl_change_key(cp, old_idx, dl);\r\n}\r\nout:\r\nraw_spin_unlock_irqrestore(&cp->lock, flags);\r\n}\r\nint cpudl_init(struct cpudl *cp)\r\n{\r\nint i;\r\nmemset(cp, 0, sizeof(*cp));\r\nraw_spin_lock_init(&cp->lock);\r\ncp->size = 0;\r\ncp->elements = kcalloc(nr_cpu_ids,\r\nsizeof(struct cpudl_item),\r\nGFP_KERNEL);\r\nif (!cp->elements)\r\nreturn -ENOMEM;\r\nif (!alloc_cpumask_var(&cp->free_cpus, GFP_KERNEL)) {\r\nkfree(cp->elements);\r\nreturn -ENOMEM;\r\n}\r\nfor_each_possible_cpu(i)\r\ncp->elements[i].idx = IDX_INVALID;\r\ncpumask_setall(cp->free_cpus);\r\nreturn 0;\r\n}\r\nvoid cpudl_cleanup(struct cpudl *cp)\r\n{\r\nfree_cpumask_var(cp->free_cpus);\r\nkfree(cp->elements);\r\n}
