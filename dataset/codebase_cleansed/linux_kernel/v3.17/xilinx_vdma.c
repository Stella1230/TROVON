static inline u32 vdma_read(struct xilinx_vdma_chan *chan, u32 reg)\r\n{\r\nreturn ioread32(chan->xdev->regs + reg);\r\n}\r\nstatic inline void vdma_write(struct xilinx_vdma_chan *chan, u32 reg, u32 value)\r\n{\r\niowrite32(value, chan->xdev->regs + reg);\r\n}\r\nstatic inline void vdma_desc_write(struct xilinx_vdma_chan *chan, u32 reg,\r\nu32 value)\r\n{\r\nvdma_write(chan, chan->desc_offset + reg, value);\r\n}\r\nstatic inline u32 vdma_ctrl_read(struct xilinx_vdma_chan *chan, u32 reg)\r\n{\r\nreturn vdma_read(chan, chan->ctrl_offset + reg);\r\n}\r\nstatic inline void vdma_ctrl_write(struct xilinx_vdma_chan *chan, u32 reg,\r\nu32 value)\r\n{\r\nvdma_write(chan, chan->ctrl_offset + reg, value);\r\n}\r\nstatic inline void vdma_ctrl_clr(struct xilinx_vdma_chan *chan, u32 reg,\r\nu32 clr)\r\n{\r\nvdma_ctrl_write(chan, reg, vdma_ctrl_read(chan, reg) & ~clr);\r\n}\r\nstatic inline void vdma_ctrl_set(struct xilinx_vdma_chan *chan, u32 reg,\r\nu32 set)\r\n{\r\nvdma_ctrl_write(chan, reg, vdma_ctrl_read(chan, reg) | set);\r\n}\r\nstatic struct xilinx_vdma_tx_segment *\r\nxilinx_vdma_alloc_tx_segment(struct xilinx_vdma_chan *chan)\r\n{\r\nstruct xilinx_vdma_tx_segment *segment;\r\ndma_addr_t phys;\r\nsegment = dma_pool_alloc(chan->desc_pool, GFP_ATOMIC, &phys);\r\nif (!segment)\r\nreturn NULL;\r\nmemset(segment, 0, sizeof(*segment));\r\nsegment->phys = phys;\r\nreturn segment;\r\n}\r\nstatic void xilinx_vdma_free_tx_segment(struct xilinx_vdma_chan *chan,\r\nstruct xilinx_vdma_tx_segment *segment)\r\n{\r\ndma_pool_free(chan->desc_pool, segment, segment->phys);\r\n}\r\nstatic struct xilinx_vdma_tx_descriptor *\r\nxilinx_vdma_alloc_tx_descriptor(struct xilinx_vdma_chan *chan)\r\n{\r\nstruct xilinx_vdma_tx_descriptor *desc;\r\nunsigned long flags;\r\nif (chan->allocated_desc)\r\nreturn chan->allocated_desc;\r\ndesc = kzalloc(sizeof(*desc), GFP_KERNEL);\r\nif (!desc)\r\nreturn NULL;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nchan->allocated_desc = desc;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nINIT_LIST_HEAD(&desc->segments);\r\nreturn desc;\r\n}\r\nstatic void\r\nxilinx_vdma_free_tx_descriptor(struct xilinx_vdma_chan *chan,\r\nstruct xilinx_vdma_tx_descriptor *desc)\r\n{\r\nstruct xilinx_vdma_tx_segment *segment, *next;\r\nif (!desc)\r\nreturn;\r\nlist_for_each_entry_safe(segment, next, &desc->segments, node) {\r\nlist_del(&segment->node);\r\nxilinx_vdma_free_tx_segment(chan, segment);\r\n}\r\nkfree(desc);\r\n}\r\nstatic void xilinx_vdma_free_desc_list(struct xilinx_vdma_chan *chan,\r\nstruct list_head *list)\r\n{\r\nstruct xilinx_vdma_tx_descriptor *desc, *next;\r\nlist_for_each_entry_safe(desc, next, list, node) {\r\nlist_del(&desc->node);\r\nxilinx_vdma_free_tx_descriptor(chan, desc);\r\n}\r\n}\r\nstatic void xilinx_vdma_free_descriptors(struct xilinx_vdma_chan *chan)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nxilinx_vdma_free_desc_list(chan, &chan->pending_list);\r\nxilinx_vdma_free_desc_list(chan, &chan->done_list);\r\nxilinx_vdma_free_tx_descriptor(chan, chan->active_desc);\r\nchan->active_desc = NULL;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void xilinx_vdma_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);\r\ndev_dbg(chan->dev, "Free all channel resources.\n");\r\nxilinx_vdma_free_descriptors(chan);\r\ndma_pool_destroy(chan->desc_pool);\r\nchan->desc_pool = NULL;\r\n}\r\nstatic void xilinx_vdma_chan_desc_cleanup(struct xilinx_vdma_chan *chan)\r\n{\r\nstruct xilinx_vdma_tx_descriptor *desc, *next;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nlist_for_each_entry_safe(desc, next, &chan->done_list, node) {\r\ndma_async_tx_callback callback;\r\nvoid *callback_param;\r\nlist_del(&desc->node);\r\ncallback = desc->async_tx.callback;\r\ncallback_param = desc->async_tx.callback_param;\r\nif (callback) {\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\ncallback(callback_param);\r\nspin_lock_irqsave(&chan->lock, flags);\r\n}\r\ndma_run_dependencies(&desc->async_tx);\r\nxilinx_vdma_free_tx_descriptor(chan, desc);\r\n}\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void xilinx_vdma_do_tasklet(unsigned long data)\r\n{\r\nstruct xilinx_vdma_chan *chan = (struct xilinx_vdma_chan *)data;\r\nxilinx_vdma_chan_desc_cleanup(chan);\r\n}\r\nstatic int xilinx_vdma_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);\r\nif (chan->desc_pool)\r\nreturn 0;\r\nchan->desc_pool = dma_pool_create("xilinx_vdma_desc_pool",\r\nchan->dev,\r\nsizeof(struct xilinx_vdma_tx_segment),\r\n__alignof__(struct xilinx_vdma_tx_segment), 0);\r\nif (!chan->desc_pool) {\r\ndev_err(chan->dev,\r\n"unable to allocate channel %d descriptor pool\n",\r\nchan->id);\r\nreturn -ENOMEM;\r\n}\r\ndma_cookie_init(dchan);\r\nreturn 0;\r\n}\r\nstatic enum dma_status xilinx_vdma_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nreturn dma_cookie_status(dchan, cookie, txstate);\r\n}\r\nstatic bool xilinx_vdma_is_running(struct xilinx_vdma_chan *chan)\r\n{\r\nreturn !(vdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR) &\r\nXILINX_VDMA_DMASR_HALTED) &&\r\n(vdma_ctrl_read(chan, XILINX_VDMA_REG_DMACR) &\r\nXILINX_VDMA_DMACR_RUNSTOP);\r\n}\r\nstatic bool xilinx_vdma_is_idle(struct xilinx_vdma_chan *chan)\r\n{\r\nreturn vdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR) &\r\nXILINX_VDMA_DMASR_IDLE;\r\n}\r\nstatic void xilinx_vdma_halt(struct xilinx_vdma_chan *chan)\r\n{\r\nint loop = XILINX_VDMA_LOOP_COUNT;\r\nvdma_ctrl_clr(chan, XILINX_VDMA_REG_DMACR, XILINX_VDMA_DMACR_RUNSTOP);\r\ndo {\r\nif (vdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR) &\r\nXILINX_VDMA_DMASR_HALTED)\r\nbreak;\r\n} while (loop--);\r\nif (!loop) {\r\ndev_err(chan->dev, "Cannot stop channel %p: %x\n",\r\nchan, vdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR));\r\nchan->err = true;\r\n}\r\nreturn;\r\n}\r\nstatic void xilinx_vdma_start(struct xilinx_vdma_chan *chan)\r\n{\r\nint loop = XILINX_VDMA_LOOP_COUNT;\r\nvdma_ctrl_set(chan, XILINX_VDMA_REG_DMACR, XILINX_VDMA_DMACR_RUNSTOP);\r\ndo {\r\nif (!(vdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR) &\r\nXILINX_VDMA_DMASR_HALTED))\r\nbreak;\r\n} while (loop--);\r\nif (!loop) {\r\ndev_err(chan->dev, "Cannot start channel %p: %x\n",\r\nchan, vdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR));\r\nchan->err = true;\r\n}\r\nreturn;\r\n}\r\nstatic void xilinx_vdma_start_transfer(struct xilinx_vdma_chan *chan)\r\n{\r\nstruct xilinx_vdma_config *config = &chan->config;\r\nstruct xilinx_vdma_tx_descriptor *desc;\r\nunsigned long flags;\r\nu32 reg;\r\nstruct xilinx_vdma_tx_segment *head, *tail = NULL;\r\nif (chan->err)\r\nreturn;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nif (chan->active_desc)\r\ngoto out_unlock;\r\nif (list_empty(&chan->pending_list))\r\ngoto out_unlock;\r\ndesc = list_first_entry(&chan->pending_list,\r\nstruct xilinx_vdma_tx_descriptor, node);\r\nif (chan->has_sg && xilinx_vdma_is_running(chan) &&\r\n!xilinx_vdma_is_idle(chan)) {\r\ndev_dbg(chan->dev, "DMA controller still busy\n");\r\ngoto out_unlock;\r\n}\r\nif (chan->has_sg) {\r\nhead = list_first_entry(&desc->segments,\r\nstruct xilinx_vdma_tx_segment, node);\r\ntail = list_entry(desc->segments.prev,\r\nstruct xilinx_vdma_tx_segment, node);\r\nvdma_ctrl_write(chan, XILINX_VDMA_REG_CURDESC, head->phys);\r\n}\r\nreg = vdma_ctrl_read(chan, XILINX_VDMA_REG_DMACR);\r\nif (config->frm_cnt_en)\r\nreg |= XILINX_VDMA_DMACR_FRAMECNT_EN;\r\nelse\r\nreg &= ~XILINX_VDMA_DMACR_FRAMECNT_EN;\r\nif (chan->has_sg || !config->park)\r\nreg |= XILINX_VDMA_DMACR_CIRC_EN;\r\nif (config->park)\r\nreg &= ~XILINX_VDMA_DMACR_CIRC_EN;\r\nvdma_ctrl_write(chan, XILINX_VDMA_REG_DMACR, reg);\r\nif (config->park && (config->park_frm >= 0) &&\r\n(config->park_frm < chan->num_frms)) {\r\nif (chan->direction == DMA_MEM_TO_DEV)\r\nvdma_write(chan, XILINX_VDMA_REG_PARK_PTR,\r\nconfig->park_frm <<\r\nXILINX_VDMA_PARK_PTR_RD_REF_SHIFT);\r\nelse\r\nvdma_write(chan, XILINX_VDMA_REG_PARK_PTR,\r\nconfig->park_frm <<\r\nXILINX_VDMA_PARK_PTR_WR_REF_SHIFT);\r\n}\r\nxilinx_vdma_start(chan);\r\nif (chan->err)\r\ngoto out_unlock;\r\nif (chan->has_sg) {\r\nvdma_ctrl_write(chan, XILINX_VDMA_REG_TAILDESC, tail->phys);\r\n} else {\r\nstruct xilinx_vdma_tx_segment *segment, *last = NULL;\r\nint i = 0;\r\nlist_for_each_entry(segment, &desc->segments, node) {\r\nvdma_desc_write(chan,\r\nXILINX_VDMA_REG_START_ADDRESS(i++),\r\nsegment->hw.buf_addr);\r\nlast = segment;\r\n}\r\nif (!last)\r\ngoto out_unlock;\r\nvdma_desc_write(chan, XILINX_VDMA_REG_HSIZE, last->hw.hsize);\r\nvdma_desc_write(chan, XILINX_VDMA_REG_FRMDLY_STRIDE,\r\nlast->hw.stride);\r\nvdma_desc_write(chan, XILINX_VDMA_REG_VSIZE, last->hw.vsize);\r\n}\r\nlist_del(&desc->node);\r\nchan->active_desc = desc;\r\nout_unlock:\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void xilinx_vdma_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);\r\nxilinx_vdma_start_transfer(chan);\r\n}\r\nstatic void xilinx_vdma_complete_descriptor(struct xilinx_vdma_chan *chan)\r\n{\r\nstruct xilinx_vdma_tx_descriptor *desc;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\ndesc = chan->active_desc;\r\nif (!desc) {\r\ndev_dbg(chan->dev, "no running descriptors\n");\r\ngoto out_unlock;\r\n}\r\ndma_cookie_complete(&desc->async_tx);\r\nlist_add_tail(&desc->node, &chan->done_list);\r\nchan->active_desc = NULL;\r\nout_unlock:\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic int xilinx_vdma_reset(struct xilinx_vdma_chan *chan)\r\n{\r\nint loop = XILINX_VDMA_LOOP_COUNT;\r\nu32 tmp;\r\nvdma_ctrl_set(chan, XILINX_VDMA_REG_DMACR, XILINX_VDMA_DMACR_RESET);\r\ntmp = vdma_ctrl_read(chan, XILINX_VDMA_REG_DMACR) &\r\nXILINX_VDMA_DMACR_RESET;\r\ndo {\r\ntmp = vdma_ctrl_read(chan, XILINX_VDMA_REG_DMACR) &\r\nXILINX_VDMA_DMACR_RESET;\r\n} while (loop-- && tmp);\r\nif (!loop) {\r\ndev_err(chan->dev, "reset timeout, cr %x, sr %x\n",\r\nvdma_ctrl_read(chan, XILINX_VDMA_REG_DMACR),\r\nvdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR));\r\nreturn -ETIMEDOUT;\r\n}\r\nchan->err = false;\r\nreturn 0;\r\n}\r\nstatic int xilinx_vdma_chan_reset(struct xilinx_vdma_chan *chan)\r\n{\r\nint err;\r\nerr = xilinx_vdma_reset(chan);\r\nif (err)\r\nreturn err;\r\nvdma_ctrl_set(chan, XILINX_VDMA_REG_DMACR,\r\nXILINX_VDMA_DMAXR_ALL_IRQ_MASK);\r\nreturn 0;\r\n}\r\nstatic irqreturn_t xilinx_vdma_irq_handler(int irq, void *data)\r\n{\r\nstruct xilinx_vdma_chan *chan = data;\r\nu32 status;\r\nstatus = vdma_ctrl_read(chan, XILINX_VDMA_REG_DMASR);\r\nif (!(status & XILINX_VDMA_DMAXR_ALL_IRQ_MASK))\r\nreturn IRQ_NONE;\r\nvdma_ctrl_write(chan, XILINX_VDMA_REG_DMASR,\r\nstatus & XILINX_VDMA_DMAXR_ALL_IRQ_MASK);\r\nif (status & XILINX_VDMA_DMASR_ERR_IRQ) {\r\nu32 errors = status & XILINX_VDMA_DMASR_ALL_ERR_MASK;\r\nvdma_ctrl_write(chan, XILINX_VDMA_REG_DMASR,\r\nerrors & XILINX_VDMA_DMASR_ERR_RECOVER_MASK);\r\nif (!chan->flush_on_fsync ||\r\n(errors & ~XILINX_VDMA_DMASR_ERR_RECOVER_MASK)) {\r\ndev_err(chan->dev,\r\n"Channel %p has errors %x, cdr %x tdr %x\n",\r\nchan, errors,\r\nvdma_ctrl_read(chan, XILINX_VDMA_REG_CURDESC),\r\nvdma_ctrl_read(chan, XILINX_VDMA_REG_TAILDESC));\r\nchan->err = true;\r\n}\r\n}\r\nif (status & XILINX_VDMA_DMASR_DLY_CNT_IRQ) {\r\ndev_dbg(chan->dev, "Inter-packet latency too long\n");\r\n}\r\nif (status & XILINX_VDMA_DMASR_FRM_CNT_IRQ) {\r\nxilinx_vdma_complete_descriptor(chan);\r\nxilinx_vdma_start_transfer(chan);\r\n}\r\ntasklet_schedule(&chan->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic dma_cookie_t xilinx_vdma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct xilinx_vdma_tx_descriptor *desc = to_vdma_tx_descriptor(tx);\r\nstruct xilinx_vdma_chan *chan = to_xilinx_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nunsigned long flags;\r\nint err;\r\nif (chan->err) {\r\nerr = xilinx_vdma_chan_reset(chan);\r\nif (err < 0)\r\nreturn err;\r\n}\r\nspin_lock_irqsave(&chan->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\nlist_add_tail(&desc->node, &chan->pending_list);\r\nchan->allocated_desc = NULL;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nxilinx_vdma_dma_prep_interleaved(struct dma_chan *dchan,\r\nstruct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);\r\nstruct xilinx_vdma_tx_descriptor *desc;\r\nstruct xilinx_vdma_tx_segment *segment, *prev = NULL;\r\nstruct xilinx_vdma_desc_hw *hw;\r\nif (!is_slave_direction(xt->dir))\r\nreturn NULL;\r\nif (!xt->numf || !xt->sgl[0].size)\r\nreturn NULL;\r\ndesc = xilinx_vdma_alloc_tx_descriptor(chan);\r\nif (!desc)\r\nreturn NULL;\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\r\ndesc->async_tx.tx_submit = xilinx_vdma_tx_submit;\r\nasync_tx_ack(&desc->async_tx);\r\nsegment = xilinx_vdma_alloc_tx_segment(chan);\r\nif (!segment)\r\ngoto error;\r\nhw = &segment->hw;\r\nhw->vsize = xt->numf;\r\nhw->hsize = xt->sgl[0].size;\r\nhw->stride = xt->sgl[0].icg <<\r\nXILINX_VDMA_FRMDLY_STRIDE_STRIDE_SHIFT;\r\nhw->stride |= chan->config.frm_dly <<\r\nXILINX_VDMA_FRMDLY_STRIDE_FRMDLY_SHIFT;\r\nif (xt->dir != DMA_MEM_TO_DEV)\r\nhw->buf_addr = xt->dst_start;\r\nelse\r\nhw->buf_addr = xt->src_start;\r\nprev = list_last_entry(&desc->segments,\r\nstruct xilinx_vdma_tx_segment, node);\r\nprev->hw.next_desc = segment->phys;\r\nlist_add_tail(&segment->node, &desc->segments);\r\nprev = segment;\r\nsegment = list_first_entry(&desc->segments,\r\nstruct xilinx_vdma_tx_segment, node);\r\nprev->hw.next_desc = segment->phys;\r\nreturn &desc->async_tx;\r\nerror:\r\nxilinx_vdma_free_tx_descriptor(chan, desc);\r\nreturn NULL;\r\n}\r\nstatic void xilinx_vdma_terminate_all(struct xilinx_vdma_chan *chan)\r\n{\r\nxilinx_vdma_halt(chan);\r\nxilinx_vdma_free_descriptors(chan);\r\n}\r\nint xilinx_vdma_channel_set_config(struct dma_chan *dchan,\r\nstruct xilinx_vdma_config *cfg)\r\n{\r\nstruct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);\r\nu32 dmacr;\r\nif (cfg->reset)\r\nreturn xilinx_vdma_chan_reset(chan);\r\ndmacr = vdma_ctrl_read(chan, XILINX_VDMA_REG_DMACR);\r\nchan->config.frm_dly = cfg->frm_dly;\r\nchan->config.park = cfg->park;\r\nchan->config.gen_lock = cfg->gen_lock;\r\nchan->config.master = cfg->master;\r\nif (cfg->gen_lock && chan->genlock) {\r\ndmacr |= XILINX_VDMA_DMACR_GENLOCK_EN;\r\ndmacr |= cfg->master << XILINX_VDMA_DMACR_MASTER_SHIFT;\r\n}\r\nchan->config.frm_cnt_en = cfg->frm_cnt_en;\r\nif (cfg->park)\r\nchan->config.park_frm = cfg->park_frm;\r\nelse\r\nchan->config.park_frm = -1;\r\nchan->config.coalesc = cfg->coalesc;\r\nchan->config.delay = cfg->delay;\r\nif (cfg->coalesc <= XILINX_VDMA_DMACR_FRAME_COUNT_MAX) {\r\ndmacr |= cfg->coalesc << XILINX_VDMA_DMACR_FRAME_COUNT_SHIFT;\r\nchan->config.coalesc = cfg->coalesc;\r\n}\r\nif (cfg->delay <= XILINX_VDMA_DMACR_DELAY_MAX) {\r\ndmacr |= cfg->delay << XILINX_VDMA_DMACR_DELAY_SHIFT;\r\nchan->config.delay = cfg->delay;\r\n}\r\ndmacr &= ~XILINX_VDMA_DMACR_FSYNCSRC_MASK;\r\ndmacr |= cfg->ext_fsync << XILINX_VDMA_DMACR_FSYNCSRC_SHIFT;\r\nvdma_ctrl_write(chan, XILINX_VDMA_REG_DMACR, dmacr);\r\nreturn 0;\r\n}\r\nstatic int xilinx_vdma_device_control(struct dma_chan *dchan,\r\nenum dma_ctrl_cmd cmd, unsigned long arg)\r\n{\r\nstruct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);\r\nif (cmd != DMA_TERMINATE_ALL)\r\nreturn -ENXIO;\r\nxilinx_vdma_terminate_all(chan);\r\nreturn 0;\r\n}\r\nstatic void xilinx_vdma_chan_remove(struct xilinx_vdma_chan *chan)\r\n{\r\nvdma_ctrl_clr(chan, XILINX_VDMA_REG_DMACR,\r\nXILINX_VDMA_DMAXR_ALL_IRQ_MASK);\r\nif (chan->irq > 0)\r\nfree_irq(chan->irq, chan);\r\ntasklet_kill(&chan->tasklet);\r\nlist_del(&chan->common.device_node);\r\n}\r\nstatic int xilinx_vdma_chan_probe(struct xilinx_vdma_device *xdev,\r\nstruct device_node *node)\r\n{\r\nstruct xilinx_vdma_chan *chan;\r\nbool has_dre = false;\r\nu32 value, width;\r\nint err;\r\nchan = devm_kzalloc(xdev->dev, sizeof(*chan), GFP_KERNEL);\r\nif (!chan)\r\nreturn -ENOMEM;\r\nchan->dev = xdev->dev;\r\nchan->xdev = xdev;\r\nchan->has_sg = xdev->has_sg;\r\nspin_lock_init(&chan->lock);\r\nINIT_LIST_HEAD(&chan->pending_list);\r\nINIT_LIST_HEAD(&chan->done_list);\r\nhas_dre = of_property_read_bool(node, "xlnx,include-dre");\r\nchan->genlock = of_property_read_bool(node, "xlnx,genlock-mode");\r\nerr = of_property_read_u32(node, "xlnx,datawidth", &value);\r\nif (err) {\r\ndev_err(xdev->dev, "missing xlnx,datawidth property\n");\r\nreturn err;\r\n}\r\nwidth = value >> 3;\r\nif (width > 8)\r\nhas_dre = false;\r\nif (!has_dre)\r\nxdev->common.copy_align = fls(width - 1);\r\nif (of_device_is_compatible(node, "xlnx,axi-vdma-mm2s-channel")) {\r\nchan->direction = DMA_MEM_TO_DEV;\r\nchan->id = 0;\r\nchan->ctrl_offset = XILINX_VDMA_MM2S_CTRL_OFFSET;\r\nchan->desc_offset = XILINX_VDMA_MM2S_DESC_OFFSET;\r\nif (xdev->flush_on_fsync == XILINX_VDMA_FLUSH_BOTH ||\r\nxdev->flush_on_fsync == XILINX_VDMA_FLUSH_MM2S)\r\nchan->flush_on_fsync = true;\r\n} else if (of_device_is_compatible(node,\r\n"xlnx,axi-vdma-s2mm-channel")) {\r\nchan->direction = DMA_DEV_TO_MEM;\r\nchan->id = 1;\r\nchan->ctrl_offset = XILINX_VDMA_S2MM_CTRL_OFFSET;\r\nchan->desc_offset = XILINX_VDMA_S2MM_DESC_OFFSET;\r\nif (xdev->flush_on_fsync == XILINX_VDMA_FLUSH_BOTH ||\r\nxdev->flush_on_fsync == XILINX_VDMA_FLUSH_S2MM)\r\nchan->flush_on_fsync = true;\r\n} else {\r\ndev_err(xdev->dev, "Invalid channel compatible node\n");\r\nreturn -EINVAL;\r\n}\r\nchan->irq = irq_of_parse_and_map(node, 0);\r\nerr = request_irq(chan->irq, xilinx_vdma_irq_handler, IRQF_SHARED,\r\n"xilinx-vdma-controller", chan);\r\nif (err) {\r\ndev_err(xdev->dev, "unable to request IRQ %d\n", chan->irq);\r\nreturn err;\r\n}\r\ntasklet_init(&chan->tasklet, xilinx_vdma_do_tasklet,\r\n(unsigned long)chan);\r\nchan->common.device = &xdev->common;\r\nlist_add_tail(&chan->common.device_node, &xdev->common.channels);\r\nxdev->chan[chan->id] = chan;\r\nerr = xilinx_vdma_chan_reset(chan);\r\nif (err < 0) {\r\ndev_err(xdev->dev, "Reset channel failed\n");\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct dma_chan *of_dma_xilinx_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct xilinx_vdma_device *xdev = ofdma->of_dma_data;\r\nint chan_id = dma_spec->args[0];\r\nif (chan_id >= XILINX_VDMA_MAX_CHANS_PER_DEVICE)\r\nreturn NULL;\r\nreturn dma_get_slave_channel(&xdev->chan[chan_id]->common);\r\n}\r\nstatic int xilinx_vdma_probe(struct platform_device *pdev)\r\n{\r\nstruct device_node *node = pdev->dev.of_node;\r\nstruct xilinx_vdma_device *xdev;\r\nstruct device_node *child;\r\nstruct resource *io;\r\nu32 num_frames;\r\nint i, err;\r\nxdev = devm_kzalloc(&pdev->dev, sizeof(*xdev), GFP_KERNEL);\r\nif (!xdev)\r\nreturn -ENOMEM;\r\nxdev->dev = &pdev->dev;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nxdev->regs = devm_ioremap_resource(&pdev->dev, io);\r\nif (IS_ERR(xdev->regs))\r\nreturn PTR_ERR(xdev->regs);\r\nxdev->has_sg = of_property_read_bool(node, "xlnx,include-sg");\r\nerr = of_property_read_u32(node, "xlnx,num-fstores", &num_frames);\r\nif (err < 0) {\r\ndev_err(xdev->dev, "missing xlnx,num-fstores property\n");\r\nreturn err;\r\n}\r\nerr = of_property_read_u32(node, "xlnx,flush-fsync",\r\n&xdev->flush_on_fsync);\r\nif (err < 0)\r\ndev_warn(xdev->dev, "missing xlnx,flush-fsync property\n");\r\nxdev->common.dev = &pdev->dev;\r\nINIT_LIST_HEAD(&xdev->common.channels);\r\ndma_cap_set(DMA_SLAVE, xdev->common.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);\r\nxdev->common.device_alloc_chan_resources =\r\nxilinx_vdma_alloc_chan_resources;\r\nxdev->common.device_free_chan_resources =\r\nxilinx_vdma_free_chan_resources;\r\nxdev->common.device_prep_interleaved_dma =\r\nxilinx_vdma_dma_prep_interleaved;\r\nxdev->common.device_control = xilinx_vdma_device_control;\r\nxdev->common.device_tx_status = xilinx_vdma_tx_status;\r\nxdev->common.device_issue_pending = xilinx_vdma_issue_pending;\r\nplatform_set_drvdata(pdev, xdev);\r\nfor_each_child_of_node(node, child) {\r\nerr = xilinx_vdma_chan_probe(xdev, child);\r\nif (err < 0)\r\ngoto error;\r\n}\r\nfor (i = 0; i < XILINX_VDMA_MAX_CHANS_PER_DEVICE; i++)\r\nif (xdev->chan[i])\r\nxdev->chan[i]->num_frms = num_frames;\r\ndma_async_device_register(&xdev->common);\r\nerr = of_dma_controller_register(node, of_dma_xilinx_xlate,\r\nxdev);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "Unable to register DMA to DT\n");\r\ndma_async_device_unregister(&xdev->common);\r\ngoto error;\r\n}\r\ndev_info(&pdev->dev, "Xilinx AXI VDMA Engine Driver Probed!!\n");\r\nreturn 0;\r\nerror:\r\nfor (i = 0; i < XILINX_VDMA_MAX_CHANS_PER_DEVICE; i++)\r\nif (xdev->chan[i])\r\nxilinx_vdma_chan_remove(xdev->chan[i]);\r\nreturn err;\r\n}\r\nstatic int xilinx_vdma_remove(struct platform_device *pdev)\r\n{\r\nstruct xilinx_vdma_device *xdev = platform_get_drvdata(pdev);\r\nint i;\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&xdev->common);\r\nfor (i = 0; i < XILINX_VDMA_MAX_CHANS_PER_DEVICE; i++)\r\nif (xdev->chan[i])\r\nxilinx_vdma_chan_remove(xdev->chan[i]);\r\nreturn 0;\r\n}
