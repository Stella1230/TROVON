static u64 notrace jiffy_sched_clock_read(void)\r\n{\r\nreturn (u64)(jiffies - INITIAL_JIFFIES);\r\n}\r\nstatic inline u64 notrace cyc_to_ns(u64 cyc, u32 mult, u32 shift)\r\n{\r\nreturn (cyc * mult) >> shift;\r\n}\r\nunsigned long long notrace sched_clock(void)\r\n{\r\nu64 cyc, res;\r\nunsigned long seq;\r\nstruct clock_read_data *rd;\r\ndo {\r\nseq = raw_read_seqcount(&cd.seq);\r\nrd = cd.read_data + (seq & 1);\r\ncyc = (rd->read_sched_clock() - rd->epoch_cyc) &\r\nrd->sched_clock_mask;\r\nres = rd->epoch_ns + cyc_to_ns(cyc, rd->mult, rd->shift);\r\n} while (read_seqcount_retry(&cd.seq, seq));\r\nreturn res;\r\n}\r\nstatic void update_clock_read_data(struct clock_read_data *rd)\r\n{\r\ncd.read_data[1] = *rd;\r\nraw_write_seqcount_latch(&cd.seq);\r\ncd.read_data[0] = *rd;\r\nraw_write_seqcount_latch(&cd.seq);\r\n}\r\nstatic void update_sched_clock(void)\r\n{\r\nu64 cyc;\r\nu64 ns;\r\nstruct clock_read_data rd;\r\nrd = cd.read_data[0];\r\ncyc = cd.actual_read_sched_clock();\r\nns = rd.epoch_ns + cyc_to_ns((cyc - rd.epoch_cyc) & rd.sched_clock_mask, rd.mult, rd.shift);\r\nrd.epoch_ns = ns;\r\nrd.epoch_cyc = cyc;\r\nupdate_clock_read_data(&rd);\r\n}\r\nstatic enum hrtimer_restart sched_clock_poll(struct hrtimer *hrt)\r\n{\r\nupdate_sched_clock();\r\nhrtimer_forward_now(hrt, cd.wrap_kt);\r\nreturn HRTIMER_RESTART;\r\n}\r\nvoid __init\r\nsched_clock_register(u64 (*read)(void), int bits, unsigned long rate)\r\n{\r\nu64 res, wrap, new_mask, new_epoch, cyc, ns;\r\nu32 new_mult, new_shift;\r\nunsigned long r;\r\nchar r_unit;\r\nstruct clock_read_data rd;\r\nif (cd.rate > rate)\r\nreturn;\r\nWARN_ON(!irqs_disabled());\r\nclocks_calc_mult_shift(&new_mult, &new_shift, rate, NSEC_PER_SEC, 3600);\r\nnew_mask = CLOCKSOURCE_MASK(bits);\r\ncd.rate = rate;\r\nwrap = clocks_calc_max_nsecs(new_mult, new_shift, 0, new_mask, NULL);\r\ncd.wrap_kt = ns_to_ktime(wrap);\r\nrd = cd.read_data[0];\r\nnew_epoch = read();\r\ncyc = cd.actual_read_sched_clock();\r\nns = rd.epoch_ns + cyc_to_ns((cyc - rd.epoch_cyc) & rd.sched_clock_mask, rd.mult, rd.shift);\r\ncd.actual_read_sched_clock = read;\r\nrd.read_sched_clock = read;\r\nrd.sched_clock_mask = new_mask;\r\nrd.mult = new_mult;\r\nrd.shift = new_shift;\r\nrd.epoch_cyc = new_epoch;\r\nrd.epoch_ns = ns;\r\nupdate_clock_read_data(&rd);\r\nif (sched_clock_timer.function != NULL) {\r\nhrtimer_start(&sched_clock_timer, cd.wrap_kt, HRTIMER_MODE_REL);\r\n}\r\nr = rate;\r\nif (r >= 4000000) {\r\nr /= 1000000;\r\nr_unit = 'M';\r\n} else {\r\nif (r >= 1000) {\r\nr /= 1000;\r\nr_unit = 'k';\r\n} else {\r\nr_unit = ' ';\r\n}\r\n}\r\nres = cyc_to_ns(1ULL, new_mult, new_shift);\r\npr_info("sched_clock: %u bits at %lu%cHz, resolution %lluns, wraps every %lluns\n",\r\nbits, r, r_unit, res, wrap);\r\nif (irqtime > 0 || (irqtime == -1 && rate >= 1000000))\r\nenable_sched_clock_irqtime();\r\npr_debug("Registered %pF as sched_clock source\n", read);\r\n}\r\nvoid __init sched_clock_postinit(void)\r\n{\r\nif (cd.actual_read_sched_clock == jiffy_sched_clock_read)\r\nsched_clock_register(jiffy_sched_clock_read, BITS_PER_LONG, HZ);\r\nupdate_sched_clock();\r\nhrtimer_init(&sched_clock_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nsched_clock_timer.function = sched_clock_poll;\r\nhrtimer_start(&sched_clock_timer, cd.wrap_kt, HRTIMER_MODE_REL);\r\n}\r\nstatic u64 notrace suspended_sched_clock_read(void)\r\n{\r\nunsigned long seq = raw_read_seqcount(&cd.seq);\r\nreturn cd.read_data[seq & 1].epoch_cyc;\r\n}\r\nstatic int sched_clock_suspend(void)\r\n{\r\nstruct clock_read_data *rd = &cd.read_data[0];\r\nupdate_sched_clock();\r\nhrtimer_cancel(&sched_clock_timer);\r\nrd->read_sched_clock = suspended_sched_clock_read;\r\nreturn 0;\r\n}\r\nstatic void sched_clock_resume(void)\r\n{\r\nstruct clock_read_data *rd = &cd.read_data[0];\r\nrd->epoch_cyc = cd.actual_read_sched_clock();\r\nhrtimer_start(&sched_clock_timer, cd.wrap_kt, HRTIMER_MODE_REL);\r\nrd->read_sched_clock = cd.actual_read_sched_clock;\r\n}\r\nstatic int __init sched_clock_syscore_init(void)\r\n{\r\nregister_syscore_ops(&sched_clock_ops);\r\nreturn 0;\r\n}
