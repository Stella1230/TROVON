static int tcmu_genl_cmd_done(struct genl_info *info, int completed_cmd)\r\n{\r\nstruct se_device *dev;\r\nstruct tcmu_dev *udev;\r\nstruct tcmu_nl_cmd *nl_cmd;\r\nint dev_id, rc, ret = 0;\r\nbool is_removed = (completed_cmd == TCMU_CMD_REMOVED_DEVICE);\r\nif (!info->attrs[TCMU_ATTR_CMD_STATUS] ||\r\n!info->attrs[TCMU_ATTR_DEVICE_ID]) {\r\nprintk(KERN_ERR "TCMU_ATTR_CMD_STATUS or TCMU_ATTR_DEVICE_ID not set, doing nothing\n");\r\nreturn -EINVAL;\r\n}\r\ndev_id = nla_get_u32(info->attrs[TCMU_ATTR_DEVICE_ID]);\r\nrc = nla_get_s32(info->attrs[TCMU_ATTR_CMD_STATUS]);\r\ndev = target_find_device(dev_id, !is_removed);\r\nif (!dev) {\r\nprintk(KERN_ERR "tcmu nl cmd %u/%u completion could not find device with dev id %u.\n",\r\ncompleted_cmd, rc, dev_id);\r\nreturn -ENODEV;\r\n}\r\nudev = TCMU_DEV(dev);\r\nspin_lock(&udev->nl_cmd_lock);\r\nnl_cmd = &udev->curr_nl_cmd;\r\npr_debug("genl cmd done got id %d curr %d done %d rc %d\n", dev_id,\r\nnl_cmd->cmd, completed_cmd, rc);\r\nif (nl_cmd->cmd != completed_cmd) {\r\nprintk(KERN_ERR "Mismatched commands (Expecting reply for %d. Current %d).\n",\r\ncompleted_cmd, nl_cmd->cmd);\r\nret = -EINVAL;\r\n} else {\r\nnl_cmd->status = rc;\r\n}\r\nspin_unlock(&udev->nl_cmd_lock);\r\nif (!is_removed)\r\ntarget_undepend_item(&dev->dev_group.cg_item);\r\nif (!ret)\r\ncomplete(&nl_cmd->complete);\r\nreturn ret;\r\n}\r\nstatic int tcmu_genl_rm_dev_done(struct sk_buff *skb, struct genl_info *info)\r\n{\r\nreturn tcmu_genl_cmd_done(info, TCMU_CMD_REMOVED_DEVICE);\r\n}\r\nstatic int tcmu_genl_add_dev_done(struct sk_buff *skb, struct genl_info *info)\r\n{\r\nreturn tcmu_genl_cmd_done(info, TCMU_CMD_ADDED_DEVICE);\r\n}\r\nstatic int tcmu_genl_reconfig_dev_done(struct sk_buff *skb,\r\nstruct genl_info *info)\r\n{\r\nreturn tcmu_genl_cmd_done(info, TCMU_CMD_RECONFIG_DEVICE);\r\n}\r\nstatic int tcmu_genl_set_features(struct sk_buff *skb, struct genl_info *info)\r\n{\r\nif (info->attrs[TCMU_ATTR_SUPP_KERN_CMD_REPLY]) {\r\ntcmu_kern_cmd_reply_supported =\r\nnla_get_u8(info->attrs[TCMU_ATTR_SUPP_KERN_CMD_REPLY]);\r\nprintk(KERN_INFO "tcmu daemon: command reply support %u.\n",\r\ntcmu_kern_cmd_reply_supported);\r\n}\r\nreturn 0;\r\n}\r\nstatic void tcmu_cmd_free_data(struct tcmu_cmd *tcmu_cmd, uint32_t len)\r\n{\r\nstruct tcmu_dev *udev = tcmu_cmd->tcmu_dev;\r\nuint32_t i;\r\nfor (i = 0; i < len; i++)\r\nclear_bit(tcmu_cmd->dbi[i], udev->data_bitmap);\r\n}\r\nstatic inline bool tcmu_get_empty_block(struct tcmu_dev *udev,\r\nstruct tcmu_cmd *tcmu_cmd)\r\n{\r\nstruct page *page;\r\nint ret, dbi;\r\ndbi = find_first_zero_bit(udev->data_bitmap, udev->dbi_thresh);\r\nif (dbi == udev->dbi_thresh)\r\nreturn false;\r\npage = radix_tree_lookup(&udev->data_blocks, dbi);\r\nif (!page) {\r\nif (atomic_add_return(1, &global_db_count) >\r\nTCMU_GLOBAL_MAX_BLOCKS) {\r\natomic_dec(&global_db_count);\r\nreturn false;\r\n}\r\npage = alloc_page(GFP_KERNEL);\r\nif (!page)\r\ngoto err_alloc;\r\nret = radix_tree_insert(&udev->data_blocks, dbi, page);\r\nif (ret)\r\ngoto err_insert;\r\n}\r\nif (dbi > udev->dbi_max)\r\nudev->dbi_max = dbi;\r\nset_bit(dbi, udev->data_bitmap);\r\ntcmu_cmd_set_dbi(tcmu_cmd, dbi);\r\nreturn true;\r\nerr_insert:\r\n__free_page(page);\r\nerr_alloc:\r\natomic_dec(&global_db_count);\r\nreturn false;\r\n}\r\nstatic bool tcmu_get_empty_blocks(struct tcmu_dev *udev,\r\nstruct tcmu_cmd *tcmu_cmd)\r\n{\r\nint i;\r\nudev->waiting_global = false;\r\nfor (i = tcmu_cmd->dbi_cur; i < tcmu_cmd->dbi_cnt; i++) {\r\nif (!tcmu_get_empty_block(udev, tcmu_cmd))\r\ngoto err;\r\n}\r\nreturn true;\r\nerr:\r\nudev->waiting_global = true;\r\nwake_up(&unmap_wait);\r\nreturn false;\r\n}\r\nstatic inline struct page *\r\ntcmu_get_block_page(struct tcmu_dev *udev, uint32_t dbi)\r\n{\r\nreturn radix_tree_lookup(&udev->data_blocks, dbi);\r\n}\r\nstatic inline void tcmu_free_cmd(struct tcmu_cmd *tcmu_cmd)\r\n{\r\nkfree(tcmu_cmd->dbi);\r\nkmem_cache_free(tcmu_cmd_cache, tcmu_cmd);\r\n}\r\nstatic inline size_t tcmu_cmd_get_data_length(struct tcmu_cmd *tcmu_cmd)\r\n{\r\nstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\r\nsize_t data_length = round_up(se_cmd->data_length, DATA_BLOCK_SIZE);\r\nif (se_cmd->se_cmd_flags & SCF_BIDI) {\r\nBUG_ON(!(se_cmd->t_bidi_data_sg && se_cmd->t_bidi_data_nents));\r\ndata_length += round_up(se_cmd->t_bidi_data_sg->length,\r\nDATA_BLOCK_SIZE);\r\n}\r\nreturn data_length;\r\n}\r\nstatic inline uint32_t tcmu_cmd_get_block_cnt(struct tcmu_cmd *tcmu_cmd)\r\n{\r\nsize_t data_length = tcmu_cmd_get_data_length(tcmu_cmd);\r\nreturn data_length / DATA_BLOCK_SIZE;\r\n}\r\nstatic struct tcmu_cmd *tcmu_alloc_cmd(struct se_cmd *se_cmd)\r\n{\r\nstruct se_device *se_dev = se_cmd->se_dev;\r\nstruct tcmu_dev *udev = TCMU_DEV(se_dev);\r\nstruct tcmu_cmd *tcmu_cmd;\r\nint cmd_id;\r\ntcmu_cmd = kmem_cache_zalloc(tcmu_cmd_cache, GFP_KERNEL);\r\nif (!tcmu_cmd)\r\nreturn NULL;\r\ntcmu_cmd->se_cmd = se_cmd;\r\ntcmu_cmd->tcmu_dev = udev;\r\nif (udev->cmd_time_out)\r\ntcmu_cmd->deadline = jiffies +\r\nmsecs_to_jiffies(udev->cmd_time_out);\r\ntcmu_cmd_reset_dbi_cur(tcmu_cmd);\r\ntcmu_cmd->dbi_cnt = tcmu_cmd_get_block_cnt(tcmu_cmd);\r\ntcmu_cmd->dbi = kcalloc(tcmu_cmd->dbi_cnt, sizeof(uint32_t),\r\nGFP_KERNEL);\r\nif (!tcmu_cmd->dbi) {\r\nkmem_cache_free(tcmu_cmd_cache, tcmu_cmd);\r\nreturn NULL;\r\n}\r\nidr_preload(GFP_KERNEL);\r\nspin_lock_irq(&udev->commands_lock);\r\ncmd_id = idr_alloc(&udev->commands, tcmu_cmd, 0,\r\nUSHRT_MAX, GFP_NOWAIT);\r\nspin_unlock_irq(&udev->commands_lock);\r\nidr_preload_end();\r\nif (cmd_id < 0) {\r\ntcmu_free_cmd(tcmu_cmd);\r\nreturn NULL;\r\n}\r\ntcmu_cmd->cmd_id = cmd_id;\r\nreturn tcmu_cmd;\r\n}\r\nstatic inline void tcmu_flush_dcache_range(void *vaddr, size_t size)\r\n{\r\nunsigned long offset = offset_in_page(vaddr);\r\nsize = round_up(size+offset, PAGE_SIZE);\r\nvaddr -= offset;\r\nwhile (size) {\r\nflush_dcache_page(virt_to_page(vaddr));\r\nsize -= PAGE_SIZE;\r\n}\r\n}\r\nstatic inline size_t spc_used(size_t head, size_t tail, size_t size)\r\n{\r\nint diff = head - tail;\r\nif (diff >= 0)\r\nreturn diff;\r\nelse\r\nreturn size + diff;\r\n}\r\nstatic inline size_t spc_free(size_t head, size_t tail, size_t size)\r\n{\r\nreturn (size - spc_used(head, tail, size) - 1);\r\n}\r\nstatic inline size_t head_to_end(size_t head, size_t size)\r\n{\r\nreturn size - head;\r\n}\r\nstatic inline void new_iov(struct iovec **iov, int *iov_cnt,\r\nstruct tcmu_dev *udev)\r\n{\r\nstruct iovec *iovec;\r\nif (*iov_cnt != 0)\r\n(*iov)++;\r\n(*iov_cnt)++;\r\niovec = *iov;\r\nmemset(iovec, 0, sizeof(struct iovec));\r\n}\r\nstatic inline size_t get_block_offset_user(struct tcmu_dev *dev,\r\nint dbi, int remaining)\r\n{\r\nreturn dev->data_off + dbi * DATA_BLOCK_SIZE +\r\nDATA_BLOCK_SIZE - remaining;\r\n}\r\nstatic inline size_t iov_tail(struct iovec *iov)\r\n{\r\nreturn (size_t)iov->iov_base + iov->iov_len;\r\n}\r\nstatic int scatter_data_area(struct tcmu_dev *udev,\r\nstruct tcmu_cmd *tcmu_cmd, struct scatterlist *data_sg,\r\nunsigned int data_nents, struct iovec **iov,\r\nint *iov_cnt, bool copy_data)\r\n{\r\nint i, dbi;\r\nint block_remaining = 0;\r\nvoid *from, *to = NULL;\r\nsize_t copy_bytes, to_offset, offset;\r\nstruct scatterlist *sg;\r\nstruct page *page;\r\nfor_each_sg(data_sg, sg, data_nents, i) {\r\nint sg_remaining = sg->length;\r\nfrom = kmap_atomic(sg_page(sg)) + sg->offset;\r\nwhile (sg_remaining > 0) {\r\nif (block_remaining == 0) {\r\nif (to)\r\nkunmap_atomic(to);\r\nblock_remaining = DATA_BLOCK_SIZE;\r\ndbi = tcmu_cmd_get_dbi(tcmu_cmd);\r\npage = tcmu_get_block_page(udev, dbi);\r\nto = kmap_atomic(page);\r\n}\r\ncopy_bytes = min_t(size_t, sg_remaining,\r\nblock_remaining);\r\nto_offset = get_block_offset_user(udev, dbi,\r\nblock_remaining);\r\nif (*iov_cnt != 0 &&\r\nto_offset == iov_tail(*iov)) {\r\n(*iov)->iov_len += copy_bytes;\r\n} else {\r\nnew_iov(iov, iov_cnt, udev);\r\n(*iov)->iov_base = (void __user *)to_offset;\r\n(*iov)->iov_len = copy_bytes;\r\n}\r\nif (copy_data) {\r\noffset = DATA_BLOCK_SIZE - block_remaining;\r\nmemcpy(to + offset,\r\nfrom + sg->length - sg_remaining,\r\ncopy_bytes);\r\ntcmu_flush_dcache_range(to, copy_bytes);\r\n}\r\nsg_remaining -= copy_bytes;\r\nblock_remaining -= copy_bytes;\r\n}\r\nkunmap_atomic(from - sg->offset);\r\n}\r\nif (to)\r\nkunmap_atomic(to);\r\nreturn 0;\r\n}\r\nstatic void gather_data_area(struct tcmu_dev *udev, struct tcmu_cmd *cmd,\r\nbool bidi)\r\n{\r\nstruct se_cmd *se_cmd = cmd->se_cmd;\r\nint i, dbi;\r\nint block_remaining = 0;\r\nvoid *from = NULL, *to;\r\nsize_t copy_bytes, offset;\r\nstruct scatterlist *sg, *data_sg;\r\nstruct page *page;\r\nunsigned int data_nents;\r\nuint32_t count = 0;\r\nif (!bidi) {\r\ndata_sg = se_cmd->t_data_sg;\r\ndata_nents = se_cmd->t_data_nents;\r\n} else {\r\ncount = DIV_ROUND_UP(se_cmd->data_length, DATA_BLOCK_SIZE);\r\ndata_sg = se_cmd->t_bidi_data_sg;\r\ndata_nents = se_cmd->t_bidi_data_nents;\r\n}\r\ntcmu_cmd_set_dbi_cur(cmd, count);\r\nfor_each_sg(data_sg, sg, data_nents, i) {\r\nint sg_remaining = sg->length;\r\nto = kmap_atomic(sg_page(sg)) + sg->offset;\r\nwhile (sg_remaining > 0) {\r\nif (block_remaining == 0) {\r\nif (from)\r\nkunmap_atomic(from);\r\nblock_remaining = DATA_BLOCK_SIZE;\r\ndbi = tcmu_cmd_get_dbi(cmd);\r\npage = tcmu_get_block_page(udev, dbi);\r\nfrom = kmap_atomic(page);\r\n}\r\ncopy_bytes = min_t(size_t, sg_remaining,\r\nblock_remaining);\r\noffset = DATA_BLOCK_SIZE - block_remaining;\r\ntcmu_flush_dcache_range(from, copy_bytes);\r\nmemcpy(to + sg->length - sg_remaining, from + offset,\r\ncopy_bytes);\r\nsg_remaining -= copy_bytes;\r\nblock_remaining -= copy_bytes;\r\n}\r\nkunmap_atomic(to - sg->offset);\r\n}\r\nif (from)\r\nkunmap_atomic(from);\r\n}\r\nstatic inline size_t spc_bitmap_free(unsigned long *bitmap, uint32_t thresh)\r\n{\r\nreturn DATA_BLOCK_SIZE * (thresh - bitmap_weight(bitmap, thresh));\r\n}\r\nstatic bool is_ring_space_avail(struct tcmu_dev *udev, struct tcmu_cmd *cmd,\r\nsize_t cmd_size, size_t data_needed)\r\n{\r\nstruct tcmu_mailbox *mb = udev->mb_addr;\r\nuint32_t blocks_needed = (data_needed + DATA_BLOCK_SIZE - 1)\r\n/ DATA_BLOCK_SIZE;\r\nsize_t space, cmd_needed;\r\nu32 cmd_head;\r\ntcmu_flush_dcache_range(mb, sizeof(*mb));\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\nif (head_to_end(cmd_head, udev->cmdr_size) >= cmd_size)\r\ncmd_needed = cmd_size;\r\nelse\r\ncmd_needed = cmd_size + head_to_end(cmd_head, udev->cmdr_size);\r\nspace = spc_free(cmd_head, udev->cmdr_last_cleaned, udev->cmdr_size);\r\nif (space < cmd_needed) {\r\npr_debug("no cmd space: %u %u %u\n", cmd_head,\r\nudev->cmdr_last_cleaned, udev->cmdr_size);\r\nreturn false;\r\n}\r\nspace = spc_bitmap_free(udev->data_bitmap, udev->dbi_thresh);\r\nif (space < data_needed) {\r\nunsigned long blocks_left = DATA_BLOCK_BITS - udev->dbi_thresh;\r\nunsigned long grow;\r\nif (blocks_left < blocks_needed) {\r\npr_debug("no data space: only %lu available, but ask for %zu\n",\r\nblocks_left * DATA_BLOCK_SIZE,\r\ndata_needed);\r\nreturn false;\r\n}\r\nif (!udev->dbi_thresh) {\r\nuint32_t init_thresh = DATA_BLOCK_INIT_BITS;\r\nudev->dbi_thresh = max(blocks_needed, init_thresh);\r\n} else {\r\ngrow = max(blocks_needed, udev->dbi_thresh / 2);\r\nudev->dbi_thresh += grow;\r\nif (udev->dbi_thresh > DATA_BLOCK_BITS)\r\nudev->dbi_thresh = DATA_BLOCK_BITS;\r\n}\r\n}\r\nreturn tcmu_get_empty_blocks(udev, cmd);\r\n}\r\nstatic inline size_t tcmu_cmd_get_base_cmd_size(size_t iov_cnt)\r\n{\r\nreturn max(offsetof(struct tcmu_cmd_entry, req.iov[iov_cnt]),\r\nsizeof(struct tcmu_cmd_entry));\r\n}\r\nstatic inline size_t tcmu_cmd_get_cmd_size(struct tcmu_cmd *tcmu_cmd,\r\nsize_t base_command_size)\r\n{\r\nstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\r\nsize_t command_size;\r\ncommand_size = base_command_size +\r\nround_up(scsi_command_size(se_cmd->t_task_cdb),\r\nTCMU_OP_ALIGN_SIZE);\r\nWARN_ON(command_size & (TCMU_OP_ALIGN_SIZE-1));\r\nreturn command_size;\r\n}\r\nstatic sense_reason_t\r\ntcmu_queue_cmd_ring(struct tcmu_cmd *tcmu_cmd)\r\n{\r\nstruct tcmu_dev *udev = tcmu_cmd->tcmu_dev;\r\nstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\r\nsize_t base_command_size, command_size;\r\nstruct tcmu_mailbox *mb;\r\nstruct tcmu_cmd_entry *entry;\r\nstruct iovec *iov;\r\nint iov_cnt, ret;\r\nuint32_t cmd_head;\r\nuint64_t cdb_off;\r\nbool copy_to_data_area;\r\nsize_t data_length = tcmu_cmd_get_data_length(tcmu_cmd);\r\nif (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags))\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\nbase_command_size = tcmu_cmd_get_base_cmd_size(tcmu_cmd->dbi_cnt);\r\ncommand_size = tcmu_cmd_get_cmd_size(tcmu_cmd, base_command_size);\r\nmutex_lock(&udev->cmdr_lock);\r\nmb = udev->mb_addr;\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\nif ((command_size > (udev->cmdr_size / 2)) ||\r\ndata_length > udev->data_size) {\r\npr_warn("TCMU: Request of size %zu/%zu is too big for %u/%zu "\r\n"cmd ring/data area\n", command_size, data_length,\r\nudev->cmdr_size, udev->data_size);\r\nmutex_unlock(&udev->cmdr_lock);\r\nreturn TCM_INVALID_CDB_FIELD;\r\n}\r\nwhile (!is_ring_space_avail(udev, tcmu_cmd, command_size, data_length)) {\r\nint ret;\r\nDEFINE_WAIT(__wait);\r\nprepare_to_wait(&udev->wait_cmdr, &__wait, TASK_INTERRUPTIBLE);\r\npr_debug("sleeping for ring space\n");\r\nmutex_unlock(&udev->cmdr_lock);\r\nif (udev->cmd_time_out)\r\nret = schedule_timeout(\r\nmsecs_to_jiffies(udev->cmd_time_out));\r\nelse\r\nret = schedule_timeout(msecs_to_jiffies(TCMU_TIME_OUT));\r\nfinish_wait(&udev->wait_cmdr, &__wait);\r\nif (!ret) {\r\npr_warn("tcmu: command timed out\n");\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\nmutex_lock(&udev->cmdr_lock);\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\n}\r\nif (head_to_end(cmd_head, udev->cmdr_size) < command_size) {\r\nsize_t pad_size = head_to_end(cmd_head, udev->cmdr_size);\r\nentry = (void *) mb + CMDR_OFF + cmd_head;\r\ntcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_PAD);\r\ntcmu_hdr_set_len(&entry->hdr.len_op, pad_size);\r\nentry->hdr.cmd_id = 0;\r\nentry->hdr.kflags = 0;\r\nentry->hdr.uflags = 0;\r\ntcmu_flush_dcache_range(entry, sizeof(*entry));\r\nUPDATE_HEAD(mb->cmd_head, pad_size, udev->cmdr_size);\r\ntcmu_flush_dcache_range(mb, sizeof(*mb));\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\nWARN_ON(cmd_head != 0);\r\n}\r\nentry = (void *) mb + CMDR_OFF + cmd_head;\r\nmemset(entry, 0, command_size);\r\ntcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_CMD);\r\nentry->hdr.cmd_id = tcmu_cmd->cmd_id;\r\ntcmu_cmd_reset_dbi_cur(tcmu_cmd);\r\niov = &entry->req.iov[0];\r\niov_cnt = 0;\r\ncopy_to_data_area = (se_cmd->data_direction == DMA_TO_DEVICE\r\n|| se_cmd->se_cmd_flags & SCF_BIDI);\r\nret = scatter_data_area(udev, tcmu_cmd, se_cmd->t_data_sg,\r\nse_cmd->t_data_nents, &iov, &iov_cnt,\r\ncopy_to_data_area);\r\nif (ret) {\r\ntcmu_cmd_free_data(tcmu_cmd, tcmu_cmd->dbi_cnt);\r\nmutex_unlock(&udev->cmdr_lock);\r\npr_err("tcmu: alloc and scatter data failed\n");\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\nentry->req.iov_cnt = iov_cnt;\r\niov_cnt = 0;\r\nif (se_cmd->se_cmd_flags & SCF_BIDI) {\r\niov++;\r\nret = scatter_data_area(udev, tcmu_cmd,\r\nse_cmd->t_bidi_data_sg,\r\nse_cmd->t_bidi_data_nents,\r\n&iov, &iov_cnt, false);\r\nif (ret) {\r\ntcmu_cmd_free_data(tcmu_cmd, tcmu_cmd->dbi_cnt);\r\nmutex_unlock(&udev->cmdr_lock);\r\npr_err("tcmu: alloc and scatter bidi data failed\n");\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\n}\r\nentry->req.iov_bidi_cnt = iov_cnt;\r\nbase_command_size = tcmu_cmd_get_base_cmd_size(entry->req.iov_cnt +\r\nentry->req.iov_bidi_cnt);\r\ncommand_size = tcmu_cmd_get_cmd_size(tcmu_cmd, base_command_size);\r\ntcmu_hdr_set_len(&entry->hdr.len_op, command_size);\r\ncdb_off = CMDR_OFF + cmd_head + base_command_size;\r\nmemcpy((void *) mb + cdb_off, se_cmd->t_task_cdb, scsi_command_size(se_cmd->t_task_cdb));\r\nentry->req.cdb_off = cdb_off;\r\ntcmu_flush_dcache_range(entry, sizeof(*entry));\r\nUPDATE_HEAD(mb->cmd_head, command_size, udev->cmdr_size);\r\ntcmu_flush_dcache_range(mb, sizeof(*mb));\r\nmutex_unlock(&udev->cmdr_lock);\r\nuio_event_notify(&udev->uio_info);\r\nif (udev->cmd_time_out)\r\nmod_timer(&udev->timeout, round_jiffies_up(jiffies +\r\nmsecs_to_jiffies(udev->cmd_time_out)));\r\nreturn TCM_NO_SENSE;\r\n}\r\nstatic sense_reason_t\r\ntcmu_queue_cmd(struct se_cmd *se_cmd)\r\n{\r\nstruct se_device *se_dev = se_cmd->se_dev;\r\nstruct tcmu_dev *udev = TCMU_DEV(se_dev);\r\nstruct tcmu_cmd *tcmu_cmd;\r\nsense_reason_t ret;\r\ntcmu_cmd = tcmu_alloc_cmd(se_cmd);\r\nif (!tcmu_cmd)\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\nret = tcmu_queue_cmd_ring(tcmu_cmd);\r\nif (ret != TCM_NO_SENSE) {\r\npr_err("TCMU: Could not queue command\n");\r\nspin_lock_irq(&udev->commands_lock);\r\nidr_remove(&udev->commands, tcmu_cmd->cmd_id);\r\nspin_unlock_irq(&udev->commands_lock);\r\ntcmu_free_cmd(tcmu_cmd);\r\n}\r\nreturn ret;\r\n}\r\nstatic void tcmu_handle_completion(struct tcmu_cmd *cmd, struct tcmu_cmd_entry *entry)\r\n{\r\nstruct se_cmd *se_cmd = cmd->se_cmd;\r\nstruct tcmu_dev *udev = cmd->tcmu_dev;\r\nif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags))\r\ngoto out;\r\ntcmu_cmd_reset_dbi_cur(cmd);\r\nif (entry->hdr.uflags & TCMU_UFLAG_UNKNOWN_OP) {\r\npr_warn("TCMU: Userspace set UNKNOWN_OP flag on se_cmd %p\n",\r\ncmd->se_cmd);\r\nentry->rsp.scsi_status = SAM_STAT_CHECK_CONDITION;\r\n} else if (entry->rsp.scsi_status == SAM_STAT_CHECK_CONDITION) {\r\ntransport_copy_sense_to_cmd(se_cmd, entry->rsp.sense_buffer);\r\n} else if (se_cmd->se_cmd_flags & SCF_BIDI) {\r\ngather_data_area(udev, cmd, true);\r\n} else if (se_cmd->data_direction == DMA_FROM_DEVICE) {\r\ngather_data_area(udev, cmd, false);\r\n} else if (se_cmd->data_direction == DMA_TO_DEVICE) {\r\n} else if (se_cmd->data_direction != DMA_NONE) {\r\npr_warn("TCMU: data direction was %d!\n",\r\nse_cmd->data_direction);\r\n}\r\ntarget_complete_cmd(cmd->se_cmd, entry->rsp.scsi_status);\r\nout:\r\ncmd->se_cmd = NULL;\r\ntcmu_cmd_free_data(cmd, cmd->dbi_cnt);\r\ntcmu_free_cmd(cmd);\r\n}\r\nstatic unsigned int tcmu_handle_completions(struct tcmu_dev *udev)\r\n{\r\nstruct tcmu_mailbox *mb;\r\nint handled = 0;\r\nif (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags)) {\r\npr_err("ring broken, not handling completions\n");\r\nreturn 0;\r\n}\r\nmb = udev->mb_addr;\r\ntcmu_flush_dcache_range(mb, sizeof(*mb));\r\nwhile (udev->cmdr_last_cleaned != ACCESS_ONCE(mb->cmd_tail)) {\r\nstruct tcmu_cmd_entry *entry = (void *) mb + CMDR_OFF + udev->cmdr_last_cleaned;\r\nstruct tcmu_cmd *cmd;\r\ntcmu_flush_dcache_range(entry, sizeof(*entry));\r\nif (tcmu_hdr_get_op(entry->hdr.len_op) == TCMU_OP_PAD) {\r\nUPDATE_HEAD(udev->cmdr_last_cleaned,\r\ntcmu_hdr_get_len(entry->hdr.len_op),\r\nudev->cmdr_size);\r\ncontinue;\r\n}\r\nWARN_ON(tcmu_hdr_get_op(entry->hdr.len_op) != TCMU_OP_CMD);\r\nspin_lock(&udev->commands_lock);\r\ncmd = idr_remove(&udev->commands, entry->hdr.cmd_id);\r\nspin_unlock(&udev->commands_lock);\r\nif (!cmd) {\r\npr_err("cmd_id not found, ring is broken\n");\r\nset_bit(TCMU_DEV_BIT_BROKEN, &udev->flags);\r\nbreak;\r\n}\r\ntcmu_handle_completion(cmd, entry);\r\nUPDATE_HEAD(udev->cmdr_last_cleaned,\r\ntcmu_hdr_get_len(entry->hdr.len_op),\r\nudev->cmdr_size);\r\nhandled++;\r\n}\r\nif (mb->cmd_tail == mb->cmd_head)\r\ndel_timer(&udev->timeout);\r\nwake_up(&udev->wait_cmdr);\r\nreturn handled;\r\n}\r\nstatic int tcmu_check_expired_cmd(int id, void *p, void *data)\r\n{\r\nstruct tcmu_cmd *cmd = p;\r\nif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags))\r\nreturn 0;\r\nif (!time_after(jiffies, cmd->deadline))\r\nreturn 0;\r\nset_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags);\r\ntarget_complete_cmd(cmd->se_cmd, SAM_STAT_CHECK_CONDITION);\r\ncmd->se_cmd = NULL;\r\nreturn 0;\r\n}\r\nstatic void tcmu_device_timedout(unsigned long data)\r\n{\r\nstruct tcmu_dev *udev = (struct tcmu_dev *)data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&udev->commands_lock, flags);\r\nidr_for_each(&udev->commands, tcmu_check_expired_cmd, NULL);\r\nspin_unlock_irqrestore(&udev->commands_lock, flags);\r\nwake_up(&unmap_wait);\r\n}\r\nstatic int tcmu_attach_hba(struct se_hba *hba, u32 host_id)\r\n{\r\nstruct tcmu_hba *tcmu_hba;\r\ntcmu_hba = kzalloc(sizeof(struct tcmu_hba), GFP_KERNEL);\r\nif (!tcmu_hba)\r\nreturn -ENOMEM;\r\ntcmu_hba->host_id = host_id;\r\nhba->hba_ptr = tcmu_hba;\r\nreturn 0;\r\n}\r\nstatic void tcmu_detach_hba(struct se_hba *hba)\r\n{\r\nkfree(hba->hba_ptr);\r\nhba->hba_ptr = NULL;\r\n}\r\nstatic struct se_device *tcmu_alloc_device(struct se_hba *hba, const char *name)\r\n{\r\nstruct tcmu_dev *udev;\r\nudev = kzalloc(sizeof(struct tcmu_dev), GFP_KERNEL);\r\nif (!udev)\r\nreturn NULL;\r\nkref_init(&udev->kref);\r\nudev->name = kstrdup(name, GFP_KERNEL);\r\nif (!udev->name) {\r\nkfree(udev);\r\nreturn NULL;\r\n}\r\nudev->hba = hba;\r\nudev->cmd_time_out = TCMU_TIME_OUT;\r\ninit_waitqueue_head(&udev->wait_cmdr);\r\nmutex_init(&udev->cmdr_lock);\r\nidr_init(&udev->commands);\r\nspin_lock_init(&udev->commands_lock);\r\nsetup_timer(&udev->timeout, tcmu_device_timedout,\r\n(unsigned long)udev);\r\ninit_waitqueue_head(&udev->nl_cmd_wq);\r\nspin_lock_init(&udev->nl_cmd_lock);\r\nreturn &udev->se_dev;\r\n}\r\nstatic int tcmu_irqcontrol(struct uio_info *info, s32 irq_on)\r\n{\r\nstruct tcmu_dev *tcmu_dev = container_of(info, struct tcmu_dev, uio_info);\r\nmutex_lock(&tcmu_dev->cmdr_lock);\r\ntcmu_handle_completions(tcmu_dev);\r\nmutex_unlock(&tcmu_dev->cmdr_lock);\r\nreturn 0;\r\n}\r\nstatic int tcmu_find_mem_index(struct vm_area_struct *vma)\r\n{\r\nstruct tcmu_dev *udev = vma->vm_private_data;\r\nstruct uio_info *info = &udev->uio_info;\r\nif (vma->vm_pgoff < MAX_UIO_MAPS) {\r\nif (info->mem[vma->vm_pgoff].size == 0)\r\nreturn -1;\r\nreturn (int)vma->vm_pgoff;\r\n}\r\nreturn -1;\r\n}\r\nstatic struct page *tcmu_try_get_block_page(struct tcmu_dev *udev, uint32_t dbi)\r\n{\r\nstruct page *page;\r\nint ret;\r\nmutex_lock(&udev->cmdr_lock);\r\npage = tcmu_get_block_page(udev, dbi);\r\nif (likely(page)) {\r\nmutex_unlock(&udev->cmdr_lock);\r\nreturn page;\r\n}\r\npr_warn("Block(%u) out of cmd's iov[] has been touched!\n", dbi);\r\npr_warn("Mostly it will be a bug of userspace, please have a check!\n");\r\nif (dbi >= udev->dbi_thresh) {\r\nudev->dbi_thresh = dbi + 1;\r\nudev->dbi_max = dbi;\r\n}\r\npage = radix_tree_lookup(&udev->data_blocks, dbi);\r\nif (!page) {\r\npage = alloc_page(GFP_KERNEL | __GFP_ZERO);\r\nif (!page) {\r\nmutex_unlock(&udev->cmdr_lock);\r\nreturn NULL;\r\n}\r\nret = radix_tree_insert(&udev->data_blocks, dbi, page);\r\nif (ret) {\r\nmutex_unlock(&udev->cmdr_lock);\r\n__free_page(page);\r\nreturn NULL;\r\n}\r\natomic_inc(&global_db_count);\r\n}\r\nmutex_unlock(&udev->cmdr_lock);\r\nreturn page;\r\n}\r\nstatic int tcmu_vma_fault(struct vm_fault *vmf)\r\n{\r\nstruct tcmu_dev *udev = vmf->vma->vm_private_data;\r\nstruct uio_info *info = &udev->uio_info;\r\nstruct page *page;\r\nunsigned long offset;\r\nvoid *addr;\r\nint mi = tcmu_find_mem_index(vmf->vma);\r\nif (mi < 0)\r\nreturn VM_FAULT_SIGBUS;\r\noffset = (vmf->pgoff - mi) << PAGE_SHIFT;\r\nif (offset < udev->data_off) {\r\naddr = (void *)(unsigned long)info->mem[mi].addr + offset;\r\npage = vmalloc_to_page(addr);\r\n} else {\r\nuint32_t dbi;\r\ndbi = (offset - udev->data_off) / DATA_BLOCK_SIZE;\r\npage = tcmu_try_get_block_page(udev, dbi);\r\nif (!page)\r\nreturn VM_FAULT_NOPAGE;\r\n}\r\nget_page(page);\r\nvmf->page = page;\r\nreturn 0;\r\n}\r\nstatic int tcmu_mmap(struct uio_info *info, struct vm_area_struct *vma)\r\n{\r\nstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\r\nvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\r\nvma->vm_ops = &tcmu_vm_ops;\r\nvma->vm_private_data = udev;\r\nif (vma_pages(vma) != (TCMU_RING_SIZE >> PAGE_SHIFT))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int tcmu_open(struct uio_info *info, struct inode *inode)\r\n{\r\nstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\r\nif (test_and_set_bit(TCMU_DEV_BIT_OPEN, &udev->flags))\r\nreturn -EBUSY;\r\nudev->inode = inode;\r\nkref_get(&udev->kref);\r\npr_debug("open\n");\r\nreturn 0;\r\n}\r\nstatic void tcmu_dev_call_rcu(struct rcu_head *p)\r\n{\r\nstruct se_device *dev = container_of(p, struct se_device, rcu_head);\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nkfree(udev->uio_info.name);\r\nkfree(udev->name);\r\nkfree(udev);\r\n}\r\nstatic void tcmu_dev_kref_release(struct kref *kref)\r\n{\r\nstruct tcmu_dev *udev = container_of(kref, struct tcmu_dev, kref);\r\nstruct se_device *dev = &udev->se_dev;\r\ncall_rcu(&dev->rcu_head, tcmu_dev_call_rcu);\r\n}\r\nstatic int tcmu_release(struct uio_info *info, struct inode *inode)\r\n{\r\nstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\r\nclear_bit(TCMU_DEV_BIT_OPEN, &udev->flags);\r\npr_debug("close\n");\r\nkref_put(&udev->kref, tcmu_dev_kref_release);\r\nreturn 0;\r\n}\r\nstatic void tcmu_init_genl_cmd_reply(struct tcmu_dev *udev, int cmd)\r\n{\r\nstruct tcmu_nl_cmd *nl_cmd = &udev->curr_nl_cmd;\r\nif (!tcmu_kern_cmd_reply_supported)\r\nreturn;\r\nrelock:\r\nspin_lock(&udev->nl_cmd_lock);\r\nif (nl_cmd->cmd != TCMU_CMD_UNSPEC) {\r\nspin_unlock(&udev->nl_cmd_lock);\r\npr_debug("sleeping for open nl cmd\n");\r\nwait_event(udev->nl_cmd_wq, (nl_cmd->cmd == TCMU_CMD_UNSPEC));\r\ngoto relock;\r\n}\r\nmemset(nl_cmd, 0, sizeof(*nl_cmd));\r\nnl_cmd->cmd = cmd;\r\ninit_completion(&nl_cmd->complete);\r\nspin_unlock(&udev->nl_cmd_lock);\r\n}\r\nstatic int tcmu_wait_genl_cmd_reply(struct tcmu_dev *udev)\r\n{\r\nstruct tcmu_nl_cmd *nl_cmd = &udev->curr_nl_cmd;\r\nint ret;\r\nDEFINE_WAIT(__wait);\r\nif (!tcmu_kern_cmd_reply_supported)\r\nreturn 0;\r\npr_debug("sleeping for nl reply\n");\r\nwait_for_completion(&nl_cmd->complete);\r\nspin_lock(&udev->nl_cmd_lock);\r\nnl_cmd->cmd = TCMU_CMD_UNSPEC;\r\nret = nl_cmd->status;\r\nnl_cmd->status = 0;\r\nspin_unlock(&udev->nl_cmd_lock);\r\nwake_up_all(&udev->nl_cmd_wq);\r\nreturn ret;;\r\n}\r\nstatic int tcmu_netlink_event(struct tcmu_dev *udev, enum tcmu_genl_cmd cmd,\r\nint reconfig_attr, const void *reconfig_data)\r\n{\r\nstruct sk_buff *skb;\r\nvoid *msg_header;\r\nint ret = -ENOMEM;\r\nskb = genlmsg_new(NLMSG_GOODSIZE, GFP_KERNEL);\r\nif (!skb)\r\nreturn ret;\r\nmsg_header = genlmsg_put(skb, 0, 0, &tcmu_genl_family, 0, cmd);\r\nif (!msg_header)\r\ngoto free_skb;\r\nret = nla_put_string(skb, TCMU_ATTR_DEVICE, udev->uio_info.name);\r\nif (ret < 0)\r\ngoto free_skb;\r\nret = nla_put_u32(skb, TCMU_ATTR_MINOR, udev->uio_info.uio_dev->minor);\r\nif (ret < 0)\r\ngoto free_skb;\r\nret = nla_put_u32(skb, TCMU_ATTR_DEVICE_ID, udev->se_dev.dev_index);\r\nif (ret < 0)\r\ngoto free_skb;\r\nif (cmd == TCMU_CMD_RECONFIG_DEVICE) {\r\nswitch (reconfig_attr) {\r\ncase TCMU_ATTR_DEV_CFG:\r\nret = nla_put_string(skb, reconfig_attr, reconfig_data);\r\nbreak;\r\ncase TCMU_ATTR_DEV_SIZE:\r\nret = nla_put_u64_64bit(skb, reconfig_attr,\r\n*((u64 *)reconfig_data),\r\nTCMU_ATTR_PAD);\r\nbreak;\r\ncase TCMU_ATTR_WRITECACHE:\r\nret = nla_put_u8(skb, reconfig_attr,\r\n*((u8 *)reconfig_data));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nif (ret < 0)\r\ngoto free_skb;\r\n}\r\ngenlmsg_end(skb, msg_header);\r\ntcmu_init_genl_cmd_reply(udev, cmd);\r\nret = genlmsg_multicast_allns(&tcmu_genl_family, skb, 0,\r\nTCMU_MCGRP_CONFIG, GFP_KERNEL);\r\nif (ret == -ESRCH)\r\nret = 0;\r\nif (!ret)\r\nret = tcmu_wait_genl_cmd_reply(udev);\r\nreturn ret;\r\nfree_skb:\r\nnlmsg_free(skb);\r\nreturn ret;\r\n}\r\nstatic int tcmu_update_uio_info(struct tcmu_dev *udev)\r\n{\r\nstruct tcmu_hba *hba = udev->hba->hba_ptr;\r\nstruct uio_info *info;\r\nsize_t size, used;\r\nchar *str;\r\ninfo = &udev->uio_info;\r\nsize = snprintf(NULL, 0, "tcm-user/%u/%s/%s", hba->host_id, udev->name,\r\nudev->dev_config);\r\nsize += 1;\r\nstr = kmalloc(size, GFP_KERNEL);\r\nif (!str)\r\nreturn -ENOMEM;\r\nused = snprintf(str, size, "tcm-user/%u/%s", hba->host_id, udev->name);\r\nif (udev->dev_config[0])\r\nsnprintf(str + used, size - used, "/%s", udev->dev_config);\r\nkfree(info->name);\r\ninfo->name = str;\r\nreturn 0;\r\n}\r\nstatic int tcmu_configure_device(struct se_device *dev)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nstruct uio_info *info;\r\nstruct tcmu_mailbox *mb;\r\nint ret = 0;\r\nret = tcmu_update_uio_info(udev);\r\nif (ret)\r\nreturn ret;\r\ninfo = &udev->uio_info;\r\nudev->mb_addr = vzalloc(CMDR_SIZE);\r\nif (!udev->mb_addr) {\r\nret = -ENOMEM;\r\ngoto err_vzalloc;\r\n}\r\nudev->cmdr_size = CMDR_SIZE - CMDR_OFF;\r\nudev->data_off = CMDR_SIZE;\r\nudev->data_size = DATA_SIZE;\r\nudev->dbi_thresh = 0;\r\nudev->waiting_global = false;\r\nmb = udev->mb_addr;\r\nmb->version = TCMU_MAILBOX_VERSION;\r\nmb->flags = TCMU_MAILBOX_FLAG_CAP_OOOC;\r\nmb->cmdr_off = CMDR_OFF;\r\nmb->cmdr_size = udev->cmdr_size;\r\nWARN_ON(!PAGE_ALIGNED(udev->data_off));\r\nWARN_ON(udev->data_size % PAGE_SIZE);\r\nWARN_ON(udev->data_size % DATA_BLOCK_SIZE);\r\nINIT_RADIX_TREE(&udev->data_blocks, GFP_KERNEL);\r\ninfo->version = __stringify(TCMU_MAILBOX_VERSION);\r\ninfo->mem[0].name = "tcm-user command & data buffer";\r\ninfo->mem[0].addr = (phys_addr_t)(uintptr_t)udev->mb_addr;\r\ninfo->mem[0].size = TCMU_RING_SIZE;\r\ninfo->mem[0].memtype = UIO_MEM_NONE;\r\ninfo->irqcontrol = tcmu_irqcontrol;\r\ninfo->irq = UIO_IRQ_CUSTOM;\r\ninfo->mmap = tcmu_mmap;\r\ninfo->open = tcmu_open;\r\ninfo->release = tcmu_release;\r\nret = uio_register_device(tcmu_root_device, info);\r\nif (ret)\r\ngoto err_register;\r\nif (dev->dev_attrib.hw_block_size == 0)\r\ndev->dev_attrib.hw_block_size = 512;\r\nif (!dev->dev_attrib.hw_max_sectors)\r\ndev->dev_attrib.hw_max_sectors = 128;\r\nif (!dev->dev_attrib.emulate_write_cache)\r\ndev->dev_attrib.emulate_write_cache = 0;\r\ndev->dev_attrib.hw_queue_depth = 128;\r\nkref_get(&udev->kref);\r\nret = tcmu_netlink_event(udev, TCMU_CMD_ADDED_DEVICE, 0, NULL);\r\nif (ret)\r\ngoto err_netlink;\r\nmutex_lock(&root_udev_mutex);\r\nlist_add(&udev->node, &root_udev);\r\nmutex_unlock(&root_udev_mutex);\r\nreturn 0;\r\nerr_netlink:\r\nkref_put(&udev->kref, tcmu_dev_kref_release);\r\nuio_unregister_device(&udev->uio_info);\r\nerr_register:\r\nvfree(udev->mb_addr);\r\nerr_vzalloc:\r\nkfree(info->name);\r\ninfo->name = NULL;\r\nreturn ret;\r\n}\r\nstatic int tcmu_check_and_free_pending_cmd(struct tcmu_cmd *cmd)\r\n{\r\nif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags)) {\r\nkmem_cache_free(tcmu_cmd_cache, cmd);\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic bool tcmu_dev_configured(struct tcmu_dev *udev)\r\n{\r\nreturn udev->uio_info.uio_dev ? true : false;\r\n}\r\nstatic void tcmu_blocks_release(struct tcmu_dev *udev)\r\n{\r\nint i;\r\nstruct page *page;\r\nmutex_lock(&udev->cmdr_lock);\r\nfor (i = 0; i <= udev->dbi_max; i++) {\r\npage = radix_tree_delete(&udev->data_blocks, i);\r\nif (page) {\r\n__free_page(page);\r\natomic_dec(&global_db_count);\r\n}\r\n}\r\nmutex_unlock(&udev->cmdr_lock);\r\n}\r\nstatic void tcmu_free_device(struct se_device *dev)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nkref_put(&udev->kref, tcmu_dev_kref_release);\r\n}\r\nstatic void tcmu_destroy_device(struct se_device *dev)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nstruct tcmu_cmd *cmd;\r\nbool all_expired = true;\r\nint i;\r\ndel_timer_sync(&udev->timeout);\r\nmutex_lock(&root_udev_mutex);\r\nlist_del(&udev->node);\r\nmutex_unlock(&root_udev_mutex);\r\nvfree(udev->mb_addr);\r\nspin_lock_irq(&udev->commands_lock);\r\nidr_for_each_entry(&udev->commands, cmd, i) {\r\nif (tcmu_check_and_free_pending_cmd(cmd) != 0)\r\nall_expired = false;\r\n}\r\nidr_destroy(&udev->commands);\r\nspin_unlock_irq(&udev->commands_lock);\r\nWARN_ON(!all_expired);\r\ntcmu_blocks_release(udev);\r\ntcmu_netlink_event(udev, TCMU_CMD_REMOVED_DEVICE, 0, NULL);\r\nuio_unregister_device(&udev->uio_info);\r\nkref_put(&udev->kref, tcmu_dev_kref_release);\r\n}\r\nstatic int tcmu_set_dev_attrib(substring_t *arg, u32 *dev_attrib)\r\n{\r\nunsigned long tmp_ul;\r\nchar *arg_p;\r\nint ret;\r\narg_p = match_strdup(arg);\r\nif (!arg_p)\r\nreturn -ENOMEM;\r\nret = kstrtoul(arg_p, 0, &tmp_ul);\r\nkfree(arg_p);\r\nif (ret < 0) {\r\npr_err("kstrtoul() failed for dev attrib\n");\r\nreturn ret;\r\n}\r\nif (!tmp_ul) {\r\npr_err("dev attrib must be nonzero\n");\r\nreturn -EINVAL;\r\n}\r\n*dev_attrib = tmp_ul;\r\nreturn 0;\r\n}\r\nstatic ssize_t tcmu_set_configfs_dev_params(struct se_device *dev,\r\nconst char *page, ssize_t count)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nchar *orig, *ptr, *opts, *arg_p;\r\nsubstring_t args[MAX_OPT_ARGS];\r\nint ret = 0, token;\r\nopts = kstrdup(page, GFP_KERNEL);\r\nif (!opts)\r\nreturn -ENOMEM;\r\norig = opts;\r\nwhile ((ptr = strsep(&opts, ",\n")) != NULL) {\r\nif (!*ptr)\r\ncontinue;\r\ntoken = match_token(ptr, tokens, args);\r\nswitch (token) {\r\ncase Opt_dev_config:\r\nif (match_strlcpy(udev->dev_config, &args[0],\r\nTCMU_CONFIG_LEN) == 0) {\r\nret = -EINVAL;\r\nbreak;\r\n}\r\npr_debug("TCMU: Referencing Path: %s\n", udev->dev_config);\r\nbreak;\r\ncase Opt_dev_size:\r\narg_p = match_strdup(&args[0]);\r\nif (!arg_p) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nret = kstrtoul(arg_p, 0, (unsigned long *) &udev->dev_size);\r\nkfree(arg_p);\r\nif (ret < 0)\r\npr_err("kstrtoul() failed for dev_size=\n");\r\nbreak;\r\ncase Opt_hw_block_size:\r\nret = tcmu_set_dev_attrib(&args[0],\r\n&(dev->dev_attrib.hw_block_size));\r\nbreak;\r\ncase Opt_hw_max_sectors:\r\nret = tcmu_set_dev_attrib(&args[0],\r\n&(dev->dev_attrib.hw_max_sectors));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (ret)\r\nbreak;\r\n}\r\nkfree(orig);\r\nreturn (!ret) ? count : ret;\r\n}\r\nstatic ssize_t tcmu_show_configfs_dev_params(struct se_device *dev, char *b)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nssize_t bl = 0;\r\nbl = sprintf(b + bl, "Config: %s ",\r\nudev->dev_config[0] ? udev->dev_config : "NULL");\r\nbl += sprintf(b + bl, "Size: %zu\n", udev->dev_size);\r\nreturn bl;\r\n}\r\nstatic sector_t tcmu_get_blocks(struct se_device *dev)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nreturn div_u64(udev->dev_size - dev->dev_attrib.block_size,\r\ndev->dev_attrib.block_size);\r\n}\r\nstatic sense_reason_t\r\ntcmu_parse_cdb(struct se_cmd *cmd)\r\n{\r\nreturn passthrough_parse_cdb(cmd, tcmu_queue_cmd);\r\n}\r\nstatic ssize_t tcmu_cmd_time_out_show(struct config_item *item, char *page)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nstruct tcmu_dev *udev = container_of(da->da_dev,\r\nstruct tcmu_dev, se_dev);\r\nreturn snprintf(page, PAGE_SIZE, "%lu\n", udev->cmd_time_out / MSEC_PER_SEC);\r\n}\r\nstatic ssize_t tcmu_cmd_time_out_store(struct config_item *item, const char *page,\r\nsize_t count)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nstruct tcmu_dev *udev = container_of(da->da_dev,\r\nstruct tcmu_dev, se_dev);\r\nu32 val;\r\nint ret;\r\nif (da->da_dev->export_count) {\r\npr_err("Unable to set tcmu cmd_time_out while exports exist\n");\r\nreturn -EINVAL;\r\n}\r\nret = kstrtou32(page, 0, &val);\r\nif (ret < 0)\r\nreturn ret;\r\nudev->cmd_time_out = val * MSEC_PER_SEC;\r\nreturn count;\r\n}\r\nstatic ssize_t tcmu_dev_config_show(struct config_item *item, char *page)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\r\nreturn snprintf(page, PAGE_SIZE, "%s\n", udev->dev_config);\r\n}\r\nstatic ssize_t tcmu_dev_config_store(struct config_item *item, const char *page,\r\nsize_t count)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\r\nint ret, len;\r\nlen = strlen(page);\r\nif (!len || len > TCMU_CONFIG_LEN - 1)\r\nreturn -EINVAL;\r\nif (tcmu_dev_configured(udev)) {\r\nret = tcmu_netlink_event(udev, TCMU_CMD_RECONFIG_DEVICE,\r\nTCMU_ATTR_DEV_CFG, page);\r\nif (ret) {\r\npr_err("Unable to reconfigure device\n");\r\nreturn ret;\r\n}\r\nstrlcpy(udev->dev_config, page, TCMU_CONFIG_LEN);\r\nret = tcmu_update_uio_info(udev);\r\nif (ret)\r\nreturn ret;\r\nreturn count;\r\n}\r\nstrlcpy(udev->dev_config, page, TCMU_CONFIG_LEN);\r\nreturn count;\r\n}\r\nstatic ssize_t tcmu_dev_size_show(struct config_item *item, char *page)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\r\nreturn snprintf(page, PAGE_SIZE, "%zu\n", udev->dev_size);\r\n}\r\nstatic ssize_t tcmu_dev_size_store(struct config_item *item, const char *page,\r\nsize_t count)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\r\nu64 val;\r\nint ret;\r\nret = kstrtou64(page, 0, &val);\r\nif (ret < 0)\r\nreturn ret;\r\nif (tcmu_dev_configured(udev)) {\r\nret = tcmu_netlink_event(udev, TCMU_CMD_RECONFIG_DEVICE,\r\nTCMU_ATTR_DEV_SIZE, &val);\r\nif (ret) {\r\npr_err("Unable to reconfigure device\n");\r\nreturn ret;\r\n}\r\n}\r\nudev->dev_size = val;\r\nreturn count;\r\n}\r\nstatic ssize_t tcmu_emulate_write_cache_show(struct config_item *item,\r\nchar *page)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nreturn snprintf(page, PAGE_SIZE, "%i\n", da->emulate_write_cache);\r\n}\r\nstatic ssize_t tcmu_emulate_write_cache_store(struct config_item *item,\r\nconst char *page, size_t count)\r\n{\r\nstruct se_dev_attrib *da = container_of(to_config_group(item),\r\nstruct se_dev_attrib, da_group);\r\nstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\r\nu8 val;\r\nint ret;\r\nret = kstrtou8(page, 0, &val);\r\nif (ret < 0)\r\nreturn ret;\r\nif (tcmu_dev_configured(udev)) {\r\nret = tcmu_netlink_event(udev, TCMU_CMD_RECONFIG_DEVICE,\r\nTCMU_ATTR_WRITECACHE, &val);\r\nif (ret) {\r\npr_err("Unable to reconfigure device\n");\r\nreturn ret;\r\n}\r\n}\r\nda->emulate_write_cache = val;\r\nreturn count;\r\n}\r\nstatic int unmap_thread_fn(void *data)\r\n{\r\nstruct tcmu_dev *udev;\r\nloff_t off;\r\nuint32_t start, end, block;\r\nstruct page *page;\r\nint i;\r\nwhile (!kthread_should_stop()) {\r\nDEFINE_WAIT(__wait);\r\nprepare_to_wait(&unmap_wait, &__wait, TASK_INTERRUPTIBLE);\r\nschedule();\r\nfinish_wait(&unmap_wait, &__wait);\r\nif (kthread_should_stop())\r\nbreak;\r\nmutex_lock(&root_udev_mutex);\r\nlist_for_each_entry(udev, &root_udev, node) {\r\nmutex_lock(&udev->cmdr_lock);\r\ntcmu_handle_completions(udev);\r\nif (udev->waiting_global || !udev->dbi_thresh) {\r\nmutex_unlock(&udev->cmdr_lock);\r\ncontinue;\r\n}\r\nend = udev->dbi_max + 1;\r\nblock = find_last_bit(udev->data_bitmap, end);\r\nif (block == udev->dbi_max) {\r\nmutex_unlock(&udev->cmdr_lock);\r\ncontinue;\r\n} else if (block == end) {\r\nudev->dbi_thresh = start = 0;\r\nudev->dbi_max = 0;\r\n} else {\r\nudev->dbi_thresh = start = block + 1;\r\nudev->dbi_max = block;\r\n}\r\noff = udev->data_off + start * DATA_BLOCK_SIZE;\r\nunmap_mapping_range(udev->inode->i_mapping, off, 0, 1);\r\nfor (i = start; i < end; i++) {\r\npage = radix_tree_delete(&udev->data_blocks, i);\r\nif (page) {\r\n__free_page(page);\r\natomic_dec(&global_db_count);\r\n}\r\n}\r\nmutex_unlock(&udev->cmdr_lock);\r\n}\r\nlist_for_each_entry(udev, &root_udev, node) {\r\nif (udev->waiting_global)\r\nwake_up(&udev->wait_cmdr);\r\n}\r\nmutex_unlock(&root_udev_mutex);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init tcmu_module_init(void)\r\n{\r\nint ret, i, k, len = 0;\r\nBUILD_BUG_ON((sizeof(struct tcmu_cmd_entry) % TCMU_OP_ALIGN_SIZE) != 0);\r\ntcmu_cmd_cache = kmem_cache_create("tcmu_cmd_cache",\r\nsizeof(struct tcmu_cmd),\r\n__alignof__(struct tcmu_cmd),\r\n0, NULL);\r\nif (!tcmu_cmd_cache)\r\nreturn -ENOMEM;\r\ntcmu_root_device = root_device_register("tcm_user");\r\nif (IS_ERR(tcmu_root_device)) {\r\nret = PTR_ERR(tcmu_root_device);\r\ngoto out_free_cache;\r\n}\r\nret = genl_register_family(&tcmu_genl_family);\r\nif (ret < 0) {\r\ngoto out_unreg_device;\r\n}\r\nfor (i = 0; passthrough_attrib_attrs[i] != NULL; i++) {\r\nlen += sizeof(struct configfs_attribute *);\r\n}\r\nfor (i = 0; tcmu_attrib_attrs[i] != NULL; i++) {\r\nlen += sizeof(struct configfs_attribute *);\r\n}\r\nlen += sizeof(struct configfs_attribute *);\r\ntcmu_attrs = kzalloc(len, GFP_KERNEL);\r\nif (!tcmu_attrs) {\r\nret = -ENOMEM;\r\ngoto out_unreg_genl;\r\n}\r\nfor (i = 0; passthrough_attrib_attrs[i] != NULL; i++) {\r\ntcmu_attrs[i] = passthrough_attrib_attrs[i];\r\n}\r\nfor (k = 0; tcmu_attrib_attrs[k] != NULL; k++) {\r\ntcmu_attrs[i] = tcmu_attrib_attrs[k];\r\ni++;\r\n}\r\ntcmu_ops.tb_dev_attrib_attrs = tcmu_attrs;\r\nret = transport_backend_register(&tcmu_ops);\r\nif (ret)\r\ngoto out_attrs;\r\ninit_waitqueue_head(&unmap_wait);\r\nunmap_thread = kthread_run(unmap_thread_fn, NULL, "tcmu_unmap");\r\nif (IS_ERR(unmap_thread)) {\r\nret = PTR_ERR(unmap_thread);\r\ngoto out_unreg_transport;\r\n}\r\nreturn 0;\r\nout_unreg_transport:\r\ntarget_backend_unregister(&tcmu_ops);\r\nout_attrs:\r\nkfree(tcmu_attrs);\r\nout_unreg_genl:\r\ngenl_unregister_family(&tcmu_genl_family);\r\nout_unreg_device:\r\nroot_device_unregister(tcmu_root_device);\r\nout_free_cache:\r\nkmem_cache_destroy(tcmu_cmd_cache);\r\nreturn ret;\r\n}\r\nstatic void __exit tcmu_module_exit(void)\r\n{\r\nkthread_stop(unmap_thread);\r\ntarget_backend_unregister(&tcmu_ops);\r\nkfree(tcmu_attrs);\r\ngenl_unregister_family(&tcmu_genl_family);\r\nroot_device_unregister(tcmu_root_device);\r\nkmem_cache_destroy(tcmu_cmd_cache);\r\n}
