static inline bool arc_uncached_addr_space(phys_addr_t paddr)\r\n{\r\nif (is_isa_arcompact()) {\r\nif (paddr >= ARC_UNCACHED_ADDR_SPACE)\r\nreturn true;\r\n} else if (paddr >= perip_base && paddr <= perip_end) {\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nvoid __iomem *ioremap(phys_addr_t paddr, unsigned long size)\r\n{\r\nphys_addr_t end;\r\nend = paddr + size - 1;\r\nif (!size || (end < paddr))\r\nreturn NULL;\r\nif (arc_uncached_addr_space(paddr))\r\nreturn (void __iomem *)(u32)paddr;\r\nreturn ioremap_prot(paddr, size, PAGE_KERNEL_NO_CACHE);\r\n}\r\nvoid __iomem *ioremap_prot(phys_addr_t paddr, unsigned long size,\r\nunsigned long flags)\r\n{\r\nunsigned long vaddr;\r\nstruct vm_struct *area;\r\nphys_addr_t off, end;\r\npgprot_t prot = __pgprot(flags);\r\nend = paddr + size - 1;\r\nif ((!size) || (end < paddr))\r\nreturn NULL;\r\nif (!slab_is_available())\r\nreturn NULL;\r\nprot = pgprot_noncached(prot);\r\noff = paddr & ~PAGE_MASK;\r\npaddr &= PAGE_MASK;\r\nsize = PAGE_ALIGN(end + 1) - paddr;\r\narea = get_vm_area(size, VM_IOREMAP);\r\nif (!area)\r\nreturn NULL;\r\narea->phys_addr = paddr;\r\nvaddr = (unsigned long)area->addr;\r\nif (ioremap_page_range(vaddr, vaddr + size, paddr, prot)) {\r\nvunmap((void __force *)vaddr);\r\nreturn NULL;\r\n}\r\nreturn (void __iomem *)(off + (char __iomem *)vaddr);\r\n}\r\nvoid iounmap(const void __iomem *addr)\r\n{\r\nif (arc_uncached_addr_space((phys_addr_t)(u32)addr))\r\nreturn;\r\nvfree((void *)(PAGE_MASK & (unsigned long __force)addr));\r\n}
