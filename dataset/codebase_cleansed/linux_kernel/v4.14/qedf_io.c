void qedf_cmd_timer_set(struct qedf_ctx *qedf, struct qedf_ioreq *io_req,\r\nunsigned int timer_msec)\r\n{\r\nqueue_delayed_work(qedf->timer_work_queue, &io_req->timeout_work,\r\nmsecs_to_jiffies(timer_msec));\r\n}\r\nstatic void qedf_cmd_timeout(struct work_struct *work)\r\n{\r\nstruct qedf_ioreq *io_req =\r\ncontainer_of(work, struct qedf_ioreq, timeout_work.work);\r\nstruct qedf_ctx *qedf = io_req->fcport->qedf;\r\nstruct qedf_rport *fcport = io_req->fcport;\r\nu8 op = 0;\r\nswitch (io_req->cmd_type) {\r\ncase QEDF_ABTS:\r\nQEDF_ERR((&qedf->dbg_ctx), "ABTS timeout, xid=0x%x.\n",\r\nio_req->xid);\r\nqedf_initiate_cleanup(io_req, true);\r\ncomplete(&io_req->abts_done);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\nqedf_restart_rport(fcport);\r\nbreak;\r\ncase QEDF_ELS:\r\nkref_get(&io_req->refcount);\r\nQEDF_ERR(&(qedf->dbg_ctx), "ELS timeout, xid=0x%x.\n",\r\nio_req->xid);\r\nio_req->event = QEDF_IOREQ_EV_ELS_TMO;\r\nif (io_req->cb_func && io_req->cb_arg) {\r\nop = io_req->cb_arg->op;\r\nio_req->cb_func(io_req->cb_arg);\r\nio_req->cb_arg = NULL;\r\n}\r\nqedf_initiate_cleanup(io_req, true);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\nbreak;\r\ncase QEDF_SEQ_CLEANUP:\r\nQEDF_ERR(&(qedf->dbg_ctx), "Sequence cleanup timeout, "\r\n"xid=0x%x.\n", io_req->xid);\r\nqedf_initiate_cleanup(io_req, true);\r\nio_req->event = QEDF_IOREQ_EV_ELS_TMO;\r\nqedf_process_seq_cleanup_compl(qedf, NULL, io_req);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nvoid qedf_cmd_mgr_free(struct qedf_cmd_mgr *cmgr)\r\n{\r\nstruct io_bdt *bdt_info;\r\nstruct qedf_ctx *qedf = cmgr->qedf;\r\nsize_t bd_tbl_sz;\r\nu16 min_xid = QEDF_MIN_XID;\r\nu16 max_xid = (FCOE_PARAMS_NUM_TASKS - 1);\r\nint num_ios;\r\nint i;\r\nstruct qedf_ioreq *io_req;\r\nnum_ios = max_xid - min_xid + 1;\r\nif (!cmgr->io_bdt_pool)\r\ngoto free_cmd_pool;\r\nbd_tbl_sz = QEDF_MAX_BDS_PER_CMD * sizeof(struct scsi_sge);\r\nfor (i = 0; i < num_ios; i++) {\r\nbdt_info = cmgr->io_bdt_pool[i];\r\nif (bdt_info->bd_tbl) {\r\ndma_free_coherent(&qedf->pdev->dev, bd_tbl_sz,\r\nbdt_info->bd_tbl, bdt_info->bd_tbl_dma);\r\nbdt_info->bd_tbl = NULL;\r\n}\r\n}\r\nfor (i = 0; i < num_ios; i++) {\r\nkfree(cmgr->io_bdt_pool[i]);\r\ncmgr->io_bdt_pool[i] = NULL;\r\n}\r\nkfree(cmgr->io_bdt_pool);\r\ncmgr->io_bdt_pool = NULL;\r\nfree_cmd_pool:\r\nfor (i = 0; i < num_ios; i++) {\r\nio_req = &cmgr->cmds[i];\r\nkfree(io_req->sgl_task_params);\r\nkfree(io_req->task_params);\r\nif (io_req->sense_buffer)\r\ndma_free_coherent(&qedf->pdev->dev,\r\nQEDF_SCSI_SENSE_BUFFERSIZE, io_req->sense_buffer,\r\nio_req->sense_buffer_dma);\r\ncancel_delayed_work_sync(&io_req->rrq_work);\r\n}\r\nvfree(cmgr);\r\n}\r\nstatic void qedf_handle_rrq(struct work_struct *work)\r\n{\r\nstruct qedf_ioreq *io_req =\r\ncontainer_of(work, struct qedf_ioreq, rrq_work.work);\r\nqedf_send_rrq(io_req);\r\n}\r\nstruct qedf_cmd_mgr *qedf_cmd_mgr_alloc(struct qedf_ctx *qedf)\r\n{\r\nstruct qedf_cmd_mgr *cmgr;\r\nstruct io_bdt *bdt_info;\r\nstruct qedf_ioreq *io_req;\r\nu16 xid;\r\nint i;\r\nint num_ios;\r\nu16 min_xid = QEDF_MIN_XID;\r\nu16 max_xid = (FCOE_PARAMS_NUM_TASKS - 1);\r\nif (!qedf->num_queues) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "num_queues is not set.\n");\r\nreturn NULL;\r\n}\r\nif (max_xid <= min_xid || max_xid == FC_XID_UNKNOWN) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "Invalid min_xid 0x%x and "\r\n"max_xid 0x%x.\n", min_xid, max_xid);\r\nreturn NULL;\r\n}\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_DISC, "min xid 0x%x, max xid "\r\n"0x%x.\n", min_xid, max_xid);\r\nnum_ios = max_xid - min_xid + 1;\r\ncmgr = vzalloc(sizeof(struct qedf_cmd_mgr));\r\nif (!cmgr) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "Failed to alloc cmd mgr.\n");\r\nreturn NULL;\r\n}\r\ncmgr->qedf = qedf;\r\nspin_lock_init(&cmgr->lock);\r\nxid = QEDF_MIN_XID;\r\nfor (i = 0; i < num_ios; i++) {\r\nio_req = &cmgr->cmds[i];\r\nINIT_DELAYED_WORK(&io_req->timeout_work, qedf_cmd_timeout);\r\nio_req->xid = xid++;\r\nINIT_DELAYED_WORK(&io_req->rrq_work, qedf_handle_rrq);\r\nio_req->sense_buffer = dma_alloc_coherent(&qedf->pdev->dev,\r\nQEDF_SCSI_SENSE_BUFFERSIZE, &io_req->sense_buffer_dma,\r\nGFP_KERNEL);\r\nif (!io_req->sense_buffer)\r\ngoto mem_err;\r\nio_req->task_params = kzalloc(sizeof(*io_req->task_params),\r\nGFP_KERNEL);\r\nif (!io_req->task_params) {\r\nQEDF_ERR(&(qedf->dbg_ctx),\r\n"Failed to allocate task_params for xid=0x%x\n",\r\ni);\r\ngoto mem_err;\r\n}\r\nio_req->sgl_task_params = kzalloc(\r\nsizeof(struct scsi_sgl_task_params), GFP_KERNEL);\r\nif (!io_req->sgl_task_params) {\r\nQEDF_ERR(&(qedf->dbg_ctx),\r\n"Failed to allocate sgl_task_params for xid=0x%x\n",\r\ni);\r\ngoto mem_err;\r\n}\r\n}\r\ncmgr->io_bdt_pool = kmalloc_array(num_ios, sizeof(struct io_bdt *),\r\nGFP_KERNEL);\r\nif (!cmgr->io_bdt_pool) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "Failed to alloc io_bdt_pool.\n");\r\ngoto mem_err;\r\n}\r\nfor (i = 0; i < num_ios; i++) {\r\ncmgr->io_bdt_pool[i] = kmalloc(sizeof(struct io_bdt),\r\nGFP_KERNEL);\r\nif (!cmgr->io_bdt_pool[i]) {\r\nQEDF_WARN(&(qedf->dbg_ctx),\r\n"Failed to alloc io_bdt_pool[%d].\n", i);\r\ngoto mem_err;\r\n}\r\n}\r\nfor (i = 0; i < num_ios; i++) {\r\nbdt_info = cmgr->io_bdt_pool[i];\r\nbdt_info->bd_tbl = dma_alloc_coherent(&qedf->pdev->dev,\r\nQEDF_MAX_BDS_PER_CMD * sizeof(struct scsi_sge),\r\n&bdt_info->bd_tbl_dma, GFP_KERNEL);\r\nif (!bdt_info->bd_tbl) {\r\nQEDF_WARN(&(qedf->dbg_ctx),\r\n"Failed to alloc bdt_tbl[%d].\n", i);\r\ngoto mem_err;\r\n}\r\n}\r\natomic_set(&cmgr->free_list_cnt, num_ios);\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"cmgr->free_list_cnt=%d.\n",\r\natomic_read(&cmgr->free_list_cnt));\r\nreturn cmgr;\r\nmem_err:\r\nqedf_cmd_mgr_free(cmgr);\r\nreturn NULL;\r\n}\r\nstruct qedf_ioreq *qedf_alloc_cmd(struct qedf_rport *fcport, u8 cmd_type)\r\n{\r\nstruct qedf_ctx *qedf = fcport->qedf;\r\nstruct qedf_cmd_mgr *cmd_mgr = qedf->cmd_mgr;\r\nstruct qedf_ioreq *io_req = NULL;\r\nstruct io_bdt *bd_tbl;\r\nu16 xid;\r\nuint32_t free_sqes;\r\nint i;\r\nunsigned long flags;\r\nfree_sqes = atomic_read(&fcport->free_sqes);\r\nif (!free_sqes) {\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"Returning NULL, free_sqes=%d.\n ",\r\nfree_sqes);\r\ngoto out_failed;\r\n}\r\nif ((atomic_read(&fcport->num_active_ios) >=\r\nNUM_RW_TASKS_PER_CONNECTION)) {\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"Returning NULL, num_active_ios=%d.\n",\r\natomic_read(&fcport->num_active_ios));\r\ngoto out_failed;\r\n}\r\nif (atomic_read(&cmd_mgr->free_list_cnt) <= GBL_RSVD_TASKS) {\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"Returning NULL, free_list_cnt=%d.\n",\r\natomic_read(&cmd_mgr->free_list_cnt));\r\ngoto out_failed;\r\n}\r\nspin_lock_irqsave(&cmd_mgr->lock, flags);\r\nfor (i = 0; i < FCOE_PARAMS_NUM_TASKS; i++) {\r\nio_req = &cmd_mgr->cmds[cmd_mgr->idx];\r\ncmd_mgr->idx++;\r\nif (cmd_mgr->idx == FCOE_PARAMS_NUM_TASKS)\r\ncmd_mgr->idx = 0;\r\nif (!test_bit(QEDF_CMD_OUTSTANDING, &io_req->flags))\r\nbreak;\r\n}\r\nif (i == FCOE_PARAMS_NUM_TASKS) {\r\nspin_unlock_irqrestore(&cmd_mgr->lock, flags);\r\ngoto out_failed;\r\n}\r\nset_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\r\nspin_unlock_irqrestore(&cmd_mgr->lock, flags);\r\natomic_inc(&fcport->num_active_ios);\r\natomic_dec(&fcport->free_sqes);\r\nxid = io_req->xid;\r\natomic_dec(&cmd_mgr->free_list_cnt);\r\nio_req->cmd_mgr = cmd_mgr;\r\nio_req->fcport = fcport;\r\nkref_init(&io_req->refcount);\r\nbd_tbl = io_req->bd_tbl = cmd_mgr->io_bdt_pool[xid];\r\nif (bd_tbl == NULL) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "bd_tbl is NULL, xid=%x.\n", xid);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\ngoto out_failed;\r\n}\r\nbd_tbl->io_req = io_req;\r\nio_req->cmd_type = cmd_type;\r\nio_req->tm_flags = 0;\r\nio_req->rx_buf_off = 0;\r\nio_req->tx_buf_off = 0;\r\nio_req->rx_id = 0xffff;\r\nreturn io_req;\r\nout_failed:\r\nqedf->alloc_failures++;\r\nreturn NULL;\r\n}\r\nstatic void qedf_free_mp_resc(struct qedf_ioreq *io_req)\r\n{\r\nstruct qedf_mp_req *mp_req = &(io_req->mp_req);\r\nstruct qedf_ctx *qedf = io_req->fcport->qedf;\r\nuint64_t sz = sizeof(struct scsi_sge);\r\nif (mp_req->mp_req_bd) {\r\ndma_free_coherent(&qedf->pdev->dev, sz,\r\nmp_req->mp_req_bd, mp_req->mp_req_bd_dma);\r\nmp_req->mp_req_bd = NULL;\r\n}\r\nif (mp_req->mp_resp_bd) {\r\ndma_free_coherent(&qedf->pdev->dev, sz,\r\nmp_req->mp_resp_bd, mp_req->mp_resp_bd_dma);\r\nmp_req->mp_resp_bd = NULL;\r\n}\r\nif (mp_req->req_buf) {\r\ndma_free_coherent(&qedf->pdev->dev, QEDF_PAGE_SIZE,\r\nmp_req->req_buf, mp_req->req_buf_dma);\r\nmp_req->req_buf = NULL;\r\n}\r\nif (mp_req->resp_buf) {\r\ndma_free_coherent(&qedf->pdev->dev, QEDF_PAGE_SIZE,\r\nmp_req->resp_buf, mp_req->resp_buf_dma);\r\nmp_req->resp_buf = NULL;\r\n}\r\n}\r\nvoid qedf_release_cmd(struct kref *ref)\r\n{\r\nstruct qedf_ioreq *io_req =\r\ncontainer_of(ref, struct qedf_ioreq, refcount);\r\nstruct qedf_cmd_mgr *cmd_mgr = io_req->cmd_mgr;\r\nstruct qedf_rport *fcport = io_req->fcport;\r\nif (io_req->cmd_type == QEDF_ELS ||\r\nio_req->cmd_type == QEDF_TASK_MGMT_CMD)\r\nqedf_free_mp_resc(io_req);\r\natomic_inc(&cmd_mgr->free_list_cnt);\r\natomic_dec(&fcport->num_active_ios);\r\nif (atomic_read(&fcport->num_active_ios) < 0)\r\nQEDF_WARN(&(fcport->qedf->dbg_ctx), "active_ios < 0.\n");\r\nio_req->task_retry_identifier++;\r\nclear_bit(QEDF_CMD_OUTSTANDING, &io_req->flags);\r\n}\r\nstatic int qedf_split_bd(struct qedf_ioreq *io_req, u64 addr, int sg_len,\r\nint bd_index)\r\n{\r\nstruct scsi_sge *bd = io_req->bd_tbl->bd_tbl;\r\nint frag_size, sg_frags;\r\nsg_frags = 0;\r\nwhile (sg_len) {\r\nif (sg_len > QEDF_BD_SPLIT_SZ)\r\nfrag_size = QEDF_BD_SPLIT_SZ;\r\nelse\r\nfrag_size = sg_len;\r\nbd[bd_index + sg_frags].sge_addr.lo = U64_LO(addr);\r\nbd[bd_index + sg_frags].sge_addr.hi = U64_HI(addr);\r\nbd[bd_index + sg_frags].sge_len = (uint16_t)frag_size;\r\naddr += (u64)frag_size;\r\nsg_frags++;\r\nsg_len -= frag_size;\r\n}\r\nreturn sg_frags;\r\n}\r\nstatic int qedf_map_sg(struct qedf_ioreq *io_req)\r\n{\r\nstruct scsi_cmnd *sc = io_req->sc_cmd;\r\nstruct Scsi_Host *host = sc->device->host;\r\nstruct fc_lport *lport = shost_priv(host);\r\nstruct qedf_ctx *qedf = lport_priv(lport);\r\nstruct scsi_sge *bd = io_req->bd_tbl->bd_tbl;\r\nstruct scatterlist *sg;\r\nint byte_count = 0;\r\nint sg_count = 0;\r\nint bd_count = 0;\r\nint sg_frags;\r\nunsigned int sg_len;\r\nu64 addr, end_addr;\r\nint i;\r\nsg_count = dma_map_sg(&qedf->pdev->dev, scsi_sglist(sc),\r\nscsi_sg_count(sc), sc->sc_data_direction);\r\nsg = scsi_sglist(sc);\r\nif ((sg_count == 1) && (sg_dma_len(sg) <=\r\nQEDF_MAX_SGLEN_FOR_CACHESGL)) {\r\nsg_len = sg_dma_len(sg);\r\naddr = (u64)sg_dma_address(sg);\r\nbd[bd_count].sge_addr.lo = (addr & 0xffffffff);\r\nbd[bd_count].sge_addr.hi = (addr >> 32);\r\nbd[bd_count].sge_len = (u16)sg_len;\r\nreturn ++bd_count;\r\n}\r\nscsi_for_each_sg(sc, sg, sg_count, i) {\r\nsg_len = sg_dma_len(sg);\r\naddr = (u64)sg_dma_address(sg);\r\nend_addr = (u64)(addr + sg_len);\r\nif ((i == 0) && (sg_count > 1) &&\r\n((end_addr % QEDF_PAGE_SIZE) ||\r\nsg_len < QEDF_PAGE_SIZE))\r\nio_req->use_slowpath = true;\r\nelse if ((i == (sg_count - 1)) && (sg_count > 1) &&\r\n(addr % QEDF_PAGE_SIZE))\r\nio_req->use_slowpath = true;\r\nelse if ((i != 0) && (i != (sg_count - 1)) &&\r\n((addr % QEDF_PAGE_SIZE) || (end_addr % QEDF_PAGE_SIZE)))\r\nio_req->use_slowpath = true;\r\nif (sg_len > QEDF_MAX_BD_LEN) {\r\nsg_frags = qedf_split_bd(io_req, addr, sg_len,\r\nbd_count);\r\n} else {\r\nsg_frags = 1;\r\nbd[bd_count].sge_addr.lo = U64_LO(addr);\r\nbd[bd_count].sge_addr.hi = U64_HI(addr);\r\nbd[bd_count].sge_len = (uint16_t)sg_len;\r\n}\r\nbd_count += sg_frags;\r\nbyte_count += sg_len;\r\n}\r\nif (byte_count != scsi_bufflen(sc))\r\nQEDF_ERR(&(qedf->dbg_ctx), "byte_count = %d != "\r\n"scsi_bufflen = %d, task_id = 0x%x.\n", byte_count,\r\nscsi_bufflen(sc), io_req->xid);\r\nreturn bd_count;\r\n}\r\nstatic int qedf_build_bd_list_from_sg(struct qedf_ioreq *io_req)\r\n{\r\nstruct scsi_cmnd *sc = io_req->sc_cmd;\r\nstruct scsi_sge *bd = io_req->bd_tbl->bd_tbl;\r\nint bd_count;\r\nif (scsi_sg_count(sc)) {\r\nbd_count = qedf_map_sg(io_req);\r\nif (bd_count == 0)\r\nreturn -ENOMEM;\r\n} else {\r\nbd_count = 0;\r\nbd[0].sge_addr.lo = bd[0].sge_addr.hi = 0;\r\nbd[0].sge_len = 0;\r\n}\r\nio_req->bd_tbl->bd_valid = bd_count;\r\nreturn 0;\r\n}\r\nstatic void qedf_build_fcp_cmnd(struct qedf_ioreq *io_req,\r\nstruct fcp_cmnd *fcp_cmnd)\r\n{\r\nstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\r\nmemset(fcp_cmnd, 0, FCP_CMND_LEN);\r\nint_to_scsilun(sc_cmd->device->lun,\r\n(struct scsi_lun *)&fcp_cmnd->fc_lun);\r\nfcp_cmnd->fc_pri_ta = 0;\r\nfcp_cmnd->fc_tm_flags = io_req->tm_flags;\r\nfcp_cmnd->fc_flags = io_req->io_req_flags;\r\nfcp_cmnd->fc_cmdref = 0;\r\nif (io_req->cmd_type == QEDF_TASK_MGMT_CMD) {\r\nfcp_cmnd->fc_flags |= FCP_CFL_RDDATA;\r\n} else {\r\nif (sc_cmd->sc_data_direction == DMA_TO_DEVICE)\r\nfcp_cmnd->fc_flags |= FCP_CFL_WRDATA;\r\nelse if (sc_cmd->sc_data_direction == DMA_FROM_DEVICE)\r\nfcp_cmnd->fc_flags |= FCP_CFL_RDDATA;\r\n}\r\nfcp_cmnd->fc_pri_ta = FCP_PTA_SIMPLE;\r\nif (io_req->cmd_type != QEDF_TASK_MGMT_CMD)\r\nmemcpy(fcp_cmnd->fc_cdb, sc_cmd->cmnd, sc_cmd->cmd_len);\r\nfcp_cmnd->fc_dl = htonl(io_req->data_xfer_len);\r\n}\r\nstatic void qedf_init_task(struct qedf_rport *fcport, struct fc_lport *lport,\r\nstruct qedf_ioreq *io_req, struct fcoe_task_context *task_ctx,\r\nstruct fcoe_wqe *sqe)\r\n{\r\nenum fcoe_task_type task_type;\r\nstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\r\nstruct io_bdt *bd_tbl = io_req->bd_tbl;\r\nu8 fcp_cmnd[32];\r\nu32 tmp_fcp_cmnd[8];\r\nint bd_count = 0;\r\nstruct qedf_ctx *qedf = fcport->qedf;\r\nuint16_t cq_idx = smp_processor_id() % qedf->num_queues;\r\nstruct regpair sense_data_buffer_phys_addr;\r\nu32 tx_io_size = 0;\r\nu32 rx_io_size = 0;\r\nint i, cnt;\r\nio_req->task = task_ctx;\r\nmemset(task_ctx, 0, sizeof(struct fcoe_task_context));\r\nmemset(io_req->task_params, 0, sizeof(struct fcoe_task_params));\r\nmemset(io_req->sgl_task_params, 0, sizeof(struct scsi_sgl_task_params));\r\nif (io_req->cmd_type == QEDF_TASK_MGMT_CMD) {\r\ntask_type = FCOE_TASK_TYPE_READ_INITIATOR;\r\n} else {\r\nif (sc_cmd->sc_data_direction == DMA_TO_DEVICE) {\r\ntask_type = FCOE_TASK_TYPE_WRITE_INITIATOR;\r\ntx_io_size = io_req->data_xfer_len;\r\n} else {\r\ntask_type = FCOE_TASK_TYPE_READ_INITIATOR;\r\nrx_io_size = io_req->data_xfer_len;\r\n}\r\n}\r\nio_req->task_params->context = task_ctx;\r\nio_req->task_params->sqe = sqe;\r\nio_req->task_params->task_type = task_type;\r\nio_req->task_params->tx_io_size = tx_io_size;\r\nio_req->task_params->rx_io_size = rx_io_size;\r\nio_req->task_params->conn_cid = fcport->fw_cid;\r\nio_req->task_params->itid = io_req->xid;\r\nio_req->task_params->cq_rss_number = cq_idx;\r\nio_req->task_params->is_tape_device = fcport->dev_type;\r\nif (io_req->cmd_type != QEDF_TASK_MGMT_CMD) {\r\nbd_count = bd_tbl->bd_valid;\r\nio_req->sgl_task_params->sgl = bd_tbl->bd_tbl;\r\nio_req->sgl_task_params->sgl_phys_addr.lo =\r\nU64_LO(bd_tbl->bd_tbl_dma);\r\nio_req->sgl_task_params->sgl_phys_addr.hi =\r\nU64_HI(bd_tbl->bd_tbl_dma);\r\nio_req->sgl_task_params->num_sges = bd_count;\r\nio_req->sgl_task_params->total_buffer_size =\r\nscsi_bufflen(io_req->sc_cmd);\r\nio_req->sgl_task_params->small_mid_sge =\r\nio_req->use_slowpath;\r\n}\r\nsense_data_buffer_phys_addr.lo = U64_LO(io_req->sense_buffer_dma);\r\nsense_data_buffer_phys_addr.hi = U64_HI(io_req->sense_buffer_dma);\r\nqedf_build_fcp_cmnd(io_req, (struct fcp_cmnd *)tmp_fcp_cmnd);\r\ncnt = sizeof(struct fcp_cmnd) / sizeof(u32);\r\nfor (i = 0; i < cnt; i++) {\r\ntmp_fcp_cmnd[i] = cpu_to_be32(tmp_fcp_cmnd[i]);\r\n}\r\nmemcpy(fcp_cmnd, tmp_fcp_cmnd, sizeof(struct fcp_cmnd));\r\ninit_initiator_rw_fcoe_task(io_req->task_params,\r\nio_req->sgl_task_params,\r\nsense_data_buffer_phys_addr,\r\nio_req->task_retry_identifier, fcp_cmnd);\r\nif (bd_count == 1) {\r\nqedf->single_sge_ios++;\r\nio_req->sge_type = QEDF_IOREQ_SINGLE_SGE;\r\n} else if (io_req->use_slowpath) {\r\nqedf->slow_sge_ios++;\r\nio_req->sge_type = QEDF_IOREQ_SLOW_SGE;\r\n} else {\r\nqedf->fast_sge_ios++;\r\nio_req->sge_type = QEDF_IOREQ_FAST_SGE;\r\n}\r\n}\r\nvoid qedf_init_mp_task(struct qedf_ioreq *io_req,\r\nstruct fcoe_task_context *task_ctx, struct fcoe_wqe *sqe)\r\n{\r\nstruct qedf_mp_req *mp_req = &(io_req->mp_req);\r\nstruct qedf_rport *fcport = io_req->fcport;\r\nstruct qedf_ctx *qedf = io_req->fcport->qedf;\r\nstruct fc_frame_header *fc_hdr;\r\nstruct fcoe_tx_mid_path_params task_fc_hdr;\r\nstruct scsi_sgl_task_params tx_sgl_task_params;\r\nstruct scsi_sgl_task_params rx_sgl_task_params;\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_DISC,\r\n"Initializing MP task for cmd_type=%d\n",\r\nio_req->cmd_type);\r\nqedf->control_requests++;\r\nmemset(&tx_sgl_task_params, 0, sizeof(struct scsi_sgl_task_params));\r\nmemset(&rx_sgl_task_params, 0, sizeof(struct scsi_sgl_task_params));\r\nmemset(task_ctx, 0, sizeof(struct fcoe_task_context));\r\nmemset(&task_fc_hdr, 0, sizeof(struct fcoe_tx_mid_path_params));\r\nio_req->task = task_ctx;\r\nio_req->task_params->context = task_ctx;\r\nio_req->task_params->sqe = sqe;\r\nio_req->task_params->task_type = FCOE_TASK_TYPE_MIDPATH;\r\nio_req->task_params->tx_io_size = io_req->data_xfer_len;\r\nio_req->task_params->rx_io_size = PAGE_SIZE;\r\nio_req->task_params->conn_cid = fcport->fw_cid;\r\nio_req->task_params->itid = io_req->xid;\r\nio_req->task_params->cq_rss_number = 0;\r\nio_req->task_params->is_tape_device = fcport->dev_type;\r\nfc_hdr = &(mp_req->req_fc_hdr);\r\nfc_hdr->fh_ox_id = io_req->xid;\r\nfc_hdr->fh_rx_id = htons(0xffff);\r\ntask_fc_hdr.parameter = fc_hdr->fh_parm_offset;\r\ntask_fc_hdr.r_ctl = fc_hdr->fh_r_ctl;\r\ntask_fc_hdr.type = fc_hdr->fh_type;\r\ntask_fc_hdr.cs_ctl = fc_hdr->fh_cs_ctl;\r\ntask_fc_hdr.df_ctl = fc_hdr->fh_df_ctl;\r\ntask_fc_hdr.rx_id = fc_hdr->fh_rx_id;\r\ntask_fc_hdr.ox_id = fc_hdr->fh_ox_id;\r\ntx_sgl_task_params.sgl = mp_req->mp_req_bd;\r\ntx_sgl_task_params.sgl_phys_addr.lo = U64_LO(mp_req->mp_req_bd_dma);\r\ntx_sgl_task_params.sgl_phys_addr.hi = U64_HI(mp_req->mp_req_bd_dma);\r\ntx_sgl_task_params.num_sges = 1;\r\ntx_sgl_task_params.total_buffer_size = io_req->data_xfer_len;\r\ntx_sgl_task_params.small_mid_sge = 0;\r\nrx_sgl_task_params.sgl = mp_req->mp_resp_bd;\r\nrx_sgl_task_params.sgl_phys_addr.lo = U64_LO(mp_req->mp_resp_bd_dma);\r\nrx_sgl_task_params.sgl_phys_addr.hi = U64_HI(mp_req->mp_resp_bd_dma);\r\nrx_sgl_task_params.num_sges = 1;\r\nrx_sgl_task_params.total_buffer_size = PAGE_SIZE;\r\nrx_sgl_task_params.small_mid_sge = 0;\r\ninit_initiator_midpath_unsolicited_fcoe_task(io_req->task_params,\r\n&task_fc_hdr,\r\n&tx_sgl_task_params,\r\n&rx_sgl_task_params, 0);\r\nqedf->single_sge_ios++;\r\n}\r\nu16 qedf_get_sqe_idx(struct qedf_rport *fcport)\r\n{\r\nuint16_t total_sqe = (fcport->sq_mem_size)/(sizeof(struct fcoe_wqe));\r\nu16 rval;\r\nrval = fcport->sq_prod_idx;\r\nfcport->sq_prod_idx++;\r\nfcport->fw_sq_prod_idx++;\r\nif (fcport->sq_prod_idx == total_sqe)\r\nfcport->sq_prod_idx = 0;\r\nreturn rval;\r\n}\r\nvoid qedf_ring_doorbell(struct qedf_rport *fcport)\r\n{\r\nstruct fcoe_db_data dbell = { 0 };\r\ndbell.agg_flags = 0;\r\ndbell.params |= DB_DEST_XCM << FCOE_DB_DATA_DEST_SHIFT;\r\ndbell.params |= DB_AGG_CMD_SET << FCOE_DB_DATA_AGG_CMD_SHIFT;\r\ndbell.params |= DQ_XCM_FCOE_SQ_PROD_CMD <<\r\nFCOE_DB_DATA_AGG_VAL_SEL_SHIFT;\r\ndbell.sq_prod = fcport->fw_sq_prod_idx;\r\nwritel(*(u32 *)&dbell, fcport->p_doorbell);\r\nwmb();\r\nmmiowb();\r\n}\r\nstatic void qedf_trace_io(struct qedf_rport *fcport, struct qedf_ioreq *io_req,\r\nint8_t direction)\r\n{\r\nstruct qedf_ctx *qedf = fcport->qedf;\r\nstruct qedf_io_log *io_log;\r\nstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\r\nunsigned long flags;\r\nuint8_t op;\r\nspin_lock_irqsave(&qedf->io_trace_lock, flags);\r\nio_log = &qedf->io_trace_buf[qedf->io_trace_idx];\r\nio_log->direction = direction;\r\nio_log->task_id = io_req->xid;\r\nio_log->port_id = fcport->rdata->ids.port_id;\r\nio_log->lun = sc_cmd->device->lun;\r\nio_log->op = op = sc_cmd->cmnd[0];\r\nio_log->lba[0] = sc_cmd->cmnd[2];\r\nio_log->lba[1] = sc_cmd->cmnd[3];\r\nio_log->lba[2] = sc_cmd->cmnd[4];\r\nio_log->lba[3] = sc_cmd->cmnd[5];\r\nio_log->bufflen = scsi_bufflen(sc_cmd);\r\nio_log->sg_count = scsi_sg_count(sc_cmd);\r\nio_log->result = sc_cmd->result;\r\nio_log->jiffies = jiffies;\r\nio_log->refcount = kref_read(&io_req->refcount);\r\nif (direction == QEDF_IO_TRACE_REQ) {\r\nio_log->req_cpu = io_req->cpu;\r\nio_log->int_cpu = 0;\r\nio_log->rsp_cpu = 0;\r\n} else if (direction == QEDF_IO_TRACE_RSP) {\r\nio_log->req_cpu = io_req->cpu;\r\nio_log->int_cpu = io_req->int_cpu;\r\nio_log->rsp_cpu = smp_processor_id();\r\n}\r\nio_log->sge_type = io_req->sge_type;\r\nqedf->io_trace_idx++;\r\nif (qedf->io_trace_idx == QEDF_IO_TRACE_SIZE)\r\nqedf->io_trace_idx = 0;\r\nspin_unlock_irqrestore(&qedf->io_trace_lock, flags);\r\n}\r\nint qedf_post_io_req(struct qedf_rport *fcport, struct qedf_ioreq *io_req)\r\n{\r\nstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\r\nstruct Scsi_Host *host = sc_cmd->device->host;\r\nstruct fc_lport *lport = shost_priv(host);\r\nstruct qedf_ctx *qedf = lport_priv(lport);\r\nstruct fcoe_task_context *task_ctx;\r\nu16 xid;\r\nenum fcoe_task_type req_type = 0;\r\nstruct fcoe_wqe *sqe;\r\nu16 sqe_idx;\r\nio_req->data_xfer_len = scsi_bufflen(sc_cmd);\r\nsc_cmd->SCp.ptr = (char *)io_req;\r\nio_req->use_slowpath = false;\r\nio_req->cpu = smp_processor_id();\r\nif (sc_cmd->sc_data_direction == DMA_FROM_DEVICE) {\r\nreq_type = FCOE_TASK_TYPE_READ_INITIATOR;\r\nio_req->io_req_flags = QEDF_READ;\r\nqedf->input_requests++;\r\n} else if (sc_cmd->sc_data_direction == DMA_TO_DEVICE) {\r\nreq_type = FCOE_TASK_TYPE_WRITE_INITIATOR;\r\nio_req->io_req_flags = QEDF_WRITE;\r\nqedf->output_requests++;\r\n} else {\r\nio_req->io_req_flags = 0;\r\nqedf->control_requests++;\r\n}\r\nxid = io_req->xid;\r\nif (qedf_build_bd_list_from_sg(io_req)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "BD list creation failed.\n");\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\nreturn -EAGAIN;\r\n}\r\nif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Session not offloaded yet.\n");\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\n}\r\nsqe_idx = qedf_get_sqe_idx(fcport);\r\nsqe = &fcport->sq[sqe_idx];\r\nmemset(sqe, 0, sizeof(struct fcoe_wqe));\r\ntask_ctx = qedf_get_task_mem(&qedf->tasks, xid);\r\nif (!task_ctx) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "task_ctx is NULL, xid=%d.\n",\r\nxid);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\nreturn -EINVAL;\r\n}\r\nqedf_init_task(fcport, lport, io_req, task_ctx, sqe);\r\nqedf_ring_doorbell(fcport);\r\nif (qedf_io_tracing && io_req->sc_cmd)\r\nqedf_trace_io(fcport, io_req, QEDF_IO_TRACE_REQ);\r\nreturn false;\r\n}\r\nint\r\nqedf_queuecommand(struct Scsi_Host *host, struct scsi_cmnd *sc_cmd)\r\n{\r\nstruct fc_lport *lport = shost_priv(host);\r\nstruct qedf_ctx *qedf = lport_priv(lport);\r\nstruct fc_rport *rport = starget_to_rport(scsi_target(sc_cmd->device));\r\nstruct fc_rport_libfc_priv *rp = rport->dd_data;\r\nstruct qedf_rport *fcport = rport->dd_data;\r\nstruct qedf_ioreq *io_req;\r\nint rc = 0;\r\nint rval;\r\nunsigned long flags = 0;\r\nif (test_bit(QEDF_UNLOADING, &qedf->flags) ||\r\ntest_bit(QEDF_DBG_STOP_IO, &qedf->flags)) {\r\nsc_cmd->result = DID_NO_CONNECT << 16;\r\nsc_cmd->scsi_done(sc_cmd);\r\nreturn 0;\r\n}\r\nrval = fc_remote_port_chkready(rport);\r\nif (rval) {\r\nsc_cmd->result = rval;\r\nsc_cmd->scsi_done(sc_cmd);\r\nreturn 0;\r\n}\r\nif (test_bit(QEDF_DRAIN_ACTIVE, &qedf->flags)) {\r\nrc = SCSI_MLQUEUE_HOST_BUSY;\r\ngoto exit_qcmd;\r\n}\r\nif (lport->state != LPORT_ST_READY ||\r\natomic_read(&qedf->link_state) != QEDF_LINK_UP) {\r\nrc = SCSI_MLQUEUE_HOST_BUSY;\r\ngoto exit_qcmd;\r\n}\r\nfcport = (struct qedf_rport *)&rp[1];\r\nif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\r\nrc = SCSI_MLQUEUE_TARGET_BUSY;\r\ngoto exit_qcmd;\r\n}\r\nif (fcport->retry_delay_timestamp) {\r\nif (time_after(jiffies, fcport->retry_delay_timestamp)) {\r\nfcport->retry_delay_timestamp = 0;\r\n} else {\r\nrc = SCSI_MLQUEUE_TARGET_BUSY;\r\ngoto exit_qcmd;\r\n}\r\n}\r\nio_req = qedf_alloc_cmd(fcport, QEDF_SCSI_CMD);\r\nif (!io_req) {\r\nrc = SCSI_MLQUEUE_HOST_BUSY;\r\ngoto exit_qcmd;\r\n}\r\nio_req->sc_cmd = sc_cmd;\r\nspin_lock_irqsave(&fcport->rport_lock, flags);\r\nif (qedf_post_io_req(fcport, io_req)) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "Unable to post io_req\n");\r\natomic_inc(&fcport->free_sqes);\r\nrc = SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\nspin_unlock_irqrestore(&fcport->rport_lock, flags);\r\nexit_qcmd:\r\nreturn rc;\r\n}\r\nstatic void qedf_parse_fcp_rsp(struct qedf_ioreq *io_req,\r\nstruct fcoe_cqe_rsp_info *fcp_rsp)\r\n{\r\nstruct scsi_cmnd *sc_cmd = io_req->sc_cmd;\r\nstruct qedf_ctx *qedf = io_req->fcport->qedf;\r\nu8 rsp_flags = fcp_rsp->rsp_flags.flags;\r\nint fcp_sns_len = 0;\r\nint fcp_rsp_len = 0;\r\nuint8_t *rsp_info, *sense_data;\r\nio_req->fcp_status = FC_GOOD;\r\nio_req->fcp_resid = 0;\r\nif (rsp_flags & (FCOE_FCP_RSP_FLAGS_FCP_RESID_OVER |\r\nFCOE_FCP_RSP_FLAGS_FCP_RESID_UNDER))\r\nio_req->fcp_resid = fcp_rsp->fcp_resid;\r\nio_req->scsi_comp_flags = rsp_flags;\r\nCMD_SCSI_STATUS(sc_cmd) = io_req->cdb_status =\r\nfcp_rsp->scsi_status_code;\r\nif (rsp_flags &\r\nFCOE_FCP_RSP_FLAGS_FCP_RSP_LEN_VALID)\r\nfcp_rsp_len = fcp_rsp->fcp_rsp_len;\r\nif (rsp_flags &\r\nFCOE_FCP_RSP_FLAGS_FCP_SNS_LEN_VALID)\r\nfcp_sns_len = fcp_rsp->fcp_sns_len;\r\nio_req->fcp_rsp_len = fcp_rsp_len;\r\nio_req->fcp_sns_len = fcp_sns_len;\r\nrsp_info = sense_data = io_req->sense_buffer;\r\nif ((fcp_rsp_len == 4) || (fcp_rsp_len == 8)) {\r\nio_req->fcp_rsp_code = rsp_info[3];\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"fcp_rsp_code = %d\n", io_req->fcp_rsp_code);\r\nsense_data += fcp_rsp_len;\r\n}\r\nif (fcp_sns_len > SCSI_SENSE_BUFFERSIZE) {\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"Truncating sense buffer\n");\r\nfcp_sns_len = SCSI_SENSE_BUFFERSIZE;\r\n}\r\nif (sc_cmd->sense_buffer) {\r\nmemset(sc_cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\r\nif (fcp_sns_len)\r\nmemcpy(sc_cmd->sense_buffer, sense_data,\r\nfcp_sns_len);\r\n}\r\n}\r\nstatic void qedf_unmap_sg_list(struct qedf_ctx *qedf, struct qedf_ioreq *io_req)\r\n{\r\nstruct scsi_cmnd *sc = io_req->sc_cmd;\r\nif (io_req->bd_tbl->bd_valid && sc && scsi_sg_count(sc)) {\r\ndma_unmap_sg(&qedf->pdev->dev, scsi_sglist(sc),\r\nscsi_sg_count(sc), sc->sc_data_direction);\r\nio_req->bd_tbl->bd_valid = 0;\r\n}\r\n}\r\nvoid qedf_scsi_completion(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\r\nstruct qedf_ioreq *io_req)\r\n{\r\nu16 xid, rval;\r\nstruct fcoe_task_context *task_ctx;\r\nstruct scsi_cmnd *sc_cmd;\r\nstruct fcoe_cqe_rsp_info *fcp_rsp;\r\nstruct qedf_rport *fcport;\r\nint refcount;\r\nu16 scope, qualifier = 0;\r\nu8 fw_residual_flag = 0;\r\nif (!io_req)\r\nreturn;\r\nif (!cqe)\r\nreturn;\r\nxid = io_req->xid;\r\ntask_ctx = qedf_get_task_mem(&qedf->tasks, xid);\r\nsc_cmd = io_req->sc_cmd;\r\nfcp_rsp = &cqe->cqe_info.rsp_info;\r\nif (!sc_cmd) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "sc_cmd is NULL!\n");\r\nreturn;\r\n}\r\nif (!sc_cmd->SCp.ptr) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "SCp.ptr is NULL, returned in "\r\n"another context.\n");\r\nreturn;\r\n}\r\nif (!sc_cmd->request) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "sc_cmd->request is NULL, "\r\n"sc_cmd=%p.\n", sc_cmd);\r\nreturn;\r\n}\r\nif (!sc_cmd->request->special) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "request->special is NULL so "\r\n"request not valid, sc_cmd=%p.\n", sc_cmd);\r\nreturn;\r\n}\r\nif (!sc_cmd->request->q) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "request->q is NULL so request "\r\n"is not valid, sc_cmd=%p.\n", sc_cmd);\r\nreturn;\r\n}\r\nfcport = io_req->fcport;\r\nqedf_parse_fcp_rsp(io_req, fcp_rsp);\r\nqedf_unmap_sg_list(qedf, io_req);\r\nif (io_req->fcp_rsp_len > 3 && io_req->fcp_rsp_code) {\r\nQEDF_ERR(&(qedf->dbg_ctx),\r\n"FCP I/O protocol failure xid=0x%x fcp_rsp_len=%d "\r\n"fcp_rsp_code=%d.\n", io_req->xid, io_req->fcp_rsp_len,\r\nio_req->fcp_rsp_code);\r\nsc_cmd->result = DID_BUS_BUSY << 16;\r\ngoto out;\r\n}\r\nfw_residual_flag = GET_FIELD(cqe->cqe_info.rsp_info.fw_error_flags,\r\nFCOE_CQE_RSP_INFO_FW_UNDERRUN);\r\nif (fw_residual_flag) {\r\nQEDF_ERR(&(qedf->dbg_ctx),\r\n"Firmware detected underrun: xid=0x%x fcp_rsp.flags=0x%02x "\r\n"fcp_resid=%d fw_residual=0x%x.\n", io_req->xid,\r\nfcp_rsp->rsp_flags.flags, io_req->fcp_resid,\r\ncqe->cqe_info.rsp_info.fw_residual);\r\nif (io_req->cdb_status == 0)\r\nsc_cmd->result = (DID_ERROR << 16) | io_req->cdb_status;\r\nelse\r\nsc_cmd->result = (DID_OK << 16) | io_req->cdb_status;\r\ninit_completion(&io_req->abts_done);\r\nrval = qedf_initiate_abts(io_req, true);\r\nif (rval) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Failed to queue ABTS.\n");\r\nsc_cmd->result = (DID_ERROR << 16) | io_req->cdb_status;\r\n}\r\nscsi_set_resid(sc_cmd, scsi_bufflen(sc_cmd));\r\ngoto out;\r\n}\r\nswitch (io_req->fcp_status) {\r\ncase FC_GOOD:\r\nif (io_req->cdb_status == 0) {\r\nsc_cmd->result = DID_OK << 16;\r\n} else {\r\nrefcount = kref_read(&io_req->refcount);\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"%d:0:%d:%lld xid=0x%0x op=0x%02x "\r\n"lba=%02x%02x%02x%02x cdb_status=%d "\r\n"fcp_resid=0x%x refcount=%d.\n",\r\nqedf->lport->host->host_no, sc_cmd->device->id,\r\nsc_cmd->device->lun, io_req->xid,\r\nsc_cmd->cmnd[0], sc_cmd->cmnd[2], sc_cmd->cmnd[3],\r\nsc_cmd->cmnd[4], sc_cmd->cmnd[5],\r\nio_req->cdb_status, io_req->fcp_resid,\r\nrefcount);\r\nsc_cmd->result = (DID_OK << 16) | io_req->cdb_status;\r\nif (io_req->cdb_status == SAM_STAT_TASK_SET_FULL ||\r\nio_req->cdb_status == SAM_STAT_BUSY) {\r\nscope = fcp_rsp->retry_delay_timer & 0xC000;\r\nqualifier = fcp_rsp->retry_delay_timer & 0x3FFF;\r\nif (qedf_retry_delay &&\r\nscope > 0 && qualifier > 0 &&\r\nqualifier <= 0x3FEF) {\r\nif (qualifier > QEDF_RETRY_DELAY_MAX)\r\nqualifier =\r\nQEDF_RETRY_DELAY_MAX;\r\nfcport->retry_delay_timestamp =\r\njiffies + (qualifier * HZ / 10);\r\n}\r\n}\r\n}\r\nif (io_req->fcp_resid)\r\nscsi_set_resid(sc_cmd, io_req->fcp_resid);\r\nbreak;\r\ndefault:\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, "fcp_status=%d.\n",\r\nio_req->fcp_status);\r\nbreak;\r\n}\r\nout:\r\nif (qedf_io_tracing)\r\nqedf_trace_io(fcport, io_req, QEDF_IO_TRACE_RSP);\r\nio_req->sc_cmd = NULL;\r\nsc_cmd->SCp.ptr = NULL;\r\nsc_cmd->scsi_done(sc_cmd);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\n}\r\nvoid qedf_scsi_done(struct qedf_ctx *qedf, struct qedf_ioreq *io_req,\r\nint result)\r\n{\r\nu16 xid;\r\nstruct scsi_cmnd *sc_cmd;\r\nint refcount;\r\nif (!io_req)\r\nreturn;\r\nxid = io_req->xid;\r\nsc_cmd = io_req->sc_cmd;\r\nif (!sc_cmd) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "sc_cmd is NULL!\n");\r\nreturn;\r\n}\r\nif (!sc_cmd->SCp.ptr) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "SCp.ptr is NULL, returned in "\r\n"another context.\n");\r\nreturn;\r\n}\r\nqedf_unmap_sg_list(qedf, io_req);\r\nsc_cmd->result = result << 16;\r\nrefcount = kref_read(&io_req->refcount);\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, "%d:0:%d:%lld: Completing "\r\n"sc_cmd=%p result=0x%08x op=0x%02x lba=0x%02x%02x%02x%02x, "\r\n"allowed=%d retries=%d refcount=%d.\n",\r\nqedf->lport->host->host_no, sc_cmd->device->id,\r\nsc_cmd->device->lun, sc_cmd, sc_cmd->result, sc_cmd->cmnd[0],\r\nsc_cmd->cmnd[2], sc_cmd->cmnd[3], sc_cmd->cmnd[4],\r\nsc_cmd->cmnd[5], sc_cmd->allowed, sc_cmd->retries,\r\nrefcount);\r\nscsi_set_resid(sc_cmd, scsi_bufflen(sc_cmd));\r\nif (qedf_io_tracing)\r\nqedf_trace_io(io_req->fcport, io_req, QEDF_IO_TRACE_RSP);\r\nio_req->sc_cmd = NULL;\r\nsc_cmd->SCp.ptr = NULL;\r\nsc_cmd->scsi_done(sc_cmd);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\n}\r\nvoid qedf_process_warning_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\r\nstruct qedf_ioreq *io_req)\r\n{\r\nint rval, i;\r\nstruct qedf_rport *fcport = io_req->fcport;\r\nu64 err_warn_bit_map;\r\nu8 err_warn = 0xff;\r\nif (!cqe)\r\nreturn;\r\nQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), "Warning CQE, "\r\n"xid=0x%x\n", io_req->xid);\r\nQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx),\r\n"err_warn_bitmap=%08x:%08x\n",\r\nle32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_hi),\r\nle32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_lo));\r\nQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), "tx_buff_off=%08x, "\r\n"rx_buff_off=%08x, rx_id=%04x\n",\r\nle32_to_cpu(cqe->cqe_info.err_info.tx_buf_off),\r\nle32_to_cpu(cqe->cqe_info.err_info.rx_buf_off),\r\nle32_to_cpu(cqe->cqe_info.err_info.rx_id));\r\nerr_warn_bit_map = (u64)\r\n((u64)cqe->cqe_info.err_info.err_warn_bitmap_hi << 32) |\r\n(u64)cqe->cqe_info.err_info.err_warn_bitmap_lo;\r\nfor (i = 0; i < 64; i++) {\r\nif (err_warn_bit_map & (u64)((u64)1 << i)) {\r\nerr_warn = i;\r\nbreak;\r\n}\r\n}\r\nif (fcport->dev_type == QEDF_RPORT_TYPE_TAPE) {\r\nif (err_warn ==\r\nFCOE_WARNING_CODE_REC_TOV_TIMER_EXPIRATION) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "REC timer expired.\n");\r\nif (!test_bit(QEDF_CMD_SRR_SENT, &io_req->flags)) {\r\nio_req->rx_buf_off =\r\ncqe->cqe_info.err_info.rx_buf_off;\r\nio_req->tx_buf_off =\r\ncqe->cqe_info.err_info.tx_buf_off;\r\nio_req->rx_id = cqe->cqe_info.err_info.rx_id;\r\nrval = qedf_send_rec(io_req);\r\nif (rval)\r\ngoto send_abort;\r\n}\r\nreturn;\r\n}\r\n}\r\nsend_abort:\r\ninit_completion(&io_req->abts_done);\r\nrval = qedf_initiate_abts(io_req, true);\r\nif (rval)\r\nQEDF_ERR(&(qedf->dbg_ctx), "Failed to queue ABTS.\n");\r\n}\r\nvoid qedf_process_error_detect(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\r\nstruct qedf_ioreq *io_req)\r\n{\r\nint rval;\r\nif (!cqe)\r\nreturn;\r\nQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), "Error detection CQE, "\r\n"xid=0x%x\n", io_req->xid);\r\nQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx),\r\n"err_warn_bitmap=%08x:%08x\n",\r\nle32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_hi),\r\nle32_to_cpu(cqe->cqe_info.err_info.err_warn_bitmap_lo));\r\nQEDF_ERR(&(io_req->fcport->qedf->dbg_ctx), "tx_buff_off=%08x, "\r\n"rx_buff_off=%08x, rx_id=%04x\n",\r\nle32_to_cpu(cqe->cqe_info.err_info.tx_buf_off),\r\nle32_to_cpu(cqe->cqe_info.err_info.rx_buf_off),\r\nle32_to_cpu(cqe->cqe_info.err_info.rx_id));\r\nif (qedf->stop_io_on_error) {\r\nqedf_stop_all_io(qedf);\r\nreturn;\r\n}\r\ninit_completion(&io_req->abts_done);\r\nrval = qedf_initiate_abts(io_req, true);\r\nif (rval)\r\nQEDF_ERR(&(qedf->dbg_ctx), "Failed to queue ABTS.\n");\r\n}\r\nstatic void qedf_flush_els_req(struct qedf_ctx *qedf,\r\nstruct qedf_ioreq *els_req)\r\n{\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"Flushing ELS request xid=0x%x refcount=%d.\n", els_req->xid,\r\nkref_read(&els_req->refcount));\r\nels_req->event = QEDF_IOREQ_EV_ELS_FLUSH;\r\ncancel_delayed_work_sync(&els_req->timeout_work);\r\nif (els_req->cb_func && els_req->cb_arg) {\r\nels_req->cb_func(els_req->cb_arg);\r\nels_req->cb_arg = NULL;\r\n}\r\nkref_put(&els_req->refcount, qedf_release_cmd);\r\n}\r\nvoid qedf_flush_active_ios(struct qedf_rport *fcport, int lun)\r\n{\r\nstruct qedf_ioreq *io_req;\r\nstruct qedf_ctx *qedf;\r\nstruct qedf_cmd_mgr *cmd_mgr;\r\nint i, rc;\r\nif (!fcport)\r\nreturn;\r\nqedf = fcport->qedf;\r\ncmd_mgr = qedf->cmd_mgr;\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, "Flush active i/o's.\n");\r\nfor (i = 0; i < FCOE_PARAMS_NUM_TASKS; i++) {\r\nio_req = &cmd_mgr->cmds[i];\r\nif (!io_req)\r\ncontinue;\r\nif (io_req->fcport != fcport)\r\ncontinue;\r\nif (io_req->cmd_type == QEDF_ELS) {\r\nrc = kref_get_unless_zero(&io_req->refcount);\r\nif (!rc) {\r\nQEDF_ERR(&(qedf->dbg_ctx),\r\n"Could not get kref for io_req=0x%p.\n",\r\nio_req);\r\ncontinue;\r\n}\r\nqedf_flush_els_req(qedf, io_req);\r\ngoto free_cmd;\r\n}\r\nif (!io_req->sc_cmd)\r\ncontinue;\r\nif (lun > 0) {\r\nif (io_req->sc_cmd->device->lun !=\r\n(u64)lun)\r\ncontinue;\r\n}\r\nrc = kref_get_unless_zero(&io_req->refcount);\r\nif (!rc) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Could not get kref for "\r\n"io_req=0x%p\n", io_req);\r\ncontinue;\r\n}\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO,\r\n"Cleanup xid=0x%x.\n", io_req->xid);\r\nqedf_initiate_cleanup(io_req, true);\r\nfree_cmd:\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\n}\r\n}\r\nint qedf_initiate_abts(struct qedf_ioreq *io_req, bool return_scsi_cmd_on_abts)\r\n{\r\nstruct fc_lport *lport;\r\nstruct qedf_rport *fcport = io_req->fcport;\r\nstruct fc_rport_priv *rdata;\r\nstruct qedf_ctx *qedf;\r\nu16 xid;\r\nu32 r_a_tov = 0;\r\nint rc = 0;\r\nunsigned long flags;\r\nstruct fcoe_wqe *sqe;\r\nu16 sqe_idx;\r\nif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\r\nQEDF_ERR(NULL, "tgt not offloaded\n");\r\nrc = 1;\r\ngoto abts_err;\r\n}\r\nrdata = fcport->rdata;\r\nr_a_tov = rdata->r_a_tov;\r\nqedf = fcport->qedf;\r\nlport = qedf->lport;\r\nif (lport->state != LPORT_ST_READY || !(lport->link_up)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "link is not ready\n");\r\nrc = 1;\r\ngoto abts_err;\r\n}\r\nif (atomic_read(&qedf->link_down_tmo_valid) > 0) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "link_down_tmo active.\n");\r\nrc = 1;\r\ngoto abts_err;\r\n}\r\nif (!atomic_read(&fcport->free_sqes)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "No SQ entries available\n");\r\nrc = 1;\r\ngoto abts_err;\r\n}\r\nkref_get(&io_req->refcount);\r\nxid = io_req->xid;\r\nqedf->control_requests++;\r\nqedf->packet_aborts++;\r\nio_req->cpu = smp_processor_id();\r\nio_req->cmd_type = QEDF_ABTS;\r\nio_req->return_scsi_cmd_on_abts = return_scsi_cmd_on_abts;\r\nset_bit(QEDF_CMD_IN_ABORT, &io_req->flags);\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM, "ABTS io_req xid = "\r\n"0x%x\n", xid);\r\nqedf_cmd_timer_set(qedf, io_req, QEDF_ABORT_TIMEOUT * HZ);\r\nspin_lock_irqsave(&fcport->rport_lock, flags);\r\nsqe_idx = qedf_get_sqe_idx(fcport);\r\nsqe = &fcport->sq[sqe_idx];\r\nmemset(sqe, 0, sizeof(struct fcoe_wqe));\r\nio_req->task_params->sqe = sqe;\r\ninit_initiator_abort_fcoe_task(io_req->task_params);\r\nqedf_ring_doorbell(fcport);\r\nspin_unlock_irqrestore(&fcport->rport_lock, flags);\r\nreturn rc;\r\nabts_err:\r\nqedf_initiate_cleanup(io_req, return_scsi_cmd_on_abts);\r\nreturn rc;\r\n}\r\nvoid qedf_process_abts_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\r\nstruct qedf_ioreq *io_req)\r\n{\r\nuint32_t r_ctl;\r\nuint16_t xid;\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM, "Entered with xid = "\r\n"0x%x cmd_type = %d\n", io_req->xid, io_req->cmd_type);\r\ncancel_delayed_work(&io_req->timeout_work);\r\nxid = io_req->xid;\r\nr_ctl = cqe->cqe_info.abts_info.r_ctl;\r\nswitch (r_ctl) {\r\ncase FC_RCTL_BA_ACC:\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM,\r\n"ABTS response - ACC Send RRQ after R_A_TOV\n");\r\nio_req->event = QEDF_IOREQ_EV_ABORT_SUCCESS;\r\nkref_get(&io_req->refcount);\r\nqueue_delayed_work(qedf->dpc_wq, &io_req->rrq_work,\r\nmsecs_to_jiffies(qedf->lport->r_a_tov));\r\nbreak;\r\ncase FC_RCTL_BA_RJT:\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM,\r\n"ABTS response - RJT\n");\r\nio_req->event = QEDF_IOREQ_EV_ABORT_FAILED;\r\nbreak;\r\ndefault:\r\nQEDF_ERR(&(qedf->dbg_ctx), "Unknown ABTS response\n");\r\nbreak;\r\n}\r\nclear_bit(QEDF_CMD_IN_ABORT, &io_req->flags);\r\nif (io_req->sc_cmd) {\r\nif (io_req->return_scsi_cmd_on_abts)\r\nqedf_scsi_done(qedf, io_req, DID_ERROR);\r\n}\r\ncomplete(&io_req->abts_done);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\n}\r\nint qedf_init_mp_req(struct qedf_ioreq *io_req)\r\n{\r\nstruct qedf_mp_req *mp_req;\r\nstruct scsi_sge *mp_req_bd;\r\nstruct scsi_sge *mp_resp_bd;\r\nstruct qedf_ctx *qedf = io_req->fcport->qedf;\r\ndma_addr_t addr;\r\nuint64_t sz;\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_MP_REQ, "Entered.\n");\r\nmp_req = (struct qedf_mp_req *)&(io_req->mp_req);\r\nmemset(mp_req, 0, sizeof(struct qedf_mp_req));\r\nif (io_req->cmd_type != QEDF_ELS) {\r\nmp_req->req_len = sizeof(struct fcp_cmnd);\r\nio_req->data_xfer_len = mp_req->req_len;\r\n} else\r\nmp_req->req_len = io_req->data_xfer_len;\r\nmp_req->req_buf = dma_alloc_coherent(&qedf->pdev->dev, QEDF_PAGE_SIZE,\r\n&mp_req->req_buf_dma, GFP_KERNEL);\r\nif (!mp_req->req_buf) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Unable to alloc MP req buffer\n");\r\nqedf_free_mp_resc(io_req);\r\nreturn -ENOMEM;\r\n}\r\nmp_req->resp_buf = dma_alloc_coherent(&qedf->pdev->dev,\r\nQEDF_PAGE_SIZE, &mp_req->resp_buf_dma, GFP_KERNEL);\r\nif (!mp_req->resp_buf) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Unable to alloc TM resp "\r\n"buffer\n");\r\nqedf_free_mp_resc(io_req);\r\nreturn -ENOMEM;\r\n}\r\nsz = sizeof(struct scsi_sge);\r\nmp_req->mp_req_bd = dma_alloc_coherent(&qedf->pdev->dev, sz,\r\n&mp_req->mp_req_bd_dma, GFP_KERNEL);\r\nif (!mp_req->mp_req_bd) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Unable to alloc MP req bd\n");\r\nqedf_free_mp_resc(io_req);\r\nreturn -ENOMEM;\r\n}\r\nmp_req->mp_resp_bd = dma_alloc_coherent(&qedf->pdev->dev, sz,\r\n&mp_req->mp_resp_bd_dma, GFP_KERNEL);\r\nif (!mp_req->mp_resp_bd) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Unable to alloc MP resp bd\n");\r\nqedf_free_mp_resc(io_req);\r\nreturn -ENOMEM;\r\n}\r\naddr = mp_req->req_buf_dma;\r\nmp_req_bd = mp_req->mp_req_bd;\r\nmp_req_bd->sge_addr.lo = U64_LO(addr);\r\nmp_req_bd->sge_addr.hi = U64_HI(addr);\r\nmp_req_bd->sge_len = QEDF_PAGE_SIZE;\r\nmp_resp_bd = mp_req->mp_resp_bd;\r\naddr = mp_req->resp_buf_dma;\r\nmp_resp_bd->sge_addr.lo = U64_LO(addr);\r\nmp_resp_bd->sge_addr.hi = U64_HI(addr);\r\nmp_resp_bd->sge_len = QEDF_PAGE_SIZE;\r\nreturn 0;\r\n}\r\nstatic void qedf_drain_request(struct qedf_ctx *qedf)\r\n{\r\nif (test_bit(QEDF_DRAIN_ACTIVE, &qedf->flags)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "MCP drain already active.\n");\r\nreturn;\r\n}\r\nset_bit(QEDF_DRAIN_ACTIVE, &qedf->flags);\r\nqed_ops->common->drain(qedf->cdev);\r\nmsleep(100);\r\nclear_bit(QEDF_DRAIN_ACTIVE, &qedf->flags);\r\n}\r\nint qedf_initiate_cleanup(struct qedf_ioreq *io_req,\r\nbool return_scsi_cmd_on_abts)\r\n{\r\nstruct qedf_rport *fcport;\r\nstruct qedf_ctx *qedf;\r\nuint16_t xid;\r\nstruct fcoe_task_context *task;\r\nint tmo = 0;\r\nint rc = SUCCESS;\r\nunsigned long flags;\r\nstruct fcoe_wqe *sqe;\r\nu16 sqe_idx;\r\nfcport = io_req->fcport;\r\nif (!fcport) {\r\nQEDF_ERR(NULL, "fcport is NULL.\n");\r\nreturn SUCCESS;\r\n}\r\nif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\r\nQEDF_ERR(NULL, "tgt not offloaded\n");\r\nrc = 1;\r\nreturn SUCCESS;\r\n}\r\nqedf = fcport->qedf;\r\nif (!qedf) {\r\nQEDF_ERR(NULL, "qedf is NULL.\n");\r\nreturn SUCCESS;\r\n}\r\nif (!test_bit(QEDF_CMD_OUTSTANDING, &io_req->flags) ||\r\ntest_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "io_req xid=0x%x already in "\r\n"cleanup processing or already completed.\n",\r\nio_req->xid);\r\nreturn SUCCESS;\r\n}\r\nif (!atomic_read(&fcport->free_sqes)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "No SQ entries available\n");\r\nreturn FAILED;\r\n}\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, "Entered xid=0x%x\n",\r\nio_req->xid);\r\nxid = io_req->xid;\r\nio_req->cmd_type = QEDF_CLEANUP;\r\nio_req->return_scsi_cmd_on_abts = return_scsi_cmd_on_abts;\r\nio_req->cpu = smp_processor_id();\r\nset_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\r\ntask = qedf_get_task_mem(&qedf->tasks, xid);\r\ninit_completion(&io_req->tm_done);\r\nspin_lock_irqsave(&fcport->rport_lock, flags);\r\nsqe_idx = qedf_get_sqe_idx(fcport);\r\nsqe = &fcport->sq[sqe_idx];\r\nmemset(sqe, 0, sizeof(struct fcoe_wqe));\r\nio_req->task_params->sqe = sqe;\r\ninit_initiator_cleanup_fcoe_task(io_req->task_params);\r\nqedf_ring_doorbell(fcport);\r\nspin_unlock_irqrestore(&fcport->rport_lock, flags);\r\ntmo = wait_for_completion_timeout(&io_req->tm_done,\r\nQEDF_CLEANUP_TIMEOUT * HZ);\r\nif (!tmo) {\r\nrc = FAILED;\r\nQEDF_ERR(&(qedf->dbg_ctx), "Cleanup command timeout, "\r\n"xid=%x.\n", io_req->xid);\r\nclear_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\r\nQEDF_ERR(&(qedf->dbg_ctx), "Issuing MCP drain request.\n");\r\nqedf_drain_request(qedf);\r\n}\r\nif (io_req->sc_cmd) {\r\nif (io_req->return_scsi_cmd_on_abts)\r\nqedf_scsi_done(qedf, io_req, DID_ERROR);\r\n}\r\nif (rc == SUCCESS)\r\nio_req->event = QEDF_IOREQ_EV_CLEANUP_SUCCESS;\r\nelse\r\nio_req->event = QEDF_IOREQ_EV_CLEANUP_FAILED;\r\nreturn rc;\r\n}\r\nvoid qedf_process_cleanup_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\r\nstruct qedf_ioreq *io_req)\r\n{\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_IO, "Entered xid = 0x%x\n",\r\nio_req->xid);\r\nclear_bit(QEDF_CMD_IN_CLEANUP, &io_req->flags);\r\ncomplete(&io_req->tm_done);\r\n}\r\nstatic int qedf_execute_tmf(struct qedf_rport *fcport, struct scsi_cmnd *sc_cmd,\r\nuint8_t tm_flags)\r\n{\r\nstruct qedf_ioreq *io_req;\r\nstruct fcoe_task_context *task;\r\nstruct qedf_ctx *qedf = fcport->qedf;\r\nstruct fc_lport *lport = qedf->lport;\r\nint rc = 0;\r\nuint16_t xid;\r\nint tmo = 0;\r\nunsigned long flags;\r\nstruct fcoe_wqe *sqe;\r\nu16 sqe_idx;\r\nif (!sc_cmd) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "invalid arg\n");\r\nreturn FAILED;\r\n}\r\nif (!test_bit(QEDF_RPORT_SESSION_READY, &fcport->flags)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "fcport not offloaded\n");\r\nrc = FAILED;\r\nreturn FAILED;\r\n}\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM, "portid = 0x%x "\r\n"tm_flags = %d\n", fcport->rdata->ids.port_id, tm_flags);\r\nio_req = qedf_alloc_cmd(fcport, QEDF_TASK_MGMT_CMD);\r\nif (!io_req) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Failed TMF");\r\nrc = -EAGAIN;\r\ngoto reset_tmf_err;\r\n}\r\nio_req->sc_cmd = sc_cmd;\r\nio_req->fcport = fcport;\r\nio_req->cmd_type = QEDF_TASK_MGMT_CMD;\r\nio_req->cpu = smp_processor_id();\r\nio_req->io_req_flags = QEDF_READ;\r\nio_req->data_xfer_len = 0;\r\nio_req->tm_flags = tm_flags;\r\nio_req->return_scsi_cmd_on_abts = true;\r\nxid = io_req->xid;\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_SCSI_TM, "TMF io_req xid = "\r\n"0x%x\n", xid);\r\ntask = qedf_get_task_mem(&qedf->tasks, xid);\r\ninit_completion(&io_req->tm_done);\r\nspin_lock_irqsave(&fcport->rport_lock, flags);\r\nsqe_idx = qedf_get_sqe_idx(fcport);\r\nsqe = &fcport->sq[sqe_idx];\r\nmemset(sqe, 0, sizeof(struct fcoe_wqe));\r\nqedf_init_task(fcport, lport, io_req, task, sqe);\r\nqedf_ring_doorbell(fcport);\r\nspin_unlock_irqrestore(&fcport->rport_lock, flags);\r\ntmo = wait_for_completion_timeout(&io_req->tm_done,\r\nQEDF_TM_TIMEOUT * HZ);\r\nif (!tmo) {\r\nrc = FAILED;\r\nQEDF_ERR(&(qedf->dbg_ctx), "wait for tm_cmpl timeout!\n");\r\n} else {\r\nif (io_req->fcp_rsp_code == 0)\r\nrc = SUCCESS;\r\nelse\r\nrc = FAILED;\r\n}\r\nif (tm_flags == FCP_TMF_LUN_RESET)\r\nqedf_flush_active_ios(fcport, (int)sc_cmd->device->lun);\r\nelse\r\nqedf_flush_active_ios(fcport, -1);\r\nkref_put(&io_req->refcount, qedf_release_cmd);\r\nif (rc != SUCCESS) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "task mgmt command failed...\n");\r\nrc = FAILED;\r\n} else {\r\nQEDF_ERR(&(qedf->dbg_ctx), "task mgmt command success...\n");\r\nrc = SUCCESS;\r\n}\r\nreset_tmf_err:\r\nreturn rc;\r\n}\r\nint qedf_initiate_tmf(struct scsi_cmnd *sc_cmd, u8 tm_flags)\r\n{\r\nstruct fc_rport *rport = starget_to_rport(scsi_target(sc_cmd->device));\r\nstruct fc_rport_libfc_priv *rp = rport->dd_data;\r\nstruct qedf_rport *fcport = (struct qedf_rport *)&rp[1];\r\nstruct qedf_ctx *qedf;\r\nstruct fc_lport *lport;\r\nint rc = SUCCESS;\r\nint rval;\r\nrval = fc_remote_port_chkready(rport);\r\nif (rval) {\r\nQEDF_ERR(NULL, "device_reset rport not ready\n");\r\nrc = FAILED;\r\ngoto tmf_err;\r\n}\r\nif (fcport == NULL) {\r\nQEDF_ERR(NULL, "device_reset: rport is NULL\n");\r\nrc = FAILED;\r\ngoto tmf_err;\r\n}\r\nqedf = fcport->qedf;\r\nlport = qedf->lport;\r\nif (test_bit(QEDF_UNLOADING, &qedf->flags) ||\r\ntest_bit(QEDF_DBG_STOP_IO, &qedf->flags)) {\r\nrc = SUCCESS;\r\ngoto tmf_err;\r\n}\r\nif (lport->state != LPORT_ST_READY || !(lport->link_up)) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "link is not ready\n");\r\nrc = FAILED;\r\ngoto tmf_err;\r\n}\r\nrc = qedf_execute_tmf(fcport, sc_cmd, tm_flags);\r\ntmf_err:\r\nreturn rc;\r\n}\r\nvoid qedf_process_tmf_compl(struct qedf_ctx *qedf, struct fcoe_cqe *cqe,\r\nstruct qedf_ioreq *io_req)\r\n{\r\nstruct fcoe_cqe_rsp_info *fcp_rsp;\r\nfcp_rsp = &cqe->cqe_info.rsp_info;\r\nqedf_parse_fcp_rsp(io_req, fcp_rsp);\r\nio_req->sc_cmd = NULL;\r\ncomplete(&io_req->tm_done);\r\n}\r\nvoid qedf_process_unsol_compl(struct qedf_ctx *qedf, uint16_t que_idx,\r\nstruct fcoe_cqe *cqe)\r\n{\r\nunsigned long flags;\r\nuint16_t tmp;\r\nuint16_t pktlen = cqe->cqe_info.unsolic_info.pkt_len;\r\nu32 payload_len, crc;\r\nstruct fc_frame_header *fh;\r\nstruct fc_frame *fp;\r\nstruct qedf_io_work *io_work;\r\nu32 bdq_idx;\r\nvoid *bdq_addr;\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_UNSOL,\r\n"address.hi=%x address.lo=%x opaque_data.hi=%x "\r\n"opaque_data.lo=%x bdq_prod_idx=%u len=%u.\n",\r\nle32_to_cpu(cqe->cqe_info.unsolic_info.bd_info.address.hi),\r\nle32_to_cpu(cqe->cqe_info.unsolic_info.bd_info.address.lo),\r\nle32_to_cpu(cqe->cqe_info.unsolic_info.bd_info.opaque.hi),\r\nle32_to_cpu(cqe->cqe_info.unsolic_info.bd_info.opaque.lo),\r\nqedf->bdq_prod_idx, pktlen);\r\nbdq_idx = le32_to_cpu(cqe->cqe_info.unsolic_info.bd_info.opaque.lo);\r\nif (bdq_idx >= QEDF_BDQ_SIZE) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "bdq_idx is out of range %d.\n",\r\nbdq_idx);\r\ngoto increment_prod;\r\n}\r\nbdq_addr = qedf->bdq[bdq_idx].buf_addr;\r\nif (!bdq_addr) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "bdq_addr is NULL, dropping "\r\n"unsolicited packet.\n");\r\ngoto increment_prod;\r\n}\r\nif (qedf_dump_frames) {\r\nQEDF_INFO(&(qedf->dbg_ctx), QEDF_LOG_UNSOL,\r\n"BDQ frame is at addr=%p.\n", bdq_addr);\r\nprint_hex_dump(KERN_WARNING, "bdq ", DUMP_PREFIX_OFFSET, 16, 1,\r\n(void *)bdq_addr, pktlen, false);\r\n}\r\npayload_len = pktlen - sizeof(struct fc_frame_header);\r\nfp = fc_frame_alloc(qedf->lport, payload_len);\r\nif (!fp) {\r\nQEDF_ERR(&(qedf->dbg_ctx), "Could not allocate fp.\n");\r\ngoto increment_prod;\r\n}\r\nfh = (struct fc_frame_header *)fc_frame_header_get(fp);\r\nmemcpy(fh, (void *)bdq_addr, pktlen);\r\ncrc = fcoe_fc_crc(fp);\r\nfc_frame_init(fp);\r\nfr_dev(fp) = qedf->lport;\r\nfr_sof(fp) = FC_SOF_I3;\r\nfr_eof(fp) = FC_EOF_T;\r\nfr_crc(fp) = cpu_to_le32(~crc);\r\nio_work = mempool_alloc(qedf->io_mempool, GFP_ATOMIC);\r\nif (!io_work) {\r\nQEDF_WARN(&(qedf->dbg_ctx), "Could not allocate "\r\n"work for I/O completion.\n");\r\nfc_frame_free(fp);\r\ngoto increment_prod;\r\n}\r\nmemset(io_work, 0, sizeof(struct qedf_io_work));\r\nINIT_WORK(&io_work->work, qedf_fp_io_handler);\r\nmemcpy(&io_work->cqe, cqe, sizeof(struct fcoe_cqe));\r\nio_work->qedf = qedf;\r\nio_work->fp = fp;\r\nqueue_work_on(smp_processor_id(), qedf_io_wq, &io_work->work);\r\nincrement_prod:\r\nspin_lock_irqsave(&qedf->hba_lock, flags);\r\nqedf->bdq_prod_idx++;\r\nif (qedf->bdq_prod_idx == 0xffff)\r\nqedf->bdq_prod_idx = 0;\r\nwritew(qedf->bdq_prod_idx, qedf->bdq_primary_prod);\r\ntmp = readw(qedf->bdq_primary_prod);\r\nwritew(qedf->bdq_prod_idx, qedf->bdq_secondary_prod);\r\ntmp = readw(qedf->bdq_secondary_prod);\r\nspin_unlock_irqrestore(&qedf->hba_lock, flags);\r\n}
