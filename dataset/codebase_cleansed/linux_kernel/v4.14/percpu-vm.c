static struct page *pcpu_chunk_page(struct pcpu_chunk *chunk,\r\nunsigned int cpu, int page_idx)\r\n{\r\nWARN_ON(chunk->immutable);\r\nreturn vmalloc_to_page((void *)pcpu_chunk_addr(chunk, cpu, page_idx));\r\n}\r\nstatic struct page **pcpu_get_pages(void)\r\n{\r\nstatic struct page **pages;\r\nsize_t pages_size = pcpu_nr_units * pcpu_unit_pages * sizeof(pages[0]);\r\nlockdep_assert_held(&pcpu_alloc_mutex);\r\nif (!pages)\r\npages = pcpu_mem_zalloc(pages_size);\r\nreturn pages;\r\n}\r\nstatic void pcpu_free_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, int page_start, int page_end)\r\n{\r\nunsigned int cpu;\r\nint i;\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = page_start; i < page_end; i++) {\r\nstruct page *page = pages[pcpu_page_idx(cpu, i)];\r\nif (page)\r\n__free_page(page);\r\n}\r\n}\r\n}\r\nstatic int pcpu_alloc_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, int page_start, int page_end)\r\n{\r\nconst gfp_t gfp = GFP_KERNEL | __GFP_HIGHMEM | __GFP_COLD;\r\nunsigned int cpu, tcpu;\r\nint i;\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = page_start; i < page_end; i++) {\r\nstruct page **pagep = &pages[pcpu_page_idx(cpu, i)];\r\n*pagep = alloc_pages_node(cpu_to_node(cpu), gfp, 0);\r\nif (!*pagep)\r\ngoto err;\r\n}\r\n}\r\nreturn 0;\r\nerr:\r\nwhile (--i >= page_start)\r\n__free_page(pages[pcpu_page_idx(cpu, i)]);\r\nfor_each_possible_cpu(tcpu) {\r\nif (tcpu == cpu)\r\nbreak;\r\nfor (i = page_start; i < page_end; i++)\r\n__free_page(pages[pcpu_page_idx(tcpu, i)]);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void pcpu_pre_unmap_flush(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nflush_cache_vunmap(\r\npcpu_chunk_addr(chunk, pcpu_low_unit_cpu, page_start),\r\npcpu_chunk_addr(chunk, pcpu_high_unit_cpu, page_end));\r\n}\r\nstatic void __pcpu_unmap_pages(unsigned long addr, int nr_pages)\r\n{\r\nunmap_kernel_range_noflush(addr, nr_pages << PAGE_SHIFT);\r\n}\r\nstatic void pcpu_unmap_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, int page_start, int page_end)\r\n{\r\nunsigned int cpu;\r\nint i;\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = page_start; i < page_end; i++) {\r\nstruct page *page;\r\npage = pcpu_chunk_page(chunk, cpu, i);\r\nWARN_ON(!page);\r\npages[pcpu_page_idx(cpu, i)] = page;\r\n}\r\n__pcpu_unmap_pages(pcpu_chunk_addr(chunk, cpu, page_start),\r\npage_end - page_start);\r\n}\r\n}\r\nstatic void pcpu_post_unmap_tlb_flush(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nflush_tlb_kernel_range(\r\npcpu_chunk_addr(chunk, pcpu_low_unit_cpu, page_start),\r\npcpu_chunk_addr(chunk, pcpu_high_unit_cpu, page_end));\r\n}\r\nstatic int __pcpu_map_pages(unsigned long addr, struct page **pages,\r\nint nr_pages)\r\n{\r\nreturn map_kernel_range_noflush(addr, nr_pages << PAGE_SHIFT,\r\nPAGE_KERNEL, pages);\r\n}\r\nstatic int pcpu_map_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, int page_start, int page_end)\r\n{\r\nunsigned int cpu, tcpu;\r\nint i, err;\r\nfor_each_possible_cpu(cpu) {\r\nerr = __pcpu_map_pages(pcpu_chunk_addr(chunk, cpu, page_start),\r\n&pages[pcpu_page_idx(cpu, page_start)],\r\npage_end - page_start);\r\nif (err < 0)\r\ngoto err;\r\nfor (i = page_start; i < page_end; i++)\r\npcpu_set_page_chunk(pages[pcpu_page_idx(cpu, i)],\r\nchunk);\r\n}\r\nreturn 0;\r\nerr:\r\nfor_each_possible_cpu(tcpu) {\r\nif (tcpu == cpu)\r\nbreak;\r\n__pcpu_unmap_pages(pcpu_chunk_addr(chunk, tcpu, page_start),\r\npage_end - page_start);\r\n}\r\npcpu_post_unmap_tlb_flush(chunk, page_start, page_end);\r\nreturn err;\r\n}\r\nstatic void pcpu_post_map_flush(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nflush_cache_vmap(\r\npcpu_chunk_addr(chunk, pcpu_low_unit_cpu, page_start),\r\npcpu_chunk_addr(chunk, pcpu_high_unit_cpu, page_end));\r\n}\r\nstatic int pcpu_populate_chunk(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nstruct page **pages;\r\npages = pcpu_get_pages();\r\nif (!pages)\r\nreturn -ENOMEM;\r\nif (pcpu_alloc_pages(chunk, pages, page_start, page_end))\r\nreturn -ENOMEM;\r\nif (pcpu_map_pages(chunk, pages, page_start, page_end)) {\r\npcpu_free_pages(chunk, pages, page_start, page_end);\r\nreturn -ENOMEM;\r\n}\r\npcpu_post_map_flush(chunk, page_start, page_end);\r\nreturn 0;\r\n}\r\nstatic void pcpu_depopulate_chunk(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nstruct page **pages;\r\npages = pcpu_get_pages();\r\nBUG_ON(!pages);\r\npcpu_pre_unmap_flush(chunk, page_start, page_end);\r\npcpu_unmap_pages(chunk, pages, page_start, page_end);\r\npcpu_free_pages(chunk, pages, page_start, page_end);\r\n}\r\nstatic struct pcpu_chunk *pcpu_create_chunk(void)\r\n{\r\nstruct pcpu_chunk *chunk;\r\nstruct vm_struct **vms;\r\nchunk = pcpu_alloc_chunk();\r\nif (!chunk)\r\nreturn NULL;\r\nvms = pcpu_get_vm_areas(pcpu_group_offsets, pcpu_group_sizes,\r\npcpu_nr_groups, pcpu_atom_size);\r\nif (!vms) {\r\npcpu_free_chunk(chunk);\r\nreturn NULL;\r\n}\r\nchunk->data = vms;\r\nchunk->base_addr = vms[0]->addr - pcpu_group_offsets[0];\r\npcpu_stats_chunk_alloc();\r\ntrace_percpu_create_chunk(chunk->base_addr);\r\nreturn chunk;\r\n}\r\nstatic void pcpu_destroy_chunk(struct pcpu_chunk *chunk)\r\n{\r\nif (!chunk)\r\nreturn;\r\npcpu_stats_chunk_dealloc();\r\ntrace_percpu_destroy_chunk(chunk->base_addr);\r\nif (chunk->data)\r\npcpu_free_vm_areas(chunk->data, pcpu_nr_groups);\r\npcpu_free_chunk(chunk);\r\n}\r\nstatic struct page *pcpu_addr_to_page(void *addr)\r\n{\r\nreturn vmalloc_to_page(addr);\r\n}\r\nstatic int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)\r\n{\r\nreturn 0;\r\n}
