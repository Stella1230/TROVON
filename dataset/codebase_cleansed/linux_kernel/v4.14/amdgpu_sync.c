void amdgpu_sync_create(struct amdgpu_sync *sync)\r\n{\r\nhash_init(sync->fences);\r\nsync->last_vm_update = NULL;\r\n}\r\nstatic bool amdgpu_sync_same_dev(struct amdgpu_device *adev,\r\nstruct dma_fence *f)\r\n{\r\nstruct amd_sched_fence *s_fence = to_amd_sched_fence(f);\r\nif (s_fence) {\r\nstruct amdgpu_ring *ring;\r\nring = container_of(s_fence->sched, struct amdgpu_ring, sched);\r\nreturn ring->adev == adev;\r\n}\r\nreturn false;\r\n}\r\nstatic void *amdgpu_sync_get_owner(struct dma_fence *f)\r\n{\r\nstruct amd_sched_fence *s_fence = to_amd_sched_fence(f);\r\nif (s_fence)\r\nreturn s_fence->owner;\r\nreturn AMDGPU_FENCE_OWNER_UNDEFINED;\r\n}\r\nstatic void amdgpu_sync_keep_later(struct dma_fence **keep,\r\nstruct dma_fence *fence)\r\n{\r\nif (*keep && dma_fence_is_later(*keep, fence))\r\nreturn;\r\ndma_fence_put(*keep);\r\n*keep = dma_fence_get(fence);\r\n}\r\nstatic bool amdgpu_sync_add_later(struct amdgpu_sync *sync, struct dma_fence *f)\r\n{\r\nstruct amdgpu_sync_entry *e;\r\nhash_for_each_possible(sync->fences, e, node, f->context) {\r\nif (unlikely(e->fence->context != f->context))\r\ncontinue;\r\namdgpu_sync_keep_later(&e->fence, f);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nint amdgpu_sync_fence(struct amdgpu_device *adev, struct amdgpu_sync *sync,\r\nstruct dma_fence *f)\r\n{\r\nstruct amdgpu_sync_entry *e;\r\nif (!f)\r\nreturn 0;\r\nif (amdgpu_sync_same_dev(adev, f) &&\r\namdgpu_sync_get_owner(f) == AMDGPU_FENCE_OWNER_VM)\r\namdgpu_sync_keep_later(&sync->last_vm_update, f);\r\nif (amdgpu_sync_add_later(sync, f))\r\nreturn 0;\r\ne = kmem_cache_alloc(amdgpu_sync_slab, GFP_KERNEL);\r\nif (!e)\r\nreturn -ENOMEM;\r\nhash_add(sync->fences, &e->node, f->context);\r\ne->fence = dma_fence_get(f);\r\nreturn 0;\r\n}\r\nint amdgpu_sync_resv(struct amdgpu_device *adev,\r\nstruct amdgpu_sync *sync,\r\nstruct reservation_object *resv,\r\nvoid *owner)\r\n{\r\nstruct reservation_object_list *flist;\r\nstruct dma_fence *f;\r\nvoid *fence_owner;\r\nunsigned i;\r\nint r = 0;\r\nif (resv == NULL)\r\nreturn -EINVAL;\r\nf = reservation_object_get_excl(resv);\r\nr = amdgpu_sync_fence(adev, sync, f);\r\nflist = reservation_object_get_list(resv);\r\nif (!flist || r)\r\nreturn r;\r\nfor (i = 0; i < flist->shared_count; ++i) {\r\nf = rcu_dereference_protected(flist->shared[i],\r\nreservation_object_held(resv));\r\nif (amdgpu_sync_same_dev(adev, f)) {\r\nfence_owner = amdgpu_sync_get_owner(f);\r\nif ((owner != AMDGPU_FENCE_OWNER_UNDEFINED) &&\r\n(fence_owner != AMDGPU_FENCE_OWNER_UNDEFINED) &&\r\n((owner == AMDGPU_FENCE_OWNER_VM) !=\r\n(fence_owner == AMDGPU_FENCE_OWNER_VM)))\r\ncontinue;\r\nif (owner != AMDGPU_FENCE_OWNER_UNDEFINED &&\r\nfence_owner == owner)\r\ncontinue;\r\n}\r\nr = amdgpu_sync_fence(adev, sync, f);\r\nif (r)\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstruct dma_fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,\r\nstruct amdgpu_ring *ring)\r\n{\r\nstruct amdgpu_sync_entry *e;\r\nstruct hlist_node *tmp;\r\nint i;\r\nhash_for_each_safe(sync->fences, i, tmp, e, node) {\r\nstruct dma_fence *f = e->fence;\r\nstruct amd_sched_fence *s_fence = to_amd_sched_fence(f);\r\nif (dma_fence_is_signaled(f)) {\r\nhash_del(&e->node);\r\ndma_fence_put(f);\r\nkmem_cache_free(amdgpu_sync_slab, e);\r\ncontinue;\r\n}\r\nif (ring && s_fence) {\r\nif (s_fence->sched == &ring->sched) {\r\nif (dma_fence_is_signaled(&s_fence->scheduled))\r\ncontinue;\r\nreturn &s_fence->scheduled;\r\n}\r\n}\r\nreturn f;\r\n}\r\nreturn NULL;\r\n}\r\nstruct dma_fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync)\r\n{\r\nstruct amdgpu_sync_entry *e;\r\nstruct hlist_node *tmp;\r\nstruct dma_fence *f;\r\nint i;\r\nhash_for_each_safe(sync->fences, i, tmp, e, node) {\r\nf = e->fence;\r\nhash_del(&e->node);\r\nkmem_cache_free(amdgpu_sync_slab, e);\r\nif (!dma_fence_is_signaled(f))\r\nreturn f;\r\ndma_fence_put(f);\r\n}\r\nreturn NULL;\r\n}\r\nint amdgpu_sync_wait(struct amdgpu_sync *sync, bool intr)\r\n{\r\nstruct amdgpu_sync_entry *e;\r\nstruct hlist_node *tmp;\r\nint i, r;\r\nhash_for_each_safe(sync->fences, i, tmp, e, node) {\r\nr = dma_fence_wait(e->fence, intr);\r\nif (r)\r\nreturn r;\r\nhash_del(&e->node);\r\ndma_fence_put(e->fence);\r\nkmem_cache_free(amdgpu_sync_slab, e);\r\n}\r\nreturn 0;\r\n}\r\nvoid amdgpu_sync_free(struct amdgpu_sync *sync)\r\n{\r\nstruct amdgpu_sync_entry *e;\r\nstruct hlist_node *tmp;\r\nunsigned i;\r\nhash_for_each_safe(sync->fences, i, tmp, e, node) {\r\nhash_del(&e->node);\r\ndma_fence_put(e->fence);\r\nkmem_cache_free(amdgpu_sync_slab, e);\r\n}\r\ndma_fence_put(sync->last_vm_update);\r\n}\r\nint amdgpu_sync_init(void)\r\n{\r\namdgpu_sync_slab = kmem_cache_create(\r\n"amdgpu_sync", sizeof(struct amdgpu_sync_entry), 0,\r\nSLAB_HWCACHE_ALIGN, NULL);\r\nif (!amdgpu_sync_slab)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid amdgpu_sync_fini(void)\r\n{\r\nkmem_cache_destroy(amdgpu_sync_slab);\r\n}
