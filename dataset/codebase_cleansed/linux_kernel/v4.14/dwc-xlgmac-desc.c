static void xlgmac_unmap_desc_data(struct xlgmac_pdata *pdata,\r\nstruct xlgmac_desc_data *desc_data)\r\n{\r\nif (desc_data->skb_dma) {\r\nif (desc_data->mapped_as_page) {\r\ndma_unmap_page(pdata->dev, desc_data->skb_dma,\r\ndesc_data->skb_dma_len, DMA_TO_DEVICE);\r\n} else {\r\ndma_unmap_single(pdata->dev, desc_data->skb_dma,\r\ndesc_data->skb_dma_len, DMA_TO_DEVICE);\r\n}\r\ndesc_data->skb_dma = 0;\r\ndesc_data->skb_dma_len = 0;\r\n}\r\nif (desc_data->skb) {\r\ndev_kfree_skb_any(desc_data->skb);\r\ndesc_data->skb = NULL;\r\n}\r\nif (desc_data->rx.hdr.pa.pages)\r\nput_page(desc_data->rx.hdr.pa.pages);\r\nif (desc_data->rx.hdr.pa_unmap.pages) {\r\ndma_unmap_page(pdata->dev, desc_data->rx.hdr.pa_unmap.pages_dma,\r\ndesc_data->rx.hdr.pa_unmap.pages_len,\r\nDMA_FROM_DEVICE);\r\nput_page(desc_data->rx.hdr.pa_unmap.pages);\r\n}\r\nif (desc_data->rx.buf.pa.pages)\r\nput_page(desc_data->rx.buf.pa.pages);\r\nif (desc_data->rx.buf.pa_unmap.pages) {\r\ndma_unmap_page(pdata->dev, desc_data->rx.buf.pa_unmap.pages_dma,\r\ndesc_data->rx.buf.pa_unmap.pages_len,\r\nDMA_FROM_DEVICE);\r\nput_page(desc_data->rx.buf.pa_unmap.pages);\r\n}\r\nmemset(&desc_data->tx, 0, sizeof(desc_data->tx));\r\nmemset(&desc_data->rx, 0, sizeof(desc_data->rx));\r\ndesc_data->mapped_as_page = 0;\r\nif (desc_data->state_saved) {\r\ndesc_data->state_saved = 0;\r\ndesc_data->state.skb = NULL;\r\ndesc_data->state.len = 0;\r\ndesc_data->state.error = 0;\r\n}\r\n}\r\nstatic void xlgmac_free_ring(struct xlgmac_pdata *pdata,\r\nstruct xlgmac_ring *ring)\r\n{\r\nstruct xlgmac_desc_data *desc_data;\r\nunsigned int i;\r\nif (!ring)\r\nreturn;\r\nif (ring->desc_data_head) {\r\nfor (i = 0; i < ring->dma_desc_count; i++) {\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, i);\r\nxlgmac_unmap_desc_data(pdata, desc_data);\r\n}\r\nkfree(ring->desc_data_head);\r\nring->desc_data_head = NULL;\r\n}\r\nif (ring->rx_hdr_pa.pages) {\r\ndma_unmap_page(pdata->dev, ring->rx_hdr_pa.pages_dma,\r\nring->rx_hdr_pa.pages_len, DMA_FROM_DEVICE);\r\nput_page(ring->rx_hdr_pa.pages);\r\nring->rx_hdr_pa.pages = NULL;\r\nring->rx_hdr_pa.pages_len = 0;\r\nring->rx_hdr_pa.pages_offset = 0;\r\nring->rx_hdr_pa.pages_dma = 0;\r\n}\r\nif (ring->rx_buf_pa.pages) {\r\ndma_unmap_page(pdata->dev, ring->rx_buf_pa.pages_dma,\r\nring->rx_buf_pa.pages_len, DMA_FROM_DEVICE);\r\nput_page(ring->rx_buf_pa.pages);\r\nring->rx_buf_pa.pages = NULL;\r\nring->rx_buf_pa.pages_len = 0;\r\nring->rx_buf_pa.pages_offset = 0;\r\nring->rx_buf_pa.pages_dma = 0;\r\n}\r\nif (ring->dma_desc_head) {\r\ndma_free_coherent(pdata->dev,\r\n(sizeof(struct xlgmac_dma_desc) *\r\nring->dma_desc_count),\r\nring->dma_desc_head,\r\nring->dma_desc_head_addr);\r\nring->dma_desc_head = NULL;\r\n}\r\n}\r\nstatic int xlgmac_init_ring(struct xlgmac_pdata *pdata,\r\nstruct xlgmac_ring *ring,\r\nunsigned int dma_desc_count)\r\n{\r\nif (!ring)\r\nreturn 0;\r\nring->dma_desc_count = dma_desc_count;\r\nring->dma_desc_head = dma_alloc_coherent(pdata->dev,\r\n(sizeof(struct xlgmac_dma_desc) *\r\ndma_desc_count),\r\n&ring->dma_desc_head_addr,\r\nGFP_KERNEL);\r\nif (!ring->dma_desc_head)\r\nreturn -ENOMEM;\r\nring->desc_data_head = kcalloc(dma_desc_count,\r\nsizeof(struct xlgmac_desc_data),\r\nGFP_KERNEL);\r\nif (!ring->desc_data_head)\r\nreturn -ENOMEM;\r\nnetif_dbg(pdata, drv, pdata->netdev,\r\n"dma_desc_head=%p, dma_desc_head_addr=%pad, desc_data_head=%p\n",\r\nring->dma_desc_head,\r\n&ring->dma_desc_head_addr,\r\nring->desc_data_head);\r\nreturn 0;\r\n}\r\nstatic void xlgmac_free_rings(struct xlgmac_pdata *pdata)\r\n{\r\nstruct xlgmac_channel *channel;\r\nunsigned int i;\r\nif (!pdata->channel_head)\r\nreturn;\r\nchannel = pdata->channel_head;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nxlgmac_free_ring(pdata, channel->tx_ring);\r\nxlgmac_free_ring(pdata, channel->rx_ring);\r\n}\r\n}\r\nstatic int xlgmac_alloc_rings(struct xlgmac_pdata *pdata)\r\n{\r\nstruct xlgmac_channel *channel;\r\nunsigned int i;\r\nint ret;\r\nchannel = pdata->channel_head;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nnetif_dbg(pdata, drv, pdata->netdev, "%s - Tx ring:\n",\r\nchannel->name);\r\nret = xlgmac_init_ring(pdata, channel->tx_ring,\r\npdata->tx_desc_count);\r\nif (ret) {\r\nnetdev_alert(pdata->netdev,\r\n"error initializing Tx ring");\r\ngoto err_init_ring;\r\n}\r\nnetif_dbg(pdata, drv, pdata->netdev, "%s - Rx ring:\n",\r\nchannel->name);\r\nret = xlgmac_init_ring(pdata, channel->rx_ring,\r\npdata->rx_desc_count);\r\nif (ret) {\r\nnetdev_alert(pdata->netdev,\r\n"error initializing Rx ring\n");\r\ngoto err_init_ring;\r\n}\r\n}\r\nreturn 0;\r\nerr_init_ring:\r\nxlgmac_free_rings(pdata);\r\nreturn ret;\r\n}\r\nstatic void xlgmac_free_channels(struct xlgmac_pdata *pdata)\r\n{\r\nif (!pdata->channel_head)\r\nreturn;\r\nkfree(pdata->channel_head->tx_ring);\r\npdata->channel_head->tx_ring = NULL;\r\nkfree(pdata->channel_head->rx_ring);\r\npdata->channel_head->rx_ring = NULL;\r\nkfree(pdata->channel_head);\r\npdata->channel_head = NULL;\r\npdata->channel_count = 0;\r\n}\r\nstatic int xlgmac_alloc_channels(struct xlgmac_pdata *pdata)\r\n{\r\nstruct xlgmac_channel *channel_head, *channel;\r\nstruct xlgmac_ring *tx_ring, *rx_ring;\r\nint ret = -ENOMEM;\r\nunsigned int i;\r\nchannel_head = kcalloc(pdata->channel_count,\r\nsizeof(struct xlgmac_channel), GFP_KERNEL);\r\nif (!channel_head)\r\nreturn ret;\r\nnetif_dbg(pdata, drv, pdata->netdev,\r\n"channel_head=%p\n", channel_head);\r\ntx_ring = kcalloc(pdata->tx_ring_count, sizeof(struct xlgmac_ring),\r\nGFP_KERNEL);\r\nif (!tx_ring)\r\ngoto err_tx_ring;\r\nrx_ring = kcalloc(pdata->rx_ring_count, sizeof(struct xlgmac_ring),\r\nGFP_KERNEL);\r\nif (!rx_ring)\r\ngoto err_rx_ring;\r\nfor (i = 0, channel = channel_head; i < pdata->channel_count;\r\ni++, channel++) {\r\nsnprintf(channel->name, sizeof(channel->name), "channel-%u", i);\r\nchannel->pdata = pdata;\r\nchannel->queue_index = i;\r\nchannel->dma_regs = pdata->mac_regs + DMA_CH_BASE +\r\n(DMA_CH_INC * i);\r\nif (pdata->per_channel_irq) {\r\nret = pdata->channel_irq[i];\r\nif (ret < 0) {\r\nnetdev_err(pdata->netdev,\r\n"get_irq %u failed\n",\r\ni + 1);\r\ngoto err_irq;\r\n}\r\nchannel->dma_irq = ret;\r\n}\r\nif (i < pdata->tx_ring_count)\r\nchannel->tx_ring = tx_ring++;\r\nif (i < pdata->rx_ring_count)\r\nchannel->rx_ring = rx_ring++;\r\nnetif_dbg(pdata, drv, pdata->netdev,\r\n"%s: dma_regs=%p, tx_ring=%p, rx_ring=%p\n",\r\nchannel->name, channel->dma_regs,\r\nchannel->tx_ring, channel->rx_ring);\r\n}\r\npdata->channel_head = channel_head;\r\nreturn 0;\r\nerr_irq:\r\nkfree(rx_ring);\r\nerr_rx_ring:\r\nkfree(tx_ring);\r\nerr_tx_ring:\r\nkfree(channel_head);\r\nreturn ret;\r\n}\r\nstatic void xlgmac_free_channels_and_rings(struct xlgmac_pdata *pdata)\r\n{\r\nxlgmac_free_rings(pdata);\r\nxlgmac_free_channels(pdata);\r\n}\r\nstatic int xlgmac_alloc_channels_and_rings(struct xlgmac_pdata *pdata)\r\n{\r\nint ret;\r\nret = xlgmac_alloc_channels(pdata);\r\nif (ret)\r\ngoto err_alloc;\r\nret = xlgmac_alloc_rings(pdata);\r\nif (ret)\r\ngoto err_alloc;\r\nreturn 0;\r\nerr_alloc:\r\nxlgmac_free_channels_and_rings(pdata);\r\nreturn ret;\r\n}\r\nstatic int xlgmac_alloc_pages(struct xlgmac_pdata *pdata,\r\nstruct xlgmac_page_alloc *pa,\r\ngfp_t gfp, int order)\r\n{\r\nstruct page *pages = NULL;\r\ndma_addr_t pages_dma;\r\ngfp |= __GFP_COLD | __GFP_COMP | __GFP_NOWARN;\r\nwhile (order >= 0) {\r\npages = alloc_pages(gfp, order);\r\nif (pages)\r\nbreak;\r\norder--;\r\n}\r\nif (!pages)\r\nreturn -ENOMEM;\r\npages_dma = dma_map_page(pdata->dev, pages, 0,\r\nPAGE_SIZE << order, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(pdata->dev, pages_dma)) {\r\nput_page(pages);\r\nreturn -ENOMEM;\r\n}\r\npa->pages = pages;\r\npa->pages_len = PAGE_SIZE << order;\r\npa->pages_offset = 0;\r\npa->pages_dma = pages_dma;\r\nreturn 0;\r\n}\r\nstatic void xlgmac_set_buffer_data(struct xlgmac_buffer_data *bd,\r\nstruct xlgmac_page_alloc *pa,\r\nunsigned int len)\r\n{\r\nget_page(pa->pages);\r\nbd->pa = *pa;\r\nbd->dma_base = pa->pages_dma;\r\nbd->dma_off = pa->pages_offset;\r\nbd->dma_len = len;\r\npa->pages_offset += len;\r\nif ((pa->pages_offset + len) > pa->pages_len) {\r\nbd->pa_unmap = *pa;\r\npa->pages = NULL;\r\npa->pages_len = 0;\r\npa->pages_offset = 0;\r\npa->pages_dma = 0;\r\n}\r\n}\r\nstatic int xlgmac_map_rx_buffer(struct xlgmac_pdata *pdata,\r\nstruct xlgmac_ring *ring,\r\nstruct xlgmac_desc_data *desc_data)\r\n{\r\nint order, ret;\r\nif (!ring->rx_hdr_pa.pages) {\r\nret = xlgmac_alloc_pages(pdata, &ring->rx_hdr_pa,\r\nGFP_ATOMIC, 0);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (!ring->rx_buf_pa.pages) {\r\norder = max_t(int, PAGE_ALLOC_COSTLY_ORDER - 1, 0);\r\nret = xlgmac_alloc_pages(pdata, &ring->rx_buf_pa,\r\nGFP_ATOMIC, order);\r\nif (ret)\r\nreturn ret;\r\n}\r\nxlgmac_set_buffer_data(&desc_data->rx.hdr, &ring->rx_hdr_pa,\r\nXLGMAC_SKB_ALLOC_SIZE);\r\nxlgmac_set_buffer_data(&desc_data->rx.buf, &ring->rx_buf_pa,\r\npdata->rx_buf_size);\r\nreturn 0;\r\n}\r\nstatic void xlgmac_tx_desc_init(struct xlgmac_pdata *pdata)\r\n{\r\nstruct xlgmac_hw_ops *hw_ops = &pdata->hw_ops;\r\nstruct xlgmac_desc_data *desc_data;\r\nstruct xlgmac_dma_desc *dma_desc;\r\nstruct xlgmac_channel *channel;\r\nstruct xlgmac_ring *ring;\r\ndma_addr_t dma_desc_addr;\r\nunsigned int i, j;\r\nchannel = pdata->channel_head;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nring = channel->tx_ring;\r\nif (!ring)\r\nbreak;\r\ndma_desc = ring->dma_desc_head;\r\ndma_desc_addr = ring->dma_desc_head_addr;\r\nfor (j = 0; j < ring->dma_desc_count; j++) {\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, j);\r\ndesc_data->dma_desc = dma_desc;\r\ndesc_data->dma_desc_addr = dma_desc_addr;\r\ndma_desc++;\r\ndma_desc_addr += sizeof(struct xlgmac_dma_desc);\r\n}\r\nring->cur = 0;\r\nring->dirty = 0;\r\nmemset(&ring->tx, 0, sizeof(ring->tx));\r\nhw_ops->tx_desc_init(channel);\r\n}\r\n}\r\nstatic void xlgmac_rx_desc_init(struct xlgmac_pdata *pdata)\r\n{\r\nstruct xlgmac_hw_ops *hw_ops = &pdata->hw_ops;\r\nstruct xlgmac_desc_data *desc_data;\r\nstruct xlgmac_dma_desc *dma_desc;\r\nstruct xlgmac_channel *channel;\r\nstruct xlgmac_ring *ring;\r\ndma_addr_t dma_desc_addr;\r\nunsigned int i, j;\r\nchannel = pdata->channel_head;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nring = channel->rx_ring;\r\nif (!ring)\r\nbreak;\r\ndma_desc = ring->dma_desc_head;\r\ndma_desc_addr = ring->dma_desc_head_addr;\r\nfor (j = 0; j < ring->dma_desc_count; j++) {\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, j);\r\ndesc_data->dma_desc = dma_desc;\r\ndesc_data->dma_desc_addr = dma_desc_addr;\r\nif (xlgmac_map_rx_buffer(pdata, ring, desc_data))\r\nbreak;\r\ndma_desc++;\r\ndma_desc_addr += sizeof(struct xlgmac_dma_desc);\r\n}\r\nring->cur = 0;\r\nring->dirty = 0;\r\nhw_ops->rx_desc_init(channel);\r\n}\r\n}\r\nstatic int xlgmac_map_tx_skb(struct xlgmac_channel *channel,\r\nstruct sk_buff *skb)\r\n{\r\nstruct xlgmac_pdata *pdata = channel->pdata;\r\nstruct xlgmac_ring *ring = channel->tx_ring;\r\nunsigned int start_index, cur_index;\r\nstruct xlgmac_desc_data *desc_data;\r\nunsigned int offset, datalen, len;\r\nstruct xlgmac_pkt_info *pkt_info;\r\nstruct skb_frag_struct *frag;\r\nunsigned int tso, vlan;\r\ndma_addr_t skb_dma;\r\nunsigned int i;\r\noffset = 0;\r\nstart_index = ring->cur;\r\ncur_index = ring->cur;\r\npkt_info = &ring->pkt_info;\r\npkt_info->desc_count = 0;\r\npkt_info->length = 0;\r\ntso = XLGMAC_GET_REG_BITS(pkt_info->attributes,\r\nTX_PACKET_ATTRIBUTES_TSO_ENABLE_POS,\r\nTX_PACKET_ATTRIBUTES_TSO_ENABLE_LEN);\r\nvlan = XLGMAC_GET_REG_BITS(pkt_info->attributes,\r\nTX_PACKET_ATTRIBUTES_VLAN_CTAG_POS,\r\nTX_PACKET_ATTRIBUTES_VLAN_CTAG_LEN);\r\nif ((tso && (pkt_info->mss != ring->tx.cur_mss)) ||\r\n(vlan && (pkt_info->vlan_ctag != ring->tx.cur_vlan_ctag)))\r\ncur_index++;\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, cur_index);\r\nif (tso) {\r\nskb_dma = dma_map_single(pdata->dev, skb->data,\r\npkt_info->header_len, DMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev, "dma_map_single failed\n");\r\ngoto err_out;\r\n}\r\ndesc_data->skb_dma = skb_dma;\r\ndesc_data->skb_dma_len = pkt_info->header_len;\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"skb header: index=%u, dma=%pad, len=%u\n",\r\ncur_index, &skb_dma, pkt_info->header_len);\r\noffset = pkt_info->header_len;\r\npkt_info->length += pkt_info->header_len;\r\ncur_index++;\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, cur_index);\r\n}\r\nfor (datalen = skb_headlen(skb) - offset; datalen; ) {\r\nlen = min_t(unsigned int, datalen, XLGMAC_TX_MAX_BUF_SIZE);\r\nskb_dma = dma_map_single(pdata->dev, skb->data + offset, len,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev, "dma_map_single failed\n");\r\ngoto err_out;\r\n}\r\ndesc_data->skb_dma = skb_dma;\r\ndesc_data->skb_dma_len = len;\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"skb data: index=%u, dma=%pad, len=%u\n",\r\ncur_index, &skb_dma, len);\r\ndatalen -= len;\r\noffset += len;\r\npkt_info->length += len;\r\ncur_index++;\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, cur_index);\r\n}\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"mapping frag %u\n", i);\r\nfrag = &skb_shinfo(skb)->frags[i];\r\noffset = 0;\r\nfor (datalen = skb_frag_size(frag); datalen; ) {\r\nlen = min_t(unsigned int, datalen,\r\nXLGMAC_TX_MAX_BUF_SIZE);\r\nskb_dma = skb_frag_dma_map(pdata->dev, frag, offset,\r\nlen, DMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev,\r\n"skb_frag_dma_map failed\n");\r\ngoto err_out;\r\n}\r\ndesc_data->skb_dma = skb_dma;\r\ndesc_data->skb_dma_len = len;\r\ndesc_data->mapped_as_page = 1;\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"skb frag: index=%u, dma=%pad, len=%u\n",\r\ncur_index, &skb_dma, len);\r\ndatalen -= len;\r\noffset += len;\r\npkt_info->length += len;\r\ncur_index++;\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, cur_index);\r\n}\r\n}\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, cur_index - 1);\r\ndesc_data->skb = skb;\r\npkt_info->desc_count = cur_index - start_index;\r\nreturn pkt_info->desc_count;\r\nerr_out:\r\nwhile (start_index < cur_index) {\r\ndesc_data = XLGMAC_GET_DESC_DATA(ring, start_index++);\r\nxlgmac_unmap_desc_data(pdata, desc_data);\r\n}\r\nreturn 0;\r\n}\r\nvoid xlgmac_init_desc_ops(struct xlgmac_desc_ops *desc_ops)\r\n{\r\ndesc_ops->alloc_channles_and_rings = xlgmac_alloc_channels_and_rings;\r\ndesc_ops->free_channels_and_rings = xlgmac_free_channels_and_rings;\r\ndesc_ops->map_tx_skb = xlgmac_map_tx_skb;\r\ndesc_ops->map_rx_buffer = xlgmac_map_rx_buffer;\r\ndesc_ops->unmap_desc_data = xlgmac_unmap_desc_data;\r\ndesc_ops->tx_desc_init = xlgmac_tx_desc_init;\r\ndesc_ops->rx_desc_init = xlgmac_rx_desc_init;\r\n}
