static int omap_crypto_copy_sg_lists(int total, int bs,\r\nstruct scatterlist **sg,\r\nstruct scatterlist *new_sg, u16 flags)\r\n{\r\nint n = sg_nents(*sg);\r\nstruct scatterlist *tmp;\r\nif (!(flags & OMAP_CRYPTO_FORCE_SINGLE_ENTRY)) {\r\nnew_sg = kmalloc_array(n, sizeof(*sg), GFP_KERNEL);\r\nif (!new_sg)\r\nreturn -ENOMEM;\r\nsg_init_table(new_sg, n);\r\n}\r\ntmp = new_sg;\r\nwhile (*sg && total) {\r\nint len = (*sg)->length;\r\nif (total < len)\r\nlen = total;\r\nif (len > 0) {\r\ntotal -= len;\r\nsg_set_page(tmp, sg_page(*sg), len, (*sg)->offset);\r\nif (total <= 0)\r\nsg_mark_end(tmp);\r\ntmp = sg_next(tmp);\r\n}\r\n*sg = sg_next(*sg);\r\n}\r\n*sg = new_sg;\r\nreturn 0;\r\n}\r\nstatic int omap_crypto_copy_sgs(int total, int bs, struct scatterlist **sg,\r\nstruct scatterlist *new_sg, u16 flags)\r\n{\r\nvoid *buf;\r\nint pages;\r\nint new_len;\r\nnew_len = ALIGN(total, bs);\r\npages = get_order(new_len);\r\nbuf = (void *)__get_free_pages(GFP_ATOMIC, pages);\r\nif (!buf) {\r\npr_err("%s: Couldn't allocate pages for unaligned cases.\n",\r\n__func__);\r\nreturn -ENOMEM;\r\n}\r\nif (flags & OMAP_CRYPTO_COPY_DATA) {\r\nscatterwalk_map_and_copy(buf, *sg, 0, total, 0);\r\nif (flags & OMAP_CRYPTO_ZERO_BUF)\r\nmemset(buf + total, 0, new_len - total);\r\n}\r\nif (!(flags & OMAP_CRYPTO_FORCE_SINGLE_ENTRY))\r\nsg_init_table(new_sg, 1);\r\nsg_set_buf(new_sg, buf, new_len);\r\n*sg = new_sg;\r\nreturn 0;\r\n}\r\nstatic int omap_crypto_check_sg(struct scatterlist *sg, int total, int bs,\r\nu16 flags)\r\n{\r\nint len = 0;\r\nint num_sg = 0;\r\nif (!IS_ALIGNED(total, bs))\r\nreturn OMAP_CRYPTO_NOT_ALIGNED;\r\nwhile (sg) {\r\nnum_sg++;\r\nif (!IS_ALIGNED(sg->offset, 4))\r\nreturn OMAP_CRYPTO_NOT_ALIGNED;\r\nif (!IS_ALIGNED(sg->length, bs))\r\nreturn OMAP_CRYPTO_NOT_ALIGNED;\r\nlen += sg->length;\r\nsg = sg_next(sg);\r\nif (len >= total)\r\nbreak;\r\n}\r\nif ((flags & OMAP_CRYPTO_FORCE_SINGLE_ENTRY) && num_sg > 1)\r\nreturn OMAP_CRYPTO_NOT_ALIGNED;\r\nif (len != total)\r\nreturn OMAP_CRYPTO_BAD_DATA_LENGTH;\r\nreturn 0;\r\n}\r\nint omap_crypto_align_sg(struct scatterlist **sg, int total, int bs,\r\nstruct scatterlist *new_sg, u16 flags,\r\nu8 flags_shift, unsigned long *dd_flags)\r\n{\r\nint ret;\r\n*dd_flags &= ~(OMAP_CRYPTO_COPY_MASK << flags_shift);\r\nif (flags & OMAP_CRYPTO_FORCE_COPY)\r\nret = OMAP_CRYPTO_NOT_ALIGNED;\r\nelse\r\nret = omap_crypto_check_sg(*sg, total, bs, flags);\r\nif (ret == OMAP_CRYPTO_NOT_ALIGNED) {\r\nret = omap_crypto_copy_sgs(total, bs, sg, new_sg, flags);\r\nif (ret)\r\nreturn ret;\r\n*dd_flags |= OMAP_CRYPTO_DATA_COPIED << flags_shift;\r\n} else if (ret == OMAP_CRYPTO_BAD_DATA_LENGTH) {\r\nret = omap_crypto_copy_sg_lists(total, bs, sg, new_sg, flags);\r\nif (ret)\r\nreturn ret;\r\nif (!(flags & OMAP_CRYPTO_FORCE_SINGLE_ENTRY))\r\n*dd_flags |= OMAP_CRYPTO_SG_COPIED << flags_shift;\r\n} else if (flags & OMAP_CRYPTO_FORCE_SINGLE_ENTRY) {\r\nsg_set_buf(new_sg, sg_virt(*sg), (*sg)->length);\r\n}\r\nreturn 0;\r\n}\r\nvoid omap_crypto_cleanup(struct scatterlist *sg, struct scatterlist *orig,\r\nint offset, int len, u8 flags_shift,\r\nunsigned long flags)\r\n{\r\nvoid *buf;\r\nint pages;\r\nflags >>= flags_shift;\r\nflags &= OMAP_CRYPTO_COPY_MASK;\r\nif (!flags)\r\nreturn;\r\nbuf = sg_virt(sg);\r\npages = get_order(len);\r\nif (orig && (flags & OMAP_CRYPTO_COPY_MASK))\r\nscatterwalk_map_and_copy(buf, orig, offset, len, 1);\r\nif (flags & OMAP_CRYPTO_DATA_COPIED)\r\nfree_pages((unsigned long)buf, pages);\r\nelse if (flags & OMAP_CRYPTO_SG_COPIED)\r\nkfree(sg);\r\n}
