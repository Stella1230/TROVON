static int vnic_rq_alloc_bufs(struct vnic_rq *rq)\r\n{\r\nstruct vnic_rq_buf *buf;\r\nstruct vnic_dev *vdev;\r\nunsigned int i, j, count = rq->ring.desc_count;\r\nunsigned int blks = VNIC_RQ_BUF_BLKS_NEEDED(count);\r\nvdev = rq->vdev;\r\nfor (i = 0; i < blks; i++) {\r\nrq->bufs[i] = kzalloc(VNIC_RQ_BUF_BLK_SZ, GFP_ATOMIC);\r\nif (!rq->bufs[i]) {\r\nprintk(KERN_ERR "Failed to alloc rq_bufs\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\nfor (i = 0; i < blks; i++) {\r\nbuf = rq->bufs[i];\r\nfor (j = 0; j < VNIC_RQ_BUF_BLK_ENTRIES; j++) {\r\nbuf->index = i * VNIC_RQ_BUF_BLK_ENTRIES + j;\r\nbuf->desc = (u8 *)rq->ring.descs +\r\nrq->ring.desc_size * buf->index;\r\nif (buf->index + 1 == count) {\r\nbuf->next = rq->bufs[0];\r\nbreak;\r\n} else if (j + 1 == VNIC_RQ_BUF_BLK_ENTRIES) {\r\nbuf->next = rq->bufs[i + 1];\r\n} else {\r\nbuf->next = buf + 1;\r\nbuf++;\r\n}\r\n}\r\n}\r\nrq->to_use = rq->to_clean = rq->bufs[0];\r\nrq->buf_index = 0;\r\nreturn 0;\r\n}\r\nvoid vnic_rq_free(struct vnic_rq *rq)\r\n{\r\nstruct vnic_dev *vdev;\r\nunsigned int i;\r\nvdev = rq->vdev;\r\nvnic_dev_free_desc_ring(vdev, &rq->ring);\r\nfor (i = 0; i < VNIC_RQ_BUF_BLKS_MAX; i++) {\r\nkfree(rq->bufs[i]);\r\nrq->bufs[i] = NULL;\r\n}\r\nrq->ctrl = NULL;\r\n}\r\nint vnic_rq_alloc(struct vnic_dev *vdev, struct vnic_rq *rq, unsigned int index,\r\nunsigned int desc_count, unsigned int desc_size)\r\n{\r\nint err;\r\nrq->index = index;\r\nrq->vdev = vdev;\r\nrq->ctrl = vnic_dev_get_res(vdev, RES_TYPE_RQ, index);\r\nif (!rq->ctrl) {\r\nprintk(KERN_ERR "Failed to hook RQ[%d] resource\n", index);\r\nreturn -EINVAL;\r\n}\r\nvnic_rq_disable(rq);\r\nerr = vnic_dev_alloc_desc_ring(vdev, &rq->ring, desc_count, desc_size);\r\nif (err)\r\nreturn err;\r\nerr = vnic_rq_alloc_bufs(rq);\r\nif (err) {\r\nvnic_rq_free(rq);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nvoid vnic_rq_init(struct vnic_rq *rq, unsigned int cq_index,\r\nunsigned int error_interrupt_enable,\r\nunsigned int error_interrupt_offset)\r\n{\r\nu64 paddr;\r\nu32 fetch_index;\r\npaddr = (u64)rq->ring.base_addr | VNIC_PADDR_TARGET;\r\nwriteq(paddr, &rq->ctrl->ring_base);\r\niowrite32(rq->ring.desc_count, &rq->ctrl->ring_size);\r\niowrite32(cq_index, &rq->ctrl->cq_index);\r\niowrite32(error_interrupt_enable, &rq->ctrl->error_interrupt_enable);\r\niowrite32(error_interrupt_offset, &rq->ctrl->error_interrupt_offset);\r\niowrite32(0, &rq->ctrl->dropped_packet_count);\r\niowrite32(0, &rq->ctrl->error_status);\r\nfetch_index = ioread32(&rq->ctrl->fetch_index);\r\nrq->to_use = rq->to_clean =\r\n&rq->bufs[fetch_index / VNIC_RQ_BUF_BLK_ENTRIES]\r\n[fetch_index % VNIC_RQ_BUF_BLK_ENTRIES];\r\niowrite32(fetch_index, &rq->ctrl->posted_index);\r\nrq->buf_index = 0;\r\n}\r\nunsigned int vnic_rq_error_status(struct vnic_rq *rq)\r\n{\r\nreturn ioread32(&rq->ctrl->error_status);\r\n}\r\nvoid vnic_rq_enable(struct vnic_rq *rq)\r\n{\r\niowrite32(1, &rq->ctrl->enable);\r\n}\r\nint vnic_rq_disable(struct vnic_rq *rq)\r\n{\r\nunsigned int wait;\r\niowrite32(0, &rq->ctrl->enable);\r\nfor (wait = 0; wait < 100; wait++) {\r\nif (!(ioread32(&rq->ctrl->running)))\r\nreturn 0;\r\nudelay(1);\r\n}\r\nprintk(KERN_ERR "Failed to disable RQ[%d]\n", rq->index);\r\nreturn -ETIMEDOUT;\r\n}\r\nvoid vnic_rq_clean(struct vnic_rq *rq,\r\nvoid (*buf_clean)(struct vnic_rq *rq, struct vnic_rq_buf *buf))\r\n{\r\nstruct vnic_rq_buf *buf;\r\nu32 fetch_index;\r\nBUG_ON(ioread32(&rq->ctrl->enable));\r\nbuf = rq->to_clean;\r\nwhile (vnic_rq_desc_used(rq) > 0) {\r\n(*buf_clean)(rq, buf);\r\nbuf = rq->to_clean = buf->next;\r\nrq->ring.desc_avail++;\r\n}\r\nfetch_index = ioread32(&rq->ctrl->fetch_index);\r\nrq->to_use = rq->to_clean =\r\n&rq->bufs[fetch_index / VNIC_RQ_BUF_BLK_ENTRIES]\r\n[fetch_index % VNIC_RQ_BUF_BLK_ENTRIES];\r\niowrite32(fetch_index, &rq->ctrl->posted_index);\r\nrq->buf_index = 0;\r\nvnic_dev_clear_desc_ring(&rq->ring);\r\n}
