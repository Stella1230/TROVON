static void intel_pt_dump(struct intel_pt *pt __maybe_unused,\r\nunsigned char *buf, size_t len)\r\n{\r\nstruct intel_pt_pkt packet;\r\nsize_t pos = 0;\r\nint ret, pkt_len, i;\r\nchar desc[INTEL_PT_PKT_DESC_MAX];\r\nconst char *color = PERF_COLOR_BLUE;\r\ncolor_fprintf(stdout, color,\r\n". ... Intel Processor Trace data: size %zu bytes\n",\r\nlen);\r\nwhile (len) {\r\nret = intel_pt_get_packet(buf, len, &packet);\r\nif (ret > 0)\r\npkt_len = ret;\r\nelse\r\npkt_len = 1;\r\nprintf(".");\r\ncolor_fprintf(stdout, color, " %08x: ", pos);\r\nfor (i = 0; i < pkt_len; i++)\r\ncolor_fprintf(stdout, color, " %02x", buf[i]);\r\nfor (; i < 16; i++)\r\ncolor_fprintf(stdout, color, " ");\r\nif (ret > 0) {\r\nret = intel_pt_pkt_desc(&packet, desc,\r\nINTEL_PT_PKT_DESC_MAX);\r\nif (ret > 0)\r\ncolor_fprintf(stdout, color, " %s\n", desc);\r\n} else {\r\ncolor_fprintf(stdout, color, " Bad packet!\n");\r\n}\r\npos += pkt_len;\r\nbuf += pkt_len;\r\nlen -= pkt_len;\r\n}\r\n}\r\nstatic void intel_pt_dump_event(struct intel_pt *pt, unsigned char *buf,\r\nsize_t len)\r\n{\r\nprintf(".\n");\r\nintel_pt_dump(pt, buf, len);\r\n}\r\nstatic int intel_pt_do_fix_overlap(struct intel_pt *pt, struct auxtrace_buffer *a,\r\nstruct auxtrace_buffer *b)\r\n{\r\nvoid *start;\r\nstart = intel_pt_find_overlap(a->data, a->size, b->data, b->size,\r\npt->have_tsc);\r\nif (!start)\r\nreturn -EINVAL;\r\nb->use_size = b->data + b->size - start;\r\nb->use_data = start;\r\nreturn 0;\r\n}\r\nstatic void intel_pt_use_buffer_pid_tid(struct intel_pt_queue *ptq,\r\nstruct auxtrace_queue *queue,\r\nstruct auxtrace_buffer *buffer)\r\n{\r\nif (queue->cpu == -1 && buffer->cpu != -1)\r\nptq->cpu = buffer->cpu;\r\nptq->pid = buffer->pid;\r\nptq->tid = buffer->tid;\r\nintel_pt_log("queue %u cpu %d pid %d tid %d\n",\r\nptq->queue_nr, ptq->cpu, ptq->pid, ptq->tid);\r\nthread__zput(ptq->thread);\r\nif (ptq->tid != -1) {\r\nif (ptq->pid != -1)\r\nptq->thread = machine__findnew_thread(ptq->pt->machine,\r\nptq->pid,\r\nptq->tid);\r\nelse\r\nptq->thread = machine__find_thread(ptq->pt->machine, -1,\r\nptq->tid);\r\n}\r\n}\r\nstatic int intel_pt_get_trace(struct intel_pt_buffer *b, void *data)\r\n{\r\nstruct intel_pt_queue *ptq = data;\r\nstruct auxtrace_buffer *buffer = ptq->buffer, *old_buffer = buffer;\r\nstruct auxtrace_queue *queue;\r\nif (ptq->stop) {\r\nb->len = 0;\r\nreturn 0;\r\n}\r\nqueue = &ptq->pt->queues.queue_array[ptq->queue_nr];\r\nnext:\r\nbuffer = auxtrace_buffer__next(queue, buffer);\r\nif (!buffer) {\r\nif (old_buffer)\r\nauxtrace_buffer__drop_data(old_buffer);\r\nb->len = 0;\r\nreturn 0;\r\n}\r\nptq->buffer = buffer;\r\nif (!buffer->data) {\r\nint fd = perf_data_file__fd(ptq->pt->session->file);\r\nbuffer->data = auxtrace_buffer__get_data(buffer, fd);\r\nif (!buffer->data)\r\nreturn -ENOMEM;\r\n}\r\nif (ptq->pt->snapshot_mode && !buffer->consecutive && old_buffer &&\r\nintel_pt_do_fix_overlap(ptq->pt, old_buffer, buffer))\r\nreturn -ENOMEM;\r\nif (buffer->use_data) {\r\nb->len = buffer->use_size;\r\nb->buf = buffer->use_data;\r\n} else {\r\nb->len = buffer->size;\r\nb->buf = buffer->data;\r\n}\r\nb->ref_timestamp = buffer->reference;\r\nif (ptq->pt->snapshot_mode && !b->len)\r\ngoto next;\r\nif (old_buffer)\r\nauxtrace_buffer__drop_data(old_buffer);\r\nif (!old_buffer || ptq->pt->sampling_mode || (ptq->pt->snapshot_mode &&\r\n!buffer->consecutive)) {\r\nb->consecutive = false;\r\nb->trace_nr = buffer->buffer_nr + 1;\r\n} else {\r\nb->consecutive = true;\r\n}\r\nif (ptq->use_buffer_pid_tid && (ptq->pid != buffer->pid ||\r\nptq->tid != buffer->tid))\r\nintel_pt_use_buffer_pid_tid(ptq, queue, buffer);\r\nif (ptq->step_through_buffers)\r\nptq->stop = true;\r\nif (!b->len)\r\nreturn intel_pt_get_trace(b, data);\r\nreturn 0;\r\n}\r\nstatic int intel_pt_config_div(const char *var, const char *value, void *data)\r\n{\r\nint *d = data;\r\nlong val;\r\nif (!strcmp(var, "intel-pt.cache-divisor")) {\r\nval = strtol(value, NULL, 0);\r\nif (val > 0 && val <= INT_MAX)\r\n*d = val;\r\n}\r\nreturn 0;\r\n}\r\nstatic int intel_pt_cache_divisor(void)\r\n{\r\nstatic int d;\r\nif (d)\r\nreturn d;\r\nperf_config(intel_pt_config_div, &d);\r\nif (!d)\r\nd = 64;\r\nreturn d;\r\n}\r\nstatic unsigned int intel_pt_cache_size(struct dso *dso,\r\nstruct machine *machine)\r\n{\r\noff_t size;\r\nsize = dso__data_size(dso, machine);\r\nsize /= intel_pt_cache_divisor();\r\nif (size < 1000)\r\nreturn 10;\r\nif (size > (1 << 21))\r\nreturn 21;\r\nreturn 32 - __builtin_clz(size);\r\n}\r\nstatic struct auxtrace_cache *intel_pt_cache(struct dso *dso,\r\nstruct machine *machine)\r\n{\r\nstruct auxtrace_cache *c;\r\nunsigned int bits;\r\nif (dso->auxtrace_cache)\r\nreturn dso->auxtrace_cache;\r\nbits = intel_pt_cache_size(dso, machine);\r\nc = auxtrace_cache__new(bits, sizeof(struct intel_pt_cache_entry), 200);\r\ndso->auxtrace_cache = c;\r\nreturn c;\r\n}\r\nstatic int intel_pt_cache_add(struct dso *dso, struct machine *machine,\r\nu64 offset, u64 insn_cnt, u64 byte_cnt,\r\nstruct intel_pt_insn *intel_pt_insn)\r\n{\r\nstruct auxtrace_cache *c = intel_pt_cache(dso, machine);\r\nstruct intel_pt_cache_entry *e;\r\nint err;\r\nif (!c)\r\nreturn -ENOMEM;\r\ne = auxtrace_cache__alloc_entry(c);\r\nif (!e)\r\nreturn -ENOMEM;\r\ne->insn_cnt = insn_cnt;\r\ne->byte_cnt = byte_cnt;\r\ne->op = intel_pt_insn->op;\r\ne->branch = intel_pt_insn->branch;\r\ne->length = intel_pt_insn->length;\r\ne->rel = intel_pt_insn->rel;\r\nmemcpy(e->insn, intel_pt_insn->buf, INTEL_PT_INSN_BUF_SZ);\r\nerr = auxtrace_cache__add(c, offset, &e->entry);\r\nif (err)\r\nauxtrace_cache__free_entry(c, e);\r\nreturn err;\r\n}\r\nstatic struct intel_pt_cache_entry *\r\nintel_pt_cache_lookup(struct dso *dso, struct machine *machine, u64 offset)\r\n{\r\nstruct auxtrace_cache *c = intel_pt_cache(dso, machine);\r\nif (!c)\r\nreturn NULL;\r\nreturn auxtrace_cache__lookup(dso->auxtrace_cache, offset);\r\n}\r\nstatic int intel_pt_walk_next_insn(struct intel_pt_insn *intel_pt_insn,\r\nuint64_t *insn_cnt_ptr, uint64_t *ip,\r\nuint64_t to_ip, uint64_t max_insn_cnt,\r\nvoid *data)\r\n{\r\nstruct intel_pt_queue *ptq = data;\r\nstruct machine *machine = ptq->pt->machine;\r\nstruct thread *thread;\r\nstruct addr_location al;\r\nunsigned char buf[INTEL_PT_INSN_BUF_SZ];\r\nssize_t len;\r\nint x86_64;\r\nu8 cpumode;\r\nu64 offset, start_offset, start_ip;\r\nu64 insn_cnt = 0;\r\nbool one_map = true;\r\nintel_pt_insn->length = 0;\r\nif (to_ip && *ip == to_ip)\r\ngoto out_no_cache;\r\nif (*ip >= ptq->pt->kernel_start)\r\ncpumode = PERF_RECORD_MISC_KERNEL;\r\nelse\r\ncpumode = PERF_RECORD_MISC_USER;\r\nthread = ptq->thread;\r\nif (!thread) {\r\nif (cpumode != PERF_RECORD_MISC_KERNEL)\r\nreturn -EINVAL;\r\nthread = ptq->pt->unknown_thread;\r\n}\r\nwhile (1) {\r\nthread__find_addr_map(thread, cpumode, MAP__FUNCTION, *ip, &al);\r\nif (!al.map || !al.map->dso)\r\nreturn -EINVAL;\r\nif (al.map->dso->data.status == DSO_DATA_STATUS_ERROR &&\r\ndso__data_status_seen(al.map->dso,\r\nDSO_DATA_STATUS_SEEN_ITRACE))\r\nreturn -ENOENT;\r\noffset = al.map->map_ip(al.map, *ip);\r\nif (!to_ip && one_map) {\r\nstruct intel_pt_cache_entry *e;\r\ne = intel_pt_cache_lookup(al.map->dso, machine, offset);\r\nif (e &&\r\n(!max_insn_cnt || e->insn_cnt <= max_insn_cnt)) {\r\n*insn_cnt_ptr = e->insn_cnt;\r\n*ip += e->byte_cnt;\r\nintel_pt_insn->op = e->op;\r\nintel_pt_insn->branch = e->branch;\r\nintel_pt_insn->length = e->length;\r\nintel_pt_insn->rel = e->rel;\r\nmemcpy(intel_pt_insn->buf, e->insn,\r\nINTEL_PT_INSN_BUF_SZ);\r\nintel_pt_log_insn_no_data(intel_pt_insn, *ip);\r\nreturn 0;\r\n}\r\n}\r\nstart_offset = offset;\r\nstart_ip = *ip;\r\nmap__load(al.map);\r\nx86_64 = al.map->dso->is_64_bit;\r\nwhile (1) {\r\nlen = dso__data_read_offset(al.map->dso, machine,\r\noffset, buf,\r\nINTEL_PT_INSN_BUF_SZ);\r\nif (len <= 0)\r\nreturn -EINVAL;\r\nif (intel_pt_get_insn(buf, len, x86_64, intel_pt_insn))\r\nreturn -EINVAL;\r\nintel_pt_log_insn(intel_pt_insn, *ip);\r\ninsn_cnt += 1;\r\nif (intel_pt_insn->branch != INTEL_PT_BR_NO_BRANCH)\r\ngoto out;\r\nif (max_insn_cnt && insn_cnt >= max_insn_cnt)\r\ngoto out_no_cache;\r\n*ip += intel_pt_insn->length;\r\nif (to_ip && *ip == to_ip)\r\ngoto out_no_cache;\r\nif (*ip >= al.map->end)\r\nbreak;\r\noffset += intel_pt_insn->length;\r\n}\r\none_map = false;\r\n}\r\nout:\r\n*insn_cnt_ptr = insn_cnt;\r\nif (!one_map)\r\ngoto out_no_cache;\r\nif (to_ip) {\r\nstruct intel_pt_cache_entry *e;\r\ne = intel_pt_cache_lookup(al.map->dso, machine, start_offset);\r\nif (e)\r\nreturn 0;\r\n}\r\nintel_pt_cache_add(al.map->dso, machine, start_offset, insn_cnt,\r\n*ip - start_ip, intel_pt_insn);\r\nreturn 0;\r\nout_no_cache:\r\n*insn_cnt_ptr = insn_cnt;\r\nreturn 0;\r\n}\r\nstatic bool intel_pt_match_pgd_ip(struct intel_pt *pt, uint64_t ip,\r\nuint64_t offset, const char *filename)\r\n{\r\nstruct addr_filter *filt;\r\nbool have_filter = false;\r\nbool hit_tracestop = false;\r\nbool hit_filter = false;\r\nlist_for_each_entry(filt, &pt->filts.head, list) {\r\nif (filt->start)\r\nhave_filter = true;\r\nif ((filename && !filt->filename) ||\r\n(!filename && filt->filename) ||\r\n(filename && strcmp(filename, filt->filename)))\r\ncontinue;\r\nif (!(offset >= filt->addr && offset < filt->addr + filt->size))\r\ncontinue;\r\nintel_pt_log("TIP.PGD ip %#"PRIx64" offset %#"PRIx64" in %s hit filter: %s offset %#"PRIx64" size %#"PRIx64"\n",\r\nip, offset, filename ? filename : "[kernel]",\r\nfilt->start ? "filter" : "stop",\r\nfilt->addr, filt->size);\r\nif (filt->start)\r\nhit_filter = true;\r\nelse\r\nhit_tracestop = true;\r\n}\r\nif (!hit_tracestop && !hit_filter)\r\nintel_pt_log("TIP.PGD ip %#"PRIx64" offset %#"PRIx64" in %s is not in a filter region\n",\r\nip, offset, filename ? filename : "[kernel]");\r\nreturn hit_tracestop || (have_filter && !hit_filter);\r\n}\r\nstatic int __intel_pt_pgd_ip(uint64_t ip, void *data)\r\n{\r\nstruct intel_pt_queue *ptq = data;\r\nstruct thread *thread;\r\nstruct addr_location al;\r\nu8 cpumode;\r\nu64 offset;\r\nif (ip >= ptq->pt->kernel_start)\r\nreturn intel_pt_match_pgd_ip(ptq->pt, ip, ip, NULL);\r\ncpumode = PERF_RECORD_MISC_USER;\r\nthread = ptq->thread;\r\nif (!thread)\r\nreturn -EINVAL;\r\nthread__find_addr_map(thread, cpumode, MAP__FUNCTION, ip, &al);\r\nif (!al.map || !al.map->dso)\r\nreturn -EINVAL;\r\noffset = al.map->map_ip(al.map, ip);\r\nreturn intel_pt_match_pgd_ip(ptq->pt, ip, offset,\r\nal.map->dso->long_name);\r\n}\r\nstatic bool intel_pt_pgd_ip(uint64_t ip, void *data)\r\n{\r\nreturn __intel_pt_pgd_ip(ip, data) > 0;\r\n}\r\nstatic bool intel_pt_get_config(struct intel_pt *pt,\r\nstruct perf_event_attr *attr, u64 *config)\r\n{\r\nif (attr->type == pt->pmu_type) {\r\nif (config)\r\n*config = attr->config;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool intel_pt_exclude_kernel(struct intel_pt *pt)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each_entry(pt->session->evlist, evsel) {\r\nif (intel_pt_get_config(pt, &evsel->attr, NULL) &&\r\n!evsel->attr.exclude_kernel)\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic bool intel_pt_return_compression(struct intel_pt *pt)\r\n{\r\nstruct perf_evsel *evsel;\r\nu64 config;\r\nif (!pt->noretcomp_bit)\r\nreturn true;\r\nevlist__for_each_entry(pt->session->evlist, evsel) {\r\nif (intel_pt_get_config(pt, &evsel->attr, &config) &&\r\n(config & pt->noretcomp_bit))\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic bool intel_pt_branch_enable(struct intel_pt *pt)\r\n{\r\nstruct perf_evsel *evsel;\r\nu64 config;\r\nevlist__for_each_entry(pt->session->evlist, evsel) {\r\nif (intel_pt_get_config(pt, &evsel->attr, &config) &&\r\n(config & 1) && !(config & 0x2000))\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic unsigned int intel_pt_mtc_period(struct intel_pt *pt)\r\n{\r\nstruct perf_evsel *evsel;\r\nunsigned int shift;\r\nu64 config;\r\nif (!pt->mtc_freq_bits)\r\nreturn 0;\r\nfor (shift = 0, config = pt->mtc_freq_bits; !(config & 1); shift++)\r\nconfig >>= 1;\r\nevlist__for_each_entry(pt->session->evlist, evsel) {\r\nif (intel_pt_get_config(pt, &evsel->attr, &config))\r\nreturn (config & pt->mtc_freq_bits) >> shift;\r\n}\r\nreturn 0;\r\n}\r\nstatic bool intel_pt_timeless_decoding(struct intel_pt *pt)\r\n{\r\nstruct perf_evsel *evsel;\r\nbool timeless_decoding = true;\r\nu64 config;\r\nif (!pt->tsc_bit || !pt->cap_user_time_zero)\r\nreturn true;\r\nevlist__for_each_entry(pt->session->evlist, evsel) {\r\nif (!(evsel->attr.sample_type & PERF_SAMPLE_TIME))\r\nreturn true;\r\nif (intel_pt_get_config(pt, &evsel->attr, &config)) {\r\nif (config & pt->tsc_bit)\r\ntimeless_decoding = false;\r\nelse\r\nreturn true;\r\n}\r\n}\r\nreturn timeless_decoding;\r\n}\r\nstatic bool intel_pt_tracing_kernel(struct intel_pt *pt)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each_entry(pt->session->evlist, evsel) {\r\nif (intel_pt_get_config(pt, &evsel->attr, NULL) &&\r\n!evsel->attr.exclude_kernel)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool intel_pt_have_tsc(struct intel_pt *pt)\r\n{\r\nstruct perf_evsel *evsel;\r\nbool have_tsc = false;\r\nu64 config;\r\nif (!pt->tsc_bit)\r\nreturn false;\r\nevlist__for_each_entry(pt->session->evlist, evsel) {\r\nif (intel_pt_get_config(pt, &evsel->attr, &config)) {\r\nif (config & pt->tsc_bit)\r\nhave_tsc = true;\r\nelse\r\nreturn false;\r\n}\r\n}\r\nreturn have_tsc;\r\n}\r\nstatic u64 intel_pt_ns_to_ticks(const struct intel_pt *pt, u64 ns)\r\n{\r\nu64 quot, rem;\r\nquot = ns / pt->tc.time_mult;\r\nrem = ns % pt->tc.time_mult;\r\nreturn (quot << pt->tc.time_shift) + (rem << pt->tc.time_shift) /\r\npt->tc.time_mult;\r\n}\r\nstatic struct intel_pt_queue *intel_pt_alloc_queue(struct intel_pt *pt,\r\nunsigned int queue_nr)\r\n{\r\nstruct intel_pt_params params = { .get_trace = 0, };\r\nstruct intel_pt_queue *ptq;\r\nptq = zalloc(sizeof(struct intel_pt_queue));\r\nif (!ptq)\r\nreturn NULL;\r\nif (pt->synth_opts.callchain) {\r\nsize_t sz = sizeof(struct ip_callchain);\r\nsz += pt->synth_opts.callchain_sz * sizeof(u64);\r\nptq->chain = zalloc(sz);\r\nif (!ptq->chain)\r\ngoto out_free;\r\n}\r\nif (pt->synth_opts.last_branch) {\r\nsize_t sz = sizeof(struct branch_stack);\r\nsz += pt->synth_opts.last_branch_sz *\r\nsizeof(struct branch_entry);\r\nptq->last_branch = zalloc(sz);\r\nif (!ptq->last_branch)\r\ngoto out_free;\r\nptq->last_branch_rb = zalloc(sz);\r\nif (!ptq->last_branch_rb)\r\ngoto out_free;\r\n}\r\nptq->event_buf = malloc(PERF_SAMPLE_MAX_SIZE);\r\nif (!ptq->event_buf)\r\ngoto out_free;\r\nptq->pt = pt;\r\nptq->queue_nr = queue_nr;\r\nptq->exclude_kernel = intel_pt_exclude_kernel(pt);\r\nptq->pid = -1;\r\nptq->tid = -1;\r\nptq->cpu = -1;\r\nptq->next_tid = -1;\r\nparams.get_trace = intel_pt_get_trace;\r\nparams.walk_insn = intel_pt_walk_next_insn;\r\nparams.data = ptq;\r\nparams.return_compression = intel_pt_return_compression(pt);\r\nparams.branch_enable = intel_pt_branch_enable(pt);\r\nparams.max_non_turbo_ratio = pt->max_non_turbo_ratio;\r\nparams.mtc_period = intel_pt_mtc_period(pt);\r\nparams.tsc_ctc_ratio_n = pt->tsc_ctc_ratio_n;\r\nparams.tsc_ctc_ratio_d = pt->tsc_ctc_ratio_d;\r\nif (pt->filts.cnt > 0)\r\nparams.pgd_ip = intel_pt_pgd_ip;\r\nif (pt->synth_opts.instructions) {\r\nif (pt->synth_opts.period) {\r\nswitch (pt->synth_opts.period_type) {\r\ncase PERF_ITRACE_PERIOD_INSTRUCTIONS:\r\nparams.period_type =\r\nINTEL_PT_PERIOD_INSTRUCTIONS;\r\nparams.period = pt->synth_opts.period;\r\nbreak;\r\ncase PERF_ITRACE_PERIOD_TICKS:\r\nparams.period_type = INTEL_PT_PERIOD_TICKS;\r\nparams.period = pt->synth_opts.period;\r\nbreak;\r\ncase PERF_ITRACE_PERIOD_NANOSECS:\r\nparams.period_type = INTEL_PT_PERIOD_TICKS;\r\nparams.period = intel_pt_ns_to_ticks(pt,\r\npt->synth_opts.period);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nif (!params.period) {\r\nparams.period_type = INTEL_PT_PERIOD_INSTRUCTIONS;\r\nparams.period = 1;\r\n}\r\n}\r\nptq->decoder = intel_pt_decoder_new(&params);\r\nif (!ptq->decoder)\r\ngoto out_free;\r\nreturn ptq;\r\nout_free:\r\nzfree(&ptq->event_buf);\r\nzfree(&ptq->last_branch);\r\nzfree(&ptq->last_branch_rb);\r\nzfree(&ptq->chain);\r\nfree(ptq);\r\nreturn NULL;\r\n}\r\nstatic void intel_pt_free_queue(void *priv)\r\n{\r\nstruct intel_pt_queue *ptq = priv;\r\nif (!ptq)\r\nreturn;\r\nthread__zput(ptq->thread);\r\nintel_pt_decoder_free(ptq->decoder);\r\nzfree(&ptq->event_buf);\r\nzfree(&ptq->last_branch);\r\nzfree(&ptq->last_branch_rb);\r\nzfree(&ptq->chain);\r\nfree(ptq);\r\n}\r\nstatic void intel_pt_set_pid_tid_cpu(struct intel_pt *pt,\r\nstruct auxtrace_queue *queue)\r\n{\r\nstruct intel_pt_queue *ptq = queue->priv;\r\nif (queue->tid == -1 || pt->have_sched_switch) {\r\nptq->tid = machine__get_current_tid(pt->machine, ptq->cpu);\r\nthread__zput(ptq->thread);\r\n}\r\nif (!ptq->thread && ptq->tid != -1)\r\nptq->thread = machine__find_thread(pt->machine, -1, ptq->tid);\r\nif (ptq->thread) {\r\nptq->pid = ptq->thread->pid_;\r\nif (queue->cpu == -1)\r\nptq->cpu = ptq->thread->cpu;\r\n}\r\n}\r\nstatic void intel_pt_sample_flags(struct intel_pt_queue *ptq)\r\n{\r\nif (ptq->state->flags & INTEL_PT_ABORT_TX) {\r\nptq->flags = PERF_IP_FLAG_BRANCH | PERF_IP_FLAG_TX_ABORT;\r\n} else if (ptq->state->flags & INTEL_PT_ASYNC) {\r\nif (ptq->state->to_ip)\r\nptq->flags = PERF_IP_FLAG_BRANCH | PERF_IP_FLAG_CALL |\r\nPERF_IP_FLAG_ASYNC |\r\nPERF_IP_FLAG_INTERRUPT;\r\nelse\r\nptq->flags = PERF_IP_FLAG_BRANCH |\r\nPERF_IP_FLAG_TRACE_END;\r\nptq->insn_len = 0;\r\n} else {\r\nif (ptq->state->from_ip)\r\nptq->flags = intel_pt_insn_type(ptq->state->insn_op);\r\nelse\r\nptq->flags = PERF_IP_FLAG_BRANCH |\r\nPERF_IP_FLAG_TRACE_BEGIN;\r\nif (ptq->state->flags & INTEL_PT_IN_TX)\r\nptq->flags |= PERF_IP_FLAG_IN_TX;\r\nptq->insn_len = ptq->state->insn_len;\r\nmemcpy(ptq->insn, ptq->state->insn, INTEL_PT_INSN_BUF_SZ);\r\n}\r\n}\r\nstatic int intel_pt_setup_queue(struct intel_pt *pt,\r\nstruct auxtrace_queue *queue,\r\nunsigned int queue_nr)\r\n{\r\nstruct intel_pt_queue *ptq = queue->priv;\r\nif (list_empty(&queue->head))\r\nreturn 0;\r\nif (!ptq) {\r\nptq = intel_pt_alloc_queue(pt, queue_nr);\r\nif (!ptq)\r\nreturn -ENOMEM;\r\nqueue->priv = ptq;\r\nif (queue->cpu != -1)\r\nptq->cpu = queue->cpu;\r\nptq->tid = queue->tid;\r\nif (pt->sampling_mode) {\r\nif (pt->timeless_decoding)\r\nptq->step_through_buffers = true;\r\nif (pt->timeless_decoding || !pt->have_sched_switch)\r\nptq->use_buffer_pid_tid = true;\r\n}\r\n}\r\nif (!ptq->on_heap &&\r\n(!pt->sync_switch ||\r\nptq->switch_state != INTEL_PT_SS_EXPECTING_SWITCH_EVENT)) {\r\nconst struct intel_pt_state *state;\r\nint ret;\r\nif (pt->timeless_decoding)\r\nreturn 0;\r\nintel_pt_log("queue %u getting timestamp\n", queue_nr);\r\nintel_pt_log("queue %u decoding cpu %d pid %d tid %d\n",\r\nqueue_nr, ptq->cpu, ptq->pid, ptq->tid);\r\nwhile (1) {\r\nstate = intel_pt_decode(ptq->decoder);\r\nif (state->err) {\r\nif (state->err == INTEL_PT_ERR_NODATA) {\r\nintel_pt_log("queue %u has no timestamp\n",\r\nqueue_nr);\r\nreturn 0;\r\n}\r\ncontinue;\r\n}\r\nif (state->timestamp)\r\nbreak;\r\n}\r\nptq->timestamp = state->timestamp;\r\nintel_pt_log("queue %u timestamp 0x%" PRIx64 "\n",\r\nqueue_nr, ptq->timestamp);\r\nptq->state = state;\r\nptq->have_sample = true;\r\nintel_pt_sample_flags(ptq);\r\nret = auxtrace_heap__add(&pt->heap, queue_nr, ptq->timestamp);\r\nif (ret)\r\nreturn ret;\r\nptq->on_heap = true;\r\n}\r\nreturn 0;\r\n}\r\nstatic int intel_pt_setup_queues(struct intel_pt *pt)\r\n{\r\nunsigned int i;\r\nint ret;\r\nfor (i = 0; i < pt->queues.nr_queues; i++) {\r\nret = intel_pt_setup_queue(pt, &pt->queues.queue_array[i], i);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void intel_pt_copy_last_branch_rb(struct intel_pt_queue *ptq)\r\n{\r\nstruct branch_stack *bs_src = ptq->last_branch_rb;\r\nstruct branch_stack *bs_dst = ptq->last_branch;\r\nsize_t nr = 0;\r\nbs_dst->nr = bs_src->nr;\r\nif (!bs_src->nr)\r\nreturn;\r\nnr = ptq->pt->synth_opts.last_branch_sz - ptq->last_branch_pos;\r\nmemcpy(&bs_dst->entries[0],\r\n&bs_src->entries[ptq->last_branch_pos],\r\nsizeof(struct branch_entry) * nr);\r\nif (bs_src->nr >= ptq->pt->synth_opts.last_branch_sz) {\r\nmemcpy(&bs_dst->entries[nr],\r\n&bs_src->entries[0],\r\nsizeof(struct branch_entry) * ptq->last_branch_pos);\r\n}\r\n}\r\nstatic inline void intel_pt_reset_last_branch_rb(struct intel_pt_queue *ptq)\r\n{\r\nptq->last_branch_pos = 0;\r\nptq->last_branch_rb->nr = 0;\r\n}\r\nstatic void intel_pt_update_last_branch_rb(struct intel_pt_queue *ptq)\r\n{\r\nconst struct intel_pt_state *state = ptq->state;\r\nstruct branch_stack *bs = ptq->last_branch_rb;\r\nstruct branch_entry *be;\r\nif (!ptq->last_branch_pos)\r\nptq->last_branch_pos = ptq->pt->synth_opts.last_branch_sz;\r\nptq->last_branch_pos -= 1;\r\nbe = &bs->entries[ptq->last_branch_pos];\r\nbe->from = state->from_ip;\r\nbe->to = state->to_ip;\r\nbe->flags.abort = !!(state->flags & INTEL_PT_ABORT_TX);\r\nbe->flags.in_tx = !!(state->flags & INTEL_PT_IN_TX);\r\nbe->flags.mispred = ptq->pt->mispred_all;\r\nif (bs->nr < ptq->pt->synth_opts.last_branch_sz)\r\nbs->nr += 1;\r\n}\r\nstatic inline bool intel_pt_skip_event(struct intel_pt *pt)\r\n{\r\nreturn pt->synth_opts.initial_skip &&\r\npt->num_events++ < pt->synth_opts.initial_skip;\r\n}\r\nstatic void intel_pt_prep_b_sample(struct intel_pt *pt,\r\nstruct intel_pt_queue *ptq,\r\nunion perf_event *event,\r\nstruct perf_sample *sample)\r\n{\r\nevent->sample.header.type = PERF_RECORD_SAMPLE;\r\nevent->sample.header.misc = PERF_RECORD_MISC_USER;\r\nevent->sample.header.size = sizeof(struct perf_event_header);\r\nif (!pt->timeless_decoding)\r\nsample->time = tsc_to_perf_time(ptq->timestamp, &pt->tc);\r\nsample->cpumode = PERF_RECORD_MISC_USER;\r\nsample->ip = ptq->state->from_ip;\r\nsample->pid = ptq->pid;\r\nsample->tid = ptq->tid;\r\nsample->addr = ptq->state->to_ip;\r\nsample->period = 1;\r\nsample->cpu = ptq->cpu;\r\nsample->flags = ptq->flags;\r\nsample->insn_len = ptq->insn_len;\r\nmemcpy(sample->insn, ptq->insn, INTEL_PT_INSN_BUF_SZ);\r\n}\r\nstatic int intel_pt_inject_event(union perf_event *event,\r\nstruct perf_sample *sample, u64 type,\r\nbool swapped)\r\n{\r\nevent->header.size = perf_event__sample_event_size(sample, type, 0);\r\nreturn perf_event__synthesize_sample(event, type, 0, sample, swapped);\r\n}\r\nstatic inline int intel_pt_opt_inject(struct intel_pt *pt,\r\nunion perf_event *event,\r\nstruct perf_sample *sample, u64 type)\r\n{\r\nif (!pt->synth_opts.inject)\r\nreturn 0;\r\nreturn intel_pt_inject_event(event, sample, type, pt->synth_needs_swap);\r\n}\r\nstatic int intel_pt_deliver_synth_b_event(struct intel_pt *pt,\r\nunion perf_event *event,\r\nstruct perf_sample *sample, u64 type)\r\n{\r\nint ret;\r\nret = intel_pt_opt_inject(pt, event, sample, type);\r\nif (ret)\r\nreturn ret;\r\nret = perf_session__deliver_synth_event(pt->session, event, sample);\r\nif (ret)\r\npr_err("Intel PT: failed to deliver event, error %d\n", ret);\r\nreturn ret;\r\n}\r\nstatic int intel_pt_synth_branch_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nstruct dummy_branch_stack {\r\nu64 nr;\r\nstruct branch_entry entries;\r\n} dummy_bs;\r\nif (pt->branches_filter && !(pt->branches_filter & ptq->flags))\r\nreturn 0;\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_b_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->branches_id;\r\nsample.stream_id = ptq->pt->branches_id;\r\nif (pt->synth_opts.last_branch && sort__mode == SORT_MODE__BRANCH) {\r\ndummy_bs = (struct dummy_branch_stack){\r\n.nr = 1,\r\n.entries = {\r\n.from = sample.ip,\r\n.to = sample.addr,\r\n},\r\n};\r\nsample.branch_stack = (struct branch_stack *)&dummy_bs;\r\n}\r\nreturn intel_pt_deliver_synth_b_event(pt, event, &sample,\r\npt->branches_sample_type);\r\n}\r\nstatic void intel_pt_prep_sample(struct intel_pt *pt,\r\nstruct intel_pt_queue *ptq,\r\nunion perf_event *event,\r\nstruct perf_sample *sample)\r\n{\r\nintel_pt_prep_b_sample(pt, ptq, event, sample);\r\nif (pt->synth_opts.callchain) {\r\nthread_stack__sample(ptq->thread, ptq->chain,\r\npt->synth_opts.callchain_sz, sample->ip);\r\nsample->callchain = ptq->chain;\r\n}\r\nif (pt->synth_opts.last_branch) {\r\nintel_pt_copy_last_branch_rb(ptq);\r\nsample->branch_stack = ptq->last_branch;\r\n}\r\n}\r\nstatic inline int intel_pt_deliver_synth_event(struct intel_pt *pt,\r\nstruct intel_pt_queue *ptq,\r\nunion perf_event *event,\r\nstruct perf_sample *sample,\r\nu64 type)\r\n{\r\nint ret;\r\nret = intel_pt_deliver_synth_b_event(pt, event, sample, type);\r\nif (pt->synth_opts.last_branch)\r\nintel_pt_reset_last_branch_rb(ptq);\r\nreturn ret;\r\n}\r\nstatic int intel_pt_synth_instruction_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->instructions_id;\r\nsample.stream_id = ptq->pt->instructions_id;\r\nsample.period = ptq->state->tot_insn_cnt - ptq->last_insn_cnt;\r\nptq->last_insn_cnt = ptq->state->tot_insn_cnt;\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->instructions_sample_type);\r\n}\r\nstatic int intel_pt_synth_transaction_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->transactions_id;\r\nsample.stream_id = ptq->pt->transactions_id;\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->transactions_sample_type);\r\n}\r\nstatic void intel_pt_prep_p_sample(struct intel_pt *pt,\r\nstruct intel_pt_queue *ptq,\r\nunion perf_event *event,\r\nstruct perf_sample *sample)\r\n{\r\nintel_pt_prep_sample(pt, ptq, event, sample);\r\nif (!sample->ip)\r\nsample->flags = 0;\r\n}\r\nstatic int intel_pt_synth_ptwrite_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nstruct perf_synth_intel_ptwrite raw;\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_p_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->ptwrites_id;\r\nsample.stream_id = ptq->pt->ptwrites_id;\r\nraw.flags = 0;\r\nraw.ip = !!(ptq->state->flags & INTEL_PT_FUP_IP);\r\nraw.payload = cpu_to_le64(ptq->state->ptw_payload);\r\nsample.raw_size = perf_synth__raw_size(raw);\r\nsample.raw_data = perf_synth__raw_data(&raw);\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->ptwrites_sample_type);\r\n}\r\nstatic int intel_pt_synth_cbr_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nstruct perf_synth_intel_cbr raw;\r\nu32 flags;\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_p_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->cbr_id;\r\nsample.stream_id = ptq->pt->cbr_id;\r\nflags = (u16)ptq->state->cbr_payload | (pt->max_non_turbo_ratio << 16);\r\nraw.flags = cpu_to_le32(flags);\r\nraw.freq = cpu_to_le32(raw.cbr * pt->cbr2khz);\r\nraw.reserved3 = 0;\r\nsample.raw_size = perf_synth__raw_size(raw);\r\nsample.raw_data = perf_synth__raw_data(&raw);\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->pwr_events_sample_type);\r\n}\r\nstatic int intel_pt_synth_mwait_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nstruct perf_synth_intel_mwait raw;\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_p_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->mwait_id;\r\nsample.stream_id = ptq->pt->mwait_id;\r\nraw.reserved = 0;\r\nraw.payload = cpu_to_le64(ptq->state->mwait_payload);\r\nsample.raw_size = perf_synth__raw_size(raw);\r\nsample.raw_data = perf_synth__raw_data(&raw);\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->pwr_events_sample_type);\r\n}\r\nstatic int intel_pt_synth_pwre_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nstruct perf_synth_intel_pwre raw;\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_p_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->pwre_id;\r\nsample.stream_id = ptq->pt->pwre_id;\r\nraw.reserved = 0;\r\nraw.payload = cpu_to_le64(ptq->state->pwre_payload);\r\nsample.raw_size = perf_synth__raw_size(raw);\r\nsample.raw_data = perf_synth__raw_data(&raw);\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->pwr_events_sample_type);\r\n}\r\nstatic int intel_pt_synth_exstop_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nstruct perf_synth_intel_exstop raw;\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_p_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->exstop_id;\r\nsample.stream_id = ptq->pt->exstop_id;\r\nraw.flags = 0;\r\nraw.ip = !!(ptq->state->flags & INTEL_PT_FUP_IP);\r\nsample.raw_size = perf_synth__raw_size(raw);\r\nsample.raw_data = perf_synth__raw_data(&raw);\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->pwr_events_sample_type);\r\n}\r\nstatic int intel_pt_synth_pwrx_sample(struct intel_pt_queue *ptq)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nunion perf_event *event = ptq->event_buf;\r\nstruct perf_sample sample = { .ip = 0, };\r\nstruct perf_synth_intel_pwrx raw;\r\nif (intel_pt_skip_event(pt))\r\nreturn 0;\r\nintel_pt_prep_p_sample(pt, ptq, event, &sample);\r\nsample.id = ptq->pt->pwrx_id;\r\nsample.stream_id = ptq->pt->pwrx_id;\r\nraw.reserved = 0;\r\nraw.payload = cpu_to_le64(ptq->state->pwrx_payload);\r\nsample.raw_size = perf_synth__raw_size(raw);\r\nsample.raw_data = perf_synth__raw_data(&raw);\r\nreturn intel_pt_deliver_synth_event(pt, ptq, event, &sample,\r\npt->pwr_events_sample_type);\r\n}\r\nstatic int intel_pt_synth_error(struct intel_pt *pt, int code, int cpu,\r\npid_t pid, pid_t tid, u64 ip)\r\n{\r\nunion perf_event event;\r\nchar msg[MAX_AUXTRACE_ERROR_MSG];\r\nint err;\r\nintel_pt__strerror(code, msg, MAX_AUXTRACE_ERROR_MSG);\r\nauxtrace_synth_error(&event.auxtrace_error, PERF_AUXTRACE_ERROR_ITRACE,\r\ncode, cpu, pid, tid, ip, msg);\r\nerr = perf_session__deliver_synth_event(pt->session, &event, NULL);\r\nif (err)\r\npr_err("Intel Processor Trace: failed to deliver error event, error %d\n",\r\nerr);\r\nreturn err;\r\n}\r\nstatic int intel_pt_next_tid(struct intel_pt *pt, struct intel_pt_queue *ptq)\r\n{\r\nstruct auxtrace_queue *queue;\r\npid_t tid = ptq->next_tid;\r\nint err;\r\nif (tid == -1)\r\nreturn 0;\r\nintel_pt_log("switch: cpu %d tid %d\n", ptq->cpu, tid);\r\nerr = machine__set_current_tid(pt->machine, ptq->cpu, -1, tid);\r\nqueue = &pt->queues.queue_array[ptq->queue_nr];\r\nintel_pt_set_pid_tid_cpu(pt, queue);\r\nptq->next_tid = -1;\r\nreturn err;\r\n}\r\nstatic inline bool intel_pt_is_switch_ip(struct intel_pt_queue *ptq, u64 ip)\r\n{\r\nstruct intel_pt *pt = ptq->pt;\r\nreturn ip == pt->switch_ip &&\r\n(ptq->flags & PERF_IP_FLAG_BRANCH) &&\r\n!(ptq->flags & (PERF_IP_FLAG_CONDITIONAL | PERF_IP_FLAG_ASYNC |\r\nPERF_IP_FLAG_INTERRUPT | PERF_IP_FLAG_TX_ABORT));\r\n}\r\nstatic int intel_pt_sample(struct intel_pt_queue *ptq)\r\n{\r\nconst struct intel_pt_state *state = ptq->state;\r\nstruct intel_pt *pt = ptq->pt;\r\nint err;\r\nif (!ptq->have_sample)\r\nreturn 0;\r\nptq->have_sample = false;\r\nif (pt->sample_pwr_events && (state->type & INTEL_PT_PWR_EVT)) {\r\nif (state->type & INTEL_PT_CBR_CHG) {\r\nerr = intel_pt_synth_cbr_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (state->type & INTEL_PT_MWAIT_OP) {\r\nerr = intel_pt_synth_mwait_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (state->type & INTEL_PT_PWR_ENTRY) {\r\nerr = intel_pt_synth_pwre_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (state->type & INTEL_PT_EX_STOP) {\r\nerr = intel_pt_synth_exstop_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (state->type & INTEL_PT_PWR_EXIT) {\r\nerr = intel_pt_synth_pwrx_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\n}\r\nif (pt->sample_instructions && (state->type & INTEL_PT_INSTRUCTION)) {\r\nerr = intel_pt_synth_instruction_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (pt->sample_transactions && (state->type & INTEL_PT_TRANSACTION)) {\r\nerr = intel_pt_synth_transaction_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (pt->sample_ptwrites && (state->type & INTEL_PT_PTW)) {\r\nerr = intel_pt_synth_ptwrite_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (!(state->type & INTEL_PT_BRANCH))\r\nreturn 0;\r\nif (pt->synth_opts.callchain || pt->synth_opts.thread_stack)\r\nthread_stack__event(ptq->thread, ptq->flags, state->from_ip,\r\nstate->to_ip, ptq->insn_len,\r\nstate->trace_nr);\r\nelse\r\nthread_stack__set_trace_nr(ptq->thread, state->trace_nr);\r\nif (pt->sample_branches) {\r\nerr = intel_pt_synth_branch_sample(ptq);\r\nif (err)\r\nreturn err;\r\n}\r\nif (pt->synth_opts.last_branch)\r\nintel_pt_update_last_branch_rb(ptq);\r\nif (!pt->sync_switch)\r\nreturn 0;\r\nif (intel_pt_is_switch_ip(ptq, state->to_ip)) {\r\nswitch (ptq->switch_state) {\r\ncase INTEL_PT_SS_UNKNOWN:\r\ncase INTEL_PT_SS_EXPECTING_SWITCH_IP:\r\nerr = intel_pt_next_tid(pt, ptq);\r\nif (err)\r\nreturn err;\r\nptq->switch_state = INTEL_PT_SS_TRACING;\r\nbreak;\r\ndefault:\r\nptq->switch_state = INTEL_PT_SS_EXPECTING_SWITCH_EVENT;\r\nreturn 1;\r\n}\r\n} else if (!state->to_ip) {\r\nptq->switch_state = INTEL_PT_SS_NOT_TRACING;\r\n} else if (ptq->switch_state == INTEL_PT_SS_NOT_TRACING) {\r\nptq->switch_state = INTEL_PT_SS_UNKNOWN;\r\n} else if (ptq->switch_state == INTEL_PT_SS_UNKNOWN &&\r\nstate->to_ip == pt->ptss_ip &&\r\n(ptq->flags & PERF_IP_FLAG_CALL)) {\r\nptq->switch_state = INTEL_PT_SS_TRACING;\r\n}\r\nreturn 0;\r\n}\r\nstatic u64 intel_pt_switch_ip(struct intel_pt *pt, u64 *ptss_ip)\r\n{\r\nstruct machine *machine = pt->machine;\r\nstruct map *map;\r\nstruct symbol *sym, *start;\r\nu64 ip, switch_ip = 0;\r\nconst char *ptss;\r\nif (ptss_ip)\r\n*ptss_ip = 0;\r\nmap = machine__kernel_map(machine);\r\nif (!map)\r\nreturn 0;\r\nif (map__load(map))\r\nreturn 0;\r\nstart = dso__first_symbol(map->dso, MAP__FUNCTION);\r\nfor (sym = start; sym; sym = dso__next_symbol(sym)) {\r\nif (sym->binding == STB_GLOBAL &&\r\n!strcmp(sym->name, "__switch_to")) {\r\nip = map->unmap_ip(map, sym->start);\r\nif (ip >= map->start && ip < map->end) {\r\nswitch_ip = ip;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (!switch_ip || !ptss_ip)\r\nreturn 0;\r\nif (pt->have_sched_switch == 1)\r\nptss = "perf_trace_sched_switch";\r\nelse\r\nptss = "__perf_event_task_sched_out";\r\nfor (sym = start; sym; sym = dso__next_symbol(sym)) {\r\nif (!strcmp(sym->name, ptss)) {\r\nip = map->unmap_ip(map, sym->start);\r\nif (ip >= map->start && ip < map->end) {\r\n*ptss_ip = ip;\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn switch_ip;\r\n}\r\nstatic int intel_pt_run_decoder(struct intel_pt_queue *ptq, u64 *timestamp)\r\n{\r\nconst struct intel_pt_state *state = ptq->state;\r\nstruct intel_pt *pt = ptq->pt;\r\nint err;\r\nif (!pt->kernel_start) {\r\npt->kernel_start = machine__kernel_start(pt->machine);\r\nif (pt->per_cpu_mmaps &&\r\n(pt->have_sched_switch == 1 || pt->have_sched_switch == 3) &&\r\n!pt->timeless_decoding && intel_pt_tracing_kernel(pt) &&\r\n!pt->sampling_mode) {\r\npt->switch_ip = intel_pt_switch_ip(pt, &pt->ptss_ip);\r\nif (pt->switch_ip) {\r\nintel_pt_log("switch_ip: %"PRIx64" ptss_ip: %"PRIx64"\n",\r\npt->switch_ip, pt->ptss_ip);\r\npt->sync_switch = true;\r\n}\r\n}\r\n}\r\nintel_pt_log("queue %u decoding cpu %d pid %d tid %d\n",\r\nptq->queue_nr, ptq->cpu, ptq->pid, ptq->tid);\r\nwhile (1) {\r\nerr = intel_pt_sample(ptq);\r\nif (err)\r\nreturn err;\r\nstate = intel_pt_decode(ptq->decoder);\r\nif (state->err) {\r\nif (state->err == INTEL_PT_ERR_NODATA)\r\nreturn 1;\r\nif (pt->sync_switch &&\r\nstate->from_ip >= pt->kernel_start) {\r\npt->sync_switch = false;\r\nintel_pt_next_tid(pt, ptq);\r\n}\r\nif (pt->synth_opts.errors) {\r\nerr = intel_pt_synth_error(pt, state->err,\r\nptq->cpu, ptq->pid,\r\nptq->tid,\r\nstate->from_ip);\r\nif (err)\r\nreturn err;\r\n}\r\ncontinue;\r\n}\r\nptq->state = state;\r\nptq->have_sample = true;\r\nintel_pt_sample_flags(ptq);\r\nif (pt->est_tsc &&\r\n(state->from_ip >= pt->kernel_start || !state->from_ip) &&\r\nstate->to_ip && state->to_ip < pt->kernel_start) {\r\nintel_pt_log("TSC %"PRIx64" est. TSC %"PRIx64"\n",\r\nstate->timestamp, state->est_timestamp);\r\nptq->timestamp = state->est_timestamp;\r\n} else if (pt->sync_switch &&\r\nptq->switch_state == INTEL_PT_SS_UNKNOWN &&\r\nintel_pt_is_switch_ip(ptq, state->to_ip) &&\r\nptq->next_tid == -1) {\r\nintel_pt_log("TSC %"PRIx64" est. TSC %"PRIx64"\n",\r\nstate->timestamp, state->est_timestamp);\r\nptq->timestamp = state->est_timestamp;\r\n} else if (state->timestamp > ptq->timestamp) {\r\nptq->timestamp = state->timestamp;\r\n}\r\nif (!pt->timeless_decoding && ptq->timestamp >= *timestamp) {\r\n*timestamp = ptq->timestamp;\r\nreturn 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int intel_pt_update_queues(struct intel_pt *pt)\r\n{\r\nif (pt->queues.new_data) {\r\npt->queues.new_data = false;\r\nreturn intel_pt_setup_queues(pt);\r\n}\r\nreturn 0;\r\n}\r\nstatic int intel_pt_process_queues(struct intel_pt *pt, u64 timestamp)\r\n{\r\nunsigned int queue_nr;\r\nu64 ts;\r\nint ret;\r\nwhile (1) {\r\nstruct auxtrace_queue *queue;\r\nstruct intel_pt_queue *ptq;\r\nif (!pt->heap.heap_cnt)\r\nreturn 0;\r\nif (pt->heap.heap_array[0].ordinal >= timestamp)\r\nreturn 0;\r\nqueue_nr = pt->heap.heap_array[0].queue_nr;\r\nqueue = &pt->queues.queue_array[queue_nr];\r\nptq = queue->priv;\r\nintel_pt_log("queue %u processing 0x%" PRIx64 " to 0x%" PRIx64 "\n",\r\nqueue_nr, pt->heap.heap_array[0].ordinal,\r\ntimestamp);\r\nauxtrace_heap__pop(&pt->heap);\r\nif (pt->heap.heap_cnt) {\r\nts = pt->heap.heap_array[0].ordinal + 1;\r\nif (ts > timestamp)\r\nts = timestamp;\r\n} else {\r\nts = timestamp;\r\n}\r\nintel_pt_set_pid_tid_cpu(pt, queue);\r\nret = intel_pt_run_decoder(ptq, &ts);\r\nif (ret < 0) {\r\nauxtrace_heap__add(&pt->heap, queue_nr, ts);\r\nreturn ret;\r\n}\r\nif (!ret) {\r\nret = auxtrace_heap__add(&pt->heap, queue_nr, ts);\r\nif (ret < 0)\r\nreturn ret;\r\n} else {\r\nptq->on_heap = false;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int intel_pt_process_timeless_queues(struct intel_pt *pt, pid_t tid,\r\nu64 time_)\r\n{\r\nstruct auxtrace_queues *queues = &pt->queues;\r\nunsigned int i;\r\nu64 ts = 0;\r\nfor (i = 0; i < queues->nr_queues; i++) {\r\nstruct auxtrace_queue *queue = &pt->queues.queue_array[i];\r\nstruct intel_pt_queue *ptq = queue->priv;\r\nif (ptq && (tid == -1 || ptq->tid == tid)) {\r\nptq->time = time_;\r\nintel_pt_set_pid_tid_cpu(pt, queue);\r\nintel_pt_run_decoder(ptq, &ts);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int intel_pt_lost(struct intel_pt *pt, struct perf_sample *sample)\r\n{\r\nreturn intel_pt_synth_error(pt, INTEL_PT_ERR_LOST, sample->cpu,\r\nsample->pid, sample->tid, 0);\r\n}\r\nstatic struct intel_pt_queue *intel_pt_cpu_to_ptq(struct intel_pt *pt, int cpu)\r\n{\r\nunsigned i, j;\r\nif (cpu < 0 || !pt->queues.nr_queues)\r\nreturn NULL;\r\nif ((unsigned)cpu >= pt->queues.nr_queues)\r\ni = pt->queues.nr_queues - 1;\r\nelse\r\ni = cpu;\r\nif (pt->queues.queue_array[i].cpu == cpu)\r\nreturn pt->queues.queue_array[i].priv;\r\nfor (j = 0; i > 0; j++) {\r\nif (pt->queues.queue_array[--i].cpu == cpu)\r\nreturn pt->queues.queue_array[i].priv;\r\n}\r\nfor (; j < pt->queues.nr_queues; j++) {\r\nif (pt->queues.queue_array[j].cpu == cpu)\r\nreturn pt->queues.queue_array[j].priv;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int intel_pt_sync_switch(struct intel_pt *pt, int cpu, pid_t tid,\r\nu64 timestamp)\r\n{\r\nstruct intel_pt_queue *ptq;\r\nint err;\r\nif (!pt->sync_switch)\r\nreturn 1;\r\nptq = intel_pt_cpu_to_ptq(pt, cpu);\r\nif (!ptq)\r\nreturn 1;\r\nswitch (ptq->switch_state) {\r\ncase INTEL_PT_SS_NOT_TRACING:\r\nptq->next_tid = -1;\r\nbreak;\r\ncase INTEL_PT_SS_UNKNOWN:\r\ncase INTEL_PT_SS_TRACING:\r\nptq->next_tid = tid;\r\nptq->switch_state = INTEL_PT_SS_EXPECTING_SWITCH_IP;\r\nreturn 0;\r\ncase INTEL_PT_SS_EXPECTING_SWITCH_EVENT:\r\nif (!ptq->on_heap) {\r\nptq->timestamp = perf_time_to_tsc(timestamp,\r\n&pt->tc);\r\nerr = auxtrace_heap__add(&pt->heap, ptq->queue_nr,\r\nptq->timestamp);\r\nif (err)\r\nreturn err;\r\nptq->on_heap = true;\r\n}\r\nptq->switch_state = INTEL_PT_SS_TRACING;\r\nbreak;\r\ncase INTEL_PT_SS_EXPECTING_SWITCH_IP:\r\nptq->next_tid = tid;\r\nintel_pt_log("ERROR: cpu %d expecting switch ip\n", cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 1;\r\n}\r\nstatic int intel_pt_process_switch(struct intel_pt *pt,\r\nstruct perf_sample *sample)\r\n{\r\nstruct perf_evsel *evsel;\r\npid_t tid;\r\nint cpu, ret;\r\nevsel = perf_evlist__id2evsel(pt->session->evlist, sample->id);\r\nif (evsel != pt->switch_evsel)\r\nreturn 0;\r\ntid = perf_evsel__intval(evsel, sample, "next_pid");\r\ncpu = sample->cpu;\r\nintel_pt_log("sched_switch: cpu %d tid %d time %"PRIu64" tsc %#"PRIx64"\n",\r\ncpu, tid, sample->time, perf_time_to_tsc(sample->time,\r\n&pt->tc));\r\nret = intel_pt_sync_switch(pt, cpu, tid, sample->time);\r\nif (ret <= 0)\r\nreturn ret;\r\nreturn machine__set_current_tid(pt->machine, cpu, -1, tid);\r\n}\r\nstatic int intel_pt_context_switch(struct intel_pt *pt, union perf_event *event,\r\nstruct perf_sample *sample)\r\n{\r\nbool out = event->header.misc & PERF_RECORD_MISC_SWITCH_OUT;\r\npid_t pid, tid;\r\nint cpu, ret;\r\ncpu = sample->cpu;\r\nif (pt->have_sched_switch == 3) {\r\nif (!out)\r\nreturn 0;\r\nif (event->header.type != PERF_RECORD_SWITCH_CPU_WIDE) {\r\npr_err("Expecting CPU-wide context switch event\n");\r\nreturn -EINVAL;\r\n}\r\npid = event->context_switch.next_prev_pid;\r\ntid = event->context_switch.next_prev_tid;\r\n} else {\r\nif (out)\r\nreturn 0;\r\npid = sample->pid;\r\ntid = sample->tid;\r\n}\r\nif (tid == -1) {\r\npr_err("context_switch event has no tid\n");\r\nreturn -EINVAL;\r\n}\r\nintel_pt_log("context_switch: cpu %d pid %d tid %d time %"PRIu64" tsc %#"PRIx64"\n",\r\ncpu, pid, tid, sample->time, perf_time_to_tsc(sample->time,\r\n&pt->tc));\r\nret = intel_pt_sync_switch(pt, cpu, tid, sample->time);\r\nif (ret <= 0)\r\nreturn ret;\r\nreturn machine__set_current_tid(pt->machine, cpu, pid, tid);\r\n}\r\nstatic int intel_pt_process_itrace_start(struct intel_pt *pt,\r\nunion perf_event *event,\r\nstruct perf_sample *sample)\r\n{\r\nif (!pt->per_cpu_mmaps)\r\nreturn 0;\r\nintel_pt_log("itrace_start: cpu %d pid %d tid %d time %"PRIu64" tsc %#"PRIx64"\n",\r\nsample->cpu, event->itrace_start.pid,\r\nevent->itrace_start.tid, sample->time,\r\nperf_time_to_tsc(sample->time, &pt->tc));\r\nreturn machine__set_current_tid(pt->machine, sample->cpu,\r\nevent->itrace_start.pid,\r\nevent->itrace_start.tid);\r\n}\r\nstatic int intel_pt_process_event(struct perf_session *session,\r\nunion perf_event *event,\r\nstruct perf_sample *sample,\r\nstruct perf_tool *tool)\r\n{\r\nstruct intel_pt *pt = container_of(session->auxtrace, struct intel_pt,\r\nauxtrace);\r\nu64 timestamp;\r\nint err = 0;\r\nif (dump_trace)\r\nreturn 0;\r\nif (!tool->ordered_events) {\r\npr_err("Intel Processor Trace requires ordered events\n");\r\nreturn -EINVAL;\r\n}\r\nif (sample->time && sample->time != (u64)-1)\r\ntimestamp = perf_time_to_tsc(sample->time, &pt->tc);\r\nelse\r\ntimestamp = 0;\r\nif (timestamp || pt->timeless_decoding) {\r\nerr = intel_pt_update_queues(pt);\r\nif (err)\r\nreturn err;\r\n}\r\nif (pt->timeless_decoding) {\r\nif (event->header.type == PERF_RECORD_EXIT) {\r\nerr = intel_pt_process_timeless_queues(pt,\r\nevent->fork.tid,\r\nsample->time);\r\n}\r\n} else if (timestamp) {\r\nerr = intel_pt_process_queues(pt, timestamp);\r\n}\r\nif (err)\r\nreturn err;\r\nif (event->header.type == PERF_RECORD_AUX &&\r\n(event->aux.flags & PERF_AUX_FLAG_TRUNCATED) &&\r\npt->synth_opts.errors) {\r\nerr = intel_pt_lost(pt, sample);\r\nif (err)\r\nreturn err;\r\n}\r\nif (pt->switch_evsel && event->header.type == PERF_RECORD_SAMPLE)\r\nerr = intel_pt_process_switch(pt, sample);\r\nelse if (event->header.type == PERF_RECORD_ITRACE_START)\r\nerr = intel_pt_process_itrace_start(pt, event, sample);\r\nelse if (event->header.type == PERF_RECORD_SWITCH ||\r\nevent->header.type == PERF_RECORD_SWITCH_CPU_WIDE)\r\nerr = intel_pt_context_switch(pt, event, sample);\r\nintel_pt_log("event %s (%u): cpu %d time %"PRIu64" tsc %#"PRIx64"\n",\r\nperf_event__name(event->header.type), event->header.type,\r\nsample->cpu, sample->time, timestamp);\r\nreturn err;\r\n}\r\nstatic int intel_pt_flush(struct perf_session *session, struct perf_tool *tool)\r\n{\r\nstruct intel_pt *pt = container_of(session->auxtrace, struct intel_pt,\r\nauxtrace);\r\nint ret;\r\nif (dump_trace)\r\nreturn 0;\r\nif (!tool->ordered_events)\r\nreturn -EINVAL;\r\nret = intel_pt_update_queues(pt);\r\nif (ret < 0)\r\nreturn ret;\r\nif (pt->timeless_decoding)\r\nreturn intel_pt_process_timeless_queues(pt, -1,\r\nMAX_TIMESTAMP - 1);\r\nreturn intel_pt_process_queues(pt, MAX_TIMESTAMP);\r\n}\r\nstatic void intel_pt_free_events(struct perf_session *session)\r\n{\r\nstruct intel_pt *pt = container_of(session->auxtrace, struct intel_pt,\r\nauxtrace);\r\nstruct auxtrace_queues *queues = &pt->queues;\r\nunsigned int i;\r\nfor (i = 0; i < queues->nr_queues; i++) {\r\nintel_pt_free_queue(queues->queue_array[i].priv);\r\nqueues->queue_array[i].priv = NULL;\r\n}\r\nintel_pt_log_disable();\r\nauxtrace_queues__free(queues);\r\n}\r\nstatic void intel_pt_free(struct perf_session *session)\r\n{\r\nstruct intel_pt *pt = container_of(session->auxtrace, struct intel_pt,\r\nauxtrace);\r\nauxtrace_heap__free(&pt->heap);\r\nintel_pt_free_events(session);\r\nsession->auxtrace = NULL;\r\nthread__put(pt->unknown_thread);\r\naddr_filters__exit(&pt->filts);\r\nzfree(&pt->filter);\r\nfree(pt);\r\n}\r\nstatic int intel_pt_process_auxtrace_event(struct perf_session *session,\r\nunion perf_event *event,\r\nstruct perf_tool *tool __maybe_unused)\r\n{\r\nstruct intel_pt *pt = container_of(session->auxtrace, struct intel_pt,\r\nauxtrace);\r\nif (pt->sampling_mode)\r\nreturn 0;\r\nif (!pt->data_queued) {\r\nstruct auxtrace_buffer *buffer;\r\noff_t data_offset;\r\nint fd = perf_data_file__fd(session->file);\r\nint err;\r\nif (perf_data_file__is_pipe(session->file)) {\r\ndata_offset = 0;\r\n} else {\r\ndata_offset = lseek(fd, 0, SEEK_CUR);\r\nif (data_offset == -1)\r\nreturn -errno;\r\n}\r\nerr = auxtrace_queues__add_event(&pt->queues, session, event,\r\ndata_offset, &buffer);\r\nif (err)\r\nreturn err;\r\nif (dump_trace) {\r\nif (auxtrace_buffer__get_data(buffer, fd)) {\r\nintel_pt_dump_event(pt, buffer->data,\r\nbuffer->size);\r\nauxtrace_buffer__put_data(buffer);\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int intel_pt_event_synth(struct perf_tool *tool,\r\nunion perf_event *event,\r\nstruct perf_sample *sample __maybe_unused,\r\nstruct machine *machine __maybe_unused)\r\n{\r\nstruct intel_pt_synth *intel_pt_synth =\r\ncontainer_of(tool, struct intel_pt_synth, dummy_tool);\r\nreturn perf_session__deliver_synth_event(intel_pt_synth->session, event,\r\nNULL);\r\n}\r\nstatic int intel_pt_synth_event(struct perf_session *session, const char *name,\r\nstruct perf_event_attr *attr, u64 id)\r\n{\r\nstruct intel_pt_synth intel_pt_synth;\r\nint err;\r\npr_debug("Synthesizing '%s' event with id %" PRIu64 " sample type %#" PRIx64 "\n",\r\nname, id, (u64)attr->sample_type);\r\nmemset(&intel_pt_synth, 0, sizeof(struct intel_pt_synth));\r\nintel_pt_synth.session = session;\r\nerr = perf_event__synthesize_attr(&intel_pt_synth.dummy_tool, attr, 1,\r\n&id, intel_pt_event_synth);\r\nif (err)\r\npr_err("%s: failed to synthesize '%s' event type\n",\r\n__func__, name);\r\nreturn err;\r\n}\r\nstatic void intel_pt_set_event_name(struct perf_evlist *evlist, u64 id,\r\nconst char *name)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each_entry(evlist, evsel) {\r\nif (evsel->id && evsel->id[0] == id) {\r\nif (evsel->name)\r\nzfree(&evsel->name);\r\nevsel->name = strdup(name);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic struct perf_evsel *intel_pt_evsel(struct intel_pt *pt,\r\nstruct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each_entry(evlist, evsel) {\r\nif (evsel->attr.type == pt->pmu_type && evsel->ids)\r\nreturn evsel;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int intel_pt_synth_events(struct intel_pt *pt,\r\nstruct perf_session *session)\r\n{\r\nstruct perf_evlist *evlist = session->evlist;\r\nstruct perf_evsel *evsel = intel_pt_evsel(pt, evlist);\r\nstruct perf_event_attr attr;\r\nu64 id;\r\nint err;\r\nif (!evsel) {\r\npr_debug("There are no selected events with Intel Processor Trace data\n");\r\nreturn 0;\r\n}\r\nmemset(&attr, 0, sizeof(struct perf_event_attr));\r\nattr.size = sizeof(struct perf_event_attr);\r\nattr.type = PERF_TYPE_HARDWARE;\r\nattr.sample_type = evsel->attr.sample_type & PERF_SAMPLE_MASK;\r\nattr.sample_type |= PERF_SAMPLE_IP | PERF_SAMPLE_TID |\r\nPERF_SAMPLE_PERIOD;\r\nif (pt->timeless_decoding)\r\nattr.sample_type &= ~(u64)PERF_SAMPLE_TIME;\r\nelse\r\nattr.sample_type |= PERF_SAMPLE_TIME;\r\nif (!pt->per_cpu_mmaps)\r\nattr.sample_type &= ~(u64)PERF_SAMPLE_CPU;\r\nattr.exclude_user = evsel->attr.exclude_user;\r\nattr.exclude_kernel = evsel->attr.exclude_kernel;\r\nattr.exclude_hv = evsel->attr.exclude_hv;\r\nattr.exclude_host = evsel->attr.exclude_host;\r\nattr.exclude_guest = evsel->attr.exclude_guest;\r\nattr.sample_id_all = evsel->attr.sample_id_all;\r\nattr.read_format = evsel->attr.read_format;\r\nid = evsel->id[0] + 1000000000;\r\nif (!id)\r\nid = 1;\r\nif (pt->synth_opts.branches) {\r\nattr.config = PERF_COUNT_HW_BRANCH_INSTRUCTIONS;\r\nattr.sample_period = 1;\r\nattr.sample_type |= PERF_SAMPLE_ADDR;\r\nerr = intel_pt_synth_event(session, "branches", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->sample_branches = true;\r\npt->branches_sample_type = attr.sample_type;\r\npt->branches_id = id;\r\nid += 1;\r\nattr.sample_type &= ~(u64)PERF_SAMPLE_ADDR;\r\n}\r\nif (pt->synth_opts.callchain)\r\nattr.sample_type |= PERF_SAMPLE_CALLCHAIN;\r\nif (pt->synth_opts.last_branch)\r\nattr.sample_type |= PERF_SAMPLE_BRANCH_STACK;\r\nif (pt->synth_opts.instructions) {\r\nattr.config = PERF_COUNT_HW_INSTRUCTIONS;\r\nif (pt->synth_opts.period_type == PERF_ITRACE_PERIOD_NANOSECS)\r\nattr.sample_period =\r\nintel_pt_ns_to_ticks(pt, pt->synth_opts.period);\r\nelse\r\nattr.sample_period = pt->synth_opts.period;\r\nerr = intel_pt_synth_event(session, "instructions", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->sample_instructions = true;\r\npt->instructions_sample_type = attr.sample_type;\r\npt->instructions_id = id;\r\nid += 1;\r\n}\r\nattr.sample_type &= ~(u64)PERF_SAMPLE_PERIOD;\r\nattr.sample_period = 1;\r\nif (pt->synth_opts.transactions) {\r\nattr.config = PERF_COUNT_HW_INSTRUCTIONS;\r\nerr = intel_pt_synth_event(session, "transactions", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->sample_transactions = true;\r\npt->transactions_sample_type = attr.sample_type;\r\npt->transactions_id = id;\r\nintel_pt_set_event_name(evlist, id, "transactions");\r\nid += 1;\r\n}\r\nattr.type = PERF_TYPE_SYNTH;\r\nattr.sample_type |= PERF_SAMPLE_RAW;\r\nif (pt->synth_opts.ptwrites) {\r\nattr.config = PERF_SYNTH_INTEL_PTWRITE;\r\nerr = intel_pt_synth_event(session, "ptwrite", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->sample_ptwrites = true;\r\npt->ptwrites_sample_type = attr.sample_type;\r\npt->ptwrites_id = id;\r\nintel_pt_set_event_name(evlist, id, "ptwrite");\r\nid += 1;\r\n}\r\nif (pt->synth_opts.pwr_events) {\r\npt->sample_pwr_events = true;\r\npt->pwr_events_sample_type = attr.sample_type;\r\nattr.config = PERF_SYNTH_INTEL_CBR;\r\nerr = intel_pt_synth_event(session, "cbr", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->cbr_id = id;\r\nintel_pt_set_event_name(evlist, id, "cbr");\r\nid += 1;\r\n}\r\nif (pt->synth_opts.pwr_events && (evsel->attr.config & 0x10)) {\r\nattr.config = PERF_SYNTH_INTEL_MWAIT;\r\nerr = intel_pt_synth_event(session, "mwait", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->mwait_id = id;\r\nintel_pt_set_event_name(evlist, id, "mwait");\r\nid += 1;\r\nattr.config = PERF_SYNTH_INTEL_PWRE;\r\nerr = intel_pt_synth_event(session, "pwre", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->pwre_id = id;\r\nintel_pt_set_event_name(evlist, id, "pwre");\r\nid += 1;\r\nattr.config = PERF_SYNTH_INTEL_EXSTOP;\r\nerr = intel_pt_synth_event(session, "exstop", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->exstop_id = id;\r\nintel_pt_set_event_name(evlist, id, "exstop");\r\nid += 1;\r\nattr.config = PERF_SYNTH_INTEL_PWRX;\r\nerr = intel_pt_synth_event(session, "pwrx", &attr, id);\r\nif (err)\r\nreturn err;\r\npt->pwrx_id = id;\r\nintel_pt_set_event_name(evlist, id, "pwrx");\r\nid += 1;\r\n}\r\npt->synth_needs_swap = evsel->needs_swap;\r\nreturn 0;\r\n}\r\nstatic struct perf_evsel *intel_pt_find_sched_switch(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each_entry_reverse(evlist, evsel) {\r\nconst char *name = perf_evsel__name(evsel);\r\nif (!strcmp(name, "sched:sched_switch"))\r\nreturn evsel;\r\n}\r\nreturn NULL;\r\n}\r\nstatic bool intel_pt_find_switch(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each_entry(evlist, evsel) {\r\nif (evsel->attr.context_switch)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int intel_pt_perf_config(const char *var, const char *value, void *data)\r\n{\r\nstruct intel_pt *pt = data;\r\nif (!strcmp(var, "intel-pt.mispred-all"))\r\npt->mispred_all = perf_config_bool(var, value);\r\nreturn 0;\r\n}\r\nstatic void intel_pt_print_info(u64 *arr, int start, int finish)\r\n{\r\nint i;\r\nif (!dump_trace)\r\nreturn;\r\nfor (i = start; i <= finish; i++)\r\nfprintf(stdout, intel_pt_info_fmts[i], arr[i]);\r\n}\r\nstatic void intel_pt_print_info_str(const char *name, const char *str)\r\n{\r\nif (!dump_trace)\r\nreturn;\r\nfprintf(stdout, " %-20s%s\n", name, str ? str : "");\r\n}\r\nstatic bool intel_pt_has(struct auxtrace_info_event *auxtrace_info, int pos)\r\n{\r\nreturn auxtrace_info->header.size >=\r\nsizeof(struct auxtrace_info_event) + (sizeof(u64) * (pos + 1));\r\n}\r\nint intel_pt_process_auxtrace_info(union perf_event *event,\r\nstruct perf_session *session)\r\n{\r\nstruct auxtrace_info_event *auxtrace_info = &event->auxtrace_info;\r\nsize_t min_sz = sizeof(u64) * INTEL_PT_PER_CPU_MMAPS;\r\nstruct intel_pt *pt;\r\nvoid *info_end;\r\nu64 *info;\r\nint err;\r\nif (auxtrace_info->header.size < sizeof(struct auxtrace_info_event) +\r\nmin_sz)\r\nreturn -EINVAL;\r\npt = zalloc(sizeof(struct intel_pt));\r\nif (!pt)\r\nreturn -ENOMEM;\r\naddr_filters__init(&pt->filts);\r\nerr = perf_config(intel_pt_perf_config, pt);\r\nif (err)\r\ngoto err_free;\r\nerr = auxtrace_queues__init(&pt->queues);\r\nif (err)\r\ngoto err_free;\r\nintel_pt_log_set_name(INTEL_PT_PMU_NAME);\r\npt->session = session;\r\npt->machine = &session->machines.host;\r\npt->auxtrace_type = auxtrace_info->type;\r\npt->pmu_type = auxtrace_info->priv[INTEL_PT_PMU_TYPE];\r\npt->tc.time_shift = auxtrace_info->priv[INTEL_PT_TIME_SHIFT];\r\npt->tc.time_mult = auxtrace_info->priv[INTEL_PT_TIME_MULT];\r\npt->tc.time_zero = auxtrace_info->priv[INTEL_PT_TIME_ZERO];\r\npt->cap_user_time_zero = auxtrace_info->priv[INTEL_PT_CAP_USER_TIME_ZERO];\r\npt->tsc_bit = auxtrace_info->priv[INTEL_PT_TSC_BIT];\r\npt->noretcomp_bit = auxtrace_info->priv[INTEL_PT_NORETCOMP_BIT];\r\npt->have_sched_switch = auxtrace_info->priv[INTEL_PT_HAVE_SCHED_SWITCH];\r\npt->snapshot_mode = auxtrace_info->priv[INTEL_PT_SNAPSHOT_MODE];\r\npt->per_cpu_mmaps = auxtrace_info->priv[INTEL_PT_PER_CPU_MMAPS];\r\nintel_pt_print_info(&auxtrace_info->priv[0], INTEL_PT_PMU_TYPE,\r\nINTEL_PT_PER_CPU_MMAPS);\r\nif (intel_pt_has(auxtrace_info, INTEL_PT_CYC_BIT)) {\r\npt->mtc_bit = auxtrace_info->priv[INTEL_PT_MTC_BIT];\r\npt->mtc_freq_bits = auxtrace_info->priv[INTEL_PT_MTC_FREQ_BITS];\r\npt->tsc_ctc_ratio_n = auxtrace_info->priv[INTEL_PT_TSC_CTC_N];\r\npt->tsc_ctc_ratio_d = auxtrace_info->priv[INTEL_PT_TSC_CTC_D];\r\npt->cyc_bit = auxtrace_info->priv[INTEL_PT_CYC_BIT];\r\nintel_pt_print_info(&auxtrace_info->priv[0], INTEL_PT_MTC_BIT,\r\nINTEL_PT_CYC_BIT);\r\n}\r\nif (intel_pt_has(auxtrace_info, INTEL_PT_MAX_NONTURBO_RATIO)) {\r\npt->max_non_turbo_ratio =\r\nauxtrace_info->priv[INTEL_PT_MAX_NONTURBO_RATIO];\r\nintel_pt_print_info(&auxtrace_info->priv[0],\r\nINTEL_PT_MAX_NONTURBO_RATIO,\r\nINTEL_PT_MAX_NONTURBO_RATIO);\r\n}\r\ninfo = &auxtrace_info->priv[INTEL_PT_FILTER_STR_LEN] + 1;\r\ninfo_end = (void *)info + auxtrace_info->header.size;\r\nif (intel_pt_has(auxtrace_info, INTEL_PT_FILTER_STR_LEN)) {\r\nsize_t len;\r\nlen = auxtrace_info->priv[INTEL_PT_FILTER_STR_LEN];\r\nintel_pt_print_info(&auxtrace_info->priv[0],\r\nINTEL_PT_FILTER_STR_LEN,\r\nINTEL_PT_FILTER_STR_LEN);\r\nif (len) {\r\nconst char *filter = (const char *)info;\r\nlen = roundup(len + 1, 8);\r\ninfo += len >> 3;\r\nif ((void *)info > info_end) {\r\npr_err("%s: bad filter string length\n", __func__);\r\nerr = -EINVAL;\r\ngoto err_free_queues;\r\n}\r\npt->filter = memdup(filter, len);\r\nif (!pt->filter) {\r\nerr = -ENOMEM;\r\ngoto err_free_queues;\r\n}\r\nif (session->header.needs_swap)\r\nmem_bswap_64(pt->filter, len);\r\nif (pt->filter[len - 1]) {\r\npr_err("%s: filter string not null terminated\n", __func__);\r\nerr = -EINVAL;\r\ngoto err_free_queues;\r\n}\r\nerr = addr_filters__parse_bare_filter(&pt->filts,\r\nfilter);\r\nif (err)\r\ngoto err_free_queues;\r\n}\r\nintel_pt_print_info_str("Filter string", pt->filter);\r\n}\r\npt->timeless_decoding = intel_pt_timeless_decoding(pt);\r\npt->have_tsc = intel_pt_have_tsc(pt);\r\npt->sampling_mode = false;\r\npt->est_tsc = !pt->timeless_decoding;\r\npt->unknown_thread = thread__new(999999999, 999999999);\r\nif (!pt->unknown_thread) {\r\nerr = -ENOMEM;\r\ngoto err_free_queues;\r\n}\r\nINIT_LIST_HEAD(&pt->unknown_thread->node);\r\nerr = thread__set_comm(pt->unknown_thread, "unknown", 0);\r\nif (err)\r\ngoto err_delete_thread;\r\nif (thread__init_map_groups(pt->unknown_thread, pt->machine)) {\r\nerr = -ENOMEM;\r\ngoto err_delete_thread;\r\n}\r\npt->auxtrace.process_event = intel_pt_process_event;\r\npt->auxtrace.process_auxtrace_event = intel_pt_process_auxtrace_event;\r\npt->auxtrace.flush_events = intel_pt_flush;\r\npt->auxtrace.free_events = intel_pt_free_events;\r\npt->auxtrace.free = intel_pt_free;\r\nsession->auxtrace = &pt->auxtrace;\r\nif (dump_trace)\r\nreturn 0;\r\nif (pt->have_sched_switch == 1) {\r\npt->switch_evsel = intel_pt_find_sched_switch(session->evlist);\r\nif (!pt->switch_evsel) {\r\npr_err("%s: missing sched_switch event\n", __func__);\r\nerr = -EINVAL;\r\ngoto err_delete_thread;\r\n}\r\n} else if (pt->have_sched_switch == 2 &&\r\n!intel_pt_find_switch(session->evlist)) {\r\npr_err("%s: missing context_switch attribute flag\n", __func__);\r\nerr = -EINVAL;\r\ngoto err_delete_thread;\r\n}\r\nif (session->itrace_synth_opts && session->itrace_synth_opts->set) {\r\npt->synth_opts = *session->itrace_synth_opts;\r\n} else {\r\nitrace_synth_opts__set_default(&pt->synth_opts);\r\nif (use_browser != -1) {\r\npt->synth_opts.branches = false;\r\npt->synth_opts.callchain = true;\r\n}\r\nif (session->itrace_synth_opts)\r\npt->synth_opts.thread_stack =\r\nsession->itrace_synth_opts->thread_stack;\r\n}\r\nif (pt->synth_opts.log)\r\nintel_pt_log_enable();\r\nif (pt->tc.time_mult) {\r\nu64 tsc_freq = intel_pt_ns_to_ticks(pt, 1000000000);\r\nif (!pt->max_non_turbo_ratio)\r\npt->max_non_turbo_ratio =\r\n(tsc_freq + 50000000) / 100000000;\r\nintel_pt_log("TSC frequency %"PRIu64"\n", tsc_freq);\r\nintel_pt_log("Maximum non-turbo ratio %u\n",\r\npt->max_non_turbo_ratio);\r\npt->cbr2khz = tsc_freq / pt->max_non_turbo_ratio / 1000;\r\n}\r\nif (pt->synth_opts.calls)\r\npt->branches_filter |= PERF_IP_FLAG_CALL | PERF_IP_FLAG_ASYNC |\r\nPERF_IP_FLAG_TRACE_END;\r\nif (pt->synth_opts.returns)\r\npt->branches_filter |= PERF_IP_FLAG_RETURN |\r\nPERF_IP_FLAG_TRACE_BEGIN;\r\nif (pt->synth_opts.callchain && !symbol_conf.use_callchain) {\r\nsymbol_conf.use_callchain = true;\r\nif (callchain_register_param(&callchain_param) < 0) {\r\nsymbol_conf.use_callchain = false;\r\npt->synth_opts.callchain = false;\r\n}\r\n}\r\nerr = intel_pt_synth_events(pt, session);\r\nif (err)\r\ngoto err_delete_thread;\r\nerr = auxtrace_queues__process_index(&pt->queues, session);\r\nif (err)\r\ngoto err_delete_thread;\r\nif (pt->queues.populated)\r\npt->data_queued = true;\r\nif (pt->timeless_decoding)\r\npr_debug2("Intel PT decoding without timestamps\n");\r\nreturn 0;\r\nerr_delete_thread:\r\nthread__zput(pt->unknown_thread);\r\nerr_free_queues:\r\nintel_pt_log_disable();\r\nauxtrace_queues__free(&pt->queues);\r\nsession->auxtrace = NULL;\r\nerr_free:\r\naddr_filters__exit(&pt->filts);\r\nzfree(&pt->filter);\r\nfree(pt);\r\nreturn err;\r\n}
