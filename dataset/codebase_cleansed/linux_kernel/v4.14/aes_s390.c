static int setkey_fallback_cip(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nint ret;\r\nsctx->fallback.cip->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nsctx->fallback.cip->base.crt_flags |= (tfm->crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nret = crypto_cipher_setkey(sctx->fallback.cip, in_key, key_len);\r\nif (ret) {\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |= (sctx->fallback.cip->base.crt_flags &\r\nCRYPTO_TFM_RES_MASK);\r\n}\r\nreturn ret;\r\n}\r\nstatic int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nunsigned long fc;\r\nfc = (key_len == 16) ? CPACF_KM_AES_128 :\r\n(key_len == 24) ? CPACF_KM_AES_192 :\r\n(key_len == 32) ? CPACF_KM_AES_256 : 0;\r\nsctx->fc = (fc && cpacf_test_func(&km_functions, fc)) ? fc : 0;\r\nif (!sctx->fc)\r\nreturn setkey_fallback_cip(tfm, in_key, key_len);\r\nsctx->key_len = key_len;\r\nmemcpy(sctx->key, in_key, key_len);\r\nreturn 0;\r\n}\r\nstatic void aes_encrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nif (unlikely(!sctx->fc)) {\r\ncrypto_cipher_encrypt_one(sctx->fallback.cip, out, in);\r\nreturn;\r\n}\r\ncpacf_km(sctx->fc, &sctx->key, out, in, AES_BLOCK_SIZE);\r\n}\r\nstatic void aes_decrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nif (unlikely(!sctx->fc)) {\r\ncrypto_cipher_decrypt_one(sctx->fallback.cip, out, in);\r\nreturn;\r\n}\r\ncpacf_km(sctx->fc | CPACF_DECRYPT,\r\n&sctx->key, out, in, AES_BLOCK_SIZE);\r\n}\r\nstatic int fallback_init_cip(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = tfm->__crt_alg->cra_name;\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nsctx->fallback.cip = crypto_alloc_cipher(name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(sctx->fallback.cip)) {\r\npr_err("Allocating AES fallback algorithm %s failed\n",\r\nname);\r\nreturn PTR_ERR(sctx->fallback.cip);\r\n}\r\nreturn 0;\r\n}\r\nstatic void fallback_exit_cip(struct crypto_tfm *tfm)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_cipher(sctx->fallback.cip);\r\nsctx->fallback.cip = NULL;\r\n}\r\nstatic int setkey_fallback_blk(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nunsigned int ret;\r\ncrypto_skcipher_clear_flags(sctx->fallback.blk, CRYPTO_TFM_REQ_MASK);\r\ncrypto_skcipher_set_flags(sctx->fallback.blk, tfm->crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nret = crypto_skcipher_setkey(sctx->fallback.blk, key, len);\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |= crypto_skcipher_get_flags(sctx->fallback.blk) &\r\nCRYPTO_TFM_RES_MASK;\r\nreturn ret;\r\n}\r\nstatic int fallback_blk_dec(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nunsigned int ret;\r\nstruct crypto_blkcipher *tfm = desc->tfm;\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(tfm);\r\nSKCIPHER_REQUEST_ON_STACK(req, sctx->fallback.blk);\r\nskcipher_request_set_tfm(req, sctx->fallback.blk);\r\nskcipher_request_set_callback(req, desc->flags, NULL, NULL);\r\nskcipher_request_set_crypt(req, src, dst, nbytes, desc->info);\r\nret = crypto_skcipher_decrypt(req);\r\nskcipher_request_zero(req);\r\nreturn ret;\r\n}\r\nstatic int fallback_blk_enc(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nunsigned int ret;\r\nstruct crypto_blkcipher *tfm = desc->tfm;\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(tfm);\r\nSKCIPHER_REQUEST_ON_STACK(req, sctx->fallback.blk);\r\nskcipher_request_set_tfm(req, sctx->fallback.blk);\r\nskcipher_request_set_callback(req, desc->flags, NULL, NULL);\r\nskcipher_request_set_crypt(req, src, dst, nbytes, desc->info);\r\nret = crypto_skcipher_encrypt(req);\r\nreturn ret;\r\n}\r\nstatic int ecb_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nunsigned long fc;\r\nfc = (key_len == 16) ? CPACF_KM_AES_128 :\r\n(key_len == 24) ? CPACF_KM_AES_192 :\r\n(key_len == 32) ? CPACF_KM_AES_256 : 0;\r\nsctx->fc = (fc && cpacf_test_func(&km_functions, fc)) ? fc : 0;\r\nif (!sctx->fc)\r\nreturn setkey_fallback_blk(tfm, in_key, key_len);\r\nsctx->key_len = key_len;\r\nmemcpy(sctx->key, in_key, key_len);\r\nreturn 0;\r\n}\r\nstatic int ecb_aes_crypt(struct blkcipher_desc *desc, unsigned long modifier,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nunsigned int nbytes, n;\r\nint ret;\r\nret = blkcipher_walk_virt(desc, walk);\r\nwhile ((nbytes = walk->nbytes) >= AES_BLOCK_SIZE) {\r\nn = nbytes & ~(AES_BLOCK_SIZE - 1);\r\ncpacf_km(sctx->fc | modifier, sctx->key,\r\nwalk->dst.virt.addr, walk->src.virt.addr, n);\r\nret = blkcipher_walk_done(desc, walk, nbytes - n);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ecb_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!sctx->fc))\r\nreturn fallback_blk_enc(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_aes_crypt(desc, 0, &walk);\r\n}\r\nstatic int ecb_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!sctx->fc))\r\nreturn fallback_blk_dec(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_aes_crypt(desc, CPACF_DECRYPT, &walk);\r\n}\r\nstatic int fallback_init_blk(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = tfm->__crt_alg->cra_name;\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nsctx->fallback.blk = crypto_alloc_skcipher(name, 0,\r\nCRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(sctx->fallback.blk)) {\r\npr_err("Allocating AES fallback algorithm %s failed\n",\r\nname);\r\nreturn PTR_ERR(sctx->fallback.blk);\r\n}\r\nreturn 0;\r\n}\r\nstatic void fallback_exit_blk(struct crypto_tfm *tfm)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_skcipher(sctx->fallback.blk);\r\n}\r\nstatic int cbc_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nunsigned long fc;\r\nfc = (key_len == 16) ? CPACF_KMC_AES_128 :\r\n(key_len == 24) ? CPACF_KMC_AES_192 :\r\n(key_len == 32) ? CPACF_KMC_AES_256 : 0;\r\nsctx->fc = (fc && cpacf_test_func(&kmc_functions, fc)) ? fc : 0;\r\nif (!sctx->fc)\r\nreturn setkey_fallback_blk(tfm, in_key, key_len);\r\nsctx->key_len = key_len;\r\nmemcpy(sctx->key, in_key, key_len);\r\nreturn 0;\r\n}\r\nstatic int cbc_aes_crypt(struct blkcipher_desc *desc, unsigned long modifier,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nunsigned int nbytes, n;\r\nint ret;\r\nstruct {\r\nu8 iv[AES_BLOCK_SIZE];\r\nu8 key[AES_MAX_KEY_SIZE];\r\n} param;\r\nret = blkcipher_walk_virt(desc, walk);\r\nmemcpy(param.iv, walk->iv, AES_BLOCK_SIZE);\r\nmemcpy(param.key, sctx->key, sctx->key_len);\r\nwhile ((nbytes = walk->nbytes) >= AES_BLOCK_SIZE) {\r\nn = nbytes & ~(AES_BLOCK_SIZE - 1);\r\ncpacf_kmc(sctx->fc | modifier, &param,\r\nwalk->dst.virt.addr, walk->src.virt.addr, n);\r\nret = blkcipher_walk_done(desc, walk, nbytes - n);\r\n}\r\nmemcpy(walk->iv, param.iv, AES_BLOCK_SIZE);\r\nreturn ret;\r\n}\r\nstatic int cbc_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!sctx->fc))\r\nreturn fallback_blk_enc(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn cbc_aes_crypt(desc, 0, &walk);\r\n}\r\nstatic int cbc_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!sctx->fc))\r\nreturn fallback_blk_dec(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn cbc_aes_crypt(desc, CPACF_DECRYPT, &walk);\r\n}\r\nstatic int xts_fallback_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int len)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\nunsigned int ret;\r\ncrypto_skcipher_clear_flags(xts_ctx->fallback, CRYPTO_TFM_REQ_MASK);\r\ncrypto_skcipher_set_flags(xts_ctx->fallback, tfm->crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nret = crypto_skcipher_setkey(xts_ctx->fallback, key, len);\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |= crypto_skcipher_get_flags(xts_ctx->fallback) &\r\nCRYPTO_TFM_RES_MASK;\r\nreturn ret;\r\n}\r\nstatic int xts_fallback_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_blkcipher *tfm = desc->tfm;\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(tfm);\r\nSKCIPHER_REQUEST_ON_STACK(req, xts_ctx->fallback);\r\nunsigned int ret;\r\nskcipher_request_set_tfm(req, xts_ctx->fallback);\r\nskcipher_request_set_callback(req, desc->flags, NULL, NULL);\r\nskcipher_request_set_crypt(req, src, dst, nbytes, desc->info);\r\nret = crypto_skcipher_decrypt(req);\r\nskcipher_request_zero(req);\r\nreturn ret;\r\n}\r\nstatic int xts_fallback_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_blkcipher *tfm = desc->tfm;\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(tfm);\r\nSKCIPHER_REQUEST_ON_STACK(req, xts_ctx->fallback);\r\nunsigned int ret;\r\nskcipher_request_set_tfm(req, xts_ctx->fallback);\r\nskcipher_request_set_callback(req, desc->flags, NULL, NULL);\r\nskcipher_request_set_crypt(req, src, dst, nbytes, desc->info);\r\nret = crypto_skcipher_encrypt(req);\r\nskcipher_request_zero(req);\r\nreturn ret;\r\n}\r\nstatic int xts_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\nunsigned long fc;\r\nint err;\r\nerr = xts_check_key(tfm, in_key, key_len);\r\nif (err)\r\nreturn err;\r\nif (fips_enabled && key_len != 32 && key_len != 64) {\r\ntfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nfc = (key_len == 32) ? CPACF_KM_XTS_128 :\r\n(key_len == 64) ? CPACF_KM_XTS_256 : 0;\r\nxts_ctx->fc = (fc && cpacf_test_func(&km_functions, fc)) ? fc : 0;\r\nif (!xts_ctx->fc)\r\nreturn xts_fallback_setkey(tfm, in_key, key_len);\r\nkey_len = key_len / 2;\r\nxts_ctx->key_len = key_len;\r\nmemcpy(xts_ctx->key, in_key, key_len);\r\nmemcpy(xts_ctx->pcc_key, in_key + key_len, key_len);\r\nreturn 0;\r\n}\r\nstatic int xts_aes_crypt(struct blkcipher_desc *desc, unsigned long modifier,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nunsigned int offset, nbytes, n;\r\nint ret;\r\nstruct {\r\nu8 key[32];\r\nu8 tweak[16];\r\nu8 block[16];\r\nu8 bit[16];\r\nu8 xts[16];\r\n} pcc_param;\r\nstruct {\r\nu8 key[32];\r\nu8 init[16];\r\n} xts_param;\r\nret = blkcipher_walk_virt(desc, walk);\r\noffset = xts_ctx->key_len & 0x10;\r\nmemset(pcc_param.block, 0, sizeof(pcc_param.block));\r\nmemset(pcc_param.bit, 0, sizeof(pcc_param.bit));\r\nmemset(pcc_param.xts, 0, sizeof(pcc_param.xts));\r\nmemcpy(pcc_param.tweak, walk->iv, sizeof(pcc_param.tweak));\r\nmemcpy(pcc_param.key + offset, xts_ctx->pcc_key, xts_ctx->key_len);\r\ncpacf_pcc(xts_ctx->fc, pcc_param.key + offset);\r\nmemcpy(xts_param.key + offset, xts_ctx->key, xts_ctx->key_len);\r\nmemcpy(xts_param.init, pcc_param.xts, 16);\r\nwhile ((nbytes = walk->nbytes) >= AES_BLOCK_SIZE) {\r\nn = nbytes & ~(AES_BLOCK_SIZE - 1);\r\ncpacf_km(xts_ctx->fc | modifier, xts_param.key + offset,\r\nwalk->dst.virt.addr, walk->src.virt.addr, n);\r\nret = blkcipher_walk_done(desc, walk, nbytes - n);\r\n}\r\nreturn ret;\r\n}\r\nstatic int xts_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!xts_ctx->fc))\r\nreturn xts_fallback_encrypt(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn xts_aes_crypt(desc, 0, &walk);\r\n}\r\nstatic int xts_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!xts_ctx->fc))\r\nreturn xts_fallback_decrypt(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn xts_aes_crypt(desc, CPACF_DECRYPT, &walk);\r\n}\r\nstatic int xts_fallback_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = tfm->__crt_alg->cra_name;\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\nxts_ctx->fallback = crypto_alloc_skcipher(name, 0,\r\nCRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(xts_ctx->fallback)) {\r\npr_err("Allocating XTS fallback algorithm %s failed\n",\r\nname);\r\nreturn PTR_ERR(xts_ctx->fallback);\r\n}\r\nreturn 0;\r\n}\r\nstatic void xts_fallback_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_skcipher(xts_ctx->fallback);\r\n}\r\nstatic int ctr_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nunsigned long fc;\r\nfc = (key_len == 16) ? CPACF_KMCTR_AES_128 :\r\n(key_len == 24) ? CPACF_KMCTR_AES_192 :\r\n(key_len == 32) ? CPACF_KMCTR_AES_256 : 0;\r\nsctx->fc = (fc && cpacf_test_func(&kmctr_functions, fc)) ? fc : 0;\r\nif (!sctx->fc)\r\nreturn setkey_fallback_blk(tfm, in_key, key_len);\r\nsctx->key_len = key_len;\r\nmemcpy(sctx->key, in_key, key_len);\r\nreturn 0;\r\n}\r\nstatic unsigned int __ctrblk_init(u8 *ctrptr, u8 *iv, unsigned int nbytes)\r\n{\r\nunsigned int i, n;\r\nmemcpy(ctrptr, iv, AES_BLOCK_SIZE);\r\nn = (nbytes > PAGE_SIZE) ? PAGE_SIZE : nbytes & ~(AES_BLOCK_SIZE - 1);\r\nfor (i = (n / AES_BLOCK_SIZE) - 1; i > 0; i--) {\r\nmemcpy(ctrptr + AES_BLOCK_SIZE, ctrptr, AES_BLOCK_SIZE);\r\ncrypto_inc(ctrptr + AES_BLOCK_SIZE, AES_BLOCK_SIZE);\r\nctrptr += AES_BLOCK_SIZE;\r\n}\r\nreturn n;\r\n}\r\nstatic int ctr_aes_crypt(struct blkcipher_desc *desc, unsigned long modifier,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nu8 buf[AES_BLOCK_SIZE], *ctrptr;\r\nunsigned int n, nbytes;\r\nint ret, locked;\r\nlocked = spin_trylock(&ctrblk_lock);\r\nret = blkcipher_walk_virt_block(desc, walk, AES_BLOCK_SIZE);\r\nwhile ((nbytes = walk->nbytes) >= AES_BLOCK_SIZE) {\r\nn = AES_BLOCK_SIZE;\r\nif (nbytes >= 2*AES_BLOCK_SIZE && locked)\r\nn = __ctrblk_init(ctrblk, walk->iv, nbytes);\r\nctrptr = (n > AES_BLOCK_SIZE) ? ctrblk : walk->iv;\r\ncpacf_kmctr(sctx->fc | modifier, sctx->key,\r\nwalk->dst.virt.addr, walk->src.virt.addr,\r\nn, ctrptr);\r\nif (ctrptr == ctrblk)\r\nmemcpy(walk->iv, ctrptr + n - AES_BLOCK_SIZE,\r\nAES_BLOCK_SIZE);\r\ncrypto_inc(walk->iv, AES_BLOCK_SIZE);\r\nret = blkcipher_walk_done(desc, walk, nbytes - n);\r\n}\r\nif (locked)\r\nspin_unlock(&ctrblk_lock);\r\nif (nbytes) {\r\ncpacf_kmctr(sctx->fc | modifier, sctx->key,\r\nbuf, walk->src.virt.addr,\r\nAES_BLOCK_SIZE, walk->iv);\r\nmemcpy(walk->dst.virt.addr, buf, nbytes);\r\ncrypto_inc(walk->iv, AES_BLOCK_SIZE);\r\nret = blkcipher_walk_done(desc, walk, 0);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ctr_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!sctx->fc))\r\nreturn fallback_blk_enc(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ctr_aes_crypt(desc, 0, &walk);\r\n}\r\nstatic int ctr_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(!sctx->fc))\r\nreturn fallback_blk_dec(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ctr_aes_crypt(desc, CPACF_DECRYPT, &walk);\r\n}\r\nstatic int aes_s390_register_alg(struct crypto_alg *alg)\r\n{\r\nint ret;\r\nret = crypto_register_alg(alg);\r\nif (!ret)\r\naes_s390_algs_ptr[aes_s390_algs_num++] = alg;\r\nreturn ret;\r\n}\r\nstatic void aes_s390_fini(void)\r\n{\r\nwhile (aes_s390_algs_num--)\r\ncrypto_unregister_alg(aes_s390_algs_ptr[aes_s390_algs_num]);\r\nif (ctrblk)\r\nfree_page((unsigned long) ctrblk);\r\n}\r\nstatic int __init aes_s390_init(void)\r\n{\r\nint ret;\r\ncpacf_query(CPACF_KM, &km_functions);\r\ncpacf_query(CPACF_KMC, &kmc_functions);\r\ncpacf_query(CPACF_KMCTR, &kmctr_functions);\r\nif (cpacf_test_func(&km_functions, CPACF_KM_AES_128) ||\r\ncpacf_test_func(&km_functions, CPACF_KM_AES_192) ||\r\ncpacf_test_func(&km_functions, CPACF_KM_AES_256)) {\r\nret = aes_s390_register_alg(&aes_alg);\r\nif (ret)\r\ngoto out_err;\r\nret = aes_s390_register_alg(&ecb_aes_alg);\r\nif (ret)\r\ngoto out_err;\r\n}\r\nif (cpacf_test_func(&kmc_functions, CPACF_KMC_AES_128) ||\r\ncpacf_test_func(&kmc_functions, CPACF_KMC_AES_192) ||\r\ncpacf_test_func(&kmc_functions, CPACF_KMC_AES_256)) {\r\nret = aes_s390_register_alg(&cbc_aes_alg);\r\nif (ret)\r\ngoto out_err;\r\n}\r\nif (cpacf_test_func(&km_functions, CPACF_KM_XTS_128) ||\r\ncpacf_test_func(&km_functions, CPACF_KM_XTS_256)) {\r\nret = aes_s390_register_alg(&xts_aes_alg);\r\nif (ret)\r\ngoto out_err;\r\n}\r\nif (cpacf_test_func(&kmctr_functions, CPACF_KMCTR_AES_128) ||\r\ncpacf_test_func(&kmctr_functions, CPACF_KMCTR_AES_192) ||\r\ncpacf_test_func(&kmctr_functions, CPACF_KMCTR_AES_256)) {\r\nctrblk = (u8 *) __get_free_page(GFP_KERNEL);\r\nif (!ctrblk) {\r\nret = -ENOMEM;\r\ngoto out_err;\r\n}\r\nret = aes_s390_register_alg(&ctr_aes_alg);\r\nif (ret)\r\ngoto out_err;\r\n}\r\nreturn 0;\r\nout_err:\r\naes_s390_fini();\r\nreturn ret;\r\n}
