static inline int\r\ngnet_stats_copy(struct gnet_dump *d, int type, void *buf, int size, int padattr)\r\n{\r\nif (nla_put_64bit(d->skb, type, size, buf, padattr))\r\ngoto nla_put_failure;\r\nreturn 0;\r\nnla_put_failure:\r\nif (d->lock)\r\nspin_unlock_bh(d->lock);\r\nkfree(d->xstats);\r\nd->xstats = NULL;\r\nd->xstats_len = 0;\r\nreturn -1;\r\n}\r\nint\r\ngnet_stats_start_copy_compat(struct sk_buff *skb, int type, int tc_stats_type,\r\nint xstats_type, spinlock_t *lock,\r\nstruct gnet_dump *d, int padattr)\r\n__acquires(lock)\r\n{\r\nmemset(d, 0, sizeof(*d));\r\nif (type)\r\nd->tail = (struct nlattr *)skb_tail_pointer(skb);\r\nd->skb = skb;\r\nd->compat_tc_stats = tc_stats_type;\r\nd->compat_xstats = xstats_type;\r\nd->padattr = padattr;\r\nif (lock) {\r\nd->lock = lock;\r\nspin_lock_bh(lock);\r\n}\r\nif (d->tail)\r\nreturn gnet_stats_copy(d, type, NULL, 0, padattr);\r\nreturn 0;\r\n}\r\nint\r\ngnet_stats_start_copy(struct sk_buff *skb, int type, spinlock_t *lock,\r\nstruct gnet_dump *d, int padattr)\r\n{\r\nreturn gnet_stats_start_copy_compat(skb, type, 0, 0, lock, d, padattr);\r\n}\r\nstatic void\r\n__gnet_stats_copy_basic_cpu(struct gnet_stats_basic_packed *bstats,\r\nstruct gnet_stats_basic_cpu __percpu *cpu)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nstruct gnet_stats_basic_cpu *bcpu = per_cpu_ptr(cpu, i);\r\nunsigned int start;\r\nu64 bytes;\r\nu32 packets;\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&bcpu->syncp);\r\nbytes = bcpu->bstats.bytes;\r\npackets = bcpu->bstats.packets;\r\n} while (u64_stats_fetch_retry_irq(&bcpu->syncp, start));\r\nbstats->bytes += bytes;\r\nbstats->packets += packets;\r\n}\r\n}\r\nvoid\r\n__gnet_stats_copy_basic(const seqcount_t *running,\r\nstruct gnet_stats_basic_packed *bstats,\r\nstruct gnet_stats_basic_cpu __percpu *cpu,\r\nstruct gnet_stats_basic_packed *b)\r\n{\r\nunsigned int seq;\r\nif (cpu) {\r\n__gnet_stats_copy_basic_cpu(bstats, cpu);\r\nreturn;\r\n}\r\ndo {\r\nif (running)\r\nseq = read_seqcount_begin(running);\r\nbstats->bytes = b->bytes;\r\nbstats->packets = b->packets;\r\n} while (running && read_seqcount_retry(running, seq));\r\n}\r\nint\r\ngnet_stats_copy_basic(const seqcount_t *running,\r\nstruct gnet_dump *d,\r\nstruct gnet_stats_basic_cpu __percpu *cpu,\r\nstruct gnet_stats_basic_packed *b)\r\n{\r\nstruct gnet_stats_basic_packed bstats = {0};\r\n__gnet_stats_copy_basic(running, &bstats, cpu, b);\r\nif (d->compat_tc_stats) {\r\nd->tc_stats.bytes = bstats.bytes;\r\nd->tc_stats.packets = bstats.packets;\r\n}\r\nif (d->tail) {\r\nstruct gnet_stats_basic sb;\r\nmemset(&sb, 0, sizeof(sb));\r\nsb.bytes = bstats.bytes;\r\nsb.packets = bstats.packets;\r\nreturn gnet_stats_copy(d, TCA_STATS_BASIC, &sb, sizeof(sb),\r\nTCA_STATS_PAD);\r\n}\r\nreturn 0;\r\n}\r\nint\r\ngnet_stats_copy_rate_est(struct gnet_dump *d,\r\nstruct net_rate_estimator __rcu **rate_est)\r\n{\r\nstruct gnet_stats_rate_est64 sample;\r\nstruct gnet_stats_rate_est est;\r\nint res;\r\nif (!gen_estimator_read(rate_est, &sample))\r\nreturn 0;\r\nest.bps = min_t(u64, UINT_MAX, sample.bps);\r\nest.pps = sample.pps;\r\nif (d->compat_tc_stats) {\r\nd->tc_stats.bps = est.bps;\r\nd->tc_stats.pps = est.pps;\r\n}\r\nif (d->tail) {\r\nres = gnet_stats_copy(d, TCA_STATS_RATE_EST, &est, sizeof(est),\r\nTCA_STATS_PAD);\r\nif (res < 0 || est.bps == sample.bps)\r\nreturn res;\r\nreturn gnet_stats_copy(d, TCA_STATS_RATE_EST64, &sample,\r\nsizeof(sample), TCA_STATS_PAD);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\n__gnet_stats_copy_queue_cpu(struct gnet_stats_queue *qstats,\r\nconst struct gnet_stats_queue __percpu *q)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nconst struct gnet_stats_queue *qcpu = per_cpu_ptr(q, i);\r\nqstats->qlen = 0;\r\nqstats->backlog += qcpu->backlog;\r\nqstats->drops += qcpu->drops;\r\nqstats->requeues += qcpu->requeues;\r\nqstats->overlimits += qcpu->overlimits;\r\n}\r\n}\r\nstatic void __gnet_stats_copy_queue(struct gnet_stats_queue *qstats,\r\nconst struct gnet_stats_queue __percpu *cpu,\r\nconst struct gnet_stats_queue *q,\r\n__u32 qlen)\r\n{\r\nif (cpu) {\r\n__gnet_stats_copy_queue_cpu(qstats, cpu);\r\n} else {\r\nqstats->qlen = q->qlen;\r\nqstats->backlog = q->backlog;\r\nqstats->drops = q->drops;\r\nqstats->requeues = q->requeues;\r\nqstats->overlimits = q->overlimits;\r\n}\r\nqstats->qlen = qlen;\r\n}\r\nint\r\ngnet_stats_copy_queue(struct gnet_dump *d,\r\nstruct gnet_stats_queue __percpu *cpu_q,\r\nstruct gnet_stats_queue *q, __u32 qlen)\r\n{\r\nstruct gnet_stats_queue qstats = {0};\r\n__gnet_stats_copy_queue(&qstats, cpu_q, q, qlen);\r\nif (d->compat_tc_stats) {\r\nd->tc_stats.drops = qstats.drops;\r\nd->tc_stats.qlen = qstats.qlen;\r\nd->tc_stats.backlog = qstats.backlog;\r\nd->tc_stats.overlimits = qstats.overlimits;\r\n}\r\nif (d->tail)\r\nreturn gnet_stats_copy(d, TCA_STATS_QUEUE,\r\n&qstats, sizeof(qstats),\r\nTCA_STATS_PAD);\r\nreturn 0;\r\n}\r\nint\r\ngnet_stats_copy_app(struct gnet_dump *d, void *st, int len)\r\n{\r\nif (d->compat_xstats) {\r\nd->xstats = kmemdup(st, len, GFP_ATOMIC);\r\nif (!d->xstats)\r\ngoto err_out;\r\nd->xstats_len = len;\r\n}\r\nif (d->tail)\r\nreturn gnet_stats_copy(d, TCA_STATS_APP, st, len,\r\nTCA_STATS_PAD);\r\nreturn 0;\r\nerr_out:\r\nif (d->lock)\r\nspin_unlock_bh(d->lock);\r\nd->xstats_len = 0;\r\nreturn -1;\r\n}\r\nint\r\ngnet_stats_finish_copy(struct gnet_dump *d)\r\n{\r\nif (d->tail)\r\nd->tail->nla_len = skb_tail_pointer(d->skb) - (u8 *)d->tail;\r\nif (d->compat_tc_stats)\r\nif (gnet_stats_copy(d, d->compat_tc_stats, &d->tc_stats,\r\nsizeof(d->tc_stats), d->padattr) < 0)\r\nreturn -1;\r\nif (d->compat_xstats && d->xstats) {\r\nif (gnet_stats_copy(d, d->compat_xstats, d->xstats,\r\nd->xstats_len, d->padattr) < 0)\r\nreturn -1;\r\n}\r\nif (d->lock)\r\nspin_unlock_bh(d->lock);\r\nkfree(d->xstats);\r\nd->xstats = NULL;\r\nd->xstats_len = 0;\r\nreturn 0;\r\n}
