static void amd_sched_rq_init(struct amd_sched_rq *rq)\r\n{\r\nspin_lock_init(&rq->lock);\r\nINIT_LIST_HEAD(&rq->entities);\r\nrq->current_entity = NULL;\r\n}\r\nstatic void amd_sched_rq_add_entity(struct amd_sched_rq *rq,\r\nstruct amd_sched_entity *entity)\r\n{\r\nif (!list_empty(&entity->list))\r\nreturn;\r\nspin_lock(&rq->lock);\r\nlist_add_tail(&entity->list, &rq->entities);\r\nspin_unlock(&rq->lock);\r\n}\r\nstatic void amd_sched_rq_remove_entity(struct amd_sched_rq *rq,\r\nstruct amd_sched_entity *entity)\r\n{\r\nif (list_empty(&entity->list))\r\nreturn;\r\nspin_lock(&rq->lock);\r\nlist_del_init(&entity->list);\r\nif (rq->current_entity == entity)\r\nrq->current_entity = NULL;\r\nspin_unlock(&rq->lock);\r\n}\r\nstatic struct amd_sched_entity *\r\namd_sched_rq_select_entity(struct amd_sched_rq *rq)\r\n{\r\nstruct amd_sched_entity *entity;\r\nspin_lock(&rq->lock);\r\nentity = rq->current_entity;\r\nif (entity) {\r\nlist_for_each_entry_continue(entity, &rq->entities, list) {\r\nif (amd_sched_entity_is_ready(entity)) {\r\nrq->current_entity = entity;\r\nspin_unlock(&rq->lock);\r\nreturn entity;\r\n}\r\n}\r\n}\r\nlist_for_each_entry(entity, &rq->entities, list) {\r\nif (amd_sched_entity_is_ready(entity)) {\r\nrq->current_entity = entity;\r\nspin_unlock(&rq->lock);\r\nreturn entity;\r\n}\r\nif (entity == rq->current_entity)\r\nbreak;\r\n}\r\nspin_unlock(&rq->lock);\r\nreturn NULL;\r\n}\r\nint amd_sched_entity_init(struct amd_gpu_scheduler *sched,\r\nstruct amd_sched_entity *entity,\r\nstruct amd_sched_rq *rq,\r\nuint32_t jobs)\r\n{\r\nint r;\r\nif (!(sched && entity && rq))\r\nreturn -EINVAL;\r\nmemset(entity, 0, sizeof(struct amd_sched_entity));\r\nINIT_LIST_HEAD(&entity->list);\r\nentity->rq = rq;\r\nentity->sched = sched;\r\nspin_lock_init(&entity->queue_lock);\r\nr = kfifo_alloc(&entity->job_queue, jobs * sizeof(void *), GFP_KERNEL);\r\nif (r)\r\nreturn r;\r\natomic_set(&entity->fence_seq, 0);\r\nentity->fence_context = dma_fence_context_alloc(2);\r\nreturn 0;\r\n}\r\nstatic bool amd_sched_entity_is_initialized(struct amd_gpu_scheduler *sched,\r\nstruct amd_sched_entity *entity)\r\n{\r\nreturn entity->sched == sched &&\r\nentity->rq != NULL;\r\n}\r\nstatic bool amd_sched_entity_is_idle(struct amd_sched_entity *entity)\r\n{\r\nrmb();\r\nif (kfifo_is_empty(&entity->job_queue))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool amd_sched_entity_is_ready(struct amd_sched_entity *entity)\r\n{\r\nif (kfifo_is_empty(&entity->job_queue))\r\nreturn false;\r\nif (ACCESS_ONCE(entity->dependency))\r\nreturn false;\r\nreturn true;\r\n}\r\nvoid amd_sched_entity_fini(struct amd_gpu_scheduler *sched,\r\nstruct amd_sched_entity *entity)\r\n{\r\nstruct amd_sched_rq *rq = entity->rq;\r\nif (!amd_sched_entity_is_initialized(sched, entity))\r\nreturn;\r\nwait_event(sched->job_scheduled, amd_sched_entity_is_idle(entity));\r\namd_sched_rq_remove_entity(rq, entity);\r\nkfifo_free(&entity->job_queue);\r\n}\r\nstatic void amd_sched_entity_wakeup(struct dma_fence *f, struct dma_fence_cb *cb)\r\n{\r\nstruct amd_sched_entity *entity =\r\ncontainer_of(cb, struct amd_sched_entity, cb);\r\nentity->dependency = NULL;\r\ndma_fence_put(f);\r\namd_sched_wakeup(entity->sched);\r\n}\r\nstatic void amd_sched_entity_clear_dep(struct dma_fence *f, struct dma_fence_cb *cb)\r\n{\r\nstruct amd_sched_entity *entity =\r\ncontainer_of(cb, struct amd_sched_entity, cb);\r\nentity->dependency = NULL;\r\ndma_fence_put(f);\r\n}\r\nbool amd_sched_dependency_optimized(struct dma_fence* fence,\r\nstruct amd_sched_entity *entity)\r\n{\r\nstruct amd_gpu_scheduler *sched = entity->sched;\r\nstruct amd_sched_fence *s_fence;\r\nif (!fence || dma_fence_is_signaled(fence))\r\nreturn false;\r\nif (fence->context == entity->fence_context)\r\nreturn true;\r\ns_fence = to_amd_sched_fence(fence);\r\nif (s_fence && s_fence->sched == sched)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool amd_sched_entity_add_dependency_cb(struct amd_sched_entity *entity)\r\n{\r\nstruct amd_gpu_scheduler *sched = entity->sched;\r\nstruct dma_fence * fence = entity->dependency;\r\nstruct amd_sched_fence *s_fence;\r\nif (fence->context == entity->fence_context) {\r\ndma_fence_put(entity->dependency);\r\nreturn false;\r\n}\r\ns_fence = to_amd_sched_fence(fence);\r\nif (s_fence && s_fence->sched == sched) {\r\nfence = dma_fence_get(&s_fence->scheduled);\r\ndma_fence_put(entity->dependency);\r\nentity->dependency = fence;\r\nif (!dma_fence_add_callback(fence, &entity->cb,\r\namd_sched_entity_clear_dep))\r\nreturn true;\r\ndma_fence_put(fence);\r\nreturn false;\r\n}\r\nif (!dma_fence_add_callback(entity->dependency, &entity->cb,\r\namd_sched_entity_wakeup))\r\nreturn true;\r\ndma_fence_put(entity->dependency);\r\nreturn false;\r\n}\r\nstatic struct amd_sched_job *\r\namd_sched_entity_pop_job(struct amd_sched_entity *entity)\r\n{\r\nstruct amd_gpu_scheduler *sched = entity->sched;\r\nstruct amd_sched_job *sched_job;\r\nif (!kfifo_out_peek(&entity->job_queue, &sched_job, sizeof(sched_job)))\r\nreturn NULL;\r\nwhile ((entity->dependency = sched->ops->dependency(sched_job)))\r\nif (amd_sched_entity_add_dependency_cb(entity))\r\nreturn NULL;\r\nreturn sched_job;\r\n}\r\nstatic bool amd_sched_entity_in(struct amd_sched_job *sched_job)\r\n{\r\nstruct amd_gpu_scheduler *sched = sched_job->sched;\r\nstruct amd_sched_entity *entity = sched_job->s_entity;\r\nbool added, first = false;\r\nspin_lock(&entity->queue_lock);\r\nadded = kfifo_in(&entity->job_queue, &sched_job,\r\nsizeof(sched_job)) == sizeof(sched_job);\r\nif (added && kfifo_len(&entity->job_queue) == sizeof(sched_job))\r\nfirst = true;\r\nspin_unlock(&entity->queue_lock);\r\nif (first) {\r\namd_sched_rq_add_entity(entity->rq, entity);\r\namd_sched_wakeup(sched);\r\n}\r\nreturn added;\r\n}\r\nstatic void amd_sched_job_finish(struct work_struct *work)\r\n{\r\nstruct amd_sched_job *s_job = container_of(work, struct amd_sched_job,\r\nfinish_work);\r\nstruct amd_gpu_scheduler *sched = s_job->sched;\r\nspin_lock(&sched->job_list_lock);\r\nlist_del_init(&s_job->node);\r\nif (sched->timeout != MAX_SCHEDULE_TIMEOUT) {\r\nstruct amd_sched_job *next;\r\nspin_unlock(&sched->job_list_lock);\r\ncancel_delayed_work_sync(&s_job->work_tdr);\r\nspin_lock(&sched->job_list_lock);\r\nnext = list_first_entry_or_null(&sched->ring_mirror_list,\r\nstruct amd_sched_job, node);\r\nif (next)\r\nschedule_delayed_work(&next->work_tdr, sched->timeout);\r\n}\r\nspin_unlock(&sched->job_list_lock);\r\nsched->ops->free_job(s_job);\r\n}\r\nstatic void amd_sched_job_finish_cb(struct dma_fence *f,\r\nstruct dma_fence_cb *cb)\r\n{\r\nstruct amd_sched_job *job = container_of(cb, struct amd_sched_job,\r\nfinish_cb);\r\nschedule_work(&job->finish_work);\r\n}\r\nstatic void amd_sched_job_begin(struct amd_sched_job *s_job)\r\n{\r\nstruct amd_gpu_scheduler *sched = s_job->sched;\r\nspin_lock(&sched->job_list_lock);\r\nlist_add_tail(&s_job->node, &sched->ring_mirror_list);\r\nif (sched->timeout != MAX_SCHEDULE_TIMEOUT &&\r\nlist_first_entry_or_null(&sched->ring_mirror_list,\r\nstruct amd_sched_job, node) == s_job)\r\nschedule_delayed_work(&s_job->work_tdr, sched->timeout);\r\nspin_unlock(&sched->job_list_lock);\r\n}\r\nstatic void amd_sched_job_timedout(struct work_struct *work)\r\n{\r\nstruct amd_sched_job *job = container_of(work, struct amd_sched_job,\r\nwork_tdr.work);\r\njob->sched->ops->timedout_job(job);\r\n}\r\nvoid amd_sched_hw_job_reset(struct amd_gpu_scheduler *sched)\r\n{\r\nstruct amd_sched_job *s_job;\r\nspin_lock(&sched->job_list_lock);\r\nlist_for_each_entry_reverse(s_job, &sched->ring_mirror_list, node) {\r\nif (s_job->s_fence->parent &&\r\ndma_fence_remove_callback(s_job->s_fence->parent,\r\n&s_job->s_fence->cb)) {\r\ndma_fence_put(s_job->s_fence->parent);\r\ns_job->s_fence->parent = NULL;\r\natomic_dec(&sched->hw_rq_count);\r\n}\r\n}\r\nspin_unlock(&sched->job_list_lock);\r\n}\r\nvoid amd_sched_job_kickout(struct amd_sched_job *s_job)\r\n{\r\nstruct amd_gpu_scheduler *sched = s_job->sched;\r\nspin_lock(&sched->job_list_lock);\r\nlist_del_init(&s_job->node);\r\nspin_unlock(&sched->job_list_lock);\r\n}\r\nvoid amd_sched_job_recovery(struct amd_gpu_scheduler *sched)\r\n{\r\nstruct amd_sched_job *s_job, *tmp;\r\nint r;\r\nspin_lock(&sched->job_list_lock);\r\ns_job = list_first_entry_or_null(&sched->ring_mirror_list,\r\nstruct amd_sched_job, node);\r\nif (s_job && sched->timeout != MAX_SCHEDULE_TIMEOUT)\r\nschedule_delayed_work(&s_job->work_tdr, sched->timeout);\r\nlist_for_each_entry_safe(s_job, tmp, &sched->ring_mirror_list, node) {\r\nstruct amd_sched_fence *s_fence = s_job->s_fence;\r\nstruct dma_fence *fence;\r\nspin_unlock(&sched->job_list_lock);\r\nfence = sched->ops->run_job(s_job);\r\natomic_inc(&sched->hw_rq_count);\r\nif (fence) {\r\ns_fence->parent = dma_fence_get(fence);\r\nr = dma_fence_add_callback(fence, &s_fence->cb,\r\namd_sched_process_job);\r\nif (r == -ENOENT)\r\namd_sched_process_job(fence, &s_fence->cb);\r\nelse if (r)\r\nDRM_ERROR("fence add callback failed (%d)\n",\r\nr);\r\ndma_fence_put(fence);\r\n} else {\r\nDRM_ERROR("Failed to run job!\n");\r\namd_sched_process_job(NULL, &s_fence->cb);\r\n}\r\nspin_lock(&sched->job_list_lock);\r\n}\r\nspin_unlock(&sched->job_list_lock);\r\n}\r\nvoid amd_sched_entity_push_job(struct amd_sched_job *sched_job)\r\n{\r\nstruct amd_sched_entity *entity = sched_job->s_entity;\r\ntrace_amd_sched_job(sched_job);\r\ndma_fence_add_callback(&sched_job->s_fence->finished, &sched_job->finish_cb,\r\namd_sched_job_finish_cb);\r\nwait_event(entity->sched->job_scheduled,\r\namd_sched_entity_in(sched_job));\r\n}\r\nint amd_sched_job_init(struct amd_sched_job *job,\r\nstruct amd_gpu_scheduler *sched,\r\nstruct amd_sched_entity *entity,\r\nvoid *owner)\r\n{\r\njob->sched = sched;\r\njob->s_entity = entity;\r\njob->s_fence = amd_sched_fence_create(entity, owner);\r\nif (!job->s_fence)\r\nreturn -ENOMEM;\r\njob->id = atomic64_inc_return(&sched->job_id_count);\r\nINIT_WORK(&job->finish_work, amd_sched_job_finish);\r\nINIT_LIST_HEAD(&job->node);\r\nINIT_DELAYED_WORK(&job->work_tdr, amd_sched_job_timedout);\r\nreturn 0;\r\n}\r\nstatic bool amd_sched_ready(struct amd_gpu_scheduler *sched)\r\n{\r\nreturn atomic_read(&sched->hw_rq_count) <\r\nsched->hw_submission_limit;\r\n}\r\nstatic void amd_sched_wakeup(struct amd_gpu_scheduler *sched)\r\n{\r\nif (amd_sched_ready(sched))\r\nwake_up_interruptible(&sched->wake_up_worker);\r\n}\r\nstatic struct amd_sched_entity *\r\namd_sched_select_entity(struct amd_gpu_scheduler *sched)\r\n{\r\nstruct amd_sched_entity *entity;\r\nint i;\r\nif (!amd_sched_ready(sched))\r\nreturn NULL;\r\nfor (i = AMD_SCHED_PRIORITY_MAX - 1; i >= AMD_SCHED_PRIORITY_MIN; i--) {\r\nentity = amd_sched_rq_select_entity(&sched->sched_rq[i]);\r\nif (entity)\r\nbreak;\r\n}\r\nreturn entity;\r\n}\r\nstatic void amd_sched_process_job(struct dma_fence *f, struct dma_fence_cb *cb)\r\n{\r\nstruct amd_sched_fence *s_fence =\r\ncontainer_of(cb, struct amd_sched_fence, cb);\r\nstruct amd_gpu_scheduler *sched = s_fence->sched;\r\natomic_dec(&sched->hw_rq_count);\r\namd_sched_fence_finished(s_fence);\r\ntrace_amd_sched_process_job(s_fence);\r\ndma_fence_put(&s_fence->finished);\r\nwake_up_interruptible(&sched->wake_up_worker);\r\n}\r\nstatic bool amd_sched_blocked(struct amd_gpu_scheduler *sched)\r\n{\r\nif (kthread_should_park()) {\r\nkthread_parkme();\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int amd_sched_main(void *param)\r\n{\r\nstruct sched_param sparam = {.sched_priority = 1};\r\nstruct amd_gpu_scheduler *sched = (struct amd_gpu_scheduler *)param;\r\nint r, count;\r\nsched_setscheduler(current, SCHED_FIFO, &sparam);\r\nwhile (!kthread_should_stop()) {\r\nstruct amd_sched_entity *entity = NULL;\r\nstruct amd_sched_fence *s_fence;\r\nstruct amd_sched_job *sched_job;\r\nstruct dma_fence *fence;\r\nwait_event_interruptible(sched->wake_up_worker,\r\n(!amd_sched_blocked(sched) &&\r\n(entity = amd_sched_select_entity(sched))) ||\r\nkthread_should_stop());\r\nif (!entity)\r\ncontinue;\r\nsched_job = amd_sched_entity_pop_job(entity);\r\nif (!sched_job)\r\ncontinue;\r\ns_fence = sched_job->s_fence;\r\natomic_inc(&sched->hw_rq_count);\r\namd_sched_job_begin(sched_job);\r\nfence = sched->ops->run_job(sched_job);\r\namd_sched_fence_scheduled(s_fence);\r\nif (fence) {\r\ns_fence->parent = dma_fence_get(fence);\r\nr = dma_fence_add_callback(fence, &s_fence->cb,\r\namd_sched_process_job);\r\nif (r == -ENOENT)\r\namd_sched_process_job(fence, &s_fence->cb);\r\nelse if (r)\r\nDRM_ERROR("fence add callback failed (%d)\n",\r\nr);\r\ndma_fence_put(fence);\r\n} else {\r\nDRM_ERROR("Failed to run job!\n");\r\namd_sched_process_job(NULL, &s_fence->cb);\r\n}\r\ncount = kfifo_out(&entity->job_queue, &sched_job,\r\nsizeof(sched_job));\r\nWARN_ON(count != sizeof(sched_job));\r\nwake_up(&sched->job_scheduled);\r\n}\r\nreturn 0;\r\n}\r\nint amd_sched_init(struct amd_gpu_scheduler *sched,\r\nconst struct amd_sched_backend_ops *ops,\r\nunsigned hw_submission, long timeout, const char *name)\r\n{\r\nint i;\r\nsched->ops = ops;\r\nsched->hw_submission_limit = hw_submission;\r\nsched->name = name;\r\nsched->timeout = timeout;\r\nfor (i = AMD_SCHED_PRIORITY_MIN; i < AMD_SCHED_PRIORITY_MAX; i++)\r\namd_sched_rq_init(&sched->sched_rq[i]);\r\ninit_waitqueue_head(&sched->wake_up_worker);\r\ninit_waitqueue_head(&sched->job_scheduled);\r\nINIT_LIST_HEAD(&sched->ring_mirror_list);\r\nspin_lock_init(&sched->job_list_lock);\r\natomic_set(&sched->hw_rq_count, 0);\r\natomic64_set(&sched->job_id_count, 0);\r\nsched->thread = kthread_run(amd_sched_main, sched, sched->name);\r\nif (IS_ERR(sched->thread)) {\r\nDRM_ERROR("Failed to create scheduler for %s.\n", name);\r\nreturn PTR_ERR(sched->thread);\r\n}\r\nreturn 0;\r\n}\r\nvoid amd_sched_fini(struct amd_gpu_scheduler *sched)\r\n{\r\nif (sched->thread)\r\nkthread_stop(sched->thread);\r\n}
