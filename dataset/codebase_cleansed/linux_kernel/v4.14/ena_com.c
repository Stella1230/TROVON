static inline int ena_com_mem_addr_set(struct ena_com_dev *ena_dev,\r\nstruct ena_common_mem_addr *ena_addr,\r\ndma_addr_t addr)\r\n{\r\nif ((addr & GENMASK_ULL(ena_dev->dma_addr_bits - 1, 0)) != addr) {\r\npr_err("dma address has more bits that the device supports\n");\r\nreturn -EINVAL;\r\n}\r\nena_addr->mem_addr_low = lower_32_bits(addr);\r\nena_addr->mem_addr_high = (u16)upper_32_bits(addr);\r\nreturn 0;\r\n}\r\nstatic int ena_com_admin_init_sq(struct ena_com_admin_queue *queue)\r\n{\r\nstruct ena_com_admin_sq *sq = &queue->sq;\r\nu16 size = ADMIN_SQ_SIZE(queue->q_depth);\r\nsq->entries = dma_zalloc_coherent(queue->q_dmadev, size, &sq->dma_addr,\r\nGFP_KERNEL);\r\nif (!sq->entries) {\r\npr_err("memory allocation failed");\r\nreturn -ENOMEM;\r\n}\r\nsq->head = 0;\r\nsq->tail = 0;\r\nsq->phase = 1;\r\nsq->db_addr = NULL;\r\nreturn 0;\r\n}\r\nstatic int ena_com_admin_init_cq(struct ena_com_admin_queue *queue)\r\n{\r\nstruct ena_com_admin_cq *cq = &queue->cq;\r\nu16 size = ADMIN_CQ_SIZE(queue->q_depth);\r\ncq->entries = dma_zalloc_coherent(queue->q_dmadev, size, &cq->dma_addr,\r\nGFP_KERNEL);\r\nif (!cq->entries) {\r\npr_err("memory allocation failed");\r\nreturn -ENOMEM;\r\n}\r\ncq->head = 0;\r\ncq->phase = 1;\r\nreturn 0;\r\n}\r\nstatic int ena_com_admin_init_aenq(struct ena_com_dev *dev,\r\nstruct ena_aenq_handlers *aenq_handlers)\r\n{\r\nstruct ena_com_aenq *aenq = &dev->aenq;\r\nu32 addr_low, addr_high, aenq_caps;\r\nu16 size;\r\ndev->aenq.q_depth = ENA_ASYNC_QUEUE_DEPTH;\r\nsize = ADMIN_AENQ_SIZE(ENA_ASYNC_QUEUE_DEPTH);\r\naenq->entries = dma_zalloc_coherent(dev->dmadev, size, &aenq->dma_addr,\r\nGFP_KERNEL);\r\nif (!aenq->entries) {\r\npr_err("memory allocation failed");\r\nreturn -ENOMEM;\r\n}\r\naenq->head = aenq->q_depth;\r\naenq->phase = 1;\r\naddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(aenq->dma_addr);\r\naddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(aenq->dma_addr);\r\nwritel(addr_low, dev->reg_bar + ENA_REGS_AENQ_BASE_LO_OFF);\r\nwritel(addr_high, dev->reg_bar + ENA_REGS_AENQ_BASE_HI_OFF);\r\naenq_caps = 0;\r\naenq_caps |= dev->aenq.q_depth & ENA_REGS_AENQ_CAPS_AENQ_DEPTH_MASK;\r\naenq_caps |= (sizeof(struct ena_admin_aenq_entry)\r\n<< ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_SHIFT) &\r\nENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_MASK;\r\nwritel(aenq_caps, dev->reg_bar + ENA_REGS_AENQ_CAPS_OFF);\r\nif (unlikely(!aenq_handlers)) {\r\npr_err("aenq handlers pointer is NULL\n");\r\nreturn -EINVAL;\r\n}\r\naenq->aenq_handlers = aenq_handlers;\r\nreturn 0;\r\n}\r\nstatic inline void comp_ctxt_release(struct ena_com_admin_queue *queue,\r\nstruct ena_comp_ctx *comp_ctx)\r\n{\r\ncomp_ctx->occupied = false;\r\natomic_dec(&queue->outstanding_cmds);\r\n}\r\nstatic struct ena_comp_ctx *get_comp_ctxt(struct ena_com_admin_queue *queue,\r\nu16 command_id, bool capture)\r\n{\r\nif (unlikely(command_id >= queue->q_depth)) {\r\npr_err("command id is larger than the queue size. cmd_id: %u queue size %d\n",\r\ncommand_id, queue->q_depth);\r\nreturn NULL;\r\n}\r\nif (unlikely(queue->comp_ctx[command_id].occupied && capture)) {\r\npr_err("Completion context is occupied\n");\r\nreturn NULL;\r\n}\r\nif (capture) {\r\natomic_inc(&queue->outstanding_cmds);\r\nqueue->comp_ctx[command_id].occupied = true;\r\n}\r\nreturn &queue->comp_ctx[command_id];\r\n}\r\nstatic struct ena_comp_ctx *__ena_com_submit_admin_cmd(struct ena_com_admin_queue *admin_queue,\r\nstruct ena_admin_aq_entry *cmd,\r\nsize_t cmd_size_in_bytes,\r\nstruct ena_admin_acq_entry *comp,\r\nsize_t comp_size_in_bytes)\r\n{\r\nstruct ena_comp_ctx *comp_ctx;\r\nu16 tail_masked, cmd_id;\r\nu16 queue_size_mask;\r\nu16 cnt;\r\nqueue_size_mask = admin_queue->q_depth - 1;\r\ntail_masked = admin_queue->sq.tail & queue_size_mask;\r\ncnt = atomic_read(&admin_queue->outstanding_cmds);\r\nif (cnt >= admin_queue->q_depth) {\r\npr_debug("admin queue is full.\n");\r\nadmin_queue->stats.out_of_space++;\r\nreturn ERR_PTR(-ENOSPC);\r\n}\r\ncmd_id = admin_queue->curr_cmd_id;\r\ncmd->aq_common_descriptor.flags |= admin_queue->sq.phase &\r\nENA_ADMIN_AQ_COMMON_DESC_PHASE_MASK;\r\ncmd->aq_common_descriptor.command_id |= cmd_id &\r\nENA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK;\r\ncomp_ctx = get_comp_ctxt(admin_queue, cmd_id, true);\r\nif (unlikely(!comp_ctx))\r\nreturn ERR_PTR(-EINVAL);\r\ncomp_ctx->status = ENA_CMD_SUBMITTED;\r\ncomp_ctx->comp_size = (u32)comp_size_in_bytes;\r\ncomp_ctx->user_cqe = comp;\r\ncomp_ctx->cmd_opcode = cmd->aq_common_descriptor.opcode;\r\nreinit_completion(&comp_ctx->wait_event);\r\nmemcpy(&admin_queue->sq.entries[tail_masked], cmd, cmd_size_in_bytes);\r\nadmin_queue->curr_cmd_id = (admin_queue->curr_cmd_id + 1) &\r\nqueue_size_mask;\r\nadmin_queue->sq.tail++;\r\nadmin_queue->stats.submitted_cmd++;\r\nif (unlikely((admin_queue->sq.tail & queue_size_mask) == 0))\r\nadmin_queue->sq.phase = !admin_queue->sq.phase;\r\nwritel(admin_queue->sq.tail, admin_queue->sq.db_addr);\r\nreturn comp_ctx;\r\n}\r\nstatic inline int ena_com_init_comp_ctxt(struct ena_com_admin_queue *queue)\r\n{\r\nsize_t size = queue->q_depth * sizeof(struct ena_comp_ctx);\r\nstruct ena_comp_ctx *comp_ctx;\r\nu16 i;\r\nqueue->comp_ctx = devm_kzalloc(queue->q_dmadev, size, GFP_KERNEL);\r\nif (unlikely(!queue->comp_ctx)) {\r\npr_err("memory allocation failed");\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < queue->q_depth; i++) {\r\ncomp_ctx = get_comp_ctxt(queue, i, false);\r\nif (comp_ctx)\r\ninit_completion(&comp_ctx->wait_event);\r\n}\r\nreturn 0;\r\n}\r\nstatic struct ena_comp_ctx *ena_com_submit_admin_cmd(struct ena_com_admin_queue *admin_queue,\r\nstruct ena_admin_aq_entry *cmd,\r\nsize_t cmd_size_in_bytes,\r\nstruct ena_admin_acq_entry *comp,\r\nsize_t comp_size_in_bytes)\r\n{\r\nunsigned long flags;\r\nstruct ena_comp_ctx *comp_ctx;\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\nif (unlikely(!admin_queue->running_state)) {\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\nreturn ERR_PTR(-ENODEV);\r\n}\r\ncomp_ctx = __ena_com_submit_admin_cmd(admin_queue, cmd,\r\ncmd_size_in_bytes,\r\ncomp,\r\ncomp_size_in_bytes);\r\nif (unlikely(IS_ERR(comp_ctx)))\r\nadmin_queue->running_state = false;\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\nreturn comp_ctx;\r\n}\r\nstatic int ena_com_init_io_sq(struct ena_com_dev *ena_dev,\r\nstruct ena_com_create_io_ctx *ctx,\r\nstruct ena_com_io_sq *io_sq)\r\n{\r\nsize_t size;\r\nint dev_node = 0;\r\nmemset(&io_sq->desc_addr, 0x0, sizeof(io_sq->desc_addr));\r\nio_sq->desc_entry_size =\r\n(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?\r\nsizeof(struct ena_eth_io_tx_desc) :\r\nsizeof(struct ena_eth_io_rx_desc);\r\nsize = io_sq->desc_entry_size * io_sq->q_depth;\r\nif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST) {\r\ndev_node = dev_to_node(ena_dev->dmadev);\r\nset_dev_node(ena_dev->dmadev, ctx->numa_node);\r\nio_sq->desc_addr.virt_addr =\r\ndma_zalloc_coherent(ena_dev->dmadev, size,\r\n&io_sq->desc_addr.phys_addr,\r\nGFP_KERNEL);\r\nset_dev_node(ena_dev->dmadev, dev_node);\r\nif (!io_sq->desc_addr.virt_addr) {\r\nio_sq->desc_addr.virt_addr =\r\ndma_zalloc_coherent(ena_dev->dmadev, size,\r\n&io_sq->desc_addr.phys_addr,\r\nGFP_KERNEL);\r\n}\r\n} else {\r\ndev_node = dev_to_node(ena_dev->dmadev);\r\nset_dev_node(ena_dev->dmadev, ctx->numa_node);\r\nio_sq->desc_addr.virt_addr =\r\ndevm_kzalloc(ena_dev->dmadev, size, GFP_KERNEL);\r\nset_dev_node(ena_dev->dmadev, dev_node);\r\nif (!io_sq->desc_addr.virt_addr) {\r\nio_sq->desc_addr.virt_addr =\r\ndevm_kzalloc(ena_dev->dmadev, size, GFP_KERNEL);\r\n}\r\n}\r\nif (!io_sq->desc_addr.virt_addr) {\r\npr_err("memory allocation failed");\r\nreturn -ENOMEM;\r\n}\r\nio_sq->tail = 0;\r\nio_sq->next_to_comp = 0;\r\nio_sq->phase = 1;\r\nreturn 0;\r\n}\r\nstatic int ena_com_init_io_cq(struct ena_com_dev *ena_dev,\r\nstruct ena_com_create_io_ctx *ctx,\r\nstruct ena_com_io_cq *io_cq)\r\n{\r\nsize_t size;\r\nint prev_node = 0;\r\nmemset(&io_cq->cdesc_addr, 0x0, sizeof(io_cq->cdesc_addr));\r\nio_cq->cdesc_entry_size_in_bytes =\r\n(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?\r\nsizeof(struct ena_eth_io_tx_cdesc) :\r\nsizeof(struct ena_eth_io_rx_cdesc_base);\r\nsize = io_cq->cdesc_entry_size_in_bytes * io_cq->q_depth;\r\nprev_node = dev_to_node(ena_dev->dmadev);\r\nset_dev_node(ena_dev->dmadev, ctx->numa_node);\r\nio_cq->cdesc_addr.virt_addr =\r\ndma_zalloc_coherent(ena_dev->dmadev, size,\r\n&io_cq->cdesc_addr.phys_addr, GFP_KERNEL);\r\nset_dev_node(ena_dev->dmadev, prev_node);\r\nif (!io_cq->cdesc_addr.virt_addr) {\r\nio_cq->cdesc_addr.virt_addr =\r\ndma_zalloc_coherent(ena_dev->dmadev, size,\r\n&io_cq->cdesc_addr.phys_addr,\r\nGFP_KERNEL);\r\n}\r\nif (!io_cq->cdesc_addr.virt_addr) {\r\npr_err("memory allocation failed");\r\nreturn -ENOMEM;\r\n}\r\nio_cq->phase = 1;\r\nio_cq->head = 0;\r\nreturn 0;\r\n}\r\nstatic void ena_com_handle_single_admin_completion(struct ena_com_admin_queue *admin_queue,\r\nstruct ena_admin_acq_entry *cqe)\r\n{\r\nstruct ena_comp_ctx *comp_ctx;\r\nu16 cmd_id;\r\ncmd_id = cqe->acq_common_descriptor.command &\r\nENA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID_MASK;\r\ncomp_ctx = get_comp_ctxt(admin_queue, cmd_id, false);\r\nif (unlikely(!comp_ctx)) {\r\npr_err("comp_ctx is NULL. Changing the admin queue running state\n");\r\nadmin_queue->running_state = false;\r\nreturn;\r\n}\r\ncomp_ctx->status = ENA_CMD_COMPLETED;\r\ncomp_ctx->comp_status = cqe->acq_common_descriptor.status;\r\nif (comp_ctx->user_cqe)\r\nmemcpy(comp_ctx->user_cqe, (void *)cqe, comp_ctx->comp_size);\r\nif (!admin_queue->polling)\r\ncomplete(&comp_ctx->wait_event);\r\n}\r\nstatic void ena_com_handle_admin_completion(struct ena_com_admin_queue *admin_queue)\r\n{\r\nstruct ena_admin_acq_entry *cqe = NULL;\r\nu16 comp_num = 0;\r\nu16 head_masked;\r\nu8 phase;\r\nhead_masked = admin_queue->cq.head & (admin_queue->q_depth - 1);\r\nphase = admin_queue->cq.phase;\r\ncqe = &admin_queue->cq.entries[head_masked];\r\nwhile ((cqe->acq_common_descriptor.flags &\r\nENA_ADMIN_ACQ_COMMON_DESC_PHASE_MASK) == phase) {\r\nrmb();\r\nena_com_handle_single_admin_completion(admin_queue, cqe);\r\nhead_masked++;\r\ncomp_num++;\r\nif (unlikely(head_masked == admin_queue->q_depth)) {\r\nhead_masked = 0;\r\nphase = !phase;\r\n}\r\ncqe = &admin_queue->cq.entries[head_masked];\r\n}\r\nadmin_queue->cq.head += comp_num;\r\nadmin_queue->cq.phase = phase;\r\nadmin_queue->sq.head += comp_num;\r\nadmin_queue->stats.completed_cmd += comp_num;\r\n}\r\nstatic int ena_com_comp_status_to_errno(u8 comp_status)\r\n{\r\nif (unlikely(comp_status != 0))\r\npr_err("admin command failed[%u]\n", comp_status);\r\nif (unlikely(comp_status > ENA_ADMIN_UNKNOWN_ERROR))\r\nreturn -EINVAL;\r\nswitch (comp_status) {\r\ncase ENA_ADMIN_SUCCESS:\r\nreturn 0;\r\ncase ENA_ADMIN_RESOURCE_ALLOCATION_FAILURE:\r\nreturn -ENOMEM;\r\ncase ENA_ADMIN_UNSUPPORTED_OPCODE:\r\nreturn -EOPNOTSUPP;\r\ncase ENA_ADMIN_BAD_OPCODE:\r\ncase ENA_ADMIN_MALFORMED_REQUEST:\r\ncase ENA_ADMIN_ILLEGAL_PARAMETER:\r\ncase ENA_ADMIN_UNKNOWN_ERROR:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ena_com_wait_and_process_admin_cq_polling(struct ena_comp_ctx *comp_ctx,\r\nstruct ena_com_admin_queue *admin_queue)\r\n{\r\nunsigned long flags, timeout;\r\nint ret;\r\ntimeout = jiffies + usecs_to_jiffies(admin_queue->completion_timeout);\r\nwhile (1) {\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\nena_com_handle_admin_completion(admin_queue);\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\nif (comp_ctx->status != ENA_CMD_SUBMITTED)\r\nbreak;\r\nif (time_is_before_jiffies(timeout)) {\r\npr_err("Wait for completion (polling) timeout\n");\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\nadmin_queue->stats.no_completion++;\r\nadmin_queue->running_state = false;\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\nret = -ETIME;\r\ngoto err;\r\n}\r\nmsleep(100);\r\n}\r\nif (unlikely(comp_ctx->status == ENA_CMD_ABORTED)) {\r\npr_err("Command was aborted\n");\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\nadmin_queue->stats.aborted_cmd++;\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\nret = -ENODEV;\r\ngoto err;\r\n}\r\nWARN(comp_ctx->status != ENA_CMD_COMPLETED, "Invalid comp status %d\n",\r\ncomp_ctx->status);\r\nret = ena_com_comp_status_to_errno(comp_ctx->comp_status);\r\nerr:\r\ncomp_ctxt_release(admin_queue, comp_ctx);\r\nreturn ret;\r\n}\r\nstatic int ena_com_wait_and_process_admin_cq_interrupts(struct ena_comp_ctx *comp_ctx,\r\nstruct ena_com_admin_queue *admin_queue)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nwait_for_completion_timeout(&comp_ctx->wait_event,\r\nusecs_to_jiffies(\r\nadmin_queue->completion_timeout));\r\nif (unlikely(comp_ctx->status == ENA_CMD_SUBMITTED)) {\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\nena_com_handle_admin_completion(admin_queue);\r\nadmin_queue->stats.no_completion++;\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\nif (comp_ctx->status == ENA_CMD_COMPLETED)\r\npr_err("The ena device have completion but the driver didn't receive any MSI-X interrupt (cmd %d)\n",\r\ncomp_ctx->cmd_opcode);\r\nelse\r\npr_err("The ena device doesn't send any completion for the admin cmd %d status %d\n",\r\ncomp_ctx->cmd_opcode, comp_ctx->status);\r\nadmin_queue->running_state = false;\r\nret = -ETIME;\r\ngoto err;\r\n}\r\nret = ena_com_comp_status_to_errno(comp_ctx->comp_status);\r\nerr:\r\ncomp_ctxt_release(admin_queue, comp_ctx);\r\nreturn ret;\r\n}\r\nstatic u32 ena_com_reg_bar_read32(struct ena_com_dev *ena_dev, u16 offset)\r\n{\r\nstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\r\nvolatile struct ena_admin_ena_mmio_req_read_less_resp *read_resp =\r\nmmio_read->read_resp;\r\nu32 mmio_read_reg, ret, i;\r\nunsigned long flags;\r\nu32 timeout = mmio_read->reg_read_to;\r\nmight_sleep();\r\nif (timeout == 0)\r\ntimeout = ENA_REG_READ_TIMEOUT;\r\nif (!mmio_read->readless_supported)\r\nreturn readl(ena_dev->reg_bar + offset);\r\nspin_lock_irqsave(&mmio_read->lock, flags);\r\nmmio_read->seq_num++;\r\nread_resp->req_id = mmio_read->seq_num + 0xDEAD;\r\nmmio_read_reg = (offset << ENA_REGS_MMIO_REG_READ_REG_OFF_SHIFT) &\r\nENA_REGS_MMIO_REG_READ_REG_OFF_MASK;\r\nmmio_read_reg |= mmio_read->seq_num &\r\nENA_REGS_MMIO_REG_READ_REQ_ID_MASK;\r\nwmb();\r\nwritel(mmio_read_reg, ena_dev->reg_bar + ENA_REGS_MMIO_REG_READ_OFF);\r\nfor (i = 0; i < timeout; i++) {\r\nif (read_resp->req_id == mmio_read->seq_num)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (unlikely(i == timeout)) {\r\npr_err("reading reg failed for timeout. expected: req id[%hu] offset[%hu] actual: req id[%hu] offset[%hu]\n",\r\nmmio_read->seq_num, offset, read_resp->req_id,\r\nread_resp->reg_off);\r\nret = ENA_MMIO_READ_TIMEOUT;\r\ngoto err;\r\n}\r\nif (read_resp->reg_off != offset) {\r\npr_err("Read failure: wrong offset provided");\r\nret = ENA_MMIO_READ_TIMEOUT;\r\n} else {\r\nret = read_resp->reg_val;\r\n}\r\nerr:\r\nspin_unlock_irqrestore(&mmio_read->lock, flags);\r\nreturn ret;\r\n}\r\nstatic int ena_com_wait_and_process_admin_cq(struct ena_comp_ctx *comp_ctx,\r\nstruct ena_com_admin_queue *admin_queue)\r\n{\r\nif (admin_queue->polling)\r\nreturn ena_com_wait_and_process_admin_cq_polling(comp_ctx,\r\nadmin_queue);\r\nreturn ena_com_wait_and_process_admin_cq_interrupts(comp_ctx,\r\nadmin_queue);\r\n}\r\nstatic int ena_com_destroy_io_sq(struct ena_com_dev *ena_dev,\r\nstruct ena_com_io_sq *io_sq)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_admin_aq_destroy_sq_cmd destroy_cmd;\r\nstruct ena_admin_acq_destroy_sq_resp_desc destroy_resp;\r\nu8 direction;\r\nint ret;\r\nmemset(&destroy_cmd, 0x0, sizeof(destroy_cmd));\r\nif (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)\r\ndirection = ENA_ADMIN_SQ_DIRECTION_TX;\r\nelse\r\ndirection = ENA_ADMIN_SQ_DIRECTION_RX;\r\ndestroy_cmd.sq.sq_identity |= (direction <<\r\nENA_ADMIN_SQ_SQ_DIRECTION_SHIFT) &\r\nENA_ADMIN_SQ_SQ_DIRECTION_MASK;\r\ndestroy_cmd.sq.sq_idx = io_sq->idx;\r\ndestroy_cmd.aq_common_descriptor.opcode = ENA_ADMIN_DESTROY_SQ;\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&destroy_cmd,\r\nsizeof(destroy_cmd),\r\n(struct ena_admin_acq_entry *)&destroy_resp,\r\nsizeof(destroy_resp));\r\nif (unlikely(ret && (ret != -ENODEV)))\r\npr_err("failed to destroy io sq error: %d\n", ret);\r\nreturn ret;\r\n}\r\nstatic void ena_com_io_queue_free(struct ena_com_dev *ena_dev,\r\nstruct ena_com_io_sq *io_sq,\r\nstruct ena_com_io_cq *io_cq)\r\n{\r\nsize_t size;\r\nif (io_cq->cdesc_addr.virt_addr) {\r\nsize = io_cq->cdesc_entry_size_in_bytes * io_cq->q_depth;\r\ndma_free_coherent(ena_dev->dmadev, size,\r\nio_cq->cdesc_addr.virt_addr,\r\nio_cq->cdesc_addr.phys_addr);\r\nio_cq->cdesc_addr.virt_addr = NULL;\r\n}\r\nif (io_sq->desc_addr.virt_addr) {\r\nsize = io_sq->desc_entry_size * io_sq->q_depth;\r\nif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST)\r\ndma_free_coherent(ena_dev->dmadev, size,\r\nio_sq->desc_addr.virt_addr,\r\nio_sq->desc_addr.phys_addr);\r\nelse\r\ndevm_kfree(ena_dev->dmadev, io_sq->desc_addr.virt_addr);\r\nio_sq->desc_addr.virt_addr = NULL;\r\n}\r\n}\r\nstatic int wait_for_reset_state(struct ena_com_dev *ena_dev, u32 timeout,\r\nu16 exp_state)\r\n{\r\nu32 val, i;\r\nfor (i = 0; i < timeout; i++) {\r\nval = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);\r\nif (unlikely(val == ENA_MMIO_READ_TIMEOUT)) {\r\npr_err("Reg read timeout occurred\n");\r\nreturn -ETIME;\r\n}\r\nif ((val & ENA_REGS_DEV_STS_RESET_IN_PROGRESS_MASK) ==\r\nexp_state)\r\nreturn 0;\r\nmsleep(100);\r\n}\r\nreturn -ETIME;\r\n}\r\nstatic bool ena_com_check_supported_feature_id(struct ena_com_dev *ena_dev,\r\nenum ena_admin_aq_feature_id feature_id)\r\n{\r\nu32 feature_mask = 1 << feature_id;\r\nif ((feature_id != ENA_ADMIN_DEVICE_ATTRIBUTES) &&\r\n!(ena_dev->supported_features & feature_mask))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int ena_com_get_feature_ex(struct ena_com_dev *ena_dev,\r\nstruct ena_admin_get_feat_resp *get_resp,\r\nenum ena_admin_aq_feature_id feature_id,\r\ndma_addr_t control_buf_dma_addr,\r\nu32 control_buff_size)\r\n{\r\nstruct ena_com_admin_queue *admin_queue;\r\nstruct ena_admin_get_feat_cmd get_cmd;\r\nint ret;\r\nif (!ena_com_check_supported_feature_id(ena_dev, feature_id)) {\r\npr_debug("Feature %d isn't supported\n", feature_id);\r\nreturn -EOPNOTSUPP;\r\n}\r\nmemset(&get_cmd, 0x0, sizeof(get_cmd));\r\nadmin_queue = &ena_dev->admin_queue;\r\nget_cmd.aq_common_descriptor.opcode = ENA_ADMIN_GET_FEATURE;\r\nif (control_buff_size)\r\nget_cmd.aq_common_descriptor.flags =\r\nENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\r\nelse\r\nget_cmd.aq_common_descriptor.flags = 0;\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&get_cmd.control_buffer.address,\r\ncontrol_buf_dma_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\nget_cmd.control_buffer.length = control_buff_size;\r\nget_cmd.feat_common.feature_id = feature_id;\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)\r\n&get_cmd,\r\nsizeof(get_cmd),\r\n(struct ena_admin_acq_entry *)\r\nget_resp,\r\nsizeof(*get_resp));\r\nif (unlikely(ret))\r\npr_err("Failed to submit get_feature command %d error: %d\n",\r\nfeature_id, ret);\r\nreturn ret;\r\n}\r\nstatic int ena_com_get_feature(struct ena_com_dev *ena_dev,\r\nstruct ena_admin_get_feat_resp *get_resp,\r\nenum ena_admin_aq_feature_id feature_id)\r\n{\r\nreturn ena_com_get_feature_ex(ena_dev,\r\nget_resp,\r\nfeature_id,\r\n0,\r\n0);\r\n}\r\nstatic int ena_com_hash_key_allocate(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nrss->hash_key =\r\ndma_zalloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_key),\r\n&rss->hash_key_dma_addr, GFP_KERNEL);\r\nif (unlikely(!rss->hash_key))\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void ena_com_hash_key_destroy(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nif (rss->hash_key)\r\ndma_free_coherent(ena_dev->dmadev, sizeof(*rss->hash_key),\r\nrss->hash_key, rss->hash_key_dma_addr);\r\nrss->hash_key = NULL;\r\n}\r\nstatic int ena_com_hash_ctrl_init(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nrss->hash_ctrl =\r\ndma_zalloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_ctrl),\r\n&rss->hash_ctrl_dma_addr, GFP_KERNEL);\r\nif (unlikely(!rss->hash_ctrl))\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void ena_com_hash_ctrl_destroy(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nif (rss->hash_ctrl)\r\ndma_free_coherent(ena_dev->dmadev, sizeof(*rss->hash_ctrl),\r\nrss->hash_ctrl, rss->hash_ctrl_dma_addr);\r\nrss->hash_ctrl = NULL;\r\n}\r\nstatic int ena_com_indirect_table_allocate(struct ena_com_dev *ena_dev,\r\nu16 log_size)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_get_feat_resp get_resp;\r\nsize_t tbl_size;\r\nint ret;\r\nret = ena_com_get_feature(ena_dev, &get_resp,\r\nENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG);\r\nif (unlikely(ret))\r\nreturn ret;\r\nif ((get_resp.u.ind_table.min_size > log_size) ||\r\n(get_resp.u.ind_table.max_size < log_size)) {\r\npr_err("indirect table size doesn't fit. requested size: %d while min is:%d and max %d\n",\r\n1 << log_size, 1 << get_resp.u.ind_table.min_size,\r\n1 << get_resp.u.ind_table.max_size);\r\nreturn -EINVAL;\r\n}\r\ntbl_size = (1ULL << log_size) *\r\nsizeof(struct ena_admin_rss_ind_table_entry);\r\nrss->rss_ind_tbl =\r\ndma_zalloc_coherent(ena_dev->dmadev, tbl_size,\r\n&rss->rss_ind_tbl_dma_addr, GFP_KERNEL);\r\nif (unlikely(!rss->rss_ind_tbl))\r\ngoto mem_err1;\r\ntbl_size = (1ULL << log_size) * sizeof(u16);\r\nrss->host_rss_ind_tbl =\r\ndevm_kzalloc(ena_dev->dmadev, tbl_size, GFP_KERNEL);\r\nif (unlikely(!rss->host_rss_ind_tbl))\r\ngoto mem_err2;\r\nrss->tbl_log_size = log_size;\r\nreturn 0;\r\nmem_err2:\r\ntbl_size = (1ULL << log_size) *\r\nsizeof(struct ena_admin_rss_ind_table_entry);\r\ndma_free_coherent(ena_dev->dmadev, tbl_size, rss->rss_ind_tbl,\r\nrss->rss_ind_tbl_dma_addr);\r\nrss->rss_ind_tbl = NULL;\r\nmem_err1:\r\nrss->tbl_log_size = 0;\r\nreturn -ENOMEM;\r\n}\r\nstatic void ena_com_indirect_table_destroy(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nsize_t tbl_size = (1ULL << rss->tbl_log_size) *\r\nsizeof(struct ena_admin_rss_ind_table_entry);\r\nif (rss->rss_ind_tbl)\r\ndma_free_coherent(ena_dev->dmadev, tbl_size, rss->rss_ind_tbl,\r\nrss->rss_ind_tbl_dma_addr);\r\nrss->rss_ind_tbl = NULL;\r\nif (rss->host_rss_ind_tbl)\r\ndevm_kfree(ena_dev->dmadev, rss->host_rss_ind_tbl);\r\nrss->host_rss_ind_tbl = NULL;\r\n}\r\nstatic int ena_com_create_io_sq(struct ena_com_dev *ena_dev,\r\nstruct ena_com_io_sq *io_sq, u16 cq_idx)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_admin_aq_create_sq_cmd create_cmd;\r\nstruct ena_admin_acq_create_sq_resp_desc cmd_completion;\r\nu8 direction;\r\nint ret;\r\nmemset(&create_cmd, 0x0, sizeof(create_cmd));\r\ncreate_cmd.aq_common_descriptor.opcode = ENA_ADMIN_CREATE_SQ;\r\nif (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)\r\ndirection = ENA_ADMIN_SQ_DIRECTION_TX;\r\nelse\r\ndirection = ENA_ADMIN_SQ_DIRECTION_RX;\r\ncreate_cmd.sq_identity |= (direction <<\r\nENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_SHIFT) &\r\nENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_MASK;\r\ncreate_cmd.sq_caps_2 |= io_sq->mem_queue_type &\r\nENA_ADMIN_AQ_CREATE_SQ_CMD_PLACEMENT_POLICY_MASK;\r\ncreate_cmd.sq_caps_2 |= (ENA_ADMIN_COMPLETION_POLICY_DESC <<\r\nENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_SHIFT) &\r\nENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_MASK;\r\ncreate_cmd.sq_caps_3 |=\r\nENA_ADMIN_AQ_CREATE_SQ_CMD_IS_PHYSICALLY_CONTIGUOUS_MASK;\r\ncreate_cmd.cq_idx = cq_idx;\r\ncreate_cmd.sq_depth = io_sq->q_depth;\r\nif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST) {\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&create_cmd.sq_ba,\r\nio_sq->desc_addr.phys_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\n}\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&create_cmd,\r\nsizeof(create_cmd),\r\n(struct ena_admin_acq_entry *)&cmd_completion,\r\nsizeof(cmd_completion));\r\nif (unlikely(ret)) {\r\npr_err("Failed to create IO SQ. error: %d\n", ret);\r\nreturn ret;\r\n}\r\nio_sq->idx = cmd_completion.sq_idx;\r\nio_sq->db_addr = (u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\r\n(uintptr_t)cmd_completion.sq_doorbell_offset);\r\nif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {\r\nio_sq->header_addr = (u8 __iomem *)((uintptr_t)ena_dev->mem_bar\r\n+ cmd_completion.llq_headers_offset);\r\nio_sq->desc_addr.pbuf_dev_addr =\r\n(u8 __iomem *)((uintptr_t)ena_dev->mem_bar +\r\ncmd_completion.llq_descriptors_offset);\r\n}\r\npr_debug("created sq[%u], depth[%u]\n", io_sq->idx, io_sq->q_depth);\r\nreturn ret;\r\n}\r\nstatic int ena_com_ind_tbl_convert_to_device(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_com_io_sq *io_sq;\r\nu16 qid;\r\nint i;\r\nfor (i = 0; i < 1 << rss->tbl_log_size; i++) {\r\nqid = rss->host_rss_ind_tbl[i];\r\nif (qid >= ENA_TOTAL_NUM_QUEUES)\r\nreturn -EINVAL;\r\nio_sq = &ena_dev->io_sq_queues[qid];\r\nif (io_sq->direction != ENA_COM_IO_QUEUE_DIRECTION_RX)\r\nreturn -EINVAL;\r\nrss->rss_ind_tbl[i].cq_idx = io_sq->idx;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ena_com_ind_tbl_convert_from_device(struct ena_com_dev *ena_dev)\r\n{\r\nu16 dev_idx_to_host_tbl[ENA_TOTAL_NUM_QUEUES] = { (u16)-1 };\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nu8 idx;\r\nu16 i;\r\nfor (i = 0; i < ENA_TOTAL_NUM_QUEUES; i++)\r\ndev_idx_to_host_tbl[ena_dev->io_sq_queues[i].idx] = i;\r\nfor (i = 0; i < 1 << rss->tbl_log_size; i++) {\r\nif (rss->rss_ind_tbl[i].cq_idx > ENA_TOTAL_NUM_QUEUES)\r\nreturn -EINVAL;\r\nidx = (u8)rss->rss_ind_tbl[i].cq_idx;\r\nif (dev_idx_to_host_tbl[idx] > ENA_TOTAL_NUM_QUEUES)\r\nreturn -EINVAL;\r\nrss->host_rss_ind_tbl[i] = dev_idx_to_host_tbl[idx];\r\n}\r\nreturn 0;\r\n}\r\nstatic int ena_com_init_interrupt_moderation_table(struct ena_com_dev *ena_dev)\r\n{\r\nsize_t size;\r\nsize = sizeof(struct ena_intr_moder_entry) * ENA_INTR_MAX_NUM_OF_LEVELS;\r\nena_dev->intr_moder_tbl =\r\ndevm_kzalloc(ena_dev->dmadev, size, GFP_KERNEL);\r\nif (!ena_dev->intr_moder_tbl)\r\nreturn -ENOMEM;\r\nena_com_config_default_interrupt_moderation_table(ena_dev);\r\nreturn 0;\r\n}\r\nstatic void ena_com_update_intr_delay_resolution(struct ena_com_dev *ena_dev,\r\nu16 intr_delay_resolution)\r\n{\r\nstruct ena_intr_moder_entry *intr_moder_tbl = ena_dev->intr_moder_tbl;\r\nunsigned int i;\r\nif (!intr_delay_resolution) {\r\npr_err("Illegal intr_delay_resolution provided. Going to use default 1 usec resolution\n");\r\nintr_delay_resolution = 1;\r\n}\r\nena_dev->intr_delay_resolution = intr_delay_resolution;\r\nfor (i = 0; i < ENA_INTR_MAX_NUM_OF_LEVELS; i++)\r\nintr_moder_tbl[i].intr_moder_interval /= intr_delay_resolution;\r\nena_dev->intr_moder_tx_interval /= intr_delay_resolution;\r\n}\r\nint ena_com_execute_admin_command(struct ena_com_admin_queue *admin_queue,\r\nstruct ena_admin_aq_entry *cmd,\r\nsize_t cmd_size,\r\nstruct ena_admin_acq_entry *comp,\r\nsize_t comp_size)\r\n{\r\nstruct ena_comp_ctx *comp_ctx;\r\nint ret;\r\ncomp_ctx = ena_com_submit_admin_cmd(admin_queue, cmd, cmd_size,\r\ncomp, comp_size);\r\nif (unlikely(IS_ERR(comp_ctx))) {\r\nif (comp_ctx == ERR_PTR(-ENODEV))\r\npr_debug("Failed to submit command [%ld]\n",\r\nPTR_ERR(comp_ctx));\r\nelse\r\npr_err("Failed to submit command [%ld]\n",\r\nPTR_ERR(comp_ctx));\r\nreturn PTR_ERR(comp_ctx);\r\n}\r\nret = ena_com_wait_and_process_admin_cq(comp_ctx, admin_queue);\r\nif (unlikely(ret)) {\r\nif (admin_queue->running_state)\r\npr_err("Failed to process command. ret = %d\n", ret);\r\nelse\r\npr_debug("Failed to process command. ret = %d\n", ret);\r\n}\r\nreturn ret;\r\n}\r\nint ena_com_create_io_cq(struct ena_com_dev *ena_dev,\r\nstruct ena_com_io_cq *io_cq)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_admin_aq_create_cq_cmd create_cmd;\r\nstruct ena_admin_acq_create_cq_resp_desc cmd_completion;\r\nint ret;\r\nmemset(&create_cmd, 0x0, sizeof(create_cmd));\r\ncreate_cmd.aq_common_descriptor.opcode = ENA_ADMIN_CREATE_CQ;\r\ncreate_cmd.cq_caps_2 |= (io_cq->cdesc_entry_size_in_bytes / 4) &\r\nENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_ENTRY_SIZE_WORDS_MASK;\r\ncreate_cmd.cq_caps_1 |=\r\nENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_MASK;\r\ncreate_cmd.msix_vector = io_cq->msix_vector;\r\ncreate_cmd.cq_depth = io_cq->q_depth;\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&create_cmd.cq_ba,\r\nio_cq->cdesc_addr.phys_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&create_cmd,\r\nsizeof(create_cmd),\r\n(struct ena_admin_acq_entry *)&cmd_completion,\r\nsizeof(cmd_completion));\r\nif (unlikely(ret)) {\r\npr_err("Failed to create IO CQ. error: %d\n", ret);\r\nreturn ret;\r\n}\r\nio_cq->idx = cmd_completion.cq_idx;\r\nio_cq->unmask_reg = (u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\r\ncmd_completion.cq_interrupt_unmask_register_offset);\r\nif (cmd_completion.cq_head_db_register_offset)\r\nio_cq->cq_head_db_reg =\r\n(u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\r\ncmd_completion.cq_head_db_register_offset);\r\nif (cmd_completion.numa_node_register_offset)\r\nio_cq->numa_node_cfg_reg =\r\n(u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\r\ncmd_completion.numa_node_register_offset);\r\npr_debug("created cq[%u], depth[%u]\n", io_cq->idx, io_cq->q_depth);\r\nreturn ret;\r\n}\r\nint ena_com_get_io_handlers(struct ena_com_dev *ena_dev, u16 qid,\r\nstruct ena_com_io_sq **io_sq,\r\nstruct ena_com_io_cq **io_cq)\r\n{\r\nif (qid >= ENA_TOTAL_NUM_QUEUES) {\r\npr_err("Invalid queue number %d but the max is %d\n", qid,\r\nENA_TOTAL_NUM_QUEUES);\r\nreturn -EINVAL;\r\n}\r\n*io_sq = &ena_dev->io_sq_queues[qid];\r\n*io_cq = &ena_dev->io_cq_queues[qid];\r\nreturn 0;\r\n}\r\nvoid ena_com_abort_admin_commands(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_comp_ctx *comp_ctx;\r\nu16 i;\r\nif (!admin_queue->comp_ctx)\r\nreturn;\r\nfor (i = 0; i < admin_queue->q_depth; i++) {\r\ncomp_ctx = get_comp_ctxt(admin_queue, i, false);\r\nif (unlikely(!comp_ctx))\r\nbreak;\r\ncomp_ctx->status = ENA_CMD_ABORTED;\r\ncomplete(&comp_ctx->wait_event);\r\n}\r\n}\r\nvoid ena_com_wait_for_abort_completion(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nunsigned long flags;\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\nwhile (atomic_read(&admin_queue->outstanding_cmds) != 0) {\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\nmsleep(20);\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\n}\r\nint ena_com_destroy_io_cq(struct ena_com_dev *ena_dev,\r\nstruct ena_com_io_cq *io_cq)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_admin_aq_destroy_cq_cmd destroy_cmd;\r\nstruct ena_admin_acq_destroy_cq_resp_desc destroy_resp;\r\nint ret;\r\nmemset(&destroy_cmd, 0x0, sizeof(destroy_cmd));\r\ndestroy_cmd.cq_idx = io_cq->idx;\r\ndestroy_cmd.aq_common_descriptor.opcode = ENA_ADMIN_DESTROY_CQ;\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&destroy_cmd,\r\nsizeof(destroy_cmd),\r\n(struct ena_admin_acq_entry *)&destroy_resp,\r\nsizeof(destroy_resp));\r\nif (unlikely(ret && (ret != -ENODEV)))\r\npr_err("Failed to destroy IO CQ. error: %d\n", ret);\r\nreturn ret;\r\n}\r\nbool ena_com_get_admin_running_state(struct ena_com_dev *ena_dev)\r\n{\r\nreturn ena_dev->admin_queue.running_state;\r\n}\r\nvoid ena_com_set_admin_running_state(struct ena_com_dev *ena_dev, bool state)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nunsigned long flags;\r\nspin_lock_irqsave(&admin_queue->q_lock, flags);\r\nena_dev->admin_queue.running_state = state;\r\nspin_unlock_irqrestore(&admin_queue->q_lock, flags);\r\n}\r\nvoid ena_com_admin_aenq_enable(struct ena_com_dev *ena_dev)\r\n{\r\nu16 depth = ena_dev->aenq.q_depth;\r\nWARN(ena_dev->aenq.head != depth, "Invalid AENQ state\n");\r\nwritel(depth, ena_dev->reg_bar + ENA_REGS_AENQ_HEAD_DB_OFF);\r\n}\r\nint ena_com_set_aenq_config(struct ena_com_dev *ena_dev, u32 groups_flag)\r\n{\r\nstruct ena_com_admin_queue *admin_queue;\r\nstruct ena_admin_set_feat_cmd cmd;\r\nstruct ena_admin_set_feat_resp resp;\r\nstruct ena_admin_get_feat_resp get_resp;\r\nint ret;\r\nret = ena_com_get_feature(ena_dev, &get_resp, ENA_ADMIN_AENQ_CONFIG);\r\nif (ret) {\r\npr_info("Can't get aenq configuration\n");\r\nreturn ret;\r\n}\r\nif ((get_resp.u.aenq.supported_groups & groups_flag) != groups_flag) {\r\npr_warn("Trying to set unsupported aenq events. supported flag: %x asked flag: %x\n",\r\nget_resp.u.aenq.supported_groups, groups_flag);\r\nreturn -EOPNOTSUPP;\r\n}\r\nmemset(&cmd, 0x0, sizeof(cmd));\r\nadmin_queue = &ena_dev->admin_queue;\r\ncmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\r\ncmd.aq_common_descriptor.flags = 0;\r\ncmd.feat_common.feature_id = ENA_ADMIN_AENQ_CONFIG;\r\ncmd.u.aenq.enabled_groups = groups_flag;\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&cmd,\r\nsizeof(cmd),\r\n(struct ena_admin_acq_entry *)&resp,\r\nsizeof(resp));\r\nif (unlikely(ret))\r\npr_err("Failed to config AENQ ret: %d\n", ret);\r\nreturn ret;\r\n}\r\nint ena_com_get_dma_width(struct ena_com_dev *ena_dev)\r\n{\r\nu32 caps = ena_com_reg_bar_read32(ena_dev, ENA_REGS_CAPS_OFF);\r\nint width;\r\nif (unlikely(caps == ENA_MMIO_READ_TIMEOUT)) {\r\npr_err("Reg read timeout occurred\n");\r\nreturn -ETIME;\r\n}\r\nwidth = (caps & ENA_REGS_CAPS_DMA_ADDR_WIDTH_MASK) >>\r\nENA_REGS_CAPS_DMA_ADDR_WIDTH_SHIFT;\r\npr_debug("ENA dma width: %d\n", width);\r\nif ((width < 32) || width > ENA_MAX_PHYS_ADDR_SIZE_BITS) {\r\npr_err("DMA width illegal value: %d\n", width);\r\nreturn -EINVAL;\r\n}\r\nena_dev->dma_addr_bits = width;\r\nreturn width;\r\n}\r\nint ena_com_validate_version(struct ena_com_dev *ena_dev)\r\n{\r\nu32 ver;\r\nu32 ctrl_ver;\r\nu32 ctrl_ver_masked;\r\nver = ena_com_reg_bar_read32(ena_dev, ENA_REGS_VERSION_OFF);\r\nctrl_ver = ena_com_reg_bar_read32(ena_dev,\r\nENA_REGS_CONTROLLER_VERSION_OFF);\r\nif (unlikely((ver == ENA_MMIO_READ_TIMEOUT) ||\r\n(ctrl_ver == ENA_MMIO_READ_TIMEOUT))) {\r\npr_err("Reg read timeout occurred\n");\r\nreturn -ETIME;\r\n}\r\npr_info("ena device version: %d.%d\n",\r\n(ver & ENA_REGS_VERSION_MAJOR_VERSION_MASK) >>\r\nENA_REGS_VERSION_MAJOR_VERSION_SHIFT,\r\nver & ENA_REGS_VERSION_MINOR_VERSION_MASK);\r\nif (ver < MIN_ENA_VER) {\r\npr_err("ENA version is lower than the minimal version the driver supports\n");\r\nreturn -1;\r\n}\r\npr_info("ena controller version: %d.%d.%d implementation version %d\n",\r\n(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_MASK) >>\r\nENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_SHIFT,\r\n(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_MASK) >>\r\nENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_SHIFT,\r\n(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION_MASK),\r\n(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_IMPL_ID_MASK) >>\r\nENA_REGS_CONTROLLER_VERSION_IMPL_ID_SHIFT);\r\nctrl_ver_masked =\r\n(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_MASK) |\r\n(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_MASK) |\r\n(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION_MASK);\r\nif (ctrl_ver_masked < MIN_ENA_CTRL_VER) {\r\npr_err("ENA ctrl version is lower than the minimal ctrl version the driver supports\n");\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nvoid ena_com_admin_destroy(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_com_admin_cq *cq = &admin_queue->cq;\r\nstruct ena_com_admin_sq *sq = &admin_queue->sq;\r\nstruct ena_com_aenq *aenq = &ena_dev->aenq;\r\nu16 size;\r\nif (admin_queue->comp_ctx)\r\ndevm_kfree(ena_dev->dmadev, admin_queue->comp_ctx);\r\nadmin_queue->comp_ctx = NULL;\r\nsize = ADMIN_SQ_SIZE(admin_queue->q_depth);\r\nif (sq->entries)\r\ndma_free_coherent(ena_dev->dmadev, size, sq->entries,\r\nsq->dma_addr);\r\nsq->entries = NULL;\r\nsize = ADMIN_CQ_SIZE(admin_queue->q_depth);\r\nif (cq->entries)\r\ndma_free_coherent(ena_dev->dmadev, size, cq->entries,\r\ncq->dma_addr);\r\ncq->entries = NULL;\r\nsize = ADMIN_AENQ_SIZE(aenq->q_depth);\r\nif (ena_dev->aenq.entries)\r\ndma_free_coherent(ena_dev->dmadev, size, aenq->entries,\r\naenq->dma_addr);\r\naenq->entries = NULL;\r\n}\r\nvoid ena_com_set_admin_polling_mode(struct ena_com_dev *ena_dev, bool polling)\r\n{\r\nu32 mask_value = 0;\r\nif (polling)\r\nmask_value = ENA_REGS_ADMIN_INTR_MASK;\r\nwritel(mask_value, ena_dev->reg_bar + ENA_REGS_INTR_MASK_OFF);\r\nena_dev->admin_queue.polling = polling;\r\n}\r\nint ena_com_mmio_reg_read_request_init(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\r\nspin_lock_init(&mmio_read->lock);\r\nmmio_read->read_resp =\r\ndma_zalloc_coherent(ena_dev->dmadev,\r\nsizeof(*mmio_read->read_resp),\r\n&mmio_read->read_resp_dma_addr, GFP_KERNEL);\r\nif (unlikely(!mmio_read->read_resp))\r\nreturn -ENOMEM;\r\nena_com_mmio_reg_read_request_write_dev_addr(ena_dev);\r\nmmio_read->read_resp->req_id = 0x0;\r\nmmio_read->seq_num = 0x0;\r\nmmio_read->readless_supported = true;\r\nreturn 0;\r\n}\r\nvoid ena_com_set_mmio_read_mode(struct ena_com_dev *ena_dev, bool readless_supported)\r\n{\r\nstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\r\nmmio_read->readless_supported = readless_supported;\r\n}\r\nvoid ena_com_mmio_reg_read_request_destroy(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\r\nwritel(0x0, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_LO_OFF);\r\nwritel(0x0, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_HI_OFF);\r\ndma_free_coherent(ena_dev->dmadev, sizeof(*mmio_read->read_resp),\r\nmmio_read->read_resp, mmio_read->read_resp_dma_addr);\r\nmmio_read->read_resp = NULL;\r\n}\r\nvoid ena_com_mmio_reg_read_request_write_dev_addr(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\r\nu32 addr_low, addr_high;\r\naddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(mmio_read->read_resp_dma_addr);\r\naddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(mmio_read->read_resp_dma_addr);\r\nwritel(addr_low, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_LO_OFF);\r\nwritel(addr_high, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_HI_OFF);\r\n}\r\nint ena_com_admin_init(struct ena_com_dev *ena_dev,\r\nstruct ena_aenq_handlers *aenq_handlers,\r\nbool init_spinlock)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nu32 aq_caps, acq_caps, dev_sts, addr_low, addr_high;\r\nint ret;\r\ndev_sts = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);\r\nif (unlikely(dev_sts == ENA_MMIO_READ_TIMEOUT)) {\r\npr_err("Reg read timeout occurred\n");\r\nreturn -ETIME;\r\n}\r\nif (!(dev_sts & ENA_REGS_DEV_STS_READY_MASK)) {\r\npr_err("Device isn't ready, abort com init\n");\r\nreturn -ENODEV;\r\n}\r\nadmin_queue->q_depth = ENA_ADMIN_QUEUE_DEPTH;\r\nadmin_queue->q_dmadev = ena_dev->dmadev;\r\nadmin_queue->polling = false;\r\nadmin_queue->curr_cmd_id = 0;\r\natomic_set(&admin_queue->outstanding_cmds, 0);\r\nif (init_spinlock)\r\nspin_lock_init(&admin_queue->q_lock);\r\nret = ena_com_init_comp_ctxt(admin_queue);\r\nif (ret)\r\ngoto error;\r\nret = ena_com_admin_init_sq(admin_queue);\r\nif (ret)\r\ngoto error;\r\nret = ena_com_admin_init_cq(admin_queue);\r\nif (ret)\r\ngoto error;\r\nadmin_queue->sq.db_addr = (u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\r\nENA_REGS_AQ_DB_OFF);\r\naddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->sq.dma_addr);\r\naddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->sq.dma_addr);\r\nwritel(addr_low, ena_dev->reg_bar + ENA_REGS_AQ_BASE_LO_OFF);\r\nwritel(addr_high, ena_dev->reg_bar + ENA_REGS_AQ_BASE_HI_OFF);\r\naddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->cq.dma_addr);\r\naddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->cq.dma_addr);\r\nwritel(addr_low, ena_dev->reg_bar + ENA_REGS_ACQ_BASE_LO_OFF);\r\nwritel(addr_high, ena_dev->reg_bar + ENA_REGS_ACQ_BASE_HI_OFF);\r\naq_caps = 0;\r\naq_caps |= admin_queue->q_depth & ENA_REGS_AQ_CAPS_AQ_DEPTH_MASK;\r\naq_caps |= (sizeof(struct ena_admin_aq_entry) <<\r\nENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_SHIFT) &\r\nENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_MASK;\r\nacq_caps = 0;\r\nacq_caps |= admin_queue->q_depth & ENA_REGS_ACQ_CAPS_ACQ_DEPTH_MASK;\r\nacq_caps |= (sizeof(struct ena_admin_acq_entry) <<\r\nENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_SHIFT) &\r\nENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_MASK;\r\nwritel(aq_caps, ena_dev->reg_bar + ENA_REGS_AQ_CAPS_OFF);\r\nwritel(acq_caps, ena_dev->reg_bar + ENA_REGS_ACQ_CAPS_OFF);\r\nret = ena_com_admin_init_aenq(ena_dev, aenq_handlers);\r\nif (ret)\r\ngoto error;\r\nadmin_queue->running_state = true;\r\nreturn 0;\r\nerror:\r\nena_com_admin_destroy(ena_dev);\r\nreturn ret;\r\n}\r\nint ena_com_create_io_queue(struct ena_com_dev *ena_dev,\r\nstruct ena_com_create_io_ctx *ctx)\r\n{\r\nstruct ena_com_io_sq *io_sq;\r\nstruct ena_com_io_cq *io_cq;\r\nint ret;\r\nif (ctx->qid >= ENA_TOTAL_NUM_QUEUES) {\r\npr_err("Qid (%d) is bigger than max num of queues (%d)\n",\r\nctx->qid, ENA_TOTAL_NUM_QUEUES);\r\nreturn -EINVAL;\r\n}\r\nio_sq = &ena_dev->io_sq_queues[ctx->qid];\r\nio_cq = &ena_dev->io_cq_queues[ctx->qid];\r\nmemset(io_sq, 0x0, sizeof(*io_sq));\r\nmemset(io_cq, 0x0, sizeof(*io_cq));\r\nio_cq->q_depth = ctx->queue_size;\r\nio_cq->direction = ctx->direction;\r\nio_cq->qid = ctx->qid;\r\nio_cq->msix_vector = ctx->msix_vector;\r\nio_sq->q_depth = ctx->queue_size;\r\nio_sq->direction = ctx->direction;\r\nio_sq->qid = ctx->qid;\r\nio_sq->mem_queue_type = ctx->mem_queue_type;\r\nif (ctx->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)\r\nio_sq->tx_max_header_size =\r\nmin_t(u32, ena_dev->tx_max_header_size, SZ_256);\r\nret = ena_com_init_io_sq(ena_dev, ctx, io_sq);\r\nif (ret)\r\ngoto error;\r\nret = ena_com_init_io_cq(ena_dev, ctx, io_cq);\r\nif (ret)\r\ngoto error;\r\nret = ena_com_create_io_cq(ena_dev, io_cq);\r\nif (ret)\r\ngoto error;\r\nret = ena_com_create_io_sq(ena_dev, io_sq, io_cq->idx);\r\nif (ret)\r\ngoto destroy_io_cq;\r\nreturn 0;\r\ndestroy_io_cq:\r\nena_com_destroy_io_cq(ena_dev, io_cq);\r\nerror:\r\nena_com_io_queue_free(ena_dev, io_sq, io_cq);\r\nreturn ret;\r\n}\r\nvoid ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid)\r\n{\r\nstruct ena_com_io_sq *io_sq;\r\nstruct ena_com_io_cq *io_cq;\r\nif (qid >= ENA_TOTAL_NUM_QUEUES) {\r\npr_err("Qid (%d) is bigger than max num of queues (%d)\n", qid,\r\nENA_TOTAL_NUM_QUEUES);\r\nreturn;\r\n}\r\nio_sq = &ena_dev->io_sq_queues[qid];\r\nio_cq = &ena_dev->io_cq_queues[qid];\r\nena_com_destroy_io_sq(ena_dev, io_sq);\r\nena_com_destroy_io_cq(ena_dev, io_cq);\r\nena_com_io_queue_free(ena_dev, io_sq, io_cq);\r\n}\r\nint ena_com_get_link_params(struct ena_com_dev *ena_dev,\r\nstruct ena_admin_get_feat_resp *resp)\r\n{\r\nreturn ena_com_get_feature(ena_dev, resp, ENA_ADMIN_LINK_CONFIG);\r\n}\r\nint ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,\r\nstruct ena_com_dev_get_features_ctx *get_feat_ctx)\r\n{\r\nstruct ena_admin_get_feat_resp get_resp;\r\nint rc;\r\nrc = ena_com_get_feature(ena_dev, &get_resp,\r\nENA_ADMIN_DEVICE_ATTRIBUTES);\r\nif (rc)\r\nreturn rc;\r\nmemcpy(&get_feat_ctx->dev_attr, &get_resp.u.dev_attr,\r\nsizeof(get_resp.u.dev_attr));\r\nena_dev->supported_features = get_resp.u.dev_attr.supported_features;\r\nrc = ena_com_get_feature(ena_dev, &get_resp,\r\nENA_ADMIN_MAX_QUEUES_NUM);\r\nif (rc)\r\nreturn rc;\r\nmemcpy(&get_feat_ctx->max_queues, &get_resp.u.max_queue,\r\nsizeof(get_resp.u.max_queue));\r\nena_dev->tx_max_header_size = get_resp.u.max_queue.max_header_size;\r\nrc = ena_com_get_feature(ena_dev, &get_resp,\r\nENA_ADMIN_AENQ_CONFIG);\r\nif (rc)\r\nreturn rc;\r\nmemcpy(&get_feat_ctx->aenq, &get_resp.u.aenq,\r\nsizeof(get_resp.u.aenq));\r\nrc = ena_com_get_feature(ena_dev, &get_resp,\r\nENA_ADMIN_STATELESS_OFFLOAD_CONFIG);\r\nif (rc)\r\nreturn rc;\r\nmemcpy(&get_feat_ctx->offload, &get_resp.u.offload,\r\nsizeof(get_resp.u.offload));\r\nrc = ena_com_get_feature(ena_dev, &get_resp, ENA_ADMIN_HW_HINTS);\r\nif (!rc)\r\nmemcpy(&get_feat_ctx->hw_hints, &get_resp.u.hw_hints,\r\nsizeof(get_resp.u.hw_hints));\r\nelse if (rc == -EOPNOTSUPP)\r\nmemset(&get_feat_ctx->hw_hints, 0x0,\r\nsizeof(get_feat_ctx->hw_hints));\r\nelse\r\nreturn rc;\r\nreturn 0;\r\n}\r\nvoid ena_com_admin_q_comp_intr_handler(struct ena_com_dev *ena_dev)\r\n{\r\nena_com_handle_admin_completion(&ena_dev->admin_queue);\r\n}\r\nstatic ena_aenq_handler ena_com_get_specific_aenq_cb(struct ena_com_dev *dev,\r\nu16 group)\r\n{\r\nstruct ena_aenq_handlers *aenq_handlers = dev->aenq.aenq_handlers;\r\nif ((group < ENA_MAX_HANDLERS) && aenq_handlers->handlers[group])\r\nreturn aenq_handlers->handlers[group];\r\nreturn aenq_handlers->unimplemented_handler;\r\n}\r\nvoid ena_com_aenq_intr_handler(struct ena_com_dev *dev, void *data)\r\n{\r\nstruct ena_admin_aenq_entry *aenq_e;\r\nstruct ena_admin_aenq_common_desc *aenq_common;\r\nstruct ena_com_aenq *aenq = &dev->aenq;\r\nena_aenq_handler handler_cb;\r\nu16 masked_head, processed = 0;\r\nu8 phase;\r\nmasked_head = aenq->head & (aenq->q_depth - 1);\r\nphase = aenq->phase;\r\naenq_e = &aenq->entries[masked_head];\r\naenq_common = &aenq_e->aenq_common_desc;\r\nwhile ((aenq_common->flags & ENA_ADMIN_AENQ_COMMON_DESC_PHASE_MASK) ==\r\nphase) {\r\npr_debug("AENQ! Group[%x] Syndrom[%x] timestamp: [%llus]\n",\r\naenq_common->group, aenq_common->syndrom,\r\n(u64)aenq_common->timestamp_low +\r\n((u64)aenq_common->timestamp_high << 32));\r\nhandler_cb = ena_com_get_specific_aenq_cb(dev,\r\naenq_common->group);\r\nhandler_cb(data, aenq_e);\r\nmasked_head++;\r\nprocessed++;\r\nif (unlikely(masked_head == aenq->q_depth)) {\r\nmasked_head = 0;\r\nphase = !phase;\r\n}\r\naenq_e = &aenq->entries[masked_head];\r\naenq_common = &aenq_e->aenq_common_desc;\r\n}\r\naenq->head += processed;\r\naenq->phase = phase;\r\nif (!processed)\r\nreturn;\r\nmb();\r\nwritel((u32)aenq->head, dev->reg_bar + ENA_REGS_AENQ_HEAD_DB_OFF);\r\n}\r\nint ena_com_dev_reset(struct ena_com_dev *ena_dev,\r\nenum ena_regs_reset_reason_types reset_reason)\r\n{\r\nu32 stat, timeout, cap, reset_val;\r\nint rc;\r\nstat = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);\r\ncap = ena_com_reg_bar_read32(ena_dev, ENA_REGS_CAPS_OFF);\r\nif (unlikely((stat == ENA_MMIO_READ_TIMEOUT) ||\r\n(cap == ENA_MMIO_READ_TIMEOUT))) {\r\npr_err("Reg read32 timeout occurred\n");\r\nreturn -ETIME;\r\n}\r\nif ((stat & ENA_REGS_DEV_STS_READY_MASK) == 0) {\r\npr_err("Device isn't ready, can't reset device\n");\r\nreturn -EINVAL;\r\n}\r\ntimeout = (cap & ENA_REGS_CAPS_RESET_TIMEOUT_MASK) >>\r\nENA_REGS_CAPS_RESET_TIMEOUT_SHIFT;\r\nif (timeout == 0) {\r\npr_err("Invalid timeout value\n");\r\nreturn -EINVAL;\r\n}\r\nreset_val = ENA_REGS_DEV_CTL_DEV_RESET_MASK;\r\nreset_val |= (reset_reason << ENA_REGS_DEV_CTL_RESET_REASON_SHIFT) &\r\nENA_REGS_DEV_CTL_RESET_REASON_MASK;\r\nwritel(reset_val, ena_dev->reg_bar + ENA_REGS_DEV_CTL_OFF);\r\nena_com_mmio_reg_read_request_write_dev_addr(ena_dev);\r\nrc = wait_for_reset_state(ena_dev, timeout,\r\nENA_REGS_DEV_STS_RESET_IN_PROGRESS_MASK);\r\nif (rc != 0) {\r\npr_err("Reset indication didn't turn on\n");\r\nreturn rc;\r\n}\r\nwritel(0, ena_dev->reg_bar + ENA_REGS_DEV_CTL_OFF);\r\nrc = wait_for_reset_state(ena_dev, timeout, 0);\r\nif (rc != 0) {\r\npr_err("Reset indication didn't turn off\n");\r\nreturn rc;\r\n}\r\ntimeout = (cap & ENA_REGS_CAPS_ADMIN_CMD_TO_MASK) >>\r\nENA_REGS_CAPS_ADMIN_CMD_TO_SHIFT;\r\nif (timeout)\r\nena_dev->admin_queue.completion_timeout = timeout * 100000;\r\nelse\r\nena_dev->admin_queue.completion_timeout = ADMIN_CMD_TIMEOUT_US;\r\nreturn 0;\r\n}\r\nstatic int ena_get_dev_stats(struct ena_com_dev *ena_dev,\r\nstruct ena_com_stats_ctx *ctx,\r\nenum ena_admin_get_stats_type type)\r\n{\r\nstruct ena_admin_aq_get_stats_cmd *get_cmd = &ctx->get_cmd;\r\nstruct ena_admin_acq_get_stats_resp *get_resp = &ctx->get_resp;\r\nstruct ena_com_admin_queue *admin_queue;\r\nint ret;\r\nadmin_queue = &ena_dev->admin_queue;\r\nget_cmd->aq_common_descriptor.opcode = ENA_ADMIN_GET_STATS;\r\nget_cmd->aq_common_descriptor.flags = 0;\r\nget_cmd->type = type;\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)get_cmd,\r\nsizeof(*get_cmd),\r\n(struct ena_admin_acq_entry *)get_resp,\r\nsizeof(*get_resp));\r\nif (unlikely(ret))\r\npr_err("Failed to get stats. error: %d\n", ret);\r\nreturn ret;\r\n}\r\nint ena_com_get_dev_basic_stats(struct ena_com_dev *ena_dev,\r\nstruct ena_admin_basic_stats *stats)\r\n{\r\nstruct ena_com_stats_ctx ctx;\r\nint ret;\r\nmemset(&ctx, 0x0, sizeof(ctx));\r\nret = ena_get_dev_stats(ena_dev, &ctx, ENA_ADMIN_GET_STATS_TYPE_BASIC);\r\nif (likely(ret == 0))\r\nmemcpy(stats, &ctx.get_resp.basic_stats,\r\nsizeof(ctx.get_resp.basic_stats));\r\nreturn ret;\r\n}\r\nint ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, int mtu)\r\n{\r\nstruct ena_com_admin_queue *admin_queue;\r\nstruct ena_admin_set_feat_cmd cmd;\r\nstruct ena_admin_set_feat_resp resp;\r\nint ret;\r\nif (!ena_com_check_supported_feature_id(ena_dev, ENA_ADMIN_MTU)) {\r\npr_debug("Feature %d isn't supported\n", ENA_ADMIN_MTU);\r\nreturn -EOPNOTSUPP;\r\n}\r\nmemset(&cmd, 0x0, sizeof(cmd));\r\nadmin_queue = &ena_dev->admin_queue;\r\ncmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\r\ncmd.aq_common_descriptor.flags = 0;\r\ncmd.feat_common.feature_id = ENA_ADMIN_MTU;\r\ncmd.u.mtu.mtu = mtu;\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&cmd,\r\nsizeof(cmd),\r\n(struct ena_admin_acq_entry *)&resp,\r\nsizeof(resp));\r\nif (unlikely(ret))\r\npr_err("Failed to set mtu %d. error: %d\n", mtu, ret);\r\nreturn ret;\r\n}\r\nint ena_com_get_offload_settings(struct ena_com_dev *ena_dev,\r\nstruct ena_admin_feature_offload_desc *offload)\r\n{\r\nint ret;\r\nstruct ena_admin_get_feat_resp resp;\r\nret = ena_com_get_feature(ena_dev, &resp,\r\nENA_ADMIN_STATELESS_OFFLOAD_CONFIG);\r\nif (unlikely(ret)) {\r\npr_err("Failed to get offload capabilities %d\n", ret);\r\nreturn ret;\r\n}\r\nmemcpy(offload, &resp.u.offload, sizeof(resp.u.offload));\r\nreturn 0;\r\n}\r\nint ena_com_set_hash_function(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_set_feat_cmd cmd;\r\nstruct ena_admin_set_feat_resp resp;\r\nstruct ena_admin_get_feat_resp get_resp;\r\nint ret;\r\nif (!ena_com_check_supported_feature_id(ena_dev,\r\nENA_ADMIN_RSS_HASH_FUNCTION)) {\r\npr_debug("Feature %d isn't supported\n",\r\nENA_ADMIN_RSS_HASH_FUNCTION);\r\nreturn -EOPNOTSUPP;\r\n}\r\nret = ena_com_get_feature(ena_dev, &get_resp,\r\nENA_ADMIN_RSS_HASH_FUNCTION);\r\nif (unlikely(ret))\r\nreturn ret;\r\nif (get_resp.u.flow_hash_func.supported_func & (1 << rss->hash_func)) {\r\npr_err("Func hash %d isn't supported by device, abort\n",\r\nrss->hash_func);\r\nreturn -EOPNOTSUPP;\r\n}\r\nmemset(&cmd, 0x0, sizeof(cmd));\r\ncmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\r\ncmd.aq_common_descriptor.flags =\r\nENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\r\ncmd.feat_common.feature_id = ENA_ADMIN_RSS_HASH_FUNCTION;\r\ncmd.u.flow_hash_func.init_val = rss->hash_init_val;\r\ncmd.u.flow_hash_func.selected_func = 1 << rss->hash_func;\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&cmd.control_buffer.address,\r\nrss->hash_key_dma_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\ncmd.control_buffer.length = sizeof(*rss->hash_key);\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&cmd,\r\nsizeof(cmd),\r\n(struct ena_admin_acq_entry *)&resp,\r\nsizeof(resp));\r\nif (unlikely(ret)) {\r\npr_err("Failed to set hash function %d. error: %d\n",\r\nrss->hash_func, ret);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint ena_com_fill_hash_function(struct ena_com_dev *ena_dev,\r\nenum ena_admin_hash_functions func,\r\nconst u8 *key, u16 key_len, u32 init_val)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_get_feat_resp get_resp;\r\nstruct ena_admin_feature_rss_flow_hash_control *hash_key =\r\nrss->hash_key;\r\nint rc;\r\nif (unlikely(key_len & 0x3))\r\nreturn -EINVAL;\r\nrc = ena_com_get_feature_ex(ena_dev, &get_resp,\r\nENA_ADMIN_RSS_HASH_FUNCTION,\r\nrss->hash_key_dma_addr,\r\nsizeof(*rss->hash_key));\r\nif (unlikely(rc))\r\nreturn rc;\r\nif (!((1 << func) & get_resp.u.flow_hash_func.supported_func)) {\r\npr_err("Flow hash function %d isn't supported\n", func);\r\nreturn -EOPNOTSUPP;\r\n}\r\nswitch (func) {\r\ncase ENA_ADMIN_TOEPLITZ:\r\nif (key_len > sizeof(hash_key->key)) {\r\npr_err("key len (%hu) is bigger than the max supported (%zu)\n",\r\nkey_len, sizeof(hash_key->key));\r\nreturn -EINVAL;\r\n}\r\nmemcpy(hash_key->key, key, key_len);\r\nrss->hash_init_val = init_val;\r\nhash_key->keys_num = key_len >> 2;\r\nbreak;\r\ncase ENA_ADMIN_CRC32:\r\nrss->hash_init_val = init_val;\r\nbreak;\r\ndefault:\r\npr_err("Invalid hash function (%d)\n", func);\r\nreturn -EINVAL;\r\n}\r\nrc = ena_com_set_hash_function(ena_dev);\r\nif (unlikely(rc))\r\nena_com_get_hash_function(ena_dev, NULL, NULL);\r\nreturn rc;\r\n}\r\nint ena_com_get_hash_function(struct ena_com_dev *ena_dev,\r\nenum ena_admin_hash_functions *func,\r\nu8 *key)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_get_feat_resp get_resp;\r\nstruct ena_admin_feature_rss_flow_hash_control *hash_key =\r\nrss->hash_key;\r\nint rc;\r\nrc = ena_com_get_feature_ex(ena_dev, &get_resp,\r\nENA_ADMIN_RSS_HASH_FUNCTION,\r\nrss->hash_key_dma_addr,\r\nsizeof(*rss->hash_key));\r\nif (unlikely(rc))\r\nreturn rc;\r\nrss->hash_func = get_resp.u.flow_hash_func.selected_func;\r\nif (func)\r\n*func = rss->hash_func;\r\nif (key)\r\nmemcpy(key, hash_key->key, (size_t)(hash_key->keys_num) << 2);\r\nreturn 0;\r\n}\r\nint ena_com_get_hash_ctrl(struct ena_com_dev *ena_dev,\r\nenum ena_admin_flow_hash_proto proto,\r\nu16 *fields)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_get_feat_resp get_resp;\r\nint rc;\r\nrc = ena_com_get_feature_ex(ena_dev, &get_resp,\r\nENA_ADMIN_RSS_HASH_INPUT,\r\nrss->hash_ctrl_dma_addr,\r\nsizeof(*rss->hash_ctrl));\r\nif (unlikely(rc))\r\nreturn rc;\r\nif (fields)\r\n*fields = rss->hash_ctrl->selected_fields[proto].fields;\r\nreturn 0;\r\n}\r\nint ena_com_set_hash_ctrl(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_feature_rss_hash_control *hash_ctrl = rss->hash_ctrl;\r\nstruct ena_admin_set_feat_cmd cmd;\r\nstruct ena_admin_set_feat_resp resp;\r\nint ret;\r\nif (!ena_com_check_supported_feature_id(ena_dev,\r\nENA_ADMIN_RSS_HASH_INPUT)) {\r\npr_debug("Feature %d isn't supported\n",\r\nENA_ADMIN_RSS_HASH_INPUT);\r\nreturn -EOPNOTSUPP;\r\n}\r\nmemset(&cmd, 0x0, sizeof(cmd));\r\ncmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\r\ncmd.aq_common_descriptor.flags =\r\nENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\r\ncmd.feat_common.feature_id = ENA_ADMIN_RSS_HASH_INPUT;\r\ncmd.u.flow_hash_input.enabled_input_sort =\r\nENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_MASK |\r\nENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_MASK;\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&cmd.control_buffer.address,\r\nrss->hash_ctrl_dma_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\ncmd.control_buffer.length = sizeof(*hash_ctrl);\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&cmd,\r\nsizeof(cmd),\r\n(struct ena_admin_acq_entry *)&resp,\r\nsizeof(resp));\r\nif (unlikely(ret))\r\npr_err("Failed to set hash input. error: %d\n", ret);\r\nreturn ret;\r\n}\r\nint ena_com_set_default_hash_ctrl(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_feature_rss_hash_control *hash_ctrl =\r\nrss->hash_ctrl;\r\nu16 available_fields = 0;\r\nint rc, i;\r\nrc = ena_com_get_hash_ctrl(ena_dev, 0, NULL);\r\nif (unlikely(rc))\r\nreturn rc;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_TCP4].fields =\r\nENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\r\nENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_UDP4].fields =\r\nENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\r\nENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_TCP6].fields =\r\nENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\r\nENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_UDP6].fields =\r\nENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\r\nENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_IP4].fields =\r\nENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_IP6].fields =\r\nENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_IP4_FRAG].fields =\r\nENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;\r\nhash_ctrl->selected_fields[ENA_ADMIN_RSS_NOT_IP].fields =\r\nENA_ADMIN_RSS_L2_DA | ENA_ADMIN_RSS_L2_SA;\r\nfor (i = 0; i < ENA_ADMIN_RSS_PROTO_NUM; i++) {\r\navailable_fields = hash_ctrl->selected_fields[i].fields &\r\nhash_ctrl->supported_fields[i].fields;\r\nif (available_fields != hash_ctrl->selected_fields[i].fields) {\r\npr_err("hash control doesn't support all the desire configuration. proto %x supported %x selected %x\n",\r\ni, hash_ctrl->supported_fields[i].fields,\r\nhash_ctrl->selected_fields[i].fields);\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nrc = ena_com_set_hash_ctrl(ena_dev);\r\nif (unlikely(rc))\r\nena_com_get_hash_ctrl(ena_dev, 0, NULL);\r\nreturn rc;\r\n}\r\nint ena_com_fill_hash_ctrl(struct ena_com_dev *ena_dev,\r\nenum ena_admin_flow_hash_proto proto,\r\nu16 hash_fields)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_feature_rss_hash_control *hash_ctrl = rss->hash_ctrl;\r\nu16 supported_fields;\r\nint rc;\r\nif (proto >= ENA_ADMIN_RSS_PROTO_NUM) {\r\npr_err("Invalid proto num (%u)\n", proto);\r\nreturn -EINVAL;\r\n}\r\nrc = ena_com_get_hash_ctrl(ena_dev, proto, NULL);\r\nif (unlikely(rc))\r\nreturn rc;\r\nsupported_fields = hash_ctrl->supported_fields[proto].fields;\r\nif ((hash_fields & supported_fields) != hash_fields) {\r\npr_err("proto %d doesn't support the required fields %x. supports only: %x\n",\r\nproto, hash_fields, supported_fields);\r\n}\r\nhash_ctrl->selected_fields[proto].fields = hash_fields;\r\nrc = ena_com_set_hash_ctrl(ena_dev);\r\nif (unlikely(rc))\r\nena_com_get_hash_ctrl(ena_dev, 0, NULL);\r\nreturn 0;\r\n}\r\nint ena_com_indirect_table_fill_entry(struct ena_com_dev *ena_dev,\r\nu16 entry_idx, u16 entry_value)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nif (unlikely(entry_idx >= (1 << rss->tbl_log_size)))\r\nreturn -EINVAL;\r\nif (unlikely((entry_value > ENA_TOTAL_NUM_QUEUES)))\r\nreturn -EINVAL;\r\nrss->host_rss_ind_tbl[entry_idx] = entry_value;\r\nreturn 0;\r\n}\r\nint ena_com_indirect_table_set(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_set_feat_cmd cmd;\r\nstruct ena_admin_set_feat_resp resp;\r\nint ret;\r\nif (!ena_com_check_supported_feature_id(\r\nena_dev, ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG)) {\r\npr_debug("Feature %d isn't supported\n",\r\nENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG);\r\nreturn -EOPNOTSUPP;\r\n}\r\nret = ena_com_ind_tbl_convert_to_device(ena_dev);\r\nif (ret) {\r\npr_err("Failed to convert host indirection table to device table\n");\r\nreturn ret;\r\n}\r\nmemset(&cmd, 0x0, sizeof(cmd));\r\ncmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\r\ncmd.aq_common_descriptor.flags =\r\nENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\r\ncmd.feat_common.feature_id = ENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG;\r\ncmd.u.ind_table.size = rss->tbl_log_size;\r\ncmd.u.ind_table.inline_index = 0xFFFFFFFF;\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&cmd.control_buffer.address,\r\nrss->rss_ind_tbl_dma_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\ncmd.control_buffer.length = (1ULL << rss->tbl_log_size) *\r\nsizeof(struct ena_admin_rss_ind_table_entry);\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&cmd,\r\nsizeof(cmd),\r\n(struct ena_admin_acq_entry *)&resp,\r\nsizeof(resp));\r\nif (unlikely(ret))\r\npr_err("Failed to set indirect table. error: %d\n", ret);\r\nreturn ret;\r\n}\r\nint ena_com_indirect_table_get(struct ena_com_dev *ena_dev, u32 *ind_tbl)\r\n{\r\nstruct ena_rss *rss = &ena_dev->rss;\r\nstruct ena_admin_get_feat_resp get_resp;\r\nu32 tbl_size;\r\nint i, rc;\r\ntbl_size = (1ULL << rss->tbl_log_size) *\r\nsizeof(struct ena_admin_rss_ind_table_entry);\r\nrc = ena_com_get_feature_ex(ena_dev, &get_resp,\r\nENA_ADMIN_RSS_REDIRECTION_TABLE_CONFIG,\r\nrss->rss_ind_tbl_dma_addr,\r\ntbl_size);\r\nif (unlikely(rc))\r\nreturn rc;\r\nif (!ind_tbl)\r\nreturn 0;\r\nrc = ena_com_ind_tbl_convert_from_device(ena_dev);\r\nif (unlikely(rc))\r\nreturn rc;\r\nfor (i = 0; i < (1 << rss->tbl_log_size); i++)\r\nind_tbl[i] = rss->host_rss_ind_tbl[i];\r\nreturn 0;\r\n}\r\nint ena_com_rss_init(struct ena_com_dev *ena_dev, u16 indr_tbl_log_size)\r\n{\r\nint rc;\r\nmemset(&ena_dev->rss, 0x0, sizeof(ena_dev->rss));\r\nrc = ena_com_indirect_table_allocate(ena_dev, indr_tbl_log_size);\r\nif (unlikely(rc))\r\ngoto err_indr_tbl;\r\nrc = ena_com_hash_key_allocate(ena_dev);\r\nif (unlikely(rc))\r\ngoto err_hash_key;\r\nrc = ena_com_hash_ctrl_init(ena_dev);\r\nif (unlikely(rc))\r\ngoto err_hash_ctrl;\r\nreturn 0;\r\nerr_hash_ctrl:\r\nena_com_hash_key_destroy(ena_dev);\r\nerr_hash_key:\r\nena_com_indirect_table_destroy(ena_dev);\r\nerr_indr_tbl:\r\nreturn rc;\r\n}\r\nvoid ena_com_rss_destroy(struct ena_com_dev *ena_dev)\r\n{\r\nena_com_indirect_table_destroy(ena_dev);\r\nena_com_hash_key_destroy(ena_dev);\r\nena_com_hash_ctrl_destroy(ena_dev);\r\nmemset(&ena_dev->rss, 0x0, sizeof(ena_dev->rss));\r\n}\r\nint ena_com_allocate_host_info(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\r\nhost_attr->host_info =\r\ndma_zalloc_coherent(ena_dev->dmadev, SZ_4K,\r\n&host_attr->host_info_dma_addr, GFP_KERNEL);\r\nif (unlikely(!host_attr->host_info))\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint ena_com_allocate_debug_area(struct ena_com_dev *ena_dev,\r\nu32 debug_area_size)\r\n{\r\nstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\r\nhost_attr->debug_area_virt_addr =\r\ndma_zalloc_coherent(ena_dev->dmadev, debug_area_size,\r\n&host_attr->debug_area_dma_addr, GFP_KERNEL);\r\nif (unlikely(!host_attr->debug_area_virt_addr)) {\r\nhost_attr->debug_area_size = 0;\r\nreturn -ENOMEM;\r\n}\r\nhost_attr->debug_area_size = debug_area_size;\r\nreturn 0;\r\n}\r\nvoid ena_com_delete_host_info(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\r\nif (host_attr->host_info) {\r\ndma_free_coherent(ena_dev->dmadev, SZ_4K, host_attr->host_info,\r\nhost_attr->host_info_dma_addr);\r\nhost_attr->host_info = NULL;\r\n}\r\n}\r\nvoid ena_com_delete_debug_area(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\r\nif (host_attr->debug_area_virt_addr) {\r\ndma_free_coherent(ena_dev->dmadev, host_attr->debug_area_size,\r\nhost_attr->debug_area_virt_addr,\r\nhost_attr->debug_area_dma_addr);\r\nhost_attr->debug_area_virt_addr = NULL;\r\n}\r\n}\r\nint ena_com_set_host_attributes(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\r\nstruct ena_com_admin_queue *admin_queue;\r\nstruct ena_admin_set_feat_cmd cmd;\r\nstruct ena_admin_set_feat_resp resp;\r\nint ret;\r\nmemset(&cmd, 0x0, sizeof(cmd));\r\nadmin_queue = &ena_dev->admin_queue;\r\ncmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\r\ncmd.feat_common.feature_id = ENA_ADMIN_HOST_ATTR_CONFIG;\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&cmd.u.host_attr.debug_ba,\r\nhost_attr->debug_area_dma_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\nret = ena_com_mem_addr_set(ena_dev,\r\n&cmd.u.host_attr.os_info_ba,\r\nhost_attr->host_info_dma_addr);\r\nif (unlikely(ret)) {\r\npr_err("memory address set failed\n");\r\nreturn ret;\r\n}\r\ncmd.u.host_attr.debug_area_size = host_attr->debug_area_size;\r\nret = ena_com_execute_admin_command(admin_queue,\r\n(struct ena_admin_aq_entry *)&cmd,\r\nsizeof(cmd),\r\n(struct ena_admin_acq_entry *)&resp,\r\nsizeof(resp));\r\nif (unlikely(ret))\r\npr_err("Failed to set host attributes: %d\n", ret);\r\nreturn ret;\r\n}\r\nbool ena_com_interrupt_moderation_supported(struct ena_com_dev *ena_dev)\r\n{\r\nreturn ena_com_check_supported_feature_id(ena_dev,\r\nENA_ADMIN_INTERRUPT_MODERATION);\r\n}\r\nint ena_com_update_nonadaptive_moderation_interval_tx(struct ena_com_dev *ena_dev,\r\nu32 tx_coalesce_usecs)\r\n{\r\nif (!ena_dev->intr_delay_resolution) {\r\npr_err("Illegal interrupt delay granularity value\n");\r\nreturn -EFAULT;\r\n}\r\nena_dev->intr_moder_tx_interval = tx_coalesce_usecs /\r\nena_dev->intr_delay_resolution;\r\nreturn 0;\r\n}\r\nint ena_com_update_nonadaptive_moderation_interval_rx(struct ena_com_dev *ena_dev,\r\nu32 rx_coalesce_usecs)\r\n{\r\nif (!ena_dev->intr_delay_resolution) {\r\npr_err("Illegal interrupt delay granularity value\n");\r\nreturn -EFAULT;\r\n}\r\nena_dev->intr_moder_tbl[ENA_INTR_MODER_LOWEST].intr_moder_interval =\r\nrx_coalesce_usecs / ena_dev->intr_delay_resolution;\r\nreturn 0;\r\n}\r\nvoid ena_com_destroy_interrupt_moderation(struct ena_com_dev *ena_dev)\r\n{\r\nif (ena_dev->intr_moder_tbl)\r\ndevm_kfree(ena_dev->dmadev, ena_dev->intr_moder_tbl);\r\nena_dev->intr_moder_tbl = NULL;\r\n}\r\nint ena_com_init_interrupt_moderation(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_admin_get_feat_resp get_resp;\r\nu16 delay_resolution;\r\nint rc;\r\nrc = ena_com_get_feature(ena_dev, &get_resp,\r\nENA_ADMIN_INTERRUPT_MODERATION);\r\nif (rc) {\r\nif (rc == -EOPNOTSUPP) {\r\npr_debug("Feature %d isn't supported\n",\r\nENA_ADMIN_INTERRUPT_MODERATION);\r\nrc = 0;\r\n} else {\r\npr_err("Failed to get interrupt moderation admin cmd. rc: %d\n",\r\nrc);\r\n}\r\nena_com_disable_adaptive_moderation(ena_dev);\r\nreturn rc;\r\n}\r\nrc = ena_com_init_interrupt_moderation_table(ena_dev);\r\nif (rc)\r\ngoto err;\r\ndelay_resolution = get_resp.u.intr_moderation.intr_delay_resolution;\r\nena_com_update_intr_delay_resolution(ena_dev, delay_resolution);\r\nena_com_enable_adaptive_moderation(ena_dev);\r\nreturn 0;\r\nerr:\r\nena_com_destroy_interrupt_moderation(ena_dev);\r\nreturn rc;\r\n}\r\nvoid ena_com_config_default_interrupt_moderation_table(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_intr_moder_entry *intr_moder_tbl = ena_dev->intr_moder_tbl;\r\nif (!intr_moder_tbl)\r\nreturn;\r\nintr_moder_tbl[ENA_INTR_MODER_LOWEST].intr_moder_interval =\r\nENA_INTR_LOWEST_USECS;\r\nintr_moder_tbl[ENA_INTR_MODER_LOWEST].pkts_per_interval =\r\nENA_INTR_LOWEST_PKTS;\r\nintr_moder_tbl[ENA_INTR_MODER_LOWEST].bytes_per_interval =\r\nENA_INTR_LOWEST_BYTES;\r\nintr_moder_tbl[ENA_INTR_MODER_LOW].intr_moder_interval =\r\nENA_INTR_LOW_USECS;\r\nintr_moder_tbl[ENA_INTR_MODER_LOW].pkts_per_interval =\r\nENA_INTR_LOW_PKTS;\r\nintr_moder_tbl[ENA_INTR_MODER_LOW].bytes_per_interval =\r\nENA_INTR_LOW_BYTES;\r\nintr_moder_tbl[ENA_INTR_MODER_MID].intr_moder_interval =\r\nENA_INTR_MID_USECS;\r\nintr_moder_tbl[ENA_INTR_MODER_MID].pkts_per_interval =\r\nENA_INTR_MID_PKTS;\r\nintr_moder_tbl[ENA_INTR_MODER_MID].bytes_per_interval =\r\nENA_INTR_MID_BYTES;\r\nintr_moder_tbl[ENA_INTR_MODER_HIGH].intr_moder_interval =\r\nENA_INTR_HIGH_USECS;\r\nintr_moder_tbl[ENA_INTR_MODER_HIGH].pkts_per_interval =\r\nENA_INTR_HIGH_PKTS;\r\nintr_moder_tbl[ENA_INTR_MODER_HIGH].bytes_per_interval =\r\nENA_INTR_HIGH_BYTES;\r\nintr_moder_tbl[ENA_INTR_MODER_HIGHEST].intr_moder_interval =\r\nENA_INTR_HIGHEST_USECS;\r\nintr_moder_tbl[ENA_INTR_MODER_HIGHEST].pkts_per_interval =\r\nENA_INTR_HIGHEST_PKTS;\r\nintr_moder_tbl[ENA_INTR_MODER_HIGHEST].bytes_per_interval =\r\nENA_INTR_HIGHEST_BYTES;\r\n}\r\nunsigned int ena_com_get_nonadaptive_moderation_interval_tx(struct ena_com_dev *ena_dev)\r\n{\r\nreturn ena_dev->intr_moder_tx_interval;\r\n}\r\nunsigned int ena_com_get_nonadaptive_moderation_interval_rx(struct ena_com_dev *ena_dev)\r\n{\r\nstruct ena_intr_moder_entry *intr_moder_tbl = ena_dev->intr_moder_tbl;\r\nif (intr_moder_tbl)\r\nreturn intr_moder_tbl[ENA_INTR_MODER_LOWEST].intr_moder_interval;\r\nreturn 0;\r\n}\r\nvoid ena_com_init_intr_moderation_entry(struct ena_com_dev *ena_dev,\r\nenum ena_intr_moder_level level,\r\nstruct ena_intr_moder_entry *entry)\r\n{\r\nstruct ena_intr_moder_entry *intr_moder_tbl = ena_dev->intr_moder_tbl;\r\nif (level >= ENA_INTR_MAX_NUM_OF_LEVELS)\r\nreturn;\r\nintr_moder_tbl[level].intr_moder_interval = entry->intr_moder_interval;\r\nif (ena_dev->intr_delay_resolution)\r\nintr_moder_tbl[level].intr_moder_interval /=\r\nena_dev->intr_delay_resolution;\r\nintr_moder_tbl[level].pkts_per_interval = entry->pkts_per_interval;\r\nif (entry->bytes_per_interval != ENA_INTR_BYTE_COUNT_NOT_SUPPORTED)\r\nintr_moder_tbl[level].bytes_per_interval = entry->bytes_per_interval;\r\n}\r\nvoid ena_com_get_intr_moderation_entry(struct ena_com_dev *ena_dev,\r\nenum ena_intr_moder_level level,\r\nstruct ena_intr_moder_entry *entry)\r\n{\r\nstruct ena_intr_moder_entry *intr_moder_tbl = ena_dev->intr_moder_tbl;\r\nif (level >= ENA_INTR_MAX_NUM_OF_LEVELS)\r\nreturn;\r\nentry->intr_moder_interval = intr_moder_tbl[level].intr_moder_interval;\r\nif (ena_dev->intr_delay_resolution)\r\nentry->intr_moder_interval *= ena_dev->intr_delay_resolution;\r\nentry->pkts_per_interval =\r\nintr_moder_tbl[level].pkts_per_interval;\r\nentry->bytes_per_interval = intr_moder_tbl[level].bytes_per_interval;\r\n}
