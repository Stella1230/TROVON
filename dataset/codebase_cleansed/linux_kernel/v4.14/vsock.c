static u32 vhost_transport_get_local_cid(void)\r\n{\r\nreturn VHOST_VSOCK_DEFAULT_HOST_CID;\r\n}\r\nstatic struct vhost_vsock *__vhost_vsock_get(u32 guest_cid)\r\n{\r\nstruct vhost_vsock *vsock;\r\nlist_for_each_entry(vsock, &vhost_vsock_list, list) {\r\nu32 other_cid = vsock->guest_cid;\r\nif (other_cid == 0)\r\ncontinue;\r\nif (other_cid == guest_cid) {\r\nreturn vsock;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct vhost_vsock *vhost_vsock_get(u32 guest_cid)\r\n{\r\nstruct vhost_vsock *vsock;\r\nspin_lock_bh(&vhost_vsock_lock);\r\nvsock = __vhost_vsock_get(guest_cid);\r\nspin_unlock_bh(&vhost_vsock_lock);\r\nreturn vsock;\r\n}\r\nstatic void\r\nvhost_transport_do_send_pkt(struct vhost_vsock *vsock,\r\nstruct vhost_virtqueue *vq)\r\n{\r\nstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\r\nbool added = false;\r\nbool restart_tx = false;\r\nmutex_lock(&vq->mutex);\r\nif (!vq->private_data)\r\ngoto out;\r\nvhost_disable_notify(&vsock->dev, vq);\r\nfor (;;) {\r\nstruct virtio_vsock_pkt *pkt;\r\nstruct iov_iter iov_iter;\r\nunsigned out, in;\r\nsize_t nbytes;\r\nsize_t len;\r\nint head;\r\nspin_lock_bh(&vsock->send_pkt_list_lock);\r\nif (list_empty(&vsock->send_pkt_list)) {\r\nspin_unlock_bh(&vsock->send_pkt_list_lock);\r\nvhost_enable_notify(&vsock->dev, vq);\r\nbreak;\r\n}\r\npkt = list_first_entry(&vsock->send_pkt_list,\r\nstruct virtio_vsock_pkt, list);\r\nlist_del_init(&pkt->list);\r\nspin_unlock_bh(&vsock->send_pkt_list_lock);\r\nhead = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),\r\n&out, &in, NULL, NULL);\r\nif (head < 0) {\r\nspin_lock_bh(&vsock->send_pkt_list_lock);\r\nlist_add(&pkt->list, &vsock->send_pkt_list);\r\nspin_unlock_bh(&vsock->send_pkt_list_lock);\r\nbreak;\r\n}\r\nif (head == vq->num) {\r\nspin_lock_bh(&vsock->send_pkt_list_lock);\r\nlist_add(&pkt->list, &vsock->send_pkt_list);\r\nspin_unlock_bh(&vsock->send_pkt_list_lock);\r\nif (unlikely(vhost_enable_notify(&vsock->dev, vq))) {\r\nvhost_disable_notify(&vsock->dev, vq);\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nif (out) {\r\nvirtio_transport_free_pkt(pkt);\r\nvq_err(vq, "Expected 0 output buffers, got %u\n", out);\r\nbreak;\r\n}\r\nlen = iov_length(&vq->iov[out], in);\r\niov_iter_init(&iov_iter, READ, &vq->iov[out], in, len);\r\nnbytes = copy_to_iter(&pkt->hdr, sizeof(pkt->hdr), &iov_iter);\r\nif (nbytes != sizeof(pkt->hdr)) {\r\nvirtio_transport_free_pkt(pkt);\r\nvq_err(vq, "Faulted on copying pkt hdr\n");\r\nbreak;\r\n}\r\nnbytes = copy_to_iter(pkt->buf, pkt->len, &iov_iter);\r\nif (nbytes != pkt->len) {\r\nvirtio_transport_free_pkt(pkt);\r\nvq_err(vq, "Faulted on copying pkt buf\n");\r\nbreak;\r\n}\r\nvhost_add_used(vq, head, sizeof(pkt->hdr) + pkt->len);\r\nadded = true;\r\nif (pkt->reply) {\r\nint val;\r\nval = atomic_dec_return(&vsock->queued_replies);\r\nif (val + 1 == tx_vq->num)\r\nrestart_tx = true;\r\n}\r\nvirtio_transport_deliver_tap_pkt(pkt);\r\nvirtio_transport_free_pkt(pkt);\r\n}\r\nif (added)\r\nvhost_signal(&vsock->dev, vq);\r\nout:\r\nmutex_unlock(&vq->mutex);\r\nif (restart_tx)\r\nvhost_poll_queue(&tx_vq->poll);\r\n}\r\nstatic void vhost_transport_send_pkt_work(struct vhost_work *work)\r\n{\r\nstruct vhost_virtqueue *vq;\r\nstruct vhost_vsock *vsock;\r\nvsock = container_of(work, struct vhost_vsock, send_pkt_work);\r\nvq = &vsock->vqs[VSOCK_VQ_RX];\r\nvhost_transport_do_send_pkt(vsock, vq);\r\n}\r\nstatic int\r\nvhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)\r\n{\r\nstruct vhost_vsock *vsock;\r\nint len = pkt->len;\r\nvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\r\nif (!vsock) {\r\nvirtio_transport_free_pkt(pkt);\r\nreturn -ENODEV;\r\n}\r\nif (pkt->reply)\r\natomic_inc(&vsock->queued_replies);\r\nspin_lock_bh(&vsock->send_pkt_list_lock);\r\nlist_add_tail(&pkt->list, &vsock->send_pkt_list);\r\nspin_unlock_bh(&vsock->send_pkt_list_lock);\r\nvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\r\nreturn len;\r\n}\r\nstatic int\r\nvhost_transport_cancel_pkt(struct vsock_sock *vsk)\r\n{\r\nstruct vhost_vsock *vsock;\r\nstruct virtio_vsock_pkt *pkt, *n;\r\nint cnt = 0;\r\nLIST_HEAD(freeme);\r\nvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\r\nif (!vsock)\r\nreturn -ENODEV;\r\nspin_lock_bh(&vsock->send_pkt_list_lock);\r\nlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\r\nif (pkt->vsk != vsk)\r\ncontinue;\r\nlist_move(&pkt->list, &freeme);\r\n}\r\nspin_unlock_bh(&vsock->send_pkt_list_lock);\r\nlist_for_each_entry_safe(pkt, n, &freeme, list) {\r\nif (pkt->reply)\r\ncnt++;\r\nlist_del(&pkt->list);\r\nvirtio_transport_free_pkt(pkt);\r\n}\r\nif (cnt) {\r\nstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\r\nint new_cnt;\r\nnew_cnt = atomic_sub_return(cnt, &vsock->queued_replies);\r\nif (new_cnt + cnt >= tx_vq->num && new_cnt < tx_vq->num)\r\nvhost_poll_queue(&tx_vq->poll);\r\n}\r\nreturn 0;\r\n}\r\nstatic struct virtio_vsock_pkt *\r\nvhost_vsock_alloc_pkt(struct vhost_virtqueue *vq,\r\nunsigned int out, unsigned int in)\r\n{\r\nstruct virtio_vsock_pkt *pkt;\r\nstruct iov_iter iov_iter;\r\nsize_t nbytes;\r\nsize_t len;\r\nif (in != 0) {\r\nvq_err(vq, "Expected 0 input buffers, got %u\n", in);\r\nreturn NULL;\r\n}\r\npkt = kzalloc(sizeof(*pkt), GFP_KERNEL);\r\nif (!pkt)\r\nreturn NULL;\r\nlen = iov_length(vq->iov, out);\r\niov_iter_init(&iov_iter, WRITE, vq->iov, out, len);\r\nnbytes = copy_from_iter(&pkt->hdr, sizeof(pkt->hdr), &iov_iter);\r\nif (nbytes != sizeof(pkt->hdr)) {\r\nvq_err(vq, "Expected %zu bytes for pkt->hdr, got %zu bytes\n",\r\nsizeof(pkt->hdr), nbytes);\r\nkfree(pkt);\r\nreturn NULL;\r\n}\r\nif (le16_to_cpu(pkt->hdr.type) == VIRTIO_VSOCK_TYPE_STREAM)\r\npkt->len = le32_to_cpu(pkt->hdr.len);\r\nif (!pkt->len)\r\nreturn pkt;\r\nif (pkt->len > VIRTIO_VSOCK_MAX_PKT_BUF_SIZE) {\r\nkfree(pkt);\r\nreturn NULL;\r\n}\r\npkt->buf = kmalloc(pkt->len, GFP_KERNEL);\r\nif (!pkt->buf) {\r\nkfree(pkt);\r\nreturn NULL;\r\n}\r\nnbytes = copy_from_iter(pkt->buf, pkt->len, &iov_iter);\r\nif (nbytes != pkt->len) {\r\nvq_err(vq, "Expected %u byte payload, got %zu bytes\n",\r\npkt->len, nbytes);\r\nvirtio_transport_free_pkt(pkt);\r\nreturn NULL;\r\n}\r\nreturn pkt;\r\n}\r\nstatic bool vhost_vsock_more_replies(struct vhost_vsock *vsock)\r\n{\r\nstruct vhost_virtqueue *vq = &vsock->vqs[VSOCK_VQ_TX];\r\nint val;\r\nsmp_rmb();\r\nval = atomic_read(&vsock->queued_replies);\r\nreturn val < vq->num;\r\n}\r\nstatic void vhost_vsock_handle_tx_kick(struct vhost_work *work)\r\n{\r\nstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\r\npoll.work);\r\nstruct vhost_vsock *vsock = container_of(vq->dev, struct vhost_vsock,\r\ndev);\r\nstruct virtio_vsock_pkt *pkt;\r\nint head;\r\nunsigned int out, in;\r\nbool added = false;\r\nmutex_lock(&vq->mutex);\r\nif (!vq->private_data)\r\ngoto out;\r\nvhost_disable_notify(&vsock->dev, vq);\r\nfor (;;) {\r\nu32 len;\r\nif (!vhost_vsock_more_replies(vsock)) {\r\ngoto no_more_replies;\r\n}\r\nhead = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),\r\n&out, &in, NULL, NULL);\r\nif (head < 0)\r\nbreak;\r\nif (head == vq->num) {\r\nif (unlikely(vhost_enable_notify(&vsock->dev, vq))) {\r\nvhost_disable_notify(&vsock->dev, vq);\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\npkt = vhost_vsock_alloc_pkt(vq, out, in);\r\nif (!pkt) {\r\nvq_err(vq, "Faulted on pkt\n");\r\ncontinue;\r\n}\r\nlen = pkt->len;\r\nvirtio_transport_deliver_tap_pkt(pkt);\r\nif (le64_to_cpu(pkt->hdr.src_cid) == vsock->guest_cid)\r\nvirtio_transport_recv_pkt(pkt);\r\nelse\r\nvirtio_transport_free_pkt(pkt);\r\nvhost_add_used(vq, head, sizeof(pkt->hdr) + len);\r\nadded = true;\r\n}\r\nno_more_replies:\r\nif (added)\r\nvhost_signal(&vsock->dev, vq);\r\nout:\r\nmutex_unlock(&vq->mutex);\r\n}\r\nstatic void vhost_vsock_handle_rx_kick(struct vhost_work *work)\r\n{\r\nstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\r\npoll.work);\r\nstruct vhost_vsock *vsock = container_of(vq->dev, struct vhost_vsock,\r\ndev);\r\nvhost_transport_do_send_pkt(vsock, vq);\r\n}\r\nstatic int vhost_vsock_start(struct vhost_vsock *vsock)\r\n{\r\nstruct vhost_virtqueue *vq;\r\nsize_t i;\r\nint ret;\r\nmutex_lock(&vsock->dev.mutex);\r\nret = vhost_dev_check_owner(&vsock->dev);\r\nif (ret)\r\ngoto err;\r\nfor (i = 0; i < ARRAY_SIZE(vsock->vqs); i++) {\r\nvq = &vsock->vqs[i];\r\nmutex_lock(&vq->mutex);\r\nif (!vhost_vq_access_ok(vq)) {\r\nret = -EFAULT;\r\ngoto err_vq;\r\n}\r\nif (!vq->private_data) {\r\nvq->private_data = vsock;\r\nret = vhost_vq_init_access(vq);\r\nif (ret)\r\ngoto err_vq;\r\n}\r\nmutex_unlock(&vq->mutex);\r\n}\r\nmutex_unlock(&vsock->dev.mutex);\r\nreturn 0;\r\nerr_vq:\r\nvq->private_data = NULL;\r\nmutex_unlock(&vq->mutex);\r\nfor (i = 0; i < ARRAY_SIZE(vsock->vqs); i++) {\r\nvq = &vsock->vqs[i];\r\nmutex_lock(&vq->mutex);\r\nvq->private_data = NULL;\r\nmutex_unlock(&vq->mutex);\r\n}\r\nerr:\r\nmutex_unlock(&vsock->dev.mutex);\r\nreturn ret;\r\n}\r\nstatic int vhost_vsock_stop(struct vhost_vsock *vsock)\r\n{\r\nsize_t i;\r\nint ret;\r\nmutex_lock(&vsock->dev.mutex);\r\nret = vhost_dev_check_owner(&vsock->dev);\r\nif (ret)\r\ngoto err;\r\nfor (i = 0; i < ARRAY_SIZE(vsock->vqs); i++) {\r\nstruct vhost_virtqueue *vq = &vsock->vqs[i];\r\nmutex_lock(&vq->mutex);\r\nvq->private_data = NULL;\r\nmutex_unlock(&vq->mutex);\r\n}\r\nerr:\r\nmutex_unlock(&vsock->dev.mutex);\r\nreturn ret;\r\n}\r\nstatic void vhost_vsock_free(struct vhost_vsock *vsock)\r\n{\r\nkvfree(vsock);\r\n}\r\nstatic int vhost_vsock_dev_open(struct inode *inode, struct file *file)\r\n{\r\nstruct vhost_virtqueue **vqs;\r\nstruct vhost_vsock *vsock;\r\nint ret;\r\nvsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_RETRY_MAYFAIL);\r\nif (!vsock)\r\nreturn -ENOMEM;\r\nvqs = kmalloc_array(ARRAY_SIZE(vsock->vqs), sizeof(*vqs), GFP_KERNEL);\r\nif (!vqs) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\natomic_set(&vsock->queued_replies, 0);\r\nvqs[VSOCK_VQ_TX] = &vsock->vqs[VSOCK_VQ_TX];\r\nvqs[VSOCK_VQ_RX] = &vsock->vqs[VSOCK_VQ_RX];\r\nvsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;\r\nvsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;\r\nvhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs));\r\nfile->private_data = vsock;\r\nspin_lock_init(&vsock->send_pkt_list_lock);\r\nINIT_LIST_HEAD(&vsock->send_pkt_list);\r\nvhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);\r\nspin_lock_bh(&vhost_vsock_lock);\r\nlist_add_tail(&vsock->list, &vhost_vsock_list);\r\nspin_unlock_bh(&vhost_vsock_lock);\r\nreturn 0;\r\nout:\r\nvhost_vsock_free(vsock);\r\nreturn ret;\r\n}\r\nstatic void vhost_vsock_flush(struct vhost_vsock *vsock)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(vsock->vqs); i++)\r\nif (vsock->vqs[i].handle_kick)\r\nvhost_poll_flush(&vsock->vqs[i].poll);\r\nvhost_work_flush(&vsock->dev, &vsock->send_pkt_work);\r\n}\r\nstatic void vhost_vsock_reset_orphans(struct sock *sk)\r\n{\r\nstruct vsock_sock *vsk = vsock_sk(sk);\r\nif (!vhost_vsock_get(vsk->remote_addr.svm_cid)) {\r\nsock_set_flag(sk, SOCK_DONE);\r\nvsk->peer_shutdown = SHUTDOWN_MASK;\r\nsk->sk_state = SS_UNCONNECTED;\r\nsk->sk_err = ECONNRESET;\r\nsk->sk_error_report(sk);\r\n}\r\n}\r\nstatic int vhost_vsock_dev_release(struct inode *inode, struct file *file)\r\n{\r\nstruct vhost_vsock *vsock = file->private_data;\r\nspin_lock_bh(&vhost_vsock_lock);\r\nlist_del(&vsock->list);\r\nspin_unlock_bh(&vhost_vsock_lock);\r\nvsock_for_each_connected_socket(vhost_vsock_reset_orphans);\r\nvhost_vsock_stop(vsock);\r\nvhost_vsock_flush(vsock);\r\nvhost_dev_stop(&vsock->dev);\r\nspin_lock_bh(&vsock->send_pkt_list_lock);\r\nwhile (!list_empty(&vsock->send_pkt_list)) {\r\nstruct virtio_vsock_pkt *pkt;\r\npkt = list_first_entry(&vsock->send_pkt_list,\r\nstruct virtio_vsock_pkt, list);\r\nlist_del_init(&pkt->list);\r\nvirtio_transport_free_pkt(pkt);\r\n}\r\nspin_unlock_bh(&vsock->send_pkt_list_lock);\r\nvhost_dev_cleanup(&vsock->dev, false);\r\nkfree(vsock->dev.vqs);\r\nvhost_vsock_free(vsock);\r\nreturn 0;\r\n}\r\nstatic int vhost_vsock_set_cid(struct vhost_vsock *vsock, u64 guest_cid)\r\n{\r\nstruct vhost_vsock *other;\r\nif (guest_cid <= VMADDR_CID_HOST ||\r\nguest_cid == U32_MAX)\r\nreturn -EINVAL;\r\nif (guest_cid > U32_MAX)\r\nreturn -EINVAL;\r\nspin_lock_bh(&vhost_vsock_lock);\r\nother = __vhost_vsock_get(guest_cid);\r\nif (other && other != vsock) {\r\nspin_unlock_bh(&vhost_vsock_lock);\r\nreturn -EADDRINUSE;\r\n}\r\nvsock->guest_cid = guest_cid;\r\nspin_unlock_bh(&vhost_vsock_lock);\r\nreturn 0;\r\n}\r\nstatic int vhost_vsock_set_features(struct vhost_vsock *vsock, u64 features)\r\n{\r\nstruct vhost_virtqueue *vq;\r\nint i;\r\nif (features & ~VHOST_VSOCK_FEATURES)\r\nreturn -EOPNOTSUPP;\r\nmutex_lock(&vsock->dev.mutex);\r\nif ((features & (1 << VHOST_F_LOG_ALL)) &&\r\n!vhost_log_access_ok(&vsock->dev)) {\r\nmutex_unlock(&vsock->dev.mutex);\r\nreturn -EFAULT;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(vsock->vqs); i++) {\r\nvq = &vsock->vqs[i];\r\nmutex_lock(&vq->mutex);\r\nvq->acked_features = features;\r\nmutex_unlock(&vq->mutex);\r\n}\r\nmutex_unlock(&vsock->dev.mutex);\r\nreturn 0;\r\n}\r\nstatic long vhost_vsock_dev_ioctl(struct file *f, unsigned int ioctl,\r\nunsigned long arg)\r\n{\r\nstruct vhost_vsock *vsock = f->private_data;\r\nvoid __user *argp = (void __user *)arg;\r\nu64 guest_cid;\r\nu64 features;\r\nint start;\r\nint r;\r\nswitch (ioctl) {\r\ncase VHOST_VSOCK_SET_GUEST_CID:\r\nif (copy_from_user(&guest_cid, argp, sizeof(guest_cid)))\r\nreturn -EFAULT;\r\nreturn vhost_vsock_set_cid(vsock, guest_cid);\r\ncase VHOST_VSOCK_SET_RUNNING:\r\nif (copy_from_user(&start, argp, sizeof(start)))\r\nreturn -EFAULT;\r\nif (start)\r\nreturn vhost_vsock_start(vsock);\r\nelse\r\nreturn vhost_vsock_stop(vsock);\r\ncase VHOST_GET_FEATURES:\r\nfeatures = VHOST_VSOCK_FEATURES;\r\nif (copy_to_user(argp, &features, sizeof(features)))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase VHOST_SET_FEATURES:\r\nif (copy_from_user(&features, argp, sizeof(features)))\r\nreturn -EFAULT;\r\nreturn vhost_vsock_set_features(vsock, features);\r\ndefault:\r\nmutex_lock(&vsock->dev.mutex);\r\nr = vhost_dev_ioctl(&vsock->dev, ioctl, argp);\r\nif (r == -ENOIOCTLCMD)\r\nr = vhost_vring_ioctl(&vsock->dev, ioctl, argp);\r\nelse\r\nvhost_vsock_flush(vsock);\r\nmutex_unlock(&vsock->dev.mutex);\r\nreturn r;\r\n}\r\n}\r\nstatic int __init vhost_vsock_init(void)\r\n{\r\nint ret;\r\nret = vsock_core_init(&vhost_transport.transport);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn misc_register(&vhost_vsock_misc);\r\n}\r\nstatic void __exit vhost_vsock_exit(void)\r\n{\r\nmisc_deregister(&vhost_vsock_misc);\r\nvsock_core_exit();\r\n}
