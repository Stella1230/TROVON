static inline bool rwb_enabled(struct rq_wb *rwb)\r\n{\r\nreturn rwb && rwb->wb_normal != 0;\r\n}\r\nstatic bool atomic_inc_below(atomic_t *v, int below)\r\n{\r\nint cur = atomic_read(v);\r\nfor (;;) {\r\nint old;\r\nif (cur >= below)\r\nreturn false;\r\nold = atomic_cmpxchg(v, cur, cur + 1);\r\nif (old == cur)\r\nbreak;\r\ncur = old;\r\n}\r\nreturn true;\r\n}\r\nstatic void wb_timestamp(struct rq_wb *rwb, unsigned long *var)\r\n{\r\nif (rwb_enabled(rwb)) {\r\nconst unsigned long cur = jiffies;\r\nif (cur != *var)\r\n*var = cur;\r\n}\r\n}\r\nstatic bool wb_recent_wait(struct rq_wb *rwb)\r\n{\r\nstruct bdi_writeback *wb = &rwb->queue->backing_dev_info->wb;\r\nreturn time_before(jiffies, wb->dirty_sleep + HZ);\r\n}\r\nstatic inline struct rq_wait *get_rq_wait(struct rq_wb *rwb, bool is_kswapd)\r\n{\r\nreturn &rwb->rq_wait[is_kswapd];\r\n}\r\nstatic void rwb_wake_all(struct rq_wb *rwb)\r\n{\r\nint i;\r\nfor (i = 0; i < WBT_NUM_RWQ; i++) {\r\nstruct rq_wait *rqw = &rwb->rq_wait[i];\r\nif (waitqueue_active(&rqw->wait))\r\nwake_up_all(&rqw->wait);\r\n}\r\n}\r\nvoid __wbt_done(struct rq_wb *rwb, enum wbt_flags wb_acct)\r\n{\r\nstruct rq_wait *rqw;\r\nint inflight, limit;\r\nif (!(wb_acct & WBT_TRACKED))\r\nreturn;\r\nrqw = get_rq_wait(rwb, wb_acct & WBT_KSWAPD);\r\ninflight = atomic_dec_return(&rqw->inflight);\r\nif (unlikely(!rwb_enabled(rwb))) {\r\nrwb_wake_all(rwb);\r\nreturn;\r\n}\r\nif (rwb->wc && !wb_recent_wait(rwb))\r\nlimit = 0;\r\nelse\r\nlimit = rwb->wb_normal;\r\nif (inflight && inflight >= limit)\r\nreturn;\r\nif (waitqueue_active(&rqw->wait)) {\r\nint diff = limit - inflight;\r\nif (!inflight || diff >= rwb->wb_background / 2)\r\nwake_up_all(&rqw->wait);\r\n}\r\n}\r\nvoid wbt_done(struct rq_wb *rwb, struct blk_issue_stat *stat)\r\n{\r\nif (!rwb)\r\nreturn;\r\nif (!wbt_is_tracked(stat)) {\r\nif (rwb->sync_cookie == stat) {\r\nrwb->sync_issue = 0;\r\nrwb->sync_cookie = NULL;\r\n}\r\nif (wbt_is_read(stat))\r\nwb_timestamp(rwb, &rwb->last_comp);\r\nwbt_clear_state(stat);\r\n} else {\r\nWARN_ON_ONCE(stat == rwb->sync_cookie);\r\n__wbt_done(rwb, wbt_stat_to_mask(stat));\r\nwbt_clear_state(stat);\r\n}\r\n}\r\nstatic bool calc_wb_limits(struct rq_wb *rwb)\r\n{\r\nunsigned int depth;\r\nbool ret = false;\r\nif (!rwb->min_lat_nsec) {\r\nrwb->wb_max = rwb->wb_normal = rwb->wb_background = 0;\r\nreturn false;\r\n}\r\nif (rwb->queue_depth == 1) {\r\nif (rwb->scale_step > 0)\r\nrwb->wb_max = rwb->wb_normal = 1;\r\nelse {\r\nrwb->wb_max = rwb->wb_normal = 2;\r\nret = true;\r\n}\r\nrwb->wb_background = 1;\r\n} else {\r\ndepth = min_t(unsigned int, RWB_DEF_DEPTH, rwb->queue_depth);\r\nif (rwb->scale_step > 0)\r\ndepth = 1 + ((depth - 1) >> min(31, rwb->scale_step));\r\nelse if (rwb->scale_step < 0) {\r\nunsigned int maxd = 3 * rwb->queue_depth / 4;\r\ndepth = 1 + ((depth - 1) << -rwb->scale_step);\r\nif (depth > maxd) {\r\ndepth = maxd;\r\nret = true;\r\n}\r\n}\r\nrwb->wb_max = depth;\r\nrwb->wb_normal = (rwb->wb_max + 1) / 2;\r\nrwb->wb_background = (rwb->wb_max + 3) / 4;\r\n}\r\nreturn ret;\r\n}\r\nstatic inline bool stat_sample_valid(struct blk_rq_stat *stat)\r\n{\r\nreturn (stat[READ].nr_samples >= 1 &&\r\nstat[WRITE].nr_samples >= RWB_MIN_WRITE_SAMPLES);\r\n}\r\nstatic u64 rwb_sync_issue_lat(struct rq_wb *rwb)\r\n{\r\nu64 now, issue = ACCESS_ONCE(rwb->sync_issue);\r\nif (!issue || !rwb->sync_cookie)\r\nreturn 0;\r\nnow = ktime_to_ns(ktime_get());\r\nreturn now - issue;\r\n}\r\nstatic int latency_exceeded(struct rq_wb *rwb, struct blk_rq_stat *stat)\r\n{\r\nstruct backing_dev_info *bdi = rwb->queue->backing_dev_info;\r\nu64 thislat;\r\nthislat = rwb_sync_issue_lat(rwb);\r\nif (thislat > rwb->cur_win_nsec ||\r\n(thislat > rwb->min_lat_nsec && !stat[READ].nr_samples)) {\r\ntrace_wbt_lat(bdi, thislat);\r\nreturn LAT_EXCEEDED;\r\n}\r\nif (!stat_sample_valid(stat)) {\r\nif (stat[WRITE].nr_samples || wb_recent_wait(rwb) ||\r\nwbt_inflight(rwb))\r\nreturn LAT_UNKNOWN_WRITES;\r\nreturn LAT_UNKNOWN;\r\n}\r\nif (stat[READ].min > rwb->min_lat_nsec) {\r\ntrace_wbt_lat(bdi, stat[READ].min);\r\ntrace_wbt_stat(bdi, stat);\r\nreturn LAT_EXCEEDED;\r\n}\r\nif (rwb->scale_step)\r\ntrace_wbt_stat(bdi, stat);\r\nreturn LAT_OK;\r\n}\r\nstatic void rwb_trace_step(struct rq_wb *rwb, const char *msg)\r\n{\r\nstruct backing_dev_info *bdi = rwb->queue->backing_dev_info;\r\ntrace_wbt_step(bdi, msg, rwb->scale_step, rwb->cur_win_nsec,\r\nrwb->wb_background, rwb->wb_normal, rwb->wb_max);\r\n}\r\nstatic void scale_up(struct rq_wb *rwb)\r\n{\r\nif (rwb->scaled_max)\r\nreturn;\r\nrwb->scale_step--;\r\nrwb->unknown_cnt = 0;\r\nrwb->scaled_max = calc_wb_limits(rwb);\r\nrwb_wake_all(rwb);\r\nrwb_trace_step(rwb, "step up");\r\n}\r\nstatic void scale_down(struct rq_wb *rwb, bool hard_throttle)\r\n{\r\nif (rwb->wb_max == 1)\r\nreturn;\r\nif (rwb->scale_step < 0 && hard_throttle)\r\nrwb->scale_step = 0;\r\nelse\r\nrwb->scale_step++;\r\nrwb->scaled_max = false;\r\nrwb->unknown_cnt = 0;\r\ncalc_wb_limits(rwb);\r\nrwb_trace_step(rwb, "step down");\r\n}\r\nstatic void rwb_arm_timer(struct rq_wb *rwb)\r\n{\r\nif (rwb->scale_step > 0) {\r\nrwb->cur_win_nsec = div_u64(rwb->win_nsec << 4,\r\nint_sqrt((rwb->scale_step + 1) << 8));\r\n} else {\r\nrwb->cur_win_nsec = rwb->win_nsec;\r\n}\r\nblk_stat_activate_nsecs(rwb->cb, rwb->cur_win_nsec);\r\n}\r\nstatic void wb_timer_fn(struct blk_stat_callback *cb)\r\n{\r\nstruct rq_wb *rwb = cb->data;\r\nunsigned int inflight = wbt_inflight(rwb);\r\nint status;\r\nstatus = latency_exceeded(rwb, cb->stat);\r\ntrace_wbt_timer(rwb->queue->backing_dev_info, status, rwb->scale_step,\r\ninflight);\r\nswitch (status) {\r\ncase LAT_EXCEEDED:\r\nscale_down(rwb, true);\r\nbreak;\r\ncase LAT_OK:\r\nscale_up(rwb);\r\nbreak;\r\ncase LAT_UNKNOWN_WRITES:\r\nscale_up(rwb);\r\nbreak;\r\ncase LAT_UNKNOWN:\r\nif (++rwb->unknown_cnt < RWB_UNKNOWN_BUMP)\r\nbreak;\r\nif (rwb->scale_step > 0)\r\nscale_up(rwb);\r\nelse if (rwb->scale_step < 0)\r\nscale_down(rwb, false);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (rwb->scale_step || inflight)\r\nrwb_arm_timer(rwb);\r\n}\r\nvoid wbt_update_limits(struct rq_wb *rwb)\r\n{\r\nrwb->scale_step = 0;\r\nrwb->scaled_max = false;\r\ncalc_wb_limits(rwb);\r\nrwb_wake_all(rwb);\r\n}\r\nstatic bool close_io(struct rq_wb *rwb)\r\n{\r\nconst unsigned long now = jiffies;\r\nreturn time_before(now, rwb->last_issue + HZ / 10) ||\r\ntime_before(now, rwb->last_comp + HZ / 10);\r\n}\r\nstatic inline unsigned int get_limit(struct rq_wb *rwb, unsigned long rw)\r\n{\r\nunsigned int limit;\r\nif ((rw & REQ_HIPRIO) || wb_recent_wait(rwb) || current_is_kswapd())\r\nlimit = rwb->wb_max;\r\nelse if ((rw & REQ_BACKGROUND) || close_io(rwb)) {\r\nlimit = rwb->wb_background;\r\n} else\r\nlimit = rwb->wb_normal;\r\nreturn limit;\r\n}\r\nstatic inline bool may_queue(struct rq_wb *rwb, struct rq_wait *rqw,\r\nwait_queue_entry_t *wait, unsigned long rw)\r\n{\r\nif (!rwb_enabled(rwb)) {\r\natomic_inc(&rqw->inflight);\r\nreturn true;\r\n}\r\nif (waitqueue_active(&rqw->wait) &&\r\nrqw->wait.head.next != &wait->entry)\r\nreturn false;\r\nreturn atomic_inc_below(&rqw->inflight, get_limit(rwb, rw));\r\n}\r\nstatic void __wbt_wait(struct rq_wb *rwb, unsigned long rw, spinlock_t *lock)\r\n__releases(lock)\r\n__acquires(lock)\r\n{\r\nstruct rq_wait *rqw = get_rq_wait(rwb, current_is_kswapd());\r\nDEFINE_WAIT(wait);\r\nif (may_queue(rwb, rqw, &wait, rw))\r\nreturn;\r\ndo {\r\nprepare_to_wait_exclusive(&rqw->wait, &wait,\r\nTASK_UNINTERRUPTIBLE);\r\nif (may_queue(rwb, rqw, &wait, rw))\r\nbreak;\r\nif (lock) {\r\nspin_unlock_irq(lock);\r\nio_schedule();\r\nspin_lock_irq(lock);\r\n} else\r\nio_schedule();\r\n} while (1);\r\nfinish_wait(&rqw->wait, &wait);\r\n}\r\nstatic inline bool wbt_should_throttle(struct rq_wb *rwb, struct bio *bio)\r\n{\r\nconst int op = bio_op(bio);\r\nif (op != REQ_OP_WRITE)\r\nreturn false;\r\nif ((bio->bi_opf & (REQ_SYNC | REQ_IDLE)) == (REQ_SYNC | REQ_IDLE))\r\nreturn false;\r\nreturn true;\r\n}\r\nenum wbt_flags wbt_wait(struct rq_wb *rwb, struct bio *bio, spinlock_t *lock)\r\n{\r\nunsigned int ret = 0;\r\nif (!rwb_enabled(rwb))\r\nreturn 0;\r\nif (bio_op(bio) == REQ_OP_READ)\r\nret = WBT_READ;\r\nif (!wbt_should_throttle(rwb, bio)) {\r\nif (ret & WBT_READ)\r\nwb_timestamp(rwb, &rwb->last_issue);\r\nreturn ret;\r\n}\r\n__wbt_wait(rwb, bio->bi_opf, lock);\r\nif (!blk_stat_is_active(rwb->cb))\r\nrwb_arm_timer(rwb);\r\nif (current_is_kswapd())\r\nret |= WBT_KSWAPD;\r\nreturn ret | WBT_TRACKED;\r\n}\r\nvoid wbt_issue(struct rq_wb *rwb, struct blk_issue_stat *stat)\r\n{\r\nif (!rwb_enabled(rwb))\r\nreturn;\r\nif (wbt_is_read(stat) && !rwb->sync_issue) {\r\nrwb->sync_cookie = stat;\r\nrwb->sync_issue = blk_stat_time(stat);\r\n}\r\n}\r\nvoid wbt_requeue(struct rq_wb *rwb, struct blk_issue_stat *stat)\r\n{\r\nif (!rwb_enabled(rwb))\r\nreturn;\r\nif (stat == rwb->sync_cookie) {\r\nrwb->sync_issue = 0;\r\nrwb->sync_cookie = NULL;\r\n}\r\n}\r\nvoid wbt_set_queue_depth(struct rq_wb *rwb, unsigned int depth)\r\n{\r\nif (rwb) {\r\nrwb->queue_depth = depth;\r\nwbt_update_limits(rwb);\r\n}\r\n}\r\nvoid wbt_set_write_cache(struct rq_wb *rwb, bool write_cache_on)\r\n{\r\nif (rwb)\r\nrwb->wc = write_cache_on;\r\n}\r\nvoid wbt_disable_default(struct request_queue *q)\r\n{\r\nstruct rq_wb *rwb = q->rq_wb;\r\nif (rwb && rwb->enable_state == WBT_STATE_ON_DEFAULT)\r\nwbt_exit(q);\r\n}\r\nvoid wbt_enable_default(struct request_queue *q)\r\n{\r\nif (q->rq_wb)\r\nreturn;\r\nif (!test_bit(QUEUE_FLAG_REGISTERED, &q->queue_flags))\r\nreturn;\r\nif ((q->mq_ops && IS_ENABLED(CONFIG_BLK_WBT_MQ)) ||\r\n(q->request_fn && IS_ENABLED(CONFIG_BLK_WBT_SQ)))\r\nwbt_init(q);\r\n}\r\nu64 wbt_default_latency_nsec(struct request_queue *q)\r\n{\r\nif (blk_queue_nonrot(q))\r\nreturn 2000000ULL;\r\nelse\r\nreturn 75000000ULL;\r\n}\r\nstatic int wbt_data_dir(const struct request *rq)\r\n{\r\nreturn rq_data_dir(rq);\r\n}\r\nint wbt_init(struct request_queue *q)\r\n{\r\nstruct rq_wb *rwb;\r\nint i;\r\nBUILD_BUG_ON(WBT_NR_BITS > BLK_STAT_RES_BITS);\r\nrwb = kzalloc(sizeof(*rwb), GFP_KERNEL);\r\nif (!rwb)\r\nreturn -ENOMEM;\r\nrwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);\r\nif (!rwb->cb) {\r\nkfree(rwb);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < WBT_NUM_RWQ; i++) {\r\natomic_set(&rwb->rq_wait[i].inflight, 0);\r\ninit_waitqueue_head(&rwb->rq_wait[i].wait);\r\n}\r\nrwb->wc = 1;\r\nrwb->queue_depth = RWB_DEF_DEPTH;\r\nrwb->last_comp = rwb->last_issue = jiffies;\r\nrwb->queue = q;\r\nrwb->win_nsec = RWB_WINDOW_NSEC;\r\nrwb->enable_state = WBT_STATE_ON_DEFAULT;\r\nwbt_update_limits(rwb);\r\nq->rq_wb = rwb;\r\nblk_stat_add_callback(q, rwb->cb);\r\nrwb->min_lat_nsec = wbt_default_latency_nsec(q);\r\nwbt_set_queue_depth(rwb, blk_queue_depth(q));\r\nwbt_set_write_cache(rwb, test_bit(QUEUE_FLAG_WC, &q->queue_flags));\r\nreturn 0;\r\n}\r\nvoid wbt_exit(struct request_queue *q)\r\n{\r\nstruct rq_wb *rwb = q->rq_wb;\r\nif (rwb) {\r\nblk_stat_remove_callback(q, rwb->cb);\r\nblk_stat_free_callback(rwb->cb);\r\nq->rq_wb = NULL;\r\nkfree(rwb);\r\n}\r\n}
