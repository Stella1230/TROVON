static inline struct can_rx_offload_cb *can_rx_offload_get_cb(struct sk_buff *skb)\r\n{\r\nBUILD_BUG_ON(sizeof(struct can_rx_offload_cb) > sizeof(skb->cb));\r\nreturn (struct can_rx_offload_cb *)skb->cb;\r\n}\r\nstatic inline bool can_rx_offload_le(struct can_rx_offload *offload, unsigned int a, unsigned int b)\r\n{\r\nif (offload->inc)\r\nreturn a <= b;\r\nelse\r\nreturn a >= b;\r\n}\r\nstatic inline unsigned int can_rx_offload_inc(struct can_rx_offload *offload, unsigned int *val)\r\n{\r\nif (offload->inc)\r\nreturn (*val)++;\r\nelse\r\nreturn (*val)--;\r\n}\r\nstatic int can_rx_offload_napi_poll(struct napi_struct *napi, int quota)\r\n{\r\nstruct can_rx_offload *offload = container_of(napi, struct can_rx_offload, napi);\r\nstruct net_device *dev = offload->dev;\r\nstruct net_device_stats *stats = &dev->stats;\r\nstruct sk_buff *skb;\r\nint work_done = 0;\r\nwhile ((work_done < quota) &&\r\n(skb = skb_dequeue(&offload->skb_queue))) {\r\nstruct can_frame *cf = (struct can_frame *)skb->data;\r\nwork_done++;\r\nstats->rx_packets++;\r\nstats->rx_bytes += cf->can_dlc;\r\nnetif_receive_skb(skb);\r\n}\r\nif (work_done < quota) {\r\nnapi_complete_done(napi, work_done);\r\nif (!skb_queue_empty(&offload->skb_queue))\r\nnapi_reschedule(&offload->napi);\r\n}\r\ncan_led_event(offload->dev, CAN_LED_EVENT_RX);\r\nreturn work_done;\r\n}\r\nstatic inline void __skb_queue_add_sort(struct sk_buff_head *head, struct sk_buff *new,\r\nint (*compare)(struct sk_buff *a, struct sk_buff *b))\r\n{\r\nstruct sk_buff *pos, *insert = (struct sk_buff *)head;\r\nskb_queue_reverse_walk(head, pos) {\r\nconst struct can_rx_offload_cb *cb_pos, *cb_new;\r\ncb_pos = can_rx_offload_get_cb(pos);\r\ncb_new = can_rx_offload_get_cb(new);\r\nnetdev_dbg(new->dev,\r\n"%s: pos=0x%08x, new=0x%08x, diff=%10d, queue_len=%d\n",\r\n__func__,\r\ncb_pos->timestamp, cb_new->timestamp,\r\ncb_new->timestamp - cb_pos->timestamp,\r\nskb_queue_len(head));\r\nif (compare(pos, new) < 0)\r\ncontinue;\r\ninsert = pos;\r\nbreak;\r\n}\r\n__skb_queue_after(head, insert, new);\r\n}\r\nstatic int can_rx_offload_compare(struct sk_buff *a, struct sk_buff *b)\r\n{\r\nconst struct can_rx_offload_cb *cb_a, *cb_b;\r\ncb_a = can_rx_offload_get_cb(a);\r\ncb_b = can_rx_offload_get_cb(b);\r\nreturn cb_b->timestamp - cb_a->timestamp;\r\n}\r\nstatic struct sk_buff *can_rx_offload_offload_one(struct can_rx_offload *offload, unsigned int n)\r\n{\r\nstruct sk_buff *skb = NULL;\r\nstruct can_rx_offload_cb *cb;\r\nstruct can_frame *cf;\r\nint ret;\r\nif (likely(skb_queue_len(&offload->skb_queue) <=\r\noffload->skb_queue_len_max))\r\nskb = alloc_can_skb(offload->dev, &cf);\r\nif (!skb) {\r\nstruct can_frame cf_overflow;\r\nu32 timestamp;\r\nret = offload->mailbox_read(offload, &cf_overflow,\r\n&timestamp, n);\r\nif (ret)\r\noffload->dev->stats.rx_dropped++;\r\nreturn NULL;\r\n}\r\ncb = can_rx_offload_get_cb(skb);\r\nret = offload->mailbox_read(offload, cf, &cb->timestamp, n);\r\nif (!ret) {\r\nkfree_skb(skb);\r\nreturn NULL;\r\n}\r\nreturn skb;\r\n}\r\nint can_rx_offload_irq_offload_timestamp(struct can_rx_offload *offload, u64 pending)\r\n{\r\nstruct sk_buff_head skb_queue;\r\nunsigned int i;\r\n__skb_queue_head_init(&skb_queue);\r\nfor (i = offload->mb_first;\r\ncan_rx_offload_le(offload, i, offload->mb_last);\r\ncan_rx_offload_inc(offload, &i)) {\r\nstruct sk_buff *skb;\r\nif (!(pending & BIT_ULL(i)))\r\ncontinue;\r\nskb = can_rx_offload_offload_one(offload, i);\r\nif (!skb)\r\nbreak;\r\n__skb_queue_add_sort(&skb_queue, skb, can_rx_offload_compare);\r\n}\r\nif (!skb_queue_empty(&skb_queue)) {\r\nunsigned long flags;\r\nu32 queue_len;\r\nspin_lock_irqsave(&offload->skb_queue.lock, flags);\r\nskb_queue_splice_tail(&skb_queue, &offload->skb_queue);\r\nspin_unlock_irqrestore(&offload->skb_queue.lock, flags);\r\nif ((queue_len = skb_queue_len(&offload->skb_queue)) >\r\n(offload->skb_queue_len_max / 8))\r\nnetdev_dbg(offload->dev, "%s: queue_len=%d\n",\r\n__func__, queue_len);\r\ncan_rx_offload_schedule(offload);\r\n}\r\nreturn skb_queue_len(&skb_queue);\r\n}\r\nint can_rx_offload_irq_offload_fifo(struct can_rx_offload *offload)\r\n{\r\nstruct sk_buff *skb;\r\nint received = 0;\r\nwhile ((skb = can_rx_offload_offload_one(offload, 0))) {\r\nskb_queue_tail(&offload->skb_queue, skb);\r\nreceived++;\r\n}\r\nif (received)\r\ncan_rx_offload_schedule(offload);\r\nreturn received;\r\n}\r\nint can_rx_offload_irq_queue_err_skb(struct can_rx_offload *offload, struct sk_buff *skb)\r\n{\r\nif (skb_queue_len(&offload->skb_queue) >\r\noffload->skb_queue_len_max)\r\nreturn -ENOMEM;\r\nskb_queue_tail(&offload->skb_queue, skb);\r\ncan_rx_offload_schedule(offload);\r\nreturn 0;\r\n}\r\nstatic int can_rx_offload_init_queue(struct net_device *dev, struct can_rx_offload *offload, unsigned int weight)\r\n{\r\noffload->dev = dev;\r\noffload->skb_queue_len_max = 2 << fls(weight);\r\noffload->skb_queue_len_max *= 4;\r\nskb_queue_head_init(&offload->skb_queue);\r\ncan_rx_offload_reset(offload);\r\nnetif_napi_add(dev, &offload->napi, can_rx_offload_napi_poll, weight);\r\ndev_dbg(dev->dev.parent, "%s: skb_queue_len_max=%d\n",\r\n__func__, offload->skb_queue_len_max);\r\nreturn 0;\r\n}\r\nint can_rx_offload_add_timestamp(struct net_device *dev, struct can_rx_offload *offload)\r\n{\r\nunsigned int weight;\r\nif (offload->mb_first > BITS_PER_LONG_LONG ||\r\noffload->mb_last > BITS_PER_LONG_LONG || !offload->mailbox_read)\r\nreturn -EINVAL;\r\nif (offload->mb_first < offload->mb_last) {\r\noffload->inc = true;\r\nweight = offload->mb_last - offload->mb_first;\r\n} else {\r\noffload->inc = false;\r\nweight = offload->mb_first - offload->mb_last;\r\n}\r\nreturn can_rx_offload_init_queue(dev, offload, weight);;\r\n}\r\nint can_rx_offload_add_fifo(struct net_device *dev, struct can_rx_offload *offload, unsigned int weight)\r\n{\r\nif (!offload->mailbox_read)\r\nreturn -EINVAL;\r\nreturn can_rx_offload_init_queue(dev, offload, weight);\r\n}\r\nvoid can_rx_offload_enable(struct can_rx_offload *offload)\r\n{\r\ncan_rx_offload_reset(offload);\r\nnapi_enable(&offload->napi);\r\n}\r\nvoid can_rx_offload_del(struct can_rx_offload *offload)\r\n{\r\nnetif_napi_del(&offload->napi);\r\nskb_queue_purge(&offload->skb_queue);\r\n}\r\nvoid can_rx_offload_reset(struct can_rx_offload *offload)\r\n{\r\n}
