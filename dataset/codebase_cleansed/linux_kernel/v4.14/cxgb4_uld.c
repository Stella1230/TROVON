static int get_msix_idx_from_bmap(struct adapter *adap)\r\n{\r\nstruct uld_msix_bmap *bmap = &adap->msix_bmap_ulds;\r\nunsigned long flags;\r\nunsigned int msix_idx;\r\nspin_lock_irqsave(&bmap->lock, flags);\r\nmsix_idx = find_first_zero_bit(bmap->msix_bmap, bmap->mapsize);\r\nif (msix_idx < bmap->mapsize) {\r\n__set_bit(msix_idx, bmap->msix_bmap);\r\n} else {\r\nspin_unlock_irqrestore(&bmap->lock, flags);\r\nreturn -ENOSPC;\r\n}\r\nspin_unlock_irqrestore(&bmap->lock, flags);\r\nreturn msix_idx;\r\n}\r\nstatic void free_msix_idx_in_bmap(struct adapter *adap, unsigned int msix_idx)\r\n{\r\nstruct uld_msix_bmap *bmap = &adap->msix_bmap_ulds;\r\nunsigned long flags;\r\nspin_lock_irqsave(&bmap->lock, flags);\r\n__clear_bit(msix_idx, bmap->msix_bmap);\r\nspin_unlock_irqrestore(&bmap->lock, flags);\r\n}\r\nstatic void uldrx_flush_handler(struct sge_rspq *q)\r\n{\r\nstruct adapter *adap = q->adap;\r\nif (adap->uld[q->uld].lro_flush)\r\nadap->uld[q->uld].lro_flush(&q->lro_mgr);\r\n}\r\nstatic int uldrx_handler(struct sge_rspq *q, const __be64 *rsp,\r\nconst struct pkt_gl *gl)\r\n{\r\nstruct adapter *adap = q->adap;\r\nstruct sge_ofld_rxq *rxq = container_of(q, struct sge_ofld_rxq, rspq);\r\nint ret;\r\nif (((const struct rss_header *)rsp)->opcode == CPL_FW4_MSG &&\r\n((const struct cpl_fw4_msg *)(rsp + 1))->type == FW_TYPE_RSSCPL)\r\nrsp += 2;\r\nif (q->flush_handler)\r\nret = adap->uld[q->uld].lro_rx_handler(adap->uld[q->uld].handle,\r\nrsp, gl, &q->lro_mgr,\r\n&q->napi);\r\nelse\r\nret = adap->uld[q->uld].rx_handler(adap->uld[q->uld].handle,\r\nrsp, gl);\r\nif (ret) {\r\nrxq->stats.nomem++;\r\nreturn -1;\r\n}\r\nif (!gl)\r\nrxq->stats.imm++;\r\nelse if (gl == CXGB4_MSG_AN)\r\nrxq->stats.an++;\r\nelse\r\nrxq->stats.pkts++;\r\nreturn 0;\r\n}\r\nstatic int alloc_uld_rxqs(struct adapter *adap,\r\nstruct sge_uld_rxq_info *rxq_info, bool lro)\r\n{\r\nstruct sge *s = &adap->sge;\r\nunsigned int nq = rxq_info->nrxq + rxq_info->nciq;\r\nstruct sge_ofld_rxq *q = rxq_info->uldrxq;\r\nunsigned short *ids = rxq_info->rspq_id;\r\nunsigned int bmap_idx = 0;\r\nunsigned int per_chan;\r\nint i, err, msi_idx, que_idx = 0;\r\nper_chan = rxq_info->nrxq / adap->params.nports;\r\nif (adap->flags & USING_MSIX)\r\nmsi_idx = 1;\r\nelse\r\nmsi_idx = -((int)s->intrq.abs_id + 1);\r\nfor (i = 0; i < nq; i++, q++) {\r\nif (i == rxq_info->nrxq) {\r\nper_chan = rxq_info->nciq / adap->params.nports;\r\nque_idx = 0;\r\n}\r\nif (msi_idx >= 0) {\r\nbmap_idx = get_msix_idx_from_bmap(adap);\r\nmsi_idx = adap->msix_info_ulds[bmap_idx].idx;\r\n}\r\nerr = t4_sge_alloc_rxq(adap, &q->rspq, false,\r\nadap->port[que_idx++ / per_chan],\r\nmsi_idx,\r\nq->fl.size ? &q->fl : NULL,\r\nuldrx_handler,\r\nlro ? uldrx_flush_handler : NULL,\r\n0);\r\nif (err)\r\ngoto freeout;\r\nif (msi_idx >= 0)\r\nrxq_info->msix_tbl[i] = bmap_idx;\r\nmemset(&q->stats, 0, sizeof(q->stats));\r\nif (ids)\r\nids[i] = q->rspq.abs_id;\r\n}\r\nreturn 0;\r\nfreeout:\r\nq = rxq_info->uldrxq;\r\nfor ( ; i; i--, q++) {\r\nif (q->rspq.desc)\r\nfree_rspq_fl(adap, &q->rspq,\r\nq->fl.size ? &q->fl : NULL);\r\n}\r\nreturn err;\r\n}\r\nstatic int\r\nsetup_sge_queues_uld(struct adapter *adap, unsigned int uld_type, bool lro)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nint i, ret = 0;\r\nif (adap->flags & USING_MSIX) {\r\nrxq_info->msix_tbl = kcalloc((rxq_info->nrxq + rxq_info->nciq),\r\nsizeof(unsigned short),\r\nGFP_KERNEL);\r\nif (!rxq_info->msix_tbl)\r\nreturn -ENOMEM;\r\n}\r\nret = !(!alloc_uld_rxqs(adap, rxq_info, lro));\r\nif (adap->flags & FULL_INIT_DONE &&\r\n!ret && uld_type == CXGB4_ULD_RDMA) {\r\nstruct sge *s = &adap->sge;\r\nunsigned int cmplqid;\r\nu32 param, cmdop;\r\ncmdop = FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL;\r\nfor_each_port(adap, i) {\r\ncmplqid = rxq_info->uldrxq[i].rspq.cntxt_id;\r\nparam = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |\r\nFW_PARAMS_PARAM_X_V(cmdop) |\r\nFW_PARAMS_PARAM_YZ_V(s->ctrlq[i].q.cntxt_id));\r\nret = t4_set_params(adap, adap->mbox, adap->pf,\r\n0, 1, &param, &cmplqid);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void t4_free_uld_rxqs(struct adapter *adap, int n,\r\nstruct sge_ofld_rxq *q)\r\n{\r\nfor ( ; n; n--, q++) {\r\nif (q->rspq.desc)\r\nfree_rspq_fl(adap, &q->rspq,\r\nq->fl.size ? &q->fl : NULL);\r\n}\r\n}\r\nstatic void free_sge_queues_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nif (adap->flags & FULL_INIT_DONE && uld_type == CXGB4_ULD_RDMA) {\r\nstruct sge *s = &adap->sge;\r\nu32 param, cmdop, cmplqid = 0;\r\nint i;\r\ncmdop = FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL;\r\nfor_each_port(adap, i) {\r\nparam = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |\r\nFW_PARAMS_PARAM_X_V(cmdop) |\r\nFW_PARAMS_PARAM_YZ_V(s->ctrlq[i].q.cntxt_id));\r\nt4_set_params(adap, adap->mbox, adap->pf,\r\n0, 1, &param, &cmplqid);\r\n}\r\n}\r\nif (rxq_info->nciq)\r\nt4_free_uld_rxqs(adap, rxq_info->nciq,\r\nrxq_info->uldrxq + rxq_info->nrxq);\r\nt4_free_uld_rxqs(adap, rxq_info->nrxq, rxq_info->uldrxq);\r\nif (adap->flags & USING_MSIX)\r\nkfree(rxq_info->msix_tbl);\r\n}\r\nstatic int cfg_queues_uld(struct adapter *adap, unsigned int uld_type,\r\nconst struct cxgb4_uld_info *uld_info)\r\n{\r\nstruct sge *s = &adap->sge;\r\nstruct sge_uld_rxq_info *rxq_info;\r\nint i, nrxq, ciq_size;\r\nrxq_info = kzalloc(sizeof(*rxq_info), GFP_KERNEL);\r\nif (!rxq_info)\r\nreturn -ENOMEM;\r\nif (adap->flags & USING_MSIX && uld_info->nrxq > s->nqs_per_uld) {\r\ni = s->nqs_per_uld;\r\nrxq_info->nrxq = roundup(i, adap->params.nports);\r\n} else {\r\ni = min_t(int, uld_info->nrxq,\r\nnum_online_cpus());\r\nrxq_info->nrxq = roundup(i, adap->params.nports);\r\n}\r\nif (!uld_info->ciq) {\r\nrxq_info->nciq = 0;\r\n} else {\r\nif (adap->flags & USING_MSIX)\r\nrxq_info->nciq = min_t(int, s->nqs_per_uld,\r\nnum_online_cpus());\r\nelse\r\nrxq_info->nciq = min_t(int, MAX_OFLD_QSETS,\r\nnum_online_cpus());\r\nrxq_info->nciq = ((rxq_info->nciq / adap->params.nports) *\r\nadap->params.nports);\r\nrxq_info->nciq = max_t(int, rxq_info->nciq,\r\nadap->params.nports);\r\n}\r\nnrxq = rxq_info->nrxq + rxq_info->nciq;\r\nrxq_info->uldrxq = kcalloc(nrxq, sizeof(struct sge_ofld_rxq),\r\nGFP_KERNEL);\r\nif (!rxq_info->uldrxq) {\r\nkfree(rxq_info);\r\nreturn -ENOMEM;\r\n}\r\nrxq_info->rspq_id = kcalloc(nrxq, sizeof(unsigned short), GFP_KERNEL);\r\nif (!rxq_info->rspq_id) {\r\nkfree(rxq_info->uldrxq);\r\nkfree(rxq_info);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < rxq_info->nrxq; i++) {\r\nstruct sge_ofld_rxq *r = &rxq_info->uldrxq[i];\r\ninit_rspq(adap, &r->rspq, 5, 1, uld_info->rxq_size, 64);\r\nr->rspq.uld = uld_type;\r\nr->fl.size = 72;\r\n}\r\nciq_size = 64 + adap->vres.cq.size + adap->tids.nftids;\r\nif (ciq_size > SGE_MAX_IQ_SIZE) {\r\ndev_warn(adap->pdev_dev, "CIQ size too small for available IQs\n");\r\nciq_size = SGE_MAX_IQ_SIZE;\r\n}\r\nfor (i = rxq_info->nrxq; i < nrxq; i++) {\r\nstruct sge_ofld_rxq *r = &rxq_info->uldrxq[i];\r\ninit_rspq(adap, &r->rspq, 5, 1, ciq_size, 64);\r\nr->rspq.uld = uld_type;\r\n}\r\nmemcpy(rxq_info->name, uld_info->name, IFNAMSIZ);\r\nadap->sge.uld_rxq_info[uld_type] = rxq_info;\r\nreturn 0;\r\n}\r\nstatic void free_queues_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nkfree(rxq_info->rspq_id);\r\nkfree(rxq_info->uldrxq);\r\nkfree(rxq_info);\r\n}\r\nstatic int\r\nrequest_msix_queue_irqs_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nint err = 0;\r\nunsigned int idx, bmap_idx;\r\nfor_each_uldrxq(rxq_info, idx) {\r\nbmap_idx = rxq_info->msix_tbl[idx];\r\nerr = request_irq(adap->msix_info_ulds[bmap_idx].vec,\r\nt4_sge_intr_msix, 0,\r\nadap->msix_info_ulds[bmap_idx].desc,\r\n&rxq_info->uldrxq[idx].rspq);\r\nif (err)\r\ngoto unwind;\r\n}\r\nreturn 0;\r\nunwind:\r\nwhile (idx-- > 0) {\r\nbmap_idx = rxq_info->msix_tbl[idx];\r\nfree_msix_idx_in_bmap(adap, bmap_idx);\r\nfree_irq(adap->msix_info_ulds[bmap_idx].vec,\r\n&rxq_info->uldrxq[idx].rspq);\r\n}\r\nreturn err;\r\n}\r\nstatic void\r\nfree_msix_queue_irqs_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nunsigned int idx, bmap_idx;\r\nfor_each_uldrxq(rxq_info, idx) {\r\nbmap_idx = rxq_info->msix_tbl[idx];\r\nfree_msix_idx_in_bmap(adap, bmap_idx);\r\nfree_irq(adap->msix_info_ulds[bmap_idx].vec,\r\n&rxq_info->uldrxq[idx].rspq);\r\n}\r\n}\r\nstatic void name_msix_vecs_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nint n = sizeof(adap->msix_info_ulds[0].desc);\r\nunsigned int idx, bmap_idx;\r\nfor_each_uldrxq(rxq_info, idx) {\r\nbmap_idx = rxq_info->msix_tbl[idx];\r\nsnprintf(adap->msix_info_ulds[bmap_idx].desc, n, "%s-%s%d",\r\nadap->port[0]->name, rxq_info->name, idx);\r\n}\r\n}\r\nstatic void enable_rx(struct adapter *adap, struct sge_rspq *q)\r\n{\r\nif (!q)\r\nreturn;\r\nif (q->handler)\r\nnapi_enable(&q->napi);\r\nt4_write_reg(adap, MYPF_REG(SGE_PF_GTS_A),\r\nSEINTARM_V(q->intr_params) |\r\nINGRESSQID_V(q->cntxt_id));\r\n}\r\nstatic void quiesce_rx(struct adapter *adap, struct sge_rspq *q)\r\n{\r\nif (q && q->handler)\r\nnapi_disable(&q->napi);\r\n}\r\nstatic void enable_rx_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nint idx;\r\nfor_each_uldrxq(rxq_info, idx)\r\nenable_rx(adap, &rxq_info->uldrxq[idx].rspq);\r\n}\r\nstatic void quiesce_rx_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nint idx;\r\nfor_each_uldrxq(rxq_info, idx)\r\nquiesce_rx(adap, &rxq_info->uldrxq[idx].rspq);\r\n}\r\nstatic void\r\nfree_sge_txq_uld(struct adapter *adap, struct sge_uld_txq_info *txq_info)\r\n{\r\nint nq = txq_info->ntxq;\r\nint i;\r\nfor (i = 0; i < nq; i++) {\r\nstruct sge_uld_txq *txq = &txq_info->uldtxq[i];\r\nif (txq && txq->q.desc) {\r\ntasklet_kill(&txq->qresume_tsk);\r\nt4_ofld_eq_free(adap, adap->mbox, adap->pf, 0,\r\ntxq->q.cntxt_id);\r\nfree_tx_desc(adap, &txq->q, txq->q.in_use, false);\r\nkfree(txq->q.sdesc);\r\n__skb_queue_purge(&txq->sendq);\r\nfree_txq(adap, &txq->q);\r\n}\r\n}\r\n}\r\nstatic int\r\nalloc_sge_txq_uld(struct adapter *adap, struct sge_uld_txq_info *txq_info,\r\nunsigned int uld_type)\r\n{\r\nstruct sge *s = &adap->sge;\r\nint nq = txq_info->ntxq;\r\nint i, j, err;\r\nj = nq / adap->params.nports;\r\nfor (i = 0; i < nq; i++) {\r\nstruct sge_uld_txq *txq = &txq_info->uldtxq[i];\r\ntxq->q.size = 1024;\r\nerr = t4_sge_alloc_uld_txq(adap, txq, adap->port[i / j],\r\ns->fw_evtq.cntxt_id, uld_type);\r\nif (err)\r\ngoto freeout;\r\n}\r\nreturn 0;\r\nfreeout:\r\nfree_sge_txq_uld(adap, txq_info);\r\nreturn err;\r\n}\r\nstatic void\r\nrelease_sge_txq_uld(struct adapter *adap, unsigned int uld_type)\r\n{\r\nstruct sge_uld_txq_info *txq_info = NULL;\r\nint tx_uld_type = TX_ULD(uld_type);\r\ntxq_info = adap->sge.uld_txq_info[tx_uld_type];\r\nif (txq_info && atomic_dec_and_test(&txq_info->users)) {\r\nfree_sge_txq_uld(adap, txq_info);\r\nkfree(txq_info->uldtxq);\r\nkfree(txq_info);\r\nadap->sge.uld_txq_info[tx_uld_type] = NULL;\r\n}\r\n}\r\nstatic int\r\nsetup_sge_txq_uld(struct adapter *adap, unsigned int uld_type,\r\nconst struct cxgb4_uld_info *uld_info)\r\n{\r\nstruct sge_uld_txq_info *txq_info = NULL;\r\nint tx_uld_type, i;\r\ntx_uld_type = TX_ULD(uld_type);\r\ntxq_info = adap->sge.uld_txq_info[tx_uld_type];\r\nif ((tx_uld_type == CXGB4_TX_OFLD) && txq_info &&\r\n(atomic_inc_return(&txq_info->users) > 1))\r\nreturn 0;\r\ntxq_info = kzalloc(sizeof(*txq_info), GFP_KERNEL);\r\nif (!txq_info)\r\nreturn -ENOMEM;\r\ni = min_t(int, uld_info->ntxq, num_online_cpus());\r\ntxq_info->ntxq = roundup(i, adap->params.nports);\r\ntxq_info->uldtxq = kcalloc(txq_info->ntxq, sizeof(struct sge_uld_txq),\r\nGFP_KERNEL);\r\nif (!txq_info->uldtxq) {\r\nkfree(txq_info);\r\nreturn -ENOMEM;\r\n}\r\nif (alloc_sge_txq_uld(adap, txq_info, tx_uld_type)) {\r\nkfree(txq_info->uldtxq);\r\nkfree(txq_info);\r\nreturn -ENOMEM;\r\n}\r\natomic_inc(&txq_info->users);\r\nadap->sge.uld_txq_info[tx_uld_type] = txq_info;\r\nreturn 0;\r\n}\r\nstatic void uld_queue_init(struct adapter *adap, unsigned int uld_type,\r\nstruct cxgb4_lld_info *lli)\r\n{\r\nstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\r\nlli->rxq_ids = rxq_info->rspq_id;\r\nlli->nrxq = rxq_info->nrxq;\r\nlli->ciq_ids = rxq_info->rspq_id + rxq_info->nrxq;\r\nlli->nciq = rxq_info->nciq;\r\n}\r\nint t4_uld_mem_alloc(struct adapter *adap)\r\n{\r\nstruct sge *s = &adap->sge;\r\nadap->uld = kcalloc(CXGB4_ULD_MAX, sizeof(*adap->uld), GFP_KERNEL);\r\nif (!adap->uld)\r\nreturn -ENOMEM;\r\ns->uld_rxq_info = kzalloc(CXGB4_ULD_MAX *\r\nsizeof(struct sge_uld_rxq_info *),\r\nGFP_KERNEL);\r\nif (!s->uld_rxq_info)\r\ngoto err_uld;\r\ns->uld_txq_info = kzalloc(CXGB4_TX_MAX *\r\nsizeof(struct sge_uld_txq_info *),\r\nGFP_KERNEL);\r\nif (!s->uld_txq_info)\r\ngoto err_uld_rx;\r\nreturn 0;\r\nerr_uld_rx:\r\nkfree(s->uld_rxq_info);\r\nerr_uld:\r\nkfree(adap->uld);\r\nreturn -ENOMEM;\r\n}\r\nvoid t4_uld_mem_free(struct adapter *adap)\r\n{\r\nstruct sge *s = &adap->sge;\r\nkfree(s->uld_txq_info);\r\nkfree(s->uld_rxq_info);\r\nkfree(adap->uld);\r\n}\r\nstatic void cxgb4_shutdown_uld_adapter(struct adapter *adap, enum cxgb4_uld type)\r\n{\r\nif (adap->uld[type].handle) {\r\nadap->uld[type].handle = NULL;\r\nadap->uld[type].add = NULL;\r\nrelease_sge_txq_uld(adap, type);\r\nif (adap->flags & FULL_INIT_DONE)\r\nquiesce_rx_uld(adap, type);\r\nif (adap->flags & USING_MSIX)\r\nfree_msix_queue_irqs_uld(adap, type);\r\nfree_sge_queues_uld(adap, type);\r\nfree_queues_uld(adap, type);\r\n}\r\n}\r\nvoid t4_uld_clean_up(struct adapter *adap)\r\n{\r\nunsigned int i;\r\nmutex_lock(&uld_mutex);\r\nfor (i = 0; i < CXGB4_ULD_MAX; i++) {\r\nif (!adap->uld[i].handle)\r\ncontinue;\r\ncxgb4_shutdown_uld_adapter(adap, i);\r\n}\r\nmutex_unlock(&uld_mutex);\r\n}\r\nstatic void uld_init(struct adapter *adap, struct cxgb4_lld_info *lld)\r\n{\r\nint i;\r\nlld->pdev = adap->pdev;\r\nlld->pf = adap->pf;\r\nlld->l2t = adap->l2t;\r\nlld->tids = &adap->tids;\r\nlld->ports = adap->port;\r\nlld->vr = &adap->vres;\r\nlld->mtus = adap->params.mtus;\r\nlld->ntxq = adap->sge.ofldqsets;\r\nlld->nchan = adap->params.nports;\r\nlld->nports = adap->params.nports;\r\nlld->wr_cred = adap->params.ofldq_wr_cred;\r\nlld->iscsi_iolen = MAXRXDATA_G(t4_read_reg(adap, TP_PARA_REG2_A));\r\nlld->iscsi_tagmask = t4_read_reg(adap, ULP_RX_ISCSI_TAGMASK_A);\r\nlld->iscsi_pgsz_order = t4_read_reg(adap, ULP_RX_ISCSI_PSZ_A);\r\nlld->iscsi_llimit = t4_read_reg(adap, ULP_RX_ISCSI_LLIMIT_A);\r\nlld->iscsi_ppm = &adap->iscsi_ppm;\r\nlld->adapter_type = adap->params.chip;\r\nlld->cclk_ps = 1000000000 / adap->params.vpd.cclk;\r\nlld->udb_density = 1 << adap->params.sge.eq_qpp;\r\nlld->ucq_density = 1 << adap->params.sge.iq_qpp;\r\nlld->filt_mode = adap->params.tp.vlan_pri_map;\r\nfor (i = 0; i < NCHAN; i++)\r\nlld->tx_modq[i] = i;\r\nlld->gts_reg = adap->regs + MYPF_REG(SGE_PF_GTS_A);\r\nlld->db_reg = adap->regs + MYPF_REG(SGE_PF_KDOORBELL_A);\r\nlld->fw_vers = adap->params.fw_vers;\r\nlld->dbfifo_int_thresh = dbfifo_int_thresh;\r\nlld->sge_ingpadboundary = adap->sge.fl_align;\r\nlld->sge_egrstatuspagesize = adap->sge.stat_len;\r\nlld->sge_pktshift = adap->sge.pktshift;\r\nlld->ulp_crypto = adap->params.crypto;\r\nlld->enable_fw_ofld_conn = adap->flags & FW_OFLD_CONN;\r\nlld->max_ordird_qp = adap->params.max_ordird_qp;\r\nlld->max_ird_adapter = adap->params.max_ird_adapter;\r\nlld->ulptx_memwrite_dsgl = adap->params.ulptx_memwrite_dsgl;\r\nlld->nodeid = dev_to_node(adap->pdev_dev);\r\nlld->fr_nsmr_tpte_wr_support = adap->params.fr_nsmr_tpte_wr_support;\r\n}\r\nstatic void uld_attach(struct adapter *adap, unsigned int uld)\r\n{\r\nvoid *handle;\r\nstruct cxgb4_lld_info lli;\r\nuld_init(adap, &lli);\r\nuld_queue_init(adap, uld, &lli);\r\nhandle = adap->uld[uld].add(&lli);\r\nif (IS_ERR(handle)) {\r\ndev_warn(adap->pdev_dev,\r\n"could not attach to the %s driver, error %ld\n",\r\nadap->uld[uld].name, PTR_ERR(handle));\r\nreturn;\r\n}\r\nadap->uld[uld].handle = handle;\r\nt4_register_netevent_notifier();\r\nif (adap->flags & FULL_INIT_DONE)\r\nadap->uld[uld].state_change(handle, CXGB4_STATE_UP);\r\n}\r\nint cxgb4_register_uld(enum cxgb4_uld type,\r\nconst struct cxgb4_uld_info *p)\r\n{\r\nint ret = 0;\r\nunsigned int adap_idx = 0;\r\nstruct adapter *adap;\r\nif (type >= CXGB4_ULD_MAX)\r\nreturn -EINVAL;\r\nmutex_lock(&uld_mutex);\r\nlist_for_each_entry(adap, &adapter_list, list_node) {\r\nif ((type == CXGB4_ULD_CRYPTO && !is_pci_uld(adap)) ||\r\n(type != CXGB4_ULD_CRYPTO && !is_offload(adap)))\r\ncontinue;\r\nif (type == CXGB4_ULD_ISCSIT && is_t4(adap->params.chip))\r\ncontinue;\r\nret = cfg_queues_uld(adap, type, p);\r\nif (ret)\r\ngoto out;\r\nret = setup_sge_queues_uld(adap, type, p->lro);\r\nif (ret)\r\ngoto free_queues;\r\nif (adap->flags & USING_MSIX) {\r\nname_msix_vecs_uld(adap, type);\r\nret = request_msix_queue_irqs_uld(adap, type);\r\nif (ret)\r\ngoto free_rxq;\r\n}\r\nif (adap->flags & FULL_INIT_DONE)\r\nenable_rx_uld(adap, type);\r\nif (adap->uld[type].add) {\r\nret = -EBUSY;\r\ngoto free_irq;\r\n}\r\nret = setup_sge_txq_uld(adap, type, p);\r\nif (ret)\r\ngoto free_irq;\r\nadap->uld[type] = *p;\r\nuld_attach(adap, type);\r\nadap_idx++;\r\n}\r\nmutex_unlock(&uld_mutex);\r\nreturn 0;\r\nfree_irq:\r\nif (adap->flags & FULL_INIT_DONE)\r\nquiesce_rx_uld(adap, type);\r\nif (adap->flags & USING_MSIX)\r\nfree_msix_queue_irqs_uld(adap, type);\r\nfree_rxq:\r\nfree_sge_queues_uld(adap, type);\r\nfree_queues:\r\nfree_queues_uld(adap, type);\r\nout:\r\nlist_for_each_entry(adap, &adapter_list, list_node) {\r\nif ((type == CXGB4_ULD_CRYPTO && !is_pci_uld(adap)) ||\r\n(type != CXGB4_ULD_CRYPTO && !is_offload(adap)))\r\ncontinue;\r\nif (type == CXGB4_ULD_ISCSIT && is_t4(adap->params.chip))\r\ncontinue;\r\nif (!adap_idx)\r\nbreak;\r\nadap->uld[type].handle = NULL;\r\nadap->uld[type].add = NULL;\r\nrelease_sge_txq_uld(adap, type);\r\nif (adap->flags & FULL_INIT_DONE)\r\nquiesce_rx_uld(adap, type);\r\nif (adap->flags & USING_MSIX)\r\nfree_msix_queue_irqs_uld(adap, type);\r\nfree_sge_queues_uld(adap, type);\r\nfree_queues_uld(adap, type);\r\nadap_idx--;\r\n}\r\nmutex_unlock(&uld_mutex);\r\nreturn ret;\r\n}\r\nint cxgb4_unregister_uld(enum cxgb4_uld type)\r\n{\r\nstruct adapter *adap;\r\nif (type >= CXGB4_ULD_MAX)\r\nreturn -EINVAL;\r\nmutex_lock(&uld_mutex);\r\nlist_for_each_entry(adap, &adapter_list, list_node) {\r\nif ((type == CXGB4_ULD_CRYPTO && !is_pci_uld(adap)) ||\r\n(type != CXGB4_ULD_CRYPTO && !is_offload(adap)))\r\ncontinue;\r\nif (type == CXGB4_ULD_ISCSIT && is_t4(adap->params.chip))\r\ncontinue;\r\ncxgb4_shutdown_uld_adapter(adap, type);\r\n}\r\nmutex_unlock(&uld_mutex);\r\nreturn 0;\r\n}
