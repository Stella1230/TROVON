void blk_mq_sched_free_hctx_data(struct request_queue *q,\r\nvoid (*exit)(struct blk_mq_hw_ctx *))\r\n{\r\nstruct blk_mq_hw_ctx *hctx;\r\nint i;\r\nqueue_for_each_hw_ctx(q, hctx, i) {\r\nif (exit && hctx->sched_data)\r\nexit(hctx);\r\nkfree(hctx->sched_data);\r\nhctx->sched_data = NULL;\r\n}\r\n}\r\nvoid blk_mq_sched_assign_ioc(struct request *rq, struct bio *bio)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct io_context *ioc = rq_ioc(bio);\r\nstruct io_cq *icq;\r\nspin_lock_irq(q->queue_lock);\r\nicq = ioc_lookup_icq(ioc, q);\r\nspin_unlock_irq(q->queue_lock);\r\nif (!icq) {\r\nicq = ioc_create_icq(ioc, q, GFP_ATOMIC);\r\nif (!icq)\r\nreturn;\r\n}\r\nget_io_context(icq->ioc);\r\nrq->elv.icq = icq;\r\n}\r\nstatic void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)\r\n{\r\nif (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))\r\nreturn;\r\nif (hctx->flags & BLK_MQ_F_TAG_SHARED) {\r\nstruct request_queue *q = hctx->queue;\r\nif (!test_and_set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))\r\natomic_inc(&q->shared_hctx_restart);\r\n} else\r\nset_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);\r\n}\r\nstatic bool blk_mq_sched_restart_hctx(struct blk_mq_hw_ctx *hctx)\r\n{\r\nif (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))\r\nreturn false;\r\nif (hctx->flags & BLK_MQ_F_TAG_SHARED) {\r\nstruct request_queue *q = hctx->queue;\r\nif (test_and_clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))\r\natomic_dec(&q->shared_hctx_restart);\r\n} else\r\nclear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);\r\nif (blk_mq_hctx_has_pending(hctx)) {\r\nblk_mq_run_hw_queue(hctx, true);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nvoid blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)\r\n{\r\nstruct request_queue *q = hctx->queue;\r\nstruct elevator_queue *e = q->elevator;\r\nconst bool has_sched_dispatch = e && e->type->ops.mq.dispatch_request;\r\nbool did_work = false;\r\nLIST_HEAD(rq_list);\r\nif (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))\r\nreturn;\r\nhctx->run++;\r\nif (!list_empty_careful(&hctx->dispatch)) {\r\nspin_lock(&hctx->lock);\r\nif (!list_empty(&hctx->dispatch))\r\nlist_splice_init(&hctx->dispatch, &rq_list);\r\nspin_unlock(&hctx->lock);\r\n}\r\nif (!list_empty(&rq_list)) {\r\nblk_mq_sched_mark_restart_hctx(hctx);\r\ndid_work = blk_mq_dispatch_rq_list(q, &rq_list);\r\n} else if (!has_sched_dispatch) {\r\nblk_mq_flush_busy_ctxs(hctx, &rq_list);\r\nblk_mq_dispatch_rq_list(q, &rq_list);\r\n}\r\nif (!did_work && has_sched_dispatch) {\r\ndo {\r\nstruct request *rq;\r\nrq = e->type->ops.mq.dispatch_request(hctx);\r\nif (!rq)\r\nbreak;\r\nlist_add(&rq->queuelist, &rq_list);\r\n} while (blk_mq_dispatch_rq_list(q, &rq_list));\r\n}\r\n}\r\nbool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,\r\nstruct request **merged_request)\r\n{\r\nstruct request *rq;\r\nswitch (elv_merge(q, &rq, bio)) {\r\ncase ELEVATOR_BACK_MERGE:\r\nif (!blk_mq_sched_allow_merge(q, rq, bio))\r\nreturn false;\r\nif (!bio_attempt_back_merge(q, rq, bio))\r\nreturn false;\r\n*merged_request = attempt_back_merge(q, rq);\r\nif (!*merged_request)\r\nelv_merged_request(q, rq, ELEVATOR_BACK_MERGE);\r\nreturn true;\r\ncase ELEVATOR_FRONT_MERGE:\r\nif (!blk_mq_sched_allow_merge(q, rq, bio))\r\nreturn false;\r\nif (!bio_attempt_front_merge(q, rq, bio))\r\nreturn false;\r\n*merged_request = attempt_front_merge(q, rq);\r\nif (!*merged_request)\r\nelv_merged_request(q, rq, ELEVATOR_FRONT_MERGE);\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic bool blk_mq_attempt_merge(struct request_queue *q,\r\nstruct blk_mq_ctx *ctx, struct bio *bio)\r\n{\r\nstruct request *rq;\r\nint checked = 8;\r\nlockdep_assert_held(&ctx->lock);\r\nlist_for_each_entry_reverse(rq, &ctx->rq_list, queuelist) {\r\nbool merged = false;\r\nif (!checked--)\r\nbreak;\r\nif (!blk_rq_merge_ok(rq, bio))\r\ncontinue;\r\nswitch (blk_try_merge(rq, bio)) {\r\ncase ELEVATOR_BACK_MERGE:\r\nif (blk_mq_sched_allow_merge(q, rq, bio))\r\nmerged = bio_attempt_back_merge(q, rq, bio);\r\nbreak;\r\ncase ELEVATOR_FRONT_MERGE:\r\nif (blk_mq_sched_allow_merge(q, rq, bio))\r\nmerged = bio_attempt_front_merge(q, rq, bio);\r\nbreak;\r\ncase ELEVATOR_DISCARD_MERGE:\r\nmerged = bio_attempt_discard_merge(q, rq, bio);\r\nbreak;\r\ndefault:\r\ncontinue;\r\n}\r\nif (merged)\r\nctx->rq_merged++;\r\nreturn merged;\r\n}\r\nreturn false;\r\n}\r\nbool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct elevator_queue *e = q->elevator;\r\nstruct blk_mq_ctx *ctx = blk_mq_get_ctx(q);\r\nstruct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);\r\nbool ret = false;\r\nif (e && e->type->ops.mq.bio_merge) {\r\nblk_mq_put_ctx(ctx);\r\nreturn e->type->ops.mq.bio_merge(hctx, bio);\r\n}\r\nif (hctx->flags & BLK_MQ_F_SHOULD_MERGE) {\r\nspin_lock(&ctx->lock);\r\nret = blk_mq_attempt_merge(q, ctx, bio);\r\nspin_unlock(&ctx->lock);\r\n}\r\nblk_mq_put_ctx(ctx);\r\nreturn ret;\r\n}\r\nbool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)\r\n{\r\nreturn rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);\r\n}\r\nvoid blk_mq_sched_request_inserted(struct request *rq)\r\n{\r\ntrace_block_rq_insert(rq->q, rq);\r\n}\r\nstatic bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,\r\nstruct request *rq)\r\n{\r\nif (rq->tag == -1) {\r\nrq->rq_flags |= RQF_SORTED;\r\nreturn false;\r\n}\r\nspin_lock(&hctx->lock);\r\nlist_add(&rq->queuelist, &hctx->dispatch);\r\nspin_unlock(&hctx->lock);\r\nreturn true;\r\n}\r\nvoid blk_mq_sched_restart(struct blk_mq_hw_ctx *const hctx)\r\n{\r\nstruct blk_mq_tags *const tags = hctx->tags;\r\nstruct blk_mq_tag_set *const set = hctx->queue->tag_set;\r\nstruct request_queue *const queue = hctx->queue, *q;\r\nstruct blk_mq_hw_ctx *hctx2;\r\nunsigned int i, j;\r\nif (set->flags & BLK_MQ_F_TAG_SHARED) {\r\nif (!atomic_read(&queue->shared_hctx_restart))\r\nreturn;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu_rr(q, queue, &set->tag_list,\r\ntag_set_list) {\r\nqueue_for_each_hw_ctx(q, hctx2, i)\r\nif (hctx2->tags == tags &&\r\nblk_mq_sched_restart_hctx(hctx2))\r\ngoto done;\r\n}\r\nj = hctx->queue_num + 1;\r\nfor (i = 0; i < queue->nr_hw_queues; i++, j++) {\r\nif (j == queue->nr_hw_queues)\r\nj = 0;\r\nhctx2 = queue->queue_hw_ctx[j];\r\nif (hctx2->tags == tags &&\r\nblk_mq_sched_restart_hctx(hctx2))\r\nbreak;\r\n}\r\ndone:\r\nrcu_read_unlock();\r\n} else {\r\nblk_mq_sched_restart_hctx(hctx);\r\n}\r\n}\r\nstatic void blk_mq_sched_insert_flush(struct blk_mq_hw_ctx *hctx,\r\nstruct request *rq, bool can_block)\r\n{\r\nif (blk_mq_get_driver_tag(rq, &hctx, can_block)) {\r\nblk_insert_flush(rq);\r\nblk_mq_run_hw_queue(hctx, true);\r\n} else\r\nblk_mq_add_to_requeue_list(rq, false, true);\r\n}\r\nvoid blk_mq_sched_insert_request(struct request *rq, bool at_head,\r\nbool run_queue, bool async, bool can_block)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct elevator_queue *e = q->elevator;\r\nstruct blk_mq_ctx *ctx = rq->mq_ctx;\r\nstruct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);\r\nif (rq->tag == -1 && op_is_flush(rq->cmd_flags)) {\r\nblk_mq_sched_insert_flush(hctx, rq, can_block);\r\nreturn;\r\n}\r\nif (e && blk_mq_sched_bypass_insert(hctx, rq))\r\ngoto run;\r\nif (e && e->type->ops.mq.insert_requests) {\r\nLIST_HEAD(list);\r\nlist_add(&rq->queuelist, &list);\r\ne->type->ops.mq.insert_requests(hctx, &list, at_head);\r\n} else {\r\nspin_lock(&ctx->lock);\r\n__blk_mq_insert_request(hctx, rq, at_head);\r\nspin_unlock(&ctx->lock);\r\n}\r\nrun:\r\nif (run_queue)\r\nblk_mq_run_hw_queue(hctx, async);\r\n}\r\nvoid blk_mq_sched_insert_requests(struct request_queue *q,\r\nstruct blk_mq_ctx *ctx,\r\nstruct list_head *list, bool run_queue_async)\r\n{\r\nstruct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);\r\nstruct elevator_queue *e = hctx->queue->elevator;\r\nif (e) {\r\nstruct request *rq, *next;\r\nlist_for_each_entry_safe(rq, next, list, queuelist) {\r\nif (WARN_ON_ONCE(rq->tag != -1)) {\r\nlist_del_init(&rq->queuelist);\r\nblk_mq_sched_bypass_insert(hctx, rq);\r\n}\r\n}\r\n}\r\nif (e && e->type->ops.mq.insert_requests)\r\ne->type->ops.mq.insert_requests(hctx, list, false);\r\nelse\r\nblk_mq_insert_requests(hctx, ctx, list);\r\nblk_mq_run_hw_queue(hctx, run_queue_async);\r\n}\r\nstatic void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,\r\nstruct blk_mq_hw_ctx *hctx,\r\nunsigned int hctx_idx)\r\n{\r\nif (hctx->sched_tags) {\r\nblk_mq_free_rqs(set, hctx->sched_tags, hctx_idx);\r\nblk_mq_free_rq_map(hctx->sched_tags);\r\nhctx->sched_tags = NULL;\r\n}\r\n}\r\nstatic int blk_mq_sched_alloc_tags(struct request_queue *q,\r\nstruct blk_mq_hw_ctx *hctx,\r\nunsigned int hctx_idx)\r\n{\r\nstruct blk_mq_tag_set *set = q->tag_set;\r\nint ret;\r\nhctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,\r\nset->reserved_tags);\r\nif (!hctx->sched_tags)\r\nreturn -ENOMEM;\r\nret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);\r\nif (ret)\r\nblk_mq_sched_free_tags(set, hctx, hctx_idx);\r\nreturn ret;\r\n}\r\nstatic void blk_mq_sched_tags_teardown(struct request_queue *q)\r\n{\r\nstruct blk_mq_tag_set *set = q->tag_set;\r\nstruct blk_mq_hw_ctx *hctx;\r\nint i;\r\nqueue_for_each_hw_ctx(q, hctx, i)\r\nblk_mq_sched_free_tags(set, hctx, i);\r\n}\r\nint blk_mq_sched_init_hctx(struct request_queue *q, struct blk_mq_hw_ctx *hctx,\r\nunsigned int hctx_idx)\r\n{\r\nstruct elevator_queue *e = q->elevator;\r\nint ret;\r\nif (!e)\r\nreturn 0;\r\nret = blk_mq_sched_alloc_tags(q, hctx, hctx_idx);\r\nif (ret)\r\nreturn ret;\r\nif (e->type->ops.mq.init_hctx) {\r\nret = e->type->ops.mq.init_hctx(hctx, hctx_idx);\r\nif (ret) {\r\nblk_mq_sched_free_tags(q->tag_set, hctx, hctx_idx);\r\nreturn ret;\r\n}\r\n}\r\nblk_mq_debugfs_register_sched_hctx(q, hctx);\r\nreturn 0;\r\n}\r\nvoid blk_mq_sched_exit_hctx(struct request_queue *q, struct blk_mq_hw_ctx *hctx,\r\nunsigned int hctx_idx)\r\n{\r\nstruct elevator_queue *e = q->elevator;\r\nif (!e)\r\nreturn;\r\nblk_mq_debugfs_unregister_sched_hctx(hctx);\r\nif (e->type->ops.mq.exit_hctx && hctx->sched_data) {\r\ne->type->ops.mq.exit_hctx(hctx, hctx_idx);\r\nhctx->sched_data = NULL;\r\n}\r\nblk_mq_sched_free_tags(q->tag_set, hctx, hctx_idx);\r\n}\r\nint blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)\r\n{\r\nstruct blk_mq_hw_ctx *hctx;\r\nstruct elevator_queue *eq;\r\nunsigned int i;\r\nint ret;\r\nif (!e) {\r\nq->elevator = NULL;\r\nreturn 0;\r\n}\r\nq->nr_requests = 2 * min_t(unsigned int, q->tag_set->queue_depth,\r\nBLKDEV_MAX_RQ);\r\nqueue_for_each_hw_ctx(q, hctx, i) {\r\nret = blk_mq_sched_alloc_tags(q, hctx, i);\r\nif (ret)\r\ngoto err;\r\n}\r\nret = e->ops.mq.init_sched(q, e);\r\nif (ret)\r\ngoto err;\r\nblk_mq_debugfs_register_sched(q);\r\nqueue_for_each_hw_ctx(q, hctx, i) {\r\nif (e->ops.mq.init_hctx) {\r\nret = e->ops.mq.init_hctx(hctx, i);\r\nif (ret) {\r\neq = q->elevator;\r\nblk_mq_exit_sched(q, eq);\r\nkobject_put(&eq->kobj);\r\nreturn ret;\r\n}\r\n}\r\nblk_mq_debugfs_register_sched_hctx(q, hctx);\r\n}\r\nreturn 0;\r\nerr:\r\nblk_mq_sched_tags_teardown(q);\r\nq->elevator = NULL;\r\nreturn ret;\r\n}\r\nvoid blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)\r\n{\r\nstruct blk_mq_hw_ctx *hctx;\r\nunsigned int i;\r\nqueue_for_each_hw_ctx(q, hctx, i) {\r\nblk_mq_debugfs_unregister_sched_hctx(hctx);\r\nif (e->type->ops.mq.exit_hctx && hctx->sched_data) {\r\ne->type->ops.mq.exit_hctx(hctx, i);\r\nhctx->sched_data = NULL;\r\n}\r\n}\r\nblk_mq_debugfs_unregister_sched(q);\r\nif (e->type->ops.mq.exit_sched)\r\ne->type->ops.mq.exit_sched(e);\r\nblk_mq_sched_tags_teardown(q);\r\nq->elevator = NULL;\r\n}\r\nint blk_mq_sched_init(struct request_queue *q)\r\n{\r\nint ret;\r\nmutex_lock(&q->sysfs_lock);\r\nret = elevator_init(q, NULL);\r\nmutex_unlock(&q->sysfs_lock);\r\nreturn ret;\r\n}
