static inline u64 get_vtimer(void)\r\n{\r\nu64 timer;\r\nasm volatile("stpt %0" : "=m" (timer));\r\nreturn timer;\r\n}\r\nstatic inline void set_vtimer(u64 expires)\r\n{\r\nu64 timer;\r\nasm volatile(\r\n" stpt %0\n"\r\n" spt %1"\r\n: "=m" (timer) : "m" (expires));\r\nS390_lowcore.system_timer += S390_lowcore.last_update_timer - timer;\r\nS390_lowcore.last_update_timer = expires;\r\n}\r\nstatic inline int virt_timer_forward(u64 elapsed)\r\n{\r\nBUG_ON(!irqs_disabled());\r\nif (list_empty(&virt_timer_list))\r\nreturn 0;\r\nelapsed = atomic64_add_return(elapsed, &virt_timer_elapsed);\r\nreturn elapsed >= atomic64_read(&virt_timer_current);\r\n}\r\nstatic void update_mt_scaling(void)\r\n{\r\nu64 cycles_new[8], *cycles_old;\r\nu64 delta, fac, mult, div;\r\nint i;\r\nstcctm5(smp_cpu_mtid + 1, cycles_new);\r\ncycles_old = this_cpu_ptr(mt_cycles);\r\nfac = 1;\r\nmult = div = 0;\r\nfor (i = 0; i <= smp_cpu_mtid; i++) {\r\ndelta = cycles_new[i] - cycles_old[i];\r\ndiv += delta;\r\nmult *= i + 1;\r\nmult += delta * fac;\r\nfac *= i + 1;\r\n}\r\ndiv *= fac;\r\nif (div > 0) {\r\n__this_cpu_write(mt_scaling_mult, mult);\r\n__this_cpu_write(mt_scaling_div, div);\r\nmemcpy(cycles_old, cycles_new,\r\nsizeof(u64) * (smp_cpu_mtid + 1));\r\n}\r\n__this_cpu_write(mt_scaling_jiffies, jiffies_64);\r\n}\r\nstatic inline u64 update_tsk_timer(unsigned long *tsk_vtime, u64 new)\r\n{\r\nu64 delta;\r\ndelta = new - *tsk_vtime;\r\n*tsk_vtime = new;\r\nreturn delta;\r\n}\r\nstatic inline u64 scale_vtime(u64 vtime)\r\n{\r\nu64 mult = __this_cpu_read(mt_scaling_mult);\r\nu64 div = __this_cpu_read(mt_scaling_div);\r\nif (smp_cpu_mtid)\r\nreturn vtime * mult / div;\r\nreturn vtime;\r\n}\r\nstatic void account_system_index_scaled(struct task_struct *p, u64 cputime,\r\nenum cpu_usage_stat index)\r\n{\r\np->stimescaled += cputime_to_nsecs(scale_vtime(cputime));\r\naccount_system_index_time(p, cputime_to_nsecs(cputime), index);\r\n}\r\nstatic int do_account_vtime(struct task_struct *tsk)\r\n{\r\nu64 timer, clock, user, guest, system, hardirq, softirq, steal;\r\ntimer = S390_lowcore.last_update_timer;\r\nclock = S390_lowcore.last_update_clock;\r\nasm volatile(\r\n" stpt %0\n"\r\n#ifdef CONFIG_HAVE_MARCH_Z9_109_FEATURES\r\n" stckf %1"\r\n#else\r\n" stck %1"\r\n#endif\r\n: "=m" (S390_lowcore.last_update_timer),\r\n"=m" (S390_lowcore.last_update_clock));\r\nclock = S390_lowcore.last_update_clock - clock;\r\ntimer -= S390_lowcore.last_update_timer;\r\nif (hardirq_count())\r\nS390_lowcore.hardirq_timer += timer;\r\nelse\r\nS390_lowcore.system_timer += timer;\r\nif (smp_cpu_mtid &&\r\ntime_after64(jiffies_64, this_cpu_read(mt_scaling_jiffies)))\r\nupdate_mt_scaling();\r\nuser = update_tsk_timer(&tsk->thread.user_timer,\r\nREAD_ONCE(S390_lowcore.user_timer));\r\nguest = update_tsk_timer(&tsk->thread.guest_timer,\r\nREAD_ONCE(S390_lowcore.guest_timer));\r\nsystem = update_tsk_timer(&tsk->thread.system_timer,\r\nREAD_ONCE(S390_lowcore.system_timer));\r\nhardirq = update_tsk_timer(&tsk->thread.hardirq_timer,\r\nREAD_ONCE(S390_lowcore.hardirq_timer));\r\nsoftirq = update_tsk_timer(&tsk->thread.softirq_timer,\r\nREAD_ONCE(S390_lowcore.softirq_timer));\r\nS390_lowcore.steal_timer +=\r\nclock - user - guest - system - hardirq - softirq;\r\nif (user) {\r\naccount_user_time(tsk, cputime_to_nsecs(user));\r\ntsk->utimescaled += cputime_to_nsecs(scale_vtime(user));\r\n}\r\nif (guest) {\r\naccount_guest_time(tsk, cputime_to_nsecs(guest));\r\ntsk->utimescaled += cputime_to_nsecs(scale_vtime(guest));\r\n}\r\nif (system)\r\naccount_system_index_scaled(tsk, system, CPUTIME_SYSTEM);\r\nif (hardirq)\r\naccount_system_index_scaled(tsk, hardirq, CPUTIME_IRQ);\r\nif (softirq)\r\naccount_system_index_scaled(tsk, softirq, CPUTIME_SOFTIRQ);\r\nsteal = S390_lowcore.steal_timer;\r\nif ((s64) steal > 0) {\r\nS390_lowcore.steal_timer = 0;\r\naccount_steal_time(cputime_to_nsecs(steal));\r\n}\r\nreturn virt_timer_forward(user + guest + system + hardirq + softirq);\r\n}\r\nvoid vtime_task_switch(struct task_struct *prev)\r\n{\r\ndo_account_vtime(prev);\r\nprev->thread.user_timer = S390_lowcore.user_timer;\r\nprev->thread.guest_timer = S390_lowcore.guest_timer;\r\nprev->thread.system_timer = S390_lowcore.system_timer;\r\nprev->thread.hardirq_timer = S390_lowcore.hardirq_timer;\r\nprev->thread.softirq_timer = S390_lowcore.softirq_timer;\r\nS390_lowcore.user_timer = current->thread.user_timer;\r\nS390_lowcore.guest_timer = current->thread.guest_timer;\r\nS390_lowcore.system_timer = current->thread.system_timer;\r\nS390_lowcore.hardirq_timer = current->thread.hardirq_timer;\r\nS390_lowcore.softirq_timer = current->thread.softirq_timer;\r\n}\r\nvoid vtime_flush(struct task_struct *tsk)\r\n{\r\nif (do_account_vtime(tsk))\r\nvirt_timer_expire();\r\n}\r\nvoid vtime_account_irq_enter(struct task_struct *tsk)\r\n{\r\nu64 timer;\r\ntimer = S390_lowcore.last_update_timer;\r\nS390_lowcore.last_update_timer = get_vtimer();\r\ntimer -= S390_lowcore.last_update_timer;\r\nif ((tsk->flags & PF_VCPU) && (irq_count() == 0))\r\nS390_lowcore.guest_timer += timer;\r\nelse if (hardirq_count())\r\nS390_lowcore.hardirq_timer += timer;\r\nelse if (in_serving_softirq())\r\nS390_lowcore.softirq_timer += timer;\r\nelse\r\nS390_lowcore.system_timer += timer;\r\nvirt_timer_forward(timer);\r\n}\r\nstatic void list_add_sorted(struct vtimer_list *timer, struct list_head *head)\r\n{\r\nstruct vtimer_list *tmp;\r\nlist_for_each_entry(tmp, head, entry) {\r\nif (tmp->expires > timer->expires) {\r\nlist_add_tail(&timer->entry, &tmp->entry);\r\nreturn;\r\n}\r\n}\r\nlist_add_tail(&timer->entry, head);\r\n}\r\nstatic void virt_timer_expire(void)\r\n{\r\nstruct vtimer_list *timer, *tmp;\r\nunsigned long elapsed;\r\nLIST_HEAD(cb_list);\r\nspin_lock(&virt_timer_lock);\r\nelapsed = atomic64_read(&virt_timer_elapsed);\r\nlist_for_each_entry_safe(timer, tmp, &virt_timer_list, entry) {\r\nif (timer->expires < elapsed)\r\nlist_move_tail(&timer->entry, &cb_list);\r\nelse\r\ntimer->expires -= elapsed;\r\n}\r\nif (!list_empty(&virt_timer_list)) {\r\ntimer = list_first_entry(&virt_timer_list,\r\nstruct vtimer_list, entry);\r\natomic64_set(&virt_timer_current, timer->expires);\r\n}\r\natomic64_sub(elapsed, &virt_timer_elapsed);\r\nspin_unlock(&virt_timer_lock);\r\nlist_for_each_entry_safe(timer, tmp, &cb_list, entry) {\r\nlist_del_init(&timer->entry);\r\ntimer->function(timer->data);\r\nif (timer->interval) {\r\ntimer->expires = timer->interval +\r\natomic64_read(&virt_timer_elapsed);\r\nspin_lock(&virt_timer_lock);\r\nlist_add_sorted(timer, &virt_timer_list);\r\nspin_unlock(&virt_timer_lock);\r\n}\r\n}\r\n}\r\nvoid init_virt_timer(struct vtimer_list *timer)\r\n{\r\ntimer->function = NULL;\r\nINIT_LIST_HEAD(&timer->entry);\r\n}\r\nstatic inline int vtimer_pending(struct vtimer_list *timer)\r\n{\r\nreturn !list_empty(&timer->entry);\r\n}\r\nstatic void internal_add_vtimer(struct vtimer_list *timer)\r\n{\r\nif (list_empty(&virt_timer_list)) {\r\natomic64_set(&virt_timer_current, timer->expires);\r\natomic64_set(&virt_timer_elapsed, 0);\r\nlist_add(&timer->entry, &virt_timer_list);\r\n} else {\r\ntimer->expires += atomic64_read(&virt_timer_elapsed);\r\nif (likely((s64) timer->expires <\r\n(s64) atomic64_read(&virt_timer_current)))\r\natomic64_set(&virt_timer_current, timer->expires);\r\nlist_add_sorted(timer, &virt_timer_list);\r\n}\r\n}\r\nstatic void __add_vtimer(struct vtimer_list *timer, int periodic)\r\n{\r\nunsigned long flags;\r\ntimer->interval = periodic ? timer->expires : 0;\r\nspin_lock_irqsave(&virt_timer_lock, flags);\r\ninternal_add_vtimer(timer);\r\nspin_unlock_irqrestore(&virt_timer_lock, flags);\r\n}\r\nvoid add_virt_timer(struct vtimer_list *timer)\r\n{\r\n__add_vtimer(timer, 0);\r\n}\r\nvoid add_virt_timer_periodic(struct vtimer_list *timer)\r\n{\r\n__add_vtimer(timer, 1);\r\n}\r\nstatic int __mod_vtimer(struct vtimer_list *timer, u64 expires, int periodic)\r\n{\r\nunsigned long flags;\r\nint rc;\r\nBUG_ON(!timer->function);\r\nif (timer->expires == expires && vtimer_pending(timer))\r\nreturn 1;\r\nspin_lock_irqsave(&virt_timer_lock, flags);\r\nrc = vtimer_pending(timer);\r\nif (rc)\r\nlist_del_init(&timer->entry);\r\ntimer->interval = periodic ? expires : 0;\r\ntimer->expires = expires;\r\ninternal_add_vtimer(timer);\r\nspin_unlock_irqrestore(&virt_timer_lock, flags);\r\nreturn rc;\r\n}\r\nint mod_virt_timer(struct vtimer_list *timer, u64 expires)\r\n{\r\nreturn __mod_vtimer(timer, expires, 0);\r\n}\r\nint mod_virt_timer_periodic(struct vtimer_list *timer, u64 expires)\r\n{\r\nreturn __mod_vtimer(timer, expires, 1);\r\n}\r\nint del_virt_timer(struct vtimer_list *timer)\r\n{\r\nunsigned long flags;\r\nif (!vtimer_pending(timer))\r\nreturn 0;\r\nspin_lock_irqsave(&virt_timer_lock, flags);\r\nlist_del_init(&timer->entry);\r\nspin_unlock_irqrestore(&virt_timer_lock, flags);\r\nreturn 1;\r\n}\r\nvoid vtime_init(void)\r\n{\r\nset_vtimer(VTIMER_MAX_SLICE);\r\nif (smp_cpu_mtid) {\r\n__this_cpu_write(mt_scaling_jiffies, jiffies);\r\n__this_cpu_write(mt_scaling_mult, 1);\r\n__this_cpu_write(mt_scaling_div, 1);\r\nstcctm5(smp_cpu_mtid + 1, this_cpu_ptr(mt_cycles));\r\n}\r\n}
