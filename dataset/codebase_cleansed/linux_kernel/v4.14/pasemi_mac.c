static int translation_enabled(void)\r\n{\r\n#if defined(CONFIG_PPC_PASEMI_IOMMU_DMA_FORCE)\r\nreturn 1;\r\n#else\r\nreturn firmware_has_feature(FW_FEATURE_LPAR);\r\n#endif\r\n}\r\nstatic void write_iob_reg(unsigned int reg, unsigned int val)\r\n{\r\npasemi_write_iob_reg(reg, val);\r\n}\r\nstatic unsigned int read_mac_reg(const struct pasemi_mac *mac, unsigned int reg)\r\n{\r\nreturn pasemi_read_mac_reg(mac->dma_if, reg);\r\n}\r\nstatic void write_mac_reg(const struct pasemi_mac *mac, unsigned int reg,\r\nunsigned int val)\r\n{\r\npasemi_write_mac_reg(mac->dma_if, reg, val);\r\n}\r\nstatic unsigned int read_dma_reg(unsigned int reg)\r\n{\r\nreturn pasemi_read_dma_reg(reg);\r\n}\r\nstatic void write_dma_reg(unsigned int reg, unsigned int val)\r\n{\r\npasemi_write_dma_reg(reg, val);\r\n}\r\nstatic struct pasemi_mac_rxring *rx_ring(const struct pasemi_mac *mac)\r\n{\r\nreturn mac->rx;\r\n}\r\nstatic struct pasemi_mac_txring *tx_ring(const struct pasemi_mac *mac)\r\n{\r\nreturn mac->tx;\r\n}\r\nstatic inline void prefetch_skb(const struct sk_buff *skb)\r\n{\r\nconst void *d = skb;\r\nprefetch(d);\r\nprefetch(d+64);\r\nprefetch(d+128);\r\nprefetch(d+192);\r\n}\r\nstatic int mac_to_intf(struct pasemi_mac *mac)\r\n{\r\nstruct pci_dev *pdev = mac->pdev;\r\nu32 tmp;\r\nint nintf, off, i, j;\r\nint devfn = pdev->devfn;\r\ntmp = read_dma_reg(PAS_DMA_CAP_IFI);\r\nnintf = (tmp & PAS_DMA_CAP_IFI_NIN_M) >> PAS_DMA_CAP_IFI_NIN_S;\r\noff = (tmp & PAS_DMA_CAP_IFI_IOFF_M) >> PAS_DMA_CAP_IFI_IOFF_S;\r\nfor (i = 0; i < (nintf+3)/4; i++) {\r\ntmp = read_dma_reg(off+4*i);\r\nfor (j = 0; j < 4; j++) {\r\nif (((tmp >> (8*j)) & 0xff) == devfn)\r\nreturn i*4 + j;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic void pasemi_mac_intf_disable(struct pasemi_mac *mac)\r\n{\r\nunsigned int flags;\r\nflags = read_mac_reg(mac, PAS_MAC_CFG_PCFG);\r\nflags &= ~PAS_MAC_CFG_PCFG_PE;\r\nwrite_mac_reg(mac, PAS_MAC_CFG_PCFG, flags);\r\n}\r\nstatic void pasemi_mac_intf_enable(struct pasemi_mac *mac)\r\n{\r\nunsigned int flags;\r\nflags = read_mac_reg(mac, PAS_MAC_CFG_PCFG);\r\nflags |= PAS_MAC_CFG_PCFG_PE;\r\nwrite_mac_reg(mac, PAS_MAC_CFG_PCFG, flags);\r\n}\r\nstatic int pasemi_get_mac_addr(struct pasemi_mac *mac)\r\n{\r\nstruct pci_dev *pdev = mac->pdev;\r\nstruct device_node *dn = pci_device_to_OF_node(pdev);\r\nint len;\r\nconst u8 *maddr;\r\nu8 addr[ETH_ALEN];\r\nif (!dn) {\r\ndev_dbg(&pdev->dev,\r\n"No device node for mac, not configuring\n");\r\nreturn -ENOENT;\r\n}\r\nmaddr = of_get_property(dn, "local-mac-address", &len);\r\nif (maddr && len == ETH_ALEN) {\r\nmemcpy(mac->mac_addr, maddr, ETH_ALEN);\r\nreturn 0;\r\n}\r\nif (maddr == NULL)\r\nmaddr = of_get_property(dn, "mac-address", NULL);\r\nif (maddr == NULL) {\r\ndev_warn(&pdev->dev,\r\n"no mac address in device tree, not configuring\n");\r\nreturn -ENOENT;\r\n}\r\nif (sscanf(maddr, "%hhx:%hhx:%hhx:%hhx:%hhx:%hhx",\r\n&addr[0], &addr[1], &addr[2], &addr[3], &addr[4], &addr[5])\r\n!= ETH_ALEN) {\r\ndev_warn(&pdev->dev,\r\n"can't parse mac address, not configuring\n");\r\nreturn -EINVAL;\r\n}\r\nmemcpy(mac->mac_addr, addr, ETH_ALEN);\r\nreturn 0;\r\n}\r\nstatic int pasemi_mac_set_mac_addr(struct net_device *dev, void *p)\r\n{\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nstruct sockaddr *addr = p;\r\nunsigned int adr0, adr1;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\r\nadr0 = dev->dev_addr[2] << 24 |\r\ndev->dev_addr[3] << 16 |\r\ndev->dev_addr[4] << 8 |\r\ndev->dev_addr[5];\r\nadr1 = read_mac_reg(mac, PAS_MAC_CFG_ADR1);\r\nadr1 &= ~0xffff;\r\nadr1 |= dev->dev_addr[0] << 8 | dev->dev_addr[1];\r\npasemi_mac_intf_disable(mac);\r\nwrite_mac_reg(mac, PAS_MAC_CFG_ADR0, adr0);\r\nwrite_mac_reg(mac, PAS_MAC_CFG_ADR1, adr1);\r\npasemi_mac_intf_enable(mac);\r\nreturn 0;\r\n}\r\nstatic int pasemi_mac_unmap_tx_skb(struct pasemi_mac *mac,\r\nconst int nfrags,\r\nstruct sk_buff *skb,\r\nconst dma_addr_t *dmas)\r\n{\r\nint f;\r\nstruct pci_dev *pdev = mac->dma_pdev;\r\npci_unmap_single(pdev, dmas[0], skb_headlen(skb), PCI_DMA_TODEVICE);\r\nfor (f = 0; f < nfrags; f++) {\r\nconst skb_frag_t *frag = &skb_shinfo(skb)->frags[f];\r\npci_unmap_page(pdev, dmas[f+1], skb_frag_size(frag), PCI_DMA_TODEVICE);\r\n}\r\ndev_kfree_skb_irq(skb);\r\nreturn (nfrags + 3) & ~1;\r\n}\r\nstatic struct pasemi_mac_csring *pasemi_mac_setup_csring(struct pasemi_mac *mac)\r\n{\r\nstruct pasemi_mac_csring *ring;\r\nu32 val;\r\nunsigned int cfg;\r\nint chno;\r\nring = pasemi_dma_alloc_chan(TXCHAN, sizeof(struct pasemi_mac_csring),\r\noffsetof(struct pasemi_mac_csring, chan));\r\nif (!ring) {\r\ndev_err(&mac->pdev->dev, "Can't allocate checksum channel\n");\r\ngoto out_chan;\r\n}\r\nchno = ring->chan.chno;\r\nring->size = CS_RING_SIZE;\r\nring->next_to_fill = 0;\r\nif (pasemi_dma_alloc_ring(&ring->chan, CS_RING_SIZE))\r\ngoto out_ring_desc;\r\nwrite_dma_reg(PAS_DMA_TXCHAN_BASEL(chno),\r\nPAS_DMA_TXCHAN_BASEL_BRBL(ring->chan.ring_dma));\r\nval = PAS_DMA_TXCHAN_BASEU_BRBH(ring->chan.ring_dma >> 32);\r\nval |= PAS_DMA_TXCHAN_BASEU_SIZ(CS_RING_SIZE >> 3);\r\nwrite_dma_reg(PAS_DMA_TXCHAN_BASEU(chno), val);\r\nring->events[0] = pasemi_dma_alloc_flag();\r\nring->events[1] = pasemi_dma_alloc_flag();\r\nif (ring->events[0] < 0 || ring->events[1] < 0)\r\ngoto out_flags;\r\npasemi_dma_clear_flag(ring->events[0]);\r\npasemi_dma_clear_flag(ring->events[1]);\r\nring->fun = pasemi_dma_alloc_fun();\r\nif (ring->fun < 0)\r\ngoto out_fun;\r\ncfg = PAS_DMA_TXCHAN_CFG_TY_FUNC | PAS_DMA_TXCHAN_CFG_UP |\r\nPAS_DMA_TXCHAN_CFG_TATTR(ring->fun) |\r\nPAS_DMA_TXCHAN_CFG_LPSQ | PAS_DMA_TXCHAN_CFG_LPDQ;\r\nif (translation_enabled())\r\ncfg |= PAS_DMA_TXCHAN_CFG_TRD | PAS_DMA_TXCHAN_CFG_TRR;\r\nwrite_dma_reg(PAS_DMA_TXCHAN_CFG(chno), cfg);\r\npasemi_dma_start_chan(&ring->chan, PAS_DMA_TXCHAN_TCMDSTA_SZ |\r\nPAS_DMA_TXCHAN_TCMDSTA_DB |\r\nPAS_DMA_TXCHAN_TCMDSTA_DE |\r\nPAS_DMA_TXCHAN_TCMDSTA_DA);\r\nreturn ring;\r\nout_fun:\r\nout_flags:\r\nif (ring->events[0] >= 0)\r\npasemi_dma_free_flag(ring->events[0]);\r\nif (ring->events[1] >= 0)\r\npasemi_dma_free_flag(ring->events[1]);\r\npasemi_dma_free_ring(&ring->chan);\r\nout_ring_desc:\r\npasemi_dma_free_chan(&ring->chan);\r\nout_chan:\r\nreturn NULL;\r\n}\r\nstatic void pasemi_mac_setup_csrings(struct pasemi_mac *mac)\r\n{\r\nint i;\r\nmac->cs[0] = pasemi_mac_setup_csring(mac);\r\nif (mac->type == MAC_TYPE_XAUI)\r\nmac->cs[1] = pasemi_mac_setup_csring(mac);\r\nelse\r\nmac->cs[1] = 0;\r\nfor (i = 0; i < MAX_CS; i++)\r\nif (mac->cs[i])\r\nmac->num_cs++;\r\n}\r\nstatic void pasemi_mac_free_csring(struct pasemi_mac_csring *csring)\r\n{\r\npasemi_dma_stop_chan(&csring->chan);\r\npasemi_dma_free_flag(csring->events[0]);\r\npasemi_dma_free_flag(csring->events[1]);\r\npasemi_dma_free_ring(&csring->chan);\r\npasemi_dma_free_chan(&csring->chan);\r\npasemi_dma_free_fun(csring->fun);\r\n}\r\nstatic int pasemi_mac_setup_rx_resources(const struct net_device *dev)\r\n{\r\nstruct pasemi_mac_rxring *ring;\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nint chno;\r\nunsigned int cfg;\r\nring = pasemi_dma_alloc_chan(RXCHAN, sizeof(struct pasemi_mac_rxring),\r\noffsetof(struct pasemi_mac_rxring, chan));\r\nif (!ring) {\r\ndev_err(&mac->pdev->dev, "Can't allocate RX channel\n");\r\ngoto out_chan;\r\n}\r\nchno = ring->chan.chno;\r\nspin_lock_init(&ring->lock);\r\nring->size = RX_RING_SIZE;\r\nring->ring_info = kzalloc(sizeof(struct pasemi_mac_buffer) *\r\nRX_RING_SIZE, GFP_KERNEL);\r\nif (!ring->ring_info)\r\ngoto out_ring_info;\r\nif (pasemi_dma_alloc_ring(&ring->chan, RX_RING_SIZE))\r\ngoto out_ring_desc;\r\nring->buffers = dma_zalloc_coherent(&mac->dma_pdev->dev,\r\nRX_RING_SIZE * sizeof(u64),\r\n&ring->buf_dma, GFP_KERNEL);\r\nif (!ring->buffers)\r\ngoto out_ring_desc;\r\nwrite_dma_reg(PAS_DMA_RXCHAN_BASEL(chno),\r\nPAS_DMA_RXCHAN_BASEL_BRBL(ring->chan.ring_dma));\r\nwrite_dma_reg(PAS_DMA_RXCHAN_BASEU(chno),\r\nPAS_DMA_RXCHAN_BASEU_BRBH(ring->chan.ring_dma >> 32) |\r\nPAS_DMA_RXCHAN_BASEU_SIZ(RX_RING_SIZE >> 3));\r\ncfg = PAS_DMA_RXCHAN_CFG_HBU(2);\r\nif (translation_enabled())\r\ncfg |= PAS_DMA_RXCHAN_CFG_CTR;\r\nwrite_dma_reg(PAS_DMA_RXCHAN_CFG(chno), cfg);\r\nwrite_dma_reg(PAS_DMA_RXINT_BASEL(mac->dma_if),\r\nPAS_DMA_RXINT_BASEL_BRBL(ring->buf_dma));\r\nwrite_dma_reg(PAS_DMA_RXINT_BASEU(mac->dma_if),\r\nPAS_DMA_RXINT_BASEU_BRBH(ring->buf_dma >> 32) |\r\nPAS_DMA_RXINT_BASEU_SIZ(RX_RING_SIZE >> 3));\r\ncfg = PAS_DMA_RXINT_CFG_DHL(2) | PAS_DMA_RXINT_CFG_L2 |\r\nPAS_DMA_RXINT_CFG_LW | PAS_DMA_RXINT_CFG_RBP |\r\nPAS_DMA_RXINT_CFG_HEN;\r\nif (translation_enabled())\r\ncfg |= PAS_DMA_RXINT_CFG_ITRR | PAS_DMA_RXINT_CFG_ITR;\r\nwrite_dma_reg(PAS_DMA_RXINT_CFG(mac->dma_if), cfg);\r\nring->next_to_fill = 0;\r\nring->next_to_clean = 0;\r\nring->mac = mac;\r\nmac->rx = ring;\r\nreturn 0;\r\nout_ring_desc:\r\nkfree(ring->ring_info);\r\nout_ring_info:\r\npasemi_dma_free_chan(&ring->chan);\r\nout_chan:\r\nreturn -ENOMEM;\r\n}\r\nstatic struct pasemi_mac_txring *\r\npasemi_mac_setup_tx_resources(const struct net_device *dev)\r\n{\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nu32 val;\r\nstruct pasemi_mac_txring *ring;\r\nunsigned int cfg;\r\nint chno;\r\nring = pasemi_dma_alloc_chan(TXCHAN, sizeof(struct pasemi_mac_txring),\r\noffsetof(struct pasemi_mac_txring, chan));\r\nif (!ring) {\r\ndev_err(&mac->pdev->dev, "Can't allocate TX channel\n");\r\ngoto out_chan;\r\n}\r\nchno = ring->chan.chno;\r\nspin_lock_init(&ring->lock);\r\nring->size = TX_RING_SIZE;\r\nring->ring_info = kzalloc(sizeof(struct pasemi_mac_buffer) *\r\nTX_RING_SIZE, GFP_KERNEL);\r\nif (!ring->ring_info)\r\ngoto out_ring_info;\r\nif (pasemi_dma_alloc_ring(&ring->chan, TX_RING_SIZE))\r\ngoto out_ring_desc;\r\nwrite_dma_reg(PAS_DMA_TXCHAN_BASEL(chno),\r\nPAS_DMA_TXCHAN_BASEL_BRBL(ring->chan.ring_dma));\r\nval = PAS_DMA_TXCHAN_BASEU_BRBH(ring->chan.ring_dma >> 32);\r\nval |= PAS_DMA_TXCHAN_BASEU_SIZ(TX_RING_SIZE >> 3);\r\nwrite_dma_reg(PAS_DMA_TXCHAN_BASEU(chno), val);\r\ncfg = PAS_DMA_TXCHAN_CFG_TY_IFACE |\r\nPAS_DMA_TXCHAN_CFG_TATTR(mac->dma_if) |\r\nPAS_DMA_TXCHAN_CFG_UP |\r\nPAS_DMA_TXCHAN_CFG_WT(4);\r\nif (translation_enabled())\r\ncfg |= PAS_DMA_TXCHAN_CFG_TRD | PAS_DMA_TXCHAN_CFG_TRR;\r\nwrite_dma_reg(PAS_DMA_TXCHAN_CFG(chno), cfg);\r\nring->next_to_fill = 0;\r\nring->next_to_clean = 0;\r\nring->mac = mac;\r\nreturn ring;\r\nout_ring_desc:\r\nkfree(ring->ring_info);\r\nout_ring_info:\r\npasemi_dma_free_chan(&ring->chan);\r\nout_chan:\r\nreturn NULL;\r\n}\r\nstatic void pasemi_mac_free_tx_resources(struct pasemi_mac *mac)\r\n{\r\nstruct pasemi_mac_txring *txring = tx_ring(mac);\r\nunsigned int i, j;\r\nstruct pasemi_mac_buffer *info;\r\ndma_addr_t dmas[MAX_SKB_FRAGS+1];\r\nint freed, nfrags;\r\nint start, limit;\r\nstart = txring->next_to_clean;\r\nlimit = txring->next_to_fill;\r\nif (start > limit)\r\nlimit += TX_RING_SIZE;\r\nfor (i = start; i < limit; i += freed) {\r\ninfo = &txring->ring_info[(i+1) & (TX_RING_SIZE-1)];\r\nif (info->dma && info->skb) {\r\nnfrags = skb_shinfo(info->skb)->nr_frags;\r\nfor (j = 0; j <= nfrags; j++)\r\ndmas[j] = txring->ring_info[(i+1+j) &\r\n(TX_RING_SIZE-1)].dma;\r\nfreed = pasemi_mac_unmap_tx_skb(mac, nfrags,\r\ninfo->skb, dmas);\r\n} else {\r\nfreed = 2;\r\n}\r\n}\r\nkfree(txring->ring_info);\r\npasemi_dma_free_chan(&txring->chan);\r\n}\r\nstatic void pasemi_mac_free_rx_buffers(struct pasemi_mac *mac)\r\n{\r\nstruct pasemi_mac_rxring *rx = rx_ring(mac);\r\nunsigned int i;\r\nstruct pasemi_mac_buffer *info;\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\ninfo = &RX_DESC_INFO(rx, i);\r\nif (info->skb && info->dma) {\r\npci_unmap_single(mac->dma_pdev,\r\ninfo->dma,\r\ninfo->skb->len,\r\nPCI_DMA_FROMDEVICE);\r\ndev_kfree_skb_any(info->skb);\r\n}\r\ninfo->dma = 0;\r\ninfo->skb = NULL;\r\n}\r\nfor (i = 0; i < RX_RING_SIZE; i++)\r\nRX_BUFF(rx, i) = 0;\r\n}\r\nstatic void pasemi_mac_free_rx_resources(struct pasemi_mac *mac)\r\n{\r\npasemi_mac_free_rx_buffers(mac);\r\ndma_free_coherent(&mac->dma_pdev->dev, RX_RING_SIZE * sizeof(u64),\r\nrx_ring(mac)->buffers, rx_ring(mac)->buf_dma);\r\nkfree(rx_ring(mac)->ring_info);\r\npasemi_dma_free_chan(&rx_ring(mac)->chan);\r\nmac->rx = NULL;\r\n}\r\nstatic void pasemi_mac_replenish_rx_ring(struct net_device *dev,\r\nconst int limit)\r\n{\r\nconst struct pasemi_mac *mac = netdev_priv(dev);\r\nstruct pasemi_mac_rxring *rx = rx_ring(mac);\r\nint fill, count;\r\nif (limit <= 0)\r\nreturn;\r\nfill = rx_ring(mac)->next_to_fill;\r\nfor (count = 0; count < limit; count++) {\r\nstruct pasemi_mac_buffer *info = &RX_DESC_INFO(rx, fill);\r\nu64 *buff = &RX_BUFF(rx, fill);\r\nstruct sk_buff *skb;\r\ndma_addr_t dma;\r\nWARN_ON(*buff);\r\nskb = netdev_alloc_skb(dev, mac->bufsz);\r\nskb_reserve(skb, LOCAL_SKB_ALIGN);\r\nif (unlikely(!skb))\r\nbreak;\r\ndma = pci_map_single(mac->dma_pdev, skb->data,\r\nmac->bufsz - LOCAL_SKB_ALIGN,\r\nPCI_DMA_FROMDEVICE);\r\nif (unlikely(pci_dma_mapping_error(mac->dma_pdev, dma))) {\r\ndev_kfree_skb_irq(info->skb);\r\nbreak;\r\n}\r\ninfo->skb = skb;\r\ninfo->dma = dma;\r\n*buff = XCT_RXB_LEN(mac->bufsz) | XCT_RXB_ADDR(dma);\r\nfill++;\r\n}\r\nwmb();\r\nwrite_dma_reg(PAS_DMA_RXINT_INCR(mac->dma_if), count);\r\nrx_ring(mac)->next_to_fill = (rx_ring(mac)->next_to_fill + count) &\r\n(RX_RING_SIZE - 1);\r\n}\r\nstatic void pasemi_mac_restart_rx_intr(const struct pasemi_mac *mac)\r\n{\r\nstruct pasemi_mac_rxring *rx = rx_ring(mac);\r\nunsigned int reg, pcnt;\r\npcnt = *rx->chan.status & PAS_STATUS_PCNT_M;\r\nreg = PAS_IOB_DMA_RXCH_RESET_PCNT(pcnt) | PAS_IOB_DMA_RXCH_RESET_PINTC;\r\nif (*rx->chan.status & PAS_STATUS_TIMER)\r\nreg |= PAS_IOB_DMA_RXCH_RESET_TINTC;\r\nwrite_iob_reg(PAS_IOB_DMA_RXCH_RESET(mac->rx->chan.chno), reg);\r\n}\r\nstatic void pasemi_mac_restart_tx_intr(const struct pasemi_mac *mac)\r\n{\r\nunsigned int reg, pcnt;\r\npcnt = *tx_ring(mac)->chan.status & PAS_STATUS_PCNT_M;\r\nreg = PAS_IOB_DMA_TXCH_RESET_PCNT(pcnt) | PAS_IOB_DMA_TXCH_RESET_PINTC;\r\nwrite_iob_reg(PAS_IOB_DMA_TXCH_RESET(tx_ring(mac)->chan.chno), reg);\r\n}\r\nstatic inline void pasemi_mac_rx_error(const struct pasemi_mac *mac,\r\nconst u64 macrx)\r\n{\r\nunsigned int rcmdsta, ccmdsta;\r\nstruct pasemi_dmachan *chan = &rx_ring(mac)->chan;\r\nif (!netif_msg_rx_err(mac))\r\nreturn;\r\nrcmdsta = read_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if));\r\nccmdsta = read_dma_reg(PAS_DMA_RXCHAN_CCMDSTA(chan->chno));\r\nprintk(KERN_ERR "pasemi_mac: rx error. macrx %016llx, rx status %llx\n",\r\nmacrx, *chan->status);\r\nprintk(KERN_ERR "pasemi_mac: rcmdsta %08x ccmdsta %08x\n",\r\nrcmdsta, ccmdsta);\r\n}\r\nstatic inline void pasemi_mac_tx_error(const struct pasemi_mac *mac,\r\nconst u64 mactx)\r\n{\r\nunsigned int cmdsta;\r\nstruct pasemi_dmachan *chan = &tx_ring(mac)->chan;\r\nif (!netif_msg_tx_err(mac))\r\nreturn;\r\ncmdsta = read_dma_reg(PAS_DMA_TXCHAN_TCMDSTA(chan->chno));\r\nprintk(KERN_ERR "pasemi_mac: tx error. mactx 0x%016llx, "\\r\n"tx status 0x%016llx\n", mactx, *chan->status);\r\nprintk(KERN_ERR "pasemi_mac: tcmdsta 0x%08x\n", cmdsta);\r\n}\r\nstatic int pasemi_mac_clean_rx(struct pasemi_mac_rxring *rx,\r\nconst int limit)\r\n{\r\nconst struct pasemi_dmachan *chan = &rx->chan;\r\nstruct pasemi_mac *mac = rx->mac;\r\nstruct pci_dev *pdev = mac->dma_pdev;\r\nunsigned int n;\r\nint count, buf_index, tot_bytes, packets;\r\nstruct pasemi_mac_buffer *info;\r\nstruct sk_buff *skb;\r\nunsigned int len;\r\nu64 macrx, eval;\r\ndma_addr_t dma;\r\ntot_bytes = 0;\r\npackets = 0;\r\nspin_lock(&rx->lock);\r\nn = rx->next_to_clean;\r\nprefetch(&RX_DESC(rx, n));\r\nfor (count = 0; count < limit; count++) {\r\nmacrx = RX_DESC(rx, n);\r\nprefetch(&RX_DESC(rx, n+4));\r\nif ((macrx & XCT_MACRX_E) ||\r\n(*chan->status & PAS_STATUS_ERROR))\r\npasemi_mac_rx_error(mac, macrx);\r\nif (!(macrx & XCT_MACRX_O))\r\nbreak;\r\ninfo = NULL;\r\nBUG_ON(!(macrx & XCT_MACRX_RR_8BRES));\r\neval = (RX_DESC(rx, n+1) & XCT_RXRES_8B_EVAL_M) >>\r\nXCT_RXRES_8B_EVAL_S;\r\nbuf_index = eval-1;\r\ndma = (RX_DESC(rx, n+2) & XCT_PTR_ADDR_M);\r\ninfo = &RX_DESC_INFO(rx, buf_index);\r\nskb = info->skb;\r\nprefetch_skb(skb);\r\nlen = (macrx & XCT_MACRX_LLEN_M) >> XCT_MACRX_LLEN_S;\r\npci_unmap_single(pdev, dma, mac->bufsz - LOCAL_SKB_ALIGN,\r\nPCI_DMA_FROMDEVICE);\r\nif (macrx & XCT_MACRX_CRC) {\r\nmac->netdev->stats.rx_errors++;\r\nmac->netdev->stats.rx_crc_errors++;\r\ngoto next;\r\n}\r\ninfo->skb = NULL;\r\ninfo->dma = 0;\r\nif (likely((macrx & XCT_MACRX_HTY_M) == XCT_MACRX_HTY_IPV4_OK)) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb->csum = (macrx & XCT_MACRX_CSUM_M) >>\r\nXCT_MACRX_CSUM_S;\r\n} else {\r\nskb_checksum_none_assert(skb);\r\n}\r\npackets++;\r\ntot_bytes += len;\r\nskb_put(skb, len-4);\r\nskb->protocol = eth_type_trans(skb, mac->netdev);\r\nnapi_gro_receive(&mac->napi, skb);\r\nnext:\r\nRX_DESC(rx, n) = 0;\r\nRX_DESC(rx, n+1) = 0;\r\nRX_BUFF(rx, buf_index) = 0;\r\nn += 4;\r\n}\r\nif (n > RX_RING_SIZE) {\r\nwrite_iob_reg(PAS_IOB_COM_PKTHDRCNT, 0);\r\nn &= (RX_RING_SIZE-1);\r\n}\r\nrx_ring(mac)->next_to_clean = n;\r\nwrite_dma_reg(PAS_DMA_RXCHAN_INCR(mac->rx->chan.chno), count << 1);\r\npasemi_mac_replenish_rx_ring(mac->netdev, count);\r\nmac->netdev->stats.rx_bytes += tot_bytes;\r\nmac->netdev->stats.rx_packets += packets;\r\nspin_unlock(&rx_ring(mac)->lock);\r\nreturn count;\r\n}\r\nstatic int pasemi_mac_clean_tx(struct pasemi_mac_txring *txring)\r\n{\r\nstruct pasemi_dmachan *chan = &txring->chan;\r\nstruct pasemi_mac *mac = txring->mac;\r\nint i, j;\r\nunsigned int start, descr_count, buf_count, batch_limit;\r\nunsigned int ring_limit;\r\nunsigned int total_count;\r\nunsigned long flags;\r\nstruct sk_buff *skbs[TX_CLEAN_BATCHSIZE];\r\ndma_addr_t dmas[TX_CLEAN_BATCHSIZE][MAX_SKB_FRAGS+1];\r\nint nf[TX_CLEAN_BATCHSIZE];\r\nint nr_frags;\r\ntotal_count = 0;\r\nbatch_limit = TX_CLEAN_BATCHSIZE;\r\nrestart:\r\nspin_lock_irqsave(&txring->lock, flags);\r\nstart = txring->next_to_clean;\r\nring_limit = txring->next_to_fill;\r\nprefetch(&TX_DESC_INFO(txring, start+1).skb);\r\nif (start > ring_limit)\r\nring_limit += TX_RING_SIZE;\r\nbuf_count = 0;\r\ndescr_count = 0;\r\nfor (i = start;\r\ndescr_count < batch_limit && i < ring_limit;\r\ni += buf_count) {\r\nu64 mactx = TX_DESC(txring, i);\r\nstruct sk_buff *skb;\r\nif ((mactx & XCT_MACTX_E) ||\r\n(*chan->status & PAS_STATUS_ERROR))\r\npasemi_mac_tx_error(mac, mactx);\r\nif (!(mactx & XCT_MACTX_LLEN_M)) {\r\nTX_DESC(txring, i) = 0;\r\nTX_DESC(txring, i+1) = 0;\r\nbuf_count = 2;\r\ncontinue;\r\n}\r\nskb = TX_DESC_INFO(txring, i+1).skb;\r\nnr_frags = TX_DESC_INFO(txring, i).dma;\r\nif (unlikely(mactx & XCT_MACTX_O))\r\nbreak;\r\nbuf_count = 2 + nr_frags;\r\nif (buf_count & 1)\r\nbuf_count++;\r\nfor (j = 0; j <= nr_frags; j++)\r\ndmas[descr_count][j] = TX_DESC_INFO(txring, i+1+j).dma;\r\nskbs[descr_count] = skb;\r\nnf[descr_count] = nr_frags;\r\nTX_DESC(txring, i) = 0;\r\nTX_DESC(txring, i+1) = 0;\r\ndescr_count++;\r\n}\r\ntxring->next_to_clean = i & (TX_RING_SIZE-1);\r\nspin_unlock_irqrestore(&txring->lock, flags);\r\nnetif_wake_queue(mac->netdev);\r\nfor (i = 0; i < descr_count; i++)\r\npasemi_mac_unmap_tx_skb(mac, nf[i], skbs[i], dmas[i]);\r\ntotal_count += descr_count;\r\nif (descr_count == batch_limit)\r\ngoto restart;\r\nreturn total_count;\r\n}\r\nstatic irqreturn_t pasemi_mac_rx_intr(int irq, void *data)\r\n{\r\nconst struct pasemi_mac_rxring *rxring = data;\r\nstruct pasemi_mac *mac = rxring->mac;\r\nconst struct pasemi_dmachan *chan = &rxring->chan;\r\nunsigned int reg;\r\nif (!(*chan->status & PAS_STATUS_CAUSE_M))\r\nreturn IRQ_NONE;\r\nreg = 0;\r\nif (*chan->status & PAS_STATUS_SOFT)\r\nreg |= PAS_IOB_DMA_RXCH_RESET_SINTC;\r\nif (*chan->status & PAS_STATUS_ERROR)\r\nreg |= PAS_IOB_DMA_RXCH_RESET_DINTC;\r\nnapi_schedule(&mac->napi);\r\nwrite_iob_reg(PAS_IOB_DMA_RXCH_RESET(chan->chno), reg);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void pasemi_mac_tx_timer(unsigned long data)\r\n{\r\nstruct pasemi_mac_txring *txring = (struct pasemi_mac_txring *)data;\r\nstruct pasemi_mac *mac = txring->mac;\r\npasemi_mac_clean_tx(txring);\r\nmod_timer(&txring->clean_timer, jiffies + TX_CLEAN_INTERVAL);\r\npasemi_mac_restart_tx_intr(mac);\r\n}\r\nstatic irqreturn_t pasemi_mac_tx_intr(int irq, void *data)\r\n{\r\nstruct pasemi_mac_txring *txring = data;\r\nconst struct pasemi_dmachan *chan = &txring->chan;\r\nstruct pasemi_mac *mac = txring->mac;\r\nunsigned int reg;\r\nif (!(*chan->status & PAS_STATUS_CAUSE_M))\r\nreturn IRQ_NONE;\r\nreg = 0;\r\nif (*chan->status & PAS_STATUS_SOFT)\r\nreg |= PAS_IOB_DMA_TXCH_RESET_SINTC;\r\nif (*chan->status & PAS_STATUS_ERROR)\r\nreg |= PAS_IOB_DMA_TXCH_RESET_DINTC;\r\nmod_timer(&txring->clean_timer, jiffies + (TX_CLEAN_INTERVAL)*2);\r\nnapi_schedule(&mac->napi);\r\nif (reg)\r\nwrite_iob_reg(PAS_IOB_DMA_TXCH_RESET(chan->chno), reg);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void pasemi_adjust_link(struct net_device *dev)\r\n{\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nint msg;\r\nunsigned int flags;\r\nunsigned int new_flags;\r\nif (!dev->phydev->link) {\r\nif (mac->link && netif_msg_link(mac))\r\nprintk(KERN_INFO "%s: Link is down.\n", dev->name);\r\nnetif_carrier_off(dev);\r\npasemi_mac_intf_disable(mac);\r\nmac->link = 0;\r\nreturn;\r\n} else {\r\npasemi_mac_intf_enable(mac);\r\nnetif_carrier_on(dev);\r\n}\r\nflags = read_mac_reg(mac, PAS_MAC_CFG_PCFG);\r\nnew_flags = flags & ~(PAS_MAC_CFG_PCFG_HD | PAS_MAC_CFG_PCFG_SPD_M |\r\nPAS_MAC_CFG_PCFG_TSR_M);\r\nif (!dev->phydev->duplex)\r\nnew_flags |= PAS_MAC_CFG_PCFG_HD;\r\nswitch (dev->phydev->speed) {\r\ncase 1000:\r\nnew_flags |= PAS_MAC_CFG_PCFG_SPD_1G |\r\nPAS_MAC_CFG_PCFG_TSR_1G;\r\nbreak;\r\ncase 100:\r\nnew_flags |= PAS_MAC_CFG_PCFG_SPD_100M |\r\nPAS_MAC_CFG_PCFG_TSR_100M;\r\nbreak;\r\ncase 10:\r\nnew_flags |= PAS_MAC_CFG_PCFG_SPD_10M |\r\nPAS_MAC_CFG_PCFG_TSR_10M;\r\nbreak;\r\ndefault:\r\nprintk("Unsupported speed %d\n", dev->phydev->speed);\r\n}\r\nmsg = mac->link != dev->phydev->link || flags != new_flags;\r\nmac->duplex = dev->phydev->duplex;\r\nmac->speed = dev->phydev->speed;\r\nmac->link = dev->phydev->link;\r\nif (new_flags != flags)\r\nwrite_mac_reg(mac, PAS_MAC_CFG_PCFG, new_flags);\r\nif (msg && netif_msg_link(mac))\r\nprintk(KERN_INFO "%s: Link is up at %d Mbps, %s duplex.\n",\r\ndev->name, mac->speed, mac->duplex ? "full" : "half");\r\n}\r\nstatic int pasemi_mac_phy_init(struct net_device *dev)\r\n{\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nstruct device_node *dn, *phy_dn;\r\nstruct phy_device *phydev;\r\ndn = pci_device_to_OF_node(mac->pdev);\r\nphy_dn = of_parse_phandle(dn, "phy-handle", 0);\r\nof_node_put(phy_dn);\r\nmac->link = 0;\r\nmac->speed = 0;\r\nmac->duplex = -1;\r\nphydev = of_phy_connect(dev, phy_dn, &pasemi_adjust_link, 0,\r\nPHY_INTERFACE_MODE_SGMII);\r\nif (!phydev) {\r\nprintk(KERN_ERR "%s: Could not attach to phy\n", dev->name);\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pasemi_mac_open(struct net_device *dev)\r\n{\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nunsigned int flags;\r\nint i, ret;\r\nflags = PAS_MAC_CFG_TXP_FCE | PAS_MAC_CFG_TXP_FPC(3) |\r\nPAS_MAC_CFG_TXP_SL(3) | PAS_MAC_CFG_TXP_COB(0xf) |\r\nPAS_MAC_CFG_TXP_TIFT(8) | PAS_MAC_CFG_TXP_TIFG(12);\r\nwrite_mac_reg(mac, PAS_MAC_CFG_TXP, flags);\r\nret = pasemi_mac_setup_rx_resources(dev);\r\nif (ret)\r\ngoto out_rx_resources;\r\nmac->tx = pasemi_mac_setup_tx_resources(dev);\r\nif (!mac->tx)\r\ngoto out_tx_ring;\r\nif (dev->mtu > 1500 && !mac->num_cs) {\r\npasemi_mac_setup_csrings(mac);\r\nif (!mac->num_cs)\r\ngoto out_tx_ring;\r\n}\r\nfor (i = 0; i < 32; i++)\r\nwrite_mac_reg(mac, PAS_MAC_RMON(i), 0);\r\nwrite_iob_reg(PAS_IOB_DMA_COM_TIMEOUTCFG,\r\nPAS_IOB_DMA_COM_TIMEOUTCFG_TCNT(0x3ff));\r\nwrite_iob_reg(PAS_IOB_DMA_RXCH_CFG(mac->rx->chan.chno),\r\nPAS_IOB_DMA_RXCH_CFG_CNTTH(256));\r\nwrite_iob_reg(PAS_IOB_DMA_TXCH_CFG(mac->tx->chan.chno),\r\nPAS_IOB_DMA_TXCH_CFG_CNTTH(32));\r\nwrite_mac_reg(mac, PAS_MAC_IPC_CHNL,\r\nPAS_MAC_IPC_CHNL_DCHNO(mac->rx->chan.chno) |\r\nPAS_MAC_IPC_CHNL_BCH(mac->rx->chan.chno));\r\nwrite_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if),\r\nPAS_DMA_RXINT_RCMDSTA_EN |\r\nPAS_DMA_RXINT_RCMDSTA_DROPS_M |\r\nPAS_DMA_RXINT_RCMDSTA_BP |\r\nPAS_DMA_RXINT_RCMDSTA_OO |\r\nPAS_DMA_RXINT_RCMDSTA_BT);\r\npasemi_dma_start_chan(&rx_ring(mac)->chan, PAS_DMA_RXCHAN_CCMDSTA_DU |\r\nPAS_DMA_RXCHAN_CCMDSTA_OD |\r\nPAS_DMA_RXCHAN_CCMDSTA_FD |\r\nPAS_DMA_RXCHAN_CCMDSTA_DT);\r\npasemi_dma_start_chan(&tx_ring(mac)->chan, PAS_DMA_TXCHAN_TCMDSTA_SZ |\r\nPAS_DMA_TXCHAN_TCMDSTA_DB |\r\nPAS_DMA_TXCHAN_TCMDSTA_DE |\r\nPAS_DMA_TXCHAN_TCMDSTA_DA);\r\npasemi_mac_replenish_rx_ring(dev, RX_RING_SIZE);\r\nwrite_dma_reg(PAS_DMA_RXCHAN_INCR(rx_ring(mac)->chan.chno),\r\nRX_RING_SIZE>>1);\r\npasemi_mac_restart_rx_intr(mac);\r\npasemi_mac_restart_tx_intr(mac);\r\nflags = PAS_MAC_CFG_PCFG_S1 | PAS_MAC_CFG_PCFG_PR | PAS_MAC_CFG_PCFG_CE;\r\nif (mac->type == MAC_TYPE_GMAC)\r\nflags |= PAS_MAC_CFG_PCFG_TSR_1G | PAS_MAC_CFG_PCFG_SPD_1G;\r\nelse\r\nflags |= PAS_MAC_CFG_PCFG_TSR_10G | PAS_MAC_CFG_PCFG_SPD_10G;\r\nwrite_mac_reg(mac, PAS_MAC_CFG_PCFG, flags);\r\nret = pasemi_mac_phy_init(dev);\r\nif (ret) {\r\npasemi_mac_intf_enable(mac);\r\nif (mac->type == MAC_TYPE_GMAC) {\r\ndev_warn(&mac->pdev->dev,\r\n"PHY init failed: %d.\n", ret);\r\ndev_warn(&mac->pdev->dev,\r\n"Defaulting to 1Gbit full duplex\n");\r\n}\r\n}\r\nnetif_start_queue(dev);\r\nnapi_enable(&mac->napi);\r\nsnprintf(mac->tx_irq_name, sizeof(mac->tx_irq_name), "%s tx",\r\ndev->name);\r\nret = request_irq(mac->tx->chan.irq, pasemi_mac_tx_intr, 0,\r\nmac->tx_irq_name, mac->tx);\r\nif (ret) {\r\ndev_err(&mac->pdev->dev, "request_irq of irq %d failed: %d\n",\r\nmac->tx->chan.irq, ret);\r\ngoto out_tx_int;\r\n}\r\nsnprintf(mac->rx_irq_name, sizeof(mac->rx_irq_name), "%s rx",\r\ndev->name);\r\nret = request_irq(mac->rx->chan.irq, pasemi_mac_rx_intr, 0,\r\nmac->rx_irq_name, mac->rx);\r\nif (ret) {\r\ndev_err(&mac->pdev->dev, "request_irq of irq %d failed: %d\n",\r\nmac->rx->chan.irq, ret);\r\ngoto out_rx_int;\r\n}\r\nif (dev->phydev)\r\nphy_start(dev->phydev);\r\nsetup_timer(&mac->tx->clean_timer, pasemi_mac_tx_timer,\r\n(unsigned long)mac->tx);\r\nmod_timer(&mac->tx->clean_timer, jiffies + HZ);\r\nreturn 0;\r\nout_rx_int:\r\nfree_irq(mac->tx->chan.irq, mac->tx);\r\nout_tx_int:\r\nnapi_disable(&mac->napi);\r\nnetif_stop_queue(dev);\r\nout_tx_ring:\r\nif (mac->tx)\r\npasemi_mac_free_tx_resources(mac);\r\npasemi_mac_free_rx_resources(mac);\r\nout_rx_resources:\r\nreturn ret;\r\n}\r\nstatic void pasemi_mac_pause_txchan(struct pasemi_mac *mac)\r\n{\r\nunsigned int sta, retries;\r\nint txch = tx_ring(mac)->chan.chno;\r\nwrite_dma_reg(PAS_DMA_TXCHAN_TCMDSTA(txch),\r\nPAS_DMA_TXCHAN_TCMDSTA_ST);\r\nfor (retries = 0; retries < MAX_RETRIES; retries++) {\r\nsta = read_dma_reg(PAS_DMA_TXCHAN_TCMDSTA(txch));\r\nif (!(sta & PAS_DMA_TXCHAN_TCMDSTA_ACT))\r\nbreak;\r\ncond_resched();\r\n}\r\nif (sta & PAS_DMA_TXCHAN_TCMDSTA_ACT)\r\ndev_err(&mac->dma_pdev->dev,\r\n"Failed to stop tx channel, tcmdsta %08x\n", sta);\r\nwrite_dma_reg(PAS_DMA_TXCHAN_TCMDSTA(txch), 0);\r\n}\r\nstatic void pasemi_mac_pause_rxchan(struct pasemi_mac *mac)\r\n{\r\nunsigned int sta, retries;\r\nint rxch = rx_ring(mac)->chan.chno;\r\nwrite_dma_reg(PAS_DMA_RXCHAN_CCMDSTA(rxch),\r\nPAS_DMA_RXCHAN_CCMDSTA_ST);\r\nfor (retries = 0; retries < MAX_RETRIES; retries++) {\r\nsta = read_dma_reg(PAS_DMA_RXCHAN_CCMDSTA(rxch));\r\nif (!(sta & PAS_DMA_RXCHAN_CCMDSTA_ACT))\r\nbreak;\r\ncond_resched();\r\n}\r\nif (sta & PAS_DMA_RXCHAN_CCMDSTA_ACT)\r\ndev_err(&mac->dma_pdev->dev,\r\n"Failed to stop rx channel, ccmdsta 08%x\n", sta);\r\nwrite_dma_reg(PAS_DMA_RXCHAN_CCMDSTA(rxch), 0);\r\n}\r\nstatic void pasemi_mac_pause_rxint(struct pasemi_mac *mac)\r\n{\r\nunsigned int sta, retries;\r\nwrite_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if),\r\nPAS_DMA_RXINT_RCMDSTA_ST);\r\nfor (retries = 0; retries < MAX_RETRIES; retries++) {\r\nsta = read_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if));\r\nif (!(sta & PAS_DMA_RXINT_RCMDSTA_ACT))\r\nbreak;\r\ncond_resched();\r\n}\r\nif (sta & PAS_DMA_RXINT_RCMDSTA_ACT)\r\ndev_err(&mac->dma_pdev->dev,\r\n"Failed to stop rx interface, rcmdsta %08x\n", sta);\r\nwrite_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if), 0);\r\n}\r\nstatic int pasemi_mac_close(struct net_device *dev)\r\n{\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nunsigned int sta;\r\nint rxch, txch, i;\r\nrxch = rx_ring(mac)->chan.chno;\r\ntxch = tx_ring(mac)->chan.chno;\r\nif (dev->phydev) {\r\nphy_stop(dev->phydev);\r\nphy_disconnect(dev->phydev);\r\n}\r\ndel_timer_sync(&mac->tx->clean_timer);\r\nnetif_stop_queue(dev);\r\nnapi_disable(&mac->napi);\r\nsta = read_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if));\r\nif (sta & (PAS_DMA_RXINT_RCMDSTA_BP |\r\nPAS_DMA_RXINT_RCMDSTA_OO |\r\nPAS_DMA_RXINT_RCMDSTA_BT))\r\nprintk(KERN_DEBUG "pasemi_mac: rcmdsta error: 0x%08x\n", sta);\r\nsta = read_dma_reg(PAS_DMA_RXCHAN_CCMDSTA(rxch));\r\nif (sta & (PAS_DMA_RXCHAN_CCMDSTA_DU |\r\nPAS_DMA_RXCHAN_CCMDSTA_OD |\r\nPAS_DMA_RXCHAN_CCMDSTA_FD |\r\nPAS_DMA_RXCHAN_CCMDSTA_DT))\r\nprintk(KERN_DEBUG "pasemi_mac: ccmdsta error: 0x%08x\n", sta);\r\nsta = read_dma_reg(PAS_DMA_TXCHAN_TCMDSTA(txch));\r\nif (sta & (PAS_DMA_TXCHAN_TCMDSTA_SZ | PAS_DMA_TXCHAN_TCMDSTA_DB |\r\nPAS_DMA_TXCHAN_TCMDSTA_DE | PAS_DMA_TXCHAN_TCMDSTA_DA))\r\nprintk(KERN_DEBUG "pasemi_mac: tcmdsta error: 0x%08x\n", sta);\r\npasemi_mac_clean_tx(tx_ring(mac));\r\npasemi_mac_clean_rx(rx_ring(mac), RX_RING_SIZE);\r\npasemi_mac_pause_txchan(mac);\r\npasemi_mac_pause_rxint(mac);\r\npasemi_mac_pause_rxchan(mac);\r\npasemi_mac_intf_disable(mac);\r\nfree_irq(mac->tx->chan.irq, mac->tx);\r\nfree_irq(mac->rx->chan.irq, mac->rx);\r\nfor (i = 0; i < mac->num_cs; i++) {\r\npasemi_mac_free_csring(mac->cs[i]);\r\nmac->cs[i] = NULL;\r\n}\r\nmac->num_cs = 0;\r\npasemi_mac_free_rx_resources(mac);\r\npasemi_mac_free_tx_resources(mac);\r\nreturn 0;\r\n}\r\nstatic void pasemi_mac_queue_csdesc(const struct sk_buff *skb,\r\nconst dma_addr_t *map,\r\nconst unsigned int *map_size,\r\nstruct pasemi_mac_txring *txring,\r\nstruct pasemi_mac_csring *csring)\r\n{\r\nu64 fund;\r\ndma_addr_t cs_dest;\r\nconst int nh_off = skb_network_offset(skb);\r\nconst int nh_len = skb_network_header_len(skb);\r\nconst int nfrags = skb_shinfo(skb)->nr_frags;\r\nint cs_size, i, fill, hdr, cpyhdr, evt;\r\ndma_addr_t csdma;\r\nfund = XCT_FUN_ST | XCT_FUN_RR_8BRES |\r\nXCT_FUN_O | XCT_FUN_FUN(csring->fun) |\r\nXCT_FUN_CRM_SIG | XCT_FUN_LLEN(skb->len - nh_off) |\r\nXCT_FUN_SHL(nh_len >> 2) | XCT_FUN_SE;\r\nswitch (ip_hdr(skb)->protocol) {\r\ncase IPPROTO_TCP:\r\nfund |= XCT_FUN_SIG_TCP4;\r\ncs_dest = map[0] + skb_transport_offset(skb) + 16;\r\nbreak;\r\ncase IPPROTO_UDP:\r\nfund |= XCT_FUN_SIG_UDP4;\r\ncs_dest = map[0] + skb_transport_offset(skb) + 6;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nfill = csring->next_to_fill;\r\nhdr = fill;\r\nCS_DESC(csring, fill++) = fund;\r\ncsdma = csring->chan.ring_dma + (fill & (CS_RING_SIZE-1)) * 8 + 2;\r\nCS_DESC(csring, fill++) = 0;\r\nCS_DESC(csring, fill) = XCT_PTR_LEN(map_size[0]-nh_off) | XCT_PTR_ADDR(map[0]+nh_off);\r\nfor (i = 1; i <= nfrags; i++)\r\nCS_DESC(csring, fill+i) = XCT_PTR_LEN(map_size[i]) | XCT_PTR_ADDR(map[i]);\r\nfill += i;\r\nif (fill & 1)\r\nfill++;\r\ncpyhdr = fill;\r\nCS_DESC(csring, fill++) = XCT_FUN_O | XCT_FUN_FUN(csring->fun) |\r\nXCT_FUN_LLEN(2) | XCT_FUN_SE;\r\nCS_DESC(csring, fill++) = XCT_PTR_LEN(2) | XCT_PTR_ADDR(cs_dest) | XCT_PTR_T;\r\nCS_DESC(csring, fill++) = XCT_PTR_LEN(2) | XCT_PTR_ADDR(csdma);\r\nfill++;\r\nevt = !csring->last_event;\r\ncsring->last_event = evt;\r\nCS_DESC(csring, fill++) = CTRL_CMD_T | CTRL_CMD_META_EVT | CTRL_CMD_O |\r\nCTRL_CMD_ETYPE_SET | CTRL_CMD_REG(csring->events[evt]);\r\nCS_DESC(csring, fill++) = 0;\r\nCS_DESC(csring, fill++) = CTRL_CMD_T | CTRL_CMD_META_EVT | CTRL_CMD_O |\r\nCTRL_CMD_ETYPE_WCLR | CTRL_CMD_REG(csring->events[!evt]);\r\nCS_DESC(csring, fill++) = 0;\r\ncsring->next_to_fill = fill & (CS_RING_SIZE-1);\r\ncs_size = fill - hdr;\r\nwrite_dma_reg(PAS_DMA_TXCHAN_INCR(csring->chan.chno), (cs_size) >> 1);\r\nfill = txring->next_to_fill;\r\nTX_DESC(txring, fill++) = CTRL_CMD_T | CTRL_CMD_META_EVT | CTRL_CMD_O |\r\nCTRL_CMD_ETYPE_WSET | CTRL_CMD_REG(csring->events[evt]);\r\nTX_DESC(txring, fill++) = 0;\r\nTX_DESC(txring, fill++) = CTRL_CMD_T | CTRL_CMD_META_EVT | CTRL_CMD_O |\r\nCTRL_CMD_ETYPE_CLR | CTRL_CMD_REG(csring->events[!evt]);\r\nTX_DESC(txring, fill++) = 0;\r\ntxring->next_to_fill = fill;\r\nwrite_dma_reg(PAS_DMA_TXCHAN_INCR(txring->chan.chno), 2);\r\n}\r\nstatic int pasemi_mac_start_tx(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct pasemi_mac * const mac = netdev_priv(dev);\r\nstruct pasemi_mac_txring * const txring = tx_ring(mac);\r\nstruct pasemi_mac_csring *csring;\r\nu64 dflags = 0;\r\nu64 mactx;\r\ndma_addr_t map[MAX_SKB_FRAGS+1];\r\nunsigned int map_size[MAX_SKB_FRAGS+1];\r\nunsigned long flags;\r\nint i, nfrags;\r\nint fill;\r\nconst int nh_off = skb_network_offset(skb);\r\nconst int nh_len = skb_network_header_len(skb);\r\nprefetch(&txring->ring_info);\r\ndflags = XCT_MACTX_O | XCT_MACTX_ST | XCT_MACTX_CRC_PAD;\r\nnfrags = skb_shinfo(skb)->nr_frags;\r\nmap[0] = pci_map_single(mac->dma_pdev, skb->data, skb_headlen(skb),\r\nPCI_DMA_TODEVICE);\r\nmap_size[0] = skb_headlen(skb);\r\nif (pci_dma_mapping_error(mac->dma_pdev, map[0]))\r\ngoto out_err_nolock;\r\nfor (i = 0; i < nfrags; i++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nmap[i + 1] = skb_frag_dma_map(&mac->dma_pdev->dev, frag, 0,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\nmap_size[i+1] = skb_frag_size(frag);\r\nif (dma_mapping_error(&mac->dma_pdev->dev, map[i + 1])) {\r\nnfrags = i;\r\ngoto out_err_nolock;\r\n}\r\n}\r\nif (skb->ip_summed == CHECKSUM_PARTIAL && skb->len <= 1540) {\r\nswitch (ip_hdr(skb)->protocol) {\r\ncase IPPROTO_TCP:\r\ndflags |= XCT_MACTX_CSUM_TCP;\r\ndflags |= XCT_MACTX_IPH(nh_len >> 2);\r\ndflags |= XCT_MACTX_IPO(nh_off);\r\nbreak;\r\ncase IPPROTO_UDP:\r\ndflags |= XCT_MACTX_CSUM_UDP;\r\ndflags |= XCT_MACTX_IPH(nh_len >> 2);\r\ndflags |= XCT_MACTX_IPO(nh_off);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\n}\r\n}\r\nmactx = dflags | XCT_MACTX_LLEN(skb->len);\r\nspin_lock_irqsave(&txring->lock, flags);\r\nif (RING_AVAIL(txring) < nfrags + 14) {\r\nnetif_stop_queue(dev);\r\ngoto out_err;\r\n}\r\nif (mac->num_cs && skb->ip_summed == CHECKSUM_PARTIAL && skb->len > 1540) {\r\ncsring = mac->cs[mac->last_cs];\r\nmac->last_cs = (mac->last_cs + 1) % mac->num_cs;\r\npasemi_mac_queue_csdesc(skb, map, map_size, txring, csring);\r\n}\r\nfill = txring->next_to_fill;\r\nTX_DESC(txring, fill) = mactx;\r\nTX_DESC_INFO(txring, fill).dma = nfrags;\r\nfill++;\r\nTX_DESC_INFO(txring, fill).skb = skb;\r\nfor (i = 0; i <= nfrags; i++) {\r\nTX_DESC(txring, fill+i) =\r\nXCT_PTR_LEN(map_size[i]) | XCT_PTR_ADDR(map[i]);\r\nTX_DESC_INFO(txring, fill+i).dma = map[i];\r\n}\r\nif (nfrags & 1)\r\nnfrags++;\r\ntxring->next_to_fill = (fill + nfrags + 1) & (TX_RING_SIZE-1);\r\ndev->stats.tx_packets++;\r\ndev->stats.tx_bytes += skb->len;\r\nspin_unlock_irqrestore(&txring->lock, flags);\r\nwrite_dma_reg(PAS_DMA_TXCHAN_INCR(txring->chan.chno), (nfrags+2) >> 1);\r\nreturn NETDEV_TX_OK;\r\nout_err:\r\nspin_unlock_irqrestore(&txring->lock, flags);\r\nout_err_nolock:\r\nwhile (nfrags--)\r\npci_unmap_single(mac->dma_pdev, map[nfrags], map_size[nfrags],\r\nPCI_DMA_TODEVICE);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nstatic void pasemi_mac_set_rx_mode(struct net_device *dev)\r\n{\r\nconst struct pasemi_mac *mac = netdev_priv(dev);\r\nunsigned int flags;\r\nflags = read_mac_reg(mac, PAS_MAC_CFG_PCFG);\r\nif (dev->flags & IFF_PROMISC)\r\nflags |= PAS_MAC_CFG_PCFG_PR;\r\nelse\r\nflags &= ~PAS_MAC_CFG_PCFG_PR;\r\nwrite_mac_reg(mac, PAS_MAC_CFG_PCFG, flags);\r\n}\r\nstatic int pasemi_mac_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct pasemi_mac *mac = container_of(napi, struct pasemi_mac, napi);\r\nint pkts;\r\npasemi_mac_clean_tx(tx_ring(mac));\r\npkts = pasemi_mac_clean_rx(rx_ring(mac), budget);\r\nif (pkts < budget) {\r\nnapi_complete_done(napi, pkts);\r\npasemi_mac_restart_rx_intr(mac);\r\npasemi_mac_restart_tx_intr(mac);\r\n}\r\nreturn pkts;\r\n}\r\nstatic void pasemi_mac_netpoll(struct net_device *dev)\r\n{\r\nconst struct pasemi_mac *mac = netdev_priv(dev);\r\ndisable_irq(mac->tx->chan.irq);\r\npasemi_mac_tx_intr(mac->tx->chan.irq, mac->tx);\r\nenable_irq(mac->tx->chan.irq);\r\ndisable_irq(mac->rx->chan.irq);\r\npasemi_mac_rx_intr(mac->rx->chan.irq, mac->rx);\r\nenable_irq(mac->rx->chan.irq);\r\n}\r\nstatic int pasemi_mac_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct pasemi_mac *mac = netdev_priv(dev);\r\nunsigned int reg;\r\nunsigned int rcmdsta = 0;\r\nint running;\r\nint ret = 0;\r\nrunning = netif_running(dev);\r\nif (running) {\r\nnapi_disable(&mac->napi);\r\nnetif_tx_disable(dev);\r\npasemi_mac_intf_disable(mac);\r\nrcmdsta = read_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if));\r\npasemi_mac_pause_rxint(mac);\r\npasemi_mac_clean_rx(rx_ring(mac), RX_RING_SIZE);\r\npasemi_mac_free_rx_buffers(mac);\r\n}\r\nif (new_mtu > PE_DEF_MTU && !mac->num_cs) {\r\npasemi_mac_setup_csrings(mac);\r\nif (!mac->num_cs) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\n}\r\nreg = read_mac_reg(mac, PAS_MAC_CFG_MACCFG);\r\nreg &= ~PAS_MAC_CFG_MACCFG_MAXF_M;\r\nreg |= PAS_MAC_CFG_MACCFG_MAXF(new_mtu + ETH_HLEN + 4);\r\nwrite_mac_reg(mac, PAS_MAC_CFG_MACCFG, reg);\r\ndev->mtu = new_mtu;\r\nmac->bufsz = new_mtu + ETH_HLEN + ETH_FCS_LEN + LOCAL_SKB_ALIGN + 128;\r\nout:\r\nif (running) {\r\nwrite_dma_reg(PAS_DMA_RXINT_RCMDSTA(mac->dma_if),\r\nrcmdsta | PAS_DMA_RXINT_RCMDSTA_EN);\r\nrx_ring(mac)->next_to_fill = 0;\r\npasemi_mac_replenish_rx_ring(dev, RX_RING_SIZE-1);\r\nnapi_enable(&mac->napi);\r\nnetif_start_queue(dev);\r\npasemi_mac_intf_enable(mac);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\npasemi_mac_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct net_device *dev;\r\nstruct pasemi_mac *mac;\r\nint err, ret;\r\nerr = pci_enable_device(pdev);\r\nif (err)\r\nreturn err;\r\ndev = alloc_etherdev(sizeof(struct pasemi_mac));\r\nif (dev == NULL) {\r\nerr = -ENOMEM;\r\ngoto out_disable_device;\r\n}\r\npci_set_drvdata(pdev, dev);\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nmac = netdev_priv(dev);\r\nmac->pdev = pdev;\r\nmac->netdev = dev;\r\nnetif_napi_add(dev, &mac->napi, pasemi_mac_poll, 64);\r\ndev->features = NETIF_F_IP_CSUM | NETIF_F_LLTX | NETIF_F_SG |\r\nNETIF_F_HIGHDMA | NETIF_F_GSO;\r\nmac->dma_pdev = pci_get_device(PCI_VENDOR_ID_PASEMI, 0xa007, NULL);\r\nif (!mac->dma_pdev) {\r\ndev_err(&mac->pdev->dev, "Can't find DMA Controller\n");\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nmac->iob_pdev = pci_get_device(PCI_VENDOR_ID_PASEMI, 0xa001, NULL);\r\nif (!mac->iob_pdev) {\r\ndev_err(&mac->pdev->dev, "Can't find I/O Bridge\n");\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nif (pasemi_get_mac_addr(mac) || !is_valid_ether_addr(mac->mac_addr)) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nmemcpy(dev->dev_addr, mac->mac_addr, sizeof(mac->mac_addr));\r\nret = mac_to_intf(mac);\r\nif (ret < 0) {\r\ndev_err(&mac->pdev->dev, "Can't map DMA interface\n");\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nmac->dma_if = ret;\r\nswitch (pdev->device) {\r\ncase 0xa005:\r\nmac->type = MAC_TYPE_GMAC;\r\nbreak;\r\ncase 0xa006:\r\nmac->type = MAC_TYPE_XAUI;\r\nbreak;\r\ndefault:\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\ndev->netdev_ops = &pasemi_netdev_ops;\r\ndev->mtu = PE_DEF_MTU;\r\ndev->min_mtu = PE_MIN_MTU;\r\ndev->max_mtu = PE_MAX_MTU;\r\nmac->bufsz = dev->mtu + ETH_HLEN + ETH_FCS_LEN + LOCAL_SKB_ALIGN + 128;\r\ndev->ethtool_ops = &pasemi_mac_ethtool_ops;\r\nif (err)\r\ngoto out;\r\nmac->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\r\nmac->msg_enable = (NETIF_MSG_IFUP << 1 ) - 1;\r\nerr = register_netdev(dev);\r\nif (err) {\r\ndev_err(&mac->pdev->dev, "register_netdev failed with error %d\n",\r\nerr);\r\ngoto out;\r\n} else if (netif_msg_probe(mac)) {\r\nprintk(KERN_INFO "%s: PA Semi %s: intf %d, hw addr %pM\n",\r\ndev->name, mac->type == MAC_TYPE_GMAC ? "GMAC" : "XAUI",\r\nmac->dma_if, dev->dev_addr);\r\n}\r\nreturn err;\r\nout:\r\npci_dev_put(mac->iob_pdev);\r\npci_dev_put(mac->dma_pdev);\r\nfree_netdev(dev);\r\nout_disable_device:\r\npci_disable_device(pdev);\r\nreturn err;\r\n}\r\nstatic void pasemi_mac_remove(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct pasemi_mac *mac;\r\nif (!netdev)\r\nreturn;\r\nmac = netdev_priv(netdev);\r\nunregister_netdev(netdev);\r\npci_disable_device(pdev);\r\npci_dev_put(mac->dma_pdev);\r\npci_dev_put(mac->iob_pdev);\r\npasemi_dma_free_chan(&mac->tx->chan);\r\npasemi_dma_free_chan(&mac->rx->chan);\r\nfree_netdev(netdev);\r\n}\r\nstatic void __exit pasemi_mac_cleanup_module(void)\r\n{\r\npci_unregister_driver(&pasemi_mac_driver);\r\n}\r\nint pasemi_mac_init_module(void)\r\n{\r\nint err;\r\nerr = pasemi_dma_init();\r\nif (err)\r\nreturn err;\r\nreturn pci_register_driver(&pasemi_mac_driver);\r\n}
