static void\r\nnv10_bo_update_tile_region(struct drm_device *dev, struct nouveau_drm_tile *reg,\r\nu32 addr, u32 size, u32 pitch, u32 flags)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nint i = reg - drm->tile.reg;\r\nstruct nvkm_device *device = nvxx_device(&drm->client.device);\r\nstruct nvkm_fb *fb = device->fb;\r\nstruct nvkm_fb_tile *tile = &fb->tile.region[i];\r\nnouveau_fence_unref(&reg->fence);\r\nif (tile->pitch)\r\nnvkm_fb_tile_fini(fb, i, tile);\r\nif (pitch)\r\nnvkm_fb_tile_init(fb, i, addr, size, pitch, flags, tile);\r\nnvkm_fb_tile_prog(fb, i, tile);\r\n}\r\nstatic struct nouveau_drm_tile *\r\nnv10_bo_get_tile_region(struct drm_device *dev, int i)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct nouveau_drm_tile *tile = &drm->tile.reg[i];\r\nspin_lock(&drm->tile.lock);\r\nif (!tile->used &&\r\n(!tile->fence || nouveau_fence_done(tile->fence)))\r\ntile->used = true;\r\nelse\r\ntile = NULL;\r\nspin_unlock(&drm->tile.lock);\r\nreturn tile;\r\n}\r\nstatic void\r\nnv10_bo_put_tile_region(struct drm_device *dev, struct nouveau_drm_tile *tile,\r\nstruct dma_fence *fence)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nif (tile) {\r\nspin_lock(&drm->tile.lock);\r\ntile->fence = (struct nouveau_fence *)dma_fence_get(fence);\r\ntile->used = false;\r\nspin_unlock(&drm->tile.lock);\r\n}\r\n}\r\nstatic struct nouveau_drm_tile *\r\nnv10_bo_set_tiling(struct drm_device *dev, u32 addr,\r\nu32 size, u32 pitch, u32 flags)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct nvkm_fb *fb = nvxx_fb(&drm->client.device);\r\nstruct nouveau_drm_tile *tile, *found = NULL;\r\nint i;\r\nfor (i = 0; i < fb->tile.regions; i++) {\r\ntile = nv10_bo_get_tile_region(dev, i);\r\nif (pitch && !found) {\r\nfound = tile;\r\ncontinue;\r\n} else if (tile && fb->tile.region[i].pitch) {\r\nnv10_bo_update_tile_region(dev, tile, 0, 0, 0, 0);\r\n}\r\nnv10_bo_put_tile_region(dev, tile, NULL);\r\n}\r\nif (found)\r\nnv10_bo_update_tile_region(dev, found, addr, size,\r\npitch, flags);\r\nreturn found;\r\n}\r\nstatic void\r\nnouveau_bo_del_ttm(struct ttm_buffer_object *bo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = drm->dev;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nif (unlikely(nvbo->gem.filp))\r\nDRM_ERROR("bo %p still attached to GEM object\n", bo);\r\nWARN_ON(nvbo->pin_refcnt > 0);\r\nnv10_bo_put_tile_region(dev, nvbo->tile, NULL);\r\nkfree(nvbo);\r\n}\r\nstatic inline u64\r\nroundup_64(u64 x, u32 y)\r\n{\r\nx += y - 1;\r\ndo_div(x, y);\r\nreturn x * y;\r\n}\r\nstatic void\r\nnouveau_bo_fixup_align(struct nouveau_bo *nvbo, u32 flags,\r\nint *align, u64 *size)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct nvif_device *device = &drm->client.device;\r\nif (device->info.family < NV_DEVICE_INFO_V0_TESLA) {\r\nif (nvbo->tile_mode) {\r\nif (device->info.chipset >= 0x40) {\r\n*align = 65536;\r\n*size = roundup_64(*size, 64 * nvbo->tile_mode);\r\n} else if (device->info.chipset >= 0x30) {\r\n*align = 32768;\r\n*size = roundup_64(*size, 64 * nvbo->tile_mode);\r\n} else if (device->info.chipset >= 0x20) {\r\n*align = 16384;\r\n*size = roundup_64(*size, 64 * nvbo->tile_mode);\r\n} else if (device->info.chipset >= 0x10) {\r\n*align = 16384;\r\n*size = roundup_64(*size, 32 * nvbo->tile_mode);\r\n}\r\n}\r\n} else {\r\n*size = roundup_64(*size, (1 << nvbo->page_shift));\r\n*align = max((1 << nvbo->page_shift), *align);\r\n}\r\n*size = roundup_64(*size, PAGE_SIZE);\r\n}\r\nint\r\nnouveau_bo_new(struct nouveau_cli *cli, u64 size, int align,\r\nuint32_t flags, uint32_t tile_mode, uint32_t tile_flags,\r\nstruct sg_table *sg, struct reservation_object *robj,\r\nstruct nouveau_bo **pnvbo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(cli->dev);\r\nstruct nouveau_bo *nvbo;\r\nsize_t acc_size;\r\nint ret;\r\nint type = ttm_bo_type_device;\r\nif (!size) {\r\nNV_WARN(drm, "skipped size %016llx\n", size);\r\nreturn -EINVAL;\r\n}\r\nif (sg)\r\ntype = ttm_bo_type_sg;\r\nnvbo = kzalloc(sizeof(struct nouveau_bo), GFP_KERNEL);\r\nif (!nvbo)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&nvbo->head);\r\nINIT_LIST_HEAD(&nvbo->entry);\r\nINIT_LIST_HEAD(&nvbo->vma_list);\r\nnvbo->tile_mode = tile_mode;\r\nnvbo->tile_flags = tile_flags;\r\nnvbo->bo.bdev = &drm->ttm.bdev;\r\nnvbo->cli = cli;\r\nif (!nvxx_device(&drm->client.device)->func->cpu_coherent)\r\nnvbo->force_coherent = flags & TTM_PL_FLAG_UNCACHED;\r\nnvbo->page_shift = 12;\r\nif (drm->client.vm) {\r\nif (!(flags & TTM_PL_FLAG_TT) && size > 256 * 1024)\r\nnvbo->page_shift = drm->client.vm->mmu->lpg_shift;\r\n}\r\nnouveau_bo_fixup_align(nvbo, flags, &align, &size);\r\nnvbo->bo.mem.num_pages = size >> PAGE_SHIFT;\r\nnouveau_bo_placement_set(nvbo, flags, 0);\r\nacc_size = ttm_bo_dma_acc_size(&drm->ttm.bdev, size,\r\nsizeof(struct nouveau_bo));\r\nret = ttm_bo_init(&drm->ttm.bdev, &nvbo->bo, size,\r\ntype, &nvbo->placement,\r\nalign >> PAGE_SHIFT, false, NULL, acc_size, sg,\r\nrobj, nouveau_bo_del_ttm);\r\nif (ret) {\r\nreturn ret;\r\n}\r\n*pnvbo = nvbo;\r\nreturn 0;\r\n}\r\nstatic void\r\nset_placement_list(struct ttm_place *pl, unsigned *n, uint32_t type, uint32_t flags)\r\n{\r\n*n = 0;\r\nif (type & TTM_PL_FLAG_VRAM)\r\npl[(*n)++].flags = TTM_PL_FLAG_VRAM | flags;\r\nif (type & TTM_PL_FLAG_TT)\r\npl[(*n)++].flags = TTM_PL_FLAG_TT | flags;\r\nif (type & TTM_PL_FLAG_SYSTEM)\r\npl[(*n)++].flags = TTM_PL_FLAG_SYSTEM | flags;\r\n}\r\nstatic void\r\nset_placement_range(struct nouveau_bo *nvbo, uint32_t type)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nu32 vram_pages = drm->client.device.info.ram_size >> PAGE_SHIFT;\r\nunsigned i, fpfn, lpfn;\r\nif (drm->client.device.info.family == NV_DEVICE_INFO_V0_CELSIUS &&\r\nnvbo->tile_mode && (type & TTM_PL_FLAG_VRAM) &&\r\nnvbo->bo.mem.num_pages < vram_pages / 4) {\r\nif (nvbo->tile_flags & NOUVEAU_GEM_TILE_ZETA) {\r\nfpfn = vram_pages / 2;\r\nlpfn = ~0;\r\n} else {\r\nfpfn = 0;\r\nlpfn = vram_pages / 2;\r\n}\r\nfor (i = 0; i < nvbo->placement.num_placement; ++i) {\r\nnvbo->placements[i].fpfn = fpfn;\r\nnvbo->placements[i].lpfn = lpfn;\r\n}\r\nfor (i = 0; i < nvbo->placement.num_busy_placement; ++i) {\r\nnvbo->busy_placements[i].fpfn = fpfn;\r\nnvbo->busy_placements[i].lpfn = lpfn;\r\n}\r\n}\r\n}\r\nvoid\r\nnouveau_bo_placement_set(struct nouveau_bo *nvbo, uint32_t type, uint32_t busy)\r\n{\r\nstruct ttm_placement *pl = &nvbo->placement;\r\nuint32_t flags = (nvbo->force_coherent ? TTM_PL_FLAG_UNCACHED :\r\nTTM_PL_MASK_CACHING) |\r\n(nvbo->pin_refcnt ? TTM_PL_FLAG_NO_EVICT : 0);\r\npl->placement = nvbo->placements;\r\nset_placement_list(nvbo->placements, &pl->num_placement,\r\ntype, flags);\r\npl->busy_placement = nvbo->busy_placements;\r\nset_placement_list(nvbo->busy_placements, &pl->num_busy_placement,\r\ntype | busy, flags);\r\nset_placement_range(nvbo, type);\r\n}\r\nint\r\nnouveau_bo_pin(struct nouveau_bo *nvbo, uint32_t memtype, bool contig)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nbool force = false, evict = false;\r\nint ret;\r\nret = ttm_bo_reserve(bo, false, false, NULL);\r\nif (ret)\r\nreturn ret;\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA &&\r\nmemtype == TTM_PL_FLAG_VRAM && contig) {\r\nif (nvbo->tile_flags & NOUVEAU_GEM_TILE_NONCONTIG) {\r\nif (bo->mem.mem_type == TTM_PL_VRAM) {\r\nstruct nvkm_mem *mem = bo->mem.mm_node;\r\nif (!nvkm_mm_contiguous(mem->mem))\r\nevict = true;\r\n}\r\nnvbo->tile_flags &= ~NOUVEAU_GEM_TILE_NONCONTIG;\r\nforce = true;\r\n}\r\n}\r\nif (nvbo->pin_refcnt) {\r\nif (!(memtype & (1 << bo->mem.mem_type)) || evict) {\r\nNV_ERROR(drm, "bo %p pinned elsewhere: "\r\n"0x%08x vs 0x%08x\n", bo,\r\n1 << bo->mem.mem_type, memtype);\r\nret = -EBUSY;\r\n}\r\nnvbo->pin_refcnt++;\r\ngoto out;\r\n}\r\nif (evict) {\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_TT, 0);\r\nret = nouveau_bo_validate(nvbo, false, false);\r\nif (ret)\r\ngoto out;\r\n}\r\nnvbo->pin_refcnt++;\r\nnouveau_bo_placement_set(nvbo, memtype, 0);\r\nnvbo->pin_refcnt--;\r\nret = nouveau_bo_validate(nvbo, false, false);\r\nif (ret)\r\ngoto out;\r\nnvbo->pin_refcnt++;\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\ndrm->gem.vram_available -= bo->mem.size;\r\nbreak;\r\ncase TTM_PL_TT:\r\ndrm->gem.gart_available -= bo->mem.size;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nout:\r\nif (force && ret)\r\nnvbo->tile_flags |= NOUVEAU_GEM_TILE_NONCONTIG;\r\nttm_bo_unreserve(bo);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_bo_unpin(struct nouveau_bo *nvbo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nint ret, ref;\r\nret = ttm_bo_reserve(bo, false, false, NULL);\r\nif (ret)\r\nreturn ret;\r\nref = --nvbo->pin_refcnt;\r\nWARN_ON_ONCE(ref < 0);\r\nif (ref)\r\ngoto out;\r\nnouveau_bo_placement_set(nvbo, bo->mem.placement, 0);\r\nret = nouveau_bo_validate(nvbo, false, false);\r\nif (ret == 0) {\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\ndrm->gem.vram_available += bo->mem.size;\r\nbreak;\r\ncase TTM_PL_TT:\r\ndrm->gem.gart_available += bo->mem.size;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nout:\r\nttm_bo_unreserve(bo);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_bo_map(struct nouveau_bo *nvbo)\r\n{\r\nint ret;\r\nret = ttm_bo_reserve(&nvbo->bo, false, false, NULL);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_kmap(&nvbo->bo, 0, nvbo->bo.mem.num_pages, &nvbo->kmap);\r\nttm_bo_unreserve(&nvbo->bo);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_bo_unmap(struct nouveau_bo *nvbo)\r\n{\r\nif (!nvbo)\r\nreturn;\r\nttm_bo_kunmap(&nvbo->kmap);\r\n}\r\nvoid\r\nnouveau_bo_sync_for_device(struct nouveau_bo *nvbo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct nvkm_device *device = nvxx_device(&drm->client.device);\r\nstruct ttm_dma_tt *ttm_dma = (struct ttm_dma_tt *)nvbo->bo.ttm;\r\nint i;\r\nif (!ttm_dma)\r\nreturn;\r\nif (nvbo->force_coherent)\r\nreturn;\r\nfor (i = 0; i < ttm_dma->ttm.num_pages; i++)\r\ndma_sync_single_for_device(device->dev, ttm_dma->dma_address[i],\r\nPAGE_SIZE, DMA_TO_DEVICE);\r\n}\r\nvoid\r\nnouveau_bo_sync_for_cpu(struct nouveau_bo *nvbo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct nvkm_device *device = nvxx_device(&drm->client.device);\r\nstruct ttm_dma_tt *ttm_dma = (struct ttm_dma_tt *)nvbo->bo.ttm;\r\nint i;\r\nif (!ttm_dma)\r\nreturn;\r\nif (nvbo->force_coherent)\r\nreturn;\r\nfor (i = 0; i < ttm_dma->ttm.num_pages; i++)\r\ndma_sync_single_for_cpu(device->dev, ttm_dma->dma_address[i],\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\n}\r\nint\r\nnouveau_bo_validate(struct nouveau_bo *nvbo, bool interruptible,\r\nbool no_wait_gpu)\r\n{\r\nint ret;\r\nret = ttm_bo_validate(&nvbo->bo, &nvbo->placement,\r\ninterruptible, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nnouveau_bo_sync_for_device(nvbo);\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_bo_wr16(struct nouveau_bo *nvbo, unsigned index, u16 val)\r\n{\r\nbool is_iomem;\r\nu16 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem += index;\r\nif (is_iomem)\r\niowrite16_native(val, (void __force __iomem *)mem);\r\nelse\r\n*mem = val;\r\n}\r\nu32\r\nnouveau_bo_rd32(struct nouveau_bo *nvbo, unsigned index)\r\n{\r\nbool is_iomem;\r\nu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem += index;\r\nif (is_iomem)\r\nreturn ioread32_native((void __force __iomem *)mem);\r\nelse\r\nreturn *mem;\r\n}\r\nvoid\r\nnouveau_bo_wr32(struct nouveau_bo *nvbo, unsigned index, u32 val)\r\n{\r\nbool is_iomem;\r\nu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem += index;\r\nif (is_iomem)\r\niowrite32_native(val, (void __force __iomem *)mem);\r\nelse\r\n*mem = val;\r\n}\r\nstatic struct ttm_tt *\r\nnouveau_ttm_tt_create(struct ttm_bo_device *bdev, unsigned long size,\r\nuint32_t page_flags, struct page *dummy_read)\r\n{\r\n#if IS_ENABLED(CONFIG_AGP)\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nif (drm->agp.bridge) {\r\nreturn ttm_agp_tt_create(bdev, drm->agp.bridge, size,\r\npage_flags, dummy_read);\r\n}\r\n#endif\r\nreturn nouveau_sgdma_create_ttm(bdev, size, page_flags, dummy_read);\r\n}\r\nstatic int\r\nnouveau_bo_invalidate_caches(struct ttm_bo_device *bdev, uint32_t flags)\r\n{\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,\r\nstruct ttm_mem_type_manager *man)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nswitch (type) {\r\ncase TTM_PL_SYSTEM:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_MASK_CACHING;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nman->flags = TTM_MEMTYPE_FLAG_FIXED |\r\nTTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_WC;\r\nman->default_caching = TTM_PL_FLAG_WC;\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA) {\r\nif (nvxx_bar(&drm->client.device)->iomap_uncached) {\r\nman->available_caching = TTM_PL_FLAG_UNCACHED;\r\nman->default_caching = TTM_PL_FLAG_UNCACHED;\r\n}\r\nman->func = &nouveau_vram_manager;\r\nman->io_reserve_fastpath = false;\r\nman->use_io_reserve_lru = true;\r\n} else {\r\nman->func = &ttm_bo_manager_func;\r\n}\r\nbreak;\r\ncase TTM_PL_TT:\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA)\r\nman->func = &nouveau_gart_manager;\r\nelse\r\nif (!drm->agp.bridge)\r\nman->func = &nv04_gart_manager;\r\nelse\r\nman->func = &ttm_bo_manager_func;\r\nif (drm->agp.bridge) {\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_WC;\r\nman->default_caching = TTM_PL_FLAG_WC;\r\n} else {\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE |\r\nTTM_MEMTYPE_FLAG_CMA;\r\nman->available_caching = TTM_PL_MASK_CACHING;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\n}\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_bo_evict_flags(struct ttm_buffer_object *bo, struct ttm_placement *pl)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_TT,\r\nTTM_PL_FLAG_SYSTEM);\r\nbreak;\r\ndefault:\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_SYSTEM, 0);\r\nbreak;\r\n}\r\n*pl = nvbo->placement;\r\n}\r\nstatic int\r\nnve0_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 2);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle & 0x0000ffff);\r\nFIRE_RING (chan);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnve0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nvkm_mem *mem = old_reg->mm_node;\r\nint ret = RING_SPACE(chan, 10);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0400, 8);\r\nOUT_RING (chan, upper_32_bits(mem->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(mem->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(mem->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(mem->vma[1].offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, new_reg->num_pages);\r\nBEGIN_IMC0(chan, NvSubCopy, 0x0300, 0x0386);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnvc0_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 2);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnvc0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nvkm_mem *mem = old_reg->mm_node;\r\nu64 src_offset = mem->vma[0].offset;\r\nu64 dst_offset = mem->vma[1].offset;\r\nu32 page_count = new_reg->num_pages;\r\nint ret;\r\npage_count = new_reg->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 8191) ? 8191 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00000110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnvc0_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nvkm_mem *mem = old_reg->mm_node;\r\nu64 src_offset = mem->vma[0].offset;\r\nu64 dst_offset = mem->vma[1].offset;\r\nu32 page_count = new_reg->num_pages;\r\nint ret;\r\npage_count = new_reg->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 2047) ? 2047 : page_count;\r\nret = RING_SPACE(chan, 12);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0238, 2);\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nBEGIN_NVC0(chan, NvSubCopy, 0x030c, 6);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00100110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnva3_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nvkm_mem *mem = old_reg->mm_node;\r\nu64 src_offset = mem->vma[0].offset;\r\nu64 dst_offset = mem->vma[1].offset;\r\nu32 page_count = new_reg->num_pages;\r\nint ret;\r\npage_count = new_reg->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 8191) ? 8191 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00000110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv98_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nvkm_mem *mem = old_reg->mm_node;\r\nint ret = RING_SPACE(chan, 7);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0320, 6);\r\nOUT_RING (chan, upper_32_bits(mem->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(mem->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(mem->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(mem->vma[1].offset));\r\nOUT_RING (chan, 0x00000000 );\r\nOUT_RING (chan, new_reg->num_pages << PAGE_SHIFT);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv84_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nvkm_mem *mem = old_reg->mm_node;\r\nint ret = RING_SPACE(chan, 7);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0304, 6);\r\nOUT_RING (chan, new_reg->num_pages << PAGE_SHIFT);\r\nOUT_RING (chan, upper_32_bits(mem->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(mem->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(mem->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(mem->vma[1].offset));\r\nOUT_RING (chan, 0x00000000 );\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv50_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 6);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0180, 3);\r\nOUT_RING (chan, chan->drm->ntfy.handle);\r\nOUT_RING (chan, chan->vram.handle);\r\nOUT_RING (chan, chan->vram.handle);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv50_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nvkm_mem *mem = old_reg->mm_node;\r\nu64 length = (new_reg->num_pages << PAGE_SHIFT);\r\nu64 src_offset = mem->vma[0].offset;\r\nu64 dst_offset = mem->vma[1].offset;\r\nint src_tiled = !!mem->memtype;\r\nint dst_tiled = !!((struct nvkm_mem *)new_reg->mm_node)->memtype;\r\nint ret;\r\nwhile (length) {\r\nu32 amount, stride, height;\r\nret = RING_SPACE(chan, 18 + 6 * (src_tiled + dst_tiled));\r\nif (ret)\r\nreturn ret;\r\namount = min(length, (u64)(4 * 1024 * 1024));\r\nstride = 16 * 4;\r\nheight = amount / stride;\r\nif (src_tiled) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0200, 7);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\n} else {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0200, 1);\r\nOUT_RING (chan, 1);\r\n}\r\nif (dst_tiled) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x021c, 7);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\n} else {\r\nBEGIN_NV04(chan, NvSubCopy, 0x021c, 1);\r\nOUT_RING (chan, 1);\r\n}\r\nBEGIN_NV04(chan, NvSubCopy, 0x0238, 2);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nBEGIN_NV04(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 0x00000101);\r\nOUT_RING (chan, 0x00000000);\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);\r\nOUT_RING (chan, 0);\r\nlength -= amount;\r\nsrc_offset += amount;\r\ndst_offset += amount;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv04_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 4);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0180, 1);\r\nOUT_RING (chan, chan->drm->ntfy.handle);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline uint32_t\r\nnouveau_bo_mem_ctxdma(struct ttm_buffer_object *bo,\r\nstruct nouveau_channel *chan, struct ttm_mem_reg *reg)\r\n{\r\nif (reg->mem_type == TTM_PL_TT)\r\nreturn NvDmaTT;\r\nreturn chan->vram.handle;\r\n}\r\nstatic int\r\nnv04_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_reg, struct ttm_mem_reg *new_reg)\r\n{\r\nu32 src_offset = old_reg->start << PAGE_SHIFT;\r\nu32 dst_offset = new_reg->start << PAGE_SHIFT;\r\nu32 page_count = new_reg->num_pages;\r\nint ret;\r\nret = RING_SPACE(chan, 3);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_DMA_SOURCE, 2);\r\nOUT_RING (chan, nouveau_bo_mem_ctxdma(bo, chan, old_reg));\r\nOUT_RING (chan, nouveau_bo_mem_ctxdma(bo, chan, new_reg));\r\npage_count = new_reg->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 2047) ? 2047 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy,\r\nNV_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN, 8);\r\nOUT_RING (chan, src_offset);\r\nOUT_RING (chan, dst_offset);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nOUT_RING (chan, 0x00000101);\r\nOUT_RING (chan, 0x00000000);\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);\r\nOUT_RING (chan, 0);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_move_prep(struct nouveau_drm *drm, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *reg)\r\n{\r\nstruct nvkm_mem *old_mem = bo->mem.mm_node;\r\nstruct nvkm_mem *new_mem = reg->mm_node;\r\nu64 size = (u64)reg->num_pages << PAGE_SHIFT;\r\nint ret;\r\nret = nvkm_vm_get(drm->client.vm, size, old_mem->page_shift,\r\nNV_MEM_ACCESS_RW, &old_mem->vma[0]);\r\nif (ret)\r\nreturn ret;\r\nret = nvkm_vm_get(drm->client.vm, size, new_mem->page_shift,\r\nNV_MEM_ACCESS_RW, &old_mem->vma[1]);\r\nif (ret) {\r\nnvkm_vm_put(&old_mem->vma[0]);\r\nreturn ret;\r\n}\r\nnvkm_vm_map(&old_mem->vma[0], old_mem);\r\nnvkm_vm_map(&old_mem->vma[1], new_mem);\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_move_m2mf(struct ttm_buffer_object *bo, int evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct nouveau_channel *chan = drm->ttm.chan;\r\nstruct nouveau_cli *cli = (void *)chan->user.client;\r\nstruct nouveau_fence *fence;\r\nint ret;\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA) {\r\nret = nouveau_bo_move_prep(drm, bo, new_reg);\r\nif (ret)\r\nreturn ret;\r\n}\r\nmutex_lock_nested(&cli->mutex, SINGLE_DEPTH_NESTING);\r\nret = nouveau_fence_sync(nouveau_bo(bo), chan, true, intr);\r\nif (ret == 0) {\r\nret = drm->ttm.move(chan, bo, &bo->mem, new_reg);\r\nif (ret == 0) {\r\nret = nouveau_fence_new(chan, false, &fence);\r\nif (ret == 0) {\r\nret = ttm_bo_move_accel_cleanup(bo,\r\n&fence->base,\r\nevict,\r\nnew_reg);\r\nnouveau_fence_unref(&fence);\r\n}\r\n}\r\n}\r\nmutex_unlock(&cli->mutex);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_bo_move_init(struct nouveau_drm *drm)\r\n{\r\nstatic const struct {\r\nconst char *name;\r\nint engine;\r\ns32 oclass;\r\nint (*exec)(struct nouveau_channel *,\r\nstruct ttm_buffer_object *,\r\nstruct ttm_mem_reg *, struct ttm_mem_reg *);\r\nint (*init)(struct nouveau_channel *, u32 handle);\r\n} _methods[] = {\r\n{ "COPY", 4, 0xc1b5, nve0_bo_move_copy, nve0_bo_move_init },\r\n{ "GRCE", 0, 0xc1b5, nve0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY", 4, 0xc0b5, nve0_bo_move_copy, nve0_bo_move_init },\r\n{ "GRCE", 0, 0xc0b5, nve0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY", 4, 0xb0b5, nve0_bo_move_copy, nve0_bo_move_init },\r\n{ "GRCE", 0, 0xb0b5, nve0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY", 4, 0xa0b5, nve0_bo_move_copy, nve0_bo_move_init },\r\n{ "GRCE", 0, 0xa0b5, nve0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY1", 5, 0x90b8, nvc0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY0", 4, 0x90b5, nvc0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY", 0, 0x85b5, nva3_bo_move_copy, nv50_bo_move_init },\r\n{ "CRYPT", 0, 0x74c1, nv84_bo_move_exec, nv50_bo_move_init },\r\n{ "M2MF", 0, 0x9039, nvc0_bo_move_m2mf, nvc0_bo_move_init },\r\n{ "M2MF", 0, 0x5039, nv50_bo_move_m2mf, nv50_bo_move_init },\r\n{ "M2MF", 0, 0x0039, nv04_bo_move_m2mf, nv04_bo_move_init },\r\n{},\r\n{ "CRYPT", 0, 0x88b4, nv98_bo_move_exec, nv50_bo_move_init },\r\n}, *mthd = _methods;\r\nconst char *name = "CPU";\r\nint ret;\r\ndo {\r\nstruct nouveau_channel *chan;\r\nif (mthd->engine)\r\nchan = drm->cechan;\r\nelse\r\nchan = drm->channel;\r\nif (chan == NULL)\r\ncontinue;\r\nret = nvif_object_init(&chan->user,\r\nmthd->oclass | (mthd->engine << 16),\r\nmthd->oclass, NULL, 0,\r\n&drm->ttm.copy);\r\nif (ret == 0) {\r\nret = mthd->init(chan, drm->ttm.copy.handle);\r\nif (ret) {\r\nnvif_object_fini(&drm->ttm.copy);\r\ncontinue;\r\n}\r\ndrm->ttm.move = mthd->exec;\r\ndrm->ttm.chan = chan;\r\nname = mthd->name;\r\nbreak;\r\n}\r\n} while ((++mthd)->exec);\r\nNV_INFO(drm, "MM: using %s for buffer copies\n", name);\r\n}\r\nstatic int\r\nnouveau_bo_move_flipd(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct ttm_place placement_memtype = {\r\n.fpfn = 0,\r\n.lpfn = 0,\r\n.flags = TTM_PL_FLAG_TT | TTM_PL_MASK_CACHING\r\n};\r\nstruct ttm_placement placement;\r\nstruct ttm_mem_reg tmp_reg;\r\nint ret;\r\nplacement.num_placement = placement.num_busy_placement = 1;\r\nplacement.placement = placement.busy_placement = &placement_memtype;\r\ntmp_reg = *new_reg;\r\ntmp_reg.mm_node = NULL;\r\nret = ttm_bo_mem_space(bo, &placement, &tmp_reg, intr, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_tt_bind(bo->ttm, &tmp_reg);\r\nif (ret)\r\ngoto out;\r\nret = nouveau_bo_move_m2mf(bo, true, intr, no_wait_gpu, &tmp_reg);\r\nif (ret)\r\ngoto out;\r\nret = ttm_bo_move_ttm(bo, intr, no_wait_gpu, new_reg);\r\nout:\r\nttm_bo_mem_put(bo, &tmp_reg);\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_bo_move_flips(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct ttm_place placement_memtype = {\r\n.fpfn = 0,\r\n.lpfn = 0,\r\n.flags = TTM_PL_FLAG_TT | TTM_PL_MASK_CACHING\r\n};\r\nstruct ttm_placement placement;\r\nstruct ttm_mem_reg tmp_reg;\r\nint ret;\r\nplacement.num_placement = placement.num_busy_placement = 1;\r\nplacement.placement = placement.busy_placement = &placement_memtype;\r\ntmp_reg = *new_reg;\r\ntmp_reg.mm_node = NULL;\r\nret = ttm_bo_mem_space(bo, &placement, &tmp_reg, intr, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_move_ttm(bo, intr, no_wait_gpu, &tmp_reg);\r\nif (ret)\r\ngoto out;\r\nret = nouveau_bo_move_m2mf(bo, true, intr, no_wait_gpu, new_reg);\r\nif (ret)\r\ngoto out;\r\nout:\r\nttm_bo_mem_put(bo, &tmp_reg);\r\nreturn ret;\r\n}\r\nstatic void\r\nnouveau_bo_move_ntfy(struct ttm_buffer_object *bo, bool evict,\r\nstruct ttm_mem_reg *new_reg)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct nvkm_vma *vma;\r\nif (bo->destroy != nouveau_bo_del_ttm)\r\nreturn;\r\nlist_for_each_entry(vma, &nvbo->vma_list, head) {\r\nif (new_reg && new_reg->mem_type != TTM_PL_SYSTEM &&\r\n(new_reg->mem_type == TTM_PL_VRAM ||\r\nnvbo->page_shift != vma->vm->mmu->lpg_shift)) {\r\nnvkm_vm_map(vma, new_reg->mm_node);\r\n} else {\r\nWARN_ON(ttm_bo_wait(bo, false, false));\r\nnvkm_vm_unmap(vma);\r\n}\r\n}\r\n}\r\nstatic int\r\nnouveau_bo_vm_bind(struct ttm_buffer_object *bo, struct ttm_mem_reg *new_reg,\r\nstruct nouveau_drm_tile **new_tile)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = drm->dev;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nu64 offset = new_reg->start << PAGE_SHIFT;\r\n*new_tile = NULL;\r\nif (new_reg->mem_type != TTM_PL_VRAM)\r\nreturn 0;\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_CELSIUS) {\r\n*new_tile = nv10_bo_set_tiling(dev, offset, new_reg->size,\r\nnvbo->tile_mode,\r\nnvbo->tile_flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_bo_vm_cleanup(struct ttm_buffer_object *bo,\r\nstruct nouveau_drm_tile *new_tile,\r\nstruct nouveau_drm_tile **old_tile)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = drm->dev;\r\nstruct dma_fence *fence = reservation_object_get_excl(bo->resv);\r\nnv10_bo_put_tile_region(dev, *old_tile, fence);\r\n*old_tile = new_tile;\r\n}\r\nstatic int\r\nnouveau_bo_move(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_reg)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct ttm_mem_reg *old_reg = &bo->mem;\r\nstruct nouveau_drm_tile *new_tile = NULL;\r\nint ret = 0;\r\nret = ttm_bo_wait(bo, intr, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nif (nvbo->pin_refcnt)\r\nNV_WARN(drm, "Moving pinned object %p!\n", nvbo);\r\nif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA) {\r\nret = nouveau_bo_vm_bind(bo, new_reg, &new_tile);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (old_reg->mem_type == TTM_PL_SYSTEM && !bo->ttm) {\r\nBUG_ON(bo->mem.mm_node != NULL);\r\nbo->mem = *new_reg;\r\nnew_reg->mm_node = NULL;\r\ngoto out;\r\n}\r\nif (drm->ttm.move) {\r\nif (new_reg->mem_type == TTM_PL_SYSTEM)\r\nret = nouveau_bo_move_flipd(bo, evict, intr,\r\nno_wait_gpu, new_reg);\r\nelse if (old_reg->mem_type == TTM_PL_SYSTEM)\r\nret = nouveau_bo_move_flips(bo, evict, intr,\r\nno_wait_gpu, new_reg);\r\nelse\r\nret = nouveau_bo_move_m2mf(bo, evict, intr,\r\nno_wait_gpu, new_reg);\r\nif (!ret)\r\ngoto out;\r\n}\r\nret = ttm_bo_wait(bo, intr, no_wait_gpu);\r\nif (ret == 0)\r\nret = ttm_bo_move_memcpy(bo, intr, no_wait_gpu, new_reg);\r\nout:\r\nif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA) {\r\nif (ret)\r\nnouveau_bo_vm_cleanup(bo, NULL, &new_tile);\r\nelse\r\nnouveau_bo_vm_cleanup(bo, new_tile, &nvbo->tile);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_bo_verify_access(struct ttm_buffer_object *bo, struct file *filp)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nreturn drm_vma_node_verify_access(&nvbo->gem.vma_node,\r\nfilp->private_data);\r\n}\r\nstatic int\r\nnouveau_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *reg)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[reg->mem_type];\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nstruct nvkm_device *device = nvxx_device(&drm->client.device);\r\nstruct nvkm_mem *mem = reg->mm_node;\r\nint ret;\r\nreg->bus.addr = NULL;\r\nreg->bus.offset = 0;\r\nreg->bus.size = reg->num_pages << PAGE_SHIFT;\r\nreg->bus.base = 0;\r\nreg->bus.is_iomem = false;\r\nif (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))\r\nreturn -EINVAL;\r\nswitch (reg->mem_type) {\r\ncase TTM_PL_SYSTEM:\r\nreturn 0;\r\ncase TTM_PL_TT:\r\n#if IS_ENABLED(CONFIG_AGP)\r\nif (drm->agp.bridge) {\r\nreg->bus.offset = reg->start << PAGE_SHIFT;\r\nreg->bus.base = drm->agp.base;\r\nreg->bus.is_iomem = !drm->agp.cma;\r\n}\r\n#endif\r\nif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA || !mem->memtype)\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nreg->bus.offset = reg->start << PAGE_SHIFT;\r\nreg->bus.base = device->func->resource_addr(device, 1);\r\nreg->bus.is_iomem = true;\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA) {\r\nstruct nvkm_bar *bar = nvxx_bar(&drm->client.device);\r\nint page_shift = 12;\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_FERMI)\r\npage_shift = mem->page_shift;\r\nret = nvkm_bar_umap(bar, mem->size << 12, page_shift,\r\n&mem->bar_vma);\r\nif (ret)\r\nreturn ret;\r\nnvkm_vm_map(&mem->bar_vma, mem);\r\nreg->bus.offset = mem->bar_vma.offset;\r\n}\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_ttm_io_mem_free(struct ttm_bo_device *bdev, struct ttm_mem_reg *reg)\r\n{\r\nstruct nvkm_mem *mem = reg->mm_node;\r\nif (!mem->bar_vma.node)\r\nreturn;\r\nnvkm_vm_unmap(&mem->bar_vma);\r\nnvkm_vm_put(&mem->bar_vma);\r\n}\r\nstatic int\r\nnouveau_ttm_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct nvkm_device *device = nvxx_device(&drm->client.device);\r\nu32 mappable = device->func->resource_size(device, 1) >> PAGE_SHIFT;\r\nint i, ret;\r\nif (bo->mem.mem_type != TTM_PL_VRAM) {\r\nif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA ||\r\n!nouveau_bo_tile_layout(nvbo))\r\nreturn 0;\r\nif (bo->mem.mem_type == TTM_PL_SYSTEM) {\r\nnouveau_bo_placement_set(nvbo, TTM_PL_TT, 0);\r\nret = nouveau_bo_validate(nvbo, false, false);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA ||\r\nbo->mem.start + bo->mem.num_pages < mappable)\r\nreturn 0;\r\nfor (i = 0; i < nvbo->placement.num_placement; ++i) {\r\nnvbo->placements[i].fpfn = 0;\r\nnvbo->placements[i].lpfn = mappable;\r\n}\r\nfor (i = 0; i < nvbo->placement.num_busy_placement; ++i) {\r\nnvbo->busy_placements[i].fpfn = 0;\r\nnvbo->busy_placements[i].lpfn = mappable;\r\n}\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_VRAM, 0);\r\nreturn nouveau_bo_validate(nvbo, false, false);\r\n}\r\nstatic int\r\nnouveau_ttm_tt_populate(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_dma_tt *ttm_dma = (void *)ttm;\r\nstruct nouveau_drm *drm;\r\nstruct nvkm_device *device;\r\nstruct drm_device *dev;\r\nstruct device *pdev;\r\nunsigned i;\r\nint r;\r\nbool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);\r\nif (ttm->state != tt_unpopulated)\r\nreturn 0;\r\nif (slave && ttm->sg) {\r\ndrm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages,\r\nttm_dma->dma_address, ttm->num_pages);\r\nttm->state = tt_unbound;\r\nreturn 0;\r\n}\r\ndrm = nouveau_bdev(ttm->bdev);\r\ndevice = nvxx_device(&drm->client.device);\r\ndev = drm->dev;\r\npdev = device->dev;\r\n#if IS_ENABLED(CONFIG_AGP)\r\nif (drm->agp.bridge) {\r\nreturn ttm_agp_tt_populate(ttm);\r\n}\r\n#endif\r\n#if IS_ENABLED(CONFIG_SWIOTLB) && IS_ENABLED(CONFIG_X86)\r\nif (swiotlb_nr_tbl()) {\r\nreturn ttm_dma_populate((void *)ttm, dev->dev);\r\n}\r\n#endif\r\nr = ttm_pool_populate(ttm);\r\nif (r) {\r\nreturn r;\r\n}\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\ndma_addr_t addr;\r\naddr = dma_map_page(pdev, ttm->pages[i], 0, PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(pdev, addr)) {\r\nwhile (i--) {\r\ndma_unmap_page(pdev, ttm_dma->dma_address[i],\r\nPAGE_SIZE, DMA_BIDIRECTIONAL);\r\nttm_dma->dma_address[i] = 0;\r\n}\r\nttm_pool_unpopulate(ttm);\r\nreturn -EFAULT;\r\n}\r\nttm_dma->dma_address[i] = addr;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_ttm_tt_unpopulate(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_dma_tt *ttm_dma = (void *)ttm;\r\nstruct nouveau_drm *drm;\r\nstruct nvkm_device *device;\r\nstruct drm_device *dev;\r\nstruct device *pdev;\r\nunsigned i;\r\nbool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);\r\nif (slave)\r\nreturn;\r\ndrm = nouveau_bdev(ttm->bdev);\r\ndevice = nvxx_device(&drm->client.device);\r\ndev = drm->dev;\r\npdev = device->dev;\r\n#if IS_ENABLED(CONFIG_AGP)\r\nif (drm->agp.bridge) {\r\nttm_agp_tt_unpopulate(ttm);\r\nreturn;\r\n}\r\n#endif\r\n#if IS_ENABLED(CONFIG_SWIOTLB) && IS_ENABLED(CONFIG_X86)\r\nif (swiotlb_nr_tbl()) {\r\nttm_dma_unpopulate((void *)ttm, dev->dev);\r\nreturn;\r\n}\r\n#endif\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\nif (ttm_dma->dma_address[i]) {\r\ndma_unmap_page(pdev, ttm_dma->dma_address[i], PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\n}\r\n}\r\nttm_pool_unpopulate(ttm);\r\n}\r\nvoid\r\nnouveau_bo_fence(struct nouveau_bo *nvbo, struct nouveau_fence *fence, bool exclusive)\r\n{\r\nstruct reservation_object *resv = nvbo->bo.resv;\r\nif (exclusive)\r\nreservation_object_add_excl_fence(resv, &fence->base);\r\nelse if (fence)\r\nreservation_object_add_shared_fence(resv, &fence->base);\r\n}\r\nstruct nvkm_vma *\r\nnouveau_bo_vma_find(struct nouveau_bo *nvbo, struct nvkm_vm *vm)\r\n{\r\nstruct nvkm_vma *vma;\r\nlist_for_each_entry(vma, &nvbo->vma_list, head) {\r\nif (vma->vm == vm)\r\nreturn vma;\r\n}\r\nreturn NULL;\r\n}\r\nint\r\nnouveau_bo_vma_add(struct nouveau_bo *nvbo, struct nvkm_vm *vm,\r\nstruct nvkm_vma *vma)\r\n{\r\nconst u32 size = nvbo->bo.mem.num_pages << PAGE_SHIFT;\r\nint ret;\r\nret = nvkm_vm_get(vm, size, nvbo->page_shift,\r\nNV_MEM_ACCESS_RW, vma);\r\nif (ret)\r\nreturn ret;\r\nif ( nvbo->bo.mem.mem_type != TTM_PL_SYSTEM &&\r\n(nvbo->bo.mem.mem_type == TTM_PL_VRAM ||\r\nnvbo->page_shift != vma->vm->mmu->lpg_shift))\r\nnvkm_vm_map(vma, nvbo->bo.mem.mm_node);\r\nlist_add_tail(&vma->head, &nvbo->vma_list);\r\nvma->refcount = 1;\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_bo_vma_del(struct nouveau_bo *nvbo, struct nvkm_vma *vma)\r\n{\r\nif (vma->node) {\r\nif (nvbo->bo.mem.mem_type != TTM_PL_SYSTEM)\r\nnvkm_vm_unmap(vma);\r\nnvkm_vm_put(vma);\r\nlist_del(&vma->head);\r\n}\r\n}
