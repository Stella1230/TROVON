static void\r\nirqfd_inject(struct work_struct *work)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd =\r\ncontainer_of(work, struct kvm_kernel_irqfd, inject);\r\nstruct kvm *kvm = irqfd->kvm;\r\nif (!irqfd->resampler) {\r\nkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,\r\nfalse);\r\nkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,\r\nfalse);\r\n} else\r\nkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\r\nirqfd->gsi, 1, false);\r\n}\r\nstatic void\r\nirqfd_resampler_ack(struct kvm_irq_ack_notifier *kian)\r\n{\r\nstruct kvm_kernel_irqfd_resampler *resampler;\r\nstruct kvm *kvm;\r\nstruct kvm_kernel_irqfd *irqfd;\r\nint idx;\r\nresampler = container_of(kian,\r\nstruct kvm_kernel_irqfd_resampler, notifier);\r\nkvm = resampler->kvm;\r\nkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\r\nresampler->notifier.gsi, 0, false);\r\nidx = srcu_read_lock(&kvm->irq_srcu);\r\nlist_for_each_entry_rcu(irqfd, &resampler->list, resampler_link)\r\neventfd_signal(irqfd->resamplefd, 1);\r\nsrcu_read_unlock(&kvm->irq_srcu, idx);\r\n}\r\nstatic void\r\nirqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)\r\n{\r\nstruct kvm_kernel_irqfd_resampler *resampler = irqfd->resampler;\r\nstruct kvm *kvm = resampler->kvm;\r\nmutex_lock(&kvm->irqfds.resampler_lock);\r\nlist_del_rcu(&irqfd->resampler_link);\r\nsynchronize_srcu(&kvm->irq_srcu);\r\nif (list_empty(&resampler->list)) {\r\nlist_del(&resampler->link);\r\nkvm_unregister_irq_ack_notifier(kvm, &resampler->notifier);\r\nkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\r\nresampler->notifier.gsi, 0, false);\r\nkfree(resampler);\r\n}\r\nmutex_unlock(&kvm->irqfds.resampler_lock);\r\n}\r\nstatic void\r\nirqfd_shutdown(struct work_struct *work)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd =\r\ncontainer_of(work, struct kvm_kernel_irqfd, shutdown);\r\nu64 cnt;\r\neventfd_ctx_remove_wait_queue(irqfd->eventfd, &irqfd->wait, &cnt);\r\nflush_work(&irqfd->inject);\r\nif (irqfd->resampler) {\r\nirqfd_resampler_shutdown(irqfd);\r\neventfd_ctx_put(irqfd->resamplefd);\r\n}\r\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\r\nirq_bypass_unregister_consumer(&irqfd->consumer);\r\n#endif\r\neventfd_ctx_put(irqfd->eventfd);\r\nkfree(irqfd);\r\n}\r\nstatic bool\r\nirqfd_is_active(struct kvm_kernel_irqfd *irqfd)\r\n{\r\nreturn list_empty(&irqfd->list) ? false : true;\r\n}\r\nstatic void\r\nirqfd_deactivate(struct kvm_kernel_irqfd *irqfd)\r\n{\r\nBUG_ON(!irqfd_is_active(irqfd));\r\nlist_del_init(&irqfd->list);\r\nqueue_work(irqfd_cleanup_wq, &irqfd->shutdown);\r\n}\r\nstatic int\r\nirqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd =\r\ncontainer_of(wait, struct kvm_kernel_irqfd, wait);\r\nunsigned long flags = (unsigned long)key;\r\nstruct kvm_kernel_irq_routing_entry irq;\r\nstruct kvm *kvm = irqfd->kvm;\r\nunsigned seq;\r\nint idx;\r\nif (flags & POLLIN) {\r\nidx = srcu_read_lock(&kvm->irq_srcu);\r\ndo {\r\nseq = read_seqcount_begin(&irqfd->irq_entry_sc);\r\nirq = irqfd->irq_entry;\r\n} while (read_seqcount_retry(&irqfd->irq_entry_sc, seq));\r\nif (kvm_arch_set_irq_inatomic(&irq, kvm,\r\nKVM_USERSPACE_IRQ_SOURCE_ID, 1,\r\nfalse) == -EWOULDBLOCK)\r\nschedule_work(&irqfd->inject);\r\nsrcu_read_unlock(&kvm->irq_srcu, idx);\r\n}\r\nif (flags & POLLHUP) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&kvm->irqfds.lock, flags);\r\nif (irqfd_is_active(irqfd))\r\nirqfd_deactivate(irqfd);\r\nspin_unlock_irqrestore(&kvm->irqfds.lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nirqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,\r\npoll_table *pt)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd =\r\ncontainer_of(pt, struct kvm_kernel_irqfd, pt);\r\nadd_wait_queue(wqh, &irqfd->wait);\r\n}\r\nstatic void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)\r\n{\r\nstruct kvm_kernel_irq_routing_entry *e;\r\nstruct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];\r\nint n_entries;\r\nn_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);\r\nwrite_seqcount_begin(&irqfd->irq_entry_sc);\r\ne = entries;\r\nif (n_entries == 1)\r\nirqfd->irq_entry = *e;\r\nelse\r\nirqfd->irq_entry.type = 0;\r\nwrite_seqcount_end(&irqfd->irq_entry_sc);\r\n}\r\nstatic int\r\nkvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd, *tmp;\r\nstruct fd f;\r\nstruct eventfd_ctx *eventfd = NULL, *resamplefd = NULL;\r\nint ret;\r\nunsigned int events;\r\nint idx;\r\nif (!kvm_arch_intc_initialized(kvm))\r\nreturn -EAGAIN;\r\nirqfd = kzalloc(sizeof(*irqfd), GFP_KERNEL);\r\nif (!irqfd)\r\nreturn -ENOMEM;\r\nirqfd->kvm = kvm;\r\nirqfd->gsi = args->gsi;\r\nINIT_LIST_HEAD(&irqfd->list);\r\nINIT_WORK(&irqfd->inject, irqfd_inject);\r\nINIT_WORK(&irqfd->shutdown, irqfd_shutdown);\r\nseqcount_init(&irqfd->irq_entry_sc);\r\nf = fdget(args->fd);\r\nif (!f.file) {\r\nret = -EBADF;\r\ngoto out;\r\n}\r\neventfd = eventfd_ctx_fileget(f.file);\r\nif (IS_ERR(eventfd)) {\r\nret = PTR_ERR(eventfd);\r\ngoto fail;\r\n}\r\nirqfd->eventfd = eventfd;\r\nif (args->flags & KVM_IRQFD_FLAG_RESAMPLE) {\r\nstruct kvm_kernel_irqfd_resampler *resampler;\r\nresamplefd = eventfd_ctx_fdget(args->resamplefd);\r\nif (IS_ERR(resamplefd)) {\r\nret = PTR_ERR(resamplefd);\r\ngoto fail;\r\n}\r\nirqfd->resamplefd = resamplefd;\r\nINIT_LIST_HEAD(&irqfd->resampler_link);\r\nmutex_lock(&kvm->irqfds.resampler_lock);\r\nlist_for_each_entry(resampler,\r\n&kvm->irqfds.resampler_list, link) {\r\nif (resampler->notifier.gsi == irqfd->gsi) {\r\nirqfd->resampler = resampler;\r\nbreak;\r\n}\r\n}\r\nif (!irqfd->resampler) {\r\nresampler = kzalloc(sizeof(*resampler), GFP_KERNEL);\r\nif (!resampler) {\r\nret = -ENOMEM;\r\nmutex_unlock(&kvm->irqfds.resampler_lock);\r\ngoto fail;\r\n}\r\nresampler->kvm = kvm;\r\nINIT_LIST_HEAD(&resampler->list);\r\nresampler->notifier.gsi = irqfd->gsi;\r\nresampler->notifier.irq_acked = irqfd_resampler_ack;\r\nINIT_LIST_HEAD(&resampler->link);\r\nlist_add(&resampler->link, &kvm->irqfds.resampler_list);\r\nkvm_register_irq_ack_notifier(kvm,\r\n&resampler->notifier);\r\nirqfd->resampler = resampler;\r\n}\r\nlist_add_rcu(&irqfd->resampler_link, &irqfd->resampler->list);\r\nsynchronize_srcu(&kvm->irq_srcu);\r\nmutex_unlock(&kvm->irqfds.resampler_lock);\r\n}\r\ninit_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);\r\ninit_poll_funcptr(&irqfd->pt, irqfd_ptable_queue_proc);\r\nspin_lock_irq(&kvm->irqfds.lock);\r\nret = 0;\r\nlist_for_each_entry(tmp, &kvm->irqfds.items, list) {\r\nif (irqfd->eventfd != tmp->eventfd)\r\ncontinue;\r\nret = -EBUSY;\r\nspin_unlock_irq(&kvm->irqfds.lock);\r\ngoto fail;\r\n}\r\nidx = srcu_read_lock(&kvm->irq_srcu);\r\nirqfd_update(kvm, irqfd);\r\nsrcu_read_unlock(&kvm->irq_srcu, idx);\r\nlist_add_tail(&irqfd->list, &kvm->irqfds.items);\r\nspin_unlock_irq(&kvm->irqfds.lock);\r\nevents = f.file->f_op->poll(f.file, &irqfd->pt);\r\nif (events & POLLIN)\r\nschedule_work(&irqfd->inject);\r\nfdput(f);\r\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\r\nif (kvm_arch_has_irq_bypass()) {\r\nirqfd->consumer.token = (void *)irqfd->eventfd;\r\nirqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;\r\nirqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;\r\nirqfd->consumer.stop = kvm_arch_irq_bypass_stop;\r\nirqfd->consumer.start = kvm_arch_irq_bypass_start;\r\nret = irq_bypass_register_consumer(&irqfd->consumer);\r\nif (ret)\r\npr_info("irq bypass consumer (token %p) registration fails: %d\n",\r\nirqfd->consumer.token, ret);\r\n}\r\n#endif\r\nreturn 0;\r\nfail:\r\nif (irqfd->resampler)\r\nirqfd_resampler_shutdown(irqfd);\r\nif (resamplefd && !IS_ERR(resamplefd))\r\neventfd_ctx_put(resamplefd);\r\nif (eventfd && !IS_ERR(eventfd))\r\neventfd_ctx_put(eventfd);\r\nfdput(f);\r\nout:\r\nkfree(irqfd);\r\nreturn ret;\r\n}\r\nbool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)\r\n{\r\nstruct kvm_irq_ack_notifier *kian;\r\nint gsi, idx;\r\nidx = srcu_read_lock(&kvm->irq_srcu);\r\ngsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\r\nif (gsi != -1)\r\nhlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,\r\nlink)\r\nif (kian->gsi == gsi) {\r\nsrcu_read_unlock(&kvm->irq_srcu, idx);\r\nreturn true;\r\n}\r\nsrcu_read_unlock(&kvm->irq_srcu, idx);\r\nreturn false;\r\n}\r\nvoid kvm_notify_acked_gsi(struct kvm *kvm, int gsi)\r\n{\r\nstruct kvm_irq_ack_notifier *kian;\r\nhlist_for_each_entry_rcu(kian, &kvm->irq_ack_notifier_list,\r\nlink)\r\nif (kian->gsi == gsi)\r\nkian->irq_acked(kian);\r\n}\r\nvoid kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)\r\n{\r\nint gsi, idx;\r\ntrace_kvm_ack_irq(irqchip, pin);\r\nidx = srcu_read_lock(&kvm->irq_srcu);\r\ngsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\r\nif (gsi != -1)\r\nkvm_notify_acked_gsi(kvm, gsi);\r\nsrcu_read_unlock(&kvm->irq_srcu, idx);\r\n}\r\nvoid kvm_register_irq_ack_notifier(struct kvm *kvm,\r\nstruct kvm_irq_ack_notifier *kian)\r\n{\r\nmutex_lock(&kvm->irq_lock);\r\nhlist_add_head_rcu(&kian->link, &kvm->irq_ack_notifier_list);\r\nmutex_unlock(&kvm->irq_lock);\r\nkvm_arch_post_irq_ack_notifier_list_update(kvm);\r\n}\r\nvoid kvm_unregister_irq_ack_notifier(struct kvm *kvm,\r\nstruct kvm_irq_ack_notifier *kian)\r\n{\r\nmutex_lock(&kvm->irq_lock);\r\nhlist_del_init_rcu(&kian->link);\r\nmutex_unlock(&kvm->irq_lock);\r\nsynchronize_srcu(&kvm->irq_srcu);\r\nkvm_arch_post_irq_ack_notifier_list_update(kvm);\r\n}\r\nvoid\r\nkvm_eventfd_init(struct kvm *kvm)\r\n{\r\n#ifdef CONFIG_HAVE_KVM_IRQFD\r\nspin_lock_init(&kvm->irqfds.lock);\r\nINIT_LIST_HEAD(&kvm->irqfds.items);\r\nINIT_LIST_HEAD(&kvm->irqfds.resampler_list);\r\nmutex_init(&kvm->irqfds.resampler_lock);\r\n#endif\r\nINIT_LIST_HEAD(&kvm->ioeventfds);\r\n}\r\nstatic int\r\nkvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd, *tmp;\r\nstruct eventfd_ctx *eventfd;\r\neventfd = eventfd_ctx_fdget(args->fd);\r\nif (IS_ERR(eventfd))\r\nreturn PTR_ERR(eventfd);\r\nspin_lock_irq(&kvm->irqfds.lock);\r\nlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list) {\r\nif (irqfd->eventfd == eventfd && irqfd->gsi == args->gsi) {\r\nwrite_seqcount_begin(&irqfd->irq_entry_sc);\r\nirqfd->irq_entry.type = 0;\r\nwrite_seqcount_end(&irqfd->irq_entry_sc);\r\nirqfd_deactivate(irqfd);\r\n}\r\n}\r\nspin_unlock_irq(&kvm->irqfds.lock);\r\neventfd_ctx_put(eventfd);\r\nflush_workqueue(irqfd_cleanup_wq);\r\nreturn 0;\r\n}\r\nint\r\nkvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)\r\n{\r\nif (args->flags & ~(KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE))\r\nreturn -EINVAL;\r\nif (args->flags & KVM_IRQFD_FLAG_DEASSIGN)\r\nreturn kvm_irqfd_deassign(kvm, args);\r\nreturn kvm_irqfd_assign(kvm, args);\r\n}\r\nvoid\r\nkvm_irqfd_release(struct kvm *kvm)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd, *tmp;\r\nspin_lock_irq(&kvm->irqfds.lock);\r\nlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list)\r\nirqfd_deactivate(irqfd);\r\nspin_unlock_irq(&kvm->irqfds.lock);\r\nflush_workqueue(irqfd_cleanup_wq);\r\n}\r\nvoid kvm_irq_routing_update(struct kvm *kvm)\r\n{\r\nstruct kvm_kernel_irqfd *irqfd;\r\nspin_lock_irq(&kvm->irqfds.lock);\r\nlist_for_each_entry(irqfd, &kvm->irqfds.items, list) {\r\nirqfd_update(kvm, irqfd);\r\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\r\nif (irqfd->producer) {\r\nint ret = kvm_arch_update_irqfd_routing(\r\nirqfd->kvm, irqfd->producer->irq,\r\nirqfd->gsi, 1);\r\nWARN_ON(ret);\r\n}\r\n#endif\r\n}\r\nspin_unlock_irq(&kvm->irqfds.lock);\r\n}\r\nint kvm_irqfd_init(void)\r\n{\r\nirqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);\r\nif (!irqfd_cleanup_wq)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid kvm_irqfd_exit(void)\r\n{\r\ndestroy_workqueue(irqfd_cleanup_wq);\r\n}\r\nstatic inline struct _ioeventfd *\r\nto_ioeventfd(struct kvm_io_device *dev)\r\n{\r\nreturn container_of(dev, struct _ioeventfd, dev);\r\n}\r\nstatic void\r\nioeventfd_release(struct _ioeventfd *p)\r\n{\r\neventfd_ctx_put(p->eventfd);\r\nlist_del(&p->list);\r\nkfree(p);\r\n}\r\nstatic bool\r\nioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)\r\n{\r\nu64 _val;\r\nif (addr != p->addr)\r\nreturn false;\r\nif (!p->length)\r\nreturn true;\r\nif (len != p->length)\r\nreturn false;\r\nif (p->wildcard)\r\nreturn true;\r\nBUG_ON(!IS_ALIGNED((unsigned long)val, len));\r\nswitch (len) {\r\ncase 1:\r\n_val = *(u8 *)val;\r\nbreak;\r\ncase 2:\r\n_val = *(u16 *)val;\r\nbreak;\r\ncase 4:\r\n_val = *(u32 *)val;\r\nbreak;\r\ncase 8:\r\n_val = *(u64 *)val;\r\nbreak;\r\ndefault:\r\nreturn false;\r\n}\r\nreturn _val == p->datamatch ? true : false;\r\n}\r\nstatic int\r\nioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,\r\nint len, const void *val)\r\n{\r\nstruct _ioeventfd *p = to_ioeventfd(this);\r\nif (!ioeventfd_in_range(p, addr, len, val))\r\nreturn -EOPNOTSUPP;\r\neventfd_signal(p->eventfd, 1);\r\nreturn 0;\r\n}\r\nstatic void\r\nioeventfd_destructor(struct kvm_io_device *this)\r\n{\r\nstruct _ioeventfd *p = to_ioeventfd(this);\r\nioeventfd_release(p);\r\n}\r\nstatic bool\r\nioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)\r\n{\r\nstruct _ioeventfd *_p;\r\nlist_for_each_entry(_p, &kvm->ioeventfds, list)\r\nif (_p->bus_idx == p->bus_idx &&\r\n_p->addr == p->addr &&\r\n(!_p->length || !p->length ||\r\n(_p->length == p->length &&\r\n(_p->wildcard || p->wildcard ||\r\n_p->datamatch == p->datamatch))))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)\r\n{\r\nif (flags & KVM_IOEVENTFD_FLAG_PIO)\r\nreturn KVM_PIO_BUS;\r\nif (flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY)\r\nreturn KVM_VIRTIO_CCW_NOTIFY_BUS;\r\nreturn KVM_MMIO_BUS;\r\n}\r\nstatic int kvm_assign_ioeventfd_idx(struct kvm *kvm,\r\nenum kvm_bus bus_idx,\r\nstruct kvm_ioeventfd *args)\r\n{\r\nstruct eventfd_ctx *eventfd;\r\nstruct _ioeventfd *p;\r\nint ret;\r\neventfd = eventfd_ctx_fdget(args->fd);\r\nif (IS_ERR(eventfd))\r\nreturn PTR_ERR(eventfd);\r\np = kzalloc(sizeof(*p), GFP_KERNEL);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nINIT_LIST_HEAD(&p->list);\r\np->addr = args->addr;\r\np->bus_idx = bus_idx;\r\np->length = args->len;\r\np->eventfd = eventfd;\r\nif (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH)\r\np->datamatch = args->datamatch;\r\nelse\r\np->wildcard = true;\r\nmutex_lock(&kvm->slots_lock);\r\nif (ioeventfd_check_collision(kvm, p)) {\r\nret = -EEXIST;\r\ngoto unlock_fail;\r\n}\r\nkvm_iodevice_init(&p->dev, &ioeventfd_ops);\r\nret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,\r\n&p->dev);\r\nif (ret < 0)\r\ngoto unlock_fail;\r\nkvm_get_bus(kvm, bus_idx)->ioeventfd_count++;\r\nlist_add_tail(&p->list, &kvm->ioeventfds);\r\nmutex_unlock(&kvm->slots_lock);\r\nreturn 0;\r\nunlock_fail:\r\nmutex_unlock(&kvm->slots_lock);\r\nfail:\r\nkfree(p);\r\neventfd_ctx_put(eventfd);\r\nreturn ret;\r\n}\r\nstatic int\r\nkvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,\r\nstruct kvm_ioeventfd *args)\r\n{\r\nstruct _ioeventfd *p, *tmp;\r\nstruct eventfd_ctx *eventfd;\r\nstruct kvm_io_bus *bus;\r\nint ret = -ENOENT;\r\neventfd = eventfd_ctx_fdget(args->fd);\r\nif (IS_ERR(eventfd))\r\nreturn PTR_ERR(eventfd);\r\nmutex_lock(&kvm->slots_lock);\r\nlist_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {\r\nbool wildcard = !(args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH);\r\nif (p->bus_idx != bus_idx ||\r\np->eventfd != eventfd ||\r\np->addr != args->addr ||\r\np->length != args->len ||\r\np->wildcard != wildcard)\r\ncontinue;\r\nif (!p->wildcard && p->datamatch != args->datamatch)\r\ncontinue;\r\nkvm_io_bus_unregister_dev(kvm, bus_idx, &p->dev);\r\nbus = kvm_get_bus(kvm, bus_idx);\r\nif (bus)\r\nbus->ioeventfd_count--;\r\nioeventfd_release(p);\r\nret = 0;\r\nbreak;\r\n}\r\nmutex_unlock(&kvm->slots_lock);\r\neventfd_ctx_put(eventfd);\r\nreturn ret;\r\n}\r\nstatic int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\r\n{\r\nenum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);\r\nint ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\r\nif (!args->len && bus_idx == KVM_MMIO_BUS)\r\nkvm_deassign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\r\nreturn ret;\r\n}\r\nstatic int\r\nkvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\r\n{\r\nenum kvm_bus bus_idx;\r\nint ret;\r\nbus_idx = ioeventfd_bus_from_flags(args->flags);\r\nswitch (args->len) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (args->addr + args->len < args->addr)\r\nreturn -EINVAL;\r\nif (args->flags & ~KVM_IOEVENTFD_VALID_FLAG_MASK)\r\nreturn -EINVAL;\r\nif (!args->len && (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH))\r\nreturn -EINVAL;\r\nret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);\r\nif (ret)\r\ngoto fail;\r\nif (!args->len && bus_idx == KVM_MMIO_BUS) {\r\nret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\r\nif (ret < 0)\r\ngoto fast_fail;\r\n}\r\nreturn 0;\r\nfast_fail:\r\nkvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\r\nfail:\r\nreturn ret;\r\n}\r\nint\r\nkvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\r\n{\r\nif (args->flags & KVM_IOEVENTFD_FLAG_DEASSIGN)\r\nreturn kvm_deassign_ioeventfd(kvm, args);\r\nreturn kvm_assign_ioeventfd(kvm, args);\r\n}
