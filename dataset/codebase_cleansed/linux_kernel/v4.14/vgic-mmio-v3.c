unsigned long extract_bytes(u64 data, unsigned int offset,\r\nunsigned int num)\r\n{\r\nreturn (data >> (offset * 8)) & GENMASK_ULL(num * 8 - 1, 0);\r\n}\r\nu64 update_64bit_reg(u64 reg, unsigned int offset, unsigned int len,\r\nunsigned long val)\r\n{\r\nint lower = (offset & 4) * 8;\r\nint upper = lower + 8 * len - 1;\r\nreg &= ~GENMASK_ULL(upper, lower);\r\nval &= GENMASK_ULL(len * 8 - 1, 0);\r\nreturn reg | ((u64)val << lower);\r\n}\r\nbool vgic_has_its(struct kvm *kvm)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nif (dist->vgic_model != KVM_DEV_TYPE_ARM_VGIC_V3)\r\nreturn false;\r\nreturn dist->has_its;\r\n}\r\nstatic unsigned long vgic_mmio_read_v3_misc(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 value = 0;\r\nswitch (addr & 0x0c) {\r\ncase GICD_CTLR:\r\nif (vcpu->kvm->arch.vgic.enabled)\r\nvalue |= GICD_CTLR_ENABLE_SS_G1;\r\nvalue |= GICD_CTLR_ARE_NS | GICD_CTLR_DS;\r\nbreak;\r\ncase GICD_TYPER:\r\nvalue = vcpu->kvm->arch.vgic.nr_spis + VGIC_NR_PRIVATE_IRQS;\r\nvalue = (value >> 5) - 1;\r\nif (vgic_has_its(vcpu->kvm)) {\r\nvalue |= (INTERRUPT_ID_BITS_ITS - 1) << 19;\r\nvalue |= GICD_TYPER_LPIS;\r\n} else {\r\nvalue |= (INTERRUPT_ID_BITS_SPIS - 1) << 19;\r\n}\r\nbreak;\r\ncase GICD_IIDR:\r\nvalue = (PRODUCT_ID_KVM << 24) | (IMPLEMENTER_ARM << 0);\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nreturn value;\r\n}\r\nstatic void vgic_mmio_write_v3_misc(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nbool was_enabled = dist->enabled;\r\nswitch (addr & 0x0c) {\r\ncase GICD_CTLR:\r\ndist->enabled = val & GICD_CTLR_ENABLE_SS_G1;\r\nif (!was_enabled && dist->enabled)\r\nvgic_kick_vcpus(vcpu->kvm);\r\nbreak;\r\ncase GICD_TYPER:\r\ncase GICD_IIDR:\r\nreturn;\r\n}\r\n}\r\nstatic unsigned long vgic_mmio_read_irouter(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nint intid = VGIC_ADDR_TO_INTID(addr, 64);\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, NULL, intid);\r\nunsigned long ret = 0;\r\nif (!irq)\r\nreturn 0;\r\nif (!(addr & 4))\r\nret = extract_bytes(READ_ONCE(irq->mpidr), addr & 7, len);\r\nvgic_put_irq(vcpu->kvm, irq);\r\nreturn ret;\r\n}\r\nstatic void vgic_mmio_write_irouter(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nint intid = VGIC_ADDR_TO_INTID(addr, 64);\r\nstruct vgic_irq *irq;\r\nif (addr & 4)\r\nreturn;\r\nirq = vgic_get_irq(vcpu->kvm, NULL, intid);\r\nif (!irq)\r\nreturn;\r\nspin_lock(&irq->irq_lock);\r\nirq->mpidr = val & GENMASK(23, 0);\r\nirq->target_vcpu = kvm_mpidr_to_vcpu(vcpu->kvm, irq->mpidr);\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nstatic unsigned long vgic_mmio_read_v3r_ctlr(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nreturn vgic_cpu->lpis_enabled ? GICR_CTLR_ENABLE_LPIS : 0;\r\n}\r\nstatic void vgic_mmio_write_v3r_ctlr(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nbool was_enabled = vgic_cpu->lpis_enabled;\r\nif (!vgic_has_its(vcpu->kvm))\r\nreturn;\r\nvgic_cpu->lpis_enabled = val & GICR_CTLR_ENABLE_LPIS;\r\nif (!was_enabled && vgic_cpu->lpis_enabled)\r\nvgic_enable_lpis(vcpu);\r\n}\r\nstatic unsigned long vgic_mmio_read_v3r_typer(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nunsigned long mpidr = kvm_vcpu_get_mpidr_aff(vcpu);\r\nint target_vcpu_id = vcpu->vcpu_id;\r\nu64 value;\r\nvalue = (u64)(mpidr & GENMASK(23, 0)) << 32;\r\nvalue |= ((target_vcpu_id & 0xffff) << 8);\r\nif (target_vcpu_id == atomic_read(&vcpu->kvm->online_vcpus) - 1)\r\nvalue |= GICR_TYPER_LAST;\r\nif (vgic_has_its(vcpu->kvm))\r\nvalue |= GICR_TYPER_PLPIS;\r\nreturn extract_bytes(value, addr & 7, len);\r\n}\r\nstatic unsigned long vgic_mmio_read_v3r_iidr(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nreturn (PRODUCT_ID_KVM << 24) | (IMPLEMENTER_ARM << 0);\r\n}\r\nstatic unsigned long vgic_mmio_read_v3_idregs(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nswitch (addr & 0xffff) {\r\ncase GICD_PIDR2:\r\nreturn 0x3b;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned long vgic_v3_uaccess_read_pending(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nu32 value = 0;\r\nint i;\r\nfor (i = 0; i < len * 8; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nif (irq->pending_latch)\r\nvalue |= (1U << i);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn value;\r\n}\r\nstatic void vgic_v3_uaccess_write_pending(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nint i;\r\nfor (i = 0; i < len * 8; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nif (test_bit(i, &val)) {\r\nirq->pending_latch = true;\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\n} else {\r\nirq->pending_latch = false;\r\nspin_unlock(&irq->irq_lock);\r\n}\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nu64 vgic_sanitise_shareability(u64 field)\r\n{\r\nswitch (field) {\r\ncase GIC_BASER_OuterShareable:\r\nreturn GIC_BASER_InnerShareable;\r\ndefault:\r\nreturn field;\r\n}\r\n}\r\nu64 vgic_sanitise_inner_cacheability(u64 field)\r\n{\r\nswitch (field) {\r\ncase GIC_BASER_CACHE_nCnB:\r\ncase GIC_BASER_CACHE_nC:\r\nreturn GIC_BASER_CACHE_RaWb;\r\ndefault:\r\nreturn field;\r\n}\r\n}\r\nu64 vgic_sanitise_outer_cacheability(u64 field)\r\n{\r\nswitch (field) {\r\ncase GIC_BASER_CACHE_SameAsInner:\r\ncase GIC_BASER_CACHE_nC:\r\nreturn field;\r\ndefault:\r\nreturn GIC_BASER_CACHE_nC;\r\n}\r\n}\r\nu64 vgic_sanitise_field(u64 reg, u64 field_mask, int field_shift,\r\nu64 (*sanitise_fn)(u64))\r\n{\r\nu64 field = (reg & field_mask) >> field_shift;\r\nfield = sanitise_fn(field) << field_shift;\r\nreturn (reg & ~field_mask) | field;\r\n}\r\nstatic u64 vgic_sanitise_pendbaser(u64 reg)\r\n{\r\nreg = vgic_sanitise_field(reg, GICR_PENDBASER_SHAREABILITY_MASK,\r\nGICR_PENDBASER_SHAREABILITY_SHIFT,\r\nvgic_sanitise_shareability);\r\nreg = vgic_sanitise_field(reg, GICR_PENDBASER_INNER_CACHEABILITY_MASK,\r\nGICR_PENDBASER_INNER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_inner_cacheability);\r\nreg = vgic_sanitise_field(reg, GICR_PENDBASER_OUTER_CACHEABILITY_MASK,\r\nGICR_PENDBASER_OUTER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_outer_cacheability);\r\nreg &= ~PENDBASER_RES0_MASK;\r\nreg &= ~GENMASK_ULL(51, 48);\r\nreturn reg;\r\n}\r\nstatic u64 vgic_sanitise_propbaser(u64 reg)\r\n{\r\nreg = vgic_sanitise_field(reg, GICR_PROPBASER_SHAREABILITY_MASK,\r\nGICR_PROPBASER_SHAREABILITY_SHIFT,\r\nvgic_sanitise_shareability);\r\nreg = vgic_sanitise_field(reg, GICR_PROPBASER_INNER_CACHEABILITY_MASK,\r\nGICR_PROPBASER_INNER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_inner_cacheability);\r\nreg = vgic_sanitise_field(reg, GICR_PROPBASER_OUTER_CACHEABILITY_MASK,\r\nGICR_PROPBASER_OUTER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_outer_cacheability);\r\nreg &= ~PROPBASER_RES0_MASK;\r\nreg &= ~GENMASK_ULL(51, 48);\r\nreturn reg;\r\n}\r\nstatic unsigned long vgic_mmio_read_propbase(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn extract_bytes(dist->propbaser, addr & 7, len);\r\n}\r\nstatic void vgic_mmio_write_propbase(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nu64 old_propbaser, propbaser;\r\nif (vgic_cpu->lpis_enabled)\r\nreturn;\r\ndo {\r\nold_propbaser = READ_ONCE(dist->propbaser);\r\npropbaser = old_propbaser;\r\npropbaser = update_64bit_reg(propbaser, addr & 4, len, val);\r\npropbaser = vgic_sanitise_propbaser(propbaser);\r\n} while (cmpxchg64(&dist->propbaser, old_propbaser,\r\npropbaser) != old_propbaser);\r\n}\r\nstatic unsigned long vgic_mmio_read_pendbase(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nreturn extract_bytes(vgic_cpu->pendbaser, addr & 7, len);\r\n}\r\nstatic void vgic_mmio_write_pendbase(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nu64 old_pendbaser, pendbaser;\r\nif (vgic_cpu->lpis_enabled)\r\nreturn;\r\ndo {\r\nold_pendbaser = READ_ONCE(vgic_cpu->pendbaser);\r\npendbaser = old_pendbaser;\r\npendbaser = update_64bit_reg(pendbaser, addr & 4, len, val);\r\npendbaser = vgic_sanitise_pendbaser(pendbaser);\r\n} while (cmpxchg64(&vgic_cpu->pendbaser, old_pendbaser,\r\npendbaser) != old_pendbaser);\r\n}\r\nunsigned int vgic_v3_init_dist_iodev(struct vgic_io_device *dev)\r\n{\r\ndev->regions = vgic_v3_dist_registers;\r\ndev->nr_regions = ARRAY_SIZE(vgic_v3_dist_registers);\r\nkvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);\r\nreturn SZ_64K;\r\n}\r\nint vgic_register_redist_iodev(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct vgic_dist *vgic = &kvm->arch.vgic;\r\nstruct vgic_io_device *rd_dev = &vcpu->arch.vgic_cpu.rd_iodev;\r\nstruct vgic_io_device *sgi_dev = &vcpu->arch.vgic_cpu.sgi_iodev;\r\ngpa_t rd_base, sgi_base;\r\nint ret;\r\nif (IS_VGIC_ADDR_UNDEF(vgic->vgic_redist_base))\r\nreturn 0;\r\nif (!vgic_v3_check_base(kvm))\r\nreturn -EINVAL;\r\nrd_base = vgic->vgic_redist_base + vgic->vgic_redist_free_offset;\r\nsgi_base = rd_base + SZ_64K;\r\nkvm_iodevice_init(&rd_dev->dev, &kvm_io_gic_ops);\r\nrd_dev->base_addr = rd_base;\r\nrd_dev->iodev_type = IODEV_REDIST;\r\nrd_dev->regions = vgic_v3_rdbase_registers;\r\nrd_dev->nr_regions = ARRAY_SIZE(vgic_v3_rdbase_registers);\r\nrd_dev->redist_vcpu = vcpu;\r\nmutex_lock(&kvm->slots_lock);\r\nret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, rd_base,\r\nSZ_64K, &rd_dev->dev);\r\nmutex_unlock(&kvm->slots_lock);\r\nif (ret)\r\nreturn ret;\r\nkvm_iodevice_init(&sgi_dev->dev, &kvm_io_gic_ops);\r\nsgi_dev->base_addr = sgi_base;\r\nsgi_dev->iodev_type = IODEV_REDIST;\r\nsgi_dev->regions = vgic_v3_sgibase_registers;\r\nsgi_dev->nr_regions = ARRAY_SIZE(vgic_v3_sgibase_registers);\r\nsgi_dev->redist_vcpu = vcpu;\r\nmutex_lock(&kvm->slots_lock);\r\nret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, sgi_base,\r\nSZ_64K, &sgi_dev->dev);\r\nif (ret) {\r\nkvm_io_bus_unregister_dev(kvm, KVM_MMIO_BUS,\r\n&rd_dev->dev);\r\ngoto out;\r\n}\r\nvgic->vgic_redist_free_offset += 2 * SZ_64K;\r\nout:\r\nmutex_unlock(&kvm->slots_lock);\r\nreturn ret;\r\n}\r\nstatic void vgic_unregister_redist_iodev(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_io_device *rd_dev = &vcpu->arch.vgic_cpu.rd_iodev;\r\nstruct vgic_io_device *sgi_dev = &vcpu->arch.vgic_cpu.sgi_iodev;\r\nkvm_io_bus_unregister_dev(vcpu->kvm, KVM_MMIO_BUS, &rd_dev->dev);\r\nkvm_io_bus_unregister_dev(vcpu->kvm, KVM_MMIO_BUS, &sgi_dev->dev);\r\n}\r\nstatic int vgic_register_all_redist_iodevs(struct kvm *kvm)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nint c, ret = 0;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nret = vgic_register_redist_iodev(vcpu);\r\nif (ret)\r\nbreak;\r\n}\r\nif (ret) {\r\nmutex_lock(&kvm->slots_lock);\r\nfor (c--; c >= 0; c--) {\r\nvcpu = kvm_get_vcpu(kvm, c);\r\nvgic_unregister_redist_iodev(vcpu);\r\n}\r\nmutex_unlock(&kvm->slots_lock);\r\n}\r\nreturn ret;\r\n}\r\nint vgic_v3_set_redist_base(struct kvm *kvm, u64 addr)\r\n{\r\nstruct vgic_dist *vgic = &kvm->arch.vgic;\r\nint ret;\r\nret = vgic_check_ioaddr(kvm, &vgic->vgic_redist_base, addr, SZ_64K);\r\nif (ret)\r\nreturn ret;\r\nvgic->vgic_redist_base = addr;\r\nif (!vgic_v3_check_base(kvm)) {\r\nvgic->vgic_redist_base = VGIC_ADDR_UNDEF;\r\nreturn -EINVAL;\r\n}\r\nret = vgic_register_all_redist_iodevs(kvm);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nint vgic_v3_has_attr_regs(struct kvm_device *dev, struct kvm_device_attr *attr)\r\n{\r\nconst struct vgic_register_region *region;\r\nstruct vgic_io_device iodev;\r\nstruct vgic_reg_attr reg_attr;\r\nstruct kvm_vcpu *vcpu;\r\ngpa_t addr;\r\nint ret;\r\nret = vgic_v3_parse_attr(dev, attr, &reg_attr);\r\nif (ret)\r\nreturn ret;\r\nvcpu = reg_attr.vcpu;\r\naddr = reg_attr.addr;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\niodev.regions = vgic_v3_dist_registers;\r\niodev.nr_regions = ARRAY_SIZE(vgic_v3_dist_registers);\r\niodev.base_addr = 0;\r\nbreak;\r\ncase KVM_DEV_ARM_VGIC_GRP_REDIST_REGS:{\r\niodev.regions = vgic_v3_rdbase_registers;\r\niodev.nr_regions = ARRAY_SIZE(vgic_v3_rdbase_registers);\r\niodev.base_addr = 0;\r\nbreak;\r\n}\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_SYSREGS: {\r\nu64 reg, id;\r\nid = (attr->attr & KVM_DEV_ARM_VGIC_SYSREG_INSTR_MASK);\r\nreturn vgic_v3_has_cpu_sysregs_attr(vcpu, 0, id, &reg);\r\n}\r\ndefault:\r\nreturn -ENXIO;\r\n}\r\nif (addr & 3)\r\nreturn -ENXIO;\r\nregion = vgic_get_mmio_region(vcpu, &iodev, addr, sizeof(u32));\r\nif (!region)\r\nreturn -ENXIO;\r\nreturn 0;\r\n}\r\nstatic int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long affinity;\r\nint level0;\r\naffinity = kvm_vcpu_get_mpidr_aff(vcpu);\r\nlevel0 = MPIDR_AFFINITY_LEVEL(affinity, 0);\r\naffinity &= ~MPIDR_LEVEL_MASK;\r\nif (sgi_aff != affinity)\r\nreturn -1;\r\nif (!(sgi_cpu_mask & BIT(level0)))\r\nreturn -1;\r\nreturn level0;\r\n}\r\nvoid vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct kvm_vcpu *c_vcpu;\r\nu16 target_cpus;\r\nu64 mpidr;\r\nint sgi, c;\r\nint vcpu_id = vcpu->vcpu_id;\r\nbool broadcast;\r\nsgi = (reg & ICC_SGI1R_SGI_ID_MASK) >> ICC_SGI1R_SGI_ID_SHIFT;\r\nbroadcast = reg & BIT_ULL(ICC_SGI1R_IRQ_ROUTING_MODE_BIT);\r\ntarget_cpus = (reg & ICC_SGI1R_TARGET_LIST_MASK) >> ICC_SGI1R_TARGET_LIST_SHIFT;\r\nmpidr = SGI_AFFINITY_LEVEL(reg, 3);\r\nmpidr |= SGI_AFFINITY_LEVEL(reg, 2);\r\nmpidr |= SGI_AFFINITY_LEVEL(reg, 1);\r\nkvm_for_each_vcpu(c, c_vcpu, kvm) {\r\nstruct vgic_irq *irq;\r\nif (!broadcast && target_cpus == 0)\r\nbreak;\r\nif (broadcast && c == vcpu_id)\r\ncontinue;\r\nif (!broadcast) {\r\nint level0;\r\nlevel0 = match_mpidr(mpidr, target_cpus, c_vcpu);\r\nif (level0 == -1)\r\ncontinue;\r\ntarget_cpus &= ~BIT(level0);\r\n}\r\nirq = vgic_get_irq(vcpu->kvm, c_vcpu, sgi);\r\nspin_lock(&irq->irq_lock);\r\nirq->pending_latch = true;\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nint vgic_v3_dist_uaccess(struct kvm_vcpu *vcpu, bool is_write,\r\nint offset, u32 *val)\r\n{\r\nstruct vgic_io_device dev = {\r\n.regions = vgic_v3_dist_registers,\r\n.nr_regions = ARRAY_SIZE(vgic_v3_dist_registers),\r\n};\r\nreturn vgic_uaccess(vcpu, &dev, is_write, offset, val);\r\n}\r\nint vgic_v3_redist_uaccess(struct kvm_vcpu *vcpu, bool is_write,\r\nint offset, u32 *val)\r\n{\r\nstruct vgic_io_device rd_dev = {\r\n.regions = vgic_v3_rdbase_registers,\r\n.nr_regions = ARRAY_SIZE(vgic_v3_rdbase_registers),\r\n};\r\nstruct vgic_io_device sgi_dev = {\r\n.regions = vgic_v3_sgibase_registers,\r\n.nr_regions = ARRAY_SIZE(vgic_v3_sgibase_registers),\r\n};\r\nif (offset >= SZ_64K)\r\nreturn vgic_uaccess(vcpu, &sgi_dev, is_write, offset - SZ_64K,\r\nval);\r\nelse\r\nreturn vgic_uaccess(vcpu, &rd_dev, is_write, offset, val);\r\n}\r\nint vgic_v3_line_level_info_uaccess(struct kvm_vcpu *vcpu, bool is_write,\r\nu32 intid, u64 *val)\r\n{\r\nif (intid % 32)\r\nreturn -EINVAL;\r\nif (is_write)\r\nvgic_write_irq_line_level_info(vcpu, intid, *val);\r\nelse\r\n*val = vgic_read_irq_line_level_info(vcpu, intid);\r\nreturn 0;\r\n}
