static int ring_interrupt_index(struct tb_ring *ring)\r\n{\r\nint bit = ring->hop;\r\nif (!ring->is_tx)\r\nbit += ring->nhi->hop_count;\r\nreturn bit;\r\n}\r\nstatic void ring_interrupt_active(struct tb_ring *ring, bool active)\r\n{\r\nint reg = REG_RING_INTERRUPT_BASE +\r\nring_interrupt_index(ring) / 32 * 4;\r\nint bit = ring_interrupt_index(ring) & 31;\r\nint mask = 1 << bit;\r\nu32 old, new;\r\nif (ring->irq > 0) {\r\nu32 step, shift, ivr, misc;\r\nvoid __iomem *ivr_base;\r\nint index;\r\nif (ring->is_tx)\r\nindex = ring->hop;\r\nelse\r\nindex = ring->hop + ring->nhi->hop_count;\r\nmisc = ioread32(ring->nhi->iobase + REG_DMA_MISC);\r\nif (!(misc & REG_DMA_MISC_INT_AUTO_CLEAR)) {\r\nmisc |= REG_DMA_MISC_INT_AUTO_CLEAR;\r\niowrite32(misc, ring->nhi->iobase + REG_DMA_MISC);\r\n}\r\nivr_base = ring->nhi->iobase + REG_INT_VEC_ALLOC_BASE;\r\nstep = index / REG_INT_VEC_ALLOC_REGS * REG_INT_VEC_ALLOC_BITS;\r\nshift = index % REG_INT_VEC_ALLOC_REGS * REG_INT_VEC_ALLOC_BITS;\r\nivr = ioread32(ivr_base + step);\r\nivr &= ~(REG_INT_VEC_ALLOC_MASK << shift);\r\nif (active)\r\nivr |= ring->vector << shift;\r\niowrite32(ivr, ivr_base + step);\r\n}\r\nold = ioread32(ring->nhi->iobase + reg);\r\nif (active)\r\nnew = old | mask;\r\nelse\r\nnew = old & ~mask;\r\ndev_info(&ring->nhi->pdev->dev,\r\n"%s interrupt at register %#x bit %d (%#x -> %#x)\n",\r\nactive ? "enabling" : "disabling", reg, bit, old, new);\r\nif (new == old)\r\ndev_WARN(&ring->nhi->pdev->dev,\r\n"interrupt for %s %d is already %s\n",\r\nRING_TYPE(ring), ring->hop,\r\nactive ? "enabled" : "disabled");\r\niowrite32(new, ring->nhi->iobase + reg);\r\n}\r\nstatic void nhi_disable_interrupts(struct tb_nhi *nhi)\r\n{\r\nint i = 0;\r\nfor (i = 0; i < RING_INTERRUPT_REG_COUNT(nhi); i++)\r\niowrite32(0, nhi->iobase + REG_RING_INTERRUPT_BASE + 4 * i);\r\nfor (i = 0; i < RING_NOTIFY_REG_COUNT(nhi); i++)\r\nioread32(nhi->iobase + REG_RING_NOTIFY_BASE + 4 * i);\r\n}\r\nstatic void __iomem *ring_desc_base(struct tb_ring *ring)\r\n{\r\nvoid __iomem *io = ring->nhi->iobase;\r\nio += ring->is_tx ? REG_TX_RING_BASE : REG_RX_RING_BASE;\r\nio += ring->hop * 16;\r\nreturn io;\r\n}\r\nstatic void __iomem *ring_options_base(struct tb_ring *ring)\r\n{\r\nvoid __iomem *io = ring->nhi->iobase;\r\nio += ring->is_tx ? REG_TX_OPTIONS_BASE : REG_RX_OPTIONS_BASE;\r\nio += ring->hop * 32;\r\nreturn io;\r\n}\r\nstatic void ring_iowrite16desc(struct tb_ring *ring, u32 value, u32 offset)\r\n{\r\niowrite16(value, ring_desc_base(ring) + offset);\r\n}\r\nstatic void ring_iowrite32desc(struct tb_ring *ring, u32 value, u32 offset)\r\n{\r\niowrite32(value, ring_desc_base(ring) + offset);\r\n}\r\nstatic void ring_iowrite64desc(struct tb_ring *ring, u64 value, u32 offset)\r\n{\r\niowrite32(value, ring_desc_base(ring) + offset);\r\niowrite32(value >> 32, ring_desc_base(ring) + offset + 4);\r\n}\r\nstatic void ring_iowrite32options(struct tb_ring *ring, u32 value, u32 offset)\r\n{\r\niowrite32(value, ring_options_base(ring) + offset);\r\n}\r\nstatic bool ring_full(struct tb_ring *ring)\r\n{\r\nreturn ((ring->head + 1) % ring->size) == ring->tail;\r\n}\r\nstatic bool ring_empty(struct tb_ring *ring)\r\n{\r\nreturn ring->head == ring->tail;\r\n}\r\nstatic void ring_write_descriptors(struct tb_ring *ring)\r\n{\r\nstruct ring_frame *frame, *n;\r\nstruct ring_desc *descriptor;\r\nlist_for_each_entry_safe(frame, n, &ring->queue, list) {\r\nif (ring_full(ring))\r\nbreak;\r\nlist_move_tail(&frame->list, &ring->in_flight);\r\ndescriptor = &ring->descriptors[ring->head];\r\ndescriptor->phys = frame->buffer_phy;\r\ndescriptor->time = 0;\r\ndescriptor->flags = RING_DESC_POSTED | RING_DESC_INTERRUPT;\r\nif (ring->is_tx) {\r\ndescriptor->length = frame->size;\r\ndescriptor->eof = frame->eof;\r\ndescriptor->sof = frame->sof;\r\n}\r\nring->head = (ring->head + 1) % ring->size;\r\nring_iowrite16desc(ring, ring->head, ring->is_tx ? 10 : 8);\r\n}\r\n}\r\nstatic void ring_work(struct work_struct *work)\r\n{\r\nstruct tb_ring *ring = container_of(work, typeof(*ring), work);\r\nstruct ring_frame *frame;\r\nbool canceled = false;\r\nLIST_HEAD(done);\r\nmutex_lock(&ring->lock);\r\nif (!ring->running) {\r\nlist_splice_tail_init(&ring->in_flight, &done);\r\nlist_splice_tail_init(&ring->queue, &done);\r\ncanceled = true;\r\ngoto invoke_callback;\r\n}\r\nwhile (!ring_empty(ring)) {\r\nif (!(ring->descriptors[ring->tail].flags\r\n& RING_DESC_COMPLETED))\r\nbreak;\r\nframe = list_first_entry(&ring->in_flight, typeof(*frame),\r\nlist);\r\nlist_move_tail(&frame->list, &done);\r\nif (!ring->is_tx) {\r\nframe->size = ring->descriptors[ring->tail].length;\r\nframe->eof = ring->descriptors[ring->tail].eof;\r\nframe->sof = ring->descriptors[ring->tail].sof;\r\nframe->flags = ring->descriptors[ring->tail].flags;\r\nif (frame->sof != 0)\r\ndev_WARN(&ring->nhi->pdev->dev,\r\n"%s %d got unexpected SOF: %#x\n",\r\nRING_TYPE(ring), ring->hop,\r\nframe->sof);\r\nif (frame->flags != 0xa)\r\ndev_WARN(&ring->nhi->pdev->dev,\r\n"%s %d got unexpected flags: %#x\n",\r\nRING_TYPE(ring), ring->hop,\r\nframe->flags);\r\n}\r\nring->tail = (ring->tail + 1) % ring->size;\r\n}\r\nring_write_descriptors(ring);\r\ninvoke_callback:\r\nmutex_unlock(&ring->lock);\r\nwhile (!list_empty(&done)) {\r\nframe = list_first_entry(&done, typeof(*frame), list);\r\nlist_del_init(&frame->list);\r\nframe->callback(ring, frame, canceled);\r\n}\r\n}\r\nint __ring_enqueue(struct tb_ring *ring, struct ring_frame *frame)\r\n{\r\nint ret = 0;\r\nmutex_lock(&ring->lock);\r\nif (ring->running) {\r\nlist_add_tail(&frame->list, &ring->queue);\r\nring_write_descriptors(ring);\r\n} else {\r\nret = -ESHUTDOWN;\r\n}\r\nmutex_unlock(&ring->lock);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t ring_msix(int irq, void *data)\r\n{\r\nstruct tb_ring *ring = data;\r\nschedule_work(&ring->work);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int ring_request_msix(struct tb_ring *ring, bool no_suspend)\r\n{\r\nstruct tb_nhi *nhi = ring->nhi;\r\nunsigned long irqflags;\r\nint ret;\r\nif (!nhi->pdev->msix_enabled)\r\nreturn 0;\r\nret = ida_simple_get(&nhi->msix_ida, 0, MSIX_MAX_VECS, GFP_KERNEL);\r\nif (ret < 0)\r\nreturn ret;\r\nring->vector = ret;\r\nring->irq = pci_irq_vector(ring->nhi->pdev, ring->vector);\r\nif (ring->irq < 0)\r\nreturn ring->irq;\r\nirqflags = no_suspend ? IRQF_NO_SUSPEND : 0;\r\nreturn request_irq(ring->irq, ring_msix, irqflags, "thunderbolt", ring);\r\n}\r\nstatic void ring_release_msix(struct tb_ring *ring)\r\n{\r\nif (ring->irq <= 0)\r\nreturn;\r\nfree_irq(ring->irq, ring);\r\nida_simple_remove(&ring->nhi->msix_ida, ring->vector);\r\nring->vector = 0;\r\nring->irq = 0;\r\n}\r\nstatic struct tb_ring *ring_alloc(struct tb_nhi *nhi, u32 hop, int size,\r\nbool transmit, unsigned int flags)\r\n{\r\nstruct tb_ring *ring = NULL;\r\ndev_info(&nhi->pdev->dev, "allocating %s ring %d of size %d\n",\r\ntransmit ? "TX" : "RX", hop, size);\r\nmutex_lock(&nhi->lock);\r\nif (hop >= nhi->hop_count) {\r\ndev_WARN(&nhi->pdev->dev, "invalid hop: %d\n", hop);\r\ngoto err;\r\n}\r\nif (transmit && nhi->tx_rings[hop]) {\r\ndev_WARN(&nhi->pdev->dev, "TX hop %d already allocated\n", hop);\r\ngoto err;\r\n} else if (!transmit && nhi->rx_rings[hop]) {\r\ndev_WARN(&nhi->pdev->dev, "RX hop %d already allocated\n", hop);\r\ngoto err;\r\n}\r\nring = kzalloc(sizeof(*ring), GFP_KERNEL);\r\nif (!ring)\r\ngoto err;\r\nmutex_init(&ring->lock);\r\nINIT_LIST_HEAD(&ring->queue);\r\nINIT_LIST_HEAD(&ring->in_flight);\r\nINIT_WORK(&ring->work, ring_work);\r\nring->nhi = nhi;\r\nring->hop = hop;\r\nring->is_tx = transmit;\r\nring->size = size;\r\nring->flags = flags;\r\nring->head = 0;\r\nring->tail = 0;\r\nring->running = false;\r\nif (ring_request_msix(ring, flags & RING_FLAG_NO_SUSPEND))\r\ngoto err;\r\nring->descriptors = dma_alloc_coherent(&ring->nhi->pdev->dev,\r\nsize * sizeof(*ring->descriptors),\r\n&ring->descriptors_dma, GFP_KERNEL | __GFP_ZERO);\r\nif (!ring->descriptors)\r\ngoto err;\r\nif (transmit)\r\nnhi->tx_rings[hop] = ring;\r\nelse\r\nnhi->rx_rings[hop] = ring;\r\nmutex_unlock(&nhi->lock);\r\nreturn ring;\r\nerr:\r\nif (ring)\r\nmutex_destroy(&ring->lock);\r\nkfree(ring);\r\nmutex_unlock(&nhi->lock);\r\nreturn NULL;\r\n}\r\nstruct tb_ring *ring_alloc_tx(struct tb_nhi *nhi, int hop, int size,\r\nunsigned int flags)\r\n{\r\nreturn ring_alloc(nhi, hop, size, true, flags);\r\n}\r\nstruct tb_ring *ring_alloc_rx(struct tb_nhi *nhi, int hop, int size,\r\nunsigned int flags)\r\n{\r\nreturn ring_alloc(nhi, hop, size, false, flags);\r\n}\r\nvoid ring_start(struct tb_ring *ring)\r\n{\r\nmutex_lock(&ring->nhi->lock);\r\nmutex_lock(&ring->lock);\r\nif (ring->nhi->going_away)\r\ngoto err;\r\nif (ring->running) {\r\ndev_WARN(&ring->nhi->pdev->dev, "ring already started\n");\r\ngoto err;\r\n}\r\ndev_info(&ring->nhi->pdev->dev, "starting %s %d\n",\r\nRING_TYPE(ring), ring->hop);\r\nring_iowrite64desc(ring, ring->descriptors_dma, 0);\r\nif (ring->is_tx) {\r\nring_iowrite32desc(ring, ring->size, 12);\r\nring_iowrite32options(ring, 0, 4);\r\nring_iowrite32options(ring,\r\nRING_FLAG_ENABLE | RING_FLAG_RAW, 0);\r\n} else {\r\nring_iowrite32desc(ring,\r\n(TB_FRAME_SIZE << 16) | ring->size, 12);\r\nring_iowrite32options(ring, 0xffffffff, 4);\r\nring_iowrite32options(ring,\r\nRING_FLAG_ENABLE | RING_FLAG_RAW, 0);\r\n}\r\nring_interrupt_active(ring, true);\r\nring->running = true;\r\nerr:\r\nmutex_unlock(&ring->lock);\r\nmutex_unlock(&ring->nhi->lock);\r\n}\r\nvoid ring_stop(struct tb_ring *ring)\r\n{\r\nmutex_lock(&ring->nhi->lock);\r\nmutex_lock(&ring->lock);\r\ndev_info(&ring->nhi->pdev->dev, "stopping %s %d\n",\r\nRING_TYPE(ring), ring->hop);\r\nif (ring->nhi->going_away)\r\ngoto err;\r\nif (!ring->running) {\r\ndev_WARN(&ring->nhi->pdev->dev, "%s %d already stopped\n",\r\nRING_TYPE(ring), ring->hop);\r\ngoto err;\r\n}\r\nring_interrupt_active(ring, false);\r\nring_iowrite32options(ring, 0, 0);\r\nring_iowrite64desc(ring, 0, 0);\r\nring_iowrite16desc(ring, 0, ring->is_tx ? 10 : 8);\r\nring_iowrite32desc(ring, 0, 12);\r\nring->head = 0;\r\nring->tail = 0;\r\nring->running = false;\r\nerr:\r\nmutex_unlock(&ring->lock);\r\nmutex_unlock(&ring->nhi->lock);\r\nschedule_work(&ring->work);\r\nflush_work(&ring->work);\r\n}\r\nvoid ring_free(struct tb_ring *ring)\r\n{\r\nmutex_lock(&ring->nhi->lock);\r\nif (ring->is_tx)\r\nring->nhi->tx_rings[ring->hop] = NULL;\r\nelse\r\nring->nhi->rx_rings[ring->hop] = NULL;\r\nif (ring->running) {\r\ndev_WARN(&ring->nhi->pdev->dev, "%s %d still running\n",\r\nRING_TYPE(ring), ring->hop);\r\n}\r\nring_release_msix(ring);\r\ndma_free_coherent(&ring->nhi->pdev->dev,\r\nring->size * sizeof(*ring->descriptors),\r\nring->descriptors, ring->descriptors_dma);\r\nring->descriptors = NULL;\r\nring->descriptors_dma = 0;\r\ndev_info(&ring->nhi->pdev->dev,\r\n"freeing %s %d\n",\r\nRING_TYPE(ring),\r\nring->hop);\r\nmutex_unlock(&ring->nhi->lock);\r\nflush_work(&ring->work);\r\nmutex_destroy(&ring->lock);\r\nkfree(ring);\r\n}\r\nint nhi_mailbox_cmd(struct tb_nhi *nhi, enum nhi_mailbox_cmd cmd, u32 data)\r\n{\r\nktime_t timeout;\r\nu32 val;\r\niowrite32(data, nhi->iobase + REG_INMAIL_DATA);\r\nval = ioread32(nhi->iobase + REG_INMAIL_CMD);\r\nval &= ~(REG_INMAIL_CMD_MASK | REG_INMAIL_ERROR);\r\nval |= REG_INMAIL_OP_REQUEST | cmd;\r\niowrite32(val, nhi->iobase + REG_INMAIL_CMD);\r\ntimeout = ktime_add_ms(ktime_get(), NHI_MAILBOX_TIMEOUT);\r\ndo {\r\nval = ioread32(nhi->iobase + REG_INMAIL_CMD);\r\nif (!(val & REG_INMAIL_OP_REQUEST))\r\nbreak;\r\nusleep_range(10, 20);\r\n} while (ktime_before(ktime_get(), timeout));\r\nif (val & REG_INMAIL_OP_REQUEST)\r\nreturn -ETIMEDOUT;\r\nif (val & REG_INMAIL_ERROR)\r\nreturn -EIO;\r\nreturn 0;\r\n}\r\nenum nhi_fw_mode nhi_mailbox_mode(struct tb_nhi *nhi)\r\n{\r\nu32 val;\r\nval = ioread32(nhi->iobase + REG_OUTMAIL_CMD);\r\nval &= REG_OUTMAIL_CMD_OPMODE_MASK;\r\nval >>= REG_OUTMAIL_CMD_OPMODE_SHIFT;\r\nreturn (enum nhi_fw_mode)val;\r\n}\r\nstatic void nhi_interrupt_work(struct work_struct *work)\r\n{\r\nstruct tb_nhi *nhi = container_of(work, typeof(*nhi), interrupt_work);\r\nint value = 0;\r\nint bit;\r\nint hop = -1;\r\nint type = 0;\r\nstruct tb_ring *ring;\r\nmutex_lock(&nhi->lock);\r\nfor (bit = 0; bit < 3 * nhi->hop_count; bit++) {\r\nif (bit % 32 == 0)\r\nvalue = ioread32(nhi->iobase\r\n+ REG_RING_NOTIFY_BASE\r\n+ 4 * (bit / 32));\r\nif (++hop == nhi->hop_count) {\r\nhop = 0;\r\ntype++;\r\n}\r\nif ((value & (1 << (bit % 32))) == 0)\r\ncontinue;\r\nif (type == 2) {\r\ndev_warn(&nhi->pdev->dev,\r\n"RX overflow for ring %d\n",\r\nhop);\r\ncontinue;\r\n}\r\nif (type == 0)\r\nring = nhi->tx_rings[hop];\r\nelse\r\nring = nhi->rx_rings[hop];\r\nif (ring == NULL) {\r\ndev_warn(&nhi->pdev->dev,\r\n"got interrupt for inactive %s ring %d\n",\r\ntype ? "RX" : "TX",\r\nhop);\r\ncontinue;\r\n}\r\nschedule_work(&ring->work);\r\n}\r\nmutex_unlock(&nhi->lock);\r\n}\r\nstatic irqreturn_t nhi_msi(int irq, void *data)\r\n{\r\nstruct tb_nhi *nhi = data;\r\nschedule_work(&nhi->interrupt_work);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int nhi_suspend_noirq(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct tb *tb = pci_get_drvdata(pdev);\r\nreturn tb_domain_suspend_noirq(tb);\r\n}\r\nstatic int nhi_resume_noirq(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct tb *tb = pci_get_drvdata(pdev);\r\nif (!pci_device_is_present(pdev))\r\ntb->nhi->going_away = true;\r\nreturn tb_domain_resume_noirq(tb);\r\n}\r\nstatic int nhi_suspend(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct tb *tb = pci_get_drvdata(pdev);\r\nreturn tb_domain_suspend(tb);\r\n}\r\nstatic void nhi_complete(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct tb *tb = pci_get_drvdata(pdev);\r\ntb_domain_complete(tb);\r\n}\r\nstatic void nhi_shutdown(struct tb_nhi *nhi)\r\n{\r\nint i;\r\ndev_info(&nhi->pdev->dev, "shutdown\n");\r\nfor (i = 0; i < nhi->hop_count; i++) {\r\nif (nhi->tx_rings[i])\r\ndev_WARN(&nhi->pdev->dev,\r\n"TX ring %d is still active\n", i);\r\nif (nhi->rx_rings[i])\r\ndev_WARN(&nhi->pdev->dev,\r\n"RX ring %d is still active\n", i);\r\n}\r\nnhi_disable_interrupts(nhi);\r\nif (!nhi->pdev->msix_enabled) {\r\ndevm_free_irq(&nhi->pdev->dev, nhi->pdev->irq, nhi);\r\nflush_work(&nhi->interrupt_work);\r\n}\r\nmutex_destroy(&nhi->lock);\r\nida_destroy(&nhi->msix_ida);\r\n}\r\nstatic int nhi_init_msi(struct tb_nhi *nhi)\r\n{\r\nstruct pci_dev *pdev = nhi->pdev;\r\nint res, irq, nvec;\r\nnhi_disable_interrupts(nhi);\r\nida_init(&nhi->msix_ida);\r\nnvec = pci_alloc_irq_vectors(pdev, MSIX_MIN_VECS, MSIX_MAX_VECS,\r\nPCI_IRQ_MSIX);\r\nif (nvec < 0) {\r\nnvec = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSI);\r\nif (nvec < 0)\r\nreturn nvec;\r\nINIT_WORK(&nhi->interrupt_work, nhi_interrupt_work);\r\nirq = pci_irq_vector(nhi->pdev, 0);\r\nif (irq < 0)\r\nreturn irq;\r\nres = devm_request_irq(&pdev->dev, irq, nhi_msi,\r\nIRQF_NO_SUSPEND, "thunderbolt", nhi);\r\nif (res) {\r\ndev_err(&pdev->dev, "request_irq failed, aborting\n");\r\nreturn res;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int nhi_probe(struct pci_dev *pdev, const struct pci_device_id *id)\r\n{\r\nstruct tb_nhi *nhi;\r\nstruct tb *tb;\r\nint res;\r\nres = pcim_enable_device(pdev);\r\nif (res) {\r\ndev_err(&pdev->dev, "cannot enable PCI device, aborting\n");\r\nreturn res;\r\n}\r\nres = pcim_iomap_regions(pdev, 1 << 0, "thunderbolt");\r\nif (res) {\r\ndev_err(&pdev->dev, "cannot obtain PCI resources, aborting\n");\r\nreturn res;\r\n}\r\nnhi = devm_kzalloc(&pdev->dev, sizeof(*nhi), GFP_KERNEL);\r\nif (!nhi)\r\nreturn -ENOMEM;\r\nnhi->pdev = pdev;\r\nnhi->iobase = pcim_iomap_table(pdev)[0];\r\nnhi->hop_count = ioread32(nhi->iobase + REG_HOP_COUNT) & 0x3ff;\r\nif (nhi->hop_count != 12 && nhi->hop_count != 32)\r\ndev_warn(&pdev->dev, "unexpected hop count: %d\n",\r\nnhi->hop_count);\r\nnhi->tx_rings = devm_kcalloc(&pdev->dev, nhi->hop_count,\r\nsizeof(*nhi->tx_rings), GFP_KERNEL);\r\nnhi->rx_rings = devm_kcalloc(&pdev->dev, nhi->hop_count,\r\nsizeof(*nhi->rx_rings), GFP_KERNEL);\r\nif (!nhi->tx_rings || !nhi->rx_rings)\r\nreturn -ENOMEM;\r\nres = nhi_init_msi(nhi);\r\nif (res) {\r\ndev_err(&pdev->dev, "cannot enable MSI, aborting\n");\r\nreturn res;\r\n}\r\nmutex_init(&nhi->lock);\r\npci_set_master(pdev);\r\niowrite32(3906250 / 10000, nhi->iobase + 0x38c00);\r\ntb = icm_probe(nhi);\r\nif (!tb)\r\ntb = tb_probe(nhi);\r\nif (!tb) {\r\ndev_err(&nhi->pdev->dev,\r\n"failed to determine connection manager, aborting\n");\r\nreturn -ENODEV;\r\n}\r\ndev_info(&nhi->pdev->dev, "NHI initialized, starting thunderbolt\n");\r\nres = tb_domain_add(tb);\r\nif (res) {\r\ntb_domain_put(tb);\r\nnhi_shutdown(nhi);\r\nreturn -EIO;\r\n}\r\npci_set_drvdata(pdev, tb);\r\nreturn 0;\r\n}\r\nstatic void nhi_remove(struct pci_dev *pdev)\r\n{\r\nstruct tb *tb = pci_get_drvdata(pdev);\r\nstruct tb_nhi *nhi = tb->nhi;\r\ntb_domain_remove(tb);\r\nnhi_shutdown(nhi);\r\n}\r\nstatic int __init nhi_init(void)\r\n{\r\nint ret;\r\nret = tb_domain_init();\r\nif (ret)\r\nreturn ret;\r\nret = pci_register_driver(&nhi_driver);\r\nif (ret)\r\ntb_domain_exit();\r\nreturn ret;\r\n}\r\nstatic void __exit nhi_unload(void)\r\n{\r\npci_unregister_driver(&nhi_driver);\r\ntb_domain_exit();\r\n}
