void cx18_buf_swap(struct cx18_buffer *buf)\r\n{\r\nint i;\r\nfor (i = 0; i < buf->bytesused; i += 4)\r\nswab32s((u32 *)(buf->buf + i));\r\n}\r\nvoid _cx18_mdl_swap(struct cx18_mdl *mdl)\r\n{\r\nstruct cx18_buffer *buf;\r\nlist_for_each_entry(buf, &mdl->buf_list, list) {\r\nif (buf->bytesused == 0)\r\nbreak;\r\ncx18_buf_swap(buf);\r\n}\r\n}\r\nvoid cx18_queue_init(struct cx18_queue *q)\r\n{\r\nINIT_LIST_HEAD(&q->list);\r\natomic_set(&q->depth, 0);\r\nq->bytesused = 0;\r\n}\r\nstruct cx18_queue *_cx18_enqueue(struct cx18_stream *s, struct cx18_mdl *mdl,\r\nstruct cx18_queue *q, int to_front)\r\n{\r\nif (q != &s->q_full) {\r\nmdl->bytesused = 0;\r\nmdl->readpos = 0;\r\nmdl->m_flags = 0;\r\nmdl->skipped = 0;\r\nmdl->curr_buf = NULL;\r\n}\r\nif (q == &s->q_busy &&\r\natomic_read(&q->depth) >= CX18_MAX_FW_MDLS_PER_STREAM)\r\nq = &s->q_free;\r\nspin_lock(&q->lock);\r\nif (to_front)\r\nlist_add(&mdl->list, &q->list);\r\nelse\r\nlist_add_tail(&mdl->list, &q->list);\r\nq->bytesused += mdl->bytesused - mdl->readpos;\r\natomic_inc(&q->depth);\r\nspin_unlock(&q->lock);\r\nreturn q;\r\n}\r\nstruct cx18_mdl *cx18_dequeue(struct cx18_stream *s, struct cx18_queue *q)\r\n{\r\nstruct cx18_mdl *mdl = NULL;\r\nspin_lock(&q->lock);\r\nif (!list_empty(&q->list)) {\r\nmdl = list_first_entry(&q->list, struct cx18_mdl, list);\r\nlist_del_init(&mdl->list);\r\nq->bytesused -= mdl->bytesused - mdl->readpos;\r\nmdl->skipped = 0;\r\natomic_dec(&q->depth);\r\n}\r\nspin_unlock(&q->lock);\r\nreturn mdl;\r\n}\r\nstatic void _cx18_mdl_update_bufs_for_cpu(struct cx18_stream *s,\r\nstruct cx18_mdl *mdl)\r\n{\r\nstruct cx18_buffer *buf;\r\nu32 buf_size = s->buf_size;\r\nu32 bytesused = mdl->bytesused;\r\nlist_for_each_entry(buf, &mdl->buf_list, list) {\r\nbuf->readpos = 0;\r\nif (bytesused >= buf_size) {\r\nbuf->bytesused = buf_size;\r\nbytesused -= buf_size;\r\n} else {\r\nbuf->bytesused = bytesused;\r\nbytesused = 0;\r\n}\r\ncx18_buf_sync_for_cpu(s, buf);\r\n}\r\n}\r\nstatic inline void cx18_mdl_update_bufs_for_cpu(struct cx18_stream *s,\r\nstruct cx18_mdl *mdl)\r\n{\r\nstruct cx18_buffer *buf;\r\nif (list_is_singular(&mdl->buf_list)) {\r\nbuf = list_first_entry(&mdl->buf_list, struct cx18_buffer,\r\nlist);\r\nbuf->bytesused = mdl->bytesused;\r\nbuf->readpos = 0;\r\ncx18_buf_sync_for_cpu(s, buf);\r\n} else {\r\n_cx18_mdl_update_bufs_for_cpu(s, mdl);\r\n}\r\n}\r\nstruct cx18_mdl *cx18_queue_get_mdl(struct cx18_stream *s, u32 id,\r\nu32 bytesused)\r\n{\r\nstruct cx18 *cx = s->cx;\r\nstruct cx18_mdl *mdl;\r\nstruct cx18_mdl *tmp;\r\nstruct cx18_mdl *ret = NULL;\r\nLIST_HEAD(sweep_up);\r\nspin_lock(&s->q_busy.lock);\r\nlist_for_each_entry_safe(mdl, tmp, &s->q_busy.list, list) {\r\nif (mdl->id != id) {\r\nmdl->skipped++;\r\nif (mdl->skipped >= atomic_read(&s->q_busy.depth)-1) {\r\nCX18_WARN("Skipped %s, MDL %d, %d times - it must have dropped out of rotation\n",\r\ns->name, mdl->id,\r\nmdl->skipped);\r\nlist_move_tail(&mdl->list, &sweep_up);\r\natomic_dec(&s->q_busy.depth);\r\n}\r\ncontinue;\r\n}\r\nlist_del_init(&mdl->list);\r\natomic_dec(&s->q_busy.depth);\r\nret = mdl;\r\nbreak;\r\n}\r\nspin_unlock(&s->q_busy.lock);\r\nif (ret != NULL) {\r\nret->bytesused = bytesused;\r\nret->skipped = 0;\r\ncx18_mdl_update_bufs_for_cpu(s, ret);\r\nif (s->type != CX18_ENC_STREAM_TYPE_TS)\r\nset_bit(CX18_F_M_NEED_SWAP, &ret->m_flags);\r\n}\r\nlist_for_each_entry_safe(mdl, tmp, &sweep_up, list) {\r\nlist_del_init(&mdl->list);\r\ncx18_enqueue(s, mdl, &s->q_free);\r\n}\r\nreturn ret;\r\n}\r\nstatic void cx18_queue_flush(struct cx18_stream *s,\r\nstruct cx18_queue *q_src, struct cx18_queue *q_dst)\r\n{\r\nstruct cx18_mdl *mdl;\r\nif (q_src == q_dst || q_dst == &s->q_full || q_dst == &s->q_busy)\r\nreturn;\r\nspin_lock(&q_src->lock);\r\nspin_lock(&q_dst->lock);\r\nwhile (!list_empty(&q_src->list)) {\r\nmdl = list_first_entry(&q_src->list, struct cx18_mdl, list);\r\nlist_move_tail(&mdl->list, &q_dst->list);\r\nmdl->bytesused = 0;\r\nmdl->readpos = 0;\r\nmdl->m_flags = 0;\r\nmdl->skipped = 0;\r\nmdl->curr_buf = NULL;\r\natomic_inc(&q_dst->depth);\r\n}\r\ncx18_queue_init(q_src);\r\nspin_unlock(&q_src->lock);\r\nspin_unlock(&q_dst->lock);\r\n}\r\nvoid cx18_flush_queues(struct cx18_stream *s)\r\n{\r\ncx18_queue_flush(s, &s->q_busy, &s->q_free);\r\ncx18_queue_flush(s, &s->q_full, &s->q_free);\r\n}\r\nvoid cx18_unload_queues(struct cx18_stream *s)\r\n{\r\nstruct cx18_queue *q_idle = &s->q_idle;\r\nstruct cx18_mdl *mdl;\r\nstruct cx18_buffer *buf;\r\ncx18_queue_flush(s, &s->q_busy, q_idle);\r\ncx18_queue_flush(s, &s->q_full, q_idle);\r\ncx18_queue_flush(s, &s->q_free, q_idle);\r\nspin_lock(&q_idle->lock);\r\nlist_for_each_entry(mdl, &q_idle->list, list) {\r\nwhile (!list_empty(&mdl->buf_list)) {\r\nbuf = list_first_entry(&mdl->buf_list,\r\nstruct cx18_buffer, list);\r\nlist_move_tail(&buf->list, &s->buf_pool);\r\nbuf->bytesused = 0;\r\nbuf->readpos = 0;\r\n}\r\nmdl->id = s->mdl_base_idx;\r\n}\r\nspin_unlock(&q_idle->lock);\r\n}\r\nvoid cx18_load_queues(struct cx18_stream *s)\r\n{\r\nstruct cx18 *cx = s->cx;\r\nstruct cx18_mdl *mdl;\r\nstruct cx18_buffer *buf;\r\nint mdl_id;\r\nint i;\r\nu32 partial_buf_size;\r\nmdl_id = s->mdl_base_idx;\r\nfor (mdl = cx18_dequeue(s, &s->q_idle), i = s->bufs_per_mdl;\r\nmdl != NULL && i == s->bufs_per_mdl;\r\nmdl = cx18_dequeue(s, &s->q_idle)) {\r\nmdl->id = mdl_id;\r\nfor (i = 0; i < s->bufs_per_mdl; i++) {\r\nif (list_empty(&s->buf_pool))\r\nbreak;\r\nbuf = list_first_entry(&s->buf_pool, struct cx18_buffer,\r\nlist);\r\nlist_move_tail(&buf->list, &mdl->buf_list);\r\ncx18_writel(cx, buf->dma_handle,\r\n&cx->scb->cpu_mdl[mdl_id + i].paddr);\r\ncx18_writel(cx, s->buf_size,\r\n&cx->scb->cpu_mdl[mdl_id + i].length);\r\n}\r\nif (i == s->bufs_per_mdl) {\r\npartial_buf_size = s->mdl_size % s->buf_size;\r\nif (partial_buf_size) {\r\ncx18_writel(cx, partial_buf_size,\r\n&cx->scb->cpu_mdl[mdl_id + i - 1].length);\r\n}\r\ncx18_enqueue(s, mdl, &s->q_free);\r\n} else {\r\ncx18_push(s, mdl, &s->q_idle);\r\n}\r\nmdl_id += i;\r\n}\r\n}\r\nvoid _cx18_mdl_sync_for_device(struct cx18_stream *s, struct cx18_mdl *mdl)\r\n{\r\nint dma = s->dma;\r\nu32 buf_size = s->buf_size;\r\nstruct pci_dev *pci_dev = s->cx->pci_dev;\r\nstruct cx18_buffer *buf;\r\nlist_for_each_entry(buf, &mdl->buf_list, list)\r\npci_dma_sync_single_for_device(pci_dev, buf->dma_handle,\r\nbuf_size, dma);\r\n}\r\nint cx18_stream_alloc(struct cx18_stream *s)\r\n{\r\nstruct cx18 *cx = s->cx;\r\nint i;\r\nif (s->buffers == 0)\r\nreturn 0;\r\nCX18_DEBUG_INFO("Allocate %s stream: %d x %d buffers (%d.%02d kB total)\n",\r\ns->name, s->buffers, s->buf_size,\r\ns->buffers * s->buf_size / 1024,\r\n(s->buffers * s->buf_size * 100 / 1024) % 100);\r\nif (((char __iomem *)&cx->scb->cpu_mdl[cx->free_mdl_idx + s->buffers] -\r\n(char __iomem *)cx->scb) > SCB_RESERVED_SIZE) {\r\nunsigned bufsz = (((char __iomem *)cx->scb) + SCB_RESERVED_SIZE -\r\n((char __iomem *)cx->scb->cpu_mdl));\r\nCX18_ERR("Too many buffers, cannot fit in SCB area\n");\r\nCX18_ERR("Max buffers = %zu\n",\r\nbufsz / sizeof(struct cx18_mdl_ent));\r\nreturn -ENOMEM;\r\n}\r\ns->mdl_base_idx = cx->free_mdl_idx;\r\nfor (i = 0; i < s->buffers; i++) {\r\nstruct cx18_mdl *mdl;\r\nstruct cx18_buffer *buf;\r\nmdl = kzalloc(sizeof(struct cx18_mdl), GFP_KERNEL|__GFP_NOWARN);\r\nif (mdl == NULL)\r\nbreak;\r\nbuf = kzalloc(sizeof(struct cx18_buffer),\r\nGFP_KERNEL|__GFP_NOWARN);\r\nif (buf == NULL) {\r\nkfree(mdl);\r\nbreak;\r\n}\r\nbuf->buf = kmalloc(s->buf_size, GFP_KERNEL|__GFP_NOWARN);\r\nif (buf->buf == NULL) {\r\nkfree(mdl);\r\nkfree(buf);\r\nbreak;\r\n}\r\nINIT_LIST_HEAD(&mdl->list);\r\nINIT_LIST_HEAD(&mdl->buf_list);\r\nmdl->id = s->mdl_base_idx;\r\ncx18_enqueue(s, mdl, &s->q_idle);\r\nINIT_LIST_HEAD(&buf->list);\r\nbuf->dma_handle = pci_map_single(s->cx->pci_dev,\r\nbuf->buf, s->buf_size, s->dma);\r\ncx18_buf_sync_for_cpu(s, buf);\r\nlist_add_tail(&buf->list, &s->buf_pool);\r\n}\r\nif (i == s->buffers) {\r\ncx->free_mdl_idx += s->buffers;\r\nreturn 0;\r\n}\r\nCX18_ERR("Couldn't allocate buffers for %s stream\n", s->name);\r\ncx18_stream_free(s);\r\nreturn -ENOMEM;\r\n}\r\nvoid cx18_stream_free(struct cx18_stream *s)\r\n{\r\nstruct cx18_mdl *mdl;\r\nstruct cx18_buffer *buf;\r\nstruct cx18 *cx = s->cx;\r\nCX18_DEBUG_INFO("Deallocating buffers for %s stream\n", s->name);\r\ncx18_unload_queues(s);\r\nwhile ((mdl = cx18_dequeue(s, &s->q_idle)))\r\nkfree(mdl);\r\nwhile (!list_empty(&s->buf_pool)) {\r\nbuf = list_first_entry(&s->buf_pool, struct cx18_buffer, list);\r\nlist_del_init(&buf->list);\r\npci_unmap_single(s->cx->pci_dev, buf->dma_handle,\r\ns->buf_size, s->dma);\r\nkfree(buf->buf);\r\nkfree(buf);\r\n}\r\n}
