static void etnaviv_core_dump_header(struct core_dump_iterator *iter,\r\nu32 type, void *data_end)\r\n{\r\nstruct etnaviv_dump_object_header *hdr = iter->hdr;\r\nhdr->magic = cpu_to_le32(ETDUMP_MAGIC);\r\nhdr->type = cpu_to_le32(type);\r\nhdr->file_offset = cpu_to_le32(iter->data - iter->start);\r\nhdr->file_size = cpu_to_le32(data_end - iter->data);\r\niter->hdr++;\r\niter->data += hdr->file_size;\r\n}\r\nstatic void etnaviv_core_dump_registers(struct core_dump_iterator *iter,\r\nstruct etnaviv_gpu *gpu)\r\n{\r\nstruct etnaviv_dump_registers *reg = iter->data;\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(etnaviv_dump_registers); i++, reg++) {\r\nreg->reg = etnaviv_dump_registers[i];\r\nreg->value = gpu_read(gpu, etnaviv_dump_registers[i]);\r\n}\r\netnaviv_core_dump_header(iter, ETDUMP_BUF_REG, reg);\r\n}\r\nstatic void etnaviv_core_dump_mmu(struct core_dump_iterator *iter,\r\nstruct etnaviv_gpu *gpu, size_t mmu_size)\r\n{\r\netnaviv_iommu_dump(gpu->mmu, iter->data);\r\netnaviv_core_dump_header(iter, ETDUMP_BUF_MMU, iter->data + mmu_size);\r\n}\r\nstatic void etnaviv_core_dump_mem(struct core_dump_iterator *iter, u32 type,\r\nvoid *ptr, size_t size, u64 iova)\r\n{\r\nmemcpy(iter->data, ptr, size);\r\niter->hdr->iova = cpu_to_le64(iova);\r\netnaviv_core_dump_header(iter, type, iter->data + size);\r\n}\r\nvoid etnaviv_core_dump(struct etnaviv_gpu *gpu)\r\n{\r\nstruct core_dump_iterator iter;\r\nstruct etnaviv_vram_mapping *vram;\r\nstruct etnaviv_gem_object *obj;\r\nstruct etnaviv_cmdbuf *cmd;\r\nunsigned int n_obj, n_bomap_pages;\r\nsize_t file_size, mmu_size;\r\n__le64 *bomap, *bomap_start;\r\nmmu_size = etnaviv_iommu_dump_size(gpu->mmu);\r\nn_obj = 4;\r\nn_bomap_pages = 0;\r\nfile_size = ARRAY_SIZE(etnaviv_dump_registers) *\r\nsizeof(struct etnaviv_dump_registers) +\r\nmmu_size + gpu->buffer->size;\r\nlist_for_each_entry(cmd, &gpu->active_cmd_list, node) {\r\nfile_size += cmd->size;\r\nn_obj++;\r\n}\r\nlist_for_each_entry(vram, &gpu->mmu->mappings, mmu_node) {\r\nif (!vram->use)\r\ncontinue;\r\nobj = vram->object;\r\nfile_size += obj->base.size;\r\nn_bomap_pages += obj->base.size >> PAGE_SHIFT;\r\nn_obj++;\r\n}\r\nif (n_bomap_pages) {\r\nfile_size += n_bomap_pages * sizeof(__le64);\r\nn_obj++;\r\n}\r\nfile_size += sizeof(*iter.hdr) * n_obj;\r\niter.start = __vmalloc(file_size, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY,\r\nPAGE_KERNEL);\r\nif (!iter.start) {\r\ndev_warn(gpu->dev, "failed to allocate devcoredump file\n");\r\nreturn;\r\n}\r\niter.hdr = iter.start;\r\niter.data = &iter.hdr[n_obj];\r\nmemset(iter.hdr, 0, iter.data - iter.start);\r\netnaviv_core_dump_registers(&iter, gpu);\r\netnaviv_core_dump_mmu(&iter, gpu, mmu_size);\r\netnaviv_core_dump_mem(&iter, ETDUMP_BUF_RING, gpu->buffer->vaddr,\r\ngpu->buffer->size,\r\netnaviv_cmdbuf_get_va(gpu->buffer));\r\nlist_for_each_entry(cmd, &gpu->active_cmd_list, node)\r\netnaviv_core_dump_mem(&iter, ETDUMP_BUF_CMD, cmd->vaddr,\r\ncmd->size, etnaviv_cmdbuf_get_va(cmd));\r\nif (n_bomap_pages) {\r\nbomap_start = bomap = iter.data;\r\nmemset(bomap, 0, sizeof(*bomap) * n_bomap_pages);\r\netnaviv_core_dump_header(&iter, ETDUMP_BUF_BOMAP,\r\nbomap + n_bomap_pages);\r\n} else {\r\nbomap_start = bomap = NULL;\r\n}\r\nlist_for_each_entry(vram, &gpu->mmu->mappings, mmu_node) {\r\nstruct page **pages;\r\nvoid *vaddr;\r\nif (vram->use == 0)\r\ncontinue;\r\nobj = vram->object;\r\nmutex_lock(&obj->lock);\r\npages = etnaviv_gem_get_pages(obj);\r\nmutex_unlock(&obj->lock);\r\nif (pages) {\r\nint j;\r\niter.hdr->data[0] = bomap - bomap_start;\r\nfor (j = 0; j < obj->base.size >> PAGE_SHIFT; j++)\r\n*bomap++ = cpu_to_le64(page_to_phys(*pages++));\r\n}\r\niter.hdr->iova = cpu_to_le64(vram->iova);\r\nvaddr = etnaviv_gem_vmap(&obj->base);\r\nif (vaddr)\r\nmemcpy(iter.data, vaddr, obj->base.size);\r\netnaviv_core_dump_header(&iter, ETDUMP_BUF_BO, iter.data +\r\nobj->base.size);\r\n}\r\netnaviv_core_dump_header(&iter, ETDUMP_BUF_END, iter.data);\r\ndev_coredumpv(gpu->dev, iter.start, iter.data - iter.start, GFP_KERNEL);\r\n}
