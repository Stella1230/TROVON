static void skl_cldma_int_enable(struct sst_dsp *ctx)\r\n{\r\nsst_dsp_shim_update_bits_unlocked(ctx, SKL_ADSP_REG_ADSPIC,\r\nSKL_ADSPIC_CL_DMA, SKL_ADSPIC_CL_DMA);\r\n}\r\nvoid skl_cldma_int_disable(struct sst_dsp *ctx)\r\n{\r\nsst_dsp_shim_update_bits_unlocked(ctx,\r\nSKL_ADSP_REG_ADSPIC, SKL_ADSPIC_CL_DMA, 0);\r\n}\r\nstatic void skl_cldma_stream_run(struct sst_dsp *ctx, bool enable)\r\n{\r\nunsigned char val;\r\nint timeout;\r\nsst_dsp_shim_update_bits_unlocked(ctx,\r\nSKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_RUN_MASK, CL_SD_CTL_RUN(enable));\r\nudelay(3);\r\ntimeout = 300;\r\ndo {\r\nval = sst_dsp_shim_read(ctx, SKL_ADSP_REG_CL_SD_CTL) &\r\nCL_SD_CTL_RUN_MASK;\r\nif (enable && val)\r\nbreak;\r\nelse if (!enable && !val)\r\nbreak;\r\nudelay(3);\r\n} while (--timeout);\r\nif (timeout == 0)\r\ndev_err(ctx->dev, "Failed to set Run bit=%d enable=%d\n", val, enable);\r\n}\r\nstatic void skl_cldma_stream_clear(struct sst_dsp *ctx)\r\n{\r\nskl_cldma_stream_run(ctx, 0);\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_IOCE_MASK, CL_SD_CTL_IOCE(0));\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_FEIE_MASK, CL_SD_CTL_FEIE(0));\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_DEIE_MASK, CL_SD_CTL_DEIE(0));\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_STRM_MASK, CL_SD_CTL_STRM(0));\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_BDLPL, CL_SD_BDLPLBA(0));\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_BDLPU, 0);\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_CBL, 0);\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_LVI, 0);\r\n}\r\nstatic void skl_cldma_setup_bdle(struct sst_dsp *ctx,\r\nstruct snd_dma_buffer *dmab_data,\r\nu32 **bdlp, int size, int with_ioc)\r\n{\r\nu32 *bdl = *bdlp;\r\nctx->cl_dev.frags = 0;\r\nwhile (size > 0) {\r\nphys_addr_t addr = virt_to_phys(dmab_data->area +\r\n(ctx->cl_dev.frags * ctx->cl_dev.bufsize));\r\nbdl[0] = cpu_to_le32(lower_32_bits(addr));\r\nbdl[1] = cpu_to_le32(upper_32_bits(addr));\r\nbdl[2] = cpu_to_le32(ctx->cl_dev.bufsize);\r\nsize -= ctx->cl_dev.bufsize;\r\nbdl[3] = (size || !with_ioc) ? 0 : cpu_to_le32(0x01);\r\nbdl += 4;\r\nctx->cl_dev.frags++;\r\n}\r\n}\r\nstatic void skl_cldma_setup_controller(struct sst_dsp *ctx,\r\nstruct snd_dma_buffer *dmab_bdl, unsigned int max_size,\r\nu32 count)\r\n{\r\nskl_cldma_stream_clear(ctx);\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_BDLPL,\r\nCL_SD_BDLPLBA(dmab_bdl->addr));\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_BDLPU,\r\nCL_SD_BDLPUBA(dmab_bdl->addr));\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_CBL, max_size);\r\nsst_dsp_shim_write(ctx, SKL_ADSP_REG_CL_SD_LVI, count - 1);\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_IOCE_MASK, CL_SD_CTL_IOCE(1));\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_FEIE_MASK, CL_SD_CTL_FEIE(1));\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_DEIE_MASK, CL_SD_CTL_DEIE(1));\r\nsst_dsp_shim_update_bits(ctx, SKL_ADSP_REG_CL_SD_CTL,\r\nCL_SD_CTL_STRM_MASK, CL_SD_CTL_STRM(FW_CL_STREAM_NUMBER));\r\n}\r\nstatic void skl_cldma_setup_spb(struct sst_dsp *ctx,\r\nunsigned int size, bool enable)\r\n{\r\nif (enable)\r\nsst_dsp_shim_update_bits_unlocked(ctx,\r\nSKL_ADSP_REG_CL_SPBFIFO_SPBFCCTL,\r\nCL_SPBFIFO_SPBFCCTL_SPIBE_MASK,\r\nCL_SPBFIFO_SPBFCCTL_SPIBE(1));\r\nsst_dsp_shim_write_unlocked(ctx, SKL_ADSP_REG_CL_SPBFIFO_SPIB, size);\r\n}\r\nstatic void skl_cldma_cleanup_spb(struct sst_dsp *ctx)\r\n{\r\nsst_dsp_shim_update_bits_unlocked(ctx,\r\nSKL_ADSP_REG_CL_SPBFIFO_SPBFCCTL,\r\nCL_SPBFIFO_SPBFCCTL_SPIBE_MASK,\r\nCL_SPBFIFO_SPBFCCTL_SPIBE(0));\r\nsst_dsp_shim_write_unlocked(ctx, SKL_ADSP_REG_CL_SPBFIFO_SPIB, 0);\r\n}\r\nstatic void skl_cldma_cleanup(struct sst_dsp *ctx)\r\n{\r\nskl_cldma_cleanup_spb(ctx);\r\nskl_cldma_stream_clear(ctx);\r\nctx->dsp_ops.free_dma_buf(ctx->dev, &ctx->cl_dev.dmab_data);\r\nctx->dsp_ops.free_dma_buf(ctx->dev, &ctx->cl_dev.dmab_bdl);\r\n}\r\nint skl_cldma_wait_interruptible(struct sst_dsp *ctx)\r\n{\r\nint ret = 0;\r\nif (!wait_event_timeout(ctx->cl_dev.wait_queue,\r\nctx->cl_dev.wait_condition,\r\nmsecs_to_jiffies(SKL_WAIT_TIMEOUT))) {\r\ndev_err(ctx->dev, "%s: Wait timeout\n", __func__);\r\nret = -EIO;\r\ngoto cleanup;\r\n}\r\ndev_dbg(ctx->dev, "%s: Event wake\n", __func__);\r\nif (ctx->cl_dev.wake_status != SKL_CL_DMA_BUF_COMPLETE) {\r\ndev_err(ctx->dev, "%s: DMA Error\n", __func__);\r\nret = -EIO;\r\n}\r\ncleanup:\r\nctx->cl_dev.wake_status = SKL_CL_DMA_STATUS_NONE;\r\nreturn ret;\r\n}\r\nstatic void skl_cldma_stop(struct sst_dsp *ctx)\r\n{\r\nskl_cldma_stream_run(ctx, false);\r\n}\r\nstatic void skl_cldma_fill_buffer(struct sst_dsp *ctx, unsigned int size,\r\nconst void *curr_pos, bool intr_enable, bool trigger)\r\n{\r\ndev_dbg(ctx->dev, "Size: %x, intr_enable: %d\n", size, intr_enable);\r\ndev_dbg(ctx->dev, "buf_pos_index:%d, trigger:%d\n",\r\nctx->cl_dev.dma_buffer_offset, trigger);\r\ndev_dbg(ctx->dev, "spib position: %d\n", ctx->cl_dev.curr_spib_pos);\r\nif (ctx->cl_dev.dma_buffer_offset + size > ctx->cl_dev.bufsize) {\r\nunsigned int size_b = ctx->cl_dev.bufsize -\r\nctx->cl_dev.dma_buffer_offset;\r\nmemcpy(ctx->cl_dev.dmab_data.area + ctx->cl_dev.dma_buffer_offset,\r\ncurr_pos, size_b);\r\nsize -= size_b;\r\ncurr_pos += size_b;\r\nctx->cl_dev.dma_buffer_offset = 0;\r\n}\r\nmemcpy(ctx->cl_dev.dmab_data.area + ctx->cl_dev.dma_buffer_offset,\r\ncurr_pos, size);\r\nif (ctx->cl_dev.curr_spib_pos == ctx->cl_dev.bufsize)\r\nctx->cl_dev.dma_buffer_offset = 0;\r\nelse\r\nctx->cl_dev.dma_buffer_offset = ctx->cl_dev.curr_spib_pos;\r\nctx->cl_dev.wait_condition = false;\r\nif (intr_enable)\r\nskl_cldma_int_enable(ctx);\r\nctx->cl_dev.ops.cl_setup_spb(ctx, ctx->cl_dev.curr_spib_pos, trigger);\r\nif (trigger)\r\nctx->cl_dev.ops.cl_trigger(ctx, true);\r\n}\r\nstatic int\r\nskl_cldma_copy_to_buf(struct sst_dsp *ctx, const void *bin,\r\nu32 total_size, bool wait)\r\n{\r\nint ret = 0;\r\nbool start = true;\r\nunsigned int excess_bytes;\r\nu32 size;\r\nunsigned int bytes_left = total_size;\r\nconst void *curr_pos = bin;\r\nif (total_size <= 0)\r\nreturn -EINVAL;\r\ndev_dbg(ctx->dev, "%s: Total binary size: %u\n", __func__, bytes_left);\r\nwhile (bytes_left) {\r\nif (bytes_left > ctx->cl_dev.bufsize) {\r\nif (ctx->cl_dev.curr_spib_pos == 0)\r\nctx->cl_dev.curr_spib_pos = ctx->cl_dev.bufsize;\r\nsize = ctx->cl_dev.bufsize;\r\nskl_cldma_fill_buffer(ctx, size, curr_pos, true, start);\r\nif (wait) {\r\nstart = false;\r\nret = skl_cldma_wait_interruptible(ctx);\r\nif (ret < 0) {\r\nskl_cldma_stop(ctx);\r\nreturn ret;\r\n}\r\n}\r\n} else {\r\nskl_cldma_int_disable(ctx);\r\nif ((ctx->cl_dev.curr_spib_pos + bytes_left)\r\n<= ctx->cl_dev.bufsize) {\r\nctx->cl_dev.curr_spib_pos += bytes_left;\r\n} else {\r\nexcess_bytes = bytes_left -\r\n(ctx->cl_dev.bufsize -\r\nctx->cl_dev.curr_spib_pos);\r\nctx->cl_dev.curr_spib_pos = excess_bytes;\r\n}\r\nsize = bytes_left;\r\nskl_cldma_fill_buffer(ctx, size,\r\ncurr_pos, false, start);\r\n}\r\nbytes_left -= size;\r\ncurr_pos = curr_pos + size;\r\nif (!wait)\r\nreturn bytes_left;\r\n}\r\nreturn bytes_left;\r\n}\r\nvoid skl_cldma_process_intr(struct sst_dsp *ctx)\r\n{\r\nu8 cl_dma_intr_status;\r\ncl_dma_intr_status =\r\nsst_dsp_shim_read_unlocked(ctx, SKL_ADSP_REG_CL_SD_STS);\r\nif (!(cl_dma_intr_status & SKL_CL_DMA_SD_INT_COMPLETE))\r\nctx->cl_dev.wake_status = SKL_CL_DMA_ERR;\r\nelse\r\nctx->cl_dev.wake_status = SKL_CL_DMA_BUF_COMPLETE;\r\nctx->cl_dev.wait_condition = true;\r\nwake_up(&ctx->cl_dev.wait_queue);\r\n}\r\nint skl_cldma_prepare(struct sst_dsp *ctx)\r\n{\r\nint ret;\r\nu32 *bdl;\r\nctx->cl_dev.bufsize = SKL_MAX_BUFFER_SIZE;\r\nctx->cl_dev.ops.cl_setup_bdle = skl_cldma_setup_bdle;\r\nctx->cl_dev.ops.cl_setup_controller = skl_cldma_setup_controller;\r\nctx->cl_dev.ops.cl_setup_spb = skl_cldma_setup_spb;\r\nctx->cl_dev.ops.cl_cleanup_spb = skl_cldma_cleanup_spb;\r\nctx->cl_dev.ops.cl_trigger = skl_cldma_stream_run;\r\nctx->cl_dev.ops.cl_cleanup_controller = skl_cldma_cleanup;\r\nctx->cl_dev.ops.cl_copy_to_dmabuf = skl_cldma_copy_to_buf;\r\nctx->cl_dev.ops.cl_stop_dma = skl_cldma_stop;\r\nret = ctx->dsp_ops.alloc_dma_buf(ctx->dev,\r\n&ctx->cl_dev.dmab_data, ctx->cl_dev.bufsize);\r\nif (ret < 0) {\r\ndev_err(ctx->dev, "Alloc buffer for base fw failed: %x\n", ret);\r\nreturn ret;\r\n}\r\nret = ctx->dsp_ops.alloc_dma_buf(ctx->dev,\r\n&ctx->cl_dev.dmab_bdl, PAGE_SIZE);\r\nif (ret < 0) {\r\ndev_err(ctx->dev, "Alloc buffer for blde failed: %x\n", ret);\r\nctx->dsp_ops.free_dma_buf(ctx->dev, &ctx->cl_dev.dmab_data);\r\nreturn ret;\r\n}\r\nbdl = (u32 *)ctx->cl_dev.dmab_bdl.area;\r\nctx->cl_dev.ops.cl_setup_bdle(ctx, &ctx->cl_dev.dmab_data,\r\n&bdl, ctx->cl_dev.bufsize, 1);\r\nctx->cl_dev.ops.cl_setup_controller(ctx, &ctx->cl_dev.dmab_bdl,\r\nctx->cl_dev.bufsize, ctx->cl_dev.frags);\r\nctx->cl_dev.curr_spib_pos = 0;\r\nctx->cl_dev.dma_buffer_offset = 0;\r\ninit_waitqueue_head(&ctx->cl_dev.wait_queue);\r\nreturn ret;\r\n}
