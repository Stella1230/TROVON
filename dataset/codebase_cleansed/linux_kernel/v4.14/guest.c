static void pci_error_handlers(struct cxl_afu *afu,\r\nint bus_error_event,\r\npci_channel_state_t state)\r\n{\r\nstruct pci_dev *afu_dev;\r\nif (afu->phb == NULL)\r\nreturn;\r\nlist_for_each_entry(afu_dev, &afu->phb->bus->devices, bus_list) {\r\nif (!afu_dev->driver)\r\ncontinue;\r\nswitch (bus_error_event) {\r\ncase CXL_ERROR_DETECTED_EVENT:\r\nafu_dev->error_state = state;\r\nif (afu_dev->driver->err_handler &&\r\nafu_dev->driver->err_handler->error_detected)\r\nafu_dev->driver->err_handler->error_detected(afu_dev, state);\r\nbreak;\r\ncase CXL_SLOT_RESET_EVENT:\r\nafu_dev->error_state = state;\r\nif (afu_dev->driver->err_handler &&\r\nafu_dev->driver->err_handler->slot_reset)\r\nafu_dev->driver->err_handler->slot_reset(afu_dev);\r\nbreak;\r\ncase CXL_RESUME_EVENT:\r\nif (afu_dev->driver->err_handler &&\r\nafu_dev->driver->err_handler->resume)\r\nafu_dev->driver->err_handler->resume(afu_dev);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic irqreturn_t guest_handle_psl_slice_error(struct cxl_context *ctx, u64 dsisr,\r\nu64 errstat)\r\n{\r\npr_devel("in %s\n", __func__);\r\ndev_crit(&ctx->afu->dev, "PSL ERROR STATUS: 0x%.16llx\n", errstat);\r\nreturn cxl_ops->ack_irq(ctx, 0, errstat);\r\n}\r\nstatic ssize_t guest_collect_vpd(struct cxl *adapter, struct cxl_afu *afu,\r\nvoid *buf, size_t len)\r\n{\r\nunsigned int entries, mod;\r\nunsigned long **vpd_buf = NULL;\r\nstruct sg_list *le;\r\nint rc = 0, i, tocopy;\r\nu64 out = 0;\r\nif (buf == NULL)\r\nreturn -EINVAL;\r\nentries = len / SG_BUFFER_SIZE;\r\nmod = len % SG_BUFFER_SIZE;\r\nif (mod)\r\nentries++;\r\nif (entries > SG_MAX_ENTRIES) {\r\nentries = SG_MAX_ENTRIES;\r\nlen = SG_MAX_ENTRIES * SG_BUFFER_SIZE;\r\nmod = 0;\r\n}\r\nvpd_buf = kzalloc(entries * sizeof(unsigned long *), GFP_KERNEL);\r\nif (!vpd_buf)\r\nreturn -ENOMEM;\r\nle = (struct sg_list *)get_zeroed_page(GFP_KERNEL);\r\nif (!le) {\r\nrc = -ENOMEM;\r\ngoto err1;\r\n}\r\nfor (i = 0; i < entries; i++) {\r\nvpd_buf[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\r\nif (!vpd_buf[i]) {\r\nrc = -ENOMEM;\r\ngoto err2;\r\n}\r\nle[i].phys_addr = cpu_to_be64(virt_to_phys(vpd_buf[i]));\r\nle[i].len = cpu_to_be64(SG_BUFFER_SIZE);\r\nif ((i == (entries - 1)) && mod)\r\nle[i].len = cpu_to_be64(mod);\r\n}\r\nif (adapter)\r\nrc = cxl_h_collect_vpd_adapter(adapter->guest->handle,\r\nvirt_to_phys(le), entries, &out);\r\nelse\r\nrc = cxl_h_collect_vpd(afu->guest->handle, 0,\r\nvirt_to_phys(le), entries, &out);\r\npr_devel("length of available (entries: %i), vpd: %#llx\n",\r\nentries, out);\r\nif (!rc) {\r\nif (out < len)\r\nlen = out;\r\nrc = len;\r\nif (out) {\r\nfor (i = 0; i < entries; i++) {\r\nif (len < SG_BUFFER_SIZE)\r\ntocopy = len;\r\nelse\r\ntocopy = SG_BUFFER_SIZE;\r\nmemcpy(buf, vpd_buf[i], tocopy);\r\nbuf += tocopy;\r\nlen -= tocopy;\r\n}\r\n}\r\n}\r\nerr2:\r\nfor (i = 0; i < entries; i++) {\r\nif (vpd_buf[i])\r\nfree_page((unsigned long) vpd_buf[i]);\r\n}\r\nfree_page((unsigned long) le);\r\nerr1:\r\nkfree(vpd_buf);\r\nreturn rc;\r\n}\r\nstatic int guest_get_irq_info(struct cxl_context *ctx, struct cxl_irq_info *info)\r\n{\r\nreturn cxl_h_collect_int_info(ctx->afu->guest->handle, ctx->process_token, info);\r\n}\r\nstatic irqreturn_t guest_psl_irq(int irq, void *data)\r\n{\r\nstruct cxl_context *ctx = data;\r\nstruct cxl_irq_info irq_info;\r\nint rc;\r\npr_devel("%d: received PSL interrupt %i\n", ctx->pe, irq);\r\nrc = guest_get_irq_info(ctx, &irq_info);\r\nif (rc) {\r\nWARN(1, "Unable to get IRQ info: %i\n", rc);\r\nreturn IRQ_HANDLED;\r\n}\r\nrc = cxl_irq_psl8(irq, ctx, &irq_info);\r\nreturn rc;\r\n}\r\nstatic int afu_read_error_state(struct cxl_afu *afu, int *state_out)\r\n{\r\nu64 state;\r\nint rc = 0;\r\nif (!afu)\r\nreturn -EIO;\r\nrc = cxl_h_read_error_state(afu->guest->handle, &state);\r\nif (!rc) {\r\nWARN_ON(state != H_STATE_NORMAL &&\r\nstate != H_STATE_DISABLE &&\r\nstate != H_STATE_TEMP_UNAVAILABLE &&\r\nstate != H_STATE_PERM_UNAVAILABLE);\r\n*state_out = state & 0xffffffff;\r\n}\r\nreturn rc;\r\n}\r\nstatic irqreturn_t guest_slice_irq_err(int irq, void *data)\r\n{\r\nstruct cxl_afu *afu = data;\r\nint rc;\r\nu64 serr, afu_error, dsisr;\r\nrc = cxl_h_get_fn_error_interrupt(afu->guest->handle, &serr);\r\nif (rc) {\r\ndev_crit(&afu->dev, "Couldn't read PSL_SERR_An: %d\n", rc);\r\nreturn IRQ_HANDLED;\r\n}\r\nafu_error = cxl_p2n_read(afu, CXL_AFU_ERR_An);\r\ndsisr = cxl_p2n_read(afu, CXL_PSL_DSISR_An);\r\ncxl_afu_decode_psl_serr(afu, serr);\r\ndev_crit(&afu->dev, "AFU_ERR_An: 0x%.16llx\n", afu_error);\r\ndev_crit(&afu->dev, "PSL_DSISR_An: 0x%.16llx\n", dsisr);\r\nrc = cxl_h_ack_fn_error_interrupt(afu->guest->handle, serr);\r\nif (rc)\r\ndev_crit(&afu->dev, "Couldn't ack slice error interrupt: %d\n",\r\nrc);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int irq_alloc_range(struct cxl *adapter, int len, int *irq)\r\n{\r\nint i, n;\r\nstruct irq_avail *cur;\r\nfor (i = 0; i < adapter->guest->irq_nranges; i++) {\r\ncur = &adapter->guest->irq_avail[i];\r\nn = bitmap_find_next_zero_area(cur->bitmap, cur->range,\r\n0, len, 0);\r\nif (n < cur->range) {\r\nbitmap_set(cur->bitmap, n, len);\r\n*irq = cur->offset + n;\r\npr_devel("guest: allocate IRQs %#x->%#x\n",\r\n*irq, *irq + len - 1);\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENOSPC;\r\n}\r\nstatic int irq_free_range(struct cxl *adapter, int irq, int len)\r\n{\r\nint i, n;\r\nstruct irq_avail *cur;\r\nif (len == 0)\r\nreturn -ENOENT;\r\nfor (i = 0; i < adapter->guest->irq_nranges; i++) {\r\ncur = &adapter->guest->irq_avail[i];\r\nif (irq >= cur->offset &&\r\n(irq + len) <= (cur->offset + cur->range)) {\r\nn = irq - cur->offset;\r\nbitmap_clear(cur->bitmap, n, len);\r\npr_devel("guest: release IRQs %#x->%#x\n",\r\nirq, irq + len - 1);\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENOENT;\r\n}\r\nstatic int guest_reset(struct cxl *adapter)\r\n{\r\nstruct cxl_afu *afu = NULL;\r\nint i, rc;\r\npr_devel("Adapter reset request\n");\r\nfor (i = 0; i < adapter->slices; i++) {\r\nif ((afu = adapter->afu[i])) {\r\npci_error_handlers(afu, CXL_ERROR_DETECTED_EVENT,\r\npci_channel_io_frozen);\r\ncxl_context_detach_all(afu);\r\n}\r\n}\r\nrc = cxl_h_reset_adapter(adapter->guest->handle);\r\nfor (i = 0; i < adapter->slices; i++) {\r\nif (!rc && (afu = adapter->afu[i])) {\r\npci_error_handlers(afu, CXL_SLOT_RESET_EVENT,\r\npci_channel_io_normal);\r\npci_error_handlers(afu, CXL_RESUME_EVENT, 0);\r\n}\r\n}\r\nreturn rc;\r\n}\r\nstatic int guest_alloc_one_irq(struct cxl *adapter)\r\n{\r\nint irq;\r\nspin_lock(&adapter->guest->irq_alloc_lock);\r\nif (irq_alloc_range(adapter, 1, &irq))\r\nirq = -ENOSPC;\r\nspin_unlock(&adapter->guest->irq_alloc_lock);\r\nreturn irq;\r\n}\r\nstatic void guest_release_one_irq(struct cxl *adapter, int irq)\r\n{\r\nspin_lock(&adapter->guest->irq_alloc_lock);\r\nirq_free_range(adapter, irq, 1);\r\nspin_unlock(&adapter->guest->irq_alloc_lock);\r\n}\r\nstatic int guest_alloc_irq_ranges(struct cxl_irq_ranges *irqs,\r\nstruct cxl *adapter, unsigned int num)\r\n{\r\nint i, try, irq;\r\nmemset(irqs, 0, sizeof(struct cxl_irq_ranges));\r\nspin_lock(&adapter->guest->irq_alloc_lock);\r\nfor (i = 0; i < CXL_IRQ_RANGES && num; i++) {\r\ntry = num;\r\nwhile (try) {\r\nif (irq_alloc_range(adapter, try, &irq) == 0)\r\nbreak;\r\ntry /= 2;\r\n}\r\nif (!try)\r\ngoto error;\r\nirqs->offset[i] = irq;\r\nirqs->range[i] = try;\r\nnum -= try;\r\n}\r\nif (num)\r\ngoto error;\r\nspin_unlock(&adapter->guest->irq_alloc_lock);\r\nreturn 0;\r\nerror:\r\nfor (i = 0; i < CXL_IRQ_RANGES; i++)\r\nirq_free_range(adapter, irqs->offset[i], irqs->range[i]);\r\nspin_unlock(&adapter->guest->irq_alloc_lock);\r\nreturn -ENOSPC;\r\n}\r\nstatic void guest_release_irq_ranges(struct cxl_irq_ranges *irqs,\r\nstruct cxl *adapter)\r\n{\r\nint i;\r\nspin_lock(&adapter->guest->irq_alloc_lock);\r\nfor (i = 0; i < CXL_IRQ_RANGES; i++)\r\nirq_free_range(adapter, irqs->offset[i], irqs->range[i]);\r\nspin_unlock(&adapter->guest->irq_alloc_lock);\r\n}\r\nstatic int guest_register_serr_irq(struct cxl_afu *afu)\r\n{\r\nafu->err_irq_name = kasprintf(GFP_KERNEL, "cxl-%s-err",\r\ndev_name(&afu->dev));\r\nif (!afu->err_irq_name)\r\nreturn -ENOMEM;\r\nif (!(afu->serr_virq = cxl_map_irq(afu->adapter, afu->serr_hwirq,\r\nguest_slice_irq_err, afu, afu->err_irq_name))) {\r\nkfree(afu->err_irq_name);\r\nafu->err_irq_name = NULL;\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void guest_release_serr_irq(struct cxl_afu *afu)\r\n{\r\ncxl_unmap_irq(afu->serr_virq, afu);\r\ncxl_ops->release_one_irq(afu->adapter, afu->serr_hwirq);\r\nkfree(afu->err_irq_name);\r\n}\r\nstatic int guest_ack_irq(struct cxl_context *ctx, u64 tfc, u64 psl_reset_mask)\r\n{\r\nreturn cxl_h_control_faults(ctx->afu->guest->handle, ctx->process_token,\r\ntfc >> 32, (psl_reset_mask != 0));\r\n}\r\nstatic void disable_afu_irqs(struct cxl_context *ctx)\r\n{\r\nirq_hw_number_t hwirq;\r\nunsigned int virq;\r\nint r, i;\r\npr_devel("Disabling AFU(%d) interrupts\n", ctx->afu->slice);\r\nfor (r = 0; r < CXL_IRQ_RANGES; r++) {\r\nhwirq = ctx->irqs.offset[r];\r\nfor (i = 0; i < ctx->irqs.range[r]; hwirq++, i++) {\r\nvirq = irq_find_mapping(NULL, hwirq);\r\ndisable_irq(virq);\r\n}\r\n}\r\n}\r\nstatic void enable_afu_irqs(struct cxl_context *ctx)\r\n{\r\nirq_hw_number_t hwirq;\r\nunsigned int virq;\r\nint r, i;\r\npr_devel("Enabling AFU(%d) interrupts\n", ctx->afu->slice);\r\nfor (r = 0; r < CXL_IRQ_RANGES; r++) {\r\nhwirq = ctx->irqs.offset[r];\r\nfor (i = 0; i < ctx->irqs.range[r]; hwirq++, i++) {\r\nvirq = irq_find_mapping(NULL, hwirq);\r\nenable_irq(virq);\r\n}\r\n}\r\n}\r\nstatic int _guest_afu_cr_readXX(int sz, struct cxl_afu *afu, int cr_idx,\r\nu64 offset, u64 *val)\r\n{\r\nunsigned long cr;\r\nchar c;\r\nint rc = 0;\r\nif (afu->crs_len < sz)\r\nreturn -ENOENT;\r\nif (unlikely(offset >= afu->crs_len))\r\nreturn -ERANGE;\r\ncr = get_zeroed_page(GFP_KERNEL);\r\nif (!cr)\r\nreturn -ENOMEM;\r\nrc = cxl_h_get_config(afu->guest->handle, cr_idx, offset,\r\nvirt_to_phys((void *)cr), sz);\r\nif (rc)\r\ngoto err;\r\nswitch (sz) {\r\ncase 1:\r\nc = *((char *) cr);\r\n*val = c;\r\nbreak;\r\ncase 2:\r\n*val = in_le16((u16 *)cr);\r\nbreak;\r\ncase 4:\r\n*val = in_le32((unsigned *)cr);\r\nbreak;\r\ncase 8:\r\n*val = in_le64((u64 *)cr);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\n}\r\nerr:\r\nfree_page(cr);\r\nreturn rc;\r\n}\r\nstatic int guest_afu_cr_read32(struct cxl_afu *afu, int cr_idx, u64 offset,\r\nu32 *out)\r\n{\r\nint rc;\r\nu64 val;\r\nrc = _guest_afu_cr_readXX(4, afu, cr_idx, offset, &val);\r\nif (!rc)\r\n*out = (u32) val;\r\nreturn rc;\r\n}\r\nstatic int guest_afu_cr_read16(struct cxl_afu *afu, int cr_idx, u64 offset,\r\nu16 *out)\r\n{\r\nint rc;\r\nu64 val;\r\nrc = _guest_afu_cr_readXX(2, afu, cr_idx, offset, &val);\r\nif (!rc)\r\n*out = (u16) val;\r\nreturn rc;\r\n}\r\nstatic int guest_afu_cr_read8(struct cxl_afu *afu, int cr_idx, u64 offset,\r\nu8 *out)\r\n{\r\nint rc;\r\nu64 val;\r\nrc = _guest_afu_cr_readXX(1, afu, cr_idx, offset, &val);\r\nif (!rc)\r\n*out = (u8) val;\r\nreturn rc;\r\n}\r\nstatic int guest_afu_cr_read64(struct cxl_afu *afu, int cr_idx, u64 offset,\r\nu64 *out)\r\n{\r\nreturn _guest_afu_cr_readXX(8, afu, cr_idx, offset, out);\r\n}\r\nstatic int guest_afu_cr_write32(struct cxl_afu *afu, int cr, u64 off, u32 in)\r\n{\r\nreturn -EPERM;\r\n}\r\nstatic int guest_afu_cr_write16(struct cxl_afu *afu, int cr, u64 off, u16 in)\r\n{\r\nreturn -EPERM;\r\n}\r\nstatic int guest_afu_cr_write8(struct cxl_afu *afu, int cr, u64 off, u8 in)\r\n{\r\nreturn -EPERM;\r\n}\r\nstatic int attach_afu_directed(struct cxl_context *ctx, u64 wed, u64 amr)\r\n{\r\nstruct cxl_process_element_hcall *elem;\r\nstruct cxl *adapter = ctx->afu->adapter;\r\nconst struct cred *cred;\r\nu32 pid, idx;\r\nint rc, r, i;\r\nu64 mmio_addr, mmio_size;\r\n__be64 flags = 0;\r\nif (!(elem = (struct cxl_process_element_hcall *)\r\nget_zeroed_page(GFP_KERNEL)))\r\nreturn -ENOMEM;\r\nelem->version = cpu_to_be64(CXL_PROCESS_ELEMENT_VERSION);\r\nif (ctx->kernel) {\r\npid = 0;\r\nflags |= CXL_PE_TRANSLATION_ENABLED;\r\nflags |= CXL_PE_PRIVILEGED_PROCESS;\r\nif (mfmsr() & MSR_SF)\r\nflags |= CXL_PE_64_BIT;\r\n} else {\r\npid = current->pid;\r\nflags |= CXL_PE_PROBLEM_STATE;\r\nflags |= CXL_PE_TRANSLATION_ENABLED;\r\nif (!test_tsk_thread_flag(current, TIF_32BIT))\r\nflags |= CXL_PE_64_BIT;\r\ncred = get_current_cred();\r\nif (uid_eq(cred->euid, GLOBAL_ROOT_UID))\r\nflags |= CXL_PE_PRIVILEGED_PROCESS;\r\nput_cred(cred);\r\n}\r\nelem->flags = cpu_to_be64(flags);\r\nelem->common.tid = cpu_to_be32(0);\r\nelem->common.pid = cpu_to_be32(pid);\r\nelem->common.csrp = cpu_to_be64(0);\r\nelem->common.u.psl8.aurp0 = cpu_to_be64(0);\r\nelem->common.u.psl8.aurp1 = cpu_to_be64(0);\r\ncxl_prefault(ctx, wed);\r\nelem->common.u.psl8.sstp0 = cpu_to_be64(ctx->sstp0);\r\nelem->common.u.psl8.sstp1 = cpu_to_be64(ctx->sstp1);\r\nif (ctx->irqs.range[0] == 0) {\r\nrc = afu_register_irqs(ctx, 0);\r\nif (rc)\r\ngoto out_free;\r\n}\r\nfor (r = 0; r < CXL_IRQ_RANGES; r++) {\r\nfor (i = 0; i < ctx->irqs.range[r]; i++) {\r\nif (r == 0 && i == 0) {\r\nelem->pslVirtualIsn = cpu_to_be32(ctx->irqs.offset[0]);\r\n} else {\r\nidx = ctx->irqs.offset[r] + i - adapter->guest->irq_base_offset;\r\nelem->applicationVirtualIsnBitmap[idx / 8] |= 0x80 >> (idx % 8);\r\n}\r\n}\r\n}\r\nelem->common.amr = cpu_to_be64(amr);\r\nelem->common.wed = cpu_to_be64(wed);\r\ndisable_afu_irqs(ctx);\r\nrc = cxl_h_attach_process(ctx->afu->guest->handle, elem,\r\n&ctx->process_token, &mmio_addr, &mmio_size);\r\nif (rc == H_SUCCESS) {\r\nif (ctx->master || !ctx->afu->pp_psa) {\r\nctx->psn_phys = ctx->afu->psn_phys;\r\nctx->psn_size = ctx->afu->adapter->ps_size;\r\n} else {\r\nctx->psn_phys = mmio_addr;\r\nctx->psn_size = mmio_size;\r\n}\r\nif (ctx->afu->pp_psa && mmio_size &&\r\nctx->afu->pp_size == 0) {\r\nctx->afu->pp_size = mmio_size;\r\n}\r\nctx->external_pe = ctx->process_token & 0xFFFFFFFF;\r\npr_devel("CXL pe=%i is known as %i for pHyp, mmio_size=%#llx",\r\nctx->pe, ctx->external_pe, ctx->psn_size);\r\nctx->pe_inserted = true;\r\nenable_afu_irqs(ctx);\r\n}\r\nout_free:\r\nfree_page((u64)elem);\r\nreturn rc;\r\n}\r\nstatic int guest_attach_process(struct cxl_context *ctx, bool kernel, u64 wed, u64 amr)\r\n{\r\npr_devel("in %s\n", __func__);\r\nif (ctx->real_mode)\r\nreturn -EPERM;\r\nctx->kernel = kernel;\r\nif (ctx->afu->current_mode == CXL_MODE_DIRECTED)\r\nreturn attach_afu_directed(ctx, wed, amr);\r\nreturn -EINVAL;\r\n}\r\nstatic int detach_afu_directed(struct cxl_context *ctx)\r\n{\r\nif (!ctx->pe_inserted)\r\nreturn 0;\r\nif (cxl_h_detach_process(ctx->afu->guest->handle, ctx->process_token))\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic int guest_detach_process(struct cxl_context *ctx)\r\n{\r\npr_devel("in %s\n", __func__);\r\ntrace_cxl_detach(ctx);\r\nif (!cxl_ops->link_ok(ctx->afu->adapter, ctx->afu))\r\nreturn -EIO;\r\nif (ctx->afu->current_mode == CXL_MODE_DIRECTED)\r\nreturn detach_afu_directed(ctx);\r\nreturn -EINVAL;\r\n}\r\nstatic void guest_release_afu(struct device *dev)\r\n{\r\nstruct cxl_afu *afu = to_cxl_afu(dev);\r\npr_devel("%s\n", __func__);\r\nidr_destroy(&afu->contexts_idr);\r\nkfree(afu->guest);\r\nkfree(afu);\r\n}\r\nssize_t cxl_guest_read_afu_vpd(struct cxl_afu *afu, void *buf, size_t len)\r\n{\r\nreturn guest_collect_vpd(NULL, afu, buf, len);\r\n}\r\nstatic ssize_t guest_afu_read_err_buffer(struct cxl_afu *afu, char *buf,\r\nloff_t off, size_t count)\r\n{\r\nvoid *tbuf = NULL;\r\nint rc = 0;\r\ntbuf = (void *) get_zeroed_page(GFP_KERNEL);\r\nif (!tbuf)\r\nreturn -ENOMEM;\r\nrc = cxl_h_get_afu_err(afu->guest->handle,\r\noff & 0x7,\r\nvirt_to_phys(tbuf),\r\ncount);\r\nif (rc)\r\ngoto err;\r\nif (count > ERR_BUFF_MAX_COPY_SIZE)\r\ncount = ERR_BUFF_MAX_COPY_SIZE - (off & 0x7);\r\nmemcpy(buf, tbuf, count);\r\nerr:\r\nfree_page((u64)tbuf);\r\nreturn rc;\r\n}\r\nstatic int guest_afu_check_and_enable(struct cxl_afu *afu)\r\n{\r\nreturn 0;\r\n}\r\nstatic bool guest_support_attributes(const char *attr_name,\r\nenum cxl_attrs type)\r\n{\r\nswitch (type) {\r\ncase CXL_ADAPTER_ATTRS:\r\nif ((strcmp(attr_name, "base_image") == 0) ||\r\n(strcmp(attr_name, "load_image_on_perst") == 0) ||\r\n(strcmp(attr_name, "perst_reloads_same_image") == 0) ||\r\n(strcmp(attr_name, "image_loaded") == 0))\r\nreturn false;\r\nbreak;\r\ncase CXL_AFU_MASTER_ATTRS:\r\nif ((strcmp(attr_name, "pp_mmio_off") == 0))\r\nreturn false;\r\nbreak;\r\ncase CXL_AFU_ATTRS:\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn true;\r\n}\r\nstatic int activate_afu_directed(struct cxl_afu *afu)\r\n{\r\nint rc;\r\ndev_info(&afu->dev, "Activating AFU(%d) directed mode\n", afu->slice);\r\nafu->current_mode = CXL_MODE_DIRECTED;\r\nafu->num_procs = afu->max_procs_virtualised;\r\nif ((rc = cxl_chardev_m_afu_add(afu)))\r\nreturn rc;\r\nif ((rc = cxl_sysfs_afu_m_add(afu)))\r\ngoto err;\r\nif ((rc = cxl_chardev_s_afu_add(afu)))\r\ngoto err1;\r\nreturn 0;\r\nerr1:\r\ncxl_sysfs_afu_m_remove(afu);\r\nerr:\r\ncxl_chardev_afu_remove(afu);\r\nreturn rc;\r\n}\r\nstatic int guest_afu_activate_mode(struct cxl_afu *afu, int mode)\r\n{\r\nif (!mode)\r\nreturn 0;\r\nif (!(mode & afu->modes_supported))\r\nreturn -EINVAL;\r\nif (mode == CXL_MODE_DIRECTED)\r\nreturn activate_afu_directed(afu);\r\nif (mode == CXL_MODE_DEDICATED)\r\ndev_err(&afu->dev, "Dedicated mode not supported\n");\r\nreturn -EINVAL;\r\n}\r\nstatic int deactivate_afu_directed(struct cxl_afu *afu)\r\n{\r\ndev_info(&afu->dev, "Deactivating AFU(%d) directed mode\n", afu->slice);\r\nafu->current_mode = 0;\r\nafu->num_procs = 0;\r\ncxl_sysfs_afu_m_remove(afu);\r\ncxl_chardev_afu_remove(afu);\r\ncxl_ops->afu_reset(afu);\r\nreturn 0;\r\n}\r\nstatic int guest_afu_deactivate_mode(struct cxl_afu *afu, int mode)\r\n{\r\nif (!mode)\r\nreturn 0;\r\nif (!(mode & afu->modes_supported))\r\nreturn -EINVAL;\r\nif (mode == CXL_MODE_DIRECTED)\r\nreturn deactivate_afu_directed(afu);\r\nreturn 0;\r\n}\r\nstatic int guest_afu_reset(struct cxl_afu *afu)\r\n{\r\npr_devel("AFU(%d) reset request\n", afu->slice);\r\nreturn cxl_h_reset_afu(afu->guest->handle);\r\n}\r\nstatic int guest_map_slice_regs(struct cxl_afu *afu)\r\n{\r\nif (!(afu->p2n_mmio = ioremap(afu->guest->p2n_phys, afu->guest->p2n_size))) {\r\ndev_err(&afu->dev, "Error mapping AFU(%d) MMIO regions\n",\r\nafu->slice);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void guest_unmap_slice_regs(struct cxl_afu *afu)\r\n{\r\nif (afu->p2n_mmio)\r\niounmap(afu->p2n_mmio);\r\n}\r\nstatic int afu_update_state(struct cxl_afu *afu)\r\n{\r\nint rc, cur_state;\r\nrc = afu_read_error_state(afu, &cur_state);\r\nif (rc)\r\nreturn rc;\r\nif (afu->guest->previous_state == cur_state)\r\nreturn 0;\r\npr_devel("AFU(%d) update state to %#x\n", afu->slice, cur_state);\r\nswitch (cur_state) {\r\ncase H_STATE_NORMAL:\r\nafu->guest->previous_state = cur_state;\r\nbreak;\r\ncase H_STATE_DISABLE:\r\npci_error_handlers(afu, CXL_ERROR_DETECTED_EVENT,\r\npci_channel_io_frozen);\r\ncxl_context_detach_all(afu);\r\nif ((rc = cxl_ops->afu_reset(afu)))\r\npr_devel("reset hcall failed %d\n", rc);\r\nrc = afu_read_error_state(afu, &cur_state);\r\nif (!rc && cur_state == H_STATE_NORMAL) {\r\npci_error_handlers(afu, CXL_SLOT_RESET_EVENT,\r\npci_channel_io_normal);\r\npci_error_handlers(afu, CXL_RESUME_EVENT, 0);\r\n}\r\nafu->guest->previous_state = 0;\r\nbreak;\r\ncase H_STATE_TEMP_UNAVAILABLE:\r\nafu->guest->previous_state = cur_state;\r\nbreak;\r\ncase H_STATE_PERM_UNAVAILABLE:\r\ndev_err(&afu->dev, "AFU is in permanent error state\n");\r\npci_error_handlers(afu, CXL_ERROR_DETECTED_EVENT,\r\npci_channel_io_perm_failure);\r\nafu->guest->previous_state = cur_state;\r\nbreak;\r\ndefault:\r\npr_err("Unexpected AFU(%d) error state: %#x\n",\r\nafu->slice, cur_state);\r\nreturn -EINVAL;\r\n}\r\nreturn rc;\r\n}\r\nstatic void afu_handle_errstate(struct work_struct *work)\r\n{\r\nstruct cxl_afu_guest *afu_guest =\r\ncontainer_of(to_delayed_work(work), struct cxl_afu_guest, work_err);\r\nif (!afu_update_state(afu_guest->parent) &&\r\nafu_guest->previous_state == H_STATE_PERM_UNAVAILABLE)\r\nreturn;\r\nif (afu_guest->handle_err)\r\nschedule_delayed_work(&afu_guest->work_err,\r\nmsecs_to_jiffies(3000));\r\n}\r\nstatic bool guest_link_ok(struct cxl *cxl, struct cxl_afu *afu)\r\n{\r\nint state;\r\nif (afu && (!afu_read_error_state(afu, &state))) {\r\nif (state == H_STATE_NORMAL)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int afu_properties_look_ok(struct cxl_afu *afu)\r\n{\r\nif (afu->pp_irqs < 0) {\r\ndev_err(&afu->dev, "Unexpected per-process minimum interrupt value\n");\r\nreturn -EINVAL;\r\n}\r\nif (afu->max_procs_virtualised < 1) {\r\ndev_err(&afu->dev, "Unexpected max number of processes virtualised value\n");\r\nreturn -EINVAL;\r\n}\r\nif (afu->crs_len < 0) {\r\ndev_err(&afu->dev, "Unexpected configuration record size value\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint cxl_guest_init_afu(struct cxl *adapter, int slice, struct device_node *afu_np)\r\n{\r\nstruct cxl_afu *afu;\r\nbool free = true;\r\nint rc;\r\npr_devel("in %s - AFU(%d)\n", __func__, slice);\r\nif (!(afu = cxl_alloc_afu(adapter, slice)))\r\nreturn -ENOMEM;\r\nif (!(afu->guest = kzalloc(sizeof(struct cxl_afu_guest), GFP_KERNEL))) {\r\nkfree(afu);\r\nreturn -ENOMEM;\r\n}\r\nif ((rc = dev_set_name(&afu->dev, "afu%i.%i",\r\nadapter->adapter_num,\r\nslice)))\r\ngoto err1;\r\nadapter->slices++;\r\nif ((rc = cxl_of_read_afu_handle(afu, afu_np)))\r\ngoto err1;\r\nif ((rc = cxl_ops->afu_reset(afu)))\r\ngoto err1;\r\nif ((rc = cxl_of_read_afu_properties(afu, afu_np)))\r\ngoto err1;\r\nif ((rc = afu_properties_look_ok(afu)))\r\ngoto err1;\r\nif ((rc = guest_map_slice_regs(afu)))\r\ngoto err1;\r\nif ((rc = guest_register_serr_irq(afu)))\r\ngoto err2;\r\nif ((rc = cxl_register_afu(afu)))\r\ngoto err_put1;\r\nif ((rc = cxl_sysfs_afu_add(afu)))\r\ngoto err_put1;\r\nif (afu->max_procs_virtualised == 1)\r\nafu->modes_supported = CXL_MODE_DEDICATED;\r\nelse\r\nafu->modes_supported = CXL_MODE_DIRECTED;\r\nif ((rc = cxl_afu_select_best_mode(afu)))\r\ngoto err_put2;\r\nadapter->afu[afu->slice] = afu;\r\nafu->enabled = true;\r\nafu->guest->parent = afu;\r\nafu->guest->handle_err = true;\r\nINIT_DELAYED_WORK(&afu->guest->work_err, afu_handle_errstate);\r\nschedule_delayed_work(&afu->guest->work_err, msecs_to_jiffies(1000));\r\nif ((rc = cxl_pci_vphb_add(afu)))\r\ndev_info(&afu->dev, "Can't register vPHB\n");\r\nreturn 0;\r\nerr_put2:\r\ncxl_sysfs_afu_remove(afu);\r\nerr_put1:\r\ndevice_unregister(&afu->dev);\r\nfree = false;\r\nguest_release_serr_irq(afu);\r\nerr2:\r\nguest_unmap_slice_regs(afu);\r\nerr1:\r\nif (free) {\r\nkfree(afu->guest);\r\nkfree(afu);\r\n}\r\nreturn rc;\r\n}\r\nvoid cxl_guest_remove_afu(struct cxl_afu *afu)\r\n{\r\npr_devel("in %s - AFU(%d)\n", __func__, afu->slice);\r\nif (!afu)\r\nreturn;\r\nafu->guest->handle_err = false;\r\nflush_delayed_work(&afu->guest->work_err);\r\ncxl_pci_vphb_remove(afu);\r\ncxl_sysfs_afu_remove(afu);\r\nspin_lock(&afu->adapter->afu_list_lock);\r\nafu->adapter->afu[afu->slice] = NULL;\r\nspin_unlock(&afu->adapter->afu_list_lock);\r\ncxl_context_detach_all(afu);\r\ncxl_ops->afu_deactivate_mode(afu, afu->current_mode);\r\nguest_release_serr_irq(afu);\r\nguest_unmap_slice_regs(afu);\r\ndevice_unregister(&afu->dev);\r\n}\r\nstatic void free_adapter(struct cxl *adapter)\r\n{\r\nstruct irq_avail *cur;\r\nint i;\r\nif (adapter->guest) {\r\nif (adapter->guest->irq_avail) {\r\nfor (i = 0; i < adapter->guest->irq_nranges; i++) {\r\ncur = &adapter->guest->irq_avail[i];\r\nkfree(cur->bitmap);\r\n}\r\nkfree(adapter->guest->irq_avail);\r\n}\r\nkfree(adapter->guest->status);\r\nkfree(adapter->guest);\r\n}\r\ncxl_remove_adapter_nr(adapter);\r\nkfree(adapter);\r\n}\r\nstatic int properties_look_ok(struct cxl *adapter)\r\n{\r\nif (strlen(adapter->guest->status) &&\r\nstrcmp(adapter->guest->status, "okay")) {\r\npr_err("ABORTING:Bad operational status of the device\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nssize_t cxl_guest_read_adapter_vpd(struct cxl *adapter, void *buf, size_t len)\r\n{\r\nreturn guest_collect_vpd(adapter, NULL, buf, len);\r\n}\r\nvoid cxl_guest_remove_adapter(struct cxl *adapter)\r\n{\r\npr_devel("in %s\n", __func__);\r\ncxl_sysfs_adapter_remove(adapter);\r\ncxl_guest_remove_chardev(adapter);\r\ndevice_unregister(&adapter->dev);\r\n}\r\nstatic void release_adapter(struct device *dev)\r\n{\r\nfree_adapter(to_cxl_adapter(dev));\r\n}\r\nstruct cxl *cxl_guest_init_adapter(struct device_node *np, struct platform_device *pdev)\r\n{\r\nstruct cxl *adapter;\r\nbool free = true;\r\nint rc;\r\nif (!(adapter = cxl_alloc_adapter()))\r\nreturn ERR_PTR(-ENOMEM);\r\nif (!(adapter->guest = kzalloc(sizeof(struct cxl_guest), GFP_KERNEL))) {\r\nfree_adapter(adapter);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nadapter->slices = 0;\r\nadapter->guest->pdev = pdev;\r\nadapter->dev.parent = &pdev->dev;\r\nadapter->dev.release = release_adapter;\r\ndev_set_drvdata(&pdev->dev, adapter);\r\nadapter->psl_timebase_synced = true;\r\nif ((rc = cxl_of_read_adapter_handle(adapter, np)))\r\ngoto err1;\r\nif ((rc = cxl_of_read_adapter_properties(adapter, np)))\r\ngoto err1;\r\nif ((rc = properties_look_ok(adapter)))\r\ngoto err1;\r\nif ((rc = cxl_guest_add_chardev(adapter)))\r\ngoto err1;\r\nif ((rc = cxl_register_adapter(adapter)))\r\ngoto err_put1;\r\nif ((rc = cxl_sysfs_adapter_add(adapter)))\r\ngoto err_put1;\r\ncxl_adapter_context_unlock(adapter);\r\nreturn adapter;\r\nerr_put1:\r\ndevice_unregister(&adapter->dev);\r\nfree = false;\r\ncxl_guest_remove_chardev(adapter);\r\nerr1:\r\nif (free)\r\nfree_adapter(adapter);\r\nreturn ERR_PTR(rc);\r\n}\r\nvoid cxl_guest_reload_module(struct cxl *adapter)\r\n{\r\nstruct platform_device *pdev;\r\npdev = adapter->guest->pdev;\r\ncxl_guest_remove_adapter(adapter);\r\ncxl_of_probe(pdev);\r\n}
