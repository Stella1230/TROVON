static int thread_stack__grow(struct thread_stack *ts)\r\n{\r\nstruct thread_stack_entry *new_stack;\r\nsize_t sz, new_sz;\r\nnew_sz = ts->sz + STACK_GROWTH;\r\nsz = new_sz * sizeof(struct thread_stack_entry);\r\nnew_stack = realloc(ts->stack, sz);\r\nif (!new_stack)\r\nreturn -ENOMEM;\r\nts->stack = new_stack;\r\nts->sz = new_sz;\r\nreturn 0;\r\n}\r\nstatic struct thread_stack *thread_stack__new(struct thread *thread,\r\nstruct call_return_processor *crp)\r\n{\r\nstruct thread_stack *ts;\r\nts = zalloc(sizeof(struct thread_stack));\r\nif (!ts)\r\nreturn NULL;\r\nif (thread_stack__grow(ts)) {\r\nfree(ts);\r\nreturn NULL;\r\n}\r\nif (thread->mg && thread->mg->machine)\r\nts->kernel_start = machine__kernel_start(thread->mg->machine);\r\nelse\r\nts->kernel_start = 1ULL << 63;\r\nts->crp = crp;\r\nreturn ts;\r\n}\r\nstatic int thread_stack__push(struct thread_stack *ts, u64 ret_addr)\r\n{\r\nint err = 0;\r\nif (ts->cnt == ts->sz) {\r\nerr = thread_stack__grow(ts);\r\nif (err) {\r\npr_warning("Out of memory: discarding thread stack\n");\r\nts->cnt = 0;\r\n}\r\n}\r\nts->stack[ts->cnt++].ret_addr = ret_addr;\r\nreturn err;\r\n}\r\nstatic void thread_stack__pop(struct thread_stack *ts, u64 ret_addr)\r\n{\r\nsize_t i;\r\nfor (i = ts->cnt; i; ) {\r\nif (ts->stack[--i].ret_addr == ret_addr) {\r\nts->cnt = i;\r\nreturn;\r\n}\r\n}\r\n}\r\nstatic bool thread_stack__in_kernel(struct thread_stack *ts)\r\n{\r\nif (!ts->cnt)\r\nreturn false;\r\nreturn ts->stack[ts->cnt - 1].cp->in_kernel;\r\n}\r\nstatic int thread_stack__call_return(struct thread *thread,\r\nstruct thread_stack *ts, size_t idx,\r\nu64 timestamp, u64 ref, bool no_return)\r\n{\r\nstruct call_return_processor *crp = ts->crp;\r\nstruct thread_stack_entry *tse;\r\nstruct call_return cr = {\r\n.thread = thread,\r\n.comm = ts->comm,\r\n.db_id = 0,\r\n};\r\ntse = &ts->stack[idx];\r\ncr.cp = tse->cp;\r\ncr.call_time = tse->timestamp;\r\ncr.return_time = timestamp;\r\ncr.branch_count = ts->branch_count - tse->branch_count;\r\ncr.call_ref = tse->ref;\r\ncr.return_ref = ref;\r\nif (tse->no_call)\r\ncr.flags |= CALL_RETURN_NO_CALL;\r\nif (no_return)\r\ncr.flags |= CALL_RETURN_NO_RETURN;\r\nreturn crp->process(&cr, crp->data);\r\n}\r\nstatic int __thread_stack__flush(struct thread *thread, struct thread_stack *ts)\r\n{\r\nstruct call_return_processor *crp = ts->crp;\r\nint err;\r\nif (!crp) {\r\nts->cnt = 0;\r\nreturn 0;\r\n}\r\nwhile (ts->cnt) {\r\nerr = thread_stack__call_return(thread, ts, --ts->cnt,\r\nts->last_time, 0, true);\r\nif (err) {\r\npr_err("Error flushing thread stack!\n");\r\nts->cnt = 0;\r\nreturn err;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint thread_stack__flush(struct thread *thread)\r\n{\r\nif (thread->ts)\r\nreturn __thread_stack__flush(thread, thread->ts);\r\nreturn 0;\r\n}\r\nint thread_stack__event(struct thread *thread, u32 flags, u64 from_ip,\r\nu64 to_ip, u16 insn_len, u64 trace_nr)\r\n{\r\nif (!thread)\r\nreturn -EINVAL;\r\nif (!thread->ts) {\r\nthread->ts = thread_stack__new(thread, NULL);\r\nif (!thread->ts) {\r\npr_warning("Out of memory: no thread stack\n");\r\nreturn -ENOMEM;\r\n}\r\nthread->ts->trace_nr = trace_nr;\r\n}\r\nif (trace_nr != thread->ts->trace_nr) {\r\nif (thread->ts->trace_nr)\r\n__thread_stack__flush(thread, thread->ts);\r\nthread->ts->trace_nr = trace_nr;\r\n}\r\nif (thread->ts->crp)\r\nreturn 0;\r\nif (flags & PERF_IP_FLAG_CALL) {\r\nu64 ret_addr;\r\nif (!to_ip)\r\nreturn 0;\r\nret_addr = from_ip + insn_len;\r\nif (ret_addr == to_ip)\r\nreturn 0;\r\nreturn thread_stack__push(thread->ts, ret_addr);\r\n} else if (flags & PERF_IP_FLAG_RETURN) {\r\nif (!from_ip)\r\nreturn 0;\r\nthread_stack__pop(thread->ts, to_ip);\r\n}\r\nreturn 0;\r\n}\r\nvoid thread_stack__set_trace_nr(struct thread *thread, u64 trace_nr)\r\n{\r\nif (!thread || !thread->ts)\r\nreturn;\r\nif (trace_nr != thread->ts->trace_nr) {\r\nif (thread->ts->trace_nr)\r\n__thread_stack__flush(thread, thread->ts);\r\nthread->ts->trace_nr = trace_nr;\r\n}\r\n}\r\nvoid thread_stack__free(struct thread *thread)\r\n{\r\nif (thread->ts) {\r\n__thread_stack__flush(thread, thread->ts);\r\nzfree(&thread->ts->stack);\r\nzfree(&thread->ts);\r\n}\r\n}\r\nvoid thread_stack__sample(struct thread *thread, struct ip_callchain *chain,\r\nsize_t sz, u64 ip)\r\n{\r\nsize_t i;\r\nif (!thread || !thread->ts)\r\nchain->nr = 1;\r\nelse\r\nchain->nr = min(sz, thread->ts->cnt + 1);\r\nchain->ips[0] = ip;\r\nfor (i = 1; i < chain->nr; i++)\r\nchain->ips[i] = thread->ts->stack[thread->ts->cnt - i].ret_addr;\r\n}\r\nstruct call_return_processor *\r\ncall_return_processor__new(int (*process)(struct call_return *cr, void *data),\r\nvoid *data)\r\n{\r\nstruct call_return_processor *crp;\r\ncrp = zalloc(sizeof(struct call_return_processor));\r\nif (!crp)\r\nreturn NULL;\r\ncrp->cpr = call_path_root__new();\r\nif (!crp->cpr)\r\ngoto out_free;\r\ncrp->process = process;\r\ncrp->data = data;\r\nreturn crp;\r\nout_free:\r\nfree(crp);\r\nreturn NULL;\r\n}\r\nvoid call_return_processor__free(struct call_return_processor *crp)\r\n{\r\nif (crp) {\r\ncall_path_root__free(crp->cpr);\r\nfree(crp);\r\n}\r\n}\r\nstatic int thread_stack__push_cp(struct thread_stack *ts, u64 ret_addr,\r\nu64 timestamp, u64 ref, struct call_path *cp,\r\nbool no_call)\r\n{\r\nstruct thread_stack_entry *tse;\r\nint err;\r\nif (ts->cnt == ts->sz) {\r\nerr = thread_stack__grow(ts);\r\nif (err)\r\nreturn err;\r\n}\r\ntse = &ts->stack[ts->cnt++];\r\ntse->ret_addr = ret_addr;\r\ntse->timestamp = timestamp;\r\ntse->ref = ref;\r\ntse->branch_count = ts->branch_count;\r\ntse->cp = cp;\r\ntse->no_call = no_call;\r\nreturn 0;\r\n}\r\nstatic int thread_stack__pop_cp(struct thread *thread, struct thread_stack *ts,\r\nu64 ret_addr, u64 timestamp, u64 ref,\r\nstruct symbol *sym)\r\n{\r\nint err;\r\nif (!ts->cnt)\r\nreturn 1;\r\nif (ts->cnt == 1) {\r\nstruct thread_stack_entry *tse = &ts->stack[0];\r\nif (tse->cp->sym == sym)\r\nreturn thread_stack__call_return(thread, ts, --ts->cnt,\r\ntimestamp, ref, false);\r\n}\r\nif (ts->stack[ts->cnt - 1].ret_addr == ret_addr) {\r\nreturn thread_stack__call_return(thread, ts, --ts->cnt,\r\ntimestamp, ref, false);\r\n} else {\r\nsize_t i = ts->cnt - 1;\r\nwhile (i--) {\r\nif (ts->stack[i].ret_addr != ret_addr)\r\ncontinue;\r\ni += 1;\r\nwhile (ts->cnt > i) {\r\nerr = thread_stack__call_return(thread, ts,\r\n--ts->cnt,\r\ntimestamp, ref,\r\ntrue);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn thread_stack__call_return(thread, ts, --ts->cnt,\r\ntimestamp, ref, false);\r\n}\r\n}\r\nreturn 1;\r\n}\r\nstatic int thread_stack__bottom(struct thread *thread, struct thread_stack *ts,\r\nstruct perf_sample *sample,\r\nstruct addr_location *from_al,\r\nstruct addr_location *to_al, u64 ref)\r\n{\r\nstruct call_path_root *cpr = ts->crp->cpr;\r\nstruct call_path *cp;\r\nstruct symbol *sym;\r\nu64 ip;\r\nif (sample->ip) {\r\nip = sample->ip;\r\nsym = from_al->sym;\r\n} else if (sample->addr) {\r\nip = sample->addr;\r\nsym = to_al->sym;\r\n} else {\r\nreturn 0;\r\n}\r\ncp = call_path__findnew(cpr, &cpr->call_path, sym, ip,\r\nts->kernel_start);\r\nif (!cp)\r\nreturn -ENOMEM;\r\nreturn thread_stack__push_cp(thread->ts, ip, sample->time, ref, cp,\r\ntrue);\r\n}\r\nstatic int thread_stack__no_call_return(struct thread *thread,\r\nstruct thread_stack *ts,\r\nstruct perf_sample *sample,\r\nstruct addr_location *from_al,\r\nstruct addr_location *to_al, u64 ref)\r\n{\r\nstruct call_path_root *cpr = ts->crp->cpr;\r\nstruct call_path *cp, *parent;\r\nu64 ks = ts->kernel_start;\r\nint err;\r\nif (sample->ip >= ks && sample->addr < ks) {\r\nwhile (thread_stack__in_kernel(ts)) {\r\nerr = thread_stack__call_return(thread, ts, --ts->cnt,\r\nsample->time, ref,\r\ntrue);\r\nif (err)\r\nreturn err;\r\n}\r\nif (!ts->cnt) {\r\ncp = call_path__findnew(cpr, &cpr->call_path,\r\nto_al->sym, sample->addr,\r\nts->kernel_start);\r\nif (!cp)\r\nreturn -ENOMEM;\r\nreturn thread_stack__push_cp(ts, 0, sample->time, ref,\r\ncp, true);\r\n}\r\n} else if (thread_stack__in_kernel(ts) && sample->ip < ks) {\r\nwhile (thread_stack__in_kernel(ts)) {\r\nerr = thread_stack__call_return(thread, ts, --ts->cnt,\r\nsample->time, ref,\r\ntrue);\r\nif (err)\r\nreturn err;\r\n}\r\n}\r\nif (ts->cnt)\r\nparent = ts->stack[ts->cnt - 1].cp;\r\nelse\r\nparent = &cpr->call_path;\r\ncp = call_path__findnew(cpr, parent, from_al->sym, sample->ip,\r\nts->kernel_start);\r\nif (!cp)\r\nreturn -ENOMEM;\r\nerr = thread_stack__push_cp(ts, sample->addr, sample->time, ref, cp,\r\ntrue);\r\nif (err)\r\nreturn err;\r\nreturn thread_stack__pop_cp(thread, ts, sample->addr, sample->time, ref,\r\nto_al->sym);\r\n}\r\nstatic int thread_stack__trace_begin(struct thread *thread,\r\nstruct thread_stack *ts, u64 timestamp,\r\nu64 ref)\r\n{\r\nstruct thread_stack_entry *tse;\r\nint err;\r\nif (!ts->cnt)\r\nreturn 0;\r\ntse = &ts->stack[ts->cnt - 1];\r\nif (tse->cp->sym == NULL && tse->cp->ip == 0) {\r\nerr = thread_stack__call_return(thread, ts, --ts->cnt,\r\ntimestamp, ref, false);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int thread_stack__trace_end(struct thread_stack *ts,\r\nstruct perf_sample *sample, u64 ref)\r\n{\r\nstruct call_path_root *cpr = ts->crp->cpr;\r\nstruct call_path *cp;\r\nu64 ret_addr;\r\nif (!ts->cnt || (ts->cnt == 1 && ts->stack[0].ref == ref))\r\nreturn 0;\r\ncp = call_path__findnew(cpr, ts->stack[ts->cnt - 1].cp, NULL, 0,\r\nts->kernel_start);\r\nif (!cp)\r\nreturn -ENOMEM;\r\nret_addr = sample->ip + sample->insn_len;\r\nreturn thread_stack__push_cp(ts, ret_addr, sample->time, ref, cp,\r\nfalse);\r\n}\r\nint thread_stack__process(struct thread *thread, struct comm *comm,\r\nstruct perf_sample *sample,\r\nstruct addr_location *from_al,\r\nstruct addr_location *to_al, u64 ref,\r\nstruct call_return_processor *crp)\r\n{\r\nstruct thread_stack *ts = thread->ts;\r\nint err = 0;\r\nif (ts) {\r\nif (!ts->crp) {\r\nthread_stack__free(thread);\r\nthread->ts = thread_stack__new(thread, crp);\r\nif (!thread->ts)\r\nreturn -ENOMEM;\r\nts = thread->ts;\r\nts->comm = comm;\r\n}\r\n} else {\r\nthread->ts = thread_stack__new(thread, crp);\r\nif (!thread->ts)\r\nreturn -ENOMEM;\r\nts = thread->ts;\r\nts->comm = comm;\r\n}\r\nif (ts->comm != comm && thread->pid_ == thread->tid) {\r\nerr = __thread_stack__flush(thread, ts);\r\nif (err)\r\nreturn err;\r\nts->comm = comm;\r\n}\r\nif (!ts->cnt) {\r\nerr = thread_stack__bottom(thread, ts, sample, from_al, to_al,\r\nref);\r\nif (err)\r\nreturn err;\r\n}\r\nts->branch_count += 1;\r\nts->last_time = sample->time;\r\nif (sample->flags & PERF_IP_FLAG_CALL) {\r\nstruct call_path_root *cpr = ts->crp->cpr;\r\nstruct call_path *cp;\r\nu64 ret_addr;\r\nif (!sample->ip || !sample->addr)\r\nreturn 0;\r\nret_addr = sample->ip + sample->insn_len;\r\nif (ret_addr == sample->addr)\r\nreturn 0;\r\ncp = call_path__findnew(cpr, ts->stack[ts->cnt - 1].cp,\r\nto_al->sym, sample->addr,\r\nts->kernel_start);\r\nif (!cp)\r\nreturn -ENOMEM;\r\nerr = thread_stack__push_cp(ts, ret_addr, sample->time, ref,\r\ncp, false);\r\n} else if (sample->flags & PERF_IP_FLAG_RETURN) {\r\nif (!sample->ip || !sample->addr)\r\nreturn 0;\r\nerr = thread_stack__pop_cp(thread, ts, sample->addr,\r\nsample->time, ref, from_al->sym);\r\nif (err) {\r\nif (err < 0)\r\nreturn err;\r\nerr = thread_stack__no_call_return(thread, ts, sample,\r\nfrom_al, to_al, ref);\r\n}\r\n} else if (sample->flags & PERF_IP_FLAG_TRACE_BEGIN) {\r\nerr = thread_stack__trace_begin(thread, ts, sample->time, ref);\r\n} else if (sample->flags & PERF_IP_FLAG_TRACE_END) {\r\nerr = thread_stack__trace_end(ts, sample, ref);\r\n}\r\nreturn err;\r\n}\r\nsize_t thread_stack__depth(struct thread *thread)\r\n{\r\nif (!thread->ts)\r\nreturn 0;\r\nreturn thread->ts->cnt;\r\n}
