static struct mtk_iommu_domain *to_mtk_domain(struct iommu_domain *dom)\r\n{\r\nreturn container_of(dom, struct mtk_iommu_domain, domain);\r\n}\r\nstatic inline int mt2701_m4u_to_larb(int id)\r\n{\r\nint i;\r\nfor (i = ARRAY_SIZE(mt2701_m4u_in_larb) - 1; i >= 0; i--)\r\nif ((id) >= mt2701_m4u_in_larb[i])\r\nreturn i;\r\nreturn 0;\r\n}\r\nstatic inline int mt2701_m4u_to_port(int id)\r\n{\r\nint larb = mt2701_m4u_to_larb(id);\r\nreturn id - mt2701_m4u_in_larb[larb];\r\n}\r\nstatic void mtk_iommu_tlb_flush_all(struct mtk_iommu_data *data)\r\n{\r\nwritel_relaxed(F_INVLD_EN1 | F_INVLD_EN0,\r\ndata->base + REG_MMU_INV_SEL);\r\nwritel_relaxed(F_ALL_INVLD, data->base + REG_MMU_INVALIDATE);\r\nwmb();\r\n}\r\nstatic void mtk_iommu_tlb_flush_range(struct mtk_iommu_data *data,\r\nunsigned long iova, size_t size)\r\n{\r\nint ret;\r\nu32 tmp;\r\nwritel_relaxed(F_INVLD_EN1 | F_INVLD_EN0,\r\ndata->base + REG_MMU_INV_SEL);\r\nwritel_relaxed(iova & F_MMU_FAULT_VA_MSK,\r\ndata->base + REG_MMU_INVLD_START_A);\r\nwritel_relaxed((iova + size - 1) & F_MMU_FAULT_VA_MSK,\r\ndata->base + REG_MMU_INVLD_END_A);\r\nwritel_relaxed(F_MMU_INV_RANGE, data->base + REG_MMU_INVALIDATE);\r\nret = readl_poll_timeout_atomic(data->base + REG_MMU_CPE_DONE,\r\ntmp, tmp != 0, 10, 100000);\r\nif (ret) {\r\ndev_warn(data->dev,\r\n"Partial TLB flush timed out, falling back to full flush\n");\r\nmtk_iommu_tlb_flush_all(data);\r\n}\r\nwritel_relaxed(0, data->base + REG_MMU_CPE_DONE);\r\n}\r\nstatic irqreturn_t mtk_iommu_isr(int irq, void *dev_id)\r\n{\r\nstruct mtk_iommu_data *data = dev_id;\r\nstruct mtk_iommu_domain *dom = data->m4u_dom;\r\nu32 int_state, regval, fault_iova, fault_pa;\r\nunsigned int fault_larb, fault_port;\r\nint_state = readl_relaxed(data->base + REG_MMU_FAULT_ST);\r\nfault_iova = readl_relaxed(data->base + REG_MMU_FAULT_VA);\r\nfault_iova &= F_MMU_FAULT_VA_MSK;\r\nfault_pa = readl_relaxed(data->base + REG_MMU_INVLD_PA);\r\nregval = readl_relaxed(data->base + REG_MMU_INT_ID);\r\nfault_larb = MT2701_M4U_TF_LARB(regval);\r\nfault_port = MT2701_M4U_TF_PORT(regval);\r\nif (report_iommu_fault(&dom->domain, data->dev, fault_iova,\r\nIOMMU_FAULT_READ))\r\ndev_err_ratelimited(data->dev,\r\n"fault type=0x%x iova=0x%x pa=0x%x larb=%d port=%d\n",\r\nint_state, fault_iova, fault_pa,\r\nfault_larb, fault_port);\r\nregval = readl_relaxed(data->base + REG_MMU_INT_CONTROL);\r\nregval |= F_INT_CLR_BIT;\r\nwritel_relaxed(regval, data->base + REG_MMU_INT_CONTROL);\r\nmtk_iommu_tlb_flush_all(data);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void mtk_iommu_config(struct mtk_iommu_data *data,\r\nstruct device *dev, bool enable)\r\n{\r\nstruct mtk_smi_larb_iommu *larb_mmu;\r\nunsigned int larbid, portid;\r\nstruct iommu_fwspec *fwspec = dev->iommu_fwspec;\r\nint i;\r\nfor (i = 0; i < fwspec->num_ids; ++i) {\r\nlarbid = mt2701_m4u_to_larb(fwspec->ids[i]);\r\nportid = mt2701_m4u_to_port(fwspec->ids[i]);\r\nlarb_mmu = &data->smi_imu.larb_imu[larbid];\r\ndev_dbg(dev, "%s iommu port: %d\n",\r\nenable ? "enable" : "disable", portid);\r\nif (enable)\r\nlarb_mmu->mmu |= MTK_SMI_MMU_EN(portid);\r\nelse\r\nlarb_mmu->mmu &= ~MTK_SMI_MMU_EN(portid);\r\n}\r\n}\r\nstatic int mtk_iommu_domain_finalise(struct mtk_iommu_data *data)\r\n{\r\nstruct mtk_iommu_domain *dom = data->m4u_dom;\r\nspin_lock_init(&dom->pgtlock);\r\ndom->pgt_va = dma_zalloc_coherent(data->dev,\r\nM2701_IOMMU_PGT_SIZE,\r\n&dom->pgt_pa, GFP_KERNEL);\r\nif (!dom->pgt_va)\r\nreturn -ENOMEM;\r\nwritel(dom->pgt_pa, data->base + REG_MMU_PT_BASE_ADDR);\r\ndom->data = data;\r\nreturn 0;\r\n}\r\nstatic struct iommu_domain *mtk_iommu_domain_alloc(unsigned type)\r\n{\r\nstruct mtk_iommu_domain *dom;\r\nif (type != IOMMU_DOMAIN_UNMANAGED)\r\nreturn NULL;\r\ndom = kzalloc(sizeof(*dom), GFP_KERNEL);\r\nif (!dom)\r\nreturn NULL;\r\nreturn &dom->domain;\r\n}\r\nstatic void mtk_iommu_domain_free(struct iommu_domain *domain)\r\n{\r\nstruct mtk_iommu_domain *dom = to_mtk_domain(domain);\r\nstruct mtk_iommu_data *data = dom->data;\r\ndma_free_coherent(data->dev, M2701_IOMMU_PGT_SIZE,\r\ndom->pgt_va, dom->pgt_pa);\r\nkfree(to_mtk_domain(domain));\r\n}\r\nstatic int mtk_iommu_attach_device(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct mtk_iommu_domain *dom = to_mtk_domain(domain);\r\nstruct mtk_iommu_data *data = dev->iommu_fwspec->iommu_priv;\r\nint ret;\r\nif (!data)\r\nreturn -ENODEV;\r\nif (!data->m4u_dom) {\r\ndata->m4u_dom = dom;\r\nret = mtk_iommu_domain_finalise(data);\r\nif (ret) {\r\ndata->m4u_dom = NULL;\r\nreturn ret;\r\n}\r\n}\r\nmtk_iommu_config(data, dev, true);\r\nreturn 0;\r\n}\r\nstatic void mtk_iommu_detach_device(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct mtk_iommu_data *data = dev->iommu_fwspec->iommu_priv;\r\nif (!data)\r\nreturn;\r\nmtk_iommu_config(data, dev, false);\r\n}\r\nstatic int mtk_iommu_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct mtk_iommu_domain *dom = to_mtk_domain(domain);\r\nunsigned int page_num = size >> MT2701_IOMMU_PAGE_SHIFT;\r\nunsigned long flags;\r\nunsigned int i;\r\nu32 *pgt_base_iova = dom->pgt_va + (iova >> MT2701_IOMMU_PAGE_SHIFT);\r\nu32 pabase = (u32)paddr;\r\nint map_size = 0;\r\nspin_lock_irqsave(&dom->pgtlock, flags);\r\nfor (i = 0; i < page_num; i++) {\r\nif (pgt_base_iova[i]) {\r\nmemset(pgt_base_iova, 0, i * sizeof(u32));\r\nbreak;\r\n}\r\npgt_base_iova[i] = pabase | F_DESC_VALID | F_DESC_NONSEC;\r\npabase += MT2701_IOMMU_PAGE_SIZE;\r\nmap_size += MT2701_IOMMU_PAGE_SIZE;\r\n}\r\nspin_unlock_irqrestore(&dom->pgtlock, flags);\r\nmtk_iommu_tlb_flush_range(dom->data, iova, size);\r\nreturn map_size == size ? 0 : -EEXIST;\r\n}\r\nstatic size_t mtk_iommu_unmap(struct iommu_domain *domain,\r\nunsigned long iova, size_t size)\r\n{\r\nstruct mtk_iommu_domain *dom = to_mtk_domain(domain);\r\nunsigned long flags;\r\nu32 *pgt_base_iova = dom->pgt_va + (iova >> MT2701_IOMMU_PAGE_SHIFT);\r\nunsigned int page_num = size >> MT2701_IOMMU_PAGE_SHIFT;\r\nspin_lock_irqsave(&dom->pgtlock, flags);\r\nmemset(pgt_base_iova, 0, page_num * sizeof(u32));\r\nspin_unlock_irqrestore(&dom->pgtlock, flags);\r\nmtk_iommu_tlb_flush_range(dom->data, iova, size);\r\nreturn size;\r\n}\r\nstatic phys_addr_t mtk_iommu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct mtk_iommu_domain *dom = to_mtk_domain(domain);\r\nunsigned long flags;\r\nphys_addr_t pa;\r\nspin_lock_irqsave(&dom->pgtlock, flags);\r\npa = *(dom->pgt_va + (iova >> MT2701_IOMMU_PAGE_SHIFT));\r\npa = pa & (~(MT2701_IOMMU_PAGE_SIZE - 1));\r\nspin_unlock_irqrestore(&dom->pgtlock, flags);\r\nreturn pa;\r\n}\r\nstatic int mtk_iommu_create_mapping(struct device *dev,\r\nstruct of_phandle_args *args)\r\n{\r\nstruct mtk_iommu_data *data;\r\nstruct platform_device *m4updev;\r\nstruct dma_iommu_mapping *mtk_mapping;\r\nstruct device *m4udev;\r\nint ret;\r\nif (args->args_count != 1) {\r\ndev_err(dev, "invalid #iommu-cells(%d) property for IOMMU\n",\r\nargs->args_count);\r\nreturn -EINVAL;\r\n}\r\nif (!dev->iommu_fwspec) {\r\nret = iommu_fwspec_init(dev, &args->np->fwnode, &mtk_iommu_ops);\r\nif (ret)\r\nreturn ret;\r\n} else if (dev->iommu_fwspec->ops != &mtk_iommu_ops) {\r\nreturn -EINVAL;\r\n}\r\nif (!dev->iommu_fwspec->iommu_priv) {\r\nm4updev = of_find_device_by_node(args->np);\r\nif (WARN_ON(!m4updev))\r\nreturn -EINVAL;\r\ndev->iommu_fwspec->iommu_priv = platform_get_drvdata(m4updev);\r\n}\r\nret = iommu_fwspec_add_ids(dev, args->args, 1);\r\nif (ret)\r\nreturn ret;\r\ndata = dev->iommu_fwspec->iommu_priv;\r\nm4udev = data->dev;\r\nmtk_mapping = m4udev->archdata.iommu;\r\nif (!mtk_mapping) {\r\nmtk_mapping = arm_iommu_create_mapping(&platform_bus_type,\r\n0, 1ULL << 32);\r\nif (IS_ERR(mtk_mapping))\r\nreturn PTR_ERR(mtk_mapping);\r\nm4udev->archdata.iommu = mtk_mapping;\r\n}\r\nret = arm_iommu_attach_device(dev, mtk_mapping);\r\nif (ret)\r\ngoto err_release_mapping;\r\nreturn 0;\r\nerr_release_mapping:\r\narm_iommu_release_mapping(mtk_mapping);\r\nm4udev->archdata.iommu = NULL;\r\nreturn ret;\r\n}\r\nstatic int mtk_iommu_add_device(struct device *dev)\r\n{\r\nstruct of_phandle_args iommu_spec;\r\nstruct of_phandle_iterator it;\r\nstruct mtk_iommu_data *data;\r\nstruct iommu_group *group;\r\nint err;\r\nof_for_each_phandle(&it, err, dev->of_node, "iommus",\r\n"#iommu-cells", 0) {\r\nint count = of_phandle_iterator_args(&it, iommu_spec.args,\r\nMAX_PHANDLE_ARGS);\r\niommu_spec.np = of_node_get(it.node);\r\niommu_spec.args_count = count;\r\nmtk_iommu_create_mapping(dev, &iommu_spec);\r\nof_node_put(iommu_spec.np);\r\n}\r\nif (!dev->iommu_fwspec || dev->iommu_fwspec->ops != &mtk_iommu_ops)\r\nreturn -ENODEV;\r\ndata = dev->iommu_fwspec->iommu_priv;\r\niommu_device_link(&data->iommu, dev);\r\ngroup = iommu_group_get_for_dev(dev);\r\nif (IS_ERR(group))\r\nreturn PTR_ERR(group);\r\niommu_group_put(group);\r\nreturn 0;\r\n}\r\nstatic void mtk_iommu_remove_device(struct device *dev)\r\n{\r\nstruct mtk_iommu_data *data;\r\nif (!dev->iommu_fwspec || dev->iommu_fwspec->ops != &mtk_iommu_ops)\r\nreturn;\r\ndata = dev->iommu_fwspec->iommu_priv;\r\niommu_device_unlink(&data->iommu, dev);\r\niommu_group_remove_device(dev);\r\niommu_fwspec_free(dev);\r\n}\r\nstatic struct iommu_group *mtk_iommu_device_group(struct device *dev)\r\n{\r\nstruct mtk_iommu_data *data = dev->iommu_fwspec->iommu_priv;\r\nif (!data)\r\nreturn ERR_PTR(-ENODEV);\r\nif (!data->m4u_group) {\r\ndata->m4u_group = iommu_group_alloc();\r\nif (IS_ERR(data->m4u_group))\r\ndev_err(dev, "Failed to allocate M4U IOMMU group\n");\r\n} else {\r\niommu_group_ref_get(data->m4u_group);\r\n}\r\nreturn data->m4u_group;\r\n}\r\nstatic int mtk_iommu_hw_init(const struct mtk_iommu_data *data)\r\n{\r\nu32 regval;\r\nint ret;\r\nret = clk_prepare_enable(data->bclk);\r\nif (ret) {\r\ndev_err(data->dev, "Failed to enable iommu bclk(%d)\n", ret);\r\nreturn ret;\r\n}\r\nregval = F_MMU_CTRL_COHERENT_EN | F_MMU_TF_PROTECT_SEL(2);\r\nwritel_relaxed(regval, data->base + REG_MMU_CTRL_REG);\r\nregval = F_INT_TRANSLATION_FAULT |\r\nF_INT_MAIN_MULTI_HIT_FAULT |\r\nF_INT_INVALID_PA_FAULT |\r\nF_INT_ENTRY_REPLACEMENT_FAULT |\r\nF_INT_TABLE_WALK_FAULT |\r\nF_INT_TLB_MISS_FAULT |\r\nF_INT_PFH_DMA_FIFO_OVERFLOW |\r\nF_INT_MISS_DMA_FIFO_OVERFLOW;\r\nwritel_relaxed(regval, data->base + REG_MMU_INT_CONTROL);\r\nwritel_relaxed(data->protect_base,\r\ndata->base + REG_MMU_IVRP_PADDR);\r\nwritel_relaxed(F_MMU_DCM_ON, data->base + REG_MMU_DCM);\r\nif (devm_request_irq(data->dev, data->irq, mtk_iommu_isr, 0,\r\ndev_name(data->dev), (void *)data)) {\r\nwritel_relaxed(0, data->base + REG_MMU_PT_BASE_ADDR);\r\nclk_disable_unprepare(data->bclk);\r\ndev_err(data->dev, "Failed @ IRQ-%d Request\n", data->irq);\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtk_iommu_probe(struct platform_device *pdev)\r\n{\r\nstruct mtk_iommu_data *data;\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *res;\r\nstruct component_match *match = NULL;\r\nstruct of_phandle_args larb_spec;\r\nstruct of_phandle_iterator it;\r\nvoid *protect;\r\nint larb_nr, ret, err;\r\ndata = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\ndata->dev = dev;\r\nprotect = devm_kzalloc(dev, MTK_PROTECT_PA_ALIGN * 2,\r\nGFP_KERNEL | GFP_DMA);\r\nif (!protect)\r\nreturn -ENOMEM;\r\ndata->protect_base = ALIGN(virt_to_phys(protect), MTK_PROTECT_PA_ALIGN);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndata->base = devm_ioremap_resource(dev, res);\r\nif (IS_ERR(data->base))\r\nreturn PTR_ERR(data->base);\r\ndata->irq = platform_get_irq(pdev, 0);\r\nif (data->irq < 0)\r\nreturn data->irq;\r\ndata->bclk = devm_clk_get(dev, "bclk");\r\nif (IS_ERR(data->bclk))\r\nreturn PTR_ERR(data->bclk);\r\nlarb_nr = 0;\r\nof_for_each_phandle(&it, err, dev->of_node,\r\n"mediatek,larbs", NULL, 0) {\r\nstruct platform_device *plarbdev;\r\nint count = of_phandle_iterator_args(&it, larb_spec.args,\r\nMAX_PHANDLE_ARGS);\r\nif (count)\r\ncontinue;\r\nlarb_spec.np = of_node_get(it.node);\r\nif (!of_device_is_available(larb_spec.np))\r\ncontinue;\r\nplarbdev = of_find_device_by_node(larb_spec.np);\r\nif (!plarbdev) {\r\nplarbdev = of_platform_device_create(\r\nlarb_spec.np, NULL,\r\nplatform_bus_type.dev_root);\r\nif (!plarbdev) {\r\nof_node_put(larb_spec.np);\r\nreturn -EPROBE_DEFER;\r\n}\r\n}\r\ndata->smi_imu.larb_imu[larb_nr].dev = &plarbdev->dev;\r\ncomponent_match_add_release(dev, &match, release_of,\r\ncompare_of, larb_spec.np);\r\nlarb_nr++;\r\n}\r\ndata->smi_imu.larb_nr = larb_nr;\r\nplatform_set_drvdata(pdev, data);\r\nret = mtk_iommu_hw_init(data);\r\nif (ret)\r\nreturn ret;\r\nret = iommu_device_sysfs_add(&data->iommu, &pdev->dev, NULL,\r\ndev_name(&pdev->dev));\r\nif (ret)\r\nreturn ret;\r\niommu_device_set_ops(&data->iommu, &mtk_iommu_ops);\r\nret = iommu_device_register(&data->iommu);\r\nif (ret)\r\nreturn ret;\r\nif (!iommu_present(&platform_bus_type))\r\nbus_set_iommu(&platform_bus_type, &mtk_iommu_ops);\r\nreturn component_master_add_with_match(dev, &mtk_iommu_com_ops, match);\r\n}\r\nstatic int mtk_iommu_remove(struct platform_device *pdev)\r\n{\r\nstruct mtk_iommu_data *data = platform_get_drvdata(pdev);\r\niommu_device_sysfs_remove(&data->iommu);\r\niommu_device_unregister(&data->iommu);\r\nif (iommu_present(&platform_bus_type))\r\nbus_set_iommu(&platform_bus_type, NULL);\r\nclk_disable_unprepare(data->bclk);\r\ndevm_free_irq(&pdev->dev, data->irq, data);\r\ncomponent_master_del(&pdev->dev, &mtk_iommu_com_ops);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused mtk_iommu_suspend(struct device *dev)\r\n{\r\nstruct mtk_iommu_data *data = dev_get_drvdata(dev);\r\nstruct mtk_iommu_suspend_reg *reg = &data->reg;\r\nvoid __iomem *base = data->base;\r\nreg->standard_axi_mode = readl_relaxed(base +\r\nREG_MMU_STANDARD_AXI_MODE);\r\nreg->dcm_dis = readl_relaxed(base + REG_MMU_DCM);\r\nreg->ctrl_reg = readl_relaxed(base + REG_MMU_CTRL_REG);\r\nreg->int_control0 = readl_relaxed(base + REG_MMU_INT_CONTROL);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused mtk_iommu_resume(struct device *dev)\r\n{\r\nstruct mtk_iommu_data *data = dev_get_drvdata(dev);\r\nstruct mtk_iommu_suspend_reg *reg = &data->reg;\r\nvoid __iomem *base = data->base;\r\nwritel_relaxed(data->m4u_dom->pgt_pa, base + REG_MMU_PT_BASE_ADDR);\r\nwritel_relaxed(reg->standard_axi_mode,\r\nbase + REG_MMU_STANDARD_AXI_MODE);\r\nwritel_relaxed(reg->dcm_dis, base + REG_MMU_DCM);\r\nwritel_relaxed(reg->ctrl_reg, base + REG_MMU_CTRL_REG);\r\nwritel_relaxed(reg->int_control0, base + REG_MMU_INT_CONTROL);\r\nwritel_relaxed(data->protect_base, base + REG_MMU_IVRP_PADDR);\r\nreturn 0;\r\n}\r\nstatic int __init m4u_init(void)\r\n{\r\nreturn platform_driver_register(&mtk_iommu_driver);\r\n}\r\nstatic void __exit m4u_exit(void)\r\n{\r\nreturn platform_driver_unregister(&mtk_iommu_driver);\r\n}
