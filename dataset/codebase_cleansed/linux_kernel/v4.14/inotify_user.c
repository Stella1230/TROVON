static inline __u32 inotify_arg_to_mask(u32 arg)\r\n{\r\n__u32 mask;\r\nmask = (FS_IN_IGNORED | FS_EVENT_ON_CHILD | FS_UNMOUNT);\r\nmask |= (arg & (IN_ALL_EVENTS | IN_ONESHOT | IN_EXCL_UNLINK));\r\nreturn mask;\r\n}\r\nstatic inline u32 inotify_mask_to_arg(__u32 mask)\r\n{\r\nreturn mask & (IN_ALL_EVENTS | IN_ISDIR | IN_UNMOUNT | IN_IGNORED |\r\nIN_Q_OVERFLOW);\r\n}\r\nstatic unsigned int inotify_poll(struct file *file, poll_table *wait)\r\n{\r\nstruct fsnotify_group *group = file->private_data;\r\nint ret = 0;\r\npoll_wait(file, &group->notification_waitq, wait);\r\nspin_lock(&group->notification_lock);\r\nif (!fsnotify_notify_queue_is_empty(group))\r\nret = POLLIN | POLLRDNORM;\r\nspin_unlock(&group->notification_lock);\r\nreturn ret;\r\n}\r\nstatic int round_event_name_len(struct fsnotify_event *fsn_event)\r\n{\r\nstruct inotify_event_info *event;\r\nevent = INOTIFY_E(fsn_event);\r\nif (!event->name_len)\r\nreturn 0;\r\nreturn roundup(event->name_len + 1, sizeof(struct inotify_event));\r\n}\r\nstatic struct fsnotify_event *get_one_event(struct fsnotify_group *group,\r\nsize_t count)\r\n{\r\nsize_t event_size = sizeof(struct inotify_event);\r\nstruct fsnotify_event *event;\r\nif (fsnotify_notify_queue_is_empty(group))\r\nreturn NULL;\r\nevent = fsnotify_peek_first_event(group);\r\npr_debug("%s: group=%p event=%p\n", __func__, group, event);\r\nevent_size += round_event_name_len(event);\r\nif (event_size > count)\r\nreturn ERR_PTR(-EINVAL);\r\nfsnotify_remove_first_event(group);\r\nreturn event;\r\n}\r\nstatic ssize_t copy_event_to_user(struct fsnotify_group *group,\r\nstruct fsnotify_event *fsn_event,\r\nchar __user *buf)\r\n{\r\nstruct inotify_event inotify_event;\r\nstruct inotify_event_info *event;\r\nsize_t event_size = sizeof(struct inotify_event);\r\nsize_t name_len;\r\nsize_t pad_name_len;\r\npr_debug("%s: group=%p event=%p\n", __func__, group, fsn_event);\r\nevent = INOTIFY_E(fsn_event);\r\nname_len = event->name_len;\r\npad_name_len = round_event_name_len(fsn_event);\r\ninotify_event.len = pad_name_len;\r\ninotify_event.mask = inotify_mask_to_arg(fsn_event->mask);\r\ninotify_event.wd = event->wd;\r\ninotify_event.cookie = event->sync_cookie;\r\nif (copy_to_user(buf, &inotify_event, event_size))\r\nreturn -EFAULT;\r\nbuf += event_size;\r\nif (pad_name_len) {\r\nif (copy_to_user(buf, event->name, name_len))\r\nreturn -EFAULT;\r\nbuf += name_len;\r\nif (clear_user(buf, pad_name_len - name_len))\r\nreturn -EFAULT;\r\nevent_size += pad_name_len;\r\n}\r\nreturn event_size;\r\n}\r\nstatic ssize_t inotify_read(struct file *file, char __user *buf,\r\nsize_t count, loff_t *pos)\r\n{\r\nstruct fsnotify_group *group;\r\nstruct fsnotify_event *kevent;\r\nchar __user *start;\r\nint ret;\r\nDEFINE_WAIT_FUNC(wait, woken_wake_function);\r\nstart = buf;\r\ngroup = file->private_data;\r\nadd_wait_queue(&group->notification_waitq, &wait);\r\nwhile (1) {\r\nspin_lock(&group->notification_lock);\r\nkevent = get_one_event(group, count);\r\nspin_unlock(&group->notification_lock);\r\npr_debug("%s: group=%p kevent=%p\n", __func__, group, kevent);\r\nif (kevent) {\r\nret = PTR_ERR(kevent);\r\nif (IS_ERR(kevent))\r\nbreak;\r\nret = copy_event_to_user(group, kevent, buf);\r\nfsnotify_destroy_event(group, kevent);\r\nif (ret < 0)\r\nbreak;\r\nbuf += ret;\r\ncount -= ret;\r\ncontinue;\r\n}\r\nret = -EAGAIN;\r\nif (file->f_flags & O_NONBLOCK)\r\nbreak;\r\nret = -ERESTARTSYS;\r\nif (signal_pending(current))\r\nbreak;\r\nif (start != buf)\r\nbreak;\r\nwait_woken(&wait, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\r\n}\r\nremove_wait_queue(&group->notification_waitq, &wait);\r\nif (start != buf && ret != -EFAULT)\r\nret = buf - start;\r\nreturn ret;\r\n}\r\nstatic int inotify_release(struct inode *ignored, struct file *file)\r\n{\r\nstruct fsnotify_group *group = file->private_data;\r\npr_debug("%s: group=%p\n", __func__, group);\r\nfsnotify_destroy_group(group);\r\nreturn 0;\r\n}\r\nstatic long inotify_ioctl(struct file *file, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nstruct fsnotify_group *group;\r\nstruct fsnotify_event *fsn_event;\r\nvoid __user *p;\r\nint ret = -ENOTTY;\r\nsize_t send_len = 0;\r\ngroup = file->private_data;\r\np = (void __user *) arg;\r\npr_debug("%s: group=%p cmd=%u\n", __func__, group, cmd);\r\nswitch (cmd) {\r\ncase FIONREAD:\r\nspin_lock(&group->notification_lock);\r\nlist_for_each_entry(fsn_event, &group->notification_list,\r\nlist) {\r\nsend_len += sizeof(struct inotify_event);\r\nsend_len += round_event_name_len(fsn_event);\r\n}\r\nspin_unlock(&group->notification_lock);\r\nret = put_user(send_len, (int __user *) p);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int inotify_find_inode(const char __user *dirname, struct path *path, unsigned flags)\r\n{\r\nint error;\r\nerror = user_path_at(AT_FDCWD, dirname, flags, path);\r\nif (error)\r\nreturn error;\r\nerror = inode_permission(path->dentry->d_inode, MAY_READ);\r\nif (error)\r\npath_put(path);\r\nreturn error;\r\n}\r\nstatic int inotify_add_to_idr(struct idr *idr, spinlock_t *idr_lock,\r\nstruct inotify_inode_mark *i_mark)\r\n{\r\nint ret;\r\nidr_preload(GFP_KERNEL);\r\nspin_lock(idr_lock);\r\nret = idr_alloc_cyclic(idr, i_mark, 1, 0, GFP_NOWAIT);\r\nif (ret >= 0) {\r\ni_mark->wd = ret;\r\nfsnotify_get_mark(&i_mark->fsn_mark);\r\n}\r\nspin_unlock(idr_lock);\r\nidr_preload_end();\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic struct inotify_inode_mark *inotify_idr_find_locked(struct fsnotify_group *group,\r\nint wd)\r\n{\r\nstruct idr *idr = &group->inotify_data.idr;\r\nspinlock_t *idr_lock = &group->inotify_data.idr_lock;\r\nstruct inotify_inode_mark *i_mark;\r\nassert_spin_locked(idr_lock);\r\ni_mark = idr_find(idr, wd);\r\nif (i_mark) {\r\nstruct fsnotify_mark *fsn_mark = &i_mark->fsn_mark;\r\nfsnotify_get_mark(fsn_mark);\r\nBUG_ON(atomic_read(&fsn_mark->refcnt) < 2);\r\n}\r\nreturn i_mark;\r\n}\r\nstatic struct inotify_inode_mark *inotify_idr_find(struct fsnotify_group *group,\r\nint wd)\r\n{\r\nstruct inotify_inode_mark *i_mark;\r\nspinlock_t *idr_lock = &group->inotify_data.idr_lock;\r\nspin_lock(idr_lock);\r\ni_mark = inotify_idr_find_locked(group, wd);\r\nspin_unlock(idr_lock);\r\nreturn i_mark;\r\n}\r\nstatic void inotify_remove_from_idr(struct fsnotify_group *group,\r\nstruct inotify_inode_mark *i_mark)\r\n{\r\nstruct idr *idr = &group->inotify_data.idr;\r\nspinlock_t *idr_lock = &group->inotify_data.idr_lock;\r\nstruct inotify_inode_mark *found_i_mark = NULL;\r\nint wd;\r\nspin_lock(idr_lock);\r\nwd = i_mark->wd;\r\nif (wd == -1) {\r\nWARN_ONCE(1, "%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\n",\r\n__func__, i_mark, i_mark->wd, i_mark->fsn_mark.group);\r\ngoto out;\r\n}\r\nfound_i_mark = inotify_idr_find_locked(group, wd);\r\nif (unlikely(!found_i_mark)) {\r\nWARN_ONCE(1, "%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\n",\r\n__func__, i_mark, i_mark->wd, i_mark->fsn_mark.group);\r\ngoto out;\r\n}\r\nif (unlikely(found_i_mark != i_mark)) {\r\nWARN_ONCE(1, "%s: i_mark=%p i_mark->wd=%d i_mark->group=%p "\r\n"found_i_mark=%p found_i_mark->wd=%d "\r\n"found_i_mark->group=%p\n", __func__, i_mark,\r\ni_mark->wd, i_mark->fsn_mark.group, found_i_mark,\r\nfound_i_mark->wd, found_i_mark->fsn_mark.group);\r\ngoto out;\r\n}\r\nif (unlikely(atomic_read(&i_mark->fsn_mark.refcnt) < 2)) {\r\nprintk(KERN_ERR "%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\n",\r\n__func__, i_mark, i_mark->wd, i_mark->fsn_mark.group);\r\nBUG();\r\n}\r\nidr_remove(idr, wd);\r\nfsnotify_put_mark(&i_mark->fsn_mark);\r\nout:\r\ni_mark->wd = -1;\r\nspin_unlock(idr_lock);\r\nif (found_i_mark)\r\nfsnotify_put_mark(&found_i_mark->fsn_mark);\r\n}\r\nvoid inotify_ignored_and_remove_idr(struct fsnotify_mark *fsn_mark,\r\nstruct fsnotify_group *group)\r\n{\r\nstruct inotify_inode_mark *i_mark;\r\ninotify_handle_event(group, NULL, fsn_mark, NULL, FS_IN_IGNORED,\r\nNULL, FSNOTIFY_EVENT_NONE, NULL, 0, NULL);\r\ni_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\r\ninotify_remove_from_idr(group, i_mark);\r\ndec_inotify_watches(group->inotify_data.ucounts);\r\n}\r\nstatic int inotify_update_existing_watch(struct fsnotify_group *group,\r\nstruct inode *inode,\r\nu32 arg)\r\n{\r\nstruct fsnotify_mark *fsn_mark;\r\nstruct inotify_inode_mark *i_mark;\r\n__u32 old_mask, new_mask;\r\n__u32 mask;\r\nint add = (arg & IN_MASK_ADD);\r\nint ret;\r\nmask = inotify_arg_to_mask(arg);\r\nfsn_mark = fsnotify_find_mark(&inode->i_fsnotify_marks, group);\r\nif (!fsn_mark)\r\nreturn -ENOENT;\r\ni_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\r\nspin_lock(&fsn_mark->lock);\r\nold_mask = fsn_mark->mask;\r\nif (add)\r\nfsn_mark->mask |= mask;\r\nelse\r\nfsn_mark->mask = mask;\r\nnew_mask = fsn_mark->mask;\r\nspin_unlock(&fsn_mark->lock);\r\nif (old_mask != new_mask) {\r\nint dropped = (old_mask & ~new_mask);\r\nint do_inode = (new_mask & ~inode->i_fsnotify_mask);\r\nif (dropped || do_inode)\r\nfsnotify_recalc_mask(inode->i_fsnotify_marks);\r\n}\r\nret = i_mark->wd;\r\nfsnotify_put_mark(fsn_mark);\r\nreturn ret;\r\n}\r\nstatic int inotify_new_watch(struct fsnotify_group *group,\r\nstruct inode *inode,\r\nu32 arg)\r\n{\r\nstruct inotify_inode_mark *tmp_i_mark;\r\n__u32 mask;\r\nint ret;\r\nstruct idr *idr = &group->inotify_data.idr;\r\nspinlock_t *idr_lock = &group->inotify_data.idr_lock;\r\nmask = inotify_arg_to_mask(arg);\r\ntmp_i_mark = kmem_cache_alloc(inotify_inode_mark_cachep, GFP_KERNEL);\r\nif (unlikely(!tmp_i_mark))\r\nreturn -ENOMEM;\r\nfsnotify_init_mark(&tmp_i_mark->fsn_mark, group);\r\ntmp_i_mark->fsn_mark.mask = mask;\r\ntmp_i_mark->wd = -1;\r\nret = inotify_add_to_idr(idr, idr_lock, tmp_i_mark);\r\nif (ret)\r\ngoto out_err;\r\nif (!inc_inotify_watches(group->inotify_data.ucounts)) {\r\ninotify_remove_from_idr(group, tmp_i_mark);\r\nret = -ENOSPC;\r\ngoto out_err;\r\n}\r\nret = fsnotify_add_mark_locked(&tmp_i_mark->fsn_mark, inode, NULL, 0);\r\nif (ret) {\r\ninotify_remove_from_idr(group, tmp_i_mark);\r\ngoto out_err;\r\n}\r\nret = tmp_i_mark->wd;\r\nout_err:\r\nfsnotify_put_mark(&tmp_i_mark->fsn_mark);\r\nreturn ret;\r\n}\r\nstatic int inotify_update_watch(struct fsnotify_group *group, struct inode *inode, u32 arg)\r\n{\r\nint ret = 0;\r\nmutex_lock(&group->mark_mutex);\r\nret = inotify_update_existing_watch(group, inode, arg);\r\nif (ret == -ENOENT)\r\nret = inotify_new_watch(group, inode, arg);\r\nmutex_unlock(&group->mark_mutex);\r\nreturn ret;\r\n}\r\nstatic struct fsnotify_group *inotify_new_group(unsigned int max_events)\r\n{\r\nstruct fsnotify_group *group;\r\nstruct inotify_event_info *oevent;\r\ngroup = fsnotify_alloc_group(&inotify_fsnotify_ops);\r\nif (IS_ERR(group))\r\nreturn group;\r\noevent = kmalloc(sizeof(struct inotify_event_info), GFP_KERNEL);\r\nif (unlikely(!oevent)) {\r\nfsnotify_destroy_group(group);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\ngroup->overflow_event = &oevent->fse;\r\nfsnotify_init_event(group->overflow_event, NULL, FS_Q_OVERFLOW);\r\noevent->wd = -1;\r\noevent->sync_cookie = 0;\r\noevent->name_len = 0;\r\ngroup->max_events = max_events;\r\nspin_lock_init(&group->inotify_data.idr_lock);\r\nidr_init(&group->inotify_data.idr);\r\ngroup->inotify_data.ucounts = inc_ucount(current_user_ns(),\r\ncurrent_euid(),\r\nUCOUNT_INOTIFY_INSTANCES);\r\nif (!group->inotify_data.ucounts) {\r\nfsnotify_destroy_group(group);\r\nreturn ERR_PTR(-EMFILE);\r\n}\r\nreturn group;\r\n}\r\nstatic int __init inotify_user_setup(void)\r\n{\r\nBUILD_BUG_ON(IN_ACCESS != FS_ACCESS);\r\nBUILD_BUG_ON(IN_MODIFY != FS_MODIFY);\r\nBUILD_BUG_ON(IN_ATTRIB != FS_ATTRIB);\r\nBUILD_BUG_ON(IN_CLOSE_WRITE != FS_CLOSE_WRITE);\r\nBUILD_BUG_ON(IN_CLOSE_NOWRITE != FS_CLOSE_NOWRITE);\r\nBUILD_BUG_ON(IN_OPEN != FS_OPEN);\r\nBUILD_BUG_ON(IN_MOVED_FROM != FS_MOVED_FROM);\r\nBUILD_BUG_ON(IN_MOVED_TO != FS_MOVED_TO);\r\nBUILD_BUG_ON(IN_CREATE != FS_CREATE);\r\nBUILD_BUG_ON(IN_DELETE != FS_DELETE);\r\nBUILD_BUG_ON(IN_DELETE_SELF != FS_DELETE_SELF);\r\nBUILD_BUG_ON(IN_MOVE_SELF != FS_MOVE_SELF);\r\nBUILD_BUG_ON(IN_UNMOUNT != FS_UNMOUNT);\r\nBUILD_BUG_ON(IN_Q_OVERFLOW != FS_Q_OVERFLOW);\r\nBUILD_BUG_ON(IN_IGNORED != FS_IN_IGNORED);\r\nBUILD_BUG_ON(IN_EXCL_UNLINK != FS_EXCL_UNLINK);\r\nBUILD_BUG_ON(IN_ISDIR != FS_ISDIR);\r\nBUILD_BUG_ON(IN_ONESHOT != FS_IN_ONESHOT);\r\nBUG_ON(hweight32(ALL_INOTIFY_BITS) != 21);\r\ninotify_inode_mark_cachep = KMEM_CACHE(inotify_inode_mark, SLAB_PANIC);\r\ninotify_max_queued_events = 16384;\r\ninit_user_ns.ucount_max[UCOUNT_INOTIFY_INSTANCES] = 128;\r\ninit_user_ns.ucount_max[UCOUNT_INOTIFY_WATCHES] = 8192;\r\nreturn 0;\r\n}
