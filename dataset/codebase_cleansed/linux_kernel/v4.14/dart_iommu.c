static inline void dart_tlb_invalidate_all(void)\r\n{\r\nunsigned long l = 0;\r\nunsigned int reg, inv_bit;\r\nunsigned long limit;\r\nunsigned long flags;\r\nspin_lock_irqsave(&invalidate_lock, flags);\r\nDBG("dart: flush\n");\r\nlimit = 0;\r\ninv_bit = dart_is_u4 ? DART_CNTL_U4_FLUSHTLB : DART_CNTL_U3_FLUSHTLB;\r\nretry:\r\nl = 0;\r\nreg = DART_IN(DART_CNTL);\r\nreg |= inv_bit;\r\nDART_OUT(DART_CNTL, reg);\r\nwhile ((DART_IN(DART_CNTL) & inv_bit) && l < (1L << limit))\r\nl++;\r\nif (l == (1L << limit)) {\r\nif (limit < 4) {\r\nlimit++;\r\nreg = DART_IN(DART_CNTL);\r\nreg &= ~inv_bit;\r\nDART_OUT(DART_CNTL, reg);\r\ngoto retry;\r\n} else\r\npanic("DART: TLB did not flush after waiting a long "\r\n"time. Buggy U3 ?");\r\n}\r\nspin_unlock_irqrestore(&invalidate_lock, flags);\r\n}\r\nstatic inline void dart_tlb_invalidate_one(unsigned long bus_rpn)\r\n{\r\nunsigned int reg;\r\nunsigned int l, limit;\r\nunsigned long flags;\r\nspin_lock_irqsave(&invalidate_lock, flags);\r\nreg = DART_CNTL_U4_ENABLE | DART_CNTL_U4_IONE |\r\n(bus_rpn & DART_CNTL_U4_IONE_MASK);\r\nDART_OUT(DART_CNTL, reg);\r\nlimit = 0;\r\nwait_more:\r\nl = 0;\r\nwhile ((DART_IN(DART_CNTL) & DART_CNTL_U4_IONE) && l < (1L << limit)) {\r\nrmb();\r\nl++;\r\n}\r\nif (l == (1L << limit)) {\r\nif (limit < 4) {\r\nlimit++;\r\ngoto wait_more;\r\n} else\r\npanic("DART: TLB did not flush after waiting a long "\r\n"time. Buggy U4 ?");\r\n}\r\nspin_unlock_irqrestore(&invalidate_lock, flags);\r\n}\r\nstatic void dart_cache_sync(unsigned int *base, unsigned int count)\r\n{\r\nunsigned long start = (unsigned long)base;\r\nunsigned long end = start + (count + 1) * sizeof(unsigned int);\r\nunsigned int tmp;\r\nflush_inval_dcache_range(start, end);\r\nasm volatile(" sync;"\r\n" isync;"\r\n" dcbf 0,%1;"\r\n" sync;"\r\n" isync;"\r\n" lwz %0,0(%1);"\r\n" isync" : "=r" (tmp) : "r" (end) : "memory");\r\n}\r\nstatic void dart_flush(struct iommu_table *tbl)\r\n{\r\nmb();\r\nif (dart_dirty) {\r\ndart_tlb_invalidate_all();\r\ndart_dirty = 0;\r\n}\r\n}\r\nstatic int dart_build(struct iommu_table *tbl, long index,\r\nlong npages, unsigned long uaddr,\r\nenum dma_data_direction direction,\r\nunsigned long attrs)\r\n{\r\nunsigned int *dp, *orig_dp;\r\nunsigned int rpn;\r\nlong l;\r\nDBG("dart: build at: %lx, %lx, addr: %x\n", index, npages, uaddr);\r\norig_dp = dp = ((unsigned int*)tbl->it_base) + index;\r\nl = npages;\r\nwhile (l--) {\r\nrpn = __pa(uaddr) >> DART_PAGE_SHIFT;\r\n*(dp++) = DARTMAP_VALID | (rpn & DARTMAP_RPNMASK);\r\nuaddr += DART_PAGE_SIZE;\r\n}\r\ndart_cache_sync(orig_dp, npages);\r\nif (dart_is_u4) {\r\nrpn = index;\r\nwhile (npages--)\r\ndart_tlb_invalidate_one(rpn++);\r\n} else {\r\ndart_dirty = 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void dart_free(struct iommu_table *tbl, long index, long npages)\r\n{\r\nunsigned int *dp, *orig_dp;\r\nlong orig_npages = npages;\r\nDBG("dart: free at: %lx, %lx\n", index, npages);\r\norig_dp = dp = ((unsigned int *)tbl->it_base) + index;\r\nwhile (npages--)\r\n*(dp++) = dart_emptyval;\r\ndart_cache_sync(orig_dp, orig_npages);\r\n}\r\nstatic void allocate_dart(void)\r\n{\r\nunsigned long tmp;\r\ndart_tablesize = 1UL << 21;\r\ndart_tablebase = __va(memblock_alloc_base(1UL<<24,\r\n1UL<<24, 0x80000000L));\r\nkmemleak_no_scan((void *)dart_tablebase);\r\ntmp = memblock_alloc(DART_PAGE_SIZE, DART_PAGE_SIZE);\r\ndart_emptyval = DARTMAP_VALID | ((tmp >> DART_PAGE_SHIFT) &\r\nDARTMAP_RPNMASK);\r\nprintk(KERN_INFO "DART table allocated at: %p\n", dart_tablebase);\r\n}\r\nstatic int __init dart_init(struct device_node *dart_node)\r\n{\r\nunsigned int i;\r\nunsigned long base, size;\r\nstruct resource r;\r\nif (iommu_is_off)\r\nreturn -ENODEV;\r\nif (!iommu_force_on && memblock_end_of_DRAM() <= 0x40000000ull)\r\nreturn -ENODEV;\r\nif (of_address_to_resource(dart_node, 0, &r))\r\npanic("DART: can't get register base ! ");\r\ndart = ioremap(r.start, resource_size(&r));\r\nif (dart == NULL)\r\npanic("DART: Cannot map registers!");\r\nallocate_dart();\r\nfor (i = 0; i < dart_tablesize/4; i++)\r\ndart_tablebase[i] = dart_emptyval;\r\ndart_cache_sync(dart_tablebase, dart_tablesize / sizeof(u32));\r\nbase = ((unsigned long)dart_tablebase) >> DART_PAGE_SHIFT;\r\nsize = dart_tablesize >> DART_PAGE_SHIFT;\r\nif (dart_is_u4) {\r\nsize &= DART_SIZE_U4_SIZE_MASK;\r\nDART_OUT(DART_BASE_U4, base);\r\nDART_OUT(DART_SIZE_U4, size);\r\nDART_OUT(DART_CNTL, DART_CNTL_U4_ENABLE);\r\n} else {\r\nsize &= DART_CNTL_U3_SIZE_MASK;\r\nDART_OUT(DART_CNTL,\r\nDART_CNTL_U3_ENABLE |\r\n(base << DART_CNTL_U3_BASE_SHIFT) |\r\n(size << DART_CNTL_U3_SIZE_SHIFT));\r\n}\r\ndart_tlb_invalidate_all();\r\nprintk(KERN_INFO "DART IOMMU initialized for %s type chipset\n",\r\ndart_is_u4 ? "U4" : "U3");\r\nreturn 0;\r\n}\r\nstatic void iommu_table_dart_setup(void)\r\n{\r\niommu_table_dart.it_busno = 0;\r\niommu_table_dart.it_offset = 0;\r\niommu_table_dart.it_size = dart_tablesize / sizeof(u32);\r\niommu_table_dart.it_page_shift = IOMMU_PAGE_SHIFT_4K;\r\niommu_table_dart.it_base = (unsigned long)dart_tablebase;\r\niommu_table_dart.it_index = 0;\r\niommu_table_dart.it_blocksize = 1;\r\niommu_table_dart.it_ops = &iommu_dart_ops;\r\niommu_init_table(&iommu_table_dart, -1);\r\nset_bit(iommu_table_dart.it_size - 1, iommu_table_dart.it_map);\r\n}\r\nstatic void pci_dma_dev_setup_dart(struct pci_dev *dev)\r\n{\r\nif (dart_is_u4)\r\nset_dma_offset(&dev->dev, DART_U4_BYPASS_BASE);\r\nset_iommu_table_base(&dev->dev, &iommu_table_dart);\r\n}\r\nstatic void pci_dma_bus_setup_dart(struct pci_bus *bus)\r\n{\r\nif (!iommu_table_dart_inited) {\r\niommu_table_dart_inited = 1;\r\niommu_table_dart_setup();\r\n}\r\n}\r\nstatic bool dart_device_on_pcie(struct device *dev)\r\n{\r\nstruct device_node *np = of_node_get(dev->of_node);\r\nwhile(np) {\r\nif (of_device_is_compatible(np, "U4-pcie") ||\r\nof_device_is_compatible(np, "u4-pcie")) {\r\nof_node_put(np);\r\nreturn true;\r\n}\r\nnp = of_get_next_parent(np);\r\n}\r\nreturn false;\r\n}\r\nstatic int dart_dma_set_mask(struct device *dev, u64 dma_mask)\r\n{\r\nif (!dev->dma_mask || !dma_supported(dev, dma_mask))\r\nreturn -EIO;\r\nif (dart_device_on_pcie(dev) && dma_mask >= DMA_BIT_MASK(40)) {\r\ndev_info(dev, "Using 64-bit DMA iommu bypass\n");\r\nset_dma_ops(dev, &dma_direct_ops);\r\n} else {\r\ndev_info(dev, "Using 32-bit DMA via iommu\n");\r\nset_dma_ops(dev, &dma_iommu_ops);\r\n}\r\n*dev->dma_mask = dma_mask;\r\nreturn 0;\r\n}\r\nvoid __init iommu_init_early_dart(struct pci_controller_ops *controller_ops)\r\n{\r\nstruct device_node *dn;\r\ndn = of_find_compatible_node(NULL, "dart", "u3-dart");\r\nif (dn == NULL) {\r\ndn = of_find_compatible_node(NULL, "dart", "u4-dart");\r\nif (dn == NULL)\r\nreturn;\r\ndart_is_u4 = 1;\r\n}\r\nif (dart_init(dn) != 0)\r\ngoto bail;\r\nif (dart_is_u4)\r\nppc_md.dma_set_mask = dart_dma_set_mask;\r\ncontroller_ops->dma_dev_setup = pci_dma_dev_setup_dart;\r\ncontroller_ops->dma_bus_setup = pci_dma_bus_setup_dart;\r\nset_pci_dma_ops(&dma_iommu_ops);\r\nreturn;\r\nbail:\r\ncontroller_ops->dma_dev_setup = NULL;\r\ncontroller_ops->dma_bus_setup = NULL;\r\nset_pci_dma_ops(&dma_direct_ops);\r\n}\r\nstatic void iommu_dart_restore(void)\r\n{\r\ndart_cache_sync(dart_tablebase, dart_tablesize / sizeof(u32));\r\ndart_tlb_invalidate_all();\r\n}\r\nstatic int __init iommu_init_late_dart(void)\r\n{\r\nif (!dart_tablebase)\r\nreturn 0;\r\nppc_md.iommu_restore = iommu_dart_restore;\r\nreturn 0;\r\n}
