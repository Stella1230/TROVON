static void si_ih_enable_interrupts(struct amdgpu_device *adev)\r\n{\r\nu32 ih_cntl = RREG32(IH_CNTL);\r\nu32 ih_rb_cntl = RREG32(IH_RB_CNTL);\r\nih_cntl |= ENABLE_INTR;\r\nih_rb_cntl |= IH_RB_ENABLE;\r\nWREG32(IH_CNTL, ih_cntl);\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nadev->irq.ih.enabled = true;\r\n}\r\nstatic void si_ih_disable_interrupts(struct amdgpu_device *adev)\r\n{\r\nu32 ih_rb_cntl = RREG32(IH_RB_CNTL);\r\nu32 ih_cntl = RREG32(IH_CNTL);\r\nih_rb_cntl &= ~IH_RB_ENABLE;\r\nih_cntl &= ~ENABLE_INTR;\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nWREG32(IH_CNTL, ih_cntl);\r\nWREG32(IH_RB_RPTR, 0);\r\nWREG32(IH_RB_WPTR, 0);\r\nadev->irq.ih.enabled = false;\r\nadev->irq.ih.rptr = 0;\r\n}\r\nstatic int si_ih_irq_init(struct amdgpu_device *adev)\r\n{\r\nint rb_bufsz;\r\nu32 interrupt_cntl, ih_cntl, ih_rb_cntl;\r\nu64 wptr_off;\r\nsi_ih_disable_interrupts(adev);\r\nWREG32(INTERRUPT_CNTL2, adev->irq.ih.gpu_addr >> 8);\r\ninterrupt_cntl = RREG32(INTERRUPT_CNTL);\r\ninterrupt_cntl &= ~IH_DUMMY_RD_OVERRIDE;\r\ninterrupt_cntl &= ~IH_REQ_NONSNOOP_EN;\r\nWREG32(INTERRUPT_CNTL, interrupt_cntl);\r\nWREG32(IH_RB_BASE, adev->irq.ih.gpu_addr >> 8);\r\nrb_bufsz = order_base_2(adev->irq.ih.ring_size / 4);\r\nih_rb_cntl = IH_WPTR_OVERFLOW_ENABLE |\r\nIH_WPTR_OVERFLOW_CLEAR |\r\n(rb_bufsz << 1) |\r\nIH_WPTR_WRITEBACK_ENABLE;\r\nwptr_off = adev->wb.gpu_addr + (adev->irq.ih.wptr_offs * 4);\r\nWREG32(IH_RB_WPTR_ADDR_LO, lower_32_bits(wptr_off));\r\nWREG32(IH_RB_WPTR_ADDR_HI, upper_32_bits(wptr_off) & 0xFF);\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nWREG32(IH_RB_RPTR, 0);\r\nWREG32(IH_RB_WPTR, 0);\r\nih_cntl = MC_WRREQ_CREDIT(0x10) | MC_WR_CLEAN_CNT(0x10) | MC_VMID(0);\r\nif (adev->irq.msi_enabled)\r\nih_cntl |= RPTR_REARM;\r\nWREG32(IH_CNTL, ih_cntl);\r\npci_set_master(adev->pdev);\r\nsi_ih_enable_interrupts(adev);\r\nreturn 0;\r\n}\r\nstatic void si_ih_irq_disable(struct amdgpu_device *adev)\r\n{\r\nsi_ih_disable_interrupts(adev);\r\nmdelay(1);\r\n}\r\nstatic u32 si_ih_get_wptr(struct amdgpu_device *adev)\r\n{\r\nu32 wptr, tmp;\r\nwptr = le32_to_cpu(adev->wb.wb[adev->irq.ih.wptr_offs]);\r\nif (wptr & IH_RB_WPTR__RB_OVERFLOW_MASK) {\r\nwptr &= ~IH_RB_WPTR__RB_OVERFLOW_MASK;\r\ndev_warn(adev->dev, "IH ring buffer overflow (0x%08X, 0x%08X, 0x%08X)\n",\r\nwptr, adev->irq.ih.rptr, (wptr + 16) & adev->irq.ih.ptr_mask);\r\nadev->irq.ih.rptr = (wptr + 16) & adev->irq.ih.ptr_mask;\r\ntmp = RREG32(IH_RB_CNTL);\r\ntmp |= IH_RB_CNTL__WPTR_OVERFLOW_CLEAR_MASK;\r\nWREG32(IH_RB_CNTL, tmp);\r\n}\r\nreturn (wptr & adev->irq.ih.ptr_mask);\r\n}\r\nstatic void si_ih_decode_iv(struct amdgpu_device *adev,\r\nstruct amdgpu_iv_entry *entry)\r\n{\r\nu32 ring_index = adev->irq.ih.rptr >> 2;\r\nuint32_t dw[4];\r\ndw[0] = le32_to_cpu(adev->irq.ih.ring[ring_index + 0]);\r\ndw[1] = le32_to_cpu(adev->irq.ih.ring[ring_index + 1]);\r\ndw[2] = le32_to_cpu(adev->irq.ih.ring[ring_index + 2]);\r\ndw[3] = le32_to_cpu(adev->irq.ih.ring[ring_index + 3]);\r\nentry->client_id = AMDGPU_IH_CLIENTID_LEGACY;\r\nentry->src_id = dw[0] & 0xff;\r\nentry->src_data[0] = dw[1] & 0xfffffff;\r\nentry->ring_id = dw[2] & 0xff;\r\nentry->vm_id = (dw[2] >> 8) & 0xff;\r\nadev->irq.ih.rptr += 16;\r\n}\r\nstatic void si_ih_set_rptr(struct amdgpu_device *adev)\r\n{\r\nWREG32(IH_RB_RPTR, adev->irq.ih.rptr);\r\n}\r\nstatic int si_ih_early_init(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nsi_ih_set_interrupt_funcs(adev);\r\nreturn 0;\r\n}\r\nstatic int si_ih_sw_init(void *handle)\r\n{\r\nint r;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nr = amdgpu_ih_ring_init(adev, 64 * 1024, false);\r\nif (r)\r\nreturn r;\r\nreturn amdgpu_irq_init(adev);\r\n}\r\nstatic int si_ih_sw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\namdgpu_irq_fini(adev);\r\namdgpu_ih_ring_fini(adev);\r\nreturn 0;\r\n}\r\nstatic int si_ih_hw_init(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nreturn si_ih_irq_init(adev);\r\n}\r\nstatic int si_ih_hw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nsi_ih_irq_disable(adev);\r\nreturn 0;\r\n}\r\nstatic int si_ih_suspend(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nreturn si_ih_hw_fini(adev);\r\n}\r\nstatic int si_ih_resume(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nreturn si_ih_hw_init(adev);\r\n}\r\nstatic bool si_ih_is_idle(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nu32 tmp = RREG32(SRBM_STATUS);\r\nif (tmp & SRBM_STATUS__IH_BUSY_MASK)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int si_ih_wait_for_idle(void *handle)\r\n{\r\nunsigned i;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nfor (i = 0; i < adev->usec_timeout; i++) {\r\nif (si_ih_is_idle(handle))\r\nreturn 0;\r\nudelay(1);\r\n}\r\nreturn -ETIMEDOUT;\r\n}\r\nstatic int si_ih_soft_reset(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nu32 srbm_soft_reset = 0;\r\nu32 tmp = RREG32(SRBM_STATUS);\r\nif (tmp & SRBM_STATUS__IH_BUSY_MASK)\r\nsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_IH_MASK;\r\nif (srbm_soft_reset) {\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\ntmp |= srbm_soft_reset;\r\ndev_info(adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~srbm_soft_reset;\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\nudelay(50);\r\n}\r\nreturn 0;\r\n}\r\nstatic int si_ih_set_clockgating_state(void *handle,\r\nenum amd_clockgating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic int si_ih_set_powergating_state(void *handle,\r\nenum amd_powergating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic void si_ih_set_interrupt_funcs(struct amdgpu_device *adev)\r\n{\r\nif (adev->irq.ih_funcs == NULL)\r\nadev->irq.ih_funcs = &si_ih_funcs;\r\n}
