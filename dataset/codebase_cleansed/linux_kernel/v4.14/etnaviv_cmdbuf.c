struct etnaviv_cmdbuf_suballoc *\r\netnaviv_cmdbuf_suballoc_new(struct etnaviv_gpu * gpu)\r\n{\r\nstruct etnaviv_cmdbuf_suballoc *suballoc;\r\nint ret;\r\nsuballoc = kzalloc(sizeof(*suballoc), GFP_KERNEL);\r\nif (!suballoc)\r\nreturn ERR_PTR(-ENOMEM);\r\nsuballoc->gpu = gpu;\r\nmutex_init(&suballoc->lock);\r\ninit_waitqueue_head(&suballoc->free_event);\r\nsuballoc->vaddr = dma_alloc_wc(gpu->dev, SUBALLOC_SIZE,\r\n&suballoc->paddr, GFP_KERNEL);\r\nif (!suballoc->vaddr)\r\ngoto free_suballoc;\r\nret = etnaviv_iommu_get_suballoc_va(gpu, suballoc->paddr,\r\n&suballoc->vram_node, SUBALLOC_SIZE,\r\n&suballoc->iova);\r\nif (ret)\r\ngoto free_dma;\r\nreturn suballoc;\r\nfree_dma:\r\ndma_free_wc(gpu->dev, SUBALLOC_SIZE, suballoc->vaddr, suballoc->paddr);\r\nfree_suballoc:\r\nkfree(suballoc);\r\nreturn NULL;\r\n}\r\nvoid etnaviv_cmdbuf_suballoc_destroy(struct etnaviv_cmdbuf_suballoc *suballoc)\r\n{\r\netnaviv_iommu_put_suballoc_va(suballoc->gpu, &suballoc->vram_node,\r\nSUBALLOC_SIZE, suballoc->iova);\r\ndma_free_wc(suballoc->gpu->dev, SUBALLOC_SIZE, suballoc->vaddr,\r\nsuballoc->paddr);\r\nkfree(suballoc);\r\n}\r\nstruct etnaviv_cmdbuf *\r\netnaviv_cmdbuf_new(struct etnaviv_cmdbuf_suballoc *suballoc, u32 size,\r\nsize_t nr_bos)\r\n{\r\nstruct etnaviv_cmdbuf *cmdbuf;\r\nsize_t sz = size_vstruct(nr_bos, sizeof(cmdbuf->bo_map[0]),\r\nsizeof(*cmdbuf));\r\nint granule_offs, order, ret;\r\ncmdbuf = kzalloc(sz, GFP_KERNEL);\r\nif (!cmdbuf)\r\nreturn NULL;\r\ncmdbuf->suballoc = suballoc;\r\ncmdbuf->size = size;\r\norder = order_base_2(ALIGN(size, SUBALLOC_GRANULE) / SUBALLOC_GRANULE);\r\nretry:\r\nmutex_lock(&suballoc->lock);\r\ngranule_offs = bitmap_find_free_region(suballoc->granule_map,\r\nSUBALLOC_GRANULES, order);\r\nif (granule_offs < 0) {\r\nsuballoc->free_space = 0;\r\nmutex_unlock(&suballoc->lock);\r\nret = wait_event_interruptible_timeout(suballoc->free_event,\r\nsuballoc->free_space,\r\nmsecs_to_jiffies(10 * 1000));\r\nif (!ret) {\r\ndev_err(suballoc->gpu->dev,\r\n"Timeout waiting for cmdbuf space\n");\r\nreturn NULL;\r\n}\r\ngoto retry;\r\n}\r\nmutex_unlock(&suballoc->lock);\r\ncmdbuf->suballoc_offset = granule_offs * SUBALLOC_GRANULE;\r\ncmdbuf->vaddr = suballoc->vaddr + cmdbuf->suballoc_offset;\r\nreturn cmdbuf;\r\n}\r\nvoid etnaviv_cmdbuf_free(struct etnaviv_cmdbuf *cmdbuf)\r\n{\r\nstruct etnaviv_cmdbuf_suballoc *suballoc = cmdbuf->suballoc;\r\nint order = order_base_2(ALIGN(cmdbuf->size, SUBALLOC_GRANULE) /\r\nSUBALLOC_GRANULE);\r\nmutex_lock(&suballoc->lock);\r\nbitmap_release_region(suballoc->granule_map,\r\ncmdbuf->suballoc_offset / SUBALLOC_GRANULE,\r\norder);\r\nsuballoc->free_space = 1;\r\nmutex_unlock(&suballoc->lock);\r\nwake_up_all(&suballoc->free_event);\r\nkfree(cmdbuf);\r\n}\r\nu32 etnaviv_cmdbuf_get_va(struct etnaviv_cmdbuf *buf)\r\n{\r\nreturn buf->suballoc->iova + buf->suballoc_offset;\r\n}\r\ndma_addr_t etnaviv_cmdbuf_get_pa(struct etnaviv_cmdbuf *buf)\r\n{\r\nreturn buf->suballoc->paddr + buf->suballoc_offset;\r\n}
