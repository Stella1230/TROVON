static void *arm_nommu_dma_alloc(struct device *dev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t gfp,\r\nunsigned long attrs)\r\n{\r\nconst struct dma_map_ops *ops = &dma_noop_ops;\r\nvoid *ret;\r\nif (attrs & DMA_ATTR_NON_CONSISTENT)\r\nreturn ops->alloc(dev, size, dma_handle, gfp, attrs);\r\nret = dma_alloc_from_global_coherent(size, dma_handle);\r\nWARN_ON_ONCE(ret == NULL);\r\nreturn ret;\r\n}\r\nstatic void arm_nommu_dma_free(struct device *dev, size_t size,\r\nvoid *cpu_addr, dma_addr_t dma_addr,\r\nunsigned long attrs)\r\n{\r\nconst struct dma_map_ops *ops = &dma_noop_ops;\r\nif (attrs & DMA_ATTR_NON_CONSISTENT) {\r\nops->free(dev, size, cpu_addr, dma_addr, attrs);\r\n} else {\r\nint ret = dma_release_from_global_coherent(get_order(size),\r\ncpu_addr);\r\nWARN_ON_ONCE(ret == 0);\r\n}\r\nreturn;\r\n}\r\nstatic int arm_nommu_dma_mmap(struct device *dev, struct vm_area_struct *vma,\r\nvoid *cpu_addr, dma_addr_t dma_addr, size_t size,\r\nunsigned long attrs)\r\n{\r\nint ret;\r\nif (dma_mmap_from_global_coherent(vma, cpu_addr, size, &ret))\r\nreturn ret;\r\nreturn dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);\r\n}\r\nstatic void __dma_page_cpu_to_dev(phys_addr_t paddr, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\ndmac_map_area(__va(paddr), size, dir);\r\nif (dir == DMA_FROM_DEVICE)\r\nouter_inv_range(paddr, paddr + size);\r\nelse\r\nouter_clean_range(paddr, paddr + size);\r\n}\r\nstatic void __dma_page_dev_to_cpu(phys_addr_t paddr, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\nif (dir != DMA_TO_DEVICE) {\r\nouter_inv_range(paddr, paddr + size);\r\ndmac_unmap_area(__va(paddr), size, dir);\r\n}\r\n}\r\nstatic dma_addr_t arm_nommu_dma_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\ndma_addr_t handle = page_to_phys(page) + offset;\r\n__dma_page_cpu_to_dev(handle, size, dir);\r\nreturn handle;\r\n}\r\nstatic void arm_nommu_dma_unmap_page(struct device *dev, dma_addr_t handle,\r\nsize_t size, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\n__dma_page_dev_to_cpu(handle, size, dir);\r\n}\r\nstatic int arm_nommu_dma_map_sg(struct device *dev, struct scatterlist *sgl,\r\nint nents, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nint i;\r\nstruct scatterlist *sg;\r\nfor_each_sg(sgl, sg, nents, i) {\r\nsg_dma_address(sg) = sg_phys(sg);\r\nsg_dma_len(sg) = sg->length;\r\n__dma_page_cpu_to_dev(sg_dma_address(sg), sg_dma_len(sg), dir);\r\n}\r\nreturn nents;\r\n}\r\nstatic void arm_nommu_dma_unmap_sg(struct device *dev, struct scatterlist *sgl,\r\nint nents, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, nents, i)\r\n__dma_page_dev_to_cpu(sg_dma_address(sg), sg_dma_len(sg), dir);\r\n}\r\nstatic void arm_nommu_dma_sync_single_for_device(struct device *dev,\r\ndma_addr_t handle, size_t size, enum dma_data_direction dir)\r\n{\r\n__dma_page_cpu_to_dev(handle, size, dir);\r\n}\r\nstatic void arm_nommu_dma_sync_single_for_cpu(struct device *dev,\r\ndma_addr_t handle, size_t size, enum dma_data_direction dir)\r\n{\r\n__dma_page_cpu_to_dev(handle, size, dir);\r\n}\r\nstatic void arm_nommu_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,\r\nint nents, enum dma_data_direction dir)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, nents, i)\r\n__dma_page_cpu_to_dev(sg_dma_address(sg), sg_dma_len(sg), dir);\r\n}\r\nstatic void arm_nommu_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sgl,\r\nint nents, enum dma_data_direction dir)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, nents, i)\r\n__dma_page_dev_to_cpu(sg_dma_address(sg), sg_dma_len(sg), dir);\r\n}\r\nstatic const struct dma_map_ops *arm_nommu_get_dma_map_ops(bool coherent)\r\n{\r\nreturn coherent ? &dma_noop_ops : &arm_nommu_dma_ops;\r\n}\r\nvoid arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,\r\nconst struct iommu_ops *iommu, bool coherent)\r\n{\r\nconst struct dma_map_ops *dma_ops;\r\nif (IS_ENABLED(CONFIG_CPU_V7M)) {\r\ndev->archdata.dma_coherent = (cacheid) ? coherent : true;\r\n} else {\r\ndev->archdata.dma_coherent = (get_cr() & CR_M) ? coherent : true;\r\n}\r\ndma_ops = arm_nommu_get_dma_map_ops(dev->archdata.dma_coherent);\r\nset_dma_ops(dev, dma_ops);\r\n}\r\nvoid arch_teardown_dma_ops(struct device *dev)\r\n{\r\n}\r\nstatic int __init dma_debug_do_init(void)\r\n{\r\ndma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);\r\nreturn 0;\r\n}
