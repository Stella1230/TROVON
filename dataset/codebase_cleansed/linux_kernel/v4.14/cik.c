int cik_get_allowed_info_register(struct radeon_device *rdev,\r\nu32 reg, u32 *val)\r\n{\r\nswitch (reg) {\r\ncase GRBM_STATUS:\r\ncase GRBM_STATUS2:\r\ncase GRBM_STATUS_SE0:\r\ncase GRBM_STATUS_SE1:\r\ncase GRBM_STATUS_SE2:\r\ncase GRBM_STATUS_SE3:\r\ncase SRBM_STATUS:\r\ncase SRBM_STATUS2:\r\ncase (SDMA0_STATUS_REG + SDMA0_REGISTER_OFFSET):\r\ncase (SDMA0_STATUS_REG + SDMA1_REGISTER_OFFSET):\r\ncase UVD_STATUS:\r\n*val = RREG32(reg);\r\nreturn 0;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nu32 cik_didt_rreg(struct radeon_device *rdev, u32 reg)\r\n{\r\nunsigned long flags;\r\nu32 r;\r\nspin_lock_irqsave(&rdev->didt_idx_lock, flags);\r\nWREG32(CIK_DIDT_IND_INDEX, (reg));\r\nr = RREG32(CIK_DIDT_IND_DATA);\r\nspin_unlock_irqrestore(&rdev->didt_idx_lock, flags);\r\nreturn r;\r\n}\r\nvoid cik_didt_wreg(struct radeon_device *rdev, u32 reg, u32 v)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&rdev->didt_idx_lock, flags);\r\nWREG32(CIK_DIDT_IND_INDEX, (reg));\r\nWREG32(CIK_DIDT_IND_DATA, (v));\r\nspin_unlock_irqrestore(&rdev->didt_idx_lock, flags);\r\n}\r\nint ci_get_temp(struct radeon_device *rdev)\r\n{\r\nu32 temp;\r\nint actual_temp = 0;\r\ntemp = (RREG32_SMC(CG_MULT_THERMAL_STATUS) & CTF_TEMP_MASK) >>\r\nCTF_TEMP_SHIFT;\r\nif (temp & 0x200)\r\nactual_temp = 255;\r\nelse\r\nactual_temp = temp & 0x1ff;\r\nactual_temp = actual_temp * 1000;\r\nreturn actual_temp;\r\n}\r\nint kv_get_temp(struct radeon_device *rdev)\r\n{\r\nu32 temp;\r\nint actual_temp = 0;\r\ntemp = RREG32_SMC(0xC0300E0C);\r\nif (temp)\r\nactual_temp = (temp / 8) - 49;\r\nelse\r\nactual_temp = 0;\r\nactual_temp = actual_temp * 1000;\r\nreturn actual_temp;\r\n}\r\nu32 cik_pciep_rreg(struct radeon_device *rdev, u32 reg)\r\n{\r\nunsigned long flags;\r\nu32 r;\r\nspin_lock_irqsave(&rdev->pciep_idx_lock, flags);\r\nWREG32(PCIE_INDEX, reg);\r\n(void)RREG32(PCIE_INDEX);\r\nr = RREG32(PCIE_DATA);\r\nspin_unlock_irqrestore(&rdev->pciep_idx_lock, flags);\r\nreturn r;\r\n}\r\nvoid cik_pciep_wreg(struct radeon_device *rdev, u32 reg, u32 v)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&rdev->pciep_idx_lock, flags);\r\nWREG32(PCIE_INDEX, reg);\r\n(void)RREG32(PCIE_INDEX);\r\nWREG32(PCIE_DATA, v);\r\n(void)RREG32(PCIE_DATA);\r\nspin_unlock_irqrestore(&rdev->pciep_idx_lock, flags);\r\n}\r\nstatic void cik_init_golden_registers(struct radeon_device *rdev)\r\n{\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\nswitch (rdev->family) {\r\ncase CHIP_BONAIRE:\r\nradeon_program_register_sequence(rdev,\r\nbonaire_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(bonaire_mgcg_cgcg_init));\r\nradeon_program_register_sequence(rdev,\r\nbonaire_golden_registers,\r\n(const u32)ARRAY_SIZE(bonaire_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\nbonaire_golden_common_registers,\r\n(const u32)ARRAY_SIZE(bonaire_golden_common_registers));\r\nradeon_program_register_sequence(rdev,\r\nbonaire_golden_spm_registers,\r\n(const u32)ARRAY_SIZE(bonaire_golden_spm_registers));\r\nbreak;\r\ncase CHIP_KABINI:\r\nradeon_program_register_sequence(rdev,\r\nkalindi_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(kalindi_mgcg_cgcg_init));\r\nradeon_program_register_sequence(rdev,\r\nkalindi_golden_registers,\r\n(const u32)ARRAY_SIZE(kalindi_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\nkalindi_golden_common_registers,\r\n(const u32)ARRAY_SIZE(kalindi_golden_common_registers));\r\nradeon_program_register_sequence(rdev,\r\nkalindi_golden_spm_registers,\r\n(const u32)ARRAY_SIZE(kalindi_golden_spm_registers));\r\nbreak;\r\ncase CHIP_MULLINS:\r\nradeon_program_register_sequence(rdev,\r\nkalindi_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(kalindi_mgcg_cgcg_init));\r\nradeon_program_register_sequence(rdev,\r\ngodavari_golden_registers,\r\n(const u32)ARRAY_SIZE(godavari_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\nkalindi_golden_common_registers,\r\n(const u32)ARRAY_SIZE(kalindi_golden_common_registers));\r\nradeon_program_register_sequence(rdev,\r\nkalindi_golden_spm_registers,\r\n(const u32)ARRAY_SIZE(kalindi_golden_spm_registers));\r\nbreak;\r\ncase CHIP_KAVERI:\r\nradeon_program_register_sequence(rdev,\r\nspectre_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(spectre_mgcg_cgcg_init));\r\nradeon_program_register_sequence(rdev,\r\nspectre_golden_registers,\r\n(const u32)ARRAY_SIZE(spectre_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\nspectre_golden_common_registers,\r\n(const u32)ARRAY_SIZE(spectre_golden_common_registers));\r\nradeon_program_register_sequence(rdev,\r\nspectre_golden_spm_registers,\r\n(const u32)ARRAY_SIZE(spectre_golden_spm_registers));\r\nbreak;\r\ncase CHIP_HAWAII:\r\nradeon_program_register_sequence(rdev,\r\nhawaii_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(hawaii_mgcg_cgcg_init));\r\nradeon_program_register_sequence(rdev,\r\nhawaii_golden_registers,\r\n(const u32)ARRAY_SIZE(hawaii_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\nhawaii_golden_common_registers,\r\n(const u32)ARRAY_SIZE(hawaii_golden_common_registers));\r\nradeon_program_register_sequence(rdev,\r\nhawaii_golden_spm_registers,\r\n(const u32)ARRAY_SIZE(hawaii_golden_spm_registers));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\n}\r\nu32 cik_get_xclk(struct radeon_device *rdev)\r\n{\r\nu32 reference_clock = rdev->clock.spll.reference_freq;\r\nif (rdev->flags & RADEON_IS_IGP) {\r\nif (RREG32_SMC(GENERAL_PWRMGT) & GPU_COUNTER_CLK)\r\nreturn reference_clock / 2;\r\n} else {\r\nif (RREG32_SMC(CG_CLKPIN_CNTL) & XTALIN_DIVIDE)\r\nreturn reference_clock / 4;\r\n}\r\nreturn reference_clock;\r\n}\r\nu32 cik_mm_rdoorbell(struct radeon_device *rdev, u32 index)\r\n{\r\nif (index < rdev->doorbell.num_doorbells) {\r\nreturn readl(rdev->doorbell.ptr + index);\r\n} else {\r\nDRM_ERROR("reading beyond doorbell aperture: 0x%08x!\n", index);\r\nreturn 0;\r\n}\r\n}\r\nvoid cik_mm_wdoorbell(struct radeon_device *rdev, u32 index, u32 v)\r\n{\r\nif (index < rdev->doorbell.num_doorbells) {\r\nwritel(v, rdev->doorbell.ptr + index);\r\n} else {\r\nDRM_ERROR("writing beyond doorbell aperture: 0x%08x!\n", index);\r\n}\r\n}\r\nstatic void cik_srbm_select(struct radeon_device *rdev,\r\nu32 me, u32 pipe, u32 queue, u32 vmid)\r\n{\r\nu32 srbm_gfx_cntl = (PIPEID(pipe & 0x3) |\r\nMEID(me & 0x3) |\r\nVMID(vmid & 0xf) |\r\nQUEUEID(queue & 0x7));\r\nWREG32(SRBM_GFX_CNTL, srbm_gfx_cntl);\r\n}\r\nint ci_mc_load_microcode(struct radeon_device *rdev)\r\n{\r\nconst __be32 *fw_data = NULL;\r\nconst __le32 *new_fw_data = NULL;\r\nu32 running, tmp;\r\nu32 *io_mc_regs = NULL;\r\nconst __le32 *new_io_mc_regs = NULL;\r\nint i, regs_size, ucode_size;\r\nif (!rdev->mc_fw)\r\nreturn -EINVAL;\r\nif (rdev->new_fw) {\r\nconst struct mc_firmware_header_v1_0 *hdr =\r\n(const struct mc_firmware_header_v1_0 *)rdev->mc_fw->data;\r\nradeon_ucode_print_mc_hdr(&hdr->header);\r\nregs_size = le32_to_cpu(hdr->io_debug_size_bytes) / (4 * 2);\r\nnew_io_mc_regs = (const __le32 *)\r\n(rdev->mc_fw->data + le32_to_cpu(hdr->io_debug_array_offset_bytes));\r\nucode_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\r\nnew_fw_data = (const __le32 *)\r\n(rdev->mc_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\n} else {\r\nucode_size = rdev->mc_fw->size / 4;\r\nswitch (rdev->family) {\r\ncase CHIP_BONAIRE:\r\nio_mc_regs = (u32 *)&bonaire_io_mc_regs;\r\nregs_size = BONAIRE_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_HAWAII:\r\nio_mc_regs = (u32 *)&hawaii_io_mc_regs;\r\nregs_size = HAWAII_IO_MC_REGS_SIZE;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nfw_data = (const __be32 *)rdev->mc_fw->data;\r\n}\r\nrunning = RREG32(MC_SEQ_SUP_CNTL) & RUN_MASK;\r\nif (running == 0) {\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000010);\r\nfor (i = 0; i < regs_size; i++) {\r\nif (rdev->new_fw) {\r\nWREG32(MC_SEQ_IO_DEBUG_INDEX, le32_to_cpup(new_io_mc_regs++));\r\nWREG32(MC_SEQ_IO_DEBUG_DATA, le32_to_cpup(new_io_mc_regs++));\r\n} else {\r\nWREG32(MC_SEQ_IO_DEBUG_INDEX, io_mc_regs[(i << 1)]);\r\nWREG32(MC_SEQ_IO_DEBUG_DATA, io_mc_regs[(i << 1) + 1]);\r\n}\r\n}\r\ntmp = RREG32(MC_SEQ_MISC0);\r\nif ((rdev->pdev->device == 0x6649) && ((tmp & 0xff00) == 0x5600)) {\r\nWREG32(MC_SEQ_IO_DEBUG_INDEX, 5);\r\nWREG32(MC_SEQ_IO_DEBUG_DATA, 0x00000023);\r\nWREG32(MC_SEQ_IO_DEBUG_INDEX, 9);\r\nWREG32(MC_SEQ_IO_DEBUG_DATA, 0x000001f0);\r\n}\r\nfor (i = 0; i < ucode_size; i++) {\r\nif (rdev->new_fw)\r\nWREG32(MC_SEQ_SUP_PGM, le32_to_cpup(new_fw_data++));\r\nelse\r\nWREG32(MC_SEQ_SUP_PGM, be32_to_cpup(fw_data++));\r\n}\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000004);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000001);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(MC_SEQ_TRAIN_WAKEUP_CNTL) & TRAIN_DONE_D0)\r\nbreak;\r\nudelay(1);\r\n}\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(MC_SEQ_TRAIN_WAKEUP_CNTL) & TRAIN_DONE_D1)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cik_init_microcode(struct radeon_device *rdev)\r\n{\r\nconst char *chip_name;\r\nconst char *new_chip_name;\r\nsize_t pfp_req_size, me_req_size, ce_req_size,\r\nmec_req_size, rlc_req_size, mc_req_size = 0,\r\nsdma_req_size, smc_req_size = 0, mc2_req_size = 0;\r\nchar fw_name[30];\r\nint new_fw = 0;\r\nint err;\r\nint num_fw;\r\nbool new_smc = false;\r\nDRM_DEBUG("\n");\r\nswitch (rdev->family) {\r\ncase CHIP_BONAIRE:\r\nchip_name = "BONAIRE";\r\nif ((rdev->pdev->revision == 0x80) ||\r\n(rdev->pdev->revision == 0x81) ||\r\n(rdev->pdev->device == 0x665f))\r\nnew_smc = true;\r\nnew_chip_name = "bonaire";\r\npfp_req_size = CIK_PFP_UCODE_SIZE * 4;\r\nme_req_size = CIK_ME_UCODE_SIZE * 4;\r\nce_req_size = CIK_CE_UCODE_SIZE * 4;\r\nmec_req_size = CIK_MEC_UCODE_SIZE * 4;\r\nrlc_req_size = BONAIRE_RLC_UCODE_SIZE * 4;\r\nmc_req_size = BONAIRE_MC_UCODE_SIZE * 4;\r\nmc2_req_size = BONAIRE_MC2_UCODE_SIZE * 4;\r\nsdma_req_size = CIK_SDMA_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(BONAIRE_SMC_UCODE_SIZE, 4);\r\nnum_fw = 8;\r\nbreak;\r\ncase CHIP_HAWAII:\r\nchip_name = "HAWAII";\r\nif (rdev->pdev->revision == 0x80)\r\nnew_smc = true;\r\nnew_chip_name = "hawaii";\r\npfp_req_size = CIK_PFP_UCODE_SIZE * 4;\r\nme_req_size = CIK_ME_UCODE_SIZE * 4;\r\nce_req_size = CIK_CE_UCODE_SIZE * 4;\r\nmec_req_size = CIK_MEC_UCODE_SIZE * 4;\r\nrlc_req_size = BONAIRE_RLC_UCODE_SIZE * 4;\r\nmc_req_size = HAWAII_MC_UCODE_SIZE * 4;\r\nmc2_req_size = HAWAII_MC2_UCODE_SIZE * 4;\r\nsdma_req_size = CIK_SDMA_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(HAWAII_SMC_UCODE_SIZE, 4);\r\nnum_fw = 8;\r\nbreak;\r\ncase CHIP_KAVERI:\r\nchip_name = "KAVERI";\r\nnew_chip_name = "kaveri";\r\npfp_req_size = CIK_PFP_UCODE_SIZE * 4;\r\nme_req_size = CIK_ME_UCODE_SIZE * 4;\r\nce_req_size = CIK_CE_UCODE_SIZE * 4;\r\nmec_req_size = CIK_MEC_UCODE_SIZE * 4;\r\nrlc_req_size = KV_RLC_UCODE_SIZE * 4;\r\nsdma_req_size = CIK_SDMA_UCODE_SIZE * 4;\r\nnum_fw = 7;\r\nbreak;\r\ncase CHIP_KABINI:\r\nchip_name = "KABINI";\r\nnew_chip_name = "kabini";\r\npfp_req_size = CIK_PFP_UCODE_SIZE * 4;\r\nme_req_size = CIK_ME_UCODE_SIZE * 4;\r\nce_req_size = CIK_CE_UCODE_SIZE * 4;\r\nmec_req_size = CIK_MEC_UCODE_SIZE * 4;\r\nrlc_req_size = KB_RLC_UCODE_SIZE * 4;\r\nsdma_req_size = CIK_SDMA_UCODE_SIZE * 4;\r\nnum_fw = 6;\r\nbreak;\r\ncase CHIP_MULLINS:\r\nchip_name = "MULLINS";\r\nnew_chip_name = "mullins";\r\npfp_req_size = CIK_PFP_UCODE_SIZE * 4;\r\nme_req_size = CIK_ME_UCODE_SIZE * 4;\r\nce_req_size = CIK_CE_UCODE_SIZE * 4;\r\nmec_req_size = CIK_MEC_UCODE_SIZE * 4;\r\nrlc_req_size = ML_RLC_UCODE_SIZE * 4;\r\nsdma_req_size = CIK_SDMA_UCODE_SIZE * 4;\r\nnum_fw = 6;\r\nbreak;\r\ndefault: BUG();\r\n}\r\nDRM_INFO("Loading %s Microcode\n", new_chip_name);\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_pfp.bin", new_chip_name);\r\nerr = request_firmware(&rdev->pfp_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_pfp.bin", chip_name);\r\nerr = request_firmware(&rdev->pfp_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->pfp_fw->size != pfp_req_size) {\r\npr_err("cik_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->pfp_fw->size, fw_name);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->pfp_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_me.bin", new_chip_name);\r\nerr = request_firmware(&rdev->me_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_me.bin", chip_name);\r\nerr = request_firmware(&rdev->me_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->me_fw->size != me_req_size) {\r\npr_err("cik_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->me_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->me_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_ce.bin", new_chip_name);\r\nerr = request_firmware(&rdev->ce_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_ce.bin", chip_name);\r\nerr = request_firmware(&rdev->ce_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->ce_fw->size != ce_req_size) {\r\npr_err("cik_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->ce_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->ce_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mec.bin", new_chip_name);\r\nerr = request_firmware(&rdev->mec_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mec.bin", chip_name);\r\nerr = request_firmware(&rdev->mec_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->mec_fw->size != mec_req_size) {\r\npr_err("cik_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->mec_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->mec_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nif (rdev->family == CHIP_KAVERI) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mec2.bin", new_chip_name);\r\nerr = request_firmware(&rdev->mec2_fw, fw_name, rdev->dev);\r\nif (err) {\r\ngoto out;\r\n} else {\r\nerr = radeon_ucode_validate(rdev->mec2_fw);\r\nif (err) {\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_rlc.bin", new_chip_name);\r\nerr = request_firmware(&rdev->rlc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_rlc.bin", chip_name);\r\nerr = request_firmware(&rdev->rlc_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->rlc_fw->size != rlc_req_size) {\r\npr_err("cik_rlc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->rlc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->rlc_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_sdma.bin", new_chip_name);\r\nerr = request_firmware(&rdev->sdma_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_sdma.bin", chip_name);\r\nerr = request_firmware(&rdev->sdma_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->sdma_fw->size != sdma_req_size) {\r\npr_err("cik_sdma: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->sdma_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->sdma_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nif (!(rdev->flags & RADEON_IS_IGP)) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc.bin", new_chip_name);\r\nerr = request_firmware(&rdev->mc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc2.bin", chip_name);\r\nerr = request_firmware(&rdev->mc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc.bin", chip_name);\r\nerr = request_firmware(&rdev->mc_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\n}\r\nif ((rdev->mc_fw->size != mc_req_size) &&\r\n(rdev->mc_fw->size != mc2_req_size)){\r\npr_err("cik_mc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->mc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\nDRM_INFO("%s: %zu bytes\n", fw_name, rdev->mc_fw->size);\r\n} else {\r\nerr = radeon_ucode_validate(rdev->mc_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\nif (new_smc)\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_k_smc.bin", new_chip_name);\r\nelse\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_smc.bin", new_chip_name);\r\nerr = request_firmware(&rdev->smc_fw, fw_name, rdev->dev);\r\nif (err) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_smc.bin", chip_name);\r\nerr = request_firmware(&rdev->smc_fw, fw_name, rdev->dev);\r\nif (err) {\r\npr_err("smc: error loading firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(rdev->smc_fw);\r\nrdev->smc_fw = NULL;\r\nerr = 0;\r\n} else if (rdev->smc_fw->size != smc_req_size) {\r\npr_err("cik_smc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->smc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n} else {\r\nerr = radeon_ucode_validate(rdev->smc_fw);\r\nif (err) {\r\npr_err("cik_fw: validation failed for firmware \"%s\"\n",\r\nfw_name);\r\ngoto out;\r\n} else {\r\nnew_fw++;\r\n}\r\n}\r\n}\r\nif (new_fw == 0) {\r\nrdev->new_fw = false;\r\n} else if (new_fw < num_fw) {\r\npr_err("ci_fw: mixing new and old firmware!\n");\r\nerr = -EINVAL;\r\n} else {\r\nrdev->new_fw = true;\r\n}\r\nout:\r\nif (err) {\r\nif (err != -EINVAL)\r\npr_err("cik_cp: Failed to load firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(rdev->pfp_fw);\r\nrdev->pfp_fw = NULL;\r\nrelease_firmware(rdev->me_fw);\r\nrdev->me_fw = NULL;\r\nrelease_firmware(rdev->ce_fw);\r\nrdev->ce_fw = NULL;\r\nrelease_firmware(rdev->mec_fw);\r\nrdev->mec_fw = NULL;\r\nrelease_firmware(rdev->mec2_fw);\r\nrdev->mec2_fw = NULL;\r\nrelease_firmware(rdev->rlc_fw);\r\nrdev->rlc_fw = NULL;\r\nrelease_firmware(rdev->sdma_fw);\r\nrdev->sdma_fw = NULL;\r\nrelease_firmware(rdev->mc_fw);\r\nrdev->mc_fw = NULL;\r\nrelease_firmware(rdev->smc_fw);\r\nrdev->smc_fw = NULL;\r\n}\r\nreturn err;\r\n}\r\nstatic void cik_tiling_mode_table_init(struct radeon_device *rdev)\r\n{\r\nu32 *tile = rdev->config.cik.tile_mode_array;\r\nu32 *macrotile = rdev->config.cik.macrotile_mode_array;\r\nconst u32 num_tile_mode_states =\r\nARRAY_SIZE(rdev->config.cik.tile_mode_array);\r\nconst u32 num_secondary_tile_mode_states =\r\nARRAY_SIZE(rdev->config.cik.macrotile_mode_array);\r\nu32 reg_offset, split_equal_to_row_size;\r\nu32 num_pipe_configs;\r\nu32 num_rbs = rdev->config.cik.max_backends_per_se *\r\nrdev->config.cik.max_shader_engines;\r\nswitch (rdev->config.cik.mem_row_size_in_kb) {\r\ncase 1:\r\nsplit_equal_to_row_size = ADDR_SURF_TILE_SPLIT_1KB;\r\nbreak;\r\ncase 2:\r\ndefault:\r\nsplit_equal_to_row_size = ADDR_SURF_TILE_SPLIT_2KB;\r\nbreak;\r\ncase 4:\r\nsplit_equal_to_row_size = ADDR_SURF_TILE_SPLIT_4KB;\r\nbreak;\r\n}\r\nnum_pipe_configs = rdev->config.cik.max_tile_pipes;\r\nif (num_pipe_configs > 8)\r\nnum_pipe_configs = 16;\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\ntile[reg_offset] = 0;\r\nfor (reg_offset = 0; reg_offset < num_secondary_tile_mode_states; reg_offset++)\r\nmacrotile[reg_offset] = 0;\r\nswitch(num_pipe_configs) {\r\ncase 16:\r\ntile[0] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B));\r\ntile[1] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B));\r\ntile[2] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[3] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B));\r\ntile[4] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[5] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING));\r\ntile[6] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[7] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[8] = (ARRAY_MODE(ARRAY_LINEAR_ALIGNED) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16));\r\ntile[9] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING));\r\ntile[10] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[11] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[12] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[13] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING));\r\ntile[14] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[16] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[17] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[27] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING));\r\ntile[28] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[29] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[30] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P16_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\nmacrotile[0] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[1] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[2] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[3] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[4] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nmacrotile[5] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_4_BANK));\r\nmacrotile[6] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_2_BANK));\r\nmacrotile[8] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[9] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[10] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[11] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nmacrotile[12] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_4_BANK));\r\nmacrotile[13] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_2_BANK));\r\nmacrotile[14] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_2_BANK));\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\nWREG32(GB_TILE_MODE0 + (reg_offset * 4), tile[reg_offset]);\r\nfor (reg_offset = 0; reg_offset < num_secondary_tile_mode_states; reg_offset++)\r\nWREG32(GB_MACROTILE_MODE0 + (reg_offset * 4), macrotile[reg_offset]);\r\nbreak;\r\ncase 8:\r\ntile[0] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B));\r\ntile[1] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B));\r\ntile[2] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[3] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B));\r\ntile[4] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[5] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING));\r\ntile[6] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[7] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[8] = (ARRAY_MODE(ARRAY_LINEAR_ALIGNED) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16));\r\ntile[9] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING));\r\ntile[10] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[11] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[12] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[13] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING));\r\ntile[14] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[16] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[17] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[27] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING));\r\ntile[28] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[29] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[30] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P8_32x32_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\nmacrotile[0] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[1] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[2] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[3] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[4] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nmacrotile[5] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_4_BANK));\r\nmacrotile[6] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_2_BANK));\r\nmacrotile[8] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_8) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[9] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[10] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[11] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[12] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nmacrotile[13] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_4_BANK));\r\nmacrotile[14] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_2_BANK));\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\nWREG32(GB_TILE_MODE0 + (reg_offset * 4), tile[reg_offset]);\r\nfor (reg_offset = 0; reg_offset < num_secondary_tile_mode_states; reg_offset++)\r\nWREG32(GB_MACROTILE_MODE0 + (reg_offset * 4), macrotile[reg_offset]);\r\nbreak;\r\ncase 4:\r\nif (num_rbs == 4) {\r\ntile[0] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B));\r\ntile[1] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B));\r\ntile[2] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[3] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B));\r\ntile[4] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[5] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING));\r\ntile[6] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[7] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[8] = (ARRAY_MODE(ARRAY_LINEAR_ALIGNED) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16));\r\ntile[9] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING));\r\ntile[10] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[11] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[12] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[13] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING));\r\ntile[14] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[16] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[17] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[27] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING));\r\ntile[28] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[29] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[30] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_16x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\n} else if (num_rbs < 4) {\r\ntile[0] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B));\r\ntile[1] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B));\r\ntile[2] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[3] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B));\r\ntile[4] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[5] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING));\r\ntile[6] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[7] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[8] = (ARRAY_MODE(ARRAY_LINEAR_ALIGNED) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16));\r\ntile[9] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING));\r\ntile[10] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[11] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[12] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[13] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING));\r\ntile[14] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[16] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[17] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[27] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING));\r\ntile[28] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[29] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[30] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P4_8x16) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\n}\r\nmacrotile[0] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[1] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[2] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[3] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[4] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[5] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nmacrotile[6] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_4_BANK));\r\nmacrotile[8] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_8) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[9] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[10] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[11] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[12] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[13] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nmacrotile[14] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_1) |\r\nNUM_BANKS(ADDR_SURF_4_BANK));\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\nWREG32(GB_TILE_MODE0 + (reg_offset * 4), tile[reg_offset]);\r\nfor (reg_offset = 0; reg_offset < num_secondary_tile_mode_states; reg_offset++)\r\nWREG32(GB_MACROTILE_MODE0 + (reg_offset * 4), macrotile[reg_offset]);\r\nbreak;\r\ncase 2:\r\ntile[0] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_64B));\r\ntile[1] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_128B));\r\ntile[2] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[3] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_512B));\r\ntile[4] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[5] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING));\r\ntile[6] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nTILE_SPLIT(ADDR_SURF_TILE_SPLIT_256B));\r\ntile[7] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DEPTH_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nTILE_SPLIT(split_equal_to_row_size));\r\ntile[8] = ARRAY_MODE(ARRAY_LINEAR_ALIGNED) |\r\nPIPE_CONFIG(ADDR_SURF_P2);\r\ntile[9] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2));\r\ntile[10] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[11] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[12] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_DISPLAY_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[13] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING));\r\ntile[14] = (ARRAY_MODE(ARRAY_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[16] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[17] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_THIN_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[27] = (ARRAY_MODE(ARRAY_1D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2));\r\ntile[28] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[29] = (ARRAY_MODE(ARRAY_PRT_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\ntile[30] = (ARRAY_MODE(ARRAY_PRT_2D_TILED_THIN1) |\r\nMICRO_TILE_MODE_NEW(ADDR_SURF_ROTATED_MICRO_TILING) |\r\nPIPE_CONFIG(ADDR_SURF_P2) |\r\nSAMPLE_SPLIT(ADDR_SURF_SAMPLE_SPLIT_2));\r\nmacrotile[0] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[1] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[2] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[3] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[4] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[5] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[6] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nmacrotile[8] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_4) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_8) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[9] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_4) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[10] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_4) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[11] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_2) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[12] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_2) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[13] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_4) |\r\nNUM_BANKS(ADDR_SURF_16_BANK));\r\nmacrotile[14] = (BANK_WIDTH(ADDR_SURF_BANK_WIDTH_1) |\r\nBANK_HEIGHT(ADDR_SURF_BANK_HEIGHT_1) |\r\nMACRO_TILE_ASPECT(ADDR_SURF_MACRO_ASPECT_2) |\r\nNUM_BANKS(ADDR_SURF_8_BANK));\r\nfor (reg_offset = 0; reg_offset < num_tile_mode_states; reg_offset++)\r\nWREG32(GB_TILE_MODE0 + (reg_offset * 4), tile[reg_offset]);\r\nfor (reg_offset = 0; reg_offset < num_secondary_tile_mode_states; reg_offset++)\r\nWREG32(GB_MACROTILE_MODE0 + (reg_offset * 4), macrotile[reg_offset]);\r\nbreak;\r\ndefault:\r\nDRM_ERROR("unknown num pipe config: 0x%x\n", num_pipe_configs);\r\n}\r\n}\r\nstatic void cik_select_se_sh(struct radeon_device *rdev,\r\nu32 se_num, u32 sh_num)\r\n{\r\nu32 data = INSTANCE_BROADCAST_WRITES;\r\nif ((se_num == 0xffffffff) && (sh_num == 0xffffffff))\r\ndata |= SH_BROADCAST_WRITES | SE_BROADCAST_WRITES;\r\nelse if (se_num == 0xffffffff)\r\ndata |= SE_BROADCAST_WRITES | SH_INDEX(sh_num);\r\nelse if (sh_num == 0xffffffff)\r\ndata |= SH_BROADCAST_WRITES | SE_INDEX(se_num);\r\nelse\r\ndata |= SH_INDEX(sh_num) | SE_INDEX(se_num);\r\nWREG32(GRBM_GFX_INDEX, data);\r\n}\r\nstatic u32 cik_create_bitmask(u32 bit_width)\r\n{\r\nu32 i, mask = 0;\r\nfor (i = 0; i < bit_width; i++) {\r\nmask <<= 1;\r\nmask |= 1;\r\n}\r\nreturn mask;\r\n}\r\nstatic u32 cik_get_rb_disabled(struct radeon_device *rdev,\r\nu32 max_rb_num_per_se,\r\nu32 sh_per_se)\r\n{\r\nu32 data, mask;\r\ndata = RREG32(CC_RB_BACKEND_DISABLE);\r\nif (data & 1)\r\ndata &= BACKEND_DISABLE_MASK;\r\nelse\r\ndata = 0;\r\ndata |= RREG32(GC_USER_RB_BACKEND_DISABLE);\r\ndata >>= BACKEND_DISABLE_SHIFT;\r\nmask = cik_create_bitmask(max_rb_num_per_se / sh_per_se);\r\nreturn data & mask;\r\n}\r\nstatic void cik_setup_rb(struct radeon_device *rdev,\r\nu32 se_num, u32 sh_per_se,\r\nu32 max_rb_num_per_se)\r\n{\r\nint i, j;\r\nu32 data, mask;\r\nu32 disabled_rbs = 0;\r\nu32 enabled_rbs = 0;\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\nfor (i = 0; i < se_num; i++) {\r\nfor (j = 0; j < sh_per_se; j++) {\r\ncik_select_se_sh(rdev, i, j);\r\ndata = cik_get_rb_disabled(rdev, max_rb_num_per_se, sh_per_se);\r\nif (rdev->family == CHIP_HAWAII)\r\ndisabled_rbs |= data << ((i * sh_per_se + j) * HAWAII_RB_BITMAP_WIDTH_PER_SH);\r\nelse\r\ndisabled_rbs |= data << ((i * sh_per_se + j) * CIK_RB_BITMAP_WIDTH_PER_SH);\r\n}\r\n}\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\nmask = 1;\r\nfor (i = 0; i < max_rb_num_per_se * se_num; i++) {\r\nif (!(disabled_rbs & mask))\r\nenabled_rbs |= mask;\r\nmask <<= 1;\r\n}\r\nrdev->config.cik.backend_enable_mask = enabled_rbs;\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\nfor (i = 0; i < se_num; i++) {\r\ncik_select_se_sh(rdev, i, 0xffffffff);\r\ndata = 0;\r\nfor (j = 0; j < sh_per_se; j++) {\r\nswitch (enabled_rbs & 3) {\r\ncase 0:\r\nif (j == 0)\r\ndata |= PKR_MAP(RASTER_CONFIG_RB_MAP_3);\r\nelse\r\ndata |= PKR_MAP(RASTER_CONFIG_RB_MAP_0);\r\nbreak;\r\ncase 1:\r\ndata |= (RASTER_CONFIG_RB_MAP_0 << (i * sh_per_se + j) * 2);\r\nbreak;\r\ncase 2:\r\ndata |= (RASTER_CONFIG_RB_MAP_3 << (i * sh_per_se + j) * 2);\r\nbreak;\r\ncase 3:\r\ndefault:\r\ndata |= (RASTER_CONFIG_RB_MAP_2 << (i * sh_per_se + j) * 2);\r\nbreak;\r\n}\r\nenabled_rbs >>= 2;\r\n}\r\nWREG32(PA_SC_RASTER_CONFIG, data);\r\n}\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\n}\r\nstatic void cik_gpu_init(struct radeon_device *rdev)\r\n{\r\nu32 gb_addr_config = RREG32(GB_ADDR_CONFIG);\r\nu32 mc_shared_chmap, mc_arb_ramcfg;\r\nu32 hdp_host_path_cntl;\r\nu32 tmp;\r\nint i, j;\r\nswitch (rdev->family) {\r\ncase CHIP_BONAIRE:\r\nrdev->config.cik.max_shader_engines = 2;\r\nrdev->config.cik.max_tile_pipes = 4;\r\nrdev->config.cik.max_cu_per_sh = 7;\r\nrdev->config.cik.max_sh_per_se = 1;\r\nrdev->config.cik.max_backends_per_se = 2;\r\nrdev->config.cik.max_texture_channel_caches = 4;\r\nrdev->config.cik.max_gprs = 256;\r\nrdev->config.cik.max_gs_threads = 32;\r\nrdev->config.cik.max_hw_contexts = 8;\r\nrdev->config.cik.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.cik.sc_prim_fifo_size_backend = 0x100;\r\nrdev->config.cik.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.cik.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = BONAIRE_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_HAWAII:\r\nrdev->config.cik.max_shader_engines = 4;\r\nrdev->config.cik.max_tile_pipes = 16;\r\nrdev->config.cik.max_cu_per_sh = 11;\r\nrdev->config.cik.max_sh_per_se = 1;\r\nrdev->config.cik.max_backends_per_se = 4;\r\nrdev->config.cik.max_texture_channel_caches = 16;\r\nrdev->config.cik.max_gprs = 256;\r\nrdev->config.cik.max_gs_threads = 32;\r\nrdev->config.cik.max_hw_contexts = 8;\r\nrdev->config.cik.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.cik.sc_prim_fifo_size_backend = 0x100;\r\nrdev->config.cik.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.cik.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = HAWAII_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_KAVERI:\r\nrdev->config.cik.max_shader_engines = 1;\r\nrdev->config.cik.max_tile_pipes = 4;\r\nif ((rdev->pdev->device == 0x1304) ||\r\n(rdev->pdev->device == 0x1305) ||\r\n(rdev->pdev->device == 0x130C) ||\r\n(rdev->pdev->device == 0x130F) ||\r\n(rdev->pdev->device == 0x1310) ||\r\n(rdev->pdev->device == 0x1311) ||\r\n(rdev->pdev->device == 0x131C)) {\r\nrdev->config.cik.max_cu_per_sh = 8;\r\nrdev->config.cik.max_backends_per_se = 2;\r\n} else if ((rdev->pdev->device == 0x1309) ||\r\n(rdev->pdev->device == 0x130A) ||\r\n(rdev->pdev->device == 0x130D) ||\r\n(rdev->pdev->device == 0x1313) ||\r\n(rdev->pdev->device == 0x131D)) {\r\nrdev->config.cik.max_cu_per_sh = 6;\r\nrdev->config.cik.max_backends_per_se = 2;\r\n} else if ((rdev->pdev->device == 0x1306) ||\r\n(rdev->pdev->device == 0x1307) ||\r\n(rdev->pdev->device == 0x130B) ||\r\n(rdev->pdev->device == 0x130E) ||\r\n(rdev->pdev->device == 0x1315) ||\r\n(rdev->pdev->device == 0x1318) ||\r\n(rdev->pdev->device == 0x131B)) {\r\nrdev->config.cik.max_cu_per_sh = 4;\r\nrdev->config.cik.max_backends_per_se = 1;\r\n} else {\r\nrdev->config.cik.max_cu_per_sh = 3;\r\nrdev->config.cik.max_backends_per_se = 1;\r\n}\r\nrdev->config.cik.max_sh_per_se = 1;\r\nrdev->config.cik.max_texture_channel_caches = 4;\r\nrdev->config.cik.max_gprs = 256;\r\nrdev->config.cik.max_gs_threads = 16;\r\nrdev->config.cik.max_hw_contexts = 8;\r\nrdev->config.cik.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.cik.sc_prim_fifo_size_backend = 0x100;\r\nrdev->config.cik.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.cik.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = BONAIRE_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_KABINI:\r\ncase CHIP_MULLINS:\r\ndefault:\r\nrdev->config.cik.max_shader_engines = 1;\r\nrdev->config.cik.max_tile_pipes = 2;\r\nrdev->config.cik.max_cu_per_sh = 2;\r\nrdev->config.cik.max_sh_per_se = 1;\r\nrdev->config.cik.max_backends_per_se = 1;\r\nrdev->config.cik.max_texture_channel_caches = 2;\r\nrdev->config.cik.max_gprs = 256;\r\nrdev->config.cik.max_gs_threads = 16;\r\nrdev->config.cik.max_hw_contexts = 8;\r\nrdev->config.cik.sc_prim_fifo_size_frontend = 0x20;\r\nrdev->config.cik.sc_prim_fifo_size_backend = 0x100;\r\nrdev->config.cik.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.cik.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = BONAIRE_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\n}\r\nfor (i = 0, j = 0; i < 32; i++, j += 0x18) {\r\nWREG32((0x2c14 + j), 0x00000000);\r\nWREG32((0x2c18 + j), 0x00000000);\r\nWREG32((0x2c1c + j), 0x00000000);\r\nWREG32((0x2c20 + j), 0x00000000);\r\nWREG32((0x2c24 + j), 0x00000000);\r\n}\r\nWREG32(GRBM_CNTL, GRBM_READ_TIMEOUT(0xff));\r\nWREG32(SRBM_INT_CNTL, 0x1);\r\nWREG32(SRBM_INT_ACK, 0x1);\r\nWREG32(BIF_FB_EN, FB_READ_EN | FB_WRITE_EN);\r\nmc_shared_chmap = RREG32(MC_SHARED_CHMAP);\r\nmc_arb_ramcfg = RREG32(MC_ARB_RAMCFG);\r\nrdev->config.cik.num_tile_pipes = rdev->config.cik.max_tile_pipes;\r\nrdev->config.cik.mem_max_burst_length_bytes = 256;\r\ntmp = (mc_arb_ramcfg & NOOFCOLS_MASK) >> NOOFCOLS_SHIFT;\r\nrdev->config.cik.mem_row_size_in_kb = (4 * (1 << (8 + tmp))) / 1024;\r\nif (rdev->config.cik.mem_row_size_in_kb > 4)\r\nrdev->config.cik.mem_row_size_in_kb = 4;\r\nrdev->config.cik.shader_engine_tile_size = 32;\r\nrdev->config.cik.num_gpus = 1;\r\nrdev->config.cik.multi_gpu_tile_size = 64;\r\ngb_addr_config &= ~ROW_SIZE_MASK;\r\nswitch (rdev->config.cik.mem_row_size_in_kb) {\r\ncase 1:\r\ndefault:\r\ngb_addr_config |= ROW_SIZE(0);\r\nbreak;\r\ncase 2:\r\ngb_addr_config |= ROW_SIZE(1);\r\nbreak;\r\ncase 4:\r\ngb_addr_config |= ROW_SIZE(2);\r\nbreak;\r\n}\r\nrdev->config.cik.tile_config = 0;\r\nswitch (rdev->config.cik.num_tile_pipes) {\r\ncase 1:\r\nrdev->config.cik.tile_config |= (0 << 0);\r\nbreak;\r\ncase 2:\r\nrdev->config.cik.tile_config |= (1 << 0);\r\nbreak;\r\ncase 4:\r\nrdev->config.cik.tile_config |= (2 << 0);\r\nbreak;\r\ncase 8:\r\ndefault:\r\nrdev->config.cik.tile_config |= (3 << 0);\r\nbreak;\r\n}\r\nrdev->config.cik.tile_config |=\r\n((mc_arb_ramcfg & NOOFBANK_MASK) >> NOOFBANK_SHIFT) << 4;\r\nrdev->config.cik.tile_config |=\r\n((gb_addr_config & PIPE_INTERLEAVE_SIZE_MASK) >> PIPE_INTERLEAVE_SIZE_SHIFT) << 8;\r\nrdev->config.cik.tile_config |=\r\n((gb_addr_config & ROW_SIZE_MASK) >> ROW_SIZE_SHIFT) << 12;\r\nWREG32(GB_ADDR_CONFIG, gb_addr_config);\r\nWREG32(HDP_ADDR_CONFIG, gb_addr_config);\r\nWREG32(DMIF_ADDR_CALC, gb_addr_config);\r\nWREG32(SDMA0_TILING_CONFIG + SDMA0_REGISTER_OFFSET, gb_addr_config & 0x70);\r\nWREG32(SDMA0_TILING_CONFIG + SDMA1_REGISTER_OFFSET, gb_addr_config & 0x70);\r\nWREG32(UVD_UDEC_ADDR_CONFIG, gb_addr_config);\r\nWREG32(UVD_UDEC_DB_ADDR_CONFIG, gb_addr_config);\r\nWREG32(UVD_UDEC_DBW_ADDR_CONFIG, gb_addr_config);\r\ncik_tiling_mode_table_init(rdev);\r\ncik_setup_rb(rdev, rdev->config.cik.max_shader_engines,\r\nrdev->config.cik.max_sh_per_se,\r\nrdev->config.cik.max_backends_per_se);\r\nrdev->config.cik.active_cus = 0;\r\nfor (i = 0; i < rdev->config.cik.max_shader_engines; i++) {\r\nfor (j = 0; j < rdev->config.cik.max_sh_per_se; j++) {\r\nrdev->config.cik.active_cus +=\r\nhweight32(cik_get_cu_active_bitmap(rdev, i, j));\r\n}\r\n}\r\nWREG32(CP_MEQ_THRESHOLDS, MEQ1_START(0x30) | MEQ2_START(0x60));\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nWREG32(SX_DEBUG_1, 0x20);\r\nWREG32(TA_CNTL_AUX, 0x00010000);\r\ntmp = RREG32(SPI_CONFIG_CNTL);\r\ntmp |= 0x03000000;\r\nWREG32(SPI_CONFIG_CNTL, tmp);\r\nWREG32(SQ_CONFIG, 1);\r\nWREG32(DB_DEBUG, 0);\r\ntmp = RREG32(DB_DEBUG2) & ~0xf00fffff;\r\ntmp |= 0x00000400;\r\nWREG32(DB_DEBUG2, tmp);\r\ntmp = RREG32(DB_DEBUG3) & ~0x0002021c;\r\ntmp |= 0x00020200;\r\nWREG32(DB_DEBUG3, tmp);\r\ntmp = RREG32(CB_HW_CONTROL) & ~0x00010000;\r\ntmp |= 0x00018208;\r\nWREG32(CB_HW_CONTROL, tmp);\r\nWREG32(SPI_CONFIG_CNTL_1, VTX_DONE_DELAY(4));\r\nWREG32(PA_SC_FIFO_SIZE, (SC_FRONTEND_PRIM_FIFO_SIZE(rdev->config.cik.sc_prim_fifo_size_frontend) |\r\nSC_BACKEND_PRIM_FIFO_SIZE(rdev->config.cik.sc_prim_fifo_size_backend) |\r\nSC_HIZ_TILE_FIFO_SIZE(rdev->config.cik.sc_hiz_tile_fifo_size) |\r\nSC_EARLYZ_TILE_FIFO_SIZE(rdev->config.cik.sc_earlyz_tile_fifo_size)));\r\nWREG32(VGT_NUM_INSTANCES, 1);\r\nWREG32(CP_PERFMON_CNTL, 0);\r\nWREG32(SQ_CONFIG, 0);\r\nWREG32(PA_SC_FORCE_EOV_MAX_CNTS, (FORCE_EOV_MAX_CLK_CNT(4095) |\r\nFORCE_EOV_MAX_REZ_CNT(255)));\r\nWREG32(VGT_CACHE_INVALIDATION, CACHE_INVALIDATION(VC_AND_TC) |\r\nAUTO_INVLD_EN(ES_AND_GS_AUTO));\r\nWREG32(VGT_GS_VERTEX_REUSE, 16);\r\nWREG32(PA_SC_LINE_STIPPLE_STATE, 0);\r\ntmp = RREG32(HDP_MISC_CNTL);\r\ntmp |= HDP_FLUSH_INVALIDATE_CACHE;\r\nWREG32(HDP_MISC_CNTL, tmp);\r\nhdp_host_path_cntl = RREG32(HDP_HOST_PATH_CNTL);\r\nWREG32(HDP_HOST_PATH_CNTL, hdp_host_path_cntl);\r\nWREG32(PA_CL_ENHANCE, CLIP_VTX_REORDER_ENA | NUM_CLIP_SEQ(3));\r\nWREG32(PA_SC_ENHANCE, ENABLE_PA_SC_OUT_OF_ORDER);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\nudelay(50);\r\n}\r\nstatic void cik_scratch_init(struct radeon_device *rdev)\r\n{\r\nint i;\r\nrdev->scratch.num_reg = 7;\r\nrdev->scratch.reg_base = SCRATCH_REG0;\r\nfor (i = 0; i < rdev->scratch.num_reg; i++) {\r\nrdev->scratch.free[i] = true;\r\nrdev->scratch.reg[i] = rdev->scratch.reg_base + (i * 4);\r\n}\r\n}\r\nint cik_ring_test(struct radeon_device *rdev, struct radeon_ring *ring)\r\n{\r\nuint32_t scratch;\r\nuint32_t tmp = 0;\r\nunsigned i;\r\nint r;\r\nr = radeon_scratch_get(rdev, &scratch);\r\nif (r) {\r\nDRM_ERROR("radeon: cp failed to get scratch reg (%d).\n", r);\r\nreturn r;\r\n}\r\nWREG32(scratch, 0xCAFEDEAD);\r\nr = radeon_ring_lock(rdev, ring, 3);\r\nif (r) {\r\nDRM_ERROR("radeon: cp failed to lock ring %d (%d).\n", ring->idx, r);\r\nradeon_scratch_free(rdev, scratch);\r\nreturn r;\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_UCONFIG_REG, 1));\r\nradeon_ring_write(ring, ((scratch - PACKET3_SET_UCONFIG_REG_START) >> 2));\r\nradeon_ring_write(ring, 0xDEADBEEF);\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\ntmp = RREG32(scratch);\r\nif (tmp == 0xDEADBEEF)\r\nbreak;\r\nDRM_UDELAY(1);\r\n}\r\nif (i < rdev->usec_timeout) {\r\nDRM_INFO("ring test on %d succeeded in %d usecs\n", ring->idx, i);\r\n} else {\r\nDRM_ERROR("radeon: ring %d test failed (scratch(0x%04X)=0x%08X)\n",\r\nring->idx, scratch, tmp);\r\nr = -EINVAL;\r\n}\r\nradeon_scratch_free(rdev, scratch);\r\nreturn r;\r\n}\r\nstatic void cik_hdp_flush_cp_ring_emit(struct radeon_device *rdev,\r\nint ridx)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[ridx];\r\nu32 ref_and_mask;\r\nswitch (ring->idx) {\r\ncase CAYMAN_RING_TYPE_CP1_INDEX:\r\ncase CAYMAN_RING_TYPE_CP2_INDEX:\r\ndefault:\r\nswitch (ring->me) {\r\ncase 0:\r\nref_and_mask = CP2 << ring->pipe;\r\nbreak;\r\ncase 1:\r\nref_and_mask = CP6 << ring->pipe;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nbreak;\r\ncase RADEON_RING_TYPE_GFX_INDEX:\r\nref_and_mask = CP0;\r\nbreak;\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_WAIT_REG_MEM, 5));\r\nradeon_ring_write(ring, (WAIT_REG_MEM_OPERATION(1) |\r\nWAIT_REG_MEM_FUNCTION(3) |\r\nWAIT_REG_MEM_ENGINE(1)));\r\nradeon_ring_write(ring, GPU_HDP_FLUSH_REQ >> 2);\r\nradeon_ring_write(ring, GPU_HDP_FLUSH_DONE >> 2);\r\nradeon_ring_write(ring, ref_and_mask);\r\nradeon_ring_write(ring, ref_and_mask);\r\nradeon_ring_write(ring, 0x20);\r\n}\r\nvoid cik_fence_gfx_ring_emit(struct radeon_device *rdev,\r\nstruct radeon_fence *fence)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[fence->ring];\r\nu64 addr = rdev->fence_drv[fence->ring].gpu_addr;\r\nradeon_ring_write(ring, PACKET3(PACKET3_EVENT_WRITE_EOP, 4));\r\nradeon_ring_write(ring, (EOP_TCL1_ACTION_EN |\r\nEOP_TC_ACTION_EN |\r\nEVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |\r\nEVENT_INDEX(5)));\r\nradeon_ring_write(ring, addr & 0xfffffffc);\r\nradeon_ring_write(ring, (upper_32_bits(addr) & 0xffff) |\r\nDATA_SEL(1) | INT_SEL(0));\r\nradeon_ring_write(ring, fence->seq - 1);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, PACKET3(PACKET3_EVENT_WRITE_EOP, 4));\r\nradeon_ring_write(ring, (EOP_TCL1_ACTION_EN |\r\nEOP_TC_ACTION_EN |\r\nEVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |\r\nEVENT_INDEX(5)));\r\nradeon_ring_write(ring, addr & 0xfffffffc);\r\nradeon_ring_write(ring, (upper_32_bits(addr) & 0xffff) | DATA_SEL(1) | INT_SEL(2));\r\nradeon_ring_write(ring, fence->seq);\r\nradeon_ring_write(ring, 0);\r\n}\r\nvoid cik_fence_compute_ring_emit(struct radeon_device *rdev,\r\nstruct radeon_fence *fence)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[fence->ring];\r\nu64 addr = rdev->fence_drv[fence->ring].gpu_addr;\r\nradeon_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 5));\r\nradeon_ring_write(ring, (EOP_TCL1_ACTION_EN |\r\nEOP_TC_ACTION_EN |\r\nEVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |\r\nEVENT_INDEX(5)));\r\nradeon_ring_write(ring, DATA_SEL(1) | INT_SEL(2));\r\nradeon_ring_write(ring, addr & 0xfffffffc);\r\nradeon_ring_write(ring, upper_32_bits(addr));\r\nradeon_ring_write(ring, fence->seq);\r\nradeon_ring_write(ring, 0);\r\n}\r\nbool cik_semaphore_ring_emit(struct radeon_device *rdev,\r\nstruct radeon_ring *ring,\r\nstruct radeon_semaphore *semaphore,\r\nbool emit_wait)\r\n{\r\nuint64_t addr = semaphore->gpu_addr;\r\nunsigned sel = emit_wait ? PACKET3_SEM_SEL_WAIT : PACKET3_SEM_SEL_SIGNAL;\r\nradeon_ring_write(ring, PACKET3(PACKET3_MEM_SEMAPHORE, 1));\r\nradeon_ring_write(ring, lower_32_bits(addr));\r\nradeon_ring_write(ring, (upper_32_bits(addr) & 0xffff) | sel);\r\nif (emit_wait && ring->idx == RADEON_RING_TYPE_GFX_INDEX) {\r\nradeon_ring_write(ring, PACKET3(PACKET3_PFP_SYNC_ME, 0));\r\nradeon_ring_write(ring, 0x0);\r\n}\r\nreturn true;\r\n}\r\nstruct radeon_fence *cik_copy_cpdma(struct radeon_device *rdev,\r\nuint64_t src_offset, uint64_t dst_offset,\r\nunsigned num_gpu_pages,\r\nstruct reservation_object *resv)\r\n{\r\nstruct radeon_fence *fence;\r\nstruct radeon_sync sync;\r\nint ring_index = rdev->asic->copy.blit_ring_index;\r\nstruct radeon_ring *ring = &rdev->ring[ring_index];\r\nu32 size_in_bytes, cur_size_in_bytes, control;\r\nint i, num_loops;\r\nint r = 0;\r\nradeon_sync_create(&sync);\r\nsize_in_bytes = (num_gpu_pages << RADEON_GPU_PAGE_SHIFT);\r\nnum_loops = DIV_ROUND_UP(size_in_bytes, 0x1fffff);\r\nr = radeon_ring_lock(rdev, ring, num_loops * 7 + 18);\r\nif (r) {\r\nDRM_ERROR("radeon: moving bo (%d).\n", r);\r\nradeon_sync_free(rdev, &sync, NULL);\r\nreturn ERR_PTR(r);\r\n}\r\nradeon_sync_resv(rdev, &sync, resv, false);\r\nradeon_sync_rings(rdev, &sync, ring->idx);\r\nfor (i = 0; i < num_loops; i++) {\r\ncur_size_in_bytes = size_in_bytes;\r\nif (cur_size_in_bytes > 0x1fffff)\r\ncur_size_in_bytes = 0x1fffff;\r\nsize_in_bytes -= cur_size_in_bytes;\r\ncontrol = 0;\r\nif (size_in_bytes == 0)\r\ncontrol |= PACKET3_DMA_DATA_CP_SYNC;\r\nradeon_ring_write(ring, PACKET3(PACKET3_DMA_DATA, 5));\r\nradeon_ring_write(ring, control);\r\nradeon_ring_write(ring, lower_32_bits(src_offset));\r\nradeon_ring_write(ring, upper_32_bits(src_offset));\r\nradeon_ring_write(ring, lower_32_bits(dst_offset));\r\nradeon_ring_write(ring, upper_32_bits(dst_offset));\r\nradeon_ring_write(ring, cur_size_in_bytes);\r\nsrc_offset += cur_size_in_bytes;\r\ndst_offset += cur_size_in_bytes;\r\n}\r\nr = radeon_fence_emit(rdev, &fence, ring->idx);\r\nif (r) {\r\nradeon_ring_unlock_undo(rdev, ring);\r\nradeon_sync_free(rdev, &sync, NULL);\r\nreturn ERR_PTR(r);\r\n}\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\nradeon_sync_free(rdev, &sync, fence);\r\nreturn fence;\r\n}\r\nvoid cik_ring_ib_execute(struct radeon_device *rdev, struct radeon_ib *ib)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[ib->ring];\r\nunsigned vm_id = ib->vm ? ib->vm->ids[ib->ring].id : 0;\r\nu32 header, control = INDIRECT_BUFFER_VALID;\r\nif (ib->is_const_ib) {\r\nradeon_ring_write(ring, PACKET3(PACKET3_SWITCH_BUFFER, 0));\r\nradeon_ring_write(ring, 0);\r\nheader = PACKET3(PACKET3_INDIRECT_BUFFER_CONST, 2);\r\n} else {\r\nu32 next_rptr;\r\nif (ring->rptr_save_reg) {\r\nnext_rptr = ring->wptr + 3 + 4;\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_UCONFIG_REG, 1));\r\nradeon_ring_write(ring, ((ring->rptr_save_reg -\r\nPACKET3_SET_UCONFIG_REG_START) >> 2));\r\nradeon_ring_write(ring, next_rptr);\r\n} else if (rdev->wb.enabled) {\r\nnext_rptr = ring->wptr + 5 + 4;\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, WRITE_DATA_DST_SEL(1));\r\nradeon_ring_write(ring, ring->next_rptr_gpu_addr & 0xfffffffc);\r\nradeon_ring_write(ring, upper_32_bits(ring->next_rptr_gpu_addr));\r\nradeon_ring_write(ring, next_rptr);\r\n}\r\nheader = PACKET3(PACKET3_INDIRECT_BUFFER, 2);\r\n}\r\ncontrol |= ib->length_dw | (vm_id << 24);\r\nradeon_ring_write(ring, header);\r\nradeon_ring_write(ring, (ib->gpu_addr & 0xFFFFFFFC));\r\nradeon_ring_write(ring, upper_32_bits(ib->gpu_addr) & 0xFFFF);\r\nradeon_ring_write(ring, control);\r\n}\r\nint cik_ib_test(struct radeon_device *rdev, struct radeon_ring *ring)\r\n{\r\nstruct radeon_ib ib;\r\nuint32_t scratch;\r\nuint32_t tmp = 0;\r\nunsigned i;\r\nint r;\r\nr = radeon_scratch_get(rdev, &scratch);\r\nif (r) {\r\nDRM_ERROR("radeon: failed to get scratch reg (%d).\n", r);\r\nreturn r;\r\n}\r\nWREG32(scratch, 0xCAFEDEAD);\r\nr = radeon_ib_get(rdev, ring->idx, &ib, NULL, 256);\r\nif (r) {\r\nDRM_ERROR("radeon: failed to get ib (%d).\n", r);\r\nradeon_scratch_free(rdev, scratch);\r\nreturn r;\r\n}\r\nib.ptr[0] = PACKET3(PACKET3_SET_UCONFIG_REG, 1);\r\nib.ptr[1] = ((scratch - PACKET3_SET_UCONFIG_REG_START) >> 2);\r\nib.ptr[2] = 0xDEADBEEF;\r\nib.length_dw = 3;\r\nr = radeon_ib_schedule(rdev, &ib, NULL, false);\r\nif (r) {\r\nradeon_scratch_free(rdev, scratch);\r\nradeon_ib_free(rdev, &ib);\r\nDRM_ERROR("radeon: failed to schedule ib (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_wait_timeout(ib.fence, false, usecs_to_jiffies(\r\nRADEON_USEC_IB_TEST_TIMEOUT));\r\nif (r < 0) {\r\nDRM_ERROR("radeon: fence wait failed (%d).\n", r);\r\nradeon_scratch_free(rdev, scratch);\r\nradeon_ib_free(rdev, &ib);\r\nreturn r;\r\n} else if (r == 0) {\r\nDRM_ERROR("radeon: fence wait timed out.\n");\r\nradeon_scratch_free(rdev, scratch);\r\nradeon_ib_free(rdev, &ib);\r\nreturn -ETIMEDOUT;\r\n}\r\nr = 0;\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\ntmp = RREG32(scratch);\r\nif (tmp == 0xDEADBEEF)\r\nbreak;\r\nDRM_UDELAY(1);\r\n}\r\nif (i < rdev->usec_timeout) {\r\nDRM_INFO("ib test on ring %d succeeded in %u usecs\n", ib.fence->ring, i);\r\n} else {\r\nDRM_ERROR("radeon: ib test failed (scratch(0x%04X)=0x%08X)\n",\r\nscratch, tmp);\r\nr = -EINVAL;\r\n}\r\nradeon_scratch_free(rdev, scratch);\r\nradeon_ib_free(rdev, &ib);\r\nreturn r;\r\n}\r\nstatic void cik_cp_gfx_enable(struct radeon_device *rdev, bool enable)\r\n{\r\nif (enable)\r\nWREG32(CP_ME_CNTL, 0);\r\nelse {\r\nif (rdev->asic->copy.copy_ring_index == RADEON_RING_TYPE_GFX_INDEX)\r\nradeon_ttm_set_active_vram_size(rdev, rdev->mc.visible_vram_size);\r\nWREG32(CP_ME_CNTL, (CP_ME_HALT | CP_PFP_HALT | CP_CE_HALT));\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = false;\r\n}\r\nudelay(50);\r\n}\r\nstatic int cik_cp_gfx_load_microcode(struct radeon_device *rdev)\r\n{\r\nint i;\r\nif (!rdev->me_fw || !rdev->pfp_fw || !rdev->ce_fw)\r\nreturn -EINVAL;\r\ncik_cp_gfx_enable(rdev, false);\r\nif (rdev->new_fw) {\r\nconst struct gfx_firmware_header_v1_0 *pfp_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->pfp_fw->data;\r\nconst struct gfx_firmware_header_v1_0 *ce_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->ce_fw->data;\r\nconst struct gfx_firmware_header_v1_0 *me_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->me_fw->data;\r\nconst __le32 *fw_data;\r\nu32 fw_size;\r\nradeon_ucode_print_gfx_hdr(&pfp_hdr->header);\r\nradeon_ucode_print_gfx_hdr(&ce_hdr->header);\r\nradeon_ucode_print_gfx_hdr(&me_hdr->header);\r\nfw_data = (const __le32 *)\r\n(rdev->pfp_fw->data + le32_to_cpu(pfp_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(pfp_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_PFP_UCODE_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_PFP_UCODE_ADDR, le32_to_cpu(pfp_hdr->header.ucode_version));\r\nfw_data = (const __le32 *)\r\n(rdev->ce_fw->data + le32_to_cpu(ce_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(ce_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_CE_UCODE_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_CE_UCODE_ADDR, le32_to_cpu(ce_hdr->header.ucode_version));\r\nfw_data = (const __be32 *)\r\n(rdev->me_fw->data + le32_to_cpu(me_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(me_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_ME_RAM_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_ME_RAM_WADDR, le32_to_cpu(me_hdr->header.ucode_version));\r\nWREG32(CP_ME_RAM_RADDR, le32_to_cpu(me_hdr->header.ucode_version));\r\n} else {\r\nconst __be32 *fw_data;\r\nfw_data = (const __be32 *)rdev->pfp_fw->data;\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfor (i = 0; i < CIK_PFP_UCODE_SIZE; i++)\r\nWREG32(CP_PFP_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfw_data = (const __be32 *)rdev->ce_fw->data;\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nfor (i = 0; i < CIK_CE_UCODE_SIZE; i++)\r\nWREG32(CP_CE_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_CE_UCODE_ADDR, 0);\r\nfw_data = (const __be32 *)rdev->me_fw->data;\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\nfor (i = 0; i < CIK_ME_UCODE_SIZE; i++)\r\nWREG32(CP_ME_RAM_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\n}\r\nreturn 0;\r\n}\r\nstatic int cik_cp_gfx_start(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nint r, i;\r\nWREG32(CP_MAX_CONTEXT, rdev->config.cik.max_hw_contexts - 1);\r\nWREG32(CP_ENDIAN_SWAP, 0);\r\nWREG32(CP_DEVICE_ID, 1);\r\ncik_cp_gfx_enable(rdev, true);\r\nr = radeon_ring_lock(rdev, ring, cik_default_size + 17);\r\nif (r) {\r\nDRM_ERROR("radeon: cp failed to lock ring (%d).\n", r);\r\nreturn r;\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_BASE, 2));\r\nradeon_ring_write(ring, PACKET3_BASE_INDEX(CE_PARTITION_BASE));\r\nradeon_ring_write(ring, 0x8000);\r\nradeon_ring_write(ring, 0x8000);\r\nradeon_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nradeon_ring_write(ring, PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);\r\nradeon_ring_write(ring, PACKET3(PACKET3_CONTEXT_CONTROL, 1));\r\nradeon_ring_write(ring, 0x80000000);\r\nradeon_ring_write(ring, 0x80000000);\r\nfor (i = 0; i < cik_default_size; i++)\r\nradeon_ring_write(ring, cik_default_state[i]);\r\nradeon_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nradeon_ring_write(ring, PACKET3_PREAMBLE_END_CLEAR_STATE);\r\nradeon_ring_write(ring, PACKET3(PACKET3_CLEAR_STATE, 0));\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_CONTEXT_REG, 2));\r\nradeon_ring_write(ring, 0x00000316);\r\nradeon_ring_write(ring, 0x0000000e);\r\nradeon_ring_write(ring, 0x00000010);\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\nreturn 0;\r\n}\r\nstatic void cik_cp_gfx_fini(struct radeon_device *rdev)\r\n{\r\ncik_cp_gfx_enable(rdev, false);\r\nradeon_ring_fini(rdev, &rdev->ring[RADEON_RING_TYPE_GFX_INDEX]);\r\n}\r\nstatic int cik_cp_gfx_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nu32 tmp;\r\nu32 rb_bufsz;\r\nu64 rb_addr;\r\nint r;\r\nWREG32(CP_SEM_WAIT_TIMER, 0x0);\r\nif (rdev->family != CHIP_HAWAII)\r\nWREG32(CP_SEM_INCOMPLETE_TIMER_CNTL, 0x0);\r\nWREG32(CP_RB_WPTR_DELAY, 0);\r\nWREG32(CP_RB_VMID, 0);\r\nWREG32(SCRATCH_ADDR, ((rdev->wb.gpu_addr + RADEON_WB_SCRATCH_OFFSET) >> 8) & 0xFFFFFFFF);\r\nring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nrb_bufsz = order_base_2(ring->ring_size / 8);\r\ntmp = (order_base_2(RADEON_GPU_PAGE_SIZE/8) << 8) | rb_bufsz;\r\n#ifdef __BIG_ENDIAN\r\ntmp |= BUF_SWAP_32BIT;\r\n#endif\r\nWREG32(CP_RB0_CNTL, tmp);\r\nWREG32(CP_RB0_CNTL, tmp | RB_RPTR_WR_ENA);\r\nring->wptr = 0;\r\nWREG32(CP_RB0_WPTR, ring->wptr);\r\nWREG32(CP_RB0_RPTR_ADDR, (rdev->wb.gpu_addr + RADEON_WB_CP_RPTR_OFFSET) & 0xFFFFFFFC);\r\nWREG32(CP_RB0_RPTR_ADDR_HI, upper_32_bits(rdev->wb.gpu_addr + RADEON_WB_CP_RPTR_OFFSET) & 0xFF);\r\nWREG32(SCRATCH_UMSK, 0);\r\nif (!rdev->wb.enabled)\r\ntmp |= RB_NO_UPDATE;\r\nmdelay(1);\r\nWREG32(CP_RB0_CNTL, tmp);\r\nrb_addr = ring->gpu_addr >> 8;\r\nWREG32(CP_RB0_BASE, rb_addr);\r\nWREG32(CP_RB0_BASE_HI, upper_32_bits(rb_addr));\r\ncik_cp_gfx_start(rdev);\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = true;\r\nr = radeon_ring_test(rdev, RADEON_RING_TYPE_GFX_INDEX, &rdev->ring[RADEON_RING_TYPE_GFX_INDEX]);\r\nif (r) {\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = false;\r\nreturn r;\r\n}\r\nif (rdev->asic->copy.copy_ring_index == RADEON_RING_TYPE_GFX_INDEX)\r\nradeon_ttm_set_active_vram_size(rdev, rdev->mc.real_vram_size);\r\nreturn 0;\r\n}\r\nu32 cik_gfx_get_rptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nu32 rptr;\r\nif (rdev->wb.enabled)\r\nrptr = rdev->wb.wb[ring->rptr_offs/4];\r\nelse\r\nrptr = RREG32(CP_RB0_RPTR);\r\nreturn rptr;\r\n}\r\nu32 cik_gfx_get_wptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nreturn RREG32(CP_RB0_WPTR);\r\n}\r\nvoid cik_gfx_set_wptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nWREG32(CP_RB0_WPTR, ring->wptr);\r\n(void)RREG32(CP_RB0_WPTR);\r\n}\r\nu32 cik_compute_get_rptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nu32 rptr;\r\nif (rdev->wb.enabled) {\r\nrptr = rdev->wb.wb[ring->rptr_offs/4];\r\n} else {\r\nmutex_lock(&rdev->srbm_mutex);\r\ncik_srbm_select(rdev, ring->me, ring->pipe, ring->queue, 0);\r\nrptr = RREG32(CP_HQD_PQ_RPTR);\r\ncik_srbm_select(rdev, 0, 0, 0, 0);\r\nmutex_unlock(&rdev->srbm_mutex);\r\n}\r\nreturn rptr;\r\n}\r\nu32 cik_compute_get_wptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nu32 wptr;\r\nif (rdev->wb.enabled) {\r\nwptr = rdev->wb.wb[ring->wptr_offs/4];\r\n} else {\r\nmutex_lock(&rdev->srbm_mutex);\r\ncik_srbm_select(rdev, ring->me, ring->pipe, ring->queue, 0);\r\nwptr = RREG32(CP_HQD_PQ_WPTR);\r\ncik_srbm_select(rdev, 0, 0, 0, 0);\r\nmutex_unlock(&rdev->srbm_mutex);\r\n}\r\nreturn wptr;\r\n}\r\nvoid cik_compute_set_wptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nrdev->wb.wb[ring->wptr_offs/4] = ring->wptr;\r\nWDOORBELL32(ring->doorbell_index, ring->wptr);\r\n}\r\nstatic void cik_compute_stop(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nu32 j, tmp;\r\ncik_srbm_select(rdev, ring->me, ring->pipe, ring->queue, 0);\r\ntmp = RREG32(CP_PQ_WPTR_POLL_CNTL);\r\ntmp &= ~WPTR_POLL_EN;\r\nWREG32(CP_PQ_WPTR_POLL_CNTL, tmp);\r\nif (RREG32(CP_HQD_ACTIVE) & 1) {\r\nWREG32(CP_HQD_DEQUEUE_REQUEST, 1);\r\nfor (j = 0; j < rdev->usec_timeout; j++) {\r\nif (!(RREG32(CP_HQD_ACTIVE) & 1))\r\nbreak;\r\nudelay(1);\r\n}\r\nWREG32(CP_HQD_DEQUEUE_REQUEST, 0);\r\nWREG32(CP_HQD_PQ_RPTR, 0);\r\nWREG32(CP_HQD_PQ_WPTR, 0);\r\n}\r\ncik_srbm_select(rdev, 0, 0, 0, 0);\r\n}\r\nstatic void cik_cp_compute_enable(struct radeon_device *rdev, bool enable)\r\n{\r\nif (enable)\r\nWREG32(CP_MEC_CNTL, 0);\r\nelse {\r\nmutex_lock(&rdev->srbm_mutex);\r\ncik_compute_stop(rdev,&rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX]);\r\ncik_compute_stop(rdev,&rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX]);\r\nmutex_unlock(&rdev->srbm_mutex);\r\nWREG32(CP_MEC_CNTL, (MEC_ME1_HALT | MEC_ME2_HALT));\r\nrdev->ring[CAYMAN_RING_TYPE_CP1_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP2_INDEX].ready = false;\r\n}\r\nudelay(50);\r\n}\r\nstatic int cik_cp_compute_load_microcode(struct radeon_device *rdev)\r\n{\r\nint i;\r\nif (!rdev->mec_fw)\r\nreturn -EINVAL;\r\ncik_cp_compute_enable(rdev, false);\r\nif (rdev->new_fw) {\r\nconst struct gfx_firmware_header_v1_0 *mec_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->mec_fw->data;\r\nconst __le32 *fw_data;\r\nu32 fw_size;\r\nradeon_ucode_print_gfx_hdr(&mec_hdr->header);\r\nfw_data = (const __le32 *)\r\n(rdev->mec_fw->data + le32_to_cpu(mec_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(mec_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_MEC_ME1_UCODE_ADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_MEC_ME1_UCODE_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_MEC_ME1_UCODE_ADDR, le32_to_cpu(mec_hdr->header.ucode_version));\r\nif (rdev->family == CHIP_KAVERI) {\r\nconst struct gfx_firmware_header_v1_0 *mec2_hdr =\r\n(const struct gfx_firmware_header_v1_0 *)rdev->mec2_fw->data;\r\nfw_data = (const __le32 *)\r\n(rdev->mec2_fw->data +\r\nle32_to_cpu(mec2_hdr->header.ucode_array_offset_bytes));\r\nfw_size = le32_to_cpu(mec2_hdr->header.ucode_size_bytes) / 4;\r\nWREG32(CP_MEC_ME2_UCODE_ADDR, 0);\r\nfor (i = 0; i < fw_size; i++)\r\nWREG32(CP_MEC_ME2_UCODE_DATA, le32_to_cpup(fw_data++));\r\nWREG32(CP_MEC_ME2_UCODE_ADDR, le32_to_cpu(mec2_hdr->header.ucode_version));\r\n}\r\n} else {\r\nconst __be32 *fw_data;\r\nfw_data = (const __be32 *)rdev->mec_fw->data;\r\nWREG32(CP_MEC_ME1_UCODE_ADDR, 0);\r\nfor (i = 0; i < CIK_MEC_UCODE_SIZE; i++)\r\nWREG32(CP_MEC_ME1_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_MEC_ME1_UCODE_ADDR, 0);\r\nif (rdev->family == CHIP_KAVERI) {\r\nfw_data = (const __be32 *)rdev->mec_fw->data;\r\nWREG32(CP_MEC_ME2_UCODE_ADDR, 0);\r\nfor (i = 0; i < CIK_MEC_UCODE_SIZE; i++)\r\nWREG32(CP_MEC_ME2_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_MEC_ME2_UCODE_ADDR, 0);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cik_cp_compute_start(struct radeon_device *rdev)\r\n{\r\ncik_cp_compute_enable(rdev, true);\r\nreturn 0;\r\n}\r\nstatic void cik_cp_compute_fini(struct radeon_device *rdev)\r\n{\r\nint i, idx, r;\r\ncik_cp_compute_enable(rdev, false);\r\nfor (i = 0; i < 2; i++) {\r\nif (i == 0)\r\nidx = CAYMAN_RING_TYPE_CP1_INDEX;\r\nelse\r\nidx = CAYMAN_RING_TYPE_CP2_INDEX;\r\nif (rdev->ring[idx].mqd_obj) {\r\nr = radeon_bo_reserve(rdev->ring[idx].mqd_obj, false);\r\nif (unlikely(r != 0))\r\ndev_warn(rdev->dev, "(%d) reserve MQD bo failed\n", r);\r\nradeon_bo_unpin(rdev->ring[idx].mqd_obj);\r\nradeon_bo_unreserve(rdev->ring[idx].mqd_obj);\r\nradeon_bo_unref(&rdev->ring[idx].mqd_obj);\r\nrdev->ring[idx].mqd_obj = NULL;\r\n}\r\n}\r\n}\r\nstatic void cik_mec_fini(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (rdev->mec.hpd_eop_obj) {\r\nr = radeon_bo_reserve(rdev->mec.hpd_eop_obj, false);\r\nif (unlikely(r != 0))\r\ndev_warn(rdev->dev, "(%d) reserve HPD EOP bo failed\n", r);\r\nradeon_bo_unpin(rdev->mec.hpd_eop_obj);\r\nradeon_bo_unreserve(rdev->mec.hpd_eop_obj);\r\nradeon_bo_unref(&rdev->mec.hpd_eop_obj);\r\nrdev->mec.hpd_eop_obj = NULL;\r\n}\r\n}\r\nstatic int cik_mec_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nu32 *hpd;\r\nrdev->mec.num_mec = 1;\r\nrdev->mec.num_pipe = 1;\r\nrdev->mec.num_queue = rdev->mec.num_mec * rdev->mec.num_pipe * 8;\r\nif (rdev->mec.hpd_eop_obj == NULL) {\r\nr = radeon_bo_create(rdev,\r\nrdev->mec.num_mec *rdev->mec.num_pipe * MEC_HPD_SIZE * 2,\r\nPAGE_SIZE, true,\r\nRADEON_GEM_DOMAIN_GTT, 0, NULL, NULL,\r\n&rdev->mec.hpd_eop_obj);\r\nif (r) {\r\ndev_warn(rdev->dev, "(%d) create HDP EOP bo failed\n", r);\r\nreturn r;\r\n}\r\n}\r\nr = radeon_bo_reserve(rdev->mec.hpd_eop_obj, false);\r\nif (unlikely(r != 0)) {\r\ncik_mec_fini(rdev);\r\nreturn r;\r\n}\r\nr = radeon_bo_pin(rdev->mec.hpd_eop_obj, RADEON_GEM_DOMAIN_GTT,\r\n&rdev->mec.hpd_eop_gpu_addr);\r\nif (r) {\r\ndev_warn(rdev->dev, "(%d) pin HDP EOP bo failed\n", r);\r\ncik_mec_fini(rdev);\r\nreturn r;\r\n}\r\nr = radeon_bo_kmap(rdev->mec.hpd_eop_obj, (void **)&hpd);\r\nif (r) {\r\ndev_warn(rdev->dev, "(%d) map HDP EOP bo failed\n", r);\r\ncik_mec_fini(rdev);\r\nreturn r;\r\n}\r\nmemset(hpd, 0, rdev->mec.num_mec *rdev->mec.num_pipe * MEC_HPD_SIZE * 2);\r\nradeon_bo_kunmap(rdev->mec.hpd_eop_obj);\r\nradeon_bo_unreserve(rdev->mec.hpd_eop_obj);\r\nreturn 0;\r\n}\r\nstatic int cik_cp_compute_resume(struct radeon_device *rdev)\r\n{\r\nint r, i, j, idx;\r\nu32 tmp;\r\nbool use_doorbell = true;\r\nu64 hqd_gpu_addr;\r\nu64 mqd_gpu_addr;\r\nu64 eop_gpu_addr;\r\nu64 wb_gpu_addr;\r\nu32 *buf;\r\nstruct bonaire_mqd *mqd;\r\nr = cik_cp_compute_start(rdev);\r\nif (r)\r\nreturn r;\r\ntmp = RREG32(CP_CPF_DEBUG);\r\ntmp |= (1 << 23);\r\nWREG32(CP_CPF_DEBUG, tmp);\r\nmutex_lock(&rdev->srbm_mutex);\r\nfor (i = 0; i < rdev->mec.num_pipe; ++i) {\r\ncik_srbm_select(rdev, 0, i, 0, 0);\r\neop_gpu_addr = rdev->mec.hpd_eop_gpu_addr + (i * MEC_HPD_SIZE * 2) ;\r\nWREG32(CP_HPD_EOP_BASE_ADDR, eop_gpu_addr >> 8);\r\nWREG32(CP_HPD_EOP_BASE_ADDR_HI, upper_32_bits(eop_gpu_addr) >> 8);\r\nWREG32(CP_HPD_EOP_VMID, 0);\r\ntmp = RREG32(CP_HPD_EOP_CONTROL);\r\ntmp &= ~EOP_SIZE_MASK;\r\ntmp |= order_base_2(MEC_HPD_SIZE / 8);\r\nWREG32(CP_HPD_EOP_CONTROL, tmp);\r\n}\r\nmutex_unlock(&rdev->srbm_mutex);\r\nfor (i = 0; i < 2; i++) {\r\nif (i == 0)\r\nidx = CAYMAN_RING_TYPE_CP1_INDEX;\r\nelse\r\nidx = CAYMAN_RING_TYPE_CP2_INDEX;\r\nif (rdev->ring[idx].mqd_obj == NULL) {\r\nr = radeon_bo_create(rdev,\r\nsizeof(struct bonaire_mqd),\r\nPAGE_SIZE, true,\r\nRADEON_GEM_DOMAIN_GTT, 0, NULL,\r\nNULL, &rdev->ring[idx].mqd_obj);\r\nif (r) {\r\ndev_warn(rdev->dev, "(%d) create MQD bo failed\n", r);\r\nreturn r;\r\n}\r\n}\r\nr = radeon_bo_reserve(rdev->ring[idx].mqd_obj, false);\r\nif (unlikely(r != 0)) {\r\ncik_cp_compute_fini(rdev);\r\nreturn r;\r\n}\r\nr = radeon_bo_pin(rdev->ring[idx].mqd_obj, RADEON_GEM_DOMAIN_GTT,\r\n&mqd_gpu_addr);\r\nif (r) {\r\ndev_warn(rdev->dev, "(%d) pin MQD bo failed\n", r);\r\ncik_cp_compute_fini(rdev);\r\nreturn r;\r\n}\r\nr = radeon_bo_kmap(rdev->ring[idx].mqd_obj, (void **)&buf);\r\nif (r) {\r\ndev_warn(rdev->dev, "(%d) map MQD bo failed\n", r);\r\ncik_cp_compute_fini(rdev);\r\nreturn r;\r\n}\r\nmemset(buf, 0, sizeof(struct bonaire_mqd));\r\nmqd = (struct bonaire_mqd *)buf;\r\nmqd->header = 0xC0310800;\r\nmqd->static_thread_mgmt01[0] = 0xffffffff;\r\nmqd->static_thread_mgmt01[1] = 0xffffffff;\r\nmqd->static_thread_mgmt23[0] = 0xffffffff;\r\nmqd->static_thread_mgmt23[1] = 0xffffffff;\r\nmutex_lock(&rdev->srbm_mutex);\r\ncik_srbm_select(rdev, rdev->ring[idx].me,\r\nrdev->ring[idx].pipe,\r\nrdev->ring[idx].queue, 0);\r\ntmp = RREG32(CP_PQ_WPTR_POLL_CNTL);\r\ntmp &= ~WPTR_POLL_EN;\r\nWREG32(CP_PQ_WPTR_POLL_CNTL, tmp);\r\nmqd->queue_state.cp_hqd_pq_doorbell_control =\r\nRREG32(CP_HQD_PQ_DOORBELL_CONTROL);\r\nif (use_doorbell)\r\nmqd->queue_state.cp_hqd_pq_doorbell_control |= DOORBELL_EN;\r\nelse\r\nmqd->queue_state.cp_hqd_pq_doorbell_control &= ~DOORBELL_EN;\r\nWREG32(CP_HQD_PQ_DOORBELL_CONTROL,\r\nmqd->queue_state.cp_hqd_pq_doorbell_control);\r\nmqd->queue_state.cp_hqd_dequeue_request = 0;\r\nmqd->queue_state.cp_hqd_pq_rptr = 0;\r\nmqd->queue_state.cp_hqd_pq_wptr= 0;\r\nif (RREG32(CP_HQD_ACTIVE) & 1) {\r\nWREG32(CP_HQD_DEQUEUE_REQUEST, 1);\r\nfor (j = 0; j < rdev->usec_timeout; j++) {\r\nif (!(RREG32(CP_HQD_ACTIVE) & 1))\r\nbreak;\r\nudelay(1);\r\n}\r\nWREG32(CP_HQD_DEQUEUE_REQUEST, mqd->queue_state.cp_hqd_dequeue_request);\r\nWREG32(CP_HQD_PQ_RPTR, mqd->queue_state.cp_hqd_pq_rptr);\r\nWREG32(CP_HQD_PQ_WPTR, mqd->queue_state.cp_hqd_pq_wptr);\r\n}\r\nmqd->queue_state.cp_mqd_base_addr = mqd_gpu_addr & 0xfffffffc;\r\nmqd->queue_state.cp_mqd_base_addr_hi = upper_32_bits(mqd_gpu_addr);\r\nWREG32(CP_MQD_BASE_ADDR, mqd->queue_state.cp_mqd_base_addr);\r\nWREG32(CP_MQD_BASE_ADDR_HI, mqd->queue_state.cp_mqd_base_addr_hi);\r\nmqd->queue_state.cp_mqd_control = RREG32(CP_MQD_CONTROL);\r\nmqd->queue_state.cp_mqd_control &= ~MQD_VMID_MASK;\r\nWREG32(CP_MQD_CONTROL, mqd->queue_state.cp_mqd_control);\r\nhqd_gpu_addr = rdev->ring[idx].gpu_addr >> 8;\r\nmqd->queue_state.cp_hqd_pq_base = hqd_gpu_addr;\r\nmqd->queue_state.cp_hqd_pq_base_hi = upper_32_bits(hqd_gpu_addr);\r\nWREG32(CP_HQD_PQ_BASE, mqd->queue_state.cp_hqd_pq_base);\r\nWREG32(CP_HQD_PQ_BASE_HI, mqd->queue_state.cp_hqd_pq_base_hi);\r\nmqd->queue_state.cp_hqd_pq_control = RREG32(CP_HQD_PQ_CONTROL);\r\nmqd->queue_state.cp_hqd_pq_control &=\r\n~(QUEUE_SIZE_MASK | RPTR_BLOCK_SIZE_MASK);\r\nmqd->queue_state.cp_hqd_pq_control |=\r\norder_base_2(rdev->ring[idx].ring_size / 8);\r\nmqd->queue_state.cp_hqd_pq_control |=\r\n(order_base_2(RADEON_GPU_PAGE_SIZE/8) << 8);\r\n#ifdef __BIG_ENDIAN\r\nmqd->queue_state.cp_hqd_pq_control |= BUF_SWAP_32BIT;\r\n#endif\r\nmqd->queue_state.cp_hqd_pq_control &=\r\n~(UNORD_DISPATCH | ROQ_PQ_IB_FLIP | PQ_VOLATILE);\r\nmqd->queue_state.cp_hqd_pq_control |=\r\nPRIV_STATE | KMD_QUEUE;\r\nWREG32(CP_HQD_PQ_CONTROL, mqd->queue_state.cp_hqd_pq_control);\r\nif (i == 0)\r\nwb_gpu_addr = rdev->wb.gpu_addr + CIK_WB_CP1_WPTR_OFFSET;\r\nelse\r\nwb_gpu_addr = rdev->wb.gpu_addr + CIK_WB_CP2_WPTR_OFFSET;\r\nmqd->queue_state.cp_hqd_pq_wptr_poll_addr = wb_gpu_addr & 0xfffffffc;\r\nmqd->queue_state.cp_hqd_pq_wptr_poll_addr_hi = upper_32_bits(wb_gpu_addr) & 0xffff;\r\nWREG32(CP_HQD_PQ_WPTR_POLL_ADDR, mqd->queue_state.cp_hqd_pq_wptr_poll_addr);\r\nWREG32(CP_HQD_PQ_WPTR_POLL_ADDR_HI,\r\nmqd->queue_state.cp_hqd_pq_wptr_poll_addr_hi);\r\nif (i == 0)\r\nwb_gpu_addr = rdev->wb.gpu_addr + RADEON_WB_CP1_RPTR_OFFSET;\r\nelse\r\nwb_gpu_addr = rdev->wb.gpu_addr + RADEON_WB_CP2_RPTR_OFFSET;\r\nmqd->queue_state.cp_hqd_pq_rptr_report_addr = wb_gpu_addr & 0xfffffffc;\r\nmqd->queue_state.cp_hqd_pq_rptr_report_addr_hi =\r\nupper_32_bits(wb_gpu_addr) & 0xffff;\r\nWREG32(CP_HQD_PQ_RPTR_REPORT_ADDR,\r\nmqd->queue_state.cp_hqd_pq_rptr_report_addr);\r\nWREG32(CP_HQD_PQ_RPTR_REPORT_ADDR_HI,\r\nmqd->queue_state.cp_hqd_pq_rptr_report_addr_hi);\r\nif (use_doorbell) {\r\nmqd->queue_state.cp_hqd_pq_doorbell_control =\r\nRREG32(CP_HQD_PQ_DOORBELL_CONTROL);\r\nmqd->queue_state.cp_hqd_pq_doorbell_control &= ~DOORBELL_OFFSET_MASK;\r\nmqd->queue_state.cp_hqd_pq_doorbell_control |=\r\nDOORBELL_OFFSET(rdev->ring[idx].doorbell_index);\r\nmqd->queue_state.cp_hqd_pq_doorbell_control |= DOORBELL_EN;\r\nmqd->queue_state.cp_hqd_pq_doorbell_control &=\r\n~(DOORBELL_SOURCE | DOORBELL_HIT);\r\n} else {\r\nmqd->queue_state.cp_hqd_pq_doorbell_control = 0;\r\n}\r\nWREG32(CP_HQD_PQ_DOORBELL_CONTROL,\r\nmqd->queue_state.cp_hqd_pq_doorbell_control);\r\nrdev->ring[idx].wptr = 0;\r\nmqd->queue_state.cp_hqd_pq_wptr = rdev->ring[idx].wptr;\r\nWREG32(CP_HQD_PQ_WPTR, mqd->queue_state.cp_hqd_pq_wptr);\r\nmqd->queue_state.cp_hqd_pq_rptr = RREG32(CP_HQD_PQ_RPTR);\r\nmqd->queue_state.cp_hqd_vmid = 0;\r\nWREG32(CP_HQD_VMID, mqd->queue_state.cp_hqd_vmid);\r\nmqd->queue_state.cp_hqd_active = 1;\r\nWREG32(CP_HQD_ACTIVE, mqd->queue_state.cp_hqd_active);\r\ncik_srbm_select(rdev, 0, 0, 0, 0);\r\nmutex_unlock(&rdev->srbm_mutex);\r\nradeon_bo_kunmap(rdev->ring[idx].mqd_obj);\r\nradeon_bo_unreserve(rdev->ring[idx].mqd_obj);\r\nrdev->ring[idx].ready = true;\r\nr = radeon_ring_test(rdev, idx, &rdev->ring[idx]);\r\nif (r)\r\nrdev->ring[idx].ready = false;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cik_cp_enable(struct radeon_device *rdev, bool enable)\r\n{\r\ncik_cp_gfx_enable(rdev, enable);\r\ncik_cp_compute_enable(rdev, enable);\r\n}\r\nstatic int cik_cp_load_microcode(struct radeon_device *rdev)\r\n{\r\nint r;\r\nr = cik_cp_gfx_load_microcode(rdev);\r\nif (r)\r\nreturn r;\r\nr = cik_cp_compute_load_microcode(rdev);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nstatic void cik_cp_fini(struct radeon_device *rdev)\r\n{\r\ncik_cp_gfx_fini(rdev);\r\ncik_cp_compute_fini(rdev);\r\n}\r\nstatic int cik_cp_resume(struct radeon_device *rdev)\r\n{\r\nint r;\r\ncik_enable_gui_idle_interrupt(rdev, false);\r\nr = cik_cp_load_microcode(rdev);\r\nif (r)\r\nreturn r;\r\nr = cik_cp_gfx_resume(rdev);\r\nif (r)\r\nreturn r;\r\nr = cik_cp_compute_resume(rdev);\r\nif (r)\r\nreturn r;\r\ncik_enable_gui_idle_interrupt(rdev, true);\r\nreturn 0;\r\n}\r\nstatic void cik_print_gpu_status_regs(struct radeon_device *rdev)\r\n{\r\ndev_info(rdev->dev, " GRBM_STATUS=0x%08X\n",\r\nRREG32(GRBM_STATUS));\r\ndev_info(rdev->dev, " GRBM_STATUS2=0x%08X\n",\r\nRREG32(GRBM_STATUS2));\r\ndev_info(rdev->dev, " GRBM_STATUS_SE0=0x%08X\n",\r\nRREG32(GRBM_STATUS_SE0));\r\ndev_info(rdev->dev, " GRBM_STATUS_SE1=0x%08X\n",\r\nRREG32(GRBM_STATUS_SE1));\r\ndev_info(rdev->dev, " GRBM_STATUS_SE2=0x%08X\n",\r\nRREG32(GRBM_STATUS_SE2));\r\ndev_info(rdev->dev, " GRBM_STATUS_SE3=0x%08X\n",\r\nRREG32(GRBM_STATUS_SE3));\r\ndev_info(rdev->dev, " SRBM_STATUS=0x%08X\n",\r\nRREG32(SRBM_STATUS));\r\ndev_info(rdev->dev, " SRBM_STATUS2=0x%08X\n",\r\nRREG32(SRBM_STATUS2));\r\ndev_info(rdev->dev, " SDMA0_STATUS_REG = 0x%08X\n",\r\nRREG32(SDMA0_STATUS_REG + SDMA0_REGISTER_OFFSET));\r\ndev_info(rdev->dev, " SDMA1_STATUS_REG = 0x%08X\n",\r\nRREG32(SDMA0_STATUS_REG + SDMA1_REGISTER_OFFSET));\r\ndev_info(rdev->dev, " CP_STAT = 0x%08x\n", RREG32(CP_STAT));\r\ndev_info(rdev->dev, " CP_STALLED_STAT1 = 0x%08x\n",\r\nRREG32(CP_STALLED_STAT1));\r\ndev_info(rdev->dev, " CP_STALLED_STAT2 = 0x%08x\n",\r\nRREG32(CP_STALLED_STAT2));\r\ndev_info(rdev->dev, " CP_STALLED_STAT3 = 0x%08x\n",\r\nRREG32(CP_STALLED_STAT3));\r\ndev_info(rdev->dev, " CP_CPF_BUSY_STAT = 0x%08x\n",\r\nRREG32(CP_CPF_BUSY_STAT));\r\ndev_info(rdev->dev, " CP_CPF_STALLED_STAT1 = 0x%08x\n",\r\nRREG32(CP_CPF_STALLED_STAT1));\r\ndev_info(rdev->dev, " CP_CPF_STATUS = 0x%08x\n", RREG32(CP_CPF_STATUS));\r\ndev_info(rdev->dev, " CP_CPC_BUSY_STAT = 0x%08x\n", RREG32(CP_CPC_BUSY_STAT));\r\ndev_info(rdev->dev, " CP_CPC_STALLED_STAT1 = 0x%08x\n",\r\nRREG32(CP_CPC_STALLED_STAT1));\r\ndev_info(rdev->dev, " CP_CPC_STATUS = 0x%08x\n", RREG32(CP_CPC_STATUS));\r\n}\r\nu32 cik_gpu_check_soft_reset(struct radeon_device *rdev)\r\n{\r\nu32 reset_mask = 0;\r\nu32 tmp;\r\ntmp = RREG32(GRBM_STATUS);\r\nif (tmp & (PA_BUSY | SC_BUSY |\r\nBCI_BUSY | SX_BUSY |\r\nTA_BUSY | VGT_BUSY |\r\nDB_BUSY | CB_BUSY |\r\nGDS_BUSY | SPI_BUSY |\r\nIA_BUSY | IA_BUSY_NO_DMA))\r\nreset_mask |= RADEON_RESET_GFX;\r\nif (tmp & (CP_BUSY | CP_COHERENCY_BUSY))\r\nreset_mask |= RADEON_RESET_CP;\r\ntmp = RREG32(GRBM_STATUS2);\r\nif (tmp & RLC_BUSY)\r\nreset_mask |= RADEON_RESET_RLC;\r\ntmp = RREG32(SDMA0_STATUS_REG + SDMA0_REGISTER_OFFSET);\r\nif (!(tmp & SDMA_IDLE))\r\nreset_mask |= RADEON_RESET_DMA;\r\ntmp = RREG32(SDMA0_STATUS_REG + SDMA1_REGISTER_OFFSET);\r\nif (!(tmp & SDMA_IDLE))\r\nreset_mask |= RADEON_RESET_DMA1;\r\ntmp = RREG32(SRBM_STATUS2);\r\nif (tmp & SDMA_BUSY)\r\nreset_mask |= RADEON_RESET_DMA;\r\nif (tmp & SDMA1_BUSY)\r\nreset_mask |= RADEON_RESET_DMA1;\r\ntmp = RREG32(SRBM_STATUS);\r\nif (tmp & IH_BUSY)\r\nreset_mask |= RADEON_RESET_IH;\r\nif (tmp & SEM_BUSY)\r\nreset_mask |= RADEON_RESET_SEM;\r\nif (tmp & GRBM_RQ_PENDING)\r\nreset_mask |= RADEON_RESET_GRBM;\r\nif (tmp & VMC_BUSY)\r\nreset_mask |= RADEON_RESET_VMC;\r\nif (tmp & (MCB_BUSY | MCB_NON_DISPLAY_BUSY |\r\nMCC_BUSY | MCD_BUSY))\r\nreset_mask |= RADEON_RESET_MC;\r\nif (evergreen_is_display_hung(rdev))\r\nreset_mask |= RADEON_RESET_DISPLAY;\r\nif (reset_mask & RADEON_RESET_MC) {\r\nDRM_DEBUG("MC busy: 0x%08X, clearing.\n", reset_mask);\r\nreset_mask &= ~RADEON_RESET_MC;\r\n}\r\nreturn reset_mask;\r\n}\r\nstatic void cik_gpu_soft_reset(struct radeon_device *rdev, u32 reset_mask)\r\n{\r\nstruct evergreen_mc_save save;\r\nu32 grbm_soft_reset = 0, srbm_soft_reset = 0;\r\nu32 tmp;\r\nif (reset_mask == 0)\r\nreturn;\r\ndev_info(rdev->dev, "GPU softreset: 0x%08X\n", reset_mask);\r\ncik_print_gpu_status_regs(rdev);\r\ndev_info(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_ADDR 0x%08X\n",\r\nRREG32(VM_CONTEXT1_PROTECTION_FAULT_ADDR));\r\ndev_info(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",\r\nRREG32(VM_CONTEXT1_PROTECTION_FAULT_STATUS));\r\ncik_fini_pg(rdev);\r\ncik_fini_cg(rdev);\r\ncik_rlc_stop(rdev);\r\nWREG32(CP_ME_CNTL, CP_ME_HALT | CP_PFP_HALT | CP_CE_HALT);\r\nWREG32(CP_MEC_CNTL, MEC_ME1_HALT | MEC_ME2_HALT);\r\nif (reset_mask & RADEON_RESET_DMA) {\r\ntmp = RREG32(SDMA0_ME_CNTL + SDMA0_REGISTER_OFFSET);\r\ntmp |= SDMA_HALT;\r\nWREG32(SDMA0_ME_CNTL + SDMA0_REGISTER_OFFSET, tmp);\r\n}\r\nif (reset_mask & RADEON_RESET_DMA1) {\r\ntmp = RREG32(SDMA0_ME_CNTL + SDMA1_REGISTER_OFFSET);\r\ntmp |= SDMA_HALT;\r\nWREG32(SDMA0_ME_CNTL + SDMA1_REGISTER_OFFSET, tmp);\r\n}\r\nevergreen_mc_stop(rdev, &save);\r\nif (evergreen_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nif (reset_mask & (RADEON_RESET_GFX | RADEON_RESET_COMPUTE | RADEON_RESET_CP))\r\ngrbm_soft_reset = SOFT_RESET_CP | SOFT_RESET_GFX;\r\nif (reset_mask & RADEON_RESET_CP) {\r\ngrbm_soft_reset |= SOFT_RESET_CP;\r\nsrbm_soft_reset |= SOFT_RESET_GRBM;\r\n}\r\nif (reset_mask & RADEON_RESET_DMA)\r\nsrbm_soft_reset |= SOFT_RESET_SDMA;\r\nif (reset_mask & RADEON_RESET_DMA1)\r\nsrbm_soft_reset |= SOFT_RESET_SDMA1;\r\nif (reset_mask & RADEON_RESET_DISPLAY)\r\nsrbm_soft_reset |= SOFT_RESET_DC;\r\nif (reset_mask & RADEON_RESET_RLC)\r\ngrbm_soft_reset |= SOFT_RESET_RLC;\r\nif (reset_mask & RADEON_RESET_SEM)\r\nsrbm_soft_reset |= SOFT_RESET_SEM;\r\nif (reset_mask & RADEON_RESET_IH)\r\nsrbm_soft_reset |= SOFT_RESET_IH;\r\nif (reset_mask & RADEON_RESET_GRBM)\r\nsrbm_soft_reset |= SOFT_RESET_GRBM;\r\nif (reset_mask & RADEON_RESET_VMC)\r\nsrbm_soft_reset |= SOFT_RESET_VMC;\r\nif (!(rdev->flags & RADEON_IS_IGP)) {\r\nif (reset_mask & RADEON_RESET_MC)\r\nsrbm_soft_reset |= SOFT_RESET_MC;\r\n}\r\nif (grbm_soft_reset) {\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\ntmp |= grbm_soft_reset;\r\ndev_info(rdev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~grbm_soft_reset;\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\n}\r\nif (srbm_soft_reset) {\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\ntmp |= srbm_soft_reset;\r\ndev_info(rdev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~srbm_soft_reset;\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\n}\r\nudelay(50);\r\nevergreen_mc_resume(rdev, &save);\r\nudelay(50);\r\ncik_print_gpu_status_regs(rdev);\r\n}\r\nstatic void kv_save_regs_for_reset(struct radeon_device *rdev,\r\nstruct kv_reset_save_regs *save)\r\n{\r\nsave->gmcon_reng_execute = RREG32(GMCON_RENG_EXECUTE);\r\nsave->gmcon_misc = RREG32(GMCON_MISC);\r\nsave->gmcon_misc3 = RREG32(GMCON_MISC3);\r\nWREG32(GMCON_RENG_EXECUTE, save->gmcon_reng_execute & ~RENG_EXECUTE_ON_PWR_UP);\r\nWREG32(GMCON_MISC, save->gmcon_misc & ~(RENG_EXECUTE_ON_REG_UPDATE |\r\nSTCTRL_STUTTER_EN));\r\n}\r\nstatic void kv_restore_regs_for_reset(struct radeon_device *rdev,\r\nstruct kv_reset_save_regs *save)\r\n{\r\nint i;\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_CONFIG, 0x200010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_CONFIG, 0x300010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x210000);\r\nWREG32(GMCON_PGFSM_CONFIG, 0xa00010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x21003);\r\nWREG32(GMCON_PGFSM_CONFIG, 0xb00010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x2b00);\r\nWREG32(GMCON_PGFSM_CONFIG, 0xc00010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_CONFIG, 0xd00010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x420000);\r\nWREG32(GMCON_PGFSM_CONFIG, 0x100010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x120202);\r\nWREG32(GMCON_PGFSM_CONFIG, 0x500010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x3e3e36);\r\nWREG32(GMCON_PGFSM_CONFIG, 0x600010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x373f3e);\r\nWREG32(GMCON_PGFSM_CONFIG, 0x700010ff);\r\nfor (i = 0; i < 5; i++)\r\nWREG32(GMCON_PGFSM_WRITE, 0);\r\nWREG32(GMCON_PGFSM_WRITE, 0x3e1332);\r\nWREG32(GMCON_PGFSM_CONFIG, 0xe00010ff);\r\nWREG32(GMCON_MISC3, save->gmcon_misc3);\r\nWREG32(GMCON_MISC, save->gmcon_misc);\r\nWREG32(GMCON_RENG_EXECUTE, save->gmcon_reng_execute);\r\n}\r\nstatic void cik_gpu_pci_config_reset(struct radeon_device *rdev)\r\n{\r\nstruct evergreen_mc_save save;\r\nstruct kv_reset_save_regs kv_save = { 0 };\r\nu32 tmp, i;\r\ndev_info(rdev->dev, "GPU pci config reset\n");\r\ncik_fini_pg(rdev);\r\ncik_fini_cg(rdev);\r\nWREG32(CP_ME_CNTL, CP_ME_HALT | CP_PFP_HALT | CP_CE_HALT);\r\nWREG32(CP_MEC_CNTL, MEC_ME1_HALT | MEC_ME2_HALT);\r\ntmp = RREG32(SDMA0_ME_CNTL + SDMA0_REGISTER_OFFSET);\r\ntmp |= SDMA_HALT;\r\nWREG32(SDMA0_ME_CNTL + SDMA0_REGISTER_OFFSET, tmp);\r\ntmp = RREG32(SDMA0_ME_CNTL + SDMA1_REGISTER_OFFSET);\r\ntmp |= SDMA_HALT;\r\nWREG32(SDMA0_ME_CNTL + SDMA1_REGISTER_OFFSET, tmp);\r\ncik_rlc_stop(rdev);\r\nudelay(50);\r\nevergreen_mc_stop(rdev, &save);\r\nif (evergreen_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timed out !\n");\r\n}\r\nif (rdev->flags & RADEON_IS_IGP)\r\nkv_save_regs_for_reset(rdev, &kv_save);\r\npci_clear_master(rdev->pdev);\r\nradeon_pci_config_reset(rdev);\r\nudelay(100);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(CONFIG_MEMSIZE) != 0xffffffff)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (rdev->flags & RADEON_IS_IGP)\r\nkv_restore_regs_for_reset(rdev, &kv_save);\r\n}\r\nint cik_asic_reset(struct radeon_device *rdev, bool hard)\r\n{\r\nu32 reset_mask;\r\nif (hard) {\r\ncik_gpu_pci_config_reset(rdev);\r\nreturn 0;\r\n}\r\nreset_mask = cik_gpu_check_soft_reset(rdev);\r\nif (reset_mask)\r\nr600_set_bios_scratch_engine_hung(rdev, true);\r\ncik_gpu_soft_reset(rdev, reset_mask);\r\nreset_mask = cik_gpu_check_soft_reset(rdev);\r\nif (reset_mask && radeon_hard_reset)\r\ncik_gpu_pci_config_reset(rdev);\r\nreset_mask = cik_gpu_check_soft_reset(rdev);\r\nif (!reset_mask)\r\nr600_set_bios_scratch_engine_hung(rdev, false);\r\nreturn 0;\r\n}\r\nbool cik_gfx_is_lockup(struct radeon_device *rdev, struct radeon_ring *ring)\r\n{\r\nu32 reset_mask = cik_gpu_check_soft_reset(rdev);\r\nif (!(reset_mask & (RADEON_RESET_GFX |\r\nRADEON_RESET_COMPUTE |\r\nRADEON_RESET_CP))) {\r\nradeon_ring_lockup_update(rdev, ring);\r\nreturn false;\r\n}\r\nreturn radeon_ring_test_lockup(rdev, ring);\r\n}\r\nstatic void cik_mc_program(struct radeon_device *rdev)\r\n{\r\nstruct evergreen_mc_save save;\r\nu32 tmp;\r\nint i, j;\r\nfor (i = 0, j = 0; i < 32; i++, j += 0x18) {\r\nWREG32((0x2c14 + j), 0x00000000);\r\nWREG32((0x2c18 + j), 0x00000000);\r\nWREG32((0x2c1c + j), 0x00000000);\r\nWREG32((0x2c20 + j), 0x00000000);\r\nWREG32((0x2c24 + j), 0x00000000);\r\n}\r\nWREG32(HDP_REG_COHERENCY_FLUSH_CNTL, 0);\r\nevergreen_mc_stop(rdev, &save);\r\nif (radeon_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nWREG32(VGA_HDP_CONTROL, VGA_MEMORY_DISABLE);\r\nWREG32(MC_VM_SYSTEM_APERTURE_LOW_ADDR,\r\nrdev->mc.vram_start >> 12);\r\nWREG32(MC_VM_SYSTEM_APERTURE_HIGH_ADDR,\r\nrdev->mc.vram_end >> 12);\r\nWREG32(MC_VM_SYSTEM_APERTURE_DEFAULT_ADDR,\r\nrdev->vram_scratch.gpu_addr >> 12);\r\ntmp = ((rdev->mc.vram_end >> 24) & 0xFFFF) << 16;\r\ntmp |= ((rdev->mc.vram_start >> 24) & 0xFFFF);\r\nWREG32(MC_VM_FB_LOCATION, tmp);\r\nWREG32(HDP_NONSURFACE_BASE, (rdev->mc.vram_start >> 8));\r\nWREG32(HDP_NONSURFACE_INFO, (2 << 7) | (1 << 30));\r\nWREG32(HDP_NONSURFACE_SIZE, 0x3FFFFFFF);\r\nWREG32(MC_VM_AGP_BASE, 0);\r\nWREG32(MC_VM_AGP_TOP, 0x0FFFFFFF);\r\nWREG32(MC_VM_AGP_BOT, 0x0FFFFFFF);\r\nif (radeon_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nevergreen_mc_resume(rdev, &save);\r\nrv515_vga_render_disable(rdev);\r\n}\r\nstatic int cik_mc_init(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\nint chansize, numchan;\r\nrdev->mc.vram_is_ddr = true;\r\ntmp = RREG32(MC_ARB_RAMCFG);\r\nif (tmp & CHANSIZE_MASK) {\r\nchansize = 64;\r\n} else {\r\nchansize = 32;\r\n}\r\ntmp = RREG32(MC_SHARED_CHMAP);\r\nswitch ((tmp & NOOFCHAN_MASK) >> NOOFCHAN_SHIFT) {\r\ncase 0:\r\ndefault:\r\nnumchan = 1;\r\nbreak;\r\ncase 1:\r\nnumchan = 2;\r\nbreak;\r\ncase 2:\r\nnumchan = 4;\r\nbreak;\r\ncase 3:\r\nnumchan = 8;\r\nbreak;\r\ncase 4:\r\nnumchan = 3;\r\nbreak;\r\ncase 5:\r\nnumchan = 6;\r\nbreak;\r\ncase 6:\r\nnumchan = 10;\r\nbreak;\r\ncase 7:\r\nnumchan = 12;\r\nbreak;\r\ncase 8:\r\nnumchan = 16;\r\nbreak;\r\n}\r\nrdev->mc.vram_width = numchan * chansize;\r\nrdev->mc.aper_base = pci_resource_start(rdev->pdev, 0);\r\nrdev->mc.aper_size = pci_resource_len(rdev->pdev, 0);\r\nrdev->mc.mc_vram_size = RREG32(CONFIG_MEMSIZE) * 1024ULL * 1024ULL;\r\nrdev->mc.real_vram_size = RREG32(CONFIG_MEMSIZE) * 1024ULL * 1024ULL;\r\nrdev->mc.visible_vram_size = rdev->mc.aper_size;\r\nsi_vram_gtt_location(rdev, &rdev->mc);\r\nradeon_update_bandwidth_info(rdev);\r\nreturn 0;\r\n}\r\nvoid cik_pcie_gart_tlb_flush(struct radeon_device *rdev)\r\n{\r\nWREG32(HDP_MEM_COHERENCY_FLUSH_CNTL, 0);\r\nWREG32(VM_INVALIDATE_REQUEST, 0x1);\r\n}\r\nstatic void cik_pcie_init_compute_vmid(struct radeon_device *rdev)\r\n{\r\nint i;\r\nuint32_t sh_mem_bases, sh_mem_config;\r\nsh_mem_bases = 0x6000 | 0x6000 << 16;\r\nsh_mem_config = ALIGNMENT_MODE(SH_MEM_ALIGNMENT_MODE_UNALIGNED);\r\nsh_mem_config |= DEFAULT_MTYPE(MTYPE_NONCACHED);\r\nmutex_lock(&rdev->srbm_mutex);\r\nfor (i = 8; i < 16; i++) {\r\ncik_srbm_select(rdev, 0, 0, 0, i);\r\nWREG32(SH_MEM_CONFIG, sh_mem_config);\r\nWREG32(SH_MEM_APE1_BASE, 1);\r\nWREG32(SH_MEM_APE1_LIMIT, 0);\r\nWREG32(SH_MEM_BASES, sh_mem_bases);\r\n}\r\ncik_srbm_select(rdev, 0, 0, 0, 0);\r\nmutex_unlock(&rdev->srbm_mutex);\r\n}\r\nstatic int cik_pcie_gart_enable(struct radeon_device *rdev)\r\n{\r\nint r, i;\r\nif (rdev->gart.robj == NULL) {\r\ndev_err(rdev->dev, "No VRAM object for PCIE GART.\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_gart_table_vram_pin(rdev);\r\nif (r)\r\nreturn r;\r\nWREG32(MC_VM_MX_L1_TLB_CNTL,\r\n(0xA << 7) |\r\nENABLE_L1_TLB |\r\nENABLE_L1_FRAGMENT_PROCESSING |\r\nSYSTEM_ACCESS_MODE_NOT_IN_SYS |\r\nENABLE_ADVANCED_DRIVER_MODEL |\r\nSYSTEM_APERTURE_UNMAPPED_ACCESS_PASS_THRU);\r\nWREG32(VM_L2_CNTL, ENABLE_L2_CACHE |\r\nENABLE_L2_FRAGMENT_PROCESSING |\r\nENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE |\r\nENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE |\r\nEFFECTIVE_L2_QUEUE_SIZE(7) |\r\nCONTEXT1_IDENTITY_ACCESS_MODE(1));\r\nWREG32(VM_L2_CNTL2, INVALIDATE_ALL_L1_TLBS | INVALIDATE_L2_CACHE);\r\nWREG32(VM_L2_CNTL3, L2_CACHE_BIGK_ASSOCIATIVITY |\r\nBANK_SELECT(4) |\r\nL2_CACHE_BIGK_FRAGMENT_SIZE(4));\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_START_ADDR, rdev->mc.gtt_start >> 12);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_END_ADDR, rdev->mc.gtt_end >> 12);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR, rdev->gart.table_addr >> 12);\r\nWREG32(VM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(rdev->dummy_page.addr >> 12));\r\nWREG32(VM_CONTEXT0_CNTL2, 0);\r\nWREG32(VM_CONTEXT0_CNTL, (ENABLE_CONTEXT | PAGE_TABLE_DEPTH(0) |\r\nRANGE_PROTECTION_FAULT_ENABLE_DEFAULT));\r\nWREG32(0x15D4, 0);\r\nWREG32(0x15D8, 0);\r\nWREG32(0x15DC, 0);\r\nWREG32(VM_CONTEXT1_PAGE_TABLE_START_ADDR, 0);\r\nWREG32(VM_CONTEXT1_PAGE_TABLE_END_ADDR, rdev->vm_manager.max_pfn - 1);\r\nfor (i = 1; i < 16; i++) {\r\nif (i < 8)\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (i << 2),\r\nrdev->vm_manager.saved_table_addr[i]);\r\nelse\r\nWREG32(VM_CONTEXT8_PAGE_TABLE_BASE_ADDR + ((i - 8) << 2),\r\nrdev->vm_manager.saved_table_addr[i]);\r\n}\r\nWREG32(VM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(rdev->dummy_page.addr >> 12));\r\nWREG32(VM_CONTEXT1_CNTL2, 4);\r\nWREG32(VM_CONTEXT1_CNTL, ENABLE_CONTEXT | PAGE_TABLE_DEPTH(1) |\r\nPAGE_TABLE_BLOCK_SIZE(radeon_vm_block_size - 9) |\r\nRANGE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nRANGE_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nDUMMY_PAGE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nDUMMY_PAGE_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nPDE0_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nPDE0_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nVALID_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nVALID_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nREAD_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nREAD_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nWRITE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nWRITE_PROTECTION_FAULT_ENABLE_DEFAULT);\r\nif (rdev->family == CHIP_KAVERI) {\r\nu32 tmp = RREG32(CHUB_CONTROL);\r\ntmp &= ~BYPASS_VM;\r\nWREG32(CHUB_CONTROL, tmp);\r\n}\r\nmutex_lock(&rdev->srbm_mutex);\r\nfor (i = 0; i < 16; i++) {\r\ncik_srbm_select(rdev, 0, 0, 0, i);\r\nWREG32(SH_MEM_CONFIG, SH_MEM_CONFIG_GFX_DEFAULT);\r\nWREG32(SH_MEM_APE1_BASE, 1);\r\nWREG32(SH_MEM_APE1_LIMIT, 0);\r\nWREG32(SH_MEM_BASES, 0);\r\nWREG32(SDMA0_GFX_VIRTUAL_ADDR + SDMA0_REGISTER_OFFSET, 0);\r\nWREG32(SDMA0_GFX_APE1_CNTL + SDMA0_REGISTER_OFFSET, 0);\r\nWREG32(SDMA0_GFX_VIRTUAL_ADDR + SDMA1_REGISTER_OFFSET, 0);\r\nWREG32(SDMA0_GFX_APE1_CNTL + SDMA1_REGISTER_OFFSET, 0);\r\n}\r\ncik_srbm_select(rdev, 0, 0, 0, 0);\r\nmutex_unlock(&rdev->srbm_mutex);\r\ncik_pcie_init_compute_vmid(rdev);\r\ncik_pcie_gart_tlb_flush(rdev);\r\nDRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",\r\n(unsigned)(rdev->mc.gtt_size >> 20),\r\n(unsigned long long)rdev->gart.table_addr);\r\nrdev->gart.ready = true;\r\nreturn 0;\r\n}\r\nstatic void cik_pcie_gart_disable(struct radeon_device *rdev)\r\n{\r\nunsigned i;\r\nfor (i = 1; i < 16; ++i) {\r\nuint32_t reg;\r\nif (i < 8)\r\nreg = VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (i << 2);\r\nelse\r\nreg = VM_CONTEXT8_PAGE_TABLE_BASE_ADDR + ((i - 8) << 2);\r\nrdev->vm_manager.saved_table_addr[i] = RREG32(reg);\r\n}\r\nWREG32(VM_CONTEXT0_CNTL, 0);\r\nWREG32(VM_CONTEXT1_CNTL, 0);\r\nWREG32(MC_VM_MX_L1_TLB_CNTL, SYSTEM_ACCESS_MODE_NOT_IN_SYS |\r\nSYSTEM_APERTURE_UNMAPPED_ACCESS_PASS_THRU);\r\nWREG32(VM_L2_CNTL,\r\nENABLE_L2_FRAGMENT_PROCESSING |\r\nENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE |\r\nENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE |\r\nEFFECTIVE_L2_QUEUE_SIZE(7) |\r\nCONTEXT1_IDENTITY_ACCESS_MODE(1));\r\nWREG32(VM_L2_CNTL2, 0);\r\nWREG32(VM_L2_CNTL3, L2_CACHE_BIGK_ASSOCIATIVITY |\r\nL2_CACHE_BIGK_FRAGMENT_SIZE(6));\r\nradeon_gart_table_vram_unpin(rdev);\r\n}\r\nstatic void cik_pcie_gart_fini(struct radeon_device *rdev)\r\n{\r\ncik_pcie_gart_disable(rdev);\r\nradeon_gart_table_vram_free(rdev);\r\nradeon_gart_fini(rdev);\r\n}\r\nint cik_ib_parse(struct radeon_device *rdev, struct radeon_ib *ib)\r\n{\r\nreturn 0;\r\n}\r\nint cik_vm_init(struct radeon_device *rdev)\r\n{\r\nrdev->vm_manager.nvm = RADEON_NUM_OF_VMIDS;\r\nif (rdev->flags & RADEON_IS_IGP) {\r\nu64 tmp = RREG32(MC_VM_FB_OFFSET);\r\ntmp <<= 22;\r\nrdev->vm_manager.vram_base_offset = tmp;\r\n} else\r\nrdev->vm_manager.vram_base_offset = 0;\r\nreturn 0;\r\n}\r\nvoid cik_vm_fini(struct radeon_device *rdev)\r\n{\r\n}\r\nstatic void cik_vm_decode_fault(struct radeon_device *rdev,\r\nu32 status, u32 addr, u32 mc_client)\r\n{\r\nu32 mc_id;\r\nu32 vmid = (status & FAULT_VMID_MASK) >> FAULT_VMID_SHIFT;\r\nu32 protections = (status & PROTECTIONS_MASK) >> PROTECTIONS_SHIFT;\r\nchar block[5] = { mc_client >> 24, (mc_client >> 16) & 0xff,\r\n(mc_client >> 8) & 0xff, mc_client & 0xff, 0 };\r\nif (rdev->family == CHIP_HAWAII)\r\nmc_id = (status & HAWAII_MEMORY_CLIENT_ID_MASK) >> MEMORY_CLIENT_ID_SHIFT;\r\nelse\r\nmc_id = (status & MEMORY_CLIENT_ID_MASK) >> MEMORY_CLIENT_ID_SHIFT;\r\nprintk("VM fault (0x%02x, vmid %d) at page %u, %s from '%s' (0x%08x) (%d)\n",\r\nprotections, vmid, addr,\r\n(status & MEMORY_CLIENT_RW_MASK) ? "write" : "read",\r\nblock, mc_client, mc_id);\r\n}\r\nvoid cik_vm_flush(struct radeon_device *rdev, struct radeon_ring *ring,\r\nunsigned vm_id, uint64_t pd_addr)\r\n{\r\nint usepfp = (ring->idx == RADEON_RING_TYPE_GFX_INDEX);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(usepfp) |\r\nWRITE_DATA_DST_SEL(0)));\r\nif (vm_id < 8) {\r\nradeon_ring_write(ring,\r\n(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (vm_id << 2)) >> 2);\r\n} else {\r\nradeon_ring_write(ring,\r\n(VM_CONTEXT8_PAGE_TABLE_BASE_ADDR + ((vm_id - 8) << 2)) >> 2);\r\n}\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, pd_addr >> 12);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(usepfp) |\r\nWRITE_DATA_DST_SEL(0)));\r\nradeon_ring_write(ring, SRBM_GFX_CNTL >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, VMID(vm_id));\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 6));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(usepfp) |\r\nWRITE_DATA_DST_SEL(0)));\r\nradeon_ring_write(ring, SH_MEM_BASES >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, SH_MEM_CONFIG_GFX_DEFAULT);\r\nradeon_ring_write(ring, 1);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(usepfp) |\r\nWRITE_DATA_DST_SEL(0)));\r\nradeon_ring_write(ring, SRBM_GFX_CNTL >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, VMID(0));\r\ncik_hdp_flush_cp_ring_emit(rdev, ring->idx);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\r\nradeon_ring_write(ring, (WRITE_DATA_ENGINE_SEL(usepfp) |\r\nWRITE_DATA_DST_SEL(0)));\r\nradeon_ring_write(ring, VM_INVALIDATE_REQUEST >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 1 << vm_id);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WAIT_REG_MEM, 5));\r\nradeon_ring_write(ring, (WAIT_REG_MEM_OPERATION(0) |\r\nWAIT_REG_MEM_FUNCTION(0) |\r\nWAIT_REG_MEM_ENGINE(0)));\r\nradeon_ring_write(ring, VM_INVALIDATE_REQUEST >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0x20);\r\nif (usepfp) {\r\nradeon_ring_write(ring, PACKET3(PACKET3_PFP_SYNC_ME, 0));\r\nradeon_ring_write(ring, 0x0);\r\n}\r\n}\r\nstatic void cik_enable_gui_idle_interrupt(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 tmp = RREG32(CP_INT_CNTL_RING0);\r\nif (enable)\r\ntmp |= (CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\nelse\r\ntmp &= ~(CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\nWREG32(CP_INT_CNTL_RING0, tmp);\r\n}\r\nstatic void cik_enable_lbpw(struct radeon_device *rdev, bool enable)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(RLC_LB_CNTL);\r\nif (enable)\r\ntmp |= LOAD_BALANCE_ENABLE;\r\nelse\r\ntmp &= ~LOAD_BALANCE_ENABLE;\r\nWREG32(RLC_LB_CNTL, tmp);\r\n}\r\nstatic void cik_wait_for_rlc_serdes(struct radeon_device *rdev)\r\n{\r\nu32 i, j, k;\r\nu32 mask;\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\nfor (i = 0; i < rdev->config.cik.max_shader_engines; i++) {\r\nfor (j = 0; j < rdev->config.cik.max_sh_per_se; j++) {\r\ncik_select_se_sh(rdev, i, j);\r\nfor (k = 0; k < rdev->usec_timeout; k++) {\r\nif (RREG32(RLC_SERDES_CU_MASTER_BUSY) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\n}\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\nmask = SE_MASTER_BUSY_MASK | GC_MASTER_BUSY | TC0_MASTER_BUSY | TC1_MASTER_BUSY;\r\nfor (k = 0; k < rdev->usec_timeout; k++) {\r\nif ((RREG32(RLC_SERDES_NONCU_MASTER_BUSY) & mask) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nstatic void cik_update_rlc(struct radeon_device *rdev, u32 rlc)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(RLC_CNTL);\r\nif (tmp != rlc)\r\nWREG32(RLC_CNTL, rlc);\r\n}\r\nstatic u32 cik_halt_rlc(struct radeon_device *rdev)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_CNTL);\r\nif (data & RLC_ENABLE) {\r\nu32 i;\r\ndata &= ~RLC_ENABLE;\r\nWREG32(RLC_CNTL, data);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif ((RREG32(RLC_GPM_STAT) & RLC_GPM_BUSY) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\ncik_wait_for_rlc_serdes(rdev);\r\n}\r\nreturn orig;\r\n}\r\nvoid cik_enter_rlc_safe_mode(struct radeon_device *rdev)\r\n{\r\nu32 tmp, i, mask;\r\ntmp = REQ | MESSAGE(MSG_ENTER_RLC_SAFE_MODE);\r\nWREG32(RLC_GPR_REG2, tmp);\r\nmask = GFX_POWER_STATUS | GFX_CLOCK_STATUS;\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif ((RREG32(RLC_GPM_STAT) & mask) == mask)\r\nbreak;\r\nudelay(1);\r\n}\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif ((RREG32(RLC_GPR_REG2) & REQ) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nvoid cik_exit_rlc_safe_mode(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\ntmp = REQ | MESSAGE(MSG_EXIT_RLC_SAFE_MODE);\r\nWREG32(RLC_GPR_REG2, tmp);\r\n}\r\nstatic void cik_rlc_stop(struct radeon_device *rdev)\r\n{\r\nWREG32(RLC_CNTL, 0);\r\ncik_enable_gui_idle_interrupt(rdev, false);\r\ncik_wait_for_rlc_serdes(rdev);\r\n}\r\nstatic void cik_rlc_start(struct radeon_device *rdev)\r\n{\r\nWREG32(RLC_CNTL, RLC_ENABLE);\r\ncik_enable_gui_idle_interrupt(rdev, true);\r\nudelay(50);\r\n}\r\nstatic int cik_rlc_resume(struct radeon_device *rdev)\r\n{\r\nu32 i, size, tmp;\r\nif (!rdev->rlc_fw)\r\nreturn -EINVAL;\r\ncik_rlc_stop(rdev);\r\ntmp = RREG32(RLC_CGCG_CGLS_CTRL) & 0xfffffffc;\r\nWREG32(RLC_CGCG_CGLS_CTRL, tmp);\r\nsi_rlc_reset(rdev);\r\ncik_init_pg(rdev);\r\ncik_init_cg(rdev);\r\nWREG32(RLC_LB_CNTR_INIT, 0);\r\nWREG32(RLC_LB_CNTR_MAX, 0x00008000);\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nWREG32(RLC_LB_INIT_CU_MASK, 0xffffffff);\r\nWREG32(RLC_LB_PARAMS, 0x00600408);\r\nWREG32(RLC_LB_CNTL, 0x80000004);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\nWREG32(RLC_MC_CNTL, 0);\r\nWREG32(RLC_UCODE_CNTL, 0);\r\nif (rdev->new_fw) {\r\nconst struct rlc_firmware_header_v1_0 *hdr =\r\n(const struct rlc_firmware_header_v1_0 *)rdev->rlc_fw->data;\r\nconst __le32 *fw_data = (const __le32 *)\r\n(rdev->rlc_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\nradeon_ucode_print_rlc_hdr(&hdr->header);\r\nsize = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\r\nWREG32(RLC_GPM_UCODE_ADDR, 0);\r\nfor (i = 0; i < size; i++)\r\nWREG32(RLC_GPM_UCODE_DATA, le32_to_cpup(fw_data++));\r\nWREG32(RLC_GPM_UCODE_ADDR, le32_to_cpu(hdr->header.ucode_version));\r\n} else {\r\nconst __be32 *fw_data;\r\nswitch (rdev->family) {\r\ncase CHIP_BONAIRE:\r\ncase CHIP_HAWAII:\r\ndefault:\r\nsize = BONAIRE_RLC_UCODE_SIZE;\r\nbreak;\r\ncase CHIP_KAVERI:\r\nsize = KV_RLC_UCODE_SIZE;\r\nbreak;\r\ncase CHIP_KABINI:\r\nsize = KB_RLC_UCODE_SIZE;\r\nbreak;\r\ncase CHIP_MULLINS:\r\nsize = ML_RLC_UCODE_SIZE;\r\nbreak;\r\n}\r\nfw_data = (const __be32 *)rdev->rlc_fw->data;\r\nWREG32(RLC_GPM_UCODE_ADDR, 0);\r\nfor (i = 0; i < size; i++)\r\nWREG32(RLC_GPM_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(RLC_GPM_UCODE_ADDR, 0);\r\n}\r\ncik_enable_lbpw(rdev, false);\r\nif (rdev->family == CHIP_BONAIRE)\r\nWREG32(RLC_DRIVER_DMA_STATUS, 0);\r\ncik_rlc_start(rdev);\r\nreturn 0;\r\n}\r\nstatic void cik_enable_cgcg(struct radeon_device *rdev, bool enable)\r\n{\r\nu32 data, orig, tmp, tmp2;\r\norig = data = RREG32(RLC_CGCG_CGLS_CTRL);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_CGCG)) {\r\ncik_enable_gui_idle_interrupt(rdev, true);\r\ntmp = cik_halt_rlc(rdev);\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_CU_MASTER_MASK, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_NONCU_MASTER_MASK, 0xffffffff);\r\ntmp2 = BPM_ADDR_MASK | CGCG_OVERRIDE_0 | CGLS_ENABLE;\r\nWREG32(RLC_SERDES_WR_CTRL, tmp2);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\ncik_update_rlc(rdev, tmp);\r\ndata |= CGCG_EN | CGLS_EN;\r\n} else {\r\ncik_enable_gui_idle_interrupt(rdev, false);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\nRREG32(CB_CGTT_SCLK_CTRL);\r\ndata &= ~(CGCG_EN | CGLS_EN);\r\n}\r\nif (orig != data)\r\nWREG32(RLC_CGCG_CGLS_CTRL, data);\r\n}\r\nstatic void cik_enable_mgcg(struct radeon_device *rdev, bool enable)\r\n{\r\nu32 data, orig, tmp = 0;\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_MGCG)) {\r\nif (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_MGLS) {\r\nif (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_CP_LS) {\r\norig = data = RREG32(CP_MEM_SLP_CNTL);\r\ndata |= CP_MEM_LS_EN;\r\nif (orig != data)\r\nWREG32(CP_MEM_SLP_CNTL, data);\r\n}\r\n}\r\norig = data = RREG32(RLC_CGTT_MGCG_OVERRIDE);\r\ndata |= 0x00000001;\r\ndata &= 0xfffffffd;\r\nif (orig != data)\r\nWREG32(RLC_CGTT_MGCG_OVERRIDE, data);\r\ntmp = cik_halt_rlc(rdev);\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_CU_MASTER_MASK, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_NONCU_MASTER_MASK, 0xffffffff);\r\ndata = BPM_ADDR_MASK | MGCG_OVERRIDE_0;\r\nWREG32(RLC_SERDES_WR_CTRL, data);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\ncik_update_rlc(rdev, tmp);\r\nif (rdev->cg_flags & RADEON_CG_SUPPORT_GFX_CGTS) {\r\norig = data = RREG32(CGTS_SM_CTRL_REG);\r\ndata &= ~SM_MODE_MASK;\r\ndata |= SM_MODE(0x2);\r\ndata |= SM_MODE_ENABLE;\r\ndata &= ~CGTS_OVERRIDE;\r\nif ((rdev->cg_flags & RADEON_CG_SUPPORT_GFX_MGLS) &&\r\n(rdev->cg_flags & RADEON_CG_SUPPORT_GFX_CGTS_LS))\r\ndata &= ~CGTS_LS_OVERRIDE;\r\ndata &= ~ON_MONITOR_ADD_MASK;\r\ndata |= ON_MONITOR_ADD_EN;\r\ndata |= ON_MONITOR_ADD(0x96);\r\nif (orig != data)\r\nWREG32(CGTS_SM_CTRL_REG, data);\r\n}\r\n} else {\r\norig = data = RREG32(RLC_CGTT_MGCG_OVERRIDE);\r\ndata |= 0x00000003;\r\nif (orig != data)\r\nWREG32(RLC_CGTT_MGCG_OVERRIDE, data);\r\ndata = RREG32(RLC_MEM_SLP_CNTL);\r\nif (data & RLC_MEM_LS_EN) {\r\ndata &= ~RLC_MEM_LS_EN;\r\nWREG32(RLC_MEM_SLP_CNTL, data);\r\n}\r\ndata = RREG32(CP_MEM_SLP_CNTL);\r\nif (data & CP_MEM_LS_EN) {\r\ndata &= ~CP_MEM_LS_EN;\r\nWREG32(CP_MEM_SLP_CNTL, data);\r\n}\r\norig = data = RREG32(CGTS_SM_CTRL_REG);\r\ndata |= CGTS_OVERRIDE | CGTS_LS_OVERRIDE;\r\nif (orig != data)\r\nWREG32(CGTS_SM_CTRL_REG, data);\r\ntmp = cik_halt_rlc(rdev);\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_CU_MASTER_MASK, 0xffffffff);\r\nWREG32(RLC_SERDES_WR_NONCU_MASTER_MASK, 0xffffffff);\r\ndata = BPM_ADDR_MASK | MGCG_OVERRIDE_1;\r\nWREG32(RLC_SERDES_WR_CTRL, data);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\ncik_update_rlc(rdev, tmp);\r\n}\r\n}\r\nstatic void cik_enable_mc_ls(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nint i;\r\nu32 orig, data;\r\nfor (i = 0; i < ARRAY_SIZE(mc_cg_registers); i++) {\r\norig = data = RREG32(mc_cg_registers[i]);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_MC_LS))\r\ndata |= MC_LS_ENABLE;\r\nelse\r\ndata &= ~MC_LS_ENABLE;\r\nif (data != orig)\r\nWREG32(mc_cg_registers[i], data);\r\n}\r\n}\r\nstatic void cik_enable_mc_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nint i;\r\nu32 orig, data;\r\nfor (i = 0; i < ARRAY_SIZE(mc_cg_registers); i++) {\r\norig = data = RREG32(mc_cg_registers[i]);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_MC_MGCG))\r\ndata |= MC_CG_ENABLE;\r\nelse\r\ndata &= ~MC_CG_ENABLE;\r\nif (data != orig)\r\nWREG32(mc_cg_registers[i], data);\r\n}\r\n}\r\nstatic void cik_enable_sdma_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_SDMA_MGCG)) {\r\nWREG32(SDMA0_CLK_CTRL + SDMA0_REGISTER_OFFSET, 0x00000100);\r\nWREG32(SDMA0_CLK_CTRL + SDMA1_REGISTER_OFFSET, 0x00000100);\r\n} else {\r\norig = data = RREG32(SDMA0_CLK_CTRL + SDMA0_REGISTER_OFFSET);\r\ndata |= 0xff000000;\r\nif (data != orig)\r\nWREG32(SDMA0_CLK_CTRL + SDMA0_REGISTER_OFFSET, data);\r\norig = data = RREG32(SDMA0_CLK_CTRL + SDMA1_REGISTER_OFFSET);\r\ndata |= 0xff000000;\r\nif (data != orig)\r\nWREG32(SDMA0_CLK_CTRL + SDMA1_REGISTER_OFFSET, data);\r\n}\r\n}\r\nstatic void cik_enable_sdma_mgls(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_SDMA_LS)) {\r\norig = data = RREG32(SDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET);\r\ndata |= 0x100;\r\nif (orig != data)\r\nWREG32(SDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET, data);\r\norig = data = RREG32(SDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET);\r\ndata |= 0x100;\r\nif (orig != data)\r\nWREG32(SDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET, data);\r\n} else {\r\norig = data = RREG32(SDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET);\r\ndata &= ~0x100;\r\nif (orig != data)\r\nWREG32(SDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET, data);\r\norig = data = RREG32(SDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET);\r\ndata &= ~0x100;\r\nif (orig != data)\r\nWREG32(SDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET, data);\r\n}\r\n}\r\nstatic void cik_enable_uvd_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_UVD_MGCG)) {\r\ndata = RREG32_UVD_CTX(UVD_CGC_MEM_CTRL);\r\ndata = 0xfff;\r\nWREG32_UVD_CTX(UVD_CGC_MEM_CTRL, data);\r\norig = data = RREG32(UVD_CGC_CTRL);\r\ndata |= DCM;\r\nif (orig != data)\r\nWREG32(UVD_CGC_CTRL, data);\r\n} else {\r\ndata = RREG32_UVD_CTX(UVD_CGC_MEM_CTRL);\r\ndata &= ~0xfff;\r\nWREG32_UVD_CTX(UVD_CGC_MEM_CTRL, data);\r\norig = data = RREG32(UVD_CGC_CTRL);\r\ndata &= ~DCM;\r\nif (orig != data)\r\nWREG32(UVD_CGC_CTRL, data);\r\n}\r\n}\r\nstatic void cik_enable_bif_mgls(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\norig = data = RREG32_PCIE_PORT(PCIE_CNTL2);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_BIF_LS))\r\ndata |= SLV_MEM_LS_EN | MST_MEM_LS_EN |\r\nREPLAY_MEM_LS_EN | SLV_MEM_AGGRESSIVE_LS_EN;\r\nelse\r\ndata &= ~(SLV_MEM_LS_EN | MST_MEM_LS_EN |\r\nREPLAY_MEM_LS_EN | SLV_MEM_AGGRESSIVE_LS_EN);\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_CNTL2, data);\r\n}\r\nstatic void cik_enable_hdp_mgcg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\norig = data = RREG32(HDP_HOST_PATH_CNTL);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_HDP_MGCG))\r\ndata &= ~CLOCK_GATING_DIS;\r\nelse\r\ndata |= CLOCK_GATING_DIS;\r\nif (orig != data)\r\nWREG32(HDP_HOST_PATH_CNTL, data);\r\n}\r\nstatic void cik_enable_hdp_ls(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 orig, data;\r\norig = data = RREG32(HDP_MEM_POWER_LS);\r\nif (enable && (rdev->cg_flags & RADEON_CG_SUPPORT_HDP_LS))\r\ndata |= HDP_LS_ENABLE;\r\nelse\r\ndata &= ~HDP_LS_ENABLE;\r\nif (orig != data)\r\nWREG32(HDP_MEM_POWER_LS, data);\r\n}\r\nvoid cik_update_cg(struct radeon_device *rdev,\r\nu32 block, bool enable)\r\n{\r\nif (block & RADEON_CG_BLOCK_GFX) {\r\ncik_enable_gui_idle_interrupt(rdev, false);\r\nif (enable) {\r\ncik_enable_mgcg(rdev, true);\r\ncik_enable_cgcg(rdev, true);\r\n} else {\r\ncik_enable_cgcg(rdev, false);\r\ncik_enable_mgcg(rdev, false);\r\n}\r\ncik_enable_gui_idle_interrupt(rdev, true);\r\n}\r\nif (block & RADEON_CG_BLOCK_MC) {\r\nif (!(rdev->flags & RADEON_IS_IGP)) {\r\ncik_enable_mc_mgcg(rdev, enable);\r\ncik_enable_mc_ls(rdev, enable);\r\n}\r\n}\r\nif (block & RADEON_CG_BLOCK_SDMA) {\r\ncik_enable_sdma_mgcg(rdev, enable);\r\ncik_enable_sdma_mgls(rdev, enable);\r\n}\r\nif (block & RADEON_CG_BLOCK_BIF) {\r\ncik_enable_bif_mgls(rdev, enable);\r\n}\r\nif (block & RADEON_CG_BLOCK_UVD) {\r\nif (rdev->has_uvd)\r\ncik_enable_uvd_mgcg(rdev, enable);\r\n}\r\nif (block & RADEON_CG_BLOCK_HDP) {\r\ncik_enable_hdp_mgcg(rdev, enable);\r\ncik_enable_hdp_ls(rdev, enable);\r\n}\r\nif (block & RADEON_CG_BLOCK_VCE) {\r\nvce_v2_0_enable_mgcg(rdev, enable);\r\n}\r\n}\r\nstatic void cik_init_cg(struct radeon_device *rdev)\r\n{\r\ncik_update_cg(rdev, RADEON_CG_BLOCK_GFX, true);\r\nif (rdev->has_uvd)\r\nsi_init_uvd_internal_cg(rdev);\r\ncik_update_cg(rdev, (RADEON_CG_BLOCK_MC |\r\nRADEON_CG_BLOCK_SDMA |\r\nRADEON_CG_BLOCK_BIF |\r\nRADEON_CG_BLOCK_UVD |\r\nRADEON_CG_BLOCK_HDP), true);\r\n}\r\nstatic void cik_fini_cg(struct radeon_device *rdev)\r\n{\r\ncik_update_cg(rdev, (RADEON_CG_BLOCK_MC |\r\nRADEON_CG_BLOCK_SDMA |\r\nRADEON_CG_BLOCK_BIF |\r\nRADEON_CG_BLOCK_UVD |\r\nRADEON_CG_BLOCK_HDP), false);\r\ncik_update_cg(rdev, RADEON_CG_BLOCK_GFX, false);\r\n}\r\nstatic void cik_enable_sck_slowdown_on_pu(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_PG_CNTL);\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_RLC_SMU_HS))\r\ndata |= SMU_CLK_SLOWDOWN_ON_PU_ENABLE;\r\nelse\r\ndata &= ~SMU_CLK_SLOWDOWN_ON_PU_ENABLE;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\n}\r\nstatic void cik_enable_sck_slowdown_on_pd(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_PG_CNTL);\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_RLC_SMU_HS))\r\ndata |= SMU_CLK_SLOWDOWN_ON_PD_ENABLE;\r\nelse\r\ndata &= ~SMU_CLK_SLOWDOWN_ON_PD_ENABLE;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\n}\r\nstatic void cik_enable_cp_pg(struct radeon_device *rdev, bool enable)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_PG_CNTL);\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_CP))\r\ndata &= ~DISABLE_CP_PG;\r\nelse\r\ndata |= DISABLE_CP_PG;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\n}\r\nstatic void cik_enable_gds_pg(struct radeon_device *rdev, bool enable)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_PG_CNTL);\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_GDS))\r\ndata &= ~DISABLE_GDS_PG;\r\nelse\r\ndata |= DISABLE_GDS_PG;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\n}\r\nvoid cik_init_cp_pg_table(struct radeon_device *rdev)\r\n{\r\nvolatile u32 *dst_ptr;\r\nint me, i, max_me = 4;\r\nu32 bo_offset = 0;\r\nu32 table_offset, table_size;\r\nif (rdev->family == CHIP_KAVERI)\r\nmax_me = 5;\r\nif (rdev->rlc.cp_table_ptr == NULL)\r\nreturn;\r\ndst_ptr = rdev->rlc.cp_table_ptr;\r\nfor (me = 0; me < max_me; me++) {\r\nif (rdev->new_fw) {\r\nconst __le32 *fw_data;\r\nconst struct gfx_firmware_header_v1_0 *hdr;\r\nif (me == 0) {\r\nhdr = (const struct gfx_firmware_header_v1_0 *)rdev->ce_fw->data;\r\nfw_data = (const __le32 *)\r\n(rdev->ce_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\ntable_offset = le32_to_cpu(hdr->jt_offset);\r\ntable_size = le32_to_cpu(hdr->jt_size);\r\n} else if (me == 1) {\r\nhdr = (const struct gfx_firmware_header_v1_0 *)rdev->pfp_fw->data;\r\nfw_data = (const __le32 *)\r\n(rdev->pfp_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\ntable_offset = le32_to_cpu(hdr->jt_offset);\r\ntable_size = le32_to_cpu(hdr->jt_size);\r\n} else if (me == 2) {\r\nhdr = (const struct gfx_firmware_header_v1_0 *)rdev->me_fw->data;\r\nfw_data = (const __le32 *)\r\n(rdev->me_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\ntable_offset = le32_to_cpu(hdr->jt_offset);\r\ntable_size = le32_to_cpu(hdr->jt_size);\r\n} else if (me == 3) {\r\nhdr = (const struct gfx_firmware_header_v1_0 *)rdev->mec_fw->data;\r\nfw_data = (const __le32 *)\r\n(rdev->mec_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\ntable_offset = le32_to_cpu(hdr->jt_offset);\r\ntable_size = le32_to_cpu(hdr->jt_size);\r\n} else {\r\nhdr = (const struct gfx_firmware_header_v1_0 *)rdev->mec2_fw->data;\r\nfw_data = (const __le32 *)\r\n(rdev->mec2_fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\ntable_offset = le32_to_cpu(hdr->jt_offset);\r\ntable_size = le32_to_cpu(hdr->jt_size);\r\n}\r\nfor (i = 0; i < table_size; i ++) {\r\ndst_ptr[bo_offset + i] =\r\ncpu_to_le32(le32_to_cpu(fw_data[table_offset + i]));\r\n}\r\nbo_offset += table_size;\r\n} else {\r\nconst __be32 *fw_data;\r\ntable_size = CP_ME_TABLE_SIZE;\r\nif (me == 0) {\r\nfw_data = (const __be32 *)rdev->ce_fw->data;\r\ntable_offset = CP_ME_TABLE_OFFSET;\r\n} else if (me == 1) {\r\nfw_data = (const __be32 *)rdev->pfp_fw->data;\r\ntable_offset = CP_ME_TABLE_OFFSET;\r\n} else if (me == 2) {\r\nfw_data = (const __be32 *)rdev->me_fw->data;\r\ntable_offset = CP_ME_TABLE_OFFSET;\r\n} else {\r\nfw_data = (const __be32 *)rdev->mec_fw->data;\r\ntable_offset = CP_MEC_TABLE_OFFSET;\r\n}\r\nfor (i = 0; i < table_size; i ++) {\r\ndst_ptr[bo_offset + i] =\r\ncpu_to_le32(be32_to_cpu(fw_data[table_offset + i]));\r\n}\r\nbo_offset += table_size;\r\n}\r\n}\r\n}\r\nstatic void cik_enable_gfx_cgpg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 data, orig;\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_GFX_PG)) {\r\norig = data = RREG32(RLC_PG_CNTL);\r\ndata |= GFX_PG_ENABLE;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\norig = data = RREG32(RLC_AUTO_PG_CTRL);\r\ndata |= AUTO_PG_EN;\r\nif (orig != data)\r\nWREG32(RLC_AUTO_PG_CTRL, data);\r\n} else {\r\norig = data = RREG32(RLC_PG_CNTL);\r\ndata &= ~GFX_PG_ENABLE;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\norig = data = RREG32(RLC_AUTO_PG_CTRL);\r\ndata &= ~AUTO_PG_EN;\r\nif (orig != data)\r\nWREG32(RLC_AUTO_PG_CTRL, data);\r\ndata = RREG32(DB_RENDER_CONTROL);\r\n}\r\n}\r\nstatic u32 cik_get_cu_active_bitmap(struct radeon_device *rdev, u32 se, u32 sh)\r\n{\r\nu32 mask = 0, tmp, tmp1;\r\nint i;\r\nmutex_lock(&rdev->grbm_idx_mutex);\r\ncik_select_se_sh(rdev, se, sh);\r\ntmp = RREG32(CC_GC_SHADER_ARRAY_CONFIG);\r\ntmp1 = RREG32(GC_USER_SHADER_ARRAY_CONFIG);\r\ncik_select_se_sh(rdev, 0xffffffff, 0xffffffff);\r\nmutex_unlock(&rdev->grbm_idx_mutex);\r\ntmp &= 0xffff0000;\r\ntmp |= tmp1;\r\ntmp >>= 16;\r\nfor (i = 0; i < rdev->config.cik.max_cu_per_sh; i ++) {\r\nmask <<= 1;\r\nmask |= 1;\r\n}\r\nreturn (~tmp) & mask;\r\n}\r\nstatic void cik_init_ao_cu_mask(struct radeon_device *rdev)\r\n{\r\nu32 i, j, k, active_cu_number = 0;\r\nu32 mask, counter, cu_bitmap;\r\nu32 tmp = 0;\r\nfor (i = 0; i < rdev->config.cik.max_shader_engines; i++) {\r\nfor (j = 0; j < rdev->config.cik.max_sh_per_se; j++) {\r\nmask = 1;\r\ncu_bitmap = 0;\r\ncounter = 0;\r\nfor (k = 0; k < rdev->config.cik.max_cu_per_sh; k ++) {\r\nif (cik_get_cu_active_bitmap(rdev, i, j) & mask) {\r\nif (counter < 2)\r\ncu_bitmap |= mask;\r\ncounter ++;\r\n}\r\nmask <<= 1;\r\n}\r\nactive_cu_number += counter;\r\ntmp |= (cu_bitmap << (i * 16 + j * 8));\r\n}\r\n}\r\nWREG32(RLC_PG_AO_CU_MASK, tmp);\r\ntmp = RREG32(RLC_MAX_PG_CU);\r\ntmp &= ~MAX_PU_CU_MASK;\r\ntmp |= MAX_PU_CU(active_cu_number);\r\nWREG32(RLC_MAX_PG_CU, tmp);\r\n}\r\nstatic void cik_enable_gfx_static_mgpg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_PG_CNTL);\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_GFX_SMG))\r\ndata |= STATIC_PER_CU_PG_ENABLE;\r\nelse\r\ndata &= ~STATIC_PER_CU_PG_ENABLE;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\n}\r\nstatic void cik_enable_gfx_dynamic_mgpg(struct radeon_device *rdev,\r\nbool enable)\r\n{\r\nu32 data, orig;\r\norig = data = RREG32(RLC_PG_CNTL);\r\nif (enable && (rdev->pg_flags & RADEON_PG_SUPPORT_GFX_DMG))\r\ndata |= DYN_PER_CU_PG_ENABLE;\r\nelse\r\ndata &= ~DYN_PER_CU_PG_ENABLE;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\n}\r\nstatic void cik_init_gfx_cgpg(struct radeon_device *rdev)\r\n{\r\nu32 data, orig;\r\nu32 i;\r\nif (rdev->rlc.cs_data) {\r\nWREG32(RLC_GPM_SCRATCH_ADDR, RLC_CLEAR_STATE_DESCRIPTOR_OFFSET);\r\nWREG32(RLC_GPM_SCRATCH_DATA, upper_32_bits(rdev->rlc.clear_state_gpu_addr));\r\nWREG32(RLC_GPM_SCRATCH_DATA, lower_32_bits(rdev->rlc.clear_state_gpu_addr));\r\nWREG32(RLC_GPM_SCRATCH_DATA, rdev->rlc.clear_state_size);\r\n} else {\r\nWREG32(RLC_GPM_SCRATCH_ADDR, RLC_CLEAR_STATE_DESCRIPTOR_OFFSET);\r\nfor (i = 0; i < 3; i++)\r\nWREG32(RLC_GPM_SCRATCH_DATA, 0);\r\n}\r\nif (rdev->rlc.reg_list) {\r\nWREG32(RLC_GPM_SCRATCH_ADDR, RLC_SAVE_AND_RESTORE_STARTING_OFFSET);\r\nfor (i = 0; i < rdev->rlc.reg_list_size; i++)\r\nWREG32(RLC_GPM_SCRATCH_DATA, rdev->rlc.reg_list[i]);\r\n}\r\norig = data = RREG32(RLC_PG_CNTL);\r\ndata |= GFX_PG_SRC;\r\nif (orig != data)\r\nWREG32(RLC_PG_CNTL, data);\r\nWREG32(RLC_SAVE_AND_RESTORE_BASE, rdev->rlc.save_restore_gpu_addr >> 8);\r\nWREG32(RLC_CP_TABLE_RESTORE, rdev->rlc.cp_table_gpu_addr >> 8);\r\ndata = RREG32(CP_RB_WPTR_POLL_CNTL);\r\ndata &= ~IDLE_POLL_COUNT_MASK;\r\ndata |= IDLE_POLL_COUNT(0x60);\r\nWREG32(CP_RB_WPTR_POLL_CNTL, data);\r\ndata = 0x10101010;\r\nWREG32(RLC_PG_DELAY, data);\r\ndata = RREG32(RLC_PG_DELAY_2);\r\ndata &= ~0xff;\r\ndata |= 0x3;\r\nWREG32(RLC_PG_DELAY_2, data);\r\ndata = RREG32(RLC_AUTO_PG_CTRL);\r\ndata &= ~GRBM_REG_SGIT_MASK;\r\ndata |= GRBM_REG_SGIT(0x700);\r\nWREG32(RLC_AUTO_PG_CTRL, data);\r\n}\r\nstatic void cik_update_gfx_pg(struct radeon_device *rdev, bool enable)\r\n{\r\ncik_enable_gfx_cgpg(rdev, enable);\r\ncik_enable_gfx_static_mgpg(rdev, enable);\r\ncik_enable_gfx_dynamic_mgpg(rdev, enable);\r\n}\r\nu32 cik_get_csb_size(struct radeon_device *rdev)\r\n{\r\nu32 count = 0;\r\nconst struct cs_section_def *sect = NULL;\r\nconst struct cs_extent_def *ext = NULL;\r\nif (rdev->rlc.cs_data == NULL)\r\nreturn 0;\r\ncount += 2;\r\ncount += 3;\r\nfor (sect = rdev->rlc.cs_data; sect->section != NULL; ++sect) {\r\nfor (ext = sect->section; ext->extent != NULL; ++ext) {\r\nif (sect->id == SECT_CONTEXT)\r\ncount += 2 + ext->reg_count;\r\nelse\r\nreturn 0;\r\n}\r\n}\r\ncount += 4;\r\ncount += 2;\r\ncount += 2;\r\nreturn count;\r\n}\r\nvoid cik_get_csb_buffer(struct radeon_device *rdev, volatile u32 *buffer)\r\n{\r\nu32 count = 0, i;\r\nconst struct cs_section_def *sect = NULL;\r\nconst struct cs_extent_def *ext = NULL;\r\nif (rdev->rlc.cs_data == NULL)\r\nreturn;\r\nif (buffer == NULL)\r\nreturn;\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nbuffer[count++] = cpu_to_le32(PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_CONTEXT_CONTROL, 1));\r\nbuffer[count++] = cpu_to_le32(0x80000000);\r\nbuffer[count++] = cpu_to_le32(0x80000000);\r\nfor (sect = rdev->rlc.cs_data; sect->section != NULL; ++sect) {\r\nfor (ext = sect->section; ext->extent != NULL; ++ext) {\r\nif (sect->id == SECT_CONTEXT) {\r\nbuffer[count++] =\r\ncpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, ext->reg_count));\r\nbuffer[count++] = cpu_to_le32(ext->reg_index - 0xa000);\r\nfor (i = 0; i < ext->reg_count; i++)\r\nbuffer[count++] = cpu_to_le32(ext->extent[i]);\r\n} else {\r\nreturn;\r\n}\r\n}\r\n}\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_SET_CONTEXT_REG, 2));\r\nbuffer[count++] = cpu_to_le32(PA_SC_RASTER_CONFIG - PACKET3_SET_CONTEXT_REG_START);\r\nswitch (rdev->family) {\r\ncase CHIP_BONAIRE:\r\nbuffer[count++] = cpu_to_le32(0x16000012);\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbreak;\r\ncase CHIP_KAVERI:\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbreak;\r\ncase CHIP_KABINI:\r\ncase CHIP_MULLINS:\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbreak;\r\ncase CHIP_HAWAII:\r\nbuffer[count++] = cpu_to_le32(0x3a00161a);\r\nbuffer[count++] = cpu_to_le32(0x0000002e);\r\nbreak;\r\ndefault:\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbuffer[count++] = cpu_to_le32(0x00000000);\r\nbreak;\r\n}\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nbuffer[count++] = cpu_to_le32(PACKET3_PREAMBLE_END_CLEAR_STATE);\r\nbuffer[count++] = cpu_to_le32(PACKET3(PACKET3_CLEAR_STATE, 0));\r\nbuffer[count++] = cpu_to_le32(0);\r\n}\r\nstatic void cik_init_pg(struct radeon_device *rdev)\r\n{\r\nif (rdev->pg_flags) {\r\ncik_enable_sck_slowdown_on_pu(rdev, true);\r\ncik_enable_sck_slowdown_on_pd(rdev, true);\r\nif (rdev->pg_flags & RADEON_PG_SUPPORT_GFX_PG) {\r\ncik_init_gfx_cgpg(rdev);\r\ncik_enable_cp_pg(rdev, true);\r\ncik_enable_gds_pg(rdev, true);\r\n}\r\ncik_init_ao_cu_mask(rdev);\r\ncik_update_gfx_pg(rdev, true);\r\n}\r\n}\r\nstatic void cik_fini_pg(struct radeon_device *rdev)\r\n{\r\nif (rdev->pg_flags) {\r\ncik_update_gfx_pg(rdev, false);\r\nif (rdev->pg_flags & RADEON_PG_SUPPORT_GFX_PG) {\r\ncik_enable_cp_pg(rdev, false);\r\ncik_enable_gds_pg(rdev, false);\r\n}\r\n}\r\n}\r\nstatic void cik_enable_interrupts(struct radeon_device *rdev)\r\n{\r\nu32 ih_cntl = RREG32(IH_CNTL);\r\nu32 ih_rb_cntl = RREG32(IH_RB_CNTL);\r\nih_cntl |= ENABLE_INTR;\r\nih_rb_cntl |= IH_RB_ENABLE;\r\nWREG32(IH_CNTL, ih_cntl);\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nrdev->ih.enabled = true;\r\n}\r\nstatic void cik_disable_interrupts(struct radeon_device *rdev)\r\n{\r\nu32 ih_rb_cntl = RREG32(IH_RB_CNTL);\r\nu32 ih_cntl = RREG32(IH_CNTL);\r\nih_rb_cntl &= ~IH_RB_ENABLE;\r\nih_cntl &= ~ENABLE_INTR;\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nWREG32(IH_CNTL, ih_cntl);\r\nWREG32(IH_RB_RPTR, 0);\r\nWREG32(IH_RB_WPTR, 0);\r\nrdev->ih.enabled = false;\r\nrdev->ih.rptr = 0;\r\n}\r\nstatic void cik_disable_interrupt_state(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\ntmp = RREG32(CP_INT_CNTL_RING0) &\r\n(CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\nWREG32(CP_INT_CNTL_RING0, tmp);\r\ntmp = RREG32(SDMA0_CNTL + SDMA0_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\nWREG32(SDMA0_CNTL + SDMA0_REGISTER_OFFSET, tmp);\r\ntmp = RREG32(SDMA0_CNTL + SDMA1_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\nWREG32(SDMA0_CNTL + SDMA1_REGISTER_OFFSET, tmp);\r\nWREG32(CP_ME1_PIPE0_INT_CNTL, 0);\r\nWREG32(CP_ME1_PIPE1_INT_CNTL, 0);\r\nWREG32(CP_ME1_PIPE2_INT_CNTL, 0);\r\nWREG32(CP_ME1_PIPE3_INT_CNTL, 0);\r\nWREG32(CP_ME2_PIPE0_INT_CNTL, 0);\r\nWREG32(CP_ME2_PIPE1_INT_CNTL, 0);\r\nWREG32(CP_ME2_PIPE2_INT_CNTL, 0);\r\nWREG32(CP_ME2_PIPE3_INT_CNTL, 0);\r\nWREG32(GRBM_INT_CNTL, 0);\r\nWREG32(SRBM_INT_CNTL, 0);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC0_REGISTER_OFFSET, 0);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC1_REGISTER_OFFSET, 0);\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC2_REGISTER_OFFSET, 0);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC3_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC4_REGISTER_OFFSET, 0);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC5_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 2) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC0_REGISTER_OFFSET, 0);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC1_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC2_REGISTER_OFFSET, 0);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC3_REGISTER_OFFSET, 0);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC4_REGISTER_OFFSET, 0);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC5_REGISTER_OFFSET, 0);\r\n}\r\nWREG32(DAC_AUTODETECT_INT_CONTROL, 0);\r\ntmp = RREG32(DC_HPD1_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD1_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD2_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD2_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD3_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD3_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD4_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD4_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD5_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD5_INT_CONTROL, tmp);\r\ntmp = RREG32(DC_HPD6_INT_CONTROL) & DC_HPDx_INT_POLARITY;\r\nWREG32(DC_HPD6_INT_CONTROL, tmp);\r\n}\r\nstatic int cik_irq_init(struct radeon_device *rdev)\r\n{\r\nint ret = 0;\r\nint rb_bufsz;\r\nu32 interrupt_cntl, ih_cntl, ih_rb_cntl;\r\nret = r600_ih_ring_alloc(rdev);\r\nif (ret)\r\nreturn ret;\r\ncik_disable_interrupts(rdev);\r\nret = cik_rlc_resume(rdev);\r\nif (ret) {\r\nr600_ih_ring_fini(rdev);\r\nreturn ret;\r\n}\r\nWREG32(INTERRUPT_CNTL2, rdev->ih.gpu_addr >> 8);\r\ninterrupt_cntl = RREG32(INTERRUPT_CNTL);\r\ninterrupt_cntl &= ~IH_DUMMY_RD_OVERRIDE;\r\ninterrupt_cntl &= ~IH_REQ_NONSNOOP_EN;\r\nWREG32(INTERRUPT_CNTL, interrupt_cntl);\r\nWREG32(IH_RB_BASE, rdev->ih.gpu_addr >> 8);\r\nrb_bufsz = order_base_2(rdev->ih.ring_size / 4);\r\nih_rb_cntl = (IH_WPTR_OVERFLOW_ENABLE |\r\nIH_WPTR_OVERFLOW_CLEAR |\r\n(rb_bufsz << 1));\r\nif (rdev->wb.enabled)\r\nih_rb_cntl |= IH_WPTR_WRITEBACK_ENABLE;\r\nWREG32(IH_RB_WPTR_ADDR_LO, (rdev->wb.gpu_addr + R600_WB_IH_WPTR_OFFSET) & 0xFFFFFFFC);\r\nWREG32(IH_RB_WPTR_ADDR_HI, upper_32_bits(rdev->wb.gpu_addr + R600_WB_IH_WPTR_OFFSET) & 0xFF);\r\nWREG32(IH_RB_CNTL, ih_rb_cntl);\r\nWREG32(IH_RB_RPTR, 0);\r\nWREG32(IH_RB_WPTR, 0);\r\nih_cntl = MC_WRREQ_CREDIT(0x10) | MC_WR_CLEAN_CNT(0x10) | MC_VMID(0);\r\nif (rdev->msi_enabled)\r\nih_cntl |= RPTR_REARM;\r\nWREG32(IH_CNTL, ih_cntl);\r\ncik_disable_interrupt_state(rdev);\r\npci_set_master(rdev->pdev);\r\ncik_enable_interrupts(rdev);\r\nreturn ret;\r\n}\r\nint cik_irq_set(struct radeon_device *rdev)\r\n{\r\nu32 cp_int_cntl;\r\nu32 cp_m1p0;\r\nu32 crtc1 = 0, crtc2 = 0, crtc3 = 0, crtc4 = 0, crtc5 = 0, crtc6 = 0;\r\nu32 hpd1, hpd2, hpd3, hpd4, hpd5, hpd6;\r\nu32 grbm_int_cntl = 0;\r\nu32 dma_cntl, dma_cntl1;\r\nif (!rdev->irq.installed) {\r\nWARN(1, "Can't enable IRQ/MSI because no handler is installed\n");\r\nreturn -EINVAL;\r\n}\r\nif (!rdev->ih.enabled) {\r\ncik_disable_interrupts(rdev);\r\ncik_disable_interrupt_state(rdev);\r\nreturn 0;\r\n}\r\ncp_int_cntl = RREG32(CP_INT_CNTL_RING0) &\r\n(CNTX_BUSY_INT_ENABLE | CNTX_EMPTY_INT_ENABLE);\r\ncp_int_cntl |= PRIV_INSTR_INT_ENABLE | PRIV_REG_INT_ENABLE;\r\nhpd1 = RREG32(DC_HPD1_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd2 = RREG32(DC_HPD2_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd3 = RREG32(DC_HPD3_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd4 = RREG32(DC_HPD4_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd5 = RREG32(DC_HPD5_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\nhpd6 = RREG32(DC_HPD6_INT_CONTROL) & ~(DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN);\r\ndma_cntl = RREG32(SDMA0_CNTL + SDMA0_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\ndma_cntl1 = RREG32(SDMA0_CNTL + SDMA1_REGISTER_OFFSET) & ~TRAP_ENABLE;\r\ncp_m1p0 = RREG32(CP_ME1_PIPE0_INT_CNTL) & ~TIME_STAMP_INT_ENABLE;\r\nif (atomic_read(&rdev->irq.ring_int[RADEON_RING_TYPE_GFX_INDEX])) {\r\nDRM_DEBUG("cik_irq_set: sw int gfx\n");\r\ncp_int_cntl |= TIME_STAMP_INT_ENABLE;\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[CAYMAN_RING_TYPE_CP1_INDEX])) {\r\nstruct radeon_ring *ring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nDRM_DEBUG("si_irq_set: sw int cp1\n");\r\nif (ring->me == 1) {\r\nswitch (ring->pipe) {\r\ncase 0:\r\ncp_m1p0 |= TIME_STAMP_INT_ENABLE;\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("si_irq_set: sw int cp1 invalid pipe %d\n", ring->pipe);\r\nbreak;\r\n}\r\n} else {\r\nDRM_DEBUG("si_irq_set: sw int cp1 invalid me %d\n", ring->me);\r\n}\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[CAYMAN_RING_TYPE_CP2_INDEX])) {\r\nstruct radeon_ring *ring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nDRM_DEBUG("si_irq_set: sw int cp2\n");\r\nif (ring->me == 1) {\r\nswitch (ring->pipe) {\r\ncase 0:\r\ncp_m1p0 |= TIME_STAMP_INT_ENABLE;\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("si_irq_set: sw int cp2 invalid pipe %d\n", ring->pipe);\r\nbreak;\r\n}\r\n} else {\r\nDRM_DEBUG("si_irq_set: sw int cp2 invalid me %d\n", ring->me);\r\n}\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[R600_RING_TYPE_DMA_INDEX])) {\r\nDRM_DEBUG("cik_irq_set: sw int dma\n");\r\ndma_cntl |= TRAP_ENABLE;\r\n}\r\nif (atomic_read(&rdev->irq.ring_int[CAYMAN_RING_TYPE_DMA1_INDEX])) {\r\nDRM_DEBUG("cik_irq_set: sw int dma1\n");\r\ndma_cntl1 |= TRAP_ENABLE;\r\n}\r\nif (rdev->irq.crtc_vblank_int[0] ||\r\natomic_read(&rdev->irq.pflip[0])) {\r\nDRM_DEBUG("cik_irq_set: vblank 0\n");\r\ncrtc1 |= VBLANK_INTERRUPT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[1] ||\r\natomic_read(&rdev->irq.pflip[1])) {\r\nDRM_DEBUG("cik_irq_set: vblank 1\n");\r\ncrtc2 |= VBLANK_INTERRUPT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[2] ||\r\natomic_read(&rdev->irq.pflip[2])) {\r\nDRM_DEBUG("cik_irq_set: vblank 2\n");\r\ncrtc3 |= VBLANK_INTERRUPT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[3] ||\r\natomic_read(&rdev->irq.pflip[3])) {\r\nDRM_DEBUG("cik_irq_set: vblank 3\n");\r\ncrtc4 |= VBLANK_INTERRUPT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[4] ||\r\natomic_read(&rdev->irq.pflip[4])) {\r\nDRM_DEBUG("cik_irq_set: vblank 4\n");\r\ncrtc5 |= VBLANK_INTERRUPT_MASK;\r\n}\r\nif (rdev->irq.crtc_vblank_int[5] ||\r\natomic_read(&rdev->irq.pflip[5])) {\r\nDRM_DEBUG("cik_irq_set: vblank 5\n");\r\ncrtc6 |= VBLANK_INTERRUPT_MASK;\r\n}\r\nif (rdev->irq.hpd[0]) {\r\nDRM_DEBUG("cik_irq_set: hpd 1\n");\r\nhpd1 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[1]) {\r\nDRM_DEBUG("cik_irq_set: hpd 2\n");\r\nhpd2 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[2]) {\r\nDRM_DEBUG("cik_irq_set: hpd 3\n");\r\nhpd3 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[3]) {\r\nDRM_DEBUG("cik_irq_set: hpd 4\n");\r\nhpd4 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[4]) {\r\nDRM_DEBUG("cik_irq_set: hpd 5\n");\r\nhpd5 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nif (rdev->irq.hpd[5]) {\r\nDRM_DEBUG("cik_irq_set: hpd 6\n");\r\nhpd6 |= DC_HPDx_INT_EN | DC_HPDx_RX_INT_EN;\r\n}\r\nWREG32(CP_INT_CNTL_RING0, cp_int_cntl);\r\nWREG32(SDMA0_CNTL + SDMA0_REGISTER_OFFSET, dma_cntl);\r\nWREG32(SDMA0_CNTL + SDMA1_REGISTER_OFFSET, dma_cntl1);\r\nWREG32(CP_ME1_PIPE0_INT_CNTL, cp_m1p0);\r\nWREG32(GRBM_INT_CNTL, grbm_int_cntl);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC0_REGISTER_OFFSET, crtc1);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC1_REGISTER_OFFSET, crtc2);\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC2_REGISTER_OFFSET, crtc3);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC3_REGISTER_OFFSET, crtc4);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC4_REGISTER_OFFSET, crtc5);\r\nWREG32(LB_INTERRUPT_MASK + EVERGREEN_CRTC5_REGISTER_OFFSET, crtc6);\r\n}\r\nif (rdev->num_crtc >= 2) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC0_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC1_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\n}\r\nif (rdev->num_crtc >= 4) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC2_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC3_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC4_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\nWREG32(GRPH_INT_CONTROL + EVERGREEN_CRTC5_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_MASK);\r\n}\r\nWREG32(DC_HPD1_INT_CONTROL, hpd1);\r\nWREG32(DC_HPD2_INT_CONTROL, hpd2);\r\nWREG32(DC_HPD3_INT_CONTROL, hpd3);\r\nWREG32(DC_HPD4_INT_CONTROL, hpd4);\r\nWREG32(DC_HPD5_INT_CONTROL, hpd5);\r\nWREG32(DC_HPD6_INT_CONTROL, hpd6);\r\nRREG32(SRBM_STATUS);\r\nreturn 0;\r\n}\r\nstatic inline void cik_irq_ack(struct radeon_device *rdev)\r\n{\r\nu32 tmp;\r\nrdev->irq.stat_regs.cik.disp_int = RREG32(DISP_INTERRUPT_STATUS);\r\nrdev->irq.stat_regs.cik.disp_int_cont = RREG32(DISP_INTERRUPT_STATUS_CONTINUE);\r\nrdev->irq.stat_regs.cik.disp_int_cont2 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE2);\r\nrdev->irq.stat_regs.cik.disp_int_cont3 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE3);\r\nrdev->irq.stat_regs.cik.disp_int_cont4 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE4);\r\nrdev->irq.stat_regs.cik.disp_int_cont5 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE5);\r\nrdev->irq.stat_regs.cik.disp_int_cont6 = RREG32(DISP_INTERRUPT_STATUS_CONTINUE6);\r\nrdev->irq.stat_regs.cik.d1grph_int = RREG32(GRPH_INT_STATUS +\r\nEVERGREEN_CRTC0_REGISTER_OFFSET);\r\nrdev->irq.stat_regs.cik.d2grph_int = RREG32(GRPH_INT_STATUS +\r\nEVERGREEN_CRTC1_REGISTER_OFFSET);\r\nif (rdev->num_crtc >= 4) {\r\nrdev->irq.stat_regs.cik.d3grph_int = RREG32(GRPH_INT_STATUS +\r\nEVERGREEN_CRTC2_REGISTER_OFFSET);\r\nrdev->irq.stat_regs.cik.d4grph_int = RREG32(GRPH_INT_STATUS +\r\nEVERGREEN_CRTC3_REGISTER_OFFSET);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nrdev->irq.stat_regs.cik.d5grph_int = RREG32(GRPH_INT_STATUS +\r\nEVERGREEN_CRTC4_REGISTER_OFFSET);\r\nrdev->irq.stat_regs.cik.d6grph_int = RREG32(GRPH_INT_STATUS +\r\nEVERGREEN_CRTC5_REGISTER_OFFSET);\r\n}\r\nif (rdev->irq.stat_regs.cik.d1grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC0_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.cik.d2grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC1_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.cik.disp_int & LB_D1_VBLANK_INTERRUPT)\r\nWREG32(LB_VBLANK_STATUS + EVERGREEN_CRTC0_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int & LB_D1_VLINE_INTERRUPT)\r\nWREG32(LB_VLINE_STATUS + EVERGREEN_CRTC0_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont & LB_D2_VBLANK_INTERRUPT)\r\nWREG32(LB_VBLANK_STATUS + EVERGREEN_CRTC1_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont & LB_D2_VLINE_INTERRUPT)\r\nWREG32(LB_VLINE_STATUS + EVERGREEN_CRTC1_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->num_crtc >= 4) {\r\nif (rdev->irq.stat_regs.cik.d3grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC2_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.cik.d4grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC3_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont2 & LB_D3_VBLANK_INTERRUPT)\r\nWREG32(LB_VBLANK_STATUS + EVERGREEN_CRTC2_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont2 & LB_D3_VLINE_INTERRUPT)\r\nWREG32(LB_VLINE_STATUS + EVERGREEN_CRTC2_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont3 & LB_D4_VBLANK_INTERRUPT)\r\nWREG32(LB_VBLANK_STATUS + EVERGREEN_CRTC3_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont3 & LB_D4_VLINE_INTERRUPT)\r\nWREG32(LB_VLINE_STATUS + EVERGREEN_CRTC3_REGISTER_OFFSET, VLINE_ACK);\r\n}\r\nif (rdev->num_crtc >= 6) {\r\nif (rdev->irq.stat_regs.cik.d5grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC4_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.cik.d6grph_int & GRPH_PFLIP_INT_OCCURRED)\r\nWREG32(GRPH_INT_STATUS + EVERGREEN_CRTC5_REGISTER_OFFSET,\r\nGRPH_PFLIP_INT_CLEAR);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont4 & LB_D5_VBLANK_INTERRUPT)\r\nWREG32(LB_VBLANK_STATUS + EVERGREEN_CRTC4_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont4 & LB_D5_VLINE_INTERRUPT)\r\nWREG32(LB_VLINE_STATUS + EVERGREEN_CRTC4_REGISTER_OFFSET, VLINE_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont5 & LB_D6_VBLANK_INTERRUPT)\r\nWREG32(LB_VBLANK_STATUS + EVERGREEN_CRTC5_REGISTER_OFFSET, VBLANK_ACK);\r\nif (rdev->irq.stat_regs.cik.disp_int_cont5 & LB_D6_VLINE_INTERRUPT)\r\nWREG32(LB_VLINE_STATUS + EVERGREEN_CRTC5_REGISTER_OFFSET, VLINE_ACK);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int & DC_HPD1_INTERRUPT) {\r\ntmp = RREG32(DC_HPD1_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD1_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont & DC_HPD2_INTERRUPT) {\r\ntmp = RREG32(DC_HPD2_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD2_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont2 & DC_HPD3_INTERRUPT) {\r\ntmp = RREG32(DC_HPD3_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD3_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont3 & DC_HPD4_INTERRUPT) {\r\ntmp = RREG32(DC_HPD4_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD4_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont4 & DC_HPD5_INTERRUPT) {\r\ntmp = RREG32(DC_HPD5_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD5_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont5 & DC_HPD6_INTERRUPT) {\r\ntmp = RREG32(DC_HPD6_INT_CONTROL);\r\ntmp |= DC_HPDx_INT_ACK;\r\nWREG32(DC_HPD6_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int & DC_HPD1_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD1_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD1_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont & DC_HPD2_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD2_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD2_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont2 & DC_HPD3_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD3_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD3_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont3 & DC_HPD4_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD4_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD4_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont4 & DC_HPD5_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD5_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD5_INT_CONTROL, tmp);\r\n}\r\nif (rdev->irq.stat_regs.cik.disp_int_cont5 & DC_HPD6_RX_INTERRUPT) {\r\ntmp = RREG32(DC_HPD6_INT_CONTROL);\r\ntmp |= DC_HPDx_RX_INT_ACK;\r\nWREG32(DC_HPD6_INT_CONTROL, tmp);\r\n}\r\n}\r\nstatic void cik_irq_disable(struct radeon_device *rdev)\r\n{\r\ncik_disable_interrupts(rdev);\r\nmdelay(1);\r\ncik_irq_ack(rdev);\r\ncik_disable_interrupt_state(rdev);\r\n}\r\nstatic void cik_irq_suspend(struct radeon_device *rdev)\r\n{\r\ncik_irq_disable(rdev);\r\ncik_rlc_stop(rdev);\r\n}\r\nstatic void cik_irq_fini(struct radeon_device *rdev)\r\n{\r\ncik_irq_suspend(rdev);\r\nr600_ih_ring_fini(rdev);\r\n}\r\nstatic inline u32 cik_get_ih_wptr(struct radeon_device *rdev)\r\n{\r\nu32 wptr, tmp;\r\nif (rdev->wb.enabled)\r\nwptr = le32_to_cpu(rdev->wb.wb[R600_WB_IH_WPTR_OFFSET/4]);\r\nelse\r\nwptr = RREG32(IH_RB_WPTR);\r\nif (wptr & RB_OVERFLOW) {\r\nwptr &= ~RB_OVERFLOW;\r\ndev_warn(rdev->dev, "IH ring buffer overflow (0x%08X, 0x%08X, 0x%08X)\n",\r\nwptr, rdev->ih.rptr, (wptr + 16) & rdev->ih.ptr_mask);\r\nrdev->ih.rptr = (wptr + 16) & rdev->ih.ptr_mask;\r\ntmp = RREG32(IH_RB_CNTL);\r\ntmp |= IH_WPTR_OVERFLOW_CLEAR;\r\nWREG32(IH_RB_CNTL, tmp);\r\n}\r\nreturn (wptr & rdev->ih.ptr_mask);\r\n}\r\nint cik_irq_process(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *cp1_ring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nstruct radeon_ring *cp2_ring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nu32 wptr;\r\nu32 rptr;\r\nu32 src_id, src_data, ring_id;\r\nu8 me_id, pipe_id, queue_id;\r\nu32 ring_index;\r\nbool queue_hotplug = false;\r\nbool queue_dp = false;\r\nbool queue_reset = false;\r\nu32 addr, status, mc_client;\r\nbool queue_thermal = false;\r\nif (!rdev->ih.enabled || rdev->shutdown)\r\nreturn IRQ_NONE;\r\nwptr = cik_get_ih_wptr(rdev);\r\nrestart_ih:\r\nif (atomic_xchg(&rdev->ih.lock, 1))\r\nreturn IRQ_NONE;\r\nrptr = rdev->ih.rptr;\r\nDRM_DEBUG("cik_irq_process start: rptr %d, wptr %d\n", rptr, wptr);\r\nrmb();\r\ncik_irq_ack(rdev);\r\nwhile (rptr != wptr) {\r\nring_index = rptr / 4;\r\nradeon_kfd_interrupt(rdev,\r\n(const void *) &rdev->ih.ring[ring_index]);\r\nsrc_id = le32_to_cpu(rdev->ih.ring[ring_index]) & 0xff;\r\nsrc_data = le32_to_cpu(rdev->ih.ring[ring_index + 1]) & 0xfffffff;\r\nring_id = le32_to_cpu(rdev->ih.ring[ring_index + 2]) & 0xff;\r\nswitch (src_id) {\r\ncase 1:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.cik.disp_int & LB_D1_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[0]) {\r\ndrm_handle_vblank(rdev->ddev, 0);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[0]))\r\nradeon_crtc_handle_vblank(rdev, 0);\r\nrdev->irq.stat_regs.cik.disp_int &= ~LB_D1_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D1 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.cik.disp_int & LB_D1_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int &= ~LB_D1_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D1 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 2:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont & LB_D2_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[1]) {\r\ndrm_handle_vblank(rdev->ddev, 1);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[1]))\r\nradeon_crtc_handle_vblank(rdev, 1);\r\nrdev->irq.stat_regs.cik.disp_int_cont &= ~LB_D2_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D2 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont & LB_D2_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont &= ~LB_D2_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D2 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 3:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont2 & LB_D3_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[2]) {\r\ndrm_handle_vblank(rdev->ddev, 2);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[2]))\r\nradeon_crtc_handle_vblank(rdev, 2);\r\nrdev->irq.stat_regs.cik.disp_int_cont2 &= ~LB_D3_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D3 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont2 & LB_D3_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont2 &= ~LB_D3_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D3 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 4:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont3 & LB_D4_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[3]) {\r\ndrm_handle_vblank(rdev->ddev, 3);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[3]))\r\nradeon_crtc_handle_vblank(rdev, 3);\r\nrdev->irq.stat_regs.cik.disp_int_cont3 &= ~LB_D4_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D4 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont3 & LB_D4_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont3 &= ~LB_D4_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D4 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 5:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont4 & LB_D5_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[4]) {\r\ndrm_handle_vblank(rdev->ddev, 4);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[4]))\r\nradeon_crtc_handle_vblank(rdev, 4);\r\nrdev->irq.stat_regs.cik.disp_int_cont4 &= ~LB_D5_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D5 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont4 & LB_D5_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont4 &= ~LB_D5_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D5 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 6:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont5 & LB_D6_VBLANK_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nif (rdev->irq.crtc_vblank_int[5]) {\r\ndrm_handle_vblank(rdev->ddev, 5);\r\nrdev->pm.vblank_sync = true;\r\nwake_up(&rdev->irq.vblank_queue);\r\n}\r\nif (atomic_read(&rdev->irq.pflip[5]))\r\nradeon_crtc_handle_vblank(rdev, 5);\r\nrdev->irq.stat_regs.cik.disp_int_cont5 &= ~LB_D6_VBLANK_INTERRUPT;\r\nDRM_DEBUG("IH: D6 vblank\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont5 & LB_D6_VLINE_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont5 &= ~LB_D6_VLINE_INTERRUPT;\r\nDRM_DEBUG("IH: D6 vline\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 8:\r\ncase 10:\r\ncase 12:\r\ncase 14:\r\ncase 16:\r\ncase 18:\r\nDRM_DEBUG("IH: D%d flip\n", ((src_id - 8) >> 1) + 1);\r\nif (radeon_use_pflipirq > 0)\r\nradeon_crtc_handle_flip(rdev, (src_id - 8) >> 1);\r\nbreak;\r\ncase 42:\r\nswitch (src_data) {\r\ncase 0:\r\nif (!(rdev->irq.stat_regs.cik.disp_int & DC_HPD1_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int &= ~DC_HPD1_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD1\n");\r\nbreak;\r\ncase 1:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont & DC_HPD2_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont &= ~DC_HPD2_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD2\n");\r\nbreak;\r\ncase 2:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont2 & DC_HPD3_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont2 &= ~DC_HPD3_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD3\n");\r\nbreak;\r\ncase 3:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont3 & DC_HPD4_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont3 &= ~DC_HPD4_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD4\n");\r\nbreak;\r\ncase 4:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont4 & DC_HPD5_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont4 &= ~DC_HPD5_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD5\n");\r\nbreak;\r\ncase 5:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont5 & DC_HPD6_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont5 &= ~DC_HPD6_INTERRUPT;\r\nqueue_hotplug = true;\r\nDRM_DEBUG("IH: HPD6\n");\r\nbreak;\r\ncase 6:\r\nif (!(rdev->irq.stat_regs.cik.disp_int & DC_HPD1_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int &= ~DC_HPD1_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 1\n");\r\nbreak;\r\ncase 7:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont & DC_HPD2_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont &= ~DC_HPD2_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 2\n");\r\nbreak;\r\ncase 8:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont2 & DC_HPD3_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont2 &= ~DC_HPD3_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 3\n");\r\nbreak;\r\ncase 9:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont3 & DC_HPD4_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont3 &= ~DC_HPD4_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 4\n");\r\nbreak;\r\ncase 10:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont4 & DC_HPD5_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont4 &= ~DC_HPD5_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 5\n");\r\nbreak;\r\ncase 11:\r\nif (!(rdev->irq.stat_regs.cik.disp_int_cont5 & DC_HPD6_RX_INTERRUPT))\r\nDRM_DEBUG("IH: IH event w/o asserted irq bit?\n");\r\nrdev->irq.stat_regs.cik.disp_int_cont5 &= ~DC_HPD6_RX_INTERRUPT;\r\nqueue_dp = true;\r\nDRM_DEBUG("IH: HPD_RX 6\n");\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 96:\r\nDRM_ERROR("SRBM_READ_ERROR: 0x%x\n", RREG32(SRBM_READ_ERROR));\r\nWREG32(SRBM_INT_ACK, 0x1);\r\nbreak;\r\ncase 124:\r\nDRM_DEBUG("IH: UVD int: 0x%08x\n", src_data);\r\nradeon_fence_process(rdev, R600_RING_TYPE_UVD_INDEX);\r\nbreak;\r\ncase 146:\r\ncase 147:\r\naddr = RREG32(VM_CONTEXT1_PROTECTION_FAULT_ADDR);\r\nstatus = RREG32(VM_CONTEXT1_PROTECTION_FAULT_STATUS);\r\nmc_client = RREG32(VM_CONTEXT1_PROTECTION_FAULT_MCCLIENT);\r\nWREG32_P(VM_CONTEXT1_CNTL2, 1, ~1);\r\nif (addr == 0x0 && status == 0x0)\r\nbreak;\r\ndev_err(rdev->dev, "GPU fault detected: %d 0x%08x\n", src_id, src_data);\r\ndev_err(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_ADDR 0x%08X\n",\r\naddr);\r\ndev_err(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",\r\nstatus);\r\ncik_vm_decode_fault(rdev, status, addr, mc_client);\r\nbreak;\r\ncase 167:\r\nDRM_DEBUG("IH: VCE int: 0x%08x\n", src_data);\r\nswitch (src_data) {\r\ncase 0:\r\nradeon_fence_process(rdev, TN_RING_TYPE_VCE1_INDEX);\r\nbreak;\r\ncase 1:\r\nradeon_fence_process(rdev, TN_RING_TYPE_VCE2_INDEX);\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nbreak;\r\ncase 176:\r\ncase 177:\r\nradeon_fence_process(rdev, RADEON_RING_TYPE_GFX_INDEX);\r\nbreak;\r\ncase 181:\r\nDRM_DEBUG("IH: CP EOP\n");\r\nme_id = (ring_id & 0x60) >> 5;\r\npipe_id = (ring_id & 0x18) >> 3;\r\nqueue_id = (ring_id & 0x7) >> 0;\r\nswitch (me_id) {\r\ncase 0:\r\nradeon_fence_process(rdev, RADEON_RING_TYPE_GFX_INDEX);\r\nbreak;\r\ncase 1:\r\ncase 2:\r\nif ((cp1_ring->me == me_id) & (cp1_ring->pipe == pipe_id))\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_CP1_INDEX);\r\nif ((cp2_ring->me == me_id) & (cp2_ring->pipe == pipe_id))\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_CP2_INDEX);\r\nbreak;\r\n}\r\nbreak;\r\ncase 184:\r\nDRM_ERROR("Illegal register access in command stream\n");\r\nme_id = (ring_id & 0x60) >> 5;\r\npipe_id = (ring_id & 0x18) >> 3;\r\nqueue_id = (ring_id & 0x7) >> 0;\r\nswitch (me_id) {\r\ncase 0:\r\nqueue_reset = true;\r\nbreak;\r\ncase 1:\r\nqueue_reset = true;\r\nbreak;\r\ncase 2:\r\nqueue_reset = true;\r\nbreak;\r\n}\r\nbreak;\r\ncase 185:\r\nDRM_ERROR("Illegal instruction in command stream\n");\r\nme_id = (ring_id & 0x60) >> 5;\r\npipe_id = (ring_id & 0x18) >> 3;\r\nqueue_id = (ring_id & 0x7) >> 0;\r\nswitch (me_id) {\r\ncase 0:\r\nqueue_reset = true;\r\nbreak;\r\ncase 1:\r\nqueue_reset = true;\r\nbreak;\r\ncase 2:\r\nqueue_reset = true;\r\nbreak;\r\n}\r\nbreak;\r\ncase 224:\r\nme_id = (ring_id & 0x3) >> 0;\r\nqueue_id = (ring_id & 0xc) >> 2;\r\nDRM_DEBUG("IH: SDMA trap\n");\r\nswitch (me_id) {\r\ncase 0:\r\nswitch (queue_id) {\r\ncase 0:\r\nradeon_fence_process(rdev, R600_RING_TYPE_DMA_INDEX);\r\nbreak;\r\ncase 1:\r\nbreak;\r\ncase 2:\r\nbreak;\r\n}\r\nbreak;\r\ncase 1:\r\nswitch (queue_id) {\r\ncase 0:\r\nradeon_fence_process(rdev, CAYMAN_RING_TYPE_DMA1_INDEX);\r\nbreak;\r\ncase 1:\r\nbreak;\r\ncase 2:\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nbreak;\r\ncase 230:\r\nDRM_DEBUG("IH: thermal low to high\n");\r\nrdev->pm.dpm.thermal.high_to_low = false;\r\nqueue_thermal = true;\r\nbreak;\r\ncase 231:\r\nDRM_DEBUG("IH: thermal high to low\n");\r\nrdev->pm.dpm.thermal.high_to_low = true;\r\nqueue_thermal = true;\r\nbreak;\r\ncase 233:\r\nDRM_DEBUG("IH: GUI idle\n");\r\nbreak;\r\ncase 241:\r\ncase 247:\r\nDRM_ERROR("Illegal instruction in SDMA command stream\n");\r\nme_id = (ring_id & 0x3) >> 0;\r\nqueue_id = (ring_id & 0xc) >> 2;\r\nswitch (me_id) {\r\ncase 0:\r\nswitch (queue_id) {\r\ncase 0:\r\nqueue_reset = true;\r\nbreak;\r\ncase 1:\r\nqueue_reset = true;\r\nbreak;\r\ncase 2:\r\nqueue_reset = true;\r\nbreak;\r\n}\r\nbreak;\r\ncase 1:\r\nswitch (queue_id) {\r\ncase 0:\r\nqueue_reset = true;\r\nbreak;\r\ncase 1:\r\nqueue_reset = true;\r\nbreak;\r\ncase 2:\r\nqueue_reset = true;\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\nDRM_DEBUG("Unhandled interrupt: %d %d\n", src_id, src_data);\r\nbreak;\r\n}\r\nrptr += 16;\r\nrptr &= rdev->ih.ptr_mask;\r\nWREG32(IH_RB_RPTR, rptr);\r\n}\r\nif (queue_dp)\r\nschedule_work(&rdev->dp_work);\r\nif (queue_hotplug)\r\nschedule_delayed_work(&rdev->hotplug_work, 0);\r\nif (queue_reset) {\r\nrdev->needs_reset = true;\r\nwake_up_all(&rdev->fence_queue);\r\n}\r\nif (queue_thermal)\r\nschedule_work(&rdev->pm.dpm.thermal.work);\r\nrdev->ih.rptr = rptr;\r\natomic_set(&rdev->ih.lock, 0);\r\nwptr = cik_get_ih_wptr(rdev);\r\nif (wptr != rptr)\r\ngoto restart_ih;\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void cik_uvd_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_uvd)\r\nreturn;\r\nr = radeon_uvd_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed UVD (%d) init.\n", r);\r\nrdev->has_uvd = 0;\r\nreturn;\r\n}\r\nrdev->ring[R600_RING_TYPE_UVD_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[R600_RING_TYPE_UVD_INDEX], 4096);\r\n}\r\nstatic void cik_uvd_start(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_uvd)\r\nreturn;\r\nr = radeon_uvd_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed UVD resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = uvd_v4_2_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed UVD 4.2 resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, R600_RING_TYPE_UVD_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD fences (%d).\n", r);\r\ngoto error;\r\n}\r\nreturn;\r\nerror:\r\nrdev->ring[R600_RING_TYPE_UVD_INDEX].ring_size = 0;\r\n}\r\nstatic void cik_uvd_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nif (!rdev->has_uvd || !rdev->ring[R600_RING_TYPE_UVD_INDEX].ring_size)\r\nreturn;\r\nring = &rdev->ring[R600_RING_TYPE_UVD_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, PACKET0(UVD_NO_OP, 0));\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD ring (%d).\n", r);\r\nreturn;\r\n}\r\nr = uvd_v1_0_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD (%d).\n", r);\r\nreturn;\r\n}\r\n}\r\nstatic void cik_vce_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_vce)\r\nreturn;\r\nr = radeon_vce_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE (%d) init.\n", r);\r\nrdev->has_vce = 0;\r\nreturn;\r\n}\r\nrdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[TN_RING_TYPE_VCE1_INDEX], 4096);\r\nrdev->ring[TN_RING_TYPE_VCE2_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[TN_RING_TYPE_VCE2_INDEX], 4096);\r\n}\r\nstatic void cik_vce_start(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_vce)\r\nreturn;\r\nr = radeon_vce_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = vce_v2_0_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, TN_RING_TYPE_VCE1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 fences (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, TN_RING_TYPE_VCE2_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE2 fences (%d).\n", r);\r\ngoto error;\r\n}\r\nreturn;\r\nerror:\r\nrdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_size = 0;\r\nrdev->ring[TN_RING_TYPE_VCE2_INDEX].ring_size = 0;\r\n}\r\nstatic void cik_vce_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nif (!rdev->has_vce || !rdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_size)\r\nreturn;\r\nring = &rdev->ring[TN_RING_TYPE_VCE1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, VCE_CMD_NO_OP);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 ring (%d).\n", r);\r\nreturn;\r\n}\r\nring = &rdev->ring[TN_RING_TYPE_VCE2_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, VCE_CMD_NO_OP);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 ring (%d).\n", r);\r\nreturn;\r\n}\r\nr = vce_v1_0_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE (%d).\n", r);\r\nreturn;\r\n}\r\n}\r\nstatic int cik_startup(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nu32 nop;\r\nint r;\r\ncik_pcie_gen3_enable(rdev);\r\ncik_program_aspm(rdev);\r\nr = r600_vram_scratch_init(rdev);\r\nif (r)\r\nreturn r;\r\ncik_mc_program(rdev);\r\nif (!(rdev->flags & RADEON_IS_IGP) && !rdev->pm.dpm_enabled) {\r\nr = ci_mc_load_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load MC firmware!\n");\r\nreturn r;\r\n}\r\n}\r\nr = cik_pcie_gart_enable(rdev);\r\nif (r)\r\nreturn r;\r\ncik_gpu_init(rdev);\r\nif (rdev->flags & RADEON_IS_IGP) {\r\nif (rdev->family == CHIP_KAVERI) {\r\nrdev->rlc.reg_list = spectre_rlc_save_restore_register_list;\r\nrdev->rlc.reg_list_size =\r\n(u32)ARRAY_SIZE(spectre_rlc_save_restore_register_list);\r\n} else {\r\nrdev->rlc.reg_list = kalindi_rlc_save_restore_register_list;\r\nrdev->rlc.reg_list_size =\r\n(u32)ARRAY_SIZE(kalindi_rlc_save_restore_register_list);\r\n}\r\n}\r\nrdev->rlc.cs_data = ci_cs_data;\r\nrdev->rlc.cp_table_size = ALIGN(CP_ME_TABLE_SIZE * 5 * 4, 2048);\r\nrdev->rlc.cp_table_size += 64 * 1024;\r\nr = sumo_rlc_init(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to init rlc BOs!\n");\r\nreturn r;\r\n}\r\nr = radeon_wb_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = cik_mec_init(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to init MEC BOs!\n");\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, RADEON_RING_TYPE_GFX_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_CP1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_CP2_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, R600_RING_TYPE_DMA_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing DMA fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_DMA1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing DMA fences (%d).\n", r);\r\nreturn r;\r\n}\r\ncik_uvd_start(rdev);\r\ncik_vce_start(rdev);\r\nif (!rdev->irq.installed) {\r\nr = radeon_irq_kms_init(rdev);\r\nif (r)\r\nreturn r;\r\n}\r\nr = cik_irq_init(rdev);\r\nif (r) {\r\nDRM_ERROR("radeon: IH init failed (%d).\n", r);\r\nradeon_irq_kms_fini(rdev);\r\nreturn r;\r\n}\r\ncik_irq_set(rdev);\r\nif (rdev->family == CHIP_HAWAII) {\r\nif (rdev->new_fw)\r\nnop = PACKET3(PACKET3_NOP, 0x3FFF);\r\nelse\r\nnop = RADEON_CP_PACKET2;\r\n} else {\r\nnop = PACKET3(PACKET3_NOP, 0x3FFF);\r\n}\r\nring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, RADEON_WB_CP_RPTR_OFFSET,\r\nnop);\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, RADEON_WB_CP1_RPTR_OFFSET,\r\nnop);\r\nif (r)\r\nreturn r;\r\nring->me = 1;\r\nring->pipe = 0;\r\nring->queue = 0;\r\nring->wptr_offs = CIK_WB_CP1_WPTR_OFFSET;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, RADEON_WB_CP2_RPTR_OFFSET,\r\nnop);\r\nif (r)\r\nreturn r;\r\nring->me = 1;\r\nring->pipe = 0;\r\nring->queue = 1;\r\nring->wptr_offs = CIK_WB_CP2_WPTR_OFFSET;\r\nring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, R600_WB_DMA_RPTR_OFFSET,\r\nSDMA_PACKET(SDMA_OPCODE_NOP, 0, 0));\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, CAYMAN_WB_DMA1_RPTR_OFFSET,\r\nSDMA_PACKET(SDMA_OPCODE_NOP, 0, 0));\r\nif (r)\r\nreturn r;\r\nr = cik_cp_resume(rdev);\r\nif (r)\r\nreturn r;\r\nr = cik_sdma_resume(rdev);\r\nif (r)\r\nreturn r;\r\ncik_uvd_resume(rdev);\r\ncik_vce_resume(rdev);\r\nr = radeon_ib_pool_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "IB initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_vm_manager_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "vm manager initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_audio_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = radeon_kfd_resume(rdev);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nint cik_resume(struct radeon_device *rdev)\r\n{\r\nint r;\r\natom_asic_init(rdev->mode_info.atom_context);\r\ncik_init_golden_registers(rdev);\r\nif (rdev->pm.pm_method == PM_METHOD_DPM)\r\nradeon_pm_resume(rdev);\r\nrdev->accel_working = true;\r\nr = cik_startup(rdev);\r\nif (r) {\r\nDRM_ERROR("cik startup failed on resume\n");\r\nrdev->accel_working = false;\r\nreturn r;\r\n}\r\nreturn r;\r\n}\r\nint cik_suspend(struct radeon_device *rdev)\r\n{\r\nradeon_kfd_suspend(rdev);\r\nradeon_pm_suspend(rdev);\r\nradeon_audio_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\ncik_cp_enable(rdev, false);\r\ncik_sdma_enable(rdev, false);\r\nif (rdev->has_uvd) {\r\nuvd_v1_0_fini(rdev);\r\nradeon_uvd_suspend(rdev);\r\n}\r\nif (rdev->has_vce)\r\nradeon_vce_suspend(rdev);\r\ncik_fini_pg(rdev);\r\ncik_fini_cg(rdev);\r\ncik_irq_suspend(rdev);\r\nradeon_wb_disable(rdev);\r\ncik_pcie_gart_disable(rdev);\r\nreturn 0;\r\n}\r\nint cik_init(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nif (!radeon_get_bios(rdev)) {\r\nif (ASIC_IS_AVIVO(rdev))\r\nreturn -EINVAL;\r\n}\r\nif (!rdev->is_atom_bios) {\r\ndev_err(rdev->dev, "Expecting atombios for cayman GPU\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_atombios_init(rdev);\r\nif (r)\r\nreturn r;\r\nif (!radeon_card_posted(rdev)) {\r\nif (!rdev->bios) {\r\ndev_err(rdev->dev, "Card not posted and no BIOS - ignoring\n");\r\nreturn -EINVAL;\r\n}\r\nDRM_INFO("GPU not posted. posting now...\n");\r\natom_asic_init(rdev->mode_info.atom_context);\r\n}\r\ncik_init_golden_registers(rdev);\r\ncik_scratch_init(rdev);\r\nradeon_surface_init(rdev);\r\nradeon_get_clock_info(rdev->ddev);\r\nr = radeon_fence_driver_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = cik_mc_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = radeon_bo_init(rdev);\r\nif (r)\r\nreturn r;\r\nif (rdev->flags & RADEON_IS_IGP) {\r\nif (!rdev->me_fw || !rdev->pfp_fw || !rdev->ce_fw ||\r\n!rdev->mec_fw || !rdev->sdma_fw || !rdev->rlc_fw) {\r\nr = cik_init_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load firmware!\n");\r\nreturn r;\r\n}\r\n}\r\n} else {\r\nif (!rdev->me_fw || !rdev->pfp_fw || !rdev->ce_fw ||\r\n!rdev->mec_fw || !rdev->sdma_fw || !rdev->rlc_fw ||\r\n!rdev->mc_fw) {\r\nr = cik_init_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load firmware!\n");\r\nreturn r;\r\n}\r\n}\r\n}\r\nradeon_pm_init(rdev);\r\nring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 1024 * 1024);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP1_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 1024 * 1024);\r\nr = radeon_doorbell_get(rdev, &ring->doorbell_index);\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_CP2_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 1024 * 1024);\r\nr = radeon_doorbell_get(rdev, &ring->doorbell_index);\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 256 * 1024);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 256 * 1024);\r\ncik_uvd_init(rdev);\r\ncik_vce_init(rdev);\r\nrdev->ih.ring_obj = NULL;\r\nr600_ih_ring_init(rdev, 64 * 1024);\r\nr = r600_pcie_gart_init(rdev);\r\nif (r)\r\nreturn r;\r\nrdev->accel_working = true;\r\nr = cik_startup(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "disabling GPU acceleration\n");\r\ncik_cp_fini(rdev);\r\ncik_sdma_fini(rdev);\r\ncik_irq_fini(rdev);\r\nsumo_rlc_fini(rdev);\r\ncik_mec_fini(rdev);\r\nradeon_wb_fini(rdev);\r\nradeon_ib_pool_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\nradeon_irq_kms_fini(rdev);\r\ncik_pcie_gart_fini(rdev);\r\nrdev->accel_working = false;\r\n}\r\nif (!rdev->mc_fw && !(rdev->flags & RADEON_IS_IGP)) {\r\nDRM_ERROR("radeon: MC ucode required for NI+.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid cik_fini(struct radeon_device *rdev)\r\n{\r\nradeon_pm_fini(rdev);\r\ncik_cp_fini(rdev);\r\ncik_sdma_fini(rdev);\r\ncik_fini_pg(rdev);\r\ncik_fini_cg(rdev);\r\ncik_irq_fini(rdev);\r\nsumo_rlc_fini(rdev);\r\ncik_mec_fini(rdev);\r\nradeon_wb_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\nradeon_ib_pool_fini(rdev);\r\nradeon_irq_kms_fini(rdev);\r\nuvd_v1_0_fini(rdev);\r\nradeon_uvd_fini(rdev);\r\nradeon_vce_fini(rdev);\r\ncik_pcie_gart_fini(rdev);\r\nr600_vram_scratch_fini(rdev);\r\nradeon_gem_fini(rdev);\r\nradeon_fence_driver_fini(rdev);\r\nradeon_bo_fini(rdev);\r\nradeon_atombios_fini(rdev);\r\nkfree(rdev->bios);\r\nrdev->bios = NULL;\r\n}\r\nvoid dce8_program_fmt(struct drm_encoder *encoder)\r\n{\r\nstruct drm_device *dev = encoder->dev;\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct radeon_encoder *radeon_encoder = to_radeon_encoder(encoder);\r\nstruct radeon_crtc *radeon_crtc = to_radeon_crtc(encoder->crtc);\r\nstruct drm_connector *connector = radeon_get_connector_for_encoder(encoder);\r\nint bpc = 0;\r\nu32 tmp = 0;\r\nenum radeon_connector_dither dither = RADEON_FMT_DITHER_DISABLE;\r\nif (connector) {\r\nstruct radeon_connector *radeon_connector = to_radeon_connector(connector);\r\nbpc = radeon_get_monitor_bpc(connector);\r\ndither = radeon_connector->dither;\r\n}\r\nif (radeon_encoder->devices & ATOM_DEVICE_LCD_SUPPORT)\r\nreturn;\r\nif ((radeon_encoder->encoder_id == ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC1) ||\r\n(radeon_encoder->encoder_id == ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC2))\r\nreturn;\r\nif (bpc == 0)\r\nreturn;\r\nswitch (bpc) {\r\ncase 6:\r\nif (dither == RADEON_FMT_DITHER_ENABLE)\r\ntmp |= (FMT_FRAME_RANDOM_ENABLE | FMT_HIGHPASS_RANDOM_ENABLE |\r\nFMT_SPATIAL_DITHER_EN | FMT_SPATIAL_DITHER_DEPTH(0));\r\nelse\r\ntmp |= (FMT_TRUNCATE_EN | FMT_TRUNCATE_DEPTH(0));\r\nbreak;\r\ncase 8:\r\nif (dither == RADEON_FMT_DITHER_ENABLE)\r\ntmp |= (FMT_FRAME_RANDOM_ENABLE | FMT_HIGHPASS_RANDOM_ENABLE |\r\nFMT_RGB_RANDOM_ENABLE |\r\nFMT_SPATIAL_DITHER_EN | FMT_SPATIAL_DITHER_DEPTH(1));\r\nelse\r\ntmp |= (FMT_TRUNCATE_EN | FMT_TRUNCATE_DEPTH(1));\r\nbreak;\r\ncase 10:\r\nif (dither == RADEON_FMT_DITHER_ENABLE)\r\ntmp |= (FMT_FRAME_RANDOM_ENABLE | FMT_HIGHPASS_RANDOM_ENABLE |\r\nFMT_RGB_RANDOM_ENABLE |\r\nFMT_SPATIAL_DITHER_EN | FMT_SPATIAL_DITHER_DEPTH(2));\r\nelse\r\ntmp |= (FMT_TRUNCATE_EN | FMT_TRUNCATE_DEPTH(2));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nWREG32(FMT_BIT_DEPTH_CONTROL + radeon_crtc->crtc_offset, tmp);\r\n}\r\nstatic u32 dce8_line_buffer_adjust(struct radeon_device *rdev,\r\nstruct radeon_crtc *radeon_crtc,\r\nstruct drm_display_mode *mode)\r\n{\r\nu32 tmp, buffer_alloc, i;\r\nu32 pipe_offset = radeon_crtc->crtc_id * 0x20;\r\nif (radeon_crtc->base.enabled && mode) {\r\nif (mode->crtc_hdisplay < 1920) {\r\ntmp = 1;\r\nbuffer_alloc = 2;\r\n} else if (mode->crtc_hdisplay < 2560) {\r\ntmp = 2;\r\nbuffer_alloc = 2;\r\n} else if (mode->crtc_hdisplay < 4096) {\r\ntmp = 0;\r\nbuffer_alloc = (rdev->flags & RADEON_IS_IGP) ? 2 : 4;\r\n} else {\r\nDRM_DEBUG_KMS("Mode too big for LB!\n");\r\ntmp = 0;\r\nbuffer_alloc = (rdev->flags & RADEON_IS_IGP) ? 2 : 4;\r\n}\r\n} else {\r\ntmp = 1;\r\nbuffer_alloc = 0;\r\n}\r\nWREG32(LB_MEMORY_CTRL + radeon_crtc->crtc_offset,\r\nLB_MEMORY_CONFIG(tmp) | LB_MEMORY_SIZE(0x6B0));\r\nWREG32(PIPE0_DMIF_BUFFER_CONTROL + pipe_offset,\r\nDMIF_BUFFERS_ALLOCATED(buffer_alloc));\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(PIPE0_DMIF_BUFFER_CONTROL + pipe_offset) &\r\nDMIF_BUFFERS_ALLOCATED_COMPLETED)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (radeon_crtc->base.enabled && mode) {\r\nswitch (tmp) {\r\ncase 0:\r\ndefault:\r\nreturn 4096 * 2;\r\ncase 1:\r\nreturn 1920 * 2;\r\ncase 2:\r\nreturn 2560 * 2;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic u32 cik_get_number_of_dram_channels(struct radeon_device *rdev)\r\n{\r\nu32 tmp = RREG32(MC_SHARED_CHMAP);\r\nswitch ((tmp & NOOFCHAN_MASK) >> NOOFCHAN_SHIFT) {\r\ncase 0:\r\ndefault:\r\nreturn 1;\r\ncase 1:\r\nreturn 2;\r\ncase 2:\r\nreturn 4;\r\ncase 3:\r\nreturn 8;\r\ncase 4:\r\nreturn 3;\r\ncase 5:\r\nreturn 6;\r\ncase 6:\r\nreturn 10;\r\ncase 7:\r\nreturn 12;\r\ncase 8:\r\nreturn 16;\r\n}\r\n}\r\nstatic u32 dce8_dram_bandwidth(struct dce8_wm_params *wm)\r\n{\r\nfixed20_12 dram_efficiency;\r\nfixed20_12 yclk, dram_channels, bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nyclk.full = dfixed_const(wm->yclk);\r\nyclk.full = dfixed_div(yclk, a);\r\ndram_channels.full = dfixed_const(wm->dram_channels * 4);\r\na.full = dfixed_const(10);\r\ndram_efficiency.full = dfixed_const(7);\r\ndram_efficiency.full = dfixed_div(dram_efficiency, a);\r\nbandwidth.full = dfixed_mul(dram_channels, yclk);\r\nbandwidth.full = dfixed_mul(bandwidth, dram_efficiency);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce8_dram_bandwidth_for_display(struct dce8_wm_params *wm)\r\n{\r\nfixed20_12 disp_dram_allocation;\r\nfixed20_12 yclk, dram_channels, bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nyclk.full = dfixed_const(wm->yclk);\r\nyclk.full = dfixed_div(yclk, a);\r\ndram_channels.full = dfixed_const(wm->dram_channels * 4);\r\na.full = dfixed_const(10);\r\ndisp_dram_allocation.full = dfixed_const(3);\r\ndisp_dram_allocation.full = dfixed_div(disp_dram_allocation, a);\r\nbandwidth.full = dfixed_mul(dram_channels, yclk);\r\nbandwidth.full = dfixed_mul(bandwidth, disp_dram_allocation);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce8_data_return_bandwidth(struct dce8_wm_params *wm)\r\n{\r\nfixed20_12 return_efficiency;\r\nfixed20_12 sclk, bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nsclk.full = dfixed_const(wm->sclk);\r\nsclk.full = dfixed_div(sclk, a);\r\na.full = dfixed_const(10);\r\nreturn_efficiency.full = dfixed_const(8);\r\nreturn_efficiency.full = dfixed_div(return_efficiency, a);\r\na.full = dfixed_const(32);\r\nbandwidth.full = dfixed_mul(a, sclk);\r\nbandwidth.full = dfixed_mul(bandwidth, return_efficiency);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce8_dmif_request_bandwidth(struct dce8_wm_params *wm)\r\n{\r\nfixed20_12 disp_clk_request_efficiency;\r\nfixed20_12 disp_clk, bandwidth;\r\nfixed20_12 a, b;\r\na.full = dfixed_const(1000);\r\ndisp_clk.full = dfixed_const(wm->disp_clk);\r\ndisp_clk.full = dfixed_div(disp_clk, a);\r\na.full = dfixed_const(32);\r\nb.full = dfixed_mul(a, disp_clk);\r\na.full = dfixed_const(10);\r\ndisp_clk_request_efficiency.full = dfixed_const(8);\r\ndisp_clk_request_efficiency.full = dfixed_div(disp_clk_request_efficiency, a);\r\nbandwidth.full = dfixed_mul(b, disp_clk_request_efficiency);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce8_available_bandwidth(struct dce8_wm_params *wm)\r\n{\r\nu32 dram_bandwidth = dce8_dram_bandwidth(wm);\r\nu32 data_return_bandwidth = dce8_data_return_bandwidth(wm);\r\nu32 dmif_req_bandwidth = dce8_dmif_request_bandwidth(wm);\r\nreturn min(dram_bandwidth, min(data_return_bandwidth, dmif_req_bandwidth));\r\n}\r\nstatic u32 dce8_average_bandwidth(struct dce8_wm_params *wm)\r\n{\r\nfixed20_12 bpp;\r\nfixed20_12 line_time;\r\nfixed20_12 src_width;\r\nfixed20_12 bandwidth;\r\nfixed20_12 a;\r\na.full = dfixed_const(1000);\r\nline_time.full = dfixed_const(wm->active_time + wm->blank_time);\r\nline_time.full = dfixed_div(line_time, a);\r\nbpp.full = dfixed_const(wm->bytes_per_pixel);\r\nsrc_width.full = dfixed_const(wm->src_width);\r\nbandwidth.full = dfixed_mul(src_width, bpp);\r\nbandwidth.full = dfixed_mul(bandwidth, wm->vsc);\r\nbandwidth.full = dfixed_div(bandwidth, line_time);\r\nreturn dfixed_trunc(bandwidth);\r\n}\r\nstatic u32 dce8_latency_watermark(struct dce8_wm_params *wm)\r\n{\r\nu32 mc_latency = 2000;\r\nu32 available_bandwidth = dce8_available_bandwidth(wm);\r\nu32 worst_chunk_return_time = (512 * 8 * 1000) / available_bandwidth;\r\nu32 cursor_line_pair_return_time = (128 * 4 * 1000) / available_bandwidth;\r\nu32 dc_latency = 40000000 / wm->disp_clk;\r\nu32 other_heads_data_return_time = ((wm->num_heads + 1) * worst_chunk_return_time) +\r\n(wm->num_heads * cursor_line_pair_return_time);\r\nu32 latency = mc_latency + other_heads_data_return_time + dc_latency;\r\nu32 max_src_lines_per_dst_line, lb_fill_bw, line_fill_time;\r\nu32 tmp, dmif_size = 12288;\r\nfixed20_12 a, b, c;\r\nif (wm->num_heads == 0)\r\nreturn 0;\r\na.full = dfixed_const(2);\r\nb.full = dfixed_const(1);\r\nif ((wm->vsc.full > a.full) ||\r\n((wm->vsc.full > b.full) && (wm->vtaps >= 3)) ||\r\n(wm->vtaps >= 5) ||\r\n((wm->vsc.full >= a.full) && wm->interlaced))\r\nmax_src_lines_per_dst_line = 4;\r\nelse\r\nmax_src_lines_per_dst_line = 2;\r\na.full = dfixed_const(available_bandwidth);\r\nb.full = dfixed_const(wm->num_heads);\r\na.full = dfixed_div(a, b);\r\ntmp = div_u64((u64) dmif_size * (u64) wm->disp_clk, mc_latency + 512);\r\ntmp = min(dfixed_trunc(a), tmp);\r\nlb_fill_bw = min(tmp, wm->disp_clk * wm->bytes_per_pixel / 1000);\r\na.full = dfixed_const(max_src_lines_per_dst_line * wm->src_width * wm->bytes_per_pixel);\r\nb.full = dfixed_const(1000);\r\nc.full = dfixed_const(lb_fill_bw);\r\nb.full = dfixed_div(c, b);\r\na.full = dfixed_div(a, b);\r\nline_fill_time = dfixed_trunc(a);\r\nif (line_fill_time < wm->active_time)\r\nreturn latency;\r\nelse\r\nreturn latency + (line_fill_time - wm->active_time);\r\n}\r\nstatic bool dce8_average_bandwidth_vs_dram_bandwidth_for_display(struct dce8_wm_params *wm)\r\n{\r\nif (dce8_average_bandwidth(wm) <=\r\n(dce8_dram_bandwidth_for_display(wm) / wm->num_heads))\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic bool dce8_average_bandwidth_vs_available_bandwidth(struct dce8_wm_params *wm)\r\n{\r\nif (dce8_average_bandwidth(wm) <=\r\n(dce8_available_bandwidth(wm) / wm->num_heads))\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic bool dce8_check_latency_hiding(struct dce8_wm_params *wm)\r\n{\r\nu32 lb_partitions = wm->lb_size / wm->src_width;\r\nu32 line_time = wm->active_time + wm->blank_time;\r\nu32 latency_tolerant_lines;\r\nu32 latency_hiding;\r\nfixed20_12 a;\r\na.full = dfixed_const(1);\r\nif (wm->vsc.full > a.full)\r\nlatency_tolerant_lines = 1;\r\nelse {\r\nif (lb_partitions <= (wm->vtaps + 1))\r\nlatency_tolerant_lines = 1;\r\nelse\r\nlatency_tolerant_lines = 2;\r\n}\r\nlatency_hiding = (latency_tolerant_lines * line_time + wm->blank_time);\r\nif (dce8_latency_watermark(wm) <= latency_hiding)\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic void dce8_program_watermarks(struct radeon_device *rdev,\r\nstruct radeon_crtc *radeon_crtc,\r\nu32 lb_size, u32 num_heads)\r\n{\r\nstruct drm_display_mode *mode = &radeon_crtc->base.mode;\r\nstruct dce8_wm_params wm_low, wm_high;\r\nu32 active_time;\r\nu32 line_time = 0;\r\nu32 latency_watermark_a = 0, latency_watermark_b = 0;\r\nu32 tmp, wm_mask;\r\nif (radeon_crtc->base.enabled && num_heads && mode) {\r\nactive_time = (u32) div_u64((u64)mode->crtc_hdisplay * 1000000,\r\n(u32)mode->clock);\r\nline_time = (u32) div_u64((u64)mode->crtc_htotal * 1000000,\r\n(u32)mode->clock);\r\nline_time = min(line_time, (u32)65535);\r\nif ((rdev->pm.pm_method == PM_METHOD_DPM) &&\r\nrdev->pm.dpm_enabled) {\r\nwm_high.yclk =\r\nradeon_dpm_get_mclk(rdev, false) * 10;\r\nwm_high.sclk =\r\nradeon_dpm_get_sclk(rdev, false) * 10;\r\n} else {\r\nwm_high.yclk = rdev->pm.current_mclk * 10;\r\nwm_high.sclk = rdev->pm.current_sclk * 10;\r\n}\r\nwm_high.disp_clk = mode->clock;\r\nwm_high.src_width = mode->crtc_hdisplay;\r\nwm_high.active_time = active_time;\r\nwm_high.blank_time = line_time - wm_high.active_time;\r\nwm_high.interlaced = false;\r\nif (mode->flags & DRM_MODE_FLAG_INTERLACE)\r\nwm_high.interlaced = true;\r\nwm_high.vsc = radeon_crtc->vsc;\r\nwm_high.vtaps = 1;\r\nif (radeon_crtc->rmx_type != RMX_OFF)\r\nwm_high.vtaps = 2;\r\nwm_high.bytes_per_pixel = 4;\r\nwm_high.lb_size = lb_size;\r\nwm_high.dram_channels = cik_get_number_of_dram_channels(rdev);\r\nwm_high.num_heads = num_heads;\r\nlatency_watermark_a = min(dce8_latency_watermark(&wm_high), (u32)65535);\r\nif (!dce8_average_bandwidth_vs_dram_bandwidth_for_display(&wm_high) ||\r\n!dce8_average_bandwidth_vs_available_bandwidth(&wm_high) ||\r\n!dce8_check_latency_hiding(&wm_high) ||\r\n(rdev->disp_priority == 2)) {\r\nDRM_DEBUG_KMS("force priority to high\n");\r\n}\r\nif ((rdev->pm.pm_method == PM_METHOD_DPM) &&\r\nrdev->pm.dpm_enabled) {\r\nwm_low.yclk =\r\nradeon_dpm_get_mclk(rdev, true) * 10;\r\nwm_low.sclk =\r\nradeon_dpm_get_sclk(rdev, true) * 10;\r\n} else {\r\nwm_low.yclk = rdev->pm.current_mclk * 10;\r\nwm_low.sclk = rdev->pm.current_sclk * 10;\r\n}\r\nwm_low.disp_clk = mode->clock;\r\nwm_low.src_width = mode->crtc_hdisplay;\r\nwm_low.active_time = active_time;\r\nwm_low.blank_time = line_time - wm_low.active_time;\r\nwm_low.interlaced = false;\r\nif (mode->flags & DRM_MODE_FLAG_INTERLACE)\r\nwm_low.interlaced = true;\r\nwm_low.vsc = radeon_crtc->vsc;\r\nwm_low.vtaps = 1;\r\nif (radeon_crtc->rmx_type != RMX_OFF)\r\nwm_low.vtaps = 2;\r\nwm_low.bytes_per_pixel = 4;\r\nwm_low.lb_size = lb_size;\r\nwm_low.dram_channels = cik_get_number_of_dram_channels(rdev);\r\nwm_low.num_heads = num_heads;\r\nlatency_watermark_b = min(dce8_latency_watermark(&wm_low), (u32)65535);\r\nif (!dce8_average_bandwidth_vs_dram_bandwidth_for_display(&wm_low) ||\r\n!dce8_average_bandwidth_vs_available_bandwidth(&wm_low) ||\r\n!dce8_check_latency_hiding(&wm_low) ||\r\n(rdev->disp_priority == 2)) {\r\nDRM_DEBUG_KMS("force priority to high\n");\r\n}\r\nradeon_crtc->lb_vblank_lead_lines = DIV_ROUND_UP(lb_size, mode->crtc_hdisplay);\r\n}\r\nwm_mask = RREG32(DPG_WATERMARK_MASK_CONTROL + radeon_crtc->crtc_offset);\r\ntmp = wm_mask;\r\ntmp &= ~LATENCY_WATERMARK_MASK(3);\r\ntmp |= LATENCY_WATERMARK_MASK(1);\r\nWREG32(DPG_WATERMARK_MASK_CONTROL + radeon_crtc->crtc_offset, tmp);\r\nWREG32(DPG_PIPE_LATENCY_CONTROL + radeon_crtc->crtc_offset,\r\n(LATENCY_LOW_WATERMARK(latency_watermark_a) |\r\nLATENCY_HIGH_WATERMARK(line_time)));\r\ntmp = RREG32(DPG_WATERMARK_MASK_CONTROL + radeon_crtc->crtc_offset);\r\ntmp &= ~LATENCY_WATERMARK_MASK(3);\r\ntmp |= LATENCY_WATERMARK_MASK(2);\r\nWREG32(DPG_WATERMARK_MASK_CONTROL + radeon_crtc->crtc_offset, tmp);\r\nWREG32(DPG_PIPE_LATENCY_CONTROL + radeon_crtc->crtc_offset,\r\n(LATENCY_LOW_WATERMARK(latency_watermark_b) |\r\nLATENCY_HIGH_WATERMARK(line_time)));\r\nWREG32(DPG_WATERMARK_MASK_CONTROL + radeon_crtc->crtc_offset, wm_mask);\r\nradeon_crtc->line_time = line_time;\r\nradeon_crtc->wm_high = latency_watermark_a;\r\nradeon_crtc->wm_low = latency_watermark_b;\r\n}\r\nvoid dce8_bandwidth_update(struct radeon_device *rdev)\r\n{\r\nstruct drm_display_mode *mode = NULL;\r\nu32 num_heads = 0, lb_size;\r\nint i;\r\nif (!rdev->mode_info.mode_config_initialized)\r\nreturn;\r\nradeon_update_display_priority(rdev);\r\nfor (i = 0; i < rdev->num_crtc; i++) {\r\nif (rdev->mode_info.crtcs[i]->base.enabled)\r\nnum_heads++;\r\n}\r\nfor (i = 0; i < rdev->num_crtc; i++) {\r\nmode = &rdev->mode_info.crtcs[i]->base.mode;\r\nlb_size = dce8_line_buffer_adjust(rdev, rdev->mode_info.crtcs[i], mode);\r\ndce8_program_watermarks(rdev, rdev->mode_info.crtcs[i], lb_size, num_heads);\r\n}\r\n}\r\nuint64_t cik_get_gpu_clock_counter(struct radeon_device *rdev)\r\n{\r\nuint64_t clock;\r\nmutex_lock(&rdev->gpu_clock_mutex);\r\nWREG32(RLC_CAPTURE_GPU_CLOCK_COUNT, 1);\r\nclock = (uint64_t)RREG32(RLC_GPU_CLOCK_COUNT_LSB) |\r\n((uint64_t)RREG32(RLC_GPU_CLOCK_COUNT_MSB) << 32ULL);\r\nmutex_unlock(&rdev->gpu_clock_mutex);\r\nreturn clock;\r\n}\r\nstatic int cik_set_uvd_clock(struct radeon_device *rdev, u32 clock,\r\nu32 cntl_reg, u32 status_reg)\r\n{\r\nint r, i;\r\nstruct atom_clock_dividers dividers;\r\nuint32_t tmp;\r\nr = radeon_atom_get_clock_dividers(rdev, COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\r\nclock, false, &dividers);\r\nif (r)\r\nreturn r;\r\ntmp = RREG32_SMC(cntl_reg);\r\ntmp &= ~(DCLK_DIR_CNTL_EN|DCLK_DIVIDER_MASK);\r\ntmp |= dividers.post_divider;\r\nWREG32_SMC(cntl_reg, tmp);\r\nfor (i = 0; i < 100; i++) {\r\nif (RREG32_SMC(status_reg) & DCLK_STATUS)\r\nbreak;\r\nmdelay(10);\r\n}\r\nif (i == 100)\r\nreturn -ETIMEDOUT;\r\nreturn 0;\r\n}\r\nint cik_set_uvd_clocks(struct radeon_device *rdev, u32 vclk, u32 dclk)\r\n{\r\nint r = 0;\r\nr = cik_set_uvd_clock(rdev, vclk, CG_VCLK_CNTL, CG_VCLK_STATUS);\r\nif (r)\r\nreturn r;\r\nr = cik_set_uvd_clock(rdev, dclk, CG_DCLK_CNTL, CG_DCLK_STATUS);\r\nreturn r;\r\n}\r\nint cik_set_vce_clocks(struct radeon_device *rdev, u32 evclk, u32 ecclk)\r\n{\r\nint r, i;\r\nstruct atom_clock_dividers dividers;\r\nu32 tmp;\r\nr = radeon_atom_get_clock_dividers(rdev, COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\r\necclk, false, &dividers);\r\nif (r)\r\nreturn r;\r\nfor (i = 0; i < 100; i++) {\r\nif (RREG32_SMC(CG_ECLK_STATUS) & ECLK_STATUS)\r\nbreak;\r\nmdelay(10);\r\n}\r\nif (i == 100)\r\nreturn -ETIMEDOUT;\r\ntmp = RREG32_SMC(CG_ECLK_CNTL);\r\ntmp &= ~(ECLK_DIR_CNTL_EN|ECLK_DIVIDER_MASK);\r\ntmp |= dividers.post_divider;\r\nWREG32_SMC(CG_ECLK_CNTL, tmp);\r\nfor (i = 0; i < 100; i++) {\r\nif (RREG32_SMC(CG_ECLK_STATUS) & ECLK_STATUS)\r\nbreak;\r\nmdelay(10);\r\n}\r\nif (i == 100)\r\nreturn -ETIMEDOUT;\r\nreturn 0;\r\n}\r\nstatic void cik_pcie_gen3_enable(struct radeon_device *rdev)\r\n{\r\nstruct pci_dev *root = rdev->pdev->bus->self;\r\nint bridge_pos, gpu_pos;\r\nu32 speed_cntl, mask, current_data_rate;\r\nint ret, i;\r\nu16 tmp16;\r\nif (pci_is_root_bus(rdev->pdev->bus))\r\nreturn;\r\nif (radeon_pcie_gen2 == 0)\r\nreturn;\r\nif (rdev->flags & RADEON_IS_IGP)\r\nreturn;\r\nif (!(rdev->flags & RADEON_IS_PCIE))\r\nreturn;\r\nret = drm_pcie_get_speed_cap_mask(rdev->ddev, &mask);\r\nif (ret != 0)\r\nreturn;\r\nif (!(mask & (DRM_PCIE_SPEED_50 | DRM_PCIE_SPEED_80)))\r\nreturn;\r\nspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\r\ncurrent_data_rate = (speed_cntl & LC_CURRENT_DATA_RATE_MASK) >>\r\nLC_CURRENT_DATA_RATE_SHIFT;\r\nif (mask & DRM_PCIE_SPEED_80) {\r\nif (current_data_rate == 2) {\r\nDRM_INFO("PCIE gen 3 link speeds already enabled\n");\r\nreturn;\r\n}\r\nDRM_INFO("enabling PCIE gen 3 link speeds, disable with radeon.pcie_gen2=0\n");\r\n} else if (mask & DRM_PCIE_SPEED_50) {\r\nif (current_data_rate == 1) {\r\nDRM_INFO("PCIE gen 2 link speeds already enabled\n");\r\nreturn;\r\n}\r\nDRM_INFO("enabling PCIE gen 2 link speeds, disable with radeon.pcie_gen2=0\n");\r\n}\r\nbridge_pos = pci_pcie_cap(root);\r\nif (!bridge_pos)\r\nreturn;\r\ngpu_pos = pci_pcie_cap(rdev->pdev);\r\nif (!gpu_pos)\r\nreturn;\r\nif (mask & DRM_PCIE_SPEED_80) {\r\nif (current_data_rate != 2) {\r\nu16 bridge_cfg, gpu_cfg;\r\nu16 bridge_cfg2, gpu_cfg2;\r\nu32 max_lw, current_lw, tmp;\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL, &bridge_cfg);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, &gpu_cfg);\r\ntmp16 = bridge_cfg | PCI_EXP_LNKCTL_HAWD;\r\npci_write_config_word(root, bridge_pos + PCI_EXP_LNKCTL, tmp16);\r\ntmp16 = gpu_cfg | PCI_EXP_LNKCTL_HAWD;\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, tmp16);\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_STATUS1);\r\nmax_lw = (tmp & LC_DETECTED_LINK_WIDTH_MASK) >> LC_DETECTED_LINK_WIDTH_SHIFT;\r\ncurrent_lw = (tmp & LC_OPERATING_LINK_WIDTH_MASK) >> LC_OPERATING_LINK_WIDTH_SHIFT;\r\nif (current_lw < max_lw) {\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\r\nif (tmp & LC_RENEGOTIATION_SUPPORT) {\r\ntmp &= ~(LC_LINK_WIDTH_MASK | LC_UPCONFIGURE_DIS);\r\ntmp |= (max_lw << LC_LINK_WIDTH_SHIFT);\r\ntmp |= LC_UPCONFIGURE_SUPPORT | LC_RENEGOTIATE_EN | LC_RECONFIG_NOW;\r\nWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, tmp);\r\n}\r\n}\r\nfor (i = 0; i < 10; i++) {\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_DEVSTA, &tmp16);\r\nif (tmp16 & PCI_EXP_DEVSTA_TRPND)\r\nbreak;\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL, &bridge_cfg);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, &gpu_cfg);\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL2, &bridge_cfg2);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, &gpu_cfg2);\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\r\ntmp |= LC_SET_QUIESCE;\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\r\ntmp |= LC_REDO_EQ;\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\r\nmdelay(100);\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL, &tmp16);\r\ntmp16 &= ~PCI_EXP_LNKCTL_HAWD;\r\ntmp16 |= (bridge_cfg & PCI_EXP_LNKCTL_HAWD);\r\npci_write_config_word(root, bridge_pos + PCI_EXP_LNKCTL, tmp16);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, &tmp16);\r\ntmp16 &= ~PCI_EXP_LNKCTL_HAWD;\r\ntmp16 |= (gpu_cfg & PCI_EXP_LNKCTL_HAWD);\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL, tmp16);\r\npci_read_config_word(root, bridge_pos + PCI_EXP_LNKCTL2, &tmp16);\r\ntmp16 &= ~((1 << 4) | (7 << 9));\r\ntmp16 |= (bridge_cfg2 & ((1 << 4) | (7 << 9)));\r\npci_write_config_word(root, bridge_pos + PCI_EXP_LNKCTL2, tmp16);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, &tmp16);\r\ntmp16 &= ~((1 << 4) | (7 << 9));\r\ntmp16 |= (gpu_cfg2 & ((1 << 4) | (7 << 9)));\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, tmp16);\r\ntmp = RREG32_PCIE_PORT(PCIE_LC_CNTL4);\r\ntmp &= ~LC_SET_QUIESCE;\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL4, tmp);\r\n}\r\n}\r\n}\r\nspeed_cntl |= LC_FORCE_EN_SW_SPEED_CHANGE | LC_FORCE_DIS_HW_SPEED_CHANGE;\r\nspeed_cntl &= ~LC_FORCE_DIS_SW_SPEED_CHANGE;\r\nWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\r\npci_read_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, &tmp16);\r\ntmp16 &= ~0xf;\r\nif (mask & DRM_PCIE_SPEED_80)\r\ntmp16 |= 3;\r\nelse if (mask & DRM_PCIE_SPEED_50)\r\ntmp16 |= 2;\r\nelse\r\ntmp16 |= 1;\r\npci_write_config_word(rdev->pdev, gpu_pos + PCI_EXP_LNKCTL2, tmp16);\r\nspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\r\nspeed_cntl |= LC_INITIATE_LINK_SPEED_CHANGE;\r\nWREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL, speed_cntl);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nspeed_cntl = RREG32_PCIE_PORT(PCIE_LC_SPEED_CNTL);\r\nif ((speed_cntl & LC_INITIATE_LINK_SPEED_CHANGE) == 0)\r\nbreak;\r\nudelay(1);\r\n}\r\n}\r\nstatic void cik_program_aspm(struct radeon_device *rdev)\r\n{\r\nu32 data, orig;\r\nbool disable_l0s = false, disable_l1 = false, disable_plloff_in_l1 = false;\r\nbool disable_clkreq = false;\r\nif (radeon_aspm == 0)\r\nreturn;\r\nif (rdev->flags & RADEON_IS_IGP)\r\nreturn;\r\nif (!(rdev->flags & RADEON_IS_PCIE))\r\nreturn;\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\r\ndata &= ~LC_XMIT_N_FTS_MASK;\r\ndata |= LC_XMIT_N_FTS(0x24) | LC_XMIT_N_FTS_OVERRIDE_EN;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL, data);\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL3);\r\ndata |= LC_GO_TO_RECOVERY;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL3, data);\r\norig = data = RREG32_PCIE_PORT(PCIE_P_CNTL);\r\ndata |= P_IGNORE_EDB_ERR;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_P_CNTL, data);\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\r\ndata &= ~(LC_L0S_INACTIVITY_MASK | LC_L1_INACTIVITY_MASK);\r\ndata |= LC_PMI_TO_L1_DIS;\r\nif (!disable_l0s)\r\ndata |= LC_L0S_INACTIVITY(7);\r\nif (!disable_l1) {\r\ndata |= LC_L1_INACTIVITY(7);\r\ndata &= ~LC_PMI_TO_L1_DIS;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\r\nif (!disable_plloff_in_l1) {\r\nbool clk_req_support;\r\norig = data = RREG32_PCIE_PORT(PB0_PIF_PWRDOWN_0);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PB0_PIF_PWRDOWN_0, data);\r\norig = data = RREG32_PCIE_PORT(PB0_PIF_PWRDOWN_1);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PB0_PIF_PWRDOWN_1, data);\r\norig = data = RREG32_PCIE_PORT(PB1_PIF_PWRDOWN_0);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_0_MASK | PLL_POWER_STATE_IN_TXS2_0_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_0(7) | PLL_POWER_STATE_IN_TXS2_0(7);\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PB1_PIF_PWRDOWN_0, data);\r\norig = data = RREG32_PCIE_PORT(PB1_PIF_PWRDOWN_1);\r\ndata &= ~(PLL_POWER_STATE_IN_OFF_1_MASK | PLL_POWER_STATE_IN_TXS2_1_MASK);\r\ndata |= PLL_POWER_STATE_IN_OFF_1(7) | PLL_POWER_STATE_IN_TXS2_1(7);\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PB1_PIF_PWRDOWN_1, data);\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL);\r\ndata &= ~LC_DYN_LANES_PWR_STATE_MASK;\r\ndata |= LC_DYN_LANES_PWR_STATE(3);\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_LINK_WIDTH_CNTL, data);\r\nif (!disable_clkreq &&\r\n!pci_is_root_bus(rdev->pdev->bus)) {\r\nstruct pci_dev *root = rdev->pdev->bus->self;\r\nu32 lnkcap;\r\nclk_req_support = false;\r\npcie_capability_read_dword(root, PCI_EXP_LNKCAP, &lnkcap);\r\nif (lnkcap & PCI_EXP_LNKCAP_CLKPM)\r\nclk_req_support = true;\r\n} else {\r\nclk_req_support = false;\r\n}\r\nif (clk_req_support) {\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL2);\r\ndata |= LC_ALLOW_PDWN_IN_L1 | LC_ALLOW_PDWN_IN_L23;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL2, data);\r\norig = data = RREG32_SMC(THM_CLK_CNTL);\r\ndata &= ~(CMON_CLK_SEL_MASK | TMON_CLK_SEL_MASK);\r\ndata |= CMON_CLK_SEL(1) | TMON_CLK_SEL(1);\r\nif (orig != data)\r\nWREG32_SMC(THM_CLK_CNTL, data);\r\norig = data = RREG32_SMC(MISC_CLK_CTRL);\r\ndata &= ~(DEEP_SLEEP_CLK_SEL_MASK | ZCLK_SEL_MASK);\r\ndata |= DEEP_SLEEP_CLK_SEL(1) | ZCLK_SEL(1);\r\nif (orig != data)\r\nWREG32_SMC(MISC_CLK_CTRL, data);\r\norig = data = RREG32_SMC(CG_CLKPIN_CNTL);\r\ndata &= ~BCLK_AS_XCLK;\r\nif (orig != data)\r\nWREG32_SMC(CG_CLKPIN_CNTL, data);\r\norig = data = RREG32_SMC(CG_CLKPIN_CNTL_2);\r\ndata &= ~FORCE_BIF_REFCLK_EN;\r\nif (orig != data)\r\nWREG32_SMC(CG_CLKPIN_CNTL_2, data);\r\norig = data = RREG32_SMC(MPLL_BYPASSCLK_SEL);\r\ndata &= ~MPLL_CLKOUT_SEL_MASK;\r\ndata |= MPLL_CLKOUT_SEL(4);\r\nif (orig != data)\r\nWREG32_SMC(MPLL_BYPASSCLK_SEL, data);\r\n}\r\n}\r\n} else {\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\r\n}\r\norig = data = RREG32_PCIE_PORT(PCIE_CNTL2);\r\ndata |= SLV_MEM_LS_EN | MST_MEM_LS_EN | REPLAY_MEM_LS_EN;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_CNTL2, data);\r\nif (!disable_l0s) {\r\ndata = RREG32_PCIE_PORT(PCIE_LC_N_FTS_CNTL);\r\nif((data & LC_N_FTS_MASK) == LC_N_FTS_MASK) {\r\ndata = RREG32_PCIE_PORT(PCIE_LC_STATUS1);\r\nif ((data & LC_REVERSE_XMIT) && (data & LC_REVERSE_RCVR)) {\r\norig = data = RREG32_PCIE_PORT(PCIE_LC_CNTL);\r\ndata &= ~LC_L0S_INACTIVITY_MASK;\r\nif (orig != data)\r\nWREG32_PCIE_PORT(PCIE_LC_CNTL, data);\r\n}\r\n}\r\n}\r\n}
