void rxrpc_new_skb(struct sk_buff *skb, enum rxrpc_skb_trace op)\r\n{\r\nconst void *here = __builtin_return_address(0);\r\nint n = atomic_inc_return(select_skb_count(op));\r\ntrace_rxrpc_skb(skb, op, refcount_read(&skb->users), n, here);\r\n}\r\nvoid rxrpc_see_skb(struct sk_buff *skb, enum rxrpc_skb_trace op)\r\n{\r\nconst void *here = __builtin_return_address(0);\r\nif (skb) {\r\nint n = atomic_read(select_skb_count(op));\r\ntrace_rxrpc_skb(skb, op, refcount_read(&skb->users), n, here);\r\n}\r\n}\r\nvoid rxrpc_get_skb(struct sk_buff *skb, enum rxrpc_skb_trace op)\r\n{\r\nconst void *here = __builtin_return_address(0);\r\nint n = atomic_inc_return(select_skb_count(op));\r\ntrace_rxrpc_skb(skb, op, refcount_read(&skb->users), n, here);\r\nskb_get(skb);\r\n}\r\nvoid rxrpc_free_skb(struct sk_buff *skb, enum rxrpc_skb_trace op)\r\n{\r\nconst void *here = __builtin_return_address(0);\r\nif (skb) {\r\nint n;\r\nCHECK_SLAB_OKAY(&skb->users);\r\nn = atomic_dec_return(select_skb_count(op));\r\ntrace_rxrpc_skb(skb, op, refcount_read(&skb->users), n, here);\r\nkfree_skb(skb);\r\n}\r\n}\r\nvoid rxrpc_lose_skb(struct sk_buff *skb, enum rxrpc_skb_trace op)\r\n{\r\nconst void *here = __builtin_return_address(0);\r\nif (skb) {\r\nint n;\r\nCHECK_SLAB_OKAY(&skb->users);\r\nn = atomic_dec_return(select_skb_count(op));\r\ntrace_rxrpc_skb(skb, op, refcount_read(&skb->users), n, here);\r\nkfree_skb(skb);\r\n}\r\n}\r\nvoid rxrpc_purge_queue(struct sk_buff_head *list)\r\n{\r\nconst void *here = __builtin_return_address(0);\r\nstruct sk_buff *skb;\r\nwhile ((skb = skb_dequeue((list))) != NULL) {\r\nint n = atomic_dec_return(select_skb_count(rxrpc_skb_rx_purged));\r\ntrace_rxrpc_skb(skb, rxrpc_skb_rx_purged,\r\nrefcount_read(&skb->users), n, here);\r\nkfree_skb(skb);\r\n}\r\n}
