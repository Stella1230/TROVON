static void queue_flush_work(struct printk_safe_seq_buf *s)\r\n{\r\nif (printk_safe_irq_ready) {\r\nsmp_rmb();\r\nirq_work_queue(&s->work);\r\n}\r\n}\r\ninline void printk_safe_flush_line(const char *text, int len)\r\n{\r\nprintk_deferred("%.*s", len, text);\r\n}\r\nstatic int printk_safe_flush_buffer(const char *start, size_t len)\r\n{\r\nconst char *c, *end;\r\nbool header;\r\nc = start;\r\nend = start + len;\r\nheader = true;\r\nwhile (c < end) {\r\nif (*c == '\n') {\r\nprintk_safe_flush_line(start, c - start + 1);\r\nstart = ++c;\r\nheader = true;\r\ncontinue;\r\n}\r\nif ((c + 1 < end) && printk_get_level(c)) {\r\nif (header) {\r\nc = printk_skip_level(c);\r\ncontinue;\r\n}\r\nprintk_safe_flush_line(start, c - start);\r\nstart = c++;\r\nheader = true;\r\ncontinue;\r\n}\r\nheader = false;\r\nc++;\r\n}\r\nif (start < end && !header) {\r\nstatic const char newline[] = KERN_CONT "\n";\r\nprintk_safe_flush_line(start, end - start);\r\nprintk_safe_flush_line(newline, strlen(newline));\r\n}\r\nreturn len;\r\n}\r\nstatic void report_message_lost(struct printk_safe_seq_buf *s)\r\n{\r\nint lost = atomic_xchg(&s->message_lost, 0);\r\nif (lost)\r\nprintk_deferred("Lost %d message(s)!\n", lost);\r\n}\r\nstatic void __printk_safe_flush(struct irq_work *work)\r\n{\r\nstatic raw_spinlock_t read_lock =\r\n__RAW_SPIN_LOCK_INITIALIZER(read_lock);\r\nstruct printk_safe_seq_buf *s =\r\ncontainer_of(work, struct printk_safe_seq_buf, work);\r\nunsigned long flags;\r\nsize_t len;\r\nint i;\r\nraw_spin_lock_irqsave(&read_lock, flags);\r\ni = 0;\r\nmore:\r\nlen = atomic_read(&s->len);\r\nif ((i && i >= len) || len > sizeof(s->buffer)) {\r\nconst char *msg = "printk_safe_flush: internal error\n";\r\nprintk_safe_flush_line(msg, strlen(msg));\r\nlen = 0;\r\n}\r\nif (!len)\r\ngoto out;\r\nsmp_rmb();\r\ni += printk_safe_flush_buffer(s->buffer + i, len - i);\r\nif (atomic_cmpxchg(&s->len, len, 0) != len)\r\ngoto more;\r\nout:\r\nreport_message_lost(s);\r\nraw_spin_unlock_irqrestore(&read_lock, flags);\r\n}\r\nvoid printk_safe_flush(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\n#ifdef CONFIG_PRINTK_NMI\r\n__printk_safe_flush(&per_cpu(nmi_print_seq, cpu).work);\r\n#endif\r\n__printk_safe_flush(&per_cpu(safe_print_seq, cpu).work);\r\n}\r\n}\r\nvoid printk_safe_flush_on_panic(void)\r\n{\r\nif (in_nmi() && raw_spin_is_locked(&logbuf_lock)) {\r\nif (num_online_cpus() > 1)\r\nreturn;\r\ndebug_locks_off();\r\nraw_spin_lock_init(&logbuf_lock);\r\n}\r\nprintk_safe_flush();\r\n}\r\nvoid printk_nmi_enter(void)\r\n{\r\nif ((this_cpu_read(printk_context) & PRINTK_SAFE_CONTEXT_MASK) &&\r\nraw_spin_is_locked(&logbuf_lock)) {\r\nthis_cpu_or(printk_context, PRINTK_NMI_CONTEXT_MASK);\r\n} else {\r\nthis_cpu_or(printk_context, PRINTK_NMI_DEFERRED_CONTEXT_MASK);\r\n}\r\n}\r\nvoid printk_nmi_exit(void)\r\n{\r\nthis_cpu_and(printk_context,\r\n~(PRINTK_NMI_CONTEXT_MASK |\r\nPRINTK_NMI_DEFERRED_CONTEXT_MASK));\r\n}\r\nvoid __printk_safe_enter(void)\r\n{\r\nthis_cpu_inc(printk_context);\r\n}\r\nvoid __printk_safe_exit(void)\r\n{\r\nthis_cpu_dec(printk_context);\r\n}\r\nint vprintk_func(const char *fmt, va_list args)\r\n{\r\nif (this_cpu_read(printk_context) & PRINTK_NMI_CONTEXT_MASK)\r\nreturn vprintk_nmi(fmt, args);\r\nif (this_cpu_read(printk_context) & PRINTK_SAFE_CONTEXT_MASK)\r\nreturn vprintk_safe(fmt, args);\r\nif (this_cpu_read(printk_context) & PRINTK_NMI_DEFERRED_CONTEXT_MASK)\r\nreturn vprintk_deferred(fmt, args);\r\nreturn vprintk_default(fmt, args);\r\n}\r\nvoid __init printk_safe_init(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct printk_safe_seq_buf *s;\r\ns = &per_cpu(safe_print_seq, cpu);\r\ninit_irq_work(&s->work, __printk_safe_flush);\r\n#ifdef CONFIG_PRINTK_NMI\r\ns = &per_cpu(nmi_print_seq, cpu);\r\ninit_irq_work(&s->work, __printk_safe_flush);\r\n#endif\r\n}\r\nsmp_wmb();\r\nprintk_safe_irq_ready = 1;\r\nprintk_safe_flush();\r\n}
