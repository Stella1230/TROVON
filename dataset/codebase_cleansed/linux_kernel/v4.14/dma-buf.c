static int dma_buf_release(struct inode *inode, struct file *file)\r\n{\r\nstruct dma_buf *dmabuf;\r\nif (!is_dma_buf_file(file))\r\nreturn -EINVAL;\r\ndmabuf = file->private_data;\r\nBUG_ON(dmabuf->vmapping_counter);\r\nBUG_ON(dmabuf->cb_shared.active || dmabuf->cb_excl.active);\r\ndmabuf->ops->release(dmabuf);\r\nmutex_lock(&db_list.lock);\r\nlist_del(&dmabuf->list_node);\r\nmutex_unlock(&db_list.lock);\r\nif (dmabuf->resv == (struct reservation_object *)&dmabuf[1])\r\nreservation_object_fini(dmabuf->resv);\r\nmodule_put(dmabuf->owner);\r\nkfree(dmabuf);\r\nreturn 0;\r\n}\r\nstatic int dma_buf_mmap_internal(struct file *file, struct vm_area_struct *vma)\r\n{\r\nstruct dma_buf *dmabuf;\r\nif (!is_dma_buf_file(file))\r\nreturn -EINVAL;\r\ndmabuf = file->private_data;\r\nif (vma->vm_pgoff + vma_pages(vma) >\r\ndmabuf->size >> PAGE_SHIFT)\r\nreturn -EINVAL;\r\nreturn dmabuf->ops->mmap(dmabuf, vma);\r\n}\r\nstatic loff_t dma_buf_llseek(struct file *file, loff_t offset, int whence)\r\n{\r\nstruct dma_buf *dmabuf;\r\nloff_t base;\r\nif (!is_dma_buf_file(file))\r\nreturn -EBADF;\r\ndmabuf = file->private_data;\r\nif (whence == SEEK_END)\r\nbase = dmabuf->size;\r\nelse if (whence == SEEK_SET)\r\nbase = 0;\r\nelse\r\nreturn -EINVAL;\r\nif (offset != 0)\r\nreturn -EINVAL;\r\nreturn base + offset;\r\n}\r\nstatic void dma_buf_poll_cb(struct dma_fence *fence, struct dma_fence_cb *cb)\r\n{\r\nstruct dma_buf_poll_cb_t *dcb = (struct dma_buf_poll_cb_t *)cb;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dcb->poll->lock, flags);\r\nwake_up_locked_poll(dcb->poll, dcb->active);\r\ndcb->active = 0;\r\nspin_unlock_irqrestore(&dcb->poll->lock, flags);\r\n}\r\nstatic unsigned int dma_buf_poll(struct file *file, poll_table *poll)\r\n{\r\nstruct dma_buf *dmabuf;\r\nstruct reservation_object *resv;\r\nstruct reservation_object_list *fobj;\r\nstruct dma_fence *fence_excl;\r\nunsigned long events;\r\nunsigned shared_count, seq;\r\ndmabuf = file->private_data;\r\nif (!dmabuf || !dmabuf->resv)\r\nreturn POLLERR;\r\nresv = dmabuf->resv;\r\npoll_wait(file, &dmabuf->poll, poll);\r\nevents = poll_requested_events(poll) & (POLLIN | POLLOUT);\r\nif (!events)\r\nreturn 0;\r\nretry:\r\nseq = read_seqcount_begin(&resv->seq);\r\nrcu_read_lock();\r\nfobj = rcu_dereference(resv->fence);\r\nif (fobj)\r\nshared_count = fobj->shared_count;\r\nelse\r\nshared_count = 0;\r\nfence_excl = rcu_dereference(resv->fence_excl);\r\nif (read_seqcount_retry(&resv->seq, seq)) {\r\nrcu_read_unlock();\r\ngoto retry;\r\n}\r\nif (fence_excl && (!(events & POLLOUT) || shared_count == 0)) {\r\nstruct dma_buf_poll_cb_t *dcb = &dmabuf->cb_excl;\r\nunsigned long pevents = POLLIN;\r\nif (shared_count == 0)\r\npevents |= POLLOUT;\r\nspin_lock_irq(&dmabuf->poll.lock);\r\nif (dcb->active) {\r\ndcb->active |= pevents;\r\nevents &= ~pevents;\r\n} else\r\ndcb->active = pevents;\r\nspin_unlock_irq(&dmabuf->poll.lock);\r\nif (events & pevents) {\r\nif (!dma_fence_get_rcu(fence_excl)) {\r\nevents &= ~pevents;\r\ndma_buf_poll_cb(NULL, &dcb->cb);\r\n} else if (!dma_fence_add_callback(fence_excl, &dcb->cb,\r\ndma_buf_poll_cb)) {\r\nevents &= ~pevents;\r\ndma_fence_put(fence_excl);\r\n} else {\r\ndma_fence_put(fence_excl);\r\ndma_buf_poll_cb(NULL, &dcb->cb);\r\n}\r\n}\r\n}\r\nif ((events & POLLOUT) && shared_count > 0) {\r\nstruct dma_buf_poll_cb_t *dcb = &dmabuf->cb_shared;\r\nint i;\r\nspin_lock_irq(&dmabuf->poll.lock);\r\nif (dcb->active)\r\nevents &= ~POLLOUT;\r\nelse\r\ndcb->active = POLLOUT;\r\nspin_unlock_irq(&dmabuf->poll.lock);\r\nif (!(events & POLLOUT))\r\ngoto out;\r\nfor (i = 0; i < shared_count; ++i) {\r\nstruct dma_fence *fence = rcu_dereference(fobj->shared[i]);\r\nif (!dma_fence_get_rcu(fence)) {\r\nevents &= ~POLLOUT;\r\ndma_buf_poll_cb(NULL, &dcb->cb);\r\nbreak;\r\n}\r\nif (!dma_fence_add_callback(fence, &dcb->cb,\r\ndma_buf_poll_cb)) {\r\ndma_fence_put(fence);\r\nevents &= ~POLLOUT;\r\nbreak;\r\n}\r\ndma_fence_put(fence);\r\n}\r\nif (i == shared_count)\r\ndma_buf_poll_cb(NULL, &dcb->cb);\r\n}\r\nout:\r\nrcu_read_unlock();\r\nreturn events;\r\n}\r\nstatic long dma_buf_ioctl(struct file *file,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nstruct dma_buf *dmabuf;\r\nstruct dma_buf_sync sync;\r\nenum dma_data_direction direction;\r\nint ret;\r\ndmabuf = file->private_data;\r\nswitch (cmd) {\r\ncase DMA_BUF_IOCTL_SYNC:\r\nif (copy_from_user(&sync, (void __user *) arg, sizeof(sync)))\r\nreturn -EFAULT;\r\nif (sync.flags & ~DMA_BUF_SYNC_VALID_FLAGS_MASK)\r\nreturn -EINVAL;\r\nswitch (sync.flags & DMA_BUF_SYNC_RW) {\r\ncase DMA_BUF_SYNC_READ:\r\ndirection = DMA_FROM_DEVICE;\r\nbreak;\r\ncase DMA_BUF_SYNC_WRITE:\r\ndirection = DMA_TO_DEVICE;\r\nbreak;\r\ncase DMA_BUF_SYNC_RW:\r\ndirection = DMA_BIDIRECTIONAL;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (sync.flags & DMA_BUF_SYNC_END)\r\nret = dma_buf_end_cpu_access(dmabuf, direction);\r\nelse\r\nret = dma_buf_begin_cpu_access(dmabuf, direction);\r\nreturn ret;\r\ndefault:\r\nreturn -ENOTTY;\r\n}\r\n}\r\nstatic inline int is_dma_buf_file(struct file *file)\r\n{\r\nreturn file->f_op == &dma_buf_fops;\r\n}\r\nstruct dma_buf *dma_buf_export(const struct dma_buf_export_info *exp_info)\r\n{\r\nstruct dma_buf *dmabuf;\r\nstruct reservation_object *resv = exp_info->resv;\r\nstruct file *file;\r\nsize_t alloc_size = sizeof(struct dma_buf);\r\nint ret;\r\nif (!exp_info->resv)\r\nalloc_size += sizeof(struct reservation_object);\r\nelse\r\nalloc_size += 1;\r\nif (WARN_ON(!exp_info->priv\r\n|| !exp_info->ops\r\n|| !exp_info->ops->map_dma_buf\r\n|| !exp_info->ops->unmap_dma_buf\r\n|| !exp_info->ops->release\r\n|| !exp_info->ops->map_atomic\r\n|| !exp_info->ops->map\r\n|| !exp_info->ops->mmap)) {\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nif (!try_module_get(exp_info->owner))\r\nreturn ERR_PTR(-ENOENT);\r\ndmabuf = kzalloc(alloc_size, GFP_KERNEL);\r\nif (!dmabuf) {\r\nret = -ENOMEM;\r\ngoto err_module;\r\n}\r\ndmabuf->priv = exp_info->priv;\r\ndmabuf->ops = exp_info->ops;\r\ndmabuf->size = exp_info->size;\r\ndmabuf->exp_name = exp_info->exp_name;\r\ndmabuf->owner = exp_info->owner;\r\ninit_waitqueue_head(&dmabuf->poll);\r\ndmabuf->cb_excl.poll = dmabuf->cb_shared.poll = &dmabuf->poll;\r\ndmabuf->cb_excl.active = dmabuf->cb_shared.active = 0;\r\nif (!resv) {\r\nresv = (struct reservation_object *)&dmabuf[1];\r\nreservation_object_init(resv);\r\n}\r\ndmabuf->resv = resv;\r\nfile = anon_inode_getfile("dmabuf", &dma_buf_fops, dmabuf,\r\nexp_info->flags);\r\nif (IS_ERR(file)) {\r\nret = PTR_ERR(file);\r\ngoto err_dmabuf;\r\n}\r\nfile->f_mode |= FMODE_LSEEK;\r\ndmabuf->file = file;\r\nmutex_init(&dmabuf->lock);\r\nINIT_LIST_HEAD(&dmabuf->attachments);\r\nmutex_lock(&db_list.lock);\r\nlist_add(&dmabuf->list_node, &db_list.head);\r\nmutex_unlock(&db_list.lock);\r\nreturn dmabuf;\r\nerr_dmabuf:\r\nkfree(dmabuf);\r\nerr_module:\r\nmodule_put(exp_info->owner);\r\nreturn ERR_PTR(ret);\r\n}\r\nint dma_buf_fd(struct dma_buf *dmabuf, int flags)\r\n{\r\nint fd;\r\nif (!dmabuf || !dmabuf->file)\r\nreturn -EINVAL;\r\nfd = get_unused_fd_flags(flags);\r\nif (fd < 0)\r\nreturn fd;\r\nfd_install(fd, dmabuf->file);\r\nreturn fd;\r\n}\r\nstruct dma_buf *dma_buf_get(int fd)\r\n{\r\nstruct file *file;\r\nfile = fget(fd);\r\nif (!file)\r\nreturn ERR_PTR(-EBADF);\r\nif (!is_dma_buf_file(file)) {\r\nfput(file);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nreturn file->private_data;\r\n}\r\nvoid dma_buf_put(struct dma_buf *dmabuf)\r\n{\r\nif (WARN_ON(!dmabuf || !dmabuf->file))\r\nreturn;\r\nfput(dmabuf->file);\r\n}\r\nstruct dma_buf_attachment *dma_buf_attach(struct dma_buf *dmabuf,\r\nstruct device *dev)\r\n{\r\nstruct dma_buf_attachment *attach;\r\nint ret;\r\nif (WARN_ON(!dmabuf || !dev))\r\nreturn ERR_PTR(-EINVAL);\r\nattach = kzalloc(sizeof(*attach), GFP_KERNEL);\r\nif (!attach)\r\nreturn ERR_PTR(-ENOMEM);\r\nattach->dev = dev;\r\nattach->dmabuf = dmabuf;\r\nmutex_lock(&dmabuf->lock);\r\nif (dmabuf->ops->attach) {\r\nret = dmabuf->ops->attach(dmabuf, dev, attach);\r\nif (ret)\r\ngoto err_attach;\r\n}\r\nlist_add(&attach->node, &dmabuf->attachments);\r\nmutex_unlock(&dmabuf->lock);\r\nreturn attach;\r\nerr_attach:\r\nkfree(attach);\r\nmutex_unlock(&dmabuf->lock);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid dma_buf_detach(struct dma_buf *dmabuf, struct dma_buf_attachment *attach)\r\n{\r\nif (WARN_ON(!dmabuf || !attach))\r\nreturn;\r\nmutex_lock(&dmabuf->lock);\r\nlist_del(&attach->node);\r\nif (dmabuf->ops->detach)\r\ndmabuf->ops->detach(dmabuf, attach);\r\nmutex_unlock(&dmabuf->lock);\r\nkfree(attach);\r\n}\r\nstruct sg_table *dma_buf_map_attachment(struct dma_buf_attachment *attach,\r\nenum dma_data_direction direction)\r\n{\r\nstruct sg_table *sg_table = ERR_PTR(-EINVAL);\r\nmight_sleep();\r\nif (WARN_ON(!attach || !attach->dmabuf))\r\nreturn ERR_PTR(-EINVAL);\r\nsg_table = attach->dmabuf->ops->map_dma_buf(attach, direction);\r\nif (!sg_table)\r\nsg_table = ERR_PTR(-ENOMEM);\r\nreturn sg_table;\r\n}\r\nvoid dma_buf_unmap_attachment(struct dma_buf_attachment *attach,\r\nstruct sg_table *sg_table,\r\nenum dma_data_direction direction)\r\n{\r\nmight_sleep();\r\nif (WARN_ON(!attach || !attach->dmabuf || !sg_table))\r\nreturn;\r\nattach->dmabuf->ops->unmap_dma_buf(attach, sg_table,\r\ndirection);\r\n}\r\nstatic int __dma_buf_begin_cpu_access(struct dma_buf *dmabuf,\r\nenum dma_data_direction direction)\r\n{\r\nbool write = (direction == DMA_BIDIRECTIONAL ||\r\ndirection == DMA_TO_DEVICE);\r\nstruct reservation_object *resv = dmabuf->resv;\r\nlong ret;\r\nret = reservation_object_wait_timeout_rcu(resv, write, true,\r\nMAX_SCHEDULE_TIMEOUT);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nint dma_buf_begin_cpu_access(struct dma_buf *dmabuf,\r\nenum dma_data_direction direction)\r\n{\r\nint ret = 0;\r\nif (WARN_ON(!dmabuf))\r\nreturn -EINVAL;\r\nif (dmabuf->ops->begin_cpu_access)\r\nret = dmabuf->ops->begin_cpu_access(dmabuf, direction);\r\nif (ret == 0)\r\nret = __dma_buf_begin_cpu_access(dmabuf, direction);\r\nreturn ret;\r\n}\r\nint dma_buf_end_cpu_access(struct dma_buf *dmabuf,\r\nenum dma_data_direction direction)\r\n{\r\nint ret = 0;\r\nWARN_ON(!dmabuf);\r\nif (dmabuf->ops->end_cpu_access)\r\nret = dmabuf->ops->end_cpu_access(dmabuf, direction);\r\nreturn ret;\r\n}\r\nvoid *dma_buf_kmap_atomic(struct dma_buf *dmabuf, unsigned long page_num)\r\n{\r\nWARN_ON(!dmabuf);\r\nreturn dmabuf->ops->map_atomic(dmabuf, page_num);\r\n}\r\nvoid dma_buf_kunmap_atomic(struct dma_buf *dmabuf, unsigned long page_num,\r\nvoid *vaddr)\r\n{\r\nWARN_ON(!dmabuf);\r\nif (dmabuf->ops->unmap_atomic)\r\ndmabuf->ops->unmap_atomic(dmabuf, page_num, vaddr);\r\n}\r\nvoid *dma_buf_kmap(struct dma_buf *dmabuf, unsigned long page_num)\r\n{\r\nWARN_ON(!dmabuf);\r\nreturn dmabuf->ops->map(dmabuf, page_num);\r\n}\r\nvoid dma_buf_kunmap(struct dma_buf *dmabuf, unsigned long page_num,\r\nvoid *vaddr)\r\n{\r\nWARN_ON(!dmabuf);\r\nif (dmabuf->ops->unmap)\r\ndmabuf->ops->unmap(dmabuf, page_num, vaddr);\r\n}\r\nint dma_buf_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma,\r\nunsigned long pgoff)\r\n{\r\nstruct file *oldfile;\r\nint ret;\r\nif (WARN_ON(!dmabuf || !vma))\r\nreturn -EINVAL;\r\nif (pgoff + vma_pages(vma) < pgoff)\r\nreturn -EOVERFLOW;\r\nif (pgoff + vma_pages(vma) >\r\ndmabuf->size >> PAGE_SHIFT)\r\nreturn -EINVAL;\r\nget_file(dmabuf->file);\r\noldfile = vma->vm_file;\r\nvma->vm_file = dmabuf->file;\r\nvma->vm_pgoff = pgoff;\r\nret = dmabuf->ops->mmap(dmabuf, vma);\r\nif (ret) {\r\nvma->vm_file = oldfile;\r\nfput(dmabuf->file);\r\n} else {\r\nif (oldfile)\r\nfput(oldfile);\r\n}\r\nreturn ret;\r\n}\r\nvoid *dma_buf_vmap(struct dma_buf *dmabuf)\r\n{\r\nvoid *ptr;\r\nif (WARN_ON(!dmabuf))\r\nreturn NULL;\r\nif (!dmabuf->ops->vmap)\r\nreturn NULL;\r\nmutex_lock(&dmabuf->lock);\r\nif (dmabuf->vmapping_counter) {\r\ndmabuf->vmapping_counter++;\r\nBUG_ON(!dmabuf->vmap_ptr);\r\nptr = dmabuf->vmap_ptr;\r\ngoto out_unlock;\r\n}\r\nBUG_ON(dmabuf->vmap_ptr);\r\nptr = dmabuf->ops->vmap(dmabuf);\r\nif (WARN_ON_ONCE(IS_ERR(ptr)))\r\nptr = NULL;\r\nif (!ptr)\r\ngoto out_unlock;\r\ndmabuf->vmap_ptr = ptr;\r\ndmabuf->vmapping_counter = 1;\r\nout_unlock:\r\nmutex_unlock(&dmabuf->lock);\r\nreturn ptr;\r\n}\r\nvoid dma_buf_vunmap(struct dma_buf *dmabuf, void *vaddr)\r\n{\r\nif (WARN_ON(!dmabuf))\r\nreturn;\r\nBUG_ON(!dmabuf->vmap_ptr);\r\nBUG_ON(dmabuf->vmapping_counter == 0);\r\nBUG_ON(dmabuf->vmap_ptr != vaddr);\r\nmutex_lock(&dmabuf->lock);\r\nif (--dmabuf->vmapping_counter == 0) {\r\nif (dmabuf->ops->vunmap)\r\ndmabuf->ops->vunmap(dmabuf, vaddr);\r\ndmabuf->vmap_ptr = NULL;\r\n}\r\nmutex_unlock(&dmabuf->lock);\r\n}\r\nstatic int dma_buf_debug_show(struct seq_file *s, void *unused)\r\n{\r\nint ret;\r\nstruct dma_buf *buf_obj;\r\nstruct dma_buf_attachment *attach_obj;\r\nstruct reservation_object *robj;\r\nstruct reservation_object_list *fobj;\r\nstruct dma_fence *fence;\r\nunsigned seq;\r\nint count = 0, attach_count, shared_count, i;\r\nsize_t size = 0;\r\nret = mutex_lock_interruptible(&db_list.lock);\r\nif (ret)\r\nreturn ret;\r\nseq_puts(s, "\nDma-buf Objects:\n");\r\nseq_printf(s, "%-8s\t%-8s\t%-8s\t%-8s\texp_name\n",\r\n"size", "flags", "mode", "count");\r\nlist_for_each_entry(buf_obj, &db_list.head, list_node) {\r\nret = mutex_lock_interruptible(&buf_obj->lock);\r\nif (ret) {\r\nseq_puts(s,\r\n"\tERROR locking buffer object: skipping\n");\r\ncontinue;\r\n}\r\nseq_printf(s, "%08zu\t%08x\t%08x\t%08ld\t%s\n",\r\nbuf_obj->size,\r\nbuf_obj->file->f_flags, buf_obj->file->f_mode,\r\nfile_count(buf_obj->file),\r\nbuf_obj->exp_name);\r\nrobj = buf_obj->resv;\r\nwhile (true) {\r\nseq = read_seqcount_begin(&robj->seq);\r\nrcu_read_lock();\r\nfobj = rcu_dereference(robj->fence);\r\nshared_count = fobj ? fobj->shared_count : 0;\r\nfence = rcu_dereference(robj->fence_excl);\r\nif (!read_seqcount_retry(&robj->seq, seq))\r\nbreak;\r\nrcu_read_unlock();\r\n}\r\nif (fence)\r\nseq_printf(s, "\tExclusive fence: %s %s %ssignalled\n",\r\nfence->ops->get_driver_name(fence),\r\nfence->ops->get_timeline_name(fence),\r\ndma_fence_is_signaled(fence) ? "" : "un");\r\nfor (i = 0; i < shared_count; i++) {\r\nfence = rcu_dereference(fobj->shared[i]);\r\nif (!dma_fence_get_rcu(fence))\r\ncontinue;\r\nseq_printf(s, "\tShared fence: %s %s %ssignalled\n",\r\nfence->ops->get_driver_name(fence),\r\nfence->ops->get_timeline_name(fence),\r\ndma_fence_is_signaled(fence) ? "" : "un");\r\n}\r\nrcu_read_unlock();\r\nseq_puts(s, "\tAttached Devices:\n");\r\nattach_count = 0;\r\nlist_for_each_entry(attach_obj, &buf_obj->attachments, node) {\r\nseq_printf(s, "\t%s\n", dev_name(attach_obj->dev));\r\nattach_count++;\r\n}\r\nseq_printf(s, "Total %d devices attached\n\n",\r\nattach_count);\r\ncount++;\r\nsize += buf_obj->size;\r\nmutex_unlock(&buf_obj->lock);\r\n}\r\nseq_printf(s, "\nTotal %d objects, %zu bytes\n", count, size);\r\nmutex_unlock(&db_list.lock);\r\nreturn 0;\r\n}\r\nstatic int dma_buf_debug_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, dma_buf_debug_show, NULL);\r\n}\r\nstatic int dma_buf_init_debugfs(void)\r\n{\r\nstruct dentry *d;\r\nint err = 0;\r\nd = debugfs_create_dir("dma_buf", NULL);\r\nif (IS_ERR(d))\r\nreturn PTR_ERR(d);\r\ndma_buf_debugfs_dir = d;\r\nd = debugfs_create_file("bufinfo", S_IRUGO, dma_buf_debugfs_dir,\r\nNULL, &dma_buf_debug_fops);\r\nif (IS_ERR(d)) {\r\npr_debug("dma_buf: debugfs: failed to create node bufinfo\n");\r\ndebugfs_remove_recursive(dma_buf_debugfs_dir);\r\ndma_buf_debugfs_dir = NULL;\r\nerr = PTR_ERR(d);\r\n}\r\nreturn err;\r\n}\r\nstatic void dma_buf_uninit_debugfs(void)\r\n{\r\nif (dma_buf_debugfs_dir)\r\ndebugfs_remove_recursive(dma_buf_debugfs_dir);\r\n}\r\nstatic inline int dma_buf_init_debugfs(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void dma_buf_uninit_debugfs(void)\r\n{\r\n}\r\nstatic int __init dma_buf_init(void)\r\n{\r\nmutex_init(&db_list.lock);\r\nINIT_LIST_HEAD(&db_list.head);\r\ndma_buf_init_debugfs();\r\nreturn 0;\r\n}\r\nstatic void __exit dma_buf_deinit(void)\r\n{\r\ndma_buf_uninit_debugfs();\r\n}
