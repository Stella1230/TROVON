int nx_hcall_sync(struct nx_crypto_ctx *nx_ctx,\r\nstruct vio_pfo_op *op,\r\nu32 may_sleep)\r\n{\r\nint rc, retries = 10;\r\nstruct vio_dev *viodev = nx_driver.viodev;\r\natomic_inc(&(nx_ctx->stats->sync_ops));\r\ndo {\r\nrc = vio_h_cop_sync(viodev, op);\r\n} while (rc == -EBUSY && !may_sleep && retries--);\r\nif (rc) {\r\ndev_dbg(&viodev->dev, "vio_h_cop_sync failed: rc: %d "\r\n"hcall rc: %ld\n", rc, op->hcall_err);\r\natomic_inc(&(nx_ctx->stats->errors));\r\natomic_set(&(nx_ctx->stats->last_error), op->hcall_err);\r\natomic_set(&(nx_ctx->stats->last_error_pid), current->pid);\r\n}\r\nreturn rc;\r\n}\r\nstruct nx_sg *nx_build_sg_list(struct nx_sg *sg_head,\r\nu8 *start_addr,\r\nunsigned int *len,\r\nu32 sgmax)\r\n{\r\nunsigned int sg_len = 0;\r\nstruct nx_sg *sg;\r\nu64 sg_addr = (u64)start_addr;\r\nu64 end_addr;\r\nif (is_vmalloc_addr(start_addr))\r\nsg_addr = page_to_phys(vmalloc_to_page(start_addr))\r\n+ offset_in_page(sg_addr);\r\nelse\r\nsg_addr = __pa(sg_addr);\r\nend_addr = sg_addr + *len;\r\nfor (sg = sg_head; sg_len < *len; sg++) {\r\nu64 next_page;\r\nsg->addr = sg_addr;\r\nsg_addr = min_t(u64, NX_PAGE_NUM(sg_addr + NX_PAGE_SIZE),\r\nend_addr);\r\nnext_page = (sg->addr & PAGE_MASK) + PAGE_SIZE;\r\nsg->len = min_t(u64, sg_addr, next_page) - sg->addr;\r\nsg_len += sg->len;\r\nif (sg_addr >= next_page &&\r\nis_vmalloc_addr(start_addr + sg_len)) {\r\nsg_addr = page_to_phys(vmalloc_to_page(\r\nstart_addr + sg_len));\r\nend_addr = sg_addr + *len - sg_len;\r\n}\r\nif ((sg - sg_head) == sgmax) {\r\npr_err("nx: scatter/gather list overflow, pid: %d\n",\r\ncurrent->pid);\r\nsg++;\r\nbreak;\r\n}\r\n}\r\n*len = sg_len;\r\nreturn sg;\r\n}\r\nstruct nx_sg *nx_walk_and_build(struct nx_sg *nx_dst,\r\nunsigned int sglen,\r\nstruct scatterlist *sg_src,\r\nunsigned int start,\r\nunsigned int *src_len)\r\n{\r\nstruct scatter_walk walk;\r\nstruct nx_sg *nx_sg = nx_dst;\r\nunsigned int n, offset = 0, len = *src_len;\r\nchar *dst;\r\nfor (;;) {\r\nscatterwalk_start(&walk, sg_src);\r\nif (start < offset + sg_src->length)\r\nbreak;\r\noffset += sg_src->length;\r\nsg_src = sg_next(sg_src);\r\n}\r\nscatterwalk_advance(&walk, start - offset);\r\nwhile (len && (nx_sg - nx_dst) < sglen) {\r\nn = scatterwalk_clamp(&walk, len);\r\nif (!n) {\r\nscatterwalk_start(&walk, sg_next(walk.sg));\r\nn = scatterwalk_clamp(&walk, len);\r\n}\r\ndst = scatterwalk_map(&walk);\r\nnx_sg = nx_build_sg_list(nx_sg, dst, &n, sglen - (nx_sg - nx_dst));\r\nlen -= n;\r\nscatterwalk_unmap(dst);\r\nscatterwalk_advance(&walk, n);\r\nscatterwalk_done(&walk, SCATTERWALK_FROM_SG, len);\r\n}\r\n*src_len -= len;\r\nreturn nx_sg;\r\n}\r\nstatic long int trim_sg_list(struct nx_sg *sg,\r\nstruct nx_sg *end,\r\nunsigned int delta,\r\nunsigned int *nbytes)\r\n{\r\nlong int oplen;\r\nlong int data_back;\r\nunsigned int is_delta = delta;\r\nwhile (delta && end > sg) {\r\nstruct nx_sg *last = end - 1;\r\nif (last->len > delta) {\r\nlast->len -= delta;\r\ndelta = 0;\r\n} else {\r\nend--;\r\ndelta -= last->len;\r\n}\r\n}\r\noplen = (sg - end) * sizeof(struct nx_sg);\r\nif (is_delta) {\r\ndata_back = (abs(oplen) / AES_BLOCK_SIZE) * sg->len;\r\ndata_back = *nbytes - (data_back & ~(AES_BLOCK_SIZE - 1));\r\n*nbytes -= data_back;\r\n}\r\nreturn oplen;\r\n}\r\nint nx_build_sg_lists(struct nx_crypto_ctx *nx_ctx,\r\nstruct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src,\r\nunsigned int *nbytes,\r\nunsigned int offset,\r\nu8 *iv)\r\n{\r\nunsigned int delta = 0;\r\nunsigned int total = *nbytes;\r\nstruct nx_sg *nx_insg = nx_ctx->in_sg;\r\nstruct nx_sg *nx_outsg = nx_ctx->out_sg;\r\nunsigned int max_sg_len;\r\nmax_sg_len = min_t(u64, nx_ctx->ap->sglen,\r\nnx_driver.of.max_sg_len/sizeof(struct nx_sg));\r\nmax_sg_len = min_t(u64, max_sg_len,\r\nnx_ctx->ap->databytelen/NX_PAGE_SIZE);\r\nif (iv)\r\nmemcpy(iv, desc->info, AES_BLOCK_SIZE);\r\n*nbytes = min_t(u64, *nbytes, nx_ctx->ap->databytelen);\r\nnx_outsg = nx_walk_and_build(nx_outsg, max_sg_len, dst,\r\noffset, nbytes);\r\nnx_insg = nx_walk_and_build(nx_insg, max_sg_len, src,\r\noffset, nbytes);\r\nif (*nbytes < total)\r\ndelta = *nbytes - (*nbytes & ~(AES_BLOCK_SIZE - 1));\r\nnx_ctx->op.inlen = trim_sg_list(nx_ctx->in_sg, nx_insg, delta, nbytes);\r\nnx_ctx->op.outlen = trim_sg_list(nx_ctx->out_sg, nx_outsg, delta, nbytes);\r\nreturn 0;\r\n}\r\nvoid nx_ctx_init(struct nx_crypto_ctx *nx_ctx, unsigned int function)\r\n{\r\nspin_lock_init(&nx_ctx->lock);\r\nmemset(nx_ctx->kmem, 0, nx_ctx->kmem_len);\r\nnx_ctx->csbcpb->csb.valid |= NX_CSB_VALID_BIT;\r\nnx_ctx->op.flags = function;\r\nnx_ctx->op.csbcpb = __pa(nx_ctx->csbcpb);\r\nnx_ctx->op.in = __pa(nx_ctx->in_sg);\r\nnx_ctx->op.out = __pa(nx_ctx->out_sg);\r\nif (nx_ctx->csbcpb_aead) {\r\nnx_ctx->csbcpb_aead->csb.valid |= NX_CSB_VALID_BIT;\r\nnx_ctx->op_aead.flags = function;\r\nnx_ctx->op_aead.csbcpb = __pa(nx_ctx->csbcpb_aead);\r\nnx_ctx->op_aead.in = __pa(nx_ctx->in_sg);\r\nnx_ctx->op_aead.out = __pa(nx_ctx->out_sg);\r\n}\r\n}\r\nstatic void nx_of_update_status(struct device *dev,\r\nstruct property *p,\r\nstruct nx_of *props)\r\n{\r\nif (!strncmp(p->value, "okay", p->length)) {\r\nprops->status = NX_WAITING;\r\nprops->flags |= NX_OF_FLAG_STATUS_SET;\r\n} else {\r\ndev_info(dev, "%s: status '%s' is not 'okay'\n", __func__,\r\n(char *)p->value);\r\n}\r\n}\r\nstatic void nx_of_update_sglen(struct device *dev,\r\nstruct property *p,\r\nstruct nx_of *props)\r\n{\r\nif (p->length != sizeof(props->max_sg_len)) {\r\ndev_err(dev, "%s: unexpected format for "\r\n"ibm,max-sg-len property\n", __func__);\r\ndev_dbg(dev, "%s: ibm,max-sg-len is %d bytes "\r\n"long, expected %zd bytes\n", __func__,\r\np->length, sizeof(props->max_sg_len));\r\nreturn;\r\n}\r\nprops->max_sg_len = *(u32 *)p->value;\r\nprops->flags |= NX_OF_FLAG_MAXSGLEN_SET;\r\n}\r\nstatic void nx_of_update_msc(struct device *dev,\r\nstruct property *p,\r\nstruct nx_of *props)\r\n{\r\nstruct msc_triplet *trip;\r\nstruct max_sync_cop *msc;\r\nunsigned int bytes_so_far, i, lenp;\r\nmsc = (struct max_sync_cop *)p->value;\r\nlenp = p->length;\r\nbytes_so_far = 0;\r\nwhile ((bytes_so_far + sizeof(struct max_sync_cop)) <= lenp) {\r\nbytes_so_far += sizeof(struct max_sync_cop);\r\ntrip = msc->trip;\r\nfor (i = 0;\r\n((bytes_so_far + sizeof(struct msc_triplet)) <= lenp) &&\r\ni < msc->triplets;\r\ni++) {\r\nif (msc->fc >= NX_MAX_FC || msc->mode >= NX_MAX_MODE) {\r\ndev_err(dev, "unknown function code/mode "\r\n"combo: %d/%d (ignored)\n", msc->fc,\r\nmsc->mode);\r\ngoto next_loop;\r\n}\r\nif (!trip->sglen || trip->databytelen < NX_PAGE_SIZE) {\r\ndev_warn(dev, "bogus sglen/databytelen: "\r\n"%u/%u (ignored)\n", trip->sglen,\r\ntrip->databytelen);\r\ngoto next_loop;\r\n}\r\nswitch (trip->keybitlen) {\r\ncase 128:\r\ncase 160:\r\nprops->ap[msc->fc][msc->mode][0].databytelen =\r\ntrip->databytelen;\r\nprops->ap[msc->fc][msc->mode][0].sglen =\r\ntrip->sglen;\r\nbreak;\r\ncase 192:\r\nprops->ap[msc->fc][msc->mode][1].databytelen =\r\ntrip->databytelen;\r\nprops->ap[msc->fc][msc->mode][1].sglen =\r\ntrip->sglen;\r\nbreak;\r\ncase 256:\r\nif (msc->fc == NX_FC_AES) {\r\nprops->ap[msc->fc][msc->mode][2].\r\ndatabytelen = trip->databytelen;\r\nprops->ap[msc->fc][msc->mode][2].sglen =\r\ntrip->sglen;\r\n} else if (msc->fc == NX_FC_AES_HMAC ||\r\nmsc->fc == NX_FC_SHA) {\r\nprops->ap[msc->fc][msc->mode][1].\r\ndatabytelen = trip->databytelen;\r\nprops->ap[msc->fc][msc->mode][1].sglen =\r\ntrip->sglen;\r\n} else {\r\ndev_warn(dev, "unknown function "\r\n"code/key bit len combo"\r\n": (%u/256)\n", msc->fc);\r\n}\r\nbreak;\r\ncase 512:\r\nprops->ap[msc->fc][msc->mode][2].databytelen =\r\ntrip->databytelen;\r\nprops->ap[msc->fc][msc->mode][2].sglen =\r\ntrip->sglen;\r\nbreak;\r\ndefault:\r\ndev_warn(dev, "unknown function code/key bit "\r\n"len combo: (%u/%u)\n", msc->fc,\r\ntrip->keybitlen);\r\nbreak;\r\n}\r\nnext_loop:\r\nbytes_so_far += sizeof(struct msc_triplet);\r\ntrip++;\r\n}\r\nmsc = (struct max_sync_cop *)trip;\r\n}\r\nprops->flags |= NX_OF_FLAG_MAXSYNCCOP_SET;\r\n}\r\nstatic void nx_of_init(struct device *dev, struct nx_of *props)\r\n{\r\nstruct device_node *base_node = dev->of_node;\r\nstruct property *p;\r\np = of_find_property(base_node, "status", NULL);\r\nif (!p)\r\ndev_info(dev, "%s: property 'status' not found\n", __func__);\r\nelse\r\nnx_of_update_status(dev, p, props);\r\np = of_find_property(base_node, "ibm,max-sg-len", NULL);\r\nif (!p)\r\ndev_info(dev, "%s: property 'ibm,max-sg-len' not found\n",\r\n__func__);\r\nelse\r\nnx_of_update_sglen(dev, p, props);\r\np = of_find_property(base_node, "ibm,max-sync-cop", NULL);\r\nif (!p)\r\ndev_info(dev, "%s: property 'ibm,max-sync-cop' not found\n",\r\n__func__);\r\nelse\r\nnx_of_update_msc(dev, p, props);\r\n}\r\nstatic bool nx_check_prop(struct device *dev, u32 fc, u32 mode, int slot)\r\n{\r\nstruct alg_props *props = &nx_driver.of.ap[fc][mode][slot];\r\nif (!props->sglen || props->databytelen < NX_PAGE_SIZE) {\r\nif (dev)\r\ndev_warn(dev, "bogus sglen/databytelen for %u/%u/%u: "\r\n"%u/%u (ignored)\n", fc, mode, slot,\r\nprops->sglen, props->databytelen);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic bool nx_check_props(struct device *dev, u32 fc, u32 mode)\r\n{\r\nint i;\r\nfor (i = 0; i < 3; i++)\r\nif (!nx_check_prop(dev, fc, mode, i))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int nx_register_alg(struct crypto_alg *alg, u32 fc, u32 mode)\r\n{\r\nreturn nx_check_props(&nx_driver.viodev->dev, fc, mode) ?\r\ncrypto_register_alg(alg) : 0;\r\n}\r\nstatic int nx_register_aead(struct aead_alg *alg, u32 fc, u32 mode)\r\n{\r\nreturn nx_check_props(&nx_driver.viodev->dev, fc, mode) ?\r\ncrypto_register_aead(alg) : 0;\r\n}\r\nstatic int nx_register_shash(struct shash_alg *alg, u32 fc, u32 mode, int slot)\r\n{\r\nreturn (slot >= 0 ? nx_check_prop(&nx_driver.viodev->dev,\r\nfc, mode, slot) :\r\nnx_check_props(&nx_driver.viodev->dev, fc, mode)) ?\r\ncrypto_register_shash(alg) : 0;\r\n}\r\nstatic void nx_unregister_alg(struct crypto_alg *alg, u32 fc, u32 mode)\r\n{\r\nif (nx_check_props(NULL, fc, mode))\r\ncrypto_unregister_alg(alg);\r\n}\r\nstatic void nx_unregister_aead(struct aead_alg *alg, u32 fc, u32 mode)\r\n{\r\nif (nx_check_props(NULL, fc, mode))\r\ncrypto_unregister_aead(alg);\r\n}\r\nstatic void nx_unregister_shash(struct shash_alg *alg, u32 fc, u32 mode,\r\nint slot)\r\n{\r\nif (slot >= 0 ? nx_check_prop(NULL, fc, mode, slot) :\r\nnx_check_props(NULL, fc, mode))\r\ncrypto_unregister_shash(alg);\r\n}\r\nstatic int nx_register_algs(void)\r\n{\r\nint rc = -1;\r\nif (nx_driver.of.flags != NX_OF_FLAG_MASK_READY)\r\ngoto out;\r\nmemset(&nx_driver.stats, 0, sizeof(struct nx_stats));\r\nrc = NX_DEBUGFS_INIT(&nx_driver);\r\nif (rc)\r\ngoto out;\r\nnx_driver.of.status = NX_OKAY;\r\nrc = nx_register_alg(&nx_ecb_aes_alg, NX_FC_AES, NX_MODE_AES_ECB);\r\nif (rc)\r\ngoto out;\r\nrc = nx_register_alg(&nx_cbc_aes_alg, NX_FC_AES, NX_MODE_AES_CBC);\r\nif (rc)\r\ngoto out_unreg_ecb;\r\nrc = nx_register_alg(&nx_ctr3686_aes_alg, NX_FC_AES, NX_MODE_AES_CTR);\r\nif (rc)\r\ngoto out_unreg_cbc;\r\nrc = nx_register_aead(&nx_gcm_aes_alg, NX_FC_AES, NX_MODE_AES_GCM);\r\nif (rc)\r\ngoto out_unreg_ctr3686;\r\nrc = nx_register_aead(&nx_gcm4106_aes_alg, NX_FC_AES, NX_MODE_AES_GCM);\r\nif (rc)\r\ngoto out_unreg_gcm;\r\nrc = nx_register_aead(&nx_ccm_aes_alg, NX_FC_AES, NX_MODE_AES_CCM);\r\nif (rc)\r\ngoto out_unreg_gcm4106;\r\nrc = nx_register_aead(&nx_ccm4309_aes_alg, NX_FC_AES, NX_MODE_AES_CCM);\r\nif (rc)\r\ngoto out_unreg_ccm;\r\nrc = nx_register_shash(&nx_shash_sha256_alg, NX_FC_SHA, NX_MODE_SHA,\r\nNX_PROPS_SHA256);\r\nif (rc)\r\ngoto out_unreg_ccm4309;\r\nrc = nx_register_shash(&nx_shash_sha512_alg, NX_FC_SHA, NX_MODE_SHA,\r\nNX_PROPS_SHA512);\r\nif (rc)\r\ngoto out_unreg_s256;\r\nrc = nx_register_shash(&nx_shash_aes_xcbc_alg,\r\nNX_FC_AES, NX_MODE_AES_XCBC_MAC, -1);\r\nif (rc)\r\ngoto out_unreg_s512;\r\ngoto out;\r\nout_unreg_s512:\r\nnx_unregister_shash(&nx_shash_sha512_alg, NX_FC_SHA, NX_MODE_SHA,\r\nNX_PROPS_SHA512);\r\nout_unreg_s256:\r\nnx_unregister_shash(&nx_shash_sha256_alg, NX_FC_SHA, NX_MODE_SHA,\r\nNX_PROPS_SHA256);\r\nout_unreg_ccm4309:\r\nnx_unregister_aead(&nx_ccm4309_aes_alg, NX_FC_AES, NX_MODE_AES_CCM);\r\nout_unreg_ccm:\r\nnx_unregister_aead(&nx_ccm_aes_alg, NX_FC_AES, NX_MODE_AES_CCM);\r\nout_unreg_gcm4106:\r\nnx_unregister_aead(&nx_gcm4106_aes_alg, NX_FC_AES, NX_MODE_AES_GCM);\r\nout_unreg_gcm:\r\nnx_unregister_aead(&nx_gcm_aes_alg, NX_FC_AES, NX_MODE_AES_GCM);\r\nout_unreg_ctr3686:\r\nnx_unregister_alg(&nx_ctr3686_aes_alg, NX_FC_AES, NX_MODE_AES_CTR);\r\nout_unreg_cbc:\r\nnx_unregister_alg(&nx_cbc_aes_alg, NX_FC_AES, NX_MODE_AES_CBC);\r\nout_unreg_ecb:\r\nnx_unregister_alg(&nx_ecb_aes_alg, NX_FC_AES, NX_MODE_AES_ECB);\r\nout:\r\nreturn rc;\r\n}\r\nstatic int nx_crypto_ctx_init(struct nx_crypto_ctx *nx_ctx, u32 fc, u32 mode)\r\n{\r\nif (nx_driver.of.status != NX_OKAY) {\r\npr_err("Attempt to initialize NX crypto context while device "\r\n"is not available!\n");\r\nreturn -ENODEV;\r\n}\r\nif (mode == NX_MODE_AES_GCM || mode == NX_MODE_AES_CCM)\r\nnx_ctx->kmem_len = (5 * NX_PAGE_SIZE) +\r\nsizeof(struct nx_csbcpb);\r\nelse\r\nnx_ctx->kmem_len = (4 * NX_PAGE_SIZE) +\r\nsizeof(struct nx_csbcpb);\r\nnx_ctx->kmem = kmalloc(nx_ctx->kmem_len, GFP_KERNEL);\r\nif (!nx_ctx->kmem)\r\nreturn -ENOMEM;\r\nnx_ctx->csbcpb = (struct nx_csbcpb *)(round_up((u64)nx_ctx->kmem,\r\n(u64)NX_PAGE_SIZE));\r\nnx_ctx->in_sg = (struct nx_sg *)((u8 *)nx_ctx->csbcpb + NX_PAGE_SIZE);\r\nnx_ctx->out_sg = (struct nx_sg *)((u8 *)nx_ctx->in_sg + NX_PAGE_SIZE);\r\nif (mode == NX_MODE_AES_GCM || mode == NX_MODE_AES_CCM)\r\nnx_ctx->csbcpb_aead =\r\n(struct nx_csbcpb *)((u8 *)nx_ctx->out_sg +\r\nNX_PAGE_SIZE);\r\nnx_ctx->stats = &nx_driver.stats;\r\nmemcpy(nx_ctx->props, nx_driver.of.ap[fc][mode],\r\nsizeof(struct alg_props) * 3);\r\nreturn 0;\r\n}\r\nint nx_crypto_ctx_aes_ccm_init(struct crypto_aead *tfm)\r\n{\r\ncrypto_aead_set_reqsize(tfm, sizeof(struct nx_ccm_rctx));\r\nreturn nx_crypto_ctx_init(crypto_aead_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_CCM);\r\n}\r\nint nx_crypto_ctx_aes_gcm_init(struct crypto_aead *tfm)\r\n{\r\ncrypto_aead_set_reqsize(tfm, sizeof(struct nx_gcm_rctx));\r\nreturn nx_crypto_ctx_init(crypto_aead_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_GCM);\r\n}\r\nint nx_crypto_ctx_aes_ctr_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_CTR);\r\n}\r\nint nx_crypto_ctx_aes_cbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_CBC);\r\n}\r\nint nx_crypto_ctx_aes_ecb_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_ECB);\r\n}\r\nint nx_crypto_ctx_sha_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_SHA, NX_MODE_SHA);\r\n}\r\nint nx_crypto_ctx_aes_xcbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn nx_crypto_ctx_init(crypto_tfm_ctx(tfm), NX_FC_AES,\r\nNX_MODE_AES_XCBC_MAC);\r\n}\r\nvoid nx_crypto_ctx_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(tfm);\r\nkzfree(nx_ctx->kmem);\r\nnx_ctx->csbcpb = NULL;\r\nnx_ctx->csbcpb_aead = NULL;\r\nnx_ctx->in_sg = NULL;\r\nnx_ctx->out_sg = NULL;\r\n}\r\nvoid nx_crypto_ctx_aead_exit(struct crypto_aead *tfm)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_aead_ctx(tfm);\r\nkzfree(nx_ctx->kmem);\r\n}\r\nstatic int nx_probe(struct vio_dev *viodev, const struct vio_device_id *id)\r\n{\r\ndev_dbg(&viodev->dev, "driver probed: %s resource id: 0x%x\n",\r\nviodev->name, viodev->resource_id);\r\nif (nx_driver.viodev) {\r\ndev_err(&viodev->dev, "%s: Attempt to register more than one "\r\n"instance of the hardware\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nnx_driver.viodev = viodev;\r\nnx_of_init(&viodev->dev, &nx_driver.of);\r\nreturn nx_register_algs();\r\n}\r\nstatic int nx_remove(struct vio_dev *viodev)\r\n{\r\ndev_dbg(&viodev->dev, "entering nx_remove for UA 0x%x\n",\r\nviodev->unit_address);\r\nif (nx_driver.of.status == NX_OKAY) {\r\nNX_DEBUGFS_FINI(&nx_driver);\r\nnx_unregister_shash(&nx_shash_aes_xcbc_alg,\r\nNX_FC_AES, NX_MODE_AES_XCBC_MAC, -1);\r\nnx_unregister_shash(&nx_shash_sha512_alg,\r\nNX_FC_SHA, NX_MODE_SHA, NX_PROPS_SHA256);\r\nnx_unregister_shash(&nx_shash_sha256_alg,\r\nNX_FC_SHA, NX_MODE_SHA, NX_PROPS_SHA512);\r\nnx_unregister_aead(&nx_ccm4309_aes_alg,\r\nNX_FC_AES, NX_MODE_AES_CCM);\r\nnx_unregister_aead(&nx_ccm_aes_alg, NX_FC_AES, NX_MODE_AES_CCM);\r\nnx_unregister_aead(&nx_gcm4106_aes_alg,\r\nNX_FC_AES, NX_MODE_AES_GCM);\r\nnx_unregister_aead(&nx_gcm_aes_alg,\r\nNX_FC_AES, NX_MODE_AES_GCM);\r\nnx_unregister_alg(&nx_ctr3686_aes_alg,\r\nNX_FC_AES, NX_MODE_AES_CTR);\r\nnx_unregister_alg(&nx_cbc_aes_alg, NX_FC_AES, NX_MODE_AES_CBC);\r\nnx_unregister_alg(&nx_ecb_aes_alg, NX_FC_AES, NX_MODE_AES_ECB);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init nx_init(void)\r\n{\r\nreturn vio_register_driver(&nx_driver.viodriver);\r\n}\r\nstatic void __exit nx_fini(void)\r\n{\r\nvio_unregister_driver(&nx_driver.viodriver);\r\n}
