void ia64_mca_printk(const char *fmt, ...)\r\n{\r\nva_list args;\r\nint printed_len;\r\nchar temp_buf[MLOGBUF_MSGMAX];\r\nchar *p;\r\nva_start(args, fmt);\r\nprinted_len = vscnprintf(temp_buf, sizeof(temp_buf), fmt, args);\r\nva_end(args);\r\nif (oops_in_progress) {\r\nprintk("%s", temp_buf);\r\n} else {\r\nspin_lock(&mlogbuf_wlock);\r\nfor (p = temp_buf; *p; p++) {\r\nunsigned long next = (mlogbuf_end + 1) % MLOGBUF_SIZE;\r\nif (next != mlogbuf_start) {\r\nmlogbuf[mlogbuf_end] = *p;\r\nmlogbuf_end = next;\r\n} else {\r\nbreak;\r\n}\r\n}\r\nmlogbuf[mlogbuf_end] = '\0';\r\nspin_unlock(&mlogbuf_wlock);\r\n}\r\n}\r\nvoid ia64_mlogbuf_dump(void)\r\n{\r\nchar temp_buf[MLOGBUF_MSGMAX];\r\nchar *p;\r\nunsigned long index;\r\nunsigned long flags;\r\nunsigned int printed_len;\r\nwhile (mlogbuf_start != mlogbuf_end) {\r\ntemp_buf[0] = '\0';\r\np = temp_buf;\r\nprinted_len = 0;\r\nspin_lock_irqsave(&mlogbuf_rlock, flags);\r\nindex = mlogbuf_start;\r\nwhile (index != mlogbuf_end) {\r\n*p = mlogbuf[index];\r\nindex = (index + 1) % MLOGBUF_SIZE;\r\nif (!*p)\r\nbreak;\r\np++;\r\nif (++printed_len >= MLOGBUF_MSGMAX - 1)\r\nbreak;\r\n}\r\n*p = '\0';\r\nif (temp_buf[0])\r\nprintk("%s", temp_buf);\r\nmlogbuf_start = index;\r\nmlogbuf_timestamp = 0;\r\nspin_unlock_irqrestore(&mlogbuf_rlock, flags);\r\n}\r\n}\r\nstatic void ia64_mlogbuf_finish(int wait)\r\n{\r\nBREAK_LOGLEVEL(console_loglevel);\r\nspin_lock_init(&mlogbuf_rlock);\r\nia64_mlogbuf_dump();\r\nprintk(KERN_EMERG "mlogbuf_finish: printing switched to urgent mode, "\r\n"MCA/INIT might be dodgy or fail.\n");\r\nif (!wait)\r\nreturn;\r\nprintk("Delaying for 5 seconds...\n");\r\nudelay(5*1000000);\r\nmlogbuf_finished = 1;\r\n}\r\nstatic void ia64_mlogbuf_dump_from_init(void)\r\n{\r\nif (mlogbuf_finished)\r\nreturn;\r\nif (mlogbuf_timestamp &&\r\ntime_before(jiffies, mlogbuf_timestamp + 30 * HZ)) {\r\nprintk(KERN_ERR "INIT: mlogbuf_dump is interrupted by INIT "\r\n" and the system seems to be messed up.\n");\r\nia64_mlogbuf_finish(0);\r\nreturn;\r\n}\r\nif (!spin_trylock(&mlogbuf_rlock)) {\r\nprintk(KERN_ERR "INIT: mlogbuf_dump is interrupted by INIT. "\r\n"Generated messages other than stack dump will be "\r\n"buffered to mlogbuf and will be printed later.\n");\r\nprintk(KERN_ERR "INIT: If messages would not printed after "\r\n"this INIT, wait 30sec and assert INIT again.\n");\r\nif (!mlogbuf_timestamp)\r\nmlogbuf_timestamp = jiffies;\r\nreturn;\r\n}\r\nspin_unlock(&mlogbuf_rlock);\r\nia64_mlogbuf_dump();\r\n}\r\nstatic inline void\r\nia64_mca_spin(const char *func)\r\n{\r\nif (monarch_cpu == smp_processor_id())\r\nia64_mlogbuf_finish(0);\r\nmprintk(KERN_EMERG "%s: spinning here, not returning to SAL\n", func);\r\nwhile (1)\r\ncpu_relax();\r\n}\r\nstatic void __init\r\nia64_log_init(int sal_info_type)\r\n{\r\nu64 max_size = 0;\r\nIA64_LOG_NEXT_INDEX(sal_info_type) = 0;\r\nIA64_LOG_LOCK_INIT(sal_info_type);\r\nmax_size = ia64_sal_get_state_info_size(sal_info_type);\r\nif (!max_size)\r\nreturn;\r\nIA64_LOG_ALLOCATE(sal_info_type, max_size);\r\nmemset(IA64_LOG_CURR_BUFFER(sal_info_type), 0, max_size);\r\nmemset(IA64_LOG_NEXT_BUFFER(sal_info_type), 0, max_size);\r\n}\r\nstatic u64\r\nia64_log_get(int sal_info_type, u8 **buffer, int irq_safe)\r\n{\r\nsal_log_record_header_t *log_buffer;\r\nu64 total_len = 0;\r\nunsigned long s;\r\nIA64_LOG_LOCK(sal_info_type);\r\nlog_buffer = IA64_LOG_NEXT_BUFFER(sal_info_type);\r\ntotal_len = ia64_sal_get_state_info(sal_info_type, (u64 *)log_buffer);\r\nif (total_len) {\r\nIA64_LOG_INDEX_INC(sal_info_type);\r\nIA64_LOG_UNLOCK(sal_info_type);\r\nif (irq_safe) {\r\nIA64_MCA_DEBUG("%s: SAL error record type %d retrieved. Record length = %ld\n",\r\n__func__, sal_info_type, total_len);\r\n}\r\n*buffer = (u8 *) log_buffer;\r\nreturn total_len;\r\n} else {\r\nIA64_LOG_UNLOCK(sal_info_type);\r\nreturn 0;\r\n}\r\n}\r\nstatic void\r\nia64_mca_log_sal_error_record(int sal_info_type)\r\n{\r\nu8 *buffer;\r\nsal_log_record_header_t *rh;\r\nu64 size;\r\nint irq_safe = sal_info_type != SAL_INFO_TYPE_MCA;\r\n#ifdef IA64_MCA_DEBUG_INFO\r\nstatic const char * const rec_name[] = { "MCA", "INIT", "CMC", "CPE" };\r\n#endif\r\nsize = ia64_log_get(sal_info_type, &buffer, irq_safe);\r\nif (!size)\r\nreturn;\r\nsalinfo_log_wakeup(sal_info_type, buffer, size, irq_safe);\r\nif (irq_safe)\r\nIA64_MCA_DEBUG("CPU %d: SAL log contains %s error record\n",\r\nsmp_processor_id(),\r\nsal_info_type < ARRAY_SIZE(rec_name) ? rec_name[sal_info_type] : "UNKNOWN");\r\nrh = (sal_log_record_header_t *)buffer;\r\nif (rh->severity == sal_log_severity_corrected)\r\nia64_sal_clear_state_info(sal_info_type);\r\n}\r\nint\r\nsearch_mca_table (const struct mca_table_entry *first,\r\nconst struct mca_table_entry *last,\r\nunsigned long ip)\r\n{\r\nconst struct mca_table_entry *curr;\r\nu64 curr_start, curr_end;\r\ncurr = first;\r\nwhile (curr <= last) {\r\ncurr_start = (u64) &curr->start_addr + curr->start_addr;\r\ncurr_end = (u64) &curr->end_addr + curr->end_addr;\r\nif ((ip >= curr_start) && (ip <= curr_end)) {\r\nreturn 1;\r\n}\r\ncurr++;\r\n}\r\nreturn 0;\r\n}\r\nint mca_recover_range(unsigned long addr)\r\n{\r\nextern struct mca_table_entry __start___mca_table[];\r\nextern struct mca_table_entry __stop___mca_table[];\r\nreturn search_mca_table(__start___mca_table, __stop___mca_table-1, addr);\r\n}\r\nstatic irqreturn_t\r\nia64_mca_cpe_int_handler (int cpe_irq, void *arg)\r\n{\r\nstatic unsigned long cpe_history[CPE_HISTORY_LENGTH];\r\nstatic int index;\r\nstatic DEFINE_SPINLOCK(cpe_history_lock);\r\nIA64_MCA_DEBUG("%s: received interrupt vector = %#x on CPU %d\n",\r\n__func__, cpe_irq, smp_processor_id());\r\nlocal_irq_enable();\r\nspin_lock(&cpe_history_lock);\r\nif (!cpe_poll_enabled && cpe_vector >= 0) {\r\nint i, count = 1;\r\nunsigned long now = jiffies;\r\nfor (i = 0; i < CPE_HISTORY_LENGTH; i++) {\r\nif (now - cpe_history[i] <= HZ)\r\ncount++;\r\n}\r\nIA64_MCA_DEBUG(KERN_INFO "CPE threshold %d/%d\n", count, CPE_HISTORY_LENGTH);\r\nif (count >= CPE_HISTORY_LENGTH) {\r\ncpe_poll_enabled = 1;\r\nspin_unlock(&cpe_history_lock);\r\ndisable_irq_nosync(local_vector_to_irq(IA64_CPE_VECTOR));\r\nprintk(KERN_WARNING "WARNING: Switching to polling CPE handler; error records may be lost\n");\r\nmod_timer(&cpe_poll_timer, jiffies + MIN_CPE_POLL_INTERVAL);\r\ngoto out;\r\n} else {\r\ncpe_history[index++] = now;\r\nif (index == CPE_HISTORY_LENGTH)\r\nindex = 0;\r\n}\r\n}\r\nspin_unlock(&cpe_history_lock);\r\nout:\r\nia64_mca_log_sal_error_record(SAL_INFO_TYPE_CPE);\r\nlocal_irq_disable();\r\nreturn IRQ_HANDLED;\r\n}\r\nvoid\r\nia64_mca_register_cpev (int cpev)\r\n{\r\nstruct ia64_sal_retval isrv;\r\nisrv = ia64_sal_mc_set_params(SAL_MC_PARAM_CPE_INT, SAL_MC_PARAM_MECHANISM_INT, cpev, 0, 0);\r\nif (isrv.status) {\r\nprintk(KERN_ERR "Failed to register Corrected Platform "\r\n"Error interrupt vector with SAL (status %ld)\n", isrv.status);\r\nreturn;\r\n}\r\nIA64_MCA_DEBUG("%s: corrected platform error "\r\n"vector %#x registered\n", __func__, cpev);\r\n}\r\nvoid\r\nia64_mca_cmc_vector_setup (void)\r\n{\r\ncmcv_reg_t cmcv;\r\ncmcv.cmcv_regval = 0;\r\ncmcv.cmcv_mask = 1;\r\ncmcv.cmcv_vector = IA64_CMC_VECTOR;\r\nia64_setreg(_IA64_REG_CR_CMCV, cmcv.cmcv_regval);\r\nIA64_MCA_DEBUG("%s: CPU %d corrected machine check vector %#x registered.\n",\r\n__func__, smp_processor_id(), IA64_CMC_VECTOR);\r\nIA64_MCA_DEBUG("%s: CPU %d CMCV = %#016lx\n",\r\n__func__, smp_processor_id(), ia64_getreg(_IA64_REG_CR_CMCV));\r\n}\r\nstatic void\r\nia64_mca_cmc_vector_disable (void *dummy)\r\n{\r\ncmcv_reg_t cmcv;\r\ncmcv.cmcv_regval = ia64_getreg(_IA64_REG_CR_CMCV);\r\ncmcv.cmcv_mask = 1;\r\nia64_setreg(_IA64_REG_CR_CMCV, cmcv.cmcv_regval);\r\nIA64_MCA_DEBUG("%s: CPU %d corrected machine check vector %#x disabled.\n",\r\n__func__, smp_processor_id(), cmcv.cmcv_vector);\r\n}\r\nstatic void\r\nia64_mca_cmc_vector_enable (void *dummy)\r\n{\r\ncmcv_reg_t cmcv;\r\ncmcv.cmcv_regval = ia64_getreg(_IA64_REG_CR_CMCV);\r\ncmcv.cmcv_mask = 0;\r\nia64_setreg(_IA64_REG_CR_CMCV, cmcv.cmcv_regval);\r\nIA64_MCA_DEBUG("%s: CPU %d corrected machine check vector %#x enabled.\n",\r\n__func__, smp_processor_id(), cmcv.cmcv_vector);\r\n}\r\nstatic void\r\nia64_mca_cmc_vector_disable_keventd(struct work_struct *unused)\r\n{\r\non_each_cpu(ia64_mca_cmc_vector_disable, NULL, 0);\r\n}\r\nstatic void\r\nia64_mca_cmc_vector_enable_keventd(struct work_struct *unused)\r\n{\r\non_each_cpu(ia64_mca_cmc_vector_enable, NULL, 0);\r\n}\r\nstatic void\r\nia64_mca_wakeup(int cpu)\r\n{\r\nplatform_send_ipi(cpu, IA64_MCA_WAKEUP_VECTOR, IA64_IPI_DM_INT, 0);\r\n}\r\nstatic void\r\nia64_mca_wakeup_all(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu) {\r\nif (ia64_mc_info.imi_rendez_checkin[cpu] == IA64_MCA_RENDEZ_CHECKIN_DONE)\r\nia64_mca_wakeup(cpu);\r\n}\r\n}\r\nstatic irqreturn_t\r\nia64_mca_rendez_int_handler(int rendez_irq, void *arg)\r\n{\r\nunsigned long flags;\r\nint cpu = smp_processor_id();\r\nstruct ia64_mca_notify_die nd =\r\n{ .sos = NULL, .monarch_cpu = &monarch_cpu };\r\nlocal_irq_save(flags);\r\nNOTIFY_MCA(DIE_MCA_RENDZVOUS_ENTER, get_irq_regs(), (long)&nd, 1);\r\nia64_mc_info.imi_rendez_checkin[cpu] = IA64_MCA_RENDEZ_CHECKIN_DONE;\r\nia64_sal_mc_rendez();\r\nNOTIFY_MCA(DIE_MCA_RENDZVOUS_PROCESS, get_irq_regs(), (long)&nd, 1);\r\nwhile (monarch_cpu != -1)\r\ncpu_relax();\r\nNOTIFY_MCA(DIE_MCA_RENDZVOUS_LEAVE, get_irq_regs(), (long)&nd, 1);\r\nia64_mc_info.imi_rendez_checkin[cpu] = IA64_MCA_RENDEZ_CHECKIN_NOTDONE;\r\nlocal_irq_restore(flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t\r\nia64_mca_wakeup_int_handler(int wakeup_irq, void *arg)\r\n{\r\nreturn IRQ_HANDLED;\r\n}\r\nint\r\nia64_reg_MCA_extension(int (*fn)(void *, struct ia64_sal_os_state *))\r\n{\r\nif (ia64_mca_ucmc_extension)\r\nreturn 1;\r\nia64_mca_ucmc_extension = fn;\r\nreturn 0;\r\n}\r\nvoid\r\nia64_unreg_MCA_extension(void)\r\n{\r\nif (ia64_mca_ucmc_extension)\r\nia64_mca_ucmc_extension = NULL;\r\n}\r\nstatic inline void\r\ncopy_reg(const u64 *fr, u64 fnat, unsigned long *tr, unsigned long *tnat)\r\n{\r\nu64 fslot, tslot, nat;\r\n*tr = *fr;\r\nfslot = ((unsigned long)fr >> 3) & 63;\r\ntslot = ((unsigned long)tr >> 3) & 63;\r\n*tnat &= ~(1UL << tslot);\r\nnat = (fnat >> fslot) & 1;\r\n*tnat |= (nat << tslot);\r\n}\r\nstatic void\r\nia64_mca_modify_comm(const struct task_struct *previous_current)\r\n{\r\nchar *p, comm[sizeof(current->comm)];\r\nif (previous_current->pid)\r\nsnprintf(comm, sizeof(comm), "%s %d",\r\ncurrent->comm, previous_current->pid);\r\nelse {\r\nint l;\r\nif ((p = strchr(previous_current->comm, ' ')))\r\nl = p - previous_current->comm;\r\nelse\r\nl = strlen(previous_current->comm);\r\nsnprintf(comm, sizeof(comm), "%s %*s %d",\r\ncurrent->comm, l, previous_current->comm,\r\ntask_thread_info(previous_current)->cpu);\r\n}\r\nmemcpy(current->comm, comm, sizeof(current->comm));\r\n}\r\nstatic void\r\nfinish_pt_regs(struct pt_regs *regs, struct ia64_sal_os_state *sos,\r\nunsigned long *nat)\r\n{\r\nconst pal_min_state_area_t *ms = sos->pal_min_state;\r\nconst u64 *bank;\r\nif (ia64_psr(regs)->ic) {\r\nregs->cr_iip = ms->pmsa_iip;\r\nregs->cr_ipsr = ms->pmsa_ipsr;\r\nregs->cr_ifs = ms->pmsa_ifs;\r\n} else {\r\nregs->cr_iip = ms->pmsa_xip;\r\nregs->cr_ipsr = ms->pmsa_xpsr;\r\nregs->cr_ifs = ms->pmsa_xfs;\r\nsos->iip = ms->pmsa_iip;\r\nsos->ipsr = ms->pmsa_ipsr;\r\nsos->ifs = ms->pmsa_ifs;\r\n}\r\nregs->pr = ms->pmsa_pr;\r\nregs->b0 = ms->pmsa_br0;\r\nregs->ar_rsc = ms->pmsa_rsc;\r\ncopy_reg(&ms->pmsa_gr[1-1], ms->pmsa_nat_bits, &regs->r1, nat);\r\ncopy_reg(&ms->pmsa_gr[2-1], ms->pmsa_nat_bits, &regs->r2, nat);\r\ncopy_reg(&ms->pmsa_gr[3-1], ms->pmsa_nat_bits, &regs->r3, nat);\r\ncopy_reg(&ms->pmsa_gr[8-1], ms->pmsa_nat_bits, &regs->r8, nat);\r\ncopy_reg(&ms->pmsa_gr[9-1], ms->pmsa_nat_bits, &regs->r9, nat);\r\ncopy_reg(&ms->pmsa_gr[10-1], ms->pmsa_nat_bits, &regs->r10, nat);\r\ncopy_reg(&ms->pmsa_gr[11-1], ms->pmsa_nat_bits, &regs->r11, nat);\r\ncopy_reg(&ms->pmsa_gr[12-1], ms->pmsa_nat_bits, &regs->r12, nat);\r\ncopy_reg(&ms->pmsa_gr[13-1], ms->pmsa_nat_bits, &regs->r13, nat);\r\ncopy_reg(&ms->pmsa_gr[14-1], ms->pmsa_nat_bits, &regs->r14, nat);\r\ncopy_reg(&ms->pmsa_gr[15-1], ms->pmsa_nat_bits, &regs->r15, nat);\r\nif (ia64_psr(regs)->bn)\r\nbank = ms->pmsa_bank1_gr;\r\nelse\r\nbank = ms->pmsa_bank0_gr;\r\ncopy_reg(&bank[16-16], ms->pmsa_nat_bits, &regs->r16, nat);\r\ncopy_reg(&bank[17-16], ms->pmsa_nat_bits, &regs->r17, nat);\r\ncopy_reg(&bank[18-16], ms->pmsa_nat_bits, &regs->r18, nat);\r\ncopy_reg(&bank[19-16], ms->pmsa_nat_bits, &regs->r19, nat);\r\ncopy_reg(&bank[20-16], ms->pmsa_nat_bits, &regs->r20, nat);\r\ncopy_reg(&bank[21-16], ms->pmsa_nat_bits, &regs->r21, nat);\r\ncopy_reg(&bank[22-16], ms->pmsa_nat_bits, &regs->r22, nat);\r\ncopy_reg(&bank[23-16], ms->pmsa_nat_bits, &regs->r23, nat);\r\ncopy_reg(&bank[24-16], ms->pmsa_nat_bits, &regs->r24, nat);\r\ncopy_reg(&bank[25-16], ms->pmsa_nat_bits, &regs->r25, nat);\r\ncopy_reg(&bank[26-16], ms->pmsa_nat_bits, &regs->r26, nat);\r\ncopy_reg(&bank[27-16], ms->pmsa_nat_bits, &regs->r27, nat);\r\ncopy_reg(&bank[28-16], ms->pmsa_nat_bits, &regs->r28, nat);\r\ncopy_reg(&bank[29-16], ms->pmsa_nat_bits, &regs->r29, nat);\r\ncopy_reg(&bank[30-16], ms->pmsa_nat_bits, &regs->r30, nat);\r\ncopy_reg(&bank[31-16], ms->pmsa_nat_bits, &regs->r31, nat);\r\n}\r\nstatic struct task_struct *\r\nia64_mca_modify_original_stack(struct pt_regs *regs,\r\nconst struct switch_stack *sw,\r\nstruct ia64_sal_os_state *sos,\r\nconst char *type)\r\n{\r\nchar *p;\r\nia64_va va;\r\nextern char ia64_leave_kernel[];\r\nconst pal_min_state_area_t *ms = sos->pal_min_state;\r\nstruct task_struct *previous_current;\r\nstruct pt_regs *old_regs;\r\nstruct switch_stack *old_sw;\r\nunsigned size = sizeof(struct pt_regs) +\r\nsizeof(struct switch_stack) + 16;\r\nunsigned long *old_bspstore, *old_bsp;\r\nunsigned long *new_bspstore, *new_bsp;\r\nunsigned long old_unat, old_rnat, new_rnat, nat;\r\nu64 slots, loadrs = regs->loadrs;\r\nu64 r12 = ms->pmsa_gr[12-1], r13 = ms->pmsa_gr[13-1];\r\nu64 ar_bspstore = regs->ar_bspstore;\r\nu64 ar_bsp = regs->ar_bspstore + (loadrs >> 16);\r\nconst char *msg;\r\nint cpu = smp_processor_id();\r\nprevious_current = curr_task(cpu);\r\nia64_set_curr_task(cpu, current);\r\nif ((p = strchr(current->comm, ' ')))\r\n*p = '\0';\r\nregs->cr_ipsr = ms->pmsa_ipsr;\r\nif (ia64_psr(regs)->dt == 0) {\r\nva.l = r12;\r\nif (va.f.reg == 0) {\r\nva.f.reg = 7;\r\nr12 = va.l;\r\n}\r\nva.l = r13;\r\nif (va.f.reg == 0) {\r\nva.f.reg = 7;\r\nr13 = va.l;\r\n}\r\n}\r\nif (ia64_psr(regs)->rt == 0) {\r\nva.l = ar_bspstore;\r\nif (va.f.reg == 0) {\r\nva.f.reg = 7;\r\nar_bspstore = va.l;\r\n}\r\nva.l = ar_bsp;\r\nif (va.f.reg == 0) {\r\nva.f.reg = 7;\r\nar_bsp = va.l;\r\n}\r\n}\r\nold_bspstore = (unsigned long *)ar_bspstore;\r\nold_bsp = (unsigned long *)ar_bsp;\r\nslots = ia64_rse_num_regs(old_bspstore, old_bsp);\r\nnew_bspstore = (unsigned long *)((u64)current + IA64_RBS_OFFSET);\r\nnew_bsp = ia64_rse_skip_regs(new_bspstore, slots);\r\nregs->loadrs = (new_bsp - new_bspstore) * 8 << 16;\r\nif (user_mode(regs)) {\r\nmsg = "occurred in user space";\r\nia64_mca_modify_comm(previous_current);\r\ngoto no_mod;\r\n}\r\nif (r13 != sos->prev_IA64_KR_CURRENT) {\r\nmsg = "inconsistent previous current and r13";\r\ngoto no_mod;\r\n}\r\nif (!mca_recover_range(ms->pmsa_iip)) {\r\nif ((r12 - r13) >= KERNEL_STACK_SIZE) {\r\nmsg = "inconsistent r12 and r13";\r\ngoto no_mod;\r\n}\r\nif ((ar_bspstore - r13) >= KERNEL_STACK_SIZE) {\r\nmsg = "inconsistent ar.bspstore and r13";\r\ngoto no_mod;\r\n}\r\nva.p = old_bspstore;\r\nif (va.f.reg < 5) {\r\nmsg = "old_bspstore is in the wrong region";\r\ngoto no_mod;\r\n}\r\nif ((ar_bsp - r13) >= KERNEL_STACK_SIZE) {\r\nmsg = "inconsistent ar.bsp and r13";\r\ngoto no_mod;\r\n}\r\nsize += (ia64_rse_skip_regs(old_bspstore, slots) - old_bspstore) * 8;\r\nif (ar_bspstore + size > r12) {\r\nmsg = "no room for blocked state";\r\ngoto no_mod;\r\n}\r\n}\r\nia64_mca_modify_comm(previous_current);\r\np = (char *)r12 - sizeof(*regs);\r\nold_regs = (struct pt_regs *)p;\r\nmemcpy(old_regs, regs, sizeof(*regs));\r\nold_regs->loadrs = loadrs;\r\nold_unat = old_regs->ar_unat;\r\nfinish_pt_regs(old_regs, sos, &old_unat);\r\np -= sizeof(struct switch_stack);\r\nold_sw = (struct switch_stack *)p;\r\nmemcpy(old_sw, sw, sizeof(*sw));\r\nold_sw->caller_unat = old_unat;\r\nold_sw->ar_fpsr = old_regs->ar_fpsr;\r\ncopy_reg(&ms->pmsa_gr[4-1], ms->pmsa_nat_bits, &old_sw->r4, &old_unat);\r\ncopy_reg(&ms->pmsa_gr[5-1], ms->pmsa_nat_bits, &old_sw->r5, &old_unat);\r\ncopy_reg(&ms->pmsa_gr[6-1], ms->pmsa_nat_bits, &old_sw->r6, &old_unat);\r\ncopy_reg(&ms->pmsa_gr[7-1], ms->pmsa_nat_bits, &old_sw->r7, &old_unat);\r\nold_sw->b0 = (u64)ia64_leave_kernel;\r\nold_sw->b1 = ms->pmsa_br1;\r\nold_sw->ar_pfs = 0;\r\nold_sw->ar_unat = old_unat;\r\nold_sw->pr = old_regs->pr | (1UL << PRED_NON_SYSCALL);\r\nprevious_current->thread.ksp = (u64)p - 16;\r\nnew_rnat = ia64_get_rnat(ia64_rse_rnat_addr(new_bspstore));\r\nold_rnat = regs->ar_rnat;\r\nwhile (slots--) {\r\nif (ia64_rse_is_rnat_slot(new_bspstore)) {\r\nnew_rnat = ia64_get_rnat(new_bspstore++);\r\n}\r\nif (ia64_rse_is_rnat_slot(old_bspstore)) {\r\n*old_bspstore++ = old_rnat;\r\nold_rnat = 0;\r\n}\r\nnat = (new_rnat >> ia64_rse_slot_num(new_bspstore)) & 1UL;\r\nold_rnat &= ~(1UL << ia64_rse_slot_num(old_bspstore));\r\nold_rnat |= (nat << ia64_rse_slot_num(old_bspstore));\r\n*old_bspstore++ = *new_bspstore++;\r\n}\r\nold_sw->ar_bspstore = (unsigned long)old_bspstore;\r\nold_sw->ar_rnat = old_rnat;\r\nsos->prev_task = previous_current;\r\nreturn previous_current;\r\nno_mod:\r\nmprintk(KERN_INFO "cpu %d, %s %s, original stack not modified\n",\r\nsmp_processor_id(), type, msg);\r\nold_unat = regs->ar_unat;\r\nfinish_pt_regs(regs, sos, &old_unat);\r\nreturn previous_current;\r\n}\r\nstatic void\r\nia64_wait_for_slaves(int monarch, const char *type)\r\n{\r\nint c, i , wait;\r\nfor (i = 0; i < 5000; i++) {\r\nwait = 0;\r\nfor_each_online_cpu(c) {\r\nif (c == monarch)\r\ncontinue;\r\nif (ia64_mc_info.imi_rendez_checkin[c]\r\n== IA64_MCA_RENDEZ_CHECKIN_NOTDONE) {\r\nudelay(1000);\r\nwait = 1;\r\nbreak;\r\n}\r\n}\r\nif (!wait)\r\ngoto all_in;\r\n}\r\nia64_mlogbuf_finish(0);\r\nmprintk(KERN_INFO "OS %s slave did not rendezvous on cpu", type);\r\nfor_each_online_cpu(c) {\r\nif (c == monarch)\r\ncontinue;\r\nif (ia64_mc_info.imi_rendez_checkin[c] == IA64_MCA_RENDEZ_CHECKIN_NOTDONE)\r\nmprintk(" %d", c);\r\n}\r\nmprintk("\n");\r\nreturn;\r\nall_in:\r\nmprintk(KERN_INFO "All OS %s slaves have reached rendezvous\n", type);\r\nreturn;\r\n}\r\nstatic void mca_insert_tr(u64 iord)\r\n{\r\nint i;\r\nu64 old_rr;\r\nstruct ia64_tr_entry *p;\r\nunsigned long psr;\r\nint cpu = smp_processor_id();\r\nif (!ia64_idtrs[cpu])\r\nreturn;\r\npsr = ia64_clear_ic();\r\nfor (i = IA64_TR_ALLOC_BASE; i < IA64_TR_ALLOC_MAX; i++) {\r\np = ia64_idtrs[cpu] + (iord - 1) * IA64_TR_ALLOC_MAX;\r\nif (p->pte & 0x1) {\r\nold_rr = ia64_get_rr(p->ifa);\r\nif (old_rr != p->rr) {\r\nia64_set_rr(p->ifa, p->rr);\r\nia64_srlz_d();\r\n}\r\nia64_ptr(iord, p->ifa, p->itir >> 2);\r\nia64_srlz_i();\r\nif (iord & 0x1) {\r\nia64_itr(0x1, i, p->ifa, p->pte, p->itir >> 2);\r\nia64_srlz_i();\r\n}\r\nif (iord & 0x2) {\r\nia64_itr(0x2, i, p->ifa, p->pte, p->itir >> 2);\r\nia64_srlz_i();\r\n}\r\nif (old_rr != p->rr) {\r\nia64_set_rr(p->ifa, old_rr);\r\nia64_srlz_d();\r\n}\r\n}\r\n}\r\nia64_set_psr(psr);\r\n}\r\nvoid\r\nia64_mca_handler(struct pt_regs *regs, struct switch_stack *sw,\r\nstruct ia64_sal_os_state *sos)\r\n{\r\nint recover, cpu = smp_processor_id();\r\nstruct task_struct *previous_current;\r\nstruct ia64_mca_notify_die nd =\r\n{ .sos = sos, .monarch_cpu = &monarch_cpu, .data = &recover };\r\nstatic atomic_t mca_count;\r\nstatic cpumask_t mca_cpu;\r\nif (atomic_add_return(1, &mca_count) == 1) {\r\nmonarch_cpu = cpu;\r\nsos->monarch = 1;\r\n} else {\r\ncpumask_set_cpu(cpu, &mca_cpu);\r\nsos->monarch = 0;\r\n}\r\nmprintk(KERN_INFO "Entered OS MCA handler. PSP=%lx cpu=%d "\r\n"monarch=%ld\n", sos->proc_state_param, cpu, sos->monarch);\r\nprevious_current = ia64_mca_modify_original_stack(regs, sw, sos, "MCA");\r\nNOTIFY_MCA(DIE_MCA_MONARCH_ENTER, regs, (long)&nd, 1);\r\nia64_mc_info.imi_rendez_checkin[cpu] = IA64_MCA_RENDEZ_CHECKIN_CONCURRENT_MCA;\r\nif (sos->monarch) {\r\nia64_wait_for_slaves(cpu, "MCA");\r\nia64_mca_wakeup_all();\r\n} else {\r\nwhile (cpumask_test_cpu(cpu, &mca_cpu))\r\ncpu_relax();\r\n}\r\nNOTIFY_MCA(DIE_MCA_MONARCH_PROCESS, regs, (long)&nd, 1);\r\nia64_mca_log_sal_error_record(SAL_INFO_TYPE_MCA);\r\nrecover = (ia64_mca_ucmc_extension\r\n&& ia64_mca_ucmc_extension(\r\nIA64_LOG_CURR_BUFFER(SAL_INFO_TYPE_MCA),\r\nsos));\r\nif (recover) {\r\nsal_log_record_header_t *rh = IA64_LOG_CURR_BUFFER(SAL_INFO_TYPE_MCA);\r\nrh->severity = sal_log_severity_corrected;\r\nia64_sal_clear_state_info(SAL_INFO_TYPE_MCA);\r\nsos->os_status = IA64_MCA_CORRECTED;\r\n} else {\r\nia64_mlogbuf_finish(1);\r\n}\r\nif (__this_cpu_read(ia64_mca_tr_reload)) {\r\nmca_insert_tr(0x1);\r\nmca_insert_tr(0x2);\r\n}\r\nNOTIFY_MCA(DIE_MCA_MONARCH_LEAVE, regs, (long)&nd, 1);\r\nif (atomic_dec_return(&mca_count) > 0) {\r\nint i;\r\nfor_each_online_cpu(i) {\r\nif (cpumask_test_cpu(i, &mca_cpu)) {\r\nmonarch_cpu = i;\r\ncpumask_clear_cpu(i, &mca_cpu);\r\nwhile (monarch_cpu != -1)\r\ncpu_relax();\r\nia64_set_curr_task(cpu, previous_current);\r\nia64_mc_info.imi_rendez_checkin[cpu]\r\n= IA64_MCA_RENDEZ_CHECKIN_NOTDONE;\r\nreturn;\r\n}\r\n}\r\n}\r\nia64_set_curr_task(cpu, previous_current);\r\nia64_mc_info.imi_rendez_checkin[cpu] = IA64_MCA_RENDEZ_CHECKIN_NOTDONE;\r\nmonarch_cpu = -1;\r\n}\r\nstatic irqreturn_t\r\nia64_mca_cmc_int_handler(int cmc_irq, void *arg)\r\n{\r\nstatic unsigned long cmc_history[CMC_HISTORY_LENGTH];\r\nstatic int index;\r\nstatic DEFINE_SPINLOCK(cmc_history_lock);\r\nIA64_MCA_DEBUG("%s: received interrupt vector = %#x on CPU %d\n",\r\n__func__, cmc_irq, smp_processor_id());\r\nlocal_irq_enable();\r\nspin_lock(&cmc_history_lock);\r\nif (!cmc_polling_enabled) {\r\nint i, count = 1;\r\nunsigned long now = jiffies;\r\nfor (i = 0; i < CMC_HISTORY_LENGTH; i++) {\r\nif (now - cmc_history[i] <= HZ)\r\ncount++;\r\n}\r\nIA64_MCA_DEBUG(KERN_INFO "CMC threshold %d/%d\n", count, CMC_HISTORY_LENGTH);\r\nif (count >= CMC_HISTORY_LENGTH) {\r\ncmc_polling_enabled = 1;\r\nspin_unlock(&cmc_history_lock);\r\nia64_mca_cmc_vector_disable(NULL);\r\nschedule_work(&cmc_disable_work);\r\nprintk(KERN_WARNING "WARNING: Switching to polling CMC handler; error records may be lost\n");\r\nmod_timer(&cmc_poll_timer, jiffies + CMC_POLL_INTERVAL);\r\ngoto out;\r\n} else {\r\ncmc_history[index++] = now;\r\nif (index == CMC_HISTORY_LENGTH)\r\nindex = 0;\r\n}\r\n}\r\nspin_unlock(&cmc_history_lock);\r\nout:\r\nia64_mca_log_sal_error_record(SAL_INFO_TYPE_CMC);\r\nlocal_irq_disable();\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t\r\nia64_mca_cmc_int_caller(int cmc_irq, void *arg)\r\n{\r\nstatic int start_count = -1;\r\nunsigned int cpuid;\r\ncpuid = smp_processor_id();\r\nif (start_count == -1)\r\nstart_count = IA64_LOG_COUNT(SAL_INFO_TYPE_CMC);\r\nia64_mca_cmc_int_handler(cmc_irq, arg);\r\ncpuid = cpumask_next(cpuid+1, cpu_online_mask);\r\nif (cpuid < nr_cpu_ids) {\r\nplatform_send_ipi(cpuid, IA64_CMCP_VECTOR, IA64_IPI_DM_INT, 0);\r\n} else {\r\nif (start_count == IA64_LOG_COUNT(SAL_INFO_TYPE_CMC)) {\r\nprintk(KERN_WARNING "Returning to interrupt driven CMC handler\n");\r\nschedule_work(&cmc_enable_work);\r\ncmc_polling_enabled = 0;\r\n} else {\r\nmod_timer(&cmc_poll_timer, jiffies + CMC_POLL_INTERVAL);\r\n}\r\nstart_count = -1;\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void\r\nia64_mca_cmc_poll (unsigned long dummy)\r\n{\r\nplatform_send_ipi(cpumask_first(cpu_online_mask), IA64_CMCP_VECTOR,\r\nIA64_IPI_DM_INT, 0);\r\n}\r\nstatic irqreturn_t\r\nia64_mca_cpe_int_caller(int cpe_irq, void *arg)\r\n{\r\nstatic int start_count = -1;\r\nstatic int poll_time = MIN_CPE_POLL_INTERVAL;\r\nunsigned int cpuid;\r\ncpuid = smp_processor_id();\r\nif (start_count == -1)\r\nstart_count = IA64_LOG_COUNT(SAL_INFO_TYPE_CPE);\r\nia64_mca_cpe_int_handler(cpe_irq, arg);\r\ncpuid = cpumask_next(cpuid+1, cpu_online_mask);\r\nif (cpuid < NR_CPUS) {\r\nplatform_send_ipi(cpuid, IA64_CPEP_VECTOR, IA64_IPI_DM_INT, 0);\r\n} else {\r\nif (start_count != IA64_LOG_COUNT(SAL_INFO_TYPE_CPE)) {\r\npoll_time = max(MIN_CPE_POLL_INTERVAL, poll_time / 2);\r\n} else if (cpe_vector < 0) {\r\npoll_time = min(MAX_CPE_POLL_INTERVAL, poll_time * 2);\r\n} else {\r\npoll_time = MIN_CPE_POLL_INTERVAL;\r\nprintk(KERN_WARNING "Returning to interrupt driven CPE handler\n");\r\nenable_irq(local_vector_to_irq(IA64_CPE_VECTOR));\r\ncpe_poll_enabled = 0;\r\n}\r\nif (cpe_poll_enabled)\r\nmod_timer(&cpe_poll_timer, jiffies + poll_time);\r\nstart_count = -1;\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void\r\nia64_mca_cpe_poll (unsigned long dummy)\r\n{\r\nplatform_send_ipi(cpumask_first(cpu_online_mask), IA64_CPEP_VECTOR,\r\nIA64_IPI_DM_INT, 0);\r\n}\r\nstatic int\r\ndefault_monarch_init_process(struct notifier_block *self, unsigned long val, void *data)\r\n{\r\nint c;\r\nstruct task_struct *g, *t;\r\nif (val != DIE_INIT_MONARCH_PROCESS)\r\nreturn NOTIFY_DONE;\r\n#ifdef CONFIG_KEXEC\r\nif (atomic_read(&kdump_in_progress))\r\nreturn NOTIFY_DONE;\r\n#endif\r\nBREAK_LOGLEVEL(console_loglevel);\r\nia64_mlogbuf_dump_from_init();\r\nprintk(KERN_ERR "Processes interrupted by INIT -");\r\nfor_each_online_cpu(c) {\r\nstruct ia64_sal_os_state *s;\r\nt = __va(__per_cpu_mca[c] + IA64_MCA_CPU_INIT_STACK_OFFSET);\r\ns = (struct ia64_sal_os_state *)((char *)t + MCA_SOS_OFFSET);\r\ng = s->prev_task;\r\nif (g) {\r\nif (g->pid)\r\nprintk(" %d", g->pid);\r\nelse\r\nprintk(" %d (cpu %d task 0x%p)", g->pid, task_cpu(g), g);\r\n}\r\n}\r\nprintk("\n\n");\r\nif (read_trylock(&tasklist_lock)) {\r\ndo_each_thread (g, t) {\r\nprintk("\nBacktrace of pid %d (%s)\n", t->pid, t->comm);\r\nshow_stack(t, NULL);\r\n} while_each_thread (g, t);\r\nread_unlock(&tasklist_lock);\r\n}\r\nRESTORE_LOGLEVEL(console_loglevel);\r\nreturn NOTIFY_DONE;\r\n}\r\nvoid\r\nia64_init_handler(struct pt_regs *regs, struct switch_stack *sw,\r\nstruct ia64_sal_os_state *sos)\r\n{\r\nstatic atomic_t slaves;\r\nstatic atomic_t monarchs;\r\nstruct task_struct *previous_current;\r\nint cpu = smp_processor_id();\r\nstruct ia64_mca_notify_die nd =\r\n{ .sos = sos, .monarch_cpu = &monarch_cpu };\r\nNOTIFY_INIT(DIE_INIT_ENTER, regs, (long)&nd, 0);\r\nmprintk(KERN_INFO "Entered OS INIT handler. PSP=%lx cpu=%d monarch=%ld\n",\r\nsos->proc_state_param, cpu, sos->monarch);\r\nsalinfo_log_wakeup(SAL_INFO_TYPE_INIT, NULL, 0, 0);\r\nprevious_current = ia64_mca_modify_original_stack(regs, sw, sos, "INIT");\r\nsos->os_status = IA64_INIT_RESUME;\r\nif (!sos->monarch && atomic_add_return(1, &slaves) == num_online_cpus()) {\r\nmprintk(KERN_WARNING "%s: Promoting cpu %d to monarch.\n",\r\n__func__, cpu);\r\natomic_dec(&slaves);\r\nsos->monarch = 1;\r\n}\r\nif (sos->monarch && atomic_add_return(1, &monarchs) > 1) {\r\nmprintk(KERN_WARNING "%s: Demoting cpu %d to slave.\n",\r\n__func__, cpu);\r\natomic_dec(&monarchs);\r\nsos->monarch = 0;\r\n}\r\nif (!sos->monarch) {\r\nia64_mc_info.imi_rendez_checkin[cpu] = IA64_MCA_RENDEZ_CHECKIN_INIT;\r\n#ifdef CONFIG_KEXEC\r\nwhile (monarch_cpu == -1 && !atomic_read(&kdump_in_progress))\r\nudelay(1000);\r\n#else\r\nwhile (monarch_cpu == -1)\r\ncpu_relax();\r\n#endif\r\nNOTIFY_INIT(DIE_INIT_SLAVE_ENTER, regs, (long)&nd, 1);\r\nNOTIFY_INIT(DIE_INIT_SLAVE_PROCESS, regs, (long)&nd, 1);\r\n#ifdef CONFIG_KEXEC\r\nwhile (monarch_cpu != -1 && !atomic_read(&kdump_in_progress))\r\nudelay(1000);\r\n#else\r\nwhile (monarch_cpu != -1)\r\ncpu_relax();\r\n#endif\r\nNOTIFY_INIT(DIE_INIT_SLAVE_LEAVE, regs, (long)&nd, 1);\r\nmprintk("Slave on cpu %d returning to normal service.\n", cpu);\r\nia64_set_curr_task(cpu, previous_current);\r\nia64_mc_info.imi_rendez_checkin[cpu] = IA64_MCA_RENDEZ_CHECKIN_NOTDONE;\r\natomic_dec(&slaves);\r\nreturn;\r\n}\r\nmonarch_cpu = cpu;\r\nNOTIFY_INIT(DIE_INIT_MONARCH_ENTER, regs, (long)&nd, 1);\r\nmprintk("Delaying for 5 seconds...\n");\r\nudelay(5*1000000);\r\nia64_wait_for_slaves(cpu, "INIT");\r\nNOTIFY_INIT(DIE_INIT_MONARCH_PROCESS, regs, (long)&nd, 1);\r\nNOTIFY_INIT(DIE_INIT_MONARCH_LEAVE, regs, (long)&nd, 1);\r\nmprintk("\nINIT dump complete. Monarch on cpu %d returning to normal service.\n", cpu);\r\natomic_dec(&monarchs);\r\nia64_set_curr_task(cpu, previous_current);\r\nmonarch_cpu = -1;\r\nreturn;\r\n}\r\nstatic int __init\r\nia64_mca_disable_cpe_polling(char *str)\r\n{\r\ncpe_poll_enabled = 0;\r\nreturn 1;\r\n}\r\nstatic void\r\nformat_mca_init_stack(void *mca_data, unsigned long offset,\r\nconst char *type, int cpu)\r\n{\r\nstruct task_struct *p = (struct task_struct *)((char *)mca_data + offset);\r\nstruct thread_info *ti;\r\nmemset(p, 0, KERNEL_STACK_SIZE);\r\nti = task_thread_info(p);\r\nti->flags = _TIF_MCA_INIT;\r\nti->preempt_count = 1;\r\nti->task = p;\r\nti->cpu = cpu;\r\np->stack = ti;\r\np->state = TASK_UNINTERRUPTIBLE;\r\ncpumask_set_cpu(cpu, &p->cpus_allowed);\r\nINIT_LIST_HEAD(&p->tasks);\r\np->parent = p->real_parent = p->group_leader = p;\r\nINIT_LIST_HEAD(&p->children);\r\nINIT_LIST_HEAD(&p->sibling);\r\nstrncpy(p->comm, type, sizeof(p->comm)-1);\r\n}\r\nstatic void * __ref mca_bootmem(void)\r\n{\r\nreturn __alloc_bootmem(sizeof(struct ia64_mca_cpu),\r\nKERNEL_STACK_SIZE, 0);\r\n}\r\nvoid\r\nia64_mca_cpu_init(void *cpu_data)\r\n{\r\nvoid *pal_vaddr;\r\nvoid *data;\r\nlong sz = sizeof(struct ia64_mca_cpu);\r\nint cpu = smp_processor_id();\r\nstatic int first_time = 1;\r\nif (__per_cpu_mca[cpu]) {\r\ndata = __va(__per_cpu_mca[cpu]);\r\n} else {\r\nif (first_time) {\r\ndata = mca_bootmem();\r\nfirst_time = 0;\r\n} else\r\ndata = (void *)__get_free_pages(GFP_KERNEL,\r\nget_order(sz));\r\nif (!data)\r\npanic("Could not allocate MCA memory for cpu %d\n",\r\ncpu);\r\n}\r\nformat_mca_init_stack(data, offsetof(struct ia64_mca_cpu, mca_stack),\r\n"MCA", cpu);\r\nformat_mca_init_stack(data, offsetof(struct ia64_mca_cpu, init_stack),\r\n"INIT", cpu);\r\n__this_cpu_write(ia64_mca_data, (__per_cpu_mca[cpu] = __pa(data)));\r\n__this_cpu_write(ia64_mca_per_cpu_pte,\r\npte_val(mk_pte_phys(__pa(cpu_data), PAGE_KERNEL)));\r\npal_vaddr = efi_get_pal_addr();\r\nif (!pal_vaddr)\r\nreturn;\r\n__this_cpu_write(ia64_mca_pal_base,\r\nGRANULEROUNDDOWN((unsigned long) pal_vaddr));\r\n__this_cpu_write(ia64_mca_pal_pte, pte_val(mk_pte_phys(__pa(pal_vaddr),\r\nPAGE_KERNEL)));\r\n}\r\nstatic int ia64_mca_cpu_online(unsigned int cpu)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (!cmc_polling_enabled)\r\nia64_mca_cmc_vector_enable(NULL);\r\nlocal_irq_restore(flags);\r\nreturn 0;\r\n}\r\nvoid __init\r\nia64_mca_init(void)\r\n{\r\nia64_fptr_t *init_hldlr_ptr_monarch = (ia64_fptr_t *)ia64_os_init_dispatch_monarch;\r\nia64_fptr_t *init_hldlr_ptr_slave = (ia64_fptr_t *)ia64_os_init_dispatch_slave;\r\nia64_fptr_t *mca_hldlr_ptr = (ia64_fptr_t *)ia64_os_mca_dispatch;\r\nint i;\r\nlong rc;\r\nstruct ia64_sal_retval isrv;\r\nunsigned long timeout = IA64_MCA_RENDEZ_TIMEOUT;\r\nstatic struct notifier_block default_init_monarch_nb = {\r\n.notifier_call = default_monarch_init_process,\r\n.priority = 0\r\n};\r\nIA64_MCA_DEBUG("%s: begin\n", __func__);\r\nfor(i = 0 ; i < NR_CPUS; i++)\r\nia64_mc_info.imi_rendez_checkin[i] = IA64_MCA_RENDEZ_CHECKIN_NOTDONE;\r\nwhile (1) {\r\nisrv = ia64_sal_mc_set_params(SAL_MC_PARAM_RENDEZ_INT,\r\nSAL_MC_PARAM_MECHANISM_INT,\r\nIA64_MCA_RENDEZ_VECTOR,\r\ntimeout,\r\nSAL_MC_PARAM_RZ_ALWAYS);\r\nrc = isrv.status;\r\nif (rc == 0)\r\nbreak;\r\nif (rc == -2) {\r\nprintk(KERN_INFO "Increasing MCA rendezvous timeout from "\r\n"%ld to %ld milliseconds\n", timeout, isrv.v0);\r\ntimeout = isrv.v0;\r\nNOTIFY_MCA(DIE_MCA_NEW_TIMEOUT, NULL, timeout, 0);\r\ncontinue;\r\n}\r\nprintk(KERN_ERR "Failed to register rendezvous interrupt "\r\n"with SAL (status %ld)\n", rc);\r\nreturn;\r\n}\r\nisrv = ia64_sal_mc_set_params(SAL_MC_PARAM_RENDEZ_WAKEUP,\r\nSAL_MC_PARAM_MECHANISM_INT,\r\nIA64_MCA_WAKEUP_VECTOR,\r\n0, 0);\r\nrc = isrv.status;\r\nif (rc) {\r\nprintk(KERN_ERR "Failed to register wakeup interrupt with SAL "\r\n"(status %ld)\n", rc);\r\nreturn;\r\n}\r\nIA64_MCA_DEBUG("%s: registered MCA rendezvous spinloop and wakeup mech.\n", __func__);\r\nia64_mc_info.imi_mca_handler = ia64_tpa(mca_hldlr_ptr->fp);\r\nia64_mc_info.imi_mca_handler_size = 0;\r\nif ((rc = ia64_sal_set_vectors(SAL_VECTOR_OS_MCA,\r\nia64_mc_info.imi_mca_handler,\r\nia64_tpa(mca_hldlr_ptr->gp),\r\nia64_mc_info.imi_mca_handler_size,\r\n0, 0, 0)))\r\n{\r\nprintk(KERN_ERR "Failed to register OS MCA handler with SAL "\r\n"(status %ld)\n", rc);\r\nreturn;\r\n}\r\nIA64_MCA_DEBUG("%s: registered OS MCA handler with SAL at 0x%lx, gp = 0x%lx\n", __func__,\r\nia64_mc_info.imi_mca_handler, ia64_tpa(mca_hldlr_ptr->gp));\r\nia64_mc_info.imi_monarch_init_handler = ia64_tpa(init_hldlr_ptr_monarch->fp);\r\nia64_mc_info.imi_monarch_init_handler_size = 0;\r\nia64_mc_info.imi_slave_init_handler = ia64_tpa(init_hldlr_ptr_slave->fp);\r\nia64_mc_info.imi_slave_init_handler_size = 0;\r\nIA64_MCA_DEBUG("%s: OS INIT handler at %lx\n", __func__,\r\nia64_mc_info.imi_monarch_init_handler);\r\nif ((rc = ia64_sal_set_vectors(SAL_VECTOR_OS_INIT,\r\nia64_mc_info.imi_monarch_init_handler,\r\nia64_tpa(ia64_getreg(_IA64_REG_GP)),\r\nia64_mc_info.imi_monarch_init_handler_size,\r\nia64_mc_info.imi_slave_init_handler,\r\nia64_tpa(ia64_getreg(_IA64_REG_GP)),\r\nia64_mc_info.imi_slave_init_handler_size)))\r\n{\r\nprintk(KERN_ERR "Failed to register m/s INIT handlers with SAL "\r\n"(status %ld)\n", rc);\r\nreturn;\r\n}\r\nif (register_die_notifier(&default_init_monarch_nb)) {\r\nprintk(KERN_ERR "Failed to register default monarch INIT process\n");\r\nreturn;\r\n}\r\nIA64_MCA_DEBUG("%s: registered OS INIT handler with SAL\n", __func__);\r\nia64_log_init(SAL_INFO_TYPE_MCA);\r\nia64_log_init(SAL_INFO_TYPE_INIT);\r\nia64_log_init(SAL_INFO_TYPE_CMC);\r\nia64_log_init(SAL_INFO_TYPE_CPE);\r\nmca_init = 1;\r\nprintk(KERN_INFO "MCA related initialization done\n");\r\n}\r\nvoid __init ia64_mca_irq_init(void)\r\n{\r\nregister_percpu_irq(IA64_CMC_VECTOR, &cmci_irqaction);\r\nregister_percpu_irq(IA64_CMCP_VECTOR, &cmcp_irqaction);\r\nia64_mca_cmc_vector_setup();\r\nregister_percpu_irq(IA64_MCA_RENDEZ_VECTOR, &mca_rdzv_irqaction);\r\nregister_percpu_irq(IA64_MCA_WAKEUP_VECTOR, &mca_wkup_irqaction);\r\n#ifdef CONFIG_ACPI\r\nregister_percpu_irq(IA64_CPEP_VECTOR, &mca_cpep_irqaction);\r\n#endif\r\n}\r\nstatic int __init\r\nia64_mca_late_init(void)\r\n{\r\nif (!mca_init)\r\nreturn 0;\r\nsetup_timer(&cmc_poll_timer, ia64_mca_cmc_poll, 0UL);\r\ncmc_polling_enabled = 0;\r\ncpuhp_setup_state(CPUHP_AP_ONLINE_DYN, "ia64/mca:online",\r\nia64_mca_cpu_online, NULL);\r\nIA64_MCA_DEBUG("%s: CMCI/P setup and enabled.\n", __func__);\r\n#ifdef CONFIG_ACPI\r\ncpe_vector = acpi_request_vector(ACPI_INTERRUPT_CPEI);\r\nsetup_timer(&cpe_poll_timer, ia64_mca_cpe_poll, 0UL);\r\n{\r\nunsigned int irq;\r\nif (cpe_vector >= 0) {\r\nirq = local_vector_to_irq(cpe_vector);\r\nif (irq > 0) {\r\ncpe_poll_enabled = 0;\r\nirq_set_status_flags(irq, IRQ_PER_CPU);\r\nsetup_irq(irq, &mca_cpe_irqaction);\r\nia64_cpe_irq = irq;\r\nia64_mca_register_cpev(cpe_vector);\r\nIA64_MCA_DEBUG("%s: CPEI/P setup and enabled.\n",\r\n__func__);\r\nreturn 0;\r\n}\r\nprintk(KERN_ERR "%s: Failed to find irq for CPE "\r\n"interrupt handler, vector %d\n",\r\n__func__, cpe_vector);\r\n}\r\nif (cpe_poll_enabled) {\r\nia64_mca_cpe_poll(0UL);\r\nIA64_MCA_DEBUG("%s: CPEP setup and enabled.\n", __func__);\r\n}\r\n}\r\n#endif\r\nreturn 0;\r\n}
