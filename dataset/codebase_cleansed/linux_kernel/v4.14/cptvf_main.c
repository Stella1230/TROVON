static void vq_work_handler(unsigned long data)\r\n{\r\nstruct cptvf_wqe_info *cwqe_info = (struct cptvf_wqe_info *)data;\r\nstruct cptvf_wqe *cwqe = &cwqe_info->vq_wqe[0];\r\nvq_post_process(cwqe->cptvf, cwqe->qno);\r\n}\r\nstatic int init_worker_threads(struct cpt_vf *cptvf)\r\n{\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nstruct cptvf_wqe_info *cwqe_info;\r\nint i;\r\ncwqe_info = kzalloc(sizeof(*cwqe_info), GFP_KERNEL);\r\nif (!cwqe_info)\r\nreturn -ENOMEM;\r\nif (cptvf->nr_queues) {\r\ndev_info(&pdev->dev, "Creating VQ worker threads (%d)\n",\r\ncptvf->nr_queues);\r\n}\r\nfor (i = 0; i < cptvf->nr_queues; i++) {\r\ntasklet_init(&cwqe_info->vq_wqe[i].twork, vq_work_handler,\r\n(u64)cwqe_info);\r\ncwqe_info->vq_wqe[i].qno = i;\r\ncwqe_info->vq_wqe[i].cptvf = cptvf;\r\n}\r\ncptvf->wqe_info = cwqe_info;\r\nreturn 0;\r\n}\r\nstatic void cleanup_worker_threads(struct cpt_vf *cptvf)\r\n{\r\nstruct cptvf_wqe_info *cwqe_info;\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nint i;\r\ncwqe_info = (struct cptvf_wqe_info *)cptvf->wqe_info;\r\nif (!cwqe_info)\r\nreturn;\r\nif (cptvf->nr_queues) {\r\ndev_info(&pdev->dev, "Cleaning VQ worker threads (%u)\n",\r\ncptvf->nr_queues);\r\n}\r\nfor (i = 0; i < cptvf->nr_queues; i++)\r\ntasklet_kill(&cwqe_info->vq_wqe[i].twork);\r\nkzfree(cwqe_info);\r\ncptvf->wqe_info = NULL;\r\n}\r\nstatic void free_pending_queues(struct pending_qinfo *pqinfo)\r\n{\r\nint i;\r\nstruct pending_queue *queue;\r\nfor_each_pending_queue(pqinfo, queue, i) {\r\nif (!queue->head)\r\ncontinue;\r\nkzfree((queue->head));\r\nqueue->front = 0;\r\nqueue->rear = 0;\r\nreturn;\r\n}\r\npqinfo->qlen = 0;\r\npqinfo->nr_queues = 0;\r\n}\r\nstatic int alloc_pending_queues(struct pending_qinfo *pqinfo, u32 qlen,\r\nu32 nr_queues)\r\n{\r\nu32 i;\r\nsize_t size;\r\nint ret;\r\nstruct pending_queue *queue = NULL;\r\npqinfo->nr_queues = nr_queues;\r\npqinfo->qlen = qlen;\r\nsize = (qlen * sizeof(struct pending_entry));\r\nfor_each_pending_queue(pqinfo, queue, i) {\r\nqueue->head = kzalloc((size), GFP_KERNEL);\r\nif (!queue->head) {\r\nret = -ENOMEM;\r\ngoto pending_qfail;\r\n}\r\nqueue->front = 0;\r\nqueue->rear = 0;\r\natomic64_set((&queue->pending_count), (0));\r\nspin_lock_init(&queue->lock);\r\n}\r\nreturn 0;\r\npending_qfail:\r\nfree_pending_queues(pqinfo);\r\nreturn ret;\r\n}\r\nstatic int init_pending_queues(struct cpt_vf *cptvf, u32 qlen, u32 nr_queues)\r\n{\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nint ret;\r\nif (!nr_queues)\r\nreturn 0;\r\nret = alloc_pending_queues(&cptvf->pqinfo, qlen, nr_queues);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to setup pending queues (%u)\n",\r\nnr_queues);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cleanup_pending_queues(struct cpt_vf *cptvf)\r\n{\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nif (!cptvf->nr_queues)\r\nreturn;\r\ndev_info(&pdev->dev, "Cleaning VQ pending queue (%u)\n",\r\ncptvf->nr_queues);\r\nfree_pending_queues(&cptvf->pqinfo);\r\n}\r\nstatic void free_command_queues(struct cpt_vf *cptvf,\r\nstruct command_qinfo *cqinfo)\r\n{\r\nint i;\r\nstruct command_queue *queue = NULL;\r\nstruct command_chunk *chunk = NULL;\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nstruct hlist_node *node;\r\nfor (i = 0; i < cptvf->nr_queues; i++) {\r\nqueue = &cqinfo->queue[i];\r\nif (hlist_empty(&cqinfo->queue[i].chead))\r\ncontinue;\r\nhlist_for_each_entry_safe(chunk, node, &cqinfo->queue[i].chead,\r\nnextchunk) {\r\ndma_free_coherent(&pdev->dev, chunk->size,\r\nchunk->head,\r\nchunk->dma_addr);\r\nchunk->head = NULL;\r\nchunk->dma_addr = 0;\r\nhlist_del(&chunk->nextchunk);\r\nkzfree(chunk);\r\n}\r\nqueue->nchunks = 0;\r\nqueue->idx = 0;\r\n}\r\ncqinfo->cmd_size = 0;\r\n}\r\nstatic int alloc_command_queues(struct cpt_vf *cptvf,\r\nstruct command_qinfo *cqinfo, size_t cmd_size,\r\nu32 qlen)\r\n{\r\nint i;\r\nsize_t q_size;\r\nstruct command_queue *queue = NULL;\r\nstruct pci_dev *pdev = cptvf->pdev;\r\ncqinfo->cmd_size = cmd_size;\r\ncptvf->qsize = min(qlen, cqinfo->qchunksize) *\r\nCPT_NEXT_CHUNK_PTR_SIZE + 1;\r\nq_size = qlen * cqinfo->cmd_size;\r\nfor (i = 0; i < cptvf->nr_queues; i++) {\r\nsize_t c_size = 0;\r\nsize_t rem_q_size = q_size;\r\nstruct command_chunk *curr = NULL, *first = NULL, *last = NULL;\r\nu32 qcsize_bytes = cqinfo->qchunksize * cqinfo->cmd_size;\r\nqueue = &cqinfo->queue[i];\r\nINIT_HLIST_HEAD(&cqinfo->queue[i].chead);\r\ndo {\r\ncurr = kzalloc(sizeof(*curr), GFP_KERNEL);\r\nif (!curr)\r\ngoto cmd_qfail;\r\nc_size = (rem_q_size > qcsize_bytes) ? qcsize_bytes :\r\nrem_q_size;\r\ncurr->head = (u8 *)dma_zalloc_coherent(&pdev->dev,\r\nc_size + CPT_NEXT_CHUNK_PTR_SIZE,\r\n&curr->dma_addr, GFP_KERNEL);\r\nif (!curr->head) {\r\ndev_err(&pdev->dev, "Command Q (%d) chunk (%d) allocation failed\n",\r\ni, queue->nchunks);\r\nkfree(curr);\r\ngoto cmd_qfail;\r\n}\r\ncurr->size = c_size;\r\nif (queue->nchunks == 0) {\r\nhlist_add_head(&curr->nextchunk,\r\n&cqinfo->queue[i].chead);\r\nfirst = curr;\r\n} else {\r\nhlist_add_behind(&curr->nextchunk,\r\n&last->nextchunk);\r\n}\r\nqueue->nchunks++;\r\nrem_q_size -= c_size;\r\nif (last)\r\n*((u64 *)(&last->head[last->size])) = (u64)curr->dma_addr;\r\nlast = curr;\r\n} while (rem_q_size);\r\ncurr = first;\r\n*((u64 *)(&last->head[last->size])) = (u64)curr->dma_addr;\r\nqueue->qhead = curr;\r\nspin_lock_init(&queue->lock);\r\n}\r\nreturn 0;\r\ncmd_qfail:\r\nfree_command_queues(cptvf, cqinfo);\r\nreturn -ENOMEM;\r\n}\r\nstatic int init_command_queues(struct cpt_vf *cptvf, u32 qlen)\r\n{\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nint ret;\r\nret = alloc_command_queues(cptvf, &cptvf->cqinfo, CPT_INST_SIZE,\r\nqlen);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to allocate AE command queues (%u)\n",\r\ncptvf->nr_queues);\r\nreturn ret;\r\n}\r\nreturn ret;\r\n}\r\nstatic void cleanup_command_queues(struct cpt_vf *cptvf)\r\n{\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nif (!cptvf->nr_queues)\r\nreturn;\r\ndev_info(&pdev->dev, "Cleaning VQ command queue (%u)\n",\r\ncptvf->nr_queues);\r\nfree_command_queues(cptvf, &cptvf->cqinfo);\r\n}\r\nstatic void cptvf_sw_cleanup(struct cpt_vf *cptvf)\r\n{\r\ncleanup_worker_threads(cptvf);\r\ncleanup_pending_queues(cptvf);\r\ncleanup_command_queues(cptvf);\r\n}\r\nstatic int cptvf_sw_init(struct cpt_vf *cptvf, u32 qlen, u32 nr_queues)\r\n{\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nint ret = 0;\r\nu32 max_dev_queues = 0;\r\nmax_dev_queues = CPT_NUM_QS_PER_VF;\r\nnr_queues = min_t(u32, nr_queues, max_dev_queues);\r\ncptvf->nr_queues = nr_queues;\r\nret = init_command_queues(cptvf, qlen);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Failed to setup command queues (%u)\n",\r\nnr_queues);\r\nreturn ret;\r\n}\r\nret = init_pending_queues(cptvf, qlen, nr_queues);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Failed to setup pending queues (%u)\n",\r\nnr_queues);\r\ngoto setup_pqfail;\r\n}\r\nret = init_worker_threads(cptvf);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Failed to setup worker threads\n");\r\ngoto init_work_fail;\r\n}\r\nreturn 0;\r\ninit_work_fail:\r\ncleanup_worker_threads(cptvf);\r\ncleanup_pending_queues(cptvf);\r\nsetup_pqfail:\r\ncleanup_command_queues(cptvf);\r\nreturn ret;\r\n}\r\nstatic void cptvf_free_irq_affinity(struct cpt_vf *cptvf, int vec)\r\n{\r\nirq_set_affinity_hint(pci_irq_vector(cptvf->pdev, vec), NULL);\r\nfree_cpumask_var(cptvf->affinity_mask[vec]);\r\n}\r\nstatic void cptvf_write_vq_ctl(struct cpt_vf *cptvf, bool val)\r\n{\r\nunion cptx_vqx_ctl vqx_ctl;\r\nvqx_ctl.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_CTL(0, 0));\r\nvqx_ctl.s.ena = val;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_CTL(0, 0), vqx_ctl.u);\r\n}\r\nvoid cptvf_write_vq_doorbell(struct cpt_vf *cptvf, u32 val)\r\n{\r\nunion cptx_vqx_doorbell vqx_dbell;\r\nvqx_dbell.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_DOORBELL(0, 0));\r\nvqx_dbell.s.dbell_cnt = val * 8;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_DOORBELL(0, 0),\r\nvqx_dbell.u);\r\n}\r\nstatic void cptvf_write_vq_inprog(struct cpt_vf *cptvf, u8 val)\r\n{\r\nunion cptx_vqx_inprog vqx_inprg;\r\nvqx_inprg.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_INPROG(0, 0));\r\nvqx_inprg.s.inflight = val;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_INPROG(0, 0), vqx_inprg.u);\r\n}\r\nstatic void cptvf_write_vq_done_numwait(struct cpt_vf *cptvf, u32 val)\r\n{\r\nunion cptx_vqx_done_wait vqx_dwait;\r\nvqx_dwait.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_DONE_WAIT(0, 0));\r\nvqx_dwait.s.num_wait = val;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_WAIT(0, 0),\r\nvqx_dwait.u);\r\n}\r\nstatic void cptvf_write_vq_done_timewait(struct cpt_vf *cptvf, u16 time)\r\n{\r\nunion cptx_vqx_done_wait vqx_dwait;\r\nvqx_dwait.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_DONE_WAIT(0, 0));\r\nvqx_dwait.s.time_wait = time;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_WAIT(0, 0),\r\nvqx_dwait.u);\r\n}\r\nstatic void cptvf_enable_swerr_interrupts(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_misc_ena_w1s vqx_misc_ena;\r\nvqx_misc_ena.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_ENA_W1S(0, 0));\r\nvqx_misc_ena.s.swerr = 1;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_ENA_W1S(0, 0),\r\nvqx_misc_ena.u);\r\n}\r\nstatic void cptvf_enable_mbox_interrupts(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_misc_ena_w1s vqx_misc_ena;\r\nvqx_misc_ena.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_ENA_W1S(0, 0));\r\nvqx_misc_ena.s.mbox = 1;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_ENA_W1S(0, 0),\r\nvqx_misc_ena.u);\r\n}\r\nstatic void cptvf_enable_done_interrupts(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_done_ena_w1s vqx_done_ena;\r\nvqx_done_ena.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_DONE_ENA_W1S(0, 0));\r\nvqx_done_ena.s.done = 1;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_ENA_W1S(0, 0),\r\nvqx_done_ena.u);\r\n}\r\nstatic void cptvf_clear_dovf_intr(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_misc_int vqx_misc_int;\r\nvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_INT(0, 0));\r\nvqx_misc_int.s.dovf = 1;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\r\nvqx_misc_int.u);\r\n}\r\nstatic void cptvf_clear_irde_intr(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_misc_int vqx_misc_int;\r\nvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_INT(0, 0));\r\nvqx_misc_int.s.irde = 1;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\r\nvqx_misc_int.u);\r\n}\r\nstatic void cptvf_clear_nwrp_intr(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_misc_int vqx_misc_int;\r\nvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_INT(0, 0));\r\nvqx_misc_int.s.nwrp = 1;\r\ncpt_write_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_INT(0, 0), vqx_misc_int.u);\r\n}\r\nstatic void cptvf_clear_mbox_intr(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_misc_int vqx_misc_int;\r\nvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_INT(0, 0));\r\nvqx_misc_int.s.mbox = 1;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\r\nvqx_misc_int.u);\r\n}\r\nstatic void cptvf_clear_swerr_intr(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_misc_int vqx_misc_int;\r\nvqx_misc_int.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_MISC_INT(0, 0));\r\nvqx_misc_int.s.swerr = 1;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0),\r\nvqx_misc_int.u);\r\n}\r\nstatic u64 cptvf_read_vf_misc_intr_status(struct cpt_vf *cptvf)\r\n{\r\nreturn cpt_read_csr64(cptvf->reg_base, CPTX_VQX_MISC_INT(0, 0));\r\n}\r\nstatic irqreturn_t cptvf_misc_intr_handler(int irq, void *cptvf_irq)\r\n{\r\nstruct cpt_vf *cptvf = (struct cpt_vf *)cptvf_irq;\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nu64 intr;\r\nintr = cptvf_read_vf_misc_intr_status(cptvf);\r\nif (likely(intr & CPT_VF_INTR_MBOX_MASK)) {\r\ndev_dbg(&pdev->dev, "Mailbox interrupt 0x%llx on CPT VF %d\n",\r\nintr, cptvf->vfid);\r\ncptvf_handle_mbox_intr(cptvf);\r\ncptvf_clear_mbox_intr(cptvf);\r\n} else if (unlikely(intr & CPT_VF_INTR_DOVF_MASK)) {\r\ncptvf_clear_dovf_intr(cptvf);\r\ncptvf_write_vq_doorbell(cptvf, 0);\r\ndev_err(&pdev->dev, "Doorbell overflow error interrupt 0x%llx on CPT VF %d\n",\r\nintr, cptvf->vfid);\r\n} else if (unlikely(intr & CPT_VF_INTR_IRDE_MASK)) {\r\ncptvf_clear_irde_intr(cptvf);\r\ndev_err(&pdev->dev, "Instruction NCB read error interrupt 0x%llx on CPT VF %d\n",\r\nintr, cptvf->vfid);\r\n} else if (unlikely(intr & CPT_VF_INTR_NWRP_MASK)) {\r\ncptvf_clear_nwrp_intr(cptvf);\r\ndev_err(&pdev->dev, "NCB response write error interrupt 0x%llx on CPT VF %d\n",\r\nintr, cptvf->vfid);\r\n} else if (unlikely(intr & CPT_VF_INTR_SERR_MASK)) {\r\ncptvf_clear_swerr_intr(cptvf);\r\ndev_err(&pdev->dev, "Software error interrupt 0x%llx on CPT VF %d\n",\r\nintr, cptvf->vfid);\r\n} else {\r\ndev_err(&pdev->dev, "Unhandled interrupt in CPT VF %d\n",\r\ncptvf->vfid);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline struct cptvf_wqe *get_cptvf_vq_wqe(struct cpt_vf *cptvf,\r\nint qno)\r\n{\r\nstruct cptvf_wqe_info *nwqe_info;\r\nif (unlikely(qno >= cptvf->nr_queues))\r\nreturn NULL;\r\nnwqe_info = (struct cptvf_wqe_info *)cptvf->wqe_info;\r\nreturn &nwqe_info->vq_wqe[qno];\r\n}\r\nstatic inline u32 cptvf_read_vq_done_count(struct cpt_vf *cptvf)\r\n{\r\nunion cptx_vqx_done vqx_done;\r\nvqx_done.u = cpt_read_csr64(cptvf->reg_base, CPTX_VQX_DONE(0, 0));\r\nreturn vqx_done.s.done;\r\n}\r\nstatic inline void cptvf_write_vq_done_ack(struct cpt_vf *cptvf,\r\nu32 ackcnt)\r\n{\r\nunion cptx_vqx_done_ack vqx_dack_cnt;\r\nvqx_dack_cnt.u = cpt_read_csr64(cptvf->reg_base,\r\nCPTX_VQX_DONE_ACK(0, 0));\r\nvqx_dack_cnt.s.done_ack = ackcnt;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_DONE_ACK(0, 0),\r\nvqx_dack_cnt.u);\r\n}\r\nstatic irqreturn_t cptvf_done_intr_handler(int irq, void *cptvf_irq)\r\n{\r\nstruct cpt_vf *cptvf = (struct cpt_vf *)cptvf_irq;\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nu32 intr = cptvf_read_vq_done_count(cptvf);\r\nif (intr) {\r\nstruct cptvf_wqe *wqe;\r\ncptvf_write_vq_done_ack(cptvf, intr);\r\nwqe = get_cptvf_vq_wqe(cptvf, 0);\r\nif (unlikely(!wqe)) {\r\ndev_err(&pdev->dev, "No work to schedule for VF (%d)",\r\ncptvf->vfid);\r\nreturn IRQ_NONE;\r\n}\r\ntasklet_hi_schedule(&wqe->twork);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void cptvf_set_irq_affinity(struct cpt_vf *cptvf, int vec)\r\n{\r\nstruct pci_dev *pdev = cptvf->pdev;\r\nint cpu;\r\nif (!zalloc_cpumask_var(&cptvf->affinity_mask[vec],\r\nGFP_KERNEL)) {\r\ndev_err(&pdev->dev, "Allocation failed for affinity_mask for VF %d",\r\ncptvf->vfid);\r\nreturn;\r\n}\r\ncpu = cptvf->vfid % num_online_cpus();\r\ncpumask_set_cpu(cpumask_local_spread(cpu, cptvf->node),\r\ncptvf->affinity_mask[vec]);\r\nirq_set_affinity_hint(pci_irq_vector(pdev, vec),\r\ncptvf->affinity_mask[vec]);\r\n}\r\nstatic void cptvf_write_vq_saddr(struct cpt_vf *cptvf, u64 val)\r\n{\r\nunion cptx_vqx_saddr vqx_saddr;\r\nvqx_saddr.u = val;\r\ncpt_write_csr64(cptvf->reg_base, CPTX_VQX_SADDR(0, 0), vqx_saddr.u);\r\n}\r\nvoid cptvf_device_init(struct cpt_vf *cptvf)\r\n{\r\nu64 base_addr = 0;\r\ncptvf_write_vq_ctl(cptvf, 0);\r\ncptvf_write_vq_doorbell(cptvf, 0);\r\ncptvf_write_vq_inprog(cptvf, 0);\r\nbase_addr = (u64)(cptvf->cqinfo.queue[0].qhead->dma_addr);\r\ncptvf_write_vq_saddr(cptvf, base_addr);\r\ncptvf_write_vq_done_timewait(cptvf, CPT_TIMER_THOLD);\r\ncptvf_write_vq_done_numwait(cptvf, 1);\r\ncptvf_write_vq_ctl(cptvf, 1);\r\ncptvf->flags |= CPT_FLAG_DEVICE_READY;\r\n}\r\nstatic int cptvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct cpt_vf *cptvf;\r\nint err;\r\ncptvf = devm_kzalloc(dev, sizeof(*cptvf), GFP_KERNEL);\r\nif (!cptvf)\r\nreturn -ENOMEM;\r\npci_set_drvdata(pdev, cptvf);\r\ncptvf->pdev = pdev;\r\nerr = pci_enable_device(pdev);\r\nif (err) {\r\ndev_err(dev, "Failed to enable PCI device\n");\r\npci_set_drvdata(pdev, NULL);\r\nreturn err;\r\n}\r\nerr = pci_request_regions(pdev, DRV_NAME);\r\nif (err) {\r\ndev_err(dev, "PCI request regions failed 0x%x\n", err);\r\ngoto cptvf_err_disable_device;\r\n}\r\ncptvf->flags |= CPT_FLAG_VF_DRIVER;\r\nerr = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));\r\nif (err) {\r\ndev_err(dev, "Unable to get usable DMA configuration\n");\r\ngoto cptvf_err_release_regions;\r\n}\r\nerr = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));\r\nif (err) {\r\ndev_err(dev, "Unable to get 48-bit DMA for consistent allocations\n");\r\ngoto cptvf_err_release_regions;\r\n}\r\ncptvf->reg_base = pcim_iomap(pdev, 0, 0);\r\nif (!cptvf->reg_base) {\r\ndev_err(dev, "Cannot map config register space, aborting\n");\r\nerr = -ENOMEM;\r\ngoto cptvf_err_release_regions;\r\n}\r\ncptvf->node = dev_to_node(&pdev->dev);\r\nerr = pci_alloc_irq_vectors(pdev, CPT_VF_MSIX_VECTORS,\r\nCPT_VF_MSIX_VECTORS, PCI_IRQ_MSIX);\r\nif (err < 0) {\r\ndev_err(dev, "Request for #%d msix vectors failed\n",\r\nCPT_VF_MSIX_VECTORS);\r\ngoto cptvf_err_release_regions;\r\n}\r\nerr = request_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC),\r\ncptvf_misc_intr_handler, 0, "CPT VF misc intr",\r\ncptvf);\r\nif (err) {\r\ndev_err(dev, "Request misc irq failed");\r\ngoto cptvf_free_vectors;\r\n}\r\ncptvf_enable_mbox_interrupts(cptvf);\r\ncptvf_enable_swerr_interrupts(cptvf);\r\nerr = cptvf_check_pf_ready(cptvf);\r\nif (err) {\r\ndev_err(dev, "PF not responding to READY msg");\r\ngoto cptvf_free_misc_irq;\r\n}\r\ncptvf->cqinfo.qchunksize = CPT_CMD_QCHUNK_SIZE;\r\nerr = cptvf_sw_init(cptvf, CPT_CMD_QLEN, CPT_NUM_QS_PER_VF);\r\nif (err) {\r\ndev_err(dev, "cptvf_sw_init() failed");\r\ngoto cptvf_free_misc_irq;\r\n}\r\nerr = cptvf_send_vq_size_msg(cptvf);\r\nif (err) {\r\ndev_err(dev, "PF not responding to QLEN msg");\r\ngoto cptvf_free_misc_irq;\r\n}\r\ncptvf_device_init(cptvf);\r\ncptvf->vfgrp = 1;\r\nerr = cptvf_send_vf_to_grp_msg(cptvf);\r\nif (err) {\r\ndev_err(dev, "PF not responding to VF_GRP msg");\r\ngoto cptvf_free_misc_irq;\r\n}\r\ncptvf->priority = 1;\r\nerr = cptvf_send_vf_priority_msg(cptvf);\r\nif (err) {\r\ndev_err(dev, "PF not responding to VF_PRIO msg");\r\ngoto cptvf_free_misc_irq;\r\n}\r\nerr = request_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_DONE),\r\ncptvf_done_intr_handler, 0, "CPT VF done intr",\r\ncptvf);\r\nif (err) {\r\ndev_err(dev, "Request done irq failed\n");\r\ngoto cptvf_free_misc_irq;\r\n}\r\ncptvf_enable_done_interrupts(cptvf);\r\ncptvf_set_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);\r\ncptvf_set_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);\r\nerr = cptvf_send_vf_up(cptvf);\r\nif (err) {\r\ndev_err(dev, "PF not responding to UP msg");\r\ngoto cptvf_free_irq_affinity;\r\n}\r\nerr = cvm_crypto_init(cptvf);\r\nif (err) {\r\ndev_err(dev, "Algorithm register failed\n");\r\ngoto cptvf_free_irq_affinity;\r\n}\r\nreturn 0;\r\ncptvf_free_irq_affinity:\r\ncptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);\r\ncptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);\r\ncptvf_free_misc_irq:\r\nfree_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC), cptvf);\r\ncptvf_free_vectors:\r\npci_free_irq_vectors(cptvf->pdev);\r\ncptvf_err_release_regions:\r\npci_release_regions(pdev);\r\ncptvf_err_disable_device:\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\nreturn err;\r\n}\r\nstatic void cptvf_remove(struct pci_dev *pdev)\r\n{\r\nstruct cpt_vf *cptvf = pci_get_drvdata(pdev);\r\nif (!cptvf) {\r\ndev_err(&pdev->dev, "Invalid CPT-VF device\n");\r\nreturn;\r\n}\r\nif (cptvf_send_vf_down(cptvf)) {\r\ndev_err(&pdev->dev, "PF not responding to DOWN msg");\r\n} else {\r\ncptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_DONE);\r\ncptvf_free_irq_affinity(cptvf, CPT_VF_INT_VEC_E_MISC);\r\nfree_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_DONE), cptvf);\r\nfree_irq(pci_irq_vector(pdev, CPT_VF_INT_VEC_E_MISC), cptvf);\r\npci_free_irq_vectors(cptvf->pdev);\r\ncptvf_sw_cleanup(cptvf);\r\npci_set_drvdata(pdev, NULL);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\ncvm_crypto_exit();\r\n}\r\n}\r\nstatic void cptvf_shutdown(struct pci_dev *pdev)\r\n{\r\ncptvf_remove(pdev);\r\n}
