static int mmu_topup_memory_cache(struct kvm_mmu_memory_cache *cache,\r\nint min, int max)\r\n{\r\nvoid *page;\r\nBUG_ON(max > KVM_NR_MEM_OBJS);\r\nif (cache->nobjs >= min)\r\nreturn 0;\r\nwhile (cache->nobjs < max) {\r\npage = (void *)__get_free_page(GFP_KERNEL);\r\nif (!page)\r\nreturn -ENOMEM;\r\ncache->objects[cache->nobjs++] = page;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc)\r\n{\r\nwhile (mc->nobjs)\r\nfree_page((unsigned long)mc->objects[--mc->nobjs]);\r\n}\r\nstatic void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)\r\n{\r\nvoid *p;\r\nBUG_ON(!mc || !mc->nobjs);\r\np = mc->objects[--mc->nobjs];\r\nreturn p;\r\n}\r\nvoid kvm_mmu_free_memory_caches(struct kvm_vcpu *vcpu)\r\n{\r\nmmu_free_memory_cache(&vcpu->arch.mmu_page_cache);\r\n}\r\nstatic void kvm_pgd_init(void *page)\r\n{\r\nunsigned long *p, *end;\r\nunsigned long entry;\r\n#ifdef __PAGETABLE_PMD_FOLDED\r\nentry = (unsigned long)invalid_pte_table;\r\n#else\r\nentry = (unsigned long)invalid_pmd_table;\r\n#endif\r\np = (unsigned long *)page;\r\nend = p + PTRS_PER_PGD;\r\ndo {\r\np[0] = entry;\r\np[1] = entry;\r\np[2] = entry;\r\np[3] = entry;\r\np[4] = entry;\r\np += 8;\r\np[-3] = entry;\r\np[-2] = entry;\r\np[-1] = entry;\r\n} while (p != end);\r\n}\r\npgd_t *kvm_pgd_alloc(void)\r\n{\r\npgd_t *ret;\r\nret = (pgd_t *)__get_free_pages(GFP_KERNEL, PGD_ORDER);\r\nif (ret)\r\nkvm_pgd_init(ret);\r\nreturn ret;\r\n}\r\nstatic pte_t *kvm_mips_walk_pgd(pgd_t *pgd, struct kvm_mmu_memory_cache *cache,\r\nunsigned long addr)\r\n{\r\npud_t *pud;\r\npmd_t *pmd;\r\npgd += pgd_index(addr);\r\nif (pgd_none(*pgd)) {\r\nBUG();\r\nreturn NULL;\r\n}\r\npud = pud_offset(pgd, addr);\r\nif (pud_none(*pud)) {\r\npmd_t *new_pmd;\r\nif (!cache)\r\nreturn NULL;\r\nnew_pmd = mmu_memory_cache_alloc(cache);\r\npmd_init((unsigned long)new_pmd,\r\n(unsigned long)invalid_pte_table);\r\npud_populate(NULL, pud, new_pmd);\r\n}\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd)) {\r\npte_t *new_pte;\r\nif (!cache)\r\nreturn NULL;\r\nnew_pte = mmu_memory_cache_alloc(cache);\r\nclear_page(new_pte);\r\npmd_populate_kernel(NULL, pmd, new_pte);\r\n}\r\nreturn pte_offset(pmd, addr);\r\n}\r\nstatic pte_t *kvm_mips_pte_for_gpa(struct kvm *kvm,\r\nstruct kvm_mmu_memory_cache *cache,\r\nunsigned long addr)\r\n{\r\nreturn kvm_mips_walk_pgd(kvm->arch.gpa_mm.pgd, cache, addr);\r\n}\r\nstatic bool kvm_mips_flush_gpa_pte(pte_t *pte, unsigned long start_gpa,\r\nunsigned long end_gpa)\r\n{\r\nint i_min = __pte_offset(start_gpa);\r\nint i_max = __pte_offset(end_gpa);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PTE - 1);\r\nint i;\r\nfor (i = i_min; i <= i_max; ++i) {\r\nif (!pte_present(pte[i]))\r\ncontinue;\r\nset_pte(pte + i, __pte(0));\r\n}\r\nreturn safe_to_remove;\r\n}\r\nstatic bool kvm_mips_flush_gpa_pmd(pmd_t *pmd, unsigned long start_gpa,\r\nunsigned long end_gpa)\r\n{\r\npte_t *pte;\r\nunsigned long end = ~0ul;\r\nint i_min = __pmd_offset(start_gpa);\r\nint i_max = __pmd_offset(end_gpa);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PMD - 1);\r\nint i;\r\nfor (i = i_min; i <= i_max; ++i, start_gpa = 0) {\r\nif (!pmd_present(pmd[i]))\r\ncontinue;\r\npte = pte_offset(pmd + i, 0);\r\nif (i == i_max)\r\nend = end_gpa;\r\nif (kvm_mips_flush_gpa_pte(pte, start_gpa, end)) {\r\npmd_clear(pmd + i);\r\npte_free_kernel(NULL, pte);\r\n} else {\r\nsafe_to_remove = false;\r\n}\r\n}\r\nreturn safe_to_remove;\r\n}\r\nstatic bool kvm_mips_flush_gpa_pud(pud_t *pud, unsigned long start_gpa,\r\nunsigned long end_gpa)\r\n{\r\npmd_t *pmd;\r\nunsigned long end = ~0ul;\r\nint i_min = __pud_offset(start_gpa);\r\nint i_max = __pud_offset(end_gpa);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PUD - 1);\r\nint i;\r\nfor (i = i_min; i <= i_max; ++i, start_gpa = 0) {\r\nif (!pud_present(pud[i]))\r\ncontinue;\r\npmd = pmd_offset(pud + i, 0);\r\nif (i == i_max)\r\nend = end_gpa;\r\nif (kvm_mips_flush_gpa_pmd(pmd, start_gpa, end)) {\r\npud_clear(pud + i);\r\npmd_free(NULL, pmd);\r\n} else {\r\nsafe_to_remove = false;\r\n}\r\n}\r\nreturn safe_to_remove;\r\n}\r\nstatic bool kvm_mips_flush_gpa_pgd(pgd_t *pgd, unsigned long start_gpa,\r\nunsigned long end_gpa)\r\n{\r\npud_t *pud;\r\nunsigned long end = ~0ul;\r\nint i_min = pgd_index(start_gpa);\r\nint i_max = pgd_index(end_gpa);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PGD - 1);\r\nint i;\r\nfor (i = i_min; i <= i_max; ++i, start_gpa = 0) {\r\nif (!pgd_present(pgd[i]))\r\ncontinue;\r\npud = pud_offset(pgd + i, 0);\r\nif (i == i_max)\r\nend = end_gpa;\r\nif (kvm_mips_flush_gpa_pud(pud, start_gpa, end)) {\r\npgd_clear(pgd + i);\r\npud_free(NULL, pud);\r\n} else {\r\nsafe_to_remove = false;\r\n}\r\n}\r\nreturn safe_to_remove;\r\n}\r\nbool kvm_mips_flush_gpa_pt(struct kvm *kvm, gfn_t start_gfn, gfn_t end_gfn)\r\n{\r\nreturn kvm_mips_flush_gpa_pgd(kvm->arch.gpa_mm.pgd,\r\nstart_gfn << PAGE_SHIFT,\r\nend_gfn << PAGE_SHIFT);\r\n}\r\nint kvm_mips_mkclean_gpa_pt(struct kvm *kvm, gfn_t start_gfn, gfn_t end_gfn)\r\n{\r\nreturn kvm_mips_mkclean_pgd(kvm->arch.gpa_mm.pgd,\r\nstart_gfn << PAGE_SHIFT,\r\nend_gfn << PAGE_SHIFT);\r\n}\r\nvoid kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,\r\nstruct kvm_memory_slot *slot,\r\ngfn_t gfn_offset, unsigned long mask)\r\n{\r\ngfn_t base_gfn = slot->base_gfn + gfn_offset;\r\ngfn_t start = base_gfn + __ffs(mask);\r\ngfn_t end = base_gfn + __fls(mask);\r\nkvm_mips_mkclean_gpa_pt(kvm, start, end);\r\n}\r\nstatic int kvm_mips_mkold_gpa_pt(struct kvm *kvm, gfn_t start_gfn,\r\ngfn_t end_gfn)\r\n{\r\nreturn kvm_mips_mkold_pgd(kvm->arch.gpa_mm.pgd,\r\nstart_gfn << PAGE_SHIFT,\r\nend_gfn << PAGE_SHIFT);\r\n}\r\nstatic int handle_hva_to_gpa(struct kvm *kvm,\r\nunsigned long start,\r\nunsigned long end,\r\nint (*handler)(struct kvm *kvm, gfn_t gfn,\r\ngpa_t gfn_end,\r\nstruct kvm_memory_slot *memslot,\r\nvoid *data),\r\nvoid *data)\r\n{\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nint ret = 0;\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nunsigned long hva_start, hva_end;\r\ngfn_t gfn, gfn_end;\r\nhva_start = max(start, memslot->userspace_addr);\r\nhva_end = min(end, memslot->userspace_addr +\r\n(memslot->npages << PAGE_SHIFT));\r\nif (hva_start >= hva_end)\r\ncontinue;\r\ngfn = hva_to_gfn_memslot(hva_start, memslot);\r\ngfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);\r\nret |= handler(kvm, gfn, gfn_end, memslot, data);\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_unmap_hva_handler(struct kvm *kvm, gfn_t gfn, gfn_t gfn_end,\r\nstruct kvm_memory_slot *memslot, void *data)\r\n{\r\nkvm_mips_flush_gpa_pt(kvm, gfn, gfn_end);\r\nreturn 1;\r\n}\r\nint kvm_unmap_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nunsigned long end = hva + PAGE_SIZE;\r\nhandle_hva_to_gpa(kvm, hva, end, &kvm_unmap_hva_handler, NULL);\r\nkvm_mips_callbacks->flush_shadow_all(kvm);\r\nreturn 0;\r\n}\r\nint kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nhandle_hva_to_gpa(kvm, start, end, &kvm_unmap_hva_handler, NULL);\r\nkvm_mips_callbacks->flush_shadow_all(kvm);\r\nreturn 0;\r\n}\r\nstatic int kvm_set_spte_handler(struct kvm *kvm, gfn_t gfn, gfn_t gfn_end,\r\nstruct kvm_memory_slot *memslot, void *data)\r\n{\r\ngpa_t gpa = gfn << PAGE_SHIFT;\r\npte_t hva_pte = *(pte_t *)data;\r\npte_t *gpa_pte = kvm_mips_pte_for_gpa(kvm, NULL, gpa);\r\npte_t old_pte;\r\nif (!gpa_pte)\r\nreturn 0;\r\nold_pte = *gpa_pte;\r\nif (memslot->flags & KVM_MEM_LOG_DIRTY_PAGES && !pte_dirty(old_pte))\r\nhva_pte = pte_mkclean(hva_pte);\r\nelse if (memslot->flags & KVM_MEM_READONLY)\r\nhva_pte = pte_wrprotect(hva_pte);\r\nset_pte(gpa_pte, hva_pte);\r\nif (!pte_present(old_pte) || !pte_young(old_pte))\r\nreturn 0;\r\nreturn !pte_present(hva_pte) ||\r\n!pte_young(hva_pte) ||\r\npte_pfn(old_pte) != pte_pfn(hva_pte) ||\r\n(pte_dirty(old_pte) && !pte_dirty(hva_pte));\r\n}\r\nvoid kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)\r\n{\r\nunsigned long end = hva + PAGE_SIZE;\r\nint ret;\r\nret = handle_hva_to_gpa(kvm, hva, end, &kvm_set_spte_handler, &pte);\r\nif (ret)\r\nkvm_mips_callbacks->flush_shadow_all(kvm);\r\n}\r\nstatic int kvm_age_hva_handler(struct kvm *kvm, gfn_t gfn, gfn_t gfn_end,\r\nstruct kvm_memory_slot *memslot, void *data)\r\n{\r\nreturn kvm_mips_mkold_gpa_pt(kvm, gfn, gfn_end);\r\n}\r\nstatic int kvm_test_age_hva_handler(struct kvm *kvm, gfn_t gfn, gfn_t gfn_end,\r\nstruct kvm_memory_slot *memslot, void *data)\r\n{\r\ngpa_t gpa = gfn << PAGE_SHIFT;\r\npte_t *gpa_pte = kvm_mips_pte_for_gpa(kvm, NULL, gpa);\r\nif (!gpa_pte)\r\nreturn 0;\r\nreturn pte_young(*gpa_pte);\r\n}\r\nint kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nreturn handle_hva_to_gpa(kvm, start, end, kvm_age_hva_handler, NULL);\r\n}\r\nint kvm_test_age_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn handle_hva_to_gpa(kvm, hva, hva, kvm_test_age_hva_handler, NULL);\r\n}\r\nstatic int _kvm_mips_map_page_fast(struct kvm_vcpu *vcpu, unsigned long gpa,\r\nbool write_fault,\r\npte_t *out_entry, pte_t *out_buddy)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\ngfn_t gfn = gpa >> PAGE_SHIFT;\r\npte_t *ptep;\r\nkvm_pfn_t pfn = 0;\r\nbool pfn_valid = false;\r\nint ret = 0;\r\nspin_lock(&kvm->mmu_lock);\r\nptep = kvm_mips_pte_for_gpa(kvm, NULL, gpa);\r\nif (!ptep || !pte_present(*ptep)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\nif (!pte_young(*ptep)) {\r\nset_pte(ptep, pte_mkyoung(*ptep));\r\npfn = pte_pfn(*ptep);\r\npfn_valid = true;\r\n}\r\nif (write_fault && !pte_dirty(*ptep)) {\r\nif (!pte_write(*ptep)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\nset_pte(ptep, pte_mkdirty(*ptep));\r\npfn = pte_pfn(*ptep);\r\nmark_page_dirty(kvm, gfn);\r\nkvm_set_pfn_dirty(pfn);\r\n}\r\nif (out_entry)\r\n*out_entry = *ptep;\r\nif (out_buddy)\r\n*out_buddy = *ptep_buddy(ptep);\r\nout:\r\nspin_unlock(&kvm->mmu_lock);\r\nif (pfn_valid)\r\nkvm_set_pfn_accessed(pfn);\r\nreturn ret;\r\n}\r\nstatic int kvm_mips_map_page(struct kvm_vcpu *vcpu, unsigned long gpa,\r\nbool write_fault,\r\npte_t *out_entry, pte_t *out_buddy)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;\r\ngfn_t gfn = gpa >> PAGE_SHIFT;\r\nint srcu_idx, err;\r\nkvm_pfn_t pfn;\r\npte_t *ptep, entry, old_pte;\r\nbool writeable;\r\nunsigned long prot_bits;\r\nunsigned long mmu_seq;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nerr = _kvm_mips_map_page_fast(vcpu, gpa, write_fault, out_entry,\r\nout_buddy);\r\nif (!err)\r\ngoto out;\r\nerr = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,\r\nKVM_NR_MEM_OBJS);\r\nif (err)\r\ngoto out;\r\nretry:\r\nmmu_seq = kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\npfn = gfn_to_pfn_prot(kvm, gfn, write_fault, &writeable);\r\nif (is_error_noslot_pfn(pfn)) {\r\nerr = -EFAULT;\r\ngoto out;\r\n}\r\nspin_lock(&kvm->mmu_lock);\r\nif (mmu_notifier_retry(kvm, mmu_seq)) {\r\nspin_unlock(&kvm->mmu_lock);\r\nkvm_release_pfn_clean(pfn);\r\ngoto retry;\r\n}\r\nptep = kvm_mips_pte_for_gpa(kvm, memcache, gpa);\r\nprot_bits = _PAGE_PRESENT | __READABLE | _page_cachable_default;\r\nif (writeable) {\r\nprot_bits |= _PAGE_WRITE;\r\nif (write_fault) {\r\nprot_bits |= __WRITEABLE;\r\nmark_page_dirty(kvm, gfn);\r\nkvm_set_pfn_dirty(pfn);\r\n}\r\n}\r\nentry = pfn_pte(pfn, __pgprot(prot_bits));\r\nold_pte = *ptep;\r\nset_pte(ptep, entry);\r\nerr = 0;\r\nif (out_entry)\r\n*out_entry = *ptep;\r\nif (out_buddy)\r\n*out_buddy = *ptep_buddy(ptep);\r\nspin_unlock(&kvm->mmu_lock);\r\nkvm_release_pfn_clean(pfn);\r\nkvm_set_pfn_accessed(pfn);\r\nout:\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\nreturn err;\r\n}\r\nstatic pte_t *kvm_trap_emul_pte_for_gva(struct kvm_vcpu *vcpu,\r\nunsigned long addr)\r\n{\r\nstruct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;\r\npgd_t *pgdp;\r\nint ret;\r\nret = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,\r\nKVM_NR_MEM_OBJS);\r\nif (ret)\r\nreturn NULL;\r\nif (KVM_GUEST_KERNEL_MODE(vcpu))\r\npgdp = vcpu->arch.guest_kernel_mm.pgd;\r\nelse\r\npgdp = vcpu->arch.guest_user_mm.pgd;\r\nreturn kvm_mips_walk_pgd(pgdp, memcache, addr);\r\n}\r\nvoid kvm_trap_emul_invalidate_gva(struct kvm_vcpu *vcpu, unsigned long addr,\r\nbool user)\r\n{\r\npgd_t *pgdp;\r\npte_t *ptep;\r\naddr &= PAGE_MASK << 1;\r\npgdp = vcpu->arch.guest_kernel_mm.pgd;\r\nptep = kvm_mips_walk_pgd(pgdp, NULL, addr);\r\nif (ptep) {\r\nptep[0] = pfn_pte(0, __pgprot(0));\r\nptep[1] = pfn_pte(0, __pgprot(0));\r\n}\r\nif (user) {\r\npgdp = vcpu->arch.guest_user_mm.pgd;\r\nptep = kvm_mips_walk_pgd(pgdp, NULL, addr);\r\nif (ptep) {\r\nptep[0] = pfn_pte(0, __pgprot(0));\r\nptep[1] = pfn_pte(0, __pgprot(0));\r\n}\r\n}\r\n}\r\nstatic bool kvm_mips_flush_gva_pte(pte_t *pte, unsigned long start_gva,\r\nunsigned long end_gva)\r\n{\r\nint i_min = __pte_offset(start_gva);\r\nint i_max = __pte_offset(end_gva);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PTE - 1);\r\nint i;\r\nif (safe_to_remove)\r\nreturn true;\r\nfor (i = i_min; i <= i_max; ++i) {\r\nif (!pte_present(pte[i]))\r\ncontinue;\r\nset_pte(pte + i, __pte(0));\r\n}\r\nreturn false;\r\n}\r\nstatic bool kvm_mips_flush_gva_pmd(pmd_t *pmd, unsigned long start_gva,\r\nunsigned long end_gva)\r\n{\r\npte_t *pte;\r\nunsigned long end = ~0ul;\r\nint i_min = __pmd_offset(start_gva);\r\nint i_max = __pmd_offset(end_gva);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PMD - 1);\r\nint i;\r\nfor (i = i_min; i <= i_max; ++i, start_gva = 0) {\r\nif (!pmd_present(pmd[i]))\r\ncontinue;\r\npte = pte_offset(pmd + i, 0);\r\nif (i == i_max)\r\nend = end_gva;\r\nif (kvm_mips_flush_gva_pte(pte, start_gva, end)) {\r\npmd_clear(pmd + i);\r\npte_free_kernel(NULL, pte);\r\n} else {\r\nsafe_to_remove = false;\r\n}\r\n}\r\nreturn safe_to_remove;\r\n}\r\nstatic bool kvm_mips_flush_gva_pud(pud_t *pud, unsigned long start_gva,\r\nunsigned long end_gva)\r\n{\r\npmd_t *pmd;\r\nunsigned long end = ~0ul;\r\nint i_min = __pud_offset(start_gva);\r\nint i_max = __pud_offset(end_gva);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PUD - 1);\r\nint i;\r\nfor (i = i_min; i <= i_max; ++i, start_gva = 0) {\r\nif (!pud_present(pud[i]))\r\ncontinue;\r\npmd = pmd_offset(pud + i, 0);\r\nif (i == i_max)\r\nend = end_gva;\r\nif (kvm_mips_flush_gva_pmd(pmd, start_gva, end)) {\r\npud_clear(pud + i);\r\npmd_free(NULL, pmd);\r\n} else {\r\nsafe_to_remove = false;\r\n}\r\n}\r\nreturn safe_to_remove;\r\n}\r\nstatic bool kvm_mips_flush_gva_pgd(pgd_t *pgd, unsigned long start_gva,\r\nunsigned long end_gva)\r\n{\r\npud_t *pud;\r\nunsigned long end = ~0ul;\r\nint i_min = pgd_index(start_gva);\r\nint i_max = pgd_index(end_gva);\r\nbool safe_to_remove = (i_min == 0 && i_max == PTRS_PER_PGD - 1);\r\nint i;\r\nfor (i = i_min; i <= i_max; ++i, start_gva = 0) {\r\nif (!pgd_present(pgd[i]))\r\ncontinue;\r\npud = pud_offset(pgd + i, 0);\r\nif (i == i_max)\r\nend = end_gva;\r\nif (kvm_mips_flush_gva_pud(pud, start_gva, end)) {\r\npgd_clear(pgd + i);\r\npud_free(NULL, pud);\r\n} else {\r\nsafe_to_remove = false;\r\n}\r\n}\r\nreturn safe_to_remove;\r\n}\r\nvoid kvm_mips_flush_gva_pt(pgd_t *pgd, enum kvm_mips_flush flags)\r\n{\r\nif (flags & KMF_GPA) {\r\nif (flags & KMF_KERN)\r\nkvm_mips_flush_gva_pgd(pgd, 0, 0x7fffffff);\r\nelse\r\nkvm_mips_flush_gva_pgd(pgd, 0, 0x3fffffff);\r\n} else {\r\nkvm_mips_flush_gva_pgd(pgd, 0, 0x3fffffff);\r\nif (flags & KMF_KERN)\r\nkvm_mips_flush_gva_pgd(pgd, 0x60000000, 0x7fffffff);\r\n}\r\n}\r\nstatic pte_t kvm_mips_gpa_pte_to_gva_unmapped(pte_t pte)\r\n{\r\nif (!pte_dirty(pte))\r\npte = pte_wrprotect(pte);\r\nreturn pte;\r\n}\r\nstatic pte_t kvm_mips_gpa_pte_to_gva_mapped(pte_t pte, long entrylo)\r\n{\r\nif (!(entrylo & ENTRYLO_D))\r\npte = pte_mkclean(pte);\r\nreturn kvm_mips_gpa_pte_to_gva_unmapped(pte);\r\n}\r\nint kvm_mips_handle_vz_root_tlb_fault(unsigned long badvaddr,\r\nstruct kvm_vcpu *vcpu,\r\nbool write_fault)\r\n{\r\nint ret;\r\nret = kvm_mips_map_page(vcpu, badvaddr, write_fault, NULL, NULL);\r\nif (ret)\r\nreturn ret;\r\nreturn kvm_vz_host_tlb_inv(vcpu, badvaddr);\r\n}\r\nint kvm_mips_handle_kseg0_tlb_fault(unsigned long badvaddr,\r\nstruct kvm_vcpu *vcpu,\r\nbool write_fault)\r\n{\r\nunsigned long gpa;\r\npte_t pte_gpa[2], *ptep_gva;\r\nint idx;\r\nif (KVM_GUEST_KSEGX(badvaddr) != KVM_GUEST_KSEG0) {\r\nkvm_err("%s: Invalid BadVaddr: %#lx\n", __func__, badvaddr);\r\nkvm_mips_dump_host_tlbs();\r\nreturn -1;\r\n}\r\ngpa = KVM_GUEST_CPHYSADDR(badvaddr);\r\nidx = (badvaddr >> PAGE_SHIFT) & 1;\r\nif (kvm_mips_map_page(vcpu, gpa, write_fault, &pte_gpa[idx],\r\n&pte_gpa[!idx]) < 0)\r\nreturn -1;\r\nptep_gva = kvm_trap_emul_pte_for_gva(vcpu, badvaddr & ~PAGE_SIZE);\r\nif (!ptep_gva) {\r\nkvm_err("No ptep for gva %lx\n", badvaddr);\r\nreturn -1;\r\n}\r\nptep_gva[0] = kvm_mips_gpa_pte_to_gva_unmapped(pte_gpa[0]);\r\nptep_gva[1] = kvm_mips_gpa_pte_to_gva_unmapped(pte_gpa[1]);\r\nkvm_mips_host_tlb_inv(vcpu, badvaddr, false, true);\r\nreturn 0;\r\n}\r\nint kvm_mips_handle_mapped_seg_tlb_fault(struct kvm_vcpu *vcpu,\r\nstruct kvm_mips_tlb *tlb,\r\nunsigned long gva,\r\nbool write_fault)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nlong tlb_lo[2];\r\npte_t pte_gpa[2], *ptep_buddy, *ptep_gva;\r\nunsigned int idx = TLB_LO_IDX(*tlb, gva);\r\nbool kernel = KVM_GUEST_KERNEL_MODE(vcpu);\r\ntlb_lo[0] = tlb->tlb_lo[0];\r\ntlb_lo[1] = tlb->tlb_lo[1];\r\nif (!((gva ^ KVM_GUEST_COMMPAGE_ADDR) & VPN2_MASK & (PAGE_MASK << 1)))\r\ntlb_lo[TLB_LO_IDX(*tlb, KVM_GUEST_COMMPAGE_ADDR)] = 0;\r\nif (kvm_mips_map_page(vcpu, mips3_tlbpfn_to_paddr(tlb_lo[idx]),\r\nwrite_fault, &pte_gpa[idx], NULL) < 0)\r\nreturn -1;\r\npte_gpa[!idx] = pfn_pte(0, __pgprot(0));\r\nif (tlb_lo[!idx] & ENTRYLO_V) {\r\nspin_lock(&kvm->mmu_lock);\r\nptep_buddy = kvm_mips_pte_for_gpa(kvm, NULL,\r\nmips3_tlbpfn_to_paddr(tlb_lo[!idx]));\r\nif (ptep_buddy)\r\npte_gpa[!idx] = *ptep_buddy;\r\nspin_unlock(&kvm->mmu_lock);\r\n}\r\nptep_gva = kvm_trap_emul_pte_for_gva(vcpu, gva & ~PAGE_SIZE);\r\nif (!ptep_gva) {\r\nkvm_err("No ptep for gva %lx\n", gva);\r\nreturn -1;\r\n}\r\nptep_gva[0] = kvm_mips_gpa_pte_to_gva_mapped(pte_gpa[0], tlb_lo[0]);\r\nptep_gva[1] = kvm_mips_gpa_pte_to_gva_mapped(pte_gpa[1], tlb_lo[1]);\r\nkvm_mips_host_tlb_inv(vcpu, gva, !kernel, kernel);\r\nkvm_debug("@ %#lx tlb_lo0: 0x%08lx tlb_lo1: 0x%08lx\n", vcpu->arch.pc,\r\ntlb->tlb_lo[0], tlb->tlb_lo[1]);\r\nreturn 0;\r\n}\r\nint kvm_mips_handle_commpage_tlb_fault(unsigned long badvaddr,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nkvm_pfn_t pfn;\r\npte_t *ptep;\r\nptep = kvm_trap_emul_pte_for_gva(vcpu, badvaddr);\r\nif (!ptep) {\r\nkvm_err("No ptep for commpage %lx\n", badvaddr);\r\nreturn -1;\r\n}\r\npfn = PFN_DOWN(virt_to_phys(vcpu->arch.kseg0_commpage));\r\n*ptep = pte_mkyoung(pte_mkdirty(pfn_pte(pfn, PAGE_SHARED)));\r\nkvm_mips_host_tlb_inv(vcpu, badvaddr, false, true);\r\nreturn 0;\r\n}\r\nstatic void kvm_mips_migrate_count(struct kvm_vcpu *vcpu)\r\n{\r\nif (hrtimer_cancel(&vcpu->arch.comparecount_timer))\r\nhrtimer_restart(&vcpu->arch.comparecount_timer);\r\n}\r\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nunsigned long flags;\r\nkvm_debug("%s: vcpu %p, cpu: %d\n", __func__, vcpu, cpu);\r\nlocal_irq_save(flags);\r\nvcpu->cpu = cpu;\r\nif (vcpu->arch.last_sched_cpu != cpu) {\r\nkvm_debug("[%d->%d]KVM VCPU[%d] switch\n",\r\nvcpu->arch.last_sched_cpu, cpu, vcpu->vcpu_id);\r\nkvm_mips_migrate_count(vcpu);\r\n}\r\nkvm_mips_callbacks->vcpu_load(vcpu, cpu);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long flags;\r\nint cpu;\r\nlocal_irq_save(flags);\r\ncpu = smp_processor_id();\r\nvcpu->arch.last_sched_cpu = cpu;\r\nvcpu->cpu = -1;\r\nkvm_mips_callbacks->vcpu_put(vcpu, cpu);\r\nlocal_irq_restore(flags);\r\n}\r\nenum kvm_mips_fault_result kvm_trap_emul_gva_fault(struct kvm_vcpu *vcpu,\r\nunsigned long gva,\r\nbool write)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_mips_tlb *tlb;\r\nint index;\r\nif (KVM_GUEST_KSEGX(gva) == KVM_GUEST_KSEG0) {\r\nif (kvm_mips_handle_kseg0_tlb_fault(gva, vcpu, write) < 0)\r\nreturn KVM_MIPS_GPA;\r\n} else if ((KVM_GUEST_KSEGX(gva) < KVM_GUEST_KSEG0) ||\r\nKVM_GUEST_KSEGX(gva) == KVM_GUEST_KSEG23) {\r\nindex = kvm_mips_guest_tlb_lookup(vcpu, (gva & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi(cop0) & KVM_ENTRYHI_ASID));\r\nif (index < 0)\r\nreturn KVM_MIPS_TLB;\r\ntlb = &vcpu->arch.guest_tlb[index];\r\nif (!TLB_IS_VALID(*tlb, gva))\r\nreturn KVM_MIPS_TLBINV;\r\nif (write && !TLB_IS_DIRTY(*tlb, gva))\r\nreturn KVM_MIPS_TLBMOD;\r\nif (kvm_mips_handle_mapped_seg_tlb_fault(vcpu, tlb, gva, write))\r\nreturn KVM_MIPS_GPA;\r\n} else {\r\nreturn KVM_MIPS_GVA;\r\n}\r\nreturn KVM_MIPS_MAPPED;\r\n}\r\nint kvm_get_inst(u32 *opc, struct kvm_vcpu *vcpu, u32 *out)\r\n{\r\nint err;\r\nif (WARN(IS_ENABLED(CONFIG_KVM_MIPS_VZ),\r\n"Expect BadInstr/BadInstrP registers to be used with VZ\n"))\r\nreturn -EINVAL;\r\nretry:\r\nkvm_trap_emul_gva_lockless_begin(vcpu);\r\nerr = get_user(*out, opc);\r\nkvm_trap_emul_gva_lockless_end(vcpu);\r\nif (unlikely(err)) {\r\nerr = kvm_trap_emul_gva_fault(vcpu, (unsigned long)opc,\r\nfalse);\r\nif (unlikely(err)) {\r\nkvm_err("%s: illegal address: %p\n",\r\n__func__, opc);\r\nreturn -EFAULT;\r\n}\r\ngoto retry;\r\n}\r\nreturn 0;\r\n}
