static inline void setbit128_bbe(void *b, int bit)\r\n{\r\n__set_bit(bit ^ (0x80 -\r\n#ifdef __BIG_ENDIAN\r\nBITS_PER_LONG\r\n#else\r\nBITS_PER_BYTE\r\n#endif\r\n), b);\r\n}\r\nint lrw_init_table(struct lrw_table_ctx *ctx, const u8 *tweak)\r\n{\r\nbe128 tmp = { 0 };\r\nint i;\r\nif (ctx->table)\r\ngf128mul_free_64k(ctx->table);\r\nctx->table = gf128mul_init_64k_bbe((be128 *)tweak);\r\nif (!ctx->table)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < 128; i++) {\r\nsetbit128_bbe(&tmp, i);\r\nctx->mulinc[i] = tmp;\r\ngf128mul_64k_bbe(&ctx->mulinc[i], ctx->table);\r\n}\r\nreturn 0;\r\n}\r\nvoid lrw_free_table(struct lrw_table_ctx *ctx)\r\n{\r\nif (ctx->table)\r\ngf128mul_free_64k(ctx->table);\r\n}\r\nstatic int setkey(struct crypto_skcipher *parent, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct priv *ctx = crypto_skcipher_ctx(parent);\r\nstruct crypto_skcipher *child = ctx->child;\r\nint err, bsize = LRW_BLOCK_SIZE;\r\nconst u8 *tweak = key + keylen - bsize;\r\ncrypto_skcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\r\ncrypto_skcipher_set_flags(child, crypto_skcipher_get_flags(parent) &\r\nCRYPTO_TFM_REQ_MASK);\r\nerr = crypto_skcipher_setkey(child, key, keylen - bsize);\r\ncrypto_skcipher_set_flags(parent, crypto_skcipher_get_flags(child) &\r\nCRYPTO_TFM_RES_MASK);\r\nif (err)\r\nreturn err;\r\nreturn lrw_init_table(&ctx->table, tweak);\r\n}\r\nstatic inline void inc(be128 *iv)\r\n{\r\nbe64_add_cpu(&iv->b, 1);\r\nif (!iv->b)\r\nbe64_add_cpu(&iv->a, 1);\r\n}\r\nstatic inline int get_index128(be128 *block)\r\n{\r\nint x;\r\n__be32 *p = (__be32 *) block;\r\nfor (p += 3, x = 0; x < 128; p--, x += 32) {\r\nu32 val = be32_to_cpup(p);\r\nif (!~val)\r\ncontinue;\r\nreturn x + ffz(val);\r\n}\r\nreturn x;\r\n}\r\nstatic int post_crypt(struct skcipher_request *req)\r\n{\r\nstruct rctx *rctx = skcipher_request_ctx(req);\r\nbe128 *buf = rctx->ext ?: rctx->buf;\r\nstruct skcipher_request *subreq;\r\nconst int bs = LRW_BLOCK_SIZE;\r\nstruct skcipher_walk w;\r\nstruct scatterlist *sg;\r\nunsigned offset;\r\nint err;\r\nsubreq = &rctx->subreq;\r\nerr = skcipher_walk_virt(&w, subreq, false);\r\nwhile (w.nbytes) {\r\nunsigned int avail = w.nbytes;\r\nbe128 *wdst;\r\nwdst = w.dst.virt.addr;\r\ndo {\r\nbe128_xor(wdst, buf++, wdst);\r\nwdst++;\r\n} while ((avail -= bs) >= bs);\r\nerr = skcipher_walk_done(&w, avail);\r\n}\r\nrctx->left -= subreq->cryptlen;\r\nif (err || !rctx->left)\r\ngoto out;\r\nrctx->dst = rctx->dstbuf;\r\nscatterwalk_done(&w.out, 0, 1);\r\nsg = w.out.sg;\r\noffset = w.out.offset;\r\nif (rctx->dst != sg) {\r\nrctx->dst[0] = *sg;\r\nsg_unmark_end(rctx->dst);\r\nscatterwalk_crypto_chain(rctx->dst, sg_next(sg), 0, 2);\r\n}\r\nrctx->dst[0].length -= offset - sg->offset;\r\nrctx->dst[0].offset = offset;\r\nout:\r\nreturn err;\r\n}\r\nstatic int pre_crypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct rctx *rctx = skcipher_request_ctx(req);\r\nstruct priv *ctx = crypto_skcipher_ctx(tfm);\r\nbe128 *buf = rctx->ext ?: rctx->buf;\r\nstruct skcipher_request *subreq;\r\nconst int bs = LRW_BLOCK_SIZE;\r\nstruct skcipher_walk w;\r\nstruct scatterlist *sg;\r\nunsigned cryptlen;\r\nunsigned offset;\r\nbe128 *iv;\r\nbool more;\r\nint err;\r\nsubreq = &rctx->subreq;\r\nskcipher_request_set_tfm(subreq, tfm);\r\ncryptlen = subreq->cryptlen;\r\nmore = rctx->left > cryptlen;\r\nif (!more)\r\ncryptlen = rctx->left;\r\nskcipher_request_set_crypt(subreq, rctx->src, rctx->dst,\r\ncryptlen, req->iv);\r\nerr = skcipher_walk_virt(&w, subreq, false);\r\niv = w.iv;\r\nwhile (w.nbytes) {\r\nunsigned int avail = w.nbytes;\r\nbe128 *wsrc;\r\nbe128 *wdst;\r\nwsrc = w.src.virt.addr;\r\nwdst = w.dst.virt.addr;\r\ndo {\r\n*buf++ = rctx->t;\r\nbe128_xor(wdst++, &rctx->t, wsrc++);\r\nbe128_xor(&rctx->t, &rctx->t,\r\n&ctx->table.mulinc[get_index128(iv)]);\r\ninc(iv);\r\n} while ((avail -= bs) >= bs);\r\nerr = skcipher_walk_done(&w, avail);\r\n}\r\nskcipher_request_set_tfm(subreq, ctx->child);\r\nskcipher_request_set_crypt(subreq, rctx->dst, rctx->dst,\r\ncryptlen, NULL);\r\nif (err || !more)\r\ngoto out;\r\nrctx->src = rctx->srcbuf;\r\nscatterwalk_done(&w.in, 0, 1);\r\nsg = w.in.sg;\r\noffset = w.in.offset;\r\nif (rctx->src != sg) {\r\nrctx->src[0] = *sg;\r\nsg_unmark_end(rctx->src);\r\nscatterwalk_crypto_chain(rctx->src, sg_next(sg), 0, 2);\r\n}\r\nrctx->src[0].length -= offset - sg->offset;\r\nrctx->src[0].offset = offset;\r\nout:\r\nreturn err;\r\n}\r\nstatic int init_crypt(struct skcipher_request *req, crypto_completion_t done)\r\n{\r\nstruct priv *ctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));\r\nstruct rctx *rctx = skcipher_request_ctx(req);\r\nstruct skcipher_request *subreq;\r\ngfp_t gfp;\r\nsubreq = &rctx->subreq;\r\nskcipher_request_set_callback(subreq, req->base.flags, done, req);\r\ngfp = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\r\nGFP_ATOMIC;\r\nrctx->ext = NULL;\r\nsubreq->cryptlen = LRW_BUFFER_SIZE;\r\nif (req->cryptlen > LRW_BUFFER_SIZE) {\r\nunsigned int n = min(req->cryptlen, (unsigned int)PAGE_SIZE);\r\nrctx->ext = kmalloc(n, gfp);\r\nif (rctx->ext)\r\nsubreq->cryptlen = n;\r\n}\r\nrctx->src = req->src;\r\nrctx->dst = req->dst;\r\nrctx->left = req->cryptlen;\r\nmemcpy(&rctx->t, req->iv, sizeof(rctx->t));\r\ngf128mul_64k_bbe(&rctx->t, ctx->table.table);\r\nreturn 0;\r\n}\r\nstatic void exit_crypt(struct skcipher_request *req)\r\n{\r\nstruct rctx *rctx = skcipher_request_ctx(req);\r\nrctx->left = 0;\r\nif (rctx->ext)\r\nkfree(rctx->ext);\r\n}\r\nstatic int do_encrypt(struct skcipher_request *req, int err)\r\n{\r\nstruct rctx *rctx = skcipher_request_ctx(req);\r\nstruct skcipher_request *subreq;\r\nsubreq = &rctx->subreq;\r\nwhile (!err && rctx->left) {\r\nerr = pre_crypt(req) ?:\r\ncrypto_skcipher_encrypt(subreq) ?:\r\npost_crypt(req);\r\nif (err == -EINPROGRESS ||\r\n(err == -EBUSY &&\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn err;\r\n}\r\nexit_crypt(req);\r\nreturn err;\r\n}\r\nstatic void encrypt_done(struct crypto_async_request *areq, int err)\r\n{\r\nstruct skcipher_request *req = areq->data;\r\nstruct skcipher_request *subreq;\r\nstruct rctx *rctx;\r\nrctx = skcipher_request_ctx(req);\r\nif (err == -EINPROGRESS) {\r\nif (rctx->left != req->cryptlen)\r\nreturn;\r\ngoto out;\r\n}\r\nsubreq = &rctx->subreq;\r\nsubreq->base.flags &= CRYPTO_TFM_REQ_MAY_BACKLOG;\r\nerr = do_encrypt(req, err ?: post_crypt(req));\r\nif (rctx->left)\r\nreturn;\r\nout:\r\nskcipher_request_complete(req, err);\r\n}\r\nstatic int encrypt(struct skcipher_request *req)\r\n{\r\nreturn do_encrypt(req, init_crypt(req, encrypt_done));\r\n}\r\nstatic int do_decrypt(struct skcipher_request *req, int err)\r\n{\r\nstruct rctx *rctx = skcipher_request_ctx(req);\r\nstruct skcipher_request *subreq;\r\nsubreq = &rctx->subreq;\r\nwhile (!err && rctx->left) {\r\nerr = pre_crypt(req) ?:\r\ncrypto_skcipher_decrypt(subreq) ?:\r\npost_crypt(req);\r\nif (err == -EINPROGRESS ||\r\n(err == -EBUSY &&\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn err;\r\n}\r\nexit_crypt(req);\r\nreturn err;\r\n}\r\nstatic void decrypt_done(struct crypto_async_request *areq, int err)\r\n{\r\nstruct skcipher_request *req = areq->data;\r\nstruct skcipher_request *subreq;\r\nstruct rctx *rctx;\r\nrctx = skcipher_request_ctx(req);\r\nif (err == -EINPROGRESS) {\r\nif (rctx->left != req->cryptlen)\r\nreturn;\r\ngoto out;\r\n}\r\nsubreq = &rctx->subreq;\r\nsubreq->base.flags &= CRYPTO_TFM_REQ_MAY_BACKLOG;\r\nerr = do_decrypt(req, err ?: post_crypt(req));\r\nif (rctx->left)\r\nreturn;\r\nout:\r\nskcipher_request_complete(req, err);\r\n}\r\nstatic int decrypt(struct skcipher_request *req)\r\n{\r\nreturn do_decrypt(req, init_crypt(req, decrypt_done));\r\n}\r\nint lrw_crypt(struct blkcipher_desc *desc, struct scatterlist *sdst,\r\nstruct scatterlist *ssrc, unsigned int nbytes,\r\nstruct lrw_crypt_req *req)\r\n{\r\nconst unsigned int bsize = LRW_BLOCK_SIZE;\r\nconst unsigned int max_blks = req->tbuflen / bsize;\r\nstruct lrw_table_ctx *ctx = req->table_ctx;\r\nstruct blkcipher_walk walk;\r\nunsigned int nblocks;\r\nbe128 *iv, *src, *dst, *t;\r\nbe128 *t_buf = req->tbuf;\r\nint err, i;\r\nBUG_ON(max_blks < 1);\r\nblkcipher_walk_init(&walk, sdst, ssrc, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nnbytes = walk.nbytes;\r\nif (!nbytes)\r\nreturn err;\r\nnblocks = min(walk.nbytes / bsize, max_blks);\r\nsrc = (be128 *)walk.src.virt.addr;\r\ndst = (be128 *)walk.dst.virt.addr;\r\niv = (be128 *)walk.iv;\r\nt_buf[0] = *iv;\r\ngf128mul_64k_bbe(&t_buf[0], ctx->table);\r\ni = 0;\r\ngoto first;\r\nfor (;;) {\r\ndo {\r\nfor (i = 0; i < nblocks; i++) {\r\nbe128_xor(&t_buf[i], t,\r\n&ctx->mulinc[get_index128(iv)]);\r\ninc(iv);\r\nfirst:\r\nt = &t_buf[i];\r\nbe128_xor(dst + i, t, src + i);\r\n}\r\nreq->crypt_fn(req->crypt_ctx, (u8 *)dst,\r\nnblocks * bsize);\r\nfor (i = 0; i < nblocks; i++)\r\nbe128_xor(dst + i, dst + i, &t_buf[i]);\r\nsrc += nblocks;\r\ndst += nblocks;\r\nnbytes -= nblocks * bsize;\r\nnblocks = min(nbytes / bsize, max_blks);\r\n} while (nblocks > 0);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\nnbytes = walk.nbytes;\r\nif (!nbytes)\r\nbreak;\r\nnblocks = min(nbytes / bsize, max_blks);\r\nsrc = (be128 *)walk.src.virt.addr;\r\ndst = (be128 *)walk.dst.virt.addr;\r\n}\r\nreturn err;\r\n}\r\nstatic int init_tfm(struct crypto_skcipher *tfm)\r\n{\r\nstruct skcipher_instance *inst = skcipher_alg_instance(tfm);\r\nstruct crypto_skcipher_spawn *spawn = skcipher_instance_ctx(inst);\r\nstruct priv *ctx = crypto_skcipher_ctx(tfm);\r\nstruct crypto_skcipher *cipher;\r\ncipher = crypto_spawn_skcipher(spawn);\r\nif (IS_ERR(cipher))\r\nreturn PTR_ERR(cipher);\r\nctx->child = cipher;\r\ncrypto_skcipher_set_reqsize(tfm, crypto_skcipher_reqsize(cipher) +\r\nsizeof(struct rctx));\r\nreturn 0;\r\n}\r\nstatic void exit_tfm(struct crypto_skcipher *tfm)\r\n{\r\nstruct priv *ctx = crypto_skcipher_ctx(tfm);\r\nlrw_free_table(&ctx->table);\r\ncrypto_free_skcipher(ctx->child);\r\n}\r\nstatic void free(struct skcipher_instance *inst)\r\n{\r\ncrypto_drop_skcipher(skcipher_instance_ctx(inst));\r\nkfree(inst);\r\n}\r\nstatic int create(struct crypto_template *tmpl, struct rtattr **tb)\r\n{\r\nstruct crypto_skcipher_spawn *spawn;\r\nstruct skcipher_instance *inst;\r\nstruct crypto_attr_type *algt;\r\nstruct skcipher_alg *alg;\r\nconst char *cipher_name;\r\nchar ecb_name[CRYPTO_MAX_ALG_NAME];\r\nint err;\r\nalgt = crypto_get_attr_type(tb);\r\nif (IS_ERR(algt))\r\nreturn PTR_ERR(algt);\r\nif ((algt->type ^ CRYPTO_ALG_TYPE_SKCIPHER) & algt->mask)\r\nreturn -EINVAL;\r\ncipher_name = crypto_attr_alg_name(tb[1]);\r\nif (IS_ERR(cipher_name))\r\nreturn PTR_ERR(cipher_name);\r\ninst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\r\nif (!inst)\r\nreturn -ENOMEM;\r\nspawn = skcipher_instance_ctx(inst);\r\ncrypto_set_skcipher_spawn(spawn, skcipher_crypto_instance(inst));\r\nerr = crypto_grab_skcipher(spawn, cipher_name, 0,\r\ncrypto_requires_sync(algt->type,\r\nalgt->mask));\r\nif (err == -ENOENT) {\r\nerr = -ENAMETOOLONG;\r\nif (snprintf(ecb_name, CRYPTO_MAX_ALG_NAME, "ecb(%s)",\r\ncipher_name) >= CRYPTO_MAX_ALG_NAME)\r\ngoto err_free_inst;\r\nerr = crypto_grab_skcipher(spawn, ecb_name, 0,\r\ncrypto_requires_sync(algt->type,\r\nalgt->mask));\r\n}\r\nif (err)\r\ngoto err_free_inst;\r\nalg = crypto_skcipher_spawn_alg(spawn);\r\nerr = -EINVAL;\r\nif (alg->base.cra_blocksize != LRW_BLOCK_SIZE)\r\ngoto err_drop_spawn;\r\nif (crypto_skcipher_alg_ivsize(alg))\r\ngoto err_drop_spawn;\r\nerr = crypto_inst_setname(skcipher_crypto_instance(inst), "lrw",\r\n&alg->base);\r\nif (err)\r\ngoto err_drop_spawn;\r\nerr = -EINVAL;\r\ncipher_name = alg->base.cra_name;\r\nif (!strncmp(cipher_name, "ecb(", 4)) {\r\nunsigned len;\r\nlen = strlcpy(ecb_name, cipher_name + 4, sizeof(ecb_name));\r\nif (len < 2 || len >= sizeof(ecb_name))\r\ngoto err_drop_spawn;\r\nif (ecb_name[len - 1] != ')')\r\ngoto err_drop_spawn;\r\necb_name[len - 1] = 0;\r\nif (snprintf(inst->alg.base.cra_name, CRYPTO_MAX_ALG_NAME,\r\n"lrw(%s)", ecb_name) >= CRYPTO_MAX_ALG_NAME)\r\nreturn -ENAMETOOLONG;\r\n}\r\ninst->alg.base.cra_flags = alg->base.cra_flags & CRYPTO_ALG_ASYNC;\r\ninst->alg.base.cra_priority = alg->base.cra_priority;\r\ninst->alg.base.cra_blocksize = LRW_BLOCK_SIZE;\r\ninst->alg.base.cra_alignmask = alg->base.cra_alignmask |\r\n(__alignof__(u64) - 1);\r\ninst->alg.ivsize = LRW_BLOCK_SIZE;\r\ninst->alg.min_keysize = crypto_skcipher_alg_min_keysize(alg) +\r\nLRW_BLOCK_SIZE;\r\ninst->alg.max_keysize = crypto_skcipher_alg_max_keysize(alg) +\r\nLRW_BLOCK_SIZE;\r\ninst->alg.base.cra_ctxsize = sizeof(struct priv);\r\ninst->alg.init = init_tfm;\r\ninst->alg.exit = exit_tfm;\r\ninst->alg.setkey = setkey;\r\ninst->alg.encrypt = encrypt;\r\ninst->alg.decrypt = decrypt;\r\ninst->free = free;\r\nerr = skcipher_register_instance(tmpl, inst);\r\nif (err)\r\ngoto err_drop_spawn;\r\nout:\r\nreturn err;\r\nerr_drop_spawn:\r\ncrypto_drop_skcipher(spawn);\r\nerr_free_inst:\r\nkfree(inst);\r\ngoto out;\r\n}\r\nstatic int __init crypto_module_init(void)\r\n{\r\nreturn crypto_register_template(&crypto_tmpl);\r\n}\r\nstatic void __exit crypto_module_exit(void)\r\n{\r\ncrypto_unregister_template(&crypto_tmpl);\r\n}
