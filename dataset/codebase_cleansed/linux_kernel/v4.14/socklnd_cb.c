struct ksock_tx *\r\nksocknal_alloc_tx(int type, int size)\r\n{\r\nstruct ksock_tx *tx = NULL;\r\nif (type == KSOCK_MSG_NOOP) {\r\nLASSERT(size == KSOCK_NOOP_TX_SIZE);\r\nspin_lock(&ksocknal_data.ksnd_tx_lock);\r\nif (!list_empty(&ksocknal_data.ksnd_idle_noop_txs)) {\r\ntx = list_entry(ksocknal_data.ksnd_idle_noop_txs.next,\r\nstruct ksock_tx, tx_list);\r\nLASSERT(tx->tx_desc_size == size);\r\nlist_del(&tx->tx_list);\r\n}\r\nspin_unlock(&ksocknal_data.ksnd_tx_lock);\r\n}\r\nif (!tx)\r\nLIBCFS_ALLOC(tx, size);\r\nif (!tx)\r\nreturn NULL;\r\natomic_set(&tx->tx_refcount, 1);\r\ntx->tx_zc_aborted = 0;\r\ntx->tx_zc_capable = 0;\r\ntx->tx_zc_checked = 0;\r\ntx->tx_desc_size = size;\r\natomic_inc(&ksocknal_data.ksnd_nactive_txs);\r\nreturn tx;\r\n}\r\nstruct ksock_tx *\r\nksocknal_alloc_tx_noop(__u64 cookie, int nonblk)\r\n{\r\nstruct ksock_tx *tx;\r\ntx = ksocknal_alloc_tx(KSOCK_MSG_NOOP, KSOCK_NOOP_TX_SIZE);\r\nif (!tx) {\r\nCERROR("Can't allocate noop tx desc\n");\r\nreturn NULL;\r\n}\r\ntx->tx_conn = NULL;\r\ntx->tx_lnetmsg = NULL;\r\ntx->tx_kiov = NULL;\r\ntx->tx_nkiov = 0;\r\ntx->tx_iov = tx->tx_frags.virt.iov;\r\ntx->tx_niov = 1;\r\ntx->tx_nonblk = nonblk;\r\ntx->tx_msg.ksm_csum = 0;\r\ntx->tx_msg.ksm_type = KSOCK_MSG_NOOP;\r\ntx->tx_msg.ksm_zc_cookies[0] = 0;\r\ntx->tx_msg.ksm_zc_cookies[1] = cookie;\r\nreturn tx;\r\n}\r\nvoid\r\nksocknal_free_tx(struct ksock_tx *tx)\r\n{\r\natomic_dec(&ksocknal_data.ksnd_nactive_txs);\r\nif (!tx->tx_lnetmsg && tx->tx_desc_size == KSOCK_NOOP_TX_SIZE) {\r\nspin_lock(&ksocknal_data.ksnd_tx_lock);\r\nlist_add(&tx->tx_list, &ksocknal_data.ksnd_idle_noop_txs);\r\nspin_unlock(&ksocknal_data.ksnd_tx_lock);\r\n} else {\r\nLIBCFS_FREE(tx, tx->tx_desc_size);\r\n}\r\n}\r\nstatic int\r\nksocknal_send_iov(struct ksock_conn *conn, struct ksock_tx *tx)\r\n{\r\nstruct kvec *iov = tx->tx_iov;\r\nint nob;\r\nint rc;\r\nLASSERT(tx->tx_niov > 0);\r\nrc = ksocknal_lib_send_iov(conn, tx);\r\nif (rc <= 0)\r\nreturn rc;\r\nnob = rc;\r\nLASSERT(nob <= tx->tx_resid);\r\ntx->tx_resid -= nob;\r\ndo {\r\nLASSERT(tx->tx_niov > 0);\r\nif (nob < (int)iov->iov_len) {\r\niov->iov_base = (void *)((char *)iov->iov_base + nob);\r\niov->iov_len -= nob;\r\nreturn rc;\r\n}\r\nnob -= iov->iov_len;\r\ntx->tx_iov = ++iov;\r\ntx->tx_niov--;\r\n} while (nob);\r\nreturn rc;\r\n}\r\nstatic int\r\nksocknal_send_kiov(struct ksock_conn *conn, struct ksock_tx *tx)\r\n{\r\nstruct bio_vec *kiov = tx->tx_kiov;\r\nint nob;\r\nint rc;\r\nLASSERT(!tx->tx_niov);\r\nLASSERT(tx->tx_nkiov > 0);\r\nrc = ksocknal_lib_send_kiov(conn, tx);\r\nif (rc <= 0)\r\nreturn rc;\r\nnob = rc;\r\nLASSERT(nob <= tx->tx_resid);\r\ntx->tx_resid -= nob;\r\ndo {\r\nLASSERT(tx->tx_nkiov > 0);\r\nif (nob < (int)kiov->bv_len) {\r\nkiov->bv_offset += nob;\r\nkiov->bv_len -= nob;\r\nreturn rc;\r\n}\r\nnob -= (int)kiov->bv_len;\r\ntx->tx_kiov = ++kiov;\r\ntx->tx_nkiov--;\r\n} while (nob);\r\nreturn rc;\r\n}\r\nstatic int\r\nksocknal_transmit(struct ksock_conn *conn, struct ksock_tx *tx)\r\n{\r\nint rc;\r\nint bufnob;\r\nif (ksocknal_data.ksnd_stall_tx) {\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nschedule_timeout(cfs_time_seconds(ksocknal_data.ksnd_stall_tx));\r\n}\r\nLASSERT(tx->tx_resid);\r\nrc = ksocknal_connsock_addref(conn);\r\nif (rc) {\r\nLASSERT(conn->ksnc_closing);\r\nreturn -ESHUTDOWN;\r\n}\r\ndo {\r\nif (ksocknal_data.ksnd_enomem_tx > 0) {\r\nksocknal_data.ksnd_enomem_tx--;\r\nrc = -EAGAIN;\r\n} else if (tx->tx_niov) {\r\nrc = ksocknal_send_iov(conn, tx);\r\n} else {\r\nrc = ksocknal_send_kiov(conn, tx);\r\n}\r\nbufnob = conn->ksnc_sock->sk->sk_wmem_queued;\r\nif (rc > 0)\r\nconn->ksnc_tx_bufnob += rc;\r\nif (bufnob < conn->ksnc_tx_bufnob) {\r\nconn->ksnc_tx_deadline =\r\ncfs_time_shift(*ksocknal_tunables.ksnd_timeout);\r\nconn->ksnc_peer->ksnp_last_alive = cfs_time_current();\r\nconn->ksnc_tx_bufnob = bufnob;\r\nmb();\r\n}\r\nif (rc <= 0) {\r\nif (!rc)\r\nrc = -EAGAIN;\r\nif (rc == -EAGAIN && ksocknal_lib_memory_pressure(conn))\r\nrc = -ENOMEM;\r\nbreak;\r\n}\r\natomic_sub(rc, &conn->ksnc_tx_nob);\r\nrc = 0;\r\n} while (tx->tx_resid);\r\nksocknal_connsock_decref(conn);\r\nreturn rc;\r\n}\r\nstatic int\r\nksocknal_recv_iov(struct ksock_conn *conn)\r\n{\r\nstruct kvec *iov = conn->ksnc_rx_iov;\r\nint nob;\r\nint rc;\r\nLASSERT(conn->ksnc_rx_niov > 0);\r\nrc = ksocknal_lib_recv_iov(conn);\r\nif (rc <= 0)\r\nreturn rc;\r\nnob = rc;\r\nconn->ksnc_peer->ksnp_last_alive = cfs_time_current();\r\nconn->ksnc_rx_deadline =\r\ncfs_time_shift(*ksocknal_tunables.ksnd_timeout);\r\nmb();\r\nconn->ksnc_rx_started = 1;\r\nconn->ksnc_rx_nob_wanted -= nob;\r\nconn->ksnc_rx_nob_left -= nob;\r\ndo {\r\nLASSERT(conn->ksnc_rx_niov > 0);\r\nif (nob < (int)iov->iov_len) {\r\niov->iov_len -= nob;\r\niov->iov_base += nob;\r\nreturn -EAGAIN;\r\n}\r\nnob -= iov->iov_len;\r\nconn->ksnc_rx_iov = ++iov;\r\nconn->ksnc_rx_niov--;\r\n} while (nob);\r\nreturn rc;\r\n}\r\nstatic int\r\nksocknal_recv_kiov(struct ksock_conn *conn)\r\n{\r\nstruct bio_vec *kiov = conn->ksnc_rx_kiov;\r\nint nob;\r\nint rc;\r\nLASSERT(conn->ksnc_rx_nkiov > 0);\r\nrc = ksocknal_lib_recv_kiov(conn);\r\nif (rc <= 0)\r\nreturn rc;\r\nnob = rc;\r\nconn->ksnc_peer->ksnp_last_alive = cfs_time_current();\r\nconn->ksnc_rx_deadline =\r\ncfs_time_shift(*ksocknal_tunables.ksnd_timeout);\r\nmb();\r\nconn->ksnc_rx_started = 1;\r\nconn->ksnc_rx_nob_wanted -= nob;\r\nconn->ksnc_rx_nob_left -= nob;\r\ndo {\r\nLASSERT(conn->ksnc_rx_nkiov > 0);\r\nif (nob < (int)kiov->bv_len) {\r\nkiov->bv_offset += nob;\r\nkiov->bv_len -= nob;\r\nreturn -EAGAIN;\r\n}\r\nnob -= kiov->bv_len;\r\nconn->ksnc_rx_kiov = ++kiov;\r\nconn->ksnc_rx_nkiov--;\r\n} while (nob);\r\nreturn 1;\r\n}\r\nstatic int\r\nksocknal_receive(struct ksock_conn *conn)\r\n{\r\nint rc;\r\nif (ksocknal_data.ksnd_stall_rx) {\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nschedule_timeout(cfs_time_seconds(ksocknal_data.ksnd_stall_rx));\r\n}\r\nrc = ksocknal_connsock_addref(conn);\r\nif (rc) {\r\nLASSERT(conn->ksnc_closing);\r\nreturn -ESHUTDOWN;\r\n}\r\nfor (;;) {\r\nif (conn->ksnc_rx_niov)\r\nrc = ksocknal_recv_iov(conn);\r\nelse\r\nrc = ksocknal_recv_kiov(conn);\r\nif (rc <= 0) {\r\nif (rc == -EAGAIN) {\r\nrc = 1;\r\n} else if (!rc && conn->ksnc_rx_started) {\r\nrc = -EPROTO;\r\n}\r\nbreak;\r\n}\r\nif (!conn->ksnc_rx_nob_wanted) {\r\nrc = 1;\r\nbreak;\r\n}\r\n}\r\nksocknal_connsock_decref(conn);\r\nreturn rc;\r\n}\r\nvoid\r\nksocknal_tx_done(struct lnet_ni *ni, struct ksock_tx *tx)\r\n{\r\nstruct lnet_msg *lnetmsg = tx->tx_lnetmsg;\r\nint rc = (!tx->tx_resid && !tx->tx_zc_aborted) ? 0 : -EIO;\r\nLASSERT(ni || tx->tx_conn);\r\nif (tx->tx_conn)\r\nksocknal_conn_decref(tx->tx_conn);\r\nif (!ni && tx->tx_conn)\r\nni = tx->tx_conn->ksnc_peer->ksnp_ni;\r\nksocknal_free_tx(tx);\r\nif (lnetmsg)\r\nlnet_finalize(ni, lnetmsg, rc);\r\n}\r\nvoid\r\nksocknal_txlist_done(struct lnet_ni *ni, struct list_head *txlist, int error)\r\n{\r\nstruct ksock_tx *tx;\r\nwhile (!list_empty(txlist)) {\r\ntx = list_entry(txlist->next, struct ksock_tx, tx_list);\r\nif (error && tx->tx_lnetmsg) {\r\nCNETERR("Deleting packet type %d len %d %s->%s\n",\r\nle32_to_cpu(tx->tx_lnetmsg->msg_hdr.type),\r\nle32_to_cpu(tx->tx_lnetmsg->msg_hdr.payload_length),\r\nlibcfs_nid2str(le64_to_cpu(tx->tx_lnetmsg->msg_hdr.src_nid)),\r\nlibcfs_nid2str(le64_to_cpu(tx->tx_lnetmsg->msg_hdr.dest_nid)));\r\n} else if (error) {\r\nCNETERR("Deleting noop packet\n");\r\n}\r\nlist_del(&tx->tx_list);\r\nLASSERT(atomic_read(&tx->tx_refcount) == 1);\r\nksocknal_tx_done(ni, tx);\r\n}\r\n}\r\nstatic void\r\nksocknal_check_zc_req(struct ksock_tx *tx)\r\n{\r\nstruct ksock_conn *conn = tx->tx_conn;\r\nstruct ksock_peer *peer = conn->ksnc_peer;\r\nLASSERT(tx->tx_msg.ksm_type != KSOCK_MSG_NOOP);\r\nLASSERT(tx->tx_zc_capable);\r\ntx->tx_zc_checked = 1;\r\nif (conn->ksnc_proto == &ksocknal_protocol_v1x ||\r\n!conn->ksnc_zc_capable)\r\nreturn;\r\nksocknal_tx_addref(tx);\r\nspin_lock(&peer->ksnp_lock);\r\ntx->tx_deadline =\r\ncfs_time_shift(*ksocknal_tunables.ksnd_timeout);\r\nLASSERT(!tx->tx_msg.ksm_zc_cookies[0]);\r\ntx->tx_msg.ksm_zc_cookies[0] = peer->ksnp_zc_next_cookie++;\r\nif (!peer->ksnp_zc_next_cookie)\r\npeer->ksnp_zc_next_cookie = SOCKNAL_KEEPALIVE_PING + 1;\r\nlist_add_tail(&tx->tx_zc_list, &peer->ksnp_zc_req_list);\r\nspin_unlock(&peer->ksnp_lock);\r\n}\r\nstatic void\r\nksocknal_uncheck_zc_req(struct ksock_tx *tx)\r\n{\r\nstruct ksock_peer *peer = tx->tx_conn->ksnc_peer;\r\nLASSERT(tx->tx_msg.ksm_type != KSOCK_MSG_NOOP);\r\nLASSERT(tx->tx_zc_capable);\r\ntx->tx_zc_checked = 0;\r\nspin_lock(&peer->ksnp_lock);\r\nif (!tx->tx_msg.ksm_zc_cookies[0]) {\r\nspin_unlock(&peer->ksnp_lock);\r\nreturn;\r\n}\r\ntx->tx_msg.ksm_zc_cookies[0] = 0;\r\nlist_del(&tx->tx_zc_list);\r\nspin_unlock(&peer->ksnp_lock);\r\nksocknal_tx_decref(tx);\r\n}\r\nstatic int\r\nksocknal_process_transmit(struct ksock_conn *conn, struct ksock_tx *tx)\r\n{\r\nint rc;\r\nif (tx->tx_zc_capable && !tx->tx_zc_checked)\r\nksocknal_check_zc_req(tx);\r\nrc = ksocknal_transmit(conn, tx);\r\nCDEBUG(D_NET, "send(%d) %d\n", tx->tx_resid, rc);\r\nif (!tx->tx_resid) {\r\nLASSERT(!rc);\r\nreturn 0;\r\n}\r\nif (rc == -EAGAIN)\r\nreturn rc;\r\nif (rc == -ENOMEM) {\r\nstatic int counter;\r\ncounter++;\r\nif ((counter & (-counter)) == counter)\r\nCWARN("%u ENOMEM tx %p\n", counter, conn);\r\nspin_lock_bh(&ksocknal_data.ksnd_reaper_lock);\r\nLASSERT(conn->ksnc_tx_scheduled);\r\nlist_add_tail(&conn->ksnc_tx_list,\r\n&ksocknal_data.ksnd_enomem_conns);\r\nif (!cfs_time_aftereq(cfs_time_add(cfs_time_current(),\r\nSOCKNAL_ENOMEM_RETRY),\r\nksocknal_data.ksnd_reaper_waketime))\r\nwake_up(&ksocknal_data.ksnd_reaper_waitq);\r\nspin_unlock_bh(&ksocknal_data.ksnd_reaper_lock);\r\nreturn rc;\r\n}\r\nLASSERT(rc < 0);\r\nif (!conn->ksnc_closing) {\r\nswitch (rc) {\r\ncase -ECONNRESET:\r\nLCONSOLE_WARN("Host %pI4h reset our connection while we were sending data; it may have rebooted.\n",\r\n&conn->ksnc_ipaddr);\r\nbreak;\r\ndefault:\r\nLCONSOLE_WARN("There was an unexpected network error while writing to %pI4h: %d.\n",\r\n&conn->ksnc_ipaddr, rc);\r\nbreak;\r\n}\r\nCDEBUG(D_NET, "[%p] Error %d on write to %s ip %pI4h:%d\n",\r\nconn, rc,\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port);\r\n}\r\nif (tx->tx_zc_checked)\r\nksocknal_uncheck_zc_req(tx);\r\nksocknal_close_conn_and_siblings(conn, (conn->ksnc_closing) ? 0 : rc);\r\nreturn rc;\r\n}\r\nstatic void\r\nksocknal_launch_connection_locked(struct ksock_route *route)\r\n{\r\nLASSERT(!route->ksnr_scheduled);\r\nLASSERT(!route->ksnr_connecting);\r\nLASSERT(ksocknal_route_mask() & ~route->ksnr_connected);\r\nroute->ksnr_scheduled = 1;\r\nksocknal_route_addref(route);\r\nspin_lock_bh(&ksocknal_data.ksnd_connd_lock);\r\nlist_add_tail(&route->ksnr_connd_list,\r\n&ksocknal_data.ksnd_connd_routes);\r\nwake_up(&ksocknal_data.ksnd_connd_waitq);\r\nspin_unlock_bh(&ksocknal_data.ksnd_connd_lock);\r\n}\r\nvoid\r\nksocknal_launch_all_connections_locked(struct ksock_peer *peer)\r\n{\r\nstruct ksock_route *route;\r\nfor (;;) {\r\nroute = ksocknal_find_connectable_route_locked(peer);\r\nif (!route)\r\nreturn;\r\nksocknal_launch_connection_locked(route);\r\n}\r\n}\r\nstruct ksock_conn *\r\nksocknal_find_conn_locked(struct ksock_peer *peer, struct ksock_tx *tx,\r\nint nonblk)\r\n{\r\nstruct list_head *tmp;\r\nstruct ksock_conn *conn;\r\nstruct ksock_conn *typed = NULL;\r\nstruct ksock_conn *fallback = NULL;\r\nint tnob = 0;\r\nint fnob = 0;\r\nlist_for_each(tmp, &peer->ksnp_conns) {\r\nstruct ksock_conn *c;\r\nint nob, rc;\r\nc = list_entry(tmp, struct ksock_conn, ksnc_list);\r\nnob = atomic_read(&c->ksnc_tx_nob) +\r\nc->ksnc_sock->sk->sk_wmem_queued;\r\nLASSERT(!c->ksnc_closing);\r\nLASSERT(c->ksnc_proto &&\r\nc->ksnc_proto->pro_match_tx);\r\nrc = c->ksnc_proto->pro_match_tx(c, tx, nonblk);\r\nswitch (rc) {\r\ndefault:\r\nLBUG();\r\ncase SOCKNAL_MATCH_NO:\r\ncontinue;\r\ncase SOCKNAL_MATCH_YES:\r\nif (!typed || tnob > nob ||\r\n(tnob == nob && *ksocknal_tunables.ksnd_round_robin &&\r\ncfs_time_after(typed->ksnc_tx_last_post, c->ksnc_tx_last_post))) {\r\ntyped = c;\r\ntnob = nob;\r\n}\r\nbreak;\r\ncase SOCKNAL_MATCH_MAY:\r\nif (!fallback || fnob > nob ||\r\n(fnob == nob && *ksocknal_tunables.ksnd_round_robin &&\r\ncfs_time_after(fallback->ksnc_tx_last_post, c->ksnc_tx_last_post))) {\r\nfallback = c;\r\nfnob = nob;\r\n}\r\nbreak;\r\n}\r\n}\r\nconn = (typed) ? typed : fallback;\r\nif (conn)\r\nconn->ksnc_tx_last_post = cfs_time_current();\r\nreturn conn;\r\n}\r\nvoid\r\nksocknal_tx_prep(struct ksock_conn *conn, struct ksock_tx *tx)\r\n{\r\nconn->ksnc_proto->pro_pack(tx);\r\natomic_add(tx->tx_nob, &conn->ksnc_tx_nob);\r\nksocknal_conn_addref(conn);\r\ntx->tx_conn = conn;\r\n}\r\nvoid\r\nksocknal_queue_tx_locked(struct ksock_tx *tx, struct ksock_conn *conn)\r\n{\r\nstruct ksock_sched *sched = conn->ksnc_scheduler;\r\nstruct ksock_msg *msg = &tx->tx_msg;\r\nstruct ksock_tx *ztx = NULL;\r\nint bufnob = 0;\r\nLASSERT(!conn->ksnc_closing);\r\nCDEBUG(D_NET, "Sending to %s ip %pI4h:%d\n",\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\n&conn->ksnc_ipaddr, conn->ksnc_port);\r\nksocknal_tx_prep(conn, tx);\r\nLASSERT(lnet_iov_nob(tx->tx_niov, tx->tx_iov) +\r\nlnet_kiov_nob(tx->tx_nkiov, tx->tx_kiov) ==\r\n(unsigned int)tx->tx_nob);\r\nLASSERT(tx->tx_niov >= 1);\r\nLASSERT(tx->tx_resid == tx->tx_nob);\r\nCDEBUG(D_NET, "Packet %p type %d, nob %d niov %d nkiov %d\n",\r\ntx, (tx->tx_lnetmsg) ? tx->tx_lnetmsg->msg_hdr.type :\r\nKSOCK_MSG_NOOP,\r\ntx->tx_nob, tx->tx_niov, tx->tx_nkiov);\r\nbufnob = conn->ksnc_sock->sk->sk_wmem_queued;\r\nspin_lock_bh(&sched->kss_lock);\r\nif (list_empty(&conn->ksnc_tx_queue) && !bufnob) {\r\nconn->ksnc_tx_deadline =\r\ncfs_time_shift(*ksocknal_tunables.ksnd_timeout);\r\nif (conn->ksnc_tx_bufnob > 0)\r\nconn->ksnc_peer->ksnp_last_alive = cfs_time_current();\r\nconn->ksnc_tx_bufnob = 0;\r\nmb();\r\n}\r\nif (msg->ksm_type == KSOCK_MSG_NOOP) {\r\nLASSERT(msg->ksm_zc_cookies[1]);\r\nLASSERT(conn->ksnc_proto->pro_queue_tx_zcack);\r\nif (conn->ksnc_proto->pro_queue_tx_zcack(conn, tx, 0))\r\nztx = tx;\r\n} else {\r\nLASSERT(!msg->ksm_zc_cookies[1]);\r\nLASSERT(conn->ksnc_proto->pro_queue_tx_msg);\r\nztx = conn->ksnc_proto->pro_queue_tx_msg(conn, tx);\r\n}\r\nif (ztx) {\r\natomic_sub(ztx->tx_nob, &conn->ksnc_tx_nob);\r\nlist_add_tail(&ztx->tx_list, &sched->kss_zombie_noop_txs);\r\n}\r\nif (conn->ksnc_tx_ready &&\r\n!conn->ksnc_tx_scheduled) {\r\nksocknal_conn_addref(conn);\r\nlist_add_tail(&conn->ksnc_tx_list, &sched->kss_tx_conns);\r\nconn->ksnc_tx_scheduled = 1;\r\nwake_up(&sched->kss_waitq);\r\n}\r\nspin_unlock_bh(&sched->kss_lock);\r\n}\r\nstruct ksock_route *\r\nksocknal_find_connectable_route_locked(struct ksock_peer *peer)\r\n{\r\nunsigned long now = cfs_time_current();\r\nstruct list_head *tmp;\r\nstruct ksock_route *route;\r\nlist_for_each(tmp, &peer->ksnp_routes) {\r\nroute = list_entry(tmp, struct ksock_route, ksnr_list);\r\nLASSERT(!route->ksnr_connecting || route->ksnr_scheduled);\r\nif (route->ksnr_scheduled)\r\ncontinue;\r\nif (!(ksocknal_route_mask() & ~route->ksnr_connected))\r\ncontinue;\r\nif (!(!route->ksnr_retry_interval ||\r\ncfs_time_aftereq(now, route->ksnr_timeout))) {\r\nCDEBUG(D_NET,\r\n"Too soon to retry route %pI4h (cnted %d, interval %ld, %ld secs later)\n",\r\n&route->ksnr_ipaddr,\r\nroute->ksnr_connected,\r\nroute->ksnr_retry_interval,\r\ncfs_duration_sec(route->ksnr_timeout - now));\r\ncontinue;\r\n}\r\nreturn route;\r\n}\r\nreturn NULL;\r\n}\r\nstruct ksock_route *\r\nksocknal_find_connecting_route_locked(struct ksock_peer *peer)\r\n{\r\nstruct list_head *tmp;\r\nstruct ksock_route *route;\r\nlist_for_each(tmp, &peer->ksnp_routes) {\r\nroute = list_entry(tmp, struct ksock_route, ksnr_list);\r\nLASSERT(!route->ksnr_connecting || route->ksnr_scheduled);\r\nif (route->ksnr_scheduled)\r\nreturn route;\r\n}\r\nreturn NULL;\r\n}\r\nint\r\nksocknal_launch_packet(struct lnet_ni *ni, struct ksock_tx *tx,\r\nstruct lnet_process_id id)\r\n{\r\nstruct ksock_peer *peer;\r\nstruct ksock_conn *conn;\r\nrwlock_t *g_lock;\r\nint retry;\r\nint rc;\r\nLASSERT(!tx->tx_conn);\r\ng_lock = &ksocknal_data.ksnd_global_lock;\r\nfor (retry = 0;; retry = 1) {\r\nread_lock(g_lock);\r\npeer = ksocknal_find_peer_locked(ni, id);\r\nif (peer) {\r\nif (!ksocknal_find_connectable_route_locked(peer)) {\r\nconn = ksocknal_find_conn_locked(peer, tx, tx->tx_nonblk);\r\nif (conn) {\r\nksocknal_queue_tx_locked(tx, conn);\r\nread_unlock(g_lock);\r\nreturn 0;\r\n}\r\n}\r\n}\r\nread_unlock(g_lock);\r\nwrite_lock_bh(g_lock);\r\npeer = ksocknal_find_peer_locked(ni, id);\r\nif (peer)\r\nbreak;\r\nwrite_unlock_bh(g_lock);\r\nif (id.pid & LNET_PID_USERFLAG) {\r\nCERROR("Refusing to create a connection to userspace process %s\n",\r\nlibcfs_id2str(id));\r\nreturn -EHOSTUNREACH;\r\n}\r\nif (retry) {\r\nCERROR("Can't find peer %s\n", libcfs_id2str(id));\r\nreturn -EHOSTUNREACH;\r\n}\r\nrc = ksocknal_add_peer(ni, id,\r\nLNET_NIDADDR(id.nid),\r\nlnet_acceptor_port());\r\nif (rc) {\r\nCERROR("Can't add peer %s: %d\n",\r\nlibcfs_id2str(id), rc);\r\nreturn rc;\r\n}\r\n}\r\nksocknal_launch_all_connections_locked(peer);\r\nconn = ksocknal_find_conn_locked(peer, tx, tx->tx_nonblk);\r\nif (conn) {\r\nksocknal_queue_tx_locked(tx, conn);\r\nwrite_unlock_bh(g_lock);\r\nreturn 0;\r\n}\r\nif (peer->ksnp_accepting > 0 ||\r\nksocknal_find_connecting_route_locked(peer)) {\r\ntx->tx_deadline =\r\ncfs_time_shift(*ksocknal_tunables.ksnd_timeout);\r\nlist_add_tail(&tx->tx_list, &peer->ksnp_tx_queue);\r\nwrite_unlock_bh(g_lock);\r\nreturn 0;\r\n}\r\nwrite_unlock_bh(g_lock);\r\nCNETERR("No usable routes to %s\n", libcfs_id2str(id));\r\nreturn -EHOSTUNREACH;\r\n}\r\nint\r\nksocknal_send(struct lnet_ni *ni, void *private, struct lnet_msg *lntmsg)\r\n{\r\nint mpflag = 1;\r\nint type = lntmsg->msg_type;\r\nstruct lnet_process_id target = lntmsg->msg_target;\r\nunsigned int payload_niov = lntmsg->msg_niov;\r\nstruct kvec *payload_iov = lntmsg->msg_iov;\r\nstruct bio_vec *payload_kiov = lntmsg->msg_kiov;\r\nunsigned int payload_offset = lntmsg->msg_offset;\r\nunsigned int payload_nob = lntmsg->msg_len;\r\nstruct ksock_tx *tx;\r\nint desc_size;\r\nint rc;\r\nCDEBUG(D_NET, "sending %u bytes in %d frags to %s\n",\r\npayload_nob, payload_niov, libcfs_id2str(target));\r\nLASSERT(!payload_nob || payload_niov > 0);\r\nLASSERT(payload_niov <= LNET_MAX_IOV);\r\nLASSERT(!(payload_kiov && payload_iov));\r\nLASSERT(!in_interrupt());\r\nif (payload_iov)\r\ndesc_size = offsetof(struct ksock_tx,\r\ntx_frags.virt.iov[1 + payload_niov]);\r\nelse\r\ndesc_size = offsetof(struct ksock_tx,\r\ntx_frags.paged.kiov[payload_niov]);\r\nif (lntmsg->msg_vmflush)\r\nmpflag = cfs_memory_pressure_get_and_set();\r\ntx = ksocknal_alloc_tx(KSOCK_MSG_LNET, desc_size);\r\nif (!tx) {\r\nCERROR("Can't allocate tx desc type %d size %d\n",\r\ntype, desc_size);\r\nif (lntmsg->msg_vmflush)\r\ncfs_memory_pressure_restore(mpflag);\r\nreturn -ENOMEM;\r\n}\r\ntx->tx_conn = NULL;\r\ntx->tx_lnetmsg = lntmsg;\r\nif (payload_iov) {\r\ntx->tx_kiov = NULL;\r\ntx->tx_nkiov = 0;\r\ntx->tx_iov = tx->tx_frags.virt.iov;\r\ntx->tx_niov = 1 +\r\nlnet_extract_iov(payload_niov, &tx->tx_iov[1],\r\npayload_niov, payload_iov,\r\npayload_offset, payload_nob);\r\n} else {\r\ntx->tx_niov = 1;\r\ntx->tx_iov = &tx->tx_frags.paged.iov;\r\ntx->tx_kiov = tx->tx_frags.paged.kiov;\r\ntx->tx_nkiov = lnet_extract_kiov(payload_niov, tx->tx_kiov,\r\npayload_niov, payload_kiov,\r\npayload_offset, payload_nob);\r\nif (payload_nob >= *ksocknal_tunables.ksnd_zc_min_payload)\r\ntx->tx_zc_capable = 1;\r\n}\r\ntx->tx_msg.ksm_csum = 0;\r\ntx->tx_msg.ksm_type = KSOCK_MSG_LNET;\r\ntx->tx_msg.ksm_zc_cookies[0] = 0;\r\ntx->tx_msg.ksm_zc_cookies[1] = 0;\r\nrc = ksocknal_launch_packet(ni, tx, target);\r\nif (!mpflag)\r\ncfs_memory_pressure_restore(mpflag);\r\nif (!rc)\r\nreturn 0;\r\nksocknal_free_tx(tx);\r\nreturn -EIO;\r\n}\r\nint\r\nksocknal_thread_start(int (*fn)(void *arg), void *arg, char *name)\r\n{\r\nstruct task_struct *task = kthread_run(fn, arg, "%s", name);\r\nif (IS_ERR(task))\r\nreturn PTR_ERR(task);\r\nwrite_lock_bh(&ksocknal_data.ksnd_global_lock);\r\nksocknal_data.ksnd_nthreads++;\r\nwrite_unlock_bh(&ksocknal_data.ksnd_global_lock);\r\nreturn 0;\r\n}\r\nvoid\r\nksocknal_thread_fini(void)\r\n{\r\nwrite_lock_bh(&ksocknal_data.ksnd_global_lock);\r\nksocknal_data.ksnd_nthreads--;\r\nwrite_unlock_bh(&ksocknal_data.ksnd_global_lock);\r\n}\r\nint\r\nksocknal_new_packet(struct ksock_conn *conn, int nob_to_skip)\r\n{\r\nstatic char ksocknal_slop_buffer[4096];\r\nint nob;\r\nunsigned int niov;\r\nint skipped;\r\nLASSERT(conn->ksnc_proto);\r\nif (*ksocknal_tunables.ksnd_eager_ack & conn->ksnc_type) {\r\nksocknal_lib_eager_ack(conn);\r\n}\r\nif (!nob_to_skip) {\r\nconn->ksnc_rx_started = 0;\r\nmb();\r\nswitch (conn->ksnc_proto->pro_version) {\r\ncase KSOCK_PROTO_V2:\r\ncase KSOCK_PROTO_V3:\r\nconn->ksnc_rx_state = SOCKNAL_RX_KSM_HEADER;\r\nconn->ksnc_rx_iov = (struct kvec *)&conn->ksnc_rx_iov_space;\r\nconn->ksnc_rx_iov[0].iov_base = &conn->ksnc_msg;\r\nconn->ksnc_rx_nob_wanted = offsetof(struct ksock_msg, ksm_u);\r\nconn->ksnc_rx_nob_left = offsetof(struct ksock_msg, ksm_u);\r\nconn->ksnc_rx_iov[0].iov_len = offsetof(struct ksock_msg, ksm_u);\r\nbreak;\r\ncase KSOCK_PROTO_V1:\r\nconn->ksnc_rx_state = SOCKNAL_RX_LNET_HEADER;\r\nconn->ksnc_rx_nob_wanted = sizeof(struct lnet_hdr);\r\nconn->ksnc_rx_nob_left = sizeof(struct lnet_hdr);\r\nconn->ksnc_rx_iov = (struct kvec *)&conn->ksnc_rx_iov_space;\r\nconn->ksnc_rx_iov[0].iov_base = &conn->ksnc_msg.ksm_u.lnetmsg;\r\nconn->ksnc_rx_iov[0].iov_len = sizeof(struct lnet_hdr);\r\nbreak;\r\ndefault:\r\nLBUG();\r\n}\r\nconn->ksnc_rx_niov = 1;\r\nconn->ksnc_rx_kiov = NULL;\r\nconn->ksnc_rx_nkiov = 0;\r\nconn->ksnc_rx_csum = ~0;\r\nreturn 1;\r\n}\r\nconn->ksnc_rx_state = SOCKNAL_RX_SLOP;\r\nconn->ksnc_rx_nob_left = nob_to_skip;\r\nconn->ksnc_rx_iov = (struct kvec *)&conn->ksnc_rx_iov_space;\r\nskipped = 0;\r\nniov = 0;\r\ndo {\r\nnob = min_t(int, nob_to_skip, sizeof(ksocknal_slop_buffer));\r\nconn->ksnc_rx_iov[niov].iov_base = ksocknal_slop_buffer;\r\nconn->ksnc_rx_iov[niov].iov_len = nob;\r\nniov++;\r\nskipped += nob;\r\nnob_to_skip -= nob;\r\n} while (nob_to_skip &&\r\nniov < sizeof(conn->ksnc_rx_iov_space) / sizeof(struct iovec));\r\nconn->ksnc_rx_niov = niov;\r\nconn->ksnc_rx_kiov = NULL;\r\nconn->ksnc_rx_nkiov = 0;\r\nconn->ksnc_rx_nob_wanted = skipped;\r\nreturn 0;\r\n}\r\nstatic int\r\nksocknal_process_receive(struct ksock_conn *conn)\r\n{\r\nstruct lnet_hdr *lhdr;\r\nstruct lnet_process_id *id;\r\nint rc;\r\nLASSERT(atomic_read(&conn->ksnc_conn_refcount) > 0);\r\nLASSERT(conn->ksnc_rx_state == SOCKNAL_RX_KSM_HEADER ||\r\nconn->ksnc_rx_state == SOCKNAL_RX_LNET_PAYLOAD ||\r\nconn->ksnc_rx_state == SOCKNAL_RX_LNET_HEADER ||\r\nconn->ksnc_rx_state == SOCKNAL_RX_SLOP);\r\nagain:\r\nif (conn->ksnc_rx_nob_wanted) {\r\nrc = ksocknal_receive(conn);\r\nif (rc <= 0) {\r\nLASSERT(rc != -EAGAIN);\r\nif (!rc)\r\nCDEBUG(D_NET, "[%p] EOF from %s ip %pI4h:%d\n",\r\nconn,\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port);\r\nelse if (!conn->ksnc_closing)\r\nCERROR("[%p] Error %d on read from %s ip %pI4h:%d\n",\r\nconn, rc,\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port);\r\nksocknal_close_conn_and_siblings(conn,\r\n(conn->ksnc_closing) ? 0 : rc);\r\nreturn (!rc ? -ESHUTDOWN : rc);\r\n}\r\nif (conn->ksnc_rx_nob_wanted) {\r\nreturn -EAGAIN;\r\n}\r\n}\r\nswitch (conn->ksnc_rx_state) {\r\ncase SOCKNAL_RX_KSM_HEADER:\r\nif (conn->ksnc_flip) {\r\n__swab32s(&conn->ksnc_msg.ksm_type);\r\n__swab32s(&conn->ksnc_msg.ksm_csum);\r\n__swab64s(&conn->ksnc_msg.ksm_zc_cookies[0]);\r\n__swab64s(&conn->ksnc_msg.ksm_zc_cookies[1]);\r\n}\r\nif (conn->ksnc_msg.ksm_type != KSOCK_MSG_NOOP &&\r\nconn->ksnc_msg.ksm_type != KSOCK_MSG_LNET) {\r\nCERROR("%s: Unknown message type: %x\n",\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\nconn->ksnc_msg.ksm_type);\r\nksocknal_new_packet(conn, 0);\r\nksocknal_close_conn_and_siblings(conn, -EPROTO);\r\nreturn -EPROTO;\r\n}\r\nif (conn->ksnc_msg.ksm_type == KSOCK_MSG_NOOP &&\r\nconn->ksnc_msg.ksm_csum &&\r\nconn->ksnc_msg.ksm_csum != conn->ksnc_rx_csum) {\r\nCERROR("%s: Checksum error, wire:0x%08X data:0x%08X\n",\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\nconn->ksnc_msg.ksm_csum, conn->ksnc_rx_csum);\r\nksocknal_new_packet(conn, 0);\r\nksocknal_close_conn_and_siblings(conn, -EPROTO);\r\nreturn -EIO;\r\n}\r\nif (conn->ksnc_msg.ksm_zc_cookies[1]) {\r\n__u64 cookie = 0;\r\nLASSERT(conn->ksnc_proto != &ksocknal_protocol_v1x);\r\nif (conn->ksnc_msg.ksm_type == KSOCK_MSG_NOOP)\r\ncookie = conn->ksnc_msg.ksm_zc_cookies[0];\r\nrc = conn->ksnc_proto->pro_handle_zcack(conn, cookie,\r\nconn->ksnc_msg.ksm_zc_cookies[1]);\r\nif (rc) {\r\nCERROR("%s: Unknown ZC-ACK cookie: %llu, %llu\n",\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\ncookie, conn->ksnc_msg.ksm_zc_cookies[1]);\r\nksocknal_new_packet(conn, 0);\r\nksocknal_close_conn_and_siblings(conn, -EPROTO);\r\nreturn rc;\r\n}\r\n}\r\nif (conn->ksnc_msg.ksm_type == KSOCK_MSG_NOOP) {\r\nksocknal_new_packet(conn, 0);\r\nreturn 0;\r\n}\r\nconn->ksnc_rx_state = SOCKNAL_RX_LNET_HEADER;\r\nconn->ksnc_rx_nob_wanted = sizeof(struct ksock_lnet_msg);\r\nconn->ksnc_rx_nob_left = sizeof(struct ksock_lnet_msg);\r\nconn->ksnc_rx_iov = (struct kvec *)&conn->ksnc_rx_iov_space;\r\nconn->ksnc_rx_iov[0].iov_base = &conn->ksnc_msg.ksm_u.lnetmsg;\r\nconn->ksnc_rx_iov[0].iov_len = sizeof(struct ksock_lnet_msg);\r\nconn->ksnc_rx_niov = 1;\r\nconn->ksnc_rx_kiov = NULL;\r\nconn->ksnc_rx_nkiov = 0;\r\ngoto again;\r\ncase SOCKNAL_RX_LNET_HEADER:\r\nconn->ksnc_proto->pro_unpack(&conn->ksnc_msg);\r\nif (conn->ksnc_peer->ksnp_id.pid & LNET_PID_USERFLAG) {\r\nlhdr = &conn->ksnc_msg.ksm_u.lnetmsg.ksnm_hdr;\r\nid = &conn->ksnc_peer->ksnp_id;\r\nlhdr->src_pid = cpu_to_le32(id->pid);\r\nlhdr->src_nid = cpu_to_le64(id->nid);\r\n}\r\nconn->ksnc_rx_state = SOCKNAL_RX_PARSE;\r\nksocknal_conn_addref(conn);\r\nrc = lnet_parse(conn->ksnc_peer->ksnp_ni,\r\n&conn->ksnc_msg.ksm_u.lnetmsg.ksnm_hdr,\r\nconn->ksnc_peer->ksnp_id.nid, conn, 0);\r\nif (rc < 0) {\r\nksocknal_new_packet(conn, 0);\r\nksocknal_close_conn_and_siblings(conn, rc);\r\nksocknal_conn_decref(conn);\r\nreturn -EPROTO;\r\n}\r\nLASSERT(conn->ksnc_rx_state == SOCKNAL_RX_PARSE ||\r\nconn->ksnc_rx_state == SOCKNAL_RX_LNET_PAYLOAD);\r\nif (conn->ksnc_rx_state != SOCKNAL_RX_LNET_PAYLOAD)\r\nreturn 0;\r\ngoto again;\r\ncase SOCKNAL_RX_LNET_PAYLOAD:\r\nrc = 0;\r\nif (!conn->ksnc_rx_nob_left &&\r\nconn->ksnc_msg.ksm_csum &&\r\nconn->ksnc_msg.ksm_csum != conn->ksnc_rx_csum) {\r\nCERROR("%s: Checksum error, wire:0x%08X data:0x%08X\n",\r\nlibcfs_id2str(conn->ksnc_peer->ksnp_id),\r\nconn->ksnc_msg.ksm_csum, conn->ksnc_rx_csum);\r\nrc = -EIO;\r\n}\r\nif (!rc && conn->ksnc_msg.ksm_zc_cookies[0]) {\r\nLASSERT(conn->ksnc_proto != &ksocknal_protocol_v1x);\r\nlhdr = &conn->ksnc_msg.ksm_u.lnetmsg.ksnm_hdr;\r\nid = &conn->ksnc_peer->ksnp_id;\r\nrc = conn->ksnc_proto->pro_handle_zcreq(conn,\r\nconn->ksnc_msg.ksm_zc_cookies[0],\r\n*ksocknal_tunables.ksnd_nonblk_zcack ||\r\nle64_to_cpu(lhdr->src_nid) != id->nid);\r\n}\r\nlnet_finalize(conn->ksnc_peer->ksnp_ni, conn->ksnc_cookie, rc);\r\nif (rc) {\r\nksocknal_new_packet(conn, 0);\r\nksocknal_close_conn_and_siblings(conn, rc);\r\nreturn -EPROTO;\r\n}\r\ncase SOCKNAL_RX_SLOP:\r\nif (ksocknal_new_packet(conn, conn->ksnc_rx_nob_left))\r\nreturn 0;\r\ngoto again;\r\ndefault:\r\nbreak;\r\n}\r\nLBUG();\r\nreturn -EINVAL;\r\n}\r\nint\r\nksocknal_recv(struct lnet_ni *ni, void *private, struct lnet_msg *msg,\r\nint delayed, struct iov_iter *to, unsigned int rlen)\r\n{\r\nstruct ksock_conn *conn = private;\r\nstruct ksock_sched *sched = conn->ksnc_scheduler;\r\nLASSERT(iov_iter_count(to) <= rlen);\r\nLASSERT(to->nr_segs <= LNET_MAX_IOV);\r\nconn->ksnc_cookie = msg;\r\nconn->ksnc_rx_nob_wanted = iov_iter_count(to);\r\nconn->ksnc_rx_nob_left = rlen;\r\nif (to->type & ITER_KVEC) {\r\nconn->ksnc_rx_nkiov = 0;\r\nconn->ksnc_rx_kiov = NULL;\r\nconn->ksnc_rx_iov = conn->ksnc_rx_iov_space.iov;\r\nconn->ksnc_rx_niov =\r\nlnet_extract_iov(LNET_MAX_IOV, conn->ksnc_rx_iov,\r\nto->nr_segs, to->kvec,\r\nto->iov_offset, iov_iter_count(to));\r\n} else {\r\nconn->ksnc_rx_niov = 0;\r\nconn->ksnc_rx_iov = NULL;\r\nconn->ksnc_rx_kiov = conn->ksnc_rx_iov_space.kiov;\r\nconn->ksnc_rx_nkiov =\r\nlnet_extract_kiov(LNET_MAX_IOV, conn->ksnc_rx_kiov,\r\nto->nr_segs, to->bvec,\r\nto->iov_offset, iov_iter_count(to));\r\n}\r\nLASSERT(conn->ksnc_rx_scheduled);\r\nspin_lock_bh(&sched->kss_lock);\r\nswitch (conn->ksnc_rx_state) {\r\ncase SOCKNAL_RX_PARSE_WAIT:\r\nlist_add_tail(&conn->ksnc_rx_list, &sched->kss_rx_conns);\r\nwake_up(&sched->kss_waitq);\r\nLASSERT(conn->ksnc_rx_ready);\r\nbreak;\r\ncase SOCKNAL_RX_PARSE:\r\nbreak;\r\n}\r\nconn->ksnc_rx_state = SOCKNAL_RX_LNET_PAYLOAD;\r\nspin_unlock_bh(&sched->kss_lock);\r\nksocknal_conn_decref(conn);\r\nreturn 0;\r\n}\r\nstatic inline int\r\nksocknal_sched_cansleep(struct ksock_sched *sched)\r\n{\r\nint rc;\r\nspin_lock_bh(&sched->kss_lock);\r\nrc = !ksocknal_data.ksnd_shuttingdown &&\r\nlist_empty(&sched->kss_rx_conns) &&\r\nlist_empty(&sched->kss_tx_conns);\r\nspin_unlock_bh(&sched->kss_lock);\r\nreturn rc;\r\n}\r\nint ksocknal_scheduler(void *arg)\r\n{\r\nstruct ksock_sched_info *info;\r\nstruct ksock_sched *sched;\r\nstruct ksock_conn *conn;\r\nstruct ksock_tx *tx;\r\nint rc;\r\nint nloops = 0;\r\nlong id = (long)arg;\r\ninfo = ksocknal_data.ksnd_sched_info[KSOCK_THREAD_CPT(id)];\r\nsched = &info->ksi_scheds[KSOCK_THREAD_SID(id)];\r\ncfs_block_allsigs();\r\nrc = cfs_cpt_bind(lnet_cpt_table(), info->ksi_cpt);\r\nif (rc) {\r\nCWARN("Can't set CPU partition affinity to %d: %d\n",\r\ninfo->ksi_cpt, rc);\r\n}\r\nspin_lock_bh(&sched->kss_lock);\r\nwhile (!ksocknal_data.ksnd_shuttingdown) {\r\nint did_something = 0;\r\nif (!list_empty(&sched->kss_rx_conns)) {\r\nconn = list_entry(sched->kss_rx_conns.next,\r\nstruct ksock_conn, ksnc_rx_list);\r\nlist_del(&conn->ksnc_rx_list);\r\nLASSERT(conn->ksnc_rx_scheduled);\r\nLASSERT(conn->ksnc_rx_ready);\r\nconn->ksnc_rx_ready = 0;\r\nspin_unlock_bh(&sched->kss_lock);\r\nrc = ksocknal_process_receive(conn);\r\nspin_lock_bh(&sched->kss_lock);\r\nLASSERT(conn->ksnc_rx_scheduled);\r\nif (!rc)\r\nconn->ksnc_rx_ready = 1;\r\nif (conn->ksnc_rx_state == SOCKNAL_RX_PARSE) {\r\nconn->ksnc_rx_state = SOCKNAL_RX_PARSE_WAIT;\r\n} else if (conn->ksnc_rx_ready) {\r\nlist_add_tail(&conn->ksnc_rx_list,\r\n&sched->kss_rx_conns);\r\n} else {\r\nconn->ksnc_rx_scheduled = 0;\r\nksocknal_conn_decref(conn);\r\n}\r\ndid_something = 1;\r\n}\r\nif (!list_empty(&sched->kss_tx_conns)) {\r\nLIST_HEAD(zlist);\r\nif (!list_empty(&sched->kss_zombie_noop_txs)) {\r\nlist_add(&zlist, &sched->kss_zombie_noop_txs);\r\nlist_del_init(&sched->kss_zombie_noop_txs);\r\n}\r\nconn = list_entry(sched->kss_tx_conns.next,\r\nstruct ksock_conn, ksnc_tx_list);\r\nlist_del(&conn->ksnc_tx_list);\r\nLASSERT(conn->ksnc_tx_scheduled);\r\nLASSERT(conn->ksnc_tx_ready);\r\nLASSERT(!list_empty(&conn->ksnc_tx_queue));\r\ntx = list_entry(conn->ksnc_tx_queue.next,\r\nstruct ksock_tx, tx_list);\r\nif (conn->ksnc_tx_carrier == tx)\r\nksocknal_next_tx_carrier(conn);\r\nlist_del(&tx->tx_list);\r\nconn->ksnc_tx_ready = 0;\r\nspin_unlock_bh(&sched->kss_lock);\r\nif (!list_empty(&zlist)) {\r\nksocknal_txlist_done(NULL, &zlist, 0);\r\n}\r\nrc = ksocknal_process_transmit(conn, tx);\r\nif (rc == -ENOMEM || rc == -EAGAIN) {\r\nspin_lock_bh(&sched->kss_lock);\r\nlist_add(&tx->tx_list, &conn->ksnc_tx_queue);\r\n} else {\r\nksocknal_tx_decref(tx);\r\nspin_lock_bh(&sched->kss_lock);\r\nconn->ksnc_tx_ready = 1;\r\n}\r\nif (rc == -ENOMEM) {\r\n} else if (conn->ksnc_tx_ready &&\r\n!list_empty(&conn->ksnc_tx_queue)) {\r\nlist_add_tail(&conn->ksnc_tx_list,\r\n&sched->kss_tx_conns);\r\n} else {\r\nconn->ksnc_tx_scheduled = 0;\r\nksocknal_conn_decref(conn);\r\n}\r\ndid_something = 1;\r\n}\r\nif (!did_something ||\r\n++nloops == SOCKNAL_RESCHED) {\r\nspin_unlock_bh(&sched->kss_lock);\r\nnloops = 0;\r\nif (!did_something) {\r\nrc = wait_event_interruptible_exclusive(\r\nsched->kss_waitq,\r\n!ksocknal_sched_cansleep(sched));\r\nLASSERT(!rc);\r\n} else {\r\ncond_resched();\r\n}\r\nspin_lock_bh(&sched->kss_lock);\r\n}\r\n}\r\nspin_unlock_bh(&sched->kss_lock);\r\nksocknal_thread_fini();\r\nreturn 0;\r\n}\r\nvoid ksocknal_read_callback(struct ksock_conn *conn)\r\n{\r\nstruct ksock_sched *sched;\r\nsched = conn->ksnc_scheduler;\r\nspin_lock_bh(&sched->kss_lock);\r\nconn->ksnc_rx_ready = 1;\r\nif (!conn->ksnc_rx_scheduled) {\r\nlist_add_tail(&conn->ksnc_rx_list, &sched->kss_rx_conns);\r\nconn->ksnc_rx_scheduled = 1;\r\nksocknal_conn_addref(conn);\r\nwake_up(&sched->kss_waitq);\r\n}\r\nspin_unlock_bh(&sched->kss_lock);\r\n}\r\nvoid ksocknal_write_callback(struct ksock_conn *conn)\r\n{\r\nstruct ksock_sched *sched;\r\nsched = conn->ksnc_scheduler;\r\nspin_lock_bh(&sched->kss_lock);\r\nconn->ksnc_tx_ready = 1;\r\nif (!conn->ksnc_tx_scheduled &&\r\n!list_empty(&conn->ksnc_tx_queue)) {\r\nlist_add_tail(&conn->ksnc_tx_list, &sched->kss_tx_conns);\r\nconn->ksnc_tx_scheduled = 1;\r\nksocknal_conn_addref(conn);\r\nwake_up(&sched->kss_waitq);\r\n}\r\nspin_unlock_bh(&sched->kss_lock);\r\n}\r\nstatic struct ksock_proto *\r\nksocknal_parse_proto_version(struct ksock_hello_msg *hello)\r\n{\r\n__u32 version = 0;\r\nif (hello->kshm_magic == LNET_PROTO_MAGIC)\r\nversion = hello->kshm_version;\r\nelse if (hello->kshm_magic == __swab32(LNET_PROTO_MAGIC))\r\nversion = __swab32(hello->kshm_version);\r\nif (version) {\r\n#if SOCKNAL_VERSION_DEBUG\r\nif (*ksocknal_tunables.ksnd_protocol == 1)\r\nreturn NULL;\r\nif (*ksocknal_tunables.ksnd_protocol == 2 &&\r\nversion == KSOCK_PROTO_V3)\r\nreturn NULL;\r\n#endif\r\nif (version == KSOCK_PROTO_V2)\r\nreturn &ksocknal_protocol_v2x;\r\nif (version == KSOCK_PROTO_V3)\r\nreturn &ksocknal_protocol_v3x;\r\nreturn NULL;\r\n}\r\nif (hello->kshm_magic == le32_to_cpu(LNET_PROTO_TCP_MAGIC)) {\r\nstruct lnet_magicversion *hmv = (struct lnet_magicversion *)hello;\r\nBUILD_BUG_ON(sizeof(struct lnet_magicversion) !=\r\noffsetof(struct ksock_hello_msg, kshm_src_nid));\r\nif (hmv->version_major == cpu_to_le16(KSOCK_PROTO_V1_MAJOR) &&\r\nhmv->version_minor == cpu_to_le16(KSOCK_PROTO_V1_MINOR))\r\nreturn &ksocknal_protocol_v1x;\r\n}\r\nreturn NULL;\r\n}\r\nint\r\nksocknal_send_hello(struct lnet_ni *ni, struct ksock_conn *conn,\r\nlnet_nid_t peer_nid, struct ksock_hello_msg *hello)\r\n{\r\nstruct ksock_net *net = (struct ksock_net *)ni->ni_data;\r\nLASSERT(hello->kshm_nips <= LNET_MAX_INTERFACES);\r\nLASSERT(conn->ksnc_proto);\r\nhello->kshm_src_nid = ni->ni_nid;\r\nhello->kshm_dst_nid = peer_nid;\r\nhello->kshm_src_pid = the_lnet.ln_pid;\r\nhello->kshm_src_incarnation = net->ksnn_incarnation;\r\nhello->kshm_ctype = conn->ksnc_type;\r\nreturn conn->ksnc_proto->pro_send_hello(conn, hello);\r\n}\r\nstatic int\r\nksocknal_invert_type(int type)\r\n{\r\nswitch (type) {\r\ncase SOCKLND_CONN_ANY:\r\ncase SOCKLND_CONN_CONTROL:\r\nreturn type;\r\ncase SOCKLND_CONN_BULK_IN:\r\nreturn SOCKLND_CONN_BULK_OUT;\r\ncase SOCKLND_CONN_BULK_OUT:\r\nreturn SOCKLND_CONN_BULK_IN;\r\ndefault:\r\nreturn SOCKLND_CONN_NONE;\r\n}\r\n}\r\nint\r\nksocknal_recv_hello(struct lnet_ni *ni, struct ksock_conn *conn,\r\nstruct ksock_hello_msg *hello,\r\nstruct lnet_process_id *peerid,\r\n__u64 *incarnation)\r\n{\r\nstruct socket *sock = conn->ksnc_sock;\r\nint active = !!conn->ksnc_proto;\r\nint timeout;\r\nint proto_match;\r\nint rc;\r\nstruct ksock_proto *proto;\r\nstruct lnet_process_id recv_id;\r\nLASSERT(!active == !(conn->ksnc_type != SOCKLND_CONN_NONE));\r\ntimeout = active ? *ksocknal_tunables.ksnd_timeout :\r\nlnet_acceptor_timeout();\r\nrc = lnet_sock_read(sock, &hello->kshm_magic,\r\nsizeof(hello->kshm_magic), timeout);\r\nif (rc) {\r\nCERROR("Error %d reading HELLO from %pI4h\n",\r\nrc, &conn->ksnc_ipaddr);\r\nLASSERT(rc < 0);\r\nreturn rc;\r\n}\r\nif (hello->kshm_magic != LNET_PROTO_MAGIC &&\r\nhello->kshm_magic != __swab32(LNET_PROTO_MAGIC) &&\r\nhello->kshm_magic != le32_to_cpu(LNET_PROTO_TCP_MAGIC)) {\r\nCERROR("Bad magic(1) %#08x (%#08x expected) from %pI4h\n",\r\n__cpu_to_le32(hello->kshm_magic),\r\nLNET_PROTO_TCP_MAGIC,\r\n&conn->ksnc_ipaddr);\r\nreturn -EPROTO;\r\n}\r\nrc = lnet_sock_read(sock, &hello->kshm_version,\r\nsizeof(hello->kshm_version), timeout);\r\nif (rc) {\r\nCERROR("Error %d reading HELLO from %pI4h\n",\r\nrc, &conn->ksnc_ipaddr);\r\nLASSERT(rc < 0);\r\nreturn rc;\r\n}\r\nproto = ksocknal_parse_proto_version(hello);\r\nif (!proto) {\r\nif (!active) {\r\nconn->ksnc_proto = &ksocknal_protocol_v3x;\r\n#if SOCKNAL_VERSION_DEBUG\r\nif (*ksocknal_tunables.ksnd_protocol == 2)\r\nconn->ksnc_proto = &ksocknal_protocol_v2x;\r\nelse if (*ksocknal_tunables.ksnd_protocol == 1)\r\nconn->ksnc_proto = &ksocknal_protocol_v1x;\r\n#endif\r\nhello->kshm_nips = 0;\r\nksocknal_send_hello(ni, conn, ni->ni_nid, hello);\r\n}\r\nCERROR("Unknown protocol version (%d.x expected) from %pI4h\n",\r\nconn->ksnc_proto->pro_version,\r\n&conn->ksnc_ipaddr);\r\nreturn -EPROTO;\r\n}\r\nproto_match = (conn->ksnc_proto == proto);\r\nconn->ksnc_proto = proto;\r\nrc = conn->ksnc_proto->pro_recv_hello(conn, hello, timeout);\r\nif (rc) {\r\nCERROR("Error %d reading or checking hello from from %pI4h\n",\r\nrc, &conn->ksnc_ipaddr);\r\nLASSERT(rc < 0);\r\nreturn rc;\r\n}\r\n*incarnation = hello->kshm_src_incarnation;\r\nif (hello->kshm_src_nid == LNET_NID_ANY) {\r\nCERROR("Expecting a HELLO hdr with a NID, but got LNET_NID_ANY from %pI4h\n",\r\n&conn->ksnc_ipaddr);\r\nreturn -EPROTO;\r\n}\r\nif (!active &&\r\nconn->ksnc_port > LNET_ACCEPTOR_MAX_RESERVED_PORT) {\r\nrecv_id.pid = conn->ksnc_port | LNET_PID_USERFLAG;\r\nrecv_id.nid = LNET_MKNID(LNET_NIDNET(ni->ni_nid),\r\nconn->ksnc_ipaddr);\r\n} else {\r\nrecv_id.nid = hello->kshm_src_nid;\r\nrecv_id.pid = hello->kshm_src_pid;\r\n}\r\nif (!active) {\r\n*peerid = recv_id;\r\nconn->ksnc_type = ksocknal_invert_type(hello->kshm_ctype);\r\nif (conn->ksnc_type == SOCKLND_CONN_NONE) {\r\nCERROR("Unexpected type %d from %s ip %pI4h\n",\r\nhello->kshm_ctype, libcfs_id2str(*peerid),\r\n&conn->ksnc_ipaddr);\r\nreturn -EPROTO;\r\n}\r\nreturn 0;\r\n}\r\nif (peerid->pid != recv_id.pid ||\r\npeerid->nid != recv_id.nid) {\r\nLCONSOLE_ERROR_MSG(0x130, "Connected successfully to %s on host %pI4h, but they claimed they were %s; please check your Lustre configuration.\n",\r\nlibcfs_id2str(*peerid),\r\n&conn->ksnc_ipaddr,\r\nlibcfs_id2str(recv_id));\r\nreturn -EPROTO;\r\n}\r\nif (hello->kshm_ctype == SOCKLND_CONN_NONE) {\r\nreturn proto_match ? EALREADY : EPROTO;\r\n}\r\nif (ksocknal_invert_type(hello->kshm_ctype) != conn->ksnc_type) {\r\nCERROR("Mismatched types: me %d, %s ip %pI4h %d\n",\r\nconn->ksnc_type, libcfs_id2str(*peerid),\r\n&conn->ksnc_ipaddr, hello->kshm_ctype);\r\nreturn -EPROTO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nksocknal_connect(struct ksock_route *route)\r\n{\r\nLIST_HEAD(zombies);\r\nstruct ksock_peer *peer = route->ksnr_peer;\r\nint type;\r\nint wanted;\r\nstruct socket *sock;\r\nunsigned long deadline;\r\nint retry_later = 0;\r\nint rc = 0;\r\ndeadline = cfs_time_add(cfs_time_current(),\r\ncfs_time_seconds(*ksocknal_tunables.ksnd_timeout));\r\nwrite_lock_bh(&ksocknal_data.ksnd_global_lock);\r\nLASSERT(route->ksnr_scheduled);\r\nLASSERT(!route->ksnr_connecting);\r\nroute->ksnr_connecting = 1;\r\nfor (;;) {\r\nwanted = ksocknal_route_mask() & ~route->ksnr_connected;\r\nif (peer->ksnp_closing || route->ksnr_deleted ||\r\n!wanted) {\r\nretry_later = 0;\r\nbreak;\r\n}\r\nif (peer->ksnp_accepting > 0) {\r\nCDEBUG(D_NET,\r\n"peer %s(%d) already connecting to me, retry later.\n",\r\nlibcfs_nid2str(peer->ksnp_id.nid),\r\npeer->ksnp_accepting);\r\nretry_later = 1;\r\n}\r\nif (retry_later)\r\nbreak;\r\nif (wanted & BIT(SOCKLND_CONN_ANY)) {\r\ntype = SOCKLND_CONN_ANY;\r\n} else if (wanted & BIT(SOCKLND_CONN_CONTROL)) {\r\ntype = SOCKLND_CONN_CONTROL;\r\n} else if (wanted & BIT(SOCKLND_CONN_BULK_IN)) {\r\ntype = SOCKLND_CONN_BULK_IN;\r\n} else {\r\nLASSERT(wanted & BIT(SOCKLND_CONN_BULK_OUT));\r\ntype = SOCKLND_CONN_BULK_OUT;\r\n}\r\nwrite_unlock_bh(&ksocknal_data.ksnd_global_lock);\r\nif (cfs_time_aftereq(cfs_time_current(), deadline)) {\r\nrc = -ETIMEDOUT;\r\nlnet_connect_console_error(rc, peer->ksnp_id.nid,\r\nroute->ksnr_ipaddr,\r\nroute->ksnr_port);\r\ngoto failed;\r\n}\r\nrc = lnet_connect(&sock, peer->ksnp_id.nid,\r\nroute->ksnr_myipaddr,\r\nroute->ksnr_ipaddr, route->ksnr_port);\r\nif (rc)\r\ngoto failed;\r\nrc = ksocknal_create_conn(peer->ksnp_ni, route, sock, type);\r\nif (rc < 0) {\r\nlnet_connect_console_error(rc, peer->ksnp_id.nid,\r\nroute->ksnr_ipaddr,\r\nroute->ksnr_port);\r\ngoto failed;\r\n}\r\nretry_later = (rc);\r\nif (retry_later)\r\nCDEBUG(D_NET, "peer %s: conn race, retry later.\n",\r\nlibcfs_nid2str(peer->ksnp_id.nid));\r\nwrite_lock_bh(&ksocknal_data.ksnd_global_lock);\r\n}\r\nroute->ksnr_scheduled = 0;\r\nroute->ksnr_connecting = 0;\r\nif (retry_later) {\r\nif (rc == EALREADY ||\r\n(!rc && peer->ksnp_accepting > 0)) {\r\nroute->ksnr_retry_interval =\r\ncfs_time_seconds(*ksocknal_tunables.ksnd_min_reconnectms) / 1000;\r\nroute->ksnr_timeout = cfs_time_add(cfs_time_current(),\r\nroute->ksnr_retry_interval);\r\n}\r\nksocknal_launch_connection_locked(route);\r\n}\r\nwrite_unlock_bh(&ksocknal_data.ksnd_global_lock);\r\nreturn retry_later;\r\nfailed:\r\nwrite_lock_bh(&ksocknal_data.ksnd_global_lock);\r\nroute->ksnr_scheduled = 0;\r\nroute->ksnr_connecting = 0;\r\nroute->ksnr_retry_interval *= 2;\r\nroute->ksnr_retry_interval =\r\nmax(route->ksnr_retry_interval,\r\ncfs_time_seconds(*ksocknal_tunables.ksnd_min_reconnectms) / 1000);\r\nroute->ksnr_retry_interval =\r\nmin(route->ksnr_retry_interval,\r\ncfs_time_seconds(*ksocknal_tunables.ksnd_max_reconnectms) / 1000);\r\nLASSERT(route->ksnr_retry_interval);\r\nroute->ksnr_timeout = cfs_time_add(cfs_time_current(),\r\nroute->ksnr_retry_interval);\r\nif (!list_empty(&peer->ksnp_tx_queue) &&\r\n!peer->ksnp_accepting &&\r\n!ksocknal_find_connecting_route_locked(peer)) {\r\nstruct ksock_conn *conn;\r\nif (!list_empty(&peer->ksnp_conns)) {\r\nconn = list_entry(peer->ksnp_conns.next,\r\nstruct ksock_conn, ksnc_list);\r\nLASSERT(conn->ksnc_proto == &ksocknal_protocol_v3x);\r\n}\r\nlist_splice_init(&peer->ksnp_tx_queue, &zombies);\r\n}\r\nwrite_unlock_bh(&ksocknal_data.ksnd_global_lock);\r\nksocknal_peer_failed(peer);\r\nksocknal_txlist_done(peer->ksnp_ni, &zombies, 1);\r\nreturn 0;\r\n}\r\nstatic int\r\nksocknal_connd_check_start(time64_t sec, long *timeout)\r\n{\r\nchar name[16];\r\nint rc;\r\nint total = ksocknal_data.ksnd_connd_starting +\r\nksocknal_data.ksnd_connd_running;\r\nif (unlikely(ksocknal_data.ksnd_init < SOCKNAL_INIT_ALL)) {\r\nreturn 0;\r\n}\r\nif (total >= *ksocknal_tunables.ksnd_nconnds_max ||\r\ntotal > ksocknal_data.ksnd_connd_connecting + SOCKNAL_CONND_RESV) {\r\nreturn 0;\r\n}\r\nif (list_empty(&ksocknal_data.ksnd_connd_routes)) {\r\nreturn 0;\r\n}\r\nif (sec - ksocknal_data.ksnd_connd_failed_stamp <= 1) {\r\n*timeout = cfs_time_seconds(1);\r\nreturn 0;\r\n}\r\nif (ksocknal_data.ksnd_connd_starting > 0) {\r\nreturn 0;\r\n}\r\nksocknal_data.ksnd_connd_starting_stamp = sec;\r\nksocknal_data.ksnd_connd_starting++;\r\nspin_unlock_bh(&ksocknal_data.ksnd_connd_lock);\r\nsnprintf(name, sizeof(name), "socknal_cd%02d", total);\r\nrc = ksocknal_thread_start(ksocknal_connd, NULL, name);\r\nspin_lock_bh(&ksocknal_data.ksnd_connd_lock);\r\nif (!rc)\r\nreturn 1;\r\nLASSERT(ksocknal_data.ksnd_connd_starting > 0);\r\nksocknal_data.ksnd_connd_starting--;\r\nksocknal_data.ksnd_connd_failed_stamp = ktime_get_real_seconds();\r\nreturn 1;\r\n}\r\nstatic int\r\nksocknal_connd_check_stop(time64_t sec, long *timeout)\r\n{\r\nint val;\r\nif (unlikely(ksocknal_data.ksnd_init < SOCKNAL_INIT_ALL)) {\r\nreturn 0;\r\n}\r\nif (ksocknal_data.ksnd_connd_starting > 0) {\r\nreturn 0;\r\n}\r\nif (ksocknal_data.ksnd_connd_running <=\r\n*ksocknal_tunables.ksnd_nconnds) {\r\nreturn 0;\r\n}\r\nval = (int)(ksocknal_data.ksnd_connd_starting_stamp +\r\nSOCKNAL_CONND_TIMEOUT - sec);\r\n*timeout = (val > 0) ? cfs_time_seconds(val) :\r\ncfs_time_seconds(SOCKNAL_CONND_TIMEOUT);\r\nif (val > 0)\r\nreturn 0;\r\nreturn ksocknal_data.ksnd_connd_running >\r\nksocknal_data.ksnd_connd_connecting + SOCKNAL_CONND_RESV;\r\n}\r\nstatic struct ksock_route *\r\nksocknal_connd_get_route_locked(signed long *timeout_p)\r\n{\r\nstruct ksock_route *route;\r\nunsigned long now;\r\nnow = cfs_time_current();\r\nlist_for_each_entry(route, &ksocknal_data.ksnd_connd_routes,\r\nksnr_connd_list) {\r\nif (!route->ksnr_retry_interval ||\r\ncfs_time_aftereq(now, route->ksnr_timeout))\r\nreturn route;\r\nif (*timeout_p == MAX_SCHEDULE_TIMEOUT ||\r\n(int)*timeout_p > (int)(route->ksnr_timeout - now))\r\n*timeout_p = (int)(route->ksnr_timeout - now);\r\n}\r\nreturn NULL;\r\n}\r\nint\r\nksocknal_connd(void *arg)\r\n{\r\nspinlock_t *connd_lock = &ksocknal_data.ksnd_connd_lock;\r\nstruct ksock_connreq *cr;\r\nwait_queue_entry_t wait;\r\nint nloops = 0;\r\nint cons_retry = 0;\r\ncfs_block_allsigs();\r\ninit_waitqueue_entry(&wait, current);\r\nspin_lock_bh(connd_lock);\r\nLASSERT(ksocknal_data.ksnd_connd_starting > 0);\r\nksocknal_data.ksnd_connd_starting--;\r\nksocknal_data.ksnd_connd_running++;\r\nwhile (!ksocknal_data.ksnd_shuttingdown) {\r\nstruct ksock_route *route = NULL;\r\ntime64_t sec = ktime_get_real_seconds();\r\nlong timeout = MAX_SCHEDULE_TIMEOUT;\r\nint dropped_lock = 0;\r\nif (ksocknal_connd_check_stop(sec, &timeout)) {\r\nwake_up(&ksocknal_data.ksnd_connd_waitq);\r\nbreak;\r\n}\r\nif (ksocknal_connd_check_start(sec, &timeout)) {\r\ndropped_lock = 1;\r\n}\r\nif (!list_empty(&ksocknal_data.ksnd_connd_connreqs)) {\r\ncr = list_entry(ksocknal_data.ksnd_connd_connreqs.next,\r\nstruct ksock_connreq, ksncr_list);\r\nlist_del(&cr->ksncr_list);\r\nspin_unlock_bh(connd_lock);\r\ndropped_lock = 1;\r\nksocknal_create_conn(cr->ksncr_ni, NULL,\r\ncr->ksncr_sock, SOCKLND_CONN_NONE);\r\nlnet_ni_decref(cr->ksncr_ni);\r\nLIBCFS_FREE(cr, sizeof(*cr));\r\nspin_lock_bh(connd_lock);\r\n}\r\nif (ksocknal_data.ksnd_connd_connecting + SOCKNAL_CONND_RESV <\r\nksocknal_data.ksnd_connd_running) {\r\nroute = ksocknal_connd_get_route_locked(&timeout);\r\n}\r\nif (route) {\r\nlist_del(&route->ksnr_connd_list);\r\nksocknal_data.ksnd_connd_connecting++;\r\nspin_unlock_bh(connd_lock);\r\ndropped_lock = 1;\r\nif (ksocknal_connect(route)) {\r\nif (cons_retry++ > SOCKNAL_INSANITY_RECONN) {\r\nCWARN("massive consecutive re-connecting to %pI4h\n",\r\n&route->ksnr_ipaddr);\r\ncons_retry = 0;\r\n}\r\n} else {\r\ncons_retry = 0;\r\n}\r\nksocknal_route_decref(route);\r\nspin_lock_bh(connd_lock);\r\nksocknal_data.ksnd_connd_connecting--;\r\n}\r\nif (dropped_lock) {\r\nif (++nloops < SOCKNAL_RESCHED)\r\ncontinue;\r\nspin_unlock_bh(connd_lock);\r\nnloops = 0;\r\ncond_resched();\r\nspin_lock_bh(connd_lock);\r\ncontinue;\r\n}\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue_exclusive(&ksocknal_data.ksnd_connd_waitq,\r\n&wait);\r\nspin_unlock_bh(connd_lock);\r\nnloops = 0;\r\nschedule_timeout(timeout);\r\nremove_wait_queue(&ksocknal_data.ksnd_connd_waitq, &wait);\r\nspin_lock_bh(connd_lock);\r\n}\r\nksocknal_data.ksnd_connd_running--;\r\nspin_unlock_bh(connd_lock);\r\nksocknal_thread_fini();\r\nreturn 0;\r\n}\r\nstatic struct ksock_conn *\r\nksocknal_find_timed_out_conn(struct ksock_peer *peer)\r\n{\r\nstruct ksock_conn *conn;\r\nstruct list_head *ctmp;\r\nlist_for_each(ctmp, &peer->ksnp_conns) {\r\nint error;\r\nconn = list_entry(ctmp, struct ksock_conn, ksnc_list);\r\nLASSERT(!conn->ksnc_closing);\r\nerror = conn->ksnc_sock->sk->sk_err;\r\nif (error) {\r\nksocknal_conn_addref(conn);\r\nswitch (error) {\r\ncase ECONNRESET:\r\nCNETERR("A connection with %s (%pI4h:%d) was reset; it may have rebooted.\n",\r\nlibcfs_id2str(peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port);\r\nbreak;\r\ncase ETIMEDOUT:\r\nCNETERR("A connection with %s (%pI4h:%d) timed out; the network or node may be down.\n",\r\nlibcfs_id2str(peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port);\r\nbreak;\r\ndefault:\r\nCNETERR("An unexpected network error %d occurred with %s (%pI4h:%d\n",\r\nerror,\r\nlibcfs_id2str(peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port);\r\nbreak;\r\n}\r\nreturn conn;\r\n}\r\nif (conn->ksnc_rx_started &&\r\ncfs_time_aftereq(cfs_time_current(),\r\nconn->ksnc_rx_deadline)) {\r\nksocknal_conn_addref(conn);\r\nCNETERR("Timeout receiving from %s (%pI4h:%d), state %d wanted %d left %d\n",\r\nlibcfs_id2str(peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port,\r\nconn->ksnc_rx_state,\r\nconn->ksnc_rx_nob_wanted,\r\nconn->ksnc_rx_nob_left);\r\nreturn conn;\r\n}\r\nif ((!list_empty(&conn->ksnc_tx_queue) ||\r\nconn->ksnc_sock->sk->sk_wmem_queued) &&\r\ncfs_time_aftereq(cfs_time_current(),\r\nconn->ksnc_tx_deadline)) {\r\nksocknal_conn_addref(conn);\r\nCNETERR("Timeout sending data to %s (%pI4h:%d) the network or that node may be down.\n",\r\nlibcfs_id2str(peer->ksnp_id),\r\n&conn->ksnc_ipaddr,\r\nconn->ksnc_port);\r\nreturn conn;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline void\r\nksocknal_flush_stale_txs(struct ksock_peer *peer)\r\n{\r\nstruct ksock_tx *tx;\r\nstruct ksock_tx *tmp;\r\nLIST_HEAD(stale_txs);\r\nwrite_lock_bh(&ksocknal_data.ksnd_global_lock);\r\nlist_for_each_entry_safe(tx, tmp, &peer->ksnp_tx_queue, tx_list) {\r\nif (!cfs_time_aftereq(cfs_time_current(),\r\ntx->tx_deadline))\r\nbreak;\r\nlist_del(&tx->tx_list);\r\nlist_add_tail(&tx->tx_list, &stale_txs);\r\n}\r\nwrite_unlock_bh(&ksocknal_data.ksnd_global_lock);\r\nksocknal_txlist_done(peer->ksnp_ni, &stale_txs, 1);\r\n}\r\nstatic int\r\nksocknal_send_keepalive_locked(struct ksock_peer *peer)\r\n__must_hold(&ksocknal_data.ksnd_global_lock\r\nstatic void\r\nksocknal_check_peer_timeouts(int idx)\r\n{\r\nstruct list_head *peers = &ksocknal_data.ksnd_peers[idx];\r\nstruct ksock_peer *peer;\r\nstruct ksock_conn *conn;\r\nstruct ksock_tx *tx;\r\nagain:\r\nread_lock(&ksocknal_data.ksnd_global_lock);\r\nlist_for_each_entry(peer, peers, ksnp_list) {\r\nunsigned long deadline = 0;\r\nstruct ksock_tx *tx_stale;\r\nint resid = 0;\r\nint n = 0;\r\nif (ksocknal_send_keepalive_locked(peer)) {\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\ngoto again;\r\n}\r\nconn = ksocknal_find_timed_out_conn(peer);\r\nif (conn) {\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\nksocknal_close_conn_and_siblings(conn, -ETIMEDOUT);\r\nksocknal_conn_decref(conn);\r\ngoto again;\r\n}\r\nif (!list_empty(&peer->ksnp_tx_queue)) {\r\ntx = list_entry(peer->ksnp_tx_queue.next,\r\nstruct ksock_tx, tx_list);\r\nif (cfs_time_aftereq(cfs_time_current(),\r\ntx->tx_deadline)) {\r\nksocknal_peer_addref(peer);\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\nksocknal_flush_stale_txs(peer);\r\nksocknal_peer_decref(peer);\r\ngoto again;\r\n}\r\n}\r\nif (list_empty(&peer->ksnp_zc_req_list))\r\ncontinue;\r\ntx_stale = NULL;\r\nspin_lock(&peer->ksnp_lock);\r\nlist_for_each_entry(tx, &peer->ksnp_zc_req_list, tx_zc_list) {\r\nif (!cfs_time_aftereq(cfs_time_current(),\r\ntx->tx_deadline))\r\nbreak;\r\nif (tx->tx_conn->ksnc_closing)\r\ncontinue;\r\nif (!tx_stale)\r\ntx_stale = tx;\r\nn++;\r\n}\r\nif (!tx_stale) {\r\nspin_unlock(&peer->ksnp_lock);\r\ncontinue;\r\n}\r\ndeadline = tx_stale->tx_deadline;\r\nresid = tx_stale->tx_resid;\r\nconn = tx_stale->tx_conn;\r\nksocknal_conn_addref(conn);\r\nspin_unlock(&peer->ksnp_lock);\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\nCERROR("Total %d stale ZC_REQs for peer %s detected; the oldest(%p) timed out %ld secs ago, resid: %d, wmem: %d\n",\r\nn, libcfs_nid2str(peer->ksnp_id.nid), tx_stale,\r\ncfs_duration_sec(cfs_time_current() - deadline),\r\nresid, conn->ksnc_sock->sk->sk_wmem_queued);\r\nksocknal_close_conn_and_siblings(conn, -ETIMEDOUT);\r\nksocknal_conn_decref(conn);\r\ngoto again;\r\n}\r\nread_unlock(&ksocknal_data.ksnd_global_lock);\r\n}\r\nint\r\nksocknal_reaper(void *arg)\r\n{\r\nwait_queue_entry_t wait;\r\nstruct ksock_conn *conn;\r\nstruct ksock_sched *sched;\r\nstruct list_head enomem_conns;\r\nint nenomem_conns;\r\nlong timeout;\r\nint i;\r\nint peer_index = 0;\r\nunsigned long deadline = cfs_time_current();\r\ncfs_block_allsigs();\r\nINIT_LIST_HEAD(&enomem_conns);\r\ninit_waitqueue_entry(&wait, current);\r\nspin_lock_bh(&ksocknal_data.ksnd_reaper_lock);\r\nwhile (!ksocknal_data.ksnd_shuttingdown) {\r\nif (!list_empty(&ksocknal_data.ksnd_deathrow_conns)) {\r\nconn = list_entry(ksocknal_data.ksnd_deathrow_conns.next,\r\nstruct ksock_conn, ksnc_list);\r\nlist_del(&conn->ksnc_list);\r\nspin_unlock_bh(&ksocknal_data.ksnd_reaper_lock);\r\nksocknal_terminate_conn(conn);\r\nksocknal_conn_decref(conn);\r\nspin_lock_bh(&ksocknal_data.ksnd_reaper_lock);\r\ncontinue;\r\n}\r\nif (!list_empty(&ksocknal_data.ksnd_zombie_conns)) {\r\nconn = list_entry(ksocknal_data.ksnd_zombie_conns.next,\r\nstruct ksock_conn, ksnc_list);\r\nlist_del(&conn->ksnc_list);\r\nspin_unlock_bh(&ksocknal_data.ksnd_reaper_lock);\r\nksocknal_destroy_conn(conn);\r\nspin_lock_bh(&ksocknal_data.ksnd_reaper_lock);\r\ncontinue;\r\n}\r\nif (!list_empty(&ksocknal_data.ksnd_enomem_conns)) {\r\nlist_add(&enomem_conns,\r\n&ksocknal_data.ksnd_enomem_conns);\r\nlist_del_init(&ksocknal_data.ksnd_enomem_conns);\r\n}\r\nspin_unlock_bh(&ksocknal_data.ksnd_reaper_lock);\r\nnenomem_conns = 0;\r\nwhile (!list_empty(&enomem_conns)) {\r\nconn = list_entry(enomem_conns.next, struct ksock_conn,\r\nksnc_tx_list);\r\nlist_del(&conn->ksnc_tx_list);\r\nsched = conn->ksnc_scheduler;\r\nspin_lock_bh(&sched->kss_lock);\r\nLASSERT(conn->ksnc_tx_scheduled);\r\nconn->ksnc_tx_ready = 1;\r\nlist_add_tail(&conn->ksnc_tx_list,\r\n&sched->kss_tx_conns);\r\nwake_up(&sched->kss_waitq);\r\nspin_unlock_bh(&sched->kss_lock);\r\nnenomem_conns++;\r\n}\r\nwhile ((timeout = cfs_time_sub(deadline,\r\ncfs_time_current())) <= 0) {\r\nconst int n = 4;\r\nconst int p = 1;\r\nint chunk = ksocknal_data.ksnd_peer_hash_size;\r\nif (*ksocknal_tunables.ksnd_timeout > n * p)\r\nchunk = (chunk * n * p) /\r\n*ksocknal_tunables.ksnd_timeout;\r\nif (!chunk)\r\nchunk = 1;\r\nfor (i = 0; i < chunk; i++) {\r\nksocknal_check_peer_timeouts(peer_index);\r\npeer_index = (peer_index + 1) %\r\nksocknal_data.ksnd_peer_hash_size;\r\n}\r\ndeadline = cfs_time_add(deadline, cfs_time_seconds(p));\r\n}\r\nif (nenomem_conns) {\r\ntimeout = SOCKNAL_ENOMEM_RETRY;\r\n}\r\nksocknal_data.ksnd_reaper_waketime =\r\ncfs_time_add(cfs_time_current(), timeout);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue(&ksocknal_data.ksnd_reaper_waitq, &wait);\r\nif (!ksocknal_data.ksnd_shuttingdown &&\r\nlist_empty(&ksocknal_data.ksnd_deathrow_conns) &&\r\nlist_empty(&ksocknal_data.ksnd_zombie_conns))\r\nschedule_timeout(timeout);\r\nset_current_state(TASK_RUNNING);\r\nremove_wait_queue(&ksocknal_data.ksnd_reaper_waitq, &wait);\r\nspin_lock_bh(&ksocknal_data.ksnd_reaper_lock);\r\n}\r\nspin_unlock_bh(&ksocknal_data.ksnd_reaper_lock);\r\nksocknal_thread_fini();\r\nreturn 0;\r\n}
