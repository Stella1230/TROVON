int qede_alloc_rx_buffer(struct qede_rx_queue *rxq, bool allow_lazy)\r\n{\r\nstruct sw_rx_data *sw_rx_data;\r\nstruct eth_rx_bd *rx_bd;\r\ndma_addr_t mapping;\r\nstruct page *data;\r\nif (allow_lazy && likely(rxq->filled_buffers > 12)) {\r\nrxq->filled_buffers--;\r\nreturn 0;\r\n}\r\ndata = alloc_pages(GFP_ATOMIC, 0);\r\nif (unlikely(!data))\r\nreturn -ENOMEM;\r\nmapping = dma_map_page(rxq->dev, data, 0,\r\nPAGE_SIZE, rxq->data_direction);\r\nif (unlikely(dma_mapping_error(rxq->dev, mapping))) {\r\n__free_page(data);\r\nreturn -ENOMEM;\r\n}\r\nsw_rx_data = &rxq->sw_rx_ring[rxq->sw_rx_prod & NUM_RX_BDS_MAX];\r\nsw_rx_data->page_offset = 0;\r\nsw_rx_data->data = data;\r\nsw_rx_data->mapping = mapping;\r\nrx_bd = (struct eth_rx_bd *)qed_chain_produce(&rxq->rx_bd_ring);\r\nWARN_ON(!rx_bd);\r\nrx_bd->addr.hi = cpu_to_le32(upper_32_bits(mapping));\r\nrx_bd->addr.lo = cpu_to_le32(lower_32_bits(mapping) +\r\nrxq->rx_headroom);\r\nrxq->sw_rx_prod++;\r\nrxq->filled_buffers++;\r\nreturn 0;\r\n}\r\nint qede_free_tx_pkt(struct qede_dev *edev, struct qede_tx_queue *txq, int *len)\r\n{\r\nu16 idx = txq->sw_tx_cons;\r\nstruct sk_buff *skb = txq->sw_tx_ring.skbs[idx].skb;\r\nstruct eth_tx_1st_bd *first_bd;\r\nstruct eth_tx_bd *tx_data_bd;\r\nint bds_consumed = 0;\r\nint nbds;\r\nbool data_split = txq->sw_tx_ring.skbs[idx].flags & QEDE_TSO_SPLIT_BD;\r\nint i, split_bd_len = 0;\r\nif (unlikely(!skb)) {\r\nDP_ERR(edev,\r\n"skb is null for txq idx=%d txq->sw_tx_cons=%d txq->sw_tx_prod=%d\n",\r\nidx, txq->sw_tx_cons, txq->sw_tx_prod);\r\nreturn -1;\r\n}\r\n*len = skb->len;\r\nfirst_bd = (struct eth_tx_1st_bd *)qed_chain_consume(&txq->tx_pbl);\r\nbds_consumed++;\r\nnbds = first_bd->data.nbds;\r\nif (data_split) {\r\nstruct eth_tx_bd *split = (struct eth_tx_bd *)\r\nqed_chain_consume(&txq->tx_pbl);\r\nsplit_bd_len = BD_UNMAP_LEN(split);\r\nbds_consumed++;\r\n}\r\ndma_unmap_single(&edev->pdev->dev, BD_UNMAP_ADDR(first_bd),\r\nBD_UNMAP_LEN(first_bd) + split_bd_len, DMA_TO_DEVICE);\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++, bds_consumed++) {\r\ntx_data_bd = (struct eth_tx_bd *)\r\nqed_chain_consume(&txq->tx_pbl);\r\ndma_unmap_page(&edev->pdev->dev, BD_UNMAP_ADDR(tx_data_bd),\r\nBD_UNMAP_LEN(tx_data_bd), DMA_TO_DEVICE);\r\n}\r\nwhile (bds_consumed++ < nbds)\r\nqed_chain_consume(&txq->tx_pbl);\r\ndev_kfree_skb_any(skb);\r\ntxq->sw_tx_ring.skbs[idx].skb = NULL;\r\ntxq->sw_tx_ring.skbs[idx].flags = 0;\r\nreturn 0;\r\n}\r\nstatic void qede_free_failed_tx_pkt(struct qede_tx_queue *txq,\r\nstruct eth_tx_1st_bd *first_bd,\r\nint nbd, bool data_split)\r\n{\r\nu16 idx = txq->sw_tx_prod;\r\nstruct sk_buff *skb = txq->sw_tx_ring.skbs[idx].skb;\r\nstruct eth_tx_bd *tx_data_bd;\r\nint i, split_bd_len = 0;\r\nqed_chain_set_prod(&txq->tx_pbl,\r\nle16_to_cpu(txq->tx_db.data.bd_prod), first_bd);\r\nfirst_bd = (struct eth_tx_1st_bd *)qed_chain_produce(&txq->tx_pbl);\r\nif (data_split) {\r\nstruct eth_tx_bd *split = (struct eth_tx_bd *)\r\nqed_chain_produce(&txq->tx_pbl);\r\nsplit_bd_len = BD_UNMAP_LEN(split);\r\nnbd--;\r\n}\r\ndma_unmap_single(txq->dev, BD_UNMAP_ADDR(first_bd),\r\nBD_UNMAP_LEN(first_bd) + split_bd_len, DMA_TO_DEVICE);\r\nfor (i = 0; i < nbd; i++) {\r\ntx_data_bd = (struct eth_tx_bd *)\r\nqed_chain_produce(&txq->tx_pbl);\r\nif (tx_data_bd->nbytes)\r\ndma_unmap_page(txq->dev,\r\nBD_UNMAP_ADDR(tx_data_bd),\r\nBD_UNMAP_LEN(tx_data_bd), DMA_TO_DEVICE);\r\n}\r\nqed_chain_set_prod(&txq->tx_pbl,\r\nle16_to_cpu(txq->tx_db.data.bd_prod), first_bd);\r\ndev_kfree_skb_any(skb);\r\ntxq->sw_tx_ring.skbs[idx].skb = NULL;\r\ntxq->sw_tx_ring.skbs[idx].flags = 0;\r\n}\r\nstatic u32 qede_xmit_type(struct sk_buff *skb, int *ipv6_ext)\r\n{\r\nu32 rc = XMIT_L4_CSUM;\r\n__be16 l3_proto;\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nreturn XMIT_PLAIN;\r\nl3_proto = vlan_get_protocol(skb);\r\nif (l3_proto == htons(ETH_P_IPV6) &&\r\n(ipv6_hdr(skb)->nexthdr == NEXTHDR_IPV6))\r\n*ipv6_ext = 1;\r\nif (skb->encapsulation) {\r\nrc |= XMIT_ENC;\r\nif (skb_is_gso(skb)) {\r\nunsigned short gso_type = skb_shinfo(skb)->gso_type;\r\nif ((gso_type & SKB_GSO_UDP_TUNNEL_CSUM) ||\r\n(gso_type & SKB_GSO_GRE_CSUM))\r\nrc |= XMIT_ENC_GSO_L4_CSUM;\r\nrc |= XMIT_LSO;\r\nreturn rc;\r\n}\r\n}\r\nif (skb_is_gso(skb))\r\nrc |= XMIT_LSO;\r\nreturn rc;\r\n}\r\nstatic void qede_set_params_for_ipv6_ext(struct sk_buff *skb,\r\nstruct eth_tx_2nd_bd *second_bd,\r\nstruct eth_tx_3rd_bd *third_bd)\r\n{\r\nu8 l4_proto;\r\nu16 bd2_bits1 = 0, bd2_bits2 = 0;\r\nbd2_bits1 |= (1 << ETH_TX_DATA_2ND_BD_IPV6_EXT_SHIFT);\r\nbd2_bits2 |= ((((u8 *)skb_transport_header(skb) - skb->data) >> 1) &\r\nETH_TX_DATA_2ND_BD_L4_HDR_START_OFFSET_W_MASK)\r\n<< ETH_TX_DATA_2ND_BD_L4_HDR_START_OFFSET_W_SHIFT;\r\nbd2_bits1 |= (ETH_L4_PSEUDO_CSUM_CORRECT_LENGTH <<\r\nETH_TX_DATA_2ND_BD_L4_PSEUDO_CSUM_MODE_SHIFT);\r\nif (vlan_get_protocol(skb) == htons(ETH_P_IPV6))\r\nl4_proto = ipv6_hdr(skb)->nexthdr;\r\nelse\r\nl4_proto = ip_hdr(skb)->protocol;\r\nif (l4_proto == IPPROTO_UDP)\r\nbd2_bits1 |= 1 << ETH_TX_DATA_2ND_BD_L4_UDP_SHIFT;\r\nif (third_bd)\r\nthird_bd->data.bitfields |=\r\ncpu_to_le16(((tcp_hdrlen(skb) / 4) &\r\nETH_TX_DATA_3RD_BD_TCP_HDR_LEN_DW_MASK) <<\r\nETH_TX_DATA_3RD_BD_TCP_HDR_LEN_DW_SHIFT);\r\nsecond_bd->data.bitfields1 = cpu_to_le16(bd2_bits1);\r\nsecond_bd->data.bitfields2 = cpu_to_le16(bd2_bits2);\r\n}\r\nstatic int map_frag_to_bd(struct qede_tx_queue *txq,\r\nskb_frag_t *frag, struct eth_tx_bd *bd)\r\n{\r\ndma_addr_t mapping;\r\nmapping = skb_frag_dma_map(txq->dev, frag, 0,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(txq->dev, mapping)))\r\nreturn -ENOMEM;\r\nBD_SET_UNMAP_ADDR_LEN(bd, mapping, skb_frag_size(frag));\r\nreturn 0;\r\n}\r\nstatic u16 qede_get_skb_hlen(struct sk_buff *skb, bool is_encap_pkt)\r\n{\r\nif (is_encap_pkt)\r\nreturn (skb_inner_transport_header(skb) +\r\ninner_tcp_hdrlen(skb) - skb->data);\r\nelse\r\nreturn (skb_transport_header(skb) +\r\ntcp_hdrlen(skb) - skb->data);\r\n}\r\nstatic bool qede_pkt_req_lin(struct sk_buff *skb, u8 xmit_type)\r\n{\r\nint allowed_frags = ETH_TX_MAX_BDS_PER_NON_LSO_PACKET - 1;\r\nif (xmit_type & XMIT_LSO) {\r\nint hlen;\r\nhlen = qede_get_skb_hlen(skb, xmit_type & XMIT_ENC);\r\nif (skb_headlen(skb) > hlen)\r\nallowed_frags--;\r\n}\r\nreturn (skb_shinfo(skb)->nr_frags > allowed_frags);\r\n}\r\nstatic inline void qede_update_tx_producer(struct qede_tx_queue *txq)\r\n{\r\nwmb();\r\nbarrier();\r\nwritel(txq->tx_db.raw, txq->doorbell_addr);\r\nmmiowb();\r\n}\r\nstatic int qede_xdp_xmit(struct qede_dev *edev, struct qede_fastpath *fp,\r\nstruct sw_rx_data *metadata, u16 padding, u16 length)\r\n{\r\nstruct qede_tx_queue *txq = fp->xdp_tx;\r\nstruct eth_tx_1st_bd *first_bd;\r\nu16 idx = txq->sw_tx_prod;\r\nu16 val;\r\nif (!qed_chain_get_elem_left(&txq->tx_pbl)) {\r\ntxq->stopped_cnt++;\r\nreturn -ENOMEM;\r\n}\r\nfirst_bd = (struct eth_tx_1st_bd *)qed_chain_produce(&txq->tx_pbl);\r\nmemset(first_bd, 0, sizeof(*first_bd));\r\nfirst_bd->data.bd_flags.bitfields =\r\nBIT(ETH_TX_1ST_BD_FLAGS_START_BD_SHIFT);\r\nval = (length & ETH_TX_DATA_1ST_BD_PKT_LEN_MASK) <<\r\nETH_TX_DATA_1ST_BD_PKT_LEN_SHIFT;\r\nfirst_bd->data.bitfields |= cpu_to_le16(val);\r\nfirst_bd->data.nbds = 1;\r\nBD_SET_UNMAP_ADDR_LEN(first_bd, metadata->mapping + padding, length);\r\ndma_sync_single_for_device(&edev->pdev->dev,\r\nmetadata->mapping + padding,\r\nlength, PCI_DMA_TODEVICE);\r\ntxq->sw_tx_ring.xdp[idx].page = metadata->data;\r\ntxq->sw_tx_ring.xdp[idx].mapping = metadata->mapping;\r\ntxq->sw_tx_prod = (txq->sw_tx_prod + 1) % txq->num_tx_buffers;\r\nfp->xdp_xmit = 1;\r\nreturn 0;\r\n}\r\nint qede_txq_has_work(struct qede_tx_queue *txq)\r\n{\r\nu16 hw_bd_cons;\r\nbarrier();\r\nhw_bd_cons = le16_to_cpu(*txq->hw_cons_ptr);\r\nif (qed_chain_get_cons_idx(&txq->tx_pbl) == hw_bd_cons + 1)\r\nreturn 0;\r\nreturn hw_bd_cons != qed_chain_get_cons_idx(&txq->tx_pbl);\r\n}\r\nstatic void qede_xdp_tx_int(struct qede_dev *edev, struct qede_tx_queue *txq)\r\n{\r\nu16 hw_bd_cons, idx;\r\nhw_bd_cons = le16_to_cpu(*txq->hw_cons_ptr);\r\nbarrier();\r\nwhile (hw_bd_cons != qed_chain_get_cons_idx(&txq->tx_pbl)) {\r\nqed_chain_consume(&txq->tx_pbl);\r\nidx = txq->sw_tx_cons;\r\ndma_unmap_page(&edev->pdev->dev,\r\ntxq->sw_tx_ring.xdp[idx].mapping,\r\nPAGE_SIZE, DMA_BIDIRECTIONAL);\r\n__free_page(txq->sw_tx_ring.xdp[idx].page);\r\ntxq->sw_tx_cons = (txq->sw_tx_cons + 1) % txq->num_tx_buffers;\r\ntxq->xmit_pkts++;\r\n}\r\n}\r\nstatic int qede_tx_int(struct qede_dev *edev, struct qede_tx_queue *txq)\r\n{\r\nstruct netdev_queue *netdev_txq;\r\nu16 hw_bd_cons;\r\nunsigned int pkts_compl = 0, bytes_compl = 0;\r\nint rc;\r\nnetdev_txq = netdev_get_tx_queue(edev->ndev, txq->index);\r\nhw_bd_cons = le16_to_cpu(*txq->hw_cons_ptr);\r\nbarrier();\r\nwhile (hw_bd_cons != qed_chain_get_cons_idx(&txq->tx_pbl)) {\r\nint len = 0;\r\nrc = qede_free_tx_pkt(edev, txq, &len);\r\nif (rc) {\r\nDP_NOTICE(edev, "hw_bd_cons = %d, chain_cons=%d\n",\r\nhw_bd_cons,\r\nqed_chain_get_cons_idx(&txq->tx_pbl));\r\nbreak;\r\n}\r\nbytes_compl += len;\r\npkts_compl++;\r\ntxq->sw_tx_cons = (txq->sw_tx_cons + 1) % txq->num_tx_buffers;\r\ntxq->xmit_pkts++;\r\n}\r\nnetdev_tx_completed_queue(netdev_txq, pkts_compl, bytes_compl);\r\nsmp_mb();\r\nif (unlikely(netif_tx_queue_stopped(netdev_txq))) {\r\n__netif_tx_lock(netdev_txq, smp_processor_id());\r\nif ((netif_tx_queue_stopped(netdev_txq)) &&\r\n(edev->state == QEDE_STATE_OPEN) &&\r\n(qed_chain_get_elem_left(&txq->tx_pbl)\r\n>= (MAX_SKB_FRAGS + 1))) {\r\nnetif_tx_wake_queue(netdev_txq);\r\nDP_VERBOSE(edev, NETIF_MSG_TX_DONE,\r\n"Wake queue was called\n");\r\n}\r\n__netif_tx_unlock(netdev_txq);\r\n}\r\nreturn 0;\r\n}\r\nbool qede_has_rx_work(struct qede_rx_queue *rxq)\r\n{\r\nu16 hw_comp_cons, sw_comp_cons;\r\nbarrier();\r\nhw_comp_cons = le16_to_cpu(*rxq->hw_cons_ptr);\r\nsw_comp_cons = qed_chain_get_cons_idx(&rxq->rx_comp_ring);\r\nreturn hw_comp_cons != sw_comp_cons;\r\n}\r\nstatic inline void qede_rx_bd_ring_consume(struct qede_rx_queue *rxq)\r\n{\r\nqed_chain_consume(&rxq->rx_bd_ring);\r\nrxq->sw_rx_cons++;\r\n}\r\nstatic inline void qede_reuse_page(struct qede_rx_queue *rxq,\r\nstruct sw_rx_data *curr_cons)\r\n{\r\nstruct eth_rx_bd *rx_bd_prod = qed_chain_produce(&rxq->rx_bd_ring);\r\nstruct sw_rx_data *curr_prod;\r\ndma_addr_t new_mapping;\r\ncurr_prod = &rxq->sw_rx_ring[rxq->sw_rx_prod & NUM_RX_BDS_MAX];\r\n*curr_prod = *curr_cons;\r\nnew_mapping = curr_prod->mapping + curr_prod->page_offset;\r\nrx_bd_prod->addr.hi = cpu_to_le32(upper_32_bits(new_mapping));\r\nrx_bd_prod->addr.lo = cpu_to_le32(lower_32_bits(new_mapping) +\r\nrxq->rx_headroom);\r\nrxq->sw_rx_prod++;\r\ncurr_cons->data = NULL;\r\n}\r\nvoid qede_recycle_rx_bd_ring(struct qede_rx_queue *rxq, u8 count)\r\n{\r\nstruct sw_rx_data *curr_cons;\r\nfor (; count > 0; count--) {\r\ncurr_cons = &rxq->sw_rx_ring[rxq->sw_rx_cons & NUM_RX_BDS_MAX];\r\nqede_reuse_page(rxq, curr_cons);\r\nqede_rx_bd_ring_consume(rxq);\r\n}\r\n}\r\nstatic inline int qede_realloc_rx_buffer(struct qede_rx_queue *rxq,\r\nstruct sw_rx_data *curr_cons)\r\n{\r\ncurr_cons->page_offset += rxq->rx_buf_seg_size;\r\nif (curr_cons->page_offset == PAGE_SIZE) {\r\nif (unlikely(qede_alloc_rx_buffer(rxq, true))) {\r\ncurr_cons->page_offset -= rxq->rx_buf_seg_size;\r\nreturn -ENOMEM;\r\n}\r\ndma_unmap_page(rxq->dev, curr_cons->mapping,\r\nPAGE_SIZE, rxq->data_direction);\r\n} else {\r\npage_ref_inc(curr_cons->data);\r\nqede_reuse_page(rxq, curr_cons);\r\n}\r\nreturn 0;\r\n}\r\nvoid qede_update_rx_prod(struct qede_dev *edev, struct qede_rx_queue *rxq)\r\n{\r\nu16 bd_prod = qed_chain_get_prod_idx(&rxq->rx_bd_ring);\r\nu16 cqe_prod = qed_chain_get_prod_idx(&rxq->rx_comp_ring);\r\nstruct eth_rx_prod_data rx_prods = {0};\r\nrx_prods.bd_prod = cpu_to_le16(bd_prod);\r\nrx_prods.cqe_prod = cpu_to_le16(cqe_prod);\r\nwmb();\r\ninternal_ram_wr(rxq->hw_rxq_prod_addr, sizeof(rx_prods),\r\n(u32 *)&rx_prods);\r\nmmiowb();\r\n}\r\nstatic void qede_get_rxhash(struct sk_buff *skb, u8 bitfields, __le32 rss_hash)\r\n{\r\nenum pkt_hash_types hash_type = PKT_HASH_TYPE_NONE;\r\nenum rss_hash_type htype;\r\nu32 hash = 0;\r\nhtype = GET_FIELD(bitfields, ETH_FAST_PATH_RX_REG_CQE_RSS_HASH_TYPE);\r\nif (htype) {\r\nhash_type = ((htype == RSS_HASH_TYPE_IPV4) ||\r\n(htype == RSS_HASH_TYPE_IPV6)) ?\r\nPKT_HASH_TYPE_L3 : PKT_HASH_TYPE_L4;\r\nhash = le32_to_cpu(rss_hash);\r\n}\r\nskb_set_hash(skb, hash, hash_type);\r\n}\r\nstatic void qede_set_skb_csum(struct sk_buff *skb, u8 csum_flag)\r\n{\r\nskb_checksum_none_assert(skb);\r\nif (csum_flag & QEDE_CSUM_UNNECESSARY)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nif (csum_flag & QEDE_TUNN_CSUM_UNNECESSARY) {\r\nskb->csum_level = 1;\r\nskb->encapsulation = 1;\r\n}\r\n}\r\nstatic inline void qede_skb_receive(struct qede_dev *edev,\r\nstruct qede_fastpath *fp,\r\nstruct qede_rx_queue *rxq,\r\nstruct sk_buff *skb, u16 vlan_tag)\r\n{\r\nif (vlan_tag)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag);\r\nnapi_gro_receive(&fp->napi, skb);\r\n}\r\nstatic void qede_set_gro_params(struct qede_dev *edev,\r\nstruct sk_buff *skb,\r\nstruct eth_fast_path_rx_tpa_start_cqe *cqe)\r\n{\r\nu16 parsing_flags = le16_to_cpu(cqe->pars_flags.flags);\r\nif (((parsing_flags >> PARSING_AND_ERR_FLAGS_L3TYPE_SHIFT) &\r\nPARSING_AND_ERR_FLAGS_L3TYPE_MASK) == 2)\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\r\nelse\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\r\nskb_shinfo(skb)->gso_size = __le16_to_cpu(cqe->len_on_first_bd) -\r\ncqe->header_len;\r\n}\r\nstatic int qede_fill_frag_skb(struct qede_dev *edev,\r\nstruct qede_rx_queue *rxq,\r\nu8 tpa_agg_index, u16 len_on_bd)\r\n{\r\nstruct sw_rx_data *current_bd = &rxq->sw_rx_ring[rxq->sw_rx_cons &\r\nNUM_RX_BDS_MAX];\r\nstruct qede_agg_info *tpa_info = &rxq->tpa_info[tpa_agg_index];\r\nstruct sk_buff *skb = tpa_info->skb;\r\nif (unlikely(tpa_info->state != QEDE_AGG_STATE_START))\r\ngoto out;\r\nskb_fill_page_desc(skb, tpa_info->frag_id++,\r\ncurrent_bd->data, current_bd->page_offset,\r\nlen_on_bd);\r\nif (unlikely(qede_realloc_rx_buffer(rxq, current_bd))) {\r\npage_ref_inc(current_bd->data);\r\ngoto out;\r\n}\r\nqed_chain_consume(&rxq->rx_bd_ring);\r\nrxq->sw_rx_cons++;\r\nskb->data_len += len_on_bd;\r\nskb->truesize += rxq->rx_buf_seg_size;\r\nskb->len += len_on_bd;\r\nreturn 0;\r\nout:\r\ntpa_info->state = QEDE_AGG_STATE_ERROR;\r\nqede_recycle_rx_bd_ring(rxq, 1);\r\nreturn -ENOMEM;\r\n}\r\nstatic bool qede_tunn_exist(u16 flag)\r\n{\r\nreturn !!(flag & (PARSING_AND_ERR_FLAGS_TUNNELEXIST_MASK <<\r\nPARSING_AND_ERR_FLAGS_TUNNELEXIST_SHIFT));\r\n}\r\nstatic u8 qede_check_tunn_csum(u16 flag)\r\n{\r\nu16 csum_flag = 0;\r\nu8 tcsum = 0;\r\nif (flag & (PARSING_AND_ERR_FLAGS_TUNNELL4CHKSMWASCALCULATED_MASK <<\r\nPARSING_AND_ERR_FLAGS_TUNNELL4CHKSMWASCALCULATED_SHIFT))\r\ncsum_flag |= PARSING_AND_ERR_FLAGS_TUNNELL4CHKSMERROR_MASK <<\r\nPARSING_AND_ERR_FLAGS_TUNNELL4CHKSMERROR_SHIFT;\r\nif (flag & (PARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_MASK <<\r\nPARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_SHIFT)) {\r\ncsum_flag |= PARSING_AND_ERR_FLAGS_L4CHKSMERROR_MASK <<\r\nPARSING_AND_ERR_FLAGS_L4CHKSMERROR_SHIFT;\r\ntcsum = QEDE_TUNN_CSUM_UNNECESSARY;\r\n}\r\ncsum_flag |= PARSING_AND_ERR_FLAGS_TUNNELIPHDRERROR_MASK <<\r\nPARSING_AND_ERR_FLAGS_TUNNELIPHDRERROR_SHIFT |\r\nPARSING_AND_ERR_FLAGS_IPHDRERROR_MASK <<\r\nPARSING_AND_ERR_FLAGS_IPHDRERROR_SHIFT;\r\nif (csum_flag & flag)\r\nreturn QEDE_CSUM_ERROR;\r\nreturn QEDE_CSUM_UNNECESSARY | tcsum;\r\n}\r\nstatic void qede_tpa_start(struct qede_dev *edev,\r\nstruct qede_rx_queue *rxq,\r\nstruct eth_fast_path_rx_tpa_start_cqe *cqe)\r\n{\r\nstruct qede_agg_info *tpa_info = &rxq->tpa_info[cqe->tpa_agg_index];\r\nstruct eth_rx_bd *rx_bd_cons = qed_chain_consume(&rxq->rx_bd_ring);\r\nstruct eth_rx_bd *rx_bd_prod = qed_chain_produce(&rxq->rx_bd_ring);\r\nstruct sw_rx_data *replace_buf = &tpa_info->buffer;\r\ndma_addr_t mapping = tpa_info->buffer_mapping;\r\nstruct sw_rx_data *sw_rx_data_cons;\r\nstruct sw_rx_data *sw_rx_data_prod;\r\nsw_rx_data_cons = &rxq->sw_rx_ring[rxq->sw_rx_cons & NUM_RX_BDS_MAX];\r\nsw_rx_data_prod = &rxq->sw_rx_ring[rxq->sw_rx_prod & NUM_RX_BDS_MAX];\r\nsw_rx_data_prod->mapping = replace_buf->mapping;\r\nsw_rx_data_prod->data = replace_buf->data;\r\nrx_bd_prod->addr.hi = cpu_to_le32(upper_32_bits(mapping));\r\nrx_bd_prod->addr.lo = cpu_to_le32(lower_32_bits(mapping));\r\nsw_rx_data_prod->page_offset = replace_buf->page_offset;\r\nrxq->sw_rx_prod++;\r\ntpa_info->buffer = *sw_rx_data_cons;\r\nmapping = HILO_U64(le32_to_cpu(rx_bd_cons->addr.hi),\r\nle32_to_cpu(rx_bd_cons->addr.lo));\r\ntpa_info->buffer_mapping = mapping;\r\nrxq->sw_rx_cons++;\r\ntpa_info->skb = netdev_alloc_skb(edev->ndev,\r\nle16_to_cpu(cqe->len_on_first_bd));\r\nif (unlikely(!tpa_info->skb)) {\r\nDP_NOTICE(edev, "Failed to allocate SKB for gro\n");\r\ntpa_info->state = QEDE_AGG_STATE_ERROR;\r\ngoto cons_buf;\r\n}\r\nskb_put(tpa_info->skb, le16_to_cpu(cqe->len_on_first_bd));\r\ntpa_info->frag_id = 0;\r\ntpa_info->state = QEDE_AGG_STATE_START;\r\ntpa_info->start_cqe_placement_offset = cqe->placement_offset;\r\ntpa_info->start_cqe_bd_len = le16_to_cpu(cqe->len_on_first_bd);\r\nif ((le16_to_cpu(cqe->pars_flags.flags) >>\r\nPARSING_AND_ERR_FLAGS_TAG8021QEXIST_SHIFT) &\r\nPARSING_AND_ERR_FLAGS_TAG8021QEXIST_MASK)\r\ntpa_info->vlan_tag = le16_to_cpu(cqe->vlan_tag);\r\nelse\r\ntpa_info->vlan_tag = 0;\r\nqede_get_rxhash(tpa_info->skb, cqe->bitfields, cqe->rss_hash);\r\nqede_set_gro_params(edev, tpa_info->skb, cqe);\r\ncons_buf:\r\nif (likely(cqe->ext_bd_len_list[0]))\r\nqede_fill_frag_skb(edev, rxq, cqe->tpa_agg_index,\r\nle16_to_cpu(cqe->ext_bd_len_list[0]));\r\nif (unlikely(cqe->ext_bd_len_list[1])) {\r\nDP_ERR(edev,\r\n"Unlikely - got a TPA aggregation with more than one ext_bd_len_list entry in the TPA start\n");\r\ntpa_info->state = QEDE_AGG_STATE_ERROR;\r\n}\r\n}\r\nstatic void qede_gro_ip_csum(struct sk_buff *skb)\r\n{\r\nconst struct iphdr *iph = ip_hdr(skb);\r\nstruct tcphdr *th;\r\nskb_set_transport_header(skb, sizeof(struct iphdr));\r\nth = tcp_hdr(skb);\r\nth->check = ~tcp_v4_check(skb->len - skb_transport_offset(skb),\r\niph->saddr, iph->daddr, 0);\r\ntcp_gro_complete(skb);\r\n}\r\nstatic void qede_gro_ipv6_csum(struct sk_buff *skb)\r\n{\r\nstruct ipv6hdr *iph = ipv6_hdr(skb);\r\nstruct tcphdr *th;\r\nskb_set_transport_header(skb, sizeof(struct ipv6hdr));\r\nth = tcp_hdr(skb);\r\nth->check = ~tcp_v6_check(skb->len - skb_transport_offset(skb),\r\n&iph->saddr, &iph->daddr, 0);\r\ntcp_gro_complete(skb);\r\n}\r\nstatic void qede_gro_receive(struct qede_dev *edev,\r\nstruct qede_fastpath *fp,\r\nstruct sk_buff *skb,\r\nu16 vlan_tag)\r\n{\r\nif (unlikely(!skb->data_len)) {\r\nskb_shinfo(skb)->gso_type = 0;\r\nskb_shinfo(skb)->gso_size = 0;\r\ngoto send_skb;\r\n}\r\n#ifdef CONFIG_INET\r\nif (skb_shinfo(skb)->gso_size) {\r\nskb_reset_network_header(skb);\r\nswitch (skb->protocol) {\r\ncase htons(ETH_P_IP):\r\nqede_gro_ip_csum(skb);\r\nbreak;\r\ncase htons(ETH_P_IPV6):\r\nqede_gro_ipv6_csum(skb);\r\nbreak;\r\ndefault:\r\nDP_ERR(edev,\r\n"Error: FW GRO supports only IPv4/IPv6, not 0x%04x\n",\r\nntohs(skb->protocol));\r\n}\r\n}\r\n#endif\r\nsend_skb:\r\nskb_record_rx_queue(skb, fp->rxq->rxq_id);\r\nqede_skb_receive(edev, fp, fp->rxq, skb, vlan_tag);\r\n}\r\nstatic inline void qede_tpa_cont(struct qede_dev *edev,\r\nstruct qede_rx_queue *rxq,\r\nstruct eth_fast_path_rx_tpa_cont_cqe *cqe)\r\n{\r\nint i;\r\nfor (i = 0; cqe->len_list[i]; i++)\r\nqede_fill_frag_skb(edev, rxq, cqe->tpa_agg_index,\r\nle16_to_cpu(cqe->len_list[i]));\r\nif (unlikely(i > 1))\r\nDP_ERR(edev,\r\n"Strange - TPA cont with more than a single len_list entry\n");\r\n}\r\nstatic int qede_tpa_end(struct qede_dev *edev,\r\nstruct qede_fastpath *fp,\r\nstruct eth_fast_path_rx_tpa_end_cqe *cqe)\r\n{\r\nstruct qede_rx_queue *rxq = fp->rxq;\r\nstruct qede_agg_info *tpa_info;\r\nstruct sk_buff *skb;\r\nint i;\r\ntpa_info = &rxq->tpa_info[cqe->tpa_agg_index];\r\nskb = tpa_info->skb;\r\nfor (i = 0; cqe->len_list[i]; i++)\r\nqede_fill_frag_skb(edev, rxq, cqe->tpa_agg_index,\r\nle16_to_cpu(cqe->len_list[i]));\r\nif (unlikely(i > 1))\r\nDP_ERR(edev,\r\n"Strange - TPA emd with more than a single len_list entry\n");\r\nif (unlikely(tpa_info->state != QEDE_AGG_STATE_START))\r\ngoto err;\r\nif (unlikely(cqe->num_of_bds != tpa_info->frag_id + 1))\r\nDP_ERR(edev,\r\n"Strange - TPA had %02x BDs, but SKB has only %d frags\n",\r\ncqe->num_of_bds, tpa_info->frag_id);\r\nif (unlikely(skb->len != le16_to_cpu(cqe->total_packet_len)))\r\nDP_ERR(edev,\r\n"Strange - total packet len [cqe] is %4x but SKB has len %04x\n",\r\nle16_to_cpu(cqe->total_packet_len), skb->len);\r\nmemcpy(skb->data,\r\npage_address(tpa_info->buffer.data) +\r\ntpa_info->start_cqe_placement_offset +\r\ntpa_info->buffer.page_offset, tpa_info->start_cqe_bd_len);\r\nskb->protocol = eth_type_trans(skb, edev->ndev);\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nNAPI_GRO_CB(skb)->count = le16_to_cpu(cqe->num_of_coalesced_segs);\r\nqede_gro_receive(edev, fp, skb, tpa_info->vlan_tag);\r\ntpa_info->state = QEDE_AGG_STATE_NONE;\r\nreturn 1;\r\nerr:\r\ntpa_info->state = QEDE_AGG_STATE_NONE;\r\ndev_kfree_skb_any(tpa_info->skb);\r\ntpa_info->skb = NULL;\r\nreturn 0;\r\n}\r\nstatic u8 qede_check_notunn_csum(u16 flag)\r\n{\r\nu16 csum_flag = 0;\r\nu8 csum = 0;\r\nif (flag & (PARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_MASK <<\r\nPARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_SHIFT)) {\r\ncsum_flag |= PARSING_AND_ERR_FLAGS_L4CHKSMERROR_MASK <<\r\nPARSING_AND_ERR_FLAGS_L4CHKSMERROR_SHIFT;\r\ncsum = QEDE_CSUM_UNNECESSARY;\r\n}\r\ncsum_flag |= PARSING_AND_ERR_FLAGS_IPHDRERROR_MASK <<\r\nPARSING_AND_ERR_FLAGS_IPHDRERROR_SHIFT;\r\nif (csum_flag & flag)\r\nreturn QEDE_CSUM_ERROR;\r\nreturn csum;\r\n}\r\nstatic u8 qede_check_csum(u16 flag)\r\n{\r\nif (!qede_tunn_exist(flag))\r\nreturn qede_check_notunn_csum(flag);\r\nelse\r\nreturn qede_check_tunn_csum(flag);\r\n}\r\nstatic bool qede_pkt_is_ip_fragmented(struct eth_fast_path_rx_reg_cqe *cqe,\r\nu16 flag)\r\n{\r\nu8 tun_pars_flg = cqe->tunnel_pars_flags.flags;\r\nif ((tun_pars_flg & (ETH_TUNNEL_PARSING_FLAGS_IPV4_FRAGMENT_MASK <<\r\nETH_TUNNEL_PARSING_FLAGS_IPV4_FRAGMENT_SHIFT)) ||\r\n(flag & (PARSING_AND_ERR_FLAGS_IPV4FRAG_MASK <<\r\nPARSING_AND_ERR_FLAGS_IPV4FRAG_SHIFT)))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool qede_rx_xdp(struct qede_dev *edev,\r\nstruct qede_fastpath *fp,\r\nstruct qede_rx_queue *rxq,\r\nstruct bpf_prog *prog,\r\nstruct sw_rx_data *bd,\r\nstruct eth_fast_path_rx_reg_cqe *cqe,\r\nu16 *data_offset, u16 *len)\r\n{\r\nstruct xdp_buff xdp;\r\nenum xdp_action act;\r\nxdp.data_hard_start = page_address(bd->data);\r\nxdp.data = xdp.data_hard_start + *data_offset;\r\nxdp.data_end = xdp.data + *len;\r\nrcu_read_lock();\r\nact = bpf_prog_run_xdp(prog, &xdp);\r\nrcu_read_unlock();\r\n*data_offset = xdp.data - xdp.data_hard_start;\r\n*len = xdp.data_end - xdp.data;\r\nif (act == XDP_PASS)\r\nreturn true;\r\nrxq->xdp_no_pass++;\r\nswitch (act) {\r\ncase XDP_TX:\r\nif (qede_alloc_rx_buffer(rxq, true)) {\r\nqede_recycle_rx_bd_ring(rxq, 1);\r\ntrace_xdp_exception(edev->ndev, prog, act);\r\nreturn false;\r\n}\r\nif (qede_xdp_xmit(edev, fp, bd, *data_offset, *len)) {\r\ndma_unmap_page(rxq->dev, bd->mapping,\r\nPAGE_SIZE, DMA_BIDIRECTIONAL);\r\n__free_page(bd->data);\r\ntrace_xdp_exception(edev->ndev, prog, act);\r\n}\r\nqede_rx_bd_ring_consume(rxq);\r\nreturn false;\r\ndefault:\r\nbpf_warn_invalid_xdp_action(act);\r\ncase XDP_ABORTED:\r\ntrace_xdp_exception(edev->ndev, prog, act);\r\ncase XDP_DROP:\r\nqede_recycle_rx_bd_ring(rxq, cqe->bd_num);\r\n}\r\nreturn false;\r\n}\r\nstatic struct sk_buff *qede_rx_allocate_skb(struct qede_dev *edev,\r\nstruct qede_rx_queue *rxq,\r\nstruct sw_rx_data *bd, u16 len,\r\nu16 pad)\r\n{\r\nunsigned int offset = bd->page_offset + pad;\r\nstruct skb_frag_struct *frag;\r\nstruct page *page = bd->data;\r\nunsigned int pull_len;\r\nstruct sk_buff *skb;\r\nunsigned char *va;\r\nskb = netdev_alloc_skb(edev->ndev, QEDE_RX_HDR_SIZE);\r\nif (unlikely(!skb))\r\nreturn NULL;\r\nif (len + pad <= edev->rx_copybreak) {\r\nskb_put_data(skb, page_address(page) + offset, len);\r\nqede_reuse_page(rxq, bd);\r\ngoto out;\r\n}\r\nfrag = &skb_shinfo(skb)->frags[0];\r\nskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\r\npage, offset, len, rxq->rx_buf_seg_size);\r\nva = skb_frag_address(frag);\r\npull_len = eth_get_headlen(va, QEDE_RX_HDR_SIZE);\r\nmemcpy(skb->data, va, ALIGN(pull_len, sizeof(long)));\r\nskb_frag_size_sub(frag, pull_len);\r\nfrag->page_offset += pull_len;\r\nskb->data_len -= pull_len;\r\nskb->tail += pull_len;\r\nif (unlikely(qede_realloc_rx_buffer(rxq, bd))) {\r\npage_ref_inc(page);\r\ndev_kfree_skb_any(skb);\r\nreturn NULL;\r\n}\r\nout:\r\nqede_rx_bd_ring_consume(rxq);\r\nreturn skb;\r\n}\r\nstatic int qede_rx_build_jumbo(struct qede_dev *edev,\r\nstruct qede_rx_queue *rxq,\r\nstruct sk_buff *skb,\r\nstruct eth_fast_path_rx_reg_cqe *cqe,\r\nu16 first_bd_len)\r\n{\r\nu16 pkt_len = le16_to_cpu(cqe->pkt_len);\r\nstruct sw_rx_data *bd;\r\nu16 bd_cons_idx;\r\nu8 num_frags;\r\npkt_len -= first_bd_len;\r\nfor (num_frags = cqe->bd_num - 1; num_frags > 0; num_frags--) {\r\nu16 cur_size = pkt_len > rxq->rx_buf_size ? rxq->rx_buf_size :\r\npkt_len;\r\nif (unlikely(!cur_size)) {\r\nDP_ERR(edev,\r\n"Still got %d BDs for mapping jumbo, but length became 0\n",\r\nnum_frags);\r\ngoto out;\r\n}\r\nif (unlikely(qede_alloc_rx_buffer(rxq, true)))\r\ngoto out;\r\nbd_cons_idx = rxq->sw_rx_cons & NUM_RX_BDS_MAX;\r\nbd = &rxq->sw_rx_ring[bd_cons_idx];\r\nqede_rx_bd_ring_consume(rxq);\r\ndma_unmap_page(rxq->dev, bd->mapping,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags++,\r\nbd->data, 0, cur_size);\r\nskb->truesize += PAGE_SIZE;\r\nskb->data_len += cur_size;\r\nskb->len += cur_size;\r\npkt_len -= cur_size;\r\n}\r\nif (unlikely(pkt_len))\r\nDP_ERR(edev,\r\n"Mapped all BDs of jumbo, but still have %d bytes\n",\r\npkt_len);\r\nout:\r\nreturn num_frags;\r\n}\r\nstatic int qede_rx_process_tpa_cqe(struct qede_dev *edev,\r\nstruct qede_fastpath *fp,\r\nstruct qede_rx_queue *rxq,\r\nunion eth_rx_cqe *cqe,\r\nenum eth_rx_cqe_type type)\r\n{\r\nswitch (type) {\r\ncase ETH_RX_CQE_TYPE_TPA_START:\r\nqede_tpa_start(edev, rxq, &cqe->fast_path_tpa_start);\r\nreturn 0;\r\ncase ETH_RX_CQE_TYPE_TPA_CONT:\r\nqede_tpa_cont(edev, rxq, &cqe->fast_path_tpa_cont);\r\nreturn 0;\r\ncase ETH_RX_CQE_TYPE_TPA_END:\r\nreturn qede_tpa_end(edev, fp, &cqe->fast_path_tpa_end);\r\ndefault:\r\nreturn 0;\r\n}\r\n}\r\nstatic int qede_rx_process_cqe(struct qede_dev *edev,\r\nstruct qede_fastpath *fp,\r\nstruct qede_rx_queue *rxq)\r\n{\r\nstruct bpf_prog *xdp_prog = READ_ONCE(rxq->xdp_prog);\r\nstruct eth_fast_path_rx_reg_cqe *fp_cqe;\r\nu16 len, pad, bd_cons_idx, parse_flag;\r\nenum eth_rx_cqe_type cqe_type;\r\nunion eth_rx_cqe *cqe;\r\nstruct sw_rx_data *bd;\r\nstruct sk_buff *skb;\r\n__le16 flags;\r\nu8 csum_flag;\r\ncqe = (union eth_rx_cqe *)qed_chain_consume(&rxq->rx_comp_ring);\r\ncqe_type = cqe->fast_path_regular.type;\r\nif (unlikely(cqe_type == ETH_RX_CQE_TYPE_SLOW_PATH)) {\r\nstruct eth_slow_path_rx_cqe *sp_cqe;\r\nsp_cqe = (struct eth_slow_path_rx_cqe *)cqe;\r\nedev->ops->eth_cqe_completion(edev->cdev, fp->id, sp_cqe);\r\nreturn 0;\r\n}\r\nif (cqe_type != ETH_RX_CQE_TYPE_REGULAR)\r\nreturn qede_rx_process_tpa_cqe(edev, fp, rxq, cqe, cqe_type);\r\nbd_cons_idx = rxq->sw_rx_cons & NUM_RX_BDS_MAX;\r\nbd = &rxq->sw_rx_ring[bd_cons_idx];\r\nfp_cqe = &cqe->fast_path_regular;\r\nlen = le16_to_cpu(fp_cqe->len_on_first_bd);\r\npad = fp_cqe->placement_offset + rxq->rx_headroom;\r\nif (xdp_prog)\r\nif (!qede_rx_xdp(edev, fp, rxq, xdp_prog, bd, fp_cqe,\r\n&pad, &len))\r\nreturn 0;\r\nflags = cqe->fast_path_regular.pars_flags.flags;\r\nparse_flag = le16_to_cpu(flags);\r\ncsum_flag = qede_check_csum(parse_flag);\r\nif (unlikely(csum_flag == QEDE_CSUM_ERROR)) {\r\nif (qede_pkt_is_ip_fragmented(fp_cqe, parse_flag)) {\r\nrxq->rx_ip_frags++;\r\n} else {\r\nDP_NOTICE(edev,\r\n"CQE has error, flags = %x, dropping incoming packet\n",\r\nparse_flag);\r\nrxq->rx_hw_errors++;\r\nqede_recycle_rx_bd_ring(rxq, fp_cqe->bd_num);\r\nreturn 0;\r\n}\r\n}\r\nskb = qede_rx_allocate_skb(edev, rxq, bd, len, pad);\r\nif (!skb) {\r\nrxq->rx_alloc_errors++;\r\nqede_recycle_rx_bd_ring(rxq, fp_cqe->bd_num);\r\nreturn 0;\r\n}\r\nif (fp_cqe->bd_num > 1) {\r\nu16 unmapped_frags = qede_rx_build_jumbo(edev, rxq, skb,\r\nfp_cqe, len);\r\nif (unlikely(unmapped_frags > 0)) {\r\nqede_recycle_rx_bd_ring(rxq, unmapped_frags);\r\ndev_kfree_skb_any(skb);\r\nreturn 0;\r\n}\r\n}\r\nskb->protocol = eth_type_trans(skb, edev->ndev);\r\nqede_get_rxhash(skb, fp_cqe->bitfields, fp_cqe->rss_hash);\r\nqede_set_skb_csum(skb, csum_flag);\r\nskb_record_rx_queue(skb, rxq->rxq_id);\r\nqede_ptp_record_rx_ts(edev, cqe, skb);\r\nqede_skb_receive(edev, fp, rxq, skb, le16_to_cpu(fp_cqe->vlan_tag));\r\nreturn 1;\r\n}\r\nstatic int qede_rx_int(struct qede_fastpath *fp, int budget)\r\n{\r\nstruct qede_rx_queue *rxq = fp->rxq;\r\nstruct qede_dev *edev = fp->edev;\r\nint work_done = 0, rcv_pkts = 0;\r\nu16 hw_comp_cons, sw_comp_cons;\r\nhw_comp_cons = le16_to_cpu(*rxq->hw_cons_ptr);\r\nsw_comp_cons = qed_chain_get_cons_idx(&rxq->rx_comp_ring);\r\nrmb();\r\nwhile ((sw_comp_cons != hw_comp_cons) && (work_done < budget)) {\r\nrcv_pkts += qede_rx_process_cqe(edev, fp, rxq);\r\nqed_chain_recycle_consumed(&rxq->rx_comp_ring);\r\nsw_comp_cons = qed_chain_get_cons_idx(&rxq->rx_comp_ring);\r\nwork_done++;\r\n}\r\nrxq->rcv_pkts += rcv_pkts;\r\nwhile (rxq->num_rx_buffers - rxq->filled_buffers)\r\nif (qede_alloc_rx_buffer(rxq, false))\r\nbreak;\r\nqede_update_rx_prod(edev, rxq);\r\nreturn work_done;\r\n}\r\nstatic bool qede_poll_is_more_work(struct qede_fastpath *fp)\r\n{\r\nqed_sb_update_sb_idx(fp->sb_info);\r\nrmb();\r\nif (likely(fp->type & QEDE_FASTPATH_RX))\r\nif (qede_has_rx_work(fp->rxq))\r\nreturn true;\r\nif (fp->type & QEDE_FASTPATH_XDP)\r\nif (qede_txq_has_work(fp->xdp_tx))\r\nreturn true;\r\nif (likely(fp->type & QEDE_FASTPATH_TX))\r\nif (qede_txq_has_work(fp->txq))\r\nreturn true;\r\nreturn false;\r\n}\r\nint qede_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct qede_fastpath *fp = container_of(napi, struct qede_fastpath,\r\nnapi);\r\nstruct qede_dev *edev = fp->edev;\r\nint rx_work_done = 0;\r\nif (likely(fp->type & QEDE_FASTPATH_TX) && qede_txq_has_work(fp->txq))\r\nqede_tx_int(edev, fp->txq);\r\nif ((fp->type & QEDE_FASTPATH_XDP) && qede_txq_has_work(fp->xdp_tx))\r\nqede_xdp_tx_int(edev, fp->xdp_tx);\r\nrx_work_done = (likely(fp->type & QEDE_FASTPATH_RX) &&\r\nqede_has_rx_work(fp->rxq)) ?\r\nqede_rx_int(fp, budget) : 0;\r\nif (rx_work_done < budget) {\r\nif (!qede_poll_is_more_work(fp)) {\r\nnapi_complete_done(napi, rx_work_done);\r\nqed_sb_ack(fp->sb_info, IGU_INT_ENABLE, 1);\r\n} else {\r\nrx_work_done = budget;\r\n}\r\n}\r\nif (fp->xdp_xmit) {\r\nu16 xdp_prod = qed_chain_get_prod_idx(&fp->xdp_tx->tx_pbl);\r\nfp->xdp_xmit = 0;\r\nfp->xdp_tx->tx_db.data.bd_prod = cpu_to_le16(xdp_prod);\r\nqede_update_tx_producer(fp->xdp_tx);\r\n}\r\nreturn rx_work_done;\r\n}\r\nirqreturn_t qede_msix_fp_int(int irq, void *fp_cookie)\r\n{\r\nstruct qede_fastpath *fp = fp_cookie;\r\nqed_sb_ack(fp->sb_info, IGU_INT_DISABLE, 0 );\r\nnapi_schedule_irqoff(&fp->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nnetdev_tx_t qede_start_xmit(struct sk_buff *skb, struct net_device *ndev)\r\n{\r\nstruct qede_dev *edev = netdev_priv(ndev);\r\nstruct netdev_queue *netdev_txq;\r\nstruct qede_tx_queue *txq;\r\nstruct eth_tx_1st_bd *first_bd;\r\nstruct eth_tx_2nd_bd *second_bd = NULL;\r\nstruct eth_tx_3rd_bd *third_bd = NULL;\r\nstruct eth_tx_bd *tx_data_bd = NULL;\r\nu16 txq_index, val = 0;\r\nu8 nbd = 0;\r\ndma_addr_t mapping;\r\nint rc, frag_idx = 0, ipv6_ext = 0;\r\nu8 xmit_type;\r\nu16 idx;\r\nu16 hlen;\r\nbool data_split = false;\r\ntxq_index = skb_get_queue_mapping(skb);\r\nWARN_ON(txq_index >= QEDE_TSS_COUNT(edev));\r\ntxq = edev->fp_array[edev->fp_num_rx + txq_index].txq;\r\nnetdev_txq = netdev_get_tx_queue(ndev, txq_index);\r\nWARN_ON(qed_chain_get_elem_left(&txq->tx_pbl) < (MAX_SKB_FRAGS + 1));\r\nxmit_type = qede_xmit_type(skb, &ipv6_ext);\r\n#if ((MAX_SKB_FRAGS + 2) > ETH_TX_MAX_BDS_PER_NON_LSO_PACKET)\r\nif (qede_pkt_req_lin(skb, xmit_type)) {\r\nif (skb_linearize(skb)) {\r\nDP_NOTICE(edev,\r\n"SKB linearization failed - silently dropping this SKB\n");\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\n}\r\n#endif\r\nidx = txq->sw_tx_prod;\r\ntxq->sw_tx_ring.skbs[idx].skb = skb;\r\nfirst_bd = (struct eth_tx_1st_bd *)\r\nqed_chain_produce(&txq->tx_pbl);\r\nmemset(first_bd, 0, sizeof(*first_bd));\r\nfirst_bd->data.bd_flags.bitfields =\r\n1 << ETH_TX_1ST_BD_FLAGS_START_BD_SHIFT;\r\nif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))\r\nqede_ptp_tx_ts(edev, skb);\r\nmapping = dma_map_single(txq->dev, skb->data,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(txq->dev, mapping))) {\r\nDP_NOTICE(edev, "SKB mapping failed\n");\r\nqede_free_failed_tx_pkt(txq, first_bd, 0, false);\r\nqede_update_tx_producer(txq);\r\nreturn NETDEV_TX_OK;\r\n}\r\nnbd++;\r\nBD_SET_UNMAP_ADDR_LEN(first_bd, mapping, skb_headlen(skb));\r\nif (unlikely((xmit_type & XMIT_LSO) | ipv6_ext)) {\r\nsecond_bd = (struct eth_tx_2nd_bd *)\r\nqed_chain_produce(&txq->tx_pbl);\r\nmemset(second_bd, 0, sizeof(*second_bd));\r\nnbd++;\r\nthird_bd = (struct eth_tx_3rd_bd *)\r\nqed_chain_produce(&txq->tx_pbl);\r\nmemset(third_bd, 0, sizeof(*third_bd));\r\nnbd++;\r\ntx_data_bd = (struct eth_tx_bd *)second_bd;\r\n}\r\nif (skb_vlan_tag_present(skb)) {\r\nfirst_bd->data.vlan = cpu_to_le16(skb_vlan_tag_get(skb));\r\nfirst_bd->data.bd_flags.bitfields |=\r\n1 << ETH_TX_1ST_BD_FLAGS_VLAN_INSERTION_SHIFT;\r\n}\r\nif (xmit_type & XMIT_L4_CSUM) {\r\nfirst_bd->data.bd_flags.bitfields |=\r\n1 << ETH_TX_1ST_BD_FLAGS_L4_CSUM_SHIFT;\r\nif (xmit_type & XMIT_ENC) {\r\nfirst_bd->data.bd_flags.bitfields |=\r\n1 << ETH_TX_1ST_BD_FLAGS_IP_CSUM_SHIFT;\r\nval |= (1 << ETH_TX_DATA_1ST_BD_TUNN_FLAG_SHIFT);\r\n}\r\nif (unlikely(txq->is_legacy))\r\nval ^= (1 << ETH_TX_DATA_1ST_BD_TUNN_FLAG_SHIFT);\r\nif (unlikely(ipv6_ext))\r\nqede_set_params_for_ipv6_ext(skb, second_bd, third_bd);\r\n}\r\nif (xmit_type & XMIT_LSO) {\r\nfirst_bd->data.bd_flags.bitfields |=\r\n(1 << ETH_TX_1ST_BD_FLAGS_LSO_SHIFT);\r\nthird_bd->data.lso_mss =\r\ncpu_to_le16(skb_shinfo(skb)->gso_size);\r\nif (unlikely(xmit_type & XMIT_ENC)) {\r\nfirst_bd->data.bd_flags.bitfields |=\r\n1 << ETH_TX_1ST_BD_FLAGS_TUNN_IP_CSUM_SHIFT;\r\nif (xmit_type & XMIT_ENC_GSO_L4_CSUM) {\r\nu8 tmp = ETH_TX_1ST_BD_FLAGS_TUNN_L4_CSUM_SHIFT;\r\nfirst_bd->data.bd_flags.bitfields |= 1 << tmp;\r\n}\r\nhlen = qede_get_skb_hlen(skb, true);\r\n} else {\r\nfirst_bd->data.bd_flags.bitfields |=\r\n1 << ETH_TX_1ST_BD_FLAGS_IP_CSUM_SHIFT;\r\nhlen = qede_get_skb_hlen(skb, false);\r\n}\r\nthird_bd->data.bitfields |=\r\ncpu_to_le16(1 << ETH_TX_DATA_3RD_BD_HDR_NBD_SHIFT);\r\nif (unlikely(skb_headlen(skb) > hlen)) {\r\nDP_VERBOSE(edev, NETIF_MSG_TX_QUEUED,\r\n"TSO split header size is %d (%x:%x)\n",\r\nfirst_bd->nbytes, first_bd->addr.hi,\r\nfirst_bd->addr.lo);\r\nmapping = HILO_U64(le32_to_cpu(first_bd->addr.hi),\r\nle32_to_cpu(first_bd->addr.lo)) +\r\nhlen;\r\nBD_SET_UNMAP_ADDR_LEN(tx_data_bd, mapping,\r\nle16_to_cpu(first_bd->nbytes) -\r\nhlen);\r\ntxq->sw_tx_ring.skbs[idx].flags |= QEDE_TSO_SPLIT_BD;\r\nfirst_bd->nbytes = cpu_to_le16(hlen);\r\ntx_data_bd = (struct eth_tx_bd *)third_bd;\r\ndata_split = true;\r\n}\r\n} else {\r\nval |= ((skb->len & ETH_TX_DATA_1ST_BD_PKT_LEN_MASK) <<\r\nETH_TX_DATA_1ST_BD_PKT_LEN_SHIFT);\r\n}\r\nfirst_bd->data.bitfields = cpu_to_le16(val);\r\nwhile (tx_data_bd && frag_idx < skb_shinfo(skb)->nr_frags) {\r\nrc = map_frag_to_bd(txq,\r\n&skb_shinfo(skb)->frags[frag_idx],\r\ntx_data_bd);\r\nif (rc) {\r\nqede_free_failed_tx_pkt(txq, first_bd, nbd, data_split);\r\nqede_update_tx_producer(txq);\r\nreturn NETDEV_TX_OK;\r\n}\r\nif (tx_data_bd == (struct eth_tx_bd *)second_bd)\r\ntx_data_bd = (struct eth_tx_bd *)third_bd;\r\nelse\r\ntx_data_bd = NULL;\r\nfrag_idx++;\r\n}\r\nfor (; frag_idx < skb_shinfo(skb)->nr_frags; frag_idx++, nbd++) {\r\ntx_data_bd = (struct eth_tx_bd *)\r\nqed_chain_produce(&txq->tx_pbl);\r\nmemset(tx_data_bd, 0, sizeof(*tx_data_bd));\r\nrc = map_frag_to_bd(txq,\r\n&skb_shinfo(skb)->frags[frag_idx],\r\ntx_data_bd);\r\nif (rc) {\r\nqede_free_failed_tx_pkt(txq, first_bd, nbd, data_split);\r\nqede_update_tx_producer(txq);\r\nreturn NETDEV_TX_OK;\r\n}\r\n}\r\nfirst_bd->data.nbds = nbd;\r\nnetdev_tx_sent_queue(netdev_txq, skb->len);\r\nskb_tx_timestamp(skb);\r\ntxq->sw_tx_prod = (txq->sw_tx_prod + 1) % txq->num_tx_buffers;\r\ntxq->tx_db.data.bd_prod =\r\ncpu_to_le16(qed_chain_get_prod_idx(&txq->tx_pbl));\r\nif (!skb->xmit_more || netif_xmit_stopped(netdev_txq))\r\nqede_update_tx_producer(txq);\r\nif (unlikely(qed_chain_get_elem_left(&txq->tx_pbl)\r\n< (MAX_SKB_FRAGS + 1))) {\r\nif (skb->xmit_more)\r\nqede_update_tx_producer(txq);\r\nnetif_tx_stop_queue(netdev_txq);\r\ntxq->stopped_cnt++;\r\nDP_VERBOSE(edev, NETIF_MSG_TX_QUEUED,\r\n"Stop queue was called\n");\r\nsmp_mb();\r\nif ((qed_chain_get_elem_left(&txq->tx_pbl) >=\r\n(MAX_SKB_FRAGS + 1)) &&\r\n(edev->state == QEDE_STATE_OPEN)) {\r\nnetif_tx_wake_queue(netdev_txq);\r\nDP_VERBOSE(edev, NETIF_MSG_TX_QUEUED,\r\n"Wake queue was called\n");\r\n}\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nnetdev_features_t qede_features_check(struct sk_buff *skb,\r\nstruct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nif (skb->encapsulation) {\r\nu8 l4_proto = 0;\r\nswitch (vlan_get_protocol(skb)) {\r\ncase htons(ETH_P_IP):\r\nl4_proto = ip_hdr(skb)->protocol;\r\nbreak;\r\ncase htons(ETH_P_IPV6):\r\nl4_proto = ipv6_hdr(skb)->nexthdr;\r\nbreak;\r\ndefault:\r\nreturn features;\r\n}\r\nif (l4_proto == IPPROTO_UDP) {\r\nstruct qede_dev *edev = netdev_priv(dev);\r\nu16 hdrlen, vxln_port, gnv_port;\r\nhdrlen = QEDE_MAX_TUN_HDR_LEN;\r\nvxln_port = edev->vxlan_dst_port;\r\ngnv_port = edev->geneve_dst_port;\r\nif ((skb_inner_mac_header(skb) -\r\nskb_transport_header(skb)) > hdrlen ||\r\n(ntohs(udp_hdr(skb)->dest) != vxln_port &&\r\nntohs(udp_hdr(skb)->dest) != gnv_port))\r\nreturn features & ~(NETIF_F_CSUM_MASK |\r\nNETIF_F_GSO_MASK);\r\n}\r\n}\r\nreturn features;\r\n}
