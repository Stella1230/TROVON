static void osc_page_transfer_get(struct osc_page *opg, const char *label)\r\n{\r\nstruct cl_page *page = opg->ops_cl.cpl_page;\r\nLASSERT(!opg->ops_transfer_pinned);\r\ncl_page_get(page);\r\nlu_ref_add_atomic(&page->cp_reference, label, page);\r\nopg->ops_transfer_pinned = 1;\r\n}\r\nstatic void osc_page_transfer_put(const struct lu_env *env,\r\nstruct osc_page *opg)\r\n{\r\nstruct cl_page *page = opg->ops_cl.cpl_page;\r\nif (opg->ops_transfer_pinned) {\r\nopg->ops_transfer_pinned = 0;\r\nlu_ref_del(&page->cp_reference, "transfer", page);\r\ncl_page_put(env, page);\r\n}\r\n}\r\nstatic void osc_page_transfer_add(const struct lu_env *env,\r\nstruct osc_page *opg, enum cl_req_type crt)\r\n{\r\nstruct osc_object *obj = cl2osc(opg->ops_cl.cpl_obj);\r\nosc_lru_use(osc_cli(obj), opg);\r\n}\r\nint osc_page_cache_add(const struct lu_env *env,\r\nconst struct cl_page_slice *slice, struct cl_io *io)\r\n{\r\nstruct osc_page *opg = cl2osc_page(slice);\r\nint result;\r\nosc_page_transfer_get(opg, "transfer\0cache");\r\nresult = osc_queue_async_io(env, io, opg);\r\nif (result != 0)\r\nosc_page_transfer_put(env, opg);\r\nelse\r\nosc_page_transfer_add(env, opg, CRT_WRITE);\r\nreturn result;\r\n}\r\nvoid osc_index2policy(union ldlm_policy_data *policy,\r\nconst struct cl_object *obj,\r\npgoff_t start, pgoff_t end)\r\n{\r\nmemset(policy, 0, sizeof(*policy));\r\npolicy->l_extent.start = cl_offset(obj, start);\r\npolicy->l_extent.end = cl_offset(obj, end + 1) - 1;\r\n}\r\nstatic const char *osc_list(struct list_head *head)\r\n{\r\nreturn list_empty(head) ? "-" : "+";\r\n}\r\nstatic inline unsigned long osc_submit_duration(struct osc_page *opg)\r\n{\r\nif (opg->ops_submit_time == 0)\r\nreturn 0;\r\nreturn (cfs_time_current() - opg->ops_submit_time);\r\n}\r\nstatic int osc_page_print(const struct lu_env *env,\r\nconst struct cl_page_slice *slice,\r\nvoid *cookie, lu_printer_t printer)\r\n{\r\nstruct osc_page *opg = cl2osc_page(slice);\r\nstruct osc_async_page *oap = &opg->ops_oap;\r\nstruct osc_object *obj = cl2osc(slice->cpl_obj);\r\nstruct client_obd *cli = &osc_export(obj)->exp_obd->u.cli;\r\nreturn (*printer)(env, cookie, LUSTRE_OSC_NAME "-page@%p %lu: 1< %#x %d %u %s %s > 2< %llu %u %u %#x %#x | %p %p %p > 3< %d %lu %d > 4< %d %d %d %lu %s | %s %s %s %s > 5< %s %s %s %s | %d %s | %d %s %s>\n",\r\nopg, osc_index(opg),\r\noap->oap_magic, oap->oap_cmd,\r\noap->oap_interrupted,\r\nosc_list(&oap->oap_pending_item),\r\nosc_list(&oap->oap_rpc_item),\r\noap->oap_obj_off, oap->oap_page_off, oap->oap_count,\r\noap->oap_async_flags, oap->oap_brw_flags,\r\noap->oap_request, oap->oap_cli, obj,\r\nopg->ops_transfer_pinned,\r\nosc_submit_duration(opg), opg->ops_srvlock,\r\ncli->cl_r_in_flight, cli->cl_w_in_flight,\r\ncli->cl_max_rpcs_in_flight,\r\ncli->cl_avail_grant,\r\nosc_list(&cli->cl_cache_waiters),\r\nosc_list(&cli->cl_loi_ready_list),\r\nosc_list(&cli->cl_loi_hp_ready_list),\r\nosc_list(&cli->cl_loi_write_list),\r\nosc_list(&cli->cl_loi_read_list),\r\nosc_list(&obj->oo_ready_item),\r\nosc_list(&obj->oo_hp_ready_item),\r\nosc_list(&obj->oo_write_item),\r\nosc_list(&obj->oo_read_item),\r\natomic_read(&obj->oo_nr_reads),\r\nosc_list(&obj->oo_reading_exts),\r\natomic_read(&obj->oo_nr_writes),\r\nosc_list(&obj->oo_hp_exts),\r\nosc_list(&obj->oo_urgent_exts));\r\n}\r\nstatic void osc_page_delete(const struct lu_env *env,\r\nconst struct cl_page_slice *slice)\r\n{\r\nstruct osc_page *opg = cl2osc_page(slice);\r\nstruct osc_object *obj = cl2osc(opg->ops_cl.cpl_obj);\r\nint rc;\r\nCDEBUG(D_TRACE, "%p\n", opg);\r\nosc_page_transfer_put(env, opg);\r\nrc = osc_teardown_async_page(env, obj, opg);\r\nif (rc) {\r\nCL_PAGE_DEBUG(D_ERROR, env, slice->cpl_page,\r\n"Trying to teardown failed: %d\n", rc);\r\nLASSERT(0);\r\n}\r\nosc_lru_del(osc_cli(obj), opg);\r\nif (slice->cpl_page->cp_type == CPT_CACHEABLE) {\r\nvoid *value;\r\nspin_lock(&obj->oo_tree_lock);\r\nvalue = radix_tree_delete(&obj->oo_tree, osc_index(opg));\r\nif (value)\r\n--obj->oo_npages;\r\nspin_unlock(&obj->oo_tree_lock);\r\nLASSERT(ergo(value, value == opg));\r\n}\r\n}\r\nstatic void osc_page_clip(const struct lu_env *env,\r\nconst struct cl_page_slice *slice, int from, int to)\r\n{\r\nstruct osc_page *opg = cl2osc_page(slice);\r\nstruct osc_async_page *oap = &opg->ops_oap;\r\nopg->ops_from = from;\r\nopg->ops_to = to;\r\nspin_lock(&oap->oap_lock);\r\noap->oap_async_flags |= ASYNC_COUNT_STABLE;\r\nspin_unlock(&oap->oap_lock);\r\n}\r\nstatic int osc_page_cancel(const struct lu_env *env,\r\nconst struct cl_page_slice *slice)\r\n{\r\nstruct osc_page *opg = cl2osc_page(slice);\r\nint rc = 0;\r\nif (opg->ops_transfer_pinned)\r\nrc = osc_cancel_async_page(env, opg);\r\nLASSERT(ergo(rc == 0, opg->ops_transfer_pinned == 0));\r\nreturn rc;\r\n}\r\nstatic int osc_page_flush(const struct lu_env *env,\r\nconst struct cl_page_slice *slice,\r\nstruct cl_io *io)\r\n{\r\nstruct osc_page *opg = cl2osc_page(slice);\r\nint rc;\r\nrc = osc_flush_async_page(env, io, opg);\r\nreturn rc;\r\n}\r\nint osc_page_init(const struct lu_env *env, struct cl_object *obj,\r\nstruct cl_page *page, pgoff_t index)\r\n{\r\nstruct osc_object *osc = cl2osc(obj);\r\nstruct osc_page *opg = cl_object_page_slice(obj, page);\r\nint result;\r\nopg->ops_from = 0;\r\nopg->ops_to = PAGE_SIZE;\r\nresult = osc_prep_async_page(osc, opg, page->cp_vmpage,\r\ncl_offset(obj, index));\r\nif (result == 0) {\r\nstruct osc_io *oio = osc_env_io(env);\r\nopg->ops_srvlock = osc_io_srvlock(oio);\r\ncl_page_slice_add(page, &opg->ops_cl, obj, index,\r\n&osc_page_ops);\r\n}\r\nINIT_LIST_HEAD(&opg->ops_lru);\r\nif (page->cp_type == CPT_CACHEABLE && result == 0) {\r\nresult = osc_lru_alloc(env, osc_cli(osc), opg);\r\nif (result == 0) {\r\nspin_lock(&osc->oo_tree_lock);\r\nresult = radix_tree_insert(&osc->oo_tree, index, opg);\r\nif (result == 0)\r\n++osc->oo_npages;\r\nspin_unlock(&osc->oo_tree_lock);\r\nLASSERT(result == 0);\r\n}\r\n}\r\nreturn result;\r\n}\r\nvoid osc_page_submit(const struct lu_env *env, struct osc_page *opg,\r\nenum cl_req_type crt, int brw_flags)\r\n{\r\nstruct osc_async_page *oap = &opg->ops_oap;\r\nLASSERTF(oap->oap_magic == OAP_MAGIC, "Bad oap magic: oap %p, magic 0x%x\n",\r\noap, oap->oap_magic);\r\nLASSERT(oap->oap_async_flags & ASYNC_READY);\r\nLASSERT(oap->oap_async_flags & ASYNC_COUNT_STABLE);\r\noap->oap_cmd = crt == CRT_WRITE ? OBD_BRW_WRITE : OBD_BRW_READ;\r\noap->oap_page_off = opg->ops_from;\r\noap->oap_count = opg->ops_to - opg->ops_from;\r\noap->oap_brw_flags = brw_flags | OBD_BRW_SYNC;\r\nif (capable(CFS_CAP_SYS_RESOURCE)) {\r\noap->oap_brw_flags |= OBD_BRW_NOQUOTA;\r\noap->oap_cmd |= OBD_BRW_NOQUOTA;\r\n}\r\nopg->ops_submit_time = cfs_time_current();\r\nosc_page_transfer_get(opg, "transfer\0imm");\r\nosc_page_transfer_add(env, opg, crt);\r\n}\r\nstatic inline int lru_shrink_min(struct client_obd *cli)\r\n{\r\nreturn cli->cl_max_pages_per_rpc * 2;\r\n}\r\nstatic inline int lru_shrink_max(struct client_obd *cli)\r\n{\r\nreturn cli->cl_max_pages_per_rpc * cli->cl_max_rpcs_in_flight;\r\n}\r\nstatic int osc_cache_too_much(struct client_obd *cli)\r\n{\r\nstruct cl_client_cache *cache = cli->cl_cache;\r\nlong pages = atomic_long_read(&cli->cl_lru_in_list);\r\nunsigned long budget;\r\nbudget = cache->ccc_lru_max / (atomic_read(&cache->ccc_users) - 2);\r\nif (atomic_long_read(cli->cl_lru_left) < cache->ccc_lru_max >> 2) {\r\nif (pages >= budget)\r\nreturn lru_shrink_max(cli);\r\nelse if (pages >= budget / 2)\r\nreturn lru_shrink_min(cli);\r\n} else {\r\ntime64_t duration = ktime_get_real_seconds();\r\nlong timediff;\r\nduration -= cli->cl_lru_last_used;\r\ntimediff = (long)(duration >> 6);\r\nif (timediff > 0 && pages >= budget / timediff)\r\nreturn lru_shrink_min(cli);\r\n}\r\nreturn 0;\r\n}\r\nint lru_queue_work(const struct lu_env *env, void *data)\r\n{\r\nstruct client_obd *cli = data;\r\nint count;\r\nCDEBUG(D_CACHE, "%s: run LRU work for client obd\n", cli_name(cli));\r\ncount = osc_cache_too_much(cli);\r\nif (count > 0) {\r\nint rc = osc_lru_shrink(env, cli, count, false);\r\nCDEBUG(D_CACHE, "%s: shrank %d/%d pages from client obd\n",\r\ncli_name(cli), rc, count);\r\nif (rc >= count) {\r\nCDEBUG(D_CACHE, "%s: queue again\n", cli_name(cli));\r\nptlrpcd_queue_work(cli->cl_lru_work);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid osc_lru_add_batch(struct client_obd *cli, struct list_head *plist)\r\n{\r\nLIST_HEAD(lru);\r\nstruct osc_async_page *oap;\r\nlong npages = 0;\r\nlist_for_each_entry(oap, plist, oap_pending_item) {\r\nstruct osc_page *opg = oap2osc_page(oap);\r\nif (!opg->ops_in_lru)\r\ncontinue;\r\n++npages;\r\nLASSERT(list_empty(&opg->ops_lru));\r\nlist_add(&opg->ops_lru, &lru);\r\n}\r\nif (npages > 0) {\r\nspin_lock(&cli->cl_lru_list_lock);\r\nlist_splice_tail(&lru, &cli->cl_lru_list);\r\natomic_long_sub(npages, &cli->cl_lru_busy);\r\natomic_long_add(npages, &cli->cl_lru_in_list);\r\ncli->cl_lru_last_used = ktime_get_real_seconds();\r\nspin_unlock(&cli->cl_lru_list_lock);\r\nif (waitqueue_active(&osc_lru_waitq))\r\n(void)ptlrpcd_queue_work(cli->cl_lru_work);\r\n}\r\n}\r\nstatic void __osc_lru_del(struct client_obd *cli, struct osc_page *opg)\r\n{\r\nLASSERT(atomic_long_read(&cli->cl_lru_in_list) > 0);\r\nlist_del_init(&opg->ops_lru);\r\natomic_long_dec(&cli->cl_lru_in_list);\r\n}\r\nstatic void osc_lru_del(struct client_obd *cli, struct osc_page *opg)\r\n{\r\nif (opg->ops_in_lru) {\r\nspin_lock(&cli->cl_lru_list_lock);\r\nif (!list_empty(&opg->ops_lru)) {\r\n__osc_lru_del(cli, opg);\r\n} else {\r\nLASSERT(atomic_long_read(&cli->cl_lru_busy) > 0);\r\natomic_long_dec(&cli->cl_lru_busy);\r\n}\r\nspin_unlock(&cli->cl_lru_list_lock);\r\natomic_long_inc(cli->cl_lru_left);\r\nif (osc_cache_too_much(cli)) {\r\nCDEBUG(D_CACHE, "%s: queue LRU work\n", cli_name(cli));\r\n(void)ptlrpcd_queue_work(cli->cl_lru_work);\r\n}\r\nwake_up(&osc_lru_waitq);\r\n} else {\r\nLASSERT(list_empty(&opg->ops_lru));\r\n}\r\n}\r\nstatic void osc_lru_use(struct client_obd *cli, struct osc_page *opg)\r\n{\r\nif (opg->ops_in_lru && !list_empty(&opg->ops_lru)) {\r\nspin_lock(&cli->cl_lru_list_lock);\r\n__osc_lru_del(cli, opg);\r\nspin_unlock(&cli->cl_lru_list_lock);\r\natomic_long_inc(&cli->cl_lru_busy);\r\n}\r\n}\r\nstatic void discard_pagevec(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page **pvec, int max_index)\r\n{\r\nint i;\r\nfor (i = 0; i < max_index; i++) {\r\nstruct cl_page *page = pvec[i];\r\nLASSERT(cl_page_is_owned(page, io));\r\ncl_page_delete(env, page);\r\ncl_page_discard(env, io, page);\r\ncl_page_disown(env, io, page);\r\ncl_page_put(env, page);\r\npvec[i] = NULL;\r\n}\r\n}\r\nstatic inline bool lru_page_busy(struct client_obd *cli, struct cl_page *page)\r\n{\r\nif (cl_page_in_use_noref(page))\r\nreturn true;\r\nif (cli->cl_cache->ccc_unstable_check) {\r\nstruct page *vmpage = cl_page_vmpage(page);\r\nif (page_count(vmpage) - page_mapcount(vmpage) > 2)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nlong osc_lru_shrink(const struct lu_env *env, struct client_obd *cli,\r\nlong target, bool force)\r\n{\r\nstruct cl_io *io;\r\nstruct cl_object *clobj = NULL;\r\nstruct cl_page **pvec;\r\nstruct osc_page *opg;\r\nint maxscan = 0;\r\nlong count = 0;\r\nint index = 0;\r\nint rc = 0;\r\nLASSERT(atomic_long_read(&cli->cl_lru_in_list) >= 0);\r\nif (atomic_long_read(&cli->cl_lru_in_list) == 0 || target <= 0)\r\nreturn 0;\r\nCDEBUG(D_CACHE, "%s: shrinkers: %d, force: %d\n",\r\ncli_name(cli), atomic_read(&cli->cl_lru_shrinkers), force);\r\nif (!force) {\r\nif (atomic_read(&cli->cl_lru_shrinkers) > 0)\r\nreturn -EBUSY;\r\nif (atomic_inc_return(&cli->cl_lru_shrinkers) > 1) {\r\natomic_dec(&cli->cl_lru_shrinkers);\r\nreturn -EBUSY;\r\n}\r\n} else {\r\natomic_inc(&cli->cl_lru_shrinkers);\r\n}\r\npvec = (struct cl_page **)osc_env_info(env)->oti_pvec;\r\nio = &osc_env_info(env)->oti_io;\r\nspin_lock(&cli->cl_lru_list_lock);\r\nif (force)\r\ncli->cl_lru_reclaim++;\r\nmaxscan = min(target << 1, atomic_long_read(&cli->cl_lru_in_list));\r\nwhile (!list_empty(&cli->cl_lru_list)) {\r\nstruct cl_page *page;\r\nbool will_free = false;\r\nif (!force && atomic_read(&cli->cl_lru_shrinkers) > 1)\r\nbreak;\r\nif (--maxscan < 0)\r\nbreak;\r\nopg = list_entry(cli->cl_lru_list.next, struct osc_page,\r\nops_lru);\r\npage = opg->ops_cl.cpl_page;\r\nif (lru_page_busy(cli, page)) {\r\nlist_move_tail(&opg->ops_lru, &cli->cl_lru_list);\r\ncontinue;\r\n}\r\nLASSERT(page->cp_obj);\r\nif (clobj != page->cp_obj) {\r\nstruct cl_object *tmp = page->cp_obj;\r\ncl_object_get(tmp);\r\nspin_unlock(&cli->cl_lru_list_lock);\r\nif (clobj) {\r\ndiscard_pagevec(env, io, pvec, index);\r\nindex = 0;\r\ncl_io_fini(env, io);\r\ncl_object_put(env, clobj);\r\nclobj = NULL;\r\n}\r\nclobj = tmp;\r\nio->ci_obj = clobj;\r\nio->ci_ignore_layout = 1;\r\nrc = cl_io_init(env, io, CIT_MISC, clobj);\r\nspin_lock(&cli->cl_lru_list_lock);\r\nif (rc != 0)\r\nbreak;\r\n++maxscan;\r\ncontinue;\r\n}\r\nif (cl_page_own_try(env, io, page) == 0) {\r\nif (!lru_page_busy(cli, page)) {\r\n__osc_lru_del(cli, opg);\r\nopg->ops_in_lru = 0;\r\ncl_page_get(page);\r\nwill_free = true;\r\n} else {\r\ncl_page_disown(env, io, page);\r\n}\r\n}\r\nif (!will_free) {\r\nlist_move_tail(&opg->ops_lru, &cli->cl_lru_list);\r\ncontinue;\r\n}\r\npvec[index++] = page;\r\nif (unlikely(index == OTI_PVEC_SIZE)) {\r\nspin_unlock(&cli->cl_lru_list_lock);\r\ndiscard_pagevec(env, io, pvec, index);\r\nindex = 0;\r\nspin_lock(&cli->cl_lru_list_lock);\r\n}\r\nif (++count >= target)\r\nbreak;\r\n}\r\nspin_unlock(&cli->cl_lru_list_lock);\r\nif (clobj) {\r\ndiscard_pagevec(env, io, pvec, index);\r\ncl_io_fini(env, io);\r\ncl_object_put(env, clobj);\r\n}\r\natomic_dec(&cli->cl_lru_shrinkers);\r\nif (count > 0) {\r\natomic_long_add(count, cli->cl_lru_left);\r\nwake_up_all(&osc_lru_waitq);\r\n}\r\nreturn count > 0 ? count : rc;\r\n}\r\nstatic long osc_lru_reclaim(struct client_obd *cli, unsigned long npages)\r\n{\r\nstruct lu_env *env;\r\nstruct cl_client_cache *cache = cli->cl_cache;\r\nint max_scans;\r\nu16 refcheck;\r\nlong rc = 0;\r\nLASSERT(cache);\r\nenv = cl_env_get(&refcheck);\r\nif (IS_ERR(env))\r\nreturn 0;\r\nnpages = max_t(int, npages, cli->cl_max_pages_per_rpc);\r\nCDEBUG(D_CACHE, "%s: start to reclaim %ld pages from LRU\n",\r\ncli_name(cli), npages);\r\nrc = osc_lru_shrink(env, cli, npages, true);\r\nif (rc >= npages) {\r\nCDEBUG(D_CACHE, "%s: reclaimed %ld/%ld pages from LRU\n",\r\ncli_name(cli), rc, npages);\r\nif (osc_cache_too_much(cli) > 0)\r\nptlrpcd_queue_work(cli->cl_lru_work);\r\ngoto out;\r\n} else if (rc > 0) {\r\nnpages -= rc;\r\n}\r\nCDEBUG(D_CACHE, "%s: cli %p no free slots, pages: %ld/%ld, want: %ld\n",\r\ncli_name(cli), cli, atomic_long_read(&cli->cl_lru_in_list),\r\natomic_long_read(&cli->cl_lru_busy), npages);\r\nspin_lock(&cache->ccc_lru_lock);\r\nLASSERT(!list_empty(&cache->ccc_lru));\r\ncache->ccc_lru_shrinkers++;\r\nlist_move_tail(&cli->cl_lru_osc, &cache->ccc_lru);\r\nmax_scans = atomic_read(&cache->ccc_users) - 2;\r\nwhile (--max_scans > 0 && !list_empty(&cache->ccc_lru)) {\r\ncli = list_entry(cache->ccc_lru.next, struct client_obd,\r\ncl_lru_osc);\r\nCDEBUG(D_CACHE, "%s: cli %p LRU pages: %ld, busy: %ld.\n",\r\ncli_name(cli), cli,\r\natomic_long_read(&cli->cl_lru_in_list),\r\natomic_long_read(&cli->cl_lru_busy));\r\nlist_move_tail(&cli->cl_lru_osc, &cache->ccc_lru);\r\nif (osc_cache_too_much(cli) > 0) {\r\nspin_unlock(&cache->ccc_lru_lock);\r\nrc = osc_lru_shrink(env, cli, npages, true);\r\nspin_lock(&cache->ccc_lru_lock);\r\nif (rc >= npages)\r\nbreak;\r\nif (rc > 0)\r\nnpages -= rc;\r\n}\r\n}\r\nspin_unlock(&cache->ccc_lru_lock);\r\nout:\r\ncl_env_put(env, &refcheck);\r\nCDEBUG(D_CACHE, "%s: cli %p freed %ld pages.\n",\r\ncli_name(cli), cli, rc);\r\nreturn rc;\r\n}\r\nstatic int osc_lru_alloc(const struct lu_env *env, struct client_obd *cli,\r\nstruct osc_page *opg)\r\n{\r\nstruct l_wait_info lwi = LWI_INTR(LWI_ON_SIGNAL_NOOP, NULL);\r\nstruct osc_io *oio = osc_env_io(env);\r\nint rc = 0;\r\nif (!cli->cl_cache)\r\nreturn 0;\r\nif (oio->oi_lru_reserved > 0) {\r\n--oio->oi_lru_reserved;\r\ngoto out;\r\n}\r\nLASSERT(atomic_long_read(cli->cl_lru_left) >= 0);\r\nwhile (!atomic_long_add_unless(cli->cl_lru_left, -1, 0)) {\r\nrc = osc_lru_reclaim(cli, 1);\r\nif (rc < 0)\r\nbreak;\r\nif (rc > 0)\r\ncontinue;\r\ncond_resched();\r\nrc = l_wait_event(osc_lru_waitq,\r\natomic_long_read(cli->cl_lru_left) > 0,\r\n&lwi);\r\nif (rc < 0)\r\nbreak;\r\n}\r\nout:\r\nif (rc >= 0) {\r\natomic_long_inc(&cli->cl_lru_busy);\r\nopg->ops_in_lru = 1;\r\nrc = 0;\r\n}\r\nreturn rc;\r\n}\r\nunsigned long osc_lru_reserve(struct client_obd *cli, unsigned long npages)\r\n{\r\nunsigned long reserved = 0;\r\nunsigned long max_pages;\r\nunsigned long c;\r\nmax_pages = cli->cl_max_pages_per_rpc * cli->cl_max_rpcs_in_flight;\r\nif (npages > max_pages)\r\nnpages = max_pages;\r\nc = atomic_long_read(cli->cl_lru_left);\r\nif (c < npages && osc_lru_reclaim(cli, npages) > 0)\r\nc = atomic_long_read(cli->cl_lru_left);\r\nwhile (c >= npages) {\r\nif (c == atomic_long_cmpxchg(cli->cl_lru_left, c, c - npages)) {\r\nreserved = npages;\r\nbreak;\r\n}\r\nc = atomic_long_read(cli->cl_lru_left);\r\n}\r\nif (atomic_long_read(cli->cl_lru_left) < max_pages) {\r\nCDEBUG(D_CACHE, "%s: queue LRU, left: %lu/%ld.\n",\r\ncli_name(cli), atomic_long_read(cli->cl_lru_left),\r\nmax_pages);\r\n(void)ptlrpcd_queue_work(cli->cl_lru_work);\r\n}\r\nreturn reserved;\r\n}\r\nvoid osc_lru_unreserve(struct client_obd *cli, unsigned long npages)\r\n{\r\natomic_long_add(npages, cli->cl_lru_left);\r\nwake_up_all(&osc_lru_waitq);\r\n}\r\nstatic inline void unstable_page_accounting(struct ptlrpc_bulk_desc *desc,\r\nint factor)\r\n{\r\nint page_count = desc->bd_iov_count;\r\npg_data_t *last = NULL;\r\nint count = 0;\r\nint i;\r\nLASSERT(ptlrpc_is_bulk_desc_kiov(desc->bd_type));\r\nfor (i = 0; i < page_count; i++) {\r\npg_data_t *pgdat = page_pgdat(BD_GET_KIOV(desc, i).bv_page);\r\nif (likely(pgdat == last)) {\r\n++count;\r\ncontinue;\r\n}\r\nif (count > 0) {\r\nmod_node_page_state(pgdat, NR_UNSTABLE_NFS,\r\nfactor * count);\r\ncount = 0;\r\n}\r\nlast = pgdat;\r\n++count;\r\n}\r\nif (count > 0)\r\nmod_node_page_state(last, NR_UNSTABLE_NFS, factor * count);\r\n}\r\nstatic inline void add_unstable_page_accounting(struct ptlrpc_bulk_desc *desc)\r\n{\r\nunstable_page_accounting(desc, 1);\r\n}\r\nstatic inline void dec_unstable_page_accounting(struct ptlrpc_bulk_desc *desc)\r\n{\r\nunstable_page_accounting(desc, -1);\r\n}\r\nvoid osc_dec_unstable_pages(struct ptlrpc_request *req)\r\n{\r\nstruct client_obd *cli = &req->rq_import->imp_obd->u.cli;\r\nstruct ptlrpc_bulk_desc *desc = req->rq_bulk;\r\nint page_count = desc->bd_iov_count;\r\nlong unstable_count;\r\nLASSERT(page_count >= 0);\r\ndec_unstable_page_accounting(desc);\r\nunstable_count = atomic_long_sub_return(page_count,\r\n&cli->cl_unstable_count);\r\nLASSERT(unstable_count >= 0);\r\nunstable_count = atomic_long_sub_return(page_count,\r\n&cli->cl_cache->ccc_unstable_nr);\r\nLASSERT(unstable_count >= 0);\r\nif (!unstable_count)\r\nwake_up_all(&cli->cl_cache->ccc_unstable_waitq);\r\nif (waitqueue_active(&osc_lru_waitq))\r\n(void)ptlrpcd_queue_work(cli->cl_lru_work);\r\n}\r\nvoid osc_inc_unstable_pages(struct ptlrpc_request *req)\r\n{\r\nstruct client_obd *cli = &req->rq_import->imp_obd->u.cli;\r\nstruct ptlrpc_bulk_desc *desc = req->rq_bulk;\r\nlong page_count = desc->bd_iov_count;\r\nif (!cli->cl_cache || !cli->cl_cache->ccc_unstable_check)\r\nreturn;\r\nadd_unstable_page_accounting(desc);\r\natomic_long_add(page_count, &cli->cl_unstable_count);\r\natomic_long_add(page_count, &cli->cl_cache->ccc_unstable_nr);\r\nspin_lock(&req->rq_lock);\r\nif (unlikely(req->rq_committed)) {\r\nspin_unlock(&req->rq_lock);\r\nosc_dec_unstable_pages(req);\r\n} else {\r\nreq->rq_unstable = 1;\r\nspin_unlock(&req->rq_lock);\r\n}\r\n}\r\nbool osc_over_unstable_soft_limit(struct client_obd *cli)\r\n{\r\nlong unstable_nr, osc_unstable_count;\r\nif (!cli->cl_cache || !cli->cl_cache->ccc_unstable_check)\r\nreturn false;\r\nosc_unstable_count = atomic_long_read(&cli->cl_unstable_count);\r\nunstable_nr = atomic_long_read(&cli->cl_cache->ccc_unstable_nr);\r\nCDEBUG(D_CACHE,\r\n"%s: cli: %p unstable pages: %lu, osc unstable pages: %lu\n",\r\ncli_name(cli), cli, unstable_nr, osc_unstable_count);\r\nreturn unstable_nr > cli->cl_cache->ccc_lru_max >> 2 &&\r\nosc_unstable_count > cli->cl_max_pages_per_rpc *\r\ncli->cl_max_rpcs_in_flight;\r\n}\r\nunsigned long osc_cache_shrink_count(struct shrinker *sk,\r\nstruct shrink_control *sc)\r\n{\r\nstruct client_obd *cli;\r\nunsigned long cached = 0;\r\nspin_lock(&osc_shrink_lock);\r\nlist_for_each_entry(cli, &osc_shrink_list, cl_shrink_list)\r\ncached += atomic_long_read(&cli->cl_lru_in_list);\r\nspin_unlock(&osc_shrink_lock);\r\nreturn (cached * sysctl_vfs_cache_pressure) / 100;\r\n}\r\nunsigned long osc_cache_shrink_scan(struct shrinker *sk,\r\nstruct shrink_control *sc)\r\n{\r\nstruct client_obd *stop_anchor = NULL;\r\nstruct client_obd *cli;\r\nstruct lu_env *env;\r\nlong shrank = 0;\r\nu16 refcheck;\r\nint rc;\r\nif (!sc->nr_to_scan)\r\nreturn 0;\r\nif (!(sc->gfp_mask & __GFP_FS))\r\nreturn SHRINK_STOP;\r\nenv = cl_env_get(&refcheck);\r\nif (IS_ERR(env))\r\nreturn SHRINK_STOP;\r\nspin_lock(&osc_shrink_lock);\r\nwhile (!list_empty(&osc_shrink_list)) {\r\ncli = list_entry(osc_shrink_list.next, struct client_obd,\r\ncl_shrink_list);\r\nif (!stop_anchor)\r\nstop_anchor = cli;\r\nelse if (cli == stop_anchor)\r\nbreak;\r\nlist_move_tail(&cli->cl_shrink_list, &osc_shrink_list);\r\nspin_unlock(&osc_shrink_lock);\r\nrc = osc_lru_shrink(env, cli, (sc->nr_to_scan - shrank) >\r\ncli->cl_max_pages_per_rpc ?\r\ncli->cl_max_pages_per_rpc :\r\nsc->nr_to_scan - shrank, true);\r\nif (rc > 0)\r\nshrank += rc;\r\nif (shrank >= sc->nr_to_scan)\r\ngoto out;\r\nspin_lock(&osc_shrink_lock);\r\n}\r\nspin_unlock(&osc_shrink_lock);\r\nout:\r\ncl_env_put(env, &refcheck);\r\nreturn shrank;\r\n}
