static struct txx9dmac_chan *to_txx9dmac_chan(struct dma_chan *chan)\r\n{\r\nreturn container_of(chan, struct txx9dmac_chan, chan);\r\n}\r\nstatic struct txx9dmac_cregs __iomem *__dma_regs(const struct txx9dmac_chan *dc)\r\n{\r\nreturn dc->ch_regs;\r\n}\r\nstatic struct txx9dmac_cregs32 __iomem *__dma_regs32(\r\nconst struct txx9dmac_chan *dc)\r\n{\r\nreturn dc->ch_regs;\r\n}\r\nstatic dma_addr_t channel64_read_CHAR(const struct txx9dmac_chan *dc)\r\n{\r\nif (sizeof(__dma_regs(dc)->CHAR) == sizeof(u64))\r\nreturn channel64_readq(dc, CHAR);\r\nelse\r\nreturn channel64_readl(dc, CHAR);\r\n}\r\nstatic void channel64_write_CHAR(const struct txx9dmac_chan *dc, dma_addr_t val)\r\n{\r\nif (sizeof(__dma_regs(dc)->CHAR) == sizeof(u64))\r\nchannel64_writeq(dc, CHAR, val);\r\nelse\r\nchannel64_writel(dc, CHAR, val);\r\n}\r\nstatic void channel64_clear_CHAR(const struct txx9dmac_chan *dc)\r\n{\r\n#if defined(CONFIG_32BIT) && !defined(CONFIG_PHYS_ADDR_T_64BIT)\r\nchannel64_writel(dc, CHAR, 0);\r\nchannel64_writel(dc, __pad_CHAR, 0);\r\n#else\r\nchannel64_writeq(dc, CHAR, 0);\r\n#endif\r\n}\r\nstatic dma_addr_t channel_read_CHAR(const struct txx9dmac_chan *dc)\r\n{\r\nif (is_dmac64(dc))\r\nreturn channel64_read_CHAR(dc);\r\nelse\r\nreturn channel32_readl(dc, CHAR);\r\n}\r\nstatic void channel_write_CHAR(const struct txx9dmac_chan *dc, dma_addr_t val)\r\n{\r\nif (is_dmac64(dc))\r\nchannel64_write_CHAR(dc, val);\r\nelse\r\nchannel32_writel(dc, CHAR, val);\r\n}\r\nstatic struct txx9dmac_regs __iomem *__txx9dmac_regs(\r\nconst struct txx9dmac_dev *ddev)\r\n{\r\nreturn ddev->regs;\r\n}\r\nstatic struct txx9dmac_regs32 __iomem *__txx9dmac_regs32(\r\nconst struct txx9dmac_dev *ddev)\r\n{\r\nreturn ddev->regs;\r\n}\r\nstatic struct device *chan2dev(struct dma_chan *chan)\r\n{\r\nreturn &chan->dev->device;\r\n}\r\nstatic struct device *chan2parent(struct dma_chan *chan)\r\n{\r\nreturn chan->dev->device.parent;\r\n}\r\nstatic struct txx9dmac_desc *\r\ntxd_to_txx9dmac_desc(struct dma_async_tx_descriptor *txd)\r\n{\r\nreturn container_of(txd, struct txx9dmac_desc, txd);\r\n}\r\nstatic dma_addr_t desc_read_CHAR(const struct txx9dmac_chan *dc,\r\nconst struct txx9dmac_desc *desc)\r\n{\r\nreturn is_dmac64(dc) ? desc->hwdesc.CHAR : desc->hwdesc32.CHAR;\r\n}\r\nstatic void desc_write_CHAR(const struct txx9dmac_chan *dc,\r\nstruct txx9dmac_desc *desc, dma_addr_t val)\r\n{\r\nif (is_dmac64(dc))\r\ndesc->hwdesc.CHAR = val;\r\nelse\r\ndesc->hwdesc32.CHAR = val;\r\n}\r\nstatic struct txx9dmac_desc *txx9dmac_first_active(struct txx9dmac_chan *dc)\r\n{\r\nreturn list_entry(dc->active_list.next,\r\nstruct txx9dmac_desc, desc_node);\r\n}\r\nstatic struct txx9dmac_desc *txx9dmac_last_active(struct txx9dmac_chan *dc)\r\n{\r\nreturn list_entry(dc->active_list.prev,\r\nstruct txx9dmac_desc, desc_node);\r\n}\r\nstatic struct txx9dmac_desc *txx9dmac_first_queued(struct txx9dmac_chan *dc)\r\n{\r\nreturn list_entry(dc->queue.next, struct txx9dmac_desc, desc_node);\r\n}\r\nstatic struct txx9dmac_desc *txx9dmac_last_child(struct txx9dmac_desc *desc)\r\n{\r\nif (!list_empty(&desc->tx_list))\r\ndesc = list_entry(desc->tx_list.prev, typeof(*desc), desc_node);\r\nreturn desc;\r\n}\r\nstatic struct txx9dmac_desc *txx9dmac_desc_alloc(struct txx9dmac_chan *dc,\r\ngfp_t flags)\r\n{\r\nstruct txx9dmac_dev *ddev = dc->ddev;\r\nstruct txx9dmac_desc *desc;\r\ndesc = kzalloc(sizeof(*desc), flags);\r\nif (!desc)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->txd, &dc->chan);\r\ndesc->txd.tx_submit = txx9dmac_tx_submit;\r\ndesc->txd.flags = DMA_CTRL_ACK;\r\ndesc->txd.phys = dma_map_single(chan2parent(&dc->chan), &desc->hwdesc,\r\nddev->descsize, DMA_TO_DEVICE);\r\nreturn desc;\r\n}\r\nstatic struct txx9dmac_desc *txx9dmac_desc_get(struct txx9dmac_chan *dc)\r\n{\r\nstruct txx9dmac_desc *desc, *_desc;\r\nstruct txx9dmac_desc *ret = NULL;\r\nunsigned int i = 0;\r\nspin_lock_bh(&dc->lock);\r\nlist_for_each_entry_safe(desc, _desc, &dc->free_list, desc_node) {\r\nif (async_tx_test_ack(&desc->txd)) {\r\nlist_del(&desc->desc_node);\r\nret = desc;\r\nbreak;\r\n}\r\ndev_dbg(chan2dev(&dc->chan), "desc %p not ACKed\n", desc);\r\ni++;\r\n}\r\nspin_unlock_bh(&dc->lock);\r\ndev_vdbg(chan2dev(&dc->chan), "scanned %u descriptors on freelist\n",\r\ni);\r\nif (!ret) {\r\nret = txx9dmac_desc_alloc(dc, GFP_ATOMIC);\r\nif (ret) {\r\nspin_lock_bh(&dc->lock);\r\ndc->descs_allocated++;\r\nspin_unlock_bh(&dc->lock);\r\n} else\r\ndev_err(chan2dev(&dc->chan),\r\n"not enough descriptors available\n");\r\n}\r\nreturn ret;\r\n}\r\nstatic void txx9dmac_sync_desc_for_cpu(struct txx9dmac_chan *dc,\r\nstruct txx9dmac_desc *desc)\r\n{\r\nstruct txx9dmac_dev *ddev = dc->ddev;\r\nstruct txx9dmac_desc *child;\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\ndma_sync_single_for_cpu(chan2parent(&dc->chan),\r\nchild->txd.phys, ddev->descsize,\r\nDMA_TO_DEVICE);\r\ndma_sync_single_for_cpu(chan2parent(&dc->chan),\r\ndesc->txd.phys, ddev->descsize,\r\nDMA_TO_DEVICE);\r\n}\r\nstatic void txx9dmac_desc_put(struct txx9dmac_chan *dc,\r\nstruct txx9dmac_desc *desc)\r\n{\r\nif (desc) {\r\nstruct txx9dmac_desc *child;\r\ntxx9dmac_sync_desc_for_cpu(dc, desc);\r\nspin_lock_bh(&dc->lock);\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\ndev_vdbg(chan2dev(&dc->chan),\r\n"moving child desc %p to freelist\n",\r\nchild);\r\nlist_splice_init(&desc->tx_list, &dc->free_list);\r\ndev_vdbg(chan2dev(&dc->chan), "moving desc %p to freelist\n",\r\ndesc);\r\nlist_add(&desc->desc_node, &dc->free_list);\r\nspin_unlock_bh(&dc->lock);\r\n}\r\n}\r\nstatic void txx9dmac_dump_regs(struct txx9dmac_chan *dc)\r\n{\r\nif (is_dmac64(dc))\r\ndev_err(chan2dev(&dc->chan),\r\n" CHAR: %#llx SAR: %#llx DAR: %#llx CNTR: %#x"\r\n" SAIR: %#x DAIR: %#x CCR: %#x CSR: %#x\n",\r\n(u64)channel64_read_CHAR(dc),\r\nchannel64_readq(dc, SAR),\r\nchannel64_readq(dc, DAR),\r\nchannel64_readl(dc, CNTR),\r\nchannel64_readl(dc, SAIR),\r\nchannel64_readl(dc, DAIR),\r\nchannel64_readl(dc, CCR),\r\nchannel64_readl(dc, CSR));\r\nelse\r\ndev_err(chan2dev(&dc->chan),\r\n" CHAR: %#x SAR: %#x DAR: %#x CNTR: %#x"\r\n" SAIR: %#x DAIR: %#x CCR: %#x CSR: %#x\n",\r\nchannel32_readl(dc, CHAR),\r\nchannel32_readl(dc, SAR),\r\nchannel32_readl(dc, DAR),\r\nchannel32_readl(dc, CNTR),\r\nchannel32_readl(dc, SAIR),\r\nchannel32_readl(dc, DAIR),\r\nchannel32_readl(dc, CCR),\r\nchannel32_readl(dc, CSR));\r\n}\r\nstatic void txx9dmac_reset_chan(struct txx9dmac_chan *dc)\r\n{\r\nchannel_writel(dc, CCR, TXX9_DMA_CCR_CHRST);\r\nif (is_dmac64(dc)) {\r\nchannel64_clear_CHAR(dc);\r\nchannel_writeq(dc, SAR, 0);\r\nchannel_writeq(dc, DAR, 0);\r\n} else {\r\nchannel_writel(dc, CHAR, 0);\r\nchannel_writel(dc, SAR, 0);\r\nchannel_writel(dc, DAR, 0);\r\n}\r\nchannel_writel(dc, CNTR, 0);\r\nchannel_writel(dc, SAIR, 0);\r\nchannel_writel(dc, DAIR, 0);\r\nchannel_writel(dc, CCR, 0);\r\nmmiowb();\r\n}\r\nstatic void txx9dmac_dostart(struct txx9dmac_chan *dc,\r\nstruct txx9dmac_desc *first)\r\n{\r\nstruct txx9dmac_slave *ds = dc->chan.private;\r\nu32 sai, dai;\r\ndev_vdbg(chan2dev(&dc->chan), "dostart %u %p\n",\r\nfirst->txd.cookie, first);\r\nif (channel_readl(dc, CSR) & TXX9_DMA_CSR_XFACT) {\r\ndev_err(chan2dev(&dc->chan),\r\n"BUG: Attempted to start non-idle channel\n");\r\ntxx9dmac_dump_regs(dc);\r\nreturn;\r\n}\r\nif (is_dmac64(dc)) {\r\nchannel64_writel(dc, CNTR, 0);\r\nchannel64_writel(dc, CSR, 0xffffffff);\r\nif (ds) {\r\nif (ds->tx_reg) {\r\nsai = ds->reg_width;\r\ndai = 0;\r\n} else {\r\nsai = 0;\r\ndai = ds->reg_width;\r\n}\r\n} else {\r\nsai = 8;\r\ndai = 8;\r\n}\r\nchannel64_writel(dc, SAIR, sai);\r\nchannel64_writel(dc, DAIR, dai);\r\nchannel64_writel(dc, CCR, dc->ccr);\r\nchannel64_write_CHAR(dc, first->txd.phys);\r\n} else {\r\nchannel32_writel(dc, CNTR, 0);\r\nchannel32_writel(dc, CSR, 0xffffffff);\r\nif (ds) {\r\nif (ds->tx_reg) {\r\nsai = ds->reg_width;\r\ndai = 0;\r\n} else {\r\nsai = 0;\r\ndai = ds->reg_width;\r\n}\r\n} else {\r\nsai = 4;\r\ndai = 4;\r\n}\r\nchannel32_writel(dc, SAIR, sai);\r\nchannel32_writel(dc, DAIR, dai);\r\nif (txx9_dma_have_SMPCHN()) {\r\nchannel32_writel(dc, CCR, dc->ccr);\r\nchannel32_writel(dc, CHAR, first->txd.phys);\r\n} else {\r\nchannel32_writel(dc, CHAR, first->txd.phys);\r\nchannel32_writel(dc, CCR, dc->ccr);\r\n}\r\n}\r\n}\r\nstatic void\r\ntxx9dmac_descriptor_complete(struct txx9dmac_chan *dc,\r\nstruct txx9dmac_desc *desc)\r\n{\r\nstruct dmaengine_desc_callback cb;\r\nstruct dma_async_tx_descriptor *txd = &desc->txd;\r\ndev_vdbg(chan2dev(&dc->chan), "descriptor %u %p complete\n",\r\ntxd->cookie, desc);\r\ndma_cookie_complete(txd);\r\ndmaengine_desc_get_callback(txd, &cb);\r\ntxx9dmac_sync_desc_for_cpu(dc, desc);\r\nlist_splice_init(&desc->tx_list, &dc->free_list);\r\nlist_move(&desc->desc_node, &dc->free_list);\r\ndma_descriptor_unmap(txd);\r\ndmaengine_desc_callback_invoke(&cb, NULL);\r\ndma_run_dependencies(txd);\r\n}\r\nstatic void txx9dmac_dequeue(struct txx9dmac_chan *dc, struct list_head *list)\r\n{\r\nstruct txx9dmac_dev *ddev = dc->ddev;\r\nstruct txx9dmac_desc *desc;\r\nstruct txx9dmac_desc *prev = NULL;\r\nBUG_ON(!list_empty(list));\r\ndo {\r\ndesc = txx9dmac_first_queued(dc);\r\nif (prev) {\r\ndesc_write_CHAR(dc, prev, desc->txd.phys);\r\ndma_sync_single_for_device(chan2parent(&dc->chan),\r\nprev->txd.phys, ddev->descsize,\r\nDMA_TO_DEVICE);\r\n}\r\nprev = txx9dmac_last_child(desc);\r\nlist_move_tail(&desc->desc_node, list);\r\nif ((desc->txd.flags & DMA_PREP_INTERRUPT) &&\r\n!txx9dmac_chan_INTENT(dc))\r\nbreak;\r\n} while (!list_empty(&dc->queue));\r\n}\r\nstatic void txx9dmac_complete_all(struct txx9dmac_chan *dc)\r\n{\r\nstruct txx9dmac_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\nlist_splice_init(&dc->active_list, &list);\r\nif (!list_empty(&dc->queue)) {\r\ntxx9dmac_dequeue(dc, &dc->active_list);\r\ntxx9dmac_dostart(dc, txx9dmac_first_active(dc));\r\n}\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\ntxx9dmac_descriptor_complete(dc, desc);\r\n}\r\nstatic void txx9dmac_dump_desc(struct txx9dmac_chan *dc,\r\nstruct txx9dmac_hwdesc *desc)\r\n{\r\nif (is_dmac64(dc)) {\r\n#ifdef TXX9_DMA_USE_SIMPLE_CHAIN\r\ndev_crit(chan2dev(&dc->chan),\r\n" desc: ch%#llx s%#llx d%#llx c%#x\n",\r\n(u64)desc->CHAR, desc->SAR, desc->DAR, desc->CNTR);\r\n#else\r\ndev_crit(chan2dev(&dc->chan),\r\n" desc: ch%#llx s%#llx d%#llx c%#x"\r\n" si%#x di%#x cc%#x cs%#x\n",\r\n(u64)desc->CHAR, desc->SAR, desc->DAR, desc->CNTR,\r\ndesc->SAIR, desc->DAIR, desc->CCR, desc->CSR);\r\n#endif\r\n} else {\r\nstruct txx9dmac_hwdesc32 *d = (struct txx9dmac_hwdesc32 *)desc;\r\n#ifdef TXX9_DMA_USE_SIMPLE_CHAIN\r\ndev_crit(chan2dev(&dc->chan),\r\n" desc: ch%#x s%#x d%#x c%#x\n",\r\nd->CHAR, d->SAR, d->DAR, d->CNTR);\r\n#else\r\ndev_crit(chan2dev(&dc->chan),\r\n" desc: ch%#x s%#x d%#x c%#x"\r\n" si%#x di%#x cc%#x cs%#x\n",\r\nd->CHAR, d->SAR, d->DAR, d->CNTR,\r\nd->SAIR, d->DAIR, d->CCR, d->CSR);\r\n#endif\r\n}\r\n}\r\nstatic void txx9dmac_handle_error(struct txx9dmac_chan *dc, u32 csr)\r\n{\r\nstruct txx9dmac_desc *bad_desc;\r\nstruct txx9dmac_desc *child;\r\nu32 errors;\r\ndev_crit(chan2dev(&dc->chan), "Abnormal Chain Completion\n");\r\ntxx9dmac_dump_regs(dc);\r\nbad_desc = txx9dmac_first_active(dc);\r\nlist_del_init(&bad_desc->desc_node);\r\nerrors = csr & (TXX9_DMA_CSR_ABCHC |\r\nTXX9_DMA_CSR_CFERR | TXX9_DMA_CSR_CHERR |\r\nTXX9_DMA_CSR_DESERR | TXX9_DMA_CSR_SORERR);\r\nchannel_writel(dc, CSR, errors);\r\nif (list_empty(&dc->active_list) && !list_empty(&dc->queue))\r\ntxx9dmac_dequeue(dc, &dc->active_list);\r\nif (!list_empty(&dc->active_list))\r\ntxx9dmac_dostart(dc, txx9dmac_first_active(dc));\r\ndev_crit(chan2dev(&dc->chan),\r\n"Bad descriptor submitted for DMA! (cookie: %d)\n",\r\nbad_desc->txd.cookie);\r\ntxx9dmac_dump_desc(dc, &bad_desc->hwdesc);\r\nlist_for_each_entry(child, &bad_desc->tx_list, desc_node)\r\ntxx9dmac_dump_desc(dc, &child->hwdesc);\r\ntxx9dmac_descriptor_complete(dc, bad_desc);\r\n}\r\nstatic void txx9dmac_scan_descriptors(struct txx9dmac_chan *dc)\r\n{\r\ndma_addr_t chain;\r\nstruct txx9dmac_desc *desc, *_desc;\r\nstruct txx9dmac_desc *child;\r\nu32 csr;\r\nif (is_dmac64(dc)) {\r\nchain = channel64_read_CHAR(dc);\r\ncsr = channel64_readl(dc, CSR);\r\nchannel64_writel(dc, CSR, csr);\r\n} else {\r\nchain = channel32_readl(dc, CHAR);\r\ncsr = channel32_readl(dc, CSR);\r\nchannel32_writel(dc, CSR, csr);\r\n}\r\nif (!(csr & (TXX9_DMA_CSR_XFACT | TXX9_DMA_CSR_ABCHC))) {\r\ntxx9dmac_complete_all(dc);\r\nreturn;\r\n}\r\nif (!(csr & TXX9_DMA_CSR_CHNEN))\r\nchain = 0;\r\ndev_vdbg(chan2dev(&dc->chan), "scan_descriptors: char=%#llx\n",\r\n(u64)chain);\r\nlist_for_each_entry_safe(desc, _desc, &dc->active_list, desc_node) {\r\nif (desc_read_CHAR(dc, desc) == chain) {\r\nif (csr & TXX9_DMA_CSR_ABCHC)\r\ngoto scan_done;\r\nreturn;\r\n}\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\nif (desc_read_CHAR(dc, child) == chain) {\r\nif (csr & TXX9_DMA_CSR_ABCHC)\r\ngoto scan_done;\r\nreturn;\r\n}\r\ntxx9dmac_descriptor_complete(dc, desc);\r\n}\r\nscan_done:\r\nif (csr & TXX9_DMA_CSR_ABCHC) {\r\ntxx9dmac_handle_error(dc, csr);\r\nreturn;\r\n}\r\ndev_err(chan2dev(&dc->chan),\r\n"BUG: All descriptors done, but channel not idle!\n");\r\ntxx9dmac_reset_chan(dc);\r\nif (!list_empty(&dc->queue)) {\r\ntxx9dmac_dequeue(dc, &dc->active_list);\r\ntxx9dmac_dostart(dc, txx9dmac_first_active(dc));\r\n}\r\n}\r\nstatic void txx9dmac_chan_tasklet(unsigned long data)\r\n{\r\nint irq;\r\nu32 csr;\r\nstruct txx9dmac_chan *dc;\r\ndc = (struct txx9dmac_chan *)data;\r\ncsr = channel_readl(dc, CSR);\r\ndev_vdbg(chan2dev(&dc->chan), "tasklet: status=%x\n", csr);\r\nspin_lock(&dc->lock);\r\nif (csr & (TXX9_DMA_CSR_ABCHC | TXX9_DMA_CSR_NCHNC |\r\nTXX9_DMA_CSR_NTRNFC))\r\ntxx9dmac_scan_descriptors(dc);\r\nspin_unlock(&dc->lock);\r\nirq = dc->irq;\r\nenable_irq(irq);\r\n}\r\nstatic irqreturn_t txx9dmac_chan_interrupt(int irq, void *dev_id)\r\n{\r\nstruct txx9dmac_chan *dc = dev_id;\r\ndev_vdbg(chan2dev(&dc->chan), "interrupt: status=%#x\n",\r\nchannel_readl(dc, CSR));\r\ntasklet_schedule(&dc->tasklet);\r\ndisable_irq_nosync(irq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void txx9dmac_tasklet(unsigned long data)\r\n{\r\nint irq;\r\nu32 csr;\r\nstruct txx9dmac_chan *dc;\r\nstruct txx9dmac_dev *ddev = (struct txx9dmac_dev *)data;\r\nu32 mcr;\r\nint i;\r\nmcr = dma_readl(ddev, MCR);\r\ndev_vdbg(ddev->chan[0]->dma.dev, "tasklet: mcr=%x\n", mcr);\r\nfor (i = 0; i < TXX9_DMA_MAX_NR_CHANNELS; i++) {\r\nif ((mcr >> (24 + i)) & 0x11) {\r\ndc = ddev->chan[i];\r\ncsr = channel_readl(dc, CSR);\r\ndev_vdbg(chan2dev(&dc->chan), "tasklet: status=%x\n",\r\ncsr);\r\nspin_lock(&dc->lock);\r\nif (csr & (TXX9_DMA_CSR_ABCHC | TXX9_DMA_CSR_NCHNC |\r\nTXX9_DMA_CSR_NTRNFC))\r\ntxx9dmac_scan_descriptors(dc);\r\nspin_unlock(&dc->lock);\r\n}\r\n}\r\nirq = ddev->irq;\r\nenable_irq(irq);\r\n}\r\nstatic irqreturn_t txx9dmac_interrupt(int irq, void *dev_id)\r\n{\r\nstruct txx9dmac_dev *ddev = dev_id;\r\ndev_vdbg(ddev->chan[0]->dma.dev, "interrupt: status=%#x\n",\r\ndma_readl(ddev, MCR));\r\ntasklet_schedule(&ddev->tasklet);\r\ndisable_irq_nosync(irq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic dma_cookie_t txx9dmac_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct txx9dmac_desc *desc = txd_to_txx9dmac_desc(tx);\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nspin_lock_bh(&dc->lock);\r\ncookie = dma_cookie_assign(tx);\r\ndev_vdbg(chan2dev(tx->chan), "tx_submit: queued %u %p\n",\r\ndesc->txd.cookie, desc);\r\nlist_add_tail(&desc->desc_node, &dc->queue);\r\nspin_unlock_bh(&dc->lock);\r\nreturn cookie;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\ntxx9dmac_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\r\nstruct txx9dmac_dev *ddev = dc->ddev;\r\nstruct txx9dmac_desc *desc;\r\nstruct txx9dmac_desc *first;\r\nstruct txx9dmac_desc *prev;\r\nsize_t xfer_count;\r\nsize_t offset;\r\ndev_vdbg(chan2dev(chan), "prep_dma_memcpy d%#llx s%#llx l%#zx f%#lx\n",\r\n(u64)dest, (u64)src, len, flags);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan), "prep_dma_memcpy: length is zero!\n");\r\nreturn NULL;\r\n}\r\nprev = first = NULL;\r\nfor (offset = 0; offset < len; offset += xfer_count) {\r\nxfer_count = min_t(size_t, len - offset, TXX9_DMA_MAX_COUNT);\r\nif (__is_dmac64(ddev)) {\r\nif (xfer_count > 0x100 &&\r\n(xfer_count & 0xff) >= 0xfa &&\r\n(xfer_count & 0xff) <= 0xff)\r\nxfer_count -= 0x20;\r\n} else {\r\nif (xfer_count > 0x80 &&\r\n(xfer_count & 0x7f) >= 0x7e &&\r\n(xfer_count & 0x7f) <= 0x7f)\r\nxfer_count -= 0x20;\r\n}\r\ndesc = txx9dmac_desc_get(dc);\r\nif (!desc) {\r\ntxx9dmac_desc_put(dc, first);\r\nreturn NULL;\r\n}\r\nif (__is_dmac64(ddev)) {\r\ndesc->hwdesc.SAR = src + offset;\r\ndesc->hwdesc.DAR = dest + offset;\r\ndesc->hwdesc.CNTR = xfer_count;\r\ntxx9dmac_desc_set_nosimple(ddev, desc, 8, 8,\r\ndc->ccr | TXX9_DMA_CCR_XFACT);\r\n} else {\r\ndesc->hwdesc32.SAR = src + offset;\r\ndesc->hwdesc32.DAR = dest + offset;\r\ndesc->hwdesc32.CNTR = xfer_count;\r\ntxx9dmac_desc_set_nosimple(ddev, desc, 4, 4,\r\ndc->ccr | TXX9_DMA_CCR_XFACT);\r\n}\r\nif (!first) {\r\nfirst = desc;\r\n} else {\r\ndesc_write_CHAR(dc, prev, desc->txd.phys);\r\ndma_sync_single_for_device(chan2parent(&dc->chan),\r\nprev->txd.phys, ddev->descsize,\r\nDMA_TO_DEVICE);\r\nlist_add_tail(&desc->desc_node, &first->tx_list);\r\n}\r\nprev = desc;\r\n}\r\nif (flags & DMA_PREP_INTERRUPT)\r\ntxx9dmac_desc_set_INTENT(ddev, prev);\r\ndesc_write_CHAR(dc, prev, 0);\r\ndma_sync_single_for_device(chan2parent(&dc->chan),\r\nprev->txd.phys, ddev->descsize,\r\nDMA_TO_DEVICE);\r\nfirst->txd.flags = flags;\r\nfirst->len = len;\r\nreturn &first->txd;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\ntxx9dmac_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\r\nstruct txx9dmac_dev *ddev = dc->ddev;\r\nstruct txx9dmac_slave *ds = chan->private;\r\nstruct txx9dmac_desc *prev;\r\nstruct txx9dmac_desc *first;\r\nunsigned int i;\r\nstruct scatterlist *sg;\r\ndev_vdbg(chan2dev(chan), "prep_dma_slave\n");\r\nBUG_ON(!ds || !ds->reg_width);\r\nif (ds->tx_reg)\r\nBUG_ON(direction != DMA_MEM_TO_DEV);\r\nelse\r\nBUG_ON(direction != DMA_DEV_TO_MEM);\r\nif (unlikely(!sg_len))\r\nreturn NULL;\r\nprev = first = NULL;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct txx9dmac_desc *desc;\r\ndma_addr_t mem;\r\nu32 sai, dai;\r\ndesc = txx9dmac_desc_get(dc);\r\nif (!desc) {\r\ntxx9dmac_desc_put(dc, first);\r\nreturn NULL;\r\n}\r\nmem = sg_dma_address(sg);\r\nif (__is_dmac64(ddev)) {\r\nif (direction == DMA_MEM_TO_DEV) {\r\ndesc->hwdesc.SAR = mem;\r\ndesc->hwdesc.DAR = ds->tx_reg;\r\n} else {\r\ndesc->hwdesc.SAR = ds->rx_reg;\r\ndesc->hwdesc.DAR = mem;\r\n}\r\ndesc->hwdesc.CNTR = sg_dma_len(sg);\r\n} else {\r\nif (direction == DMA_MEM_TO_DEV) {\r\ndesc->hwdesc32.SAR = mem;\r\ndesc->hwdesc32.DAR = ds->tx_reg;\r\n} else {\r\ndesc->hwdesc32.SAR = ds->rx_reg;\r\ndesc->hwdesc32.DAR = mem;\r\n}\r\ndesc->hwdesc32.CNTR = sg_dma_len(sg);\r\n}\r\nif (direction == DMA_MEM_TO_DEV) {\r\nsai = ds->reg_width;\r\ndai = 0;\r\n} else {\r\nsai = 0;\r\ndai = ds->reg_width;\r\n}\r\ntxx9dmac_desc_set_nosimple(ddev, desc, sai, dai,\r\ndc->ccr | TXX9_DMA_CCR_XFACT);\r\nif (!first) {\r\nfirst = desc;\r\n} else {\r\ndesc_write_CHAR(dc, prev, desc->txd.phys);\r\ndma_sync_single_for_device(chan2parent(&dc->chan),\r\nprev->txd.phys,\r\nddev->descsize,\r\nDMA_TO_DEVICE);\r\nlist_add_tail(&desc->desc_node, &first->tx_list);\r\n}\r\nprev = desc;\r\n}\r\nif (flags & DMA_PREP_INTERRUPT)\r\ntxx9dmac_desc_set_INTENT(ddev, prev);\r\ndesc_write_CHAR(dc, prev, 0);\r\ndma_sync_single_for_device(chan2parent(&dc->chan),\r\nprev->txd.phys, ddev->descsize,\r\nDMA_TO_DEVICE);\r\nfirst->txd.flags = flags;\r\nfirst->len = 0;\r\nreturn &first->txd;\r\n}\r\nstatic int txx9dmac_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\r\nstruct txx9dmac_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\ndev_vdbg(chan2dev(chan), "terminate_all\n");\r\nspin_lock_bh(&dc->lock);\r\ntxx9dmac_reset_chan(dc);\r\nlist_splice_init(&dc->queue, &list);\r\nlist_splice_init(&dc->active_list, &list);\r\nspin_unlock_bh(&dc->lock);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\ntxx9dmac_descriptor_complete(dc, desc);\r\nreturn 0;\r\n}\r\nstatic enum dma_status\r\ntxx9dmac_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn DMA_COMPLETE;\r\nspin_lock_bh(&dc->lock);\r\ntxx9dmac_scan_descriptors(dc);\r\nspin_unlock_bh(&dc->lock);\r\nreturn dma_cookie_status(chan, cookie, txstate);\r\n}\r\nstatic void txx9dmac_chain_dynamic(struct txx9dmac_chan *dc,\r\nstruct txx9dmac_desc *prev)\r\n{\r\nstruct txx9dmac_dev *ddev = dc->ddev;\r\nstruct txx9dmac_desc *desc;\r\nLIST_HEAD(list);\r\nprev = txx9dmac_last_child(prev);\r\ntxx9dmac_dequeue(dc, &list);\r\ndesc = list_entry(list.next, struct txx9dmac_desc, desc_node);\r\ndesc_write_CHAR(dc, prev, desc->txd.phys);\r\ndma_sync_single_for_device(chan2parent(&dc->chan),\r\nprev->txd.phys, ddev->descsize,\r\nDMA_TO_DEVICE);\r\nmmiowb();\r\nif (!(channel_readl(dc, CSR) & TXX9_DMA_CSR_CHNEN) &&\r\nchannel_read_CHAR(dc) == prev->txd.phys)\r\nchannel_write_CHAR(dc, desc->txd.phys);\r\nlist_splice_tail(&list, &dc->active_list);\r\n}\r\nstatic void txx9dmac_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\r\nspin_lock_bh(&dc->lock);\r\nif (!list_empty(&dc->active_list))\r\ntxx9dmac_scan_descriptors(dc);\r\nif (!list_empty(&dc->queue)) {\r\nif (list_empty(&dc->active_list)) {\r\ntxx9dmac_dequeue(dc, &dc->active_list);\r\ntxx9dmac_dostart(dc, txx9dmac_first_active(dc));\r\n} else if (txx9_dma_have_SMPCHN()) {\r\nstruct txx9dmac_desc *prev = txx9dmac_last_active(dc);\r\nif (!(prev->txd.flags & DMA_PREP_INTERRUPT) ||\r\ntxx9dmac_chan_INTENT(dc))\r\ntxx9dmac_chain_dynamic(dc, prev);\r\n}\r\n}\r\nspin_unlock_bh(&dc->lock);\r\n}\r\nstatic int txx9dmac_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\r\nstruct txx9dmac_slave *ds = chan->private;\r\nstruct txx9dmac_desc *desc;\r\nint i;\r\ndev_vdbg(chan2dev(chan), "alloc_chan_resources\n");\r\nif (channel_readl(dc, CSR) & TXX9_DMA_CSR_XFACT) {\r\ndev_dbg(chan2dev(chan), "DMA channel not idle?\n");\r\nreturn -EIO;\r\n}\r\ndma_cookie_init(chan);\r\ndc->ccr = TXX9_DMA_CCR_IMMCHN | TXX9_DMA_CCR_INTENE | CCR_LE;\r\ntxx9dmac_chan_set_SMPCHN(dc);\r\nif (!txx9_dma_have_SMPCHN() || (dc->ccr & TXX9_DMA_CCR_SMPCHN))\r\ndc->ccr |= TXX9_DMA_CCR_INTENC;\r\nif (chan->device->device_prep_dma_memcpy) {\r\nif (ds)\r\nreturn -EINVAL;\r\ndc->ccr |= TXX9_DMA_CCR_XFSZ_X8;\r\n} else {\r\nif (!ds ||\r\n(ds->tx_reg && ds->rx_reg) || (!ds->tx_reg && !ds->rx_reg))\r\nreturn -EINVAL;\r\ndc->ccr |= TXX9_DMA_CCR_EXTRQ |\r\nTXX9_DMA_CCR_XFSZ(__ffs(ds->reg_width));\r\ntxx9dmac_chan_set_INTENT(dc);\r\n}\r\nspin_lock_bh(&dc->lock);\r\ni = dc->descs_allocated;\r\nwhile (dc->descs_allocated < TXX9_DMA_INITIAL_DESC_COUNT) {\r\nspin_unlock_bh(&dc->lock);\r\ndesc = txx9dmac_desc_alloc(dc, GFP_KERNEL);\r\nif (!desc) {\r\ndev_info(chan2dev(chan),\r\n"only allocated %d descriptors\n", i);\r\nspin_lock_bh(&dc->lock);\r\nbreak;\r\n}\r\ntxx9dmac_desc_put(dc, desc);\r\nspin_lock_bh(&dc->lock);\r\ni = ++dc->descs_allocated;\r\n}\r\nspin_unlock_bh(&dc->lock);\r\ndev_dbg(chan2dev(chan),\r\n"alloc_chan_resources allocated %d descriptors\n", i);\r\nreturn i;\r\n}\r\nstatic void txx9dmac_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\r\nstruct txx9dmac_dev *ddev = dc->ddev;\r\nstruct txx9dmac_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\ndev_dbg(chan2dev(chan), "free_chan_resources (descs allocated=%u)\n",\r\ndc->descs_allocated);\r\nBUG_ON(!list_empty(&dc->active_list));\r\nBUG_ON(!list_empty(&dc->queue));\r\nBUG_ON(channel_readl(dc, CSR) & TXX9_DMA_CSR_XFACT);\r\nspin_lock_bh(&dc->lock);\r\nlist_splice_init(&dc->free_list, &list);\r\ndc->descs_allocated = 0;\r\nspin_unlock_bh(&dc->lock);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node) {\r\ndev_vdbg(chan2dev(chan), " freeing descriptor %p\n", desc);\r\ndma_unmap_single(chan2parent(chan), desc->txd.phys,\r\nddev->descsize, DMA_TO_DEVICE);\r\nkfree(desc);\r\n}\r\ndev_vdbg(chan2dev(chan), "free_chan_resources done\n");\r\n}\r\nstatic void txx9dmac_off(struct txx9dmac_dev *ddev)\r\n{\r\ndma_writel(ddev, MCR, 0);\r\nmmiowb();\r\n}\r\nstatic int __init txx9dmac_chan_probe(struct platform_device *pdev)\r\n{\r\nstruct txx9dmac_chan_platform_data *cpdata =\r\ndev_get_platdata(&pdev->dev);\r\nstruct platform_device *dmac_dev = cpdata->dmac_dev;\r\nstruct txx9dmac_platform_data *pdata = dev_get_platdata(&dmac_dev->dev);\r\nstruct txx9dmac_chan *dc;\r\nint err;\r\nint ch = pdev->id % TXX9_DMA_MAX_NR_CHANNELS;\r\nint irq;\r\ndc = devm_kzalloc(&pdev->dev, sizeof(*dc), GFP_KERNEL);\r\nif (!dc)\r\nreturn -ENOMEM;\r\ndc->dma.dev = &pdev->dev;\r\ndc->dma.device_alloc_chan_resources = txx9dmac_alloc_chan_resources;\r\ndc->dma.device_free_chan_resources = txx9dmac_free_chan_resources;\r\ndc->dma.device_terminate_all = txx9dmac_terminate_all;\r\ndc->dma.device_tx_status = txx9dmac_tx_status;\r\ndc->dma.device_issue_pending = txx9dmac_issue_pending;\r\nif (pdata && pdata->memcpy_chan == ch) {\r\ndc->dma.device_prep_dma_memcpy = txx9dmac_prep_dma_memcpy;\r\ndma_cap_set(DMA_MEMCPY, dc->dma.cap_mask);\r\n} else {\r\ndc->dma.device_prep_slave_sg = txx9dmac_prep_slave_sg;\r\ndma_cap_set(DMA_SLAVE, dc->dma.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, dc->dma.cap_mask);\r\n}\r\nINIT_LIST_HEAD(&dc->dma.channels);\r\ndc->ddev = platform_get_drvdata(dmac_dev);\r\nif (dc->ddev->irq < 0) {\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0)\r\nreturn irq;\r\ntasklet_init(&dc->tasklet, txx9dmac_chan_tasklet,\r\n(unsigned long)dc);\r\ndc->irq = irq;\r\nerr = devm_request_irq(&pdev->dev, dc->irq,\r\ntxx9dmac_chan_interrupt, 0, dev_name(&pdev->dev), dc);\r\nif (err)\r\nreturn err;\r\n} else\r\ndc->irq = -1;\r\ndc->ddev->chan[ch] = dc;\r\ndc->chan.device = &dc->dma;\r\nlist_add_tail(&dc->chan.device_node, &dc->chan.device->channels);\r\ndma_cookie_init(&dc->chan);\r\nif (is_dmac64(dc))\r\ndc->ch_regs = &__txx9dmac_regs(dc->ddev)->CHAN[ch];\r\nelse\r\ndc->ch_regs = &__txx9dmac_regs32(dc->ddev)->CHAN[ch];\r\nspin_lock_init(&dc->lock);\r\nINIT_LIST_HEAD(&dc->active_list);\r\nINIT_LIST_HEAD(&dc->queue);\r\nINIT_LIST_HEAD(&dc->free_list);\r\ntxx9dmac_reset_chan(dc);\r\nplatform_set_drvdata(pdev, dc);\r\nerr = dma_async_device_register(&dc->dma);\r\nif (err)\r\nreturn err;\r\ndev_dbg(&pdev->dev, "TXx9 DMA Channel (dma%d%s%s)\n",\r\ndc->dma.dev_id,\r\ndma_has_cap(DMA_MEMCPY, dc->dma.cap_mask) ? " memcpy" : "",\r\ndma_has_cap(DMA_SLAVE, dc->dma.cap_mask) ? " slave" : "");\r\nreturn 0;\r\n}\r\nstatic int txx9dmac_chan_remove(struct platform_device *pdev)\r\n{\r\nstruct txx9dmac_chan *dc = platform_get_drvdata(pdev);\r\ndma_async_device_unregister(&dc->dma);\r\nif (dc->irq >= 0) {\r\ndevm_free_irq(&pdev->dev, dc->irq, dc);\r\ntasklet_kill(&dc->tasklet);\r\n}\r\ndc->ddev->chan[pdev->id % TXX9_DMA_MAX_NR_CHANNELS] = NULL;\r\nreturn 0;\r\n}\r\nstatic int __init txx9dmac_probe(struct platform_device *pdev)\r\n{\r\nstruct txx9dmac_platform_data *pdata = dev_get_platdata(&pdev->dev);\r\nstruct resource *io;\r\nstruct txx9dmac_dev *ddev;\r\nu32 mcr;\r\nint err;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!io)\r\nreturn -EINVAL;\r\nddev = devm_kzalloc(&pdev->dev, sizeof(*ddev), GFP_KERNEL);\r\nif (!ddev)\r\nreturn -ENOMEM;\r\nif (!devm_request_mem_region(&pdev->dev, io->start, resource_size(io),\r\ndev_name(&pdev->dev)))\r\nreturn -EBUSY;\r\nddev->regs = devm_ioremap(&pdev->dev, io->start, resource_size(io));\r\nif (!ddev->regs)\r\nreturn -ENOMEM;\r\nddev->have_64bit_regs = pdata->have_64bit_regs;\r\nif (__is_dmac64(ddev))\r\nddev->descsize = sizeof(struct txx9dmac_hwdesc);\r\nelse\r\nddev->descsize = sizeof(struct txx9dmac_hwdesc32);\r\ntxx9dmac_off(ddev);\r\nddev->irq = platform_get_irq(pdev, 0);\r\nif (ddev->irq >= 0) {\r\ntasklet_init(&ddev->tasklet, txx9dmac_tasklet,\r\n(unsigned long)ddev);\r\nerr = devm_request_irq(&pdev->dev, ddev->irq,\r\ntxx9dmac_interrupt, 0, dev_name(&pdev->dev), ddev);\r\nif (err)\r\nreturn err;\r\n}\r\nmcr = TXX9_DMA_MCR_MSTEN | MCR_LE;\r\nif (pdata && pdata->memcpy_chan >= 0)\r\nmcr |= TXX9_DMA_MCR_FIFUM(pdata->memcpy_chan);\r\ndma_writel(ddev, MCR, mcr);\r\nplatform_set_drvdata(pdev, ddev);\r\nreturn 0;\r\n}\r\nstatic int txx9dmac_remove(struct platform_device *pdev)\r\n{\r\nstruct txx9dmac_dev *ddev = platform_get_drvdata(pdev);\r\ntxx9dmac_off(ddev);\r\nif (ddev->irq >= 0) {\r\ndevm_free_irq(&pdev->dev, ddev->irq, ddev);\r\ntasklet_kill(&ddev->tasklet);\r\n}\r\nreturn 0;\r\n}\r\nstatic void txx9dmac_shutdown(struct platform_device *pdev)\r\n{\r\nstruct txx9dmac_dev *ddev = platform_get_drvdata(pdev);\r\ntxx9dmac_off(ddev);\r\n}\r\nstatic int txx9dmac_suspend_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct txx9dmac_dev *ddev = platform_get_drvdata(pdev);\r\ntxx9dmac_off(ddev);\r\nreturn 0;\r\n}\r\nstatic int txx9dmac_resume_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct txx9dmac_dev *ddev = platform_get_drvdata(pdev);\r\nstruct txx9dmac_platform_data *pdata = dev_get_platdata(&pdev->dev);\r\nu32 mcr;\r\nmcr = TXX9_DMA_MCR_MSTEN | MCR_LE;\r\nif (pdata && pdata->memcpy_chan >= 0)\r\nmcr |= TXX9_DMA_MCR_FIFUM(pdata->memcpy_chan);\r\ndma_writel(ddev, MCR, mcr);\r\nreturn 0;\r\n}\r\nstatic int __init txx9dmac_init(void)\r\n{\r\nint rc;\r\nrc = platform_driver_probe(&txx9dmac_driver, txx9dmac_probe);\r\nif (!rc) {\r\nrc = platform_driver_probe(&txx9dmac_chan_driver,\r\ntxx9dmac_chan_probe);\r\nif (rc)\r\nplatform_driver_unregister(&txx9dmac_driver);\r\n}\r\nreturn rc;\r\n}\r\nstatic void __exit txx9dmac_exit(void)\r\n{\r\nplatform_driver_unregister(&txx9dmac_chan_driver);\r\nplatform_driver_unregister(&txx9dmac_driver);\r\n}
