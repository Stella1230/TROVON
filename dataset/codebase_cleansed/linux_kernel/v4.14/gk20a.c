static enum nvkm_memory_target\r\ngk20a_instobj_target(struct nvkm_memory *memory)\r\n{\r\nreturn NVKM_MEM_TARGET_NCOH;\r\n}\r\nstatic u64\r\ngk20a_instobj_addr(struct nvkm_memory *memory)\r\n{\r\nreturn gk20a_instobj(memory)->mem.offset;\r\n}\r\nstatic u64\r\ngk20a_instobj_size(struct nvkm_memory *memory)\r\n{\r\nreturn (u64)gk20a_instobj(memory)->mem.size << 12;\r\n}\r\nstatic void\r\ngk20a_instobj_iommu_recycle_vaddr(struct gk20a_instobj_iommu *obj)\r\n{\r\nstruct gk20a_instmem *imem = obj->base.imem;\r\nWARN_ON(obj->use_cpt);\r\nlist_del(&obj->vaddr_node);\r\nvunmap(obj->base.vaddr);\r\nobj->base.vaddr = NULL;\r\nimem->vaddr_use -= nvkm_memory_size(&obj->base.memory);\r\nnvkm_debug(&imem->base.subdev, "vaddr used: %x/%x\n", imem->vaddr_use,\r\nimem->vaddr_max);\r\n}\r\nstatic void\r\ngk20a_instmem_vaddr_gc(struct gk20a_instmem *imem, const u64 size)\r\n{\r\nwhile (imem->vaddr_use + size > imem->vaddr_max) {\r\nif (list_empty(&imem->vaddr_lru))\r\nbreak;\r\ngk20a_instobj_iommu_recycle_vaddr(\r\nlist_first_entry(&imem->vaddr_lru,\r\nstruct gk20a_instobj_iommu, vaddr_node));\r\n}\r\n}\r\nstatic void __iomem *\r\ngk20a_instobj_acquire_dma(struct nvkm_memory *memory)\r\n{\r\nstruct gk20a_instobj *node = gk20a_instobj(memory);\r\nstruct gk20a_instmem *imem = node->imem;\r\nstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\r\nnvkm_ltc_flush(ltc);\r\nreturn node->vaddr;\r\n}\r\nstatic void __iomem *\r\ngk20a_instobj_acquire_iommu(struct nvkm_memory *memory)\r\n{\r\nstruct gk20a_instobj_iommu *node = gk20a_instobj_iommu(memory);\r\nstruct gk20a_instmem *imem = node->base.imem;\r\nstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\r\nconst u64 size = nvkm_memory_size(memory);\r\nnvkm_ltc_flush(ltc);\r\nmutex_lock(&imem->lock);\r\nif (node->base.vaddr) {\r\nif (!node->use_cpt) {\r\nlist_del(&node->vaddr_node);\r\n}\r\ngoto out;\r\n}\r\ngk20a_instmem_vaddr_gc(imem, size);\r\nnode->base.vaddr = vmap(node->pages, size >> PAGE_SHIFT, VM_MAP,\r\npgprot_writecombine(PAGE_KERNEL));\r\nif (!node->base.vaddr) {\r\nnvkm_error(&imem->base.subdev, "cannot map instobj - "\r\n"this is not going to end well...\n");\r\ngoto out;\r\n}\r\nimem->vaddr_use += size;\r\nnvkm_debug(&imem->base.subdev, "vaddr used: %x/%x\n",\r\nimem->vaddr_use, imem->vaddr_max);\r\nout:\r\nnode->use_cpt++;\r\nmutex_unlock(&imem->lock);\r\nreturn node->base.vaddr;\r\n}\r\nstatic void\r\ngk20a_instobj_release_dma(struct nvkm_memory *memory)\r\n{\r\nstruct gk20a_instobj *node = gk20a_instobj(memory);\r\nstruct gk20a_instmem *imem = node->imem;\r\nstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\r\nwmb();\r\nnvkm_ltc_invalidate(ltc);\r\n}\r\nstatic void\r\ngk20a_instobj_release_iommu(struct nvkm_memory *memory)\r\n{\r\nstruct gk20a_instobj_iommu *node = gk20a_instobj_iommu(memory);\r\nstruct gk20a_instmem *imem = node->base.imem;\r\nstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\r\nmutex_lock(&imem->lock);\r\nif (WARN_ON(node->use_cpt == 0))\r\ngoto out;\r\nif (--node->use_cpt == 0)\r\nlist_add_tail(&node->vaddr_node, &imem->vaddr_lru);\r\nout:\r\nmutex_unlock(&imem->lock);\r\nwmb();\r\nnvkm_ltc_invalidate(ltc);\r\n}\r\nstatic u32\r\ngk20a_instobj_rd32(struct nvkm_memory *memory, u64 offset)\r\n{\r\nstruct gk20a_instobj *node = gk20a_instobj(memory);\r\nreturn node->vaddr[offset / 4];\r\n}\r\nstatic void\r\ngk20a_instobj_wr32(struct nvkm_memory *memory, u64 offset, u32 data)\r\n{\r\nstruct gk20a_instobj *node = gk20a_instobj(memory);\r\nnode->vaddr[offset / 4] = data;\r\n}\r\nstatic void\r\ngk20a_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)\r\n{\r\nstruct gk20a_instobj *node = gk20a_instobj(memory);\r\nnvkm_vm_map_at(vma, offset, &node->mem);\r\n}\r\nstatic void *\r\ngk20a_instobj_dtor_dma(struct nvkm_memory *memory)\r\n{\r\nstruct gk20a_instobj_dma *node = gk20a_instobj_dma(memory);\r\nstruct gk20a_instmem *imem = node->base.imem;\r\nstruct device *dev = imem->base.subdev.device->dev;\r\nif (unlikely(!node->base.vaddr))\r\ngoto out;\r\ndma_free_attrs(dev, node->base.mem.size << PAGE_SHIFT, node->base.vaddr,\r\nnode->handle, imem->attrs);\r\nout:\r\nreturn node;\r\n}\r\nstatic void *\r\ngk20a_instobj_dtor_iommu(struct nvkm_memory *memory)\r\n{\r\nstruct gk20a_instobj_iommu *node = gk20a_instobj_iommu(memory);\r\nstruct gk20a_instmem *imem = node->base.imem;\r\nstruct device *dev = imem->base.subdev.device->dev;\r\nstruct nvkm_mm_node *r = node->base.mem.mem;\r\nint i;\r\nif (unlikely(!r))\r\ngoto out;\r\nmutex_lock(&imem->lock);\r\nif (node->base.vaddr)\r\ngk20a_instobj_iommu_recycle_vaddr(node);\r\nmutex_unlock(&imem->lock);\r\nr->offset &= ~BIT(imem->iommu_bit - imem->iommu_pgshift);\r\nfor (i = 0; i < node->base.mem.size; i++) {\r\niommu_unmap(imem->domain,\r\n(r->offset + i) << imem->iommu_pgshift, PAGE_SIZE);\r\ndma_unmap_page(dev, node->dma_addrs[i], PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\n__free_page(node->pages[i]);\r\n}\r\nmutex_lock(imem->mm_mutex);\r\nnvkm_mm_free(imem->mm, &r);\r\nmutex_unlock(imem->mm_mutex);\r\nout:\r\nreturn node;\r\n}\r\nstatic int\r\ngk20a_instobj_ctor_dma(struct gk20a_instmem *imem, u32 npages, u32 align,\r\nstruct gk20a_instobj **_node)\r\n{\r\nstruct gk20a_instobj_dma *node;\r\nstruct nvkm_subdev *subdev = &imem->base.subdev;\r\nstruct device *dev = subdev->device->dev;\r\nif (!(node = kzalloc(sizeof(*node), GFP_KERNEL)))\r\nreturn -ENOMEM;\r\n*_node = &node->base;\r\nnvkm_memory_ctor(&gk20a_instobj_func_dma, &node->base.memory);\r\nnode->base.vaddr = dma_alloc_attrs(dev, npages << PAGE_SHIFT,\r\n&node->handle, GFP_KERNEL,\r\nimem->attrs);\r\nif (!node->base.vaddr) {\r\nnvkm_error(subdev, "cannot allocate DMA memory\n");\r\nreturn -ENOMEM;\r\n}\r\nif (unlikely(node->handle & (align - 1)))\r\nnvkm_warn(subdev,\r\n"memory not aligned as requested: %pad (0x%x)\n",\r\n&node->handle, align);\r\nnode->r.type = 12;\r\nnode->r.offset = node->handle >> 12;\r\nnode->r.length = (npages << PAGE_SHIFT) >> 12;\r\nnode->base.mem.offset = node->handle;\r\nnode->base.mem.mem = &node->r;\r\nreturn 0;\r\n}\r\nstatic int\r\ngk20a_instobj_ctor_iommu(struct gk20a_instmem *imem, u32 npages, u32 align,\r\nstruct gk20a_instobj **_node)\r\n{\r\nstruct gk20a_instobj_iommu *node;\r\nstruct nvkm_subdev *subdev = &imem->base.subdev;\r\nstruct device *dev = subdev->device->dev;\r\nstruct nvkm_mm_node *r;\r\nint ret;\r\nint i;\r\nif (!(node = kzalloc(sizeof(*node) + ((sizeof(node->pages[0]) +\r\nsizeof(*node->dma_addrs)) * npages), GFP_KERNEL)))\r\nreturn -ENOMEM;\r\n*_node = &node->base;\r\nnode->dma_addrs = (void *)(node->pages + npages);\r\nnvkm_memory_ctor(&gk20a_instobj_func_iommu, &node->base.memory);\r\nfor (i = 0; i < npages; i++) {\r\nstruct page *p = alloc_page(GFP_KERNEL);\r\ndma_addr_t dma_adr;\r\nif (p == NULL) {\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\nnode->pages[i] = p;\r\ndma_adr = dma_map_page(dev, p, 0, PAGE_SIZE, DMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(dev, dma_adr)) {\r\nnvkm_error(subdev, "DMA mapping error!\n");\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\nnode->dma_addrs[i] = dma_adr;\r\n}\r\nmutex_lock(imem->mm_mutex);\r\nret = nvkm_mm_head(imem->mm, 0, 1, npages, npages,\r\nalign >> imem->iommu_pgshift, &r);\r\nmutex_unlock(imem->mm_mutex);\r\nif (ret) {\r\nnvkm_error(subdev, "IOMMU space is full!\n");\r\ngoto free_pages;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\nu32 offset = (r->offset + i) << imem->iommu_pgshift;\r\nret = iommu_map(imem->domain, offset, node->dma_addrs[i],\r\nPAGE_SIZE, IOMMU_READ | IOMMU_WRITE);\r\nif (ret < 0) {\r\nnvkm_error(subdev, "IOMMU mapping failure: %d\n", ret);\r\nwhile (i-- > 0) {\r\noffset -= PAGE_SIZE;\r\niommu_unmap(imem->domain, offset, PAGE_SIZE);\r\n}\r\ngoto release_area;\r\n}\r\n}\r\nr->offset |= BIT(imem->iommu_bit - imem->iommu_pgshift);\r\nnode->base.mem.offset = ((u64)r->offset) << imem->iommu_pgshift;\r\nnode->base.mem.mem = r;\r\nreturn 0;\r\nrelease_area:\r\nmutex_lock(imem->mm_mutex);\r\nnvkm_mm_free(imem->mm, &r);\r\nmutex_unlock(imem->mm_mutex);\r\nfree_pages:\r\nfor (i = 0; i < npages && node->pages[i] != NULL; i++) {\r\ndma_addr_t dma_addr = node->dma_addrs[i];\r\nif (dma_addr)\r\ndma_unmap_page(dev, dma_addr, PAGE_SIZE,\r\nDMA_BIDIRECTIONAL);\r\n__free_page(node->pages[i]);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\ngk20a_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,\r\nstruct nvkm_memory **pmemory)\r\n{\r\nstruct gk20a_instmem *imem = gk20a_instmem(base);\r\nstruct nvkm_subdev *subdev = &imem->base.subdev;\r\nstruct gk20a_instobj *node = NULL;\r\nint ret;\r\nnvkm_debug(subdev, "%s (%s): size: %x align: %x\n", __func__,\r\nimem->domain ? "IOMMU" : "DMA", size, align);\r\nsize = max(roundup(size, PAGE_SIZE), PAGE_SIZE);\r\nalign = max(roundup(align, PAGE_SIZE), PAGE_SIZE);\r\nif (imem->domain)\r\nret = gk20a_instobj_ctor_iommu(imem, size >> PAGE_SHIFT,\r\nalign, &node);\r\nelse\r\nret = gk20a_instobj_ctor_dma(imem, size >> PAGE_SHIFT,\r\nalign, &node);\r\n*pmemory = node ? &node->memory : NULL;\r\nif (ret)\r\nreturn ret;\r\nnode->imem = imem;\r\nnode->mem.size = size >> 12;\r\nnode->mem.memtype = 0;\r\nnode->mem.page_shift = 12;\r\nnvkm_debug(subdev, "alloc size: 0x%x, align: 0x%x, gaddr: 0x%llx\n",\r\nsize, align, node->mem.offset);\r\nreturn 0;\r\n}\r\nstatic void *\r\ngk20a_instmem_dtor(struct nvkm_instmem *base)\r\n{\r\nstruct gk20a_instmem *imem = gk20a_instmem(base);\r\nif (!list_empty(&imem->vaddr_lru))\r\nnvkm_warn(&base->subdev, "instobj LRU not empty!\n");\r\nif (imem->vaddr_use != 0)\r\nnvkm_warn(&base->subdev, "instobj vmap area not empty! "\r\n"0x%x bytes still mapped\n", imem->vaddr_use);\r\nreturn imem;\r\n}\r\nint\r\ngk20a_instmem_new(struct nvkm_device *device, int index,\r\nstruct nvkm_instmem **pimem)\r\n{\r\nstruct nvkm_device_tegra *tdev = device->func->tegra(device);\r\nstruct gk20a_instmem *imem;\r\nif (!(imem = kzalloc(sizeof(*imem), GFP_KERNEL)))\r\nreturn -ENOMEM;\r\nnvkm_instmem_ctor(&gk20a_instmem, device, index, &imem->base);\r\nmutex_init(&imem->lock);\r\n*pimem = &imem->base;\r\nimem->vaddr_use = 0;\r\nimem->vaddr_max = 0x100000;\r\nINIT_LIST_HEAD(&imem->vaddr_lru);\r\nif (tdev->iommu.domain) {\r\nimem->mm_mutex = &tdev->iommu.mutex;\r\nimem->mm = &tdev->iommu.mm;\r\nimem->domain = tdev->iommu.domain;\r\nimem->iommu_pgshift = tdev->iommu.pgshift;\r\nimem->iommu_bit = tdev->func->iommu_bit;\r\nnvkm_info(&imem->base.subdev, "using IOMMU\n");\r\n} else {\r\nimem->attrs = DMA_ATTR_NON_CONSISTENT |\r\nDMA_ATTR_WEAK_ORDERING |\r\nDMA_ATTR_WRITE_COMBINE;\r\nnvkm_info(&imem->base.subdev, "using DMA API\n");\r\n}\r\nreturn 0;\r\n}
