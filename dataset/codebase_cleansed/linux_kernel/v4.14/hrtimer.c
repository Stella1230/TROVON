static\r\nstruct hrtimer_clock_base *lock_hrtimer_base(const struct hrtimer *timer,\r\nunsigned long *flags)\r\n{\r\nstruct hrtimer_clock_base *base;\r\nfor (;;) {\r\nbase = timer->base;\r\nif (likely(base != &migration_base)) {\r\nraw_spin_lock_irqsave(&base->cpu_base->lock, *flags);\r\nif (likely(base == timer->base))\r\nreturn base;\r\nraw_spin_unlock_irqrestore(&base->cpu_base->lock, *flags);\r\n}\r\ncpu_relax();\r\n}\r\n}\r\nstatic int\r\nhrtimer_check_target(struct hrtimer *timer, struct hrtimer_clock_base *new_base)\r\n{\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\nktime_t expires;\r\nif (!new_base->cpu_base->hres_active)\r\nreturn 0;\r\nexpires = ktime_sub(hrtimer_get_expires(timer), new_base->offset);\r\nreturn expires <= new_base->cpu_base->expires_next;\r\n#else\r\nreturn 0;\r\n#endif\r\n}\r\nstatic inline\r\nstruct hrtimer_cpu_base *get_target_base(struct hrtimer_cpu_base *base,\r\nint pinned)\r\n{\r\nif (pinned || !base->migration_enabled)\r\nreturn base;\r\nreturn &per_cpu(hrtimer_bases, get_nohz_timer_target());\r\n}\r\nstatic inline\r\nstruct hrtimer_cpu_base *get_target_base(struct hrtimer_cpu_base *base,\r\nint pinned)\r\n{\r\nreturn base;\r\n}\r\nstatic inline struct hrtimer_clock_base *\r\nswitch_hrtimer_base(struct hrtimer *timer, struct hrtimer_clock_base *base,\r\nint pinned)\r\n{\r\nstruct hrtimer_cpu_base *new_cpu_base, *this_cpu_base;\r\nstruct hrtimer_clock_base *new_base;\r\nint basenum = base->index;\r\nthis_cpu_base = this_cpu_ptr(&hrtimer_bases);\r\nnew_cpu_base = get_target_base(this_cpu_base, pinned);\r\nagain:\r\nnew_base = &new_cpu_base->clock_base[basenum];\r\nif (base != new_base) {\r\nif (unlikely(hrtimer_callback_running(timer)))\r\nreturn base;\r\ntimer->base = &migration_base;\r\nraw_spin_unlock(&base->cpu_base->lock);\r\nraw_spin_lock(&new_base->cpu_base->lock);\r\nif (new_cpu_base != this_cpu_base &&\r\nhrtimer_check_target(timer, new_base)) {\r\nraw_spin_unlock(&new_base->cpu_base->lock);\r\nraw_spin_lock(&base->cpu_base->lock);\r\nnew_cpu_base = this_cpu_base;\r\ntimer->base = base;\r\ngoto again;\r\n}\r\ntimer->base = new_base;\r\n} else {\r\nif (new_cpu_base != this_cpu_base &&\r\nhrtimer_check_target(timer, new_base)) {\r\nnew_cpu_base = this_cpu_base;\r\ngoto again;\r\n}\r\n}\r\nreturn new_base;\r\n}\r\nstatic inline struct hrtimer_clock_base *\r\nlock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)\r\n{\r\nstruct hrtimer_clock_base *base = timer->base;\r\nraw_spin_lock_irqsave(&base->cpu_base->lock, *flags);\r\nreturn base;\r\n}\r\ns64 __ktime_divns(const ktime_t kt, s64 div)\r\n{\r\nint sft = 0;\r\ns64 dclc;\r\nu64 tmp;\r\ndclc = ktime_to_ns(kt);\r\ntmp = dclc < 0 ? -dclc : dclc;\r\nwhile (div >> 32) {\r\nsft++;\r\ndiv >>= 1;\r\n}\r\ntmp >>= sft;\r\ndo_div(tmp, (unsigned long) div);\r\nreturn dclc < 0 ? -tmp : tmp;\r\n}\r\nktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs)\r\n{\r\nktime_t res = ktime_add_unsafe(lhs, rhs);\r\nif (res < 0 || res < lhs || res < rhs)\r\nres = ktime_set(KTIME_SEC_MAX, 0);\r\nreturn res;\r\n}\r\nstatic void *hrtimer_debug_hint(void *addr)\r\n{\r\nreturn ((struct hrtimer *) addr)->function;\r\n}\r\nstatic bool hrtimer_fixup_init(void *addr, enum debug_obj_state state)\r\n{\r\nstruct hrtimer *timer = addr;\r\nswitch (state) {\r\ncase ODEBUG_STATE_ACTIVE:\r\nhrtimer_cancel(timer);\r\ndebug_object_init(timer, &hrtimer_debug_descr);\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic bool hrtimer_fixup_activate(void *addr, enum debug_obj_state state)\r\n{\r\nswitch (state) {\r\ncase ODEBUG_STATE_ACTIVE:\r\nWARN_ON(1);\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic bool hrtimer_fixup_free(void *addr, enum debug_obj_state state)\r\n{\r\nstruct hrtimer *timer = addr;\r\nswitch (state) {\r\ncase ODEBUG_STATE_ACTIVE:\r\nhrtimer_cancel(timer);\r\ndebug_object_free(timer, &hrtimer_debug_descr);\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic inline void debug_hrtimer_init(struct hrtimer *timer)\r\n{\r\ndebug_object_init(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_activate(struct hrtimer *timer)\r\n{\r\ndebug_object_activate(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_deactivate(struct hrtimer *timer)\r\n{\r\ndebug_object_deactivate(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_free(struct hrtimer *timer)\r\n{\r\ndebug_object_free(timer, &hrtimer_debug_descr);\r\n}\r\nvoid hrtimer_init_on_stack(struct hrtimer *timer, clockid_t clock_id,\r\nenum hrtimer_mode mode)\r\n{\r\ndebug_object_init_on_stack(timer, &hrtimer_debug_descr);\r\n__hrtimer_init(timer, clock_id, mode);\r\n}\r\nvoid destroy_hrtimer_on_stack(struct hrtimer *timer)\r\n{\r\ndebug_object_free(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_init(struct hrtimer *timer) { }\r\nstatic inline void debug_hrtimer_activate(struct hrtimer *timer) { }\r\nstatic inline void debug_hrtimer_deactivate(struct hrtimer *timer) { }\r\nstatic inline void\r\ndebug_init(struct hrtimer *timer, clockid_t clockid,\r\nenum hrtimer_mode mode)\r\n{\r\ndebug_hrtimer_init(timer);\r\ntrace_hrtimer_init(timer, clockid, mode);\r\n}\r\nstatic inline void debug_activate(struct hrtimer *timer)\r\n{\r\ndebug_hrtimer_activate(timer);\r\ntrace_hrtimer_start(timer);\r\n}\r\nstatic inline void debug_deactivate(struct hrtimer *timer)\r\n{\r\ndebug_hrtimer_deactivate(timer);\r\ntrace_hrtimer_cancel(timer);\r\n}\r\nstatic inline void hrtimer_update_next_timer(struct hrtimer_cpu_base *cpu_base,\r\nstruct hrtimer *timer)\r\n{\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\ncpu_base->next_timer = timer;\r\n#endif\r\n}\r\nstatic ktime_t __hrtimer_get_next_event(struct hrtimer_cpu_base *cpu_base)\r\n{\r\nstruct hrtimer_clock_base *base = cpu_base->clock_base;\r\nunsigned int active = cpu_base->active_bases;\r\nktime_t expires, expires_next = KTIME_MAX;\r\nhrtimer_update_next_timer(cpu_base, NULL);\r\nfor (; active; base++, active >>= 1) {\r\nstruct timerqueue_node *next;\r\nstruct hrtimer *timer;\r\nif (!(active & 0x01))\r\ncontinue;\r\nnext = timerqueue_getnext(&base->active);\r\ntimer = container_of(next, struct hrtimer, node);\r\nexpires = ktime_sub(hrtimer_get_expires(timer), base->offset);\r\nif (expires < expires_next) {\r\nexpires_next = expires;\r\nhrtimer_update_next_timer(cpu_base, timer);\r\n}\r\n}\r\nif (expires_next < 0)\r\nexpires_next = 0;\r\nreturn expires_next;\r\n}\r\nstatic inline ktime_t hrtimer_update_base(struct hrtimer_cpu_base *base)\r\n{\r\nktime_t *offs_real = &base->clock_base[HRTIMER_BASE_REALTIME].offset;\r\nktime_t *offs_boot = &base->clock_base[HRTIMER_BASE_BOOTTIME].offset;\r\nktime_t *offs_tai = &base->clock_base[HRTIMER_BASE_TAI].offset;\r\nreturn ktime_get_update_offsets_now(&base->clock_was_set_seq,\r\noffs_real, offs_boot, offs_tai);\r\n}\r\nstatic int __init setup_hrtimer_hres(char *str)\r\n{\r\nreturn (kstrtobool(str, &hrtimer_hres_enabled) == 0);\r\n}\r\nstatic inline int hrtimer_is_hres_enabled(void)\r\n{\r\nreturn hrtimer_hres_enabled;\r\n}\r\nstatic inline int __hrtimer_hres_active(struct hrtimer_cpu_base *cpu_base)\r\n{\r\nreturn cpu_base->hres_active;\r\n}\r\nstatic inline int hrtimer_hres_active(void)\r\n{\r\nreturn __hrtimer_hres_active(this_cpu_ptr(&hrtimer_bases));\r\n}\r\nstatic void\r\nhrtimer_force_reprogram(struct hrtimer_cpu_base *cpu_base, int skip_equal)\r\n{\r\nktime_t expires_next;\r\nif (!cpu_base->hres_active)\r\nreturn;\r\nexpires_next = __hrtimer_get_next_event(cpu_base);\r\nif (skip_equal && expires_next == cpu_base->expires_next)\r\nreturn;\r\ncpu_base->expires_next = expires_next;\r\nif (cpu_base->hang_detected)\r\nreturn;\r\ntick_program_event(cpu_base->expires_next, 1);\r\n}\r\nstatic void hrtimer_reprogram(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\r\nktime_t expires = ktime_sub(hrtimer_get_expires(timer), base->offset);\r\nWARN_ON_ONCE(hrtimer_get_expires_tv64(timer) < 0);\r\nif (base->cpu_base != cpu_base)\r\nreturn;\r\nif (cpu_base->in_hrtirq)\r\nreturn;\r\nif (expires < 0)\r\nexpires = 0;\r\nif (expires >= cpu_base->expires_next)\r\nreturn;\r\ncpu_base->next_timer = timer;\r\nif (cpu_base->hang_detected)\r\nreturn;\r\ncpu_base->expires_next = expires;\r\ntick_program_event(expires, 1);\r\n}\r\nstatic inline void hrtimer_init_hres(struct hrtimer_cpu_base *base)\r\n{\r\nbase->expires_next = KTIME_MAX;\r\nbase->hres_active = 0;\r\n}\r\nstatic void retrigger_next_event(void *arg)\r\n{\r\nstruct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);\r\nif (!base->hres_active)\r\nreturn;\r\nraw_spin_lock(&base->lock);\r\nhrtimer_update_base(base);\r\nhrtimer_force_reprogram(base, 0);\r\nraw_spin_unlock(&base->lock);\r\n}\r\nstatic void hrtimer_switch_to_hres(void)\r\n{\r\nstruct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);\r\nif (tick_init_highres()) {\r\nprintk(KERN_WARNING "Could not switch to high resolution "\r\n"mode on CPU %d\n", base->cpu);\r\nreturn;\r\n}\r\nbase->hres_active = 1;\r\nhrtimer_resolution = HIGH_RES_NSEC;\r\ntick_setup_sched_timer();\r\nretrigger_next_event(NULL);\r\n}\r\nstatic void clock_was_set_work(struct work_struct *work)\r\n{\r\nclock_was_set();\r\n}\r\nvoid clock_was_set_delayed(void)\r\n{\r\nschedule_work(&hrtimer_work);\r\n}\r\nstatic inline int __hrtimer_hres_active(struct hrtimer_cpu_base *b) { return 0; }\r\nstatic inline int hrtimer_hres_active(void) { return 0; }\r\nstatic inline int hrtimer_is_hres_enabled(void) { return 0; }\r\nstatic inline void hrtimer_switch_to_hres(void) { }\r\nstatic inline void\r\nhrtimer_force_reprogram(struct hrtimer_cpu_base *base, int skip_equal) { }\r\nstatic inline int hrtimer_reprogram(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void hrtimer_init_hres(struct hrtimer_cpu_base *base) { }\r\nstatic inline void retrigger_next_event(void *arg) { }\r\nvoid clock_was_set(void)\r\n{\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\non_each_cpu(retrigger_next_event, NULL, 1);\r\n#endif\r\ntimerfd_clock_was_set();\r\n}\r\nvoid hrtimers_resume(void)\r\n{\r\nWARN_ONCE(!irqs_disabled(),\r\nKERN_INFO "hrtimers_resume() called with IRQs enabled!");\r\nretrigger_next_event(NULL);\r\nclock_was_set_delayed();\r\n}\r\nstatic inline\r\nvoid unlock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)\r\n{\r\nraw_spin_unlock_irqrestore(&timer->base->cpu_base->lock, *flags);\r\n}\r\nu64 hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval)\r\n{\r\nu64 orun = 1;\r\nktime_t delta;\r\ndelta = ktime_sub(now, hrtimer_get_expires(timer));\r\nif (delta < 0)\r\nreturn 0;\r\nif (WARN_ON(timer->state & HRTIMER_STATE_ENQUEUED))\r\nreturn 0;\r\nif (interval < hrtimer_resolution)\r\ninterval = hrtimer_resolution;\r\nif (unlikely(delta >= interval)) {\r\ns64 incr = ktime_to_ns(interval);\r\norun = ktime_divns(delta, incr);\r\nhrtimer_add_expires_ns(timer, incr * orun);\r\nif (hrtimer_get_expires_tv64(timer) > now)\r\nreturn orun;\r\norun++;\r\n}\r\nhrtimer_add_expires(timer, interval);\r\nreturn orun;\r\n}\r\nstatic int enqueue_hrtimer(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base)\r\n{\r\ndebug_activate(timer);\r\nbase->cpu_base->active_bases |= 1 << base->index;\r\ntimer->state = HRTIMER_STATE_ENQUEUED;\r\nreturn timerqueue_add(&base->active, &timer->node);\r\n}\r\nstatic void __remove_hrtimer(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base,\r\nu8 newstate, int reprogram)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = base->cpu_base;\r\nu8 state = timer->state;\r\ntimer->state = newstate;\r\nif (!(state & HRTIMER_STATE_ENQUEUED))\r\nreturn;\r\nif (!timerqueue_del(&base->active, &timer->node))\r\ncpu_base->active_bases &= ~(1 << base->index);\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\nif (reprogram && timer == cpu_base->next_timer)\r\nhrtimer_force_reprogram(cpu_base, 1);\r\n#endif\r\n}\r\nstatic inline int\r\nremove_hrtimer(struct hrtimer *timer, struct hrtimer_clock_base *base, bool restart)\r\n{\r\nif (hrtimer_is_queued(timer)) {\r\nu8 state = timer->state;\r\nint reprogram;\r\ndebug_deactivate(timer);\r\nreprogram = base->cpu_base == this_cpu_ptr(&hrtimer_bases);\r\nif (!restart)\r\nstate = HRTIMER_STATE_INACTIVE;\r\n__remove_hrtimer(timer, base, state, reprogram);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline ktime_t hrtimer_update_lowres(struct hrtimer *timer, ktime_t tim,\r\nconst enum hrtimer_mode mode)\r\n{\r\n#ifdef CONFIG_TIME_LOW_RES\r\ntimer->is_rel = mode & HRTIMER_MODE_REL;\r\nif (timer->is_rel)\r\ntim = ktime_add_safe(tim, hrtimer_resolution);\r\n#endif\r\nreturn tim;\r\n}\r\nvoid hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,\r\nu64 delta_ns, const enum hrtimer_mode mode)\r\n{\r\nstruct hrtimer_clock_base *base, *new_base;\r\nunsigned long flags;\r\nint leftmost;\r\nbase = lock_hrtimer_base(timer, &flags);\r\nremove_hrtimer(timer, base, true);\r\nif (mode & HRTIMER_MODE_REL)\r\ntim = ktime_add_safe(tim, base->get_time());\r\ntim = hrtimer_update_lowres(timer, tim, mode);\r\nhrtimer_set_expires_range_ns(timer, tim, delta_ns);\r\nnew_base = switch_hrtimer_base(timer, base, mode & HRTIMER_MODE_PINNED);\r\nleftmost = enqueue_hrtimer(timer, new_base);\r\nif (!leftmost)\r\ngoto unlock;\r\nif (!hrtimer_is_hres_active(timer)) {\r\nif (new_base->cpu_base->nohz_active)\r\nwake_up_nohz_cpu(new_base->cpu_base->cpu);\r\n} else {\r\nhrtimer_reprogram(timer, new_base);\r\n}\r\nunlock:\r\nunlock_hrtimer_base(timer, &flags);\r\n}\r\nint hrtimer_try_to_cancel(struct hrtimer *timer)\r\n{\r\nstruct hrtimer_clock_base *base;\r\nunsigned long flags;\r\nint ret = -1;\r\nif (!hrtimer_active(timer))\r\nreturn 0;\r\nbase = lock_hrtimer_base(timer, &flags);\r\nif (!hrtimer_callback_running(timer))\r\nret = remove_hrtimer(timer, base, false);\r\nunlock_hrtimer_base(timer, &flags);\r\nreturn ret;\r\n}\r\nint hrtimer_cancel(struct hrtimer *timer)\r\n{\r\nfor (;;) {\r\nint ret = hrtimer_try_to_cancel(timer);\r\nif (ret >= 0)\r\nreturn ret;\r\ncpu_relax();\r\n}\r\n}\r\nktime_t __hrtimer_get_remaining(const struct hrtimer *timer, bool adjust)\r\n{\r\nunsigned long flags;\r\nktime_t rem;\r\nlock_hrtimer_base(timer, &flags);\r\nif (IS_ENABLED(CONFIG_TIME_LOW_RES) && adjust)\r\nrem = hrtimer_expires_remaining_adjusted(timer);\r\nelse\r\nrem = hrtimer_expires_remaining(timer);\r\nunlock_hrtimer_base(timer, &flags);\r\nreturn rem;\r\n}\r\nu64 hrtimer_get_next_event(void)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\r\nu64 expires = KTIME_MAX;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&cpu_base->lock, flags);\r\nif (!__hrtimer_hres_active(cpu_base))\r\nexpires = __hrtimer_get_next_event(cpu_base);\r\nraw_spin_unlock_irqrestore(&cpu_base->lock, flags);\r\nreturn expires;\r\n}\r\nstatic inline int hrtimer_clockid_to_base(clockid_t clock_id)\r\n{\r\nif (likely(clock_id < MAX_CLOCKS)) {\r\nint base = hrtimer_clock_to_base_table[clock_id];\r\nif (likely(base != HRTIMER_MAX_CLOCK_BASES))\r\nreturn base;\r\n}\r\nWARN(1, "Invalid clockid %d. Using MONOTONIC\n", clock_id);\r\nreturn HRTIMER_BASE_MONOTONIC;\r\n}\r\nstatic void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\r\nenum hrtimer_mode mode)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base;\r\nint base;\r\nmemset(timer, 0, sizeof(struct hrtimer));\r\ncpu_base = raw_cpu_ptr(&hrtimer_bases);\r\nif (clock_id == CLOCK_REALTIME && mode != HRTIMER_MODE_ABS)\r\nclock_id = CLOCK_MONOTONIC;\r\nbase = hrtimer_clockid_to_base(clock_id);\r\ntimer->base = &cpu_base->clock_base[base];\r\ntimerqueue_init(&timer->node);\r\n}\r\nvoid hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\r\nenum hrtimer_mode mode)\r\n{\r\ndebug_init(timer, clock_id, mode);\r\n__hrtimer_init(timer, clock_id, mode);\r\n}\r\nbool hrtimer_active(const struct hrtimer *timer)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base;\r\nunsigned int seq;\r\ndo {\r\ncpu_base = READ_ONCE(timer->base->cpu_base);\r\nseq = raw_read_seqcount_begin(&cpu_base->seq);\r\nif (timer->state != HRTIMER_STATE_INACTIVE ||\r\ncpu_base->running == timer)\r\nreturn true;\r\n} while (read_seqcount_retry(&cpu_base->seq, seq) ||\r\ncpu_base != READ_ONCE(timer->base->cpu_base));\r\nreturn false;\r\n}\r\nstatic void __run_hrtimer(struct hrtimer_cpu_base *cpu_base,\r\nstruct hrtimer_clock_base *base,\r\nstruct hrtimer *timer, ktime_t *now)\r\n{\r\nenum hrtimer_restart (*fn)(struct hrtimer *);\r\nint restart;\r\nlockdep_assert_held(&cpu_base->lock);\r\ndebug_deactivate(timer);\r\ncpu_base->running = timer;\r\nraw_write_seqcount_barrier(&cpu_base->seq);\r\n__remove_hrtimer(timer, base, HRTIMER_STATE_INACTIVE, 0);\r\nfn = timer->function;\r\nif (IS_ENABLED(CONFIG_TIME_LOW_RES))\r\ntimer->is_rel = false;\r\nraw_spin_unlock(&cpu_base->lock);\r\ntrace_hrtimer_expire_entry(timer, now);\r\nrestart = fn(timer);\r\ntrace_hrtimer_expire_exit(timer);\r\nraw_spin_lock(&cpu_base->lock);\r\nif (restart != HRTIMER_NORESTART &&\r\n!(timer->state & HRTIMER_STATE_ENQUEUED))\r\nenqueue_hrtimer(timer, base);\r\nraw_write_seqcount_barrier(&cpu_base->seq);\r\nWARN_ON_ONCE(cpu_base->running != timer);\r\ncpu_base->running = NULL;\r\n}\r\nstatic void __hrtimer_run_queues(struct hrtimer_cpu_base *cpu_base, ktime_t now)\r\n{\r\nstruct hrtimer_clock_base *base = cpu_base->clock_base;\r\nunsigned int active = cpu_base->active_bases;\r\nfor (; active; base++, active >>= 1) {\r\nstruct timerqueue_node *node;\r\nktime_t basenow;\r\nif (!(active & 0x01))\r\ncontinue;\r\nbasenow = ktime_add(now, base->offset);\r\nwhile ((node = timerqueue_getnext(&base->active))) {\r\nstruct hrtimer *timer;\r\ntimer = container_of(node, struct hrtimer, node);\r\nif (basenow < hrtimer_get_softexpires_tv64(timer))\r\nbreak;\r\n__run_hrtimer(cpu_base, base, timer, &basenow);\r\n}\r\n}\r\n}\r\nvoid hrtimer_interrupt(struct clock_event_device *dev)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\r\nktime_t expires_next, now, entry_time, delta;\r\nint retries = 0;\r\nBUG_ON(!cpu_base->hres_active);\r\ncpu_base->nr_events++;\r\ndev->next_event = KTIME_MAX;\r\nraw_spin_lock(&cpu_base->lock);\r\nentry_time = now = hrtimer_update_base(cpu_base);\r\nretry:\r\ncpu_base->in_hrtirq = 1;\r\ncpu_base->expires_next = KTIME_MAX;\r\n__hrtimer_run_queues(cpu_base, now);\r\nexpires_next = __hrtimer_get_next_event(cpu_base);\r\ncpu_base->expires_next = expires_next;\r\ncpu_base->in_hrtirq = 0;\r\nraw_spin_unlock(&cpu_base->lock);\r\nif (!tick_program_event(expires_next, 0)) {\r\ncpu_base->hang_detected = 0;\r\nreturn;\r\n}\r\nraw_spin_lock(&cpu_base->lock);\r\nnow = hrtimer_update_base(cpu_base);\r\ncpu_base->nr_retries++;\r\nif (++retries < 3)\r\ngoto retry;\r\ncpu_base->nr_hangs++;\r\ncpu_base->hang_detected = 1;\r\nraw_spin_unlock(&cpu_base->lock);\r\ndelta = ktime_sub(now, entry_time);\r\nif ((unsigned int)delta > cpu_base->max_hang_time)\r\ncpu_base->max_hang_time = (unsigned int) delta;\r\nif (delta > 100 * NSEC_PER_MSEC)\r\nexpires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);\r\nelse\r\nexpires_next = ktime_add(now, delta);\r\ntick_program_event(expires_next, 1);\r\nprintk_once(KERN_WARNING "hrtimer: interrupt took %llu ns\n",\r\nktime_to_ns(delta));\r\n}\r\nstatic inline void __hrtimer_peek_ahead_timers(void)\r\n{\r\nstruct tick_device *td;\r\nif (!hrtimer_hres_active())\r\nreturn;\r\ntd = this_cpu_ptr(&tick_cpu_device);\r\nif (td && td->evtdev)\r\nhrtimer_interrupt(td->evtdev);\r\n}\r\nstatic inline void __hrtimer_peek_ahead_timers(void) { }\r\nvoid hrtimer_run_queues(void)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\r\nktime_t now;\r\nif (__hrtimer_hres_active(cpu_base))\r\nreturn;\r\nif (tick_check_oneshot_change(!hrtimer_is_hres_enabled())) {\r\nhrtimer_switch_to_hres();\r\nreturn;\r\n}\r\nraw_spin_lock(&cpu_base->lock);\r\nnow = hrtimer_update_base(cpu_base);\r\n__hrtimer_run_queues(cpu_base, now);\r\nraw_spin_unlock(&cpu_base->lock);\r\n}\r\nstatic enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer)\r\n{\r\nstruct hrtimer_sleeper *t =\r\ncontainer_of(timer, struct hrtimer_sleeper, timer);\r\nstruct task_struct *task = t->task;\r\nt->task = NULL;\r\nif (task)\r\nwake_up_process(task);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nvoid hrtimer_init_sleeper(struct hrtimer_sleeper *sl, struct task_struct *task)\r\n{\r\nsl->timer.function = hrtimer_wakeup;\r\nsl->task = task;\r\n}\r\nint nanosleep_copyout(struct restart_block *restart, struct timespec64 *ts)\r\n{\r\nswitch(restart->nanosleep.type) {\r\n#ifdef CONFIG_COMPAT\r\ncase TT_COMPAT:\r\nif (compat_put_timespec64(ts, restart->nanosleep.compat_rmtp))\r\nreturn -EFAULT;\r\nbreak;\r\n#endif\r\ncase TT_NATIVE:\r\nif (put_timespec64(ts, restart->nanosleep.rmtp))\r\nreturn -EFAULT;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn -ERESTART_RESTARTBLOCK;\r\n}\r\nstatic int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode)\r\n{\r\nstruct restart_block *restart;\r\nhrtimer_init_sleeper(t, current);\r\ndo {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nhrtimer_start_expires(&t->timer, mode);\r\nif (likely(t->task))\r\nfreezable_schedule();\r\nhrtimer_cancel(&t->timer);\r\nmode = HRTIMER_MODE_ABS;\r\n} while (t->task && !signal_pending(current));\r\n__set_current_state(TASK_RUNNING);\r\nif (!t->task)\r\nreturn 0;\r\nrestart = &current->restart_block;\r\nif (restart->nanosleep.type != TT_NONE) {\r\nktime_t rem = hrtimer_expires_remaining(&t->timer);\r\nstruct timespec64 rmt;\r\nif (rem <= 0)\r\nreturn 0;\r\nrmt = ktime_to_timespec64(rem);\r\nreturn nanosleep_copyout(restart, &rmt);\r\n}\r\nreturn -ERESTART_RESTARTBLOCK;\r\n}\r\nstatic long __sched hrtimer_nanosleep_restart(struct restart_block *restart)\r\n{\r\nstruct hrtimer_sleeper t;\r\nint ret;\r\nhrtimer_init_on_stack(&t.timer, restart->nanosleep.clockid,\r\nHRTIMER_MODE_ABS);\r\nhrtimer_set_expires_tv64(&t.timer, restart->nanosleep.expires);\r\nret = do_nanosleep(&t, HRTIMER_MODE_ABS);\r\ndestroy_hrtimer_on_stack(&t.timer);\r\nreturn ret;\r\n}\r\nlong hrtimer_nanosleep(const struct timespec64 *rqtp,\r\nconst enum hrtimer_mode mode, const clockid_t clockid)\r\n{\r\nstruct restart_block *restart;\r\nstruct hrtimer_sleeper t;\r\nint ret = 0;\r\nu64 slack;\r\nslack = current->timer_slack_ns;\r\nif (dl_task(current) || rt_task(current))\r\nslack = 0;\r\nhrtimer_init_on_stack(&t.timer, clockid, mode);\r\nhrtimer_set_expires_range_ns(&t.timer, timespec64_to_ktime(*rqtp), slack);\r\nret = do_nanosleep(&t, mode);\r\nif (ret != -ERESTART_RESTARTBLOCK)\r\ngoto out;\r\nif (mode == HRTIMER_MODE_ABS) {\r\nret = -ERESTARTNOHAND;\r\ngoto out;\r\n}\r\nrestart = &current->restart_block;\r\nrestart->fn = hrtimer_nanosleep_restart;\r\nrestart->nanosleep.clockid = t.timer.base->clockid;\r\nrestart->nanosleep.expires = hrtimer_get_expires_tv64(&t.timer);\r\nout:\r\ndestroy_hrtimer_on_stack(&t.timer);\r\nreturn ret;\r\n}\r\nint hrtimers_prepare_cpu(unsigned int cpu)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu);\r\nint i;\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {\r\ncpu_base->clock_base[i].cpu_base = cpu_base;\r\ntimerqueue_init_head(&cpu_base->clock_base[i].active);\r\n}\r\ncpu_base->cpu = cpu;\r\nhrtimer_init_hres(cpu_base);\r\nreturn 0;\r\n}\r\nstatic void migrate_hrtimer_list(struct hrtimer_clock_base *old_base,\r\nstruct hrtimer_clock_base *new_base)\r\n{\r\nstruct hrtimer *timer;\r\nstruct timerqueue_node *node;\r\nwhile ((node = timerqueue_getnext(&old_base->active))) {\r\ntimer = container_of(node, struct hrtimer, node);\r\nBUG_ON(hrtimer_callback_running(timer));\r\ndebug_deactivate(timer);\r\n__remove_hrtimer(timer, old_base, HRTIMER_STATE_ENQUEUED, 0);\r\ntimer->base = new_base;\r\nenqueue_hrtimer(timer, new_base);\r\n}\r\n}\r\nint hrtimers_dead_cpu(unsigned int scpu)\r\n{\r\nstruct hrtimer_cpu_base *old_base, *new_base;\r\nint i;\r\nBUG_ON(cpu_online(scpu));\r\ntick_cancel_sched_timer(scpu);\r\nlocal_irq_disable();\r\nold_base = &per_cpu(hrtimer_bases, scpu);\r\nnew_base = this_cpu_ptr(&hrtimer_bases);\r\nraw_spin_lock(&new_base->lock);\r\nraw_spin_lock_nested(&old_base->lock, SINGLE_DEPTH_NESTING);\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {\r\nmigrate_hrtimer_list(&old_base->clock_base[i],\r\n&new_base->clock_base[i]);\r\n}\r\nraw_spin_unlock(&old_base->lock);\r\nraw_spin_unlock(&new_base->lock);\r\n__hrtimer_peek_ahead_timers();\r\nlocal_irq_enable();\r\nreturn 0;\r\n}\r\nvoid __init hrtimers_init(void)\r\n{\r\nhrtimers_prepare_cpu(smp_processor_id());\r\n}\r\nint __sched\r\nschedule_hrtimeout_range_clock(ktime_t *expires, u64 delta,\r\nconst enum hrtimer_mode mode, int clock)\r\n{\r\nstruct hrtimer_sleeper t;\r\nif (expires && *expires == 0) {\r\n__set_current_state(TASK_RUNNING);\r\nreturn 0;\r\n}\r\nif (!expires) {\r\nschedule();\r\nreturn -EINTR;\r\n}\r\nhrtimer_init_on_stack(&t.timer, clock, mode);\r\nhrtimer_set_expires_range_ns(&t.timer, *expires, delta);\r\nhrtimer_init_sleeper(&t, current);\r\nhrtimer_start_expires(&t.timer, mode);\r\nif (likely(t.task))\r\nschedule();\r\nhrtimer_cancel(&t.timer);\r\ndestroy_hrtimer_on_stack(&t.timer);\r\n__set_current_state(TASK_RUNNING);\r\nreturn !t.task ? 0 : -EINTR;\r\n}\r\nint __sched schedule_hrtimeout_range(ktime_t *expires, u64 delta,\r\nconst enum hrtimer_mode mode)\r\n{\r\nreturn schedule_hrtimeout_range_clock(expires, delta, mode,\r\nCLOCK_MONOTONIC);\r\n}\r\nint __sched schedule_hrtimeout(ktime_t *expires,\r\nconst enum hrtimer_mode mode)\r\n{\r\nreturn schedule_hrtimeout_range(expires, 0, mode);\r\n}
