static inline unsigned long idx_to_pfn(struct xenvif_queue *queue,\r\nu16 idx)\r\n{\r\nreturn page_to_pfn(queue->mmap_pages[idx]);\r\n}\r\nstatic inline unsigned long idx_to_kaddr(struct xenvif_queue *queue,\r\nu16 idx)\r\n{\r\nreturn (unsigned long)pfn_to_kaddr(idx_to_pfn(queue, idx));\r\n}\r\nstatic inline struct xenvif_queue *ubuf_to_queue(const struct ubuf_info *ubuf)\r\n{\r\nu16 pending_idx = ubuf->desc;\r\nstruct pending_tx_info *temp =\r\ncontainer_of(ubuf, struct pending_tx_info, callback_struct);\r\nreturn container_of(temp - pending_idx,\r\nstruct xenvif_queue,\r\npending_tx_info[0]);\r\n}\r\nstatic u16 frag_get_pending_idx(skb_frag_t *frag)\r\n{\r\nreturn (u16)frag->page_offset;\r\n}\r\nstatic void frag_set_pending_idx(skb_frag_t *frag, u16 pending_idx)\r\n{\r\nfrag->page_offset = pending_idx;\r\n}\r\nstatic inline pending_ring_idx_t pending_index(unsigned i)\r\n{\r\nreturn i & (MAX_PENDING_REQS-1);\r\n}\r\nvoid xenvif_kick_thread(struct xenvif_queue *queue)\r\n{\r\nwake_up(&queue->wq);\r\n}\r\nvoid xenvif_napi_schedule_or_enable_events(struct xenvif_queue *queue)\r\n{\r\nint more_to_do;\r\nRING_FINAL_CHECK_FOR_REQUESTS(&queue->tx, more_to_do);\r\nif (more_to_do)\r\nnapi_schedule(&queue->napi);\r\n}\r\nstatic void tx_add_credit(struct xenvif_queue *queue)\r\n{\r\nunsigned long max_burst, max_credit;\r\nmax_burst = max(131072UL, queue->credit_bytes);\r\nmax_credit = queue->remaining_credit + queue->credit_bytes;\r\nif (max_credit < queue->remaining_credit)\r\nmax_credit = ULONG_MAX;\r\nqueue->remaining_credit = min(max_credit, max_burst);\r\nqueue->rate_limited = false;\r\n}\r\nvoid xenvif_tx_credit_callback(unsigned long data)\r\n{\r\nstruct xenvif_queue *queue = (struct xenvif_queue *)data;\r\ntx_add_credit(queue);\r\nxenvif_napi_schedule_or_enable_events(queue);\r\n}\r\nstatic void xenvif_tx_err(struct xenvif_queue *queue,\r\nstruct xen_netif_tx_request *txp,\r\nunsigned int extra_count, RING_IDX end)\r\n{\r\nRING_IDX cons = queue->tx.req_cons;\r\nunsigned long flags;\r\ndo {\r\nspin_lock_irqsave(&queue->response_lock, flags);\r\nmake_tx_response(queue, txp, extra_count, XEN_NETIF_RSP_ERROR);\r\npush_tx_responses(queue);\r\nspin_unlock_irqrestore(&queue->response_lock, flags);\r\nif (cons == end)\r\nbreak;\r\nRING_COPY_REQUEST(&queue->tx, cons++, txp);\r\nextra_count = 0;\r\n} while (1);\r\nqueue->tx.req_cons = cons;\r\n}\r\nstatic void xenvif_fatal_tx_err(struct xenvif *vif)\r\n{\r\nnetdev_err(vif->dev, "fatal error; disabling device\n");\r\nvif->disabled = true;\r\nif (vif->num_queues)\r\nxenvif_kick_thread(&vif->queues[0]);\r\n}\r\nstatic int xenvif_count_requests(struct xenvif_queue *queue,\r\nstruct xen_netif_tx_request *first,\r\nunsigned int extra_count,\r\nstruct xen_netif_tx_request *txp,\r\nint work_to_do)\r\n{\r\nRING_IDX cons = queue->tx.req_cons;\r\nint slots = 0;\r\nint drop_err = 0;\r\nint more_data;\r\nif (!(first->flags & XEN_NETTXF_more_data))\r\nreturn 0;\r\ndo {\r\nstruct xen_netif_tx_request dropped_tx = { 0 };\r\nif (slots >= work_to_do) {\r\nnetdev_err(queue->vif->dev,\r\n"Asked for %d slots but exceeds this limit\n",\r\nwork_to_do);\r\nxenvif_fatal_tx_err(queue->vif);\r\nreturn -ENODATA;\r\n}\r\nif (unlikely(slots >= fatal_skb_slots)) {\r\nnetdev_err(queue->vif->dev,\r\n"Malicious frontend using %d slots, threshold %u\n",\r\nslots, fatal_skb_slots);\r\nxenvif_fatal_tx_err(queue->vif);\r\nreturn -E2BIG;\r\n}\r\nif (!drop_err && slots >= XEN_NETBK_LEGACY_SLOTS_MAX) {\r\nif (net_ratelimit())\r\nnetdev_dbg(queue->vif->dev,\r\n"Too many slots (%d) exceeding limit (%d), dropping packet\n",\r\nslots, XEN_NETBK_LEGACY_SLOTS_MAX);\r\ndrop_err = -E2BIG;\r\n}\r\nif (drop_err)\r\ntxp = &dropped_tx;\r\nRING_COPY_REQUEST(&queue->tx, cons + slots, txp);\r\nif (!drop_err && txp->size > first->size) {\r\nif (net_ratelimit())\r\nnetdev_dbg(queue->vif->dev,\r\n"Invalid tx request, slot size %u > remaining size %u\n",\r\ntxp->size, first->size);\r\ndrop_err = -EIO;\r\n}\r\nfirst->size -= txp->size;\r\nslots++;\r\nif (unlikely((txp->offset + txp->size) > XEN_PAGE_SIZE)) {\r\nnetdev_err(queue->vif->dev, "Cross page boundary, txp->offset: %u, size: %u\n",\r\ntxp->offset, txp->size);\r\nxenvif_fatal_tx_err(queue->vif);\r\nreturn -EINVAL;\r\n}\r\nmore_data = txp->flags & XEN_NETTXF_more_data;\r\nif (!drop_err)\r\ntxp++;\r\n} while (more_data);\r\nif (drop_err) {\r\nxenvif_tx_err(queue, first, extra_count, cons + slots);\r\nreturn drop_err;\r\n}\r\nreturn slots;\r\n}\r\nstatic inline void xenvif_tx_create_map_op(struct xenvif_queue *queue,\r\nu16 pending_idx,\r\nstruct xen_netif_tx_request *txp,\r\nunsigned int extra_count,\r\nstruct gnttab_map_grant_ref *mop)\r\n{\r\nqueue->pages_to_map[mop-queue->tx_map_ops] = queue->mmap_pages[pending_idx];\r\ngnttab_set_map_op(mop, idx_to_kaddr(queue, pending_idx),\r\nGNTMAP_host_map | GNTMAP_readonly,\r\ntxp->gref, queue->vif->domid);\r\nmemcpy(&queue->pending_tx_info[pending_idx].req, txp,\r\nsizeof(*txp));\r\nqueue->pending_tx_info[pending_idx].extra_count = extra_count;\r\n}\r\nstatic inline struct sk_buff *xenvif_alloc_skb(unsigned int size)\r\n{\r\nstruct sk_buff *skb =\r\nalloc_skb(size + NET_SKB_PAD + NET_IP_ALIGN,\r\nGFP_ATOMIC | __GFP_NOWARN);\r\nif (unlikely(skb == NULL))\r\nreturn NULL;\r\nskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\r\nskb_shinfo(skb)->destructor_arg = NULL;\r\nreturn skb;\r\n}\r\nstatic struct gnttab_map_grant_ref *xenvif_get_requests(struct xenvif_queue *queue,\r\nstruct sk_buff *skb,\r\nstruct xen_netif_tx_request *txp,\r\nstruct gnttab_map_grant_ref *gop,\r\nunsigned int frag_overflow,\r\nstruct sk_buff *nskb)\r\n{\r\nstruct skb_shared_info *shinfo = skb_shinfo(skb);\r\nskb_frag_t *frags = shinfo->frags;\r\nu16 pending_idx = XENVIF_TX_CB(skb)->pending_idx;\r\nint start;\r\npending_ring_idx_t index;\r\nunsigned int nr_slots;\r\nnr_slots = shinfo->nr_frags;\r\nstart = (frag_get_pending_idx(&shinfo->frags[0]) == pending_idx);\r\nfor (shinfo->nr_frags = start; shinfo->nr_frags < nr_slots;\r\nshinfo->nr_frags++, txp++, gop++) {\r\nindex = pending_index(queue->pending_cons++);\r\npending_idx = queue->pending_ring[index];\r\nxenvif_tx_create_map_op(queue, pending_idx, txp, 0, gop);\r\nfrag_set_pending_idx(&frags[shinfo->nr_frags], pending_idx);\r\n}\r\nif (frag_overflow) {\r\nshinfo = skb_shinfo(nskb);\r\nfrags = shinfo->frags;\r\nfor (shinfo->nr_frags = 0; shinfo->nr_frags < frag_overflow;\r\nshinfo->nr_frags++, txp++, gop++) {\r\nindex = pending_index(queue->pending_cons++);\r\npending_idx = queue->pending_ring[index];\r\nxenvif_tx_create_map_op(queue, pending_idx, txp, 0,\r\ngop);\r\nfrag_set_pending_idx(&frags[shinfo->nr_frags],\r\npending_idx);\r\n}\r\nskb_shinfo(skb)->frag_list = nskb;\r\n}\r\nreturn gop;\r\n}\r\nstatic inline void xenvif_grant_handle_set(struct xenvif_queue *queue,\r\nu16 pending_idx,\r\ngrant_handle_t handle)\r\n{\r\nif (unlikely(queue->grant_tx_handle[pending_idx] !=\r\nNETBACK_INVALID_HANDLE)) {\r\nnetdev_err(queue->vif->dev,\r\n"Trying to overwrite active handle! pending_idx: 0x%x\n",\r\npending_idx);\r\nBUG();\r\n}\r\nqueue->grant_tx_handle[pending_idx] = handle;\r\n}\r\nstatic inline void xenvif_grant_handle_reset(struct xenvif_queue *queue,\r\nu16 pending_idx)\r\n{\r\nif (unlikely(queue->grant_tx_handle[pending_idx] ==\r\nNETBACK_INVALID_HANDLE)) {\r\nnetdev_err(queue->vif->dev,\r\n"Trying to unmap invalid handle! pending_idx: 0x%x\n",\r\npending_idx);\r\nBUG();\r\n}\r\nqueue->grant_tx_handle[pending_idx] = NETBACK_INVALID_HANDLE;\r\n}\r\nstatic int xenvif_tx_check_gop(struct xenvif_queue *queue,\r\nstruct sk_buff *skb,\r\nstruct gnttab_map_grant_ref **gopp_map,\r\nstruct gnttab_copy **gopp_copy)\r\n{\r\nstruct gnttab_map_grant_ref *gop_map = *gopp_map;\r\nu16 pending_idx = XENVIF_TX_CB(skb)->pending_idx;\r\nstruct skb_shared_info *shinfo = skb_shinfo(skb);\r\nstruct skb_shared_info *first_shinfo = NULL;\r\nint nr_frags = shinfo->nr_frags;\r\nconst bool sharedslot = nr_frags &&\r\nfrag_get_pending_idx(&shinfo->frags[0]) == pending_idx;\r\nint i, err;\r\nerr = (*gopp_copy)->status;\r\nif (unlikely(err)) {\r\nif (net_ratelimit())\r\nnetdev_dbg(queue->vif->dev,\r\n"Grant copy of header failed! status: %d pending_idx: %u ref: %u\n",\r\n(*gopp_copy)->status,\r\npending_idx,\r\n(*gopp_copy)->source.u.ref);\r\nif (!sharedslot)\r\nxenvif_idx_release(queue, pending_idx,\r\nXEN_NETIF_RSP_ERROR);\r\n}\r\n(*gopp_copy)++;\r\ncheck_frags:\r\nfor (i = 0; i < nr_frags; i++, gop_map++) {\r\nint j, newerr;\r\npending_idx = frag_get_pending_idx(&shinfo->frags[i]);\r\nnewerr = gop_map->status;\r\nif (likely(!newerr)) {\r\nxenvif_grant_handle_set(queue,\r\npending_idx,\r\ngop_map->handle);\r\nif (unlikely(err)) {\r\nxenvif_idx_unmap(queue, pending_idx);\r\nif (i == 0 && sharedslot)\r\nxenvif_idx_release(queue, pending_idx,\r\nXEN_NETIF_RSP_ERROR);\r\nelse\r\nxenvif_idx_release(queue, pending_idx,\r\nXEN_NETIF_RSP_OKAY);\r\n}\r\ncontinue;\r\n}\r\nif (net_ratelimit())\r\nnetdev_dbg(queue->vif->dev,\r\n"Grant map of %d. frag failed! status: %d pending_idx: %u ref: %u\n",\r\ni,\r\ngop_map->status,\r\npending_idx,\r\ngop_map->ref);\r\nxenvif_idx_release(queue, pending_idx, XEN_NETIF_RSP_ERROR);\r\nif (err)\r\ncontinue;\r\nif (!sharedslot)\r\nxenvif_idx_release(queue,\r\nXENVIF_TX_CB(skb)->pending_idx,\r\nXEN_NETIF_RSP_OKAY);\r\nfor (j = 0; j < i; j++) {\r\npending_idx = frag_get_pending_idx(&shinfo->frags[j]);\r\nxenvif_idx_unmap(queue, pending_idx);\r\nxenvif_idx_release(queue, pending_idx,\r\nXEN_NETIF_RSP_OKAY);\r\n}\r\nif (first_shinfo) {\r\nfor (j = 0; j < first_shinfo->nr_frags; j++) {\r\npending_idx = frag_get_pending_idx(&first_shinfo->frags[j]);\r\nxenvif_idx_unmap(queue, pending_idx);\r\nxenvif_idx_release(queue, pending_idx,\r\nXEN_NETIF_RSP_OKAY);\r\n}\r\n}\r\nerr = newerr;\r\n}\r\nif (skb_has_frag_list(skb) && !first_shinfo) {\r\nfirst_shinfo = skb_shinfo(skb);\r\nshinfo = skb_shinfo(skb_shinfo(skb)->frag_list);\r\nnr_frags = shinfo->nr_frags;\r\ngoto check_frags;\r\n}\r\n*gopp_map = gop_map;\r\nreturn err;\r\n}\r\nstatic void xenvif_fill_frags(struct xenvif_queue *queue, struct sk_buff *skb)\r\n{\r\nstruct skb_shared_info *shinfo = skb_shinfo(skb);\r\nint nr_frags = shinfo->nr_frags;\r\nint i;\r\nu16 prev_pending_idx = INVALID_PENDING_IDX;\r\nfor (i = 0; i < nr_frags; i++) {\r\nskb_frag_t *frag = shinfo->frags + i;\r\nstruct xen_netif_tx_request *txp;\r\nstruct page *page;\r\nu16 pending_idx;\r\npending_idx = frag_get_pending_idx(frag);\r\nif (prev_pending_idx == INVALID_PENDING_IDX)\r\nskb_shinfo(skb)->destructor_arg =\r\n&callback_param(queue, pending_idx);\r\nelse\r\ncallback_param(queue, prev_pending_idx).ctx =\r\n&callback_param(queue, pending_idx);\r\ncallback_param(queue, pending_idx).ctx = NULL;\r\nprev_pending_idx = pending_idx;\r\ntxp = &queue->pending_tx_info[pending_idx].req;\r\npage = virt_to_page(idx_to_kaddr(queue, pending_idx));\r\n__skb_fill_page_desc(skb, i, page, txp->offset, txp->size);\r\nskb->len += txp->size;\r\nskb->data_len += txp->size;\r\nskb->truesize += txp->size;\r\nget_page(queue->mmap_pages[pending_idx]);\r\n}\r\n}\r\nstatic int xenvif_get_extras(struct xenvif_queue *queue,\r\nstruct xen_netif_extra_info *extras,\r\nunsigned int *extra_count,\r\nint work_to_do)\r\n{\r\nstruct xen_netif_extra_info extra;\r\nRING_IDX cons = queue->tx.req_cons;\r\ndo {\r\nif (unlikely(work_to_do-- <= 0)) {\r\nnetdev_err(queue->vif->dev, "Missing extra info\n");\r\nxenvif_fatal_tx_err(queue->vif);\r\nreturn -EBADR;\r\n}\r\nRING_COPY_REQUEST(&queue->tx, cons, &extra);\r\nqueue->tx.req_cons = ++cons;\r\n(*extra_count)++;\r\nif (unlikely(!extra.type ||\r\nextra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {\r\nnetdev_err(queue->vif->dev,\r\n"Invalid extra type: %d\n", extra.type);\r\nxenvif_fatal_tx_err(queue->vif);\r\nreturn -EINVAL;\r\n}\r\nmemcpy(&extras[extra.type - 1], &extra, sizeof(extra));\r\n} while (extra.flags & XEN_NETIF_EXTRA_FLAG_MORE);\r\nreturn work_to_do;\r\n}\r\nstatic int xenvif_set_skb_gso(struct xenvif *vif,\r\nstruct sk_buff *skb,\r\nstruct xen_netif_extra_info *gso)\r\n{\r\nif (!gso->u.gso.size) {\r\nnetdev_err(vif->dev, "GSO size must not be zero.\n");\r\nxenvif_fatal_tx_err(vif);\r\nreturn -EINVAL;\r\n}\r\nswitch (gso->u.gso.type) {\r\ncase XEN_NETIF_GSO_TYPE_TCPV4:\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\r\nbreak;\r\ncase XEN_NETIF_GSO_TYPE_TCPV6:\r\nskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\r\nbreak;\r\ndefault:\r\nnetdev_err(vif->dev, "Bad GSO type %d.\n", gso->u.gso.type);\r\nxenvif_fatal_tx_err(vif);\r\nreturn -EINVAL;\r\n}\r\nskb_shinfo(skb)->gso_size = gso->u.gso.size;\r\nreturn 0;\r\n}\r\nstatic int checksum_setup(struct xenvif_queue *queue, struct sk_buff *skb)\r\n{\r\nbool recalculate_partial_csum = false;\r\nif (skb->ip_summed != CHECKSUM_PARTIAL && skb_is_gso(skb)) {\r\nqueue->stats.rx_gso_checksum_fixup++;\r\nskb->ip_summed = CHECKSUM_PARTIAL;\r\nrecalculate_partial_csum = true;\r\n}\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nreturn 0;\r\nreturn skb_checksum_setup(skb, recalculate_partial_csum);\r\n}\r\nstatic bool tx_credit_exceeded(struct xenvif_queue *queue, unsigned size)\r\n{\r\nu64 now = get_jiffies_64();\r\nu64 next_credit = queue->credit_window_start +\r\nmsecs_to_jiffies(queue->credit_usec / 1000);\r\nif (timer_pending(&queue->credit_timeout)) {\r\nqueue->rate_limited = true;\r\nreturn true;\r\n}\r\nif (time_after_eq64(now, next_credit)) {\r\nqueue->credit_window_start = now;\r\ntx_add_credit(queue);\r\n}\r\nif (size > queue->remaining_credit) {\r\nqueue->credit_timeout.data =\r\n(unsigned long)queue;\r\nmod_timer(&queue->credit_timeout,\r\nnext_credit);\r\nqueue->credit_window_start = next_credit;\r\nqueue->rate_limited = true;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int xenvif_mcast_add(struct xenvif *vif, const u8 *addr)\r\n{\r\nstruct xenvif_mcast_addr *mcast;\r\nif (vif->fe_mcast_count == XEN_NETBK_MCAST_MAX) {\r\nif (net_ratelimit())\r\nnetdev_err(vif->dev,\r\n"Too many multicast addresses\n");\r\nreturn -ENOSPC;\r\n}\r\nmcast = kzalloc(sizeof(*mcast), GFP_ATOMIC);\r\nif (!mcast)\r\nreturn -ENOMEM;\r\nether_addr_copy(mcast->addr, addr);\r\nlist_add_tail_rcu(&mcast->entry, &vif->fe_mcast_addr);\r\nvif->fe_mcast_count++;\r\nreturn 0;\r\n}\r\nstatic void xenvif_mcast_del(struct xenvif *vif, const u8 *addr)\r\n{\r\nstruct xenvif_mcast_addr *mcast;\r\nlist_for_each_entry_rcu(mcast, &vif->fe_mcast_addr, entry) {\r\nif (ether_addr_equal(addr, mcast->addr)) {\r\n--vif->fe_mcast_count;\r\nlist_del_rcu(&mcast->entry);\r\nkfree_rcu(mcast, rcu);\r\nbreak;\r\n}\r\n}\r\n}\r\nbool xenvif_mcast_match(struct xenvif *vif, const u8 *addr)\r\n{\r\nstruct xenvif_mcast_addr *mcast;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(mcast, &vif->fe_mcast_addr, entry) {\r\nif (ether_addr_equal(addr, mcast->addr)) {\r\nrcu_read_unlock();\r\nreturn true;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn false;\r\n}\r\nvoid xenvif_mcast_addr_list_free(struct xenvif *vif)\r\n{\r\nwhile (!list_empty(&vif->fe_mcast_addr)) {\r\nstruct xenvif_mcast_addr *mcast;\r\nmcast = list_first_entry(&vif->fe_mcast_addr,\r\nstruct xenvif_mcast_addr,\r\nentry);\r\n--vif->fe_mcast_count;\r\nlist_del(&mcast->entry);\r\nkfree(mcast);\r\n}\r\n}\r\nstatic void xenvif_tx_build_gops(struct xenvif_queue *queue,\r\nint budget,\r\nunsigned *copy_ops,\r\nunsigned *map_ops)\r\n{\r\nstruct gnttab_map_grant_ref *gop = queue->tx_map_ops;\r\nstruct sk_buff *skb, *nskb;\r\nint ret;\r\nunsigned int frag_overflow;\r\nwhile (skb_queue_len(&queue->tx_queue) < budget) {\r\nstruct xen_netif_tx_request txreq;\r\nstruct xen_netif_tx_request txfrags[XEN_NETBK_LEGACY_SLOTS_MAX];\r\nstruct xen_netif_extra_info extras[XEN_NETIF_EXTRA_TYPE_MAX-1];\r\nunsigned int extra_count;\r\nu16 pending_idx;\r\nRING_IDX idx;\r\nint work_to_do;\r\nunsigned int data_len;\r\npending_ring_idx_t index;\r\nif (queue->tx.sring->req_prod - queue->tx.req_cons >\r\nXEN_NETIF_TX_RING_SIZE) {\r\nnetdev_err(queue->vif->dev,\r\n"Impossible number of requests. "\r\n"req_prod %d, req_cons %d, size %ld\n",\r\nqueue->tx.sring->req_prod, queue->tx.req_cons,\r\nXEN_NETIF_TX_RING_SIZE);\r\nxenvif_fatal_tx_err(queue->vif);\r\nbreak;\r\n}\r\nwork_to_do = RING_HAS_UNCONSUMED_REQUESTS(&queue->tx);\r\nif (!work_to_do)\r\nbreak;\r\nidx = queue->tx.req_cons;\r\nrmb();\r\nRING_COPY_REQUEST(&queue->tx, idx, &txreq);\r\nif (txreq.size > queue->remaining_credit &&\r\ntx_credit_exceeded(queue, txreq.size))\r\nbreak;\r\nqueue->remaining_credit -= txreq.size;\r\nwork_to_do--;\r\nqueue->tx.req_cons = ++idx;\r\nmemset(extras, 0, sizeof(extras));\r\nextra_count = 0;\r\nif (txreq.flags & XEN_NETTXF_extra_info) {\r\nwork_to_do = xenvif_get_extras(queue, extras,\r\n&extra_count,\r\nwork_to_do);\r\nidx = queue->tx.req_cons;\r\nif (unlikely(work_to_do < 0))\r\nbreak;\r\n}\r\nif (extras[XEN_NETIF_EXTRA_TYPE_MCAST_ADD - 1].type) {\r\nstruct xen_netif_extra_info *extra;\r\nextra = &extras[XEN_NETIF_EXTRA_TYPE_MCAST_ADD - 1];\r\nret = xenvif_mcast_add(queue->vif, extra->u.mcast.addr);\r\nmake_tx_response(queue, &txreq, extra_count,\r\n(ret == 0) ?\r\nXEN_NETIF_RSP_OKAY :\r\nXEN_NETIF_RSP_ERROR);\r\npush_tx_responses(queue);\r\ncontinue;\r\n}\r\nif (extras[XEN_NETIF_EXTRA_TYPE_MCAST_DEL - 1].type) {\r\nstruct xen_netif_extra_info *extra;\r\nextra = &extras[XEN_NETIF_EXTRA_TYPE_MCAST_DEL - 1];\r\nxenvif_mcast_del(queue->vif, extra->u.mcast.addr);\r\nmake_tx_response(queue, &txreq, extra_count,\r\nXEN_NETIF_RSP_OKAY);\r\npush_tx_responses(queue);\r\ncontinue;\r\n}\r\nret = xenvif_count_requests(queue, &txreq, extra_count,\r\ntxfrags, work_to_do);\r\nif (unlikely(ret < 0))\r\nbreak;\r\nidx += ret;\r\nif (unlikely(txreq.size < ETH_HLEN)) {\r\nnetdev_dbg(queue->vif->dev,\r\n"Bad packet size: %d\n", txreq.size);\r\nxenvif_tx_err(queue, &txreq, extra_count, idx);\r\nbreak;\r\n}\r\nif (unlikely((txreq.offset + txreq.size) > XEN_PAGE_SIZE)) {\r\nnetdev_err(queue->vif->dev,\r\n"txreq.offset: %u, size: %u, end: %lu\n",\r\ntxreq.offset, txreq.size,\r\n(unsigned long)(txreq.offset&~XEN_PAGE_MASK) + txreq.size);\r\nxenvif_fatal_tx_err(queue->vif);\r\nbreak;\r\n}\r\nindex = pending_index(queue->pending_cons);\r\npending_idx = queue->pending_ring[index];\r\ndata_len = (txreq.size > XEN_NETBACK_TX_COPY_LEN &&\r\nret < XEN_NETBK_LEGACY_SLOTS_MAX) ?\r\nXEN_NETBACK_TX_COPY_LEN : txreq.size;\r\nskb = xenvif_alloc_skb(data_len);\r\nif (unlikely(skb == NULL)) {\r\nnetdev_dbg(queue->vif->dev,\r\n"Can't allocate a skb in start_xmit.\n");\r\nxenvif_tx_err(queue, &txreq, extra_count, idx);\r\nbreak;\r\n}\r\nskb_shinfo(skb)->nr_frags = ret;\r\nif (data_len < txreq.size)\r\nskb_shinfo(skb)->nr_frags++;\r\nfrag_overflow = 0;\r\nnskb = NULL;\r\nif (skb_shinfo(skb)->nr_frags > MAX_SKB_FRAGS) {\r\nfrag_overflow = skb_shinfo(skb)->nr_frags - MAX_SKB_FRAGS;\r\nBUG_ON(frag_overflow > MAX_SKB_FRAGS);\r\nskb_shinfo(skb)->nr_frags = MAX_SKB_FRAGS;\r\nnskb = xenvif_alloc_skb(0);\r\nif (unlikely(nskb == NULL)) {\r\nkfree_skb(skb);\r\nxenvif_tx_err(queue, &txreq, extra_count, idx);\r\nif (net_ratelimit())\r\nnetdev_err(queue->vif->dev,\r\n"Can't allocate the frag_list skb.\n");\r\nbreak;\r\n}\r\n}\r\nif (extras[XEN_NETIF_EXTRA_TYPE_GSO - 1].type) {\r\nstruct xen_netif_extra_info *gso;\r\ngso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\r\nif (xenvif_set_skb_gso(queue->vif, skb, gso)) {\r\nkfree_skb(skb);\r\nkfree_skb(nskb);\r\nbreak;\r\n}\r\n}\r\nif (extras[XEN_NETIF_EXTRA_TYPE_HASH - 1].type) {\r\nstruct xen_netif_extra_info *extra;\r\nenum pkt_hash_types type = PKT_HASH_TYPE_NONE;\r\nextra = &extras[XEN_NETIF_EXTRA_TYPE_HASH - 1];\r\nswitch (extra->u.hash.type) {\r\ncase _XEN_NETIF_CTRL_HASH_TYPE_IPV4:\r\ncase _XEN_NETIF_CTRL_HASH_TYPE_IPV6:\r\ntype = PKT_HASH_TYPE_L3;\r\nbreak;\r\ncase _XEN_NETIF_CTRL_HASH_TYPE_IPV4_TCP:\r\ncase _XEN_NETIF_CTRL_HASH_TYPE_IPV6_TCP:\r\ntype = PKT_HASH_TYPE_L4;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (type != PKT_HASH_TYPE_NONE)\r\nskb_set_hash(skb,\r\n*(u32 *)extra->u.hash.value,\r\ntype);\r\n}\r\nXENVIF_TX_CB(skb)->pending_idx = pending_idx;\r\n__skb_put(skb, data_len);\r\nqueue->tx_copy_ops[*copy_ops].source.u.ref = txreq.gref;\r\nqueue->tx_copy_ops[*copy_ops].source.domid = queue->vif->domid;\r\nqueue->tx_copy_ops[*copy_ops].source.offset = txreq.offset;\r\nqueue->tx_copy_ops[*copy_ops].dest.u.gmfn =\r\nvirt_to_gfn(skb->data);\r\nqueue->tx_copy_ops[*copy_ops].dest.domid = DOMID_SELF;\r\nqueue->tx_copy_ops[*copy_ops].dest.offset =\r\noffset_in_page(skb->data) & ~XEN_PAGE_MASK;\r\nqueue->tx_copy_ops[*copy_ops].len = data_len;\r\nqueue->tx_copy_ops[*copy_ops].flags = GNTCOPY_source_gref;\r\n(*copy_ops)++;\r\nif (data_len < txreq.size) {\r\nfrag_set_pending_idx(&skb_shinfo(skb)->frags[0],\r\npending_idx);\r\nxenvif_tx_create_map_op(queue, pending_idx, &txreq,\r\nextra_count, gop);\r\ngop++;\r\n} else {\r\nfrag_set_pending_idx(&skb_shinfo(skb)->frags[0],\r\nINVALID_PENDING_IDX);\r\nmemcpy(&queue->pending_tx_info[pending_idx].req,\r\n&txreq, sizeof(txreq));\r\nqueue->pending_tx_info[pending_idx].extra_count =\r\nextra_count;\r\n}\r\nqueue->pending_cons++;\r\ngop = xenvif_get_requests(queue, skb, txfrags, gop,\r\nfrag_overflow, nskb);\r\n__skb_queue_tail(&queue->tx_queue, skb);\r\nqueue->tx.req_cons = idx;\r\nif (((gop-queue->tx_map_ops) >= ARRAY_SIZE(queue->tx_map_ops)) ||\r\n(*copy_ops >= ARRAY_SIZE(queue->tx_copy_ops)))\r\nbreak;\r\n}\r\n(*map_ops) = gop - queue->tx_map_ops;\r\nreturn;\r\n}\r\nstatic int xenvif_handle_frag_list(struct xenvif_queue *queue, struct sk_buff *skb)\r\n{\r\nunsigned int offset = skb_headlen(skb);\r\nskb_frag_t frags[MAX_SKB_FRAGS];\r\nint i, f;\r\nstruct ubuf_info *uarg;\r\nstruct sk_buff *nskb = skb_shinfo(skb)->frag_list;\r\nqueue->stats.tx_zerocopy_sent += 2;\r\nqueue->stats.tx_frag_overflow++;\r\nxenvif_fill_frags(queue, nskb);\r\nskb->truesize -= skb->data_len;\r\nskb->len += nskb->len;\r\nskb->data_len += nskb->len;\r\nfor (i = 0; offset < skb->len; i++) {\r\nstruct page *page;\r\nunsigned int len;\r\nBUG_ON(i >= MAX_SKB_FRAGS);\r\npage = alloc_page(GFP_ATOMIC);\r\nif (!page) {\r\nint j;\r\nskb->truesize += skb->data_len;\r\nfor (j = 0; j < i; j++)\r\nput_page(frags[j].page.p);\r\nreturn -ENOMEM;\r\n}\r\nif (offset + PAGE_SIZE < skb->len)\r\nlen = PAGE_SIZE;\r\nelse\r\nlen = skb->len - offset;\r\nif (skb_copy_bits(skb, offset, page_address(page), len))\r\nBUG();\r\noffset += len;\r\nfrags[i].page.p = page;\r\nfrags[i].page_offset = 0;\r\nskb_frag_size_set(&frags[i], len);\r\n}\r\nskb_frag_list_init(skb);\r\nxenvif_skb_zerocopy_prepare(queue, nskb);\r\nkfree_skb(nskb);\r\nfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\r\nskb_frag_unref(skb, f);\r\nuarg = skb_shinfo(skb)->destructor_arg;\r\natomic_inc(&queue->inflight_packets);\r\nuarg->callback(uarg, true);\r\nskb_shinfo(skb)->destructor_arg = NULL;\r\nmemcpy(skb_shinfo(skb)->frags, frags, i * sizeof(skb_frag_t));\r\nskb_shinfo(skb)->nr_frags = i;\r\nskb->truesize += i * PAGE_SIZE;\r\nreturn 0;\r\n}\r\nstatic int xenvif_tx_submit(struct xenvif_queue *queue)\r\n{\r\nstruct gnttab_map_grant_ref *gop_map = queue->tx_map_ops;\r\nstruct gnttab_copy *gop_copy = queue->tx_copy_ops;\r\nstruct sk_buff *skb;\r\nint work_done = 0;\r\nwhile ((skb = __skb_dequeue(&queue->tx_queue)) != NULL) {\r\nstruct xen_netif_tx_request *txp;\r\nu16 pending_idx;\r\nunsigned data_len;\r\npending_idx = XENVIF_TX_CB(skb)->pending_idx;\r\ntxp = &queue->pending_tx_info[pending_idx].req;\r\nif (unlikely(xenvif_tx_check_gop(queue, skb, &gop_map, &gop_copy))) {\r\nskb_shinfo(skb)->nr_frags = 0;\r\nif (skb_has_frag_list(skb)) {\r\nstruct sk_buff *nskb =\r\nskb_shinfo(skb)->frag_list;\r\nskb_shinfo(nskb)->nr_frags = 0;\r\n}\r\nkfree_skb(skb);\r\ncontinue;\r\n}\r\ndata_len = skb->len;\r\ncallback_param(queue, pending_idx).ctx = NULL;\r\nif (data_len < txp->size) {\r\ntxp->offset += data_len;\r\ntxp->size -= data_len;\r\n} else {\r\nxenvif_idx_release(queue, pending_idx,\r\nXEN_NETIF_RSP_OKAY);\r\n}\r\nif (txp->flags & XEN_NETTXF_csum_blank)\r\nskb->ip_summed = CHECKSUM_PARTIAL;\r\nelse if (txp->flags & XEN_NETTXF_data_validated)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nxenvif_fill_frags(queue, skb);\r\nif (unlikely(skb_has_frag_list(skb))) {\r\nif (xenvif_handle_frag_list(queue, skb)) {\r\nif (net_ratelimit())\r\nnetdev_err(queue->vif->dev,\r\n"Not enough memory to consolidate frag_list!\n");\r\nxenvif_skb_zerocopy_prepare(queue, skb);\r\nkfree_skb(skb);\r\ncontinue;\r\n}\r\n}\r\nskb->dev = queue->vif->dev;\r\nskb->protocol = eth_type_trans(skb, skb->dev);\r\nskb_reset_network_header(skb);\r\nif (checksum_setup(queue, skb)) {\r\nnetdev_dbg(queue->vif->dev,\r\n"Can't setup checksum in net_tx_action\n");\r\nif (skb_shinfo(skb)->destructor_arg)\r\nxenvif_skb_zerocopy_prepare(queue, skb);\r\nkfree_skb(skb);\r\ncontinue;\r\n}\r\nskb_probe_transport_header(skb, 0);\r\nif (skb_is_gso(skb)) {\r\nint mss = skb_shinfo(skb)->gso_size;\r\nint hdrlen = skb_transport_header(skb) -\r\nskb_mac_header(skb) +\r\ntcp_hdrlen(skb);\r\nskb_shinfo(skb)->gso_segs =\r\nDIV_ROUND_UP(skb->len - hdrlen, mss);\r\n}\r\nqueue->stats.rx_bytes += skb->len;\r\nqueue->stats.rx_packets++;\r\nwork_done++;\r\nif (skb_shinfo(skb)->destructor_arg) {\r\nxenvif_skb_zerocopy_prepare(queue, skb);\r\nqueue->stats.tx_zerocopy_sent++;\r\n}\r\nnetif_receive_skb(skb);\r\n}\r\nreturn work_done;\r\n}\r\nvoid xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success)\r\n{\r\nunsigned long flags;\r\npending_ring_idx_t index;\r\nstruct xenvif_queue *queue = ubuf_to_queue(ubuf);\r\nspin_lock_irqsave(&queue->callback_lock, flags);\r\ndo {\r\nu16 pending_idx = ubuf->desc;\r\nubuf = (struct ubuf_info *) ubuf->ctx;\r\nBUG_ON(queue->dealloc_prod - queue->dealloc_cons >=\r\nMAX_PENDING_REQS);\r\nindex = pending_index(queue->dealloc_prod);\r\nqueue->dealloc_ring[index] = pending_idx;\r\nsmp_wmb();\r\nqueue->dealloc_prod++;\r\n} while (ubuf);\r\nspin_unlock_irqrestore(&queue->callback_lock, flags);\r\nif (likely(zerocopy_success))\r\nqueue->stats.tx_zerocopy_success++;\r\nelse\r\nqueue->stats.tx_zerocopy_fail++;\r\nxenvif_skb_zerocopy_complete(queue);\r\n}\r\nstatic inline void xenvif_tx_dealloc_action(struct xenvif_queue *queue)\r\n{\r\nstruct gnttab_unmap_grant_ref *gop;\r\npending_ring_idx_t dc, dp;\r\nu16 pending_idx, pending_idx_release[MAX_PENDING_REQS];\r\nunsigned int i = 0;\r\ndc = queue->dealloc_cons;\r\ngop = queue->tx_unmap_ops;\r\ndo {\r\ndp = queue->dealloc_prod;\r\nsmp_rmb();\r\nwhile (dc != dp) {\r\nBUG_ON(gop - queue->tx_unmap_ops >= MAX_PENDING_REQS);\r\npending_idx =\r\nqueue->dealloc_ring[pending_index(dc++)];\r\npending_idx_release[gop - queue->tx_unmap_ops] =\r\npending_idx;\r\nqueue->pages_to_unmap[gop - queue->tx_unmap_ops] =\r\nqueue->mmap_pages[pending_idx];\r\ngnttab_set_unmap_op(gop,\r\nidx_to_kaddr(queue, pending_idx),\r\nGNTMAP_host_map,\r\nqueue->grant_tx_handle[pending_idx]);\r\nxenvif_grant_handle_reset(queue, pending_idx);\r\n++gop;\r\n}\r\n} while (dp != queue->dealloc_prod);\r\nqueue->dealloc_cons = dc;\r\nif (gop - queue->tx_unmap_ops > 0) {\r\nint ret;\r\nret = gnttab_unmap_refs(queue->tx_unmap_ops,\r\nNULL,\r\nqueue->pages_to_unmap,\r\ngop - queue->tx_unmap_ops);\r\nif (ret) {\r\nnetdev_err(queue->vif->dev, "Unmap fail: nr_ops %tu ret %d\n",\r\ngop - queue->tx_unmap_ops, ret);\r\nfor (i = 0; i < gop - queue->tx_unmap_ops; ++i) {\r\nif (gop[i].status != GNTST_okay)\r\nnetdev_err(queue->vif->dev,\r\n" host_addr: 0x%llx handle: 0x%x status: %d\n",\r\ngop[i].host_addr,\r\ngop[i].handle,\r\ngop[i].status);\r\n}\r\nBUG();\r\n}\r\n}\r\nfor (i = 0; i < gop - queue->tx_unmap_ops; ++i)\r\nxenvif_idx_release(queue, pending_idx_release[i],\r\nXEN_NETIF_RSP_OKAY);\r\n}\r\nint xenvif_tx_action(struct xenvif_queue *queue, int budget)\r\n{\r\nunsigned nr_mops, nr_cops = 0;\r\nint work_done, ret;\r\nif (unlikely(!tx_work_todo(queue)))\r\nreturn 0;\r\nxenvif_tx_build_gops(queue, budget, &nr_cops, &nr_mops);\r\nif (nr_cops == 0)\r\nreturn 0;\r\ngnttab_batch_copy(queue->tx_copy_ops, nr_cops);\r\nif (nr_mops != 0) {\r\nret = gnttab_map_refs(queue->tx_map_ops,\r\nNULL,\r\nqueue->pages_to_map,\r\nnr_mops);\r\nBUG_ON(ret);\r\n}\r\nwork_done = xenvif_tx_submit(queue);\r\nreturn work_done;\r\n}\r\nstatic void xenvif_idx_release(struct xenvif_queue *queue, u16 pending_idx,\r\nu8 status)\r\n{\r\nstruct pending_tx_info *pending_tx_info;\r\npending_ring_idx_t index;\r\nunsigned long flags;\r\npending_tx_info = &queue->pending_tx_info[pending_idx];\r\nspin_lock_irqsave(&queue->response_lock, flags);\r\nmake_tx_response(queue, &pending_tx_info->req,\r\npending_tx_info->extra_count, status);\r\nindex = pending_index(queue->pending_prod++);\r\nqueue->pending_ring[index] = pending_idx;\r\npush_tx_responses(queue);\r\nspin_unlock_irqrestore(&queue->response_lock, flags);\r\n}\r\nstatic void make_tx_response(struct xenvif_queue *queue,\r\nstruct xen_netif_tx_request *txp,\r\nunsigned int extra_count,\r\ns8 st)\r\n{\r\nRING_IDX i = queue->tx.rsp_prod_pvt;\r\nstruct xen_netif_tx_response *resp;\r\nresp = RING_GET_RESPONSE(&queue->tx, i);\r\nresp->id = txp->id;\r\nresp->status = st;\r\nwhile (extra_count-- != 0)\r\nRING_GET_RESPONSE(&queue->tx, ++i)->status = XEN_NETIF_RSP_NULL;\r\nqueue->tx.rsp_prod_pvt = ++i;\r\n}\r\nstatic void push_tx_responses(struct xenvif_queue *queue)\r\n{\r\nint notify;\r\nRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&queue->tx, notify);\r\nif (notify)\r\nnotify_remote_via_irq(queue->tx_irq);\r\n}\r\nvoid xenvif_idx_unmap(struct xenvif_queue *queue, u16 pending_idx)\r\n{\r\nint ret;\r\nstruct gnttab_unmap_grant_ref tx_unmap_op;\r\ngnttab_set_unmap_op(&tx_unmap_op,\r\nidx_to_kaddr(queue, pending_idx),\r\nGNTMAP_host_map,\r\nqueue->grant_tx_handle[pending_idx]);\r\nxenvif_grant_handle_reset(queue, pending_idx);\r\nret = gnttab_unmap_refs(&tx_unmap_op, NULL,\r\n&queue->mmap_pages[pending_idx], 1);\r\nif (ret) {\r\nnetdev_err(queue->vif->dev,\r\n"Unmap fail: ret: %d pending_idx: %d host_addr: %llx handle: 0x%x status: %d\n",\r\nret,\r\npending_idx,\r\ntx_unmap_op.host_addr,\r\ntx_unmap_op.handle,\r\ntx_unmap_op.status);\r\nBUG();\r\n}\r\n}\r\nstatic inline int tx_work_todo(struct xenvif_queue *queue)\r\n{\r\nif (likely(RING_HAS_UNCONSUMED_REQUESTS(&queue->tx)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline bool tx_dealloc_work_todo(struct xenvif_queue *queue)\r\n{\r\nreturn queue->dealloc_cons != queue->dealloc_prod;\r\n}\r\nvoid xenvif_unmap_frontend_data_rings(struct xenvif_queue *queue)\r\n{\r\nif (queue->tx.sring)\r\nxenbus_unmap_ring_vfree(xenvif_to_xenbus_device(queue->vif),\r\nqueue->tx.sring);\r\nif (queue->rx.sring)\r\nxenbus_unmap_ring_vfree(xenvif_to_xenbus_device(queue->vif),\r\nqueue->rx.sring);\r\n}\r\nint xenvif_map_frontend_data_rings(struct xenvif_queue *queue,\r\ngrant_ref_t tx_ring_ref,\r\ngrant_ref_t rx_ring_ref)\r\n{\r\nvoid *addr;\r\nstruct xen_netif_tx_sring *txs;\r\nstruct xen_netif_rx_sring *rxs;\r\nint err = -ENOMEM;\r\nerr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(queue->vif),\r\n&tx_ring_ref, 1, &addr);\r\nif (err)\r\ngoto err;\r\ntxs = (struct xen_netif_tx_sring *)addr;\r\nBACK_RING_INIT(&queue->tx, txs, XEN_PAGE_SIZE);\r\nerr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(queue->vif),\r\n&rx_ring_ref, 1, &addr);\r\nif (err)\r\ngoto err;\r\nrxs = (struct xen_netif_rx_sring *)addr;\r\nBACK_RING_INIT(&queue->rx, rxs, XEN_PAGE_SIZE);\r\nreturn 0;\r\nerr:\r\nxenvif_unmap_frontend_data_rings(queue);\r\nreturn err;\r\n}\r\nstatic bool xenvif_dealloc_kthread_should_stop(struct xenvif_queue *queue)\r\n{\r\nreturn kthread_should_stop() &&\r\n!atomic_read(&queue->inflight_packets);\r\n}\r\nint xenvif_dealloc_kthread(void *data)\r\n{\r\nstruct xenvif_queue *queue = data;\r\nfor (;;) {\r\nwait_event_interruptible(queue->dealloc_wq,\r\ntx_dealloc_work_todo(queue) ||\r\nxenvif_dealloc_kthread_should_stop(queue));\r\nif (xenvif_dealloc_kthread_should_stop(queue))\r\nbreak;\r\nxenvif_tx_dealloc_action(queue);\r\ncond_resched();\r\n}\r\nif (tx_dealloc_work_todo(queue))\r\nxenvif_tx_dealloc_action(queue);\r\nreturn 0;\r\n}\r\nstatic void make_ctrl_response(struct xenvif *vif,\r\nconst struct xen_netif_ctrl_request *req,\r\nu32 status, u32 data)\r\n{\r\nRING_IDX idx = vif->ctrl.rsp_prod_pvt;\r\nstruct xen_netif_ctrl_response rsp = {\r\n.id = req->id,\r\n.type = req->type,\r\n.status = status,\r\n.data = data,\r\n};\r\n*RING_GET_RESPONSE(&vif->ctrl, idx) = rsp;\r\nvif->ctrl.rsp_prod_pvt = ++idx;\r\n}\r\nstatic void push_ctrl_response(struct xenvif *vif)\r\n{\r\nint notify;\r\nRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&vif->ctrl, notify);\r\nif (notify)\r\nnotify_remote_via_irq(vif->ctrl_irq);\r\n}\r\nstatic void process_ctrl_request(struct xenvif *vif,\r\nconst struct xen_netif_ctrl_request *req)\r\n{\r\nu32 status = XEN_NETIF_CTRL_STATUS_NOT_SUPPORTED;\r\nu32 data = 0;\r\nswitch (req->type) {\r\ncase XEN_NETIF_CTRL_TYPE_SET_HASH_ALGORITHM:\r\nstatus = xenvif_set_hash_alg(vif, req->data[0]);\r\nbreak;\r\ncase XEN_NETIF_CTRL_TYPE_GET_HASH_FLAGS:\r\nstatus = xenvif_get_hash_flags(vif, &data);\r\nbreak;\r\ncase XEN_NETIF_CTRL_TYPE_SET_HASH_FLAGS:\r\nstatus = xenvif_set_hash_flags(vif, req->data[0]);\r\nbreak;\r\ncase XEN_NETIF_CTRL_TYPE_SET_HASH_KEY:\r\nstatus = xenvif_set_hash_key(vif, req->data[0],\r\nreq->data[1]);\r\nbreak;\r\ncase XEN_NETIF_CTRL_TYPE_GET_HASH_MAPPING_SIZE:\r\nstatus = XEN_NETIF_CTRL_STATUS_SUCCESS;\r\ndata = XEN_NETBK_MAX_HASH_MAPPING_SIZE;\r\nbreak;\r\ncase XEN_NETIF_CTRL_TYPE_SET_HASH_MAPPING_SIZE:\r\nstatus = xenvif_set_hash_mapping_size(vif,\r\nreq->data[0]);\r\nbreak;\r\ncase XEN_NETIF_CTRL_TYPE_SET_HASH_MAPPING:\r\nstatus = xenvif_set_hash_mapping(vif, req->data[0],\r\nreq->data[1],\r\nreq->data[2]);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nmake_ctrl_response(vif, req, status, data);\r\npush_ctrl_response(vif);\r\n}\r\nstatic void xenvif_ctrl_action(struct xenvif *vif)\r\n{\r\nfor (;;) {\r\nRING_IDX req_prod, req_cons;\r\nreq_prod = vif->ctrl.sring->req_prod;\r\nreq_cons = vif->ctrl.req_cons;\r\nrmb();\r\nif (req_cons == req_prod)\r\nbreak;\r\nwhile (req_cons != req_prod) {\r\nstruct xen_netif_ctrl_request req;\r\nRING_COPY_REQUEST(&vif->ctrl, req_cons, &req);\r\nreq_cons++;\r\nprocess_ctrl_request(vif, &req);\r\n}\r\nvif->ctrl.req_cons = req_cons;\r\nvif->ctrl.sring->req_event = req_cons + 1;\r\n}\r\n}\r\nstatic bool xenvif_ctrl_work_todo(struct xenvif *vif)\r\n{\r\nif (likely(RING_HAS_UNCONSUMED_REQUESTS(&vif->ctrl)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nirqreturn_t xenvif_ctrl_irq_fn(int irq, void *data)\r\n{\r\nstruct xenvif *vif = data;\r\nwhile (xenvif_ctrl_work_todo(vif))\r\nxenvif_ctrl_action(vif);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int __init netback_init(void)\r\n{\r\nint rc = 0;\r\nif (!xen_domain())\r\nreturn -ENODEV;\r\nif (xenvif_max_queues == 0)\r\nxenvif_max_queues = min_t(unsigned int, MAX_QUEUES_DEFAULT,\r\nnum_online_cpus());\r\nif (fatal_skb_slots < XEN_NETBK_LEGACY_SLOTS_MAX) {\r\npr_info("fatal_skb_slots too small (%d), bump it to XEN_NETBK_LEGACY_SLOTS_MAX (%d)\n",\r\nfatal_skb_slots, XEN_NETBK_LEGACY_SLOTS_MAX);\r\nfatal_skb_slots = XEN_NETBK_LEGACY_SLOTS_MAX;\r\n}\r\nrc = xenvif_xenbus_init();\r\nif (rc)\r\ngoto failed_init;\r\n#ifdef CONFIG_DEBUG_FS\r\nxen_netback_dbg_root = debugfs_create_dir("xen-netback", NULL);\r\nif (IS_ERR_OR_NULL(xen_netback_dbg_root))\r\npr_warn("Init of debugfs returned %ld!\n",\r\nPTR_ERR(xen_netback_dbg_root));\r\n#endif\r\nreturn 0;\r\nfailed_init:\r\nreturn rc;\r\n}\r\nstatic void __exit netback_fini(void)\r\n{\r\n#ifdef CONFIG_DEBUG_FS\r\nif (!IS_ERR_OR_NULL(xen_netback_dbg_root))\r\ndebugfs_remove_recursive(xen_netback_dbg_root);\r\n#endif\r\nxenvif_xenbus_fini();\r\n}
