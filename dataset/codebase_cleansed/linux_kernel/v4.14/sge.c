static void tx_sched_stop(struct sge *sge)\r\n{\r\nstruct sched *s = sge->tx_sched;\r\nint i;\r\ntasklet_kill(&s->sched_tsk);\r\nfor (i = 0; i < MAX_NPORTS; i++)\r\n__skb_queue_purge(&s->p[s->port].skbq);\r\n}\r\nunsigned int t1_sched_update_parms(struct sge *sge, unsigned int port,\r\nunsigned int mtu, unsigned int speed)\r\n{\r\nstruct sched *s = sge->tx_sched;\r\nstruct sched_port *p = &s->p[port];\r\nunsigned int max_avail_segs;\r\npr_debug("%s mtu=%d speed=%d\n", __func__, mtu, speed);\r\nif (speed)\r\np->speed = speed;\r\nif (mtu)\r\np->mtu = mtu;\r\nif (speed || mtu) {\r\nunsigned long long drain = 1024ULL * p->speed * (p->mtu - 40);\r\ndo_div(drain, (p->mtu + 50) * 1000);\r\np->drain_bits_per_1024ns = (unsigned int) drain;\r\nif (p->speed < 1000)\r\np->drain_bits_per_1024ns =\r\n90 * p->drain_bits_per_1024ns / 100;\r\n}\r\nif (board_info(sge->adapter)->board == CHBT_BOARD_CHT204) {\r\np->drain_bits_per_1024ns -= 16;\r\ns->max_avail = max(4096U, p->mtu + 16 + 14 + 4);\r\nmax_avail_segs = max(1U, 4096 / (p->mtu - 40));\r\n} else {\r\ns->max_avail = 16384;\r\nmax_avail_segs = max(1U, 9000 / (p->mtu - 40));\r\n}\r\npr_debug("t1_sched_update_parms: mtu %u speed %u max_avail %u "\r\n"max_avail_segs %u drain_bits_per_1024ns %u\n", p->mtu,\r\np->speed, s->max_avail, max_avail_segs,\r\np->drain_bits_per_1024ns);\r\nreturn max_avail_segs * (p->mtu - 40);\r\n}\r\nstatic int tx_sched_init(struct sge *sge)\r\n{\r\nstruct sched *s;\r\nint i;\r\ns = kzalloc(sizeof (struct sched), GFP_KERNEL);\r\nif (!s)\r\nreturn -ENOMEM;\r\npr_debug("tx_sched_init\n");\r\ntasklet_init(&s->sched_tsk, restart_sched, (unsigned long) sge);\r\nsge->tx_sched = s;\r\nfor (i = 0; i < MAX_NPORTS; i++) {\r\nskb_queue_head_init(&s->p[i].skbq);\r\nt1_sched_update_parms(sge, i, 1500, 1000);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int sched_update_avail(struct sge *sge)\r\n{\r\nstruct sched *s = sge->tx_sched;\r\nktime_t now = ktime_get();\r\nunsigned int i;\r\nlong long delta_time_ns;\r\ndelta_time_ns = ktime_to_ns(ktime_sub(now, s->last_updated));\r\npr_debug("sched_update_avail delta=%lld\n", delta_time_ns);\r\nif (delta_time_ns < 15000)\r\nreturn 0;\r\nfor (i = 0; i < MAX_NPORTS; i++) {\r\nstruct sched_port *p = &s->p[i];\r\nunsigned int delta_avail;\r\ndelta_avail = (p->drain_bits_per_1024ns * delta_time_ns) >> 13;\r\np->avail = min(p->avail + delta_avail, s->max_avail);\r\n}\r\ns->last_updated = now;\r\nreturn 1;\r\n}\r\nstatic struct sk_buff *sched_skb(struct sge *sge, struct sk_buff *skb,\r\nunsigned int credits)\r\n{\r\nstruct sched *s = sge->tx_sched;\r\nstruct sk_buff_head *skbq;\r\nunsigned int i, len, update = 1;\r\npr_debug("sched_skb %p\n", skb);\r\nif (!skb) {\r\nif (!s->num)\r\nreturn NULL;\r\n} else {\r\nskbq = &s->p[skb->dev->if_port].skbq;\r\n__skb_queue_tail(skbq, skb);\r\ns->num++;\r\nskb = NULL;\r\n}\r\nif (credits < MAX_SKB_FRAGS + 1)\r\ngoto out;\r\nagain:\r\nfor (i = 0; i < MAX_NPORTS; i++) {\r\ns->port = (s->port + 1) & (MAX_NPORTS - 1);\r\nskbq = &s->p[s->port].skbq;\r\nskb = skb_peek(skbq);\r\nif (!skb)\r\ncontinue;\r\nlen = skb->len;\r\nif (len <= s->p[s->port].avail) {\r\ns->p[s->port].avail -= len;\r\ns->num--;\r\n__skb_unlink(skb, skbq);\r\ngoto out;\r\n}\r\nskb = NULL;\r\n}\r\nif (update-- && sched_update_avail(sge))\r\ngoto again;\r\nout:\r\nif (s->num && !skb) {\r\nstruct cmdQ *q = &sge->cmdQ[0];\r\nclear_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\r\nif (test_and_set_bit(CMDQ_STAT_RUNNING, &q->status) == 0) {\r\nset_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\r\nwritel(F_CMDQ0_ENABLE, sge->adapter->regs + A_SG_DOORBELL);\r\n}\r\n}\r\npr_debug("sched_skb ret %p\n", skb);\r\nreturn skb;\r\n}\r\nstatic inline void doorbell_pio(struct adapter *adapter, u32 val)\r\n{\r\nwmb();\r\nwritel(val, adapter->regs + A_SG_DOORBELL);\r\n}\r\nstatic void free_freelQ_buffers(struct pci_dev *pdev, struct freelQ *q)\r\n{\r\nunsigned int cidx = q->cidx;\r\nwhile (q->credits--) {\r\nstruct freelQ_ce *ce = &q->centries[cidx];\r\npci_unmap_single(pdev, dma_unmap_addr(ce, dma_addr),\r\ndma_unmap_len(ce, dma_len),\r\nPCI_DMA_FROMDEVICE);\r\ndev_kfree_skb(ce->skb);\r\nce->skb = NULL;\r\nif (++cidx == q->size)\r\ncidx = 0;\r\n}\r\n}\r\nstatic void free_rx_resources(struct sge *sge)\r\n{\r\nstruct pci_dev *pdev = sge->adapter->pdev;\r\nunsigned int size, i;\r\nif (sge->respQ.entries) {\r\nsize = sizeof(struct respQ_e) * sge->respQ.size;\r\npci_free_consistent(pdev, size, sge->respQ.entries,\r\nsge->respQ.dma_addr);\r\n}\r\nfor (i = 0; i < SGE_FREELQ_N; i++) {\r\nstruct freelQ *q = &sge->freelQ[i];\r\nif (q->centries) {\r\nfree_freelQ_buffers(pdev, q);\r\nkfree(q->centries);\r\n}\r\nif (q->entries) {\r\nsize = sizeof(struct freelQ_e) * q->size;\r\npci_free_consistent(pdev, size, q->entries,\r\nq->dma_addr);\r\n}\r\n}\r\n}\r\nstatic int alloc_rx_resources(struct sge *sge, struct sge_params *p)\r\n{\r\nstruct pci_dev *pdev = sge->adapter->pdev;\r\nunsigned int size, i;\r\nfor (i = 0; i < SGE_FREELQ_N; i++) {\r\nstruct freelQ *q = &sge->freelQ[i];\r\nq->genbit = 1;\r\nq->size = p->freelQ_size[i];\r\nq->dma_offset = sge->rx_pkt_pad ? 0 : NET_IP_ALIGN;\r\nsize = sizeof(struct freelQ_e) * q->size;\r\nq->entries = pci_alloc_consistent(pdev, size, &q->dma_addr);\r\nif (!q->entries)\r\ngoto err_no_mem;\r\nsize = sizeof(struct freelQ_ce) * q->size;\r\nq->centries = kzalloc(size, GFP_KERNEL);\r\nif (!q->centries)\r\ngoto err_no_mem;\r\n}\r\nsge->freelQ[!sge->jumbo_fl].rx_buffer_size = SGE_RX_SM_BUF_SIZE +\r\nsizeof(struct cpl_rx_data) +\r\nsge->freelQ[!sge->jumbo_fl].dma_offset;\r\nsize = (16 * 1024) -\r\nSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\r\nsge->freelQ[sge->jumbo_fl].rx_buffer_size = size;\r\nsge->freelQ[!sge->jumbo_fl].recycleq_idx = 0;\r\nsge->freelQ[sge->jumbo_fl].recycleq_idx = 1;\r\nsge->respQ.genbit = 1;\r\nsge->respQ.size = SGE_RESPQ_E_N;\r\nsge->respQ.credits = 0;\r\nsize = sizeof(struct respQ_e) * sge->respQ.size;\r\nsge->respQ.entries =\r\npci_alloc_consistent(pdev, size, &sge->respQ.dma_addr);\r\nif (!sge->respQ.entries)\r\ngoto err_no_mem;\r\nreturn 0;\r\nerr_no_mem:\r\nfree_rx_resources(sge);\r\nreturn -ENOMEM;\r\n}\r\nstatic void free_cmdQ_buffers(struct sge *sge, struct cmdQ *q, unsigned int n)\r\n{\r\nstruct cmdQ_ce *ce;\r\nstruct pci_dev *pdev = sge->adapter->pdev;\r\nunsigned int cidx = q->cidx;\r\nq->in_use -= n;\r\nce = &q->centries[cidx];\r\nwhile (n--) {\r\nif (likely(dma_unmap_len(ce, dma_len))) {\r\npci_unmap_single(pdev, dma_unmap_addr(ce, dma_addr),\r\ndma_unmap_len(ce, dma_len),\r\nPCI_DMA_TODEVICE);\r\nif (q->sop)\r\nq->sop = 0;\r\n}\r\nif (ce->skb) {\r\ndev_kfree_skb_any(ce->skb);\r\nq->sop = 1;\r\n}\r\nce++;\r\nif (++cidx == q->size) {\r\ncidx = 0;\r\nce = q->centries;\r\n}\r\n}\r\nq->cidx = cidx;\r\n}\r\nstatic void free_tx_resources(struct sge *sge)\r\n{\r\nstruct pci_dev *pdev = sge->adapter->pdev;\r\nunsigned int size, i;\r\nfor (i = 0; i < SGE_CMDQ_N; i++) {\r\nstruct cmdQ *q = &sge->cmdQ[i];\r\nif (q->centries) {\r\nif (q->in_use)\r\nfree_cmdQ_buffers(sge, q, q->in_use);\r\nkfree(q->centries);\r\n}\r\nif (q->entries) {\r\nsize = sizeof(struct cmdQ_e) * q->size;\r\npci_free_consistent(pdev, size, q->entries,\r\nq->dma_addr);\r\n}\r\n}\r\n}\r\nstatic int alloc_tx_resources(struct sge *sge, struct sge_params *p)\r\n{\r\nstruct pci_dev *pdev = sge->adapter->pdev;\r\nunsigned int size, i;\r\nfor (i = 0; i < SGE_CMDQ_N; i++) {\r\nstruct cmdQ *q = &sge->cmdQ[i];\r\nq->genbit = 1;\r\nq->sop = 1;\r\nq->size = p->cmdQ_size[i];\r\nq->in_use = 0;\r\nq->status = 0;\r\nq->processed = q->cleaned = 0;\r\nq->stop_thres = 0;\r\nspin_lock_init(&q->lock);\r\nsize = sizeof(struct cmdQ_e) * q->size;\r\nq->entries = pci_alloc_consistent(pdev, size, &q->dma_addr);\r\nif (!q->entries)\r\ngoto err_no_mem;\r\nsize = sizeof(struct cmdQ_ce) * q->size;\r\nq->centries = kzalloc(size, GFP_KERNEL);\r\nif (!q->centries)\r\ngoto err_no_mem;\r\n}\r\nsge->cmdQ[0].stop_thres = sge->adapter->params.nports *\r\n(MAX_SKB_FRAGS + 1);\r\nreturn 0;\r\nerr_no_mem:\r\nfree_tx_resources(sge);\r\nreturn -ENOMEM;\r\n}\r\nstatic inline void setup_ring_params(struct adapter *adapter, u64 addr,\r\nu32 size, int base_reg_lo,\r\nint base_reg_hi, int size_reg)\r\n{\r\nwritel((u32)addr, adapter->regs + base_reg_lo);\r\nwritel(addr >> 32, adapter->regs + base_reg_hi);\r\nwritel(size, adapter->regs + size_reg);\r\n}\r\nvoid t1_vlan_mode(struct adapter *adapter, netdev_features_t features)\r\n{\r\nstruct sge *sge = adapter->sge;\r\nif (features & NETIF_F_HW_VLAN_CTAG_RX)\r\nsge->sge_control |= F_VLAN_XTRACT;\r\nelse\r\nsge->sge_control &= ~F_VLAN_XTRACT;\r\nif (adapter->open_device_map) {\r\nwritel(sge->sge_control, adapter->regs + A_SG_CONTROL);\r\nreadl(adapter->regs + A_SG_CONTROL);\r\n}\r\n}\r\nstatic void configure_sge(struct sge *sge, struct sge_params *p)\r\n{\r\nstruct adapter *ap = sge->adapter;\r\nwritel(0, ap->regs + A_SG_CONTROL);\r\nsetup_ring_params(ap, sge->cmdQ[0].dma_addr, sge->cmdQ[0].size,\r\nA_SG_CMD0BASELWR, A_SG_CMD0BASEUPR, A_SG_CMD0SIZE);\r\nsetup_ring_params(ap, sge->cmdQ[1].dma_addr, sge->cmdQ[1].size,\r\nA_SG_CMD1BASELWR, A_SG_CMD1BASEUPR, A_SG_CMD1SIZE);\r\nsetup_ring_params(ap, sge->freelQ[0].dma_addr,\r\nsge->freelQ[0].size, A_SG_FL0BASELWR,\r\nA_SG_FL0BASEUPR, A_SG_FL0SIZE);\r\nsetup_ring_params(ap, sge->freelQ[1].dma_addr,\r\nsge->freelQ[1].size, A_SG_FL1BASELWR,\r\nA_SG_FL1BASEUPR, A_SG_FL1SIZE);\r\nwritel(SGE_RX_SM_BUF_SIZE + 1, ap->regs + A_SG_FLTHRESHOLD);\r\nsetup_ring_params(ap, sge->respQ.dma_addr, sge->respQ.size,\r\nA_SG_RSPBASELWR, A_SG_RSPBASEUPR, A_SG_RSPSIZE);\r\nwritel((u32)sge->respQ.size - 1, ap->regs + A_SG_RSPQUEUECREDIT);\r\nsge->sge_control = F_CMDQ0_ENABLE | F_CMDQ1_ENABLE | F_FL0_ENABLE |\r\nF_FL1_ENABLE | F_CPL_ENABLE | F_RESPONSE_QUEUE_ENABLE |\r\nV_CMDQ_PRIORITY(2) | F_DISABLE_CMDQ1_GTS | F_ISCSI_COALESCE |\r\nV_RX_PKT_OFFSET(sge->rx_pkt_pad);\r\n#if defined(__BIG_ENDIAN_BITFIELD)\r\nsge->sge_control |= F_ENABLE_BIG_ENDIAN;\r\n#endif\r\nsge->intrtimer_nres = SGE_INTRTIMER_NRES * core_ticks_per_usec(ap);\r\nt1_sge_set_coalesce_params(sge, p);\r\n}\r\nstatic inline unsigned int jumbo_payload_capacity(const struct sge *sge)\r\n{\r\nreturn sge->freelQ[sge->jumbo_fl].rx_buffer_size -\r\nsge->freelQ[sge->jumbo_fl].dma_offset -\r\nsizeof(struct cpl_rx_data);\r\n}\r\nvoid t1_sge_destroy(struct sge *sge)\r\n{\r\nint i;\r\nfor_each_port(sge->adapter, i)\r\nfree_percpu(sge->port_stats[i]);\r\nkfree(sge->tx_sched);\r\nfree_tx_resources(sge);\r\nfree_rx_resources(sge);\r\nkfree(sge);\r\n}\r\nstatic void refill_free_list(struct sge *sge, struct freelQ *q)\r\n{\r\nstruct pci_dev *pdev = sge->adapter->pdev;\r\nstruct freelQ_ce *ce = &q->centries[q->pidx];\r\nstruct freelQ_e *e = &q->entries[q->pidx];\r\nunsigned int dma_len = q->rx_buffer_size - q->dma_offset;\r\nwhile (q->credits < q->size) {\r\nstruct sk_buff *skb;\r\ndma_addr_t mapping;\r\nskb = dev_alloc_skb(q->rx_buffer_size);\r\nif (!skb)\r\nbreak;\r\nskb_reserve(skb, q->dma_offset);\r\nmapping = pci_map_single(pdev, skb->data, dma_len,\r\nPCI_DMA_FROMDEVICE);\r\nskb_reserve(skb, sge->rx_pkt_pad);\r\nce->skb = skb;\r\ndma_unmap_addr_set(ce, dma_addr, mapping);\r\ndma_unmap_len_set(ce, dma_len, dma_len);\r\ne->addr_lo = (u32)mapping;\r\ne->addr_hi = (u64)mapping >> 32;\r\ne->len_gen = V_CMD_LEN(dma_len) | V_CMD_GEN1(q->genbit);\r\nwmb();\r\ne->gen2 = V_CMD_GEN2(q->genbit);\r\ne++;\r\nce++;\r\nif (++q->pidx == q->size) {\r\nq->pidx = 0;\r\nq->genbit ^= 1;\r\nce = q->centries;\r\ne = q->entries;\r\n}\r\nq->credits++;\r\n}\r\n}\r\nstatic void freelQs_empty(struct sge *sge)\r\n{\r\nstruct adapter *adapter = sge->adapter;\r\nu32 irq_reg = readl(adapter->regs + A_SG_INT_ENABLE);\r\nu32 irqholdoff_reg;\r\nrefill_free_list(sge, &sge->freelQ[0]);\r\nrefill_free_list(sge, &sge->freelQ[1]);\r\nif (sge->freelQ[0].credits > (sge->freelQ[0].size >> 2) &&\r\nsge->freelQ[1].credits > (sge->freelQ[1].size >> 2)) {\r\nirq_reg |= F_FL_EXHAUSTED;\r\nirqholdoff_reg = sge->fixed_intrtimer;\r\n} else {\r\nirq_reg &= ~F_FL_EXHAUSTED;\r\nirqholdoff_reg = sge->intrtimer_nres;\r\n}\r\nwritel(irqholdoff_reg, adapter->regs + A_SG_INTRTIMER);\r\nwritel(irq_reg, adapter->regs + A_SG_INT_ENABLE);\r\ndoorbell_pio(adapter, F_FL0_ENABLE | F_FL1_ENABLE);\r\n}\r\nvoid t1_sge_intr_disable(struct sge *sge)\r\n{\r\nu32 val = readl(sge->adapter->regs + A_PL_ENABLE);\r\nwritel(val & ~SGE_PL_INTR_MASK, sge->adapter->regs + A_PL_ENABLE);\r\nwritel(0, sge->adapter->regs + A_SG_INT_ENABLE);\r\n}\r\nvoid t1_sge_intr_enable(struct sge *sge)\r\n{\r\nu32 en = SGE_INT_ENABLE;\r\nu32 val = readl(sge->adapter->regs + A_PL_ENABLE);\r\nif (sge->adapter->port[0].dev->hw_features & NETIF_F_TSO)\r\nen &= ~F_PACKET_TOO_BIG;\r\nwritel(en, sge->adapter->regs + A_SG_INT_ENABLE);\r\nwritel(val | SGE_PL_INTR_MASK, sge->adapter->regs + A_PL_ENABLE);\r\n}\r\nvoid t1_sge_intr_clear(struct sge *sge)\r\n{\r\nwritel(SGE_PL_INTR_MASK, sge->adapter->regs + A_PL_CAUSE);\r\nwritel(0xffffffff, sge->adapter->regs + A_SG_INT_CAUSE);\r\n}\r\nint t1_sge_intr_error_handler(struct sge *sge)\r\n{\r\nstruct adapter *adapter = sge->adapter;\r\nu32 cause = readl(adapter->regs + A_SG_INT_CAUSE);\r\nif (adapter->port[0].dev->hw_features & NETIF_F_TSO)\r\ncause &= ~F_PACKET_TOO_BIG;\r\nif (cause & F_RESPQ_EXHAUSTED)\r\nsge->stats.respQ_empty++;\r\nif (cause & F_RESPQ_OVERFLOW) {\r\nsge->stats.respQ_overflow++;\r\npr_alert("%s: SGE response queue overflow\n",\r\nadapter->name);\r\n}\r\nif (cause & F_FL_EXHAUSTED) {\r\nsge->stats.freelistQ_empty++;\r\nfreelQs_empty(sge);\r\n}\r\nif (cause & F_PACKET_TOO_BIG) {\r\nsge->stats.pkt_too_big++;\r\npr_alert("%s: SGE max packet size exceeded\n",\r\nadapter->name);\r\n}\r\nif (cause & F_PACKET_MISMATCH) {\r\nsge->stats.pkt_mismatch++;\r\npr_alert("%s: SGE packet mismatch\n", adapter->name);\r\n}\r\nif (cause & SGE_INT_FATAL)\r\nt1_fatal_err(adapter);\r\nwritel(cause, adapter->regs + A_SG_INT_CAUSE);\r\nreturn 0;\r\n}\r\nconst struct sge_intr_counts *t1_sge_get_intr_counts(const struct sge *sge)\r\n{\r\nreturn &sge->stats;\r\n}\r\nvoid t1_sge_get_port_stats(const struct sge *sge, int port,\r\nstruct sge_port_stats *ss)\r\n{\r\nint cpu;\r\nmemset(ss, 0, sizeof(*ss));\r\nfor_each_possible_cpu(cpu) {\r\nstruct sge_port_stats *st = per_cpu_ptr(sge->port_stats[port], cpu);\r\nss->rx_cso_good += st->rx_cso_good;\r\nss->tx_cso += st->tx_cso;\r\nss->tx_tso += st->tx_tso;\r\nss->tx_need_hdrroom += st->tx_need_hdrroom;\r\nss->vlan_xtract += st->vlan_xtract;\r\nss->vlan_insert += st->vlan_insert;\r\n}\r\n}\r\nstatic void recycle_fl_buf(struct freelQ *fl, int idx)\r\n{\r\nstruct freelQ_e *from = &fl->entries[idx];\r\nstruct freelQ_e *to = &fl->entries[fl->pidx];\r\nfl->centries[fl->pidx] = fl->centries[idx];\r\nto->addr_lo = from->addr_lo;\r\nto->addr_hi = from->addr_hi;\r\nto->len_gen = G_CMD_LEN(from->len_gen) | V_CMD_GEN1(fl->genbit);\r\nwmb();\r\nto->gen2 = V_CMD_GEN2(fl->genbit);\r\nfl->credits++;\r\nif (++fl->pidx == fl->size) {\r\nfl->pidx = 0;\r\nfl->genbit ^= 1;\r\n}\r\n}\r\nstatic inline struct sk_buff *get_packet(struct adapter *adapter,\r\nstruct freelQ *fl, unsigned int len)\r\n{\r\nconst struct freelQ_ce *ce = &fl->centries[fl->cidx];\r\nstruct pci_dev *pdev = adapter->pdev;\r\nstruct sk_buff *skb;\r\nif (len < copybreak) {\r\nskb = napi_alloc_skb(&adapter->napi, len);\r\nif (!skb)\r\ngoto use_orig_buf;\r\nskb_put(skb, len);\r\npci_dma_sync_single_for_cpu(pdev,\r\ndma_unmap_addr(ce, dma_addr),\r\ndma_unmap_len(ce, dma_len),\r\nPCI_DMA_FROMDEVICE);\r\nskb_copy_from_linear_data(ce->skb, skb->data, len);\r\npci_dma_sync_single_for_device(pdev,\r\ndma_unmap_addr(ce, dma_addr),\r\ndma_unmap_len(ce, dma_len),\r\nPCI_DMA_FROMDEVICE);\r\nrecycle_fl_buf(fl, fl->cidx);\r\nreturn skb;\r\n}\r\nuse_orig_buf:\r\nif (fl->credits < 2) {\r\nrecycle_fl_buf(fl, fl->cidx);\r\nreturn NULL;\r\n}\r\npci_unmap_single(pdev, dma_unmap_addr(ce, dma_addr),\r\ndma_unmap_len(ce, dma_len), PCI_DMA_FROMDEVICE);\r\nskb = ce->skb;\r\nprefetch(skb->data);\r\nskb_put(skb, len);\r\nreturn skb;\r\n}\r\nstatic void unexpected_offload(struct adapter *adapter, struct freelQ *fl)\r\n{\r\nstruct freelQ_ce *ce = &fl->centries[fl->cidx];\r\nstruct sk_buff *skb = ce->skb;\r\npci_dma_sync_single_for_cpu(adapter->pdev, dma_unmap_addr(ce, dma_addr),\r\ndma_unmap_len(ce, dma_len), PCI_DMA_FROMDEVICE);\r\npr_err("%s: unexpected offload packet, cmd %u\n",\r\nadapter->name, *skb->data);\r\nrecycle_fl_buf(fl, fl->cidx);\r\n}\r\nstatic inline unsigned int compute_large_page_tx_descs(struct sk_buff *skb)\r\n{\r\nunsigned int count = 0;\r\nif (PAGE_SIZE > SGE_TX_DESC_MAX_PLEN) {\r\nunsigned int nfrags = skb_shinfo(skb)->nr_frags;\r\nunsigned int i, len = skb_headlen(skb);\r\nwhile (len > SGE_TX_DESC_MAX_PLEN) {\r\ncount++;\r\nlen -= SGE_TX_DESC_MAX_PLEN;\r\n}\r\nfor (i = 0; nfrags--; i++) {\r\nconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nlen = skb_frag_size(frag);\r\nwhile (len > SGE_TX_DESC_MAX_PLEN) {\r\ncount++;\r\nlen -= SGE_TX_DESC_MAX_PLEN;\r\n}\r\n}\r\n}\r\nreturn count;\r\n}\r\nstatic inline void write_tx_desc(struct cmdQ_e *e, dma_addr_t mapping,\r\nunsigned int len, unsigned int gen,\r\nunsigned int eop)\r\n{\r\nBUG_ON(len > SGE_TX_DESC_MAX_PLEN);\r\ne->addr_lo = (u32)mapping;\r\ne->addr_hi = (u64)mapping >> 32;\r\ne->len_gen = V_CMD_LEN(len) | V_CMD_GEN1(gen);\r\ne->flags = F_CMD_DATAVALID | V_CMD_EOP(eop) | V_CMD_GEN2(gen);\r\n}\r\nstatic inline unsigned int write_large_page_tx_descs(unsigned int pidx,\r\nstruct cmdQ_e **e,\r\nstruct cmdQ_ce **ce,\r\nunsigned int *gen,\r\ndma_addr_t *desc_mapping,\r\nunsigned int *desc_len,\r\nunsigned int nfrags,\r\nstruct cmdQ *q)\r\n{\r\nif (PAGE_SIZE > SGE_TX_DESC_MAX_PLEN) {\r\nstruct cmdQ_e *e1 = *e;\r\nstruct cmdQ_ce *ce1 = *ce;\r\nwhile (*desc_len > SGE_TX_DESC_MAX_PLEN) {\r\n*desc_len -= SGE_TX_DESC_MAX_PLEN;\r\nwrite_tx_desc(e1, *desc_mapping, SGE_TX_DESC_MAX_PLEN,\r\n*gen, nfrags == 0 && *desc_len == 0);\r\nce1->skb = NULL;\r\ndma_unmap_len_set(ce1, dma_len, 0);\r\n*desc_mapping += SGE_TX_DESC_MAX_PLEN;\r\nif (*desc_len) {\r\nce1++;\r\ne1++;\r\nif (++pidx == q->size) {\r\npidx = 0;\r\n*gen ^= 1;\r\nce1 = q->centries;\r\ne1 = q->entries;\r\n}\r\n}\r\n}\r\n*e = e1;\r\n*ce = ce1;\r\n}\r\nreturn pidx;\r\n}\r\nstatic inline void write_tx_descs(struct adapter *adapter, struct sk_buff *skb,\r\nunsigned int pidx, unsigned int gen,\r\nstruct cmdQ *q)\r\n{\r\ndma_addr_t mapping, desc_mapping;\r\nstruct cmdQ_e *e, *e1;\r\nstruct cmdQ_ce *ce;\r\nunsigned int i, flags, first_desc_len, desc_len,\r\nnfrags = skb_shinfo(skb)->nr_frags;\r\ne = e1 = &q->entries[pidx];\r\nce = &q->centries[pidx];\r\nmapping = pci_map_single(adapter->pdev, skb->data,\r\nskb_headlen(skb), PCI_DMA_TODEVICE);\r\ndesc_mapping = mapping;\r\ndesc_len = skb_headlen(skb);\r\nflags = F_CMD_DATAVALID | F_CMD_SOP |\r\nV_CMD_EOP(nfrags == 0 && desc_len <= SGE_TX_DESC_MAX_PLEN) |\r\nV_CMD_GEN2(gen);\r\nfirst_desc_len = (desc_len <= SGE_TX_DESC_MAX_PLEN) ?\r\ndesc_len : SGE_TX_DESC_MAX_PLEN;\r\ne->addr_lo = (u32)desc_mapping;\r\ne->addr_hi = (u64)desc_mapping >> 32;\r\ne->len_gen = V_CMD_LEN(first_desc_len) | V_CMD_GEN1(gen);\r\nce->skb = NULL;\r\ndma_unmap_len_set(ce, dma_len, 0);\r\nif (PAGE_SIZE > SGE_TX_DESC_MAX_PLEN &&\r\ndesc_len > SGE_TX_DESC_MAX_PLEN) {\r\ndesc_mapping += first_desc_len;\r\ndesc_len -= first_desc_len;\r\ne1++;\r\nce++;\r\nif (++pidx == q->size) {\r\npidx = 0;\r\ngen ^= 1;\r\ne1 = q->entries;\r\nce = q->centries;\r\n}\r\npidx = write_large_page_tx_descs(pidx, &e1, &ce, &gen,\r\n&desc_mapping, &desc_len,\r\nnfrags, q);\r\nif (likely(desc_len))\r\nwrite_tx_desc(e1, desc_mapping, desc_len, gen,\r\nnfrags == 0);\r\n}\r\nce->skb = NULL;\r\ndma_unmap_addr_set(ce, dma_addr, mapping);\r\ndma_unmap_len_set(ce, dma_len, skb_headlen(skb));\r\nfor (i = 0; nfrags--; i++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\ne1++;\r\nce++;\r\nif (++pidx == q->size) {\r\npidx = 0;\r\ngen ^= 1;\r\ne1 = q->entries;\r\nce = q->centries;\r\n}\r\nmapping = skb_frag_dma_map(&adapter->pdev->dev, frag, 0,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\ndesc_mapping = mapping;\r\ndesc_len = skb_frag_size(frag);\r\npidx = write_large_page_tx_descs(pidx, &e1, &ce, &gen,\r\n&desc_mapping, &desc_len,\r\nnfrags, q);\r\nif (likely(desc_len))\r\nwrite_tx_desc(e1, desc_mapping, desc_len, gen,\r\nnfrags == 0);\r\nce->skb = NULL;\r\ndma_unmap_addr_set(ce, dma_addr, mapping);\r\ndma_unmap_len_set(ce, dma_len, skb_frag_size(frag));\r\n}\r\nce->skb = skb;\r\nwmb();\r\ne->flags = flags;\r\n}\r\nstatic inline void reclaim_completed_tx(struct sge *sge, struct cmdQ *q)\r\n{\r\nunsigned int reclaim = q->processed - q->cleaned;\r\nif (reclaim) {\r\npr_debug("reclaim_completed_tx processed:%d cleaned:%d\n",\r\nq->processed, q->cleaned);\r\nfree_cmdQ_buffers(sge, q, reclaim);\r\nq->cleaned += reclaim;\r\n}\r\n}\r\nstatic void restart_sched(unsigned long arg)\r\n{\r\nstruct sge *sge = (struct sge *) arg;\r\nstruct adapter *adapter = sge->adapter;\r\nstruct cmdQ *q = &sge->cmdQ[0];\r\nstruct sk_buff *skb;\r\nunsigned int credits, queued_skb = 0;\r\nspin_lock(&q->lock);\r\nreclaim_completed_tx(sge, q);\r\ncredits = q->size - q->in_use;\r\npr_debug("restart_sched credits=%d\n", credits);\r\nwhile ((skb = sched_skb(sge, NULL, credits)) != NULL) {\r\nunsigned int genbit, pidx, count;\r\ncount = 1 + skb_shinfo(skb)->nr_frags;\r\ncount += compute_large_page_tx_descs(skb);\r\nq->in_use += count;\r\ngenbit = q->genbit;\r\npidx = q->pidx;\r\nq->pidx += count;\r\nif (q->pidx >= q->size) {\r\nq->pidx -= q->size;\r\nq->genbit ^= 1;\r\n}\r\nwrite_tx_descs(adapter, skb, pidx, genbit, q);\r\ncredits = q->size - q->in_use;\r\nqueued_skb = 1;\r\n}\r\nif (queued_skb) {\r\nclear_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\r\nif (test_and_set_bit(CMDQ_STAT_RUNNING, &q->status) == 0) {\r\nset_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\r\nwritel(F_CMDQ0_ENABLE, adapter->regs + A_SG_DOORBELL);\r\n}\r\n}\r\nspin_unlock(&q->lock);\r\n}\r\nstatic void sge_rx(struct sge *sge, struct freelQ *fl, unsigned int len)\r\n{\r\nstruct sk_buff *skb;\r\nconst struct cpl_rx_pkt *p;\r\nstruct adapter *adapter = sge->adapter;\r\nstruct sge_port_stats *st;\r\nstruct net_device *dev;\r\nskb = get_packet(adapter, fl, len - sge->rx_pkt_pad);\r\nif (unlikely(!skb)) {\r\nsge->stats.rx_drops++;\r\nreturn;\r\n}\r\np = (const struct cpl_rx_pkt *) skb->data;\r\nif (p->iff >= adapter->params.nports) {\r\nkfree_skb(skb);\r\nreturn;\r\n}\r\n__skb_pull(skb, sizeof(*p));\r\nst = this_cpu_ptr(sge->port_stats[p->iff]);\r\ndev = adapter->port[p->iff].dev;\r\nskb->protocol = eth_type_trans(skb, dev);\r\nif ((dev->features & NETIF_F_RXCSUM) && p->csum == 0xffff &&\r\nskb->protocol == htons(ETH_P_IP) &&\r\n(skb->data[9] == IPPROTO_TCP || skb->data[9] == IPPROTO_UDP)) {\r\n++st->rx_cso_good;\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n} else\r\nskb_checksum_none_assert(skb);\r\nif (p->vlan_valid) {\r\nst->vlan_xtract++;\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(p->vlan));\r\n}\r\nnetif_receive_skb(skb);\r\n}\r\nstatic inline int enough_free_Tx_descs(const struct cmdQ *q)\r\n{\r\nunsigned int r = q->processed - q->cleaned;\r\nreturn q->in_use - r < (q->size >> 1);\r\n}\r\nstatic void restart_tx_queues(struct sge *sge)\r\n{\r\nstruct adapter *adap = sge->adapter;\r\nint i;\r\nif (!enough_free_Tx_descs(&sge->cmdQ[0]))\r\nreturn;\r\nfor_each_port(adap, i) {\r\nstruct net_device *nd = adap->port[i].dev;\r\nif (test_and_clear_bit(nd->if_port, &sge->stopped_tx_queues) &&\r\nnetif_running(nd)) {\r\nsge->stats.cmdQ_restarted[2]++;\r\nnetif_wake_queue(nd);\r\n}\r\n}\r\n}\r\nstatic unsigned int update_tx_info(struct adapter *adapter,\r\nunsigned int flags,\r\nunsigned int pr0)\r\n{\r\nstruct sge *sge = adapter->sge;\r\nstruct cmdQ *cmdq = &sge->cmdQ[0];\r\ncmdq->processed += pr0;\r\nif (flags & (F_FL0_ENABLE | F_FL1_ENABLE)) {\r\nfreelQs_empty(sge);\r\nflags &= ~(F_FL0_ENABLE | F_FL1_ENABLE);\r\n}\r\nif (flags & F_CMDQ0_ENABLE) {\r\nclear_bit(CMDQ_STAT_RUNNING, &cmdq->status);\r\nif (cmdq->cleaned + cmdq->in_use != cmdq->processed &&\r\n!test_and_set_bit(CMDQ_STAT_LAST_PKT_DB, &cmdq->status)) {\r\nset_bit(CMDQ_STAT_RUNNING, &cmdq->status);\r\nwritel(F_CMDQ0_ENABLE, adapter->regs + A_SG_DOORBELL);\r\n}\r\nif (sge->tx_sched)\r\ntasklet_hi_schedule(&sge->tx_sched->sched_tsk);\r\nflags &= ~F_CMDQ0_ENABLE;\r\n}\r\nif (unlikely(sge->stopped_tx_queues != 0))\r\nrestart_tx_queues(sge);\r\nreturn flags;\r\n}\r\nstatic int process_responses(struct adapter *adapter, int budget)\r\n{\r\nstruct sge *sge = adapter->sge;\r\nstruct respQ *q = &sge->respQ;\r\nstruct respQ_e *e = &q->entries[q->cidx];\r\nint done = 0;\r\nunsigned int flags = 0;\r\nunsigned int cmdq_processed[SGE_CMDQ_N] = {0, 0};\r\nwhile (done < budget && e->GenerationBit == q->genbit) {\r\nflags |= e->Qsleeping;\r\ncmdq_processed[0] += e->Cmdq0CreditReturn;\r\ncmdq_processed[1] += e->Cmdq1CreditReturn;\r\nif (unlikely((flags & F_CMDQ0_ENABLE) || cmdq_processed[0] > 64)) {\r\nflags = update_tx_info(adapter, flags, cmdq_processed[0]);\r\ncmdq_processed[0] = 0;\r\n}\r\nif (unlikely(cmdq_processed[1] > 16)) {\r\nsge->cmdQ[1].processed += cmdq_processed[1];\r\ncmdq_processed[1] = 0;\r\n}\r\nif (likely(e->DataValid)) {\r\nstruct freelQ *fl = &sge->freelQ[e->FreelistQid];\r\nBUG_ON(!e->Sop || !e->Eop);\r\nif (unlikely(e->Offload))\r\nunexpected_offload(adapter, fl);\r\nelse\r\nsge_rx(sge, fl, e->BufferLength);\r\n++done;\r\nif (++fl->cidx == fl->size)\r\nfl->cidx = 0;\r\nprefetch(fl->centries[fl->cidx].skb);\r\nif (unlikely(--fl->credits <\r\nfl->size - SGE_FREEL_REFILL_THRESH))\r\nrefill_free_list(sge, fl);\r\n} else\r\nsge->stats.pure_rsps++;\r\ne++;\r\nif (unlikely(++q->cidx == q->size)) {\r\nq->cidx = 0;\r\nq->genbit ^= 1;\r\ne = q->entries;\r\n}\r\nprefetch(e);\r\nif (++q->credits > SGE_RESPQ_REPLENISH_THRES) {\r\nwritel(q->credits, adapter->regs + A_SG_RSPQUEUECREDIT);\r\nq->credits = 0;\r\n}\r\n}\r\nflags = update_tx_info(adapter, flags, cmdq_processed[0]);\r\nsge->cmdQ[1].processed += cmdq_processed[1];\r\nreturn done;\r\n}\r\nstatic inline int responses_pending(const struct adapter *adapter)\r\n{\r\nconst struct respQ *Q = &adapter->sge->respQ;\r\nconst struct respQ_e *e = &Q->entries[Q->cidx];\r\nreturn e->GenerationBit == Q->genbit;\r\n}\r\nstatic int process_pure_responses(struct adapter *adapter)\r\n{\r\nstruct sge *sge = adapter->sge;\r\nstruct respQ *q = &sge->respQ;\r\nstruct respQ_e *e = &q->entries[q->cidx];\r\nconst struct freelQ *fl = &sge->freelQ[e->FreelistQid];\r\nunsigned int flags = 0;\r\nunsigned int cmdq_processed[SGE_CMDQ_N] = {0, 0};\r\nprefetch(fl->centries[fl->cidx].skb);\r\nif (e->DataValid)\r\nreturn 1;\r\ndo {\r\nflags |= e->Qsleeping;\r\ncmdq_processed[0] += e->Cmdq0CreditReturn;\r\ncmdq_processed[1] += e->Cmdq1CreditReturn;\r\ne++;\r\nif (unlikely(++q->cidx == q->size)) {\r\nq->cidx = 0;\r\nq->genbit ^= 1;\r\ne = q->entries;\r\n}\r\nprefetch(e);\r\nif (++q->credits > SGE_RESPQ_REPLENISH_THRES) {\r\nwritel(q->credits, adapter->regs + A_SG_RSPQUEUECREDIT);\r\nq->credits = 0;\r\n}\r\nsge->stats.pure_rsps++;\r\n} while (e->GenerationBit == q->genbit && !e->DataValid);\r\nflags = update_tx_info(adapter, flags, cmdq_processed[0]);\r\nsge->cmdQ[1].processed += cmdq_processed[1];\r\nreturn e->GenerationBit == q->genbit;\r\n}\r\nint t1_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct adapter *adapter = container_of(napi, struct adapter, napi);\r\nint work_done = process_responses(adapter, budget);\r\nif (likely(work_done < budget)) {\r\nnapi_complete_done(napi, work_done);\r\nwritel(adapter->sge->respQ.cidx,\r\nadapter->regs + A_SG_SLEEPING);\r\n}\r\nreturn work_done;\r\n}\r\nirqreturn_t t1_interrupt(int irq, void *data)\r\n{\r\nstruct adapter *adapter = data;\r\nstruct sge *sge = adapter->sge;\r\nint handled;\r\nif (likely(responses_pending(adapter))) {\r\nwritel(F_PL_INTR_SGE_DATA, adapter->regs + A_PL_CAUSE);\r\nif (napi_schedule_prep(&adapter->napi)) {\r\nif (process_pure_responses(adapter))\r\n__napi_schedule(&adapter->napi);\r\nelse {\r\nwritel(sge->respQ.cidx, adapter->regs + A_SG_SLEEPING);\r\nnapi_enable(&adapter->napi);\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nspin_lock(&adapter->async_lock);\r\nhandled = t1_slow_intr_handler(adapter);\r\nspin_unlock(&adapter->async_lock);\r\nif (!handled)\r\nsge->stats.unhandled_irqs++;\r\nreturn IRQ_RETVAL(handled != 0);\r\n}\r\nstatic int t1_sge_tx(struct sk_buff *skb, struct adapter *adapter,\r\nunsigned int qid, struct net_device *dev)\r\n{\r\nstruct sge *sge = adapter->sge;\r\nstruct cmdQ *q = &sge->cmdQ[qid];\r\nunsigned int credits, pidx, genbit, count, use_sched_skb = 0;\r\nspin_lock(&q->lock);\r\nreclaim_completed_tx(sge, q);\r\npidx = q->pidx;\r\ncredits = q->size - q->in_use;\r\ncount = 1 + skb_shinfo(skb)->nr_frags;\r\ncount += compute_large_page_tx_descs(skb);\r\nif (unlikely(credits < count)) {\r\nif (!netif_queue_stopped(dev)) {\r\nnetif_stop_queue(dev);\r\nset_bit(dev->if_port, &sge->stopped_tx_queues);\r\nsge->stats.cmdQ_full[2]++;\r\npr_err("%s: Tx ring full while queue awake!\n",\r\nadapter->name);\r\n}\r\nspin_unlock(&q->lock);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nif (unlikely(credits - count < q->stop_thres)) {\r\nnetif_stop_queue(dev);\r\nset_bit(dev->if_port, &sge->stopped_tx_queues);\r\nsge->stats.cmdQ_full[2]++;\r\n}\r\nif (sge->tx_sched && !qid && skb->dev) {\r\nuse_sched:\r\nuse_sched_skb = 1;\r\nskb = sched_skb(sge, skb, credits);\r\nif (!skb) {\r\nspin_unlock(&q->lock);\r\nreturn NETDEV_TX_OK;\r\n}\r\npidx = q->pidx;\r\ncount = 1 + skb_shinfo(skb)->nr_frags;\r\ncount += compute_large_page_tx_descs(skb);\r\n}\r\nq->in_use += count;\r\ngenbit = q->genbit;\r\npidx = q->pidx;\r\nq->pidx += count;\r\nif (q->pidx >= q->size) {\r\nq->pidx -= q->size;\r\nq->genbit ^= 1;\r\n}\r\nspin_unlock(&q->lock);\r\nwrite_tx_descs(adapter, skb, pidx, genbit, q);\r\nif (qid)\r\ndoorbell_pio(adapter, F_CMDQ1_ENABLE);\r\nelse {\r\nclear_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\r\nif (test_and_set_bit(CMDQ_STAT_RUNNING, &q->status) == 0) {\r\nset_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\r\nwritel(F_CMDQ0_ENABLE, adapter->regs + A_SG_DOORBELL);\r\n}\r\n}\r\nif (use_sched_skb) {\r\nif (spin_trylock(&q->lock)) {\r\ncredits = q->size - q->in_use;\r\nskb = NULL;\r\ngoto use_sched;\r\n}\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic inline int eth_hdr_len(const void *data)\r\n{\r\nconst struct ethhdr *e = data;\r\nreturn e->h_proto == htons(ETH_P_8021Q) ? VLAN_ETH_HLEN : ETH_HLEN;\r\n}\r\nnetdev_tx_t t1_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct adapter *adapter = dev->ml_priv;\r\nstruct sge *sge = adapter->sge;\r\nstruct sge_port_stats *st = this_cpu_ptr(sge->port_stats[dev->if_port]);\r\nstruct cpl_tx_pkt *cpl;\r\nstruct sk_buff *orig_skb = skb;\r\nint ret;\r\nif (skb->protocol == htons(ETH_P_CPL5))\r\ngoto send;\r\nif (unlikely(skb_headroom(skb) < dev->hard_header_len - ETH_HLEN)) {\r\nskb = skb_realloc_headroom(skb, sizeof(struct cpl_tx_pkt_lso));\r\n++st->tx_need_hdrroom;\r\ndev_kfree_skb_any(orig_skb);\r\nif (!skb)\r\nreturn NETDEV_TX_OK;\r\n}\r\nif (skb_shinfo(skb)->gso_size) {\r\nint eth_type;\r\nstruct cpl_tx_pkt_lso *hdr;\r\n++st->tx_tso;\r\neth_type = skb_network_offset(skb) == ETH_HLEN ?\r\nCPL_ETH_II : CPL_ETH_II_VLAN;\r\nhdr = skb_push(skb, sizeof(*hdr));\r\nhdr->opcode = CPL_TX_PKT_LSO;\r\nhdr->ip_csum_dis = hdr->l4_csum_dis = 0;\r\nhdr->ip_hdr_words = ip_hdr(skb)->ihl;\r\nhdr->tcp_hdr_words = tcp_hdr(skb)->doff;\r\nhdr->eth_type_mss = htons(MK_ETH_TYPE_MSS(eth_type,\r\nskb_shinfo(skb)->gso_size));\r\nhdr->len = htonl(skb->len - sizeof(*hdr));\r\ncpl = (struct cpl_tx_pkt *)hdr;\r\n} else {\r\nif (unlikely(skb->len < ETH_HLEN ||\r\nskb->len > dev->mtu + eth_hdr_len(skb->data))) {\r\nnetdev_dbg(dev, "packet size %d hdr %d mtu%d\n",\r\nskb->len, eth_hdr_len(skb->data), dev->mtu);\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nif (skb->ip_summed == CHECKSUM_PARTIAL &&\r\nip_hdr(skb)->protocol == IPPROTO_UDP) {\r\nif (unlikely(skb_checksum_help(skb))) {\r\nnetdev_dbg(dev, "unable to do udp checksum\n");\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\n}\r\nif ((unlikely(!adapter->sge->espibug_skb[dev->if_port]))) {\r\nif (skb->protocol == htons(ETH_P_ARP) &&\r\narp_hdr(skb)->ar_op == htons(ARPOP_REQUEST)) {\r\nadapter->sge->espibug_skb[dev->if_port] = skb;\r\nskb = skb_get(skb);\r\n}\r\n}\r\ncpl = __skb_push(skb, sizeof(*cpl));\r\ncpl->opcode = CPL_TX_PKT;\r\ncpl->ip_csum_dis = 1;\r\ncpl->l4_csum_dis = skb->ip_summed == CHECKSUM_PARTIAL ? 0 : 1;\r\nst->tx_cso += (skb->ip_summed == CHECKSUM_PARTIAL);\r\n}\r\ncpl->iff = dev->if_port;\r\nif (skb_vlan_tag_present(skb)) {\r\ncpl->vlan_valid = 1;\r\ncpl->vlan = htons(skb_vlan_tag_get(skb));\r\nst->vlan_insert++;\r\n} else\r\ncpl->vlan_valid = 0;\r\nsend:\r\nret = t1_sge_tx(skb, adapter, 0, dev);\r\nif (unlikely(ret != NETDEV_TX_OK && skb != orig_skb)) {\r\ndev_kfree_skb_any(skb);\r\nret = NETDEV_TX_OK;\r\n}\r\nreturn ret;\r\n}\r\nstatic void sge_tx_reclaim_cb(unsigned long data)\r\n{\r\nint i;\r\nstruct sge *sge = (struct sge *)data;\r\nfor (i = 0; i < SGE_CMDQ_N; ++i) {\r\nstruct cmdQ *q = &sge->cmdQ[i];\r\nif (!spin_trylock(&q->lock))\r\ncontinue;\r\nreclaim_completed_tx(sge, q);\r\nif (i == 0 && q->in_use) {\r\nwritel(F_CMDQ0_ENABLE, sge->adapter->regs + A_SG_DOORBELL);\r\n}\r\nspin_unlock(&q->lock);\r\n}\r\nmod_timer(&sge->tx_reclaim_timer, jiffies + TX_RECLAIM_PERIOD);\r\n}\r\nint t1_sge_set_coalesce_params(struct sge *sge, struct sge_params *p)\r\n{\r\nsge->fixed_intrtimer = p->rx_coalesce_usecs *\r\ncore_ticks_per_usec(sge->adapter);\r\nwritel(sge->fixed_intrtimer, sge->adapter->regs + A_SG_INTRTIMER);\r\nreturn 0;\r\n}\r\nint t1_sge_configure(struct sge *sge, struct sge_params *p)\r\n{\r\nif (alloc_rx_resources(sge, p))\r\nreturn -ENOMEM;\r\nif (alloc_tx_resources(sge, p)) {\r\nfree_rx_resources(sge);\r\nreturn -ENOMEM;\r\n}\r\nconfigure_sge(sge, p);\r\np->large_buf_capacity = jumbo_payload_capacity(sge);\r\nreturn 0;\r\n}\r\nvoid t1_sge_stop(struct sge *sge)\r\n{\r\nint i;\r\nwritel(0, sge->adapter->regs + A_SG_CONTROL);\r\nreadl(sge->adapter->regs + A_SG_CONTROL);\r\nif (is_T2(sge->adapter))\r\ndel_timer_sync(&sge->espibug_timer);\r\ndel_timer_sync(&sge->tx_reclaim_timer);\r\nif (sge->tx_sched)\r\ntx_sched_stop(sge);\r\nfor (i = 0; i < MAX_NPORTS; i++)\r\nkfree_skb(sge->espibug_skb[i]);\r\n}\r\nvoid t1_sge_start(struct sge *sge)\r\n{\r\nrefill_free_list(sge, &sge->freelQ[0]);\r\nrefill_free_list(sge, &sge->freelQ[1]);\r\nwritel(sge->sge_control, sge->adapter->regs + A_SG_CONTROL);\r\ndoorbell_pio(sge->adapter, F_FL0_ENABLE | F_FL1_ENABLE);\r\nreadl(sge->adapter->regs + A_SG_CONTROL);\r\nmod_timer(&sge->tx_reclaim_timer, jiffies + TX_RECLAIM_PERIOD);\r\nif (is_T2(sge->adapter))\r\nmod_timer(&sge->espibug_timer, jiffies + sge->espibug_timeout);\r\n}\r\nstatic void espibug_workaround_t204(unsigned long data)\r\n{\r\nstruct adapter *adapter = (struct adapter *)data;\r\nstruct sge *sge = adapter->sge;\r\nunsigned int nports = adapter->params.nports;\r\nu32 seop[MAX_NPORTS];\r\nif (adapter->open_device_map & PORT_MASK) {\r\nint i;\r\nif (t1_espi_get_mon_t204(adapter, &(seop[0]), 0) < 0)\r\nreturn;\r\nfor (i = 0; i < nports; i++) {\r\nstruct sk_buff *skb = sge->espibug_skb[i];\r\nif (!netif_running(adapter->port[i].dev) ||\r\nnetif_queue_stopped(adapter->port[i].dev) ||\r\n!seop[i] || ((seop[i] & 0xfff) != 0) || !skb)\r\ncontinue;\r\nif (!skb->cb[0]) {\r\nskb_copy_to_linear_data_offset(skb,\r\nsizeof(struct cpl_tx_pkt),\r\nch_mac_addr,\r\nETH_ALEN);\r\nskb_copy_to_linear_data_offset(skb,\r\nskb->len - 10,\r\nch_mac_addr,\r\nETH_ALEN);\r\nskb->cb[0] = 0xff;\r\n}\r\nskb = skb_get(skb);\r\nt1_sge_tx(skb, adapter, 0, adapter->port[i].dev);\r\n}\r\n}\r\nmod_timer(&sge->espibug_timer, jiffies + sge->espibug_timeout);\r\n}\r\nstatic void espibug_workaround(unsigned long data)\r\n{\r\nstruct adapter *adapter = (struct adapter *)data;\r\nstruct sge *sge = adapter->sge;\r\nif (netif_running(adapter->port[0].dev)) {\r\nstruct sk_buff *skb = sge->espibug_skb[0];\r\nu32 seop = t1_espi_get_mon(adapter, 0x930, 0);\r\nif ((seop & 0xfff0fff) == 0xfff && skb) {\r\nif (!skb->cb[0]) {\r\nskb_copy_to_linear_data_offset(skb,\r\nsizeof(struct cpl_tx_pkt),\r\nch_mac_addr,\r\nETH_ALEN);\r\nskb_copy_to_linear_data_offset(skb,\r\nskb->len - 10,\r\nch_mac_addr,\r\nETH_ALEN);\r\nskb->cb[0] = 0xff;\r\n}\r\nskb = skb_get(skb);\r\nt1_sge_tx(skb, adapter, 0, adapter->port[0].dev);\r\n}\r\n}\r\nmod_timer(&sge->espibug_timer, jiffies + sge->espibug_timeout);\r\n}\r\nstruct sge *t1_sge_create(struct adapter *adapter, struct sge_params *p)\r\n{\r\nstruct sge *sge = kzalloc(sizeof(*sge), GFP_KERNEL);\r\nint i;\r\nif (!sge)\r\nreturn NULL;\r\nsge->adapter = adapter;\r\nsge->netdev = adapter->port[0].dev;\r\nsge->rx_pkt_pad = t1_is_T1B(adapter) ? 0 : 2;\r\nsge->jumbo_fl = t1_is_T1B(adapter) ? 1 : 0;\r\nfor_each_port(adapter, i) {\r\nsge->port_stats[i] = alloc_percpu(struct sge_port_stats);\r\nif (!sge->port_stats[i])\r\ngoto nomem_port;\r\n}\r\ninit_timer(&sge->tx_reclaim_timer);\r\nsge->tx_reclaim_timer.data = (unsigned long)sge;\r\nsge->tx_reclaim_timer.function = sge_tx_reclaim_cb;\r\nif (is_T2(sge->adapter)) {\r\ninit_timer(&sge->espibug_timer);\r\nif (adapter->params.nports > 1) {\r\ntx_sched_init(sge);\r\nsge->espibug_timer.function = espibug_workaround_t204;\r\n} else\r\nsge->espibug_timer.function = espibug_workaround;\r\nsge->espibug_timer.data = (unsigned long)sge->adapter;\r\nsge->espibug_timeout = 1;\r\nif (adapter->params.nports > 1)\r\nsge->espibug_timeout = HZ/100;\r\n}\r\np->cmdQ_size[0] = SGE_CMDQ0_E_N;\r\np->cmdQ_size[1] = SGE_CMDQ1_E_N;\r\np->freelQ_size[!sge->jumbo_fl] = SGE_FREEL_SIZE;\r\np->freelQ_size[sge->jumbo_fl] = SGE_JUMBO_FREEL_SIZE;\r\nif (sge->tx_sched) {\r\nif (board_info(sge->adapter)->board == CHBT_BOARD_CHT204)\r\np->rx_coalesce_usecs = 15;\r\nelse\r\np->rx_coalesce_usecs = 50;\r\n} else\r\np->rx_coalesce_usecs = 50;\r\np->coalesce_enable = 0;\r\np->sample_interval_usecs = 0;\r\nreturn sge;\r\nnomem_port:\r\nwhile (i >= 0) {\r\nfree_percpu(sge->port_stats[i]);\r\n--i;\r\n}\r\nkfree(sge);\r\nreturn NULL;\r\n}
