static u32 hhf_time_stamp(void)\r\n{\r\nreturn jiffies;\r\n}\r\nstatic struct hh_flow_state *seek_list(const u32 hash,\r\nstruct list_head *head,\r\nstruct hhf_sched_data *q)\r\n{\r\nstruct hh_flow_state *flow, *next;\r\nu32 now = hhf_time_stamp();\r\nif (list_empty(head))\r\nreturn NULL;\r\nlist_for_each_entry_safe(flow, next, head, flowchain) {\r\nu32 prev = flow->hit_timestamp + q->hhf_evict_timeout;\r\nif (hhf_time_before(prev, now)) {\r\nif (list_is_last(&flow->flowchain, head))\r\nreturn NULL;\r\nlist_del(&flow->flowchain);\r\nkfree(flow);\r\nq->hh_flows_current_cnt--;\r\n} else if (flow->hash_id == hash) {\r\nreturn flow;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct hh_flow_state *alloc_new_hh(struct list_head *head,\r\nstruct hhf_sched_data *q)\r\n{\r\nstruct hh_flow_state *flow;\r\nu32 now = hhf_time_stamp();\r\nif (!list_empty(head)) {\r\nlist_for_each_entry(flow, head, flowchain) {\r\nu32 prev = flow->hit_timestamp + q->hhf_evict_timeout;\r\nif (hhf_time_before(prev, now))\r\nreturn flow;\r\n}\r\n}\r\nif (q->hh_flows_current_cnt >= q->hh_flows_limit) {\r\nq->hh_flows_overlimit++;\r\nreturn NULL;\r\n}\r\nflow = kzalloc(sizeof(struct hh_flow_state), GFP_ATOMIC);\r\nif (!flow)\r\nreturn NULL;\r\nq->hh_flows_current_cnt++;\r\nINIT_LIST_HEAD(&flow->flowchain);\r\nlist_add_tail(&flow->flowchain, head);\r\nreturn flow;\r\n}\r\nstatic enum wdrr_bucket_idx hhf_classify(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nu32 tmp_hash, hash;\r\nu32 xorsum, filter_pos[HHF_ARRAYS_CNT], flow_pos;\r\nstruct hh_flow_state *flow;\r\nu32 pkt_len, min_hhf_val;\r\nint i;\r\nu32 prev;\r\nu32 now = hhf_time_stamp();\r\nprev = q->hhf_arrays_reset_timestamp + q->hhf_reset_timeout;\r\nif (hhf_time_before(prev, now)) {\r\nfor (i = 0; i < HHF_ARRAYS_CNT; i++)\r\nbitmap_zero(q->hhf_valid_bits[i], HHF_ARRAYS_LEN);\r\nq->hhf_arrays_reset_timestamp = now;\r\n}\r\nhash = skb_get_hash_perturb(skb, q->perturbation);\r\nflow_pos = hash & HHF_BIT_MASK;\r\nflow = seek_list(hash, &q->hh_flows[flow_pos], q);\r\nif (flow) {\r\nflow->hit_timestamp = now;\r\nreturn WDRR_BUCKET_FOR_HH;\r\n}\r\ntmp_hash = hash;\r\nxorsum = 0;\r\nfor (i = 0; i < HHF_ARRAYS_CNT - 1; i++) {\r\nfilter_pos[i] = tmp_hash & HHF_BIT_MASK;\r\nxorsum ^= filter_pos[i];\r\ntmp_hash >>= HHF_BIT_MASK_LEN;\r\n}\r\nfilter_pos[HHF_ARRAYS_CNT - 1] = xorsum ^ tmp_hash;\r\npkt_len = qdisc_pkt_len(skb);\r\nmin_hhf_val = ~0U;\r\nfor (i = 0; i < HHF_ARRAYS_CNT; i++) {\r\nu32 val;\r\nif (!test_bit(filter_pos[i], q->hhf_valid_bits[i])) {\r\nq->hhf_arrays[i][filter_pos[i]] = 0;\r\n__set_bit(filter_pos[i], q->hhf_valid_bits[i]);\r\n}\r\nval = q->hhf_arrays[i][filter_pos[i]] + pkt_len;\r\nif (min_hhf_val > val)\r\nmin_hhf_val = val;\r\n}\r\nif (min_hhf_val > q->hhf_admit_bytes) {\r\nflow = alloc_new_hh(&q->hh_flows[flow_pos], q);\r\nif (!flow)\r\nreturn WDRR_BUCKET_FOR_NON_HH;\r\nflow->hash_id = hash;\r\nflow->hit_timestamp = now;\r\nq->hh_flows_total_cnt++;\r\nreturn WDRR_BUCKET_FOR_HH;\r\n}\r\nfor (i = 0; i < HHF_ARRAYS_CNT; i++) {\r\nif (q->hhf_arrays[i][filter_pos[i]] < min_hhf_val)\r\nq->hhf_arrays[i][filter_pos[i]] = min_hhf_val;\r\n}\r\nreturn WDRR_BUCKET_FOR_NON_HH;\r\n}\r\nstatic struct sk_buff *dequeue_head(struct wdrr_bucket *bucket)\r\n{\r\nstruct sk_buff *skb = bucket->head;\r\nbucket->head = skb->next;\r\nskb->next = NULL;\r\nreturn skb;\r\n}\r\nstatic void bucket_add(struct wdrr_bucket *bucket, struct sk_buff *skb)\r\n{\r\nif (bucket->head == NULL)\r\nbucket->head = skb;\r\nelse\r\nbucket->tail->next = skb;\r\nbucket->tail = skb;\r\nskb->next = NULL;\r\n}\r\nstatic unsigned int hhf_drop(struct Qdisc *sch, struct sk_buff **to_free)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nstruct wdrr_bucket *bucket;\r\nbucket = &q->buckets[WDRR_BUCKET_FOR_HH];\r\nif (!bucket->head)\r\nbucket = &q->buckets[WDRR_BUCKET_FOR_NON_HH];\r\nif (bucket->head) {\r\nstruct sk_buff *skb = dequeue_head(bucket);\r\nsch->q.qlen--;\r\nqdisc_qstats_backlog_dec(sch, skb);\r\nqdisc_drop(skb, sch, to_free);\r\n}\r\nreturn bucket - q->buckets;\r\n}\r\nstatic int hhf_enqueue(struct sk_buff *skb, struct Qdisc *sch,\r\nstruct sk_buff **to_free)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nenum wdrr_bucket_idx idx;\r\nstruct wdrr_bucket *bucket;\r\nunsigned int prev_backlog;\r\nidx = hhf_classify(skb, sch);\r\nbucket = &q->buckets[idx];\r\nbucket_add(bucket, skb);\r\nqdisc_qstats_backlog_inc(sch, skb);\r\nif (list_empty(&bucket->bucketchain)) {\r\nunsigned int weight;\r\nif (idx == WDRR_BUCKET_FOR_HH) {\r\nweight = 1;\r\nlist_add_tail(&bucket->bucketchain, &q->old_buckets);\r\n} else {\r\nweight = q->hhf_non_hh_weight;\r\nlist_add_tail(&bucket->bucketchain, &q->new_buckets);\r\n}\r\nbucket->deficit = weight * q->quantum;\r\n}\r\nif (++sch->q.qlen <= sch->limit)\r\nreturn NET_XMIT_SUCCESS;\r\nprev_backlog = sch->qstats.backlog;\r\nq->drop_overlimit++;\r\nif (hhf_drop(sch, to_free) == idx)\r\nreturn NET_XMIT_CN;\r\nqdisc_tree_reduce_backlog(sch, 1, prev_backlog - sch->qstats.backlog);\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic struct sk_buff *hhf_dequeue(struct Qdisc *sch)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb = NULL;\r\nstruct wdrr_bucket *bucket;\r\nstruct list_head *head;\r\nbegin:\r\nhead = &q->new_buckets;\r\nif (list_empty(head)) {\r\nhead = &q->old_buckets;\r\nif (list_empty(head))\r\nreturn NULL;\r\n}\r\nbucket = list_first_entry(head, struct wdrr_bucket, bucketchain);\r\nif (bucket->deficit <= 0) {\r\nint weight = (bucket - q->buckets == WDRR_BUCKET_FOR_HH) ?\r\n1 : q->hhf_non_hh_weight;\r\nbucket->deficit += weight * q->quantum;\r\nlist_move_tail(&bucket->bucketchain, &q->old_buckets);\r\ngoto begin;\r\n}\r\nif (bucket->head) {\r\nskb = dequeue_head(bucket);\r\nsch->q.qlen--;\r\nqdisc_qstats_backlog_dec(sch, skb);\r\n}\r\nif (!skb) {\r\nif ((head == &q->new_buckets) && !list_empty(&q->old_buckets))\r\nlist_move_tail(&bucket->bucketchain, &q->old_buckets);\r\nelse\r\nlist_del_init(&bucket->bucketchain);\r\ngoto begin;\r\n}\r\nqdisc_bstats_update(sch, skb);\r\nbucket->deficit -= qdisc_pkt_len(skb);\r\nreturn skb;\r\n}\r\nstatic void hhf_reset(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nwhile ((skb = hhf_dequeue(sch)) != NULL)\r\nrtnl_kfree_skbs(skb, skb);\r\n}\r\nstatic void hhf_destroy(struct Qdisc *sch)\r\n{\r\nint i;\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nfor (i = 0; i < HHF_ARRAYS_CNT; i++) {\r\nkvfree(q->hhf_arrays[i]);\r\nkvfree(q->hhf_valid_bits[i]);\r\n}\r\nif (!q->hh_flows)\r\nreturn;\r\nfor (i = 0; i < HH_FLOWS_CNT; i++) {\r\nstruct hh_flow_state *flow, *next;\r\nstruct list_head *head = &q->hh_flows[i];\r\nif (list_empty(head))\r\ncontinue;\r\nlist_for_each_entry_safe(flow, next, head, flowchain) {\r\nlist_del(&flow->flowchain);\r\nkfree(flow);\r\n}\r\n}\r\nkvfree(q->hh_flows);\r\n}\r\nstatic int hhf_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_HHF_MAX + 1];\r\nunsigned int qlen, prev_backlog;\r\nint err;\r\nu64 non_hh_quantum;\r\nu32 new_quantum = q->quantum;\r\nu32 new_hhf_non_hh_weight = q->hhf_non_hh_weight;\r\nif (!opt)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_HHF_MAX, opt, hhf_policy, NULL);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_HHF_QUANTUM])\r\nnew_quantum = nla_get_u32(tb[TCA_HHF_QUANTUM]);\r\nif (tb[TCA_HHF_NON_HH_WEIGHT])\r\nnew_hhf_non_hh_weight = nla_get_u32(tb[TCA_HHF_NON_HH_WEIGHT]);\r\nnon_hh_quantum = (u64)new_quantum * new_hhf_non_hh_weight;\r\nif (non_hh_quantum > INT_MAX)\r\nreturn -EINVAL;\r\nsch_tree_lock(sch);\r\nif (tb[TCA_HHF_BACKLOG_LIMIT])\r\nsch->limit = nla_get_u32(tb[TCA_HHF_BACKLOG_LIMIT]);\r\nq->quantum = new_quantum;\r\nq->hhf_non_hh_weight = new_hhf_non_hh_weight;\r\nif (tb[TCA_HHF_HH_FLOWS_LIMIT])\r\nq->hh_flows_limit = nla_get_u32(tb[TCA_HHF_HH_FLOWS_LIMIT]);\r\nif (tb[TCA_HHF_RESET_TIMEOUT]) {\r\nu32 us = nla_get_u32(tb[TCA_HHF_RESET_TIMEOUT]);\r\nq->hhf_reset_timeout = usecs_to_jiffies(us);\r\n}\r\nif (tb[TCA_HHF_ADMIT_BYTES])\r\nq->hhf_admit_bytes = nla_get_u32(tb[TCA_HHF_ADMIT_BYTES]);\r\nif (tb[TCA_HHF_EVICT_TIMEOUT]) {\r\nu32 us = nla_get_u32(tb[TCA_HHF_EVICT_TIMEOUT]);\r\nq->hhf_evict_timeout = usecs_to_jiffies(us);\r\n}\r\nqlen = sch->q.qlen;\r\nprev_backlog = sch->qstats.backlog;\r\nwhile (sch->q.qlen > sch->limit) {\r\nstruct sk_buff *skb = hhf_dequeue(sch);\r\nrtnl_kfree_skbs(skb, skb);\r\n}\r\nqdisc_tree_reduce_backlog(sch, qlen - sch->q.qlen,\r\nprev_backlog - sch->qstats.backlog);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic int hhf_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nint i;\r\nsch->limit = 1000;\r\nq->quantum = psched_mtu(qdisc_dev(sch));\r\nq->perturbation = prandom_u32();\r\nINIT_LIST_HEAD(&q->new_buckets);\r\nINIT_LIST_HEAD(&q->old_buckets);\r\nq->hhf_reset_timeout = HZ / 25;\r\nq->hhf_admit_bytes = 131072;\r\nq->hhf_evict_timeout = HZ;\r\nq->hhf_non_hh_weight = 2;\r\nif (opt) {\r\nint err = hhf_change(sch, opt);\r\nif (err)\r\nreturn err;\r\n}\r\nif (!q->hh_flows) {\r\nq->hh_flows = kvzalloc(HH_FLOWS_CNT *\r\nsizeof(struct list_head), GFP_KERNEL);\r\nif (!q->hh_flows)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < HH_FLOWS_CNT; i++)\r\nINIT_LIST_HEAD(&q->hh_flows[i]);\r\nq->hh_flows_limit = 2 * HH_FLOWS_CNT;\r\nq->hh_flows_overlimit = 0;\r\nq->hh_flows_total_cnt = 0;\r\nq->hh_flows_current_cnt = 0;\r\nfor (i = 0; i < HHF_ARRAYS_CNT; i++) {\r\nq->hhf_arrays[i] = kvzalloc(HHF_ARRAYS_LEN *\r\nsizeof(u32), GFP_KERNEL);\r\nif (!q->hhf_arrays[i]) {\r\nreturn -ENOMEM;\r\n}\r\n}\r\nq->hhf_arrays_reset_timestamp = hhf_time_stamp();\r\nfor (i = 0; i < HHF_ARRAYS_CNT; i++) {\r\nq->hhf_valid_bits[i] = kvzalloc(HHF_ARRAYS_LEN /\r\nBITS_PER_BYTE, GFP_KERNEL);\r\nif (!q->hhf_valid_bits[i]) {\r\nreturn -ENOMEM;\r\n}\r\n}\r\nfor (i = 0; i < WDRR_BUCKET_CNT; i++) {\r\nstruct wdrr_bucket *bucket = q->buckets + i;\r\nINIT_LIST_HEAD(&bucket->bucketchain);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int hhf_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *opts;\r\nopts = nla_nest_start(skb, TCA_OPTIONS);\r\nif (opts == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put_u32(skb, TCA_HHF_BACKLOG_LIMIT, sch->limit) ||\r\nnla_put_u32(skb, TCA_HHF_QUANTUM, q->quantum) ||\r\nnla_put_u32(skb, TCA_HHF_HH_FLOWS_LIMIT, q->hh_flows_limit) ||\r\nnla_put_u32(skb, TCA_HHF_RESET_TIMEOUT,\r\njiffies_to_usecs(q->hhf_reset_timeout)) ||\r\nnla_put_u32(skb, TCA_HHF_ADMIT_BYTES, q->hhf_admit_bytes) ||\r\nnla_put_u32(skb, TCA_HHF_EVICT_TIMEOUT,\r\njiffies_to_usecs(q->hhf_evict_timeout)) ||\r\nnla_put_u32(skb, TCA_HHF_NON_HH_WEIGHT, q->hhf_non_hh_weight))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, opts);\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nstatic int hhf_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\r\n{\r\nstruct hhf_sched_data *q = qdisc_priv(sch);\r\nstruct tc_hhf_xstats st = {\r\n.drop_overlimit = q->drop_overlimit,\r\n.hh_overlimit = q->hh_flows_overlimit,\r\n.hh_tot_count = q->hh_flows_total_cnt,\r\n.hh_cur_count = q->hh_flows_current_cnt,\r\n};\r\nreturn gnet_stats_copy_app(d, &st, sizeof(st));\r\n}\r\nstatic int __init hhf_module_init(void)\r\n{\r\nreturn register_qdisc(&hhf_qdisc_ops);\r\n}\r\nstatic void __exit hhf_module_exit(void)\r\n{\r\nunregister_qdisc(&hhf_qdisc_ops);\r\n}
