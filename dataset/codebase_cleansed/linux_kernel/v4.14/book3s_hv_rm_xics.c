static void ics_rm_check_resend(struct kvmppc_xics *xics,\r\nstruct kvmppc_ics *ics, struct kvmppc_icp *icp)\r\n{\r\nint i;\r\nfor (i = 0; i < KVMPPC_XICS_IRQ_PER_ICS; i++) {\r\nstruct ics_irq_state *state = &ics->irq_state[i];\r\nif (state->resend)\r\nicp_rm_deliver_irq(xics, icp, state->number, true);\r\n}\r\n}\r\nstatic inline void icp_send_hcore_msg(int hcore, struct kvm_vcpu *vcpu)\r\n{\r\nint hcpu;\r\nhcpu = hcore << threads_shift;\r\nkvmppc_host_rm_ops_hv->rm_core[hcore].rm_data = vcpu;\r\nsmp_muxed_ipi_set_message(hcpu, PPC_MSG_RM_HOST_ACTION);\r\nkvmppc_set_host_ipi(hcpu, 1);\r\nsmp_mb();\r\nkvmhv_rm_send_ipi(hcpu);\r\n}\r\nstatic inline void icp_send_hcore_msg(int hcore, struct kvm_vcpu *vcpu) { }\r\nstatic inline int grab_next_hostcore(int start,\r\nstruct kvmppc_host_rm_core *rm_core, int max, int action)\r\n{\r\nbool success;\r\nint core;\r\nunion kvmppc_rm_state old, new;\r\nfor (core = start + 1; core < max; core++) {\r\nold = new = READ_ONCE(rm_core[core].rm_state);\r\nif (!old.in_host || old.rm_action)\r\ncontinue;\r\nnew.rm_action = action;\r\nsuccess = cmpxchg64(&rm_core[core].rm_state.raw,\r\nold.raw, new.raw) == old.raw;\r\nif (success) {\r\nsmp_wmb();\r\nreturn core;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic inline int find_available_hostcore(int action)\r\n{\r\nint core;\r\nint my_core = smp_processor_id() >> threads_shift;\r\nstruct kvmppc_host_rm_core *rm_core = kvmppc_host_rm_ops_hv->rm_core;\r\ncore = grab_next_hostcore(my_core, rm_core, cpu_nr_cores(), action);\r\nif (core == -1)\r\ncore = grab_next_hostcore(core, rm_core, my_core, action);\r\nreturn core;\r\n}\r\nstatic void icp_rm_set_vcpu_irq(struct kvm_vcpu *vcpu,\r\nstruct kvm_vcpu *this_vcpu)\r\n{\r\nstruct kvmppc_icp *this_icp = this_vcpu->arch.icp;\r\nint cpu;\r\nint hcore;\r\nvcpu->stat.queue_intr++;\r\nset_bit(BOOK3S_IRQPRIO_EXTERNAL_LEVEL, &vcpu->arch.pending_exceptions);\r\nif (vcpu == this_vcpu) {\r\nmtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_MER);\r\nreturn;\r\n}\r\ncpu = vcpu->arch.thread_cpu;\r\nif (cpu < 0 || cpu >= nr_cpu_ids) {\r\nhcore = -1;\r\nif (kvmppc_host_rm_ops_hv && h_ipi_redirect)\r\nhcore = find_available_hostcore(XICS_RM_KICK_VCPU);\r\nif (hcore != -1) {\r\nicp_send_hcore_msg(hcore, vcpu);\r\n} else {\r\nthis_icp->rm_action |= XICS_RM_KICK_VCPU;\r\nthis_icp->rm_kick_target = vcpu;\r\n}\r\nreturn;\r\n}\r\nsmp_mb();\r\nkvmhv_rm_send_ipi(cpu);\r\n}\r\nstatic void icp_rm_clr_vcpu_irq(struct kvm_vcpu *vcpu)\r\n{\r\nclear_bit(BOOK3S_IRQPRIO_EXTERNAL_LEVEL,\r\n&vcpu->arch.pending_exceptions);\r\nmtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~LPCR_MER);\r\n}\r\nstatic inline bool icp_rm_try_update(struct kvmppc_icp *icp,\r\nunion kvmppc_icp_state old,\r\nunion kvmppc_icp_state new)\r\n{\r\nstruct kvm_vcpu *this_vcpu = local_paca->kvm_hstate.kvm_vcpu;\r\nbool success;\r\nnew.out_ee = (new.xisr && (new.pending_pri < new.cppr));\r\nsuccess = cmpxchg64(&icp->state.raw, old.raw, new.raw) == old.raw;\r\nif (!success)\r\ngoto bail;\r\nif (new.out_ee)\r\nicp_rm_set_vcpu_irq(icp->vcpu, this_vcpu);\r\nthis_vcpu->arch.icp->rm_dbgstate = new;\r\nthis_vcpu->arch.icp->rm_dbgtgt = icp->vcpu;\r\nbail:\r\nreturn success;\r\n}\r\nstatic inline int check_too_hard(struct kvmppc_xics *xics,\r\nstruct kvmppc_icp *icp)\r\n{\r\nreturn (xics->real_mode_dbg || icp->rm_action) ? H_TOO_HARD : H_SUCCESS;\r\n}\r\nstatic void icp_rm_check_resend(struct kvmppc_xics *xics,\r\nstruct kvmppc_icp *icp)\r\n{\r\nu32 icsid;\r\nsmp_rmb();\r\nfor_each_set_bit(icsid, icp->resend_map, xics->max_icsid + 1) {\r\nstruct kvmppc_ics *ics = xics->ics[icsid];\r\nif (!test_and_clear_bit(icsid, icp->resend_map))\r\ncontinue;\r\nif (!ics)\r\ncontinue;\r\nics_rm_check_resend(xics, ics, icp);\r\n}\r\n}\r\nstatic bool icp_rm_try_to_deliver(struct kvmppc_icp *icp, u32 irq, u8 priority,\r\nu32 *reject)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nbool success;\r\ndo {\r\nold_state = new_state = READ_ONCE(icp->state);\r\n*reject = 0;\r\nsuccess = new_state.cppr > priority &&\r\nnew_state.mfrr > priority &&\r\nnew_state.pending_pri > priority;\r\nif (success) {\r\n*reject = new_state.xisr;\r\nnew_state.xisr = irq;\r\nnew_state.pending_pri = priority;\r\n} else {\r\nnew_state.need_resend = true;\r\n}\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nreturn success;\r\n}\r\nstatic void icp_rm_deliver_irq(struct kvmppc_xics *xics, struct kvmppc_icp *icp,\r\nu32 new_irq, bool check_resend)\r\n{\r\nstruct ics_irq_state *state;\r\nstruct kvmppc_ics *ics;\r\nu32 reject;\r\nu16 src;\r\nagain:\r\nics = kvmppc_xics_find_ics(xics, new_irq, &src);\r\nif (!ics) {\r\nxics->err_noics++;\r\nreturn;\r\n}\r\nstate = &ics->irq_state[src];\r\narch_spin_lock(&ics->lock);\r\nif (!icp || state->server != icp->server_num) {\r\nicp = kvmppc_xics_find_server(xics->kvm, state->server);\r\nif (!icp) {\r\nxics->err_noicp++;\r\ngoto out;\r\n}\r\n}\r\nif (check_resend)\r\nif (!state->resend)\r\ngoto out;\r\nstate->resend = 0;\r\nif (state->priority == MASKED) {\r\nstate->masked_pending = 1;\r\ngoto out;\r\n}\r\nif (icp_rm_try_to_deliver(icp, new_irq, state->priority, &reject)) {\r\nif (reject && reject != XICS_IPI) {\r\narch_spin_unlock(&ics->lock);\r\nicp->n_reject++;\r\nnew_irq = reject;\r\ncheck_resend = 0;\r\ngoto again;\r\n}\r\n} else {\r\nstate->resend = 1;\r\nsmp_wmb();\r\nset_bit(ics->icsid, icp->resend_map);\r\nsmp_mb();\r\nif (!icp->state.need_resend) {\r\nstate->resend = 0;\r\narch_spin_unlock(&ics->lock);\r\ncheck_resend = 0;\r\ngoto again;\r\n}\r\n}\r\nout:\r\narch_spin_unlock(&ics->lock);\r\n}\r\nstatic void icp_rm_down_cppr(struct kvmppc_xics *xics, struct kvmppc_icp *icp,\r\nu8 new_cppr)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nbool resend;\r\ndo {\r\nold_state = new_state = READ_ONCE(icp->state);\r\nnew_state.cppr = new_cppr;\r\nif (new_state.mfrr < new_cppr &&\r\nnew_state.mfrr <= new_state.pending_pri) {\r\nnew_state.pending_pri = new_state.mfrr;\r\nnew_state.xisr = XICS_IPI;\r\n}\r\nresend = new_state.need_resend;\r\nnew_state.need_resend = 0;\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nif (resend) {\r\nicp->n_check_resend++;\r\nicp_rm_check_resend(xics, icp);\r\n}\r\n}\r\nunsigned long xics_rm_h_xirr(struct kvm_vcpu *vcpu)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp = vcpu->arch.icp;\r\nu32 xirr;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nicp_rm_clr_vcpu_irq(icp->vcpu);\r\ndo {\r\nold_state = new_state = READ_ONCE(icp->state);\r\nxirr = old_state.xisr | (((u32)old_state.cppr) << 24);\r\nif (!old_state.xisr)\r\nbreak;\r\nnew_state.cppr = new_state.pending_pri;\r\nnew_state.pending_pri = 0xff;\r\nnew_state.xisr = 0;\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nvcpu->arch.gpr[4] = xirr;\r\nreturn check_too_hard(xics, icp);\r\n}\r\nint xics_rm_h_ipi(struct kvm_vcpu *vcpu, unsigned long server,\r\nunsigned long mfrr)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp, *this_icp = vcpu->arch.icp;\r\nu32 reject;\r\nbool resend;\r\nbool local;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nlocal = this_icp->server_num == server;\r\nif (local)\r\nicp = this_icp;\r\nelse\r\nicp = kvmppc_xics_find_server(vcpu->kvm, server);\r\nif (!icp)\r\nreturn H_PARAMETER;\r\ndo {\r\nold_state = new_state = READ_ONCE(icp->state);\r\nnew_state.mfrr = mfrr;\r\nreject = 0;\r\nresend = false;\r\nif (mfrr < new_state.cppr) {\r\nif (mfrr <= new_state.pending_pri) {\r\nreject = new_state.xisr;\r\nnew_state.pending_pri = mfrr;\r\nnew_state.xisr = XICS_IPI;\r\n}\r\n}\r\nif (mfrr > old_state.mfrr) {\r\nresend = new_state.need_resend;\r\nnew_state.need_resend = 0;\r\n}\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nif (reject && reject != XICS_IPI) {\r\nthis_icp->n_reject++;\r\nicp_rm_deliver_irq(xics, icp, reject, false);\r\n}\r\nif (resend) {\r\nthis_icp->n_check_resend++;\r\nicp_rm_check_resend(xics, icp);\r\n}\r\nreturn check_too_hard(xics, this_icp);\r\n}\r\nint xics_rm_h_cppr(struct kvm_vcpu *vcpu, unsigned long cppr)\r\n{\r\nunion kvmppc_icp_state old_state, new_state;\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp = vcpu->arch.icp;\r\nu32 reject;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nif (cppr > icp->state.cppr) {\r\nicp_rm_down_cppr(xics, icp, cppr);\r\ngoto bail;\r\n} else if (cppr == icp->state.cppr)\r\nreturn H_SUCCESS;\r\nicp_rm_clr_vcpu_irq(icp->vcpu);\r\ndo {\r\nold_state = new_state = READ_ONCE(icp->state);\r\nreject = 0;\r\nnew_state.cppr = cppr;\r\nif (cppr <= new_state.pending_pri) {\r\nreject = new_state.xisr;\r\nnew_state.xisr = 0;\r\nnew_state.pending_pri = 0xff;\r\n}\r\n} while (!icp_rm_try_update(icp, old_state, new_state));\r\nif (reject && reject != XICS_IPI) {\r\nicp->n_reject++;\r\nicp_rm_deliver_irq(xics, icp, reject, false);\r\n}\r\nbail:\r\nreturn check_too_hard(xics, icp);\r\n}\r\nstatic int ics_rm_eoi(struct kvm_vcpu *vcpu, u32 irq)\r\n{\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp = vcpu->arch.icp;\r\nstruct kvmppc_ics *ics;\r\nstruct ics_irq_state *state;\r\nu16 src;\r\nu32 pq_old, pq_new;\r\nics = kvmppc_xics_find_ics(xics, irq, &src);\r\nif (!ics)\r\ngoto bail;\r\nstate = &ics->irq_state[src];\r\nif (state->lsi)\r\npq_new = state->pq_state;\r\nelse\r\ndo {\r\npq_old = state->pq_state;\r\npq_new = pq_old >> 1;\r\n} while (cmpxchg(&state->pq_state, pq_old, pq_new) != pq_old);\r\nif (pq_new & PQ_PRESENTED)\r\nicp_rm_deliver_irq(xics, NULL, irq, false);\r\nif (!hlist_empty(&vcpu->kvm->irq_ack_notifier_list)) {\r\nicp->rm_action |= XICS_RM_NOTIFY_EOI;\r\nicp->rm_eoied_irq = irq;\r\n}\r\nif (state->host_irq) {\r\n++vcpu->stat.pthru_all;\r\nif (state->intr_cpu != -1) {\r\nint pcpu = raw_smp_processor_id();\r\npcpu = cpu_first_thread_sibling(pcpu);\r\n++vcpu->stat.pthru_host;\r\nif (state->intr_cpu != pcpu) {\r\n++vcpu->stat.pthru_bad_aff;\r\nxics_opal_set_server(state->host_irq, pcpu);\r\n}\r\nstate->intr_cpu = -1;\r\n}\r\n}\r\nbail:\r\nreturn check_too_hard(xics, icp);\r\n}\r\nint xics_rm_h_eoi(struct kvm_vcpu *vcpu, unsigned long xirr)\r\n{\r\nstruct kvmppc_xics *xics = vcpu->kvm->arch.xics;\r\nstruct kvmppc_icp *icp = vcpu->arch.icp;\r\nu32 irq = xirr & 0x00ffffff;\r\nif (!xics || !xics->real_mode)\r\nreturn H_TOO_HARD;\r\nicp_rm_down_cppr(xics, icp, xirr >> 24);\r\nif (irq == XICS_IPI)\r\nreturn check_too_hard(xics, icp);\r\nreturn ics_rm_eoi(vcpu, irq);\r\n}\r\nstatic void icp_eoi(struct irq_chip *c, u32 hwirq, __be32 xirr, bool *again)\r\n{\r\nvoid __iomem *xics_phys;\r\nint64_t rc;\r\nrc = pnv_opal_pci_msi_eoi(c, hwirq);\r\nif (rc)\r\neoi_rc = rc;\r\niosync();\r\nxics_phys = local_paca->kvm_hstate.xics_phys;\r\nif (xics_phys) {\r\n__raw_rm_writel(xirr, xics_phys + XICS_XIRR);\r\n} else {\r\nrc = opal_int_eoi(be32_to_cpu(xirr));\r\n*again = rc > 0;\r\n}\r\n}\r\nstatic int xics_opal_set_server(unsigned int hw_irq, int server_cpu)\r\n{\r\nunsigned int mangle_cpu = get_hard_smp_processor_id(server_cpu) << 2;\r\nreturn opal_set_xive(hw_irq, mangle_cpu, DEFAULT_PRIORITY);\r\n}\r\nstatic inline void this_cpu_inc_rm(unsigned int __percpu *addr)\r\n{\r\nunsigned long l;\r\nunsigned int *raddr;\r\nint cpu = smp_processor_id();\r\nraddr = per_cpu_ptr(addr, cpu);\r\nl = (unsigned long)raddr;\r\nif (REGION_ID(l) == VMALLOC_REGION_ID) {\r\nl = vmalloc_to_phys(raddr);\r\nraddr = (unsigned int *)l;\r\n}\r\n++*raddr;\r\n}\r\nstatic void kvmppc_rm_handle_irq_desc(struct irq_desc *desc)\r\n{\r\nthis_cpu_inc_rm(desc->kstat_irqs);\r\n__this_cpu_inc(kstat.irqs_sum);\r\n}\r\nlong kvmppc_deliver_irq_passthru(struct kvm_vcpu *vcpu,\r\n__be32 xirr,\r\nstruct kvmppc_irq_map *irq_map,\r\nstruct kvmppc_passthru_irqmap *pimap,\r\nbool *again)\r\n{\r\nstruct kvmppc_xics *xics;\r\nstruct kvmppc_icp *icp;\r\nstruct kvmppc_ics *ics;\r\nstruct ics_irq_state *state;\r\nu32 irq;\r\nu16 src;\r\nu32 pq_old, pq_new;\r\nirq = irq_map->v_hwirq;\r\nxics = vcpu->kvm->arch.xics;\r\nicp = vcpu->arch.icp;\r\nkvmppc_rm_handle_irq_desc(irq_map->desc);\r\nics = kvmppc_xics_find_ics(xics, irq, &src);\r\nif (!ics)\r\nreturn 2;\r\nstate = &ics->irq_state[src];\r\ndo {\r\npq_old = state->pq_state;\r\npq_new = ((pq_old << 1) & 3) | PQ_PRESENTED;\r\n} while (cmpxchg(&state->pq_state, pq_old, pq_new) != pq_old);\r\nif (pq_new == PQ_PRESENTED)\r\nicp_rm_deliver_irq(xics, icp, irq, false);\r\nicp_eoi(irq_desc_get_chip(irq_map->desc), irq_map->r_hwirq, xirr,\r\nagain);\r\nif (check_too_hard(xics, icp) == H_TOO_HARD)\r\nreturn 2;\r\nelse\r\nreturn -2;\r\n}\r\nstatic void rm_host_ipi_action(int action, void *data)\r\n{\r\nswitch (action) {\r\ncase XICS_RM_KICK_VCPU:\r\nkvmppc_host_rm_ops_hv->vcpu_kick(data);\r\nbreak;\r\ndefault:\r\nWARN(1, "Unexpected rm_action=%d data=%p\n", action, data);\r\nbreak;\r\n}\r\n}\r\nvoid kvmppc_xics_ipi_action(void)\r\n{\r\nint core;\r\nunsigned int cpu = smp_processor_id();\r\nstruct kvmppc_host_rm_core *rm_corep;\r\ncore = cpu >> threads_shift;\r\nrm_corep = &kvmppc_host_rm_ops_hv->rm_core[core];\r\nif (rm_corep->rm_data) {\r\nrm_host_ipi_action(rm_corep->rm_state.rm_action,\r\nrm_corep->rm_data);\r\nrm_corep->rm_data = NULL;\r\nsmp_wmb();\r\nrm_corep->rm_state.rm_action = 0;\r\n}\r\n}
