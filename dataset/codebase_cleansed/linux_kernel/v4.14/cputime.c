void enable_sched_clock_irqtime(void)\r\n{\r\nsched_clock_irqtime = 1;\r\n}\r\nvoid disable_sched_clock_irqtime(void)\r\n{\r\nsched_clock_irqtime = 0;\r\n}\r\nstatic void irqtime_account_delta(struct irqtime *irqtime, u64 delta,\r\nenum cpu_usage_stat idx)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\nu64_stats_update_begin(&irqtime->sync);\r\ncpustat[idx] += delta;\r\nirqtime->total += delta;\r\nirqtime->tick_delta += delta;\r\nu64_stats_update_end(&irqtime->sync);\r\n}\r\nvoid irqtime_account_irq(struct task_struct *curr)\r\n{\r\nstruct irqtime *irqtime = this_cpu_ptr(&cpu_irqtime);\r\ns64 delta;\r\nint cpu;\r\nif (!sched_clock_irqtime)\r\nreturn;\r\ncpu = smp_processor_id();\r\ndelta = sched_clock_cpu(cpu) - irqtime->irq_start_time;\r\nirqtime->irq_start_time += delta;\r\nif (hardirq_count())\r\nirqtime_account_delta(irqtime, delta, CPUTIME_IRQ);\r\nelse if (in_serving_softirq() && curr != this_cpu_ksoftirqd())\r\nirqtime_account_delta(irqtime, delta, CPUTIME_SOFTIRQ);\r\n}\r\nstatic u64 irqtime_tick_accounted(u64 maxtime)\r\n{\r\nstruct irqtime *irqtime = this_cpu_ptr(&cpu_irqtime);\r\nu64 delta;\r\ndelta = min(irqtime->tick_delta, maxtime);\r\nirqtime->tick_delta -= delta;\r\nreturn delta;\r\n}\r\nstatic u64 irqtime_tick_accounted(u64 dummy)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void task_group_account_field(struct task_struct *p, int index,\r\nu64 tmp)\r\n{\r\n__this_cpu_add(kernel_cpustat.cpustat[index], tmp);\r\ncpuacct_account_field(p, index, tmp);\r\n}\r\nvoid account_user_time(struct task_struct *p, u64 cputime)\r\n{\r\nint index;\r\np->utime += cputime;\r\naccount_group_user_time(p, cputime);\r\nindex = (task_nice(p) > 0) ? CPUTIME_NICE : CPUTIME_USER;\r\ntask_group_account_field(p, index, cputime);\r\nacct_account_cputime(p);\r\n}\r\nvoid account_guest_time(struct task_struct *p, u64 cputime)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\np->utime += cputime;\r\naccount_group_user_time(p, cputime);\r\np->gtime += cputime;\r\nif (task_nice(p) > 0) {\r\ncpustat[CPUTIME_NICE] += cputime;\r\ncpustat[CPUTIME_GUEST_NICE] += cputime;\r\n} else {\r\ncpustat[CPUTIME_USER] += cputime;\r\ncpustat[CPUTIME_GUEST] += cputime;\r\n}\r\n}\r\nvoid account_system_index_time(struct task_struct *p,\r\nu64 cputime, enum cpu_usage_stat index)\r\n{\r\np->stime += cputime;\r\naccount_group_system_time(p, cputime);\r\ntask_group_account_field(p, index, cputime);\r\nacct_account_cputime(p);\r\n}\r\nvoid account_system_time(struct task_struct *p, int hardirq_offset, u64 cputime)\r\n{\r\nint index;\r\nif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0)) {\r\naccount_guest_time(p, cputime);\r\nreturn;\r\n}\r\nif (hardirq_count() - hardirq_offset)\r\nindex = CPUTIME_IRQ;\r\nelse if (in_serving_softirq())\r\nindex = CPUTIME_SOFTIRQ;\r\nelse\r\nindex = CPUTIME_SYSTEM;\r\naccount_system_index_time(p, cputime, index);\r\n}\r\nvoid account_steal_time(u64 cputime)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\ncpustat[CPUTIME_STEAL] += cputime;\r\n}\r\nvoid account_idle_time(u64 cputime)\r\n{\r\nu64 *cpustat = kcpustat_this_cpu->cpustat;\r\nstruct rq *rq = this_rq();\r\nif (atomic_read(&rq->nr_iowait) > 0)\r\ncpustat[CPUTIME_IOWAIT] += cputime;\r\nelse\r\ncpustat[CPUTIME_IDLE] += cputime;\r\n}\r\nstatic __always_inline u64 steal_account_process_time(u64 maxtime)\r\n{\r\n#ifdef CONFIG_PARAVIRT\r\nif (static_key_false(&paravirt_steal_enabled)) {\r\nu64 steal;\r\nsteal = paravirt_steal_clock(smp_processor_id());\r\nsteal -= this_rq()->prev_steal_time;\r\nsteal = min(steal, maxtime);\r\naccount_steal_time(steal);\r\nthis_rq()->prev_steal_time += steal;\r\nreturn steal;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic inline u64 account_other_time(u64 max)\r\n{\r\nu64 accounted;\r\nWARN_ON_ONCE(!irqs_disabled());\r\naccounted = steal_account_process_time(max);\r\nif (accounted < max)\r\naccounted += irqtime_tick_accounted(max - accounted);\r\nreturn accounted;\r\n}\r\nstatic inline u64 read_sum_exec_runtime(struct task_struct *t)\r\n{\r\nreturn t->se.sum_exec_runtime;\r\n}\r\nstatic u64 read_sum_exec_runtime(struct task_struct *t)\r\n{\r\nu64 ns;\r\nstruct rq_flags rf;\r\nstruct rq *rq;\r\nrq = task_rq_lock(t, &rf);\r\nns = t->se.sum_exec_runtime;\r\ntask_rq_unlock(rq, t, &rf);\r\nreturn ns;\r\n}\r\nvoid thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)\r\n{\r\nstruct signal_struct *sig = tsk->signal;\r\nu64 utime, stime;\r\nstruct task_struct *t;\r\nunsigned int seq, nextseq;\r\nunsigned long flags;\r\nif (same_thread_group(current, tsk))\r\n(void) task_sched_runtime(current);\r\nrcu_read_lock();\r\nnextseq = 0;\r\ndo {\r\nseq = nextseq;\r\nflags = read_seqbegin_or_lock_irqsave(&sig->stats_lock, &seq);\r\ntimes->utime = sig->utime;\r\ntimes->stime = sig->stime;\r\ntimes->sum_exec_runtime = sig->sum_sched_runtime;\r\nfor_each_thread(tsk, t) {\r\ntask_cputime(t, &utime, &stime);\r\ntimes->utime += utime;\r\ntimes->stime += stime;\r\ntimes->sum_exec_runtime += read_sum_exec_runtime(t);\r\n}\r\nnextseq = 1;\r\n} while (need_seqretry(&sig->stats_lock, seq));\r\ndone_seqretry_irqrestore(&sig->stats_lock, seq, flags);\r\nrcu_read_unlock();\r\n}\r\nstatic void irqtime_account_process_tick(struct task_struct *p, int user_tick,\r\nstruct rq *rq, int ticks)\r\n{\r\nu64 other, cputime = TICK_NSEC * ticks;\r\nother = account_other_time(ULONG_MAX);\r\nif (other >= cputime)\r\nreturn;\r\ncputime -= other;\r\nif (this_cpu_ksoftirqd() == p) {\r\naccount_system_index_time(p, cputime, CPUTIME_SOFTIRQ);\r\n} else if (user_tick) {\r\naccount_user_time(p, cputime);\r\n} else if (p == rq->idle) {\r\naccount_idle_time(cputime);\r\n} else if (p->flags & PF_VCPU) {\r\naccount_guest_time(p, cputime);\r\n} else {\r\naccount_system_index_time(p, cputime, CPUTIME_SYSTEM);\r\n}\r\n}\r\nstatic void irqtime_account_idle_ticks(int ticks)\r\n{\r\nstruct rq *rq = this_rq();\r\nirqtime_account_process_tick(current, 0, rq, ticks);\r\n}\r\nstatic inline void irqtime_account_idle_ticks(int ticks) {}\r\nstatic inline void irqtime_account_process_tick(struct task_struct *p, int user_tick,\r\nstruct rq *rq, int nr_ticks) {}\r\nvoid vtime_common_task_switch(struct task_struct *prev)\r\n{\r\nif (is_idle_task(prev))\r\nvtime_account_idle(prev);\r\nelse\r\nvtime_account_system(prev);\r\nvtime_flush(prev);\r\narch_vtime_task_switch(prev);\r\n}\r\nvoid vtime_account_irq_enter(struct task_struct *tsk)\r\n{\r\nif (!in_interrupt() && is_idle_task(tsk))\r\nvtime_account_idle(tsk);\r\nelse\r\nvtime_account_system(tsk);\r\n}\r\nvoid task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\r\n{\r\n*ut = p->utime;\r\n*st = p->stime;\r\n}\r\nvoid thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\r\n{\r\nstruct task_cputime cputime;\r\nthread_group_cputime(p, &cputime);\r\n*ut = cputime.utime;\r\n*st = cputime.stime;\r\n}\r\nvoid account_process_tick(struct task_struct *p, int user_tick)\r\n{\r\nu64 cputime, steal;\r\nstruct rq *rq = this_rq();\r\nif (vtime_accounting_cpu_enabled())\r\nreturn;\r\nif (sched_clock_irqtime) {\r\nirqtime_account_process_tick(p, user_tick, rq, 1);\r\nreturn;\r\n}\r\ncputime = TICK_NSEC;\r\nsteal = steal_account_process_time(ULONG_MAX);\r\nif (steal >= cputime)\r\nreturn;\r\ncputime -= steal;\r\nif (user_tick)\r\naccount_user_time(p, cputime);\r\nelse if ((p != rq->idle) || (irq_count() != HARDIRQ_OFFSET))\r\naccount_system_time(p, HARDIRQ_OFFSET, cputime);\r\nelse\r\naccount_idle_time(cputime);\r\n}\r\nvoid account_idle_ticks(unsigned long ticks)\r\n{\r\nu64 cputime, steal;\r\nif (sched_clock_irqtime) {\r\nirqtime_account_idle_ticks(ticks);\r\nreturn;\r\n}\r\ncputime = ticks * TICK_NSEC;\r\nsteal = steal_account_process_time(ULONG_MAX);\r\nif (steal >= cputime)\r\nreturn;\r\ncputime -= steal;\r\naccount_idle_time(cputime);\r\n}\r\nstatic u64 scale_stime(u64 stime, u64 rtime, u64 total)\r\n{\r\nu64 scaled;\r\nfor (;;) {\r\nif (stime > rtime)\r\nswap(rtime, stime);\r\nif (total >> 32)\r\ngoto drop_precision;\r\nif (!(rtime >> 32))\r\nbreak;\r\nif (stime >> 31)\r\ngoto drop_precision;\r\nstime <<= 1;\r\nrtime >>= 1;\r\ncontinue;\r\ndrop_precision:\r\nrtime >>= 1;\r\ntotal >>= 1;\r\n}\r\nscaled = div_u64((u64) (u32) stime * (u64) (u32) rtime, (u32)total);\r\nreturn scaled;\r\n}\r\nstatic void cputime_adjust(struct task_cputime *curr,\r\nstruct prev_cputime *prev,\r\nu64 *ut, u64 *st)\r\n{\r\nu64 rtime, stime, utime;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&prev->lock, flags);\r\nrtime = curr->sum_exec_runtime;\r\nif (prev->stime + prev->utime >= rtime)\r\ngoto out;\r\nstime = curr->stime;\r\nutime = curr->utime;\r\nif (stime == 0) {\r\nutime = rtime;\r\ngoto update;\r\n}\r\nif (utime == 0) {\r\nstime = rtime;\r\ngoto update;\r\n}\r\nstime = scale_stime(stime, rtime, stime + utime);\r\nupdate:\r\nif (stime < prev->stime)\r\nstime = prev->stime;\r\nutime = rtime - stime;\r\nif (utime < prev->utime) {\r\nutime = prev->utime;\r\nstime = rtime - utime;\r\n}\r\nprev->stime = stime;\r\nprev->utime = utime;\r\nout:\r\n*ut = prev->utime;\r\n*st = prev->stime;\r\nraw_spin_unlock_irqrestore(&prev->lock, flags);\r\n}\r\nvoid task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\r\n{\r\nstruct task_cputime cputime = {\r\n.sum_exec_runtime = p->se.sum_exec_runtime,\r\n};\r\ntask_cputime(p, &cputime.utime, &cputime.stime);\r\ncputime_adjust(&cputime, &p->prev_cputime, ut, st);\r\n}\r\nvoid thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\r\n{\r\nstruct task_cputime cputime;\r\nthread_group_cputime(p, &cputime);\r\ncputime_adjust(&cputime, &p->signal->prev_cputime, ut, st);\r\n}\r\nstatic u64 vtime_delta(struct vtime *vtime)\r\n{\r\nunsigned long long clock;\r\nclock = sched_clock();\r\nif (clock < vtime->starttime)\r\nreturn 0;\r\nreturn clock - vtime->starttime;\r\n}\r\nstatic u64 get_vtime_delta(struct vtime *vtime)\r\n{\r\nu64 delta = vtime_delta(vtime);\r\nu64 other;\r\nother = account_other_time(delta);\r\nWARN_ON_ONCE(vtime->state == VTIME_INACTIVE);\r\nvtime->starttime += delta;\r\nreturn delta - other;\r\n}\r\nstatic void __vtime_account_system(struct task_struct *tsk,\r\nstruct vtime *vtime)\r\n{\r\nvtime->stime += get_vtime_delta(vtime);\r\nif (vtime->stime >= TICK_NSEC) {\r\naccount_system_time(tsk, irq_count(), vtime->stime);\r\nvtime->stime = 0;\r\n}\r\n}\r\nstatic void vtime_account_guest(struct task_struct *tsk,\r\nstruct vtime *vtime)\r\n{\r\nvtime->gtime += get_vtime_delta(vtime);\r\nif (vtime->gtime >= TICK_NSEC) {\r\naccount_guest_time(tsk, vtime->gtime);\r\nvtime->gtime = 0;\r\n}\r\n}\r\nvoid vtime_account_system(struct task_struct *tsk)\r\n{\r\nstruct vtime *vtime = &tsk->vtime;\r\nif (!vtime_delta(vtime))\r\nreturn;\r\nwrite_seqcount_begin(&vtime->seqcount);\r\nif (current->flags & PF_VCPU)\r\nvtime_account_guest(tsk, vtime);\r\nelse\r\n__vtime_account_system(tsk, vtime);\r\nwrite_seqcount_end(&vtime->seqcount);\r\n}\r\nvoid vtime_user_enter(struct task_struct *tsk)\r\n{\r\nstruct vtime *vtime = &tsk->vtime;\r\nwrite_seqcount_begin(&vtime->seqcount);\r\n__vtime_account_system(tsk, vtime);\r\nvtime->state = VTIME_USER;\r\nwrite_seqcount_end(&vtime->seqcount);\r\n}\r\nvoid vtime_user_exit(struct task_struct *tsk)\r\n{\r\nstruct vtime *vtime = &tsk->vtime;\r\nwrite_seqcount_begin(&vtime->seqcount);\r\nvtime->utime += get_vtime_delta(vtime);\r\nif (vtime->utime >= TICK_NSEC) {\r\naccount_user_time(tsk, vtime->utime);\r\nvtime->utime = 0;\r\n}\r\nvtime->state = VTIME_SYS;\r\nwrite_seqcount_end(&vtime->seqcount);\r\n}\r\nvoid vtime_guest_enter(struct task_struct *tsk)\r\n{\r\nstruct vtime *vtime = &tsk->vtime;\r\nwrite_seqcount_begin(&vtime->seqcount);\r\n__vtime_account_system(tsk, vtime);\r\ncurrent->flags |= PF_VCPU;\r\nwrite_seqcount_end(&vtime->seqcount);\r\n}\r\nvoid vtime_guest_exit(struct task_struct *tsk)\r\n{\r\nstruct vtime *vtime = &tsk->vtime;\r\nwrite_seqcount_begin(&vtime->seqcount);\r\nvtime_account_guest(tsk, vtime);\r\ncurrent->flags &= ~PF_VCPU;\r\nwrite_seqcount_end(&vtime->seqcount);\r\n}\r\nvoid vtime_account_idle(struct task_struct *tsk)\r\n{\r\naccount_idle_time(get_vtime_delta(&tsk->vtime));\r\n}\r\nvoid arch_vtime_task_switch(struct task_struct *prev)\r\n{\r\nstruct vtime *vtime = &prev->vtime;\r\nwrite_seqcount_begin(&vtime->seqcount);\r\nvtime->state = VTIME_INACTIVE;\r\nwrite_seqcount_end(&vtime->seqcount);\r\nvtime = &current->vtime;\r\nwrite_seqcount_begin(&vtime->seqcount);\r\nvtime->state = VTIME_SYS;\r\nvtime->starttime = sched_clock();\r\nwrite_seqcount_end(&vtime->seqcount);\r\n}\r\nvoid vtime_init_idle(struct task_struct *t, int cpu)\r\n{\r\nstruct vtime *vtime = &t->vtime;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nwrite_seqcount_begin(&vtime->seqcount);\r\nvtime->state = VTIME_SYS;\r\nvtime->starttime = sched_clock();\r\nwrite_seqcount_end(&vtime->seqcount);\r\nlocal_irq_restore(flags);\r\n}\r\nu64 task_gtime(struct task_struct *t)\r\n{\r\nstruct vtime *vtime = &t->vtime;\r\nunsigned int seq;\r\nu64 gtime;\r\nif (!vtime_accounting_enabled())\r\nreturn t->gtime;\r\ndo {\r\nseq = read_seqcount_begin(&vtime->seqcount);\r\ngtime = t->gtime;\r\nif (vtime->state == VTIME_SYS && t->flags & PF_VCPU)\r\ngtime += vtime->gtime + vtime_delta(vtime);\r\n} while (read_seqcount_retry(&vtime->seqcount, seq));\r\nreturn gtime;\r\n}\r\nvoid task_cputime(struct task_struct *t, u64 *utime, u64 *stime)\r\n{\r\nstruct vtime *vtime = &t->vtime;\r\nunsigned int seq;\r\nu64 delta;\r\nif (!vtime_accounting_enabled()) {\r\n*utime = t->utime;\r\n*stime = t->stime;\r\nreturn;\r\n}\r\ndo {\r\nseq = read_seqcount_begin(&vtime->seqcount);\r\n*utime = t->utime;\r\n*stime = t->stime;\r\nif (vtime->state == VTIME_INACTIVE || is_idle_task(t))\r\ncontinue;\r\ndelta = vtime_delta(vtime);\r\nif (vtime->state == VTIME_USER || t->flags & PF_VCPU)\r\n*utime += vtime->utime + delta;\r\nelse if (vtime->state == VTIME_SYS)\r\n*stime += vtime->stime + delta;\r\n} while (read_seqcount_retry(&vtime->seqcount, seq));\r\n}
