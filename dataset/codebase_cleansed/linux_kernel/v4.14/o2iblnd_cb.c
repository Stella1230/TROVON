static void\r\nkiblnd_tx_done(struct lnet_ni *ni, struct kib_tx *tx)\r\n{\r\nstruct lnet_msg *lntmsg[2];\r\nstruct kib_net *net = ni->ni_data;\r\nint rc;\r\nint i;\r\nLASSERT(net);\r\nLASSERT(!in_interrupt());\r\nLASSERT(!tx->tx_queued);\r\nLASSERT(!tx->tx_sending);\r\nLASSERT(!tx->tx_waiting);\r\nLASSERT(tx->tx_pool);\r\nkiblnd_unmap_tx(ni, tx);\r\nlntmsg[0] = tx->tx_lntmsg[0]; tx->tx_lntmsg[0] = NULL;\r\nlntmsg[1] = tx->tx_lntmsg[1]; tx->tx_lntmsg[1] = NULL;\r\nrc = tx->tx_status;\r\nif (tx->tx_conn) {\r\nLASSERT(ni == tx->tx_conn->ibc_peer->ibp_ni);\r\nkiblnd_conn_decref(tx->tx_conn);\r\ntx->tx_conn = NULL;\r\n}\r\ntx->tx_nwrq = 0;\r\ntx->tx_status = 0;\r\nkiblnd_pool_free_node(&tx->tx_pool->tpo_pool, &tx->tx_list);\r\nfor (i = 0; i < 2; i++) {\r\nif (!lntmsg[i])\r\ncontinue;\r\nlnet_finalize(ni, lntmsg[i], rc);\r\n}\r\n}\r\nvoid\r\nkiblnd_txlist_done(struct lnet_ni *ni, struct list_head *txlist, int status)\r\n{\r\nstruct kib_tx *tx;\r\nwhile (!list_empty(txlist)) {\r\ntx = list_entry(txlist->next, struct kib_tx, tx_list);\r\nlist_del(&tx->tx_list);\r\ntx->tx_waiting = 0;\r\ntx->tx_status = status;\r\nkiblnd_tx_done(ni, tx);\r\n}\r\n}\r\nstatic struct kib_tx *\r\nkiblnd_get_idle_tx(struct lnet_ni *ni, lnet_nid_t target)\r\n{\r\nstruct kib_net *net = (struct kib_net *)ni->ni_data;\r\nstruct list_head *node;\r\nstruct kib_tx *tx;\r\nstruct kib_tx_poolset *tps;\r\ntps = net->ibn_tx_ps[lnet_cpt_of_nid(target)];\r\nnode = kiblnd_pool_alloc_node(&tps->tps_poolset);\r\nif (!node)\r\nreturn NULL;\r\ntx = list_entry(node, struct kib_tx, tx_list);\r\nLASSERT(!tx->tx_nwrq);\r\nLASSERT(!tx->tx_queued);\r\nLASSERT(!tx->tx_sending);\r\nLASSERT(!tx->tx_waiting);\r\nLASSERT(!tx->tx_status);\r\nLASSERT(!tx->tx_conn);\r\nLASSERT(!tx->tx_lntmsg[0]);\r\nLASSERT(!tx->tx_lntmsg[1]);\r\nLASSERT(!tx->tx_nfrags);\r\nreturn tx;\r\n}\r\nstatic void\r\nkiblnd_drop_rx(struct kib_rx *rx)\r\n{\r\nstruct kib_conn *conn = rx->rx_conn;\r\nstruct kib_sched_info *sched = conn->ibc_sched;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\nLASSERT(conn->ibc_nrx > 0);\r\nconn->ibc_nrx--;\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\nkiblnd_conn_decref(conn);\r\n}\r\nint\r\nkiblnd_post_rx(struct kib_rx *rx, int credit)\r\n{\r\nstruct kib_conn *conn = rx->rx_conn;\r\nstruct kib_net *net = conn->ibc_peer->ibp_ni->ni_data;\r\nstruct ib_recv_wr *bad_wrq = NULL;\r\nint rc;\r\nLASSERT(net);\r\nLASSERT(!in_interrupt());\r\nLASSERT(credit == IBLND_POSTRX_NO_CREDIT ||\r\ncredit == IBLND_POSTRX_PEER_CREDIT ||\r\ncredit == IBLND_POSTRX_RSRVD_CREDIT);\r\nrx->rx_sge.lkey = conn->ibc_hdev->ibh_pd->local_dma_lkey;\r\nrx->rx_sge.addr = rx->rx_msgaddr;\r\nrx->rx_sge.length = IBLND_MSG_SIZE;\r\nrx->rx_wrq.next = NULL;\r\nrx->rx_wrq.sg_list = &rx->rx_sge;\r\nrx->rx_wrq.num_sge = 1;\r\nrx->rx_wrq.wr_id = kiblnd_ptr2wreqid(rx, IBLND_WID_RX);\r\nLASSERT(conn->ibc_state >= IBLND_CONN_INIT);\r\nLASSERT(rx->rx_nob >= 0);\r\nif (conn->ibc_state > IBLND_CONN_ESTABLISHED) {\r\nkiblnd_drop_rx(rx);\r\nreturn 0;\r\n}\r\nrx->rx_nob = -1;\r\nkiblnd_conn_addref(conn);\r\nrc = ib_post_recv(conn->ibc_cmid->qp, &rx->rx_wrq, &bad_wrq);\r\nif (unlikely(rc)) {\r\nCERROR("Can't post rx for %s: %d, bad_wrq: %p\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), rc, bad_wrq);\r\nrx->rx_nob = 0;\r\n}\r\nif (conn->ibc_state < IBLND_CONN_ESTABLISHED)\r\ngoto out;\r\nif (unlikely(rc)) {\r\nkiblnd_close_conn(conn, rc);\r\nkiblnd_drop_rx(rx);\r\ngoto out;\r\n}\r\nif (credit == IBLND_POSTRX_NO_CREDIT)\r\ngoto out;\r\nspin_lock(&conn->ibc_lock);\r\nif (credit == IBLND_POSTRX_PEER_CREDIT)\r\nconn->ibc_outstanding_credits++;\r\nelse\r\nconn->ibc_reserved_credits++;\r\nkiblnd_check_sends_locked(conn);\r\nspin_unlock(&conn->ibc_lock);\r\nout:\r\nkiblnd_conn_decref(conn);\r\nreturn rc;\r\n}\r\nstatic struct kib_tx *\r\nkiblnd_find_waiting_tx_locked(struct kib_conn *conn, int txtype, __u64 cookie)\r\n{\r\nstruct list_head *tmp;\r\nlist_for_each(tmp, &conn->ibc_active_txs) {\r\nstruct kib_tx *tx = list_entry(tmp, struct kib_tx, tx_list);\r\nLASSERT(!tx->tx_queued);\r\nLASSERT(tx->tx_sending || tx->tx_waiting);\r\nif (tx->tx_cookie != cookie)\r\ncontinue;\r\nif (tx->tx_waiting &&\r\ntx->tx_msg->ibm_type == txtype)\r\nreturn tx;\r\nCWARN("Bad completion: %swaiting, type %x (wanted %x)\n",\r\ntx->tx_waiting ? "" : "NOT ",\r\ntx->tx_msg->ibm_type, txtype);\r\n}\r\nreturn NULL;\r\n}\r\nstatic void\r\nkiblnd_handle_completion(struct kib_conn *conn, int txtype, int status, __u64 cookie)\r\n{\r\nstruct kib_tx *tx;\r\nstruct lnet_ni *ni = conn->ibc_peer->ibp_ni;\r\nint idle;\r\nspin_lock(&conn->ibc_lock);\r\ntx = kiblnd_find_waiting_tx_locked(conn, txtype, cookie);\r\nif (!tx) {\r\nspin_unlock(&conn->ibc_lock);\r\nCWARN("Unmatched completion type %x cookie %#llx from %s\n",\r\ntxtype, cookie, libcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nkiblnd_close_conn(conn, -EPROTO);\r\nreturn;\r\n}\r\nif (!tx->tx_status) {\r\nif (status < 0)\r\ntx->tx_status = status;\r\nelse if (txtype == IBLND_MSG_GET_REQ)\r\nlnet_set_reply_msg_len(ni, tx->tx_lntmsg[1], status);\r\n}\r\ntx->tx_waiting = 0;\r\nidle = !tx->tx_queued && !tx->tx_sending;\r\nif (idle)\r\nlist_del(&tx->tx_list);\r\nspin_unlock(&conn->ibc_lock);\r\nif (idle)\r\nkiblnd_tx_done(ni, tx);\r\n}\r\nstatic void\r\nkiblnd_send_completion(struct kib_conn *conn, int type, int status, __u64 cookie)\r\n{\r\nstruct lnet_ni *ni = conn->ibc_peer->ibp_ni;\r\nstruct kib_tx *tx = kiblnd_get_idle_tx(ni, conn->ibc_peer->ibp_nid);\r\nif (!tx) {\r\nCERROR("Can't get tx for completion %x for %s\n",\r\ntype, libcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nreturn;\r\n}\r\ntx->tx_msg->ibm_u.completion.ibcm_status = status;\r\ntx->tx_msg->ibm_u.completion.ibcm_cookie = cookie;\r\nkiblnd_init_tx_msg(ni, tx, type, sizeof(struct kib_completion_msg));\r\nkiblnd_queue_tx(tx, conn);\r\n}\r\nstatic void\r\nkiblnd_handle_rx(struct kib_rx *rx)\r\n{\r\nstruct kib_msg *msg = rx->rx_msg;\r\nstruct kib_conn *conn = rx->rx_conn;\r\nstruct lnet_ni *ni = conn->ibc_peer->ibp_ni;\r\nint credits = msg->ibm_credits;\r\nstruct kib_tx *tx;\r\nint rc = 0;\r\nint rc2;\r\nint post_credit;\r\nLASSERT(conn->ibc_state >= IBLND_CONN_ESTABLISHED);\r\nCDEBUG(D_NET, "Received %x[%d] from %s\n",\r\nmsg->ibm_type, credits,\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nif (credits) {\r\nspin_lock(&conn->ibc_lock);\r\nif (conn->ibc_credits + credits >\r\nconn->ibc_queue_depth) {\r\nrc2 = conn->ibc_credits;\r\nspin_unlock(&conn->ibc_lock);\r\nCERROR("Bad credits from %s: %d + %d > %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid),\r\nrc2, credits, conn->ibc_queue_depth);\r\nkiblnd_close_conn(conn, -EPROTO);\r\nkiblnd_post_rx(rx, IBLND_POSTRX_NO_CREDIT);\r\nreturn;\r\n}\r\nconn->ibc_credits += credits;\r\nif (msg->ibm_type == IBLND_MSG_NOOP &&\r\n!IBLND_OOB_CAPABLE(conn->ibc_version))\r\nconn->ibc_outstanding_credits++;\r\nkiblnd_check_sends_locked(conn);\r\nspin_unlock(&conn->ibc_lock);\r\n}\r\nswitch (msg->ibm_type) {\r\ndefault:\r\nCERROR("Bad IBLND message type %x from %s\n",\r\nmsg->ibm_type, libcfs_nid2str(conn->ibc_peer->ibp_nid));\r\npost_credit = IBLND_POSTRX_NO_CREDIT;\r\nrc = -EPROTO;\r\nbreak;\r\ncase IBLND_MSG_NOOP:\r\nif (IBLND_OOB_CAPABLE(conn->ibc_version)) {\r\npost_credit = IBLND_POSTRX_NO_CREDIT;\r\nbreak;\r\n}\r\nif (credits)\r\npost_credit = IBLND_POSTRX_NO_CREDIT;\r\nelse\r\npost_credit = IBLND_POSTRX_PEER_CREDIT;\r\nbreak;\r\ncase IBLND_MSG_IMMEDIATE:\r\npost_credit = IBLND_POSTRX_DONT_POST;\r\nrc = lnet_parse(ni, &msg->ibm_u.immediate.ibim_hdr,\r\nmsg->ibm_srcnid, rx, 0);\r\nif (rc < 0)\r\npost_credit = IBLND_POSTRX_PEER_CREDIT;\r\nbreak;\r\ncase IBLND_MSG_PUT_REQ:\r\npost_credit = IBLND_POSTRX_DONT_POST;\r\nrc = lnet_parse(ni, &msg->ibm_u.putreq.ibprm_hdr,\r\nmsg->ibm_srcnid, rx, 1);\r\nif (rc < 0)\r\npost_credit = IBLND_POSTRX_PEER_CREDIT;\r\nbreak;\r\ncase IBLND_MSG_PUT_NAK:\r\nCWARN("PUT_NACK from %s\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\npost_credit = IBLND_POSTRX_RSRVD_CREDIT;\r\nkiblnd_handle_completion(conn, IBLND_MSG_PUT_REQ,\r\nmsg->ibm_u.completion.ibcm_status,\r\nmsg->ibm_u.completion.ibcm_cookie);\r\nbreak;\r\ncase IBLND_MSG_PUT_ACK:\r\npost_credit = IBLND_POSTRX_RSRVD_CREDIT;\r\nspin_lock(&conn->ibc_lock);\r\ntx = kiblnd_find_waiting_tx_locked(conn, IBLND_MSG_PUT_REQ,\r\nmsg->ibm_u.putack.ibpam_src_cookie);\r\nif (tx)\r\nlist_del(&tx->tx_list);\r\nspin_unlock(&conn->ibc_lock);\r\nif (!tx) {\r\nCERROR("Unmatched PUT_ACK from %s\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nrc = -EPROTO;\r\nbreak;\r\n}\r\nLASSERT(tx->tx_waiting);\r\ntx->tx_nwrq = 0;\r\nrc2 = kiblnd_init_rdma(conn, tx, IBLND_MSG_PUT_DONE,\r\nkiblnd_rd_size(&msg->ibm_u.putack.ibpam_rd),\r\n&msg->ibm_u.putack.ibpam_rd,\r\nmsg->ibm_u.putack.ibpam_dst_cookie);\r\nif (rc2 < 0)\r\nCERROR("Can't setup rdma for PUT to %s: %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), rc2);\r\nspin_lock(&conn->ibc_lock);\r\ntx->tx_waiting = 0;\r\nkiblnd_queue_tx_locked(tx, conn);\r\nspin_unlock(&conn->ibc_lock);\r\nbreak;\r\ncase IBLND_MSG_PUT_DONE:\r\npost_credit = IBLND_POSTRX_PEER_CREDIT;\r\nkiblnd_handle_completion(conn, IBLND_MSG_PUT_ACK,\r\nmsg->ibm_u.completion.ibcm_status,\r\nmsg->ibm_u.completion.ibcm_cookie);\r\nbreak;\r\ncase IBLND_MSG_GET_REQ:\r\npost_credit = IBLND_POSTRX_DONT_POST;\r\nrc = lnet_parse(ni, &msg->ibm_u.get.ibgm_hdr,\r\nmsg->ibm_srcnid, rx, 1);\r\nif (rc < 0)\r\npost_credit = IBLND_POSTRX_PEER_CREDIT;\r\nbreak;\r\ncase IBLND_MSG_GET_DONE:\r\npost_credit = IBLND_POSTRX_RSRVD_CREDIT;\r\nkiblnd_handle_completion(conn, IBLND_MSG_GET_REQ,\r\nmsg->ibm_u.completion.ibcm_status,\r\nmsg->ibm_u.completion.ibcm_cookie);\r\nbreak;\r\n}\r\nif (rc < 0)\r\nkiblnd_close_conn(conn, rc);\r\nif (post_credit != IBLND_POSTRX_DONT_POST)\r\nkiblnd_post_rx(rx, post_credit);\r\n}\r\nstatic void\r\nkiblnd_rx_complete(struct kib_rx *rx, int status, int nob)\r\n{\r\nstruct kib_msg *msg = rx->rx_msg;\r\nstruct kib_conn *conn = rx->rx_conn;\r\nstruct lnet_ni *ni = conn->ibc_peer->ibp_ni;\r\nstruct kib_net *net = ni->ni_data;\r\nint rc;\r\nint err = -EIO;\r\nLASSERT(net);\r\nLASSERT(rx->rx_nob < 0);\r\nrx->rx_nob = 0;\r\nif (conn->ibc_state > IBLND_CONN_ESTABLISHED)\r\ngoto ignore;\r\nif (status != IB_WC_SUCCESS) {\r\nCNETERR("Rx from %s failed: %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), status);\r\ngoto failed;\r\n}\r\nLASSERT(nob >= 0);\r\nrx->rx_nob = nob;\r\nrc = kiblnd_unpack_msg(msg, rx->rx_nob);\r\nif (rc) {\r\nCERROR("Error %d unpacking rx from %s\n",\r\nrc, libcfs_nid2str(conn->ibc_peer->ibp_nid));\r\ngoto failed;\r\n}\r\nif (msg->ibm_srcnid != conn->ibc_peer->ibp_nid ||\r\nmsg->ibm_dstnid != ni->ni_nid ||\r\nmsg->ibm_srcstamp != conn->ibc_incarnation ||\r\nmsg->ibm_dststamp != net->ibn_incarnation) {\r\nCERROR("Stale rx from %s\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nerr = -ESTALE;\r\ngoto failed;\r\n}\r\nkiblnd_peer_alive(conn->ibc_peer);\r\nif (conn->ibc_state < IBLND_CONN_ESTABLISHED) {\r\nrwlock_t *g_lock = &kiblnd_data.kib_global_lock;\r\nunsigned long flags;\r\nwrite_lock_irqsave(g_lock, flags);\r\nif (conn->ibc_state < IBLND_CONN_ESTABLISHED) {\r\nlist_add_tail(&rx->rx_list, &conn->ibc_early_rxs);\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nreturn;\r\n}\r\nwrite_unlock_irqrestore(g_lock, flags);\r\n}\r\nkiblnd_handle_rx(rx);\r\nreturn;\r\nfailed:\r\nCDEBUG(D_NET, "rx %p conn %p\n", rx, conn);\r\nkiblnd_close_conn(conn, err);\r\nignore:\r\nkiblnd_drop_rx(rx);\r\n}\r\nstatic struct page *\r\nkiblnd_kvaddr_to_page(unsigned long vaddr)\r\n{\r\nstruct page *page;\r\nif (is_vmalloc_addr((void *)vaddr)) {\r\npage = vmalloc_to_page((void *)vaddr);\r\nLASSERT(page);\r\nreturn page;\r\n}\r\n#ifdef CONFIG_HIGHMEM\r\nif (vaddr >= PKMAP_BASE &&\r\nvaddr < (PKMAP_BASE + LAST_PKMAP * PAGE_SIZE)) {\r\nCERROR("find page for address in highmem\n");\r\nLBUG();\r\n}\r\n#endif\r\npage = virt_to_page(vaddr);\r\nLASSERT(page);\r\nreturn page;\r\n}\r\nstatic int\r\nkiblnd_fmr_map_tx(struct kib_net *net, struct kib_tx *tx, struct kib_rdma_desc *rd, __u32 nob)\r\n{\r\nstruct kib_hca_dev *hdev;\r\nstruct kib_fmr_poolset *fps;\r\nint cpt;\r\nint rc;\r\nLASSERT(tx->tx_pool);\r\nLASSERT(tx->tx_pool->tpo_pool.po_owner);\r\nhdev = tx->tx_pool->tpo_hdev;\r\ncpt = tx->tx_pool->tpo_pool.po_owner->ps_cpt;\r\nfps = net->ibn_fmr_ps[cpt];\r\nrc = kiblnd_fmr_pool_map(fps, tx, rd, nob, 0, &tx->fmr);\r\nif (rc) {\r\nCERROR("Can't map %u bytes: %d\n", nob, rc);\r\nreturn rc;\r\n}\r\nrd->rd_key = tx->fmr.fmr_key;\r\nrd->rd_frags[0].rf_addr &= ~hdev->ibh_page_mask;\r\nrd->rd_frags[0].rf_nob = nob;\r\nrd->rd_nfrags = 1;\r\nreturn 0;\r\n}\r\nstatic void kiblnd_unmap_tx(struct lnet_ni *ni, struct kib_tx *tx)\r\n{\r\nstruct kib_net *net = ni->ni_data;\r\nLASSERT(net);\r\nif (net->ibn_fmr_ps)\r\nkiblnd_fmr_pool_unmap(&tx->fmr, tx->tx_status);\r\nif (tx->tx_nfrags) {\r\nkiblnd_dma_unmap_sg(tx->tx_pool->tpo_hdev->ibh_ibdev,\r\ntx->tx_frags, tx->tx_nfrags, tx->tx_dmadir);\r\ntx->tx_nfrags = 0;\r\n}\r\n}\r\nstatic int kiblnd_map_tx(struct lnet_ni *ni, struct kib_tx *tx,\r\nstruct kib_rdma_desc *rd, int nfrags)\r\n{\r\nstruct kib_net *net = ni->ni_data;\r\nstruct kib_hca_dev *hdev = net->ibn_dev->ibd_hdev;\r\n__u32 nob;\r\nint i;\r\ntx->tx_dmadir = (rd != tx->tx_rd) ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\r\ntx->tx_nfrags = nfrags;\r\nrd->rd_nfrags = kiblnd_dma_map_sg(hdev->ibh_ibdev, tx->tx_frags,\r\ntx->tx_nfrags, tx->tx_dmadir);\r\nfor (i = 0, nob = 0; i < rd->rd_nfrags; i++) {\r\nrd->rd_frags[i].rf_nob = kiblnd_sg_dma_len(\r\nhdev->ibh_ibdev, &tx->tx_frags[i]);\r\nrd->rd_frags[i].rf_addr = kiblnd_sg_dma_address(\r\nhdev->ibh_ibdev, &tx->tx_frags[i]);\r\nnob += rd->rd_frags[i].rf_nob;\r\n}\r\nif (net->ibn_fmr_ps)\r\nreturn kiblnd_fmr_map_tx(net, tx, rd, nob);\r\nreturn -EINVAL;\r\n}\r\nstatic int\r\nkiblnd_setup_rd_iov(struct lnet_ni *ni, struct kib_tx *tx,\r\nstruct kib_rdma_desc *rd, unsigned int niov,\r\nconst struct kvec *iov, int offset, int nob)\r\n{\r\nstruct kib_net *net = ni->ni_data;\r\nstruct page *page;\r\nstruct scatterlist *sg;\r\nunsigned long vaddr;\r\nint fragnob;\r\nint page_offset;\r\nLASSERT(nob > 0);\r\nLASSERT(niov > 0);\r\nLASSERT(net);\r\nwhile (offset >= iov->iov_len) {\r\noffset -= iov->iov_len;\r\nniov--;\r\niov++;\r\nLASSERT(niov > 0);\r\n}\r\nsg = tx->tx_frags;\r\ndo {\r\nLASSERT(niov > 0);\r\nvaddr = ((unsigned long)iov->iov_base) + offset;\r\npage_offset = vaddr & (PAGE_SIZE - 1);\r\npage = kiblnd_kvaddr_to_page(vaddr);\r\nif (!page) {\r\nCERROR("Can't find page\n");\r\nreturn -EFAULT;\r\n}\r\nfragnob = min((int)(iov->iov_len - offset), nob);\r\nfragnob = min(fragnob, (int)PAGE_SIZE - page_offset);\r\nsg_set_page(sg, page, fragnob, page_offset);\r\nsg = sg_next(sg);\r\nif (!sg) {\r\nCERROR("lacking enough sg entries to map tx\n");\r\nreturn -EFAULT;\r\n}\r\nif (offset + fragnob < iov->iov_len) {\r\noffset += fragnob;\r\n} else {\r\noffset = 0;\r\niov++;\r\nniov--;\r\n}\r\nnob -= fragnob;\r\n} while (nob > 0);\r\nreturn kiblnd_map_tx(ni, tx, rd, sg - tx->tx_frags);\r\n}\r\nstatic int\r\nkiblnd_setup_rd_kiov(struct lnet_ni *ni, struct kib_tx *tx,\r\nstruct kib_rdma_desc *rd, int nkiov,\r\nconst struct bio_vec *kiov, int offset, int nob)\r\n{\r\nstruct kib_net *net = ni->ni_data;\r\nstruct scatterlist *sg;\r\nint fragnob;\r\nCDEBUG(D_NET, "niov %d offset %d nob %d\n", nkiov, offset, nob);\r\nLASSERT(nob > 0);\r\nLASSERT(nkiov > 0);\r\nLASSERT(net);\r\nwhile (offset >= kiov->bv_len) {\r\noffset -= kiov->bv_len;\r\nnkiov--;\r\nkiov++;\r\nLASSERT(nkiov > 0);\r\n}\r\nsg = tx->tx_frags;\r\ndo {\r\nLASSERT(nkiov > 0);\r\nfragnob = min((int)(kiov->bv_len - offset), nob);\r\nsg_set_page(sg, kiov->bv_page, fragnob,\r\nkiov->bv_offset + offset);\r\nsg = sg_next(sg);\r\nif (!sg) {\r\nCERROR("lacking enough sg entries to map tx\n");\r\nreturn -EFAULT;\r\n}\r\noffset = 0;\r\nkiov++;\r\nnkiov--;\r\nnob -= fragnob;\r\n} while (nob > 0);\r\nreturn kiblnd_map_tx(ni, tx, rd, sg - tx->tx_frags);\r\n}\r\nstatic int\r\nkiblnd_post_tx_locked(struct kib_conn *conn, struct kib_tx *tx, int credit)\r\n__must_hold(&conn->ibc_lock\r\nstatic void\r\nkiblnd_check_sends_locked(struct kib_conn *conn)\r\n{\r\nint ver = conn->ibc_version;\r\nstruct lnet_ni *ni = conn->ibc_peer->ibp_ni;\r\nstruct kib_tx *tx;\r\nif (conn->ibc_state < IBLND_CONN_ESTABLISHED) {\r\nCDEBUG(D_NET, "%s too soon\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nreturn;\r\n}\r\nLASSERT(conn->ibc_nsends_posted <= kiblnd_concurrent_sends(ver, ni));\r\nLASSERT(!IBLND_OOB_CAPABLE(ver) ||\r\nconn->ibc_noops_posted <= IBLND_OOB_MSGS(ver));\r\nLASSERT(conn->ibc_reserved_credits >= 0);\r\nwhile (conn->ibc_reserved_credits > 0 &&\r\n!list_empty(&conn->ibc_tx_queue_rsrvd)) {\r\ntx = list_entry(conn->ibc_tx_queue_rsrvd.next,\r\nstruct kib_tx, tx_list);\r\nlist_del(&tx->tx_list);\r\nlist_add_tail(&tx->tx_list, &conn->ibc_tx_queue);\r\nconn->ibc_reserved_credits--;\r\n}\r\nif (kiblnd_need_noop(conn)) {\r\nspin_unlock(&conn->ibc_lock);\r\ntx = kiblnd_get_idle_tx(ni, conn->ibc_peer->ibp_nid);\r\nif (tx)\r\nkiblnd_init_tx_msg(ni, tx, IBLND_MSG_NOOP, 0);\r\nspin_lock(&conn->ibc_lock);\r\nif (tx)\r\nkiblnd_queue_tx_locked(tx, conn);\r\n}\r\nfor (;;) {\r\nint credit;\r\nif (!list_empty(&conn->ibc_tx_queue_nocred)) {\r\ncredit = 0;\r\ntx = list_entry(conn->ibc_tx_queue_nocred.next,\r\nstruct kib_tx, tx_list);\r\n} else if (!list_empty(&conn->ibc_tx_noops)) {\r\nLASSERT(!IBLND_OOB_CAPABLE(ver));\r\ncredit = 1;\r\ntx = list_entry(conn->ibc_tx_noops.next,\r\nstruct kib_tx, tx_list);\r\n} else if (!list_empty(&conn->ibc_tx_queue)) {\r\ncredit = 1;\r\ntx = list_entry(conn->ibc_tx_queue.next,\r\nstruct kib_tx, tx_list);\r\n} else {\r\nbreak;\r\n}\r\nif (kiblnd_post_tx_locked(conn, tx, credit))\r\nbreak;\r\n}\r\n}\r\nstatic void\r\nkiblnd_tx_complete(struct kib_tx *tx, int status)\r\n{\r\nint failed = (status != IB_WC_SUCCESS);\r\nstruct kib_conn *conn = tx->tx_conn;\r\nint idle;\r\nLASSERT(tx->tx_sending > 0);\r\nif (failed) {\r\nif (conn->ibc_state == IBLND_CONN_ESTABLISHED)\r\nCNETERR("Tx -> %s cookie %#llx sending %d waiting %d: failed %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid),\r\ntx->tx_cookie, tx->tx_sending, tx->tx_waiting,\r\nstatus);\r\nkiblnd_close_conn(conn, -EIO);\r\n} else {\r\nkiblnd_peer_alive(conn->ibc_peer);\r\n}\r\nspin_lock(&conn->ibc_lock);\r\ntx->tx_sending--;\r\nconn->ibc_nsends_posted--;\r\nif (tx->tx_msg->ibm_type == IBLND_MSG_NOOP)\r\nconn->ibc_noops_posted--;\r\nif (failed) {\r\ntx->tx_waiting = 0;\r\ntx->tx_status = -EIO;\r\n}\r\nidle = !tx->tx_sending &&\r\n!tx->tx_waiting &&\r\n!tx->tx_queued;\r\nif (idle)\r\nlist_del(&tx->tx_list);\r\nkiblnd_check_sends_locked(conn);\r\nspin_unlock(&conn->ibc_lock);\r\nif (idle)\r\nkiblnd_tx_done(conn->ibc_peer->ibp_ni, tx);\r\n}\r\nstatic void\r\nkiblnd_init_tx_msg(struct lnet_ni *ni, struct kib_tx *tx, int type,\r\nint body_nob)\r\n{\r\nstruct kib_hca_dev *hdev = tx->tx_pool->tpo_hdev;\r\nstruct ib_sge *sge = &tx->tx_sge[tx->tx_nwrq];\r\nstruct ib_rdma_wr *wrq = &tx->tx_wrq[tx->tx_nwrq];\r\nint nob = offsetof(struct kib_msg, ibm_u) + body_nob;\r\nLASSERT(tx->tx_nwrq >= 0);\r\nLASSERT(tx->tx_nwrq < IBLND_MAX_RDMA_FRAGS + 1);\r\nLASSERT(nob <= IBLND_MSG_SIZE);\r\nkiblnd_init_msg(tx->tx_msg, type, body_nob);\r\nsge->lkey = hdev->ibh_pd->local_dma_lkey;\r\nsge->addr = tx->tx_msgaddr;\r\nsge->length = nob;\r\nmemset(wrq, 0, sizeof(*wrq));\r\nwrq->wr.next = NULL;\r\nwrq->wr.wr_id = kiblnd_ptr2wreqid(tx, IBLND_WID_TX);\r\nwrq->wr.sg_list = sge;\r\nwrq->wr.num_sge = 1;\r\nwrq->wr.opcode = IB_WR_SEND;\r\nwrq->wr.send_flags = IB_SEND_SIGNALED;\r\ntx->tx_nwrq++;\r\n}\r\nstatic int\r\nkiblnd_init_rdma(struct kib_conn *conn, struct kib_tx *tx, int type,\r\nint resid, struct kib_rdma_desc *dstrd, __u64 dstcookie)\r\n{\r\nstruct kib_msg *ibmsg = tx->tx_msg;\r\nstruct kib_rdma_desc *srcrd = tx->tx_rd;\r\nstruct ib_sge *sge = &tx->tx_sge[0];\r\nstruct ib_rdma_wr *wrq, *next;\r\nint rc = resid;\r\nint srcidx = 0;\r\nint dstidx = 0;\r\nint wrknob;\r\nLASSERT(!in_interrupt());\r\nLASSERT(!tx->tx_nwrq);\r\nLASSERT(type == IBLND_MSG_GET_DONE ||\r\ntype == IBLND_MSG_PUT_DONE);\r\nif (kiblnd_rd_size(srcrd) > conn->ibc_max_frags << PAGE_SHIFT) {\r\nCERROR("RDMA is too large for peer %s (%d), src size: %d dst size: %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid),\r\nconn->ibc_max_frags << PAGE_SHIFT,\r\nkiblnd_rd_size(srcrd), kiblnd_rd_size(dstrd));\r\nrc = -EMSGSIZE;\r\ngoto too_big;\r\n}\r\nwhile (resid > 0) {\r\nif (srcidx >= srcrd->rd_nfrags) {\r\nCERROR("Src buffer exhausted: %d frags\n", srcidx);\r\nrc = -EPROTO;\r\nbreak;\r\n}\r\nif (dstidx == dstrd->rd_nfrags) {\r\nCERROR("Dst buffer exhausted: %d frags\n", dstidx);\r\nrc = -EPROTO;\r\nbreak;\r\n}\r\nif (tx->tx_nwrq >= IBLND_MAX_RDMA_FRAGS) {\r\nCERROR("RDMA has too many fragments for peer %s (%d), src idx/frags: %d/%d dst idx/frags: %d/%d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid),\r\nIBLND_MAX_RDMA_FRAGS,\r\nsrcidx, srcrd->rd_nfrags,\r\ndstidx, dstrd->rd_nfrags);\r\nrc = -EMSGSIZE;\r\nbreak;\r\n}\r\nwrknob = min3(kiblnd_rd_frag_size(srcrd, srcidx),\r\nkiblnd_rd_frag_size(dstrd, dstidx),\r\n(__u32)resid);\r\nsge = &tx->tx_sge[tx->tx_nwrq];\r\nsge->addr = kiblnd_rd_frag_addr(srcrd, srcidx);\r\nsge->lkey = kiblnd_rd_frag_key(srcrd, srcidx);\r\nsge->length = wrknob;\r\nwrq = &tx->tx_wrq[tx->tx_nwrq];\r\nnext = wrq + 1;\r\nwrq->wr.next = &next->wr;\r\nwrq->wr.wr_id = kiblnd_ptr2wreqid(tx, IBLND_WID_RDMA);\r\nwrq->wr.sg_list = sge;\r\nwrq->wr.num_sge = 1;\r\nwrq->wr.opcode = IB_WR_RDMA_WRITE;\r\nwrq->wr.send_flags = 0;\r\nwrq->remote_addr = kiblnd_rd_frag_addr(dstrd, dstidx);\r\nwrq->rkey = kiblnd_rd_frag_key(dstrd, dstidx);\r\nsrcidx = kiblnd_rd_consume_frag(srcrd, srcidx, wrknob);\r\ndstidx = kiblnd_rd_consume_frag(dstrd, dstidx, wrknob);\r\nresid -= wrknob;\r\ntx->tx_nwrq++;\r\nwrq++;\r\nsge++;\r\n}\r\ntoo_big:\r\nif (rc < 0)\r\ntx->tx_nwrq = 0;\r\nibmsg->ibm_u.completion.ibcm_status = rc;\r\nibmsg->ibm_u.completion.ibcm_cookie = dstcookie;\r\nkiblnd_init_tx_msg(conn->ibc_peer->ibp_ni, tx,\r\ntype, sizeof(struct kib_completion_msg));\r\nreturn rc;\r\n}\r\nstatic void\r\nkiblnd_queue_tx_locked(struct kib_tx *tx, struct kib_conn *conn)\r\n{\r\nstruct list_head *q;\r\nLASSERT(tx->tx_nwrq > 0);\r\nLASSERT(!tx->tx_queued);\r\nLASSERT(conn->ibc_state >= IBLND_CONN_ESTABLISHED);\r\ntx->tx_queued = 1;\r\ntx->tx_deadline = jiffies +\r\nmsecs_to_jiffies(*kiblnd_tunables.kib_timeout *\r\nMSEC_PER_SEC);\r\nif (!tx->tx_conn) {\r\nkiblnd_conn_addref(conn);\r\ntx->tx_conn = conn;\r\nLASSERT(tx->tx_msg->ibm_type != IBLND_MSG_PUT_DONE);\r\n} else {\r\nLASSERT(tx->tx_conn == conn);\r\nLASSERT(tx->tx_msg->ibm_type == IBLND_MSG_PUT_DONE);\r\n}\r\nswitch (tx->tx_msg->ibm_type) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_MSG_PUT_REQ:\r\ncase IBLND_MSG_GET_REQ:\r\nq = &conn->ibc_tx_queue_rsrvd;\r\nbreak;\r\ncase IBLND_MSG_PUT_NAK:\r\ncase IBLND_MSG_PUT_ACK:\r\ncase IBLND_MSG_PUT_DONE:\r\ncase IBLND_MSG_GET_DONE:\r\nq = &conn->ibc_tx_queue_nocred;\r\nbreak;\r\ncase IBLND_MSG_NOOP:\r\nif (IBLND_OOB_CAPABLE(conn->ibc_version))\r\nq = &conn->ibc_tx_queue_nocred;\r\nelse\r\nq = &conn->ibc_tx_noops;\r\nbreak;\r\ncase IBLND_MSG_IMMEDIATE:\r\nq = &conn->ibc_tx_queue;\r\nbreak;\r\n}\r\nlist_add_tail(&tx->tx_list, q);\r\n}\r\nstatic void\r\nkiblnd_queue_tx(struct kib_tx *tx, struct kib_conn *conn)\r\n{\r\nspin_lock(&conn->ibc_lock);\r\nkiblnd_queue_tx_locked(tx, conn);\r\nkiblnd_check_sends_locked(conn);\r\nspin_unlock(&conn->ibc_lock);\r\n}\r\nstatic int kiblnd_resolve_addr(struct rdma_cm_id *cmid,\r\nstruct sockaddr_in *srcaddr,\r\nstruct sockaddr_in *dstaddr,\r\nint timeout_ms)\r\n{\r\nunsigned short port;\r\nint rc;\r\nrc = rdma_set_reuseaddr(cmid, 1);\r\nif (rc) {\r\nCERROR("Unable to set reuse on cmid: %d\n", rc);\r\nreturn rc;\r\n}\r\nfor (port = PROT_SOCK - 1; port > 0; port--) {\r\nsrcaddr->sin_port = htons(port);\r\nrc = rdma_resolve_addr(cmid,\r\n(struct sockaddr *)srcaddr,\r\n(struct sockaddr *)dstaddr,\r\ntimeout_ms);\r\nif (!rc) {\r\nCDEBUG(D_NET, "bound to port %hu\n", port);\r\nreturn 0;\r\n} else if (rc == -EADDRINUSE || rc == -EADDRNOTAVAIL) {\r\nCDEBUG(D_NET, "bind to port %hu failed: %d\n",\r\nport, rc);\r\n} else {\r\nreturn rc;\r\n}\r\n}\r\nCERROR("Failed to bind to a free privileged port\n");\r\nreturn rc;\r\n}\r\nstatic void\r\nkiblnd_connect_peer(struct kib_peer *peer)\r\n{\r\nstruct rdma_cm_id *cmid;\r\nstruct kib_dev *dev;\r\nstruct kib_net *net = peer->ibp_ni->ni_data;\r\nstruct sockaddr_in srcaddr;\r\nstruct sockaddr_in dstaddr;\r\nint rc;\r\nLASSERT(net);\r\nLASSERT(peer->ibp_connecting > 0);\r\nLASSERT(!peer->ibp_reconnecting);\r\ncmid = kiblnd_rdma_create_id(kiblnd_cm_callback, peer, RDMA_PS_TCP,\r\nIB_QPT_RC);\r\nif (IS_ERR(cmid)) {\r\nCERROR("Can't create CMID for %s: %ld\n",\r\nlibcfs_nid2str(peer->ibp_nid), PTR_ERR(cmid));\r\nrc = PTR_ERR(cmid);\r\ngoto failed;\r\n}\r\ndev = net->ibn_dev;\r\nmemset(&srcaddr, 0, sizeof(srcaddr));\r\nsrcaddr.sin_family = AF_INET;\r\nsrcaddr.sin_addr.s_addr = htonl(dev->ibd_ifip);\r\nmemset(&dstaddr, 0, sizeof(dstaddr));\r\ndstaddr.sin_family = AF_INET;\r\ndstaddr.sin_port = htons(*kiblnd_tunables.kib_service);\r\ndstaddr.sin_addr.s_addr = htonl(LNET_NIDADDR(peer->ibp_nid));\r\nkiblnd_peer_addref(peer);\r\nif (*kiblnd_tunables.kib_use_priv_port) {\r\nrc = kiblnd_resolve_addr(cmid, &srcaddr, &dstaddr,\r\n*kiblnd_tunables.kib_timeout * 1000);\r\n} else {\r\nrc = rdma_resolve_addr(cmid,\r\n(struct sockaddr *)&srcaddr,\r\n(struct sockaddr *)&dstaddr,\r\n*kiblnd_tunables.kib_timeout * 1000);\r\n}\r\nif (rc) {\r\nCERROR("Can't resolve addr for %s: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), rc);\r\ngoto failed2;\r\n}\r\nLASSERT(cmid->device);\r\nCDEBUG(D_NET, "%s: connection bound to %s:%pI4h:%s\n",\r\nlibcfs_nid2str(peer->ibp_nid), dev->ibd_ifname,\r\n&dev->ibd_ifip, cmid->device->name);\r\nreturn;\r\nfailed2:\r\nkiblnd_peer_connect_failed(peer, 1, rc);\r\nkiblnd_peer_decref(peer);\r\nrdma_destroy_id(cmid);\r\nreturn;\r\nfailed:\r\nkiblnd_peer_connect_failed(peer, 1, rc);\r\n}\r\nbool\r\nkiblnd_reconnect_peer(struct kib_peer *peer)\r\n{\r\nrwlock_t *glock = &kiblnd_data.kib_global_lock;\r\nchar *reason = NULL;\r\nstruct list_head txs;\r\nunsigned long flags;\r\nINIT_LIST_HEAD(&txs);\r\nwrite_lock_irqsave(glock, flags);\r\nif (!peer->ibp_reconnecting) {\r\nif (peer->ibp_accepting)\r\nreason = "accepting";\r\nelse if (peer->ibp_connecting)\r\nreason = "connecting";\r\nelse if (!list_empty(&peer->ibp_conns))\r\nreason = "connected";\r\nelse\r\nreason = "closed";\r\ngoto no_reconnect;\r\n}\r\nLASSERT(!peer->ibp_accepting && !peer->ibp_connecting &&\r\nlist_empty(&peer->ibp_conns));\r\npeer->ibp_reconnecting = 0;\r\nif (!kiblnd_peer_active(peer)) {\r\nlist_splice_init(&peer->ibp_tx_queue, &txs);\r\nreason = "unlinked";\r\ngoto no_reconnect;\r\n}\r\npeer->ibp_connecting++;\r\npeer->ibp_reconnected++;\r\nwrite_unlock_irqrestore(glock, flags);\r\nkiblnd_connect_peer(peer);\r\nreturn true;\r\nno_reconnect:\r\nwrite_unlock_irqrestore(glock, flags);\r\nCWARN("Abort reconnection of %s: %s\n",\r\nlibcfs_nid2str(peer->ibp_nid), reason);\r\nkiblnd_txlist_done(peer->ibp_ni, &txs, -ECONNABORTED);\r\nreturn false;\r\n}\r\nvoid\r\nkiblnd_launch_tx(struct lnet_ni *ni, struct kib_tx *tx, lnet_nid_t nid)\r\n{\r\nstruct kib_peer *peer;\r\nstruct kib_peer *peer2;\r\nstruct kib_conn *conn;\r\nrwlock_t *g_lock = &kiblnd_data.kib_global_lock;\r\nunsigned long flags;\r\nint rc;\r\nLASSERT(!tx || !tx->tx_conn);\r\nLASSERT(!tx || tx->tx_nwrq > 0);\r\nread_lock_irqsave(g_lock, flags);\r\npeer = kiblnd_find_peer_locked(nid);\r\nif (peer && !list_empty(&peer->ibp_conns)) {\r\nconn = kiblnd_get_conn_locked(peer);\r\nkiblnd_conn_addref(conn);\r\nread_unlock_irqrestore(g_lock, flags);\r\nif (tx)\r\nkiblnd_queue_tx(tx, conn);\r\nkiblnd_conn_decref(conn);\r\nreturn;\r\n}\r\nread_unlock(g_lock);\r\nwrite_lock(g_lock);\r\npeer = kiblnd_find_peer_locked(nid);\r\nif (peer) {\r\nif (list_empty(&peer->ibp_conns)) {\r\nLASSERT(kiblnd_peer_connecting(peer));\r\nif (tx)\r\nlist_add_tail(&tx->tx_list,\r\n&peer->ibp_tx_queue);\r\nwrite_unlock_irqrestore(g_lock, flags);\r\n} else {\r\nconn = kiblnd_get_conn_locked(peer);\r\nkiblnd_conn_addref(conn);\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nif (tx)\r\nkiblnd_queue_tx(tx, conn);\r\nkiblnd_conn_decref(conn);\r\n}\r\nreturn;\r\n}\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nrc = kiblnd_create_peer(ni, &peer, nid);\r\nif (rc) {\r\nCERROR("Can't create peer %s\n", libcfs_nid2str(nid));\r\nif (tx) {\r\ntx->tx_status = -EHOSTUNREACH;\r\ntx->tx_waiting = 0;\r\nkiblnd_tx_done(ni, tx);\r\n}\r\nreturn;\r\n}\r\nwrite_lock_irqsave(g_lock, flags);\r\npeer2 = kiblnd_find_peer_locked(nid);\r\nif (peer2) {\r\nif (list_empty(&peer2->ibp_conns)) {\r\nLASSERT(kiblnd_peer_connecting(peer2));\r\nif (tx)\r\nlist_add_tail(&tx->tx_list,\r\n&peer2->ibp_tx_queue);\r\nwrite_unlock_irqrestore(g_lock, flags);\r\n} else {\r\nconn = kiblnd_get_conn_locked(peer2);\r\nkiblnd_conn_addref(conn);\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nif (tx)\r\nkiblnd_queue_tx(tx, conn);\r\nkiblnd_conn_decref(conn);\r\n}\r\nkiblnd_peer_decref(peer);\r\nreturn;\r\n}\r\nLASSERT(!peer->ibp_connecting);\r\npeer->ibp_connecting = 1;\r\nLASSERT(!((struct kib_net *)ni->ni_data)->ibn_shutdown);\r\nif (tx)\r\nlist_add_tail(&tx->tx_list, &peer->ibp_tx_queue);\r\nkiblnd_peer_addref(peer);\r\nlist_add_tail(&peer->ibp_list, kiblnd_nid2peerlist(nid));\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nkiblnd_connect_peer(peer);\r\nkiblnd_peer_decref(peer);\r\n}\r\nint\r\nkiblnd_send(struct lnet_ni *ni, void *private, struct lnet_msg *lntmsg)\r\n{\r\nstruct lnet_hdr *hdr = &lntmsg->msg_hdr;\r\nint type = lntmsg->msg_type;\r\nstruct lnet_process_id target = lntmsg->msg_target;\r\nint target_is_router = lntmsg->msg_target_is_router;\r\nint routing = lntmsg->msg_routing;\r\nunsigned int payload_niov = lntmsg->msg_niov;\r\nstruct kvec *payload_iov = lntmsg->msg_iov;\r\nstruct bio_vec *payload_kiov = lntmsg->msg_kiov;\r\nunsigned int payload_offset = lntmsg->msg_offset;\r\nunsigned int payload_nob = lntmsg->msg_len;\r\nstruct iov_iter from;\r\nstruct kib_msg *ibmsg;\r\nstruct kib_rdma_desc *rd;\r\nstruct kib_tx *tx;\r\nint nob;\r\nint rc;\r\nCDEBUG(D_NET, "sending %d bytes in %d frags to %s\n",\r\npayload_nob, payload_niov, libcfs_id2str(target));\r\nLASSERT(!payload_nob || payload_niov > 0);\r\nLASSERT(payload_niov <= LNET_MAX_IOV);\r\nLASSERT(!in_interrupt());\r\nLASSERT(!(payload_kiov && payload_iov));\r\nif (payload_kiov)\r\niov_iter_bvec(&from, ITER_BVEC | WRITE,\r\npayload_kiov, payload_niov,\r\npayload_nob + payload_offset);\r\nelse\r\niov_iter_kvec(&from, ITER_KVEC | WRITE,\r\npayload_iov, payload_niov,\r\npayload_nob + payload_offset);\r\niov_iter_advance(&from, payload_offset);\r\nswitch (type) {\r\ndefault:\r\nLBUG();\r\nreturn -EIO;\r\ncase LNET_MSG_ACK:\r\nLASSERT(!payload_nob);\r\nbreak;\r\ncase LNET_MSG_GET:\r\nif (routing || target_is_router)\r\nbreak;\r\nnob = offsetof(struct kib_msg, ibm_u.immediate.ibim_payload[lntmsg->msg_md->md_length]);\r\nif (nob <= IBLND_MSG_SIZE)\r\nbreak;\r\ntx = kiblnd_get_idle_tx(ni, target.nid);\r\nif (!tx) {\r\nCERROR("Can't allocate txd for GET to %s\n",\r\nlibcfs_nid2str(target.nid));\r\nreturn -ENOMEM;\r\n}\r\nibmsg = tx->tx_msg;\r\nrd = &ibmsg->ibm_u.get.ibgm_rd;\r\nif (!(lntmsg->msg_md->md_options & LNET_MD_KIOV))\r\nrc = kiblnd_setup_rd_iov(ni, tx, rd,\r\nlntmsg->msg_md->md_niov,\r\nlntmsg->msg_md->md_iov.iov,\r\n0, lntmsg->msg_md->md_length);\r\nelse\r\nrc = kiblnd_setup_rd_kiov(ni, tx, rd,\r\nlntmsg->msg_md->md_niov,\r\nlntmsg->msg_md->md_iov.kiov,\r\n0, lntmsg->msg_md->md_length);\r\nif (rc) {\r\nCERROR("Can't setup GET sink for %s: %d\n",\r\nlibcfs_nid2str(target.nid), rc);\r\nkiblnd_tx_done(ni, tx);\r\nreturn -EIO;\r\n}\r\nnob = offsetof(struct kib_get_msg, ibgm_rd.rd_frags[rd->rd_nfrags]);\r\nibmsg->ibm_u.get.ibgm_cookie = tx->tx_cookie;\r\nibmsg->ibm_u.get.ibgm_hdr = *hdr;\r\nkiblnd_init_tx_msg(ni, tx, IBLND_MSG_GET_REQ, nob);\r\ntx->tx_lntmsg[1] = lnet_create_reply_msg(ni, lntmsg);\r\nif (!tx->tx_lntmsg[1]) {\r\nCERROR("Can't create reply for GET -> %s\n",\r\nlibcfs_nid2str(target.nid));\r\nkiblnd_tx_done(ni, tx);\r\nreturn -EIO;\r\n}\r\ntx->tx_lntmsg[0] = lntmsg;\r\ntx->tx_waiting = 1;\r\nkiblnd_launch_tx(ni, tx, target.nid);\r\nreturn 0;\r\ncase LNET_MSG_REPLY:\r\ncase LNET_MSG_PUT:\r\nnob = offsetof(struct kib_msg, ibm_u.immediate.ibim_payload[payload_nob]);\r\nif (nob <= IBLND_MSG_SIZE)\r\nbreak;\r\ntx = kiblnd_get_idle_tx(ni, target.nid);\r\nif (!tx) {\r\nCERROR("Can't allocate %s txd for %s\n",\r\ntype == LNET_MSG_PUT ? "PUT" : "REPLY",\r\nlibcfs_nid2str(target.nid));\r\nreturn -ENOMEM;\r\n}\r\nif (!payload_kiov)\r\nrc = kiblnd_setup_rd_iov(ni, tx, tx->tx_rd,\r\npayload_niov, payload_iov,\r\npayload_offset, payload_nob);\r\nelse\r\nrc = kiblnd_setup_rd_kiov(ni, tx, tx->tx_rd,\r\npayload_niov, payload_kiov,\r\npayload_offset, payload_nob);\r\nif (rc) {\r\nCERROR("Can't setup PUT src for %s: %d\n",\r\nlibcfs_nid2str(target.nid), rc);\r\nkiblnd_tx_done(ni, tx);\r\nreturn -EIO;\r\n}\r\nibmsg = tx->tx_msg;\r\nibmsg->ibm_u.putreq.ibprm_hdr = *hdr;\r\nibmsg->ibm_u.putreq.ibprm_cookie = tx->tx_cookie;\r\nkiblnd_init_tx_msg(ni, tx, IBLND_MSG_PUT_REQ, sizeof(struct kib_putreq_msg));\r\ntx->tx_lntmsg[0] = lntmsg;\r\ntx->tx_waiting = 1;\r\nkiblnd_launch_tx(ni, tx, target.nid);\r\nreturn 0;\r\n}\r\nLASSERT(offsetof(struct kib_msg, ibm_u.immediate.ibim_payload[payload_nob])\r\n<= IBLND_MSG_SIZE);\r\ntx = kiblnd_get_idle_tx(ni, target.nid);\r\nif (!tx) {\r\nCERROR("Can't send %d to %s: tx descs exhausted\n",\r\ntype, libcfs_nid2str(target.nid));\r\nreturn -ENOMEM;\r\n}\r\nibmsg = tx->tx_msg;\r\nibmsg->ibm_u.immediate.ibim_hdr = *hdr;\r\nrc = copy_from_iter(&ibmsg->ibm_u.immediate.ibim_payload, payload_nob,\r\n&from);\r\nif (rc != payload_nob) {\r\nkiblnd_pool_free_node(&tx->tx_pool->tpo_pool, &tx->tx_list);\r\nreturn -EFAULT;\r\n}\r\nnob = offsetof(struct kib_immediate_msg, ibim_payload[payload_nob]);\r\nkiblnd_init_tx_msg(ni, tx, IBLND_MSG_IMMEDIATE, nob);\r\ntx->tx_lntmsg[0] = lntmsg;\r\nkiblnd_launch_tx(ni, tx, target.nid);\r\nreturn 0;\r\n}\r\nstatic void\r\nkiblnd_reply(struct lnet_ni *ni, struct kib_rx *rx, struct lnet_msg *lntmsg)\r\n{\r\nstruct lnet_process_id target = lntmsg->msg_target;\r\nunsigned int niov = lntmsg->msg_niov;\r\nstruct kvec *iov = lntmsg->msg_iov;\r\nstruct bio_vec *kiov = lntmsg->msg_kiov;\r\nunsigned int offset = lntmsg->msg_offset;\r\nunsigned int nob = lntmsg->msg_len;\r\nstruct kib_tx *tx;\r\nint rc;\r\ntx = kiblnd_get_idle_tx(ni, rx->rx_conn->ibc_peer->ibp_nid);\r\nif (!tx) {\r\nCERROR("Can't get tx for REPLY to %s\n",\r\nlibcfs_nid2str(target.nid));\r\ngoto failed_0;\r\n}\r\nif (!nob)\r\nrc = 0;\r\nelse if (!kiov)\r\nrc = kiblnd_setup_rd_iov(ni, tx, tx->tx_rd,\r\nniov, iov, offset, nob);\r\nelse\r\nrc = kiblnd_setup_rd_kiov(ni, tx, tx->tx_rd,\r\nniov, kiov, offset, nob);\r\nif (rc) {\r\nCERROR("Can't setup GET src for %s: %d\n",\r\nlibcfs_nid2str(target.nid), rc);\r\ngoto failed_1;\r\n}\r\nrc = kiblnd_init_rdma(rx->rx_conn, tx,\r\nIBLND_MSG_GET_DONE, nob,\r\n&rx->rx_msg->ibm_u.get.ibgm_rd,\r\nrx->rx_msg->ibm_u.get.ibgm_cookie);\r\nif (rc < 0) {\r\nCERROR("Can't setup rdma for GET from %s: %d\n",\r\nlibcfs_nid2str(target.nid), rc);\r\ngoto failed_1;\r\n}\r\nif (!nob) {\r\nlnet_finalize(ni, lntmsg, 0);\r\n} else {\r\ntx->tx_lntmsg[0] = lntmsg;\r\n}\r\nkiblnd_queue_tx(tx, rx->rx_conn);\r\nreturn;\r\nfailed_1:\r\nkiblnd_tx_done(ni, tx);\r\nfailed_0:\r\nlnet_finalize(ni, lntmsg, -EIO);\r\n}\r\nint\r\nkiblnd_recv(struct lnet_ni *ni, void *private, struct lnet_msg *lntmsg,\r\nint delayed, struct iov_iter *to, unsigned int rlen)\r\n{\r\nstruct kib_rx *rx = private;\r\nstruct kib_msg *rxmsg = rx->rx_msg;\r\nstruct kib_conn *conn = rx->rx_conn;\r\nstruct kib_tx *tx;\r\nint nob;\r\nint post_credit = IBLND_POSTRX_PEER_CREDIT;\r\nint rc = 0;\r\nLASSERT(iov_iter_count(to) <= rlen);\r\nLASSERT(!in_interrupt());\r\nswitch (rxmsg->ibm_type) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_MSG_IMMEDIATE:\r\nnob = offsetof(struct kib_msg, ibm_u.immediate.ibim_payload[rlen]);\r\nif (nob > rx->rx_nob) {\r\nCERROR("Immediate message from %s too big: %d(%d)\n",\r\nlibcfs_nid2str(rxmsg->ibm_u.immediate.ibim_hdr.src_nid),\r\nnob, rx->rx_nob);\r\nrc = -EPROTO;\r\nbreak;\r\n}\r\nrc = copy_to_iter(&rxmsg->ibm_u.immediate.ibim_payload, rlen,\r\nto);\r\nif (rc != rlen) {\r\nrc = -EFAULT;\r\nbreak;\r\n}\r\nrc = 0;\r\nlnet_finalize(ni, lntmsg, 0);\r\nbreak;\r\ncase IBLND_MSG_PUT_REQ: {\r\nstruct kib_msg *txmsg;\r\nstruct kib_rdma_desc *rd;\r\nif (!iov_iter_count(to)) {\r\nlnet_finalize(ni, lntmsg, 0);\r\nkiblnd_send_completion(rx->rx_conn, IBLND_MSG_PUT_NAK, 0,\r\nrxmsg->ibm_u.putreq.ibprm_cookie);\r\nbreak;\r\n}\r\ntx = kiblnd_get_idle_tx(ni, conn->ibc_peer->ibp_nid);\r\nif (!tx) {\r\nCERROR("Can't allocate tx for %s\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nrc = -ENOMEM;\r\nbreak;\r\n}\r\ntxmsg = tx->tx_msg;\r\nrd = &txmsg->ibm_u.putack.ibpam_rd;\r\nif (!(to->type & ITER_BVEC))\r\nrc = kiblnd_setup_rd_iov(ni, tx, rd,\r\nto->nr_segs, to->kvec,\r\nto->iov_offset,\r\niov_iter_count(to));\r\nelse\r\nrc = kiblnd_setup_rd_kiov(ni, tx, rd,\r\nto->nr_segs, to->bvec,\r\nto->iov_offset,\r\niov_iter_count(to));\r\nif (rc) {\r\nCERROR("Can't setup PUT sink for %s: %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), rc);\r\nkiblnd_tx_done(ni, tx);\r\nkiblnd_send_completion(rx->rx_conn, IBLND_MSG_PUT_NAK, rc,\r\nrxmsg->ibm_u.putreq.ibprm_cookie);\r\nbreak;\r\n}\r\nnob = offsetof(struct kib_putack_msg, ibpam_rd.rd_frags[rd->rd_nfrags]);\r\ntxmsg->ibm_u.putack.ibpam_src_cookie = rxmsg->ibm_u.putreq.ibprm_cookie;\r\ntxmsg->ibm_u.putack.ibpam_dst_cookie = tx->tx_cookie;\r\nkiblnd_init_tx_msg(ni, tx, IBLND_MSG_PUT_ACK, nob);\r\ntx->tx_lntmsg[0] = lntmsg;\r\ntx->tx_waiting = 1;\r\nkiblnd_queue_tx(tx, conn);\r\npost_credit = IBLND_POSTRX_NO_CREDIT;\r\nbreak;\r\n}\r\ncase IBLND_MSG_GET_REQ:\r\nif (lntmsg) {\r\nkiblnd_reply(ni, rx, lntmsg);\r\n} else {\r\nkiblnd_send_completion(rx->rx_conn, IBLND_MSG_GET_DONE,\r\n-ENODATA,\r\nrxmsg->ibm_u.get.ibgm_cookie);\r\n}\r\nbreak;\r\n}\r\nkiblnd_post_rx(rx, post_credit);\r\nreturn rc;\r\n}\r\nint\r\nkiblnd_thread_start(int (*fn)(void *arg), void *arg, char *name)\r\n{\r\nstruct task_struct *task = kthread_run(fn, arg, "%s", name);\r\nif (IS_ERR(task))\r\nreturn PTR_ERR(task);\r\natomic_inc(&kiblnd_data.kib_nthreads);\r\nreturn 0;\r\n}\r\nstatic void\r\nkiblnd_thread_fini(void)\r\n{\r\natomic_dec(&kiblnd_data.kib_nthreads);\r\n}\r\nstatic void\r\nkiblnd_peer_alive(struct kib_peer *peer)\r\n{\r\npeer->ibp_last_alive = cfs_time_current();\r\nmb();\r\n}\r\nstatic void\r\nkiblnd_peer_notify(struct kib_peer *peer)\r\n{\r\nint error = 0;\r\nunsigned long last_alive = 0;\r\nunsigned long flags;\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nif (kiblnd_peer_idle(peer) && peer->ibp_error) {\r\nerror = peer->ibp_error;\r\npeer->ibp_error = 0;\r\nlast_alive = peer->ibp_last_alive;\r\n}\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nif (error)\r\nlnet_notify(peer->ibp_ni,\r\npeer->ibp_nid, 0, last_alive);\r\n}\r\nvoid\r\nkiblnd_close_conn_locked(struct kib_conn *conn, int error)\r\n{\r\nstruct kib_peer *peer = conn->ibc_peer;\r\nstruct kib_dev *dev;\r\nunsigned long flags;\r\nLASSERT(error || conn->ibc_state >= IBLND_CONN_ESTABLISHED);\r\nif (error && !conn->ibc_comms_error)\r\nconn->ibc_comms_error = error;\r\nif (conn->ibc_state != IBLND_CONN_ESTABLISHED)\r\nreturn;\r\nif (!error &&\r\nlist_empty(&conn->ibc_tx_noops) &&\r\nlist_empty(&conn->ibc_tx_queue) &&\r\nlist_empty(&conn->ibc_tx_queue_rsrvd) &&\r\nlist_empty(&conn->ibc_tx_queue_nocred) &&\r\nlist_empty(&conn->ibc_active_txs)) {\r\nCDEBUG(D_NET, "closing conn to %s\n",\r\nlibcfs_nid2str(peer->ibp_nid));\r\n} else {\r\nCNETERR("Closing conn to %s: error %d%s%s%s%s%s\n",\r\nlibcfs_nid2str(peer->ibp_nid), error,\r\nlist_empty(&conn->ibc_tx_queue) ? "" : "(sending)",\r\nlist_empty(&conn->ibc_tx_noops) ? "" : "(sending_noops)",\r\nlist_empty(&conn->ibc_tx_queue_rsrvd) ? "" : "(sending_rsrvd)",\r\nlist_empty(&conn->ibc_tx_queue_nocred) ? "" : "(sending_nocred)",\r\nlist_empty(&conn->ibc_active_txs) ? "" : "(waiting)");\r\n}\r\ndev = ((struct kib_net *)peer->ibp_ni->ni_data)->ibn_dev;\r\nlist_del(&conn->ibc_list);\r\nif (list_empty(&peer->ibp_conns) &&\r\nkiblnd_peer_active(peer)) {\r\nkiblnd_unlink_peer_locked(peer);\r\npeer->ibp_error = conn->ibc_comms_error;\r\n}\r\nkiblnd_set_conn_state(conn, IBLND_CONN_CLOSING);\r\nif (error &&\r\nkiblnd_dev_can_failover(dev)) {\r\nlist_add_tail(&dev->ibd_fail_list,\r\n&kiblnd_data.kib_failed_devs);\r\nwake_up(&kiblnd_data.kib_failover_waitq);\r\n}\r\nspin_lock_irqsave(&kiblnd_data.kib_connd_lock, flags);\r\nlist_add_tail(&conn->ibc_list, &kiblnd_data.kib_connd_conns);\r\nwake_up(&kiblnd_data.kib_connd_waitq);\r\nspin_unlock_irqrestore(&kiblnd_data.kib_connd_lock, flags);\r\n}\r\nvoid\r\nkiblnd_close_conn(struct kib_conn *conn, int error)\r\n{\r\nunsigned long flags;\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nkiblnd_close_conn_locked(conn, error);\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\n}\r\nstatic void\r\nkiblnd_handle_early_rxs(struct kib_conn *conn)\r\n{\r\nunsigned long flags;\r\nstruct kib_rx *rx;\r\nstruct kib_rx *tmp;\r\nLASSERT(!in_interrupt());\r\nLASSERT(conn->ibc_state >= IBLND_CONN_ESTABLISHED);\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nlist_for_each_entry_safe(rx, tmp, &conn->ibc_early_rxs, rx_list) {\r\nlist_del(&rx->rx_list);\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nkiblnd_handle_rx(rx);\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\n}\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\n}\r\nstatic void\r\nkiblnd_abort_txs(struct kib_conn *conn, struct list_head *txs)\r\n{\r\nLIST_HEAD(zombies);\r\nstruct list_head *tmp;\r\nstruct list_head *nxt;\r\nstruct kib_tx *tx;\r\nspin_lock(&conn->ibc_lock);\r\nlist_for_each_safe(tmp, nxt, txs) {\r\ntx = list_entry(tmp, struct kib_tx, tx_list);\r\nif (txs == &conn->ibc_active_txs) {\r\nLASSERT(!tx->tx_queued);\r\nLASSERT(tx->tx_waiting || tx->tx_sending);\r\n} else {\r\nLASSERT(tx->tx_queued);\r\n}\r\ntx->tx_status = -ECONNABORTED;\r\ntx->tx_waiting = 0;\r\nif (!tx->tx_sending) {\r\ntx->tx_queued = 0;\r\nlist_del(&tx->tx_list);\r\nlist_add(&tx->tx_list, &zombies);\r\n}\r\n}\r\nspin_unlock(&conn->ibc_lock);\r\nkiblnd_txlist_done(conn->ibc_peer->ibp_ni, &zombies, -ECONNABORTED);\r\n}\r\nstatic void\r\nkiblnd_finalise_conn(struct kib_conn *conn)\r\n{\r\nLASSERT(!in_interrupt());\r\nLASSERT(conn->ibc_state > IBLND_CONN_INIT);\r\nkiblnd_set_conn_state(conn, IBLND_CONN_DISCONNECTED);\r\nkiblnd_abort_receives(conn);\r\nkiblnd_abort_txs(conn, &conn->ibc_tx_noops);\r\nkiblnd_abort_txs(conn, &conn->ibc_tx_queue);\r\nkiblnd_abort_txs(conn, &conn->ibc_tx_queue_rsrvd);\r\nkiblnd_abort_txs(conn, &conn->ibc_tx_queue_nocred);\r\nkiblnd_abort_txs(conn, &conn->ibc_active_txs);\r\nkiblnd_handle_early_rxs(conn);\r\n}\r\nstatic void\r\nkiblnd_peer_connect_failed(struct kib_peer *peer, int active, int error)\r\n{\r\nLIST_HEAD(zombies);\r\nunsigned long flags;\r\nLASSERT(error);\r\nLASSERT(!in_interrupt());\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nif (active) {\r\nLASSERT(peer->ibp_connecting > 0);\r\npeer->ibp_connecting--;\r\n} else {\r\nLASSERT(peer->ibp_accepting > 0);\r\npeer->ibp_accepting--;\r\n}\r\nif (kiblnd_peer_connecting(peer)) {\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock,\r\nflags);\r\nreturn;\r\n}\r\npeer->ibp_reconnected = 0;\r\nif (list_empty(&peer->ibp_conns)) {\r\nlist_add(&zombies, &peer->ibp_tx_queue);\r\nlist_del_init(&peer->ibp_tx_queue);\r\nif (kiblnd_peer_active(peer))\r\nkiblnd_unlink_peer_locked(peer);\r\npeer->ibp_error = error;\r\n} else {\r\nLASSERT(list_empty(&peer->ibp_tx_queue));\r\n}\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nkiblnd_peer_notify(peer);\r\nif (list_empty(&zombies))\r\nreturn;\r\nCNETERR("Deleting messages for %s: connection failed\n",\r\nlibcfs_nid2str(peer->ibp_nid));\r\nkiblnd_txlist_done(peer->ibp_ni, &zombies, -EHOSTUNREACH);\r\n}\r\nstatic void\r\nkiblnd_connreq_done(struct kib_conn *conn, int status)\r\n{\r\nstruct kib_peer *peer = conn->ibc_peer;\r\nstruct kib_tx *tx;\r\nstruct kib_tx *tmp;\r\nstruct list_head txs;\r\nunsigned long flags;\r\nint active;\r\nactive = (conn->ibc_state == IBLND_CONN_ACTIVE_CONNECT);\r\nCDEBUG(D_NET, "%s: active(%d), version(%x), status(%d)\n",\r\nlibcfs_nid2str(peer->ibp_nid), active,\r\nconn->ibc_version, status);\r\nLASSERT(!in_interrupt());\r\nLASSERT((conn->ibc_state == IBLND_CONN_ACTIVE_CONNECT &&\r\npeer->ibp_connecting > 0) ||\r\n(conn->ibc_state == IBLND_CONN_PASSIVE_WAIT &&\r\npeer->ibp_accepting > 0));\r\nLIBCFS_FREE(conn->ibc_connvars, sizeof(*conn->ibc_connvars));\r\nconn->ibc_connvars = NULL;\r\nif (status) {\r\nkiblnd_peer_connect_failed(peer, active, status);\r\nkiblnd_finalise_conn(conn);\r\nreturn;\r\n}\r\nwrite_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nconn->ibc_last_send = jiffies;\r\nkiblnd_set_conn_state(conn, IBLND_CONN_ESTABLISHED);\r\nkiblnd_peer_alive(peer);\r\nkiblnd_conn_addref(conn);\r\nlist_add(&conn->ibc_list, &peer->ibp_conns);\r\npeer->ibp_reconnected = 0;\r\nif (active)\r\npeer->ibp_connecting--;\r\nelse\r\npeer->ibp_accepting--;\r\nif (!peer->ibp_version) {\r\npeer->ibp_version = conn->ibc_version;\r\npeer->ibp_incarnation = conn->ibc_incarnation;\r\n}\r\nif (peer->ibp_version != conn->ibc_version ||\r\npeer->ibp_incarnation != conn->ibc_incarnation) {\r\nkiblnd_close_stale_conns_locked(peer, conn->ibc_version,\r\nconn->ibc_incarnation);\r\npeer->ibp_version = conn->ibc_version;\r\npeer->ibp_incarnation = conn->ibc_incarnation;\r\n}\r\nlist_add(&txs, &peer->ibp_tx_queue);\r\nlist_del_init(&peer->ibp_tx_queue);\r\nif (!kiblnd_peer_active(peer) ||\r\nconn->ibc_comms_error) {\r\nstruct lnet_ni *ni = peer->ibp_ni;\r\nkiblnd_close_conn_locked(conn, -ECONNABORTED);\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nkiblnd_txlist_done(ni, &txs, -ECONNABORTED);\r\nreturn;\r\n}\r\nkiblnd_conn_addref(conn);\r\nwrite_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nspin_lock(&conn->ibc_lock);\r\nlist_for_each_entry_safe(tx, tmp, &txs, tx_list) {\r\nlist_del(&tx->tx_list);\r\nkiblnd_queue_tx_locked(tx, conn);\r\n}\r\nkiblnd_check_sends_locked(conn);\r\nspin_unlock(&conn->ibc_lock);\r\nkiblnd_handle_early_rxs(conn);\r\nkiblnd_conn_decref(conn);\r\n}\r\nstatic void\r\nkiblnd_reject(struct rdma_cm_id *cmid, struct kib_rej *rej)\r\n{\r\nint rc;\r\nrc = rdma_reject(cmid, rej, sizeof(*rej));\r\nif (rc)\r\nCWARN("Error %d sending reject\n", rc);\r\n}\r\nstatic int\r\nkiblnd_passive_connect(struct rdma_cm_id *cmid, void *priv, int priv_nob)\r\n{\r\nrwlock_t *g_lock = &kiblnd_data.kib_global_lock;\r\nstruct kib_msg *reqmsg = priv;\r\nstruct kib_msg *ackmsg;\r\nstruct kib_dev *ibdev;\r\nstruct kib_peer *peer;\r\nstruct kib_peer *peer2;\r\nstruct kib_conn *conn;\r\nstruct lnet_ni *ni = NULL;\r\nstruct kib_net *net = NULL;\r\nlnet_nid_t nid;\r\nstruct rdma_conn_param cp;\r\nstruct kib_rej rej;\r\nint version = IBLND_MSG_VERSION;\r\nunsigned long flags;\r\nint max_frags;\r\nint rc;\r\nstruct sockaddr_in *peer_addr;\r\nLASSERT(!in_interrupt());\r\nibdev = (struct kib_dev *)cmid->context;\r\nLASSERT(ibdev);\r\nmemset(&rej, 0, sizeof(rej));\r\nrej.ibr_magic = IBLND_MSG_MAGIC;\r\nrej.ibr_why = IBLND_REJECT_FATAL;\r\nrej.ibr_cp.ibcp_max_msg_size = IBLND_MSG_SIZE;\r\npeer_addr = (struct sockaddr_in *)&cmid->route.addr.dst_addr;\r\nif (*kiblnd_tunables.kib_require_priv_port &&\r\nntohs(peer_addr->sin_port) >= PROT_SOCK) {\r\n__u32 ip = ntohl(peer_addr->sin_addr.s_addr);\r\nCERROR("Peer's port (%pI4h:%hu) is not privileged\n",\r\n&ip, ntohs(peer_addr->sin_port));\r\ngoto failed;\r\n}\r\nif (priv_nob < offsetof(struct kib_msg, ibm_type)) {\r\nCERROR("Short connection request\n");\r\ngoto failed;\r\n}\r\nif (reqmsg->ibm_magic == LNET_PROTO_MAGIC ||\r\nreqmsg->ibm_magic == __swab32(LNET_PROTO_MAGIC))\r\ngoto failed;\r\nif (reqmsg->ibm_magic == IBLND_MSG_MAGIC &&\r\nreqmsg->ibm_version != IBLND_MSG_VERSION &&\r\nreqmsg->ibm_version != IBLND_MSG_VERSION_1)\r\ngoto failed;\r\nif (reqmsg->ibm_magic == __swab32(IBLND_MSG_MAGIC) &&\r\nreqmsg->ibm_version != __swab16(IBLND_MSG_VERSION) &&\r\nreqmsg->ibm_version != __swab16(IBLND_MSG_VERSION_1))\r\ngoto failed;\r\nrc = kiblnd_unpack_msg(reqmsg, priv_nob);\r\nif (rc) {\r\nCERROR("Can't parse connection request: %d\n", rc);\r\ngoto failed;\r\n}\r\nnid = reqmsg->ibm_srcnid;\r\nni = lnet_net2ni(LNET_NIDNET(reqmsg->ibm_dstnid));\r\nif (ni) {\r\nnet = (struct kib_net *)ni->ni_data;\r\nrej.ibr_incarnation = net->ibn_incarnation;\r\n}\r\nif (!ni ||\r\nni->ni_nid != reqmsg->ibm_dstnid ||\r\nnet->ibn_dev != ibdev) {\r\nCERROR("Can't accept conn from %s on %s (%s:%d:%pI4h): bad dst nid %s\n",\r\nlibcfs_nid2str(nid),\r\n!ni ? "NA" : libcfs_nid2str(ni->ni_nid),\r\nibdev->ibd_ifname, ibdev->ibd_nnets,\r\n&ibdev->ibd_ifip,\r\nlibcfs_nid2str(reqmsg->ibm_dstnid));\r\ngoto failed;\r\n}\r\nif (reqmsg->ibm_dststamp &&\r\nreqmsg->ibm_dststamp != net->ibn_incarnation) {\r\nCWARN("Stale connection request\n");\r\nrej.ibr_why = IBLND_REJECT_CONN_STALE;\r\ngoto failed;\r\n}\r\nversion = reqmsg->ibm_version;\r\nif (reqmsg->ibm_type != IBLND_MSG_CONNREQ) {\r\nCERROR("Unexpected connreq msg type: %x from %s\n",\r\nreqmsg->ibm_type, libcfs_nid2str(nid));\r\ngoto failed;\r\n}\r\nif (reqmsg->ibm_u.connparams.ibcp_queue_depth >\r\nkiblnd_msg_queue_size(version, ni)) {\r\nCERROR("Can't accept conn from %s, queue depth too large: %d (<=%d wanted)\n",\r\nlibcfs_nid2str(nid),\r\nreqmsg->ibm_u.connparams.ibcp_queue_depth,\r\nkiblnd_msg_queue_size(version, ni));\r\nif (version == IBLND_MSG_VERSION)\r\nrej.ibr_why = IBLND_REJECT_MSG_QUEUE_SIZE;\r\ngoto failed;\r\n}\r\nmax_frags = reqmsg->ibm_u.connparams.ibcp_max_frags >> IBLND_FRAG_SHIFT;\r\nif (max_frags > kiblnd_rdma_frags(version, ni)) {\r\nCWARN("Can't accept conn from %s (version %x): max message size %d is too large (%d wanted)\n",\r\nlibcfs_nid2str(nid), version, max_frags,\r\nkiblnd_rdma_frags(version, ni));\r\nif (version >= IBLND_MSG_VERSION)\r\nrej.ibr_why = IBLND_REJECT_RDMA_FRAGS;\r\ngoto failed;\r\n} else if (max_frags < kiblnd_rdma_frags(version, ni) &&\r\n!net->ibn_fmr_ps) {\r\nCWARN("Can't accept conn from %s (version %x): max message size %d incompatible without FMR pool (%d wanted)\n",\r\nlibcfs_nid2str(nid), version, max_frags,\r\nkiblnd_rdma_frags(version, ni));\r\nif (version == IBLND_MSG_VERSION)\r\nrej.ibr_why = IBLND_REJECT_RDMA_FRAGS;\r\ngoto failed;\r\n}\r\nif (reqmsg->ibm_u.connparams.ibcp_max_msg_size > IBLND_MSG_SIZE) {\r\nCERROR("Can't accept %s: message size %d too big (%d max)\n",\r\nlibcfs_nid2str(nid),\r\nreqmsg->ibm_u.connparams.ibcp_max_msg_size,\r\nIBLND_MSG_SIZE);\r\ngoto failed;\r\n}\r\nrc = kiblnd_create_peer(ni, &peer, nid);\r\nif (rc) {\r\nCERROR("Can't create peer for %s\n", libcfs_nid2str(nid));\r\nrej.ibr_why = IBLND_REJECT_NO_RESOURCES;\r\ngoto failed;\r\n}\r\npeer->ibp_max_frags = max_frags;\r\npeer->ibp_queue_depth = reqmsg->ibm_u.connparams.ibcp_queue_depth;\r\nwrite_lock_irqsave(g_lock, flags);\r\npeer2 = kiblnd_find_peer_locked(nid);\r\nif (peer2) {\r\nif (!peer2->ibp_version) {\r\npeer2->ibp_version = version;\r\npeer2->ibp_incarnation = reqmsg->ibm_srcstamp;\r\n}\r\nif (peer2->ibp_incarnation != reqmsg->ibm_srcstamp ||\r\npeer2->ibp_version != version) {\r\nkiblnd_close_peer_conns_locked(peer2, -ESTALE);\r\nif (kiblnd_peer_active(peer2)) {\r\npeer2->ibp_incarnation = reqmsg->ibm_srcstamp;\r\npeer2->ibp_version = version;\r\n}\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nCWARN("Conn stale %s version %x/%x incarnation %llu/%llu\n",\r\nlibcfs_nid2str(nid), peer2->ibp_version, version,\r\npeer2->ibp_incarnation, reqmsg->ibm_srcstamp);\r\nkiblnd_peer_decref(peer);\r\nrej.ibr_why = IBLND_REJECT_CONN_STALE;\r\ngoto failed;\r\n}\r\nif (peer2->ibp_connecting &&\r\nnid < ni->ni_nid && peer2->ibp_races <\r\nMAX_CONN_RACES_BEFORE_ABORT) {\r\npeer2->ibp_races++;\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nCDEBUG(D_NET, "Conn race %s\n",\r\nlibcfs_nid2str(peer2->ibp_nid));\r\nkiblnd_peer_decref(peer);\r\nrej.ibr_why = IBLND_REJECT_CONN_RACE;\r\ngoto failed;\r\n}\r\nif (peer2->ibp_races >= MAX_CONN_RACES_BEFORE_ABORT)\r\nCNETERR("Conn race %s: unresolved after %d attempts, letting lower NID win\n",\r\nlibcfs_nid2str(peer2->ibp_nid),\r\nMAX_CONN_RACES_BEFORE_ABORT);\r\npeer2->ibp_reconnecting = 0;\r\npeer2->ibp_races = 0;\r\npeer2->ibp_accepting++;\r\nkiblnd_peer_addref(peer2);\r\npeer2->ibp_max_frags = peer->ibp_max_frags;\r\npeer2->ibp_queue_depth = peer->ibp_queue_depth;\r\nwrite_unlock_irqrestore(g_lock, flags);\r\nkiblnd_peer_decref(peer);\r\npeer = peer2;\r\n} else {\r\nLASSERT(!peer->ibp_accepting);\r\nLASSERT(!peer->ibp_version &&\r\n!peer->ibp_incarnation);\r\npeer->ibp_accepting = 1;\r\npeer->ibp_version = version;\r\npeer->ibp_incarnation = reqmsg->ibm_srcstamp;\r\nLASSERT(!net->ibn_shutdown);\r\nkiblnd_peer_addref(peer);\r\nlist_add_tail(&peer->ibp_list, kiblnd_nid2peerlist(nid));\r\nwrite_unlock_irqrestore(g_lock, flags);\r\n}\r\nconn = kiblnd_create_conn(peer, cmid, IBLND_CONN_PASSIVE_WAIT,\r\nversion);\r\nif (!conn) {\r\nkiblnd_peer_connect_failed(peer, 0, -ENOMEM);\r\nkiblnd_peer_decref(peer);\r\nrej.ibr_why = IBLND_REJECT_NO_RESOURCES;\r\ngoto failed;\r\n}\r\nconn->ibc_incarnation = reqmsg->ibm_srcstamp;\r\nconn->ibc_credits = conn->ibc_queue_depth;\r\nconn->ibc_reserved_credits = conn->ibc_queue_depth;\r\nLASSERT(conn->ibc_credits + conn->ibc_reserved_credits +\r\nIBLND_OOB_MSGS(version) <= IBLND_RX_MSGS(conn));\r\nackmsg = &conn->ibc_connvars->cv_msg;\r\nmemset(ackmsg, 0, sizeof(*ackmsg));\r\nkiblnd_init_msg(ackmsg, IBLND_MSG_CONNACK,\r\nsizeof(ackmsg->ibm_u.connparams));\r\nackmsg->ibm_u.connparams.ibcp_queue_depth = conn->ibc_queue_depth;\r\nackmsg->ibm_u.connparams.ibcp_max_frags = conn->ibc_max_frags << IBLND_FRAG_SHIFT;\r\nackmsg->ibm_u.connparams.ibcp_max_msg_size = IBLND_MSG_SIZE;\r\nkiblnd_pack_msg(ni, ackmsg, version, 0, nid, reqmsg->ibm_srcstamp);\r\nmemset(&cp, 0, sizeof(cp));\r\ncp.private_data = ackmsg;\r\ncp.private_data_len = ackmsg->ibm_nob;\r\ncp.responder_resources = 0;\r\ncp.initiator_depth = 0;\r\ncp.flow_control = 1;\r\ncp.retry_count = *kiblnd_tunables.kib_retry_count;\r\ncp.rnr_retry_count = *kiblnd_tunables.kib_rnr_retry_count;\r\nCDEBUG(D_NET, "Accept %s\n", libcfs_nid2str(nid));\r\nrc = rdma_accept(cmid, &cp);\r\nif (rc) {\r\nCERROR("Can't accept %s: %d\n", libcfs_nid2str(nid), rc);\r\nrej.ibr_version = version;\r\nrej.ibr_why = IBLND_REJECT_FATAL;\r\nkiblnd_reject(cmid, &rej);\r\nkiblnd_connreq_done(conn, rc);\r\nkiblnd_conn_decref(conn);\r\n}\r\nlnet_ni_decref(ni);\r\nreturn 0;\r\nfailed:\r\nif (ni) {\r\nrej.ibr_cp.ibcp_queue_depth = kiblnd_msg_queue_size(version, ni);\r\nrej.ibr_cp.ibcp_max_frags = kiblnd_rdma_frags(version, ni);\r\nlnet_ni_decref(ni);\r\n}\r\nrej.ibr_version = version;\r\nkiblnd_reject(cmid, &rej);\r\nreturn -ECONNREFUSED;\r\n}\r\nstatic void\r\nkiblnd_check_reconnect(struct kib_conn *conn, int version,\r\n__u64 incarnation, int why, struct kib_connparams *cp)\r\n{\r\nrwlock_t *glock = &kiblnd_data.kib_global_lock;\r\nstruct kib_peer *peer = conn->ibc_peer;\r\nchar *reason;\r\nint msg_size = IBLND_MSG_SIZE;\r\nint frag_num = -1;\r\nint queue_dep = -1;\r\nbool reconnect;\r\nunsigned long flags;\r\nLASSERT(conn->ibc_state == IBLND_CONN_ACTIVE_CONNECT);\r\nLASSERT(peer->ibp_connecting > 0);\r\nLASSERT(!peer->ibp_reconnecting);\r\nif (cp) {\r\nmsg_size = cp->ibcp_max_msg_size;\r\nfrag_num = cp->ibcp_max_frags << IBLND_FRAG_SHIFT;\r\nqueue_dep = cp->ibcp_queue_depth;\r\n}\r\nwrite_lock_irqsave(glock, flags);\r\nreconnect = (!list_empty(&peer->ibp_tx_queue) ||\r\npeer->ibp_version != version) &&\r\npeer->ibp_connecting == 1 &&\r\n!peer->ibp_accepting;\r\nif (!reconnect) {\r\nreason = "no need";\r\ngoto out;\r\n}\r\nswitch (why) {\r\ndefault:\r\nreason = "Unknown";\r\nbreak;\r\ncase IBLND_REJECT_RDMA_FRAGS: {\r\nstruct lnet_ioctl_config_lnd_tunables *tunables;\r\nif (!cp) {\r\nreason = "can't negotiate max frags";\r\ngoto out;\r\n}\r\ntunables = peer->ibp_ni->ni_lnd_tunables;\r\nif (!tunables->lt_tun_u.lt_o2ib.lnd_map_on_demand) {\r\nreason = "map_on_demand must be enabled";\r\ngoto out;\r\n}\r\nif (conn->ibc_max_frags <= frag_num) {\r\nreason = "unsupported max frags";\r\ngoto out;\r\n}\r\npeer->ibp_max_frags = frag_num;\r\nreason = "rdma fragments";\r\nbreak;\r\n}\r\ncase IBLND_REJECT_MSG_QUEUE_SIZE:\r\nif (!cp) {\r\nreason = "can't negotiate queue depth";\r\ngoto out;\r\n}\r\nif (conn->ibc_queue_depth <= queue_dep) {\r\nreason = "unsupported queue depth";\r\ngoto out;\r\n}\r\npeer->ibp_queue_depth = queue_dep;\r\nreason = "queue depth";\r\nbreak;\r\ncase IBLND_REJECT_CONN_STALE:\r\nreason = "stale";\r\nbreak;\r\ncase IBLND_REJECT_CONN_RACE:\r\nreason = "conn race";\r\nbreak;\r\ncase IBLND_REJECT_CONN_UNCOMPAT:\r\nreason = "version negotiation";\r\nbreak;\r\n}\r\nconn->ibc_reconnect = 1;\r\npeer->ibp_reconnecting = 1;\r\npeer->ibp_version = version;\r\nif (incarnation)\r\npeer->ibp_incarnation = incarnation;\r\nout:\r\nwrite_unlock_irqrestore(glock, flags);\r\nCNETERR("%s: %s (%s), %x, %x, msg_size: %d, queue_depth: %d/%d, max_frags: %d/%d\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nreconnect ? "reconnect" : "don't reconnect",\r\nreason, IBLND_MSG_VERSION, version, msg_size,\r\nconn->ibc_queue_depth, queue_dep,\r\nconn->ibc_max_frags, frag_num);\r\n}\r\nstatic void\r\nkiblnd_rejected(struct kib_conn *conn, int reason, void *priv, int priv_nob)\r\n{\r\nstruct kib_peer *peer = conn->ibc_peer;\r\nLASSERT(!in_interrupt());\r\nLASSERT(conn->ibc_state == IBLND_CONN_ACTIVE_CONNECT);\r\nswitch (reason) {\r\ncase IB_CM_REJ_STALE_CONN:\r\nkiblnd_check_reconnect(conn, IBLND_MSG_VERSION, 0,\r\nIBLND_REJECT_CONN_STALE, NULL);\r\nbreak;\r\ncase IB_CM_REJ_INVALID_SERVICE_ID:\r\nCNETERR("%s rejected: no listener at %d\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\n*kiblnd_tunables.kib_service);\r\nbreak;\r\ncase IB_CM_REJ_CONSUMER_DEFINED:\r\nif (priv_nob >= offsetof(struct kib_rej, ibr_padding)) {\r\nstruct kib_rej *rej = priv;\r\nstruct kib_connparams *cp = NULL;\r\nint flip = 0;\r\n__u64 incarnation = -1;\r\nif (rej->ibr_magic == __swab32(IBLND_MSG_MAGIC) ||\r\nrej->ibr_magic == __swab32(LNET_PROTO_MAGIC)) {\r\n__swab32s(&rej->ibr_magic);\r\n__swab16s(&rej->ibr_version);\r\nflip = 1;\r\n}\r\nif (priv_nob >= sizeof(struct kib_rej) &&\r\nrej->ibr_version > IBLND_MSG_VERSION_1) {\r\ncp = &rej->ibr_cp;\r\nif (flip) {\r\n__swab64s(&rej->ibr_incarnation);\r\n__swab16s(&cp->ibcp_queue_depth);\r\n__swab16s(&cp->ibcp_max_frags);\r\n__swab32s(&cp->ibcp_max_msg_size);\r\n}\r\nincarnation = rej->ibr_incarnation;\r\n}\r\nif (rej->ibr_magic != IBLND_MSG_MAGIC &&\r\nrej->ibr_magic != LNET_PROTO_MAGIC) {\r\nCERROR("%s rejected: consumer defined fatal error\n",\r\nlibcfs_nid2str(peer->ibp_nid));\r\nbreak;\r\n}\r\nif (rej->ibr_version != IBLND_MSG_VERSION &&\r\nrej->ibr_version != IBLND_MSG_VERSION_1) {\r\nCERROR("%s rejected: o2iblnd version %x error\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nrej->ibr_version);\r\nbreak;\r\n}\r\nif (rej->ibr_why == IBLND_REJECT_FATAL &&\r\nrej->ibr_version == IBLND_MSG_VERSION_1) {\r\nCDEBUG(D_NET, "rejected by old version peer %s: %x\n",\r\nlibcfs_nid2str(peer->ibp_nid), rej->ibr_version);\r\nif (conn->ibc_version != IBLND_MSG_VERSION_1)\r\nrej->ibr_why = IBLND_REJECT_CONN_UNCOMPAT;\r\n}\r\nswitch (rej->ibr_why) {\r\ncase IBLND_REJECT_CONN_RACE:\r\ncase IBLND_REJECT_CONN_STALE:\r\ncase IBLND_REJECT_CONN_UNCOMPAT:\r\ncase IBLND_REJECT_MSG_QUEUE_SIZE:\r\ncase IBLND_REJECT_RDMA_FRAGS:\r\nkiblnd_check_reconnect(conn, rej->ibr_version,\r\nincarnation,\r\nrej->ibr_why, cp);\r\nbreak;\r\ncase IBLND_REJECT_NO_RESOURCES:\r\nCERROR("%s rejected: o2iblnd no resources\n",\r\nlibcfs_nid2str(peer->ibp_nid));\r\nbreak;\r\ncase IBLND_REJECT_FATAL:\r\nCERROR("%s rejected: o2iblnd fatal error\n",\r\nlibcfs_nid2str(peer->ibp_nid));\r\nbreak;\r\ndefault:\r\nCERROR("%s rejected: o2iblnd reason %d\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nrej->ibr_why);\r\nbreak;\r\n}\r\nbreak;\r\n}\r\ndefault:\r\nCNETERR("%s rejected: reason %d, size %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), reason, priv_nob);\r\nbreak;\r\n}\r\nkiblnd_connreq_done(conn, -ECONNREFUSED);\r\n}\r\nstatic void\r\nkiblnd_check_connreply(struct kib_conn *conn, void *priv, int priv_nob)\r\n{\r\nstruct kib_peer *peer = conn->ibc_peer;\r\nstruct lnet_ni *ni = peer->ibp_ni;\r\nstruct kib_net *net = ni->ni_data;\r\nstruct kib_msg *msg = priv;\r\nint ver = conn->ibc_version;\r\nint rc = kiblnd_unpack_msg(msg, priv_nob);\r\nunsigned long flags;\r\nLASSERT(net);\r\nif (rc) {\r\nCERROR("Can't unpack connack from %s: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), rc);\r\ngoto failed;\r\n}\r\nif (msg->ibm_type != IBLND_MSG_CONNACK) {\r\nCERROR("Unexpected message %d from %s\n",\r\nmsg->ibm_type, libcfs_nid2str(peer->ibp_nid));\r\nrc = -EPROTO;\r\ngoto failed;\r\n}\r\nif (ver != msg->ibm_version) {\r\nCERROR("%s replied version %x is different with requested version %x\n",\r\nlibcfs_nid2str(peer->ibp_nid), msg->ibm_version, ver);\r\nrc = -EPROTO;\r\ngoto failed;\r\n}\r\nif (msg->ibm_u.connparams.ibcp_queue_depth >\r\nconn->ibc_queue_depth) {\r\nCERROR("%s has incompatible queue depth %d (<=%d wanted)\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nmsg->ibm_u.connparams.ibcp_queue_depth,\r\nconn->ibc_queue_depth);\r\nrc = -EPROTO;\r\ngoto failed;\r\n}\r\nif ((msg->ibm_u.connparams.ibcp_max_frags >> IBLND_FRAG_SHIFT) >\r\nconn->ibc_max_frags) {\r\nCERROR("%s has incompatible max_frags %d (<=%d wanted)\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nmsg->ibm_u.connparams.ibcp_max_frags >> IBLND_FRAG_SHIFT,\r\nconn->ibc_max_frags);\r\nrc = -EPROTO;\r\ngoto failed;\r\n}\r\nif (msg->ibm_u.connparams.ibcp_max_msg_size > IBLND_MSG_SIZE) {\r\nCERROR("%s max message size %d too big (%d max)\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\nmsg->ibm_u.connparams.ibcp_max_msg_size,\r\nIBLND_MSG_SIZE);\r\nrc = -EPROTO;\r\ngoto failed;\r\n}\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nif (msg->ibm_dstnid == ni->ni_nid &&\r\nmsg->ibm_dststamp == net->ibn_incarnation)\r\nrc = 0;\r\nelse\r\nrc = -ESTALE;\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nif (rc) {\r\nCERROR("Bad connection reply from %s, rc = %d, version: %x max_frags: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), rc,\r\nmsg->ibm_version, msg->ibm_u.connparams.ibcp_max_frags);\r\ngoto failed;\r\n}\r\nconn->ibc_incarnation = msg->ibm_srcstamp;\r\nconn->ibc_credits = msg->ibm_u.connparams.ibcp_queue_depth;\r\nconn->ibc_reserved_credits = msg->ibm_u.connparams.ibcp_queue_depth;\r\nconn->ibc_queue_depth = msg->ibm_u.connparams.ibcp_queue_depth;\r\nconn->ibc_max_frags = msg->ibm_u.connparams.ibcp_max_frags >> IBLND_FRAG_SHIFT;\r\nLASSERT(conn->ibc_credits + conn->ibc_reserved_credits +\r\nIBLND_OOB_MSGS(ver) <= IBLND_RX_MSGS(conn));\r\nkiblnd_connreq_done(conn, 0);\r\nreturn;\r\nfailed:\r\nLASSERT(rc);\r\nconn->ibc_comms_error = rc;\r\nkiblnd_connreq_done(conn, 0);\r\n}\r\nstatic int\r\nkiblnd_active_connect(struct rdma_cm_id *cmid)\r\n{\r\nstruct kib_peer *peer = (struct kib_peer *)cmid->context;\r\nstruct kib_conn *conn;\r\nstruct kib_msg *msg;\r\nstruct rdma_conn_param cp;\r\nint version;\r\n__u64 incarnation;\r\nunsigned long flags;\r\nint rc;\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nincarnation = peer->ibp_incarnation;\r\nversion = !peer->ibp_version ? IBLND_MSG_VERSION :\r\npeer->ibp_version;\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nconn = kiblnd_create_conn(peer, cmid, IBLND_CONN_ACTIVE_CONNECT,\r\nversion);\r\nif (!conn) {\r\nkiblnd_peer_connect_failed(peer, 1, -ENOMEM);\r\nkiblnd_peer_decref(peer);\r\nreturn -ENOMEM;\r\n}\r\nmsg = &conn->ibc_connvars->cv_msg;\r\nmemset(msg, 0, sizeof(*msg));\r\nkiblnd_init_msg(msg, IBLND_MSG_CONNREQ, sizeof(msg->ibm_u.connparams));\r\nmsg->ibm_u.connparams.ibcp_queue_depth = conn->ibc_queue_depth;\r\nmsg->ibm_u.connparams.ibcp_max_frags = conn->ibc_max_frags << IBLND_FRAG_SHIFT;\r\nmsg->ibm_u.connparams.ibcp_max_msg_size = IBLND_MSG_SIZE;\r\nkiblnd_pack_msg(peer->ibp_ni, msg, version,\r\n0, peer->ibp_nid, incarnation);\r\nmemset(&cp, 0, sizeof(cp));\r\ncp.private_data = msg;\r\ncp.private_data_len = msg->ibm_nob;\r\ncp.responder_resources = 0;\r\ncp.initiator_depth = 0;\r\ncp.flow_control = 1;\r\ncp.retry_count = *kiblnd_tunables.kib_retry_count;\r\ncp.rnr_retry_count = *kiblnd_tunables.kib_rnr_retry_count;\r\nLASSERT(cmid->context == (void *)conn);\r\nLASSERT(conn->ibc_cmid == cmid);\r\nrc = rdma_connect(cmid, &cp);\r\nif (rc) {\r\nCERROR("Can't connect to %s: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), rc);\r\nkiblnd_connreq_done(conn, rc);\r\nkiblnd_conn_decref(conn);\r\n}\r\nreturn 0;\r\n}\r\nint\r\nkiblnd_cm_callback(struct rdma_cm_id *cmid, struct rdma_cm_event *event)\r\n{\r\nstruct kib_peer *peer;\r\nstruct kib_conn *conn;\r\nint rc;\r\nswitch (event->event) {\r\ndefault:\r\nCERROR("Unexpected event: %d, status: %d\n",\r\nevent->event, event->status);\r\nLBUG();\r\ncase RDMA_CM_EVENT_CONNECT_REQUEST:\r\nrc = kiblnd_passive_connect(cmid,\r\n(void *)KIBLND_CONN_PARAM(event),\r\nKIBLND_CONN_PARAM_LEN(event));\r\nCDEBUG(D_NET, "connreq: %d\n", rc);\r\nreturn rc;\r\ncase RDMA_CM_EVENT_ADDR_ERROR:\r\npeer = (struct kib_peer *)cmid->context;\r\nCNETERR("%s: ADDR ERROR %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), event->status);\r\nkiblnd_peer_connect_failed(peer, 1, -EHOSTUNREACH);\r\nkiblnd_peer_decref(peer);\r\nreturn -EHOSTUNREACH;\r\ncase RDMA_CM_EVENT_ADDR_RESOLVED:\r\npeer = (struct kib_peer *)cmid->context;\r\nCDEBUG(D_NET, "%s Addr resolved: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), event->status);\r\nif (event->status) {\r\nCNETERR("Can't resolve address for %s: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), event->status);\r\nrc = event->status;\r\n} else {\r\nrc = rdma_resolve_route(\r\ncmid, *kiblnd_tunables.kib_timeout * 1000);\r\nif (!rc)\r\nreturn 0;\r\nCERROR("Can't resolve route for %s: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), rc);\r\n}\r\nkiblnd_peer_connect_failed(peer, 1, rc);\r\nkiblnd_peer_decref(peer);\r\nreturn rc;\r\ncase RDMA_CM_EVENT_ROUTE_ERROR:\r\npeer = (struct kib_peer *)cmid->context;\r\nCNETERR("%s: ROUTE ERROR %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), event->status);\r\nkiblnd_peer_connect_failed(peer, 1, -EHOSTUNREACH);\r\nkiblnd_peer_decref(peer);\r\nreturn -EHOSTUNREACH;\r\ncase RDMA_CM_EVENT_ROUTE_RESOLVED:\r\npeer = (struct kib_peer *)cmid->context;\r\nCDEBUG(D_NET, "%s Route resolved: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), event->status);\r\nif (!event->status)\r\nreturn kiblnd_active_connect(cmid);\r\nCNETERR("Can't resolve route for %s: %d\n",\r\nlibcfs_nid2str(peer->ibp_nid), event->status);\r\nkiblnd_peer_connect_failed(peer, 1, event->status);\r\nkiblnd_peer_decref(peer);\r\nreturn event->status;\r\ncase RDMA_CM_EVENT_UNREACHABLE:\r\nconn = (struct kib_conn *)cmid->context;\r\nLASSERT(conn->ibc_state == IBLND_CONN_ACTIVE_CONNECT ||\r\nconn->ibc_state == IBLND_CONN_PASSIVE_WAIT);\r\nCNETERR("%s: UNREACHABLE %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), event->status);\r\nkiblnd_connreq_done(conn, -ENETDOWN);\r\nkiblnd_conn_decref(conn);\r\nreturn 0;\r\ncase RDMA_CM_EVENT_CONNECT_ERROR:\r\nconn = (struct kib_conn *)cmid->context;\r\nLASSERT(conn->ibc_state == IBLND_CONN_ACTIVE_CONNECT ||\r\nconn->ibc_state == IBLND_CONN_PASSIVE_WAIT);\r\nCNETERR("%s: CONNECT ERROR %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), event->status);\r\nkiblnd_connreq_done(conn, -ENOTCONN);\r\nkiblnd_conn_decref(conn);\r\nreturn 0;\r\ncase RDMA_CM_EVENT_REJECTED:\r\nconn = (struct kib_conn *)cmid->context;\r\nswitch (conn->ibc_state) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_CONN_PASSIVE_WAIT:\r\nCERROR("%s: REJECTED %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid),\r\nevent->status);\r\nkiblnd_connreq_done(conn, -ECONNRESET);\r\nbreak;\r\ncase IBLND_CONN_ACTIVE_CONNECT:\r\nkiblnd_rejected(conn, event->status,\r\n(void *)KIBLND_CONN_PARAM(event),\r\nKIBLND_CONN_PARAM_LEN(event));\r\nbreak;\r\n}\r\nkiblnd_conn_decref(conn);\r\nreturn 0;\r\ncase RDMA_CM_EVENT_ESTABLISHED:\r\nconn = (struct kib_conn *)cmid->context;\r\nswitch (conn->ibc_state) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_CONN_PASSIVE_WAIT:\r\nCDEBUG(D_NET, "ESTABLISHED (passive): %s\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nkiblnd_connreq_done(conn, 0);\r\nbreak;\r\ncase IBLND_CONN_ACTIVE_CONNECT:\r\nCDEBUG(D_NET, "ESTABLISHED(active): %s\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nkiblnd_check_connreply(conn,\r\n(void *)KIBLND_CONN_PARAM(event),\r\nKIBLND_CONN_PARAM_LEN(event));\r\nbreak;\r\n}\r\nreturn 0;\r\ncase RDMA_CM_EVENT_TIMEWAIT_EXIT:\r\nCDEBUG(D_NET, "Ignore TIMEWAIT_EXIT event\n");\r\nreturn 0;\r\ncase RDMA_CM_EVENT_DISCONNECTED:\r\nconn = (struct kib_conn *)cmid->context;\r\nif (conn->ibc_state < IBLND_CONN_ESTABLISHED) {\r\nCERROR("%s DISCONNECTED\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nkiblnd_connreq_done(conn, -ECONNRESET);\r\n} else {\r\nkiblnd_close_conn(conn, 0);\r\n}\r\nkiblnd_conn_decref(conn);\r\ncmid->context = NULL;\r\nreturn 0;\r\ncase RDMA_CM_EVENT_DEVICE_REMOVAL:\r\nLCONSOLE_ERROR_MSG(0x131,\r\n"Received notification of device removal\n"\r\n"Please shutdown LNET to allow this to proceed\n");\r\nreturn 0;\r\ncase RDMA_CM_EVENT_ADDR_CHANGE:\r\nLCONSOLE_INFO("Physical link changed (eg hca/port)\n");\r\nreturn 0;\r\n}\r\n}\r\nstatic int\r\nkiblnd_check_txs_locked(struct kib_conn *conn, struct list_head *txs)\r\n{\r\nstruct kib_tx *tx;\r\nstruct list_head *ttmp;\r\nlist_for_each(ttmp, txs) {\r\ntx = list_entry(ttmp, struct kib_tx, tx_list);\r\nif (txs != &conn->ibc_active_txs) {\r\nLASSERT(tx->tx_queued);\r\n} else {\r\nLASSERT(!tx->tx_queued);\r\nLASSERT(tx->tx_waiting || tx->tx_sending);\r\n}\r\nif (cfs_time_aftereq(jiffies, tx->tx_deadline)) {\r\nCERROR("Timed out tx: %s, %lu seconds\n",\r\nkiblnd_queue2str(conn, txs),\r\ncfs_duration_sec(jiffies - tx->tx_deadline));\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nkiblnd_conn_timed_out_locked(struct kib_conn *conn)\r\n{\r\nreturn kiblnd_check_txs_locked(conn, &conn->ibc_tx_queue) ||\r\nkiblnd_check_txs_locked(conn, &conn->ibc_tx_noops) ||\r\nkiblnd_check_txs_locked(conn, &conn->ibc_tx_queue_rsrvd) ||\r\nkiblnd_check_txs_locked(conn, &conn->ibc_tx_queue_nocred) ||\r\nkiblnd_check_txs_locked(conn, &conn->ibc_active_txs);\r\n}\r\nstatic void\r\nkiblnd_check_conns(int idx)\r\n{\r\nLIST_HEAD(closes);\r\nLIST_HEAD(checksends);\r\nstruct list_head *peers = &kiblnd_data.kib_peers[idx];\r\nstruct list_head *ptmp;\r\nstruct kib_peer *peer;\r\nstruct kib_conn *conn;\r\nstruct kib_conn *temp;\r\nstruct kib_conn *tmp;\r\nstruct list_head *ctmp;\r\nunsigned long flags;\r\nread_lock_irqsave(&kiblnd_data.kib_global_lock, flags);\r\nlist_for_each(ptmp, peers) {\r\npeer = list_entry(ptmp, struct kib_peer, ibp_list);\r\nlist_for_each(ctmp, &peer->ibp_conns) {\r\nint timedout;\r\nint sendnoop;\r\nconn = list_entry(ctmp, struct kib_conn, ibc_list);\r\nLASSERT(conn->ibc_state == IBLND_CONN_ESTABLISHED);\r\nspin_lock(&conn->ibc_lock);\r\nsendnoop = kiblnd_need_noop(conn);\r\ntimedout = kiblnd_conn_timed_out_locked(conn);\r\nif (!sendnoop && !timedout) {\r\nspin_unlock(&conn->ibc_lock);\r\ncontinue;\r\n}\r\nif (timedout) {\r\nCERROR("Timed out RDMA with %s (%lu): c: %u, oc: %u, rc: %u\n",\r\nlibcfs_nid2str(peer->ibp_nid),\r\ncfs_duration_sec(cfs_time_current() -\r\npeer->ibp_last_alive),\r\nconn->ibc_credits,\r\nconn->ibc_outstanding_credits,\r\nconn->ibc_reserved_credits);\r\nlist_add(&conn->ibc_connd_list, &closes);\r\n} else {\r\nlist_add(&conn->ibc_connd_list, &checksends);\r\n}\r\nkiblnd_conn_addref(conn);\r\nspin_unlock(&conn->ibc_lock);\r\n}\r\n}\r\nread_unlock_irqrestore(&kiblnd_data.kib_global_lock, flags);\r\nlist_for_each_entry_safe(conn, tmp, &closes, ibc_connd_list) {\r\nlist_del(&conn->ibc_connd_list);\r\nkiblnd_close_conn(conn, -ETIMEDOUT);\r\nkiblnd_conn_decref(conn);\r\n}\r\nlist_for_each_entry_safe(conn, temp, &checksends, ibc_connd_list) {\r\nlist_del(&conn->ibc_connd_list);\r\nspin_lock(&conn->ibc_lock);\r\nkiblnd_check_sends_locked(conn);\r\nspin_unlock(&conn->ibc_lock);\r\nkiblnd_conn_decref(conn);\r\n}\r\n}\r\nstatic void\r\nkiblnd_disconnect_conn(struct kib_conn *conn)\r\n{\r\nLASSERT(!in_interrupt());\r\nLASSERT(current == kiblnd_data.kib_connd);\r\nLASSERT(conn->ibc_state == IBLND_CONN_CLOSING);\r\nrdma_disconnect(conn->ibc_cmid);\r\nkiblnd_finalise_conn(conn);\r\nkiblnd_peer_notify(conn->ibc_peer);\r\n}\r\nint\r\nkiblnd_connd(void *arg)\r\n{\r\nspinlock_t *lock = &kiblnd_data.kib_connd_lock;\r\nwait_queue_entry_t wait;\r\nunsigned long flags;\r\nstruct kib_conn *conn;\r\nint timeout;\r\nint i;\r\nint dropped_lock;\r\nint peer_index = 0;\r\nunsigned long deadline = jiffies;\r\ncfs_block_allsigs();\r\ninit_waitqueue_entry(&wait, current);\r\nkiblnd_data.kib_connd = current;\r\nspin_lock_irqsave(lock, flags);\r\nwhile (!kiblnd_data.kib_shutdown) {\r\nint reconn = 0;\r\ndropped_lock = 0;\r\nif (!list_empty(&kiblnd_data.kib_connd_zombies)) {\r\nstruct kib_peer *peer = NULL;\r\nconn = list_entry(kiblnd_data.kib_connd_zombies.next,\r\nstruct kib_conn, ibc_list);\r\nlist_del(&conn->ibc_list);\r\nif (conn->ibc_reconnect) {\r\npeer = conn->ibc_peer;\r\nkiblnd_peer_addref(peer);\r\n}\r\nspin_unlock_irqrestore(lock, flags);\r\ndropped_lock = 1;\r\nkiblnd_destroy_conn(conn, !peer);\r\nspin_lock_irqsave(lock, flags);\r\nif (!peer)\r\ncontinue;\r\nconn->ibc_peer = peer;\r\nif (peer->ibp_reconnected < KIB_RECONN_HIGH_RACE)\r\nlist_add_tail(&conn->ibc_list,\r\n&kiblnd_data.kib_reconn_list);\r\nelse\r\nlist_add_tail(&conn->ibc_list,\r\n&kiblnd_data.kib_reconn_wait);\r\n}\r\nif (!list_empty(&kiblnd_data.kib_connd_conns)) {\r\nconn = list_entry(kiblnd_data.kib_connd_conns.next,\r\nstruct kib_conn, ibc_list);\r\nlist_del(&conn->ibc_list);\r\nspin_unlock_irqrestore(lock, flags);\r\ndropped_lock = 1;\r\nkiblnd_disconnect_conn(conn);\r\nkiblnd_conn_decref(conn);\r\nspin_lock_irqsave(lock, flags);\r\n}\r\nwhile (reconn < KIB_RECONN_BREAK) {\r\nif (kiblnd_data.kib_reconn_sec !=\r\nktime_get_real_seconds()) {\r\nkiblnd_data.kib_reconn_sec = ktime_get_real_seconds();\r\nlist_splice_init(&kiblnd_data.kib_reconn_wait,\r\n&kiblnd_data.kib_reconn_list);\r\n}\r\nif (list_empty(&kiblnd_data.kib_reconn_list))\r\nbreak;\r\nconn = list_entry(kiblnd_data.kib_reconn_list.next,\r\nstruct kib_conn, ibc_list);\r\nlist_del(&conn->ibc_list);\r\nspin_unlock_irqrestore(lock, flags);\r\ndropped_lock = 1;\r\nreconn += kiblnd_reconnect_peer(conn->ibc_peer);\r\nkiblnd_peer_decref(conn->ibc_peer);\r\nLIBCFS_FREE(conn, sizeof(*conn));\r\nspin_lock_irqsave(lock, flags);\r\n}\r\ntimeout = (int)(deadline - jiffies);\r\nif (timeout <= 0) {\r\nconst int n = 4;\r\nconst int p = 1;\r\nint chunk = kiblnd_data.kib_peer_hash_size;\r\nspin_unlock_irqrestore(lock, flags);\r\ndropped_lock = 1;\r\nif (*kiblnd_tunables.kib_timeout > n * p)\r\nchunk = (chunk * n * p) /\r\n*kiblnd_tunables.kib_timeout;\r\nif (!chunk)\r\nchunk = 1;\r\nfor (i = 0; i < chunk; i++) {\r\nkiblnd_check_conns(peer_index);\r\npeer_index = (peer_index + 1) %\r\nkiblnd_data.kib_peer_hash_size;\r\n}\r\ndeadline += msecs_to_jiffies(p * MSEC_PER_SEC);\r\nspin_lock_irqsave(lock, flags);\r\n}\r\nif (dropped_lock)\r\ncontinue;\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue(&kiblnd_data.kib_connd_waitq, &wait);\r\nspin_unlock_irqrestore(lock, flags);\r\nschedule_timeout(timeout);\r\nremove_wait_queue(&kiblnd_data.kib_connd_waitq, &wait);\r\nspin_lock_irqsave(lock, flags);\r\n}\r\nspin_unlock_irqrestore(lock, flags);\r\nkiblnd_thread_fini();\r\nreturn 0;\r\n}\r\nvoid\r\nkiblnd_qp_event(struct ib_event *event, void *arg)\r\n{\r\nstruct kib_conn *conn = arg;\r\nswitch (event->event) {\r\ncase IB_EVENT_COMM_EST:\r\nCDEBUG(D_NET, "%s established\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid));\r\nrdma_notify(conn->ibc_cmid, IB_EVENT_COMM_EST);\r\nreturn;\r\ndefault:\r\nCERROR("%s: Async QP event type %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), event->event);\r\nreturn;\r\n}\r\n}\r\nstatic void\r\nkiblnd_complete(struct ib_wc *wc)\r\n{\r\nswitch (kiblnd_wreqid2type(wc->wr_id)) {\r\ndefault:\r\nLBUG();\r\ncase IBLND_WID_MR:\r\nif (wc->status != IB_WC_SUCCESS &&\r\nwc->status != IB_WC_WR_FLUSH_ERR)\r\nCNETERR("FastReg failed: %d\n", wc->status);\r\nbreak;\r\ncase IBLND_WID_RDMA:\r\nCNETERR("RDMA (tx: %p) failed: %d\n",\r\nkiblnd_wreqid2ptr(wc->wr_id), wc->status);\r\nreturn;\r\ncase IBLND_WID_TX:\r\nkiblnd_tx_complete(kiblnd_wreqid2ptr(wc->wr_id), wc->status);\r\nreturn;\r\ncase IBLND_WID_RX:\r\nkiblnd_rx_complete(kiblnd_wreqid2ptr(wc->wr_id), wc->status,\r\nwc->byte_len);\r\nreturn;\r\n}\r\n}\r\nvoid\r\nkiblnd_cq_completion(struct ib_cq *cq, void *arg)\r\n{\r\nstruct kib_conn *conn = arg;\r\nstruct kib_sched_info *sched = conn->ibc_sched;\r\nunsigned long flags;\r\nLASSERT(cq == conn->ibc_cq);\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\nconn->ibc_ready = 1;\r\nif (!conn->ibc_scheduled &&\r\n(conn->ibc_nrx > 0 ||\r\nconn->ibc_nsends_posted > 0)) {\r\nkiblnd_conn_addref(conn);\r\nconn->ibc_scheduled = 1;\r\nlist_add_tail(&conn->ibc_sched_list, &sched->ibs_conns);\r\nif (waitqueue_active(&sched->ibs_waitq))\r\nwake_up(&sched->ibs_waitq);\r\n}\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\n}\r\nvoid\r\nkiblnd_cq_event(struct ib_event *event, void *arg)\r\n{\r\nstruct kib_conn *conn = arg;\r\nCERROR("%s: async CQ event type %d\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), event->event);\r\n}\r\nint\r\nkiblnd_scheduler(void *arg)\r\n{\r\nlong id = (long)arg;\r\nstruct kib_sched_info *sched;\r\nstruct kib_conn *conn;\r\nwait_queue_entry_t wait;\r\nunsigned long flags;\r\nstruct ib_wc wc;\r\nint did_something;\r\nint busy_loops = 0;\r\nint rc;\r\ncfs_block_allsigs();\r\ninit_waitqueue_entry(&wait, current);\r\nsched = kiblnd_data.kib_scheds[KIB_THREAD_CPT(id)];\r\nrc = cfs_cpt_bind(lnet_cpt_table(), sched->ibs_cpt);\r\nif (rc) {\r\nCWARN("Unable to bind on CPU partition %d, please verify whether all CPUs are healthy and reload modules if necessary, otherwise your system might under risk of low performance\n",\r\nsched->ibs_cpt);\r\n}\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\nwhile (!kiblnd_data.kib_shutdown) {\r\nif (busy_loops++ >= IBLND_RESCHED) {\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\ncond_resched();\r\nbusy_loops = 0;\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\n}\r\ndid_something = 0;\r\nif (!list_empty(&sched->ibs_conns)) {\r\nconn = list_entry(sched->ibs_conns.next, struct kib_conn,\r\nibc_sched_list);\r\nLASSERT(conn->ibc_scheduled);\r\nlist_del(&conn->ibc_sched_list);\r\nconn->ibc_ready = 0;\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\nwc.wr_id = IBLND_WID_INVAL;\r\nrc = ib_poll_cq(conn->ibc_cq, 1, &wc);\r\nif (!rc) {\r\nrc = ib_req_notify_cq(conn->ibc_cq,\r\nIB_CQ_NEXT_COMP);\r\nif (rc < 0) {\r\nCWARN("%s: ib_req_notify_cq failed: %d, closing connection\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid), rc);\r\nkiblnd_close_conn(conn, -EIO);\r\nkiblnd_conn_decref(conn);\r\nspin_lock_irqsave(&sched->ibs_lock,\r\nflags);\r\ncontinue;\r\n}\r\nrc = ib_poll_cq(conn->ibc_cq, 1, &wc);\r\n}\r\nif (unlikely(rc > 0 && wc.wr_id == IBLND_WID_INVAL)) {\r\nLCONSOLE_ERROR("ib_poll_cq (rc: %d) returned invalid wr_id, opcode %d, status: %d, vendor_err: %d, conn: %s status: %d\nplease upgrade firmware and OFED or contact vendor.\n",\r\nrc, wc.opcode, wc.status,\r\nwc.vendor_err,\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid),\r\nconn->ibc_state);\r\nrc = -EINVAL;\r\n}\r\nif (rc < 0) {\r\nCWARN("%s: ib_poll_cq failed: %d, closing connection\n",\r\nlibcfs_nid2str(conn->ibc_peer->ibp_nid),\r\nrc);\r\nkiblnd_close_conn(conn, -EIO);\r\nkiblnd_conn_decref(conn);\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\ncontinue;\r\n}\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\nif (rc || conn->ibc_ready) {\r\nkiblnd_conn_addref(conn);\r\nlist_add_tail(&conn->ibc_sched_list,\r\n&sched->ibs_conns);\r\nif (waitqueue_active(&sched->ibs_waitq))\r\nwake_up(&sched->ibs_waitq);\r\n} else {\r\nconn->ibc_scheduled = 0;\r\n}\r\nif (rc) {\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\nkiblnd_complete(&wc);\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\n}\r\nkiblnd_conn_decref(conn);\r\ndid_something = 1;\r\n}\r\nif (did_something)\r\ncontinue;\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue_exclusive(&sched->ibs_waitq, &wait);\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\nschedule();\r\nbusy_loops = 0;\r\nremove_wait_queue(&sched->ibs_waitq, &wait);\r\nspin_lock_irqsave(&sched->ibs_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&sched->ibs_lock, flags);\r\nkiblnd_thread_fini();\r\nreturn 0;\r\n}\r\nint\r\nkiblnd_failover_thread(void *arg)\r\n{\r\nrwlock_t *glock = &kiblnd_data.kib_global_lock;\r\nstruct kib_dev *dev;\r\nwait_queue_entry_t wait;\r\nunsigned long flags;\r\nint rc;\r\nLASSERT(*kiblnd_tunables.kib_dev_failover);\r\ncfs_block_allsigs();\r\ninit_waitqueue_entry(&wait, current);\r\nwrite_lock_irqsave(glock, flags);\r\nwhile (!kiblnd_data.kib_shutdown) {\r\nint do_failover = 0;\r\nint long_sleep;\r\nlist_for_each_entry(dev, &kiblnd_data.kib_failed_devs,\r\nibd_fail_list) {\r\nif (time_before(cfs_time_current(),\r\ndev->ibd_next_failover))\r\ncontinue;\r\ndo_failover = 1;\r\nbreak;\r\n}\r\nif (do_failover) {\r\nlist_del_init(&dev->ibd_fail_list);\r\ndev->ibd_failover = 1;\r\nwrite_unlock_irqrestore(glock, flags);\r\nrc = kiblnd_dev_failover(dev);\r\nwrite_lock_irqsave(glock, flags);\r\nLASSERT(dev->ibd_failover);\r\ndev->ibd_failover = 0;\r\nif (rc >= 0) {\r\ndev->ibd_next_failover = cfs_time_shift(3);\r\ncontinue;\r\n}\r\ndev->ibd_next_failover =\r\ncfs_time_shift(min(dev->ibd_failed_failover, 10));\r\nif (kiblnd_dev_can_failover(dev)) {\r\nlist_add_tail(&dev->ibd_fail_list,\r\n&kiblnd_data.kib_failed_devs);\r\n}\r\ncontinue;\r\n}\r\nlong_sleep = list_empty(&kiblnd_data.kib_failed_devs);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue(&kiblnd_data.kib_failover_waitq, &wait);\r\nwrite_unlock_irqrestore(glock, flags);\r\nrc = schedule_timeout(long_sleep ? cfs_time_seconds(10) :\r\ncfs_time_seconds(1));\r\nremove_wait_queue(&kiblnd_data.kib_failover_waitq, &wait);\r\nwrite_lock_irqsave(glock, flags);\r\nif (!long_sleep || rc)\r\ncontinue;\r\nlist_for_each_entry(dev, &kiblnd_data.kib_devs, ibd_list) {\r\nif (kiblnd_dev_can_failover(dev)) {\r\nlist_add_tail(&dev->ibd_fail_list,\r\n&kiblnd_data.kib_failed_devs);\r\n}\r\n}\r\n}\r\nwrite_unlock_irqrestore(glock, flags);\r\nkiblnd_thread_fini();\r\nreturn 0;\r\n}
