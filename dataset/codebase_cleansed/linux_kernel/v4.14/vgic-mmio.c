unsigned long vgic_mmio_read_raz(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nreturn 0;\r\n}\r\nunsigned long vgic_mmio_read_rao(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nreturn -1UL;\r\n}\r\nvoid vgic_mmio_write_wi(struct kvm_vcpu *vcpu, gpa_t addr,\r\nunsigned int len, unsigned long val)\r\n{\r\n}\r\nunsigned long vgic_mmio_read_enable(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nu32 value = 0;\r\nint i;\r\nfor (i = 0; i < len * 8; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nif (irq->enabled)\r\nvalue |= (1U << i);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn value;\r\n}\r\nvoid vgic_mmio_write_senable(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nint i;\r\nfor_each_set_bit(i, &val, len * 8) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nirq->enabled = true;\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nvoid vgic_mmio_write_cenable(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nint i;\r\nfor_each_set_bit(i, &val, len * 8) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nirq->enabled = false;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nunsigned long vgic_mmio_read_pending(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nu32 value = 0;\r\nint i;\r\nfor (i = 0; i < len * 8; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nif (irq_is_pending(irq))\r\nvalue |= (1U << i);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn value;\r\n}\r\nvoid vgic_mmio_write_spending(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nint i;\r\nfor_each_set_bit(i, &val, len * 8) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nirq->pending_latch = true;\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nvoid vgic_mmio_write_cpending(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nint i;\r\nfor_each_set_bit(i, &val, len * 8) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nirq->pending_latch = false;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nunsigned long vgic_mmio_read_active(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nu32 value = 0;\r\nint i;\r\nfor (i = 0; i < len * 8; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nif (irq->active)\r\nvalue |= (1U << i);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn value;\r\n}\r\nstatic void vgic_mmio_change_active(struct kvm_vcpu *vcpu, struct vgic_irq *irq,\r\nbool new_active_state)\r\n{\r\nstruct kvm_vcpu *requester_vcpu;\r\nspin_lock(&irq->irq_lock);\r\nrequester_vcpu = kvm_arm_get_running_vcpu();\r\nwhile (irq->vcpu &&\r\nirq->vcpu != requester_vcpu &&\r\nirq->vcpu->cpu != -1)\r\ncond_resched_lock(&irq->irq_lock);\r\nirq->active = new_active_state;\r\nif (new_active_state)\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\nelse\r\nspin_unlock(&irq->irq_lock);\r\n}\r\nstatic void vgic_change_active_prepare(struct kvm_vcpu *vcpu, u32 intid)\r\n{\r\nif (intid > VGIC_NR_PRIVATE_IRQS)\r\nkvm_arm_halt_guest(vcpu->kvm);\r\n}\r\nstatic void vgic_change_active_finish(struct kvm_vcpu *vcpu, u32 intid)\r\n{\r\nif (intid > VGIC_NR_PRIVATE_IRQS)\r\nkvm_arm_resume_guest(vcpu->kvm);\r\n}\r\nstatic void __vgic_mmio_write_cactive(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nint i;\r\nfor_each_set_bit(i, &val, len * 8) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nvgic_mmio_change_active(vcpu, irq, false);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nvoid vgic_mmio_write_cactive(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nmutex_lock(&vcpu->kvm->lock);\r\nvgic_change_active_prepare(vcpu, intid);\r\n__vgic_mmio_write_cactive(vcpu, addr, len, val);\r\nvgic_change_active_finish(vcpu, intid);\r\nmutex_unlock(&vcpu->kvm->lock);\r\n}\r\nvoid vgic_mmio_uaccess_write_cactive(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\n__vgic_mmio_write_cactive(vcpu, addr, len, val);\r\n}\r\nstatic void __vgic_mmio_write_sactive(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nint i;\r\nfor_each_set_bit(i, &val, len * 8) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nvgic_mmio_change_active(vcpu, irq, true);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nvoid vgic_mmio_write_sactive(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 1);\r\nmutex_lock(&vcpu->kvm->lock);\r\nvgic_change_active_prepare(vcpu, intid);\r\n__vgic_mmio_write_sactive(vcpu, addr, len, val);\r\nvgic_change_active_finish(vcpu, intid);\r\nmutex_unlock(&vcpu->kvm->lock);\r\n}\r\nvoid vgic_mmio_uaccess_write_sactive(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\n__vgic_mmio_write_sactive(vcpu, addr, len, val);\r\n}\r\nunsigned long vgic_mmio_read_priority(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 8);\r\nint i;\r\nu64 val = 0;\r\nfor (i = 0; i < len; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nval |= (u64)irq->priority << (i * 8);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn val;\r\n}\r\nvoid vgic_mmio_write_priority(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 8);\r\nint i;\r\nfor (i = 0; i < len; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nirq->priority = (val >> (i * 8)) & GENMASK(7, 8 - VGIC_PRI_BITS);\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nunsigned long vgic_mmio_read_config(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 2);\r\nu32 value = 0;\r\nint i;\r\nfor (i = 0; i < len * 4; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nif (irq->config == VGIC_CONFIG_EDGE)\r\nvalue |= (2U << (i * 2));\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn value;\r\n}\r\nvoid vgic_mmio_write_config(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 2);\r\nint i;\r\nfor (i = 0; i < len * 4; i++) {\r\nstruct vgic_irq *irq;\r\nif (intid + i < VGIC_NR_PRIVATE_IRQS)\r\ncontinue;\r\nirq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nif (test_bit(i * 2 + 1, &val))\r\nirq->config = VGIC_CONFIG_EDGE;\r\nelse\r\nirq->config = VGIC_CONFIG_LEVEL;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nu64 vgic_read_irq_line_level_info(struct kvm_vcpu *vcpu, u32 intid)\r\n{\r\nint i;\r\nu64 val = 0;\r\nint nr_irqs = vcpu->kvm->arch.vgic.nr_spis + VGIC_NR_PRIVATE_IRQS;\r\nfor (i = 0; i < 32; i++) {\r\nstruct vgic_irq *irq;\r\nif ((intid + i) < VGIC_NR_SGIS || (intid + i) >= nr_irqs)\r\ncontinue;\r\nirq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nif (irq->config == VGIC_CONFIG_LEVEL && irq->line_level)\r\nval |= (1U << i);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn val;\r\n}\r\nvoid vgic_write_irq_line_level_info(struct kvm_vcpu *vcpu, u32 intid,\r\nconst u64 val)\r\n{\r\nint i;\r\nint nr_irqs = vcpu->kvm->arch.vgic.nr_spis + VGIC_NR_PRIVATE_IRQS;\r\nfor (i = 0; i < 32; i++) {\r\nstruct vgic_irq *irq;\r\nbool new_level;\r\nif ((intid + i) < VGIC_NR_SGIS || (intid + i) >= nr_irqs)\r\ncontinue;\r\nirq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nnew_level = !!(val & (1U << i));\r\nspin_lock(&irq->irq_lock);\r\nirq->line_level = new_level;\r\nif (new_level)\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\nelse\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nstatic int match_region(const void *key, const void *elt)\r\n{\r\nconst unsigned int offset = (unsigned long)key;\r\nconst struct vgic_register_region *region = elt;\r\nif (offset < region->reg_offset)\r\nreturn -1;\r\nif (offset >= region->reg_offset + region->len)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nconst struct vgic_register_region *\r\nvgic_find_mmio_region(const struct vgic_register_region *regions,\r\nint nr_regions, unsigned int offset)\r\n{\r\nreturn bsearch((void *)(uintptr_t)offset, regions, nr_regions,\r\nsizeof(regions[0]), match_region);\r\n}\r\nvoid vgic_set_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcr)\r\n{\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_set_vmcr(vcpu, vmcr);\r\nelse\r\nvgic_v3_set_vmcr(vcpu, vmcr);\r\n}\r\nvoid vgic_get_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcr)\r\n{\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_get_vmcr(vcpu, vmcr);\r\nelse\r\nvgic_v3_get_vmcr(vcpu, vmcr);\r\n}\r\nunsigned long vgic_data_mmio_bus_to_host(const void *val, unsigned int len)\r\n{\r\nunsigned long data = kvm_mmio_read_buf(val, len);\r\nswitch (len) {\r\ncase 1:\r\nreturn data;\r\ncase 2:\r\nreturn le16_to_cpu(data);\r\ncase 4:\r\nreturn le32_to_cpu(data);\r\ndefault:\r\nreturn le64_to_cpu(data);\r\n}\r\n}\r\nvoid vgic_data_host_to_mmio_bus(void *buf, unsigned int len,\r\nunsigned long data)\r\n{\r\nswitch (len) {\r\ncase 1:\r\nbreak;\r\ncase 2:\r\ndata = cpu_to_le16(data);\r\nbreak;\r\ncase 4:\r\ndata = cpu_to_le32(data);\r\nbreak;\r\ndefault:\r\ndata = cpu_to_le64(data);\r\n}\r\nkvm_mmio_write_buf(buf, len, data);\r\n}\r\nstatic\r\nstruct vgic_io_device *kvm_to_vgic_iodev(const struct kvm_io_device *dev)\r\n{\r\nreturn container_of(dev, struct vgic_io_device, dev);\r\n}\r\nstatic bool check_region(const struct kvm *kvm,\r\nconst struct vgic_register_region *region,\r\ngpa_t addr, int len)\r\n{\r\nint flags, nr_irqs = kvm->arch.vgic.nr_spis + VGIC_NR_PRIVATE_IRQS;\r\nswitch (len) {\r\ncase sizeof(u8):\r\nflags = VGIC_ACCESS_8bit;\r\nbreak;\r\ncase sizeof(u32):\r\nflags = VGIC_ACCESS_32bit;\r\nbreak;\r\ncase sizeof(u64):\r\nflags = VGIC_ACCESS_64bit;\r\nbreak;\r\ndefault:\r\nreturn false;\r\n}\r\nif ((region->access_flags & flags) && IS_ALIGNED(addr, len)) {\r\nif (!region->bits_per_irq)\r\nreturn true;\r\nreturn VGIC_ADDR_TO_INTID(addr, region->bits_per_irq) < nr_irqs;\r\n}\r\nreturn false;\r\n}\r\nconst struct vgic_register_region *\r\nvgic_get_mmio_region(struct kvm_vcpu *vcpu, struct vgic_io_device *iodev,\r\ngpa_t addr, int len)\r\n{\r\nconst struct vgic_register_region *region;\r\nregion = vgic_find_mmio_region(iodev->regions, iodev->nr_regions,\r\naddr - iodev->base_addr);\r\nif (!region || !check_region(vcpu->kvm, region, addr, len))\r\nreturn NULL;\r\nreturn region;\r\n}\r\nstatic int vgic_uaccess_read(struct kvm_vcpu *vcpu, struct kvm_io_device *dev,\r\ngpa_t addr, u32 *val)\r\n{\r\nstruct vgic_io_device *iodev = kvm_to_vgic_iodev(dev);\r\nconst struct vgic_register_region *region;\r\nstruct kvm_vcpu *r_vcpu;\r\nregion = vgic_get_mmio_region(vcpu, iodev, addr, sizeof(u32));\r\nif (!region) {\r\n*val = 0;\r\nreturn 0;\r\n}\r\nr_vcpu = iodev->redist_vcpu ? iodev->redist_vcpu : vcpu;\r\nif (region->uaccess_read)\r\n*val = region->uaccess_read(r_vcpu, addr, sizeof(u32));\r\nelse\r\n*val = region->read(r_vcpu, addr, sizeof(u32));\r\nreturn 0;\r\n}\r\nstatic int vgic_uaccess_write(struct kvm_vcpu *vcpu, struct kvm_io_device *dev,\r\ngpa_t addr, const u32 *val)\r\n{\r\nstruct vgic_io_device *iodev = kvm_to_vgic_iodev(dev);\r\nconst struct vgic_register_region *region;\r\nstruct kvm_vcpu *r_vcpu;\r\nregion = vgic_get_mmio_region(vcpu, iodev, addr, sizeof(u32));\r\nif (!region)\r\nreturn 0;\r\nr_vcpu = iodev->redist_vcpu ? iodev->redist_vcpu : vcpu;\r\nif (region->uaccess_write)\r\nregion->uaccess_write(r_vcpu, addr, sizeof(u32), *val);\r\nelse\r\nregion->write(r_vcpu, addr, sizeof(u32), *val);\r\nreturn 0;\r\n}\r\nint vgic_uaccess(struct kvm_vcpu *vcpu, struct vgic_io_device *dev,\r\nbool is_write, int offset, u32 *val)\r\n{\r\nif (is_write)\r\nreturn vgic_uaccess_write(vcpu, &dev->dev, offset, val);\r\nelse\r\nreturn vgic_uaccess_read(vcpu, &dev->dev, offset, val);\r\n}\r\nstatic int dispatch_mmio_read(struct kvm_vcpu *vcpu, struct kvm_io_device *dev,\r\ngpa_t addr, int len, void *val)\r\n{\r\nstruct vgic_io_device *iodev = kvm_to_vgic_iodev(dev);\r\nconst struct vgic_register_region *region;\r\nunsigned long data = 0;\r\nregion = vgic_get_mmio_region(vcpu, iodev, addr, len);\r\nif (!region) {\r\nmemset(val, 0, len);\r\nreturn 0;\r\n}\r\nswitch (iodev->iodev_type) {\r\ncase IODEV_CPUIF:\r\ndata = region->read(vcpu, addr, len);\r\nbreak;\r\ncase IODEV_DIST:\r\ndata = region->read(vcpu, addr, len);\r\nbreak;\r\ncase IODEV_REDIST:\r\ndata = region->read(iodev->redist_vcpu, addr, len);\r\nbreak;\r\ncase IODEV_ITS:\r\ndata = region->its_read(vcpu->kvm, iodev->its, addr, len);\r\nbreak;\r\n}\r\nvgic_data_host_to_mmio_bus(val, len, data);\r\nreturn 0;\r\n}\r\nstatic int dispatch_mmio_write(struct kvm_vcpu *vcpu, struct kvm_io_device *dev,\r\ngpa_t addr, int len, const void *val)\r\n{\r\nstruct vgic_io_device *iodev = kvm_to_vgic_iodev(dev);\r\nconst struct vgic_register_region *region;\r\nunsigned long data = vgic_data_mmio_bus_to_host(val, len);\r\nregion = vgic_get_mmio_region(vcpu, iodev, addr, len);\r\nif (!region)\r\nreturn 0;\r\nswitch (iodev->iodev_type) {\r\ncase IODEV_CPUIF:\r\nregion->write(vcpu, addr, len, data);\r\nbreak;\r\ncase IODEV_DIST:\r\nregion->write(vcpu, addr, len, data);\r\nbreak;\r\ncase IODEV_REDIST:\r\nregion->write(iodev->redist_vcpu, addr, len, data);\r\nbreak;\r\ncase IODEV_ITS:\r\nregion->its_write(vcpu->kvm, iodev->its, addr, len, data);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nint vgic_register_dist_iodev(struct kvm *kvm, gpa_t dist_base_address,\r\nenum vgic_type type)\r\n{\r\nstruct vgic_io_device *io_device = &kvm->arch.vgic.dist_iodev;\r\nint ret = 0;\r\nunsigned int len;\r\nswitch (type) {\r\ncase VGIC_V2:\r\nlen = vgic_v2_init_dist_iodev(io_device);\r\nbreak;\r\ncase VGIC_V3:\r\nlen = vgic_v3_init_dist_iodev(io_device);\r\nbreak;\r\ndefault:\r\nBUG_ON(1);\r\n}\r\nio_device->base_addr = dist_base_address;\r\nio_device->iodev_type = IODEV_DIST;\r\nio_device->redist_vcpu = NULL;\r\nmutex_lock(&kvm->slots_lock);\r\nret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, dist_base_address,\r\nlen, &io_device->dev);\r\nmutex_unlock(&kvm->slots_lock);\r\nreturn ret;\r\n}
