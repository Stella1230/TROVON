static struct vgic_irq *vgic_get_lpi(struct kvm *kvm, u32 intid)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct vgic_irq *irq = NULL;\r\nspin_lock(&dist->lpi_list_lock);\r\nlist_for_each_entry(irq, &dist->lpi_list_head, lpi_list) {\r\nif (irq->intid != intid)\r\ncontinue;\r\nvgic_get_irq_kref(irq);\r\ngoto out_unlock;\r\n}\r\nirq = NULL;\r\nout_unlock:\r\nspin_unlock(&dist->lpi_list_lock);\r\nreturn irq;\r\n}\r\nstruct vgic_irq *vgic_get_irq(struct kvm *kvm, struct kvm_vcpu *vcpu,\r\nu32 intid)\r\n{\r\nif (intid <= VGIC_MAX_PRIVATE)\r\nreturn &vcpu->arch.vgic_cpu.private_irqs[intid];\r\nif (intid <= VGIC_MAX_SPI)\r\nreturn &kvm->arch.vgic.spis[intid - VGIC_NR_PRIVATE_IRQS];\r\nif (intid >= VGIC_MIN_LPI)\r\nreturn vgic_get_lpi(kvm, intid);\r\nWARN(1, "Looking up struct vgic_irq for reserved INTID");\r\nreturn NULL;\r\n}\r\nstatic void vgic_irq_release(struct kref *ref)\r\n{\r\n}\r\nvoid vgic_put_irq(struct kvm *kvm, struct vgic_irq *irq)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nif (irq->intid < VGIC_MIN_LPI)\r\nreturn;\r\nspin_lock(&dist->lpi_list_lock);\r\nif (!kref_put(&irq->refcount, vgic_irq_release)) {\r\nspin_unlock(&dist->lpi_list_lock);\r\nreturn;\r\n};\r\nlist_del(&irq->lpi_list);\r\ndist->lpi_list_count--;\r\nspin_unlock(&dist->lpi_list_lock);\r\nkfree(irq);\r\n}\r\nstatic struct kvm_vcpu *vgic_target_oracle(struct vgic_irq *irq)\r\n{\r\nDEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&irq->irq_lock));\r\nif (irq->active)\r\nreturn irq->vcpu ? : irq->target_vcpu;\r\nif (irq->enabled && irq_is_pending(irq)) {\r\nif (unlikely(irq->target_vcpu &&\r\n!irq->target_vcpu->kvm->arch.vgic.enabled))\r\nreturn NULL;\r\nreturn irq->target_vcpu;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int vgic_irq_cmp(void *priv, struct list_head *a, struct list_head *b)\r\n{\r\nstruct vgic_irq *irqa = container_of(a, struct vgic_irq, ap_list);\r\nstruct vgic_irq *irqb = container_of(b, struct vgic_irq, ap_list);\r\nbool penda, pendb;\r\nint ret;\r\nspin_lock(&irqa->irq_lock);\r\nspin_lock_nested(&irqb->irq_lock, SINGLE_DEPTH_NESTING);\r\nif (irqa->active || irqb->active) {\r\nret = (int)irqb->active - (int)irqa->active;\r\ngoto out;\r\n}\r\npenda = irqa->enabled && irq_is_pending(irqa);\r\npendb = irqb->enabled && irq_is_pending(irqb);\r\nif (!penda || !pendb) {\r\nret = (int)pendb - (int)penda;\r\ngoto out;\r\n}\r\nret = irqa->priority - irqb->priority;\r\nout:\r\nspin_unlock(&irqb->irq_lock);\r\nspin_unlock(&irqa->irq_lock);\r\nreturn ret;\r\n}\r\nstatic void vgic_sort_ap_list(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nDEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&vgic_cpu->ap_list_lock));\r\nlist_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);\r\n}\r\nstatic bool vgic_validate_injection(struct vgic_irq *irq, bool level, void *owner)\r\n{\r\nif (irq->owner != owner)\r\nreturn false;\r\nswitch (irq->config) {\r\ncase VGIC_CONFIG_LEVEL:\r\nreturn irq->line_level != level;\r\ncase VGIC_CONFIG_EDGE:\r\nreturn level;\r\n}\r\nreturn false;\r\n}\r\nbool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nDEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&irq->irq_lock));\r\nretry:\r\nvcpu = vgic_target_oracle(irq);\r\nif (irq->vcpu || !vcpu) {\r\nspin_unlock(&irq->irq_lock);\r\nif (vcpu) {\r\nkvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);\r\nkvm_vcpu_kick(vcpu);\r\n}\r\nreturn false;\r\n}\r\nspin_unlock(&irq->irq_lock);\r\nspin_lock(&vcpu->arch.vgic_cpu.ap_list_lock);\r\nspin_lock(&irq->irq_lock);\r\nif (unlikely(irq->vcpu || vcpu != vgic_target_oracle(irq))) {\r\nspin_unlock(&irq->irq_lock);\r\nspin_unlock(&vcpu->arch.vgic_cpu.ap_list_lock);\r\nspin_lock(&irq->irq_lock);\r\ngoto retry;\r\n}\r\nvgic_get_irq_kref(irq);\r\nlist_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);\r\nirq->vcpu = vcpu;\r\nspin_unlock(&irq->irq_lock);\r\nspin_unlock(&vcpu->arch.vgic_cpu.ap_list_lock);\r\nkvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);\r\nkvm_vcpu_kick(vcpu);\r\nreturn true;\r\n}\r\nint kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,\r\nbool level, void *owner)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nstruct vgic_irq *irq;\r\nint ret;\r\ntrace_vgic_update_irq_pending(cpuid, intid, level);\r\nret = vgic_lazy_init(kvm);\r\nif (ret)\r\nreturn ret;\r\nvcpu = kvm_get_vcpu(kvm, cpuid);\r\nif (!vcpu && intid < VGIC_NR_PRIVATE_IRQS)\r\nreturn -EINVAL;\r\nirq = vgic_get_irq(kvm, vcpu, intid);\r\nif (!irq)\r\nreturn -EINVAL;\r\nspin_lock(&irq->irq_lock);\r\nif (!vgic_validate_injection(irq, level, owner)) {\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(kvm, irq);\r\nreturn 0;\r\n}\r\nif (irq->config == VGIC_CONFIG_LEVEL)\r\nirq->line_level = level;\r\nelse\r\nirq->pending_latch = true;\r\nvgic_queue_irq_unlock(kvm, irq);\r\nvgic_put_irq(kvm, irq);\r\nreturn 0;\r\n}\r\nint kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu, u32 virt_irq, u32 phys_irq)\r\n{\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, virt_irq);\r\nBUG_ON(!irq);\r\nspin_lock(&irq->irq_lock);\r\nirq->hw = true;\r\nirq->hwintid = phys_irq;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\nreturn 0;\r\n}\r\nint kvm_vgic_unmap_phys_irq(struct kvm_vcpu *vcpu, unsigned int virt_irq)\r\n{\r\nstruct vgic_irq *irq;\r\nif (!vgic_initialized(vcpu->kvm))\r\nreturn -EAGAIN;\r\nirq = vgic_get_irq(vcpu->kvm, vcpu, virt_irq);\r\nBUG_ON(!irq);\r\nspin_lock(&irq->irq_lock);\r\nirq->hw = false;\r\nirq->hwintid = 0;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\nreturn 0;\r\n}\r\nint kvm_vgic_set_owner(struct kvm_vcpu *vcpu, unsigned int intid, void *owner)\r\n{\r\nstruct vgic_irq *irq;\r\nint ret = 0;\r\nif (!vgic_initialized(vcpu->kvm))\r\nreturn -EAGAIN;\r\nif (!irq_is_ppi(intid) && !vgic_valid_spi(vcpu->kvm, intid))\r\nreturn -EINVAL;\r\nirq = vgic_get_irq(vcpu->kvm, vcpu, intid);\r\nspin_lock(&irq->irq_lock);\r\nif (irq->owner && irq->owner != owner)\r\nret = -EEXIST;\r\nelse\r\nirq->owner = owner;\r\nspin_unlock(&irq->irq_lock);\r\nreturn ret;\r\n}\r\nstatic void vgic_prune_ap_list(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_irq *irq, *tmp;\r\nretry:\r\nspin_lock(&vgic_cpu->ap_list_lock);\r\nlist_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {\r\nstruct kvm_vcpu *target_vcpu, *vcpuA, *vcpuB;\r\nspin_lock(&irq->irq_lock);\r\nBUG_ON(vcpu != irq->vcpu);\r\ntarget_vcpu = vgic_target_oracle(irq);\r\nif (!target_vcpu) {\r\nlist_del(&irq->ap_list);\r\nirq->vcpu = NULL;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\ncontinue;\r\n}\r\nif (target_vcpu == vcpu) {\r\nspin_unlock(&irq->irq_lock);\r\ncontinue;\r\n}\r\nspin_unlock(&irq->irq_lock);\r\nspin_unlock(&vgic_cpu->ap_list_lock);\r\nif (vcpu->vcpu_id < target_vcpu->vcpu_id) {\r\nvcpuA = vcpu;\r\nvcpuB = target_vcpu;\r\n} else {\r\nvcpuA = target_vcpu;\r\nvcpuB = vcpu;\r\n}\r\nspin_lock(&vcpuA->arch.vgic_cpu.ap_list_lock);\r\nspin_lock_nested(&vcpuB->arch.vgic_cpu.ap_list_lock,\r\nSINGLE_DEPTH_NESTING);\r\nspin_lock(&irq->irq_lock);\r\nif (target_vcpu == vgic_target_oracle(irq)) {\r\nstruct vgic_cpu *new_cpu = &target_vcpu->arch.vgic_cpu;\r\nlist_del(&irq->ap_list);\r\nirq->vcpu = target_vcpu;\r\nlist_add_tail(&irq->ap_list, &new_cpu->ap_list_head);\r\n}\r\nspin_unlock(&irq->irq_lock);\r\nspin_unlock(&vcpuB->arch.vgic_cpu.ap_list_lock);\r\nspin_unlock(&vcpuA->arch.vgic_cpu.ap_list_lock);\r\ngoto retry;\r\n}\r\nspin_unlock(&vgic_cpu->ap_list_lock);\r\n}\r\nstatic inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)\r\n{\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_fold_lr_state(vcpu);\r\nelse\r\nvgic_v3_fold_lr_state(vcpu);\r\n}\r\nstatic inline void vgic_populate_lr(struct kvm_vcpu *vcpu,\r\nstruct vgic_irq *irq, int lr)\r\n{\r\nDEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&irq->irq_lock));\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_populate_lr(vcpu, irq, lr);\r\nelse\r\nvgic_v3_populate_lr(vcpu, irq, lr);\r\n}\r\nstatic inline void vgic_clear_lr(struct kvm_vcpu *vcpu, int lr)\r\n{\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_clear_lr(vcpu, lr);\r\nelse\r\nvgic_v3_clear_lr(vcpu, lr);\r\n}\r\nstatic inline void vgic_set_underflow(struct kvm_vcpu *vcpu)\r\n{\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_set_underflow(vcpu);\r\nelse\r\nvgic_v3_set_underflow(vcpu);\r\n}\r\nstatic int compute_ap_list_depth(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_irq *irq;\r\nint count = 0;\r\nDEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&vgic_cpu->ap_list_lock));\r\nlist_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {\r\nspin_lock(&irq->irq_lock);\r\nif (vgic_irq_is_sgi(irq->intid) && irq->source)\r\ncount += hweight8(irq->source);\r\nelse\r\ncount++;\r\nspin_unlock(&irq->irq_lock);\r\n}\r\nreturn count;\r\n}\r\nstatic void vgic_flush_lr_state(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_irq *irq;\r\nint count = 0;\r\nDEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&vgic_cpu->ap_list_lock));\r\nif (compute_ap_list_depth(vcpu) > kvm_vgic_global_state.nr_lr)\r\nvgic_sort_ap_list(vcpu);\r\nlist_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {\r\nspin_lock(&irq->irq_lock);\r\nif (unlikely(vgic_target_oracle(irq) != vcpu))\r\ngoto next;\r\ndo {\r\nvgic_populate_lr(vcpu, irq, count++);\r\n} while (irq->source && count < kvm_vgic_global_state.nr_lr);\r\nnext:\r\nspin_unlock(&irq->irq_lock);\r\nif (count == kvm_vgic_global_state.nr_lr) {\r\nif (!list_is_last(&irq->ap_list,\r\n&vgic_cpu->ap_list_head))\r\nvgic_set_underflow(vcpu);\r\nbreak;\r\n}\r\n}\r\nvcpu->arch.vgic_cpu.used_lrs = count;\r\nfor ( ; count < kvm_vgic_global_state.nr_lr; count++)\r\nvgic_clear_lr(vcpu, count);\r\n}\r\nvoid kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nif (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))\r\nreturn;\r\nif (vgic_cpu->used_lrs)\r\nvgic_fold_lr_state(vcpu);\r\nvgic_prune_ap_list(vcpu);\r\n}\r\nvoid kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nif (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))\r\nreturn;\r\nspin_lock(&vcpu->arch.vgic_cpu.ap_list_lock);\r\nvgic_flush_lr_state(vcpu);\r\nspin_unlock(&vcpu->arch.vgic_cpu.ap_list_lock);\r\n}\r\nvoid kvm_vgic_load(struct kvm_vcpu *vcpu)\r\n{\r\nif (unlikely(!vgic_initialized(vcpu->kvm)))\r\nreturn;\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_load(vcpu);\r\nelse\r\nvgic_v3_load(vcpu);\r\n}\r\nvoid kvm_vgic_put(struct kvm_vcpu *vcpu)\r\n{\r\nif (unlikely(!vgic_initialized(vcpu->kvm)))\r\nreturn;\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_put(vcpu);\r\nelse\r\nvgic_v3_put(vcpu);\r\n}\r\nint kvm_vgic_vcpu_pending_irq(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_irq *irq;\r\nbool pending = false;\r\nif (!vcpu->kvm->arch.vgic.enabled)\r\nreturn false;\r\nspin_lock(&vgic_cpu->ap_list_lock);\r\nlist_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {\r\nspin_lock(&irq->irq_lock);\r\npending = irq_is_pending(irq) && irq->enabled;\r\nspin_unlock(&irq->irq_lock);\r\nif (pending)\r\nbreak;\r\n}\r\nspin_unlock(&vgic_cpu->ap_list_lock);\r\nreturn pending;\r\n}\r\nvoid vgic_kick_vcpus(struct kvm *kvm)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nint c;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nif (kvm_vgic_vcpu_pending_irq(vcpu)) {\r\nkvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);\r\nkvm_vcpu_kick(vcpu);\r\n}\r\n}\r\n}\r\nbool kvm_vgic_map_is_active(struct kvm_vcpu *vcpu, unsigned int virt_irq)\r\n{\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, virt_irq);\r\nbool map_is_active;\r\nspin_lock(&irq->irq_lock);\r\nmap_is_active = irq->hw && irq->active;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\nreturn map_is_active;\r\n}
