static long privcmd_ioctl_hypercall(struct file *file, void __user *udata)\r\n{\r\nstruct privcmd_data *data = file->private_data;\r\nstruct privcmd_hypercall hypercall;\r\nlong ret;\r\nif (data->domid != DOMID_INVALID)\r\nreturn -EPERM;\r\nif (copy_from_user(&hypercall, udata, sizeof(hypercall)))\r\nreturn -EFAULT;\r\nxen_preemptible_hcall_begin();\r\nret = privcmd_call(hypercall.op,\r\nhypercall.arg[0], hypercall.arg[1],\r\nhypercall.arg[2], hypercall.arg[3],\r\nhypercall.arg[4]);\r\nxen_preemptible_hcall_end();\r\nreturn ret;\r\n}\r\nstatic void free_page_list(struct list_head *pages)\r\n{\r\nstruct page *p, *n;\r\nlist_for_each_entry_safe(p, n, pages, lru)\r\n__free_page(p);\r\nINIT_LIST_HEAD(pages);\r\n}\r\nstatic int gather_array(struct list_head *pagelist,\r\nunsigned nelem, size_t size,\r\nconst void __user *data)\r\n{\r\nunsigned pageidx;\r\nvoid *pagedata;\r\nint ret;\r\nif (size > PAGE_SIZE)\r\nreturn 0;\r\npageidx = PAGE_SIZE;\r\npagedata = NULL;\r\nwhile (nelem--) {\r\nif (pageidx > PAGE_SIZE-size) {\r\nstruct page *page = alloc_page(GFP_KERNEL);\r\nret = -ENOMEM;\r\nif (page == NULL)\r\ngoto fail;\r\npagedata = page_address(page);\r\nlist_add_tail(&page->lru, pagelist);\r\npageidx = 0;\r\n}\r\nret = -EFAULT;\r\nif (copy_from_user(pagedata + pageidx, data, size))\r\ngoto fail;\r\ndata += size;\r\npageidx += size;\r\n}\r\nret = 0;\r\nfail:\r\nreturn ret;\r\n}\r\nstatic int traverse_pages(unsigned nelem, size_t size,\r\nstruct list_head *pos,\r\nint (*fn)(void *data, void *state),\r\nvoid *state)\r\n{\r\nvoid *pagedata;\r\nunsigned pageidx;\r\nint ret = 0;\r\nBUG_ON(size > PAGE_SIZE);\r\npageidx = PAGE_SIZE;\r\npagedata = NULL;\r\nwhile (nelem--) {\r\nif (pageidx > PAGE_SIZE-size) {\r\nstruct page *page;\r\npos = pos->next;\r\npage = list_entry(pos, struct page, lru);\r\npagedata = page_address(page);\r\npageidx = 0;\r\n}\r\nret = (*fn)(pagedata + pageidx, state);\r\nif (ret)\r\nbreak;\r\npageidx += size;\r\n}\r\nreturn ret;\r\n}\r\nstatic int traverse_pages_block(unsigned nelem, size_t size,\r\nstruct list_head *pos,\r\nint (*fn)(void *data, int nr, void *state),\r\nvoid *state)\r\n{\r\nvoid *pagedata;\r\nunsigned pageidx;\r\nint ret = 0;\r\nBUG_ON(size > PAGE_SIZE);\r\npageidx = PAGE_SIZE;\r\nwhile (nelem) {\r\nint nr = (PAGE_SIZE/size);\r\nstruct page *page;\r\nif (nr > nelem)\r\nnr = nelem;\r\npos = pos->next;\r\npage = list_entry(pos, struct page, lru);\r\npagedata = page_address(page);\r\nret = (*fn)(pagedata, nr, state);\r\nif (ret)\r\nbreak;\r\nnelem -= nr;\r\n}\r\nreturn ret;\r\n}\r\nstatic int mmap_gfn_range(void *data, void *state)\r\n{\r\nstruct privcmd_mmap_entry *msg = data;\r\nstruct mmap_gfn_state *st = state;\r\nstruct vm_area_struct *vma = st->vma;\r\nint rc;\r\nif ((msg->npages > (LONG_MAX >> PAGE_SHIFT)) ||\r\n((unsigned long)(msg->npages << PAGE_SHIFT) >= -st->va))\r\nreturn -EINVAL;\r\nif ((msg->va != st->va) ||\r\n((msg->va+(msg->npages<<PAGE_SHIFT)) > vma->vm_end))\r\nreturn -EINVAL;\r\nrc = xen_remap_domain_gfn_range(vma,\r\nmsg->va & PAGE_MASK,\r\nmsg->mfn, msg->npages,\r\nvma->vm_page_prot,\r\nst->domain, NULL);\r\nif (rc < 0)\r\nreturn rc;\r\nst->va += msg->npages << PAGE_SHIFT;\r\nreturn 0;\r\n}\r\nstatic long privcmd_ioctl_mmap(struct file *file, void __user *udata)\r\n{\r\nstruct privcmd_data *data = file->private_data;\r\nstruct privcmd_mmap mmapcmd;\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nint rc;\r\nLIST_HEAD(pagelist);\r\nstruct mmap_gfn_state state;\r\nif (xen_feature(XENFEAT_auto_translated_physmap))\r\nreturn -ENOSYS;\r\nif (copy_from_user(&mmapcmd, udata, sizeof(mmapcmd)))\r\nreturn -EFAULT;\r\nif (data->domid != DOMID_INVALID && data->domid != mmapcmd.dom)\r\nreturn -EPERM;\r\nrc = gather_array(&pagelist,\r\nmmapcmd.num, sizeof(struct privcmd_mmap_entry),\r\nmmapcmd.entry);\r\nif (rc || list_empty(&pagelist))\r\ngoto out;\r\ndown_write(&mm->mmap_sem);\r\n{\r\nstruct page *page = list_first_entry(&pagelist,\r\nstruct page, lru);\r\nstruct privcmd_mmap_entry *msg = page_address(page);\r\nvma = find_vma(mm, msg->va);\r\nrc = -EINVAL;\r\nif (!vma || (msg->va != vma->vm_start) || vma->vm_private_data)\r\ngoto out_up;\r\nvma->vm_private_data = PRIV_VMA_LOCKED;\r\n}\r\nstate.va = vma->vm_start;\r\nstate.vma = vma;\r\nstate.domain = mmapcmd.dom;\r\nrc = traverse_pages(mmapcmd.num, sizeof(struct privcmd_mmap_entry),\r\n&pagelist,\r\nmmap_gfn_range, &state);\r\nout_up:\r\nup_write(&mm->mmap_sem);\r\nout:\r\nfree_page_list(&pagelist);\r\nreturn rc;\r\n}\r\nstatic int mmap_batch_fn(void *data, int nr, void *state)\r\n{\r\nxen_pfn_t *gfnp = data;\r\nstruct mmap_batch_state *st = state;\r\nstruct vm_area_struct *vma = st->vma;\r\nstruct page **pages = vma->vm_private_data;\r\nstruct page **cur_pages = NULL;\r\nint ret;\r\nif (xen_feature(XENFEAT_auto_translated_physmap))\r\ncur_pages = &pages[st->index];\r\nBUG_ON(nr < 0);\r\nret = xen_remap_domain_gfn_array(st->vma, st->va & PAGE_MASK, gfnp, nr,\r\n(int *)gfnp, st->vma->vm_page_prot,\r\nst->domain, cur_pages);\r\nif (ret != nr) {\r\nif (ret == -ENOENT)\r\nst->global_error = -ENOENT;\r\nelse {\r\nif (st->global_error == 0)\r\nst->global_error = 1;\r\n}\r\n}\r\nst->va += XEN_PAGE_SIZE * nr;\r\nst->index += nr / XEN_PFN_PER_PAGE;\r\nreturn 0;\r\n}\r\nstatic int mmap_return_error(int err, struct mmap_batch_state *st)\r\n{\r\nint ret;\r\nif (st->version == 1) {\r\nif (err) {\r\nxen_pfn_t gfn;\r\nret = get_user(gfn, st->user_gfn);\r\nif (ret < 0)\r\nreturn ret;\r\ngfn |= (err == -ENOENT) ?\r\nPRIVCMD_MMAPBATCH_PAGED_ERROR :\r\nPRIVCMD_MMAPBATCH_MFN_ERROR;\r\nreturn __put_user(gfn, st->user_gfn++);\r\n} else\r\nst->user_gfn++;\r\n} else {\r\nif (err)\r\nreturn __put_user(err, st->user_err++);\r\nelse\r\nst->user_err++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mmap_return_errors(void *data, int nr, void *state)\r\n{\r\nstruct mmap_batch_state *st = state;\r\nint *errs = data;\r\nint i;\r\nint ret;\r\nfor (i = 0; i < nr; i++) {\r\nret = mmap_return_error(errs[i], st);\r\nif (ret < 0)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int alloc_empty_pages(struct vm_area_struct *vma, int numpgs)\r\n{\r\nint rc;\r\nstruct page **pages;\r\npages = kcalloc(numpgs, sizeof(pages[0]), GFP_KERNEL);\r\nif (pages == NULL)\r\nreturn -ENOMEM;\r\nrc = alloc_xenballooned_pages(numpgs, pages);\r\nif (rc != 0) {\r\npr_warn("%s Could not alloc %d pfns rc:%d\n", __func__,\r\nnumpgs, rc);\r\nkfree(pages);\r\nreturn -ENOMEM;\r\n}\r\nBUG_ON(vma->vm_private_data != NULL);\r\nvma->vm_private_data = pages;\r\nreturn 0;\r\n}\r\nstatic long privcmd_ioctl_mmap_batch(\r\nstruct file *file, void __user *udata, int version)\r\n{\r\nstruct privcmd_data *data = file->private_data;\r\nint ret;\r\nstruct privcmd_mmapbatch_v2 m;\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nunsigned long nr_pages;\r\nLIST_HEAD(pagelist);\r\nstruct mmap_batch_state state;\r\nswitch (version) {\r\ncase 1:\r\nif (copy_from_user(&m, udata, sizeof(struct privcmd_mmapbatch)))\r\nreturn -EFAULT;\r\nm.err = NULL;\r\nif (!access_ok(VERIFY_WRITE, m.arr, m.num * sizeof(*m.arr)))\r\nreturn -EFAULT;\r\nbreak;\r\ncase 2:\r\nif (copy_from_user(&m, udata, sizeof(struct privcmd_mmapbatch_v2)))\r\nreturn -EFAULT;\r\nif (!access_ok(VERIFY_WRITE, m.err, m.num * (sizeof(*m.err))))\r\nreturn -EFAULT;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (data->domid != DOMID_INVALID && data->domid != m.dom)\r\nreturn -EPERM;\r\nnr_pages = DIV_ROUND_UP(m.num, XEN_PFN_PER_PAGE);\r\nif ((m.num <= 0) || (nr_pages > (LONG_MAX >> PAGE_SHIFT)))\r\nreturn -EINVAL;\r\nret = gather_array(&pagelist, m.num, sizeof(xen_pfn_t), m.arr);\r\nif (ret)\r\ngoto out;\r\nif (list_empty(&pagelist)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (version == 2) {\r\nif (clear_user(m.err, sizeof(int) * m.num)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\n}\r\ndown_write(&mm->mmap_sem);\r\nvma = find_vma(mm, m.addr);\r\nif (!vma ||\r\nvma->vm_ops != &privcmd_vm_ops) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nif (vma->vm_private_data == NULL) {\r\nif (m.addr != vma->vm_start ||\r\nm.addr + (nr_pages << PAGE_SHIFT) != vma->vm_end) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nif (xen_feature(XENFEAT_auto_translated_physmap)) {\r\nret = alloc_empty_pages(vma, nr_pages);\r\nif (ret < 0)\r\ngoto out_unlock;\r\n} else\r\nvma->vm_private_data = PRIV_VMA_LOCKED;\r\n} else {\r\nif (m.addr < vma->vm_start ||\r\nm.addr + (nr_pages << PAGE_SHIFT) > vma->vm_end) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nif (privcmd_vma_range_is_mapped(vma, m.addr, nr_pages)) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\n}\r\nstate.domain = m.dom;\r\nstate.vma = vma;\r\nstate.va = m.addr;\r\nstate.index = 0;\r\nstate.global_error = 0;\r\nstate.version = version;\r\nBUILD_BUG_ON(((PAGE_SIZE / sizeof(xen_pfn_t)) % XEN_PFN_PER_PAGE) != 0);\r\nBUG_ON(traverse_pages_block(m.num, sizeof(xen_pfn_t),\r\n&pagelist, mmap_batch_fn, &state));\r\nup_write(&mm->mmap_sem);\r\nif (state.global_error) {\r\nstate.user_gfn = (xen_pfn_t *)m.arr;\r\nstate.user_err = m.err;\r\nret = traverse_pages_block(m.num, sizeof(xen_pfn_t),\r\n&pagelist, mmap_return_errors, &state);\r\n} else\r\nret = 0;\r\nif ((ret == 0) && (state.global_error == -ENOENT))\r\nret = -ENOENT;\r\nout:\r\nfree_page_list(&pagelist);\r\nreturn ret;\r\nout_unlock:\r\nup_write(&mm->mmap_sem);\r\ngoto out;\r\n}\r\nstatic int lock_pages(\r\nstruct privcmd_dm_op_buf kbufs[], unsigned int num,\r\nstruct page *pages[], unsigned int nr_pages)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < num; i++) {\r\nunsigned int requested;\r\nint pinned;\r\nrequested = DIV_ROUND_UP(\r\noffset_in_page(kbufs[i].uptr) + kbufs[i].size,\r\nPAGE_SIZE);\r\nif (requested > nr_pages)\r\nreturn -ENOSPC;\r\npinned = get_user_pages_fast(\r\n(unsigned long) kbufs[i].uptr,\r\nrequested, FOLL_WRITE, pages);\r\nif (pinned < 0)\r\nreturn pinned;\r\nnr_pages -= pinned;\r\npages += pinned;\r\n}\r\nreturn 0;\r\n}\r\nstatic void unlock_pages(struct page *pages[], unsigned int nr_pages)\r\n{\r\nunsigned int i;\r\nif (!pages)\r\nreturn;\r\nfor (i = 0; i < nr_pages; i++) {\r\nif (pages[i])\r\nput_page(pages[i]);\r\n}\r\n}\r\nstatic long privcmd_ioctl_dm_op(struct file *file, void __user *udata)\r\n{\r\nstruct privcmd_data *data = file->private_data;\r\nstruct privcmd_dm_op kdata;\r\nstruct privcmd_dm_op_buf *kbufs;\r\nunsigned int nr_pages = 0;\r\nstruct page **pages = NULL;\r\nstruct xen_dm_op_buf *xbufs = NULL;\r\nunsigned int i;\r\nlong rc;\r\nif (copy_from_user(&kdata, udata, sizeof(kdata)))\r\nreturn -EFAULT;\r\nif (data->domid != DOMID_INVALID && data->domid != kdata.dom)\r\nreturn -EPERM;\r\nif (kdata.num == 0)\r\nreturn 0;\r\nif (kdata.num > privcmd_dm_op_max_num)\r\nreturn -E2BIG;\r\nkbufs = kcalloc(kdata.num, sizeof(*kbufs), GFP_KERNEL);\r\nif (!kbufs)\r\nreturn -ENOMEM;\r\nif (copy_from_user(kbufs, kdata.ubufs,\r\nsizeof(*kbufs) * kdata.num)) {\r\nrc = -EFAULT;\r\ngoto out;\r\n}\r\nfor (i = 0; i < kdata.num; i++) {\r\nif (kbufs[i].size > privcmd_dm_op_buf_max_size) {\r\nrc = -E2BIG;\r\ngoto out;\r\n}\r\nif (!access_ok(VERIFY_WRITE, kbufs[i].uptr,\r\nkbufs[i].size)) {\r\nrc = -EFAULT;\r\ngoto out;\r\n}\r\nnr_pages += DIV_ROUND_UP(\r\noffset_in_page(kbufs[i].uptr) + kbufs[i].size,\r\nPAGE_SIZE);\r\n}\r\npages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\r\nif (!pages) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nxbufs = kcalloc(kdata.num, sizeof(*xbufs), GFP_KERNEL);\r\nif (!xbufs) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nrc = lock_pages(kbufs, kdata.num, pages, nr_pages);\r\nif (rc)\r\ngoto out;\r\nfor (i = 0; i < kdata.num; i++) {\r\nset_xen_guest_handle(xbufs[i].h, kbufs[i].uptr);\r\nxbufs[i].size = kbufs[i].size;\r\n}\r\nxen_preemptible_hcall_begin();\r\nrc = HYPERVISOR_dm_op(kdata.dom, kdata.num, xbufs);\r\nxen_preemptible_hcall_end();\r\nout:\r\nunlock_pages(pages, nr_pages);\r\nkfree(xbufs);\r\nkfree(pages);\r\nkfree(kbufs);\r\nreturn rc;\r\n}\r\nstatic long privcmd_ioctl_restrict(struct file *file, void __user *udata)\r\n{\r\nstruct privcmd_data *data = file->private_data;\r\ndomid_t dom;\r\nif (copy_from_user(&dom, udata, sizeof(dom)))\r\nreturn -EFAULT;\r\nif (data->domid == DOMID_INVALID)\r\ndata->domid = dom;\r\nelse if (data->domid != dom)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic long privcmd_ioctl(struct file *file,\r\nunsigned int cmd, unsigned long data)\r\n{\r\nint ret = -ENOTTY;\r\nvoid __user *udata = (void __user *) data;\r\nswitch (cmd) {\r\ncase IOCTL_PRIVCMD_HYPERCALL:\r\nret = privcmd_ioctl_hypercall(file, udata);\r\nbreak;\r\ncase IOCTL_PRIVCMD_MMAP:\r\nret = privcmd_ioctl_mmap(file, udata);\r\nbreak;\r\ncase IOCTL_PRIVCMD_MMAPBATCH:\r\nret = privcmd_ioctl_mmap_batch(file, udata, 1);\r\nbreak;\r\ncase IOCTL_PRIVCMD_MMAPBATCH_V2:\r\nret = privcmd_ioctl_mmap_batch(file, udata, 2);\r\nbreak;\r\ncase IOCTL_PRIVCMD_DM_OP:\r\nret = privcmd_ioctl_dm_op(file, udata);\r\nbreak;\r\ncase IOCTL_PRIVCMD_RESTRICT:\r\nret = privcmd_ioctl_restrict(file, udata);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int privcmd_open(struct inode *ino, struct file *file)\r\n{\r\nstruct privcmd_data *data = kzalloc(sizeof(*data), GFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\ndata->domid = DOMID_INVALID;\r\nfile->private_data = data;\r\nreturn 0;\r\n}\r\nstatic int privcmd_release(struct inode *ino, struct file *file)\r\n{\r\nstruct privcmd_data *data = file->private_data;\r\nkfree(data);\r\nreturn 0;\r\n}\r\nstatic void privcmd_close(struct vm_area_struct *vma)\r\n{\r\nstruct page **pages = vma->vm_private_data;\r\nint numpgs = vma_pages(vma);\r\nint numgfns = (vma->vm_end - vma->vm_start) >> XEN_PAGE_SHIFT;\r\nint rc;\r\nif (!xen_feature(XENFEAT_auto_translated_physmap) || !numpgs || !pages)\r\nreturn;\r\nrc = xen_unmap_domain_gfn_range(vma, numgfns, pages);\r\nif (rc == 0)\r\nfree_xenballooned_pages(numpgs, pages);\r\nelse\r\npr_crit("unable to unmap MFN range: leaking %d pages. rc=%d\n",\r\nnumpgs, rc);\r\nkfree(pages);\r\n}\r\nstatic int privcmd_fault(struct vm_fault *vmf)\r\n{\r\nprintk(KERN_DEBUG "privcmd_fault: vma=%p %lx-%lx, pgoff=%lx, uv=%p\n",\r\nvmf->vma, vmf->vma->vm_start, vmf->vma->vm_end,\r\nvmf->pgoff, (void *)vmf->address);\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\nstatic int privcmd_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTCOPY |\r\nVM_DONTEXPAND | VM_DONTDUMP;\r\nvma->vm_ops = &privcmd_vm_ops;\r\nvma->vm_private_data = NULL;\r\nreturn 0;\r\n}\r\nstatic int is_mapped_fn(pte_t *pte, struct page *pmd_page,\r\nunsigned long addr, void *data)\r\n{\r\nreturn pte_none(*pte) ? 0 : -EBUSY;\r\n}\r\nstatic int privcmd_vma_range_is_mapped(\r\nstruct vm_area_struct *vma,\r\nunsigned long addr,\r\nunsigned long nr_pages)\r\n{\r\nreturn apply_to_page_range(vma->vm_mm, addr, nr_pages << PAGE_SHIFT,\r\nis_mapped_fn, NULL) != 0;\r\n}\r\nstatic int __init privcmd_init(void)\r\n{\r\nint err;\r\nif (!xen_domain())\r\nreturn -ENODEV;\r\nerr = misc_register(&privcmd_dev);\r\nif (err != 0) {\r\npr_err("Could not register Xen privcmd device\n");\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __exit privcmd_exit(void)\r\n{\r\nmisc_deregister(&privcmd_dev);\r\n}
